
[00:00:00.000 --> 00:00:08.880]   Okay. So I'm here with Eugene. We are in Singapore. This is the first time I'm podcasting in Singapore,
[00:00:08.880 --> 00:00:15.520]   the first time I'm podcasting with my Singaporean accent. Eugene has been a very valued part
[00:00:15.520 --> 00:00:19.920]   of our Latentspace Discord for a while, and also diving deep into RWBKV. I think you're
[00:00:19.920 --> 00:00:23.600]   actually the first person that brought it to my attention as a potential Transformers
[00:00:23.600 --> 00:00:31.200]   Alternative. You're also CTO of UIlicious, which is a UI testing company that's in Singapore
[00:00:31.200 --> 00:00:37.120]   here. Anything else that you would flag out as like your high level intro?
[00:00:37.120 --> 00:00:43.040]   What brought me into AI machine learning is actually I started, I originally wrote GPU.js,
[00:00:43.040 --> 00:00:49.680]   so that allows you to run JavaScript code on the GPU. This was pre-neural network boom,
[00:00:49.680 --> 00:00:55.280]   my project got picked up by Braintop.js and merged in, and that's how I actually went to
[00:00:55.280 --> 00:00:59.440]   the mad rush. There's neural networks and then now subsequently large language models.
[00:00:59.440 --> 00:01:04.080]   So okay, let's talk about that a little bit. What was the origin story for GPU.js?
[00:01:04.080 --> 00:01:12.640]   So the origin story for GPU.js is that me and my friends at NUS, the local university here,
[00:01:12.640 --> 00:01:17.680]   we just wanted to run JavaScript. I think it was like the era where everyone's just trying to do
[00:01:17.680 --> 00:01:21.440]   everything on Node.js and npm packages. And we were just like...
[00:01:21.440 --> 00:01:23.600]   This was like 2016, 17?
[00:01:23.600 --> 00:01:28.240]   Yeah, it's quite far back. And then we were like, let's just do this for fun. Let's just prove that
[00:01:28.240 --> 00:01:33.440]   you can run JavaScript on a GPU, just because it should be faster theoretically for matrix
[00:01:33.440 --> 00:01:41.760]   multiplications. This is like Porsche. And it was meant to be a joke that yes, you can run
[00:01:41.760 --> 00:01:47.760]   JavaScript on anything. And we managed to get it to run it for that very narrow case of matrix
[00:01:47.760 --> 00:01:53.040]   multiplication. We outperformed the base V8 engine by running it on the WebGL.
[00:01:53.040 --> 00:01:53.540]   By a lot?
[00:01:53.540 --> 00:02:01.440]   Especially when you scale past 2000 dimensions. There is a gotcha, because you have to transfer
[00:02:01.440 --> 00:02:09.680]   your variables from the JavaScript space to the GPU space. So anything less than a thousand,
[00:02:09.680 --> 00:02:14.160]   five thousand, it tends to be not worth it. And then we just let the project just sit there on
[00:02:14.160 --> 00:02:22.080]   the internet. And it just sat there for one whole year until neural networks came in full steam,
[00:02:22.080 --> 00:02:26.640]   and someone picked it up and clustered it together. And it's like, hey, we can train neural
[00:02:26.640 --> 00:02:34.560]   networks in the browser in JavaScript. And that's how BrainJS grew on top of GPU.js.
[00:02:34.560 --> 00:02:39.520]   Right. And just because I have a little bit of background to this, I actually still don't know
[00:02:39.840 --> 00:02:47.760]   what specific APIs. Are you using WebGL? Are you basically abusing WebGL to get access to the GPU?
[00:02:47.760 --> 00:02:49.120]   Like, how do you get access to the GPU, basically?
[00:02:49.120 --> 00:02:54.240]   Oh, there's not really so much of an abuse. So the crazier abuse part is actually up front. So
[00:02:54.240 --> 00:03:00.720]   what we actually do is that when you submit a JavaScript code to GPU.js to execute in parallel,
[00:03:00.720 --> 00:03:06.400]   I think you can just view it as a very common reduce function. So you have that function and
[00:03:06.400 --> 00:03:11.360]   then your data. So you've got your large data arrays. You put it in there. What happens is
[00:03:11.360 --> 00:03:19.680]   we serialize your function into code. And then we do an analysis on it. And then we
[00:03:19.680 --> 00:03:27.040]   translate that into WebGL code. So we had to implement a lot of things that were in JavaScript,
[00:03:27.040 --> 00:03:33.120]   that were like shader code. At that point, it's still considered shader code that did not have
[00:03:33.120 --> 00:03:40.240]   support for. So for example, if you want to do a large number of manipulation, and we only had
[00:03:40.240 --> 00:03:45.120]   small floats in the system, what we do, we just had two floats, and then we just abuse the heck
[00:03:45.120 --> 00:03:51.760]   out of it. To simulate a big int? Yeah, things like that. Okay. So that's, in essence, what
[00:03:51.760 --> 00:03:58.960]   the GPU.js library did is that we took your code, abstract syntax tree, analyze it, we figure out
[00:03:58.960 --> 00:04:08.320]   what it does, then we rebuild the code in WebGL. Okay. So this is a compiler? Yeah.
[00:04:08.320 --> 00:04:13.360]   Why the compilation approach instead of like a library approach where people can just kind of
[00:04:13.360 --> 00:04:18.800]   use functions that you've made? I think it's back to the original goal of making it a joke.
[00:04:18.800 --> 00:04:25.520]   To run JavaScript on. Literally run JavaScript. Okay. So we didn't want you to need to learn
[00:04:26.720 --> 00:04:32.720]   new commands and things like that. Yeah, that's pretty crazy. Yeah. Okay. And because I had this
[00:04:32.720 --> 00:04:38.720]   initial confusion, Brain.js has nothing to do with TensorFlow, even though I think both were
[00:04:38.720 --> 00:04:46.080]   run by Google? No, Brain.js is not run by Google. It's more of a community driven project. Okay.
[00:04:46.080 --> 00:04:52.880]   So, and I think it's commonly confused with TensorFlow because, let's be realistic,
[00:04:52.880 --> 00:04:58.160]   if you want to train real models, you're not going to train it on JS. You're going to train
[00:04:58.160 --> 00:05:03.360]   it directly with CUDA and so on because it just performs much better. But there is a benefit of
[00:05:03.360 --> 00:05:09.440]   running it purely in a browser because you make it completely possible for like teachers. And yeah,
[00:05:09.440 --> 00:05:14.080]   in fact, one of our most popular users were teachers teaching students on how to make
[00:05:14.080 --> 00:05:19.120]   newer networks. And the barrier of entry is no, it's not you need a CUDA, you need a setup. No,
[00:05:19.120 --> 00:05:23.200]   you just need your browser, which makes it significantly easier, even though it's all
[00:05:23.200 --> 00:05:29.440]   toy models. And in that use case, TensorFlow.js and Brain.js is functionally the same with just
[00:05:29.440 --> 00:05:35.360]   different APIs, at least for serving this target market. Yeah. Yeah. I mean, it's the best user
[00:05:35.360 --> 00:05:40.320]   experience for sandboxing. You're just spinning something up without dependencies. Okay. And then
[00:05:40.320 --> 00:05:47.760]   so fast forward after GPU.js, what else did you get up to? So after GPU.js, that's where I moved
[00:05:47.760 --> 00:05:53.680]   on to running my own startup. So UIlicious. And I guess that was because I was at a time
[00:05:53.680 --> 00:06:01.200]   professionally working for banks and private institutes. And surprisingly for me, it's like
[00:06:01.200 --> 00:06:04.640]   why we have so much high tech applications, but at the end of the day, we are just testing a lot
[00:06:04.640 --> 00:06:09.840]   of things manually. And I just wanted to automate that. And that is why I started effectively a
[00:06:09.840 --> 00:06:16.240]   test automation company. And even then early on, we actually tried to automate things more
[00:06:16.240 --> 00:06:22.560]   with AI even, but we found that at least at that time, it was not ready. And fast forward,
[00:06:22.560 --> 00:06:27.360]   so we built a product around it where you can automate your browser using low code. Just go
[00:06:27.360 --> 00:06:34.000]   there, type simple command, go to Google, click on this text, run. Which is another compiler,
[00:06:34.000 --> 00:06:37.440]   compiled language, right? You had your own- Oh, that's actually in JavaScript.
[00:06:37.440 --> 00:06:42.880]   Testing language. Oh, there's a JavaScript library, but we focused on making it easy for
[00:06:43.440 --> 00:06:49.520]   manual testers. So if you see all the existing, let's say, browser automation libraries,
[00:06:49.520 --> 00:06:56.240]   they are all heavily async based. Teaching someone with zero programming skill how to deal with
[00:06:56.240 --> 00:07:02.720]   asyncs is a complete nightmare. So we make steps that, for example, we make it synchronous.
[00:07:02.720 --> 00:07:08.960]   We don't expect you to know CSS selector. We just ask you for your text on screen.
[00:07:08.960 --> 00:07:11.520]   Yeah. But it's still JavaScript.
[00:07:11.520 --> 00:07:16.560]   Yeah. Then that runs on Selenium, and then it does all that. So it's not AI,
[00:07:16.560 --> 00:07:21.040]   but the big jump for us was that subsequently, more recently, because we've been building our
[00:07:21.040 --> 00:07:27.680]   data set, we started having our own self AI on our platform where you can just describe your test,
[00:07:27.680 --> 00:07:29.520]   and it will generate for you. Right.
[00:07:29.520 --> 00:07:30.720]   Including hallucinations.
[00:07:30.720 --> 00:07:37.680]   So lots of fun. Yeah. And so how did you... So you were running UALicious,
[00:07:37.680 --> 00:07:41.360]   which is a local platform. I got the first demo maybe four years ago.
[00:07:41.360 --> 00:07:44.720]   Yes. And I was like, "Okay, fine. You're doing
[00:07:44.720 --> 00:07:48.640]   testing." There wasn't an obvious AI angle. I mean, now that you explained it, it was great. But
[00:07:48.640 --> 00:07:53.760]   what was your personal, like, "Okay, I'm going to be the dedicated AI guy for UALicious?"
[00:07:53.760 --> 00:08:02.160]   I think because for the most part, we knew that... Okay, so one of the things that I found very
[00:08:02.160 --> 00:08:10.240]   interesting with the huge transformer boom right now is that traditionally, and I think I have an
[00:08:10.240 --> 00:08:15.120]   article on this also, is that when you tell companies that you need, when you want to build
[00:08:15.120 --> 00:08:22.640]   your own AI, you need a really large data set. And over time, actually, the amount of data sets
[00:08:22.640 --> 00:08:25.840]   that you need is actually scaled down because you can just now find...
[00:08:25.840 --> 00:08:26.640]   Foundation models.
[00:08:26.880 --> 00:08:33.440]   Find your own foundation models. And when we started UALicious, we always knew at that time,
[00:08:33.440 --> 00:08:37.680]   because a lot of our other companies that were launched at the same time were dealing with neural
[00:08:37.680 --> 00:08:42.320]   networks that at some point, the data that we've been collecting data on, let's say,
[00:08:42.320 --> 00:08:48.080]   how to do testing website, it's just a very specific focus. Basically, every single test
[00:08:48.080 --> 00:08:53.920]   that has run on our platform, unless our customer has opt out or delete their account, basically
[00:08:53.920 --> 00:08:59.840]   privacy-related stuff, we actually still retain the test data. And that's something that we always
[00:08:59.840 --> 00:09:04.240]   felt that was useful in the long run to be able to actually build a huge training model.
[00:09:04.240 --> 00:09:07.680]   The irony of that was that even though we were building all those data sets,
[00:09:07.680 --> 00:09:10.800]   as the threshold came in and the transformer boom happened,
[00:09:10.800 --> 00:09:16.800]   we realized we don't actually need that big of a data set anymore to actually get a functional AI.
[00:09:16.800 --> 00:09:22.240]   Can you give order of magnitude? What were you expecting? And then what did you find? How off
[00:09:22.240 --> 00:09:31.760]   are we? Do you need millions of, I don't know, customer of test data? And then you found that
[00:09:31.760 --> 00:09:35.600]   it was just thousands? Just quantify something like that.
[00:09:35.600 --> 00:09:43.040]   And I think this is actually one of the key insights, especially for people who are trying
[00:09:43.040 --> 00:09:48.960]   to build on top of transformer model for their companies. Pre-transformer, large language
[00:09:48.960 --> 00:09:54.480]   models, we will always be thinking of in terms of 100 gigabytes of data, 1 gigabyte of data,
[00:09:54.480 --> 00:10:01.600]   multi-million dollar, millions of records for all the different examples. Post-transformer,
[00:10:01.600 --> 00:10:10.320]   you probably need only 1,000 or 10,000, enough data that you can literally get it in turn a few
[00:10:10.320 --> 00:10:16.320]   weeks to just get it done. And you have a working model. It may not be that great, but frankly,
[00:10:16.320 --> 00:10:22.240]   every piece of data you add after that is a diminishing returns.
[00:10:22.240 --> 00:10:27.760]   And it's specifically structured as, I mean, because it's a language model, it doesn't
[00:10:27.760 --> 00:10:30.560]   actually have any inherent understanding that it's automating the browser.
[00:10:30.560 --> 00:10:35.360]   So it's presented as like a prompt answer pair, like question answer pair.
[00:10:35.360 --> 00:10:41.040]   So typically, so at least for our internal model that our users are using, it's presented as here's
[00:10:41.040 --> 00:10:45.840]   the prompt, describe your test or what you want to modify the code, and then subsequently generate
[00:10:45.840 --> 00:10:53.920]   the code for you. So it's now in hindsight, it's now basically a copilot. I think now copilot is
[00:10:53.920 --> 00:11:00.000]   adding that chat widget. Are they fully on chat? Yes. I actually downloaded it yesterday. I haven't
[00:11:00.000 --> 00:11:05.760]   actually used it yet, but it is a separate VS Code extension. So there are now three copilot
[00:11:05.760 --> 00:11:11.360]   extensions shipped by GitHub because they have shipped their own chart. I'm very quite friendly
[00:11:11.360 --> 00:11:16.960]   with that team, but it's very funny. But just to come back to you, so did you implement this
[00:11:16.960 --> 00:11:23.360]   with GPT-3? Is that where it was? So what we implemented, what we trained for,
[00:11:23.360 --> 00:11:28.960]   at least our code model, we based it off the Salesforce CodeGen model. So that was the
[00:11:28.960 --> 00:11:34.160]   foundation model that we built on top. We are looking into replacing it in parts, but that
[00:11:34.160 --> 00:11:42.400]   becomes a longer conversation. CodeGen being the first really credible, open-source, code-specific
[00:11:42.400 --> 00:11:46.640]   language model that was released by literally anyone, I think about three years ago.
[00:11:46.640 --> 00:11:53.360]   And then they recently released CodeGen2. Any opinions on CodeGen2 while we're on this topic?
[00:11:53.360 --> 00:12:02.160]   I actually think, so in terms of CodeGen, one big appeal for the CodeGen and even CodeGen2 model is
[00:12:02.160 --> 00:12:05.920]   that Salesforce took a very clear and clean approach to the licensing.
[00:12:05.920 --> 00:12:11.520]   Meaning they were very, very clear that everything that they trained on was open-source?
[00:12:11.520 --> 00:12:18.320]   Yeah. MIT, they didn't touch the problematic like this. And you can imagine-
[00:12:18.320 --> 00:12:20.240]   And do you think that Copilot did?
[00:12:20.240 --> 00:12:29.520]   I'm knowing Microsoft's statement on how liberal they were about GitHub data. And they were saying,
[00:12:29.520 --> 00:12:32.880]   they used a term that is under fair use. I see.
[00:12:32.880 --> 00:12:39.840]   Yeah. I have no reason to believe that they didn't. But this same problem happens to actually
[00:12:39.840 --> 00:12:47.120]   a lot of existing CodeGen models. And that was actually the main appeal for me for running,
[00:12:47.120 --> 00:12:53.120]   for actually building on top of the Salesforce CodeGen model. Mostly also because for us,
[00:12:53.120 --> 00:12:58.560]   we deploy on-premise into enterprises in Europe, and they ask questions.
[00:12:58.560 --> 00:13:05.040]   So what does this deploy on-premise mean? You pack your UI into a container and you
[00:13:05.040 --> 00:13:08.080]   give it to them? And then it's like a license fee or something?
[00:13:08.080 --> 00:13:08.560]   Correct.
[00:13:08.560 --> 00:13:14.480]   Okay. Cool. That's very interesting. Yeah. Okay. I don't know if I have any other questions
[00:13:14.480 --> 00:13:20.720]   based on that. Anything else before we go into the reasons for alternative models?
[00:13:22.720 --> 00:13:35.200]   So let me set the premise, right? Transformers have won, for now.
[00:13:35.200 --> 00:13:36.480]   They've slid the neural networks?
[00:13:36.480 --> 00:13:44.640]   Yes. And it seems like you have had a history with machine learning since before Transformers,
[00:13:44.640 --> 00:13:52.320]   and now they're at the peak of their power. And I see that there's a desire for alternative
[00:13:52.320 --> 00:13:58.400]   models for a number of reasons, but I'm very curious as to what drives your personal interest
[00:13:58.400 --> 00:13:59.280]   in alternative models.
[00:13:59.280 --> 00:14:04.720]   So first things first, to be clear, the majority of our AI is still based on Transformer,
[00:14:04.720 --> 00:14:10.560]   at least within my company. But what drove me into alternatives beyond Transformer? In essence,
[00:14:10.560 --> 00:14:17.200]   once we actually managed to get our bot to generate UI testing code, the most obvious
[00:14:17.200 --> 00:14:25.200]   next thing that our customers started asking, "Hey, let's say the test failed. Can your AI now
[00:14:25.200 --> 00:14:30.880]   analyze my website and then tell me what's wrong and tell me what to change?" Basically,
[00:14:30.880 --> 00:14:34.480]   they're getting crazier and crazier. And that's the big issue.
[00:14:34.480 --> 00:14:36.320]   Humans are very good at moving goalposts.
[00:14:36.320 --> 00:14:44.160]   Yeah. And I was like, "Okay, yeah, that's something I was working on." And we had something
[00:14:44.160 --> 00:14:52.320]   working for toy websites. But the first thing that we did was that we started... One thing that
[00:14:52.320 --> 00:14:59.280]   we do internally is that we look at, I think, what was the list? Top 100, top 1,000 websites.
[00:14:59.280 --> 00:15:03.520]   And we basically just run, or we actually do run our test platform against that to see,
[00:15:03.520 --> 00:15:07.200]   make sure that our code works against any front-end platform.
[00:15:07.200 --> 00:15:11.040]   Well, what do you mean run your test platform, right? Because you don't have tests for them.
[00:15:11.760 --> 00:15:15.360]   Yeah. We have some very rudimentary basic test, like go to website, see something,
[00:15:15.360 --> 00:15:20.000]   click something, add to cart. Yeah, that's it. The idea is more of like, because there's so
[00:15:20.000 --> 00:15:22.320]   many frameworks out there. And our-
[00:15:22.320 --> 00:15:23.840]   You just want to make sure you cover all of them.
[00:15:23.840 --> 00:15:28.160]   Yeah. And so we did the same thing for our AI. And the first thing that it died on was
[00:15:28.160 --> 00:15:30.480]   literally Amazon.
[00:15:30.480 --> 00:15:32.240]   Why? Oh, five megabytes.
[00:15:32.240 --> 00:15:38.080]   Yeah. I think you heard me mention that. So when you are trying to analyze a website,
[00:15:38.080 --> 00:15:45.600]   it's like, we've been talking about increasing token count size, right? But for e-commerce
[00:15:45.600 --> 00:15:49.680]   websites in particular, even if it's stripped off of CSS, even if it's stripped off of JavaScript,
[00:15:49.680 --> 00:15:56.240]   having the entire HTML in megabyte size is not unheard of. And that's where it's like,
[00:15:56.240 --> 00:16:00.720]   how am I supposed to solve this in terms of an AI point of view?
[00:16:00.720 --> 00:16:01.760]   How many tokens would that be?
[00:16:02.320 --> 00:16:09.040]   Oh my gosh. Easily? I mean, for today, it's nothing, right? Like 10,000 tokens? It's not
[00:16:09.040 --> 00:16:09.840]   that much, right?
[00:16:09.840 --> 00:16:14.880]   No, because, okay, the tokenizer doesn't do very well with HTML for them.
[00:16:14.880 --> 00:16:15.760]   Oh, right. Okay.
[00:16:15.760 --> 00:16:18.720]   So you could easily be looking at over a million tokens.
[00:16:18.720 --> 00:16:21.440]   I see. Which is still too much even for today.
[00:16:21.440 --> 00:16:21.940]   Yeah.
[00:16:21.940 --> 00:16:25.280]   Did you look into making your own tokenizer?
[00:16:26.240 --> 00:16:32.240]   That's something that we explored. I think what we found more realistic was to actually
[00:16:32.240 --> 00:16:38.000]   pass the HTML into a more token-friendly format. So this way we can still build on top of existing
[00:16:38.000 --> 00:16:45.200]   models. But yeah, we are exploring that as well. But back to the alternative.
[00:16:45.200 --> 00:16:53.120]   So the key things for me was at that point, and subsequently, I think I showed you the
[00:16:53.120 --> 00:16:58.240]   experiments with English compiler and things like that, right? AI agent generating code.
[00:16:58.240 --> 00:17:06.800]   You also have your own small dev. Was that the context size is a real problem and transformer,
[00:17:06.800 --> 00:17:11.200]   inherently by its nature, at least the vanilla transformer, I know there's transformer XL and
[00:17:11.200 --> 00:17:21.280]   some other attempts, is that it quadratically scales with the context size. So if we scale
[00:17:21.280 --> 00:17:26.320]   to like, let's say 100,000, that's already requiring a shit ton of compute everywhere.
[00:17:26.320 --> 00:17:29.040]   And I don't even want to imagine what happens to 1 million or 10 million.
[00:17:29.040 --> 00:17:37.120]   And that's where I was like, okay, this is a fundamental problem that needs to be changed.
[00:17:37.120 --> 00:17:43.520]   If not, we will not go past this. And I think there's also now a lot of people who are very
[00:17:43.520 --> 00:17:47.760]   interested in models that can handle large context size, because they also want it to
[00:17:47.760 --> 00:17:52.640]   be able to use in use cases where they will never need to do fine-tuning. Fine-tuning is a pain,
[00:17:52.640 --> 00:17:59.280]   apparently. Yes. That said, okay, well, there's issues with just throwing everything in context,
[00:17:59.280 --> 00:18:06.480]   right? It's shown that retrieval is only best when the item that's relevant is in front or
[00:18:06.480 --> 00:18:12.080]   in the back of the context window. So basically, I'm just like, maybe we've just tapped out.
[00:18:12.640 --> 00:18:18.480]   Context is working memory, and maybe transformers are very similar to humans in that a working
[00:18:18.480 --> 00:18:22.720]   memory is only of a given size. If you try to artificially extend it, you just make it very
[00:18:22.720 --> 00:18:29.840]   lossy. Yeah. So that's where I ended up landing on the RWKV model, because in that sense, right,
[00:18:29.840 --> 00:18:36.240]   so one thing that I always found very weird for transformers, but I mean, it's my design,
[00:18:36.240 --> 00:18:41.680]   is as you infer each token, you are re-computing everything up front.
[00:18:41.680 --> 00:18:48.480]   That's the quadratic part. And, well, you're mentioning about the working memory problem.
[00:18:48.480 --> 00:18:55.600]   In theory, with enough attention heads on it, and people seem to be trying to cram more and
[00:18:55.600 --> 00:19:02.800]   more attention heads into the process, it could scale that way, ignoring compute costs. Ignoring
[00:19:02.800 --> 00:19:08.000]   compute costs is just like a very liberal, let's just throw as much H100s, it doesn't make sense.
[00:19:08.000 --> 00:19:17.680]   But, RWKV is still fundamentally a neural network at its core. It ends up scaling linearly as it
[00:19:17.680 --> 00:19:27.360]   goes through the tokens. It will still suffer from the memory issue. So, within the RWKV, we do
[00:19:27.360 --> 00:19:33.040]   measure two separate things. One, we call it the perfect memory. So, the model will have only a
[00:19:33.040 --> 00:19:38.960]   certain amount of capacity where it can remember things perfectly, just like humans. And then,
[00:19:38.960 --> 00:19:44.080]   beyond that, that is where it will start to discard things from its perfect memory.
[00:19:44.080 --> 00:19:44.640]   Right.
[00:19:44.640 --> 00:19:52.400]   And I felt that this was actually a lot more in line with our goals commercially. And also,
[00:19:52.400 --> 00:19:57.440]   what I felt was that it was more useful in the long run, because it's cheaper compute,
[00:19:57.440 --> 00:20:00.480]   and it could be potentially paralyzable for a very long time.
[00:20:00.480 --> 00:20:05.440]   Right. So, we're going to go into our RWKV paper in a bit, but one thing I wanted to ask,
[00:20:05.440 --> 00:20:08.640]   you kind of glossed over how you found it in the first place.
[00:20:08.640 --> 00:20:09.280]   How did I find it?
[00:20:09.280 --> 00:20:14.000]   Because you're not a researcher. I don't imagine you're reading papers every day or something.
[00:20:14.000 --> 00:20:15.600]   Until recently.
[00:20:15.600 --> 00:20:18.960]   Until recently. How did you find it?
[00:20:18.960 --> 00:20:19.760]   How did I find it?
[00:20:19.760 --> 00:20:24.400]   How do you know this is the one to bet on versus there's a bunch of other alternatives, right?
[00:20:25.040 --> 00:20:32.640]   I think what was quick, I think it was rather quick after I concluded that
[00:20:32.640 --> 00:20:36.320]   Transformer as it is will not scale to 10 million tokens.
[00:20:36.320 --> 00:20:41.520]   Okay. And so, by the way, you mentioned Transformer 6L.
[00:20:41.520 --> 00:20:48.000]   We also did an episode on Flash Attention, which helps to make part of it sublinear, at least.
[00:20:48.000 --> 00:20:52.560]   Yeah, but that is like way, way after I already dived into RWKV. So, history-wise,
[00:20:52.560 --> 00:20:58.880]   at that point in time, we're talking about when 4K was the limit that everyone knew.
[00:20:58.880 --> 00:21:02.640]   Right. And this was last year. I mean, just to set context. Okay.
[00:21:02.640 --> 00:21:10.960]   Okay. And then, yeah. So, you just kind of were searching around and you found RWKV.
[00:21:10.960 --> 00:21:14.320]   Presumably, did you go straight into the Discord?
[00:21:14.320 --> 00:21:18.000]   Was it primarily a GitHub repo? What was it?
[00:21:19.680 --> 00:21:23.840]   As far as I can tell, there was no paper until maybe about two months ago.
[00:21:23.840 --> 00:21:27.120]   Oh, and I talked about it before the paper, right?
[00:21:27.120 --> 00:21:33.040]   Yes. So, you found it before they did any publicity, which is weird. It's not normal.
[00:21:33.040 --> 00:21:35.360]   So, what did you do?
[00:21:35.360 --> 00:21:43.200]   So, what I did... Okay. So, it was basically... I believe... Okay. So, it's a mixture of things
[00:21:43.200 --> 00:21:49.600]   because it's like, I was searching GitHub, I was searching forums, other Discords,
[00:21:49.600 --> 00:21:51.680]   and also blogs, actually.
[00:21:51.680 --> 00:21:55.760]   Can you shout out which Discords and which forums were super helpful to you?
[00:21:55.760 --> 00:22:02.400]   Super helpful would be mostly Elutian's forum, Discord itself. Blogs... It's very hard to
[00:22:02.400 --> 00:22:04.800]   pinpoint today because at that point in time, it was just like...
[00:22:04.800 --> 00:22:06.080]   Random people's blogs.
[00:22:06.080 --> 00:22:10.160]   Yeah. I was just getting all the... Because everyone was just creating lists of lists,
[00:22:10.160 --> 00:22:13.600]   right? And I believe you also have a list of lists somewhere.
[00:22:13.600 --> 00:22:18.960]   Yeah, but mine is very... So, I would consider myself very trad in the sense that I would
[00:22:18.960 --> 00:22:23.520]   just follow the large model labs, whereas the kind of list that you have to follow in order
[00:22:23.520 --> 00:22:30.960]   to get to something like RWBKB before they've done any publicity is the non-trad... The kind
[00:22:30.960 --> 00:22:36.480]   of people that is not working on Windows Hermes, Wizard, no credentials. I don't even know who
[00:22:36.480 --> 00:22:38.640]   the hell they are, but they're just working on it.
[00:22:38.640 --> 00:22:48.160]   Oh, so the list... Okay, this is all foggy memory, and I might be hallucinating this
[00:22:48.160 --> 00:22:52.160]   because there was too many lists, but I believe the list that actually what brought me to
[00:22:52.160 --> 00:22:58.640]   RWBKB was that beyond... So, this is something... This is a topic that we can actually touch
[00:22:58.640 --> 00:23:06.400]   upon later, right? Beyond OpenAI's model, and beyond Chet Chibiti and Claudia, the two
[00:23:06.400 --> 00:23:12.320]   big models, outside of the English-speaking nations, a lot of the open source models really
[00:23:12.320 --> 00:23:23.120]   fall flat. And that is why when you actually go through lists for doing things in other
[00:23:23.120 --> 00:23:30.800]   languages, RWBKB actually stood out at that point. And just on the basic premise, and
[00:23:30.800 --> 00:23:33.840]   we're not even talking about architectural advantages, it's just the basic premise that
[00:23:33.840 --> 00:23:38.640]   they imported the data set in other languages in the training data.
[00:23:38.640 --> 00:23:43.360]   Was that a... Because, I mean, I imagine 99% of your customers are English.
[00:23:43.360 --> 00:23:43.840]   Yeah.
[00:23:43.840 --> 00:23:45.360]   Was that really a driver for you?
[00:23:45.360 --> 00:23:46.080]   It wasn't a driver, but...
[00:23:46.080 --> 00:23:46.960]   Or you just tried to explain it?
[00:23:46.960 --> 00:23:50.480]   Yeah, that's how I landed onto all these blogs and...
[00:23:50.480 --> 00:23:54.960]   And can you say... When you say fall flat, the main one that I know about is there's
[00:23:54.960 --> 00:23:57.600]   a tokenizer penalty for non-English.
[00:23:57.600 --> 00:23:58.480]   Yeah, that's it.
[00:23:58.480 --> 00:24:03.280]   Right? So, Chinese is up to... Chinese or Japanese or Thai or something, it's like 16
[00:24:03.280 --> 00:24:07.520]   times the number of tokens for a typical English sentence.
[00:24:07.520 --> 00:24:12.720]   Yeah, but even before that, right? Because, I mean, I think you understand a lot of community
[00:24:12.720 --> 00:24:15.920]   users, they want to not use the commercial APIs.
[00:24:15.920 --> 00:24:16.480]   Okay.
[00:24:16.480 --> 00:24:18.320]   So they try to find open source models.
[00:24:18.320 --> 00:24:20.800]   Yes. And we'll talk about the not safe for work people.
[00:24:20.800 --> 00:24:24.960]   I really want... Because you've actually talked to them. I have never talked to these people,
[00:24:24.960 --> 00:24:29.920]   but when I discovered them, it's a huge community, they're extremely passionate,
[00:24:29.920 --> 00:24:31.040]   and they're actually good.
[00:24:31.040 --> 00:24:32.080]   Yeah, they're really good.
[00:24:32.080 --> 00:24:36.000]   They're good at this. So let's talk about that, right? Yeah, we can talk about it later.
[00:24:36.000 --> 00:24:44.000]   Yeah, so they don't want to use the commercial models, and they want to use the open source
[00:24:44.000 --> 00:24:49.360]   model. And there is a tokenizer penalty, which is true. But I think on the more fundamental
[00:24:49.360 --> 00:24:54.960]   basis, if you look through the data sets, and this is also partially in fault, because
[00:24:54.960 --> 00:25:01.520]   the way we set up our evals, all evals are written in English. And at least for the majority
[00:25:01.520 --> 00:25:07.360]   of them, and if we are racing toward building AI models, at least right now, yes, you see
[00:25:07.360 --> 00:25:10.640]   all the companies as they build their open source model, and they just want to narrowly
[00:25:10.640 --> 00:25:17.440]   focus on the evals, adding in a foreign data set is actually a loss. Because once you're
[00:25:17.440 --> 00:25:20.160]   below a certain parameter, so we're talking about seven and four, right?
[00:25:20.160 --> 00:25:26.880]   The more you add that's not in line with your evals, the more it will degrade. And they
[00:25:26.880 --> 00:25:29.680]   just excluded it. So the model just...
[00:25:29.680 --> 00:25:31.280]   The priority is English. Yeah, I get it.
[00:25:31.280 --> 00:25:33.520]   The model just fundamentally didn't support...
[00:25:33.520 --> 00:25:38.960]   So what's the trade-off? I mean, okay, so English and Chinese, or... There's all these
[00:25:38.960 --> 00:25:40.320]   other languages, what do you pick?
[00:25:40.320 --> 00:25:50.720]   So Adobe KB started with... Also in context, the main person leading the Adobe KB project,
[00:25:50.720 --> 00:25:55.200]   Blink, is from China. So he naturally has an interest to make sure it supports Chinese.
[00:25:55.200 --> 00:25:55.680]   Of course.
[00:25:55.680 --> 00:25:56.800]   Yeah, so English...
[00:25:56.800 --> 00:26:00.720]   And there are a fair amount of bilingual models, especially English and Chinese from
[00:26:00.720 --> 00:26:02.560]   the major universities in China.
[00:26:02.560 --> 00:26:09.360]   So we started from basically English, Chinese, Japanese, Korean. Frankly, this is a large
[00:26:09.360 --> 00:26:15.440]   part, mostly because there were fans in those communities that came on board. And then
[00:26:15.440 --> 00:26:17.920]   subsequently, we tried to onboard other languages as well.
[00:26:17.920 --> 00:26:22.320]   Yeah. But these people are, again, not researchers.
[00:26:22.320 --> 00:26:22.560]   Nope.
[00:26:22.560 --> 00:26:23.840]   No money.
[00:26:23.840 --> 00:26:24.960]   Nope.
[00:26:24.960 --> 00:26:27.200]   Training on their home GPU lab or whatever, right?
[00:26:28.480 --> 00:26:33.600]   Partially true, but... So how this works out, right? So for the Adobe KB model, at
[00:26:33.600 --> 00:26:38.800]   least how I see it works out for a lot of the other languages was that we have the
[00:26:38.800 --> 00:26:44.000]   foundation model. And this is the foundation model where we just kind of say, "If I was
[00:26:44.000 --> 00:26:46.960]   to be them, let's just make sure to include all the other languages."
[00:26:46.960 --> 00:26:57.040]   And when we included the other languages, the model works for most parts for the other
[00:26:57.040 --> 00:27:03.680]   language. Subsequently, these individuals who wanted to use these models for their
[00:27:03.680 --> 00:27:09.120]   respective use cases, we will then fine-tune respectively. Because it's easier to fine-tune
[00:27:09.120 --> 00:27:13.120]   in another language for your use case than... I mean, this is just classic fine-tuning,
[00:27:13.120 --> 00:27:14.960]   than to train the language from scratch.
[00:27:14.960 --> 00:27:21.360]   And I think more recently, and this model is not 100% trained yet, but more recently,
[00:27:22.480 --> 00:27:29.280]   Adobe KB has released what we call the World Model, where we go the next step of even
[00:27:29.280 --> 00:27:36.400]   including all the translation data sets that we can find, even for minority languages that
[00:27:36.400 --> 00:27:41.600]   people end in our Discord. Because the goal for them, the long-term goal for us, at least
[00:27:41.600 --> 00:27:46.240]   internally, is that we wanted an AI model for everyone. And everyone does not mean USA,
[00:27:46.240 --> 00:27:47.040]   it means the world.
[00:27:47.040 --> 00:27:48.720]   Wow.
[00:27:48.720 --> 00:27:51.120]   So there are a lot of languages in there.
[00:27:51.120 --> 00:27:56.480]   Well, is it Asia-biased? Give me a sense.
[00:27:56.480 --> 00:28:01.840]   It's probably, no offense, probably still going to be US-biased in terms of knowledge.
[00:28:01.840 --> 00:28:07.520]   Because what we are doing is still PAL, Red Pyjamas for the knowledge, but in terms of
[00:28:07.520 --> 00:28:12.720]   language, we add all the other languages, wiki and translation set. So it's hard. I mean,
[00:28:12.720 --> 00:28:17.760]   we haven't fully evaluated the bias yet, but I'm quite sure that when disproportionately
[00:28:17.760 --> 00:28:23.760]   knowledge is still within the English universe, there's the bias there. But frankly, we are
[00:28:23.760 --> 00:28:30.800]   still at the stage where we can support the other languages. And I think I mentioned this,
[00:28:30.800 --> 00:28:35.680]   one of the interesting parallels that sometimes I have is that I can be in the, I can see in
[00:28:35.680 --> 00:28:40.080]   the illiterate forums and all that. And then we're talking about alignment and we're talking
[00:28:40.080 --> 00:28:40.880]   about it in very...
[00:28:40.880 --> 00:28:45.840]   Which is, yeah, very keen on safety and all that, which is great, but it's not your goal
[00:28:45.840 --> 00:28:47.520]   as the Adobe KB community.
[00:28:47.840 --> 00:28:52.800]   Yeah. And when you talk to members of the community that came on board and said, "Oh,
[00:28:52.800 --> 00:29:00.400]   I want to get this to work for Korean, Japanese, Thai, Arabic languages," and so on, they just
[00:29:00.400 --> 00:29:06.160]   want something that worked. They don't want it to be... They are not after the big model
[00:29:06.160 --> 00:29:09.920]   that does everything. They just want something that they can play with in their language.
[00:29:09.920 --> 00:29:11.840]   And that was very important to them.
[00:29:11.840 --> 00:29:20.480]   Yeah. And these are literally just hackers doing it for personal enjoyment, not yet for
[00:29:20.480 --> 00:29:23.440]   work, or maybe some of them for work. We don't know.
[00:29:23.440 --> 00:29:30.000]   We don't know. I mean, the whole character AI category, there's quite a number of them
[00:29:30.000 --> 00:29:33.520]   using it for that, so professionally.
[00:29:33.520 --> 00:29:40.160]   Professionally. Okay. As in they run character companies, let's call it. Okay, cool. Yeah.
[00:29:40.160 --> 00:29:47.280]   So, I'll signal that I'm interested in doing an AI waifu episode, and I need to find the
[00:29:47.280 --> 00:29:52.720]   perfect... Someone doing that to just explain everything that they found. Actually, I'm
[00:29:52.720 --> 00:29:57.360]   very interested in basically pairing this with a psychology professor who can ask psychological
[00:29:57.360 --> 00:30:02.560]   questions about, "What have you found about human sexuality and human behavior when you're
[00:30:02.560 --> 00:30:06.400]   just talking to an AI bot?" I think it's very... I don't know. I think no one's covering this.
[00:30:06.400 --> 00:30:12.800]   So, I listened to... I actually listened to a few psychology podcasts, and they're completely
[00:30:12.800 --> 00:30:17.520]   out of the loop. They're not even aware that this is going on, and it's so huge. It's literally
[00:30:17.520 --> 00:30:18.800]   millions of people, right?
[00:30:18.800 --> 00:30:24.240]   Yeah. So, they're not aware about people using AI, I guess, in the form of therapy?
[00:30:24.240 --> 00:30:26.560]   Or personal companionship?
[00:30:26.560 --> 00:30:28.640]   Well, they're not talking about it.
[00:30:28.640 --> 00:30:30.720]   Oh. Okay.
[00:30:30.720 --> 00:30:36.240]   It's maybe not a polite conversation, especially because it's not safe for work, but I think
[00:30:36.240 --> 00:30:38.720]   it's just an emerging category that is interesting.
[00:30:38.720 --> 00:30:43.520]   Yeah. Especially... I mean, it's just going to be cut straight to the chase, especially
[00:30:43.520 --> 00:30:43.920]   Japan.
[00:30:43.920 --> 00:30:51.840]   Yeah. Yeah. Well, and then there's also... We always say AI waifu, but actually, I always
[00:30:51.840 --> 00:30:54.000]   call this AI husbando. It's actually more...
[00:30:54.000 --> 00:30:54.880]   Yeah, that's it, too.
[00:30:54.880 --> 00:30:55.440]   It's bigger.
[00:30:55.440 --> 00:30:58.240]   Bigger? Oh, I wasn't aware about market science.
[00:30:58.240 --> 00:31:04.400]   It's bigger. Yes. I've actually looked into this, and so I can resolve this with a very,
[00:31:04.400 --> 00:31:10.000]   very simple example that everybody will understand, right? Amazon Kindle Unlimited is the
[00:31:10.000 --> 00:31:14.720]   subscription service where you can just pay a monthly fee and get all the books you want.
[00:31:14.720 --> 00:31:16.080]   What sells the most?
[00:31:16.080 --> 00:31:20.400]   Romance novels? I mean, romance novels?
[00:31:20.400 --> 00:31:20.960]   For women.
[00:31:20.960 --> 00:31:22.160]   Oh.
[00:31:22.160 --> 00:31:24.160]   Because they like to read about romance.
[00:31:24.160 --> 00:31:26.480]   I mean, that makes a lot of sense.
[00:31:26.480 --> 00:31:28.880]   Men are visual, women are verbal.
[00:31:28.880 --> 00:31:32.320]   And in this case, language models are text.
[00:31:32.320 --> 00:31:33.760]   Exactly.
[00:31:33.760 --> 00:31:36.720]   I mean, they do try to dress it up.
[00:31:36.720 --> 00:31:42.320]   Yes. Okay, cool. So I think that's great. Shall we pause here, and then I'll switch
[00:31:42.320 --> 00:31:43.120]   to the screen?
[00:31:43.120 --> 00:31:43.600]   Sure, sure.
[00:31:43.600 --> 00:31:50.720]   Okay. All right, so we have it pulled up. We are going to screen share for the bulk
[00:31:50.720 --> 00:31:54.320]   of this, so if you're listening on audio, it might be a good time to switch to the YouTube
[00:31:54.320 --> 00:31:58.400]   channel. So we're just going to start with an intro. What is RWKV?
[00:31:58.400 --> 00:32:07.280]   So RWKV is a modern recursive neural network with transformer-like level of LMM performance,
[00:32:07.280 --> 00:32:12.480]   which can be trained in a transformer mode. And this part has already been benchmarked
[00:32:12.480 --> 00:32:19.760]   against GPT-NeoX in the paper, and it has similar training performance compared to
[00:32:19.760 --> 00:32:24.160]   transformer models of the same data set and parent count, so specifically the GPT-NeoX
[00:32:24.160 --> 00:32:31.440]   model. So the key thing is that even though it's matching in performance, well, trading
[00:32:31.440 --> 00:32:36.880]   both in GPT-NeoX, it's doing all this without attention layers. And in the process, right,
[00:32:36.880 --> 00:32:40.880]   it's actually having a much substantially lower compute based on its design, and also
[00:32:40.880 --> 00:32:44.800]   because it's a neural network, which we will dive into later why that's substantially
[00:32:44.800 --> 00:32:50.880]   lower in both training and inference. And this is back to, like I mentioned previously,
[00:32:51.440 --> 00:32:56.400]   transformer, traditionally transformer until we found out about transformer XL and things
[00:32:56.400 --> 00:33:02.640]   like that, tends to scale quadratically based on the contact size. And this applies not
[00:33:02.640 --> 00:33:09.760]   just in inference, but in training. And due to how this is still a neural network in its
[00:33:09.760 --> 00:33:14.960]   heart, even though it can train like a transformer, it's able to do so much more efficiently and
[00:33:14.960 --> 00:33:22.240]   faster, especially when you hit contact size of 8K, 16K, and above. And once you do quadratic
[00:33:22.240 --> 00:33:28.400]   and linear, the differences start to go crazy once you scale the numbers up. And that was
[00:33:28.400 --> 00:33:34.640]   the main benefits of the IWKV model, per se. There were a few prominent researchers when
[00:33:34.640 --> 00:33:39.680]   they actually reviewed through the IWKV paper when it came out, they did highlight an important
[00:33:39.680 --> 00:33:45.600]   question of like, is this evidence to literally, maybe all that really matters is that we need
[00:33:45.600 --> 00:33:54.720]   a large data set and a scalable model. That makes sense, obviously, to some approximation.
[00:33:54.720 --> 00:34:02.160]   But you are still using attention? No, we don't use attention inside.
[00:34:02.160 --> 00:34:08.800]   Okay. Yeah. Maybe let's rewind a little bit. Specifically attention as you understood it.
[00:34:08.800 --> 00:34:16.960]   Yeah. Okay. Tell us more. So we use weighted receptors and...
[00:34:16.960 --> 00:34:19.760]   And if there's any diagrams I should pull up, let me know.
[00:34:19.760 --> 00:34:28.880]   Oh, okay. Okay, so we are using AFD. So this attention-free transformer, and this paper was
[00:34:28.880 --> 00:34:34.640]   written by... What the hell is an attention-free transformer? Okay, this is unusual.
[00:34:34.640 --> 00:34:44.800]   Yeah, so we basically, we use the weighted retention weights and we compute over it.
[00:34:44.800 --> 00:34:52.720]   And in essence, right, this is like the classic stacking more layers. Once you do on top of it,
[00:34:52.720 --> 00:35:01.600]   you don't really need attention once you have enough weights and layers stacked on it.
[00:35:04.400 --> 00:35:08.960]   Okay. I don't know whether we want to go into the deep dive of AFD.
[00:35:08.960 --> 00:35:11.680]   Sure. That's interesting. I've never heard of this paper.
[00:35:11.680 --> 00:35:17.680]   Yeah. So this was written by Apple and subsequently we integrate, at least blink,
[00:35:17.680 --> 00:35:24.880]   the creator, RWKB, took this and applied it to a language model and scaled it up.
[00:35:24.880 --> 00:35:32.080]   Right. And that is how we landed on RWKB that doesn't use attention. So
[00:35:33.120 --> 00:35:37.520]   sometimes within the community, we use the word "light attention" because what happens is that
[00:35:37.520 --> 00:35:42.320]   these layers and these weights will still play the role of attention.
[00:35:42.320 --> 00:35:45.680]   I was going to say, you end up approximating attention.
[00:35:45.680 --> 00:35:52.640]   Exactly. So it ends up like looking at the tokens or parts of the memory and then applying it to
[00:35:52.640 --> 00:35:58.240]   the output. So, well, and the key benefits is that, because remember the attention model is
[00:35:58.240 --> 00:36:03.600]   a multi-head part, it will need to scan all the tokens back and forth. This removes that requirement
[00:36:03.600 --> 00:36:08.560]   and hence it reduced the overall compute count. I might be jumping back and forth a bit, but that's
[00:36:08.560 --> 00:36:15.120]   the one of the key essence of the WKB segments. And we call it light attention. And this is the
[00:36:15.120 --> 00:36:20.720]   part where I would disagree with the RWKB community in some parts. I think that was a bad name.
[00:36:20.720 --> 00:36:22.320]   Ah, whatever.
[00:36:23.760 --> 00:36:32.160]   Why is it a bad name? This is the part where, because when the RWKB paper came out,
[00:36:32.160 --> 00:36:40.240]   RWKB paper came out, right? And then we talk about like, we use this and we call it light
[00:36:40.240 --> 00:36:45.520]   attention, but by design, it's really nothing like your existing attention weight models.
[00:36:45.520 --> 00:36:51.280]   And it ended up like sidetracking the Hacker Noon debate on like one corner. I was like,
[00:36:51.280 --> 00:36:55.040]   no, this is technically attention, approximating attention. Then another group is like, no,
[00:36:55.040 --> 00:36:56.880]   this is not attention. I see.
[00:36:56.880 --> 00:37:02.480]   But I'm like, propose a better name because I have no idea what to call it.
[00:37:02.480 --> 00:37:09.200]   Okay. What else should people know? Maybe we can explain what RWKB stands for.
[00:37:09.200 --> 00:37:13.120]   You have to open that in the paper.
[00:37:13.120 --> 00:37:16.320]   I think the paper is here.
[00:37:16.560 --> 00:37:22.720]   So this is RWKB receptive with the key value.
[00:37:22.720 --> 00:37:26.880]   Okay. Yeah. And each of these are like actual things that you model in the code, right?
[00:37:26.880 --> 00:37:29.920]   Correct. So we can go into that.
[00:37:29.920 --> 00:37:33.760]   Which attention historically is a query key value.
[00:37:33.760 --> 00:37:38.800]   Correct. Okay. So do you want to jump straight into the layer architecture?
[00:37:38.800 --> 00:37:40.080]   Should we cover something else first?
[00:37:43.520 --> 00:37:46.240]   I mean, anything like high level, right?
[00:37:46.240 --> 00:37:48.240]   High level. Okay. There's a 7B, there's a 14B.
[00:37:48.240 --> 00:37:52.080]   Oh, okay. So that's one of the assets or the artifacts.
[00:37:52.080 --> 00:37:56.800]   Okay. So before we go into the nitty gritties of how the layering and everything works,
[00:37:56.800 --> 00:38:01.760]   on a high level, right, currently RWKB architecturally as a model, it can be,
[00:38:01.760 --> 00:38:06.080]   what we have already proven is that it can be scaled and trained like a transformer.
[00:38:06.080 --> 00:38:12.720]   How I do so, we'll cover later. And this can be scaled to as many parameters as we want.
[00:38:12.720 --> 00:38:19.440]   Currently, what we have is a dominant, our main models is the 7B model and the 14B model,
[00:38:19.440 --> 00:38:23.360]   which you can find on Hugging Face or respectively our demos.
[00:38:23.360 --> 00:38:30.000]   We also have, there'll be the, there'll be the RWKB Raven models.
[00:38:30.000 --> 00:38:34.880]   These are also instructionally tuned for, it's not here.
[00:38:34.880 --> 00:38:37.920]   I'm so sorry.
[00:38:39.760 --> 00:38:41.120]   There's probably at the bottom, models.
[00:38:41.120 --> 00:38:45.520]   I see. Yeah. Okay. It's on Hugging Face.
[00:38:45.520 --> 00:38:48.400]   These are the UX issues that I need to fix.
[00:38:48.400 --> 00:38:51.200]   You only discover it when you talk about it.
[00:38:51.200 --> 00:38:52.000]   Yeah, I know.
[00:38:52.000 --> 00:38:56.720]   Okay. So there's world, there's Raven, there's music. Oh my God. There's novel. What is all this?
[00:38:56.720 --> 00:39:08.960]   Okay. So before we go, the current main models is RWKB for the PAL and Raven.
[00:39:08.960 --> 00:39:11.760]   So this, so PAL is basically just a PAL plus model.
[00:39:11.760 --> 00:39:13.360]   What is PAL plus?
[00:39:13.360 --> 00:39:14.880]   I know about PAL, but what is PAL plus?
[00:39:14.880 --> 00:39:17.520]   Random data sets that the community should read about.
[00:39:17.520 --> 00:39:19.760]   How many tokens worth?
[00:39:19.760 --> 00:39:25.440]   I would just say slightly 1.1 or 1.2 times the PAL.
[00:39:25.440 --> 00:39:25.840]   Okay.
[00:39:25.840 --> 00:39:32.160]   Yeah. This is not instruction tuned and stuff.
[00:39:32.160 --> 00:39:34.880]   Yeah. The plus one is typically all the other languages.
[00:39:34.880 --> 00:39:38.640]   Subsequently, Raven are the instruction tuned model.
[00:39:38.640 --> 00:39:41.040]   This is the current main complete models.
[00:39:41.040 --> 00:39:44.000]   We subsequently have-
[00:39:44.000 --> 00:39:45.440]   And the instruction data sets are from?
[00:39:45.440 --> 00:39:53.200]   Typically, GPT-4, but then we scrub it for every move or the SLR.
[00:39:53.200 --> 00:39:55.360]   So yeah, this would be the uncensored.
[00:39:55.360 --> 00:39:58.960]   There's someone, there's some other project that's kind of doing something similar
[00:39:58.960 --> 00:40:02.400]   and they call it uncensored, but really they just scrubbed it as a larger model.
[00:40:02.400 --> 00:40:03.360]   Correct. Yeah.
[00:40:03.360 --> 00:40:10.400]   So that makes it technically breaking TOS of OpenAI, right?
[00:40:10.400 --> 00:40:10.880]   Yeah.
[00:40:10.880 --> 00:40:11.840]   Okay. But yeah.
[00:40:11.840 --> 00:40:13.280]   But that's a, I mean-
[00:40:13.280 --> 00:40:14.080]   That's a later problem.
[00:40:14.080 --> 00:40:15.840]   Listen, frankly, let's be honest.
[00:40:15.840 --> 00:40:20.080]   Even if we don't remove it, someone is going to remove it.
[00:40:20.080 --> 00:40:25.760]   I mean, so there's ways around this, which is you get clean data sets that are not GPT-4.
[00:40:25.760 --> 00:40:30.320]   The one that I typically mention is Yonic Culture's Open Assistance.
[00:40:30.320 --> 00:40:32.640]   And I believe that was included subsequently as well.
[00:40:32.640 --> 00:40:33.140]   Yeah.
[00:40:33.140 --> 00:40:36.960]   Yeah, obviously all these release orders are all over the place.
[00:40:36.960 --> 00:40:37.520]   Yeah.
[00:40:37.520 --> 00:40:39.040]   So okay, Raven, World.
[00:40:39.040 --> 00:40:40.800]   So Raven is the instruction team model.
[00:40:40.800 --> 00:40:46.480]   And then subsequently, the World model is a new model that we are training.
[00:40:46.480 --> 00:40:47.680]   It's not 100% complete yet.
[00:40:47.680 --> 00:40:48.180]   Okay.
[00:40:48.180 --> 00:40:52.320]   With the focus on a new tokenizer and all the languages.
[00:40:52.320 --> 00:40:54.320]   So what we-
[00:40:54.320 --> 00:40:55.280]   All the languages.
[00:40:55.280 --> 00:40:58.320]   All the languages that we can grab from the internet.
[00:40:58.320 --> 00:41:00.800]   All the wikis in all the respective languages.
[00:41:00.800 --> 00:41:04.880]   Now, please don't use five words, not yet, really.
[00:41:04.880 --> 00:41:05.380]   Okay, okay.
[00:41:05.380 --> 00:41:07.680]   No, no, I just want to see the description, right?
[00:41:07.680 --> 00:41:09.200]   Like, what do you mean when you say all languages?
[00:41:09.200 --> 00:41:09.920]   100 languages.
[00:41:09.920 --> 00:41:10.640]   Okay, fine.
[00:41:10.640 --> 00:41:12.240]   So 100 languages.
[00:41:12.240 --> 00:41:15.920]   It wasn't really a very precise sign.
[00:41:15.920 --> 00:41:16.960]   We just basically-
[00:41:16.960 --> 00:41:22.640]   Whatever the wiki tool that allows us to download the ex-wiki languages.
[00:41:22.640 --> 00:41:24.240]   If it works, it's in the set.
[00:41:24.240 --> 00:41:25.520]   If it doesn't work, skip.
[00:41:25.520 --> 00:41:26.880]   Yeah.
[00:41:26.880 --> 00:41:30.880]   And all the major prominent OSCQR translation sets.
[00:41:30.880 --> 00:41:32.720]   So as you can see, PAL, red pajamas.
[00:41:32.720 --> 00:41:33.680]   All right, what is OSCQR?
[00:41:33.680 --> 00:41:37.360]   OSCQR is just a common term that we use in-
[00:41:37.360 --> 00:41:40.160]   You can just search OSCQR in Hugging Face dataset, and it just means translations.
[00:41:40.160 --> 00:41:41.540]   Okay.
[00:41:41.540 --> 00:41:45.040]   So you can find, like, English X pairs.
[00:41:45.040 --> 00:41:45.600]   I see.
[00:41:45.600 --> 00:41:46.800]   Yeah, all the respective pairs.
[00:41:46.800 --> 00:41:47.600]   Okay, yeah.
[00:41:47.600 --> 00:41:50.320]   So, and then all charity data I can find.
[00:41:50.320 --> 00:41:53.520]   Okay, so 70% English, 15% multilang, 15% code.
[00:41:53.520 --> 00:41:56.800]   Is there a strong grounding for why 15% code?
[00:41:57.440 --> 00:41:58.480]   Um, no.
[00:41:58.480 --> 00:42:00.960]   It was just, it was already there.
[00:42:00.960 --> 00:42:01.600]   Yeah.
[00:42:01.600 --> 00:42:05.840]   The focus of the whole model was not to improve everything else.
[00:42:05.840 --> 00:42:08.080]   It was literally that 15% multilang.
[00:42:08.080 --> 00:42:09.120]   We wanted to increase-
[00:42:09.120 --> 00:42:11.440]   It was English and code, and then you just added multilang.
[00:42:11.440 --> 00:42:15.120]   Yeah, we had a fair bit of multilang, but we wanted to bump it up.
[00:42:15.120 --> 00:42:18.400]   Right, so this is primarily English?
[00:42:18.400 --> 00:42:19.840]   Whatever, okay.
[00:42:19.840 --> 00:42:20.340]   Yeah.
[00:42:20.340 --> 00:42:23.760]   What I would like is, like, basically like a visual of, like,
[00:42:23.760 --> 00:42:24.800]   here's all the building blocks,
[00:42:24.800 --> 00:42:27.120]   and here's how they combine to create all these things.
[00:42:27.120 --> 00:42:31.520]   Ah, so we have the RDMKV architecture code.
[00:42:31.520 --> 00:42:34.560]   So that's the main model building block, and basically we feed it the data.
[00:42:34.560 --> 00:42:41.040]   PowerPlus, Red Pyjama, then subsequently some of the code data.
[00:42:41.040 --> 00:42:43.280]   For the whole model, we subsequently add on top of that
[00:42:43.280 --> 00:42:47.520]   all the translation, OSCAR sets, and so on.
[00:42:47.520 --> 00:42:48.800]   And so you're training these things.
[00:42:48.800 --> 00:42:52.880]   You've mentioned that you're intentionally taking a hit on evals,
[00:42:52.880 --> 00:42:55.760]   on traditional evals, like MLU or whatever.
[00:42:55.760 --> 00:42:57.200]   I wouldn't say intentionally.
[00:42:57.200 --> 00:42:59.760]   Also to clarify, like, I am not training it.
[00:42:59.760 --> 00:43:00.720]   I'm just part of the community.
[00:43:00.720 --> 00:43:04.240]   The community and Blink is the one training it.
[00:43:04.240 --> 00:43:08.960]   But I would say it's more of, like, the lack of care for the evals.
[00:43:08.960 --> 00:43:15.520]   So the reason why we add things to the dataset was never about improving evals.
[00:43:15.520 --> 00:43:20.000]   It's about directly in response to user feedback.
[00:43:20.000 --> 00:43:22.400]   It's like, "Oh, not good enough at this."
[00:43:22.400 --> 00:43:23.840]   So they're like, "Okay, just throw it in."
[00:43:23.840 --> 00:43:24.720]   Yes, literally.
[00:43:24.720 --> 00:43:31.840]   So take, for example, even for Raven and the world model,
[00:43:31.840 --> 00:43:33.120]   as we go through the training stages,
[00:43:33.120 --> 00:43:39.920]   we specifically ask people in other nationalities within our Discord community
[00:43:39.920 --> 00:43:41.920]   to test it for their language.
[00:43:41.920 --> 00:43:46.000]   And our rule that we set is that, our informal rule is that
[00:43:46.000 --> 00:43:49.920]   the only person who can decide whether this improved world model
[00:43:49.920 --> 00:43:53.440]   is better in Japanese or Thai or whatever it is,
[00:43:53.440 --> 00:43:54.560]   is a native speaker.
[00:43:54.560 --> 00:43:57.360]   Where does it take place?
[00:43:57.360 --> 00:44:00.400]   So it's mostly in within linguistics,
[00:44:00.400 --> 00:44:02.320]   but sometimes we do a shortcut in general as well.
[00:44:02.320 --> 00:44:03.200]   Okay, linguistics.
[00:44:03.200 --> 00:44:08.320]   So do you have, like, an appointed ambassador?
[00:44:08.320 --> 00:44:09.680]   Like, you have 100 languages?
[00:44:09.680 --> 00:44:10.240]   Yeah.
[00:44:10.240 --> 00:44:14.560]   You just have, like, a czar of Japanese, a czar of Thai?
[00:44:14.560 --> 00:44:16.160]   It's not so pointed.
[00:44:16.160 --> 00:44:19.120]   It's more of like, "Hey, this is the Japanese model. Please try."
[00:44:19.920 --> 00:44:22.240]   But there's no "the Japanese model."
[00:44:22.240 --> 00:44:23.360]   There's one model.
[00:44:23.360 --> 00:44:24.640]   There's the world model.
[00:44:24.640 --> 00:44:27.040]   So if you go to world model, I don't know whether it's inside here.
[00:44:27.040 --> 00:44:27.600]   No, four.
[00:44:27.600 --> 00:44:28.480]   Oh, sorry.
[00:44:28.480 --> 00:44:32.960]   Five is, we should never put five on top because five is fully experimental.
[00:44:32.960 --> 00:44:35.120]   Okay, so under files and versions.
[00:44:35.120 --> 00:44:36.960]   I see, I see, I see, I see.
[00:44:36.960 --> 00:44:39.680]   So there's, you see, there's a Japanese-specific tune.
[00:44:39.680 --> 00:44:40.400]   Yeah.
[00:44:40.400 --> 00:44:41.360]   Chinese tune.
[00:44:41.360 --> 00:44:42.400]   Arabic.
[00:44:42.400 --> 00:44:43.920]   Then for all the other smaller languages,
[00:44:43.920 --> 00:44:46.640]   we actually ask them, "Hey, what's the Japanese model?"
[00:44:46.640 --> 00:44:52.800]   All the other smaller languages, we actually ask them from the base world model itself.
[00:44:52.800 --> 00:44:55.440]   So, feedback on that.
[00:44:55.440 --> 00:44:59.360]   So we actually released previously, like, 10% train, 15%, 20%.
[00:44:59.360 --> 00:45:02.560]   Like, as it goes through the stages, and then it's like, "Hey, is this working?"
[00:45:02.560 --> 00:45:04.160]   Is it regressing?
[00:45:04.160 --> 00:45:07.680]   So it's like evals, but real humans.
[00:45:07.680 --> 00:45:10.880]   Done by real humans and not systematically.
[00:45:10.880 --> 00:45:15.360]   Is there a reason that you release, you also, so you mentioned 7b, 14b.
[00:45:15.360 --> 00:45:18.720]   I see also 0.1b, 0.4b, 3b, 1.5b.
[00:45:18.720 --> 00:45:23.520]   Like, what, is that useful for people or is it just for research?
[00:45:23.520 --> 00:45:26.640]   0.1 and 0.4 is frankly more for research,
[00:45:26.640 --> 00:45:28.800]   but some people do try to make use of them.
[00:45:28.800 --> 00:45:30.880]   Nothing's stopping them.
[00:45:30.880 --> 00:45:36.000]   Well, I mean, it's extra, like, these are just different architectures, different dimensions.
[00:45:36.000 --> 00:45:36.720]   Yeah.
[00:45:36.720 --> 00:45:39.840]   So it's actually extra cost to you to provide these things.
[00:45:39.840 --> 00:45:43.920]   But specifically for the world model, because we are trying a new tokenizer,
[00:45:43.920 --> 00:45:53.360]   we are, and the reason why we're trying a new tokenizer is that as I think I'm,
[00:45:53.360 --> 00:45:58.480]   is that one thing that we found, more like I found surprisingly frustrating
[00:45:58.480 --> 00:46:02.480]   in existing tokenizer was that it was very English centric.
[00:46:02.480 --> 00:46:04.880]   And the existing tokenizer you took from DPT Neo?
[00:46:04.880 --> 00:46:05.360]   Yeah.
[00:46:05.360 --> 00:46:06.160]   Okay.
[00:46:06.160 --> 00:46:09.840]   And just to, I need to backtrack a little bit, just for people who are not following along.
[00:46:09.840 --> 00:46:13.200]   DPT-J was the original Luther reproduction of DPT-3.
[00:46:13.200 --> 00:46:16.880]   And then DPT Neo was the bigger DPT-J?
[00:46:16.880 --> 00:46:18.000]   Yeah.
[00:46:18.000 --> 00:46:20.240]   20b, something like that.
[00:46:20.240 --> 00:46:21.680]   Yeah, I do believe they have a 20b model.
[00:46:21.680 --> 00:46:22.180]   Okay.
[00:46:22.180 --> 00:46:31.040]   And there's actually, I mean, for those outside of the open source space,
[00:46:31.040 --> 00:46:36.080]   in particular for the transformer, I think one thing significant about DPT Neo X was that
[00:46:36.080 --> 00:46:40.480]   it was one of the major models that had everything fully documented and they,
[00:46:40.480 --> 00:46:43.120]   like why they make this change in the architecture and so on and so forth.
[00:46:43.120 --> 00:46:49.520]   And that became like a, basically reference notes for all other subsequent open source models,
[00:46:49.520 --> 00:46:55.680]   because they were the early ones that were like doing a good transformer model.
[00:46:55.680 --> 00:46:56.240]   Yeah.
[00:46:56.240 --> 00:46:59.040]   And at least for a large language model.
[00:46:59.040 --> 00:47:04.480]   So DPT-2 was actually open source, you didn't, people didn't find that useful?
[00:47:04.480 --> 00:47:08.160]   No, people do find, do reference that as well, but it's like the code is there.
[00:47:08.160 --> 00:47:09.620]   And?
[00:47:09.620 --> 00:47:11.840]   Why do you do this?
[00:47:11.840 --> 00:47:13.040]   Oh, it's not documented.
[00:47:13.040 --> 00:47:19.440]   So in that sense, was OPT from Facebook useful?
[00:47:19.440 --> 00:47:23.120]   Because I've heard very good things about the logbook of OPT,
[00:47:23.120 --> 00:47:26.640]   where they had the daily logbook and they just published that.
[00:47:26.640 --> 00:47:28.000]   Yeah, those were useful as well.
[00:47:28.000 --> 00:47:28.560]   Yeah, okay.
[00:47:29.360 --> 00:47:33.600]   I think one thing that Neo X had going for it,
[00:47:33.600 --> 00:47:37.360]   especially the illegal community, that it's not just logbook, it's just like,
[00:47:37.360 --> 00:47:39.200]   you could just go to Discord, "Hey, why do you do this?"
[00:47:39.200 --> 00:47:40.420]   Right.
[00:47:40.420 --> 00:47:42.640]   And the person who trained it will tell you.
[00:47:42.640 --> 00:47:46.400]   Yep, someone there will get by, hopefully, one of them.
[00:47:46.400 --> 00:47:52.080]   So that's why we had the 0.1 and 0.4 models, because we were just in uncharted waters here.
[00:47:52.080 --> 00:47:57.120]   So like a lot of existing tokenizer took space as a major delimiter to detect and split.
[00:47:57.840 --> 00:48:02.240]   And the tokenizer we are using is actually a lot more simplified.
[00:48:02.240 --> 00:48:05.600]   So existing tokenizers, I mean, they scan all the tags,
[00:48:05.600 --> 00:48:11.680]   they do a statistical model of what pairs well with what, and so on and so forth, right?
[00:48:11.680 --> 00:48:17.680]   We did a similar approach, but instead of using this token pairs well with this,
[00:48:17.680 --> 00:48:22.480]   and should be paired with that, we just made it a trio list.
[00:48:22.480 --> 00:48:27.520]   So basically, we find the data structure.
[00:48:27.520 --> 00:48:30.240]   Yeah, so we just find the longest matching string,
[00:48:30.240 --> 00:48:35.200]   in that matching string that we have trained inside our token list,
[00:48:35.200 --> 00:48:37.520]   and then we just use that token.
[00:48:37.520 --> 00:48:44.320]   It's a drastically simplified tokenizer, and it doesn't use spaces as an assumption, which I know.
[00:48:44.320 --> 00:48:45.040]   Which is good.
[00:48:45.040 --> 00:48:45.520]   Yeah.
[00:48:45.520 --> 00:48:50.640]   And that helps a lot of the Japanese, Chinese, and character models, because they don't have spaces.
[00:48:51.840 --> 00:48:59.760]   And I would even argue to fair say, if you look at the really large model,
[00:48:59.760 --> 00:49:04.240]   like with OpenAI or Cloudera, tokenizers are not really a thing.
[00:49:04.240 --> 00:49:09.440]   I mean, in the sense that the model can work even if you tell it character by character.
[00:49:09.440 --> 00:49:12.080]   It may be inefficient.
[00:49:12.080 --> 00:49:13.200]   Did someone try it?
[00:49:13.200 --> 00:49:16.880]   I mean, there was that jailbreak where the system prompt you put the character,
[00:49:16.880 --> 00:49:19.200]   then enter, enter, enter. Do you remember that jailbreak?
[00:49:19.200 --> 00:49:20.160]   No, I didn't see that one.
[00:49:20.160 --> 00:49:26.160]   Yeah, so you can literally, like instead of left to right, you can usually up to down.
[00:49:26.160 --> 00:49:26.640]   Okay.
[00:49:26.640 --> 00:49:29.040]   And you're just eating tokens for every character.
[00:49:29.040 --> 00:49:31.280]   No, actually you're eating two, because there's also the new line.
[00:49:31.280 --> 00:49:39.520]   And the model understood it, because there's enough dumb data on the internet
[00:49:39.520 --> 00:49:42.240]   that it has learned how to deal with this kind of formatting.
[00:49:42.240 --> 00:49:44.000]   Got it, okay.
[00:49:44.000 --> 00:49:47.200]   And if these models are already understanding things at the character level,
[00:49:47.760 --> 00:49:50.400]   everything else is just improved compute.
[00:49:50.400 --> 00:49:50.800]   Okay.
[00:49:50.800 --> 00:49:53.360]   Because we jump the multiple tokens.
[00:49:53.360 --> 00:49:58.160]   Do you have any idea of your dictionary size when you use this 3D data structure?
[00:49:58.160 --> 00:49:58.400]   Yeah.
[00:49:58.400 --> 00:50:04.880]   Because the typical tokenizer is like 80,000 tokens, dictionary size.
[00:50:04.880 --> 00:50:06.480]   I presume yours will be bigger.
[00:50:06.480 --> 00:50:10.400]   Yeah, I can remember offhand, our previous tokenizer is around 50,000.
[00:50:10.400 --> 00:50:15.920]   It's the new tokenizer, then subsequently I believe this is around the same size.
[00:50:16.560 --> 00:50:18.000]   It's not bad, pretty good.
[00:50:18.000 --> 00:50:23.040]   We didn't want to change too much on that size, but we just wanted to change the format.
[00:50:23.040 --> 00:50:24.880]   Yeah, cool.
[00:50:24.880 --> 00:50:27.360]   All right, what else should people know?
[00:50:27.360 --> 00:50:29.200]   So world model is the...
[00:50:29.200 --> 00:50:30.880]   There's music.
[00:50:30.880 --> 00:50:36.720]   You literally just landed into like, here's the experiment zone.
[00:50:36.720 --> 00:50:37.520]   Let's talk about it.
[00:50:37.520 --> 00:50:38.400]   Yeah, this is cool.
[00:50:38.400 --> 00:50:44.880]   So, RWKB fundamentally is still an input/output model,
[00:50:44.880 --> 00:50:48.400]   and you could do it for anything that you want.
[00:50:48.400 --> 00:50:53.280]   So there is actually another project internally on the Discord
[00:50:53.280 --> 00:50:57.200]   where it's doing vision modeling.
[00:50:57.200 --> 00:51:02.480]   And this is based on the Mini-GPT-4 paper,
[00:51:02.480 --> 00:51:05.680]   where you have an image model, put everything inside the latent space,
[00:51:05.680 --> 00:51:07.920]   and then you have the language model interact with that latent space,
[00:51:07.920 --> 00:51:10.560]   and then train both, and then you can do image stuff.
[00:51:10.560 --> 00:51:13.680]   Music was basically, let's just take the same model, same code.
[00:51:13.680 --> 00:51:16.160]   You know how MIDI files work, right?
[00:51:16.160 --> 00:51:19.360]   So the MIDI files, just input and output MIDI files.
[00:51:19.360 --> 00:51:25.840]   And there's actually a lot of other experiments based on vision.
[00:51:25.840 --> 00:51:29.200]   There's even an image generation experiment using RWKB.
[00:51:29.200 --> 00:51:31.360]   I'm not sure whether it's in the list.
[00:51:31.360 --> 00:51:34.640]   Yeah, it's clip-guided or auto-encoded, but I don't think that's...
[00:51:34.640 --> 00:51:37.760]   Yeah, I won't say it's a good image generator.
[00:51:38.480 --> 00:51:40.720]   Admittedly, but it worked.
[00:51:40.720 --> 00:51:45.280]   So what I like about the transformer-driven image generators
[00:51:45.280 --> 00:51:48.400]   is that they can do text well, and they can do control very well.
[00:51:48.400 --> 00:51:55.680]   So if you ask for green, blue, red cars arranged next to each other,
[00:51:55.680 --> 00:51:57.040]   they will actually know how to follow that,
[00:51:57.040 --> 00:52:00.240]   whereas the diffusion models tend to treat it more as a suggestion.
[00:52:00.240 --> 00:52:02.480]   You know what I mean?
[00:52:02.480 --> 00:52:05.120]   Or they'll combine the green, blue, and red into one car.
[00:52:05.120 --> 00:52:06.240]   Whatever felt like it, right?
[00:52:07.200 --> 00:52:09.280]   So, okay, but just to get back on this.
[00:52:09.280 --> 00:52:11.360]   Okay, what else?
[00:52:11.360 --> 00:52:15.200]   Yeah, so again, I actually kind of want to establish the credentials of this thing.
[00:52:15.200 --> 00:52:16.800]   So who is Blink?
[00:52:16.800 --> 00:52:20.880]   Is it Randall on the internet?
[00:52:20.880 --> 00:52:25.120]   Or like, again, never heard of this guy until he published.
[00:52:25.120 --> 00:52:26.240]   This is his real name.
[00:52:26.240 --> 00:52:26.740]   Right.
[00:52:26.740 --> 00:52:30.720]   And you had, like, I have this paper to work with,
[00:52:30.720 --> 00:52:32.080]   but it was only published in May.
[00:52:32.080 --> 00:52:32.880]   Yeah.
[00:52:32.880 --> 00:52:34.800]   You found this before the paper.
[00:52:35.680 --> 00:52:39.600]   And so I think it's very unusual for a researcher to
[00:52:39.600 --> 00:52:45.360]   effectively launch to the wider public without a paper,
[00:52:45.360 --> 00:52:48.640]   and just get some kind of pretty decent community going,
[00:52:48.640 --> 00:52:51.600]   and then publish the paper.
[00:52:51.600 --> 00:52:52.880]   Actually, it's the other way around.
[00:52:52.880 --> 00:52:57.200]   He got the basic community going before the paper.
[00:52:57.200 --> 00:52:57.920]   That's what I'm saying.
[00:52:57.920 --> 00:52:58.960]   This is unusual.
[00:52:59.760 --> 00:53:06.560]   So the history behind it, right, is that I think, like,
[00:53:06.560 --> 00:53:09.200]   a few years back, once with GPT-2,
[00:53:09.200 --> 00:53:10.720]   Transformer started to pick up steam.
[00:53:10.720 --> 00:53:13.680]   And I guess the whole world is starting to think,
[00:53:13.680 --> 00:53:15.680]   let's just abandon neural networks.
[00:53:15.680 --> 00:53:17.840]   So we haven't even gone into the code part.
[00:53:17.840 --> 00:53:21.360]   But like, so the main reason why neural networks were bad
[00:53:21.360 --> 00:53:24.560]   compared to Transformer was that when you train a,
[00:53:24.560 --> 00:53:26.320]   let's say you just input a token,
[00:53:26.320 --> 00:53:28.240]   and train a token for a data sample,
[00:53:28.240 --> 00:53:30.880]   you have to wait for the compute to finish for that token,
[00:53:30.880 --> 00:53:33.440]   take the state, and then you train the next token.
[00:53:33.440 --> 00:53:36.480]   We'll get into how RWA-KB solves that.
[00:53:36.480 --> 00:53:39.520]   But basically, the whole world at that point just concluded,
[00:53:39.520 --> 00:53:42.000]   yeah, neural networks, it cannot scale as well as Transformer.
[00:53:42.000 --> 00:53:42.960]   Let's just abandon it.
[00:53:42.960 --> 00:53:46.240]   And everyone just went in that direction.
[00:53:46.240 --> 00:53:49.280]   And Blink, or Blupeng, is his actual name,
[00:53:49.280 --> 00:53:53.360]   decided, basically as an individual,
[00:53:53.360 --> 00:53:55.440]   literally at the elusive AI firm,
[00:53:55.440 --> 00:54:01.120]   decided that, hey, I think we can modify recurrent neural network,
[00:54:01.120 --> 00:54:04.080]   no, neural networks, based on the Apple paper,
[00:54:04.080 --> 00:54:06.080]   the light engine that I showed previously,
[00:54:06.080 --> 00:54:11.680]   to make, to scale this up without,
[00:54:11.680 --> 00:54:16.560]   to make neural networks scalable and parallelizable
[00:54:16.560 --> 00:54:18.400]   in the same way Transformers work.
[00:54:18.400 --> 00:54:21.440]   Because the reason why we branch away and focus Transformer
[00:54:21.440 --> 00:54:23.120]   is because neural networks were slow to train.
[00:54:23.120 --> 00:54:24.880]   It was never, I mean,
[00:54:24.880 --> 00:54:27.120]   it wasn't so much about whether it was good or bad.
[00:54:27.120 --> 00:54:30.400]   It was just, no one wants to wait 100 years
[00:54:30.400 --> 00:54:33.280]   for their billion tokens to train and finish,
[00:54:33.280 --> 00:54:35.360]   even if they can throw a GPU farm at it.
[00:54:35.360 --> 00:54:39.360]   And that's where he started looking into it,
[00:54:39.360 --> 00:54:42.640]   how to make the neural network trainable in parallel.
[00:54:42.640 --> 00:54:45.440]   And specifically RNNs?
[00:54:45.440 --> 00:54:48.880]   Yes. And subsequently, the AI,
[00:54:48.880 --> 00:54:50.320]   and I believe there was also a few others,
[00:54:50.320 --> 00:54:53.040]   because he was doing it very publicly there,
[00:54:54.480 --> 00:54:58.480]   came on board to sponsor the GPU computes required.
[00:54:58.480 --> 00:55:01.520]   Because even though it, I mentioned that on a large context size,
[00:55:01.520 --> 00:55:03.840]   it is substantially cheaper.
[00:55:03.840 --> 00:55:09.120]   I think, especially if you run an open source discord forum for an AI model,
[00:55:09.120 --> 00:55:12.400]   it's like every day there'll be someone who thinks
[00:55:12.400 --> 00:55:15.680]   that they can train a 20D model on a single GPU coming in.
[00:55:15.680 --> 00:55:19.920]   The skill is still large,
[00:55:19.920 --> 00:55:22.720]   even though it's like 1/3 of 1/10 compared to Transformer,
[00:55:22.720 --> 00:55:24.720]   it still needs a large GPU.
[00:55:24.720 --> 00:55:27.120]   So that's where Agilent, AI, and the rest,
[00:55:27.120 --> 00:55:29.440]   Stability, I believe also is involved,
[00:55:29.440 --> 00:55:33.200]   stepped up and donated the A100s needed
[00:55:33.200 --> 00:55:36.320]   to train the basic models that RWKB had.
[00:55:36.320 --> 00:55:41.280]   So before those models were trained,
[00:55:41.280 --> 00:55:44.720]   we were only having in theory the toy models
[00:55:44.720 --> 00:55:47.360]   or the small model that this can match Transformer.
[00:55:47.360 --> 00:55:51.040]   We have no idea whether it can match Transformer at that scale.
[00:55:52.000 --> 00:55:54.400]   And subsequently, with the larger models,
[00:55:54.400 --> 00:55:55.840]   the 14D models and all that,
[00:55:55.840 --> 00:55:59.280]   we can compare it directly with NeoX model,
[00:55:59.280 --> 00:56:01.520]   and that's where this paper came out.
[00:56:01.520 --> 00:56:05.200]   So that's the history behind it.
[00:56:05.200 --> 00:56:08.080]   It's like he wasn't really doing it in silence,
[00:56:08.080 --> 00:56:10.560]   he was doing it from ILLUTR,
[00:56:10.560 --> 00:56:11.680]   then he branched out.
[00:56:11.680 --> 00:56:16.240]   Because this became a big project on its own,
[00:56:16.240 --> 00:56:18.080]   and that's where other people started coming in.
[00:56:21.120 --> 00:56:24.160]   So the part where we say that RWKB is a neural network
[00:56:24.160 --> 00:56:25.040]   that can be scaled up,
[00:56:25.040 --> 00:56:26.400]   can be rolled out as a Transformer,
[00:56:26.400 --> 00:56:30.320]   the key thing that you would want to see is this diagram here.
[00:56:30.320 --> 00:56:32.800]   This should be in the paper.
[00:56:32.800 --> 00:56:38.480]   Yeah, accordingly.
[00:56:38.480 --> 00:56:40.080]   So what you get,
[00:56:40.080 --> 00:56:43.200]   so when you do inference,
[00:56:43.200 --> 00:56:44.720]   when you are running inference mode,
[00:56:44.720 --> 00:56:46.320]   ideally you should run it as a neural network,
[00:56:46.320 --> 00:56:47.760]   so this is a layer.
[00:56:47.760 --> 00:56:50.560]   So as per, so classic neural networks is that
[00:56:51.520 --> 00:56:52.240]   you have a state,
[00:56:52.240 --> 00:56:54.320]   the state could be start from blank,
[00:56:54.320 --> 00:56:56.880]   you process a token,
[00:56:56.880 --> 00:56:57.680]   you output a state,
[00:56:57.680 --> 00:56:59.360]   and then you rinse and repeat,
[00:56:59.360 --> 00:57:01.600]   and then as you keep doing the output,
[00:57:01.600 --> 00:57:02.400]   it makes a prediction.
[00:57:02.400 --> 00:57:07.680]   One thing that,
[00:57:07.680 --> 00:57:09.120]   so subsequently for RWKB,
[00:57:09.120 --> 00:57:10.720]   what happens here is that
[00:57:10.720 --> 00:57:14.880]   we can roll out this neural network side by side,
[00:57:14.880 --> 00:57:16.560]   and then it runs similar to Transformer,
[00:57:16.560 --> 00:57:18.320]   but the key thing here is that
[00:57:18.320 --> 00:57:20.560]   the states are split across the layer.
[00:57:20.560 --> 00:57:22.240]   So this is what we call,
[00:57:22.240 --> 00:57:23.600]   in this diagram here specifically,
[00:57:23.600 --> 00:57:26.400]   this is what we call the time mix and channel mix.
[00:57:26.400 --> 00:57:28.960]   These are operations within the layer.
[00:57:28.960 --> 00:57:30.160]   Depending on how long you view it,
[00:57:30.160 --> 00:57:31.920]   you could view this as individual layers,
[00:57:31.920 --> 00:57:34.720]   or as how we view it,
[00:57:34.720 --> 00:57:37.600]   we view like this collection of layers as one layer block,
[00:57:37.600 --> 00:57:42.400]   and each layer block pass the states to its sibling,
[00:57:42.400 --> 00:57:44.800]   subsequently down the road,
[00:57:44.800 --> 00:57:46.080]   as you process the next token.
[00:57:46.080 --> 00:57:47.920]   Which is a similar RNN type.
[00:57:47.920 --> 00:57:48.640]   Correct.
[00:57:48.640 --> 00:57:50.000]   However, the key thing is,
[00:57:50.000 --> 00:57:54.080]   you do not need to wait for the upper layers to complete
[00:57:54.080 --> 00:57:56.800]   before you can go to the next token.
[00:57:56.800 --> 00:57:59.600]   So what happens in practice?
[00:57:59.600 --> 00:58:01.680]   And if I were to jump to the diagram,
[00:58:01.680 --> 00:58:04.640]   there's this graphic here.
[00:58:04.640 --> 00:58:06.640]   This is not 100% how it runs.
[00:58:06.640 --> 00:58:07.360]   You want to see?
[00:58:07.360 --> 00:58:07.920]   I like it.
[00:58:07.920 --> 00:58:10.320]   Yeah, whoever put time into this, kudos.
[00:58:10.320 --> 00:58:11.860]   I made it.
[00:58:11.860 --> 00:58:17.600]   So this is how you can visualize it.
[00:58:17.600 --> 00:58:19.520]   So the first layer is the layer norm.
[00:58:19.520 --> 00:58:20.800]   The layer norm doesn't...
[00:58:20.800 --> 00:58:22.800]   This is standard layer normalization.
[00:58:22.800 --> 00:58:25.520]   It just does it on the token,
[00:58:25.520 --> 00:58:27.040]   and doesn't need to wait for the other layers.
[00:58:27.040 --> 00:58:27.920]   But if you notice,
[00:58:27.920 --> 00:58:30.080]   subsequently to the right and to the top,
[00:58:30.080 --> 00:58:32.320]   these tokens, these blocks,
[00:58:32.320 --> 00:58:33.680]   need to wait for the blocks on the left.
[00:58:33.680 --> 00:58:36.480]   And this is like,
[00:58:36.480 --> 00:58:39.280]   once you go past the first few tokens,
[00:58:39.280 --> 00:58:41.120]   this cascades very rapidly.
[00:58:41.120 --> 00:58:44.160]   Especially, this is only like one, two, three, four layers.
[00:58:45.280 --> 00:58:47.920]   Most models have like 20, 40 plus layers,
[00:58:47.920 --> 00:58:50.560]   and the cascading patterns are happening.
[00:58:50.560 --> 00:58:53.840]   And in practice, once you start cascading there,
[00:58:53.840 --> 00:58:55.280]   you just saturate the GPU.
[00:58:55.280 --> 00:58:57.520]   And that's how it starts being parallelizable to train.
[00:58:57.520 --> 00:59:00.480]   You no longer need to train in slices like traditional RNNs.
[00:59:00.480 --> 00:59:03.120]   Does big O notation help?
[00:59:03.120 --> 00:59:07.600]   Like, so we're talking about big O, N squared for attention.
[00:59:07.600 --> 00:59:11.680]   Is this O of 1 or O of N?
[00:59:13.280 --> 00:59:16.720]   I'm talking about like to go through the entire context.
[00:59:16.720 --> 00:59:20.080]   This will be O of 1 per token.
[00:59:20.080 --> 00:59:21.840]   O of 1 per token, O of N for whole sequence.
[00:59:21.840 --> 00:59:23.760]   Yeah, yeah, yeah, okay, cool.
[00:59:23.760 --> 00:59:24.880]   Yeah, and--
[00:59:24.880 --> 00:59:26.800]   And that's the core idea.
[00:59:26.800 --> 00:59:28.240]   That was one of the key things.
[00:59:28.240 --> 00:59:29.120]   What else is the key thing?
[00:59:29.120 --> 00:59:31.760]   So other things is that,
[00:59:31.760 --> 00:59:34.960]   so I think you're familiar with LSTM, right?
[00:59:34.960 --> 00:59:38.800]   This is how traditional neural networks
[00:59:38.800 --> 00:59:40.240]   keeps things within memories.
[00:59:41.680 --> 00:59:47.120]   Within here, within RLKB, we have two channels.
[00:59:47.120 --> 00:59:49.600]   So we call it the channel mix and the time mix, respectively.
[00:59:49.600 --> 00:59:53.040]   Is there a formal definition of channel mix and time mix?
[00:59:53.040 --> 00:59:54.640]   Yeah, we can actually scroll.
[00:59:54.640 --> 00:59:59.200]   But this will be like going more--
[00:59:59.200 --> 01:00:01.600]   We are going more into the code itself.
[01:00:01.600 --> 01:00:02.240]   They're just weights?
[01:00:02.240 --> 01:00:06.320]   They're just weights that applies according to the formula.
[01:00:06.320 --> 01:00:08.880]   But how, in a sense, does it work?
[01:00:08.880 --> 01:00:12.640]   More importantly, you can see the data
[01:00:12.640 --> 01:00:14.560]   from the respective time mix and channel mix
[01:00:14.560 --> 01:00:17.280]   move to the next segment.
[01:00:17.280 --> 01:00:20.880]   How time mix is designed, per se,
[01:00:20.880 --> 01:00:24.720]   was that it's how it retains--
[01:00:24.720 --> 01:00:27.920]   So similar to LSTMs, right,
[01:00:27.920 --> 01:00:30.640]   where it processes the state and the input,
[01:00:30.640 --> 01:00:32.640]   it may decide to discard certain states
[01:00:32.640 --> 01:00:33.920]   and keep new things in the state.
[01:00:33.920 --> 01:00:37.120]   Time mix does the same thing,
[01:00:37.120 --> 01:00:38.640]   but with a different formula.
[01:00:38.640 --> 01:00:42.320]   So it replaces the LSTM, in a sense,
[01:00:42.320 --> 01:00:45.120]   and it can decide to keep things indefinitely.
[01:00:45.120 --> 01:00:46.560]   So this represents the long-term memories,
[01:00:46.560 --> 01:00:47.520]   if you want to view it that way.
[01:00:47.520 --> 01:00:50.880]   But classically, the problem with that
[01:00:50.880 --> 01:00:54.160]   is that it struggles with long distance.
[01:00:54.160 --> 01:00:55.460]   Correct.
[01:00:55.460 --> 01:00:57.600]   Does it have the same issue?
[01:00:57.600 --> 01:01:00.000]   So that's subsequent.
[01:01:00.000 --> 01:01:04.160]   It struggles with long distance
[01:01:04.160 --> 01:01:05.840]   because it also needs to keep track
[01:01:05.840 --> 01:01:09.600]   of both near-term memory and long-term memory.
[01:01:09.600 --> 01:01:10.480]   So you split it up.
[01:01:10.480 --> 01:01:11.760]   Yeah, effectively split it up.
[01:01:11.760 --> 01:01:13.360]   So channel mix is subsequent.
[01:01:13.360 --> 01:01:14.560]   Is this the perfect memory?
[01:01:14.560 --> 01:01:17.200]   Yeah, this is the closer to the perfect memory
[01:01:17.200 --> 01:01:18.480]   that is the short-term.
[01:01:18.480 --> 01:01:22.800]   So time mix, it has trainable weights
[01:01:22.800 --> 01:01:24.400]   on what it decides to keep and discard.
[01:01:24.400 --> 01:01:29.200]   Channel mix, it has a very strong bias in it
[01:01:29.200 --> 01:01:31.920]   towards just the next token.
[01:01:32.560 --> 01:01:38.160]   So subsequently, it was just like memories
[01:01:38.160 --> 01:01:39.520]   are stored in the lower layers,
[01:01:39.520 --> 01:01:41.840]   it just slowly shifts upwards through the channel mix.
[01:01:41.840 --> 01:01:43.280]   And this is the short-term memory,
[01:01:43.280 --> 01:01:47.120]   which at some point, as it just shifts all the way up,
[01:01:47.120 --> 01:01:48.640]   it will just disappear into the void.
[01:01:48.640 --> 01:01:50.480]   At that point, subsequently,
[01:01:50.480 --> 01:01:52.880]   then time mix should be retaining
[01:01:52.880 --> 01:01:53.840]   the longer-term memory.
[01:01:53.840 --> 01:01:55.840]   Are you also predicting,
[01:01:55.840 --> 01:01:57.440]   are you also sampling from a distribution?
[01:01:57.440 --> 01:02:00.640]   So are you also sampling from a distribution?
[01:02:00.640 --> 01:02:01.760]   So I noticed, for example, here,
[01:02:01.760 --> 01:02:03.440]   that the illustrative demo is like,
[01:02:03.440 --> 01:02:05.760]   it says, you know, my name is,
[01:02:05.760 --> 01:02:07.920]   and then it's predicting name is Bob.
[01:02:07.920 --> 01:02:08.480]   Yeah, correct.
[01:02:08.480 --> 01:02:09.600]   That's a classic.
[01:02:09.600 --> 01:02:12.080]   But is there some amount of temperature?
[01:02:12.080 --> 01:02:13.600]   Like, it's the same concepts that we--
[01:02:13.600 --> 01:02:14.160]   Same concept.
[01:02:14.160 --> 01:02:14.800]   Okay.
[01:02:14.800 --> 01:02:17.200]   So it's literally the same concept.
[01:02:17.200 --> 01:02:19.120]   Lot of probability of distribution
[01:02:19.120 --> 01:02:21.440]   across your token space and, yeah, okay.
[01:02:21.440 --> 01:02:24.240]   You could use hugging face sampler
[01:02:24.240 --> 01:02:25.760]   on top of it, literally.
[01:02:25.760 --> 01:02:28.400]   So yeah, the output is actually
[01:02:28.400 --> 01:02:29.600]   more like a set of logic.
[01:02:30.160 --> 01:02:30.720]   Should we pause?
[01:02:30.720 --> 01:02:39.600]   So we took a break for a bit,
[01:02:39.600 --> 01:02:41.760]   but now we're trying to cover,
[01:02:41.760 --> 01:02:43.840]   like, what is the big aha moment for you?
[01:02:43.840 --> 01:02:45.600]   And you said it was something to do with cost.
[01:02:45.600 --> 01:02:46.720]   Correct.
[01:02:46.720 --> 01:02:48.880]   So we have this chart on screen.
[01:02:48.880 --> 01:02:51.040]   There's literally a chart of quadratic scaling
[01:02:51.040 --> 01:02:51.920]   versus linear scaling
[01:02:51.920 --> 01:02:56.160]   in terms of GPU time spent in text generation.
[01:02:56.160 --> 01:02:58.000]   And you said it was at training time
[01:02:58.000 --> 01:02:58.880]   and at inference time?
[01:02:58.880 --> 01:03:00.560]   Just basically in everything that matters.
[01:03:00.560 --> 01:03:01.120]   Correct.
[01:03:01.120 --> 01:03:06.240]   So I mean, look back to how RNN works.
[01:03:06.240 --> 01:03:10.480]   From a high level, we do an O1 operation
[01:03:10.480 --> 01:03:13.200]   on a token, create a state.
[01:03:13.200 --> 01:03:15.280]   O1 operation, create a state.
[01:03:15.280 --> 01:03:16.480]   So this just scales linearly.
[01:03:16.480 --> 01:03:18.800]   You want to throw a thousand tokens at it,
[01:03:18.800 --> 01:03:21.120]   it just, on inference, it just scales linearly.
[01:03:21.120 --> 01:03:24.000]   Subsequently, for a transformer,
[01:03:24.000 --> 01:03:26.240]   you're taking a token,
[01:03:26.720 --> 01:03:32.240]   you process your first token, it may be O1 here.
[01:03:32.240 --> 01:03:34.480]   Subsequently, when you generate your third token,
[01:03:34.480 --> 01:03:36.880]   you need to compute your second and first,
[01:03:36.880 --> 01:03:38.160]   and then vice versa.
[01:03:38.160 --> 01:03:40.080]   So you do your 1,000 tokens,
[01:03:40.080 --> 01:03:43.280]   you need to compute back your 999 previous tokens.
[01:03:43.280 --> 01:03:44.880]   And as this keeps growing and growing,
[01:03:44.880 --> 01:03:46.080]   this is your quadratic scaling.
[01:03:46.080 --> 01:03:49.120]   And this is why we had this graph
[01:03:49.120 --> 01:03:51.440]   of the amount of cumulative GPU time
[01:03:51.440 --> 01:03:52.640]   that you need to spend
[01:03:52.640 --> 01:03:56.000]   to generate all these tokens respectively.
[01:03:56.880 --> 01:03:59.680]   And this is fundamentally just transformer
[01:03:59.680 --> 01:04:01.200]   versus neural networks.
[01:04:01.200 --> 01:04:04.480]   Yeah, on inference.
[01:04:04.480 --> 01:04:09.120]   The reason why, and subsequently,
[01:04:09.120 --> 01:04:11.440]   neural networks did have disadvantage
[01:04:11.440 --> 01:04:13.600]   of, let's say, not being able to parallelise well in training.
[01:04:13.600 --> 01:04:17.280]   But as I covered, RWKB kind of solved that
[01:04:17.280 --> 01:04:19.680]   by effectively splitting the layers,
[01:04:19.680 --> 01:04:21.680]   allowing you to train different parts in parallel.
[01:04:22.480 --> 01:04:26.160]   And some people will go into the academic debate
[01:04:26.160 --> 01:04:28.720]   of, technically, the second and third token
[01:04:28.720 --> 01:04:30.720]   is not parallelisable until the first is done.
[01:04:30.720 --> 01:04:34.080]   But once you get into, I can saturate a GPU length,
[01:04:34.080 --> 01:04:36.080]   it's just way better.
[01:04:36.080 --> 01:04:37.520]   It's just academic debate.
[01:04:37.520 --> 01:04:38.000]   We are done.
[01:04:38.000 --> 01:04:41.280]   So training, in essence, has always--
[01:04:41.280 --> 01:04:42.880]   I mean, this is a bit of transformer.
[01:04:42.880 --> 01:04:45.200]   A neural network is, I need to do an inference pass,
[01:04:45.200 --> 01:04:46.240]   I look at the logits,
[01:04:46.240 --> 01:04:49.440]   then I backprop to see what went wrong,
[01:04:49.440 --> 01:04:50.640]   and I update the weights.
[01:04:51.760 --> 01:04:54.400]   So the inference is the forward pass.
[01:04:54.400 --> 01:04:56.320]   You still need to-- it's part of the training course.
[01:04:56.320 --> 01:04:59.840]   As you backprop as well, as you backprop as well,
[01:04:59.840 --> 01:05:03.200]   having meaning to only look at the current cell tokens
[01:05:03.200 --> 01:05:04.640]   and the state, instead of everything,
[01:05:04.640 --> 01:05:06.560]   also reduce the amount of things that you need to backprop.
[01:05:06.560 --> 01:05:09.280]   So it's just that there's so many factors involved
[01:05:09.280 --> 01:05:12.480]   in just reducing the overall inference and training time.
[01:05:12.480 --> 01:05:15.920]   And that was something that appealed to me,
[01:05:15.920 --> 01:05:17.360]   because in the long run--
[01:05:17.360 --> 01:05:20.080]   I mean, all of us want our model to just run blazingly fast, right?
[01:05:20.080 --> 01:05:20.580]   Yeah.
[01:05:20.580 --> 01:05:23.440]   And also on minimal hardware.
[01:05:23.440 --> 01:05:23.940]   Oh, yes.
[01:05:23.940 --> 01:05:26.560]   Which, as far as I understand,
[01:05:26.560 --> 01:05:28.160]   you still have 14 billion parameters.
[01:05:28.160 --> 01:05:29.360]   That's not going away.
[01:05:29.360 --> 01:05:32.240]   You still need the RAM
[01:05:32.240 --> 01:05:34.560]   to store 14 billion parameters worth of stuff.
[01:05:34.560 --> 01:05:35.360]   That's not going away.
[01:05:35.360 --> 01:05:35.840]   Yeah.
[01:05:35.840 --> 01:05:36.400]   OK.
[01:05:36.400 --> 01:05:37.680]   So RAM is unchanged.
[01:05:37.680 --> 01:05:40.160]   Yeah, on the RAM side--
[01:05:40.160 --> 01:05:42.960]   but the working memory is reduced.
[01:05:42.960 --> 01:05:47.600]   So typically, you need more than 14 for transformer.
[01:05:47.600 --> 01:05:50.960]   I mean, let's not touch quantization.
[01:05:50.960 --> 01:05:52.640]   But in this case, we don't need to keep--
[01:05:52.640 --> 01:05:55.600]   like, if you really, really want to save RAM,
[01:05:55.600 --> 01:05:59.280]   it is possible for you to do token-by-token inference
[01:05:59.280 --> 01:06:02.640]   so that you don't need to keep your states in history.
[01:06:02.640 --> 01:06:06.400]   You only need to keep your current token state and your next.
[01:06:06.400 --> 01:06:06.900]   Yeah.
[01:06:06.900 --> 01:06:11.600]   And yeah, and there's actually one segment of our community
[01:06:11.600 --> 01:06:14.880]   is just purely porting other activity
[01:06:14.880 --> 01:06:16.720]   to C++-based model.
[01:06:16.720 --> 01:06:17.680]   When and next.
[01:06:17.680 --> 01:06:20.000]   Yeah, and running it on pies and stuff.
[01:06:20.000 --> 01:06:22.320]   Raspberry pies.
[01:06:22.320 --> 01:06:23.920]   It's interesting to watch those.
[01:06:23.920 --> 01:06:26.000]   Is JAX interesting to people, TPUs?
[01:06:26.000 --> 01:06:28.560]   There is some interest, but--
[01:06:28.560 --> 01:06:29.760]   Not.
[01:06:29.760 --> 01:06:31.120]   People don't have access.
[01:06:31.120 --> 01:06:34.000]   I would say, frankly, the people with the most interest
[01:06:34.000 --> 01:06:36.160]   also happen to be the people who have free TPUs.
[01:06:36.160 --> 01:06:36.660]   Yeah.
[01:06:36.660 --> 01:06:40.080]   So I don't know--
[01:06:40.080 --> 01:06:42.160]   My understanding was Eleuther was also given
[01:06:42.160 --> 01:06:43.760]   a whole bunch of TPU hours.
[01:06:43.760 --> 01:06:45.840]   Therefore, they wrote all their stuff in JAX.
[01:06:46.560 --> 01:06:48.960]   Yeah, and if you can train it and then you've got the weights,
[01:06:48.960 --> 01:06:50.320]   you can always just run in something else.
[01:06:50.320 --> 01:06:51.120]   It doesn't matter, right?
[01:06:51.120 --> 01:06:51.760]   Yeah, yeah.
[01:06:51.760 --> 01:06:52.480]   Okay, cool.
[01:06:52.480 --> 01:06:57.520]   All right, and then there's a chart about performance,
[01:06:57.520 --> 01:07:00.880]   and it shows that RWKB is competitive,
[01:07:00.880 --> 01:07:04.320]   or actually better in some of the reasoning challenges,
[01:07:04.320 --> 01:07:07.760]   which that's something I definitely would look for, right?
[01:07:07.760 --> 01:07:11.600]   And it's fine if your speed is faster and all that,
[01:07:11.600 --> 01:07:13.440]   but if the reasoning quality sucks,
[01:07:13.440 --> 01:07:15.680]   then it's not a very useful language model.
[01:07:15.680 --> 01:07:16.240]   Exactly.
[01:07:16.240 --> 01:07:17.600]   So--
[01:07:17.600 --> 01:07:20.400]   So this is like literally us saying there's--
[01:07:20.400 --> 01:07:21.120]   No trade-offs.
[01:07:21.120 --> 01:07:23.280]   Yeah, you don't lose out in that process.
[01:07:23.280 --> 01:07:25.120]   Okay, big question then.
[01:07:25.120 --> 01:07:28.000]   Why isn't RWKB a bigger deal right now?
[01:07:28.000 --> 01:07:34.240]   So, one, we are not a commercial organization.
[01:07:34.240 --> 01:07:34.560]   Okay.
[01:07:34.560 --> 01:07:36.480]   This is literally the pure open-source play.
[01:07:36.480 --> 01:07:40.560]   But you could have done the stable diffusion thing,
[01:07:41.840 --> 01:07:44.640]   which, you know, stable diffusion launched.
[01:07:44.640 --> 01:07:48.560]   It was by a bunch of nobodies before that.
[01:07:48.560 --> 01:07:51.520]   It's from, like, literally split out from Luther.
[01:07:51.520 --> 01:07:55.520]   And-- but they definitely had some hype.
[01:07:55.520 --> 01:07:58.240]   They definitely-- like, you know, I interviewed Sharif Shamim,
[01:07:58.240 --> 01:08:02.480]   who was-- who got in-- and I-- this is something I--
[01:08:02.480 --> 01:08:04.080]   the reason I ask you so many things about
[01:08:04.080 --> 01:08:05.680]   how did you find out about RWKB,
[01:08:05.680 --> 01:08:08.720]   because I think the generalizable skill is how to be early in AI.
[01:08:08.720 --> 01:08:11.680]   Because being early in AI is very valuable.
[01:08:12.240 --> 01:08:15.840]   Then you were there to see the-- how things developed
[01:08:15.840 --> 01:08:17.760]   instead of, like, picking it up later like me.
[01:08:17.760 --> 01:08:23.040]   Anyway, so, yeah, why is it not a bigger deal?
[01:08:23.040 --> 01:08:24.160]   You want me to be frank?
[01:08:24.160 --> 01:08:24.880]   Yeah.
[01:08:24.880 --> 01:08:26.160]   We just suck at marketing.
[01:08:26.160 --> 01:08:27.440]   Okay, that's fair.
[01:08:27.440 --> 01:08:27.760]   I mean--
[01:08:27.760 --> 01:08:29.680]   This is part of it.
[01:08:29.680 --> 01:08:30.880]   Yeah, this is part of it.
[01:08:30.880 --> 01:08:33.600]   Like, so, like, maybe--
[01:08:33.600 --> 01:08:37.520]   But, like, again, like, I don't think that is entirely the cause.
[01:08:37.520 --> 01:08:38.640]   Yeah, I'm sure, definitely.
[01:08:38.640 --> 01:08:42.080]   I think the other major segment right now as well is that--
[01:08:42.080 --> 01:08:46.960]   is that we were really late on the paper, okay?
[01:08:46.960 --> 01:08:50.160]   Like, one of the weirdest thing right now is--
[01:08:50.160 --> 01:08:52.560]   weirdest thing right now, I feel that is that
[01:08:52.560 --> 01:08:54.880]   RWKB is starting to have its moment right now.
[01:08:54.880 --> 01:08:55.360]   Okay.
[01:08:55.360 --> 01:08:58.000]   Is that ever since that initial paper came out,
[01:08:58.000 --> 01:08:59.520]   there was ResNet, there's a--
[01:08:59.520 --> 01:09:02.240]   I think there's two more--
[01:09:02.240 --> 01:09:04.720]   there's a few more additional papers coming out.
[01:09:04.720 --> 01:09:07.200]   One from Microsoft, one from other organizations
[01:09:07.200 --> 01:09:10.640]   that are literally exploring the whole idea,
[01:09:10.640 --> 01:09:13.200]   once again, of scalable neural networks.
[01:09:13.200 --> 01:09:13.520]   Okay.
[01:09:13.520 --> 01:09:15.760]   And they are citing RWKB as part of it as well.
[01:09:15.760 --> 01:09:16.240]   Okay.
[01:09:16.240 --> 01:09:18.240]   And I think foremost--
[01:09:18.240 --> 01:09:26.240]   I think it's interesting why switch to this model when--
[01:09:26.240 --> 01:09:30.640]   even though we have proven that, yes, it's scalable to 7 and 14,
[01:09:30.640 --> 01:09:35.600]   and that it can match transformer at similar param and training size,
[01:09:36.640 --> 01:09:38.960]   but all this is very academic,
[01:09:38.960 --> 01:09:44.320]   because the community, right, the community at large,
[01:09:44.320 --> 01:09:47.440]   especially for the English-speaking community, right,
[01:09:47.440 --> 01:09:49.360]   they don't really care about this.
[01:09:49.360 --> 01:09:52.560]   They care about what's the best model that I can run on my computer,
[01:09:52.560 --> 01:09:54.480]   at least within the open-source space.
[01:09:54.480 --> 01:09:59.120]   And by that-- and even though we match in performance
[01:09:59.120 --> 01:10:01.920]   for things in the same data set, the keyword is "same data set."
[01:10:01.920 --> 01:10:05.200]   Like, this benchmark is not--
[01:10:05.200 --> 01:10:06.560]   it's not even red pajamas yet.
[01:10:06.560 --> 01:10:08.240]   It's the PAL.
[01:10:08.240 --> 01:10:12.400]   And when you have models that are like--
[01:10:12.400 --> 01:10:14.720]   be it like Alken being trained on much larger data set,
[01:10:14.720 --> 01:10:18.080]   especially for an English use case, it makes more sense to use that.
[01:10:18.080 --> 01:10:19.920]   I see.
[01:10:19.920 --> 01:10:25.360]   So there will be another paper coming that is RWKB trained on red pajama,
[01:10:25.360 --> 01:10:25.920]   and that will--
[01:10:25.920 --> 01:10:27.760]   For larger data set, yeah.
[01:10:27.760 --> 01:10:28.480]   And so on and so forth.
[01:10:28.480 --> 01:10:29.840]   So I think that's the--
[01:10:29.840 --> 01:10:32.800]   we are still in the stages of reaching that point
[01:10:32.800 --> 01:10:35.040]   where we train on the larger data set.
[01:10:35.040 --> 01:10:38.240]   The only reason why we have a bigger outsized impact
[01:10:38.240 --> 01:10:40.320]   compared to the other models is, frankly,
[01:10:40.320 --> 01:10:45.120]   because half of our discord came in not for English.
[01:10:45.120 --> 01:10:47.200]   It's for other languages.
[01:10:47.200 --> 01:10:47.840]   Yeah, that's great.
[01:10:47.840 --> 01:10:52.160]   And there is a definite very US and English-centric bias
[01:10:52.160 --> 01:10:55.280]   towards these models.
[01:10:55.280 --> 01:10:58.480]   And it's, to me, kind of poetic.
[01:10:58.480 --> 01:11:02.240]   Like, there's nothing in the architecture of RWKB
[01:11:02.240 --> 01:11:06.080]   that particularly bias it to be really good at other languages.
[01:11:06.080 --> 01:11:09.280]   It's just that, as a community, you decided to prioritize it
[01:11:09.280 --> 01:11:11.600]   in your tokenization in the data sets.
[01:11:11.600 --> 01:11:12.160]   That's it.
[01:11:12.160 --> 01:11:12.660]   Yeah, that's it.
[01:11:12.660 --> 01:11:17.280]   I would even argue that I'm surprised--
[01:11:17.280 --> 01:11:20.640]   more surprised that, especially on the European side of things,
[01:11:20.640 --> 01:11:26.320]   that we don't have more models that actually focus on
[01:11:26.320 --> 01:11:27.520]   even the European languages.
[01:11:28.320 --> 01:11:32.720]   Because there is, like, a softer jump to character,
[01:11:32.720 --> 01:11:34.000]   Japanese and Chinese characters.
[01:11:34.000 --> 01:11:35.040]   They're all romantic.
[01:11:35.040 --> 01:11:38.720]   I would say, well, one, Europeans are very hostile
[01:11:38.720 --> 01:11:39.920]   to tech advancement.
[01:11:39.920 --> 01:11:43.840]   They have never met a technology they cannot regulate.
[01:11:43.840 --> 01:11:45.520]   Everything is ban, regulate, ban.
[01:11:45.520 --> 01:11:51.840]   And then, on our side, the Asians like to have waifus.
[01:11:51.840 --> 01:11:55.520]   So that would be my guess.
[01:11:56.960 --> 01:11:58.240]   But I think, back to the benchmark,
[01:11:58.240 --> 01:12:01.600]   what excites me most still about this is that it just
[01:12:01.600 --> 01:12:03.360]   means that we just need to scale.
[01:12:03.360 --> 01:12:07.120]   We just need to scale this model and read the right data--
[01:12:07.120 --> 01:12:07.920]   To, like, 40B?
[01:12:07.920 --> 01:12:10.000]   40B, 60B.
[01:12:10.000 --> 01:12:12.880]   I mean, params is one thing.
[01:12:12.880 --> 01:12:15.040]   It's data sets and GPU time.
[01:12:15.040 --> 01:12:18.240]   Yeah, so you and I are talking offline about ideas
[01:12:18.240 --> 01:12:20.560]   for getting data, getting compute, and all this.
[01:12:20.560 --> 01:12:23.600]   So this is like a project that's ongoing.
[01:12:24.720 --> 01:12:27.200]   OK, anything else for the future of RWA-KB?
[01:12:27.200 --> 01:12:30.320]   And the biggest one would be--
[01:12:30.320 --> 01:12:35.040]   OK, so this is back to how, remember I said,
[01:12:35.040 --> 01:12:38.320]   evals doesn't hide or doesn't highlight everything.
[01:12:38.320 --> 01:12:41.680]   Like, this is nice and all, the evals.
[01:12:41.680 --> 01:12:44.320]   But there's a big realistic on another weakness
[01:12:44.320 --> 01:12:47.520]   on the RWA-KB side, is that now with the rise of,
[01:12:47.520 --> 01:12:50.720]   let's say, 100K or 32K context science windows,
[01:12:52.320 --> 01:12:57.520]   transformable model, RWA-KB currently is trained to handle,
[01:12:57.520 --> 01:12:59.920]   let's say, eight or even some people have already
[01:12:59.920 --> 01:13:02.080]   trained it to 16K sizes.
[01:13:02.080 --> 01:13:06.000]   It has-- and well, it will-- as a neural network,
[01:13:06.000 --> 01:13:08.480]   it will happily keep going on for infinite context length.
[01:13:08.480 --> 01:13:09.600]   It will just keep generating.
[01:13:09.600 --> 01:13:11.680]   Does it do well?
[01:13:11.680 --> 01:13:15.600]   The answer is no, because you didn't train it
[01:13:15.600 --> 01:13:16.720]   to handle that situation.
[01:13:16.720 --> 01:13:17.920]   And there's actually a chart involved.
[01:13:18.800 --> 01:13:23.520]   So for example, if the prediction, the power test loss,
[01:13:23.520 --> 01:13:24.880]   it does improve over time, let's say,
[01:13:24.880 --> 01:13:26.480]   if you go down the context length.
[01:13:26.480 --> 01:13:28.160]   But this is if we train it.
[01:13:28.160 --> 01:13:31.200]   And what is not seen here is that if we were to do,
[01:13:31.200 --> 01:13:33.200]   let's say, run it further, it'll just go back up.
[01:13:33.200 --> 01:13:35.440]   Because it was not trained to handle that.
[01:13:35.440 --> 01:13:37.760]   Well, it technically can run.
[01:13:37.760 --> 01:13:40.560]   It suffers from the longer context length.
[01:13:40.560 --> 01:13:45.520]   And that's the part where RWA-KB,
[01:13:45.520 --> 01:13:48.560]   especially in Q&A tasks, in huge documents,
[01:13:48.560 --> 01:13:51.840]   you get closer to summarize giant documents.
[01:13:51.840 --> 01:13:53.200]   That's where it starts to--
[01:13:53.200 --> 01:13:55.360]   Look, none of this is fundamental.
[01:13:55.360 --> 01:13:56.560]   It's just you need more money.
[01:13:56.560 --> 01:13:57.060]   Yeah.
[01:13:57.060 --> 01:14:00.320]   No, there is actually a fundamental part.
[01:14:00.320 --> 01:14:03.440]   So one of the things that I was doing,
[01:14:03.440 --> 01:14:07.520]   and I am actively helping within the community right now,
[01:14:07.520 --> 01:14:11.280]   is that we found that the existing way
[01:14:11.280 --> 01:14:14.480]   to scale the memory was not that efficient.
[01:14:15.120 --> 01:14:16.880]   And we were just being realistic ourselves.
[01:14:16.880 --> 01:14:20.320]   If we want to hit 100K, we need to change this.
[01:14:20.320 --> 01:14:23.760]   So one thing that I'm actually looking forward to right now
[01:14:23.760 --> 01:14:24.880]   is actually those experiments.
[01:14:24.880 --> 01:14:29.040]   We have already started scaling things
[01:14:29.040 --> 01:14:31.600]   to be able to handle things at transformer scale,
[01:14:31.600 --> 01:14:33.760]   be it the 4K, 8K,
[01:14:33.760 --> 01:14:36.240]   in terms of how it handles memory really well.
[01:14:36.240 --> 01:14:38.560]   And we found that we want to extend it
[01:14:38.560 --> 01:14:40.400]   to be like 16, 32, and 64.
[01:14:40.400 --> 01:14:42.160]   And that is within our roadmap.
[01:14:42.160 --> 01:14:44.720]   And that's the exciting thing,
[01:14:44.720 --> 01:14:46.480]   because once we have that,
[01:14:46.480 --> 01:14:50.320]   it's able to handle long-term memory within those sizes.
[01:14:50.320 --> 01:14:54.400]   It removed what many people in the community felt
[01:14:54.400 --> 01:14:56.160]   was the last architectural limit.
[01:14:56.160 --> 01:14:59.440]   Because once it's able to handle memories
[01:14:59.440 --> 01:15:03.760]   like context length, the same as transformer,
[01:15:03.760 --> 01:15:04.720]   we know what we need to do.
[01:15:04.720 --> 01:15:07.200]   You know how existingly people do
[01:15:07.200 --> 01:15:08.400]   long composition in transformer,
[01:15:08.400 --> 01:15:11.440]   they just discard the rest and the sliding window?
[01:15:11.440 --> 01:15:13.760]   This is the better version of sliding window.
[01:15:13.760 --> 01:15:16.960]   You have the model can handle the sliding window perfectly,
[01:15:16.960 --> 01:15:18.880]   but it can keep remnants behind it.
[01:15:18.880 --> 01:15:20.100]   Sure.
[01:15:20.100 --> 01:15:23.600]   And that's something that I'm really excited and invested towards,
[01:15:23.600 --> 01:15:26.560]   because this is back to the full circle
[01:15:26.560 --> 01:15:29.120]   of how I came into RMKE.
[01:15:29.120 --> 01:15:32.640]   I want my model to handle 100K tokens,
[01:15:32.640 --> 01:15:34.640]   four megabytes of HTML,
[01:15:34.640 --> 01:15:36.400]   whatever I throw at it,
[01:15:36.400 --> 01:15:37.760]   and be able to process it.
[01:15:37.760 --> 01:15:39.120]   But it'll be lossy.
[01:15:39.120 --> 01:15:42.080]   The later half will be lossy,
[01:15:42.080 --> 01:15:45.280]   but the key thing is extending the non-lossy part,
[01:15:45.280 --> 01:15:47.440]   and we are aiming to extend the non-lossy part.
[01:15:47.440 --> 01:15:49.300]   Okay.
[01:15:49.300 --> 01:15:50.500]   Interesting.
[01:15:50.500 --> 01:15:52.960]   Great, that was really good.
[01:15:52.960 --> 01:15:54.320]   Oh, one thing I wanted to cover
[01:15:54.320 --> 01:15:58.240]   before we leave the topic of RWKB altogether.
[01:15:58.240 --> 01:16:00.640]   There's a couple things.
[01:16:00.640 --> 01:16:03.440]   But first of all, what is it like working...
[01:16:03.440 --> 01:16:06.960]   Basically, it's an all-volunteer Discord anonymous community.
[01:16:06.960 --> 01:16:08.720]   You've never met any of these other people,
[01:16:11.200 --> 01:16:13.360]   it's only been done one other time successfully,
[01:16:13.360 --> 01:16:14.880]   which is Eluther, right?
[01:16:14.880 --> 01:16:17.760]   In a way, RWKB is kind of new Eluther.
[01:16:17.760 --> 01:16:19.840]   Obviously, Eluther is still going.
[01:16:19.840 --> 01:16:23.760]   But in as far as active research
[01:16:23.760 --> 01:16:25.520]   in something that's completely untested
[01:16:25.520 --> 01:16:26.640]   by complete nobodies,
[01:16:26.640 --> 01:16:27.840]   it's you guys.
[01:16:27.840 --> 01:16:32.320]   What is it like to organize a group like this?
[01:16:32.320 --> 01:16:37.040]   I've never been involved in something like this before.
[01:16:37.040 --> 01:16:37.540]   It's so weird.
[01:16:38.640 --> 01:16:39.760]   When we use the word organize,
[01:16:39.760 --> 01:16:42.160]   it makes it sound like there's more organization
[01:16:42.160 --> 01:16:42.960]   than there actually is.
[01:16:42.960 --> 01:16:47.200]   If I think about how I've typically done projects,
[01:16:47.200 --> 01:16:48.480]   I would try to assign roles,
[01:16:48.480 --> 01:16:49.840]   or try to have regular meetings,
[01:16:49.840 --> 01:16:51.680]   try to have some...
[01:16:51.680 --> 01:16:52.800]   Everyone is volunteers,
[01:16:52.800 --> 01:16:55.840]   nobody has any means to order people around
[01:16:55.840 --> 01:16:56.800]   or anything like that.
[01:16:56.800 --> 01:16:58.240]   But how do you collaborate
[01:16:58.240 --> 01:17:00.160]   if you don't know what each other are doing,
[01:17:00.160 --> 01:17:03.200]   and you don't have people that are not coming to deadlines?
[01:17:03.200 --> 01:17:05.280]   Do you have a Jira board?
[01:17:07.920 --> 01:17:08.960]   Bringing back to the Discord.
[01:17:08.960 --> 01:17:12.560]   Blink is a busy person.
[01:17:12.560 --> 01:17:14.160]   You are definitely very involved
[01:17:14.160 --> 01:17:15.840]   in the Discord community organizing.
[01:17:15.840 --> 01:17:16.800]   How do you get stuff done?
[01:17:16.800 --> 01:17:23.040]   Blink is also the one who has access
[01:17:23.040 --> 01:17:27.760]   to the main Eluther AI and stability GPU donation.
[01:17:27.760 --> 01:17:30.640]   He's the one that is very focused
[01:17:30.640 --> 01:17:33.120]   on training the big foundation models.
[01:17:33.120 --> 01:17:35.680]   And that's what you do.
[01:17:35.680 --> 01:17:38.000]   So right now, in our current pipeline,
[01:17:38.000 --> 01:17:40.480]   he is focusing on the world model,
[01:17:40.480 --> 01:17:42.640]   and subsequently some experimental models
[01:17:42.640 --> 01:17:44.960]   for RDPKT5, which is the next generation.
[01:17:44.960 --> 01:17:49.360]   And the world model is our next major foundation model
[01:17:49.360 --> 01:17:50.160]   when it's fully trained.
[01:17:50.160 --> 01:17:53.760]   It will cover all the other languages.
[01:17:53.760 --> 01:17:55.760]   And from there onwards,
[01:17:55.760 --> 01:17:59.280]   he just generally continuously keep the Discord updated
[01:17:59.280 --> 01:18:00.480]   on the progress of it.
[01:18:00.480 --> 01:18:02.240]   How's it going?
[01:18:02.240 --> 01:18:03.040]   Where's it going?
[01:18:04.480 --> 01:18:08.080]   He constantly highlights the projects
[01:18:08.080 --> 01:18:08.800]   that are being submitted,
[01:18:08.800 --> 01:18:10.240]   and the internet is just now...
[01:18:10.240 --> 01:18:12.800]   I've been tethering the whole time.
[01:18:12.800 --> 01:18:14.320]   Oh, it's okay.
[01:18:14.320 --> 01:18:14.820]   It's okay.
[01:18:14.820 --> 01:18:16.800]   And then subsequently he updates
[01:18:16.800 --> 01:18:18.560]   with his ideas and his plans and so on.
[01:18:18.560 --> 01:18:20.000]   Like there's even ideas, as you can see.
[01:18:20.000 --> 01:18:20.500]   It's pretty cool.
[01:18:20.500 --> 01:18:22.720]   It's like long-term.
[01:18:22.720 --> 01:18:26.000]   But these are like big ideas.
[01:18:26.000 --> 01:18:29.280]   And sometimes, in a lot of times,
[01:18:29.280 --> 01:18:31.200]   he's very focused on the text models.
[01:18:31.200 --> 01:18:33.520]   And also some of these ideas need
[01:18:33.520 --> 01:18:34.720]   to be tested and validated.
[01:18:34.720 --> 01:18:38.960]   So that's where things start branching off, per se.
[01:18:38.960 --> 01:18:41.120]   So, for example,
[01:18:41.120 --> 01:18:44.880]   one area that I started being active in
[01:18:44.880 --> 01:18:47.440]   was that I was...
[01:18:47.440 --> 01:18:49.440]   At first, when I came in,
[01:18:49.440 --> 01:18:51.360]   I first was being more active in, let's say,
[01:18:51.360 --> 01:18:54.080]   the packaging, the inference code,
[01:18:54.080 --> 01:18:55.040]   to make it more accessible.
[01:18:55.040 --> 01:18:56.560]   So I think one of the things that I showed
[01:18:56.560 --> 01:18:59.520]   was the RDFKV Node.js module.
[01:18:59.520 --> 01:19:01.520]   I can see it.
[01:19:01.520 --> 01:19:02.640]   Yeah, this is fair enough.
[01:19:02.640 --> 01:19:04.240]   The Node.js package, where basically
[01:19:04.240 --> 01:19:06.480]   you can run RDFKV in Node.js,
[01:19:06.480 --> 01:19:08.160]   just to make it more accessible.
[01:19:08.160 --> 01:19:10.560]   And then subsequently, I was supporting that.
[01:19:10.560 --> 01:19:13.040]   Then as more people came on board,
[01:19:13.040 --> 01:19:14.880]   like trying to run it in their respective languages,
[01:19:14.880 --> 01:19:16.260]   I subsequently...
[01:19:16.260 --> 01:19:18.420]   It's okay.
[01:19:18.420 --> 01:19:20.000]   I'm just going to keep going.
[01:19:20.000 --> 01:19:22.880]   I subsequently moved on to focusing
[01:19:22.880 --> 01:19:27.680]   more towards datasets and RDFKV5.
[01:19:27.680 --> 01:19:29.520]   But this is the area that I'm focusing on
[01:19:29.520 --> 01:19:30.240]   and most active.
[01:19:30.240 --> 01:19:32.000]   And this is how we start organizing as well.
[01:19:32.000 --> 01:19:34.880]   Like, individuals have generally
[01:19:34.880 --> 01:19:37.600]   have their own area of focus of what they want.
[01:19:37.600 --> 01:19:40.720]   And it's very focus-driven on,
[01:19:40.720 --> 01:19:42.320]   in a lot of cases, aligned to them.
[01:19:42.320 --> 01:19:44.480]   So for example, like the people
[01:19:44.480 --> 01:19:45.600]   who are working on inference,
[01:19:45.600 --> 01:19:49.200]   the CPP model, the ONIX model,
[01:19:49.200 --> 01:19:50.880]   the CPP versions, right?
[01:19:50.880 --> 01:19:52.640]   Where it takes the existing model
[01:19:52.640 --> 01:19:54.560]   and converts it accordingly.
[01:19:54.560 --> 01:19:56.720]   They are highly motivated to do this
[01:19:56.720 --> 01:19:58.640]   because they want to do the inference
[01:19:58.640 --> 01:20:00.080]   in their use cases,
[01:20:00.080 --> 01:20:01.920]   in their Raspberry Pis, etc.
[01:20:01.920 --> 01:20:05.840]   People like me who's in RDFKV5,
[01:20:05.840 --> 01:20:07.040]   we are actually more of like,
[01:20:07.040 --> 01:20:09.200]   we know there are some weaknesses in the model
[01:20:09.200 --> 01:20:11.440]   and we are trying to make those changes to improve.
[01:20:11.440 --> 01:20:15.040]   So we are actively changing the foundation code.
[01:20:15.040 --> 01:20:17.120]   Then from there onwards, there are channels.
[01:20:17.120 --> 01:20:18.640]   So these RDFKV5 channels,
[01:20:18.640 --> 01:20:22.080]   I mentioned the inference channels,
[01:20:22.080 --> 01:20:23.520]   the CPP channels.
[01:20:23.520 --> 01:20:25.040]   And then from subsequently,
[01:20:25.040 --> 01:20:27.120]   there is also the mounting model channel.
[01:20:27.120 --> 01:20:28.640]   So, and this is an area
[01:20:28.640 --> 01:20:30.960]   where I am not fully active in,
[01:20:30.960 --> 01:20:32.000]   but there are individuals
[01:20:32.000 --> 01:20:34.080]   who are very interested in like,
[01:20:34.080 --> 01:20:35.760]   getting visual recognition,
[01:20:35.760 --> 01:20:39.360]   MiniGBT4, audio.
[01:20:39.360 --> 01:20:43.040]   Apparently the music thing is catching up
[01:20:43.040 --> 01:20:44.160]   within the community right now.
[01:20:44.160 --> 01:20:47.200]   People are getting excited about it.
[01:20:47.200 --> 01:20:49.920]   But this is where various other individuals
[01:20:49.920 --> 01:20:53.040]   come in to just contribute to that site.
[01:20:53.040 --> 01:20:54.720]   And this is still within the sphere
[01:20:54.720 --> 01:20:56.960]   of like, code and engineering.
[01:20:57.520 --> 01:21:01.360]   And like, if I go subsequently back down another step,
[01:21:01.360 --> 01:21:02.880]   there is also the multi-language channel
[01:21:02.880 --> 01:21:04.240]   and the dataset channel.
[01:21:04.240 --> 01:21:06.320]   And this is where you find individuals
[01:21:06.320 --> 01:21:08.240]   who are just, I would call,
[01:21:08.240 --> 01:21:09.520]   I wouldn't say they are like,
[01:21:09.520 --> 01:21:11.200]   playing the role of librarians,
[01:21:11.200 --> 01:21:12.400]   who's just trying to like,
[01:21:12.400 --> 01:21:14.080]   find the right datasets,
[01:21:14.080 --> 01:21:16.880]   label it, collate it, clean it up,
[01:21:16.880 --> 01:21:18.800]   and then put it in part of the training.
[01:21:18.800 --> 01:21:21.840]   And their typical focus
[01:21:21.840 --> 01:21:24.800]   is that they want to support their language better,
[01:21:24.800 --> 01:21:26.240]   or they have their,
[01:21:26.240 --> 01:21:27.600]   I guess like you alluded,
[01:21:27.600 --> 01:21:28.480]   their waifu use case,
[01:21:28.480 --> 01:21:29.680]   they want to make it look better.
[01:21:29.680 --> 01:21:33.360]   And that's how the community-driven effort is done
[01:21:33.360 --> 01:21:36.080]   because everyone actually has a certain incentive
[01:21:36.080 --> 01:21:37.040]   and alignment,
[01:21:37.040 --> 01:21:39.680]   and they just double down on it effectively.
[01:21:39.680 --> 01:21:42.560]   And they start to take a heavy active role in the channel.
[01:21:42.560 --> 01:21:43.920]   So like, frankly,
[01:21:43.920 --> 01:21:45.760]   I'm not going to say that I'm active in multimodal
[01:21:45.760 --> 01:21:48.400]   because that's an area where I'm not really active in.
[01:21:48.400 --> 01:21:49.940]   And so on.
[01:21:49.940 --> 01:21:55.040]   And that's how we try to like, self-organize.
[01:21:55.040 --> 01:21:56.400]   And we share our notes accordingly.
[01:21:56.400 --> 01:21:59.040]   We sometimes just casually just hang out
[01:21:59.040 --> 01:22:00.800]   on the Discord voice chat or whatever,
[01:22:00.800 --> 01:22:02.160]   and then we just talk casually.
[01:22:02.160 --> 01:22:03.440]   But that's more of like,
[01:22:03.440 --> 01:22:04.960]   the more casual stuff of it.
[01:22:04.960 --> 01:22:06.320]   But how things get done,
[01:22:06.320 --> 01:22:08.480]   it's down to the individual groups.
[01:22:08.480 --> 01:22:11.200]   Has Beau stated his ultimate end goal?
[01:22:11.200 --> 01:22:14.320]   Apart from this is cool.
[01:22:14.320 --> 01:22:17.520]   I think we had several,
[01:22:17.520 --> 01:22:20.960]   I had several Discord conversations with him.
[01:22:20.960 --> 01:22:23.280]   I believe that what he,
[01:22:24.080 --> 01:22:25.200]   because I did ask him, frankly,
[01:22:25.200 --> 01:22:27.760]   like, is he planning to make a commercial entity out of it?
[01:22:27.760 --> 01:22:30.000]   Actually, tons of people have asked this
[01:22:30.000 --> 01:22:31.680]   because that seems to be the pattern.
[01:22:31.680 --> 01:22:34.240]   And he seems to be heavily inspired
[01:22:34.240 --> 01:22:36.000]   and wants to go towards the direction of
[01:22:36.000 --> 01:22:39.360]   creating the equivalent of a Linux foundation
[01:22:39.360 --> 01:22:40.640]   but for an AI model.
[01:22:40.640 --> 01:22:42.320]   So he really wants this to be open source.
[01:22:42.320 --> 01:22:46.640]   And that's actually part of what motivates me
[01:22:46.640 --> 01:22:48.880]   to just continue on in this Discord as well.
[01:22:48.880 --> 01:22:49.520]   Yeah, yeah, yeah.
[01:22:49.520 --> 01:22:51.120]   Do you think that,
[01:22:51.120 --> 01:22:53.520]   is that a serious effort?
[01:22:53.520 --> 01:22:58.320]   Because I might be also looking to explore,
[01:22:58.320 --> 01:23:01.360]   I know some friends who are also working on
[01:23:01.360 --> 01:23:02.480]   like an agent protocol
[01:23:02.480 --> 01:23:04.320]   that could benefit from a neutral,
[01:23:04.320 --> 01:23:06.080]   non-profit foundation type thing.
[01:23:06.080 --> 01:23:11.280]   So we might want to work together to set it up.
[01:23:11.280 --> 01:23:12.080]   Yeah, sure.
[01:23:12.080 --> 01:23:16.000]   Because I did post to him a few times,
[01:23:16.000 --> 01:23:17.520]   like, we should, at some point,
[01:23:17.520 --> 01:23:21.280]   organize and set up the actual foundation
[01:23:21.280 --> 01:23:22.720]   rather than the informal...
[01:23:22.720 --> 01:23:25.840]   I think I know the people who would be able to help.
[01:23:25.840 --> 01:23:26.800]   Yeah, that would be great because,
[01:23:26.800 --> 01:23:31.440]   I mean, like, I think for us,
[01:23:31.440 --> 01:23:32.880]   setting up the foundation will probably be
[01:23:32.880 --> 01:23:34.160]   one big major step
[01:23:34.160 --> 01:23:37.280]   because then it will also simplify the process
[01:23:37.280 --> 01:23:39.200]   in terms of like being able to handle
[01:23:39.200 --> 01:23:41.040]   GPU donations and stuff like that.
[01:23:41.040 --> 01:23:44.080]   Yes, that's a good point.
[01:23:44.080 --> 01:23:48.160]   Because right now, a lot of the donations...
[01:23:48.160 --> 01:23:50.320]   So I saw that there is an RWKB foundation.
[01:23:51.280 --> 01:23:52.960]   Oh, no, it doesn't really exist yet.
[01:23:52.960 --> 01:23:53.520]   Oh, okay.
[01:23:53.520 --> 01:23:55.360]   Because he listed himself in the paper as...
[01:23:55.360 --> 01:23:57.600]   This is back to the paper.
[01:23:57.600 --> 01:23:59.600]   The paper requires you to list an organization
[01:23:59.600 --> 01:24:00.480]   that you belong to
[01:24:00.480 --> 01:24:02.320]   and if you don't have an organization,
[01:24:02.320 --> 01:24:02.960]   what do you put?
[01:24:02.960 --> 01:24:05.200]   Okay, interesting.
[01:24:05.200 --> 01:24:07.280]   So we, it's like,
[01:24:07.280 --> 01:24:10.480]   okay, at some point, we will need to set that up.
[01:24:10.480 --> 01:24:12.080]   So he just went ahead and filled it out.
[01:24:12.080 --> 01:24:13.840]   Yeah, cool.
[01:24:13.840 --> 01:24:15.520]   I think that's the RWKB portion
[01:24:15.520 --> 01:24:17.360]   unless there's any other parting notes.
[01:24:19.280 --> 01:24:22.960]   Yeah, the Discord is filled with people
[01:24:22.960 --> 01:24:25.040]   always trying to do many things.
[01:24:25.040 --> 01:24:28.080]   If anyone has any interest in a really specific task,
[01:24:28.080 --> 01:24:29.120]   go ahead, join in.
[01:24:29.120 --> 01:24:30.000]   If you just want,
[01:24:30.000 --> 01:24:31.680]   if you are from a foreign country
[01:24:31.680 --> 01:24:33.840]   that it seems like no model
[01:24:33.840 --> 01:24:35.040]   seems to care about your language,
[01:24:35.040 --> 01:24:36.000]   please do join in
[01:24:36.000 --> 01:24:37.840]   because we want these people,
[01:24:37.840 --> 01:24:39.040]   we want to support your language
[01:24:39.040 --> 01:24:40.880]   and we want to know how good
[01:24:40.880 --> 01:24:42.880]   or how bad our model is in that language.
[01:24:42.880 --> 01:24:45.200]   So what I would do here as a product manager
[01:24:45.200 --> 01:24:48.080]   is like put up a public repo somewhere of like,
[01:24:48.080 --> 01:24:49.360]   here's all the language you want to target,
[01:24:49.360 --> 01:24:50.800]   here's our completion ratio,
[01:24:50.800 --> 01:24:52.480]   like, you know, check, check, check, check,
[01:24:52.480 --> 01:24:53.120]   blank, blank, blank.
[01:24:53.120 --> 01:24:55.120]   We need some of the toolkit.
[01:24:55.120 --> 01:24:58.880]   Exactly, this would be a classic PM type of thing.
[01:24:58.880 --> 01:25:00.480]   But anyway, so anyone listening,
[01:25:00.480 --> 01:25:04.480]   if you are interested, Eugene is Pico creator.
[01:25:04.480 --> 01:25:04.980]   Yep.
[01:25:04.980 --> 01:25:07.840]   You seem to be all over the Discord,
[01:25:07.840 --> 01:25:09.440]   so it should be pretty easy to find you.
[01:25:09.440 --> 01:25:09.940]   Yeah.
[01:25:09.940 --> 01:25:14.640]   Okay, and so that's basically the RWKB portion.
[01:25:14.640 --> 01:25:17.520]   You had one more comment
[01:25:17.520 --> 01:25:18.720]   about alternative models
[01:25:18.720 --> 01:25:20.400]   and you mentioned that you actually,
[01:25:20.400 --> 01:25:23.040]   apart from RWKB, which is one thing,
[01:25:23.040 --> 01:25:24.560]   it's not like your whole identity.
[01:25:24.560 --> 01:25:25.060]   Yeah.
[01:25:25.060 --> 01:25:26.720]   You're very involved right now.
[01:25:26.720 --> 01:25:28.480]   You said that there's also potentials
[01:25:28.480 --> 01:25:30.000]   for diffusion models and tests.
[01:25:30.000 --> 01:25:31.120]   Oh, yeah.
[01:25:31.120 --> 01:25:35.520]   So I think for me, the key principle
[01:25:35.520 --> 01:25:37.280]   is that we want to make sure
[01:25:37.280 --> 01:25:40.960]   we avoid the trap into landing on that one model
[01:25:40.960 --> 01:25:41.760]   to rule them all
[01:25:41.760 --> 01:25:44.160]   because all models were at some,
[01:25:44.160 --> 01:25:46.080]   from an architecture point of view,
[01:25:46.080 --> 01:25:47.040]   may do some trade-off.
[01:25:47.440 --> 01:25:50.000]   And if, let's say, we go back to the point
[01:25:50.000 --> 01:25:52.640]   where maybe all we need is a scalable model
[01:25:52.640 --> 01:25:54.640]   and a good data set,
[01:25:54.640 --> 01:25:56.800]   it's in the community's best interest
[01:25:56.800 --> 01:25:58.320]   or more like the whole world's best interest
[01:25:58.320 --> 01:26:01.760]   because we are putting a lot of GPU energy and time
[01:26:01.760 --> 01:26:03.200]   to find an efficient model
[01:26:03.200 --> 01:26:05.200]   for all the respective use case.
[01:26:05.200 --> 01:26:05.700]   Okay.
[01:26:05.700 --> 01:26:09.840]   And all these are all trade-offs.
[01:26:09.840 --> 01:26:12.640]   So even if, let's say, fast forward,
[01:26:12.640 --> 01:26:14.640]   maybe RWKB became the prominent model,
[01:26:14.640 --> 01:26:16.560]   I would still say that we need to explore
[01:26:16.560 --> 01:26:17.360]   all of these models
[01:26:17.360 --> 01:26:19.120]   because all models will have its weaknesses.
[01:26:19.120 --> 01:26:22.640]   So one of RWKB's and Transformer's model's weakness
[01:26:22.640 --> 01:26:24.800]   is that, and I think there was a paper that covered it,
[01:26:24.800 --> 01:26:26.960]   is the multi-epoch
[01:26:26.960 --> 01:26:31.760]   and how training,
[01:26:31.760 --> 01:26:34.000]   you should ideally train for one to two epoch.
[01:26:34.000 --> 01:26:36.080]   Yeah, and that's Arun Kotsumarski,
[01:26:36.080 --> 01:26:37.040]   or whatever his name is.
[01:26:37.040 --> 01:26:39.040]   Yeah, I can't remember off my head, sorry.
[01:26:39.040 --> 01:26:41.760]   Yeah, his paper is literally titled
[01:26:41.760 --> 01:26:42.960]   "One Epoch is All You Need."
[01:26:42.960 --> 01:26:43.520]   Correct.
[01:26:43.520 --> 01:26:47.120]   I actually have observed that this is strange to me,
[01:26:47.120 --> 01:26:49.760]   that you only train one epoch for a whole dataset.
[01:26:49.760 --> 01:26:52.640]   Yeah, and anything beyond that,
[01:26:52.640 --> 01:26:54.480]   and we can confirm, even for our model,
[01:26:54.480 --> 01:26:57.200]   ours is more like closer to two,
[01:26:57.200 --> 01:26:58.720]   but the idea is still there,
[01:26:58.720 --> 01:27:00.320]   that it's starting to overfit
[01:27:00.320 --> 01:27:02.800]   and it starts to degrade in a lot of things.
[01:27:02.800 --> 01:27:06.560]   And I think this is a serious enough problem
[01:27:06.560 --> 01:27:08.560]   that within the Transformer community,
[01:27:08.560 --> 01:27:12.400]   that we sometimes joke about the token crisis.
[01:27:12.400 --> 01:27:12.800]   Yes.
[01:27:12.800 --> 01:27:14.640]   That eventually you'll run out of tokens.
[01:27:14.640 --> 01:27:15.680]   Do you think there's a token crisis?
[01:27:15.680 --> 01:27:19.520]   I would say if we are aiming for AGI,
[01:27:19.520 --> 01:27:21.120]   there is a token crisis.
[01:27:21.120 --> 01:27:25.360]   But if we are aiming for useful small models,
[01:27:25.360 --> 01:27:27.120]   I don't think there is a token crisis.
[01:27:27.120 --> 01:27:28.260]   Right.
[01:27:28.260 --> 01:27:31.200]   Let's talk about AGI,
[01:27:31.200 --> 01:27:32.640]   because the small model stuff
[01:27:32.640 --> 01:27:34.480]   is, I think, a given at this point.
[01:27:34.480 --> 01:27:36.880]   But right now, let's say,
[01:27:36.880 --> 01:27:40.560]   Lama 2 was trained on 2 trillion tokens.
[01:27:41.360 --> 01:27:42.800]   Can we go to 20 trillion?
[01:27:42.800 --> 01:27:44.080]   Can we go to 200 trillion?
[01:27:44.080 --> 01:27:46.320]   Is there orders of magnitude left,
[01:27:46.320 --> 01:27:48.320]   or are we basically almost done?
[01:27:48.320 --> 01:27:50.960]   I think that one thing amazing about the Lama paper
[01:27:50.960 --> 01:27:54.080]   is that it showed that even at 2 trillion...
[01:27:54.080 --> 01:27:54.720]   It's not levelling off.
[01:27:54.720 --> 01:27:55.360]   Yeah, it's not.
[01:27:55.360 --> 01:27:56.320]   It's still going, yeah.
[01:27:56.320 --> 01:27:57.680]   So you could potentially train it
[01:27:57.680 --> 01:27:59.040]   for all 16 or whatever it is.
[01:27:59.040 --> 01:28:00.000]   We don't know what's in it.
[01:28:00.000 --> 01:28:01.280]   But the problem here is,
[01:28:01.280 --> 01:28:02.320]   where are we going to get the tokens?
[01:28:02.320 --> 01:28:03.680]   Because we already established that
[01:28:03.680 --> 01:28:07.280]   it's equally important that you have good data.
[01:28:07.280 --> 01:28:08.000]   Quality tokens.
[01:28:08.000 --> 01:28:10.320]   Yeah, that goes in rather junk data.
[01:28:10.320 --> 01:28:13.840]   And that's the crisis,
[01:28:13.840 --> 01:28:14.720]   for lack of a better word.
[01:28:14.720 --> 01:28:17.360]   And I feel that it might actually get worse,
[01:28:17.360 --> 01:28:19.120]   mostly because,
[01:28:19.120 --> 01:28:20.880]   well, yeah, we can keep crawling the internet,
[01:28:20.880 --> 01:28:22.320]   but now with AI models
[01:28:22.320 --> 01:28:24.240]   dumping content to the internet,
[01:28:24.240 --> 01:28:26.160]   you actually need to figure out
[01:28:26.160 --> 01:28:27.680]   what is quality content,
[01:28:27.680 --> 01:28:28.640]   and you need to start filtering.
[01:28:28.640 --> 01:28:31.200]   So this is literally a librarian's job.
[01:28:31.200 --> 01:28:35.040]   One of the things that we export
[01:28:35.040 --> 01:28:35.680]   within our company
[01:28:35.680 --> 01:28:37.920]   is starting to classify our models,
[01:28:37.920 --> 01:28:38.960]   no, I mean, our data sets,
[01:28:39.920 --> 01:28:42.160]   literally taking the library classification.
[01:28:42.160 --> 01:28:44.640]   Yeah, the Dewey Decimal System.
[01:28:44.640 --> 01:28:46.880]   Yeah, and then using that accordingly,
[01:28:46.880 --> 01:28:48.080]   because there's just so much things.
[01:28:48.080 --> 01:28:51.440]   And as long as we,
[01:28:51.440 --> 01:28:53.440]   currently one of the biggest gap
[01:28:53.440 --> 01:28:54.800]   that we've noticed is that,
[01:28:54.800 --> 01:28:56.320]   well, there are a lot of books,
[01:28:56.320 --> 01:29:00.800]   a lot of them are stored digitally as images.
[01:29:00.800 --> 01:29:02.640]   So in terms of text,
[01:29:02.640 --> 01:29:03.760]   there is actually a shortage.
[01:29:03.760 --> 01:29:05.920]   Okay, run an OCR step.
[01:29:05.920 --> 01:29:08.720]   Easier said than done.
[01:29:09.680 --> 01:29:13.200]   And that's where the token crisis went.
[01:29:13.200 --> 01:29:15.520]   But I mean, this is back to
[01:29:15.520 --> 01:29:17.600]   why I'm interested in Alternate,
[01:29:17.600 --> 01:29:19.040]   because the reason why I pointed out
[01:29:19.040 --> 01:29:20.160]   the Fusion model is that,
[01:29:20.160 --> 01:29:24.400]   transformer and large-angle models
[01:29:24.400 --> 01:29:26.320]   right now having that one, two epoch limitation,
[01:29:26.320 --> 01:29:29.760]   and you go talk to people in the image space,
[01:29:29.760 --> 01:29:31.040]   and they're like, what?
[01:29:31.040 --> 01:29:31.760]   50 epochs.
[01:29:31.760 --> 01:29:34.320]   50 epochs is low.
[01:29:34.320 --> 01:29:35.760]   We do 200, 250.
[01:29:37.680 --> 01:29:40.640]   And there's various reasons for it.
[01:29:40.640 --> 01:29:42.160]   I mean, this is pure speculative.
[01:29:42.160 --> 01:29:44.720]   My speculative reason for it is that,
[01:29:44.720 --> 01:29:47.680]   the Fusion models work so well
[01:29:47.680 --> 01:29:48.800]   in multiple epoch,
[01:29:48.800 --> 01:29:51.520]   because each training epoch, right,
[01:29:51.520 --> 01:29:53.040]   it is randomized with noise.
[01:29:53.040 --> 01:29:56.640]   And effectively, each training run,
[01:29:56.640 --> 01:29:58.480]   even if it's the same data sample,
[01:29:58.480 --> 01:30:01.680]   it is different due to the noise introduced
[01:30:01.680 --> 01:30:03.040]   or whatever's truncated and removed.
[01:30:03.040 --> 01:30:05.920]   And that's why it held up well.
[01:30:05.920 --> 01:30:08.800]   I mean, and if that is the case,
[01:30:08.800 --> 01:30:12.880]   shouldn't we be exploring more,
[01:30:12.880 --> 01:30:14.400]   as well, into diffusion models,
[01:30:14.400 --> 01:30:15.600]   even for text,
[01:30:15.600 --> 01:30:17.760]   into emulating parts of this behavior,
[01:30:17.760 --> 01:30:20.240]   or exploring, as I said,
[01:30:20.240 --> 01:30:22.800]   like one of the reasons why diffusion models
[01:30:22.800 --> 01:30:24.480]   are not being used for text is because it's slow.
[01:30:24.480 --> 01:30:27.440]   Shouldn't we, alternatively,
[01:30:27.440 --> 01:30:29.280]   could we be exploring how to make it faster?
[01:30:29.280 --> 01:30:30.880]   And this is why I feel like,
[01:30:30.880 --> 01:30:32.560]   like, even from,
[01:30:33.440 --> 01:30:35.120]   even if we talk about RLKV being,
[01:30:35.120 --> 01:30:35.920]   having the trade-off,
[01:30:35.920 --> 01:30:38.000]   yes, it's faster, it's scalable, and whatsoever,
[01:30:38.000 --> 01:30:40.240]   there is other trade-offs that is still limited.
[01:30:40.240 --> 01:30:42.400]   It still suffers from the multi-epoch problem,
[01:30:42.400 --> 01:30:44.800]   and the Fusion models may actually represent
[01:30:44.800 --> 01:30:49.680]   a potential for us to escape this token crisis,
[01:30:49.680 --> 01:30:52.480]   and maybe train on our dataset 200, 500 times.
[01:30:52.480 --> 01:30:54.880]   That's interesting.
[01:30:54.880 --> 01:30:56.800]   I don't know how to respond to that apart from,
[01:30:56.800 --> 01:30:59.520]   like, I think it's a new perspective I haven't heard.
[01:30:59.520 --> 01:31:02.080]   Yeah, but, to be clear, this is all
[01:31:02.080 --> 01:31:04.640]   NetStreetMath theory, and I could be completely wrong.
[01:31:04.640 --> 01:31:07.520]   Okay, you know, so, to me,
[01:31:07.520 --> 01:31:08.960]   the speed thing really does matter,
[01:31:08.960 --> 01:31:12.320]   and being able to stream token by token actually is a,
[01:31:12.320 --> 01:31:15.200]   it's known to be good UX, right?
[01:31:15.200 --> 01:31:17.920]   And I'm not going to wait for my essay
[01:31:17.920 --> 01:31:20.640]   to, like, slowly materialize from the diffusion process, right?
[01:31:20.640 --> 01:31:24.480]   Maybe, but maybe you'll find some use cases there.
[01:31:24.480 --> 01:31:26.800]   Or maybe we can just extract the part
[01:31:26.800 --> 01:31:28.400]   where it's trained with noise
[01:31:28.400 --> 01:31:30.320]   and somehow survive multi-epoch.
[01:31:30.320 --> 01:31:30.820]   Right.
[01:31:30.820 --> 01:31:34.080]   And then the other criticism off the top of my head
[01:31:34.080 --> 01:31:35.760]   of what you're saying is that, like, you know,
[01:31:35.760 --> 01:31:39.920]   even RWKV and typical transformer models
[01:31:39.920 --> 01:31:42.000]   would have random initializations,
[01:31:42.000 --> 01:31:44.000]   but why can't we just, if your thesis is that
[01:31:44.000 --> 01:31:47.040]   starting from random initializations
[01:31:47.040 --> 01:31:50.480]   gives you the ability to do multi-epoch, right?
[01:31:50.480 --> 01:31:53.360]   It's not, so not, diffusion is not just random initialization.
[01:31:53.360 --> 01:31:57.120]   It's, there is randomness in the data
[01:31:57.120 --> 01:31:58.880]   that they intentionally put in,
[01:31:58.880 --> 01:32:01.120]   and as they remove in training.
[01:32:01.120 --> 01:32:05.840]   So it's not just at the start.
[01:32:05.840 --> 01:32:07.040]   It's part of the training process.
[01:32:07.040 --> 01:32:08.480]   In the middle of the image.
[01:32:08.480 --> 01:32:09.360]   Right, right, that makes sense.
[01:32:09.360 --> 01:32:09.860]   Yeah.
[01:32:09.860 --> 01:32:13.440]   How we translate that into a
[01:32:13.440 --> 01:32:16.240]   transformer prediction training,
[01:32:16.240 --> 01:32:17.600]   I have no idea.
[01:32:17.600 --> 01:32:18.240]   Yeah, yeah.
[01:32:18.240 --> 01:32:20.080]   I mean, so my, you know,
[01:32:20.080 --> 01:32:21.600]   analogy would be,
[01:32:21.600 --> 01:32:24.560]   they should make a Frankenstein RWKVD
[01:32:24.560 --> 01:32:26.400]   that just has some weird thing,
[01:32:26.400 --> 01:32:27.760]   diffusion kind of slapped onto it,
[01:32:27.760 --> 01:32:29.120]   and then you're fine, you know?
[01:32:29.120 --> 01:32:30.800]   And then maybe it proves that it's yes,
[01:32:30.800 --> 01:32:31.840]   or maybe it just goes wrong.
[01:32:31.840 --> 01:32:33.280]   And I'm all for it.
[01:32:33.280 --> 01:32:34.240]   Like, someone needs to try it.
[01:32:34.240 --> 01:32:35.440]   Yeah, someone needs to try it.
[01:32:35.440 --> 01:32:36.000]   Okay, cool.
[01:32:36.000 --> 01:32:38.160]   So we're going to wrap up with just your,
[01:32:38.160 --> 01:32:41.600]   so, you know, you have displayed today
[01:32:41.600 --> 01:32:43.040]   an impressive amount of knowledge
[01:32:43.040 --> 01:32:45.520]   just across the, you know, all this stuff,
[01:32:45.520 --> 01:32:48.800]   and you don't have, like, a research background.
[01:32:48.800 --> 01:32:53.120]   Your advice to AI engineers getting as deep as you,
[01:32:53.120 --> 01:32:54.400]   who want to get as deep as you.
[01:32:54.400 --> 01:32:57.040]   Any thoughts?
[01:32:57.040 --> 01:33:00.400]   So I think your article articulated very well
[01:33:00.400 --> 01:33:02.240]   that there's going to be divisions
[01:33:02.240 --> 01:33:03.680]   within how we approach this.
[01:33:03.680 --> 01:33:05.360]   So AI engineers,
[01:33:05.360 --> 01:33:10.160]   sorry if I don't quote it correctly,
[01:33:10.160 --> 01:33:12.400]   AI engineers, and in my head, the next level.
[01:33:12.400 --> 01:33:15.040]   The beauty of it is that I define the two words,
[01:33:15.040 --> 01:33:17.200]   and then everyone has their own definition,
[01:33:17.200 --> 01:33:18.800]   but they all roughly project
[01:33:18.800 --> 01:33:19.920]   onto the same embedding space.
[01:33:19.920 --> 01:33:23.360]   Okay, it's beautiful.
[01:33:24.320 --> 01:33:27.360]   So AI engineers, model trainers,
[01:33:27.360 --> 01:33:28.800]   and dataset curators,
[01:33:28.800 --> 01:33:31.600]   and ML scientists.
[01:33:31.600 --> 01:33:33.600]   So I'll loosely define the tree.
[01:33:33.600 --> 01:33:34.480]   I ignore the full stack
[01:33:34.480 --> 01:33:36.240]   because every company needs it.
[01:33:36.240 --> 01:33:39.760]   So within this tree space,
[01:33:39.760 --> 01:33:43.520]   there is actually a lot of ways
[01:33:43.520 --> 01:33:46.400]   anyone can come in without knowing anything.
[01:33:46.400 --> 01:33:48.160]   So let's just start with AI engineers.
[01:33:48.160 --> 01:33:52.560]   Don't be, like, even though this whole topic,
[01:33:52.560 --> 01:33:54.640]   we even dive into how the layers work.
[01:33:54.640 --> 01:33:56.320]   We also show how the math works.
[01:33:56.320 --> 01:33:58.080]   Frankly, for an AI engineer,
[01:33:58.080 --> 01:33:58.720]   you don't need it.
[01:33:58.720 --> 01:34:03.920]   Your main thing that you needed to do was to,
[01:34:03.920 --> 01:34:07.040]   frankly, just play around with chatGPT
[01:34:07.040 --> 01:34:09.280]   of all the alternatives,
[01:34:09.280 --> 01:34:10.880]   be aware of the alternatives,
[01:34:10.880 --> 01:34:12.480]   just be very mercenary,
[01:34:12.480 --> 01:34:15.840]   swap out to Cloudera if it's better for you,
[01:34:15.840 --> 01:34:17.920]   or swap out to an open source if it's better for you,
[01:34:17.920 --> 01:34:19.120]   and just play around with prompts.
[01:34:20.480 --> 01:34:22.080]   Learn prompting techniques,
[01:34:22.080 --> 01:34:23.760]   like one shot, two shots, few shots,
[01:34:23.760 --> 01:34:26.320]   and then from there on,
[01:34:26.320 --> 01:34:28.080]   you can start building your agents,
[01:34:28.080 --> 01:34:31.520]   stacking your prompts in sequences,
[01:34:31.520 --> 01:34:32.320]   and stuff like that,
[01:34:32.320 --> 01:34:33.680]   and you are able to build applications
[01:34:33.680 --> 01:34:37.280]   that do anything in terms of the AI space.
[01:34:37.280 --> 01:34:41.520]   All this without knowing all this nerdy stuff
[01:34:41.520 --> 01:34:44.240]   or the hard engineering,
[01:34:44.240 --> 01:34:45.840]   because that's all you really need
[01:34:45.840 --> 01:34:47.680]   to actually build a product for the user.
[01:34:47.680 --> 01:34:49.520]   Remember, you are supposed to focus
[01:34:49.520 --> 01:34:51.360]   on making it for the user.
[01:34:51.360 --> 01:34:54.160]   They don't care if it's RWKB or Transformer
[01:34:54.160 --> 01:34:55.280]   underneath the hood,
[01:34:55.280 --> 01:34:56.720]   they just care that it helps them.
[01:34:56.720 --> 01:34:59.840]   And I would say like Notion,
[01:34:59.840 --> 01:35:01.600]   probably, is probably one good example
[01:35:01.600 --> 01:35:02.560]   of how they use it,
[01:35:02.560 --> 01:35:05.040]   because we know underneath the hood is OpenAI,
[01:35:05.040 --> 01:35:06.800]   but it's really useful.
[01:35:06.800 --> 01:35:07.520]   It's great, right?
[01:35:07.520 --> 01:35:08.020]   Yeah.
[01:35:08.020 --> 01:35:11.680]   No, so I obviously agree with all that.
[01:35:11.680 --> 01:35:14.160]   Let's just say that people are there already,
[01:35:14.160 --> 01:35:15.280]   and they're just curious,
[01:35:15.280 --> 01:35:16.240]   they want to do what you did.
[01:35:16.960 --> 01:35:19.440]   So that's where you start going down the layers.
[01:35:19.440 --> 01:35:19.940]   Yes.
[01:35:19.940 --> 01:35:23.520]   So the next layer you go down in
[01:35:23.520 --> 01:35:26.080]   is subsequently training the model
[01:35:26.080 --> 01:35:28.480]   from scratch, fine-tuning,
[01:35:28.480 --> 01:35:29.680]   and incorporating the dataset.
[01:35:29.680 --> 01:35:33.360]   And this is where
[01:35:33.360 --> 01:35:35.760]   you still do not need to know the math,
[01:35:35.760 --> 01:35:38.560]   but you need to have a rough sensing
[01:35:38.560 --> 01:35:40.320]   on how the model works,
[01:35:40.320 --> 01:35:42.720]   and how the certain models,
[01:35:42.720 --> 01:35:45.440]   and in this, even within the open source Transformer space,
[01:35:45.440 --> 01:35:47.440]   certain models are better trained
[01:35:47.440 --> 01:35:48.400]   in certain sequences,
[01:35:48.400 --> 01:35:49.760]   with certain learning rates,
[01:35:49.760 --> 01:35:51.040]   and you just need to get a feel of it.
[01:35:51.040 --> 01:35:52.000]   So this is just like,
[01:35:52.000 --> 01:35:52.800]   collect the dataset,
[01:35:52.800 --> 01:35:55.040]   try it, see the loss.
[01:35:55.040 --> 01:35:56.880]   You literally did this?
[01:35:56.880 --> 01:35:58.720]   Yeah, at least for RWKB and the CodeGen model.
[01:35:58.720 --> 01:36:00.240]   That's a lot of work.
[01:36:00.240 --> 01:36:01.760]   Yeah, it's not a cheap work, too,
[01:36:01.760 --> 01:36:03.200]   because you need GPUs.
[01:36:03.200 --> 01:36:05.120]   Okay, and that took you how long?
[01:36:05.120 --> 01:36:10.480]   I think CodeGen alone was like six months,
[01:36:10.480 --> 01:36:11.760]   and then this RWKB,
[01:36:11.760 --> 01:36:13.360]   I've been doing this for like another six months,
[01:36:14.320 --> 01:36:18.640]   and that is just pure experimentation.
[01:36:18.640 --> 01:36:20.240]   There's no right or wrong,
[01:36:20.240 --> 01:36:22.960]   because especially if it's in a different domain.
[01:36:22.960 --> 01:36:26.560]   Recently, I was helping someone on the RWKB discord
[01:36:26.560 --> 01:36:29.280]   regarding the music generation domain,
[01:36:29.280 --> 01:36:30.880]   and my assumptions for learning rate
[01:36:30.880 --> 01:36:33.440]   and all the patterns were just completely thrown out the window,
[01:36:33.440 --> 01:36:36.640]   because the music model just fundamentally is different
[01:36:36.640 --> 01:36:37.760]   in those sense.
[01:36:37.760 --> 01:36:40.400]   The exciting thing is,
[01:36:40.400 --> 01:36:43.680]   because it doesn't really have any specific rules,
[01:36:43.680 --> 01:36:45.040]   any guidelines until you get,
[01:36:45.040 --> 01:36:48.400]   until you trial and error to a certain space,
[01:36:48.400 --> 01:36:51.840]   it also means that you coming in
[01:36:51.840 --> 01:36:54.400]   is as fresh as anyone else coming in last year.
[01:36:54.400 --> 01:36:58.880]   It's really that kind of uncharted space for everyone,
[01:36:58.880 --> 01:37:02.000]   and especially as you start exploring to new domains,
[01:37:02.000 --> 01:37:06.720]   your existing knowledge may actually matter,
[01:37:06.720 --> 01:37:07.680]   because sometimes,
[01:37:07.680 --> 01:37:09.840]   I mean, I think a few papers already covered this,
[01:37:09.920 --> 01:37:14.880]   that how you train your model in certain sequences also matter,
[01:37:14.880 --> 01:37:16.880]   like you want to train a certain set of knowledge,
[01:37:16.880 --> 01:37:19.280]   and then you extend that knowledge subsequently.
[01:37:19.280 --> 01:37:22.640]   But if you're talking about material science or genetics,
[01:37:22.640 --> 01:37:24.320]   how am I supposed to know what is foundational
[01:37:24.320 --> 01:37:25.680]   and what is extended knowledge?
[01:37:25.680 --> 01:37:26.800]   I have no idea.
[01:37:26.800 --> 01:37:27.520]   Maybe you do.
[01:37:27.520 --> 01:37:30.160]   I'm just picking an example.
[01:37:30.160 --> 01:37:33.680]   And the same thing for music and so on.
[01:37:33.680 --> 01:37:36.560]   So those are things where even though you're outside the space,
[01:37:36.560 --> 01:37:38.960]   it's where you can come in just at the dataset level.
[01:37:39.360 --> 01:37:42.160]   Now, you want to peel off to the next layer, let's just say.
[01:37:42.160 --> 01:37:45.680]   Let's just say you want to look into modifying the model,
[01:37:45.680 --> 01:37:49.520]   the foundations of it.
[01:37:49.520 --> 01:37:53.520]   I think one of the beauties about this current boom
[01:37:53.520 --> 01:37:57.280]   is that even though I dipped my toes early,
[01:37:57.280 --> 01:37:59.280]   like before the transformer wave
[01:37:59.280 --> 01:38:01.040]   and in the early neural network phase,
[01:38:01.040 --> 01:38:05.280]   frankly, almost everything that matters
[01:38:05.280 --> 01:38:08.240]   was basically in the past four years.
[01:38:08.400 --> 01:38:13.040]   Like there were a lot of things that fit in academics
[01:38:13.040 --> 01:38:14.000]   that were before that,
[01:38:14.000 --> 01:38:16.160]   and they were mostly dealing with models
[01:38:16.160 --> 01:38:18.000]   that were under abelian parameters.
[01:38:18.000 --> 01:38:20.640]   They pretty much no longer matter.
[01:38:20.640 --> 01:38:23.680]   And can you be more specific?
[01:38:23.680 --> 01:38:26.800]   Are you talking about concepts like dropouts?
[01:38:26.800 --> 01:38:29.840]   Dropout, surprisingly, is coming back,
[01:38:29.840 --> 01:38:31.840]   but things like, for example,
[01:38:31.840 --> 01:38:33.680]   like, okay, I know I'm shooting myself in the foot
[01:38:33.680 --> 01:38:35.600]   because I'm never curious in neural network,
[01:38:35.600 --> 01:38:37.120]   but if you're just trying to get transformers,
[01:38:37.120 --> 01:38:38.960]   but if you're just trying to get transformers to work,
[01:38:38.960 --> 01:38:40.240]   you don't need to know LSTM.
[01:38:40.240 --> 01:38:42.400]   (laughing)
[01:38:42.400 --> 01:38:42.900]   - Yes.
[01:38:42.900 --> 01:38:47.680]   - You don't, yeah, there's a lot of pre-knowledge
[01:38:47.680 --> 01:38:50.560]   in neural networks that is irrelevant
[01:38:50.560 --> 01:38:51.760]   in the transformer era,
[01:38:51.760 --> 01:38:54.960]   and maybe some of it will have a resurgence,
[01:38:54.960 --> 01:38:59.360]   but to get up and running is not a requirement.
[01:38:59.360 --> 01:39:04.480]   And I think this is where you could either go
[01:39:04.480 --> 01:39:06.880]   the very academic way of reading papers and stuff,
[01:39:06.880 --> 01:39:09.760]   but frankly, what I found was way more useful was,
[01:39:09.760 --> 01:39:12.000]   I can't pronounce the name again,
[01:39:12.000 --> 01:39:13.540]   the...
[01:39:13.540 --> 01:39:15.760]   - Carpathy.
[01:39:15.760 --> 01:39:16.880]   - Yeah, Carpathy, yeah.
[01:39:16.880 --> 01:39:17.760]   A series of videos.
[01:39:17.760 --> 01:39:18.640]   - A Series of Heroes.
[01:39:18.640 --> 01:39:22.720]   - Yeah, that is really, really good.
[01:39:22.720 --> 01:39:26.400]   I think even if I, even though I read some of the,
[01:39:26.400 --> 01:39:29.760]   read some of the papers and guides before that,
[01:39:29.760 --> 01:39:32.960]   it really helps that it starts from zero
[01:39:32.960 --> 01:39:36.640]   because you can see how it happens part by part.
[01:39:36.640 --> 01:39:39.440]   And even though we will not use how,
[01:39:39.440 --> 01:39:40.720]   the exact same code that he used,
[01:39:40.720 --> 01:39:43.440]   because he re-implemented the backprop and all that,
[01:39:43.440 --> 01:39:45.280]   and we're just gonna use Torch for that, yeah,
[01:39:45.280 --> 01:39:46.800]   PyTorch for that,
[01:39:46.800 --> 01:39:51.200]   that's where you get the aha moments
[01:39:51.200 --> 01:39:53.360]   on how these building blocks work
[01:39:53.360 --> 01:39:55.360]   and how it fall into place.
[01:39:55.360 --> 01:39:58.560]   And I had fundamental misunderstanding
[01:39:58.560 --> 01:40:01.360]   on how backprop worked until I actually watched his video.
[01:40:01.360 --> 01:40:02.000]   - Oh, really?
[01:40:02.000 --> 01:40:06.240]   - Yeah, and I think that's the scariest
[01:40:06.240 --> 01:40:07.680]   and craziest thing about AI models sometimes
[01:40:07.680 --> 01:40:10.640]   is that you can actually have fundamental misunderstanding,
[01:40:10.640 --> 01:40:12.880]   but as long as you make the building blocks
[01:40:12.880 --> 01:40:15.280]   and then you connect, and okay, loss is great.
[01:40:15.280 --> 01:40:15.840]   It works.
[01:40:15.840 --> 01:40:21.760]   - Yeah, well, so even the gods of the industry,
[01:40:21.760 --> 01:40:23.760]   I don't know if you read the Swiglu paper.
[01:40:23.760 --> 01:40:27.520]   So there's this alternative activation functions,
[01:40:27.520 --> 01:40:28.880]   like there's ReLU,
[01:40:28.880 --> 01:40:32.320]   and then people are always looking for different slopes.
[01:40:32.320 --> 01:40:35.840]   And very famously, the Swiglu paper,
[01:40:36.320 --> 01:40:38.560]   had this line in there that was like,
[01:40:38.560 --> 01:40:40.000]   "Yeah, we don't know why this works, but it works."
[01:40:40.000 --> 01:40:42.720]   Can't explain it.
[01:40:42.720 --> 01:40:44.800]   - Yeah, it literally happens here and there.
[01:40:44.800 --> 01:40:49.440]   One of the funny things that I'm doing right now
[01:40:49.440 --> 01:40:52.000]   in other KVE-5 experiments is that,
[01:40:52.000 --> 01:40:54.080]   okay, we are going to do this change,
[01:40:54.080 --> 01:40:55.280]   where we're going to run this train.
[01:40:55.280 --> 01:40:56.800]   Make your prediction.
[01:40:56.800 --> 01:41:00.000]   Will this model beat this model in this loss curve?
[01:41:00.000 --> 01:41:02.160]   - As a game, as a betting?
[01:41:02.960 --> 01:41:08.880]   - It's a very informal, it's literally a buddy kind of bet.
[01:41:08.880 --> 01:41:14.320]   The fact that we can do this kind of bets,
[01:41:14.320 --> 01:41:16.480]   even though we understand the code,
[01:41:16.480 --> 01:41:19.280]   it just goes to show how often,
[01:41:19.280 --> 01:41:21.600]   "Oh, wait, this didn't go to what we predicted."
[01:41:21.600 --> 01:41:26.000]   And that's why, even if, let's say,
[01:41:26.000 --> 01:41:29.040]   you don't have a PhD or so on and so forth,
[01:41:29.040 --> 01:41:31.680]   even if math is not your specialization,
[01:41:31.680 --> 01:41:32.960]   you're coming in as a developer,
[01:41:32.960 --> 01:41:34.800]   I'm going to come in, I'm going to say frankly,
[01:41:34.800 --> 01:41:36.320]   like, I didn't come from the research right now,
[01:41:36.320 --> 01:41:40.320]   the extremely math-heavy stuff is what I struggle with.
[01:41:40.320 --> 01:41:45.920]   What I do sometimes is I copy and paste the math into GPT-4
[01:41:45.920 --> 01:41:47.280]   and ask it to explain to me.
[01:41:47.280 --> 01:41:49.520]   - Which is good, in plain old language.
[01:41:49.520 --> 01:41:50.240]   - It's very good at that.
[01:41:50.240 --> 01:41:56.080]   But the thing is, there is lots of value beyond that.
[01:41:56.080 --> 01:41:58.160]   One thing that I realized,
[01:41:58.160 --> 01:41:59.920]   and this is not specific to RWKV,
[01:42:01.120 --> 01:42:04.880]   this also happens across a lot of open source models,
[01:42:04.880 --> 01:42:08.400]   is that a lot of ML scientists,
[01:42:08.400 --> 01:42:10.080]   when they really build this stuff,
[01:42:10.080 --> 01:42:12.240]   the focus was more of like, "Oh, let's get it to work."
[01:42:12.240 --> 01:42:16.160]   It was never about getting it to work efficiently
[01:42:16.160 --> 01:42:18.720]   or getting the code documented or organized.
[01:42:18.720 --> 01:42:22.160]   And Stable Diffusion literally went through this whole journey.
[01:42:22.160 --> 01:42:25.360]   They had the code and the model that worked,
[01:42:25.360 --> 01:42:27.680]   and the community just started,
[01:42:27.760 --> 01:42:32.080]   and engineers that came in with zero machine learning background
[01:42:32.080 --> 01:42:34.000]   started picking it apart.
[01:42:34.000 --> 01:42:36.320]   It's like, "No, you should replace this with this
[01:42:36.320 --> 01:42:38.960]   that does the exact same thing, but it's more efficient."
[01:42:38.960 --> 01:42:43.760]   One of the major breakthroughs, for example, for GML,
[01:42:43.760 --> 01:42:49.680]   and this happened sometime back for the Lama models,
[01:42:49.680 --> 01:42:53.680]   was that someone external from the AI committee
[01:42:53.680 --> 01:42:55.440]   went in and implemented memory mapping.
[01:42:55.440 --> 01:42:56.800]   - Yes, I saw that.
[01:42:57.680 --> 01:43:02.160]   I forget her name, but yeah, Justine Dot Law is her URL.
[01:43:02.160 --> 01:43:05.600]   - Yeah, and she didn't come in as an AI expert.
[01:43:05.600 --> 01:43:07.440]   She came in as a software engineer.
[01:43:07.440 --> 01:43:10.560]   - Yeah, these are all just very, very straightforward.
[01:43:10.560 --> 01:43:13.840]   In her world, this is normal,
[01:43:13.840 --> 01:43:15.680]   whereas for the researchers, they will be like,
[01:43:15.680 --> 01:43:16.160]   "I don't know."
[01:43:16.160 --> 01:43:17.680]   - "Wait, what is memory mapping?"
[01:43:17.680 --> 01:43:18.480]   - Yeah, exactly.
[01:43:18.480 --> 01:43:19.920]   - Yeah, and there are a lot of things.
[01:43:19.920 --> 01:43:22.560]   One of the jokes that I have right now is that
[01:43:23.840 --> 01:43:27.200]   every month, there is a research ML scientist
[01:43:27.200 --> 01:43:29.600]   that's rediscovering the number 32.
[01:43:29.600 --> 01:43:30.880]   - Why?
[01:43:30.880 --> 01:43:34.320]   - Because, be it like someone in the committee
[01:43:34.320 --> 01:43:35.520]   writing the inference code,
[01:43:35.520 --> 01:43:41.440]   because GPUs, especially Nvidia GPUs,
[01:43:41.440 --> 01:43:42.640]   tends to work really well
[01:43:42.640 --> 01:43:45.600]   when they align to the batch size of multiples of 32.
[01:43:45.600 --> 01:43:49.120]   And if you've been in the gaming industry,
[01:43:49.120 --> 01:43:51.280]   especially when you write shader code,
[01:43:51.280 --> 01:43:55.040]   this is well-known, just given knowledge.
[01:43:55.040 --> 01:43:58.720]   And people are just constantly rediscovering,
[01:43:58.720 --> 01:44:00.880]   "Oh, maybe if I just adjust my data set
[01:44:00.880 --> 01:44:04.080]   "or my data size to fit this batch size,
[01:44:04.080 --> 01:44:06.480]   "suddenly I get 10% improvement."
[01:44:06.480 --> 01:44:12.720]   And yeah, these are things that, once again,
[01:44:12.720 --> 01:44:14.880]   because they were so focused on just making it work,
[01:44:14.880 --> 01:44:17.520]   that they won't know outside the space.
[01:44:17.520 --> 01:44:20.000]   And that's why I would say, if anything,
[01:44:20.720 --> 01:44:24.400]   now is the best time that you don't know AI
[01:44:24.400 --> 01:44:26.640]   to have people from different backgrounds come in,
[01:44:26.640 --> 01:44:29.040]   because your contribution could be from data set level,
[01:44:29.040 --> 01:44:32.160]   how to train the knowledge, to shader code,
[01:44:32.160 --> 01:44:36.880]   to hack, how to memory map, how to cache data.
[01:44:36.880 --> 01:44:38.000]   There's so many gaps.
[01:44:38.000 --> 01:44:41.360]   - Building the UI, I saw that you guys have a UI as well,
[01:44:41.360 --> 01:44:44.000]   or maybe it's not maintained, I don't know.
[01:44:44.000 --> 01:44:46.320]   - No, yeah, there's someone in the community, yeah.
[01:44:46.320 --> 01:44:48.640]   - Yeah, cool, so many ways.
[01:44:48.640 --> 01:44:50.640]   - Yeah, it's very encouraging and good to know.
[01:44:51.200 --> 01:44:52.480]   And then I think the last thing,
[01:44:52.480 --> 01:44:56.640]   I left this to the end because it's kind of uncomfortable,
[01:44:56.640 --> 01:44:59.200]   but also just fun bonus,
[01:44:59.200 --> 01:45:03.200]   which is I'm really trying to do an AI Waifu episode.
[01:45:03.200 --> 01:45:06.560]   I think that, at least in the open source model space,
[01:45:06.560 --> 01:45:11.760]   the most motivated and surprisingly competent people
[01:45:11.760 --> 01:45:13.600]   are the people trying to build AI Girlfriend.
[01:45:13.600 --> 01:45:17.120]   And you are one of the few people I've actually met
[01:45:17.120 --> 01:45:19.360]   who interact with these people, right?
[01:45:19.360 --> 01:45:22.800]   Like they are just, what are you seeing?
[01:45:22.800 --> 01:45:23.600]   What's interesting?
[01:45:23.600 --> 01:45:25.680]   Like, and there's, apart from RWEKB,
[01:45:25.680 --> 01:45:27.120]   there's also other communities, right?
[01:45:27.120 --> 01:45:30.320]   The Uncensored Models, I think Wizard LM is part of that.
[01:45:30.320 --> 01:45:30.880]   - Correct.
[01:45:30.880 --> 01:45:32.960]   - Just like, can you sketch out
[01:45:32.960 --> 01:45:34.800]   what is happening in that world?
[01:45:34.800 --> 01:45:39.440]   - So, I mean, Creative Record, you're right.
[01:45:39.440 --> 01:45:45.840]   We shouldn't be king-shaming or anything on that.
[01:45:45.840 --> 01:45:48.960]   And these are some of the most motivated
[01:45:48.960 --> 01:45:51.680]   and sometimes even the most technical competent people
[01:45:51.680 --> 01:45:54.560]   that literally move mountains in the code base.
[01:45:54.560 --> 01:45:57.280]   And I don't mean that lightly.
[01:45:57.280 --> 01:46:03.840]   It's like, I think those active in the RWEKB discord,
[01:46:03.840 --> 01:46:05.760]   we're no working members that literally
[01:46:05.760 --> 01:46:06.880]   just came in out of nowhere.
[01:46:06.880 --> 01:46:10.960]   And it's like, okay, let's just rewrite the whole
[01:46:10.960 --> 01:46:13.440]   how CPP and GGML code does work.
[01:46:13.440 --> 01:46:16.240]   And great, it's way faster.
[01:46:17.360 --> 01:46:23.600]   And there's a lot of them, their motivations
[01:46:23.600 --> 01:46:26.560]   is still very inherently is that they are very,
[01:46:26.560 --> 01:46:31.040]   I guess it's the fastest feedback loop from code.
[01:46:31.040 --> 01:46:32.640]   - They are the users.
[01:46:32.640 --> 01:46:35.040]   - To the user, yes, exactly.
[01:46:35.040 --> 01:46:39.040]   And they want the model to align better.
[01:46:39.040 --> 01:46:42.240]   So, and the thing is getting an AI waifu
[01:46:42.240 --> 01:46:44.720]   actually spreads the whole freaking domain.
[01:46:44.720 --> 01:46:45.440]   - Why?
[01:46:45.760 --> 01:46:51.040]   - Because from the very top, from the very bottom,
[01:46:51.040 --> 01:46:52.800]   it will be like, let's say the model architecture.
[01:46:52.800 --> 01:46:55.280]   So let's say if the model architecture has issues
[01:46:55.280 --> 01:46:58.080]   paying attention to historical conversations,
[01:46:58.080 --> 01:47:01.920]   for example, you can have long conversations
[01:47:01.920 --> 01:47:04.000]   and then the model will just forget stuff.
[01:47:04.000 --> 01:47:07.040]   Yes, not ideal, let's say.
[01:47:07.040 --> 01:47:10.080]   All the way to the very top would be like,
[01:47:10.080 --> 01:47:13.440]   like you want your model to stay in character,
[01:47:13.440 --> 01:47:14.960]   your system prompts.
[01:47:14.960 --> 01:47:17.200]   This is literally alignment problems,
[01:47:17.200 --> 01:47:19.440]   but the alignment is not to an ethical standard,
[01:47:19.440 --> 01:47:21.440]   the alignment is to stay in character.
[01:47:21.440 --> 01:47:26.640]   And that includes doing things that makes no sense.
[01:47:26.640 --> 01:47:29.600]   Like let's just say you take one of your favorite,
[01:47:29.600 --> 01:47:33.680]   what's the character for this?
[01:47:33.680 --> 01:47:40.560]   The silly scientist or silly airhead girl.
[01:47:40.560 --> 01:47:43.760]   I think the American equivalent would be dumb blonde.
[01:47:43.760 --> 01:47:45.120]   - Yeah, a bit of both.
[01:47:45.120 --> 01:47:46.320]   - I'm sorry if I offended you.
[01:47:46.320 --> 01:47:55.680]   And the idea there is that the characters may make,
[01:47:55.680 --> 01:47:59.360]   as in character will make some very silly mistakes
[01:47:59.360 --> 01:48:01.760]   and you want to align your model that way.
[01:48:01.760 --> 01:48:02.880]   So there's alignment.
[01:48:02.880 --> 01:48:05.520]   - So, okay, what are people doing to solve that?
[01:48:05.520 --> 01:48:08.000]   Just in case you've seen anything interesting.
[01:48:08.000 --> 01:48:11.920]   For example, the Dan prompt to me was very interesting.
[01:48:11.920 --> 01:48:13.520]   Like give people points and then deduct points
[01:48:13.520 --> 01:48:16.960]   and like it's trained to be very scared of losing points.
[01:48:16.960 --> 01:48:17.520]   - Correct.
[01:48:17.520 --> 01:48:22.880]   So from that, it's really more of like prompt training methods.
[01:48:22.880 --> 01:48:25.920]   - Which makes it slower.
[01:48:25.920 --> 01:48:27.120]   - Which makes it slower.
[01:48:27.120 --> 01:48:28.880]   And then so it keeps going back and forth the chain.
[01:48:28.880 --> 01:48:31.920]   So you see, they adjust the prompt, then it's too slow.
[01:48:31.920 --> 01:48:33.040]   Then they want to optimize it.
[01:48:33.040 --> 01:48:35.760]   Then they look into how to train better data sets,
[01:48:35.760 --> 01:48:39.280]   including their favorite character stories
[01:48:39.280 --> 01:48:42.800]   from whatever sources they can get.
[01:48:43.360 --> 01:48:45.760]   Because one of the existing problems for AI models,
[01:48:45.760 --> 01:48:47.520]   even from the foundation model, right?
[01:48:47.520 --> 01:48:50.800]   Is that even though it can partially impersonate a character,
[01:48:50.800 --> 01:48:56.640]   if you ask a real fan, in a lot of cases, it falls flat.
[01:48:56.640 --> 01:48:59.440]   Because what's happening is it's reading summaries
[01:48:59.440 --> 01:49:04.080]   and quotes and memes and impersonating at a very high level.
[01:49:04.080 --> 01:49:07.520]   But it's not impersonating on a very deep level.
[01:49:07.520 --> 01:49:11.520]   And that's where people start exploring the data set.
[01:49:11.520 --> 01:49:15.040]   And because these members are also the same members
[01:49:15.040 --> 01:49:16.880]   that do not have a giant GPU farm,
[01:49:16.880 --> 01:49:19.280]   they are very interested in optimizing it,
[01:49:19.280 --> 01:49:22.160]   be it through LoRa or fine tuning.
[01:49:22.160 --> 01:49:23.920]   It's like, what's the best learning rate?
[01:49:23.920 --> 01:49:27.920]   What's the best way to fine tune this limited GPU resource
[01:49:27.920 --> 01:49:30.080]   for the benefit of all people?
[01:49:30.080 --> 01:49:33.200]   - Are the LoRa techniques and whatever else,
[01:49:33.200 --> 01:49:34.640]   are they applicable to RWKB?
[01:49:34.640 --> 01:49:37.840]   - Yeah, RWKB does have a LoRa trainer as well.
[01:49:37.840 --> 01:49:41.600]   - Okay, and that's relatively commonplace now.
[01:49:41.600 --> 01:49:42.240]   Everyone has it.
[01:49:42.240 --> 01:49:44.160]   - Yeah, I think pretty much every open source model
[01:49:44.160 --> 01:49:45.040]   has a LoRa trainer.
[01:49:45.040 --> 01:49:48.320]   - I will say I've actually struggled to find,
[01:49:48.320 --> 01:49:50.000]   like LoRa seems to be very common
[01:49:50.000 --> 01:49:51.840]   in the stable diffusion community.
[01:49:51.840 --> 01:49:53.760]   But in text models,
[01:49:53.760 --> 01:49:57.040]   I haven't really seen that much adoption in my circles.
[01:49:57.040 --> 01:49:58.240]   But I think maybe you've seen...
[01:49:58.240 --> 01:50:00.800]   - I guess the problem is that LoRa has...
[01:50:00.800 --> 01:50:04.560]   Okay, so I think stable diffusion LoRa
[01:50:04.560 --> 01:50:06.880]   is a lot more powerful,
[01:50:06.880 --> 01:50:10.080]   as in I find it hard to come up with a use case
[01:50:10.080 --> 01:50:11.280]   that LoRa cannot support.
[01:50:11.280 --> 01:50:19.360]   But for example, in the language models case,
[01:50:19.360 --> 01:50:20.880]   LoRa cannot teach new language.
[01:50:20.880 --> 01:50:28.960]   It sometimes may struggle to teach new techniques
[01:50:28.960 --> 01:50:30.960]   or new concepts.
[01:50:32.000 --> 01:50:36.480]   It does well into adding and refining existing knowledge.
[01:50:36.480 --> 01:50:40.480]   And this is the part where
[01:50:40.480 --> 01:50:42.080]   how do we know whether it works or doesn't?
[01:50:42.080 --> 01:50:44.560]   We don't really know because the line is very gray.
[01:50:44.560 --> 01:50:46.160]   And I think that frustrates a lot of people
[01:50:46.160 --> 01:50:50.080]   when they're using LoRa for professional use
[01:50:50.080 --> 01:50:52.160]   because you can end up doing LoRa
[01:50:52.160 --> 01:50:54.080]   in 4A to 4B completely.
[01:50:54.080 --> 01:50:58.800]   But this is where, back to the character AI community,
[01:50:58.800 --> 01:51:01.840]   it's actually very suited for that use case
[01:51:01.840 --> 01:51:03.840]   because if your character's popular enough,
[01:51:03.840 --> 01:51:05.840]   there is some base data in there
[01:51:05.840 --> 01:51:08.560]   and you're just effectively fine-tuning
[01:51:08.560 --> 01:51:10.560]   the speech patterns and the data from there.
[01:51:10.560 --> 01:51:11.280]   - Yeah.
[01:51:11.280 --> 01:51:12.720]   So I'll call out...
[01:51:12.720 --> 01:51:14.240]   So I think you say character AI,
[01:51:14.240 --> 01:51:16.480]   but you don't actually mean the company character AI.
[01:51:16.480 --> 01:51:17.600]   - Oh yeah, sorry about that.
[01:51:17.600 --> 01:51:19.840]   - It's the companies that are like them,
[01:51:19.840 --> 01:51:22.640]   but sex-positive, I should say.
[01:51:22.640 --> 01:51:23.440]   - Okay, yeah.
[01:51:23.440 --> 01:51:24.080]   - Whatever.
[01:51:24.080 --> 01:51:25.760]   So there's character AI, there's replica.
[01:51:25.760 --> 01:51:27.280]   These are the two tread...
[01:51:27.280 --> 01:51:29.120]   I would call them tread in terms of
[01:51:30.240 --> 01:51:32.240]   they are in the common consciousness
[01:51:32.240 --> 01:51:34.480]   in at least in traditional AI circles.
[01:51:34.480 --> 01:51:35.200]   - Yeah.
[01:51:35.200 --> 01:51:36.320]   - And then, for example,
[01:51:36.320 --> 01:51:39.360]   I recently came across venus.chub,
[01:51:39.360 --> 01:51:41.440]   which, yes, it's one of those.
[01:51:41.440 --> 01:51:44.240]   But like 2 million users in one week.
[01:51:44.240 --> 01:51:46.720]   That's the number that I got.
[01:51:46.720 --> 01:51:49.200]   Crazy, like just huge.
[01:51:49.200 --> 01:51:50.960]   - Yeah, and then there's...
[01:51:50.960 --> 01:51:52.720]   I think there's also a lot of it,
[01:51:52.720 --> 01:51:54.800]   especially when it comes to specific domains.
[01:51:54.800 --> 01:51:55.280]   - Yeah.
[01:51:55.280 --> 01:51:56.960]   - Like be it enemy...
[01:51:56.960 --> 01:51:57.920]   - Furries.
[01:51:58.720 --> 01:52:00.320]   - These are all like...
[01:52:00.320 --> 01:52:03.040]   Look, I mean, this is all the full range.
[01:52:03.040 --> 01:52:05.600]   You want to simulate humanity, there you go.
[01:52:05.600 --> 01:52:06.320]   - Fair enough.
[01:52:06.320 --> 01:52:07.920]   - A lot of times it's about sex.
[01:52:07.920 --> 01:52:08.480]   - Yeah.
[01:52:08.480 --> 01:52:11.840]   - Okay, so I don't know if you have anything else.
[01:52:11.840 --> 01:52:14.000]   I'll mention like one other piece
[01:52:14.000 --> 01:52:15.280]   of why I'm interested in this
[01:52:15.280 --> 01:52:19.360]   is because if these people could be...
[01:52:19.360 --> 01:52:22.000]   Actually, honestly, they're the pioneers
[01:52:22.000 --> 01:52:25.920]   in terms of modeling what a human is.
[01:52:25.920 --> 01:52:31.440]   And we actually end up figuring out
[01:52:31.440 --> 01:52:36.400]   how to encode a human personality and identity.
[01:52:36.400 --> 01:52:38.480]   And we might actually end up...
[01:52:38.480 --> 01:52:40.080]   Like this weird path that we're taking
[01:52:40.080 --> 01:52:42.240]   might actually end up in mind uploading,
[01:52:42.240 --> 01:52:43.200]   which is what I'm thinking about.
[01:52:43.200 --> 01:52:44.320]   - I don't think...
[01:52:44.320 --> 01:52:47.120]   Yeah, I think that makes sense in many ways
[01:52:47.120 --> 01:52:49.760]   because they're also the most nitpicky about it.
[01:52:49.760 --> 01:52:50.160]   - Yeah.
[01:52:50.160 --> 01:52:52.160]   - It's like they can tell
[01:52:52.160 --> 01:52:53.600]   when a character is a hot character.
[01:52:54.000 --> 01:52:56.080]   (both laughing)
[01:52:56.080 --> 01:52:58.800]   - Yeah, and they're doing it
[01:52:58.800 --> 01:53:00.320]   without access to the full information.
[01:53:00.320 --> 01:53:03.280]   But I do think that this is a real path
[01:53:03.280 --> 01:53:06.400]   towards immortality in some form.
[01:53:06.400 --> 01:53:08.640]   And I think there will be people
[01:53:08.640 --> 01:53:09.680]   interested in mind upload
[01:53:09.680 --> 01:53:10.880]   and it will come from this community
[01:53:10.880 --> 01:53:12.640]   because no one else is working as hard
[01:53:12.640 --> 01:53:15.520]   on essentially serialization of a person.
[01:53:15.520 --> 01:53:17.840]   - I think there are two variants for it.
[01:53:17.840 --> 01:53:22.240]   I think one is the one that Facebook is attempting,
[01:53:23.120 --> 01:53:25.120]   which is I have all the data on you.
[01:53:25.120 --> 01:53:29.680]   And same thing, I have all data on this character.
[01:53:29.680 --> 01:53:34.000]   And now you have a virtual half, per se.
[01:53:34.000 --> 01:53:37.360]   And when you deceased,
[01:53:37.360 --> 01:53:38.800]   whoever's left can interact with that.
[01:53:38.800 --> 01:53:42.320]   I think that's slightly different from mind upload.
[01:53:42.320 --> 01:53:44.000]   But then subsequently,
[01:53:44.000 --> 01:53:46.000]   I think then the next jump would be...
[01:53:46.000 --> 01:53:48.400]   But that could be like the building block
[01:53:48.400 --> 01:53:49.520]   to the next major jump,
[01:53:49.520 --> 01:53:52.640]   which is like really scanning your brain
[01:53:52.640 --> 01:53:54.720]   and then figuring out how all this connects.
[01:53:54.720 --> 01:53:58.480]   - And sequence your DNA, do whatever.
[01:53:58.480 --> 01:54:03.120]   - This is like a completely wild engine.
[01:54:03.120 --> 01:54:08.400]   I sometimes think that we overestimate
[01:54:08.400 --> 01:54:10.400]   how far we are.
[01:54:10.400 --> 01:54:13.680]   Because in my opinion,
[01:54:13.680 --> 01:54:17.360]   and this is for me in particular
[01:54:17.360 --> 01:54:18.480]   with the stable diffusion model,
[01:54:18.560 --> 01:54:24.240]   is that if I can get the world image model effectively,
[01:54:24.240 --> 01:54:25.920]   I mean, stable diffusion, whatever,
[01:54:25.920 --> 01:54:28.080]   in under 100 gigabytes.
[01:54:28.080 --> 01:54:31.040]   And now I have all the world knowledge
[01:54:31.040 --> 01:54:33.200]   literally in a transformer
[01:54:33.200 --> 01:54:34.560]   that's less than 100 gigabytes.
[01:54:34.560 --> 01:54:37.680]   No offense to myself,
[01:54:37.680 --> 01:54:40.000]   I don't think my personality and my memories
[01:54:40.000 --> 01:54:41.200]   is more than this.
[01:54:41.200 --> 01:54:44.320]   Even if I 10x it,
[01:54:44.320 --> 01:54:46.000]   I could store this in two SSDs,
[01:54:46.000 --> 01:54:48.080]   two hard drives.
[01:54:48.240 --> 01:54:48.740]   - Yeah.
[01:54:48.740 --> 01:54:53.760]   - And if we really break it down
[01:54:53.760 --> 01:54:55.120]   how to serialize it and handle it,
[01:54:55.120 --> 01:54:58.560]   perhaps we are actually not as big as we think we are.
[01:54:58.560 --> 01:54:58.960]   - Yeah, yeah.
[01:54:58.960 --> 01:55:00.560]   - Because our brains are actually handling
[01:55:00.560 --> 01:55:02.080]   a crap ton of other functions
[01:55:02.080 --> 01:55:04.800]   and this is like a tangent to the biological side.
[01:55:04.800 --> 01:55:07.840]   Yeah, your whole body.
[01:55:07.840 --> 01:55:09.840]   - Your breathing, your pumping blood.
[01:55:09.840 --> 01:55:11.760]   - Your movement, that actually takes up a lot.
[01:55:11.760 --> 01:55:14.880]   And if you really want to strip it down
[01:55:14.880 --> 01:55:17.040]   to like just pure text and vision,
[01:55:17.040 --> 01:55:18.880]   because since now if you upload your mind,
[01:55:18.880 --> 01:55:20.640]   you no longer need the rest of that,
[01:55:20.640 --> 01:55:22.800]   perhaps we may find out
[01:55:22.800 --> 01:55:24.240]   that it's actually a lot less than we think.
[01:55:24.240 --> 01:55:26.800]   - Yeah, so George Hartz was on our podcast
[01:55:26.800 --> 01:55:27.920]   and he said two gigs.
[01:55:27.920 --> 01:55:28.960]   - Two gigs.
[01:55:28.960 --> 01:55:31.360]   - He wants to quantize himself,
[01:55:31.360 --> 01:55:32.160]   which I'm like,
[01:55:32.160 --> 01:55:34.640]   I think you'll lose something if you quantize yourself, but.
[01:55:34.640 --> 01:55:37.200]   - I won't push so far to do it.
[01:55:37.200 --> 01:55:38.720]   I'm still waiting a terabyte really,
[01:55:38.720 --> 01:55:40.480]   because frankly, that's all we need.
[01:55:40.480 --> 01:55:41.680]   - That's all we need, that's all we need.
[01:55:41.680 --> 01:55:43.520]   Cool, great.
[01:55:44.080 --> 01:55:46.880]   So yeah, thanks so much for being very willing
[01:55:46.880 --> 01:55:48.880]   to get on and talk with No Prep.
[01:55:48.880 --> 01:55:50.080]   Well, we did some prep,
[01:55:50.080 --> 01:55:52.880]   but it's very unusual podcast episode,
[01:55:52.880 --> 01:55:53.920]   but I really enjoyed it.
[01:55:53.920 --> 01:55:56.080]   - We literally just met yesterday in Singapore.
[01:55:56.080 --> 01:55:57.840]   - But I know you've been on the Discord for a while
[01:55:57.840 --> 01:56:01.200]   and I can tell you like you're very serious about all this.
[01:56:01.200 --> 01:56:03.200]   I think it's very unusual for someone,
[01:56:03.200 --> 01:56:04.560]   like you have a job,
[01:56:04.560 --> 01:56:06.640]   but this is like a second job essentially.
[01:56:06.640 --> 01:56:07.360]   - Yes.
[01:56:07.360 --> 01:56:11.840]   - But you are really enthusiastic and passionate about it
[01:56:11.840 --> 01:56:12.960]   and I think that's very rare
[01:56:12.960 --> 01:56:15.840]   and I don't want to encourage more people to do it.
[01:56:15.840 --> 01:56:17.120]   And so thanks for sharing.
[01:56:17.120 --> 01:56:20.320]   - Yeah, I'm glad for having me here on a very last minute basis.
[01:56:20.320 --> 01:56:22.160]   Like we did not book this room.
[01:56:22.160 --> 01:56:23.600]   - There's no room.
[01:56:23.600 --> 01:56:28.000]   - We are literally gorilla podcasting in some corner.
[01:56:28.000 --> 01:56:30.240]   So if you see random intermissions and cut, right,
[01:56:30.240 --> 01:56:31.840]   that was because a crowd just went by
[01:56:31.840 --> 01:56:33.440]   and there was noise and we needed more.
[01:56:33.440 --> 01:56:34.560]   - Aunties had to go for lunch.
[01:56:34.560 --> 01:56:37.200]   But no, I think it's actually a bit charming.
[01:56:37.200 --> 01:56:41.120]   You know, I think some podcasts can be too polished
[01:56:41.120 --> 01:56:42.560]   and sometimes it's just nice to see like,
[01:56:42.560 --> 01:56:43.440]   "Hey, it's just two guys."
[01:56:43.440 --> 01:56:44.000]   - Oh yeah.
[01:56:44.000 --> 01:56:44.960]   - Yeah, it's all of this.
[01:56:44.960 --> 01:56:46.400]   Cool, thanks.
[01:56:46.400 --> 01:56:47.520]   - Thanks for having me here.

