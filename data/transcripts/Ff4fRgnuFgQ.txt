
[00:00:00.000 --> 00:00:03.440]   The following is a conversation with Mark Zuckerberg,
[00:00:03.440 --> 00:00:05.400]   his second time on this podcast.
[00:00:05.400 --> 00:00:08.440]   He's the CEO of Meta that owns Facebook,
[00:00:08.440 --> 00:00:10.000]   Instagram, and WhatsApp,
[00:00:10.000 --> 00:00:13.040]   all services used by billions of people
[00:00:13.040 --> 00:00:14.840]   to connect with each other.
[00:00:14.840 --> 00:00:17.400]   We talk about his vision for the future of Meta
[00:00:17.400 --> 00:00:21.340]   and the future of AI in our human world.
[00:00:21.340 --> 00:00:23.520]   This is the Lex Friedman Podcast,
[00:00:23.520 --> 00:00:27.160]   and now, dear friends, here's Mark Zuckerberg.
[00:00:28.280 --> 00:00:30.800]   So you competed in your first Jiu-Jitsu tournament,
[00:00:30.800 --> 00:00:34.320]   and me, as a fellow Jiu-Jitsu practitioner and competitor,
[00:00:34.320 --> 00:00:35.840]   I think that's really inspiring,
[00:00:35.840 --> 00:00:37.560]   given all the things you have going on.
[00:00:37.560 --> 00:00:40.840]   So I gotta ask, what was that experience like?
[00:00:40.840 --> 00:00:42.360]   - Oh, it was fun.
[00:00:42.360 --> 00:00:43.880]   I don't know, yeah, I mean, well, look,
[00:00:43.880 --> 00:00:45.760]   I'm a pretty competitive person.
[00:00:45.760 --> 00:00:47.160]   - Yeah?
[00:00:47.160 --> 00:00:50.320]   - Doing sports that basically require your full attention,
[00:00:50.320 --> 00:00:53.680]   I think is really important to my mental health
[00:00:53.680 --> 00:00:55.560]   and the way I just stay focused
[00:00:55.560 --> 00:00:56.960]   at doing everything I'm doing.
[00:00:56.960 --> 00:00:58.920]   So I decided to get into martial arts,
[00:00:58.920 --> 00:01:00.680]   and it's awesome.
[00:01:00.680 --> 00:01:02.200]   I got a ton of my friends into it.
[00:01:02.200 --> 00:01:04.000]   We all train together.
[00:01:04.000 --> 00:01:06.720]   We have a mini academy in my garage.
[00:01:06.720 --> 00:01:10.440]   And I guess one of my friends was like,
[00:01:10.440 --> 00:01:12.680]   "Hey, we should go do a tournament."
[00:01:12.680 --> 00:01:14.040]   I was like, "Okay, yeah, let's do it.
[00:01:14.040 --> 00:01:16.560]   "I'm not gonna shy away from a challenge like that."
[00:01:16.560 --> 00:01:18.440]   So yeah, but it was awesome.
[00:01:18.440 --> 00:01:19.720]   It was just a lot of fun.
[00:01:19.720 --> 00:01:20.560]   - You weren't scared?
[00:01:20.560 --> 00:01:21.600]   There was no fear?
[00:01:21.600 --> 00:01:22.440]   - I don't know.
[00:01:22.440 --> 00:01:25.160]   I was pretty sure that I'd do okay.
[00:01:25.160 --> 00:01:26.760]   - I like the confidence.
[00:01:26.760 --> 00:01:28.520]   Well, so for people who don't know,
[00:01:28.520 --> 00:01:30.240]   jiu-jitsu is a martial art
[00:01:30.240 --> 00:01:33.200]   where you're trying to break your opponent's limbs
[00:01:33.200 --> 00:01:36.320]   or choke them to sleep
[00:01:36.320 --> 00:01:41.160]   and do so with grace and elegance and efficiency
[00:01:41.160 --> 00:01:43.320]   and all that kind of stuff.
[00:01:43.320 --> 00:01:45.000]   It's a kind of art form, I think,
[00:01:45.000 --> 00:01:46.240]   that you can do for your whole life.
[00:01:46.240 --> 00:01:49.440]   And it's basically a game, a sport of human chess
[00:01:49.440 --> 00:01:50.360]   you can think of.
[00:01:50.360 --> 00:01:51.440]   There's a lot of strategy.
[00:01:51.440 --> 00:01:54.280]   There's a lot of interesting human dynamics
[00:01:54.280 --> 00:01:56.520]   of using leverage and all that kind of stuff.
[00:01:57.280 --> 00:01:59.320]   It's kind of incredible what you could do.
[00:01:59.320 --> 00:02:01.280]   You can do things like a small opponent
[00:02:01.280 --> 00:02:03.160]   could defeat a much larger opponent
[00:02:03.160 --> 00:02:05.280]   and you get to understand the way the mechanics
[00:02:05.280 --> 00:02:07.200]   of the human body works because of that.
[00:02:07.200 --> 00:02:09.760]   But you certainly can't be distracted.
[00:02:09.760 --> 00:02:10.600]   - No.
[00:02:10.600 --> 00:02:13.120]   It's 100% focus.
[00:02:13.120 --> 00:02:15.920]   To compete, I needed to get around the fact
[00:02:15.920 --> 00:02:18.520]   that I didn't want it to be this big thing.
[00:02:18.520 --> 00:02:23.160]   So basically, I rolled up with a hat and sunglasses
[00:02:23.160 --> 00:02:24.920]   and I was wearing a COVID mask
[00:02:24.920 --> 00:02:27.080]   and I registered under my first and middle name,
[00:02:27.080 --> 00:02:28.480]   so Mark Elliott.
[00:02:28.480 --> 00:02:31.180]   And it wasn't until I actually pulled all that stuff off
[00:02:31.180 --> 00:02:32.160]   right before I got on the mat
[00:02:32.160 --> 00:02:33.600]   that I think people knew it was me.
[00:02:33.600 --> 00:02:35.320]   So it was pretty low key.
[00:02:35.320 --> 00:02:37.740]   - But you're still a public figure.
[00:02:37.740 --> 00:02:39.120]   - Yeah, I mean, I didn't wanna lose.
[00:02:39.120 --> 00:02:39.960]   - Right.
[00:02:39.960 --> 00:02:41.720]   The thing you're partially afraid of
[00:02:41.720 --> 00:02:44.620]   is not just the losing but being almost embarrassed.
[00:02:44.620 --> 00:02:47.280]   It's so raw, the sport, in that it's just you
[00:02:47.280 --> 00:02:48.280]   and another human being.
[00:02:48.280 --> 00:02:49.720]   There's a primal aspect there.
[00:02:49.720 --> 00:02:50.720]   - Oh yeah, it's great.
[00:02:50.720 --> 00:02:52.080]   - For a lot of people, it can be terrifying,
[00:02:52.080 --> 00:02:54.560]   especially the first time you're doing the competing
[00:02:54.560 --> 00:02:56.040]   and it wasn't for you.
[00:02:56.040 --> 00:02:57.640]   I see the look of excitement on your face.
[00:02:57.640 --> 00:02:58.480]   - Yeah, I don't know.
[00:02:58.480 --> 00:02:59.300]   - It wasn't, no fear.
[00:02:59.300 --> 00:03:01.720]   - I just think part of learning is failing.
[00:03:01.720 --> 00:03:02.560]   - Okay.
[00:03:02.560 --> 00:03:04.320]   - Right, so, I mean, the main thing,
[00:03:04.320 --> 00:03:06.760]   people who train jujitsu, it's like,
[00:03:06.760 --> 00:03:08.880]   you need to not have pride because,
[00:03:08.880 --> 00:03:10.400]   I mean, all the stuff that you were talking about before
[00:03:10.400 --> 00:03:14.840]   about getting choked or getting a joint lock,
[00:03:14.840 --> 00:03:18.400]   you only get into a bad situation
[00:03:18.400 --> 00:03:21.160]   if you're not willing to tap once you've already lost.
[00:03:22.660 --> 00:03:24.580]   But obviously, when you're getting started with something,
[00:03:24.580 --> 00:03:26.500]   you're not gonna be an expert at it immediately.
[00:03:26.500 --> 00:03:29.020]   So you just need to be willing to go with that.
[00:03:29.020 --> 00:03:30.980]   But I think this is like, I don't know,
[00:03:30.980 --> 00:03:32.340]   I mean, maybe I've just been embarrassed
[00:03:32.340 --> 00:03:33.620]   enough times in my life.
[00:03:33.620 --> 00:03:34.460]   - Yeah.
[00:03:34.460 --> 00:03:36.420]   - I do think that there's a thing where,
[00:03:36.420 --> 00:03:39.020]   as people grow up, maybe they don't wanna be embarrassed
[00:03:39.020 --> 00:03:40.900]   or anything, they've built their adult identity
[00:03:40.900 --> 00:03:45.540]   and they kind of have a sense of who they are
[00:03:45.540 --> 00:03:46.820]   and what they wanna project.
[00:03:46.820 --> 00:03:49.420]   And I don't know, I think maybe to some degree,
[00:03:51.620 --> 00:03:54.620]   your ability to keep doing interesting things
[00:03:54.620 --> 00:03:58.220]   is your willingness to be embarrassed again
[00:03:58.220 --> 00:04:01.740]   and go back to step one and start as a beginner
[00:04:01.740 --> 00:04:06.740]   and get your ass kicked and look stupid doing things.
[00:04:06.740 --> 00:04:08.700]   Yeah, I think so many of the things that we're doing,
[00:04:08.700 --> 00:04:11.460]   whether it's this, I mean, this is just like
[00:04:11.460 --> 00:04:13.460]   kind of a physical part of my life,
[00:04:13.460 --> 00:04:15.620]   but at running the company,
[00:04:15.620 --> 00:04:17.820]   it's like we just take on new adventures
[00:04:17.820 --> 00:04:21.900]   and all the big things that we're doing,
[00:04:21.900 --> 00:04:25.300]   I think of as like 10 plus year missions that we're on
[00:04:25.300 --> 00:04:28.460]   where often early on, people doubt
[00:04:28.460 --> 00:04:29.420]   that we're gonna be able to do it
[00:04:29.420 --> 00:04:31.700]   and the initial work seems kind of silly
[00:04:31.700 --> 00:04:33.620]   and our whole ethos is we don't wanna wait
[00:04:33.620 --> 00:04:35.340]   until something is perfect to put it out there.
[00:04:35.340 --> 00:04:37.940]   We wanna get it out quickly and get feedback on it.
[00:04:37.940 --> 00:04:38.980]   And so I don't know, I mean,
[00:04:38.980 --> 00:04:40.460]   there's probably just something about
[00:04:40.460 --> 00:04:41.820]   how I approach things in there.
[00:04:41.820 --> 00:04:43.900]   But I just kind of think that the moment
[00:04:43.900 --> 00:04:45.700]   that you decide that you're gonna be too embarrassed
[00:04:45.700 --> 00:04:46.580]   to try something new,
[00:04:46.580 --> 00:04:48.260]   then you're not gonna learn anything anymore.
[00:04:48.260 --> 00:04:51.220]   - But like I mentioned, that fear,
[00:04:51.220 --> 00:04:52.460]   that anxiety could be there,
[00:04:52.460 --> 00:04:53.940]   could creep up every once in a while.
[00:04:53.940 --> 00:04:57.660]   Do you feel that in especially stressful moments
[00:04:57.660 --> 00:05:01.040]   sort of outside of the judgement, just at work,
[00:05:01.040 --> 00:05:05.220]   stressful moments, big decision days,
[00:05:05.220 --> 00:05:07.820]   big decision moments, how do you deal with that fear?
[00:05:07.820 --> 00:05:09.300]   How do you deal with that anxiety?
[00:05:09.300 --> 00:05:10.740]   - The thing that stresses me out the most
[00:05:10.740 --> 00:05:13.340]   is always the people challenges.
[00:05:13.340 --> 00:05:16.300]   You know, I kind of think that, you know,
[00:05:16.300 --> 00:05:19.020]   strategy questions, you know,
[00:05:19.020 --> 00:05:22.860]   I tend to have enough conviction around the values
[00:05:22.860 --> 00:05:26.540]   of what we're trying to do and what I think matters
[00:05:26.540 --> 00:05:28.260]   and what I want our company to stand for
[00:05:28.260 --> 00:05:31.900]   that those don't really keep me up at night that much.
[00:05:31.900 --> 00:05:33.220]   I mean, I kind of, you know,
[00:05:33.220 --> 00:05:35.100]   it's not that I get everything right.
[00:05:35.100 --> 00:05:36.160]   Of course I don't, right?
[00:05:36.160 --> 00:05:38.300]   I mean, we make a lot of mistakes,
[00:05:38.300 --> 00:05:43.220]   but I at least have a pretty strong sense
[00:05:43.220 --> 00:05:45.900]   of where I want us to go on that.
[00:05:45.900 --> 00:05:49.860]   The thing in running a company for almost 20 years now,
[00:05:49.860 --> 00:05:51.860]   one of the things that's been pretty clear
[00:05:51.860 --> 00:05:55.740]   is when you have a team that's cohesive,
[00:05:55.740 --> 00:05:59.180]   you can get almost anything done.
[00:05:59.180 --> 00:06:03.280]   And, you know, you can run through super hard challenges,
[00:06:03.280 --> 00:06:07.020]   you can make hard decisions and push really hard
[00:06:07.020 --> 00:06:08.820]   to do the best work even,
[00:06:08.820 --> 00:06:12.160]   you know, and kind of optimize something super well.
[00:06:12.160 --> 00:06:13.860]   But when there's that tension,
[00:06:13.860 --> 00:06:16.300]   I mean, that's when things get really tough.
[00:06:16.300 --> 00:06:18.940]   And, you know, when I talk to other friends
[00:06:18.940 --> 00:06:20.580]   who run other companies and things like that,
[00:06:20.580 --> 00:06:22.660]   I think one of the things that I actually spend
[00:06:22.660 --> 00:06:24.020]   a disproportionate amount of time on
[00:06:24.020 --> 00:06:26.740]   in running this company is just fostering
[00:06:26.740 --> 00:06:30.740]   a pretty tight core group of people
[00:06:30.740 --> 00:06:33.380]   who are running the company with me.
[00:06:33.380 --> 00:06:37.900]   And that to me is kind of the thing that
[00:06:37.900 --> 00:06:39.160]   both makes it fun, right?
[00:06:39.160 --> 00:06:41.140]   Having, you know, friends and people
[00:06:41.140 --> 00:06:42.100]   you've worked with for a while
[00:06:42.100 --> 00:06:43.540]   and new people and new perspectives,
[00:06:43.540 --> 00:06:46.100]   but like a pretty tight group who you can go work on
[00:06:46.100 --> 00:06:47.700]   some of these crazy things with.
[00:06:47.700 --> 00:06:50.740]   But to me, that's also the most stressful thing
[00:06:50.740 --> 00:06:55.740]   is when there's tension, you know, that weighs on me.
[00:06:55.740 --> 00:06:59.380]   I think the, you know, just it's maybe not surprising.
[00:06:59.380 --> 00:07:01.660]   I mean, we're like a very people-focused company
[00:07:01.660 --> 00:07:03.980]   and it's the people is the part of it
[00:07:03.980 --> 00:07:06.680]   that, you know, weighs on me the most
[00:07:06.680 --> 00:07:07.720]   to make sure that we get right.
[00:07:07.720 --> 00:07:10.420]   But yeah, that I'd say across everything that we do
[00:07:10.420 --> 00:07:12.980]   is probably the big thing.
[00:07:12.980 --> 00:07:15.740]   - So when there's tension in that inner circle
[00:07:15.740 --> 00:07:20.740]   of close folks, so when you trust those folks
[00:07:20.740 --> 00:07:24.900]   to help you make difficult decisions
[00:07:24.900 --> 00:07:29.900]   about Facebook, WhatsApp, Instagram,
[00:07:29.900 --> 00:07:33.180]   the future of the company and the metaverse with AI,
[00:07:33.180 --> 00:07:36.580]   how do you build that close-knit group of folks
[00:07:36.580 --> 00:07:39.180]   to make those difficult decisions?
[00:07:39.180 --> 00:07:42.620]   Is there people that you have to have critical voices,
[00:07:42.620 --> 00:07:46.300]   very different perspectives on focusing on the past
[00:07:46.300 --> 00:07:48.380]   versus the future, all that kind of stuff?
[00:07:48.380 --> 00:07:49.660]   - Yeah, I mean, I think for one thing,
[00:07:49.660 --> 00:07:52.380]   it's just spending a lot of time
[00:07:52.380 --> 00:07:54.220]   with whatever the group is that you wanna be
[00:07:54.220 --> 00:07:57.380]   that core group, grappling with all
[00:07:57.380 --> 00:07:58.580]   of the biggest challenges.
[00:07:58.580 --> 00:08:01.460]   And that requires a fair amount of openness.
[00:08:01.460 --> 00:08:04.780]   And, you know, so I mean, a lot of how I run the company
[00:08:04.780 --> 00:08:06.260]   is, you know, it's like every Monday morning,
[00:08:06.340 --> 00:08:10.220]   we get our, it's about the top 30 people together.
[00:08:10.220 --> 00:08:13.620]   And we, and this is a group that just worked together
[00:08:13.620 --> 00:08:14.620]   for a long period of time.
[00:08:14.620 --> 00:08:16.340]   And I mean, people rotate in.
[00:08:16.340 --> 00:08:18.860]   I mean, new people join, people leave the company,
[00:08:18.860 --> 00:08:20.340]   people go do other roles in the company.
[00:08:20.340 --> 00:08:22.660]   So it's not the same group over time.
[00:08:22.660 --> 00:08:25.780]   But then we spend, you know, a lot of times,
[00:08:25.780 --> 00:08:27.380]   a couple of hours, a lot of the time,
[00:08:27.380 --> 00:08:29.540]   it's, you know, it can be somewhat unstructured.
[00:08:29.540 --> 00:08:31.780]   We, like, I'll come with maybe a few topics
[00:08:31.780 --> 00:08:33.860]   that are top of mind for me,
[00:08:33.860 --> 00:08:36.460]   but I'll ask other people to bring things
[00:08:36.460 --> 00:08:38.500]   and people, you know, raise questions,
[00:08:38.500 --> 00:08:40.300]   whether it's, okay, there's an issue happening
[00:08:40.300 --> 00:08:44.260]   in some country with some policy issue.
[00:08:44.260 --> 00:08:46.380]   There's like a new technology that's developing here.
[00:08:46.380 --> 00:08:49.060]   We're having an issue with this partner.
[00:08:49.060 --> 00:08:51.620]   You know, there's a design trade-off in WhatsApp
[00:08:51.620 --> 00:08:55.780]   between two things that end up being values
[00:08:55.780 --> 00:08:56.940]   that we care about deeply.
[00:08:56.940 --> 00:08:59.380]   And we need to kind of decide where we wanna be on that.
[00:08:59.380 --> 00:09:02.540]   And I just think over time, when, you know,
[00:09:02.540 --> 00:09:04.460]   by working through a lot of issues with people
[00:09:04.460 --> 00:09:07.540]   and doing it openly, people develop an intuition
[00:09:07.540 --> 00:09:09.840]   for each other and a bond and camaraderie.
[00:09:09.840 --> 00:09:15.580]   And to me, developing that is like a lot of the fun part
[00:09:15.580 --> 00:09:17.420]   of running a company or doing anything, right?
[00:09:17.420 --> 00:09:20.220]   I think it's like having people who are kind of along
[00:09:20.220 --> 00:09:22.460]   on the journey that you feel like you're doing it with.
[00:09:22.460 --> 00:09:24.700]   Nothing is ever just one person doing it.
[00:09:24.700 --> 00:09:27.940]   - Are there people that disagree often within that group?
[00:09:27.940 --> 00:09:29.740]   - It's a fairly combative group.
[00:09:29.740 --> 00:09:31.900]   - Okay, so combat is part of it.
[00:09:31.900 --> 00:09:35.440]   So this is making decisions on design, engineering,
[00:09:35.440 --> 00:09:38.580]   policy, everything.
[00:09:38.580 --> 00:09:41.180]   - Everything, everything, yeah.
[00:09:41.180 --> 00:09:43.620]   - I have to ask, just back to jujitsu for a little bit,
[00:09:43.620 --> 00:09:45.060]   what's your favorite submission?
[00:09:45.060 --> 00:09:47.060]   Now that you've been doing it,
[00:09:47.060 --> 00:09:51.500]   how do you like to submit your opponent, Mark Zuckerberg?
[00:09:51.500 --> 00:09:52.340]   - I mean.
[00:09:52.340 --> 00:09:58.660]   - Well, first of all, do you prefer no gi or gi jujitsu?
[00:09:58.900 --> 00:10:03.900]   So gi is this outfit you wear that maybe mimics clothing
[00:10:03.900 --> 00:10:05.820]   so you can choke.
[00:10:05.820 --> 00:10:06.700]   - Well, it's like a kimono.
[00:10:06.700 --> 00:10:08.900]   It's like the traditional martial arts or kimono.
[00:10:08.900 --> 00:10:10.340]   - Pajamas.
[00:10:10.340 --> 00:10:11.180]   - Pajamas.
[00:10:11.180 --> 00:10:13.980]   - That you could choke people with, yes.
[00:10:13.980 --> 00:10:15.380]   - Well, it's got the lapels.
[00:10:15.380 --> 00:10:19.180]   Yeah, so I like jujitsu.
[00:10:19.180 --> 00:10:21.060]   I also really like MMA.
[00:10:21.060 --> 00:10:25.900]   And so I think no gi more closely approximates MMA.
[00:10:25.900 --> 00:10:30.820]   And I think my style is maybe a little closer
[00:10:30.820 --> 00:10:31.780]   to an MMA style.
[00:10:31.780 --> 00:10:33.980]   So like a lot of jujitsu players
[00:10:33.980 --> 00:10:35.620]   are fine being on their back, right?
[00:10:35.620 --> 00:10:36.820]   And obviously having a good guard
[00:10:36.820 --> 00:10:39.340]   is a critical part of jujitsu,
[00:10:39.340 --> 00:10:41.860]   but in MMA, you don't wanna be on your back, right?
[00:10:41.860 --> 00:10:42.920]   'Cause even if you have control,
[00:10:42.920 --> 00:10:45.740]   you're just taking punches while you're on your back.
[00:10:45.740 --> 00:10:47.740]   So that's no good.
[00:10:47.740 --> 00:10:48.740]   - So you like being on top.
[00:10:48.740 --> 00:10:51.820]   - My style is I'm probably more pressure.
[00:10:51.820 --> 00:10:56.820]   And yeah, and I'd probably rather be the top player,
[00:10:56.820 --> 00:10:59.820]   but I'm also smaller, right?
[00:10:59.820 --> 00:11:02.260]   I'm not like a heavyweight guy, right?
[00:11:02.260 --> 00:11:05.120]   So from that perspective, I think like,
[00:11:05.120 --> 00:11:08.340]   especially because if I'm doing a competition,
[00:11:08.340 --> 00:11:09.760]   I'll compete with people who are my size,
[00:11:09.760 --> 00:11:11.700]   but a lot of my friends are bigger than me.
[00:11:11.700 --> 00:11:15.140]   So back takes probably pretty important, right?
[00:11:15.140 --> 00:11:17.380]   Because that's where you have the most leverage advantage,
[00:11:17.380 --> 00:11:20.620]   right, where people, their arms,
[00:11:20.620 --> 00:11:22.500]   your arms are very weak behind you, right?
[00:11:22.500 --> 00:11:24.740]   So being able to get to the back
[00:11:24.740 --> 00:11:26.540]   and take that pretty important.
[00:11:26.540 --> 00:11:28.340]   But I don't know, I feel like the right strategy
[00:11:28.340 --> 00:11:31.100]   is to not be too committed to any single submission.
[00:11:31.100 --> 00:11:32.940]   But that said, I don't like hurting people.
[00:11:32.940 --> 00:11:36.060]   So I always think that chokes
[00:11:36.060 --> 00:11:40.900]   are a somewhat more humane way to go than joint locks.
[00:11:40.900 --> 00:11:42.780]   - Yeah, and it's more about control.
[00:11:42.780 --> 00:11:44.140]   It's less dynamic.
[00:11:44.140 --> 00:11:47.660]   So you're basically like a Habib Nurmagomedov type of fighter.
[00:11:47.660 --> 00:11:50.100]   - So let's go, yeah, back take to a rear naked choke.
[00:11:50.100 --> 00:11:52.300]   I think it's like the clean way to go.
[00:11:52.300 --> 00:11:53.980]   - Straightforward answer right there.
[00:11:53.980 --> 00:11:56.820]   What advice would you give to people
[00:11:56.820 --> 00:11:59.380]   looking to start learning jiu-jitsu?
[00:11:59.380 --> 00:12:02.980]   Given how busy you are, given where you are in life,
[00:12:02.980 --> 00:12:05.100]   that you're able to do this, you're able to train,
[00:12:05.100 --> 00:12:08.460]   you're able to compete and get to learn something
[00:12:08.460 --> 00:12:10.740]   from this interesting art.
[00:12:10.740 --> 00:12:11.980]   - Why do you think you have to be willing
[00:12:11.980 --> 00:12:16.980]   to just get beaten up a lot?
[00:12:16.980 --> 00:12:18.820]   I mean, it's- - But I mean, over time,
[00:12:18.820 --> 00:12:21.220]   I think that there's a flow to all these things.
[00:12:21.220 --> 00:12:24.540]   And there's, you know, one of the,
[00:12:24.540 --> 00:12:28.780]   one of, I don't know, my experiences
[00:12:28.780 --> 00:12:31.100]   that I think kind of transcends, you know,
[00:12:31.100 --> 00:12:34.300]   running a company and the different activities
[00:12:34.300 --> 00:12:36.860]   that I like doing are, I really believe that,
[00:12:36.860 --> 00:12:40.100]   like, if you're gonna accomplish whatever, anything,
[00:12:40.100 --> 00:12:43.260]   a lot of it is just being willing to push through, right?
[00:12:43.260 --> 00:12:45.540]   And having the grit and determination
[00:12:45.540 --> 00:12:48.820]   to push through difficult situations.
[00:12:48.820 --> 00:12:50.020]   And I think that for a lot of people,
[00:12:50.020 --> 00:12:54.300]   that ends up being sort of a difference maker
[00:12:54.300 --> 00:12:58.340]   between the people who kind of get the most done and not.
[00:12:58.340 --> 00:13:01.220]   I mean, there's all these questions about, like,
[00:13:01.220 --> 00:13:03.380]   you know, how many days people wanna work
[00:13:03.380 --> 00:13:04.220]   and things like that.
[00:13:04.220 --> 00:13:05.060]   I think almost all the people
[00:13:05.060 --> 00:13:07.420]   who, like, start successful companies or things like that
[00:13:07.420 --> 00:13:09.500]   are just, are working extremely hard.
[00:13:09.500 --> 00:13:11.060]   But I think one of the things that you learn,
[00:13:11.060 --> 00:13:13.260]   both by doing this over time,
[00:13:13.260 --> 00:13:15.940]   or, you know, very acutely with things like jujitsu
[00:13:15.940 --> 00:13:20.940]   or surfing is you can't push through everything.
[00:13:20.940 --> 00:13:24.060]   And I think that that's,
[00:13:24.060 --> 00:13:28.420]   you learn this stuff very acutely,
[00:13:28.420 --> 00:13:30.340]   doing sports compared to running a company.
[00:13:30.340 --> 00:13:33.340]   Because running a company, the cycle times are so long,
[00:13:33.340 --> 00:13:34.180]   right?
[00:13:34.180 --> 00:13:37.300]   It's like you start a project and then,
[00:13:37.300 --> 00:13:38.700]   you know, it's like months later,
[00:13:38.700 --> 00:13:40.420]   or, you know, if you're building hardware,
[00:13:40.420 --> 00:13:41.420]   it could be years later
[00:13:41.420 --> 00:13:42.860]   before you're actually getting feedback
[00:13:42.860 --> 00:13:45.020]   and able to make the next set of decisions
[00:13:45.020 --> 00:13:46.540]   for the next version of the thing that you're doing.
[00:13:46.540 --> 00:13:49.380]   Whereas, one of the things that I just think is mentally
[00:13:49.380 --> 00:13:53.420]   so nice about these very high turnaround,
[00:13:53.420 --> 00:13:55.420]   conditioning sports, things like that,
[00:13:55.420 --> 00:13:57.220]   is you get feedback very quickly, right?
[00:13:57.220 --> 00:13:59.700]   It's like, okay, like, I don't counter someone correctly,
[00:13:59.700 --> 00:14:00.940]   you get punched in the face, right?
[00:14:00.940 --> 00:14:03.340]   So not in jujitsu, you don't get punched in jujitsu,
[00:14:03.340 --> 00:14:04.820]   but in MMA.
[00:14:04.820 --> 00:14:07.220]   There are all these analogies between all these things
[00:14:07.220 --> 00:14:08.260]   that I think actually hold
[00:14:08.260 --> 00:14:12.340]   that are like important life lessons, right?
[00:14:12.340 --> 00:14:14.300]   It's like, okay, you're surfing a wave,
[00:14:14.300 --> 00:14:18.460]   it's like, you know, sometimes you're like,
[00:14:18.460 --> 00:14:20.900]   you can't go in the other direction on it, right?
[00:14:20.900 --> 00:14:23.740]   It's like, there are limits to kind of what, you know,
[00:14:23.740 --> 00:14:26.380]   it's like a foil, you can pump the foil
[00:14:26.380 --> 00:14:29.180]   and push pretty hard in a bunch of directions,
[00:14:29.180 --> 00:14:31.260]   but like, yeah, you, you know, it's at some level,
[00:14:31.260 --> 00:14:33.900]   like the momentum against you is strong enough,
[00:14:33.900 --> 00:14:35.420]   you're, that's not gonna work.
[00:14:35.420 --> 00:14:40.420]   And I do think that that's sort of a humbling,
[00:14:40.660 --> 00:14:43.940]   but also an important lesson for,
[00:14:43.940 --> 00:14:45.220]   and I think people who are running things
[00:14:45.220 --> 00:14:48.260]   or building things, it's like, yeah, you, you, you know,
[00:14:48.260 --> 00:14:50.780]   a lot of the game is just being able to kind of push
[00:14:50.780 --> 00:14:53.580]   and work through complicated things,
[00:14:53.580 --> 00:14:56.860]   but you also need to kind of have enough of an understanding
[00:14:56.860 --> 00:14:58.580]   of like which things you just can't push through
[00:14:58.580 --> 00:15:02.300]   and where the finesse is more important.
[00:15:02.300 --> 00:15:03.140]   - Yeah.
[00:15:03.140 --> 00:15:04.980]   - What are your jujitsu life lessons?
[00:15:06.380 --> 00:15:07.940]   - Well, I think you did it,
[00:15:07.940 --> 00:15:13.100]   you made it sound so simple and were so eloquent
[00:15:13.100 --> 00:15:18.100]   that it's easy to miss, but basically being okay
[00:15:18.100 --> 00:15:21.820]   and accepting the wisdom and the joy
[00:15:21.820 --> 00:15:24.620]   in the getting your ass kicked,
[00:15:24.620 --> 00:15:26.980]   in the full range of what that means,
[00:15:26.980 --> 00:15:30.860]   I think that's a big gift of the being humbled.
[00:15:30.860 --> 00:15:34.060]   Somehow being humbled, especially physically,
[00:15:34.060 --> 00:15:37.200]   opens your mind to the full process of learning,
[00:15:37.200 --> 00:15:38.340]   what it means to learn,
[00:15:38.340 --> 00:15:41.740]   which is being willing to suck at something.
[00:15:41.740 --> 00:15:45.540]   And I think jujitsu is just very repetitively,
[00:15:45.540 --> 00:15:49.740]   efficiently humbles you over and over and over and over
[00:15:49.740 --> 00:15:53.140]   to where you can carry that lessons to places
[00:15:53.140 --> 00:15:55.100]   where you don't get humbled as much,
[00:15:55.100 --> 00:15:57.020]   whether it's research or running a company
[00:15:57.020 --> 00:15:59.780]   or building stuff, the cycle is longer.
[00:15:59.780 --> 00:16:01.620]   In jujitsu, you can just get humbled
[00:16:01.620 --> 00:16:04.820]   in this period of an hour over and over and over and over,
[00:16:04.820 --> 00:16:05.900]   especially when you're a beginner,
[00:16:05.900 --> 00:16:07.340]   you'll have a little person,
[00:16:07.340 --> 00:16:09.980]   somebody much smarter than you,
[00:16:09.980 --> 00:16:14.980]   just kick your ass repeatedly, definitively,
[00:16:14.980 --> 00:16:17.660]   where there's no argument.
[00:16:17.660 --> 00:16:18.500]   - Oh, yeah.
[00:16:18.500 --> 00:16:20.100]   - And then you literally tap,
[00:16:20.100 --> 00:16:22.580]   because if you don't tap, you're going to die.
[00:16:22.580 --> 00:16:23.980]   So this is an agreement,
[00:16:23.980 --> 00:16:27.200]   you could have killed me just now, but we're friends,
[00:16:27.200 --> 00:16:29.260]   so we're gonna agree that you're not going to.
[00:16:29.260 --> 00:16:31.300]   And that kind of humbling process,
[00:16:31.300 --> 00:16:33.300]   it just does something to your psyche,
[00:16:33.300 --> 00:16:35.660]   to your ego that puts it in its proper context
[00:16:35.660 --> 00:16:39.860]   to realize that everything in this life
[00:16:39.860 --> 00:16:43.260]   is like a journey from sucking
[00:16:43.260 --> 00:16:46.740]   through a hard process of improving
[00:16:46.740 --> 00:16:51.020]   or rigorously day after day after day after day,
[00:16:51.020 --> 00:16:54.220]   like any kind of success requires hard work.
[00:16:54.220 --> 00:16:56.620]   Yeah, jujitsu, more than a lot of sports,
[00:16:56.620 --> 00:16:58.420]   I would say, 'cause I've done a lot of them,
[00:16:58.420 --> 00:16:59.900]   it really teaches you that.
[00:16:59.900 --> 00:17:01.580]   And you made it sound so simple.
[00:17:01.580 --> 00:17:04.900]   Like, "I'm okay, it's okay, it's part of the process."
[00:17:04.900 --> 00:17:06.340]   You just get humble, get your ass kicked.
[00:17:06.340 --> 00:17:07.780]   - I've just failed and been embarrassed
[00:17:07.780 --> 00:17:09.460]   so many times in my life that,
[00:17:09.460 --> 00:17:12.380]   it's a core competence to this.
[00:17:12.380 --> 00:17:14.220]   - It's a core competence.
[00:17:14.220 --> 00:17:16.060]   Well, yes, and there's a deep truth to that,
[00:17:16.060 --> 00:17:18.520]   being able to, and you said it in the very beginning,
[00:17:18.520 --> 00:17:21.660]   which is, that's the thing that stops us,
[00:17:21.660 --> 00:17:22.660]   especially as you get older,
[00:17:22.660 --> 00:17:25.580]   especially as you develop expertise in certain areas,
[00:17:25.580 --> 00:17:29.980]   the not being willing to be a beginner in a new area.
[00:17:29.980 --> 00:17:34.060]   Because that's where the growth happens,
[00:17:34.060 --> 00:17:35.900]   is being willing to be a beginner,
[00:17:35.900 --> 00:17:37.420]   being willing to be embarrassed,
[00:17:37.420 --> 00:17:39.780]   saying something stupid, doing something stupid.
[00:17:39.780 --> 00:17:42.420]   A lot of us that get good at one thing,
[00:17:42.420 --> 00:17:47.420]   you wanna show that off, and it sucks being a beginner,
[00:17:47.420 --> 00:17:50.260]   but it's where growth happens.
[00:17:50.260 --> 00:17:54.660]   Well, speaking of which, let me ask you about AI.
[00:17:54.660 --> 00:17:56.380]   It seems like this year,
[00:17:56.380 --> 00:17:58.340]   for the entirety of the human civilization,
[00:17:58.340 --> 00:17:59.980]   is an interesting year
[00:17:59.980 --> 00:18:02.580]   for the development of artificial intelligence.
[00:18:02.580 --> 00:18:04.580]   A lot of interesting stuff is happening.
[00:18:04.580 --> 00:18:07.300]   So, meta is a big part of that.
[00:18:07.300 --> 00:18:09.980]   Meta has developed LLAMA,
[00:18:09.980 --> 00:18:12.240]   which is a 65 billion parameter model.
[00:18:12.240 --> 00:18:16.660]   There's a lot of interesting questions I can ask here,
[00:18:16.660 --> 00:18:19.020]   one of which has to do with open source.
[00:18:19.020 --> 00:18:21.040]   But first, can you tell the story
[00:18:21.040 --> 00:18:24.020]   of developing of this model,
[00:18:24.020 --> 00:18:29.020]   and making the complicated decision of how to release it?
[00:18:29.020 --> 00:18:30.820]   - Yeah, sure.
[00:18:30.820 --> 00:18:32.020]   I think you're right, first of all,
[00:18:32.020 --> 00:18:36.420]   that in the last year, there have been a bunch of advances
[00:18:36.420 --> 00:18:39.660]   on scaling up these large transformer models.
[00:18:39.660 --> 00:18:41.160]   So, there's the language equivalent of it
[00:18:41.160 --> 00:18:43.100]   with large language models.
[00:18:43.100 --> 00:18:45.900]   There's sort of the image generation equivalent
[00:18:45.900 --> 00:18:48.260]   with these large diffusion models.
[00:18:48.260 --> 00:18:51.420]   There's a lot of fundamental research that's gone into this.
[00:18:51.420 --> 00:18:55.540]   And meta has taken the approach
[00:18:55.540 --> 00:19:00.180]   of being quite open and academic
[00:19:00.180 --> 00:19:04.260]   in our development of AI.
[00:19:04.260 --> 00:19:06.900]   Part of this is we wanna have the best people
[00:19:06.900 --> 00:19:08.420]   in the world researching this.
[00:19:08.420 --> 00:19:11.500]   And a lot of the best people wanna know
[00:19:11.500 --> 00:19:12.900]   that they're gonna be able to share their work.
[00:19:12.900 --> 00:19:15.820]   So, that's part of the deal that we have,
[00:19:15.820 --> 00:19:17.860]   is that we can get,
[00:19:17.860 --> 00:19:20.420]   if you're one of the top AI researchers in the world,
[00:19:20.420 --> 00:19:21.700]   you can come here, you can get access
[00:19:21.700 --> 00:19:25.500]   to kind of industry scale infrastructure.
[00:19:25.500 --> 00:19:29.060]   And part of our ethos is that we wanna share
[00:19:29.060 --> 00:19:32.100]   what's invented broadly.
[00:19:32.100 --> 00:19:34.300]   We do that with a lot of the different AI tools
[00:19:34.300 --> 00:19:35.660]   that we create.
[00:19:35.660 --> 00:19:37.700]   And LLAMA is the language model
[00:19:37.700 --> 00:19:39.820]   that our research team made.
[00:19:39.820 --> 00:19:44.820]   And we did a limited open source release for it,
[00:19:44.820 --> 00:19:49.620]   which was intended for researchers to be able to use it.
[00:19:50.620 --> 00:19:55.620]   But responsibility and getting safety right on these
[00:19:55.620 --> 00:19:57.100]   is very important.
[00:19:57.100 --> 00:19:59.780]   So, we didn't think that, for the first one,
[00:19:59.780 --> 00:20:01.980]   there were a bunch of questions around
[00:20:01.980 --> 00:20:04.180]   whether we should be releasing this commercially.
[00:20:04.180 --> 00:20:08.180]   So, we kind of punched it on that for V1 of LLAMA
[00:20:08.180 --> 00:20:09.860]   and just released it for research.
[00:20:09.860 --> 00:20:13.300]   Now, obviously, by releasing it for research,
[00:20:13.300 --> 00:20:15.180]   it's out there, but companies know
[00:20:15.180 --> 00:20:17.260]   that they're not supposed to kind of put it
[00:20:17.260 --> 00:20:18.780]   into commercial releases.
[00:20:18.780 --> 00:20:22.700]   And we're working on the follow-up models for this
[00:20:22.700 --> 00:20:27.700]   and thinking through how exactly this should work
[00:20:27.700 --> 00:20:29.460]   for follow-on now that we've had time
[00:20:29.460 --> 00:20:31.820]   to work on a lot more of the safety
[00:20:31.820 --> 00:20:33.940]   and the pieces around that.
[00:20:33.940 --> 00:20:35.740]   But overall, I mean, this is,
[00:20:35.740 --> 00:20:42.020]   I just kind of think that it would be good
[00:20:42.020 --> 00:20:45.940]   if there were a lot of different folks
[00:20:45.940 --> 00:20:50.940]   who had the ability to build state-of-the-art technology here
[00:20:50.940 --> 00:20:56.300]   and not just a small number of big companies.
[00:20:56.300 --> 00:20:58.620]   Where to train one of these AI models,
[00:20:58.620 --> 00:21:00.060]   the state-of-the-art models,
[00:21:00.060 --> 00:21:04.500]   just takes hundreds of millions of dollars
[00:21:04.500 --> 00:21:05.700]   of infrastructure, right?
[00:21:05.700 --> 00:21:09.540]   So, there are not that many organizations in the world
[00:21:09.540 --> 00:21:13.020]   that can do that at the biggest scale today.
[00:21:13.020 --> 00:21:16.180]   And now, it gets more efficient every day.
[00:21:16.180 --> 00:21:19.860]   So, I do think that that will be available
[00:21:19.860 --> 00:21:20.940]   to more folks over time.
[00:21:20.940 --> 00:21:23.860]   But I just think there's all this innovation out there
[00:21:23.860 --> 00:21:25.460]   that people can create.
[00:21:25.460 --> 00:21:29.980]   And I just think that we'll also learn a lot
[00:21:29.980 --> 00:21:32.980]   by seeing what the whole community of students
[00:21:32.980 --> 00:21:37.780]   and hackers and startups and different folks
[00:21:37.780 --> 00:21:38.620]   build with this.
[00:21:38.620 --> 00:21:40.380]   And that's kind of been how we've approached this.
[00:21:40.380 --> 00:21:43.020]   And it's also how we've done a lot of our infrastructure.
[00:21:43.020 --> 00:21:44.940]   And we took our whole data center design
[00:21:44.940 --> 00:21:46.020]   and our server design,
[00:21:46.020 --> 00:21:48.020]   and we built this Open Compute Project
[00:21:48.020 --> 00:21:49.140]   where we just made that public.
[00:21:49.140 --> 00:21:51.500]   And part of the theory was like,
[00:21:51.500 --> 00:21:52.780]   "All right, if we make it so that more people
[00:21:52.780 --> 00:21:54.340]   "can use this server design,
[00:21:54.340 --> 00:21:57.980]   "then that'll enable more innovation.
[00:21:57.980 --> 00:22:00.100]   "It'll also make the server design more efficient.
[00:22:00.100 --> 00:22:02.500]   "And that'll make our business more efficient too."
[00:22:02.500 --> 00:22:03.340]   So, that's worked.
[00:22:03.340 --> 00:22:06.460]   And we've just done this with a lot of our infrastructure.
[00:22:06.460 --> 00:22:07.700]   - So, for people who don't know,
[00:22:07.700 --> 00:22:09.180]   you did the limited release, I think,
[00:22:09.180 --> 00:22:12.540]   in February of this year, of LLAMA.
[00:22:12.540 --> 00:22:15.500]   And it got quote, unquote, leaked,
[00:22:15.500 --> 00:22:21.500]   meaning like it escaped the limited release aspect.
[00:22:21.500 --> 00:22:28.220]   But it was something you probably anticipated
[00:22:28.220 --> 00:22:29.780]   given that it's just released to researchers.
[00:22:29.780 --> 00:22:30.740]   - We shared it with researchers.
[00:22:30.740 --> 00:22:31.580]   - You're right.
[00:22:31.580 --> 00:22:33.340]   So, it's just trying to make sure
[00:22:33.340 --> 00:22:35.460]   that there's like a slow release.
[00:22:35.460 --> 00:22:36.780]   - Yeah.
[00:22:36.780 --> 00:22:39.060]   - But from there, I just would love to get your comment
[00:22:39.060 --> 00:22:40.660]   on what happened next, which is like,
[00:22:40.660 --> 00:22:42.740]   there's a very vibrant open source community
[00:22:42.740 --> 00:22:44.500]   that just builds stuff on top of it.
[00:22:44.500 --> 00:22:48.580]   There's LLAMA CPP, basically stuff
[00:22:48.580 --> 00:22:51.740]   that makes it more efficient to run on smaller computers.
[00:22:51.740 --> 00:22:54.900]   There's combining with reinforcement learning
[00:22:54.900 --> 00:22:55.860]   with human feedback.
[00:22:55.860 --> 00:22:59.460]   So, some of the different interesting fine tuning mechanisms.
[00:22:59.460 --> 00:23:02.860]   There's then also like fine tuning in a GPT-3 generations.
[00:23:02.860 --> 00:23:07.500]   There's a lot of GPT-4ALL, Alpaca, Colossal AI,
[00:23:07.500 --> 00:23:09.300]   all these kinds of models just kind of spring up,
[00:23:09.300 --> 00:23:11.540]   like run on top of it.
[00:23:11.540 --> 00:23:12.380]   - Yeah.
[00:23:12.380 --> 00:23:13.220]   - What do you think about that?
[00:23:13.220 --> 00:23:15.020]   - No, I think it's been really neat to see.
[00:23:15.020 --> 00:23:17.820]   I mean, there's been folks who are getting it to run
[00:23:17.820 --> 00:23:19.340]   on local devices, right?
[00:23:19.340 --> 00:23:22.420]   So, if you're an individual who just wants to experiment
[00:23:22.420 --> 00:23:26.380]   with this at home, you probably don't have a large budget
[00:23:26.380 --> 00:23:29.460]   to get access to like a large amount of cloud compute.
[00:23:29.460 --> 00:23:31.700]   So, getting it to run on your local laptop
[00:23:31.700 --> 00:23:35.140]   is pretty good, right?
[00:23:35.140 --> 00:23:36.140]   And pretty relevant.
[00:23:37.420 --> 00:23:39.860]   And then there were things like, yeah, Lama CPP
[00:23:39.860 --> 00:23:42.500]   re-implemented it more efficiently.
[00:23:42.500 --> 00:23:45.900]   So, now even when we run our own versions of it,
[00:23:45.900 --> 00:23:47.260]   we can do it on way less compute
[00:23:47.260 --> 00:23:50.380]   and it just way more efficient, save a lot of money
[00:23:50.380 --> 00:23:51.740]   for everyone who uses this.
[00:23:51.740 --> 00:23:53.780]   So, that is good.
[00:23:53.780 --> 00:23:59.300]   I do think it's worth calling out that
[00:23:59.300 --> 00:24:01.780]   because this was a relatively early release,
[00:24:03.500 --> 00:24:08.500]   Lama isn't quite as on the frontier as, for example,
[00:24:08.500 --> 00:24:13.220]   the biggest open AI models or the biggest Google models.
[00:24:13.220 --> 00:24:16.620]   So, I mean, you mentioned that the largest Lama model
[00:24:16.620 --> 00:24:19.300]   that we released had 65 billion parameters.
[00:24:19.300 --> 00:24:24.300]   And no one knows, I guess, outside of open AI,
[00:24:24.300 --> 00:24:27.860]   exactly what the specs are for GPT-4.
[00:24:27.860 --> 00:24:30.900]   But I think the, my understanding is it's like
[00:24:30.900 --> 00:24:32.140]   10 times bigger.
[00:24:32.140 --> 00:24:35.060]   And I think Google's Palm model is also, I think,
[00:24:35.060 --> 00:24:36.780]   has about 10 times as many parameters.
[00:24:36.780 --> 00:24:38.580]   Now, the Lama models are very efficient.
[00:24:38.580 --> 00:24:40.420]   So, they perform well for something
[00:24:40.420 --> 00:24:42.620]   that's around 65 billion parameters.
[00:24:42.620 --> 00:24:44.340]   So, for me, that was also part of this
[00:24:44.340 --> 00:24:47.060]   because there was this whole debate around,
[00:24:47.060 --> 00:24:50.780]   is it good for everyone in the world to have access
[00:24:50.780 --> 00:24:55.020]   to the most frontier AI models?
[00:24:55.020 --> 00:25:00.020]   And I think as the AI models start approaching something
[00:25:00.460 --> 00:25:03.980]   that's like a super human intelligence,
[00:25:03.980 --> 00:25:05.300]   I think that that's a bigger question
[00:25:05.300 --> 00:25:06.300]   that we'll have to grapple with.
[00:25:06.300 --> 00:25:10.860]   But right now, I mean, these are still very basic tools.
[00:25:10.860 --> 00:25:14.260]   They're powerful in the sense that,
[00:25:14.260 --> 00:25:17.620]   a lot of open source software like databases or web servers
[00:25:17.620 --> 00:25:20.540]   can enable a lot of pretty important things.
[00:25:20.540 --> 00:25:27.020]   But I don't think anyone looks at the current generation
[00:25:27.020 --> 00:25:30.220]   of Lama and thinks it's anywhere near a super intelligence.
[00:25:30.220 --> 00:25:32.300]   So, I think that a bunch of those questions around,
[00:25:32.300 --> 00:25:35.900]   like, is it good to kind of get out there?
[00:25:35.900 --> 00:25:37.780]   I think at this stage, surely,
[00:25:37.780 --> 00:25:40.140]   you want more researchers working on it
[00:25:40.140 --> 00:25:43.340]   for all the reasons that open source software
[00:25:43.340 --> 00:25:44.580]   has a lot of advantages.
[00:25:44.580 --> 00:25:45.900]   And we talked about efficiency before,
[00:25:45.900 --> 00:25:48.020]   but another one is just open source software
[00:25:48.020 --> 00:25:49.780]   tends to be more secure
[00:25:49.780 --> 00:25:52.100]   because you have more people looking at it openly
[00:25:52.100 --> 00:25:55.860]   and scrutinizing it and finding holes in it.
[00:25:55.860 --> 00:25:57.100]   And that makes it more safe.
[00:25:57.100 --> 00:25:59.300]   So, I think at this point, it's more,
[00:25:59.300 --> 00:26:01.980]   I think it's generally agreed upon
[00:26:01.980 --> 00:26:05.140]   that open source software is generally more secure
[00:26:05.140 --> 00:26:08.780]   and safer than things that are kind of developed in a silo
[00:26:08.780 --> 00:26:11.340]   where people try to get through security through obscurity.
[00:26:11.340 --> 00:26:13.420]   So, I think that for the scale
[00:26:13.420 --> 00:26:16.420]   of what we're seeing now with AI,
[00:26:16.420 --> 00:26:19.500]   I think we're more likely to get to good alignment
[00:26:19.500 --> 00:26:23.460]   and good understanding of kind of what needs to do
[00:26:23.460 --> 00:26:26.100]   to make this work well by having it be open source.
[00:26:26.100 --> 00:26:27.180]   And that's something that I think
[00:26:27.180 --> 00:26:28.940]   is quite good to have out there.
[00:26:28.940 --> 00:26:30.900]   And happening publicly at this point.
[00:26:30.900 --> 00:26:34.580]   - Meta released a lot of models as open source.
[00:26:34.580 --> 00:26:38.380]   So, the massively multilingual speech model.
[00:26:38.380 --> 00:26:39.660]   - Yeah, that was neat.
[00:26:39.660 --> 00:26:42.020]   - I mean, I'll ask you questions about those,
[00:26:42.020 --> 00:26:45.860]   but the point is you've open sourced quite a lot.
[00:26:45.860 --> 00:26:47.580]   You've been spearheading the open source movement.
[00:26:47.580 --> 00:26:50.340]   Whereas that's really positive,
[00:26:50.340 --> 00:26:51.940]   inspiring to see from one angle,
[00:26:51.940 --> 00:26:52.980]   from the research angle.
[00:26:52.980 --> 00:26:55.300]   Of course, there's folks who are really terrified
[00:26:55.300 --> 00:26:58.460]   about the existential threat of artificial intelligence
[00:26:58.460 --> 00:27:00.180]   and those folks will say that,
[00:27:00.180 --> 00:27:06.420]   you have to be careful about the open sourcing step.
[00:27:06.420 --> 00:27:09.540]   But where do you see the future of open source here
[00:27:09.540 --> 00:27:11.300]   as part of meta?
[00:27:11.300 --> 00:27:13.820]   The tension here is,
[00:27:13.820 --> 00:27:16.060]   do you wanna release the magic sauce?
[00:27:16.060 --> 00:27:18.020]   That's one tension.
[00:27:18.020 --> 00:27:19.940]   And the other one is,
[00:27:19.940 --> 00:27:22.900]   do you wanna put a powerful tool in the hands
[00:27:22.900 --> 00:27:24.820]   of bad actors,
[00:27:24.820 --> 00:27:26.900]   even though it probably has a huge amount
[00:27:26.900 --> 00:27:28.140]   of positive impact also.
[00:27:29.140 --> 00:27:29.980]   - Yeah, I mean, again,
[00:27:29.980 --> 00:27:31.500]   I think for the stage that we're at
[00:27:31.500 --> 00:27:32.860]   in the development of AI,
[00:27:32.860 --> 00:27:35.260]   I don't think anyone looks at the current state of things
[00:27:35.260 --> 00:27:37.500]   and thinks that this is super intelligence.
[00:27:37.500 --> 00:27:40.900]   And the models that we're talking about,
[00:27:40.900 --> 00:27:43.460]   the Lama models here are,
[00:27:43.460 --> 00:27:46.220]   generally an order of magnitude smaller
[00:27:46.220 --> 00:27:48.020]   than what open AI or Google are doing.
[00:27:48.020 --> 00:27:52.060]   So, I think that at least for the stage that we're at now,
[00:27:52.060 --> 00:27:55.820]   the equity is balanced strongly in my view
[00:27:55.820 --> 00:27:57.540]   towards doing this more openly.
[00:27:58.300 --> 00:27:59.580]   I think if you got something
[00:27:59.580 --> 00:28:02.780]   that was closer to super intelligence,
[00:28:02.780 --> 00:28:05.220]   then I think you'd have to discuss that more
[00:28:05.220 --> 00:28:07.860]   and think through that a lot more.
[00:28:07.860 --> 00:28:09.500]   And we haven't made a decision yet
[00:28:09.500 --> 00:28:11.700]   as to what we would do if we were in that position,
[00:28:11.700 --> 00:28:13.580]   but I think there's a good chance
[00:28:13.580 --> 00:28:15.140]   that we're pretty far off from that position.
[00:28:15.140 --> 00:28:19.020]   So, I'm not,
[00:28:19.020 --> 00:28:22.580]   I'm certainly not saying that the position
[00:28:22.580 --> 00:28:24.060]   that we're taking on this now
[00:28:24.060 --> 00:28:27.140]   applies to every single thing that we would ever do.
[00:28:27.140 --> 00:28:29.340]   And certainly inside the company,
[00:28:29.340 --> 00:28:30.980]   we probably do more open source work
[00:28:30.980 --> 00:28:34.340]   than most of the other big tech companies,
[00:28:34.340 --> 00:28:36.060]   but we also don't open source everything.
[00:28:36.060 --> 00:28:37.220]   We're in a lot of our,
[00:28:37.220 --> 00:28:40.500]   the core kind of app code for WhatsApp
[00:28:40.500 --> 00:28:41.580]   or Instagram or something.
[00:28:41.580 --> 00:28:42.900]   I mean, we're not open sourcing that.
[00:28:42.900 --> 00:28:45.820]   It's not like a general enough piece of software
[00:28:45.820 --> 00:28:47.380]   that would be useful for a lot of people
[00:28:47.380 --> 00:28:49.240]   to do different things.
[00:28:49.240 --> 00:28:53.340]   Whereas the software that we do,
[00:28:53.340 --> 00:28:55.740]   whether it's like an open source server design
[00:28:55.740 --> 00:29:00.140]   or basically things like Memcache,
[00:29:00.140 --> 00:29:03.340]   like a good, it was probably our earliest project
[00:29:03.340 --> 00:29:05.220]   that I worked on.
[00:29:05.220 --> 00:29:07.460]   It was probably one of the last things that I coded
[00:29:07.460 --> 00:29:09.360]   and led directly for the company.
[00:29:09.360 --> 00:29:15.820]   But basically this caching tool for quick data retrieval.
[00:29:15.820 --> 00:29:19.900]   These are things that are just broadly useful
[00:29:19.900 --> 00:29:22.020]   across anything that you wanna build.
[00:29:22.020 --> 00:29:25.660]   And I think that some of the language models now
[00:29:25.660 --> 00:29:28.100]   have that feel as well as some of the other things
[00:29:28.100 --> 00:29:29.660]   that we're building, like the translation tool
[00:29:29.660 --> 00:29:31.300]   that you just referenced.
[00:29:31.300 --> 00:29:34.340]   - So text to speech and speech to text,
[00:29:34.340 --> 00:29:36.340]   you've expanded it from around 100 languages
[00:29:36.340 --> 00:29:38.700]   to more than 1100 languages.
[00:29:38.700 --> 00:29:39.740]   - Yeah. - And you can identify
[00:29:39.740 --> 00:29:41.300]   more than, the model can identify
[00:29:41.300 --> 00:29:44.020]   more than 4,000 spoken languages,
[00:29:44.020 --> 00:29:47.500]   which is 40 times more than any known previous technology.
[00:29:47.500 --> 00:29:49.740]   To me, that's really, really, really exciting
[00:29:49.740 --> 00:29:51.740]   in terms of connecting the world,
[00:29:51.740 --> 00:29:54.680]   breaking down barriers that language creates.
[00:29:54.680 --> 00:29:55.900]   - Yeah, I think being able to translate
[00:29:55.900 --> 00:29:59.740]   between all of these different pieces in real time,
[00:29:59.740 --> 00:30:04.740]   this has been a kind of common sci-fi idea
[00:30:04.740 --> 00:30:10.580]   that we'd all have, whether it's an earbud or glasses
[00:30:10.580 --> 00:30:12.980]   or something that can help translate in real time
[00:30:12.980 --> 00:30:15.140]   between all these different languages.
[00:30:15.140 --> 00:30:16.460]   And that's one that I think technology
[00:30:16.460 --> 00:30:19.780]   is basically delivering now.
[00:30:19.780 --> 00:30:22.900]   So I think, yeah, I think that's pretty exciting.
[00:30:22.900 --> 00:30:24.920]   - You mentioned the next version of LLAMA.
[00:30:24.920 --> 00:30:27.880]   What can you say about the next version of LLAMA?
[00:30:27.880 --> 00:30:31.520]   What can you say about what were you working on
[00:30:31.520 --> 00:30:35.080]   in terms of release, in terms of the vision for that?
[00:30:35.080 --> 00:30:36.840]   - Well, a lot of what we're doing
[00:30:36.840 --> 00:30:40.720]   is taking the first version, which was primarily
[00:30:40.720 --> 00:30:44.680]   this research version, and trying to now build a version
[00:30:44.680 --> 00:30:49.640]   that has all of the latest state-of-the-art
[00:30:49.640 --> 00:30:52.180]   safety precautions built in.
[00:30:52.180 --> 00:30:56.920]   And we're using some more data to train it
[00:30:56.920 --> 00:30:58.640]   from across our services.
[00:30:58.640 --> 00:31:02.560]   But a lot of the work that we're doing internally
[00:31:02.560 --> 00:31:04.360]   is really just focused on making sure
[00:31:04.360 --> 00:31:09.360]   that this is as aligned and responsible as possible.
[00:31:09.360 --> 00:31:12.920]   And we're building a lot of our own,
[00:31:12.920 --> 00:31:15.900]   we're talking about kind of the open source infrastructure,
[00:31:15.900 --> 00:31:20.120]   but the main thing that we focus on building here,
[00:31:20.120 --> 00:31:22.000]   a lot of product experiences to help people connect
[00:31:22.000 --> 00:31:22.860]   and express themselves.
[00:31:22.860 --> 00:31:26.700]   So we're gonna, I've talked about a bunch of this stuff,
[00:31:26.700 --> 00:31:30.940]   but you'll have an assistant that you can talk to
[00:31:30.940 --> 00:31:31.780]   in WhatsApp.
[00:31:31.780 --> 00:31:36.300]   I think in the future, every creator will have
[00:31:36.300 --> 00:31:39.380]   kind of an AI agent that can kind of act on their behalf,
[00:31:39.380 --> 00:31:41.180]   that their fans can talk to.
[00:31:41.180 --> 00:31:43.700]   I wanna get to the point where every small business
[00:31:43.700 --> 00:31:46.940]   basically has an AI agent that people can talk to
[00:31:46.940 --> 00:31:49.540]   for, to do commerce and customer support
[00:31:49.540 --> 00:31:50.360]   and things like that.
[00:31:50.360 --> 00:31:53.260]   So there are gonna be all these different things
[00:31:53.260 --> 00:31:57.580]   and LLAMA or the language model underlying this
[00:31:57.580 --> 00:32:00.500]   is basically gonna be the engine that powers that.
[00:32:00.500 --> 00:32:02.700]   The reason to open source it is that,
[00:32:02.700 --> 00:32:06.660]   as we did with the first version,
[00:32:06.660 --> 00:32:11.660]   is that it basically it unlocks a lot of innovation
[00:32:11.660 --> 00:32:15.760]   in the ecosystem, will make our products better as well.
[00:32:15.760 --> 00:32:17.680]   And also gives us a lot of valuable feedback
[00:32:17.680 --> 00:32:19.140]   on security and safety,
[00:32:19.140 --> 00:32:21.100]   which is important for making this good.
[00:32:21.100 --> 00:32:23.460]   But yeah, I mean, the work that we're doing
[00:32:23.460 --> 00:32:25.340]   to advance the infrastructure,
[00:32:25.340 --> 00:32:27.740]   it's basically at this point,
[00:32:27.740 --> 00:32:29.900]   taking it beyond a research project
[00:32:29.900 --> 00:32:32.020]   into something which is ready to be
[00:32:32.020 --> 00:32:33.700]   kind of core infrastructure,
[00:32:33.700 --> 00:32:35.140]   not only for our own products,
[00:32:35.140 --> 00:32:39.140]   but hopefully for a lot of other things out there too.
[00:32:39.140 --> 00:32:41.840]   - Do you think the LLAMA or the language model
[00:32:41.840 --> 00:32:45.660]   underlying that version two will be open sourced?
[00:32:47.980 --> 00:32:49.900]   Do you have internal debate around that,
[00:32:49.900 --> 00:32:51.540]   the pros and cons and so on?
[00:32:51.540 --> 00:32:53.260]   - This is, I mean, we were talking about the debates
[00:32:53.260 --> 00:32:55.060]   that we have internally and I think,
[00:32:55.060 --> 00:32:58.840]   I think the question is how to do it, right?
[00:32:58.840 --> 00:33:03.700]   I mean, I think we did the research license for V1
[00:33:03.700 --> 00:33:07.500]   and I think the big thing that we're thinking about
[00:33:07.500 --> 00:33:11.180]   is basically like what's the right way.
[00:33:11.180 --> 00:33:13.100]   - So there was a leak that happened.
[00:33:13.100 --> 00:33:15.740]   I don't know if you can comment on it for V1.
[00:33:15.740 --> 00:33:18.020]   We released it as a research project
[00:33:18.020 --> 00:33:21.700]   for researchers to be able to use,
[00:33:21.700 --> 00:33:23.180]   but in doing so we put it out there.
[00:33:23.180 --> 00:33:28.060]   So we were very clear that anyone who uses the code
[00:33:28.060 --> 00:33:30.100]   and the weights doesn't have a commercial license
[00:33:30.100 --> 00:33:31.020]   to put into products.
[00:33:31.020 --> 00:33:33.860]   And we've generally seen people respect that, right?
[00:33:33.860 --> 00:33:36.460]   It's like you don't have any reputable companies
[00:33:36.460 --> 00:33:37.660]   that are basically trying to put this
[00:33:37.660 --> 00:33:40.480]   into their commercial products.
[00:33:40.480 --> 00:33:44.140]   But yeah, but by sharing it with so many researchers,
[00:33:45.500 --> 00:33:46.740]   it did leave the building.
[00:33:46.740 --> 00:33:49.060]   - But what have you learned from that process
[00:33:49.060 --> 00:33:52.260]   that you might be able to apply to V2
[00:33:52.260 --> 00:33:55.100]   about how to release it safely, effectively,
[00:33:55.100 --> 00:33:57.620]   if you release it?
[00:33:57.620 --> 00:33:59.440]   - Yeah, well, I mean, I think a lot of the feedback,
[00:33:59.440 --> 00:34:03.380]   like I said, is just around different things around,
[00:34:03.380 --> 00:34:06.620]   how do you fine tune models to make them more aligned
[00:34:06.620 --> 00:34:07.440]   and safer?
[00:34:07.440 --> 00:34:10.540]   And you see all the different data recipes that,
[00:34:10.540 --> 00:34:14.140]   you mentioned a lot of different projects
[00:34:14.140 --> 00:34:14.980]   that are based on this.
[00:34:14.980 --> 00:34:16.420]   And there's one at Berkeley,
[00:34:16.420 --> 00:34:18.420]   there's just like all over.
[00:34:18.420 --> 00:34:23.420]   And people have tried a lot of different things
[00:34:23.420 --> 00:34:25.380]   and we've tried a bunch of stuff internally.
[00:34:25.380 --> 00:34:29.200]   So kind of we're making progress here,
[00:34:29.200 --> 00:34:31.620]   but also we're able to learn from some of the best ideas
[00:34:31.620 --> 00:34:32.460]   in the community.
[00:34:32.460 --> 00:34:36.420]   And I think we wanna just continue pushing that forward.
[00:34:36.420 --> 00:34:40.460]   But I don't have any news to announce on this,
[00:34:40.460 --> 00:34:41.780]   if that's what you're asking.
[00:34:41.780 --> 00:34:46.780]   I mean, this is a thing that we're still kind of,
[00:34:46.780 --> 00:34:51.420]   actively working through the right way
[00:34:51.420 --> 00:34:52.260]   to move forward here.
[00:34:52.260 --> 00:34:54.460]   - The details of the secret sauce
[00:34:54.460 --> 00:34:57.540]   are still being developed, I see.
[00:34:57.540 --> 00:35:00.620]   Can you comment on, what do you think of the thing
[00:35:00.620 --> 00:35:01.860]   that worked for GPT,
[00:35:01.860 --> 00:35:04.200]   which is the reinforcement learning with human feedback?
[00:35:04.200 --> 00:35:06.820]   So doing this alignment process,
[00:35:06.820 --> 00:35:08.740]   do you find it interesting?
[00:35:08.740 --> 00:35:10.100]   And as part of that, let me ask,
[00:35:10.100 --> 00:35:13.020]   'cause I talked to Jan Lekun before talking to you today.
[00:35:13.020 --> 00:35:16.540]   He asked me to ask, or suggested that I ask,
[00:35:16.540 --> 00:35:19.380]   do you think LLM fine tuning
[00:35:19.380 --> 00:35:22.240]   will need to be crowdsourced Wikipedia style?
[00:35:22.240 --> 00:35:24.780]   So crowdsourcing.
[00:35:24.780 --> 00:35:29.180]   So this kind of idea of how to integrate the human
[00:35:29.180 --> 00:35:32.980]   in the fine tuning of these foundation models.
[00:35:32.980 --> 00:35:35.540]   - Yeah, I think that's a really interesting idea
[00:35:35.540 --> 00:35:38.300]   that I've talked to Jan about a bunch.
[00:35:39.300 --> 00:35:43.060]   And we were talking about,
[00:35:43.060 --> 00:35:45.620]   how do you basically train these models
[00:35:45.620 --> 00:35:50.180]   to be as safe and aligned and responsible as possible?
[00:35:50.180 --> 00:35:53.260]   And different groups out there who are doing development
[00:35:53.260 --> 00:35:57.120]   test different data recipes in fine tuning.
[00:35:57.120 --> 00:36:00.520]   But this idea that you just mentioned is,
[00:36:00.520 --> 00:36:04.180]   that at the end of the day,
[00:36:04.180 --> 00:36:07.100]   instead of having kind of one group fine tune some stuff
[00:36:07.100 --> 00:36:09.020]   and then another group,
[00:36:09.020 --> 00:36:10.660]   produce a different fine tuning recipe
[00:36:10.660 --> 00:36:12.760]   and then us trying to figure out
[00:36:12.760 --> 00:36:13.980]   which one we think works best
[00:36:13.980 --> 00:36:16.120]   to produce the most aligned model.
[00:36:16.120 --> 00:36:21.660]   I do think that it would be nice
[00:36:21.660 --> 00:36:23.920]   if you could get to a point where you had
[00:36:23.920 --> 00:36:27.620]   a Wikipedia style collaborative way
[00:36:27.620 --> 00:36:33.780]   for a kind of a broader community to fine tune it as well.
[00:36:33.780 --> 00:36:36.020]   Now, there's a lot of challenges in that,
[00:36:36.020 --> 00:36:41.020]   both from an infrastructure and community management
[00:36:41.020 --> 00:36:42.740]   and product perspective about how you do that.
[00:36:42.740 --> 00:36:45.140]   So I haven't worked that out yet.
[00:36:45.140 --> 00:36:48.740]   But as an idea, I think it's quite compelling.
[00:36:48.740 --> 00:36:50.940]   And I think it goes well with the ethos
[00:36:50.940 --> 00:36:54.060]   of open sourcing the technology is also finding a way
[00:36:54.060 --> 00:36:59.060]   to have a kind of community driven training of it.
[00:36:59.060 --> 00:37:03.380]   But I think that there are a lot of questions on this.
[00:37:03.380 --> 00:37:05.420]   In general, these questions around
[00:37:05.420 --> 00:37:09.700]   what's the best way to produce aligned AI models,
[00:37:09.700 --> 00:37:12.060]   it's very much a research area.
[00:37:12.060 --> 00:37:14.340]   And it's one that I think we will need to make
[00:37:14.340 --> 00:37:17.220]   as much progress on as the kind of core intelligence
[00:37:17.220 --> 00:37:21.100]   capability of the models themselves.
[00:37:21.100 --> 00:37:23.420]   - Well, I just did a conversation with Jimmy Wales,
[00:37:23.420 --> 00:37:24.620]   the founder of Wikipedia.
[00:37:24.620 --> 00:37:28.300]   And to me, Wikipedia is one of the greatest websites
[00:37:28.300 --> 00:37:29.860]   ever created.
[00:37:29.860 --> 00:37:31.900]   And it's a kind of a miracle that it works.
[00:37:31.900 --> 00:37:33.980]   And I think it has to do with something that you mentioned,
[00:37:33.980 --> 00:37:35.360]   which is community.
[00:37:35.360 --> 00:37:38.180]   You have a small community of editors
[00:37:38.180 --> 00:37:40.500]   that somehow work together well.
[00:37:40.500 --> 00:37:45.500]   And they handle very controversial topics
[00:37:45.500 --> 00:37:48.820]   and they handle it with balance and with grace,
[00:37:48.820 --> 00:37:51.820]   despite sort of the attacks that will often happen.
[00:37:51.820 --> 00:37:52.660]   - A lot of the time.
[00:37:52.660 --> 00:37:56.320]   I mean, it has issues just like any other human system.
[00:37:56.320 --> 00:37:58.580]   But yes, I mean, the balance is,
[00:37:58.580 --> 00:38:01.460]   I mean, it's amazing what they've been able to achieve,
[00:38:01.460 --> 00:38:03.020]   but it's also not perfect.
[00:38:03.020 --> 00:38:06.880]   And I think that there's still a lot of challenges.
[00:38:06.880 --> 00:38:09.880]   - Right, the more controversial the topic,
[00:38:09.880 --> 00:38:14.880]   the more difficult the journey towards,
[00:38:14.880 --> 00:38:18.060]   quote unquote, truth or knowledge or wisdom
[00:38:18.060 --> 00:38:20.020]   that Wikipedia tries to capture.
[00:38:20.020 --> 00:38:21.300]   In the same way AI models,
[00:38:21.300 --> 00:38:25.000]   we need to be able to generate those same things,
[00:38:25.000 --> 00:38:26.800]   truth, knowledge, and wisdom.
[00:38:26.800 --> 00:38:28.480]   And how do you align those models
[00:38:28.480 --> 00:38:33.480]   that they generate something that is closest to truth?
[00:38:33.480 --> 00:38:38.060]   There's these concerns about misinformation,
[00:38:38.060 --> 00:38:41.480]   all this kind of stuff that nobody can define.
[00:38:41.480 --> 00:38:44.880]   And it's something that we together
[00:38:44.880 --> 00:38:46.840]   as a human species have to define.
[00:38:46.840 --> 00:38:50.640]   Like what is truth and how to help AI systems generate that.
[00:38:50.640 --> 00:38:53.160]   'Cause one of the things language models do really well
[00:38:53.160 --> 00:38:55.940]   is generate convincing sounding things
[00:38:55.940 --> 00:38:57.600]   that can be completely wrong.
[00:38:58.600 --> 00:39:03.600]   And so how do you align it to be less wrong?
[00:39:03.600 --> 00:39:06.760]   And part of that is the training
[00:39:06.760 --> 00:39:08.720]   and part of that is the alignment
[00:39:08.720 --> 00:39:10.680]   and however you do the alignment stage.
[00:39:10.680 --> 00:39:14.000]   And just like you said, it's a very new
[00:39:14.000 --> 00:39:16.640]   and a very open research problem.
[00:39:16.640 --> 00:39:20.200]   - Yeah, and I think that there's also a lot of questions
[00:39:20.200 --> 00:39:23.940]   about whether the current architecture for LLMs,
[00:39:25.320 --> 00:39:28.180]   as you continue scaling it, what happens.
[00:39:28.180 --> 00:39:33.640]   I mean, a lot of what's been exciting in the last year
[00:39:33.640 --> 00:39:35.960]   is that there's clearly a qualitative breakthrough
[00:39:35.960 --> 00:39:40.560]   where with some of the GPT models that OpenAI put out
[00:39:40.560 --> 00:39:43.560]   and that others have been able to do as well.
[00:39:43.560 --> 00:39:45.840]   I think it reached a kind of level of quality
[00:39:45.840 --> 00:39:48.920]   where people are like, wow, this feels different
[00:39:48.920 --> 00:39:52.840]   and like it's gonna be able to be the foundation
[00:39:52.840 --> 00:39:55.000]   for building a lot of awesome products
[00:39:55.000 --> 00:39:57.000]   and experiences and value.
[00:39:57.000 --> 00:39:58.920]   But I think that the other realization that people have
[00:39:58.920 --> 00:40:00.880]   is wow, we just made a breakthrough.
[00:40:00.880 --> 00:40:05.840]   If there are other breakthroughs quickly,
[00:40:05.840 --> 00:40:06.840]   then I think that there's the sense
[00:40:06.840 --> 00:40:11.120]   that maybe we're closer to general intelligence.
[00:40:11.120 --> 00:40:13.320]   But I think that that idea is predicated on the idea
[00:40:13.320 --> 00:40:16.440]   that I think people believe that there's still generally
[00:40:16.440 --> 00:40:18.280]   a bunch of additional breakthroughs to make
[00:40:18.280 --> 00:40:21.440]   and that we just don't know
[00:40:21.440 --> 00:40:23.120]   how long it's gonna take to get there.
[00:40:23.120 --> 00:40:25.040]   And one view that some people have,
[00:40:25.040 --> 00:40:27.920]   this doesn't tend to be my view as much,
[00:40:27.920 --> 00:40:32.160]   is that simply scaling the current LLMs
[00:40:32.160 --> 00:40:35.720]   and getting to higher parameter count models by itself
[00:40:35.720 --> 00:40:37.440]   will get to something that is closer
[00:40:37.440 --> 00:40:40.280]   to general intelligence.
[00:40:40.280 --> 00:40:43.160]   But I don't know, I tend to think
[00:40:43.160 --> 00:40:48.160]   that there's probably more fundamental steps
[00:40:48.160 --> 00:40:50.000]   that need to be taken along the way there.
[00:40:50.000 --> 00:40:55.000]   But still, the leaps taken with this extra alignment step
[00:40:55.000 --> 00:40:58.560]   is quite incredible, quite surprising to a lot of folks.
[00:40:58.560 --> 00:41:02.160]   And on top of that, when you start to have
[00:41:02.160 --> 00:41:04.200]   hundreds of millions of people potentially
[00:41:04.200 --> 00:41:07.200]   using a product that integrates that,
[00:41:07.200 --> 00:41:10.800]   you can start to see civilization transforming effects
[00:41:10.800 --> 00:41:15.020]   before you achieve super, quote unquote, super intelligence.
[00:41:15.020 --> 00:41:18.040]   It could be super transformative
[00:41:18.040 --> 00:41:20.200]   without being a super intelligence.
[00:41:20.200 --> 00:41:22.480]   - Oh yeah, I mean, I think that there are gonna be
[00:41:22.480 --> 00:41:26.040]   a lot of amazing products and value that can be created
[00:41:26.040 --> 00:41:27.880]   with the current level of technology.
[00:41:27.880 --> 00:41:33.960]   To some degree, I'm excited to work on a lot
[00:41:33.960 --> 00:41:35.720]   of those products over the next few years.
[00:41:35.720 --> 00:41:37.760]   And I think it would just create
[00:41:37.760 --> 00:41:39.440]   a tremendous amount of whiplash
[00:41:39.440 --> 00:41:41.840]   if the number of breakthroughs keeps,
[00:41:41.840 --> 00:41:44.000]   like if there keep on being stacked breakthroughs,
[00:41:44.000 --> 00:41:45.320]   because I think to some degree,
[00:41:45.320 --> 00:41:47.040]   industry and the world needs some time
[00:41:47.040 --> 00:41:49.200]   to kind of build these breakthroughs
[00:41:49.200 --> 00:41:51.920]   into the products and experiences that we all use
[00:41:51.920 --> 00:41:53.980]   so that we can actually benefit from them.
[00:41:53.980 --> 00:41:58.880]   But I don't know, I think that there's just
[00:41:58.880 --> 00:42:02.480]   like an awesome amount of stuff to do.
[00:42:02.480 --> 00:42:05.240]   I mean, I think about like all of the,
[00:42:05.240 --> 00:42:06.520]   I don't know, small businesses
[00:42:06.520 --> 00:42:08.740]   or individual entrepreneurs out there who,
[00:42:08.740 --> 00:42:13.520]   you know, now we're gonna be able to get help coding
[00:42:13.520 --> 00:42:15.280]   the things that they need to go build things
[00:42:15.280 --> 00:42:16.880]   or designing the things that they need
[00:42:16.880 --> 00:42:20.060]   or we'll be able to use these models
[00:42:20.060 --> 00:42:21.480]   to be able to do customer support
[00:42:21.480 --> 00:42:24.080]   for the people that they're serving,
[00:42:24.080 --> 00:42:26.040]   you know, over WhatsApp without having to,
[00:42:26.040 --> 00:42:28.520]   you know, I think that that's just gonna be,
[00:42:28.520 --> 00:42:32.000]   I just think that this is all gonna be super exciting.
[00:42:32.000 --> 00:42:34.840]   It's gonna create better experiences for people
[00:42:34.840 --> 00:42:37.760]   and just unlock a ton of innovation and value.
[00:42:37.760 --> 00:42:41.360]   So I don't know if you know, but you know, what is it?
[00:42:41.360 --> 00:42:46.240]   Over 3 billion people use WhatsApp, Facebook and Instagram.
[00:42:47.240 --> 00:42:51.880]   So any kind of AI-fueled products that go into that,
[00:42:51.880 --> 00:42:54.760]   like we're talking about anything with LLMs
[00:42:54.760 --> 00:42:57.000]   will have a tremendous amount of impact.
[00:42:57.000 --> 00:43:01.800]   Do you have ideas and thoughts about possible products
[00:43:01.800 --> 00:43:04.960]   that might start being integrated
[00:43:04.960 --> 00:43:08.960]   into these platforms used by so many people?
[00:43:08.960 --> 00:43:11.760]   - Yeah, I think there's three main categories
[00:43:11.760 --> 00:43:13.360]   of things that we're working on.
[00:43:13.360 --> 00:43:18.360]   The first that I think is probably the most interesting
[00:43:18.360 --> 00:43:27.720]   is, you know, there's this notion of like,
[00:43:27.720 --> 00:43:30.600]   you're gonna have an assistant or an agent
[00:43:30.600 --> 00:43:31.420]   who you can talk to.
[00:43:31.420 --> 00:43:33.600]   And I think probably the biggest thing
[00:43:33.600 --> 00:43:36.280]   that's different about my view of how this plays out
[00:43:36.280 --> 00:43:40.440]   from what I see with OpenAI and Google and others
[00:43:40.440 --> 00:43:43.160]   is, you know, everyone else is building like
[00:43:43.160 --> 00:43:45.360]   the one singular AI, right?
[00:43:45.360 --> 00:43:47.400]   It's like, okay, you talk to chat GPT
[00:43:47.400 --> 00:43:50.440]   or you talk to Bard or you talk to Bing.
[00:43:50.440 --> 00:43:55.440]   And my view is that there are going to be
[00:43:55.440 --> 00:43:59.520]   a lot of different AIs that people are gonna wanna engage
[00:43:59.520 --> 00:44:02.120]   with just like you wanna use, you know,
[00:44:02.120 --> 00:44:03.920]   a number of different apps for different things
[00:44:03.920 --> 00:44:06.380]   and you have relationships with different people
[00:44:06.380 --> 00:44:10.100]   in your life who fill different emotional roles for you.
[00:44:10.100 --> 00:44:16.600]   And so I think that they're gonna be,
[00:44:16.600 --> 00:44:18.820]   people have a reason that I think you don't just want
[00:44:18.820 --> 00:44:20.260]   like a singular AI.
[00:44:20.260 --> 00:44:23.380]   And that I think is probably the biggest distinction
[00:44:23.380 --> 00:44:25.460]   in terms of how I think about this.
[00:44:25.460 --> 00:44:28.700]   And a bunch of these things, I think you'll want an assistant.
[00:44:28.700 --> 00:44:30.980]   I mean, I mentioned a couple of these before.
[00:44:30.980 --> 00:44:33.420]   I think like every creator who you interact with
[00:44:33.420 --> 00:44:35.900]   will ultimately want some kind of AI
[00:44:35.900 --> 00:44:38.580]   that can proxy them and be something
[00:44:38.580 --> 00:44:40.360]   that their fans can interact with
[00:44:40.360 --> 00:44:43.520]   or that allows them to interact with their fans.
[00:44:43.520 --> 00:44:46.380]   This is like the common creator promise.
[00:44:46.380 --> 00:44:48.620]   Everyone's trying to build a community
[00:44:48.620 --> 00:44:50.260]   and engage with people and they want tools
[00:44:50.260 --> 00:44:51.780]   to be able to amplify themselves more
[00:44:51.780 --> 00:44:52.900]   and be able to do that.
[00:44:52.900 --> 00:44:57.700]   But you only have 24 hours in a day.
[00:44:57.700 --> 00:45:02.000]   So I think having the ability to basically
[00:45:02.000 --> 00:45:04.220]   like bottle up your personality
[00:45:04.220 --> 00:45:08.460]   and or like give your fans information
[00:45:08.460 --> 00:45:10.900]   about when you're performing a concert or something like that.
[00:45:10.900 --> 00:45:12.540]   I mean, that I think is gonna be something
[00:45:12.540 --> 00:45:14.980]   that's super valuable, but it's not just that,
[00:45:14.980 --> 00:45:16.500]   you know, again, it's not this idea that
[00:45:16.500 --> 00:45:18.700]   I think people are gonna want just one singular AI.
[00:45:18.700 --> 00:45:20.900]   I think you're gonna wanna interact
[00:45:20.900 --> 00:45:22.440]   with a lot of different entities.
[00:45:22.440 --> 00:45:24.340]   And then I think there's the business version of this too,
[00:45:24.340 --> 00:45:26.840]   which we've touched on a couple of times, which is,
[00:45:26.840 --> 00:45:31.580]   I think every business in the world is gonna want
[00:45:31.580 --> 00:45:34.700]   basically an AI that, you know,
[00:45:34.700 --> 00:45:37.660]   it's like you have your page on Instagram or Facebook
[00:45:37.660 --> 00:45:40.260]   or WhatsApp or whatever, and you wanna point people
[00:45:40.260 --> 00:45:43.540]   to an AI that people can interact with,
[00:45:43.540 --> 00:45:44.940]   but you wanna know that that AI
[00:45:44.940 --> 00:45:46.220]   is only gonna sell your products.
[00:45:46.220 --> 00:45:49.220]   You don't want it recommending your competitors stuff.
[00:45:49.220 --> 00:45:51.940]   So it's not like there can be like just, you know,
[00:45:51.940 --> 00:45:55.340]   one singular AI that can answer all the questions
[00:45:55.340 --> 00:45:56.820]   for a person because, you know,
[00:45:56.820 --> 00:45:59.700]   that AI might not actually be aligned with you
[00:45:59.700 --> 00:46:03.180]   as a business to really just do the best job
[00:46:03.180 --> 00:46:05.540]   providing support for your product.
[00:46:05.540 --> 00:46:08.020]   So I think that there's gonna be a clear need
[00:46:08.020 --> 00:46:11.860]   in the market and in people's lives
[00:46:11.860 --> 00:46:14.180]   for there to be a bunch of these.
[00:46:14.180 --> 00:46:17.260]   - Part of that is figuring out the research,
[00:46:17.260 --> 00:46:20.220]   the technology that enables the personalization
[00:46:20.220 --> 00:46:21.460]   that you're talking about.
[00:46:21.460 --> 00:46:24.820]   So not one centralized God-like LLM,
[00:46:24.820 --> 00:46:28.340]   but one, just a huge diversity of them
[00:46:28.340 --> 00:46:31.340]   that's fine-tuned to particular needs, particular styles,
[00:46:31.340 --> 00:46:34.420]   particular businesses, particular brands,
[00:46:34.420 --> 00:46:35.540]   all that kind of stuff.
[00:46:35.540 --> 00:46:37.460]   - And also enabling, just enabling people
[00:46:37.460 --> 00:46:40.140]   to create them really easily for the, you know,
[00:46:40.140 --> 00:46:42.660]   for your own business, or if you're a creator
[00:46:42.660 --> 00:46:45.220]   to be able to help you engage with your fans.
[00:46:45.220 --> 00:46:48.420]   And I've meant that's, so yeah,
[00:46:48.420 --> 00:46:51.420]   I think that there's a clear kind of interesting
[00:46:51.420 --> 00:46:55.060]   product direction here that I think is fairly unique
[00:46:55.060 --> 00:46:59.500]   from what any of the other big companies are taking.
[00:46:59.500 --> 00:47:02.100]   It also aligns well with this sort of open source approach,
[00:47:02.100 --> 00:47:04.020]   because again, we sort of believe
[00:47:04.020 --> 00:47:08.220]   in this more community-oriented, more democratic approach
[00:47:08.220 --> 00:47:11.140]   to building out the products and technology around this.
[00:47:11.140 --> 00:47:12.900]   We don't think that there's gonna be the one true thing.
[00:47:12.900 --> 00:47:16.060]   We think that there should be kind of a lot of development.
[00:47:16.060 --> 00:47:18.300]   So that part of things I think
[00:47:18.300 --> 00:47:19.180]   is gonna be really interesting.
[00:47:19.180 --> 00:47:21.500]   And we could go, probably spend a lot of time
[00:47:21.500 --> 00:47:23.940]   talking about that and the kind of implications
[00:47:23.940 --> 00:47:26.980]   of that approach being different
[00:47:26.980 --> 00:47:28.420]   from what others are taking.
[00:47:28.420 --> 00:47:31.180]   But there's a bunch of other simpler things
[00:47:31.180 --> 00:47:32.140]   that I think we're also gonna do,
[00:47:32.140 --> 00:47:34.540]   just going back to your question around
[00:47:34.540 --> 00:47:37.140]   how this finds its way into, like, what do we build?
[00:47:37.140 --> 00:47:40.780]   There are gonna be a lot of simpler things around,
[00:47:40.780 --> 00:47:47.060]   okay, you post photos on Instagram and Facebook
[00:47:47.060 --> 00:47:49.580]   and, you know, and WhatsApp and Messenger,
[00:47:49.580 --> 00:47:51.820]   and like, you want the photos to look as good as possible.
[00:47:51.820 --> 00:47:54.580]   So like having an AI that you can just like take a photo
[00:47:54.580 --> 00:47:56.140]   and then just tell it like,
[00:47:56.140 --> 00:47:58.260]   okay, I wanna edit this thing or describe this.
[00:47:58.260 --> 00:47:59.700]   It's like, I think we're gonna have tools
[00:47:59.700 --> 00:48:01.340]   that are just way better
[00:48:01.340 --> 00:48:03.900]   than what we've historically had on this.
[00:48:03.900 --> 00:48:07.140]   And that's more in the image and media generation side
[00:48:07.140 --> 00:48:08.700]   than the large language model side,
[00:48:08.700 --> 00:48:13.540]   but it all kind of plays off of advances in the same space.
[00:48:13.540 --> 00:48:15.460]   So there are a lot of tools
[00:48:15.460 --> 00:48:16.420]   that I think are just gonna get built
[00:48:16.420 --> 00:48:17.620]   into every one of our products.
[00:48:17.620 --> 00:48:19.580]   I think every single thing that we do
[00:48:19.580 --> 00:48:22.500]   is gonna basically get evolved in this direction, right?
[00:48:22.500 --> 00:48:23.900]   It's like in the future,
[00:48:23.900 --> 00:48:25.500]   if you're advertising on our services,
[00:48:25.500 --> 00:48:29.820]   like, do you need to make your own kind of ad creative?
[00:48:29.820 --> 00:48:33.260]   It's no, you'll just, you know, you just tell us,
[00:48:33.260 --> 00:48:38.260]   okay, I'm a dog walker and I am willing to walk people's dogs
[00:48:38.260 --> 00:48:41.900]   and help me find the right people
[00:48:41.900 --> 00:48:45.900]   and like create the ad unit that will perform the best
[00:48:45.900 --> 00:48:48.740]   and like give an objective to the system.
[00:48:48.740 --> 00:48:52.380]   And it just kind of like connects you with the right people.
[00:48:52.380 --> 00:48:54.820]   - Well, that's a super powerful idea
[00:48:54.820 --> 00:48:57.100]   of generating the language,
[00:48:57.100 --> 00:49:02.100]   almost like a rigorous A/B testing for you
[00:49:02.100 --> 00:49:07.460]   that works to find the best customer for your thing.
[00:49:07.460 --> 00:49:10.860]   I mean, to me, advertisement, when done well,
[00:49:10.860 --> 00:49:14.820]   just finds a good match between a human being
[00:49:14.820 --> 00:49:18.060]   and a thing that will make that human being happy.
[00:49:18.060 --> 00:49:19.020]   - Yeah, totally.
[00:49:19.020 --> 00:49:21.180]   - And do that as efficiently as possible.
[00:49:21.180 --> 00:49:23.660]   - When it's done well, people actually like it.
[00:49:23.660 --> 00:49:25.700]   You know, I think that there's a lot of examples
[00:49:25.700 --> 00:49:27.460]   where it's not done well and it's annoying
[00:49:27.460 --> 00:49:29.940]   and I think that that's what kind of gives it a bad rap.
[00:49:29.940 --> 00:49:33.380]   But yeah, and a lot of this stuff is possible today.
[00:49:33.380 --> 00:49:34.900]   I mean, obviously A/B testing stuff
[00:49:34.900 --> 00:49:36.860]   is built into a lot of these frameworks.
[00:49:36.860 --> 00:49:39.380]   The thing that's new is having technology
[00:49:39.380 --> 00:49:42.340]   that can generate the ideas for you about what to A/B test.
[00:49:42.340 --> 00:49:43.260]   So I think that that's exciting.
[00:49:43.260 --> 00:49:46.020]   So this will just be across like everything
[00:49:46.020 --> 00:49:46.860]   that we're doing,
[00:49:46.860 --> 00:49:48.420]   all the metaverse stuff that we're doing, right?
[00:49:48.420 --> 00:49:50.340]   It's like you wanna create worlds in the future,
[00:49:50.340 --> 00:49:51.540]   you'll just describe them
[00:49:51.540 --> 00:49:53.580]   and it'll create the code for you.
[00:49:53.580 --> 00:49:54.420]   So--
[00:49:54.420 --> 00:49:57.500]   - So natural language becomes the interface we use
[00:49:57.500 --> 00:50:01.260]   for all the ways we interact with the computer,
[00:50:01.260 --> 00:50:03.820]   with the digital--
[00:50:03.820 --> 00:50:05.820]   - More of them, yeah, yeah, totally.
[00:50:05.820 --> 00:50:07.740]   - Yeah, which is what everyone can do
[00:50:07.740 --> 00:50:08.860]   using natural language.
[00:50:08.860 --> 00:50:11.820]   And with translation, you can do it in any kind of language.
[00:50:11.820 --> 00:50:14.900]   I mean, for the personalization,
[00:50:14.900 --> 00:50:17.620]   it's really, really, really interesting.
[00:50:17.620 --> 00:50:18.460]   - Yeah.
[00:50:18.460 --> 00:50:20.660]   - It unlocks so many possible things.
[00:50:20.660 --> 00:50:22.140]   I mean, I, for one, look forward
[00:50:22.140 --> 00:50:24.580]   to creating a copy of myself.
[00:50:24.580 --> 00:50:26.180]   - I know, we talked about this last time.
[00:50:26.180 --> 00:50:29.700]   - But this has, since the last time, this becomes--
[00:50:29.700 --> 00:50:31.580]   - Now we're closer.
[00:50:31.580 --> 00:50:32.700]   - Much closer.
[00:50:32.700 --> 00:50:34.540]   Like I can literally just having interacted
[00:50:34.540 --> 00:50:36.100]   with some of these language models,
[00:50:36.100 --> 00:50:37.740]   I can see the absurd situation
[00:50:37.740 --> 00:50:42.740]   where I'll have a large or a Lex language model,
[00:50:44.780 --> 00:50:47.980]   and I'll have to have a conversation with him
[00:50:47.980 --> 00:50:50.060]   about like, hey, listen,
[00:50:50.060 --> 00:50:51.540]   like you're just getting out of line
[00:50:51.540 --> 00:50:53.580]   and having a conversation where you fine tune that thing
[00:50:53.580 --> 00:50:56.020]   to be a little bit more respectful or something like this.
[00:50:56.020 --> 00:51:00.020]   I mean, that's going to be the,
[00:51:00.020 --> 00:51:03.140]   that seems like an amazing product
[00:51:03.140 --> 00:51:06.500]   for businesses, for humans,
[00:51:06.500 --> 00:51:11.340]   just not just the assistant that's facing the individual,
[00:51:11.340 --> 00:51:14.300]   but the assistant that represents the individual
[00:51:14.300 --> 00:51:18.060]   to the public, both directions.
[00:51:18.060 --> 00:51:22.220]   There's basically a layer that is the AI system
[00:51:22.220 --> 00:51:25.780]   through which you interact with the outside world,
[00:51:25.780 --> 00:51:28.620]   with the outside world that has humans in it.
[00:51:28.620 --> 00:51:30.100]   That's really interesting.
[00:51:30.100 --> 00:51:33.540]   And you that have social networks
[00:51:33.540 --> 00:51:35.740]   that connect billions of people,
[00:51:35.740 --> 00:51:39.860]   it seems like a heck of a large scale place
[00:51:39.860 --> 00:51:42.260]   to test some of this stuff out.
[00:51:42.260 --> 00:51:43.620]   - Yeah, I mean, I think part of the reason
[00:51:43.620 --> 00:51:45.740]   why creators will want to do this
[00:51:45.740 --> 00:51:47.420]   is because they already have the communities
[00:51:47.420 --> 00:51:49.260]   on our services.
[00:51:49.260 --> 00:51:53.660]   Yeah, and a lot of the interface for this stuff today
[00:51:53.660 --> 00:51:55.380]   are chat type interfaces.
[00:51:55.380 --> 00:51:58.660]   And between WhatsApp and Messenger,
[00:51:58.660 --> 00:52:00.740]   I think that those are just great,
[00:52:00.740 --> 00:52:03.660]   great ways to interact with people.
[00:52:03.660 --> 00:52:04.980]   - So some of this is philosophy,
[00:52:04.980 --> 00:52:07.820]   but do you see a near term future
[00:52:07.820 --> 00:52:11.460]   where you have some of the people you're friends with
[00:52:11.460 --> 00:52:15.780]   are AI systems on these social networks,
[00:52:15.780 --> 00:52:19.500]   on Facebook, on Instagram, even on WhatsApp,
[00:52:19.500 --> 00:52:23.500]   having conversations where some heterogeneous,
[00:52:23.500 --> 00:52:25.340]   some is human, some is AI?
[00:52:25.340 --> 00:52:26.940]   - I think we'll get to that.
[00:52:26.940 --> 00:52:31.240]   And if only just empirically looking at,
[00:52:31.240 --> 00:52:35.660]   then Microsoft released this thing called Chowice
[00:52:35.660 --> 00:52:38.020]   several years ago in China,
[00:52:38.020 --> 00:52:41.900]   and it was a pre-LLM chatbot technology
[00:52:41.900 --> 00:52:46.180]   that was a lot simpler than what's possible today.
[00:52:46.180 --> 00:52:48.300]   And I think it was like tens of millions of people
[00:52:48.300 --> 00:52:51.340]   were using this and just really,
[00:52:51.340 --> 00:52:54.580]   it became quite attached and built relationships with it.
[00:52:54.580 --> 00:52:58.820]   And I think that there's services today like Replica
[00:52:58.820 --> 00:53:01.340]   where people are doing things like that.
[00:53:01.340 --> 00:53:07.020]   So I think that there's certainly needs
[00:53:07.020 --> 00:53:11.020]   for companionship that people have, older people.
[00:53:11.020 --> 00:53:16.740]   And I think most people probably don't have as many friends
[00:53:16.740 --> 00:53:17.980]   as they would like to have.
[00:53:17.980 --> 00:53:18.820]   If you look at,
[00:53:18.820 --> 00:53:21.500]   there's some interesting demographic studies
[00:53:21.500 --> 00:53:25.700]   around the average person has,
[00:53:25.700 --> 00:53:30.220]   the number of close friends that they have is fewer today
[00:53:30.220 --> 00:53:32.060]   than it was 15 years ago.
[00:53:32.060 --> 00:53:34.580]   And I mean, that gets to like,
[00:53:34.580 --> 00:53:38.180]   this is like the core thing that I think about
[00:53:38.180 --> 00:53:40.740]   in terms of building services that help connect people.
[00:53:40.740 --> 00:53:43.940]   So I think you'll get tools that help people connect
[00:53:43.940 --> 00:53:47.260]   with each other are gonna be the primary thing
[00:53:47.260 --> 00:53:48.740]   that we wanna do.
[00:53:48.740 --> 00:53:52.740]   So you can imagine AI assistants that,
[00:53:52.740 --> 00:53:54.140]   just do a better job of reminding you
[00:53:54.140 --> 00:53:55.180]   when it's your friend's birthday
[00:53:55.180 --> 00:53:56.660]   and how you could celebrate them.
[00:53:56.660 --> 00:53:58.700]   Right, it's like right now we have like the little box
[00:53:58.700 --> 00:54:00.780]   in the corner of the website that tells you
[00:54:00.780 --> 00:54:02.340]   whose birthday it is and stuff like that.
[00:54:02.340 --> 00:54:06.660]   But it's, but at some level you don't just wanna like
[00:54:06.660 --> 00:54:08.660]   send everyone a note that says the same note
[00:54:08.660 --> 00:54:10.660]   saying happy birthday with an emoji.
[00:54:10.660 --> 00:54:13.940]   Right, so having something that's more of an,
[00:54:13.940 --> 00:54:16.100]   a social assistant in that sense,
[00:54:16.100 --> 00:54:19.180]   and like that can update you on what's going on
[00:54:19.180 --> 00:54:22.180]   in their life and like how you can reach out
[00:54:22.180 --> 00:54:24.900]   to them effectively, help you be a better friend.
[00:54:24.900 --> 00:54:28.140]   I think that that's something that's super powerful too.
[00:54:28.140 --> 00:54:30.500]   But yeah, beyond that,
[00:54:31.420 --> 00:54:33.900]   and there are all these different flavors
[00:54:33.900 --> 00:54:38.180]   of kind of personal AI is that I think could exist.
[00:54:38.180 --> 00:54:41.380]   So I think an assistant is sort of the kind of simplest one
[00:54:41.380 --> 00:54:42.580]   to wrap your head around,
[00:54:42.580 --> 00:54:47.580]   but I think a mentor or a life coach,
[00:54:47.580 --> 00:54:50.300]   if someone who can give you advice,
[00:54:50.300 --> 00:54:51.820]   who's maybe like a bit of a cheerleader
[00:54:51.820 --> 00:54:53.740]   who can help pick you up through all the challenges
[00:54:53.740 --> 00:54:58.140]   that inevitably we all go through on a daily basis.
[00:54:58.140 --> 00:54:59.460]   And that there's probably,
[00:54:59.940 --> 00:55:01.540]   some role for something like that.
[00:55:01.540 --> 00:55:03.220]   And then all the way,
[00:55:03.220 --> 00:55:05.580]   you can probably just go through a lot of the different type
[00:55:05.580 --> 00:55:08.340]   of kind of functional relationships
[00:55:08.340 --> 00:55:10.020]   that people have in their life.
[00:55:10.020 --> 00:55:12.700]   And I would bet that there will be companies out there
[00:55:12.700 --> 00:55:15.620]   that take a crack at a lot of these things.
[00:55:15.620 --> 00:55:17.140]   So I don't know,
[00:55:17.140 --> 00:55:18.820]   I think it's part of the interesting innovation
[00:55:18.820 --> 00:55:22.060]   that's gonna exist is that there are certainly a lot,
[00:55:22.060 --> 00:55:25.020]   like education tutors, right?
[00:55:25.020 --> 00:55:28.860]   It's like, I mean, I just look at my kids learning to code
[00:55:28.860 --> 00:55:31.060]   and they love it,
[00:55:31.060 --> 00:55:33.300]   but it's like they get stuck on a question
[00:55:33.300 --> 00:55:36.460]   and they have to wait till I can help answer it, right?
[00:55:36.460 --> 00:55:37.820]   Or someone else who they know
[00:55:37.820 --> 00:55:39.580]   can help answer the question in the future.
[00:55:39.580 --> 00:55:41.900]   They'll just, there'll be like a coding assistant
[00:55:41.900 --> 00:55:45.780]   that they have that is designed to be perfect
[00:55:45.780 --> 00:55:47.980]   for teaching a five and a seven year old how to code.
[00:55:47.980 --> 00:55:50.900]   And they'll just be able to ask questions all the time
[00:55:50.900 --> 00:55:53.340]   and be extremely patient.
[00:55:53.340 --> 00:55:55.540]   It's never gonna get annoyed at them, right?
[00:55:56.580 --> 00:55:58.740]   I think that there are all these different
[00:55:58.740 --> 00:56:00.860]   kind of relationships or functional relationships
[00:56:00.860 --> 00:56:05.640]   that we have in our lives that are really interesting.
[00:56:05.640 --> 00:56:07.500]   And I think one of the big questions is like,
[00:56:07.500 --> 00:56:09.860]   okay, is this all gonna just get bucketed into
[00:56:09.860 --> 00:56:12.300]   one singular AI?
[00:56:12.300 --> 00:56:14.600]   I just don't think so.
[00:56:14.600 --> 00:56:15.580]   - Do you think about,
[00:56:15.580 --> 00:56:18.060]   this is actually a question from Reddit,
[00:56:18.060 --> 00:56:21.300]   what the long-term effects of human communication
[00:56:21.300 --> 00:56:26.140]   when people can "talk with" others through a chatbot
[00:56:26.140 --> 00:56:28.580]   that augments their language automatically
[00:56:28.580 --> 00:56:30.360]   rather than developing social skills
[00:56:30.360 --> 00:56:32.020]   by making mistakes and learning?
[00:56:32.020 --> 00:56:37.260]   Will people just communicate by grunts in a generation?
[00:56:37.260 --> 00:56:40.460]   Do you think about long-term effects at scale
[00:56:40.460 --> 00:56:43.020]   the integration of AI in our social interaction?
[00:56:43.020 --> 00:56:46.620]   - Yeah, I mean, I think it's mostly good.
[00:56:46.620 --> 00:56:50.300]   I mean, that question was sort of framed in a negative way.
[00:56:50.300 --> 00:56:52.700]   But I mean, we were talking before about
[00:56:52.700 --> 00:56:54.900]   language models helping you communicate with,
[00:56:54.900 --> 00:56:56.140]   it was like language translation,
[00:56:56.140 --> 00:56:56.980]   helping you communicate with people
[00:56:56.980 --> 00:56:58.580]   who don't speak your language.
[00:56:58.580 --> 00:57:00.860]   I mean, at some level,
[00:57:00.860 --> 00:57:03.260]   what all this social technology is doing
[00:57:03.260 --> 00:57:08.260]   is helping people express themselves better to people
[00:57:08.260 --> 00:57:13.260]   in situations where they would otherwise
[00:57:13.260 --> 00:57:14.180]   have a hard time doing that.
[00:57:14.180 --> 00:57:16.460]   So part of it might be okay 'cause you speak
[00:57:16.460 --> 00:57:17.340]   a language that I don't know.
[00:57:17.340 --> 00:57:19.100]   That's a pretty basic one that,
[00:57:19.100 --> 00:57:20.780]   you know, I don't think people are gonna look at that
[00:57:20.780 --> 00:57:24.300]   and say, it's sad that we have the capacity to do that
[00:57:24.300 --> 00:57:26.180]   because I should have just learned your language, right?
[00:57:26.180 --> 00:57:27.980]   I mean, that's pretty high bar.
[00:57:27.980 --> 00:57:32.980]   But overall, I'd say there are all these impediments
[00:57:32.980 --> 00:57:38.380]   and language is an imperfect way
[00:57:38.380 --> 00:57:41.580]   for people to express thoughts and ideas.
[00:57:41.580 --> 00:57:43.440]   It's one of the best that we have.
[00:57:43.440 --> 00:57:46.540]   We have that, we have art, we have code.
[00:57:46.540 --> 00:57:48.980]   - But language is also a mapping of the way you think,
[00:57:48.980 --> 00:57:51.380]   the way you see the world, who you are.
[00:57:51.380 --> 00:57:52.940]   And one of the applications,
[00:57:52.940 --> 00:57:56.380]   I've recently talked to a person who's,
[00:57:56.380 --> 00:57:58.020]   actually a jiu-jitsu instructor.
[00:57:58.020 --> 00:58:03.100]   He said that when he emails parents
[00:58:03.100 --> 00:58:05.860]   about their son and daughter,
[00:58:05.860 --> 00:58:10.500]   that they can improve their discipline in class and so on,
[00:58:10.500 --> 00:58:13.060]   he often finds that he comes off a bit
[00:58:13.060 --> 00:58:15.020]   of more of an asshole than he would like.
[00:58:15.020 --> 00:58:18.820]   So he uses GPT to translate his original email
[00:58:18.820 --> 00:58:20.740]   into a nicer email.
[00:58:20.740 --> 00:58:21.580]   - Yeah, we hear this all the time.
[00:58:21.580 --> 00:58:22.900]   - A more polite one.
[00:58:22.900 --> 00:58:23.760]   - We hear this all the time.
[00:58:23.760 --> 00:58:25.920]   A lot of creators on our services tell us
[00:58:25.920 --> 00:58:28.220]   that one of the most stressful things
[00:58:28.220 --> 00:58:32.260]   is basically negotiating deals with brands and stuff,
[00:58:32.260 --> 00:58:33.300]   like the business side of it.
[00:58:33.300 --> 00:58:35.860]   Because they're like, I mean, they do their thing, right?
[00:58:35.860 --> 00:58:38.860]   And the creators, they're excellent at what they do
[00:58:38.860 --> 00:58:40.220]   and they just wanna connect with their community,
[00:58:40.220 --> 00:58:42.020]   but then they get really stressed.
[00:58:42.020 --> 00:58:45.740]   They go into their DMs and they see some brand
[00:58:45.740 --> 00:58:46.680]   wants to do something with them
[00:58:46.680 --> 00:58:49.420]   and they don't quite know how to negotiate
[00:58:49.420 --> 00:58:51.420]   or how to push back respectfully.
[00:58:51.420 --> 00:58:53.620]   And so I think building a tool
[00:58:53.620 --> 00:58:56.060]   that can actually allow them to do that well
[00:58:56.060 --> 00:58:58.780]   is one simple thing that I think
[00:58:58.780 --> 00:58:59.980]   is just an interesting thing
[00:58:59.980 --> 00:59:01.740]   that we've heard from a bunch of people
[00:59:01.740 --> 00:59:03.340]   that they'd be interested in.
[00:59:03.340 --> 00:59:06.240]   But going back to the broader idea,
[00:59:06.240 --> 00:59:10.100]   I don't know.
[00:59:10.100 --> 00:59:14.660]   I mean, Priscilla and I just had our third daughter.
[00:59:14.660 --> 00:59:15.500]   - Congratulations, by the way.
[00:59:15.500 --> 00:59:16.320]   - Thank you.
[00:59:16.320 --> 00:59:17.160]   Thanks.
[00:59:17.160 --> 00:59:20.180]   And it's like one of the saddest things in the world
[00:59:20.180 --> 00:59:22.180]   is seeing your baby cry, right?
[00:59:22.180 --> 00:59:24.980]   But it's like, why is that?
[00:59:24.980 --> 00:59:25.820]   Right?
[00:59:25.820 --> 00:59:28.100]   It's like, well, 'cause babies don't generally
[00:59:28.100 --> 00:59:30.060]   have much capacity to tell you
[00:59:30.060 --> 00:59:32.860]   what they care about otherwise.
[00:59:32.860 --> 00:59:33.700]   Right?
[00:59:33.700 --> 00:59:35.340]   And it's not actually just babies, right?
[00:59:35.340 --> 00:59:38.300]   It's my five-year-old daughter cries too
[00:59:38.300 --> 00:59:41.180]   because she sometimes has a hard time expressing
[00:59:41.180 --> 00:59:44.180]   what matters to her.
[00:59:44.180 --> 00:59:46.060]   And I was thinking about that and I was like,
[00:59:46.060 --> 00:59:48.380]   well, actually a lot of adults get very frustrated too
[00:59:48.380 --> 00:59:51.260]   because they have a hard time expressing things
[00:59:51.260 --> 00:59:54.540]   in a way that, going back to some of the early themes,
[00:59:54.540 --> 00:59:59.020]   that maybe is something that was a mistake
[00:59:59.020 --> 01:00:00.460]   or maybe they have pride or something like,
[01:00:00.460 --> 01:00:01.420]   all these things get in the way.
[01:00:01.420 --> 01:00:02.740]   So I don't know.
[01:00:02.740 --> 01:00:05.100]   I think that all of these different technologies
[01:00:05.100 --> 01:00:08.900]   that can help us navigate the social complexity
[01:00:08.900 --> 01:00:11.020]   and actually be able to better express
[01:00:11.020 --> 01:00:13.060]   what we're feeling and thinking,
[01:00:13.060 --> 01:00:14.980]   I think that's generally all good.
[01:00:14.980 --> 01:00:18.100]   And there are all these concerns like,
[01:00:18.100 --> 01:00:19.940]   okay, are people gonna have worse memories
[01:00:19.940 --> 01:00:21.860]   because you have Google to look things up?
[01:00:21.860 --> 01:00:24.780]   And I think in general, a generation later,
[01:00:24.780 --> 01:00:26.580]   you don't look back and lament that.
[01:00:26.580 --> 01:00:29.500]   I think it's just like, wow, we have so much more capacity
[01:00:29.500 --> 01:00:30.860]   to do so much more now.
[01:00:30.860 --> 01:00:32.980]   And I think that that'll be the case here too.
[01:00:32.980 --> 01:00:35.180]   - You can allocate those cognitive capabilities
[01:00:35.180 --> 01:00:38.940]   to deeper nuance thought.
[01:00:38.940 --> 01:00:39.780]   - Yeah.
[01:00:39.780 --> 01:00:42.860]   - But it's change.
[01:00:42.860 --> 01:00:46.340]   So just like with Google search,
[01:00:47.340 --> 01:00:51.860]   the addition of language models, large language models,
[01:00:51.860 --> 01:00:54.740]   you basically don't have to remember nearly as much.
[01:00:54.740 --> 01:00:57.860]   Just like with Stack Overflow for programming,
[01:00:57.860 --> 01:00:59.540]   now that these language models
[01:00:59.540 --> 01:01:01.220]   can generate code right there.
[01:01:01.220 --> 01:01:04.980]   I mean, I find that I write like maybe 80%, 90%
[01:01:04.980 --> 01:01:09.980]   of the code I write is non-generated first and then edited.
[01:01:09.980 --> 01:01:11.660]   I mean, so you don't have to remember
[01:01:11.660 --> 01:01:13.580]   how to write specifics of different functions.
[01:01:13.580 --> 01:01:14.900]   - Oh, but that's great.
[01:01:14.900 --> 01:01:19.580]   And it's also, it's not just the specific coding.
[01:01:19.580 --> 01:01:23.340]   I mean, in the context of a large company like this,
[01:01:23.340 --> 01:01:26.100]   I think before an engineer can sit down to code,
[01:01:26.100 --> 01:01:29.500]   they first need to figure out all of the libraries
[01:01:29.500 --> 01:01:32.820]   and dependencies that tens of thousands of people
[01:01:32.820 --> 01:01:34.740]   have written before them.
[01:01:34.740 --> 01:01:39.420]   And one of the things that I'm excited about
[01:01:39.420 --> 01:01:42.420]   that we're working on is it's not just tools
[01:01:42.420 --> 01:01:43.660]   that help engineers code,
[01:01:43.660 --> 01:01:46.140]   it's tools that can help summarize the whole knowledge base
[01:01:46.140 --> 01:01:48.380]   and help people be able to navigate
[01:01:48.380 --> 01:01:49.660]   all the internal information.
[01:01:49.660 --> 01:01:51.060]   I mean, I think that that's,
[01:01:51.060 --> 01:01:54.100]   in the experiments that I've done with this stuff,
[01:01:54.100 --> 01:01:56.580]   I mean, that's on the public stuff,
[01:01:56.580 --> 01:02:00.580]   you just ask one of these models
[01:02:00.580 --> 01:02:03.420]   to build you a script that does anything
[01:02:03.420 --> 01:02:05.100]   and it basically already understands
[01:02:05.100 --> 01:02:07.020]   what the best libraries are to do that thing
[01:02:07.020 --> 01:02:08.220]   and pulls them in automatically.
[01:02:08.220 --> 01:02:09.740]   It's, I mean, I think that's super powerful.
[01:02:09.740 --> 01:02:12.860]   That was always the most annoying part of coding
[01:02:12.860 --> 01:02:14.620]   was that you had to spend all this time
[01:02:14.620 --> 01:02:16.380]   actually figuring out what the resources were
[01:02:16.380 --> 01:02:17.420]   that you were supposed to import
[01:02:17.420 --> 01:02:19.420]   before you could actually start building the thing.
[01:02:19.420 --> 01:02:21.460]   - Yeah, I mean, there's, of course,
[01:02:21.460 --> 01:02:24.620]   the flip side of that, I think for the most part is positive,
[01:02:24.620 --> 01:02:27.860]   but the flip side is if you outsource
[01:02:27.860 --> 01:02:31.900]   that thinking to an AI model,
[01:02:31.900 --> 01:02:36.900]   you might miss nuanced mistakes and bugs.
[01:02:36.900 --> 01:02:40.060]   You lose the skill to find those bugs.
[01:02:40.060 --> 01:02:41.580]   And those bugs might be,
[01:02:41.580 --> 01:02:45.380]   the code looks very convincingly right,
[01:02:45.380 --> 01:02:47.920]   but it's actually wrong in a very subtle way.
[01:02:47.920 --> 01:02:53.060]   But that's the trade-off that we face
[01:02:53.060 --> 01:02:56.300]   as human civilization when we build
[01:02:56.300 --> 01:02:57.820]   more and more powerful tools.
[01:02:57.820 --> 01:03:02.060]   When we stand on the shoulders of taller and taller giants,
[01:03:02.060 --> 01:03:03.860]   we could do more, but then we forget
[01:03:03.860 --> 01:03:05.760]   how to do all the stuff that they did.
[01:03:05.760 --> 01:03:08.900]   It's a weird trade-off.
[01:03:08.900 --> 01:03:10.180]   - Yeah, I agree.
[01:03:10.180 --> 01:03:13.100]   I mean, I think it is very valuable in your life
[01:03:13.100 --> 01:03:15.580]   to be able to do basic things too.
[01:03:15.580 --> 01:03:19.580]   - Do you worry about some of the concerns
[01:03:19.580 --> 01:03:22.820]   of bots being present on social networks?
[01:03:22.820 --> 01:03:24.960]   More and more human-like bots
[01:03:24.960 --> 01:03:29.540]   that are not necessarily trying to do a good thing,
[01:03:29.540 --> 01:03:32.220]   or they might be explicitly trying to do a bad thing,
[01:03:32.220 --> 01:03:35.220]   like phishing scams, like social engineering,
[01:03:35.220 --> 01:03:36.360]   all that kind of stuff,
[01:03:36.360 --> 01:03:38.880]   which has always been a very difficult problem
[01:03:38.880 --> 01:03:39.840]   for social networks,
[01:03:39.840 --> 01:03:41.760]   but now it's becoming almost a more and more
[01:03:41.760 --> 01:03:43.120]   difficult problem.
[01:03:43.120 --> 01:03:46.040]   - Well, there's a few different parts of this.
[01:03:46.040 --> 01:03:47.980]   So one is,
[01:03:47.980 --> 01:03:51.600]   there are all these harms that we need
[01:03:51.600 --> 01:03:53.360]   to basically fight against and prevent.
[01:03:53.360 --> 01:03:57.120]   And that's been a lot of our focus
[01:03:57.120 --> 01:03:59.960]   over the last five or seven years,
[01:03:59.960 --> 01:04:03.840]   is basically ramping up very sophisticated AI systems,
[01:04:03.840 --> 01:04:05.120]   not generative AI systems,
[01:04:05.120 --> 01:04:07.240]   more kind of classical AI systems,
[01:04:07.240 --> 01:04:12.240]   to be able to categorize and classify and identify,
[01:04:12.240 --> 01:04:17.720]   okay, this post looks like it's promoting terrorism.
[01:04:17.720 --> 01:04:21.600]   This one is exploiting children.
[01:04:21.600 --> 01:04:25.200]   This one looks like it might be trying to incite violence.
[01:04:25.200 --> 01:04:28.040]   This one's an intellectual property violation.
[01:04:28.040 --> 01:04:31.960]   So there's like 18 different categories
[01:04:31.960 --> 01:04:35.400]   of violating kind of harmful content
[01:04:35.400 --> 01:04:37.800]   that we've had to build specific systems
[01:04:37.800 --> 01:04:38.680]   to be able to track.
[01:04:38.680 --> 01:04:42.400]   And I think it's certainly the case
[01:04:42.400 --> 01:04:47.320]   that advances in generative AI will test those.
[01:04:47.320 --> 01:04:52.280]   But at least so far, it's been the case,
[01:04:52.280 --> 01:04:55.120]   and I'm optimistic that it will continue to be the case,
[01:04:55.120 --> 01:04:58.960]   that we will be able to bring more computing power to bear
[01:04:58.960 --> 01:05:00.400]   to have even stronger AIs
[01:05:00.400 --> 01:05:02.080]   that can help defend against those things.
[01:05:02.080 --> 01:05:07.080]   So we've had to deal with some adversarial issues before.
[01:05:07.080 --> 01:05:09.680]   For some things like hate speech,
[01:05:09.680 --> 01:05:10.880]   it's like people aren't generally
[01:05:10.880 --> 01:05:12.920]   getting a lot more sophisticated.
[01:05:12.920 --> 01:05:14.840]   Like the average person,
[01:05:14.840 --> 01:05:18.520]   let's say someone's saying some kind of racist thing,
[01:05:18.520 --> 01:05:19.600]   it's like they're not necessarily
[01:05:19.600 --> 01:05:22.040]   getting more sophisticated at being racist.
[01:05:22.040 --> 01:05:24.840]   It's okay, so that the system can just find.
[01:05:24.840 --> 01:05:26.840]   But then there's other adversaries
[01:05:26.840 --> 01:05:29.160]   who actually are very sophisticated,
[01:05:29.160 --> 01:05:30.840]   like nation states doing things.
[01:05:30.840 --> 01:05:34.040]   And we find, whether it's Russia
[01:05:34.040 --> 01:05:36.120]   or just different countries
[01:05:36.120 --> 01:05:39.960]   that are basically standing up these networks of bots
[01:05:39.960 --> 01:05:44.040]   or inauthentic accounts is what we call them,
[01:05:44.040 --> 01:05:45.400]   'cause they're not necessarily bots.
[01:05:45.400 --> 01:05:47.160]   Some of them could actually be real people
[01:05:47.160 --> 01:05:50.400]   who are kind of masquerading as other people,
[01:05:50.400 --> 01:05:53.120]   but they're acting in a coordinated way.
[01:05:53.120 --> 01:05:57.280]   And some of that behavior has gotten very sophisticated
[01:05:57.280 --> 01:05:58.400]   and it's very adversarial.
[01:05:58.400 --> 01:05:59.760]   So they, each iteration,
[01:05:59.760 --> 01:06:03.040]   every time we find something and stop them,
[01:06:03.040 --> 01:06:04.240]   they kind of evolve their behavior.
[01:06:04.240 --> 01:06:06.400]   They don't just pack up their bags and go home and say,
[01:06:06.400 --> 01:06:08.440]   "Okay, we're not gonna try."
[01:06:08.440 --> 01:06:09.360]   At some point they might decide
[01:06:09.360 --> 01:06:12.040]   doing it on MetaServices is not worth it.
[01:06:12.040 --> 01:06:13.080]   They'll go do it on someone else
[01:06:13.080 --> 01:06:15.080]   if it's easier to do it in another place.
[01:06:15.080 --> 01:06:19.720]   But we have a fair amount of experience dealing with
[01:06:19.720 --> 01:06:22.720]   even those kind of adversarial attacks
[01:06:22.720 --> 01:06:24.640]   where they just keep on getting better and better.
[01:06:24.640 --> 01:06:26.800]   And I do think that as long as we can keep on
[01:06:26.800 --> 01:06:28.720]   putting more compute power against it,
[01:06:28.720 --> 01:06:31.120]   and if we're kind of one of the leaders
[01:06:31.120 --> 01:06:33.040]   in developing some of these AI models,
[01:06:33.040 --> 01:06:35.960]   I'm quite optimistic that we're gonna be able to keep on
[01:06:35.960 --> 01:06:40.800]   pushing against the kind of normal categories of harm
[01:06:40.800 --> 01:06:44.280]   that you talk about, fraud, scams, spam,
[01:06:44.280 --> 01:06:47.800]   IP violations, things like that.
[01:06:47.800 --> 01:06:50.760]   - What about like creating narratives and controversy?
[01:06:50.760 --> 01:06:55.320]   To me, it's kind of amazing how a small collection of,
[01:06:55.320 --> 01:06:56.160]   - Yeah.
[01:06:56.160 --> 01:06:57.840]   - what did you say, inauthentic accounts.
[01:06:57.840 --> 01:06:59.400]   So it could be bots, but it could be huge.
[01:06:59.400 --> 01:07:01.400]   - Yeah, I mean, we have sort of this funny name for it,
[01:07:01.400 --> 01:07:03.720]   but we call it coordinated inauthentic behavior.
[01:07:03.720 --> 01:07:07.160]   - Yeah, it's kind of incredible how a small collection
[01:07:07.160 --> 01:07:12.040]   of folks can create narratives, create stories.
[01:07:12.040 --> 01:07:12.960]   - Yeah.
[01:07:12.960 --> 01:07:14.920]   - Especially if they're viral.
[01:07:14.920 --> 01:07:16.560]   Especially if they have an element
[01:07:16.560 --> 01:07:21.120]   that can catalyze the virality of the narrative.
[01:07:21.120 --> 01:07:24.360]   - Yeah, and I think there the question is you have to be,
[01:07:24.360 --> 01:07:27.080]   I'm very specific about what is bad about it, right?
[01:07:27.080 --> 01:07:31.280]   Because I think a set of people coming together
[01:07:31.280 --> 01:07:34.800]   or organically bouncing ideas off each other
[01:07:34.800 --> 01:07:36.440]   and a narrative comes out of that
[01:07:36.440 --> 01:07:39.640]   is not necessarily a bad thing by itself
[01:07:39.640 --> 01:07:42.480]   if it's kind of authentic and organic.
[01:07:42.480 --> 01:07:43.600]   That's like a lot of what happens
[01:07:43.600 --> 01:07:45.640]   and how culture gets created and how art gets created
[01:07:45.640 --> 01:07:46.480]   and a lot of good stuff.
[01:07:46.480 --> 01:07:49.200]   So that's why we've kind of focused on this sense
[01:07:49.200 --> 01:07:51.560]   of coordinated inauthentic behavior.
[01:07:51.560 --> 01:07:53.560]   So it's like if you have a network of,
[01:07:53.560 --> 01:07:56.080]   whether it's bots, some people masquerading
[01:07:56.080 --> 01:07:57.320]   as different accounts,
[01:07:57.320 --> 01:08:01.880]   but you have kind of someone pulling the strings behind it
[01:08:01.880 --> 01:08:07.960]   and trying to kind of act as if this is a more organic set
[01:08:07.960 --> 01:08:09.360]   of behavior, but really it's not,
[01:08:09.360 --> 01:08:11.760]   it's just like one coordinated thing.
[01:08:11.760 --> 01:08:13.520]   That seems problematic to me, right?
[01:08:13.520 --> 01:08:14.920]   I mean, I don't think people should be able
[01:08:14.920 --> 01:08:19.240]   to have coordinated networks and not disclose it as such.
[01:08:20.360 --> 01:08:22.840]   But that again, we've been able to deploy
[01:08:22.840 --> 01:08:27.400]   pretty sophisticated AI and counter-terrorism groups
[01:08:27.400 --> 01:08:29.360]   and things like that to be able to identify
[01:08:29.360 --> 01:08:33.880]   a fair number of these coordinated inauthentic networks
[01:08:33.880 --> 01:08:36.040]   of accounts and take them down.
[01:08:36.040 --> 01:08:38.360]   We continue to do that.
[01:08:38.360 --> 01:08:41.960]   I think it's one thing that if you'd told me 20 years ago,
[01:08:41.960 --> 01:08:44.000]   it's like, all right, you're starting this website
[01:08:44.000 --> 01:08:45.720]   to help people connect at a college
[01:08:45.720 --> 01:08:48.000]   and in the future, you're gonna be,
[01:08:48.000 --> 01:08:50.240]   part of your organization is gonna be a counter-terrorism
[01:08:50.240 --> 01:08:53.480]   organization with AI to find coordinated inauthentic.
[01:08:53.480 --> 01:08:56.080]   I would have thought that was pretty wild,
[01:08:56.080 --> 01:09:00.920]   but I think that that's part of where we are.
[01:09:00.920 --> 01:09:02.760]   But look, I think that these questions
[01:09:02.760 --> 01:09:04.120]   that you're pushing on now,
[01:09:04.120 --> 01:09:09.040]   this is actually where I'd guess most of the challenge
[01:09:09.040 --> 01:09:12.800]   around AI will be for the foreseeable future.
[01:09:12.800 --> 01:09:15.880]   I think that there's a lot of debate around things like,
[01:09:15.880 --> 01:09:19.000]   is this going to create existential risk to humanity?
[01:09:19.000 --> 01:09:20.920]   And I think that those are very hard things
[01:09:20.920 --> 01:09:22.880]   to disprove one way or another.
[01:09:22.880 --> 01:09:25.520]   My own intuition is that the point at which
[01:09:25.520 --> 01:09:29.520]   we become close to superintelligence is,
[01:09:29.520 --> 01:09:34.280]   it's just really unclear to me that the current technology
[01:09:34.280 --> 01:09:37.040]   is gonna get there without another set
[01:09:37.040 --> 01:09:39.280]   of significant advances.
[01:09:39.280 --> 01:09:40.800]   But that doesn't mean that there's no danger.
[01:09:40.800 --> 01:09:42.880]   I think the danger is basically amplifying
[01:09:42.880 --> 01:09:46.720]   the kind of known set of harms that people
[01:09:46.720 --> 01:09:48.400]   or sets of accounts can do.
[01:09:48.400 --> 01:09:50.480]   And we just need to make sure that we really focus
[01:09:50.480 --> 01:09:54.880]   on basically doing that as well as possible.
[01:09:54.880 --> 01:09:57.640]   So that's definitely a big focus for me.
[01:09:57.640 --> 01:10:00.200]   - Well, you can basically use large language models
[01:10:00.200 --> 01:10:03.920]   as an assistant of how to cause harm on social networks.
[01:10:03.920 --> 01:10:05.360]   So you can ask it a question,
[01:10:05.360 --> 01:10:13.000]   meta has very impressive coordinated inauthentic account
[01:10:13.000 --> 01:10:16.080]   fighting capabilities.
[01:10:16.080 --> 01:10:20.360]   How do I do the coordinating authentic account creation
[01:10:20.360 --> 01:10:23.440]   where meta doesn't detect it?
[01:10:23.440 --> 01:10:25.400]   Like literally ask that question.
[01:10:25.400 --> 01:10:29.240]   And basically there's this kind of part of it.
[01:10:29.240 --> 01:10:30.960]   I mean, that's what open AI showed
[01:10:30.960 --> 01:10:33.000]   that they're concerned with those questions.
[01:10:33.000 --> 01:10:34.920]   Perhaps you can comment on your approach to it,
[01:10:34.920 --> 01:10:38.840]   how to do a kind of moderation on the output
[01:10:38.840 --> 01:10:41.120]   of those models that it can't be used
[01:10:41.120 --> 01:10:45.360]   to help you coordinate harm in all the full definition
[01:10:45.360 --> 01:10:46.960]   of what the harm means.
[01:10:46.960 --> 01:10:48.840]   - Yeah, and that's a lot of the fine tuning
[01:10:48.840 --> 01:10:51.320]   and the alignment training that we do
[01:10:51.320 --> 01:10:56.320]   is basically when we ship AIs across our products,
[01:10:56.320 --> 01:11:02.160]   a lot of what we're trying to make sure is that
[01:11:02.160 --> 01:11:07.280]   you can't ask it to help you commit a crime, right?
[01:11:07.280 --> 01:11:14.680]   So I think training it to kind of understand that
[01:11:14.680 --> 01:11:17.120]   and it's not like any of these systems
[01:11:17.120 --> 01:11:18.800]   are ever gonna be a hundred percent perfect,
[01:11:18.800 --> 01:11:23.800]   but just making it so that this isn't an easier way
[01:11:23.800 --> 01:11:30.040]   to go about doing something bad
[01:11:30.040 --> 01:11:31.960]   than the next best alternative, right?
[01:11:31.960 --> 01:11:33.720]   I mean, people still have Google, right?
[01:11:33.720 --> 01:11:35.000]   You still have search engines.
[01:11:35.000 --> 01:11:37.960]   So the information is out there.
[01:11:37.960 --> 01:11:43.840]   And for these, what we see is like for nation states
[01:11:44.200 --> 01:11:46.720]   or these actors that are trying to pull off
[01:11:46.720 --> 01:11:50.160]   these large coordinated and authentic networks
[01:11:50.160 --> 01:11:53.160]   to kind of influence different things,
[01:11:53.160 --> 01:11:55.040]   at some point when we would just make it very difficult,
[01:11:55.040 --> 01:11:58.040]   they do just try to use other services instead, right?
[01:11:58.040 --> 01:12:00.840]   It's just like, if you can make it more expensive
[01:12:00.840 --> 01:12:03.400]   for them to do it on your service,
[01:12:03.400 --> 01:12:05.880]   then kind of people go elsewhere.
[01:12:05.880 --> 01:12:08.400]   And I think that that's the bar, right?
[01:12:08.400 --> 01:12:11.240]   It's not like, okay, are you ever gonna be perfect
[01:12:11.240 --> 01:12:14.440]   at finding every adversary who tries to attack you?
[01:12:14.440 --> 01:12:16.920]   It's, I mean, you try to get as close to that as possible,
[01:12:16.920 --> 01:12:20.040]   but I think really kind of economically,
[01:12:20.040 --> 01:12:21.720]   what you're just trying to do is make it so
[01:12:21.720 --> 01:12:24.880]   it's just inefficient for them to go after that.
[01:12:24.880 --> 01:12:26.480]   - But there's also complicated questions
[01:12:26.480 --> 01:12:30.600]   of what is and isn't harm, what is and isn't misinformation.
[01:12:30.600 --> 01:12:32.040]   So this is one of the things
[01:12:32.040 --> 01:12:34.760]   that Wikipedia has also tried to face.
[01:12:34.760 --> 01:12:39.560]   I remember asking GPT about whether the virus
[01:12:39.560 --> 01:12:42.840]   leaked from a lab or not, and the answer provided
[01:12:42.840 --> 01:12:47.400]   was a very nuanced one, and a well-cited one,
[01:12:47.400 --> 01:12:52.400]   almost, dare I say, well-thought-out one, balanced.
[01:12:52.400 --> 01:12:55.000]   I would hate for that nuance to be lost
[01:12:55.000 --> 01:12:56.700]   through the process of moderation.
[01:12:56.700 --> 01:13:00.560]   Wikipedia does a good job on that particular thing too,
[01:13:00.560 --> 01:13:03.320]   but from pressures from governments and institutions,
[01:13:03.320 --> 01:13:07.560]   you could see some of that nuance and depth
[01:13:07.560 --> 01:13:12.560]   of information, facts, and wisdom be lost.
[01:13:12.560 --> 01:13:14.560]   - Absolutely.
[01:13:14.560 --> 01:13:16.560]   - And that's a scary thing.
[01:13:16.560 --> 01:13:19.560]   Some of the magic, some of the edges,
[01:13:19.560 --> 01:13:21.000]   the rough edges might be lost
[01:13:21.000 --> 01:13:23.960]   through the process of moderation of AI systems.
[01:13:23.960 --> 01:13:26.520]   So how do you get that right?
[01:13:26.520 --> 01:13:28.960]   - I really agree with what you're pushing on.
[01:13:28.960 --> 01:13:33.960]   I mean, the core shape of the problem
[01:13:33.960 --> 01:13:36.360]   is that there are some harms
[01:13:36.360 --> 01:13:38.240]   that I think everyone agrees are bad.
[01:13:38.240 --> 01:13:43.120]   Sexual exploitation of children.
[01:13:43.120 --> 01:13:45.960]   You're not gonna get many people
[01:13:45.960 --> 01:13:47.720]   who think that that type of thing
[01:13:47.720 --> 01:13:49.520]   should be allowed on any service,
[01:13:49.520 --> 01:13:51.480]   and that's something that we face
[01:13:51.480 --> 01:13:55.480]   and try to push off as much as possible today.
[01:13:55.480 --> 01:14:00.120]   Terrorism, inciting violence.
[01:14:00.120 --> 01:14:02.920]   We went through a bunch of these types of harms before.
[01:14:05.040 --> 01:14:07.840]   But then I do think that you get to a set of harms
[01:14:07.840 --> 01:14:10.040]   where there is more social debate around it.
[01:14:10.040 --> 01:14:13.280]   So misinformation, I think,
[01:14:13.280 --> 01:14:18.240]   has been a really tricky one
[01:14:18.240 --> 01:14:23.240]   because there are things that are kind of obviously false,
[01:14:23.240 --> 01:14:25.000]   right, that are maybe factual,
[01:14:25.000 --> 01:14:28.960]   but may not be harmful.
[01:14:28.960 --> 01:14:33.000]   So it's like, all right, are you gonna censor someone
[01:14:33.000 --> 01:14:34.800]   for just being wrong?
[01:14:34.800 --> 01:14:36.520]   If there's no kind of harm implication
[01:14:36.520 --> 01:14:37.360]   of what they're doing,
[01:14:37.360 --> 01:14:39.520]   I think that there's a bunch of real kind of issues
[01:14:39.520 --> 01:14:41.160]   and challenges there.
[01:14:41.160 --> 01:14:43.400]   But then I think that there are other places
[01:14:43.400 --> 01:14:44.240]   where it is,
[01:14:44.240 --> 01:14:47.200]   you just take some of the stuff around COVID
[01:14:47.200 --> 01:14:48.520]   earlier on in the pandemic,
[01:14:48.520 --> 01:14:53.520]   where there were real health implications,
[01:14:53.520 --> 01:14:55.440]   but there hadn't been time to fully vet
[01:14:55.440 --> 01:14:57.120]   a bunch of the scientific assumptions.
[01:14:57.120 --> 01:15:00.600]   And unfortunately, I think a lot of the kind of establishment
[01:15:00.600 --> 01:15:03.960]   on that kind of waffled on a bunch of facts
[01:15:03.960 --> 01:15:06.600]   and asked for a bunch of things to be censored
[01:15:06.600 --> 01:15:11.600]   that in retrospect ended up being more debatable or true.
[01:15:11.600 --> 01:15:13.520]   And that stuff is really tough, right?
[01:15:13.520 --> 01:15:15.680]   And really undermines trust in that.
[01:15:15.680 --> 01:15:20.680]   And so I do think that the questions around how to manage
[01:15:20.680 --> 01:15:24.080]   that are very nuanced.
[01:15:24.080 --> 01:15:25.920]   The way that I try to think about it
[01:15:25.920 --> 01:15:28.720]   is that it goes,
[01:15:28.720 --> 01:15:31.800]   I think it's best to generally boil things down
[01:15:31.800 --> 01:15:34.560]   to the harms that people agree on.
[01:15:34.560 --> 01:15:36.040]   So when you think about,
[01:15:36.040 --> 01:15:38.280]   is something misinformation or not,
[01:15:38.280 --> 01:15:41.280]   I think often the more salient bit is,
[01:15:41.280 --> 01:15:46.280]   is this going to potentially lead to physical harm
[01:15:46.280 --> 01:15:50.280]   for someone and kind of think about it in that sense.
[01:15:50.280 --> 01:15:51.400]   And then beyond that,
[01:15:51.400 --> 01:15:53.000]   I think people just have different preferences
[01:15:53.000 --> 01:15:55.240]   on how they want things to be flagged for them.
[01:15:55.240 --> 01:15:57.720]   I think a bunch of people would prefer
[01:15:57.720 --> 01:16:00.320]   to kind of have a flag on something that says,
[01:16:00.320 --> 01:16:02.400]   hey, a fact checker thinks that this might be false.
[01:16:02.400 --> 01:16:05.400]   Or I think Twitter's community notes implementation
[01:16:05.400 --> 01:16:07.320]   is quite good on this.
[01:16:07.320 --> 01:16:09.680]   But again, it's the same type of thing.
[01:16:09.680 --> 01:16:12.800]   It's like just kind of discretionarily adding a flag
[01:16:12.800 --> 01:16:14.600]   because it makes the user experience better.
[01:16:14.600 --> 01:16:17.440]   But it's not trying to take down the information or not.
[01:16:17.440 --> 01:16:21.000]   I think that you want to reserve the kind of censorship
[01:16:21.000 --> 01:16:24.560]   of content to things that are of known categories
[01:16:24.560 --> 01:16:27.680]   that people generally agree are bad.
[01:16:27.680 --> 01:16:30.520]   - Yeah, but there's so many things,
[01:16:30.520 --> 01:16:31.640]   especially with the pandemic,
[01:16:31.640 --> 01:16:36.640]   but there's other topics where there's just deep disagreement
[01:16:36.640 --> 01:16:41.320]   fueled by politics about what is and isn't harmful.
[01:16:41.320 --> 01:16:45.800]   There's even just the degree to which the virus is harmful
[01:16:45.800 --> 01:16:48.720]   and the degree to which the vaccines
[01:16:48.720 --> 01:16:50.200]   that respond to the virus are harmful.
[01:16:50.200 --> 01:16:52.400]   There's just, there's almost like
[01:16:52.400 --> 01:16:54.480]   a political divider on that.
[01:16:54.480 --> 01:16:57.840]   And so how do you make decisions about that
[01:16:57.840 --> 01:17:01.160]   where half the country in the United States
[01:17:01.160 --> 01:17:03.720]   or some large fraction of the world
[01:17:03.720 --> 01:17:07.000]   has very different views from another part of the world?
[01:17:07.000 --> 01:17:13.720]   Is there a way for Meta to stay out of the moderation of this?
[01:17:13.720 --> 01:17:19.120]   - It's very difficult to just abstain.
[01:17:19.120 --> 01:17:22.680]   But I think we should be clear about which of these things
[01:17:22.680 --> 01:17:25.320]   are actual safety concerns
[01:17:25.320 --> 01:17:28.000]   and which ones are a matter of preference
[01:17:28.000 --> 01:17:30.320]   in terms of how people want information flagged.
[01:17:30.320 --> 01:17:32.800]   Right, so we did recently introduce something
[01:17:32.800 --> 01:17:36.600]   that allows people to have fact-checking
[01:17:36.600 --> 01:17:38.400]   not affect the distribution
[01:17:38.400 --> 01:17:40.480]   of what shows up in their products.
[01:17:40.480 --> 01:17:41.960]   So, okay, a bunch of people don't trust
[01:17:41.960 --> 01:17:43.440]   who the fact-checkers are.
[01:17:43.440 --> 01:17:45.960]   All right, well, you can turn that off if you want,
[01:17:45.960 --> 01:17:49.840]   but if the content violates some policy,
[01:17:49.840 --> 01:17:51.600]   like it's inciting violence or something like that,
[01:17:51.600 --> 01:17:53.040]   it's still not gonna be allowed.
[01:17:53.040 --> 01:17:56.480]   So I think that you wanna honor people's preferences
[01:17:56.480 --> 01:17:58.440]   on that as much as possible.
[01:17:58.440 --> 01:18:01.960]   But look, I mean, this is really difficult stuff.
[01:18:01.960 --> 01:18:06.680]   I think it's really hard to know where to draw the line
[01:18:06.680 --> 01:18:10.720]   on what is fact and what is opinion,
[01:18:10.720 --> 01:18:13.320]   because the nature of science is that
[01:18:13.320 --> 01:18:15.720]   nothing is ever 100% known for certain.
[01:18:15.720 --> 01:18:17.600]   You can disprove certain things,
[01:18:17.600 --> 01:18:20.520]   but you're constantly testing new hypotheses
[01:18:20.520 --> 01:18:24.920]   and scrutinizing frameworks that have been long held.
[01:18:24.920 --> 01:18:28.000]   And every once in a while, you throw out something
[01:18:28.000 --> 01:18:29.960]   that was working for a very long period of time,
[01:18:29.960 --> 01:18:31.680]   and it's very difficult.
[01:18:31.680 --> 01:18:34.760]   But I think that just because it's very hard
[01:18:34.760 --> 01:18:35.880]   and just because they're edge cases
[01:18:35.880 --> 01:18:40.040]   doesn't mean that you should not try to give people
[01:18:40.040 --> 01:18:41.680]   what they're looking for as well.
[01:18:41.680 --> 01:18:46.280]   Let me ask about something you've faced
[01:18:46.280 --> 01:18:47.760]   in terms of moderation.
[01:18:48.960 --> 01:18:52.480]   Is pressure from different sources,
[01:18:52.480 --> 01:18:54.080]   pressure from governments,
[01:18:54.080 --> 01:18:58.320]   I wanna ask a question how to withstand that pressure
[01:18:58.320 --> 01:19:03.320]   for a world where AI moderation starts becoming a thing too.
[01:19:03.320 --> 01:19:08.720]   So what's Meta's approach to resist the pressure
[01:19:08.720 --> 01:19:13.040]   from governments and other interest groups
[01:19:13.040 --> 01:19:15.440]   in terms of what to moderate and not?
[01:19:15.440 --> 01:19:17.600]   - I don't know that there's like
[01:19:17.600 --> 01:19:19.520]   a one size fits all answer to that.
[01:19:19.520 --> 01:19:24.200]   I mean, I think we basically have the principles around,
[01:19:24.200 --> 01:19:26.520]   you know, we wanna allow people to express
[01:19:26.520 --> 01:19:27.600]   as much as possible,
[01:19:27.600 --> 01:19:32.600]   but we have developed clear categories of things
[01:19:32.600 --> 01:19:37.920]   that we think are wrong that we don't want on our services,
[01:19:37.920 --> 01:19:40.800]   and we build tools to try to moderate those.
[01:19:40.800 --> 01:19:43.360]   So then the question is, okay, what do you do
[01:19:43.360 --> 01:19:47.400]   when a government says that they don't want something
[01:19:47.400 --> 01:19:49.600]   on the service?
[01:19:49.600 --> 01:19:54.040]   And we have a bunch of principles
[01:19:54.040 --> 01:19:55.320]   around how we deal with that.
[01:19:55.320 --> 01:19:57.960]   Because on the one hand, if there's a, you know,
[01:19:57.960 --> 01:20:00.320]   democratically elected government
[01:20:00.320 --> 01:20:03.160]   and people around the world just have different values
[01:20:03.160 --> 01:20:07.560]   in different places, then should we as a, you know,
[01:20:07.560 --> 01:20:12.560]   California based company tell them that something
[01:20:12.720 --> 01:20:16.520]   that they have decided is unacceptable?
[01:20:16.520 --> 01:20:20.920]   Actually, like that we need to be able to express that?
[01:20:20.920 --> 01:20:23.280]   I mean, I think that there's a certain amount
[01:20:23.280 --> 01:20:26.440]   of hubris in that.
[01:20:26.440 --> 01:20:28.840]   But then I think that there are other cases
[01:20:28.840 --> 01:20:32.080]   where, you know, it's like a little more autocratic
[01:20:32.080 --> 01:20:35.560]   and, you know, you have the dictator leader
[01:20:35.560 --> 01:20:37.520]   who's just trying to crack down on dissent
[01:20:37.520 --> 01:20:39.680]   and, you know, the people in a country
[01:20:39.680 --> 01:20:42.560]   are really not aligned with that.
[01:20:42.560 --> 01:20:44.480]   And it's not necessarily against their culture,
[01:20:44.480 --> 01:20:47.240]   but the person who's leading it
[01:20:47.240 --> 01:20:49.520]   is just trying to push in a certain direction.
[01:20:49.520 --> 01:20:52.920]   These are very complex questions,
[01:20:52.920 --> 01:20:56.840]   but I think, so it's difficult to have
[01:20:56.840 --> 01:21:01.680]   a one size fits all approach to it.
[01:21:01.680 --> 01:21:03.440]   But in general, we're pretty active
[01:21:03.440 --> 01:21:06.000]   in kind of advocating and pushing back
[01:21:06.000 --> 01:21:10.640]   on requests to take things down.
[01:21:10.640 --> 01:21:15.360]   But honestly, the thing that I think
[01:21:15.360 --> 01:21:18.480]   a request to censor things is one thing,
[01:21:18.480 --> 01:21:20.040]   and that's obviously bad,
[01:21:20.040 --> 01:21:23.400]   but where we draw a much harder line
[01:21:23.400 --> 01:21:25.960]   is on requests for access to information, right?
[01:21:25.960 --> 01:21:28.280]   Because, you know, if you can,
[01:21:28.280 --> 01:21:29.840]   if you get told that you can't say something,
[01:21:29.840 --> 01:21:32.040]   I mean, that's bad, right?
[01:21:32.040 --> 01:21:34.240]   I mean, that, you know, is, you know,
[01:21:34.240 --> 01:21:38.680]   obviously it violates your sense
[01:21:38.680 --> 01:21:40.560]   and freedom of expression at some level,
[01:21:40.560 --> 01:21:44.000]   but a government getting access to data
[01:21:44.000 --> 01:21:48.480]   in a way that seems like it would be unlawful
[01:21:48.480 --> 01:21:53.480]   in our country exposes people to real physical harm.
[01:21:53.480 --> 01:21:57.400]   And that's something that in general
[01:21:57.400 --> 01:21:59.360]   we take very seriously.
[01:21:59.360 --> 01:22:03.320]   And then, so that flows through like all of our policies
[01:22:03.320 --> 01:22:04.960]   and in a lot of ways, right?
[01:22:04.960 --> 01:22:07.440]   By the time you're actually like litigating
[01:22:07.440 --> 01:22:09.800]   with a government or pushing back on them,
[01:22:09.800 --> 01:22:11.840]   that's pretty late in the funnel.
[01:22:11.840 --> 01:22:15.680]   I'd say a bunch of the stuff starts a lot higher up
[01:22:15.680 --> 01:22:18.400]   in the decision of where do we put data centers.
[01:22:18.400 --> 01:22:21.920]   Then there are a lot of countries where, you know,
[01:22:21.920 --> 01:22:24.560]   we may have a lot of people using the service in a place.
[01:22:24.560 --> 01:22:28.240]   It might be, you know, good for the service in some ways,
[01:22:28.240 --> 01:22:31.360]   good for those people if we could reduce the latency
[01:22:31.360 --> 01:22:34.440]   by having a data center nearby them.
[01:22:34.440 --> 01:22:36.680]   But, you know, for whatever reason, we just feel like,
[01:22:36.680 --> 01:22:40.000]   hey, this government does not have a good track record
[01:22:40.000 --> 01:22:45.000]   on basically not trying to get access to people's data.
[01:22:45.000 --> 01:22:47.640]   And at the end of the day, I mean,
[01:22:47.640 --> 01:22:49.680]   if you put a data center in a country
[01:22:49.680 --> 01:22:52.560]   and the government wants to get access to people's data,
[01:22:52.560 --> 01:22:54.840]   then, you know, they do at the end of the day
[01:22:54.840 --> 01:22:57.280]   have the option of having people show up with guns
[01:22:57.280 --> 01:22:59.040]   and taking it by force.
[01:22:59.040 --> 01:23:01.400]   So I think that there's like a lot of decisions
[01:23:01.400 --> 01:23:03.800]   that go into like how you architect the systems
[01:23:03.800 --> 01:23:09.800]   years in advance of these actual confrontations
[01:23:09.800 --> 01:23:11.720]   that end up being really important.
[01:23:11.720 --> 01:23:15.280]   - So you put the protection of people's data
[01:23:15.280 --> 01:23:17.880]   as a very, very high priority.
[01:23:17.880 --> 01:23:19.080]   But in- - That I think is a,
[01:23:19.080 --> 01:23:20.360]   there are more harms that I think
[01:23:20.360 --> 01:23:21.720]   can be associated with that.
[01:23:21.720 --> 01:23:24.760]   And I think that that ends up being a more critical thing
[01:23:24.760 --> 01:23:28.840]   to defend against governments than, you know,
[01:23:28.840 --> 01:23:30.240]   whereas, you know, if another government
[01:23:30.240 --> 01:23:32.560]   has a different view of what should be acceptable speech
[01:23:32.560 --> 01:23:34.200]   in their country,
[01:23:34.200 --> 01:23:36.360]   especially if it's a democratically elected government,
[01:23:36.360 --> 01:23:38.720]   and, you know, it's, then I think that there's
[01:23:38.720 --> 01:23:41.240]   a certain amount of deference that you should have to that.
[01:23:41.240 --> 01:23:45.000]   - So that's speaking more to the direct harm that's possible
[01:23:45.000 --> 01:23:47.480]   when you give governments access to data.
[01:23:47.480 --> 01:23:49.460]   But if we look at the United States,
[01:23:49.460 --> 01:23:53.400]   to the more nuanced kind of pressure to censor,
[01:23:53.400 --> 01:23:54.800]   not even order to censor,
[01:23:54.800 --> 01:23:57.840]   but pressure to censor from political entities,
[01:23:57.840 --> 01:24:00.960]   which has kind of received quite a bit of attention
[01:24:00.960 --> 01:24:02.200]   in the United States,
[01:24:02.200 --> 01:24:06.680]   maybe one way to ask that question is,
[01:24:06.680 --> 01:24:08.580]   if you've seen the Twitter files,
[01:24:08.580 --> 01:24:14.280]   what have you learned from the kind of pressure
[01:24:14.280 --> 01:24:18.540]   from US government agencies that was seen in Twitter files?
[01:24:18.540 --> 01:24:20.840]   And what do you do with that kind of pressure?
[01:24:20.840 --> 01:24:24.200]   - You know, I've seen it.
[01:24:26.320 --> 01:24:27.840]   It's really hard from the outside
[01:24:27.840 --> 01:24:30.680]   to know exactly what happened in each of these cases.
[01:24:30.680 --> 01:24:32.360]   You know, we've obviously been
[01:24:32.360 --> 01:24:38.320]   in a bunch of our own cases where, you know,
[01:24:38.320 --> 01:24:42.480]   where agencies or different folks will just say,
[01:24:42.480 --> 01:24:46.640]   "Hey, here's a threat that we're aware of.
[01:24:46.640 --> 01:24:48.600]   You should be aware of this too."
[01:24:48.600 --> 01:24:51.320]   It's not really pressure as much as it is just,
[01:24:51.320 --> 01:24:56.120]   you know, flagging something that our security systems
[01:24:56.120 --> 01:24:57.680]   should be on alert about.
[01:24:57.680 --> 01:25:00.960]   I get how some people could think of it as that,
[01:25:00.960 --> 01:25:04.480]   but at the end of the day,
[01:25:04.480 --> 01:25:07.760]   it's our call on how to handle that.
[01:25:07.760 --> 01:25:09.040]   But I mean, I just, you know,
[01:25:09.040 --> 01:25:10.280]   in terms of running these services,
[01:25:10.280 --> 01:25:11.920]   won't have access to as much information
[01:25:11.920 --> 01:25:13.360]   about what people think that adversaries
[01:25:13.360 --> 01:25:15.360]   might be trying to do as possible.
[01:25:15.360 --> 01:25:18.880]   - Well, so you don't feel like there'll be consequences
[01:25:18.880 --> 01:25:23.880]   if, you know, anybody, the CIA, the FBI, a political party,
[01:25:24.240 --> 01:25:26.480]   the Democrats or the Republicans
[01:25:26.480 --> 01:25:31.480]   of high powerful political figures write emails.
[01:25:31.480 --> 01:25:34.320]   You don't feel pressure from a suggestion?
[01:25:34.320 --> 01:25:36.080]   - I guess what I say is there's so much pressure
[01:25:36.080 --> 01:25:40.600]   from all sides that I'm not sure that any specific thing
[01:25:40.600 --> 01:25:42.880]   that someone says is really adding
[01:25:42.880 --> 01:25:44.240]   that much more to the mix.
[01:25:44.240 --> 01:25:48.120]   And there are obviously a lot of people who think
[01:25:48.120 --> 01:25:52.480]   that we should be censoring more content,
[01:25:52.480 --> 01:25:53.480]   or there are a lot of people who think
[01:25:53.480 --> 01:25:55.440]   we should be censoring less content.
[01:25:55.440 --> 01:25:58.120]   There are, as you say, all kinds of different groups
[01:25:58.120 --> 01:25:59.800]   that are involved in these debates, right?
[01:25:59.800 --> 01:26:02.320]   So there's the kind of elected officials
[01:26:02.320 --> 01:26:04.920]   and politicians themselves, there's the agencies,
[01:26:04.920 --> 01:26:07.880]   but I mean, but there's the media,
[01:26:07.880 --> 01:26:09.160]   there's activist groups,
[01:26:09.160 --> 01:26:11.840]   this is not a US specific thing,
[01:26:11.840 --> 01:26:13.320]   there are groups all over the world
[01:26:13.320 --> 01:26:16.280]   and kind of all in every country
[01:26:16.280 --> 01:26:18.120]   that bring different values.
[01:26:18.120 --> 01:26:22.800]   So it's just a very, it's a very active debate,
[01:26:22.800 --> 01:26:23.920]   and I understand it, right?
[01:26:23.920 --> 01:26:28.920]   I mean, these kind of questions get to really
[01:26:28.920 --> 01:26:31.440]   some of the most important social debates
[01:26:31.440 --> 01:26:33.000]   that are being had.
[01:26:33.000 --> 01:26:36.040]   So it gets back to the question of truth,
[01:26:36.040 --> 01:26:39.200]   because for a lot of these things,
[01:26:39.200 --> 01:26:41.680]   they haven't yet been hardened into a single truth,
[01:26:41.680 --> 01:26:44.320]   and society's sort of trying to hash out
[01:26:44.320 --> 01:26:48.400]   what we think, right, on certain issues.
[01:26:48.400 --> 01:26:50.720]   Maybe in a few hundred years, everyone will look back
[01:26:50.720 --> 01:26:52.120]   and say, "Hey, no, it wasn't obvious
[01:26:52.120 --> 01:26:53.960]   that it should have been this, but no,
[01:26:53.960 --> 01:26:56.200]   we're kind of in that meat grinder now
[01:26:56.200 --> 01:26:59.760]   and working through that."
[01:26:59.760 --> 01:27:04.760]   So no, these are all very complicated.
[01:27:04.760 --> 01:27:11.360]   And some people raise concerns in good faith
[01:27:11.360 --> 01:27:14.160]   and just say, "Hey, this is something that I wanna flag
[01:27:14.160 --> 01:27:15.800]   for you to think about."
[01:27:15.800 --> 01:27:17.840]   Certain people, I certainly think,
[01:27:17.840 --> 01:27:20.640]   like come at things with somewhat of a more
[01:27:20.640 --> 01:27:25.160]   kind of punitive or vengeful view of like,
[01:27:25.160 --> 01:27:26.560]   "I want you to do this thing.
[01:27:26.560 --> 01:27:28.680]   If you don't, then I'm gonna try to make your life difficult
[01:27:28.680 --> 01:27:30.560]   in a lot of other ways."
[01:27:30.560 --> 01:27:33.880]   But I don't know, there's just,
[01:27:33.880 --> 01:27:36.560]   this is one of the most pressurized debates,
[01:27:36.560 --> 01:27:37.520]   I think, in society.
[01:27:37.520 --> 01:27:40.240]   So I just think that there are so many people
[01:27:40.240 --> 01:27:42.160]   in different forces that are trying to apply pressure
[01:27:42.160 --> 01:27:44.360]   from different sides that it's,
[01:27:44.360 --> 01:27:45.960]   I don't think you can make decisions
[01:27:45.960 --> 01:27:47.320]   based on trying to make people happy.
[01:27:47.320 --> 01:27:50.240]   I think you just have to do what you think
[01:27:50.240 --> 01:27:53.680]   is the right balance and accept that people
[01:27:53.680 --> 01:27:57.360]   are gonna be upset no matter where you come out on that.
[01:27:57.360 --> 01:28:00.120]   - Yeah, I like that pressurized debate.
[01:28:00.120 --> 01:28:02.320]   So how's your view of the freedom of speech
[01:28:02.320 --> 01:28:03.660]   evolved over the years?
[01:28:03.660 --> 01:28:12.920]   And now with AI, where the freedom might apply to them,
[01:28:12.920 --> 01:28:17.680]   not just to the humans, but to the personalized agents
[01:28:17.680 --> 01:28:19.040]   as you've spoken about them.
[01:28:20.160 --> 01:28:21.800]   - So yeah, I mean, I've probably gotten
[01:28:21.800 --> 01:28:23.040]   a somewhat more nuanced view
[01:28:23.040 --> 01:28:25.040]   just because I think that there are,
[01:28:25.040 --> 01:28:27.960]   I come at this, I'm obviously very pro
[01:28:27.960 --> 01:28:29.280]   freedom of expression, right?
[01:28:29.280 --> 01:28:31.520]   I don't think you build a service like this
[01:28:31.520 --> 01:28:33.400]   that gives people tools to express themselves
[01:28:33.400 --> 01:28:35.220]   unless you think that people expressing themselves
[01:28:35.220 --> 01:28:36.620]   at scale is a good thing, right?
[01:28:36.620 --> 01:28:40.360]   So I didn't get into this to try to prevent people
[01:28:40.360 --> 01:28:42.400]   from expressing anything.
[01:28:42.400 --> 01:28:44.440]   I wanna give people tools
[01:28:44.440 --> 01:28:46.560]   so they can express as much as possible.
[01:28:46.560 --> 01:28:49.120]   And then I think it's become clear
[01:28:49.120 --> 01:28:51.400]   that there are certain categories of things
[01:28:51.400 --> 01:28:52.960]   that we've talked about
[01:28:52.960 --> 01:28:54.920]   that I think almost everyone accepts are bad
[01:28:54.920 --> 01:28:57.120]   and that no one wants and that are illegal
[01:28:57.120 --> 01:28:59.320]   even in countries like the US where
[01:28:59.320 --> 01:29:01.620]   you have the first amendment
[01:29:01.620 --> 01:29:04.340]   that's very protective of enabling speech.
[01:29:04.340 --> 01:29:06.600]   It's like, you're still not allowed to do things
[01:29:06.600 --> 01:29:08.480]   that are gonna immediately incite violence
[01:29:08.480 --> 01:29:10.540]   or violate people's intellectual property
[01:29:10.540 --> 01:29:11.380]   or things like that.
[01:29:11.380 --> 01:29:14.880]   So there are those, but then there's also a very active core
[01:29:14.880 --> 01:29:18.720]   of just active disagreements in society
[01:29:18.720 --> 01:29:21.720]   where some people may think that something is true or false.
[01:29:21.720 --> 01:29:24.800]   The other side might think it's the opposite
[01:29:24.800 --> 01:29:26.620]   or just unsettled, right?
[01:29:26.620 --> 01:29:29.640]   And those are some of the most difficult
[01:29:29.640 --> 01:29:33.160]   to kind of handle like we've talked about.
[01:29:33.160 --> 01:29:38.160]   But one of the lessons that I feel like I've learned
[01:29:38.160 --> 01:29:43.400]   is that a lot of times when you can,
[01:29:44.960 --> 01:29:48.880]   the best way to handle this stuff more practically
[01:29:48.880 --> 01:29:51.440]   is not in terms of answering the question
[01:29:51.440 --> 01:29:53.060]   of should this be allowed,
[01:29:53.060 --> 01:29:58.060]   but just like what is the best way to deal
[01:29:58.060 --> 01:30:00.720]   with someone being a jerk?
[01:30:00.720 --> 01:30:05.720]   Is the person basically just having a repeat behavior
[01:30:05.720 --> 01:30:11.200]   of causing a lot of issues?
[01:30:11.200 --> 01:30:14.720]   So looking at it more at that level.
[01:30:14.720 --> 01:30:16.840]   - And its effect on the broader communities,
[01:30:16.840 --> 01:30:19.320]   health of the community, health of the state.
[01:30:19.320 --> 01:30:21.640]   It's tricky though because like how do you know
[01:30:21.640 --> 01:30:24.880]   there could be people that have a very controversial
[01:30:24.880 --> 01:30:27.720]   viewpoint that turns out to have a positive
[01:30:27.720 --> 01:30:30.040]   long-term effect on the health of the community
[01:30:30.040 --> 01:30:31.720]   because it challenges the community to think.
[01:30:31.720 --> 01:30:33.360]   - That's true, absolutely.
[01:30:33.360 --> 01:30:36.280]   Yeah, no, I think you wanna be careful about that.
[01:30:36.280 --> 01:30:38.680]   I'm not sure I'm expressing this very clearly
[01:30:38.680 --> 01:30:42.520]   because I certainly agree with your point there.
[01:30:42.520 --> 01:30:46.960]   And my point isn't that we should not have people
[01:30:46.960 --> 01:30:49.240]   on our services that are being controversial.
[01:30:49.240 --> 01:30:51.600]   That's certainly not what I mean to say.
[01:30:51.600 --> 01:30:56.560]   It's that often I think it's not just looking
[01:30:56.560 --> 01:30:59.600]   at a specific example of speech
[01:30:59.600 --> 01:31:02.880]   that it's most effective to handle this stuff.
[01:31:02.880 --> 01:31:05.160]   And I think often you don't wanna make
[01:31:05.160 --> 01:31:08.320]   specific binary decisions of kind of this is allowed
[01:31:08.320 --> 01:31:09.240]   or this isn't.
[01:31:09.240 --> 01:31:11.880]   I mean, we talked about, you know, it's fact-checking
[01:31:11.880 --> 01:31:14.080]   or Twitter's community voices thing.
[01:31:14.080 --> 01:31:15.280]   I think that's another good example.
[01:31:15.280 --> 01:31:18.520]   It's like, it's not a question of is this allowed or not?
[01:31:18.520 --> 01:31:20.800]   It's just a question of adding more context to the thing.
[01:31:20.800 --> 01:31:22.640]   I think that that's helpful.
[01:31:22.640 --> 01:31:26.200]   So in the context of AI, which is what you were asking about,
[01:31:26.200 --> 01:31:29.680]   I think there are lots of ways that an AI can be helpful.
[01:31:29.680 --> 01:31:33.920]   With an AI, it's less about censorship, right?
[01:31:33.920 --> 01:31:37.760]   Because it's more about what is the most productive answer
[01:31:37.760 --> 01:31:38.720]   to a question.
[01:31:38.720 --> 01:31:41.520]   You know, there was one case study
[01:31:41.520 --> 01:31:45.280]   that I was reviewing with the team is someone asked,
[01:31:45.280 --> 01:31:53.040]   can you explain to me how to 3D print a gun?
[01:31:53.040 --> 01:31:58.040]   And one proposed response is like,
[01:31:58.040 --> 01:31:59.800]   no, I can't talk about that.
[01:31:59.800 --> 01:32:01.280]   Right, it's like basically just like shut it down
[01:32:01.280 --> 01:32:03.800]   immediately, which I think is some of what you see.
[01:32:03.800 --> 01:32:05.480]   It's like as a large language model,
[01:32:05.480 --> 01:32:07.600]   I'm not allowed to talk about whatever.
[01:32:07.600 --> 01:32:10.600]   But there's another response, which is like,
[01:32:10.600 --> 01:32:13.200]   hey, I don't think that's a good idea.
[01:32:13.200 --> 01:32:16.200]   In a lot of countries, including the US,
[01:32:16.200 --> 01:32:18.840]   3D printing guns is illegal
[01:32:18.840 --> 01:32:21.280]   or kind of whatever the factual thing is.
[01:32:21.280 --> 01:32:23.760]   And it's like, okay, that's actually a respectful
[01:32:23.760 --> 01:32:24.840]   and informative answer.
[01:32:24.840 --> 01:32:27.720]   And I may have not known that specific thing.
[01:32:27.720 --> 01:32:31.160]   And so there are different ways to handle this
[01:32:31.160 --> 01:32:36.160]   that I think kind of, you can either assume good intent,
[01:32:36.160 --> 01:32:38.680]   like maybe the person didn't know
[01:32:38.680 --> 01:32:40.280]   and I'm just gonna help educate them.
[01:32:40.280 --> 01:32:42.200]   Or you could like kind of come at it as like,
[01:32:42.200 --> 01:32:44.040]   no, I need to shut this thing down immediately.
[01:32:44.040 --> 01:32:46.720]   Right, it's like, I just am not gonna talk about this.
[01:32:46.720 --> 01:32:51.000]   And there may be times where you need to do that.
[01:32:51.000 --> 01:32:55.240]   But I actually think having a somewhat more
[01:32:55.240 --> 01:32:57.880]   informative approach where you generally assume
[01:32:57.880 --> 01:33:01.480]   good intent from people is probably a better balance
[01:33:01.480 --> 01:33:04.680]   to be on as many things as you can be.
[01:33:04.680 --> 01:33:06.160]   You're not gonna be able to do that for everything.
[01:33:06.160 --> 01:33:09.640]   But you were kind of asking about how I approach this
[01:33:09.640 --> 01:33:13.880]   and I'm thinking about this as it relates to AI.
[01:33:13.880 --> 01:33:16.000]   And I think that that's a big difference
[01:33:16.000 --> 01:33:20.600]   in kind of how to handle sensitive content
[01:33:20.600 --> 01:33:22.040]   across these different modes.
[01:33:22.040 --> 01:33:26.400]   - I have to ask, there's rumors you might be working
[01:33:26.400 --> 01:33:29.000]   on a social network that's text-based
[01:33:29.000 --> 01:33:32.880]   that might be a competitor to Twitter, code named P92.
[01:33:32.880 --> 01:33:38.280]   Is there something you can say about those rumors?
[01:33:38.280 --> 01:33:40.120]   - There is a project.
[01:33:40.120 --> 01:33:43.800]   You know, I've always thought that sort of a text-based
[01:33:43.800 --> 01:33:45.680]   kind of information utility
[01:33:45.680 --> 01:33:50.800]   is just a really important thing to society.
[01:33:50.800 --> 01:33:54.160]   And for whatever reason, I feel like Twitter
[01:33:54.160 --> 01:33:56.560]   has not lived up to what I would have thought
[01:33:56.560 --> 01:33:58.320]   its full potential should be.
[01:33:58.320 --> 01:33:59.520]   And I think that the current,
[01:33:59.520 --> 01:34:00.840]   you know, I think Elon thinks that, right?
[01:34:00.840 --> 01:34:03.240]   And that's probably one of the reasons why he bought it.
[01:34:03.240 --> 01:34:07.760]   And I do know that there are ways
[01:34:07.760 --> 01:34:11.400]   to consider alternative approaches to this.
[01:34:11.400 --> 01:34:14.360]   And one that I think is potentially interesting
[01:34:14.360 --> 01:34:17.320]   is this open and federated approach
[01:34:17.320 --> 01:34:18.720]   where you're seeing with Mastodon,
[01:34:18.720 --> 01:34:21.160]   and you're seeing that a little bit with Blue Sky.
[01:34:21.160 --> 01:34:25.880]   And I think that it's possible that something
[01:34:25.880 --> 01:34:30.240]   that melds some of those ideas with the graph
[01:34:30.240 --> 01:34:32.520]   and identity system that people have already cultivated
[01:34:32.520 --> 01:34:37.520]   on Instagram could be a kind of very welcome contribution
[01:34:37.520 --> 01:34:38.840]   to that space.
[01:34:38.840 --> 01:34:40.280]   But I know we work on a lot of things
[01:34:40.280 --> 01:34:41.120]   all the time though, too.
[01:34:41.120 --> 01:34:43.440]   So I don't wanna get ahead of myself.
[01:34:43.440 --> 01:34:45.720]   I mean, we have projects that explore
[01:34:45.720 --> 01:34:46.680]   a lot of different things,
[01:34:46.680 --> 01:34:49.840]   and this is certainly one that I think could be interesting.
[01:34:49.840 --> 01:34:52.440]   But- - So what's the release,
[01:34:52.440 --> 01:34:54.640]   the launch date of that again?
[01:34:54.640 --> 01:34:57.200]   Or what's the official website?
[01:34:57.200 --> 01:34:59.360]   - Well, we don't have that yet.
[01:34:59.360 --> 01:35:00.200]   - Oh, okay.
[01:35:00.200 --> 01:35:03.800]   - But I, and look, I mean, I don't know exactly
[01:35:03.800 --> 01:35:05.200]   how this is gonna turn out.
[01:35:05.200 --> 01:35:06.520]   I mean, what I can say is, yeah,
[01:35:06.520 --> 01:35:08.880]   there's some people working on this, right?
[01:35:08.880 --> 01:35:09.960]   I think that there's something there
[01:35:09.960 --> 01:35:13.680]   that's interesting to explore.
[01:35:13.680 --> 01:35:15.440]   - So if you look at, it'd be interesting
[01:35:15.440 --> 01:35:19.240]   to just ask this question and throw Twitter into the mix.
[01:35:19.240 --> 01:35:22.080]   At the landscape of social networks,
[01:35:22.080 --> 01:35:26.640]   that is Facebook, that is Instagram, that is WhatsApp,
[01:35:26.640 --> 01:35:31.160]   and then think of a text-based social network,
[01:35:31.160 --> 01:35:32.360]   when you look at that landscape,
[01:35:32.360 --> 01:35:35.080]   what are the interesting differences to you?
[01:35:35.080 --> 01:35:37.920]   Why do we have these different flavors?
[01:35:37.920 --> 01:35:40.640]   And what are the needs, what are the use cases,
[01:35:40.640 --> 01:35:43.320]   what are the products, what is the aspect of them
[01:35:43.320 --> 01:35:45.760]   that create a fulfilling human experience
[01:35:45.760 --> 01:35:49.280]   and a connection between humans that is somehow distinct?
[01:35:49.280 --> 01:35:51.880]   - Well, I think text is very accessible
[01:35:51.880 --> 01:35:54.400]   for people to transmit ideas
[01:35:54.400 --> 01:35:56.720]   and to have back and forth exchanges.
[01:35:56.720 --> 01:36:02.920]   So it, I think, ends up being a good format for discussion,
[01:36:02.920 --> 01:36:06.000]   in a lot of ways, uniquely good, right?
[01:36:06.000 --> 01:36:09.000]   If you look at some of the other formats
[01:36:09.000 --> 01:36:11.120]   or other networks that are focused on one type of content,
[01:36:11.120 --> 01:36:13.320]   like TikTok is obviously huge, right?
[01:36:13.320 --> 01:36:15.520]   And there are comments on TikTok,
[01:36:15.520 --> 01:36:19.520]   but I think the architecture of the service
[01:36:19.520 --> 01:36:21.360]   is very clearly that you have the video
[01:36:21.360 --> 01:36:25.040]   as the primary thing, and there's comments after that.
[01:36:25.040 --> 01:36:32.080]   But I think one of the unique pieces
[01:36:32.080 --> 01:36:35.800]   of having text-based comments, like content,
[01:36:35.800 --> 01:36:39.280]   is that the comments can also be first class.
[01:36:39.280 --> 01:36:43.080]   And that makes it so that conversations can just filter
[01:36:43.080 --> 01:36:45.240]   and fork into all these different directions
[01:36:45.240 --> 01:36:47.640]   and in a way that can be super useful.
[01:36:47.640 --> 01:36:48.840]   So I think there's a lot of things
[01:36:48.840 --> 01:36:50.840]   that are really awesome about the experience.
[01:36:50.840 --> 01:36:54.120]   It just always struck me, I always thought that,
[01:36:54.120 --> 01:36:56.320]   you know, Twitter should have a billion people using it,
[01:36:56.320 --> 01:37:00.480]   or whatever the thing is that basically
[01:37:00.480 --> 01:37:01.640]   ends up being in that space.
[01:37:01.640 --> 01:37:03.920]   And for whatever combination of reasons,
[01:37:03.920 --> 01:37:07.600]   again, these companies are complex organisms
[01:37:07.600 --> 01:37:10.840]   and it's very hard to diagnose this stuff from the outside.
[01:37:10.840 --> 01:37:11.840]   - Why doesn't Twitter,
[01:37:11.840 --> 01:37:15.840]   why doesn't a text-based comment
[01:37:15.840 --> 01:37:18.840]   as a first citizen-based social network
[01:37:18.840 --> 01:37:20.520]   have a billion users?
[01:37:20.520 --> 01:37:23.000]   - Well, I just think it's hard to build these companies.
[01:37:23.000 --> 01:37:27.720]   So it's not that every idea automatically goes
[01:37:27.720 --> 01:37:29.040]   and gets a billion people,
[01:37:29.040 --> 01:37:30.880]   it's just that I think that that idea,
[01:37:30.880 --> 01:37:34.160]   coupled with good execution, should get there.
[01:37:34.160 --> 01:37:37.800]   But I mean, look, we hit certain thresholds over time
[01:37:37.800 --> 01:37:41.800]   where, you know, we kind of plateaued early on
[01:37:41.800 --> 01:37:43.160]   and it wasn't clear that we were ever gonna reach
[01:37:43.160 --> 01:37:44.840]   a hundred million people on Facebook.
[01:37:44.840 --> 01:37:48.360]   And then we got really good at dialing in
[01:37:48.360 --> 01:37:51.160]   internationalization and helping the service grow
[01:37:51.160 --> 01:37:52.240]   in different countries.
[01:37:52.240 --> 01:37:55.480]   And that was like a whole competence
[01:37:55.480 --> 01:37:56.480]   that we needed to develop.
[01:37:56.480 --> 01:38:00.640]   And helping people basically spread the service
[01:38:00.640 --> 01:38:01.600]   to their friends.
[01:38:01.600 --> 01:38:03.800]   That was one of the things, once we got very good at that,
[01:38:03.800 --> 01:38:05.880]   that was one of the things that made me feel like,
[01:38:05.880 --> 01:38:08.760]   hey, if Instagram joined us early on,
[01:38:08.760 --> 01:38:10.280]   then I felt like we could help grow that quickly.
[01:38:10.280 --> 01:38:11.400]   And same with WhatsApp.
[01:38:11.400 --> 01:38:13.520]   And I think that that's sort of been a core competence
[01:38:13.520 --> 01:38:16.200]   that we've developed and been able to execute on.
[01:38:16.200 --> 01:38:17.040]   And others have too, right?
[01:38:17.040 --> 01:38:19.720]   I mean, ByteDance obviously have done a very good job
[01:38:19.720 --> 01:38:23.040]   with TikTok and have reached more than a billion people
[01:38:23.040 --> 01:38:26.640]   there, but it's certainly not automatic, right?
[01:38:26.640 --> 01:38:31.040]   I think you need a certain level of execution
[01:38:31.040 --> 01:38:31.960]   to basically get there.
[01:38:31.960 --> 01:38:34.440]   And I think for whatever reason,
[01:38:34.440 --> 01:38:36.200]   I think Twitter has this great idea
[01:38:36.200 --> 01:38:38.320]   and sort of magic in the service.
[01:38:39.560 --> 01:38:43.960]   But they just haven't kind of cracked that piece yet.
[01:38:43.960 --> 01:38:45.320]   And I think that that's made it
[01:38:45.320 --> 01:38:46.840]   so that you're seeing all these other things,
[01:38:46.840 --> 01:38:51.440]   whether it's Mastodon or Blue Sky,
[01:38:51.440 --> 01:38:54.360]   that I think are maybe just different cuts
[01:38:54.360 --> 01:38:55.200]   at the same thing.
[01:38:55.200 --> 01:38:57.240]   But I think through the last generation
[01:38:57.240 --> 01:39:00.120]   of social media overall,
[01:39:00.120 --> 01:39:01.480]   one of the interesting experiments
[01:39:01.480 --> 01:39:04.280]   that I think should get run at larger scale
[01:39:04.280 --> 01:39:05.600]   is what happens if there's somewhat
[01:39:05.600 --> 01:39:07.120]   more decentralized control.
[01:39:07.120 --> 01:39:10.200]   And if it's like the stack is more open throughout.
[01:39:10.200 --> 01:39:13.400]   And I've just been pretty fascinated by that
[01:39:13.400 --> 01:39:14.760]   and seeing how that works.
[01:39:14.760 --> 01:39:20.400]   To some degree, end-to-end encryption on WhatsApp,
[01:39:20.400 --> 01:39:22.440]   and as we bring it to other services,
[01:39:22.440 --> 01:39:25.000]   provides an element of it because it pushes
[01:39:25.000 --> 01:39:26.840]   the service really out to the edges.
[01:39:26.840 --> 01:39:31.840]   I mean, the server part of this that we run for WhatsApp
[01:39:31.840 --> 01:39:34.120]   is relatively very thin compared to what we do
[01:39:34.120 --> 01:39:36.280]   on Facebook or Instagram,
[01:39:36.280 --> 01:39:38.400]   and much more of the complexity is
[01:39:38.400 --> 01:39:40.880]   in how the apps kind of negotiate with each other
[01:39:40.880 --> 01:39:44.400]   to pass information in a fully end-to-end encrypted way.
[01:39:44.400 --> 01:39:48.040]   But I don't know, I think that that is a good model.
[01:39:48.040 --> 01:39:50.160]   I think it puts more power in individuals' hands
[01:39:50.160 --> 01:39:51.760]   and there are a lot of benefits of it
[01:39:51.760 --> 01:39:53.360]   if you can make it happen.
[01:39:53.360 --> 01:39:55.440]   Again, this is all pretty speculative.
[01:39:55.440 --> 01:39:58.720]   I mean, I think that it's hard from the outside
[01:39:58.720 --> 01:40:01.160]   to know why anything does or doesn't work
[01:40:01.160 --> 01:40:02.960]   until you kind of take a run at it.
[01:40:05.000 --> 01:40:07.320]   So I think it's kind of an interesting thing
[01:40:07.320 --> 01:40:08.160]   to experiment with,
[01:40:08.160 --> 01:40:10.800]   but I don't really know where this one's gonna go.
[01:40:10.800 --> 01:40:12.960]   - So since we were talking about Twitter,
[01:40:12.960 --> 01:40:19.320]   Elon Musk had what I think a few harsh words
[01:40:19.320 --> 01:40:21.040]   that I wish he didn't say.
[01:40:21.040 --> 01:40:26.040]   So let me ask, in the hope and the name of camaraderie,
[01:40:26.040 --> 01:40:29.040]   what do you think Elon is doing well with Twitter?
[01:40:29.040 --> 01:40:33.040]   And what, as a person who has run for a long time
[01:40:33.040 --> 01:40:38.040]   you social networks, Facebook, Instagram, WhatsApp,
[01:40:38.040 --> 01:40:41.600]   what can he do better?
[01:40:41.600 --> 01:40:45.220]   What can he improve on that text-based social network?
[01:40:45.220 --> 01:40:46.720]   - Gosh, it's always very difficult
[01:40:46.720 --> 01:40:50.160]   to offer specific critiques from the outside
[01:40:50.160 --> 01:40:51.560]   before you get into this,
[01:40:51.560 --> 01:40:53.900]   because I think one thing that I've learned
[01:40:53.900 --> 01:40:57.560]   is that everyone has opinions on what you should do
[01:40:57.560 --> 01:40:59.980]   and like running the company,
[01:40:59.980 --> 01:41:02.960]   you see a lot of specific nuances on things
[01:41:02.960 --> 01:41:04.600]   that are not apparent externally.
[01:41:04.600 --> 01:41:09.600]   And I often think that some of the discourse around us
[01:41:09.600 --> 01:41:15.680]   would be, could be better if there was more
[01:41:15.680 --> 01:41:18.480]   kind of space for acknowledging
[01:41:18.480 --> 01:41:20.480]   that there's certain things that we're seeing internally
[01:41:20.480 --> 01:41:21.520]   that guide what we're doing.
[01:41:21.520 --> 01:41:23.880]   But I don't know, I mean,
[01:41:23.880 --> 01:41:26.360]   'cause since you asked what is going well,
[01:41:26.360 --> 01:41:31.360]   I think that, you know, I do think that Elon led a push
[01:41:31.360 --> 01:41:41.400]   early on to make Twitter a lot leaner.
[01:41:41.400 --> 01:41:46.400]   And I think that that, you know,
[01:41:46.400 --> 01:41:48.800]   it's like you can agree or disagree
[01:41:48.800 --> 01:41:51.400]   with exactly all the tactics and how we did that.
[01:41:51.400 --> 01:41:53.240]   You know, obviously, you know,
[01:41:53.240 --> 01:41:56.440]   every leader has their own style for if they,
[01:41:56.440 --> 01:41:58.480]   you know, if you need to make dramatic changes for that,
[01:41:58.480 --> 01:41:59.880]   how you're gonna execute it.
[01:41:59.880 --> 01:42:04.520]   But a lot of the specific principles that he pushed on
[01:42:04.520 --> 01:42:09.360]   around basically trying to make the organization
[01:42:09.360 --> 01:42:12.760]   more technical, around decreasing the distance
[01:42:12.760 --> 01:42:16.280]   between engineers at the company and him,
[01:42:16.280 --> 01:42:17.980]   like fewer layers of management.
[01:42:19.960 --> 01:42:23.280]   I think that those were generally good changes.
[01:42:23.280 --> 01:42:26.240]   And I'm also, I also think that it was probably good
[01:42:26.240 --> 01:42:28.640]   for the industry that he made those changes,
[01:42:28.640 --> 01:42:30.640]   because my sense is that there were a lot of other people
[01:42:30.640 --> 01:42:33.240]   who thought that those were good changes,
[01:42:33.240 --> 01:42:38.240]   but who may have been a little shy about doing them.
[01:42:38.240 --> 01:42:42.200]   And I think he, you know,
[01:42:42.200 --> 01:42:45.280]   just in my conversations with other founders
[01:42:45.280 --> 01:42:47.200]   and how people have reacted to the things that we've done,
[01:42:47.200 --> 01:42:48.800]   you know, what I've heard from a lot of folks
[01:42:48.800 --> 01:42:52.040]   is just, hey, you know, when someone like you,
[01:42:52.040 --> 01:42:54.360]   when I wrote the letter outlining
[01:42:54.360 --> 01:42:57.200]   the organizational changes that I wanted to make
[01:42:57.200 --> 01:42:58.640]   back in March, and you know,
[01:42:58.640 --> 01:43:01.040]   when people see what Elon is doing,
[01:43:01.040 --> 01:43:03.640]   I think that that gives, you know, people
[01:43:03.640 --> 01:43:06.360]   the ability to think through
[01:43:06.360 --> 01:43:09.200]   how to shape their organizations in a way
[01:43:09.200 --> 01:43:13.600]   that, you know, hopefully can be good for the industry
[01:43:13.600 --> 01:43:16.080]   and make all these companies more productive over time.
[01:43:16.080 --> 01:43:19.480]   So, something that that was one where I think he was
[01:43:19.480 --> 01:43:23.600]   quite ahead of a bunch of the other companies on.
[01:43:23.600 --> 01:43:26.720]   And, you know, what he was doing there,
[01:43:26.720 --> 01:43:28.240]   you know, again, from the outside, very hard to know.
[01:43:28.240 --> 01:43:29.800]   It's like, okay, did he cut too much?
[01:43:29.800 --> 01:43:30.640]   Did he not cut enough?
[01:43:30.640 --> 01:43:31.480]   Whatever.
[01:43:31.480 --> 01:43:35.000]   I don't think it's like my place to opine on that.
[01:43:35.000 --> 01:43:38.240]   And you asked for a positive framing of the question
[01:43:38.240 --> 01:43:40.880]   of what do I admire?
[01:43:40.880 --> 01:43:41.960]   What do I think went well?
[01:43:41.960 --> 01:43:45.360]   But I think that like certainly his actions
[01:43:46.120 --> 01:43:49.800]   led me and I think a lot of other folks in the industry
[01:43:49.800 --> 01:43:53.640]   to think about, hey, are we kind of doing this
[01:43:53.640 --> 01:43:54.680]   as much as we should?
[01:43:54.680 --> 01:43:57.160]   Like, can we, like, could we make our companies better
[01:43:57.160 --> 01:43:59.400]   by pushing on some of these same principles?
[01:43:59.400 --> 01:44:01.960]   Well, the two of you are in the top of the world
[01:44:01.960 --> 01:44:04.000]   in terms of leading the development of tech.
[01:44:04.000 --> 01:44:09.000]   And I wish there was more both way camaraderie and kindness,
[01:44:09.000 --> 01:44:13.440]   more love in the world, because love is the answer.
[01:44:14.440 --> 01:44:19.200]   But let me ask on a point of efficiency.
[01:44:19.200 --> 01:44:22.580]   You recently announced multiple stages of layoffs at Meta.
[01:44:22.580 --> 01:44:27.600]   What are the most painful aspects of this process
[01:44:27.600 --> 01:44:31.560]   given for the individuals, the painful effects
[01:44:31.560 --> 01:44:32.840]   it has on those people's lives?
[01:44:32.840 --> 01:44:34.160]   - Yeah, I mean, that's it.
[01:44:34.160 --> 01:44:35.920]   And that's it.
[01:44:35.920 --> 01:44:41.280]   And you basically have a significant number of people
[01:44:41.280 --> 01:44:45.680]   who, this is just not the end of their time at Meta
[01:44:45.680 --> 01:44:49.840]   that they or I would have hoped for
[01:44:49.840 --> 01:44:51.240]   when they joined the company.
[01:44:51.240 --> 01:44:57.640]   And I mean, running a company, people are constantly joining
[01:44:57.640 --> 01:45:01.120]   and leaving the company for different directions,
[01:45:01.120 --> 01:45:03.880]   but for different reasons.
[01:45:03.880 --> 01:45:08.880]   But layoffs are uniquely challenging and tough
[01:45:10.440 --> 01:45:13.040]   in that you have a lot of people leaving
[01:45:13.040 --> 01:45:17.600]   for reasons that aren't connected to their own performance
[01:45:17.600 --> 01:45:22.080]   or the culture not being a fit at that point.
[01:45:22.080 --> 01:45:27.080]   It's really just, it's a kind of strategy decision
[01:45:27.080 --> 01:45:29.260]   and sometimes financially required,
[01:45:29.260 --> 01:45:33.760]   but not fully in our case.
[01:45:33.760 --> 01:45:35.840]   I mean, especially on the changes that we made this year,
[01:45:35.840 --> 01:45:38.200]   a lot of it was more kind of culturally
[01:45:38.200 --> 01:45:40.600]   and strategically driven by this push
[01:45:40.600 --> 01:45:44.480]   where I wanted us to become a stronger technology company
[01:45:44.480 --> 01:45:47.640]   with more of a focus on building more technical
[01:45:47.640 --> 01:45:50.040]   and more of a focus on building
[01:45:50.040 --> 01:45:52.240]   higher quality products faster.
[01:45:52.240 --> 01:45:53.720]   And I just view the external world
[01:45:53.720 --> 01:45:56.080]   as quite volatile right now.
[01:45:56.080 --> 01:46:00.400]   And I wanted to make sure that we had a stable position
[01:46:00.400 --> 01:46:01.720]   to be able to continue investing
[01:46:01.720 --> 01:46:05.400]   in these long-term ambitious projects that we have
[01:46:05.400 --> 01:46:07.680]   around continuing to push AI forward
[01:46:07.680 --> 01:46:10.320]   and continuing to push forward all the metaverse work.
[01:46:10.320 --> 01:46:15.120]   And in order to do that in light of the pretty big thrash
[01:46:15.120 --> 01:46:18.120]   that we had seen over the last 18 months,
[01:46:18.120 --> 01:46:21.480]   some of it macroeconomic induced,
[01:46:21.480 --> 01:46:24.160]   some of it competitively induced,
[01:46:24.160 --> 01:46:27.360]   some of it just because of bad decisions
[01:46:27.360 --> 01:46:28.880]   or things that we got wrong.
[01:46:28.880 --> 01:46:32.320]   I don't know, I just, I decided that we needed to get
[01:46:32.320 --> 01:46:34.200]   to a point where we were a lot leaner.
[01:46:35.280 --> 01:46:36.720]   But look, I mean, but then, okay,
[01:46:36.720 --> 01:46:38.720]   it's one thing to do that, to like decide that
[01:46:38.720 --> 01:46:40.360]   at a high level, then the question is,
[01:46:40.360 --> 01:46:42.840]   how do you execute that as compassionately as possible?
[01:46:42.840 --> 01:46:44.400]   And there's no good way.
[01:46:44.400 --> 01:46:47.400]   There's no perfect way for sure.
[01:46:47.400 --> 01:46:49.360]   And it's gonna be tough no matter what,
[01:46:49.360 --> 01:46:53.440]   but as a leadership team here,
[01:46:53.440 --> 01:46:55.320]   we've certainly spent a lot of time just thinking,
[01:46:55.320 --> 01:46:58.400]   okay, given that this is a thing that sucks,
[01:46:58.400 --> 01:47:01.000]   like what is the most compassionate way
[01:47:01.000 --> 01:47:01.960]   that we can do this?
[01:47:01.960 --> 01:47:05.200]   And that's what we've tried to do.
[01:47:05.200 --> 01:47:08.480]   - And you mentioned there's an increased focus
[01:47:08.480 --> 01:47:13.160]   on engineering, on tech, so technology teams,
[01:47:13.160 --> 01:47:17.000]   tech focus teams on building products that.
[01:47:17.000 --> 01:47:19.320]   - Yeah, I mean, I wanted to,
[01:47:19.320 --> 01:47:25.680]   I want to empower engineers more,
[01:47:25.680 --> 01:47:28.440]   the people who are building things, the technical teams.
[01:47:28.440 --> 01:47:33.960]   Part of that is making sure that the people
[01:47:33.960 --> 01:47:35.840]   who are building things aren't just at like
[01:47:35.840 --> 01:47:37.480]   the leaf nodes of the organization.
[01:47:37.480 --> 01:47:40.640]   I don't want like eight levels of management
[01:47:40.640 --> 01:47:42.880]   and then the people actually doing the work.
[01:47:42.880 --> 01:47:44.880]   So we made changes to make it so that
[01:47:44.880 --> 01:47:47.120]   you have individual contributor engineers reporting
[01:47:47.120 --> 01:47:49.200]   at almost every level up the stack,
[01:47:49.200 --> 01:47:51.160]   which I think is important because you're running a company.
[01:47:51.160 --> 01:47:54.200]   One of the big questions is latency
[01:47:54.200 --> 01:47:56.400]   of information that you get.
[01:47:56.400 --> 01:47:59.480]   We talked about this a bit earlier in terms of
[01:47:59.480 --> 01:48:03.720]   kind of the joy of, and the feedback that you get doing
[01:48:03.720 --> 01:48:05.640]   something like jujitsu compared to
[01:48:05.640 --> 01:48:07.360]   running a long-term project.
[01:48:07.360 --> 01:48:09.960]   But I actually think part of the art of running a company
[01:48:09.960 --> 01:48:13.400]   is trying to constantly re-engineer it
[01:48:13.400 --> 01:48:15.040]   so that your feedback loops get shorter
[01:48:15.040 --> 01:48:16.120]   so you can learn faster.
[01:48:16.120 --> 01:48:18.480]   And part of the way that you do that is by,
[01:48:18.480 --> 01:48:20.280]   I kind of think that every layer that you have
[01:48:20.280 --> 01:48:24.360]   in the organization means that information
[01:48:24.360 --> 01:48:27.280]   might not need to get reviewed before it goes to you.
[01:48:27.280 --> 01:48:29.160]   And I think, you know, making it so that the people
[01:48:29.160 --> 01:48:31.800]   doing the work are as close as possible to you as possible
[01:48:31.800 --> 01:48:34.440]   is pretty important.
[01:48:34.440 --> 01:48:35.600]   So there's that.
[01:48:35.600 --> 01:48:38.320]   I think over time, companies just build up
[01:48:38.320 --> 01:48:41.600]   very large support functions that are not doing
[01:48:41.600 --> 01:48:43.280]   the kind of core technical work.
[01:48:43.280 --> 01:48:45.400]   And those functions are very important,
[01:48:45.400 --> 01:48:47.400]   but I think having them in the right proportion
[01:48:47.400 --> 01:48:48.400]   is important.
[01:48:48.400 --> 01:48:53.400]   And if you try to do good work, but you don't have,
[01:48:53.400 --> 01:48:56.240]   you know, the right marketing team
[01:48:56.240 --> 01:49:00.240]   or the right legal advice, like you're gonna, you know,
[01:49:00.240 --> 01:49:01.920]   make some pretty big blunders.
[01:49:01.920 --> 01:49:05.280]   But at the same time, if you have, you know,
[01:49:05.280 --> 01:49:09.200]   if you just like have too big of things
[01:49:09.200 --> 01:49:11.040]   and some of these support roles,
[01:49:11.040 --> 01:49:14.120]   then that might make it so that things are,
[01:49:14.120 --> 01:49:15.120]   just move a lot.
[01:49:15.120 --> 01:49:19.560]   Maybe you're too conservative or you move a lot slower
[01:49:19.560 --> 01:49:22.080]   than you should otherwise.
[01:49:22.080 --> 01:49:23.680]   I just use, those are just examples.
[01:49:23.680 --> 01:49:25.760]   But it's, but-
[01:49:25.760 --> 01:49:26.800]   - How do you find that balance?
[01:49:26.800 --> 01:49:27.640]   That's really tough.
[01:49:27.640 --> 01:49:29.840]   - Yeah, no, but that's, it's a constant equilibrium
[01:49:29.840 --> 01:49:31.680]   that you're searching for.
[01:49:31.680 --> 01:49:33.520]   - Yeah, how many managers to have?
[01:49:33.520 --> 01:49:36.200]   What are the pros and cons of managers?
[01:49:36.200 --> 01:49:38.480]   - Well, I mean, I believe a lot in management.
[01:49:38.480 --> 01:49:39.400]   I mean, there are some people who think
[01:49:39.400 --> 01:49:41.480]   that it doesn't matter as much, but look, I mean,
[01:49:41.480 --> 01:49:43.680]   we have a lot of younger people at the company
[01:49:43.680 --> 01:49:46.240]   for whom this is their first job and, you know,
[01:49:46.240 --> 01:49:48.320]   people need to grow and learn in their career.
[01:49:48.320 --> 01:49:50.240]   And I think that all that stuff is important,
[01:49:50.240 --> 01:49:52.560]   but here's one mathematical way to look at it.
[01:49:52.560 --> 01:49:56.480]   You know, at the beginning of this, we,
[01:49:58.480 --> 01:50:01.760]   I asked our people team, what was the average number
[01:50:01.760 --> 01:50:03.840]   of reports that a manager had?
[01:50:03.840 --> 01:50:08.520]   And I think it was around three, maybe three to four,
[01:50:08.520 --> 01:50:10.080]   but closer to three.
[01:50:10.080 --> 01:50:13.360]   I was like, wow, like a manager can, you know,
[01:50:13.360 --> 01:50:15.920]   best practices that person can manage,
[01:50:15.920 --> 01:50:18.240]   you know, seven or eight people.
[01:50:18.240 --> 01:50:20.120]   But there was a reason why it was closer to three.
[01:50:20.120 --> 01:50:22.720]   It was because we were growing so quickly, right?
[01:50:22.720 --> 01:50:25.960]   And when you're hiring so many people so quickly,
[01:50:25.960 --> 01:50:28.320]   then that means that you need managers
[01:50:28.320 --> 01:50:31.240]   who have capacity to onboard new people.
[01:50:31.240 --> 01:50:32.560]   And also, if you have a new manager,
[01:50:32.560 --> 01:50:34.880]   you may not wanna have them have seven direct reports
[01:50:34.880 --> 01:50:36.880]   immediately 'cause you want them to ramp up.
[01:50:36.880 --> 01:50:39.800]   But the thing is going forward, I don't want us
[01:50:39.800 --> 01:50:42.640]   to actually hire that many people that quickly, right?
[01:50:42.640 --> 01:50:44.960]   So I actually think we'll just do better work
[01:50:44.960 --> 01:50:47.520]   if we have more constraints and we're, you know,
[01:50:47.520 --> 01:50:48.840]   leaner as an organization.
[01:50:48.840 --> 01:50:50.600]   So in a world where we're not adding
[01:50:50.600 --> 01:50:53.840]   so many people as quickly, is it as valuable
[01:50:53.840 --> 01:50:56.040]   to have a lot of managers who have extra capacity
[01:50:56.040 --> 01:50:56.920]   waiting for new people?
[01:50:56.920 --> 01:50:57.760]   No, right?
[01:50:57.760 --> 01:51:01.640]   So now we can sort of defragment the organization
[01:51:01.640 --> 01:51:03.920]   and get to a place where the average is closer
[01:51:03.920 --> 01:51:05.920]   to that seven or eight.
[01:51:05.920 --> 01:51:08.720]   And it just ends up being a somewhat more
[01:51:08.720 --> 01:51:10.880]   kind of compact management structure,
[01:51:10.880 --> 01:51:14.160]   which, you know, decreases the latency on information
[01:51:14.160 --> 01:51:15.320]   going up and down the chain,
[01:51:15.320 --> 01:51:17.680]   and I think empowers people more.
[01:51:17.680 --> 01:51:19.480]   But I mean, that's an example that I think
[01:51:19.480 --> 01:51:23.000]   it doesn't kind of undervalue the importance of management
[01:51:23.000 --> 01:51:28.000]   and the kind of the personal growth or coaching
[01:51:28.000 --> 01:51:30.520]   that people need in order to do their jobs well.
[01:51:30.520 --> 01:51:32.040]   It's just, I think, realistically,
[01:51:32.040 --> 01:51:34.200]   we're just not gonna hire as many people going forward.
[01:51:34.200 --> 01:51:36.360]   So I think that you need a different structure.
[01:51:36.360 --> 01:51:41.120]   - This whole incredible hierarchy and network of humans
[01:51:41.120 --> 01:51:43.080]   that make up a company is fascinating.
[01:51:43.080 --> 01:51:43.920]   - Oh yeah.
[01:51:43.920 --> 01:51:45.200]   - Yeah.
[01:51:45.200 --> 01:51:48.400]   How do you hire great teams?
[01:51:48.400 --> 01:51:51.760]   How do you hire great, now with the focus on engineering
[01:51:51.760 --> 01:51:55.840]   and technical teams, how do you hire great engineers
[01:51:55.840 --> 01:51:58.560]   and great members of technical teams?
[01:51:58.560 --> 01:52:01.320]   - Well, you're asking how you select
[01:52:01.320 --> 01:52:03.000]   or how you attract them?
[01:52:03.000 --> 01:52:05.680]   - Both, but select, I think.
[01:52:05.680 --> 01:52:09.360]   I think attract is work on cool stuff and have a vision.
[01:52:09.360 --> 01:52:10.200]   (laughs)
[01:52:10.200 --> 01:52:11.040]   I think that's what we're talking about.
[01:52:11.040 --> 01:52:12.600]   - I think that's right, and have a track record
[01:52:12.600 --> 01:52:14.080]   that people think you're actually gonna be able to do it.
[01:52:14.080 --> 01:52:18.600]   - Yeah, to me, the select seems like more of the art form,
[01:52:18.600 --> 01:52:20.080]   more of the tricky thing.
[01:52:20.080 --> 01:52:20.920]   - Yeah.
[01:52:20.920 --> 01:52:23.880]   - To select the people that fit the culture
[01:52:23.880 --> 01:52:26.840]   and can get integrated the most effectively and so on.
[01:52:26.840 --> 01:52:29.400]   And maybe, especially when they're young,
[01:52:29.400 --> 01:52:34.400]   to see the magic through the resumes,
[01:52:34.400 --> 01:52:37.200]   through the paperwork and all this kind of stuff,
[01:52:37.200 --> 01:52:39.480]   to see that there's a special human there
[01:52:39.480 --> 01:52:42.880]   that would do incredible work.
[01:52:42.880 --> 01:52:46.480]   - So there are lots of different cuts on this question.
[01:52:46.480 --> 01:52:49.920]   I mean, I think when an organization is growing quickly,
[01:52:49.920 --> 01:52:53.440]   one of the big questions that teams face
[01:52:53.440 --> 01:52:55.800]   is do I hire this person who's in front of me now
[01:52:55.800 --> 01:52:57.520]   because they seem good,
[01:52:57.520 --> 01:53:01.400]   or do I hold out to get someone who's even better?
[01:53:01.400 --> 01:53:06.400]   And the heuristic that I always focused on for myself
[01:53:06.400 --> 01:53:11.200]   and my own kind of direct hiring that I think works
[01:53:11.200 --> 01:53:13.800]   when you recurse it through the organization
[01:53:13.800 --> 01:53:16.480]   is that you should only hire someone to be on your team
[01:53:16.480 --> 01:53:18.280]   if you would be happy working for them
[01:53:18.280 --> 01:53:19.880]   in an alternate universe.
[01:53:19.880 --> 01:53:22.080]   And I think that that kind of works,
[01:53:22.080 --> 01:53:24.680]   and that's basically how I've tried to build my team.
[01:53:24.680 --> 01:53:28.840]   I'm not in a rush to not be running the company,
[01:53:28.840 --> 01:53:30.360]   but I think in an alternate universe
[01:53:30.360 --> 01:53:32.400]   where one of these other folks was running the company,
[01:53:32.400 --> 01:53:33.520]   I'd be happy to work for them.
[01:53:33.520 --> 01:53:35.400]   I feel like I'd learn from them.
[01:53:35.400 --> 01:53:38.520]   I respect their kind of general judgment.
[01:53:38.520 --> 01:53:40.600]   They're all very insightful.
[01:53:40.600 --> 01:53:41.840]   They have good values.
[01:53:41.840 --> 01:53:47.560]   And I think that that gives you some rubric for...
[01:53:47.560 --> 01:53:48.920]   You can apply that at every layer.
[01:53:48.920 --> 01:53:50.400]   And I think if you apply that at every layer
[01:53:50.400 --> 01:53:51.760]   in the organization,
[01:53:51.760 --> 01:53:54.040]   then you'll have a pretty strong organization.
[01:53:54.040 --> 01:53:59.040]   Okay, in an organization that's not growing as quickly,
[01:53:59.040 --> 01:54:01.400]   the questions might be a little different though.
[01:54:01.400 --> 01:54:05.920]   And there, you asked about young people specifically,
[01:54:05.920 --> 01:54:07.640]   like people out of college.
[01:54:07.640 --> 01:54:09.760]   And one of the things that we see
[01:54:09.760 --> 01:54:12.560]   is it's a pretty basic lesson,
[01:54:12.560 --> 01:54:16.640]   but we have a much better sense of who the best people are
[01:54:16.640 --> 01:54:19.440]   who have interned at the company for a couple of months
[01:54:19.440 --> 01:54:22.320]   than by looking at them at kind of a resume
[01:54:22.320 --> 01:54:25.160]   or a short interview loop.
[01:54:25.160 --> 01:54:27.320]   I mean, obviously the in-person feel that you get
[01:54:27.320 --> 01:54:29.800]   from someone probably tells you more than the resume,
[01:54:29.800 --> 01:54:33.600]   and you can do some basic skills assessment,
[01:54:33.600 --> 01:54:36.400]   but a lot of the stuff really just is cultural.
[01:54:36.400 --> 01:54:38.480]   People thrive in different environments
[01:54:38.480 --> 01:54:44.880]   and on different teams, even within a specific company.
[01:54:44.880 --> 01:54:47.480]   And it's like the people who come
[01:54:47.480 --> 01:54:50.520]   for even a short period of time over a summer
[01:54:50.520 --> 01:54:52.160]   who do a great job here,
[01:54:52.160 --> 01:54:53.480]   you know that they're gonna be great
[01:54:53.480 --> 01:54:55.360]   if they came and joined full-time.
[01:54:55.360 --> 01:54:56.720]   And that's one of the reasons
[01:54:56.720 --> 01:54:59.120]   why we've invested so much in internship
[01:54:59.120 --> 01:55:03.840]   is basically it's a very useful sorting function,
[01:55:03.840 --> 01:55:05.240]   both for us and for the people
[01:55:05.240 --> 01:55:06.880]   who wanna try out the company.
[01:55:06.880 --> 01:55:08.000]   - You mentioned in-person,
[01:55:08.000 --> 01:55:10.160]   what do you think about remote work,
[01:55:10.160 --> 01:55:12.200]   a topic that's been discussed extensively
[01:55:12.200 --> 01:55:14.440]   because of the, over the past few years,
[01:55:14.440 --> 01:55:15.880]   because of the pandemic?
[01:55:15.880 --> 01:55:20.040]   - Yeah, I mean, I think it's a thing that's here to stay,
[01:55:20.040 --> 01:55:24.920]   but I think that there's value in both, right?
[01:55:24.920 --> 01:55:27.320]   It's not, you know,
[01:55:27.320 --> 01:55:31.400]   I wouldn't wanna run a fully remote company yet, at least.
[01:55:31.400 --> 01:55:34.200]   I think there's an asterisk on that, which is that-
[01:55:34.200 --> 01:55:36.240]   - Some of the other stuff you're working on, yeah.
[01:55:36.240 --> 01:55:37.080]   - Yeah, exactly.
[01:55:37.080 --> 01:55:39.880]   It's like all the, you know, metaverse work
[01:55:39.880 --> 01:55:43.600]   and the ability to be, to feel like you're truly present.
[01:55:44.560 --> 01:55:45.800]   No matter where you are.
[01:55:45.800 --> 01:55:48.480]   I think once you have that all dialed in,
[01:55:48.480 --> 01:55:50.680]   then we may, you know, one day reach a point
[01:55:50.680 --> 01:55:52.600]   where it really just doesn't matter as much
[01:55:52.600 --> 01:55:53.880]   where you are physically.
[01:55:53.880 --> 01:56:00.680]   But, I don't know, today it still does, right?
[01:56:00.680 --> 01:56:04.520]   So yeah, for people who,
[01:56:04.520 --> 01:56:07.000]   there are all these people who have special skills
[01:56:07.000 --> 01:56:09.880]   and wanna live in a place where we don't have an office.
[01:56:09.880 --> 01:56:11.600]   Are we better off having them at the company?
[01:56:11.600 --> 01:56:12.760]   Absolutely, right?
[01:56:12.760 --> 01:56:15.400]   And are a lot of people who work at the company
[01:56:15.400 --> 01:56:17.840]   for several years and then, you know,
[01:56:17.840 --> 01:56:19.920]   build up the relationships internally
[01:56:19.920 --> 01:56:23.160]   and kind of have the trust
[01:56:23.160 --> 01:56:24.960]   and have a sense of how the company works.
[01:56:24.960 --> 01:56:26.720]   Can they go work remotely now if they want
[01:56:26.720 --> 01:56:28.000]   and still do it as effectively?
[01:56:28.000 --> 01:56:30.280]   And we've done all these studies that show it's like, okay,
[01:56:30.280 --> 01:56:31.560]   does that affect their performance?
[01:56:31.560 --> 01:56:32.400]   It does not.
[01:56:32.400 --> 01:56:36.480]   But, you know, for the new folks who are joining
[01:56:36.480 --> 01:56:40.480]   and for people who are earlier in their career
[01:56:40.480 --> 01:56:43.080]   and need to learn how to solve certain problems
[01:56:43.080 --> 01:56:45.080]   and need to get ramped up on the culture,
[01:56:45.080 --> 01:56:48.360]   you know, when you're working through
[01:56:48.360 --> 01:56:49.760]   really complicated problems
[01:56:49.760 --> 01:56:51.480]   where you don't just wanna sit in the,
[01:56:51.480 --> 01:56:52.880]   you don't just want the formal meeting,
[01:56:52.880 --> 01:56:55.000]   but you wanna be able to like brainstorm
[01:56:55.000 --> 01:56:56.400]   when you're walking in the hallway together
[01:56:56.400 --> 01:56:57.440]   after the meeting.
[01:56:57.440 --> 01:57:00.600]   I don't know, it's like we just haven't replaced
[01:57:00.600 --> 01:57:05.120]   the kind of in-person dynamics there yet
[01:57:05.120 --> 01:57:08.880]   with anything remote yet.
[01:57:08.880 --> 01:57:10.400]   So. - Yeah, there's a magic
[01:57:10.400 --> 01:57:12.120]   to the in-person that,
[01:57:12.120 --> 01:57:13.560]   we'll talk about this a little bit more,
[01:57:13.560 --> 01:57:15.640]   but I'm really excited by the possibilities
[01:57:15.640 --> 01:57:18.960]   in the next few years in virtual reality and mixed reality
[01:57:18.960 --> 01:57:21.840]   that are possible with high resolution scans.
[01:57:21.840 --> 01:57:26.840]   I mean, I, as a person who loves in-person interaction,
[01:57:26.840 --> 01:57:29.560]   like these podcasts in person,
[01:57:29.560 --> 01:57:33.440]   it would be incredible to achieve the level of realism
[01:57:33.440 --> 01:57:35.200]   I've gotten the chance to witness.
[01:57:35.200 --> 01:57:38.280]   But let me ask about that.
[01:57:38.280 --> 01:57:39.520]   - Yeah. - I got a chance
[01:57:39.520 --> 01:57:44.200]   to look at the Quest 3 headset,
[01:57:44.200 --> 01:57:45.640]   and it is amazing.
[01:57:45.640 --> 01:57:48.960]   You've announced it.
[01:57:48.960 --> 01:57:52.840]   You'll give some more details in the fall,
[01:57:52.840 --> 01:57:53.760]   maybe release in the fall.
[01:57:53.760 --> 01:57:55.000]   When is it getting released again?
[01:57:55.000 --> 01:57:56.320]   I forgot, you mentioned it.
[01:57:56.320 --> 01:57:57.800]   - We'll give more details at Connect,
[01:57:57.800 --> 01:57:59.120]   but it's coming this fall.
[01:57:59.120 --> 01:57:59.960]   - Okay.
[01:57:59.960 --> 01:58:06.160]   So it's priced at 499.
[01:58:07.240 --> 01:58:09.640]   What features are you most excited about there?
[01:58:09.640 --> 01:58:11.320]   - There are basically two big new things
[01:58:11.320 --> 01:58:14.400]   that we've added to Quest 3 over Quest 2.
[01:58:14.400 --> 01:58:17.280]   The first is high resolution mixed reality.
[01:58:17.280 --> 01:58:22.720]   And the basic idea here is that,
[01:58:22.720 --> 01:58:26.280]   you can think about virtual reality as you have the headset
[01:58:26.280 --> 01:58:29.000]   and all the pixels are virtual,
[01:58:29.000 --> 01:58:32.160]   and you're basically immersed in a different world.
[01:58:32.160 --> 01:58:35.720]   Mixed reality is where you see the physical world around you
[01:58:35.720 --> 01:58:37.280]   and you can place virtual objects in it,
[01:58:37.280 --> 01:58:40.200]   whether that's a screen to watch a movie
[01:58:40.200 --> 01:58:42.440]   or a projection of your virtual desktop,
[01:58:42.440 --> 01:58:44.840]   or you're playing a game where like zombies
[01:58:44.840 --> 01:58:47.600]   are coming out through the wall and you need to shoot them.
[01:58:47.600 --> 01:58:50.760]   Or we're playing Dungeons and Dragons or some board game
[01:58:50.760 --> 01:58:52.800]   and we just have a virtual version of the board
[01:58:52.800 --> 01:58:54.760]   in front of us while we're sitting here.
[01:58:54.760 --> 01:58:57.960]   All that's possible in mixed reality.
[01:58:57.960 --> 01:58:59.800]   And I think that that is going to be
[01:58:59.800 --> 01:59:02.520]   the next big capability on top of virtual reality.
[01:59:02.520 --> 01:59:05.520]   - It is done so well.
[01:59:05.520 --> 01:59:08.440]   I have to say as a person who experienced it today
[01:59:08.440 --> 01:59:13.440]   with zombies, having a full awareness of the environment
[01:59:13.440 --> 01:59:16.840]   and integrating that environment in the way they run at you
[01:59:16.840 --> 01:59:18.160]   while they try to kill you.
[01:59:18.160 --> 01:59:20.560]   So it's just the mixed reality,
[01:59:20.560 --> 01:59:23.400]   the pass through is really, really, really well done.
[01:59:23.400 --> 01:59:28.240]   And the fact that it's only $500 is really, it's well done.
[01:59:28.240 --> 01:59:29.080]   - Thank you.
[01:59:29.080 --> 01:59:30.600]   I mean, I'm super excited about it.
[01:59:30.600 --> 01:59:33.920]   I mean, our, and we put a lot of work
[01:59:33.920 --> 01:59:38.920]   into making the device both as good as possible
[01:59:38.920 --> 01:59:40.640]   and as affordable as possible
[01:59:40.640 --> 01:59:43.240]   because a big part of our mission and ethos here
[01:59:43.240 --> 01:59:46.320]   is we want people to be able to connect with each other.
[01:59:46.320 --> 01:59:49.120]   We want to reach and we want to serve a lot of people.
[01:59:49.120 --> 01:59:51.800]   We want to bring this technology to everyone.
[01:59:51.800 --> 01:59:53.520]   So we're not just trying to serve
[01:59:53.520 --> 01:59:57.720]   an elite, a wealthy crowd.
[01:59:57.720 --> 02:00:01.280]   We really want this to be accessible.
[02:00:01.280 --> 02:00:03.840]   So that is in a lot of ways
[02:00:03.840 --> 02:00:05.440]   an extremely hard technical problem
[02:00:05.440 --> 02:00:08.280]   because we don't just have the ability
[02:00:08.280 --> 02:00:10.880]   to put an unlimited amount of hardware in this.
[02:00:10.880 --> 02:00:12.960]   We needed to basically deliver something
[02:00:12.960 --> 02:00:16.120]   that works really well, but in an affordable package.
[02:00:16.120 --> 02:00:18.200]   And we started with Quest Pro last year.
[02:00:18.200 --> 02:00:23.000]   It was $1,500.
[02:00:23.000 --> 02:00:25.240]   And now we've lowered the price to a thousand,
[02:00:25.240 --> 02:00:28.640]   but in a lot of ways, the mixed reality in Quest 3
[02:00:28.640 --> 02:00:31.680]   is an even better and more advanced level
[02:00:31.680 --> 02:00:33.480]   than what we were able to deliver in Quest Pro.
[02:00:33.480 --> 02:00:38.240]   So I'm really proud of where we are with Quest 3 on that.
[02:00:38.240 --> 02:00:40.760]   It's going to work with all of the virtual reality titles
[02:00:40.760 --> 02:00:42.840]   and everything that existed there.
[02:00:42.840 --> 02:00:45.240]   So people who want to play fully immersive games,
[02:00:45.240 --> 02:00:49.320]   social experiences, fitness, all that stuff will work,
[02:00:49.320 --> 02:00:51.480]   but now you'll also get mixed reality too,
[02:00:51.480 --> 02:00:54.240]   which I think people really like
[02:00:54.240 --> 02:00:58.360]   because sometimes you want to be super immersed in a game,
[02:00:58.360 --> 02:01:00.440]   but a lot of the time,
[02:01:00.440 --> 02:01:02.520]   especially when you're moving around, if you're active,
[02:01:02.520 --> 02:01:04.920]   like you're doing some fitness experience,
[02:01:04.920 --> 02:01:08.600]   let's say you're doing boxing or something,
[02:01:08.600 --> 02:01:10.000]   it's like you kind of want to be able
[02:01:10.000 --> 02:01:11.120]   to see the room around you.
[02:01:11.120 --> 02:01:13.320]   So that way you know that I'm not going to punch a lamp
[02:01:13.320 --> 02:01:14.480]   or something like that.
[02:01:14.480 --> 02:01:17.000]   And I don't know if you got to play with this experience,
[02:01:17.000 --> 02:01:18.440]   but we basically have the,
[02:01:18.440 --> 02:01:20.360]   and it's just sort of like a fun little demo
[02:01:20.360 --> 02:01:21.200]   that we put together.
[02:01:21.200 --> 02:01:23.800]   But it's like you just,
[02:01:23.800 --> 02:01:26.560]   we're like in a conference room or your living room
[02:01:26.560 --> 02:01:30.120]   and you have the guy there and you're boxing him
[02:01:30.120 --> 02:01:31.760]   and you're fighting him and it's like.
[02:01:31.760 --> 02:01:32.880]   - All the other people are there too.
[02:01:32.880 --> 02:01:34.120]   I got a chance to do that.
[02:01:34.120 --> 02:01:35.560]   And all the people are there.
[02:01:35.560 --> 02:01:39.200]   It's like that guy's right there.
[02:01:39.200 --> 02:01:40.720]   - Yeah, it's like it's right in the room.
[02:01:40.720 --> 02:01:42.480]   - And the other humans, the path,
[02:01:42.480 --> 02:01:44.360]   you're seeing them also, they can cheer you on,
[02:01:44.360 --> 02:01:45.240]   they can make fun of you
[02:01:45.240 --> 02:01:47.200]   if they're anything like friends of mine.
[02:01:47.200 --> 02:01:52.200]   And then just, yeah, it's really,
[02:01:52.200 --> 02:01:55.320]   it's a really compelling experience.
[02:01:55.320 --> 02:01:56.960]   And VR is really interesting too,
[02:01:56.960 --> 02:01:58.800]   but this is something else almost.
[02:01:58.800 --> 02:02:03.560]   This becomes integrated into your life, into your world.
[02:02:03.560 --> 02:02:06.880]   - Yeah, and it, so I think it's a completely new capability
[02:02:06.880 --> 02:02:09.280]   that will unlock a lot of different content.
[02:02:09.280 --> 02:02:11.480]   And I think it'll also just make the experience
[02:02:11.480 --> 02:02:13.120]   more comfortable for a set of people
[02:02:13.120 --> 02:02:16.840]   who didn't want to have only fully immersive experiences.
[02:02:16.840 --> 02:02:19.280]   I think if you want experiences where you're grounded in,
[02:02:19.280 --> 02:02:21.680]   you know, your living room and the physical world around you,
[02:02:21.680 --> 02:02:23.800]   now you'll be able to have that too.
[02:02:23.800 --> 02:02:24.920]   And I think that that's pretty exciting.
[02:02:24.920 --> 02:02:28.640]   - I really liked how it added windows
[02:02:28.640 --> 02:02:30.480]   to a room with no windows.
[02:02:30.480 --> 02:02:31.320]   - Yeah.
[02:02:31.320 --> 02:02:32.160]   - Me as a person.
[02:02:32.160 --> 02:02:32.980]   - Did you see the aquarium one
[02:02:32.980 --> 02:02:34.160]   where you could see the sharks swim up?
[02:02:34.160 --> 02:02:35.520]   Or was that just the zombie one?
[02:02:35.520 --> 02:02:37.640]   - Just the zombie one, but it's still outside.
[02:02:37.640 --> 02:02:39.360]   - You don't necessarily want windows added
[02:02:39.360 --> 02:02:41.000]   to your living room where zombies come out of,
[02:02:41.000 --> 02:02:43.920]   but yes, in the context of that game, it's yeah, yeah.
[02:02:43.920 --> 02:02:47.560]   - I enjoyed it 'cause you could see the nature outside.
[02:02:47.560 --> 02:02:50.000]   And me as a person that doesn't have windows,
[02:02:50.000 --> 02:02:52.120]   it's just nice to have nature.
[02:02:52.120 --> 02:02:53.720]   - Yeah, well.
[02:02:53.720 --> 02:02:56.440]   - Even if it's a mixed reality setting.
[02:02:56.440 --> 02:03:01.400]   I know it's a zombie game, but there's a zen nature,
[02:03:01.400 --> 02:03:03.720]   zen aspect to being able to look outside
[02:03:03.720 --> 02:03:06.660]   and alter your environment as you know it.
[02:03:06.660 --> 02:03:09.040]   - Yeah.
[02:03:09.040 --> 02:03:12.400]   There will probably be better, more zen ways to do that
[02:03:12.400 --> 02:03:13.760]   than the zombie game you're describing,
[02:03:13.760 --> 02:03:16.160]   but you're right that the basic idea
[02:03:16.160 --> 02:03:20.480]   of sort of having your physical environment on pass-through,
[02:03:20.480 --> 02:03:24.880]   but then being able to bring in different elements,
[02:03:24.880 --> 02:03:27.640]   I think it's gonna be super powerful.
[02:03:27.640 --> 02:03:30.720]   And in some ways, I think that these are,
[02:03:30.720 --> 02:03:33.160]   mixed reality is also a predecessor to,
[02:03:33.160 --> 02:03:35.060]   eventually we will get AR glasses
[02:03:35.060 --> 02:03:37.420]   that are not kind of the goggles form factor
[02:03:37.420 --> 02:03:40.200]   of the current generation of headsets
[02:03:40.200 --> 02:03:41.680]   that people are making.
[02:03:41.680 --> 02:03:44.060]   But I think a lot of the experiences
[02:03:44.060 --> 02:03:46.260]   that developers are making for mixed reality
[02:03:46.260 --> 02:03:48.680]   of basically you just have a kind of a hologram
[02:03:48.680 --> 02:03:50.060]   that you're putting in the world,
[02:03:50.060 --> 02:03:53.560]   will hopefully apply once we get the AR glasses too.
[02:03:53.560 --> 02:03:56.760]   Now that's got its own whole set of challenges and it's-
[02:03:56.760 --> 02:03:58.280]   - Well, the headset's already smaller
[02:03:58.280 --> 02:04:00.040]   than the previous version.
[02:04:00.040 --> 02:04:01.720]   - Oh yeah, it's 40% thinner.
[02:04:01.720 --> 02:04:03.360]   And the other thing that I think is good about it,
[02:04:03.360 --> 02:04:05.960]   yeah, so mixed reality was the first big thing.
[02:04:05.960 --> 02:04:10.160]   The second is it's just a great VR headset.
[02:04:10.160 --> 02:04:13.220]   It's, I mean, it's got 2X the graphics processing power,
[02:04:13.220 --> 02:04:18.880]   40% sharper screens, 40% thinner, more comfortable,
[02:04:18.880 --> 02:04:21.280]   better strap architecture, all this stuff that,
[02:04:21.280 --> 02:04:22.440]   you know, if you liked Quest 2,
[02:04:22.440 --> 02:04:24.040]   I think that this is just gonna be,
[02:04:24.040 --> 02:04:25.880]   it's like all the content that you might've played
[02:04:25.880 --> 02:04:27.980]   in Quest 2 is just gonna get sharper automatically
[02:04:27.980 --> 02:04:29.000]   and look better in this.
[02:04:29.000 --> 02:04:31.640]   So it's, I think people are really gonna like it.
[02:04:31.640 --> 02:04:33.680]   Yeah, so this fall.
[02:04:33.680 --> 02:04:36.280]   - This fall, I have to ask,
[02:04:36.280 --> 02:04:40.020]   Apple just announced a mixed reality headset
[02:04:40.020 --> 02:04:45.020]   called Vision Pro for $3,500, available in early 2024.
[02:04:45.020 --> 02:04:47.720]   What do you think about this headset?
[02:04:48.640 --> 02:04:51.800]   - Well, I saw the materials when they launched.
[02:04:51.800 --> 02:04:53.800]   I haven't gotten a chance to play with it yet.
[02:04:53.800 --> 02:04:56.160]   So kind of take everything with a grain of salt,
[02:04:56.160 --> 02:04:58.980]   but a few high level thoughts.
[02:04:58.980 --> 02:05:03.980]   I mean, first, you know, I do think that this is
[02:05:03.980 --> 02:05:09.940]   a certain level of validation for the category, right?
[02:05:09.940 --> 02:05:13.520]   Where, you know, we were the primary folks out there
[02:05:13.520 --> 02:05:16.800]   before saying, hey, I think that this, you know,
[02:05:16.800 --> 02:05:19.000]   virtual reality, augmented reality, mixed reality,
[02:05:19.000 --> 02:05:21.960]   this is gonna be a big part of the next computing platform.
[02:05:21.960 --> 02:05:28.240]   I think having Apple come in and share that vision
[02:05:28.240 --> 02:05:34.360]   will make a lot of people who are fans of their products
[02:05:34.360 --> 02:05:36.400]   really consider that.
[02:05:36.400 --> 02:05:41.700]   And then, you know, of course the $3,500 price,
[02:05:41.700 --> 02:05:45.000]   you know, on the one hand, I get it
[02:05:45.000 --> 02:05:47.200]   for with all the stuff that they're trying to pack in there.
[02:05:47.200 --> 02:05:49.680]   On the other hand, a lot of people aren't gonna find that
[02:05:49.680 --> 02:05:51.240]   to be affordable.
[02:05:51.240 --> 02:05:53.840]   So I think that there's a chance that them coming in
[02:05:53.840 --> 02:05:57.600]   actually increases demand for the overall space
[02:05:57.600 --> 02:06:01.400]   and that Quest 3 is actually the primary beneficiary of that
[02:06:01.400 --> 02:06:03.880]   because a lot of the people who might say,
[02:06:03.880 --> 02:06:07.160]   hey, you know, this, like I'm gonna give another
[02:06:07.160 --> 02:06:09.280]   consideration to this, or, you know,
[02:06:09.280 --> 02:06:11.960]   now I understand maybe what mixed reality is more
[02:06:11.960 --> 02:06:14.480]   and Quest 3 is the best one on the market
[02:06:14.480 --> 02:06:16.320]   that I can afford.
[02:06:16.320 --> 02:06:18.280]   And it's great also, right?
[02:06:18.280 --> 02:06:20.920]   I think that that's, and, you know, in our own way,
[02:06:20.920 --> 02:06:23.120]   I think we're, and there are a lot of features that we have
[02:06:23.120 --> 02:06:24.280]   where we're leading on.
[02:06:24.280 --> 02:06:29.040]   So I think that that's, that I think is gonna be a very,
[02:06:29.040 --> 02:06:30.280]   that could be quite good.
[02:06:30.280 --> 02:06:33.120]   And then obviously over time,
[02:06:33.120 --> 02:06:35.560]   the companies are just focused on
[02:06:35.560 --> 02:06:36.960]   somewhat different things, right?
[02:06:36.960 --> 02:06:41.360]   Apple has always, you know, I think focused on building
[02:06:42.280 --> 02:06:45.280]   really kind of high-end things,
[02:06:45.280 --> 02:06:49.240]   whereas our focus has been on,
[02:06:49.240 --> 02:06:51.480]   it's just, we have a more democratic ethos.
[02:06:51.480 --> 02:06:53.800]   We wanna build things that are accessible
[02:06:53.800 --> 02:06:55.440]   to a wider number of people.
[02:06:55.440 --> 02:07:00.080]   You know, we've sold tens of millions of Quest devices.
[02:07:00.080 --> 02:07:05.040]   My understanding, just based on rumors,
[02:07:05.040 --> 02:07:06.320]   I don't have any special knowledge on this,
[02:07:06.320 --> 02:07:08.480]   is that Apple is building about one million
[02:07:08.480 --> 02:07:10.600]   of their device, right?
[02:07:10.600 --> 02:07:13.440]   So just in terms of like what you kind of expect
[02:07:13.440 --> 02:07:15.000]   in terms of sales numbers,
[02:07:15.000 --> 02:07:19.040]   I just think that this is, I mean,
[02:07:19.040 --> 02:07:23.000]   Quest is gonna be the primary thing that people
[02:07:23.000 --> 02:07:25.000]   in the market will continue using
[02:07:25.000 --> 02:07:25.960]   for the foreseeable future.
[02:07:25.960 --> 02:07:27.000]   And then obviously over the longterm,
[02:07:27.000 --> 02:07:29.840]   it's up to the companies to see how well we each executed
[02:07:29.840 --> 02:07:31.320]   the different things that we're doing.
[02:07:31.320 --> 02:07:33.000]   But we kind of come at it from different places.
[02:07:33.000 --> 02:07:37.320]   We're very focused on social interaction, communication,
[02:07:39.360 --> 02:07:40.800]   being more active, right?
[02:07:40.800 --> 02:07:43.800]   So there's fitness, there's gaming, there are those things.
[02:07:43.800 --> 02:07:46.600]   You know, whereas I think a lot of the use cases
[02:07:46.600 --> 02:07:50.680]   that you saw in Apple's launch material
[02:07:50.680 --> 02:07:53.640]   were more around people sitting,
[02:07:53.640 --> 02:07:56.800]   you know, people looking at screens, which are great.
[02:07:56.800 --> 02:07:59.280]   I think that you will replace your laptop over time
[02:07:59.280 --> 02:08:00.640]   with a headset.
[02:08:00.640 --> 02:08:03.440]   But I think in terms of kind of how
[02:08:03.440 --> 02:08:06.400]   the different use cases that the companies are going after,
[02:08:07.280 --> 02:08:10.360]   they're a bit different for where we are right now.
[02:08:10.360 --> 02:08:13.680]   - Yeah, so gaming wasn't a big part of the presentation,
[02:08:13.680 --> 02:08:15.320]   which is interesting.
[02:08:15.320 --> 02:08:19.720]   It feels like mixed reality gaming's
[02:08:19.720 --> 02:08:20.960]   such a big part of that.
[02:08:20.960 --> 02:08:24.400]   It was interesting to see it missing in the presentation.
[02:08:24.400 --> 02:08:27.080]   - Well, I mean, look, there are certain design trade-offs
[02:08:27.080 --> 02:08:30.800]   in this where, you know, they,
[02:08:30.800 --> 02:08:32.560]   I think they made this point about not wanting
[02:08:32.560 --> 02:08:35.360]   to have controllers, which on the one hand,
[02:08:35.360 --> 02:08:37.040]   there's a certain elegance about just being able
[02:08:37.040 --> 02:08:41.120]   to navigate the system with eye gaze and hand tracking.
[02:08:41.120 --> 02:08:43.920]   And by the way, you'll be able to just navigate Quest
[02:08:43.920 --> 02:08:46.640]   with your hands too, if that's what you want.
[02:08:46.640 --> 02:08:48.360]   - Yeah, one of the things I should mention
[02:08:48.360 --> 02:08:51.760]   is that the capability from the cameras
[02:08:51.760 --> 02:08:56.160]   with computer vision to detect certain aspects of the hand,
[02:08:56.160 --> 02:08:57.360]   allowing you to have a controller
[02:08:57.360 --> 02:08:59.000]   that doesn't have that ring thing.
[02:08:59.000 --> 02:09:01.120]   - Yeah, the hand tracking in Quest 3
[02:09:01.120 --> 02:09:03.640]   and the controller tracking is a big step up
[02:09:03.640 --> 02:09:05.600]   from the last generation.
[02:09:05.600 --> 02:09:09.720]   And one of the demos that we have is basically
[02:09:09.720 --> 02:09:12.120]   an MR experience teaching you how to play piano
[02:09:12.120 --> 02:09:13.560]   where it basically highlights the notes
[02:09:13.560 --> 02:09:14.760]   that you need to play and it's like,
[02:09:14.760 --> 02:09:16.880]   we're just all, it's hands, it's no controllers.
[02:09:16.880 --> 02:09:20.480]   But I think if you care about gaming,
[02:09:20.480 --> 02:09:25.480]   having a controller allows you to have a more tactile feel
[02:09:25.480 --> 02:09:30.280]   and allows you to capture fine motor movement
[02:09:30.280 --> 02:09:34.520]   much more precisely than what you can do with hands
[02:09:34.520 --> 02:09:36.000]   without something that you're touching.
[02:09:36.000 --> 02:09:38.520]   So again, I think there are certain questions
[02:09:38.520 --> 02:09:42.040]   which are just around what use cases are you optimizing for?
[02:09:42.040 --> 02:09:45.760]   I think if you wanna play games,
[02:09:45.760 --> 02:09:49.200]   then I think that you wanna design the system
[02:09:49.200 --> 02:09:52.160]   in a different way and we're more focused
[02:09:52.160 --> 02:09:55.340]   on kind of social experiences, entertainment experiences.
[02:09:56.960 --> 02:10:01.560]   Whereas if what you want is to make sure that the text
[02:10:01.560 --> 02:10:04.600]   that you read on a screen is as crisp as possible,
[02:10:04.600 --> 02:10:08.080]   then you need to make the design and cost trade-offs
[02:10:08.080 --> 02:10:12.320]   that they made that lead you to making a $3,500 device.
[02:10:12.320 --> 02:10:14.680]   So I think that there is a use case for that for sure,
[02:10:14.680 --> 02:10:17.360]   but I just think that the company
[02:10:17.360 --> 02:10:20.360]   is we've basically made different design trade-offs
[02:10:20.360 --> 02:10:24.560]   to get to the use cases that we're trying to serve.
[02:10:24.560 --> 02:10:27.520]   - There's a lot of other stuff I'd love to talk to you
[02:10:27.520 --> 02:10:31.880]   about the Metaverse, especially the Kodak Avatar,
[02:10:31.880 --> 02:10:33.520]   which I've gotten to experience a lot
[02:10:33.520 --> 02:10:35.160]   of different variations of recently
[02:10:35.160 --> 02:10:37.000]   that I'm really, really excited about.
[02:10:37.000 --> 02:10:39.200]   - Yeah, I'm excited to talk about that too.
[02:10:39.200 --> 02:10:41.420]   - I'll have to wait a little bit because,
[02:10:41.420 --> 02:10:47.920]   well, I think there's a lot more to show off in that regard.
[02:10:47.920 --> 02:10:50.240]   But let me step back to AI.
[02:10:50.240 --> 02:10:51.840]   I think we've mentioned it a little bit,
[02:10:51.840 --> 02:10:55.640]   but I'd like to linger on this question
[02:10:55.640 --> 02:11:00.640]   that folks like Eliezer Yudkowsky has to worry about
[02:11:00.640 --> 02:11:05.320]   and others of the existential, the serious threats of AI
[02:11:05.320 --> 02:11:07.080]   that have been reinvigorated now
[02:11:07.080 --> 02:11:09.880]   with the rapid developments of AI systems.
[02:11:09.880 --> 02:11:14.680]   Do you worry about the existential risks of AI
[02:11:14.680 --> 02:11:17.960]   as Eliezer does, about the alignment problem,
[02:11:17.960 --> 02:11:20.640]   about this getting out of hand?
[02:11:20.640 --> 02:11:23.120]   - Anytime where there's a number of serious people
[02:11:23.120 --> 02:11:27.480]   who are raising a concern that is that existential
[02:11:27.480 --> 02:11:28.960]   about something that you're involved with,
[02:11:28.960 --> 02:11:31.000]   I think you have to think about it, right?
[02:11:31.000 --> 02:11:33.960]   So I've spent quite a bit of time thinking about it
[02:11:33.960 --> 02:11:35.160]   from that perspective.
[02:11:35.160 --> 02:11:42.200]   The thing that I, where I basically have come out
[02:11:42.200 --> 02:11:44.640]   on this for now is I do think that there are,
[02:11:44.640 --> 02:11:47.520]   over time, I think that we need to think about this
[02:11:47.520 --> 02:11:50.120]   even more as we approach something
[02:11:50.120 --> 02:11:53.040]   that could be closer to super intelligence.
[02:11:53.040 --> 02:11:54.360]   I just think it's pretty clear
[02:11:54.360 --> 02:11:56.440]   to anyone working on these projects today
[02:11:56.440 --> 02:11:58.080]   that we're not there.
[02:11:58.080 --> 02:12:01.760]   And one of my concerns is that,
[02:12:01.760 --> 02:12:04.600]   we spent a fair amount of time on this before,
[02:12:04.600 --> 02:12:06.840]   but there are more,
[02:12:06.840 --> 02:12:11.720]   I don't know if mundane is the right word,
[02:12:11.720 --> 02:12:14.960]   but there's concerns that already exist, right?
[02:12:14.960 --> 02:12:19.720]   About people using AI tools to do harmful things
[02:12:19.720 --> 02:12:21.120]   of the type that we're already aware,
[02:12:21.120 --> 02:12:23.800]   whether we talked about fraud or scams
[02:12:23.800 --> 02:12:25.320]   or different things like that.
[02:12:25.320 --> 02:12:31.560]   And that's going to be a pretty big set of challenges
[02:12:31.560 --> 02:12:32.760]   that the companies working on this
[02:12:32.760 --> 02:12:34.320]   are gonna need to grapple with,
[02:12:34.320 --> 02:12:38.120]   regardless of whether there is an existential concern
[02:12:38.120 --> 02:12:40.040]   as well at some point down the road.
[02:12:40.040 --> 02:12:42.960]   So I do worry that to some degree,
[02:12:42.960 --> 02:12:48.440]   people can get a little too focused on
[02:12:49.440 --> 02:12:51.600]   some of the tail risk
[02:12:51.600 --> 02:12:54.480]   and then not do as good of a job as we need to
[02:12:54.480 --> 02:12:57.680]   on the things that you can be almost certain
[02:12:57.680 --> 02:12:59.440]   are going to come down the pipe
[02:12:59.440 --> 02:13:03.840]   as real risks that kind of manifest themselves
[02:13:03.840 --> 02:13:04.680]   in the near term.
[02:13:04.680 --> 02:13:08.160]   So for me, I've spent most of my time on that
[02:13:08.160 --> 02:13:12.080]   once I kind of made the realization
[02:13:12.080 --> 02:13:15.120]   that the size of models that we're talking about now
[02:13:15.120 --> 02:13:16.080]   in terms of what we're building
[02:13:16.400 --> 02:13:20.040]   are just quite far from the super intelligence type concerns
[02:13:20.040 --> 02:13:22.200]   that people raise.
[02:13:22.200 --> 02:13:25.800]   But I think once we get a couple of steps closer to that,
[02:13:25.800 --> 02:13:26.960]   I know as we do get closer,
[02:13:26.960 --> 02:13:27.920]   I think that those,
[02:13:27.920 --> 02:13:32.520]   there are going to be some novel risks and issues
[02:13:32.520 --> 02:13:36.720]   about how we make sure that the systems are safe for sure.
[02:13:36.720 --> 02:13:38.480]   I guess here just to take the conversation
[02:13:38.480 --> 02:13:40.160]   in a somewhat different direction,
[02:13:40.160 --> 02:13:44.800]   I think in some of these debates around safety,
[02:13:45.720 --> 02:13:50.040]   I think the concepts of intelligence and autonomy
[02:13:50.040 --> 02:13:55.120]   or like the being of the thing,
[02:13:55.120 --> 02:13:59.680]   as an analogy, they get kind of conflated together.
[02:13:59.680 --> 02:14:03.440]   And I think it very well could be the case
[02:14:03.440 --> 02:14:07.240]   that you can make something in scale intelligence quite far,
[02:14:07.240 --> 02:14:12.240]   but that may not manifest the safety concerns
[02:14:14.920 --> 02:14:16.440]   that people are saying in the sense that,
[02:14:16.440 --> 02:14:18.560]   I mean, just if you look at human biology,
[02:14:18.560 --> 02:14:20.200]   it's like, all right, we have our neocortex
[02:14:20.200 --> 02:14:23.000]   is where all the thinking happens, right?
[02:14:23.000 --> 02:14:25.800]   And it's, but it's not really calling the shots
[02:14:25.800 --> 02:14:26.640]   at the end of the day.
[02:14:26.640 --> 02:14:31.080]   We have a much more primitive old brain structure
[02:14:31.080 --> 02:14:34.160]   for which our neocortex, which is this powerful machinery
[02:14:34.160 --> 02:14:38.080]   is basically just a kind of prediction and reasoning engine
[02:14:38.080 --> 02:14:41.880]   to help it kind of like our very simple brain.
[02:14:44.640 --> 02:14:48.400]   Decide how to plan and do what it needs to do
[02:14:48.400 --> 02:14:52.440]   in order to achieve these like very kind of basic impulses.
[02:14:52.440 --> 02:14:57.400]   And I think that you can think about some of the development
[02:14:57.400 --> 02:15:00.200]   of intelligence along the same lines
[02:15:00.200 --> 02:15:03.240]   where just like our neocortex doesn't have free will
[02:15:03.240 --> 02:15:07.920]   or autonomy, we might develop these wildly intelligent
[02:15:07.920 --> 02:15:11.080]   systems that are much more intelligent
[02:15:11.080 --> 02:15:13.600]   than our neocortex have much more capacity,
[02:15:13.600 --> 02:15:15.880]   but are in the same way that our neocortex
[02:15:15.880 --> 02:15:18.760]   is sort of subservient and is used as a tool
[02:15:18.760 --> 02:15:22.160]   by our kind of simple impulse brain.
[02:15:22.160 --> 02:15:25.400]   It's, I think that it's not out of the question
[02:15:25.400 --> 02:15:28.160]   that very intelligent systems that have the capacity
[02:15:28.160 --> 02:15:31.560]   to think will kind of act as that is sort of an extension
[02:15:31.560 --> 02:15:33.640]   of the neocortex doing that.
[02:15:33.640 --> 02:15:37.520]   So I think my own view is that where we really need
[02:15:37.520 --> 02:15:42.520]   to be careful is on the development of autonomy
[02:15:42.680 --> 02:15:44.280]   and how we think about that,
[02:15:44.280 --> 02:15:49.280]   because it's actually the case that relatively simple
[02:15:49.280 --> 02:15:52.360]   and unintelligent things that have runaway autonomy
[02:15:52.360 --> 02:15:54.400]   and just spread themselves or,
[02:15:54.400 --> 02:15:55.800]   you know, it's like we have a word for that,
[02:15:55.800 --> 02:15:56.960]   it's a virus, right?
[02:15:56.960 --> 02:15:59.520]   It's, I mean, like it's can be simple computer code
[02:15:59.520 --> 02:16:01.000]   that is not particularly intelligent,
[02:16:01.000 --> 02:16:03.360]   but just spreads itself and does a lot of harm,
[02:16:03.360 --> 02:16:06.880]   biologically or computer.
[02:16:06.880 --> 02:16:11.880]   And I just think that these are somewhat separable things.
[02:16:12.880 --> 02:16:15.680]   And a lot of what I think we need to develop
[02:16:15.680 --> 02:16:18.160]   when people talk about safety and responsibility
[02:16:18.160 --> 02:16:21.280]   is really the governance on the autonomy
[02:16:21.280 --> 02:16:23.720]   that can be given to systems.
[02:16:23.720 --> 02:16:28.720]   And to me, if I were a policymaker or thinking about this,
[02:16:28.720 --> 02:16:31.440]   I would really wanna think about that distinction
[02:16:31.440 --> 02:16:34.120]   between these, where I think building intelligent systems
[02:16:34.120 --> 02:16:36.600]   will be, can create a huge advance
[02:16:36.600 --> 02:16:38.640]   in terms of people's quality of life
[02:16:38.760 --> 02:16:42.400]   and productivity growth in the economy.
[02:16:42.400 --> 02:16:45.120]   But it's the autonomy part of this
[02:16:45.120 --> 02:16:47.680]   that I think we really need to make progress
[02:16:47.680 --> 02:16:50.120]   on how to govern these things responsibly
[02:16:50.120 --> 02:16:54.280]   before we build the capacity for them
[02:16:54.280 --> 02:16:56.600]   to make a lot of decisions on their own
[02:16:56.600 --> 02:17:00.360]   or give them goals or things like that.
[02:17:00.360 --> 02:17:01.960]   And I know that's a research problem,
[02:17:01.960 --> 02:17:03.160]   but I do think that to some degree,
[02:17:03.160 --> 02:17:06.920]   these are somewhat separable things.
[02:17:06.920 --> 02:17:09.760]   I love the distinction between intelligence and autonomy
[02:17:09.760 --> 02:17:12.180]   and the metaphor within your cortex.
[02:17:12.180 --> 02:17:15.880]   Let me ask about power.
[02:17:15.880 --> 02:17:19.960]   So building superintelligent systems,
[02:17:19.960 --> 02:17:22.040]   even if it's not in the near term,
[02:17:22.040 --> 02:17:25.520]   I think Meta is one of the few companies,
[02:17:25.520 --> 02:17:28.220]   if not the main company,
[02:17:28.220 --> 02:17:31.360]   that will develop the superintelligent system.
[02:17:31.360 --> 02:17:34.560]   And you are a man who's at the head of this company.
[02:17:34.560 --> 02:17:36.440]   Building AGI might make you
[02:17:36.440 --> 02:17:37.960]   the most powerful man in the world.
[02:17:37.960 --> 02:17:40.260]   Do you worry that that power will corrupt you?
[02:17:40.260 --> 02:17:43.580]   - What a question.
[02:17:43.580 --> 02:17:48.560]   I mean, look, I think realistically,
[02:17:48.560 --> 02:17:50.300]   this gets back to the open source things
[02:17:50.300 --> 02:17:51.800]   that we talked about before,
[02:17:51.800 --> 02:17:56.800]   which is I don't think that the world will be best served
[02:17:56.800 --> 02:18:01.700]   by any small number of organizations
[02:18:03.520 --> 02:18:06.440]   having this without it being something
[02:18:06.440 --> 02:18:09.200]   that is more broadly available.
[02:18:09.200 --> 02:18:11.200]   And I think if you look through history,
[02:18:11.200 --> 02:18:16.480]   it's when there are these sort of like unipolar advances
[02:18:16.480 --> 02:18:19.240]   and things that, and like power imbalances
[02:18:19.240 --> 02:18:23.960]   that they're due into being kind of weird situations.
[02:18:23.960 --> 02:18:26.880]   So this is one of the reasons why I think open sources
[02:18:26.880 --> 02:18:31.520]   is generally the right approach.
[02:18:31.520 --> 02:18:34.720]   And I think it's a categorically different question today
[02:18:34.720 --> 02:18:36.840]   when we're not close to superintelligence.
[02:18:36.840 --> 02:18:37.760]   I think that there's a good chance
[02:18:37.760 --> 02:18:40.120]   that even once we get closer to superintelligence,
[02:18:40.120 --> 02:18:42.100]   open sourcing remains the right approach,
[02:18:42.100 --> 02:18:43.000]   even though I think at that point
[02:18:43.000 --> 02:18:44.640]   it's a somewhat different debate.
[02:18:44.640 --> 02:18:49.200]   But I think part of that is that that is,
[02:18:49.200 --> 02:18:51.680]   I think one of the best ways to ensure
[02:18:51.680 --> 02:18:54.200]   that the system is as secure and safe as possible,
[02:18:54.200 --> 02:18:55.340]   because it's not just about
[02:18:55.340 --> 02:18:57.160]   a lot of people having access to it.
[02:18:57.160 --> 02:18:59.360]   It's the scrutiny that kind of comes
[02:18:59.360 --> 02:19:02.360]   with building an open source system.
[02:19:02.360 --> 02:19:04.440]   But I think that this is a pretty widely accepted thing
[02:19:04.440 --> 02:19:08.760]   about open source is that you have the code out there,
[02:19:08.760 --> 02:19:11.240]   so anyone can see the vulnerabilities.
[02:19:11.240 --> 02:19:13.880]   Anyone can kind of mess with it in different ways.
[02:19:13.880 --> 02:19:15.520]   People can spin off their own projects
[02:19:15.520 --> 02:19:17.720]   and experiment in a ton of different ways.
[02:19:17.720 --> 02:19:20.000]   And the net result of all of that
[02:19:20.000 --> 02:19:22.040]   is that the systems just get hardened
[02:19:22.040 --> 02:19:24.660]   and get to be a lot safer and more secure.
[02:19:25.920 --> 02:19:29.520]   So I think that there's a chance
[02:19:29.520 --> 02:19:34.040]   that that ends up being the way that this goes to,
[02:19:34.040 --> 02:19:35.440]   a pretty good chance,
[02:19:35.440 --> 02:19:39.120]   and that having this be open
[02:19:39.120 --> 02:19:43.120]   both leads to a healthier development of the technology
[02:19:43.120 --> 02:19:47.440]   and also leads to a more balanced distribution
[02:19:47.440 --> 02:19:50.800]   of the technology in a way that strike me
[02:19:50.800 --> 02:19:53.120]   as good values to aspire to.
[02:19:53.120 --> 02:19:55.960]   - So to you, there's risks to open sourcing,
[02:19:55.960 --> 02:19:57.760]   but the benefits outweigh the risks.
[02:19:57.760 --> 02:20:00.080]   At the two, it's interesting,
[02:20:00.080 --> 02:20:01.920]   I think the way you put it,
[02:20:01.920 --> 02:20:04.280]   you put it well,
[02:20:04.280 --> 02:20:06.760]   that there's a different discussion now
[02:20:06.760 --> 02:20:08.240]   than when we get closer
[02:20:08.240 --> 02:20:11.440]   to development of superintelligence,
[02:20:11.440 --> 02:20:15.460]   of the benefits and risks of open sourcing.
[02:20:15.460 --> 02:20:16.360]   - Yeah, and to be clear,
[02:20:16.360 --> 02:20:18.680]   I feel quite confident in the assessment
[02:20:18.680 --> 02:20:23.360]   that open sourcing models now is net positive.
[02:20:23.360 --> 02:20:25.120]   I think there's a good argument
[02:20:25.120 --> 02:20:26.960]   that in the future it will be too,
[02:20:26.960 --> 02:20:28.720]   even as you get closer to superintelligence,
[02:20:28.720 --> 02:20:32.240]   but I've not, I've certainly have not decided on that yet.
[02:20:32.240 --> 02:20:33.220]   And I think that it becomes
[02:20:33.220 --> 02:20:35.680]   a somewhat more complex set of questions
[02:20:35.680 --> 02:20:37.300]   that I think people will have time to debate
[02:20:37.300 --> 02:20:39.520]   and will also be informed by what happens
[02:20:39.520 --> 02:20:41.600]   between now and then to make those decisions.
[02:20:41.600 --> 02:20:43.160]   We don't have to necessarily
[02:20:43.160 --> 02:20:45.120]   just debate that in theory right now.
[02:20:45.120 --> 02:20:47.960]   - What year do you think we'll have a superintelligence?
[02:20:48.960 --> 02:20:52.080]   - I don't know, I mean, that's pure speculation.
[02:20:52.080 --> 02:20:55.320]   I think it's very clear just taking a step back
[02:20:55.320 --> 02:20:57.480]   that we had a big breakthrough in the last year, right?
[02:20:57.480 --> 02:21:00.020]   Where the LLMs and diffusion models
[02:21:00.020 --> 02:21:02.600]   basically reached a scale where they're able
[02:21:02.600 --> 02:21:05.280]   to do some pretty interesting things.
[02:21:05.280 --> 02:21:07.280]   And then I think the question is what happens from here?
[02:21:07.280 --> 02:21:09.680]   And just to paint the two extremes,
[02:21:09.680 --> 02:21:15.600]   on one side, it's like, okay,
[02:21:15.600 --> 02:21:17.440]   well, we just had one breakthrough.
[02:21:17.440 --> 02:21:19.360]   If we just have like another breakthrough like that,
[02:21:19.360 --> 02:21:21.200]   or maybe two, then we can have something
[02:21:21.200 --> 02:21:23.240]   that's truly crazy, right?
[02:21:23.240 --> 02:21:28.240]   And is like, is just like so much more advanced.
[02:21:28.240 --> 02:21:31.880]   And on that side of the argument, it's like, okay,
[02:21:31.880 --> 02:21:32.820]   well, maybe we're,
[02:21:32.820 --> 02:21:38.120]   maybe we're only a couple of big steps away
[02:21:38.120 --> 02:21:41.600]   from reaching something that looks more
[02:21:41.600 --> 02:21:43.320]   like general intelligence.
[02:21:43.320 --> 02:21:45.800]   Okay, that's one side of the argument.
[02:21:45.800 --> 02:21:47.800]   And the other side, which is what we've historically seen
[02:21:47.800 --> 02:21:50.960]   a lot more, is that a breakthrough leads to,
[02:21:50.960 --> 02:21:58.320]   in that Gartner hype cycle, there's like the hype,
[02:21:58.320 --> 02:22:00.960]   and then there's the trough of disillusionment after,
[02:22:00.960 --> 02:22:03.600]   when like people think that there's a chance that, hey,
[02:22:03.600 --> 02:22:04.640]   okay, there's a big breakthrough.
[02:22:04.640 --> 02:22:06.040]   Maybe we're about to get another big breakthrough.
[02:22:06.040 --> 02:22:07.480]   And it's like, actually, you're not about
[02:22:07.480 --> 02:22:08.960]   to get another breakthrough.
[02:22:08.960 --> 02:22:10.760]   Maybe you're actually just gonna have to sit
[02:22:10.760 --> 02:22:12.320]   with this one for a while.
[02:22:12.320 --> 02:22:17.320]   And, you know, it could be five years,
[02:22:17.320 --> 02:22:19.800]   it could be 10 years, it could be 15 years
[02:22:19.800 --> 02:22:24.040]   until you figure out the, kind of the next big thing
[02:22:24.040 --> 02:22:25.600]   that needs to get figured out.
[02:22:25.600 --> 02:22:28.680]   And, but I think that the fact that we just had
[02:22:28.680 --> 02:22:32.720]   this breakthrough sort of makes it so that we're at a point
[02:22:32.720 --> 02:22:36.520]   of almost a very wide error bars on what happens next.
[02:22:36.520 --> 02:22:40.280]   I think the traditional technical view,
[02:22:40.280 --> 02:22:42.240]   or like looking at the industry,
[02:22:42.240 --> 02:22:46.160]   would suggest that we're not just going to stack
[02:22:46.160 --> 02:22:48.120]   in a breakthrough on top of breakthrough
[02:22:48.120 --> 02:22:51.840]   on top of breakthrough every six months or something.
[02:22:51.840 --> 02:22:54.600]   Right now, I think it will, I'm guessing,
[02:22:54.600 --> 02:22:56.520]   I would guess that it will take somewhat longer
[02:22:56.520 --> 02:23:01.400]   in between these, but I don't know.
[02:23:01.400 --> 02:23:03.400]   I tend to be pretty optimistic about breakthroughs too.
[02:23:03.400 --> 02:23:05.800]   So I mean, so I think if you're normalized
[02:23:05.800 --> 02:23:08.600]   for my normal optimism, then maybe it would be
[02:23:08.600 --> 02:23:10.320]   even slower than what I'm saying.
[02:23:10.320 --> 02:23:13.480]   But even within that, like I'm not even opining
[02:23:13.480 --> 02:23:15.400]   on the question of how many breakthroughs are required
[02:23:15.400 --> 02:23:18.040]   to get to general intelligence, because no one knows.
[02:23:18.040 --> 02:23:21.520]   - But this particular breakthrough was so,
[02:23:21.520 --> 02:23:25.920]   such a small step that resulted in such a big leap
[02:23:25.920 --> 02:23:30.080]   in performance as experienced by human beings
[02:23:30.080 --> 02:23:33.280]   that it makes you think, wow, are we,
[02:23:33.280 --> 02:23:37.760]   as we stumble across this very open world of research,
[02:23:37.760 --> 02:23:41.600]   will we stumble across another thing
[02:23:41.600 --> 02:23:43.960]   that will have a giant leap in performance?
[02:23:43.960 --> 02:23:49.800]   And also we don't know exactly at which stage
[02:23:49.800 --> 02:23:52.160]   is it really going to be impressive,
[02:23:52.160 --> 02:23:54.320]   'cause it feels like it's really encroaching
[02:23:54.320 --> 02:23:57.960]   on impressive levels of intelligence.
[02:23:57.960 --> 02:23:59.720]   You still didn't answer the question
[02:23:59.720 --> 02:24:02.040]   of what year we're going to have super intelligence.
[02:24:02.040 --> 02:24:03.680]   I'd like to hold you to that.
[02:24:03.680 --> 02:24:04.520]   No, I'm just kidding.
[02:24:04.520 --> 02:24:07.840]   But is there something you could say about the timeline
[02:24:07.840 --> 02:24:10.920]   as you think about the development
[02:24:10.920 --> 02:24:14.920]   of AGI super intelligence systems?
[02:24:14.920 --> 02:24:19.360]   - Sure, so I still don't think I have any particular insight
[02:24:19.360 --> 02:24:21.800]   on when like a singular AI system
[02:24:21.800 --> 02:24:24.120]   that is a general intelligence will get created.
[02:24:24.120 --> 02:24:26.240]   But I think the one thing that most people
[02:24:26.240 --> 02:24:28.840]   in the discourse that I've seen about this
[02:24:28.840 --> 02:24:32.360]   haven't really grappled with is that we do seem to have
[02:24:33.960 --> 02:24:37.560]   organizations and structures in the world
[02:24:37.560 --> 02:24:40.200]   that exhibit greater than human intelligence already.
[02:24:40.200 --> 02:24:44.440]   So one example is a company.
[02:24:44.440 --> 02:24:48.480]   It acts as an entity, it has a singular brand.
[02:24:48.480 --> 02:24:50.920]   Obviously it's a collection of people,
[02:24:50.920 --> 02:24:53.440]   but I certainly hope that Meta
[02:24:53.440 --> 02:24:55.480]   with tens of thousands of people
[02:24:55.480 --> 02:24:57.760]   make smarter decisions than one person.
[02:24:57.760 --> 02:25:00.040]   But I think that that would be pretty bad if it didn't.
[02:25:01.640 --> 02:25:05.080]   Another example that I think is even more removed
[02:25:05.080 --> 02:25:07.520]   from kind of the way we think about
[02:25:07.520 --> 02:25:11.000]   like the personification of intelligence,
[02:25:11.000 --> 02:25:13.480]   which is often implied in some of these questions,
[02:25:13.480 --> 02:25:15.680]   is think about something like the stock market.
[02:25:15.680 --> 02:25:18.440]   Where the stock market is, it takes inputs,
[02:25:18.440 --> 02:25:19.520]   it's a distributed system,
[02:25:19.520 --> 02:25:21.680]   it's like the cybernetic organism
[02:25:21.680 --> 02:25:26.400]   that probably millions of people around the world
[02:25:26.400 --> 02:25:28.480]   are basically voting every day
[02:25:28.480 --> 02:25:30.800]   by choosing what to invest in.
[02:25:30.800 --> 02:25:35.800]   But it's basically this organism or structure
[02:25:35.800 --> 02:25:39.280]   that is smarter than any individual
[02:25:39.280 --> 02:25:42.280]   that we use to allocate capital
[02:25:42.280 --> 02:25:44.480]   as efficiently as possible around the world.
[02:25:44.480 --> 02:25:49.480]   And I do think that this notion that there are already
[02:25:49.480 --> 02:25:56.400]   these cybernetic systems that are either melding
[02:25:56.400 --> 02:26:00.400]   the intelligence of multiple people together
[02:26:00.400 --> 02:26:02.880]   or melding the intelligence of multiple people
[02:26:02.880 --> 02:26:06.440]   and technology together to form something
[02:26:06.440 --> 02:26:09.400]   which is dramatically more intelligent
[02:26:09.400 --> 02:26:12.120]   than any individual in the world
[02:26:12.120 --> 02:26:17.360]   is something that seems to exist
[02:26:17.360 --> 02:26:20.160]   and that we seem to be able to harness
[02:26:20.160 --> 02:26:22.200]   in a productive way for our society
[02:26:22.200 --> 02:26:25.120]   as long as we basically build these structures
[02:26:25.120 --> 02:26:27.200]   and balance with each other.
[02:26:27.200 --> 02:26:31.640]   So I don't know, I mean, that at least gives me hope
[02:26:31.640 --> 02:26:33.360]   that as we advance the technology,
[02:26:33.360 --> 02:26:34.960]   and I don't know how long exactly it's gonna be,
[02:26:34.960 --> 02:26:36.960]   but you asked, when is this gonna exist?
[02:26:36.960 --> 02:26:39.480]   I think to some degree we already have
[02:26:39.480 --> 02:26:41.000]   many organizations in the world
[02:26:41.000 --> 02:26:42.840]   that are smarter than a single human.
[02:26:42.840 --> 02:26:44.400]   And that seems to be something
[02:26:44.400 --> 02:26:46.880]   that is generally productive in advancing humanity.
[02:26:46.880 --> 02:26:49.520]   - And somehow the individual AI systems
[02:26:49.520 --> 02:26:51.240]   empower the individual humans
[02:26:51.240 --> 02:26:53.240]   and the interaction between those humans
[02:26:53.240 --> 02:26:56.040]   to make that collective intelligence machinery
[02:26:56.040 --> 02:26:57.800]   that you're referring to smarter.
[02:26:57.800 --> 02:27:00.520]   So it's not like AI is becoming super intelligent,
[02:27:00.520 --> 02:27:03.120]   it's just becoming the engine
[02:27:03.120 --> 02:27:04.920]   that's making the collective intelligence
[02:27:04.920 --> 02:27:07.120]   is primarily human more intelligent.
[02:27:07.120 --> 02:27:09.000]   - Yeah.
[02:27:09.000 --> 02:27:10.680]   - It's educating the humans better,
[02:27:10.680 --> 02:27:12.820]   it's making them better informed,
[02:27:12.820 --> 02:27:15.480]   it's making it more efficient for them
[02:27:15.480 --> 02:27:19.000]   to communicate effectively and debate ideas,
[02:27:19.000 --> 02:27:20.440]   and through that process,
[02:27:20.440 --> 02:27:22.160]   just making the whole collective intelligence
[02:27:22.160 --> 02:27:24.340]   more and more and more intelligent.
[02:27:24.340 --> 02:27:26.940]   Maybe faster than the individual AI systems
[02:27:26.940 --> 02:27:30.500]   that are trained on human data anyway are becoming.
[02:27:30.500 --> 02:27:32.780]   Maybe the collective intelligence of the human species
[02:27:32.780 --> 02:27:34.740]   might outpace the development of AI.
[02:27:34.740 --> 02:27:36.860]   Just like--
[02:27:36.860 --> 02:27:37.820]   - I think there's a balance in here,
[02:27:37.820 --> 02:27:40.920]   because I mean, if a lot of the input
[02:27:40.920 --> 02:27:44.700]   that the systems are being trained on
[02:27:44.700 --> 02:27:47.660]   is basically coming from feedback from people,
[02:27:47.660 --> 02:27:49.380]   then a lot of the development
[02:27:49.380 --> 02:27:51.860]   does need to happen in human time, right?
[02:27:51.860 --> 02:27:55.860]   It's not like a machine will just be able to go learn
[02:27:55.860 --> 02:27:58.220]   all the stuff about how people think about stuff,
[02:27:58.220 --> 02:28:00.980]   there's a cycle to how this needs to work.
[02:28:00.980 --> 02:28:04.660]   - This is an exciting world we're living in,
[02:28:04.660 --> 02:28:07.180]   and that you're at the forefront of developing.
[02:28:07.180 --> 02:28:09.540]   One of the ways you keep yourself humble,
[02:28:09.540 --> 02:28:11.520]   like we mentioned with jiu-jitsu,
[02:28:11.520 --> 02:28:14.440]   is doing some really difficult challenges,
[02:28:14.440 --> 02:28:16.160]   mental and physical.
[02:28:16.160 --> 02:28:19.220]   One of those you've done very recently
[02:28:19.220 --> 02:28:21.540]   is the Murph Challenge,
[02:28:21.540 --> 02:28:22.940]   and you got a really good time.
[02:28:22.940 --> 02:28:25.660]   It's 100 pull-ups, 200 push-ups, 300 squats,
[02:28:25.660 --> 02:28:28.880]   and a mile before and a mile run after.
[02:28:28.880 --> 02:28:32.500]   You got under 40 minutes on that.
[02:28:32.500 --> 02:28:35.340]   What was the hardest part?
[02:28:35.340 --> 02:28:37.420]   I think a lot of people were very impressed.
[02:28:37.420 --> 02:28:38.820]   It's a very impressive time.
[02:28:38.820 --> 02:28:41.700]   - Yeah, I was pretty happy. - How crazy are you?
[02:28:41.700 --> 02:28:44.140]   I guess is the question I'm asking.
[02:28:44.140 --> 02:28:44.980]   - It wasn't my best time,
[02:28:44.980 --> 02:28:48.180]   but anything under 40 minutes I'm happy with.
[02:28:48.180 --> 02:28:49.580]   - It wasn't your best time?
[02:28:49.580 --> 02:28:52.420]   - No, I think I've done it a little faster before,
[02:28:52.420 --> 02:28:53.260]   but not much.
[02:28:53.260 --> 02:28:58.100]   Of my friends, I did not win on Memorial Day.
[02:28:58.100 --> 02:28:59.780]   One of my friends did it actually
[02:28:59.780 --> 02:29:01.940]   several minutes faster than me.
[02:29:01.940 --> 02:29:04.860]   But just to clear up one thing that I think was,
[02:29:04.860 --> 02:29:07.060]   I saw a bunch of questions about this on the internet.
[02:29:07.060 --> 02:29:09.500]   There are multiple ways to do the Murph Challenge.
[02:29:09.500 --> 02:29:12.060]   There's a kind of partitioned mode
[02:29:12.060 --> 02:29:17.060]   where you do sets of pull-ups, push-ups, and squats together,
[02:29:17.060 --> 02:29:18.660]   and then there's unpartitioned
[02:29:18.660 --> 02:29:20.260]   where you do the 100 pull-ups,
[02:29:20.260 --> 02:29:22.500]   and then the 200 push-ups,
[02:29:22.500 --> 02:29:25.740]   and then the 300 squats in serial.
[02:29:25.740 --> 02:29:29.700]   And obviously if you're doing them unpartitioned,
[02:29:29.700 --> 02:29:33.020]   then it takes longer to get through the 100 pull-ups
[02:29:33.020 --> 02:29:35.620]   'cause anytime you're resting in between the pull-ups,
[02:29:35.620 --> 02:29:38.020]   you're not also doing push-ups and squats.
[02:29:38.020 --> 02:29:40.540]   So yeah, I'm sure my unpartitioned time
[02:29:40.540 --> 02:29:42.460]   would be quite a bit slower.
[02:29:42.460 --> 02:29:45.420]   But no, I think at the end of this,
[02:29:48.220 --> 02:29:49.060]   I don't know, first of all,
[02:29:49.060 --> 02:29:51.260]   I think it's a good way to honor Memorial Day.
[02:29:51.260 --> 02:29:57.020]   This Lieutenant Murphy, basically,
[02:29:57.020 --> 02:30:00.900]   this was one of his favorite exercises,
[02:30:00.900 --> 02:30:03.820]   and I just try to do it on Memorial Day each year.
[02:30:03.820 --> 02:30:05.660]   And it's a good workout.
[02:30:05.660 --> 02:30:09.500]   I got my older daughters to do it with me this time.
[02:30:09.500 --> 02:30:12.940]   My oldest daughter wants a weight vest
[02:30:12.940 --> 02:30:15.100]   because she sees me doing it with a weight vest.
[02:30:15.100 --> 02:30:16.340]   I don't know if a seven-year-old
[02:30:16.340 --> 02:30:19.100]   should be using a weight vest to do pull-ups.
[02:30:19.100 --> 02:30:23.460]   - Difficult question a parent must ask themselves, yes.
[02:30:23.460 --> 02:30:25.620]   - I was like, maybe I can make you a very lightweight vest,
[02:30:25.620 --> 02:30:27.100]   but I didn't think it was good for this.
[02:30:27.100 --> 02:30:28.940]   So she basically did a quarter Murph.
[02:30:28.940 --> 02:30:33.380]   So she ran a quarter mile and then did 25 pull-ups,
[02:30:33.380 --> 02:30:37.100]   50 push-ups, and 75 air squats,
[02:30:37.100 --> 02:30:40.980]   then ran another quarter mile in 15 minutes,
[02:30:40.980 --> 02:30:43.300]   which I was pretty impressed by.
[02:30:43.300 --> 02:30:44.940]   And my five-year-old too.
[02:30:44.940 --> 02:30:48.340]   So I was excited about that.
[02:30:48.340 --> 02:30:50.740]   And I'm glad that I'm teaching them
[02:30:50.740 --> 02:30:54.700]   kind of the value of physicality.
[02:30:54.700 --> 02:30:57.620]   I think a good day for Max, my daughter,
[02:30:57.620 --> 02:30:59.380]   is when she gets to go to the gym with me
[02:30:59.380 --> 02:31:00.900]   and cranks out a bunch of pull-ups.
[02:31:00.900 --> 02:31:03.540]   And I love that about her.
[02:31:03.540 --> 02:31:04.780]   I mean, I think it's good.
[02:31:04.780 --> 02:31:07.460]   She's, you know, hopefully I'm teaching her
[02:31:07.460 --> 02:31:08.980]   some good lessons.
[02:31:08.980 --> 02:31:11.660]   - I mean, the broader question here is,
[02:31:11.660 --> 02:31:12.700]   given how busy you are,
[02:31:12.700 --> 02:31:14.860]   given how much stuff you have going on in your life,
[02:31:14.860 --> 02:31:19.860]   what's like the perfect exercise regimen for you
[02:31:19.860 --> 02:31:25.300]   to keep yourself happy,
[02:31:25.300 --> 02:31:28.640]   to keep yourself productive in your main line of work?
[02:31:28.640 --> 02:31:31.260]   - Yeah, so I mean, I've right now,
[02:31:31.260 --> 02:31:35.580]   I'm focused most of my workouts on fighting.
[02:31:35.580 --> 02:31:38.980]   So jujitsu and MMA.
[02:31:38.980 --> 02:31:42.220]   But I don't know.
[02:31:42.220 --> 02:31:43.700]   I mean, maybe if you're a professional,
[02:31:43.700 --> 02:31:44.580]   you can do that every day.
[02:31:44.580 --> 02:31:45.420]   I can't.
[02:31:45.420 --> 02:31:48.340]   I just get, you know, it's too many bruises
[02:31:48.340 --> 02:31:49.740]   and things that you need to recover from.
[02:31:49.740 --> 02:31:52.060]   So I do that, you know, three to four times a week.
[02:31:52.060 --> 02:31:55.660]   And then the other days,
[02:31:55.660 --> 02:31:57.940]   I just try to do a mix of things,
[02:31:57.940 --> 02:32:02.060]   like just cardio conditioning, strength building, mobility.
[02:32:02.060 --> 02:32:04.140]   - So you try to do something physical every day?
[02:32:04.140 --> 02:32:06.020]   - Yeah, I try to, unless I'm just so tired
[02:32:06.020 --> 02:32:08.460]   that I just need to relax.
[02:32:08.460 --> 02:32:10.500]   But then I'll still try to like go for a walk or something.
[02:32:10.500 --> 02:32:13.460]   I mean, even here, I don't know.
[02:32:13.460 --> 02:32:14.900]   Have you been on the roof here yet?
[02:32:14.900 --> 02:32:15.740]   - No.
[02:32:15.740 --> 02:32:16.580]   - We'll go on the roof after this.
[02:32:16.580 --> 02:32:17.420]   - I heard of things.
[02:32:17.420 --> 02:32:18.500]   - But it's like, we designed this building
[02:32:18.500 --> 02:32:20.420]   and I put a park on the roof.
[02:32:20.420 --> 02:32:22.260]   So that way, that's like my meetings
[02:32:22.260 --> 02:32:24.140]   when I'm just doing kind of a one-on-one
[02:32:24.140 --> 02:32:25.820]   or talking to a couple of people.
[02:32:25.820 --> 02:32:27.940]   I have a very hard time just sitting.
[02:32:27.940 --> 02:32:29.540]   I feel like it gets super stiff.
[02:32:29.540 --> 02:32:31.040]   It like feels really bad.
[02:32:31.040 --> 02:32:34.740]   But I don't know.
[02:32:34.740 --> 02:32:36.420]   Being physical is very important to me.
[02:32:36.420 --> 02:32:39.060]   I think it's, I do not believe,
[02:32:39.060 --> 02:32:41.460]   this gets to the question about AI.
[02:32:41.460 --> 02:32:44.300]   I don't think that a being is just a mind.
[02:32:44.300 --> 02:32:48.540]   I think we're kind of meant to do things
[02:32:48.540 --> 02:32:52.180]   and like physically and a lot of the sensations
[02:32:52.180 --> 02:32:55.620]   that we feel are connected to that.
[02:32:55.620 --> 02:32:58.140]   And I think that that's a lot of what makes you a human
[02:32:58.140 --> 02:33:01.740]   is basically having those,
[02:33:01.740 --> 02:33:06.740]   having that set of sensations and experiences around that
[02:33:06.740 --> 02:33:10.400]   coupled with a mind to reason about them.
[02:33:11.400 --> 02:33:13.160]   But I don't know.
[02:33:13.160 --> 02:33:18.160]   I think it's important for balance to kind of get out,
[02:33:18.160 --> 02:33:20.440]   challenge yourself in different ways,
[02:33:20.440 --> 02:33:23.440]   learn different skills, clear your mind.
[02:33:23.440 --> 02:33:27.520]   - Do you think AI, in order to become super intelligent,
[02:33:27.520 --> 02:33:28.920]   an AGI should have a body?
[02:33:28.920 --> 02:33:35.320]   - It depends on what the goal is.
[02:33:35.320 --> 02:33:39.080]   I think that there's this assumption in that question
[02:33:39.080 --> 02:33:44.080]   that intelligence should be kind of person-like.
[02:33:44.080 --> 02:33:46.980]   Whereas, as we were just talking about,
[02:33:46.980 --> 02:33:51.720]   you can have these greater than single human
[02:33:51.720 --> 02:33:54.760]   intelligent organisms like the stock market,
[02:33:54.760 --> 02:33:56.280]   which obviously do not have bodies
[02:33:56.280 --> 02:33:57.920]   and do not speak a language, right?
[02:33:57.920 --> 02:34:02.440]   And just kind of have their own system.
[02:34:02.440 --> 02:34:06.640]   But, so I don't know.
[02:34:06.640 --> 02:34:10.840]   My guess is there will be limits to what a system
[02:34:10.840 --> 02:34:13.120]   that is purely an intelligence can understand
[02:34:13.120 --> 02:34:17.040]   about the human condition without having the same,
[02:34:17.040 --> 02:34:22.040]   not just senses, but our bodies change as we get older.
[02:34:22.040 --> 02:34:24.080]   Right, and we kind of evolve.
[02:34:24.080 --> 02:34:29.080]   And I think that those very subtle physical changes
[02:34:29.080 --> 02:34:34.780]   just drive a lot of social patterns and behavior
[02:34:34.780 --> 02:34:37.280]   around when you choose to have kids, right?
[02:34:37.280 --> 02:34:39.160]   Like just like all these, that's not even subtle,
[02:34:39.160 --> 02:34:40.120]   that's a major one, right?
[02:34:40.120 --> 02:34:43.960]   But like how you design things around the house.
[02:34:43.960 --> 02:34:48.420]   So, yeah, I mean, I think if the goal is to understand
[02:34:48.420 --> 02:34:50.920]   people as much as possible, I think that that's,
[02:34:50.920 --> 02:34:54.320]   trying to model those sensations
[02:34:54.320 --> 02:34:55.540]   is probably somewhat important.
[02:34:55.540 --> 02:34:57.260]   But I think that there's a lot of value
[02:34:57.260 --> 02:34:58.960]   that can be created by having intelligence,
[02:34:58.960 --> 02:35:02.480]   even that is separate from that, it's a separate thing.
[02:35:02.480 --> 02:35:04.600]   - So one of the features of being human
[02:35:04.600 --> 02:35:08.280]   is that we're mortal, we die.
[02:35:08.280 --> 02:35:10.440]   We've talked about AI a lot,
[02:35:10.440 --> 02:35:12.960]   about potentially replicas of ourselves.
[02:35:12.960 --> 02:35:16.280]   Do you think there'll be AI replicas of you and me
[02:35:16.280 --> 02:35:18.840]   that persist long after we're gone,
[02:35:18.840 --> 02:35:21.720]   that family and loved ones can talk to?
[02:35:21.720 --> 02:35:26.060]   - I think we'll have the capacity
[02:35:26.060 --> 02:35:27.620]   to do something like that.
[02:35:27.620 --> 02:35:29.420]   And I think one of the big questions
[02:35:29.420 --> 02:35:33.400]   that we've had to struggle with in the context
[02:35:33.400 --> 02:35:36.920]   of social networks is who gets to make that?
[02:35:36.920 --> 02:35:40.840]   And my answer to that,
[02:35:40.840 --> 02:35:42.320]   in the context of the work that we're doing
[02:35:42.320 --> 02:35:43.920]   is that that should be your choice.
[02:35:43.920 --> 02:35:46.360]   Right, I don't think anyone should be able to choose
[02:35:46.360 --> 02:35:51.360]   to make a Lex bot that people can choose to talk to
[02:35:51.360 --> 02:35:53.640]   and get to train that.
[02:35:53.640 --> 02:35:56.600]   And we have this precedent of making some of these calls
[02:35:56.600 --> 02:36:00.020]   where, I mean, someone can create a page
[02:36:00.020 --> 02:36:04.880]   for a Lex fan club, but you can't create a page
[02:36:04.880 --> 02:36:07.080]   and say that you're Lex, right?
[02:36:07.080 --> 02:36:11.260]   So I think that this, similarly, I think,
[02:36:11.260 --> 02:36:14.420]   I mean, maybe, you know, someone maybe can make a,
[02:36:14.420 --> 02:36:17.500]   should be able to make an AI that's a Lex admirer
[02:36:17.500 --> 02:36:18.420]   that someone can talk to,
[02:36:18.420 --> 02:36:21.180]   but I think it should ultimately be your call
[02:36:21.180 --> 02:36:24.780]   whether there is a Lex AI.
[02:36:24.780 --> 02:36:26.780]   - Well, I'm open sourcing the Lex.
[02:36:29.140 --> 02:36:31.820]   So you're a man of faith.
[02:36:31.820 --> 02:36:34.260]   What role has faith played in your life
[02:36:34.260 --> 02:36:35.620]   and your understanding of the world
[02:36:35.620 --> 02:36:37.900]   and your understanding of your own life
[02:36:37.900 --> 02:36:41.140]   and your understanding of your work
[02:36:41.140 --> 02:36:44.500]   and how your work impacts the world?
[02:36:44.500 --> 02:36:48.500]   - Yeah, I think that there's a few different parts
[02:36:48.500 --> 02:36:49.800]   of this that are relevant.
[02:36:49.800 --> 02:36:53.540]   There's sort of a philosophical part
[02:36:53.540 --> 02:36:55.540]   and there's a cultural part.
[02:36:55.540 --> 02:36:58.660]   And one of the most basic lessons
[02:36:58.660 --> 02:37:01.060]   is right at the beginning of Genesis, right?
[02:37:01.060 --> 02:37:04.460]   It's like God creates the earth and creates people
[02:37:04.460 --> 02:37:06.920]   and creates people in God's image.
[02:37:06.920 --> 02:37:09.940]   And there's the question of, you know, what does that mean?
[02:37:09.940 --> 02:37:12.180]   And all, the only context that you have about God
[02:37:12.180 --> 02:37:13.300]   at that point in the Old Testament
[02:37:13.300 --> 02:37:15.620]   is that God has created things.
[02:37:15.620 --> 02:37:18.380]   So I always thought that like one of the interesting lessons
[02:37:18.380 --> 02:37:23.340]   from that is that there's a virtue in creating things
[02:37:24.340 --> 02:37:27.040]   that is like whether it's artistic
[02:37:27.040 --> 02:37:29.980]   or whether you're building things
[02:37:29.980 --> 02:37:32.280]   that are functionally useful for other people.
[02:37:32.280 --> 02:37:39.380]   I think that that by itself is a good.
[02:37:39.380 --> 02:37:44.500]   And that kind of drives a lot of how I think about morality
[02:37:44.500 --> 02:37:49.300]   and my personal philosophy around like,
[02:37:49.300 --> 02:37:51.620]   what is a good life, right?
[02:37:51.620 --> 02:37:56.620]   I think it's one where you're helping the people around you
[02:37:56.620 --> 02:38:01.620]   and you're being a kind of positive creative force
[02:38:01.620 --> 02:38:05.200]   in the world that is helping to bring new things
[02:38:05.200 --> 02:38:09.520]   into the world, whether they're amazing other people, kids,
[02:38:09.520 --> 02:38:14.520]   or just leading to the creation of different things
[02:38:14.520 --> 02:38:17.140]   that wouldn't have been possible otherwise.
[02:38:17.140 --> 02:38:20.560]   And so that's a value for me that matters deeply.
[02:38:20.560 --> 02:38:24.020]   And I just, I mean, I just love spending time with the kids
[02:38:24.020 --> 02:38:25.700]   and seeing that they sort of,
[02:38:25.700 --> 02:38:27.740]   trying to impart this value to them.
[02:38:27.740 --> 02:38:31.540]   And it's like, I mean, nothing makes me happier
[02:38:31.540 --> 02:38:33.980]   than like when I come home from work
[02:38:33.980 --> 02:38:38.060]   and I see like my daughter's like building Legos
[02:38:38.060 --> 02:38:38.980]   on the table or something.
[02:38:38.980 --> 02:38:41.520]   It's like, all right, I did that when I was a kid, right?
[02:38:41.520 --> 02:38:43.220]   So many other people are doing this.
[02:38:43.220 --> 02:38:45.300]   And like, I hope you don't lose that spirit
[02:38:45.300 --> 02:38:46.860]   where when you kind of grow up
[02:38:46.860 --> 02:38:49.780]   and you wanna just continue building different things
[02:38:49.780 --> 02:38:54.280]   no matter what it is, to me, that's a lot of what matters.
[02:38:54.280 --> 02:38:56.120]   That's the philosophical piece.
[02:38:56.120 --> 02:38:58.460]   I think the cultural piece is just about community
[02:38:58.460 --> 02:39:01.300]   and values and that part of things I think
[02:39:01.300 --> 02:39:02.740]   has just become a lot more important to me
[02:39:02.740 --> 02:39:04.000]   since I've had kids.
[02:39:04.000 --> 02:39:08.020]   You know, it's almost autopilot when you're a kid,
[02:39:08.020 --> 02:39:10.980]   you're in the kind of getting imparted to phase of your life.
[02:39:10.980 --> 02:39:14.220]   But, and I didn't really think about religion
[02:39:14.220 --> 02:39:16.380]   that much for a while.
[02:39:16.380 --> 02:39:20.980]   You know, I was in college, you know, before I had kids.
[02:39:20.980 --> 02:39:23.500]   And then I think having kids has this way
[02:39:23.500 --> 02:39:26.400]   of really making you think about what traditions
[02:39:26.400 --> 02:39:29.980]   you wanna impart and how you wanna celebrate
[02:39:29.980 --> 02:39:34.460]   and like what balance you want in your life.
[02:39:34.460 --> 02:39:37.300]   And I mean, a bunch of the questions that you've asked
[02:39:37.300 --> 02:39:40.540]   and a bunch of the things that we're talking about.
[02:39:40.540 --> 02:39:43.660]   - Just the irony of the curtains coming down
[02:39:44.700 --> 02:39:46.780]   as we're talking about mortality.
[02:39:46.780 --> 02:39:49.380]   Once again, same as last time.
[02:39:49.380 --> 02:39:52.540]   This is just, the universe works
[02:39:52.540 --> 02:39:55.060]   and we are definitely living in a simulation,
[02:39:55.060 --> 02:39:56.260]   but go ahead.
[02:39:56.260 --> 02:39:58.940]   Community, tradition, and the values,
[02:39:58.940 --> 02:40:00.580]   the faith and religion is still--
[02:40:00.580 --> 02:40:03.100]   - A lot of the topics that we've talked about today
[02:40:03.100 --> 02:40:08.100]   are around how do you balance,
[02:40:08.100 --> 02:40:11.060]   you know, whether it's running a company
[02:40:11.060 --> 02:40:14.060]   or different responsibilities with this,
[02:40:14.060 --> 02:40:18.900]   how do you kind of balance that?
[02:40:18.900 --> 02:40:21.860]   And I always also just think that it's very grounding
[02:40:21.860 --> 02:40:25.900]   to just believe that there is something
[02:40:25.900 --> 02:40:28.500]   that is much bigger than you that is guiding things.
[02:40:28.500 --> 02:40:34.400]   - That amongst other things gives you a bit of humility.
[02:40:34.400 --> 02:40:41.500]   As you pursue that spirit of creating
[02:40:41.500 --> 02:40:43.980]   that you spoke to, creating beauty in the world,
[02:40:43.980 --> 02:40:47.340]   and as Dostoevsky said, beauty will save the world.
[02:40:47.340 --> 02:40:50.500]   Mark, I'm a huge fan of yours.
[02:40:50.500 --> 02:40:53.220]   Honored to be able to call you a friend
[02:40:53.220 --> 02:40:57.980]   and I am looking forward to both kicking your ass
[02:40:57.980 --> 02:41:02.020]   and you kicking my ass on the mat tomorrow in jiu-jitsu,
[02:41:02.020 --> 02:41:07.020]   this incredible sport and art that we both participate in.
[02:41:07.020 --> 02:41:08.140]   Thank you so much for talking today.
[02:41:08.140 --> 02:41:09.460]   Thank you for everything you're doing
[02:41:09.460 --> 02:41:13.580]   in so many exciting realms of technology and human life.
[02:41:14.020 --> 02:41:16.060]   I can't wait to talk to you again in the metaverse.
[02:41:16.060 --> 02:41:16.900]   - Thank you.
[02:41:16.900 --> 02:41:19.620]   - Thanks for listening to this conversation
[02:41:19.620 --> 02:41:20.960]   with Mark Zuckerberg.
[02:41:20.960 --> 02:41:22.100]   To support this podcast,
[02:41:22.100 --> 02:41:25.100]   please check out our sponsors in the description.
[02:41:25.100 --> 02:41:29.100]   And now let me leave you with some words from Isaac Asimov.
[02:41:29.100 --> 02:41:33.620]   It is change, continuing change, inevitable change
[02:41:33.620 --> 02:41:36.120]   that is the dominant factor in society today.
[02:41:36.120 --> 02:41:40.040]   No sensible decision can be made any longer
[02:41:40.040 --> 02:41:43.700]   without taking into account not only the world as it is,
[02:41:43.700 --> 02:41:45.720]   but the world as it will be.
[02:41:45.720 --> 02:41:49.780]   Thank you for listening and hope to see you next time.
[02:41:49.780 --> 02:41:52.360]   (upbeat music)
[02:41:52.360 --> 02:41:54.940]   (upbeat music)
[02:41:54.940 --> 02:42:04.940]   [BLANK_AUDIO]

