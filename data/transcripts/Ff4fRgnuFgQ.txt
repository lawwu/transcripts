
[00:00:00.000 --> 00:00:04.760]   The following is a conversation with Mark Zuckerberg, his second time on this podcast.
[00:00:04.760 --> 00:00:12.640]   He's the CEO of Meta that owns Facebook, Instagram, and WhatsApp, all services used by billions of
[00:00:12.640 --> 00:00:18.320]   people to connect with each other. We talk about his vision for the future of Meta and the future
[00:00:18.320 --> 00:00:25.920]   of AI in our human world. This is the Lex Friedman Podcast. And now, dear friends, here's Mark
[00:00:26.400 --> 00:00:33.020]   Zuckerberg. So you competed in your first jiu-jitsu tournament, and me as a fellow jiu-jitsu
[00:00:33.020 --> 00:00:37.480]   practitioner and competitor, I think that's really inspiring given all the things you have going on.
[00:00:37.480 --> 00:00:43.120]   So I got to ask, what was that experience like? Oh, it was fun. I don't know. Yeah. I mean,
[00:00:43.120 --> 00:00:49.620]   well, look, I'm a pretty competitive person. Doing sports that basically require your full
[00:00:49.620 --> 00:00:55.420]   attention, I think is really important to my mental health and the way I just stay focused
[00:00:55.420 --> 00:00:56.380]   at doing everything I'm doing.
[00:00:56.380 --> 00:01:02.380]   So I decided to get into martial arts, and it's awesome. I got a ton of my friends into it. We all
[00:01:02.380 --> 00:01:10.460]   trained together. We have a mini academy in my garage. And I guess one of my friends was like,
[00:01:10.460 --> 00:01:14.380]   "Hey, we should go do a tournament." I was like, "Okay. Yeah, let's do it. I'm not going to
[00:01:14.380 --> 00:01:19.500]   shy away from a challenge like that." So yeah, but it was awesome. It was just a lot of fun.
[00:01:19.500 --> 00:01:21.100]   You weren't scared? There was no fear?
[00:01:21.100 --> 00:01:24.780]   I don't know. I was pretty sure that I'd do okay.
[00:01:24.780 --> 00:01:25.900]   I like the confidence.
[00:01:25.900 --> 00:01:26.140]   Yeah.
[00:01:26.140 --> 00:01:26.380]   Yeah.
[00:01:26.380 --> 00:01:31.740]   Well, so for people who don't know, jiu-jitsu is a martial art where you're trying to break your
[00:01:31.740 --> 00:01:41.180]   opponent's limbs or choke them to sleep and do so with grace and elegance and efficiency and all
[00:01:41.180 --> 00:01:46.220]   that kind of stuff. It's a kind of art form, I think, that you can do for your whole life. And
[00:01:46.220 --> 00:01:51.580]   it's basically a game, a sport of human chess you can think of. There's a lot of strategy. There's a
[00:01:51.580 --> 00:01:56.140]   lot of sort of interesting human dynamics of using leverage and all that kind of stuff. And
[00:01:56.140 --> 00:02:01.580]   it's kind of incredible what you could do. You could do things like a small opponent could defeat
[00:02:01.580 --> 00:02:06.060]   a much larger opponent. And you get to understand the way the mechanics of the human body works
[00:02:06.060 --> 00:02:09.660]   because of that. But you certainly can't be distracted.
[00:02:09.660 --> 00:02:12.220]   No. It's 100% focused.
[00:02:12.220 --> 00:02:12.620]   Yeah.
[00:02:12.620 --> 00:02:17.020]   But to compete, I needed to get around the fact that I didn't want it to be like this
[00:02:17.020 --> 00:02:24.300]   big thing. So I basically just rolled up with a hat and sunglasses, and I was wearing a COVID mask.
[00:02:24.300 --> 00:02:24.780]   Mm-hmm.
[00:02:24.780 --> 00:02:30.060]   And I registered under my first and middle name, so Mark Elliott. And it wasn't until I actually
[00:02:30.060 --> 00:02:34.300]   pulled all that stuff off right before I got on the mat that I think people knew it was me. So it was
[00:02:34.300 --> 00:02:35.100]   pretty low key.
[00:02:35.100 --> 00:02:37.500]   But you're still a public figure.
[00:02:37.500 --> 00:02:39.020]   Yeah. I mean, I didn't want to lose.
[00:02:39.020 --> 00:02:43.900]   Right. The thing you're partially afraid of is not just the losing, but being almost embarrassed.
[00:02:43.900 --> 00:02:49.420]   It's so raw, the sport, in that it's just you and another human being. There's a primal aspect there.
[00:02:49.420 --> 00:02:50.300]   Oh, yeah. It's great.
[00:02:50.300 --> 00:02:53.260]   For a lot of people, it can be terrifying, especially the first time you're doing the
[00:02:53.260 --> 00:02:54.540]   competing. And you want to be the first one to lose.
[00:02:54.540 --> 00:02:57.580]   And it wasn't for you. I see the look of excitement in your face.
[00:02:57.580 --> 00:02:58.220]   Yeah, I don't know.
[00:02:58.220 --> 00:02:59.020]   It wasn't. No fear.
[00:02:59.020 --> 00:03:01.260]   I just think part of learning is failing.
[00:03:01.260 --> 00:03:01.900]   Okay.
[00:03:01.900 --> 00:03:07.980]   Right? So, I mean, the main thing, like people who train jujitsu, it's like you need to not have
[00:03:07.980 --> 00:03:10.540]   pride because, I mean, all the stuff that you were talking about before about,
[00:03:10.540 --> 00:03:18.380]   you know, getting choked or getting, you know, a joint lock, you only get into a bad situation if
[00:03:18.380 --> 00:03:24.140]   you're not willing to tap once you've already lost. But obviously, when you're getting started with
[00:03:24.140 --> 00:03:24.300]   something, you're not going to be able to do it. You're going to be able to do it. You're going to
[00:03:24.300 --> 00:03:27.740]   be able to do something. You're not going to be an expert at it immediately. So, you just need to be
[00:03:27.740 --> 00:03:31.660]   willing to go with that. But I think this is like, I don't know. I mean, maybe I've just been
[00:03:31.660 --> 00:03:36.460]   embarrassed enough times in my life. I do think that there's a thing where like, you know, as
[00:03:36.460 --> 00:03:39.740]   people grow up, maybe they don't want to be embarrassed or anything. They've built their
[00:03:39.740 --> 00:03:46.620]   adult identity and they kind of have a sense of who they are and what they want to project.
[00:03:46.620 --> 00:03:53.580]   And I don't know, I think maybe to some degree, you know, your ability to keep doing interesting
[00:03:53.580 --> 00:03:54.060]   things, you know, is going to be a big part of your life. And I think that's a big part of your
[00:03:54.060 --> 00:03:54.460]   life. I think that's a big part of your life. I think that's a big part of your life. I think
[00:03:54.460 --> 00:04:01.900]   is your willingness to be embarrassed again and go back to step one and start as a beginner and
[00:04:01.900 --> 00:04:07.660]   get your ass kicked and, you know, look stupid doing things. And, you know, I think so many
[00:04:07.660 --> 00:04:12.540]   of the things that we're doing, whether it's this, I mean, this is just like a kind of a physical
[00:04:12.540 --> 00:04:17.820]   part of my life, but running the company, it's like we just take on new adventures and,
[00:04:17.820 --> 00:04:23.580]   you know, all the big things that we're doing, I think of as like 10 plus year
[00:04:23.580 --> 00:04:28.540]   missions that we're on where, you know, often early on, you know, people doubt that we're
[00:04:28.540 --> 00:04:33.020]   going to be able to do it. And the initial work seems kind of silly. And our whole ethos is we
[00:04:33.020 --> 00:04:36.620]   don't want to wait until something is perfect to put it out there. We want to get it out quickly
[00:04:36.620 --> 00:04:40.300]   and get feedback on it. And so I don't know. I mean, there's probably just something about
[00:04:40.300 --> 00:04:44.460]   how I approach things in there, but I just kind of think that the moment that you decide that
[00:04:44.460 --> 00:04:47.260]   you're going to be too embarrassed to try something new, then you're not going to learn
[00:04:47.260 --> 00:04:53.180]   anything anymore. But like I mentioned, that fear, that anxiety could be there, could creep up every
[00:04:53.180 --> 00:04:53.340]   once in a while. Yeah. I think that's a big part of your life. I think that's a big part of your life.
[00:04:53.340 --> 00:04:58.460]   Once in a while. Do you feel that in especially stressful moments sort of outside of the
[00:04:58.460 --> 00:05:06.780]   jiu-jitsu mat, just at work? Stressful moments, big decision days, big decision moments. How do
[00:05:06.780 --> 00:05:10.300]   you deal with that fear? How do you deal with that anxiety? The thing that stresses me out the
[00:05:10.300 --> 00:05:17.820]   most is always the people challenges. You know, I kind of think that, you know, strategy questions,
[00:05:17.820 --> 00:05:23.100]   you know, I tend to have enough conviction around the values of what we're
[00:05:23.100 --> 00:05:28.380]   trying to do and what I think matters and what I want our company to stand for that
[00:05:28.380 --> 00:05:34.300]   those don't really keep me up at night that much. I mean, I kind of, you know, it's not that I get
[00:05:34.300 --> 00:05:38.380]   everything right. Of course I don't, right? I mean, we make a lot of mistakes, but
[00:05:38.380 --> 00:05:47.580]   I at least have a pretty strong sense of where I want us to go on that. The thing in running
[00:05:47.580 --> 00:05:52.860]   a company for, you know, almost 20 years now, one of the things that's been pretty clear is when you have
[00:05:53.420 --> 00:06:01.420]   a team that's cohesive, you can get almost anything done. And, you know, you can run
[00:06:01.420 --> 00:06:08.140]   through super hard challenges. You can make hard decisions and push really hard to do the best work
[00:06:08.140 --> 00:06:13.740]   even, you know, and kind of optimize something super well. But when there's that tension,
[00:06:13.740 --> 00:06:17.900]   I mean, that's when things get really tough. And, you know, when I talk to other
[00:06:18.540 --> 00:06:22.300]   friends who run other companies and things like that, I think one of the things that I actually
[00:06:22.300 --> 00:06:28.140]   spend a disproportionate amount of time on in running this company is just fostering a pretty
[00:06:28.140 --> 00:06:36.300]   tight core group of people who are running the company with me. And that to me is kind of the
[00:06:36.300 --> 00:06:41.740]   thing that both makes it fun, right? Having, you know, friends and people you've worked with for a
[00:06:41.740 --> 00:06:45.980]   while and new people and new perspectives, but like a pretty tight group who you can go work on
[00:06:45.980 --> 00:06:47.200]   some of these crazy things with. But I think that's kind of the thing that I think is really important.
[00:06:47.200 --> 00:06:55.580]   But to me, that's also the most stressful thing is when there's tension, you know, that weighs on me.
[00:06:55.580 --> 00:07:00.340]   I think the, you know, just it's maybe not surprising. I mean, we're like a very people
[00:07:00.340 --> 00:07:06.660]   focused company and it's the people is the part of it that, you know, weighs on me the most to
[00:07:06.660 --> 00:07:10.860]   make sure that we get right. But yeah, that I'd say across everything that we do is probably
[00:07:10.860 --> 00:07:12.780]   the big thing.
[00:07:12.780 --> 00:07:15.860]   So when there's tension in that inner circle of
[00:07:16.900 --> 00:07:25.280]   close folks, so when you trust those folks to help you make difficult decisions about
[00:07:25.280 --> 00:07:32.800]   Facebook, WhatsApp, Instagram, the future of the company and the metaverse or the AI,
[00:07:32.800 --> 00:07:39.720]   how do you build that close-knit group of folks to make those difficult decisions? Is there people
[00:07:39.720 --> 00:07:46.200]   that you have to have critical voices, very different perspectives on focusing on the past
[00:07:46.200 --> 00:07:47.780]   versus the future, all that kind of stuff?
[00:07:47.780 --> 00:07:52.860]   Yeah, I mean, I think for one thing, it's just spending a lot of time with whatever
[00:07:52.860 --> 00:07:58.540]   the group is that you want to be that core group grappling with all of the biggest challenges.
[00:07:58.540 --> 00:08:04.340]   And that requires a fair amount of openness. And, you know, so I mean, a lot of how I run the
[00:08:04.340 --> 00:08:09.100]   company is, you know, it's like every Monday morning we get our, it's about the top 30 people
[00:08:09.100 --> 00:08:14.620]   together. And we, and this is a group that just worked together for a long period of time. And,
[00:08:14.620 --> 00:08:18.700]   and I mean, people, people rotate in, I mean, new people join, people leave the company,
[00:08:18.700 --> 00:08:22.780]   people go do other roles in the company. So it's, it's not the same group over time, but
[00:08:22.780 --> 00:08:27.920]   then we spend, you know, a lot of times a couple of hours, a lot of the time it's,
[00:08:27.920 --> 00:08:32.040]   you know, it can be somewhat unstructured. We like, I'll come with maybe a few topics that I,
[00:08:32.040 --> 00:08:37.040]   that are top of mind for me, but I'll ask other people to bring things and people,
[00:08:37.040 --> 00:08:41.540]   you know, raise questions, whether it's okay, there's an issue happening in some country
[00:08:41.540 --> 00:08:44.180]   with, with some policy issue.
[00:08:44.320 --> 00:08:47.420]   There's like a new technology that's developing here. We're having an issue with this
[00:08:47.420 --> 00:08:54.220]   partner. You know, there's a design trade-off and WhatsApp between two things that, that end up
[00:08:54.220 --> 00:08:59.180]   being values that we care about deeply. And we need to kind of decide where we want to be on
[00:08:59.180 --> 00:09:04.360]   that. And I just think over time when, you know, by working through a lot of issues with people
[00:09:04.360 --> 00:09:09.600]   and doing it openly, people develop an intuition for each other and a bond and camaraderie.
[00:09:09.600 --> 00:09:13.940]   And to me, developing that is, is,
[00:09:14.020 --> 00:09:17.440]   is like a lot of the fun part of running a company or doing anything, right?
[00:09:17.440 --> 00:09:21.400]   I think it's like having, having people who are kind of along on the journey that you're,
[00:09:21.400 --> 00:09:24.640]   that you feel like you're doing it with nothing is ever just one person doing it.
[00:09:24.640 --> 00:09:27.880]   Are there people that disagree often within that group?
[00:09:27.880 --> 00:09:29.620]   It's a fairly combative group.
[00:09:29.620 --> 00:09:35.320]   Okay. So combat is part of it. So this is making decisions on design, engineering,
[00:09:35.320 --> 00:09:40.540]   uh, policy, everything, everything, everything. Yeah.
[00:09:40.540 --> 00:09:43.660]   I have to ask just back to jujitsu for a little bit.
[00:09:43.720 --> 00:09:47.080]   What's your favorite submission now that you've been doing it? What's, uh,
[00:09:47.080 --> 00:09:51.100]   how do you like to submit your opponent? Mark Zuckerberg?
[00:09:51.100 --> 00:09:51.960]   I'm in.
[00:09:51.960 --> 00:10:01.000]   Well, first of all, I, um, I do prefer no gi or gi jujitsu. So gi is this outfit you wear that,
[00:10:01.000 --> 00:10:05.620]   uh, is maybe mimics clothing. So you can choke.
[00:10:05.620 --> 00:10:08.840]   Well, it's like a kimono. It's like the traditional martial arts or kimono.
[00:10:08.840 --> 00:10:09.320]   Pajamas.
[00:10:09.320 --> 00:10:10.760]   Um, pajamas.
[00:10:10.760 --> 00:10:13.660]   That you could choke people with. Yeah.
[00:10:13.660 --> 00:10:13.680]   Yeah.
[00:10:13.680 --> 00:10:13.700]   Yeah.
[00:10:13.700 --> 00:10:14.840]   Well, it's got the lapels.
[00:10:14.840 --> 00:10:15.140]   Yes.
[00:10:15.140 --> 00:10:24.300]   Yeah. Um, so I, I like jujitsu. I also really like MMA. And so I think no gi more closely
[00:10:24.300 --> 00:10:32.160]   approximates MMA. And I think my style is, um, is maybe a little closer to an MMA style. So like a
[00:10:32.160 --> 00:10:36.720]   lot of jujitsu players are fine being on their back. Right. And obviously having a good guard
[00:10:36.720 --> 00:10:41.360]   is, is, is a critical part of, of, of jujitsu, but, but in MMA, you don't want to be on your
[00:10:41.360 --> 00:10:43.400]   back. Right. Cause even if you have control, you're just, you're not going to be on your back.
[00:10:43.400 --> 00:10:47.460]   You're not going to be taking punches while you're on your back. So, um, so that's no good.
[00:10:47.460 --> 00:10:48.660]   Do you like being on top?
[00:10:48.660 --> 00:11:12.940]   My, my style is I'm, I'm probably more pressure and, um, and yeah. And, and I'd, I'd probably rather be the top player, but, um, but I'm also smaller. Right. I'm not, I'm not like a heavyweight guy. Right. So from that perspective, I think like, you know, it's especially because, you know, if I'm doing a competition, I'll compete with people who are my size, but a lot of my friends are bigger than me. So, um, so back to the point. Yeah.
[00:11:12.940 --> 00:11:13.240]   Yeah.
[00:11:13.240 --> 00:11:13.360]   Yeah.
[00:11:13.360 --> 00:11:30.540]   Back takes probably pretty important, right? Because that's where you have the most leverage advantage, right? Where, where, um, you know, people, you know, their arms, your arms are very weak behind you. Right. So, um, so being able to get to the back and, and, and take that pretty, pretty important, but I don't know. I feel like the right strategy is to not be too committed to any single submission.
[00:11:30.540 --> 00:11:40.360]   But that said, I don't like hurting people. So, um, so I always think that chokes are, are a somewhat more humane way to go than, than joint locks.
[00:11:40.360 --> 00:11:43.360]   Yeah. And it's more about control. It's less dynamic.
[00:11:43.360 --> 00:11:43.860]   Yeah.
[00:11:43.860 --> 00:11:47.440]   So you're basically like a Habib Nurmagomedov type of fighter.
[00:11:47.440 --> 00:11:52.240]   So, so let's go. Yeah. Back take to a rear naked choke. I think it's like the clean, the clean way to go.
[00:11:52.240 --> 00:12:09.880]   Straightforward answer right there. What advice would you give to, um, to people looking to start learning jujitsu? Given how busy you are, given where you are in life, that you're able to do this, you're able to train, you're able to compete and get, uh, uh, to learn something from this interesting art.
[00:12:09.880 --> 00:12:12.880]   I just think you have to be willing to, um,
[00:12:12.880 --> 00:12:16.500]   to just get beaten up a lot.
[00:12:16.500 --> 00:12:16.800]   Yeah.
[00:12:16.800 --> 00:12:42.740]   I mean, it's, but, but I mean, over time, I think that there's, there's a flow to all these things and there's, um, you know, one of the, one of, I don't know, my, my experiences that I think kind of transcends, you know, running a company and the different, different activities that I like doing are, I, I really believe that like, if you're going to accomplish whatever, anything, a lot of it is just being willing to push through.
[00:12:42.740 --> 00:12:43.220]   Right.
[00:12:43.220 --> 00:12:48.340]   And, and having the grit and determination to, to, to push through difficult situations.
[00:12:48.340 --> 00:13:03.940]   Um, and I think that for a lot of people that, um, that ends up being sort of a, a difference maker between the people who, who, who kind of get the most done and not, I mean, there's all these questions about like, um, you know, how, how many days people want to work and things like that.
[00:13:03.940 --> 00:13:08.780]   I think almost all the people who like start successful companies or things like that are just, are working extremely hard.
[00:13:08.780 --> 00:13:12.580]   But I think one of the things that you learn both by doing this over time.
[00:13:12.580 --> 00:13:19.860]   Or, you know, very acutely with things like jujitsu or surfing is, um, you can't push through everything.
[00:13:19.860 --> 00:13:32.860]   And I think that that's, you, you learn this stuff very acutely, you run, uh, doing sports compared to running a company because running a company, the cycle times are so long, right?
[00:13:32.860 --> 00:13:42.220]   It's like you start a project and then, you know, it's like months later, or, you know, if you're, you're building hardware, it could be years later before you're actually getting feedback.
[00:13:42.220 --> 00:13:42.400]   Right.
[00:13:42.400 --> 00:13:46.360]   And able to, you know, make the next set of decisions for the next version of the thing that you're doing.
[00:13:46.360 --> 00:13:57.020]   Whereas you, one of the things that I just think is mentally so nice about these very high turnaround conditioning sports, things like that is you, you get feedback very quickly, right?
[00:13:57.020 --> 00:13:59.480]   It's like, okay, like I, I don't counter something correctly.
[00:13:59.480 --> 00:14:00.720]   You get punched in the face, right?
[00:14:00.720 --> 00:14:12.120]   So not in jujitsu, you don't, you don't get punched in jujitsu, but in MMA, um, there are all these analogies between all these things that I think actually hold that are, that are like important life lessons, right?
[00:14:12.120 --> 00:14:14.100]   It's like, okay, you're surfing a wave.
[00:14:14.100 --> 00:14:20.500]   It's like, you know, sometimes you're like, you can't go in the other direction on it.
[00:14:20.500 --> 00:14:20.700]   Right.
[00:14:20.700 --> 00:14:24.420]   It's like, there are limits to kind of what, you know, it's like foil.
[00:14:24.420 --> 00:14:33.720]   You can, you can pump the foil and push pretty hard in a bunch of directions, but like, yeah, you, you know, it's, it's some level, like the momentum against you is strong enough.
[00:14:33.720 --> 00:14:35.220]   You're that's not going to work.
[00:14:35.220 --> 00:14:42.080]   And, and I do think that, um, that's sort of a, a humbling, but also an important.
[00:14:42.080 --> 00:15:01.940]   Lesson for, and I think people who are running things or building things, it's like, yeah, you, you, um, you know, a lot of the game is just being able to kind of push and, and, and, and work through complicated things, but you also need to kind of have enough of an understanding of like which things you're, you just can't push through and where, where, um, um, the finesse is more important.
[00:15:01.940 --> 00:15:02.720]   Yeah.
[00:15:02.720 --> 00:15:04.700]   What are your jujitsu life lessons?
[00:15:04.700 --> 00:15:07.800]   Well, I think you did it.
[00:15:07.800 --> 00:15:10.880]   You made it sound so simple.
[00:15:11.800 --> 00:15:26.500]   You were so eloquent that it's easy to miss, but basically being okay and accepting the wisdom and the joy in the, uh, getting your ass kicked in the full range of what that means.
[00:15:26.500 --> 00:15:41.160]   I think that's a big gift of the being humbled, somehow being humbled, especially physically opens your mind to the full process of learning what it means to learn, which is being willing to suck at something.
[00:15:41.520 --> 00:15:59.520]   And I think jujitsu is just very repetitively efficiently humbles you over and over and over and over to where you can carry that lessons to places where you, you don't get humbled as much, whether it's research or running a company or building stuff, the, the cycle is longer.
[00:15:59.520 --> 00:16:11.240]   And jujitsu, you can just get humbled in this period of an hour over and over and over and over, especially when you're a beginner, you'll have a little person, just, you know, somebody much smarter than you just kick your ass.
[00:16:11.240 --> 00:16:17.120]   Uh, repeatedly, uh, definitively where there's no argument.
[00:16:17.120 --> 00:16:17.660]   Oh yeah.
[00:16:17.660 --> 00:16:21.860]   And then you literally tap because if you don't tap, you're going to die.
[00:16:21.860 --> 00:16:23.420]   So this is an agreement.
[00:16:23.420 --> 00:16:26.780]   You could have killed me just now, but we're friends.
[00:16:26.780 --> 00:16:40.960]   So we're going to agree that you're not going to, and that kind of humbling process, it just does something to your psyche, to your ego that puts it in its proper context to realize that, you know, everything in this life is like a journey.
[00:16:40.960 --> 00:16:50.340]   me from sucking through a hard process of improving rigorously, day after day after
[00:16:50.340 --> 00:16:56.300]   day after day, any kind of success requires hard work. Yeah, just more than a lot of sports,
[00:16:56.300 --> 00:17:00.660]   I would say, because I've done a lot of them, really teaches you that. And you made it sound
[00:17:00.660 --> 00:17:05.100]   so simple, like I'm okay, you know, it's okay, it's part of the process, you just get humble,
[00:17:05.100 --> 00:17:06.100]   get your ass kicked-
[00:17:06.100 --> 00:17:10.940]   I've just failed and been embarrassed so many times in my life, that like, I'm, it's a
[00:17:10.940 --> 00:17:12.500]   core competence at this point.
[00:17:12.500 --> 00:17:17.300]   It's a core competence. Well, yes, and there's a deep truth to that, being able to, and you
[00:17:17.300 --> 00:17:22.160]   said it in the very beginning, which is, that's the thing that stops us, especially as you
[00:17:22.160 --> 00:17:28.180]   get older, especially as you develop expertise in certain areas, the not being willing to
[00:17:28.180 --> 00:17:34.740]   be a beginner in a new area. Because that's where the growth happens, is being willing
[00:17:34.740 --> 00:17:40.760]   to be a beginner, being willing to be embarrassed, saying something stupid, doing something stupid.
[00:17:40.760 --> 00:17:40.760]   A lot of people don't do that. And I think that's the thing that's really important,
[00:17:40.760 --> 00:17:40.760]   because that's where the growth happens, is being willing to be a beginner, being willing
[00:17:40.760 --> 00:17:40.760]   to be embarrassed, saying something stupid, doing something stupid. A lot of people don't
[00:17:40.760 --> 00:17:40.760]   do that. And I think that's the thing that's really important, because that's where the growth
[00:17:40.760 --> 00:17:41.180]   happens, is being willing to be embarrassed, saying something stupid, doing something stupid. A lot of
[00:17:41.180 --> 00:17:48.780]   us that get good at one thing, you want to show that off. And it sucks being a beginner, but it's
[00:17:48.780 --> 00:17:55.740]   where growth happens. Well, speaking of which, let me ask you about AI. It seems like this year,
[00:17:55.740 --> 00:18:00.600]   for the entirety of the human civilization, is an interesting year for the development
[00:18:00.600 --> 00:18:07.080]   of artificial intelligence. A lot of interesting stuff is happening. So meta is a big part of that.
[00:18:10.580 --> 00:18:28.920]   And the idea of the 65 billion parameter model, there's a lot of interesting questions I can ask here, one of which has to do with open source. But first, can you tell the story of developing of this model and making the complicated decision of how to release it?
[00:18:28.920 --> 00:18:40.400]   Yeah, sure. I think you're right, first of all, that in the last year, there have been a bunch of advances on scaling up these large transformer models. So there's the language
[00:18:40.400 --> 00:18:45.680]   equivalent of it with large language models. There's sort of the image generation equivalent
[00:18:45.680 --> 00:18:51.680]   with these large diffusion models. There's a lot of fundamental research that's gone into this. And
[00:18:51.680 --> 00:19:03.500]   meta has taken the approach of being quite open and academic in our development of AI.
[00:19:03.500 --> 00:19:10.220]   Part of this is we want to have the best people in the world researching this. And a lot of
[00:19:10.220 --> 00:19:14.220]   the best people want to know that they're going to be able to share their work. So that's part of the
[00:19:14.220 --> 00:19:20.460]   deal that we have is that we can get, if you're one of the top AI researchers in the world and
[00:19:20.460 --> 00:19:27.420]   come here, you can get access to kind of industry scale infrastructure. And part of our ethos is that
[00:19:27.420 --> 00:19:34.460]   we want to share what's invented broadly. We do that with a lot of the different AI tools that we
[00:19:34.460 --> 00:19:40.040]   create. And LLAMA is the language model that our research team made. And LLAMA is the language model that our research team made. And LLAMA is the language model that our research team made. And
[00:19:40.040 --> 00:19:49.320]   we did a limited open source release for it, which was intended for researchers to be able to use it.
[00:19:49.320 --> 00:19:58.200]   But responsibility and getting safety right on these is very important. So we didn't think that
[00:19:58.200 --> 00:20:03.080]   for the first one, there were a bunch of questions around whether we should be releasing this
[00:20:03.080 --> 00:20:09.860]   commercially. So we kind of punted on that for V1 of LLAMA and just released it from research. Now,
[00:20:09.860 --> 00:20:16.420]   obviously by releasing it for research, it's out there, but companies know that they're not supposed
[00:20:16.420 --> 00:20:22.980]   to kind of put it into commercial releases. And we're working on the follow-up models for this and
[00:20:22.980 --> 00:20:30.580]   thinking through how exactly this should work for follow-on now that we've had time to work on a lot
[00:20:30.580 --> 00:20:38.260]   more of the safety and the pieces around that. But overall, I mean, this is... I just kind of think that
[00:20:40.660 --> 00:20:47.060]   it would be good if there were a lot of different folks who had the ability
[00:20:47.060 --> 00:20:55.860]   to build state-of-the-art technology here. And not just a small number of big companies.
[00:20:55.860 --> 00:21:03.460]   But to train one of these AI models, the state-of-the-art models, just takes hundreds
[00:21:03.460 --> 00:21:09.540]   of millions of dollars of infrastructure, right? So there are not that many organizations in the world
[00:21:09.540 --> 00:21:18.820]   that can do that at the biggest scale today. And now it gets more efficient every day. So I do think
[00:21:18.820 --> 00:21:23.460]   that that will be available to more folks over time. But I just think there's all this innovation
[00:21:23.460 --> 00:21:31.060]   out there that people can create. And I just think that we'll also learn a lot by seeing what the
[00:21:31.060 --> 00:21:38.340]   whole community of students and hackers and startups and different folks build with this.
[00:21:38.340 --> 00:21:39.380]   And that's kind of been the goal. And I think that's kind of what we're trying to do. And that's
[00:21:39.380 --> 00:21:43.140]   been how we've approached this. And it's also how we've done a lot of our infrastructure. And we
[00:21:43.140 --> 00:21:47.860]   took our whole data center design and our server design, and we built this open compute project
[00:21:47.860 --> 00:21:52.260]   where we just made that public. And part of the theory was like, "All right, if we make it so that
[00:21:52.260 --> 00:21:58.500]   more people can use this server design, then that'll enable more innovation. It'll also make
[00:21:58.500 --> 00:22:02.660]   the server design more efficient. And that'll make our business more efficient too." So that's
[00:22:02.660 --> 00:22:05.620]   worked. And we've just done this with a lot of our infrastructure.
[00:22:05.620 --> 00:22:09.220]   So for people who don't know, you did the limited release, I think, in
[00:22:09.220 --> 00:22:23.300]   February of this year of LAMA. And it got "leaked," meaning it escaped the limited release
[00:22:23.300 --> 00:22:29.380]   aspect. But it was something you probably anticipated, given that it's just released
[00:22:29.380 --> 00:22:30.660]   through researchers. We shared it with researchers.
[00:22:30.660 --> 00:22:35.060]   You're right. So it's just trying to make sure that there's a slow release.
[00:22:35.060 --> 00:22:39.060]   Yeah. But from there, I just would love to get your comment on
[00:22:39.060 --> 00:22:42.900]   what happened next, which is like, there's a very vibrant open source community that just
[00:22:42.900 --> 00:22:49.300]   builds stuff on top of it. There's LAMA CPP, basically stuff that makes it more efficient
[00:22:49.300 --> 00:22:55.700]   to run on smaller computers. There's combining with reinforcement learning with human feedback,
[00:22:55.700 --> 00:23:01.860]   so some of the different interesting fine-tuning mechanisms. There's then also fine-tuning in GPT-3
[00:23:01.860 --> 00:23:08.900]   generations. There's a lot of GPT-4-all, Alpaca, Colossal AI, all these kinds of models just spring
[00:23:08.900 --> 00:23:12.820]   up, run on top of it. What do you think about that?
[00:23:12.820 --> 00:23:17.140]   No, I think it's been really neat to see. I mean, there's been folks who are getting it to
[00:23:17.140 --> 00:23:24.100]   run on local devices. So if you're an individual who just wants to experiment with this at home,
[00:23:24.100 --> 00:23:29.300]   you probably don't have a large budget to get access to a large amount of cloud compute,
[00:23:29.300 --> 00:23:35.780]   so getting it to run on your local laptop is pretty good and pretty relevant.
[00:23:35.780 --> 00:23:38.740]   And then there are things like LAMA,
[00:23:38.740 --> 00:23:45.060]   CPP, re-implemented it more efficiently. So now even when we run our own versions of it,
[00:23:45.060 --> 00:23:49.460]   we can do it on way less compute and it's just way more efficient, save a lot of money
[00:23:49.460 --> 00:23:58.260]   for everyone who uses this. So that is good. I do think it's worth calling out that
[00:23:58.260 --> 00:24:08.580]   because this was a relatively early release, LAMA isn't quite as on the frontier as, for example,
[00:24:08.580 --> 00:24:16.500]   the biggest OpenAI models or the biggest Google models. You mentioned that the largest LAMA model
[00:24:16.500 --> 00:24:23.700]   that we released had 65 billion parameters. And no one knows, I guess outside of OpenAI,
[00:24:23.700 --> 00:24:31.700]   exactly what the specs are for GPT-4, but I think my understanding is it's like 10 times bigger.
[00:24:31.700 --> 00:24:37.060]   And I think Google's Palm model is also, I think, has about 10 times as many parameters. Now,
[00:24:37.060 --> 00:24:38.420]   the LAMA models are very efficient. They're not as efficient as the Google Palm model, but they're
[00:24:38.420 --> 00:24:42.500]   very efficient. So they perform well for something that's around 65 billion parameters.
[00:24:42.500 --> 00:24:46.020]   So for me, that was also part of this because there was this whole debate around,
[00:24:46.020 --> 00:24:55.140]   is it good for everyone in the world to have access to the most frontier AI models? And
[00:24:55.140 --> 00:25:03.540]   I think as the AI models start approaching something that's like a super human intelligence,
[00:25:03.540 --> 00:25:08.260]   that's a bigger question that we'll have to grapple with. But right now, I mean, these are still
[00:25:08.260 --> 00:25:16.580]   very basic tools. They're powerful in the sense that a lot of open source software like databases
[00:25:16.580 --> 00:25:24.020]   or web servers can enable a lot of pretty important things. But I don't think anyone
[00:25:24.020 --> 00:25:30.100]   looks at the current generation of LAMA and thinks it's anywhere near a super intelligence.
[00:25:30.100 --> 00:25:36.180]   So I think that a bunch of those questions around like, is it good to kind of get out there? I think
[00:25:36.180 --> 00:25:38.100]   at this stage, surely. You want to be able to get out there and do a lot of things. But I think at
[00:25:38.100 --> 00:25:42.340]   this stage, surely. You want more researchers working on it for all the reasons that open
[00:25:42.340 --> 00:25:46.420]   source software has a lot of advantages. And we talked about efficiency before, but another one is
[00:25:46.420 --> 00:25:51.940]   just open source software tends to be more secure because you have more people looking at it openly
[00:25:51.940 --> 00:25:57.780]   and scrutinizing it and finding holes in it. And that makes it more safe. So I think at this point,
[00:25:57.780 --> 00:26:05.220]   it's more, I think it's generally agreed upon that open source software is generally more secure and
[00:26:05.220 --> 00:26:07.940]   safer than things that are kind of developed and developed by people who are more skilled at
[00:26:07.940 --> 00:26:12.100]   it. And I think that's something that's more likely to get developed in a silo where people
[00:26:12.100 --> 00:26:17.540]   try to get through security through obscurity. So I think that for the scale of what we're seeing
[00:26:17.540 --> 00:26:25.220]   now with AI, I think we're more likely to get to good alignment and good understanding of kind of
[00:26:25.220 --> 00:26:29.540]   what needs to do to make this work well by having it be open source. And that's something that I
[00:26:29.540 --> 00:26:32.740]   think is quite good to have out there and happening publicly at this point.
[00:26:32.740 --> 00:26:37.780]   Matt have released a lot of models as open source. So the
[00:26:37.780 --> 00:26:42.580]   image buying model. I mean, I'll ask you questions about those, but the point is
[00:26:42.580 --> 00:26:47.780]   you've open sourced quite a lot. You've been spearheading the open source movement. Where's
[00:26:47.780 --> 00:26:53.220]   that's really positive, inspiring to see from one angle, from the research angle, of course,
[00:26:53.220 --> 00:26:58.500]   there's folks who are really terrified about the existential threat of artificial intelligence. And
[00:26:58.500 --> 00:27:07.300]   those folks will say that you have to be careful about the open sourcing step. But where do you see
[00:27:07.300 --> 00:27:07.620]   the future? Matt:
[00:27:07.620 --> 00:27:14.660]   I think the future of open source here as part of meta, the tension here is, do you want to release
[00:27:14.660 --> 00:27:21.620]   the magic sauce? That's one tension. And the other one is, do you want to put a powerful tool in the
[00:27:21.620 --> 00:27:27.940]   hands of bad actors, even though it probably has a huge amount of positive impact also?
[00:27:27.940 --> 00:27:32.660]   Yeah. I mean, again, I think for the stage that we're at in the development of AI,
[00:27:32.660 --> 00:27:37.140]   I don't think anyone looks at the current state of things and thinks that this is super intelligence.
[00:27:37.460 --> 00:27:40.500]   And the models that we're talking about,
[00:27:40.500 --> 00:27:47.700]   the llama models here are generally an order of magnitude smaller than what open AI or Google are
[00:27:47.700 --> 00:27:54.740]   doing. So I think that at least for the stage that we're at now, the equity is balanced strongly,
[00:27:54.740 --> 00:28:00.500]   in my view, towards doing this more openly. I think if you got something that was closer to
[00:28:00.500 --> 00:28:07.300]   super intelligence, then I think you'd have to discuss that more and think through that a lot
[00:28:07.300 --> 00:28:11.460]   more. And we haven't made a decision yet as to what we would do if we were in that position,
[00:28:11.460 --> 00:28:14.900]   but I think there's a good chance that we're pretty far off from that position.
[00:28:14.900 --> 00:28:23.460]   I'm certainly not saying that the position that we're taking on this now
[00:28:23.460 --> 00:28:29.220]   applies to every single thing that we would ever do. And certainly inside the company,
[00:28:29.220 --> 00:28:34.100]   we probably do more open source work than most of the other big tech companies,
[00:28:34.100 --> 00:28:36.660]   but we also don't open source everything. I know a lot of our
[00:28:37.140 --> 00:28:40.660]   companies are open source, but we're not open sourcing the core app code for WhatsApp or
[00:28:40.660 --> 00:28:45.620]   Instagram or something. We're not open sourcing that. It's not a general enough piece of software
[00:28:45.620 --> 00:28:53.140]   that would be useful for a lot of people to do different things. Whereas the software that we do,
[00:28:53.140 --> 00:28:59.700]   whether it's an open source server design or basically things like Memcache,
[00:28:59.700 --> 00:29:06.660]   it was probably our earliest project that I worked on. It was probably one of the last things that I
[00:29:06.660 --> 00:29:06.980]   coded.
[00:29:06.980 --> 00:29:15.860]   And led directly for the company. But basically this caching tool for quick data retrieval,
[00:29:15.860 --> 00:29:22.740]   these are things that are just broadly useful across anything that you want to build. And I
[00:29:22.740 --> 00:29:28.020]   think that some of the language models now have that feel as well as some of the other things that
[00:29:28.020 --> 00:29:30.660]   we're building, like the translation tool that you just referenced.
[00:29:30.660 --> 00:29:36.820]   So text to speech and speech to text, you've expanded it from around 100 languages to more than
[00:29:36.820 --> 00:29:41.540]   1100 languages. And you can identify more than, the model can identify more than
[00:29:41.540 --> 00:29:46.740]   4000 spoken languages, which is 40 times more than any known previous technology.
[00:29:46.740 --> 00:29:50.980]   To me, that's really, really, really exciting in terms of connecting the world,
[00:29:50.980 --> 00:29:54.020]   breaking down barriers that language creates.
[00:29:54.020 --> 00:29:56.820]   Yeah, I think being able to translate between all of these different
[00:29:56.820 --> 00:30:06.020]   pieces in real time, this has been a kind of common sci-fi idea that we'd
[00:30:06.660 --> 00:30:12.420]   all have, whether it's an earbud or glasses or something that can help translate in real time
[00:30:12.420 --> 00:30:16.500]   between all these different languages. And that's one that I think technology is
[00:30:16.500 --> 00:30:21.780]   basically delivering now. So yeah, I think that's pretty exciting.
[00:30:21.780 --> 00:30:26.980]   You mentioned the next version of Llama. What can you say about the next version of Llama?
[00:30:26.980 --> 00:30:34.180]   What can you say about what you're working on in terms of release, in terms of the vision for that?
[00:30:34.180 --> 00:30:36.260]   Well, a lot of what we're doing,
[00:30:36.740 --> 00:30:41.540]   is taking the first version, which was primarily this research version,
[00:30:41.540 --> 00:30:50.660]   and trying to now build a version that has all of the latest state-of-the-art safety precautions
[00:30:50.660 --> 00:30:58.020]   built in. And we're using some more data to train it from across our services.
[00:30:58.020 --> 00:31:04.820]   But a lot of the work that we're doing internally is really just focused on making sure that this is
[00:31:06.340 --> 00:31:12.500]   as aligned and responsible as possible. And we're building a lot of our own,
[00:31:12.500 --> 00:31:18.740]   we're talking about kind of the open source infrastructure, but the main thing that we focus
[00:31:18.740 --> 00:31:22.740]   on building here, a lot of product experience is to help people connect and express themselves.
[00:31:22.740 --> 00:31:30.020]   So we're going to, I've talked about a bunch of this stuff, but then you'll have an assistant
[00:31:30.020 --> 00:31:36.180]   that you can talk to in WhatsApp. I think in the future, every creator will have
[00:31:36.180 --> 00:31:40.500]   kind of an AI agent that can kind of act on their behalf, that their fans can talk to.
[00:31:40.500 --> 00:31:44.900]   I want to get to the point where every small business basically has an AI agent
[00:31:44.900 --> 00:31:49.860]   that people can talk to for, you know, to do commerce and customer support and things like
[00:31:49.860 --> 00:31:56.820]   that. So there are going to be all these different things. And Lama, or the language model underlying
[00:31:56.820 --> 00:32:02.340]   this, is basically going to be the engine that powers that. The reason to open source it is that,
[00:32:02.340 --> 00:32:06.020]   as we did with the first version,
[00:32:06.020 --> 00:32:12.900]   is that it basically it unlocks a lot of innovation in the ecosystem,
[00:32:12.900 --> 00:32:18.100]   will make our products better as well, and also gives us a lot of valuable feedback on security
[00:32:18.100 --> 00:32:23.300]   and safety, which is important for making this good. But yeah, I mean, the work that we're doing
[00:32:23.300 --> 00:32:29.780]   to advance the infrastructure, it's basically at this point taking it beyond a research project
[00:32:29.780 --> 00:32:35.220]   into something which is ready to be kind of core infrastructure, not only for our own products, but
[00:32:35.860 --> 00:32:38.660]   you know, hopefully for a lot of other things out there too.
[00:32:38.660 --> 00:32:45.220]   Do you think the Lama or the language model underlying that version 2 will be open sourced?
[00:32:45.220 --> 00:32:50.980]   Do you have internal debate around that, the pros and cons and so on?
[00:32:50.980 --> 00:32:54.340]   This is, I mean, we were talking about the debates that we have internally, and I think
[00:32:54.340 --> 00:33:04.020]   the question is how to do it, right? I mean, I think we did the research license for V1, and I
[00:33:04.020 --> 00:33:05.700]   think the big thing that we're...
[00:33:05.700 --> 00:33:10.660]   That we're thinking about is basically like, what's the right way?
[00:33:10.660 --> 00:33:14.980]   So there was a leak that happened. I don't know if you can comment on it for V1.
[00:33:14.980 --> 00:33:17.700]   You know, we released it as a research project
[00:33:17.700 --> 00:33:25.540]   for researchers to be able to use, but in doing so, we put it out there. So, you know, we were
[00:33:25.540 --> 00:33:30.260]   very clear that anyone who uses the code and the weights doesn't have a commercial license to put
[00:33:30.260 --> 00:33:35.540]   into products. And we've generally seen people respect that, right? It's like you don't have any
[00:33:35.540 --> 00:33:41.300]   reputable companies that are basically trying to put this into their commercial products. But yeah,
[00:33:41.300 --> 00:33:46.820]   by sharing it with so many researchers, it did leave the building.
[00:33:46.820 --> 00:33:52.820]   But what have you learned from that process that you might be able to apply to V2 about how to
[00:33:52.820 --> 00:33:57.620]   release it safely, effectively, if you release it?
[00:33:57.620 --> 00:34:02.260]   Yeah, well, I mean, I think a lot of the feedback, like I said, is just around different things,
[00:34:02.260 --> 00:34:05.380]   around how do you fine tune models to make them...
[00:34:05.380 --> 00:34:10.100]   ...more aligned and safer. And you see all the different data recipes that...
[00:34:10.100 --> 00:34:14.900]   You know, you mentioned a lot of different projects that are based on this. I mean,
[00:34:14.900 --> 00:34:18.500]   there's one at Berkeley, there's... You know, there's just like all over. And
[00:34:18.500 --> 00:34:25.940]   people have tried a lot of different things, and we've tried a bunch of stuff internally. So
[00:34:25.940 --> 00:34:30.900]   kind of we're making progress here, but also we're able to learn from some of the
[00:34:30.900 --> 00:34:35.220]   best ideas in the community. And, you know, I think it... You know, we want to just continue
[00:34:35.220 --> 00:34:41.540]   pushing that forward. But I don't have any news to announce on this, if that's what you're asking.
[00:34:41.540 --> 00:34:51.220]   I mean, this is a thing that we're still kind of, you know, actively working through the right way
[00:34:51.220 --> 00:34:52.180]   to move forward here.
[00:34:52.180 --> 00:34:58.820]   The details of the secret sauce are still being developed. I see. Can you comment on what do you
[00:34:58.820 --> 00:35:04.260]   think of the thing that worked for GPT, which is the reinforcement learning with human feedback? So
[00:35:05.060 --> 00:35:10.740]   do you think that, you know, that's a really important part of doing this alignment process? Do
[00:35:10.740 --> 00:35:15.140]   you find it interesting? And as part of that, let me ask, because I talked to Jan LeCun before talking
[00:35:15.140 --> 00:35:22.420]   to you today, he asked me to ask or suggested that I ask, do you think LLM fine tuning will need to be
[00:35:22.420 --> 00:35:31.220]   crowdsourced Wikipedia style? So crowdsourcing. So this kind of idea of how to integrate the human
[00:35:31.220 --> 00:35:34.900]   in the fine tuning of these foundation models.
[00:35:34.900 --> 00:35:42.180]   Yeah, that's an interesting idea that I've talked to Jan about a bunch. And you were talking about,
[00:35:42.180 --> 00:35:50.420]   how do you basically train these models to be as safe and aligned and responsible as possible? And,
[00:35:50.420 --> 00:35:55.620]   you know, different groups out there who are doing development, test different data recipes
[00:35:55.620 --> 00:36:02.900]   in fine tuning. But this idea that you just mentioned is that at the end of the day,
[00:36:02.900 --> 00:36:04.740]   instead of having
[00:36:04.740 --> 00:36:10.100]   kind of one group fine tune some stuff and another group, you know, produce a different fine tuning
[00:36:10.100 --> 00:36:15.380]   recipe, and then us trying to figure out which one we think works best to produce the most aligned
[00:36:15.380 --> 00:36:24.740]   model. I do think that it would be nice if you could get to a point where you had a Wikipedia
[00:36:24.740 --> 00:36:34.340]   style collaborative way for a kind of a broader community to fine tune it as well. Now, there's a
[00:36:34.340 --> 00:36:41.060]   lot of challenges in that both from an infrastructure and like community management and
[00:36:41.060 --> 00:36:47.300]   product perspective about how you do that. So I haven't worked that out yet. But as an idea, I
[00:36:47.300 --> 00:36:52.340]   think it's quite compelling. And I think it goes well with the ethos of open sourcing the technology
[00:36:52.340 --> 00:36:54.580]   is also finding a way to have a, you know, a more open source approach to the technology. And I think
[00:36:54.580 --> 00:36:54.600]   that's a really good example that I think is really interesting. So I think that's a really good example
[00:36:54.600 --> 00:36:54.600]   of how you can do that. So I think it's quite compelling. And I think that's a really good example
[00:36:54.600 --> 00:37:02.200]   a kind of community driven, a community driven training of it. But I think that there are a lot
[00:37:02.200 --> 00:37:08.680]   of questions on this, in general, these questions around what's the best way to produce aligned AI
[00:37:08.680 --> 00:37:14.640]   models, it's very much a research area. And it's one that I think we will need to make as much
[00:37:14.640 --> 00:37:20.400]   progress on as the kind of core intelligence capability of the of the models themselves.
[00:37:20.400 --> 00:37:24.420]   Well, I just did a conversation with Jimmy Wales, the founder of Wikipedia,
[00:37:24.420 --> 00:37:30.420]   and to me, Wikipedia is one of the greatest websites ever created. And it's a kind of a
[00:37:30.420 --> 00:37:34.260]   miracle that it works. And I think it has to do with something that you mentioned, which is
[00:37:34.260 --> 00:37:40.740]   community, you have a small community of editors that somehow work together well, and they,
[00:37:40.740 --> 00:37:49.060]   they handle very controversial topics, and they handle it with balance and with grace, despite
[00:37:49.060 --> 00:37:54.260]   sort of the attacks that will often happen a lot of the time. I mean, it's not it's it has
[00:37:54.260 --> 00:37:59.140]   issues just like any other human system. But yes, I mean, the balance is, I mean, it's a,
[00:37:59.140 --> 00:38:03.700]   it's amazing what they've been able to achieve. But it's also not perfect. And I think that that's
[00:38:03.700 --> 00:38:06.500]   there's still a lot of challenges.
[00:38:06.500 --> 00:38:12.340]   Right. So the more controversial the topic, the more the more difficult the
[00:38:12.340 --> 00:38:19.140]   the journey towards, quote unquote, truth, or knowledge or wisdom that Wikipedia tries to
[00:38:19.140 --> 00:38:23.540]   capture. In the same way AI models, we need to be able to generate
[00:38:24.100 --> 00:38:28.740]   those same things, truth, knowledge and wisdom. And how do you align those models that they
[00:38:28.740 --> 00:38:38.020]   generate something that is closest to truth. There's these concerns about misinformation,
[00:38:38.020 --> 00:38:45.300]   all this kind of stuff that nobody can define. And this is something that we together as a human
[00:38:45.300 --> 00:38:50.420]   species have to define, like what is truth, and how to help AI systems generate that.
[00:38:50.420 --> 00:38:53.940]   And one of the things that language models do really well is generate,
[00:38:53.940 --> 00:38:57.940]   and I think that's the key to this, is that they're not just the
[00:38:57.940 --> 00:39:00.580]   human beings that are the ones that are the most important. And that's the key to this.
[00:39:00.580 --> 00:39:02.020]   And so I think that's the key to this. And I think that's the key to this.
[00:39:02.020 --> 00:39:03.060]   And so I think that's the key to this. And I think that's the key to this.
[00:39:03.060 --> 00:39:04.180]   And so I think that's the key to this. And I think that's the key to this.
[00:39:04.180 --> 00:39:05.380]   And I think that's the key to this. And I think that's the key to this.
[00:39:05.380 --> 00:39:08.500]   And part of that is the training and part of that is the alignment,
[00:39:08.500 --> 00:39:13.220]   and however you do the alignment stage. And just like you said, it's a very new
[00:39:13.220 --> 00:39:15.620]   and a very open research problem.
[00:39:15.620 --> 00:39:22.660]   Yeah. And I think that there's also a lot of questions about whether the current architecture
[00:39:22.660 --> 00:39:31.780]   for LLMs, as you continue scaling it, what happens. I mean, a lot of what's been exciting
[00:39:31.780 --> 00:39:37.300]   in the last year is that there's clearly a qualitative breakthrough where with some of
[00:39:37.300 --> 00:39:44.980]   the GPT models that OpenAI put out and that others have been able to do as well, I think it reached a
[00:39:44.980 --> 00:39:52.100]   level of quality where people are like, "Wow, this feels different and it's going to be able to be
[00:39:52.100 --> 00:39:52.500]   the foundation for the future." And I think that's the key to this.
[00:39:52.500 --> 00:39:57.480]   for building a lot of awesome products and experiences and value. But I think that the
[00:39:57.480 --> 00:40:04.340]   other realization that people have is, wow, we just made a breakthrough. If there are other
[00:40:04.340 --> 00:40:10.460]   breakthroughs quickly, then I think that there's the sense that maybe we're closer to general
[00:40:10.460 --> 00:40:15.240]   intelligence. But I think that that idea is predicated on the idea that I think people
[00:40:15.240 --> 00:40:19.040]   believe that there's still generally a bunch of additional breakthroughs to make and that
[00:40:19.040 --> 00:40:24.820]   we just don't know how long it's going to take to get there. And one view that some people have,
[00:40:24.820 --> 00:40:33.040]   this doesn't tend to be my view as much, is that simply scaling the current LLMs and getting to
[00:40:33.040 --> 00:40:40.040]   higher parameter count models by itself will get to something that is closer to general intelligence.
[00:40:40.040 --> 00:40:48.480]   But I don't know. I tend to think that there's probably more fundamental steps that need to
[00:40:48.480 --> 00:40:49.020]   be taken along.
[00:40:49.020 --> 00:40:56.640]   But still, the leaps taken with this extra alignment step is quite incredible, quite
[00:40:56.640 --> 00:41:03.240]   surprising to a lot of folks. And on top of that, when you start to have hundreds of millions of
[00:41:03.240 --> 00:41:09.780]   people potentially using a product that integrates that, you can start to see civilization transforming
[00:41:09.780 --> 00:41:17.220]   effects before you achieve super, quote unquote, super intelligence. It could be super transformative
[00:41:17.220 --> 00:41:19.000]   without being a super intelligence.
[00:41:19.000 --> 00:41:48.980]   Oh, yeah. I mean, I think that there are going to be a lot of amazing products and value that can be created with the current level of technology. To some degree, I'm excited to work on a lot of those products over the next few years. And I think it would just create a tremendous amount of whiplash if the number of breakthroughs keeps, if they keep on being stacked breakthroughs, because I think to some degree, industry in the world needs some time to kind of build these breakthroughs. And I think that's going to be a big part of the future.
[00:41:48.980 --> 00:41:50.980]   Yeah.
[00:41:50.980 --> 00:41:58.740]   that we all use that we can actually benefit from them. But I don't know, I think that there's just
[00:41:58.740 --> 00:42:05.500]   an awesome amount of stuff to do. I mean, I think about all of the, I don't know,
[00:42:05.500 --> 00:42:11.540]   small businesses or individual entrepreneurs out there who, you know, now we're going to be able to
[00:42:11.540 --> 00:42:16.420]   get help coding the things that they need to go build things or designing the things that they
[00:42:16.420 --> 00:42:21.740]   need, or we'll be able to, you know, use these models to be able to do customer support for the
[00:42:21.740 --> 00:42:27.140]   people that they're serving, you know, over WhatsApp without having to, you know, I think
[00:42:27.140 --> 00:42:32.240]   that's just going to be, I just think that this is all going to be super exciting. It's going to
[00:42:32.240 --> 00:42:37.060]   create better experiences for people and just unlock a ton of innovation and value.
[00:42:37.060 --> 00:42:42.720]   So I don't know if you know, but, you know, what is it over 3 billion people
[00:42:42.720 --> 00:42:45.760]   use WhatsApp, Facebook, and Instagram.
[00:42:45.760 --> 00:42:53.200]   Uh, so any kind of AI fueled products that go into that, like we're talking about anything
[00:42:53.200 --> 00:42:58.780]   with LLMs will have a tremendous amount of impact. Do you have ideas and thoughts about
[00:42:58.780 --> 00:43:07.540]   possible products that might start being integrated into, uh, into these platforms
[00:43:07.540 --> 00:43:13.100]   used by so many people? Yeah, I think there's three main categories of things that we're working on.
[00:43:13.100 --> 00:43:14.180]   Um,
[00:43:14.180 --> 00:43:26.260]   the first that I think is probably the most interesting is, um, you know, there's this
[00:43:26.260 --> 00:43:31.660]   notion of like, you're going to have an assistant or an agent who you can talk to. And I think
[00:43:31.660 --> 00:43:37.060]   probably the biggest thing that's different about my view of how this plays out from what I see
[00:43:37.060 --> 00:43:42.300]   with, um, with open AI and Google and others is, you know, everyone else is building like
[00:43:42.300 --> 00:43:44.160]   the one single platform that's going to be able to do that. And I think that's going to be the
[00:43:44.160 --> 00:43:44.660]   biggest thing that's going to happen. And then the second thing that I think is going to be the most, um,
[00:43:44.660 --> 00:43:45.160]   the biggest thing that's going to happen is that there's going to be a lot of different AI. Right?
[00:43:45.160 --> 00:43:52.540]   It's like, okay, you talk to chat GPT, or you talk to Bard or you talk to Bing. And my view is that,
[00:43:52.540 --> 00:43:59.620]   that they're going to be a lot of different AIs that people are going to want to engage with,
[00:43:59.620 --> 00:44:04.660]   just like you want to use, um, you know, a number of different apps for different things. And you
[00:44:04.660 --> 00:44:09.660]   have relationships with different people in your life who fill different emotional roles for you.
[00:44:09.660 --> 00:44:13.200]   Um, and I, um,
[00:44:13.200 --> 00:44:14.140]   I think that's, um, I think that's a really good question. And I think that's a really good question.
[00:44:14.140 --> 00:44:18.320]   Um, so I think that they're going to be, people have a reason that they, that I think you don't
[00:44:18.320 --> 00:44:23.680]   just want like a singular AI and that, that I think is probably the biggest distinction in, in,
[00:44:23.680 --> 00:44:27.840]   in terms of how I think about this and a bunch of these things, I think you'll, you'll want an
[00:44:27.840 --> 00:44:32.120]   assistant. Um, I, I mean, I mentioned a couple of these before. I think like every creator who
[00:44:32.120 --> 00:44:38.720]   you interact with will ultimately want some kind of AI that can proxy them and be something that
[00:44:38.720 --> 00:44:44.120]   their fans can interact with, or that allows them to interact with their fans. Um, and, and, and, and
[00:44:44.120 --> 00:44:49.000]   this is like the common creator promise. Everyone's trying to build a community and engage
[00:44:49.000 --> 00:44:52.520]   with people and they want tools to be able to amplify themselves more and be able to do that.
[00:44:52.520 --> 00:45:01.880]   Um, but, but you only have 24 hours in a day. So, um, so I think having the ability to basically
[00:45:01.880 --> 00:45:08.520]   like bottle up your personality and, um, or, you know, like give your fans information about when
[00:45:08.520 --> 00:45:12.200]   you're performing a concert or, or something like that. I mean, that's, that I think is gonna be
[00:45:12.200 --> 00:45:14.100]   something that's super valuable, but it's not just like, you know, like, you're going to be able to
[00:45:14.100 --> 00:45:14.260]   do this, you're going to be able to do this, you're going to be able to do this. I mean, I think that's,
[00:45:14.260 --> 00:45:18.140]   that, you know, again, it's not this idea that I think people are going to want just one singular
[00:45:18.140 --> 00:45:21.500]   AI. I think you're going to, you know, you're going to want to interact with a lot of different
[00:45:21.500 --> 00:45:25.040]   entities. And then I think there's the business version of this too, which we've touched on a
[00:45:25.040 --> 00:45:32.020]   couple of times, which is, um, I think every business in the world is going to want basically
[00:45:32.020 --> 00:45:38.440]   an AI that, um, that, you know, it's like you have your page on Instagram or Facebook or WhatsApp or
[00:45:38.440 --> 00:45:44.080]   whatever, and you want to, you want to point people to an AI that people can interact with, but you want
[00:45:44.080 --> 00:45:47.540]   to know that that AI is only going to sell your products. You don't want it, you know, recommending
[00:45:47.540 --> 00:45:52.700]   your competitors stuff, right? So, so it's not like there can be like just a, you know, one singular
[00:45:52.700 --> 00:45:58.280]   AI that, that can answer all the questions for a person because, you know, that quite like that AI
[00:45:58.280 --> 00:46:02.920]   might not actually be aligned with you as a business to, um, to, to really just do the best
[00:46:02.920 --> 00:46:08.740]   job providing support for, for your product. So I think that there's going to be a clear need, um,
[00:46:08.740 --> 00:46:13.660]   in the market and in people's lives for there to be a bunch of these.
[00:46:14.060 --> 00:46:20.060]   Part of that is figuring out the research, the technology that enables the personalization
[00:46:20.060 --> 00:46:27.500]   that you're talking about. So not one centralized God-like LLM, but one, just a huge diversity of
[00:46:27.500 --> 00:46:33.340]   them that's fine-tuned to particular needs, particular styles, particular businesses,
[00:46:33.340 --> 00:46:37.820]   particular brands, all that kind of stuff. And also enabling, just enabling people to create
[00:46:37.820 --> 00:46:43.020]   them really easily for the, you know, for, to, for your own business, or if you're a creator to,
[00:46:43.020 --> 00:46:44.040]   to be able to help people. Yeah.
[00:46:44.040 --> 00:46:49.540]   You engage with your fans and I, I've met that's, um, so yeah, I, I think that there,
[00:46:49.540 --> 00:46:54.980]   there's a clear kind of interesting product direction here that I think is fairly unique
[00:46:54.980 --> 00:47:00.500]   from, from what, you know, any of the other big companies are, are taking. Um, it also aligns well
[00:47:00.500 --> 00:47:04.740]   with this sort of open source approach, because again, we, we sort of believe in this more
[00:47:04.740 --> 00:47:10.880]   community oriented, uh, more democratic approach to building out the products and technology around
[00:47:10.880 --> 00:47:14.020]   this. We don't think that there's going to be the one true thing. We think that there, there should be
[00:47:14.020 --> 00:47:19.120]   kind of a lot of development. So that part of things I think is going to be really interesting.
[00:47:19.120 --> 00:47:22.720]   And we could, we could go probably spend a lot of time talking about that and the,
[00:47:22.720 --> 00:47:27.940]   the kind of implications of, um, of that approach being different from what others are taking.
[00:47:27.940 --> 00:47:32.060]   Um, but then there's a bunch of other simpler things that I think we're also going to do,
[00:47:32.060 --> 00:47:35.940]   just going back to your, your question around how this finds its way into like,
[00:47:35.940 --> 00:47:43.480]   what do we build? Um, there are going to be a lot of simpler things around, um, okay. You,
[00:47:44.000 --> 00:47:49.920]   you post photos on Instagram and Facebook and, you know, and WhatsApp and messenger. And like,
[00:47:49.920 --> 00:47:53.980]   you want the photos to look as good as possible. So like having an AI that you can just like take
[00:47:53.980 --> 00:47:58.440]   a photo and then just tell it like, okay, I want to edit this thing or describe this. It's like,
[00:47:58.440 --> 00:48:02.680]   I think we're, we're going to have tools that are just way better than, than what we've historically
[00:48:02.680 --> 00:48:07.900]   had on this. Um, and that's more in the image and media generation side than the large language
[00:48:07.900 --> 00:48:13.240]   model side, but, but it's, it all kind of, you know, plays off of advances in the same space.
[00:48:13.980 --> 00:48:17.160]   Um, so there are a lot of tools that I think are just going to get built into every one of our
[00:48:17.160 --> 00:48:22.280]   products. I think every single thing that we do is going to basically get evolved in this direction,
[00:48:22.280 --> 00:48:26.220]   right? It's like in the future, if you're advertising on our services, like, do you
[00:48:26.220 --> 00:48:32.920]   need to make your own kind of ad creative? It's no, you'll just, you know, you just tell us,
[00:48:32.920 --> 00:48:39.320]   okay, I'm, I'm a dog walker and I, you know, willing to walk people's dogs and
[00:48:39.320 --> 00:48:43.960]   help me find the right people and like create the
[00:48:43.960 --> 00:48:49.640]   ad unit that will perform the best and like give an objective to, to the system. And it just kind of
[00:48:49.640 --> 00:48:52.240]   like connects you with the right people.
[00:48:52.240 --> 00:49:01.140]   Well, that's a super powerful idea of generating the language almost like, uh, rigorous AB testing
[00:49:01.140 --> 00:49:02.320]   for you.
[00:49:02.320 --> 00:49:13.940]   Yeah, it works to find the best customer for your thing. I mean, to me, advertisement when done well, just finds a good match between a human
[00:49:13.940 --> 00:49:16.940]   being and a thing that will make that human being happy.
[00:49:16.940 --> 00:49:18.940]   Yeah, totally.
[00:49:18.940 --> 00:49:21.060]   And do that as efficiently as possible.
[00:49:21.060 --> 00:49:43.920]   When it's done well, people actually like it. You know, it's, um, yeah, I think that there's a lot of examples where it's not done well and it's annoying and I think that that's what kind of gives it a bad rap. But, um, but yeah, and a lot of the stuff is possible today. I mean, obviously AB testing stuff is built into a lot of these frameworks. The thing that's new is having technology that can generate the ideas for you about what to AB test. So I think that that's exciting. So this will just be a little bit of a
[00:49:43.920 --> 00:49:53.920]   just be across like everything that we're doing, right? All the metaverse stuff that we're doing, right? It's like you want to create worlds in the future. You'll just describe them and then it'll create the code for you. So
[00:49:53.920 --> 00:50:03.820]   so natural language becomes the interface we use for all the ways we interact with the computer with with the digital
[00:50:03.820 --> 00:50:05.460]   more of them. Yeah, yeah, totally.
[00:50:05.460 --> 00:50:13.900]   Yeah, which is what everyone can do using natural language and with translation, you can do it in any kind of language. Um, I mean,
[00:50:13.900 --> 00:50:23.600]   for the personalization is really, really, really interesting. Yeah, it unlocks so many possible things. I mean, I, for one, look forward to creating a copy of myself.
[00:50:23.600 --> 00:50:43.880]   I know we talked about this last time, but this has since the last time this becomes our closer, much closer. Like I could literally just having interacted with some of these language models. I can see the absurd situation where I'll have a large or a lex language
[00:50:43.880 --> 00:51:13.860]   model, and I'll have to have a conversation with him about like, Hey, listen, like you're just getting out of line and having a conversation where you fine tune that thing to be a little bit more respectful or something like this. And yeah, that's, that's going to be the, that seems like an amazing product for businesses for humans, just not, not just the assistant that's facing the individual, but the assistant that represents the individual.
[00:51:13.860 --> 00:51:43.840]   to the public, both, both directions. There's basically a layer that is the AI system through which you interact with the outside world, with the outside world that has humans in it. That's really interesting. And you that have social networks that connect billions of people, it seems like a heck of a large scale place to test some of this stuff out. Yeah, I mean, I think part of the reason why
[00:51:43.840 --> 00:52:03.200]   creators will want to do this is because they already have the communities on our services. Yeah. And, and, and a lot of the interface for this stuff today are chat type interfaces and, and between WhatsApp and, and messenger. I think that those are, you know, just great, great ways to, to interact with people.
[00:52:13.820 --> 00:52:24.800]   these social networks on Facebook, on Instagram, even, even on WhatsApp, having, having conversations where some heterogeneous, some as humans, some as AI.
[00:52:24.800 --> 00:52:43.000]   I think we'll get to that. You know, and, and, you know, if only just empirically looking at, then Microsoft released this thing called Xiaoice several years ago in China, it was a pre-LLM chatbot technology that, so it was a lot simpler.
[00:52:43.000 --> 00:53:12.980]   Yeah. I think that there's, there's, you know, there's certainly, you know, needs for companionship that people have, you know, older people.
[00:53:12.980 --> 00:53:42.960]   Yeah. I think most people probably don't have as many friends as they would like to have, right? If you look at, there's some interesting demographic studies around that, like the average person has the number of close friends that they have is fewer today than it was 15 years ago. And I mean, that gets to like, this is like the core thing that, that I think about in terms of, you know, building services that help connect people. So I think you'll get tools to do that.
[00:53:42.960 --> 00:54:12.940]   Yeah.
[00:54:12.940 --> 00:54:42.920]   Yeah.
[00:54:42.920 --> 00:55:12.900]   Yeah.
[00:55:12.900 --> 00:55:42.880]   Yeah.
[00:55:42.880 --> 00:56:12.860]   Yeah.
[00:56:12.860 --> 00:56:42.840]   Yeah.
[00:56:42.840 --> 00:57:12.820]   Yeah. I mean, I think it's mostly good. I mean, that was, that question was sort of framed in a negative way, but I mean, we were talking before about language models helping you communicate with, it was like language translation, helping you communicate with people who don't speak your language. I mean, at some level, what all this social technology is doing is helping people express themselves better to people in situations where they would have to be able to communicate with people.
[00:57:12.820 --> 00:57:42.800]   Yeah.
[00:57:42.800 --> 00:57:43.340]   that we have
[00:57:43.340 --> 00:57:43.860]   we have that
[00:57:43.860 --> 00:57:44.440]   we have art
[00:57:44.440 --> 00:57:45.640]   we have code
[00:57:45.640 --> 00:57:46.940]   but language
[00:57:46.940 --> 00:57:47.940]   is also a mapping
[00:57:47.940 --> 00:57:48.900]   of the way you think
[00:57:48.900 --> 00:57:49.580]   the way you see
[00:57:49.580 --> 00:57:49.940]   the world
[00:57:49.940 --> 00:57:51.080]   who you are
[00:57:51.080 --> 00:57:52.100]   and one of the
[00:57:52.100 --> 00:57:52.600]   applications
[00:57:52.600 --> 00:57:53.460]   I've recently
[00:57:53.460 --> 00:57:54.260]   talked to a person
[00:57:54.260 --> 00:57:55.880]   who's a
[00:57:55.880 --> 00:57:56.900]   actually a
[00:57:56.900 --> 00:57:57.180]   jiu-jitsu
[00:57:57.180 --> 00:57:57.700]   instructor
[00:57:57.700 --> 00:57:59.700]   he said that
[00:57:59.700 --> 00:58:00.240]   when he
[00:58:00.240 --> 00:58:02.360]   emails parents
[00:58:02.360 --> 00:58:03.460]   about
[00:58:03.460 --> 00:58:05.080]   their son
[00:58:05.080 --> 00:58:05.520]   and daughter
[00:58:05.520 --> 00:58:07.780]   that they
[00:58:07.780 --> 00:58:08.340]   can improve
[00:58:08.340 --> 00:58:09.020]   their discipline
[00:58:09.020 --> 00:58:09.500]   in class
[00:58:09.500 --> 00:58:10.060]   and so on
[00:58:10.060 --> 00:58:11.160]   he often
[00:58:11.160 --> 00:58:11.800]   finds that
[00:58:11.800 --> 00:58:12.640]   he comes off
[00:58:12.640 --> 00:58:13.200]   a bit of
[00:58:13.200 --> 00:58:13.680]   more of an
[00:58:13.680 --> 00:58:14.000]   asshole
[00:58:14.000 --> 00:58:14.620]   than he would
[00:58:14.620 --> 00:58:14.940]   like
[00:58:14.940 --> 00:58:15.760]   so he uses
[00:58:15.760 --> 00:58:16.480]   GPT
[00:58:16.480 --> 00:58:17.340]   to translate
[00:58:17.340 --> 00:58:18.040]   his original
[00:58:18.040 --> 00:58:18.440]   email
[00:58:18.440 --> 00:58:19.660]   into a nicer
[00:58:19.660 --> 00:58:20.540]   email
[00:58:20.540 --> 00:58:21.740]   a more polite
[00:58:21.740 --> 00:58:22.080]   one
[00:58:22.080 --> 00:58:23.400]   we hear this
[00:58:23.400 --> 00:58:23.700]   all the time
[00:58:23.700 --> 00:58:24.040]   a lot of
[00:58:24.040 --> 00:58:24.440]   creators
[00:58:24.440 --> 00:58:24.960]   on our
[00:58:24.960 --> 00:58:25.340]   services
[00:58:25.340 --> 00:58:25.880]   tell us
[00:58:25.880 --> 00:58:26.120]   that
[00:58:26.120 --> 00:58:26.920]   one of the
[00:58:26.920 --> 00:58:27.440]   most stressful
[00:58:27.440 --> 00:58:28.040]   things
[00:58:28.040 --> 00:58:30.020]   is basically
[00:58:30.020 --> 00:58:30.700]   negotiating
[00:58:30.700 --> 00:58:31.140]   deals
[00:58:31.140 --> 00:58:31.740]   with brands
[00:58:31.740 --> 00:58:32.220]   and stuff
[00:58:32.220 --> 00:58:32.540]   like the
[00:58:32.540 --> 00:58:32.760]   business
[00:58:32.760 --> 00:58:33.240]   side of it
[00:58:33.240 --> 00:58:33.640]   because they're
[00:58:33.640 --> 00:58:33.840]   like
[00:58:33.840 --> 00:58:34.360]   I mean
[00:58:34.360 --> 00:58:35.040]   they do
[00:58:35.040 --> 00:58:35.560]   their thing
[00:58:35.560 --> 00:58:35.780]   right
[00:58:35.780 --> 00:58:36.640]   and you know
[00:58:36.640 --> 00:58:37.000]   the creators
[00:58:37.000 --> 00:58:37.340]   they're
[00:58:37.340 --> 00:58:38.080]   excellent
[00:58:38.080 --> 00:58:38.420]   at what
[00:58:38.420 --> 00:58:38.780]   they do
[00:58:38.780 --> 00:58:39.040]   and they
[00:58:39.040 --> 00:58:39.280]   just want
[00:58:39.280 --> 00:58:39.580]   to connect
[00:58:39.580 --> 00:58:39.800]   with their
[00:58:39.800 --> 00:58:40.120]   community
[00:58:40.120 --> 00:58:40.400]   but then
[00:58:40.400 --> 00:58:40.660]   they get
[00:58:40.660 --> 00:58:41.780]   really stressed
[00:58:41.780 --> 00:58:42.000]   you know
[00:58:42.000 --> 00:58:42.260]   they go
[00:58:42.260 --> 00:58:45.860]   into their DMs and they see some brand wants
[00:58:45.860 --> 00:58:48.620]   to do something with them and they don't quite know
[00:58:48.620 --> 00:58:51.440]   how to negotiate or how to push back respectfully.
[00:58:51.440 --> 00:58:55.380]   And so I think building a tool that can actually allow them
[00:58:55.380 --> 00:58:58.820]   to do that well, is one simple thing that I think
[00:58:58.820 --> 00:59:00.980]   is just like an interesting thing that we've heard
[00:59:00.980 --> 00:59:03.380]   from a bunch of people that they'd be interested in.
[00:59:03.380 --> 00:59:06.200]   But I'm going back to the broader idea.
[00:59:06.200 --> 00:59:10.120]   I don't know.
[00:59:10.120 --> 00:59:14.800]   I mean, Priscilla and I just had our third daughter.
[00:59:14.800 --> 00:59:17.560]   - Congratulations by the way.
[00:59:17.560 --> 00:59:20.240]   - And it's like one of the saddest things in the world
[00:59:20.240 --> 00:59:22.240]   is like seeing your baby cry, right?
[00:59:22.240 --> 00:59:25.000]   But like, it's like, why is that?
[00:59:25.000 --> 00:59:28.140]   Right, it's like, well, 'cause babies don't generally
[00:59:28.140 --> 00:59:32.420]   have much capacity to tell you what they care about
[00:59:32.420 --> 00:59:33.260]   otherwise, right?
[00:59:33.260 --> 00:59:35.380]   And it's not actually just babies, right?
[00:59:35.380 --> 00:59:38.340]   It's, my five-year-old daughter cries too
[00:59:38.340 --> 00:59:40.000]   because she sometimes has a hard time
[00:59:40.000 --> 00:59:44.240]   expressing what matters to her.
[00:59:44.240 --> 00:59:46.100]   And then I was thinking about that and I was like,
[00:59:46.100 --> 00:59:48.420]   well, actually a lot of adults get very frustrated too
[00:59:48.420 --> 00:59:51.300]   because they can't, they have a hard time expressing things
[00:59:51.300 --> 00:59:54.560]   in a way that, going back to some of the early themes,
[00:59:54.560 --> 00:59:59.060]   that maybe is something that was a mistake
[00:59:59.060 --> 01:00:00.500]   or maybe they have pride or something like,
[01:00:00.500 --> 01:00:01.460]   all these things get in the way.
[01:00:01.460 --> 01:00:02.780]   So I don't know.
[01:00:02.780 --> 01:00:05.120]   I think that all of these different technologies
[01:00:05.120 --> 01:00:08.920]   that can help us navigate the social complexity
[01:00:08.920 --> 01:00:09.880]   and actually be able to better understand
[01:00:09.880 --> 01:00:13.100]   and better express what we're feeling and thinking,
[01:00:13.100 --> 01:00:15.020]   I think that's generally all good.
[01:00:15.020 --> 01:00:18.320]   And there are always these concerns like, okay,
[01:00:18.320 --> 01:00:19.960]   are people gonna have worse memories
[01:00:19.960 --> 01:00:21.880]   because you have Google to look things up?
[01:00:21.880 --> 01:00:24.780]   And I think in general, a generation later,
[01:00:24.780 --> 01:00:26.560]   you don't look back and lament that.
[01:00:26.560 --> 01:00:29.500]   I think it's just like, wow, we have so much more capacity
[01:00:29.500 --> 01:00:30.860]   to do so much more now.
[01:00:30.860 --> 01:00:32.980]   And I think that that'll be the case here too.
[01:00:32.980 --> 01:00:35.200]   - You can allocate those cognitive capabilities
[01:00:35.200 --> 01:00:38.960]   to like deeper, nuanced thought.
[01:00:38.960 --> 01:00:39.760]   - Yeah.
[01:00:39.760 --> 01:00:42.840]   - But it's change.
[01:00:42.840 --> 01:00:46.300]   So just like with Google search,
[01:00:46.300 --> 01:00:51.860]   the additional language models, large language models,
[01:00:51.860 --> 01:00:54.780]   you basically don't have to remember nearly as much.
[01:00:54.780 --> 01:00:57.920]   Just like with Stack Overflow for programming,
[01:00:57.920 --> 01:00:59.580]   now that these language models
[01:00:59.580 --> 01:01:01.260]   can generate code right there.
[01:01:01.260 --> 01:01:04.380]   I mean, I find that I write like maybe 80%,
[01:01:04.380 --> 01:01:08.880]   90% of the code I write is now generated first
[01:01:08.880 --> 01:01:09.640]   and then edited.
[01:01:09.640 --> 01:01:12.820]   So you don't have to remember how to write specifics
[01:01:12.820 --> 01:01:13.660]   of different functions.
[01:01:13.660 --> 01:01:14.980]   - Oh, but that's great.
[01:01:14.980 --> 01:01:19.640]   And it's also, it's not just the specific coding.
[01:01:19.640 --> 01:01:23.400]   I mean, in the context of a large company like this,
[01:01:23.400 --> 01:01:26.180]   I think before an engineer can sit down to code,
[01:01:26.180 --> 01:01:29.580]   they first need to figure out all of the libraries
[01:01:29.580 --> 01:01:32.900]   and dependencies that tens of thousands of people
[01:01:32.900 --> 01:01:34.840]   have written before them.
[01:01:34.840 --> 01:01:39.520]   And one of the things that I'm excited about,
[01:01:39.520 --> 01:01:42.500]   that we're working on is it's not just tools
[01:01:42.500 --> 01:01:43.760]   that help engineers code,
[01:01:43.760 --> 01:01:46.240]   it's tools that can help summarize the whole knowledge base
[01:01:46.240 --> 01:01:48.480]   and help people be able to navigate
[01:01:48.480 --> 01:01:49.760]   all the internal information.
[01:01:49.760 --> 01:01:50.900]   I mean, I think that that's,
[01:01:50.900 --> 01:01:54.200]   in the experiments that I've done with this stuff,
[01:01:54.200 --> 01:01:56.680]   I mean, that's on the public stuff,
[01:01:56.680 --> 01:02:01.680]   you just ask one of these models to build you a script
[01:02:01.680 --> 01:02:05.180]   that does anything and it basically already understands
[01:02:05.180 --> 01:02:07.120]   what the best libraries are to do that thing
[01:02:07.120 --> 01:02:08.280]   and pulls them in automatically.
[01:02:08.280 --> 01:02:09.400]   It's, I mean, I think that's super powerful.
[01:02:09.400 --> 01:02:12.960]   That was always the most annoying part of coding
[01:02:12.960 --> 01:02:14.680]   was that you had to spend all this time
[01:02:14.680 --> 01:02:16.440]   actually figuring out what the resources were
[01:02:16.440 --> 01:02:17.480]   that you were supposed to import
[01:02:17.480 --> 01:02:19.480]   before you could actually start building the thing.
[01:02:19.480 --> 01:02:20.320]   - Yeah.
[01:02:20.320 --> 01:02:23.420]   I mean, there's, of course, the flip side of that,
[01:02:23.420 --> 01:02:24.680]   I think for the most part is positive,
[01:02:24.680 --> 01:02:29.280]   but the flip side is if you outsource that thinking
[01:02:29.280 --> 01:02:34.280]   to an AI model, you might miss nuanced mistakes and bugs.
[01:02:34.280 --> 01:02:39.280]   You lose the skill to find those bugs.
[01:02:39.280 --> 01:02:41.560]   And those bugs might be,
[01:02:41.560 --> 01:02:44.520]   the code looks very convincingly right,
[01:02:44.520 --> 01:02:47.920]   but it's actually wrong in a very subtle way.
[01:02:47.920 --> 01:02:52.920]   But that's the trade-off that we face as human civilization
[01:02:52.920 --> 01:02:57.820]   when we build more and more powerful tools.
[01:02:57.820 --> 01:03:02.040]   When we stand on the shoulders of taller and taller giants,
[01:03:02.040 --> 01:03:04.300]   we could do more, but then we forget how to do
[01:03:04.300 --> 01:03:05.700]   all the stuff that they did.
[01:03:05.700 --> 01:03:09.160]   It's a weird trade-off.
[01:03:09.160 --> 01:03:10.000]   - Yeah, I agree.
[01:03:10.000 --> 01:03:13.100]   I mean, I think it is very valuable in your life
[01:03:13.100 --> 01:03:15.600]   to be able to do basic things too.
[01:03:15.600 --> 01:03:19.600]   - Do you worry about some of the concerns
[01:03:19.600 --> 01:03:22.840]   of bots being present on social networks?
[01:03:22.840 --> 01:03:24.980]   More and more human-like bots
[01:03:24.980 --> 01:03:29.560]   that are not necessarily trying to do a good thing,
[01:03:29.560 --> 01:03:32.260]   or they might be explicitly trying to do a bad thing,
[01:03:32.260 --> 01:03:35.260]   like phishing scams, like social engineering,
[01:03:35.260 --> 01:03:36.400]   all that kind of stuff,
[01:03:36.400 --> 01:03:39.040]   which has always been a very difficult problem for some.
[01:03:39.040 --> 01:03:39.880]   - Yeah.
[01:03:39.880 --> 01:03:40.700]   - For social networks,
[01:03:40.700 --> 01:03:41.780]   but now it's becoming almost a more and more
[01:03:41.780 --> 01:03:43.160]   difficult problem.
[01:03:43.160 --> 01:03:46.080]   - Well, I think there's a few different parts of this.
[01:03:46.080 --> 01:03:51.080]   So one is there are all these harms
[01:03:51.080 --> 01:03:53.400]   that we need to basically fight against and prevent.
[01:03:53.400 --> 01:03:57.160]   And that's been a lot of our focus
[01:03:57.160 --> 01:04:00.000]   over the last five or seven years
[01:04:00.000 --> 01:04:03.880]   is basically ramping up very sophisticated AI systems,
[01:04:03.880 --> 01:04:05.160]   not generative AI systems,
[01:04:05.160 --> 01:04:08.920]   more kind of classical AI systems to be able to,
[01:04:08.920 --> 01:04:13.920]   you know, categorize and classify and identify,
[01:04:13.920 --> 01:04:17.820]   okay, this post looks like it's promoting terrorism.
[01:04:17.820 --> 01:04:21.680]   This one is, you know, like exploiting children.
[01:04:21.680 --> 01:04:24.160]   This one is, looks like it might be trying
[01:04:24.160 --> 01:04:25.300]   to incite violence.
[01:04:25.300 --> 01:04:28.140]   This one's an intellectual property violation.
[01:04:28.140 --> 01:04:30.080]   So there's like,
[01:04:30.080 --> 01:04:32.040]   that's like 18 different categories
[01:04:32.040 --> 01:04:35.480]   of violating kind of harmful content
[01:04:35.480 --> 01:04:37.860]   that we've had to build specific systems
[01:04:37.860 --> 01:04:38.800]   to be able to track.
[01:04:38.800 --> 01:04:42.440]   And I think it's certainly the case
[01:04:42.440 --> 01:04:47.360]   that advances in generative AI will test those,
[01:04:47.360 --> 01:04:52.340]   but at least so far it's been the case,
[01:04:52.340 --> 01:04:55.160]   and I'm optimistic that it will continue to be the case
[01:04:55.160 --> 01:04:59.020]   that we will be able to bring more computing power to bear
[01:04:59.020 --> 01:05:00.440]   to have even stronger AIs
[01:05:00.440 --> 01:05:02.120]   that can help defend against those things.
[01:05:02.120 --> 01:05:06.640]   So we've had to deal with some adversarial issues before.
[01:05:06.640 --> 01:05:08.680]   Right, it's, I mean, for some things like,
[01:05:08.680 --> 01:05:11.160]   hate speech, it's like people aren't generally getting
[01:05:11.160 --> 01:05:14.860]   a lot more sophisticated, like the average person who,
[01:05:14.860 --> 01:05:16.880]   let's say, you know, if someone's saying
[01:05:16.880 --> 01:05:18.540]   some kind of racist thing, right?
[01:05:18.540 --> 01:05:20.680]   It's like, they're not necessarily getting more sophisticated
[01:05:20.680 --> 01:05:21.760]   at being racist, right?
[01:05:21.760 --> 01:05:22.600]   It just, it's okay.
[01:05:22.600 --> 01:05:24.840]   So that the system can just find,
[01:05:24.840 --> 01:05:26.840]   but then there's other adversaries
[01:05:26.840 --> 01:05:29.180]   who actually are very sophisticated,
[01:05:29.180 --> 01:05:30.860]   like nation states doing things.
[01:05:30.860 --> 01:05:34.020]   And, you know, we find, you know, whether it's Russia
[01:05:34.020 --> 01:05:36.120]   or, you know, just different countries
[01:05:36.120 --> 01:05:38.560]   that are basically standing up these networks of,
[01:05:38.560 --> 01:05:42.960]   of bots or, you know, inauthentic accounts
[01:05:42.960 --> 01:05:44.060]   is what we call them,
[01:05:44.060 --> 01:05:45.280]   'cause they're not necessarily bots.
[01:05:45.280 --> 01:05:47.200]   That some of them could actually be real people
[01:05:47.200 --> 01:05:50.460]   who are kind of masquerading as other people,
[01:05:50.460 --> 01:05:53.140]   but they're acting in a coordinated way.
[01:05:53.140 --> 01:05:57.320]   And some of that behavior has gotten very sophisticated
[01:05:57.320 --> 01:05:58.440]   and it's very adversarial.
[01:05:58.440 --> 01:05:59.800]   So they, you know, each iteration,
[01:05:59.800 --> 01:06:03.060]   every time we find something and stop them,
[01:06:03.060 --> 01:06:04.260]   they kind of evolve their behavior.
[01:06:04.260 --> 01:06:06.440]   They don't just pack up their bags and go home and say,
[01:06:06.440 --> 01:06:08.440]   "Okay, we're not gonna try."
[01:06:08.440 --> 01:06:10.740]   At some point they might decide doing it on meta services
[01:06:10.740 --> 01:06:12.080]   is not worth it.
[01:06:12.080 --> 01:06:13.100]   They'll go do it on someone else
[01:06:13.100 --> 01:06:14.980]   if it's easier to do it in another place.
[01:06:14.980 --> 01:06:19.780]   But we have a fair amount of experience dealing with
[01:06:19.780 --> 01:06:22.740]   even those kind of adversarial attacks
[01:06:22.740 --> 01:06:24.680]   where they just keep on getting better and better.
[01:06:24.680 --> 01:06:27.100]   And I do think that as long as we can keep on putting
[01:06:27.100 --> 01:06:28.780]   more compute power against it,
[01:06:28.780 --> 01:06:31.160]   and if we're kind of one of the leaders
[01:06:31.160 --> 01:06:33.060]   in developing some of these AI models,
[01:06:33.060 --> 01:06:34.680]   I'm quite optimistic that we're gonna be able
[01:06:34.680 --> 01:06:38.320]   to keep on pushing against the,
[01:06:38.320 --> 01:06:41.480]   kind of normal categories of harm that you talk about,
[01:06:41.480 --> 01:06:46.480]   fraud, scams, spam, IP violations, things like that.
[01:06:46.480 --> 01:06:50.820]   - What about like creating narratives and controversy?
[01:06:50.820 --> 01:06:55.400]   To me, it's kind of amazing how a small collection of,
[01:06:55.400 --> 01:06:56.240]   - Yeah.
[01:06:56.240 --> 01:06:57.080]   - What did you say?
[01:06:57.080 --> 01:06:59.440]   Inauthentic accounts, so it could be bots, but-
[01:06:59.440 --> 01:07:01.460]   - Yeah, I mean, we have sort of this funny name for it,
[01:07:01.460 --> 01:07:03.760]   but we call it coordinated inauthentic behavior.
[01:07:03.760 --> 01:07:04.600]   - Yeah.
[01:07:04.600 --> 01:07:08.200]   It's kind of incredible how a small collection of folks can,
[01:07:08.200 --> 01:07:12.080]   create narratives, create stories.
[01:07:12.080 --> 01:07:13.000]   - Yeah.
[01:07:13.000 --> 01:07:14.360]   - Especially if they're viral.
[01:07:14.360 --> 01:07:16.600]   So if, especially if they have a element
[01:07:16.600 --> 01:07:21.180]   that can catalyze the virality of the narrative.
[01:07:21.180 --> 01:07:24.420]   - Yeah, and I think there, the question is you have to be,
[01:07:24.420 --> 01:07:27.160]   I'm very specific about what is bad about it, right?
[01:07:27.160 --> 01:07:31.360]   Because I think a set of people coming together
[01:07:31.360 --> 01:07:34.880]   or organically bouncing ideas off each other
[01:07:34.880 --> 01:07:38.080]   and a narrative comes out of that is not necessarily
[01:07:38.080 --> 01:07:39.660]   a bad thing by itself.
[01:07:39.660 --> 01:07:42.520]   If it's kind of authentic and organic,
[01:07:42.520 --> 01:07:43.620]   that's like a lot of what happens
[01:07:43.620 --> 01:07:45.660]   and how culture gets created and how art gets created
[01:07:45.660 --> 01:07:46.500]   and a lot of good stuff.
[01:07:46.500 --> 01:07:49.240]   So that's why we've kind of focused on this sense
[01:07:49.240 --> 01:07:51.580]   of coordinated inauthentic behavior.
[01:07:51.580 --> 01:07:53.640]   So it's like, if you have a network of, you know,
[01:07:53.640 --> 01:07:56.120]   whether it's bots, some people masquerading
[01:07:56.120 --> 01:07:57.360]   as different accounts,
[01:07:57.360 --> 01:08:01.960]   but you have kind of someone pulling the strings behind it
[01:08:01.960 --> 01:08:07.960]   and trying to kind of act as if this is a more organic set
[01:08:07.960 --> 01:08:09.400]   of behavior, but really it's not,
[01:08:09.400 --> 01:08:11.780]   it's just like one coordinated thing.
[01:08:11.780 --> 01:08:13.540]   That seems problematic to me, right?
[01:08:13.540 --> 01:08:14.960]   I mean, I don't think people should be able
[01:08:14.960 --> 01:08:19.280]   to have coordinated networks and not disclose it as such,
[01:08:19.280 --> 01:08:22.880]   but that again, you know, we've been able to deploy
[01:08:22.880 --> 01:08:25.200]   pretty sophisticated AI and, you know,
[01:08:25.200 --> 01:08:28.020]   counter-terrorism groups and things like that
[01:08:28.020 --> 01:08:32.220]   to be able to identify a fair number of these coordinated
[01:08:32.220 --> 01:08:36.080]   and authentic networks of accounts and take them down.
[01:08:37.840 --> 01:08:38.680]   - Yeah, I think we need to do that.
[01:08:38.680 --> 01:08:40.620]   And I think we've, you know, it's one thing
[01:08:40.620 --> 01:08:42.520]   that if you'd told me 20 years ago, it's like, all right,
[01:08:42.520 --> 01:08:45.040]   you're starting this website to help people connect
[01:08:45.040 --> 01:08:47.000]   at a college and, you know, in the future,
[01:08:47.000 --> 01:08:49.040]   you're gonna be, you know, part of your organization
[01:08:49.040 --> 01:08:51.420]   is gonna be a counter-terrorism organization with AI
[01:08:51.420 --> 01:08:53.520]   to find coordinated inauthentic,
[01:08:53.520 --> 01:08:56.000]   I would have thought that was pretty wild,
[01:08:56.000 --> 01:09:00.960]   but it's, but no, I think that that's part of where we are.
[01:09:00.960 --> 01:09:02.820]   But look, I think that these questions
[01:09:02.820 --> 01:09:07.720]   that you're pushing on now, this is actually where I'd guess,
[01:09:07.720 --> 01:09:10.800]   most of the challenge around AI will be
[01:09:10.800 --> 01:09:12.840]   for the foreseeable future.
[01:09:12.840 --> 01:09:15.920]   I think that there's a lot of debate around things like,
[01:09:15.920 --> 01:09:19.040]   is this going to create existential risk to humanity?
[01:09:19.040 --> 01:09:20.940]   And I think that those are very hard things
[01:09:20.940 --> 01:09:22.920]   to disprove one way or another.
[01:09:22.920 --> 01:09:26.520]   My own intuition is that the point at which we become close
[01:09:26.520 --> 01:09:31.520]   to super intelligence is, it's just really unclear to me
[01:09:31.520 --> 01:09:35.660]   that the current technology is gonna get there
[01:09:35.660 --> 01:09:37.600]   without another set of significant,
[01:09:37.600 --> 01:09:39.340]   significant advances.
[01:09:39.340 --> 01:09:40.840]   But that doesn't mean that there's no danger.
[01:09:40.840 --> 01:09:44.100]   I think the danger is basically amplifying the kind of known
[01:09:44.100 --> 01:09:48.460]   set of harms that people or sets of accounts can do.
[01:09:48.460 --> 01:09:51.060]   And we just need to make sure that we really focus on,
[01:09:51.060 --> 01:09:54.920]   on basically doing that as well as possible.
[01:09:54.920 --> 01:09:57.680]   So that's definitely a big focus for me.
[01:09:57.680 --> 01:10:00.260]   - Well, you can basically use large language models
[01:10:00.260 --> 01:10:03.940]   as an assistant of how to cause harm on social networks.
[01:10:03.940 --> 01:10:05.280]   So you can ask it a question,
[01:10:07.480 --> 01:10:11.720]   you know, Meta has very impressive coordinated
[01:10:11.720 --> 01:10:16.100]   inauthentic account fighting capabilities.
[01:10:16.100 --> 01:10:21.100]   How do I do the coordinating authentic account creation
[01:10:21.100 --> 01:10:23.460]   where Meta doesn't detect it?
[01:10:23.460 --> 01:10:25.400]   Like literally ask that question.
[01:10:25.400 --> 01:10:29.260]   And basically there's this kind of, part of it,
[01:10:29.260 --> 01:10:30.980]   I mean, that's what OpenAI showed,
[01:10:30.980 --> 01:10:33.020]   that they're concerned with those questions.
[01:10:33.020 --> 01:10:34.920]   Perhaps you can comment on your approach to it,
[01:10:34.920 --> 01:10:37.360]   how to do a kind of modeling,
[01:10:37.360 --> 01:10:39.920]   moderation on the output of those models
[01:10:39.920 --> 01:10:43.860]   that it can't be used to help you coordinate harm
[01:10:43.860 --> 01:10:46.980]   in all the full definition of what the harm means.
[01:10:46.980 --> 01:10:48.860]   - Yeah, and that's a lot of the fine tuning
[01:10:48.860 --> 01:10:52.680]   and the alignment training that we do is basically,
[01:10:52.680 --> 01:10:57.680]   you know, when we ship AIs across our products,
[01:10:57.680 --> 01:11:02.520]   a lot of what we're trying to make sure is that,
[01:11:02.520 --> 01:11:07.240]   you know, you can't ask it to help you commit a crime, right?
[01:11:07.240 --> 01:11:12.240]   It's, so I think training it to kind of understand that,
[01:11:12.240 --> 01:11:16.140]   and it's not that,
[01:11:16.140 --> 01:11:17.180]   not like any of these systems
[01:11:17.180 --> 01:11:18.840]   are ever gonna be a hundred percent perfect,
[01:11:18.840 --> 01:11:23.840]   but you know, just making it so that this isn't a,
[01:11:23.840 --> 01:11:30.060]   an easier way to go about doing something bad
[01:11:30.060 --> 01:11:32.020]   than the next best alternative, right?
[01:11:32.020 --> 01:11:33.300]   I mean, people still have Google, right?
[01:11:33.300 --> 01:11:35.040]   They, you know, you still have search engines.
[01:11:35.040 --> 01:11:37.120]   So the information is out there.
[01:11:37.120 --> 01:11:42.120]   And for these, you know, what we see is like for nation states
[01:11:42.120 --> 01:11:46.740]   or, you know, these actors that are trying to pull off
[01:11:46.740 --> 01:11:50.180]   these large, you know, coordinated and authentic networks
[01:11:50.180 --> 01:11:53.180]   to kind of influence different things,
[01:11:53.180 --> 01:11:55.060]   at some point when we would just make it very difficult,
[01:11:55.060 --> 01:11:56.080]   they do just, you know,
[01:11:56.080 --> 01:11:58.060]   try to use other services instead, right?
[01:11:58.060 --> 01:12:00.860]   It's just like, if you can make it more expensive
[01:12:00.860 --> 01:12:03.400]   for them to do it on your service,
[01:12:03.400 --> 01:12:05.900]   then kind of people go elsewhere.
[01:12:05.900 --> 01:12:07.000]   And I think that that's,
[01:12:07.000 --> 01:12:08.460]   that's the bar, right?
[01:12:08.460 --> 01:12:10.000]   It's like, it's not like, okay,
[01:12:10.000 --> 01:12:12.300]   are you ever gonna be perfect at finding, you know,
[01:12:12.300 --> 01:12:14.480]   every adversary who tries to attack you?
[01:12:14.480 --> 01:12:16.940]   It's, I mean, you try to get as close to that as possible,
[01:12:16.940 --> 01:12:20.060]   but I think really kind of economically,
[01:12:20.060 --> 01:12:21.780]   what you're just trying to do is make it so that it's,
[01:12:21.780 --> 01:12:24.920]   it's just inefficient for them to go after that.
[01:12:24.920 --> 01:12:27.360]   - But there's also complicated questions of what is
[01:12:27.360 --> 01:12:30.640]   and isn't harm, what is and isn't misinformation.
[01:12:30.640 --> 01:12:32.820]   So this is one of the things that Wikipedia
[01:12:32.820 --> 01:12:34.800]   has also tried to face.
[01:12:34.800 --> 01:12:35.640]   - Yeah.
[01:12:35.640 --> 01:12:36.880]   - I remember asking
[01:12:36.880 --> 01:12:40.900]   the GPT about whether the virus leaked from a lab or not.
[01:12:40.900 --> 01:12:45.120]   And the answer provided was a very nuanced one
[01:12:45.120 --> 01:12:48.720]   and a well-cited one, almost, dare I say,
[01:12:48.720 --> 01:12:52.720]   well thought out one, balanced.
[01:12:52.720 --> 01:12:55.000]   I would hate for that nuance to be lost
[01:12:55.000 --> 01:12:56.700]   through the process of moderation.
[01:12:56.700 --> 01:13:00.560]   Wikipedia does a good job on that particular thing too.
[01:13:00.560 --> 01:13:03.280]   But from pressures from governments and institutions,
[01:13:03.280 --> 01:13:06.760]   it's, you could see some of that nuance
[01:13:06.760 --> 01:13:11.760]   and depth of information, facts, and wisdom be lost.
[01:13:11.760 --> 01:13:14.560]   - Absolutely.
[01:13:14.560 --> 01:13:16.540]   - And that's a scary thing.
[01:13:16.540 --> 01:13:19.580]   Some of the magic, some of the edges,
[01:13:19.580 --> 01:13:21.000]   the rough edges might be lost
[01:13:21.000 --> 01:13:23.980]   to the process of moderation of AI systems.
[01:13:23.980 --> 01:13:26.540]   So how do you get that right?
[01:13:26.540 --> 01:13:28.940]   - I really agree with what you're pushing on.
[01:13:28.940 --> 01:13:33.940]   I mean, the core, I think the core shape of the problem
[01:13:33.940 --> 01:13:36.640]   is that there are some harms that I think,
[01:13:36.640 --> 01:13:38.820]   I think everyone agrees are bad, right?
[01:13:38.820 --> 01:13:43.820]   So sexual exploitation of children, right?
[01:13:43.820 --> 01:13:46.000]   Like you're not gonna get many people
[01:13:46.000 --> 01:13:47.760]   who think that that type of thing
[01:13:47.760 --> 01:13:49.520]   should be allowed on any service, right?
[01:13:49.520 --> 01:13:51.520]   And that's something that we face
[01:13:51.520 --> 01:13:56.180]   and try to push off the, as much as possible today.
[01:13:56.180 --> 01:13:59.560]   You know, terrorism, inciting violence, right?
[01:13:59.560 --> 01:14:01.520]   It's like, we went through a bunch of these types
[01:14:01.520 --> 01:14:06.520]   of harms before, but then I do think that you get to a set
[01:14:06.520 --> 01:14:09.800]   of harms where there is more social debate around it.
[01:14:09.800 --> 01:14:13.600]   So misinformation I think is,
[01:14:13.600 --> 01:14:18.600]   has been a really tricky one because there are things
[01:14:18.600 --> 01:14:23.380]   that are kind of obviously false, right?
[01:14:23.380 --> 01:14:28.380]   That are maybe factual, but may not be harmful.
[01:14:28.380 --> 01:14:32.800]   So it's like, all right, are you gonna censor someone
[01:14:32.800 --> 01:14:34.060]   for just being wrong?
[01:14:34.060 --> 01:14:36.400]   It's, you know, if there's no kind of harm implication
[01:14:36.400 --> 01:14:37.840]   of what they're doing, I think that that's,
[01:14:37.840 --> 01:14:41.180]   there's a bunch of real kind of issues and challenges there.
[01:14:41.180 --> 01:14:44.300]   But then I think that there are other places where it is,
[01:14:44.300 --> 01:14:47.260]   you know, just take some of the stuff around COVID
[01:14:47.260 --> 01:14:51.240]   earlier on in the pandemic, where there were, you know,
[01:14:51.240 --> 01:14:54.860]   real health implications, but there hadn't been time
[01:14:54.860 --> 01:14:57.160]   to fully vet a bunch of the scientific assumptions.
[01:14:57.160 --> 01:14:59.900]   And, you know, unfortunately, I think a lot of the kind
[01:14:59.900 --> 01:15:03.120]   of establishment on that, you know, kind of waffled
[01:15:03.120 --> 01:15:05.920]   on a bunch of facts and, you know, asked for a bunch of things
[01:15:05.920 --> 01:15:06.280]   to be censored. And so I think that's a really good way to, you know,
[01:15:06.280 --> 01:15:11.200]   That in retrospect ended up being, you know, more debatable or, or true.
[01:15:11.200 --> 01:15:13.240]   And that stuff is really tough, right.
[01:15:13.240 --> 01:15:15.380]   And really undermines trust and that.
[01:15:15.380 --> 01:15:23.160]   And, um, so I do think that the questions around how to manage that are, are, are very
[01:15:23.160 --> 01:15:29.400]   nuanced the way that I try to think about it is that, um, it goes, I think it's best
[01:15:29.400 --> 01:15:34.260]   to generally boil things down to the harms that people agree on.
[01:15:34.280 --> 01:15:38.780]   So when you think about, you know, is, is something misinformation or not, I think often
[01:15:38.780 --> 01:15:45.900]   the more salient bit is, is this going to potentially leave lead to, um, to physical
[01:15:45.900 --> 01:15:50.040]   harm for someone, um, and, and kind of think about it in that sense.
[01:15:50.040 --> 01:15:53.480]   And then beyond that, I think people just have different preferences on how they want
[01:15:53.480 --> 01:15:55.020]   things to be flagged for them.
[01:15:55.020 --> 01:16:00.080]   I think a bunch of people would prefer to kind of have a flag on something that says,
[01:16:00.080 --> 01:16:04.260]   Hey, a fact checker thinks that this might be false or, um, I think Twitter's community
[01:16:04.260 --> 01:16:06.880]   notes implementation is quite good on, on this.
[01:16:06.880 --> 01:16:09.440]   Um, but again, it's the same type of thing.
[01:16:09.440 --> 01:16:13.800]   It's like just kind of discretionarily adding a flag because it makes the user experience
[01:16:13.800 --> 01:16:17.200]   better, but it's not, it's not, you know, trying to take down the information or not.
[01:16:17.200 --> 01:16:22.380]   I think that you want to reserve the kind of censorship of, of content to things that
[01:16:22.380 --> 01:16:26.700]   are of known categories that, that people generally agree are bad.
[01:16:26.700 --> 01:16:27.680]   Yeah.
[01:16:27.680 --> 01:16:33.480]   But there's so many things, especially with the pandemic, but there's other topics where
[01:16:33.480 --> 01:16:34.080]   there's just.
[01:16:34.080 --> 01:16:40.180]   Deep disagreement fueled by politics about what is and isn't harmful.
[01:16:40.180 --> 01:16:46.280]   There's, uh, even just the degree to which the virus is harmful and the degree to which
[01:16:46.280 --> 01:16:49.720]   the vaccines that respond to the virus are harmful.
[01:16:49.720 --> 01:16:53.620]   There's just, there's a, almost like a political divide around that.
[01:16:53.620 --> 01:16:57.360]   And so how do you make decisions about that?
[01:16:57.440 --> 01:17:03.940]   Where half the country in the United States or some large fraction of the world has very
[01:17:03.940 --> 01:17:06.440]   different views from another part of the world?
[01:17:06.440 --> 01:17:12.960]   Is, is there a way for meta to stay out of the moderation of this?
[01:17:12.960 --> 01:17:20.660]   I think we, it's very difficult to just abstain, but, but I think we should be clear about
[01:17:20.660 --> 01:17:27.260]   which of these things are actual safety concerns and which ones are a matter of priority.
[01:17:27.260 --> 01:17:27.360]   Yeah.
[01:17:27.360 --> 01:17:27.400]   Yeah.
[01:17:27.400 --> 01:17:27.420]   Yeah.
[01:17:27.420 --> 01:17:28.660]   So I think that's, I think that's a good point.
[01:17:28.660 --> 01:17:28.880]   I think that's a good point.
[01:17:28.880 --> 01:17:28.980]   I think that's a good point.
[01:17:28.980 --> 01:17:29.200]   I think that's a good point.
[01:17:29.200 --> 01:17:29.380]   I think that's a good point.
[01:17:29.380 --> 01:17:30.080]   I think that's a good point.
[01:17:30.080 --> 01:17:30.300]   I think that's a good point.
[01:17:30.300 --> 01:17:30.420]   Right.
[01:17:30.420 --> 01:17:40.380]   So we did recently introduce something that allows people to have fact checking, not affect the distribution of, of, of, um, of what shows up in their product.
[01:17:40.380 --> 01:17:40.900]   So, okay.
[01:17:40.900 --> 01:17:42.920]   A bunch of people don't trust who the fact checkers are.
[01:17:42.920 --> 01:17:43.700]   All right.
[01:17:43.700 --> 01:17:48.380]   Well, you can, you can turn that off if you want, but if the, if the, if the content, you know,
[01:17:48.380 --> 01:17:52.620]   violates some policy, like it's inciting violence or something like that, it's still not gonna be allowed.
[01:17:53.000 --> 01:17:58.280]   So I, I think that you wanna honor people's preferences on, on that as much as possible.
[01:17:58.280 --> 01:18:01.940]   Um, but look, I mean, this is really difficult stuff.
[01:18:01.940 --> 01:18:15.740]   I think the, it's really hard to know where to draw the line on what is fact and what is opinion, because the nature of science is that nothing is ever a hundred percent known for certain.
[01:18:15.740 --> 01:18:22.960]   You can disprove certain things, but you're constantly testing new hypotheses and, um, you know, scrutinizing.
[01:18:22.960 --> 01:18:31.360]   Frameworks that have been long held and every once in a while you, you throw out something that was working for a very long period of time and it's very difficult.
[01:18:31.360 --> 01:18:40.900]   But, um, but I think that just because it's very hard and just because they're edge cases doesn't mean that you, you know, should not try to give people what they're looking for as well.
[01:18:40.900 --> 01:18:52.920]   Let me ask about something you've faced in terms of moderation is, uh, pressure from different sources, pressure from work.
[01:18:52.920 --> 01:18:53.760]   From governments.
[01:18:53.760 --> 01:19:02.940]   I wanna ask a question, how to withstand that pressure for a world where AI moderation starts becoming a thing too.
[01:19:02.940 --> 01:19:15.360]   So what's, um, meta's approach to, um, to resist the pressure from governments and other interest groups in terms of what, uh, to moderate and not.
[01:19:15.360 --> 01:19:19.560]   I don't know that there's like a one size fits all answer to that.
[01:19:19.560 --> 01:19:22.860]   I mean, I think we basically have the principles.
[01:19:22.880 --> 01:19:40.220]   Around, you know, we wanna allow people to express as much as possible, but we have developed clear categories of things that we think are wrong that we don't want on our services and we build tools to try to moderate those.
[01:19:40.220 --> 01:19:49.580]   So then the question is, okay, what do you do when a government says that they don't want something on, on the service?
[01:19:49.580 --> 01:19:52.760]   And I think we have, we have a bunch of, uh.
[01:19:52.760 --> 01:20:09.140]   Um, principles around how we deal with that, because on, on the one hand, if there's a, you know, democratically elected government and people around the world just have different values and different places, then, you know, should we, as a, you know, California based company.
[01:20:09.140 --> 01:20:20.900]   Tell them that something that they have decided is unacceptable, actually, like that we need to be able to, to, to, to express that.
[01:20:20.900 --> 01:20:22.600]   I mean, I think that that's, there's a.
[01:20:22.720 --> 01:20:26.260]   Certain amount of, um, of hubris in that.
[01:20:26.260 --> 01:20:42.460]   Um, but then I think that there are other cases where, you know, it's, it's like a little more autocratic and, you know, you have the dictator leader who's just trying to crack down on descent and, you know, the people in a country are really, um, not aligned with that.
[01:20:42.460 --> 01:20:49.060]   Um, and it's not necessarily against their culture, but, um, but the, the, the person who's, who's leading it is, is just trying to push in a certain direction.
[01:20:49.060 --> 01:20:52.500]   Um, these are very complex questions.
[01:20:52.680 --> 01:21:01.160]   Uh, but I, I think, so it's, it's difficult to have, have a one size fits all, um, approach to it.
[01:21:01.160 --> 01:21:10.340]   But in general, we're, we're pretty active in, in kind of advocating and pushing back on, on, um, requests to take things down.
[01:21:10.340 --> 01:21:17.740]   Um, but honestly, the thing that I think a request to censor things is one thing.
[01:21:17.740 --> 01:21:22.520]   Um, and that's obviously bad, but where we, um, draw a much harder line.
[01:21:22.640 --> 01:21:25.920]   Um, is on requests for access to information, right?
[01:21:25.920 --> 01:21:32.020]   Because, you know, if you can, if you get told that you can't say something, I mean, that's bad, right?
[01:21:32.020 --> 01:21:40.480]   I mean, that, that, you know, is, is, you know, obviously it violates your sense and freedom of expression at some level.
[01:21:40.480 --> 01:21:52.480]   But, um, but a government getting access to data in a way that seems, um, like it would be unlawful in, in, in our country, um, exposes people to real physicality.
[01:21:52.600 --> 01:21:59.220]   Um, and that's something that in general we take very seriously.
[01:21:59.220 --> 01:22:04.660]   And then, so there's, that flows through like all of our policies and in a lot of ways, right?
[01:22:04.660 --> 01:22:11.380]   It's by the time you're actually like litigating with a government or pushing back on them, that's pretty late in the funnel.
[01:22:11.380 --> 01:22:17.860]   I'd say a bunch of this stuff starts a lot higher up in the decision of where do we put data centers.
[01:22:18.240 --> 01:22:24.020]   And, um, there are a lot of countries where, you know, we may have a lot of people using the service in a place.
[01:22:24.020 --> 01:22:27.960]   It might be, you know, good for the service in some ways.
[01:22:27.960 --> 01:22:33.540]   Um, good for those people if we could reduce the latency by having a data center nearby them.
[01:22:33.540 --> 01:22:46.240]   But, you know, for whatever reason, we just feel like, hey, this government does not have a good track record on, on, um, basically not trying to get access to people's data.
[01:22:46.240 --> 01:22:48.200]   And at the end of the day, I mean, if you put a,
[01:22:48.200 --> 01:22:58.600]   data center in a country and the government wants to get access to people's data, then, you know, they do at the end of the day have the option of having people show up with guns and taking it by force.
[01:22:58.600 --> 01:23:11.340]   So I think that there's like a lot of decisions that go into like how you architect the systems, um, years in advance of, of these actual confrontations that end up being really important.
[01:23:11.340 --> 01:23:17.060]   So you put the protection of people's data as a very, very high priority.
[01:23:18.160 --> 01:23:21.620]   But in that, I think is a, there are more harms that I think can be associated with that.
[01:23:21.620 --> 01:23:26.620]   And, and I think that that ends up being a more critical thing to defend against governments.
[01:23:26.620 --> 01:23:40.700]   Um, then, you know, whereas, you know, if another government has a different view of what should be acceptable speech in their country, especially if it's a democratically elected government and, you know, it's then I think that there's a certain amount of deference that you should have to that.
[01:23:40.700 --> 01:23:46.960]   So it's, uh, that's speaking more to the direct harm that's possible when you give governments access to data.
[01:23:47.280 --> 01:24:01.880]   But if we look at the United States to the more nuanced kind of pressure to censor, not even order to censor, but pressure to censor from political entities, which has kind of received quite a bit of attention in the United States.
[01:24:01.880 --> 01:24:17.240]   Um, maybe one way to ask that question is if you've seen the Twitter files, uh, what have you learned from the kind of, uh, pressure from us government agencies that was seen in Twitter?
[01:24:17.240 --> 01:24:20.480]   And what do you do with that kind of pressure?
[01:24:20.480 --> 01:24:23.920]   You know, I mean, I've, I've, I've seen it.
[01:24:23.920 --> 01:24:30.080]   Um, it's really hard from the outside to know exactly what happened in each of these cases.
[01:24:30.080 --> 01:24:45.940]   You know, we've, we've obviously been, been in a bunch of our own cases where, you know, where agencies or different folks will, will just say, Hey, here's a threat that we're aware of.
[01:24:45.940 --> 01:24:47.200]   You should be aware of it.
[01:24:47.200 --> 01:24:48.060]   You should be aware of this too.
[01:24:48.060 --> 01:24:57.400]   It's not really pressure as much as it is just, um, you know, flagging something that, that our, our security systems should be on, on alert about.
[01:24:57.400 --> 01:25:00.700]   I, I get how some people could think of it as that.
[01:25:00.700 --> 01:25:07.700]   Um, but at the end of the day, it's our, it's our call on how to, on, on how to handle that.
[01:25:07.700 --> 01:25:14.660]   But I mean, I, I just, you know, in terms of running these services won't have access to as much information about what people think that adversaries might be trying to do as possible.
[01:25:14.660 --> 01:25:16.640]   Wait, so you don't feel.
[01:25:17.160 --> 01:25:30.200]   Like there will be consequences if, uh, you know, anybody, the CIA, the FBI, a political party, the Democrats or the Republicans of high powerful political figures, right?
[01:25:30.200 --> 01:25:30.800]   Emails.
[01:25:30.800 --> 01:25:33.240]   You don't feel pressure from.
[01:25:33.240 --> 01:25:43.960]   I guess what I say is there's so much pressure from all sides that I'm not sure that any specific thing that someone says is really adding that much more to the mix.
[01:25:43.960 --> 01:25:47.120]   It's, um, there are obviously a lot of people.
[01:25:47.160 --> 01:25:52.320]   Who think that, um, that we should be censoring more content, right?
[01:25:52.320 --> 01:25:54.700]   There are a lot of people who think we should be censoring less content.
[01:25:54.700 --> 01:25:59.500]   There are, as you say, all kinds of different groups that are involved in these debates, right?
[01:25:59.500 --> 01:26:03.540]   So there's the kind of elected officials and politicians themselves.
[01:26:03.540 --> 01:26:08.700]   There's the agencies, but, but I mean, but there's the, the media, um, there's activist groups.
[01:26:08.700 --> 01:26:11.500]   There's, um, this is not a us specific thing.
[01:26:11.500 --> 01:26:17.080]   There are groups all over the world and, and, and kind of all, um, in every country that, that bring different.
[01:26:17.080 --> 01:26:17.840]   Values.
[01:26:17.840 --> 01:26:23.780]   Um, so it's, it's just a very, it's a very active debate and I, and I understand it, right?
[01:26:23.780 --> 01:26:32.840]   I mean, these are, you know, these, these kind of questions get to really some of the most important social debates that, that, that are, that are being had.
[01:26:32.840 --> 01:26:46.840]   So, um, I think it's back to the question of truth because for a lot of these things, they haven't yet been hardened into a single truth and, um, society sort of trying to hash out what, um, you know, what we think, right?
[01:26:46.840 --> 01:26:47.040]   On.
[01:26:47.040 --> 01:26:59.580]   On, on, on certain issues, maybe in a few hundred years, everyone will look back and say, Hey, no, it wasn't an obvious that it should have been this, but you know, no, we're, we're kind of in the, in that meat grinder now and, you know, and, and, and working through that.
[01:26:59.580 --> 01:27:16.460]   So, um, so no, these are, these are all are all very complicated and, you know, some people raise concerns in good faith and just say, Hey, this is something that I want to flag for you to think about certain people.
[01:27:16.460 --> 01:27:17.000]   I.
[01:27:17.000 --> 01:27:26.540]   Certainly think like come at things with a somewhat of a more kind of punitive or vengeful view of like, I, like, I want you to do this thing.
[01:27:26.540 --> 01:27:30.480]   If you don't, then I'm going to try to make your life difficult and, and a lot of other ways.
[01:27:30.480 --> 01:27:37.380]   But like, I don't know, there, there's just, this is like, this is one of the most pressurized debates I think in society.
[01:27:37.380 --> 01:27:46.920]   So I, I just think that there are so many people in different forces that are trying to apply pressure from different sides that it's, I, I, I don't think you can make decisions based on trying to make people.
[01:27:46.960 --> 01:27:47.300]   Happy.
[01:27:47.300 --> 01:27:56.960]   I think you just have to do what you think is the right balance and accept that people are going to be upset no matter where you come out on that.
[01:27:56.960 --> 01:27:57.640]   Yeah.
[01:27:57.640 --> 01:27:59.480]   I like that pressurized debate.
[01:27:59.480 --> 01:28:03.480]   Uh, so how's your view of the freedom of speech evolved over the years?
[01:28:03.480 --> 01:28:16.920]   Um, and now with AI where the freedom might apply to them, not just to the humans, but to the, uh, the personal.
[01:28:17.260 --> 01:28:17.420]   Yeah.
[01:28:17.420 --> 01:28:27.680]   I mean, I, I've probably gotten a somewhat more nuanced view just because I think that there are, you know, I, I come at this, I'm obviously very pro freedom of expression, right?
[01:28:27.680 --> 01:28:34.140]   I don't think you build a service like this that gives people tools to express themselves unless you think that people expressing themselves at scale is a good thing.
[01:28:34.140 --> 01:28:34.360]   Right.
[01:28:34.360 --> 01:28:40.020]   So I, I didn't get into this to like, try to prevent people from, from expressing anything.
[01:28:40.020 --> 01:28:43.860]   I like want to give people tools so they can express as much as possible.
[01:28:43.860 --> 01:28:46.880]   And then I think that's, that's a good thing.
[01:28:46.880 --> 01:29:04.320]   I think it's become clear that there are certain categories of things that we've talked about that I think almost everyone accepts are, are bad and that no one wants and that they're, that are illegal, even in countries like the US where, you know, you have the, the first amendment that's very protective of, of, of enabling speech.
[01:29:04.320 --> 01:29:11.260]   It's like, you're still not allowed to, you know, do things that are gonna immediately incite violence or, you know, violate people's intellectual property or things like that.
[01:29:11.260 --> 01:29:15.180]   So there are those, but then there's also a very active core of.
[01:29:15.180 --> 01:29:16.840]   Just.
[01:29:16.840 --> 01:29:21.600]   Active disagreements in society where some people may think that something is true or false.
[01:29:21.600 --> 01:29:26.140]   The other side might think it's the opposite or just unsettled.
[01:29:26.140 --> 01:29:26.560]   Right.
[01:29:26.560 --> 01:29:46.540]   And, um, and those are some of the most difficult to, to, to kind of handle, like, like we've talked about, but, um, one of the lessons that I feel like I've learned is that a lot of times when you can, the best way to handle this stuff.
[01:29:46.800 --> 01:30:00.100]   More practically is not in terms of answering the question of, should this be allowed, but just like what, what is the best way to deal with someone being a jerk?
[01:30:00.100 --> 01:30:11.040]   Is the person basically just having a, a like repeat behavior of like causing a lot of, a lot of issues.
[01:30:11.040 --> 01:30:14.140]   Um, so looking at it more at, at that level.
[01:30:14.140 --> 01:30:16.760]   And it's effect on the broader community.
[01:30:16.760 --> 01:30:18.340]   It's health of the community, health of the state.
[01:30:18.340 --> 01:30:19.340]   Yeah.
[01:30:19.340 --> 01:30:31.520]   It's tricky though, because like, how do you know there could be people that have a very controversial viewpoint that turns out to have a positive long-term effect on the health of the community because it challenges the community.
[01:30:31.520 --> 01:30:32.260]   That's true.
[01:30:32.260 --> 01:30:32.920]   Absolutely.
[01:30:32.920 --> 01:30:33.520]   It's yeah.
[01:30:33.520 --> 01:30:36.260]   No, I think you, and I think you want to be careful about that.
[01:30:36.260 --> 01:30:39.100]   I'm not sure I'm expressing this very, very clearly.
[01:30:39.100 --> 01:30:46.720]   Um, because I, I certainly agree with your, your point there and my, my point isn't that we should not have.
[01:30:46.720 --> 01:30:49.260]   People on our services that are, that are, that are being controversial.
[01:30:49.260 --> 01:30:51.220]   That's, that's certainly not what I mean to say.
[01:30:51.220 --> 01:31:02.880]   Um, it's that often I think it's not just looking at a specific example of speech that it's most effective to, to, to handle this stuff.
[01:31:02.880 --> 01:31:09.260]   Um, and, and, and I, I think often you don't want to make specific binary decisions of kind of, this is allowed or this isn't.
[01:31:09.260 --> 01:31:14.100]   I mean, I, I, we talked about, you know, it's fact checking or, or Twitter's community voices thing.
[01:31:14.100 --> 01:31:15.300]   I think that that's another good example.
[01:31:15.300 --> 01:31:16.680]   It's like, it's not a question of.
[01:31:16.680 --> 01:31:18.520]   Is this allowed or not?
[01:31:18.520 --> 01:31:20.760]   It's just a question of adding more context to the thing.
[01:31:20.760 --> 01:31:22.640]   And I think that that's helpful.
[01:31:22.640 --> 01:31:30.440]   So in the context of AI, which is, is what you were asking about, and there are lots of ways that an AI can be helpful.
[01:31:30.440 --> 01:31:33.880]   You know, with, with an AI, it's, it's less about censorship, right?
[01:31:33.880 --> 01:31:39.140]   Because, and it's, it's more about what is the most productive answer to a question.
[01:31:39.140 --> 01:31:46.640]   Um, you know, there was one case study that I was reviewing with the team is someone asked, um,
[01:31:46.640 --> 01:31:48.640]   can you explain to me how to 3d print a gun?
[01:31:48.640 --> 01:31:49.640]   And one proposed response is like, no, I can't talk about that.
[01:31:49.640 --> 01:31:50.640]   Right.
[01:31:50.640 --> 01:31:51.640]   It's like basically just like shut it down immediately, which I think is, is some of what you see.
[01:31:51.640 --> 01:31:52.640]   It's like as a large language model, I'm not allowed to talk about, you know, whatever.
[01:31:52.640 --> 01:31:53.640]   Um, but there's another response, which is like, Hey, you know, I don't think that's a good idea.
[01:31:53.640 --> 01:31:54.640]   And a lot of countries, um, including the U S 3d printing industry, they're like, well,
[01:31:54.640 --> 01:31:55.640]   I don't think that's a good idea.
[01:31:55.640 --> 01:31:56.640]   And I think that's a good idea.
[01:31:56.640 --> 01:31:57.640]   And I think that's a good idea.
[01:31:57.640 --> 01:31:58.640]   And I think that's a good idea.
[01:31:58.640 --> 01:31:59.640]   And I think that's a good idea.
[01:31:59.640 --> 01:32:00.640]   And I think that's a good idea.
[01:32:00.640 --> 01:32:01.640]   And I think that's a good idea.
[01:32:01.640 --> 01:32:02.640]   And I think that's a good idea.
[01:32:02.640 --> 01:32:03.640]   And I think that's a good idea.
[01:32:03.640 --> 01:32:04.640]   And I think that's a good idea.
[01:32:04.640 --> 01:32:05.640]   And I think that's a good idea.
[01:32:05.640 --> 01:32:06.640]   And I think that's a good idea.
[01:32:06.640 --> 01:32:07.640]   And I think that's a good idea.
[01:32:07.640 --> 01:32:08.640]   And I think that's a good idea.
[01:32:08.640 --> 01:32:09.640]   And I think that's a good idea.
[01:32:09.640 --> 01:32:10.640]   And I think that's a good idea.
[01:32:10.640 --> 01:32:11.640]   And I think that's a good idea.
[01:32:11.640 --> 01:32:12.640]   And I think that's a good idea.
[01:32:12.640 --> 01:32:13.640]   And I think that's a good idea.
[01:32:13.640 --> 01:32:14.640]   And I think that's a good idea.
[01:32:14.640 --> 01:32:15.640]   And I think that's a good idea.
[01:32:15.640 --> 01:32:16.640]   And I think that's a good idea.
[01:32:16.640 --> 01:32:17.640]   And I think that's a good idea.
[01:32:17.640 --> 01:32:18.640]   And I think that's a good idea.
[01:32:18.640 --> 01:32:19.640]   And I think that's a good idea.
[01:32:19.640 --> 01:32:20.640]   And I think that's a good idea.
[01:32:20.640 --> 01:32:21.640]   And I think that's a good idea.
[01:32:21.640 --> 01:32:22.640]   And I think that's a good idea.
[01:32:22.640 --> 01:32:23.640]   And I think that's a good idea.
[01:32:23.640 --> 01:32:24.640]   And I think that's a good idea.
[01:32:24.640 --> 01:32:25.640]   And I think that's a good idea.
[01:32:25.640 --> 01:32:26.640]   And I think that's a good idea.
[01:32:26.640 --> 01:32:27.640]   And I think that's a good idea.
[01:32:27.640 --> 01:32:28.640]   And I think that's a good idea.
[01:32:28.640 --> 01:32:29.640]   And I think that's a good idea.
[01:32:29.640 --> 01:32:30.640]   And I think that's a good idea.
[01:32:30.640 --> 01:32:31.640]   And I think that's a good idea.
[01:32:31.640 --> 01:32:32.640]   And I think that's a good idea.
[01:32:32.640 --> 01:32:33.640]   And I think that's a good idea.
[01:32:33.640 --> 01:32:34.640]   And I think that's a good idea.
[01:32:34.640 --> 01:32:35.640]   And I think that's a good idea.
[01:32:35.640 --> 01:32:36.640]   And I think that's a good idea.
[01:32:36.640 --> 01:32:37.640]   And I think that's a good idea.
[01:32:37.640 --> 01:32:38.640]   And I think that's a good idea.
[01:32:38.640 --> 01:32:39.640]   And I think that's a good idea.
[01:32:39.640 --> 01:32:40.640]   And I think that's a good idea.
[01:32:40.640 --> 01:32:41.640]   And I think that's a good idea.
[01:32:41.640 --> 01:32:42.640]   And I think that's a good idea.
[01:32:42.640 --> 01:32:43.640]   And I think that's a good idea.
[01:32:43.640 --> 01:32:44.640]   And I think that's a good idea.
[01:32:44.640 --> 01:32:45.640]   And I think that's a good idea.
[01:32:45.640 --> 01:32:46.640]   And I think that's a good idea.
[01:32:46.640 --> 01:32:47.640]   And I think that's a good idea.
[01:32:47.640 --> 01:32:48.640]   And I think that's a good idea.
[01:32:48.640 --> 01:32:49.640]   And I think that's a good idea.
[01:32:49.640 --> 01:32:50.640]   And I think that's a good idea.
[01:32:50.640 --> 01:32:51.640]   And I think that's a good idea.
[01:32:51.640 --> 01:32:52.640]   And I think that's a good idea.
[01:32:52.640 --> 01:32:53.640]   And I think that's a good idea.
[01:32:53.640 --> 01:32:54.640]   And I think that's a good idea.
[01:32:54.640 --> 01:32:55.640]   And I think that's a good idea.
[01:32:55.640 --> 01:32:56.640]   And I think that's a good idea.
[01:32:56.640 --> 01:32:57.640]   And I think that's a good idea.
[01:32:57.640 --> 01:32:58.640]   And I think that's a good idea.
[01:32:58.640 --> 01:32:59.640]   And I think that's a good idea.
[01:32:59.640 --> 01:33:00.640]   And I think that's a good idea.
[01:33:00.640 --> 01:33:01.640]   And I think that's a good idea.
[01:33:01.640 --> 01:33:02.640]   And I think that's a good idea.
[01:33:02.640 --> 01:33:03.640]   And I think that's a good idea.
[01:33:03.640 --> 01:33:04.640]   And I think that's a good idea.
[01:33:04.640 --> 01:33:05.640]   And I think that's a good idea.
[01:33:05.640 --> 01:33:06.640]   And I think that's a good idea.
[01:33:06.640 --> 01:33:07.640]   And I think that's a good idea.
[01:33:07.640 --> 01:33:08.640]   And I think that's a good idea.
[01:33:08.640 --> 01:33:09.640]   And I think that's a good idea.
[01:33:09.640 --> 01:33:10.640]   And I think that's a good idea.
[01:33:10.640 --> 01:33:11.640]   And I think that's a good idea.
[01:33:11.640 --> 01:33:12.640]   And I think that's a good idea.
[01:33:12.640 --> 01:33:13.640]   And I think that's a good idea.
[01:33:13.640 --> 01:33:14.640]   And I think that's a good idea.
[01:33:14.640 --> 01:33:15.640]   And I think that's a good idea.
[01:33:15.640 --> 01:33:16.640]   And I think that's a good idea.
[01:33:16.640 --> 01:33:17.640]   And I think that's a good idea.
[01:33:17.640 --> 01:33:18.640]   And I think that's a good idea.
[01:33:18.640 --> 01:33:19.640]   And I think that's a good idea.
[01:33:19.640 --> 01:33:20.640]   And I think that's a good idea.
[01:33:20.640 --> 01:33:21.640]   And I think that's a good idea.
[01:33:21.640 --> 01:33:22.640]   And I think that's a good idea.
[01:33:22.640 --> 01:33:23.640]   And I think that's a good idea.
[01:33:23.640 --> 01:33:24.640]   And I think that's a good idea.
[01:33:24.640 --> 01:33:25.640]   And I think that's a good idea.
[01:33:25.640 --> 01:33:26.640]   And I think that's a good idea.
[01:33:26.640 --> 01:33:27.640]   And I think that's a good idea.
[01:33:27.640 --> 01:33:28.640]   And I think that's a good idea.
[01:33:28.640 --> 01:33:29.640]   And I think that's a good idea.
[01:33:29.640 --> 01:33:30.640]   And I think that's a good idea.
[01:33:30.640 --> 01:33:31.640]   And I think that's a good idea.
[01:33:31.640 --> 01:33:32.640]   And I think that's a good idea.
[01:33:32.640 --> 01:33:33.640]   And I think that's a good idea.
[01:33:33.640 --> 01:33:34.640]   And I think that's a good idea.
[01:33:34.640 --> 01:33:35.640]   And I think that's a good idea.
[01:33:35.640 --> 01:33:36.640]   And I think that's a good idea.
[01:33:36.640 --> 01:33:37.640]   And I think that's a good idea.
[01:33:37.640 --> 01:33:38.640]   And I think that's a good idea.
[01:33:38.640 --> 01:33:39.640]   And I think that's a good idea.
[01:33:39.640 --> 01:33:40.640]   And I think that's a good idea.
[01:33:40.640 --> 01:33:41.640]   And I think that's a good idea.
[01:33:41.640 --> 01:33:42.640]   And I think that's a good idea.
[01:33:42.640 --> 01:33:43.640]   And I think that's a good idea.
[01:33:43.640 --> 01:33:44.640]   And I think that's a good idea.
[01:33:44.640 --> 01:33:45.640]   And I think that's a good idea.
[01:33:45.640 --> 01:33:46.640]   And I think that's a good idea.
[01:33:46.640 --> 01:33:47.640]   And I think that's a good idea.
[01:33:47.640 --> 01:33:48.640]   And I think that's a good idea.
[01:33:48.640 --> 01:33:49.640]   And I think that's a good idea.
[01:33:49.640 --> 01:33:50.640]   And I think that's a good idea.
[01:33:50.640 --> 01:33:51.640]   And I think that's a good idea.
[01:33:51.640 --> 01:33:52.640]   And I think that's a good idea.
[01:33:52.640 --> 01:33:53.640]   And I think that's a good idea.
[01:33:53.640 --> 01:33:54.640]   And I think that's a good idea.
[01:33:54.640 --> 01:33:55.640]   And I think that's a good idea.
[01:33:55.640 --> 01:33:56.640]   And I think that's a good idea.
[01:33:56.640 --> 01:33:57.640]   And I think that's a good idea.
[01:33:57.640 --> 01:33:58.640]   And I think that's a good idea.
[01:33:58.640 --> 01:33:59.640]   And I think that's a good idea.
[01:33:59.640 --> 01:34:00.640]   And I think that's a good idea.
[01:34:00.640 --> 01:34:01.640]   And I think that's a good idea.
[01:34:01.640 --> 01:34:02.640]   And I think that's a good idea.
[01:34:02.640 --> 01:34:03.640]   And I think that's a good idea.
[01:34:03.640 --> 01:34:04.640]   And I think that's a good idea.
[01:34:04.640 --> 01:34:05.640]   And I think that's a good idea.
[01:34:05.640 --> 01:34:06.640]   And I think that's a good idea.
[01:34:06.640 --> 01:34:07.640]   And I think that's a good idea.
[01:34:07.640 --> 01:34:08.640]   And I think that's a good idea.
[01:34:08.640 --> 01:34:09.640]   And I think that's a good idea.
[01:34:09.640 --> 01:34:10.640]   And I think that's a good idea.
[01:34:10.640 --> 01:34:11.640]   And I think that's a good idea.
[01:34:11.640 --> 01:34:12.640]   And I think that's a good idea.
[01:34:12.640 --> 01:34:13.640]   And I think that's a good idea.
[01:34:13.640 --> 01:34:14.640]   And I think that's a good idea.
[01:34:14.640 --> 01:34:15.640]   And I think that's a good idea.
[01:34:15.640 --> 01:34:16.640]   And I think that's a good idea.
[01:34:16.640 --> 01:34:17.640]   And I think that's a good idea.
[01:34:17.640 --> 01:34:18.640]   And I think that's a good idea.
[01:34:18.640 --> 01:34:19.640]   And I think that's a good idea.
[01:34:19.640 --> 01:34:20.640]   And I think that's a good idea.
[01:34:20.640 --> 01:34:21.640]   And I think that's a good idea.
[01:34:21.640 --> 01:34:30.320]   And I think that it's possible that something that melds some of those ideas with the graph
[01:34:30.320 --> 01:34:35.480]   and identity system that people have already cultivated on Instagram could be a kind of
[01:34:35.480 --> 01:34:38.840]   very welcome contribution to that space.
[01:34:38.840 --> 01:34:41.320]   But I know we work on a lot of things all the time though too.
[01:34:41.320 --> 01:34:43.880]   So I don't want to get ahead of myself.
[01:34:43.880 --> 01:34:46.880]   We have projects that explore a lot of different things.
[01:34:46.880 --> 01:34:50.140]   And this is certainly one that I think could be interesting.
[01:34:50.140 --> 01:34:51.140]   But-
[01:34:51.140 --> 01:34:54.820]   - What's the release, the launch date of that again?
[01:34:54.820 --> 01:34:57.380]   Or what's the official website and-
[01:34:57.380 --> 01:35:00.440]   - Well, we don't have that yet.
[01:35:00.440 --> 01:35:05.260]   But I, and look, I mean, I don't know exactly how this is gonna turn out.
[01:35:05.260 --> 01:35:08.940]   I mean, what I can say is, yeah, there's some people working on this, right?
[01:35:08.940 --> 01:35:13.020]   I think that there's something there that's interesting to explore.
[01:35:13.020 --> 01:35:18.060]   - So if you look at, it'd be interesting to just ask this question and throw Twitter into
[01:35:18.060 --> 01:35:19.060]   the mix.
[01:35:19.060 --> 01:35:20.640]   It's a landscape of social networking.
[01:35:20.640 --> 01:35:28.240]   What are the social networks that is Facebook, that is Instagram, that is WhatsApp?
[01:35:28.240 --> 01:35:31.300]   And then think of a text-based social network.
[01:35:31.300 --> 01:35:35.220]   When you look at that landscape, what are the interesting differences to you?
[01:35:35.220 --> 01:35:38.220]   Why do we have these different flavors?
[01:35:38.220 --> 01:35:39.880]   And what are the needs?
[01:35:39.880 --> 01:35:40.880]   What are the use cases?
[01:35:40.880 --> 01:35:41.940]   What are the products?
[01:35:41.940 --> 01:35:47.300]   What is the aspect of them that create a fulfilling human experience and a connection between
[01:35:47.300 --> 01:35:48.320]   humans that is somehow distinct?
[01:35:48.320 --> 01:35:49.320]   - Well, I think text is very important.
[01:35:49.320 --> 01:35:50.320]   I think it's very important.
[01:35:50.320 --> 01:35:55.700]   I think it's very accessible for people to transmit ideas and to have back and forth
[01:35:55.700 --> 01:35:58.120]   exchanges.
[01:35:58.120 --> 01:36:05.620]   So it, I think, ends up being a good format for discussion, in a lot of ways, uniquely
[01:36:05.620 --> 01:36:06.620]   good, right?
[01:36:06.620 --> 01:36:10.680]   If you look at some of the other formats or other networks that are focused on one type
[01:36:10.680 --> 01:36:13.560]   of content, like TikTok is obviously huge, right?
[01:36:13.560 --> 01:36:19.000]   And there are comments on TikTok, but, you know, I think the architecture of the service,
[01:36:19.000 --> 01:36:20.000]   you know, is very clear.
[01:36:20.000 --> 01:36:22.420]   clearly that you have the video as the primary thing
[01:36:22.420 --> 01:36:25.300]   and there's comments after that.
[01:36:25.300 --> 01:36:32.120]   But I think one of the unique pieces
[01:36:32.120 --> 01:36:35.880]   of having text-based comments, like content,
[01:36:35.880 --> 01:36:39.300]   is that the comments can also be first class
[01:36:39.300 --> 01:36:43.120]   and that makes it so that conversations can just filter
[01:36:43.120 --> 01:36:45.280]   and fork into all these different directions
[01:36:45.280 --> 01:36:47.720]   and in a way that can be super useful.
[01:36:47.720 --> 01:36:48.900]   So I think there's a lot of things
[01:36:48.900 --> 01:36:50.900]   that are really awesome about the experience.
[01:36:50.900 --> 01:36:52.740]   It just always struck me,
[01:36:52.740 --> 01:36:54.660]   I always thought that Twitter
[01:36:54.660 --> 01:36:56.360]   should have a billion people using it
[01:36:56.360 --> 01:36:57.460]   or whatever the thing is
[01:36:57.460 --> 01:37:01.680]   that basically ends up being in that space.
[01:37:01.680 --> 01:37:03.960]   And for whatever combination of reasons,
[01:37:03.960 --> 01:37:07.660]   again, these companies are complex organisms
[01:37:07.660 --> 01:37:10.880]   and it's very hard to diagnose this stuff from the outside.
[01:37:10.880 --> 01:37:15.880]   - Why doesn't Twitter, why doesn't a text-based comment
[01:37:15.880 --> 01:37:18.880]   as a first citizen-based social network
[01:37:18.880 --> 01:37:20.560]   have a billion users?
[01:37:20.560 --> 01:37:23.040]   - Well, I just think it's hard to build these companies.
[01:37:23.040 --> 01:37:27.780]   So it's not that every idea automatically goes
[01:37:27.780 --> 01:37:29.080]   and gets a billion people.
[01:37:29.080 --> 01:37:30.920]   It's just that I think that that idea,
[01:37:30.920 --> 01:37:34.220]   coupled with good execution, should get there.
[01:37:34.220 --> 01:37:37.860]   But I mean, look, we hit certain thresholds over time
[01:37:37.860 --> 01:37:41.840]   where we kind of plateaued early on
[01:37:41.840 --> 01:37:43.200]   and it wasn't clear that we were ever gonna reach
[01:37:43.200 --> 01:37:44.900]   a hundred million people on Facebook.
[01:37:44.900 --> 01:37:48.880]   And then we got really good at dialing in international,
[01:37:48.880 --> 01:37:52.260]   and helping the service grow in different countries.
[01:37:52.260 --> 01:37:55.480]   And that was like a whole competence
[01:37:55.480 --> 01:37:56.480]   that we needed to develop.
[01:37:56.480 --> 01:38:00.620]   And helping people basically spread the service
[01:38:00.620 --> 01:38:01.600]   to their friends.
[01:38:01.600 --> 01:38:03.780]   That was one of the things, once we got very good at that,
[01:38:03.780 --> 01:38:05.900]   that was one of the things that made me feel like,
[01:38:05.900 --> 01:38:08.760]   hey, if Instagram joined us early on,
[01:38:08.760 --> 01:38:10.300]   then I felt like we could help grow that quickly.
[01:38:10.300 --> 01:38:11.420]   And same with WhatsApp.
[01:38:11.420 --> 01:38:13.540]   And I think that that's sort of been a core competence
[01:38:13.540 --> 01:38:16.200]   that we've developed and been able to execute on.
[01:38:16.200 --> 01:38:17.040]   And others have too, right?
[01:38:17.040 --> 01:38:18.860]   I mean, ByteDance obviously have done,
[01:38:18.860 --> 01:38:20.480]   a very good job with TikTok
[01:38:20.480 --> 01:38:23.920]   and have reached more than a billion people there.
[01:38:23.920 --> 01:38:26.700]   But it's certainly not automatic, right?
[01:38:26.700 --> 01:38:31.080]   I think you need a certain level of execution
[01:38:31.080 --> 01:38:32.000]   to basically get there.
[01:38:32.000 --> 01:38:34.480]   And I think for whatever reason,
[01:38:34.480 --> 01:38:36.260]   I think Twitter has this great idea
[01:38:36.260 --> 01:38:38.360]   and sort of magic in the service,
[01:38:38.360 --> 01:38:44.000]   but they just haven't kind of cracked that piece yet.
[01:38:44.000 --> 01:38:45.380]   And I think that that's made it
[01:38:45.380 --> 01:38:46.900]   so that you're seeing all these other things,
[01:38:46.900 --> 01:38:48.000]   whether it's Mastodon,
[01:38:48.000 --> 01:38:51.460]   or Blue Sky,
[01:38:51.460 --> 01:38:54.400]   that I think are maybe just different cuts
[01:38:54.400 --> 01:38:55.240]   at the same thing.
[01:38:55.240 --> 01:38:57.300]   But I think through the last generation
[01:38:57.300 --> 01:39:00.180]   of social media overall,
[01:39:00.180 --> 01:39:01.540]   one of the interesting experiments
[01:39:01.540 --> 01:39:04.320]   that I think should get run at larger scale
[01:39:04.320 --> 01:39:05.160]   is what happens
[01:39:05.160 --> 01:39:07.140]   if there's somewhat more decentralized control.
[01:39:07.140 --> 01:39:10.240]   And if it's like the stack is more open throughout.
[01:39:10.240 --> 01:39:13.420]   And I've just been pretty fascinated by that
[01:39:13.420 --> 01:39:14.720]   and seeing how that works.
[01:39:14.720 --> 01:39:17.360]   To some degree,
[01:39:17.360 --> 01:39:20.440]   end-to-end encryption on WhatsApp,
[01:39:20.440 --> 01:39:22.480]   and as we bring it to other services,
[01:39:22.480 --> 01:39:23.600]   provides an element of it
[01:39:23.600 --> 01:39:26.880]   because it pushes the service really out to the edges.
[01:39:26.880 --> 01:39:31.880]   I mean, the server part of this that we run for WhatsApp
[01:39:31.880 --> 01:39:33.400]   is relatively very thin
[01:39:33.400 --> 01:39:36.320]   compared to what we do on Facebook or Instagram.
[01:39:36.320 --> 01:39:38.460]   And much more of the complexity is
[01:39:38.460 --> 01:39:40.920]   in how the apps kind of negotiate with each other
[01:39:40.920 --> 01:39:44.420]   to pass information in a fully end-to-end encrypted way.
[01:39:44.420 --> 01:39:47.360]   But I don't know, I think that that's, that is a good,
[01:39:47.360 --> 01:39:48.200]   is a good model.
[01:39:48.200 --> 01:39:50.180]   I think it puts more power in individual's hands
[01:39:50.180 --> 01:39:51.780]   and there are a lot of benefits of it
[01:39:51.780 --> 01:39:53.420]   if you can make it happen.
[01:39:53.420 --> 01:39:55.460]   Again, this is all like pretty speculative.
[01:39:55.460 --> 01:39:58.760]   I mean, I think that it's hard from the outside
[01:39:58.760 --> 01:40:01.180]   to know why anything does or doesn't work
[01:40:01.180 --> 01:40:02.880]   until you kind of take a run at it.
[01:40:02.880 --> 01:40:07.360]   And so I think it's kind of an interesting thing
[01:40:07.360 --> 01:40:08.220]   to experiment with,
[01:40:08.220 --> 01:40:10.840]   but I don't really know where this one's gonna go.
[01:40:10.840 --> 01:40:13.020]   - So since we were talking about Twitter,
[01:40:14.400 --> 01:40:19.400]   Elon Musk had what I think a few harsh words
[01:40:19.400 --> 01:40:21.120]   that I wish he didn't say.
[01:40:21.120 --> 01:40:26.120]   So let me ask, in the hope and the name of camaraderie,
[01:40:26.120 --> 01:40:29.100]   what do you think Elon is doing well with Twitter?
[01:40:29.100 --> 01:40:33.080]   And what, as a person who has run for a long time,
[01:40:33.080 --> 01:40:38.080]   you, social networks, Facebook, Instagram, WhatsApp,
[01:40:38.080 --> 01:40:41.640]   what can he do better?
[01:40:41.640 --> 01:40:44.280]   What can he improve on that text-based social network?
[01:40:44.280 --> 01:40:46.760]   - Gosh, it's always very difficult
[01:40:46.760 --> 01:40:50.160]   to offer specific critiques from the outside
[01:40:50.160 --> 01:40:51.560]   before you get into this,
[01:40:51.560 --> 01:40:53.900]   because I think one thing that I've learned
[01:40:53.900 --> 01:40:57.540]   is that everyone has opinions on what you should do.
[01:40:57.540 --> 01:40:59.980]   And like running the company,
[01:40:59.980 --> 01:41:02.980]   you see a lot of specific nuances on things
[01:41:02.980 --> 01:41:04.620]   that are not apparent externally.
[01:41:04.620 --> 01:41:09.620]   And I often think that some of the discourse around us
[01:41:09.620 --> 01:41:13.600]   would be, could be better than the other.
[01:41:13.600 --> 01:41:17.760]   Could be better if there was more kind of space
[01:41:17.760 --> 01:41:19.400]   for acknowledging that there's certain things
[01:41:19.400 --> 01:41:21.520]   that we're seeing internally that guide what we're doing.
[01:41:21.520 --> 01:41:23.560]   But I don't know.
[01:41:23.560 --> 01:41:26.340]   I mean, since you asked what is going well,
[01:41:26.340 --> 01:41:38.480]   I do think that Elon led a push early on
[01:41:38.480 --> 01:41:41.080]   to make Twitter a lot leaner.
[01:41:41.080 --> 01:41:45.080]   And I think that that,
[01:41:45.080 --> 01:41:48.820]   it's like you can agree or disagree
[01:41:48.820 --> 01:41:51.620]   with exactly all the tactics and how he did that.
[01:41:51.620 --> 01:41:55.360]   Obviously, every leader has their own style
[01:41:55.360 --> 01:41:58.500]   for if you need to make dramatic changes for that,
[01:41:58.500 --> 01:41:59.900]   how you're gonna execute it.
[01:41:59.900 --> 01:42:04.580]   But a lot of the specific principles that he pushed on
[01:42:04.580 --> 01:42:09.400]   around basically trying to make the organization
[01:42:09.400 --> 01:42:12.800]   more technical, around decreasing the distance
[01:42:12.800 --> 01:42:16.340]   between engineers at the company and him,
[01:42:16.340 --> 01:42:18.020]   like fewer layers of management.
[01:42:18.020 --> 01:42:23.300]   I think that those were generally good changes.
[01:42:23.300 --> 01:42:26.240]   And I'm also, I also think that it was probably good
[01:42:26.240 --> 01:42:28.640]   for the industry that he made those changes
[01:42:28.640 --> 01:42:30.640]   because my sense is that there were a lot of other people
[01:42:30.640 --> 01:42:33.240]   who thought that those were good changes,
[01:42:33.240 --> 01:42:38.240]   but who may have been a little shy about doing them
[01:42:38.240 --> 01:42:43.240]   and I think he, and just in my conversations
[01:42:43.240 --> 01:42:46.140]   with other founders and how people have reacted
[01:42:46.140 --> 01:42:47.460]   to the things that we've done,
[01:42:47.460 --> 01:42:49.940]   what I've heard from a lot of folks is just,
[01:42:49.940 --> 01:42:52.040]   hey, when someone like you,
[01:42:52.040 --> 01:42:55.640]   when I wrote the letter outlining the organizational changes
[01:42:55.640 --> 01:42:58.040]   that I wanted to make back in March,
[01:42:58.040 --> 01:43:00.240]   and when people see what Elon is doing,
[01:43:00.240 --> 01:43:05.240]   I think that that gives people the ability to think through
[01:43:05.240 --> 01:43:07.940]   how to shape their organizations.
[01:43:07.940 --> 01:43:12.940]   And in a way that hopefully can be good for the industry
[01:43:12.940 --> 01:43:16.080]   and make all these companies more productive over time.
[01:43:16.080 --> 01:43:19.480]   So something that that was one where I think he was
[01:43:19.480 --> 01:43:23.620]   quite ahead of a bunch of the other companies on.
[01:43:23.620 --> 01:43:26.760]   And what he was doing there,
[01:43:26.760 --> 01:43:28.240]   and again, from the outside, very hard to know.
[01:43:28.240 --> 01:43:29.820]   It's like, okay, did he cut too much?
[01:43:29.820 --> 01:43:30.660]   Did he not cut enough?
[01:43:30.660 --> 01:43:31.500]   Whatever.
[01:43:31.500 --> 01:43:35.020]   I don't think it's like my place to opine on that.
[01:43:35.020 --> 01:43:37.640]   And you asked for a positive framing of the question,
[01:43:37.640 --> 01:43:40.920]   of what do I admire?
[01:43:40.920 --> 01:43:42.020]   What do I think went well?
[01:43:42.020 --> 01:43:46.020]   But I think that like certainly his actions led me
[01:43:46.020 --> 01:43:49.840]   and I think a lot of other folks in the industry
[01:43:49.840 --> 01:43:53.660]   to think about, hey, are we kind of doing this
[01:43:53.660 --> 01:43:54.720]   as much as we should?
[01:43:54.720 --> 01:43:57.180]   Like, could we make our companies better
[01:43:57.180 --> 01:43:59.440]   by pushing on some of these same principles?
[01:43:59.440 --> 01:44:02.020]   - Well, the two of you are on the top of the world
[01:44:02.020 --> 01:44:04.040]   in terms of leading the development of tech.
[01:44:04.040 --> 01:44:07.340]   And I wish there was more both way,
[01:44:07.340 --> 01:44:11.880]   camaraderie and kindness, more love in the world,
[01:44:11.880 --> 01:44:13.440]   because love is the answer.
[01:44:13.440 --> 01:44:19.220]   But let me ask on a point of efficiency.
[01:44:19.220 --> 01:44:22.620]   You recently announced multiple stages of layoffs at Meta.
[01:44:22.620 --> 01:44:27.620]   What are the most painful aspects of this process,
[01:44:27.620 --> 01:44:30.520]   given for the individuals,
[01:44:30.520 --> 01:44:32.880]   the painful effects it has on those people's lives?
[01:44:32.880 --> 01:44:34.080]   - Yeah, I mean, that's it.
[01:44:34.080 --> 01:44:35.960]   That's it.
[01:44:37.040 --> 01:44:41.300]   And you basically have a significant number of people
[01:44:41.300 --> 01:44:45.700]   who this is just not the end of their time at Meta
[01:44:45.700 --> 01:44:49.860]   that they or I would have hoped for
[01:44:49.860 --> 01:44:51.260]   when they joined the company.
[01:44:51.260 --> 01:44:56.240]   And I mean, running a company,
[01:44:56.240 --> 01:45:00.280]   people are constantly joining and leaving the company
[01:45:00.280 --> 01:45:04.060]   for different directions, but for different reasons.
[01:45:06.740 --> 01:45:08.740]   And I think that's the most challenging part
[01:45:08.740 --> 01:45:09.580]   of the process.
[01:45:09.580 --> 01:45:11.080]   I think it's the most challenging part
[01:45:11.080 --> 01:45:14.080]   because it's a very uniquely challenging and tough
[01:45:14.080 --> 01:45:17.040]   in that you have a lot of people leaving
[01:45:17.040 --> 01:45:21.040]   for reasons that aren't connected to their own performance
[01:45:21.040 --> 01:45:25.040]   or the culture not being a fit at that point.
[01:45:25.040 --> 01:45:30.040]   It's really just, it's a kind of strategy decision
[01:45:30.040 --> 01:45:32.200]   and sometimes financially required,
[01:45:32.200 --> 01:45:35.440]   but not fully.
[01:45:35.440 --> 01:45:36.440]   And in our case, I mean, especially on the changes that we made this year, a lot of it was,
[01:45:36.440 --> 01:45:38.200]   I think, it was more kind of culturally
[01:45:38.200 --> 01:45:40.560]   and strategically driven by this push
[01:45:40.560 --> 01:45:44.440]   where I wanted us to become a stronger technology company
[01:45:44.440 --> 01:45:47.620]   with more of a focus on building more technical
[01:45:47.620 --> 01:45:52.240]   and more of a focus on building higher quality products faster.
[01:45:52.240 --> 01:45:56.040]   And I just view the external world is quite volatile right now.
[01:45:56.040 --> 01:46:00.380]   And I wanted to make sure that we had a stable position
[01:46:00.380 --> 01:46:03.720]   to be able to continue investing in these long-term,
[01:46:03.720 --> 01:46:06.140]   ambitious projects that we have around, you know,
[01:46:06.140 --> 01:46:07.680]   continuing to push AI forward
[01:46:07.680 --> 01:46:10.340]   and continuing to push forward all the metaverse work.
[01:46:10.340 --> 01:46:15.140]   And in order to do that in light of the pretty big thrash
[01:46:15.140 --> 01:46:17.980]   that we had seen over the last 18 months,
[01:46:17.980 --> 01:46:21.520]   you know, some of it, you know, macroeconomic induced,
[01:46:21.520 --> 01:46:24.180]   some of it specific, some of it competitively induced,
[01:46:24.180 --> 01:46:27.380]   some of it just because of bad decisions, right?
[01:46:27.380 --> 01:46:28.900]   Or things that we got wrong.
[01:46:28.900 --> 01:46:32.760]   I don't know, I just, I decided that we needed to get to a point
[01:46:32.760 --> 01:46:35.840]   where we were a lot leaner and, but look, I mean, but then, you know,
[01:46:35.840 --> 01:46:37.860]   but then, okay, it's one thing to do that,
[01:46:37.860 --> 01:46:39.780]   to like decide that at a high level,
[01:46:39.780 --> 01:46:41.400]   then the question is how do you execute that
[01:46:41.400 --> 01:46:42.900]   as compassionately as possible?
[01:46:42.900 --> 01:46:44.460]   And there's no good way.
[01:46:44.460 --> 01:46:47.460]   There's no perfect way for sure.
[01:46:47.460 --> 01:46:49.400]   And it's gonna be tough no matter what,
[01:46:49.400 --> 01:46:53.500]   but I, you know, as a leadership team here,
[01:46:53.500 --> 01:46:55.300]   we've certainly spent a lot of time just thinking,
[01:46:55.300 --> 01:46:58.460]   "Okay, given that this is a thing that sucks,
[01:46:58.460 --> 01:47:02.000]   like what is the most compassionate way that we can do this?"
[01:47:02.000 --> 01:47:05.280]   And that's what we've tried to do.
[01:47:05.280 --> 01:47:10.220]   - And you mentioned there's an increased focus on engineering,
[01:47:10.220 --> 01:47:14.740]   on tech, so technology teams, tech-focused teams,
[01:47:14.740 --> 01:47:17.920]   on building products, that.
[01:47:17.920 --> 01:47:19.400]   - Yeah, I mean, I wanted to,
[01:47:19.400 --> 01:47:25.760]   I want to empower engineers more,
[01:47:25.760 --> 01:47:28.460]   the people who are building things, the technical teams.
[01:47:28.460 --> 01:47:31.300]   Part of that is making sure that the people who are building things aren't just, you know,
[01:47:31.300 --> 01:47:31.340]   part of that is making sure that the people who are building things aren't just,
[01:47:31.340 --> 01:47:31.400]   part of that is making sure that the people who are building things aren't just,
[01:47:31.400 --> 01:47:31.420]   part of that is making sure that the people who are building things aren't just,
[01:47:31.420 --> 01:47:31.460]   part of that is making sure that the people who are building things aren't just,
[01:47:31.460 --> 01:47:31.520]   part of that is making sure that the people who are building things aren't just,
[01:47:31.520 --> 01:47:31.540]   part of that is making sure that the people who are building things aren't just,
[01:47:31.540 --> 01:47:35.300]   part of that is making sure that the people who are building things aren't just,
[01:47:35.300 --> 01:47:37.300]   at like the leaf nodes of the organization.
[01:47:37.300 --> 01:47:40.500]   I don't want like, you know, eight levels of management,
[01:47:40.500 --> 01:47:42.660]   and then the people actually doing the work.
[01:47:42.660 --> 01:47:46.500]   So we made changes to make it so that you have individual contributor engineers
[01:47:46.500 --> 01:47:48.820]   reporting at almost every level up the stack,
[01:47:48.820 --> 01:47:50.260]   which I think is important because, you know,
[01:47:50.260 --> 01:47:52.100]   you're running a company, one of the big questions is,
[01:47:52.100 --> 01:47:55.780]   you know, latency of information that you get.
[01:47:55.780 --> 01:48:00.180]   You know, we talked about this a bit earlier in terms of kind of the joy of
[01:48:00.180 --> 01:48:07.220]   the feedback that you get doing something like jujitsu compared to running a long-term project.
[01:48:07.220 --> 01:48:10.820]   But I actually think part of the art of running a company is trying to
[01:48:10.820 --> 01:48:14.900]   constantly re-engineer it so that your feedback loops get shorter,
[01:48:14.900 --> 01:48:16.020]   so you can learn faster.
[01:48:16.020 --> 01:48:18.260]   And part of the way that you do that is by,
[01:48:18.260 --> 01:48:20.980]   I kind of think that every layer that you have in the organization
[01:48:20.980 --> 01:48:27.220]   means that information might not need to get reviewed before it goes to you.
[01:48:27.220 --> 01:48:29.940]   And I think, you know, making it so that the people doing the work are as
[01:48:29.940 --> 01:48:34.260]   close as possible to you as possible is pretty important.
[01:48:34.260 --> 01:48:34.820]   So there's that.
[01:48:34.820 --> 01:48:41.220]   And I think over time, companies just build up very large support functions that are not
[01:48:41.220 --> 01:48:43.140]   doing the kind of core technical work.
[01:48:43.140 --> 01:48:47.300]   And those functions are very important, but I think having them in the right proportion
[01:48:47.300 --> 01:48:48.260]   is important.
[01:48:48.260 --> 01:48:54.020]   And if you try to do good work, but you don't have, you know,
[01:48:54.020 --> 01:48:59.700]   the right marketing team or the right legal advice, like you're going to,
[01:48:59.700 --> 01:49:01.460]   you know, make some pretty big blunders.
[01:49:01.460 --> 01:49:09.380]   But at the same time, if you have, you know, if you just like have too big of things and
[01:49:09.380 --> 01:49:14.820]   some of these support roles, then that might make it so that things are just move a lot.
[01:49:14.820 --> 01:49:22.020]   Maybe you're too conservative or you move a lot slower than you should otherwise.
[01:49:22.020 --> 01:49:23.620]   I just use those just examples.
[01:49:23.620 --> 01:49:25.220]   But it's, but-
[01:49:25.220 --> 01:49:26.740]   How do you find that balance?
[01:49:26.740 --> 01:49:27.300]   It's really tough.
[01:49:27.300 --> 01:49:29.460]   Yeah, no, but that's, it's a constant equilibrium.
[01:49:29.460 --> 01:49:31.060]   That you're, that you're searching for.
[01:49:31.060 --> 01:49:31.780]   Yeah.
[01:49:31.780 --> 01:49:33.380]   How many managers to have?
[01:49:33.380 --> 01:49:35.140]   What are the pros and cons of managers?
[01:49:35.140 --> 01:49:38.340]   Well, I mean, I believe a lot in management.
[01:49:38.340 --> 01:49:41.380]   I think there are some people who think that it doesn't matter as much, but look, I mean,
[01:49:41.380 --> 01:49:43.860]   we have a lot of younger people at the company for him.
[01:49:43.860 --> 01:49:48.420]   This is their first job and, you know, people need to grow and learn in their career and
[01:49:48.420 --> 01:49:52.180]   like that all that stuff is important, but here's one mathematical way to look at it.
[01:49:52.180 --> 01:49:59.220]   You know, at the beginning of this, we, I asked our
[01:49:59.220 --> 01:50:03.620]   people team, what was the average number of reports that a manager had?
[01:50:03.620 --> 01:50:09.300]   And I think it was, it was around three, maybe three to four, but closer to three.
[01:50:09.300 --> 01:50:16.020]   I was like, wow, like a manager can, you know, best practices that person can manage, you know,
[01:50:16.020 --> 01:50:16.820]   seven or eight people.
[01:50:16.820 --> 01:50:19.940]   But there was a reason why it was closer to three.
[01:50:19.940 --> 01:50:22.580]   It was because we were growing so quickly, right?
[01:50:22.580 --> 01:50:28.420]   And when you're hiring so many people so quickly, then that means that you need managers who have
[01:50:28.420 --> 01:50:28.980]   capacity to understand.
[01:50:28.980 --> 01:50:29.940]   To onboard new people.
[01:50:29.940 --> 01:50:34.740]   And also if you have a new manager, you may not want to have them have seven direct reports
[01:50:34.740 --> 01:50:36.740]   immediately because you want them to ramp up.
[01:50:36.740 --> 01:50:41.700]   But the thing is going forward, I don't want us to actually hire that many people that
[01:50:41.700 --> 01:50:42.500]   quickly, right?
[01:50:42.500 --> 01:50:46.260]   So I actually think we'll just do better work if we have more constraints and we're,
[01:50:46.260 --> 01:50:48.740]   you know, leaner as an organization.
[01:50:48.740 --> 01:50:54.100]   So in a world where we're not adding so many people as quickly, is it as valuable to have
[01:50:54.100 --> 01:50:56.820]   a lot of managers who have extra capacity waiting for new people?
[01:50:56.820 --> 01:50:57.540]   No, right?
[01:50:57.540 --> 01:51:03.780]   So now we can sort of defragment the organization and get to a place where the average is closer
[01:51:03.780 --> 01:51:04.660]   to that seven or eight.
[01:51:04.660 --> 01:51:11.380]   And it just ends up being a somewhat more kind of compact management structure, which,
[01:51:11.380 --> 01:51:17.060]   you know, decreases the latency on information going up and down the chain and I think empowers
[01:51:17.060 --> 01:51:17.540]   people more.
[01:51:17.540 --> 01:51:22.180]   But I mean, that's an example that I think it doesn't kind of undervalue the importance
[01:51:22.180 --> 01:51:26.340]   of management and the kind of the personal...
[01:51:26.340 --> 01:51:26.980]   Yeah.
[01:51:26.980 --> 01:51:30.420]   ...growth or coaching that people need in order to do their jobs well.
[01:51:30.420 --> 01:51:34.100]   It's just, I think, realistically, we're just not going to hire as many people going forward.
[01:51:34.100 --> 01:51:35.620]   So I think that you need a different structure.
[01:51:35.620 --> 01:51:42.020]   This whole incredible hierarchy and network of humans that make up a company is
[01:51:42.020 --> 01:51:42.660]   fascinating.
[01:51:42.660 --> 01:51:43.220]   Oh, yeah.
[01:51:43.220 --> 01:51:44.180]   Yeah.
[01:51:44.180 --> 01:51:47.620]   How do you hire great teams?
[01:51:47.620 --> 01:51:49.380]   How do you hire great...
[01:51:49.380 --> 01:51:56.340]   Now with the focus on engineering and technical teams, how do you hire great engineers and
[01:51:56.340 --> 01:51:56.660]   great...
[01:51:56.660 --> 01:51:58.180]   ...members of technical teams?
[01:51:58.180 --> 01:52:02.420]   Well, you're asking how you select or how you attract them?
[01:52:02.420 --> 01:52:04.820]   Both, but select, I think.
[01:52:04.820 --> 01:52:09.060]   I think attract is work on cool stuff and have a vision.
[01:52:09.060 --> 01:52:10.100]   I think the stuff we're talking about...
[01:52:10.100 --> 01:52:10.900]   I think that's right.
[01:52:10.900 --> 01:52:13.860]   And have a track record that people think you're actually going to be able to do it.
[01:52:13.860 --> 01:52:14.660]   Yeah.
[01:52:14.660 --> 01:52:19.620]   To me, the select seems like more of the art form, more of the tricky thing.
[01:52:19.620 --> 01:52:20.020]   Yeah.
[01:52:20.020 --> 01:52:26.020]   Do you select the people that fit the culture and can get integrated the most effectively
[01:52:26.020 --> 01:52:26.340]   and so on?
[01:52:26.340 --> 01:52:26.740]   Yeah.
[01:52:26.740 --> 01:52:35.540]   And maybe, especially when they're young, to see the magic through the resumes, through
[01:52:35.540 --> 01:52:39.300]   the paperwork and all this kind of stuff, to see that there's a special human there
[01:52:39.300 --> 01:52:41.860]   that would do incredible work.
[01:52:41.860 --> 01:52:42.740]   Yeah.
[01:52:42.740 --> 01:52:46.260]   So there are lots of different cuts on this question.
[01:52:46.260 --> 01:52:51.620]   I mean, I think when an organization is growing quickly, one of the big questions that teams
[01:52:51.620 --> 01:52:56.020]   face is, do I hire this person who's in front of me now because they seem...
[01:52:56.020 --> 01:52:56.420]   Yeah.
[01:52:56.420 --> 01:53:01.380]   ...good or do I hold out to get someone who's even better?
[01:53:01.380 --> 01:53:08.740]   And the heuristic that I always focused on for myself and my own kind of direct hiring
[01:53:08.740 --> 01:53:14.740]   that I think works when you recurse it through the organization is that you should only hire
[01:53:14.740 --> 01:53:19.220]   someone to be on your team if you would be happy working for them in an alternate universe.
[01:53:19.220 --> 01:53:19.780]   Yeah.
[01:53:19.780 --> 01:53:24.340]   And something that that kind of works and that's basically how I've tried to build my team.
[01:53:24.340 --> 01:53:24.500]   Yeah.
[01:53:24.500 --> 01:53:24.580]   It's...
[01:53:24.580 --> 01:53:25.060]   Yeah.
[01:53:25.060 --> 01:53:25.700]   You know, I'm not...
[01:53:26.340 --> 01:53:30.740]   I'm not in a rush to not be running the company, but I think in an alternate universe where one of
[01:53:30.740 --> 01:53:33.380]   these other folks was running the company, I'd be happy to work for them.
[01:53:33.380 --> 01:53:35.300]   I feel like I'd learn from them.
[01:53:35.300 --> 01:53:38.100]   I respect their kind of general judgment.
[01:53:38.100 --> 01:53:40.500]   They're all very insightful.
[01:53:40.500 --> 01:53:41.460]   They have good values.
[01:53:41.460 --> 01:53:46.180]   And I think that that gives you some rubric for...
[01:53:46.180 --> 01:53:48.740]   You can apply that at every layer.
[01:53:48.740 --> 01:53:52.340]   And I think if you apply that at every layer in the organization, then you'll have a pretty
[01:53:52.340 --> 01:53:53.300]   strong organization.
[01:53:53.300 --> 01:53:54.820]   Yeah.
[01:53:54.820 --> 01:53:55.860]   And I think that's a really good point.
[01:53:55.860 --> 01:53:56.260]   Yeah.
[01:53:56.260 --> 01:53:56.420]   I think that's a really good point.
[01:53:56.420 --> 01:53:56.740]   Okay.
[01:53:56.740 --> 01:54:00.340]   In an organization that's not growing as quickly, the questions might be a little different,
[01:54:00.340 --> 01:54:00.580]   though.
[01:54:00.580 --> 01:54:06.820]   And there, you asked about young people specifically, like people out of college.
[01:54:06.820 --> 01:54:14.820]   And one of the things that we see is it's a pretty basic lesson, but we have a much better
[01:54:14.820 --> 01:54:19.300]   sense of who the best people are who have interned at the company for a couple of months
[01:54:19.300 --> 01:54:24.740]   than by looking at them at kind of a resume or a short interview list.
[01:54:24.740 --> 01:54:28.380]   loop. I mean, obviously the in-person feel that you get from someone probably tells you
[01:54:28.380 --> 01:54:35.060]   more than the resume and you can do some basic skills assessment, but a lot of the stuff
[01:54:35.060 --> 01:54:42.620]   really just is cultural. People thrive in different environments and on different teams
[01:54:42.620 --> 01:54:48.980]   even within a specific company. And it's like the people who come for even a short period
[01:54:48.980 --> 01:54:54.140]   of time over a summer who do a great job here. You know that they're gonna be great if they
[01:54:54.140 --> 01:54:57.820]   came and joined full-time. And that's one of the reasons why we've invested so much
[01:54:57.820 --> 01:55:05.200]   in internship is basically it's a very useful sorting function both for us and for the people
[01:55:05.200 --> 01:55:06.980]   who wanna try out the company.
[01:55:06.980 --> 01:55:11.480]   - You mentioned in-person, what do you think about remote work? A topic that's been discussed
[01:55:11.480 --> 01:55:15.740]   extensively over the past few years because of the pandemic.
[01:55:15.740 --> 01:55:23.540]   - Yeah, I mean, it's a thing that's here to stay, but I think that there's value
[01:55:23.540 --> 01:55:31.460]   in both, right? It's not, you know, I wouldn't wanna run a fully remote company yet at least.
[01:55:31.460 --> 01:55:33.220]   I think there's an asterisk on that, which is that-
[01:55:33.220 --> 01:55:36.140]   - Some of the other stuff you're working on, yeah.
[01:55:36.140 --> 01:55:42.800]   - Yeah, exactly. It's like all the metaverse work and the ability to feel like you're truly
[01:55:42.800 --> 01:55:49.840]   present no matter where you are. I think once you have that all dialed in, then we may one
[01:55:49.840 --> 01:55:52.940]   day reach a point where it really just doesn't matter as much where you are physically. But,
[01:55:52.940 --> 01:56:05.300]   I don't know, today it still does, right? So yeah, for people who, there are all these people
[01:56:05.300 --> 01:56:10.820]   who have special skills and wanna live in a place where we don't have an office, are we better off
[01:56:10.820 --> 01:56:14.900]   having them at the company? Absolutely, right? And are a lot of people who work at the company
[01:56:14.900 --> 01:56:22.880]   for several years and then, you know, build up the relationships internally and kind of have the
[01:56:22.880 --> 01:56:26.960]   trust and have a sense of how the company works. Can they go work remotely now if they want and
[01:56:26.960 --> 01:56:30.380]   still do it as effectively? And we've done all these studies that show it's like, okay,
[01:56:30.380 --> 01:56:36.380]   does that affect their performance? It does not. But, you know, for the new folks who are joining
[01:56:36.380 --> 01:56:42.560]   and for people who are earlier in their career and, you know, need to learn how to solve certain
[01:56:42.560 --> 01:56:48.740]   problems and need to get ramped up on the culture, you know, when you're working through really
[01:56:48.740 --> 01:56:52.820]   complicated problems where you don't just wanna sit in the, you don't just want the formal meeting,
[01:56:52.820 --> 01:56:56.780]   but you wanna be able to like brainstorm when you're walking in the hallway together after
[01:56:56.780 --> 01:57:03.440]   the meeting. I don't know. It's like we just haven't replaced the kind of in-person
[01:57:03.440 --> 01:57:11.600]   dynamics there yet with anything remote yet. So. Yeah, there's a magic to the in-person that
[01:57:11.600 --> 01:57:16.100]   we'll talk about this a little bit more, but I'm really excited by the possibilities in the next
[01:57:16.100 --> 01:57:22.220]   two years in virtual reality and mixed reality that are possible with high resolution scans. I mean,
[01:57:22.760 --> 01:57:30.620]   I as a person who loves in-person interaction, like these podcasts in person, it would be incredible
[01:57:30.620 --> 01:57:37.700]   to achieve the level of realism I've gotten the chance to witness. But let me ask about that.
[01:57:37.700 --> 01:57:48.740]   Yeah. I got a chance to look at the Quest 3 headset, and it is amazing. You've announced it.
[01:57:50.720 --> 01:57:54.740]   You'll get some more details in the fall, maybe a release in the fall. When is it getting released
[01:57:54.740 --> 01:57:57.560]   again? I forgot you mentioned it. We'll give more details at Connect.
[01:57:57.560 --> 01:58:06.020]   Okay. But it's coming this fall. Okay. So it's priced at $499.
[01:58:06.020 --> 01:58:11.540]   What features are you most excited about there? There are basically two big new things that
[01:58:11.540 --> 01:58:20.660]   we've added to Quest 3 over Quest 2. The first is high resolution mixed reality. And the basic,
[01:58:20.660 --> 01:58:26.720]   basic idea here is that you can think about virtual reality as you have the headset and
[01:58:26.720 --> 01:58:31.280]   like all the pixels are virtual and you're basically like immersed in a different world.
[01:58:31.280 --> 01:58:36.680]   Mixed reality is where you see the physical world around you and you can place virtual
[01:58:36.680 --> 01:58:42.020]   objects in it, whether that's a screen to watch a movie or a projection of your virtual desktop,
[01:58:42.020 --> 01:58:46.340]   or you're playing a game where like zombies are coming out through the wall and you need
[01:58:46.340 --> 01:58:50.600]   to shoot them. Or, you know, we're, you know, we're playing Dungeons and Dragons or some board
[01:58:50.600 --> 01:58:54.200]   game and we just have a virtual version of the board in front of us while we're sitting here.
[01:58:54.200 --> 01:59:01.040]   All that's possible in mixed reality. And I think that that is going to be the next big capability
[01:59:01.040 --> 01:59:08.060]   on top of virtual reality. It has done so well. I have to say as a person who experienced it today
[01:59:08.060 --> 01:59:15.080]   with zombies, having a full awareness of the environment and integrating that environment
[01:59:15.080 --> 01:59:20.540]   in the way they run at you while they try to kill you. So it's, it's, it's just the mixed reality.
[01:59:20.540 --> 01:59:26.060]   The pass through is really, really, really well done. And the fact that it's only $500 is really,
[01:59:26.060 --> 01:59:34.460]   it's well done. Thank you. I'm super excited about it. I mean, our, and we put a lot of work into
[01:59:34.460 --> 01:59:41.480]   making the device both as good as possible and as affordable as possible, because a big part
[01:59:41.480 --> 01:59:46.340]   of our mission and ethos here is we, we, we want people to be able to connect with each other.
[01:59:46.340 --> 01:59:50.480]   We want to reach, and we want to serve a lot of people, right? We want to bring this technology to,
[01:59:50.480 --> 01:59:57.380]   to everyone, right? So we're not just trying to serve like a, you know, an elite, a wealthy crowd.
[01:59:57.380 --> 02:00:03.500]   We, we want to we, we really want this to be accessible. So that, that is in a lot of ways,
[02:00:03.500 --> 02:00:08.680]   an extremely hard technical problem because, you know, we don't just have the ability to put
[02:00:08.680 --> 02:00:12.420]   an unlimited amount of hardware in this. We needed to basically deliver something
[02:00:12.420 --> 02:00:17.660]   that works really well, but in an affordable package. And we started with quest pro last year.
[02:00:17.860 --> 02:00:25.600]   It was it's it's it was $1,500. And now we've, we've lowered the price to a thousand, but in a
[02:00:25.600 --> 02:00:31.900]   lot of ways, the mixed reality in quest three is an even better and more advanced level than what
[02:00:31.900 --> 02:00:36.840]   we were able to deliver in quest pro. So I'm, I'm really proud of where we are with, with with
[02:00:36.840 --> 02:00:41.100]   quest three on that. It's going to work with all of the virtual reality titles and everything
[02:00:41.100 --> 02:00:46.220]   that, that existed there. So people who want to play fully immersive games, social experiences,
[02:00:46.220 --> 02:00:47.840]   fitness, all that stuff. And we're going to be able to deliver that to a lot of people. So I'm
[02:00:47.840 --> 02:00:54.160]   stuff will work. But now you'll also get mixed reality too, which I think people really like
[02:00:54.160 --> 02:01:00.800]   because sometimes you want to be super immersed in a game, but a lot of the time, especially when
[02:01:00.800 --> 02:01:06.560]   you're moving around, if you're active, you're doing some fitness experience. Let's say you're
[02:01:06.560 --> 02:01:11.360]   doing boxing or something. You kind of want to be able to see the room around you so that way
[02:01:11.360 --> 02:01:16.080]   you know that I'm not going to punch a lamp or something like that. I don't know if you got to
[02:01:16.080 --> 02:01:19.920]   play with this experience, but we basically have the... I mean, it's just sort of like a fun little
[02:01:19.920 --> 02:01:25.760]   demo that we put together. But it's like you just, you know, you were like in a conference room or
[02:01:25.760 --> 02:01:30.720]   your living room and you have the guy there and you're boxing him and you're fighting him.
[02:01:30.720 --> 02:01:34.640]   All the other people are there too. I got a chance to do that. And all the people
[02:01:34.640 --> 02:01:40.640]   are there. It's like, that guy's right there. Yeah. It's like it's right in the room.
[02:01:40.640 --> 02:01:44.960]   And the other human, the path, you're seeing them also, they can cheer you on. They can make fun of
[02:01:44.960 --> 02:01:54.160]   you if they're anything like friends of mine. And then just it's really, it's a really compelling
[02:01:54.160 --> 02:01:58.880]   experience. I mean, VR is really interesting too, but this is something else almost. This
[02:01:58.880 --> 02:02:02.400]   becomes integrated into your life, into your world.
[02:02:02.400 --> 02:02:08.080]   Yeah. And so I think it's a completely new capability that will unlock a lot of different
[02:02:08.080 --> 02:02:12.960]   content. And I think it'll also just make the experience more comfortable for a set of people
[02:02:12.960 --> 02:02:13.840]   who didn't want to have only the experience. Yeah.
[02:02:13.840 --> 02:02:15.440]   And I think that's the thing that's really exciting about VR is that you can have only
[02:02:15.440 --> 02:02:18.720]   fully immersive experiences. I think if you want experiences where you're grounded in
[02:02:18.720 --> 02:02:22.880]   your living room and the physical world around you, now you'll be able to have that too.
[02:02:22.880 --> 02:02:27.920]   And I think that that's pretty exciting. I really liked how it added windows
[02:02:27.920 --> 02:02:30.640]   to a room with no windows. Yeah.
[02:02:30.640 --> 02:02:32.960]   Me as a person. Did you see the aquarium one where you
[02:02:32.960 --> 02:02:35.360]   could see the sharks swim up or was that just the zombie one?
[02:02:35.360 --> 02:02:39.360]   Just the zombie one, but it's still outside. You don't necessarily want windows added to
[02:02:39.360 --> 02:02:43.120]   your living room where zombies come out of, but yes, in the context of that game, it's yeah.
[02:02:43.120 --> 02:02:43.840]   Yeah. Yeah. Yeah. That's good.
[02:02:43.840 --> 02:02:49.120]   I enjoyed it because you could see the nature outside. And me as a person that doesn't have
[02:02:49.120 --> 02:02:52.800]   windows, it's just nice to have nature. Yeah. Well.
[02:02:52.800 --> 02:03:00.960]   Even if it's a mixed reality setting. I know it's a zombie game, but there's a zen nature,
[02:03:00.960 --> 02:03:06.320]   zen aspect to being able to look outside and alter your environment as you know it.
[02:03:06.320 --> 02:03:12.640]   Yeah. There will probably be better, more zen ways to do that than the zombie game.
[02:03:12.640 --> 02:03:13.200]   Yes. I mean, it's not the game you're
[02:03:13.200 --> 02:03:19.280]   describing, but you're right that the basic idea of sort of having your physical environment
[02:03:19.280 --> 02:03:26.720]   on pass through, but then being able to bring in different elements, I think it's going to be super
[02:03:26.720 --> 02:03:33.040]   powerful. And in some ways I think that these are... Mixed reality is also a predecessor to
[02:03:33.040 --> 02:03:38.240]   eventually we will get AR glasses that are not the goggles form factor of the current
[02:03:38.240 --> 02:03:41.280]   generation of headsets that people are making.
[02:03:41.280 --> 02:03:46.240]   But I think a lot of the experiences that developers are making for mixed reality of
[02:03:46.240 --> 02:03:50.640]   basically you just have a kind of a hologram that you're putting in the world will hopefully apply
[02:03:50.640 --> 02:03:55.760]   once we get the AR glasses too. Now that's got its own whole set of challenges and it's...
[02:03:55.760 --> 02:03:59.440]   Well, the headset's already smaller than the previous version.
[02:03:59.440 --> 02:04:03.600]   Oh yeah. It's 40% thinner. And the other thing that I think is good about it, it's... Yeah. So
[02:04:03.600 --> 02:04:10.800]   mixed reality was the first big thing. The second is it's just a great VR headset. I mean, it's got
[02:04:10.800 --> 02:04:19.120]   2X the graphics processing power, 40% sharper screens, 40% thinner, more comfortable, better
[02:04:19.120 --> 02:04:23.440]   strap architecture, all this stuff that if you liked Quest 2, I think that this is just going to
[02:04:23.440 --> 02:04:27.280]   be... It's like all the content that you might've played in Quest 2 is just going to get sharper
[02:04:27.280 --> 02:04:31.760]   automatically and look better in this. So it's... I think people are really going to like it. Yeah.
[02:04:31.760 --> 02:04:32.560]   So this fall.
[02:04:32.560 --> 02:04:40.560]   This fall. I have to ask, Apple just announced a mixed reality headset called Vision
[02:04:40.560 --> 02:04:47.120]   Pro for $3,500 available in early 2024. What do you think about this headset?
[02:04:47.120 --> 02:04:53.920]   Well, I saw the materials when they launched. I haven't gotten a chance to play with it yet. So
[02:04:53.920 --> 02:04:59.520]   take everything with a grain of salt, but a few high level thoughts. I mean, first,
[02:04:59.520 --> 02:05:10.320]   I do think that this is a certain level of validation for the category, right? Where,
[02:05:10.320 --> 02:05:16.720]   you know, we were the primary folks out there before saying, "Hey, I think that this, you know,
[02:05:16.720 --> 02:05:20.640]   virtual reality, augmented reality, mixed reality, this is going to be a big part of the next
[02:05:20.640 --> 02:05:32.400]   computing platform." I think having Apple come in and share that vision will make a lot of people
[02:05:32.400 --> 02:05:40.080]   who are fans of their products really consider that. And then, you know, of course the $35,000,
[02:05:40.080 --> 02:05:46.400]   $3,500 price, you know, on the one hand, I get it for with all the stuff that they're trying to pack
[02:05:46.400 --> 02:05:50.400]   in there. On the other hand, a lot of people aren't going to find that to be affordable.
[02:05:50.400 --> 02:05:57.520]   So I think that there's a chance that them coming in actually increases demand for the overall space
[02:05:57.520 --> 02:06:02.480]   and that Quest 3 is actually the primary beneficiary of that because a lot of the people
[02:06:02.480 --> 02:06:08.640]   who might say, "Hey, you know, this, I think I'm going to give another consideration to this." Or,
[02:06:08.640 --> 02:06:09.840]   you know, "Now I understand why this is so important to me." You know, I think that's a good thing.
[02:06:09.840 --> 02:06:09.920]   I think that's a good thing. I think that's a good thing. I think that's a good thing. I think
[02:06:09.920 --> 02:06:15.760]   maybe what mixed reality is more, and Quest 3 is the best one on the market that I can afford.
[02:06:15.760 --> 02:06:21.600]   And it's great also, right? I think that that's, you know, in our own way, I think we're,
[02:06:21.600 --> 02:06:23.840]   and there are a lot of features that we have where we're leading on.
[02:06:23.840 --> 02:06:32.000]   So I think that that's, that I think is going to be a very, that could be quite good. And then
[02:06:32.000 --> 02:06:36.880]   obviously over time, the companies are just focused on somewhat different things, right?
[02:06:36.880 --> 02:06:39.600]   Apple has always, you know, I think, you know, I think it's a good thing that they're doing this
[02:06:39.600 --> 02:06:49.200]   I think focused on building really kind of high-end things. Whereas our focus has been on,
[02:06:49.200 --> 02:06:53.040]   it's just, we have a more democratic ethos. We want to build things that are
[02:06:53.040 --> 02:06:59.680]   accessible to a wider number of people. You know, we've sold tens of millions of Quest devices.
[02:06:59.680 --> 02:07:06.240]   My understanding, just based on rumors, I don't have any special knowledge on this,
[02:07:06.240 --> 02:07:08.720]   is that Apple is building about 1 million of their
[02:07:09.360 --> 02:07:14.640]   of their device, right? So just in terms of like what you kind of expect in terms of sales numbers,
[02:07:14.640 --> 02:07:24.000]   I just think that this is, I mean, Quest is going to be the primary thing that people in the market
[02:07:24.000 --> 02:07:27.600]   will continue using for the foreseeable future. And then obviously over the long-term, it's up to
[02:07:27.600 --> 02:07:31.760]   the companies to see how well we executed the different things that we're doing. But we kind
[02:07:31.760 --> 02:07:36.880]   of come at it from different places. We're very focused on social interaction, communication,
[02:07:39.120 --> 02:07:43.360]   being more active, right? So there's fitness, there's gaming, there are those things.
[02:07:43.360 --> 02:07:51.360]   Whereas I think a lot of the use cases that you saw in Apple's launch material were more around
[02:07:51.360 --> 02:07:58.400]   people sitting, people looking at screens, which are great. I think that you will replace your
[02:07:58.400 --> 02:08:04.720]   laptop over time with a headset. But I think in terms of kind of how the different use cases that
[02:08:04.720 --> 02:08:08.880]   the companies are going after, they're a bit different for what they're doing. I think that's
[02:08:08.880 --> 02:08:12.880]   a good thing for where we are right now. Yeah. So gaming wasn't a big part of the
[02:08:12.880 --> 02:08:20.480]   presentation, which is interesting. It feels like mixed reality gaming is such a big part of that.
[02:08:20.480 --> 02:08:24.240]   It was interesting to see it missing in the presentation.
[02:08:24.240 --> 02:08:27.360]   Well, I mean, look, there are certain design trade-offs in this
[02:08:27.360 --> 02:08:33.280]   where, you know, they made this point about not wanting to have controllers,
[02:08:33.280 --> 02:08:38.640]   which on the one hand, there's a certain elegance about just being able to navigate the system with
[02:08:39.280 --> 02:08:44.640]   eye gaze and hand tracking. And by the way, you'll be able to just navigate Quest with your hands too,
[02:08:44.640 --> 02:08:51.120]   if that's what you want. Yeah. One of the things I should mention is that the capability from the
[02:08:51.120 --> 02:08:56.720]   cameras with computer vision to detect certain aspects of the hand, allowing you to have a
[02:08:56.720 --> 02:09:01.520]   controller that doesn't have that ring thing. Yeah. The hand tracking in Quest 3 and the
[02:09:01.520 --> 02:09:08.400]   controller tracking is a big step up from the last generation. And one of the demos that we have has
[02:09:08.400 --> 02:09:13.120]   basically an MR experience teaching you how to play piano, where it basically highlights the
[02:09:13.120 --> 02:09:16.960]   notes that you need to play. And it's like, it's just all, it's hands, it's no controllers. But
[02:09:16.960 --> 02:09:25.840]   I think if you care about gaming, having a controller allows you to have a more tactile feel
[02:09:25.840 --> 02:09:34.000]   and allows you to capture fine motor movement much more precisely than what you can do with
[02:09:34.000 --> 02:09:38.160]   hands without something that you're touching. So again, I think it's, there are certain questions,
[02:09:38.160 --> 02:09:45.600]   which are just around what use cases are you optimizing for? I think if you want to play games,
[02:09:45.600 --> 02:09:52.640]   then I think that you want to design the system in a different way. And we're more focused on kind of
[02:09:52.640 --> 02:09:59.840]   social experiences, entertainment experiences. Whereas if what you want is to make sure that
[02:09:59.840 --> 02:10:07.120]   the text that you read on a screen is as crisp as possible, then you need to make the design and
[02:10:07.120 --> 02:10:07.920]   cost trade-offs that you're doing. And that's something that I think is really important.
[02:10:07.920 --> 02:10:15.280]   So I think that there is a use case for that for sure. But I just think that
[02:10:15.280 --> 02:10:22.640]   the companies, we've basically made different design trade-offs to get to the use cases that
[02:10:22.640 --> 02:10:27.920]   we're trying to serve. There's a lot of other stuff I'd love to talk to you about,
[02:10:27.920 --> 02:10:33.200]   about the metaverse, especially the Kodak Avatar, which I've gotten to experience a
[02:10:33.200 --> 02:10:37.680]   lot of different variations of recently that I'm really, really excited about.
[02:10:37.680 --> 02:10:38.480]   To talk about that too.
[02:10:38.480 --> 02:10:46.800]   I'll have to wait a little bit because, well, I think there's a lot more to show off in that
[02:10:46.800 --> 02:10:51.920]   regard. But let me step back to AI. I think we've mentioned it a little bit, but
[02:10:51.920 --> 02:10:59.600]   I'd like to linger on this question that folks like Eliezer Yudkowsky has to worry about
[02:10:59.600 --> 02:11:06.960]   and others of the existential, of the serious threats of AI that have been reinvigorated now
[02:11:06.960 --> 02:11:07.440]   with the rapid-fire AI. And I think that's a really important question. And I think that's a really
[02:11:07.440 --> 02:11:09.920]   rapid developments of AI systems.
[02:11:09.920 --> 02:11:14.740]   Do you worry about the existential risks of AI
[02:11:14.740 --> 02:11:17.980]   as Eliezer does, about the alignment problem,
[02:11:17.980 --> 02:11:20.680]   about this getting out of hand?
[02:11:20.680 --> 02:11:23.140]   - Any time where there's a number of serious people
[02:11:23.140 --> 02:11:27.520]   who are raising a concern that is that existential
[02:11:27.520 --> 02:11:29.000]   about something that you're involved with,
[02:11:29.000 --> 02:11:30.840]   I think you have to think about it.
[02:11:30.840 --> 02:11:33.540]   Right, so, I've spent quite a bit of time
[02:11:33.540 --> 02:11:36.060]   thinking about it from that perspective.
[02:11:36.060 --> 02:11:41.060]   And the thing that I, where I basically have come out
[02:11:41.060 --> 02:11:44.660]   on this for now is I do think that there are,
[02:11:44.660 --> 02:11:47.540]   over time, I think that we need to think about this
[02:11:47.540 --> 02:11:52.160]   even more as we approach something that could be closer
[02:11:52.160 --> 02:11:53.060]   to super intelligence.
[02:11:53.060 --> 02:11:54.920]   I just think it's pretty clear to anyone working
[02:11:54.920 --> 02:11:58.060]   on these projects today that we're not there.
[02:11:58.060 --> 02:12:03.300]   And one of my concerns is that we spent a fair amount
[02:12:03.300 --> 02:12:04.620]   of time on this before,
[02:12:04.620 --> 02:12:06.040]   but there are a lot of people
[02:12:06.040 --> 02:12:11.040]   who are more, I don't know if mundane is the right word,
[02:12:11.040 --> 02:12:14.980]   but there's like concerns that already exist, right?
[02:12:14.980 --> 02:12:19.740]   About like people using AI tools to do harmful things
[02:12:19.740 --> 02:12:21.100]   of the type that we're already aware,
[02:12:21.100 --> 02:12:23.800]   whether we talked about fraud or scams
[02:12:23.800 --> 02:12:25.300]   or different things like that.
[02:12:25.300 --> 02:12:31.540]   And that's going to be a pretty big set of challenges
[02:12:31.540 --> 02:12:32.760]   that the companies working on this
[02:12:32.760 --> 02:12:35.180]   are gonna need to grapple with regardless of the,
[02:12:35.180 --> 02:12:38.120]   regardless of whether there is an existential concern
[02:12:38.120 --> 02:12:40.060]   as well at some point down the road.
[02:12:40.060 --> 02:12:44.160]   So I do worry that to some degree you can,
[02:12:44.160 --> 02:12:48.560]   people can get a little too focused on,
[02:12:48.560 --> 02:12:53.280]   on some of the tail risk and then not do as good of a job
[02:12:53.280 --> 02:12:56.640]   as we need to on the things that you are,
[02:12:56.640 --> 02:12:59.520]   can be almost certain are going to come down the pipe
[02:12:59.520 --> 02:13:03.900]   as real risks that kind of manifest themselves
[02:13:03.900 --> 02:13:05.160]   in the near term.
[02:13:05.160 --> 02:13:08.220]   - Yeah, I've spent most of my time on that
[02:13:08.220 --> 02:13:12.120]   once I kind of made the realization
[02:13:12.120 --> 02:13:15.160]   that the size of models that we're talking about now
[02:13:15.160 --> 02:13:17.820]   in terms of what we're building are quite far
[02:13:17.820 --> 02:13:20.040]   from the super intelligence type concerns
[02:13:20.040 --> 02:13:22.220]   that people raise.
[02:13:22.220 --> 02:13:24.840]   But I think once we get a couple steps closer to that,
[02:13:24.840 --> 02:13:28.140]   I know as we do get closer, I think that those,
[02:13:28.140 --> 02:13:32.540]   there are going to be some novel risks and issues
[02:13:32.540 --> 02:13:35.140]   about how we make sure that the systems are safe.
[02:13:35.140 --> 02:13:36.780]   For sure.
[02:13:36.780 --> 02:13:38.540]   I guess here, just to take the conversation
[02:13:38.540 --> 02:13:40.240]   in a somewhat different direction,
[02:13:40.240 --> 02:13:44.820]   I think in some of these debates around safety,
[02:13:44.820 --> 02:13:50.160]   I think the concepts of intelligence and autonomy
[02:13:50.160 --> 02:13:57.240]   or like the being of the thing, you know, as an analogy,
[02:13:57.240 --> 02:13:59.720]   they get kind of conflated together.
[02:13:59.720 --> 02:14:03.460]   And I think it very well could be the case
[02:14:03.460 --> 02:14:05.120]   that you can make something and scale it up,
[02:14:05.120 --> 02:14:07.280]   scale intelligence quite far,
[02:14:07.280 --> 02:14:12.280]   but that may not manifest the safety concerns
[02:14:12.280 --> 02:14:16.880]   that people are saying in the sense that, I mean,
[02:14:16.880 --> 02:14:19.020]   just if you look at human biology, it's like, all right,
[02:14:19.020 --> 02:14:22.840]   we have our neocortex is where all the thinking happens,
[02:14:22.840 --> 02:14:25.860]   right, but it's not really calling the shots
[02:14:25.860 --> 02:14:26.700]   at the end of the day.
[02:14:26.700 --> 02:14:31.100]   We have a much more primitive old brain structure
[02:14:31.100 --> 02:14:32.840]   for which our neocortex,
[02:14:32.840 --> 02:14:35.100]   which is this powerful machinery is basically,
[02:14:35.100 --> 02:14:38.140]   just a kind of prediction and reasoning engine
[02:14:38.140 --> 02:14:41.820]   to help it kind of like our very simple brain
[02:14:41.820 --> 02:14:48.440]   decide how to plan and do what it needs to do
[02:14:48.440 --> 02:14:52.460]   in order to achieve these like very kind of basic impulses.
[02:14:52.460 --> 02:14:57.420]   And I think that you can think about some of the development
[02:14:57.420 --> 02:15:00.220]   of intelligence along the same lines
[02:15:00.220 --> 02:15:03.260]   where just like our neocortex doesn't have free will
[02:15:03.260 --> 02:15:05.080]   or autonomy,
[02:15:05.080 --> 02:15:08.460]   we might develop these wildly intelligent systems
[02:15:08.460 --> 02:15:12.020]   that are much more intelligent than our neocortex,
[02:15:12.020 --> 02:15:13.640]   have much more capacity,
[02:15:13.640 --> 02:15:15.900]   but are in the same way that our neocortex
[02:15:15.900 --> 02:15:18.800]   is sort of subservient and is used as a tool
[02:15:18.800 --> 02:15:22.200]   by our kind of simple impulse brain.
[02:15:22.200 --> 02:15:25.460]   It's, I think that it's not out of the question
[02:15:25.460 --> 02:15:28.180]   that very intelligent systems that have the capacity
[02:15:28.180 --> 02:15:31.580]   to think will kind of act as that as sort of an extension
[02:15:31.580 --> 02:15:33.680]   of the neocortex doing that.
[02:15:33.680 --> 02:15:35.060]   So I think, my own view, I think, is that, you know,
[02:15:35.060 --> 02:15:38.920]   my view is that where we really need to be careful
[02:15:38.920 --> 02:15:42.720]   is on the development of autonomy
[02:15:42.720 --> 02:15:44.360]   and how we think about that,
[02:15:44.360 --> 02:15:49.360]   because it's actually the case that relatively simple
[02:15:49.360 --> 02:15:52.440]   and unintelligent things that have runaway autonomy
[02:15:52.440 --> 02:15:55.020]   and just spread themselves, or, you know, it's like,
[02:15:55.020 --> 02:15:57.040]   we have a word for that, it's a virus, right?
[02:15:57.040 --> 02:15:59.600]   It's, I mean, like it's, can be simple computer code
[02:15:59.600 --> 02:16:01.100]   that is not particularly intelligent,
[02:16:01.100 --> 02:16:03.400]   but just spreads itself and does a lot of harm.
[02:16:05.040 --> 02:16:06.920]   You know, biologically or computer.
[02:16:06.920 --> 02:16:11.920]   And I just think that these are somewhat separable things.
[02:16:11.920 --> 02:16:15.780]   And a lot of what I think we need to develop
[02:16:15.780 --> 02:16:18.260]   when people talk about safety and responsibility
[02:16:18.260 --> 02:16:21.360]   is really the governance on the autonomy
[02:16:21.360 --> 02:16:23.840]   that can be given to systems.
[02:16:23.840 --> 02:16:26.780]   And to me, if, you know, if I were, you know,
[02:16:26.780 --> 02:16:29.600]   a policymaker or thinking about this,
[02:16:29.600 --> 02:16:31.540]   I would really wanna think about that distinction
[02:16:31.540 --> 02:16:35.020]   between these, where I think building intelligent systems
[02:16:35.020 --> 02:16:38.740]   can create a huge advance in terms of people's quality of life
[02:16:38.740 --> 02:16:42.380]   and productivity growth in the economy.
[02:16:42.380 --> 02:16:45.080]   But it's the autonomy part of this
[02:16:45.080 --> 02:16:47.660]   that I think we really need to make progress
[02:16:47.660 --> 02:16:50.080]   on how to govern these things responsibly
[02:16:50.080 --> 02:16:54.220]   before we build the capacity for them
[02:16:54.220 --> 02:16:56.560]   to make a lot of decisions on their own
[02:16:56.560 --> 02:17:00.320]   or give them goals or things like that.
[02:17:00.320 --> 02:17:01.940]   And I know that's a research problem,
[02:17:01.940 --> 02:17:05.000]   but I do think that to some degree, these are somewhat,
[02:17:05.000 --> 02:17:06.940]   somewhat separable things.
[02:17:06.940 --> 02:17:09.820]   - I love the distinction between intelligence and autonomy
[02:17:09.820 --> 02:17:12.220]   and the metaphor within your cortex.
[02:17:12.220 --> 02:17:15.920]   Let me ask about power.
[02:17:15.920 --> 02:17:20.000]   So building super intelligent systems,
[02:17:20.000 --> 02:17:22.080]   even if it's not in the near term,
[02:17:22.080 --> 02:17:25.560]   I think Meta is one of the few companies,
[02:17:25.560 --> 02:17:29.340]   if not the main company that will develop
[02:17:29.340 --> 02:17:31.180]   the super intelligence system.
[02:17:31.180 --> 02:17:34.580]   And you are a man who's at the head of this company.
[02:17:34.580 --> 02:17:34.980]   Building it, you're a man who's at the head of this company.
[02:17:34.980 --> 02:17:37.340]   Building AGI might make you the most powerful man
[02:17:37.340 --> 02:17:38.180]   in the world.
[02:17:38.180 --> 02:17:40.280]   Do you worry that that power will corrupt you?
[02:17:40.280 --> 02:17:43.540]   - What a question.
[02:17:43.540 --> 02:17:48.560]   I mean, look, I think realistically,
[02:17:48.560 --> 02:17:50.300]   this gets back to the open source things
[02:17:50.300 --> 02:17:51.820]   that we talked about before,
[02:17:51.820 --> 02:17:56.400]   which is I don't think that the world will be best served
[02:17:56.400 --> 02:18:02.280]   by any small number of organizations having this.
[02:18:04.960 --> 02:18:08.960]   Without it being something that is more broadly available.
[02:18:08.960 --> 02:18:10.960]   And I think if you look through history,
[02:18:10.960 --> 02:18:16.300]   it's when there are these sort of like unipolar advances
[02:18:16.300 --> 02:18:19.060]   and things that, and like power imbalances
[02:18:19.060 --> 02:18:23.780]   that they're doing to being kind of weird situations.
[02:18:23.780 --> 02:18:26.740]   So this is one of the reasons why I think open sources
[02:18:26.740 --> 02:18:31.360]   is generally the right approach.
[02:18:31.360 --> 02:18:34.580]   And I think it's a categorically different question today
[02:18:34.580 --> 02:18:34.940]   when we're not talking about the power imbalance.
[02:18:34.940 --> 02:18:37.760]   I think that there's a good chance
[02:18:37.760 --> 02:18:40.140]   that even once we get closer to super intelligence,
[02:18:40.140 --> 02:18:42.100]   open sourcing remains the right approach,
[02:18:42.100 --> 02:18:42.980]   even though I think at that point,
[02:18:42.980 --> 02:18:44.620]   it's a somewhat different debate.
[02:18:44.620 --> 02:18:49.200]   But I think part of that is that that is,
[02:18:49.200 --> 02:18:51.680]   I think one of the best ways to ensure
[02:18:51.680 --> 02:18:54.180]   that the system is as secure and safe as possible,
[02:18:54.180 --> 02:18:55.820]   because it's not just about a lot of people
[02:18:55.820 --> 02:18:57.180]   having access to it.
[02:18:57.180 --> 02:18:59.360]   It's the scrutiny that kind of comes
[02:18:59.360 --> 02:19:02.380]   with building an open source system.
[02:19:02.380 --> 02:19:04.920]   I think that this is a pretty widely accepted thing about open
[02:19:04.920 --> 02:19:08.760]   sources that you have the code out there.
[02:19:08.760 --> 02:19:11.260]   So anyone can see the vulnerabilities.
[02:19:11.260 --> 02:19:13.900]   Anyone can kind of mess with it in different ways.
[02:19:13.900 --> 02:19:15.540]   People can spin off their own projects
[02:19:15.540 --> 02:19:17.740]   and experiment in a ton of different ways.
[02:19:17.740 --> 02:19:20.780]   And the net result of all of that is that the systems
[02:19:20.780 --> 02:19:24.680]   just get hardened and get to be a lot safer and more secure.
[02:19:24.680 --> 02:19:30.900]   So I think that there's a chance that that ends up being
[02:19:30.900 --> 02:19:34.900]   the way that this goes to, a pretty good chance,
[02:19:34.900 --> 02:19:39.900]   and that having this be open both leads
[02:19:39.900 --> 02:19:43.140]   to a healthier development of the technology
[02:19:43.140 --> 02:19:47.460]   and also leads to a more balanced distribution
[02:19:47.460 --> 02:19:50.020]   of the technology in a way that,
[02:19:50.020 --> 02:19:53.140]   that strike me as good values to aspire to.
[02:19:53.140 --> 02:19:55.960]   - So to you, the risks, there's risks to open sourcing,
[02:19:55.960 --> 02:19:57.780]   but the benefits outweigh the risks.
[02:19:57.780 --> 02:20:01.900]   At the two, it's interesting, I think the way you put it,
[02:20:01.900 --> 02:20:04.880]   you put it well, that there's,
[02:20:04.880 --> 02:20:08.260]   there's a different discussion now than when we get closer
[02:20:08.260 --> 02:20:11.480]   to the, to development of super intelligence
[02:20:11.480 --> 02:20:15.500]   of the benefits and risks of open sourcing.
[02:20:15.500 --> 02:20:17.900]   - Yeah, and to be clear, I feel quite confident
[02:20:17.900 --> 02:20:21.260]   in the assessment that open sourcing models now
[02:20:21.260 --> 02:20:23.420]   is net positive.
[02:20:23.420 --> 02:20:26.300]   I think there's a good argument that in the future,
[02:20:26.300 --> 02:20:28.780]   it will be too, even as you get closer to super intelligence,
[02:20:28.780 --> 02:20:32.300]   but I've not, I've certainly have not decided on that yet.
[02:20:32.300 --> 02:20:34.860]   And I think that it becomes a somewhat more complex set of
[02:20:34.860 --> 02:20:37.340]   decisions that I think people will have time to debate
[02:20:37.340 --> 02:20:40.040]   and we'll also be informed by what happens between now
[02:20:40.040 --> 02:20:41.660]   and then to make those decisions.
[02:20:41.660 --> 02:20:43.780]   We don't have to necessarily just debate that
[02:20:43.780 --> 02:20:44.880]   in theory right now.
[02:20:44.880 --> 02:20:48.020]   - What year do you think we'll have a super intelligence?
[02:20:48.020 --> 02:20:52.140]   - I don't know. I mean, that's pure speculation.
[02:20:52.140 --> 02:20:54.660]   I think it's, I think it's very clear,
[02:20:54.660 --> 02:20:56.360]   just taking a step back that we had a big breakthrough
[02:20:56.360 --> 02:20:57.520]   in the last year, right?
[02:20:57.520 --> 02:21:00.900]   Where the, the LLMs and diffusion models basically reached
[02:21:00.900 --> 02:21:03.740]   a scale where they're able to do some,
[02:21:03.740 --> 02:21:04.840]   some pretty interesting things.
[02:21:04.840 --> 02:21:07.340]   And then I think the question is what happens from here.
[02:21:07.340 --> 02:21:11.380]   And just to paint the two extremes on the,
[02:21:11.380 --> 02:21:15.760]   on one side, it's like, okay, well,
[02:21:15.760 --> 02:21:17.500]   we just had one breakthrough.
[02:21:17.500 --> 02:21:19.400]   If we just have like another breakthrough like that,
[02:21:19.400 --> 02:21:21.260]   or maybe two, then we can have something
[02:21:21.260 --> 02:21:23.280]   that's truly crazy, right?
[02:21:23.280 --> 02:21:28.240]   And is like, is just like so much more advanced.
[02:21:28.240 --> 02:21:31.920]   And on that side of the argument, it's like, okay,
[02:21:31.920 --> 02:21:32.860]   well maybe we're,
[02:21:34.820 --> 02:21:39.320]   maybe we're only a couple of big steps away from,
[02:21:39.320 --> 02:21:41.620]   from, from, from reaching something that looks more
[02:21:41.620 --> 02:21:43.320]   like general intelligence.
[02:21:43.320 --> 02:21:45.820]   Okay. That's one, that's one side of the argument.
[02:21:45.820 --> 02:21:46.660]   And the other side,
[02:21:46.660 --> 02:21:49.340]   which is what we've historically seen a lot more is that a
[02:21:49.340 --> 02:21:54.340]   breakthrough leads to, you know, in that,
[02:21:54.340 --> 02:21:58.340]   in that Gartner hype cycle, there's like the hype.
[02:21:58.340 --> 02:22:01.180]   And then there's the trough of disillusionment after when
[02:22:01.180 --> 02:22:03.880]   like people think that there's a chance that, hey, okay,
[02:22:03.880 --> 02:22:04.800]   there's a big breakthrough. Maybe we're gonna get there.
[02:22:04.800 --> 02:22:06.060]   Maybe we're about to get another big breakthrough.
[02:22:06.060 --> 02:22:07.700]   And it's like, actually, you're not about to get
[02:22:07.700 --> 02:22:08.540]   another breakthrough.
[02:22:08.540 --> 02:22:10.900]   You're maybe you're actually just gonna have to sit with
[02:22:10.900 --> 02:22:12.340]   this one for a while.
[02:22:12.340 --> 02:22:17.340]   And, and, you know, it could be, could be five years.
[02:22:17.340 --> 02:22:18.760]   It could be 10 years.
[02:22:18.760 --> 02:22:23.320]   It could be 15 years until you figure out the, the kind of
[02:22:23.320 --> 02:22:25.660]   the next big thing that needs to get figured out.
[02:22:25.660 --> 02:22:28.920]   And, but I think that the fact that we just had this
[02:22:28.920 --> 02:22:33.680]   breakthrough sort of makes it that we're at a point of almost
[02:22:33.680 --> 02:22:34.780]   a very wide area.
[02:22:34.780 --> 02:22:35.780]   I mean, I think that's, I think that's, I think that's, I think that's, I think that's, I think that's, I think that's, I think that's, I think that's a very good point.
[02:22:35.780 --> 02:22:36.780]   I think that's a very good point.
[02:22:36.780 --> 02:22:37.780]   I think that's a very good point.
[02:22:37.780 --> 02:22:38.780]   I think that's a very good point.
[02:22:38.780 --> 02:22:39.780]   I think that's a very good point.
[02:22:39.780 --> 02:22:40.780]   I think that's a very good point.
[02:22:40.780 --> 02:22:41.780]   I think that's a very good point.
[02:22:41.780 --> 02:22:42.780]   I think that's a very good point.
[02:22:42.780 --> 02:22:43.780]   I think that's a very good point.
[02:22:43.780 --> 02:22:44.780]   I think that's a very good point.
[02:22:44.780 --> 02:22:45.780]   I think that's a very good point.
[02:22:45.780 --> 02:22:46.780]   I think that's a very good point.
[02:22:46.780 --> 02:22:47.780]   I think that's a very good point.
[02:22:47.780 --> 02:22:48.780]   I think that's a very good point.
[02:22:48.780 --> 02:22:49.780]   I think that's a very good point.
[02:22:49.780 --> 02:22:50.780]   I think that's a very good point.
[02:22:50.780 --> 02:22:51.780]   I think that's a very good point.
[02:22:51.780 --> 02:22:52.780]   I think that's a very good point.
[02:22:52.780 --> 02:22:53.780]   I think that's a very good point.
[02:22:53.780 --> 02:22:54.780]   I think that's a very good point.
[02:22:54.780 --> 02:22:55.780]   I think that's a very good point.
[02:22:55.780 --> 02:22:56.780]   I think that's a very good point.
[02:22:56.780 --> 02:22:57.780]   I think that's a very good point.
[02:22:57.780 --> 02:22:58.780]   I think that's a very good point.
[02:22:58.780 --> 02:22:59.780]   I think that's a very good point.
[02:22:59.780 --> 02:23:00.780]   I think that's a very good point.
[02:23:00.780 --> 02:23:01.780]   I think that's a very good point.
[02:23:01.780 --> 02:23:02.780]   I think that's a very good point.
[02:23:02.780 --> 02:23:03.780]   I think that's a very good point.
[02:23:03.780 --> 02:23:04.780]   I think that's a very good point.
[02:23:04.780 --> 02:23:05.780]   I think that's a very good point.
[02:23:05.780 --> 02:23:06.780]   I think that's a very good point.
[02:23:06.780 --> 02:23:07.780]   I think that's a very good point.
[02:23:07.780 --> 02:23:08.780]   I think that's a very good point.
[02:23:08.780 --> 02:23:09.780]   I think that's a very good point.
[02:23:09.780 --> 02:23:10.780]   I think that's a very good point.
[02:23:10.780 --> 02:23:11.780]   I think that's a very good point.
[02:23:11.780 --> 02:23:12.780]   I think that's a very good point.
[02:23:12.780 --> 02:23:13.780]   I think that's a very good point.
[02:23:13.780 --> 02:23:14.780]   I think that's a very good point.
[02:23:14.780 --> 02:23:15.780]   I think that's a very good point.
[02:23:15.780 --> 02:23:16.780]   I think that's a very good point.
[02:23:16.780 --> 02:23:17.780]   I think that's a very good point.
[02:23:17.780 --> 02:23:18.780]   I think that's a very good point.
[02:23:18.780 --> 02:23:19.780]   I think that's a very good point.
[02:23:19.780 --> 02:23:20.780]   I think that's a very good point.
[02:23:20.780 --> 02:23:21.780]   I think that's a very good point.
[02:23:21.780 --> 02:23:22.780]   I think that's a very good point.
[02:23:22.780 --> 02:23:23.780]   I think that's a very good point.
[02:23:23.780 --> 02:23:24.780]   I think that's a very good point.
[02:23:24.780 --> 02:23:25.780]   I think that's a very good point.
[02:23:25.780 --> 02:23:26.780]   I think that's a very good point.
[02:23:26.780 --> 02:23:27.780]   I think that's a very good point.
[02:23:27.780 --> 02:23:28.780]   I think that's a very good point.
[02:23:28.780 --> 02:23:29.780]   I think that's a very good point.
[02:23:29.780 --> 02:23:30.780]   I think that's a very good point.
[02:23:30.780 --> 02:23:31.780]   I think that's a very good point.
[02:23:31.780 --> 02:23:32.780]   I think that's a very good point.
[02:23:32.780 --> 02:23:33.780]   I think that's a very good point.
[02:23:33.780 --> 02:23:34.780]   I think that's a very good point.
[02:23:34.780 --> 02:23:35.780]   I think that's a very good point.
[02:23:35.780 --> 02:23:36.780]   I think that's a very good point.
[02:23:36.780 --> 02:23:37.780]   I think that's a very good point.
[02:23:37.780 --> 02:23:38.780]   I think that's a very good point.
[02:23:38.780 --> 02:23:39.780]   I think that's a very good point.
[02:23:39.780 --> 02:23:40.780]   I think that's a very good point.
[02:23:40.780 --> 02:23:41.780]   I think that's a very good point.
[02:23:41.780 --> 02:23:42.780]   I think that's a very good point.
[02:23:42.780 --> 02:23:43.780]   I think that's a very good point.
[02:23:43.780 --> 02:23:44.780]   I think that's a very good point.
[02:23:44.780 --> 02:23:45.780]   I think that's a very good point.
[02:23:45.780 --> 02:23:46.780]   I think that's a very good point.
[02:23:46.780 --> 02:23:47.780]   I think that's a very good point.
[02:23:47.780 --> 02:23:48.780]   I think that's a very good point.
[02:23:48.780 --> 02:23:49.780]   I think that's a very good point.
[02:23:49.780 --> 02:23:50.780]   I think that's a very good point.
[02:23:50.780 --> 02:23:51.780]   I think that's a very good point.
[02:23:51.780 --> 02:23:52.780]   I think that's a very good point.
[02:23:52.780 --> 02:23:53.780]   I think that's a very good point.
[02:23:53.780 --> 02:23:54.780]   I think that's a very good point.
[02:23:54.780 --> 02:23:55.780]   I think that's a very good point.
[02:23:55.780 --> 02:23:56.780]   I think that's a very good point.
[02:23:56.780 --> 02:23:57.780]   I think that's a very good point.
[02:23:57.780 --> 02:23:58.780]   I think that's a very good point.
[02:23:58.780 --> 02:23:59.780]   I think that's a very good point.
[02:23:59.780 --> 02:24:00.780]   I think that's a very good point.
[02:24:00.780 --> 02:24:01.780]   I think that's a very good point.
[02:24:01.780 --> 02:24:02.780]   I think that's a very good point.
[02:24:02.780 --> 02:24:03.780]   I think that's a very good point.
[02:24:03.780 --> 02:24:04.780]   I think that's a very good point.
[02:24:04.780 --> 02:24:05.780]   I think that's a very good point.
[02:24:05.780 --> 02:24:06.780]   I think that's a very good point.
[02:24:06.780 --> 02:24:07.780]   I think that's a very good point.
[02:24:07.780 --> 02:24:08.780]   I think that's a very good point.
[02:24:08.780 --> 02:24:09.780]   I think that's a very good point.
[02:24:09.780 --> 02:24:10.780]   I think that's a very good point.
[02:24:10.780 --> 02:24:11.780]   I think that's a very good point.
[02:24:11.780 --> 02:24:12.780]   I think that's a very good point.
[02:24:12.780 --> 02:24:13.780]   I think that's a very good point.
[02:24:13.780 --> 02:24:14.780]   I think that's a very good point.
[02:24:14.780 --> 02:24:15.780]   I think that's a very good point.
[02:24:15.780 --> 02:24:16.780]   I think that's a very good point.
[02:24:16.780 --> 02:24:17.780]   I think that's a very good point.
[02:24:17.780 --> 02:24:18.780]   I think that's a very good point.
[02:24:18.780 --> 02:24:19.780]   I think that's a very good point.
[02:24:19.780 --> 02:24:20.780]   I think that's a very good point.
[02:24:20.780 --> 02:24:21.780]   I think that's a very good point.
[02:24:21.780 --> 02:24:22.780]   I think that's a very good point.
[02:24:22.780 --> 02:24:23.780]   I think that's a very good point.
[02:24:23.780 --> 02:24:24.780]   I think that's a very good point.
[02:24:24.780 --> 02:24:25.780]   I think that's a very good point.
[02:24:25.780 --> 02:24:26.780]   I think that's a very good point.
[02:24:26.780 --> 02:24:27.780]   I think that's a very good point.
[02:24:27.780 --> 02:24:28.780]   I think that's a very good point.
[02:24:28.780 --> 02:24:29.780]   I think that's a very good point.
[02:24:29.780 --> 02:24:30.780]   I think that's a very good point.
[02:24:30.780 --> 02:24:31.780]   I think that's a very good point.
[02:24:31.780 --> 02:24:32.780]   I think that's a very good point.
[02:24:32.780 --> 02:24:33.780]   I think that's a very good point.
[02:24:33.780 --> 02:24:34.780]   I think that's a very good point.
[02:24:34.780 --> 02:24:35.780]   I think that's a very good point.
[02:24:35.780 --> 02:24:36.780]   I think that's a very good point.
[02:24:36.780 --> 02:24:37.780]   I think that's a very good point.
[02:24:37.780 --> 02:24:38.780]   I think that's a very good point.
[02:24:38.780 --> 02:24:39.780]   I think that's a very good point.
[02:24:39.780 --> 02:24:40.780]   I think that's a very good point.
[02:24:40.780 --> 02:24:41.780]   I think that's a very good point.
[02:24:41.780 --> 02:24:42.780]   I think that's a very good point.
[02:24:42.780 --> 02:24:43.780]   I think that's a very good point.
[02:24:43.780 --> 02:24:44.780]   I think that's a very good point.
[02:24:44.780 --> 02:24:45.780]   I think that's a very good point.
[02:24:45.780 --> 02:24:46.780]   I think that's a very good point.
[02:24:46.780 --> 02:24:47.780]   I think that's a very good point.
[02:24:47.780 --> 02:24:48.780]   I think that's a very good point.
[02:24:48.780 --> 02:24:49.780]   I think that's a very good point.
[02:24:49.780 --> 02:24:50.780]   I think that's a very good point.
[02:24:50.780 --> 02:24:51.780]   I think that's a very good point.
[02:24:51.780 --> 02:24:52.780]   I think that's a very good point.
[02:24:52.780 --> 02:24:53.780]   I think that's a very good point.
[02:24:53.780 --> 02:24:54.780]   I think that's a very good point.
[02:24:54.780 --> 02:24:55.780]   I think that's a very good point.
[02:24:55.780 --> 02:24:56.780]   I think that's a very good point.
[02:24:56.780 --> 02:24:57.780]   I think that's a very good point.
[02:24:57.780 --> 02:24:58.780]   I think that's a very good point.
[02:24:58.780 --> 02:24:59.780]   I think that's a very good point.
[02:24:59.780 --> 02:25:00.780]   I think that's a very good point.
[02:25:00.780 --> 02:25:01.780]   I think that's a very good point.
[02:25:01.780 --> 02:25:02.780]   I think that's a very good point.
[02:25:02.780 --> 02:25:03.780]   I think that's a very good point.
[02:25:03.780 --> 02:25:04.780]   I think that's a very good point.
[02:25:04.780 --> 02:25:05.780]   I think that's a very good point.
[02:25:05.780 --> 02:25:06.780]   I think that's a very good point.
[02:25:06.780 --> 02:25:07.780]   I think that's a very good point.
[02:25:07.780 --> 02:25:08.780]   I think that's a very good point.
[02:25:08.780 --> 02:25:09.780]   I think that's a very good point.
[02:25:09.780 --> 02:25:10.780]   I think that's a very good point.
[02:25:10.780 --> 02:25:11.780]   I think that's a very good point.
[02:25:11.780 --> 02:25:12.780]   I think that's a very good point.
[02:25:12.780 --> 02:25:13.780]   I think that's a very good point.
[02:25:13.780 --> 02:25:14.780]   I think that's a very good point.
[02:25:14.780 --> 02:25:15.780]   I think that's a very good point.
[02:25:15.780 --> 02:25:16.780]   I think that's a very good point.
[02:25:16.780 --> 02:25:17.780]   I think that's a very good point.
[02:25:17.780 --> 02:25:18.780]   I think that's a very good point.
[02:25:18.780 --> 02:25:19.780]   I think that's a very good point.
[02:25:19.780 --> 02:25:20.780]   I think that's a very good point.
[02:25:20.780 --> 02:25:21.780]   I think that's a very good point.
[02:25:21.780 --> 02:25:22.780]   I think that's a very good point.
[02:25:22.780 --> 02:25:23.780]   I think that's a very good point.
[02:25:23.780 --> 02:25:24.780]   I think that's a very good point.
[02:25:24.780 --> 02:25:25.780]   I think that's a very good point.
[02:25:25.780 --> 02:25:26.780]   I think that's a very good point.
[02:25:26.780 --> 02:25:27.780]   I think that's a very good point.
[02:25:27.780 --> 02:25:28.780]   I think that's a very good point.
[02:25:28.780 --> 02:25:29.780]   I think that's a very good point.
[02:25:29.780 --> 02:25:30.780]   I think that's a very good point.
[02:25:30.780 --> 02:25:31.780]   I think that's a very good point.
[02:25:31.780 --> 02:25:32.780]   I think that's a very good point.
[02:25:32.780 --> 02:25:33.780]   I think that's a very good point.
[02:25:33.780 --> 02:25:34.780]   I think that's a very good point.
[02:25:34.780 --> 02:25:35.780]   I think that's a very good point.
[02:25:35.780 --> 02:25:36.780]   I think that's a very good point.
[02:25:36.780 --> 02:25:37.780]   I think that's a very good point.
[02:25:37.780 --> 02:25:38.780]   I think that's a very good point.
[02:25:38.780 --> 02:25:39.780]   I think that's a very good point.
[02:25:39.780 --> 02:25:40.780]   I think that's a very good point.
[02:25:40.780 --> 02:25:41.780]   I think that's a very good point.
[02:25:41.780 --> 02:25:42.780]   I think that's a very good point.
[02:25:42.780 --> 02:25:43.780]   I think that's a very good point.
[02:25:43.780 --> 02:25:44.780]   I think that's a very good point.
[02:25:44.780 --> 02:25:45.780]   I think that's a very good point.
[02:25:45.780 --> 02:25:46.780]   I think that's a very good point.
[02:25:46.780 --> 02:25:47.780]   I think that's a very good point.
[02:25:47.780 --> 02:25:48.780]   I think that's a very good point.
[02:25:48.780 --> 02:25:49.780]   I think that's a very good point.
[02:25:49.780 --> 02:25:50.780]   I think that's a very good point.
[02:25:50.780 --> 02:25:51.780]   I think that's a very good point.
[02:25:51.780 --> 02:25:52.780]   I think that's a very good point.
[02:25:52.780 --> 02:25:53.780]   I think that's a very good point.
[02:25:53.780 --> 02:25:54.780]   I think that's a very good point.
[02:25:54.780 --> 02:25:55.780]   I think that's a very good point.
[02:25:55.780 --> 02:25:56.780]   I think that's a very good point.
[02:25:56.780 --> 02:25:57.780]   I think that's a very good point.
[02:25:57.780 --> 02:25:58.780]   I think that's a very good point.
[02:25:58.780 --> 02:25:59.780]   I think that's a very good point.
[02:25:59.780 --> 02:26:00.780]   I think that's a very good point.
[02:26:00.780 --> 02:26:01.780]   I think that's a very good point.
[02:26:01.780 --> 02:26:02.780]   I think that's a very good point.
[02:26:02.780 --> 02:26:03.780]   I think that's a very good point.
[02:26:03.780 --> 02:26:04.780]   I think that's a very good point.
[02:26:04.780 --> 02:26:05.780]   I think that's a very good point.
[02:26:05.780 --> 02:26:06.780]   I think that's a very good point.
[02:26:06.780 --> 02:26:07.780]   I think that's a very good point.
[02:26:07.780 --> 02:26:08.780]   I think that's a very good point.
[02:26:08.780 --> 02:26:09.780]   I think that's a very good point.
[02:26:09.780 --> 02:26:10.780]   I think that's a very good point.
[02:26:10.780 --> 02:26:11.780]   I think that's a very good point.
[02:26:11.780 --> 02:26:12.780]   I think that's a very good point.
[02:26:12.780 --> 02:26:13.780]   I think that's a very good point.
[02:26:13.780 --> 02:26:14.780]   I think that's a very good point.
[02:26:14.780 --> 02:26:15.780]   I think that's a very good point.
[02:26:15.780 --> 02:26:16.780]   I think that's a very good point.
[02:26:16.780 --> 02:26:17.780]   I think that's a very good point.
[02:26:17.780 --> 02:26:18.780]   I think that's a very good point.
[02:26:18.780 --> 02:26:19.780]   I think that's a very good point.
[02:26:19.780 --> 02:26:20.780]   I think that's a very good point.
[02:26:20.780 --> 02:26:21.780]   I think that's a very good point.
[02:26:21.780 --> 02:26:22.780]   I think that's a very good point.
[02:26:22.780 --> 02:26:23.780]   I think that's a very good point.
[02:26:23.780 --> 02:26:24.780]   I think that's a very good point.
[02:26:24.780 --> 02:26:25.780]   I think that's a very good point.
[02:26:25.780 --> 02:26:26.780]   I think that's a very good point.
[02:26:26.780 --> 02:26:27.780]   I think that's a very good point.
[02:26:27.780 --> 02:26:28.780]   I think that's a very good point.
[02:26:28.780 --> 02:26:29.780]   I think that's a very good point.
[02:26:29.780 --> 02:26:30.780]   I think that's a very good point.
[02:26:30.780 --> 02:26:31.780]   I think that's a very good point.
[02:26:31.780 --> 02:26:32.780]   I think that's a very good point.
[02:26:32.780 --> 02:26:33.780]   I think that's a very good point.
[02:26:33.780 --> 02:26:34.780]   I think that's a very good point.
[02:26:34.780 --> 02:26:35.780]   I think that's a very good point.
[02:26:35.780 --> 02:26:36.780]   I think that's a very good point.
[02:26:36.780 --> 02:26:37.780]   I think that's a very good point.
[02:26:37.780 --> 02:26:38.780]   I think that's a very good point.
[02:26:38.780 --> 02:26:39.780]   I think that's a very good point.
[02:26:39.780 --> 02:26:40.780]   I think that's a very good point.
[02:26:40.780 --> 02:26:41.780]   I think that's a very good point.
[02:26:41.780 --> 02:26:42.780]   I think that's a very good point.
[02:26:42.780 --> 02:26:43.780]   I think that's a very good point.
[02:26:43.780 --> 02:26:44.780]   I think that's a very good point.
[02:26:44.780 --> 02:26:45.780]   I think that's a very good point.
[02:26:45.780 --> 02:26:46.780]   I think that's a very good point.
[02:26:46.780 --> 02:26:47.780]   I think that's a very good point.
[02:26:47.780 --> 02:26:48.780]   I think that's a very good point.
[02:26:48.780 --> 02:26:49.780]   I think that's a very good point.
[02:26:49.780 --> 02:26:50.780]   I think that's a very good point.
[02:26:50.780 --> 02:26:51.780]   I think that's a very good point.
[02:26:51.780 --> 02:26:52.780]   I think that's a very good point.
[02:26:52.780 --> 02:26:53.780]   I think that's a very good point.
[02:26:53.780 --> 02:26:54.780]   I think that's a very good point.
[02:26:54.780 --> 02:26:55.780]   I think that's a very good point.
[02:26:55.780 --> 02:26:56.780]   I think that's a very good point.
[02:26:56.780 --> 02:26:57.780]   I think that's a very good point.
[02:26:57.780 --> 02:26:58.780]   I think that's a very good point.
[02:26:58.780 --> 02:26:59.780]   I think that's a very good point.
[02:26:59.780 --> 02:27:00.780]   I think that's a very good point.
[02:27:00.780 --> 02:27:01.780]   I think that's a very good point.
[02:27:01.780 --> 02:27:02.780]   I think that's a very good point.
[02:27:02.780 --> 02:27:03.780]   I think that's a very good point.
[02:27:03.780 --> 02:27:04.780]   I think that's a very good point.
[02:27:04.780 --> 02:27:05.780]   I think that's a very good point.
[02:27:05.780 --> 02:27:06.780]   I think that's a very good point.
[02:27:06.780 --> 02:27:07.780]   I think that's a very good point.
[02:27:07.780 --> 02:27:08.780]   I think that's a very good point.
[02:27:08.780 --> 02:27:09.780]   I think that's a very good point.
[02:27:09.780 --> 02:27:10.780]   I think that's a very good point.
[02:27:10.780 --> 02:27:11.780]   I think that's a very good point.
[02:27:11.780 --> 02:27:12.780]   I think that's a very good point.
[02:27:12.780 --> 02:27:13.780]   I think that's a very good point.
[02:27:13.780 --> 02:27:14.780]   I think that's a very good point.
[02:27:14.780 --> 02:27:15.780]   I think that's a very good point.
[02:27:15.780 --> 02:27:16.780]   I think that's a very good point.
[02:27:16.780 --> 02:27:17.780]   I think that's a very good point.
[02:27:17.780 --> 02:27:18.780]   I think that's a very good point.
[02:27:18.780 --> 02:27:19.780]   I think that's a very good point.
[02:27:19.780 --> 02:27:20.780]   I think that's a very good point.
[02:27:20.780 --> 02:27:21.780]   I think that's a very good point.
[02:27:21.780 --> 02:27:22.780]   I think that's a very good point.
[02:27:22.780 --> 02:27:23.780]   I think that's a very good point.
[02:27:23.780 --> 02:27:24.780]   I think that's a very good point.
[02:27:24.780 --> 02:27:25.780]   I think that's a very good point.
[02:27:25.780 --> 02:27:26.780]   I think that's a very good point.
[02:27:26.780 --> 02:27:27.780]   I think that's a very good point.
[02:27:27.780 --> 02:27:28.780]   I think that's a very good point.
[02:27:28.780 --> 02:27:29.780]   I think that's a very good point.
[02:27:29.780 --> 02:27:30.780]   I think that's a very good point.
[02:27:30.780 --> 02:27:31.780]   I think that's a very good point.
[02:27:31.780 --> 02:27:32.780]   I think that's a very good point.
[02:27:32.780 --> 02:27:33.780]   I think that's a very good point.
[02:27:33.780 --> 02:27:34.780]   I think that's a very good point.
[02:27:34.780 --> 02:27:35.780]   I think that's a very good point.
[02:27:35.780 --> 02:27:36.780]   I think that's a very good point.
[02:27:36.780 --> 02:27:37.780]   I think that's a very good point.
[02:27:37.780 --> 02:27:38.780]   I think that's a very good point.
[02:27:38.780 --> 02:27:39.780]   I think that's a very good point.
[02:27:39.780 --> 02:27:40.780]   I think that's a very good point.
[02:27:40.780 --> 02:27:41.780]   I think that's a very good point.
[02:27:41.780 --> 02:27:42.780]   I think that's a very good point.
[02:27:42.780 --> 02:27:43.780]   I think that's a very good point.
[02:27:43.780 --> 02:27:44.780]   I think that's a very good point.
[02:27:44.780 --> 02:27:45.780]   I think that's a very good point.
[02:27:45.780 --> 02:27:46.780]   I think that's a very good point.
[02:27:46.780 --> 02:27:47.780]   I think that's a very good point.
[02:27:47.780 --> 02:27:48.780]   I think that's a very good point.
[02:27:48.780 --> 02:27:49.780]   I think that's a very good point.
[02:27:49.780 --> 02:27:50.780]   I think that's a very good point.
[02:27:50.780 --> 02:27:51.780]   I think that's a very good point.
[02:27:51.780 --> 02:27:52.780]   I think that's a very good point.
[02:27:52.780 --> 02:27:53.780]   I think that's a very good point.
[02:27:53.780 --> 02:27:54.780]   I think that's a very good point.
[02:27:54.780 --> 02:27:55.780]   I think that's a very good point.
[02:27:55.780 --> 02:27:56.780]   I think that's a very good point.
[02:27:56.780 --> 02:27:57.780]   I think that's a very good point.
[02:27:57.780 --> 02:27:58.780]   I think that's a very good point.
[02:27:58.780 --> 02:27:59.780]   I think that's a very good point.
[02:27:59.780 --> 02:28:00.780]   I think that's a very good point.
[02:28:00.780 --> 02:28:01.780]   I think that's a very good point.
[02:28:01.780 --> 02:28:02.780]   I think that's a very good point.
[02:28:02.780 --> 02:28:03.780]   I think that's a very good point.
[02:28:03.780 --> 02:28:04.780]   I think that's a very good point.
[02:28:04.780 --> 02:28:05.780]   I think that's a very good point.
[02:28:05.780 --> 02:28:06.780]   I think that's a very good point.
[02:28:06.780 --> 02:28:07.780]   I think that's a very good point.
[02:28:07.780 --> 02:28:08.780]   I think that's a very good point.
[02:28:08.780 --> 02:28:09.780]   I think that's a very good point.
[02:28:09.780 --> 02:28:10.780]   I think that's a very good point.
[02:28:10.780 --> 02:28:11.780]   I think that's a very good point.
[02:28:11.780 --> 02:28:12.780]   I think that's a very good point.
[02:28:12.780 --> 02:28:13.780]   I think that's a very good point.
[02:28:13.780 --> 02:28:14.780]   I think that's a very good point.
[02:28:14.780 --> 02:28:15.780]   I think that's a very good point.
[02:28:15.780 --> 02:28:16.780]   I think that's a very good point.
[02:28:16.780 --> 02:28:17.780]   I think that's a very good point.
[02:28:17.780 --> 02:28:18.780]   I think that's a very good point.
[02:28:18.780 --> 02:28:19.780]   I think that's a very good point.
[02:28:19.780 --> 02:28:20.780]   I think that's a very good point.
[02:28:20.780 --> 02:28:21.780]   I think that's a very good point.
[02:28:21.780 --> 02:28:22.780]   I think that's a very good point.
[02:28:22.780 --> 02:28:23.780]   I think that's a very good point.
[02:28:23.780 --> 02:28:24.780]   I think that's a very good point.
[02:28:24.780 --> 02:28:25.780]   I think that's a very good point.
[02:28:25.780 --> 02:28:26.780]   I think that's a very good point.
[02:28:26.780 --> 02:28:27.780]   I think that's a very good point.
[02:28:27.780 --> 02:28:28.780]   I think that's a very good point.
[02:28:28.780 --> 02:28:29.780]   I think that's a very good point.
[02:28:29.780 --> 02:28:30.780]   I think that's a very good point.
[02:28:30.780 --> 02:28:31.780]   I think that's a very good point.
[02:28:31.780 --> 02:28:32.780]   I think that's a very good point.
[02:28:32.780 --> 02:28:33.780]   I think that's a very good point.
[02:28:33.780 --> 02:28:34.780]   I think that's a very good point.
[02:28:34.780 --> 02:28:35.780]   I think that's a very good point.
[02:28:35.780 --> 02:28:36.780]   I think that's a very good point.
[02:28:36.780 --> 02:28:37.780]   I think that's a very good point.
[02:28:37.780 --> 02:28:38.780]   I think that's a very good point.
[02:28:38.780 --> 02:28:39.780]   I think that's a very good point.
[02:28:39.780 --> 02:28:40.780]   I think that's a very good point.
[02:28:40.780 --> 02:28:41.780]   I think that's a very good point.
[02:28:41.780 --> 02:28:42.780]   I think that's a very good point.
[02:28:42.780 --> 02:28:43.780]   I think that's a very good point.
[02:28:43.780 --> 02:28:44.780]   I think that's a very good point.
[02:28:44.780 --> 02:28:45.780]   I think that's a very good point.
[02:28:45.780 --> 02:28:46.780]   I think that's a very good point.
[02:28:46.780 --> 02:28:47.780]   I think that's a very good point.
[02:28:47.780 --> 02:28:48.780]   I think that's a very good point.
[02:28:48.780 --> 02:28:49.780]   I think that's a very good point.
[02:28:49.780 --> 02:28:50.780]   I think that's a very good point.
[02:28:50.780 --> 02:28:51.780]   I think that's a very good point.
[02:28:51.780 --> 02:28:52.780]   I think that's a very good point.
[02:28:52.780 --> 02:28:53.780]   I think that's a very good point.
[02:28:53.780 --> 02:28:54.780]   I think that's a very good point.
[02:28:54.780 --> 02:28:55.780]   I think that's a very good point.
[02:28:55.780 --> 02:28:56.780]   I think that's a very good point.
[02:28:56.780 --> 02:28:57.780]   I think that's a very good point.
[02:28:57.780 --> 02:28:58.780]   I think that's a very good point.
[02:28:58.780 --> 02:28:59.780]   I think that's a very good point.
[02:28:59.780 --> 02:29:00.780]   I think that's a very good point.
[02:29:00.780 --> 02:29:01.780]   I think that's a very good point.
[02:29:01.780 --> 02:29:02.780]   I think that's a very good point.
[02:29:02.780 --> 02:29:03.780]   I think that's a very good point.
[02:29:03.780 --> 02:29:04.780]   I think that's a very good point.
[02:29:04.780 --> 02:29:05.780]   I think that's a very good point.
[02:29:05.780 --> 02:29:06.780]   I think that's a very good point.
[02:29:06.780 --> 02:29:07.780]   I think that's a very good point.
[02:29:07.780 --> 02:29:08.780]   I think that's a very good point.
[02:29:08.780 --> 02:29:09.780]   I think that's a very good point.
[02:29:09.780 --> 02:29:10.780]   I think that's a very good point.
[02:29:10.780 --> 02:29:11.780]   I think that's a very good point.
[02:29:11.780 --> 02:29:12.780]   I think that's a very good point.
[02:29:12.780 --> 02:29:13.780]   I think that's a very good point.
[02:29:13.780 --> 02:29:14.780]   I think that's a very good point.
[02:29:14.780 --> 02:29:15.780]   I think that's a very good point.
[02:29:15.780 --> 02:29:16.780]   I think that's a very good point.
[02:29:16.780 --> 02:29:17.780]   I think that's a very good point.
[02:29:17.780 --> 02:29:18.780]   I think that's a very good point.
[02:29:18.780 --> 02:29:19.780]   I think that's a very good point.
[02:29:19.780 --> 02:29:20.780]   I think that's a very good point.
[02:29:20.780 --> 02:29:21.780]   I think that's a very good point.
[02:29:21.780 --> 02:29:22.780]   I think that's a very good point.
[02:29:22.780 --> 02:29:23.780]   I think that's a very good point.
[02:29:23.780 --> 02:29:24.780]   I think that's a very good point.
[02:29:24.780 --> 02:29:25.780]   I think that's a very good point.
[02:29:25.780 --> 02:29:26.780]   I think that's a very good point.
[02:29:26.780 --> 02:29:27.780]   I think that's a very good point.
[02:29:27.780 --> 02:29:28.780]   I think that's a very good point.
[02:29:28.780 --> 02:29:29.780]   I think that's a very good point.
[02:29:29.780 --> 02:29:30.780]   I think that's a very good point.
[02:29:30.780 --> 02:29:31.780]   I think that's a very good point.
[02:29:31.780 --> 02:29:32.780]   I think that's a very good point.
[02:29:32.780 --> 02:29:33.780]   I think that's a very good point.
[02:29:33.780 --> 02:29:34.780]   I think that's a very good point.
[02:29:34.780 --> 02:29:35.780]   I think that's a very good point.
[02:29:35.780 --> 02:29:36.780]   I think that's a very good point.
[02:29:36.780 --> 02:29:37.780]   I think that's a very good point.
[02:29:37.780 --> 02:29:38.780]   I think that's a very good point.
[02:29:38.780 --> 02:29:39.780]   I think that's a very good point.
[02:29:39.780 --> 02:29:40.780]   I think that's a very good point.
[02:29:40.780 --> 02:29:41.780]   I think that's a very good point.
[02:29:41.780 --> 02:29:42.780]   I think that's a very good point.
[02:29:42.780 --> 02:29:43.780]   I think that's a very good point.
[02:29:43.780 --> 02:29:44.780]   I think that's a very good point.
[02:29:44.780 --> 02:29:45.780]   I think that's a very good point.
[02:29:45.780 --> 02:29:46.780]   I think that's a very good point.
[02:29:46.780 --> 02:29:47.780]   I think that's a very good point.
[02:29:47.780 --> 02:29:48.780]   I think that's a very good point.
[02:29:48.780 --> 02:29:49.780]   I think that's a very good point.
[02:29:49.780 --> 02:29:50.780]   I think that's a very good point.
[02:29:50.780 --> 02:29:51.780]   I think that's a very good point.
[02:29:51.780 --> 02:29:52.780]   I think that's a very good point.
[02:29:52.780 --> 02:29:53.780]   I think that's a very good point.
[02:29:53.780 --> 02:29:54.780]   I think that's a very good point.
[02:29:54.780 --> 02:29:55.780]   I think that's a very good point.
[02:29:55.780 --> 02:29:56.780]   I think that's a very good point.
[02:29:56.780 --> 02:29:57.780]   I think that's a very good point.
[02:29:57.780 --> 02:29:58.780]   I think that's a very good point.
[02:29:58.780 --> 02:29:59.780]   I think that's a very good point.
[02:29:59.780 --> 02:30:00.780]   I think that's a very good point.
[02:30:00.780 --> 02:30:01.780]   I think that's a very good point.
[02:30:01.780 --> 02:30:02.780]   I think that's a very good point.
[02:30:02.780 --> 02:30:03.780]   I think that's a very good point.
[02:30:03.780 --> 02:30:04.780]   I think that's a very good point.
[02:30:04.780 --> 02:30:05.780]   I think that's a very good point.
[02:30:05.780 --> 02:30:06.780]   I think that's a very good point.
[02:30:06.780 --> 02:30:07.780]   I think that's a very good point.
[02:30:07.780 --> 02:30:08.780]   I think that's a very good point.
[02:30:08.780 --> 02:30:09.780]   I think that's a very good point.
[02:30:09.780 --> 02:30:10.780]   I think that's a very good point.
[02:30:10.780 --> 02:30:11.780]   I think that's a very good point.
[02:30:11.780 --> 02:30:12.780]   I think that's a very good point.
[02:30:12.780 --> 02:30:13.780]   I think that's a very good point.
[02:30:13.780 --> 02:30:14.780]   I think that's a very good point.
[02:30:14.780 --> 02:30:15.780]   I think that's a very good point.
[02:30:15.780 --> 02:30:16.780]   I think that's a very good point.
[02:30:16.780 --> 02:30:17.780]   I think that's a very good point.
[02:30:17.780 --> 02:30:18.780]   I think that's a very good point.
[02:30:18.780 --> 02:30:19.780]   I think that's a very good point.
[02:30:19.780 --> 02:30:20.780]   I think that's a very good point.
[02:30:20.780 --> 02:30:21.780]   I think that's a very good point.
[02:30:21.780 --> 02:30:22.780]   I think that's a very good point.
[02:30:22.780 --> 02:30:23.780]   I think that's a very good point.
[02:30:23.780 --> 02:30:24.780]   I think that's a very good point.
[02:30:24.780 --> 02:30:25.780]   I think that's a very good point.
[02:30:25.780 --> 02:30:26.780]   I think that's a very good point.
[02:30:26.780 --> 02:30:27.780]   I think that's a very good point.
[02:30:27.780 --> 02:30:28.780]   I think that's a very good point.
[02:30:28.780 --> 02:30:29.780]   I think that's a very good point.
[02:30:29.780 --> 02:30:30.780]   I think that's a very good point.
[02:30:30.780 --> 02:30:31.780]   I think that's a very good point.
[02:30:31.780 --> 02:30:32.780]   I think that's a very good point.
[02:30:32.780 --> 02:30:33.780]   I think that's a very good point.
[02:30:33.780 --> 02:30:34.780]   I think that's a very good point.
[02:30:34.780 --> 02:30:35.780]   I think that's a very good point.
[02:30:35.780 --> 02:30:36.780]   I think that's a very good point.
[02:30:36.780 --> 02:30:37.780]   I think that's a very good point.
[02:30:37.780 --> 02:30:38.780]   I think that's a very good point.
[02:30:38.780 --> 02:30:39.780]   I think that's a very good point.
[02:30:39.780 --> 02:30:40.780]   I think that's a very good point.
[02:30:40.780 --> 02:30:41.780]   I think that's a very good point.
[02:30:41.780 --> 02:30:42.780]   I think that's a very good point.
[02:30:42.780 --> 02:30:43.780]   I think that's a very good point.
[02:30:43.780 --> 02:30:44.780]   I think that's a very good point.
[02:30:44.780 --> 02:30:45.780]   I think that's a very good point.
[02:30:45.780 --> 02:30:46.780]   I think that's a very good point.
[02:30:46.780 --> 02:30:47.780]   I think that's a very good point.
[02:30:47.780 --> 02:30:48.780]   I think that's a very good point.
[02:30:48.780 --> 02:30:49.780]   I think that's a very good point.
[02:30:49.780 --> 02:30:50.780]   I think that's a very good point.
[02:30:50.780 --> 02:30:51.780]   I think that's a very good point.
[02:30:51.780 --> 02:30:52.780]   I think that's a very good point.
[02:30:52.780 --> 02:30:53.780]   I think that's a very good point.
[02:30:53.780 --> 02:30:54.780]   I think that's a very good point.
[02:30:54.780 --> 02:30:55.780]   I think that's a very good point.
[02:30:55.780 --> 02:30:56.780]   I think that's a very good point.
[02:30:56.780 --> 02:30:57.780]   I think that's a very good point.
[02:30:57.780 --> 02:30:58.780]   I think that's a very good point.
[02:30:58.780 --> 02:30:59.780]   I think that's a very good point.
[02:30:59.780 --> 02:31:00.780]   I think that's a very good point.
[02:31:00.780 --> 02:31:01.780]   I think that's a very good point.
[02:31:01.780 --> 02:31:02.780]   I think that's a very good point.
[02:31:02.780 --> 02:31:03.780]   I think that's a very good point.
[02:31:03.780 --> 02:31:04.780]   I think that's a very good point.
[02:31:04.780 --> 02:31:05.780]   I think that's a very good point.
[02:31:05.780 --> 02:31:06.780]   I think that's a very good point.
[02:31:06.780 --> 02:31:07.780]   I think that's a very good point.
[02:31:07.780 --> 02:31:08.780]   I think that's a very good point.
[02:31:08.780 --> 02:31:09.780]   I think that's a very good point.
[02:31:09.780 --> 02:31:10.780]   I think that's a very good point.
[02:31:10.780 --> 02:31:11.780]   I think that's a very good point.
[02:31:11.780 --> 02:31:12.780]   I think that's a very good point.
[02:31:12.780 --> 02:31:13.780]   I think that's a very good point.
[02:31:13.780 --> 02:31:14.780]   I think that's a very good point.
[02:31:14.780 --> 02:31:15.780]   I think that's a very good point.
[02:31:15.780 --> 02:31:16.780]   I think that's a very good point.
[02:31:16.780 --> 02:31:17.780]   I think that's a very good point.
[02:31:17.780 --> 02:31:18.780]   I think that's a very good point.
[02:31:18.780 --> 02:31:19.780]   I think that's a very good point.
[02:31:19.780 --> 02:31:20.780]   I think that's a very good point.
[02:31:20.780 --> 02:31:21.780]   I think that's a very good point.
[02:31:21.780 --> 02:31:22.780]   I think that's a very good point.
[02:31:22.780 --> 02:31:23.780]   I think that's a very good point.
[02:31:23.780 --> 02:31:24.780]   I think that's a very good point.
[02:31:24.780 --> 02:31:25.780]   I think that's a very good point.
[02:31:25.780 --> 02:31:26.780]   I think that's a very good point.
[02:31:26.780 --> 02:31:27.780]   I think that's a very good point.
[02:31:27.780 --> 02:31:28.780]   I think that's a very good point.
[02:31:28.780 --> 02:31:29.780]   I think that's a very good point.
[02:31:29.780 --> 02:31:30.780]   I think that's a very good point.
[02:31:30.780 --> 02:31:31.780]   I think that's a very good point.
[02:31:31.780 --> 02:31:32.780]   I think that's a very good point.
[02:31:32.780 --> 02:31:33.780]   I think that's a very good point.
[02:31:33.780 --> 02:31:34.780]   I think that's a very good point.
[02:31:34.780 --> 02:31:35.780]   I think that's a very good point.
[02:31:35.780 --> 02:31:36.780]   I think that's a very good point.
[02:31:36.780 --> 02:31:37.780]   I think that's a very good point.
[02:31:37.780 --> 02:31:38.780]   I think that's a very good point.
[02:31:38.780 --> 02:31:39.780]   I think that's a very good point.
[02:31:39.780 --> 02:31:40.780]   I think that's a very good point.
[02:31:40.780 --> 02:31:41.780]   I think that's a very good point.
[02:31:41.780 --> 02:31:42.780]   I think that's a very good point.
[02:31:42.780 --> 02:31:43.780]   I think that's a very good point.
[02:31:43.780 --> 02:31:44.780]   I think that's a very good point.
[02:31:44.780 --> 02:31:45.780]   I think that's a very good point.
[02:31:45.780 --> 02:31:46.780]   I think that's a very good point.
[02:31:46.780 --> 02:31:47.780]   I think that's a very good point.
[02:31:47.780 --> 02:31:48.780]   I think that's a very good point.
[02:31:48.780 --> 02:31:49.780]   I think that's a very good point.
[02:31:49.780 --> 02:31:50.780]   I think that's a very good point.
[02:31:50.780 --> 02:31:51.780]   I think that's a very good point.
[02:31:51.780 --> 02:31:52.780]   I think that's a very good point.
[02:31:52.780 --> 02:31:53.780]   I think that's a very good point.
[02:31:53.780 --> 02:31:54.780]   I think that's a very good point.
[02:31:54.780 --> 02:31:55.780]   I think that's a very good point.
[02:31:55.780 --> 02:31:56.780]   I think that's a very good point.
[02:31:56.780 --> 02:31:57.780]   I think that's a very good point.
[02:31:57.780 --> 02:31:58.780]   I think that's a very good point.
[02:31:58.780 --> 02:31:59.780]   I think that's a very good point.
[02:31:59.780 --> 02:32:00.780]   I think that's a very good point.
[02:32:00.780 --> 02:32:01.780]   I think that's a very good point.
[02:32:01.780 --> 02:32:02.780]   I think that's a very good point.
[02:32:02.780 --> 02:32:03.780]   I think that's a very good point.
[02:32:03.780 --> 02:32:04.780]   I think that's a very good point.
[02:32:04.780 --> 02:32:05.780]   I think that's a very good point.
[02:32:05.780 --> 02:32:06.780]   I think that's a very good point.
[02:32:06.780 --> 02:32:07.780]   I think that's a very good point.
[02:32:07.780 --> 02:32:08.780]   I think that's a very good point.
[02:32:08.780 --> 02:32:09.780]   I think that's a very good point.
[02:32:09.780 --> 02:32:10.780]   I think that's a very good point.
[02:32:10.780 --> 02:32:11.780]   I think that's a very good point.
[02:32:11.780 --> 02:32:12.780]   I think that's a very good point.
[02:32:12.780 --> 02:32:13.780]   I think that's a very good point.
[02:32:13.780 --> 02:32:14.780]   I think that's a very good point.
[02:32:14.780 --> 02:32:15.780]   I think that's a very good point.
[02:32:15.780 --> 02:32:16.780]   I think that's a very good point.
[02:32:16.780 --> 02:32:17.780]   I think that's a very good point.
[02:32:17.780 --> 02:32:18.780]   I think that's a very good point.
[02:32:18.780 --> 02:32:19.780]   I think that's a very good point.
[02:32:19.780 --> 02:32:20.780]   I think that's a very good point.
[02:32:20.780 --> 02:32:21.780]   I think that's a very good point.
[02:32:21.780 --> 02:32:22.780]   I think that's a very good point.
[02:32:22.780 --> 02:32:23.780]   I think that's a very good point.
[02:32:23.780 --> 02:32:24.780]   I think that's a very good point.
[02:32:24.780 --> 02:32:25.780]   I think that's a very good point.
[02:32:25.780 --> 02:32:26.780]   I think that's a very good point.
[02:32:26.780 --> 02:32:27.780]   I think that's a very good point.
[02:32:27.780 --> 02:32:28.780]   I think that's a very good point.
[02:32:28.780 --> 02:32:29.780]   I think that's a very good point.
[02:32:29.780 --> 02:32:30.780]   I think that's a very good point.
[02:32:30.780 --> 02:32:31.780]   I think that's a very good point.
[02:32:31.780 --> 02:32:32.780]   I think that's a very good point.
[02:32:32.780 --> 02:32:33.780]   I think that's a very good point.
[02:32:33.780 --> 02:32:34.780]   I think that's a very good point.
[02:32:34.780 --> 02:32:35.780]   I think that's a very good point.
[02:32:35.780 --> 02:32:36.780]   I think that's a very good point.
[02:32:36.780 --> 02:32:37.780]   I think that's a very good point.
[02:32:37.780 --> 02:32:38.780]   I think that's a very good point.
[02:32:38.780 --> 02:32:39.780]   I think that's a very good point.
[02:32:39.780 --> 02:32:40.780]   I think that's a very good point.
[02:32:40.780 --> 02:32:41.780]   I think that's a very good point.
[02:32:41.780 --> 02:32:42.780]   I think that's a very good point.
[02:32:42.780 --> 02:32:43.780]   I think that's a very good point.
[02:32:43.780 --> 02:32:44.780]   I think that's a very good point.
[02:32:44.780 --> 02:32:45.780]   I think that's a very good point.
[02:32:45.780 --> 02:32:46.780]   I think that's a very good point.
[02:32:46.780 --> 02:32:47.780]   I think that's a very good point.
[02:32:47.780 --> 02:32:48.780]   I think that's a very good point.
[02:32:48.780 --> 02:32:49.780]   I think that's a very good point.
[02:32:49.780 --> 02:32:50.780]   I think that's a very good point.
[02:32:50.780 --> 02:32:51.780]   I think that's a very good point.
[02:32:51.780 --> 02:32:52.780]   I think that's a very good point.
[02:32:52.780 --> 02:32:53.780]   I think that's a very good point.
[02:32:53.780 --> 02:32:54.780]   I think that's a very good point.
[02:32:54.780 --> 02:32:55.780]   I think that's a very good point.
[02:32:55.780 --> 02:32:56.780]   I think that's a very good point.
[02:32:56.780 --> 02:32:57.780]   I think that's a very good point.
[02:32:57.780 --> 02:32:58.780]   I think that's a very good point.
[02:32:58.780 --> 02:32:59.780]   I think that's a very good point.
[02:32:59.780 --> 02:33:00.780]   I think that's a very good point.
[02:33:00.780 --> 02:33:01.780]   I think that's a very good point.
[02:33:01.780 --> 02:33:02.780]   I think that's a very good point.
[02:33:02.780 --> 02:33:03.780]   I think that's a very good point.
[02:33:03.780 --> 02:33:04.780]   I think that's a very good point.
[02:33:04.780 --> 02:33:05.780]   I think that's a very good point.
[02:33:05.780 --> 02:33:06.780]   I think that's a very good point.
[02:33:06.780 --> 02:33:07.780]   I think that's a very good point.
[02:33:07.780 --> 02:33:08.780]   I think that's a very good point.
[02:33:08.780 --> 02:33:09.780]   I think that's a very good point.
[02:33:09.780 --> 02:33:10.780]   I think that's a very good point.
[02:33:10.780 --> 02:33:11.780]   I think that's a very good point.
[02:33:11.780 --> 02:33:12.780]   I think that's a very good point.
[02:33:12.780 --> 02:33:13.780]   I think that's a very good point.
[02:33:13.780 --> 02:33:14.780]   I think that's a very good point.
[02:33:14.780 --> 02:33:15.780]   I think that's a very good point.
[02:33:15.780 --> 02:33:16.780]   I think that's a very good point.
[02:33:16.780 --> 02:33:17.780]   I think that's a very good point.
[02:33:17.780 --> 02:33:18.780]   I think that's a very good point.
[02:33:18.780 --> 02:33:19.780]   I think that's a very good point.
[02:33:19.780 --> 02:33:20.780]   I think that's a very good point.
[02:33:20.780 --> 02:33:21.780]   I think that's a very good point.
[02:33:21.780 --> 02:33:22.780]   I think that's a very good point.
[02:33:22.780 --> 02:33:23.780]   I think that's a very good point.
[02:33:23.780 --> 02:33:24.780]   I think that's a very good point.
[02:33:24.780 --> 02:33:25.780]   I think that's a very good point.
[02:33:25.780 --> 02:33:26.780]   I think that's a very good point.
[02:33:26.780 --> 02:33:27.780]   I think that's a very good point.
[02:33:27.780 --> 02:33:28.780]   I think that's a very good point.
[02:33:28.780 --> 02:33:29.780]   I think that's a very good point.
[02:33:29.780 --> 02:33:30.780]   I think that's a very good point.
[02:33:30.780 --> 02:33:31.780]   I think that's a very good point.
[02:33:31.780 --> 02:33:32.780]   I think that's a very good point.
[02:33:32.780 --> 02:33:33.780]   I think that's a very good point.
[02:33:33.780 --> 02:33:34.780]   I think that's a very good point.
[02:33:34.780 --> 02:33:35.780]   I think that's a very good point.
[02:33:35.780 --> 02:33:36.780]   I think that's a very good point.
[02:33:36.780 --> 02:33:37.780]   I think that's a very good point.
[02:33:37.780 --> 02:33:38.780]   I think that's a very good point.
[02:33:38.780 --> 02:33:39.780]   I think that's a very good point.
[02:33:39.780 --> 02:33:40.780]   I think that's a very good point.
[02:33:40.780 --> 02:33:41.780]   I think that's a very good point.
[02:33:41.780 --> 02:33:42.780]   I think that's a very good point.
[02:33:42.780 --> 02:33:43.780]   I think that's a very good point.
[02:33:43.780 --> 02:33:44.780]   I think that's a very good point.
[02:33:44.780 --> 02:33:45.780]   I think that's a very good point.
[02:33:45.780 --> 02:33:46.780]   I think that's a very good point.
[02:33:46.780 --> 02:33:47.780]   I think that's a very good point.
[02:33:47.780 --> 02:33:48.780]   I think that's a very good point.
[02:33:48.780 --> 02:33:49.780]   I think that's a very good point.
[02:33:49.780 --> 02:33:50.780]   I think that's a very good point.
[02:33:50.780 --> 02:33:51.780]   I think that's a very good point.
[02:33:51.780 --> 02:33:52.780]   I think that's a very good point.
[02:33:52.780 --> 02:33:53.780]   I think that's a very good point.
[02:33:53.780 --> 02:33:54.780]   I think that's a very good point.
[02:33:54.780 --> 02:33:55.780]   I think that's a very good point.
[02:33:55.780 --> 02:33:56.780]   I think that's a very good point.
[02:33:56.780 --> 02:33:57.780]   I think that's a very good point.
[02:33:57.780 --> 02:33:58.780]   I think that's a very good point.
[02:33:58.780 --> 02:33:59.780]   I think that's a very good point.
[02:33:59.780 --> 02:34:00.780]   I think that's a very good point.
[02:34:00.780 --> 02:34:01.780]   I think that's a very good point.
[02:34:01.780 --> 02:34:02.780]   I think that's a very good point.
[02:34:02.780 --> 02:34:03.780]   I think that's a very good point.
[02:34:03.780 --> 02:34:04.780]   I think that's a very good point.
[02:34:04.780 --> 02:34:05.780]   I think that's a very good point.
[02:34:05.780 --> 02:34:06.780]   I think that's a very good point.
[02:34:06.780 --> 02:34:07.780]   I think that's a very good point.
[02:34:07.780 --> 02:34:08.780]   I think that's a very good point.
[02:34:08.780 --> 02:34:09.780]   I think that's a very good point.
[02:34:09.780 --> 02:34:10.780]   I think that's a very good point.
[02:34:10.780 --> 02:34:11.780]   I think that's a very good point.
[02:34:11.780 --> 02:34:12.780]   I think that's a very good point.
[02:34:12.780 --> 02:34:13.780]   I think that's a very good point.
[02:34:13.780 --> 02:34:14.780]   I think that's a very good point.
[02:34:14.780 --> 02:34:15.780]   I think that's a very good point.
[02:34:15.780 --> 02:34:16.780]   I think that's a very good point.
[02:34:16.780 --> 02:34:17.780]   I think that's a very good point.
[02:34:17.780 --> 02:34:18.780]   I think that's a very good point.
[02:34:18.780 --> 02:34:19.780]   I think that's a very good point.
[02:34:19.780 --> 02:34:20.780]   I think that's a very good point.
[02:34:20.780 --> 02:34:21.780]   I think that's a very good point.
[02:34:21.780 --> 02:34:22.780]   I think that's a very good point.
[02:34:22.780 --> 02:34:23.780]   I think that's a very good point.
[02:34:23.780 --> 02:34:24.780]   I think that's a very good point.
[02:34:24.780 --> 02:34:25.780]   I think that's a very good point.
[02:34:25.780 --> 02:34:26.780]   I think that's a very good point.
[02:34:26.780 --> 02:34:27.780]   I think that's a very good point.
[02:34:27.780 --> 02:34:28.780]   I think that's a very good point.
[02:34:28.780 --> 02:34:29.780]   I think that's a very good point.
[02:34:29.780 --> 02:34:30.780]   I think that's a very good point.
[02:34:30.780 --> 02:34:31.780]   I think that's a very good point.
[02:34:31.780 --> 02:34:32.780]   I think that's a very good point.
[02:34:32.780 --> 02:34:33.780]   I think that's a very good point.
[02:34:33.780 --> 02:34:34.780]   I think that's a very good point.
[02:34:34.780 --> 02:34:35.780]   I think that's a very good point.
[02:34:35.780 --> 02:34:36.780]   I think that's a very good point.
[02:34:36.780 --> 02:34:37.780]   I think that's a very good point.
[02:34:37.780 --> 02:34:38.780]   I think that's a very good point.
[02:34:38.780 --> 02:34:39.780]   I think that's a very good point.
[02:34:39.780 --> 02:34:40.780]   I think that's a very good point.
[02:34:40.780 --> 02:34:41.780]   I think that's a very good point.
[02:34:41.780 --> 02:34:42.780]   I think that's a very good point.
[02:34:42.780 --> 02:34:43.780]   I think that's a very good point.
[02:34:43.780 --> 02:34:44.780]   I think that's a very good point.
[02:34:44.780 --> 02:34:45.780]   I think that's a very good point.
[02:34:45.780 --> 02:34:46.780]   I think that's a very good point.
[02:34:46.780 --> 02:34:47.780]   I think that's a very good point.
[02:34:47.780 --> 02:34:48.780]   I think that's a very good point.
[02:34:48.780 --> 02:34:49.780]   I think that's a very good point.
[02:34:49.780 --> 02:34:50.780]   I think that's a very good point.
[02:34:50.780 --> 02:34:51.780]   I think that's a very good point.
[02:34:51.780 --> 02:34:52.780]   I think that's a very good point.
[02:34:52.780 --> 02:34:53.780]   I think that's a very good point.
[02:34:53.780 --> 02:34:54.780]   I think that's a very good point.
[02:34:54.780 --> 02:34:55.780]   I think that's a very good point.
[02:34:55.780 --> 02:34:56.780]   I think that's a very good point.
[02:34:56.780 --> 02:34:57.780]   I think that's a very good point.
[02:34:57.780 --> 02:34:58.780]   I think that's a very good point.
[02:34:58.780 --> 02:34:59.780]   I think that's a very good point.
[02:34:59.780 --> 02:35:00.780]   I think that's a very good point.
[02:35:00.780 --> 02:35:01.780]   I think that's a very good point.
[02:35:01.780 --> 02:35:02.780]   I think that's a very good point.
[02:35:02.780 --> 02:35:03.780]   I think that's a very good point.
[02:35:03.780 --> 02:35:04.780]   I think that's a very good point.
[02:35:04.780 --> 02:35:05.780]   I think that's a very good point.
[02:35:05.780 --> 02:35:06.780]   I think that's a very good point.
[02:35:06.780 --> 02:35:07.780]   I think that's a very good point.
[02:35:07.780 --> 02:35:08.780]   I think that's a very good point.
[02:35:08.780 --> 02:35:09.780]   I think that's a very good point.
[02:35:09.780 --> 02:35:10.780]   I think that's a very good point.
[02:35:10.780 --> 02:35:11.780]   I think that's a very good point.
[02:35:11.780 --> 02:35:12.780]   I think that's a very good point.
[02:35:12.780 --> 02:35:13.780]   I think that's a very good point.
[02:35:13.780 --> 02:35:14.780]   I think that's a very good point.
[02:35:14.780 --> 02:35:15.780]   I think that's a very good point.
[02:35:15.780 --> 02:35:16.780]   I think that's a very good point.
[02:35:16.780 --> 02:35:17.780]   I think that's a very good point.
[02:35:17.780 --> 02:35:18.780]   I think that's a very good point.
[02:35:18.780 --> 02:35:19.780]   I think that's a very good point.
[02:35:19.780 --> 02:35:20.780]   I think that's a very good point.
[02:35:20.780 --> 02:35:21.780]   I think that's a very good point.
[02:35:21.780 --> 02:35:22.780]   I think that's a very good point.
[02:35:22.780 --> 02:35:23.780]   I think that's a very good point.
[02:35:23.780 --> 02:35:24.780]   I think that's a very good point.
[02:35:24.780 --> 02:35:25.780]   I think that's a very good point.
[02:35:25.780 --> 02:35:26.780]   I think that's a very good point.
[02:35:26.780 --> 02:35:27.780]   I think that's a very good point.
[02:35:27.780 --> 02:35:28.780]   I think that's a very good point.
[02:35:28.780 --> 02:35:29.780]   I think that's a very good point.
[02:35:29.780 --> 02:35:30.780]   I think that's a very good point.
[02:35:30.780 --> 02:35:31.780]   I think that's a very good point.
[02:35:31.780 --> 02:35:32.780]   I think that's a very good point.
[02:35:32.780 --> 02:35:33.780]   I think that's a very good point.
[02:35:33.780 --> 02:35:34.780]   I think that's a very good point.
[02:35:34.780 --> 02:35:35.780]   I think that's a very good point.
[02:35:35.780 --> 02:35:36.780]   I think that's a very good point.
[02:35:36.780 --> 02:35:37.780]   I think that's a very good point.
[02:35:37.780 --> 02:35:38.780]   I think that's a very good point.
[02:35:38.780 --> 02:35:39.780]   I think that's a very good point.
[02:35:39.780 --> 02:35:40.780]   I think that's a very good point.
[02:35:40.780 --> 02:35:41.780]   I think that's a very good point.
[02:35:41.780 --> 02:35:42.780]   I think that's a very good point.
[02:35:42.780 --> 02:35:43.780]   I think that's a very good point.
[02:35:43.780 --> 02:35:44.780]   I think that's a very good point.
[02:35:44.780 --> 02:35:45.780]   I think that's a very good point.
[02:35:45.780 --> 02:35:46.780]   I think that's a very good point.
[02:35:46.780 --> 02:35:47.780]   I think that's a very good point.
[02:35:47.780 --> 02:35:48.780]   I think that's a very good point.
[02:35:48.780 --> 02:35:49.780]   I think that's a very good point.
[02:35:49.780 --> 02:35:50.780]   I think that's a very good point.
[02:35:50.780 --> 02:35:51.780]   I think that's a very good point.
[02:35:51.780 --> 02:35:52.780]   I think that's a very good point.
[02:35:52.780 --> 02:35:53.780]   I think that's a very good point.
[02:35:53.780 --> 02:35:54.780]   I think that's a very good point.
[02:35:54.780 --> 02:35:55.780]   I think that's a very good point.
[02:35:55.780 --> 02:35:56.780]   I think that's a very good point.
[02:35:56.780 --> 02:35:57.780]   I think that's a very good point.
[02:35:57.780 --> 02:35:58.780]   I think that's a very good point.
[02:35:58.780 --> 02:35:59.780]   I think that's a very good point.
[02:35:59.780 --> 02:36:00.780]   I think that's a very good point.
[02:36:00.780 --> 02:36:01.780]   I think that's a very good point.
[02:36:01.780 --> 02:36:02.780]   I think that's a very good point.
[02:36:02.780 --> 02:36:03.780]   I think that's a very good point.
[02:36:03.780 --> 02:36:04.780]   I think that's a very good point.
[02:36:04.780 --> 02:36:05.780]   I think that's a very good point.
[02:36:05.780 --> 02:36:06.780]   I think that's a very good point.
[02:36:06.780 --> 02:36:07.780]   I think that's a very good point.
[02:36:07.780 --> 02:36:08.780]   I think that's a very good point.
[02:36:08.780 --> 02:36:09.780]   I think that's a very good point.
[02:36:09.780 --> 02:36:10.780]   I think that's a very good point.
[02:36:10.780 --> 02:36:11.780]   I think that's a very good point.
[02:36:11.780 --> 02:36:12.780]   I think that's a very good point.
[02:36:12.780 --> 02:36:13.780]   I think that's a very good point.
[02:36:13.780 --> 02:36:14.780]   I think that's a very good point.
[02:36:14.780 --> 02:36:15.780]   I think that's a very good point.
[02:36:15.780 --> 02:36:16.780]   I think that's a very good point.
[02:36:16.780 --> 02:36:17.780]   I think that's a very good point.
[02:36:17.780 --> 02:36:18.780]   I think that's a very good point.
[02:36:18.780 --> 02:36:19.780]   I think that's a very good point.
[02:36:19.780 --> 02:36:20.780]   I think that's a very good point.
[02:36:20.780 --> 02:36:21.780]   I think that's a very good point.
[02:36:21.780 --> 02:36:22.780]   I think that's a very good point.
[02:36:22.780 --> 02:36:23.780]   I think that's a very good point.
[02:36:23.780 --> 02:36:24.780]   I think that's a very good point.
[02:36:24.780 --> 02:36:25.780]   I think that's a very good point.
[02:36:25.780 --> 02:36:26.780]   I think that's a very good point.
[02:36:26.780 --> 02:36:27.780]   I think that's a very good point.
[02:36:27.780 --> 02:36:28.780]   I think that's a very good point.
[02:36:28.780 --> 02:36:29.780]   I think that's a very good point.
[02:36:29.780 --> 02:36:30.780]   I think that's a very good point.
[02:36:30.780 --> 02:36:31.780]   I think that's a very good point.
[02:36:31.780 --> 02:36:32.780]   I think that's a very good point.
[02:36:32.780 --> 02:36:33.780]   I think that's a very good point.
[02:36:33.780 --> 02:36:34.780]   I think that's a very good point.
[02:36:34.780 --> 02:36:35.780]   I think that's a very good point.
[02:36:35.780 --> 02:36:36.780]   I think that's a very good point.
[02:36:36.780 --> 02:36:37.780]   I think that's a very good point.
[02:36:37.780 --> 02:36:38.780]   I think that's a very good point.
[02:36:38.780 --> 02:36:39.780]   I think that's a very good point.
[02:36:39.780 --> 02:36:40.780]   I think that's a very good point.
[02:36:40.780 --> 02:36:41.780]   I think that's a very good point.
[02:36:41.780 --> 02:36:42.780]   I think that's a very good point.
[02:36:42.780 --> 02:36:43.780]   I think that's a very good point.
[02:36:43.780 --> 02:36:44.780]   I think that's a very good point.
[02:36:44.780 --> 02:36:45.780]   I think that's a very good point.
[02:36:45.780 --> 02:36:46.780]   I think that's a very good point.
[02:36:46.780 --> 02:36:47.780]   I think that's a very good point.
[02:36:47.780 --> 02:36:48.780]   I think that's a very good point.
[02:36:48.780 --> 02:36:49.780]   I think that's a very good point.
[02:36:49.780 --> 02:36:50.780]   I think that's a very good point.
[02:36:50.780 --> 02:36:51.780]   I think that's a very good point.
[02:36:51.780 --> 02:36:52.780]   I think that's a very good point.
[02:36:52.780 --> 02:36:53.780]   I think that's a very good point.
[02:36:53.780 --> 02:36:54.780]   I think that's a very good point.
[02:36:54.780 --> 02:36:55.780]   I think that's a very good point.
[02:36:55.780 --> 02:36:56.780]   I think that's a very good point.
[02:36:56.780 --> 02:36:57.780]   I think that's a very good point.
[02:36:57.780 --> 02:36:58.780]   I think that's a very good point.
[02:36:58.780 --> 02:36:59.780]   I think that's a very good point.
[02:36:59.780 --> 02:37:00.780]   I think that's a very good point.
[02:37:00.780 --> 02:37:01.780]   I think that's a very good point.
[02:37:01.780 --> 02:37:02.780]   I think that's a very good point.
[02:37:02.780 --> 02:37:03.780]   I think that's a very good point.
[02:37:03.780 --> 02:37:04.780]   I think that's a very good point.
[02:37:04.780 --> 02:37:05.780]   I think that's a very good point.
[02:37:05.780 --> 02:37:06.780]   I think that's a very good point.
[02:37:06.780 --> 02:37:07.780]   I think that's a very good point.
[02:37:07.780 --> 02:37:08.780]   I think that's a very good point.
[02:37:08.780 --> 02:37:09.780]   I think that's a very good point.
[02:37:09.780 --> 02:37:10.780]   I think that's a very good point.
[02:37:10.780 --> 02:37:11.780]   I think that's a very good point.
[02:37:11.780 --> 02:37:12.780]   I think that's a very good point.
[02:37:12.780 --> 02:37:13.780]   I think that's a very good point.
[02:37:13.780 --> 02:37:14.780]   I think that's a very good point.
[02:37:14.780 --> 02:37:15.780]   I think that's a very good point.
[02:37:15.780 --> 02:37:16.780]   I think that's a very good point.
[02:37:16.780 --> 02:37:17.780]   I think that's a very good point.
[02:37:17.780 --> 02:37:18.780]   I think that's a very good point.
[02:37:18.780 --> 02:37:19.780]   I think that's a very good point.
[02:37:19.780 --> 02:37:20.780]   I think that's a very good point.
[02:37:20.780 --> 02:37:21.780]   I think that's a very good point.
[02:37:21.780 --> 02:37:22.780]   I think that's a very good point.
[02:37:22.780 --> 02:37:23.780]   I think that's a very good point.
[02:37:23.780 --> 02:37:24.780]   I think that's a very good point.
[02:37:24.780 --> 02:37:25.780]   I think that's a very good point.
[02:37:25.780 --> 02:37:26.780]   I think that's a very good point.
[02:37:26.780 --> 02:37:27.780]   I think that's a very good point.
[02:37:27.780 --> 02:37:28.780]   I think that's a very good point.
[02:37:28.780 --> 02:37:29.780]   I think that's a very good point.
[02:37:29.780 --> 02:37:30.780]   I think that's a very good point.
[02:37:30.780 --> 02:37:31.780]   I think that's a very good point.
[02:37:31.780 --> 02:37:32.780]   I think that's a very good point.
[02:37:32.780 --> 02:37:33.780]   I think that's a very good point.
[02:37:33.780 --> 02:37:34.780]   I think that's a very good point.
[02:37:34.780 --> 02:37:35.780]   I think that's a very good point.
[02:37:35.780 --> 02:37:36.780]   I think that's a very good point.
[02:37:36.780 --> 02:37:37.780]   I think that's a very good point.
[02:37:37.780 --> 02:37:38.780]   I think that's a very good point.
[02:37:38.780 --> 02:37:39.780]   I think that's a very good point.
[02:37:39.780 --> 02:37:40.780]   I think that's a very good point.
[02:37:40.780 --> 02:37:41.780]   I think that's a very good point.
[02:37:41.780 --> 02:37:42.780]   I think that's a very good point.
[02:37:42.780 --> 02:37:43.780]   I think that's a very good point.
[02:37:43.780 --> 02:37:44.780]   I think that's a very good point.
[02:37:44.780 --> 02:37:45.780]   I think that's a very good point.
[02:37:45.780 --> 02:37:46.780]   I think that's a very good point.
[02:37:46.780 --> 02:37:47.780]   I think that's a very good point.
[02:37:47.780 --> 02:37:48.780]   I think that's a very good point.
[02:37:48.780 --> 02:37:49.780]   I think that's a very good point.
[02:37:49.780 --> 02:37:50.780]   I think that's a very good point.
[02:37:50.780 --> 02:37:51.780]   I think that's a very good point.
[02:37:51.780 --> 02:37:52.780]   I think that's a very good point.
[02:37:52.780 --> 02:37:53.780]   I think that's a very good point.
[02:37:53.780 --> 02:37:54.780]   I think that's a very good point.
[02:37:54.780 --> 02:37:55.780]   I think that's a very good point.
[02:37:55.780 --> 02:37:56.780]   I think that's a very good point.
[02:37:56.780 --> 02:37:57.780]   I think that's a very good point.
[02:37:57.780 --> 02:37:58.780]   I think that's a very good point.
[02:37:58.780 --> 02:37:59.780]   I think that's a very good point.
[02:37:59.780 --> 02:38:00.780]   I think that's a very good point.
[02:38:00.780 --> 02:38:01.780]   I think that's a very good point.
[02:38:01.780 --> 02:38:02.780]   I think that's a very good point.
[02:38:02.780 --> 02:38:03.780]   I think that's a very good point.
[02:38:03.780 --> 02:38:04.780]   I think that's a very good point.
[02:38:04.780 --> 02:38:05.780]   I think that's a very good point.
[02:38:05.780 --> 02:38:06.780]   I think that's a very good point.
[02:38:06.780 --> 02:38:07.780]   I think that's a very good point.
[02:38:07.780 --> 02:38:08.780]   I think that's a very good point.
[02:38:08.780 --> 02:38:09.780]   I think that's a very good point.
[02:38:09.780 --> 02:38:10.780]   I think that's a very good point.
[02:38:10.780 --> 02:38:11.780]   I think that's a very good point.
[02:38:11.780 --> 02:38:12.780]   I think that's a very good point.
[02:38:12.780 --> 02:38:13.780]   I think that's a very good point.
[02:38:13.780 --> 02:38:14.780]   I think that's a very good point.
[02:38:14.780 --> 02:38:15.780]   I think that's a very good point.
[02:38:15.780 --> 02:38:16.780]   I think that's a very good point.
[02:38:16.780 --> 02:38:17.780]   I think that's a very good point.
[02:38:17.780 --> 02:38:18.780]   I think that's a very good point.
[02:38:18.780 --> 02:38:19.780]   I think that's a very good point.
[02:38:19.780 --> 02:38:20.780]   I think that's a very good point.
[02:38:20.780 --> 02:38:21.780]   I think that's a very good point.
[02:38:21.780 --> 02:38:22.780]   I think that's a very good point.
[02:38:22.780 --> 02:38:23.780]   I think that's a very good point.
[02:38:23.780 --> 02:38:24.780]   I think that's a very good point.
[02:38:24.780 --> 02:38:25.780]   I think that's a very good point.
[02:38:25.780 --> 02:38:26.780]   I think that's a very good point.
[02:38:26.780 --> 02:38:27.780]   I think that's a very good point.
[02:38:27.780 --> 02:38:28.780]   I think that's a very good point.
[02:38:28.780 --> 02:38:29.780]   I think that's a very good point.
[02:38:29.780 --> 02:38:30.780]   I think that's a very good point.
[02:38:30.780 --> 02:38:31.780]   I think that's a very good point.
[02:38:31.780 --> 02:38:32.780]   I think that's a very good point.
[02:38:32.780 --> 02:38:33.780]   I think that's a very good point.
[02:38:33.780 --> 02:38:34.780]   I think that's a very good point.
[02:38:34.780 --> 02:38:35.780]   I think that's a very good point.
[02:38:35.780 --> 02:38:36.780]   I think that's a very good point.
[02:38:36.780 --> 02:38:37.780]   I think that's a very good point.
[02:38:37.780 --> 02:38:38.780]   I think that's a very good point.
[02:38:38.780 --> 02:38:39.780]   I think that's a very good point.
[02:38:39.780 --> 02:38:40.780]   I think that's a very good point.
[02:38:40.780 --> 02:38:41.780]   I think that's a very good point.
[02:38:41.780 --> 02:38:42.780]   I think that's a very good point.
[02:38:42.780 --> 02:38:43.780]   I think that's a very good point.
[02:38:43.780 --> 02:38:44.780]   I think that's a very good point.
[02:38:44.780 --> 02:38:45.780]   I think that's a very good point.
[02:38:45.780 --> 02:38:46.780]   I think that's a very good point.
[02:38:46.780 --> 02:38:47.780]   I think that's a very good point.
[02:38:47.780 --> 02:38:48.780]   I think that's a very good point.
[02:38:48.780 --> 02:38:49.780]   I think that's a very good point.
[02:38:49.780 --> 02:38:50.780]   I think that's a very good point.
[02:38:50.780 --> 02:38:51.780]   I think that's a very good point.
[02:38:51.780 --> 02:38:52.780]   I think that's a very good point.
[02:38:52.780 --> 02:38:53.780]   I think that's a very good point.
[02:38:53.780 --> 02:38:54.780]   I think that's a very good point.
[02:38:54.780 --> 02:38:55.780]   I think that's a very good point.
[02:38:55.780 --> 02:38:56.780]   I think that's a very good point.
[02:38:56.780 --> 02:38:57.780]   I think that's a very good point.
[02:38:57.780 --> 02:38:58.780]   I think that's a very good point.
[02:38:58.780 --> 02:38:59.780]   I think that's a very good point.
[02:38:59.780 --> 02:39:00.780]   I think that's a very good point.
[02:39:00.780 --> 02:39:01.780]   I think that's a very good point.
[02:39:01.780 --> 02:39:02.780]   I think that's a very good point.
[02:39:02.780 --> 02:39:03.780]   I think that's a very good point.
[02:39:03.780 --> 02:39:04.780]   I think that's a very good point.
[02:39:04.780 --> 02:39:05.780]   I think that's a very good point.
[02:39:05.780 --> 02:39:06.780]   I think that's a very good point.
[02:39:06.780 --> 02:39:07.780]   I think that's a very good point.
[02:39:07.780 --> 02:39:08.780]   I think that's a very good point.
[02:39:08.780 --> 02:39:09.780]   I think that's a very good point.
[02:39:09.780 --> 02:39:10.780]   I think that's a very good point.
[02:39:10.780 --> 02:39:11.780]   I think that's a very good point.
[02:39:11.780 --> 02:39:12.780]   I think that's a very good point.
[02:39:12.780 --> 02:39:13.780]   I think that's a very good point.
[02:39:13.780 --> 02:39:14.780]   I think that's a very good point.
[02:39:14.780 --> 02:39:15.780]   I think that's a very good point.
[02:39:15.780 --> 02:39:16.780]   I think that's a very good point.
[02:39:16.780 --> 02:39:17.780]   I think that's a very good point.
[02:39:17.780 --> 02:39:18.780]   I think that's a very good point.
[02:39:18.780 --> 02:39:19.780]   I think that's a very good point.
[02:39:19.780 --> 02:39:20.780]   I think that's a very good point.
[02:39:20.780 --> 02:39:21.780]   I think that's a very good point.
[02:39:21.780 --> 02:39:22.780]   I think that's a very good point.
[02:39:22.780 --> 02:39:23.780]   I think that's a very good point.
[02:39:23.780 --> 02:39:24.780]   I think that's a very good point.
[02:39:24.780 --> 02:39:25.780]   I think that's a very good point.
[02:39:25.780 --> 02:39:26.780]   I think that's a very good point.
[02:39:26.780 --> 02:39:27.780]   I think that's a very good point.
[02:39:27.780 --> 02:39:28.780]   I think that's a very good point.
[02:39:28.780 --> 02:39:29.780]   I think that's a very good point.
[02:39:29.780 --> 02:39:30.780]   I think that's a very good point.
[02:39:30.780 --> 02:39:31.780]   I think that's a very good point.
[02:39:31.780 --> 02:39:32.780]   I think that's a very good point.
[02:39:32.780 --> 02:39:33.780]   I think that's a very good point.
[02:39:33.780 --> 02:39:34.780]   I think that's a very good point.
[02:39:34.780 --> 02:39:35.780]   I think that's a very good point.
[02:39:35.780 --> 02:39:36.780]   I think that's a very good point.
[02:39:36.780 --> 02:39:37.780]   I think that's a very good point.
[02:39:37.780 --> 02:39:38.780]   I think that's a very good point.
[02:39:38.780 --> 02:39:39.780]   I think that's a very good point.
[02:39:39.780 --> 02:39:40.780]   I think that's a very good point.
[02:39:40.780 --> 02:39:41.780]   I think that's a very good point.
[02:39:41.780 --> 02:39:42.780]   I think that's a very good point.
[02:39:42.780 --> 02:39:43.780]   I think that's a very good point.
[02:39:43.780 --> 02:39:44.780]   I think that's a very good point.
[02:39:44.780 --> 02:39:45.780]   I think that's a very good point.
[02:39:45.780 --> 02:39:46.780]   I think that's a very good point.
[02:39:46.780 --> 02:39:47.780]   I think that's a very good point.
[02:39:47.780 --> 02:39:48.780]   I think that's a very good point.
[02:39:48.780 --> 02:39:49.780]   I think that's a very good point.
[02:39:49.780 --> 02:39:50.780]   I think that's a very good point.
[02:39:50.780 --> 02:39:51.780]   I think that's a very good point.
[02:39:51.780 --> 02:39:52.780]   I think that's a very good point.
[02:39:52.780 --> 02:39:53.780]   I think that's a very good point.
[02:39:53.780 --> 02:39:54.780]   I think that's a very good point.
[02:39:54.780 --> 02:39:55.780]   I think that's a very good point.
[02:39:55.780 --> 02:39:56.780]   I think that's a very good point.
[02:39:56.780 --> 02:39:57.780]   I think that's a very good point.
[02:39:57.780 --> 02:39:58.780]   I think that's a very good point.
[02:39:58.780 --> 02:39:59.780]   I think that's a very good point.
[02:39:59.780 --> 02:40:00.780]   I think that's a very good point.
[02:40:00.780 --> 02:40:01.780]   I think that's a very good point.
[02:40:01.780 --> 02:40:02.780]   I think that's a very good point.
[02:40:02.780 --> 02:40:03.780]   I think that's a very good point.
[02:40:03.780 --> 02:40:04.780]   I think that's a very good point.
[02:40:04.780 --> 02:40:05.780]   I think that's a very good point.
[02:40:05.780 --> 02:40:06.780]   I think that's a very good point.
[02:40:06.780 --> 02:40:07.780]   I think that's a very good point.
[02:40:07.780 --> 02:40:08.780]   I think that's a very good point.
[02:40:08.780 --> 02:40:09.780]   I think that's a very good point.
[02:40:09.780 --> 02:40:10.780]   I think that's a very good point.
[02:40:10.780 --> 02:40:11.780]   I think that's a very good point.
[02:40:11.780 --> 02:40:12.780]   I think that's a very good point.
[02:40:12.780 --> 02:40:13.780]   I think that's a very good point.
[02:40:13.780 --> 02:40:14.780]   I think that's a very good point.
[02:40:14.780 --> 02:40:15.780]   I think that's a very good point.
[02:40:15.780 --> 02:40:16.780]   I think that's a very good point.
[02:40:16.780 --> 02:40:17.780]   I think that's a very good point.
[02:40:17.780 --> 02:40:18.780]   I think that's a very good point.
[02:40:18.780 --> 02:40:19.780]   I think that's a very good point.
[02:40:19.780 --> 02:40:20.780]   I think that's a very good point.
[02:40:20.780 --> 02:40:21.780]   I think that's a very good point.
[02:40:21.780 --> 02:40:22.780]   I think that's a very good point.
[02:40:22.780 --> 02:40:23.780]   I think that's a very good point.
[02:40:23.780 --> 02:40:24.780]   I think that's a very good point.
[02:40:24.780 --> 02:40:25.780]   I think that's a very good point.
[02:40:25.780 --> 02:40:26.780]   I think that's a very good point.
[02:40:26.780 --> 02:40:27.780]   I think that's a very good point.
[02:40:27.780 --> 02:40:28.780]   I think that's a very good point.
[02:40:28.780 --> 02:40:29.780]   I think that's a very good point.
[02:40:29.780 --> 02:40:30.780]   I think that's a very good point.
[02:40:30.780 --> 02:40:31.780]   I think that's a very good point.
[02:40:31.780 --> 02:40:32.780]   I think that's a very good point.
[02:40:32.780 --> 02:40:33.780]   I think that's a very good point.
[02:40:33.780 --> 02:40:34.780]   I think that's a very good point.
[02:40:34.780 --> 02:40:35.780]   I think that's a very good point.
[02:40:35.780 --> 02:40:36.780]   I think that's a very good point.
[02:40:36.780 --> 02:40:37.780]   I think that's a very good point.
[02:40:37.780 --> 02:40:38.780]   I think that's a very good point.
[02:40:38.780 --> 02:40:39.780]   I think that's a very good point.
[02:40:39.780 --> 02:40:40.780]   I think that's a very good point.
[02:40:40.780 --> 02:40:41.780]   I think that's a very good point.
[02:40:41.780 --> 02:40:42.780]   I think that's a very good point.
[02:40:42.780 --> 02:40:43.780]   I think that's a very good point.
[02:40:43.780 --> 02:40:44.780]   I think that's a very good point.
[02:40:44.780 --> 02:40:45.780]   I think that's a very good point.
[02:40:45.780 --> 02:40:46.780]   I think that's a very good point.
[02:40:46.780 --> 02:40:47.780]   I think that's a very good point.
[02:40:47.780 --> 02:40:48.780]   I think that's a very good point.
[02:40:48.780 --> 02:40:49.780]   I think that's a very good point.
[02:40:49.780 --> 02:40:50.780]   I think that's a very good point.
[02:40:50.780 --> 02:40:51.780]   I think that's a very good point.
[02:40:51.780 --> 02:40:52.780]   I think that's a very good point.
[02:40:52.780 --> 02:40:53.780]   I think that's a very good point.
[02:40:53.780 --> 02:40:54.780]   I think that's a very good point.
[02:40:54.780 --> 02:40:55.780]   I think that's a very good point.
[02:40:55.780 --> 02:40:56.780]   I think that's a very good point.
[02:40:56.780 --> 02:40:57.780]   I think that's a very good point.
[02:40:57.780 --> 02:40:58.780]   I think that's a very good point.
[02:40:58.780 --> 02:40:59.780]   I think that's a very good point.
[02:40:59.780 --> 02:41:00.780]   I think that's a very good point.
[02:41:00.780 --> 02:41:01.780]   I think that's a very good point.
[02:41:01.780 --> 02:41:02.780]   I think that's a very good point.
[02:41:02.780 --> 02:41:03.780]   I think that's a very good point.
[02:41:03.780 --> 02:41:04.780]   I think that's a very good point.
[02:41:04.780 --> 02:41:05.780]   I think that's a very good point.
[02:41:05.780 --> 02:41:06.780]   I think that's a very good point.
[02:41:06.780 --> 02:41:07.780]   I think that's a very good point.
[02:41:07.780 --> 02:41:08.780]   I think that's a very good point.
[02:41:08.780 --> 02:41:09.780]   I think that's a very good point.
[02:41:09.780 --> 02:41:10.780]   I think that's a very good point.
[02:41:10.780 --> 02:41:11.780]   I think that's a very good point.
[02:41:11.780 --> 02:41:12.780]   I think that's a very good point.
[02:41:12.780 --> 02:41:13.780]   I think that's a very good point.
[02:41:13.780 --> 02:41:14.780]   I think that's a very good point.
[02:41:14.780 --> 02:41:15.780]   I think that's a very good point.
[02:41:15.780 --> 02:41:16.780]   I think that's a very good point.
[02:41:16.780 --> 02:41:17.780]   I think that's a very good point.
[02:41:17.780 --> 02:41:18.780]   I think that's a very good point.
[02:41:18.780 --> 02:41:19.780]   I think that's a very good point.
[02:41:19.780 --> 02:41:20.780]   I think that's a very good point.
[02:41:20.780 --> 02:41:21.780]   I think that's a very good point.
[02:41:21.780 --> 02:41:22.780]   I think that's a very good point.
[02:41:22.780 --> 02:41:23.780]   I think that's a very good point.
[02:41:23.780 --> 02:41:24.780]   I think that's a very good point.
[02:41:24.780 --> 02:41:25.780]   I think that's a very good point.
[02:41:25.780 --> 02:41:26.780]   I think that's a very good point.
[02:41:26.780 --> 02:41:27.780]   I think that's a very good point.
[02:41:27.780 --> 02:41:28.780]   I think that's a very good point.
[02:41:28.780 --> 02:41:29.780]   I think that's a very good point.
[02:41:29.780 --> 02:41:30.780]   I think that's a very good point.
[02:41:30.780 --> 02:41:31.780]   I think that's a very good point.
[02:41:31.780 --> 02:41:32.780]   I think that's a very good point.
[02:41:32.780 --> 02:41:33.780]   I think that's a very good point.
[02:41:33.780 --> 02:41:34.780]   I think that's a very good point.
[02:41:34.780 --> 02:41:35.780]   I think that's a very good point.
[02:41:35.780 --> 02:41:36.780]   I think that's a very good point.
[02:41:36.780 --> 02:41:37.780]   I think that's a very good point.
[02:41:37.780 --> 02:41:38.780]   I think that's a very good point.
[02:41:38.780 --> 02:41:39.780]   I think that's a very good point.
[02:41:39.780 --> 02:41:40.780]   I think that's a very good point.
[02:41:40.780 --> 02:41:41.780]   I think that's a very good point.
[02:41:41.780 --> 02:41:42.780]   I think that's a very good point.
[02:41:42.780 --> 02:41:43.780]   I think that's a very good point.
[02:41:43.780 --> 02:41:44.780]   I think that's a very good point.
[02:41:44.780 --> 02:41:45.780]   I think that's a very good point.
[02:41:45.780 --> 02:41:46.780]   I think that's a very good point.
[02:41:46.780 --> 02:41:47.780]   I think that's a very good point.
[02:41:47.780 --> 02:41:48.780]   I think that's a very good point.
[02:41:48.780 --> 02:41:49.780]   I think that's a very good point.
[02:41:49.780 --> 02:41:50.780]   I think that's a very good point.
[02:41:50.780 --> 02:41:51.780]   I think that's a very good point.
[02:41:51.780 --> 02:41:52.780]   I think that's a very good point.
[02:41:52.780 --> 02:41:53.780]   I think that's a very good point.
[02:41:53.780 --> 02:41:54.780]   I think that's a very good point.
[02:41:54.780 --> 02:41:55.780]   I think that's a very good point.
[02:41:55.780 --> 02:41:56.780]   I think that's a very good point.
[02:41:56.780 --> 02:41:57.780]   I think that's a very good point.

