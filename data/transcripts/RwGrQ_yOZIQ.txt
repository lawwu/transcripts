
[00:00:00.000 --> 00:00:07.640]   We need to train machines up that can help us establish ground truth so that when new
[00:00:07.640 --> 00:00:13.440]   information comes available, we can measure it up against that and say, is this consistent
[00:00:13.440 --> 00:00:15.180]   or is this contradictory?
[00:00:15.180 --> 00:00:18.680]   Now just because it's contradictory to ground truth doesn't make it false, but it does mean
[00:00:18.680 --> 00:00:20.640]   you want to look closer at it.
[00:00:20.640 --> 00:00:25.080]   And this is kind of, I think, as we build up defenses for democracy, we need, you know,
[00:00:25.080 --> 00:00:28.480]   and I've talked about this, a Manhattan Project to establish ground truth.
[00:00:28.480 --> 00:00:32.140]   It's going to take a lot of work and a lot of effort, but it's very, very hard to see
[00:00:32.140 --> 00:00:37.820]   a democracy functioning if we can't establish information provenance, if we can't establish
[00:00:37.820 --> 00:00:40.800]   whether information is likely to be part of a manipulative attack.
[00:00:40.800 --> 00:00:45.080]   And if we don't have any infrastructure to kind of lean back on and say, well, here's
[00:00:45.080 --> 00:00:49.440]   what we do know about the world and here's what we do understand with it.
[00:00:49.440 --> 00:00:54.360]   And so this is a big problem, I think, for democracies and we need a way around it.
[00:00:54.360 --> 00:00:57.240]   It's an asymmetric fight, but it's one that we have to win.
[00:00:57.240 --> 00:01:01.080]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:01:01.080 --> 00:01:02.720]   models work in the real world.
[00:01:02.720 --> 00:01:04.840]   I'm your host, Lukas Biewald.
[00:01:04.840 --> 00:01:09.440]   Sean Gurley is the founder and CEO of Primer, a natural language processing startup in San
[00:01:09.440 --> 00:01:10.440]   Francisco.
[00:01:10.440 --> 00:01:15.840]   Previously, he was CTO of Quid, an augmented intelligence company that he co-founded back
[00:01:15.840 --> 00:01:17.340]   in 2009.
[00:01:17.340 --> 00:01:22.240]   And prior to that, he worked on self-repairing nanocircuits at NASA Ames.
[00:01:22.240 --> 00:01:26.940]   Sean also has a PhD in physics from Oxford, where his research as a Rhodes Scholar focused
[00:01:26.940 --> 00:01:32.440]   on graph theory, complex systems, and the mathematical patterns underlying modern war.
[00:01:32.440 --> 00:01:36.200]   I'm super excited to talk to him today.
[00:01:36.200 --> 00:01:37.400]   So Sean, it's great to talk to you.
[00:01:37.400 --> 00:01:39.120]   I mean, I really appreciate you taking the time.
[00:01:39.120 --> 00:01:42.800]   The first thing I want to ask you, since you're an entrepreneur and so am I, is tell me about
[00:01:42.800 --> 00:01:43.800]   your company Primer.
[00:01:43.800 --> 00:01:46.000]   I'm sure you want to talk about it.
[00:01:46.000 --> 00:01:50.480]   We're a company that specializes in training machine learning models to understand language,
[00:01:50.480 --> 00:01:54.140]   to replicate different kinds of human tasks that run on top of language.
[00:01:54.140 --> 00:02:00.280]   Everything from identifying key bits of information, to summarizing documents, to extracting relationships
[00:02:00.280 --> 00:02:02.720]   between entities for a knowledge graph.
[00:02:02.720 --> 00:02:07.400]   We also do a lot of work on language generation as well, and particularly fact-aware language
[00:02:07.400 --> 00:02:08.400]   generation.
[00:02:08.400 --> 00:02:11.880]   So we spend a lot of time trying to teach our machines not to hallucinate, which tends
[00:02:11.880 --> 00:02:15.520]   to be one of the issues of these transformer-based models.
[00:02:15.520 --> 00:02:19.200]   And so it's really interesting when you're in this kind of world of machines that dream
[00:02:19.200 --> 00:02:21.000]   and try to teach them not to.
[00:02:21.000 --> 00:02:26.400]   But the goal for us is to take human actions on top of text and automate them at scale
[00:02:26.400 --> 00:02:30.080]   so that you can kind of find insights that no individual human would be able to see by
[00:02:30.080 --> 00:02:31.080]   themselves.
[00:02:31.080 --> 00:02:34.200]   And we've had a lot of success in doing that over the last few years.
[00:02:34.200 --> 00:02:39.920]   And is your goal to kind of make these individual tasks available to someone who wanted to use
[00:02:39.920 --> 00:02:43.400]   them, or is it to deliver these insights to a customer?
[00:02:43.400 --> 00:02:47.480]   I think the goal for us is ultimately to deliver these tools to the customer so that they can
[00:02:47.480 --> 00:02:51.480]   take actions that were done by humans and ultimately automate them.
[00:02:51.480 --> 00:02:56.200]   Now you get the automation, but if you do it at scale, then all of a sudden you do get
[00:02:56.200 --> 00:02:59.320]   these insights that no individual human would have found.
[00:02:59.320 --> 00:03:02.480]   What we've found though, as we've gone through that, is that the internal kind of data science
[00:03:02.480 --> 00:03:07.000]   teams within these organizations have said, look, we'd love to kind of have these different
[00:03:07.000 --> 00:03:08.580]   components that you've built.
[00:03:08.580 --> 00:03:12.800]   And so we've also been able to sell the different API components to users as well.
[00:03:12.800 --> 00:03:17.080]   But look, the end goal for us is to make this available for users with no technical knowledge
[00:03:17.080 --> 00:03:19.440]   and that's where we're focusing.
[00:03:19.440 --> 00:03:24.160]   And do you have a particular end user or domain that you care about, or is this like a broad
[00:03:24.160 --> 00:03:26.080]   based platform for insights?
[00:03:26.080 --> 00:03:29.600]   Yeah, look, so we've been focused on defense from day one.
[00:03:29.600 --> 00:03:33.880]   And my background, my PhD work was in the mathematical dynamics of insurgency.
[00:03:33.880 --> 00:03:38.040]   And so I spent a lot of time in the world of intelligence and defense.
[00:03:38.040 --> 00:03:40.800]   I think they have a really particularly useful use case.
[00:03:40.800 --> 00:03:45.120]   They spend a lot of time dealing with text-based information, perhaps more than anyone else
[00:03:45.120 --> 00:03:46.120]   in the world.
[00:03:46.120 --> 00:03:50.880]   And so if you're an analyst sitting there inside of a three-letter agency, you're going
[00:03:50.880 --> 00:03:54.920]   to be dealing with hundreds of thousands of text-based documents coming across your feed
[00:03:54.920 --> 00:03:56.120]   every day.
[00:03:56.120 --> 00:04:00.080]   And I think there's no surprise to anyone in the industry that that's just not a scalable
[00:04:00.080 --> 00:04:01.320]   human task.
[00:04:01.320 --> 00:04:02.600]   So we were able to go into that.
[00:04:02.600 --> 00:04:05.720]   I think there's three things that make that really attractive for us.
[00:04:05.720 --> 00:04:07.220]   One is the volume of text.
[00:04:07.220 --> 00:04:12.840]   I think the second is that any edge that you can get as an intelligence or defense operator
[00:04:12.840 --> 00:04:15.120]   or analyst, you're going to want to take that.
[00:04:15.120 --> 00:04:18.840]   And then the third thing is, we've seen a really, really good defensibility once you're
[00:04:18.840 --> 00:04:20.180]   in and deployed.
[00:04:20.180 --> 00:04:23.480]   And these organizations, there's a two-year process to get in there.
[00:04:23.480 --> 00:04:27.200]   And so it's a good market to kind of land in once you've deployed your technology and
[00:04:27.200 --> 00:04:29.200]   got it working.
[00:04:29.200 --> 00:04:35.080]   Has the state of the art in natural language processing changed to enable a company like
[00:04:35.080 --> 00:04:38.320]   this or is there some specific insight that you felt you had?
[00:04:38.320 --> 00:04:41.360]   Or how do you think about that, this moment for your company?
[00:04:41.360 --> 00:04:42.360]   Yeah, look.
[00:04:42.600 --> 00:04:44.640]   When I started this, it was sort of 2015.
[00:04:44.640 --> 00:04:48.560]   I was watching, as you probably were, a lot of my friends and a lot of our friends would
[00:04:48.560 --> 00:04:52.720]   have been playing with neural nets and doing image processing.
[00:04:52.720 --> 00:04:57.160]   And I remember Jeremy Howard was showing me some of the stuff he was doing with caption
[00:04:57.160 --> 00:04:59.560]   generation on top of images.
[00:04:59.560 --> 00:05:01.880]   And I remember watching that and seeing the caption generation piece.
[00:05:01.880 --> 00:05:04.240]   And I was like, this is going to come to language, right?
[00:05:04.240 --> 00:05:06.480]   These technologies is going to come to language.
[00:05:06.480 --> 00:05:11.120]   And so that was sort of end of 2014, start of 15, watching friends do that.
[00:05:11.120 --> 00:05:16.760]   And for me, I made a bet and said, look, we've seen computer vision go from 30% error rates
[00:05:16.760 --> 00:05:20.760]   to 5% error rates with these new neural approaches.
[00:05:20.760 --> 00:05:23.840]   And language felt like the next logical place that that had happened.
[00:05:23.840 --> 00:05:28.600]   I think if I'm honest, the first two to three years of the company, the technology hadn't
[00:05:28.600 --> 00:05:29.960]   caught up to the vision.
[00:05:29.960 --> 00:05:34.480]   But then we saw transformer-based models emerge and that's just been a game changer.
[00:05:34.480 --> 00:05:39.640]   And what that's meant for customers is it's meant that these are actually trainable, which
[00:05:39.640 --> 00:05:44.600]   means they can be customizable, which means that you can actually start to deploy them
[00:05:44.600 --> 00:05:46.600]   to a pretty diverse set of use cases.
[00:05:46.600 --> 00:05:50.040]   So you mean like fine-tuned or something on their own datasets?
[00:05:50.040 --> 00:05:51.040]   Yeah.
[00:05:51.040 --> 00:05:54.520]   So instead of having to kind of train with hundreds of thousands of documents and data
[00:05:54.520 --> 00:05:58.900]   points and training examples, you can start with a model that's got a pretty good embedding
[00:05:58.900 --> 00:06:02.240]   structure from reading kind of general information.
[00:06:02.240 --> 00:06:05.960]   And then you can retrain that obviously on a fraction of the information that would otherwise
[00:06:05.960 --> 00:06:07.440]   have been required.
[00:06:07.440 --> 00:06:11.760]   So I think that's probably the single biggest thing and that allows users to engage with
[00:06:11.760 --> 00:06:12.760]   this technology.
[00:06:12.760 --> 00:06:16.440]   I think with what we talk about, what's your return on investment for the time you want
[00:06:16.440 --> 00:06:18.740]   to take to train this or to get a payoff?
[00:06:18.740 --> 00:06:22.360]   And that's come down significantly with these models.
[00:06:22.360 --> 00:06:27.280]   You do a wide range of kind of traditional NLP use cases.
[00:06:27.280 --> 00:06:30.440]   Which ones have you seen the biggest change and maybe which ones have you still kind of
[00:06:30.440 --> 00:06:32.880]   not seen the improvement from this new technology?
[00:06:32.880 --> 00:06:34.960]   Yeah, that's a good question.
[00:06:34.960 --> 00:06:41.480]   When we started language generation, it was sort of recursive neural nets and LSTMs and
[00:06:41.480 --> 00:06:44.680]   you couldn't really generate a sentence with any kind of credible output.
[00:06:44.680 --> 00:06:50.940]   So the idea of even kind of like doing a multi-paragraph summary of a document was just science fiction.
[00:06:50.940 --> 00:06:54.280]   So there's stuff that this technology has enabled that you just couldn't have done.
[00:06:54.280 --> 00:07:00.400]   I think the second bit here is the idea of training a model with a few dozen examples
[00:07:00.400 --> 00:07:04.120]   to pick up a relationship extraction between two entities.
[00:07:04.120 --> 00:07:06.880]   And that was a scientific paper that you had to write.
[00:07:06.880 --> 00:07:12.440]   So there's stuff that this has enabled that just wasn't even within the realm.
[00:07:12.440 --> 00:07:18.260]   I think where this has come, where it hasn't had as big an impact, I think it's really
[00:07:18.260 --> 00:07:21.600]   only limited by the training data that you're so willing to throw at it.
[00:07:21.600 --> 00:07:27.000]   And perhaps there are tasks in NLP that this wouldn't be appropriate for, but we honestly
[00:07:27.000 --> 00:07:28.000]   haven't seen it.
[00:07:28.000 --> 00:07:32.760]   Everything that we've given the training data for these models, they've performed in a good
[00:07:32.760 --> 00:07:33.760]   way.
[00:07:33.760 --> 00:07:40.840]   I think they make errors that the older NLP models don't make, but they make less errors.
[00:07:40.840 --> 00:07:43.880]   So you're going to take that every time.
[00:07:43.880 --> 00:07:49.240]   And your name Primer is evocative, at least to me, of kind of summarization.
[00:07:49.240 --> 00:07:52.360]   Am I correct in making that connection?
[00:07:52.360 --> 00:07:56.600]   It's actually, it comes from inspiration in Neil Stevenson's book, The Diamond Age.
[00:07:56.600 --> 00:08:02.240]   If you're a science fiction fan, the subtitle of that is The Young Lady's Illustrated Primer.
[00:08:02.240 --> 00:08:10.440]   And in that book, the protagonist has a nanotechnology, which creates a nanotechnological book that
[00:08:10.440 --> 00:08:13.080]   is designed to educate the world.
[00:08:13.080 --> 00:08:17.240]   And of course, without spoiling the book, it kind of falls into the hands of manipulation
[00:08:17.240 --> 00:08:21.020]   versus education, which I think is a wonderful kind of thing.
[00:08:21.020 --> 00:08:24.880]   And so, obviously underneath that is this idea that if you could have a self-writing
[00:08:24.880 --> 00:08:28.920]   book that could educate us about the world, we'd be in a science fiction world and we'd
[00:08:28.920 --> 00:08:31.280]   be able to kind of do fascinating things with that.
[00:08:31.280 --> 00:08:37.040]   And so for us as a guiding principle is how do we train machines to observe the world
[00:08:37.040 --> 00:08:40.600]   and teach us about what they're seeing so that we can be smarter about the world that
[00:08:40.600 --> 00:08:41.600]   we're living in.
[00:08:41.600 --> 00:08:44.880]   So I guess there's some connection, maybe not directly.
[00:08:44.880 --> 00:08:50.200]   I guess I was feeling impressed that, I feel like summarization or text generation, like
[00:08:50.200 --> 00:08:55.180]   you said, has been kind of the most interesting, maybe most impressive use of the new kind
[00:08:55.180 --> 00:08:56.880]   of transformer technology.
[00:08:56.880 --> 00:09:00.840]   And I was wondering if you sort of felt that that was coming or if you were surprised by
[00:09:00.840 --> 00:09:01.840]   it.
[00:09:01.840 --> 00:09:06.520]   Well, my thing always at the start was we wanted to build a self-writing Wikipedia and
[00:09:06.520 --> 00:09:10.640]   that was going to ultimately be something that this was going to enable.
[00:09:10.640 --> 00:09:14.440]   We were a long way away in 2015 from that technology even kind of existing.
[00:09:14.440 --> 00:09:18.840]   And so it was a bet on this becoming available and it turns out it's been a good bet.
[00:09:18.840 --> 00:09:24.320]   So I'll take the win on being right, but I don't know if I had the right information.
[00:09:24.320 --> 00:09:29.320]   So maybe I'm just lucky, but we'll take it.
[00:09:29.320 --> 00:09:32.680]   And I was kind of curious, you're one of the few people like me, kind of like a second
[00:09:32.680 --> 00:09:37.680]   time founder doing something in sort of a similar space as your first company, like
[00:09:37.680 --> 00:09:38.800]   I am too.
[00:09:38.800 --> 00:09:41.960]   I'm curious if that kind of shaped your views of your new company, like kind of what you
[00:09:41.960 --> 00:09:47.160]   were sort of thinking of maybe doing differently and what you wanted to keep from your last
[00:09:47.160 --> 00:09:48.160]   company.
[00:09:48.160 --> 00:09:55.200]   Yeah, look, I think it's kind of like I always sort of joke, when you're a writer, your first
[00:09:55.200 --> 00:09:59.480]   novel is sort of the easiest because it's just sort of a collection of all your experiences
[00:09:59.480 --> 00:10:00.840]   up to that point.
[00:10:00.840 --> 00:10:03.760]   Your second novel has to be something new.
[00:10:03.760 --> 00:10:07.960]   To kind of carry that analogy on, your first novel is kind of a biography.
[00:10:07.960 --> 00:10:12.760]   So I think in your first company, for me anyway, it was that idea you'd always had the back
[00:10:12.760 --> 00:10:14.620]   of your head that you wanted to make real.
[00:10:14.620 --> 00:10:19.240]   I think in your second company, and it's been true for me, I've become more grounded in
[00:10:19.240 --> 00:10:25.400]   the commercial realities of what's actually going to sell, what's going to scale, how
[00:10:25.400 --> 00:10:29.840]   big the opportunity is, what are the kind of the mega trends that are unfolding.
[00:10:29.840 --> 00:10:33.960]   And you were being very conscious of wanting to catch those waves and having a kind of
[00:10:33.960 --> 00:10:38.320]   a large commercial market to go after, having defensibility in the space that you're in
[00:10:38.320 --> 00:10:39.320]   becomes really important.
[00:10:39.320 --> 00:10:42.600]   But I think overall, the biggest thing is just operationally.
[00:10:42.600 --> 00:10:47.040]   I think when you're creating your first company, you don't really know what it's like to scale
[00:10:47.040 --> 00:10:48.960]   an organization.
[00:10:48.960 --> 00:10:52.200]   And I think until anyone's been through that, you don't really have that idea.
[00:10:52.200 --> 00:10:56.600]   I think once you've done it the second time, there's a lot of familiar signposts along
[00:10:56.600 --> 00:11:00.400]   the way where you're like, "Oh, this is what happens at this time and that's fine.
[00:11:00.400 --> 00:11:03.080]   And this is what happens at that time and that's fine."
[00:11:03.080 --> 00:11:07.080]   Whereas I think the first time you see it, you're sort of like, "Oh my God, is this the
[00:11:07.080 --> 00:11:08.080]   end?
[00:11:08.080 --> 00:11:09.080]   Or is this danger?
[00:11:09.080 --> 00:11:11.440]   Or is this what winning looks like?"
[00:11:11.440 --> 00:11:15.960]   And the second time you do it, you're like, "No, I've got a few more data points."
[00:11:15.960 --> 00:11:20.000]   And just having seen something once before is night and day versus seeing it the first
[00:11:20.000 --> 00:11:21.000]   time.
[00:11:21.000 --> 00:11:25.720]   Yeah, I guess I can relate to that.
[00:11:25.720 --> 00:11:28.720]   I'm curious too, I don't know if you think of yourself this way, but when you look at
[00:11:28.720 --> 00:11:31.320]   your background, it sort of feels like a data scientist, right?
[00:11:31.320 --> 00:11:33.440]   Like you have a PhD in physics, I think, right?
[00:11:33.440 --> 00:11:39.720]   And then did some really interesting kind of data stuff we could talk about on mathematics
[00:11:39.720 --> 00:11:40.720]   and war, I think.
[00:11:40.720 --> 00:11:45.880]   But do you think, I don't meet a lot of other data scientists that run companies.
[00:11:45.880 --> 00:11:49.960]   Do you think that that, Ben, informs your leadership style?
[00:11:49.960 --> 00:11:56.240]   It's funny, I probably only hang out with other data scientists that run companies.
[00:11:56.240 --> 00:12:02.640]   I think me and Mike Driscoll, Pete Scamorach, we tend to kind of console ourselves with
[00:12:02.640 --> 00:12:05.120]   the data scientists found in therapy sessions.
[00:12:05.120 --> 00:12:08.680]   So you're probably right though, and balance is probably not a lot of us.
[00:12:08.680 --> 00:12:12.360]   I think there's a few things that come through as a data scientist.
[00:12:12.360 --> 00:12:15.920]   One is I think you have an appreciation of the algorithms.
[00:12:15.920 --> 00:12:20.040]   And I think the single biggest thing that I've seen is when it comes to kind of product
[00:12:20.040 --> 00:12:23.920]   design, you're designing products that have algorithms at their heart.
[00:12:23.920 --> 00:12:28.400]   It's not algorithms to optimize the product experience, the product is the algorithm,
[00:12:28.400 --> 00:12:29.400]   the algorithm is the product.
[00:12:29.400 --> 00:12:33.680]   And I think that appreciation is really, really important when it comes to kind of this idea
[00:12:33.680 --> 00:12:38.280]   of building a product and what a product market fit means and all of that.
[00:12:38.280 --> 00:12:42.840]   And it's not a direct translation from sort of the old world where you're designing products
[00:12:42.840 --> 00:12:44.880]   that don't have algorithms at their heart.
[00:12:44.880 --> 00:12:46.400]   So I think that's one piece of it.
[00:12:46.400 --> 00:12:51.320]   I think a second bit is that the reality is, as you're growing these organizations, you're
[00:12:51.320 --> 00:12:54.520]   never going to have all the data you need at the start.
[00:12:54.520 --> 00:12:58.160]   And so like if you're in a bigger organization, I chat with a lot of friends that have come
[00:12:58.160 --> 00:13:02.600]   from LinkedIn and so on, you've got data that you can optimize, you can run A/B tests on,
[00:13:02.600 --> 00:13:04.320]   you can do all of that.
[00:13:04.320 --> 00:13:10.000]   When no one's using your product because you're trying to get the algorithms to work, you
[00:13:10.000 --> 00:13:15.320]   don't have the traditional kind of data science methodology, it's not that useful for you.
[00:13:15.320 --> 00:13:19.080]   So that's definitely a frustrating piece.
[00:13:19.080 --> 00:13:20.080]   You can't lean on that.
[00:13:20.080 --> 00:13:24.760]   So I think on the upside, you understand the algorithms, but on the downside, you don't
[00:13:24.760 --> 00:13:27.000]   really have data to make decisions on.
[00:13:27.000 --> 00:13:33.080]   It's probably a bit of both worlds, but I got to say it would be tough to be CEO and
[00:13:33.080 --> 00:13:37.440]   founder of a company if you didn't have a good grasp of these kinds of technologies.
[00:13:37.440 --> 00:13:42.000]   It's a pretty steep learning curve, so I definitely wouldn't trade the background by any means.
[00:13:42.000 --> 00:13:47.400]   It's funny, I think myself, I wonder if I'm maybe less data driven in some ways than other
[00:13:47.400 --> 00:13:53.520]   CEOs that don't come from a data background because I feel like sometimes people use data
[00:13:53.520 --> 00:13:56.480]   as almost like a wedge to reinforce their confirmation bias.
[00:13:56.480 --> 00:13:59.680]   And I think as a data scientist, or at least for me, I feel like I may be a little more
[00:13:59.680 --> 00:14:05.000]   skeptical of the data because I work with it so much, which I think sometimes makes
[00:14:05.000 --> 00:14:07.720]   me maybe in some realms less data driven.
[00:14:07.720 --> 00:14:10.160]   I wonder if you identify with that at all.
[00:14:10.160 --> 00:14:14.080]   Yeah, there's always skepticism of the question, it's always where do you get this data from?
[00:14:14.080 --> 00:14:17.320]   And then my mind immediately goes to kind of what's wrong with the data.
[00:14:17.320 --> 00:14:19.080]   And that side of it, I think is right.
[00:14:19.080 --> 00:14:25.360]   I think in this, there's a lot more gut instinct than I think anyone kind of appreciates.
[00:14:25.360 --> 00:14:31.680]   I don't think you can run a deep tech emerging company from data.
[00:14:31.680 --> 00:14:34.480]   Your data-oriented decision framework is probably not right.
[00:14:34.480 --> 00:14:39.480]   I think where I spend a lot of time is in this kind of like space between the scientific
[00:14:39.480 --> 00:14:42.080]   publishing and commercialization.
[00:14:42.080 --> 00:14:47.320]   I think perhaps more than anything, having a PhD and being familiar with how science
[00:14:47.320 --> 00:14:53.640]   evolves allows you to sort of make these bets on scientific breakthroughs that maybe seem
[00:14:53.640 --> 00:14:58.120]   risky to the outsider, but when you're following it and you know what the trajectory of a kind
[00:14:58.120 --> 00:15:02.520]   of an emerging scientific breakthrough feels like, you can kind of put your chips behind
[00:15:02.520 --> 00:15:08.120]   that, place a bet on it, and in 12 months, 18 months, you can cash in on that.
[00:15:08.120 --> 00:15:13.720]   I think perhaps more than anything, the benefit of a PhD in something like physics is a familiarity
[00:15:13.720 --> 00:15:18.880]   with science and a familiarity with the scientific process and translating that into a set of
[00:15:18.880 --> 00:15:23.840]   strategic bets that you can make as a CEO to position your company to best have upside
[00:15:23.840 --> 00:15:25.560]   with what's going to unfold.
[00:15:25.560 --> 00:15:28.640]   I went back, as you're saying here, maybe I'm lucky.
[00:15:28.640 --> 00:15:32.320]   The other way to look at it perhaps more generously is I just had a really good grasp of where
[00:15:32.320 --> 00:15:35.680]   the field was going and maybe I can claim some success on that.
[00:15:35.680 --> 00:15:38.320]   But that's the bet here is familiarity with science.
[00:15:38.320 --> 00:15:43.320]   I think as we've seen here, you've got one hand on archive and one hand on your email.
[00:15:43.320 --> 00:15:45.960]   Between the two of those, you're probably steering the company.
[00:15:45.960 --> 00:15:46.960]   Interesting.
[00:15:46.960 --> 00:15:48.920]   So where do you try to put your algorithms?
[00:15:48.920 --> 00:15:54.560]   Are you trying to push the very state of the art in terms of things like architecture or
[00:15:54.560 --> 00:16:00.080]   are you intentionally drawing from research and mostly using results that you find?
[00:16:00.080 --> 00:16:01.760]   Well, so it's interesting.
[00:16:01.760 --> 00:16:02.760]   There's two things.
[00:16:02.760 --> 00:16:05.080]   So one is research for sure, right?
[00:16:05.080 --> 00:16:09.600]   If you've got breakthroughs and these aren't always the obvious ones, but absolutely, right?
[00:16:09.600 --> 00:16:10.600]   Science unfolds.
[00:16:10.600 --> 00:16:13.120]   You want to take that learning and commercialize it.
[00:16:13.120 --> 00:16:18.360]   Now the commercializing of science can everything from making it cost efficient to run through
[00:16:18.360 --> 00:16:23.440]   to kind of training it on the right data through to kind of understanding how to kind of correct
[00:16:23.440 --> 00:16:30.200]   for the 15% false positives that pop up, which you can't do in a kind of a mathematically
[00:16:30.200 --> 00:16:34.280]   elegant way and it becomes a set of kind of rule-based kind of corrections at the edge.
[00:16:34.280 --> 00:16:37.900]   So that all of that kind of is part of commercialization.
[00:16:37.900 --> 00:16:43.000]   But the other side of it is there's a whole bunch of stuff that just doesn't fit the scientific
[00:16:43.000 --> 00:16:45.400]   publishing paradigm.
[00:16:45.400 --> 00:16:50.280]   And a lot of language generation doesn't fit the scientific publishing paradigm because
[00:16:50.280 --> 00:16:56.040]   all they've got are blue and rouge and these are useless with regards to kind of any customer
[00:16:56.040 --> 00:16:58.440]   experience of language generation.
[00:16:58.440 --> 00:17:02.720]   So in order to evaluate the quality of your language output, you've literally got to put
[00:17:02.720 --> 00:17:07.320]   humans on top of this and kind of have them evaluate everything that you're doing, which
[00:17:07.320 --> 00:17:12.720]   is incredibly expensive and it sort of hasn't been part of the scientific paradigm.
[00:17:12.720 --> 00:17:19.280]   So there's very little kind of publishing on language generation, I think largely because
[00:17:19.280 --> 00:17:23.520]   the ability to kind of like get a decent F score is really, really hard.
[00:17:23.520 --> 00:17:27.840]   And you can probably go through a whole bunch of language processing tasks that just don't
[00:17:27.840 --> 00:17:34.120]   have a decent F score measure or have a difficult F score measure and as such don't have a really
[00:17:34.120 --> 00:17:36.340]   an active scientific space.
[00:17:36.340 --> 00:17:40.360]   So it's been interesting kind of tracking that through.
[00:17:40.360 --> 00:17:44.120]   And I think the other bit here is science is still some of the best inspiration, right?
[00:17:44.120 --> 00:17:48.920]   And in terms of like, it can just sort of spark an idea and you're like, "Wow, that's
[00:17:48.920 --> 00:17:50.200]   a super cool attempt."
[00:17:50.200 --> 00:17:52.640]   And that side of science is pretty valuable too.
[00:17:52.640 --> 00:17:58.440]   Well, we're sitting here in August, 2020 talking about text generation.
[00:17:58.440 --> 00:18:01.160]   So I have to ask you what you make of GPT-3, right?
[00:18:01.160 --> 00:18:04.640]   That recently came out and people seemed very impressed.
[00:18:04.640 --> 00:18:09.320]   How impressed/surprised were you by its performance?
[00:18:09.320 --> 00:18:12.560]   I think the GPT-2 was the bigger jump, right?
[00:18:12.560 --> 00:18:19.040]   I think when GPT-2 came along, it was like, "Wow, these transformers scale and they scale
[00:18:19.040 --> 00:18:20.040]   really well."
[00:18:20.040 --> 00:18:21.040]   You know, it's funny.
[00:18:21.040 --> 00:18:22.600]   That was exactly my reaction.
[00:18:22.600 --> 00:18:26.200]   I didn't want to bias the question, but I totally, totally agree.
[00:18:26.200 --> 00:18:33.440]   Because prior to that language generation, you know, via LSTM and that was pretty bad.
[00:18:33.440 --> 00:18:37.240]   You could make a sentence, but you couldn't string two sentences together.
[00:18:37.240 --> 00:18:39.520]   And so that was the first thing was GPT-2.
[00:18:39.520 --> 00:18:40.520]   I was like, "Wow."
[00:18:40.520 --> 00:18:47.320]   Now where GPT-3 came, and I think it's useful, you know, was I was like, "Oh, it keeps scaling."
[00:18:47.320 --> 00:18:48.320]   Right?
[00:18:48.320 --> 00:18:52.240]   Like it doesn't seem to like have a finite kind of like scaling effect at this sort of
[00:18:52.240 --> 00:18:54.280]   level of parameter space.
[00:18:54.280 --> 00:18:55.280]   So that's useful, right?
[00:18:55.280 --> 00:18:57.880]   But for me, the big jump was GPT-2.
[00:18:57.880 --> 00:19:02.880]   Now what we found on that, and you can take a different set of transformers, you can take
[00:19:02.880 --> 00:19:09.360]   ExcelNet or BART or BERT or whatever you want, but what you found is, although the party
[00:19:09.360 --> 00:19:13.480]   trick is language generation, I think the true value of that is the trainability of
[00:19:13.480 --> 00:19:14.480]   these models.
[00:19:14.480 --> 00:19:19.600]   Is that you can train them to do tasks that are sort of the traditional NLP tasks, but
[00:19:19.600 --> 00:19:21.800]   you can train them with a lot less data.
[00:19:21.800 --> 00:19:27.120]   And it's super impressive to kind of see language generation, but in terms of the value for
[00:19:27.120 --> 00:19:31.560]   our customers, it's basically saying, "Oh, with 20 training examples, you can build this
[00:19:31.560 --> 00:19:38.480]   thing that with 95 plus precision and 90 plus percent recall will automate your human task
[00:19:38.480 --> 00:19:39.920]   every time."
[00:19:39.920 --> 00:19:45.240]   So I think the true commercial value of this is the retrainability.
[00:19:45.240 --> 00:19:47.560]   The party trick is the language generation.
[00:19:47.560 --> 00:19:50.680]   Although if you put on your hat of, and maybe we'll get to that later, of disinformation
[00:19:50.680 --> 00:19:54.640]   and manipulation, then there's definitely a whole industry that's going to spawn up
[00:19:54.640 --> 00:19:58.280]   around language generation, but we'll get to that later maybe.
[00:19:58.280 --> 00:20:04.040]   Well, maybe we should move in that direction, but I'm kind of curious, do you have a thought
[00:20:04.040 --> 00:20:10.560]   on why GPT-3 captured people's imagination so thoroughly?
[00:20:10.560 --> 00:20:11.560]   It's funny.
[00:20:11.560 --> 00:20:12.560]   It was one of those ones.
[00:20:12.560 --> 00:20:16.720]   We saw the paper get published, we went through it, and the thing that captured me was the
[00:20:16.720 --> 00:20:19.320]   few-shot learning, which was super interesting.
[00:20:19.320 --> 00:20:21.400]   And I think it got underplayed in the paper.
[00:20:21.400 --> 00:20:25.680]   The few-shot learnings was probably the most, I think, impressive piece of that work.
[00:20:25.680 --> 00:20:30.120]   Then I woke up a month after the paper was published, and then all of a sudden, an entire
[00:20:30.120 --> 00:20:33.120]   VC Twitter was going bananas for GPT-3.
[00:20:33.120 --> 00:20:36.440]   And I had that moment, I was like, "What's going on here?"
[00:20:36.440 --> 00:20:38.880]   And I just sort of scratched my head.
[00:20:38.880 --> 00:20:45.880]   I think OpenAI has done one thing incredibly well, and we don't probably appreciate that.
[00:20:45.880 --> 00:20:51.600]   The marketing that they do is par excellence for the world of AI.
[00:20:51.600 --> 00:20:53.120]   It really is impressive.
[00:20:53.120 --> 00:21:00.520]   And how they rolled out that release, I think, of GPT-3 versus the GPT-2, it's too dangerous,
[00:21:00.520 --> 00:21:01.520]   don't touch it.
[00:21:01.520 --> 00:21:04.760]   GPT-3 was like, "Come and play with it if you're special."
[00:21:04.760 --> 00:21:08.880]   And it was a perfect influencer campaign that was run beautifully.
[00:21:08.880 --> 00:21:12.160]   It's up there with the influencer campaigns of Fyre Festival.
[00:21:12.160 --> 00:21:16.200]   You're being nice, but now I feel like maybe you're not.
[00:21:16.200 --> 00:21:17.200]   I don't know.
[00:21:17.200 --> 00:21:20.160]   I can't tell if you admire it or if you're...
[00:21:20.160 --> 00:21:23.640]   But that was a wonderful influencer campaign, they sent everything to back it up with.
[00:21:23.640 --> 00:21:26.240]   I actually think there's a lot more there.
[00:21:26.240 --> 00:21:28.320]   But in terms of the campaign that they did, it was wonderful.
[00:21:28.320 --> 00:21:32.480]   And I think that that captured sort of the minds of VC Twitter.
[00:21:32.480 --> 00:21:37.920]   I think the bit that people miss on this is it matters what training data you've given
[00:21:37.920 --> 00:21:43.400]   to these machines, and it matters a lot more than you think.
[00:21:43.400 --> 00:21:45.040]   And that's the bit that everyone's sort of missed.
[00:21:45.040 --> 00:21:46.800]   It's like, "Out of the box, we can use this."
[00:21:46.800 --> 00:21:51.640]   With a few examples that it learns, people talk about steerability, or they talk about
[00:21:51.640 --> 00:21:54.080]   priming the system.
[00:21:54.080 --> 00:21:58.000]   What you're trying to do is correct for this sort of the somewhat random nature of the
[00:21:58.000 --> 00:21:59.640]   training data.
[00:21:59.640 --> 00:22:03.240]   And it's a really bad way to steer a model where you don't know what it's been trained
[00:22:03.240 --> 00:22:08.320]   on and you're trying to give it kind of hints in order to keep it away from being racist.
[00:22:08.320 --> 00:22:10.320]   And you don't know what it's read.
[00:22:10.320 --> 00:22:13.960]   It kind of feels like just the blind kind of like exploration.
[00:22:13.960 --> 00:22:17.880]   So I think the learning out of all this is training data matters.
[00:22:17.880 --> 00:22:23.760]   And the other bit I think here is that Twitter is a wonderful medium for displaying outputs
[00:22:23.760 --> 00:22:29.240]   of models that have 30% precision, because you don't see the other 70% where it missed.
[00:22:29.240 --> 00:22:33.280]   And I think that's the other piece here is that if you look at 10 cherry-picked examples
[00:22:33.280 --> 00:22:37.400]   of these outputs, you're going to see some great results.
[00:22:37.400 --> 00:22:42.080]   But as we know in the commercial world, for most applications, human kind of precision
[00:22:42.080 --> 00:22:43.880]   is plus 90%.
[00:22:43.880 --> 00:22:48.200]   And if you don't hit plus 90% on your task, it's very, very difficult to commercialize
[00:22:48.200 --> 00:22:49.200]   it.
[00:22:49.200 --> 00:22:55.640]   And so I think that the race as we look at NLP tasks is always the race to a 95% precision
[00:22:55.640 --> 00:22:58.200]   and that kind of is human comparable.
[00:22:58.200 --> 00:23:05.320]   Well, so you've touched on AI and safety a couple of times in the last few minutes.
[00:23:05.320 --> 00:23:09.280]   And you also kind of operate in a world that I think is considered a gray area to a lot
[00:23:09.280 --> 00:23:11.360]   of AI researchers, right?
[00:23:11.360 --> 00:23:13.040]   Defense or military applications.
[00:23:13.040 --> 00:23:17.240]   I'm curious what you think about generally about, especially natural language models
[00:23:17.240 --> 00:23:23.560]   and safety and what should be done, how worried you think people should be about misuse of
[00:23:23.560 --> 00:23:28.840]   these models and what role you think you should play as sort of like a leading company in
[00:23:28.840 --> 00:23:29.840]   the space?
[00:23:29.840 --> 00:23:35.920]   Well, I think first and foremost, if we want to be a global superpower as America, you
[00:23:35.920 --> 00:23:38.320]   have to have defense, you have to have intelligence.
[00:23:38.320 --> 00:23:43.120]   You may not want to have them, but then you don't get to be the global superpower.
[00:23:43.120 --> 00:23:46.440]   So that's the first thing to kind of just accept is that defense and intelligence are
[00:23:46.440 --> 00:23:49.280]   part and parcel of being a global superpower.
[00:23:49.280 --> 00:23:52.840]   It's also part and parcel of defending liberal Western democracy.
[00:23:52.840 --> 00:23:56.040]   And there are plenty of other organizations and governments in the world that don't want
[00:23:56.040 --> 00:23:57.520]   that to exist.
[00:23:57.520 --> 00:23:59.360]   So we need that.
[00:23:59.360 --> 00:24:02.320]   As you come back from that, the second thing you say, well, we want it, but we want it
[00:24:02.320 --> 00:24:03.320]   to be good, right?
[00:24:03.320 --> 00:24:08.180]   And so you say, well, if I want it to be good, well, we need to bring artificial intelligence
[00:24:08.180 --> 00:24:12.240]   and the latest technologies that we're developing to bear on that problem space.
[00:24:12.240 --> 00:24:16.480]   It's sort of a strange philosophical ground to say, well, we need to have defense, but
[00:24:16.480 --> 00:24:17.480]   it shouldn't be good.
[00:24:17.480 --> 00:24:18.480]   Right?
[00:24:18.480 --> 00:24:20.080]   It's just a strange position.
[00:24:20.080 --> 00:24:24.440]   Now, as you go through that, you say, well, yeah, there are also ethical concerns and
[00:24:24.440 --> 00:24:25.440]   moral concerns.
[00:24:25.440 --> 00:24:30.440]   There are very, very few organizations in the world that think more deeply about the
[00:24:30.440 --> 00:24:36.360]   ethical and moral implications of war than defense and intelligence.
[00:24:36.360 --> 00:24:40.800]   They live and breathe this stuff and we can kind of sit here and I'm back quarterback
[00:24:40.800 --> 00:24:42.480]   from the valley.
[00:24:42.480 --> 00:24:46.320]   But the reality is, is this is something that has been thought very, very deeply about and
[00:24:46.320 --> 00:24:51.080]   has a lot of care and that kind of rules of engagement, very, very well defined and very,
[00:24:51.080 --> 00:24:57.320]   very well thought through and have been shaped and kind of constructed over many, many years.
[00:24:57.320 --> 00:25:02.240]   Now a lot of them haven't imagined what AI does in that, but there's also been a huge
[00:25:02.240 --> 00:25:08.560]   amount of work going back to me over the last decade with defense, with intelligence, talking
[00:25:08.560 --> 00:25:13.840]   about these exact scenarios of what it means to have artificial intelligence engaging in
[00:25:13.840 --> 00:25:15.920]   this kind of process.
[00:25:15.920 --> 00:25:21.680]   So for me here, bringing this technology to bear and defense and intelligence is something
[00:25:21.680 --> 00:25:24.360]   that I think is the right thing to do.
[00:25:24.360 --> 00:25:27.860]   And it's a very, very important mission for myself and for our company.
[00:25:27.860 --> 00:25:31.880]   As we do that, we also realize we've got a responsibility that it matters.
[00:25:31.880 --> 00:25:36.840]   If we're generating models that are classifying things that are unfolding in the world and
[00:25:36.840 --> 00:25:39.480]   saying, look, we identified an event.
[00:25:39.480 --> 00:25:44.200]   And if you misclassify that, that intelligence is now percolating up a chain, which is going
[00:25:44.200 --> 00:25:45.200]   to have consequences.
[00:25:45.200 --> 00:25:46.200]   Right?
[00:25:46.200 --> 00:25:49.320]   So they're very real consequences when you talk about the precision of the models that
[00:25:49.320 --> 00:25:50.320]   you're working with.
[00:25:50.320 --> 00:25:54.000]   There are very real consequences when you talk about what the data is being trained
[00:25:54.000 --> 00:25:59.000]   on, what the susceptibility of the models that you've got are to outside adversarial
[00:25:59.000 --> 00:26:00.000]   attacks.
[00:26:00.000 --> 00:26:03.800]   So all of this becomes something that you need to kind of work with and deal with.
[00:26:03.800 --> 00:26:10.320]   I think the sort of the ethical components of this woven into the decisions that we make.
[00:26:10.320 --> 00:26:14.000]   And it's something that's also moving, I think, pretty quickly.
[00:26:14.000 --> 00:26:18.560]   And there's one thing you learn in science and technology is that science and technology
[00:26:18.560 --> 00:26:23.380]   moves a whole lot faster than the philosophical and ideological kind of foundations on which
[00:26:23.380 --> 00:26:25.360]   you can kind of make decisions on top of.
[00:26:25.360 --> 00:26:29.120]   And so you are by nature going to be in gray zones.
[00:26:29.120 --> 00:26:33.720]   And this is something you've got to be kind of open to and say, look, we're going to navigate
[00:26:33.720 --> 00:26:37.240]   where perhaps no one's ever thought about this before.
[00:26:37.240 --> 00:26:42.560]   And there isn't kind of a strong kind of rule that you can fall back to and say, hey, this
[00:26:42.560 --> 00:26:43.560]   is the answer.
[00:26:43.560 --> 00:26:46.840]   This is what you're supposed to do in this situation, because the situation has never
[00:26:46.840 --> 00:26:48.280]   existed before.
[00:26:48.280 --> 00:26:55.000]   So it's something that we spend a lot of time and with both ourselves and also our advisors,
[00:26:55.000 --> 00:26:58.840]   spending time each and every week going through this stuff, making decisions and trying to
[00:26:58.840 --> 00:27:01.520]   kind of navigate the best path that we can through this.
[00:27:01.520 --> 00:27:04.960]   But I think it would be a lie to say that this is really easy and there's this clear
[00:27:04.960 --> 00:27:09.720]   black and white kind of distinctions, because we're dealing with stuff that simply didn't
[00:27:09.720 --> 00:27:10.720]   exist in the world before.
[00:27:10.720 --> 00:27:14.400]   But we're also dealing on the geopolitical scale with stuff that simply didn't exist
[00:27:14.400 --> 00:27:16.640]   in the world before.
[00:27:16.640 --> 00:27:28.400]   And do you think like at this moment in time, August 2020, do you think that for governments,
[00:27:28.400 --> 00:27:34.040]   natural language processing, like machine learning is an important part of their defense
[00:27:34.040 --> 00:27:35.040]   capability?
[00:27:35.040 --> 00:27:38.440]   Yeah, I think there's three places where it comes through.
[00:27:38.440 --> 00:27:40.360]   The first is on the intelligence side.
[00:27:40.360 --> 00:27:42.560]   There's too much information coming in.
[00:27:42.560 --> 00:27:47.120]   And simply put, if you don't have machines playing some role in helping you navigate
[00:27:47.120 --> 00:27:50.760]   that information, you're going to have information that no one ever sees.
[00:27:50.760 --> 00:27:54.000]   And if you don't see information, you can't bring it to bear on decisions that you're
[00:27:54.000 --> 00:27:55.000]   making.
[00:27:55.000 --> 00:28:01.000]   So the volume of information requires a natural language toolkit to actually help navigate.
[00:28:01.000 --> 00:28:05.760]   The second thing here is that the complexity of the world that we're in means that during
[00:28:05.760 --> 00:28:11.120]   inferences between something that's happening in Russia and something that's happening in
[00:28:11.120 --> 00:28:16.040]   East Africa is very, very difficult for an individual that has to specialize.
[00:28:16.040 --> 00:28:19.800]   And I'm an East African specialist, or I'm a Russian specialist.
[00:28:19.800 --> 00:28:20.800]   Machines don't have that limitation.
[00:28:20.800 --> 00:28:24.960]   They can look further, they can look wider, they can draw inference across larger scales.
[00:28:24.960 --> 00:28:27.400]   They can look at larger set of data points because they're not fundamentally constrained
[00:28:27.400 --> 00:28:30.200]   by the bandwidth of information they can consume.
[00:28:30.200 --> 00:28:34.240]   So I think as we move to a more complex world, it's essential to have machines that can make
[00:28:34.240 --> 00:28:38.800]   connections across domains that humans aren't necessarily looking at.
[00:28:38.800 --> 00:28:42.680]   The third thing is, and this is sort of, I think, become increasingly important, is that
[00:28:42.680 --> 00:28:47.720]   more and more information is being generated by machines, and that's being used to manipulate.
[00:28:47.720 --> 00:28:51.920]   And if you've got humans that are trying to filter through the output of propaganda from
[00:28:51.920 --> 00:28:55.880]   China that's being machine generated, you've brought a knife to a gunfight.
[00:28:55.880 --> 00:28:56.880]   You're going to lose that.
[00:28:56.880 --> 00:29:03.400]   And so as we look at things like the operations out of Pacific Command, there's a huge volume
[00:29:03.400 --> 00:29:08.360]   of information now that China's got its head around disinformation and manipulation.
[00:29:08.360 --> 00:29:10.800]   You can't navigate this as a set of humans.
[00:29:10.800 --> 00:29:11.800]   It's just not possible.
[00:29:11.800 --> 00:29:14.200]   And if you try and do that, you're going to lose.
[00:29:14.200 --> 00:29:18.440]   So I think the disinformation landscape has necessitated a set of machines that need to
[00:29:18.440 --> 00:29:19.440]   come into this.
[00:29:19.440 --> 00:29:23.080]   Sorry, could you be more concrete about the disinformation?
[00:29:23.080 --> 00:29:27.760]   Should I be imagining Facebook bots or what's-
[00:29:27.760 --> 00:29:30.480]   It's actually evolved a lot.
[00:29:30.480 --> 00:29:35.920]   So our standard kind of thing was Facebook bots back in 2016.
[00:29:35.920 --> 00:29:38.200]   What you've got now is a manipulation ecosystem.
[00:29:38.200 --> 00:29:40.400]   So it's everything from state broadcasting.
[00:29:40.400 --> 00:29:46.860]   So if you're in the Russian example, you've got Russia Today and that sort of state broadcasting.
[00:29:46.860 --> 00:29:51.420]   You've got state supported broadcasters, so things like Sputnik in Russia.
[00:29:51.420 --> 00:29:55.240]   Then you've got kind of fringe publications, which is supported.
[00:29:55.240 --> 00:30:00.480]   These can be kind of fringe versions of Huffington Post, but it would be a fringe version of
[00:30:00.480 --> 00:30:02.540]   that where anyone can kind of submit.
[00:30:02.540 --> 00:30:07.040]   Then you've got social media and then you've got sort of cyber enabled hacking, right?
[00:30:07.040 --> 00:30:11.940]   Where you may falsely release a set of emails that have been doctored.
[00:30:11.940 --> 00:30:15.980]   So all of these components make up the sort of the ecosystem of information manipulation
[00:30:15.980 --> 00:30:17.740]   and actually layer together.
[00:30:17.740 --> 00:30:24.700]   So you can hack a set of emails, falsify emails, spread them out, have them found on social
[00:30:24.700 --> 00:30:31.460]   media, have it amplified by a third party fringe voice on a user submitted site like
[00:30:31.460 --> 00:30:34.900]   Huffington Post, but not Huffington Post probably.
[00:30:34.900 --> 00:30:39.100]   You can have it kind of rebroadcast through Sputnik and then end up on RT and then be
[00:30:39.100 --> 00:30:41.560]   connected back into Fox News, right?
[00:30:41.560 --> 00:30:46.440]   So that cycle allows layering of information to come where you don't know the original
[00:30:46.440 --> 00:30:48.440]   source of it.
[00:30:48.440 --> 00:30:53.120]   You may not be aware of how it came to be and you may be hit with the information from
[00:30:53.120 --> 00:30:59.640]   three different angles that makes it feel like it's a lot more kind of authentic.
[00:30:59.640 --> 00:31:01.060]   You can do this with fake information.
[00:31:01.060 --> 00:31:05.160]   You can also do it with information that's actually real, but perhaps isn't as important
[00:31:05.160 --> 00:31:06.280]   as it should be.
[00:31:06.280 --> 00:31:11.880]   So maybe there's a shooting that happens, you know, which becomes front and center news
[00:31:11.880 --> 00:31:15.720]   when a reality is it was just a local shooting and if it hadn't been amplified, it never
[00:31:15.720 --> 00:31:17.480]   would have been on the radar.
[00:31:17.480 --> 00:31:22.280]   So you're not just in this world of is this real or is it fake?
[00:31:22.280 --> 00:31:27.400]   It's actually whose agenda is being pushed and what organism is actually pushing this
[00:31:27.400 --> 00:31:28.760]   agenda.
[00:31:28.760 --> 00:31:34.000]   And this is kind of where I think we're sitting now is actually a very, very complex disinformation
[00:31:34.000 --> 00:31:37.880]   ecosystem designed to manipulate.
[00:31:37.880 --> 00:31:39.760]   How does machine learning enable that though?
[00:31:39.760 --> 00:31:44.520]   Because all those examples you said, I could picture that being done with just human beings,
[00:31:44.520 --> 00:31:48.200]   you know, motivated human beings doing a lot of typing, I guess.
[00:31:48.200 --> 00:31:49.680]   Is ML really changed this?
[00:31:49.680 --> 00:31:50.680]   Yeah.
[00:31:50.680 --> 00:31:54.960]   So I think state of the art at the moment is humans at the Internet Research Agency sitting
[00:31:54.960 --> 00:32:00.160]   down and, you know, from what we know, they have a set of objectives they have to hit.
[00:32:00.160 --> 00:32:05.440]   They have a sort of a scoreboard of topics they need to cover every day and then they
[00:32:05.440 --> 00:32:07.200]   get rewarded based on the performance.
[00:32:07.200 --> 00:32:08.920]   It's all very manual.
[00:32:08.920 --> 00:32:15.040]   I think what we're looking at is it generally takes on order of 18 months, 24 months for
[00:32:15.040 --> 00:32:19.320]   a sort of an emerging technology to become sort of weaponized.
[00:32:19.320 --> 00:32:24.820]   So we're not seeing yet the weaponization of language generation.
[00:32:24.820 --> 00:32:31.680]   We have just started to see the weaponization of image generation for fake images and fake
[00:32:31.680 --> 00:32:32.680]   profiles.
[00:32:32.680 --> 00:32:39.860]   We haven't seen the weaponization of yet really, although we should expect it soon, of video
[00:32:39.860 --> 00:32:41.300]   generation.
[00:32:41.300 --> 00:32:44.440]   So language generation is a lot newer.
[00:32:44.440 --> 00:32:46.880]   I think we're probably two years away from seeing that.
[00:32:46.880 --> 00:32:53.220]   But there's obviously a very, very clear path that if you can generate all sorts of anti-vaccination
[00:32:53.220 --> 00:32:59.100]   articles that target to different demographics, and you can do that by the scale of millions,
[00:32:59.100 --> 00:33:03.860]   you're going to get some really, really persuasive arguments that are able to be captured and
[00:33:03.860 --> 00:33:04.860]   propagated.
[00:33:04.860 --> 00:33:11.140]   So whilst it hasn't unfolded yet because the technology is new, I think it's very, very
[00:33:11.140 --> 00:33:15.780]   clear that this is a weapon that if you were going to take this on, that is absolutely
[00:33:15.780 --> 00:33:18.700]   something you'd want to kind of have in your disposal.
[00:33:18.700 --> 00:33:20.260]   And so I think that's one piece of it.
[00:33:20.260 --> 00:33:23.460]   But I think the second bit, it gets back to more of the traditional data science, which
[00:33:23.460 --> 00:33:26.420]   is A/B testing on the scale of millions.
[00:33:26.420 --> 00:33:30.700]   And whilst you can't really do that when humans are typing this stuff out, once machines are
[00:33:30.700 --> 00:33:32.860]   producing it, you absolutely can.
[00:33:32.860 --> 00:33:39.340]   So I think that gets you into a world where this is going to be a lot more coherent.
[00:33:39.340 --> 00:33:44.020]   The other bit that I'd flag going back to science is one of the most fascinating areas
[00:33:44.020 --> 00:33:47.940]   of scientific research at the moment has been, for me anyway, has been opinion formation
[00:33:47.940 --> 00:33:48.940]   and crowd dynamics.
[00:33:48.940 --> 00:33:52.340]   So it's got roots in kind of a little bit in epidemiology.
[00:33:52.340 --> 00:33:54.940]   It's got roots in a little bit in stock market trading.
[00:33:54.940 --> 00:34:00.260]   It's got roots obviously in the world of idea formation and diffusion of ideas.
[00:34:00.260 --> 00:34:06.420]   But this is an area where we're actually seeing that crowds can actually be very manipulable.
[00:34:06.420 --> 00:34:07.420]   That research is happening.
[00:34:07.420 --> 00:34:08.860]   It's going on.
[00:34:08.860 --> 00:34:12.980]   Once you couple these other technologies into that, I think we're going to start to see
[00:34:12.980 --> 00:34:17.180]   that you can move and manipulate large groups of people through the information they're
[00:34:17.180 --> 00:34:18.460]   exposed to.
[00:34:18.460 --> 00:34:21.860]   And at that point, you've got a fundamental issue with democracy.
[00:34:21.860 --> 00:34:23.700]   And this is why it's such a big issue.
[00:34:23.700 --> 00:34:29.060]   We are based as a society on the free and open debate and sharing of ideas to come to
[00:34:29.060 --> 00:34:33.540]   consensus and a democratic process to elect governance for us.
[00:34:33.540 --> 00:34:37.540]   Once we lose faith in that, democracy dies.
[00:34:37.540 --> 00:34:45.620]   And there's a very, very clear vector of attack with manipulation of information by machines.
[00:34:45.620 --> 00:34:47.460]   And so we need defenses against that.
[00:34:47.460 --> 00:34:48.460]   It's coming.
[00:34:48.460 --> 00:34:53.060]   And the defense and intelligence sector has realized this, and we're working very closely
[00:34:53.060 --> 00:34:55.340]   with them to help with that defense.
[00:34:55.340 --> 00:34:58.620]   Could you sketch out what a defense to that might look like?
[00:34:58.620 --> 00:35:02.780]   Because it doesn't seem obvious there's a way to kind of prevent people from creating
[00:35:02.780 --> 00:35:04.180]   very persuasive content.
[00:35:04.180 --> 00:35:07.540]   In fact, you might argue that's happening right now.
[00:35:07.540 --> 00:35:09.100]   Yeah.
[00:35:09.100 --> 00:35:10.340]   So I think that's right.
[00:35:10.340 --> 00:35:13.460]   Look, so one of the things to recognize is an asymmetry.
[00:35:13.460 --> 00:35:18.260]   So with any asymmetrical conflict, one side has the advantage over the other.
[00:35:18.260 --> 00:35:21.420]   And I sort of draw the example with image generation.
[00:35:21.420 --> 00:35:25.500]   If you generate a face of a person, you've got two options.
[00:35:25.500 --> 00:35:29.380]   If you want to know if that's real, you can go and check every single person in the world
[00:35:29.380 --> 00:35:30.380]   and see if it's there.
[00:35:30.380 --> 00:35:33.420]   And if you get through everyone, you don't find them, then it's fake.
[00:35:33.420 --> 00:35:37.780]   So obviously, it's easy to generate an image that is to determine if it's fake or not.
[00:35:37.780 --> 00:35:40.980]   Now, of course, as you go through that, there are signs and telltale signs.
[00:35:40.980 --> 00:35:42.460]   They're a little too blurry.
[00:35:42.460 --> 00:35:43.460]   The ears are asymmetrical.
[00:35:43.460 --> 00:35:45.460]   The teeth don't quite line up.
[00:35:45.460 --> 00:35:48.180]   And so then people kind of figure that out.
[00:35:48.180 --> 00:35:49.660]   And then they generate a new image.
[00:35:49.660 --> 00:35:53.300]   And then the old techniques for identifying it aren't working anymore.
[00:35:53.300 --> 00:35:57.700]   And now you're in a cycle of effectively what we've seen in cybersecurity, which is things
[00:35:57.700 --> 00:36:02.660]   like zero-day attacks, where you get a new model that hasn't been shown before.
[00:36:02.660 --> 00:36:06.060]   And the statistical signatures of that model aren't known to the defense systems.
[00:36:06.060 --> 00:36:09.380]   So it's a game of detection and deception.
[00:36:09.380 --> 00:36:13.340]   Can I deceive the algorithms that are designed to detect whether this is real or not?
[00:36:13.340 --> 00:36:16.540]   Or can I actually detect it and kind of like stop it?
[00:36:16.540 --> 00:36:17.540]   So that's one side of it.
[00:36:17.540 --> 00:36:18.540]   Now, that's in images.
[00:36:18.540 --> 00:36:21.660]   But if you go into language, obviously, there are signals in here.
[00:36:21.660 --> 00:36:24.900]   And one of the ones that we spot and look at is the ZIP distribution.
[00:36:24.900 --> 00:36:29.140]   So if you look at language, there's a ZIP distribution, which is a relative frequency
[00:36:29.140 --> 00:36:30.500]   of words that we use.
[00:36:30.500 --> 00:36:33.940]   And each author has a kind of a statistical signature of language.
[00:36:33.940 --> 00:36:35.780]   And machines have a statistical signature of language.
[00:36:35.780 --> 00:36:37.240]   And so you can spot them.
[00:36:37.240 --> 00:36:40.780]   But if you generate a new model, then the old methods of detecting it aren't necessarily
[00:36:40.780 --> 00:36:41.780]   there.
[00:36:41.780 --> 00:36:44.260]   So you've got the whole kind of like detection and deception.
[00:36:44.260 --> 00:36:46.980]   Is this being generated by a machine or not?
[00:36:46.980 --> 00:36:51.260]   But on the other side of it, you've also got things like claims that are being made.
[00:36:51.260 --> 00:36:56.620]   So if a claim is being made that 5G causes coronavirus, well, you can actually trace
[00:36:56.620 --> 00:36:57.620]   that claim backwards.
[00:36:57.620 --> 00:36:59.420]   Where did it first originate?
[00:36:59.420 --> 00:37:00.420]   How did it propagate?
[00:37:00.420 --> 00:37:05.860]   And so it's not so much as the language real or fake, but has it been propagated by grassroots
[00:37:05.860 --> 00:37:11.420]   or has it been propagated through the network via actors that are intentional about that?
[00:37:11.420 --> 00:37:16.620]   Now, to do that, you've got to classify a relationship between 5G and coronavirus.
[00:37:16.620 --> 00:37:20.420]   And as you look at that, there's all sorts of ways to say that it's caused by, it's a
[00:37:20.420 --> 00:37:23.340]   result of, and so you're now into a kind of a relationship classifier.
[00:37:23.340 --> 00:37:24.860]   And so you can do that.
[00:37:24.860 --> 00:37:30.700]   We deploy that technology, looking at relationships for claim extraction, propagating that backwards.
[00:37:30.700 --> 00:37:33.540]   But we also look for things that counter that claim.
[00:37:33.540 --> 00:37:42.060]   So coronavirus is not caused by 5G or coronavirus was likely caused by an infection of a bat
[00:37:42.060 --> 00:37:43.580]   into a wet market.
[00:37:43.580 --> 00:37:45.900]   So these are big claims that are at odds with each other.
[00:37:45.900 --> 00:37:50.260]   So ultimately, the dynamic is how do you get a ground truth?
[00:37:50.260 --> 00:37:51.980]   How do you get a ground truth?
[00:37:51.980 --> 00:37:55.300]   And I think if we're looking at kind of a long-term kind of game on this is we need
[00:37:55.300 --> 00:38:01.020]   to train machines up that can help us establish ground truth so that when new information
[00:38:01.020 --> 00:38:06.620]   becomes available, we can measure it up against that and say, is this consistent or is this
[00:38:06.620 --> 00:38:07.620]   contradictory?
[00:38:07.620 --> 00:38:11.340]   Now, just because it's contradictory to ground truth doesn't make it false, but it does mean
[00:38:11.340 --> 00:38:13.340]   you want to look closer at it.
[00:38:13.340 --> 00:38:18.060]   And this is kind of, I think, as we build up defenses for democracy, we need, and I've
[00:38:18.060 --> 00:38:21.180]   talked about this, a Manhattan Project to establish ground truth.
[00:38:21.180 --> 00:38:24.820]   It's going to take a lot of work and a lot of effort, but it's very, very hard to see
[00:38:24.820 --> 00:38:30.500]   a democracy functioning if we can't establish information provenance, if we can't establish
[00:38:30.500 --> 00:38:34.500]   whether information is likely to be part of a manipulative attack, and if we don't have
[00:38:34.500 --> 00:38:38.860]   any infrastructure to kind of lean back on and say, well, here's what we do know about
[00:38:38.860 --> 00:38:42.140]   the world and here's what we do understand with it.
[00:38:42.140 --> 00:38:46.980]   And so this is a big problem, I think, for democracies and we need a way around it.
[00:38:46.980 --> 00:38:50.940]   And so this is going to come down to, it's an asymmetric fight, but it's one that we
[00:38:50.940 --> 00:38:52.940]   have to win.
[00:38:52.940 --> 00:38:57.380]   Do you think that it would be wise to use the same kind of manipulation techniques to
[00:38:57.380 --> 00:38:59.780]   spread true information?
[00:38:59.780 --> 00:39:01.500]   Yeah, this is interesting.
[00:39:01.500 --> 00:39:04.180]   So on the one side, you've got detection.
[00:39:04.180 --> 00:39:06.740]   I think the other side, you've got, well, what's your reaction?
[00:39:06.740 --> 00:39:09.060]   What's the action that you take on top of this?
[00:39:09.060 --> 00:39:13.100]   I think at this point here, and you can go into just kind of the health crisis kind of
[00:39:13.100 --> 00:39:17.060]   dynamic of COVID, and that sort of maybe makes it a little more real.
[00:39:17.060 --> 00:39:23.380]   And so if you've got stuff here around the diffusion of HCQ being an effective treatment
[00:39:23.380 --> 00:39:27.700]   or masks don't work, this is really dangerous.
[00:39:27.700 --> 00:39:28.700]   This is incredibly dangerous.
[00:39:28.700 --> 00:39:33.540]   The propagation, and we've seen bot activity around masks don't work.
[00:39:33.540 --> 00:39:37.660]   There does seem to be coordinated attacks around pushing devices and masks.
[00:39:37.660 --> 00:39:40.340]   Sorry, why would that be true?
[00:39:40.340 --> 00:39:44.780]   Who would stand to gain from pushing the idea that masks don't work?
[00:39:44.780 --> 00:39:49.980]   Well, so if you want to create political division, which has been a stated goal of the IRA, the
[00:39:49.980 --> 00:39:54.940]   Internet Research Agency, you find any hot button issue that will divide a country, push
[00:39:54.940 --> 00:39:55.940]   it.
[00:39:55.940 --> 00:39:56.940]   It puts it into tribalism.
[00:39:57.180 --> 00:40:00.060]   It puts it into the mask and then when you lose the cohesivity, why do you want to do
[00:40:00.060 --> 00:40:01.060]   that?
[00:40:01.060 --> 00:40:06.500]   Well, if you don't have a unified set of political consensus on anything, it's very, very hard
[00:40:06.500 --> 00:40:07.740]   to go to war.
[00:40:07.740 --> 00:40:12.300]   It's very, very hard to rally the US to say, don't invade Crimea if you can't even agree
[00:40:12.300 --> 00:40:13.300]   on masks.
[00:40:13.300 --> 00:40:14.300]   Right.
[00:40:14.300 --> 00:40:18.820]   So like one way to kind of neutralize the strongest military in the world is to ensure
[00:40:18.820 --> 00:40:24.260]   that the political actions will never come to agreement about how it will be used.
[00:40:24.260 --> 00:40:26.340]   And Russia has been incredibly smart on that.
[00:40:26.340 --> 00:40:30.220]   And so one of their kind of goals as they look through is to divide the nation so that
[00:40:30.220 --> 00:40:32.100]   you can't agree on anything.
[00:40:32.100 --> 00:40:33.580]   And so one of the things has been masks.
[00:40:33.580 --> 00:40:37.900]   Now that the added kind of benefit of the masks is that it kind of ruins the health
[00:40:37.900 --> 00:40:41.140]   of society by having division on that.
[00:40:41.140 --> 00:40:45.940]   And it also ruins trust in the political system, which is again to Russia's advantage.
[00:40:45.940 --> 00:40:49.940]   So there's absolutely been something that if you're sitting there, this has been one
[00:40:49.940 --> 00:40:53.940]   of the things that have popped up on your daily kind of topic board of things you have
[00:40:53.940 --> 00:40:55.020]   to act on.
[00:40:55.020 --> 00:40:58.700]   And we can see that kind of manifest from the way in which information is propagating
[00:40:58.700 --> 00:41:02.140]   in the way in which bot type activity is engaging.
[00:41:02.140 --> 00:41:05.940]   And so if you look at that and you say, well, right, there's nothing we can do about that.
[00:41:05.940 --> 00:41:09.820]   Well, that's that's the wrong that's the wrong thing to do, because not only are you creating
[00:41:09.820 --> 00:41:12.020]   political divisiveness, lives are being lost.
[00:41:12.020 --> 00:41:13.020]   Right.
[00:41:13.020 --> 00:41:16.260]   So it's a hard position to hold that we shouldn't do something.
[00:41:16.260 --> 00:41:19.980]   I think the question then comes is like you do want to propagate information out that
[00:41:19.980 --> 00:41:25.260]   is that is true and that does kind of conform to the scientific consensus.
[00:41:25.260 --> 00:41:28.780]   But the interesting thing on that is masks were not a scientific consensus.
[00:41:28.780 --> 00:41:29.780]   Right.
[00:41:29.780 --> 00:41:33.140]   And if you went early on, it was against WHO regulation.
[00:41:33.140 --> 00:41:37.620]   And so if you posted and I had conversations with Jeremy Howard about this, he posted on
[00:41:37.620 --> 00:41:39.480]   Reddit and they said, you can't put that here.
[00:41:39.480 --> 00:41:43.380]   You can't post that masks are an effective solution.
[00:41:43.380 --> 00:41:48.100]   And the reason you can't post it is because this is pseudoscience, because science hadn't
[00:41:48.100 --> 00:41:50.280]   come to a conclusion.
[00:41:50.280 --> 00:41:51.280]   So it's really, really tough.
[00:41:51.280 --> 00:41:52.280]   Right.
[00:41:52.280 --> 00:41:55.300]   As you go through this is to say, well, what is ground truth, particularly if science hasn't
[00:41:55.300 --> 00:41:56.300]   figured it out?
[00:41:56.300 --> 00:42:01.460]   And then how do we police content that may or may not conform to this?
[00:42:01.460 --> 00:42:05.740]   And so immediately as you go through this, you start to realize that it's a very, very
[00:42:05.740 --> 00:42:07.300]   difficult problem.
[00:42:07.300 --> 00:42:10.840]   However, it's also one that you feel like you've got to act on.
[00:42:10.840 --> 00:42:16.660]   So I think we're going to have to be in a place where we do use this technology to inoculate
[00:42:16.660 --> 00:42:19.860]   ourselves against kind of disinformation.
[00:42:19.860 --> 00:42:23.980]   And one of the things here is it's kind of to take the virus analogy.
[00:42:23.980 --> 00:42:30.560]   If you haven't been exposed to a political stance on masks, you'll probably take whatever
[00:42:30.560 --> 00:42:32.260]   you're first exposed to.
[00:42:32.260 --> 00:42:36.060]   And if you're exposed, the first information is that masks don't work.
[00:42:36.060 --> 00:42:37.820]   It's a conspiracy.
[00:42:37.820 --> 00:42:42.660]   If that becomes your first exposure, it's much, much harder to change your opinion than
[00:42:42.660 --> 00:42:45.580]   if your first exposure were masks are a good idea.
[00:42:45.580 --> 00:42:48.680]   If you help me, I help you.
[00:42:48.680 --> 00:42:49.980]   It's a good idea.
[00:42:49.980 --> 00:42:55.380]   So one of the things you look about is identify the manipulation campaigns early and inoculate
[00:42:55.380 --> 00:43:01.520]   susceptible populations to the messages by exposing them to good, well-grounded ground
[00:43:01.520 --> 00:43:02.800]   truth.
[00:43:02.800 --> 00:43:05.020]   With those similar techniques?
[00:43:05.020 --> 00:43:06.020]   Similar techniques.
[00:43:06.020 --> 00:43:07.860]   I think you're going to have to use similar techniques.
[00:43:07.860 --> 00:43:13.340]   And this is kind of to go back to the book from Stevenson, the line between education
[00:43:13.340 --> 00:43:16.540]   and manipulation is a very, very fine and often blurry line.
[00:43:16.540 --> 00:43:17.540]   And it's that dynamic, right?
[00:43:17.540 --> 00:43:20.300]   It's like, well, if I'm educating, I am manipulating.
[00:43:20.300 --> 00:43:22.860]   But the difference is I'm doing it for the benefit of you.
[00:43:22.860 --> 00:43:25.980]   I'm doing it for the benefit of the society.
[00:43:25.980 --> 00:43:27.500]   Not I'm doing it for my own benefit.
[00:43:27.500 --> 00:43:31.420]   And I think that's kind of the dynamic here is undoubtedly we're training machines to
[00:43:31.420 --> 00:43:35.580]   understand the world in ways that we can't, to do things that we can't.
[00:43:35.580 --> 00:43:40.300]   What we teach them, how we teach them is really important because they're going to then be
[00:43:40.300 --> 00:43:45.100]   tools that either benefit us or work to our detriment.
[00:43:45.100 --> 00:43:48.820]   But that kind of dynamic is it's, they're undoubtedly going to see things that we can't
[00:43:48.820 --> 00:43:52.380]   see and they're going to understand things that we just can't understand.
[00:43:52.380 --> 00:43:55.140]   And we need that because we can't navigate this world without them.
[00:43:55.140 --> 00:43:59.980]   So they're here, but we need to take responsibility with what's in front of us.
[00:43:59.980 --> 00:44:03.140]   Well, I have lots more questions, but I'm running out of time.
[00:44:03.140 --> 00:44:04.880]   And we always end with two questions.
[00:44:04.880 --> 00:44:09.540]   If you look at the subtopics in machine learning, is there one that you think doesn't get as
[00:44:09.540 --> 00:44:13.860]   much attention as it deserves that you think is way more important than people give it
[00:44:13.860 --> 00:44:15.860]   credit for?
[00:44:15.860 --> 00:44:18.980]   Yeah, I think it's information retrieval.
[00:44:18.980 --> 00:44:22.860]   So the world of IR is sort of machine learning, kind of.
[00:44:22.860 --> 00:44:27.060]   I mean, it's sort of being 25 algorithms and so on and sort of that.
[00:44:27.060 --> 00:44:31.280]   But I think information retrieval has been something that we've totally forgotten about,
[00:44:31.280 --> 00:44:34.740]   but it's so fundamental to all database technology in the world.
[00:44:34.740 --> 00:44:37.780]   And yet we haven't really kind of given it the attention that it deserves.
[00:44:37.780 --> 00:44:42.600]   So aside from some sort of researchers that I'm sure are not getting their papers submitted
[00:44:42.600 --> 00:44:47.020]   to NIPS because their information retrieval is not top of the list, but more information
[00:44:47.020 --> 00:44:48.020]   retrieval, for sure.
[00:44:48.020 --> 00:44:51.940]   I feel like that was really the first major application of machine learning, at least
[00:44:51.940 --> 00:44:52.940]   that I was aware of.
[00:44:52.940 --> 00:44:54.620]   Yeah, and we just haven't touched it.
[00:44:54.620 --> 00:44:59.140]   The volume of information retrieval literature with these new kind of technologies is pretty
[00:44:59.140 --> 00:45:02.140]   low and yet underneath it, it's a search and recall problem.
[00:45:02.140 --> 00:45:03.140]   Interesting.
[00:45:03.140 --> 00:45:04.140]   I love it.
[00:45:04.140 --> 00:45:05.700]   You're the first person that said that.
[00:45:05.700 --> 00:45:07.340]   I think it's a great answer.
[00:45:07.340 --> 00:45:12.700]   And the final question is, when you look at the projects that you've had of kind of taking
[00:45:12.700 --> 00:45:18.140]   machine learning from conception to deployed in production and useful, where were the surprising
[00:45:18.140 --> 00:45:21.900]   bottlenecks in that entire process?
[00:45:21.900 --> 00:45:27.900]   I think the surprising ones have been just the amount of training data and the importance
[00:45:27.900 --> 00:45:28.900]   of training data.
[00:45:28.900 --> 00:45:32.380]   I think coming in, we knew that data had to be cleaned.
[00:45:32.380 --> 00:45:33.980]   We knew that there was cost functions.
[00:45:33.980 --> 00:45:35.540]   We knew that there'd be deploy issues.
[00:45:35.540 --> 00:45:39.860]   We knew that there'd be security issues for deploying on-prem, on sensitive data.
[00:45:39.860 --> 00:45:41.340]   All of that was known.
[00:45:41.340 --> 00:45:45.700]   I think coming into this, the importance of not just the...
[00:45:45.700 --> 00:45:48.820]   And we also knew that there'd be a volume of training data.
[00:45:48.820 --> 00:45:53.980]   What I didn't think at the top of this and it surprised me was the specificity of the
[00:45:53.980 --> 00:45:59.740]   training data drives the performance of the models in ways that are just not obvious when
[00:45:59.740 --> 00:46:01.420]   you start out on this.
[00:46:01.420 --> 00:46:07.900]   And these things are kind of excellent prediction machines, but they're also excellent cheaters.
[00:46:07.900 --> 00:46:13.500]   And they'll find ways to cheat and find the right answer, but it's because you gave them
[00:46:13.500 --> 00:46:14.500]   the wrong data.
[00:46:14.500 --> 00:46:18.220]   And I think that sensitivity to the data is something that's really surprised me.
[00:46:18.220 --> 00:46:23.500]   Now on the flip side of that, if you start investigating methods of exposing these models
[00:46:23.500 --> 00:46:27.380]   to the right data, you also get wonderful performance in ways that go above and beyond
[00:46:27.380 --> 00:46:29.980]   sort of the general applications.
[00:46:29.980 --> 00:46:33.460]   So I think it's a blessing and a curse, but I don't think going into this, if I'd been
[00:46:33.460 --> 00:46:37.860]   told that that would be the thing that kind of drove the most kind of performance, that
[00:46:37.860 --> 00:46:39.100]   I would have agreed to that.
[00:46:39.100 --> 00:46:41.020]   So that's probably the biggest surprise.
[00:46:41.020 --> 00:46:42.660]   Well, thanks so much.
[00:46:42.660 --> 00:46:45.300]   This is really fun and fascinating.
[00:46:45.300 --> 00:46:46.300]   My pleasure.
[00:46:46.300 --> 00:46:47.300]   I've enjoyed it a lot.
[00:46:47.300 --> 00:46:48.300]   Thanks, Lucas.
[00:46:48.300 --> 00:46:52.460]   Thanks for listening to another episode of Gradient Dissent.
[00:46:52.460 --> 00:46:56.700]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:46:56.700 --> 00:46:59.460]   hear from the people that are listening to these episodes.
[00:46:59.460 --> 00:47:03.540]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:47:03.540 --> 00:47:07.500]   a conversation, that would make me inspired to do more of these episodes.
[00:47:07.500 --> 00:47:11.020]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.
[00:47:11.020 --> 00:47:13.600]   (upbeat music)

