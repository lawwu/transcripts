
[00:00:00.000 --> 00:00:02.160]   Why did you choose to work on robotics?
[00:00:02.160 --> 00:00:09.040]   So actually here is a reveal. I was actually working for several years on robotics and
[00:00:09.040 --> 00:00:16.000]   as of recently we changed the focus at OpenAI and I'm actually, I disbanded the robotics team.
[00:00:16.000 --> 00:00:18.800]   Oh wow. Why did you do that?
[00:00:18.800 --> 00:00:29.680]   So, okay. So the reasoning is that there's like a few pieces. So it turns out that we can
[00:00:30.160 --> 00:00:37.200]   make a gigantic progress whenever we have access to data. And all our machinery and
[00:00:37.200 --> 00:00:42.000]   supervisor there in reinforcement learning, they work extremely well. And there is actually
[00:00:42.000 --> 00:00:49.120]   plenty of domains that are very, very rich with data. And ultimately that was holding us back in
[00:00:49.120 --> 00:00:57.680]   case of robotics. And I mean, this decision was quite hard for me. I got the realization some
[00:00:57.680 --> 00:01:03.520]   time ago that actually that's the best from perspective of the company. And the sad thing is,
[00:01:03.520 --> 00:01:11.360]   I think if it would be a robotics company or if the mission of the company would be different,
[00:01:11.360 --> 00:01:16.560]   then I think we would just continue. I actually quite strongly believe in the
[00:01:16.560 --> 00:01:22.720]   approach that robotics took and the direction. But from perspective of what we want to achieve,
[00:01:22.720 --> 00:01:28.240]   which is to build AGI, I think there was actually some components missing. So when we created
[00:01:28.240 --> 00:01:35.360]   robotics, we thought that we can go very far with self-generated data and the reinforcement learning.
[00:01:35.360 --> 00:01:44.400]   At the moment, I believe that actually pre-training allows to give model 100x
[00:01:44.400 --> 00:01:49.360]   cheaper IQ points. And then that might be followed with other techniques.
[00:01:49.360 --> 00:01:51.680]   And what is pre-training?
[00:01:52.240 --> 00:02:00.080]   Pre-training, that's I can explain it in case of GPT-3. So pre-training in case of GPT-3 or in
[00:02:00.080 --> 00:02:07.040]   case of like a language models means training them on some unsupervised task such as next
[00:02:07.040 --> 00:02:12.320]   word prediction. And that builds in all the internal representation that allows model to
[00:02:12.320 --> 00:02:19.360]   off the bat to solve many tasks. And in case of robotics, we haven't had such a data.
[00:02:20.560 --> 00:02:24.880]   I see. So do you regret working on robotics?
[00:02:24.880 --> 00:02:32.880]   No. I think that actually we've got plenty of insights for other projects. I think that also
[00:02:32.880 --> 00:02:39.280]   we build really amazing technology. I would say I'm actually very proud. There was like,
[00:02:39.280 --> 00:02:45.600]   of course, moments of sadness when I was making this decision, but I'm quite happy where we've
[00:02:45.600 --> 00:02:51.680]   got. Also, I would say even from my own perspective, in the meanwhile, I manage
[00:02:51.680 --> 00:03:00.640]   also other things that made some significant progress in the meanwhile. And there will be
[00:03:00.640 --> 00:03:02.800]   more information about it sometime.
[00:03:03.600 --> 00:03:07.520]   Thanks for watching this clip. You can see the full episode on our YouTube channel.
[00:03:07.520 --> 00:03:14.000]   And you can join our friendly Slack community with over 4,000 ML engineers to participate in
[00:03:14.000 --> 00:03:22.960]   paper reading groups, AMAs, and other fun events.

