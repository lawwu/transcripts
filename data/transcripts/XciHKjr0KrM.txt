
[00:00:00.000 --> 00:00:05.680]   Okay, hi everybody, my name is Boris. Today I'm going to talk to you about a little project that
[00:00:05.680 --> 00:00:12.400]   I had that I did recently that I think is pretty fun. It's called, I called it Hugging Tweets
[00:00:12.400 --> 00:00:20.000]   because it's using like a hugging face on Twitter. So the idea is basically to be able to generate
[00:00:20.000 --> 00:00:26.720]   tweets using hugging face. And I'm going to show you quickly the library but very quickly who I am.
[00:00:28.720 --> 00:00:35.120]   I graduated from France and a school in Brazil and I studied, my background was mainly related
[00:00:35.120 --> 00:00:41.760]   to math, science, engineering, computer science. I was actually before working for the energy
[00:00:41.760 --> 00:00:46.480]   industry so something totally unrelated and something that's pretty cool I think about
[00:00:46.480 --> 00:00:51.840]   machine learning in general and probably a lot of you are in the same case is we've been able to
[00:00:52.720 --> 00:00:59.600]   learn ourselves with all the resources online. So I did a lot of the Coursera, Udacity and
[00:00:59.600 --> 00:01:06.000]   all kind of public channels and then I was able to learn a bit more by reading the
[00:01:06.000 --> 00:01:12.720]   publications that interest me and then finally doing my own projects. So I've been involved
[00:01:12.720 --> 00:01:21.520]   doing some kind of AI machine learning projects with Waze and Biosys now for a little while and
[00:01:21.520 --> 00:01:26.640]   started with a competition about colorizing and I've been very lucky to work with them because
[00:01:26.640 --> 00:01:32.480]   I've been able to do a lot of really cool projects. So I'm excited to show you the one that I'm doing
[00:01:32.480 --> 00:01:44.800]   today. So quickly what is Hugging Tweets? So the idea is basically you have a user that you choose
[00:01:44.800 --> 00:01:50.240]   with your handle. So for people who shouldn't recognize it, it's André Karpathy. He was like
[00:01:50.800 --> 00:01:57.440]   with another director of AI and does a lot of really cool stuff. Then what you want to do,
[00:01:57.440 --> 00:02:06.000]   you want to create a Hugging Face model with it and train it by downloading all the data from
[00:02:06.000 --> 00:02:11.040]   André Karpathy and optimize it, fine-tune the model on that. Then the idea is you just give
[00:02:11.040 --> 00:02:16.560]   an input sentence. So you just give a start. So for example, I put "I don't like" and this is
[00:02:16.560 --> 00:02:22.960]   what you put. I think that it is pretty amazing because like I let you read it, but like I don't
[00:02:22.960 --> 00:02:28.880]   like this. "920am, forget this little low-code and preprocessor optimization, even if it's neat
[00:02:28.880 --> 00:02:35.920]   for top-level projects. 927am, other useful code examples." It's not kind of base code. "937am,
[00:02:35.920 --> 00:02:41.760]   Python drawing dog like crazy. Restarts regular web browsing. 946am, okay, I don't mind. Maybe
[00:02:41.760 --> 00:02:47.520]   I should try that out. I'll investigate it. 10am, I think I should try Shimigitsu in gear page or
[00:02:47.520 --> 00:02:53.840]   the minimalist website if you're after 10 results. Also maybe Google ImageNet on Yelp instead.
[00:02:53.840 --> 00:02:59.520]   "10.05, looking forward to watching it talk." That's like all really crazy and like impossible
[00:02:59.520 --> 00:03:03.440]   to understand, but I feel like it's something that André Karpathy could actually have said.
[00:03:03.440 --> 00:03:10.800]   So it's kind of fun to see that happening. So now I'm going to show you quickly how it works. So
[00:03:11.520 --> 00:03:15.920]   what is HackingFace for the people who don't know it? Honestly, they have a really, really
[00:03:15.920 --> 00:03:22.960]   cool documentation. I would recommend to just go on their website. The first thing I do when I
[00:03:22.960 --> 00:03:28.320]   want to learn about something, I just go on their website and I go on the docs and I just read all
[00:03:28.320 --> 00:03:34.800]   of it. And some people approach it a bit differently, but I think their documentation
[00:03:34.800 --> 00:03:39.840]   is really, really interesting and really well done. So I'm going to summarize it really quickly
[00:03:39.840 --> 00:03:45.600]   in a few words. Basically, they have a library that's using Python that's used for natural
[00:03:45.600 --> 00:03:52.480]   language processing, understanding, and generation. And they provide you some tokenizers to process
[00:03:52.480 --> 00:03:59.680]   some text data. So typically when you train a model, you won't be able to input directly the
[00:03:59.680 --> 00:04:06.560]   text. You could convert it into some direct input like character by character or sentence by
[00:04:06.560 --> 00:04:12.160]   sentence. But typically, there's more advanced tokenizer that does something a bit more complex
[00:04:12.160 --> 00:04:17.600]   and that gives you much better results. They have a lot of pre-trained models in PyTorch and
[00:04:17.600 --> 00:04:22.880]   TensorFlow. They also let you convert from one to the other, which is pretty cool because some
[00:04:22.880 --> 00:04:27.040]   people like to develop in PyTorch, but maybe they use TensorFlow as production models.
[00:04:27.040 --> 00:04:31.600]   And then something that's really interesting, actually, I'm going back to their website.
[00:04:32.960 --> 00:04:38.640]   I recommend really that for most of the tasks, what you do, they have an examples page.
[00:04:38.640 --> 00:04:47.040]   And basically on that page, they created scripts that can solve most of the problems you will want.
[00:04:47.040 --> 00:04:53.120]   If they don't solve it directly, which really, really, they're applicable to a lot of different
[00:04:53.120 --> 00:04:58.240]   tasks. But if they don't solve it directly, you just choose whatever is the most common,
[00:04:58.240 --> 00:05:03.120]   you look at it. They're really well written, really clean. You have arguments if you want to do
[00:05:03.120 --> 00:05:08.640]   gradient distribution, gradient accumulation. You do kind of do have precision training just by
[00:05:08.640 --> 00:05:14.240]   adding like command lines and they're really, really well done. So I would definitely recommend
[00:05:14.240 --> 00:05:19.600]   you to look at them. And the other thing that's nice, you can basically go as deep as you want.
[00:05:19.600 --> 00:05:24.720]   If you want to create your own custom models, complex pipeline, I'll let you do that. So
[00:05:24.720 --> 00:05:29.680]   you're as free to directly use it with pre-trained model. That's fine tune anything at all.
[00:05:29.680 --> 00:05:35.440]   Or like going into more advanced level. It's pretty nice that customization.
[00:05:35.440 --> 00:05:42.880]   The new release that Lavanya was talking about is basically now Hacking Face and Weights and Biases
[00:05:42.880 --> 00:05:50.240]   are directly integrated. So Weights and Biases is part of the Hacking Face library. The way it
[00:05:50.240 --> 00:05:55.760]   works is really, really easy. Again, I put a link to the document page, but the way it works is
[00:05:55.760 --> 00:06:02.400]   really, really convenient. You do just keep install 1db. And if your Weights and Biases is installed
[00:06:02.400 --> 00:06:09.920]   and we did the login in the command line or in your Jupyter notebook or wherever, your data is
[00:06:09.920 --> 00:06:17.440]   automatically going to be logged. It's not entirely true yet because it's mainly on the PyTorch model.
[00:06:17.440 --> 00:06:24.080]   But so on TensorFlow model, it's not implemented yet, but it's probably going to happen soon.
[00:06:24.080 --> 00:06:31.520]   I'm going to show you really quickly how it works. Basically, Hacking Face refactored their
[00:06:31.520 --> 00:06:37.760]   entire code, made it much, much cleaner. And they have that class that's called Trainer.
[00:06:37.760 --> 00:06:44.720]   And in that class, if I search for 1db, you can see basically the program only check,
[00:06:44.720 --> 00:06:50.720]   "Do you have 1db?" And if it's installed, everything is going to happen automatically.
[00:06:50.720 --> 00:06:56.240]   Your metrics are going to be logged, your config parameters, your losses,
[00:06:56.240 --> 00:07:02.480]   and you will be able to see all the things you want. We are going to see it in that example.
[00:07:02.480 --> 00:07:09.760]   So now let's go directly into the Hacking Tweets. How does it work?
[00:07:11.040 --> 00:07:16.160]   So I have a repo. It's probably going to change really quickly.
[00:07:16.160 --> 00:07:26.000]   And the way it works is very, very simple. We're going to go through the code to understand a bit
[00:07:26.000 --> 00:07:32.560]   better the principle. Basically, we need to develop the tweets for that. There's a library
[00:07:32.560 --> 00:07:38.080]   that's pretty cool that integrates with the Twitter API. So you need to have a Twitter
[00:07:38.080 --> 00:07:43.600]   development account. Honestly, all that process takes maybe two, three minutes, and you will have
[00:07:43.600 --> 00:07:49.920]   API keys. And once you have it, there's a cool library called Tweepy. And with that, you can
[00:07:49.920 --> 00:07:54.240]   just make some easy calls through Python to download all tweets and all.
[00:07:54.240 --> 00:08:00.480]   So here, what I do typically, I will put the name of a person I want their tweets.
[00:08:00.480 --> 00:08:07.760]   And then you iterate through it and you download it. Now there's a limit. You cannot download more
[00:08:07.760 --> 00:08:16.240]   than 3,200 tweets. Actually, if you try to play, 3,200 tweets sounds like a lot. It's really not a
[00:08:16.240 --> 00:08:23.200]   lot. It's not that much data. I mean, I'm really far from it. But some people are much, much beyond
[00:08:23.200 --> 00:08:30.080]   that. And you need to consider that in the tweets, there's the retweets that are kind of useless.
[00:08:30.080 --> 00:08:35.440]   There's the ones that you just share a URL, and they're useless as well. So there's a lot of data
[00:08:35.440 --> 00:08:41.840]   that's not that interesting. So all that function does is just to not link tweets.
[00:08:41.840 --> 00:08:51.440]   Then what we want to do, basically to create our data set, we are going to fine-tune our model
[00:08:51.440 --> 00:08:57.760]   based on GPT-2, which is the open AI model that was trained on a lot, a lot, a lot of text, but
[00:08:57.760 --> 00:09:04.240]   it obviously was not trained on Twitters. So it works better if the text is not too strange. So
[00:09:05.200 --> 00:09:11.440]   there's a few things that are problematic. For example, you have a lot of @ to mention people.
[00:09:11.440 --> 00:09:19.920]   And what we do here to clean this up, we replace the @ something like @chiropathy by the name of
[00:09:19.920 --> 00:09:29.120]   the person so that the text doesn't look as strange. Then we try to clean up the data. So
[00:09:29.120 --> 00:09:35.280]   we have a lot of hashtags. So imagine you're trying to predict some text and the person says,
[00:09:35.280 --> 00:09:45.120]   "Here is my new paper, #convnets, #ilovegans." It's not really something that GPT-2 was trained
[00:09:45.120 --> 00:09:50.880]   on. So it really wouldn't work. So what we do is something we remove, something we change.
[00:09:50.880 --> 00:09:57.280]   There's a lot of improvement I can still do there, but the idea is you want your text to not look
[00:09:57.280 --> 00:10:03.600]   too strange, but you still want to train to have some predictions that look like tweets.
[00:10:03.600 --> 00:10:11.680]   So as you can see so far, most of my work is really on creating the dataset. And there's a
[00:10:11.680 --> 00:10:16.080]   lot of work there and a lot of improvements I want to do are related to that. Like here,
[00:10:16.080 --> 00:10:22.320]   I tried to remove all the retweets. I tried to clean up the text. It's presented in some strange
[00:10:22.320 --> 00:10:30.640]   manner. Something also funny, like if I go on Twitter, I start replacing the names of people.
[00:10:30.640 --> 00:10:37.040]   Let's say, for example, I have Jeremy Howard. Jeremy Howard, I replaced by his handle,
[00:10:37.040 --> 00:10:45.040]   it's Jeremy #mask4all Howard. That's not really convenient, so you need to be able to work with
[00:10:45.040 --> 00:10:54.480]   that. Another one, I want to do Lavanya. She has an emoji in her name. How do I train on that? It's
[00:10:54.480 --> 00:11:00.720]   not something that GPT-2 is trained on. So those things are things you need to clean up, and
[00:11:00.720 --> 00:11:06.320]   otherwise your model is not really going to work. So once I have all those tweets, I split training
[00:11:06.320 --> 00:11:14.720]   data, test data. All that so far, I only worked on cleaning the dataset, right? So then that's
[00:11:14.720 --> 00:11:22.720]   when it becomes interesting training the model. So I just have to log in on my 1DB so that I have
[00:11:22.720 --> 00:11:31.680]   my run instrumented automatically. Here I have a variable just so that they are cleanly organized
[00:11:31.680 --> 00:11:38.240]   into my HuggingFace product. But here is what is cool. My training program is only that. I'm just
[00:11:38.240 --> 00:11:44.800]   using the script. I didn't write any single line of code. The scripts are pretty nice. The main
[00:11:44.800 --> 00:11:51.440]   stuff I changed here, you see I say I want a GPT-2 model, so it's just going to download a
[00:11:51.440 --> 00:11:57.680]   pre-trained model. I say where is my data? So I have training set validation set I saved.
[00:11:59.200 --> 00:12:04.160]   I just say I want to evaluate to have validation loss, et cetera, how much I want to log to have
[00:12:04.160 --> 00:12:12.320]   enough points. The main parameter I changed was that one. I could train only one batch per GPU
[00:12:12.320 --> 00:12:21.200]   because otherwise I would run out of CUDA memory. By the way, that can have an impact because by
[00:12:21.200 --> 00:12:27.120]   default I think I have eight. I have only one. Something I didn't try. Technically,
[00:12:27.760 --> 00:12:36.720]   I should maybe want to accumulate eight batches before updating my weights. It could have some
[00:12:36.720 --> 00:12:44.320]   impact. I didn't test that yet. Then I trained on a lot of epochs. Basically, when I had my first
[00:12:44.320 --> 00:12:51.280]   runs, I saw that I start overfitting after four epochs. Typically, it's that for most of the
[00:12:51.280 --> 00:12:57.760]   people I used it on. I just trained for four epochs. Then what you do, I want to test it.
[00:12:57.760 --> 00:13:07.520]   I feed a few sentences and then what is cool is it's the same way. I just run my generation model
[00:13:07.520 --> 00:13:16.800]   with a script from Hacking Face again, which is that script. I didn't really write anything. All
[00:13:16.800 --> 00:13:24.000]   the part that was related to having a pre-trained model, tokenizing, fine tuning, running generation,
[00:13:24.000 --> 00:13:30.720]   I didn't have to think about that, which is pretty cool. I got pretty interesting results.
[00:13:30.720 --> 00:13:34.240]   Here I'm going to go quickly to my dashboard.
[00:13:34.240 --> 00:13:45.200]   You can see I have predictions on a lot of different people. Here is my input. Here is my
[00:13:45.200 --> 00:13:56.720]   output. A lot of them are actually really funny. I had actually a lot of fun running it. Some of
[00:13:56.720 --> 00:14:01.840]   the fun ones were obviously from Chiropathy. Then there was some other stuff like Lavanya.
[00:14:01.840 --> 00:14:09.120]   She used a lot of emojis in her tweets, which is kind of funny. I have some Hacking Face people
[00:14:09.120 --> 00:14:15.520]   like Julia. There's some Hacking Face emoji in his predictions, which is interesting. You see,
[00:14:15.520 --> 00:14:23.920]   it's kind of learning something. Then I have Jan Lokum talking that he likes French or sometimes
[00:14:23.920 --> 00:14:33.200]   he starts speaking in French. I have a cool one too. Just quickly, I want to show from Google.
[00:14:37.840 --> 00:14:44.240]   Google AI talking about a new paper, right? C. D. Moe and the VP Blog. I like the title of that
[00:14:44.240 --> 00:14:51.520]   paper. A new algorithm that can simulate motion in NLP speech is being developed that solves the
[00:14:51.520 --> 00:14:57.040]   natural language recognition problem of developing computational models for motion-related content
[00:14:57.040 --> 00:15:01.680]   and experience. Wow, that sounds pretty cool. That sounds pretty interesting. I'd like to read that.
[00:15:03.040 --> 00:15:07.440]   That actually sounds like a real paper. I don't know if it could be actually something,
[00:15:07.440 --> 00:15:11.920]   but that's interesting to see that kind of prediction. What I'd like to say that I didn't
[00:15:11.920 --> 00:15:18.640]   look that much, that's kind of strange to me. You have different losses based on the person, you see?
[00:15:18.640 --> 00:15:23.440]   It's interesting to see where does that come from. Does it come from the dataset?
[00:15:23.440 --> 00:15:29.920]   Does it come from maybe some people use more vocabulary than others? Basically, the higher
[00:15:29.920 --> 00:15:34.800]   your loss is, the more unpredictable you are. Here you see, like, "Andrej Karpathy is the most
[00:15:34.800 --> 00:15:38.640]   unpredictable when you write something." Then you have people that are more predictable. Maybe the
[00:15:38.640 --> 00:15:43.840]   subject comes more often or they use more often the same words. But you have also interesting
[00:15:43.840 --> 00:15:48.560]   stuff. Some people start low, but they don't decrease much, while some people start high,
[00:15:48.560 --> 00:15:54.320]   but they decrease really, really fast. Once the program has learned what people are talking about,
[00:15:55.440 --> 00:16:04.400]   it kind of trains much faster. That's about it. Here I have a lot of different ideas of things
[00:16:04.400 --> 00:16:09.600]   to try. You can see most of it is related to actually the dataset. That's where I need to
[00:16:09.600 --> 00:16:15.040]   work on. The model and the text generation, I don't need to do anything because Hugging Face
[00:16:15.040 --> 00:16:20.320]   does it for me. It's pretty cool. Something that's kind of impressive that I liked is that
[00:16:20.320 --> 00:16:25.920]   my datasets are really, really small. In the best case, I have 200 kilobytes, and sometimes I have
[00:16:25.920 --> 00:16:33.440]   less than 100. When I get 200, it's much, much better, but typically a good dataset is at least
[00:16:33.440 --> 00:16:38.560]   five megabytes or much bigger. It's like 20 times more. Some are like 100 times more.
[00:16:38.560 --> 00:16:45.120]   You still get cool results. Something that I want to try, too, I want to pre-train on the
[00:16:45.120 --> 00:16:50.960]   entire Twitter so that basically I would have a model that learns how to make tweet-like messages,
[00:16:50.960 --> 00:16:57.680]   and then basically you fine-tune on the person. I think that would be a cool iteration to try.
[00:16:57.680 --> 00:17:03.120]   The last thing I like, too, is that idea, which I think between the top and bottom layers,
[00:17:03.120 --> 00:17:09.920]   some are more related to predicting the next word. It's kind of more like memorization,
[00:17:10.560 --> 00:17:16.000]   while some are more related to understanding what the person is talking about. For example,
[00:17:16.000 --> 00:17:22.240]   you train on the machine learning people. Typically, they all talk about AI. It's like
[00:17:22.240 --> 00:17:27.440]   the lexical field. I think those kinds of concepts are in different layers, and maybe if you train
[00:17:27.440 --> 00:17:32.720]   them differently, you will get some models that are better on memorization or some that are
[00:17:33.760 --> 00:17:38.080]   more creative and that mainly take on text and on the subject.
[00:17:38.080 --> 00:17:45.760]   That's about it. Feel free to play with it. I think there's a lot of things to see,
[00:17:45.760 --> 00:17:51.760]   and I'd love to see your results. There's a lot of further things to try.
[00:17:51.760 --> 00:17:56.240]   I don't know if you have any questions. I saw my screen beeping a little bit.
[00:17:59.120 --> 00:18:04.800]   Thanks, Boris. That was amazing. Can people ask questions? We were having some technical issues,
[00:18:04.800 --> 00:18:09.760]   so we were trying to deal with those. Nothing related to you. You were perfect.
[00:18:09.760 --> 00:18:16.880]   If people want to start asking questions, you can do that either in the Q&A or you can do it in the
[00:18:16.880 --> 00:18:23.840]   chat, and then maybe Nick and Kayla, you can pull questions from the YouTube channel. I see a
[00:18:23.840 --> 00:18:29.360]   question here. Someone asked, "Are there plans to support recent biases logging of NLP tests
[00:18:29.360 --> 00:18:35.280]   beyond the table format, specifically token tagging?" I love this question. Currently,
[00:18:35.280 --> 00:18:40.800]   I'm writing something custom with HTML, but it would be nice to have it built in.
[00:18:40.800 --> 00:18:48.400]   So, yes, we are working on that. I can share with you one report that we haven't made public,
[00:18:48.400 --> 00:18:55.120]   but if you would like to test it out and give us some feedback, you can find you can log attention
[00:18:55.120 --> 00:19:03.440]   maps, NER, so named entity recognition stuff, part of speech tagging graphs, and there's a
[00:19:03.440 --> 00:19:10.480]   bunch of other cool stuff in there. We also have built-in support for LIMES, Eli5 predictions,
[00:19:11.040 --> 00:19:21.040]   and all of that stuff you can find in this report right here. But if you're looking for
[00:19:21.040 --> 00:19:28.640]   something specific, please let us know. Either DM me or something. >> Actually, just to add on that,
[00:19:28.640 --> 00:19:33.920]   the tables, they're not logged directly by the integration right now. Maybe they would be later
[00:19:33.920 --> 00:19:41.680]   if it's something that everybody wants to do. But what is done automatically are all the metrics
[00:19:41.680 --> 00:19:48.720]   and all the reports. That, I just decided to edit myself, and it's pretty easy. If you go to the
[00:19:48.720 --> 00:19:58.080]   documentation, you have a logging page, and you can log all kinds of objects. I just went here.
[00:19:58.080 --> 00:20:04.080]   I went to log text tables, and you just have a specific format, and I added it in there. Same,
[00:20:04.080 --> 00:20:10.000]   if you want to log HTML, you already have some functions you can use, and you can add your own.
[00:20:10.000 --> 00:20:20.720]   You have all kinds of objects that can be added. >> Boris, I was wondering if we could get a few
[00:20:20.720 --> 00:20:27.920]   more example tweets from your Lavanya tweet bot. I'm just kind of curious what we got there.
[00:20:27.920 --> 00:20:35.440]   >> So, let's go through here quickly. I'm going to go through this one quickly. >> And then also,
[00:20:35.440 --> 00:20:42.800]   maybe we can do Lucas next. >> Lucas, yeah. I have a lot of people. I had way too much fun.
[00:20:42.800 --> 00:20:51.680]   I need to scroll it. Okay, Lavanya, right? So, you see some emojis appearing. I'll give you that
[00:20:51.680 --> 00:20:59.040]   link. They're really, really fun. So, you see Lavanya, she wants to create a collection of
[00:20:59.040 --> 00:21:03.840]   fiction scripts for women. So, I learned a lot of stuff from Lavanya without having to read all her
[00:21:03.840 --> 00:21:11.200]   tweets. It's pretty cool. Something funny, too, actually. Let me show you quickly. Elon Musk.
[00:21:11.200 --> 00:21:20.000]   Something I didn't know about him is where is that? >> Because we're taking this as facts now,
[00:21:20.000 --> 00:21:29.520]   whatever this guy is tweeting. >> Oh, no, Jack Clark. Sorry. Wrong one. Oh, yeah. He wants
[00:21:29.520 --> 00:21:38.160]   he had the cool one. He has the one that he wants to create a solar electric car,
[00:21:38.160 --> 00:21:46.160]   which is pretty interesting. >> I'm going to pop a link to Boris's GitHub repo so you can actually
[00:21:46.160 --> 00:21:50.400]   plug in your own Twitter account or someone else's Twitter account and actually create these and
[00:21:50.400 --> 00:21:58.400]   have fun with your friends. >> Yeah, and you can open at the bottom, you will have a link to the
[00:21:58.400 --> 00:22:04.720]   dashboard so you can read all the sentences. I think some are great, some are so-so, but what
[00:22:04.720 --> 00:22:10.720]   I like is just the first version, first iteration. There's so many things to improve, and I think
[00:22:10.720 --> 00:22:17.840]   it's going to get much, much better from there. Actually, for Lucas, for Lucas, I had trouble
[00:22:17.840 --> 00:22:23.120]   in having nice generation because I don't know, I need to look more at his Twitter account. I think
[00:22:23.120 --> 00:22:30.480]   he must be talking about too many random subjects. One thing, though, he's always so positive. I
[00:22:30.480 --> 00:22:35.120]   think that's really good. I think that is great fun. I want to help. I imagine it's really Lucas.
[00:22:35.680 --> 00:22:46.800]   It's really the way he talks. It's kind of fun. >> I have a question, Boris. You mentioned to me
[00:22:46.800 --> 00:22:52.480]   earlier you spent most of the time on data, not on fine tuning the model itself. Can you talk about
[00:22:52.480 --> 00:22:58.240]   why you spent most of your time on the data? >> Yeah, so in most of the projects I've done so
[00:22:58.240 --> 00:23:04.960]   far, when I started learning, you always spend all your time on fine tuning and should I add
[00:23:04.960 --> 00:23:12.560]   more layers or should I try another model and all. In the end, typically, just use a model that's
[00:23:12.560 --> 00:23:18.400]   famous and that works for your task. For example, I want to do text generation. Just use GPT-2.
[00:23:18.400 --> 00:23:24.720]   Don't try to change the model. GPT-2 is good for that. You won't get a better model than OpenAI
[00:23:24.720 --> 00:23:31.760]   easily. There are default parameters that just worked, but when you have a better dataset,
[00:23:31.760 --> 00:23:39.040]   you get quickly results really fast. Your model improves. It's a better investment of your time,
[00:23:39.040 --> 00:23:43.680]   I think. So, typically, I start improving the model when I have no more ideas.
[00:23:43.680 --> 00:23:53.280]   Yeah, I still have too many. I'm not going to change the model for a while, but feel free to
[00:23:53.280 --> 00:24:01.360]   change it and maybe you'll get cool stuff. >> Thanks. Can you share the link for the
[00:24:01.360 --> 00:24:08.400]   Slack channel? Yes, Kayla, can you share the link? And then also, Boris is going to be in that
[00:24:08.400 --> 00:24:15.120]   Slack channel. So, if you want to pop in tomorrow, the day after this weekend to ask him questions,
[00:24:15.120 --> 00:24:22.240]   we'd be happy to answer your questions. Yeah. And also, the integration is officially launching
[00:24:22.240 --> 00:24:26.640]   tomorrow. There's a lot more examples on fine tuning, running hyperparameter optimization
[00:24:26.640 --> 00:24:32.640]   through Hugging Face that are coming tomorrow. So, watch out. I'll tweet about them and we'll
[00:24:32.640 --> 00:24:37.600]   also post them in the Slack so you can check them out. Thank you, Boris. That was really fun.
[00:24:37.600 --> 00:24:39.280]   >> Thank you. That was a lot of fun.

