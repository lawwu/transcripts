
[00:00:00.000 --> 00:00:04.320]   tumor competition that just launched yesterday, I think or the day before.
[00:00:04.320 --> 00:00:05.840]   >> Yeah. >> Okay, we're live.
[00:00:05.840 --> 00:00:10.400]   >> So feel free to start. >> Cool, we'll start now then.
[00:00:10.400 --> 00:00:16.720]   Excellent. Hey, everybody, and welcome back to another session of our paper reading
[00:00:16.720 --> 00:00:22.160]   of birthweights and biases. And I'm really excited today because we're joined by Dr. Habib Bukhari.
[00:00:22.160 --> 00:00:28.960]   Before that, we'll be discussing the MLP mixer and all MLP architecture for vision,
[00:00:28.960 --> 00:00:35.600]   which came out recently, I think about a month or two ago. And this paper has been really exciting.
[00:00:35.600 --> 00:00:42.000]   And it's like, it takes away the architectural constraints. It's a really easy architecture
[00:00:42.000 --> 00:00:46.880]   to understand and yet it performs really well. So we're going to be looking at what this
[00:00:46.880 --> 00:00:56.960]   architecture is. And then as usual, as every paper reading group, there's a link for everybody. So
[00:00:56.960 --> 00:01:02.160]   I'm just going to post this in the chat. So if you have any questions, please go to this link that
[00:01:02.160 --> 00:01:10.240]   will take you to a report. So let me do it now. I'll share. Can everybody still see my screen?
[00:01:10.240 --> 00:01:16.480]   You should be able to see my Firefox. Does that work?
[00:01:16.480 --> 00:01:20.320]   >> I don't see you, dude. >> Oh, you don't see my screen?
[00:01:20.320 --> 00:01:22.400]   I've been screen sharing for a while. >> Me neither.
[00:01:22.400 --> 00:01:29.440]   >> Oh, I'm sorry. Let me try again. So you guys didn't see the PowerPoint presentation either.
[00:01:29.440 --> 00:01:35.360]   I see. Okay. Does that work? Is that better?
[00:01:35.360 --> 00:01:39.840]   Or not? >> No.
[00:01:39.840 --> 00:01:41.960]   >> I don't see anything still. >>
[00:01:44.480 --> 00:01:53.600]   Okay. I guess I will try and join back. One last try before I go. Is that still not sharing
[00:01:53.600 --> 00:02:12.320]   anything? I'm really sorry. I'll try and join again. I'll leave the chat and join again.
[00:02:12.320 --> 00:02:24.160]   >>
[00:02:24.160 --> 00:02:35.920]   Someone mentioned in the chat to me that they want to know about the competition. So if you just head
[00:02:35.920 --> 00:02:41.120]   over to Kaggle, Habib is an expert in medical competitions. He'll be a grandmaster really
[00:02:41.120 --> 00:02:47.040]   soon. I'm really hoping that. So a brain tumor classification competition launched this week.
[00:02:47.040 --> 00:02:51.280]   That's what we're talking about. >> One last try. Can everybody see
[00:02:51.280 --> 00:02:52.800]   my screen now? >> Yes.
[00:02:52.800 --> 00:02:55.200]   >> Yes. >> Okay. Finally. So I've been
[00:02:55.200 --> 00:03:00.080]   using the presentation so far. Okay. We'll have to get started again. I'll just do a quick one this
[00:03:00.080 --> 00:03:06.400]   time just to save time. So welcome, everybody, to the paper reading group. And we've had a
[00:03:06.400 --> 00:03:11.520]   fair share of technical difficulties today, but I hope that's the last of them. So as part of our
[00:03:11.520 --> 00:03:17.840]   paper reading group today, we're looking at the MLP mixer paper. And I'm really, really excited
[00:03:17.840 --> 00:03:24.720]   that I'm joined by Dr. Habib today. Dr. Habib and I have spent the past, I think, a few days,
[00:03:24.720 --> 00:03:29.440]   almost speaking every day, a few hours, just discussing the MLP mixer architecture between
[00:03:29.440 --> 00:03:35.600]   ourselves. And I'm really excited Dr. Habib is here with us today. But before that, just a very
[00:03:35.600 --> 00:03:44.160]   quick pointer, just like every previous paper reading group, we still have this link, 1db.me/mlpmixer.
[00:03:44.160 --> 00:03:49.920]   So I believe I've posted that in the chat. Anjalika has. Perfect. Thanks, Anjalika.
[00:03:49.920 --> 00:03:58.480]   So if I go to my -- if I type in my browser 1db.me/mlpmixer, that will take us to a report,
[00:03:58.480 --> 00:04:03.360]   which is this paper reading group, MLP mixer with special guest Dr. Habib. And then
[00:04:04.000 --> 00:04:08.880]   towards the end, I could just write a comment and I could say test. So this is where we'll be using
[00:04:08.880 --> 00:04:13.680]   all our questions and answers. And this has been the format since the beginning. So we're just
[00:04:13.680 --> 00:04:20.160]   going to be using the comments here as we go to ask questions. So I won't be monitoring the Zoom
[00:04:20.160 --> 00:04:24.400]   chat. I won't be looking at the YouTube live chat. It's just very difficult to manage too. So we're
[00:04:24.400 --> 00:04:30.800]   just going to be using the comments here. So about Dr. Habib, really excited and welcome,
[00:04:30.800 --> 00:04:36.720]   Dr. Habib. I know you have a PhD in structural biology and then you decided to learn coding and
[00:04:36.720 --> 00:04:43.840]   like many, many others ended up being a part of the FastAI family. And then you started Kaggle.
[00:04:43.840 --> 00:04:52.720]   And starting Kaggle was -- I've seen your work. I've known you. I've followed you. I've been a
[00:04:52.720 --> 00:04:59.120]   big fan of what you've achieved through the years and seeing you as current rank 211 with four
[00:04:59.120 --> 00:05:04.560]   golds and eight silvers, I think the only next step is like a solo gold and you'll be a Kaggle
[00:05:04.560 --> 00:05:10.400]   grandmaster very soon. And now you're working as an applied deep learning engineer at Janelia. So
[00:05:10.400 --> 00:05:18.160]   really excited to have you with us. Welcome. And if I missed anything and if there's anything you
[00:05:18.160 --> 00:05:23.520]   want to add. >> Thank you. First of all, thank you very much for inviting me. It's an honor
[00:05:23.520 --> 00:05:31.520]   to be here. Really, you know, together with you. I also, you know, I think our journey started in
[00:05:31.520 --> 00:05:37.920]   parallel. You started with the blog. I started with the Kaggle. And now we're kind of, you know,
[00:05:37.920 --> 00:05:45.920]   merging. Yeah. I'm really -- yeah. This is my journey. I started from zero. Yeah. Now I do
[00:05:45.920 --> 00:05:52.480]   deep learning. And as Amman mentioned, I only need one gold solo medal, which is the hardest thing.
[00:05:53.280 --> 00:06:02.240]   I will -- for the next, I don't know, one or two years, I will try my best to get it. It's
[00:06:02.240 --> 00:06:08.640]   definitely very challenging for me. But we'll see. Yeah. >> I'm 100% sure you're going to get it
[00:06:08.640 --> 00:06:14.480]   very soon. I think it's only a few weeks, if not days away. But fingers crossed and good luck for
[00:06:14.480 --> 00:06:20.880]   that. >> Thank you. >> So with that being said, as Dr. Habib mentioned, he and I have worked together
[00:06:20.880 --> 00:06:27.760]   before. And previously we were working on the Vision Transformer blog post. Something -- I'll
[00:06:27.760 --> 00:06:33.120]   share this in the chat once more. But something which I didn't know until this blog post is that
[00:06:33.120 --> 00:06:38.880]   Dr. Habib's really, really skilled at making wonderful visualizations and making complex
[00:06:38.880 --> 00:06:44.880]   ideas look very easy. And that's something I've been learning from him. In every meeting that I
[00:06:44.880 --> 00:06:50.000]   meet with him, there's something definitely there to learn. And these are visualizations that Dr.
[00:06:50.000 --> 00:06:55.360]   Habib did make for us in this previous blog post which ended up being really, really popular.
[00:06:55.360 --> 00:07:02.800]   And looking at the Vision Transformer that way was -- for me, it was very easy to look at these
[00:07:02.800 --> 00:07:08.720]   visualizations and then understand Vision Transformer from his perspective. And he and I
[00:07:08.720 --> 00:07:16.480]   have gotten together again. So I'll share the blog post in the URL once more. But this time we've
[00:07:16.480 --> 00:07:23.680]   gotten together again for the MLP Mixer paper. So as a supplement to this paper reading group,
[00:07:23.680 --> 00:07:30.400]   please feel free to have a look at this blog post. Again, there's lots of beautiful visualizations
[00:07:30.400 --> 00:07:38.240]   from Dr. Habib as we go down. As we keep going down. So that's the architecture from the paper.
[00:07:38.240 --> 00:07:43.520]   But again, you'll see lots of similarities in terms of visualizations. But we've again
[00:07:44.400 --> 00:07:49.920]   made Mixer layer really easy. And so you can have a look at every possible operation that goes
[00:07:49.920 --> 00:07:54.320]   inside the Mixer layer. So we're going to be using these visualizations today. And we're going to be
[00:07:54.320 --> 00:08:02.480]   reading through the paper to understand the MLP Mixer paper. But something I did want to touch
[00:08:02.480 --> 00:08:09.440]   just before we started was as part of the MLP Mixer paper, when the MLP Mixer paper came around,
[00:08:10.240 --> 00:08:17.680]   the MLP Mixer paper was introduced initially as a convolution-free network. So the researchers
[00:08:17.680 --> 00:08:22.000]   from Google, and it says so in the paper as well, that it's convolution-free. Do you need
[00:08:22.000 --> 00:08:26.880]   convolution? Do you need attention? Probably not. And that's when the MLP Mixer paper came in.
[00:08:26.880 --> 00:08:36.240]   But there's also a line or text inside the paper that did mention. So there's a special text in
[00:08:36.240 --> 00:08:45.920]   the paper that does say, in the extreme case, so this is here. I'm really sorry, one second.
[00:08:45.920 --> 00:09:00.800]   Sorry, my setup is a bit different today. So it does say in the extreme case,
[00:09:03.440 --> 00:09:08.960]   our architecture can be seen as a very special convolution neural network, which uses one-by-one
[00:09:08.960 --> 00:09:14.240]   convolutions for channel mixing and single-channel depth-wise convolutions of full receptive field
[00:09:14.240 --> 00:09:18.880]   and parameter sharing for token mixing. So we'll understand everything what these things are. But
[00:09:18.880 --> 00:09:25.440]   I guess the main point right now is to see that the paper mentions that through some operations,
[00:09:25.440 --> 00:09:30.960]   through some constraints, the MLP Mixer paper in itself would still be seen as a convolution
[00:09:30.960 --> 00:09:36.560]   neural network. What exactly those constraints are, Dr. Habib and I have spent hours and hours
[00:09:36.560 --> 00:09:42.720]   trying to demystify what exactly those constraints are. And something that happened along the line
[00:09:42.720 --> 00:09:48.560]   was then Jan LeCun, he went in and he said, well, it's not exactly conv-free in the first layer.
[00:09:48.560 --> 00:09:53.760]   Could be a per-patch, fully connected, could be a conv layer with 16 cross 16 kernels and 16 cross
[00:09:53.760 --> 00:10:01.120]   16 stride. And then the MLP Mixer layer in itself could be represented as a conv layer. To which
[00:10:01.120 --> 00:10:06.400]   then there was this really interesting competition that was hosted, which is Mixer is a CNN problem,
[00:10:06.400 --> 00:10:14.960]   which was an attempt to make the MLP Mixer be implemented just by using the convolution
[00:10:14.960 --> 00:10:21.840]   operations. So I guess before we got started with this paper, I just wanted to give some background
[00:10:21.840 --> 00:10:28.880]   on how the MLP Mixer paper has been really exciting. And yet at the same time, the deep
[00:10:28.880 --> 00:10:33.520]   learning community has been a little bit split on whether it is a convolutional network or it's not.
[00:10:33.520 --> 00:10:40.240]   We feel it's pretty much like a touchy subject that just comes down to semantics. And as part
[00:10:40.240 --> 00:10:45.280]   of this paper reading group, then we're going to dig dive into every bit of this paper with Dr.
[00:10:45.280 --> 00:10:49.120]   Habib. So Dr. Habib, is there anything else that you wanted to add over here?
[00:10:50.000 --> 00:10:58.560]   No, I think I don't have anything yet to add. But it's a nice background introduction to why
[00:10:58.560 --> 00:11:01.120]   there is so much for us about this paper.
[00:11:01.120 --> 00:11:06.320]   Yeah, totally. Thank you. So then let's get started with this paper.
[00:11:06.320 --> 00:11:14.720]   So something that we know-- I'm just reading the abstract right now. Something that we know so far
[00:11:14.720 --> 00:11:19.520]   is that convolutional neural networks have been the go-to model for computer vision. But recently,
[00:11:19.520 --> 00:11:24.560]   that's been changing. We just saw the Vision Transformer blog. So ever since the Vision
[00:11:24.560 --> 00:11:28.720]   Transformer came around, in our previous paper reading groups, we've looked at convolution--
[00:11:28.720 --> 00:11:33.520]   sorry, we've looked at transformer architectures. And we just discussed the Convit paper as well.
[00:11:33.520 --> 00:11:43.200]   And since then, a new type of architecture has emerged, which is the MLP Mixer. What this MLP
[00:11:43.200 --> 00:11:48.800]   Mixer is, it's not using self-attention. It's not using convolution. It's pretty much just using
[00:11:49.440 --> 00:11:55.680]   MLP layers, which are fully connected layers. And it's just using fully connected layers and a very
[00:11:55.680 --> 00:12:03.680]   novel way of using the transpose operation that makes things work. And by doing that, by using
[00:12:03.680 --> 00:12:08.160]   such a simple architecture, they're still able to get competitive performance with the state-of-the-art.
[00:12:08.160 --> 00:12:14.480]   So they're still able to-- with lots of pre-training and lots of infrastructure and compute requirements,
[00:12:14.480 --> 00:12:20.800]   of course, to get it to work. But they're able to still get it to work in terms of having a good,
[00:12:20.800 --> 00:12:26.160]   respectable accuracy and competitive to the state-of-the-art. So that's the background
[00:12:26.160 --> 00:12:33.600]   about what MLP Mixer is. Sorry, Sanyam. I'm sorry. Could you please zoom in a little?
[00:12:33.600 --> 00:12:38.640]   Of course. Is that better? Yes, thank you.
[00:12:39.680 --> 00:12:46.880]   Hold on. Let me try and make it a bit better. So I think that should be fine now.
[00:12:46.880 --> 00:12:50.960]   This is perfect. Thank you. Okay, excellent. No worries. Thanks for pointing that out.
[00:12:50.960 --> 00:12:56.560]   So then, as part of the-- this paper is really, really easy to understand. It's not like
[00:12:56.560 --> 00:13:03.920]   a lot of complexity in this. But it's about making things simple and yet getting them to work.
[00:13:04.480 --> 00:13:11.600]   That's what I was really excited to read about this paper. So if we go down into the architecture,
[00:13:11.600 --> 00:13:17.840]   I think, Dr. Habib, did you want to quickly talk about the architecture and just present the paper
[00:13:17.840 --> 00:13:24.880]   as a high-level overview? Yeah. I just want to also kind of unpack, rephrase what you said
[00:13:24.880 --> 00:13:32.160]   a little bit earlier. So just to have a bigger picture, so so far, before this MLP
[00:13:34.080 --> 00:13:42.400]   paper, we had two main ways to classify images. One was this convolutional-based, where we use
[00:13:42.400 --> 00:13:48.320]   convolutions. And this method, or the way convolutional neural networks were around
[00:13:48.320 --> 00:13:57.280]   maybe decades, and everybody was using-- and until last year, I'm correct, the vision transformer
[00:13:57.280 --> 00:14:04.720]   came, which kind of said, look, we don't need convolutions. You actually just need attention
[00:14:04.720 --> 00:14:12.720]   and positional embeddings and just linear layers. And this vision transformers was-- they kind of
[00:14:12.720 --> 00:14:18.320]   overtook convolutional neural networks. And they are currently even ranked top one in image
[00:14:18.320 --> 00:14:24.080]   net classification. So basically, we have two categories, convolutional networks and then
[00:14:24.080 --> 00:14:33.280]   transformers. And it was kind of interesting, something new came. And then within next six
[00:14:33.280 --> 00:14:41.680]   months comes this paper. And their main claim is following that you actually don't need convolutions.
[00:14:41.680 --> 00:14:49.280]   You don't need attentions. You don't need positional embeddings. You can get competitive
[00:14:49.280 --> 00:14:57.200]   performance. And competitive, I mean, they claim that their best MLP, simple MLP module has less
[00:14:57.200 --> 00:15:03.760]   than 0.3% accuracy than top vision transformer model, which is pretty impressive. So they come
[00:15:03.760 --> 00:15:10.640]   up with this very simple model, just MLPs. MLP stands for multilayer perceptron. So if you're
[00:15:10.640 --> 00:15:17.120]   a PyTorch user, you're just an n dot linear. And they say, yeah, that's pretty much everything
[00:15:17.120 --> 00:15:26.080]   what you need. And then obviously, Aman already introduced all the argument that MLP are kind of
[00:15:26.080 --> 00:15:31.920]   convolutional or not. So this is just a big picture from my point of view that it's kind of a third
[00:15:31.920 --> 00:15:42.320]   way to classify images. And so the way how it works, just a big over-- just a global overview.
[00:15:42.320 --> 00:15:48.560]   Again, we will go into details. There are some parts which are very similar to transformers. So
[00:15:48.560 --> 00:15:54.240]   here what you see that we have this image, we basically break down into the patches. Right,
[00:15:54.240 --> 00:16:03.440]   thank you, Aman. And these patches, right? And then we have this single shared linear layer.
[00:16:03.440 --> 00:16:11.520]   What we do, we flatten the patches. And then we project them. So we convert them to a single
[00:16:11.520 --> 00:16:18.320]   vector. You can call it embedding, patch embedding. Or you can call it latent representation.
[00:16:18.320 --> 00:16:23.040]   Doesn't matter whatever the terminology is. It's just a single-- for each patch,
[00:16:23.040 --> 00:16:33.760]   we get single vector. And then these vectors, a combination of vectors, goes to this mixer layer.
[00:16:33.760 --> 00:16:39.200]   And we'll go in detail what's inside the mixer layer. But to be honest, what's happening,
[00:16:39.200 --> 00:16:46.400]   there is two MLPs there which do convolutions either directly on the patches or on the channels.
[00:16:46.400 --> 00:16:54.720]   And you have multiple layers of those. And at the end, you take the mean. And then you have
[00:16:54.720 --> 00:17:02.960]   this classification ahead. So to be honest, it uses two building blocks, right? So it uses an
[00:17:02.960 --> 00:17:10.400]   end up linear. And then it uses Jilly, which is activation function. And then obviously,
[00:17:10.400 --> 00:17:15.920]   there is a dropout. I don't know if you consider it layer and skip connections. So it's very simple.
[00:17:15.920 --> 00:17:24.880]   Yeah, that's pretty much it. Yeah, that's really-- I think if we were to explain the paper,
[00:17:24.880 --> 00:17:30.800]   it would take five minutes because it's that easy. But I guess the interesting bits that come from
[00:17:30.800 --> 00:17:37.440]   this paper is that even after being so easy, it's still able to perform very well with those
[00:17:37.440 --> 00:17:44.080]   very complex architectures out there that use self-attention. And I think that's where
[00:17:44.080 --> 00:17:50.720]   this paper-- that's why this paper is one of the first papers to do so. And then there's
[00:17:50.720 --> 00:17:55.520]   other papers that have followed. But something I would like to highlight is as part of this, then
[00:17:56.160 --> 00:18:01.680]   Dr. Habib mentioned we start with an input image. So let's say you have an input image like this.
[00:18:01.680 --> 00:18:06.880]   What we do is we break it into patches. So you're pretty much converting that into small
[00:18:06.880 --> 00:18:14.960]   patches. And this thing has been explained very well in a previous one of our blogs in patch
[00:18:14.960 --> 00:18:19.120]   embedding. So I'm not going to go into the details of this. I'm just going to do a high-level
[00:18:19.120 --> 00:18:24.960]   overview. But if you are to have a look at exactly what's going on in patch embeddings and how this
[00:18:24.960 --> 00:18:32.480]   thing gets converted into-- how everything gets converted into a vector from a very detailed
[00:18:32.480 --> 00:18:37.920]   perspective, then I've shared that link again in that blog. But then coming back to-- from a high-
[00:18:37.920 --> 00:18:44.560]   level perspective, then, what happens is each one of these patches, like 1, 2, 3, 4, 5, 6,
[00:18:44.560 --> 00:18:49.360]   they're pretty much nine patches, right? Each one of these patches is represented by a vector. So
[00:18:49.360 --> 00:18:55.840]   there's two, and then that's the ninth one. So each long vector is a representation of that patch.
[00:18:55.840 --> 00:19:03.520]   So think of it that the first vector is a representation of this small patch at the
[00:19:03.520 --> 00:19:09.440]   top left. So that's the first vector. Then the second vector is this representation of this part
[00:19:09.440 --> 00:19:13.440]   of the image. So that's the second vector, and so on. The ninth vector then becomes the
[00:19:14.400 --> 00:19:20.960]   representation of this part of the image. So that's just basically then-- because a computer
[00:19:20.960 --> 00:19:26.240]   doesn't know-- it can't read an image directly. You have to convert it into a vector or a matrix.
[00:19:26.240 --> 00:19:33.600]   So you pretty much get a matrix that's consisting of nine patches, just in this particular example.
[00:19:33.600 --> 00:19:38.960]   I mean, then, as Dr. Habib said, it goes into the Mixer layer. And in the Mixer layer,
[00:19:38.960 --> 00:19:44.800]   there's some operations that we will look into. But pretty much an MLP everywhere that you see
[00:19:44.800 --> 00:19:50.480]   or you will see in part of this paper, an MLP, that's exactly as Dr. Habib already mentioned.
[00:19:50.480 --> 00:19:57.840]   It's a first convolution layer, two convolution layers, and then separated by a GLUE or non-linearity,
[00:19:57.840 --> 00:20:03.600]   basically. And fully connected then is pretty much in PyTorch, you could just say nn.linear.
[00:20:03.600 --> 00:20:09.600]   Or in TensorFlow, I believe you could say dense. So I'm not a TensorFlow user. I could be very
[00:20:09.600 --> 00:20:15.040]   well wrong here, but I do believe dense should be what is a fully connected layer in terms of
[00:20:15.040 --> 00:20:21.040]   PyTorch and TensorFlow. So that's the main overall idea. I guess now the next step is for us--
[00:20:21.040 --> 00:20:27.120]   from a high-level perspective, then that's exactly what's going on. But then the next step for us is
[00:20:27.120 --> 00:20:36.000]   to understand this Mixer layer on what exactly is going on in the Mixer layer. And let's start by
[00:20:36.000 --> 00:20:42.640]   doing that. Let's start by looking at things in a simpler way. So I believe, again, from the
[00:20:42.640 --> 00:20:52.720]   MLP Mixer blog, Dr. Habib, I'll take your visualizations because it makes things really
[00:20:52.720 --> 00:20:56.880]   easy to explain. So maybe figure two, is that something that's going to help you explain how
[00:20:56.880 --> 00:21:02.480]   everything gets converted into a patch embedding? Yes. Should I copy and paste in one node?
[00:21:02.480 --> 00:21:13.600]   Yeah, whatever you prefer. Yeah. OK. So we are right now-- so what we will present to you is
[00:21:13.600 --> 00:21:20.560]   one forward pass. So we'll take one image, and we will go one by one each step to explain exactly
[00:21:20.560 --> 00:21:25.680]   what's happening. And then you can just apply this operation to millions of images. It will
[00:21:25.680 --> 00:21:33.840]   be the same. So in this case, we have image of the frog. And so the step which we are describing
[00:21:33.840 --> 00:21:42.480]   is converting patch to some embedding vector. So this process is very similar to Vision Transformer.
[00:21:42.480 --> 00:21:49.200]   So this part is one-to-one. So the way how it works, we have image 224 by 224. And you know
[00:21:49.200 --> 00:21:57.040]   what we can do? We can divide this image into a grid or into the patches. So the patch which we
[00:21:57.040 --> 00:22:06.960]   will use, it will be 16 by 16. So if we do that, we will have 196 patches. So here, the first patch
[00:22:06.960 --> 00:22:12.880]   will be the top left corner, which is green. And then the last one will be this black one, which
[00:22:12.880 --> 00:22:21.760]   is 196 patch. It's located in the top right corner. And so if we'll now-- imagine we just break down
[00:22:21.760 --> 00:22:27.600]   image, and we have this 196 patches. And each of these patches will have dimension of 16 by 16.
[00:22:27.600 --> 00:22:36.080]   And we have three channels, right? Because images comes in red, green, blue. So we have three
[00:22:36.080 --> 00:22:43.840]   channels. So now what we're going to do, we will take this-- so we'll take this patch, and we'll
[00:22:43.840 --> 00:22:51.200]   completely flatten it. So 16 multiplied by 16 will be 256. And we have three channels.
[00:22:51.200 --> 00:22:59.120]   We have three channels. It will be 25-- 256-- 700--
[00:22:59.120 --> 00:22:59.840]   768.
[00:23:00.480 --> 00:23:08.000]   768, sorry, 768. So we basically take the patch. We flatten it to 768. So each pixels will be
[00:23:08.000 --> 00:23:20.400]   just a single vector. And then we do for all 196 patches. And then what we can do, we can--
[00:23:22.480 --> 00:23:31.200]   in PyTorch, we have an n-dot linear, which takes the 768. And we can project it to any dimension
[00:23:31.200 --> 00:23:40.560]   we want. So we can either do one-to-one measure-- either we can project to 768 or to 512.
[00:23:40.560 --> 00:23:41.600]   Yeah.
[00:23:41.600 --> 00:23:50.400]   OK? So this is it. So it's a one shared layer. And we go through all the patches. So what we will
[00:23:50.400 --> 00:23:58.720]   get, we will get 196 for each patch and 768 vector. So think about this. We will get a table,
[00:23:58.720 --> 00:24:07.680]   which 196, 768. So there is a terminology which I would like to say. So in the paper, they use
[00:24:07.680 --> 00:24:18.240]   channel mixings. Channels-- channel refers to the-- so for each patch, if we use the
[00:24:18.240 --> 00:24:24.720]   embedding dimension 512, so we will have 512 channels. If we use 768, we will have 768
[00:24:24.720 --> 00:24:33.840]   channels. So channels will refer to a column in a vector. And then I believe tokens-- Amman,
[00:24:33.840 --> 00:24:41.040]   correct me if I'm wrong-- refers to the number of patches. So it's 190. So basically, you can say
[00:24:41.040 --> 00:24:48.080]   for each image, we'll get token by channels or number of patches by embedding dimension.
[00:24:48.720 --> 00:24:49.200]   Yeah.
[00:24:49.200 --> 00:25:00.240]   Right. So this is the first step. And this is very similar to transformers. This is the only
[00:25:00.240 --> 00:25:03.200]   thing which is similar between transformers and MLPs.
[00:25:03.200 --> 00:25:07.040]   Yeah. It's so far, like until here, this is--
[00:25:07.040 --> 00:25:07.680]   Yes.
[00:25:07.680 --> 00:25:13.360]   All of this from back on is the same as vision transformers. I'm just going to type
[00:25:13.360 --> 00:25:18.960]   width. Yeah. So this is exactly the same. So so far, as Dr. Habib is saying, because you have--
[00:25:18.960 --> 00:25:24.000]   say you have an input of 2 to 4 by 2 to 4 image, and each patch, basically the height and width
[00:25:24.000 --> 00:25:31.760]   of this patch is 16 by 16. So you'll get basically 196 patches in total. And then each of these
[00:25:31.760 --> 00:25:36.160]   patches could be flattened or it could be projected using this linear projection layer.
[00:25:36.160 --> 00:25:42.160]   So in the end, every patch becomes a vector here like this. So you can have every patch
[00:25:42.160 --> 00:25:46.800]   represented as a vector. The dimensions of this vector could be depending on this
[00:25:46.800 --> 00:25:52.560]   linear projection layer. It could be 512. It could be 768. It could be whatever we want it to be.
[00:25:52.560 --> 00:25:57.680]   But the main point is that until this point, then you get like this. You could stack these
[00:25:57.680 --> 00:26:06.320]   all vectors together. So you get a 196 by 768 matrix. So now your input image has been converted
[00:26:06.320 --> 00:26:14.480]   into a 196 by 768 matrix. Now whatever happens after this point is what's different in the MLP
[00:26:14.480 --> 00:26:21.280]   mixer than the Vision Consumer. And that's what we're going to look at now. Right. So I hope this
[00:26:21.280 --> 00:26:27.600]   part is clear. I mean, we tried to explain very nicely. I think after this, we will come back to
[00:26:27.600 --> 00:26:35.120]   this and explain-- we are using right now nn.linear. But we will show you how you can replace the same
[00:26:35.120 --> 00:26:41.840]   operation by convolutions. But I think we will come back to this towards the end once we go how
[00:26:41.840 --> 00:26:47.520]   exactly it is done in the paper. So Amal, should we move to mixer layer after today?
[00:26:47.520 --> 00:26:56.880]   Yes. One second. So I was thinking for the mixer layer, I did have a way to explain the mixer
[00:26:56.880 --> 00:27:03.680]   layer. Let me-- it's OK if you want to go to it. Dr. Habib, you can definitely go through this.
[00:27:04.240 --> 00:27:08.000]   I'll just copy paste this. Do you want me to copy paste this or do you want to use your
[00:27:08.000 --> 00:27:15.120]   visualization? I think I will use mine. I think this will be easier for me because-- OK. So if
[00:27:15.120 --> 00:27:21.920]   we go down-- Is it this one? Yes. Oh, yes. It's the first one. Yeah. So this whole thing, if you
[00:27:21.920 --> 00:27:28.480]   can copy paste because this is the whole mixer layer. Yeah. OK. One second. So I'll just copy
[00:27:28.480 --> 00:27:40.320]   this bit here. Yeah. OK. One second. OK. Is that what you want? Oh, that's perfect. That's
[00:27:40.320 --> 00:27:51.120]   perfect. Right. So in my-- OK. So we have-- yes. Thank you, Amal. It's 512. So we use 512.
[00:27:51.120 --> 00:27:56.800]   Yeah. I think there was a mistake in the image. We will correct it later. So basically,
[00:27:57.360 --> 00:28:04.800]   we took this 768 patch. We took a patch, we flattened, and then we projected to the 512
[00:28:04.800 --> 00:28:10.480]   dimensions. And so this way, we will refer them to channels. So for each patch,
[00:28:10.480 --> 00:28:24.960]   have 512 channels and we have 196 tokens. OK. So here's what's very-- something very interesting
[00:28:24.960 --> 00:28:31.680]   starts. So here's the-- we enter to mixer layer. So mixer layer is something very simple. You
[00:28:31.680 --> 00:28:37.360]   always start with the layer norm. It's a type of normalization. I'm pretty sure you're familiar
[00:28:37.360 --> 00:28:44.320]   with batch norms. So layer norms is people prefer to use in kind of transformer-like architecture.
[00:28:44.320 --> 00:28:52.560]   It just normalizes all features between-- I forgot-- 1 and 0, or there is some arbitrary value.
[00:28:52.560 --> 00:29:02.160]   It doesn't matter. It's just a normalization layer. OK. So there is first-- so what we do-- so
[00:29:02.160 --> 00:29:11.120]   first, we will flip these features. We will have 512 by 196. OK. So first layer called
[00:29:11.120 --> 00:29:20.800]   token mixing layer. And token refers to-- I believe it's across the-- we mix across the patches,
[00:29:20.800 --> 00:29:25.680]   right? Yeah. Is it OK if I quickly make a visualization? Yeah. And I know exactly what
[00:29:25.680 --> 00:29:29.440]   you're trying to say. Yeah. Yeah. Please. I know exactly what you're trying to say. Yeah. Yes.
[00:29:29.440 --> 00:29:35.920]   OK. So basically, because our input looks something like this, right? You had 196 rows
[00:29:35.920 --> 00:29:42.400]   and 512 channels. But because you stacked all the-- you stacked all the vectors together,
[00:29:42.400 --> 00:29:49.120]   because we don't want to forget what exactly was happening. So each patch then is a vector
[00:29:49.120 --> 00:29:56.720]   like this, right? This is the first-- all of this, the first line, is just a 512-long vector.
[00:29:56.720 --> 00:30:03.600]   So what these 512 are, these are the channels. So that's basically the embedding dimension,
[00:30:03.600 --> 00:30:11.200]   if you want to call it. It basically means that you're representing a single token with 512
[00:30:11.200 --> 00:30:19.200]   channels. So that's what this-- but if you look at it, then every row is a token in this-- like,
[00:30:19.200 --> 00:30:26.560]   the input matrix of 196 by 512, which we are looking at this, in this input matrix of 196 by
[00:30:26.560 --> 00:30:34.560]   512, every row is a token, which is coming from this patch on the left. So every row then is
[00:30:34.560 --> 00:30:40.960]   basically this representation of the patch. So patch and token are used in a similar way. Token
[00:30:40.960 --> 00:30:46.000]   is just a word of saying that the patch has been converted to a vector. So every row then,
[00:30:46.000 --> 00:30:52.800]   in this case, is again like that token. And then every column is basically a single channel. So
[00:30:52.800 --> 00:30:58.800]   you have 512 columns, which means you have 512 channels. But when you take the transpose,
[00:30:58.800 --> 00:31:07.680]   then you have 196 on the columns, and you have 512 along the rows. So what that means is that
[00:31:07.680 --> 00:31:14.320]   every row pretty much now is a single channel. Every row is a single channel. So this is a
[00:31:14.320 --> 00:31:25.440]   channel. And then every column is a token. So a single token with all the 512 channels is basically
[00:31:25.440 --> 00:31:32.000]   this column. And then a single channel with all the 196 tokens is basically this row. Does that
[00:31:32.000 --> 00:31:36.720]   help, Dr. Habib? Is that exactly what you're trying to say? Yes. Yeah. Basically, we are
[00:31:36.720 --> 00:31:43.520]   looking for each position across the patches, across the tokens. So yes, that's beautifully
[00:31:43.520 --> 00:31:49.840]   explained. So now what we have, we have one linear layers, which takes-- there is something,
[00:31:49.840 --> 00:31:59.440]   but it takes 196 features. But just ignore for now the projection. But what it does, it takes
[00:31:59.440 --> 00:32:07.440]   196 features, and then it returns 196 features. So it's a single shared linear layer. What we do,
[00:32:07.440 --> 00:32:18.560]   we apply all across all the channels. We apply patch, and we look across the patches first.
[00:32:18.560 --> 00:32:25.360]   Think about this as a global view. Can I jump in?
[00:32:25.360 --> 00:32:27.200]   Yes. Can I quickly jump in? Feel free.
[00:32:27.200 --> 00:32:33.840]   Yeah. I guess the main point then to look in this token mixing layer is that the mixing
[00:32:33.840 --> 00:32:39.040]   is actually happening along the rows, because that's how the linear layers do the mixing.
[00:32:39.040 --> 00:32:44.080]   The mixing always happen across the rows. So they will take each row in one by one,
[00:32:44.080 --> 00:32:48.720]   and then the mixing is going to happen across the rows. But what does that mean when we say
[00:32:48.720 --> 00:32:53.440]   the mixing is happening across the rows? Which means we are mixing the columns, right? If you
[00:32:53.440 --> 00:33:00.160]   have a single vector like this, or a single token, basically, this row represents a token.
[00:33:00.160 --> 00:33:07.840]   And then you have 512 channels, right? Oh, sorry. I wrote that wrong. In this,
[00:33:07.840 --> 00:33:14.880]   the rows are the channels. So you have a row as a single channel, and the columns are the 196
[00:33:14.880 --> 00:33:20.640]   tokens. So when you're doing mixing along the rows, you're actually mixing these 196 tokens,
[00:33:20.640 --> 00:33:26.480]   right? Every single channel goes in as input, and then you mix the 512 channels.
[00:33:26.480 --> 00:33:34.240]   Sorry. Every single channel goes as an input, and then you mix the 196 tokens. That's why this layer
[00:33:34.240 --> 00:33:38.560]   has been called as token mixing, because when you're mixing the rows, you're essentially
[00:33:38.560 --> 00:33:42.000]   mixing the tokens. Does that help? Yeah.
[00:33:42.000 --> 00:33:48.880]   I know I missed the token and channel quite a few times, but I hope I was able to get the point
[00:33:48.880 --> 00:33:55.680]   across. Right. Yeah. So, you know, right. Maybe, Aman, you can just explain the next part,
[00:33:55.680 --> 00:34:02.400]   because you explained so nicely. Oh, okay. Sure. So then, in the next part,
[00:34:02.400 --> 00:34:09.920]   then, because we're mixing the rows in this bit, which means we're mixing the tokens, right? So
[00:34:09.920 --> 00:34:16.080]   now we're mixing spatial information in the image. So each token is being mixed with every other
[00:34:16.080 --> 00:34:21.600]   token, which means you're mixing the spatial information. But in a convolution, what we also
[00:34:21.600 --> 00:34:27.200]   mix is the information that we need to add is also we need to be able to mix the channels,
[00:34:27.200 --> 00:34:31.760]   because we also have cross channels. So all the previous papers, like squeeze and excitation,
[00:34:31.760 --> 00:34:37.840]   pretty much, or even in a convolution, when you say the number of in channels is X and out channels
[00:34:37.840 --> 00:34:44.240]   is Y, you're actually mixing across the channel dimension as well. So what we do is then we take
[00:34:44.240 --> 00:34:51.680]   this output from the token mixer, and we do a transpose again. So when we do a transpose this
[00:34:51.680 --> 00:35:01.360]   time, we get 196 by 512. What does that mean? At this point now, every row becomes a token,
[00:35:01.360 --> 00:35:09.120]   and every column becomes a channel, just as when we started, right? So every row becomes a token,
[00:35:09.120 --> 00:35:16.720]   and every column becomes a channel. So then again, we pass this through the same sort of
[00:35:16.720 --> 00:35:24.080]   MLP is what it is called. We pass it through another MLP. But actually, as I've previously
[00:35:24.080 --> 00:35:28.080]   mentioned, and a fully connected will take in row by row, like one row in,
[00:35:28.080 --> 00:35:34.080]   row by row. And what it is going to do is it's going to do the mixing across the rows. So the
[00:35:34.080 --> 00:35:41.520]   mixing is actually happening across the rows. And what that means that you're actually mixing
[00:35:41.520 --> 00:35:47.440]   the channels because every column is a channel. So when you're mixing that columns, you're actually
[00:35:47.440 --> 00:35:53.120]   mixing the channels. So that's why this particular layer is called channel mixing. So the first layer
[00:35:53.120 --> 00:35:57.440]   is called token mixer because you're mixing across the rows that are tokens. And then the second one,
[00:35:57.440 --> 00:36:02.800]   you're mixing the channels. That's the key difference in the two. Yeah. So, you know,
[00:36:03.600 --> 00:36:10.000]   just to get a global picture, you have patches. You're doing global and local mixing. This is,
[00:36:10.000 --> 00:36:16.160]   this two operation basically perform the same, you know, kind of performing those things. And
[00:36:16.160 --> 00:36:21.200]   that's it. So yeah, this is the architecture. And then, you know, you get the output,
[00:36:21.200 --> 00:36:25.440]   which is exactly 196, 512. And then depending on the configuration,
[00:36:29.520 --> 00:36:34.800]   depending on the configuration, we will, you know, you can have 10 or 12 layers, I forgot.
[00:36:34.800 --> 00:36:40.160]   And that's pretty much it. And then at the end, you just take a mean, and then you have this
[00:36:40.160 --> 00:36:46.400]   output layer, which predicts number of the classes. And that's it. It's really,
[00:36:46.400 --> 00:36:55.600]   it's really as simple as it is. It's not complicated. Yeah. Yeah. So, Man, do you
[00:36:55.600 --> 00:37:02.160]   want to add something to this? Yeah, I guess. I guess I was also baffled when I was, I'm used to
[00:37:02.160 --> 00:37:08.480]   like spending more time trying to understand the paper. And it takes me, because I'm very slow,
[00:37:08.480 --> 00:37:12.240]   it takes me longer to understand the paper. But when I was reading this paper, it took me like
[00:37:12.240 --> 00:37:17.040]   five, 10 minutes to understand what was going on in the paper to understand the architecture.
[00:37:17.040 --> 00:37:20.640]   And I was like, okay, I must be missing something. There's this something definitely,
[00:37:21.280 --> 00:37:26.400]   there's some detail I'm missing or something like that. But this is really it. This is
[00:37:26.400 --> 00:37:36.000]   really MLP. Yeah. And you just do a transpose. Yeah. So now if we go a little bit down and,
[00:37:36.000 --> 00:37:41.600]   so this simple thing can compare to transformers. So if we go down now to the paper,
[00:37:41.600 --> 00:37:46.160]   let's go to the table. Yeah. Did you mean the different architectures?
[00:37:47.680 --> 00:37:53.840]   So yeah. So this is basically, yeah. So this is basically a standard, your standard archive paper,
[00:37:53.840 --> 00:37:58.560]   which will tell you, look, we tested a different version of this architecture, different,
[00:37:58.560 --> 00:38:06.720]   different pack sizes, embedding dimensions, and so on. And they all kind of work nicely. There's
[00:38:06.720 --> 00:38:13.920]   nothing interesting in this table. It just shows you what they tested. So if we go down now a
[00:38:13.920 --> 00:38:20.640]   little bit, there is this, okay. Is this one? Right. So this one, yeah, basically,
[00:38:20.640 --> 00:38:32.320]   this, it just shows comparison. So for example, so they tried to do different pre-training tasks,
[00:38:32.320 --> 00:38:39.280]   and you can see, for example, if, so since it's Google, they have this GFC 300 million,
[00:38:39.280 --> 00:38:43.120]   it's a secret data set, which only Google had access. So what they do recently, they just
[00:38:43.120 --> 00:38:51.600]   pre-train on this, and then they train on ImageNet. And you can see, so the mixers has,
[00:38:51.600 --> 00:39:04.160]   performs 87.94 on ImageNet. And then the vision transformer performs 88.55. It's really close,
[00:39:04.160 --> 00:39:10.000]   right? The vision transformers is very heavy architecture. It's uses attention and blah,
[00:39:10.000 --> 00:39:17.600]   and it's complicated. But you can see that they get really very, very close performance,
[00:39:17.600 --> 00:39:24.640]   which is very nice, right? So if we go to, Aman, do you want to add something here?
[00:39:24.640 --> 00:39:29.120]   No, I think you've got this covered. Keep going.
[00:39:29.120 --> 00:39:35.520]   Yeah. Okay. So one thing which I want to point out that, so they claim there's, if we look,
[00:39:35.520 --> 00:39:41.680]   right? So if you are, there is one advantage of this MLP architecture. So imagine you are a company,
[00:39:41.680 --> 00:39:47.520]   right? And you want to deploy something. And you probably, I don't know whether you will care about
[00:39:47.520 --> 00:39:54.080]   accuracy or not, because the accuracy is so close. One thing about this MLP mixture is that it has
[00:39:54.080 --> 00:40:00.720]   extremely high throughput. So it can predict images. You can have, in some cases, you predict
[00:40:00.720 --> 00:40:09.520]   120 images through TPU cores. And for example, for vision transformers. So if we look on the top,
[00:40:09.520 --> 00:40:16.720]   you can see that the mixer can predict one of five. Oh yeah. So in this case, which is a very
[00:40:16.720 --> 00:40:24.480]   heavy architectures, MLP mixtures are much more lighter and they can predict a lot of images,
[00:40:24.480 --> 00:40:28.080]   high throughput images. So if you're a company- Almost three times, still three times the vision.
[00:40:28.800 --> 00:40:35.280]   Right. So if you are interested in deploying the models, you might be thinking like, maybe you
[00:40:35.280 --> 00:40:44.160]   don't need vision transformers. You can just probably go just use MLP mixtures. And you have,
[00:40:44.160 --> 00:40:50.480]   depending on what's your objective. So this is one of the advantage. So if we'll go to the next slide.
[00:40:50.480 --> 00:40:56.800]   Yeah. Just the one thing I want to add here is the training times. So I know it says like TPU
[00:40:56.800 --> 00:41:04.160]   v3 core days, but one thing when like this architecture, when it's pre-trained on ImageNet
[00:41:04.160 --> 00:41:09.760]   21K, it's pre-trained on GFD 300 million. That just means that when you're using a simple
[00:41:09.760 --> 00:41:14.480]   architecture, you pre-train it on lots and lots of data. So you're actually pre-training on 300
[00:41:14.480 --> 00:41:20.400]   million images. It's not like you take the simple architecture and then you take EfficientNet and
[00:41:20.400 --> 00:41:25.600]   you train both of them on ImageNet, which is 1 million images. And then you think that you'll
[00:41:25.600 --> 00:41:29.440]   still get comparative performance. That's not going to happen because when you have a simpler
[00:41:29.440 --> 00:41:35.680]   architecture, you pretty much have to give it a lot more data for it to learn things as compared
[00:41:35.680 --> 00:41:39.920]   to an EfficientNet that could have a very competitive performance just from the 1 million
[00:41:39.920 --> 00:41:45.840]   images. So in terms of like costs, yes, it's going to cost a lot to do the first training.
[00:41:45.840 --> 00:41:50.640]   But if you're definitely, as Dr. Habib says, if you're definitely a company that cares about the
[00:41:50.640 --> 00:41:54.560]   throughput, because it's a simple architecture, which means it can do things faster.
[00:41:55.440 --> 00:41:59.360]   Yeah, that's the only thing I want to add is just the size of the pre-training datasets.
[00:41:59.360 --> 00:42:05.680]   Right. So then if we go down, there was one more. Oh, so yeah, this graph. So I found,
[00:42:05.680 --> 00:42:11.680]   so for example, they also make really nice points. So if we, so yeah, so this, you see this on the
[00:42:11.680 --> 00:42:16.240]   right graph. So this is very interesting. So there's something called scaling. So they did
[00:42:16.240 --> 00:42:23.440]   very simple experiments. So they use vision transformers and mixers. And so what they did,
[00:42:23.440 --> 00:42:29.120]   they, instead of training on 300 million images, they first train on 10 million,
[00:42:29.120 --> 00:42:37.920]   and then they measure accuracy on ImageNet. And so, yes, so you can see like, so this is on 10
[00:42:37.920 --> 00:42:44.080]   million. And if we go to the 30 million, the vision transformers, you can see it's kind of
[00:42:44.080 --> 00:42:49.840]   plateaus a little bit. So there is no difference between 30 millions and 100 million. It's kind of,
[00:42:51.600 --> 00:42:58.560]   you will get the same accuracy. But in the case of mixers, what's interesting, the more data it has,
[00:42:58.560 --> 00:43:06.560]   it kind of goes kind of linearly, the better it's performance. So it requires a lot of data to
[00:43:06.560 --> 00:43:12.480]   train. So this was also one of the points which author making that perhaps if you have a bigger,
[00:43:12.480 --> 00:43:21.200]   more data, so it scales by the data. So if you have provide more data, maybe we will see better
[00:43:21.200 --> 00:43:26.960]   performance than vision transformer or so on. And you can see for vision transformers, it's kind of
[00:43:26.960 --> 00:43:37.200]   after 10 million, it's kind of flat. So just to sum up, it's lighter architecture, it has high
[00:43:37.200 --> 00:43:44.240]   throughput and it scales with the pre-trained data. I think which is very interesting and nice property.
[00:43:46.240 --> 00:43:54.400]   Yeah, definitely. Okay. So let's go to the next, not this one, but the shuffling thing.
[00:43:54.400 --> 00:44:01.040]   Yeah. So this, okay. Yeah. I'm trying to find where is the shuffling. This is,
[00:44:01.040 --> 00:44:06.080]   that was, so this is just the table that's just comparing the results, right?
[00:44:06.080 --> 00:44:10.080]   Yeah. Different. It's just a bigger table which compares different architecture.
[00:44:10.080 --> 00:44:15.360]   This is just the showing like the weights and visualizing the, like, cause it's just linear
[00:44:15.360 --> 00:44:20.240]   layers. I think the point here is like, you can make it, it's easier to look at all the weights
[00:44:20.240 --> 00:44:23.680]   and you can understand the architecture better because it's so simple and simple.
[00:44:23.680 --> 00:44:31.200]   Right. So here they want to make arguments, the weights, you can draw them and there are some,
[00:44:31.200 --> 00:44:40.480]   if you see, there is this thing that in CNNs, early layers are responsible for recognizing
[00:44:41.520 --> 00:44:47.440]   small features. And then as you go, there's some global features. So they here, they are
[00:44:47.440 --> 00:44:53.360]   also showing that this, they were curious whether it worked in the same way as CNN.
[00:44:53.360 --> 00:45:02.560]   And there are some evidence, you can see there are these filters in this linear layers, and they have
[00:45:02.560 --> 00:45:09.760]   like, so red means negative number, blue means positive numbers. And in the early layers,
[00:45:09.760 --> 00:45:19.280]   it just, they're focusing on something, but as you go to the, a little bit later layers,
[00:45:19.280 --> 00:45:24.880]   you can see there is some kind of complex patterns emerging, which they try to recognize.
[00:45:24.880 --> 00:45:30.000]   So that's pretty much it. Yeah. In this.
[00:45:30.000 --> 00:45:37.120]   I just want to add context. Yeah. I just want to add context. Where Dr. Habib mentioned that,
[00:45:38.480 --> 00:45:44.720]   like, it's similar to CNNs. There's this paper visualizing CNN layers by, I guess that's the
[00:45:44.720 --> 00:45:49.840]   paper, Zyler and Ferguson, I believe that's the one. So I just want to quickly find that paper.
[00:45:49.840 --> 00:45:54.480]   It's this one, visualizing and understanding convolution neural networks. In this paper,
[00:45:54.480 --> 00:46:00.480]   you will see basically that the patterns are very similar to MLP mixer. So I just want to add
[00:46:00.480 --> 00:46:06.480]   context, like that's the paper for anybody watching this. Okay. Yeah. And there's also
[00:46:06.480 --> 00:46:12.960]   very nice distil articles about different visualizing filters of neural networks,
[00:46:12.960 --> 00:46:20.320]   just in case somebody wants to read. Okay. So you find the mixing, where did that go? Hold on.
[00:46:20.320 --> 00:46:28.240]   Is it after the role of model scale, the role of pre-trained that's related work,
[00:46:28.960 --> 00:46:35.920]   conclusion. Is it in the experiments at the bottom? Maybe it wasn't. Let me just Google
[00:46:35.920 --> 00:46:47.040]   quickly search for it here. What was it? It was called? It was shuffle. Yes. Oh, here it is.
[00:46:47.040 --> 00:46:56.720]   Okay. So that's something very interesting, but it's not surprising. So this is ablation studies,
[00:46:56.720 --> 00:47:03.760]   very interesting study. So they did three experiments. We took original images.
[00:47:03.760 --> 00:47:15.200]   Actually, somebody on the chat already asked questions about what will happen if we change
[00:47:15.200 --> 00:47:21.280]   the order of pixels or the patches. So this is the exact experiment what they did. So we have
[00:47:21.280 --> 00:47:28.240]   three sets of experiment. We take original image and then feed it, train on original image. And
[00:47:28.240 --> 00:47:34.640]   then what we do is that we take the patch, we shuffle the patches, and then we shuffle pixels
[00:47:34.640 --> 00:47:41.200]   within the patches. And then the third experiment is that we just take image and just randomly
[00:47:41.200 --> 00:47:48.560]   shuffle everything. Right? So on the left, you will see the result of the, for the mixers,
[00:47:48.560 --> 00:47:55.360]   because it's used NMValc linear, so original. And when you shuffle the pixel, the shuffle,
[00:47:55.360 --> 00:48:02.880]   the patches and the pixel inside, it's actually, it doesn't care because it's just linear layers.
[00:48:02.880 --> 00:48:07.680]   Right? They don't care about the position because everything is connected to everything.
[00:48:07.680 --> 00:48:10.880]   And you will find the same- Oh, sorry.
[00:48:12.720 --> 00:48:18.240]   You will find the thing. But when you, and you know, the performance is the same, right? But
[00:48:18.240 --> 00:48:29.040]   in the case of ResNet, if you take the classical ResNet, so in original, they are fine. But if you
[00:48:29.040 --> 00:48:36.080]   shuffle the patches and then you shuffle the pixel within the patches. So again, this is using
[00:48:36.080 --> 00:48:40.640]   ResNet. It doesn't care about patches, but just to be consistent. And you can see the performance
[00:48:40.640 --> 00:48:47.600]   immediately drops because ResNet cares about the order of the pixels and where they are located.
[00:48:47.600 --> 00:48:54.480]   I think it's called inductive bias if I'm correct. And then for example, and then the bot shuffle
[00:48:54.480 --> 00:49:01.120]   from global shuffle, global, suffer from global, global shuffling, but it seems like MLP does
[00:49:01.120 --> 00:49:07.600]   better. I don't know the scale. Yes. MLP does better. Right? Because it's more than 20, right?
[00:49:07.600 --> 00:49:14.800]   In some cases. And this, in the case of ResNet, it's never, it's never goes below.
[00:49:14.800 --> 00:49:21.520]   So does that, then does, is it, then Dr. Habib, is it easy to say that it can handle
[00:49:21.520 --> 00:49:28.480]   corruptions better, the MLP mixer? Or is it like it can handle noise better or is that a, is that a
[00:49:28.480 --> 00:49:35.360]   tough to make inference? I think, I think, I think the point where they want to say that this,
[00:49:36.880 --> 00:49:43.360]   that MLP probably doesn't care about order so much. However, this ResNet or vision to
[00:49:43.360 --> 00:49:48.080]   transformers, they suffer from inductive bias. They care about order. So maybe it's much more
[00:49:48.080 --> 00:49:53.440]   robust, but I'm, I'm very afraid to make this kind of statement. So I will, I will just say
[00:49:53.440 --> 00:49:59.120]   what I see from this experiment. Absolutely. Keeping it safe. Yes. Let me have a look at
[00:49:59.120 --> 00:50:02.960]   the questions of this. Any, oh, sorry. Yeah. Keep going. So I want to do one,
[00:50:06.400 --> 00:50:11.840]   what's a Y, okay. Yeah. So let's, let's go to figure five. Can we go to figure five on,
[00:50:11.840 --> 00:50:22.560]   on this? No, on the supplementary after an experiment. So, so I, I found something very
[00:50:22.560 --> 00:50:29.040]   interesting when I was reading through this. Yeah. So here, so this is fine, right? So yeah.
[00:50:29.040 --> 00:50:32.880]   Sorry, which one? I've missed it. Yeah. Was it this one?
[00:50:33.920 --> 00:50:36.480]   It was yes, this one. If you can. This one?
[00:50:36.480 --> 00:50:43.600]   Yes. So, you know, so this is very interesting experiment. So you, you remember we explained
[00:50:43.600 --> 00:50:54.240]   about the filters, right? So can you just zoom on block zero? Yeah. So I don't know. I don't know
[00:50:54.240 --> 00:50:59.120]   whether after discuss this a lot or not, but I found it interesting. So, you know, you remember
[00:50:59.120 --> 00:51:04.960]   we talked about filters, right? There are filters, which responsible for recognizing some kind of
[00:51:04.960 --> 00:51:10.080]   features, right? So they did very interesting experiments. First, they did, they're just
[00:51:10.080 --> 00:51:17.760]   looking at one block, one layer. And so this is the result. If you just train on ImageNet,
[00:51:17.760 --> 00:51:24.240]   okay. And then you plot the filter. So this are filters are exactly the same location
[00:51:24.240 --> 00:51:29.280]   in each of these experiments. So ImageNet, and then if you're trained on ImageNet 20K,
[00:51:29.280 --> 00:51:33.520]   and then on the right side, it will be, if you move a little bit on the right.
[00:51:33.520 --> 00:51:36.720]   Aman? Yeah.
[00:51:36.720 --> 00:51:45.680]   Yeah. So there is this GFT 300 dataset. And you know, what's interesting that depending on the
[00:51:45.680 --> 00:51:51.040]   dataset or like on how much data you have, this filters look completely different. You know,
[00:51:53.040 --> 00:51:59.360]   there is no correlation at all, but, you know, for convolution neural networks, you will see
[00:51:59.360 --> 00:52:03.920]   that early, like, you know, this filters in, at least in the earlier layers, they look kind of
[00:52:03.920 --> 00:52:09.680]   similar, you know, between different architectures. So I, so for example, I personally don't know
[00:52:09.680 --> 00:52:17.520]   whether, you know, yeah, I, it's hard. I don't know what to think about this. Maybe there is,
[00:52:17.520 --> 00:52:23.200]   you know, it seems like this filter depends on the date, on the amount of datasets you train,
[00:52:23.200 --> 00:52:28.080]   because, you know, they are completely different. So this is, this is, I don't know if somebody has
[00:52:28.080 --> 00:52:34.160]   some ideas or explanation for this. I found this kind of interesting.
[00:52:34.160 --> 00:52:39.680]   No, I think that, that really is a good point that, you know, if like with CNNs, at least then
[00:52:39.680 --> 00:52:45.680]   they like the earlier layers, they're finding top edges or they're finding top left corners or like
[00:52:45.680 --> 00:52:50.400]   lines or diagonals or whatever. They're just trying to find abstract edges and stuff. But
[00:52:50.400 --> 00:52:55.280]   in terms of the MLP mixer, then it's changing the, there's no such thing that the earlier layers are
[00:52:55.280 --> 00:53:01.280]   always, always finding the same pattern depending on the dataset size. That really is an interesting,
[00:53:01.280 --> 00:53:06.480]   interesting point that you make there, Dr. Habib. And yeah, it's, it's hard to make of it.
[00:53:06.480 --> 00:53:12.000]   I mean, one thing then, does it mean if we apply it on medical imaging, then it will adapt better?
[00:53:12.000 --> 00:53:17.520]   I don't know. These are like questions. These are open research questions that should be tried
[00:53:17.520 --> 00:53:25.040]   and tested before making any claims. So conscious of time as well, I'll go to all the questions that
[00:53:25.040 --> 00:53:30.560]   have been asked so far. One second. Sorry. Yes. Let's go to the question.
[00:53:30.560 --> 00:53:36.560]   So Amman, maybe you just want to add one word about this controversy between
[00:53:36.560 --> 00:53:42.640]   Conf2D and OneCog. Yeah, I will, I will add it. I've actually got the whole blog post.
[00:53:42.640 --> 00:53:49.360]   I released two more this morning, so I'll share that very quickly in terms of the controversy.
[00:53:49.360 --> 00:53:55.840]   But I just want to also get the questions sorted. Right. Perfect. Let's go. Okay. Have you tried
[00:53:55.840 --> 00:54:02.080]   using this on any present past forms? There have been some kernels that have, I know of them, but
[00:54:02.880 --> 00:54:08.880]   I don't know about Dr. Habib if he has. Oh, I haven't tried. I'm not brave enough yet to tell
[00:54:08.880 --> 00:54:16.800]   them. Hi, Dr. Habib. I mean, why should one chain 768 vector to 512? That's a good question,
[00:54:16.800 --> 00:54:21.680]   but it's basically, again, going back to the table in the paper. So if we go back to the paper
[00:54:21.680 --> 00:54:26.880]   and we go back to the, basically it's just different architectures. If you want a bigger
[00:54:26.880 --> 00:54:33.760]   architecture, it's a hyper parameter. Yeah. Think about this as a hyper parameter. And I don't know
[00:54:33.760 --> 00:54:38.000]   whether they say, ah, yeah, they have, right. We can see. They've got different hidden size.
[00:54:38.000 --> 00:54:43.520]   They've got different hidden size. So like they've got 512, they've got 768 and they've got 1024. So
[00:54:43.520 --> 00:54:48.960]   in the bigger architectures and even 1280. So it's just, you want to change the hyper parameter
[00:54:48.960 --> 00:54:53.200]   depending on the architecture. So the bigger the architecture, the bigger the hidden size.
[00:54:54.000 --> 00:55:02.640]   Yeah. Okay. Let's go. The 16 cross 16 patch is flattened and then it is projected using
[00:55:02.640 --> 00:55:07.360]   linear layer to any dimension of our choice. Yes, that is correct. That is absolutely correct. You've
[00:55:07.360 --> 00:55:12.480]   got this. And then the dimension of choices has been mentioned as in this paper that we just,
[00:55:12.480 --> 00:55:21.200]   we just looked into, but you've got this. Why do we transpose 196 by 512 to 512 by 196? So 196
[00:55:21.200 --> 00:55:27.360]   by 512. So the linear layers actually only work along the rows. And you want to mix the channels
[00:55:27.360 --> 00:55:32.080]   and you want to mix the spatial information. I'm sorry. I guess when this is on YouTube,
[00:55:32.080 --> 00:55:38.480]   have a go back at when this was being explained in detail with everything. But the short answer is
[00:55:38.480 --> 00:55:45.200]   because you're always mixing the rows. In the first one, you're mixing the channels because
[00:55:45.200 --> 00:55:51.200]   every column is a channel. And in the second one, you're mixing the tokens. Have I got this right?
[00:55:51.200 --> 00:55:55.920]   Yes, I've got this right. In the second one, you're mixing the tokens because every column
[00:55:55.920 --> 00:56:02.400]   is a token. So that's the only reason why you want to do a transpose in the initial one. Because in
[00:56:02.400 --> 00:56:06.400]   the token mixing, you want every column to be a token. Is there anything you want to add here,
[00:56:06.400 --> 00:56:12.080]   Dr. Habib? No, I think this is very nicely explained. What does token mixing achieve?
[00:56:12.080 --> 00:56:17.040]   Okay. Is this you want to answer? Because I've got a really good visualization or
[00:56:17.040 --> 00:56:20.240]   easier way to explain what it is. Yes, please. You do.
[00:56:20.240 --> 00:56:30.560]   Okay. I'm going to use your image. Whatever you want. If I can find it. Okay, here it is. Yeah.
[00:56:30.560 --> 00:56:37.280]   So basically, look at this, right? We started with the image of a frog. Every patch,
[00:56:38.240 --> 00:56:44.400]   we don't need the highlighter. Every patch is like this. And the last batch is here. And then
[00:56:44.400 --> 00:56:50.560]   every patch is represented as a vector, right? You've got every patch as a vector.
[00:56:50.560 --> 00:56:58.320]   When you're doing the token mixing, basically, and what you're doing is you're mixing the first
[00:56:58.320 --> 00:57:04.880]   channel or just one channel of this token with all the other tokens. So that's the token mixing. So
[00:57:04.880 --> 00:57:10.160]   basically what you're doing is you're mixing the spatial information because it's important, right?
[00:57:10.160 --> 00:57:14.800]   We need to know where the eyes are. The eyes are here in the image, but it does it channel by
[00:57:14.800 --> 00:57:20.800]   channel. And the number of channels could be 512, could be 768, but that's what the token mixing
[00:57:20.800 --> 00:57:28.880]   does. What the token mixing is doing for every patch, channel by channel for 512 or 768 channels,
[00:57:28.880 --> 00:57:34.080]   it's mixing the information in each of the patches. So you know, okay, eyes are here,
[00:57:34.080 --> 00:57:41.200]   other information is here. So you can mix actually the spatial information as said.
[00:57:41.200 --> 00:57:47.520]   And then the next one in the channel mixing, you pretty much take just the one patch and you mix
[00:57:47.520 --> 00:57:55.360]   all the 512 or 768 channels in that one patch. So that's the intuition of token mixing and channel
[00:57:55.360 --> 00:57:59.680]   mixing. But is there anything you want to add, Dr. Habib? No, I think we should have explained
[00:57:59.680 --> 00:58:06.240]   in this using this broad picture. This was very nice. Okay. It just struck, it just occurred to
[00:58:06.240 --> 00:58:13.280]   me to do it that way. Is there a comparison of Flops for Mixer versus other architectures?
[00:58:13.280 --> 00:58:18.000]   One thing I do want to say is Flops is not a good way to compare because Flops doesn't mean that
[00:58:18.000 --> 00:58:23.840]   the images, like the architecture is going to be faster. There's like different research papers or
[00:58:23.840 --> 00:58:29.920]   different discussions on Twitter that I was reading recently. But in terms of the throughput,
[00:58:29.920 --> 00:58:35.840]   when we were looking at the MLP mix architecture, then there was a table that was showing throughput.
[00:58:35.840 --> 00:58:41.200]   So I think that's the comparison. It's not this table, but it's this table. You see how it's
[00:58:41.200 --> 00:58:44.560]   comparing the throughput. Right.
[00:58:44.560 --> 00:58:51.200]   I am still unclear on the token and channel mixer. Can we review that one more time?
[00:58:51.200 --> 00:58:56.160]   I hope I just explained it. Does that help, Ramesh? Could you maybe reply to this comment
[00:58:56.160 --> 00:59:01.280]   here and let me know if what I just said in terms of an intuition, does that help?
[00:59:01.280 --> 00:59:05.920]   Yeah, that sounds good. So one question I had follow up. So do you do this token mixer,
[00:59:05.920 --> 00:59:11.360]   channel mixer parallelly and then do some sort of concatenation? Is it like sequentially? You
[00:59:11.360 --> 00:59:16.880]   do token mixer first and then token mixer output and then the channel mixer?
[00:59:16.880 --> 00:59:20.400]   Yeah, it's sequential. You do the token mixer first and then the channel mixer.
[00:59:20.720 --> 00:59:26.240]   Yeah. But one experiment you could try is do the channel mixer first and then the token mixer,
[00:59:26.240 --> 00:59:33.360]   but I don't think it will change anything because the idea is to mix both the dimensions.
[00:59:33.360 --> 00:59:40.960]   Like a 2D kernel, right? OK, I guess there's a deeper discussion to be had on what exactly goes
[00:59:40.960 --> 00:59:46.560]   on in a convolution, but wait for it. I think we might go-- Dr. Habib, conscious of your time,
[00:59:46.560 --> 00:59:50.240]   it's almost midnight there. Are you still OK to keep going for another--
[00:59:50.240 --> 00:59:54.400]   Yeah, yeah, yeah. Yeah, yeah, yeah. Sure. We can just go through all the questions and
[00:59:54.400 --> 00:59:56.800]   remain whatever is remaining, we can go. I'm fine.
[00:59:56.800 --> 01:00:02.880]   Yeah, I just still want to touch upon what exactly a convolution does and why do we need this token
[01:00:02.880 --> 01:00:08.080]   mixing and channel mixing and that whole controversy which we haven't touched on yet.
[01:00:08.080 --> 01:00:15.280]   But I'll go into that. OK. Has there been further work on using MLP mixer as backbone for object
[01:00:15.280 --> 01:00:22.640]   detection or segmentation tasks? I'm not aware of it if it has been. I think the MLP mixer,
[01:00:22.640 --> 01:00:29.120]   does it say fine tuning on other-- does the paper fine tune on-- I think the paper in itself fine
[01:00:29.120 --> 01:00:34.160]   tunes, right? I'm not sure. The answer is we're not sure. The paper is very recent, it's only a
[01:00:34.160 --> 01:00:35.840]   month or two ago. Yes.
[01:00:35.840 --> 01:00:43.120]   And there's been other works, though. There's been more mixed architectures like GMLP,
[01:00:43.120 --> 01:00:47.760]   ResMLP. So you could have a look at those as well. There's been definitely follow-up work.
[01:00:47.760 --> 01:00:53.760]   If we change the order-- exactly, you read my mind. If we change the order of token mixing
[01:00:53.760 --> 01:01:02.160]   and channel mixing, I expect it to be. I won't put the money-- I won't put $1,000 in it, but
[01:01:02.160 --> 01:01:05.760]   of course it should be. That's my expectation. What do you think, Dr. Abir?
[01:01:05.760 --> 01:01:08.720]   I think that it will be similar. Yeah.
[01:01:08.720 --> 01:01:11.040]   But you never know. You never know.
[01:01:11.040 --> 01:01:14.320]   Yeah, you never know with deep learning. One thing you will see is deep learning
[01:01:14.320 --> 01:01:21.760]   practitioners don't know what's happening until it happens. Any ideas on how to do transfer
[01:01:21.760 --> 01:01:26.800]   learning to adapt to other data sets? Yeah, you could fine tune. This is still
[01:01:26.800 --> 01:01:30.240]   same architecture. We could still go and fine tune.
[01:01:30.240 --> 01:01:36.240]   You just have to-- right. So you'd take lost MLP layer, which for ImageNet is 1,000 classes,
[01:01:36.240 --> 01:01:44.240]   and you just freeze. I don't want to say freeze. Yes, I think you just freeze depending-- I don't
[01:01:44.240 --> 01:01:49.920]   know how it-- I haven't done experiments, but usually on CNN, you can freeze the backbone
[01:01:49.920 --> 01:01:56.240]   and then just fine tune the-- add new NN linear depending if you have five classes and then
[01:01:56.240 --> 01:02:02.240]   fine tune the last layer. And hopefully it will work. But we can see on the paper how they fine
[01:02:02.240 --> 01:02:06.560]   tune because they're trained on GFC and then how they fine tune. And you can just repeat the same
[01:02:06.560 --> 01:02:09.920]   thing with your data set. OK.
[01:02:09.920 --> 01:02:13.520]   We have one more question. I'm curious to know if you agree with me on the following.
[01:02:13.520 --> 01:02:19.520]   One benefit of convolution layers over FC layers is their translation invariance property.
[01:02:19.520 --> 01:02:25.920]   Can we say that translation invariance in MLP mixes is achieved since the input is
[01:02:25.920 --> 01:02:31.440]   broken into small patches? I did read about the translation in-- OK, you go, Dr. Veeb. I know you--
[01:02:31.440 --> 01:02:43.120]   So they prove-- so if my understanding is correct, they prove that you can shuffle--
[01:02:43.120 --> 01:02:48.720]   so translation invariance, my understanding is following, that you don't remember the positions.
[01:02:48.720 --> 01:02:55.200]   So it means that your prediction are independent of the positions of where things are located.
[01:02:55.200 --> 01:03:01.200]   And the nice experiment will be that you just mix. And if result changes, it means it's invariant
[01:03:01.200 --> 01:03:10.640]   or-- so it's invariant or not variant. So because you're right. You said that translation invariance
[01:03:10.640 --> 01:03:16.880]   achieved because you break down in the patches and then you flatten. And therefore, you connect
[01:03:16.880 --> 01:03:23.440]   to fully connected. But if you think about this, you can take whole image and you can make it this
[01:03:23.440 --> 01:03:29.600]   one vector. And then you just can have also fully connected layer. And then you probably also will
[01:03:29.600 --> 01:03:34.720]   achieve translation invariance. So breaking down in patches and then what happened afterwards,
[01:03:34.720 --> 01:03:40.160]   like that you apply an n dot linear, I think this all results in achieving this translation
[01:03:40.160 --> 01:03:51.040]   invariance. But again, don't quote me on this. Please. OK. Great session. Appreciate the effort.
[01:03:51.040 --> 01:03:55.840]   Thank you. Is there any intuition behind why MLP mixes with less number of parameters would
[01:03:55.840 --> 01:04:00.320]   require significantly more training data to achieve on power performance with vision
[01:04:00.320 --> 01:04:04.640]   transformers? I do have one. I do have a theory. But Dr. Habib, do you have something to add?
[01:04:04.640 --> 01:04:13.280]   I will go with your answer. OK. I just saw Gian-LeCun tweet that when the architectures
[01:04:13.280 --> 01:04:19.520]   are less rigid, then-- sorry, when you have lots of data, then you could pretty much get away with
[01:04:19.520 --> 01:04:25.280]   less rigid architectures. So that just means that because we're pre-training on 300 million,
[01:04:25.280 --> 01:04:31.760]   we're pre-training on a large, large amount of data sets. Gian-LeCun's tweet was basically saying
[01:04:31.760 --> 01:04:38.560]   when you have that much amount of data, then you could pretty much get away with less--
[01:04:38.560 --> 01:04:44.800]   with an easy architecture. That was the main point. And I think that's the intuition here.
[01:04:44.800 --> 01:04:48.480]   That's why you need that much data. That's my intuition. That's what I've read. But
[01:04:49.920 --> 01:04:57.360]   I don't have proof. The mixer seems very much like self-attention layer in transformer.
[01:04:57.360 --> 01:05:05.040]   Yeah, that's a conversation I don't want to go into. Everything then could be broken. One thing
[01:05:05.040 --> 01:05:10.320]   is everything is matrix multiplication. And then everything could be related to any other thing
[01:05:10.320 --> 01:05:16.960]   in a similar way. So I think we should be careful when we say this looks like self-attention
[01:05:17.600 --> 01:05:24.720]   layer in transformer. You have a query key and value in transformer. And then to me,
[01:05:24.720 --> 01:05:31.040]   this is very different. But again, we should be very careful when making these comparisons.
[01:05:31.040 --> 01:05:39.680]   Yeah, yes. I agree with what you said. There was specifically a point made that this MLP mixer
[01:05:39.680 --> 01:05:45.520]   doesn't require attention. And attention layer is slightly different. It has more complexity
[01:05:45.520 --> 01:05:55.520]   when you increase the data set. But yeah, so it's not similar, at least from what I know.
[01:05:55.520 --> 01:06:03.520]   And the whole point of this paper was that, hey, you don't need attention. You just need MLP.
[01:06:03.520 --> 01:06:09.920]   You just need the mixing. The main idea is you mix the information in two dimensions.
[01:06:09.920 --> 01:06:12.000]   I guess that's my main idea that I got from it.
[01:06:13.120 --> 01:06:20.640]   Yeah. OK. So now I guess that's it for the questions. But we are still left with the
[01:06:20.640 --> 01:06:26.640]   one last thing is where this controversy is coming from on when the MLP mixer is that a CNN or is
[01:06:26.640 --> 01:06:36.080]   that a convolution, basically a CNN? Or is it a fully connected? So something I wrote this morning
[01:06:36.080 --> 01:06:43.840]   and announced on my Twitter is essentially-- and it did take me some time-- but essentially,
[01:06:43.840 --> 01:06:48.160]   then, are fully connected and convolution layers equivalent? So when I was reading this paper,
[01:06:48.160 --> 01:06:53.440]   I actually asked on Twitter, is there a nice visualization that explains why conv1d and
[01:06:53.440 --> 01:06:58.800]   nnm.lne layers are the same? The answer is they are the same. And I have the answer now.
[01:06:58.800 --> 01:07:04.160]   But the main points-- I'll share this blog post as well. One second. I'll just post this in
[01:07:04.160 --> 01:07:16.480]   the chat if I can find where the chat is. OK. So I've just posted in the chat. But the key idea,
[01:07:16.480 --> 01:07:24.640]   the answer is, yes, conv1d operation-- as part of this blog post, I ended up implementing conv1d
[01:07:24.640 --> 01:07:31.200]   operation in Microsoft Excel. So this is how conv1d basically looks in Microsoft Excel.
[01:07:31.200 --> 01:07:36.960]   So if you have an input 3 by 3 matrix-- so let's just say you have 3 by 3 matrix as input. And you
[01:07:36.960 --> 01:07:44.800]   basically have, then, you say-- you create a convolution 1D kernel saying you have three
[01:07:44.800 --> 01:07:51.920]   input channels and five output channels. What that means is you have five filters. And each filter is
[01:07:51.920 --> 01:07:57.360]   of length 3 by 1 because your kernel size is 1 and your input channels is 3. So essentially,
[01:07:57.360 --> 01:08:02.800]   you do a column by column multiplication. Each filter-- so for this first column, this filter
[01:08:02.800 --> 01:08:06.880]   gets multiplied, then this, then this, then this, then the last one. So you get five outputs. Then
[01:08:06.880 --> 01:08:11.760]   the same thing happens for the second column and so on. So that's the convolution operation in Excel.
[01:08:11.760 --> 01:08:18.400]   And I compared the results with-- basically, I implemented this in PyTorch. Just to compare
[01:08:18.400 --> 01:08:23.280]   results, I created convolution 1D. And then the results matched. So that's convolution,
[01:08:24.080 --> 01:08:30.800]   essentially. In Excel, then, you could also implement fully connected layers, which is the
[01:08:30.800 --> 01:08:35.920]   matrix multiplication. So if you have your input, but this time you take the transpose-- you don't
[01:08:35.920 --> 01:08:40.080]   keep the same input. You'd rather just take the transpose, which is-- I'm just going to convert
[01:08:40.080 --> 01:08:45.360]   it like the height to rows if you take the transpose. And then you still have the same
[01:08:45.360 --> 01:08:51.520]   weight matrix. This time, it's not five different kernels. It's a 3 by 5 weight matrix. So you do a
[01:08:51.520 --> 01:08:57.040]   row by column multiplication. And in a fully connected layer, then, you still get the same
[01:08:57.040 --> 01:09:04.560]   result. So if I-- the answer is exactly the same as the conv 1D operation. So I guess the main point
[01:09:04.560 --> 01:09:11.680]   of, then, this blog post is to say that conv 1D, or basically the convolution operation with 1
[01:09:11.680 --> 01:09:17.120]   cross 1 kernel, and the linear layer are the same. Just believe me on that. And if you don't believe
[01:09:17.120 --> 01:09:22.960]   me, have a read of this blog post, then you'll believe me. But what I found is that the convolution
[01:09:22.960 --> 01:09:28.800]   with 1 by 1 kernel and fully connected layer are the same. So let's just stop it at that.
[01:09:28.800 --> 01:09:37.200]   And let's keep that in mind. But then if we go into the MLP mixer, you see how the mixer has
[01:09:37.200 --> 01:09:43.360]   linear layers. So it has nn.linear, nn.linear, nn.linear here and here. What if you replaced,
[01:09:43.360 --> 01:09:50.000]   then, those linear layers with conv 1D, or just a 1 by 1 kernel of a convolution, basically a
[01:09:50.000 --> 01:09:56.880]   convolution 1 by 1 kernel? So you could replace all of these with just a convolution 1 by 1 kernel.
[01:09:56.880 --> 01:10:04.000]   And then you could say that the mixer is basically a CNN, where the mixer layer consists of
[01:10:04.000 --> 01:10:13.120]   convolutions with 1 by 1 kernel. And that's exactly what Jan Lecomte said. So in his tweet,
[01:10:13.120 --> 01:10:18.800]   he basically said, oh, MLP mixer is just a conv layer. I'm opening the tweet as well.
[01:10:18.800 --> 01:10:25.600]   The MLP mixer is basically just a conv layer with 1 by 1 kernel. So that's exactly what he said.
[01:10:25.600 --> 01:10:33.280]   And my understanding of this is because the fully connected and the 1 by 1 kernel convolution layer
[01:10:33.280 --> 01:10:38.480]   are equivalent. So you could replace the fully connected. And as part of this blog post, we
[01:10:38.480 --> 01:10:46.240]   look at the implementation of MLP mixer in PyTorch. And we also pretty much implement the
[01:10:46.240 --> 01:10:53.360]   MLP architecture using just convolutions. So this is the mixer layer, which we implement just using
[01:10:53.360 --> 01:11:00.480]   convolution. So we replace every linear layer with a convolution operation and 1 by 1 kernel.
[01:11:00.480 --> 01:11:05.520]   And we see that the output matches that of the linear layer. So you could have all these
[01:11:05.520 --> 01:11:12.720]   arguments. So then that just proves that MLP mixer is like a conv layer with 1 by 1 kernels.
[01:11:12.720 --> 01:11:20.320]   But does it really? So then the controversy is basically you could even do the patch embedding,
[01:11:20.320 --> 01:11:24.400]   the first part of the architecture, with a 2D conv, which is this part, which is converting
[01:11:24.400 --> 01:11:29.440]   the image to the per patch fully connected and to get vector representations of each patch.
[01:11:29.440 --> 01:11:32.960]   So that's, again, also been explained in the blog post. But you could also then use a convolution
[01:11:32.960 --> 01:11:39.440]   2D to do that. So there's these different ways of doing things because convolution operations are so
[01:11:39.440 --> 01:11:48.320]   flexible. But then you could get into all these arguments of are they the same? Is it just a CNN?
[01:11:48.320 --> 01:11:56.000]   But what Dr. Habib and I feel at the end of the day is that it comes down to semantics.
[01:11:56.000 --> 01:12:03.840]   So we asked Ross-- I pretty much asked Ross Whitman, the author of Team. And he also said
[01:12:03.840 --> 01:12:07.840]   that it comes down to semantics. It's like the deep learning community is split. If you see this
[01:12:07.840 --> 01:12:14.560]   as a CNN, that's for you. I mean, that's your interpretation of it. If we see this as a fully
[01:12:14.560 --> 01:12:18.480]   connected, that's our interpretation of it. Or like there's no right and wrong. That's what
[01:12:18.480 --> 01:12:20.800]   we think. But Dr. Habib, do you want to add something here?
[01:12:21.680 --> 01:12:33.120]   Yes. So I would say, yes. I think-- yeah, I think-- I literally don't have anything to add
[01:12:33.120 --> 01:12:44.800]   because I kind of agree with both sides. The main argument is that it doesn't operate like CNN.
[01:12:45.360 --> 01:12:53.680]   Therefore, it's not a CNN. But the thing is that you can use the operation which CNN performs to
[01:12:53.680 --> 01:13:00.480]   replicate MLP. We can replace nn.linear with columns and so on. So the question is just
[01:13:00.480 --> 01:13:06.800]   whether this operation, which are designed for convolutional neural networks, if you replace
[01:13:06.800 --> 01:13:17.120]   them in MLP, whether it will convert to convolution type network. You can see the
[01:13:17.120 --> 01:13:23.040]   argument in both sides, basically. So yeah. It's up to you.
[01:13:23.040 --> 01:13:25.200]   In my head, I called it a potato-potato problem.
[01:13:25.200 --> 01:13:32.400]   Right. It's up to you. It's up to you. Maybe we will get after this video-- I hope we will
[01:13:32.400 --> 01:13:39.280]   be not get criticized. But I think it's up to you to decide what it will be. And maybe there will be
[01:13:39.280 --> 01:13:45.600]   some-- I don't know. Maybe there will be some-- somebody will do some experiment and will come
[01:13:45.600 --> 01:13:50.160]   up with some solution to this problem. I don't think they-- I think it's still not agreed,
[01:13:50.160 --> 01:13:53.440]   right? What's happening? OK. So yeah.
[01:13:53.440 --> 01:14:02.000]   Yeah. I don't think there's a common consensus yet. And that's the difficulty of discussing
[01:14:02.000 --> 01:14:05.520]   very recent papers, because you don't know what's going to happen next.
[01:14:05.520 --> 01:14:11.920]   Yes. And we'll be like Swiss people, very independent and natural.
[01:14:11.920 --> 01:14:19.200]   Yep. But anyway, then there's this GitHub gist as well that kind of showcases that you could
[01:14:19.200 --> 01:14:24.960]   replace conv1d with linear and all that stuff. So yeah. There's lots of content that we do
[01:14:24.960 --> 01:14:31.200]   provide as supplement. Thanks, Dr. Habib, for all the wonderful, wonderful visualizations.
[01:14:31.200 --> 01:14:36.640]   I will go one last time and just have a look if there's any questions on the report.
[01:14:36.640 --> 01:14:45.360]   The paper reading group. There is one, I believe. I'll share my screen.
[01:14:45.360 --> 01:14:52.720]   Can you guys still see my screen? Yes.
[01:14:52.720 --> 01:14:59.520]   OK. The question is here. On the discussion for FC layers being equivalent to one-by-one cons,
[01:14:59.520 --> 01:15:05.520]   see the shared MLP layers in figure 2 and the code implementation, where the shared FC layers
[01:15:05.520 --> 01:15:10.640]   are implemented as one-by-one cons. OK. Yeah, absolutely. I think there's lots of architectures
[01:15:10.640 --> 01:15:18.240]   that implement basically linear layers as one-by-one cons. And I also found an example of
[01:15:18.240 --> 01:15:27.840]   this back in 2020, January, was GPT-2 implementation in Hugging Face. And they were pretty much using
[01:15:27.840 --> 01:15:33.600]   conv1d operation instead of linear layers. Yeah, that's definitely-- that's being followed quite a
[01:15:33.600 --> 01:15:40.880]   few times in the past and has been implemented that way. So with that being said, I guess that's
[01:15:40.880 --> 01:15:47.920]   a wrap for MLP Mixer. Thanks very much, Dr. Habib, for joining us. Really, really appreciate
[01:15:47.920 --> 01:15:54.800]   your time and effort that you've put into the MLP Mixer and being a part of this today and
[01:15:55.920 --> 01:16:02.080]   sharing your views, sharing your opinions, and explaining things in a wonderful way. And I always,
[01:16:02.080 --> 01:16:07.200]   every minute, I learn new things from you. So thank you for being a part of the paper.
[01:16:07.200 --> 01:16:13.040]   Yeah, it was a pleasure. Thank you for inviting and hosting this wonderful paper reading group.
[01:16:13.040 --> 01:16:21.040]   I mean, I hope we will-- we just have to find some nice paper, and hopefully we can hold one more
[01:16:21.040 --> 01:16:26.480]   at some time in the future. Yeah, absolutely. Look forward to that. Thank you, guys. Thanks,
[01:16:26.480 --> 01:16:29.680]   everybody. Bye-bye. Thank you. Bye.
[01:16:29.680 --> 01:16:39.680]   [BLANK_AUDIO]
[01:16:39.680 --> 01:16:49.680]   [BLANK_AUDIO]

