
[00:00:00.000 --> 00:00:07.520]   The hard part is diversifying the content.
[00:00:07.520 --> 00:00:11.680]   So if we just have the same character in an environment doing everything, it's not going
[00:00:11.680 --> 00:00:12.680]   to work, right?
[00:00:12.680 --> 00:00:16.400]   So how do you actually create hundreds or thousands of variations of that character
[00:00:16.400 --> 00:00:19.320]   model with different behavior and things like that?
[00:00:19.320 --> 00:00:23.880]   That's been really the core focus of how we're thinking about our technology.
[00:00:23.880 --> 00:00:27.720]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:27.720 --> 00:00:29.360]   models work in the real world.
[00:00:29.360 --> 00:00:31.440]   I'm your host, Lukas Biewald.
[00:00:31.440 --> 00:00:37.760]   Dale Kim is the co-founder and CEO of AI.Revery, a startup that specializes in creating high-quality
[00:00:37.760 --> 00:00:41.800]   synthetic training data for computer vision algorithms.
[00:00:41.800 --> 00:00:45.720]   Before that, he was a senior data scientist at the New York Times, and before that, he
[00:00:45.720 --> 00:00:50.440]   got his PhD in computer science from Brown University, focusing on machine learning and
[00:00:50.440 --> 00:00:52.440]   Bayesian statistics.
[00:00:52.440 --> 00:00:55.720]   He's going to talk about tools that will advance machine learning progress, and he's going
[00:00:55.720 --> 00:00:57.080]   to talk about synthetic data.
[00:00:57.080 --> 00:00:59.800]   I'm super excited for this.
[00:00:59.800 --> 00:01:03.400]   I was looking at your LinkedIn and you have a little bit of an unusual path, right?
[00:01:03.400 --> 00:01:06.360]   You did a liberal arts undergrad.
[00:01:06.360 --> 00:01:07.360]   Can you say a little bit about...
[00:01:07.360 --> 00:01:14.720]   I feel like I come across people quite a lot that want to make career transitions into
[00:01:14.720 --> 00:01:16.200]   machine learning and related field.
[00:01:16.200 --> 00:01:18.240]   How was that for you?
[00:01:18.240 --> 00:01:20.040]   What prompted you to do it?
[00:01:20.040 --> 00:01:22.720]   That's a great question.
[00:01:22.720 --> 00:01:25.000]   Wow, searching back.
[00:01:25.000 --> 00:01:32.360]   I studied literature in college, so I actually did not have a lot of computer science background,
[00:01:32.360 --> 00:01:35.760]   and I've taken a lot of twists and turns in my life.
[00:01:35.760 --> 00:01:38.280]   Sarah Lawrence College is a pretty unique educational system.
[00:01:38.280 --> 00:01:43.320]   It's really small class sizes, Socratic system, liberal arts, humanities.
[00:01:43.320 --> 00:01:47.840]   I think from that, I garnered just a curiosity about the world.
[00:01:47.840 --> 00:01:53.160]   Then afterwards, I did a lot of research in schizophrenia.
[00:01:53.160 --> 00:01:58.920]   I studied mental illness and I was sticking people inside MRI scanners and then analyzing
[00:01:58.920 --> 00:02:00.920]   their brain data.
[00:02:00.920 --> 00:02:05.040]   I spent about four years doing that after college.
[00:02:05.040 --> 00:02:08.000]   Again, transition.
[00:02:08.000 --> 00:02:13.120]   After college, no skills, working at a wine shop, and then over time, volunteering at
[00:02:13.120 --> 00:02:17.080]   a lab and getting into that position where I started actually publishing papers and really
[00:02:17.080 --> 00:02:19.640]   getting into computational neuroscience.
[00:02:19.640 --> 00:02:25.400]   I wanted to be a doctor at some point, but then decided at the last minute to do a study
[00:02:25.400 --> 00:02:31.480]   machine learning because I was actually really interested in understanding the underlying
[00:02:31.480 --> 00:02:33.720]   fundamental aspects of intelligence.
[00:02:33.720 --> 00:02:35.000]   What does that mean?
[00:02:35.000 --> 00:02:37.880]   How can you actually model things like that?
[00:02:37.880 --> 00:02:43.760]   Instead of going to medical school, I decided to just do a PhD in computer science.
[00:02:43.760 --> 00:02:48.160]   After that, I wanted to try journalism, trying to see if I can apply and build tools to help
[00:02:48.160 --> 00:02:49.160]   journalism.
[00:02:49.160 --> 00:02:50.160]   I did that a few times for a few years.
[00:02:50.160 --> 00:02:57.120]   Then finally, I was like, "Okay, I really want to do this stuff, synthetic data."
[00:02:57.120 --> 00:03:01.000]   It's a lot of twists and turns, I have to say.
[00:03:01.000 --> 00:03:05.960]   I would have never been able to tell you this is where I would have ended up 10 years ago.
[00:03:05.960 --> 00:03:06.960]   It's been a long journey.
[00:03:06.960 --> 00:03:07.960]   That's so cool.
[00:03:07.960 --> 00:03:11.440]   That's an impressive skill to be able to completely switch fields like that.
[00:03:11.440 --> 00:03:16.240]   I think I'd be too afraid maybe to make a leap that long.
[00:03:16.240 --> 00:03:19.720]   I think it's not easy.
[00:03:19.720 --> 00:03:21.120]   Let me just be clear.
[00:03:21.120 --> 00:03:25.080]   It's not easy learning the math, for example, with machine learning.
[00:03:25.080 --> 00:03:26.400]   Yeah, when did you learn the math?
[00:03:26.400 --> 00:03:28.800]   Because I feel like that's a place where a lot of people feel nervous.
[00:03:28.800 --> 00:03:32.360]   Did you take a math course as an undergrad?
[00:03:32.360 --> 00:03:33.360]   Not really.
[00:03:33.360 --> 00:03:36.560]   I didn't actually take a single math course in undergrad.
[00:03:36.560 --> 00:03:37.560]   I had to learn.
[00:03:37.560 --> 00:03:42.960]   Actually, my PhD was Bayesian nonparametric, which really gets into pretty complicated
[00:03:42.960 --> 00:03:46.000]   math with variational calculus and things like that.
[00:03:46.000 --> 00:03:50.840]   Basically, I suffered.
[00:03:50.840 --> 00:03:53.160]   I spent a lot of hours just learning.
[00:03:53.160 --> 00:03:58.440]   I took some classes as I could during schizophrenia stuff, during the research of that aspect
[00:03:58.440 --> 00:03:59.440]   of my life.
[00:03:59.440 --> 00:04:03.960]   I had to learn some level of statistics and math and probability to be able to analyze
[00:04:03.960 --> 00:04:04.960]   that data.
[00:04:04.960 --> 00:04:10.160]   But then once you get into the machine learning stuff, especially in that area I was in, you
[00:04:10.160 --> 00:04:12.200]   really needed to up your game.
[00:04:12.200 --> 00:04:15.640]   That's where I spent a lot of time trying to play catch up.
[00:04:15.640 --> 00:04:19.920]   I learned a lot and it was an unbelievably fruitful experience, I would say.
[00:04:19.920 --> 00:04:28.800]   Do you have any tips for people trying to learn math outside of an undergrad curriculum?
[00:04:28.800 --> 00:04:33.360]   I think actually one of the best ways for me was actually appreciating the beauty of
[00:04:33.360 --> 00:04:34.360]   math.
[00:04:34.360 --> 00:04:38.960]   A lot of people are scared of math and thinking, "Oh my God, I have to learn these rules and
[00:04:38.960 --> 00:04:40.320]   first, second derivatives.
[00:04:40.320 --> 00:04:42.120]   I have to memorize these things."
[00:04:42.120 --> 00:04:48.960]   But once you get into more of the theoretical stuff and you start thinking about, basically,
[00:04:48.960 --> 00:04:55.560]   I'm not sure if you've heard of these insane millennium problems and P equals NP or the
[00:04:55.560 --> 00:04:59.760]   prime number, stuff like that, the Riemann hypothesis.
[00:04:59.760 --> 00:05:04.160]   There's so much beauty there and you can actually read about it and understand how challenging
[00:05:04.160 --> 00:05:08.680]   some of these problems have been and how profound they are.
[00:05:08.680 --> 00:05:12.920]   And being able to appreciate it from an aesthetic level, I think, helped me give the patience
[00:05:12.920 --> 00:05:15.680]   I need to learn it a little bit more.
[00:05:15.680 --> 00:05:17.080]   But you need to be patient.
[00:05:17.080 --> 00:05:21.240]   Your brain is not going to just pick this stuff up if you've never been exposed to it,
[00:05:21.240 --> 00:05:25.280]   unless you're a lot smarter than I am, which might be the case.
[00:05:25.280 --> 00:05:26.280]   Sounds unlikely.
[00:05:26.280 --> 00:05:27.280]   I don't know.
[00:05:27.280 --> 00:05:34.440]   For a long time, I've had a real interest in synthetic data, which is what your company
[00:05:34.440 --> 00:05:35.440]   does.
[00:05:35.440 --> 00:05:41.040]   How did you get interested in synthetic data working journalism?
[00:05:41.040 --> 00:05:46.040]   My advisor, actually, at Brown was a computer vision person.
[00:05:46.040 --> 00:05:48.680]   I got exposed to a lot of the problems there.
[00:05:48.680 --> 00:05:53.840]   You go to these conferences and it's always the same data sets being used.
[00:05:53.840 --> 00:05:57.240]   One thing I actually wanted to do was one day build my own video game.
[00:05:57.240 --> 00:06:03.200]   I wanted to be able to actually create worlds and I wanted to see if you can integrate machine
[00:06:03.200 --> 00:06:04.200]   learning.
[00:06:04.200 --> 00:06:07.840]   That was an early interest of mine as I was learning this stuff.
[00:06:07.840 --> 00:06:12.200]   And I've always believed simulation was such a powerful tool for a lot of things.
[00:06:12.200 --> 00:06:15.820]   So at some point in the New York Times, I had a really great experience there learning
[00:06:15.820 --> 00:06:19.120]   all sorts of things, amazing community of people.
[00:06:19.120 --> 00:06:22.400]   And then from there, I really wanted to do this thing I've been dreaming of doing.
[00:06:22.400 --> 00:06:25.200]   And I knew that there was such a huge issue.
[00:06:25.200 --> 00:06:31.600]   The way I look at sometimes how science advances, I think, is actually through tools.
[00:06:31.600 --> 00:06:34.120]   You're building a great one with WANDB.
[00:06:34.120 --> 00:06:40.680]   And I think if you think about the microscope, for example, before that, there's entire fields
[00:06:40.680 --> 00:06:42.200]   that open up.
[00:06:42.200 --> 00:06:46.740]   And so what I'm hoping to do is I'm hoping to figure out a way to create a simulation
[00:06:46.740 --> 00:06:50.240]   platform that can one day be used by a lot of people.
[00:06:50.240 --> 00:06:55.320]   And at some point, just introduce new people to ideas about how you can train computer
[00:06:55.320 --> 00:07:01.520]   vision algorithms without the standard process of collecting data in the real world and where
[00:07:01.520 --> 00:07:04.240]   simulation can actually play a really useful role.
[00:07:04.240 --> 00:07:05.820]   So I think that really excited me.
[00:07:05.820 --> 00:07:10.600]   And I actually think that there could be a lot of really important advancements and acceleration
[00:07:10.600 --> 00:07:14.320]   of computer vision with the adoption of synthetic data.
[00:07:14.320 --> 00:07:15.320]   I see.
[00:07:15.320 --> 00:07:20.360]   So you've actually been thinking about synthetic data for quite a long time.
[00:07:20.360 --> 00:07:25.480]   I should say, I don't know if you know about my previous company we were talking about
[00:07:25.480 --> 00:07:27.760]   is CrowdFlower and became Figure Eight.
[00:07:27.760 --> 00:07:28.760]   Yeah.
[00:07:28.760 --> 00:07:30.160]   And we did a lot of data collection.
[00:07:30.160 --> 00:07:31.160]   I think it's funny.
[00:07:31.160 --> 00:07:35.000]   I think it was sort of similar experience to you of actually looking at conference papers
[00:07:35.000 --> 00:07:40.480]   and realizing they're kind of all built around the same data set, almost based on the data
[00:07:40.480 --> 00:07:43.400]   sets that were available, which feels totally backwards.
[00:07:43.400 --> 00:07:44.400]   Right?
[00:07:44.400 --> 00:07:45.400]   Yeah.
[00:07:45.400 --> 00:07:46.400]   Yeah.
[00:07:46.400 --> 00:07:52.240]   I think it's especially like, as a, just starting out like grad student researcher, you're the
[00:07:52.240 --> 00:07:54.160]   one that ends up spending a lot of time with the data sets.
[00:07:54.160 --> 00:07:58.560]   You realize how kind of messy they are and idiosyncratic they are.
[00:07:58.560 --> 00:07:59.560]   Absolutely.
[00:07:59.560 --> 00:08:04.800]   And I would just also add that, a lot of my work was in, during my PhD was in sort of
[00:08:04.800 --> 00:08:06.080]   Bayesian models.
[00:08:06.080 --> 00:08:11.680]   So there you have this notion of prior belief, you then estimate your posterior from that.
[00:08:11.680 --> 00:08:17.100]   But in deep learning, you kind of, it's not sort of that easy to establish a prior.
[00:08:17.100 --> 00:08:20.320]   And I actually think in a way that you can really control.
[00:08:20.320 --> 00:08:24.840]   And I actually think synthetic data, at least for computer vision, the data itself can actually
[00:08:24.840 --> 00:08:27.040]   act as a really interesting prior.
[00:08:27.040 --> 00:08:31.360]   So there's connections there that I think I took from my own work of wanting to think
[00:08:31.360 --> 00:08:33.600]   about how to incorporate that.
[00:08:33.600 --> 00:08:38.160]   And simulation is one aspect of using data to generate that prior.
[00:08:38.160 --> 00:08:43.640]   Well, so, we always want to make this show for people that work in machine learning,
[00:08:43.640 --> 00:08:46.160]   but aren't necessarily domain experts in every-
[00:08:46.160 --> 00:08:47.160]   Sure.
[00:08:47.160 --> 00:08:48.160]   Sure.
[00:08:48.160 --> 00:08:52.360]   Maybe you could explain what synthetic data is and-
[00:08:52.360 --> 00:08:53.360]   Yeah, absolutely.
[00:08:53.360 --> 00:08:57.360]   And kind of your take and how your system works today and then how you imagine it working
[00:08:57.360 --> 00:08:58.360]   in the future.
[00:08:58.360 --> 00:08:59.360]   Yeah.
[00:08:59.360 --> 00:09:05.400]   So the way we're talking about synthetic data is basically data that is generated from,
[00:09:05.400 --> 00:09:09.120]   let's say a gaming engine or something that doesn't come from the real world, it's sort
[00:09:09.120 --> 00:09:10.860]   of artificially generated.
[00:09:10.860 --> 00:09:15.720]   And of course, people talk about synthetic data in LLP as well, in generating fake text
[00:09:15.720 --> 00:09:17.800]   or text that's relatively useful there.
[00:09:17.800 --> 00:09:22.900]   But for our purpose in our startup, we're primarily focused on computer vision.
[00:09:22.900 --> 00:09:28.000]   And so, what we try to do is we try to create these very photorealistic virtual worlds.
[00:09:28.000 --> 00:09:30.080]   We extract images from them.
[00:09:30.080 --> 00:09:35.920]   And then the nice thing about doing that in a simulated world is that you can encode some
[00:09:35.920 --> 00:09:40.440]   of the things you need for supervised learning in computer vision, like the annotations and
[00:09:40.440 --> 00:09:42.040]   all that stuff directly.
[00:09:42.040 --> 00:09:47.480]   So you can sort of help bypass that part of it and then help sort of streamline that process.
[00:09:47.480 --> 00:09:49.920]   So that's what we're focused on.
[00:09:49.920 --> 00:09:53.880]   And we've been at it for close to four years now.
[00:09:53.880 --> 00:09:59.400]   And essentially, we're trying to see where synthetic data works really well and how to
[00:09:59.400 --> 00:10:02.360]   push the boundaries there.
[00:10:02.360 --> 00:10:03.880]   So where does it work well?
[00:10:03.880 --> 00:10:06.800]   How real is it?
[00:10:06.800 --> 00:10:07.800]   Yeah.
[00:10:07.800 --> 00:10:08.800]   Great question.
[00:10:08.800 --> 00:10:13.960]   I like to think of what is a narrow problem and what is not so narrow.
[00:10:13.960 --> 00:10:16.000]   So I say narrow AI all the time.
[00:10:16.000 --> 00:10:18.760]   And then think things like conveyor belts.
[00:10:18.760 --> 00:10:23.680]   Let's say you're processing certain types of food items, things like that.
[00:10:23.680 --> 00:10:27.760]   You're not going to see a random golden retriever jump on or things like that.
[00:10:27.760 --> 00:10:30.200]   The diversity of that scenario is not that large.
[00:10:30.200 --> 00:10:33.320]   I think there, synthetic data really shines.
[00:10:33.320 --> 00:10:34.320]   That's one of those places.
[00:10:34.320 --> 00:10:38.260]   And of course, people are using it for really complex things like self-driving cars and
[00:10:38.260 --> 00:10:39.580]   things like that.
[00:10:39.580 --> 00:10:43.600]   But I would say if you want to think of a heuristic, the more narrow the problem, the
[00:10:43.600 --> 00:10:45.960]   more synthetic data will play a role.
[00:10:45.960 --> 00:10:51.360]   But of course, on the other end, there's attempts that we'll make to try to create that diversity.
[00:10:51.360 --> 00:10:56.840]   So the way we think about it as a company is how do you create diversity and how do
[00:10:56.840 --> 00:10:58.280]   you scale that?
[00:10:58.280 --> 00:11:01.560]   So we incorporate a lot of proceduralism in our world.
[00:11:01.560 --> 00:11:06.720]   We think about how to procedurally generate meshes, geometry, 3D models, things like that,
[00:11:06.720 --> 00:11:09.920]   how to automatically change the terrain, all that stuff.
[00:11:09.920 --> 00:11:12.660]   That's really a big focus of our work.
[00:11:12.660 --> 00:11:18.740]   And then understanding how you can quantify that gap between synthetic and real data through
[00:11:18.740 --> 00:11:20.100]   benchmarking of algorithms.
[00:11:20.100 --> 00:11:24.020]   And that's where we use a lot of 1DB as well to understand that.
[00:11:24.020 --> 00:11:25.860]   So how would it work?
[00:11:25.860 --> 00:11:29.620]   Say I'm trying to imagine what I might be doing where I would want to come to you.
[00:11:29.620 --> 00:11:30.620]   You can tell me like a real quick.
[00:11:30.620 --> 00:11:36.900]   If I'm trying to do factory automation, since you said conveyor belt.
[00:11:36.900 --> 00:11:42.340]   I want to classify, does this machine look like in a normal state or like a broken state?
[00:11:42.340 --> 00:11:43.540]   Right, right, right.
[00:11:43.540 --> 00:11:47.100]   So I'll give you an example I can talk about a little bit.
[00:11:47.100 --> 00:11:51.340]   So one problem is this company we're working with called Blue River.
[00:11:51.340 --> 00:11:57.620]   They're trying to solve this problem of being able to identify weeds in a crop field.
[00:11:57.620 --> 00:12:03.940]   And it turns out that if you were able to target the herbicide you use, you can reduce
[00:12:03.940 --> 00:12:07.580]   the amount of herbicides by like 95%.
[00:12:07.580 --> 00:12:10.320]   So farmers are just spraying all over.
[00:12:10.320 --> 00:12:14.100]   So for what we've done on our end is that we've created an environment where we actually
[00:12:14.100 --> 00:12:18.780]   procedurally generate weeds with different vegetation stages and all sorts of things
[00:12:18.780 --> 00:12:20.620]   like that.
[00:12:20.620 --> 00:12:26.380]   And then be able to then automatically annotate that, like via a segmentation mask, and then
[00:12:26.380 --> 00:12:31.280]   train an algorithm and show that we're getting X amount of improvement.
[00:12:31.280 --> 00:12:33.780]   Another example I can talk about is 7-Eleven.
[00:12:33.780 --> 00:12:40.600]   We're working with them where we're actually creating a retail store with all these items.
[00:12:40.600 --> 00:12:46.880]   And there they're interested in things like activity, understanding grasp, pose detection,
[00:12:46.880 --> 00:12:49.960]   things like that, grasp intention.
[00:12:49.960 --> 00:12:53.680]   And so there we have our own motion capture studio.
[00:12:53.680 --> 00:12:57.680]   So we actually have a lot of really cool animations that we can generate from there.
[00:12:57.680 --> 00:13:02.720]   And then so we create all that simulated data, and then all of that has perfect ground annotations,
[00:13:02.720 --> 00:13:08.280]   and we feed it to them that they can basically download and then use to train their own algorithms.
[00:13:08.280 --> 00:13:11.080]   So what's the point where...
[00:13:11.080 --> 00:13:13.840]   I mean, both of those, those are great examples.
[00:13:13.840 --> 00:13:15.840]   It makes total sense.
[00:13:15.840 --> 00:13:21.680]   But it also strike me as kind of tricky to set up, to make it really realistic.
[00:13:21.680 --> 00:13:26.920]   What's the sort of scale that you need to be at for this type of approach to make sense?
[00:13:26.920 --> 00:13:33.520]   Yeah, so it depends ultimately on their data set they're benchmarking against.
[00:13:33.520 --> 00:13:39.880]   So actually, when we work with companies, we often ask, "Can you share at least an evaluation
[00:13:39.880 --> 00:13:43.160]   real world data set that we can benchmark against?"
[00:13:43.160 --> 00:13:48.060]   So oftentimes, the first iteration that we run and create this environment might get
[00:13:48.060 --> 00:13:52.260]   you a certain percentage, like 60% of the real data baseline.
[00:13:52.260 --> 00:13:56.440]   Real data baseline being if you were to train the same algorithm on the real data only,
[00:13:56.440 --> 00:13:59.760]   what is the sort of thing you would get from that?
[00:13:59.760 --> 00:14:06.000]   And then we keep iterating and improving, and we have ways of finding out where the
[00:14:06.000 --> 00:14:08.980]   gaps are in terms of the synthetic and real data.
[00:14:08.980 --> 00:14:15.040]   And then we have a whole team of procedural artists from the game industry that actually
[00:14:15.040 --> 00:14:19.680]   work to develop better ways of actually creating more diversity within those scenes.
[00:14:19.680 --> 00:14:25.160]   So it's not something that happens instantaneously, but it is something that once you build it,
[00:14:25.160 --> 00:14:26.160]   it's there forever.
[00:14:26.160 --> 00:14:29.560]   And so you can just keep generating more and more data and iterating on that.
[00:14:29.560 --> 00:14:33.880]   So the early parts of our company was just trying to create that infrastructure, and
[00:14:33.880 --> 00:14:38.520]   then being able to have a streamlined process of iterating on that.
[00:14:38.520 --> 00:14:41.080]   The way I like to think about it is sort of a virtuous cycle.
[00:14:41.080 --> 00:14:46.160]   We generate the environment, we collect data, we benchmark it, and then we iterate again
[00:14:46.160 --> 00:14:49.980]   and again and again until we get to a point where we're happy with the synthetic data.
[00:14:49.980 --> 00:14:53.140]   But on the first time, it's usually never...
[00:14:53.140 --> 00:15:00.760]   Unless it's a very simple, narrow problem, you usually don't get up to the same performance.
[00:15:00.760 --> 00:15:04.260]   And then depending on the problem, you'll look for different things in terms of what
[00:15:04.260 --> 00:15:05.820]   to improve.
[00:15:05.820 --> 00:15:10.620]   You might miss a certain type of orientation of an object, or you might have zoom levels
[00:15:10.620 --> 00:15:13.060]   that are off that you didn't account for.
[00:15:13.060 --> 00:15:17.380]   Do the images that you generate look extremely realistic?
[00:15:17.380 --> 00:15:20.860]   Is that really important to making it work well?
[00:15:20.860 --> 00:15:25.380]   I think if I had to choose one, diversity is more important than photorealism.
[00:15:25.380 --> 00:15:29.700]   And so I'm defining photorealism as the way people think about in computer graphics, where
[00:15:29.700 --> 00:15:36.260]   you're modeling the light rays bouncing off of every part of an object and calculating
[00:15:36.260 --> 00:15:37.260]   that.
[00:15:37.260 --> 00:15:40.300]   That's how you get those CGI level realism.
[00:15:40.300 --> 00:15:44.740]   Because I do think the technology that's coming out with the latest version of Unreal Engine
[00:15:44.740 --> 00:15:52.380]   and NVIDIA is coming out with a global illumination system, that is just going to happen and GPUs
[00:15:52.380 --> 00:15:53.380]   are getting more powerful.
[00:15:53.380 --> 00:15:58.060]   So that level of realism is there, but the hard part is diversifying the content.
[00:15:58.060 --> 00:16:02.180]   So if we just have the same character in an environment doing everything, it's not going
[00:16:02.180 --> 00:16:03.180]   to work.
[00:16:03.180 --> 00:16:06.900]   So how do you actually create hundreds or thousands of variations of that character
[00:16:06.900 --> 00:16:09.820]   model with different behavior and things like that?
[00:16:09.820 --> 00:16:14.220]   That's been really the core focus of how we're thinking about our technology.
[00:16:14.220 --> 00:16:15.220]   I see.
[00:16:15.220 --> 00:16:19.900]   And this must be really hard, but if I came to you and I was like, "Hey, I want my accuracy
[00:16:19.900 --> 00:16:20.900]   to go."
[00:16:20.900 --> 00:16:22.260]   How would you even think about that?
[00:16:22.260 --> 00:16:26.980]   What kinds of performance gains do you predict?
[00:16:26.980 --> 00:16:29.320]   Let me answer that question in two ways.
[00:16:29.320 --> 00:16:33.580]   One way is there are scenarios where the only thing that could really work is synthetic
[00:16:33.580 --> 00:16:34.580]   data.
[00:16:34.580 --> 00:16:40.220]   So let's say you have a conveyor belt of, I don't know, ceramic mugs, and you need to
[00:16:40.220 --> 00:16:44.880]   also have an annotation around how much they weigh.
[00:16:44.880 --> 00:16:49.420]   You could potentially actually estimate that in a synthetic environment because you might
[00:16:49.420 --> 00:16:52.180]   understand the materials and you can calculate that.
[00:16:52.180 --> 00:16:57.380]   While it might be hard for a human annotator to look at that and be like, "This is 37 grams."
[00:16:57.380 --> 00:17:05.100]   So there are scenarios where actually it can only seem to work with a ground truth thing.
[00:17:05.100 --> 00:17:07.420]   So there's an advantage there.
[00:17:07.420 --> 00:17:10.000]   So in terms of performance, I think it really depends.
[00:17:10.000 --> 00:17:15.860]   So I can give you off the top of my head, for the narrow cases, you're essentially looking
[00:17:15.860 --> 00:17:22.220]   at 90.99, 0.98 mean average precision for things like that.
[00:17:22.220 --> 00:17:26.060]   When you're starting to talk about much more complex things, we released a paper called
[00:17:26.060 --> 00:17:32.920]   Rare Planes, where we actually released with Cosmic Labs, a huge synthetic satellite image
[00:17:32.920 --> 00:17:40.320]   with airplanes and all that stuff that's already been annotated in a synthetic version of that.
[00:17:40.320 --> 00:17:46.440]   Synthetic alone will give you 65 to 70% of the real baseline performance.
[00:17:46.440 --> 00:17:49.960]   But then what we do, and we would like to advocate for this, is that there's several
[00:17:49.960 --> 00:17:51.160]   things you can do on top of that.
[00:17:51.160 --> 00:17:52.680]   One is transfer learning.
[00:17:52.680 --> 00:17:56.480]   So you can actually just take 10% of the real data, and then you start getting into the
[00:17:56.480 --> 00:18:01.880]   95% of the performance of the real world data, just using 10% of that.
[00:18:01.880 --> 00:18:03.600]   And then you also have things like domain...
[00:18:03.600 --> 00:18:04.600]   Sorry?
[00:18:04.600 --> 00:18:06.440]   I just want to make sure I understand what you're saying.
[00:18:06.440 --> 00:18:11.360]   So you train on the synthetic data first, and then you transfer to the non-synthetic
[00:18:11.360 --> 00:18:12.360]   data?
[00:18:12.360 --> 00:18:15.380]   Yeah, you take the real world, just 10% of the real world data, and you fine tune it
[00:18:15.380 --> 00:18:16.560]   off of that.
[00:18:16.560 --> 00:18:19.960]   So you can either pre-train it that way.
[00:18:19.960 --> 00:18:20.960]   What's that?
[00:18:20.960 --> 00:18:23.360]   The final step, you fine tune it on...
[00:18:23.360 --> 00:18:24.720]   Yeah, exactly.
[00:18:24.720 --> 00:18:27.200]   And then you get much better performances.
[00:18:27.200 --> 00:18:32.080]   Of course, that 10% comes from the real world training set, not the test set.
[00:18:32.080 --> 00:18:37.880]   And so the fine tuning step, I think the way I look at why that performance gets up to
[00:18:37.880 --> 00:18:44.420]   95% is that I think you're feeding the prior version of what the algorithm thinks the world
[00:18:44.420 --> 00:18:45.420]   should be.
[00:18:45.420 --> 00:18:49.980]   And then all the noise that comes from the sensors, and any unique variations of that,
[00:18:49.980 --> 00:18:52.900]   can be transferred in that fine tuning step.
[00:18:52.900 --> 00:18:58.060]   And sort of taking that fuzzy vision and then sharpening it with some real world data.
[00:18:58.060 --> 00:19:01.660]   You say 10% of the training data.
[00:19:01.660 --> 00:19:06.460]   So did you take the other 90% and use it in the initial model?
[00:19:06.460 --> 00:19:07.460]   No.
[00:19:07.460 --> 00:19:13.060]   So we would just randomly sample 10% of the real world training data for the fine tuning
[00:19:13.060 --> 00:19:14.060]   step.
[00:19:14.060 --> 00:19:17.340]   And then we'll first train it off of just synthetic only.
[00:19:17.340 --> 00:19:19.860]   And then so we train it off of the synthetic data first.
[00:19:19.860 --> 00:19:24.100]   That gets us to something like 60 to 70%, at least in the airplanes scenario, which
[00:19:24.100 --> 00:19:25.420]   is a bit complex.
[00:19:25.420 --> 00:19:31.220]   And then when we take just 10% randomly sampled from the training data set in the real world,
[00:19:31.220 --> 00:19:37.700]   real world images, then we get to the same 95% of, if you were to train on 100% of the
[00:19:37.700 --> 00:19:38.700]   real world data.
[00:19:38.700 --> 00:19:39.700]   Oh, I see.
[00:19:39.700 --> 00:19:43.420]   So you're saying it's like kind of 10 times as efficiently using the label.
[00:19:43.420 --> 00:19:47.340]   You're saving a little bit of 90% of the real world data needs.
[00:19:47.340 --> 00:19:54.060]   And presumably if you used all the real world data, you'd make an even better model then?
[00:19:54.060 --> 00:19:57.660]   We found it actually tapers off a bit after 10%.
[00:19:57.660 --> 00:19:59.460]   After 10%, it tends to taper off.
[00:19:59.460 --> 00:20:05.240]   I mean, diminishing returns, which is what I'm saying.
[00:20:05.240 --> 00:20:07.140]   But of course, they're still there.
[00:20:07.140 --> 00:20:11.220]   But again, this is one scenario.
[00:20:11.220 --> 00:20:13.620]   Different scenarios have different performances.
[00:20:13.620 --> 00:20:18.120]   And it really actually depends on the data set you have at the end of the day.
[00:20:18.120 --> 00:20:21.900]   I think other people point this out all the time, not enough people focus on the data
[00:20:21.900 --> 00:20:22.900]   itself.
[00:20:22.900 --> 00:20:27.300]   And if your data set's really wonky, who knows what you can train off of it?
[00:20:27.300 --> 00:20:31.180]   Who knows if the benchmarks even are useful there at that point?
[00:20:31.180 --> 00:20:33.620]   So there's a lot of things to consider.
[00:20:33.620 --> 00:20:36.440]   But generally, we find that fine tuning helps.
[00:20:36.440 --> 00:20:40.180]   And then the other thing I wanted to bring up was domain adaptation, which is the sort
[00:20:40.180 --> 00:20:45.860]   of set of algorithms in computer vision that try to transfer the statistics from real world
[00:20:45.860 --> 00:20:47.740]   images to synthetic images.
[00:20:47.740 --> 00:20:53.020]   So algorithms are like image to image translation, where you can maybe think style transfer,
[00:20:53.020 --> 00:20:57.420]   sort of take that from the real world images and try to incorporate that noise, interesting
[00:20:57.420 --> 00:20:59.980]   real world noise into the synthetic images themselves.
[00:20:59.980 --> 00:21:01.460]   Oh, interesting.
[00:21:01.460 --> 00:21:03.060]   Can you really see that in the image?
[00:21:03.060 --> 00:21:04.340]   Yeah, yeah, actually.
[00:21:04.340 --> 00:21:06.340]   I mean, NVIDIA has done some great work on this.
[00:21:06.340 --> 00:21:10.700]   So NVIDIA has definitely done a lot of good work in domain adaptation.
[00:21:10.700 --> 00:21:15.580]   And then at these computer vision conferences, it's been a really active area of research.
[00:21:15.580 --> 00:21:17.980]   And it's sometimes not that distinguishable.
[00:21:17.980 --> 00:21:22.440]   What you find is more texture differences versus shape differences.
[00:21:22.440 --> 00:21:26.660]   As you can imagine, those are probably more difficult things to transfer.
[00:21:26.660 --> 00:21:27.860]   But it does help.
[00:21:27.860 --> 00:21:30.140]   It definitely does help for certain scenarios.
[00:21:30.140 --> 00:21:31.140]   Interesting.
[00:21:31.140 --> 00:21:35.740]   So, I mean, I think the first thing you said when we were talking is, you envision this
[00:21:35.740 --> 00:21:39.140]   as being like a tool for people to use.
[00:21:39.140 --> 00:21:43.420]   But it sounds like maybe today you sort of need to involve real artists, right?
[00:21:43.420 --> 00:21:47.820]   So I would assume that the interface isn't really like a tool that I, you know, it wouldn't
[00:21:47.820 --> 00:21:49.820]   be like a TensorFlow that I know.
[00:21:49.820 --> 00:21:51.940]   No, no, no, no, no, no.
[00:21:51.940 --> 00:21:55.060]   So I guess, what's your plan to kind of bridge that gap?
[00:21:55.060 --> 00:21:57.220]   Yeah, that's a great question.
[00:21:57.220 --> 00:22:01.780]   So you can almost think of it like a downloadable video game, right?
[00:22:01.780 --> 00:22:05.540]   At some point, if you build an awesome enough environment, and we're building out a whole
[00:22:05.540 --> 00:22:07.700]   UI and productizing that process.
[00:22:07.700 --> 00:22:11.100]   So you can imagine these virtual environments living on the cloud somewhere.
[00:22:11.100 --> 00:22:14.860]   And then you have an API that allows you to tweak certain things like lighting, time of
[00:22:14.860 --> 00:22:18.420]   day, how things are spawned, all of that stuff.
[00:22:18.420 --> 00:22:20.980]   And then you'll be able to collect your own data that way.
[00:22:20.980 --> 00:22:21.980]   Right?
[00:22:21.980 --> 00:22:26.260]   So it's not so much that we give people the ability to create their own 3D worlds as much
[00:22:26.260 --> 00:22:30.780]   as we'll create it and give them access to this huge environment that allows them to
[00:22:30.780 --> 00:22:35.900]   collect as much data as they want, and to sort of see how far we can push that.
[00:22:35.900 --> 00:22:38.380]   And it's, we're still building that out.
[00:22:38.380 --> 00:22:42.340]   But I'm hoping that once we start doing that, and we set the paradigm for that other people
[00:22:42.340 --> 00:22:48.140]   will follow and understand the value of that when it comes to computer vision.
[00:22:48.140 --> 00:22:51.900]   And yeah, so hopefully that's, that's how and, you know, there's some really other cool
[00:22:51.900 --> 00:22:57.580]   ideas to like stuff around medicine stuff where you're actually, if you can create an
[00:22:57.580 --> 00:23:03.140]   environment that has a lot of really interesting ways, you can modify it through API calls
[00:23:03.140 --> 00:23:04.620]   or some scripts.
[00:23:04.620 --> 00:23:09.580]   You can then imagine the reinforcement learning algorithm that can explore and exploit a whole
[00:23:09.580 --> 00:23:14.340]   range of parameters to figure out how to actually get the best synthetic data, right?
[00:23:14.340 --> 00:23:19.060]   Where the reward function is tied to, let's say your mean average for precision and things
[00:23:19.060 --> 00:23:20.060]   like that.
[00:23:20.060 --> 00:23:25.140]   So, I'm curious, like in your company, is it mostly sort of like artists making this
[00:23:25.140 --> 00:23:28.820]   stuff or is it mostly machine learning people or is it graphics people?
[00:23:28.820 --> 00:23:30.820]   Like what's the composition of...
[00:23:30.820 --> 00:23:33.020]   Yeah, it's a very interesting mix.
[00:23:33.020 --> 00:23:38.940]   One of the best things about working with all these folks is that they come from a wide
[00:23:38.940 --> 00:23:39.940]   range.
[00:23:39.940 --> 00:23:43.900]   So, we have procedural artists, we have your standard technical artists.
[00:23:43.900 --> 00:23:47.580]   So procedural artists will be able to do some amazing things with geometry and create all
[00:23:47.580 --> 00:23:50.020]   sorts of geometry procedurally.
[00:23:50.020 --> 00:23:54.340]   We have people who understand how to create procedural textures, materials on those three.
[00:23:54.340 --> 00:24:01.020]   So, a lot of game developers, we have animation people, we have to monitor the motion capture.
[00:24:01.020 --> 00:24:05.940]   We have game engineers to sort of be the glue that puts all the stuff together.
[00:24:05.940 --> 00:24:10.860]   And then we have a whole team of deep learning people who actually benchmark that data.
[00:24:10.860 --> 00:24:16.340]   And so, the content is generated from one side of the company, gets fed to the ML people,
[00:24:16.340 --> 00:24:18.060]   and then they're like, "Eh, it's great.
[00:24:18.060 --> 00:24:19.060]   No, it's good."
[00:24:19.060 --> 00:24:22.100]   Like, you know, we need to do, you know, this is not working, this is working, and then
[00:24:22.100 --> 00:24:23.100]   it goes back.
[00:24:23.100 --> 00:24:26.780]   So, there's a constant conversation between these two groups of people where we're always
[00:24:26.780 --> 00:24:31.060]   trying to improve the data and understand what's missing.
[00:24:31.060 --> 00:24:37.060]   Did the ML people do any of the image generation now, like with GANs and other techniques?
[00:24:37.060 --> 00:24:41.940]   Like, have you started to do that or is it mostly sort of more classical procedural generation?
[00:24:41.940 --> 00:24:45.580]   I'm not super familiar with the field, so I don't know how much...
[00:24:45.580 --> 00:24:46.580]   Oh, yeah.
[00:24:46.580 --> 00:24:48.980]   We've tried some of the adversarial stuff.
[00:24:48.980 --> 00:24:51.300]   It's not easy to get GANs to optimize.
[00:24:51.300 --> 00:24:54.820]   There's a lot of, you know, issues of mode collapse and things like that.
[00:24:54.820 --> 00:24:59.180]   So, the adversarial networks, we tend to use that more for domain adaptation.
[00:24:59.180 --> 00:25:05.640]   So, you can imagine those techniques where, you know, you're trying to create no distinguishable
[00:25:05.640 --> 00:25:09.540]   difference between the synthetic and real data, that those GANs can be sort of very
[00:25:09.540 --> 00:25:11.220]   useful there.
[00:25:11.220 --> 00:25:16.260]   We are still actively doing some R&D on geometry creation.
[00:25:16.260 --> 00:25:20.620]   There's a really cool paper out called Polygen from DeepMind that does some cool work on
[00:25:20.620 --> 00:25:21.740]   that space.
[00:25:21.740 --> 00:25:24.940]   But yeah, it's right now...
[00:25:24.940 --> 00:25:30.220]   What we're trying to really focus on is trying to create a whole suite of procedural tools
[00:25:30.220 --> 00:25:32.180]   that are based off of tools like Houdini.
[00:25:32.180 --> 00:25:37.460]   I don't know if you've heard of Houdini before, but it's a way to create procedural geometry.
[00:25:37.460 --> 00:25:40.420]   And then we've done a lot of good work with that.
[00:25:40.420 --> 00:25:46.620]   And at some point, we want to move more towards a system where we can just train off of our
[00:25:46.620 --> 00:25:50.220]   current library of 3D models, which is really large right now.
[00:25:50.220 --> 00:25:53.580]   And then be able to generate new models there.
[00:25:53.580 --> 00:25:58.260]   But I think it's still a little bit more R&D that's necessary to get that stable.
[00:25:58.260 --> 00:26:00.780]   So that's my guess, but I don't know.
[00:26:00.780 --> 00:26:04.980]   Maybe somebody has an amazing algorithm out there that works all the time.
[00:26:04.980 --> 00:26:05.980]   Yeah.
[00:26:05.980 --> 00:26:11.380]   And so, most of the models that you're actually building are vision models, it sounds like.
[00:26:11.380 --> 00:26:14.100]   Yeah, we're primarily focused on vision.
[00:26:14.100 --> 00:26:15.100]   I think...
[00:26:15.100 --> 00:26:21.580]   And the reason why is because, as much as we'd love to do RL-based things, vision is
[00:26:21.580 --> 00:26:26.580]   nice because it doesn't have to require the kind of physics necessary in a game engine,
[00:26:26.580 --> 00:26:32.060]   which what we use is we use Unreal game engine to build our platform.
[00:26:32.060 --> 00:26:38.420]   And there, the physics isn't as accurate as you would need to get the right RL stuff working.
[00:26:38.420 --> 00:26:42.460]   So we're waiting until that becomes more mature before jumping into an RL.
[00:26:42.460 --> 00:26:45.060]   But right now, computer vision is our primary focus.
[00:26:45.060 --> 00:26:46.060]   Interesting.
[00:26:46.060 --> 00:26:48.300]   You're waiting for a good physics engine to do RL?
[00:26:48.300 --> 00:26:49.860]   That sounds like a real opportunity.
[00:26:49.860 --> 00:26:51.060]   Yeah, exactly.
[00:26:51.060 --> 00:26:56.420]   But once we can create these unbelievably rich, realistic worlds, then incorporating
[00:26:56.420 --> 00:26:58.740]   the physics will be the next step.
[00:26:58.740 --> 00:27:06.340]   And then we'll get a cute little dog running around in the field, jumping over rocks.
[00:27:06.340 --> 00:27:07.340]   So that'll be...
[00:27:07.340 --> 00:27:08.340]   I'm curious.
[00:27:08.340 --> 00:27:11.020]   I've played a little bit with Majoko.
[00:27:11.020 --> 00:27:14.580]   What makes that not something that you could use for this kind of thing?
[00:27:14.580 --> 00:27:15.580]   It's just this...
[00:27:15.580 --> 00:27:19.340]   And I don't have as much familiarity with that platform as well.
[00:27:19.340 --> 00:27:24.700]   But what we love about the Unreal game engine is that it is such a powerful suite of tools.
[00:27:24.700 --> 00:27:27.140]   And it is capable of a lot of stuff.
[00:27:27.140 --> 00:27:36.260]   A lot of huge scaled worlds, really, really being able to have high performance, photorealism,
[00:27:36.260 --> 00:27:40.220]   especially with the new Unreal engine coming out, you're going to be able to get near photorealism
[00:27:40.220 --> 00:27:41.500]   in real time.
[00:27:41.500 --> 00:27:42.820]   So all that stuff is just...
[00:27:42.820 --> 00:27:44.860]   It really allows you to create rich worlds.
[00:27:44.860 --> 00:27:50.540]   And some of the other platforms that I've seen really aren't built for that in the same
[00:27:50.540 --> 00:27:52.060]   way, in my opinion.
[00:27:52.060 --> 00:27:57.460]   So I was looking for the most robust system to build all of this stuff to be able to...
[00:27:57.460 --> 00:28:01.240]   One of the cool things we do is we, for example, generate huge cities.
[00:28:01.240 --> 00:28:05.020]   So we'll take things like OpenStreetMap, geospatial data, things like that.
[00:28:05.020 --> 00:28:08.140]   And then we'll generate a big part of Manhattan, for example.
[00:28:08.140 --> 00:28:11.780]   And that takes us a few days to just put it through a system.
[00:28:11.780 --> 00:28:16.100]   And then out pops this fully virtual 3D world that you can walk around in.
[00:28:16.100 --> 00:28:20.780]   So this is stuff that I think the Unreal engine is quite well suited for.
[00:28:20.780 --> 00:28:25.020]   Switching gears slightly to the ML team, because I think that's going to really resonate for
[00:28:25.020 --> 00:28:27.900]   people listening and watching this.
[00:28:27.900 --> 00:28:33.500]   You've now been building models for customers for four years, I guess, which is probably
[00:28:33.500 --> 00:28:34.500]   longer than...
[00:28:34.500 --> 00:28:36.700]   Or at least like building proxy models.
[00:28:36.700 --> 00:28:37.700]   Yeah.
[00:28:37.700 --> 00:28:39.740]   For enterprise and production.
[00:28:39.740 --> 00:28:44.820]   And I wonder how have your processes and tools changed over the years that you've been doing
[00:28:44.820 --> 00:28:45.820]   it?
[00:28:45.820 --> 00:28:46.820]   Yeah.
[00:28:46.820 --> 00:28:51.060]   So I just want to caveat this by saying that we don't try to actually create models that
[00:28:51.060 --> 00:28:55.860]   are going to be used by everybody in the world or production.
[00:28:55.860 --> 00:29:00.280]   We train models for the purpose of understanding how good our synthetic data is.
[00:29:00.280 --> 00:29:06.660]   So unfortunately, we're not spending all our time pushing the boundaries on the next version,
[00:29:06.660 --> 00:29:09.660]   the next version of transformer architectures.
[00:29:09.660 --> 00:29:15.420]   We're not as focused on that, for example, but we're more focused on trying to understand.
[00:29:15.420 --> 00:29:20.180]   And I actually think it's a different way to think about optimizing your model.
[00:29:20.180 --> 00:29:26.060]   Of course, you can optimize it through hyperparameter searches, messing around with learning rates,
[00:29:26.060 --> 00:29:27.680]   all sorts of things like that.
[00:29:27.680 --> 00:29:31.580]   But actually, the way we do it is that we'll try a few things here and there in terms of
[00:29:31.580 --> 00:29:35.780]   the hyperparameters, but we're really focused on what the data tells us.
[00:29:35.780 --> 00:29:41.540]   And so we can quickly go back and within a few days, make considerable changes to the
[00:29:41.540 --> 00:29:42.660]   data we have.
[00:29:42.660 --> 00:29:47.300]   And then that's almost how we think about tuning our model and improving it.
[00:29:47.300 --> 00:29:52.100]   So we're taking a data first approach in terms of optimizing the performance of our vision
[00:29:52.100 --> 00:29:53.100]   models.
[00:29:53.100 --> 00:29:55.820]   And we do it for the benefit of the customer.
[00:29:55.820 --> 00:30:00.060]   We want to be able to show and prove to the customer that this data is valuable and that's
[00:30:00.060 --> 00:30:01.060]   useful.
[00:30:01.060 --> 00:30:06.820]   I mean, I think that'll resonate with a lot of the people that I've talked to.
[00:30:06.820 --> 00:30:10.820]   Most people in the real world tend to focus a lot on picking and choosing the data to
[00:30:10.820 --> 00:30:14.500]   make the models work well.
[00:30:14.500 --> 00:30:16.380]   Have your systems evolved for doing that?
[00:30:16.380 --> 00:30:17.940]   Obviously, it's weights and biases.
[00:30:17.940 --> 00:30:21.540]   I think that's how we got connected.
[00:30:21.540 --> 00:30:23.340]   What other tools do you use?
[00:30:23.340 --> 00:30:25.740]   So yeah, weights and biases is awesome.
[00:30:25.740 --> 00:30:26.740]   We love it.
[00:30:26.740 --> 00:30:30.260]   We've been using it a lot for understanding how our models are performing.
[00:30:30.260 --> 00:30:32.140]   But for us, we have our own data center.
[00:30:32.140 --> 00:30:34.100]   So we have our own cone location system.
[00:30:34.100 --> 00:30:37.780]   And then we use something called Polyaxon to orchestrate all of the experiments.
[00:30:37.780 --> 00:30:40.420]   So let's say you want to run 20 or 30 experiments.
[00:30:40.420 --> 00:30:45.560]   We have a system like Polyaxon that orchestrates all that, but it's also tied to 1dB.
[00:30:45.560 --> 00:30:49.220]   So we get all sorts of cool metrics and understand how the model is doing.
[00:30:49.220 --> 00:30:50.860]   And we can plot out a lot of stuff.
[00:30:50.860 --> 00:30:55.060]   We've also created our own customized dashboards to understand the difference between synthetic
[00:30:55.060 --> 00:30:57.140]   and real data.
[00:30:57.140 --> 00:31:00.580]   And there's some really cool things you can do with some of the new transformer architectures
[00:31:00.580 --> 00:31:05.300]   that can generate visual attention maps to understand some of the differences between
[00:31:05.300 --> 00:31:07.260]   the synthetic and real data.
[00:31:07.260 --> 00:31:11.340]   But at the end of the day, a lot of it is around that part of just trying to get the
[00:31:11.340 --> 00:31:12.420]   synthetic data.
[00:31:12.420 --> 00:31:15.440]   It's all focused on improve the synthetic data, improve the synthetic data.
[00:31:15.440 --> 00:31:17.980]   And then once we get it to a point, then we feel good.
[00:31:17.980 --> 00:31:20.340]   And then we can start doing crazy things with it.
[00:31:20.340 --> 00:31:26.460]   Like OK, this edge case that never happens in the real world, we'll create it.
[00:31:26.460 --> 00:31:30.820]   Or this perspective all of a sudden has changed and you need a whole new data set where the
[00:31:30.820 --> 00:31:34.020]   camera angle is now different because it's in a different place.
[00:31:34.020 --> 00:31:35.780]   Well, OK, we'll generate all that.
[00:31:35.780 --> 00:31:39.740]   So there's things like that that we also do a lot of.
[00:31:39.740 --> 00:31:42.180]   So it's a relationship we have.
[00:31:42.180 --> 00:31:44.500]   So those are roughly the tools.
[00:31:44.500 --> 00:31:49.500]   We're not like a huge startup where we have like 50 ML people, but it's a pretty nice
[00:31:49.500 --> 00:31:50.500]   pipeline.
[00:31:50.500 --> 00:31:53.700]   And also our data is all API driven.
[00:31:53.700 --> 00:31:59.680]   So we just literally a few lines of API code, we get the data we need, and then it's streamlined
[00:31:59.680 --> 00:32:02.340]   into this whole orchestration of experiments.
[00:32:02.340 --> 00:32:07.260]   And then once we get the performance of that, and then we have our weights and biases and
[00:32:07.260 --> 00:32:12.300]   dashboards and all these nice visualizations to understand where the differences are.
[00:32:12.300 --> 00:32:17.380]   Do you use TensorFlow or PyTorch or something else?
[00:32:17.380 --> 00:32:18.380]   We're PyTorch fans.
[00:32:18.380 --> 00:32:20.700]   Yeah, yeah, yeah, yeah.
[00:32:20.700 --> 00:32:25.820]   I mean, TensorFlow is great for production stuff, but PyTorch is just so nice in terms
[00:32:25.820 --> 00:32:27.940]   of debugging.
[00:32:27.940 --> 00:32:29.940]   And so there's a lot of stuff.
[00:32:29.940 --> 00:32:31.540]   And it's just also kind of a culture thing.
[00:32:31.540 --> 00:32:37.220]   You start off with PyTorch and then making the switch to TensorFlow is a little bit hard.
[00:32:37.220 --> 00:32:38.820]   But yeah, so most of our stuff is in PyTorch.
[00:32:38.820 --> 00:32:43.180]   So you started with PyTorch like four years ago when you started?
[00:32:43.180 --> 00:32:45.420]   No, no, no, no, no, no.
[00:32:45.420 --> 00:32:48.220]   Well keep in mind the first year is a little bit like...
[00:32:48.220 --> 00:32:49.220]   Yeah, sure.
[00:32:49.220 --> 00:32:53.100]   You know, swimming in the open ocean, trying to find the island.
[00:32:53.100 --> 00:32:56.580]   There's a little bit of that that happens, right?
[00:32:56.580 --> 00:32:59.380]   And of course, PyTorch has matured a bit over the years.
[00:32:59.380 --> 00:33:04.340]   So definitely there was a little bit of just trying to get the other stuff working.
[00:33:04.340 --> 00:33:08.260]   But yeah, so in the past year, year and a half, it's been PyTorch primarily.
[00:33:08.260 --> 00:33:09.260]   Cool.
[00:33:09.260 --> 00:33:10.420]   Well, thanks so much.
[00:33:10.420 --> 00:33:12.260]   This has been super interesting.
[00:33:12.260 --> 00:33:13.860]   We actually always end with two questions.
[00:33:13.860 --> 00:33:14.860]   Okay, sure.
[00:33:14.860 --> 00:33:16.460]   The first one I'll tell you.
[00:33:16.460 --> 00:33:21.580]   So what is one underrated aspect of machine learning that you think people should pay
[00:33:21.580 --> 00:33:23.500]   more attention to than they do?
[00:33:23.500 --> 00:33:27.860]   I mean, given that I work in synthetic data, I'm a bit biased here in my response.
[00:33:27.860 --> 00:33:33.400]   But I really think that as much time as you can spend on the architecture, sometimes it's
[00:33:33.400 --> 00:33:37.020]   really just the data that is an issue.
[00:33:37.020 --> 00:33:42.820]   And so I think people need to think more about the data and what that data looks like and
[00:33:42.820 --> 00:33:47.540]   understand what you're working with and the biases inherent in that data.
[00:33:47.540 --> 00:33:49.500]   So that's a big thing.
[00:33:49.500 --> 00:33:54.180]   When people come to you, do you feel like there's a common misconceptions they have
[00:33:54.180 --> 00:33:55.540]   about synthetic data?
[00:33:55.540 --> 00:33:58.140]   A lot of people are skeptical, right?
[00:33:58.140 --> 00:34:01.000]   A lot of people are skeptical, like, can this really work?
[00:34:01.000 --> 00:34:04.900]   So a lot of the work that we had to do early on was proving it out.
[00:34:04.900 --> 00:34:10.180]   But what's really cool is that over time, a lot of the clients we've had have been just
[00:34:10.180 --> 00:34:11.720]   coming back to us.
[00:34:11.720 --> 00:34:15.500]   So they've been coming back and they'll be like, okay, yeah, we need this thing and we
[00:34:15.500 --> 00:34:16.500]   need this iteration.
[00:34:16.500 --> 00:34:17.500]   This is cool.
[00:34:17.500 --> 00:34:18.500]   Can you make things roll now?
[00:34:18.500 --> 00:34:20.060]   Can you make things jump?
[00:34:20.060 --> 00:34:21.420]   Things like that.
[00:34:21.420 --> 00:34:23.780]   So it's been good in that sense.
[00:34:23.780 --> 00:34:26.940]   But early on, it was a lot of proving this out.
[00:34:26.940 --> 00:34:32.620]   And this is why we built this whole pipeline of benchmarking and showing how this works
[00:34:32.620 --> 00:34:33.620]   and how well it works.
[00:34:33.620 --> 00:34:38.740]   At the end of the day, they really just want to see metrics and show that this data set
[00:34:38.740 --> 00:34:40.060]   actually improves something.
[00:34:40.060 --> 00:34:41.060]   All right.
[00:34:41.060 --> 00:34:46.460]   So the final question, what's been the biggest technical challenge that you faced making
[00:34:46.460 --> 00:34:49.100]   synthetic data work?
[00:34:49.100 --> 00:34:51.540]   It's an enormous engineering challenge.
[00:34:51.540 --> 00:35:02.020]   So if you're trying to create large worlds, it requires a lot of optimization of huge
[00:35:02.020 --> 00:35:05.760]   amounts of data, which is not a trivial thing.
[00:35:05.760 --> 00:35:10.820]   And you have to organize it in a way that is also modular so that you can swap this
[00:35:10.820 --> 00:35:13.160]   and that and create that diversity.
[00:35:13.160 --> 00:35:17.320]   So the hardest technical challenge was how do you scale diversity, which is what you
[00:35:17.320 --> 00:35:19.880]   need to do with synthetic data.
[00:35:19.880 --> 00:35:24.160]   That is definitely the hardest part.
[00:35:24.160 --> 00:35:29.680]   And of course, to the point about trying to create an early technology and trying to create
[00:35:29.680 --> 00:35:35.120]   something that where early on the market, like I remember going to investors and they're
[00:35:35.120 --> 00:35:36.120]   like, "What's synthetic data?
[00:35:36.120 --> 00:35:37.120]   Like what?"
[00:35:37.120 --> 00:35:38.680]   Like they didn't even understand the concepts.
[00:35:38.680 --> 00:35:40.920]   There's so many things we have to explain.
[00:35:40.920 --> 00:35:44.440]   And so there was definitely, but it's now changing.
[00:35:44.440 --> 00:35:46.160]   And so that we're super excited about it.
[00:35:46.160 --> 00:35:53.560]   But I would say creating a system that allows you to scale diversity in a simulation environment
[00:35:53.560 --> 00:35:55.920]   is a significant challenge.
[00:35:55.920 --> 00:36:01.560]   And how do you do that in a way that is controllable versus just, you can throw adversarial networks
[00:36:01.560 --> 00:36:02.880]   at it and things like that.
[00:36:02.880 --> 00:36:07.440]   And that might help you to some degree, but at the end of the day, I do think that there
[00:36:07.440 --> 00:36:13.760]   is a role for control that is more driven by people in terms of how these simulations
[00:36:13.760 --> 00:36:14.760]   work.
[00:36:14.760 --> 00:36:18.640]   So there's something there, but it's not to exclude the adversarial stuff is really important
[00:36:18.640 --> 00:36:20.360]   and will play a role in the future.
[00:36:20.360 --> 00:36:22.920]   But I think you have front row seats to this.
[00:36:22.920 --> 00:36:26.480]   I mean, I would trust your assessment after.
[00:36:26.480 --> 00:36:27.480]   Yeah.
[00:36:27.480 --> 00:36:28.480]   Yeah.
[00:36:28.480 --> 00:36:29.480]   That's super.
[00:36:29.480 --> 00:36:32.800]   I mean, trust me, I would love to just throw an adversarial algorithm and generate that
[00:36:32.800 --> 00:36:36.000]   everything, but I wish it was that easy.
[00:36:36.000 --> 00:36:37.640]   But it's unfortunately not, I think.
[00:36:37.640 --> 00:36:38.640]   So yeah.
[00:36:38.640 --> 00:36:39.640]   Awesome.
[00:36:39.640 --> 00:36:40.640]   Well, thanks so much for your time.
[00:36:40.640 --> 00:36:41.640]   This is super fun.
[00:36:41.640 --> 00:36:42.640]   Absolutely.
[00:36:42.640 --> 00:36:43.640]   Thank you.
[00:36:43.640 --> 00:36:44.640]   I really appreciate it.
[00:36:44.640 --> 00:36:48.080]   When we first started making these videos, we didn't know if anyone would be interested
[00:36:48.080 --> 00:36:51.160]   or want to see them, but we made them for fun.
[00:36:51.160 --> 00:36:54.140]   And we started off by making videos that would teach people.
[00:36:54.140 --> 00:36:58.080]   And now we get these great interviews with real industry practitioners.
[00:36:58.080 --> 00:37:01.760]   And I love making this available to the whole world so everyone can watch these things for
[00:37:01.760 --> 00:37:02.760]   free.
[00:37:02.760 --> 00:37:05.120]   And the more feedback you give us, the better stuff we can produce.
[00:37:05.120 --> 00:37:08.040]   So please subscribe, leave a comment, engage with us.
[00:37:08.040 --> 00:37:08.920]   We really appreciate it.

