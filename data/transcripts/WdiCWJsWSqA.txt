
[00:00:00.000 --> 00:00:03.640]   by basically talking to a bunch of folks, building stuff with language models and
[00:00:03.640 --> 00:00:06.280]   noticing that there was a lot of common abstractions in terms of what they were
[00:00:06.280 --> 00:00:09.860]   doing in terms of constructing prompts, in terms of calling out to language models,
[00:00:09.860 --> 00:00:13.040]   in terms of calling out to embeddings, in terms of calling out to vector stores.
[00:00:13.040 --> 00:00:16.500]   So for the example of document question answering, the motivation for doing this
[00:00:16.500 --> 00:00:19.380]   is that you might, you want to have a chat experience over your documents.
[00:00:19.380 --> 00:00:23.940]   So chat GPT, but it knows specific contents of your documents or of particular
[00:00:23.940 --> 00:00:25.320]   documents that it wasn't trained on.
[00:00:25.320 --> 00:00:29.000]   And so what that general process looks like is you'll get the user question
[00:00:29.020 --> 00:00:31.200]   that comes in based on that user question.
[00:00:31.200 --> 00:00:33.160]   You'll then look up relevant documents.
[00:00:33.160 --> 00:00:37.300]   So there's this flow of going from question to embedding to documents that
[00:00:37.300 --> 00:00:40.840]   you get back to constructed prompt, to calling the language model, to getting a
[00:00:40.840 --> 00:00:41.360]   response.
[00:00:41.360 --> 00:00:45.120]   And so all of these components are individual kind of like modules and
[00:00:45.120 --> 00:00:47.960]   link chain that you can string together and swap out really easily.
[00:00:47.960 --> 00:00:49.960]   So you can swap out the embedding model that you use.
[00:00:49.960 --> 00:00:51.880]   You can swap out the language model that you use.
[00:00:51.880 --> 00:00:55.780]   So the idea is to make it really easy to have these building blocks, swap them out
[00:00:55.780 --> 00:00:58.540]   for each other, assemble them, and then have some pre-run templates to get

