
[00:00:00.000 --> 00:00:03.600]   The following is a conversation with Lisa Feldman Barrett,
[00:00:03.600 --> 00:00:05.740]   her second time on the podcast.
[00:00:05.740 --> 00:00:09.060]   She's a neuroscientist at Northeastern University
[00:00:09.060 --> 00:00:11.240]   and one of my favorite people.
[00:00:11.240 --> 00:00:13.400]   Her new book called "Seven and a Half Lessons
[00:00:13.400 --> 00:00:16.760]   "About the Brain" is out now as of a couple days ago,
[00:00:16.760 --> 00:00:19.520]   so you should definitely support Lisa by buying it
[00:00:19.520 --> 00:00:21.800]   and sharing with friends if you like it.
[00:00:21.800 --> 00:00:25.460]   It's a great short intro to the human brain.
[00:00:25.460 --> 00:00:27.040]   Quick mention of each sponsor,
[00:00:27.040 --> 00:00:29.880]   followed by some thoughts related to the episode.
[00:00:29.880 --> 00:00:32.080]   Athleta Greens, the all-in-one drink
[00:00:32.080 --> 00:00:33.960]   that I start every day with
[00:00:33.960 --> 00:00:36.520]   to cover all my nutritional bases.
[00:00:36.520 --> 00:00:39.480]   8 Sleep, a mattress that cools itself
[00:00:39.480 --> 00:00:42.360]   and gives me yet another reason to enjoy sleep.
[00:00:42.360 --> 00:00:45.160]   Masterclass, online courses that I enjoy
[00:00:45.160 --> 00:00:48.660]   from some of the most amazing people in history.
[00:00:48.660 --> 00:00:52.680]   And BetterHelp, online therapy with a licensed professional.
[00:00:52.680 --> 00:00:55.040]   Please check out these sponsors in the description
[00:00:55.040 --> 00:00:58.040]   to get a discount and to support this podcast.
[00:00:58.040 --> 00:01:00.200]   As a side note, let me say that Lisa,
[00:01:00.200 --> 00:01:02.380]   just like Manolis Kellis,
[00:01:02.380 --> 00:01:05.360]   is a local brilliant mind and friend
[00:01:05.360 --> 00:01:08.160]   and someone I can see talking to many more times.
[00:01:08.160 --> 00:01:10.760]   Sometimes it's fun to talk to a scientist
[00:01:10.760 --> 00:01:12.620]   not just about their field of expertise,
[00:01:12.620 --> 00:01:15.880]   but also about random topics, even silly ones,
[00:01:15.880 --> 00:01:19.200]   from love to music to philosophy.
[00:01:19.200 --> 00:01:21.520]   Ultimately, it's about having fun,
[00:01:21.520 --> 00:01:23.800]   something I know nothing about.
[00:01:23.800 --> 00:01:25.840]   This conversation is certainly that.
[00:01:25.840 --> 00:01:28.760]   It may not always work, but it's worth a shot.
[00:01:28.760 --> 00:01:31.040]   I think it's valuable to alternate
[00:01:31.040 --> 00:01:32.980]   along all kinds of dimensions,
[00:01:32.980 --> 00:01:35.720]   like between deeper technical discussions
[00:01:35.720 --> 00:01:37.920]   and more fun random discussion,
[00:01:37.920 --> 00:01:40.760]   from liberal thinker to conservative thinker,
[00:01:40.760 --> 00:01:43.040]   from musician to athlete,
[00:01:43.040 --> 00:01:45.920]   from CEO to junior engineer,
[00:01:45.920 --> 00:01:48.020]   from friend to stranger.
[00:01:48.020 --> 00:01:51.480]   Variety makes life and conversation more interesting.
[00:01:51.480 --> 00:01:54.840]   Let's see where this little podcast journey goes.
[00:01:54.840 --> 00:01:57.040]   If you enjoy this thing, subscribe on YouTube,
[00:01:57.040 --> 00:01:59.280]   review it with Five Stars on Apple Podcasts,
[00:01:59.280 --> 00:02:02.000]   follow on Spotify, support it on Patreon,
[00:02:02.000 --> 00:02:05.240]   or connect with me on Twitter @LexFriedman.
[00:02:05.240 --> 00:02:10.000]   And now, here's my conversation with Lisa Feldman Barrett.
[00:02:10.000 --> 00:02:13.800]   Based on the comments in our previous conversation,
[00:02:13.800 --> 00:02:17.040]   I think a lot of people would be very disappointed,
[00:02:17.040 --> 00:02:20.920]   I should say, to learn that you are, in fact, married.
[00:02:20.920 --> 00:02:22.960]   As they say, all the good ones are taken.
[00:02:22.960 --> 00:02:27.960]   Okay, so I'm a fan of your husband as well, Dan.
[00:02:27.960 --> 00:02:29.760]   He's a programmer, a musician,
[00:02:29.760 --> 00:02:32.080]   so a man after my own heart.
[00:02:32.080 --> 00:02:36.160]   Can I ask a ridiculously over-romanticized question
[00:02:36.160 --> 00:02:40.960]   of when did you first fall in love with Dan?
[00:02:40.960 --> 00:02:45.380]   - It's actually, it's a really romantic story, I think.
[00:02:45.380 --> 00:02:50.380]   So I was divorced by the time I was 26, 27, 26, I guess.
[00:02:50.740 --> 00:02:52.660]   And I was in my first academic job,
[00:02:52.660 --> 00:02:54.940]   which was Penn State University,
[00:02:54.940 --> 00:02:57.540]   which is in the middle of Pennsylvania,
[00:02:57.540 --> 00:02:58.700]   surrounded by mountains.
[00:02:58.700 --> 00:03:00.820]   So you have, it's four hours to get anywhere,
[00:03:00.820 --> 00:03:03.540]   to get to Philadelphia, New York, Washington.
[00:03:03.540 --> 00:03:05.580]   I mean, you're basically stuck, you know?
[00:03:05.580 --> 00:03:10.060]   And I was very fortunate to have
[00:03:10.060 --> 00:03:11.980]   a lot of other assistant professors
[00:03:11.980 --> 00:03:13.900]   who were hired at the same time as I was.
[00:03:13.900 --> 00:03:15.900]   So there were a lot of us, we were all friends,
[00:03:15.900 --> 00:03:17.620]   which was really fun.
[00:03:17.620 --> 00:03:21.420]   But I was single, and I didn't wanna date a student.
[00:03:21.420 --> 00:03:24.580]   And there were no,
[00:03:24.580 --> 00:03:26.780]   and I wasn't gonna date somebody in my department,
[00:03:26.780 --> 00:03:29.220]   that's just a recipe for disaster.
[00:03:29.220 --> 00:03:30.060]   - Yeah.
[00:03:30.060 --> 00:03:30.900]   - So--
[00:03:30.900 --> 00:03:32.540]   - But even at 20, whatever you were,
[00:03:32.540 --> 00:03:34.300]   you were already wise enough to know that.
[00:03:34.300 --> 00:03:36.460]   - Yeah, a little bit, maybe, yeah.
[00:03:36.460 --> 00:03:38.180]   I wouldn't call me wise at that age.
[00:03:38.180 --> 00:03:41.180]   But anyways, not sure that I would say that I'm wise now.
[00:03:41.180 --> 00:03:44.940]   But, and so,
[00:03:44.940 --> 00:03:49.940]   and so, after, you know,
[00:03:49.940 --> 00:03:53.740]   I was spending probably 16 hours a day in the lab
[00:03:53.740 --> 00:03:55.700]   because it was my first year,
[00:03:55.700 --> 00:03:58.820]   and as an assistant professor, and there's a lot to do.
[00:03:58.820 --> 00:04:02.660]   And I was also bitching and moaning to my friends
[00:04:02.660 --> 00:04:06.380]   that I hadn't had sex in I don't know how many months.
[00:04:06.380 --> 00:04:10.580]   And I was starting to become unhappy with my life.
[00:04:10.580 --> 00:04:13.060]   And I think at a certain point,
[00:04:13.060 --> 00:04:15.580]   they just got tired of listening to me bitch and moan
[00:04:15.580 --> 00:04:18.300]   and said, "Just do something about it then,
[00:04:18.300 --> 00:04:20.380]   "like do, you know, if you're unhappy."
[00:04:20.380 --> 00:04:23.740]   And so the first thing I did was I made friends
[00:04:23.740 --> 00:04:25.740]   with a sushi chef in town.
[00:04:25.740 --> 00:04:28.860]   And this is like a State College, Pennsylvania
[00:04:28.860 --> 00:04:32.140]   in the early '90s was there was like a pizza shop
[00:04:32.140 --> 00:04:36.260]   and a sub shop and actually a very good bagel shop
[00:04:36.260 --> 00:04:39.100]   and one good coffee shop and maybe one nice restaurant.
[00:04:39.100 --> 00:04:40.140]   I mean, there was really,
[00:04:40.180 --> 00:04:44.460]   but there was the second son of a Japanese sushi chef
[00:04:44.460 --> 00:04:47.220]   who was not going to inherit the restaurant.
[00:04:47.220 --> 00:04:51.220]   And so he moved to Pennsylvania and was giving sushi lessons.
[00:04:51.220 --> 00:04:54.260]   So I met this guy, the sushi chef,
[00:04:54.260 --> 00:04:57.420]   and we decided to throw a sushi party at the coffee shop.
[00:04:57.420 --> 00:04:59.700]   So we basically, it was the goal was to invite
[00:04:59.700 --> 00:05:04.700]   every eligible bachelor really within like a 20 mile radius.
[00:05:04.700 --> 00:05:07.020]   We had a totally fun time.
[00:05:07.020 --> 00:05:11.060]   I wore an awesome crushed velvet burgundy dress,
[00:05:11.060 --> 00:05:12.540]   it was beautiful dress.
[00:05:12.540 --> 00:05:16.540]   And I didn't meet any, I met a lot of new friends,
[00:05:16.540 --> 00:05:17.740]   but I did not meet anybody.
[00:05:17.740 --> 00:05:18.700]   So then I thought, okay, well,
[00:05:18.700 --> 00:05:20.940]   maybe I'll try the personals ads,
[00:05:20.940 --> 00:05:23.420]   which I had never used before in my life.
[00:05:23.420 --> 00:05:28.420]   And I first tried the paper personals ads.
[00:05:28.420 --> 00:05:30.020]   - Like in the newspaper?
[00:05:30.020 --> 00:05:33.020]   - Like in the newspaper, that didn't work.
[00:05:33.020 --> 00:05:33.860]   And then a friend of mine said,
[00:05:33.860 --> 00:05:36.620]   "Oh, you know, there's this thing called net news."
[00:05:36.620 --> 00:05:40.140]   This is like 1992 maybe.
[00:05:40.140 --> 00:05:43.820]   So there was this anonymous, you could do it anonymously.
[00:05:43.820 --> 00:05:48.820]   So you would read, you could post or you could read ads
[00:05:48.820 --> 00:05:54.900]   and then respond to an address, which was anonymous.
[00:05:54.900 --> 00:05:57.540]   And that was yoked to somebody's real address.
[00:05:57.540 --> 00:06:01.020]   And there was always a lag
[00:06:01.020 --> 00:06:04.860]   because it was this like a bulletin board sort of thing.
[00:06:04.860 --> 00:06:08.900]   So at first I read them over
[00:06:08.900 --> 00:06:12.700]   and I decided to respond to one or two.
[00:06:12.700 --> 00:06:15.300]   And it was interesting.
[00:06:15.300 --> 00:06:16.980]   - Sorry, this is not on the internet.
[00:06:16.980 --> 00:06:18.300]   - Yeah, this is totally on the internet.
[00:06:18.300 --> 00:06:20.180]   - But it takes, there's a delay of a couple of days
[00:06:20.180 --> 00:06:21.020]   or whatever.
[00:06:21.020 --> 00:06:21.860]   - Yeah, right, right.
[00:06:21.860 --> 00:06:23.060]   It's 1992.
[00:06:23.060 --> 00:06:24.180]   There's no web.
[00:06:24.180 --> 00:06:25.180]   - No pictures.
[00:06:25.180 --> 00:06:26.020]   - There's no pictures.
[00:06:26.020 --> 00:06:27.140]   The web doesn't exist.
[00:06:27.140 --> 00:06:29.780]   It's all done in ASCII format sort of.
[00:06:29.780 --> 00:06:32.300]   But the ratio--
[00:06:32.300 --> 00:06:33.260]   - Loving ASCII.
[00:06:33.300 --> 00:06:38.300]   - But the ratio of men to women was like 10 to one.
[00:06:38.300 --> 00:06:39.860]   I mean, there were many more men
[00:06:39.860 --> 00:06:42.900]   because it was basically academics and the government.
[00:06:42.900 --> 00:06:43.740]   That was it.
[00:06:43.740 --> 00:06:46.060]   That was no, I mean, I think AOL maybe
[00:06:46.060 --> 00:06:47.980]   was just starting to become popular.
[00:06:47.980 --> 00:06:55.700]   And so the first person I met told me
[00:06:55.700 --> 00:07:00.060]   that he was a scientist who worked for NASA.
[00:07:00.060 --> 00:07:00.900]   And--
[00:07:00.900 --> 00:07:01.740]   - Impressive.
[00:07:01.740 --> 00:07:02.580]   - Yeah.
[00:07:03.420 --> 00:07:06.700]   Anyways, it turned out that he didn't actually.
[00:07:06.700 --> 00:07:07.540]   - Yeah.
[00:07:07.540 --> 00:07:11.220]   This is how they brag is you elevate your,
[00:07:11.220 --> 00:07:12.820]   as opposed to saying you're taller than you are,
[00:07:12.820 --> 00:07:14.020]   you say like your position is higher.
[00:07:14.020 --> 00:07:16.740]   - Yeah, and I actually, I would have been fine
[00:07:16.740 --> 00:07:18.380]   dating somebody who wasn't a scientist.
[00:07:18.380 --> 00:07:21.820]   It's just that they have, it's just that whoever I date
[00:07:21.820 --> 00:07:26.820]   has to just accept that I am and that I was pretty ambitious
[00:07:26.820 --> 00:07:30.340]   and was trying to make my career.
[00:07:30.340 --> 00:07:34.820]   And that's not, I think it's maybe more common now
[00:07:34.820 --> 00:07:38.540]   for men to maybe accept that in their female partners,
[00:07:38.540 --> 00:07:40.500]   but at that time, not so common.
[00:07:40.500 --> 00:07:41.940]   - Could be intimidating, I guess.
[00:07:41.940 --> 00:07:44.140]   - Yes, that has been said.
[00:07:44.140 --> 00:07:49.140]   And so then the next one I actually corresponded with,
[00:07:49.140 --> 00:07:51.660]   and we actually got to the point of talking on the phone
[00:07:51.660 --> 00:07:53.900]   and we had this really kind of funny conversation
[00:07:53.900 --> 00:07:57.540]   where we're chatting and he said,
[00:07:57.540 --> 00:08:00.980]   he introduces the idea that he's really looking
[00:08:00.980 --> 00:08:03.780]   for a dominant woman and I'm thinking,
[00:08:03.780 --> 00:08:05.260]   I'm a psychologist by training,
[00:08:05.260 --> 00:08:07.140]   so I'm thinking, oh, he means sex roles.
[00:08:07.140 --> 00:08:09.020]   Like, I'm like, no, I'm very assertive
[00:08:09.020 --> 00:08:10.380]   and I'm glad you think that, you know, okay.
[00:08:10.380 --> 00:08:13.980]   Anyways, long story short, that's not really what he meant.
[00:08:13.980 --> 00:08:16.380]   (laughing)
[00:08:16.380 --> 00:08:17.220]   - Okay, got it.
[00:08:17.220 --> 00:08:20.420]   - Yeah, so, and I just, you know,
[00:08:20.420 --> 00:08:22.340]   that will just show you my level of naivete.
[00:08:22.340 --> 00:08:24.620]   Like I was like, I didn't completely understand,
[00:08:24.620 --> 00:08:27.500]   but I was like, well, yeah, you know, no.
[00:08:27.500 --> 00:08:29.260]   At one point he asked me how I felt
[00:08:29.260 --> 00:08:33.540]   about him wearing my lingerie and I was like,
[00:08:33.540 --> 00:08:35.700]   I don't even share my lingerie with my sister.
[00:08:35.700 --> 00:08:39.620]   Like, I don't share my lingerie with anybody, you know?
[00:08:39.620 --> 00:08:40.820]   No.
[00:08:40.820 --> 00:08:45.300]   The third one I interacted with was a banker
[00:08:45.300 --> 00:08:50.300]   who lived in Singapore and that conversation
[00:08:50.300 --> 00:08:55.180]   didn't last very long because he made an,
[00:08:55.180 --> 00:08:57.740]   I guess he made an analogy between me
[00:08:57.740 --> 00:09:01.740]   and a character in "The Fountainhead,"
[00:09:01.740 --> 00:09:06.220]   the woman who's raped in "The Fountainhead,"
[00:09:06.220 --> 00:09:08.660]   and I was like, okay, that's not.
[00:09:08.660 --> 00:09:10.220]   - That's not a good-- - That's not a good,
[00:09:10.220 --> 00:09:11.500]   no, that's not a good one.
[00:09:11.500 --> 00:09:12.980]   - Not that part, not that scene.
[00:09:12.980 --> 00:09:13.820]   - Not that scene.
[00:09:13.820 --> 00:09:16.260]   So then I was like, okay, you know what?
[00:09:16.260 --> 00:09:17.780]   I'm gonna post my own ad.
[00:09:17.780 --> 00:09:20.140]   And so I did, I posted, well, first I wrote my ad
[00:09:20.140 --> 00:09:22.620]   and then I, of course, I checked it with my friends
[00:09:22.620 --> 00:09:24.740]   who were all also assistant professors
[00:09:24.740 --> 00:09:26.460]   who are like my little Greek chorus,
[00:09:26.460 --> 00:09:30.200]   and then I posted it and I got something like,
[00:09:30.200 --> 00:09:34.300]   I don't know, 80-something responses in 24 hours.
[00:09:34.300 --> 00:09:35.140]   I mean, it was very--
[00:09:35.140 --> 00:09:36.540]   - Do you remember the pitch?
[00:09:36.540 --> 00:09:40.940]   Like how you, I guess, condensed yourself?
[00:09:40.940 --> 00:09:43.900]   - I don't remember it exactly, although Dan has it.
[00:09:43.900 --> 00:09:48.100]   But actually for our 20th wedding anniversary,
[00:09:48.100 --> 00:09:51.660]   he took our exchanges and he printed them off
[00:09:51.660 --> 00:09:54.660]   and put them in a leather-bound book for us to read,
[00:09:54.660 --> 00:09:55.900]   which was really sweet.
[00:09:55.900 --> 00:09:58.740]   Yeah, I think I was just really direct.
[00:09:58.740 --> 00:10:01.260]   Like I'm almost 30, I'm a scientist,
[00:10:01.260 --> 00:10:02.820]   I'm not looking to, you know,
[00:10:02.820 --> 00:10:04.980]   I'm looking for something serious and, you know.
[00:10:04.980 --> 00:10:09.020]   But the thing is I forgot to say where my location was
[00:10:09.020 --> 00:10:12.020]   and my age, which I forgot.
[00:10:12.020 --> 00:10:14.700]   So I got lots of, I mean, I will say,
[00:10:14.700 --> 00:10:17.900]   so I printed off all of the responses
[00:10:17.900 --> 00:10:21.620]   and I had all my friends over and we were, you know,
[00:10:21.620 --> 00:10:24.520]   had a big, I made a big pot of gumbo
[00:10:24.520 --> 00:10:27.300]   and we drank through several bottles of wine
[00:10:27.300 --> 00:10:28.660]   reading these responses.
[00:10:28.660 --> 00:10:32.500]   And I would say for the most part, they were really sweet,
[00:10:32.500 --> 00:10:35.220]   like earnest and genuine,
[00:10:35.220 --> 00:10:37.380]   as much as you could tell that somebody's being genuine.
[00:10:37.380 --> 00:10:38.420]   I mean, it seemed, you know,
[00:10:38.420 --> 00:10:40.340]   there were a couple of really funky ones,
[00:10:40.340 --> 00:10:42.500]   like, you know, this one couple who told me
[00:10:42.500 --> 00:10:44.500]   that I was their soulmate, the two of them,
[00:10:44.500 --> 00:10:47.180]   then they were looking for, you know, a third person
[00:10:47.180 --> 00:10:48.780]   and I was like, "Oh, okay."
[00:10:48.780 --> 00:10:53.780]   But mostly super, seemed like super genuine people.
[00:10:54.020 --> 00:10:57.640]   And so I chose five men to start corresponding with
[00:10:57.640 --> 00:10:58.940]   and I was corresponding with them.
[00:10:58.940 --> 00:11:03.200]   And then about a week later, I get this other email.
[00:11:03.200 --> 00:11:05.040]   And okay, and then I post something the next day
[00:11:05.040 --> 00:11:06.800]   that said, "Okay, you know, thank you so much."
[00:11:06.800 --> 00:11:10.360]   And I'm gonna, I answered every person back.
[00:11:10.360 --> 00:11:11.400]   But then after that I said,
[00:11:11.400 --> 00:11:13.400]   "Okay, and I'm not gonna answer anymore."
[00:11:13.400 --> 00:11:15.120]   You know, 'cause it was, they were still coming in
[00:11:15.120 --> 00:11:16.800]   and I couldn't, you know, I have a job
[00:11:16.800 --> 00:11:18.760]   and, you know, a house to take care of and stuff.
[00:11:18.760 --> 00:11:22.720]   So, and then about a week later, I get this other email
[00:11:22.720 --> 00:11:27.500]   and he says, you know, he just describes himself
[00:11:27.500 --> 00:11:30.260]   like I'm this, I'm this, I'm this, I'm a chef,
[00:11:30.260 --> 00:11:32.540]   I'm a scientist, I'm a this, I'm a this.
[00:11:32.540 --> 00:11:35.900]   And so I emailed him back and I said,
[00:11:35.900 --> 00:11:37.540]   "You know, you seem interesting.
[00:11:37.540 --> 00:11:40.020]   "You can write me at my actual address if you want.
[00:11:40.020 --> 00:11:40.860]   "Here's my address.
[00:11:40.860 --> 00:11:42.100]   "I'm not really responding,
[00:11:42.100 --> 00:11:43.660]   "I'm not really responding to other people anymore,
[00:11:43.660 --> 00:11:44.740]   "but you seem interesting.
[00:11:44.740 --> 00:11:46.860]   "You know, you can write to me if you want."
[00:11:46.860 --> 00:11:50.760]   And then he wrote to me and I,
[00:11:51.780 --> 00:11:52.760]   then I wrote him back and I,
[00:11:52.760 --> 00:11:55.600]   it was a nondescript kind of email and I wrote him back
[00:11:55.600 --> 00:11:56.800]   and I said, "Thanks for responding.
[00:11:56.800 --> 00:11:58.760]   "You know, I'm really busy right now.
[00:11:58.760 --> 00:12:01.260]   "I was in the middle of writing my first slate
[00:12:01.260 --> 00:12:04.520]   "of grant applications, so I was really consumed."
[00:12:04.520 --> 00:12:06.920]   And I said, "I'll get back to you in a couple of days."
[00:12:06.920 --> 00:12:09.240]   And so I did, I waited a couple of days
[00:12:09.240 --> 00:12:11.240]   till my grants were, you know, safe,
[00:12:11.240 --> 00:12:13.400]   grant application safely out the door.
[00:12:13.400 --> 00:12:16.920]   And then I emailed him back and then he emailed me
[00:12:16.920 --> 00:12:21.500]   and then really across two days, we sent 100 emails.
[00:12:21.500 --> 00:12:23.740]   - And text only?
[00:12:23.740 --> 00:12:25.220]   Was there pictures or anything of that?
[00:12:25.220 --> 00:12:27.500]   - Text only, text only.
[00:12:27.500 --> 00:12:30.740]   And then, so this was like a Thursday and a Friday.
[00:12:30.740 --> 00:12:33.740]   And then Friday, he said,
[00:12:33.740 --> 00:12:35.620]   "Let's talk on the weekend on the phone."
[00:12:35.620 --> 00:12:37.040]   And I said, "Okay."
[00:12:37.040 --> 00:12:39.820]   And he wanted to talk Sunday night
[00:12:39.820 --> 00:12:42.460]   and I had a date Sunday night.
[00:12:42.460 --> 00:12:46.780]   So I said, "Okay, sure, we can talk Sunday night."
[00:12:46.780 --> 00:12:49.340]   And then I was like, "Well, you know,
[00:12:49.340 --> 00:12:50.600]   "I don't really wanna cancel my date,
[00:12:50.600 --> 00:12:52.400]   "so I'm just gonna call him on Saturday."
[00:12:52.400 --> 00:12:55.440]   So I just called, I cold called him on Saturday
[00:12:55.440 --> 00:12:57.240]   and a woman answered.
[00:12:57.240 --> 00:12:58.080]   - Oh, wow.
[00:12:58.080 --> 00:12:59.960]   That's not cool.
[00:12:59.960 --> 00:13:01.640]   - Not cool.
[00:13:01.640 --> 00:13:04.520]   And so she says, you know, "Hello."
[00:13:04.520 --> 00:13:06.880]   And I say, "Oh, you know, stand there."
[00:13:06.880 --> 00:13:09.760]   And she said, "Sure, can I ask who's calling?"
[00:13:09.760 --> 00:13:11.720]   And I said, "Tell him it's Lisa."
[00:13:11.720 --> 00:13:14.400]   And she went, "Oh my God, oh my God, I'm just a friend.
[00:13:14.400 --> 00:13:16.180]   "I'm just a friend, I just need to tell you,
[00:13:16.180 --> 00:13:17.760]   "I'm just a friend."
[00:13:17.760 --> 00:13:21.760]   And I was like, this is adorable, right?
[00:13:21.760 --> 00:13:24.480]   And then he gets on the phone, not high, nice to meet.
[00:13:24.480 --> 00:13:27.400]   The first thing he says to me, "She's just a friend."
[00:13:27.400 --> 00:13:32.400]   So I was just so charmed, really, by the whole thing.
[00:13:32.400 --> 00:13:37.480]   So it was Yom Kippur, it was the Jewish Day of Atonement
[00:13:37.480 --> 00:13:39.120]   that was ending and they were baking cookies
[00:13:39.120 --> 00:13:40.240]   and going to a break fast.
[00:13:40.240 --> 00:13:42.480]   So people, as you know, fast all day
[00:13:42.480 --> 00:13:44.840]   and then they go to a party and they break fast.
[00:13:44.840 --> 00:13:49.840]   So I thought, okay, I'll just cancel my date.
[00:13:49.840 --> 00:13:55.000]   So I did and I stayed home and we talked for eight hours
[00:13:55.000 --> 00:13:58.600]   and then the next night for six hours.
[00:13:58.600 --> 00:14:00.440]   And it basically, it just went on like that.
[00:14:00.440 --> 00:14:05.440]   And then by the end of the week, he flew to State College.
[00:14:05.440 --> 00:14:09.560]   And we'd gone through this whole thing where I'd said,
[00:14:09.560 --> 00:14:10.840]   we're gonna take it slow,
[00:14:10.840 --> 00:14:13.160]   we're gonna get to know each other.
[00:14:13.160 --> 00:14:15.600]   And then really by, I think we talked like two
[00:14:15.600 --> 00:14:18.320]   or three times, these like really long conversations.
[00:14:18.320 --> 00:14:20.440]   And then he said, "I'm just gonna fly there."
[00:14:20.440 --> 00:14:23.080]   And then, so of course there's,
[00:14:23.080 --> 00:14:26.760]   I don't even know that there were fax machines
[00:14:26.760 --> 00:14:31.280]   at that point, maybe there were, but I don't think so.
[00:14:31.280 --> 00:14:34.320]   Anyway, so we decided we'll exchange pictures.
[00:14:34.320 --> 00:14:37.980]   And so he, I take my photograph
[00:14:37.980 --> 00:14:40.780]   and I give it to my secretary and I say to my secretary.
[00:14:40.780 --> 00:14:42.720]   - Fax this.
[00:14:42.720 --> 00:14:45.200]   - I say that, send this priority mail.
[00:14:45.200 --> 00:14:46.040]   - Priority mail.
[00:14:46.040 --> 00:14:47.400]   - And he goes, okay, I'll send a priority mail.
[00:14:47.400 --> 00:14:48.480]   Let me, it's a priority mail.
[00:14:48.480 --> 00:14:50.440]   He's like, I know, priority mail, okay.
[00:14:50.440 --> 00:14:54.800]   And then, so I get Dan's photograph in the mail
[00:14:54.800 --> 00:15:00.360]   and it's him in shorts and you can see
[00:15:00.360 --> 00:15:02.680]   that he's probably somewhere like the Bahamas
[00:15:02.680 --> 00:15:05.080]   or something like that and it's like cropped.
[00:15:05.080 --> 00:15:07.480]   So clearly what he's done is he's taken a photograph
[00:15:07.480 --> 00:15:10.340]   where he's in it with someone else
[00:15:10.340 --> 00:15:11.940]   who turned out to be his ex-wife.
[00:15:11.940 --> 00:15:14.420]   So I'm thinking, well, this is awesome.
[00:15:14.420 --> 00:15:15.540]   I've hit the jackpot.
[00:15:15.540 --> 00:15:18.700]   He's very appealing to me, very attractive.
[00:15:18.700 --> 00:15:23.100]   And then my photograph doesn't show up
[00:15:23.100 --> 00:15:24.380]   and it doesn't show up.
[00:15:24.380 --> 00:15:27.380]   And so like one day and then two days
[00:15:27.380 --> 00:15:30.740]   and then he's like, I said,
[00:15:30.740 --> 00:15:34.180]   well, I asked my secretary to send a priority.
[00:15:34.180 --> 00:15:37.260]   I mean, I don't know what he did.
[00:15:37.260 --> 00:15:40.980]   And he's like, I said, I'm like, well, you don't have to,
[00:15:40.980 --> 00:15:41.940]   you know, you don't have to come.
[00:15:41.940 --> 00:15:43.640]   And he's like, no, no, no, I'm gonna, you know,
[00:15:43.640 --> 00:15:45.440]   we've had like five dates,
[00:15:45.440 --> 00:15:47.540]   the equivalent of five dates practically.
[00:15:47.540 --> 00:15:52.380]   And then, so he's supposed to fly on a Thursday or Friday,
[00:15:52.380 --> 00:15:53.580]   I can't remember.
[00:15:53.580 --> 00:15:56.400]   And I get a call like maybe an hour
[00:15:56.400 --> 00:15:58.140]   before his flight's supposed to leave.
[00:15:58.140 --> 00:15:59.040]   And he says, hi.
[00:15:59.040 --> 00:16:00.980]   And I say, and it's just something in his voice, right?
[00:16:00.980 --> 00:16:02.360]   And I say, 'cause at this point,
[00:16:02.360 --> 00:16:05.440]   I think I've talked to him like for 25 hours, I don't know.
[00:16:05.440 --> 00:16:07.300]   And he says, hi.
[00:16:07.300 --> 00:16:09.280]   And I'm like, you got the picture?
[00:16:09.280 --> 00:16:10.540]   And he's like, yeah.
[00:16:10.540 --> 00:16:12.660]   And I'm like, you don't like it?
[00:16:12.660 --> 00:16:17.660]   And he's like, well, I'm sure it's not,
[00:16:17.660 --> 00:16:21.580]   I'm sure it's your, I'm sure it's just not a good,
[00:16:21.580 --> 00:16:24.260]   you know, it's probably not your best.
[00:16:24.260 --> 00:16:25.220]   - Oh, no.
[00:16:25.220 --> 00:16:28.060]   - You know, you don't have to come.
[00:16:28.060 --> 00:16:29.420]   And he's like, no, no, no, I'm coming.
[00:16:29.420 --> 00:16:30.660]   And I'm like, no, you don't have to come.
[00:16:30.660 --> 00:16:32.220]   And he's like, no, no, I really wanna,
[00:16:32.220 --> 00:16:33.860]   I'm getting on the plane.
[00:16:33.860 --> 00:16:35.860]   I'm like, you don't have to get on the plane.
[00:16:35.860 --> 00:16:38.240]   He's like, no, I'm getting on the plane.
[00:16:38.240 --> 00:16:40.640]   And so I go down to my, I go,
[00:16:40.640 --> 00:16:42.040]   I'm in my office, this is happening, right?
[00:16:42.040 --> 00:16:44.680]   So I go downstairs to my, one of my closest friends,
[00:16:44.680 --> 00:16:47.040]   who's still actually one of my closest friends,
[00:16:47.040 --> 00:16:51.280]   who is one of my colleagues and Kevin.
[00:16:51.280 --> 00:16:54.280]   And I say, Kevin, and I go to Kevin, I go, Kevin, Kevin,
[00:16:54.280 --> 00:16:55.760]   Kevin, he doesn't like the photograph.
[00:16:55.760 --> 00:16:57.480]   And Kevin's like, well, which photograph did you send?
[00:16:57.480 --> 00:16:58.560]   And I'm like, well, you know,
[00:16:58.560 --> 00:16:59.880]   the one where we're shooting pool?
[00:16:59.900 --> 00:17:03.900]   And he's like, you sent that photograph?
[00:17:03.900 --> 00:17:05.040]   That's a horrible photograph.
[00:17:05.040 --> 00:17:07.340]   I'm like, yeah, but it's the only one that I had
[00:17:07.340 --> 00:17:09.620]   that was like, where my hair was kind of similar
[00:17:09.620 --> 00:17:10.460]   to what it is now.
[00:17:10.460 --> 00:17:14.780]   And he's like, Lisa, do I have to check everything for you?
[00:17:14.780 --> 00:17:17.420]   You should not have sent that.
[00:17:17.420 --> 00:17:20.340]   - But still, he flew over.
[00:17:20.340 --> 00:17:21.300]   - So he flew.
[00:17:21.300 --> 00:17:22.580]   - Where from, by the way?
[00:17:22.580 --> 00:17:25.900]   - He was in graduate school at Amherst,
[00:17:25.900 --> 00:17:27.940]   yeah, at UMass Amherst.
[00:17:27.940 --> 00:17:32.940]   So he flew and I picked him up at the airport
[00:17:32.940 --> 00:17:36.520]   and he was happy.
[00:17:36.520 --> 00:17:40.040]   So whatever the concern was, was gone.
[00:17:40.040 --> 00:17:43.800]   And I was dressed, you know, I carefully, carefully dressed.
[00:17:43.800 --> 00:17:44.840]   - Were you nervous?
[00:17:44.840 --> 00:17:46.960]   - I was really, really nervous.
[00:17:46.960 --> 00:17:50.560]   'Cause I don't really believe in fate
[00:17:50.560 --> 00:17:53.000]   and I don't really think there's only one person
[00:17:53.000 --> 00:17:54.700]   that you can be with.
[00:17:54.700 --> 00:17:59.700]   But I think, you know, people who,
[00:17:59.700 --> 00:18:04.320]   some people are curvy, they're kind of complicated.
[00:18:04.320 --> 00:18:06.400]   And so the number of people who fit them
[00:18:06.400 --> 00:18:08.080]   is maybe less than.
[00:18:08.080 --> 00:18:10.800]   - I like it, mathematically speaking, yeah.
[00:18:10.800 --> 00:18:13.160]   - And so when I was going to pick him up at the airport,
[00:18:13.160 --> 00:18:15.760]   I was thinking, well, this could,
[00:18:15.760 --> 00:18:19.100]   I could be going to pick up the person I'm gonna marry.
[00:18:19.100 --> 00:18:21.520]   Or not.
[00:18:21.520 --> 00:18:24.620]   I mean, like I really, but I really, you know,
[00:18:24.720 --> 00:18:27.840]   like our conversations were just very authentic
[00:18:27.840 --> 00:18:32.840]   and very moving and we really connected.
[00:18:32.840 --> 00:18:37.720]   And I really felt like he understood me, actually,
[00:18:37.720 --> 00:18:41.680]   in a way that a lot of people don't.
[00:18:41.680 --> 00:18:46.680]   And what was really nice was at the time,
[00:18:46.680 --> 00:18:53.200]   you know, the airport was this tiny little airport
[00:18:53.200 --> 00:18:54.760]   out in a cornfield, basically.
[00:18:54.760 --> 00:18:58.480]   And so driving back to the town,
[00:18:58.480 --> 00:19:00.200]   we were in the car for 15 minutes,
[00:19:00.200 --> 00:19:02.420]   completely in the dark as I was driving.
[00:19:02.420 --> 00:19:04.500]   And so it was very similar to,
[00:19:04.500 --> 00:19:06.460]   we had just spent, you know,
[00:19:06.460 --> 00:19:09.200]   20 something hours on the telephone,
[00:19:09.200 --> 00:19:11.320]   sitting in the dark, talking to each other.
[00:19:11.320 --> 00:19:14.540]   So it was very familiar.
[00:19:14.540 --> 00:19:16.440]   And we basically spent the whole weekend together
[00:19:16.440 --> 00:19:19.160]   and he met all my friends and we had a big party.
[00:19:21.000 --> 00:19:25.800]   And at the end of the weekend, I said, okay,
[00:19:25.800 --> 00:19:30.200]   you know, if we're gonna give this a shot,
[00:19:30.200 --> 00:19:33.160]   we probably shouldn't see other people.
[00:19:33.160 --> 00:19:35.680]   So it's a risk, you know?
[00:19:35.680 --> 00:19:36.520]   - Commitment.
[00:19:36.520 --> 00:19:40.220]   - But I just didn't see how it would work
[00:19:40.220 --> 00:19:42.220]   if we were dating people locally
[00:19:42.220 --> 00:19:44.200]   and then also seeing each other at a distance.
[00:19:44.200 --> 00:19:46.360]   'Cause I've had long distance relationships before
[00:19:46.360 --> 00:19:50.280]   and they're hard and they take a lot of effort.
[00:19:50.280 --> 00:19:51.960]   And so we decided we'd give it three months
[00:19:51.960 --> 00:19:53.320]   and see what happened.
[00:19:53.320 --> 00:19:54.640]   And that was it.
[00:19:54.640 --> 00:19:57.280]   - This is an interesting thing.
[00:19:57.280 --> 00:19:58.880]   Like we're all, what is it?
[00:19:58.880 --> 00:20:00.480]   There's several billion of us
[00:20:00.480 --> 00:20:02.400]   and we're kind of roaming this world.
[00:20:02.400 --> 00:20:04.480]   And then you kind of stick together.
[00:20:04.480 --> 00:20:07.680]   You find somebody that just like gets you.
[00:20:07.680 --> 00:20:10.040]   And it's interesting to think about,
[00:20:10.040 --> 00:20:12.160]   there's probably thousands, if not millions,
[00:20:12.160 --> 00:20:14.960]   of people that would be sticky to you,
[00:20:14.960 --> 00:20:17.760]   depending on the curvature of your space.
[00:20:17.760 --> 00:20:22.760]   But what is the, could you speak to the stickiness?
[00:20:22.760 --> 00:20:25.960]   Like to the, just the falling in love?
[00:20:25.960 --> 00:20:29.920]   Like seeing that somebody really gets you?
[00:20:29.920 --> 00:20:34.920]   Maybe by way of telling, do you think,
[00:20:34.920 --> 00:20:36.400]   do you remember there was a moment
[00:20:36.400 --> 00:20:40.600]   when you just realized, damn it, I think I'm,
[00:20:40.600 --> 00:20:42.800]   like I think this is the guy.
[00:20:42.800 --> 00:20:44.280]   I think I'm in love.
[00:20:44.280 --> 00:20:46.600]   - We were having these conversations actually
[00:20:46.600 --> 00:20:49.560]   from the really from the second weekend we were together.
[00:20:49.560 --> 00:20:51.560]   So he flew back the next weekend to State College
[00:20:51.560 --> 00:20:52.400]   'cause it was my birthday.
[00:20:52.400 --> 00:20:53.240]   It was my 30th birthday.
[00:20:53.240 --> 00:20:55.200]   My friends were throwing me a party.
[00:20:55.200 --> 00:20:58.080]   And we went hiking and we hiked up some mountain
[00:20:58.080 --> 00:21:01.520]   and we were sitting on a cliff over this overlook
[00:21:01.520 --> 00:21:02.480]   and talking to each other.
[00:21:02.480 --> 00:21:04.040]   And I was thinking, and I actually said to him,
[00:21:04.040 --> 00:21:06.880]   I'm like, I haven't really known you very long,
[00:21:06.880 --> 00:21:08.520]   but I feel like I'm falling in love with you,
[00:21:08.520 --> 00:21:10.160]   which can't possibly be happening.
[00:21:10.160 --> 00:21:11.600]   I must be projecting.
[00:21:11.600 --> 00:21:12.920]   - Must be projecting.
[00:21:12.920 --> 00:21:14.800]   - But it certainly feels that way, right?
[00:21:14.800 --> 00:21:16.640]   Like I don't believe in love at first sight.
[00:21:16.640 --> 00:21:18.880]   So this can't really be happening,
[00:21:18.880 --> 00:21:20.200]   but it sort of feels like it is.
[00:21:20.200 --> 00:21:21.680]   And he was like, I know what you mean.
[00:21:21.680 --> 00:21:24.480]   And so for the first three months or four months,
[00:21:24.480 --> 00:21:26.280]   we would say things to each other like,
[00:21:26.280 --> 00:21:28.080]   I feel like I'm in love with you,
[00:21:28.080 --> 00:21:32.640]   but you know, but that can't,
[00:21:32.640 --> 00:21:34.160]   but things don't really work like that.
[00:21:34.160 --> 00:21:37.040]   So, but you know, so, and then it became a joke.
[00:21:37.040 --> 00:21:38.520]   Like, I feel like I'm in love with you.
[00:21:38.520 --> 00:21:41.680]   And then eventually, you know, I think,
[00:21:41.680 --> 00:21:43.280]   but I think that was one moment
[00:21:43.280 --> 00:21:47.960]   where we were talking about, I don't know, just,
[00:21:47.960 --> 00:21:53.880]   you know, not just all the great aspirations you have
[00:21:53.880 --> 00:21:54.720]   or all the things,
[00:21:54.720 --> 00:21:56.560]   but also things you don't like about yourself,
[00:21:56.560 --> 00:21:57.920]   things that you're worried about,
[00:21:57.920 --> 00:21:59.720]   things that you're scared of.
[00:21:59.720 --> 00:22:03.200]   And then I think that was sort of solidified
[00:22:03.200 --> 00:22:04.040]   the relationship.
[00:22:04.040 --> 00:22:06.400]   And then there was one weekend
[00:22:06.400 --> 00:22:09.120]   where we went to Maine in the winter,
[00:22:09.120 --> 00:22:12.720]   which I mean, I really love the beach always,
[00:22:12.720 --> 00:22:15.680]   but in the winter, particularly.
[00:22:15.680 --> 00:22:18.560]   - 'Cause it's just beautiful and calm and whatever.
[00:22:18.560 --> 00:22:23.560]   - Yeah, and I also, I do find beauty in starkness sometimes.
[00:22:23.560 --> 00:22:28.880]   Like, so there's this grand majestic scene of, you know,
[00:22:28.880 --> 00:22:30.400]   this very powerful ocean
[00:22:30.400 --> 00:22:33.040]   and it's all these like beautiful blue grays
[00:22:33.040 --> 00:22:35.680]   and it's just stunning.
[00:22:35.680 --> 00:22:39.440]   And so we were sitting on this huge rock in Maine
[00:22:39.440 --> 00:22:42.440]   and where we'd gone for the weekend, it was freezing cold.
[00:22:42.440 --> 00:22:45.880]   And I honestly can't remember what he said
[00:22:45.880 --> 00:22:48.560]   or what I said or what,
[00:22:48.560 --> 00:22:52.920]   but I definitely remember having this feeling of,
[00:22:52.920 --> 00:22:57.160]   I absolutely wanna stay with this person.
[00:22:57.160 --> 00:22:58.840]   And I don't know what my life will be like
[00:22:58.840 --> 00:23:00.120]   if I'm not with this person.
[00:23:00.120 --> 00:23:02.400]   Like, I need to be with this person.
[00:23:02.400 --> 00:23:05.680]   - Can we, from a scientific and a human perspective,
[00:23:05.680 --> 00:23:10.680]   dig into your belief that love at first sight is not possible,
[00:23:11.520 --> 00:23:13.200]   you don't believe in it?
[00:23:13.200 --> 00:23:15.840]   'Cause there is, you don't think there's like a magic
[00:23:15.840 --> 00:23:19.720]   where you see somebody in the Jack Kerouac way
[00:23:19.720 --> 00:23:22.920]   and you're like, wow, that's something.
[00:23:22.920 --> 00:23:26.800]   That's a special little glimmer or something.
[00:23:26.800 --> 00:23:29.720]   - Oh, I definitely think you can connect with someone
[00:23:29.720 --> 00:23:32.160]   instant, in an instance.
[00:23:32.160 --> 00:23:34.760]   And I definitely think you can say,
[00:23:34.760 --> 00:23:35.800]   oh, there's something there
[00:23:35.800 --> 00:23:37.780]   and I'm really clicking with that person.
[00:23:37.780 --> 00:23:39.640]   Romantically, but also just with friends,
[00:23:39.640 --> 00:23:40.800]   it's possible to do that.
[00:23:40.800 --> 00:23:44.160]   You recognize a mind that's like yours
[00:23:44.160 --> 00:23:47.560]   or that's compatible with yours.
[00:23:47.560 --> 00:23:50.480]   There are ways that you feel like you're being understood
[00:23:50.480 --> 00:23:52.760]   or that you understand something about this person
[00:23:52.760 --> 00:23:54.480]   or maybe you see something in this person
[00:23:54.480 --> 00:23:58.280]   that you find really compelling or intriguing.
[00:23:58.280 --> 00:24:02.840]   But I think, you know, your brain is predictive organ.
[00:24:02.840 --> 00:24:03.680]   Right?
[00:24:03.680 --> 00:24:05.520]   You're using your past.
[00:24:05.520 --> 00:24:06.640]   - You're projecting.
[00:24:06.640 --> 00:24:10.160]   - You're using your past to make predictions
[00:24:10.160 --> 00:24:12.920]   and I mean, not deliberately.
[00:24:12.920 --> 00:24:14.400]   That's how your brain is wired.
[00:24:14.400 --> 00:24:15.320]   That's what it does.
[00:24:15.320 --> 00:24:20.240]   And so it's filling in all of the gaps that you,
[00:24:20.240 --> 00:24:24.760]   there are lots of gaps of information that you don't,
[00:24:24.760 --> 00:24:26.320]   information you don't have.
[00:24:26.320 --> 00:24:28.880]   And so your brain is filling those in and--
[00:24:28.880 --> 00:24:32.640]   - But isn't that what love is?
[00:24:32.640 --> 00:24:35.200]   - No, I don't think so, actually.
[00:24:35.200 --> 00:24:36.720]   I mean, to some extent, sure.
[00:24:36.720 --> 00:24:39.400]   You always, there's research to show
[00:24:39.400 --> 00:24:42.960]   that people who are in love always see the best
[00:24:42.960 --> 00:24:45.000]   in each other and they, you know,
[00:24:45.000 --> 00:24:48.280]   when there's a negative interpretation
[00:24:48.280 --> 00:24:49.480]   or a positive interpretation,
[00:24:49.480 --> 00:24:50.840]   you know, they choose the positive ones.
[00:24:50.840 --> 00:24:52.840]   There's a little bit of positive illusion there,
[00:24:52.840 --> 00:24:54.080]   you know, going on.
[00:24:54.080 --> 00:24:55.520]   That's what the research shows.
[00:24:55.520 --> 00:24:56.400]   But I think,
[00:24:56.400 --> 00:25:04.000]   I think that when you find somebody
[00:25:04.000 --> 00:25:09.000]   who not just appreciates your feelings
[00:25:09.240 --> 00:25:12.640]   and your faults, but loves you for them, actually,
[00:25:12.640 --> 00:25:16.600]   you know, like maybe even doesn't see them as a fault,
[00:25:16.600 --> 00:25:20.520]   that's, so you have to be honest enough
[00:25:20.520 --> 00:25:24.000]   about what your faults are.
[00:25:24.000 --> 00:25:26.960]   So it's easy to love someone for all the things that they,
[00:25:26.960 --> 00:25:34.080]   for all the wonderful characteristics they have.
[00:25:34.080 --> 00:25:37.920]   It's harder, I think, to love someone despite their faults
[00:25:37.920 --> 00:25:39.840]   or maybe even the faults that they see
[00:25:39.840 --> 00:25:41.120]   aren't really faults at all to you.
[00:25:41.120 --> 00:25:43.840]   They're actually something really special.
[00:25:43.840 --> 00:25:45.680]   - But isn't that, can't you explain that
[00:25:45.680 --> 00:25:48.600]   by saying the brain kind of, like you're projecting,
[00:25:48.600 --> 00:25:53.600]   it's your, you have a conception of a human being
[00:25:53.600 --> 00:25:58.120]   or just a spirit that really connects with you
[00:25:58.120 --> 00:26:01.240]   and you're projecting that onto that person
[00:26:01.240 --> 00:26:04.800]   and within that framework, all their faults
[00:26:04.800 --> 00:26:06.400]   then become beautiful, like little--
[00:26:06.400 --> 00:26:09.160]   - Maybe, but you just have to pay attention
[00:26:09.160 --> 00:26:10.400]   to the prediction error.
[00:26:10.400 --> 00:26:13.760]   - No, but maybe that's what love,
[00:26:13.760 --> 00:26:17.640]   like maybe you start ignoring the prediction error.
[00:26:17.640 --> 00:26:19.960]   Maybe love is just your ability--
[00:26:19.960 --> 00:26:22.800]   - To ignore the prediction error?
[00:26:22.800 --> 00:26:24.920]   Well, I think that there's some research
[00:26:24.920 --> 00:26:29.920]   that might say that, but that's not my experience, I guess.
[00:26:29.920 --> 00:26:32.000]   But there is some research that says,
[00:26:32.000 --> 00:26:33.320]   I mean, there's some research that says
[00:26:33.320 --> 00:26:35.840]   you have to have an optimal margin of illusion,
[00:26:35.840 --> 00:26:40.840]   which means that you put a positive spin on smaller things,
[00:26:40.840 --> 00:26:45.120]   but you don't ignore the bigger things, right?
[00:26:45.120 --> 00:26:48.400]   And I think without being judgmental at all,
[00:26:48.400 --> 00:26:52.720]   when someone says to me, you're not who I thought you were,
[00:26:52.720 --> 00:26:55.040]   I mean, nobody has said that to me in a really long time,
[00:26:55.040 --> 00:26:56.560]   but certainly when I was younger,
[00:26:56.560 --> 00:26:58.520]   that was, you're not who I thought you were.
[00:26:58.520 --> 00:27:01.440]   My reaction to that was, well, whose fault is that?
[00:27:01.440 --> 00:27:04.520]   (Lyle laughs)
[00:27:04.520 --> 00:27:07.280]   I'm a pretty upfront person.
[00:27:07.280 --> 00:27:11.200]   I mean, I will though say that in my experience,
[00:27:11.200 --> 00:27:15.880]   people don't lie to you about who they are.
[00:27:15.880 --> 00:27:18.420]   They lie to themselves in your presence.
[00:27:18.420 --> 00:27:22.040]   - Yeah.
[00:27:22.040 --> 00:27:27.040]   - And so, you don't wanna get tied up in that,
[00:27:27.040 --> 00:27:30.320]   tangled up in that.
[00:27:30.320 --> 00:27:32.540]   And I think from the get-go,
[00:27:32.540 --> 00:27:34.480]   Dan and I were just, for whatever reason,
[00:27:34.480 --> 00:27:36.240]   maybe it's 'cause we both have been divorced already,
[00:27:36.240 --> 00:27:41.240]   and he told me who he thought he was,
[00:27:41.240 --> 00:27:46.800]   and he was pretty accurate as far as I could--
[00:27:46.800 --> 00:27:47.640]   - He was accurate?
[00:27:47.640 --> 00:27:48.600]   - Pretty much, actually.
[00:27:48.600 --> 00:27:50.200]   I mean, there's very,
[00:27:50.200 --> 00:27:54.820]   I can't say that I've ever come across a characteristic
[00:27:54.820 --> 00:27:58.520]   in him that really surprised me in a bad way.
[00:27:58.520 --> 00:28:00.440]   - It's hard to know yourself.
[00:28:00.440 --> 00:28:01.280]   - It is hard to know yourself.
[00:28:01.280 --> 00:28:02.600]   - And to communicate that.
[00:28:02.600 --> 00:28:03.440]   - For sure.
[00:28:03.440 --> 00:28:07.640]   And I'll say, I had the advantage of training
[00:28:07.640 --> 00:28:09.840]   as a therapist, which meant for five years,
[00:28:09.840 --> 00:28:11.680]   I was under a fucking microscope.
[00:28:11.680 --> 00:28:12.840]   - Yeah.
[00:28:12.840 --> 00:28:14.880]   - When I was training as a therapist,
[00:28:14.880 --> 00:28:17.040]   it was hour for hour supervision,
[00:28:17.040 --> 00:28:20.600]   which meant if you were in a room with a client for an hour,
[00:28:20.600 --> 00:28:23.880]   you had an hour with a supervisor.
[00:28:23.880 --> 00:28:28.320]   So that supervisor was behind the mirror for your session,
[00:28:28.320 --> 00:28:30.720]   and then you went and had an hour of discussion
[00:28:30.720 --> 00:28:33.200]   about what you said, what you didn't say,
[00:28:33.200 --> 00:28:37.920]   learning to use your own feelings and thoughts
[00:28:37.920 --> 00:28:42.280]   as a tool to probe the mind of the client and so on.
[00:28:42.280 --> 00:28:45.720]   And so you can't help but learn a lot of,
[00:28:45.720 --> 00:28:47.760]   you can't help but learn a lot about yourself
[00:28:47.760 --> 00:28:48.920]   in that process.
[00:28:48.920 --> 00:28:53.920]   - Do you think knowing or learning how the sausage is made
[00:28:53.920 --> 00:28:58.120]   ruins the magic of the actual experience?
[00:28:58.120 --> 00:29:01.600]   Like, you as a neuroscientist who studies the brain,
[00:29:01.600 --> 00:29:05.640]   do you think it ruins the magic of love at first sight?
[00:29:05.640 --> 00:29:09.240]   Do you consciously are still able
[00:29:09.240 --> 00:29:11.320]   to lose yourself in the moment?
[00:29:11.320 --> 00:29:13.680]   - I'm definitely able to lose myself in the moment.
[00:29:13.680 --> 00:29:14.800]   - Is wine involved?
[00:29:14.800 --> 00:29:16.640]   - Not always.
[00:29:16.640 --> 00:29:17.480]   Chocolate?
[00:29:17.480 --> 00:29:20.320]   I mean, some kind of wine, I'll drink some substance, right?
[00:29:20.320 --> 00:29:23.200]   But yeah, for sure.
[00:29:23.200 --> 00:29:25.600]   I mean, I guess what I would say though is that,
[00:29:25.600 --> 00:29:31.240]   for me, part of the magic is the process.
[00:29:31.240 --> 00:29:35.200]   Like, so I remember a day,
[00:29:35.200 --> 00:29:38.280]   well, I was working on this book of essays.
[00:29:38.280 --> 00:29:39.800]   I was in New York.
[00:29:39.800 --> 00:29:42.840]   I can't remember why I was in New York,
[00:29:42.840 --> 00:29:44.680]   but I was in New York for something,
[00:29:44.680 --> 00:29:46.920]   and I was in Central Park,
[00:29:46.920 --> 00:29:50.400]   and I was looking at all the people with their babies,
[00:29:50.400 --> 00:29:51.580]   and I was thinking,
[00:29:51.580 --> 00:29:57.440]   each one of these, there's a tiny little brain
[00:29:57.440 --> 00:30:00.160]   that's wiring itself right now.
[00:30:00.160 --> 00:30:03.600]   And I just, I felt in that moment,
[00:30:03.600 --> 00:30:06.040]   I was like, I am never gonna look at an infant
[00:30:06.040 --> 00:30:08.320]   in the same way ever again.
[00:30:08.320 --> 00:30:11.200]   And so to me, I mean, honestly,
[00:30:11.200 --> 00:30:14.360]   before I started learning about brain development,
[00:30:14.360 --> 00:30:17.760]   I thought babies were cute, but not that interesting
[00:30:17.760 --> 00:30:21.400]   until they could interact with you and do things.
[00:30:21.400 --> 00:30:22.520]   Of course, my own infant,
[00:30:22.520 --> 00:30:24.360]   I thought was extraordinarily interesting,
[00:30:24.360 --> 00:30:27.160]   but they're kind of like lumps.
[00:30:27.160 --> 00:30:30.000]   That's until they can interact with you,
[00:30:30.000 --> 00:30:31.600]   but they are anything but lumps.
[00:30:31.600 --> 00:30:34.960]   I mean, so, and part of the,
[00:30:34.960 --> 00:30:38.920]   I mean, all I can say is I have deep affection now
[00:30:38.920 --> 00:30:43.920]   for tiny little babies in a way that I didn't really before
[00:30:43.920 --> 00:30:51.600]   because of the, I'm just so curious.
[00:30:51.600 --> 00:30:55.440]   - But the actual process, the mechanisms of the wiring
[00:30:55.440 --> 00:30:56.480]   of the brain, the learning,
[00:30:56.480 --> 00:30:58.360]   all the magic of the neurobiology.
[00:30:58.360 --> 00:31:01.120]   - Yeah, and or something like,
[00:31:01.120 --> 00:31:05.960]   when you make eye contact with someone directly,
[00:31:05.960 --> 00:31:10.960]   sometimes you feel something, right?
[00:31:10.960 --> 00:31:14.440]   And what is it?
[00:31:14.440 --> 00:31:15.800]   And what is that?
[00:31:15.800 --> 00:31:20.240]   And so to me, that's not backing away from the moment.
[00:31:20.240 --> 00:31:22.040]   That's like expanding the moment.
[00:31:22.040 --> 00:31:24.760]   It's like, that's incredibly cool.
[00:31:26.640 --> 00:31:30.480]   I'll just say that when I was in graduate school,
[00:31:30.480 --> 00:31:34.800]   I also was in therapy because it's almost a given
[00:31:34.800 --> 00:31:36.920]   that you're gonna be in therapy yourself
[00:31:36.920 --> 00:31:38.400]   if you're gonna become a therapist.
[00:31:38.400 --> 00:31:42.040]   And I had a deal with my therapist,
[00:31:42.040 --> 00:31:44.120]   which was that I could call time out
[00:31:44.120 --> 00:31:46.200]   at any moment that I wanted to,
[00:31:46.200 --> 00:31:48.240]   as long as I was being responsible about it.
[00:31:48.240 --> 00:31:50.680]   And I wasn't using it as a way to get out of something.
[00:31:50.680 --> 00:31:54.400]   And he could tell me, no, he could decline and say,
[00:31:54.400 --> 00:31:56.960]   no, you're using this to get out of something.
[00:31:56.960 --> 00:31:59.440]   But I could call time out whenever I want and say,
[00:31:59.440 --> 00:32:01.280]   what are you doing right now?
[00:32:01.280 --> 00:32:02.400]   Here's what I'm experiencing.
[00:32:02.400 --> 00:32:03.760]   What are you trying to do?
[00:32:03.760 --> 00:32:07.480]   I wanted to use my own experience to interrogate
[00:32:07.480 --> 00:32:10.920]   what the process was.
[00:32:10.920 --> 00:32:15.920]   And that made it more helpful in a way.
[00:32:15.920 --> 00:32:18.880]   Do you know what I mean?
[00:32:18.880 --> 00:32:21.640]   So yeah, I don't think learning how something works
[00:32:21.640 --> 00:32:23.440]   makes it less magical, actually.
[00:32:23.440 --> 00:32:25.360]   But that's just me, I guess.
[00:32:25.360 --> 00:32:26.560]   I don't know, would you?
[00:32:26.560 --> 00:32:28.640]   - Yes.
[00:32:28.640 --> 00:32:32.160]   I tend to have two modes.
[00:32:32.160 --> 00:32:35.600]   One is an engineer and one is romantic.
[00:32:35.600 --> 00:32:40.600]   And I'm conscious of like, there's two rooms.
[00:32:40.600 --> 00:32:43.760]   You can go into the one, the engineer room,
[00:32:43.760 --> 00:32:45.840]   and I think that ruins the romance.
[00:32:45.840 --> 00:32:48.560]   So I tend to, there's two rooms.
[00:32:48.560 --> 00:32:50.480]   One is the engineering room.
[00:32:50.480 --> 00:32:51.920]   Think from first principles.
[00:32:51.920 --> 00:32:53.400]   How do we build the thing
[00:32:53.400 --> 00:32:56.080]   that creates this kind of behavior?
[00:32:56.080 --> 00:32:58.000]   And then you go into the romantic room
[00:32:58.000 --> 00:33:00.200]   where you're like emotional, it's a roller coaster.
[00:33:00.200 --> 00:33:03.880]   And then you're, the thing is, let's take it slow.
[00:33:03.880 --> 00:33:05.880]   And then you get married the next night.
[00:33:05.880 --> 00:33:08.680]   Then you're just this giant mess and you write a song
[00:33:08.680 --> 00:33:12.200]   and then you cry and then you send a bunch of texts
[00:33:12.200 --> 00:33:14.160]   and anger and whatever.
[00:33:14.160 --> 00:33:17.160]   And somehow you're in Vegas and there's random people
[00:33:17.160 --> 00:33:18.760]   and you're drunk and whatever, all that.
[00:33:18.760 --> 00:33:21.040]   Like in poetry, just mess of it.
[00:33:21.040 --> 00:33:24.880]   Fighting, yeah, that's not, those are two rooms.
[00:33:24.880 --> 00:33:27.040]   And you go back between them.
[00:33:27.040 --> 00:33:29.600]   But I think the way you put it is quite poetic.
[00:33:29.600 --> 00:33:34.600]   I think you're much better at adulting with love
[00:33:34.600 --> 00:33:40.200]   than perhaps I am, because there's a magic to children.
[00:33:40.200 --> 00:33:45.200]   I also think like of adults as children.
[00:33:45.200 --> 00:33:48.680]   It's kind of cool to see, it's a cool thought experiment
[00:33:48.680 --> 00:33:53.360]   to look at adults and think like that used to be a baby.
[00:33:53.360 --> 00:33:56.280]   And then that's like a fully wired baby.
[00:33:56.280 --> 00:33:58.520]   And it's just walking around pretending to be like
[00:33:58.520 --> 00:34:01.840]   all serious and important, wearing a suit or something.
[00:34:01.840 --> 00:34:03.680]   But that used to be a baby.
[00:34:03.680 --> 00:34:05.500]   And then you think of like the parenting
[00:34:05.500 --> 00:34:07.520]   and all the experiences they had.
[00:34:07.520 --> 00:34:09.920]   Like it's cool to think of it that way.
[00:34:09.920 --> 00:34:11.460]   But then I started thinking of it like
[00:34:11.460 --> 00:34:13.500]   from a machine learning perspective.
[00:34:13.500 --> 00:34:16.280]   But once you're like the romantic moments,
[00:34:16.280 --> 00:34:19.120]   all that kind of stuff, all that falls away.
[00:34:19.120 --> 00:34:21.640]   I forget about all that, I don't know.
[00:34:21.640 --> 00:34:23.460]   That's the Russian thing.
[00:34:23.460 --> 00:34:24.720]   - Maybe, maybe.
[00:34:24.720 --> 00:34:26.480]   But I also think it might be an age thing
[00:34:26.480 --> 00:34:28.080]   or maybe an experience thing.
[00:34:28.080 --> 00:34:33.040]   So I think we all, I mean,
[00:34:33.040 --> 00:34:35.320]   if you're exposed to Western culture at all,
[00:34:35.320 --> 00:34:40.040]   you are exposed to the sort of idealized,
[00:34:40.040 --> 00:34:43.040]   stereotypic, romantic exchange.
[00:34:45.640 --> 00:34:48.080]   And what does it mean to be romantic?
[00:34:48.080 --> 00:34:50.100]   And so here's a test.
[00:34:50.100 --> 00:34:55.000]   I'm gonna see how to phrase it.
[00:34:55.000 --> 00:34:56.960]   Okay, so not really a test,
[00:34:56.960 --> 00:34:58.720]   but this tells you something about
[00:34:58.720 --> 00:35:00.380]   your own ideas about romance.
[00:35:00.380 --> 00:35:06.340]   For Valentine's Day one year,
[00:35:06.340 --> 00:35:09.760]   my husband bought me a six-way plug.
[00:35:09.760 --> 00:35:12.860]   Is that romantic or not romantic?
[00:35:14.960 --> 00:35:17.960]   - Like, sorry, six-way plug, that's like an outlet.
[00:35:17.960 --> 00:35:19.420]   - Yeah, like to put it in an outlet.
[00:35:19.420 --> 00:35:21.600]   Is that romantic or not romantic?
[00:35:21.600 --> 00:35:28.000]   - I mean, depends the look in his eyes when he does it.
[00:35:28.000 --> 00:35:31.520]   I mean, it depends on the conversation
[00:35:31.520 --> 00:35:33.760]   that led up to that point.
[00:35:33.760 --> 00:35:38.120]   Depends how much, it's like the music,
[00:35:38.120 --> 00:35:42.040]   'cause you have a very, you're both from the,
[00:35:42.040 --> 00:35:44.300]   my experience is with you as a fan,
[00:35:44.300 --> 00:35:45.720]   you have both a romantic nature,
[00:35:45.720 --> 00:35:46.960]   but you have a very pragmatic,
[00:35:46.960 --> 00:35:51.260]   like you cut through the bullshit of the fuzziness.
[00:35:51.260 --> 00:35:53.320]   And there's something about a six-way plug
[00:35:53.320 --> 00:35:54.360]   that cuts through the bullshit,
[00:35:54.360 --> 00:35:55.360]   that connects to the human,
[00:35:55.360 --> 00:35:57.360]   like he understands who you are.
[00:35:57.360 --> 00:35:58.200]   - Exactly.
[00:35:58.200 --> 00:35:59.080]   - Yeah.
[00:35:59.080 --> 00:36:00.500]   - Exactly.
[00:36:00.500 --> 00:36:03.120]   That was the most romantic gift he could have given me
[00:36:03.120 --> 00:36:05.960]   because he knows me so well.
[00:36:05.960 --> 00:36:08.000]   He has a deep understanding of me,
[00:36:08.000 --> 00:36:12.160]   which is that I will sit and suffer and complain
[00:36:12.160 --> 00:36:15.580]   about the fact that I have to plug and unplug things,
[00:36:15.580 --> 00:36:17.940]   and I will bitch and moan until the cows come home,
[00:36:17.940 --> 00:36:20.480]   but it would never occur to me
[00:36:20.480 --> 00:36:23.380]   to go buy a bloody six-way plug.
[00:36:23.380 --> 00:36:27.120]   Whereas for him, he bought it, he plugged it in,
[00:36:27.120 --> 00:36:29.340]   he arranged, he taped up all my wires,
[00:36:29.340 --> 00:36:31.020]   he made it like really usable.
[00:36:31.020 --> 00:36:32.020]   (both laughing)
[00:36:32.020 --> 00:36:37.020]   And for me, that was the best present.
[00:36:37.020 --> 00:36:38.340]   - The most romantic thing.
[00:36:38.340 --> 00:36:40.500]   - It was the most romantic thing
[00:36:40.500 --> 00:36:43.600]   because he understood who I was,
[00:36:43.600 --> 00:36:47.200]   and he did something very, or just the casual,
[00:36:47.200 --> 00:36:50.280]   like we moved into a house that we went
[00:36:50.280 --> 00:36:52.920]   from having a two-car garage to a one-car garage.
[00:36:52.920 --> 00:36:54.600]   And I said, "Okay, I'm from Canada,
[00:36:54.600 --> 00:36:56.040]   I'm not bothered by snow."
[00:36:56.040 --> 00:36:57.600]   Well, I mean, I'm a little bothered by snow,
[00:36:57.600 --> 00:36:59.240]   but he's very bothered by snow.
[00:36:59.240 --> 00:37:01.840]   So I'm like, "Okay, you can park your car in the garage,
[00:37:01.840 --> 00:37:03.280]   it's fine."
[00:37:03.280 --> 00:37:06.500]   Every day when it snows, he goes out and cleans my car.
[00:37:06.500 --> 00:37:07.340]   Every day.
[00:37:09.020 --> 00:37:12.060]   I never asked him to do it, he just does it
[00:37:12.060 --> 00:37:15.220]   because he knows that I'm cutting it really close
[00:37:15.220 --> 00:37:17.880]   in the morning, when we all used to go to work.
[00:37:17.880 --> 00:37:20.860]   I have it timed to the second
[00:37:20.860 --> 00:37:23.460]   so that I can get up as late as possible,
[00:37:23.460 --> 00:37:25.620]   work out as long as possible,
[00:37:25.620 --> 00:37:29.220]   and make it into my office a minute before my first meeting.
[00:37:29.220 --> 00:37:32.420]   And so if it snows unexpectedly or something, I'm screwed
[00:37:32.420 --> 00:37:35.200]   because now that's an added 10 or 15 minutes
[00:37:35.200 --> 00:37:36.740]   and I'm gonna be late.
[00:37:36.740 --> 00:37:39.400]   Anyways, it's just these little tiny things.
[00:37:39.400 --> 00:37:43.640]   He's a really easygoing guy,
[00:37:43.640 --> 00:37:45.360]   and he doesn't look like somebody
[00:37:45.360 --> 00:37:47.380]   who pays attention to detail.
[00:37:47.380 --> 00:37:50.520]   He doesn't fuss about detail,
[00:37:50.520 --> 00:37:53.160]   but he definitely pays attention to detail.
[00:37:53.160 --> 00:37:57.520]   And it is very, very romantic in the sense that
[00:37:57.520 --> 00:38:04.680]   he loves me despite my little details.
[00:38:04.680 --> 00:38:05.840]   - And understands you.
[00:38:05.840 --> 00:38:06.680]   - Yeah, he understands me.
[00:38:06.680 --> 00:38:09.540]   - But it is kind of hilarious that that is,
[00:38:09.540 --> 00:38:14.540]   the six-way plug is the most fulfilling,
[00:38:14.540 --> 00:38:19.180]   richest display of romance in your life.
[00:38:19.180 --> 00:38:20.020]   I love it.
[00:38:20.020 --> 00:38:20.840]   I love it.
[00:38:20.840 --> 00:38:21.680]   - That's what I mean about romance.
[00:38:21.680 --> 00:38:24.380]   Romance is really, it's not all about chocolates and flowers
[00:38:24.380 --> 00:38:25.500]   and whatever.
[00:38:25.500 --> 00:38:28.220]   I mean, those are all nice too, but--
[00:38:28.220 --> 00:38:29.740]   - Sometimes it's about the six-way plug.
[00:38:29.740 --> 00:38:32.260]   - Sometimes it's about the six-way plug.
[00:38:32.260 --> 00:38:35.520]   So maybe one way I could ask
[00:38:35.520 --> 00:38:36.680]   before we talk about the details,
[00:38:36.680 --> 00:38:38.520]   you also have the author of another book
[00:38:38.520 --> 00:38:41.280]   as we talked about how emotions are made.
[00:38:41.280 --> 00:38:44.040]   So it's interesting to talk about the process of writing.
[00:38:44.040 --> 00:38:46.000]   You mentioned you were in New York.
[00:38:46.000 --> 00:38:48.200]   What have you learned from writing these two books
[00:38:48.200 --> 00:38:50.400]   about the actual process of writing?
[00:38:50.400 --> 00:38:53.400]   And maybe, I don't know what's the most interesting thing
[00:38:53.400 --> 00:38:55.880]   to talk about there, maybe the biggest challenges
[00:38:55.880 --> 00:38:58.360]   or the boring, mundane, systematic,
[00:38:58.360 --> 00:39:00.360]   like day-to-day of what worked for you,
[00:39:00.360 --> 00:39:04.000]   like hacks or even just about the neuroscience
[00:39:04.000 --> 00:39:06.980]   that you've learned through the process
[00:39:06.980 --> 00:39:08.360]   of trying to write them.
[00:39:08.360 --> 00:39:09.680]   - Here's the thing I learned.
[00:39:09.680 --> 00:39:11.760]   If you think that it's gonna take you a year
[00:39:11.760 --> 00:39:14.160]   to write your book, it's going to take you three years
[00:39:14.160 --> 00:39:15.240]   to write your book.
[00:39:15.240 --> 00:39:17.120]   That's the first thing I learned,
[00:39:17.120 --> 00:39:22.120]   is that no matter how organized you are,
[00:39:22.120 --> 00:39:27.560]   it's always gonna take way longer than what you think
[00:39:28.760 --> 00:39:33.760]   in part because very few people make an outline
[00:39:33.760 --> 00:39:35.220]   and then just stick to it.
[00:39:35.220 --> 00:39:39.020]   Some of the topics really take on a life of their own
[00:39:39.020 --> 00:39:43.920]   and to some extent, you wanna let them have their voice.
[00:39:43.920 --> 00:39:46.960]   You wanna follow leads until you feel satisfied
[00:39:46.960 --> 00:39:51.960]   that you've dealt with the topic appropriately.
[00:39:51.960 --> 00:39:54.460]   But I, and that part is actually fun.
[00:39:54.460 --> 00:39:57.040]   It's not fun to feel like you're constantly behind
[00:39:57.040 --> 00:39:59.480]   the eight ball in terms of time,
[00:39:59.480 --> 00:40:02.880]   but it is the exploration and the foraging for information
[00:40:02.880 --> 00:40:05.760]   is incredibly fun for me anyways.
[00:40:05.760 --> 00:40:07.040]   I found it really enjoyable.
[00:40:07.040 --> 00:40:09.160]   And if I wasn't also running a lab at the same time
[00:40:09.160 --> 00:40:12.520]   and trying to keep my family going,
[00:40:12.520 --> 00:40:15.920]   it would have been, the whole thing would have just been fun.
[00:40:15.920 --> 00:40:18.440]   But I would say the hardest thing about,
[00:40:18.440 --> 00:40:20.200]   the most important thing I think I learned
[00:40:20.200 --> 00:40:22.200]   is also the hardest thing for me,
[00:40:22.200 --> 00:40:27.200]   which is knowing what to leave out.
[00:40:27.200 --> 00:40:31.720]   A really good storyteller knows what to leave out.
[00:40:31.720 --> 00:40:37.540]   In academic writing, you shouldn't leave anything out.
[00:40:37.540 --> 00:40:40.400]   All the details should be there.
[00:40:40.400 --> 00:40:50.400]   I've written or participated in writing over 200 papers
[00:40:51.240 --> 00:40:54.640]   peer reviewed papers.
[00:40:54.640 --> 00:40:57.680]   So I'm pretty good with detail.
[00:40:57.680 --> 00:40:59.720]   Knowing what to leave out,
[00:40:59.720 --> 00:41:00.720]   knowing what to leave out
[00:41:00.720 --> 00:41:04.160]   and not harming the validity of the story.
[00:41:04.160 --> 00:41:06.840]   That is a tricky, tricky thing.
[00:41:06.840 --> 00:41:10.160]   It was tricky when I wrote "How Emotions Are Made,"
[00:41:10.160 --> 00:41:13.780]   but that's a standard popular science book.
[00:41:13.780 --> 00:41:15.400]   So it's 300 something pages.
[00:41:15.400 --> 00:41:18.280]   And then, it has like a thousand end notes
[00:41:18.280 --> 00:41:22.400]   and then each of the end notes is attached to a web note,
[00:41:22.400 --> 00:41:23.920]   which is also long.
[00:41:23.920 --> 00:41:25.800]   So I mean, it's,
[00:41:25.800 --> 00:41:30.520]   and it start, and I mean the final draft,
[00:41:30.520 --> 00:41:33.620]   I mean, I wrote three drafts of that book actually,
[00:41:33.620 --> 00:41:36.960]   and the final draft, and then I had to cut by a third.
[00:41:36.960 --> 00:41:38.660]   I mean, or, I mean, I, you know,
[00:41:38.660 --> 00:41:42.560]   it was like 150,000 words or something
[00:41:42.560 --> 00:41:44.960]   and I had to cut it down to like 110.
[00:41:44.960 --> 00:41:49.020]   So obviously, I struggle with what to leave out.
[00:41:49.020 --> 00:41:50.520]   You know, brevity is not my strong suit.
[00:41:50.520 --> 00:41:52.840]   I'm always telling people that, it's a warning.
[00:41:52.840 --> 00:41:55.120]   So that's why this book was,
[00:41:55.120 --> 00:41:58.280]   I, you know, I'd always been really fascinated with essays.
[00:41:58.280 --> 00:41:59.960]   I love reading essays.
[00:41:59.960 --> 00:42:04.960]   And after reading a small set of essays by Anne Fadiman
[00:42:04.960 --> 00:42:07.800]   called "At Large and at Small,"
[00:42:07.800 --> 00:42:10.320]   which I just loved these little essays.
[00:42:10.320 --> 00:42:12.280]   - What's the topic of those essays?
[00:42:12.280 --> 00:42:15.280]   - They are, they're called familiar essays.
[00:42:15.280 --> 00:42:18.240]   So the topics are like everyday topics,
[00:42:18.240 --> 00:42:22.160]   like mail, coffee, chocolate.
[00:42:22.160 --> 00:42:23.120]   I mean, just like,
[00:42:23.120 --> 00:42:26.160]   and what she does is she weaves her own experience.
[00:42:26.160 --> 00:42:28.060]   It's a little bit like these conversations
[00:42:28.060 --> 00:42:30.560]   that you're so good at curating actually.
[00:42:30.560 --> 00:42:36.240]   You're weaving together history and philosophy and science
[00:42:36.240 --> 00:42:38.600]   and also personal reflections.
[00:42:38.600 --> 00:42:43.600]   And a little bit you feel like you're like eavesdropping
[00:42:43.600 --> 00:42:47.680]   on someone's train of thought in a way.
[00:42:47.680 --> 00:42:51.600]   It's really, they're really compelling to me.
[00:42:51.600 --> 00:42:53.360]   - Even if it's just like a mundane topic.
[00:42:53.360 --> 00:42:58.360]   - Yeah, but it's so interesting to learn about
[00:42:58.360 --> 00:43:02.400]   like all of these little stories
[00:43:02.400 --> 00:43:08.120]   in the wrapping of the history of like mail.
[00:43:08.120 --> 00:43:10.080]   Like that's really interesting.
[00:43:10.080 --> 00:43:12.040]   And so I read these essays
[00:43:12.040 --> 00:43:15.440]   and then I wrote to her a little fangirl email.
[00:43:15.440 --> 00:43:16.960]   This was many years ago.
[00:43:16.960 --> 00:43:21.680]   And I said, "I just love this book.
[00:43:21.680 --> 00:43:24.000]   "And how did you learn to write essays like this?"
[00:43:24.000 --> 00:43:26.840]   And she gave me a reading list of essays that I should read,
[00:43:26.840 --> 00:43:27.720]   like writers.
[00:43:27.720 --> 00:43:29.040]   And so I read them all.
[00:43:29.040 --> 00:43:32.160]   And anyway, so I decided
[00:43:32.160 --> 00:43:34.360]   it would be a really good challenge for me
[00:43:34.360 --> 00:43:37.240]   to try to write something really brief
[00:43:37.240 --> 00:43:41.720]   where I could focus on, you know,
[00:43:41.720 --> 00:43:46.720]   one or two really fascinating tidbits of neuroscience.
[00:43:46.720 --> 00:43:51.800]   Connect it to, connect each one to something philosophical
[00:43:51.800 --> 00:43:56.600]   or, you know, like just a question about human nature.
[00:43:56.600 --> 00:43:58.720]   Do it in a really brief format
[00:43:58.720 --> 00:44:03.720]   without violating the validity of the science.
[00:44:05.200 --> 00:44:07.280]   And that was a, I just set myself this,
[00:44:07.280 --> 00:44:09.520]   what I thought of as a really, really big challenge
[00:44:09.520 --> 00:44:11.520]   in part because it was an incredibly hard thing
[00:44:11.520 --> 00:44:13.360]   for me to do in the first book.
[00:44:13.360 --> 00:44:15.520]   - Yeah, we should say that this is,
[00:44:15.520 --> 00:44:18.000]   "The Seven and a Half Lessons" is a very short book.
[00:44:18.000 --> 00:44:22.880]   I mean, it's like it embodies brevity, right?
[00:44:22.880 --> 00:44:25.720]   The whole point throughout is just,
[00:44:25.720 --> 00:44:27.960]   I mean, you could tell that there's editing,
[00:44:27.960 --> 00:44:31.240]   like there's pain in trying to bring it
[00:44:31.240 --> 00:44:35.080]   as brief as possible, as clean as possible, yeah.
[00:44:35.080 --> 00:44:37.720]   - Yeah, so it's, the way I think of it is,
[00:44:37.720 --> 00:44:41.400]   you know, it's a little book of big science and big ideas.
[00:44:41.400 --> 00:44:45.000]   - Yeah, really big ideas in brief little packages.
[00:44:45.000 --> 00:44:49.880]   - And, you know, I wrote it so that people could read it.
[00:44:49.880 --> 00:44:51.840]   I love reading on the beach.
[00:44:51.840 --> 00:44:53.520]   I love reading essays on the beach.
[00:44:53.520 --> 00:44:55.900]   I read it, I wrote it so people could read it on the beach
[00:44:55.900 --> 00:44:58.960]   or in the bathtub or, you know, a subway stop.
[00:44:58.960 --> 00:45:02.080]   - Even if the beach is frozen over in the snow.
[00:45:02.080 --> 00:45:04.280]   - Yeah, so my husband, Dan,
[00:45:04.280 --> 00:45:06.760]   calls it the first neuroscience beach read.
[00:45:06.760 --> 00:45:10.080]   That's his phrasing, yeah.
[00:45:10.080 --> 00:45:13.400]   - Yeah, and like you said, you learn a lot about writing
[00:45:13.400 --> 00:45:15.320]   from your husband, like you were saying offline.
[00:45:15.320 --> 00:45:20.320]   - Well, he is, of the two of us, he is the better writer.
[00:45:20.320 --> 00:45:22.320]   He is a masterful writer.
[00:45:22.320 --> 00:45:27.120]   He's also, I mean, you know,
[00:45:27.120 --> 00:45:28.480]   he's a PhD in computer science.
[00:45:28.480 --> 00:45:30.000]   He's a software engineer,
[00:45:30.000 --> 00:45:35.000]   but he's also really good at organization of knowledge.
[00:45:35.000 --> 00:45:38.880]   So he built, for a company he used to work for,
[00:45:38.880 --> 00:45:41.720]   he built one of the first knowledge management systems.
[00:45:41.720 --> 00:45:44.240]   And he now works at Google
[00:45:44.240 --> 00:45:46.600]   where he does engineering education.
[00:45:46.600 --> 00:45:50.080]   Like he understands how to tell a good story
[00:45:50.080 --> 00:45:54.800]   just, you know, about anything really.
[00:45:54.800 --> 00:45:57.320]   He's got impeccable timing.
[00:45:57.320 --> 00:45:59.160]   He's really funny.
[00:45:59.160 --> 00:46:00.760]   And luckily for me,
[00:46:00.760 --> 00:46:03.720]   he knows very little about psychology or neuroscience.
[00:46:03.720 --> 00:46:07.440]   Well, now he knows more, obviously, but so, you know,
[00:46:07.440 --> 00:46:09.920]   he was really, when "How Emotions Were Made,"
[00:46:09.920 --> 00:46:13.920]   you know, he was really, really helpful to me
[00:46:13.920 --> 00:46:17.200]   because the first draft of every chapter
[00:46:17.200 --> 00:46:19.760]   was me talking to him about what, you know,
[00:46:19.760 --> 00:46:22.560]   I would talk out loud about what I wanted to say
[00:46:22.560 --> 00:46:24.840]   and the order in which I wanted to say it.
[00:46:24.840 --> 00:46:26.240]   And then I would write it,
[00:46:27.600 --> 00:46:28.880]   and then he would read it
[00:46:28.880 --> 00:46:32.040]   and tell me all the bits that could be excised.
[00:46:32.040 --> 00:46:35.480]   And sometimes we would, you know, I should say,
[00:46:35.480 --> 00:46:39.040]   I mean, we don't, he and I don't really argue about much
[00:46:39.040 --> 00:46:41.400]   except directions in the car.
[00:46:41.400 --> 00:46:44.240]   Like that's, if we're gonna have an argument,
[00:46:44.240 --> 00:46:46.880]   that's gonna be where it's gonna happen, where.
[00:46:46.880 --> 00:46:48.600]   - What's the nature of the argument
[00:46:48.600 --> 00:46:49.920]   about directions exactly?
[00:46:49.920 --> 00:46:50.920]   - I don't really know.
[00:46:50.920 --> 00:46:52.480]   It's just that we're very,
[00:46:52.480 --> 00:46:54.480]   I think it's that spatially, you know,
[00:46:56.600 --> 00:46:58.680]   I use egocentric space.
[00:46:58.680 --> 00:47:01.400]   So I wanna say, you know, turn left.
[00:47:01.400 --> 00:47:03.880]   Like I'm reasoning in relation
[00:47:03.880 --> 00:47:06.680]   to like my own physical corporeal body.
[00:47:06.680 --> 00:47:08.200]   So, you know, you walk to the church
[00:47:08.200 --> 00:47:11.080]   and you turn left and you, then, you know, whatever.
[00:47:11.080 --> 00:47:12.120]   You know, I'm always like,
[00:47:12.120 --> 00:47:16.280]   and his, you know, he gives directions allocentrically,
[00:47:16.280 --> 00:47:21.040]   which means organized around North, South, East, West.
[00:47:21.040 --> 00:47:24.320]   - So to you, the Earth is at the center of the solar system
[00:47:24.320 --> 00:47:26.520]   and to him, reasonably. - No, I'm at the center.
[00:47:26.520 --> 00:47:28.000]   - I'm at the center. - You're at the center
[00:47:28.000 --> 00:47:29.000]   of the solar system.
[00:47:29.000 --> 00:47:32.160]   Okay, so. - Anyway, so we,
[00:47:32.160 --> 00:47:34.400]   but here we, you know,
[00:47:34.400 --> 00:47:37.640]   we had some really rip roaring arguments,
[00:47:37.640 --> 00:47:39.560]   like really rip roaring arguments
[00:47:39.560 --> 00:47:41.880]   where he would say like, "Who is this for?
[00:47:41.880 --> 00:47:44.320]   Is this for the 1%?"
[00:47:44.320 --> 00:47:47.880]   And I'd be like, 1% meaning not, you know, not wealth,
[00:47:47.880 --> 00:47:50.760]   but like civilians versus academics.
[00:47:50.760 --> 00:47:52.160]   You know, so are these for the scientists
[00:47:52.160 --> 00:47:54.120]   or is this for the civilians, right?
[00:47:54.120 --> 00:47:56.040]   - So he speaks for the people, for the civilians.
[00:47:56.040 --> 00:47:57.760]   - He speaks for the people and I'd be like,
[00:47:57.760 --> 00:47:59.200]   "No, you have to."
[00:47:59.200 --> 00:48:00.880]   And so he made, you know,
[00:48:00.880 --> 00:48:03.120]   after one terrible argument that we had
[00:48:03.120 --> 00:48:05.840]   where it was really starting to affect our relationship
[00:48:05.840 --> 00:48:09.080]   because we were so mad at each other all the time,
[00:48:09.080 --> 00:48:14.080]   he made these little signs, writing and science.
[00:48:14.080 --> 00:48:16.800]   And we only use them, this was like,
[00:48:16.800 --> 00:48:20.520]   when you pulled out a sign, that's it.
[00:48:20.520 --> 00:48:22.240]   Like the other person just wins
[00:48:22.240 --> 00:48:24.320]   and you have to stop fighting about it.
[00:48:24.320 --> 00:48:25.240]   And that's it. - Great.
[00:48:25.240 --> 00:48:26.600]   - And so we just did that.
[00:48:26.600 --> 00:48:30.200]   And we didn't really have to use it too much for this book
[00:48:30.200 --> 00:48:33.120]   'cause this book was in some ways,
[00:48:33.120 --> 00:48:37.680]   you know, I didn't have to learn a lot of new things
[00:48:37.680 --> 00:48:39.080]   for this book, I had to learn some,
[00:48:39.080 --> 00:48:44.080]   but a lot of what I learned
[00:48:44.080 --> 00:48:47.480]   for "How Emotions Are Made"
[00:48:47.480 --> 00:48:50.520]   really stood me in good stead for this book.
[00:48:50.520 --> 00:48:51.400]   So there was a little bit,
[00:48:51.400 --> 00:48:53.360]   each essay was a little bit of learning.
[00:48:53.360 --> 00:48:56.640]   A couple were, was a little more than the small amount,
[00:48:56.640 --> 00:48:59.960]   but I didn't have so much trouble here.
[00:48:59.960 --> 00:49:03.800]   I had a lot of trouble with the first book,
[00:49:03.800 --> 00:49:05.800]   but still even here, you know,
[00:49:05.800 --> 00:49:09.520]   he would tell me that I could take something out
[00:49:09.520 --> 00:49:11.040]   and I really wanted to keep it.
[00:49:11.040 --> 00:49:15.320]   And I think we only used the signs once.
[00:49:15.320 --> 00:49:17.960]   - Well, if we could dive in some aspects of the book,
[00:49:17.960 --> 00:49:19.120]   I would love that.
[00:49:19.120 --> 00:49:23.280]   Can we talk about, so one of the essays,
[00:49:23.280 --> 00:49:24.640]   it looks at evolution.
[00:49:24.640 --> 00:49:30.920]   Let me ask the big question,
[00:49:30.920 --> 00:49:33.920]   did the human brain evolve to think?
[00:49:33.920 --> 00:49:38.360]   That's essentially the question that you address in the essay
[00:49:38.360 --> 00:49:39.480]   can you speak to it?
[00:49:39.480 --> 00:49:43.360]   - Sure, you know, the big caveat here is that
[00:49:43.360 --> 00:49:45.760]   we don't really know why brains evolved.
[00:49:45.760 --> 00:49:49.800]   The big why questions are called teleological questions.
[00:49:49.800 --> 00:49:54.800]   And in general, scientists should avoid those questions
[00:49:54.800 --> 00:49:58.640]   because we don't know really why, we don't know the why.
[00:49:58.640 --> 00:50:03.640]   However, for a very long time,
[00:50:03.640 --> 00:50:07.080]   the assumption was that evolution worked
[00:50:07.080 --> 00:50:09.400]   in a progressive upward scale,
[00:50:09.400 --> 00:50:11.200]   that you start off with simple organisms
[00:50:11.200 --> 00:50:13.160]   and those organisms get more complex
[00:50:13.160 --> 00:50:14.920]   and more complex and more complex.
[00:50:14.920 --> 00:50:17.600]   Now, obviously that's true in some like
[00:50:17.600 --> 00:50:19.640]   really general way, right?
[00:50:19.640 --> 00:50:22.720]   That life started off as single cell organisms
[00:50:22.720 --> 00:50:24.440]   and things got more complex.
[00:50:24.440 --> 00:50:29.440]   But the idea that brains evolved in some upward trajectory
[00:50:29.440 --> 00:50:34.700]   from simple brains in simple animals
[00:50:34.700 --> 00:50:37.040]   to complex brains in complex animals
[00:50:37.040 --> 00:50:39.340]   is called a phylogenetic scale.
[00:50:39.340 --> 00:50:44.360]   And that phylogenetic scale is embedded
[00:50:44.360 --> 00:50:46.000]   in a lot of evolutionary thinking,
[00:50:46.000 --> 00:50:47.920]   including Darwin's actually.
[00:50:48.880 --> 00:50:53.120]   And it's been seriously challenged, I would say,
[00:50:53.120 --> 00:50:56.340]   by modern evolutionary biology.
[00:50:56.340 --> 00:51:01.540]   And so, you know, thinking is something that,
[00:51:01.540 --> 00:51:04.320]   rationality is something that humans,
[00:51:04.320 --> 00:51:07.340]   at least in the West, really prize
[00:51:07.340 --> 00:51:10.640]   as a great human achievement.
[00:51:10.640 --> 00:51:15.640]   And so the idea that the most common evolutionary story
[00:51:15.880 --> 00:51:20.360]   is that brains evolved in like sedimentary rock
[00:51:20.360 --> 00:51:25.520]   with a layer for instincts, that's your lizard brain,
[00:51:25.520 --> 00:51:30.480]   and a layer on top of that for emotions,
[00:51:30.480 --> 00:51:33.260]   that's your limbic system, limbic meaning border.
[00:51:33.260 --> 00:51:36.480]   So it borders the parts that are for instincts.
[00:51:36.480 --> 00:51:37.320]   - Oh, interesting.
[00:51:37.320 --> 00:51:42.320]   - And then the neocortex or new cortex
[00:51:42.840 --> 00:51:46.200]   where rationality is supposed to live.
[00:51:46.200 --> 00:51:48.480]   That's the sort of traditional story.
[00:51:48.480 --> 00:51:52.040]   - It just keeps getting layered on top by evolution.
[00:51:52.040 --> 00:51:54.840]   - Right, and so you can think about, you know,
[00:51:54.840 --> 00:51:57.080]   I mean, sedimentary rock is the way
[00:51:57.080 --> 00:51:58.480]   typically people describe it.
[00:51:58.480 --> 00:52:01.560]   The way I sometimes like to think about it is,
[00:52:01.560 --> 00:52:03.360]   you know, thinking about the cerebral cortex
[00:52:03.360 --> 00:52:07.800]   like icing on an already baked cake, you know,
[00:52:07.800 --> 00:52:11.160]   where, you know, the cake is your inner beast.
[00:52:11.160 --> 00:52:14.000]   These like boiling, you know, roiling instincts
[00:52:14.000 --> 00:52:15.880]   and emotions that have to be contained.
[00:52:15.880 --> 00:52:20.880]   And by the cortex, and it's just, it's a fiction.
[00:52:20.880 --> 00:52:23.880]   It's a myth.
[00:52:23.880 --> 00:52:26.280]   It's a myth that you can trace all the way back
[00:52:26.280 --> 00:52:31.080]   to stories about morality in ancient Greece.
[00:52:31.080 --> 00:52:35.640]   But what you can do is look at the scientific record
[00:52:35.640 --> 00:52:38.160]   and say, well, there are other stories
[00:52:38.160 --> 00:52:40.200]   that you could tell about brain evolution
[00:52:40.200 --> 00:52:45.200]   and the context in which brains evolved.
[00:52:45.200 --> 00:52:50.600]   So when you look at creatures who don't have brains
[00:52:50.600 --> 00:52:55.040]   and you look at creatures who do, what's the difference?
[00:52:55.040 --> 00:53:00.960]   And you can look at, you know, some animals.
[00:53:00.960 --> 00:53:05.680]   So we call, scientists call an environment
[00:53:05.680 --> 00:53:09.320]   that an animal lives in a niche, their environmental niche.
[00:53:09.320 --> 00:53:11.320]   What are the things, what are the parts of the environment
[00:53:11.320 --> 00:53:13.200]   that matter to that animal?
[00:53:13.200 --> 00:53:16.600]   And so there's some animals whose niche hasn't changed
[00:53:16.600 --> 00:53:18.440]   in 400 million years.
[00:53:18.440 --> 00:53:21.560]   So they're not, these creatures are modern creatures,
[00:53:21.560 --> 00:53:24.520]   but they're living in a niche that hasn't changed much.
[00:53:24.520 --> 00:53:27.240]   And so their biology hasn't changed much.
[00:53:27.240 --> 00:53:30.440]   And you can kind of verify that by looking at the genes
[00:53:30.440 --> 00:53:32.280]   that lurk deep, you know,
[00:53:32.280 --> 00:53:34.460]   in the molecular structure of cells.
[00:53:35.480 --> 00:53:39.840]   And so you can, by looking at various animals
[00:53:39.840 --> 00:53:41.720]   in their developmental state,
[00:53:41.720 --> 00:53:43.840]   meaning not, you don't look at adult animals,
[00:53:43.840 --> 00:53:47.600]   you look at embryos of animals and developing animals,
[00:53:47.600 --> 00:53:50.400]   you can see, you can piece together a different story.
[00:53:50.400 --> 00:53:54.640]   And that story is that brains evolved
[00:53:54.640 --> 00:53:58.600]   under the selection pressure of hunting.
[00:53:58.600 --> 00:54:00.760]   That in the Cambrian period,
[00:54:00.760 --> 00:54:02.460]   hunting emerged on the scene
[00:54:02.480 --> 00:54:05.620]   where animals deliberately ate one another.
[00:54:05.620 --> 00:54:11.720]   And what, so, you know, before the Cambrian period,
[00:54:11.720 --> 00:54:15.040]   the animals didn't really have,
[00:54:15.040 --> 00:54:16.480]   well, they didn't have brains,
[00:54:16.480 --> 00:54:19.600]   but they also didn't have senses, really,
[00:54:19.600 --> 00:54:21.520]   the very, very rudimentary senses.
[00:54:21.520 --> 00:54:26.520]   So the animal that I wrote about in seven and a half lessons
[00:54:26.520 --> 00:54:29.720]   is called an amphioxus or a lancelet.
[00:54:29.720 --> 00:54:34.720]   And little amphioxus has no eyes,
[00:54:34.720 --> 00:54:37.480]   it has no ears, it has no nose,
[00:54:37.480 --> 00:54:41.580]   it has no eyes, it has a couple of cells
[00:54:41.580 --> 00:54:46.580]   for detecting light and dark for circadian rhythm purposes.
[00:54:46.580 --> 00:54:50.460]   So, and it can't hear,
[00:54:50.460 --> 00:54:53.560]   it has a vestibular cell to keep its body upright.
[00:54:53.560 --> 00:54:57.540]   It has a very rudimentary sense of touch,
[00:54:57.540 --> 00:55:00.460]   and it doesn't really have any internal organs
[00:55:00.460 --> 00:55:03.060]   other than this, like, basically stomach.
[00:55:03.060 --> 00:55:04.820]   It's like a, just like a,
[00:55:04.820 --> 00:55:06.980]   it doesn't have an enteric nervous system,
[00:55:06.980 --> 00:55:10.700]   it doesn't have, like, a gut that, you know, moves,
[00:55:10.700 --> 00:55:13.620]   like we do, it just has basically a tube.
[00:55:13.620 --> 00:55:15.540]   - Yeah, a little container.
[00:55:15.540 --> 00:55:16.740]   - Like a little container, yeah.
[00:55:16.740 --> 00:55:20.140]   And so, and really, it doesn't move very much.
[00:55:20.140 --> 00:55:22.160]   It can move, it just sort of wriggles,
[00:55:22.160 --> 00:55:24.740]   it doesn't have very sophisticated movement.
[00:55:24.740 --> 00:55:27.820]   And it's this really sweet little animal,
[00:55:27.820 --> 00:55:30.580]   it sort of wriggles its way to a spot
[00:55:30.580 --> 00:55:33.560]   and then plants itself in the sand
[00:55:33.560 --> 00:55:36.860]   and just filters food as the food goes by.
[00:55:36.860 --> 00:55:41.580]   And then when the food concentration decreases,
[00:55:41.580 --> 00:55:46.580]   it just ejects itself, wriggles to some spot randomly,
[00:55:46.580 --> 00:55:50.500]   where probabilistically there will be more food,
[00:55:50.500 --> 00:55:51.980]   and plants itself again.
[00:55:51.980 --> 00:55:56.340]   So it's not really aware,
[00:55:56.340 --> 00:55:58.380]   very aware that it has an environment.
[00:55:58.380 --> 00:56:00.740]   It has a niche, but that niche is very small
[00:56:00.740 --> 00:56:05.260]   and it's not really experiencing that niche very much.
[00:56:05.260 --> 00:56:08.100]   So it's basically like a little stomach on a stick.
[00:56:08.100 --> 00:56:09.860]   That's really what it is.
[00:56:09.860 --> 00:56:14.860]   And, but when animals start to literally hunt each other,
[00:56:20.420 --> 00:56:23.180]   all of a sudden it becomes important
[00:56:23.180 --> 00:56:25.860]   to be able to sense your environment.
[00:56:25.860 --> 00:56:29.260]   'Cause you need to know, is that blob up ahead
[00:56:29.260 --> 00:56:31.760]   gonna eat me or should I eat it?
[00:56:31.760 --> 00:56:34.860]   And so all of a sudden you want,
[00:56:34.860 --> 00:56:36.980]   distance senses are very useful.
[00:56:36.980 --> 00:56:41.180]   And so in the water, distance senses are vision
[00:56:41.180 --> 00:56:44.100]   and a little bit hearing,
[00:56:44.100 --> 00:56:49.500]   olfaction, smelling, and touch.
[00:56:49.500 --> 00:56:51.780]   'Cause in the water, touch is a distance sense
[00:56:51.780 --> 00:56:53.540]   'cause you can feel the vibration.
[00:56:53.540 --> 00:56:59.900]   So on land, vision is a distance sense,
[00:56:59.900 --> 00:57:03.020]   touch not so much, but for elephants maybe.
[00:57:03.020 --> 00:57:06.180]   - The vibrations. - Vibrations.
[00:57:06.180 --> 00:57:10.580]   Olfaction definitely because of the concentration of,
[00:57:10.580 --> 00:57:12.260]   the more concentrated something is,
[00:57:12.260 --> 00:57:14.820]   the more likely it is to be close to you.
[00:57:14.820 --> 00:57:17.940]   So animals developed senses.
[00:57:17.940 --> 00:57:20.340]   They developed a head, like a literal head.
[00:57:20.340 --> 00:57:22.140]   So amphioxus doesn't even have a head really.
[00:57:22.140 --> 00:57:23.700]   It's just a long--
[00:57:23.700 --> 00:57:25.780]   - What's the purpose of a head?
[00:57:25.780 --> 00:57:27.380]   - That's a great question.
[00:57:27.380 --> 00:57:29.620]   - Is it to have a jaw?
[00:57:29.620 --> 00:57:30.820]   - That's a great question.
[00:57:30.820 --> 00:57:35.580]   So jaw, so yes, jaws are a major--
[00:57:35.580 --> 00:57:36.700]   - Useful feature?
[00:57:36.700 --> 00:57:39.260]   - Yeah, I would say they're a major adaptation
[00:57:39.260 --> 00:57:42.620]   after there's a split between vertebrates and invertebrates.
[00:57:42.620 --> 00:57:45.340]   So amphioxus is thought to be very, very similar
[00:57:45.340 --> 00:57:48.740]   to the animal that's before that split.
[00:57:48.740 --> 00:57:50.140]   But then after the development,
[00:57:50.140 --> 00:57:52.660]   very quickly after the development of a head
[00:57:52.660 --> 00:57:56.060]   is the development of a jaw, which is a big thing.
[00:57:56.060 --> 00:58:01.060]   And what goes along with that is the development of a brain.
[00:58:01.060 --> 00:58:04.140]   - It's weird, is that just a coincidence
[00:58:04.140 --> 00:58:07.820]   that the thing, the part of our body,
[00:58:07.820 --> 00:58:12.500]   of the mammal, I think, body that we eat with
[00:58:12.500 --> 00:58:15.300]   and attack others with is also the thing
[00:58:15.300 --> 00:58:20.300]   that contains all the majority of the brain type of stuff?
[00:58:20.300 --> 00:58:23.100]   - Well, actually, the brain goes
[00:58:23.100 --> 00:58:24.980]   with the development of a head
[00:58:24.980 --> 00:58:27.740]   and the development of a visual system
[00:58:27.740 --> 00:58:31.420]   and an auditory system and an olfactory system and so on.
[00:58:31.420 --> 00:58:34.780]   So your senses are developing
[00:58:34.780 --> 00:58:38.740]   and the other thing that's happening, right,
[00:58:38.740 --> 00:58:40.500]   is that animals are getting bigger.
[00:58:40.500 --> 00:58:44.580]   Because they're, and also their niche is getting bigger.
[00:58:44.580 --> 00:58:47.860]   - Well, this is the, just sorry to take a tiny tangent
[00:58:47.860 --> 00:58:50.380]   on the niche thing is it seems like the niche
[00:58:50.380 --> 00:58:53.380]   is getting bigger, but not just bigger,
[00:58:53.380 --> 00:58:56.660]   like more complicated, like shaped in weird ways.
[00:58:56.660 --> 00:58:59.000]   So like predation seems to create,
[00:58:59.000 --> 00:59:03.380]   like the whole world becomes your oyster, whatever.
[00:59:03.380 --> 00:59:05.980]   But like you also start to carve out
[00:59:05.980 --> 00:59:08.380]   the places in which you can operate the best.
[00:59:08.380 --> 00:59:10.500]   - Yeah, and in fact, that's absolutely right.
[00:59:10.500 --> 00:59:15.060]   And in fact, some scientists think that theory of mind,
[00:59:15.060 --> 00:59:18.660]   your ability to make inferences about the inner life
[00:59:18.660 --> 00:59:22.100]   of other creatures actually developed
[00:59:22.100 --> 00:59:24.300]   under the selection pressure of predation.
[00:59:24.300 --> 00:59:28.140]   Because it makes you a better predator.
[00:59:28.140 --> 00:59:31.220]   - Do you ever look at, you just said you looked at babies
[00:59:31.220 --> 00:59:35.020]   as these wiring creatures.
[00:59:35.020 --> 00:59:39.340]   Do you ever think of humans as just clever predators?
[00:59:39.340 --> 00:59:43.240]   Like that there is under, underneath it all is this,
[00:59:43.240 --> 00:59:49.500]   the Nietzschean will to power in all of its forms?
[00:59:49.500 --> 00:59:52.100]   Or are we now friendlier?
[00:59:52.100 --> 00:59:54.260]   - Yeah, so it's interesting.
[00:59:54.260 --> 00:59:57.300]   I mean, there are zeitgeists
[00:59:57.300 --> 00:59:59.700]   in how humans think about themselves, right?
[00:59:59.700 --> 01:00:02.480]   And so if you look in the 20th century,
[01:00:02.480 --> 01:00:08.060]   you can see that the idea of an inner beast
[01:00:08.060 --> 01:00:10.900]   that were just predators, were just basically animals,
[01:00:10.900 --> 01:00:13.340]   baseless animals, violent animals
[01:00:13.340 --> 01:00:15.260]   that have to be contained by culture
[01:00:15.260 --> 01:00:17.760]   and by our prodigious neocortex,
[01:00:17.760 --> 01:00:24.100]   really took hold particularly after World War I
[01:00:24.100 --> 01:00:30.340]   and really held sway for much of that century.
[01:00:30.340 --> 01:00:36.980]   And then around, at least in Western writing, I would say,
[01:00:36.980 --> 01:00:41.140]   we're talking mainly about Western scientific writing,
[01:00:41.140 --> 01:00:42.780]   Western philosophical writing.
[01:00:42.780 --> 01:00:47.160]   And then late '90s maybe,
[01:00:47.160 --> 01:00:52.200]   you start to see books and articles about our social nature,
[01:00:52.200 --> 01:00:53.540]   that we're social animals.
[01:00:53.540 --> 01:00:56.860]   And we are social animals, but what does that mean exactly?
[01:00:56.860 --> 01:01:00.480]   About--
[01:01:00.480 --> 01:01:02.020]   - It's us carving out different niches
[01:01:02.020 --> 01:01:03.460]   in the space of ideas, it looks like.
[01:01:03.460 --> 01:01:04.660]   - I think so, I think so.
[01:01:06.460 --> 01:01:11.460]   So, do humans, can humans be violent?
[01:01:11.460 --> 01:01:15.540]   Yes.
[01:01:15.540 --> 01:01:18.220]   Can humans be really helpful?
[01:01:18.220 --> 01:01:19.720]   Yes, actually.
[01:01:19.720 --> 01:01:22.920]   And humans are interesting creatures
[01:01:22.920 --> 01:01:27.920]   because other animals can also be helpful to one another.
[01:01:27.920 --> 01:01:30.940]   In fact, there's a whole literature, booming literature
[01:01:30.940 --> 01:01:35.140]   on how other animals are,
[01:01:35.340 --> 01:01:38.580]   support one another.
[01:01:38.580 --> 01:01:40.660]   They regulate each other's nervous systems
[01:01:40.660 --> 01:01:41.740]   in interesting ways
[01:01:41.740 --> 01:01:43.580]   and they will be helpful to one another, right?
[01:01:43.580 --> 01:01:46.440]   So for example, there's a whole literature on rodents
[01:01:46.440 --> 01:01:51.440]   and how they signal one another, what is safe to eat?
[01:01:51.440 --> 01:01:57.040]   And they will perform acts of generosity
[01:01:57.040 --> 01:02:03.220]   to their conspecifics that are related to them
[01:02:03.220 --> 01:02:05.400]   or who they were raised with.
[01:02:05.400 --> 01:02:08.100]   So if an animal was raised in a litter
[01:02:08.100 --> 01:02:09.900]   that they were raised in,
[01:02:09.900 --> 01:02:11.500]   although not even at the same time,
[01:02:11.500 --> 01:02:13.460]   they'll be more likely to help that animal.
[01:02:13.460 --> 01:02:16.940]   So there's always some kind of physical relationship
[01:02:16.940 --> 01:02:20.620]   between animals that predicts whether or not
[01:02:20.620 --> 01:02:22.060]   they'll help one another.
[01:02:22.060 --> 01:02:27.060]   For humans, humans, you know,
[01:02:27.060 --> 01:02:31.560]   we have ways of categorizing who's in our group
[01:02:31.560 --> 01:02:34.860]   and who isn't by non-physical ways, right?
[01:02:34.860 --> 01:02:38.060]   Even by just something abstract like an idea.
[01:02:38.060 --> 01:02:41.980]   And we are much more likely to extend help
[01:02:41.980 --> 01:02:43.920]   to people in our own group,
[01:02:43.920 --> 01:02:47.020]   whatever that group may be at that moment,
[01:02:47.020 --> 01:02:51.060]   whatever feature you're using to define who's in your group
[01:02:51.060 --> 01:02:55.860]   and who isn't, we're more likely to help those people
[01:02:55.860 --> 01:02:59.260]   than even members of our own family at times.
[01:02:59.260 --> 01:03:03.280]   So humans are much more flexible in their,
[01:03:03.280 --> 01:03:07.000]   in the way that they help one another,
[01:03:07.000 --> 01:03:08.900]   but also in the way that they harm one another.
[01:03:08.900 --> 01:03:13.900]   So I don't, I don't think I subscribe to,
[01:03:13.900 --> 01:03:21.040]   you know, we are primarily this or we are primarily that.
[01:03:21.040 --> 01:03:24.340]   I don't think humans have essences in that way, really.
[01:03:24.340 --> 01:03:27.320]   - I apologize to take us in this direction
[01:03:27.320 --> 01:03:29.860]   for a brief moment, but I've been really deep
[01:03:29.860 --> 01:03:34.100]   on Stalin and Hitler recently in terms of reading.
[01:03:34.100 --> 01:03:37.900]   And is there something that you think about
[01:03:37.900 --> 01:03:41.620]   in terms of the nature of evil
[01:03:41.620 --> 01:03:44.060]   from a neuroscience perspective?
[01:03:44.060 --> 01:03:49.060]   Is there some lessons that are sort of hopeful
[01:03:49.060 --> 01:03:57.260]   about human civilization that we can find
[01:03:57.260 --> 01:04:00.280]   in our brain with regard to the Hitlers of the world?
[01:04:00.280 --> 01:04:05.380]   Do you think about the nature of evil?
[01:04:05.380 --> 01:04:07.460]   - Yeah, I do.
[01:04:07.460 --> 01:04:12.460]   I don't know that what I have to say is so useful from a,
[01:04:12.460 --> 01:04:14.140]   I don't know that I can say as a neuroscientist,
[01:04:14.140 --> 01:04:17.740]   well, here's a study that, you know,
[01:04:17.740 --> 01:04:20.220]   so I sort of have to take off my lab coat, right?
[01:04:20.220 --> 01:04:22.840]   And now I'm gonna now conjecture as a human
[01:04:22.840 --> 01:04:24.700]   who just also, who has opinions,
[01:04:24.700 --> 01:04:26.700]   but who also maybe has some knowledge
[01:04:26.700 --> 01:04:29.300]   about neuroscience, but I'm not speaking
[01:04:29.300 --> 01:04:30.660]   as a neuroscientist when I say this
[01:04:30.660 --> 01:04:33.720]   'cause I don't think neuroscientists know enough, really,
[01:04:33.720 --> 01:04:36.100]   to be able to say, but I guess,
[01:04:36.100 --> 01:04:38.040]   the kinds of things I think about are,
[01:04:38.040 --> 01:04:44.260]   what, so I have always thought,
[01:04:44.260 --> 01:04:47.260]   even before I knew anything about neuroscience,
[01:04:47.260 --> 01:04:49.700]   I've always thought that,
[01:04:49.700 --> 01:04:54.980]   I don't think anybody could become Hitler,
[01:04:54.980 --> 01:04:58.520]   but I think the majority of people can be,
[01:04:58.520 --> 01:05:02.760]   are capable of doing very bad things.
[01:05:02.760 --> 01:05:06.660]   It's just, the question is really,
[01:05:06.660 --> 01:05:09.240]   how much encouragement does it take from the environment
[01:05:09.240 --> 01:05:11.300]   to get them to do something bad?
[01:05:11.300 --> 01:05:14.460]   - That's what I, kind of when I look at the life of Hitler,
[01:05:14.460 --> 01:05:19.460]   it seems like there's so many places where--
[01:05:19.460 --> 01:05:20.980]   - Something could have intervened.
[01:05:20.980 --> 01:05:23.380]   - Intervened, no, it could change completely the person.
[01:05:23.380 --> 01:05:25.260]   I mean, there's the caricature,
[01:05:25.260 --> 01:05:28.500]   like the obvious places where he was an artist,
[01:05:28.500 --> 01:05:30.380]   and if he wasn't rejected as an artist,
[01:05:30.380 --> 01:05:32.060]   he was a reasonably good artist,
[01:05:32.060 --> 01:05:34.420]   so that could have changed, but just his entire,
[01:05:34.420 --> 01:05:37.740]   like where he went in Vienna and all these kinds of things,
[01:05:37.740 --> 01:05:39.980]   like little interactions could have changed,
[01:05:39.980 --> 01:05:44.260]   and there's probably millions of other people
[01:05:44.260 --> 01:05:49.260]   who are capable, who the environment may be able to mold
[01:05:49.260 --> 01:05:51.760]   in the same way it did this particular person
[01:05:51.760 --> 01:05:55.940]   to create this particular kind of charismatic leader
[01:05:55.940 --> 01:05:57.540]   in this particular moment of time.
[01:05:57.540 --> 01:06:01.340]   - Absolutely, and I guess the way that I would say it,
[01:06:01.340 --> 01:06:02.660]   I would agree 100%,
[01:06:02.660 --> 01:06:05.300]   and I guess the way that I would say it is like this.
[01:06:05.300 --> 01:06:10.380]   In the West, we have a way of reasoning
[01:06:10.380 --> 01:06:17.100]   about causation, which focuses on single,
[01:06:17.100 --> 01:06:19.080]   simple causes for things.
[01:06:20.380 --> 01:06:22.680]   There's an essence to Hitler,
[01:06:22.680 --> 01:06:24.800]   there's an essence to his character.
[01:06:24.800 --> 01:06:26.640]   He was born with that essence,
[01:06:26.640 --> 01:06:30.180]   or it was forged very, very early in his life,
[01:06:30.180 --> 01:06:35.180]   and that explains the landscape of his,
[01:06:35.180 --> 01:06:37.880]   the horrible landscape of his behavior,
[01:06:37.880 --> 01:06:41.120]   but there's another way to think about it,
[01:06:41.120 --> 01:06:42.820]   a way that actually is much more consistent
[01:06:42.820 --> 01:06:45.760]   with what we know about biology,
[01:06:45.760 --> 01:06:49.080]   how biology works in the physical world,
[01:06:49.080 --> 01:06:52.160]   and that is that most things are complex,
[01:06:52.160 --> 01:06:54.280]   not as in, wow, this is really complex and hard,
[01:06:54.280 --> 01:06:56.880]   but complex as in complexity,
[01:06:56.880 --> 01:06:59.800]   that is more than the sum of their parts,
[01:06:59.800 --> 01:07:03.880]   and that most phenomena have many, many
[01:07:03.880 --> 01:07:08.760]   weak, nonlinear interacting causes,
[01:07:08.760 --> 01:07:13.760]   and so little things that we might not even be aware of
[01:07:13.760 --> 01:07:17.200]   can shift someone's developmental trajectory
[01:07:17.200 --> 01:07:20.000]   from this to that, and that's enough
[01:07:20.000 --> 01:07:23.280]   to take it on a whole set of other paths,
[01:07:23.280 --> 01:07:28.400]   and that these things are happening all the time.
[01:07:28.400 --> 01:07:31.240]   So it's not random, and it's not really,
[01:07:31.240 --> 01:07:32.840]   it's not deterministic in the sense
[01:07:32.840 --> 01:07:35.940]   that everything you do determines your outcome,
[01:07:35.940 --> 01:07:37.880]   but it's a little more like
[01:07:37.880 --> 01:07:44.520]   you're nudging someone from one set of possibilities
[01:07:44.520 --> 01:07:46.640]   to another set of possibilities,
[01:07:46.640 --> 01:07:48.560]   but I think the thing is,
[01:07:48.560 --> 01:07:50.000]   the thing that I find optimistic
[01:07:50.000 --> 01:07:55.000]   is that the other side of that coin is also true, right?
[01:07:55.000 --> 01:08:00.920]   So look at all the people who risked their lives
[01:08:00.920 --> 01:08:05.320]   to help people they didn't even know.
[01:08:05.320 --> 01:08:10.000]   I mean, I just watched "Borat," the new "Borat" movie,
[01:08:10.000 --> 01:08:12.460]   and the thing that I came away with,
[01:08:12.460 --> 01:08:15.040]   but you know, the thing I came away with was
[01:08:16.080 --> 01:08:19.720]   look at how generous people were in that,
[01:08:19.720 --> 01:08:20.720]   or 'cause he's making,
[01:08:20.720 --> 01:08:23.460]   there are a lot of people he makes fun of, and that's fine,
[01:08:23.460 --> 01:08:27.000]   but think about those two guys, those--
[01:08:27.000 --> 01:08:28.440]   - The Trump supporter guys.
[01:08:28.440 --> 01:08:29.960]   - The Trump supporter guys.
[01:08:29.960 --> 01:08:31.120]   Those guys-- - That was cool.
[01:08:31.120 --> 01:08:33.180]   - Those kindness in them, right?
[01:08:33.180 --> 01:08:38.180]   - They took a complete stranger in a pandemic
[01:08:38.180 --> 01:08:40.380]   into their house.
[01:08:40.380 --> 01:08:42.920]   Who does that?
[01:08:42.920 --> 01:08:44.720]   Like, that's a really nice thing,
[01:08:44.720 --> 01:08:46.160]   or there's one scene,
[01:08:46.160 --> 01:08:47.800]   I mean, I don't wanna spoil it for people
[01:08:47.800 --> 01:08:49.320]   who haven't seen it,
[01:08:49.320 --> 01:08:51.920]   but there's one scene where he goes in,
[01:08:51.920 --> 01:08:53.620]   he dresses up as a Jew.
[01:08:53.620 --> 01:08:58.520]   I laugh myself sick at that scene, seriously,
[01:08:58.520 --> 01:09:03.080]   but he goes in, and there are these two old Jewish ladies.
[01:09:03.080 --> 01:09:09.160]   What a bunch of sweethearts, oh my gosh, like, really?
[01:09:09.160 --> 01:09:12.240]   I mean, that was what I was struck by, actually.
[01:09:12.240 --> 01:09:15.440]   I mean, there are other ones, or like the babysitter, right?
[01:09:15.440 --> 01:09:18.560]   I mean, she was really kind,
[01:09:18.560 --> 01:09:22.840]   and yeah, so that's really what I was more struck by.
[01:09:22.840 --> 01:09:28.640]   Sure, there are other people who do very bad things,
[01:09:28.640 --> 01:09:30.520]   or say bad things, or whatever,
[01:09:30.520 --> 01:09:35.160]   but there's one guy who's completely stoic,
[01:09:35.160 --> 01:09:38.280]   like the guy who's doing the,
[01:09:38.280 --> 01:09:41.760]   sending the messages, I don't know if it's fax or whatever.
[01:09:41.760 --> 01:09:43.440]   He's just completely stoic,
[01:09:43.440 --> 01:09:45.600]   but he's doing his job, actually.
[01:09:45.600 --> 01:09:48.880]   You don't know what he was thinking inside his head.
[01:09:48.880 --> 01:09:49.840]   You don't know what he was feeling,
[01:09:49.840 --> 01:09:52.140]   but he was totally professional doing his job.
[01:09:52.140 --> 01:09:58.080]   So I guess I just, I had a bit of a different view, I guess,
[01:09:58.080 --> 01:10:00.420]   so I also think that about people.
[01:10:00.420 --> 01:10:04.020]   I think everybody is capable of kindness,
[01:10:04.020 --> 01:10:08.820]   but the question is, how much does it take,
[01:10:08.820 --> 01:10:09.880]   and what are the circumstances?
[01:10:09.880 --> 01:10:12.080]   So for some people, it's gonna take a lot,
[01:10:12.080 --> 01:10:14.360]   and for some people, it only takes a little bit,
[01:10:14.360 --> 01:10:19.360]   but are we actually cultivating an environment
[01:10:19.360 --> 01:10:26.680]   for the next generation that provides opportunities
[01:10:26.680 --> 01:10:32.280]   for people to go in the direction of caring and kindness?
[01:10:32.280 --> 01:10:33.120]   - Yeah.
[01:10:33.120 --> 01:10:38.120]   - Or, and I'm not saying that as like a Pollyanna-ish person.
[01:10:39.840 --> 01:10:42.600]   I think there's a lot of room for competition
[01:10:42.600 --> 01:10:45.240]   and debate and so on,
[01:10:45.240 --> 01:10:49.100]   but I don't see Hitler as an anomaly,
[01:10:49.100 --> 01:10:50.200]   and I never have.
[01:10:50.200 --> 01:10:52.680]   That was even before I learned anything about neuroscience,
[01:10:52.680 --> 01:10:55.000]   and now, I would say, knowing what we know
[01:10:55.000 --> 01:10:57.280]   about developmental trajectories and life histories
[01:10:57.280 --> 01:10:58.640]   and how important that is,
[01:10:58.640 --> 01:11:03.600]   knowing what we know about the whole question
[01:11:03.600 --> 01:11:07.860]   of nature versus nurture is a completely wrong question.
[01:11:07.860 --> 01:11:11.000]   We have the kind of nature that requires nurture.
[01:11:11.000 --> 01:11:14.960]   We have the kind of genes that allow infants to be born
[01:11:14.960 --> 01:11:19.800]   with unfinished brains, where their brains are wired
[01:11:19.800 --> 01:11:23.040]   across a 25-year period with wiring instructions
[01:11:23.040 --> 01:11:25.960]   from the world that is created for them,
[01:11:25.960 --> 01:11:29.920]   and so I don't think Hitler is an anomaly.
[01:11:29.920 --> 01:11:37.020]   Even if it's less probable that that would happen,
[01:11:37.020 --> 01:11:39.160]   it's possible that it could happen again,
[01:11:39.160 --> 01:11:43.280]   and it's not like he's a bad seed.
[01:11:43.280 --> 01:11:45.640]   I mean, that doesn't, I just wanna say,
[01:11:45.640 --> 01:11:48.720]   of course, he's completely 100% responsible for his actions
[01:11:48.720 --> 01:11:50.120]   and all the bad things that happen,
[01:11:50.120 --> 01:11:53.400]   so I'm not in any way, this is not me saying--
[01:11:53.400 --> 01:11:56.240]   - But the environment is also responsible, in part,
[01:11:56.240 --> 01:11:59.240]   for creating the evil in this world,
[01:11:59.240 --> 01:12:04.240]   so like Hitler's in different versions of,
[01:12:04.440 --> 01:12:07.500]   more subtle, more smaller-scale versions of evil,
[01:12:07.500 --> 01:12:12.500]   but I tend to believe that there's a much stronger,
[01:12:12.500 --> 01:12:16.340]   I don't like to talk about evolutionary advantages,
[01:12:16.340 --> 01:12:20.500]   but it seems like it makes sense for love
[01:12:20.500 --> 01:12:25.220]   to be a more powerful, emergent phenomena
[01:12:25.220 --> 01:12:26.700]   of our collective intelligence
[01:12:26.700 --> 01:12:30.540]   versus hate and evil and destruction,
[01:12:30.540 --> 01:12:34.380]   because from a survival, from a niche perspective,
[01:12:34.380 --> 01:12:38.600]   it seems to be, like, in my own life,
[01:12:38.600 --> 01:12:40.280]   in my thinking about the intuition
[01:12:40.280 --> 01:12:44.580]   about the way humans work together to solve problems,
[01:12:44.580 --> 01:12:47.780]   it seems that love is a very useful tool.
[01:12:47.780 --> 01:12:50.080]   - I definitely agree with you,
[01:12:50.080 --> 01:12:55.080]   but I think the caveat here is that, you know, humans,
[01:12:55.080 --> 01:13:00.680]   the research suggests that humans are capable
[01:13:00.680 --> 01:13:04.600]   of great acts of kindness and great acts of generosity
[01:13:04.600 --> 01:13:06.520]   to people in their in-group.
[01:13:06.520 --> 01:13:08.200]   - Right.
[01:13:08.200 --> 01:13:10.880]   - And-- - So we're also tribal.
[01:13:10.880 --> 01:13:14.420]   - Yeah, I mean, that's the kitschy way to say it.
[01:13:14.420 --> 01:13:16.600]   We're tribes, we're tribal, yeah.
[01:13:16.600 --> 01:13:18.300]   So that's the kitschy way to say it.
[01:13:18.300 --> 01:13:22.360]   What I would say is that, you know,
[01:13:22.360 --> 01:13:24.620]   there are a lot of features
[01:13:24.620 --> 01:13:28.760]   that you can use to describe yourself.
[01:13:28.760 --> 01:13:30.160]   You don't have one identity,
[01:13:30.160 --> 01:13:32.200]   you don't have one self, you have many selves,
[01:13:32.200 --> 01:13:33.620]   you have many identities.
[01:13:33.620 --> 01:13:37.440]   Sometimes you're a man, sometimes you're a scientist,
[01:13:37.440 --> 01:13:40.160]   sometimes you're a, do you have a brother or a sister?
[01:13:40.160 --> 01:13:41.000]   - Yeah, brother.
[01:13:41.000 --> 01:13:43.680]   - So sometimes you're a brother, you know, you,
[01:13:43.680 --> 01:13:45.160]   sometimes you're a friend.
[01:13:45.160 --> 01:13:47.680]   - Sometimes you're a human, so you can keep zooming out.
[01:13:47.680 --> 01:13:49.700]   - Yes, exactly. - Living organism on Earth.
[01:13:49.700 --> 01:13:53.440]   - Yes, exactly, that's exactly, that's exactly right.
[01:13:53.440 --> 01:13:58.440]   And so there are some people who,
[01:13:59.360 --> 01:14:01.860]   there is research which suggests
[01:14:01.860 --> 01:14:05.200]   that there are some people who will tell you,
[01:14:05.200 --> 01:14:08.680]   I think it's appropriate and better to help,
[01:14:08.680 --> 01:14:11.460]   I should help my family more than I should help my neighbors
[01:14:11.460 --> 01:14:12.880]   and I should help my neighbors more
[01:14:12.880 --> 01:14:15.400]   than I should help the average stranger
[01:14:15.400 --> 01:14:18.420]   and I should help, you know,
[01:14:18.420 --> 01:14:20.040]   the average stranger in my country
[01:14:20.040 --> 01:14:22.160]   more than I should help somebody outside my country
[01:14:22.160 --> 01:14:25.520]   and I should help humans more than I should help,
[01:14:25.520 --> 01:14:27.080]   you know, other animals and I should, right?
[01:14:27.080 --> 01:14:29.240]   So there's a clear hierarchy of helping.
[01:14:29.240 --> 01:14:33.160]   And there are other people who, you know,
[01:14:33.160 --> 01:14:37.160]   they are, their niche is much more inclusive, right?
[01:14:37.160 --> 01:14:40.960]   And that they're humans first, right?
[01:14:40.960 --> 01:14:43.700]   Or creatures of the Earth first, let's say.
[01:14:43.700 --> 01:14:50.440]   And I don't think we know how flexible those attitudes are
[01:14:50.440 --> 01:14:54.000]   because I don't think the research really tells us that,
[01:14:54.000 --> 01:14:56.880]   but in any case, there are, you know,
[01:14:56.880 --> 01:15:00.000]   and there are beliefs, people also have beliefs about,
[01:15:00.000 --> 01:15:02.280]   there's this really interesting research
[01:15:02.280 --> 01:15:06.760]   in really in anthropology that looks at
[01:15:06.760 --> 01:15:12.440]   what are cultures particularly afraid of?
[01:15:12.440 --> 01:15:15.200]   Like what, the people in a particular culture
[01:15:15.200 --> 01:15:17.400]   are organizing their social systems
[01:15:17.400 --> 01:15:20.160]   to prevent certain types of problems.
[01:15:20.160 --> 01:15:21.880]   So what are the problems that they're worried about?
[01:15:21.880 --> 01:15:23.880]   And so there are some cultures
[01:15:23.880 --> 01:15:25.980]   that are much more hierarchical
[01:15:25.980 --> 01:15:30.560]   and some cultures that are, you know, much more egalitarian.
[01:15:30.560 --> 01:15:32.420]   There are some cultures that, you know,
[01:15:32.420 --> 01:15:35.740]   in the debate of like getting along versus getting ahead,
[01:15:35.740 --> 01:15:38.700]   there are some cultures that really prioritize
[01:15:38.700 --> 01:15:40.140]   the individual over the group.
[01:15:40.140 --> 01:15:41.900]   And there are other cultures that really prioritize
[01:15:41.900 --> 01:15:43.340]   the group over the individual.
[01:15:43.340 --> 01:15:45.380]   You know, it's not like one of these is right
[01:15:45.380 --> 01:15:46.660]   and one of these is wrong.
[01:15:46.660 --> 01:15:48.940]   It's that, you know, different combinations
[01:15:48.940 --> 01:15:51.460]   of these features are different solutions
[01:15:51.460 --> 01:15:55.660]   that humans have come up with for living in groups,
[01:15:55.660 --> 01:15:58.320]   which is a major adaptive advantage of our species.
[01:15:58.320 --> 01:16:02.700]   And it's not the case that one of these is better
[01:16:02.700 --> 01:16:03.860]   and one of these is worse.
[01:16:03.860 --> 01:16:07.380]   Although as a person, of course, I have opinions about that.
[01:16:07.380 --> 01:16:10.660]   And as a person, I can say,
[01:16:10.660 --> 01:16:12.860]   I would very much prefer certain,
[01:16:12.860 --> 01:16:15.340]   I have certain beliefs and I really want everyone
[01:16:15.340 --> 01:16:17.400]   in the world to live by those beliefs, you know.
[01:16:17.400 --> 01:16:21.420]   But as a scientist, I know that it's not really the case
[01:16:21.420 --> 01:16:25.940]   that for the species, any one of these is better
[01:16:25.940 --> 01:16:26.980]   than any other.
[01:16:26.980 --> 01:16:29.960]   There are different solutions that work differentially well
[01:16:29.960 --> 01:16:34.960]   in particular, you know, ecological parts of the world.
[01:16:34.960 --> 01:16:40.900]   But for individual humans, there are definitely
[01:16:40.900 --> 01:16:42.140]   some systems that are better
[01:16:42.140 --> 01:16:43.860]   and some systems that are worse, right?
[01:16:43.860 --> 01:16:46.860]   But when anthropologists or when neuroscientists
[01:16:46.860 --> 01:16:48.420]   or biologists are talking,
[01:16:48.860 --> 01:16:52.060]   not usually talking about the lives of individual people,
[01:16:52.060 --> 01:16:54.460]   they're talking about, you know, the species,
[01:16:54.460 --> 01:16:55.700]   what's better for the species,
[01:16:55.700 --> 01:16:57.420]   the survivability of the species.
[01:16:57.420 --> 01:17:00.020]   And what's better for the survivability of the species
[01:17:00.020 --> 01:17:03.700]   is variation, that we have lots of cultures
[01:17:03.700 --> 01:17:05.820]   with lots of different solutions
[01:17:05.820 --> 01:17:09.900]   because if the environment were to change drastically,
[01:17:09.900 --> 01:17:17.420]   some of those solutions will work better than others.
[01:17:17.620 --> 01:17:21.540]   And you can see that happening with COVID.
[01:17:21.540 --> 01:17:23.820]   - Right, so some people might be more susceptible
[01:17:23.820 --> 01:17:26.300]   to this virus than others.
[01:17:26.300 --> 01:17:28.420]   And so variation is very useful.
[01:17:28.420 --> 01:17:32.060]   Say COVID was much, much more destructive than it is
[01:17:32.060 --> 01:17:36.040]   and like, I don't know, 20% of the population died.
[01:17:36.040 --> 01:17:40.220]   You know, it's good to have variability
[01:17:40.220 --> 01:17:42.900]   because then at least some percent will survive.
[01:17:42.900 --> 01:17:46.580]   - Yeah, I mean, you know, the way that I used to describe it
[01:17:46.580 --> 01:17:51.340]   was, you know, using, you know, those movies
[01:17:51.340 --> 01:17:55.220]   like "The War of the Worlds" or "Pacific Rim,"
[01:17:55.220 --> 01:17:58.400]   you know, where like aliens come down from outer space
[01:17:58.400 --> 01:18:01.260]   and they, you know, wanna kill humans.
[01:18:01.260 --> 01:18:04.220]   And so all the humans band together as a species,
[01:18:04.220 --> 01:18:06.540]   like, and they all, like all the, you know,
[01:18:06.540 --> 01:18:08.940]   little squabbling from countries and whatever,
[01:18:08.940 --> 01:18:10.500]   all, you know, goes away.
[01:18:10.500 --> 01:18:13.380]   And everyone is just one big, you know,
[01:18:13.380 --> 01:18:18.380]   well, that, you know, that doesn't happen.
[01:18:18.380 --> 01:18:24.740]   I mean, 'cause COVID is, you know, a virus like COVID-19
[01:18:24.740 --> 01:18:29.980]   is like a creature from outer space.
[01:18:29.980 --> 01:18:31.820]   And that's not what you see happening.
[01:18:31.820 --> 01:18:35.340]   What you do see happening, it is true that some people,
[01:18:35.340 --> 01:18:36.860]   I mean, we could use this as an example
[01:18:36.860 --> 01:18:37.900]   of essentialism also.
[01:18:37.900 --> 01:18:40.580]   So just to say like exposure to the virus
[01:18:40.580 --> 01:18:43.180]   does not mean that you will become infected
[01:18:43.180 --> 01:18:44.300]   with a disease.
[01:18:44.300 --> 01:18:48.220]   So, I mean, in controlled studies,
[01:18:48.220 --> 01:18:51.580]   one of which was actually a coronavirus, not COVID,
[01:18:51.580 --> 01:18:53.540]   but this was, these are studies from 10,
[01:18:53.540 --> 01:18:55.460]   10 or so years ago, you know,
[01:18:55.460 --> 01:18:58.660]   only somewhere between 20 and 40% of people
[01:18:58.660 --> 01:19:02.220]   were developed respiratory illness
[01:19:02.220 --> 01:19:04.700]   when a virus was placed in their nose.
[01:19:04.700 --> 01:19:05.540]   - Yeah.
[01:19:05.540 --> 01:19:07.180]   - And so--
[01:19:07.180 --> 01:19:09.100]   - And there's a dose question, all those--
[01:19:09.100 --> 01:19:10.700]   - Well, not in these studies, actually.
[01:19:10.700 --> 01:19:13.220]   So in these studies, the dose was consistent
[01:19:13.220 --> 01:19:14.540]   across all people.
[01:19:14.540 --> 01:19:17.900]   And everything, you know, they were sequestered
[01:19:17.900 --> 01:19:21.420]   in hotel rooms and what they ate was, you know,
[01:19:21.420 --> 01:19:23.260]   measured out by scientists and so on.
[01:19:23.260 --> 01:19:25.660]   And so when you hold dose, I mean,
[01:19:25.660 --> 01:19:28.020]   the dose issue is a real issue in the real world,
[01:19:28.020 --> 01:19:31.980]   but in these studies, that was controlled.
[01:19:31.980 --> 01:19:35.420]   And only somewhere between 20, depending on the study,
[01:19:35.420 --> 01:19:38.740]   between 20 and 40% of people became infected with a disease.
[01:19:38.740 --> 01:19:43.340]   So exposure to a virus doesn't mean de facto
[01:19:43.340 --> 01:19:46.340]   that you will develop an illness.
[01:19:46.340 --> 01:19:49.500]   You will be a carrier and you will spread the virus
[01:19:49.500 --> 01:19:52.540]   to other people, but you yourself may not,
[01:19:52.540 --> 01:19:55.820]   your immune system may be in a state
[01:19:55.820 --> 01:19:58.740]   that you can make enough antibodies
[01:19:58.740 --> 01:20:03.460]   to not show symptoms, not develop symptoms.
[01:20:03.460 --> 01:20:08.300]   And so, of course, what this means is,
[01:20:08.300 --> 01:20:11.340]   again, is that, you know, like if I asked you,
[01:20:11.340 --> 01:20:16.340]   do you think a virus is the cause of a common cold,
[01:20:16.340 --> 01:20:19.780]   or, you know, most people, if I asked this question,
[01:20:19.780 --> 01:20:21.740]   I can tell you, 'cause I asked this question.
[01:20:21.740 --> 01:20:25.740]   So do you think a virus is the cause of a cold?
[01:20:25.740 --> 01:20:27.740]   Most people would say, yes, I think it is.
[01:20:27.740 --> 01:20:30.560]   And then I say, yeah, well, only 20 to 40% of people
[01:20:30.560 --> 01:20:34.260]   develop respiratory illness in exposure to a virus.
[01:20:34.260 --> 01:20:37.980]   So clearly it is a necessary cause,
[01:20:37.980 --> 01:20:39.620]   but it's not a sufficient cause.
[01:20:39.620 --> 01:20:41.020]   And there are other causes, again,
[01:20:41.020 --> 01:20:44.060]   so not simple single causes for things, right?
[01:20:44.060 --> 01:20:47.500]   Multiple interacting influences.
[01:20:47.500 --> 01:20:50.380]   So it is true that individuals vary
[01:20:50.380 --> 01:20:53.520]   in their susceptibility to illness upon exposure,
[01:20:53.520 --> 01:20:57.860]   but different cultures have different sets of norms
[01:20:57.860 --> 01:21:02.220]   and practices that allow, that will slow
[01:21:02.220 --> 01:21:04.680]   or speed the spread.
[01:21:05.780 --> 01:21:08.780]   And that's the point that I was actually trying to make here
[01:21:08.780 --> 01:21:13.780]   that, you know, when the environment changes,
[01:21:13.780 --> 01:21:18.700]   that is, there's a mutation of a virus
[01:21:18.700 --> 01:21:21.580]   that is incredibly infectious,
[01:21:21.580 --> 01:21:25.500]   some cultures will succumb,
[01:21:25.500 --> 01:21:27.560]   people in some cultures will succumb faster
[01:21:27.560 --> 01:21:32.340]   because of the particular norms and practices
[01:21:32.340 --> 01:21:35.580]   that they've developed in their culture
[01:21:35.580 --> 01:21:36.580]   versus other cultures.
[01:21:36.580 --> 01:21:40.540]   Now, there could be some other, you know,
[01:21:40.540 --> 01:21:41.980]   thing that changes that,
[01:21:41.980 --> 01:21:46.420]   where those other cultures, you know, would do better.
[01:21:46.420 --> 01:21:49.820]   So very individualistic cultures like ours
[01:21:49.820 --> 01:21:53.900]   may do much better under other types of selection pressures.
[01:21:53.900 --> 01:21:58.140]   But for COVID, for things like COVID, you know,
[01:21:58.140 --> 01:22:00.040]   my colleague, Michelle Gelfand,
[01:22:00.040 --> 01:22:04.000]   her research shows that she looks at like loose cultures
[01:22:04.000 --> 01:22:07.640]   and tight cultures, so cultures that have very, very strict
[01:22:07.640 --> 01:22:10.440]   rules versus cultures that are much more individualistic
[01:22:10.440 --> 01:22:14.220]   and where personal freedoms are more valued.
[01:22:14.220 --> 01:22:17.200]   And she, you know, her research suggests
[01:22:17.200 --> 01:22:20.520]   that for pandemic circumstances,
[01:22:20.520 --> 01:22:23.480]   tight cultures actually, the people survive better.
[01:22:23.480 --> 01:22:27.200]   - She's still lingering a little bit longer.
[01:22:27.200 --> 01:22:30.640]   We started this part of the conversation talking about,
[01:22:30.640 --> 01:22:33.360]   you know, did humans evolve to think,
[01:22:33.360 --> 01:22:36.000]   did the human brain evolve to think,
[01:22:36.000 --> 01:22:39.480]   implying is there like a progress to the thing
[01:22:39.480 --> 01:22:41.480]   that's always improving?
[01:22:41.480 --> 01:22:42.320]   (laughing)
[01:22:42.320 --> 01:22:43.600]   - That's right, we never, yeah.
[01:22:43.600 --> 01:22:46.040]   And so the answer is no.
[01:22:46.040 --> 01:22:47.840]   - But let me sort of push back.
[01:22:47.840 --> 01:22:51.000]   But so your intuition is very strong here,
[01:22:51.000 --> 01:22:54.000]   not your intuition, the way you described this,
[01:22:54.000 --> 01:22:58.560]   but is it possible there's a direction to this evolution?
[01:22:58.560 --> 01:23:01.920]   Like, do you think of this evolution as having a direction?
[01:23:01.920 --> 01:23:04.620]   Like it's like walking along a certain path
[01:23:04.620 --> 01:23:06.520]   towards something?
[01:23:06.520 --> 01:23:08.960]   'Cause we, you know, what is it?
[01:23:08.960 --> 01:23:11.200]   (laughing)
[01:23:11.200 --> 01:23:16.720]   Is it Elon Musk said like the earth got bombarded
[01:23:16.720 --> 01:23:20.680]   with photons and then all of a sudden,
[01:23:20.680 --> 01:23:23.200]   like a Tesla was launched into space or whatever,
[01:23:23.200 --> 01:23:24.440]   a rocket started coming.
[01:23:24.440 --> 01:23:26.800]   Like, is there a sense in which,
[01:23:26.800 --> 01:23:30.800]   even though in the, like within the system,
[01:23:30.800 --> 01:23:33.160]   the evolution seems to be this mess of variation,
[01:23:33.160 --> 01:23:36.000]   we're kind of trying to find our niches and so on.
[01:23:36.000 --> 01:23:38.800]   But do you think there ultimately when you zoom out,
[01:23:38.800 --> 01:23:40.900]   there is a direction that's strong,
[01:23:40.900 --> 01:23:45.900]   that does tend towards greater complexity and intelligence?
[01:23:45.900 --> 01:23:50.120]   - No.
[01:23:50.120 --> 01:23:50.960]   (laughing)
[01:23:50.960 --> 01:23:53.640]   So, I mean, and I, and again, what I would say is,
[01:23:53.640 --> 01:23:58.280]   I'm really just echoing people who are much smarter
[01:23:58.280 --> 01:24:00.120]   than I am about this.
[01:24:00.120 --> 01:24:01.600]   - I see you're saying smarter.
[01:24:01.600 --> 01:24:03.400]   I thought it doesn't, there's no,
[01:24:03.400 --> 01:24:04.960]   I thought there's no smarter.
[01:24:04.960 --> 01:24:06.160]   - No, I didn't say there's no smarter.
[01:24:06.160 --> 01:24:07.560]   I said there's no direction.
[01:24:07.560 --> 01:24:08.400]   - Okay.
[01:24:08.400 --> 01:24:10.320]   - So I think the thing to say,
[01:24:10.320 --> 01:24:13.040]   or what I understand to be the case is that
[01:24:13.040 --> 01:24:17.400]   there's variation, it's not unbounded variation.
[01:24:17.400 --> 01:24:18.760]   And there are selectors.
[01:24:18.760 --> 01:24:22.540]   There are pressures that will select.
[01:24:22.540 --> 01:24:26.840]   And so not anything is possible because we live on a planet
[01:24:26.840 --> 01:24:30.200]   that has certain physical realities to it, right?
[01:24:30.200 --> 01:24:33.920]   But those physical realities are what constrain
[01:24:33.920 --> 01:24:40.160]   the possibilities, the physical realities of our genes
[01:24:40.160 --> 01:24:43.960]   and the physical realities of our corporeal bodies
[01:24:43.960 --> 01:24:48.960]   and the physical realities of life on this planet.
[01:24:48.960 --> 01:24:56.060]   So what I would say is that there's no direction
[01:24:57.060 --> 01:25:02.060]   but there is, it's not infinite possibility
[01:25:02.060 --> 01:25:07.020]   because we live on a particular planet
[01:25:07.020 --> 01:25:10.220]   that has particular statistical regularities in it
[01:25:10.220 --> 01:25:12.380]   and some things will never happen.
[01:25:12.380 --> 01:25:15.980]   And so all of those things are interacting
[01:25:15.980 --> 01:25:20.980]   with our genes and so on and our,
[01:25:20.980 --> 01:25:23.820]   the physical nature of our bodies
[01:25:23.820 --> 01:25:25.460]   to make some things more possible
[01:25:25.460 --> 01:25:26.740]   and some things less possible.
[01:25:26.740 --> 01:25:29.340]   Look, I mean, humans have very complex brains
[01:25:29.340 --> 01:25:31.060]   but birds have complex brains.
[01:25:31.060 --> 01:25:36.060]   And so do octopuses have very complex brains
[01:25:36.060 --> 01:25:40.660]   and all three sets of, all three of those brains
[01:25:40.660 --> 01:25:43.500]   are somewhat different from one another.
[01:25:43.500 --> 01:25:47.320]   Some birds have very complex brains.
[01:25:47.320 --> 01:25:48.820]   Some even have rudimentary language.
[01:25:48.820 --> 01:25:51.140]   They have no cerebral cortex.
[01:25:51.140 --> 01:25:53.140]   I mean, they admittedly, they have,
[01:25:53.140 --> 01:25:54.900]   this is now lesson two, right?
[01:25:54.900 --> 01:25:56.580]   They have, is it lesson two or lesson one?
[01:25:56.580 --> 01:25:57.420]   Let me think.
[01:25:57.420 --> 01:25:58.900]   No, this is lesson one.
[01:25:58.900 --> 01:26:03.900]   They have the same neurons,
[01:26:03.900 --> 01:26:07.620]   the same neurons that in a human
[01:26:07.620 --> 01:26:10.980]   become the cerebral cortex, birds have those neurons.
[01:26:10.980 --> 01:26:13.660]   They just don't form themselves into a cerebral cortex.
[01:26:13.660 --> 01:26:15.260]   But I mean, crows, for example,
[01:26:15.260 --> 01:26:17.140]   are very sophisticated animals.
[01:26:17.140 --> 01:26:19.780]   They can do a lot of the things that humans can do.
[01:26:19.780 --> 01:26:22.340]   In fact, all of the things that humans do
[01:26:22.340 --> 01:26:24.900]   that are very special, that seem very special,
[01:26:24.900 --> 01:26:26.940]   there's at least one other animal on the planet
[01:26:26.940 --> 01:26:29.040]   that can do those things too.
[01:26:29.040 --> 01:26:30.780]   What's special about the human brain
[01:26:30.780 --> 01:26:33.060]   is that we put them all together.
[01:26:33.060 --> 01:26:35.900]   So we learn from one another.
[01:26:35.900 --> 01:26:37.620]   We don't have to experience everything ourselves.
[01:26:37.620 --> 01:26:40.760]   We can watch another animal or another human
[01:26:40.760 --> 01:26:42.600]   experience something and we can learn from that.
[01:26:42.600 --> 01:26:44.060]   Well, there are many other animals
[01:26:44.060 --> 01:26:45.820]   who can learn by copying.
[01:26:45.820 --> 01:26:48.620]   That we communicate with each other very, very efficiently.
[01:26:48.620 --> 01:26:49.460]   We have language.
[01:26:49.460 --> 01:26:51.080]   But we're not the only animals
[01:26:51.080 --> 01:26:52.900]   who are efficient communicators.
[01:26:52.900 --> 01:26:54.280]   There are lots of other animals
[01:26:54.280 --> 01:26:57.080]   who can efficiently communicate, like bees, for example.
[01:26:57.080 --> 01:27:01.700]   We cooperate really well with one another to do grand things
[01:27:01.700 --> 01:27:03.420]   but there are other animals that cooperate too.
[01:27:03.420 --> 01:27:06.180]   And so every innovation that we have,
[01:27:06.180 --> 01:27:07.740]   other animals have too.
[01:27:07.740 --> 01:27:11.300]   What we have is we have all of those together
[01:27:11.300 --> 01:27:14.900]   interwoven in this very complex dance
[01:27:14.900 --> 01:27:19.900]   in a brain that is not unique, exactly,
[01:27:20.900 --> 01:27:25.300]   but it does have some features
[01:27:25.300 --> 01:27:29.920]   that make it particularly useful for us
[01:27:29.920 --> 01:27:32.560]   to do all of these things,
[01:27:32.560 --> 01:27:35.700]   to have all of these things intertwined.
[01:27:35.700 --> 01:27:40.700]   So our brains are, actually the last time we talked,
[01:27:40.700 --> 01:27:43.640]   I made a mistake 'cause I said,
[01:27:43.640 --> 01:27:46.680]   in my enthusiasm I said,
[01:27:46.680 --> 01:27:50.480]   our brains are not larger,
[01:27:50.480 --> 01:27:51.640]   relative to our bodies,
[01:27:51.640 --> 01:27:55.800]   our brains are not larger than other primates.
[01:27:55.800 --> 01:27:57.400]   And that's actually not true, actually.
[01:27:57.400 --> 01:28:01.520]   Our brains relative to our body size is somewhat larger.
[01:28:01.520 --> 01:28:05.440]   So an ape who's not a human, that's not a human,
[01:28:05.440 --> 01:28:09.760]   their brains are larger than their body sizes
[01:28:09.760 --> 01:28:13.320]   than say, relative to like a smaller monkey.
[01:28:13.320 --> 01:28:15.880]   And a human's brain is larger
[01:28:15.880 --> 01:28:18.160]   relative to its body size than a gorilla.
[01:28:18.160 --> 01:28:21.000]   - So that's a good approximation of your,
[01:28:21.000 --> 01:28:23.480]   of whatever, of the bunch of stuff
[01:28:23.480 --> 01:28:25.200]   that you can shove in there.
[01:28:25.200 --> 01:28:26.600]   - But, well what I was gonna say is,
[01:28:26.600 --> 01:28:29.560]   but our cerebral cortex is not larger
[01:28:29.560 --> 01:28:33.560]   than what you would expect for a brain of its size.
[01:28:33.560 --> 01:28:38.560]   So relative to say an ape, like a gorilla or a chimp,
[01:28:38.560 --> 01:28:42.700]   or even a mammal like a dolphin or an elephant,
[01:28:44.440 --> 01:28:49.280]   you know, our brains, our cerebral cortex
[01:28:49.280 --> 01:28:51.640]   is as large as you would expect it to be
[01:28:51.640 --> 01:28:54.160]   for a brain of our size.
[01:28:54.160 --> 01:28:58.000]   So there's nothing special about our cerebral cortex.
[01:28:58.000 --> 01:29:00.240]   And this is something I explain in the book,
[01:29:00.240 --> 01:29:03.320]   where I say, okay, you know, like by analogy,
[01:29:03.320 --> 01:29:05.640]   if you walk into somebody's house
[01:29:05.640 --> 01:29:08.340]   and you see that they have a huge kitchen,
[01:29:08.340 --> 01:29:10.720]   you know, you might think, well, maybe, you know,
[01:29:10.720 --> 01:29:13.520]   maybe this is a place I really definitely wanna eat dinner
[01:29:13.520 --> 01:29:16.680]   at because, you know, these people must be gourmet cooks.
[01:29:16.680 --> 01:29:18.360]   But you don't know anything about what the size
[01:29:18.360 --> 01:29:20.160]   of their kitchen means unless you consider it
[01:29:20.160 --> 01:29:23.120]   in relation to the size of the rest of the house.
[01:29:23.120 --> 01:29:25.800]   If it's a big kitchen in a really big house,
[01:29:25.800 --> 01:29:29.320]   it's not telling you anything special, right?
[01:29:29.320 --> 01:29:32.000]   If it's a big kitchen in a small house,
[01:29:32.000 --> 01:29:33.920]   then that might be a place that you wanna eat for,
[01:29:33.920 --> 01:29:36.760]   you wanna stay for dinner because it's more likely
[01:29:36.760 --> 01:29:39.400]   that that kitchen is large for a special reason.
[01:29:39.400 --> 01:29:43.680]   And so the cerebral cortex of a human brain
[01:29:43.680 --> 01:29:48.520]   isn't in and of itself special because of its size.
[01:29:48.520 --> 01:29:53.520]   However, there are some genetic changes
[01:29:53.520 --> 01:29:58.000]   that have happened in the human brain as it's grown
[01:29:58.000 --> 01:30:01.640]   with to whatever size is, you know,
[01:30:01.640 --> 01:30:04.200]   typical for the whole brain size, right?
[01:30:04.200 --> 01:30:07.660]   There are some changes that do give the human brain
[01:30:07.660 --> 01:30:12.460]   slightly more of some capacities.
[01:30:12.460 --> 01:30:14.800]   They're not special, but there's just,
[01:30:14.800 --> 01:30:17.360]   they just, you know, we can do some things
[01:30:17.360 --> 01:30:21.620]   much better than other animals.
[01:30:21.620 --> 01:30:22.900]   And, you know, correspondingly,
[01:30:22.900 --> 01:30:25.800]   other animals can do some things much better than we can.
[01:30:25.800 --> 01:30:27.000]   We can't grow back limbs,
[01:30:27.000 --> 01:30:29.020]   we can't lift 50 times our own body weight.
[01:30:29.020 --> 01:30:30.040]   Well, I mean, maybe you can,
[01:30:30.040 --> 01:30:31.880]   but I can't lift 50 times my own body weight.
[01:30:31.880 --> 01:30:34.120]   - Ants with that regard are very impressive.
[01:30:34.120 --> 01:30:36.880]   And then you're saying with the frontal cortex,
[01:30:36.880 --> 01:30:40.760]   like that's, the size is not always the right measure
[01:30:40.760 --> 01:30:44.140]   of capability, I guess.
[01:30:44.140 --> 01:30:46.520]   So size isn't everything.
[01:30:46.520 --> 01:30:48.320]   - Size isn't everything.
[01:30:48.320 --> 01:30:49.800]   - That's a quote about, you know,
[01:30:49.800 --> 01:30:51.080]   people like it when I disagree,
[01:30:51.080 --> 01:30:53.800]   so let me disagree with you on something
[01:30:53.800 --> 01:30:56.360]   or just like play devil's advocate a little bit.
[01:30:56.360 --> 01:30:58.620]   So you've painted a really nice picture
[01:30:58.620 --> 01:31:00.840]   that evolution doesn't have a direction,
[01:31:00.840 --> 01:31:06.380]   but is it possible if we just ran earth over and over again,
[01:31:06.380 --> 01:31:08.660]   like this video game,
[01:31:08.660 --> 01:31:11.840]   that the final result would be the same?
[01:31:11.840 --> 01:31:14.160]   So in the sense that we're,
[01:31:14.160 --> 01:31:18.760]   eventually there'll be an AGI type HAL 9000 type system
[01:31:18.760 --> 01:31:23.760]   that just like flies and colonizes nearby earth-like planets.
[01:31:23.760 --> 01:31:26.920]   And it's always will be the same.
[01:31:26.920 --> 01:31:29.040]   And the different organisms
[01:31:29.040 --> 01:31:31.640]   and the different evolution of the brain,
[01:31:31.640 --> 01:31:35.200]   like it doesn't feel like it has like a direction,
[01:31:35.200 --> 01:31:37.380]   but given the constraints of earth
[01:31:37.380 --> 01:31:40.640]   and whatever this imperative,
[01:31:40.640 --> 01:31:43.260]   whatever the hell is running this universe,
[01:31:43.260 --> 01:31:46.920]   like it seems like it's running towards something,
[01:31:46.920 --> 01:31:49.920]   is it possible that it will always be the same?
[01:31:49.920 --> 01:31:51.780]   Thereby, it will be a direction.
[01:31:51.780 --> 01:31:54.660]   - Yeah, I think, you know,
[01:31:54.660 --> 01:31:57.620]   as you know better than anyone else
[01:31:57.620 --> 01:31:59.280]   that the answer to that question is,
[01:31:59.280 --> 01:32:00.800]   of course, there's some probability
[01:32:00.800 --> 01:32:03.020]   that that could happen, right?
[01:32:03.020 --> 01:32:04.340]   It's not a yes or no answer.
[01:32:04.340 --> 01:32:07.600]   It's what's the probability that that would happen?
[01:32:07.600 --> 01:32:12.600]   And there's a whole distribution of possibilities.
[01:32:12.600 --> 01:32:16.280]   So maybe we end up,
[01:32:16.280 --> 01:32:17.420]   what's the probability we end up
[01:32:17.420 --> 01:32:22.420]   with exactly the same complement of creatures, including us?
[01:32:22.420 --> 01:32:25.940]   What's the likelihood that we end up with, you know,
[01:32:25.940 --> 01:32:29.980]   creatures that are similar to humans that are,
[01:32:29.980 --> 01:32:33.100]   but you know, similar in certain ways, let's say,
[01:32:33.100 --> 01:32:35.300]   but not exactly humans or, you know,
[01:32:35.300 --> 01:32:37.820]   all the way to a completely different
[01:32:37.820 --> 01:32:41.460]   distribution of creatures?
[01:32:41.460 --> 01:32:42.300]   - What's your intuition?
[01:32:42.300 --> 01:32:43.860]   Like if you were to bet money,
[01:32:43.860 --> 01:32:45.420]   what does that distribution look like
[01:32:45.420 --> 01:32:47.300]   if we ran earth over and over and over again?
[01:32:47.300 --> 01:32:49.680]   - I would say given the,
[01:32:49.680 --> 01:32:51.180]   you're now asking me questions that--
[01:32:51.180 --> 01:32:52.220]   - This is not science.
[01:32:52.220 --> 01:32:53.440]   - This is not science.
[01:32:53.440 --> 01:32:54.620]   But I would say, okay, well,
[01:32:54.620 --> 01:32:58.700]   what's the probability that it's gonna be a carbon life form?
[01:32:58.700 --> 01:33:00.700]   Probably high,
[01:33:00.700 --> 01:33:03.180]   but that's because I don't know anything about--
[01:33:03.180 --> 01:33:04.020]   - Alternatives?
[01:33:04.020 --> 01:33:05.100]   - Yeah, you know, I don't,
[01:33:05.100 --> 01:33:07.900]   I'm not really well versed that.
[01:33:07.900 --> 01:33:09.380]   What's the probability that, you know,
[01:33:09.380 --> 01:33:12.160]   so what's the probability that the animals will begin
[01:33:12.160 --> 01:33:14.740]   in the ocean and crawl out onto land?
[01:33:14.740 --> 01:33:15.580]   - Versus the other way.
[01:33:15.580 --> 01:33:18.280]   - Versus the, I would say probably high.
[01:33:18.280 --> 01:33:20.620]   I don't know, but, you know,
[01:33:20.620 --> 01:33:22.800]   but do I think, what's the likelihood
[01:33:22.800 --> 01:33:26.780]   that we would end up with exactly the same or very similar?
[01:33:26.780 --> 01:33:28.700]   I think it's low, actually.
[01:33:28.700 --> 01:33:29.840]   I wouldn't say it's low,
[01:33:29.840 --> 01:33:32.140]   but I would say it's not 100%
[01:33:32.140 --> 01:33:34.540]   and I'm not even sure it's 50%.
[01:33:34.540 --> 01:33:36.440]   You know, I would say,
[01:33:36.440 --> 01:33:38.980]   I don't think that we're here by accident
[01:33:38.980 --> 01:33:41.700]   because I think, like I said, there are constraints.
[01:33:41.700 --> 01:33:44.800]   Like, there are some physical constraints about Earth.
[01:33:44.800 --> 01:33:46.600]   Now, of course, if you were a cosmologist,
[01:33:46.600 --> 01:33:49.220]   you could say, well, the fact that the Earth is,
[01:33:49.220 --> 01:33:51.300]   if you were to do the Big Bang over again
[01:33:51.300 --> 01:33:53.060]   and keep doing it over and over and over again,
[01:33:53.060 --> 01:33:56.340]   would you still get the same solar systems?
[01:33:56.340 --> 01:33:58.240]   Would you still get the same planets?
[01:33:58.240 --> 01:34:00.820]   Would, you know, would you still get the same galaxies,
[01:34:00.820 --> 01:34:02.800]   the same solar systems, the same planets?
[01:34:02.800 --> 01:34:05.780]   You know, I don't know, but my guess is probably not
[01:34:05.780 --> 01:34:08.540]   because there are random things that happen
[01:34:08.540 --> 01:34:12.580]   that can, again, send things in one direct, you know,
[01:34:12.580 --> 01:34:14.500]   make one set of trajectories possible
[01:34:14.500 --> 01:34:15.900]   and another set impossible.
[01:34:15.900 --> 01:34:19.040]   So, but I guess my,
[01:34:19.040 --> 01:34:25.740]   if I were gonna bet something, money or something valuable,
[01:34:25.740 --> 01:34:30.740]   I would probably say it's not zero and it's not 100%
[01:34:30.740 --> 01:34:33.280]   and it's probably not even 50%.
[01:34:33.280 --> 01:34:34.120]   So, there's some probability, but I don't know.
[01:34:34.120 --> 01:34:35.680]   - That it will be similar.
[01:34:35.680 --> 01:34:37.320]   - That it would be similar, but I don't think,
[01:34:37.320 --> 01:34:40.560]   I just think there are too many degrees of freedom.
[01:34:40.560 --> 01:34:42.840]   There are too many degrees of freedom.
[01:34:42.840 --> 01:34:47.660]   I mean, one of the real tensions in writing this book
[01:34:47.660 --> 01:34:51.880]   is to, on the one hand, there's some truth in saying
[01:34:52.720 --> 01:34:57.680]   that humans are not special.
[01:34:57.680 --> 01:35:01.720]   We are just, you know, we're not special
[01:35:01.720 --> 01:35:03.020]   in the animal kingdom.
[01:35:03.020 --> 01:35:08.020]   All animals are well-adapted, if they're survived,
[01:35:08.020 --> 01:35:11.100]   they're well-adapted to their niche.
[01:35:11.100 --> 01:35:15.520]   It does happen to be the case that our niche is large.
[01:35:15.520 --> 01:35:18.380]   For any individual human, your niche is whatever it is,
[01:35:18.380 --> 01:35:22.580]   but for the species, right, we live almost everywhere,
[01:35:22.580 --> 01:35:25.180]   not everywhere, but almost everywhere on the planet,
[01:35:25.180 --> 01:35:28.540]   but not in the ocean.
[01:35:28.540 --> 01:35:32.100]   And actually, other animals like bacteria, for example,
[01:35:32.100 --> 01:35:35.540]   have us beat miles, you know, hands down, right?
[01:35:35.540 --> 01:35:40.540]   So, by any definition, we're not special.
[01:35:40.540 --> 01:35:46.660]   We're just, you know, adapted to our environment.
[01:35:46.660 --> 01:35:48.340]   - But bacteria don't have a podcast.
[01:35:48.340 --> 01:35:50.300]   - Exactly, exactly.
[01:35:50.300 --> 01:35:51.620]   - They're not able to introspect.
[01:35:51.620 --> 01:35:53.060]   - So, that's the tension, right?
[01:35:53.060 --> 01:35:55.320]   So, on the one hand, you know, we're not special animals.
[01:35:55.320 --> 01:35:58.260]   We're just, you know, particularly well-adapted to our niche.
[01:35:58.260 --> 01:36:00.040]   On the other hand, our niche is huge,
[01:36:00.040 --> 01:36:03.060]   and we don't just adapt to our environment.
[01:36:03.060 --> 01:36:04.740]   We add to our environment.
[01:36:04.740 --> 01:36:08.460]   We make stuff up, give it a name, and then it becomes real.
[01:36:08.460 --> 01:36:10.820]   And so, no other animal can do that.
[01:36:10.820 --> 01:36:14.460]   And so, I think the way to think about it
[01:36:14.460 --> 01:36:16.780]   from my perspective, or the way I made sense of it,
[01:36:16.780 --> 01:36:20.620]   is to say, you can look at any individual
[01:36:20.620 --> 01:36:23.020]   single characteristic that a human has
[01:36:23.020 --> 01:36:26.420]   that seems remarkable,
[01:36:26.420 --> 01:36:28.620]   and you can find that in some other animal.
[01:36:28.620 --> 01:36:33.780]   What you can't find in any other animal
[01:36:33.780 --> 01:36:37.240]   is all of those characteristics together
[01:36:37.240 --> 01:36:43.060]   in a brain that is souped up in particular ways,
[01:36:43.060 --> 01:36:46.340]   like ours is, and if you combine these things,
[01:36:46.340 --> 01:36:48.820]   multiple interacting causes, right?
[01:36:48.820 --> 01:36:53.720]   Not one essence, like your cortex, your big neocortex,
[01:36:53.720 --> 01:36:56.160]   but, which isn't really that big.
[01:36:56.160 --> 01:36:59.860]   I mean, it's just big for your big brain,
[01:36:59.860 --> 01:37:01.220]   for the size of your big brain.
[01:37:01.220 --> 01:37:02.860]   It's the size it should be.
[01:37:02.860 --> 01:37:05.980]   If you add all those things together,
[01:37:05.980 --> 01:37:07.360]   and they interact with each other,
[01:37:07.360 --> 01:37:10.100]   that produces some pretty remarkable results.
[01:37:10.100 --> 01:37:13.140]   And if you're aware of that,
[01:37:14.420 --> 01:37:19.420]   then you can start asking different kinds of questions
[01:37:19.420 --> 01:37:22.620]   about what it means to be human,
[01:37:22.620 --> 01:37:25.180]   and what kind of a human you wanna be,
[01:37:25.180 --> 01:37:28.800]   and what kind of a world do you wanna curate
[01:37:28.800 --> 01:37:31.100]   for the next generation of humans?
[01:37:31.100 --> 01:37:33.780]   I think that's the goal anyways, right?
[01:37:33.780 --> 01:37:36.220]   It's just to have a glimpse of,
[01:37:36.220 --> 01:37:42.780]   instead of thinking about things in a simple, linear way,
[01:37:42.780 --> 01:37:45.780]   just to have a glimpse of some of the things that matter,
[01:37:45.780 --> 01:37:49.060]   that evidence suggests matters,
[01:37:49.060 --> 01:37:54.060]   to the kind of brain, and the kind of bodies that we have.
[01:37:54.060 --> 01:37:58.780]   Once you know that, you can work with it a little bit.
[01:37:58.780 --> 01:38:02.100]   - You write, "Words have power over your biology."
[01:38:02.100 --> 01:38:05.300]   Right now, I can text the words, "I love you,"
[01:38:05.300 --> 01:38:08.500]   from the United States to my close friend in Belgium,
[01:38:08.500 --> 01:38:12.060]   and even though she cannot hear my voice or see my face,
[01:38:12.060 --> 01:38:13.940]   I will change her heart rate,
[01:38:13.940 --> 01:38:16.660]   her breathing, and her metabolism.
[01:38:16.660 --> 01:38:18.260]   By the way, beautifully written.
[01:38:18.260 --> 01:38:22.060]   Or someone could text something ambiguous to you,
[01:38:22.060 --> 01:38:24.740]   like, "Is your door locked?"
[01:38:24.740 --> 01:38:27.380]   And odds are that it would affect your nervous system
[01:38:27.380 --> 01:38:29.500]   in an unpleasant way.
[01:38:29.500 --> 01:38:33.060]   So, I mean, there's a lot of stuff to talk about here,
[01:38:33.060 --> 01:38:36.020]   but just one way to ask is,
[01:38:37.460 --> 01:38:42.260]   why do you think words have so much power over our brain?
[01:38:42.260 --> 01:38:46.260]   - Well, I think we just have to look at the anatomy
[01:38:46.260 --> 01:38:48.220]   of the brain to answer that question.
[01:38:48.220 --> 01:38:52.780]   So, if you look at the parts of the brain,
[01:38:52.780 --> 01:38:57.780]   the systems that are important for processing language,
[01:38:57.780 --> 01:39:03.060]   you can see that some of these regions
[01:39:03.060 --> 01:39:06.540]   are also important for controlling your major organ systems,
[01:39:06.540 --> 01:39:08.540]   and your autonomic nervous system
[01:39:08.540 --> 01:39:11.020]   that controls your cardiovascular system,
[01:39:11.020 --> 01:39:13.420]   your respiratory system, and so on,
[01:39:13.420 --> 01:39:18.420]   that these regions control your endocrine system,
[01:39:18.420 --> 01:39:21.340]   your immune system, and so on.
[01:39:21.340 --> 01:39:24.140]   So, and you can actually see this in other animals, too.
[01:39:24.140 --> 01:39:26.140]   So, in birds, for example,
[01:39:26.140 --> 01:39:29.340]   the neurons that are responsible for bird song
[01:39:29.340 --> 01:39:32.020]   also control the systems of a bird's body.
[01:39:32.020 --> 01:39:33.260]   And the reason why I bring that up
[01:39:33.260 --> 01:39:38.260]   is that some scientists think that the anatomy
[01:39:38.260 --> 01:39:43.100]   of a bird's brain that control bird song
[01:39:43.100 --> 01:39:46.860]   are homologous or structurally have a similar origin
[01:39:46.860 --> 01:39:49.540]   to the human system for language.
[01:39:49.540 --> 01:39:52.140]   So, the parts of the brain that are important
[01:39:52.140 --> 01:39:54.460]   for processing language are not unique
[01:39:54.460 --> 01:39:57.580]   and specialized for language.
[01:39:57.580 --> 01:39:59.180]   They do many things.
[01:39:59.180 --> 01:40:00.340]   And one of the things they do
[01:40:00.340 --> 01:40:03.700]   is control your major organ systems.
[01:40:03.700 --> 01:40:05.260]   - Do you think we can fall in love,
[01:40:05.260 --> 01:40:07.860]   I have arguments about this all the time,
[01:40:07.860 --> 01:40:10.580]   do you think we can fall in love based on words alone?
[01:40:10.580 --> 01:40:14.020]   - Well, I think people have been doing it for centuries.
[01:40:14.020 --> 01:40:15.500]   I mean, it used to be the case
[01:40:15.500 --> 01:40:17.460]   that people wrote letters to each other,
[01:40:17.460 --> 01:40:22.180]   and then that was how they communicated.
[01:40:22.180 --> 01:40:24.180]   - I guess that's how you and Dan got--
[01:40:24.180 --> 01:40:28.100]   - Exactly, exactly, exactly, yeah, exactly.
[01:40:28.100 --> 01:40:31.180]   - So, is the answer a clear yes there?
[01:40:31.180 --> 01:40:34.060]   Because I get a lot of pushback from people often
[01:40:34.060 --> 01:40:37.860]   that you need the touch and the smell
[01:40:37.860 --> 01:40:42.140]   and the bodily stuff.
[01:40:42.140 --> 01:40:43.500]   - I think the touch and the smell
[01:40:43.500 --> 01:40:45.620]   and the bodily stuff helps.
[01:40:45.620 --> 01:40:46.440]   - Okay.
[01:40:46.440 --> 01:40:47.900]   - But I don't think it's necessary.
[01:40:47.900 --> 01:40:50.700]   - Do you think you can have a lifelong monogamous
[01:40:50.700 --> 01:40:54.620]   relationship with an AI system that only communicates
[01:40:54.620 --> 01:40:56.860]   with you on text, romantic relationship?
[01:40:57.700 --> 01:41:00.820]   - Well, I suppose that's an empirical question
[01:41:00.820 --> 01:41:02.740]   that hasn't been answered yet, but--
[01:41:02.740 --> 01:41:03.580]   - So, yeah.
[01:41:03.580 --> 01:41:05.020]   - I guess what I would say is,
[01:41:05.020 --> 01:41:09.320]   I don't think I could.
[01:41:09.320 --> 01:41:14.460]   Could any human, could the average human,
[01:41:14.460 --> 01:41:18.060]   could, you know, so, if I,
[01:41:18.060 --> 01:41:25.580]   I wanna even modify that and say,
[01:41:25.580 --> 01:41:30.580]   I'm thinking now of Tom Hanks and the movie--
[01:41:30.580 --> 01:41:31.940]   - Castaway?
[01:41:31.940 --> 01:41:33.300]   - Yeah, you know, with Wilson.
[01:41:33.300 --> 01:41:34.140]   - Yeah.
[01:41:34.140 --> 01:41:37.460]   - I think if that was, if you had to make that work,
[01:41:37.460 --> 01:41:39.060]   if you had to make that work--
[01:41:39.060 --> 01:41:40.420]   - With the volleyball, yeah.
[01:41:40.420 --> 01:41:43.180]   - If you had to make it work, could you,
[01:41:43.180 --> 01:41:45.340]   could you, prediction and simulation, right?
[01:41:45.340 --> 01:41:49.500]   So, if you had to make it work, could you make it work?
[01:41:49.500 --> 01:41:53.460]   Using simulation and, you know, your past experience,
[01:41:53.460 --> 01:41:54.620]   could you make it work?
[01:41:55.620 --> 01:41:59.020]   Could you make it work, you as a human, could you,
[01:41:59.020 --> 01:41:59.860]   could you, like--
[01:41:59.860 --> 01:42:01.500]   - Could you have a relationship,
[01:42:01.500 --> 01:42:03.620]   literally with an inanimate object,
[01:42:03.620 --> 01:42:05.900]   and have it sustain you in the way
[01:42:05.900 --> 01:42:07.740]   that another human could?
[01:42:07.740 --> 01:42:08.860]   - Yeah.
[01:42:08.860 --> 01:42:11.020]   - Your life would probably be shorter,
[01:42:11.020 --> 01:42:12.900]   because you wouldn't actually derive
[01:42:12.900 --> 01:42:15.380]   the body budgeting benefits from, right?
[01:42:15.380 --> 01:42:18.940]   So, we've talked about, you know,
[01:42:18.940 --> 01:42:22.660]   how your brain, its most important job
[01:42:22.660 --> 01:42:25.980]   is to control your body, and you can describe that
[01:42:25.980 --> 01:42:28.220]   as your brain running a budget for your body.
[01:42:28.220 --> 01:42:31.860]   And there are metaphorical, you know,
[01:42:31.860 --> 01:42:34.660]   deposits and withdrawals into your body budget,
[01:42:34.660 --> 01:42:37.420]   and you also make deposits and withdrawals
[01:42:37.420 --> 01:42:40.420]   in other people's body budgets, figuratively speaking.
[01:42:40.420 --> 01:42:43.840]   So, you wouldn't have that particular benefit,
[01:42:43.840 --> 01:42:48.300]   so your life would probably be shorter,
[01:42:48.300 --> 01:42:51.100]   but I think it would be harder for some people
[01:42:51.100 --> 01:42:52.420]   than for other people.
[01:42:52.420 --> 01:42:53.860]   - Yeah, I tend to, my intuition is that
[01:42:53.860 --> 01:42:56.700]   you can have a deep, fulfilling relationship
[01:42:56.700 --> 01:42:57.860]   with a volleyball.
[01:42:57.860 --> 01:43:04.020]   I think a lot of the environments that set up,
[01:43:04.020 --> 01:43:05.460]   I think that's a really good example,
[01:43:05.460 --> 01:43:10.460]   like the constraints of your particular environment
[01:43:10.460 --> 01:43:14.060]   define the, like, I believe like scarcity
[01:43:14.060 --> 01:43:19.060]   is a good catalyst for deep, meaningful connection
[01:43:19.060 --> 01:43:21.980]   with other humans and with inanimate objects.
[01:43:21.980 --> 01:43:24.700]   So, the less you have, the more fulfilling
[01:43:24.700 --> 01:43:27.540]   those relationships are, and I would say
[01:43:27.540 --> 01:43:29.260]   a relationship with a volleyball,
[01:43:29.260 --> 01:43:31.620]   the sex is not great, but everything else,
[01:43:31.620 --> 01:43:34.660]   I feel like it could be a very fulfilling relationship,
[01:43:34.660 --> 01:43:37.260]   which I don't know, from an engineering perspective,
[01:43:37.260 --> 01:43:38.780]   what to do with that.
[01:43:38.780 --> 01:43:41.500]   Just like you said, it is an empirical question, but.
[01:43:41.500 --> 01:43:43.540]   - But there are places to learn about that, right?
[01:43:43.540 --> 01:43:48.140]   So, for example, think about children and their blankets.
[01:43:48.140 --> 01:43:51.220]   Right, so there, there's something tactile
[01:43:51.220 --> 01:43:53.660]   and there's something olfactory,
[01:43:53.660 --> 01:43:55.380]   and it's very comforting.
[01:43:55.380 --> 01:44:00.220]   I mean, even for non-human little animals, right?
[01:44:00.220 --> 01:44:03.100]   Like puppies and, so I don't know about cats, but.
[01:44:03.100 --> 01:44:07.620]   - Cats are cold-hearted, there's no,
[01:44:07.620 --> 01:44:08.820]   there's nothing going on there.
[01:44:08.820 --> 01:44:10.460]   - I don't know, there are some cats
[01:44:10.460 --> 01:44:14.100]   that are very dog-like, I mean, really, so.
[01:44:14.100 --> 01:44:15.820]   - Some cats identify as dogs, yes.
[01:44:15.820 --> 01:44:19.140]   - I think that's true, yeah, they're species fluid.
[01:44:19.340 --> 01:44:21.380]   (both laughing)
[01:44:21.380 --> 01:44:26.100]   - So, you also write, "When it comes to human minds,
[01:44:26.100 --> 01:44:29.420]   "variation is the norm, and what we call, quote,
[01:44:29.420 --> 01:44:32.860]   "human nature is really many human natures."
[01:44:32.860 --> 01:44:36.300]   Again, many questions I can ask here,
[01:44:36.300 --> 01:44:39.300]   but maybe an interesting one to ask is,
[01:44:39.300 --> 01:44:42.700]   I often hear, you know, we often hear this idea
[01:44:42.700 --> 01:44:43.820]   of be yourself.
[01:44:43.820 --> 01:44:48.160]   Is this possible, to be yourself?
[01:44:48.160 --> 01:44:51.580]   Is it a good idea to strive to be yourself?
[01:44:51.580 --> 01:44:54.500]   Is it, does that even have any meaning?
[01:44:54.500 --> 01:44:57.340]   - It's a very Western question, first of all,
[01:44:57.340 --> 01:44:59.540]   because which self are you talking about?
[01:44:59.540 --> 01:45:01.900]   You don't have one self,
[01:45:01.900 --> 01:45:04.580]   there is no self that's an essence of you.
[01:45:04.580 --> 01:45:06.480]   You have multiple selves, actually,
[01:45:06.480 --> 01:45:09.020]   there is research on this.
[01:45:09.020 --> 01:45:12.540]   You know, to quote the great social psychologist,
[01:45:12.540 --> 01:45:13.900]   Hazel Marcus, you're never,
[01:45:13.900 --> 01:45:15.900]   you cannot be a self by yourself.
[01:45:16.820 --> 01:45:21.080]   You, you know, you, and so different contexts
[01:45:21.080 --> 01:45:26.000]   pull for or draw on different features of your,
[01:45:26.000 --> 01:45:28.960]   of who you are or what you believe, what you feel,
[01:45:28.960 --> 01:45:30.140]   what your actions are.
[01:45:30.140 --> 01:45:35.120]   Different contexts, you know, will put certain things,
[01:45:35.120 --> 01:45:37.840]   or make more, some features be more in the foreground
[01:45:37.840 --> 01:45:39.840]   and some in the background.
[01:45:39.840 --> 01:45:42.120]   It takes us back right to our discussion earlier
[01:45:42.120 --> 01:45:46.100]   about Stalin and Hitler and so on.
[01:45:46.100 --> 01:45:48.560]   The thing that I would caution,
[01:45:48.560 --> 01:45:51.420]   in addition to the fact that there is no single self,
[01:45:51.420 --> 01:45:54.340]   you know, that you have multiple selves, who you can be,
[01:45:54.340 --> 01:45:59.080]   and you can certainly choose the situations
[01:45:59.080 --> 01:46:01.240]   that you put yourself in to some extent.
[01:46:01.240 --> 01:46:02.880]   Not everybody has complete choice,
[01:46:02.880 --> 01:46:04.480]   but everybody has a little bit of choice.
[01:46:04.480 --> 01:46:06.420]   And I think I said this to you before,
[01:46:06.420 --> 01:46:10.240]   that one of the pieces of advice that we gave Sophia,
[01:46:10.240 --> 01:46:11.960]   you know, when she went, our daughter,
[01:46:11.960 --> 01:46:13.860]   when she was going off to college was,
[01:46:14.560 --> 01:46:18.440]   try to spend time around people,
[01:46:18.440 --> 01:46:21.340]   choose relationships that allow you to be your best self.
[01:46:21.340 --> 01:46:26.840]   We should have said your best selves, but--
[01:46:26.840 --> 01:46:31.600]   - The pool of selves given the environment.
[01:46:31.600 --> 01:46:35.400]   - Yeah, but the one thing I do wanna say is that
[01:46:35.400 --> 01:46:38.320]   the risk of saying be yourself, just be yourself,
[01:46:38.320 --> 01:46:42.440]   is that that can be used as an excuse.
[01:46:42.440 --> 01:46:45.540]   Well, this is just the way that I am, I'm just like this.
[01:46:45.540 --> 01:46:50.540]   And that I think should be tremendously resistant.
[01:46:50.540 --> 01:46:54.360]   - So that's one, that's for the excuse side,
[01:46:54.360 --> 01:46:57.640]   but you know, I'm really self-critical often,
[01:46:57.640 --> 01:47:00.600]   I'm full of doubt, and people often tell me,
[01:47:00.600 --> 01:47:04.120]   just don't worry about it, just be yourself, man.
[01:47:04.120 --> 01:47:09.120]   And the thing is, it's not, from an engineering perspective,
[01:47:09.880 --> 01:47:12.480]   does not seem like actionable advice,
[01:47:12.480 --> 01:47:17.480]   because I guess constantly worrying about who,
[01:47:17.480 --> 01:47:24.080]   what are the right words to say
[01:47:24.080 --> 01:47:29.080]   to express how I'm feeling is, I guess, myself.
[01:47:29.080 --> 01:47:32.000]   There's a kind of line, I guess,
[01:47:32.000 --> 01:47:34.500]   that this might be a Western idea,
[01:47:34.500 --> 01:47:37.520]   but something that feels genuine
[01:47:37.520 --> 01:47:39.420]   and something that feels not genuine.
[01:47:39.420 --> 01:47:42.600]   And I'm not sure what that means,
[01:47:42.600 --> 01:47:45.820]   'cause I would like to be fully genuine and fully open,
[01:47:45.820 --> 01:47:49.160]   but I'm also aware, like this morning,
[01:47:49.160 --> 01:47:54.160]   I was very silly and giddy, I was just being funny
[01:47:54.160 --> 01:47:58.320]   and relaxed and light, like there's nothing
[01:47:58.320 --> 01:48:01.120]   that could bother me in the world,
[01:48:01.120 --> 01:48:02.680]   I was just smiling and happy.
[01:48:02.680 --> 01:48:04.080]   And then I remember last night,
[01:48:04.080 --> 01:48:06.000]   I was just feeling very grumpy,
[01:48:06.000 --> 01:48:09.120]   like stuff was bothering me,
[01:48:09.120 --> 01:48:10.960]   like certain things were bothering me.
[01:48:10.960 --> 01:48:14.520]   And what are those, are those the different selves?
[01:48:14.520 --> 01:48:17.720]   Like what, who am I in that, and what do I do?
[01:48:17.720 --> 01:48:20.560]   Because if we take Twitter as an example,
[01:48:20.560 --> 01:48:23.280]   if I actually send a tweet last night
[01:48:23.280 --> 01:48:24.360]   and a tweet this morning,
[01:48:24.360 --> 01:48:28.960]   it's gonna be very two different people tweeting that.
[01:48:28.960 --> 01:48:30.600]   And I don't know what to do with that,
[01:48:30.600 --> 01:48:35.000]   because one does seem to be more me than the other,
[01:48:35.000 --> 01:48:36.960]   but that's maybe because there's a narrative,
[01:48:36.960 --> 01:48:38.320]   the story that I'm trying,
[01:48:38.320 --> 01:48:40.520]   there's something I'm striving to be,
[01:48:40.520 --> 01:48:43.240]   like the ultimate human that I might become.
[01:48:43.240 --> 01:48:44.640]   I have maybe a vision of that,
[01:48:44.640 --> 01:48:46.500]   and I'm trying to become that,
[01:48:46.500 --> 01:48:49.000]   but it does seem like there's a lot
[01:48:49.000 --> 01:48:50.440]   of different minds in there.
[01:48:50.440 --> 01:48:54.520]   And they're all like having a discussion
[01:48:54.520 --> 01:48:56.800]   and a battle for who's gonna win.
[01:48:56.800 --> 01:48:58.200]   - I suppose you could think of it that way,
[01:48:58.200 --> 01:49:00.160]   but there's another way to think of it, I think,
[01:49:00.160 --> 01:49:03.080]   and that is that maybe the more Buddhist way
[01:49:03.080 --> 01:49:04.000]   to think of it, right,
[01:49:04.000 --> 01:49:05.760]   or a more contemplative way to think about it,
[01:49:05.760 --> 01:49:08.800]   which is not that you have multiple personalities
[01:49:08.800 --> 01:49:11.800]   inside your head, but you have,
[01:49:11.800 --> 01:49:16.800]   your brain has this amazing capacity.
[01:49:16.800 --> 01:49:23.160]   It has a population of experiences that you've had
[01:49:23.160 --> 01:49:27.840]   that it can regenerate, reconstitute.
[01:49:27.840 --> 01:49:30.960]   And it can even take bits and pieces
[01:49:30.960 --> 01:49:35.020]   of those experiences and combine them into something new.
[01:49:35.020 --> 01:49:39.200]   And it's often doing this to predict
[01:49:39.200 --> 01:49:42.240]   what's going to happen next and to plan your actions,
[01:49:42.240 --> 01:49:46.320]   but it's also happening, this also happens just,
[01:49:46.320 --> 01:49:47.840]   that's what mind-wandering is,
[01:49:47.840 --> 01:49:50.240]   or just internal thought and so on.
[01:49:50.240 --> 01:49:52.320]   It's the same mechanism, really.
[01:49:52.320 --> 01:49:57.040]   And so a lot of times we hear the saying,
[01:49:57.040 --> 01:49:58.840]   just think, if you think differently,
[01:49:58.840 --> 01:50:00.840]   you'll feel differently.
[01:50:00.840 --> 01:50:04.200]   But your brain is having a conversation
[01:50:04.200 --> 01:50:06.540]   continually with your body.
[01:50:06.540 --> 01:50:10.640]   And your body, your brain's trying to control your body,
[01:50:10.640 --> 01:50:13.000]   well, trying, your brain is controlling your body,
[01:50:13.000 --> 01:50:16.120]   your body is sending information back to the brain.
[01:50:16.120 --> 01:50:19.760]   And in part, the information that your body sends back
[01:50:19.760 --> 01:50:23.600]   to your brain, just like the information
[01:50:23.600 --> 01:50:27.760]   coming from the world, initiates the next volley
[01:50:27.760 --> 01:50:30.280]   of predictions or simulations.
[01:50:30.280 --> 01:50:32.300]   So in some ways, you could also say,
[01:50:32.300 --> 01:50:37.240]   the way that you feel, I think we talked before
[01:50:37.240 --> 01:50:41.960]   about affective feeling or mood coming from the sensations
[01:50:41.960 --> 01:50:46.960]   of body budgeting, you know, influences what you think.
[01:50:50.240 --> 01:50:54.800]   And as much as, so feelings influence thought
[01:50:54.800 --> 01:50:58.720]   as much as thought influence feeling, and maybe more.
[01:50:58.720 --> 01:51:01.440]   - But just, the whole thing doesn't seem stable.
[01:51:01.440 --> 01:51:04.360]   - Well, it's a dynamic system, Mr. Engineer.
[01:51:04.360 --> 01:51:05.200]   - Yeah.
[01:51:05.200 --> 01:51:07.760]   - Right, it's a dynamic, it's a dynamical system, right?
[01:51:07.760 --> 01:51:09.360]   Non-linear dynamical system.
[01:51:09.360 --> 01:51:11.780]   And I think that's, I'm actually writing a paper
[01:51:11.780 --> 01:51:14.980]   with a bunch of engineers about this, actually.
[01:51:14.980 --> 01:51:17.520]   But I mean, other people have talked about the brain
[01:51:17.520 --> 01:51:20.120]   as a dynamical system before, but you know,
[01:51:20.120 --> 01:51:22.260]   the real tricky bit is trying to figure out
[01:51:22.260 --> 01:51:24.880]   how do you get mental features out of that system?
[01:51:24.880 --> 01:51:26.220]   I guess one thing to figure out how you get
[01:51:26.220 --> 01:51:27.760]   a motor movement out of that system,
[01:51:27.760 --> 01:51:29.320]   it's another thing to figure out how you get
[01:51:29.320 --> 01:51:32.920]   a mental feature, like a feeling of being loved
[01:51:32.920 --> 01:51:36.800]   or a feeling of being worthwhile, or a feeling of,
[01:51:36.800 --> 01:51:38.600]   you know, just basically feeling like shit.
[01:51:38.600 --> 01:51:41.440]   How do you get a feeling, a mental features
[01:51:41.440 --> 01:51:42.840]   out of that system?
[01:51:42.840 --> 01:51:48.020]   So what I would say is that you aren't,
[01:51:48.020 --> 01:51:50.760]   the Buddhist thing to say is that you're not one person
[01:51:50.760 --> 01:51:52.620]   and you're not many people.
[01:51:52.620 --> 01:51:57.620]   You are, you are the sum of your experiences
[01:51:57.620 --> 01:52:00.860]   and who you are in any given moment,
[01:52:00.860 --> 01:52:05.000]   meaning what your actions will be,
[01:52:05.000 --> 01:52:07.680]   is influenced by the state of your body
[01:52:07.680 --> 01:52:10.280]   and the state of the world that you've put yourself in.
[01:52:10.280 --> 01:52:12.880]   And you can change either of those things.
[01:52:12.880 --> 01:52:15.200]   One is a little easier to change than the other, right?
[01:52:15.200 --> 01:52:17.820]   You can change your environment by literally getting up
[01:52:17.820 --> 01:52:21.000]   and moving, or you can change it by paying attention
[01:52:21.000 --> 01:52:23.400]   to some things differently and letting other,
[01:52:23.400 --> 01:52:26.080]   some features come to the fore
[01:52:26.080 --> 01:52:28.000]   and other features be backgrounded.
[01:52:28.000 --> 01:52:30.040]   Like I'm looking around your place.
[01:52:30.040 --> 01:52:32.600]   - Oh no, this is not something you should do.
[01:52:32.600 --> 01:52:34.960]   - No, but I'm gonna say one thing.
[01:52:34.960 --> 01:52:36.680]   No green plants.
[01:52:36.680 --> 01:52:39.640]   No green plants.
[01:52:39.640 --> 01:52:41.380]   - 'Cause green plants mean a home
[01:52:41.380 --> 01:52:43.160]   and I want this to be temporary.
[01:52:43.160 --> 01:52:45.880]   - Fair, fair, but--
[01:52:45.880 --> 01:52:47.360]   - What goes through your mind
[01:52:47.360 --> 01:52:48.500]   when you see no green plants?
[01:52:48.500 --> 01:52:53.500]   - No, I'm just making the point that what if you,
[01:52:53.500 --> 01:52:59.280]   again, not everybody has control over their environment.
[01:52:59.280 --> 01:53:01.520]   Some people don't have control over the noise
[01:53:01.520 --> 01:53:04.500]   or the temperature or any of those things.
[01:53:04.500 --> 01:53:07.100]   But everybody has a little bit of control
[01:53:07.100 --> 01:53:10.280]   and you can place things in your environment,
[01:53:10.280 --> 01:53:15.280]   photographs, plants, anything that's meaningful to you
[01:53:16.300 --> 01:53:20.280]   and use it as a shift of environment when you need it.
[01:53:20.280 --> 01:53:22.700]   You can also do things to change
[01:53:22.700 --> 01:53:24.960]   the conditions of your body.
[01:53:24.960 --> 01:53:26.720]   When you exercise every day,
[01:53:26.720 --> 01:53:29.780]   you're making an investment in your body.
[01:53:29.780 --> 01:53:32.140]   Actually, you're making an investment in your brain too.
[01:53:32.140 --> 01:53:34.720]   It makes you, even though it's unpleasant
[01:53:34.720 --> 01:53:38.140]   and there's a cost to it, if you replenish,
[01:53:38.140 --> 01:53:40.580]   if you invest and you make up that,
[01:53:40.580 --> 01:53:45.220]   you make a deposit and you make up that, what you've spent,
[01:53:45.220 --> 01:53:47.140]   you're basically making an investment
[01:53:47.140 --> 01:53:49.660]   in making it easier for your brain
[01:53:49.660 --> 01:53:52.180]   to control your body in the future.
[01:53:52.180 --> 01:53:56.400]   So you can make sure you're hydrated, drink water.
[01:53:56.400 --> 01:53:57.820]   You don't have to drink bottled water.
[01:53:57.820 --> 01:53:59.380]   You can drink water from the tap.
[01:53:59.380 --> 01:54:02.420]   This is in most places, maybe not everywhere,
[01:54:02.420 --> 01:54:07.060]   but most places in the developed world.
[01:54:07.060 --> 01:54:10.420]   You can try to get enough sleep.
[01:54:10.420 --> 01:54:11.980]   Not everybody has that luxury,
[01:54:11.980 --> 01:54:16.980]   but everybody can do something to make their body budgets
[01:54:16.980 --> 01:54:18.540]   a little more solvent.
[01:54:18.540 --> 01:54:22.340]   And that will also make it more likely
[01:54:22.340 --> 01:54:24.280]   that certain thoughts will emerge
[01:54:24.280 --> 01:54:27.660]   from that prediction machine.
[01:54:27.660 --> 01:54:29.420]   - That's the control you do have,
[01:54:29.420 --> 01:54:32.100]   is being able to control the environment.
[01:54:32.100 --> 01:54:33.260]   That's really well put.
[01:54:33.260 --> 01:54:37.380]   I don't think we've talked about this,
[01:54:37.380 --> 01:54:39.680]   so let's go to the biggest unanswerable questions
[01:54:39.680 --> 01:54:41.600]   of consciousness.
[01:54:41.600 --> 01:54:44.100]   What is, you just rolled your eyes.
[01:54:44.100 --> 01:54:45.940]   - I did, that was my, yeah.
[01:54:45.940 --> 01:54:49.220]   - So what is consciousness from a neuroscience perspective?
[01:54:49.220 --> 01:54:50.400]   I know you, I mean.
[01:54:50.400 --> 01:54:54.500]   - I made notes, you know,
[01:54:54.500 --> 01:54:56.660]   'cause you gave me some questions in advance
[01:54:56.660 --> 01:54:58.300]   and I made notes for every single.
[01:54:58.300 --> 01:54:59.140]   - Oh, except that one?
[01:54:59.140 --> 01:55:01.380]   - Yeah, well, that one I had, what the fuck?
[01:55:01.380 --> 01:55:02.620]   And then I took it out.
[01:55:02.620 --> 01:55:06.140]   - So is there something interesting,
[01:55:06.140 --> 01:55:07.760]   because you're so pragmatic,
[01:55:07.760 --> 01:55:09.060]   is there something interesting to say
[01:55:09.060 --> 01:55:13.440]   about intuition building about consciousness?
[01:55:13.440 --> 01:55:16.440]   Or is this something that we're just totally clueless about,
[01:55:16.440 --> 01:55:20.840]   that this is, let's focus on the body,
[01:55:20.840 --> 01:55:22.340]   the brain listens to the body,
[01:55:22.340 --> 01:55:24.720]   the body speaks to the brain,
[01:55:24.720 --> 01:55:27.120]   and let's just figure this piece out,
[01:55:27.120 --> 01:55:29.400]   and then consciousness will probably emerge somehow
[01:55:29.400 --> 01:55:30.640]   after that.
[01:55:30.640 --> 01:55:33.360]   - No, I think, you know, well, first of all,
[01:55:33.360 --> 01:55:35.320]   I'll just say up front,
[01:55:35.320 --> 01:55:37.840]   I am not a philosopher of consciousness,
[01:55:37.840 --> 01:55:40.560]   and I'm not a neuroscientist who focuses on consciousness.
[01:55:40.560 --> 01:55:42.060]   I mean, in some sense, I do study it
[01:55:42.060 --> 01:55:44.680]   because I study affect and mood,
[01:55:44.680 --> 01:55:46.720]   and that is the,
[01:55:46.720 --> 01:55:51.120]   you know, to use the phrase,
[01:55:51.120 --> 01:55:54.280]   that is the hard question of consciousness.
[01:55:54.280 --> 01:55:58.160]   How is it that your brain is modeling your body?
[01:55:58.160 --> 01:56:00.900]   Brain is modeling the sensory conditions of your body,
[01:56:00.900 --> 01:56:04.040]   and it's being updated,
[01:56:04.040 --> 01:56:06.640]   that model is being updated by the sense data
[01:56:06.640 --> 01:56:08.000]   that's coming from your body,
[01:56:08.000 --> 01:56:10.400]   and it's happening continuously your whole life,
[01:56:10.400 --> 01:56:15.800]   and you don't feel those sensations directly.
[01:56:15.800 --> 01:56:19.360]   What you feel is a general sense of pleasantness
[01:56:19.360 --> 01:56:21.360]   or unpleasantness, comfort, discomfort,
[01:56:21.360 --> 01:56:22.580]   feeling worked up, feeling calm.
[01:56:22.580 --> 01:56:24.800]   So we call that affect, you know,
[01:56:24.800 --> 01:56:26.600]   most people call it mood.
[01:56:26.600 --> 01:56:29.080]   So how is it that your brain gives you
[01:56:29.080 --> 01:56:34.040]   this very low-dimensional feeling of mood or affect
[01:56:34.040 --> 01:56:36.080]   when it's presumably receiving
[01:56:36.080 --> 01:56:39.440]   a very high-dimensional array of sense data,
[01:56:39.440 --> 01:56:42.840]   and the model that the brain is running of the body
[01:56:42.840 --> 01:56:44.720]   has to be high-dimensional
[01:56:44.720 --> 01:56:48.000]   because there's a lot going on in there, right?
[01:56:48.000 --> 01:56:50.180]   You're not aware, but as you're sitting there quietly,
[01:56:50.180 --> 01:56:54.800]   as your listeners, as our viewers are sitting--
[01:56:54.800 --> 01:56:56.720]   - They might be working out, running now,
[01:56:56.720 --> 01:56:58.320]   or as many of them write to me--
[01:56:58.320 --> 01:56:59.160]   - That's fair.
[01:56:59.160 --> 01:56:59.980]   - They're laying in bed,
[01:56:59.980 --> 01:57:01.160]   smoking weed with their eyes closed.
[01:57:01.160 --> 01:57:02.240]   - (laughs) That's fair.
[01:57:02.240 --> 01:57:04.200]   So maybe we should say that bit again then.
[01:57:04.200 --> 01:57:05.040]   (both laugh)
[01:57:05.040 --> 01:57:07.800]   So if, so some people may be working out,
[01:57:07.800 --> 01:57:09.720]   some people may be--
[01:57:09.720 --> 01:57:10.560]   - Relaxing.
[01:57:10.560 --> 01:57:14.100]   - Relaxing, but even if you're sitting very still
[01:57:14.100 --> 01:57:16.920]   while you're watching this or listening to this,
[01:57:16.920 --> 01:57:19.600]   there's a whole drama going on inside your body
[01:57:19.600 --> 01:57:21.400]   that you're largely unaware of,
[01:57:21.400 --> 01:57:26.160]   yet your brain makes you aware
[01:57:26.160 --> 01:57:29.640]   or gives you a status report in a sense
[01:57:29.640 --> 01:57:32.680]   by virtue of these mental features of feeling pleasant,
[01:57:32.680 --> 01:57:34.440]   feeling unpleasant, feeling comfortable,
[01:57:34.440 --> 01:57:36.400]   feeling uncomfortable, feeling energetic,
[01:57:36.400 --> 01:57:38.120]   feeling tired, and so on.
[01:57:38.120 --> 01:57:41.300]   And so how the hell is it doing that?
[01:57:41.300 --> 01:57:46.300]   That is the basic question of consciousness.
[01:57:46.300 --> 01:57:49.240]   - And like the status reports seem to be,
[01:57:49.240 --> 01:57:52.280]   in the way we experience them, seem to be quite simple.
[01:57:52.280 --> 01:57:56.600]   It doesn't feel like there's a lot of data.
[01:57:56.600 --> 01:57:57.680]   - Yeah, no, there isn't.
[01:57:57.680 --> 01:58:02.680]   So when you feel discomfort,
[01:58:02.680 --> 01:58:04.840]   when you're feeling basically like shit,
[01:58:04.840 --> 01:58:06.960]   you feel like shit, what does that tell you?
[01:58:06.960 --> 01:58:08.360]   Like what are you supposed to do next?
[01:58:08.360 --> 01:58:09.320]   What caused it?
[01:58:09.320 --> 01:58:12.160]   I mean, the thing is not one thing caused it, right?
[01:58:12.160 --> 01:58:15.560]   It's multiple factors probably influencing
[01:58:15.560 --> 01:58:16.920]   your physical state.
[01:58:16.920 --> 01:58:17.760]   Your body budget-- - You said it's
[01:58:17.760 --> 01:58:18.600]   very high dimensional, yeah.
[01:58:18.600 --> 01:58:20.200]   - It's very high dimensional.
[01:58:20.200 --> 01:58:25.480]   And that, and the,
[01:58:25.480 --> 01:58:27.960]   there are different temporal scales of influence, right?
[01:58:27.960 --> 01:58:32.960]   So the state of your gut is not just influenced
[01:58:32.960 --> 01:58:34.680]   by what you ate five minutes ago.
[01:58:34.680 --> 01:58:36.680]   It's also what you ate a day ago
[01:58:36.680 --> 01:58:38.600]   and two days ago and so on.
[01:58:38.600 --> 01:58:45.260]   So I think the,
[01:58:45.260 --> 01:58:50.300]   I'm not trying to weasel out of the question.
[01:58:50.300 --> 01:58:55.120]   I just think it's the hardest question, actually.
[01:58:55.120 --> 01:58:57.120]   - Do you think we'll ever understand it?
[01:58:57.120 --> 01:59:03.280]   As scientists.
[01:59:03.280 --> 01:59:06.680]   - I think that we will understand it
[01:59:06.680 --> 01:59:09.720]   as well as we understand other things
[01:59:09.720 --> 01:59:13.200]   like the birth of the universe
[01:59:13.200 --> 01:59:18.200]   or the nature of the universe, I guess I would say.
[01:59:18.200 --> 01:59:21.920]   So do I think we can get to that level of an explanation?
[01:59:21.920 --> 01:59:24.860]   I do, actually, but I think that we have to start asking
[01:59:24.860 --> 01:59:28.080]   somewhat different questions and approaching the science
[01:59:28.080 --> 01:59:30.280]   somewhat differently than we have in the past.
[01:59:30.280 --> 01:59:32.040]   - I mean, it's also possible that consciousness
[01:59:32.040 --> 01:59:33.600]   is much more difficult to understand
[01:59:33.600 --> 01:59:35.320]   than the nature of the universe.
[01:59:35.320 --> 01:59:37.680]   - It is, but I wasn't necessarily saying
[01:59:37.680 --> 01:59:40.800]   that it was a question that was of equivalent complexity.
[01:59:40.800 --> 01:59:45.240]   I was saying that I do think that we could get to some,
[01:59:45.240 --> 01:59:51.040]   I am optimistic that, I would not,
[01:59:51.040 --> 01:59:56.020]   I would be very willing to invest my time on this earth
[01:59:56.020 --> 01:59:58.380]   as a scientist in trying to answer that question
[01:59:58.380 --> 02:00:01.920]   if I could do it the way that I wanna do it,
[02:00:01.920 --> 02:00:04.380]   not in the way that it's currently being done.
[02:00:04.380 --> 02:00:05.600]   - So like rigorously?
[02:00:05.600 --> 02:00:07.980]   - I don't wanna say unrigorously.
[02:00:07.980 --> 02:00:10.340]   I just wanna say that there are a certain set of assumptions
[02:00:10.340 --> 02:00:13.420]   that, you know, scientists have what I would call
[02:00:13.420 --> 02:00:14.720]   ontological commitments.
[02:00:14.720 --> 02:00:17.060]   They're commitments about the way the world is
[02:00:17.060 --> 02:00:19.720]   or the way that nature is.
[02:00:19.720 --> 02:00:24.720]   And these commitments lead scientists sometimes blindly,
[02:00:24.720 --> 02:00:27.220]   scientists sometimes, sometimes scientists are aware
[02:00:27.220 --> 02:00:29.380]   of these commitments, but sometimes they're not.
[02:00:29.380 --> 02:00:31.580]   And these commitments on the list influence
[02:00:31.580 --> 02:00:35.500]   how scientists ask questions, what they measure,
[02:00:35.500 --> 02:00:40.100]   how they measure, and I just have very different views
[02:00:40.100 --> 02:00:41.660]   than a lot of my colleagues
[02:00:41.660 --> 02:00:43.740]   about the ways to approach this.
[02:00:43.740 --> 02:00:47.020]   Not everybody, but the way that I would approach it
[02:00:47.020 --> 02:00:50.920]   would be different and it would cost more
[02:00:50.920 --> 02:00:53.160]   and it would take longer.
[02:00:53.160 --> 02:00:54.840]   It doesn't fit very well
[02:00:54.840 --> 02:00:56.920]   into the current incentive structure of science.
[02:00:56.920 --> 02:00:59.680]   And so do I think that doing science
[02:00:59.680 --> 02:01:01.120]   the way science is currently done
[02:01:01.120 --> 02:01:02.860]   with the budget that it currently has
[02:01:02.860 --> 02:01:04.840]   and the incentive structure that it currently has,
[02:01:04.840 --> 02:01:05.760]   will we have an answer?
[02:01:05.760 --> 02:01:07.320]   No, I think absolutely not.
[02:01:07.320 --> 02:01:08.820]   Good luck is what I would say.
[02:01:08.820 --> 02:01:13.040]   - People love book recommendations.
[02:01:13.040 --> 02:01:15.160]   Let me ask what three books?
[02:01:15.160 --> 02:01:17.780]   - Well, you can't just give me three.
[02:01:17.780 --> 02:01:19.180]   I mean, like really, three?
[02:01:19.180 --> 02:01:23.100]   - What seven and a half books you can recommend.
[02:01:23.100 --> 02:01:25.140]   So you're also the author of seven and a half lessons
[02:01:25.140 --> 02:01:26.640]   about the brain.
[02:01:26.640 --> 02:01:29.780]   You're author of "How Emotions Are Made."
[02:01:29.780 --> 02:01:33.340]   Okay, so definitely those are the top two recommendations
[02:01:33.340 --> 02:01:35.300]   of all, the two greatest books of all time.
[02:01:35.300 --> 02:01:37.940]   Other than that, are there books that,
[02:01:37.940 --> 02:01:41.140]   technical, fiction, philosophical, that you've enjoyed
[02:01:41.140 --> 02:01:42.780]   and you might recommend to others?
[02:01:42.780 --> 02:01:43.620]   - Yes.
[02:01:44.740 --> 02:01:46.900]   Actually, every PhD student,
[02:01:46.900 --> 02:01:50.420]   when they graduate with their PhD,
[02:01:50.420 --> 02:01:52.940]   I give them a set, like a little library,
[02:01:52.940 --> 02:01:55.980]   like a set of books, some of which they've already read,
[02:01:55.980 --> 02:01:57.820]   some of which I want them to read.
[02:01:57.820 --> 02:02:04.060]   But I think nonfiction books, I would read,
[02:02:04.060 --> 02:02:08.220]   the things I would recommend are "The Triple Helix"
[02:02:08.220 --> 02:02:10.860]   by Richard Luontan.
[02:02:10.860 --> 02:02:14.640]   It's a little book published in 2000,
[02:02:14.640 --> 02:02:18.100]   which is, I think, a really good introduction
[02:02:18.100 --> 02:02:23.100]   to complexity and population thinking,
[02:02:23.100 --> 02:02:25.720]   as opposed to essentialism.
[02:02:25.720 --> 02:02:28.780]   So this idea, essentialism is this idea that, you know,
[02:02:28.780 --> 02:02:30.480]   there's an essence to each person,
[02:02:30.480 --> 02:02:33.420]   whether it's a soul or your genes or what have you,
[02:02:33.420 --> 02:02:35.980]   as opposed to this idea that you,
[02:02:35.980 --> 02:02:38.700]   we have the kind of nature that requires a nurturer.
[02:02:38.700 --> 02:02:43.700]   We are, we are, you are the product of a complex dance
[02:02:44.000 --> 02:02:47.060]   between an environment,
[02:02:47.060 --> 02:02:49.460]   between a set of genes and an environment
[02:02:49.460 --> 02:02:52.580]   that turns those genes on and off
[02:02:52.580 --> 02:02:54.460]   to produce your brain and your body,
[02:02:54.460 --> 02:02:57.180]   and really who you are at any given moment.
[02:02:57.180 --> 02:02:59.300]   - It's a good title for that, "Triple Helix."
[02:02:59.300 --> 02:03:00.500]   So playing on the double helix,
[02:03:00.500 --> 02:03:03.960]   where it's just the biology, it's bigger than the biology.
[02:03:03.960 --> 02:03:05.400]   - Exactly.
[02:03:05.400 --> 02:03:06.280]   It's a wonderful book.
[02:03:06.280 --> 02:03:08.300]   I've read it probably six or seven times
[02:03:08.300 --> 02:03:09.140]   throughout the year.
[02:03:09.140 --> 02:03:10.820]   He has another book, too, which is,
[02:03:11.780 --> 02:03:14.360]   it's more, I think scientists would find it,
[02:03:14.360 --> 02:03:15.500]   I don't know, I've loved it.
[02:03:15.500 --> 02:03:18.600]   It's called "Biology as Ideology."
[02:03:18.600 --> 02:03:20.860]   And it really is all about,
[02:03:20.860 --> 02:03:22.860]   I wouldn't call it one of the best books of all time,
[02:03:22.860 --> 02:03:26.360]   but I love the book because it really does point out,
[02:03:26.360 --> 02:03:31.640]   you know, that science as it's currently practiced,
[02:03:31.640 --> 02:03:33.160]   I mean, the book was written in 1991,
[02:03:33.160 --> 02:03:34.920]   but it actually, I think, still holds,
[02:03:34.920 --> 02:03:36.980]   that science as it's currently practiced
[02:03:36.980 --> 02:03:38.920]   has a set of ontological commitments
[02:03:38.920 --> 02:03:40.480]   which are somewhat problematic.
[02:03:41.600 --> 02:03:43.540]   - So the assumptions are limiting.
[02:03:43.540 --> 02:03:44.820]   - Yeah, in ways that you,
[02:03:44.820 --> 02:03:47.300]   it's like you're a fish in water and you don't,
[02:03:47.300 --> 02:03:49.060]   like, okay, so, yeah, so here's--
[02:03:49.060 --> 02:03:50.500]   - David Foster Wallace and stuff.
[02:03:50.500 --> 02:03:52.700]   - Well, but, you know, but here's a really cool thing
[02:03:52.700 --> 02:03:55.180]   I just learned recently.
[02:03:55.180 --> 02:03:57.940]   Is it okay to go off on this tangent for a minute?
[02:03:57.940 --> 02:04:00.580]   - Yeah, yeah, let's go tangency, great.
[02:04:00.580 --> 02:04:02.620]   - I was just gonna say that I just learned recently
[02:04:02.620 --> 02:04:06.140]   that we don't have water receptors on our skin.
[02:04:06.140 --> 02:04:07.740]   So how do you know when you're sweating?
[02:04:07.740 --> 02:04:10.620]   How do you know when a raindrop,
[02:04:10.620 --> 02:04:12.500]   when it's gonna rain and, you know,
[02:04:12.500 --> 02:04:13.780]   like a raindrop hits your skin
[02:04:13.780 --> 02:04:16.620]   and you can feel that little drop of wetness?
[02:04:16.620 --> 02:04:18.500]   How is it that you feel that drop of wetness
[02:04:18.500 --> 02:04:22.340]   when we don't have water receptors in our skin?
[02:04:22.340 --> 02:04:23.180]   And I was, when I--
[02:04:23.180 --> 02:04:24.900]   - My mind's blown already.
[02:04:24.900 --> 02:04:27.220]   - Yeah, that was, I have my reaction too, right?
[02:04:27.220 --> 02:04:29.260]   I was like, of course we don't
[02:04:29.260 --> 02:04:31.500]   because we evolved in the water.
[02:04:31.500 --> 02:04:33.300]   Like, why would we need, you know, it just,
[02:04:33.300 --> 02:04:34.500]   it was just this, like, you know,
[02:04:34.500 --> 02:04:36.420]   you have these moments where you're like, oh,
[02:04:36.420 --> 02:04:38.300]   of course, let me just like go, yeah, so--
[02:04:38.300 --> 02:04:40.740]   - And you'll never see rain the same way again.
[02:04:40.740 --> 02:04:44.260]   - So the answer is it's a combination
[02:04:44.260 --> 02:04:47.780]   of temperature and touch.
[02:04:47.780 --> 02:04:52.580]   But it's a complex sense that's only computed in your brain.
[02:04:52.580 --> 02:04:54.340]   There's no receptor for it.
[02:04:54.340 --> 02:04:55.380]   Anyways.
[02:04:55.380 --> 02:04:58.500]   - Yeah, that's why, like, snow versus cold rain
[02:04:58.500 --> 02:05:00.580]   versus warm rain all feel different
[02:05:00.580 --> 02:05:03.660]   'cause you're trying to infer stuff from the temperature
[02:05:03.660 --> 02:05:05.900]   and the size of the droplets, it's fascinating.
[02:05:05.900 --> 02:05:07.660]   - Yeah, your brain is a prediction machine.
[02:05:07.660 --> 02:05:11.220]   It's using lots and lots of information combining it.
[02:05:11.220 --> 02:05:16.220]   Anyways, so, but, so, "Biology is Ideology"
[02:05:16.220 --> 02:05:18.580]   is, I wouldn't say it's one of the greatest books
[02:05:18.580 --> 02:05:22.580]   of all time, but it is a really useful book.
[02:05:22.580 --> 02:05:25.700]   There's a book by, if you're interested in psychology
[02:05:25.700 --> 02:05:28.460]   or the mind at all, there's a wonderful book,
[02:05:28.460 --> 02:05:33.020]   a little, it's a fairly small book
[02:05:33.020 --> 02:05:36.300]   called "Naming the Mind" by Kurt Danziger
[02:05:36.300 --> 02:05:38.580]   who's a historian of psychology.
[02:05:38.580 --> 02:05:42.500]   Everybody in my lab reads both of these books.
[02:05:42.500 --> 02:05:43.980]   - So what's the book?
[02:05:43.980 --> 02:05:45.620]   - It's about the origin of the,
[02:05:45.620 --> 02:05:49.620]   where did we get the theory of mind that we have
[02:05:49.620 --> 02:05:53.500]   that the human mind is populated by thoughts and feelings
[02:05:53.500 --> 02:05:57.860]   and perceptions, and where did those categories come from?
[02:05:57.860 --> 02:06:00.580]   Because they don't exist in all cultures.
[02:06:00.580 --> 02:06:05.980]   - Oh, so this isn't, that's a cultural construct?
[02:06:05.980 --> 02:06:08.060]   - The idea that you have thoughts and feelings
[02:06:08.060 --> 02:06:11.300]   and they're very distinct is definitely a cultural construct.
[02:06:11.300 --> 02:06:15.260]   - It's another mind-blowing thing, just like the rain?
[02:06:15.260 --> 02:06:21.220]   - So Kurt Danziger is a, the opening chapter in that book
[02:06:21.220 --> 02:06:26.260]   is absolutely mind-blowing.
[02:06:26.260 --> 02:06:29.420]   I love it, I love it.
[02:06:29.420 --> 02:06:30.980]   I just think it's fantastic.
[02:06:32.140 --> 02:06:35.980]   And I would say that there are many, many
[02:06:35.980 --> 02:06:39.060]   popular science books that I could recommend
[02:06:39.060 --> 02:06:42.940]   that I think are extremely well-written in their own way.
[02:06:42.940 --> 02:06:44.580]   Before I, maybe I said this to you,
[02:06:44.580 --> 02:06:49.180]   but before I undertook writing "How Emotions Are Made,"
[02:06:49.180 --> 02:06:53.060]   I read, I don't know, somewhere on the order of 50 or 60
[02:06:53.060 --> 02:06:56.580]   popular science books to try to figure out
[02:06:56.580 --> 02:07:00.500]   how to write a popular science book
[02:07:00.500 --> 02:07:03.980]   because while there are many books about writing,
[02:07:03.980 --> 02:07:05.980]   Stephen King has a great book about--
[02:07:05.980 --> 02:07:07.380]   - On writing? - On writing.
[02:07:07.380 --> 02:07:11.500]   And where he gives tips interlaced
[02:07:11.500 --> 02:07:13.380]   with his own personal history.
[02:07:13.380 --> 02:07:17.500]   That was where I learned you write for a specific person.
[02:07:17.500 --> 02:07:19.340]   You have a specific person in mind.
[02:07:19.340 --> 02:07:22.740]   And that's, for me, that person is Dan.
[02:07:22.740 --> 02:07:23.580]   - That's fascinating.
[02:07:23.580 --> 02:07:24.780]   I mean, that's a whole 'nother conversation
[02:07:24.780 --> 02:07:27.980]   to have like which popular science books,
[02:07:27.980 --> 02:07:31.100]   like what you learned from that search.
[02:07:31.100 --> 02:07:34.580]   Because there's, I have, for me,
[02:07:34.580 --> 02:07:37.020]   some popular science books that I just roll my eyes,
[02:07:37.020 --> 02:07:41.780]   like this is too, it's the same with TED Talks.
[02:07:41.780 --> 02:07:45.140]   Like some of them go too much into the flowery
[02:07:45.140 --> 02:07:48.860]   and I would say don't give enough respect
[02:07:48.860 --> 02:07:50.660]   to the intelligence of the reader.
[02:07:50.660 --> 02:07:55.140]   But this is my own bias, very specific.
[02:07:55.140 --> 02:07:56.660]   - I completely agree with you.
[02:07:56.660 --> 02:07:59.220]   And in fact, I have a colleague,
[02:07:59.220 --> 02:08:03.220]   his name is Van Yang, who,
[02:08:03.220 --> 02:08:08.020]   he produced a cinematic lecture
[02:08:08.020 --> 02:08:09.300]   of how emotions are made
[02:08:09.300 --> 02:08:13.740]   that we wrote together with Joseph Fridman, no relation.
[02:08:13.740 --> 02:08:16.060]   - Well, we're all related.
[02:08:16.060 --> 02:08:17.380]   - Well, I mean, you and I are probably,
[02:08:17.380 --> 02:08:18.660]   you know, have some, yeah.
[02:08:18.660 --> 02:08:21.260]   - Yeah, I remember.
[02:08:21.260 --> 02:08:22.780]   The memories are in there somewhere.
[02:08:22.780 --> 02:08:26.100]   - Yeah, it's from many, many, many generations ago.
[02:08:26.100 --> 02:08:28.700]   Well, half my family is Russian, so from--
[02:08:28.700 --> 02:08:29.540]   - The good half.
[02:08:29.540 --> 02:08:30.380]   - The good half, right.
[02:08:30.380 --> 02:08:33.060]   (both laughing)
[02:08:33.060 --> 02:08:38.060]   But, you know, he, his goal actually is to produce
[02:08:38.060 --> 02:08:48.740]   you know, videos and lectures that are beautiful
[02:08:48.740 --> 02:08:53.740]   and educational and that don't dumb the material down.
[02:08:54.140 --> 02:08:57.300]   And he's really remarkable at it, actually.
[02:08:57.300 --> 02:09:00.220]   I mean, just, but again, you know,
[02:09:00.220 --> 02:09:02.380]   that requires a bit of a paradigm shift.
[02:09:02.380 --> 02:09:03.540]   We could have a whole conversation
[02:09:03.540 --> 02:09:06.460]   about the split between entertainment
[02:09:06.460 --> 02:09:07.900]   and education in this country
[02:09:07.900 --> 02:09:09.140]   and why it is the way it is,
[02:09:09.140 --> 02:09:11.620]   but that's another conversation.
[02:09:11.620 --> 02:09:12.460]   - To be continued.
[02:09:12.460 --> 02:09:15.420]   - But I would say, if I were to pick one book
[02:09:15.420 --> 02:09:18.620]   that I think is a really good example
[02:09:18.620 --> 02:09:20.220]   of good science writing,
[02:09:20.220 --> 02:09:21.940]   it would be "The Beak of the Finch."
[02:09:21.940 --> 02:09:26.940]   Which won a Pulitzer Prize a number of years ago.
[02:09:26.940 --> 02:09:31.980]   And I'm not, I'm not remembering the author's name.
[02:09:31.980 --> 02:09:32.820]   I'm blanking.
[02:09:32.820 --> 02:09:37.820]   - But the, I'm guessing, is it focusing on birds
[02:09:37.820 --> 02:09:40.620]   and the evolution of birds?
[02:09:40.620 --> 02:09:43.220]   - Actually, there's also "The Evolution of Beauty."
[02:09:43.220 --> 02:09:44.060]   - That's, yeah.
[02:09:44.060 --> 02:09:45.980]   - Yeah, which is also a great book.
[02:09:45.980 --> 02:09:49.220]   But no, "The Beak of the Finch" is,
[02:09:49.220 --> 02:09:52.420]   "The Beak of the Finch" is, it's a,
[02:09:52.420 --> 02:09:56.820]   it has two storylines that are interwoven.
[02:09:56.820 --> 02:10:01.820]   One is about Darwin and Darwin's explorations
[02:10:01.820 --> 02:10:04.180]   in the Galapagos Island.
[02:10:04.180 --> 02:10:07.820]   And then modern day researchers from Princeton
[02:10:07.820 --> 02:10:10.900]   who have a research program in the Galapagos
[02:10:10.900 --> 02:10:13.060]   looking at Darwin's finches.
[02:10:13.060 --> 02:10:18.060]   And it's just a really, first of all,
[02:10:18.300 --> 02:10:21.380]   there's top-notch science in there.
[02:10:21.380 --> 02:10:24.820]   And really science, like, you know,
[02:10:24.820 --> 02:10:28.460]   evolutionary biology that a lot of people don't know.
[02:10:28.460 --> 02:10:30.860]   And it's told really, really well.
[02:10:30.860 --> 02:10:34.180]   - It sounds like there also, there's a narrative in there.
[02:10:34.180 --> 02:10:35.820]   There's, it's like storytelling too.
[02:10:35.820 --> 02:10:38.700]   - Yeah, I think all good popular science books
[02:10:38.700 --> 02:10:40.740]   are storytelling for just, you know,
[02:10:40.740 --> 02:10:42.980]   but storytelling grounded, constrained by,
[02:10:42.980 --> 02:10:44.660]   you know, the evidence.
[02:10:44.660 --> 02:10:47.220]   And then I just want to say that there are,
[02:10:47.220 --> 02:10:51.300]   for fiction, I'm a really big fan of love stories,
[02:10:51.300 --> 02:10:54.980]   just to return us to the topic that we began with.
[02:10:54.980 --> 02:10:59.620]   And so my, some of my favorite love stories
[02:10:59.620 --> 02:11:04.240]   are "Major Pettigrew's Last Stand" by Helen Simonson.
[02:11:04.240 --> 02:11:08.700]   It's a love story about people
[02:11:08.700 --> 02:11:11.260]   who you wouldn't expect to fall in love
[02:11:11.260 --> 02:11:12.980]   and all the people around them
[02:11:12.980 --> 02:11:15.000]   who have to overcome their prejudices.
[02:11:16.100 --> 02:11:19.500]   And I love this book.
[02:11:19.500 --> 02:11:20.340]   - What do you like?
[02:11:20.340 --> 02:11:21.900]   Like what makes a good love story?
[02:11:21.900 --> 02:11:24.540]   - There isn't one thing, you know,
[02:11:24.540 --> 02:11:26.860]   there are many different things that make a good love story,
[02:11:26.860 --> 02:11:28.500]   but I think in this case,
[02:11:28.500 --> 02:11:32.020]   you can feel,
[02:11:32.020 --> 02:11:36.140]   you can feel the journey.
[02:11:36.140 --> 02:11:39.100]   You can feel the journey that these characters are on
[02:11:39.100 --> 02:11:42.860]   and all the people around them are on this journey too,
[02:11:42.860 --> 02:11:44.460]   basically to come to grips
[02:11:44.460 --> 02:11:47.700]   with this really unexpected love,
[02:11:47.700 --> 02:11:50.260]   really profound love that develops
[02:11:50.260 --> 02:11:52.020]   between these two characters
[02:11:52.020 --> 02:11:55.660]   who are very unlikely to have fallen in love, but they do.
[02:11:55.660 --> 02:11:59.940]   And it's just, it's very gentle.
[02:11:59.940 --> 02:12:01.580]   Another book like that is
[02:12:01.580 --> 02:12:07.000]   the storied life of A.J. Feerke,
[02:12:07.000 --> 02:12:11.080]   which is also a love story,
[02:12:11.080 --> 02:12:12.880]   but in this case, it's a love story
[02:12:12.880 --> 02:12:17.880]   between a little girl and her adopted dad.
[02:12:17.880 --> 02:12:23.000]   And the dad is this like real curmudgeony,
[02:12:23.000 --> 02:12:26.460]   you know, guy,
[02:12:26.460 --> 02:12:28.100]   but of course there's a story there.
[02:12:28.100 --> 02:12:31.740]   And it's just a beautiful love story.
[02:12:31.740 --> 02:12:36.620]   But it also, it's like everybody in this community
[02:12:36.620 --> 02:12:40.940]   falls in love with him because he falls in love with her.
[02:12:40.940 --> 02:12:44.420]   And he, you know, she just gets left at his store,
[02:12:44.420 --> 02:12:46.840]   his bookstore, he has this failing bookstore.
[02:12:46.840 --> 02:12:51.820]   And he discovers that, you know,
[02:12:51.820 --> 02:12:54.440]   he feels like inexplicably this need
[02:12:54.440 --> 02:12:56.540]   to take care of this little baby.
[02:12:56.540 --> 02:13:00.380]   And this whole life emerges out of that one decision,
[02:13:00.380 --> 02:13:03.960]   which is really beautiful, actually.
[02:13:03.960 --> 02:13:06.660]   It's very poignant.
[02:13:06.660 --> 02:13:10.320]   - Do you think the greatest stories have a happy ending
[02:13:10.320 --> 02:13:14.060]   or a heartbreak at the end?
[02:13:14.060 --> 02:13:15.460]   - That's such a Russian question.
[02:13:15.460 --> 02:13:18.140]   It's like Russian tragedies, you know?
[02:13:18.140 --> 02:13:20.020]   - So I would say the answer to that for me,
[02:13:20.020 --> 02:13:21.860]   there has to be heartbreak.
[02:13:21.860 --> 02:13:24.180]   - Yeah, I really don't like heartbreak.
[02:13:24.180 --> 02:13:25.620]   I don't like heartbreak.
[02:13:25.620 --> 02:13:27.580]   I want there to be a happy ending,
[02:13:27.580 --> 02:13:32.320]   or at least a hopeful ending.
[02:13:32.320 --> 02:13:35.300]   But, you know, like Dr. Zhivago,
[02:13:35.300 --> 02:13:37.940]   like, or the English patient.
[02:13:37.940 --> 02:13:40.820]   Oh my goodness, like why?
[02:13:40.820 --> 02:13:44.100]   Oh, it's just, yeah, no.
[02:13:44.100 --> 02:13:47.760]   - Well, I don't think there's a better way to end it
[02:13:47.760 --> 02:13:51.000]   on a happy note like this.
[02:13:51.000 --> 02:13:53.000]   Lisa, like I said, I'm a huge fan of yours.
[02:13:53.000 --> 02:13:57.720]   Thank you for wasting yet more time with me talking again.
[02:13:57.720 --> 02:13:59.520]   People should definitely get your book.
[02:13:59.520 --> 02:14:02.040]   And maybe one day I can't wait
[02:14:02.040 --> 02:14:03.520]   to talk to your husband as well.
[02:14:03.520 --> 02:14:05.520]   - Well, right back at you, Lexi.
[02:14:07.720 --> 02:14:09.240]   - Thanks for listening to this conversation
[02:14:09.240 --> 02:14:10.880]   with Lisa Feldman Barrett,
[02:14:10.880 --> 02:14:12.760]   and thank you to our sponsors.
[02:14:12.760 --> 02:14:15.400]   Athletic Greens, the all-in-one drink
[02:14:15.400 --> 02:14:16.760]   that I start every day with
[02:14:16.760 --> 02:14:19.280]   to cover all my nutritional bases.
[02:14:19.280 --> 02:14:21.980]   8 Sleep, a mattress that cools itself
[02:14:21.980 --> 02:14:24.960]   and gives me yet another reason to enjoy sleep.
[02:14:24.960 --> 02:14:28.240]   Masterclass, online courses that I enjoy
[02:14:28.240 --> 02:14:31.520]   from some of the most amazing humans in history.
[02:14:31.520 --> 02:14:36.140]   And BetterHelp, online therapy with a licensed professional.
[02:14:36.140 --> 02:14:38.640]   Please check out these sponsors in the description
[02:14:38.640 --> 02:14:42.040]   to get a discount and to support this podcast.
[02:14:42.040 --> 02:14:44.220]   If you enjoy this thing, subscribe on YouTube,
[02:14:44.220 --> 02:14:46.600]   review it with Five Stars on Apple Podcasts,
[02:14:46.600 --> 02:14:49.160]   follow on Spotify, support on Patreon,
[02:14:49.160 --> 02:14:52.440]   or connect with me on Twitter @LexFriedman.
[02:14:52.440 --> 02:14:54.840]   And now, let me leave you with some words
[02:14:54.840 --> 02:14:58.320]   from Sun Tzu and the Art of War.
[02:14:58.320 --> 02:15:01.000]   There are not more than five musical notes,
[02:15:01.000 --> 02:15:03.600]   yet the combination of these five give rise
[02:15:03.600 --> 02:15:06.680]   to more melodies than can ever be heard.
[02:15:06.680 --> 02:15:09.640]   There are not more than five primary colors,
[02:15:09.640 --> 02:15:12.920]   yet in combination they produce more hues
[02:15:12.920 --> 02:15:15.140]   than can ever be seen.
[02:15:15.140 --> 02:15:18.840]   There are not more than five cardinal tastes,
[02:15:18.840 --> 02:15:22.920]   and yet combinations of them yield more flavors
[02:15:22.920 --> 02:15:24.400]   than can ever be tasted.
[02:15:24.400 --> 02:15:28.700]   Thank you for listening, and hope to see you next time.
[02:15:28.700 --> 02:15:31.280]   (upbeat music)
[02:15:31.280 --> 02:15:33.860]   (upbeat music)
[02:15:33.860 --> 02:15:43.860]   [BLANK_AUDIO]

