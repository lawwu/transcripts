
[00:00:00.000 --> 00:00:02.280]   In this video, I propose to ask the question
[00:00:02.280 --> 00:00:05.240]   that was asked by Alan Turing almost 70 years ago
[00:00:05.240 --> 00:00:09.060]   in his paper, Computing Machinery and Intelligence.
[00:00:09.060 --> 00:00:10.780]   Can machines think?
[00:00:10.780 --> 00:00:16.280]   This is the first paper in a paper reading club
[00:00:16.280 --> 00:00:18.600]   that we started focused on artificial intelligence,
[00:00:18.600 --> 00:00:21.080]   but also including mathematics, physics,
[00:00:21.080 --> 00:00:22.360]   computer science, neuroscience,
[00:00:22.360 --> 00:00:25.020]   all of the scientific and engineering disciplines.
[00:00:25.020 --> 00:00:27.440]   On the surface, this is a philosophical paper,
[00:00:27.440 --> 00:00:30.200]   but really it's one of the most impactful, important
[00:00:30.200 --> 00:00:32.720]   first steps towards actually engineering
[00:00:32.720 --> 00:00:35.680]   intelligent systems by providing a test,
[00:00:35.680 --> 00:00:39.480]   a benchmark that we call today the Turing test
[00:00:39.480 --> 00:00:43.040]   of how we can actually know quantifiably
[00:00:43.040 --> 00:00:46.560]   that a system has become intelligent.
[00:00:46.560 --> 00:00:50.200]   So I'd like to talk about an overview of ideas
[00:00:50.200 --> 00:00:52.740]   in the paper, provide some of the objections
[00:00:52.740 --> 00:00:55.520]   inside the paper and external to the paper,
[00:00:55.520 --> 00:00:58.320]   consider some alternatives to the test proposed
[00:00:58.320 --> 00:01:03.100]   within the paper, and then finished with some takeaways.
[00:01:03.100 --> 00:01:04.720]   Like I said, the title of the paper
[00:01:04.720 --> 00:01:07.080]   was Computing Machinery and Intelligence,
[00:01:07.080 --> 00:01:10.240]   published almost 70 years ago in 1950,
[00:01:10.240 --> 00:01:12.400]   author Alan Turing.
[00:01:12.400 --> 00:01:15.120]   And to me, now we can argue about this,
[00:01:15.120 --> 00:01:17.720]   on the slide I say it's one of the most impactful papers.
[00:01:17.720 --> 00:01:20.620]   To me, it probably is the most impactful paper
[00:01:20.620 --> 00:01:22.680]   in the history of artificial intelligence
[00:01:22.680 --> 00:01:26.320]   while only being a philosophy paper.
[00:01:26.320 --> 00:01:29.160]   I think the number of researchers
[00:01:29.160 --> 00:01:31.680]   from inside computer science and from outside
[00:01:31.680 --> 00:01:34.400]   that has inspired, has made dream
[00:01:34.400 --> 00:01:38.440]   at a collective intelligence level of our species
[00:01:38.440 --> 00:01:42.320]   inspired that this is possible, I think is immeasurable.
[00:01:42.320 --> 00:01:44.960]   For all the major engineering breakthroughs
[00:01:44.960 --> 00:01:47.920]   and computer science breakthroughs and papers
[00:01:47.920 --> 00:01:50.600]   stretching all the way back to the 30s and 40s
[00:01:50.600 --> 00:01:55.040]   with even the work by Alan Turing with the Turing machine,
[00:01:55.040 --> 00:01:59.220]   some of the mathematical foundations of computer science
[00:01:59.220 --> 00:02:02.680]   to today with deep learning, a sequence of papers
[00:02:02.680 --> 00:02:04.680]   from the very practical Alex Ned paper
[00:02:04.680 --> 00:02:06.000]   to the back propagation papers.
[00:02:06.000 --> 00:02:09.400]   So all of these papers that underlie
[00:02:09.400 --> 00:02:11.720]   the actual successes of the field,
[00:02:11.720 --> 00:02:15.120]   I think the seed was planted.
[00:02:15.120 --> 00:02:19.800]   The dream was born with this paper.
[00:02:19.800 --> 00:02:23.520]   And it happens to have some of my favorite opening lines
[00:02:23.520 --> 00:02:25.600]   of any paper I've ever read.
[00:02:25.600 --> 00:02:29.680]   It goes, I propose to consider the question,
[00:02:29.680 --> 00:02:31.720]   can machines think?
[00:02:31.720 --> 00:02:33.280]   This should begin with the definitions
[00:02:33.280 --> 00:02:37.000]   of the meaning of the terms machine and think.
[00:02:37.000 --> 00:02:39.880]   The definition might be framed so as to reflect
[00:02:39.880 --> 00:02:42.920]   so far as possible the normal use of the words,
[00:02:42.920 --> 00:02:44.960]   but this attitude is dangerous.
[00:02:44.960 --> 00:02:47.360]   If the meaning of the words machine and think
[00:02:47.360 --> 00:02:49.200]   are to be found in examining
[00:02:49.200 --> 00:02:50.720]   how they're commonly used,
[00:02:50.720 --> 00:02:52.520]   it is difficult to escape the conclusion
[00:02:52.520 --> 00:02:54.640]   that the meaning and the answer to the question,
[00:02:54.640 --> 00:02:57.240]   can machines think is to be sought
[00:02:57.240 --> 00:03:00.240]   in a statistical survey such as a Gallup poll.
[00:03:00.240 --> 00:03:02.040]   But this is absurd.
[00:03:02.040 --> 00:03:03.960]   Instead of attempting such a definition,
[00:03:03.960 --> 00:03:06.320]   I shall replace the question by another,
[00:03:06.320 --> 00:03:08.200]   which is closely related to it
[00:03:08.200 --> 00:03:12.040]   and is expressed in relatively unambiguous terms.
[00:03:12.040 --> 00:03:14.760]   And he goes on to define the imitation game,
[00:03:14.760 --> 00:03:18.160]   the construction that we today call the Turing test,
[00:03:18.160 --> 00:03:19.640]   which goes like this.
[00:03:19.640 --> 00:03:23.580]   There's a human interrogator on one side of the wall,
[00:03:23.580 --> 00:03:24.960]   and there's two entities,
[00:03:24.960 --> 00:03:29.320]   one a machine, one a human on the other side.
[00:03:29.320 --> 00:03:32.080]   And the human interrogator communicates
[00:03:32.080 --> 00:03:34.680]   with the two entities on the other side of the wall
[00:03:34.680 --> 00:03:38.280]   by written word, by passing those back and forth.
[00:03:38.280 --> 00:03:40.280]   And after some time of this conversation,
[00:03:40.280 --> 00:03:43.280]   the human interrogator is tasked with making a decision,
[00:03:43.280 --> 00:03:46.040]   which of the other two entities is a human
[00:03:46.040 --> 00:03:47.560]   and which is a machine.
[00:03:47.560 --> 00:03:50.480]   I think this is a powerful leap of engineering,
[00:03:50.480 --> 00:03:54.040]   which is take an ambiguous but a profound question
[00:03:54.040 --> 00:03:59.040]   like can machines think and convert it into a concrete test
[00:03:59.040 --> 00:04:01.960]   that can serve as a benchmark of intelligence.
[00:04:01.960 --> 00:04:04.720]   But there's echoes in this question
[00:04:04.720 --> 00:04:07.520]   to some of the other profound questions that we often ask.
[00:04:07.520 --> 00:04:09.000]   So not only can machines think,
[00:04:09.000 --> 00:04:11.120]   but can machines be conscious?
[00:04:11.120 --> 00:04:12.780]   Can machines fall in love?
[00:04:12.780 --> 00:04:17.080]   Can machines create art, music, poetry?
[00:04:17.080 --> 00:04:19.840]   Can machines enjoy a delicious meal,
[00:04:19.840 --> 00:04:22.000]   a piece of chocolate cake?
[00:04:22.000 --> 00:04:24.800]   I think these are really, really important questions,
[00:04:24.800 --> 00:04:26.920]   but very difficult to ask
[00:04:26.920 --> 00:04:29.840]   when we're trying to create a non-human system
[00:04:29.840 --> 00:04:33.120]   that tries to achieve human level capabilities.
[00:04:33.120 --> 00:04:36.680]   So that's where Turing formulates this imitation game.
[00:04:36.680 --> 00:04:40.580]   And his prediction was that by the year 2000,
[00:04:40.580 --> 00:04:44.040]   or in 50 years since the paper,
[00:04:44.040 --> 00:04:47.400]   that a machine with 100 megabytes of storage
[00:04:47.400 --> 00:04:49.240]   will fool 30% of humans
[00:04:49.240 --> 00:04:52.020]   in a five-minute test of conversation.
[00:04:52.020 --> 00:04:56.480]   Another broader societal prediction he made,
[00:04:56.480 --> 00:04:58.160]   which I think is also interesting,
[00:04:58.160 --> 00:05:00.800]   is that people will no longer consider a phrase
[00:05:00.800 --> 00:05:04.280]   like thinking machine contradictory.
[00:05:04.280 --> 00:05:08.400]   So basically artificial intelligence at a human level
[00:05:08.400 --> 00:05:10.440]   becomes so commonplace that we would just take it
[00:05:10.440 --> 00:05:11.700]   for granted.
[00:05:11.700 --> 00:05:14.420]   And the other part that he goes at length
[00:05:14.420 --> 00:05:16.680]   towards the end of the paper to describe,
[00:05:16.680 --> 00:05:18.820]   which he believes that learning machines,
[00:05:18.820 --> 00:05:20.700]   or machine learning,
[00:05:20.700 --> 00:05:23.740]   will be a critical component of this success.
[00:05:23.740 --> 00:05:25.940]   I think it's also useful to break apart
[00:05:25.940 --> 00:05:28.860]   two implied claims within the paper,
[00:05:28.860 --> 00:05:31.940]   open claims, open questions.
[00:05:31.940 --> 00:05:36.420]   One is that the imitation game as Turing proposes
[00:05:36.420 --> 00:05:38.440]   is a good test of intelligence.
[00:05:39.300 --> 00:05:44.300]   And the second is that machines can actually pass this test.
[00:05:44.300 --> 00:05:46.540]   So when you say can machines think,
[00:05:46.540 --> 00:05:50.540]   you're both proposing an engineering benchmark
[00:05:50.540 --> 00:05:52.480]   for the word think,
[00:05:52.480 --> 00:05:54.400]   and raising the questions,
[00:05:54.400 --> 00:05:57.860]   can machines pass this benchmark?
[00:05:57.860 --> 00:05:59.580]   One of the perhaps tragic,
[00:05:59.580 --> 00:06:03.580]   but also exciting aspects of this whole area of work
[00:06:03.580 --> 00:06:06.140]   is that we still have a lot of work to do.
[00:06:06.140 --> 00:06:08.300]   So throughout this presentation,
[00:06:08.300 --> 00:06:10.780]   I will not only describe some of the ideas in the paper
[00:06:10.780 --> 00:06:12.820]   and outside of it in the years since,
[00:06:12.820 --> 00:06:16.480]   but also some of the open questions that remain,
[00:06:16.480 --> 00:06:18.620]   both at the philosophical, the psychological,
[00:06:18.620 --> 00:06:20.020]   and the technical levels.
[00:06:20.020 --> 00:06:21.800]   So here the open question stands,
[00:06:21.800 --> 00:06:25.380]   is it even possible to create a test of intelligence
[00:06:25.380 --> 00:06:29.340]   for artificial systems that will be convincing to us?
[00:06:29.340 --> 00:06:31.400]   Or will we always raise the bar?
[00:06:31.400 --> 00:06:34.700]   A corollary of that question is,
[00:06:34.700 --> 00:06:36.940]   looking at the prediction that Turing made
[00:06:36.940 --> 00:06:38.620]   that people will no longer find
[00:06:38.620 --> 00:06:41.440]   the phrase thinking machines contradictory,
[00:06:41.440 --> 00:06:45.620]   why do we still find that phrase contradictory?
[00:06:45.620 --> 00:06:47.940]   Why do we still think that computers
[00:06:47.940 --> 00:06:50.100]   are not at all intelligent?
[00:06:50.100 --> 00:06:53.020]   For many people, the game of chess
[00:06:53.020 --> 00:06:55.100]   was seen as the highest level of intelligence
[00:06:55.100 --> 00:06:57.120]   in these early days.
[00:06:57.120 --> 00:07:00.540]   In fact, we assign a lot of intelligence
[00:07:00.540 --> 00:07:03.300]   to Garry Kasparov for being one of the greatest,
[00:07:03.300 --> 00:07:06.260]   if not the greatest chess players of all time,
[00:07:06.260 --> 00:07:07.560]   as a human.
[00:07:07.560 --> 00:07:10.900]   Why do we not assign at least an inkling of that
[00:07:10.900 --> 00:07:14.320]   to IBM Deep Blue when it beat Garry Kasparov?
[00:07:14.320 --> 00:07:16.460]   Now, of course, you might start saying
[00:07:16.460 --> 00:07:18.020]   that it's a brute force algorithm,
[00:07:18.020 --> 00:07:20.900]   or in the case of AlphaGo and AlphaZero,
[00:07:20.900 --> 00:07:22.940]   you know how the learning mechanisms
[00:07:22.940 --> 00:07:24.340]   behind those algorithms work
[00:07:24.340 --> 00:07:27.140]   when they mastered the game of Go and the game of chess.
[00:07:27.140 --> 00:07:30.200]   And we'll get to some of those objections,
[00:07:30.200 --> 00:07:32.460]   but there's something deeply psychological
[00:07:32.460 --> 00:07:34.260]   within those objections
[00:07:34.260 --> 00:07:37.780]   that almost fear an artificial intelligence
[00:07:37.780 --> 00:07:39.740]   that passes the test.
[00:07:39.740 --> 00:07:41.620]   So the Turing test is very interesting
[00:07:41.620 --> 00:07:45.100]   as a thought experiment, as a philosophical construct,
[00:07:45.100 --> 00:07:48.120]   but it's also interesting as a real engineering test.
[00:07:48.120 --> 00:07:50.020]   And one of the implementations of it
[00:07:50.020 --> 00:07:52.440]   has been called the Lobner Prize,
[00:07:52.440 --> 00:07:55.660]   which has been running since 1991 to today.
[00:07:55.660 --> 00:07:58.700]   And the awards behind it, the award structure,
[00:07:58.700 --> 00:08:03.700]   is $25,000 for a system that you can use
[00:08:03.740 --> 00:08:06.180]   that using text alone passes the test,
[00:08:06.180 --> 00:08:09.820]   and $100,000 that uses other modalities
[00:08:09.820 --> 00:08:12.420]   like visual and auditory input.
[00:08:12.420 --> 00:08:14.460]   The rules of the competition have changed through the years,
[00:08:14.460 --> 00:08:16.340]   but they're currently are as follows.
[00:08:16.340 --> 00:08:19.140]   It's a 25-minute conversation,
[00:08:19.140 --> 00:08:21.540]   and in order to win, to pass the test,
[00:08:21.540 --> 00:08:24.180]   you have to fool 50% of the judges
[00:08:24.180 --> 00:08:26.780]   with which the system communicates.
[00:08:26.780 --> 00:08:31.780]   Mitsuko and Rose from Steve Warswick and Bruce Wilcox
[00:08:32.340 --> 00:08:34.540]   have been dominating the past 10 years,
[00:08:34.540 --> 00:08:37.080]   winning all but one of the years.
[00:08:37.080 --> 00:08:41.220]   Some details, Mitsuko and Rose are both
[00:08:41.220 --> 00:08:43.740]   mostly scripted, rule-based chatbots,
[00:08:43.740 --> 00:08:48.320]   so they're not end-to-end learning systems.
[00:08:48.320 --> 00:08:50.660]   I believe there is a little bit of machine learning,
[00:08:50.660 --> 00:08:54.540]   but as I understand, at the core, they're mostly scripted.
[00:08:54.540 --> 00:08:56.620]   And on the next slide, we'll go through a transcript
[00:08:56.620 --> 00:08:59.100]   of a conversation with Mitsuko to give you a sense
[00:08:59.100 --> 00:09:01.020]   of what those conversations are like.
[00:09:01.020 --> 00:09:04.820]   But I'd like to mention sort of a tragic note, really,
[00:09:04.820 --> 00:09:08.660]   that as far as I understand, the prize is no longer funded.
[00:09:08.660 --> 00:09:13.380]   But money aside, it shows that there's not a broad interest
[00:09:13.380 --> 00:09:15.940]   in this construction, both inside
[00:09:15.940 --> 00:09:18.300]   and outside the research community.
[00:09:18.300 --> 00:09:21.600]   It's really surprising to me that Google DeepMind,
[00:09:21.600 --> 00:09:25.940]   Facebook AI, major groups, major individual researchers
[00:09:25.940 --> 00:09:29.280]   in the field of AI have not taken on this challenge.
[00:09:29.280 --> 00:09:32.100]   I hope to actually be a small part of changing that
[00:09:32.100 --> 00:09:35.060]   a little bit, because I think there is a lot of value
[00:09:35.060 --> 00:09:36.460]   in considering the Turing test.
[00:09:36.460 --> 00:09:38.020]   I don't think it's a distraction.
[00:09:38.020 --> 00:09:41.660]   And I'll return to that point later on in this presentation.
[00:09:41.660 --> 00:09:45.580]   So you can pause and maybe read parts of this transcript.
[00:09:45.580 --> 00:09:48.660]   I'll highlight on the right, at the top of the right column,
[00:09:48.660 --> 00:09:50.940]   the human says, "You're a robot."
[00:09:50.940 --> 00:09:54.660]   Mitsuko answers, "How can a person be a robot?"
[00:09:54.660 --> 00:09:58.380]   Human, "Many people live lives akin to robots."
[00:09:58.380 --> 00:10:01.140]   Mitsuko, "How many, more than a million?"
[00:10:01.140 --> 00:10:03.180]   Human, "Yes."
[00:10:03.180 --> 00:10:06.020]   Mitsuko, "That seems like rather a lot."
[00:10:06.020 --> 00:10:10.260]   Human, "Do you think Southampton will beat Swansea?"
[00:10:10.260 --> 00:10:12.180]   Mitsuko, "I think so."
[00:10:12.180 --> 00:10:13.500]   Human, "Why?"
[00:10:13.500 --> 00:10:16.140]   Mitsuko, "No reason, it's just what I think."
[00:10:16.140 --> 00:10:18.180]   Human, "You must have a reason."
[00:10:18.180 --> 00:10:20.780]   Mitsuko, "Nope, no real reason."
[00:10:20.780 --> 00:10:23.340]   Human, "Which team do you support?"
[00:10:23.340 --> 00:10:26.340]   Mitsuko, "I support Leeds United."
[00:10:26.340 --> 00:10:29.500]   Human, "Leeds!"
[00:10:29.500 --> 00:10:31.140]   How's the season going?
[00:10:31.140 --> 00:10:34.820]   Mitsuko, "Yes, it is a city in the north of England.
[00:10:34.820 --> 00:10:37.980]   "About half a million people live there.
[00:10:37.980 --> 00:10:39.520]   "It is the same as ever."
[00:10:39.520 --> 00:10:43.900]   So you see in this transcript,
[00:10:43.900 --> 00:10:46.780]   it's just some little gap of inhumanity
[00:10:46.780 --> 00:10:48.380]   that becomes apparent.
[00:10:48.380 --> 00:10:52.940]   There's a lingering and a repeatability of points.
[00:10:52.940 --> 00:10:56.260]   There's certain gaps in understanding
[00:10:56.260 --> 00:10:59.180]   and ability to follow tangents, all those kinds of things.
[00:10:59.180 --> 00:11:04.460]   But it's still not clear to me as an open question
[00:11:04.460 --> 00:11:09.100]   how to make explicit where exactly
[00:11:09.100 --> 00:11:11.780]   the point of the failure of the test is.
[00:11:11.780 --> 00:11:15.220]   I believe that hasn't actually been really researched
[00:11:15.220 --> 00:11:17.540]   that well in these constructions.
[00:11:17.540 --> 00:11:20.660]   As opposed to decision making at the very end
[00:11:20.660 --> 00:11:22.900]   of a conversation, is this human or not,
[00:11:22.900 --> 00:11:25.220]   rather marking parts of a conversation
[00:11:25.220 --> 00:11:28.140]   as more or less human, like suspicious parts
[00:11:28.140 --> 00:11:30.700]   that make you wonder this is not human.
[00:11:30.700 --> 00:11:32.540]   I think that'll be really interesting to see
[00:11:32.540 --> 00:11:34.940]   if it's possible to make explicit what aspects
[00:11:34.940 --> 00:11:37.460]   of the conversation are the failure points.
[00:11:37.460 --> 00:11:39.280]   One of the times that the claim
[00:11:39.280 --> 00:11:41.140]   that the Turing test was passed,
[00:11:41.140 --> 00:11:43.700]   I think most famously was in 2014
[00:11:43.700 --> 00:11:46.700]   at an exhibition event that marked the 60th anniversary
[00:11:46.700 --> 00:11:51.700]   of Turing's death, Eugene Guzman fooled 33%
[00:11:51.700 --> 00:11:53.900]   of the event judges.
[00:11:53.900 --> 00:11:55.940]   And the method he used was to portray
[00:11:55.940 --> 00:11:58.740]   a 13-year-old Ukrainian boy that had a bunch
[00:11:58.740 --> 00:12:00.100]   of different personality quirks
[00:12:00.100 --> 00:12:02.220]   and obviously the language barrier,
[00:12:02.220 --> 00:12:05.180]   and had some humor and a constant sort of drive
[00:12:05.180 --> 00:12:08.620]   towards misdirecting the conversation back
[00:12:08.620 --> 00:12:11.420]   to the places where it was comfortable doing.
[00:12:11.420 --> 00:12:13.000]   So there's some criticism that you can make
[00:12:13.000 --> 00:12:16.820]   of this event due to some sort of smoke and mirrors,
[00:12:16.820 --> 00:12:18.820]   kind of the PR and marketing side of things
[00:12:18.820 --> 00:12:23.140]   that I think is always there with these kind
[00:12:23.140 --> 00:12:24.900]   of exhibition events.
[00:12:24.900 --> 00:12:29.260]   But setting that aside, I think the interesting lessons here
[00:12:29.260 --> 00:12:33.260]   is that the parameters, the rules of the actual engineering
[00:12:33.260 --> 00:12:38.060]   of the Turing test can determine whether it contains
[00:12:38.060 --> 00:12:40.620]   sort of the spirit of the Turing test,
[00:12:40.620 --> 00:12:44.260]   which is the test that captures the ability
[00:12:44.260 --> 00:12:48.660]   of an agent to have a deep, meaningful conversation.
[00:12:48.660 --> 00:12:53.660]   So in this case, you can argue that a few tricks were used
[00:12:53.660 --> 00:12:57.980]   to circumvent the need to have a deep,
[00:12:57.980 --> 00:12:59.460]   meaningful conversation.
[00:12:59.460 --> 00:13:03.300]   And 30% of judges were fooled without rigorous,
[00:13:03.300 --> 00:13:06.260]   thorough, transparent, open-domain testing.
[00:13:06.260 --> 00:13:08.940]   On the left is a transcript with Scott Aronson,
[00:13:08.940 --> 00:13:10.300]   the famed computer scientist,
[00:13:10.300 --> 00:13:13.380]   the quantum computing researcher.
[00:13:13.380 --> 00:13:15.980]   Talked to him on the podcast, brilliant guy.
[00:13:15.980 --> 00:13:18.200]   He posted some of the conversation that he had
[00:13:18.200 --> 00:13:20.420]   with Eugene, he was one of the judges,
[00:13:20.420 --> 00:13:22.980]   on his blog that I think is really interesting.
[00:13:22.980 --> 00:13:26.660]   So it shows that the judge, the interrogator,
[00:13:26.660 --> 00:13:29.500]   when they're an expert, they can drive,
[00:13:29.500 --> 00:13:32.500]   they can truly put the bot to the test.
[00:13:32.500 --> 00:13:35.940]   As Scott did, he really didn't allow the kind
[00:13:35.940 --> 00:13:39.660]   of misdirection that Eugene nonstop tried to do.
[00:13:39.660 --> 00:13:41.700]   And you can see that in the transcript.
[00:13:41.700 --> 00:13:44.540]   Scott refuses to take the misdirection.
[00:13:44.540 --> 00:13:48.060]   So as I mentioned, despite the waning,
[00:13:48.060 --> 00:13:50.260]   I guess, popularity of the Lobner Prize
[00:13:50.260 --> 00:13:53.400]   and the Turing Test idea in general,
[00:13:53.400 --> 00:13:56.460]   Google has published a paper and proposed a system
[00:13:56.460 --> 00:13:59.220]   called MINA that's a chatbot,
[00:13:59.220 --> 00:14:02.720]   that's an end-to-end deep learning system.
[00:14:02.720 --> 00:14:06.180]   The representational goal in the 2.6 billion parameters
[00:14:06.180 --> 00:14:09.160]   is to capture the conversational context well,
[00:14:09.160 --> 00:14:10.520]   to be able to generate the text
[00:14:10.520 --> 00:14:12.740]   that fits the conversational context well.
[00:14:12.740 --> 00:14:14.600]   Now, one interesting aspect of this,
[00:14:14.600 --> 00:14:16.280]   besides being a serious attempt
[00:14:16.280 --> 00:14:18.420]   at creating a learning-based system
[00:14:18.420 --> 00:14:21.760]   for open domain conversational agents,
[00:14:21.760 --> 00:14:23.960]   is that a new metric is proposed.
[00:14:23.960 --> 00:14:28.960]   And it's a two-part metric of sensibleness and specificity.
[00:14:28.960 --> 00:14:32.280]   Now, sensibleness is that a bot's responses
[00:14:32.280 --> 00:14:33.840]   have to make sense in context.
[00:14:33.840 --> 00:14:35.560]   They have to fit the context.
[00:14:35.560 --> 00:14:36.520]   Just to give you a sense,
[00:14:36.520 --> 00:14:39.460]   for humans, we have 97% sensibleness.
[00:14:39.460 --> 00:14:44.460]   So ability to match what we're saying to the context.
[00:14:45.080 --> 00:14:48.900]   Now, the reason you need another side of that metric
[00:14:48.900 --> 00:14:52.020]   is because you can be sensible,
[00:14:52.020 --> 00:14:55.320]   you can fit the context by being boring, by being generic,
[00:14:55.320 --> 00:14:57.300]   by making statements like, I don't know,
[00:14:57.300 --> 00:14:58.880]   or that's a good point.
[00:14:58.880 --> 00:15:00.180]   So these generic statements
[00:15:00.180 --> 00:15:02.460]   that fit a lot of different kinds of context.
[00:15:02.460 --> 00:15:05.160]   So the other side of the metric is specificity.
[00:15:05.160 --> 00:15:08.480]   Basically, the goal being there is don't be boring.
[00:15:08.480 --> 00:15:12.460]   It's to say something very specific to this context.
[00:15:12.460 --> 00:15:14.980]   So not only does it match the context,
[00:15:14.980 --> 00:15:17.180]   but it captures something very unique
[00:15:17.180 --> 00:15:21.980]   to this particular set of lines of conversation
[00:15:21.980 --> 00:15:24.060]   that form the context.
[00:15:24.060 --> 00:15:28.820]   I think it's fair to say that the beauty of the music,
[00:15:28.820 --> 00:15:31.300]   the humor, the wit of conversation
[00:15:31.300 --> 00:15:35.380]   comes from that ability to play with the specifics,
[00:15:35.380 --> 00:15:37.220]   the specificity metric.
[00:15:37.220 --> 00:15:39.400]   So both are really important.
[00:15:39.400 --> 00:15:44.400]   Humans achieve 86% sensibleness and specificity.
[00:15:44.540 --> 00:15:49.520]   Mina achieves 79% compared to Mitsuku, who achieves 56%.
[00:15:49.520 --> 00:15:52.140]   Now, take this all with a grain of salt.
[00:15:52.140 --> 00:15:53.980]   I want to be very careful here
[00:15:53.980 --> 00:15:57.580]   because there is also, not to throw shade,
[00:15:57.580 --> 00:16:00.140]   but it's closed source currently.
[00:16:00.140 --> 00:16:02.320]   And there's a little bit of a feeling
[00:16:02.320 --> 00:16:05.500]   of a PR marketing situation here.
[00:16:05.500 --> 00:16:10.220]   Naturally, perhaps the paper is made in such a way,
[00:16:10.220 --> 00:16:12.580]   the methodology and the results are made in such a way
[00:16:12.580 --> 00:16:16.380]   that benefit the way the learning framework was constructed.
[00:16:16.380 --> 00:16:18.740]   Now, I don't want to over-criticize that
[00:16:18.740 --> 00:16:21.460]   because I think there's still a lot of interesting ideas
[00:16:21.460 --> 00:16:23.340]   in this paper, but in terms of looking
[00:16:23.340 --> 00:16:26.980]   at the actual percentages of 86% human performance
[00:16:26.980 --> 00:16:31.600]   and 79% Mina performance, I think we're quite away
[00:16:31.600 --> 00:16:34.060]   from being able to make conclusive statements
[00:16:34.060 --> 00:16:35.780]   about a system achieving
[00:16:35.780 --> 00:16:38.380]   human-level conversational capabilities.
[00:16:38.380 --> 00:16:41.220]   So those plots should be taken with a grain of salt,
[00:16:41.220 --> 00:16:43.260]   but the actual content of the ideas,
[00:16:43.260 --> 00:16:44.780]   I think is really interesting.
[00:16:44.780 --> 00:16:49.240]   I think quite obviously the future, long-term,
[00:16:49.240 --> 00:16:53.600]   but hopefully short-term, is in learning end-to-end,
[00:16:53.600 --> 00:16:57.020]   learning-based approaches to open domain conversation.
[00:16:57.020 --> 00:16:59.620]   So just like Turing described, funny enough,
[00:16:59.620 --> 00:17:02.220]   70 years ago in his paper that machine learning
[00:17:02.220 --> 00:17:05.600]   will be essential to success, I believe the same.
[00:17:05.600 --> 00:17:07.940]   It's a lot less interesting and revolutionary
[00:17:07.940 --> 00:17:10.500]   to think so today, but I believe
[00:17:10.500 --> 00:17:12.340]   that machine learning will also need
[00:17:12.340 --> 00:17:15.580]   to be a very central part of achieving
[00:17:15.580 --> 00:17:18.300]   human-level conversational capabilities.
[00:17:18.300 --> 00:17:20.320]   So let's talk through some objections.
[00:17:20.320 --> 00:17:25.940]   Nine of them are highlighted by Turing himself in his paper.
[00:17:25.940 --> 00:17:30.940]   Here I provide some informal, highly informal summaries.
[00:17:30.940 --> 00:17:33.460]   The first objection is religious,
[00:17:33.460 --> 00:17:37.640]   which connects thinking to, quote-unquote, the soul.
[00:17:37.640 --> 00:17:42.640]   And God, presumably, is the giver of the soul to humans.
[00:17:42.640 --> 00:17:49.180]   Now, Turing's response to that is God is all-powerful.
[00:17:49.180 --> 00:17:53.780]   There is no reason why he can't assign souls
[00:17:53.780 --> 00:17:58.220]   to anything biological or artificial.
[00:17:58.220 --> 00:18:01.400]   So it doesn't seem that whatever mechanism
[00:18:01.400 --> 00:18:04.720]   by which the soul arrives in the human
[00:18:04.720 --> 00:18:07.260]   cannot also be repeated for artificial creatures.
[00:18:07.260 --> 00:18:10.580]   The second objection is the, quote-unquote,
[00:18:10.580 --> 00:18:11.980]   head in the sand.
[00:18:11.980 --> 00:18:13.480]   It's a bit of a ridiculous one,
[00:18:13.480 --> 00:18:14.700]   but I think it's an important one
[00:18:14.700 --> 00:18:16.860]   because it keeps coming up often,
[00:18:16.860 --> 00:18:19.340]   even in today's context, highlighted by folks
[00:18:19.340 --> 00:18:22.420]   like Elon Musk, Stuart Russell, and so on.
[00:18:22.420 --> 00:18:27.260]   The head in the sand objection is that AGI is scary.
[00:18:27.260 --> 00:18:29.820]   So human-level and superhuman-level intelligence
[00:18:29.820 --> 00:18:31.340]   is kind of scary.
[00:18:31.340 --> 00:18:34.140]   Today we talk about existential threats.
[00:18:34.140 --> 00:18:36.360]   It seems like the world would be totally transformed
[00:18:36.360 --> 00:18:37.900]   if we have something like that.
[00:18:37.900 --> 00:18:40.440]   Then it could be transformed in a highly negative way.
[00:18:40.440 --> 00:18:42.160]   So let's not think about it
[00:18:42.160 --> 00:18:45.000]   because it's kind of seems far away.
[00:18:45.000 --> 00:18:46.820]   So it probably won't happen.
[00:18:46.820 --> 00:18:49.000]   So let's just not think about it.
[00:18:49.000 --> 00:18:50.840]   That's kind of the objection of the Turing test.
[00:18:50.840 --> 00:18:53.680]   It's so far away, it's not worthwhile
[00:18:53.680 --> 00:18:56.320]   to even think about a test for this intelligence
[00:18:56.320 --> 00:18:58.560]   or what human-level intelligence means
[00:18:58.560 --> 00:19:01.320]   or what superhuman-level intelligence means.
[00:19:01.320 --> 00:19:02.640]   The response, quite naturally,
[00:19:02.640 --> 00:19:05.580]   is that it doesn't matter how you feel about something
[00:19:05.580 --> 00:19:08.560]   and whether it's going to happen or not.
[00:19:08.560 --> 00:19:11.160]   So we kind of have to set our feelings aside
[00:19:11.160 --> 00:19:16.080]   and not allow fear or emotion to model our thinking
[00:19:16.080 --> 00:19:19.680]   or detract us from thinking about it at all.
[00:19:19.680 --> 00:19:23.020]   The third objection is from Gato's incompleteness theorem,
[00:19:23.020 --> 00:19:24.800]   saying there's limits to computation.
[00:19:24.800 --> 00:19:28.080]   This is the Roger Penrose line of thinking
[00:19:28.080 --> 00:19:30.880]   that basically if a machine is a computation system,
[00:19:30.880 --> 00:19:33.440]   there is limits to its capabilities
[00:19:33.440 --> 00:19:37.440]   in that it can never be a perfectly rational system.
[00:19:37.440 --> 00:19:38.760]   Turing's response to this
[00:19:38.760 --> 00:19:40.960]   is that humans are not rational either.
[00:19:40.960 --> 00:19:42.160]   They're flawed.
[00:19:42.160 --> 00:19:46.220]   Nowhere does it say that intelligence equals infallibility.
[00:19:46.220 --> 00:19:47.720]   In fact, it could probably be argued
[00:19:47.720 --> 00:19:51.860]   that fallibility is at the core of intelligence.
[00:19:51.860 --> 00:19:54.560]   The fourth objection is that consciousness
[00:19:54.560 --> 00:19:56.920]   may be required for intelligence.
[00:19:56.920 --> 00:19:59.620]   Turing's response to this is to separate
[00:19:59.620 --> 00:20:00.980]   whether something is conscious
[00:20:00.980 --> 00:20:03.200]   and whether something appears to be conscious.
[00:20:03.200 --> 00:20:06.240]   So the focus of the Turing test is how something appears.
[00:20:06.240 --> 00:20:11.240]   And so in some sense, humans, to us, as far as we know,
[00:20:11.240 --> 00:20:12.880]   only appear to be conscious.
[00:20:12.880 --> 00:20:14.960]   We can't prove that they're actually conscious,
[00:20:14.960 --> 00:20:17.080]   humans outside of ourselves.
[00:20:17.080 --> 00:20:20.920]   And so since humans only appear to be conscious,
[00:20:20.920 --> 00:20:22.720]   there's no reason to think that machines
[00:20:22.720 --> 00:20:25.240]   can't also appear to be conscious,
[00:20:25.240 --> 00:20:27.320]   and that's at the core of the Turing test.
[00:20:27.320 --> 00:20:30.240]   So the Turing test kind of skirts around the question
[00:20:30.240 --> 00:20:32.220]   whether something is or isn't intelligence,
[00:20:32.220 --> 00:20:34.540]   whether is or isn't conscious.
[00:20:34.540 --> 00:20:35.760]   The fundamental question is,
[00:20:35.760 --> 00:20:37.400]   does it appear to be intelligent?
[00:20:37.400 --> 00:20:39.060]   Does it appear to be conscious?
[00:20:39.060 --> 00:20:41.700]   So he actually doesn't respond to the idea
[00:20:41.700 --> 00:20:44.820]   that consciousness is or isn't required for intelligence.
[00:20:44.820 --> 00:20:46.460]   He just says that if it is,
[00:20:46.460 --> 00:20:50.100]   there's no reason why you can't fake it,
[00:20:50.100 --> 00:20:52.320]   and that will be sufficient
[00:20:52.320 --> 00:20:55.020]   to achieve the display of intelligence.
[00:20:56.000 --> 00:21:01.000]   The fifth objection is the Negative Nancy objection
[00:21:01.000 --> 00:21:05.100]   of machines will never be able to do X, whatever X is.
[00:21:05.100 --> 00:21:08.420]   You can make it love, joke, humor,
[00:21:08.420 --> 00:21:13.420]   understand or generate humor, eat, enjoy food,
[00:21:13.420 --> 00:21:16.380]   create art, music, poetry, and so on.
[00:21:16.380 --> 00:21:18.500]   So there's a lot of things we can put in that X
[00:21:18.500 --> 00:21:20.080]   that machines can never do.
[00:21:20.080 --> 00:21:24.060]   And basically highlighting our human intuition
[00:21:24.060 --> 00:21:26.540]   about the limitations of machines.
[00:21:26.540 --> 00:21:28.240]   Just like with the second objection,
[00:21:28.240 --> 00:21:32.140]   naturally the response here is that the objection
[00:21:32.140 --> 00:21:33.640]   that machines will never do X
[00:21:33.640 --> 00:21:37.260]   doesn't have any actual reasoning behind it.
[00:21:37.260 --> 00:21:42.260]   It is just a vapid opinion based on the world today,
[00:21:42.260 --> 00:21:46.140]   refusing to believe that the world of tomorrow
[00:21:46.140 --> 00:21:47.020]   will be different.
[00:21:47.020 --> 00:21:51.700]   The sixth objection, probably the most important,
[00:21:51.700 --> 00:21:53.180]   one of the most interesting,
[00:21:53.180 --> 00:21:57.100]   comes by way of Ada Lovelace, Lady Lovelace,
[00:21:57.100 --> 00:21:59.120]   the mother of computer science,
[00:21:59.120 --> 00:22:01.820]   with the basic idea that machines can only do
[00:22:01.820 --> 00:22:03.820]   what we program them to do.
[00:22:03.820 --> 00:22:06.940]   Now this is an objection that appears in many forms
[00:22:06.940 --> 00:22:09.540]   throughout, before Turing and after Turing.
[00:22:09.540 --> 00:22:11.340]   And I think it's a really important objection
[00:22:11.340 --> 00:22:12.380]   to think about.
[00:22:12.380 --> 00:22:14.560]   So in this particular case,
[00:22:14.560 --> 00:22:17.580]   I think Turing's response is quite shallow,
[00:22:17.580 --> 00:22:19.940]   but it is nevertheless pretty interesting,
[00:22:19.940 --> 00:22:23.260]   and we'll talk about it again later on.
[00:22:23.260 --> 00:22:26.740]   His response is, well, if machines can only do
[00:22:26.740 --> 00:22:28.180]   what we program them to do,
[00:22:28.180 --> 00:22:30.900]   we can rephrase that statement as saying,
[00:22:30.900 --> 00:22:32.760]   machines can't surprise us.
[00:22:32.760 --> 00:22:35.400]   And when you rephrase it that way,
[00:22:35.400 --> 00:22:37.700]   it becomes clear that machines actually surprise us
[00:22:37.700 --> 00:22:38.540]   all the time.
[00:22:38.540 --> 00:22:41.140]   A system that is sufficiently complex
[00:22:41.140 --> 00:22:45.900]   will no longer be one of which we have a solid intuition
[00:22:45.900 --> 00:22:47.260]   of how it behaves,
[00:22:47.260 --> 00:22:49.820]   even if we built all the individual pieces of code
[00:22:49.820 --> 00:22:52.180]   for those of you who have programmed things.
[00:22:52.180 --> 00:22:54.280]   So I've written a lot of programs.
[00:22:54.280 --> 00:22:56.900]   In the initial design stage,
[00:22:56.900 --> 00:22:58.820]   you have an intuition about how it should behave.
[00:22:58.820 --> 00:23:00.820]   There's a design, there's a plan,
[00:23:00.820 --> 00:23:02.980]   you know what the individual functions do.
[00:23:02.980 --> 00:23:05.180]   But as the piece of code grows,
[00:23:05.180 --> 00:23:09.340]   your ability to intuit exactly the mapping
[00:23:09.340 --> 00:23:14.220]   from input to output fades with the size of the code base,
[00:23:14.220 --> 00:23:17.620]   even if you understand everything about the code,
[00:23:17.620 --> 00:23:22.180]   and even if you set logical and syntactic bugs aside.
[00:23:22.180 --> 00:23:25.220]   The seventh objection looks to the brain
[00:23:25.220 --> 00:23:27.280]   and looks to the continuous analog nature
[00:23:27.280 --> 00:23:30.840]   of that particular neural network system.
[00:23:30.840 --> 00:23:33.900]   So Turing's response to that is,
[00:23:33.900 --> 00:23:36.260]   sure, the brain might be analog,
[00:23:36.260 --> 00:23:39.580]   and then digital computers are discrete,
[00:23:39.580 --> 00:23:41.620]   but if you have a big enough digital computer,
[00:23:41.620 --> 00:23:45.700]   it can sufficiently approximate the analog system,
[00:23:45.700 --> 00:23:48.060]   meaning to a sufficient degree
[00:23:48.060 --> 00:23:50.540]   that it would appear intelligent.
[00:23:50.540 --> 00:23:55.300]   The eighth objection is the free will objection, right?
[00:23:55.300 --> 00:24:00.300]   Is that when you have deterministic rules, laws, algorithms,
[00:24:00.300 --> 00:24:05.100]   they're going to result in predictable behavior.
[00:24:05.100 --> 00:24:09.980]   And this kind of exactly deterministic predictable behavior
[00:24:09.980 --> 00:24:14.540]   doesn't quite feel like the mind
[00:24:14.540 --> 00:24:16.900]   that we know us humans is possessing.
[00:24:16.900 --> 00:24:19.740]   This kind of feeling that underlies
[00:24:19.740 --> 00:24:23.700]   what's required for intelligence for a mind,
[00:24:23.700 --> 00:24:28.220]   I think is behind the Chinese room thought experiment
[00:24:28.220 --> 00:24:29.780]   that we'll talk about next.
[00:24:29.780 --> 00:24:33.340]   So Turing's response here is that
[00:24:33.340 --> 00:24:38.340]   humans very well could be a complex collection of rules.
[00:24:38.340 --> 00:24:40.180]   There's no indication that we're not,
[00:24:40.180 --> 00:24:42.780]   just because we don't understand
[00:24:42.780 --> 00:24:44.860]   or don't even have the tools to explore
[00:24:44.860 --> 00:24:48.860]   the kind of rules that underlie our brain,
[00:24:48.860 --> 00:24:51.500]   doesn't mean it's not just a collection
[00:24:51.500 --> 00:24:56.500]   of deterministic, perfectly predictable sets of rules.
[00:24:56.500 --> 00:24:59.620]   Objection number nine is kind of fun.
[00:24:59.620 --> 00:25:01.960]   Quite possibly Turing is trolling us,
[00:25:01.960 --> 00:25:04.820]   but more likely the ideas of mind reading,
[00:25:04.820 --> 00:25:07.460]   extrasensory perception, telepathy,
[00:25:07.460 --> 00:25:10.180]   were a little bit more popular in his time.
[00:25:10.180 --> 00:25:13.620]   So the objection here is what if mind reading
[00:25:13.620 --> 00:25:15.820]   was used to cheat the test?
[00:25:15.820 --> 00:25:19.060]   So basically if human to human communication
[00:25:19.060 --> 00:25:22.420]   through telepathy could be used,
[00:25:22.420 --> 00:25:25.340]   then a machine can't achieve that same kind
[00:25:25.340 --> 00:25:27.100]   of telepathic communication.
[00:25:27.100 --> 00:25:30.900]   And so that could be used to circumvent
[00:25:30.900 --> 00:25:32.780]   the effectiveness of the test.
[00:25:32.780 --> 00:25:35.500]   Now Turing's response to this is,
[00:25:35.500 --> 00:25:38.520]   well, you just have to design a room
[00:25:38.520 --> 00:25:40.780]   that not only protects you from being able to see,
[00:25:40.780 --> 00:25:42.740]   whether it's a robot or a human,
[00:25:42.740 --> 00:25:47.740]   but also design a telepathy proof room
[00:25:47.740 --> 00:25:50.580]   that prevents telepathic communication.
[00:25:50.580 --> 00:25:53.940]   Again, could be Turing trolling us,
[00:25:53.940 --> 00:25:55.120]   but I think more importantly,
[00:25:55.120 --> 00:25:57.240]   I think it's a nice illustration at the time,
[00:25:57.240 --> 00:26:00.180]   and even still today, that there's a lot of mystery
[00:26:00.180 --> 00:26:01.980]   about how our mind works.
[00:26:01.980 --> 00:26:04.620]   If you chuckle and completely laugh off
[00:26:04.620 --> 00:26:07.860]   the possibility of telepathic communication,
[00:26:07.860 --> 00:26:11.060]   I think you're assuming too much about your own knowledge
[00:26:11.060 --> 00:26:13.580]   about how our mind works.
[00:26:13.580 --> 00:26:15.920]   I think we know very little about how our mind works.
[00:26:15.920 --> 00:26:19.180]   It is true, we have very little scientific evidence
[00:26:19.180 --> 00:26:21.220]   of telepathic communication,
[00:26:21.220 --> 00:26:23.800]   but you shouldn't take the next leap
[00:26:23.800 --> 00:26:26.580]   and have a feeling like you understand
[00:26:26.580 --> 00:26:29.300]   that telepathic communication is impossible.
[00:26:29.300 --> 00:26:31.980]   You should nevertheless maintain an open mind.
[00:26:31.980 --> 00:26:33.640]   But as an objection,
[00:26:33.640 --> 00:26:36.340]   it doesn't seem to be a very effective one.
[00:26:36.340 --> 00:26:38.500]   I wanted to dedicate just one slide
[00:26:38.500 --> 00:26:40.980]   and probably the most famous objection to the Turing test
[00:26:40.980 --> 00:26:43.620]   proposed by John Searle in 1980
[00:26:43.620 --> 00:26:47.260]   in his paper "Minds, Brains, and Programs,"
[00:26:47.260 --> 00:26:50.820]   commonly known as the Chinese Room Thought Experiment.
[00:26:50.820 --> 00:26:53.660]   And it's kind of a combination of number four, number six,
[00:26:53.660 --> 00:26:56.140]   and number eight objections on the previous slide,
[00:26:56.140 --> 00:27:01.100]   which is the consciousness is required for intelligence,
[00:27:01.100 --> 00:27:03.460]   the Ada Lovelace objection that programs
[00:27:03.460 --> 00:27:06.580]   can only do what we program them to do,
[00:27:06.580 --> 00:27:10.060]   and the deterministic free will objection
[00:27:10.060 --> 00:27:13.820]   that deterministic rules lead to predictable behavior.
[00:27:13.820 --> 00:27:16.620]   And that doesn't seem to be like what the mind does.
[00:27:16.620 --> 00:27:18.560]   So there's echoes of all those objections
[00:27:18.560 --> 00:27:21.100]   that Turing anticipated all put together
[00:27:21.100 --> 00:27:23.320]   into the Chinese Room.
[00:27:23.320 --> 00:27:27.500]   As a small aside, it is now 6 a.m.
[00:27:27.500 --> 00:27:28.700]   I did not sleep last night,
[00:27:28.700 --> 00:27:31.780]   so this video is brought to you
[00:27:31.780 --> 00:27:36.380]   by this magic potion called Nitro Cold Brew,
[00:27:36.380 --> 00:27:41.700]   an excessively expensive canned beverage
[00:27:41.700 --> 00:27:44.780]   from Starbucks that fuels me
[00:27:44.780 --> 00:27:48.080]   this wonderful Saturday morning.
[00:27:48.080 --> 00:27:50.800]   Here's to you, dear friends.
[00:27:50.800 --> 00:27:56.780]   Okay, the Chinese Room involves following instructions
[00:27:56.780 --> 00:27:57.900]   of an algorithm.
[00:27:57.900 --> 00:28:00.220]   So there's a human sitting inside a room
[00:28:00.220 --> 00:28:02.220]   that doesn't know how to speak Chinese,
[00:28:02.220 --> 00:28:04.660]   but there's notes being passed to them
[00:28:04.660 --> 00:28:07.860]   inside the room from outside in Chinese,
[00:28:07.860 --> 00:28:10.140]   and all they do is follow a set of rules
[00:28:10.140 --> 00:28:12.740]   in order to respond to that language.
[00:28:12.740 --> 00:28:17.740]   So the idea is if the brain inside the system
[00:28:17.740 --> 00:28:21.300]   that passes the Turing test
[00:28:21.300 --> 00:28:24.300]   is simply following a set of rules
[00:28:24.300 --> 00:28:26.980]   that it's not truly understanding,
[00:28:26.980 --> 00:28:30.100]   it is not conscious, it does not have a mind,
[00:28:30.100 --> 00:28:32.100]   the objection is philosophical.
[00:28:32.100 --> 00:28:37.100]   So there's not, for my computer science engineering self,
[00:28:37.100 --> 00:28:38.480]   there's not enough meat in it
[00:28:38.480 --> 00:28:40.580]   to even make it that interesting.
[00:28:40.580 --> 00:28:41.860]   It's very human-centric,
[00:28:41.860 --> 00:28:45.360]   but allow us to explore it further.
[00:28:45.360 --> 00:28:48.980]   So the key argument is that programs,
[00:28:48.980 --> 00:28:52.740]   computational systems, are formal,
[00:28:52.740 --> 00:28:55.580]   and so they can capture syntactic structure.
[00:28:55.580 --> 00:28:59.900]   Minds, our brains, have mental content,
[00:28:59.900 --> 00:29:01.860]   so they can capture semantics.
[00:29:01.860 --> 00:29:05.300]   And so the claim that I think is the most important,
[00:29:05.300 --> 00:29:06.740]   the clearest in the paper,
[00:29:06.740 --> 00:29:11.000]   is that syntax by itself is neither constitutive of
[00:29:11.000 --> 00:29:13.620]   nor sufficient for semantics.
[00:29:13.620 --> 00:29:16.980]   So just because you can replicate the syntax of the language
[00:29:16.980 --> 00:29:19.980]   doesn't mean you can truly understand it.
[00:29:19.980 --> 00:29:21.580]   Now this is the same kind of criticism
[00:29:21.580 --> 00:29:24.820]   we hear of language models of today with transformers,
[00:29:24.820 --> 00:29:28.700]   that OpenAI's GP2 really doesn't understand the language,
[00:29:28.700 --> 00:29:32.700]   it's just mimicking the statistics of it so well
[00:29:32.700 --> 00:29:35.380]   that it can generate syntactically correct,
[00:29:35.380 --> 00:29:39.340]   and even have echoes of semantic structure
[00:29:39.340 --> 00:29:43.320]   that indicates some kind of understanding, but it doesn't.
[00:29:43.320 --> 00:29:45.300]   To me, that argument is not very interesting
[00:29:45.300 --> 00:29:46.600]   from an engineering perspective,
[00:29:46.600 --> 00:29:48.940]   because it just sounds like saying
[00:29:48.940 --> 00:29:52.900]   humans can understand things, humans are special,
[00:29:52.900 --> 00:29:56.680]   therefore machines cannot understand things.
[00:29:56.680 --> 00:29:59.260]   It's a very human-centric argument
[00:29:59.260 --> 00:30:02.680]   that's not allowing us to rigorously explore
[00:30:02.680 --> 00:30:06.620]   what exactly does understanding mean
[00:30:06.620 --> 00:30:08.140]   from a computational perspective.
[00:30:08.140 --> 00:30:12.540]   Or put in other words, if understanding, intelligence,
[00:30:12.540 --> 00:30:14.980]   consciousness, either one of those,
[00:30:14.980 --> 00:30:18.020]   is not achievable through computation,
[00:30:18.020 --> 00:30:21.900]   then where is the point that computation hits the wall?
[00:30:21.900 --> 00:30:25.140]   The most interesting open questions to me here
[00:30:25.140 --> 00:30:27.760]   are on the point of faking things, or mimicking,
[00:30:27.760 --> 00:30:29.440]   or the appearance of things.
[00:30:29.440 --> 00:30:32.460]   Does the mimicking of thinking equal thinking?
[00:30:32.460 --> 00:30:35.020]   Does the mimicking of consciousness equal consciousness?
[00:30:35.020 --> 00:30:37.860]   Does the mimicking of love equal love?
[00:30:37.860 --> 00:30:40.460]   This is something that I think a lot about,
[00:30:40.460 --> 00:30:43.200]   and depending on the day, go back and forth.
[00:30:43.200 --> 00:30:45.460]   But I tend to believe from an engineering perspective,
[00:30:45.460 --> 00:30:47.900]   I tend to agree with the spirit
[00:30:47.900 --> 00:30:49.700]   and the work of Alan Turing,
[00:30:49.700 --> 00:30:51.980]   in that at this time as engineers,
[00:30:51.980 --> 00:30:55.460]   we can only focus on building the appearance of thinking,
[00:30:55.460 --> 00:30:58.780]   the appearance of consciousness, the appearance of love.
[00:30:58.780 --> 00:31:01.980]   I think as we work towards creating that appearance,
[00:31:01.980 --> 00:31:06.620]   we'll actually begin to understand the fundamentals
[00:31:06.620 --> 00:31:08.140]   of what it means to be conscious,
[00:31:08.140 --> 00:31:12.760]   what it means to love, what it means to think.
[00:31:12.760 --> 00:31:14.600]   You may have even heard me say sometimes
[00:31:14.600 --> 00:31:18.800]   that the appearance of consciousness is consciousness.
[00:31:18.800 --> 00:31:21.780]   I think that's me being a little bit poetic,
[00:31:21.780 --> 00:31:23.100]   but I think from our perspective,
[00:31:23.100 --> 00:31:27.180]   from our exceptionally limited understanding,
[00:31:27.180 --> 00:31:31.500]   both problems are in the same direction.
[00:31:31.500 --> 00:31:33.300]   So it's not like if we focus on creating
[00:31:33.300 --> 00:31:34.500]   the appearance of consciousness,
[00:31:34.500 --> 00:31:37.600]   that's gonna lead us astray, in my personal view.
[00:31:37.600 --> 00:31:39.700]   It's going to lead us very far down the road
[00:31:39.700 --> 00:31:40.940]   of actually understanding,
[00:31:40.940 --> 00:31:44.380]   and maybe one day engineering consciousness.
[00:31:44.380 --> 00:31:46.620]   And now I'd like to talk about some alternatives
[00:31:46.620 --> 00:31:48.180]   and variations of the Turing test
[00:31:48.180 --> 00:31:50.140]   that I find quite interesting.
[00:31:50.140 --> 00:31:52.780]   So there's a lot of kind of natural variations
[00:31:52.780 --> 00:31:54.980]   and extensions to the Turing test.
[00:31:54.980 --> 00:31:59.980]   First, the total Turing test proposed in 1989.
[00:31:59.980 --> 00:32:01.660]   It extends the Turing test
[00:32:01.660 --> 00:32:04.420]   in the natural language conversation domain
[00:32:04.420 --> 00:32:06.940]   to perception, computer vision,
[00:32:06.940 --> 00:32:09.100]   and object manipulation of robotics.
[00:32:09.100 --> 00:32:12.300]   So it takes it into the physical world.
[00:32:12.300 --> 00:32:14.020]   The interesting question here to me
[00:32:14.020 --> 00:32:17.460]   is whether adding extra modalities
[00:32:17.460 --> 00:32:21.580]   like audio, visual, manipulation
[00:32:21.580 --> 00:32:24.200]   makes the test harder or easier.
[00:32:24.200 --> 00:32:28.100]   To me, it's very possible that a test
[00:32:28.100 --> 00:32:31.760]   with a narrow bandwidth of communication,
[00:32:31.760 --> 00:32:33.620]   such as the natural language communication
[00:32:33.620 --> 00:32:36.620]   of the Turing test is actually harder to pass
[00:32:36.620 --> 00:32:38.820]   than the one that includes other modalities.
[00:32:38.820 --> 00:32:42.580]   But anyway, one of the powerful things
[00:32:42.580 --> 00:32:45.500]   about the original Turing test is that it's so simple.
[00:32:45.500 --> 00:32:49.660]   The Lovelace test proposed in 2001
[00:32:49.660 --> 00:32:52.700]   builds on the Ada Lovelace objection
[00:32:52.700 --> 00:32:55.100]   to form the test that says the machine
[00:32:55.100 --> 00:32:57.540]   has to do something surprising
[00:32:57.540 --> 00:33:02.100]   that the creator or the person who's aware
[00:33:02.100 --> 00:33:05.380]   how the program was created cannot explain.
[00:33:05.380 --> 00:33:07.580]   So it should be truly surprised.
[00:33:07.580 --> 00:33:12.580]   There is also, in 2014, was proposed a Lovelace 2.0 test,
[00:33:12.940 --> 00:33:15.660]   which emphasizes a more constrained definition
[00:33:15.660 --> 00:33:17.600]   of what surprising is, 'cause it's very difficult
[00:33:17.600 --> 00:33:21.820]   to pin down, to formalize the idea of surprise
[00:33:21.820 --> 00:33:25.940]   and explain, right, in the original formulation
[00:33:25.940 --> 00:33:27.300]   of the Lovelace test.
[00:33:27.300 --> 00:33:29.820]   But with Lovelace 2.0, it emphasizes
[00:33:29.820 --> 00:33:33.660]   sort of creativity, art, so on.
[00:33:33.660 --> 00:33:35.540]   So it's more concrete than surprise,
[00:33:35.540 --> 00:33:38.260]   especially if you define constraints
[00:33:38.260 --> 00:33:41.860]   to which creative medium we're operating in.
[00:33:41.860 --> 00:33:45.300]   You basically have to create an impressive piece
[00:33:45.300 --> 00:33:46.600]   of artistic work.
[00:33:46.600 --> 00:33:49.640]   I think that's an interesting conception,
[00:33:49.640 --> 00:33:53.180]   but it takes us in the land that's much more,
[00:33:53.180 --> 00:33:58.180]   not less subjective than the original Turing test.
[00:33:58.180 --> 00:34:00.900]   But this brings us to the open
[00:34:00.900 --> 00:34:02.940]   and the very interesting question of surprise,
[00:34:02.940 --> 00:34:06.820]   which I think is really at the core
[00:34:06.820 --> 00:34:09.200]   of our conception of intelligence.
[00:34:09.200 --> 00:34:12.540]   I think it is true that our idea
[00:34:12.540 --> 00:34:14.500]   of what makes an intelligent machine
[00:34:14.500 --> 00:34:16.580]   is one that really surprises.
[00:34:16.580 --> 00:34:19.880]   So when we one day finally create a system
[00:34:19.880 --> 00:34:23.000]   of human-level or superhuman-level intelligence,
[00:34:23.000 --> 00:34:25.680]   we will surely be surprised.
[00:34:25.680 --> 00:34:28.800]   So we have to think, what kind of behavior
[00:34:28.800 --> 00:34:31.200]   is one that will surprise us to the core?
[00:34:31.200 --> 00:34:34.620]   To me, I have many examples in mind
[00:34:34.620 --> 00:34:36.880]   that I'll cover in future videos,
[00:34:36.880 --> 00:34:40.840]   but one certainly, one of the hardest ones is humor.
[00:34:40.840 --> 00:34:45.840]   And finally, the truly total Turing test proposed in 1998
[00:34:45.840 --> 00:34:49.020]   proposes an interesting philosophical idea
[00:34:49.020 --> 00:34:52.600]   that we should not judge the performance
[00:34:52.600 --> 00:34:55.960]   of an individual agent in an isolated context,
[00:34:55.960 --> 00:34:58.200]   but instead look at the body of work
[00:34:58.200 --> 00:35:01.880]   produced by a collection of intelligent agents
[00:35:01.880 --> 00:35:03.720]   throughout their evolution,
[00:35:03.720 --> 00:35:06.520]   with some constraints on the consistency
[00:35:06.520 --> 00:35:09.600]   underlying the evolutionary process.
[00:35:09.600 --> 00:35:15.000]   It's interesting to suggest that the way we conceive
[00:35:15.000 --> 00:35:17.920]   of intelligence amongst us humans
[00:35:17.920 --> 00:35:20.880]   is grounded in the long arc of history
[00:35:20.880 --> 00:35:23.280]   of the body of work we've created together.
[00:35:23.280 --> 00:35:25.720]   I don't find that argument convincing,
[00:35:25.720 --> 00:35:27.560]   but I do find the interesting question
[00:35:27.560 --> 00:35:31.000]   and the open question, the idea
[00:35:31.000 --> 00:35:35.840]   that we should measure systems
[00:35:35.840 --> 00:35:38.720]   not in the moment or particular five minute period
[00:35:38.720 --> 00:35:43.440]   or 20 minute period, but over a period of months and years,
[00:35:43.440 --> 00:35:46.560]   perhaps condensed in a simulated context.
[00:35:46.560 --> 00:35:49.760]   So really increase the scale
[00:35:49.760 --> 00:35:51.720]   at which we judge interactions
[00:35:51.720 --> 00:35:54.040]   by several orders of magnitude.
[00:35:54.040 --> 00:35:59.040]   That to me is a really interesting idea,
[00:35:59.040 --> 00:36:01.520]   you know, to judge alpha zero performance
[00:36:01.520 --> 00:36:03.840]   not on a single game of chess,
[00:36:03.840 --> 00:36:06.960]   but looking at millions of games
[00:36:06.960 --> 00:36:09.240]   and not looking at a million games
[00:36:09.240 --> 00:36:11.240]   for a static set of parameters,
[00:36:11.240 --> 00:36:14.640]   but looking at the millions of games played
[00:36:14.640 --> 00:36:17.520]   as the system was trained from scratch
[00:36:17.520 --> 00:36:19.680]   and became better and better and better.
[00:36:19.680 --> 00:36:23.520]   There's something about that full journey
[00:36:23.520 --> 00:36:25.960]   that may capture intelligence.
[00:36:25.960 --> 00:36:30.000]   So intelligence very well could be the journey,
[00:36:30.000 --> 00:36:31.600]   not the destination.
[00:36:31.600 --> 00:36:32.800]   I think there's something there.
[00:36:32.800 --> 00:36:35.720]   It's very imprecise in this construction,
[00:36:35.720 --> 00:36:39.880]   but it struck me as a very novel idea
[00:36:39.880 --> 00:36:43.920]   for benchmark not to measure instantaneous performance,
[00:36:43.920 --> 00:36:45.240]   but performance over time
[00:36:45.240 --> 00:36:47.600]   and the improvement of performance over time.
[00:36:47.600 --> 00:36:49.120]   It appears that there's something to that,
[00:36:49.120 --> 00:36:51.000]   but I can't quite make it concrete.
[00:36:51.000 --> 00:36:52.760]   And I'm not sure it's possible to formalize
[00:36:52.760 --> 00:36:56.480]   in the way that the original Turing test is formalized.
[00:36:56.480 --> 00:37:00.340]   Another kind of test is the Winograd Schema Challenge,
[00:37:00.340 --> 00:37:03.280]   which I think is really compelling in many ways.
[00:37:03.280 --> 00:37:05.840]   So first to explain it with an example,
[00:37:05.840 --> 00:37:09.280]   there's a sentence, really two sentences.
[00:37:09.280 --> 00:37:11.940]   Let's say the trophy doesn't fit into the brown suitcase
[00:37:11.940 --> 00:37:13.540]   because it's too small,
[00:37:13.540 --> 00:37:15.880]   and the trophy doesn't fit into the brown suitcase
[00:37:15.880 --> 00:37:17.680]   because it is too large.
[00:37:17.680 --> 00:37:20.280]   And the question is, what is too small?
[00:37:20.280 --> 00:37:22.120]   What is too large?
[00:37:22.120 --> 00:37:24.720]   The answer for the small, what is too small,
[00:37:24.720 --> 00:37:27.080]   is the suitcase is too small.
[00:37:27.080 --> 00:37:29.240]   The trophy doesn't fit into the brown suitcase
[00:37:29.240 --> 00:37:31.120]   because it is too small.
[00:37:31.120 --> 00:37:34.280]   And then the second question is, what is too large?
[00:37:34.280 --> 00:37:36.360]   The answer there is the trophy.
[00:37:36.360 --> 00:37:38.380]   The trophy doesn't fit into the brown suitcase
[00:37:38.380 --> 00:37:40.600]   because it is too large.
[00:37:40.600 --> 00:37:42.620]   The basic idea behind this challenge
[00:37:42.620 --> 00:37:47.380]   is the ambiguity in the sentence can only be resolved
[00:37:47.380 --> 00:37:51.200]   with common sense reasoning about ideas in this world.
[00:37:51.200 --> 00:37:56.160]   And so the strength of this test is it's quite clear,
[00:37:56.160 --> 00:38:01.100]   quite simple, and yet requires, at least in theory,
[00:38:01.100 --> 00:38:05.960]   this deep thing that we think makes us human,
[00:38:05.960 --> 00:38:07.940]   which is the ability to reason
[00:38:07.940 --> 00:38:10.640]   at the very basic level of common sense reasoning.
[00:38:10.640 --> 00:38:15.840]   The other nice thing is it can be a benchmark,
[00:38:15.840 --> 00:38:18.080]   like we're used to in the machine learning world,
[00:38:18.080 --> 00:38:21.240]   that doesn't require subjective human judges.
[00:38:21.240 --> 00:38:23.420]   There's literally a right answer.
[00:38:23.420 --> 00:38:28.420]   The weakness here that holds for other similar challenges
[00:38:28.420 --> 00:38:30.780]   in the space is that it's very difficult
[00:38:30.780 --> 00:38:33.720]   to come up with a large amount of questions.
[00:38:33.720 --> 00:38:36.560]   I mean, each one is handcrafted.
[00:38:36.560 --> 00:38:40.340]   And so that means you can't build a benchmark
[00:38:40.340 --> 00:38:43.340]   of millions or billions of questions.
[00:38:43.340 --> 00:38:45.860]   It has to be on a small scale.
[00:38:45.860 --> 00:38:50.420]   Variations of the Winograd scheme are included
[00:38:50.420 --> 00:38:54.860]   in some natural language benchmarks of today
[00:38:54.860 --> 00:38:57.820]   that people use in the machine learning context.
[00:38:57.820 --> 00:39:00.500]   The Amazon Alexa prize, I think,
[00:39:00.500 --> 00:39:03.900]   captures nicely the spirit of the Turing test.
[00:39:03.900 --> 00:39:06.100]   I think it's actually quite an amazing challenge
[00:39:06.100 --> 00:39:10.260]   and competition that uses voice conversation in the wild,
[00:39:10.260 --> 00:39:13.420]   so with real people, and they can use a,
[00:39:13.420 --> 00:39:15.680]   I think it's called a social bot skill
[00:39:15.680 --> 00:39:17.340]   on their Alexa devices.
[00:39:17.340 --> 00:39:20.300]   And I don't wanna wake up my own Alexa devices,
[00:39:20.300 --> 00:39:23.460]   but basically say her name and say, let's chat.
[00:39:23.460 --> 00:39:26.540]   And that brings up one of the bots involved in the challenge
[00:39:26.540 --> 00:39:28.420]   and then you can have a conversation.
[00:39:28.420 --> 00:39:32.700]   And then the bar that's to be reached is for you
[00:39:32.700 --> 00:39:36.860]   to have a 20 minute or longer conversation with the bot
[00:39:36.860 --> 00:39:40.000]   and for two thirds or more of the interactions
[00:39:40.000 --> 00:39:41.280]   to be that long.
[00:39:41.280 --> 00:39:45.120]   So the basic metric of successful interaction
[00:39:45.120 --> 00:39:47.420]   is the duration of the interaction.
[00:39:47.420 --> 00:39:49.580]   And as of today, we're still really,
[00:39:49.580 --> 00:39:51.180]   really far away from that.
[00:39:51.180 --> 00:39:52.580]   So why is this a good metric?
[00:39:52.580 --> 00:39:55.660]   And I do think it's a really powerful metric.
[00:39:55.660 --> 00:39:57.940]   As opposed to us judging the quality of conversation
[00:39:57.940 --> 00:40:01.340]   in retrospect, we speak with our actions.
[00:40:01.340 --> 00:40:04.100]   So a deep, meaningful conversation
[00:40:04.100 --> 00:40:05.940]   is one we don't want to leave.
[00:40:05.940 --> 00:40:09.980]   When we have other things contending for our time,
[00:40:09.980 --> 00:40:12.900]   when we make the choice to stay in that conversation,
[00:40:12.900 --> 00:40:15.500]   that's as powerful a signal as any
[00:40:15.500 --> 00:40:19.220]   to show that that conversation has content,
[00:40:19.220 --> 00:40:21.900]   has meaning, is enjoyable.
[00:40:21.900 --> 00:40:25.060]   I think that is what passing the Turing Test
[00:40:25.060 --> 00:40:28.540]   in its original spirit actually is.
[00:40:28.540 --> 00:40:30.700]   And I should mention that as of today,
[00:40:30.700 --> 00:40:35.060]   no team has even come close to passing the Turing Test
[00:40:35.060 --> 00:40:37.820]   as it is constructed by the Alexa Prize.
[00:40:37.820 --> 00:40:39.460]   There's several things that are really surprising
[00:40:39.460 --> 00:40:40.460]   about this challenge.
[00:40:40.460 --> 00:40:43.300]   One is that it's not a lot more popular
[00:40:43.300 --> 00:40:48.300]   and two, that Amazon chose to limit it to students only.
[00:40:48.300 --> 00:40:51.900]   I mean, almost making it an educational exercise
[00:40:51.900 --> 00:40:54.620]   as opposed to a moonshot challenge
[00:40:54.620 --> 00:40:57.980]   for our entire generation of researchers.
[00:40:57.980 --> 00:41:00.140]   I mentioned it before, but I'll say it again here
[00:41:00.140 --> 00:41:03.740]   that it's surprising to me that the biggest research lab
[00:41:03.740 --> 00:41:08.140]   and industry in academia have not focused on this problem,
[00:41:08.140 --> 00:41:12.060]   have not found the magic within the Turing Test problem
[00:41:12.060 --> 00:41:15.180]   and the Alexa Prize as it formulates,
[00:41:15.180 --> 00:41:19.140]   I believe, the spirit of the Turing Test quite well.
[00:41:19.140 --> 00:41:22.460]   A very different kind of test is the Hutter Prize
[00:41:22.460 --> 00:41:24.220]   started by Marcus Hutter,
[00:41:24.220 --> 00:41:26.980]   which I think is really fascinating
[00:41:26.980 --> 00:41:29.460]   on both the philosophical and mathematical angle.
[00:41:29.460 --> 00:41:33.460]   Underlying it is the idea that compression
[00:41:33.460 --> 00:41:38.140]   is strongly correlated with intelligence.
[00:41:38.140 --> 00:41:42.020]   Put another way, the ability to compress knowledge
[00:41:42.020 --> 00:41:44.380]   well requires intelligence.
[00:41:44.380 --> 00:41:46.380]   And the better you compress that knowledge,
[00:41:46.380 --> 00:41:47.900]   the more intelligent you are.
[00:41:47.900 --> 00:41:51.180]   I think this is a really compelling notion
[00:41:51.180 --> 00:41:53.620]   because then we can make explicit,
[00:41:53.620 --> 00:41:56.580]   we can quantify how intelligent you are
[00:41:56.580 --> 00:41:59.660]   by how well you're able to compress knowledge.
[00:41:59.660 --> 00:42:02.740]   As the prize webpage puts it,
[00:42:02.740 --> 00:42:05.300]   being able to compress well is closely related
[00:42:05.300 --> 00:42:07.060]   to acting intelligently,
[00:42:07.060 --> 00:42:10.020]   thus reducing the slippery concept of intelligence
[00:42:10.020 --> 00:42:12.420]   to hard file size numbers.
[00:42:12.420 --> 00:42:17.420]   So the task is to take one gigabyte of Wikipedia data
[00:42:17.420 --> 00:42:20.860]   and compress it down as much as possible.
[00:42:20.860 --> 00:42:25.860]   The current best is a 8.58 compression factor.
[00:42:25.860 --> 00:42:30.220]   So down from one gigabyte to 117 megabytes.
[00:42:30.220 --> 00:42:35.180]   And the award for each 1% improvement, you win 5,000 euros.
[00:42:35.180 --> 00:42:37.780]   I find this competition just amazing
[00:42:37.780 --> 00:42:40.100]   and fascinating on many levels.
[00:42:40.100 --> 00:42:43.020]   I think it's a really good formulation
[00:42:43.020 --> 00:42:47.980]   of an intelligence challenge, but it's not a test.
[00:42:47.980 --> 00:42:50.460]   That's one of its kind of limitations,
[00:42:50.460 --> 00:42:52.260]   at least in the poetic sense,
[00:42:52.260 --> 00:42:53.860]   that it doesn't set a bar
[00:42:53.860 --> 00:42:56.700]   beyond which we're really damn impressed.
[00:42:56.700 --> 00:42:59.300]   Meaning it's harder to set a bar,
[00:42:59.300 --> 00:43:01.340]   like the one formulated by the Turing test,
[00:43:01.340 --> 00:43:04.620]   beyond which we feel it would be human level intelligence.
[00:43:04.620 --> 00:43:06.260]   Now the bar that's set by the Turing,
[00:43:06.260 --> 00:43:07.820]   Alan Turing and others,
[00:43:07.820 --> 00:43:11.780]   Lobna Prize, Alexa Prize, are also arbitrary,
[00:43:11.780 --> 00:43:14.180]   but it feels like we're able to intuit a good bar
[00:43:14.180 --> 00:43:17.700]   in that context better than being able to intuit
[00:43:17.700 --> 00:43:21.260]   the kind of bar we need to set for the compression challenge.
[00:43:21.260 --> 00:43:22.660]   Another fascinating challenge
[00:43:22.660 --> 00:43:24.740]   is the abstraction and reasoning challenge
[00:43:24.740 --> 00:43:28.500]   put forth by Francois Chollet just a few months ago.
[00:43:28.500 --> 00:43:29.460]   So this is very exciting.
[00:43:29.460 --> 00:43:31.980]   It's actually ongoing as a competition on Kegel,
[00:43:31.980 --> 00:43:35.020]   I think with a deadline in May.
[00:43:35.020 --> 00:43:37.020]   It's a really, really interesting idea.
[00:43:37.020 --> 00:43:39.380]   I haven't internalized it fully yet,
[00:43:39.380 --> 00:43:41.860]   and perhaps we'll do a separate video
[00:43:41.860 --> 00:43:43.780]   on just this paper alone,
[00:43:43.780 --> 00:43:46.140]   and I'll talk to Francois, I'm sure, on the podcast
[00:43:46.140 --> 00:43:48.140]   and other contacts in the future about it.
[00:43:48.140 --> 00:43:50.820]   I think there's a lot of brilliant ideas here
[00:43:50.820 --> 00:43:53.500]   that I still have to kind of digest a little bit,
[00:43:53.500 --> 00:43:57.860]   but let me describe the high-level ideas
[00:43:57.860 --> 00:43:59.420]   behind this benchmark.
[00:43:59.420 --> 00:44:03.140]   So first of all, the name is abstraction reasoning corpus
[00:44:03.140 --> 00:44:05.140]   or challenge arc.
[00:44:05.140 --> 00:44:08.860]   The domain is in a grid world of patterns,
[00:44:08.860 --> 00:44:10.020]   not limited in size,
[00:44:10.020 --> 00:44:13.780]   but the grid world is filled with cells
[00:44:13.780 --> 00:44:15.700]   that can be of different colors.
[00:44:15.700 --> 00:44:19.380]   And the spirit of the set of tests that Francois proposes
[00:44:19.380 --> 00:44:22.060]   is to stay close to IQ tests,
[00:44:22.060 --> 00:44:23.740]   so psychometric intelligence tests
[00:44:23.740 --> 00:44:28.740]   that we use to measure the intelligence of human beings.
[00:44:29.100 --> 00:44:33.580]   Now, the Turing test is kind of at a higher level
[00:44:33.580 --> 00:44:35.580]   of natural language.
[00:44:35.580 --> 00:44:38.300]   In this construction of arc,
[00:44:38.300 --> 00:44:40.940]   it goes as close as possible
[00:44:40.940 --> 00:44:44.980]   to the very basic elements of reasoning,
[00:44:44.980 --> 00:44:47.700]   just like in the IQ test of patterns.
[00:44:47.700 --> 00:44:49.920]   It gets to the very core,
[00:44:49.920 --> 00:44:55.620]   such that we can then make explicit the priors,
[00:44:55.620 --> 00:44:58.780]   the concepts that we bring to the table of those tests.
[00:44:58.780 --> 00:45:00.700]   And if we can make them explicit,
[00:45:00.700 --> 00:45:04.420]   it reduces the test as close as possible
[00:45:04.420 --> 00:45:07.920]   to the measure of the system's ability to reason.
[00:45:07.920 --> 00:45:11.180]   Now, the concepts that are brought to this grid world,
[00:45:11.180 --> 00:45:13.100]   here's just a couple of example of priors
[00:45:13.100 --> 00:45:15.780]   that Francois shows in his paper.
[00:45:15.780 --> 00:45:16.820]   I recommend highly,
[00:45:16.820 --> 00:45:19.260]   called "On the Measure of Intelligence."
[00:45:19.260 --> 00:45:22.420]   Here, prior concept is not referring to a previous concept.
[00:45:22.420 --> 00:45:25.700]   It's referring to a prior set of knowledge
[00:45:25.700 --> 00:45:27.420]   that you bring to the table.
[00:45:27.420 --> 00:45:31.220]   So this first row of illustrations of the two grid worlds
[00:45:31.220 --> 00:45:34.420]   illustrates the idea of object persistence with noise.
[00:45:34.420 --> 00:45:39.420]   So we're able to understand that large objects,
[00:45:39.420 --> 00:45:45.180]   when there is some visual noise occluding
[00:45:45.180 --> 00:45:47.580]   our ability to see them,
[00:45:47.580 --> 00:45:49.480]   that they still exist in the world.
[00:45:49.480 --> 00:45:53.580]   And if that noise changes, the object is still unchanged.
[00:45:53.580 --> 00:45:57.900]   So that idea of object persistence in the world
[00:45:57.900 --> 00:46:01.140]   is a prior that we bring to the table
[00:46:01.140 --> 00:46:02.700]   of understanding this grid world.
[00:46:02.700 --> 00:46:06.820]   Another prior is on the left at the bottom
[00:46:06.820 --> 00:46:11.820]   is objects are defined by spatial contiguity.
[00:46:11.820 --> 00:46:16.220]   So objects in this grid world,
[00:46:16.220 --> 00:46:18.540]   when the cells are of the same color
[00:46:18.540 --> 00:46:20.120]   and they're touching each other,
[00:46:20.120 --> 00:46:22.180]   they're probably part of the same object.
[00:46:22.180 --> 00:46:25.420]   And if there's black cells that separate
[00:46:25.420 --> 00:46:26.980]   those groupings of cells,
[00:46:26.980 --> 00:46:28.660]   that means there's multiple objects.
[00:46:28.660 --> 00:46:33.660]   So this kind of spatial contiguity of colored cells
[00:46:33.660 --> 00:46:37.860]   define the entity of the object.
[00:46:37.860 --> 00:46:42.780]   And on the right at the bottom is the color-based contiguity,
[00:46:42.780 --> 00:46:44.760]   which means that even if the cells
[00:46:44.760 --> 00:46:46.780]   of different colors are touching,
[00:46:46.780 --> 00:46:48.300]   if their colors are different,
[00:46:48.300 --> 00:46:51.280]   that means it likely belongs to a different object.
[00:46:51.280 --> 00:46:52.820]   That's a basic prior.
[00:46:52.820 --> 00:46:54.980]   And there's a few others, by the way,
[00:46:54.980 --> 00:46:58.660]   just beautiful pictures in that paper
[00:46:58.660 --> 00:47:00.660]   that make you really think
[00:47:00.660 --> 00:47:03.340]   about the core elements of intelligence.
[00:47:03.340 --> 00:47:06.380]   I love that paper, worth looking at.
[00:47:06.380 --> 00:47:09.280]   There's a lot of interesting insights in there.
[00:47:09.280 --> 00:47:13.700]   Just to give you some examples of what the actual task
[00:47:13.700 --> 00:47:15.940]   for the machine in this test looks like,
[00:47:15.940 --> 00:47:18.700]   it's similar to the kind of task you would see in an IQ test.
[00:47:18.700 --> 00:47:21.680]   So here there's three pairings,
[00:47:21.680 --> 00:47:25.640]   and the task is for the fourth pairing of images
[00:47:25.640 --> 00:47:30.640]   to generate the grid world that fits the other three,
[00:47:30.640 --> 00:47:34.760]   that fits the generating pattern of the other three.
[00:47:34.760 --> 00:47:37.960]   So in this case, figure four from the paper,
[00:47:37.960 --> 00:47:39.400]   a task where the implicit goal
[00:47:39.400 --> 00:47:41.940]   is to complete a symmetrical pattern.
[00:47:41.940 --> 00:47:43.600]   The nature of the task is specified
[00:47:43.600 --> 00:47:45.820]   by the three input-output examples.
[00:47:45.820 --> 00:47:47.920]   The test taker must generate the output grid
[00:47:47.920 --> 00:47:49.820]   corresponding to the input grid
[00:47:49.820 --> 00:47:52.200]   of the test input bottom right.
[00:47:52.200 --> 00:47:55.280]   So here, what you're tasked with understanding
[00:47:55.280 --> 00:47:57.540]   in the first three pairings
[00:47:57.540 --> 00:48:02.540]   is that the input has a perfect global symmetry to it.
[00:48:02.540 --> 00:48:10.000]   And also that there's parts of the image that are missing
[00:48:10.000 --> 00:48:12.540]   that can be filled in order to complete
[00:48:12.540 --> 00:48:14.360]   that perfect symmetry.
[00:48:14.360 --> 00:48:16.440]   Now that's relying on another prior,
[00:48:16.440 --> 00:48:19.080]   another basic concept of symmetry,
[00:48:19.080 --> 00:48:22.160]   which I think underlies a lot of our understanding
[00:48:22.160 --> 00:48:23.780]   of visual patterns.
[00:48:23.780 --> 00:48:25.920]   Again, so the intelligence system
[00:48:25.920 --> 00:48:30.920]   has to have a good representation of symmetry
[00:48:30.920 --> 00:48:33.140]   in various contexts.
[00:48:33.140 --> 00:48:36.760]   This is fascinating and beautiful, beautiful images.
[00:48:36.760 --> 00:48:39.840]   Okay, another example, figure 10 from the paper,
[00:48:39.840 --> 00:48:43.240]   a task where the implicit goal is to count unique objects
[00:48:43.240 --> 00:48:46.320]   and select the objects that appears the most times.
[00:48:46.320 --> 00:48:48.480]   The actual task has more demonstration pairs
[00:48:48.480 --> 00:48:49.740]   than these three.
[00:48:49.740 --> 00:48:52.100]   So figure 10 here from the paper,
[00:48:52.100 --> 00:48:55.080]   a task where the implicit goal is to count unique objects
[00:48:55.080 --> 00:48:58.400]   and select the objects that appear the most times.
[00:48:58.400 --> 00:49:01.320]   So again, there's three pairings.
[00:49:01.320 --> 00:49:04.520]   You see in the first one, there's three blue objects.
[00:49:04.520 --> 00:49:06.880]   In the second one, there's four yellow objects.
[00:49:06.880 --> 00:49:09.280]   In the third one, there's three red objects.
[00:49:09.280 --> 00:49:10.560]   So you have to figure that out.
[00:49:10.560 --> 00:49:15.440]   And then the output is the grid cells capturing that object
[00:49:15.440 --> 00:49:17.120]   that appears the most times.
[00:49:17.120 --> 00:49:20.380]   And so apply that kind of reasoning
[00:49:20.380 --> 00:49:24.120]   to complete the output of the fourth pairing.
[00:49:24.120 --> 00:49:26.200]   One of the challenges for this kind of test
[00:49:26.200 --> 00:49:28.060]   is it's difficult to generate.
[00:49:28.060 --> 00:49:29.120]   But just like I said,
[00:49:29.120 --> 00:49:31.640]   I think there's a lot of really interesting technical
[00:49:31.640 --> 00:49:35.680]   and philosophical ideas here that are worth exploring.
[00:49:35.680 --> 00:49:37.980]   So let's quickly talk through a few takeaways.
[00:49:37.980 --> 00:49:42.160]   So zooming out, is the Turing test
[00:49:42.160 --> 00:49:43.380]   a good measure of intelligence
[00:49:43.380 --> 00:49:48.100]   and can it serve as an answer to the big ambiguous
[00:49:48.100 --> 00:49:52.060]   but profound philosophical question of can machines think?
[00:49:52.060 --> 00:49:54.020]   So first some notes
[00:49:54.020 --> 00:49:57.280]   on the underlying challenges of the Turing test.
[00:49:57.280 --> 00:50:00.660]   Let's talk about intelligence.
[00:50:00.660 --> 00:50:05.360]   So if we compare human behavior and intelligent behavior,
[00:50:05.360 --> 00:50:09.620]   it's clear that the Turing test hopes to capture
[00:50:09.620 --> 00:50:13.180]   the intelligent parts of human behavior.
[00:50:13.180 --> 00:50:16.140]   But if we're trying to really capture
[00:50:16.140 --> 00:50:18.700]   human level intelligence,
[00:50:18.700 --> 00:50:21.820]   it's also possible that we wanna capture
[00:50:21.820 --> 00:50:25.500]   the unintelligent, the irrational parts of human behavior.
[00:50:25.500 --> 00:50:28.460]   So it's an open question whether natural conversation
[00:50:28.460 --> 00:50:33.460]   is a test of intelligence or humanness.
[00:50:33.460 --> 00:50:36.380]   Because if it's a test of intelligence,
[00:50:36.380 --> 00:50:40.740]   it's focusing only on kind of rational systematic thinking.
[00:50:40.740 --> 00:50:42.620]   If it's a test of humanness,
[00:50:42.620 --> 00:50:45.720]   then you have to capture the full range of emotion,
[00:50:45.720 --> 00:50:50.180]   the mess, the irrationality, the laziness, the boredom,
[00:50:50.180 --> 00:50:52.020]   all the things that make us human
[00:50:52.020 --> 00:50:55.220]   and all the things that then project themselves
[00:50:55.220 --> 00:50:57.740]   into the way we carry on through conversation.
[00:50:57.740 --> 00:50:59.700]   As I mentioned in the previous objectives,
[00:50:59.700 --> 00:51:03.900]   the Turing test really focuses on the external appearances,
[00:51:03.900 --> 00:51:06.100]   not the internal processes.
[00:51:06.100 --> 00:51:09.340]   So like I said, from an engineering perspective,
[00:51:09.340 --> 00:51:12.460]   I think it's very difficult to create a test
[00:51:12.460 --> 00:51:15.580]   for internal processes for some of these concepts
[00:51:15.580 --> 00:51:17.820]   that we have a very poor understanding of,
[00:51:17.820 --> 00:51:20.380]   like intelligence, like consciousness.
[00:51:20.380 --> 00:51:23.300]   I think the best we can do right now
[00:51:23.300 --> 00:51:27.180]   in terms of quantifying and having a measure of something,
[00:51:27.180 --> 00:51:31.620]   we have to look at the external performance of the system
[00:51:31.620 --> 00:51:34.940]   as opposed to some properties of the internal processes.
[00:51:34.940 --> 00:51:37.980]   Another challenge for the Turing test,
[00:51:37.980 --> 00:51:41.940]   as Scott Aronson's conversation with Eugene Guzman indicates
[00:51:41.940 --> 00:51:45.420]   is that the skill of the interrogator
[00:51:45.420 --> 00:51:47.100]   is really important here.
[00:51:47.100 --> 00:51:51.080]   That's both on just the conversational skill
[00:51:51.080 --> 00:51:52.460]   of how much you can stretch
[00:51:52.460 --> 00:51:54.860]   and challenge the conversation with a bot,
[00:51:54.860 --> 00:51:58.100]   and two, on the human side of it,
[00:51:58.100 --> 00:52:01.620]   the ability of the interrogator to identify the humanness
[00:52:01.620 --> 00:52:03.500]   of both the human and the machine.
[00:52:03.500 --> 00:52:07.180]   So the ability to have a conversation
[00:52:07.180 --> 00:52:09.540]   that challenges the bot,
[00:52:09.540 --> 00:52:12.300]   and the ability to make the actual identification
[00:52:12.300 --> 00:52:13.500]   of human or machine.
[00:52:13.500 --> 00:52:17.740]   Those are both skills that are essential
[00:52:17.740 --> 00:52:19.980]   to the Turing test.
[00:52:19.980 --> 00:52:21.660]   Also, to me, it's really interesting,
[00:52:21.660 --> 00:52:25.500]   the anthropomorphization of human
[00:52:25.500 --> 00:52:29.060]   to inanimate object interaction,
[00:52:29.060 --> 00:52:31.060]   I think is really fascinating.
[00:52:31.060 --> 00:52:32.660]   And it's an open question
[00:52:32.660 --> 00:52:35.220]   whether in some construction of the Turing test,
[00:52:35.220 --> 00:52:37.740]   whether anthropomorphism is leveraged
[00:52:37.740 --> 00:52:39.180]   to convince the human,
[00:52:39.180 --> 00:52:41.220]   whether that's cheating the Turing test,
[00:52:41.220 --> 00:52:44.220]   or in fact, that's an essential element
[00:52:44.220 --> 00:52:47.700]   to convincing us humans that something is intelligent.
[00:52:47.700 --> 00:52:49.660]   Perhaps as a starting point,
[00:52:49.660 --> 00:52:51.460]   we have to anthropomorphize something
[00:52:51.460 --> 00:52:54.980]   before we allow it to be intelligent
[00:52:54.980 --> 00:52:59.940]   in our subjective judgment of its intelligence.
[00:52:59.940 --> 00:53:02.500]   And finally, another limitation of the Turing test
[00:53:02.500 --> 00:53:04.860]   that could be narrowly stated as
[00:53:04.860 --> 00:53:07.420]   why do we expect a bot to talk?
[00:53:07.420 --> 00:53:11.020]   What if it doesn't feel like talking?
[00:53:11.020 --> 00:53:12.340]   Does it still fail?
[00:53:12.340 --> 00:53:15.300]   I think a more general way to phrase that is
[00:53:15.300 --> 00:53:18.940]   why do we judge the performance of a system
[00:53:18.940 --> 00:53:21.740]   on such a narrow window of time?
[00:53:21.740 --> 00:53:23.540]   I think, as I mentioned before,
[00:53:23.540 --> 00:53:25.460]   there could be something interesting
[00:53:25.460 --> 00:53:28.820]   on expanding the window of time
[00:53:28.820 --> 00:53:34.060]   over which we analyze the intelligence of the system,
[00:53:34.060 --> 00:53:37.140]   looking not just at the average performance
[00:53:37.140 --> 00:53:38.740]   but the growth of its performance
[00:53:38.740 --> 00:53:41.700]   as it interacts with you as the individual.
[00:53:41.700 --> 00:53:44.540]   I think one key aspect of intelligence
[00:53:44.540 --> 00:53:49.540]   is a social aspect and a social connection,
[00:53:49.540 --> 00:53:54.700]   I think in part may require getting to know the person.
[00:53:54.700 --> 00:53:57.220]   And there's something to rethink in the Turing test
[00:53:57.220 --> 00:53:59.700]   that relies on us building a relationship
[00:53:59.700 --> 00:54:02.140]   with the person as part of the test.
[00:54:02.140 --> 00:54:07.060]   So you can think of it as kind of the ex machina Turing test
[00:54:07.060 --> 00:54:10.980]   where they spend a series of conversations together,
[00:54:10.980 --> 00:54:14.060]   several days together, all those kinds of things.
[00:54:14.060 --> 00:54:17.220]   That feels like an interesting extension
[00:54:17.220 --> 00:54:19.700]   of the Turing test which could reveal
[00:54:19.700 --> 00:54:22.540]   the significant limitation of the current construction
[00:54:22.540 --> 00:54:25.020]   of the Turing test which is a limited window of time,
[00:54:25.020 --> 00:54:28.780]   one time at the end interrogate or judgment of it
[00:54:28.780 --> 00:54:30.620]   whether it's human or machine.
[00:54:31.700 --> 00:54:35.700]   Now my view overall on the Turing test is that yes,
[00:54:35.700 --> 00:54:39.780]   something like the Turing test as originally constructed
[00:54:39.780 --> 00:54:44.100]   so the natural language conversation
[00:54:44.100 --> 00:54:47.700]   is close to the ultimate test of intelligence.
[00:54:47.700 --> 00:54:49.900]   And moreover, this is where I disagree.
[00:54:49.900 --> 00:54:51.900]   I think I disagree with Francois Chollet
[00:54:51.900 --> 00:54:55.580]   and other world-class researchers in the area,
[00:54:55.580 --> 00:54:58.980]   Stuart Russell and so on, that I think the Turing test
[00:54:58.980 --> 00:55:01.700]   is not a distraction for us to think about.
[00:55:01.700 --> 00:55:05.700]   It doesn't pull us away from actually making progress
[00:55:05.700 --> 00:55:06.780]   in the field.
[00:55:06.780 --> 00:55:08.540]   I think it keeps us honest.
[00:55:08.540 --> 00:55:12.140]   I think truly analyzing where we stand
[00:55:12.140 --> 00:55:15.700]   in natural language conversation will help us understand
[00:55:15.700 --> 00:55:17.040]   how far away we are.
[00:55:17.040 --> 00:55:21.340]   And more than that, I think there should be active research
[00:55:21.340 --> 00:55:22.180]   on this field.
[00:55:22.180 --> 00:55:24.220]   I think the Lubna Prize type of formulations,
[00:55:24.220 --> 00:55:27.260]   the Alexa Prize formulations should be more popular
[00:55:27.260 --> 00:55:30.220]   than they are and I think researchers should take them
[00:55:30.220 --> 00:55:31.660]   very seriously.
[00:55:31.660 --> 00:55:36.500]   Now that doesn't mean that the work of the ARC benchmark
[00:55:36.500 --> 00:55:40.420]   with the IQ test type of intelligent test
[00:55:40.420 --> 00:55:45.220]   is not also going to be fruitful, potentially very fruitful.
[00:55:45.220 --> 00:55:48.760]   But I think ultimately the real test
[00:55:48.760 --> 00:55:52.900]   of human-level intelligence will occur in something
[00:55:52.900 --> 00:55:55.660]   like the construction of the Turing test
[00:55:55.660 --> 00:55:58.580]   with natural language open domain conversation
[00:55:58.580 --> 00:56:02.100]   that results in deep, meaningful connection
[00:56:02.100 --> 00:56:04.460]   between human and machine.
[00:56:04.460 --> 00:56:08.100]   Zooming out a little bit, I think in general,
[00:56:08.100 --> 00:56:12.740]   I think AI researchers don't like and try to avoid
[00:56:12.740 --> 00:56:16.380]   the messiness of human beings as is captured
[00:56:16.380 --> 00:56:20.260]   by the human-robot interaction field and set of problems.
[00:56:20.260 --> 00:56:23.980]   I think more than just embracing the Turing test,
[00:56:23.980 --> 00:56:27.780]   I think we should embrace the messiness of the human being
[00:56:27.780 --> 00:56:31.700]   in all the different domains of computer vision,
[00:56:31.700 --> 00:56:36.700]   of natural language, of robotics, of autonomous vehicles.
[00:56:36.700 --> 00:56:40.480]   I've been a long-time advocate that semi-autonomous vehicles
[00:56:40.480 --> 00:56:41.980]   are here to stay for a long time.
[00:56:41.980 --> 00:56:43.420]   We're going to have to figure out
[00:56:43.420 --> 00:56:46.500]   the human-robot interaction problem
[00:56:46.500 --> 00:56:49.620]   and for that we have to embrace perceiving everything
[00:56:49.620 --> 00:56:51.540]   about the human inside the car,
[00:56:51.540 --> 00:56:54.240]   perceiving everything about the humans outside the car.
[00:56:54.240 --> 00:56:57.880]   As I mentioned, this presentation of the paper
[00:56:57.880 --> 00:57:02.100]   is actually part of our paper reading club
[00:57:02.100 --> 00:57:04.140]   focused on artificial intelligence
[00:57:04.140 --> 00:57:07.100]   where we discuss a couple of times a week
[00:57:07.100 --> 00:57:10.180]   on a Discord server called LexPlus AI Podcast
[00:57:10.180 --> 00:57:12.220]   that you're welcome to join.
[00:57:12.220 --> 00:57:14.600]   We have an amazing community of brilliant people there
[00:57:14.600 --> 00:57:16.900]   that discuss all kinds of topics
[00:57:16.900 --> 00:57:19.200]   in artificial intelligence and beyond.
[00:57:19.200 --> 00:57:22.300]   This particular illustration that I just love
[00:57:22.300 --> 00:57:24.580]   is from Will Scobie who is an illustrator
[00:57:24.580 --> 00:57:29.160]   from United Kingdom who is part of this Discord community
[00:57:29.160 --> 00:57:30.940]   so he contributed it.
[00:57:30.940 --> 00:57:34.640]   And in general, aside from the amazing conversations,
[00:57:34.640 --> 00:57:38.260]   I encourage and hope to see other members of the community
[00:57:38.260 --> 00:57:43.260]   contribute art, code, visualizations, slides,
[00:57:43.260 --> 00:57:45.820]   ideas for these kinds of videos.
[00:57:45.820 --> 00:57:48.980]   I'm really excited by the kind of conversations I've seen.
[00:57:48.980 --> 00:57:51.420]   If you're watching this video and wanna join in,
[00:57:51.420 --> 00:57:54.440]   click on the Discord link in the description on the slide.
[00:57:54.440 --> 00:57:58.920]   Join the conversation, new paper every week, it's fun.
[00:57:58.920 --> 00:58:02.020]   Just to give you a little sense of the ideas
[00:58:02.020 --> 00:58:04.340]   behind this AI paper reading club,
[00:58:04.340 --> 00:58:05.860]   like what the goals are.
[00:58:05.860 --> 00:58:07.380]   So what is it?
[00:58:07.380 --> 00:58:11.360]   I think the goal is to take a seminal paper in the field
[00:58:11.360 --> 00:58:13.960]   that doesn't just focus in on the specific
[00:58:13.960 --> 00:58:17.700]   sort of paragraph to paragraph, section to section analysis
[00:58:17.700 --> 00:58:20.400]   what the paper is saying, but actually use the paper
[00:58:20.400 --> 00:58:23.860]   to discuss the history, the big picture development
[00:58:23.860 --> 00:58:26.860]   of the field within the context of that paper.
[00:58:26.860 --> 00:58:28.580]   Now that could be philosophical papers
[00:58:28.580 --> 00:58:30.220]   like this Turing test paper,
[00:58:30.220 --> 00:58:33.420]   or it could be very specific papers in the field.
[00:58:33.420 --> 00:58:35.900]   Again, physics, mathematics, computer science,
[00:58:35.900 --> 00:58:38.340]   and probably quite a bit of deep learning.
[00:58:38.340 --> 00:58:43.340]   So the hope is to prioritize beautiful, powerful,
[00:58:43.340 --> 00:58:46.780]   impactful insights as opposed to full coverage
[00:58:46.780 --> 00:58:49.100]   of all the contents of the paper.
[00:58:49.100 --> 00:58:52.860]   And the actual meetings on Discord,
[00:58:52.860 --> 00:58:55.900]   hopefully are less one person presenting
[00:58:55.900 --> 00:58:57.360]   and more discussion.
[00:58:57.360 --> 00:59:00.140]   There's a lot of brilliant people, they're civil,
[00:59:00.140 --> 00:59:03.820]   so you can have 300, 400 people on voice chat,
[00:59:03.820 --> 00:59:05.540]   which is a really intimate setting.
[00:59:05.540 --> 00:59:08.220]   And yet people aren't interrupting each other,
[00:59:08.220 --> 00:59:11.060]   it's not chaos, it's quite an amazing community.
[00:59:11.060 --> 00:59:13.740]   The other goal I'd love to see
[00:59:13.740 --> 00:59:16.020]   is even if we cover technical papers,
[00:59:16.020 --> 00:59:19.380]   the goal is for it to be accessible to everyone.
[00:59:19.380 --> 00:59:21.820]   So both high school students,
[00:59:21.820 --> 00:59:24.780]   people outside of all of these fields in general,
[00:59:24.780 --> 00:59:27.980]   but also I'd love to make it useful
[00:59:27.980 --> 00:59:31.300]   to experts in the field, expert researchers.
[00:59:31.300 --> 00:59:34.340]   So avoid using technical jargon,
[00:59:34.340 --> 00:59:38.380]   but still try to discover insights that are new,
[00:59:38.380 --> 00:59:40.180]   that are interesting, that are important
[00:59:40.180 --> 00:59:42.060]   for the researchers in the field.
[00:59:42.060 --> 00:59:44.340]   That's what I would love to achieve here
[00:59:44.340 --> 00:59:45.980]   with this paper reading club.
[00:59:45.980 --> 00:59:48.940]   If you're interested, join in, listen in,
[00:59:48.940 --> 00:59:50.420]   or contribute to the conversation,
[00:59:50.420 --> 00:59:55.100]   suggest papers, suggest content, visualizations, code,
[00:59:55.100 --> 00:59:57.540]   all is welcome, it's an amazing community.
[00:59:57.540 --> 01:00:00.620]   Thanks for watching this excessively long presentation.
[01:00:00.620 --> 01:00:03.220]   If you have suggestions, let me know.
[01:00:03.220 --> 01:00:05.220]   Otherwise, hope to see you next time.
[01:00:05.220 --> 01:00:07.820]   (upbeat music)
[01:00:07.820 --> 01:00:10.420]   (upbeat music)
[01:00:10.420 --> 01:00:13.020]   (upbeat music)
[01:00:13.020 --> 01:00:15.620]   (upbeat music)
[01:00:15.620 --> 01:00:18.220]   (upbeat music)
[01:00:18.220 --> 01:00:20.820]   (upbeat music)
[01:00:20.820 --> 01:00:30.820]   [BLANK_AUDIO]

