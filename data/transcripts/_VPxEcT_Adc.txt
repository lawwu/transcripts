
[00:00:00.000 --> 00:00:02.920]   The following is a conversation with Peter Norvig.
[00:00:02.920 --> 00:00:05.040]   He's the director of research at Google
[00:00:05.040 --> 00:00:06.960]   and the co-author with Stuart Russell
[00:00:06.960 --> 00:00:10.680]   of the book "Artificial Intelligence, A Modern Approach"
[00:00:10.680 --> 00:00:14.480]   that educated and inspired a whole generation of researchers
[00:00:14.480 --> 00:00:17.080]   including myself to get into the field
[00:00:17.080 --> 00:00:18.880]   of artificial intelligence.
[00:00:18.880 --> 00:00:21.760]   This is the Artificial Intelligence Podcast.
[00:00:21.760 --> 00:00:24.160]   If you enjoy it, subscribe on YouTube,
[00:00:24.160 --> 00:00:27.200]   give it five stars on iTunes, support on Patreon,
[00:00:27.200 --> 00:00:29.080]   or simply connect with me on Twitter
[00:00:29.080 --> 00:00:32.840]   @LexFriedman, spelled F-R-I-D-M-A-N.
[00:00:32.840 --> 00:00:36.680]   And now, here's my conversation with Peter Norvig.
[00:00:36.680 --> 00:00:40.840]   Most researchers in the AI community, including myself,
[00:00:40.840 --> 00:00:43.120]   own all three editions, red, green, and blue,
[00:00:43.120 --> 00:00:46.480]   of the "Artificial Intelligence, A Modern Approach."
[00:00:46.480 --> 00:00:49.400]   It's a field-defining textbook, as many people are aware,
[00:00:49.400 --> 00:00:52.160]   that you wrote with Stuart Russell.
[00:00:52.160 --> 00:00:55.360]   How has the book changed and how have you changed
[00:00:55.360 --> 00:00:57.240]   in relation to it from the first edition
[00:00:57.240 --> 00:01:00.080]   to the second to the third and now fourth edition
[00:01:00.080 --> 00:01:00.920]   as you work on it?
[00:01:00.920 --> 00:01:04.320]   - Yeah, so it's been a lot of years, a lot of changes.
[00:01:04.320 --> 00:01:06.040]   One of the things changing from the first
[00:01:06.040 --> 00:01:08.680]   to maybe the second or third
[00:01:08.680 --> 00:01:13.000]   was just the rise of computing power, right?
[00:01:13.000 --> 00:01:15.880]   So I think in the first edition,
[00:01:15.880 --> 00:01:20.680]   we said, "Here's predicate logic,"
[00:01:20.680 --> 00:01:24.440]   but that only goes so far 'cause pretty soon
[00:01:24.440 --> 00:01:28.800]   you have millions of short little predicate expressions
[00:01:28.800 --> 00:01:31.520]   and they couldn't possibly fit in memory.
[00:01:31.520 --> 00:01:34.720]   So we're gonna use first-order logic that's more concise.
[00:01:34.720 --> 00:01:38.000]   And then we quickly realized,
[00:01:38.000 --> 00:01:40.400]   "Oh, predicate logic is pretty nice
[00:01:40.400 --> 00:01:44.200]   "because there are really fast SAT solvers and other things.
[00:01:44.200 --> 00:01:46.320]   "And look, there's only millions of expressions
[00:01:46.320 --> 00:01:48.240]   "and that fits easily into memory,
[00:01:48.240 --> 00:01:51.280]   "or maybe even billions fit into memory now."
[00:01:51.280 --> 00:01:54.640]   So that was a change of the type of technology we needed
[00:01:54.640 --> 00:01:56.800]   just because the hardware expanded.
[00:01:56.800 --> 00:01:58.080]   - Even to the second edition?
[00:01:58.080 --> 00:01:59.200]   - Yeah. - So resource constraints
[00:01:59.200 --> 00:02:01.360]   were loosened significantly for the second?
[00:02:01.360 --> 00:02:02.200]   - Yeah, yeah.
[00:02:02.200 --> 00:02:04.960]   - And that was early 2000s second edition?
[00:02:04.960 --> 00:02:09.960]   - Right, so '95 was the first and then 2000, 2001 or so.
[00:02:09.960 --> 00:02:12.360]   And then moving on from there,
[00:02:12.360 --> 00:02:14.840]   I think we're starting to see that again
[00:02:14.840 --> 00:02:19.840]   with the GPUs and then more specific type of machinery
[00:02:20.720 --> 00:02:25.520]   like the TPUs and you're seeing custom ASICs and so on
[00:02:25.520 --> 00:02:26.360]   for deep learning.
[00:02:26.360 --> 00:02:30.560]   So we're seeing another advance in terms of the hardware.
[00:02:30.560 --> 00:02:33.720]   Then I think another thing that we especially noticed
[00:02:33.720 --> 00:02:37.240]   this time around is in all three of the first editions,
[00:02:37.240 --> 00:02:40.200]   we kind of said, "Well, we're gonna find AI
[00:02:40.200 --> 00:02:43.000]   "as maximizing expected utility.
[00:02:43.000 --> 00:02:45.600]   "And you tell me your utility function."
[00:02:45.600 --> 00:02:49.680]   And now we've got 27 chapters worth of cool techniques
[00:02:49.680 --> 00:02:51.880]   for how to optimize that.
[00:02:51.880 --> 00:02:54.040]   I think in this edition, we're saying more,
[00:02:54.040 --> 00:02:55.280]   "You know what?
[00:02:55.280 --> 00:02:58.120]   "Maybe that optimization part is the easy part
[00:02:58.120 --> 00:03:01.600]   "and the hard part is deciding what is my utility function?
[00:03:01.600 --> 00:03:02.980]   "What do I want?
[00:03:02.980 --> 00:03:06.280]   "And if I'm a collection of agents or a society,
[00:03:06.280 --> 00:03:08.400]   "what do we want as a whole?"
[00:03:08.400 --> 00:03:10.120]   - So you touched that topic in this edition.
[00:03:10.120 --> 00:03:11.960]   You get a little bit more into utility.
[00:03:11.960 --> 00:03:13.480]   - Yeah, yeah. - That's really interesting.
[00:03:13.480 --> 00:03:15.480]   On a technical level,
[00:03:15.480 --> 00:03:17.560]   or almost pushing the philosophical?
[00:03:17.560 --> 00:03:19.320]   - I guess it is philosophical, right?
[00:03:19.320 --> 00:03:21.640]   So we've always had a philosophy chapter,
[00:03:21.640 --> 00:03:26.040]   which I was glad that we were supporting.
[00:03:26.040 --> 00:03:32.360]   And now it's less kind of the Chinese room type argument
[00:03:32.360 --> 00:03:37.560]   and more of these ethical and societal type issues.
[00:03:37.560 --> 00:03:41.920]   So we get into the issues of fairness and bias
[00:03:41.920 --> 00:03:45.960]   and just the issue of aggregating utilities.
[00:03:45.960 --> 00:03:48.520]   - So how do you encode human values
[00:03:48.520 --> 00:03:49.800]   into a utility function?
[00:03:49.800 --> 00:03:53.520]   Is this something that you can do purely through data
[00:03:53.520 --> 00:03:56.840]   in a learned way, or is there some systematic?
[00:03:56.840 --> 00:03:58.560]   Obviously, there's no good answers yet.
[00:03:58.560 --> 00:04:01.560]   There's just beginnings to this,
[00:04:01.560 --> 00:04:02.400]   to even opening the door to these questions.
[00:04:02.400 --> 00:04:04.320]   - Right, so there is no one answer.
[00:04:04.320 --> 00:04:07.520]   Yes, there are techniques to try to learn that.
[00:04:07.520 --> 00:04:10.800]   So we talk about inverse reinforcement learning, right?
[00:04:10.800 --> 00:04:14.120]   So reinforcement learning, you take some actions,
[00:04:14.120 --> 00:04:15.440]   you get some rewards,
[00:04:15.440 --> 00:04:18.020]   and you figure out what actions you should take.
[00:04:18.020 --> 00:04:20.160]   And inverse reinforcement learning,
[00:04:20.160 --> 00:04:23.000]   you observe somebody taking actions
[00:04:23.000 --> 00:04:25.680]   and you figure out, well,
[00:04:25.680 --> 00:04:27.240]   this must be what they were trying to do.
[00:04:27.240 --> 00:04:30.360]   If they did this action, it must be because they want it.
[00:04:30.360 --> 00:04:33.000]   Of course, there's restrictions to that, right?
[00:04:33.000 --> 00:04:36.160]   So lots of people take actions that are self-destructive
[00:04:36.160 --> 00:04:39.200]   or they're suboptimal in certain ways.
[00:04:39.200 --> 00:04:40.640]   So you don't want to learn that.
[00:04:40.640 --> 00:04:44.760]   You want to somehow learn the perfect actions
[00:04:44.760 --> 00:04:46.440]   rather than the ones they actually take.
[00:04:46.440 --> 00:04:50.040]   So that's a challenge for that field.
[00:04:50.040 --> 00:04:55.760]   Then another big part of it is just kind of theoretical
[00:04:55.760 --> 00:04:58.680]   of saying, what can we accomplish?
[00:04:58.680 --> 00:05:03.680]   And so you look at like this work on the programs
[00:05:03.680 --> 00:05:09.420]   to predict recidivism and decide who should get parole
[00:05:09.420 --> 00:05:12.860]   or who should get bail or whatever.
[00:05:12.860 --> 00:05:14.600]   And how are you going to evaluate that?
[00:05:14.600 --> 00:05:17.520]   And one of the big issues is fairness
[00:05:17.520 --> 00:05:19.600]   across protected classes,
[00:05:19.600 --> 00:05:24.600]   protected classes being things like sex and race and so on.
[00:05:24.600 --> 00:05:28.480]   And so two things you want is you want to say,
[00:05:28.480 --> 00:05:32.640]   well, if I get a score of say a six out of 10,
[00:05:32.640 --> 00:05:34.960]   then I want that to mean the same,
[00:05:34.960 --> 00:05:37.640]   whether no matter what race I'm on, right?
[00:05:37.640 --> 00:05:42.640]   So I want to have a 60% chance of reoccurring regardless.
[00:05:44.360 --> 00:05:46.520]   And the makers of the,
[00:05:46.520 --> 00:05:48.880]   one of the makers of a commercial program to do that says,
[00:05:48.880 --> 00:05:50.040]   that's what we're trying to optimize.
[00:05:50.040 --> 00:05:51.280]   And look, we achieved that.
[00:05:51.280 --> 00:05:56.120]   We've reached that kind of balance.
[00:05:56.120 --> 00:05:58.800]   And then on the other side, you also want to say,
[00:05:58.800 --> 00:06:01.840]   well, if it makes mistakes,
[00:06:01.840 --> 00:06:04.700]   I want that to affect both sides
[00:06:04.700 --> 00:06:07.240]   of the protected class equally.
[00:06:07.240 --> 00:06:09.000]   And it turns out they don't do that, right?
[00:06:09.000 --> 00:06:12.160]   So they're twice as likely to make a mistake
[00:06:12.160 --> 00:06:14.800]   that would harm a black person over a white person.
[00:06:14.800 --> 00:06:16.500]   So that seems unfair.
[00:06:16.500 --> 00:06:17.340]   So you'd like to say, well,
[00:06:17.340 --> 00:06:19.600]   I want to achieve both those goals.
[00:06:19.600 --> 00:06:21.360]   And then it turns out you do the analysis
[00:06:21.360 --> 00:06:22.960]   and it's theoretically impossible
[00:06:22.960 --> 00:06:24.120]   to achieve both those goals.
[00:06:24.120 --> 00:06:27.100]   So you have to trade them off one against the other.
[00:06:27.100 --> 00:06:29.060]   So that analysis is really helpful
[00:06:29.060 --> 00:06:32.280]   to know what you can aim for and how much you can get.
[00:06:32.280 --> 00:06:33.920]   You can't have everything,
[00:06:33.920 --> 00:06:35.480]   but the analysis certainly can't tell you
[00:06:35.480 --> 00:06:38.400]   where should we make that trade off point.
[00:06:38.400 --> 00:06:40.600]   - But nevertheless, then we can,
[00:06:40.600 --> 00:06:43.120]   as humans, deliberate where that trade off should be.
[00:06:43.120 --> 00:06:45.840]   - Yeah, so at least we now, we're arguing in an informed way.
[00:06:45.840 --> 00:06:48.240]   We're not asking for something impossible.
[00:06:48.240 --> 00:06:50.040]   We're saying, here's where we are
[00:06:50.040 --> 00:06:51.720]   and here's what we aim for.
[00:06:51.720 --> 00:06:55.840]   And this strategy is better than that strategy.
[00:06:55.840 --> 00:06:57.480]   - So that's, I would argue,
[00:06:57.480 --> 00:07:00.560]   is a really powerful and really important first step,
[00:07:00.560 --> 00:07:01.680]   but it's a doable one,
[00:07:01.680 --> 00:07:06.680]   sort of removing undesirable degrees of bias in systems
[00:07:06.680 --> 00:07:08.900]   in terms of protected classes.
[00:07:08.900 --> 00:07:09.740]   And then there's something,
[00:07:09.740 --> 00:07:12.520]   I listened to your commencement speech,
[00:07:12.520 --> 00:07:14.640]   where there's some fuzzier things,
[00:07:14.640 --> 00:07:17.280]   like you mentioned angry birds.
[00:07:17.280 --> 00:07:19.960]   Do you want to create systems
[00:07:19.960 --> 00:07:23.080]   that feed the dopamine enjoyment,
[00:07:23.080 --> 00:07:26.760]   that feed, that optimize for you returning to the system,
[00:07:26.760 --> 00:07:29.120]   enjoying the moment of playing the game,
[00:07:29.120 --> 00:07:32.040]   of getting likes or whatever, this kind of thing,
[00:07:32.040 --> 00:07:34.920]   or some kind of long-term improvement.
[00:07:36.080 --> 00:07:39.000]   Are you even thinking about that?
[00:07:39.000 --> 00:07:43.740]   That's really going to the philosophical area.
[00:07:43.740 --> 00:07:45.780]   - I think that's a really important issue too,
[00:07:45.780 --> 00:07:46.780]   certainly thinking about that.
[00:07:46.780 --> 00:07:50.780]   I don't think about that as an AI issue as much.
[00:07:50.780 --> 00:07:55.100]   But as you say, the point is,
[00:07:55.100 --> 00:07:59.820]   we've built this society and this infrastructure
[00:07:59.820 --> 00:08:04.500]   where we say we have a marketplace for attention
[00:08:04.500 --> 00:08:08.100]   and we've decided as a society
[00:08:08.100 --> 00:08:10.280]   that we like things that are free.
[00:08:10.280 --> 00:08:13.800]   And so we want all apps on our phone to be free.
[00:08:13.800 --> 00:08:16.200]   And that means they're all competing for your attention.
[00:08:16.200 --> 00:08:18.800]   And then eventually they make some money some way
[00:08:18.800 --> 00:08:21.860]   through ads or in-game sales or whatever.
[00:08:21.860 --> 00:08:27.400]   But they can only win by defeating all the other apps
[00:08:27.400 --> 00:08:29.560]   by instilling your attention.
[00:08:29.560 --> 00:08:33.760]   And we built a marketplace
[00:08:33.760 --> 00:08:37.020]   where it seems like they're working against you
[00:08:37.020 --> 00:08:39.080]   rather than working with you.
[00:08:39.080 --> 00:08:40.820]   And I'd like to find a way
[00:08:40.820 --> 00:08:42.720]   where we can change the playing field
[00:08:42.720 --> 00:08:45.640]   so we feel more like, well, these things are on my side.
[00:08:45.640 --> 00:08:49.760]   Yes, they're letting me have some fun in the short term,
[00:08:49.760 --> 00:08:52.200]   but they're also helping me in the long term
[00:08:52.200 --> 00:08:55.020]   rather than competing against me.
[00:08:55.020 --> 00:08:57.240]   - And those aren't necessarily conflicting objectives.
[00:08:57.240 --> 00:09:01.320]   They're just the incentives, the direct current incentives
[00:09:01.320 --> 00:09:03.240]   as we try to figure out this whole new world
[00:09:03.240 --> 00:09:06.680]   that seemed to be on the easier part of that,
[00:09:06.680 --> 00:09:09.280]   which is feeding the dopamine, the rush.
[00:09:09.280 --> 00:09:10.120]   - Right.
[00:09:10.120 --> 00:09:14.480]   - But, so maybe take a quick step back
[00:09:14.480 --> 00:09:18.040]   at the beginning of the "Artificial Intelligence,
[00:09:18.040 --> 00:09:20.160]   A Modern Approach" book of writing.
[00:09:20.160 --> 00:09:22.280]   So here you are in the '90s,
[00:09:22.280 --> 00:09:25.760]   when you first sat down with Stuart to write the book
[00:09:25.760 --> 00:09:27.880]   to cover an entire field,
[00:09:27.880 --> 00:09:29.480]   which is one of the only books
[00:09:29.480 --> 00:09:31.200]   that has successfully done that for AI.
[00:09:31.200 --> 00:09:34.600]   And actually in a lot of other computer science fields,
[00:09:34.600 --> 00:09:37.440]   it's a huge undertaking.
[00:09:37.440 --> 00:09:40.880]   So it must've been quite daunting.
[00:09:40.880 --> 00:09:42.160]   What was that process like?
[00:09:42.160 --> 00:09:44.600]   Did you envision that you would be trying
[00:09:44.600 --> 00:09:46.140]   to cover the entire field?
[00:09:46.140 --> 00:09:48.880]   Was there a systematic approach to it
[00:09:48.880 --> 00:09:50.400]   that was more step-by-step?
[00:09:50.400 --> 00:09:52.240]   How did it feel?
[00:09:52.240 --> 00:09:53.960]   - So I guess it came about,
[00:09:53.960 --> 00:09:57.480]   I'd go to lunch with the other AI faculty at Berkeley
[00:09:57.480 --> 00:10:00.760]   and we'd say, "The field is changing.
[00:10:00.760 --> 00:10:03.640]   "It seems like the current books are a little bit behind.
[00:10:03.640 --> 00:10:05.240]   "Nobody's come out with a new book recently.
[00:10:05.240 --> 00:10:06.920]   "We should do that."
[00:10:06.920 --> 00:10:09.160]   And everybody said, "Yeah, yeah, that's a great thing to do."
[00:10:09.160 --> 00:10:10.200]   And we never did anything.
[00:10:10.200 --> 00:10:11.160]   - Right.
[00:10:11.160 --> 00:10:14.440]   - And then I ended up heading off to industry.
[00:10:14.440 --> 00:10:16.000]   I went to Sun Lab.
[00:10:16.000 --> 00:10:17.200]   So I thought, well, that's the end
[00:10:17.200 --> 00:10:20.800]   of my possible academic publishing career.
[00:10:20.800 --> 00:10:25.280]   But I met Stuart again at a conference a year later
[00:10:25.280 --> 00:10:28.200]   and said, "You know that book we were always talking about?
[00:10:28.200 --> 00:10:30.360]   "You guys must be half done with it by now, right?"
[00:10:30.360 --> 00:10:32.040]   (laughing)
[00:10:32.040 --> 00:10:34.120]   And he said, "Well, we keep talking, we never do anything."
[00:10:34.120 --> 00:10:36.080]   So I said, "Well, you know, we should do it."
[00:10:36.080 --> 00:10:40.560]   And I think the reason is that we all felt
[00:10:40.560 --> 00:10:43.440]   it was a time where the field was changing.
[00:10:43.440 --> 00:10:46.560]   And that was in two ways.
[00:10:46.560 --> 00:10:52.160]   So the good old-fashioned AI was based primarily
[00:10:52.160 --> 00:10:54.840]   on Boolean logic and you had a few tricks
[00:10:54.840 --> 00:10:56.800]   to deal with uncertainty.
[00:10:56.800 --> 00:11:00.080]   And it was based primarily on knowledge engineering
[00:11:00.080 --> 00:11:01.960]   that the way you got something done is you went out,
[00:11:01.960 --> 00:11:04.640]   you interviewed an expert and you wrote down by hand
[00:11:04.640 --> 00:11:05.680]   everything they knew.
[00:11:05.680 --> 00:11:11.560]   And we saw in '95 that the field was changing in two ways.
[00:11:11.560 --> 00:11:14.800]   One, we were moving more towards probability
[00:11:14.800 --> 00:11:16.280]   rather than Boolean logic,
[00:11:16.280 --> 00:11:18.680]   and we were moving more towards machine learning
[00:11:18.680 --> 00:11:21.360]   rather than knowledge engineering.
[00:11:21.360 --> 00:11:24.120]   And the other books hadn't caught that wave.
[00:11:24.120 --> 00:11:27.960]   They were still more in the old school.
[00:11:27.960 --> 00:11:30.880]   So certainly they had part of that on the way.
[00:11:30.880 --> 00:11:34.640]   But we said, if we start now completely taking
[00:11:34.640 --> 00:11:37.640]   that point of view, we can have a different kind of book.
[00:11:37.640 --> 00:11:39.540]   And we were able to put that together.
[00:11:39.540 --> 00:11:45.320]   - And what was literally the process, if you remember?
[00:11:45.320 --> 00:11:46.840]   Did you start writing a chapter?
[00:11:46.840 --> 00:11:48.720]   Did you outline?
[00:11:48.720 --> 00:11:50.680]   - Yeah, I guess we did an outline
[00:11:50.680 --> 00:11:54.960]   and then we sort of assigned chapters to each person.
[00:11:56.000 --> 00:11:58.240]   At the time, I had moved to Boston,
[00:11:58.240 --> 00:12:00.120]   and Stuart was in Berkeley.
[00:12:00.120 --> 00:12:04.480]   So basically we did it over the internet.
[00:12:04.480 --> 00:12:08.040]   And that wasn't the same as doing it today.
[00:12:08.040 --> 00:12:13.040]   It meant dial up lines and telnetting in.
[00:12:13.040 --> 00:12:20.360]   You telnet it into one shell and you type cat file name
[00:12:20.360 --> 00:12:23.880]   and you hoped it was captured at the other end.
[00:12:23.880 --> 00:12:26.160]   - And certainly you're not sending images
[00:12:26.160 --> 00:12:27.240]   and figures back and forth.
[00:12:27.240 --> 00:12:29.680]   - Right, right, that didn't work.
[00:12:29.680 --> 00:12:33.200]   - But did you anticipate where the field would go
[00:12:33.200 --> 00:12:37.720]   from that day, from the '90s?
[00:12:37.720 --> 00:12:42.720]   Did you see the growth into learning-based methods,
[00:12:42.720 --> 00:12:47.080]   into data-driven methods that followed in the future decades?
[00:12:47.080 --> 00:12:50.940]   - We certainly thought that learning was important.
[00:12:52.000 --> 00:12:57.000]   I guess we missed it as being as important as it is today.
[00:12:57.000 --> 00:13:00.120]   We missed this idea of big data.
[00:13:00.120 --> 00:13:02.800]   We missed the idea of deep learning,
[00:13:02.800 --> 00:13:04.480]   hadn't been invented yet.
[00:13:04.480 --> 00:13:07.520]   We could have taken the book
[00:13:07.520 --> 00:13:11.200]   from a complete machine learning point of view
[00:13:11.200 --> 00:13:12.440]   right from the start.
[00:13:12.440 --> 00:13:15.080]   We chose to do it more from a point of view
[00:13:15.080 --> 00:13:16.960]   of we're gonna first develop
[00:13:16.960 --> 00:13:19.160]   different types of representations.
[00:13:19.160 --> 00:13:20.760]   And we're gonna talk about
[00:13:20.760 --> 00:13:22.600]   different types of environments.
[00:13:22.600 --> 00:13:26.600]   Is it fully observable or partially observable?
[00:13:26.600 --> 00:13:29.720]   And is it deterministic or stochastic and so on?
[00:13:29.720 --> 00:13:33.360]   And we made it more complex along those axes
[00:13:33.360 --> 00:13:38.000]   rather than focusing on the machine learning axis first.
[00:13:38.000 --> 00:13:40.000]   - Do you think, you know, there's some sense
[00:13:40.000 --> 00:13:42.880]   in which the deep learning craze
[00:13:42.880 --> 00:13:46.320]   is extremely successful for a particular set of problems.
[00:13:46.320 --> 00:13:49.400]   And, you know, eventually it's going to,
[00:13:49.400 --> 00:13:52.520]   in the general case, hit challenges.
[00:13:52.520 --> 00:13:56.280]   So in terms of the difference between perception systems
[00:13:56.280 --> 00:13:59.000]   and robots that have to act in the world,
[00:13:59.000 --> 00:14:02.680]   do you think we're gonna return to AI,
[00:14:02.680 --> 00:14:07.680]   a modern approach type breadth in edition five and six
[00:14:07.680 --> 00:14:11.360]   in future decades?
[00:14:11.360 --> 00:14:13.260]   Do you think deep learning will take its place
[00:14:13.280 --> 00:14:17.920]   as a chapter in this bigger view of AI?
[00:14:17.920 --> 00:14:19.320]   - Yeah, I think we don't know yet
[00:14:19.320 --> 00:14:21.080]   how it's all gonna play out.
[00:14:21.080 --> 00:14:26.080]   So in the new edition, we have a chapter on deep learning.
[00:14:26.080 --> 00:14:29.480]   We got Ian Goodfellow to be the guest author
[00:14:29.480 --> 00:14:30.600]   for that chapter.
[00:14:30.600 --> 00:14:34.780]   So he said he could condense his whole deep learning book
[00:14:34.780 --> 00:14:35.960]   into one chapter.
[00:14:35.960 --> 00:14:38.220]   I think he did a great job.
[00:14:38.220 --> 00:14:40.560]   We were also encouraged that he, you know,
[00:14:40.560 --> 00:14:44.400]   we gave him the old neural net chapter and said-
[00:14:44.400 --> 00:14:46.400]   - Have fun with it.
[00:14:46.400 --> 00:14:47.300]   - Modernize that.
[00:14:47.300 --> 00:14:50.280]   And he said, you know, half of that was okay.
[00:14:50.280 --> 00:14:52.960]   That certainly there's lots of new things
[00:14:52.960 --> 00:14:54.000]   that have been developed,
[00:14:54.000 --> 00:14:56.420]   but some of the core was still the same.
[00:14:56.420 --> 00:15:02.340]   So I think we'll gain a better understanding
[00:15:02.340 --> 00:15:04.240]   of what you can do there.
[00:15:04.240 --> 00:15:08.200]   I think we'll need to incorporate all the things we can do
[00:15:08.200 --> 00:15:10.040]   with the other technologies, right?
[00:15:10.040 --> 00:15:14.680]   So deep learning started out convolutional networks
[00:15:14.680 --> 00:15:17.860]   and very close to perception.
[00:15:17.860 --> 00:15:23.920]   And it's since moved to be able to do more with actions
[00:15:23.920 --> 00:15:27.360]   and some degree of longer term planning.
[00:15:27.360 --> 00:15:31.720]   But we need to do a better job with representation
[00:15:31.720 --> 00:15:36.320]   and reasoning and one shot learning and so on.
[00:15:36.320 --> 00:15:41.120]   And I think we don't know yet how that's gonna play out.
[00:15:41.120 --> 00:15:45.840]   - So do you think looking at the some success,
[00:15:45.840 --> 00:15:49.840]   but certainly eventual demise,
[00:15:49.840 --> 00:15:54.160]   a partial demise of experts to symbolic systems in the 80s,
[00:15:54.160 --> 00:15:56.560]   do you think there is kernels of wisdom
[00:15:56.560 --> 00:16:00.440]   in the work that was done there with logic and reasoning
[00:16:00.440 --> 00:16:05.440]   and so on that will rise again in your view?
[00:16:05.700 --> 00:16:08.640]   - So certainly I think the idea of representation
[00:16:08.640 --> 00:16:12.600]   and reasoning is crucial that sometimes
[00:16:12.600 --> 00:16:14.720]   you just don't have enough data about the world
[00:16:14.720 --> 00:16:17.360]   to learn de novo.
[00:16:17.360 --> 00:16:21.980]   So you've got to have some idea of representation,
[00:16:21.980 --> 00:16:24.920]   whether that was programmed in or told or whatever,
[00:16:24.920 --> 00:16:28.600]   and then be able to take steps of reasoning.
[00:16:28.600 --> 00:16:33.600]   I think the problem with the good old fashioned AI
[00:16:35.260 --> 00:16:39.920]   was one, we tried to base everything on these symbols
[00:16:39.920 --> 00:16:41.400]   that were atomic.
[00:16:41.400 --> 00:16:45.500]   And that's great if you're like trying to define
[00:16:45.500 --> 00:16:47.540]   the properties of a triangle, right?
[00:16:47.540 --> 00:16:50.680]   'Cause they have necessary and sufficient conditions.
[00:16:50.680 --> 00:16:52.000]   But things in the real world don't.
[00:16:52.000 --> 00:16:55.240]   The real world is messy and doesn't have sharp edges
[00:16:55.240 --> 00:16:57.380]   and atomic symbols do.
[00:16:57.380 --> 00:16:59.280]   So that was a poor match.
[00:17:00.640 --> 00:17:05.640]   And then the other aspect was that the reasoning
[00:17:05.640 --> 00:17:09.760]   was universal and applied anywhere,
[00:17:09.760 --> 00:17:11.140]   which in some sense is good,
[00:17:11.140 --> 00:17:15.140]   but it also means there's no guidance as to where to apply.
[00:17:15.140 --> 00:17:18.340]   And so you started getting these paradoxes like,
[00:17:18.340 --> 00:17:23.000]   well if I have a mountain and I remove one grain of sand,
[00:17:23.000 --> 00:17:24.300]   then it's still a mountain.
[00:17:24.300 --> 00:17:27.640]   But if I do that repeatedly at some point it's not.
[00:17:28.920 --> 00:17:32.280]   And with logic, there's nothing to stop you
[00:17:32.280 --> 00:17:35.880]   from applying things repeatedly.
[00:17:35.880 --> 00:17:42.020]   But maybe with something like deep learning,
[00:17:42.020 --> 00:17:44.660]   and I don't really know what the right name for it is,
[00:17:44.660 --> 00:17:46.240]   we could separate out those ideas.
[00:17:46.240 --> 00:17:47.920]   So one, we could say,
[00:17:47.920 --> 00:17:52.900]   mountain isn't just an atomic notion.
[00:17:52.900 --> 00:17:56.060]   It's some sort of something like a word embedding
[00:17:56.060 --> 00:18:01.060]   that has a more complex representation.
[00:18:01.060 --> 00:18:05.100]   And secondly, we could somehow learn,
[00:18:05.100 --> 00:18:06.740]   yeah, there's this rule that you can remove
[00:18:06.740 --> 00:18:09.260]   one grain of sand, and you can do that a bunch of times,
[00:18:09.260 --> 00:18:12.880]   but you can't do it a near infinite amount of times.
[00:18:12.880 --> 00:18:15.240]   But on the other hand, when you're doing induction
[00:18:15.240 --> 00:18:17.260]   on the integers, sure, then it's fine to do it
[00:18:17.260 --> 00:18:18.800]   an infinite number of times.
[00:18:18.800 --> 00:18:22.180]   And if we could, somehow we have to learn
[00:18:22.180 --> 00:18:24.660]   when these strategies are applicable,
[00:18:24.660 --> 00:18:28.180]   rather than having the strategies be completely neutral
[00:18:28.180 --> 00:18:31.180]   and available everywhere.
[00:18:31.180 --> 00:18:32.340]   - Anytime you use neural networks,
[00:18:32.340 --> 00:18:34.300]   anytime you learn from data,
[00:18:34.300 --> 00:18:36.920]   form representation from data in an automated way,
[00:18:36.920 --> 00:18:40.980]   it's not very explainable as to,
[00:18:40.980 --> 00:18:44.140]   or it's not introspective to us humans
[00:18:44.140 --> 00:18:48.140]   in terms of how this neural network sees the world.
[00:18:48.140 --> 00:18:51.180]   Where, why does it succeed so brilliantly
[00:18:51.180 --> 00:18:53.220]   on so many, in so many cases,
[00:18:53.220 --> 00:18:56.460]   and fail so miserably in surprising ways in small?
[00:18:56.460 --> 00:19:00.980]   So what do you think is the future there?
[00:19:00.980 --> 00:19:03.460]   Can simply more data, better data,
[00:19:03.460 --> 00:19:06.100]   more organized data solve that problem?
[00:19:06.100 --> 00:19:09.300]   Or is there elements of symbolic systems
[00:19:09.300 --> 00:19:10.380]   that need to be brought in,
[00:19:10.380 --> 00:19:12.140]   which are a little bit more explainable?
[00:19:12.140 --> 00:19:13.260]   - Yeah.
[00:19:13.260 --> 00:19:16.820]   So I prefer to talk about trust
[00:19:16.820 --> 00:19:20.340]   and validation and verification,
[00:19:20.340 --> 00:19:22.500]   rather than just about explainability.
[00:19:22.500 --> 00:19:25.300]   And then I think explanations are one tool
[00:19:25.300 --> 00:19:27.720]   that you use towards those goals.
[00:19:27.720 --> 00:19:30.660]   And I think it is an important issue
[00:19:30.660 --> 00:19:33.980]   that we don't wanna use these systems unless we trust them,
[00:19:33.980 --> 00:19:35.500]   and we wanna understand where they work
[00:19:35.500 --> 00:19:37.060]   and where they don't work.
[00:19:37.060 --> 00:19:40.820]   And an explanation can be part of that, right?
[00:19:40.820 --> 00:19:44.460]   So I apply for a loan and I get denied,
[00:19:44.460 --> 00:19:46.140]   I want some explanation of why.
[00:19:46.140 --> 00:19:50.220]   And you have, in Europe, we have the GDPR
[00:19:50.220 --> 00:19:52.680]   that says you're required to be able to get that.
[00:19:52.680 --> 00:19:54.860]   But on the other hand,
[00:19:54.860 --> 00:19:57.220]   an explanation alone is not enough, right?
[00:19:57.220 --> 00:20:01.300]   So we are used to dealing with people
[00:20:01.300 --> 00:20:04.820]   and with organizations and corporations and so on,
[00:20:04.820 --> 00:20:06.260]   and they can give you an explanation,
[00:20:06.260 --> 00:20:07.340]   and you have no guarantee
[00:20:07.340 --> 00:20:11.220]   that that explanation relates to reality, right?
[00:20:11.220 --> 00:20:12.540]   So the bank can tell me,
[00:20:12.540 --> 00:20:13.940]   "Well, you didn't get the loan
[00:20:13.940 --> 00:20:16.100]   "'cause you didn't have enough collateral."
[00:20:16.100 --> 00:20:18.260]   And that may be true, or it may be true
[00:20:18.260 --> 00:20:22.540]   that they just didn't like my religion or something else.
[00:20:22.540 --> 00:20:24.620]   I can't tell from the explanation,
[00:20:24.620 --> 00:20:27.660]   and that's true whether the decision was made
[00:20:27.660 --> 00:20:29.500]   by a computer or by a person.
[00:20:29.500 --> 00:20:32.040]   So I want more.
[00:20:32.040 --> 00:20:35.060]   I do wanna have the explanations,
[00:20:35.060 --> 00:20:37.300]   and I wanna be able to have a conversation
[00:20:37.300 --> 00:20:39.300]   to go back and forth and said,
[00:20:39.300 --> 00:20:41.840]   "Well, you gave this explanation, but what about this?
[00:20:41.840 --> 00:20:44.100]   "And what would have happened if this had happened?
[00:20:44.100 --> 00:20:48.020]   "And what would I need to change that?"
[00:20:48.020 --> 00:20:50.860]   So I think a conversation is a better way to think about it
[00:20:50.860 --> 00:20:54.380]   than just an explanation as a single output.
[00:20:54.380 --> 00:20:58.060]   And I think we need testing of various kinds, right?
[00:20:58.060 --> 00:20:59.380]   So in order to know,
[00:20:59.380 --> 00:21:03.460]   was the decision really based on my collateral
[00:21:03.460 --> 00:21:08.460]   or was it based on my religion or skin color or whatever?
[00:21:08.460 --> 00:21:10.920]   I can't tell if I'm only looking at my case,
[00:21:10.920 --> 00:21:12.940]   but if I look across all the cases,
[00:21:12.940 --> 00:21:15.620]   then I can detect a pattern, right?
[00:21:15.620 --> 00:21:18.340]   So you wanna have that kind of capability.
[00:21:18.340 --> 00:21:21.180]   You wanna have these adversarial testing, right?
[00:21:21.180 --> 00:21:23.060]   So we thought we were doing pretty good
[00:21:23.060 --> 00:21:25.820]   at object recognition in images.
[00:21:25.820 --> 00:21:28.420]   We said, "Look, we're at sort of pretty close
[00:21:28.420 --> 00:21:31.860]   "to human level performance on ImageNet and so on."
[00:21:31.860 --> 00:21:34.860]   And then you start seeing these adversarial images,
[00:21:34.860 --> 00:21:36.140]   and you say, "Wait a minute.
[00:21:36.140 --> 00:21:39.260]   "That part is nothing like human performance."
[00:21:39.260 --> 00:21:40.900]   - And you can mess with it really easily.
[00:21:40.900 --> 00:21:42.660]   - You can mess with it really easily, right?
[00:21:42.660 --> 00:21:45.500]   And yeah, you can do that to humans too, right?
[00:21:45.500 --> 00:21:47.180]   So-- - In a different way perhaps.
[00:21:47.180 --> 00:21:49.500]   - Right, humans don't know what color the dress was.
[00:21:49.500 --> 00:21:50.540]   - Right.
[00:21:50.540 --> 00:21:52.460]   - And so they're vulnerable to certain attacks
[00:21:52.460 --> 00:21:55.680]   that are different than the attacks on the machines.
[00:21:55.680 --> 00:21:59.400]   But the attacks on the machines are so striking,
[00:21:59.400 --> 00:22:03.060]   they really change the way you think about what we've done.
[00:22:03.060 --> 00:22:05.660]   And the way I think about it is,
[00:22:05.660 --> 00:22:08.300]   I think part of the problem is we're seduced
[00:22:08.300 --> 00:22:13.300]   by our low dimensional metaphors, right?
[00:22:13.300 --> 00:22:14.500]   - Yeah.
[00:22:14.500 --> 00:22:15.740]   - You know, you look-- - I like that phrase.
[00:22:15.740 --> 00:22:18.500]   - You look in a textbook and you say,
[00:22:18.500 --> 00:22:20.340]   "Okay, now we've mapped out the space."
[00:22:20.340 --> 00:22:24.980]   And you know, a cat is here and dog is here,
[00:22:24.980 --> 00:22:27.540]   and maybe there's a tiny little spot in the middle
[00:22:27.540 --> 00:22:28.620]   where you can't tell the difference,
[00:22:28.620 --> 00:22:30.740]   but mostly we've got it all covered.
[00:22:30.740 --> 00:22:33.300]   And if you believe that metaphor,
[00:22:33.300 --> 00:22:35.020]   then you say, "Well, we're nearly there."
[00:22:35.020 --> 00:22:37.180]   And you know, there's only gonna be
[00:22:37.180 --> 00:22:39.220]   a couple adversarial images.
[00:22:39.220 --> 00:22:40.620]   But I think that's the wrong metaphor.
[00:22:40.620 --> 00:22:42.260]   And what you should really say is,
[00:22:42.260 --> 00:22:45.900]   it's not a 2D flat space that we've got mostly covered.
[00:22:45.900 --> 00:22:47.580]   It's a million dimension space,
[00:22:47.580 --> 00:22:52.580]   and a cat is this string that goes out in this crazy path.
[00:22:52.580 --> 00:22:55.780]   And if you step a little bit off the path in any direction,
[00:22:55.780 --> 00:22:57.780]   you're in nowhere's land
[00:22:57.780 --> 00:22:59.380]   and you don't know what's gonna happen.
[00:22:59.380 --> 00:23:01.140]   And so I think that's where we are.
[00:23:01.140 --> 00:23:03.380]   And now we've got to deal with that.
[00:23:03.380 --> 00:23:06.140]   So it wasn't so much an explanation,
[00:23:06.140 --> 00:23:09.940]   but it was an understanding of what the models are
[00:23:09.940 --> 00:23:10.780]   and what they're doing.
[00:23:10.780 --> 00:23:11.900]   And now we can start exploring,
[00:23:11.900 --> 00:23:12.780]   how do you fix that?
[00:23:12.780 --> 00:23:15.260]   - Yeah, validating the robustness of the system, so on.
[00:23:15.260 --> 00:23:19.980]   But take it back to this word trust.
[00:23:19.980 --> 00:23:22.860]   Do you think we're a little too hard on our robots
[00:23:22.860 --> 00:23:25.660]   in terms of the standards we apply?
[00:23:25.660 --> 00:23:27.820]   So, you know,
[00:23:27.820 --> 00:23:31.700]   there's a dance,
[00:23:31.700 --> 00:23:34.020]   there's a dance in nonverbal
[00:23:34.020 --> 00:23:36.460]   and verbal communication between humans.
[00:23:36.460 --> 00:23:38.860]   You know, if we apply the same kind of standard
[00:23:38.860 --> 00:23:40.660]   in terms of humans, you know,
[00:23:40.740 --> 00:23:43.060]   we trust each other pretty quickly.
[00:23:43.060 --> 00:23:45.620]   You know, you and I haven't met before
[00:23:45.620 --> 00:23:48.340]   and there's some degree of trust, right?
[00:23:48.340 --> 00:23:50.580]   That nothing's gonna go crazy wrong.
[00:23:50.580 --> 00:23:53.620]   And yet to AI, when we look at AI systems,
[00:23:53.620 --> 00:23:58.620]   we seem to approach through skepticism always, always.
[00:23:58.620 --> 00:24:01.220]   And it's like, they have to prove
[00:24:01.220 --> 00:24:03.060]   through a lot of hard work
[00:24:03.060 --> 00:24:06.700]   that they're even worthy of even inkling of our trust.
[00:24:06.700 --> 00:24:08.020]   What do you think about that?
[00:24:08.380 --> 00:24:11.220]   How do we break that barrier, close that gap?
[00:24:11.220 --> 00:24:12.060]   - I think that's right.
[00:24:12.060 --> 00:24:13.820]   I think that's a big issue.
[00:24:13.820 --> 00:24:18.820]   Just listening, my friend Mark Moffat is a naturalist
[00:24:18.820 --> 00:24:22.180]   and he says, "The most amazing thing about humans
[00:24:22.180 --> 00:24:25.080]   "is that you can walk into a coffee shop
[00:24:25.080 --> 00:24:28.440]   "or a busy street in a city
[00:24:28.440 --> 00:24:30.420]   "and there's lots of people around you
[00:24:30.420 --> 00:24:31.980]   "that you've never met before
[00:24:31.980 --> 00:24:33.620]   "and you don't kill each other."
[00:24:33.620 --> 00:24:34.460]   (laughing)
[00:24:34.460 --> 00:24:35.280]   - Yeah.
[00:24:35.280 --> 00:24:36.620]   - He says, chimpanzees cannot do that.
[00:24:36.620 --> 00:24:37.460]   - Yeah, right.
[00:24:37.460 --> 00:24:38.660]   (laughing)
[00:24:38.660 --> 00:24:40.580]   If a chimpanzee's in a situation
[00:24:40.580 --> 00:24:43.620]   where here's some that aren't from my tribe,
[00:24:43.620 --> 00:24:46.620]   bad things happen.
[00:24:46.620 --> 00:24:47.580]   - Especially in a coffee shop,
[00:24:47.580 --> 00:24:48.980]   there's delicious food around.
[00:24:48.980 --> 00:24:49.900]   - Yeah, yeah.
[00:24:49.900 --> 00:24:52.620]   But we humans have figured that out, right?
[00:24:52.620 --> 00:24:55.040]   - For the most part.
[00:24:55.040 --> 00:24:55.860]   - For the most part.
[00:24:55.860 --> 00:24:58.180]   We still go to war, we still do terrible things,
[00:24:58.180 --> 00:25:00.780]   but for the most part, we've learned to trust each other
[00:25:00.780 --> 00:25:02.780]   and live together.
[00:25:02.780 --> 00:25:07.440]   So that's gonna be important for our AI systems as well.
[00:25:07.440 --> 00:25:13.420]   And also I think a lot of the emphasis is on AI,
[00:25:13.420 --> 00:25:18.000]   but in many cases, AI is part of the technology,
[00:25:18.000 --> 00:25:19.300]   but isn't really the main thing.
[00:25:19.300 --> 00:25:21.740]   So a lot of what we've seen
[00:25:21.740 --> 00:25:26.740]   is more due to communications technology than AI technology.
[00:25:26.740 --> 00:25:30.120]   Yeah, you wanna make these good decisions,
[00:25:30.120 --> 00:25:33.900]   but the reason we're able to have any kind of system at all
[00:25:33.900 --> 00:25:35.860]   is we've got the communications
[00:25:35.860 --> 00:25:37.540]   so that we're collecting the data
[00:25:37.540 --> 00:25:41.500]   and so that we can reach lots of people around the world.
[00:25:41.500 --> 00:25:45.040]   I think that's a bigger change that we're dealing with.
[00:25:45.040 --> 00:25:47.780]   - Speaking of reaching a lot of people around the world,
[00:25:47.780 --> 00:25:49.280]   on the side of education,
[00:25:49.280 --> 00:25:53.660]   one of the many things in terms of education you've done,
[00:25:53.660 --> 00:25:56.980]   you've taught the Intro to Artificial Intelligence course,
[00:25:56.980 --> 00:26:00.620]   that signed up 160,000 students.
[00:26:00.620 --> 00:26:02.300]   It was one of the first successful example
[00:26:02.300 --> 00:26:06.780]   of a MOOC, Massive Open Online Course.
[00:26:06.780 --> 00:26:09.160]   What did you learn from that experience?
[00:26:09.160 --> 00:26:11.620]   What do you think is the future of MOOCs,
[00:26:11.620 --> 00:26:12.860]   of education online?
[00:26:12.860 --> 00:26:15.320]   - Yeah, it was a great fun doing it,
[00:26:15.320 --> 00:26:18.520]   particularly being right at the start,
[00:26:18.520 --> 00:26:21.660]   just because it was exciting and new,
[00:26:21.660 --> 00:26:24.620]   but it also meant that we had less competition.
[00:26:24.620 --> 00:26:27.860]   Right, so one of the things you hear about,
[00:26:27.860 --> 00:26:31.180]   well, the problem with MOOCs is the completion rates
[00:26:31.180 --> 00:26:33.820]   are so low, so there must be a failure.
[00:26:33.820 --> 00:26:37.580]   And I gotta admit, I'm a prime contributor.
[00:26:37.580 --> 00:26:40.780]   I probably started 50 different courses
[00:26:40.780 --> 00:26:42.400]   that I haven't finished,
[00:26:42.400 --> 00:26:44.220]   but I got exactly what I wanted out of them,
[00:26:44.220 --> 00:26:46.100]   'cause I had never intended to finish them.
[00:26:46.100 --> 00:26:48.680]   I just wanted to dabble in a little bit,
[00:26:48.680 --> 00:26:50.300]   either to see the topic matter
[00:26:50.300 --> 00:26:51.820]   or just to see the pedagogy
[00:26:51.820 --> 00:26:53.460]   of how are they doing this class.
[00:26:54.460 --> 00:26:56.820]   So I guess the main thing I learned is,
[00:26:56.820 --> 00:27:01.820]   when I came in, I thought the challenge was information,
[00:27:01.820 --> 00:27:07.460]   saying if I'm just, take the stuff I want you to know,
[00:27:07.460 --> 00:27:10.500]   and I'm very clear and explain it well,
[00:27:10.500 --> 00:27:13.700]   then my job is done and good things are gonna happen.
[00:27:13.700 --> 00:27:17.260]   And then in doing the course, I learned,
[00:27:17.260 --> 00:27:19.180]   well, yeah, you gotta have the information,
[00:27:19.180 --> 00:27:23.000]   but really, the motivation is the most important thing,
[00:27:23.000 --> 00:27:26.140]   that if students don't stick with it,
[00:27:26.140 --> 00:27:28.500]   then it doesn't matter how good the content is.
[00:27:28.500 --> 00:27:32.780]   And I think being one of the first classes,
[00:27:32.780 --> 00:27:36.780]   we were helped by sort of exterior motivation.
[00:27:36.780 --> 00:27:39.340]   So we tried to do a good job of making it enticing
[00:27:39.340 --> 00:27:44.340]   and setting up ways for the community
[00:27:44.340 --> 00:27:46.980]   to work with each other, to make it more motivating,
[00:27:46.980 --> 00:27:49.500]   but really a lot of it was, hey, this is a new thing,
[00:27:49.500 --> 00:27:51.580]   and I'm really excited to be part of a new thing.
[00:27:51.580 --> 00:27:54.580]   And so the students brought their own motivation.
[00:27:54.580 --> 00:27:56.860]   And so I think this is great,
[00:27:56.860 --> 00:27:58.680]   'cause there's lots of people around the world
[00:27:58.680 --> 00:28:00.620]   who have never had this before,
[00:28:00.620 --> 00:28:07.060]   would never have the opportunity to go to Stanford
[00:28:07.060 --> 00:28:08.560]   and take a class or go to MIT
[00:28:08.560 --> 00:28:10.460]   or go to one of the other schools,
[00:28:10.460 --> 00:28:12.840]   but now we can bring that to them.
[00:28:12.840 --> 00:28:15.800]   And if they bring their own motivation,
[00:28:15.800 --> 00:28:18.960]   they can be successful in a way they couldn't before.
[00:28:18.960 --> 00:28:21.600]   But that's really just the top tier of people
[00:28:21.600 --> 00:28:22.800]   that are ready to do that.
[00:28:22.800 --> 00:28:27.000]   The rest of the people just don't see
[00:28:27.000 --> 00:28:29.520]   or don't have the motivation
[00:28:29.520 --> 00:28:31.620]   and don't see how if they push through
[00:28:31.620 --> 00:28:34.680]   and were able to do it, what advantage that would get them.
[00:28:34.680 --> 00:28:36.240]   So I think we got a long way to go
[00:28:36.240 --> 00:28:37.920]   before we're able to do that.
[00:28:37.920 --> 00:28:40.960]   And I think some of it is based on technology,
[00:28:40.960 --> 00:28:43.840]   but more of it's based on the idea of community,
[00:28:43.840 --> 00:28:46.160]   that you gotta actually get people together.
[00:28:47.120 --> 00:28:49.400]   Some of the getting together can be done online.
[00:28:49.400 --> 00:28:51.840]   I think some of it really has to be done in person
[00:28:51.840 --> 00:28:55.800]   in order to build that type of community and trust.
[00:28:55.800 --> 00:28:59.560]   - There's an intentional mechanism
[00:28:59.560 --> 00:29:02.680]   that we've developed a short attention span,
[00:29:02.680 --> 00:29:04.520]   especially younger people,
[00:29:04.520 --> 00:29:08.240]   because sort of shorter and shorter videos online.
[00:29:08.240 --> 00:29:13.840]   The way the brain is developing now
[00:29:13.840 --> 00:29:16.720]   with people that have grown up with the internet,
[00:29:16.720 --> 00:29:19.280]   they have quite a short attention span.
[00:29:19.280 --> 00:29:22.360]   And I would say I had the same when I was growing up too,
[00:29:22.360 --> 00:29:24.000]   probably for different reasons.
[00:29:24.000 --> 00:29:28.160]   So I probably wouldn't have learned as much as I have
[00:29:28.160 --> 00:29:31.440]   if I wasn't forced to sit in a physical classroom,
[00:29:31.440 --> 00:29:34.040]   sort of bored, sometimes falling asleep,
[00:29:34.040 --> 00:29:36.720]   but sort of forcing myself through that process,
[00:29:36.720 --> 00:29:39.760]   to sometimes extremely difficult computer science courses.
[00:29:39.760 --> 00:29:42.160]   What's the difference, in your view,
[00:29:42.160 --> 00:29:46.360]   between in-person education experience,
[00:29:46.360 --> 00:29:48.920]   which you, first of all, yourself had,
[00:29:48.920 --> 00:29:52.120]   and you yourself taught, and online education?
[00:29:52.120 --> 00:29:54.320]   And how do we close that gap, if it's even possible?
[00:29:54.320 --> 00:29:56.360]   - Yeah, so I think there's two issues.
[00:29:56.360 --> 00:30:00.760]   One is whether it's in-person or online,
[00:30:00.760 --> 00:30:03.000]   so it's sort of the physical location.
[00:30:03.000 --> 00:30:07.120]   And then the other is kind of the affiliation, right?
[00:30:07.120 --> 00:30:10.920]   So you stuck with it, in part,
[00:30:10.920 --> 00:30:12.560]   because you were in the classroom
[00:30:12.560 --> 00:30:14.600]   and you saw everybody else was suffering
[00:30:14.600 --> 00:30:16.520]   the same way you were.
[00:30:16.520 --> 00:30:22.160]   But also because you were enrolled, you had paid tuition,
[00:30:22.160 --> 00:30:25.360]   sort of everybody was expecting you to stick with it.
[00:30:25.360 --> 00:30:28.840]   - Society, parents, peers.
[00:30:28.840 --> 00:30:31.120]   - Right, and so those are two separate things.
[00:30:31.120 --> 00:30:32.960]   I mean, you could certainly imagine,
[00:30:32.960 --> 00:30:35.200]   I pay a huge amount of tuition,
[00:30:35.200 --> 00:30:39.160]   and everybody signed up and says, "Yes, you're doing this."
[00:30:39.160 --> 00:30:40.720]   But then I'm in my room,
[00:30:40.720 --> 00:30:43.200]   and my classmates are in different rooms, right?
[00:30:43.200 --> 00:30:45.080]   We could have things set up that way.
[00:30:45.080 --> 00:30:48.880]   So it's not just the online versus offline.
[00:30:48.880 --> 00:30:50.000]   I think what's more important
[00:30:50.000 --> 00:30:52.840]   is the commitment that you've made.
[00:30:52.840 --> 00:30:56.720]   And certainly it is important to have
[00:30:56.720 --> 00:30:59.960]   that kind of informal, you know,
[00:30:59.960 --> 00:31:01.760]   I meet people outside of class,
[00:31:01.760 --> 00:31:04.960]   we talk together because we're all in it together.
[00:31:04.960 --> 00:31:07.560]   I think that's really important,
[00:31:07.560 --> 00:31:10.160]   both in keeping your motivation,
[00:31:10.160 --> 00:31:12.520]   and also that's where some of the most important learning
[00:31:12.520 --> 00:31:13.440]   goes on.
[00:31:13.440 --> 00:31:15.360]   So you wanna have that.
[00:31:15.360 --> 00:31:17.480]   Maybe, you know, especially now,
[00:31:17.480 --> 00:31:19.760]   we start getting into higher bandwidths,
[00:31:19.760 --> 00:31:22.560]   and augmented reality, and virtual reality.
[00:31:22.560 --> 00:31:23.600]   You might be able to get that
[00:31:23.600 --> 00:31:25.920]   without being in the same physical place.
[00:31:25.920 --> 00:31:30.760]   - Do you think it's possible we'll see a course at Stanford,
[00:31:30.760 --> 00:31:34.840]   for example, that for students, enrolled students,
[00:31:34.840 --> 00:31:37.400]   is only online in the near future?
[00:31:37.400 --> 00:31:39.760]   Or literally, sort of, it's part of the curriculum,
[00:31:39.760 --> 00:31:41.160]   and there is no--
[00:31:41.160 --> 00:31:42.520]   - Yeah, so you're starting to see that.
[00:31:42.520 --> 00:31:46.640]   I know Georgia Tech has a master's that's done that way.
[00:31:46.640 --> 00:31:48.400]   - Oftentimes, it's sort of,
[00:31:48.400 --> 00:31:51.000]   they're creeping in, in terms of a master's program,
[00:31:51.000 --> 00:31:54.320]   or sort of further education,
[00:31:54.320 --> 00:31:56.640]   considering the constraints of students and so on.
[00:31:56.640 --> 00:32:00.800]   But I mean, literally, is it possible that we,
[00:32:00.800 --> 00:32:02.760]   you know, Stanford, MIT, Berkeley,
[00:32:02.760 --> 00:32:07.760]   all these places go online only in the next few decades?
[00:32:07.760 --> 00:32:09.360]   - Yeah, probably not, 'cause, you know,
[00:32:09.360 --> 00:32:13.280]   they've got a big commitment to a physical campus.
[00:32:13.280 --> 00:32:16.520]   - Sure, so there's a momentum
[00:32:16.520 --> 00:32:18.320]   that's both financial and cultural.
[00:32:18.320 --> 00:32:21.200]   - Right, and then there are certain things
[00:32:21.200 --> 00:32:25.080]   that's just hard to do virtually, right?
[00:32:25.080 --> 00:32:29.320]   So, you know, we're in a field where,
[00:32:29.320 --> 00:32:32.800]   if you have your own computer, and your own paper,
[00:32:32.800 --> 00:32:35.620]   and so on, you can do the work anywhere.
[00:32:36.760 --> 00:32:39.400]   But if you're in a biology lab or something,
[00:32:39.400 --> 00:32:42.840]   you know, you don't have all the right stuff at home.
[00:32:42.840 --> 00:32:45.720]   - Right, so our field, programming,
[00:32:45.720 --> 00:32:47.400]   you've also done a lot of,
[00:32:47.400 --> 00:32:49.500]   you've done a lot of programming yourself.
[00:32:49.500 --> 00:32:54.280]   In 2001, you wrote a great article about programming
[00:32:54.280 --> 00:32:57.300]   called "Teach Yourself Programming in 10 Years,"
[00:32:57.300 --> 00:32:59.320]   sort of response to all the books
[00:32:59.320 --> 00:33:01.560]   that say "Teach Yourself Programming in 21 Days."
[00:33:01.560 --> 00:33:02.580]   So if you were giving advice
[00:33:02.580 --> 00:33:04.840]   to someone getting into programming today,
[00:33:04.840 --> 00:33:07.280]   this is a few years since you've written that article,
[00:33:07.280 --> 00:33:09.640]   what's the best way to undertake that journey?
[00:33:09.640 --> 00:33:12.360]   - I think there's lots of different ways,
[00:33:12.360 --> 00:33:15.980]   and I think programming means more things now.
[00:33:15.980 --> 00:33:20.160]   And I guess, you know, when I wrote that article,
[00:33:20.160 --> 00:33:21.840]   I was thinking more about
[00:33:21.840 --> 00:33:25.720]   becoming a professional software engineer.
[00:33:25.720 --> 00:33:27.760]   And I thought that's a, you know,
[00:33:27.760 --> 00:33:30.480]   sort of a career-long field of study.
[00:33:30.480 --> 00:33:33.360]   But I think there's lots of things now
[00:33:33.360 --> 00:33:37.600]   that people can do where programming is a part
[00:33:37.600 --> 00:33:41.000]   of solving what they wanna solve
[00:33:41.000 --> 00:33:44.840]   without achieving that professional-level status, right?
[00:33:44.840 --> 00:33:45.800]   So I'm not gonna be going
[00:33:45.800 --> 00:33:47.640]   and writing a million lines of code,
[00:33:47.640 --> 00:33:51.640]   but, you know, I'm a biologist or a physicist or something,
[00:33:51.640 --> 00:33:55.640]   or even a historian, and I've got some data,
[00:33:55.640 --> 00:33:58.440]   and I wanna ask a question of that data.
[00:33:58.440 --> 00:34:02.100]   And I think for that, you don't need 10 years, right?
[00:34:02.100 --> 00:34:04.240]   So there are many shortcuts
[00:34:04.240 --> 00:34:08.480]   to being able to answer those kinds of questions.
[00:34:08.480 --> 00:34:11.840]   And, you know, you see today a lot of emphasis
[00:34:11.840 --> 00:34:15.860]   on learning to code, teaching kids how to code.
[00:34:15.860 --> 00:34:18.760]   I think that's great,
[00:34:18.760 --> 00:34:22.080]   but I wish they would change the message a little bit, right?
[00:34:22.080 --> 00:34:24.720]   So I think code isn't the main thing.
[00:34:24.720 --> 00:34:28.240]   I don't really care if you know the syntax of JavaScript
[00:34:28.240 --> 00:34:31.500]   or if you can connect these blocks together
[00:34:31.500 --> 00:34:33.440]   in this visual language.
[00:34:33.440 --> 00:34:38.220]   But what I do care about is that you can analyze a problem,
[00:34:38.220 --> 00:34:40.960]   you can think of a solution,
[00:34:40.960 --> 00:34:45.640]   you can carry out, you know, make a model,
[00:34:45.640 --> 00:34:49.460]   run that model, test the model, see the results,
[00:34:49.460 --> 00:34:53.640]   verify that they're reasonable,
[00:34:53.640 --> 00:34:55.660]   ask questions and answer them, right?
[00:34:55.660 --> 00:34:58.520]   So it's more modeling and problem solving.
[00:34:58.520 --> 00:35:01.860]   And you use coding in order to do that,
[00:35:01.860 --> 00:35:04.300]   but it's not just learning coding for its own sake.
[00:35:04.300 --> 00:35:05.180]   - That's really interesting.
[00:35:05.180 --> 00:35:08.140]   So it's actually almost, in many cases,
[00:35:08.140 --> 00:35:10.060]   it's learning to work with data,
[00:35:10.060 --> 00:35:11.980]   to extract something useful out of data.
[00:35:11.980 --> 00:35:13.660]   So when you say problem solving,
[00:35:13.660 --> 00:35:15.300]   you really mean taking some kind of,
[00:35:15.300 --> 00:35:17.700]   maybe collecting some kind of data set,
[00:35:17.700 --> 00:35:20.320]   cleaning it up and saying something interesting about it,
[00:35:20.320 --> 00:35:22.380]   which is useful in all kinds of domains.
[00:35:22.380 --> 00:35:28.100]   - You know, and I see myself being stuck sometimes
[00:35:28.100 --> 00:35:30.460]   in kind of the old ways, right?
[00:35:30.460 --> 00:35:33.260]   So, you know, I'll be working on a project,
[00:35:33.260 --> 00:35:37.140]   maybe with a younger employee,
[00:35:37.140 --> 00:35:39.220]   and we say, "Oh, well, here's this new package
[00:35:39.220 --> 00:35:42.340]   "that could help solve this problem."
[00:35:42.340 --> 00:35:44.500]   And I'll go and I'll start reading the manuals.
[00:35:44.500 --> 00:35:48.180]   And, you know, I'll be two hours into reading the manuals.
[00:35:48.180 --> 00:35:51.100]   And then my colleague comes back and says, "I'm done."
[00:35:51.100 --> 00:35:52.140]   - Yep. - You know,
[00:35:52.140 --> 00:35:53.820]   I downloaded the package, I installed it,
[00:35:53.820 --> 00:35:56.500]   I tried calling some things, the first one didn't work,
[00:35:56.500 --> 00:35:58.380]   the second one didn't work, now I'm done.
[00:35:58.380 --> 00:36:00.780]   And I say, "But I have 100 questions about
[00:36:00.780 --> 00:36:02.100]   "how does this work and how does that work?"
[00:36:02.100 --> 00:36:04.060]   And they say, "Who cares, right?
[00:36:04.060 --> 00:36:05.920]   "I don't need to understand the whole thing.
[00:36:05.920 --> 00:36:09.120]   "I answered my question, it's a big complicated package.
[00:36:09.120 --> 00:36:10.480]   "I don't understand the rest of it,
[00:36:10.480 --> 00:36:12.180]   "but I got the right answer."
[00:36:12.180 --> 00:36:15.900]   And I'm just, it's hard for me to get into that mindset.
[00:36:15.900 --> 00:36:17.640]   I wanna understand the whole thing.
[00:36:17.640 --> 00:36:19.420]   And, you know, if they wrote a manual,
[00:36:19.420 --> 00:36:21.380]   I should probably read it.
[00:36:21.380 --> 00:36:23.420]   And, but that's not necessarily the right way.
[00:36:23.420 --> 00:36:28.420]   I think I have to get used to dealing with more,
[00:36:28.420 --> 00:36:30.500]   being more comfortable with uncertainty
[00:36:30.500 --> 00:36:32.060]   and not knowing everything.
[00:36:32.060 --> 00:36:33.620]   - Yeah, so I struggle with the same.
[00:36:33.620 --> 00:36:37.340]   It's sort of the spectrum between Donald and Don Knuth.
[00:36:37.340 --> 00:36:39.020]   - Yeah. - It's kind of the very,
[00:36:39.020 --> 00:36:42.460]   you know, before he can say anything about a problem,
[00:36:42.460 --> 00:36:45.420]   he really has to get down to the machine code assembly.
[00:36:45.420 --> 00:36:47.460]   - Yeah. - Versus exactly
[00:36:47.460 --> 00:36:48.300]   what you said.
[00:36:48.300 --> 00:36:52.380]   I have several students in my group that, you know,
[00:36:52.380 --> 00:36:55.780]   20 years old, and they can solve almost any problem
[00:36:55.780 --> 00:36:56.820]   within a few hours.
[00:36:56.820 --> 00:36:58.260]   That would take me probably weeks
[00:36:58.260 --> 00:37:00.940]   because I would try to, as you said, read the manual.
[00:37:00.940 --> 00:37:04.380]   So do you think the nature of mastery,
[00:37:04.380 --> 00:37:08.560]   you're mentioning biology, sort of outside disciplines,
[00:37:08.560 --> 00:37:13.560]   applying programming, but computer scientists.
[00:37:13.560 --> 00:37:16.460]   So over time, there's higher and higher levels
[00:37:16.460 --> 00:37:18.380]   of abstraction available now.
[00:37:18.380 --> 00:37:23.380]   So with this week, there's the TensorFlow Summit, right?
[00:37:23.380 --> 00:37:27.540]   So if you're not particularly into deep learning,
[00:37:27.540 --> 00:37:29.980]   but you're still a computer scientist,
[00:37:29.980 --> 00:37:33.180]   you can accomplish an incredible amount with TensorFlow
[00:37:33.180 --> 00:37:35.980]   without really knowing any fundamental internals
[00:37:35.980 --> 00:37:37.460]   of machine learning.
[00:37:37.460 --> 00:37:40.900]   Do you think the nature of mastery is changing,
[00:37:40.900 --> 00:37:42.380]   even for computer scientists,
[00:37:42.380 --> 00:37:45.700]   like what it means to be an expert programmer?
[00:37:45.700 --> 00:37:47.740]   - Yeah, I think that's true.
[00:37:47.740 --> 00:37:49.660]   You know, we never really should have focused
[00:37:49.660 --> 00:37:51.500]   on programmers, right?
[00:37:51.500 --> 00:37:53.660]   'Cause it's still, it's a skill,
[00:37:53.660 --> 00:37:56.580]   and what we really want to focus on is the result.
[00:37:56.580 --> 00:38:00.940]   So we built this ecosystem where the way you can get stuff
[00:38:00.940 --> 00:38:03.200]   done is by programming it yourself.
[00:38:03.200 --> 00:38:08.060]   At least when I started, you know, library functions meant
[00:38:08.060 --> 00:38:10.620]   you had square root, and that was about it.
[00:38:10.620 --> 00:38:13.060]   Right, everything else you built from scratch.
[00:38:13.060 --> 00:38:16.140]   And then we built up an ecosystem where a lot of times,
[00:38:16.140 --> 00:38:19.040]   well, you can download a lot of stuff that does a big part
[00:38:19.040 --> 00:38:20.220]   of what you need.
[00:38:20.220 --> 00:38:23.740]   And so now it's more a question of assembly
[00:38:23.740 --> 00:38:27.220]   rather than manufacturing.
[00:38:27.220 --> 00:38:32.220]   And that's a different way of looking at problems.
[00:38:32.220 --> 00:38:34.240]   - From another perspective in terms of mastery
[00:38:34.240 --> 00:38:37.660]   and looking at programmers or people that reason
[00:38:37.660 --> 00:38:39.780]   about problems in a computational way.
[00:38:39.780 --> 00:38:44.140]   So Google, you know, from the hiring perspective,
[00:38:44.140 --> 00:38:45.860]   from the perspective of hiring or building a team
[00:38:45.860 --> 00:38:48.860]   of programmers, how do you determine if someone's
[00:38:48.860 --> 00:38:50.280]   a good programmer?
[00:38:50.280 --> 00:38:53.500]   Or if somebody, again, so I want to deviate from,
[00:38:53.500 --> 00:38:55.380]   I want to move away from the word programmer,
[00:38:55.380 --> 00:38:58.820]   but somebody who could solve problems of large scale data
[00:38:58.820 --> 00:38:59.660]   and so on.
[00:38:59.660 --> 00:39:02.740]   How do you build a team like that
[00:39:02.740 --> 00:39:03.940]   through the interviewing process?
[00:39:03.940 --> 00:39:08.860]   - Yeah, and I think as a company grows,
[00:39:08.860 --> 00:39:12.560]   you get more expansive in the types of people
[00:39:12.560 --> 00:39:14.460]   you're looking for, right?
[00:39:14.460 --> 00:39:16.580]   So I think, you know, in the early days,
[00:39:16.580 --> 00:39:19.380]   we'd interview people and the question we were trying
[00:39:19.380 --> 00:39:22.480]   to ask is, how close are they to Jeff Dean?
[00:39:22.480 --> 00:39:26.780]   And most people were pretty far away,
[00:39:26.780 --> 00:39:28.180]   but we'd take the ones that were, you know,
[00:39:28.180 --> 00:39:29.380]   not that far away.
[00:39:29.380 --> 00:39:32.540]   And so we got kind of a homogeneous group of people
[00:39:32.540 --> 00:39:34.560]   who are really great programmers.
[00:39:34.560 --> 00:39:37.380]   Then as a company grows, you say, well,
[00:39:37.380 --> 00:39:39.100]   we don't want everybody to be the same,
[00:39:39.100 --> 00:39:40.700]   to have the same skill set.
[00:39:40.700 --> 00:39:45.700]   And so now we're hiring biologists in our health areas
[00:39:45.700 --> 00:39:48.980]   and we're hiring physicists,
[00:39:48.980 --> 00:39:51.220]   we're hiring mechanical engineers,
[00:39:51.220 --> 00:39:56.100]   we're hiring, you know, social scientists and ethnographers
[00:39:56.100 --> 00:39:59.180]   and people with different backgrounds
[00:39:59.180 --> 00:40:00.900]   who bring different skills.
[00:40:00.900 --> 00:40:06.300]   - So you have mentioned that you still may partake
[00:40:06.300 --> 00:40:07.500]   in code reviews.
[00:40:08.620 --> 00:40:10.740]   Given that you have a wealth of experience,
[00:40:10.740 --> 00:40:11.980]   as you've also mentioned,
[00:40:11.980 --> 00:40:16.660]   what errors do you often see and tend to highlight
[00:40:16.660 --> 00:40:20.020]   in the code of junior developers of people coming up now,
[00:40:20.020 --> 00:40:23.460]   given your background from Wisp
[00:40:23.460 --> 00:40:26.020]   to a couple of decades of programming?
[00:40:26.020 --> 00:40:27.620]   - Yeah, that's a great question.
[00:40:27.620 --> 00:40:31.900]   You know, sometimes I try to look at the flexibility
[00:40:31.900 --> 00:40:36.420]   of the design of, yes, you know, this API solves
[00:40:36.420 --> 00:40:39.900]   this problem, but where is it gonna go in the future?
[00:40:39.900 --> 00:40:41.940]   Who else is gonna wanna call this?
[00:40:41.940 --> 00:40:45.180]   And, you know, are you making it easier
[00:40:45.180 --> 00:40:46.940]   for them to do that?
[00:40:46.940 --> 00:40:50.640]   - That's a matter of design, is it documentation,
[00:40:50.640 --> 00:40:53.900]   is it sort of an amorphous thing
[00:40:53.900 --> 00:40:55.140]   you can't really put into words?
[00:40:55.140 --> 00:40:56.660]   It's just how it feels.
[00:40:56.660 --> 00:40:58.340]   If you put yourself in the shoes of a developer,
[00:40:58.340 --> 00:40:59.540]   would you use this kind of thing?
[00:40:59.540 --> 00:41:01.500]   - I think it is how you feel, right?
[00:41:01.500 --> 00:41:03.900]   And so yeah, documentation is good,
[00:41:04.300 --> 00:41:06.500]   but it's more a design question, right?
[00:41:06.500 --> 00:41:10.260]   If you get the design right, then people will figure it out
[00:41:10.260 --> 00:41:12.140]   whether the documentation is good or not.
[00:41:12.140 --> 00:41:16.220]   And if the design's wrong, then it'll be harder to use.
[00:41:16.220 --> 00:41:20.740]   - How have you yourself changed as a programmer
[00:41:20.740 --> 00:41:21.840]   over the years?
[00:41:21.840 --> 00:41:25.580]   In a way, you already started to say,
[00:41:25.580 --> 00:41:28.140]   sort of, you want to read the manual,
[00:41:28.140 --> 00:41:30.900]   you want to understand the core of the syntax
[00:41:30.900 --> 00:41:33.820]   to how the language is supposed to be used and so on.
[00:41:33.820 --> 00:41:36.580]   But what's the evolution been like
[00:41:36.580 --> 00:41:39.840]   from the '80s, '90s to today?
[00:41:39.840 --> 00:41:42.860]   - I guess one thing is you don't have to worry
[00:41:42.860 --> 00:41:46.420]   about the small details of efficiency
[00:41:46.420 --> 00:41:48.100]   as much as you used to, right?
[00:41:48.100 --> 00:41:53.100]   So like I remember, I did my list book in the '90s
[00:41:53.100 --> 00:41:56.300]   and one of the things I wanted to do was say,
[00:41:56.300 --> 00:41:58.940]   here's how you do an object system.
[00:41:58.940 --> 00:42:01.580]   And basically, we're gonna make it
[00:42:01.580 --> 00:42:03.620]   so each object is a hash table
[00:42:03.620 --> 00:42:05.580]   and you look up the methods and here's how it works.
[00:42:05.580 --> 00:42:07.380]   And then I said, of course,
[00:42:07.380 --> 00:42:12.220]   the real Common Lisp object system is much more complicated.
[00:42:12.220 --> 00:42:15.180]   It's got all these efficiency type issues
[00:42:15.180 --> 00:42:16.620]   and this is just a toy.
[00:42:16.620 --> 00:42:18.980]   Nobody would do this in real life.
[00:42:18.980 --> 00:42:21.380]   And it turns out Python pretty much did exactly
[00:42:21.380 --> 00:42:27.460]   what I said and said objects are just dictionaries.
[00:42:27.460 --> 00:42:30.100]   And yeah, they have a few little tricks as well.
[00:42:30.100 --> 00:42:34.820]   But mostly, the thing that would have been 100 times
[00:42:34.820 --> 00:42:39.140]   too slow in the '80s is now plenty fast for most everything.
[00:42:39.140 --> 00:42:43.220]   - So you had to, as a programmer, let go of perhaps
[00:42:43.220 --> 00:42:45.880]   an obsession that I remember coming up with
[00:42:45.880 --> 00:42:48.340]   of trying to write efficient code.
[00:42:48.340 --> 00:42:53.340]   - Yeah, to say what really matters is the total time
[00:42:53.340 --> 00:42:56.100]   it takes to get the project done.
[00:42:56.100 --> 00:42:59.060]   And most of that's gonna be the programmer time.
[00:42:59.060 --> 00:43:00.660]   So if you're a little bit less efficient
[00:43:00.660 --> 00:43:04.220]   but it makes it easier to understand and modify,
[00:43:04.220 --> 00:43:05.860]   then that's the right trade-off.
[00:43:05.860 --> 00:43:07.660]   - So you've written quite a bit about Lisp.
[00:43:07.660 --> 00:43:09.460]   Your book on programming is in Lisp.
[00:43:09.460 --> 00:43:12.860]   You have a lot of code out there that's in Lisp.
[00:43:12.860 --> 00:43:16.940]   So myself and people who don't know what Lisp is
[00:43:16.940 --> 00:43:17.980]   should look it up.
[00:43:17.980 --> 00:43:20.740]   It's my favorite language for many AI researchers.
[00:43:20.740 --> 00:43:22.420]   It is a favorite language.
[00:43:22.420 --> 00:43:25.500]   The favorite language they never use these days.
[00:43:25.500 --> 00:43:26.940]   So what part of Lisp do you find
[00:43:26.940 --> 00:43:28.940]   most beautiful and powerful?
[00:43:28.940 --> 00:43:31.660]   - So I think the beautiful part is the simplicity
[00:43:31.660 --> 00:43:35.180]   that in half a page you can define the whole language.
[00:43:35.180 --> 00:43:38.460]   And other languages don't have that.
[00:43:38.460 --> 00:43:41.380]   So you feel like you can hold everything in your head.
[00:43:41.380 --> 00:43:47.780]   And then a lot of people say, well, then that's too simple.
[00:43:47.780 --> 00:43:50.420]   Here's all these things I wanna do.
[00:43:50.420 --> 00:43:55.420]   And my Java or Python or whatever has 100 or 200
[00:43:55.420 --> 00:43:58.740]   or 300 different syntax rules
[00:43:58.740 --> 00:44:00.380]   and don't I need all those?
[00:44:00.380 --> 00:44:03.900]   And Lisp's answer was, no, we're only gonna give you
[00:44:03.900 --> 00:44:06.020]   eight or so syntax rules,
[00:44:06.020 --> 00:44:09.060]   but we're gonna allow you to define your own.
[00:44:09.060 --> 00:44:11.340]   And so that was a very powerful idea.
[00:44:11.340 --> 00:44:15.900]   And I think this idea of saying,
[00:44:15.900 --> 00:44:20.340]   I can start with my problem and with my data,
[00:44:20.340 --> 00:44:23.540]   and then I can build the language I want
[00:44:23.540 --> 00:44:25.940]   for that problem and for that data.
[00:44:25.940 --> 00:44:28.420]   And then I can make Lisp define that language.
[00:44:28.420 --> 00:44:32.180]   So you're sort of mixing levels
[00:44:32.180 --> 00:44:36.060]   and saying I'm simultaneously a programmer in a language
[00:44:36.060 --> 00:44:37.460]   and a language designer.
[00:44:37.460 --> 00:44:41.860]   And that allows a better match between your problem
[00:44:41.860 --> 00:44:43.660]   and your eventual code.
[00:44:43.660 --> 00:44:47.460]   And I think Lisp had done that better than other languages.
[00:44:47.460 --> 00:44:49.420]   - Yeah, it's a very elegant implementation
[00:44:49.420 --> 00:44:51.260]   of functional programming.
[00:44:51.260 --> 00:44:55.220]   But why do you think Lisp has not had the mass adoption
[00:44:55.220 --> 00:44:57.220]   and success of languages like Python?
[00:44:57.220 --> 00:44:58.420]   Is it the parentheses?
[00:44:58.420 --> 00:45:00.580]   Is it all the parentheses?
[00:45:00.580 --> 00:45:03.940]   - Yeah, so I think a couple of things.
[00:45:03.940 --> 00:45:10.140]   So one was, I think it was designed for a single programmer
[00:45:10.140 --> 00:45:14.860]   or a small team and a skilled programmer
[00:45:14.860 --> 00:45:17.100]   who had the good taste to say,
[00:45:17.100 --> 00:45:19.540]   well, I am doing language design
[00:45:19.540 --> 00:45:21.700]   and I have to make good choices.
[00:45:21.700 --> 00:45:23.780]   And if you make good choices, that's great.
[00:45:23.780 --> 00:45:28.020]   If you make bad choices, you can hurt yourself
[00:45:28.020 --> 00:45:30.220]   and it can be hard for other people on the team
[00:45:30.220 --> 00:45:31.060]   to understand it.
[00:45:31.060 --> 00:45:34.220]   So I think there was a limit to the scale
[00:45:34.220 --> 00:45:36.940]   of the size of a project in terms of number of people
[00:45:36.940 --> 00:45:38.500]   that Lisp was good for.
[00:45:38.500 --> 00:45:42.100]   And as an industry, we kind of grew beyond that.
[00:45:42.100 --> 00:45:45.940]   I think it is in part the parentheses.
[00:45:45.940 --> 00:45:49.580]   One of the jokes is the acronym for Lisp
[00:45:49.580 --> 00:45:52.260]   is lots of irritating, silly parentheses.
[00:45:52.460 --> 00:45:53.980]   (laughing)
[00:45:53.980 --> 00:45:57.220]   My acronym was Lisp is syntactically pure,
[00:45:57.220 --> 00:46:01.420]   saying all you need is parentheses and atoms.
[00:46:01.420 --> 00:46:05.180]   But I remember, so we had the AI textbook
[00:46:05.180 --> 00:46:08.660]   and because we did it in the 90s,
[00:46:08.660 --> 00:46:11.380]   we had pseudocode in the book,
[00:46:11.380 --> 00:46:13.340]   but then we said, well, we'll have Lisp online
[00:46:13.340 --> 00:46:16.180]   'cause that's the language of AI at the time.
[00:46:16.180 --> 00:46:18.300]   And I remember some of the students complaining
[00:46:18.300 --> 00:46:20.020]   'cause they hadn't had Lisp before
[00:46:20.020 --> 00:46:22.100]   and they didn't quite understand what was going on.
[00:46:22.100 --> 00:46:24.860]   And I remember one student complained,
[00:46:24.860 --> 00:46:26.660]   I don't understand how this pseudocode
[00:46:26.660 --> 00:46:29.180]   corresponds to this Lisp.
[00:46:29.180 --> 00:46:31.540]   And there was a one-to-one correspondence
[00:46:31.540 --> 00:46:35.820]   between the symbols in the code and the pseudocode.
[00:46:35.820 --> 00:46:38.180]   And the only thing difference was the parentheses.
[00:46:38.180 --> 00:46:41.260]   So I said, it must be that for some people,
[00:46:41.260 --> 00:46:45.060]   a certain number of left parentheses shuts off their brain.
[00:46:45.060 --> 00:46:47.180]   - Yeah, it's very possible in that sense
[00:46:47.180 --> 00:46:48.980]   and Python just goes the other way.
[00:46:48.980 --> 00:46:51.100]   - And so that was the point at which I said,
[00:46:51.100 --> 00:46:54.300]   okay, can't have only Lisp as a language
[00:46:54.300 --> 00:46:58.580]   'cause I don't wanna, you only got 10 or 12 or 15 weeks
[00:46:58.580 --> 00:46:59.940]   or whatever it is to teach AI
[00:46:59.940 --> 00:47:03.020]   and I don't want to waste two weeks of that teaching Lisp.
[00:47:03.020 --> 00:47:04.460]   So I said, I gotta have another language.
[00:47:04.460 --> 00:47:06.940]   Java was the most popular language at the time.
[00:47:06.940 --> 00:47:09.100]   I started doing that and then I said,
[00:47:09.100 --> 00:47:12.740]   it's really hard to have a one-to-one correspondence
[00:47:12.740 --> 00:47:14.500]   between the pseudocode and the Java
[00:47:14.500 --> 00:47:16.020]   'cause Java is so verbose.
[00:47:16.020 --> 00:47:18.940]   So then I said, I'm gonna do a survey
[00:47:18.940 --> 00:47:22.900]   and find the language that's most like my pseudocode.
[00:47:22.900 --> 00:47:26.220]   And it turned out Python basically was my pseudocode.
[00:47:26.220 --> 00:47:29.220]   Somehow I had channeled Guido
[00:47:29.220 --> 00:47:32.660]   and designed a pseudocode that was the same as Python,
[00:47:32.660 --> 00:47:36.140]   although I hadn't heard of Python at that point.
[00:47:36.140 --> 00:47:38.300]   And from then on, that's what I've been using
[00:47:38.300 --> 00:47:39.800]   'cause it's been a good match.
[00:47:39.800 --> 00:47:45.660]   - So what's the story in Python behind PyTudes,
[00:47:45.660 --> 00:47:48.380]   your GitHub repository with puzzles and exercises
[00:47:48.380 --> 00:47:49.740]   in Python is pretty fun.
[00:47:49.740 --> 00:47:51.920]   - Yeah, it seems like fun.
[00:47:51.920 --> 00:47:57.500]   I like doing puzzles and I like being an educator.
[00:47:57.500 --> 00:48:02.180]   I did a class with Udacity, Udacity 212, I think it was.
[00:48:02.180 --> 00:48:05.100]   It was basically problem solving,
[00:48:05.100 --> 00:48:08.980]   using Python and looking at different problems.
[00:48:08.980 --> 00:48:11.900]   - Does PyTudes feed that class in terms of the exercises?
[00:48:11.900 --> 00:48:12.740]   I was wondering what the--
[00:48:12.740 --> 00:48:15.060]   - Yeah, so the class came first.
[00:48:15.060 --> 00:48:17.660]   Some of the stuff that's in PyTudes was write-ups
[00:48:17.660 --> 00:48:19.260]   of what was in the class and then some of it
[00:48:19.260 --> 00:48:23.200]   was just continuing to work on new problems.
[00:48:23.200 --> 00:48:26.820]   - So what's the organizing madness of PyTudes?
[00:48:26.820 --> 00:48:30.060]   Is it just a collection of cool exercises?
[00:48:30.060 --> 00:48:31.300]   - Just whatever I thought was fun.
[00:48:31.300 --> 00:48:32.780]   - Okay, awesome.
[00:48:32.780 --> 00:48:35.860]   So you were the director of search quality at Google
[00:48:35.860 --> 00:48:40.540]   from 2001 to 2005 in the early days
[00:48:40.540 --> 00:48:41.820]   when there's just a few employees
[00:48:41.820 --> 00:48:46.380]   and when the company was growing like crazy, right?
[00:48:46.380 --> 00:48:51.380]   So, I mean, Google revolutionized the way we discover,
[00:48:51.380 --> 00:48:55.380]   share and aggregate knowledge.
[00:48:55.380 --> 00:49:00.300]   So just, this is one of the fundamental aspects
[00:49:00.300 --> 00:49:01.420]   of civilization, right?
[00:49:01.420 --> 00:49:03.180]   Is information being shared
[00:49:03.180 --> 00:49:04.940]   and there's different mechanisms throughout history
[00:49:04.940 --> 00:49:08.620]   but Google is just 10x improve that, right?
[00:49:08.620 --> 00:49:10.280]   And you were a part of that, right?
[00:49:10.280 --> 00:49:11.900]   People discovering that information.
[00:49:11.900 --> 00:49:14.260]   So what were some of the challenges
[00:49:14.260 --> 00:49:16.340]   on a philosophical or the technical level
[00:49:16.340 --> 00:49:17.460]   in those early days?
[00:49:17.460 --> 00:49:20.140]   - It definitely was an exciting time
[00:49:20.140 --> 00:49:23.060]   and as you say, we were doubling in size every year
[00:49:23.060 --> 00:49:28.000]   and the challenges were we wanted to get the right answers.
[00:49:28.000 --> 00:49:32.540]   And we had to figure out what that meant.
[00:49:32.540 --> 00:49:36.380]   We had to implement that and we had to make it all efficient
[00:49:36.380 --> 00:49:41.380]   and we had to keep on testing
[00:49:41.380 --> 00:49:44.140]   and seeing if we were delivering good answers.
[00:49:44.140 --> 00:49:45.660]   - And now when you say good answers,
[00:49:45.660 --> 00:49:47.780]   it means whatever people are typing in
[00:49:47.780 --> 00:49:50.380]   in terms of keywords, in terms of that kind of thing,
[00:49:50.380 --> 00:49:53.660]   that the results they get are ordered
[00:49:53.660 --> 00:49:56.580]   by the desirability for them of those results.
[00:49:56.580 --> 00:49:58.620]   Like the first thing they click on
[00:49:58.620 --> 00:49:59.500]   will likely be the thing
[00:49:59.500 --> 00:50:01.540]   that they were actually looking for.
[00:50:01.540 --> 00:50:03.180]   - Right, one of the metrics we had
[00:50:03.180 --> 00:50:05.060]   was focused on the first thing.
[00:50:05.060 --> 00:50:07.580]   Some of it was focused on the whole page.
[00:50:07.580 --> 00:50:10.720]   Some of it was focused on the top three or so.
[00:50:10.720 --> 00:50:13.500]   So we looked at a lot of different metrics
[00:50:13.500 --> 00:50:15.700]   for how well we were doing
[00:50:15.700 --> 00:50:19.260]   and we broke it down into subclasses of,
[00:50:19.260 --> 00:50:23.540]   maybe here's a type of query that we're not doing well on
[00:50:23.540 --> 00:50:25.540]   and we try to fix that.
[00:50:25.540 --> 00:50:27.940]   Early on, we started to realize
[00:50:27.940 --> 00:50:30.380]   that we were in an adversarial position.
[00:50:30.380 --> 00:50:32.780]   So we started thinking,
[00:50:32.780 --> 00:50:36.460]   well, we're kind of like the card catalog in the library.
[00:50:36.460 --> 00:50:39.540]   So the books are here and we're off to the side
[00:50:39.540 --> 00:50:42.700]   and we're just reflecting what's there.
[00:50:42.700 --> 00:50:45.660]   And then we realized every time we make a change,
[00:50:45.660 --> 00:50:47.940]   the webmasters make a change.
[00:50:47.940 --> 00:50:50.100]   And it's a game theoretic.
[00:50:50.100 --> 00:50:52.260]   And so we had to think not only
[00:50:52.260 --> 00:50:55.540]   is this the right move for us to make now,
[00:50:55.540 --> 00:50:57.820]   but also if we make this move,
[00:50:57.820 --> 00:50:59.860]   what's the counter move gonna be?
[00:50:59.860 --> 00:51:02.300]   Is that gonna get us into a worse place?
[00:51:02.300 --> 00:51:03.740]   In which case, we won't make that move,
[00:51:03.740 --> 00:51:05.540]   we'll make a different move.
[00:51:05.540 --> 00:51:06.640]   - And did you find, I mean,
[00:51:06.640 --> 00:51:09.500]   I assume with the popularity and the growth of the internet
[00:51:09.500 --> 00:51:11.540]   that people were creating new content.
[00:51:11.540 --> 00:51:14.260]   So you're almost helping guide the creation of new content.
[00:51:14.260 --> 00:51:15.820]   - Yeah, so that's certainly true.
[00:51:15.820 --> 00:51:20.820]   So we definitely changed the structure of the network.
[00:51:20.820 --> 00:51:24.540]   So if you think back in the very early days,
[00:51:24.540 --> 00:51:28.300]   Larry and Sergey had the PageRank paper
[00:51:28.300 --> 00:51:33.260]   and John Kleinberg had this hubs and authorities model,
[00:51:33.260 --> 00:51:37.260]   which says the web is made out of these hubs,
[00:51:38.500 --> 00:51:43.500]   which will be my page of cool links about dogs or whatever.
[00:51:43.500 --> 00:51:46.900]   And people would just list links.
[00:51:46.900 --> 00:51:47.980]   And then there'd be authorities,
[00:51:47.980 --> 00:51:50.900]   which were the ones that page about dogs
[00:51:50.900 --> 00:51:52.140]   that most people link to.
[00:51:52.140 --> 00:51:54.260]   That doesn't happen anymore.
[00:51:54.260 --> 00:51:57.820]   People don't bother to say my page of cool links,
[00:51:57.820 --> 00:51:59.700]   'cause we took over that function, right?
[00:51:59.700 --> 00:52:03.420]   So we changed the way that worked.
[00:52:03.420 --> 00:52:05.700]   - Did you imagine back then that the internet
[00:52:05.700 --> 00:52:08.860]   would be as massively vibrant as it is today?
[00:52:08.860 --> 00:52:10.380]   I mean, it was already growing quickly,
[00:52:10.380 --> 00:52:14.820]   but it's just another, I don't know if you've ever,
[00:52:14.820 --> 00:52:18.020]   today, if you sit back and just look at the internet
[00:52:18.020 --> 00:52:20.580]   with wonder, the amount of content
[00:52:20.580 --> 00:52:22.060]   that's just constantly being created,
[00:52:22.060 --> 00:52:24.260]   constantly being shared and deployed.
[00:52:24.260 --> 00:52:27.420]   - Yeah, it's always been surprising to me.
[00:52:27.420 --> 00:52:30.540]   I guess I'm not very good at predicting the future.
[00:52:30.540 --> 00:52:31.380]   - Okay.
[00:52:31.380 --> 00:52:35.740]   - I remember being a graduate student in 1980 or so,
[00:52:35.740 --> 00:52:39.460]   and we had the ARPANET,
[00:52:39.460 --> 00:52:44.460]   and then there was this proposal to commercialize it,
[00:52:44.460 --> 00:52:48.260]   have this internet, and this crazy Senator Gore
[00:52:48.260 --> 00:52:51.260]   thought that might be a good idea.
[00:52:51.260 --> 00:52:53.020]   And I remember thinking, oh, come on,
[00:52:53.020 --> 00:52:55.820]   you can't expect a commercial company
[00:52:55.820 --> 00:52:58.300]   to understand this technology.
[00:52:58.300 --> 00:52:59.300]   They'll never be able to do it.
[00:52:59.300 --> 00:53:01.540]   Yeah, okay, we can have this dot-com domain,
[00:53:01.540 --> 00:53:03.340]   but it won't go anywhere.
[00:53:03.340 --> 00:53:05.540]   So I was wrong, Al Gore was right.
[00:53:05.540 --> 00:53:07.900]   - At the same time, the nature of what it means
[00:53:07.900 --> 00:53:09.860]   to be a commercial company has changed too.
[00:53:09.860 --> 00:53:12.700]   So Google, in many ways, at its founding,
[00:53:12.700 --> 00:53:16.780]   is different than what companies were before, I think.
[00:53:16.780 --> 00:53:19.740]   - Right, so there's all these business models
[00:53:19.740 --> 00:53:23.060]   that are so different than what was possible back then.
[00:53:23.060 --> 00:53:24.940]   - So in terms of predicting the future,
[00:53:24.940 --> 00:53:27.220]   what do you think it takes to build a system
[00:53:27.220 --> 00:53:29.900]   that approaches human-level intelligence?
[00:53:29.900 --> 00:53:31.740]   You've talked about, of course,
[00:53:31.740 --> 00:53:34.100]   that we shouldn't be so obsessed
[00:53:34.100 --> 00:53:36.300]   about creating human-level intelligence.
[00:53:36.300 --> 00:53:39.280]   We just create systems that are very useful for humans.
[00:53:39.280 --> 00:53:44.280]   But what do you think it takes to approach that level?
[00:53:44.280 --> 00:53:47.380]   - Right, so certainly I don't think
[00:53:47.380 --> 00:53:49.860]   human-level intelligence is one thing, right?
[00:53:49.860 --> 00:53:51.660]   So I think there's lots of different tasks,
[00:53:51.660 --> 00:53:54.060]   lots of different capabilities.
[00:53:54.060 --> 00:53:56.740]   I also don't think that should be the goal, right?
[00:53:56.740 --> 00:54:01.620]   So I wouldn't want to create a calculator
[00:54:01.620 --> 00:54:04.300]   that could do multiplication at human level, right?
[00:54:04.300 --> 00:54:06.020]   That would be a step backwards.
[00:54:06.020 --> 00:54:07.500]   And so for many things,
[00:54:07.500 --> 00:54:09.700]   we should be aiming far beyond human level.
[00:54:09.700 --> 00:54:13.020]   For other things, maybe human level
[00:54:13.020 --> 00:54:14.460]   is a good level to aim at.
[00:54:14.460 --> 00:54:16.820]   And for others, we'd say,
[00:54:16.820 --> 00:54:18.020]   "Well, let's not bother doing this
[00:54:18.020 --> 00:54:20.860]   "'cause we already have humans can take on those tasks."
[00:54:20.860 --> 00:54:26.380]   So as you say, I like to focus on what's a useful tool.
[00:54:26.380 --> 00:54:30.500]   And in some cases, being at human level
[00:54:30.500 --> 00:54:32.580]   is an important part of crossing that threshold
[00:54:32.580 --> 00:54:34.540]   to make the tool useful.
[00:54:34.540 --> 00:54:39.380]   So we see in things like these personal assistants now
[00:54:39.380 --> 00:54:41.020]   that you get either on your phone
[00:54:41.020 --> 00:54:44.540]   or on a speaker that sits on the table,
[00:54:44.540 --> 00:54:46.460]   you wanna be able to have a conversation with those.
[00:54:46.460 --> 00:54:49.840]   And I think as an industry,
[00:54:49.840 --> 00:54:51.860]   we haven't quite figured out what the right model is
[00:54:51.860 --> 00:54:53.900]   for what these things can do.
[00:54:54.980 --> 00:54:56.260]   And we're aiming towards,
[00:54:56.260 --> 00:54:57.900]   well, you just have a conversation with them
[00:54:57.900 --> 00:55:00.260]   the way you can with a person.
[00:55:00.260 --> 00:55:02.940]   But we haven't delivered on that model yet, right?
[00:55:02.940 --> 00:55:04.940]   So you can ask it, "What's the weather?"
[00:55:04.940 --> 00:55:08.400]   You can ask it, "Play some nice songs,"
[00:55:08.400 --> 00:55:11.620]   and five or six other things,
[00:55:11.620 --> 00:55:13.980]   and then you run out of stuff that it can do.
[00:55:13.980 --> 00:55:16.340]   - In terms of a deep, meaningful connection.
[00:55:16.340 --> 00:55:17.980]   So you've mentioned the movie "Her"
[00:55:17.980 --> 00:55:20.220]   as one of your favorite AI movies.
[00:55:20.220 --> 00:55:21.980]   Do you think it's possible for a human being
[00:55:21.980 --> 00:55:24.060]   to fall in love with an AI system?
[00:55:24.060 --> 00:55:25.740]   AI assistant, as you mentioned.
[00:55:25.740 --> 00:55:28.860]   So taking this big leap from what's the weather
[00:55:28.860 --> 00:55:31.260]   to having a deep connection.
[00:55:31.260 --> 00:55:35.860]   - Yeah, I think as people, that's what we love to do.
[00:55:35.860 --> 00:55:39.420]   And I was at a showing of "Her"
[00:55:39.420 --> 00:55:41.140]   where we had a panel discussion,
[00:55:41.140 --> 00:55:43.500]   and somebody asked me,
[00:55:43.500 --> 00:55:46.940]   "What other movie do you think 'Her' is similar to?"
[00:55:46.940 --> 00:55:50.300]   And my answer was "Life of Brian,"
[00:55:50.300 --> 00:55:52.580]   which is not a science fiction movie,
[00:55:53.580 --> 00:55:57.260]   but both movies are about wanting to believe
[00:55:57.260 --> 00:55:59.380]   in something that's not necessarily real.
[00:55:59.380 --> 00:56:01.860]   - Yeah, by the way, for people who don't know,
[00:56:01.860 --> 00:56:02.980]   it's Monty Python.
[00:56:02.980 --> 00:56:05.100]   Yeah, that's been brilliantly put.
[00:56:05.100 --> 00:56:07.580]   - Right, so I think that's just the way we are.
[00:56:07.580 --> 00:56:11.060]   We want to trust, we want to believe,
[00:56:11.060 --> 00:56:12.460]   we want to fall in love,
[00:56:12.460 --> 00:56:15.980]   and it doesn't necessarily take that much, right?
[00:56:15.980 --> 00:56:20.740]   So my kids fell in love with their teddy bear,
[00:56:20.740 --> 00:56:23.380]   and the teddy bear was not very interactive, right?
[00:56:23.380 --> 00:56:26.820]   So that's all us pushing our feelings
[00:56:26.820 --> 00:56:29.700]   onto our devices and our things,
[00:56:29.700 --> 00:56:31.900]   and I think that that's what we like to do,
[00:56:31.900 --> 00:56:33.340]   so we'll continue to do that.
[00:56:33.340 --> 00:56:36.220]   - So yeah, as human beings, we long for that connection,
[00:56:36.220 --> 00:56:39.580]   and just AI has to do a little bit of work
[00:56:39.580 --> 00:56:41.860]   to catch us in the other end.
[00:56:41.860 --> 00:56:46.140]   - Yeah, and certainly, if you can get to dog level,
[00:56:46.140 --> 00:56:49.460]   a lot of people have invested a lot of love in their pets.
[00:56:49.460 --> 00:56:50.300]   - In their pets.
[00:56:50.300 --> 00:56:52.940]   Some people, as I've been told,
[00:56:52.940 --> 00:56:54.420]   in working with autonomous vehicles
[00:56:54.420 --> 00:56:58.260]   have invested a lot of love into their inanimate cars,
[00:56:58.260 --> 00:57:00.900]   so it really doesn't take much.
[00:57:00.900 --> 00:57:05.220]   So what is a good test to linger on a topic
[00:57:05.220 --> 00:57:07.860]   that may be silly or a little bit philosophical?
[00:57:07.860 --> 00:57:10.300]   What is a good test of intelligence in your view?
[00:57:10.300 --> 00:57:14.420]   Is natural conversation like in the Turing test
[00:57:14.420 --> 00:57:16.460]   a good test?
[00:57:16.460 --> 00:57:19.960]   Put another way, what would impress you
[00:57:19.960 --> 00:57:22.700]   if you saw a computer do it these days?
[00:57:22.700 --> 00:57:24.380]   - Yeah, I mean, I get impressed all the time.
[00:57:24.380 --> 00:57:26.180]   (laughing)
[00:57:26.180 --> 00:57:27.940]   Right, so-- - But like really impressive.
[00:57:27.940 --> 00:57:31.060]   - You know, Go playing, StarCraft playing,
[00:57:31.060 --> 00:57:33.220]   those are all pretty cool.
[00:57:33.220 --> 00:57:39.900]   You know, and I think, sure, conversation is important.
[00:57:39.900 --> 00:57:44.820]   I think, you know, we sometimes have these tests
[00:57:44.820 --> 00:57:46.740]   where it's easy to fool the system,
[00:57:46.740 --> 00:57:51.420]   where you can have a chatbot that can have a conversation,
[00:57:51.420 --> 00:57:54.580]   but it never gets into a situation
[00:57:54.580 --> 00:57:55.660]   where it has to be deep enough
[00:57:55.660 --> 00:58:00.660]   that it really reveals itself as being intelligent or not.
[00:58:00.660 --> 00:58:05.020]   I think, you know, Turing suggested that,
[00:58:05.020 --> 00:58:07.980]   but I think if he were alive, he'd say,
[00:58:07.980 --> 00:58:10.620]   you know, I didn't really mean that seriously.
[00:58:10.620 --> 00:58:11.620]   Right? - Yeah.
[00:58:11.620 --> 00:58:15.180]   - And I think, you know, this is just my opinion,
[00:58:15.180 --> 00:58:19.180]   but I think Turing's point was not that
[00:58:19.180 --> 00:58:21.540]   this test of conversation is a good test.
[00:58:21.540 --> 00:58:25.420]   I think his point was having a test is the right thing.
[00:58:25.420 --> 00:58:27.980]   So rather than having the philosopher say,
[00:58:27.980 --> 00:58:31.100]   oh no, AI is impossible, you should say,
[00:58:31.100 --> 00:58:32.140]   well, we'll just have a test,
[00:58:32.140 --> 00:58:34.700]   and then the result of that will tell us the answer.
[00:58:34.700 --> 00:58:37.340]   And it doesn't necessarily have to be a conversation test.
[00:58:37.340 --> 00:58:39.300]   - That's right, and coming up with a new, better test
[00:58:39.300 --> 00:58:42.240]   as the technology evolves is probably the right way.
[00:58:42.240 --> 00:58:45.900]   Do you worry, as a lot of the general public does,
[00:58:45.900 --> 00:58:50.900]   about, not a lot, but some vocal part of the general public
[00:58:50.900 --> 00:58:53.660]   about the existential threat of artificial intelligence?
[00:58:53.660 --> 00:58:56.980]   So looking farther into the future, as you said,
[00:58:56.980 --> 00:58:59.080]   most of us are not able to predict much.
[00:58:59.080 --> 00:59:02.560]   So when shrouded in such mystery, there's a concern of,
[00:59:02.560 --> 00:59:05.100]   well, you start thinking about worst case.
[00:59:05.100 --> 00:59:09.180]   Is that something that occupies your mind space much?
[00:59:09.180 --> 00:59:11.460]   - So I certainly think about threats.
[00:59:11.460 --> 00:59:16.460]   I think about dangers, and I think any new technology
[00:59:16.460 --> 00:59:19.980]   has positives and negatives.
[00:59:19.980 --> 00:59:21.540]   And if it's a powerful technology,
[00:59:21.540 --> 00:59:23.860]   it can be used for bad as well as for good.
[00:59:23.860 --> 00:59:28.780]   So I'm certainly not worried about the robot apocalypse
[00:59:28.780 --> 00:59:32.700]   and the Terminator-type scenarios.
[00:59:32.700 --> 00:59:37.700]   I am worried about change in employment,
[00:59:37.700 --> 00:59:41.100]   and are we gonna be able to react fast enough
[00:59:41.100 --> 00:59:41.940]   to deal with that?
[00:59:41.940 --> 00:59:44.100]   I think we're already seeing it today,
[00:59:44.100 --> 00:59:46.580]   where a lot of people are disgruntled
[00:59:46.580 --> 00:59:50.260]   about the way income inequality is working,
[00:59:50.260 --> 00:59:53.380]   and automation could help accelerate
[00:59:53.380 --> 00:59:55.660]   those kinds of problems.
[00:59:55.660 --> 01:00:00.060]   I see powerful technologies can always be used as weapons,
[01:00:00.060 --> 01:00:03.500]   whether they're robots or drones or whatever.
[01:00:03.500 --> 01:00:06.220]   Some of that we're seeing due to AI.
[01:00:06.220 --> 01:00:08.380]   A lot of it, you don't need AI.
[01:00:09.580 --> 01:00:12.580]   And I don't know what's a worst threat,
[01:00:12.580 --> 01:00:14.380]   if it's an autonomous drone,
[01:00:14.380 --> 01:00:18.940]   or it's CRISPR technology becoming available,
[01:00:18.940 --> 01:00:21.420]   or we have lots of threats to face,
[01:00:21.420 --> 01:00:24.700]   and some of them involve AI and some of them don't.
[01:00:24.700 --> 01:00:27.340]   - So the threats that technology presents,
[01:00:27.340 --> 01:00:31.060]   are you, for the most part, optimistic about technology
[01:00:31.060 --> 01:00:34.380]   also alleviating those threats or creating new opportunities
[01:00:34.380 --> 01:00:38.340]   or protecting us from the more detrimental effects
[01:00:38.340 --> 01:00:39.180]   of these new technologies?
[01:00:39.180 --> 01:00:41.420]   - No, again, it's hard to predict the future.
[01:00:41.420 --> 01:00:45.420]   And as a society so far,
[01:00:45.420 --> 01:00:50.420]   we've survived nuclear bombs and other things.
[01:00:50.420 --> 01:00:53.660]   Of course, only societies that have survived
[01:00:53.660 --> 01:00:54.740]   are having this conversation.
[01:00:54.740 --> 01:00:59.220]   So maybe that's a survivorship bias there.
[01:00:59.220 --> 01:01:01.900]   - What problem stands out to you as exciting,
[01:01:01.900 --> 01:01:05.620]   challenging, impactful to work on in the near future
[01:01:05.620 --> 01:01:08.620]   for yourself, for the community, and broadly?
[01:01:09.340 --> 01:01:13.060]   - So we talked about these assistance in conversation.
[01:01:13.060 --> 01:01:14.940]   I think that's a great area.
[01:01:14.940 --> 01:01:19.940]   I think combining common sense reasoning
[01:01:19.940 --> 01:01:25.460]   with the power of data is a great area.
[01:01:25.460 --> 01:01:28.460]   - In which application, in conversation,
[01:01:28.460 --> 01:01:29.380]   or just broadly speaking?
[01:01:29.380 --> 01:01:31.060]   - Just in general, yeah.
[01:01:31.060 --> 01:01:35.500]   As a programmer, I'm interested in programming tools,
[01:01:35.500 --> 01:01:39.900]   both in terms of the current systems we have today
[01:01:39.900 --> 01:01:41.700]   with TensorFlow and so on.
[01:01:41.700 --> 01:01:43.460]   Can we make them much easier to use
[01:01:43.460 --> 01:01:46.100]   for a broader class of people?
[01:01:46.100 --> 01:01:49.380]   And also, can we apply machine learning
[01:01:49.380 --> 01:01:52.420]   to the more traditional type of programming?
[01:01:52.420 --> 01:01:57.420]   So when you go to Google and you type in a query
[01:01:57.420 --> 01:02:00.620]   and you spell something wrong, it says, "Did you mean?"
[01:02:00.620 --> 01:02:01.940]   And the reason we're able to do that
[01:02:01.940 --> 01:02:04.500]   is 'cause lots of other people made a similar error
[01:02:04.500 --> 01:02:06.620]   and then they corrected it.
[01:02:06.620 --> 01:02:09.300]   We should be able to go into our code bases
[01:02:09.300 --> 01:02:12.700]   and our bug fix bases, and when I type a line of code,
[01:02:12.700 --> 01:02:15.220]   it should be able to say, "Did you mean such and such?"
[01:02:15.220 --> 01:02:17.580]   If you type this today, you're probably gonna
[01:02:17.580 --> 01:02:19.720]   type in this bug fix tomorrow.
[01:02:19.720 --> 01:02:22.700]   - Yeah, that's a really exciting application
[01:02:22.700 --> 01:02:27.700]   of almost an assistant for the coding programming experience
[01:02:27.700 --> 01:02:29.420]   at every level.
[01:02:29.420 --> 01:02:34.420]   So I think I could safely speak for the entire AI community
[01:02:35.300 --> 01:02:36.900]   first of all, for thanking you
[01:02:36.900 --> 01:02:38.800]   for the amazing work you've done,
[01:02:38.800 --> 01:02:40.620]   certainly for the amazing work you've done
[01:02:40.620 --> 01:02:43.380]   with AI, a modern approach book.
[01:02:43.380 --> 01:02:45.260]   I think we're all looking forward very much
[01:02:45.260 --> 01:02:48.340]   for the fourth edition and then the fifth edition and so on.
[01:02:48.340 --> 01:02:51.460]   So Peter, thank you so much for talking today.
[01:02:51.460 --> 01:02:52.860]   - Yeah, thank you, pleasure.
[01:02:52.860 --> 01:02:55.440]   (upbeat music)
[01:02:55.440 --> 01:02:58.020]   (upbeat music)
[01:02:58.020 --> 01:03:00.600]   (upbeat music)
[01:03:00.600 --> 01:03:03.180]   (upbeat music)
[01:03:03.180 --> 01:03:05.760]   (upbeat music)
[01:03:05.760 --> 01:03:08.340]   (upbeat music)
[01:03:08.340 --> 01:03:18.340]   [BLANK_AUDIO]

