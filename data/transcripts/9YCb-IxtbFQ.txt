
[00:00:00.000 --> 00:00:06.200]   Welcome back everyone.
[00:00:06.200 --> 00:00:09.180]   This is part 3 in our series on information retrieval.
[00:00:09.180 --> 00:00:12.080]   In part 2, we talked about classical IR models.
[00:00:12.080 --> 00:00:13.740]   I hope that gave you a sense for how
[00:00:13.740 --> 00:00:15.840]   IR systems work and we're now in
[00:00:15.840 --> 00:00:18.140]   a good position to think about how to evaluate them.
[00:00:18.140 --> 00:00:20.660]   That is the topic of IR metrics.
[00:00:20.660 --> 00:00:23.200]   Right at the start, I want to emphasize that there are
[00:00:23.200 --> 00:00:27.560]   many ways in which we can assess the quality of IR systems.
[00:00:27.560 --> 00:00:29.760]   Of course, inevitably, we'll end up
[00:00:29.760 --> 00:00:31.880]   focused on accuracy style metrics.
[00:00:31.880 --> 00:00:33.880]   They're prominent in the literature and they
[00:00:33.880 --> 00:00:36.580]   are an important aspect of system quality.
[00:00:36.580 --> 00:00:39.120]   But there are many other things we should think about for
[00:00:39.120 --> 00:00:41.980]   IR systems particularly when they are deployed.
[00:00:41.980 --> 00:00:44.740]   For example, in industrial context,
[00:00:44.740 --> 00:00:47.240]   latency is often incredibly important.
[00:00:47.240 --> 00:00:50.920]   Latency is the time it takes to execute a single query.
[00:00:50.920 --> 00:00:52.600]   In industrial context,
[00:00:52.600 --> 00:00:55.320]   latency constraints are often very tight.
[00:00:55.320 --> 00:00:58.880]   Users expect low latency systems and as a result,
[00:00:58.880 --> 00:01:00.920]   high latency systems are basically
[00:01:00.920 --> 00:01:03.600]   non-starters no matter how accurate they are.
[00:01:03.600 --> 00:01:06.160]   You often see accuracy and latency
[00:01:06.160 --> 00:01:10.160]   paired as crucial aspects of system performance.
[00:01:10.160 --> 00:01:11.720]   Throughput is similar.
[00:01:11.720 --> 00:01:15.140]   Throughput is the total queries served in a fixed time,
[00:01:15.140 --> 00:01:17.400]   maybe via batch processing.
[00:01:17.400 --> 00:01:19.040]   That's related to latency,
[00:01:19.040 --> 00:01:20.900]   but it could trade off against latency.
[00:01:20.900 --> 00:01:24.740]   You might decide to sacrifice some per query speed
[00:01:24.740 --> 00:01:28.280]   in order to process batches of examples efficiently.
[00:01:28.280 --> 00:01:31.000]   Whether you favor latency or throughput might
[00:01:31.000 --> 00:01:34.440]   depend on how users interact with your system.
[00:01:34.440 --> 00:01:37.400]   Flops is a hardware agnostic measure
[00:01:37.400 --> 00:01:39.360]   of total compute resources.
[00:01:39.360 --> 00:01:41.680]   This could be a good holistic measure.
[00:01:41.680 --> 00:01:45.920]   It might be hard to measure and hard to reason about,
[00:01:45.920 --> 00:01:48.240]   but it could be a good summary number.
[00:01:48.240 --> 00:01:50.980]   But we might want to break it down into component pieces.
[00:01:50.980 --> 00:01:53.360]   For example, disk usage for
[00:01:53.360 --> 00:01:55.120]   your model or for your index.
[00:01:55.120 --> 00:01:56.680]   That could be an important cost,
[00:01:56.680 --> 00:01:58.660]   especially for our index.
[00:01:58.660 --> 00:02:00.560]   If we're going to index the entire web,
[00:02:00.560 --> 00:02:03.080]   then the cost of storing our index on disk could be
[00:02:03.080 --> 00:02:04.800]   very large and we need to think about
[00:02:04.800 --> 00:02:07.560]   that as a component of system quality.
[00:02:07.560 --> 00:02:10.120]   For modern systems, maybe more pressing would be
[00:02:10.120 --> 00:02:13.040]   memory usage again for the model or the index.
[00:02:13.040 --> 00:02:15.340]   If we need to hold the entire index in
[00:02:15.340 --> 00:02:18.120]   memory to have a low latency system,
[00:02:18.120 --> 00:02:20.020]   that could get very expensive,
[00:02:20.020 --> 00:02:22.360]   very fast, for example.
[00:02:22.360 --> 00:02:26.640]   We could think about cost as a way to summarize all of
[00:02:26.640 --> 00:02:30.760]   2-6 in a way that gets us thinking holistically
[00:02:30.760 --> 00:02:32.580]   about the system and about trade-offs
[00:02:32.580 --> 00:02:34.320]   that are inherent in these metrics.
[00:02:34.320 --> 00:02:38.080]   For example, if we want a really low latency system,
[00:02:38.080 --> 00:02:40.320]   we might need to hold everything in memory,
[00:02:40.320 --> 00:02:43.440]   and that could get very expensive, very fast.
[00:02:43.440 --> 00:02:45.160]   But we could decide, for example,
[00:02:45.160 --> 00:02:46.440]   that we're going to cut costs by
[00:02:46.440 --> 00:02:48.840]   making our system smaller overall,
[00:02:48.840 --> 00:02:52.040]   but that could lead to a sacrifice in accuracy.
[00:02:52.040 --> 00:02:53.440]   So forth and so on.
[00:02:53.440 --> 00:02:55.180]   Given a cost constraint,
[00:02:55.180 --> 00:02:57.280]   we'll start thinking about trade-offs
[00:02:57.280 --> 00:02:59.440]   in a way that seems very healthy.
[00:02:59.440 --> 00:03:03.280]   I think more IR evaluation should be in that holistic mode,
[00:03:03.280 --> 00:03:06.080]   balancing all of these considerations relative
[00:03:06.080 --> 00:03:07.980]   to the interest that we have
[00:03:07.980 --> 00:03:10.840]   and the constraints that we're operating under.
[00:03:10.840 --> 00:03:12.960]   All that said though,
[00:03:12.960 --> 00:03:15.360]   we are now about to focus maybe too
[00:03:15.360 --> 00:03:18.320]   obsessively on accuracy style metrics.
[00:03:18.320 --> 00:03:19.600]   Let's dive into that.
[00:03:19.600 --> 00:03:21.560]   As a preliminary, we should talk about
[00:03:21.560 --> 00:03:23.120]   the kinds of datasets that
[00:03:23.120 --> 00:03:24.960]   you're likely to have at your disposal.
[00:03:24.960 --> 00:03:26.800]   They're a little bit different from the ones
[00:03:26.800 --> 00:03:28.760]   that we're accustomed to in NLP,
[00:03:28.760 --> 00:03:30.660]   so this is worth a review.
[00:03:30.660 --> 00:03:34.720]   Given a query queue and a collection of N documents D,
[00:03:34.720 --> 00:03:37.080]   one data type that you might have would be
[00:03:37.080 --> 00:03:39.440]   a complete partial gold ranking of
[00:03:39.440 --> 00:03:42.160]   all the documents with respect to your query,
[00:03:42.160 --> 00:03:43.560]   and you would need such rankings for
[00:03:43.560 --> 00:03:45.640]   every query in your dataset.
[00:03:45.640 --> 00:03:48.320]   That would obviously be inordinately
[00:03:48.320 --> 00:03:51.240]   expensive to do with all human labeling.
[00:03:51.240 --> 00:03:54.600]   Most likely, if you have a dataset like this,
[00:03:54.600 --> 00:03:56.640]   the rankings were automatically
[00:03:56.640 --> 00:03:59.040]   generated via some process.
[00:03:59.040 --> 00:04:00.800]   It might be that you have
[00:04:00.800 --> 00:04:02.760]   an incomplete partial ranking
[00:04:02.760 --> 00:04:05.160]   of the documents with respect to queries.
[00:04:05.160 --> 00:04:07.080]   That might be that via some heuristics,
[00:04:07.080 --> 00:04:11.080]   some documents were presented to humans who then ranked them,
[00:04:11.080 --> 00:04:13.680]   maybe just a handful of them for each query.
[00:04:13.680 --> 00:04:15.240]   That could be the basis for
[00:04:15.240 --> 00:04:18.000]   inferring automatically a total ranking,
[00:04:18.000 --> 00:04:20.000]   but there will be some noise in
[00:04:20.000 --> 00:04:23.120]   this process that goes beyond being strictly gold labels,
[00:04:23.120 --> 00:04:25.280]   or you'll have only partial labels
[00:04:25.280 --> 00:04:27.200]   and need to think about that.
[00:04:27.200 --> 00:04:31.040]   Another common labeling in this space is to simply have
[00:04:31.040 --> 00:04:34.280]   binary judgments for whether or not documents in
[00:04:34.280 --> 00:04:37.600]   our corpus are relevant or not to the given query.
[00:04:37.600 --> 00:04:39.600]   That's very common to see.
[00:04:39.600 --> 00:04:41.760]   This could be based on human labeling,
[00:04:41.760 --> 00:04:43.200]   but it could also be based on
[00:04:43.200 --> 00:04:44.800]   a weak supervision heuristic.
[00:04:44.800 --> 00:04:47.320]   For example, whether each document
[00:04:47.320 --> 00:04:49.820]   contains the query that we're interested in,
[00:04:49.820 --> 00:04:51.360]   as a substring.
[00:04:51.360 --> 00:04:53.320]   That would obviously be very noisy,
[00:04:53.320 --> 00:04:54.840]   but we found in practice that
[00:04:54.840 --> 00:04:57.400]   that weak supervision heuristic can be
[00:04:57.400 --> 00:05:01.640]   powerful when it comes to training good IR systems.
[00:05:01.640 --> 00:05:05.440]   Then maybe the most relevant data type for us as we think
[00:05:05.440 --> 00:05:07.120]   about neural IR systems in
[00:05:07.120 --> 00:05:10.040]   particular is the one given in item 4 here.
[00:05:10.040 --> 00:05:12.400]   Here we have a tuple consisting of
[00:05:12.400 --> 00:05:14.920]   one positive document for our query,
[00:05:14.920 --> 00:05:18.400]   and one or more negative documents for our query.
[00:05:18.400 --> 00:05:20.380]   That can be a device for both training
[00:05:20.380 --> 00:05:24.180]   IR systems and for evaluating them.
[00:05:24.180 --> 00:05:27.160]   With those data types in place,
[00:05:27.160 --> 00:05:29.380]   let's start to think about the metrics themselves.
[00:05:29.380 --> 00:05:31.100]   We'll start with the simplest ones,
[00:05:31.100 --> 00:05:33.840]   which are success and reciprocal rank.
[00:05:33.840 --> 00:05:35.740]   A common ingredient for both of
[00:05:35.740 --> 00:05:38.020]   them is what I've called rank here.
[00:05:38.020 --> 00:05:40.740]   For a ranking D of our documents,
[00:05:40.740 --> 00:05:45.020]   we say that the rank for a query in that ranking is an integer,
[00:05:45.020 --> 00:05:46.500]   and that is the position of
[00:05:46.500 --> 00:05:50.640]   the first relevant document for the query in our ranking,
[00:05:50.640 --> 00:05:51.900]   the first one.
[00:05:51.900 --> 00:05:54.960]   On that basis, we can define success at k.
[00:05:54.960 --> 00:05:56.580]   We pick some value k,
[00:05:56.580 --> 00:05:59.460]   and then we say for our query and our ranking,
[00:05:59.460 --> 00:06:01.840]   the value for success at k is one.
[00:06:01.840 --> 00:06:05.540]   If the rank for the query in our ranking is less than or equal to
[00:06:05.540 --> 00:06:09.380]   k, otherwise zero, so a binary judgment.
[00:06:09.380 --> 00:06:12.380]   Then the reciprocal rank is similar.
[00:06:12.380 --> 00:06:16.060]   Rr at k for a query in a document ranking is
[00:06:16.060 --> 00:06:19.100]   one over the rank for that query in the ranking.
[00:06:19.100 --> 00:06:23.220]   If the rank is less than or equal to k, otherwise zero.
[00:06:23.220 --> 00:06:27.100]   This is identical to success except where success we have one,
[00:06:27.100 --> 00:06:30.160]   now we have one over the actual rank value,
[00:06:30.160 --> 00:06:35.340]   but we map to zero all cases where rank is below k.
[00:06:35.340 --> 00:06:39.220]   Then Mrr at k is a common metric that you see in the literature,
[00:06:39.220 --> 00:06:41.340]   and that's simply the average over
[00:06:41.340 --> 00:06:45.860]   multiple queries for the Rr at k values.
[00:06:45.860 --> 00:06:48.060]   Let's get a deeper feel for
[00:06:48.060 --> 00:06:50.220]   these metrics by looking at some examples,
[00:06:50.220 --> 00:06:53.060]   and I'm going to use these rankings as running examples.
[00:06:53.060 --> 00:06:56.460]   Here's the first one. This is a ranking of six documents
[00:06:56.460 --> 00:06:59.040]   relative to some fixed query q,
[00:06:59.040 --> 00:07:01.220]   and a star indicates that
[00:07:01.220 --> 00:07:04.300]   the document was judged relevant to the query.
[00:07:04.300 --> 00:07:07.760]   In this ranking, the first two are considered relevant,
[00:07:07.760 --> 00:07:11.420]   and the final one in the ranking is also considered relevant.
[00:07:11.420 --> 00:07:13.020]   Here's a second ranking.
[00:07:13.020 --> 00:07:15.100]   In this ranking, again, it's the same three stars,
[00:07:15.100 --> 00:07:17.220]   but they appear in different positions now.
[00:07:17.220 --> 00:07:20.020]   The first relevant document is at position 2,
[00:07:20.020 --> 00:07:23.300]   and the other two are in positions 5 and 6.
[00:07:23.300 --> 00:07:25.420]   Then for document ranking 3, again,
[00:07:25.420 --> 00:07:26.980]   same three stars, but now they're at
[00:07:26.980 --> 00:07:30.100]   positions 3, 4, and 5 in the ranking.
[00:07:30.100 --> 00:07:32.300]   Before we even think about metrics,
[00:07:32.300 --> 00:07:34.500]   you might step back and ask yourselves,
[00:07:34.500 --> 00:07:37.980]   which one of these rankings is the best?
[00:07:37.980 --> 00:07:39.860]   It might not be so obvious,
[00:07:39.860 --> 00:07:41.380]   it might depend on perspective.
[00:07:41.380 --> 00:07:44.300]   For example, document ranking 1
[00:07:44.300 --> 00:07:46.540]   looks really good because the first two
[00:07:46.540 --> 00:07:49.020]   documents in the ranking are relevant.
[00:07:49.020 --> 00:07:51.780]   However, it has the third star
[00:07:51.780 --> 00:07:54.020]   all the way down in last place.
[00:07:54.020 --> 00:07:56.660]   Whereas, just for a comparison,
[00:07:56.660 --> 00:07:58.480]   if you look at ranking 3,
[00:07:58.480 --> 00:08:02.140]   all of its stars are low in positions 3, 4, and 5,
[00:08:02.140 --> 00:08:05.860]   but at least it didn't put any of them in last place.
[00:08:05.860 --> 00:08:07.620]   Then document ranking 2 might look
[00:08:07.620 --> 00:08:10.700]   intermediate between those two extremes.
[00:08:10.700 --> 00:08:12.900]   Obviously, different metrics will be
[00:08:12.900 --> 00:08:16.340]   sensitive to different notions of quality in that sense,
[00:08:16.340 --> 00:08:19.140]   and it might be hard to decide a priori,
[00:08:19.140 --> 00:08:21.700]   which quality we're actually seeking.
[00:08:21.700 --> 00:08:23.740]   It's going to be all about trade-offs and
[00:08:23.740 --> 00:08:27.300]   reflecting on those high-level considerations.
[00:08:27.300 --> 00:08:30.220]   Let's return to success and reciprocal rank.
[00:08:30.220 --> 00:08:33.020]   How do they do? Success at 2 for
[00:08:33.020 --> 00:08:35.860]   document ranking 1 given our fixed query is 1,
[00:08:35.860 --> 00:08:38.980]   and that's because there is some relevant document
[00:08:38.980 --> 00:08:41.860]   at or above position 2 in this ranking.
[00:08:41.860 --> 00:08:43.540]   It doesn't really matter in this case
[00:08:43.540 --> 00:08:45.000]   that there are two of them.
[00:08:45.000 --> 00:08:46.980]   Same thing for ranking 2.
[00:08:46.980 --> 00:08:48.860]   We get a value of 1 because there is
[00:08:48.860 --> 00:08:51.940]   a star at or above position 2.
[00:08:51.940 --> 00:08:55.900]   Whereas, ranking 3 gives us success at 2 of 0,
[00:08:55.900 --> 00:08:58.180]   and that's because there are no stars
[00:08:58.180 --> 00:09:01.380]   at or above position 2 in the ranking.
[00:09:01.380 --> 00:09:03.860]   The reciprocal rank values at 2
[00:09:03.860 --> 00:09:06.180]   will be similar but a little bit more nuanced.
[00:09:06.180 --> 00:09:08.700]   The RR at 2 for the first ranking is 1,
[00:09:08.700 --> 00:09:11.020]   and that's because it's 1 over 1,
[00:09:11.020 --> 00:09:17.060]   so we have our first relevant document in position 1.
[00:09:17.060 --> 00:09:20.740]   The denominator there is the rank.
[00:09:20.740 --> 00:09:22.380]   Whereas, for ranking 2,
[00:09:22.380 --> 00:09:24.060]   it's 1 over 2, and that's because
[00:09:24.060 --> 00:09:28.100]   the first relevant document now is in position 2 here.
[00:09:28.100 --> 00:09:30.140]   Whereas, for document 3,
[00:09:30.140 --> 00:09:33.220]   as before, we get an RR at 2 of 0,
[00:09:33.220 --> 00:09:37.420]   because there are no stars at or above 2 in the ranking.
[00:09:37.420 --> 00:09:40.540]   Let's move now to precision and recall.
[00:09:40.540 --> 00:09:42.420]   These are classic IR metrics,
[00:09:42.420 --> 00:09:45.820]   and they're going to be more nuanced than success and RR,
[00:09:45.820 --> 00:09:49.100]   because they are going to be sensitive to multiple stars.
[00:09:49.100 --> 00:09:50.740]   For success and RR,
[00:09:50.740 --> 00:09:52.620]   we really only cared about one star,
[00:09:52.620 --> 00:09:54.120]   whereas now we're going to care about
[00:09:54.120 --> 00:09:56.180]   the full set of them that we have.
[00:09:56.180 --> 00:09:58.100]   Two preliminary concepts.
[00:09:58.100 --> 00:10:01.860]   First, the return set for a ranking at value k,
[00:10:01.860 --> 00:10:05.820]   is the set of documents in the ranking at or above k.
[00:10:05.820 --> 00:10:08.780]   The relevant set for a query given
[00:10:08.780 --> 00:10:10.940]   a document ranking is simply the set of
[00:10:10.940 --> 00:10:13.900]   all documents that are relevant to the query.
[00:10:13.900 --> 00:10:16.580]   That is all the ones in my notation that have stars
[00:10:16.580 --> 00:10:19.740]   attached to them anywhere in the ranking.
[00:10:19.740 --> 00:10:24.420]   Then we can define precision at a chosen value k,
[00:10:24.420 --> 00:10:25.900]   the precision at k.
[00:10:25.900 --> 00:10:28.740]   Here the numerator is the return set
[00:10:28.740 --> 00:10:30.940]   intersected with the relevant set,
[00:10:30.940 --> 00:10:34.260]   and the denominator is the value k.
[00:10:34.260 --> 00:10:36.980]   This is intuitive in terms of precision.
[00:10:36.980 --> 00:10:39.500]   If we think about the values at or above k
[00:10:39.500 --> 00:10:41.100]   as the guesses that we made,
[00:10:41.100 --> 00:10:43.100]   with precision we're saying how many of
[00:10:43.100 --> 00:10:45.620]   those guesses were good ones.
[00:10:45.620 --> 00:10:47.980]   Recall is a dual of that.
[00:10:47.980 --> 00:10:50.540]   Recall has the same denominator.
[00:10:50.540 --> 00:10:53.860]   The recall at k is the return set
[00:10:53.860 --> 00:10:55.540]   intersected with the relevant set,
[00:10:55.540 --> 00:10:56.900]   the number of those,
[00:10:56.900 --> 00:10:58.700]   divided by, in this case,
[00:10:58.700 --> 00:11:01.260]   the number of relevant documents.
[00:11:01.260 --> 00:11:03.480]   This is like saying,
[00:11:03.480 --> 00:11:07.160]   if the set of things at or above k are our guesses,
[00:11:07.160 --> 00:11:09.640]   how many of the relevant ones actually
[00:11:09.640 --> 00:11:12.400]   burbled up to be at or above k,
[00:11:12.400 --> 00:11:15.000]   dual of precision.
[00:11:15.000 --> 00:11:17.360]   Let's see how these values play out in
[00:11:17.360 --> 00:11:18.920]   terms of our three rankings.
[00:11:18.920 --> 00:11:20.560]   We'll do precision first.
[00:11:20.560 --> 00:11:24.200]   Precision at two in our first ranking is two out of two,
[00:11:24.200 --> 00:11:26.840]   because we set k at or above
[00:11:26.840 --> 00:11:30.440]   two and both of those documents are relevant.
[00:11:30.440 --> 00:11:32.440]   For ranking two, it's one out of two,
[00:11:32.440 --> 00:11:34.960]   because again, we have two documents in
[00:11:34.960 --> 00:11:38.560]   our denominator and only one in our numerator,
[00:11:38.560 --> 00:11:40.760]   only one relevant document there.
[00:11:40.760 --> 00:11:42.720]   Whereas for ranking three,
[00:11:42.720 --> 00:11:45.840]   precision at two is zero out of two.
[00:11:45.840 --> 00:11:47.780]   Let's look at the recall.
[00:11:47.780 --> 00:11:50.640]   The recall at two for our first ranking is two out of three.
[00:11:50.640 --> 00:11:52.320]   Recall the denominator there is three,
[00:11:52.320 --> 00:11:54.060]   because there are three relevant documents,
[00:11:54.060 --> 00:11:57.600]   three stars, and two of them are at or above k.
[00:11:57.600 --> 00:11:59.000]   That's quite an impressive value,
[00:11:59.000 --> 00:12:00.520]   it's like the max there.
[00:12:00.520 --> 00:12:03.040]   For document ranking two, it's one out of three.
[00:12:03.040 --> 00:12:04.920]   Again, we have three relevant documents in
[00:12:04.920 --> 00:12:08.920]   the denominator and only one of them is at or above k.
[00:12:08.920 --> 00:12:11.840]   Then finally, as you could predict for ranking three,
[00:12:11.840 --> 00:12:12.920]   it's zero out of three,
[00:12:12.920 --> 00:12:17.400]   because there are no relevant documents at or above two here.
[00:12:17.400 --> 00:12:21.400]   That reproduces more or less the ranking that we saw
[00:12:21.400 --> 00:12:25.400]   for success and reciprocal rank.
[00:12:25.400 --> 00:12:26.800]   But here's a twist.
[00:12:26.800 --> 00:12:30.440]   Suppose we set our value of k to five,
[00:12:30.440 --> 00:12:32.280]   whereas before it was two.
[00:12:32.280 --> 00:12:35.320]   Now we get this set of precision and recall values.
[00:12:35.320 --> 00:12:38.880]   The noteworthy thing here is that having set it at five,
[00:12:38.880 --> 00:12:42.400]   document ranking three is now clearly in the lead.
[00:12:42.400 --> 00:12:46.320]   It's in the lead because it didn't put anything in position six.
[00:12:46.320 --> 00:12:49.440]   I got precision at five out of three out of five,
[00:12:49.440 --> 00:12:52.280]   and it got recall at five of three out of three,
[00:12:52.280 --> 00:12:54.320]   whereas both of these are less good.
[00:12:54.320 --> 00:12:59.320]   That shows you how important the value of k was to
[00:12:59.320 --> 00:13:02.040]   our overall assessment of quality.
[00:13:02.040 --> 00:13:06.000]   We should think when we use these metrics about what we're doing when we
[00:13:06.000 --> 00:13:10.920]   set k and how it might affect our assessment of ranking quality.
[00:13:10.920 --> 00:13:14.880]   Average precision is a nice alternative because it's
[00:13:14.880 --> 00:13:19.160]   less sensitive to the value of k and we just saw how impactful that can be.
[00:13:19.160 --> 00:13:21.960]   Average precision for a query relative to
[00:13:21.960 --> 00:13:25.880]   a document ranking is intuitively spelled out like this.
[00:13:25.880 --> 00:13:27.360]   For the numerator, we're going to get
[00:13:27.360 --> 00:13:32.520]   precision values for every step where there is a relevant document,
[00:13:32.520 --> 00:13:34.360]   every place where there is a star,
[00:13:34.360 --> 00:13:36.160]   and we sum those up.
[00:13:36.160 --> 00:13:40.920]   Then the denominator is the set of relevant documents.
[00:13:40.920 --> 00:13:42.600]   Here's how this plays out.
[00:13:42.600 --> 00:13:45.840]   Again, using those same three rankings that we had before,
[00:13:45.840 --> 00:13:50.720]   we have relevant documents for ranking one at one, two, and six.
[00:13:50.720 --> 00:13:53.520]   Those are the three precision values that we check,
[00:13:53.520 --> 00:13:58.080]   and we simply sum those up to get two out of five over three.
[00:13:58.080 --> 00:14:00.160]   For document ranking two,
[00:14:00.160 --> 00:14:01.760]   we sum up the same things,
[00:14:01.760 --> 00:14:03.680]   but now at positions two, five,
[00:14:03.680 --> 00:14:07.240]   and six, and we get 1.4 divided by three.
[00:14:07.240 --> 00:14:12.960]   Then for D3, we have relevant documents at three, four, and five.
[00:14:12.960 --> 00:14:16.760]   Those are our precision values and they sum to 1.43.
[00:14:16.760 --> 00:14:21.320]   This is noteworthy because document ranking one is the clear winner,
[00:14:21.320 --> 00:14:24.120]   even though we have no sensitivity to K anymore.
[00:14:24.120 --> 00:14:29.360]   But in fact, ranking three inched ahead of ranking two in this metric,
[00:14:29.360 --> 00:14:32.080]   which is something that we hadn't really seen before,
[00:14:32.080 --> 00:14:36.240]   except for that one case where we set K low for precision and recall.
[00:14:36.240 --> 00:14:39.920]   This is nice because we don't have the sensitivity to K anymore,
[00:14:39.920 --> 00:14:43.600]   we've abstracted over all the different values we could have chosen.
[00:14:43.600 --> 00:14:45.520]   We still have this numerator here,
[00:14:45.520 --> 00:14:49.320]   keeping track of the number of relevant documents.
[00:14:50.440 --> 00:14:53.360]   That's a sampling of common metrics.
[00:14:53.360 --> 00:14:54.560]   There are of course many more,
[00:14:54.560 --> 00:14:56.400]   but they'll follow similar patterns.
[00:14:56.400 --> 00:14:58.080]   Let's step back here and ask,
[00:14:58.080 --> 00:15:00.080]   which metric should you be using?
[00:15:00.080 --> 00:15:02.800]   You can probably anticipate my answer at this point.
[00:15:02.800 --> 00:15:04.440]   There is no single answer.
[00:15:04.440 --> 00:15:06.360]   Let's just think about some trade-offs.
[00:15:06.360 --> 00:15:12.000]   For you, is the cost of scrolling through K passages low for your users say,
[00:15:12.000 --> 00:15:15.640]   or for you, then maybe success at K is fine-grained enough,
[00:15:15.640 --> 00:15:20.120]   because all you need to do is find a relevant one in that set of K documents.
[00:15:20.120 --> 00:15:23.800]   Are there multiple relevant documents per query?
[00:15:23.800 --> 00:15:27.760]   If so, success and reciprocal rank are probably going to be
[00:15:27.760 --> 00:15:30.080]   too coarse-grained because they're not sensitive to
[00:15:30.080 --> 00:15:33.200]   having these multiple relevant documents.
[00:15:33.200 --> 00:15:37.160]   Is it more important to find every relevant document?
[00:15:37.160 --> 00:15:39.200]   If so, favor recall.
[00:15:39.200 --> 00:15:42.720]   Is it more important to review only relevant documents?
[00:15:42.720 --> 00:15:44.800]   If so, favor precision.
[00:15:44.800 --> 00:15:48.560]   This would be a case where the cost of missing something is high,
[00:15:48.560 --> 00:15:51.080]   but the cost of review is low.
[00:15:51.080 --> 00:15:54.240]   Down here, maybe the cost of review could be high,
[00:15:54.240 --> 00:15:56.560]   but we don't really pay too much if we miss things.
[00:15:56.560 --> 00:15:58.960]   We just need to find a few relevant exemplars,
[00:15:58.960 --> 00:16:02.160]   and so we can favor precision in that case.
[00:16:02.160 --> 00:16:07.640]   F1 at K is the harmonic mean of the precision and recall values at K,
[00:16:07.640 --> 00:16:10.760]   and that can be used where there are multiple relevant documents,
[00:16:10.760 --> 00:16:14.480]   but their relative order above K doesn't matter that much,
[00:16:14.480 --> 00:16:18.440]   and so you've decided to balance precision and recall.
[00:16:18.440 --> 00:16:22.240]   Average precision of all the metrics I showed you will give
[00:16:22.240 --> 00:16:26.040]   the finest grain distinctions of all the metrics that we discussed.
[00:16:26.040 --> 00:16:30.280]   It's sensitive to rank and precision and recall.
[00:16:30.280 --> 00:16:33.160]   If you don't have very much information and you would like to make
[00:16:33.160 --> 00:16:36.640]   fine-grained distinctions among different rankings you've predicted,
[00:16:36.640 --> 00:16:40.360]   average precision may be a very good choice.
[00:16:40.360 --> 00:16:43.800]   But as I said at the start,
[00:16:43.800 --> 00:16:46.000]   I would love for us to break out of
[00:16:46.000 --> 00:16:48.760]   the mold of thinking only about accuracy.
[00:16:48.760 --> 00:16:50.760]   To get us thinking in that direction,
[00:16:50.760 --> 00:16:53.800]   what I have here is what we call a synthetic leaderboard
[00:16:53.800 --> 00:16:56.280]   in this paper that we recently completed.
[00:16:56.280 --> 00:16:58.800]   We just assembled from the literature a lot of
[00:16:58.800 --> 00:17:03.160]   different systems and tried to figure out from the papers what they had done in
[00:17:03.160 --> 00:17:08.320]   terms of hardware, accuracy, your MRR, and latency.
[00:17:08.320 --> 00:17:11.320]   If you zoom in on the MRR column,
[00:17:11.320 --> 00:17:12.960]   that is the accuracy column,
[00:17:12.960 --> 00:17:15.600]   I think it emerges that you should pick
[00:17:15.600 --> 00:17:19.320]   one of these Colbert systems or this one here.
[00:17:19.320 --> 00:17:21.800]   But for example,
[00:17:21.800 --> 00:17:28.440]   the Colbert systems to achieve that MRR need pretty heavy-duty hardware for GPUs.
[00:17:28.440 --> 00:17:32.000]   Whereas if we go up to some of these splayed systems,
[00:17:32.000 --> 00:17:34.680]   they're comparable in terms of quality,
[00:17:34.680 --> 00:17:39.040]   but they have fewer resources that they require in terms of hardware.
[00:17:39.040 --> 00:17:41.160]   Then within those splayed systems,
[00:17:41.160 --> 00:17:45.360]   you can start to think about the trade-offs relative to hardware and latency,
[00:17:45.360 --> 00:17:48.360]   and that again gets you thinking in a totally new way
[00:17:48.360 --> 00:17:51.240]   about which of these systems is the best.
[00:17:51.240 --> 00:17:53.520]   That's the thinking that I want to encourage.
[00:17:53.520 --> 00:17:54.960]   Here's another perspective on this.
[00:17:54.960 --> 00:17:56.960]   This is also from that same paper.
[00:17:56.960 --> 00:18:03.120]   We've got cost along the x-axis here and a measure of accuracy along the y-axis.
[00:18:03.120 --> 00:18:05.600]   This is BM25 down at the bottom.
[00:18:05.600 --> 00:18:07.520]   It's very inexpensive,
[00:18:07.520 --> 00:18:10.920]   but it is very ineffective as well.
[00:18:10.920 --> 00:18:13.720]   For essentially the same cost,
[00:18:13.720 --> 00:18:16.360]   you could have a huge jump in your accuracy,
[00:18:16.360 --> 00:18:22.520]   in your MRR, if you just pick that small BT splayed system that's given in green.
[00:18:22.520 --> 00:18:24.680]   You could go up a little bit yet again with
[00:18:24.680 --> 00:18:27.320]   a slight cost increase by picking the medium model,
[00:18:27.320 --> 00:18:30.280]   and then even just a slight cost increase after that,
[00:18:30.280 --> 00:18:33.200]   and you get the BT splayed large model.
[00:18:33.200 --> 00:18:37.040]   Huge jump in performance with hardly any cost increase.
[00:18:37.040 --> 00:18:41.200]   Whereas correspondingly, this picture makes it seem like you wouldn't pick
[00:18:41.200 --> 00:18:45.120]   this ANTS system because it's both expensive and
[00:18:45.120 --> 00:18:49.200]   less good in terms of accuracy than some of these more inexpensive systems.
[00:18:49.200 --> 00:18:53.080]   More generally, this is sometimes called the Pareto frontier of systems.
[00:18:53.080 --> 00:18:55.400]   Given the two things we've decided to measure,
[00:18:55.400 --> 00:18:59.320]   some systems are strictly dominating some other systems.
[00:18:59.320 --> 00:19:01.760]   But of course, there may be other dimensions to
[00:19:01.760 --> 00:19:04.200]   this that would cause ANTS to pull ahead,
[00:19:04.200 --> 00:19:06.840]   and so we need to think holistically again.
[00:19:06.840 --> 00:19:09.800]   This is just reinforcing the notion that there isn't
[00:19:09.800 --> 00:19:12.720]   one fixed answer to the question of system quality.
[00:19:12.720 --> 00:19:17.680]   There are lots of dimensions and lots of trade-offs to consider ultimately.
[00:19:17.680 --> 00:19:27.680]   [BLANK_AUDIO]

