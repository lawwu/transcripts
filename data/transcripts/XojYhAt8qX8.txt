
[00:00:00.000 --> 00:00:04.400]   the idea up there on Twitter about a hugging face course
[00:00:04.400 --> 00:00:05.760]   with the Vasti I've been,
[00:00:05.760 --> 00:00:08.320]   and I probably got the most responses
[00:00:08.320 --> 00:00:10.380]   out of anything I've ever put on Twitter.
[00:00:10.380 --> 00:00:13.120]   And in addition to myself,
[00:00:13.120 --> 00:00:16.200]   I'm glad to also be joined by the Avengers.
[00:00:16.200 --> 00:00:17.920]   I don't think I could be Captain America
[00:00:17.920 --> 00:00:20.080]   'cause I haven't really been lifting the weight.
[00:00:20.080 --> 00:00:21.760]   So that's probably Zach,
[00:00:21.760 --> 00:00:26.080]   but hopefully I can at least be Thor or at worst the Hulk.
[00:00:26.080 --> 00:00:28.640]   So there's a lot of people on this call.
[00:00:28.640 --> 00:00:33.640]   So if you have questions as we go through the material today
[00:00:33.640 --> 00:00:36.080]   just feel free to put them on there.
[00:00:36.080 --> 00:00:37.800]   I'm gonna stop periodically.
[00:00:37.800 --> 00:00:40.240]   Sonja I'm gonna be answering questions
[00:00:40.240 --> 00:00:41.640]   as well as hopefully other folks.
[00:00:41.640 --> 00:00:44.240]   And if there's any outstanding questions,
[00:00:44.240 --> 00:00:48.240]   we'll answer them as we hit every individual section.
[00:00:48.240 --> 00:00:51.520]   And also thanks to the folks at Weights and Biases,
[00:00:51.520 --> 00:00:54.000]   Andrea and Morgan,
[00:00:54.000 --> 00:00:56.720]   who I also know from the Vasti I community.
[00:00:56.720 --> 00:01:00.480]   I appreciate you all handing all the logistics and whatnot.
[00:01:00.480 --> 00:01:01.960]   So let's begin.
[00:01:01.960 --> 00:01:06.960]   So this week we'll start by looking at section zero and one
[00:01:06.960 --> 00:01:10.720]   and section zero is pretty simple as the setup.
[00:01:10.720 --> 00:01:13.520]   And really our goal here is to make sure
[00:01:13.520 --> 00:01:17.680]   that you can go through and run the examples in CoLab.
[00:01:17.680 --> 00:01:21.520]   And then the section one is probably going to be
[00:01:21.520 --> 00:01:25.480]   the least fast AI-ish part of the study group
[00:01:25.480 --> 00:01:27.520]   since it's kind of just covering the basics
[00:01:27.520 --> 00:01:30.920]   of how the transformer architecture works
[00:01:30.920 --> 00:01:35.640]   and what type of NLP tasks it's suited for
[00:01:35.640 --> 00:01:37.240]   and how they work.
[00:01:37.240 --> 00:01:40.720]   And so we're gonna look at their high level API
[00:01:40.720 --> 00:01:43.480]   which is the pipeline API.
[00:01:43.480 --> 00:01:47.040]   And we'll see how we can actually just start using models
[00:01:47.040 --> 00:01:50.720]   in Hugging Face within a few lines of code with raw text
[00:01:50.720 --> 00:01:53.600]   to get predictions for a variety of tasks.
[00:01:53.600 --> 00:01:56.560]   And then next week we'll actually spend some time
[00:01:56.560 --> 00:01:59.000]   looking at Blur's high level API.
[00:01:59.000 --> 00:02:03.360]   And I cross it out because it's actually more for training
[00:02:03.360 --> 00:02:05.320]   than just inference, whereas the pipeline API
[00:02:05.320 --> 00:02:06.720]   we're just looking at inference.
[00:02:06.720 --> 00:02:09.120]   So we're looking at pre-trained models that,
[00:02:09.120 --> 00:02:11.000]   or not pre-trained models,
[00:02:11.000 --> 00:02:13.480]   but models that folks have put on the model hub
[00:02:13.480 --> 00:02:14.720]   or Hugging Face has put up there
[00:02:14.720 --> 00:02:16.080]   or models that you've trained.
[00:02:16.080 --> 00:02:20.120]   How you can use the pipeline API to do inference on that.
[00:02:20.120 --> 00:02:23.240]   And then we'll also have some resources and homework.
[00:02:23.240 --> 00:02:25.040]   The homework is optional,
[00:02:25.040 --> 00:02:28.000]   but I hope you'll be excited to do it anyways.
[00:02:28.000 --> 00:02:30.480]   I guarantee if you do do the homework,
[00:02:30.480 --> 00:02:33.600]   it will help you understand so much more
[00:02:33.600 --> 00:02:36.200]   about how all the transformers work.
[00:02:36.200 --> 00:02:37.240]   And there's a lot of them.
[00:02:37.240 --> 00:02:39.680]   So when you go to the Hugging Face documentation,
[00:02:39.680 --> 00:02:41.880]   you've probably seen, it used to be like
[00:02:41.880 --> 00:02:44.120]   maybe 10 transformer architectures.
[00:02:44.120 --> 00:02:45.680]   Now I don't even know how many are on there.
[00:02:45.680 --> 00:02:48.720]   It's like 50 or it's just like an endless list
[00:02:48.720 --> 00:02:49.960]   you scroll through.
[00:02:49.960 --> 00:02:52.880]   And understanding the transformer architecture,
[00:02:52.880 --> 00:02:53.720]   the core architecture
[00:02:53.720 --> 00:02:55.160]   that all of these things were based on
[00:02:55.160 --> 00:02:57.920]   will help you know which ones to
[00:02:57.920 --> 00:02:59.320]   at least narrow your choices down
[00:02:59.320 --> 00:03:03.000]   depending on your task and how things work.
[00:03:03.000 --> 00:03:05.120]   So we'll look at some homework there
[00:03:05.120 --> 00:03:07.000]   and hopefully everybody will do it.
[00:03:07.000 --> 00:03:10.360]   First off, some resources.
[00:03:10.360 --> 00:03:15.320]   If you're here, you've obviously found a resource number one,
[00:03:15.320 --> 00:03:16.840]   but if you want to share that with folks
[00:03:16.840 --> 00:03:19.520]   who may want to join in over the next four weeks,
[00:03:19.520 --> 00:03:21.200]   that's the link to the registration page
[00:03:21.200 --> 00:03:22.280]   for the SETI group.
[00:03:23.040 --> 00:03:27.080]   We also have a SETI group Discord
[00:03:27.080 --> 00:03:28.440]   that's hosted by Fast.ai.
[00:03:28.440 --> 00:03:29.720]   And so the links here.
[00:03:29.720 --> 00:03:33.840]   In addition, I will share my slide deck.
[00:03:33.840 --> 00:03:37.120]   So don't feel the need to write all this stuff down
[00:03:37.120 --> 00:03:38.000]   real quickly.
[00:03:38.000 --> 00:03:41.560]   This will be shared and folks will be able
[00:03:41.560 --> 00:03:44.200]   to have these links at their disposal.
[00:03:44.200 --> 00:03:49.040]   Next we have Fast.ai resources.
[00:03:49.040 --> 00:03:52.080]   And so I think people in the SETI group
[00:03:52.080 --> 00:03:53.560]   are at a variety of levels.
[00:03:53.560 --> 00:03:56.640]   Some have been doing Fast.ai for far too long, like myself,
[00:03:56.640 --> 00:03:59.360]   ever since it was taught in Theano, believe it or not.
[00:03:59.360 --> 00:04:02.240]   And others are new to the course
[00:04:02.240 --> 00:04:06.920]   and are maybe doing the reading SETI group with Amon
[00:04:06.920 --> 00:04:11.160]   in the Waitspice's Fastbooks reading SETI group.
[00:04:11.160 --> 00:04:13.800]   But these are some really great resources.
[00:04:13.800 --> 00:04:17.560]   It's first the course and it's free, it's online.
[00:04:17.560 --> 00:04:19.960]   Absolutely the best deep learning course.
[00:04:19.960 --> 00:04:22.240]   If you wanna get involved and learn
[00:04:22.240 --> 00:04:25.080]   how to actually apply deep learning and how it works
[00:04:25.080 --> 00:04:27.680]   and how to build models, how to put them in production,
[00:04:27.680 --> 00:04:29.960]   I think the Fast.ai course is the best.
[00:04:29.960 --> 00:04:32.840]   And also a tremendous community.
[00:04:32.840 --> 00:04:35.080]   And a testament to that are a lot of the folks
[00:04:35.080 --> 00:04:38.000]   that are on this call.
[00:04:38.000 --> 00:04:39.840]   Most of them, probably all of them,
[00:04:39.840 --> 00:04:42.240]   except for Sonia because of the coffee comment
[00:04:42.240 --> 00:04:43.680]   are probably smarter than me.
[00:04:43.680 --> 00:04:47.200]   And so a lot of great folks on here
[00:04:47.200 --> 00:04:52.200]   who have become exceptional machine learning
[00:04:52.200 --> 00:04:55.080]   and deep learning practitioners through the course
[00:04:55.080 --> 00:04:57.120]   over the last few years.
[00:04:57.120 --> 00:04:59.560]   Second is the Walk with Fast.ai course
[00:04:59.560 --> 00:05:02.920]   that Zach has up there and that's all free as well.
[00:05:02.920 --> 00:05:05.680]   And that's a great compendium to the course
[00:05:05.680 --> 00:05:09.080]   and it answers a lot of questions and expands on some ideas
[00:05:09.080 --> 00:05:10.360]   that come up in the course
[00:05:10.360 --> 00:05:12.160]   but aren't necessarily covered in there.
[00:05:12.160 --> 00:05:14.640]   So that's a great resource as well.
[00:05:14.640 --> 00:05:16.960]   And then of course the Fastbook
[00:05:16.960 --> 00:05:19.080]   and the Fastbook Reading Group.
[00:05:19.080 --> 00:05:23.360]   So you can get this book online,
[00:05:23.360 --> 00:05:25.200]   you can purchase the hard copy
[00:05:25.200 --> 00:05:29.040]   or Jeremy and Sylvan have made it available for free
[00:05:29.040 --> 00:05:31.320]   via Jupyter Notebooks.
[00:05:31.320 --> 00:05:32.560]   And if you just do a Google search,
[00:05:32.560 --> 00:05:35.800]   you should be able to find that.
[00:05:35.800 --> 00:05:38.040]   And then of course there's the Fastbook Reading Group
[00:05:38.040 --> 00:05:41.200]   and I participated, I've been on there a few times
[00:05:41.200 --> 00:05:42.840]   just watching and observing
[00:05:42.840 --> 00:05:44.680]   and that's actually a great resource.
[00:05:44.680 --> 00:05:46.000]   Amin does a really great job
[00:05:46.000 --> 00:05:48.240]   of going through all the chapters
[00:05:48.240 --> 00:05:51.120]   and I think his goal is to go through the entire book.
[00:05:51.120 --> 00:05:52.760]   So check that out.
[00:05:52.760 --> 00:05:56.000]   In terms of Fast.ai and Hugging Face Libraries,
[00:05:56.000 --> 00:05:58.920]   if there's more, let us know and we'll add those
[00:05:58.920 --> 00:06:01.520]   but these are the three that I'm familiar with.
[00:06:01.520 --> 00:06:04.800]   The first is Adapt NLP and that's by Novetta
[00:06:04.800 --> 00:06:08.080]   and that's what Zach Mueller is heading up.
[00:06:08.080 --> 00:06:10.040]   And then really, I think it really wasn't me
[00:06:10.040 --> 00:06:11.160]   that started with Blur.
[00:06:11.160 --> 00:06:13.200]   It's the Fast Hugs by Morgan.
[00:06:13.200 --> 00:06:17.160]   He was like the first one I think put Fast Hugs up there.
[00:06:17.160 --> 00:06:18.800]   Great name by the way.
[00:06:18.800 --> 00:06:23.480]   And I think Ardo now is heading that up
[00:06:23.480 --> 00:06:25.600]   but Fast Hugs is kind of like,
[00:06:25.600 --> 00:06:29.440]   at least for me was my initial inspiration for like,
[00:06:29.440 --> 00:06:32.480]   oh yeah, we could actually do something with Fast.ai
[00:06:32.480 --> 00:06:35.040]   and not just with sequence classification
[00:06:35.040 --> 00:06:38.840]   which is where I think Fast Hugs was focused on initially
[00:06:38.840 --> 00:06:42.240]   but actually started applying it to the other NLP tasks.
[00:06:42.240 --> 00:06:45.640]   So, you know, summarization, translation, everything.
[00:06:45.640 --> 00:06:47.640]   And so third is Blur.
[00:06:47.640 --> 00:06:49.200]   That's the library I created.
[00:06:49.200 --> 00:06:50.640]   I'm still working on it.
[00:06:50.640 --> 00:06:55.640]   I think it's like version 0.26 or something.
[00:06:55.640 --> 00:06:57.680]   I don't even know.
[00:06:57.680 --> 00:07:00.120]   But speaking for myself,
[00:07:00.120 --> 00:07:01.880]   I would love to get more people involved.
[00:07:01.880 --> 00:07:02.800]   So check it out.
[00:07:02.800 --> 00:07:04.840]   There's documentation, the code's there.
[00:07:04.840 --> 00:07:06.560]   You can email me.
[00:07:06.560 --> 00:07:08.240]   I would love to get people involved
[00:07:08.240 --> 00:07:09.440]   in helping build this out.
[00:07:09.440 --> 00:07:12.680]   And that would be a great way to also develop
[00:07:12.680 --> 00:07:15.200]   your understanding and learning of how transformers work
[00:07:15.200 --> 00:07:16.760]   and how Fast.ai works.
[00:07:16.760 --> 00:07:20.200]   Then ML and data science in general,
[00:07:20.200 --> 00:07:25.200]   we got the Chai Time Data Science Podcast by Sanyam.
[00:07:25.200 --> 00:07:28.440]   And one of these days we maybe have
[00:07:28.440 --> 00:07:33.000]   the Dark Roasted Coffee Time Data Science Podcast
[00:07:33.000 --> 00:07:34.440]   but for right now it's Chai.
[00:07:34.440 --> 00:07:36.280]   So, but check that out.
[00:07:36.280 --> 00:07:38.680]   And Sanyam interviews tons of people in the community
[00:07:38.680 --> 00:07:42.440]   and they're really great, really great insights
[00:07:42.440 --> 00:07:44.120]   into not only what's happening
[00:07:44.120 --> 00:07:46.000]   in the realm of machine learning,
[00:07:46.000 --> 00:07:47.840]   but how people got started,
[00:07:47.840 --> 00:07:50.320]   how they developed their careers and whatnot.
[00:07:50.320 --> 00:07:52.640]   So that's a great podcast.
[00:07:52.640 --> 00:07:55.680]   And then of course we got Weights and Biases
[00:07:55.680 --> 00:07:57.080]   who are hosting this.
[00:07:57.080 --> 00:07:59.480]   And for all things in terms of tracking
[00:07:59.480 --> 00:08:01.240]   your machine learning projects and whatnot,
[00:08:01.240 --> 00:08:05.120]   it's super easy to integrate their product
[00:08:05.120 --> 00:08:07.640]   into any type of machine learning
[00:08:07.640 --> 00:08:09.280]   or deep learning project you have.
[00:08:09.280 --> 00:08:13.160]   And they got tons of like free content and whatnot.
[00:08:13.160 --> 00:08:15.440]   And there's folks on this call as well
[00:08:15.440 --> 00:08:17.960]   if you wanna get more information,
[00:08:17.960 --> 00:08:19.880]   could provide that for you.
[00:08:19.880 --> 00:08:23.800]   So let's get started.
[00:08:23.800 --> 00:08:26.520]   So number one, setting up your environment.
[00:08:26.520 --> 00:08:28.760]   You'll notice that when you go through the course,
[00:08:28.760 --> 00:08:31.040]   there's a collab button at the top
[00:08:31.040 --> 00:08:32.840]   of a lot of the course pages.
[00:08:32.920 --> 00:08:35.400]   And you should be able to open that or click on that
[00:08:35.400 --> 00:08:39.520]   and it will open that page with the code in collab.
[00:08:39.520 --> 00:08:43.520]   But I noticed that sometimes it's missing dependencies.
[00:08:43.520 --> 00:08:48.840]   And so if you want things to work,
[00:08:48.840 --> 00:08:51.600]   this is the right PIP install right here
[00:08:51.600 --> 00:08:52.720]   that you wanna include.
[00:08:52.720 --> 00:08:55.840]   If you wanna do transformers with sentence space.
[00:08:55.840 --> 00:08:58.400]   I think by default, it just does PIP install transformers
[00:08:58.400 --> 00:09:00.960]   and then some of the examples will break.
[00:09:00.960 --> 00:09:04.400]   So for collab, make sure that you use this PIP install
[00:09:04.400 --> 00:09:05.800]   and we'll actually look at a notebook.
[00:09:05.800 --> 00:09:08.520]   You can see how I set it up.
[00:09:08.520 --> 00:09:12.080]   And if you're going to local virtual environment front,
[00:09:12.080 --> 00:09:16.520]   I typically use Conda or mini Conda.
[00:09:16.520 --> 00:09:20.320]   And then Jeremy Howard recommended to several of us
[00:09:20.320 --> 00:09:24.440]   a product called Mamba that just makes Conda work fast.
[00:09:24.440 --> 00:09:26.960]   So I've been using Mamba
[00:09:26.960 --> 00:09:29.640]   and essentially it's just a fill in for Conda.
[00:09:29.640 --> 00:09:32.640]   So wherever you put Conda install, you put Mamba install
[00:09:32.640 --> 00:09:34.640]   and it is a lot faster.
[00:09:34.640 --> 00:09:37.040]   So check out those resources
[00:09:37.040 --> 00:09:39.280]   if you wanna go the local route.
[00:09:39.280 --> 00:09:44.280]   And then also Fast.ai has a Conda channel
[00:09:44.280 --> 00:09:47.000]   for installing all things Fast.ai.
[00:09:47.000 --> 00:09:49.400]   And so the link is right here.
[00:09:49.400 --> 00:09:51.000]   I've been using it for Blur
[00:09:51.000 --> 00:09:54.680]   and it does make things simple along with Mamba
[00:09:54.680 --> 00:09:58.040]   and really fast to keep updated with everything.
[00:09:58.040 --> 00:09:59.800]   And I'll also mention,
[00:09:59.800 --> 00:10:02.440]   'cause there's a lot of discussion about using Poetry
[00:10:02.440 --> 00:10:04.640]   to manage your environments.
[00:10:04.640 --> 00:10:05.760]   I've never used it.
[00:10:05.760 --> 00:10:07.880]   So if there's folks that are using Poetry
[00:10:07.880 --> 00:10:10.440]   that like to share example with the group,
[00:10:10.440 --> 00:10:13.080]   I know personally, I would love to see
[00:10:13.080 --> 00:10:17.400]   a Poetry configuration set up with Fast.ai and Hugging Face
[00:10:17.400 --> 00:10:19.960]   to see how that works and the benefits.
[00:10:19.960 --> 00:10:20.920]   So I hear a lot about it,
[00:10:20.920 --> 00:10:24.160]   but I don't know anything from personal experience.
[00:10:24.160 --> 00:10:27.640]   Any questions coming up yet, Sonia?
[00:10:28.640 --> 00:10:30.640]   - No, I don't see any questions.
[00:10:30.640 --> 00:10:34.760]   There was one broadly that asked you,
[00:10:34.760 --> 00:10:38.320]   should they be doing fast, the Hugging Face API course
[00:10:38.320 --> 00:10:40.280]   or is it better to start from scratch?
[00:10:40.280 --> 00:10:46.160]   - The Hugging Face course or-
[00:10:46.160 --> 00:10:49.480]   - Hugging Face course or just learn from scratch?
[00:10:49.480 --> 00:10:50.840]   I think that's the question.
[00:10:50.840 --> 00:10:53.320]   - I think the Hugging Face course is worth doing
[00:10:53.320 --> 00:10:54.680]   and maybe even for this,
[00:10:54.680 --> 00:10:58.560]   'cause especially as we get into week three,
[00:10:58.560 --> 00:11:00.960]   we're gonna be looking at more Fast.ai code.
[00:11:00.960 --> 00:11:03.040]   So we're gonna deviate from the course content.
[00:11:03.040 --> 00:11:04.480]   So I think it's actually good
[00:11:04.480 --> 00:11:07.320]   if folks can do the course during the week,
[00:11:07.320 --> 00:11:10.360]   at least watch the videos and do a cursory reading
[00:11:10.360 --> 00:11:12.080]   so that they have the background
[00:11:12.080 --> 00:11:14.680]   for what we're gonna be covering in the study group.
[00:11:14.680 --> 00:11:19.000]   - Yep, no other questions, that's it.
[00:11:19.000 --> 00:11:22.240]   - All right, so essentially the course
[00:11:22.240 --> 00:11:24.400]   that Hugging Face has put on, it's really great.
[00:11:24.400 --> 00:11:28.640]   It has a lot of like great folks with video content.
[00:11:28.640 --> 00:11:31.120]   They're actually doing some study groups as well.
[00:11:31.120 --> 00:11:32.280]   And the initial course,
[00:11:32.280 --> 00:11:35.200]   they've only introduced this part, the introduction.
[00:11:35.200 --> 00:11:39.960]   And so there's four sections on transformer models,
[00:11:39.960 --> 00:11:41.920]   using transformers, fine tuning
[00:11:41.920 --> 00:11:44.160]   and sharing models and tokenizers.
[00:11:44.160 --> 00:11:48.440]   This is the only part of the course that is out now.
[00:11:48.440 --> 00:11:51.080]   I think the diving in portion comes in the fall
[00:11:51.080 --> 00:11:55.040]   and the advances either late this year or early next year.
[00:11:55.040 --> 00:11:58.760]   But we're gonna see what this first four week session,
[00:11:58.760 --> 00:12:01.560]   there's so much that you could actually get going
[00:12:01.560 --> 00:12:03.200]   in terms of building transformers,
[00:12:03.200 --> 00:12:06.120]   using them in production and also sharing them
[00:12:06.120 --> 00:12:09.080]   with the community through the Hugging Face Hub.
[00:12:09.080 --> 00:12:10.840]   And in particular, over the next few weeks,
[00:12:10.840 --> 00:12:13.880]   we're gonna show how to actually fine tune
[00:12:13.880 --> 00:12:18.000]   these models using Blur, Adapt NLP, Fast Hugs
[00:12:18.000 --> 00:12:19.400]   and then how to take those models
[00:12:19.400 --> 00:12:21.440]   and actually share them on the Hub,
[00:12:21.440 --> 00:12:23.640]   create a model cart, all that stuff.
[00:12:23.640 --> 00:12:25.760]   So I'm hoping by week four,
[00:12:25.760 --> 00:12:28.360]   a lot of folks are training models
[00:12:28.360 --> 00:12:30.040]   and sharing them on the Hub
[00:12:30.040 --> 00:12:34.080]   and contributing to the community.
[00:12:34.080 --> 00:12:35.880]   It should be pretty straightforward.
[00:12:35.880 --> 00:12:39.600]   All right, so the first section
[00:12:39.600 --> 00:12:42.160]   is we've got natural language processing.
[00:12:42.160 --> 00:12:44.520]   Essentially NLP is a field of linguistics
[00:12:44.520 --> 00:12:46.520]   and machine learning focused on understanding
[00:12:46.520 --> 00:12:49.240]   everything related to human language.
[00:12:49.240 --> 00:12:52.840]   And this is really a tough problem
[00:12:52.840 --> 00:12:54.440]   in terms of machine learning
[00:12:54.440 --> 00:12:58.680]   because ML models only understand numbers.
[00:12:58.680 --> 00:13:03.680]   So how do we use numbers to create some type of system
[00:13:03.680 --> 00:13:08.040]   that can understand sentiment or toxicity
[00:13:08.040 --> 00:13:10.480]   or summarize or answer questions?
[00:13:10.480 --> 00:13:14.360]   And so it's a really tough area in machine learning.
[00:13:14.360 --> 00:13:16.400]   And some of the common tasks
[00:13:16.400 --> 00:13:19.880]   that we'll be going through over the next four weeks
[00:13:19.880 --> 00:13:22.280]   are sequence classification tasks.
[00:13:22.280 --> 00:13:25.200]   So that's where we're looking at whole documents,
[00:13:25.200 --> 00:13:30.200]   for example, movie reviews or comments, for example,
[00:13:30.200 --> 00:13:33.000]   and we're trying to classify them,
[00:13:33.000 --> 00:13:35.440]   things like sentiment, whether it's spam,
[00:13:35.440 --> 00:13:37.680]   whether it's grammatically correct,
[00:13:37.680 --> 00:13:41.120]   whether two sentences are logically related.
[00:13:41.120 --> 00:13:41.960]   And we're also talking
[00:13:41.960 --> 00:13:44.320]   about multi-label classification problems.
[00:13:44.320 --> 00:13:48.200]   So on Kaggle, there's a jigsaw toxicity challenge
[00:13:48.200 --> 00:13:51.680]   where there's I think seven or eight levels of toxicity
[00:13:51.680 --> 00:13:54.360]   and you wanna predict for every document
[00:13:54.360 --> 00:13:58.880]   whether it actually has none, one,
[00:13:58.880 --> 00:14:02.440]   or more of those labels or should be associated with it.
[00:14:02.440 --> 00:14:07.520]   Another task as common as token classification.
[00:14:07.520 --> 00:14:10.080]   And here we're talking about things
[00:14:10.080 --> 00:14:12.040]   like named entity recognition,
[00:14:12.040 --> 00:14:14.280]   where we got a text document
[00:14:14.280 --> 00:14:17.160]   and we wanna find where all the people's names mentioned,
[00:14:17.160 --> 00:14:19.440]   where all the geographical locations mentioned,
[00:14:19.440 --> 00:14:20.880]   where our organizations mentioned,
[00:14:20.880 --> 00:14:23.120]   we wanna identify those entities.
[00:14:23.120 --> 00:14:27.040]   So we're looking there to identify and classify tokens.
[00:14:27.040 --> 00:14:31.960]   Also, we have text generation, which is complete,
[00:14:31.960 --> 00:14:34.520]   you have a prompt you start out with
[00:14:34.520 --> 00:14:36.560]   and then it generates a bunch of text
[00:14:36.560 --> 00:14:39.320]   that sometimes makes sense and sometimes doesn't.
[00:14:39.320 --> 00:14:43.280]   And a great example of this is the new GitHub autopilot,
[00:14:43.280 --> 00:14:45.400]   which is actually doing this for code.
[00:14:45.400 --> 00:14:49.120]   And it's like scary how good it is, really impressive stuff.
[00:14:49.120 --> 00:14:52.320]   And so that's looking there at text generation.
[00:14:52.320 --> 00:14:54.160]   And then another one that we'll also look at
[00:14:54.160 --> 00:14:57.080]   is extractive question and answering.
[00:14:57.080 --> 00:14:59.320]   And essentially what you do is you feed the model
[00:14:59.320 --> 00:15:02.600]   a context like an article and a question
[00:15:02.600 --> 00:15:07.600]   that that article has somewhere in there, the actual answer.
[00:15:08.040 --> 00:15:11.000]   And you want the model to provide that segment of text
[00:15:11.000 --> 00:15:14.360]   in the article or context that is the correct answer.
[00:15:14.360 --> 00:15:17.280]   So these are really kind of common tasks
[00:15:17.280 --> 00:15:21.080]   and deep learning is where the state of the art is.
[00:15:21.080 --> 00:15:23.560]   And there's a lot you can just accomplish
[00:15:23.560 --> 00:15:27.160]   in terms of these things using the Transformer
[00:15:27.160 --> 00:15:29.520]   or the Transformers library from Hugging Face.
[00:15:29.520 --> 00:15:33.720]   So the Transformers library provides functionality
[00:15:33.720 --> 00:15:36.680]   to create and use these shared models.
[00:15:36.680 --> 00:15:39.960]   The model hub contains thousands of pre-trained models
[00:15:39.960 --> 00:15:41.800]   that folks can download and use.
[00:15:41.800 --> 00:15:43.840]   We'll take a look at that.
[00:15:43.840 --> 00:15:44.880]   It's got all kinds of filters.
[00:15:44.880 --> 00:15:48.680]   You can filter by task, by language, et cetera.
[00:15:48.680 --> 00:15:53.680]   And then right now we'll take a look at the pipeline API.
[00:15:53.680 --> 00:15:58.080]   And essentially the pipeline API connects a model
[00:15:58.080 --> 00:16:02.800]   with the necessary pre-processing and post-processing steps
[00:16:02.800 --> 00:16:05.360]   allowing us to directly input any texts
[00:16:05.360 --> 00:16:08.000]   and get an intelligible answer.
[00:16:08.000 --> 00:16:09.680]   And so what the pipeline does
[00:16:09.680 --> 00:16:12.240]   is it's going to take the raw text.
[00:16:12.240 --> 00:16:16.360]   It's going to do a process of tokenization
[00:16:16.360 --> 00:16:18.240]   that actually includes tokenization,
[00:16:18.240 --> 00:16:21.200]   which is splitting the text into individual words
[00:16:21.200 --> 00:16:26.200]   and numericalization, which is for each of those words,
[00:16:26.200 --> 00:16:29.600]   it has a vocab, which is essentially a lookup list.
[00:16:29.600 --> 00:16:34.600]   So it will convert every one of those words to a ID
[00:16:34.640 --> 00:16:36.760]   in that vocab list.
[00:16:36.760 --> 00:16:39.920]   And that is the numerical representation
[00:16:39.920 --> 00:16:43.640]   that we start with to feed the text into our models.
[00:16:43.640 --> 00:16:45.480]   And we'll look at how that works.
[00:16:45.480 --> 00:16:50.000]   Then it runs it through those post-processed inputs
[00:16:50.000 --> 00:16:51.120]   into the model.
[00:16:51.120 --> 00:16:53.400]   And then afterwards you get predictions.
[00:16:53.400 --> 00:16:56.280]   It cleans those up so that they actually look
[00:16:56.280 --> 00:16:59.080]   understandable to us humans.
[00:16:59.080 --> 00:17:02.960]   So we'll take a look at how that works right now.
[00:17:03.800 --> 00:17:08.800]   And hopefully this is big enough for folks.
[00:17:08.800 --> 00:17:11.760]   If not, let me know.
[00:17:11.760 --> 00:17:14.720]   I can make my screen bigger.
[00:17:14.720 --> 00:17:19.600]   So in terms of installing in Colab,
[00:17:19.600 --> 00:17:22.480]   if you put this a little dash QQ,
[00:17:22.480 --> 00:17:24.800]   you won't get all that output
[00:17:24.800 --> 00:17:28.440]   that usually just clutters your screen.
[00:17:28.440 --> 00:17:31.440]   So here I'm actually installing
[00:17:31.440 --> 00:17:34.720]   both the Transformers library with sentence piece
[00:17:34.720 --> 00:17:36.800]   and also the data sets library.
[00:17:36.800 --> 00:17:39.640]   And then I'm gonna import the Transformers pipeline.
[00:17:39.640 --> 00:17:43.560]   And we were talking about a moment ago about tokenization.
[00:17:43.560 --> 00:17:46.240]   So I wanted to just go over that real briefly,
[00:17:46.240 --> 00:17:48.040]   what that looks like.
[00:17:48.040 --> 00:17:50.200]   So in the first example up here,
[00:17:50.200 --> 00:17:54.200]   I'm going to use the BERT tokenizer.
[00:17:54.200 --> 00:17:56.960]   And I just wanted to show you that
[00:17:58.400 --> 00:18:01.880]   if you print out the type, I see BERT tokenizer, right?
[00:18:01.880 --> 00:18:03.720]   And if I print tokenizer,
[00:18:03.720 --> 00:18:06.440]   you can actually see there's some really helpful information
[00:18:06.440 --> 00:18:09.080]   that HuggingFace has when you look at the
[00:18:09.080 --> 00:18:12.080]   string representation of that object,
[00:18:12.080 --> 00:18:15.400]   such as the vocab, which is how many tokens are in there,
[00:18:15.400 --> 00:18:17.520]   the max length, et cetera.
[00:18:17.520 --> 00:18:21.920]   But the recommended way to actually build
[00:18:21.920 --> 00:18:24.960]   HuggingFace objects, including the tokenizer
[00:18:24.960 --> 00:18:28.120]   is by using these auto models.
[00:18:28.120 --> 00:18:31.160]   So there's auto tokenizer, there's auto config,
[00:18:31.160 --> 00:18:35.600]   then there's auto model, there's auto model for task,
[00:18:35.600 --> 00:18:37.560]   and that's what they recommend.
[00:18:37.560 --> 00:18:41.320]   And it will automatically infer the correct objects
[00:18:41.320 --> 00:18:46.320]   based on the model name or path that you type in here.
[00:18:46.320 --> 00:18:50.720]   So you can see this code right here is,
[00:18:50.720 --> 00:18:53.800]   this code right here is essentially the same
[00:18:53.800 --> 00:18:55.960]   except I'm using auto tokenizer,
[00:18:55.960 --> 00:19:00.040]   and it knows to create a BERT tokenizer,
[00:19:00.040 --> 00:19:03.360]   and you can see it's the same information as before.
[00:19:03.360 --> 00:19:04.720]   So when you're working with HuggingFace,
[00:19:04.720 --> 00:19:09.160]   you typically wanna use these auto tokenizer,
[00:19:09.160 --> 00:19:11.520]   auto config, auto model for
[00:19:11.520 --> 00:19:13.560]   when building your HuggingFace objects.
[00:19:13.560 --> 00:19:19.560]   So as I said, when the tokenizer runs on raw text,
[00:19:19.560 --> 00:19:22.000]   it actually is doing more than tokenization,
[00:19:22.000 --> 00:19:24.880]   it's also doing numericalization,
[00:19:24.880 --> 00:19:28.760]   which is converting each of the tokens to an ID
[00:19:28.760 --> 00:19:31.560]   that can be looked up in a vocab.
[00:19:31.560 --> 00:19:35.360]   And so here I am tokenizing this text,
[00:19:35.360 --> 00:19:37.680]   and if we look at the type of inputs,
[00:19:37.680 --> 00:19:39.680]   it's a type batch encoding,
[00:19:39.680 --> 00:19:44.320]   we can convert that to a dictionary by calling .data.
[00:19:44.320 --> 00:19:45.440]   And if we look at it,
[00:19:45.440 --> 00:19:49.000]   we can see that it's giving us a dictionary
[00:19:49.000 --> 00:19:50.600]   of a couple of things.
[00:19:50.600 --> 00:19:52.920]   So first are the input IDs.
[00:19:52.920 --> 00:19:56.600]   So these are the actual indices into the vocab
[00:19:56.600 --> 00:19:57.640]   for the tokens.
[00:19:57.640 --> 00:20:00.560]   In the case of BERT,
[00:20:00.560 --> 00:20:04.600]   it's also giving us token type IDs,
[00:20:04.600 --> 00:20:06.840]   and that is because BERT was trained
[00:20:06.840 --> 00:20:08.840]   on a next sentence prediction task.
[00:20:08.840 --> 00:20:11.200]   So you had two sentences come in,
[00:20:11.200 --> 00:20:13.840]   and the token type IDs would be zero
[00:20:13.840 --> 00:20:16.040]   for the first sentence tokens,
[00:20:16.040 --> 00:20:18.280]   and then one for the second sentence tokens.
[00:20:18.280 --> 00:20:20.200]   So it was a way for the model to know,
[00:20:20.200 --> 00:20:23.600]   to be able to differentiate between the two sentences.
[00:20:23.600 --> 00:20:27.080]   And then we'll talk about this a little bit more,
[00:20:27.080 --> 00:20:29.120]   but there's an attention mask.
[00:20:29.120 --> 00:20:31.360]   And this is the key capability
[00:20:31.360 --> 00:20:33.120]   of the transformer architecture,
[00:20:33.120 --> 00:20:36.560]   is the ability to, as it learns to represent each token,
[00:20:36.560 --> 00:20:40.560]   to look at other tokens to help it learn that representation.
[00:20:40.560 --> 00:20:43.040]   And the attention mask is essentially a way,
[00:20:43.040 --> 00:20:46.680]   if it's one to say, yes, pay attention to those tokens.
[00:20:46.680 --> 00:20:50.040]   If there's a zero, let's say there's a zero right here,
[00:20:50.040 --> 00:20:53.040]   essentially it says, don't pay attention to token zero
[00:20:53.040 --> 00:20:54.040]   when you're trying to figure out
[00:20:54.040 --> 00:20:56.760]   how to represent these other tokens, ignore that one.
[00:20:56.760 --> 00:21:00.080]   And if we look at that vocab
[00:21:00.080 --> 00:21:01.680]   that I was just talking about,
[00:21:01.680 --> 00:21:02.680]   here's what it looks like.
[00:21:02.680 --> 00:21:06.160]   Here's, I just picked out the first five tokens.
[00:21:06.160 --> 00:21:09.360]   So there's the token, there's the ID.
[00:21:09.360 --> 00:21:11.000]   So if hey was up here,
[00:21:11.000 --> 00:21:16.040]   we would actually see token 16164
[00:21:16.040 --> 00:21:19.000]   in the input IDs right there.
[00:21:19.400 --> 00:21:23.280]   And once we, with those IDs,
[00:21:23.280 --> 00:21:24.640]   we can actually decode them.
[00:21:24.640 --> 00:21:29.440]   And you can see that a couple of other special tokens
[00:21:29.440 --> 00:21:32.200]   have been added to the text by the BERT tokenizer.
[00:21:32.200 --> 00:21:35.520]   There's a class token and a set token.
[00:21:35.520 --> 00:21:40.040]   And these are used by the model, right?
[00:21:40.040 --> 00:21:42.960]   And if you wanna look,
[00:21:42.960 --> 00:21:45.560]   and so when you actually decode the input IDs,
[00:21:45.560 --> 00:21:48.320]   it's going to include those special tokens.
[00:21:48.320 --> 00:21:51.640]   And these will vary depending on what architecture
[00:21:51.640 --> 00:21:53.320]   you're actually using.
[00:21:53.320 --> 00:21:57.920]   And we can also look at the actual tokens
[00:21:57.920 --> 00:22:01.920]   that the IDs represent.
[00:22:01.920 --> 00:22:05.040]   And so for most tokenizers,
[00:22:05.040 --> 00:22:06.560]   you're gonna see something like this,
[00:22:06.560 --> 00:22:09.160]   and you're gonna see characters like this
[00:22:09.160 --> 00:22:11.040]   because most tokenizers use something
[00:22:11.040 --> 00:22:13.200]   called subword tokenization.
[00:22:13.200 --> 00:22:15.880]   And so they have, by keeping a,
[00:22:15.880 --> 00:22:17.800]   they keep a relatively small vocab,
[00:22:17.800 --> 00:22:20.200]   which is important to model performance.
[00:22:20.200 --> 00:22:23.280]   And they also eliminate a lot of unknown tokens,
[00:22:23.280 --> 00:22:24.240]   which is bad, right?
[00:22:24.240 --> 00:22:27.600]   'Cause if you have tokens in there that are unknown,
[00:22:27.600 --> 00:22:29.400]   you can't really learn anything.
[00:22:29.400 --> 00:22:33.600]   And so it breaks things up into subwords
[00:22:33.600 --> 00:22:36.480]   for kind of rare words and very common words
[00:22:36.480 --> 00:22:39.560]   get their own token, like names of common word.
[00:22:39.560 --> 00:22:42.080]   Wade, especially with my spelling is not common.
[00:22:42.080 --> 00:22:44.280]   So it breaks it up with way,
[00:22:44.280 --> 00:22:47.200]   and then two hashtags and DE.
[00:22:48.000 --> 00:22:49.720]   So that's how the tokenizer works.
[00:22:49.720 --> 00:22:52.680]   Now, in terms of the pipeline,
[00:22:52.680 --> 00:22:59.560]   we can do a bunch of things with the models
[00:22:59.560 --> 00:23:03.040]   in the HuggingFace model hub.
[00:23:03.040 --> 00:23:08.040]   And just real quick, if you go to huggingface.co
[00:23:08.040 --> 00:23:12.360]   and go to models, this is the hub right here.
[00:23:12.360 --> 00:23:16.640]   We're doing a text classification.
[00:23:16.640 --> 00:23:19.960]   So you can click on that to filter by text classification.
[00:23:19.960 --> 00:23:22.360]   You can click one of these models,
[00:23:22.360 --> 00:23:24.480]   and it's gonna give you a bunch of information
[00:23:24.480 --> 00:23:27.120]   about how it was trained, for example,
[00:23:27.120 --> 00:23:29.000]   even through the inference API,
[00:23:29.000 --> 00:23:31.240]   you can actually test it out right here.
[00:23:31.240 --> 00:23:36.320]   And if you want to, if you find a model that's trained
[00:23:36.320 --> 00:23:39.760]   for exactly what you're doing,
[00:23:39.760 --> 00:23:41.280]   you can actually just click on here,
[00:23:41.280 --> 00:23:45.160]   using transformers and copy and paste this code,
[00:23:45.160 --> 00:23:46.520]   and you'll have everything you need
[00:23:46.520 --> 00:23:50.720]   in terms of the tokenizer and model.
[00:23:50.720 --> 00:23:53.840]   And probably the biggest thing to keep in mind
[00:23:53.840 --> 00:23:56.640]   is that when you're using the pipeline,
[00:23:56.640 --> 00:23:59.920]   these things aren't fine tuned on your particular tasks.
[00:23:59.920 --> 00:24:04.040]   So if you're using a model that's predicting two labels
[00:24:04.040 --> 00:24:06.600]   for sentiment, maybe positive or negative,
[00:24:06.600 --> 00:24:09.080]   and you're trying to train that on the toxicity
[00:24:09.080 --> 00:24:10.440]   of multi-label problem,
[00:24:10.440 --> 00:24:12.440]   those models aren't gonna work with the pipeline.
[00:24:12.440 --> 00:24:15.480]   You would actually have to fine tune that model
[00:24:15.480 --> 00:24:16.880]   on your particular task.
[00:24:16.880 --> 00:24:19.400]   That's something we'll look at in week three.
[00:24:19.400 --> 00:24:22.640]   But you can see it's pretty easy to actually
[00:24:22.640 --> 00:24:27.000]   just put the task, the raw test, the raw text,
[00:24:27.000 --> 00:24:30.960]   and get results for it.
[00:24:30.960 --> 00:24:32.480]   And then if you're interested,
[00:24:32.480 --> 00:24:37.480]   you can also look at what is the model being used here.
[00:24:37.480 --> 00:24:41.920]   So you can see, we just put a sentiment analysis, right?
[00:24:41.920 --> 00:24:46.920]   If we look at the classifier.model name or path,
[00:24:46.920 --> 00:24:50.760]   we can actually see that the architecture
[00:24:50.760 --> 00:24:54.120]   or the checkpoint being used is the distilled BERT
[00:24:54.120 --> 00:24:59.120]   based on case fine-tuned SST2 English architecture.
[00:24:59.120 --> 00:25:03.320]   And so if you're curious of what's being used.
[00:25:03.320 --> 00:25:04.840]   And as a side note,
[00:25:04.840 --> 00:25:08.320]   these distilled versions of BERT or Roberta
[00:25:08.320 --> 00:25:12.600]   are simply smaller versions of the original architecture.
[00:25:12.600 --> 00:25:14.120]   So they're faster
[00:25:14.120 --> 00:25:19.120]   and they only suffer minimal performance reduction.
[00:25:19.120 --> 00:25:21.840]   So for, depending on your constraints,
[00:25:21.840 --> 00:25:24.880]   like GPU or time constraints,
[00:25:24.880 --> 00:25:27.800]   these might be a really good option for experimentation.
[00:25:27.800 --> 00:25:29.800]   You might wanna look at the distilled versions
[00:25:29.800 --> 00:25:31.760]   of BERT or Roberta or whatever.
[00:25:31.760 --> 00:25:36.520]   And then also through the course,
[00:25:37.080 --> 00:25:40.800]   you learn that you can also say, yeah, I want this task,
[00:25:40.800 --> 00:25:43.880]   but let's say there's a specific model you wanna use.
[00:25:43.880 --> 00:25:45.920]   So like here, let's say I want to use
[00:25:45.920 --> 00:25:50.200]   this German sentiment BERT model
[00:25:50.200 --> 00:25:53.800]   that this user right here created.
[00:25:53.800 --> 00:25:57.080]   And you can go ahead and run that.
[00:25:57.080 --> 00:26:00.640]   I don't know a lot of German, but I know that is good.
[00:26:00.640 --> 00:26:02.400]   And we can see that, yes,
[00:26:02.400 --> 00:26:04.880]   that's a positive statement right there.
[00:26:04.880 --> 00:26:07.480]   And so you can actually go to the model hub,
[00:26:07.480 --> 00:26:09.760]   find a model that you wanna work with
[00:26:09.760 --> 00:26:12.120]   and use that instead of the default version
[00:26:12.120 --> 00:26:13.040]   that we saw above.
[00:26:13.040 --> 00:26:19.800]   There's also models for zero-shock classification.
[00:26:19.800 --> 00:26:23.160]   And these are really interesting models
[00:26:23.160 --> 00:26:27.520]   where we actually, in addition to saying,
[00:26:27.520 --> 00:26:31.240]   we actually supply the labels
[00:26:31.240 --> 00:26:33.760]   and it can still figure out,
[00:26:33.760 --> 00:26:37.600]   like whether or not it's one of these labels,
[00:26:37.600 --> 00:26:40.520]   even though we're defining these on the fly.
[00:26:40.520 --> 00:26:42.480]   I don't know a lot about how these
[00:26:42.480 --> 00:26:43.880]   particular models are trained.
[00:26:43.880 --> 00:26:46.920]   It definitely seems like a little bit of black magic,
[00:26:46.920 --> 00:26:50.400]   but as an example, we can pass in some text and tell it,
[00:26:50.400 --> 00:26:52.520]   hey, these are the labels we're interested in.
[00:26:52.520 --> 00:26:55.520]   And it will try to infer whether this is about education,
[00:26:55.520 --> 00:26:57.800]   politics, and/or business.
[00:26:57.800 --> 00:27:01.600]   And you can see here's our labels and here's our scores.
[00:27:01.600 --> 00:27:06.600]   It got it right and classified this as being education.
[00:27:06.600 --> 00:27:11.760]   And then again, using that .model.name path,
[00:27:11.760 --> 00:27:16.720]   we can actually see what model Hugging Face is defaulting to
[00:27:16.720 --> 00:27:19.560]   for the pipeline API.
[00:27:19.560 --> 00:27:24.080]   And then always fun is the text generation.
[00:27:24.080 --> 00:27:26.800]   And so this is where you're gonna supply a prompt
[00:27:26.800 --> 00:27:30.280]   and then it's gonna generate a text
[00:27:30.280 --> 00:27:31.480]   where it's gonna continue.
[00:27:31.480 --> 00:27:32.840]   So here we start with, in this course,
[00:27:32.840 --> 00:27:34.560]   we will teach you how to,
[00:27:34.560 --> 00:27:37.520]   and we see we will teach you how to get started
[00:27:37.520 --> 00:27:41.520]   using Docker as a tool to manage and whatever.
[00:27:41.520 --> 00:27:42.720]   Obviously this is wrong.
[00:27:42.720 --> 00:27:44.600]   That's one of the dangers with text generation,
[00:27:44.600 --> 00:27:47.040]   but it is kind of interesting to see
[00:27:47.040 --> 00:27:50.320]   what it actually produces.
[00:27:50.320 --> 00:27:55.000]   So you have that pipeline available to you as well.
[00:27:55.000 --> 00:27:56.840]   And also there's a variety,
[00:27:56.840 --> 00:27:59.480]   you can see like I'm not passing anything in here
[00:27:59.480 --> 00:28:02.280]   in terms of controlling the text generation,
[00:28:02.280 --> 00:28:04.000]   but you can, there's a bunch of parameters
[00:28:04.000 --> 00:28:05.800]   you can pass into that.
[00:28:05.800 --> 00:28:08.560]   And so you can see here, we're doing text generation.
[00:28:08.560 --> 00:28:11.840]   We're using the distilled version of GPT-2
[00:28:11.840 --> 00:28:15.120]   and I'm passing in a max length and also saying,
[00:28:15.120 --> 00:28:19.720]   hey, give me the two most likely predictions.
[00:28:19.720 --> 00:28:22.800]   And for a really good understanding
[00:28:22.800 --> 00:28:25.280]   of those particular parameters,
[00:28:26.120 --> 00:28:29.480]   make sure you check out this article from Hugging Face
[00:28:29.480 --> 00:28:30.960]   on how to generate.
[00:28:30.960 --> 00:28:34.800]   They go over all the different parameters,
[00:28:34.800 --> 00:28:39.280]   how they work, when you might wanna use one over the other.
[00:28:39.280 --> 00:28:41.480]   So check that out 'cause there's a lot more
[00:28:41.480 --> 00:28:45.640]   to text generation than it may seem on the surface.
[00:28:45.640 --> 00:28:49.600]   And then of course we've got language modeling.
[00:28:49.600 --> 00:28:51.560]   So for like mass language modeling,
[00:28:51.560 --> 00:28:55.800]   we're saying basically it's like a fill in the blank
[00:28:55.800 --> 00:28:57.880]   type of thing.
[00:28:57.880 --> 00:29:02.880]   And so here I'm returning a couple examples right here.
[00:29:02.880 --> 00:29:07.440]   So all about blank models, about mathematical models,
[00:29:07.440 --> 00:29:11.760]   computational models, and it gives you the score.
[00:29:11.760 --> 00:29:13.360]   Here I'm looking at the top two.
[00:29:13.360 --> 00:29:19.000]   And then we also have, I mentioned text classification
[00:29:19.000 --> 00:29:21.800]   and this is covered in the course,
[00:29:21.800 --> 00:29:23.800]   but if you pass in grouped entities,
[00:29:23.800 --> 00:29:28.360]   remember when we saw the tokenization of my name
[00:29:28.360 --> 00:29:33.360]   had W-A-Y was one token and then #DE was another token.
[00:29:33.360 --> 00:29:37.040]   And when we do name entity recognition,
[00:29:37.040 --> 00:29:40.080]   we'd rather it just say, Wade is a person's name,
[00:29:40.080 --> 00:29:44.560]   not way and #DE is a person's name.
[00:29:44.560 --> 00:29:46.880]   And grouped entities takes those sub tokens
[00:29:46.880 --> 00:29:48.120]   and puts them back together.
[00:29:48.120 --> 00:29:52.360]   So it reconstructs the original full word.
[00:29:52.360 --> 00:29:57.360]   And so you can see like it's predicted using this model
[00:29:57.360 --> 00:30:00.360]   that Sylvan is a person's name,
[00:30:00.360 --> 00:30:05.160]   Hugging Face is an organization and Brooklyn is a location.
[00:30:05.160 --> 00:30:09.320]   If you wanna see what group entities equals false does,
[00:30:09.320 --> 00:30:12.400]   this is actually just how a lot of times
[00:30:12.400 --> 00:30:13.320]   I code and learn things.
[00:30:13.320 --> 00:30:16.520]   I just change the values and see what happens.
[00:30:16.520 --> 00:30:20.400]   You can see that Sylvan's name is actually broken up
[00:30:20.400 --> 00:30:25.400]   into S, #YL, #VA, #IN, and they all have I person.
[00:30:25.400 --> 00:30:31.440]   And what the group entities basically did is just say,
[00:30:31.440 --> 00:30:33.360]   no, that's really just Sylvan,
[00:30:33.360 --> 00:30:35.720]   put those all together and said, that's a person.
[00:30:35.720 --> 00:30:37.840]   So that's what the group entities does.
[00:30:37.840 --> 00:30:44.560]   Question answering, if we supply a question and a context,
[00:30:44.560 --> 00:30:50.080]   it should pull out the answer within that text.
[00:30:50.080 --> 00:30:51.920]   So the answer is right here.
[00:30:51.920 --> 00:30:54.920]   And we can see that it actually does this
[00:30:54.920 --> 00:30:57.280]   with pretty high confidence.
[00:30:57.280 --> 00:31:01.240]   So about 70%, it says, this is where the start token is.
[00:31:01.240 --> 00:31:02.800]   This is where the end token is.
[00:31:02.800 --> 00:31:04.200]   That's extractive QA.
[00:31:04.200 --> 00:31:07.320]   Summarization is pretty straightforward.
[00:31:07.320 --> 00:31:09.400]   Take a big text document,
[00:31:09.400 --> 00:31:11.480]   summarize it in one or two sentences.
[00:31:11.480 --> 00:31:16.720]   And then we also have translation tasks,
[00:31:16.720 --> 00:31:19.040]   which is also pretty straightforward.
[00:31:19.040 --> 00:31:22.640]   You're gonna translate from one language to another.
[00:31:22.640 --> 00:31:27.600]   So here we're using a Merian MT model
[00:31:27.600 --> 00:31:30.800]   and going from French to English.
[00:31:30.800 --> 00:31:33.760]   And then we can see that this is the translation right here.
[00:31:33.760 --> 00:31:36.560]   This course is produced by Hugging Face.
[00:31:36.560 --> 00:31:39.520]   So again, the course kind of goes through
[00:31:39.520 --> 00:31:41.000]   these little bit more detail,
[00:31:41.000 --> 00:31:43.400]   but are there any questions so far
[00:31:43.400 --> 00:31:46.960]   on using the pipeline API or tokenization?
[00:31:47.960 --> 00:31:51.800]   - This one question on tokenization by Ritobrata.
[00:31:51.800 --> 00:31:54.120]   So he's asking how to gain the wisdom
[00:31:54.120 --> 00:31:56.840]   of which kind of tokenization to use
[00:31:56.840 --> 00:31:59.240]   for a particular architecture or datasets?
[00:31:59.240 --> 00:32:02.480]   For example, you should be using subword for German,
[00:32:02.480 --> 00:32:04.840]   Finnish and character level for Mandarin,
[00:32:04.840 --> 00:32:08.640]   but beyond that, how do you decide what to use?
[00:32:08.640 --> 00:32:13.640]   - Well, so each architecture has basically
[00:32:13.640 --> 00:32:16.240]   their tokenization format they're using.
[00:32:16.240 --> 00:32:17.680]   And so you're really forced to use
[00:32:17.680 --> 00:32:20.120]   whatever that architecture is using.
[00:32:20.120 --> 00:32:21.280]   And you don't wanna deviate
[00:32:21.280 --> 00:32:24.680]   because then you're not able to use models
[00:32:24.680 --> 00:32:27.320]   that were pre-trained using that particular type
[00:32:27.320 --> 00:32:29.480]   of tokenization strategy.
[00:32:29.480 --> 00:32:31.280]   So probably the best thing to do
[00:32:31.280 --> 00:32:33.760]   is to look at the documentation.
[00:32:33.760 --> 00:32:39.640]   If you go to resources
[00:32:39.640 --> 00:32:42.800]   and you can look at the tokenizer stock
[00:32:42.800 --> 00:32:46.520]   and you can get more information about different strategies.
[00:32:46.520 --> 00:32:50.520]   If you look at the transformers documentation,
[00:32:50.520 --> 00:32:53.080]   you can actually see details
[00:32:53.080 --> 00:32:56.040]   about each of the architectures tokenizers,
[00:32:56.040 --> 00:32:57.440]   and you'll be able to discover,
[00:32:57.440 --> 00:32:59.440]   like, are they multilingual capable?
[00:32:59.440 --> 00:33:01.080]   How many languages were they trained on?
[00:33:01.080 --> 00:33:01.920]   Things like that.
[00:33:01.920 --> 00:33:04.640]   So that would probably give you the guidance you need.
[00:33:04.640 --> 00:33:07.440]   - Yeah, the docs are amazing.
[00:33:07.440 --> 00:33:09.000]   There are a few questions around the link.
[00:33:09.000 --> 00:33:11.080]   So after the study group is over,
[00:33:11.080 --> 00:33:12.320]   whenever the email goes out,
[00:33:12.320 --> 00:33:14.840]   we'll make sure the collab and all of the links are there.
[00:33:14.840 --> 00:33:17.000]   So make sure you join the Faster Discord
[00:33:17.000 --> 00:33:19.320]   and you can check out the notebook.
[00:33:19.320 --> 00:33:20.600]   One more question.
[00:33:20.600 --> 00:33:23.400]   I wish to include my labels or categories
[00:33:23.400 --> 00:33:25.520]   in a named entity recognition model.
[00:33:25.520 --> 00:33:28.000]   Any guidance or pointers for that?
[00:33:28.000 --> 00:33:33.240]   - So yeah, you can actually create your own labels.
[00:33:33.240 --> 00:33:35.000]   And then what you could do is start
[00:33:35.000 --> 00:33:37.480]   with one of the pre-trained models
[00:33:37.480 --> 00:33:39.720]   that was maybe trained on a subset.
[00:33:39.720 --> 00:33:41.760]   Like a real common one is,
[00:33:41.760 --> 00:33:46.160]   I think it's the German coal NNL or something data set.
[00:33:46.160 --> 00:33:47.800]   I forget what it is.
[00:33:47.800 --> 00:33:50.760]   And you can start with even the data set.
[00:33:50.760 --> 00:33:52.600]   For example, I have one model,
[00:33:52.600 --> 00:33:55.160]   an NER model that I trained.
[00:33:55.160 --> 00:33:57.880]   I started with the German data set
[00:33:57.880 --> 00:33:59.720]   and I fine-tuned it on English.
[00:33:59.720 --> 00:34:02.720]   And it was amazing how good it turned out.
[00:34:02.720 --> 00:34:04.720]   And so in week three,
[00:34:04.720 --> 00:34:06.880]   we'll spend more time going through,
[00:34:06.880 --> 00:34:09.720]   like picking a pre-trained model and fine-tuning it.
[00:34:09.720 --> 00:34:11.240]   But absolutely, you can go ahead
[00:34:11.240 --> 00:34:12.840]   and fine-tune it for your own labels,
[00:34:12.840 --> 00:34:15.720]   even if the labels in the pre-trained model are different.
[00:34:15.720 --> 00:34:19.600]   - And I just like to nudge everyone towards,
[00:34:19.600 --> 00:34:20.920]   you know, writing about this
[00:34:20.920 --> 00:34:22.320]   or just publishing your notebooks,
[00:34:22.320 --> 00:34:25.440]   use fast pages or any blog that you want.
[00:34:25.440 --> 00:34:26.880]   I'm sure Wade will talk about this,
[00:34:26.880 --> 00:34:29.120]   but at least I can speak for Wade and I,
[00:34:29.120 --> 00:34:32.080]   all of this creates amazing career opportunities
[00:34:32.080 --> 00:34:32.920]   for you as well.
[00:34:32.920 --> 00:34:36.160]   So make sure you learn and also write about what you learn.
[00:34:36.160 --> 00:34:37.000]   - Yeah, absolutely.
[00:34:37.000 --> 00:34:41.680]   I think Parul did something on weights and biases last week
[00:34:41.680 --> 00:34:43.560]   and that's really kind of how she got started.
[00:34:43.560 --> 00:34:44.920]   I think she works at weights and biases.
[00:34:44.920 --> 00:34:47.000]   I'm not a hundred percent sure.
[00:34:47.000 --> 00:34:49.200]   - Parul works at H2O.ai along with me.
[00:34:49.200 --> 00:34:50.040]   - Oh, she does.
[00:34:50.040 --> 00:34:50.880]   She works at, okay.
[00:34:50.880 --> 00:34:53.080]   Yeah, H2O.ai.
[00:34:53.080 --> 00:34:55.480]   But she, I think really kind of got started
[00:34:55.480 --> 00:34:58.960]   and her career exploded because she was blogging
[00:34:58.960 --> 00:35:03.000]   and then also giving lectures and things and whatnot.
[00:35:03.000 --> 00:35:05.000]   And so even stuff like this, when you teach people,
[00:35:05.000 --> 00:35:07.640]   and I think this is something that we've all learned
[00:35:07.640 --> 00:35:11.880]   from Jeremy Howard is that when you, wherever you're at,
[00:35:11.880 --> 00:35:14.200]   you have learned something that someone is,
[00:35:14.200 --> 00:35:15.520]   that you can teach someone behind you
[00:35:15.520 --> 00:35:17.960]   that hasn't had that content yet,
[00:35:17.960 --> 00:35:19.480]   even if you're just starting out.
[00:35:19.480 --> 00:35:22.480]   And a great way to develop your own understanding
[00:35:22.480 --> 00:35:25.200]   and also help others is to blog about it
[00:35:25.200 --> 00:35:27.600]   or to do things like this, to do little Zoom sessions
[00:35:27.600 --> 00:35:30.600]   and study groups and teach others.
[00:35:30.600 --> 00:35:33.000]   So yeah, that's great advice.
[00:35:34.600 --> 00:35:35.840]   Anything else?
[00:35:35.840 --> 00:35:36.680]   - No questions.
[00:35:36.680 --> 00:35:38.240]   No other questions.
[00:35:38.240 --> 00:35:39.320]   - All right.
[00:35:39.320 --> 00:35:42.320]   All right, so let's go back to the presentation here.
[00:35:42.320 --> 00:35:48.080]   So we've gone through the pipeline API.
[00:35:48.080 --> 00:35:52.400]   We've also seen how we can go to the Hugging Face Model Hub
[00:35:52.400 --> 00:35:55.480]   and use the inference API there right on the web,
[00:35:55.480 --> 00:35:56.520]   which is really cool.
[00:35:56.520 --> 00:35:59.640]   So transformers.
[00:35:59.640 --> 00:36:04.640]   So the big thing in section one is really like,
[00:36:04.640 --> 00:36:06.680]   how do transformers work?
[00:36:06.680 --> 00:36:08.600]   What does the architecture look like?
[00:36:08.600 --> 00:36:12.160]   And you're not going to have a complete understanding
[00:36:12.160 --> 00:36:14.520]   just by going through this study group
[00:36:14.520 --> 00:36:15.960]   or by going through the course.
[00:36:15.960 --> 00:36:19.000]   I'm gonna have some links to some papers
[00:36:19.000 --> 00:36:21.160]   and to some articles that you wanna read,
[00:36:21.160 --> 00:36:24.160]   but understanding how the transformer architecture works,
[00:36:24.160 --> 00:36:26.640]   the initial transformer architecture,
[00:36:26.640 --> 00:36:28.720]   everything you see as a transformer model
[00:36:28.720 --> 00:36:30.200]   Hugging Face is based on it.
[00:36:30.200 --> 00:36:32.880]   So if you wanna understand like how it works,
[00:36:32.880 --> 00:36:35.800]   you need to understand the transformer architecture.
[00:36:35.800 --> 00:36:39.600]   And essentially all transformer models
[00:36:39.600 --> 00:36:42.160]   have been trained as language models.
[00:36:42.160 --> 00:36:44.160]   So a language model,
[00:36:44.160 --> 00:36:47.560]   and the reason why this is key is that
[00:36:47.560 --> 00:36:49.840]   a language model is essentially,
[00:36:49.840 --> 00:36:51.800]   you're either trying to predict the next word
[00:36:51.800 --> 00:36:56.800]   given a word and its preceding words,
[00:36:58.040 --> 00:37:00.600]   like in the causal language model,
[00:37:00.600 --> 00:37:02.840]   or you have a mass language model where you've got text
[00:37:02.840 --> 00:37:06.280]   and you're trying to figure out what these mass tokens
[00:37:06.280 --> 00:37:09.240]   or corrupted tokens really should refer to.
[00:37:09.240 --> 00:37:11.000]   And the idea of a language model is that
[00:37:11.000 --> 00:37:13.200]   if you can build a model that does either
[00:37:13.200 --> 00:37:15.120]   one of these tasks really well,
[00:37:15.120 --> 00:37:18.920]   then you've created a model that understands
[00:37:18.920 --> 00:37:22.200]   the particular grammar of the text that it's saying.
[00:37:22.200 --> 00:37:26.720]   And a model that has a good understanding of the text
[00:37:26.720 --> 00:37:30.560]   is a model that you can then apply to downstream tasks
[00:37:30.560 --> 00:37:31.880]   related to that text.
[00:37:31.880 --> 00:37:34.920]   So it's figuring out sentiment or figuring out entities
[00:37:34.920 --> 00:37:37.280]   or figuring out answers in there.
[00:37:37.280 --> 00:37:40.680]   And so that's why language modeling is so critical
[00:37:40.680 --> 00:37:43.880]   and all these transformers and other architectures
[00:37:43.880 --> 00:37:48.000]   like ULMFIT and LSTMs,
[00:37:48.000 --> 00:37:49.800]   why they're all based on language models
[00:37:49.800 --> 00:37:52.480]   is because they start with that intuitive idea that,
[00:37:52.480 --> 00:37:55.520]   yeah, if I have a model that can do this really well
[00:37:55.520 --> 00:37:59.240]   and predict the next token or predict mass tokens,
[00:37:59.240 --> 00:38:02.040]   that I have something that understands
[00:38:02.040 --> 00:38:03.240]   the particular grammar,
[00:38:03.240 --> 00:38:06.080]   the particular language that it's trained on.
[00:38:06.080 --> 00:38:08.160]   So that's the whole idea.
[00:38:08.160 --> 00:38:10.400]   And so you take these language models
[00:38:10.400 --> 00:38:14.680]   and a lot of them have been trained on a lot of text,
[00:38:14.680 --> 00:38:19.680]   on a lot of GPUs or TPUs over many days at a lot of costs,
[00:38:21.040 --> 00:38:26.040]   things that at least I can't do with my 1080 Ti GPU
[00:38:26.040 --> 00:38:29.720]   and my little DL rig here at the house.
[00:38:29.720 --> 00:38:34.720]   And so these folks have pre-trained these big models
[00:38:34.720 --> 00:38:37.160]   and we can take what they've learned,
[00:38:37.160 --> 00:38:41.160]   the weights that it's learned through that process.
[00:38:41.160 --> 00:38:43.120]   And we can go ahead and grab those models
[00:38:43.120 --> 00:38:45.520]   and through a process called transfer learning,
[00:38:45.520 --> 00:38:47.320]   we could take those pre-trained language models
[00:38:47.320 --> 00:38:50.200]   and fine tune them on our particular text.
[00:38:51.160 --> 00:38:54.880]   And that's really where this comes in handy.
[00:38:54.880 --> 00:38:57.520]   If you're going through the Fast.ai course or about to,
[00:38:57.520 --> 00:39:01.240]   you'll see that transfer learning is the thing in Fast.ai
[00:39:01.240 --> 00:39:04.200]   because most folks just don't have the ability
[00:39:04.200 --> 00:39:07.920]   to go through billions of Wikipedia articles
[00:39:07.920 --> 00:39:11.480]   and spend tens or hundreds of thousands of dollars
[00:39:11.480 --> 00:39:13.720]   in compute costs to build these things.
[00:39:13.720 --> 00:39:16.160]   But fortunately, big companies like Microsoft,
[00:39:16.160 --> 00:39:18.760]   OpenAI and whatnot, they've actually done that for us
[00:39:18.760 --> 00:39:21.160]   and we can use those models on our own,
[00:39:21.160 --> 00:39:26.160]   on our limited resources for our particular raw text
[00:39:26.160 --> 00:39:29.280]   and for our particular constraints.
[00:39:29.280 --> 00:39:32.920]   So how do transformers work?
[00:39:32.920 --> 00:39:36.880]   So at a high level, there's two parts.
[00:39:36.880 --> 00:39:39.040]   There's an encoder part that our inputs
[00:39:39.040 --> 00:39:40.480]   are going to come through.
[00:39:40.480 --> 00:39:43.080]   And this part right here is optimized
[00:39:43.080 --> 00:39:46.200]   to acquire understanding from the input.
[00:39:46.200 --> 00:39:48.760]   And then there is this,
[00:39:48.760 --> 00:39:52.840]   and also it converts text to numerical representations.
[00:39:52.840 --> 00:39:56.720]   So essentially each token is going to get an embedding
[00:39:56.720 --> 00:40:00.880]   which is a higher dimensional numerical representation
[00:40:00.880 --> 00:40:02.080]   that's going to be learned.
[00:40:02.080 --> 00:40:07.040]   So we're gonna learn a high dimensional representation
[00:40:07.040 --> 00:40:08.800]   for every single token.
[00:40:08.800 --> 00:40:12.760]   And they go through a couple layers
[00:40:12.760 --> 00:40:15.560]   and probably the most important is the self-attention layer
[00:40:15.560 --> 00:40:17.360]   which we'll look at in a second.
[00:40:17.360 --> 00:40:20.760]   And what it outputs is a high level representation
[00:40:20.760 --> 00:40:22.720]   of those inputs.
[00:40:22.720 --> 00:40:24.280]   Then what it does,
[00:40:24.280 --> 00:40:26.200]   and remember that the transformer architecture
[00:40:26.200 --> 00:40:29.360]   was originally built to handle translation.
[00:40:29.360 --> 00:40:33.240]   And so the inputs would be, for example, German text
[00:40:33.240 --> 00:40:37.320]   and the outputs would be English text.
[00:40:37.320 --> 00:40:38.880]   So the English translation.
[00:40:38.880 --> 00:40:42.320]   So that's what happens in the encoder.
[00:40:42.320 --> 00:40:44.520]   The decoder during training,
[00:40:44.520 --> 00:40:49.520]   we will actually feed in the correct tokens,
[00:40:49.520 --> 00:40:56.240]   but we use something in math language model
[00:40:56.240 --> 00:40:58.040]   called a math self-attention math
[00:40:58.040 --> 00:41:03.040]   to make sure that tokens can't look at subsequent tokens
[00:41:03.040 --> 00:41:07.640]   for building a representation of itself.
[00:41:07.640 --> 00:41:10.200]   And we'll take a look at that as well
[00:41:10.200 --> 00:41:11.240]   a little bit more detailed,
[00:41:11.240 --> 00:41:15.640]   but here the encoder is focused on understanding the input.
[00:41:15.640 --> 00:41:20.280]   The decoder is really focused on generating the output.
[00:41:20.280 --> 00:41:23.600]   So generating a high level representation of that output
[00:41:23.600 --> 00:41:26.600]   that we can use for predicting the next text.
[00:41:26.600 --> 00:41:29.920]   And then also for things like being able to generate
[00:41:29.920 --> 00:41:32.360]   a classification prediction.
[00:41:32.360 --> 00:41:36.040]   Now, the biggest thing that we already mentioned
[00:41:36.040 --> 00:41:38.240]   is this idea of attention.
[00:41:38.240 --> 00:41:42.120]   And essentially there's three different types
[00:41:42.120 --> 00:41:43.200]   of attention layers
[00:41:43.200 --> 00:41:45.800]   in the original transformer architecture.
[00:41:45.800 --> 00:41:49.360]   And a high level definition would be that this layer
[00:41:49.360 --> 00:41:52.000]   will tell the model to pay specific attention
[00:41:52.000 --> 00:41:54.880]   to certain words in the sentence you passed it
[00:41:54.880 --> 00:41:56.320]   and more or less ignore others
[00:41:56.320 --> 00:41:59.280]   when dealing with the representation of each word.
[00:41:59.280 --> 00:42:00.960]   A word by itself has a meaning
[00:42:00.960 --> 00:42:04.040]   that that meaning is deeply affected by the context,
[00:42:04.040 --> 00:42:07.040]   which can be any other word or words before
[00:42:07.040 --> 00:42:09.200]   or after the word being studied.
[00:42:09.200 --> 00:42:11.480]   And so this is actually pretty intuitive, right?
[00:42:11.480 --> 00:42:14.760]   That when we're looking at a sentence,
[00:42:14.760 --> 00:42:17.120]   the meaning of each word in that sentence
[00:42:17.120 --> 00:42:19.480]   is going to be affected to one degree or another
[00:42:19.480 --> 00:42:21.680]   by words around it.
[00:42:21.680 --> 00:42:25.000]   And so this is an example from Jay Alomar's
[00:42:25.000 --> 00:42:27.440]   the illustrated transformer article.
[00:42:27.440 --> 00:42:30.120]   And the animal didn't cross the street
[00:42:30.120 --> 00:42:31.960]   because it was too tired.
[00:42:31.960 --> 00:42:33.600]   And he asked the question,
[00:42:33.600 --> 00:42:36.680]   what does it in this sentence refer to?
[00:42:36.680 --> 00:42:38.880]   Is it referring to the street
[00:42:38.880 --> 00:42:41.080]   or is it referring to the animal?
[00:42:41.080 --> 00:42:44.360]   And as we read this, it's pretty simple, right?
[00:42:44.360 --> 00:42:46.760]   We can understand what it means,
[00:42:46.760 --> 00:42:50.760]   but a model is, you know, computers are pretty dumb.
[00:42:50.760 --> 00:42:53.280]   So by using this thing called attention,
[00:42:53.280 --> 00:42:56.240]   we can have it when it's creating
[00:42:56.240 --> 00:42:58.520]   encoding a representation for it,
[00:42:58.520 --> 00:43:01.200]   it can actually learn to pay attention
[00:43:01.200 --> 00:43:03.760]   to the right word such as animal.
[00:43:03.760 --> 00:43:04.600]   Whoops.
[00:43:06.560 --> 00:43:07.880]   A little too far there.
[00:43:07.880 --> 00:43:14.280]   And so that's the whole idea of how attention works.
[00:43:14.280 --> 00:43:17.080]   It's basically allowing us to look at words
[00:43:17.080 --> 00:43:20.760]   around each other word to develop a representation of it
[00:43:20.760 --> 00:43:24.720]   so that we can understand a word's meaning in context.
[00:43:24.720 --> 00:43:28.600]   And this is what the original
[00:43:28.600 --> 00:43:31.040]   transformer architecture looks like.
[00:43:31.040 --> 00:43:34.280]   And this will also be part of the homework
[00:43:34.280 --> 00:43:35.600]   for those who choose to do it,
[00:43:35.600 --> 00:43:37.680]   is to actually read these papers.
[00:43:37.680 --> 00:43:39.360]   You might be intimidated and think,
[00:43:39.360 --> 00:43:41.160]   oh, I'm not gonna be able to understand any of this.
[00:43:41.160 --> 00:43:42.360]   I guarantee it.
[00:43:42.360 --> 00:43:44.080]   You will be able to understand the attention
[00:43:44.080 --> 00:43:45.400]   is all you need.
[00:43:45.400 --> 00:43:47.840]   And I've read it probably several times.
[00:43:47.840 --> 00:43:49.880]   I still go back and look at it
[00:43:49.880 --> 00:43:51.880]   because it's really informative about
[00:43:51.880 --> 00:43:53.520]   not just the transformer architecture,
[00:43:53.520 --> 00:43:55.720]   but how to apply it to different tasks.
[00:43:55.720 --> 00:43:59.360]   So this will also be included in the slide deck
[00:43:59.360 --> 00:44:01.560]   that we send out to everybody.
[00:44:01.560 --> 00:44:02.880]   And if you're confused at all
[00:44:02.880 --> 00:44:05.880]   by any of the concepts in the paper,
[00:44:05.880 --> 00:44:11.880]   read it alongside the illustrated transformer by Jay.
[00:44:11.880 --> 00:44:14.600]   And he's got all kinds of great animations
[00:44:14.600 --> 00:44:19.600]   and graphics that explain in detail how attention works.
[00:44:19.600 --> 00:44:23.040]   And literally probably my favorite resource
[00:44:23.040 --> 00:44:26.000]   in terms of, actually all his stuff is really good.
[00:44:26.000 --> 00:44:31.000]   He's got illustrated guides to the transformer,
[00:44:32.320 --> 00:44:34.000]   to BERT, a bunch of stuff.
[00:44:34.000 --> 00:44:39.000]   Great resource just to keep that in mind.
[00:44:39.000 --> 00:44:41.080]   So the transformer architecture, as I said,
[00:44:41.080 --> 00:44:43.680]   there's really three types of attention.
[00:44:43.680 --> 00:44:48.960]   And right here, don't worry so much right now
[00:44:48.960 --> 00:44:50.920]   what multi-head attention means,
[00:44:50.920 --> 00:44:53.960]   but here we have bidirectional attention.
[00:44:53.960 --> 00:44:56.680]   And so when it's encoding the inputs,
[00:44:56.680 --> 00:44:59.440]   the attention layer is allowed to look at
[00:44:59.440 --> 00:45:01.840]   the preceding and subsequent tokens
[00:45:01.840 --> 00:45:03.640]   and figuring out a representation
[00:45:03.640 --> 00:45:06.320]   for each token that's passed through.
[00:45:06.320 --> 00:45:09.800]   On the decoder side, we have mass attention.
[00:45:09.800 --> 00:45:13.400]   And as I said before, the idea there is to keep in mind
[00:45:13.400 --> 00:45:15.760]   is that there's a mask.
[00:45:15.760 --> 00:45:17.440]   And remember we saw that attention mask
[00:45:17.440 --> 00:45:18.720]   when we looked at the tokenizer,
[00:45:18.720 --> 00:45:20.800]   it produced that attention mask.
[00:45:20.800 --> 00:45:23.680]   Well, here there's an attention mask
[00:45:23.680 --> 00:45:26.440]   and it essentially ensures that
[00:45:26.440 --> 00:45:29.200]   as we are looking during training
[00:45:29.200 --> 00:45:32.320]   and we're looking at our target texts
[00:45:32.320 --> 00:45:37.320]   that we're trying to replicate in the form of predictions,
[00:45:37.320 --> 00:45:41.280]   it's essentially masking subsequent words.
[00:45:41.280 --> 00:45:42.960]   So it's not able to look at that
[00:45:42.960 --> 00:45:44.280]   because when we do inference,
[00:45:44.280 --> 00:45:46.400]   it's going to generate one word at a time.
[00:45:46.400 --> 00:45:48.280]   So it's gonna generate word one,
[00:45:48.280 --> 00:45:51.120]   and then it's gonna try to predict two from word one.
[00:45:51.120 --> 00:45:52.720]   Then having word two predicted,
[00:45:52.720 --> 00:45:54.280]   it's going to try to predict word three
[00:45:54.280 --> 00:45:56.480]   from word one and two.
[00:45:56.480 --> 00:45:57.720]   So when we're training,
[00:45:57.720 --> 00:46:00.520]   we have to mask those future words out.
[00:46:00.520 --> 00:46:02.920]   And then the third form of self-attention
[00:46:02.920 --> 00:46:07.560]   is encoder attention.
[00:46:07.560 --> 00:46:11.760]   So we're actually going to take the representations
[00:46:11.760 --> 00:46:14.640]   of each of the tokens that are learned on the encoder side,
[00:46:14.640 --> 00:46:17.960]   and we're going to let the decoder learn
[00:46:17.960 --> 00:46:21.640]   how to pay attention to those outputs as well.
[00:46:21.640 --> 00:46:22.480]   And those are really,
[00:46:22.480 --> 00:46:24.880]   so those they're all using a form of attention,
[00:46:24.880 --> 00:46:25.800]   there's three of them.
[00:46:25.800 --> 00:46:27.240]   And so that's really the key concept
[00:46:27.240 --> 00:46:32.320]   that has made transformers super popular.
[00:46:32.320 --> 00:46:35.720]   So when we go look at Hugging Face,
[00:46:35.720 --> 00:46:38.960]   we'll see that some transformers are just encoder models.
[00:46:38.960 --> 00:46:40.240]   So they're just the, actually,
[00:46:40.240 --> 00:46:44.400]   any questions coming up yet, Sanyam?
[00:46:44.400 --> 00:46:46.360]   - This is a follow-up question from earlier.
[00:46:46.360 --> 00:46:49.320]   So for the tokenization strategy,
[00:46:49.320 --> 00:46:51.000]   they were asking about if they're working
[00:46:51.000 --> 00:46:52.600]   on a novel architecture,
[00:46:52.680 --> 00:46:57.560]   which approach should they use for tokenization?
[00:46:57.560 --> 00:46:59.560]   - For tokenization?
[00:46:59.560 --> 00:47:03.080]   I'm not sure how to answer that.
[00:47:03.080 --> 00:47:05.760]   If they're working for a novel approach,
[00:47:05.760 --> 00:47:08.160]   yeah, I don't really know.
[00:47:08.160 --> 00:47:11.400]   I don't know what the best answer to that would be.
[00:47:11.400 --> 00:47:14.800]   I would say post that on the Hugging Face forums,
[00:47:14.800 --> 00:47:19.240]   and which are staffed by all the ML engineers over there,
[00:47:19.240 --> 00:47:21.600]   and see what they say in terms of starting out.
[00:47:22.600 --> 00:47:24.520]   - Apart from that, we're just sharing the sources
[00:47:24.520 --> 00:47:26.520]   and just talking about different blog posts
[00:47:26.520 --> 00:47:31.040]   around understanding attention and transformers.
[00:47:31.040 --> 00:47:34.720]   - Yeah, anything else?
[00:47:34.720 --> 00:47:36.480]   Or is that a question?
[00:47:36.480 --> 00:47:37.560]   - No, that's it.
[00:47:37.560 --> 00:47:40.080]   - Okay, all right.
[00:47:40.080 --> 00:47:42.680]   So when you look at transformer models,
[00:47:42.680 --> 00:47:45.720]   or you're looking at the transformers library, I should say,
[00:47:45.720 --> 00:47:48.000]   you'll see that some are encoder only.
[00:47:48.000 --> 00:47:50.080]   So they just include that encoder part
[00:47:51.120 --> 00:47:52.600]   that we looked at right here.
[00:47:52.600 --> 00:47:54.320]   So they just include this part.
[00:47:54.320 --> 00:47:56.560]   Others are decoder specific,
[00:47:56.560 --> 00:47:58.680]   and there's other ones that are sequence to sequence
[00:47:58.680 --> 00:48:00.640]   that include both.
[00:48:00.640 --> 00:48:02.160]   And just real briefly,
[00:48:02.160 --> 00:48:04.160]   I'll include some,
[00:48:04.160 --> 00:48:06.760]   here's some examples of encoder only ones.
[00:48:06.760 --> 00:48:09.240]   And what I've had really good success with Roberta,
[00:48:09.240 --> 00:48:11.040]   if you're just looking to start something out,
[00:48:11.040 --> 00:48:14.440]   try the Roberta architecture or the distilled version.
[00:48:14.440 --> 00:48:16.480]   But essentially, these are really good
[00:48:16.480 --> 00:48:19.160]   for natural language understanding tasks.
[00:48:19.160 --> 00:48:22.360]   So if your task is sequence classification,
[00:48:22.360 --> 00:48:25.040]   token classification, extractive QA,
[00:48:25.040 --> 00:48:27.080]   a mass language model,
[00:48:27.080 --> 00:48:29.240]   and you're looking to narrow down your options,
[00:48:29.240 --> 00:48:31.520]   think about starting with an encoder model
[00:48:31.520 --> 00:48:33.080]   and seeing how that works.
[00:48:33.080 --> 00:48:38.560]   Decoder models, again, which is this part right here,
[00:48:38.560 --> 00:48:41.760]   focus on text generation.
[00:48:41.760 --> 00:48:44.760]   So if you're looking to do something like
[00:48:44.760 --> 00:48:47.520]   just generating a dynamic text,
[00:48:47.520 --> 00:48:49.360]   there's decoder only models
[00:48:49.360 --> 00:48:50.840]   you might wanna pay attention to.
[00:48:50.840 --> 00:48:54.080]   Probably the most famous is GPT and GPT-2.
[00:48:54.080 --> 00:48:57.720]   And then a third type of transformer
[00:48:57.720 --> 00:48:59.720]   are sequence to sequence models,
[00:48:59.720 --> 00:49:04.720]   which really follow the transformer architecture as a whole.
[00:49:04.720 --> 00:49:08.200]   They use the encoder and decoder.
[00:49:08.200 --> 00:49:10.200]   And these are good where you have tasks
[00:49:10.200 --> 00:49:15.200]   where the input length is independent of the output length.
[00:49:15.760 --> 00:49:17.520]   So when you translate a lot of times,
[00:49:17.520 --> 00:49:19.040]   you might have five English words,
[00:49:19.040 --> 00:49:21.360]   but it may only be three German words.
[00:49:21.360 --> 00:49:24.840]   And so they're not gonna be the same.
[00:49:24.840 --> 00:49:26.800]   And it's also good to use where the output
[00:49:26.800 --> 00:49:30.120]   is highly dependent on the input.
[00:49:30.120 --> 00:49:32.560]   And so this is good for tasks like summarization,
[00:49:32.560 --> 00:49:34.400]   where you have a big article
[00:49:34.400 --> 00:49:38.000]   and you wanna accurately summarize in a sentence or two
[00:49:38.000 --> 00:49:42.240]   or in translation, or also in generative QA,
[00:49:42.240 --> 00:49:44.880]   where instead of extracting the answer,
[00:49:44.880 --> 00:49:48.040]   we're asking the model to actually just generate the answer.
[00:49:48.040 --> 00:49:51.440]   And that's what that is.
[00:49:51.440 --> 00:49:52.680]   That's the difference there.
[00:49:52.680 --> 00:49:54.640]   And in terms of sequence to sequence models,
[00:49:54.640 --> 00:49:58.360]   I've had a really good success on summarization using BART
[00:49:58.360 --> 00:50:01.800]   and for, and I think Pegasus.
[00:50:01.800 --> 00:50:03.080]   And then on translation,
[00:50:03.080 --> 00:50:06.440]   the Merian-MT models have proven to be really well
[00:50:06.440 --> 00:50:09.640]   if you're looking for architectures to start out with.
[00:50:09.640 --> 00:50:14.640]   Any question on any of those three forms of transformers?
[00:50:15.640 --> 00:50:17.960]   (mouse clicking)
[00:50:17.960 --> 00:50:19.520]   - No, I don't see anything yet.
[00:50:19.520 --> 00:50:23.360]   - And then the last part of this section
[00:50:23.360 --> 00:50:25.680]   is bias and limitations.
[00:50:25.680 --> 00:50:26.920]   It's just something to be aware of
[00:50:26.920 --> 00:50:28.480]   when you're building models,
[00:50:28.480 --> 00:50:30.360]   especially when you're using pre-trained models
[00:50:30.360 --> 00:50:31.720]   that others have made,
[00:50:31.720 --> 00:50:33.440]   is that more than likely,
[00:50:33.440 --> 00:50:36.680]   there's going to be some form of bias in the data
[00:50:36.680 --> 00:50:39.720]   as a result, or in the model based on the data
[00:50:39.720 --> 00:50:43.400]   it was trained on, or even the metric it was optimized for.
[00:50:43.400 --> 00:50:47.280]   And if you look at the model hub,
[00:50:47.280 --> 00:50:50.040]   a lot of those models will actually have that
[00:50:50.040 --> 00:50:53.280]   as something they includes, which is really nice, right?
[00:50:53.280 --> 00:50:57.880]   'Cause in terms of mitigating negative effects of this,
[00:50:57.880 --> 00:51:01.080]   knowledge is probably 80% of it.
[00:51:01.080 --> 00:51:02.640]   'Cause you have that information,
[00:51:02.640 --> 00:51:05.640]   you can make your end users aware of it, et cetera.
[00:51:05.640 --> 00:51:10.280]   If you're looking for more information about how,
[00:51:10.280 --> 00:51:13.080]   basically where bias comes from,
[00:51:13.080 --> 00:51:15.200]   and also other mitigation strategies,
[00:51:15.200 --> 00:51:21.080]   Rachel does a really good job in chapter three of Fastbook
[00:51:21.080 --> 00:51:23.640]   to going into this in more detail.
[00:51:23.640 --> 00:51:28.400]   So feel free to check that out as another resource.
[00:51:28.400 --> 00:51:34.040]   And any questions on any of the material?
[00:51:34.040 --> 00:51:36.600]   'Cause that's pretty much it for section one.
[00:51:36.600 --> 00:51:39.280]   - So there's a question around,
[00:51:39.280 --> 00:51:41.360]   is zero short learning just encoded
[00:51:41.360 --> 00:51:43.720]   or is it an encoded decoder model?
[00:51:43.720 --> 00:51:46.120]   - I'll say that again, Sanyam, I didn't hear all that.
[00:51:46.120 --> 00:51:46.960]   - Sorry.
[00:51:46.960 --> 00:51:50.560]   So is zero short learning just using an encoder model
[00:51:50.560 --> 00:51:53.280]   or is it an encoded decoder model?
[00:51:53.280 --> 00:51:56.120]   - Let's take a look here.
[00:51:56.120 --> 00:52:03.800]   Okay, let's see, did I actually pull that up there?
[00:52:03.800 --> 00:52:09.240]   Okay, tokenizer.
[00:52:09.440 --> 00:52:12.200]   (mouse clicking)
[00:52:12.200 --> 00:52:17.520]   So it's actually using the sequence to sequence,
[00:52:17.520 --> 00:52:21.920]   so it's using BART for that particular task.
[00:52:21.920 --> 00:52:25.480]   And so you'll see that even these sequence
[00:52:25.480 --> 00:52:27.920]   to sequence models like BART,
[00:52:27.920 --> 00:52:32.280]   they actually are, there's checkpoints available
[00:52:32.280 --> 00:52:34.520]   for classification tasks.
[00:52:34.520 --> 00:52:35.720]   And I'm not exactly sure,
[00:52:35.720 --> 00:52:38.760]   they might actually discard parts.
[00:52:38.760 --> 00:52:40.680]   I'm not actually sure how they use the decoder
[00:52:40.680 --> 00:52:41.880]   in those particular architectures,
[00:52:41.880 --> 00:52:44.560]   but those can be used on classification tasks.
[00:52:44.560 --> 00:52:49.480]   And they either probably alter the head of the decoder,
[00:52:49.480 --> 00:52:50.760]   is my guess.
[00:52:50.760 --> 00:52:52.080]   And they're actually really good too.
[00:52:52.080 --> 00:52:54.320]   So I've actually used BART for classification tasks,
[00:52:54.320 --> 00:52:57.200]   got really great results on that as well.
[00:52:57.200 --> 00:53:00.680]   So again, when you look at these pipeline API,
[00:53:00.680 --> 00:53:05.080]   if you, after building it, do dot model name or path,
[00:53:05.080 --> 00:53:08.240]   you can see the architecture or the checkpoint
[00:53:08.240 --> 00:53:12.400]   that the pipeline model is based on.
[00:53:12.400 --> 00:53:15.520]   And then if you go to the Hugging Face Transformers docs,
[00:53:15.520 --> 00:53:19.000]   look up BART and you can see exactly how that works.
[00:53:19.000 --> 00:53:21.760]   You can actually see the code for the different checkpoints
[00:53:21.760 --> 00:53:24.000]   and to see really what's going on.
[00:53:24.000 --> 00:53:27.280]   - Awesome.
[00:53:27.280 --> 00:53:28.720]   So the next question is,
[00:53:28.720 --> 00:53:32.960]   most language model use next prediction as their objective.
[00:53:32.960 --> 00:53:34.680]   How do you come up with an objective
[00:53:34.680 --> 00:53:36.320]   like mass language models?
[00:53:36.320 --> 00:53:37.960]   Any intuition behind that?
[00:53:38.520 --> 00:53:41.560]   - Yeah, so you probably a good,
[00:53:41.560 --> 00:53:43.120]   another paper to start with,
[00:53:43.120 --> 00:53:46.000]   if you wanna kind of learn different strategies
[00:53:46.000 --> 00:53:51.000]   and how that works is,
[00:53:51.000 --> 00:53:52.880]   in terms of mass language models,
[00:53:52.880 --> 00:53:55.200]   BERT probably is the most famous
[00:53:55.200 --> 00:53:58.280]   and kind of the foundation for everything.
[00:53:58.280 --> 00:54:02.440]   And they take an approach of dynamically masking every batch
[00:54:02.440 --> 00:54:04.600]   15% of the tokens.
[00:54:04.600 --> 00:54:08.720]   And a certain percent are replaced with the mass token.
[00:54:08.720 --> 00:54:13.280]   Another percent is replaced with just a wrong token.
[00:54:13.280 --> 00:54:17.200]   And then another percentage actually keeps the same token.
[00:54:17.200 --> 00:54:20.120]   And they do that so that the model doesn't learn to cheat
[00:54:20.120 --> 00:54:23.080]   in terms of how it actually figures things out.
[00:54:23.080 --> 00:54:24.160]   And so if you read this paper,
[00:54:24.160 --> 00:54:26.800]   you'll see they've explored other strategies
[00:54:26.800 --> 00:54:28.400]   and why they went with that
[00:54:28.400 --> 00:54:31.800]   and why it turned out to be pretty successful.
[00:54:31.800 --> 00:54:36.800]   There's also a, it's the T5 paper by Google.
[00:54:36.800 --> 00:54:40.400]   They actually explore a variety
[00:54:40.400 --> 00:54:41.960]   of different masking strategies
[00:54:41.960 --> 00:54:46.040]   and what worked well for their, the T5 architecture.
[00:54:46.040 --> 00:54:47.440]   So there's different things you can do.
[00:54:47.440 --> 00:54:49.400]   You can mass tokens, you can mass words,
[00:54:49.400 --> 00:54:53.840]   you can mass like phrases, parts of the sentence.
[00:54:53.840 --> 00:54:55.320]   You can flip things around.
[00:54:55.320 --> 00:54:57.720]   You can do all kinds of things to add noise
[00:54:57.720 --> 00:55:00.160]   for the model to figure out like what,
[00:55:00.160 --> 00:55:01.320]   how to handle it.
[00:55:01.320 --> 00:55:04.480]   So the T5 has a lot of example implementations,
[00:55:04.480 --> 00:55:07.360]   but I would start with the BERT paper to kind of learn more
[00:55:07.360 --> 00:55:09.720]   since that's the, where it all kind of started.
[00:55:09.720 --> 00:55:11.760]   - Awesome.
[00:55:11.760 --> 00:55:13.840]   So there was a question earlier on YouTube
[00:55:13.840 --> 00:55:16.840]   around research is going on in lost languages
[00:55:16.840 --> 00:55:18.080]   and how we're restoring them.
[00:55:18.080 --> 00:55:19.080]   I'll take this one
[00:55:19.080 --> 00:55:21.720]   because I've interviewed someone from this.
[00:55:21.720 --> 00:55:23.800]   There's a dialect in Japanese, I believe.
[00:55:23.800 --> 00:55:26.040]   It's called Kuzushiji.
[00:55:26.040 --> 00:55:30.200]   And Taryn has been doing some amazing research
[00:55:30.200 --> 00:55:34.520]   and there's a Kaggle competition as well on this.
[00:55:34.520 --> 00:55:37.280]   So it's about using OCR to really restore
[00:55:37.280 --> 00:55:39.440]   these older dialects of Japanese,
[00:55:39.440 --> 00:55:41.000]   which I believe no one speaks today.
[00:55:41.000 --> 00:55:45.120]   So it's called K-U-Z-U-S-H-I-J-I.
[00:55:45.120 --> 00:55:46.800]   You can look that up.
[00:55:46.800 --> 00:55:48.520]   Anything to add there, Wade?
[00:55:48.520 --> 00:55:50.720]   - I'm glad I didn't have to pronounce that.
[00:55:50.720 --> 00:55:54.280]   Yeah, I don't really know too much about that,
[00:55:54.280 --> 00:55:57.120]   but that sounds like really kind of interesting application
[00:55:57.120 --> 00:55:58.440]   of these things for sure.
[00:55:59.360 --> 00:56:00.200]   - Awesome.
[00:56:00.200 --> 00:56:02.640]   No other questions that I see right now.
[00:56:02.640 --> 00:56:03.480]   - Okay.
[00:56:03.480 --> 00:56:06.320]   So with that, here's some homework.
[00:56:06.320 --> 00:56:08.720]   Read the attention is all you need paper.
[00:56:08.720 --> 00:56:09.560]   I guarantee it.
[00:56:09.560 --> 00:56:10.400]   You can read it.
[00:56:10.400 --> 00:56:15.400]   If you got questions, hit up the Discord.
[00:56:15.400 --> 00:56:18.160]   Like I said, there's plenty of people smarter than me
[00:56:18.160 --> 00:56:21.840]   that are willing to answer those questions.
[00:56:21.840 --> 00:56:24.520]   You can also look at the BERT paper that I just mentioned.
[00:56:24.520 --> 00:56:26.920]   There's the illustrated BERT Elmo
[00:56:26.920 --> 00:56:31.000]   and co or how NLP crack transfer learning from Jay Alomar
[00:56:31.000 --> 00:56:33.160]   that has a lot of nice animations and graphics
[00:56:33.160 --> 00:56:35.360]   to help explain that paper.
[00:56:35.360 --> 00:56:37.520]   And then as a third piece is the,
[00:56:37.520 --> 00:56:42.640]   I mentioned three of the FASTA Hugging Face libraries.
[00:56:42.640 --> 00:56:47.200]   Take a look at those, find one, start playing with it.
[00:56:47.200 --> 00:56:48.680]   I'd take a look at all three of them
[00:56:48.680 --> 00:56:53.200]   and find which one fits kind of your style of coding.
[00:56:53.200 --> 00:56:55.400]   They're similar, but there's also some differences.
[00:56:55.400 --> 00:56:58.000]   So check those out and kind of get a feel for like,
[00:56:58.000 --> 00:57:01.200]   you know, which one you like working with
[00:57:01.200 --> 00:57:04.600]   based on its syntax, design decisions,
[00:57:04.600 --> 00:57:07.560]   and then see if you can build a basic classification model.
[00:57:07.560 --> 00:57:09.080]   And if you look at the docs for all of them,
[00:57:09.080 --> 00:57:11.400]   you'll find examples on how to do that.
[00:57:11.400 --> 00:57:14.800]   And that's it.
[00:57:14.800 --> 00:57:18.240]   Unless there's any final questions here.
[00:57:18.240 --> 00:57:22.880]   - I think most of them are being answered.
[00:57:22.880 --> 00:57:24.200]   So I'll just take a look at them.
[00:57:24.200 --> 00:57:26.680]   So I'll just take a second to wrap up
[00:57:26.680 --> 00:57:29.520]   and just appreciate this study group.
[00:57:29.520 --> 00:57:32.640]   Really like I've not enjoyed university so much
[00:57:32.640 --> 00:57:34.640]   because it's so boring and like this,
[00:57:34.640 --> 00:57:36.320]   this makes it so much fun.
[00:57:36.320 --> 00:57:38.320]   It's almost midnight in India.
[00:57:38.320 --> 00:57:39.600]   Wade has sunlight there.
[00:57:39.600 --> 00:57:40.760]   I see Ricardo in the call.
[00:57:40.760 --> 00:57:42.360]   He also has some daylight.
[00:57:42.360 --> 00:57:43.600]   Mateo is taking notes.
[00:57:43.600 --> 00:57:44.680]   He has his headlight on.
[00:57:44.680 --> 00:57:46.960]   So it's really this global classroom
[00:57:46.960 --> 00:57:48.040]   that we're joining really.
[00:57:48.040 --> 00:57:51.160]   And please do interact with people.
[00:57:51.160 --> 00:57:52.360]   That's what makes this fun.
[00:57:52.360 --> 00:57:54.080]   Make sure you join the FASTA discord.
[00:57:54.080 --> 00:57:55.920]   Make sure you join the Hugging Face forums.
[00:57:55.920 --> 00:57:59.440]   Tag us, ask questions, discuss as much as you want.
[00:57:59.440 --> 00:58:00.600]   That's why we're really doing this.
[00:58:00.600 --> 00:58:01.840]   I'm sure Wade enjoys this.
[00:58:01.840 --> 00:58:02.880]   That's why he does this.
[00:58:02.880 --> 00:58:06.520]   And please join me in thanking Wade and Weights and Biases
[00:58:06.520 --> 00:58:08.840]   for helping us host these.
[00:58:08.840 --> 00:58:10.360]   - Yeah, thank you everybody for coming.
[00:58:10.360 --> 00:58:12.240]   Yeah, it's really a lot of fun for me.
[00:58:12.240 --> 00:58:15.080]   Like I definitely, my passion is learning
[00:58:15.080 --> 00:58:20.080]   and there's no way to develop your understanding of things,
[00:58:20.080 --> 00:58:23.080]   more so than teaching and things like this.
[00:58:23.080 --> 00:58:24.680]   And I appreciate Sanyam,
[00:58:24.680 --> 00:58:27.600]   you handling all the chat and logistics.
[00:58:27.600 --> 00:58:29.800]   So I can just focus on the easy stuff
[00:58:29.800 --> 00:58:31.840]   and I appreciate everybody for coming
[00:58:31.840 --> 00:58:33.440]   regardless of time zone.
[00:58:33.440 --> 00:58:35.400]   And yeah, thank you.
[00:58:35.400 --> 00:58:38.360]   - Awesome, thanks everyone.
[00:58:38.360 --> 00:58:40.400]   We'll see you next week at the same time.
[00:58:40.400 --> 00:58:50.400]   [BLANK_AUDIO]

