
[00:00:00.000 --> 00:00:08.000]   So you've got a new PADI submission. Let's take a look. Kaggle competition.
[00:00:08.000 --> 00:00:16.000]   By the way, it's really beautiful to see over the last week or two all these fast AI people just pop up at the top of that leaderboard.
[00:00:16.000 --> 00:00:22.000]   It's so cool. Okay. Fast AI, fast AI, fast AI, fast AI.
[00:00:22.000 --> 00:00:28.000]   Who's this person? Is this fast AI? At least the top five.
[00:00:28.000 --> 00:00:36.000]   Yeah, like most of the top five or top ten are following you in these walkthroughs.
[00:00:36.000 --> 00:00:42.000]   You've all got the same score though. Somebody's got to like, you know, Kurian's got something.
[00:00:42.000 --> 00:00:46.000]   Secret source.
[00:00:46.000 --> 00:00:54.000]   Well, I've got a few ideas I can show you guys today if you want to try and take it a bit further.
[00:00:54.000 --> 00:00:58.000]   Which I bet you do.
[00:00:58.000 --> 00:01:16.000]   Anybody have any comments or questions in the meantime?
[00:01:16.000 --> 00:01:27.000]   All right. Chair screen.
[00:01:27.000 --> 00:01:30.000]   And that's the right screen.
[00:01:30.000 --> 00:01:36.000]   And I'll move you guys onto the other screen. And now I can see.
[00:01:36.000 --> 00:01:48.000]   All right. So, Addy leaderboard.
[00:01:48.000 --> 00:01:58.000]   There we are.
[00:01:58.000 --> 00:02:07.000]   Where's Radek? Not here. Serada? I see.
[00:02:07.000 --> 00:02:27.000]   One thing that I guess it would be nice if it wasn't so sort of, I don't know, a little bit missed because I set this up in paper space, and then started running it, and then I went to bed, because it was taking so long.
[00:02:27.000 --> 00:02:47.000]   I just have a fear that if my browser sleeps or goes to sleep that it'll just basically stop the session, even though there's more hours and processor in the workstation is running.
[00:02:47.000 --> 00:02:53.000]   I wasn't sure. So, I mean, it, it shouldn't.
[00:02:53.000 --> 00:03:05.000]   But what happens is it queues up, you know, for when your browser comes back. But the problem is, there is some limit to how much it will queue.
[00:03:05.000 --> 00:03:18.000]   So, although it'll have run, if you've hit that limit, you won't see all the outputs, which is nearly just as bad.
[00:03:18.000 --> 00:03:25.000]   So, you know, there's a few things you can do.
[00:03:25.000 --> 00:03:36.000]   The most obvious one would be to use nbdev to export the notebook to a script, and then run the script in tmux.
[00:03:36.000 --> 00:03:50.000]   Because then you can close it down, come back, reattach tmux, and there it is. Okay, interesting. Now something, yeah, so maybe we'll look at that sometime.
[00:03:50.000 --> 00:04:04.000]   Yeah, something I don't, well, does paper space gradient, like you have, doesn't let your SSH in with a suitable IP? I'm not sure.
[00:04:04.000 --> 00:04:20.000]   If you've got your own GPU at home, you know, or on AWS or GCP or whatever, then what I do is I run xrdp on it, which is a remote desktop server.
[00:04:20.000 --> 00:04:33.000]   And then I can connect to it, like so, and run Firefox.
[00:04:33.000 --> 00:05:01.000]   And so this is my, yeah, this is my server's screen, you know, remote desktoping in. So if I now go in and run something.
[00:05:01.000 --> 00:05:10.000]   Hattie, I remember from last time.
[00:05:10.000 --> 00:05:27.000]   Okay, so I can set this running, and then I can close it down, go to sleep, come back the next day, reconnect to that screen, and it's still been running.
[00:05:27.000 --> 00:05:32.000]   So that's the, that's a preferred way to do it.
[00:05:32.000 --> 00:05:44.000]   But I, yeah, as I say, I don't know if it's possible on paper space gradient.
[00:05:44.000 --> 00:06:07.000]   Sorry, go on. Machines seem to have a limit of six hours that I've seen so far. If you subscribe to their pro or whatever, you can bump it up or get rid of it all together.
[00:06:07.000 --> 00:06:16.000]   So
[00:06:16.000 --> 00:06:33.000]   it's this tab here. Machine tab, you can change the auto shutdown. Okay, looks like a week so maximum. I know there's no limit there as well.
[00:06:33.000 --> 00:06:45.000]   You're paying, but I mean, you know, it's, I think it's like eight bucks a month yeah eight bucks a month. You may as well.
[00:06:45.000 --> 00:06:53.000]   Yeah, I've got the pride but I don't know when you pick a free machine. Oh, yeah.
[00:06:53.000 --> 00:07:02.000]   Right, free P 5000.
[00:07:02.000 --> 00:07:08.000]   Right.
[00:07:08.000 --> 00:07:11.000]   maximum six hours. Yep.
[00:07:11.000 --> 00:07:23.000]   So, Jeremy. Yeah, sorry to interrupt hyperspace when they're in a support channels they they talk about you can assign a public IP to a machine, and then SSH to it.
[00:07:23.000 --> 00:07:29.000]   So you could SSH and then T marks. Is that to a radiant machine though.
[00:07:29.000 --> 00:07:31.000]   Well, good question.
[00:07:31.000 --> 00:07:39.000]   Look, I'm not sure that it would be.
[00:07:39.000 --> 00:07:45.000]   No, it's not.
[00:07:45.000 --> 00:08:00.000]   And so they also have this thing called core right, which are like some more like AWS or Google servers, which absolutely lets you do a static IP.
[00:08:00.000 --> 00:08:07.000]   Even neatly I don't even know if you need static IP necessarily but you could use a dynamic IP.
[00:08:07.000 --> 00:08:09.000]   Just as well.
[00:08:09.000 --> 00:08:15.000]   Bit cheaper. The thing is, though, I reckon they're pretty expensive.
[00:08:15.000 --> 00:08:20.000]   Yeah, cool product.
[00:08:20.000 --> 00:08:25.000]   So that these are very basic GPUs.
[00:08:25.000 --> 00:08:29.000]   So that's not bad 45 cents an hour.
[00:08:29.000 --> 00:08:32.000]   I guess they're not too terrible.
[00:08:32.000 --> 00:08:38.000]   If you want to Tx, I guess they're the same price really 56 cents so I take that back.
[00:08:38.000 --> 00:08:46.000]   I guess the thing I found expensive was this CPU pricing for running it all the time.
[00:08:46.000 --> 00:08:49.000]   Yeah.
[00:08:49.000 --> 00:08:56.000]   So, Jeremy, with this RDP solution that you showed, how does that work.
[00:08:56.000 --> 00:08:59.000]   Do you have.
[00:08:59.000 --> 00:09:28.000]   Close here.
[00:09:28.000 --> 00:09:49.000]   Okay.
[00:09:49.000 --> 00:09:55.000]   So how does that work.
[00:09:55.000 --> 00:10:07.000]   I didn't get to where what computer you're already into my own GPU machine, but it could just as well be a AWS machine or GCP machine.
[00:10:07.000 --> 00:10:12.000]   This is basically the same as VNC, if you've come across the NC before.
[00:10:12.000 --> 00:10:20.000]   RDP is the kind of Microsoft version of that. I like it. Generally quite a lot better.
[00:10:20.000 --> 00:10:28.000]   And much to my surprise, the Mac client RDP is better than the Windows client ready.
[00:10:28.000 --> 00:10:35.000]   He even shows you a little mini screenshot, you know, with the screen.
[00:10:35.000 --> 00:10:41.000]   This is now finished training. No, no, no, we finished halfway through training, whatever.
[00:10:41.000 --> 00:10:50.000]   But what's this tricky to set up because you're like, you're running an index server. Not even slightly tricky to set up.
[00:10:50.000 --> 00:10:57.000]   So, yeah, you just
[00:10:57.000 --> 00:11:02.000]   it's called XRDP since it's RDP for X Windows.
[00:11:02.000 --> 00:11:13.000]   You just go after install. Yeah, I hate installing this kind of thing. It drives me crazy. But this is it. You just sudo apt install
[00:11:13.000 --> 00:11:17.000]   sudo ad user sudo system CTO restart.
[00:11:17.000 --> 00:11:28.000]   And then you might also want to run sudo system CTO enable, which will cause it to automatically start when you start your computer.
[00:11:28.000 --> 00:11:35.000]   And I don't think I, oh, you know, if you've got a firewall, you have to let it in.
[00:11:35.000 --> 00:11:44.000]   So it's port 3389. Basically this line of code and I think I did have a firewall so I also ran this.
[00:11:44.000 --> 00:11:54.000]   Yeah, that was it. It just used my username and password that I had on the machine.
[00:11:54.000 --> 00:12:02.000]   Yeah, so very surprisingly not annoying.
[00:12:02.000 --> 00:12:14.000]   And then I think I just installed Microsoft Remote Desktop from the Mac App Store or on Windows. I think it comes with Windows.
[00:12:14.000 --> 00:12:22.000]   So that was easy. Yeah, nobody says to talk about it much. People mainly talk about BNC, which is also fine, but
[00:12:22.000 --> 00:12:28.000]   I find it a bit slower and a little bit more awkward. All right.
[00:12:28.000 --> 00:12:40.000]   I mean, one weird thing, I guess, is I guess my machine, and this is pretty common,
[00:12:40.000 --> 00:12:48.000]   I haven't set up really to be a graphical workstation. I always use it from the console.
[00:12:48.000 --> 00:12:56.000]   So I actually don't really have much of a window manager here. I can't even like, I can do a little bit. I don't know.
[00:12:56.000 --> 00:13:01.000]   I don't know what the whole window manager is even using.
[00:13:01.000 --> 00:13:12.000]   But often you'll find like there is no window manager or whatever running. But, you know, a bit of Googling will show you how to app to install.
[00:13:12.000 --> 00:13:23.000]   You know, whatever. KDAE or stuff.
[00:13:23.000 --> 00:13:24.000]   Okay.
[00:13:24.000 --> 00:13:29.000]   Since we're on the installation topic, could I ask a question?
[00:13:29.000 --> 00:13:41.000]   So I think I kind of brought it up a little bit, but I can't launch FastAI, a machine that runs FastAI, and PyTorch, a PyTorch one would work.
[00:13:41.000 --> 00:13:45.000]   So what suggestions would you have?
[00:13:45.000 --> 00:13:49.000]   So that means that your prerun.sh file has got a problem.
[00:13:49.000 --> 00:13:52.000]   So maybe comment it out from PyTorch, just start it up.
[00:13:52.000 --> 00:14:01.000]   Yeah, open up your PyTorch, open up a PyTorch machine, move prerun.sh to prerun.back or something.
[00:14:01.000 --> 00:14:05.000]   Or just open it and see, like, it might be obvious what's wrong with it.
[00:14:05.000 --> 00:14:11.000]   Yeah, I couldn't see anything.
[00:14:11.000 --> 00:14:14.000]   When you say it's not working, what's like, what's not working?
[00:14:14.000 --> 00:14:18.000]   Well, it just says error when I try to start it up, just says error.
[00:14:18.000 --> 00:14:30.000]   And I tried to reach out to the paper space support a couple of times, but maybe it's a too abstract question.
[00:14:30.000 --> 00:14:37.000]   But I'll try that.
[00:14:37.000 --> 00:14:40.000]   People are putting stuff in the text chat.
[00:14:40.000 --> 00:14:49.000]   Please try to say things, verbal chat, if you can, because it's way nicer for me, and I don't have to check multiple windows.
[00:14:49.000 --> 00:14:59.000]   I know it's not possible for everybody.
[00:14:59.000 --> 00:15:09.000]   Okay, so sorry, Jeremy, there is a way to SSH into a gradient machine, but you have to trigger the virtual machine to be built from the command line.
[00:15:09.000 --> 00:15:14.000]   So you have to initiate the job and there's a space to have a GitHub repo.
[00:15:14.000 --> 00:15:18.000]   And is there any reason to do that? Like, that sounds complicated, like, would you just run a...
[00:15:18.000 --> 00:15:21.000]   It's way more effort than it's worth.
[00:15:21.000 --> 00:15:25.000]   Just run a paper space core machine if you want to, I guess.
[00:15:25.000 --> 00:15:28.000]   Yeah, exactly. So you can do it. It's just, why would you?
[00:15:28.000 --> 00:15:43.000]   So yeah, I mean, so for paper space, the issue around the notebook closing, I would like start running something, close the notebook, and then reopen it just to see what happens.
[00:15:43.000 --> 00:16:03.000]   Ghetto. And, you know, let's try it here, right?
[00:16:03.000 --> 00:16:08.000]   Now, what was that thing we learned the other day? It was shifting.
[00:16:08.000 --> 00:16:14.000]   Then go to the other one.
[00:16:14.000 --> 00:16:18.000]   Oh, that was my one. Okay, I got to learn how to...
[00:16:18.000 --> 00:16:20.000]   Hey, Jeremy. Yeah.
[00:16:20.000 --> 00:16:29.000]   Can you... Are you using I-term too? Because you can do tmux minus cc and you'll get native windows in tmux instead of the little sort of terminal ones.
[00:16:29.000 --> 00:16:31.000]   That sounds interesting. Let me try that.
[00:16:31.000 --> 00:16:36.000]   Yeah, I'm addicted to that. It's awesome.
[00:16:36.000 --> 00:16:43.000]   Minus capital capital, minus capital cc.
[00:16:43.000 --> 00:16:46.000]   Unknown option C. Does that have to be before the A?
[00:16:46.000 --> 00:16:50.000]   Yeah, so it'll be tmux minus capital capital. Yeah, there you go.
[00:16:50.000 --> 00:16:54.000]   Okay.
[00:16:54.000 --> 00:16:57.000]   And what are the benefits of this approach?
[00:16:57.000 --> 00:17:03.000]   They're native windows. You can click and drag them and move them around, pop them out. Yeah, all that stuff.
[00:17:03.000 --> 00:17:16.000]   You can click and drag tmux windows as well.
[00:17:16.000 --> 00:17:25.000]   Okay, this is all the same as... Like, if you've got to have mouse mode on for them to work...
[00:17:25.000 --> 00:17:33.000]   If the shortcut is like Command + Shift + D will split panes. You don't have to go into I think it's a colon or something and command something. It's just like less dimmy.
[00:17:33.000 --> 00:17:36.000]   You just use Control + B.
[00:17:36.000 --> 00:17:38.000]   Maybe it's exactly the same.
[00:17:38.000 --> 00:17:40.000]   Yeah.
[00:17:40.000 --> 00:17:42.000]   I mean, you'll have the same shortcuts.
[00:17:42.000 --> 00:17:46.000]   Control + B doesn't work anymore, so what about tmux shortcuts are not going to work anymore? How do I detect now?
[00:17:46.000 --> 00:17:51.000]   Yeah, I think they're different. Escape, I think, or...
[00:17:51.000 --> 00:18:04.000]   If you go back to the original window that launched it, it'll have like a...
[00:18:04.000 --> 00:18:06.000]   Okay.
[00:18:06.000 --> 00:18:08.000]   Okay.
[00:18:08.000 --> 00:18:20.000]   Yeah, I'm not convinced it's going to help my workflow, but I think, yeah, for people who are more familiar with tmux shortcuts, that could be cool.
[00:18:20.000 --> 00:18:24.000]   Thanks for the tip.
[00:18:24.000 --> 00:18:31.000]   What's going on down here? It's really good.
[00:18:31.000 --> 00:18:49.000]   The trick to get mouse support working, so, for example, my scroll wheel, as you can see, works nicely in this normal tmux window, is to...
[00:18:49.000 --> 00:19:01.000]   Have a .tmux.conf file that contains set option minus G mouse on.
[00:19:01.000 --> 00:19:04.000]   And then you can also increase your history limit.
[00:19:04.000 --> 00:19:08.000]   And, yeah, that's how come I can scroll.
[00:19:08.000 --> 00:19:24.000]   I think the thing like, you know, or a thing I like about tmux is it's very integrated with my kind of the normal way of doing things in Unix, you know?
[00:19:24.000 --> 00:19:35.000]   So, for example, if I want to search through my previous session, I could just hit question mark to search up, and I could search for makefile, for example.
[00:19:35.000 --> 00:19:42.000]   And I, you know, hit N, just like I would in vim, hit slash to look forwards.
[00:19:42.000 --> 00:19:53.000]   You know, it's like my terminal works the same way as vim or whatever, which I, yeah, which I really like.
[00:19:53.000 --> 00:20:05.000]   And I think, yeah, that way I don't have to know like, oh, the I-term sort of shortcuts and some other sort of shortcuts. It's just this kind of like general Unix-y way of doing things, I guess.
[00:20:05.000 --> 00:20:15.000]   And, of course, they'll also all work on the paper space terminal as well.
[00:20:15.000 --> 00:20:22.000]   Yeah, so let's try this.
[00:20:22.000 --> 00:20:30.000]   So if we start running this.
[00:20:30.000 --> 00:20:32.000]   Okay.
[00:20:32.000 --> 00:20:38.000]   Close that.
[00:20:38.000 --> 00:20:41.000]   Leave it for a few seconds.
[00:20:41.000 --> 00:20:51.000]   And you can see here it says in my console, starting buffering.
[00:20:51.000 --> 00:21:08.000]   So it's remembering things that were sent to me. So if I click now back here, there we go. It's, let's see.
[00:21:08.000 --> 00:21:12.000]   Hmm.
[00:21:12.000 --> 00:21:15.000]   That didn't seem to work, did it? That's interesting.
[00:21:15.000 --> 00:21:23.000]   Okay, so let's try something different. So I don't think you can just close it and reopen it.
[00:21:23.000 --> 00:21:31.000]   Let's try something else.
[00:21:31.000 --> 00:21:39.000]   What if we fake a network disconnection by closing SSH?
[00:21:39.000 --> 00:21:50.000]   Okay, so now, all right, connections failed. So I'll leave that window open, and then we reconnect.
[00:21:50.000 --> 00:21:54.000]   And,
[00:21:54.000 --> 00:22:10.000]   yep, okay, so that worked. So there's some of her answer. But yeah, I think there's something now, if you leave it long enough, it says I've stopped listening for events because there's been too many and tells you there's some configuration option you can change to make it bigger.
[00:22:10.000 --> 00:22:17.000]   Should probably be a useful thing to know about.
[00:22:17.000 --> 00:22:32.000]   Let me just go and turn this alarm off. Hang on.
[00:22:32.000 --> 00:23:01.000]   Okay.
[00:23:01.000 --> 00:23:09.000]   Okay.
[00:23:09.000 --> 00:23:38.000]   Yeah.
[00:23:38.000 --> 00:23:57.000]   Sorry about that.
[00:23:57.000 --> 00:24:18.000]   My daughter likes to be permanently entertained so any gaps in her homeschooling schedule.
[00:24:18.000 --> 00:24:30.000]   She likes to be amused. She doesn't like the fact that I'm doing this and Rachel's a CrossFit.
[00:24:30.000 --> 00:24:34.000]   Okay.
[00:24:34.000 --> 00:24:43.000]   So we had a look the other day at progressive resizing, right.
[00:24:43.000 --> 00:24:56.000]   And so this is where I got to, I think like progressive resizing one interesting thing you can do is like you can go crazy like you can go extra lunch.
[00:24:56.000 --> 00:25:11.000]   And, you know, we start out with some teeny tiny images and train for a while.
[00:25:11.000 --> 00:25:32.000]   And then combine that with gradient accumulation to then go up to big images that don't have to train so long.
[00:25:32.000 --> 00:25:51.000]   I think this is a good trick for probably particularly for code competitions on Kaggle where you've got serious resource constraints, you know, or just wanting to do more with less time.
[00:25:51.000 --> 00:26:06.000]   So I think, yeah, Kaggle you would have needed accumulation level of four rather than two to make this fit because they've got 16 gig cards we also have a 24 gig card.
[00:26:06.000 --> 00:26:11.000]   So then something else that
[00:26:11.000 --> 00:26:20.000]   then we started talking about was weighted models.
[00:26:20.000 --> 00:26:29.000]   That's weird. What happened to my weighted model?
[00:26:29.000 --> 00:26:35.000]   Did I move it to course 22?
[00:26:35.000 --> 00:26:42.000]   Well, that's fine.
[00:26:42.000 --> 00:26:57.000]   So the question I think we had yesterday was about unbalanced data sets and would it be a good idea to balance our data set.
[00:26:57.000 --> 00:27:18.000]   So let's start with a nice small model
[00:27:18.000 --> 00:27:26.000]   to use as a base case, something we've done before.
[00:27:26.000 --> 00:27:29.000]   Con next.
[00:27:29.000 --> 00:27:37.000]   Okay, let's use this one.
[00:27:37.000 --> 00:27:46.000]   So actually there's no point copying progressive, I guess. Let's copy
[00:27:46.000 --> 00:27:59.000]   small models.
[00:27:59.000 --> 00:28:03.000]   Okay.
[00:28:03.000 --> 00:28:13.000]   Rename and so this is going to be for weighted.
[00:28:13.000 --> 00:28:23.000]   Okay.
[00:28:23.000 --> 00:28:43.000]   So the resizing that we needed on my machine but since we'll be putting it on Kaggle and as well.
[00:28:43.000 --> 00:28:54.000]   Okay, so that's going to be our base case.
[00:28:54.000 --> 00:29:14.000]   So for weighting, we can df.label.value accounts.
[00:29:14.000 --> 00:29:21.000]   So there's our level of unbalancedness.
[00:29:21.000 --> 00:29:26.000]   So it's not too bad. There's a lot of normals, a lot of blasts.
[00:29:26.000 --> 00:29:33.000]   Not many of these are bacterial thingies.
[00:29:33.000 --> 00:29:38.000]   Nick, I don't know if you're around. I mean, I can see you are around. I don't know if you're able to talk.
[00:29:38.000 --> 00:29:42.000]   But if you are, you might be able to tell us about what you found because I know you've been looking at these,
[00:29:42.000 --> 00:29:46.000]   which of these are hard to kind of visually see the difference between.
[00:29:46.000 --> 00:29:52.000]   Yeah, yeah, for sure. I'm sorry. I dropped out earlier because we had a power cut here, but I'm back now.
[00:29:52.000 --> 00:29:54.000]   So I'm intentionally video lists or.
[00:29:54.000 --> 00:29:59.000]   I am. I'm not intentionally video list, but that's that's the the break at the moment. Sorry about that.
[00:29:59.000 --> 00:30:10.000]   But yeah, like one thing that I did just to, I guess, get a better handle on the data set was going through them and having a look at the different types.
[00:30:10.000 --> 00:30:17.000]   It's really hard to pick even what what the difference was between a normal image and, you know, say like Downey mildew or whatever.
[00:30:17.000 --> 00:30:27.000]   It could be quite hard to pick out. And so one thing I thought it would be fun to do was to almost like segment or mask the images playing with the color channel to see if they would come out a bit better.
[00:30:27.000 --> 00:30:35.000]   And then when I did that, I was able to take kind of, I guess, the yellow dead bits or disease parts and I could see them better when they were, you know, like in bright red.
[00:30:35.000 --> 00:30:53.000]   And the thing is, is that so many of these like when I found like when I've trained them, I find that there is a handful of a handful of images really like like 20 to 25 images that are very difficult to classify.
[00:30:53.000 --> 00:31:04.000]   And it tends to be these actually from these imbalance classes where it tends to categorize them as blast when it's not. And I think you're all ones tend to get.
[00:31:04.000 --> 00:31:10.000]   Yeah, in fact, let me just pull up in one of my notebooks share your screen.
[00:31:10.000 --> 00:31:17.000]   Yeah, let me see if I like when you look at this. Are you able to see.
[00:31:17.000 --> 00:31:24.000]   But it helped to make these bigger. Are you able to see the disease and these because I don't know what I'm looking for.
[00:31:24.000 --> 00:31:49.000]   Yeah.
[00:31:49.000 --> 00:31:53.000]   How do we make this bigger.
[00:31:53.000 --> 00:32:04.000]   Probably there's like a figure size in that plot lib isn't there that plot lib. So like a big size, big size.
[00:32:04.000 --> 00:32:07.000]   Yes.
[00:32:07.000 --> 00:32:13.000]   Big signs equals.
[00:32:13.000 --> 00:32:15.000]   I don't know.
[00:32:15.000 --> 00:32:18.000]   Which way around is it.
[00:32:18.000 --> 00:32:39.000]   We can't hear you by the way, Nick. I don't know if you if we lost you.
[00:32:39.000 --> 00:32:55.000]   I also tried to look into the image using the confusion matrix and then the most loss to put it over. It's just too hard. It's beyond my domain and I was planning to do that today, actually.
[00:32:55.000 --> 00:32:58.000]   So that's yeah.
[00:32:58.000 --> 00:33:02.000]   I don't know what happened to Nick. Maybe he's having some internet problems again.
[00:33:02.000 --> 00:33:09.000]   I wonder if it's just like red spots or something.
[00:33:09.000 --> 00:33:20.000]   So, yeah, I mean, it's an anyway. It's interesting that Nick said he found these ones difficult. So, yeah, there's basically two reasons to to wait different rows differently.
[00:33:20.000 --> 00:33:34.000]   One is that some of them are harder and that you want them to be shown more often to give the computer more of a chance to learn them. And the other is some are less common and same thing.
[00:33:34.000 --> 00:33:56.000]   So, you know, one possible waiting for these would be to take their reciprocal. And so then, you know, normal is going to be shown less often if we wait all the normal ones by this amount and all the bacterial, panical, blight ones, this amount, you're going to get more of these.
[00:33:56.000 --> 00:34:15.000]   So that's like one approach we could use. I feel like that might be overkill. So I'd be inclined to kind of like not do it quite that much. So like another approach would be to like take the square root, maybe one over the square root, kind of like that.
[00:34:15.000 --> 00:34:22.000]   So then these are going to be shown about twice as often as these, you know.
[00:34:22.000 --> 00:34:30.000]   So maybe like let's start with trying this as our set of waitings.
[00:34:30.000 --> 00:34:35.000]   Jeremy, if I could ask a question at this point.
[00:34:35.000 --> 00:34:44.000]   So the waiting and when you talk about waiting such that images are shown more or less often.
[00:34:44.000 --> 00:35:03.000]   I wonder in cases where it's very imbalanced, whether that could lead to some classes being overfitted to because the model learns about the images themselves, I came across in looking at classification.
[00:35:03.000 --> 00:35:13.000]   Yeah. And whether there was a way to, I read about how to deal with imbalances.
[00:35:13.000 --> 00:35:26.000]   And I've seen some recommendations to try to wait when calculating the losses, rather than resampling the input. So I just wondered whether it was possible.
[00:35:26.000 --> 00:35:42.000]   It's different, right? So in the end, you want it to be able to recognize the features of the images you care about. And there's no substitute for like having them see the images enough times to recognize them.
[00:35:42.000 --> 00:35:55.000]   However, when it does that, it is then going to, because it sees the rare cases more often, it's going to think that those rare cases are more probable than they actually are.
[00:35:55.000 --> 00:36:02.000]   So you have to reverse that then when you make predictions.
[00:36:02.000 --> 00:36:13.000]   So that's, yeah, that's something to be careful of. So I mean, I think it probably just helped to try to take a look at it to see what that looks like.
[00:36:13.000 --> 00:36:18.000]   So yeah, so here's our waits, right?
[00:36:18.000 --> 00:36:34.000]   I would be inclined to probably, can we merge things directly? Let's take a look. So if I go df.merge, which is kind of like a way of doing a join in pandas.
[00:36:34.000 --> 00:36:42.000]   And the right hand side, yeah, the right hand side can be a series. Cool. So merge on waits.
[00:36:42.000 --> 00:36:50.000]   What does that look like? Nope. Why not?
[00:36:50.000 --> 00:36:55.000]   And then, okay.
[00:36:55.000 --> 00:37:07.000]   Left. I see. So left. Okay, so on. Left. Left on. Left on equals label.
[00:37:07.000 --> 00:37:18.000]   And right. I think that's called the index. I'm not a pandas expert. I don't know if anybody is.
[00:37:18.000 --> 00:37:30.000]   There we go. Okay, so that's added these waits here.
[00:37:30.000 --> 00:37:36.000]   Given the slightly weird name, but that's okay.
[00:37:36.000 --> 00:37:42.000]   So if we called that way to df.
[00:37:42.000 --> 00:37:52.000]   And so then.
[00:37:52.000 --> 00:38:00.000]   We could take out a little function and move them over here.
[00:38:00.000 --> 00:38:08.000]   And I think what we want to do is use data blocks at this point.
[00:38:08.000 --> 00:38:17.000]   It's often a good idea.
[00:38:17.000 --> 00:38:22.000]   And we have a data blocks version.
[00:38:22.000 --> 00:38:28.000]   Certainly make one otherwise.
[00:38:28.000 --> 00:38:35.000]   Okay, here's a data block.
[00:38:35.000 --> 00:38:41.000]   So let's get a data block.
[00:38:41.000 --> 00:38:51.000]   Got an image block and a category block.
[00:38:51.000 --> 00:38:57.000]   Get why is parent label.
[00:38:57.000 --> 00:39:06.000]   Okay, item transforms is.
[00:39:06.000 --> 00:39:13.000]   Item transforms is this. Jeremy, I think you're in the wrong book should be weighted.
[00:39:13.000 --> 00:39:16.000]   Thank you.
[00:39:16.000 --> 00:39:17.000]   Thank you.
[00:39:17.000 --> 00:39:35.000]   Yes, I had these here but thank you. Okay.
[00:39:35.000 --> 00:39:40.000]   Okay, and batch transforms.
[00:39:40.000 --> 00:39:47.000]   Let's use the same ones we had here to make it fair.
[00:39:47.000 --> 00:39:49.000]   Okay.
[00:39:49.000 --> 00:39:55.000]   So there's our data block.
[00:39:55.000 --> 00:40:01.000]   We actually use this resizing.
[00:40:01.000 --> 00:40:22.000]   Okay, Jeremy. Yeah, sorry, sorry to interrupt there. So this approach is we're going to use the data block to even the numbers of what's being sampled so that we can more augmentations of the same images for the lower represented samples or
[00:40:22.000 --> 00:40:40.000]   relative. So, it's nothing to do with the data block, we're going to use things called weighted data loaders, and the way to data loader is going to use these numbers here to as as basically like probabilities of how likely it is to pick that
[00:40:40.000 --> 00:40:43.000]   row, but it grabs a row in a batch.
[00:40:43.000 --> 00:40:50.000]   Yeah, I was going to add them all up and do each of these divided by the sum so they're allowed to one.
[00:40:50.000 --> 00:40:59.000]   So what we need to data blocks is because the weighted data loaders method is a method of data block.
[00:40:59.000 --> 00:41:11.000]   It's not something we get in the, you know, quick and dirty image data loaders thing that doesn't have as much flexibility. So now that we've got a data block we can type that day block dot.
[00:41:11.000 --> 00:41:21.000]   Import it import fast.ai dot call back dot.
[00:41:21.000 --> 00:41:23.000]   What was it in again.
[00:41:23.000 --> 00:41:29.000]   I don't remember fast.ai weighted data loader.
[00:41:29.000 --> 00:41:37.000]   It's a data callback.
[00:41:37.000 --> 00:41:50.000]   Oh, okay, so that's it's a, it's actually a method of data sets.
[00:41:50.000 --> 00:41:56.000]   So we can get a data sets object from a data block.
[00:41:56.000 --> 00:41:59.000]   Like so.
[00:41:59.000 --> 00:42:11.000]   And we pass in source. So that would be our list of image files.
[00:42:11.000 --> 00:42:19.000]   So we can files equals get image files.
[00:42:19.000 --> 00:42:31.000]   In our training set, pass those in and there's our training set and there's a validation set.
[00:42:31.000 --> 00:42:37.000]   So they're data sets. So these are the things that remember we can index into and get a single xy pair.
[00:42:37.000 --> 00:42:49.000]   And so weighted data loaders is then something we can pass data sets to and give it weights and a batch size.
[00:42:49.000 --> 00:42:51.000]   Okay.
[00:42:51.000 --> 00:42:55.000]   And the weights are for the training set.
[00:42:55.000 --> 00:43:03.000]   Okay, we're gonna have to be careful about this.
[00:43:03.000 --> 00:43:08.000]   So we should go to DSS dot weighted data loaders.
[00:43:08.000 --> 00:43:12.000]   And so the source code.
[00:43:12.000 --> 00:43:14.000]   Yes, it calls.
[00:43:14.000 --> 00:43:18.000]   I said to do.
[00:43:18.000 --> 00:43:31.000]   Which is here.
[00:43:31.000 --> 00:43:33.000]   Okay.
[00:43:33.000 --> 00:43:43.000]   What's called weights.
[00:43:43.000 --> 00:43:55.000]   All right, I'm not 100% sure how this is going to work but let's try it.
[00:43:55.000 --> 00:44:01.000]   So our weighted data frame.
[00:44:01.000 --> 00:44:13.000]   So this is the weight for each row, right.
[00:44:13.000 --> 00:44:16.000]   And then we've got our files.
[00:44:16.000 --> 00:44:29.000]   Yeah, we've got to be a bit careful here, right, because they're in different orders.
[00:44:29.000 --> 00:44:44.000]   So we actually need a way to get a list of weights where the two orders are going to match each other.
[00:44:44.000 --> 00:44:58.000]   And you do it by key lookup. Can you put again we could do it by key lookup. I'm actually thinking of something a little lazier, which is just to sort them both.
[00:44:58.000 --> 00:45:05.000]   Okay, so
[00:45:05.000 --> 00:45:10.000]   although
[00:45:10.000 --> 00:45:18.000]   I don't have what's going on here.
[00:45:18.000 --> 00:45:26.000]   Doesn't have them all.
[00:45:26.000 --> 00:45:34.000]   Are they not contiguous.
[00:45:34.000 --> 00:45:40.000]   So values by image ID.
[00:45:40.000 --> 00:45:42.000]   They are contiguous.
[00:45:42.000 --> 00:45:48.000]   So where is image 100001.
[00:45:48.000 --> 00:45:52.000]   The sorting must be by folder Thursday.
[00:45:52.000 --> 00:45:55.000]   Yes, of course, that's exactly what it is.
[00:45:55.000 --> 00:45:58.000]   Thank you.
[00:45:58.000 --> 00:46:03.000]   Okay.
[00:46:03.000 --> 00:46:08.000]   So we could use a key.
[00:46:08.000 --> 00:46:16.000]   That looks hopeful it says here if the key is a string use attribute getters so I think I can just pass in the key name.
[00:46:16.000 --> 00:46:20.000]   Ah, that is magic.
[00:46:20.000 --> 00:46:26.000]   That is the magic of fast call right there.
[00:46:26.000 --> 00:46:30.000]   There we go. So that's sorting by name.
[00:46:30.000 --> 00:46:38.000]   And we can do the same thing for this one.
[00:46:38.000 --> 00:46:41.000]   Like so.
[00:46:41.000 --> 00:46:54.000]   And so now they're sorted by the same thing. So that's a good step.
[00:46:54.000 --> 00:47:06.000]   So the weights are basically WDF dot label Y.
[00:47:06.000 --> 00:47:18.000]   Now that's a pandas series, which
[00:47:18.000 --> 00:47:25.000]   Yes to numpy would turn it into an array.
[00:47:25.000 --> 00:47:29.000]   That is not quite sure whether this has to be just for the training set or is for both.
[00:47:29.000 --> 00:47:31.000]   We'll find out in a moment.
[00:47:31.000 --> 00:47:39.000]   If I run that, it doesn't like it.
[00:47:39.000 --> 00:47:48.000]   That's interesting.
[00:47:48.000 --> 00:47:58.000]   Of course, so the batch transforms actually didn't end up getting applied, because
[00:47:58.000 --> 00:48:10.000]   we use data sets which doesn't apply batch transforms. So we would need to now apply them here.
[00:48:10.000 --> 00:48:12.000]   So that's quite confusing.
[00:48:12.000 --> 00:48:23.000]   So presumably, I don't see it here but I would expect to be able to go batch transforms at this point.
[00:48:23.000 --> 00:48:28.000]   This is all quite awkward, isn't it?
[00:48:28.000 --> 00:48:31.000]   So data loader
[00:48:31.000 --> 00:48:34.000]   keyword arguments
[00:48:34.000 --> 00:48:39.000]   equals
[00:48:39.000 --> 00:48:41.000]   batch.
[00:48:41.000 --> 00:48:49.000]   So if we're creating a data loader, a weighted data loader.
[00:48:49.000 --> 00:49:00.000]   You know what would be a good idea would probably be to look at the data block data loaders source code to see how that does it.
[00:49:00.000 --> 00:49:06.000]   Data sets data loaders. Here we go. After underscore batch
[00:49:06.000 --> 00:49:11.000]   is what it is.
[00:49:11.000 --> 00:49:29.000]   After underscore batch
[00:49:29.000 --> 00:49:33.000]   that's not it.
[00:49:33.000 --> 00:49:50.000]   Let's see.
[00:49:50.000 --> 00:49:59.000]   Okay, it's calling data loaders passing in the keyword arguments and
[00:49:59.000 --> 00:50:23.000]   Okay, data loaders does not call it after batch.
[00:50:23.000 --> 00:50:29.000]   That's dot data sets.
[00:50:29.000 --> 00:50:35.000]   Yeah, so okay, so data sets dot data loaders is this thing here, and that doesn't equal it after underscore batch.
[00:50:35.000 --> 00:51:03.000]   Oh, and I think I know why I think that's because when we look the other day at data block,
[00:51:03.000 --> 00:51:11.000]   we noticed that it like adds
[00:51:11.000 --> 00:51:20.000]   Oh, yes, yes, yes, the image block
[00:51:20.000 --> 00:51:24.000]   that adds int to float tensor
[00:51:24.000 --> 00:51:29.000]   as a batch transform.
[00:51:29.000 --> 00:51:43.000]   So we might need to add that as well.
[00:51:43.000 --> 00:51:46.000]   Okay.
[00:51:46.000 --> 00:51:52.000]   So it's getting pale images.
[00:51:52.000 --> 00:52:13.000]   So the fact is getting pale images means it's never been converted to a tensor.
[00:52:13.000 --> 00:52:22.000]   So data block. I think there's something that calls to tensor or something at some point.
[00:52:22.000 --> 00:52:26.000]   Oh, there is here item transforms.
[00:52:26.000 --> 00:52:30.000]   So why isn't that getting called
[00:52:30.000 --> 00:52:35.000]   because
[00:52:35.000 --> 00:52:44.000]   Oh, item transforms, I think, are also done at the data loaders stage.
[00:52:44.000 --> 00:52:47.000]   Item transforms. Let's see.
[00:52:47.000 --> 00:52:54.000]   I am transforms.
[00:52:54.000 --> 00:52:57.000]   Yes, that's also done. Okay.
[00:52:57.000 --> 00:53:05.000]   So basically, using data sets instead of data loaders is quite awkward.
[00:53:05.000 --> 00:53:12.000]   I think we need to fix this in fast AI because
[00:53:12.000 --> 00:53:22.000]   yes, it's not being done for us. But you know, what we could do actually is what we could do
[00:53:22.000 --> 00:53:29.000]   is the same thing that
[00:53:29.000 --> 00:53:36.000]   data block does, which is just to use these self dot item transforms and self dot batch transforms.
[00:53:36.000 --> 00:53:43.000]   So if we have a look at our data block.
[00:53:43.000 --> 00:53:47.000]   Oops, Daisy.
[00:53:47.000 --> 00:53:56.000]   Okay, I think this is all going to become clear in a moment. Hopefully it's got these item transforms in it.
[00:53:56.000 --> 00:54:02.000]   And it's got these batch transforms in it.
[00:54:02.000 --> 00:54:09.000]   And so what we actually want to do when we create a data loaders is say that after batch is
[00:54:09.000 --> 00:54:16.000]   whatever the data block says the batch transforms are and after item.
[00:54:16.000 --> 00:54:22.000]   Is whatever the data block.
[00:54:22.000 --> 00:54:26.000]   Says the item transforms are.
[00:54:26.000 --> 00:54:28.000]   Okay, that's ugly.
[00:54:28.000 --> 00:54:31.000]   So that's something I think we should try to make easier.
[00:54:31.000 --> 00:54:39.000]   So hopefully by the time people see this video, this will all be easier.
[00:54:39.000 --> 00:54:50.000]   So there's some data loaders.
[00:54:50.000 --> 00:54:57.000]   Okay.
[00:54:57.000 --> 00:55:07.000]   So my guess is that here is we've given the wrong number of weights. I'm guessing this needs to be weights just for the training set.
[00:55:07.000 --> 00:55:17.000]   So the way I would check this is I would type percent debug and that puts us into the Python debugger and the Python debugger is a very, very cool thing.
[00:55:17.000 --> 00:55:37.000]   It's called PDB and definitely want to know how to use it. H gives you the help and W shows you where in the stack you are.
[00:55:37.000 --> 00:55:40.000]   So you can see this is the line of code I'm about to run.
[00:55:40.000 --> 00:55:54.000]   And so I can print out with P self dot n and I can print out with P self dot weights and I can you don't actually normally need to even say P.
[00:55:54.000 --> 00:55:58.000]   It just assumes that so I can just say soft or it's touch shape.
[00:55:58.000 --> 00:56:08.000]   And so there's the problem. So it's expecting eight thousand three hundred and twenty six weights, not ten thousand four hundred and seven weights.
[00:56:08.000 --> 00:56:15.000]   And so that's because and to be fair, the documentation warned us about this.
[00:56:15.000 --> 00:56:27.000]   It's expecting weights just for the training set, not for both training and validation sets.
[00:56:27.000 --> 00:56:37.000]   Okay, no problem.
[00:56:37.000 --> 00:56:45.000]   Could you pre-determine you split both by adding another column in you in the same data set there to put the weights in?
[00:56:45.000 --> 00:56:48.000]   Yeah, I could do that.
[00:56:48.000 --> 00:56:52.000]   But actually and somebody actually asked about this the other day.
[00:56:52.000 --> 00:56:55.000]   This is our training set.
[00:56:55.000 --> 00:57:01.000]   And items tells you the the file names, actually.
[00:57:01.000 --> 00:57:10.000]   So we just need to look each of these up.
[00:57:10.000 --> 00:57:15.000]   In the data frame.
[00:57:15.000 --> 00:57:19.000]   So what we could do is we could say weights.
[00:57:19.000 --> 00:57:24.000]   Equals.
[00:57:24.000 --> 00:57:28.000]   And so we could go through each of those. So that's going to be all of our files.
[00:57:28.000 --> 00:57:34.000]   And then we need to.
[00:57:34.000 --> 00:57:37.000]   Look up the image ID.
[00:57:37.000 --> 00:57:46.000]   And I think something you could possibly do here is.
[00:57:46.000 --> 00:57:56.000]   Set the index to image ID.
[00:57:56.000 --> 00:58:02.000]   Which is this kind of pandas idea WTF equals.
[00:58:02.000 --> 00:58:06.000]   And then.
[00:58:06.000 --> 00:58:10.000]   We say.
[00:58:10.000 --> 00:58:14.000]   Location of one.
[00:58:14.000 --> 00:58:18.000]   One dot JPG.
[00:58:18.000 --> 00:58:21.000]   There it is.
[00:58:21.000 --> 00:58:27.000]   For label Y.
[00:58:27.000 --> 00:58:33.000]   There it is. So.
[00:58:33.000 --> 00:58:38.000]   If we copy that over to here.
[00:58:38.000 --> 00:58:46.000]   And replace that with our.
[00:58:46.000 --> 00:58:51.000]   Oh, oh, don't name.
[00:58:51.000 --> 00:58:54.000]   Look at that.
[00:58:54.000 --> 00:59:03.000]   Okay, so.
[00:59:03.000 --> 00:59:15.000]   Okay, so we don't want to sort values. We want to set index. I should probably take more use make more use of indices in pandas. I guess I still don't have a great sense in my head of quite how they work.
[00:59:15.000 --> 00:59:20.000]   So I tend to under use them. Okay, so weights should now be the right length.
[00:59:20.000 --> 00:59:30.000]   For the training set. Okay, so now.
[00:59:30.000 --> 00:59:33.000]   Our weights here.
[00:59:33.000 --> 00:59:44.000]   It's just weights.
[00:59:44.000 --> 00:59:55.000]   Cool. And then what I'd be inclined to do is to do a few more.
[00:59:55.000 --> 01:00:08.000]   And what I find encouraging here is that we've got a lot of bacterials.
[01:00:08.000 --> 01:00:20.000]   But, yeah, you know, this seems like a good mix, right?
[01:00:20.000 --> 01:00:23.000]   So then.
[01:00:23.000 --> 01:00:33.000]   We should just be able to.
[01:00:33.000 --> 01:00:38.000]   Add those to a learner.
[01:00:38.000 --> 01:00:45.000]   Fine tune for five epochs.
[01:00:45.000 --> 01:00:56.000]   All right, sorry that was a bit more awkward than I would have liked and definitely used a whole bunch of concepts which we haven't covered before.
[01:00:56.000 --> 01:01:01.000]   So don't worry if you're feeling lost about the implementation here.
[01:01:01.000 --> 01:01:07.000]   Yeah, I mean, just about the how the sampling works.
[01:01:07.000 --> 01:01:11.000]   We've got weights and that's creating.
[01:01:11.000 --> 01:01:15.000]   How is that actually sampled from the training set is it.
[01:01:15.000 --> 01:01:22.000]   Do we have a number of rows or number of images that we're trying to create a sample.
[01:01:22.000 --> 01:01:28.000]   Yeah, so what happens is it creates it creates batches. So each batch will have 64 things in.
[01:01:28.000 --> 01:01:33.000]   And so it's going to grab at random 64 images.
[01:01:33.000 --> 01:01:40.000]   But it's a weighted random sample where each row is weighted by this.
[01:01:40.000 --> 01:01:43.000]   This weight.
[01:01:43.000 --> 01:01:57.000]   And so an epochs not exactly an epoch anymore in that it won't necessarily see every image once an epochs and epochs just equal to the total number of rows in the data set is how many rows I've seen.
[01:01:57.000 --> 01:02:04.000]   But, you know, we'll see a lot of the less common ones multiple times.
[01:02:04.000 --> 01:02:10.000]   And so there's a definite danger of overfitting.
[01:02:10.000 --> 01:02:15.000]   The weighted sampling is not done for the validation set.
[01:02:15.000 --> 01:02:32.000]   So we should be able to compare these. Let's take a look. So five point six versus four point six. Now, you know, this is expected that where this might be interesting would be like.
[01:02:32.000 --> 01:02:53.000]   Do all of our training and then maybe at the very end do a few epochs with weighted training, you know, at the point that it's already really good just to show it a few more examples of the less common ones or just train it for longer with more data augmentation.
[01:02:53.000 --> 01:03:12.000]   But yeah, I mean, you know, you would expect the error rate at this point to be worse, I think, because the most common types, which it's particularly want to care about because they're the ones that's going to have mainly in the training set, it hasn't seen very much.
[01:03:12.000 --> 01:03:15.000]   So the overall error has gone down.
[01:03:15.000 --> 01:03:23.000]   But yeah, I think you like it. It might they may well be ways to to use this.
[01:03:23.000 --> 01:03:35.000]   I'm Jerry. Yeah, it's possible you could quickly explain where the deficiency was in this random weighted API, how you would prefer that to look like you said you.
[01:03:35.000 --> 01:03:54.000]   Oh, yeah, sure. Fix it up later. But I mean, I think I think the way this ought to look would be that I can say deals equals D block dot weighted data loader like that.
[01:03:54.000 --> 01:03:57.000]   In fact, you know, we could we could fix it up now.
[01:03:57.000 --> 01:04:10.000]   The reusing the existing after batch and after items already. And yeah, we could we can fix it up now if you're interested. Yeah, I'd love to see how to commit a change.
[01:04:10.000 --> 01:04:21.000]   So, you know, the first thing I'd do before I change the fast AI library is make sure I've got the latest version of it by doing a get pool because nobody likes conflicts.
[01:04:21.000 --> 01:04:38.000]   All right, it's up to date. So then I would go into the notebooks and it was in the data callbacks to call back data.
[01:04:38.000 --> 01:04:55.000]   And so here's where the data loaders. Jeremy, is this a bit of a silly question? But is it a callback or is it just kind of like a transform within the actual data block?
[01:04:55.000 --> 01:05:01.000]   Should it be if you send weights to a data block, then it just does it.
[01:05:01.000 --> 01:05:09.000]   Is it a call back?
[01:05:09.000 --> 01:05:13.000]   No, it's not a callback.
[01:05:13.000 --> 01:05:18.000]   So it's it's in a strange place.
[01:05:18.000 --> 01:05:21.000]   It's not a callback.
[01:05:21.000 --> 01:05:32.000]   What it is, it's a data loader, actually, and a patch to data sets.
[01:05:32.000 --> 01:05:35.000]   So there's a.
[01:05:35.000 --> 01:05:48.000]   You know, something I like very much in fast core called patch, which is allows us to add a method with this name to this class.
[01:05:48.000 --> 01:06:07.000]   And I want to add something to the data block class.
[01:06:07.000 --> 01:06:13.000]   Like so.
[01:06:13.000 --> 01:06:19.000]   And but yeah, I think that the doc string is correct.
[01:06:19.000 --> 01:06:26.000]   And I would then be inclined to just grab this.
[01:06:26.000 --> 01:06:36.000]   Here copy and paste it in here.
[01:06:36.000 --> 01:06:44.000]   OK, and so this would be calling.
[01:06:44.000 --> 01:06:54.000]   Yeah, so we're calling the data blocks.
[01:06:54.000 --> 01:06:58.000]   So I guess we're going to do the two steps.
[01:06:58.000 --> 01:07:06.000]   Manually, aren't we? So we're just going to go.
[01:07:06.000 --> 01:07:15.000]   The data sets.
[01:07:15.000 --> 01:07:27.000]   And so that means we need to be passed in.
[01:07:27.000 --> 01:07:31.000]   The items.
[01:07:31.000 --> 01:07:37.000]   Called source, and I'd be inclined to like grab all that.
[01:07:37.000 --> 01:07:44.000]   OK, so this.
[01:07:44.000 --> 01:07:50.000]   This thing in data block.
[01:07:50.000 --> 01:07:57.000]   It's going to need a source.
[01:07:57.000 --> 01:08:01.000]   It's going to need the weights.
[01:08:01.000 --> 01:08:03.000]   It's going to need a batch size.
[01:08:03.000 --> 01:08:08.000]   Apparently there's something called verbose. I don't know what that means, but that's fine.
[01:08:08.000 --> 01:08:16.000]   The so the data sets is self data sets passing in the source.
[01:08:16.000 --> 01:08:20.000]   And verbose equals verbose.
[01:08:20.000 --> 01:08:23.000]   And then.
[01:08:23.000 --> 01:08:27.000]   We called DSS data loaders.
[01:08:27.000 --> 01:08:37.000]   And when we did that. OK, so now we're going to be passing doing DSS.weighted data loaders.
[01:08:37.000 --> 01:08:40.000]   This.
[01:08:40.000 --> 01:08:48.000]   Weighted data loaders.
[01:08:48.000 --> 01:08:53.000]   And that.
[01:08:53.000 --> 01:09:03.000]   That's basically. Oops, what happened there?
[01:09:03.000 --> 01:09:10.000]   And then we pass in the weights.
[01:09:10.000 --> 01:09:12.000]   Weights.
[01:09:12.000 --> 01:09:16.000]   So weighted data loaders gets the weights.
[01:09:16.000 --> 01:09:20.000]   And then the batch size and then the things we added.
[01:09:20.000 --> 01:09:26.000]   Any additional keyword arguments.
[01:09:26.000 --> 01:09:36.000]   And this will delegate down to.
[01:09:36.000 --> 01:09:43.000]   Data sets.weighted data loaders is where the keyword arguments get passed to.
[01:09:43.000 --> 01:09:52.000]   OK, so. As far as I can tell, these same tests.
[01:09:52.000 --> 01:09:56.000]   Should all work. We don't need these labels anymore.
[01:09:56.000 --> 01:10:00.000]   It is valid. We've already got a data block.
[01:10:00.000 --> 01:10:06.000]   So previously we called data set and item transforms and weights manually.
[01:10:06.000 --> 01:10:13.000]   So that is our source.
[01:10:13.000 --> 01:10:17.000]   So we could get rid of all this.
[01:10:17.000 --> 01:10:22.000]   And we're now going to go data block.weighted data loaders.
[01:10:22.000 --> 01:10:26.000]   And we've got to pass in our source.
[01:10:26.000 --> 01:10:35.000]   OK, and we've got to pass in our weights, which were called weights.
[01:10:35.000 --> 01:10:39.000]   And we don't need that.
[01:10:39.000 --> 01:10:49.000]   And we don't need that anymore.
[01:10:49.000 --> 01:10:58.000]   OK, why did I get zero? That's slightly surprising to me.
[01:10:58.000 --> 01:11:13.000]   I can get zero. Yeah, that's fine.
[01:11:13.000 --> 01:11:23.000]   Yeah, get zero or one. Yeah, because it depends how it.
[01:11:23.000 --> 01:11:25.000]   Why is it slightly random?
[01:11:25.000 --> 01:11:29.000]   I'm sure something slightly random. But anyway, it's working.
[01:11:29.000 --> 01:11:36.000]   So then. OK, then again, for this one, we shouldn't need to do data set start.
[01:11:36.000 --> 01:11:41.000]   We should be able to go data block.weighted data loaders.
[01:11:41.000 --> 01:11:47.000]   And we should be able to pass in our items.
[01:11:47.000 --> 01:12:10.000]   And our weights. And.
[01:12:10.000 --> 01:12:18.000]   OK, what did I do wrong there? Data block, weighted data loaders. Oh, it's got it.
[01:12:18.000 --> 01:12:26.000]   OK, let's see.
[01:12:26.000 --> 01:12:37.000]   We've got our source, weights.
[01:12:37.000 --> 01:12:45.000]   Why doesn't it like that?
[01:12:45.000 --> 01:12:49.000]   Source equals.
[01:12:49.000 --> 01:12:54.000]   So let's see how it's different to what this one said.
[01:12:54.000 --> 01:13:01.000]   Data sets.
[01:13:01.000 --> 01:13:06.000]   OK, this doesn't use a data block. So, OK, I can't replicate that.
[01:13:06.000 --> 01:13:11.000]   That's fine. OK, so.
[01:13:11.000 --> 01:13:19.000]   That's our test.
[01:13:19.000 --> 01:13:25.000]   There we go. So what I would then do is I would export it.
[01:13:25.000 --> 01:13:30.000]   And if.
[01:13:30.000 --> 01:13:35.000]   So that that I don't have to like rebuild or reinstall or anything like that.
[01:13:35.000 --> 01:13:40.000]   My first library, that's because I have it installed using something called an editable install.
[01:13:40.000 --> 01:13:44.000]   So if you haven't seen that before, basically, or maybe you have a new one.
[01:13:44.000 --> 01:13:51.000]   Why? When you go pip install minus a dot in a get repo.
[01:13:51.000 --> 01:13:57.000]   Basically, that creates like a symlink from your Python library to this folder.
[01:13:57.000 --> 01:14:05.000]   And so fastai, when I when I import fastai, it's actually going to import it from this folder.
[01:14:05.000 --> 01:14:12.000]   And so now back over here in my weighted thingy.
[01:14:12.000 --> 01:14:18.000]   If I.
[01:14:18.000 --> 01:14:22.000]   Do all this.
[01:14:22.000 --> 01:14:28.000]   Data block.
[01:14:28.000 --> 01:14:32.000]   We should find that there's now a deep block.
[01:14:32.000 --> 01:14:34.000]   Data loaders.
[01:14:34.000 --> 01:14:36.000]   Which.
[01:14:36.000 --> 01:14:39.000]   I can pass source and weights.
[01:14:39.000 --> 01:14:47.000]   And my sources files and my weights is.
[01:14:47.000 --> 01:14:51.000]   Wait.
[01:14:51.000 --> 01:15:00.000]   And my weights. Okay, so that's interesting.
[01:15:00.000 --> 01:15:10.000]   I wait. Yes, we don't have data sets yet.
[01:15:10.000 --> 01:15:17.000]   So that's a very interesting point.
[01:15:17.000 --> 01:15:22.000]   So how do we know what our weights are we don't because they haven't been split.
[01:15:22.000 --> 01:15:28.000]   So the.
[01:15:28.000 --> 01:15:38.000]   And then through is one of the blocks in the as a column get from and then use that because then it would be linked quite intimately with the actual row.
[01:15:38.000 --> 01:15:44.000]   Well, we don't need to. I think what we need to do is pass in weights.
[01:15:44.000 --> 01:15:59.000]   We should pass in all the weights and then this thing here should then be responsible for grabbing the subset for the training set.
[01:15:59.000 --> 01:16:04.000]   And that would actually be much more convenient, which is after all is what we want.
[01:16:04.000 --> 01:16:17.000]   So we should determine the weights based on the the distribution across the classes rather than just a lot. We should split the weights based on the splitter into training and test set.
[01:16:17.000 --> 01:16:23.000]   So then we don't need any of this. So then weights.
[01:16:23.000 --> 01:16:34.000]   Actually, we'll simply be.
[01:16:34.000 --> 01:16:37.000]   That's our way to data frame.
[01:16:37.000 --> 01:16:45.000]   So basically what I would do here is this will actually we'll go back to saying this is sort values.
[01:16:45.000 --> 01:16:52.000]   And then our weights will be.
[01:16:52.000 --> 01:16:57.000]   WTF dot label Y.
[01:16:57.000 --> 01:17:01.000]   That's actually our weights.
[01:17:01.000 --> 01:17:03.000]   As a number.
[01:17:03.000 --> 01:17:15.000]   Silly, silly question. Could you not just see the function for weights to the standard data block and if it doesn't get one, then it does nothing.
[01:17:15.000 --> 01:17:23.000]   Potentially, we could.
[01:17:23.000 --> 01:17:40.000]   It's I kind of like this though because like yeah, I don't know. It's like weights were all one as a default then could use the one solution for yeah, yeah, you could.
[01:17:40.000 --> 01:17:46.000]   I just I don't I find it's a little bit too coupled for me. I don't love it, but it's it's it would be doable.
[01:17:46.000 --> 01:17:53.000]   It's an unnecessary multiplication, I suppose, you know, I like how nicely decoupled this is.
[01:17:53.000 --> 01:17:59.000]   So I think this is what I want it to look like.
[01:17:59.000 --> 01:18:10.000]   So.
[01:18:10.000 --> 01:18:15.000]   So I would look at how the splitters work.
[01:18:15.000 --> 01:18:19.000]   So the splitter.
[01:18:19.000 --> 01:18:30.000]   OK, so the splits gets created here in datasets.
[01:18:30.000 --> 01:18:36.000]   Cool. And then.
[01:18:36.000 --> 01:18:44.000]   I wonder if datasets remembers what those splits are.
[01:18:44.000 --> 01:18:54.000]   Oh, I don't have tags here.
[01:18:54.000 --> 01:19:23.000]   What do you mean no tags file?
[01:19:24.000 --> 01:19:28.000]   OK, there we go. Datasets.
[01:19:28.000 --> 01:19:39.000]   So that's control right square bracket to remind you to jump to a symbol in Vim.
[01:19:39.000 --> 01:19:48.000]   I see. And that's actually mainly happening in this inheritance that superclass is where.
[01:19:48.000 --> 01:19:55.000]   This is split stuff here, yes, splits. I see. There is a splits.
[01:19:55.000 --> 01:20:00.000]   So DSS dot splits.
[01:20:00.000 --> 01:20:10.000]   Oh, OK. DSS dot splits.
[01:20:10.000 --> 01:20:14.000]   Yeah, so there's the indices of the training and test sets.
[01:20:14.000 --> 01:20:18.000]   And so that's the indices of the training set.
[01:20:18.000 --> 01:20:25.000]   So the actual weights we want to those ones.
[01:20:25.000 --> 01:20:31.000]   So over here.
[01:20:31.000 --> 01:20:39.000]   We can say training weights.
[01:20:39.000 --> 01:20:43.000]   So we'll change this to data set from training set.
[01:20:43.000 --> 01:20:47.000]   And so this will be the weights.
[01:20:47.000 --> 01:20:49.000]   At those indices.
[01:20:49.000 --> 01:20:54.000]   And that's what we'd use.
[01:20:54.000 --> 01:20:57.000]   Like so self dot splits.
[01:20:57.000 --> 01:21:00.000]   Thank you so much.
[01:21:00.000 --> 01:21:06.000]   No DSS dot splits self is a data block and it's actually the DSS that has the splits.
[01:21:06.000 --> 01:21:12.000]   The data block has a function that knows how to split, but the split doesn't happen until you create it.
[01:21:12.000 --> 01:21:18.000]   That way you can get different random splits each time if you want them.
[01:21:18.000 --> 01:21:20.000]   Thank you for checking though.
[01:21:20.000 --> 01:21:23.000]   OK, so I'll export that.
[01:21:23.000 --> 01:21:26.000]   And.
[01:21:26.000 --> 01:21:35.000]   Probably be good to have auto load going, but we don't. So be it.
[01:21:35.000 --> 01:21:45.000]   OK.
[01:21:45.000 --> 01:21:59.000]   Now that we did miss the self, but it's not the one you thought of.
[01:21:59.000 --> 01:22:11.000]   This one here.
[01:22:11.000 --> 01:22:14.000]   Yeah.
[01:22:14.000 --> 01:22:36.000]   OK.
[01:22:36.000 --> 01:22:39.000]   I guess actually if I just comment this out.
[01:22:39.000 --> 01:22:54.000]   Then we can just run all above without worrying.
[01:22:54.000 --> 01:23:15.000]   Okay, things are happening so deals equals that.
[01:23:15.000 --> 01:23:17.000]   OK, that looks pretty good.
[01:23:17.000 --> 01:23:24.000]   OK, so I think we've created our feature.
[01:23:24.000 --> 01:23:37.000]   So then the next thing I would do is to be very, very weird if any tests broke, but I would go ahead and run the tests.
[01:23:37.000 --> 01:23:55.000]   I would then create an issue for my feature. And so I'm going to. So I've got a bunch of tiny little aliases and functions ones called enhancement, which creates an issue with the enhancement label.
[01:23:55.000 --> 01:24:10.000]   So I'll go enhancement add data block dot weighted data loaders.
[01:24:10.000 --> 01:24:16.000]   That creates the issue as 3706.
[01:24:16.000 --> 01:24:22.000]   So if you were interested, you could take a look at that issue.
[01:24:22.000 --> 01:24:29.000]   Not the world's most interesting issue, but there it is.
[01:24:29.000 --> 01:24:38.000]   All right, looks like the tests are basically. Oh, no, we've got an issue. There we go. So we've got a test that's failed.
[01:24:38.000 --> 01:24:43.000]   Range in just use must be integers or slices.
[01:24:43.000 --> 01:24:56.000]   Yes, right. So I'm glad we checked.
[01:24:56.000 --> 01:25:16.000]   OK, so the problem here is that I sliced into my weights on the assumption that this is something I can slice into, which would only be true if it was a tensor or an array.
[01:25:16.000 --> 01:25:26.000]   But in this case, actually, my weights are not either of those things.
[01:25:26.000 --> 01:25:32.000]   So what would I do to fix that?
[01:25:32.000 --> 01:25:49.000]   Yeah. When you split, you only keep you back the index of the training and validation data set. And how can you know this is the weights because you haven't actually do the calculation and do the inverse of one square work kind of thing.
[01:25:49.000 --> 01:25:58.000]   The weights are being passed in as a parameter. And so we calculated the weights up here.
[01:25:58.000 --> 01:26:04.000]   And then we passed them in here.
[01:26:04.000 --> 01:26:17.000]   What's the incorrect type that's coming through in the test? It's not that it's an incorrect type. It's that see how here I'm indexing into the weights using my splits.
[01:26:17.000 --> 01:26:31.000]   This here is a list or an array. You can't index into a Python list with a list. You can only do that with tensors or numpy arrays.
[01:26:31.000 --> 01:26:41.000]   Yeah, I mean, what we actually want to do is check whether it's an array type.
[01:26:41.000 --> 01:26:56.000]   Is there a listy or something that function? There is, but that's not quite. I think we want the opposite, which is, is this the kind of thing that one could expect to be able to do numpy style indexing on.
[01:26:56.000 --> 01:27:10.000]   And I believe the correct way to do that might be to look for this thing.
[01:27:10.000 --> 01:27:22.000]   Yeah, so I would be inclined to say.
[01:27:22.000 --> 01:27:42.000]   There may well already be something in first AI that knows how to check for this, to be honest.
[01:27:42.000 --> 01:27:53.000]   Okay, so this, what's this thing?
[01:27:53.000 --> 01:28:07.000]   Oh, that's something that's commented out. Alright, so I guess I don't have anything which checks for that. So we'll just do it manually. So if weights has the dunder array attribute.
[01:28:07.000 --> 01:28:13.000]   I'm pretty sure that tensors have that as well.
[01:28:13.000 --> 01:28:22.000]   Yeah, it does. So if it has that attribute, then I think we're good to go.
[01:28:22.000 --> 01:28:28.000]   Otherwise, we can use a list comprehension.
[01:28:28.000 --> 01:28:41.000]   Oh, you know what we could do. Yeah, okay. What we'll do is we'll just say if it doesn't have that.
[01:28:41.000 --> 01:28:45.000]   I don't know if this is too,
[01:28:45.000 --> 01:28:52.000]   too rude to change their
[01:28:52.000 --> 01:28:57.000]   values, but I think this is fine.
[01:28:57.000 --> 01:29:03.000]   It's not a numpy type array. It's probably going to benefit from being converted to one anyway, right?
[01:29:03.000 --> 01:29:08.000]   Yeah, I mean, I don't, I mean, I don't see a downside.
[01:29:08.000 --> 01:29:12.000]   Passes our test.
[01:29:12.000 --> 01:29:16.000]   Passes all of our tests.
[01:29:16.000 --> 01:29:27.000]   Okay, so, and that was our only test that failed, which is now passing.
[01:29:27.000 --> 01:29:34.000]   So I would now say we've fixed issue 3706.
[01:29:34.000 --> 01:29:43.000]   So I've got a fixes little function that does that 3706.
[01:29:43.000 --> 01:29:51.000]   Okay, and so now, if we look at that issue,
[01:29:51.000 --> 01:30:06.000]   you'll see that it's been resolved using this commit.
[01:30:06.000 --> 01:30:15.000]   Yeah, before, but what do you commit from the notebook? Do you sort of have it like reset with empty cells or do you run the cells?
[01:30:15.000 --> 01:30:24.000]   I commit them basically however they are, but with unnecessary metadata removed.
[01:30:24.000 --> 01:30:43.000]   So there's a hook that automatically runs this function, which is the thing that removes stuff like the execution count, unnecessary notebook metadata, stuff like that.
[01:30:43.000 --> 01:30:52.000]   So, the idea is that the notebooks want to have all the outputs in place, because they get turned into documentation.
[01:30:52.000 --> 01:31:04.000]   And we wouldn't want to run them all in continuous integration to create documentation because they can like involve like spending 10 hours training an NLP model, for example.
[01:31:04.000 --> 01:31:18.000]   We don't remove the outputs for that reason and also because I want people to be able to look at the notebooks in GitHub and see, you know, all the pictures and stuff.
[01:31:18.000 --> 01:31:26.000]   All right, I better stop there. Oh, that's interesting today.
[01:31:26.000 --> 01:31:41.000]   Okay, I guess I don't have my hook installed so I'm glad I ran that manually so you can see exactly what it does right empties out the execution counts and removes the metadata.
[01:31:41.000 --> 01:31:47.000]   I'm sorry for another question. I'm just trying to find it. Is that get hook available in the repo or do you do?
[01:31:47.000 --> 01:31:56.000]   Yeah, so it's if you go mb dev install get hooks, it installs the hook.
[01:31:56.000 --> 01:31:59.000]   And specifically, it's gonna.
[01:31:59.000 --> 01:32:01.000]   Whoopsie daisy.
[01:32:01.000 --> 01:32:04.000]   Is that under MBS folder?
[01:32:04.000 --> 01:32:07.000]   No, this is part of nb dev.
[01:32:07.000 --> 01:32:10.000]   Oh, okay. Right. So once that package is installed, it's a building.
[01:32:10.000 --> 01:32:14.000]   And so that then installs a filter here.
[01:32:14.000 --> 01:32:17.000]   I'll read more about it.
[01:32:17.000 --> 01:32:21.000]   And it also
[01:32:21.000 --> 01:32:26.000]   installs a get hook
[01:32:26.000 --> 01:32:36.000]   to trust the notebooks, which calls nb dev trust mbs. Anyway, yeah, that's all in the nb dev docs.
[01:32:36.000 --> 01:32:45.000]   And then what's going to happen now on the first day ice on the GitHub side
[01:32:45.000 --> 01:33:01.000]   is it's now busily running all the tests again.
[01:33:01.000 --> 01:33:11.000]   And one of the things that checks is to make sure that the notebooks are clean and that the exports been run, then it checks all the notebooks, somewhat in parallel.
[01:33:11.000 --> 01:33:13.000]   Yeah. All right, I'm gonna go.
[01:33:13.000 --> 01:33:15.000]   See you all.
[01:33:15.000 --> 01:33:16.000]   Thanks.
[01:33:16.000 --> 01:33:17.000]   Bye.
[01:33:17.000 --> 01:33:18.000]   Thank you.
[01:33:18.000 --> 01:33:18.000]   Bye-bye.
[01:33:18.000 --> 01:33:20.000]   - Thank you, bye bye.

