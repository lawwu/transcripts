
[00:00:00.000 --> 00:00:09.640]   >> Hi, Ian, Matt, Sonek. I'm excited for this webinar today. And I would love to welcome
[00:00:09.640 --> 00:00:16.880]   everyone who has joined us for the webinar. We have an exciting plan for all of you. We
[00:00:16.880 --> 00:00:23.120]   really want to showcase the future of ML development. We just want to make sure that this webinar
[00:00:23.120 --> 00:00:30.200]   is going to be really educative for all of you. I think let's wait for a few more seconds
[00:00:30.200 --> 00:00:34.160]   for folks to join in. And then we can get started.
[00:00:34.160 --> 00:00:41.480]   >> Sounds good. >> Meanwhile, all of whom who are here would
[00:00:41.480 --> 00:00:49.120]   love to know what your favorite ML framework is. I'm just joking. It should be Keras, right?
[00:00:49.120 --> 00:00:54.120]   And just drop in the chat where are you joining from. At least in India it's 1030 p.m. It's
[00:00:54.120 --> 00:01:00.240]   quite late. But I would love to know where are folks joining from. And your favorite
[00:01:00.240 --> 00:01:12.720]   ML framework. Right. We can get going. I guess it's 1032 already. For the housekeeping, you
[00:01:12.720 --> 00:01:20.080]   can use the chat. Like ask questions to the speakers. Just keep the questions flowing.
[00:01:20.080 --> 00:01:26.960]   We will have Ian speaking first. We have some reserved time at the end for Q&A. We will
[00:01:26.960 --> 00:01:32.080]   try to have 10 to 15 minutes of Q&A. We'll try to cover as much questions as possible.
[00:01:32.080 --> 00:01:37.360]   But keep the questions flowing. If there's something exciting, we would love to maybe
[00:01:37.360 --> 00:01:43.680]   break the flow and ask a question. But, yeah, it's my pleasure to welcome both you, Ian
[00:01:43.680 --> 00:01:49.880]   and Matt, for this webinar. And thank you for doing this with us. And now the stage
[00:01:49.880 --> 00:01:56.120]   is yours. Feel free to, yeah, let the folks know what the agenda for this session is.
[00:01:56.120 --> 00:02:00.920]   And we are really excited for this. Over to you, Ian.
[00:02:00.920 --> 00:02:05.120]   >> Great. Thank you so much. Well, Matt and I will just introduce ourselves real quick.
[00:02:05.120 --> 00:02:12.720]   My name is Ian. I work on Keras CV. >> And I'm Matt. Also working on the Keras
[00:02:12.720 --> 00:02:17.200]   team for a couple years. And all on the natural language processing side.
[00:02:17.200 --> 00:02:23.640]   >> Awesome. Before we get started, I want to say a big thanks on behalf of myself and
[00:02:23.640 --> 00:02:27.440]   Matt to the Weights and Biases team. We're excited to have this opportunity to talk with
[00:02:27.440 --> 00:02:34.000]   you all today and show off some of what we've been doing with Keras and Keras Core.
[00:02:34.000 --> 00:02:36.960]   Before we hop into it, I'll give you a quick sense of what our agenda is going to look
[00:02:36.960 --> 00:02:41.240]   like. I'm going to give you a quick intro on what Keras Core is, if you're not familiar
[00:02:41.240 --> 00:02:45.840]   with it. And then we're going to give you a little bit of background on what Keras CV
[00:02:45.840 --> 00:02:50.940]   and Keras NLP are, which we call our Keras domain packages. I'm going to show a hello
[00:02:50.940 --> 00:02:56.120]   world example of using Keras CV with Keras Core. And then I'm going to dive into a little
[00:02:56.120 --> 00:03:01.520]   bit of technical talk about how you can use a pre-trained component from a framework of
[00:03:01.520 --> 00:03:06.200]   your choice and plug it into Keras Core. So for that example, in particular, we're going
[00:03:06.200 --> 00:03:11.840]   to be talking about Torch Vision. So to just sort of hop right into it here,
[00:03:11.840 --> 00:03:17.200]   what is Keras Core? So Keras Core is everything that you know and love about Keras. It is
[00:03:17.200 --> 00:03:23.080]   a new implementation of the same Keras API, and it is a full rewrite of Keras. It will
[00:03:23.080 --> 00:03:30.320]   become Keras 3.0 in the fall. And it is greatly reduced in size as part of this rewrite, because
[00:03:30.320 --> 00:03:35.280]   we were able to get rid of a lot of tech debt related to TF1, which Keras used to be closely
[00:03:35.280 --> 00:03:40.980]   married to. The big exciting feature about this new rewrite is that we are no longer
[00:03:40.980 --> 00:03:46.880]   tied to specifically the TensorFlow backend. So Keras Core now supports all four of the
[00:03:46.880 --> 00:03:52.480]   backends listed here, which means that anytime you write code in Keras Core using the same
[00:03:52.480 --> 00:03:59.000]   Keras API that you're used to, that code will run in TensorFlow, Jax, PyTorch, or NumPy.
[00:03:59.000 --> 00:04:03.720]   With an important caveat that NumPy does not have autograd, so the NumPy backend can only
[00:04:03.720 --> 00:04:09.440]   be used for inference, not for training. And because the API is exactly identical, Keras
[00:04:09.440 --> 00:04:13.680]   Core is a completely drop-in replacement for TF Keras when you're using the TensorFlow
[00:04:13.680 --> 00:04:18.640]   backend in Keras Core. So any code that you've written in TF Keras, or what we call Keras
[00:04:18.640 --> 00:04:23.040]   today, will automatically run the same way in Keras Core, as long as you're using the
[00:04:23.040 --> 00:04:27.120]   TensorFlow backend. You can read a little bit more about Keras
[00:04:27.120 --> 00:04:31.680]   Core and some of the development behind it at this announcement that I've linked below,
[00:04:31.680 --> 00:04:35.680]   and we'll share these slides with you all at the end of our presentation. But it's on
[00:04:35.680 --> 00:04:39.160]   Keras IO, and you can see a little more of the details about some of the theory behind
[00:04:39.160 --> 00:04:44.840]   it, why we've done it, and some of the things we're excited about with Keras Core.
[00:04:44.840 --> 00:04:49.180]   So to give you a little example of what this means and what this looks like to run things
[00:04:49.180 --> 00:04:53.880]   in different backends, here's just a little bit of code written in the Keras API. And
[00:04:53.880 --> 00:04:58.780]   you can see here that we've imported Keras Core as Keras, and that's just the same way
[00:04:58.780 --> 00:05:05.320]   as you would today import Keras from TensorFlow. And we can create all of the constructs that
[00:05:05.320 --> 00:05:10.480]   we're used to in Keras, like a sequential model built, composed of Keras layers. We
[00:05:10.480 --> 00:05:14.840]   can compile that model with Keras losses, Keras metrics, and a Keras optimizer, and
[00:05:14.840 --> 00:05:19.480]   we can use the fit, evaluate, and predict API that we're used to.
[00:05:19.480 --> 00:05:23.640]   But now, when we write this code this way, with simply a configuration change, we can
[00:05:23.640 --> 00:05:29.120]   run this exact same code across all three backends, and the numerics and functionality
[00:05:29.120 --> 00:05:33.920]   are exactly the same across all three backends. And there's a lot of really cool features
[00:05:33.920 --> 00:05:38.200]   of Keras Core that you can check out in the announcement, things like loading weights
[00:05:38.200 --> 00:05:42.680]   from one backend and saving them into another backend. All those sort of things work beautifully,
[00:05:42.680 --> 00:05:46.740]   and the interop is quite impressive. But we're going to dive into a couple of more specific
[00:05:46.740 --> 00:05:51.400]   features in this talk. Before we get into the depths of Keras Core,
[00:05:51.400 --> 00:05:56.200]   I want to talk about what Keras domain packages are. So there are two Keras domain packages,
[00:05:56.200 --> 00:06:01.080]   Keras CV and Keras NLP. And when we say domain packages, what we mean is that these are specific
[00:06:01.080 --> 00:06:06.880]   to a particular field of ML. So CV is computer vision, and NLP is natural language processing.
[00:06:06.880 --> 00:06:12.120]   And these packages are simply extensions of the Keras API. So they're not a new sort of
[00:06:12.120 --> 00:06:16.680]   framework that you need to learn. They're not a new set of APIs or a different sort
[00:06:16.680 --> 00:06:22.760]   of paradigm. And in fact, it is just the same as the existing Keras API in how all of the
[00:06:22.760 --> 00:06:27.380]   components work. It's just that the components are very specifically tuned for vision and
[00:06:27.380 --> 00:06:31.520]   language processing. And one of the things that's newer about the
[00:06:31.520 --> 00:06:35.740]   domain packages compared to Keras itself is that we include pre-configured as well as
[00:06:35.740 --> 00:06:40.760]   pre-trained model presets. So in a single line of code, you can actually load up a pre-trained,
[00:06:40.760 --> 00:06:46.520]   for example, BERT model or YOLOv8 model, and actually run inference on that in just a couple
[00:06:46.520 --> 00:06:52.520]   of lines of code. And both of the domain packages are fully rebased on Keras core. So any code
[00:06:52.520 --> 00:06:58.240]   that you run in the domain packages can run on all three backends that Keras core supports
[00:06:58.240 --> 00:07:05.320]   with a couple of sort of caveats to mention. The main one is that pre-processing components
[00:07:05.320 --> 00:07:11.240]   in Keras CV and Keras NLP today are built with TF data. So if you're using, for example,
[00:07:11.240 --> 00:07:15.880]   data augmentation in Keras CV or tokenization in Keras NLP, those things will run inside
[00:07:15.880 --> 00:07:20.940]   of a TF data pipeline. However, one of the things that's really awesome about Keras core
[00:07:20.940 --> 00:07:26.560]   is that you can run a TF data pipeline and feed that into a Torch or Jaxx model. So you
[00:07:26.560 --> 00:07:31.080]   can use all of our pre-processing regardless of what backend you're using, but you do have
[00:07:31.080 --> 00:07:36.520]   to use TF data for your data pipelining if you want to use those components.
[00:07:36.520 --> 00:07:43.120]   So here's a quick hello world of how you would use Keras CV. So you can see the first couple
[00:07:43.120 --> 00:07:47.280]   of lines are just setting this environment variable. You can do this in Python or just
[00:07:47.280 --> 00:07:53.160]   persistently in your bash environment. You can also do it using Keras's JSON config,
[00:07:53.160 --> 00:07:56.560]   but here we're just specifying what backend we want to use. And it's literally as simple
[00:07:56.560 --> 00:08:00.960]   as this by specifying this backend. Now we know that everything we do subsequently in
[00:08:00.960 --> 00:08:05.920]   Keras and Keras CV is going to run in Jaxx. So in these few lines of code, which I'll
[00:08:05.920 --> 00:08:10.920]   dig into a little later, we're loading up an image from injur. We are resizing it to
[00:08:10.920 --> 00:08:15.080]   the shape that we expect for our YOLOv8 model. We're loading up a pre-trained version of
[00:08:15.080 --> 00:08:19.920]   the YOLOv8 and then we're running predictions using that model. So here it's truly just
[00:08:19.920 --> 00:08:25.960]   a couple of lines of code, and this is actually running a YOLOv8 model in Jaxx with Keras
[00:08:25.960 --> 00:08:32.080]   CV and Keras core. So to get into sort of the more technical
[00:08:32.080 --> 00:08:36.080]   stuff that I want to talk about today, I want to talk about building custom components in
[00:08:36.080 --> 00:08:43.120]   Keras. And historically, the way that that has been done is by running, writing an implementation
[00:08:43.120 --> 00:08:48.840]   of, for example, a layer using TensorFlow ops. But the Keras core API offers this new
[00:08:48.840 --> 00:08:55.840]   API called Keras core.ops, which is a set of num, it's the NumPy API and a set of all
[00:08:55.840 --> 00:09:01.840]   of the functions within it, which are implemented in Keras core across all backends. So for
[00:09:01.840 --> 00:09:09.200]   example, if I want to say ops.arange, it behaves exactly like NumPy.arange in TensorFlow, Jaxx,
[00:09:09.200 --> 00:09:14.440]   Torch, and of course NumPy. So if you want to write a custom Keras layer, you can do
[00:09:14.440 --> 00:09:20.160]   that by just using the ops API. And then when you drop this, this custom Keras core layer
[00:09:20.160 --> 00:09:25.360]   into a Keras model, it will automatically work with all backends. And this is extremely
[00:09:25.360 --> 00:09:30.000]   flexible. This is how all of Keras CV and Keras NLP is written. And I think we have
[00:09:30.000 --> 00:09:32.800]   found that it's a really pleasant API to work with.
[00:09:32.800 --> 00:09:37.160]   However, one of the things that's really special about Keras core is that you don't have to
[00:09:37.160 --> 00:09:43.480]   use our API. You are not tied to the internals of how our ops API works. And if you have
[00:09:43.480 --> 00:09:48.720]   a specific component or a specific framework or API that you really love for one of our
[00:09:48.720 --> 00:09:53.080]   specific backends that we support, you can actually pull that in directly into Keras
[00:09:53.080 --> 00:09:58.920]   core. So on this next slide, we have the same layer that was defined in the previous slide,
[00:09:58.920 --> 00:10:03.280]   which is this token and position embedding, but it's actually implemented specifically
[00:10:03.280 --> 00:10:07.920]   in Torch, Jaxx, and TensorFlow. And what this is demonstrating is that, you know, if you're
[00:10:07.920 --> 00:10:13.680]   a Torch user and you really love using the Torch API, you can build a Keras layer using
[00:10:13.680 --> 00:10:18.820]   pure Torch ops, and it will still behave like a Keras layer. And the same is true for Jaxx
[00:10:18.820 --> 00:10:23.120]   and TensorFlow. And in this case, you can see for token and position embedding, the
[00:10:23.120 --> 00:10:27.280]   APIs happen to be quite similar across the three. But in some situations, you might want
[00:10:27.280 --> 00:10:30.920]   to use some custom functionality that's only available in one backend.
[00:10:30.920 --> 00:10:36.880]   So to get into this a little more, I'm going to take us over to Colab, and we're going
[00:10:36.880 --> 00:10:41.160]   to walk through our code example of doing something just like this with Keras core on
[00:10:41.160 --> 00:10:48.400]   a vision model using PyTorch. So here we are in Colab, and you can see I've got this instance
[00:10:48.400 --> 00:10:52.560]   running. I've done some pip installs. But the first thing that we do is we're going
[00:10:52.560 --> 00:10:57.360]   to specify our Keras backend. And in this case, we're going to use PyTorch. Then we're
[00:10:57.360 --> 00:11:02.040]   going to run some imports. And when you import Keras CD or Keras core for the first time,
[00:11:02.040 --> 00:11:06.400]   you'll get a little print statement that says you're using the Torch backend. In this case,
[00:11:06.400 --> 00:11:10.000]   I've already run this runtime. So the imports already occurred, and we're not going to get
[00:11:10.000 --> 00:11:14.920]   that again. But from there, we've got Keras loaded. We've specified that we're using our
[00:11:14.920 --> 00:11:19.000]   Torch backend. And so everything from here on out is going to be running in PyTorch behind
[00:11:19.000 --> 00:11:25.920]   the Keras API. So here we'll load up a file. We'll just load it from Imgur using Keras.
[00:11:25.920 --> 00:11:30.760]   And then we'll use the Keras CD resizing layer to resize it to the size that we want for
[00:11:30.760 --> 00:11:36.480]   our model. Here we're just specifying we're going to use YOLOv8 for object detection,
[00:11:36.480 --> 00:11:40.360]   and we're going to use a model that's pre-trained on the VOC dataset. So here we're just specifying
[00:11:40.360 --> 00:11:45.800]   what the names of the 20 classes are in the VOC dataset. And then here we get to jump
[00:11:45.800 --> 00:11:51.520]   into the Keras CD API. So here we can see in Keras CD as well as in Keras NLP under
[00:11:51.520 --> 00:11:58.280]   the models package, any model within models will have this from preset constructor. And
[00:11:58.280 --> 00:12:03.320]   you can load these string presets, which are available on Keras IO. And many of them are
[00:12:03.320 --> 00:12:09.400]   pre-trained. Not all of them are pre-trained. And this allows you to skip a lot of the complex
[00:12:09.400 --> 00:12:15.680]   configuration steps or potentially pre-training or fine tuning for a specific model or task.
[00:12:15.680 --> 00:12:20.840]   So in this case, we're going to load a pre-trained medium-sized YOLOv8 model that's been fine-tuned
[00:12:20.840 --> 00:12:26.600]   on the Pascal VOC object detection dataset. And in general, in object detection, we need
[00:12:26.600 --> 00:12:30.000]   to make sure we keep track of what bounding box format we're using. So we'll say that
[00:12:30.000 --> 00:12:38.740]   we want our detector to use this XY with height format. We can then just say model.predict.
[00:12:38.740 --> 00:12:42.720]   And regardless of what backend we've specified, this is going to go load the weights for this
[00:12:42.720 --> 00:12:49.000]   model and run inference on whichever backend we specify. And if you're familiar with implementing
[00:12:49.000 --> 00:12:53.460]   things in multiple backends, you might know that things like non-max suppression or even
[00:12:53.460 --> 00:12:58.040]   just the forward pass of your model can be very difficult to express in different backends.
[00:12:58.040 --> 00:13:02.680]   And porting that can be very challenging. But in this case, and in all cases with Keras
[00:13:02.680 --> 00:13:08.760]   CV and NLP, it's just one line of code. So this exact same code will run in JAX or Torch.
[00:13:08.760 --> 00:13:13.020]   And then we can see Keras CV will visualize this for us. And this is the same visualization
[00:13:13.020 --> 00:13:16.920]   we saw in our little quick start earlier, where the model has detected some potted plants
[00:13:16.920 --> 00:13:22.380]   as well as a dog. So now we're going to get into sort of the
[00:13:22.380 --> 00:13:27.380]   meat of the demo, which is how can we actually plug in some custom Torch component that we
[00:13:27.380 --> 00:13:33.280]   like into our vision model. And as we do this, I'm going to take advantage of the weights
[00:13:33.280 --> 00:13:40.040]   and biases API to use some of the niceties of its metric logging as we train our new
[00:13:40.040 --> 00:13:43.840]   model. You'll notice I'm using this weights and biases
[00:13:43.840 --> 00:13:48.600]   add-ons package right now. This is something that the weights and biases team is building
[00:13:48.600 --> 00:13:53.800]   to support Keras core. And it's got some cool new features for training object detection
[00:13:53.800 --> 00:13:58.520]   and image classification models. So I'll go ahead and initialize my weights and biases
[00:13:58.520 --> 00:14:04.120]   run. And then we can jump right into using Torch. So if you've used Torch a lot, you're
[00:14:04.120 --> 00:14:09.080]   probably familiar with Torch Vision. Torch Vision has an awesome set of pre-trained models.
[00:14:09.080 --> 00:14:13.240]   And for this demo, we're going to go take one of these Torch Vision models and plug
[00:14:13.240 --> 00:14:20.200]   it into a Keras classifier. So in this case, we will just load Torch models.resnet18. And
[00:14:20.200 --> 00:14:24.960]   this is fully in the Torch API. At this point, we are not using Keras at all. And we're just
[00:14:24.960 --> 00:14:30.480]   loading this pre-trained resnet. And then we're going to strip off the back, the final
[00:14:30.480 --> 00:14:35.520]   layer of the model so that we're not getting the classification head for ImageNet. And
[00:14:35.520 --> 00:14:39.800]   we're just going to get the backbone leading into that classification head. Then we're
[00:14:39.800 --> 00:14:44.440]   going to move it over to the CUDA device as one does in PyTorch because we're going to
[00:14:44.440 --> 00:14:50.000]   train this model on our GPU here. And now we're going to move fully into Keras. So we
[00:14:50.000 --> 00:14:56.480]   have this backbone, which is just a Torch.nn.module. This is not any fancy Keras construct. It's
[00:14:56.480 --> 00:15:01.440]   just a Torch module. And we're going to drop it directly into a Keras model. So the way
[00:15:01.440 --> 00:15:06.920]   that we can do this is define a Keras model. And the only Keras component that it's going
[00:15:06.920 --> 00:15:10.600]   to need to have is this classification head, which in this case is going to be a dense
[00:15:10.600 --> 00:15:15.640]   head because we're looking at the cats and dogs data set for image classification. And
[00:15:15.640 --> 00:15:19.920]   then we can just define the call method of our model such that when we call our model,
[00:15:19.920 --> 00:15:25.360]   first we're going to call this Torch.nn.module on the inputs, which is of course our backbone.
[00:15:25.360 --> 00:15:28.800]   And then we're going to take the results and pass it through a classification head, which
[00:15:28.800 --> 00:15:33.840]   in this case is a Keras layer. And when we do this, the resulting cats and dogs classifier
[00:15:33.840 --> 00:15:40.240]   that we build is a fully Keras object that is no longer tied to the Torch API, but is
[00:15:40.240 --> 00:15:46.600]   part of the Keras API. And as a result, we can compile that model, specify a Keras loss,
[00:15:46.600 --> 00:15:51.460]   Keras optimizer, and Keras metrics. And so we've taken something that is fully Torch
[00:15:51.460 --> 00:15:56.200]   and fully Torch vision, and we've plugged it in to Keras. And we feel that this is a
[00:15:56.200 --> 00:16:00.440]   really exciting feature, and we've seen a lot of really cool use cases of it already
[00:16:00.440 --> 00:16:06.600]   because it allows users to not strongly tie themselves to a specific backend or a specific
[00:16:06.600 --> 00:16:12.140]   framework because they can take pieces of whichever frameworks they love or whichever
[00:16:12.140 --> 00:16:17.840]   packages they're used to using and drop it right into Keras and Keras core.
[00:16:17.840 --> 00:16:23.880]   Once we've done that, I've mentioned before that Keras CD and Keras NLP use TensorFlow
[00:16:23.880 --> 00:16:29.480]   data as the pre-processing utility of choice. And we're going to use that here. And what
[00:16:29.480 --> 00:16:34.400]   we're going to demonstrate here is that we can load up a TF data set and pass it seamlessly
[00:16:34.400 --> 00:16:40.000]   into a PyTorch backend Keras model and let it train. So I'm going to sort of skip over
[00:16:40.000 --> 00:16:44.600]   the details of this because what we're doing here is really just loading up a TF data set,
[00:16:44.600 --> 00:16:49.400]   and that's not the purpose of this talk. But effectively, at a really high level, we're
[00:16:49.400 --> 00:16:54.400]   loading this data set from TFDS, which is TensorFlow's data sets package. We're resizing
[00:16:54.400 --> 00:16:58.480]   it to the image size that we want for our classifier. And then we're just going to shuffle
[00:16:58.480 --> 00:17:04.160]   it for typical training efficacy reasons. And then once we've done that, we can use
[00:17:04.160 --> 00:17:09.400]   Keras CD to visualize some of our pictures. So this is cats feed dogs classification.
[00:17:09.400 --> 00:17:14.220]   So you can see we've got a couple of cats and a dog. And then we're going to get ready
[00:17:14.220 --> 00:17:20.980]   to actually fit this model using PyTorch and Keras core. So because we're using a torch
[00:17:20.980 --> 00:17:25.760]   vision backbone, we need to normalize our inputs to the structure that that torch vision
[00:17:25.760 --> 00:17:31.240]   backbone expects. So in this case, the image net classifiers with torch vision are trained
[00:17:31.240 --> 00:17:35.520]   using images that are rescaled based on the mean and standard deviation of the pixels
[00:17:35.520 --> 00:17:41.040]   and image net. And that's exactly what you see here is that mean and standard deviation.
[00:17:41.040 --> 00:17:46.240]   So we're normalizing what was previously 0 to 255 inputs using the scaling. And then
[00:17:46.240 --> 00:17:50.880]   critically, we're also going to transpose our images into channels first format, which
[00:17:50.880 --> 00:17:56.400]   is typical of PyTorch and is also how the torch vision models are chained. You do not
[00:17:56.400 --> 00:18:01.000]   have to use channels first format in PyTorch. However, the torch vision models use channels
[00:18:01.000 --> 00:18:05.480]   first. So if you want to use that backbone, you need to be consistent with that format
[00:18:05.480 --> 00:18:11.040]   throughout your model. So here we're just we're defining that pre processing, we're
[00:18:11.040 --> 00:18:14.440]   going to map our TensorFlow data set through that. And then we're going to go ahead and
[00:18:14.440 --> 00:18:20.920]   fit our model. So we can use the weights and biases metrics logger callback to keep track
[00:18:20.920 --> 00:18:25.960]   of this as it trains. And I've commented out here, because it doesn't yet support channels
[00:18:25.960 --> 00:18:30.600]   first format. But I wanted to give a shout out to the weights and biases image classification
[00:18:30.600 --> 00:18:34.900]   callback, because I use it quite a bit in testing, and it's it's really quite nifty.
[00:18:34.900 --> 00:18:39.600]   So when you include this callback in your training, your weights and biases run on the
[00:18:39.600 --> 00:18:44.680]   weights and biases website will show example images from training as well as the classes
[00:18:44.680 --> 00:18:49.880]   that were assigned to them by the model during training time. And if you're familiar with
[00:18:49.880 --> 00:18:54.840]   training vision models, you know that it can often be very difficult to map sort of what
[00:18:54.840 --> 00:18:59.680]   the metrics are saying to how the model is performing from a human perspective, particularly
[00:18:59.680 --> 00:19:05.360]   in object detection, where you might see a, an average precision score, but what does
[00:19:05.360 --> 00:19:11.040]   that really mean from a human from a human look. And so these callbacks for image classification
[00:19:11.040 --> 00:19:14.920]   and object detection are really nice, because you can actually see, okay, here's the box
[00:19:14.920 --> 00:19:21.060]   that the model drew, or here's the class that the model assigned. So once we've done that,
[00:19:21.060 --> 00:19:24.840]   we can just call model buffet. And again, this is just the Keras API that we're used
[00:19:24.840 --> 00:19:29.680]   to. And you can see, because this has a pre trained backbone and cats v dogs is a pretty
[00:19:29.680 --> 00:19:35.720]   simple task. The accuracy goes up pretty quick. And after just one epoch, we will find that
[00:19:35.720 --> 00:19:42.840]   the this model is above 90, 90 or so percent accuracy on the training set. And of course,
[00:19:42.840 --> 00:19:49.800]   once we've done this, we can actually go look at our run in weights and biases. And I'll
[00:19:49.800 --> 00:19:56.800]   pull it up real quick. And you can actually see, we're getting the live results here in
[00:19:56.800 --> 00:20:03.040]   our in our weights and biases run, which of course, is the lovely feature of weights and
[00:20:03.040 --> 00:20:06.880]   biases. We love using this to get notified on how our training jobs are going and all
[00:20:06.880 --> 00:20:13.040]   that, all that good stuff. So back in Colab, you can see our model is continuing to train.
[00:20:13.040 --> 00:20:16.400]   And then once it's done training, we'll just call weights and biases on finish to close
[00:20:16.400 --> 00:20:24.000]   up our experiment. So that is the gist of the plugging in a PyTorch specific module
[00:20:24.000 --> 00:20:30.240]   into Keras core for a vision model. And before I hand it over to Matt, I wanted to give you
[00:20:30.240 --> 00:20:35.000]   all a chance to chat about questions that are specific to Keras CV or the demo that
[00:20:35.000 --> 00:20:39.720]   I just did. We're excited about all your great questions about Keras core. But to respect
[00:20:39.720 --> 00:20:43.440]   Matt's time, we're going to wait for Keras core questions until the end. So if there
[00:20:43.440 --> 00:20:48.120]   are questions now about the PyTorch demo or Keras CV, I'm happy to cover them. And if
[00:20:48.120 --> 00:20:57.080]   not, we can hand it back over to Matt to talk about NLP.
[00:20:57.080 --> 00:21:11.760]   And let me see. It looks like we've got a little bit of a few questions. Let's see.
[00:21:11.760 --> 00:21:17.280]   Most of these look like they are Keras core generally. I will answer one that is a domain
[00:21:17.280 --> 00:21:23.360]   packages question. I see, are domain packages going to support domains other than Keras
[00:21:23.360 --> 00:21:30.960]   CV or NLP, such as 3D data or geometric deep learning? There is currently not any projects
[00:21:30.960 --> 00:21:36.160]   in the work to expand the domain packages to these. We are trying to expand the offerings
[00:21:36.160 --> 00:21:42.240]   of CV and NLP for now. And if this continues to succeed, then we will look into that in
[00:21:42.240 --> 00:21:48.800]   the future. But as of today, there is not a plan to add 3D or geometric as extensions
[00:21:48.800 --> 00:21:56.400]   of the API. Today, CV does include some 3D modeling. But it's not the focus, as it's
[00:21:56.400 --> 00:22:01.920]   not what we view as the most impactful user base to be working with right now.
[00:22:01.920 --> 00:22:09.280]   Okay. Awesome. With that, I think, Matt, I can hand it over to you to start talking about
[00:22:09.280 --> 00:22:10.280]   NLP.
[00:22:10.280 --> 00:22:16.760]   Cool. Thank you. So, yeah, for the second half of this talk, we're going to switch from
[00:22:16.760 --> 00:22:21.280]   computer vision to natural language. So, natural language basically as opposed to programming
[00:22:21.280 --> 00:22:28.400]   languages, anything written or spoken, English, Spanish. And kind of as Ian's was a bit of
[00:22:28.400 --> 00:22:32.760]   like CV plus an exploration of what you can do on the Torch backend, my talk is going
[00:22:32.760 --> 00:22:38.480]   to be NLP plus a bit of an exploration of what you can do on the Jax backend. So, yeah,
[00:22:38.480 --> 00:22:44.000]   I'm going to give a quick overview of the back half of this talk. And then I'm going
[00:22:44.000 --> 00:22:49.400]   to show a quick kind of hello world slide here that shows like a very simple training
[00:22:49.400 --> 00:22:55.040]   script for Keras NLP, just to kind of get everyone oriented in the library what Keras
[00:22:55.040 --> 00:23:01.840]   NLP code looks like. And then we'll hop over to a Colab and do some lab coding. And we
[00:23:01.840 --> 00:23:09.360]   will take a GPT-2 base model and do some really simple instruction fine tuning on it on the
[00:23:09.360 --> 00:23:14.160]   Jax backend. So, we'll talk a bit more about what instruction fine tuning means as we do
[00:23:14.160 --> 00:23:18.440]   that. And then the last thing we're going to do for this half of the talk is we're going
[00:23:18.440 --> 00:23:24.320]   to basically recreate that exact same training setup, but at a much lower level by writing
[00:23:24.320 --> 00:23:29.240]   our own Jax train step and train loop so we can kind of see what's happening under the
[00:23:29.240 --> 00:23:36.360]   hood there. And that will be it. And we can answer questions about any of the Keras CV,
[00:23:36.360 --> 00:23:45.640]   Keras Core, Keras NLP. So, yeah, let's hop into hello world with Keras NLP. So, this
[00:23:45.640 --> 00:23:50.200]   should look quite actually symmetrical to the hello world that Ian showed for Keras
[00:23:50.200 --> 00:23:57.880]   CV. The first thing you want to do if you're using the multibackend version of Keras Core
[00:23:57.880 --> 00:24:02.800]   and Keras CV is you want to set your backend before you actually import the library. So,
[00:24:02.800 --> 00:24:06.720]   the first thing we're doing here is we're setting our Keras backend to Jax. And that
[00:24:06.720 --> 00:24:11.040]   will actually affect how the library imports itself. So, that's why you have to do it first.
[00:24:11.040 --> 00:24:17.200]   And then the overall goal with this training example here, it's kind of a classic NLP benchmark.
[00:24:17.200 --> 00:24:24.840]   We are trying to make a review predictor. So, for a, I think it's 50,000 reviews from
[00:24:24.840 --> 00:24:30.600]   IMDB, we have labels of whether they were a positive or negative review. So, we're going
[00:24:30.600 --> 00:24:34.560]   to load that up here with TensorFlow data sets. It's not actually important that it's
[00:24:34.560 --> 00:24:40.120]   TensorFlow data sets here. One of the great things about Keras Core is it's quite flexible
[00:24:40.120 --> 00:24:44.120]   in the data input formats you have. But I'm just showing this because it's a convenient
[00:24:44.120 --> 00:24:49.720]   one liner. And then we're going to load our classifier from Keras NLP. And you'll notice
[00:24:49.720 --> 00:24:54.820]   here that similar to Keras CV, we have a models namespace. And in that models namespace, we
[00:24:54.820 --> 00:24:59.880]   have a bunch of classes with a from preset constructor. And that from preset constructor
[00:24:59.880 --> 00:25:06.240]   is going to be how you access all of your pre-trained weights. So, we're loading up
[00:25:06.240 --> 00:25:11.120]   BERT, which is a model that's great at picking up just general patterns and text. Great for
[00:25:11.120 --> 00:25:16.520]   classification problems, but not out of the box for any particular classification problem.
[00:25:16.520 --> 00:25:21.460]   And then we're going to go ahead and fine tune it on our IMDB reviews, which will turn
[00:25:21.460 --> 00:25:27.700]   our BERT model into a great movie review classifier. So, we can predict two new unseen examples
[00:25:27.700 --> 00:25:31.500]   at the end of it. And again, this is just to get a high level sense of the library.
[00:25:31.500 --> 00:25:36.460]   We're not going to go deep into BERT kind of internals here or anything like that. But
[00:25:36.460 --> 00:25:42.060]   roughly for Keras NLP, this will be the flow you see. Usually, you're loading a pre-trained
[00:25:42.060 --> 00:25:48.020]   model and doing either some fine tuning or inference or both with it. Okay. And with
[00:25:48.020 --> 00:26:08.380]   that, I will actually take over screen sharing and present a collab. And great. Looks like
[00:26:08.380 --> 00:26:15.540]   we are on. So, let me hop over to that window. So, I actually went ahead and ran the first
[00:26:15.540 --> 00:26:20.500]   half of this collab already. And that's just so you didn't have to kind of sit around watching
[00:26:20.500 --> 00:26:26.220]   me download pip packages and everything. But to go over the setup we've done here, I've
[00:26:26.220 --> 00:26:32.300]   just installed Keras NLP. I've installed Weights and Biases. I also installed that Sumik in
[00:26:32.300 --> 00:26:38.180]   this call wrote a Weights and Biases add-on. And that's particularly for this multi-backend
[00:26:38.180 --> 00:26:43.180]   Keras we're talking about. It gets you the callbacks that work with multi-backend Keras
[00:26:43.180 --> 00:26:49.380]   today. We've logged in and I've set the backend to Jax like we just talked about. The other
[00:26:49.380 --> 00:26:52.980]   thing I included here, and this is really for after the call, if anyone is interested
[00:26:52.980 --> 00:27:00.340]   in the quick start, I just recreated it here verbatim for reference. But the -- yeah. This
[00:27:00.340 --> 00:27:05.300]   is just if you wanted to play around with the quick start after the call. But we can
[00:27:05.300 --> 00:27:10.300]   hop over that to the instruction fine tuning part of this talk.
[00:27:10.300 --> 00:27:15.300]   Before we dive into this, it's probably good to talk about what actually is instruction
[00:27:15.300 --> 00:27:18.940]   fine tuning. So when we're talking about instruction fine tuning, we're usually going to be talking
[00:27:18.940 --> 00:27:24.140]   about generative language models. So these are models that essentially are just really
[00:27:24.140 --> 00:27:30.220]   good at predicting the next word based on all the words that have come before it. So
[00:27:30.220 --> 00:27:40.020]   we're going to be using GPT-2 here. This training setup also applies to LLAMA or chat GPT or
[00:27:40.020 --> 00:27:44.940]   whatever. But essentially the way that you're training these kind of base generative models
[00:27:44.940 --> 00:27:50.940]   is you are playing this kind of predict the next word game at this massive scale on a
[00:27:50.940 --> 00:27:57.140]   huge amount of text from the internet. So you're just taking a ton of examples of natural
[00:27:57.140 --> 00:28:03.580]   language and loading them all up and training your model by progressively feeding it sequences
[00:28:03.580 --> 00:28:08.020]   and asking it to predict the next word based on everything that came before it. That gives
[00:28:08.020 --> 00:28:12.940]   you this base model that has all these super interesting immersion properties. But the
[00:28:12.940 --> 00:28:19.060]   issue with these base models is that you kind of get what you pay for. They tend to just
[00:28:19.060 --> 00:28:24.740]   take a direction and ramble off on it and basically are just trying to parrot what sounds
[00:28:24.740 --> 00:28:28.460]   like might be text that came from somewhere on the internet, which is exactly what you
[00:28:28.460 --> 00:28:34.180]   trained them on. Which is generally not that useful if you think of something like chat
[00:28:34.180 --> 00:28:40.260]   GPT or BARD. What can be a lot more useful is something that's attempting to have a kind
[00:28:40.260 --> 00:28:47.060]   of question answer flow to it. So given some instructions, try to give a response that's
[00:28:47.060 --> 00:28:52.260]   helpful based on those instructions. So that's the idea behind instruction fine tuning. You're
[00:28:52.260 --> 00:28:57.620]   kind of trying to just steer this model that's been trained sometimes on, I think the big
[00:28:57.620 --> 00:29:03.060]   ones these days are on trillions of words on the internet, trillions of tokens. And
[00:29:03.060 --> 00:29:07.020]   you're just kind of steering it without trying to teach it really any new facts or anything.
[00:29:07.020 --> 00:29:14.180]   You're just trying to steer it in the direction of being a helpful question answering thing.
[00:29:14.180 --> 00:29:18.760]   And there's a bunch of different ways to do instruction fine tuning. Some of them are
[00:29:18.760 --> 00:29:23.020]   quite complex and very expensive involving reinforcement learning and a bunch of human
[00:29:23.020 --> 00:29:28.740]   annotators. We're going to do the really simple version here because it fits well in our talk,
[00:29:28.740 --> 00:29:33.300]   which is just supervised instruction fine tuning. So to show what that means, let's
[00:29:33.300 --> 00:29:40.660]   go ahead and load up our dataset here. And this is a dataset from Databricks of kind
[00:29:40.660 --> 00:29:45.540]   of questions and answers. And what I've done here is some of these, actually, let's go
[00:29:45.540 --> 00:29:49.420]   ahead and take a look at the output so we can see it. We're just going to be taking
[00:29:49.420 --> 00:29:56.140]   this dataset of Jason and giving it a specific format of an instruction and response. And
[00:29:56.140 --> 00:30:03.260]   the only things I've done here that are non-trivial are some of these examples have some additional
[00:30:03.260 --> 00:30:07.300]   context. It would be great to use that, but to keep our code really simple here, I've
[00:30:07.300 --> 00:30:10.700]   just thrown all of those out. And the other thing I've done is I've chopped our data down
[00:30:10.700 --> 00:30:15.660]   to just a thousand training examples. That is purely for speed in this talk. It would
[00:30:15.660 --> 00:30:20.220]   be better to use more examples. I just want to keep this talk moving quickly. So yeah,
[00:30:20.220 --> 00:30:26.340]   you can see that we've kind of formed these examples of kind of strings with a question
[00:30:26.340 --> 00:30:32.100]   and a response. And it's all just in one big long sequence. So the way that we're going
[00:30:32.100 --> 00:30:37.940]   to do our instruction fine tuning is the exact same way that GPT-2 or other models are trained
[00:30:37.940 --> 00:30:43.380]   in the first place, which is just play that guess the next word game throughout training.
[00:30:43.380 --> 00:30:46.940]   But instead of doing that guess the next word game on random documents on the internet,
[00:30:46.940 --> 00:30:51.060]   you're doing it on question answer pairs. And that kind of steers the model towards
[00:30:51.060 --> 00:30:58.180]   being a question answering model. So with that, we can get into actually loading up
[00:30:58.180 --> 00:31:06.540]   GPT-2. So yeah, again, we have our models namespace. We have our from preset constructor.
[00:31:06.540 --> 00:31:14.220]   Yeah, you can see we're 300 million parameters, which I think is a few orders of magnitude
[00:31:14.220 --> 00:31:20.140]   larger than what Ian was showing earlier in this talk, but is actually on the large language
[00:31:20.140 --> 00:31:24.820]   model side of things is very small. Usually when you're talking about large language models
[00:31:24.820 --> 00:31:28.660]   these days, you're talking about billions of parameters. I'm just using a small model
[00:31:28.660 --> 00:31:31.660]   because this is a short talk and I want to be able to show training quickly, but we'll
[00:31:31.660 --> 00:31:36.860]   actually see that effect performance later on. The other thing I'm showing here without
[00:31:36.860 --> 00:31:44.180]   getting too much into detail is that you can pass your optimizers, pass metrics, pass losses,
[00:31:44.180 --> 00:31:49.220]   and you can use Keras NLP and Keras at CVR extensions of the Keras API, but they allow
[00:31:49.220 --> 00:31:55.020]   you to use the same constructs that you're used to with core Keras. So with our model
[00:31:55.020 --> 00:32:01.540]   loaded up and our dataset loaded up, we can go ahead and one, set up our weights and biases
[00:32:01.540 --> 00:32:09.940]   metrics and go ahead and fit our model. So one thing we're talking about here as we kind
[00:32:09.940 --> 00:32:16.540]   of let this spinner run for a while is the first kind of time to see the first train
[00:32:16.540 --> 00:32:21.220]   step show up is going to be really big. And this is true on both the, the Jax and the
[00:32:21.220 --> 00:32:26.620]   TensorFlow backends. And the reason for this is compilation. Essentially what's happening
[00:32:26.620 --> 00:32:32.140]   here under the hood, and we'll see this in more detail later on, is Jax is taking this
[00:32:32.140 --> 00:32:36.780]   description of all the linear algebra and your forward and backwards passes and optimizing
[00:32:36.780 --> 00:32:43.020]   it as much as you can with a library called XLA, accelerated linear algebra, and gets
[00:32:43.020 --> 00:32:48.460]   you this super fast version of your train step that doesn't actually, you don't actually
[00:32:48.460 --> 00:32:52.780]   need to like re-execute all of your Python code for your train step every time you're
[00:32:52.780 --> 00:32:59.060]   running your train step. So you're essentially paying this big down payment for compilation
[00:32:59.060 --> 00:33:03.980]   here, which will be slow, but then the benefit is after you've done that and you're on training
[00:33:03.980 --> 00:33:10.540]   your model, and I think soon we should start to see some train steps pop up here. You'll
[00:33:10.540 --> 00:33:18.420]   see it. It's, I think we'll hit like, yeah, what are we roughly three, three or four train
[00:33:18.420 --> 00:33:25.300]   steps a second now that we've actually finished our compilation. So yeah, we are up and running
[00:33:25.300 --> 00:33:31.860]   with our training, so I can actually go ahead and hop over to when I logged into weights
[00:33:31.860 --> 00:33:40.940]   and biases are run and take a look at the metrics. So yeah, I think Ian already showed
[00:33:40.940 --> 00:33:46.020]   all of this. The only thing kind of worth calling out here is one thing as we compiled
[00:33:46.020 --> 00:33:51.540]   our model, we, we tracked just a really simple accuracy metric and you're able to see that
[00:33:51.540 --> 00:33:57.620]   as our training progresses, we are able to get to essentially a 50% accuracy on this
[00:33:57.620 --> 00:34:03.940]   guess the next word game. So half the time the model is able to predict correctly the
[00:34:03.940 --> 00:34:12.180]   next word in the sequence, which is decent. So yeah, I'll, I'll let that run, but we can
[00:34:12.180 --> 00:34:17.300]   kind of cover the next bit of code as it's going. So after that finishes up, we're going
[00:34:17.300 --> 00:34:24.940]   to actually draw samples from our fine tuned model. That's what this generate call will
[00:34:24.940 --> 00:34:28.780]   give you. So generate, this is something that's not part of core Keras, but it's actually
[00:34:28.780 --> 00:34:35.780]   a kind of new top level API that we're giving as part of Keras NLP for language models.
[00:34:35.780 --> 00:34:40.780]   And all generate is going to be doing is running your prediction for the next word in a loop.
[00:34:40.780 --> 00:34:45.660]   So you'll just get GPT to predict one word and then the next word and then the next word,
[00:34:45.660 --> 00:34:52.020]   and that will create an entire generated sequence of text and response. So we'll go ahead and
[00:34:52.020 --> 00:34:55.980]   run, run that on these three examples and we'll see how, how well we do here. One thing
[00:34:55.980 --> 00:35:02.500]   I will note is because this is a small model, this is 300 million parameters and not like,
[00:35:02.500 --> 00:35:09.540]   I think 7 billion parameters as a quote unquote small model release for like the, the llama,
[00:35:09.540 --> 00:35:18.060]   llama two. So you'll definitely notice performances is it, we'll probably see this attempt to
[00:35:18.060 --> 00:35:23.540]   answer a question, but I expect to see kind of a lot of somewhat hallucinatory results
[00:35:23.540 --> 00:35:30.180]   as we go ahead and do this. But yeah, I think we should be wrapping up training soon so
[00:35:30.180 --> 00:35:40.620]   we can see how it does with our three questions here.
[00:35:40.620 --> 00:35:46.860]   And the other thing you'll notice again with generate, so this is going to be, we're running
[00:35:46.860 --> 00:35:54.260]   prediction in a loop, but it's actually a compiled loop. So the entire process of running
[00:35:54.260 --> 00:35:59.940]   a forward pass, generating a probability distribution for your next word, then we sample from that
[00:35:59.940 --> 00:36:06.140]   distribution and we do that in a loop up to the sequence length that we want to generate.
[00:36:06.140 --> 00:36:12.300]   And we will actually compile that thing with Jacks. So similar to we saw for fit, the first
[00:36:12.300 --> 00:36:17.100]   time we called generate is going to be very slow and the second two times, once this one
[00:36:17.100 --> 00:36:26.420]   finishes executing should be very fast. And yeah, the other thing I can note here as we're
[00:36:26.420 --> 00:36:32.460]   waiting for this to finish up is one thing you can do to kind of customize the way that
[00:36:32.460 --> 00:36:40.940]   you sample things out, hear things come through, is during compilation, you can pass a sampler
[00:36:40.940 --> 00:36:46.460]   as well, which will control kind of the way that you're sampling your distribution. So
[00:36:46.460 --> 00:36:51.980]   if you want to do greedy sampling, which would be kind of a very tight read of your, just
[00:36:51.980 --> 00:36:56.620]   the most probable word in a distribution or random sampling or top case sampling, you
[00:36:56.620 --> 00:37:01.700]   can do that by passing it to our compile.
[00:37:01.700 --> 00:37:06.760]   And here we've got our responses. So machine learning is the use of data to learn new things
[00:37:06.760 --> 00:37:10.540]   about the world. You can definitely see that we've kind of steered our model in the direction
[00:37:10.540 --> 00:37:15.940]   of being a question answering thing. If you want to travel around Europe, you should plan
[00:37:15.940 --> 00:37:19.980]   to visit a lot of cities and cities should be the first thing you do. Now it's super
[00:37:19.980 --> 00:37:23.700]   helpful. And then how many bowling balls in the car? I've just included this for fun.
[00:37:23.700 --> 00:37:30.020]   I've never seen a model of this size do well on this. And indeed, it's just talking about
[00:37:30.020 --> 00:37:34.940]   a description of a car here. So yeah, feel free to play around with this afterwards if
[00:37:34.940 --> 00:37:39.620]   you're interested. You'll definitely get very different responses if you kind of rerun this
[00:37:39.620 --> 00:37:42.660]   a few times.
[00:37:42.660 --> 00:37:46.980]   But with that, let's go on to the last kind of section of the talk here, where we're going
[00:37:46.980 --> 00:37:53.500]   to peel back a layer and kind of recreate that exact training setup. But with lower
[00:37:53.500 --> 00:37:58.200]   level JAX primitives, we're going to write our own train step. And then we are going
[00:37:58.200 --> 00:38:04.020]   to write our own entire training loop. And one reason to do this is just because it's,
[00:38:04.020 --> 00:38:07.580]   you know, when this is a talk about multivac and Keras, it's genuinely useful to just kind
[00:38:07.580 --> 00:38:13.620]   of understand what happens when you call model.fit. There's a lot going on under the hood, and
[00:38:13.620 --> 00:38:18.860]   it's nice to know what that actually is. But there are some other reasons you might want
[00:38:18.860 --> 00:38:24.500]   to run a custom train loop in production. Some of them would be like a more kind of
[00:38:24.500 --> 00:38:29.620]   complicated training setup. Like a GAN would be a time that you might want to write your
[00:38:29.620 --> 00:38:36.140]   own custom training loop. But very relevant for JAX is JAX has some really efficient and
[00:38:36.140 --> 00:38:44.340]   scalable distribution APIs for kind of training a model across an entire fleet of GPUs or
[00:38:44.340 --> 00:38:48.780]   TPUs. So one thing you could use writing a lower level custom training loop in JAX is
[00:38:48.780 --> 00:38:55.260]   really get kind of access to these distribution APIs in JAX at a really low level and optimize
[00:38:55.260 --> 00:39:01.660]   your train step in JAX if you wanted to. So yeah, with that segue, let's go ahead and
[00:39:01.660 --> 00:39:07.620]   load up our model again. And this time I'm going to show loading our model with the pre-processing
[00:39:07.620 --> 00:39:15.780]   split app. So as Ian was talking about with Keras CD, both of our libraries are running
[00:39:15.780 --> 00:39:22.740]   all of our pre-processing with TF data. And we won't get too deep into this in this talk,
[00:39:22.740 --> 00:39:27.700]   but the things to note are TF data isn't actually going to restrict you from what backend you
[00:39:27.700 --> 00:39:31.980]   want to be using for training. We're using TF data with JAX here, and you could be using
[00:39:31.980 --> 00:39:38.900]   TF data on the Torch backend or the TensorFlow backend. And TF data is a very efficient and
[00:39:38.900 --> 00:39:45.900]   scalable API for running pre-processing, which is why we are built on top of it. So yeah,
[00:39:45.900 --> 00:39:50.820]   here I'm kind of showing this pre-processor when it's attached to the model was running
[00:39:50.820 --> 00:39:55.220]   kind of implicitly in fit, but it was really running everything that's being shown here,
[00:39:55.220 --> 00:40:01.700]   which is making a TF data dataset of your data and then mapping your pre-processor over
[00:40:01.700 --> 00:40:06.260]   your dataset. And if we run this here, we can see that that will take our kind of long
[00:40:06.260 --> 00:40:12.820]   strings of input, this instruction response input, and turn it into a sequence of token
[00:40:12.820 --> 00:40:17.340]   IDs that we can pass to a model. And then the other thing I will do here is create our
[00:40:17.340 --> 00:40:22.220]   optimizer and loss. This is the same optimizer and loss that I created earlier in the Colab.
[00:40:22.220 --> 00:40:26.380]   We're just splitting it up so we can use it again. And then here we'll kind of land at
[00:40:26.380 --> 00:40:37.300]   like the actual JAX bits of our custom train loop. So yeah, there's a lot of kind of code
[00:40:37.300 --> 00:40:41.740]   here, but you really need to only understand one real principle to understand like kind
[00:40:41.740 --> 00:40:46.900]   of why the JAX code looks the way it does, which is that JAX is all about pure function
[00:40:46.900 --> 00:40:52.740]   transforms. Basically, JAX really, really hates side effects. So if you're used to Torch
[00:40:52.740 --> 00:40:56.940]   and TensorFlow, you might be used to variables that you can just kind of, they're stateful
[00:40:56.940 --> 00:41:02.020]   things and whenever you want to kind of poke new data into a variable, you absolutely can.
[00:41:02.020 --> 00:41:07.260]   And you just assign a new value to a variable and you're off and running. JAX has a different
[00:41:07.260 --> 00:41:12.060]   approach where it wants to kind of know about every time you're writing a function, you
[00:41:12.060 --> 00:41:16.620]   have to pass all of the variables that you care about in and all of the updated variables
[00:41:16.620 --> 00:41:22.340]   that you care about out. And some of this is stylistic, you know, functional style of
[00:41:22.340 --> 00:41:27.300]   programming is something some programmers prefer. Another reason for this is performance
[00:41:27.300 --> 00:41:32.140]   because JAX can know that you have these nice stateless functions. It can really do a lot
[00:41:32.140 --> 00:41:37.860]   more in terms of optimizing what you write. So here we're going to define a loss function,
[00:41:37.860 --> 00:41:42.900]   but unlike a loss function where you're just using a stateless model, we pass all of our
[00:41:42.900 --> 00:41:50.140]   variables in along with our batch. And then as we compute our loss, we need to use one
[00:41:50.140 --> 00:41:55.460]   thing that Keras core provides is a stateless version of kind of most of the kind of top
[00:41:55.460 --> 00:41:59.740]   level methods that you're used to, basically so that you can write things like this in
[00:41:59.740 --> 00:42:04.620]   JAX. So here we're going to use a stateless call, which is the same as a normal model
[00:42:04.620 --> 00:42:08.740]   dot call, except it allows you to pass your variables in and get your updated variables
[00:42:08.740 --> 00:42:14.740]   out. So with that, we've created this stateless function for computing our loss, and we can
[00:42:14.740 --> 00:42:19.580]   pass it to a JAX function transform, which is something that takes in a function and
[00:42:19.580 --> 00:42:26.620]   spits a function out. Here, JAX value in grad takes in this nice stateless loss function
[00:42:26.620 --> 00:42:32.260]   and gives you a function that competes the loss, but also competes gradients for that
[00:42:32.260 --> 00:42:37.380]   loss. And with that, we can actually define our train step, which is going to take in
[00:42:37.380 --> 00:42:41.780]   all of our state, again, trainable variables, untrainable variables. And here we actually
[00:42:41.780 --> 00:42:46.980]   have to care about optimizer variables because we're going to be using an optimizer to update
[00:42:46.980 --> 00:42:52.740]   all of our JAX variables here. We'll use that gradient function we just defined to actually
[00:42:52.740 --> 00:42:59.660]   compute gradients for our loss. And then optimizer, much like our model has a stateless call with
[00:42:59.660 --> 00:43:05.900]   Keras core, in Keras core, an optimizer has a stateless apply, which is something that
[00:43:05.900 --> 00:43:11.700]   allows you to apply gradients to your variables by passing all your variables in and getting
[00:43:11.700 --> 00:43:18.140]   updated variables out. So we're able to write this where we're computing our loss, updating
[00:43:18.140 --> 00:43:24.540]   all our variables, and it is completely stateless. And now that we have that stateless no side
[00:43:24.540 --> 00:43:29.980]   effect function, we can run a JAX.JIT function transform on it. And this is the compilation
[00:43:29.980 --> 00:43:35.300]   we were talking about earlier. That compilation that was making the first batch of our fit
[00:43:35.300 --> 00:43:40.260]   call slow. It really, if you look in at the Keras core code, it's we are just calling
[00:43:40.260 --> 00:43:45.900]   JAX.JIT to run XLA compilation on our training step. So we can go ahead and run this. And
[00:43:45.900 --> 00:43:50.620]   then we are basically all done here. The last thing we need to do is create our optimizer
[00:43:50.620 --> 00:43:58.220]   variables. So we have all of our state ready to pass to our pure function train step. And
[00:43:58.220 --> 00:44:04.420]   then you can see, we just kind of go batch by batch over our data, pass all of our state
[00:44:04.420 --> 00:44:10.140]   and our batch in, and we'll just get an updated version of our state each time as we go. And
[00:44:10.140 --> 00:44:16.780]   again, because we've wrapped things in this JAX.JIT call, you'll kind of see us have to
[00:44:16.780 --> 00:44:22.940]   pause for a while here as we let the first train step compile. And then after we've done
[00:44:22.940 --> 00:44:31.580]   that initial compilation, you'll be off and running and we'll be running more two or three
[00:44:31.580 --> 00:44:39.340]   train steps every second or something like that. So yeah, that's, I think all to show
[00:44:39.340 --> 00:44:44.020]   and here we go on our training for writing a JAX custom train loop. And again, what this
[00:44:44.020 --> 00:44:50.220]   does is you can really use any of the low level JAX APIs that you want in your training,
[00:44:50.220 --> 00:44:56.380]   in particular JAX distribution APIs. So if you're interested in going deeper, I've included
[00:44:56.380 --> 00:45:00.940]   some extra links here for distributed training with JAX, as well as the getting started guide
[00:45:00.940 --> 00:45:14.260]   for Keras MLP. Yeah. So at this point I will stop screen sharing and open it up for questions,
[00:45:14.260 --> 00:45:23.580]   but yeah, I think overall we're very excited about multi backend Keras, particularly for
[00:45:23.580 --> 00:45:27.620]   pre-trained models like this. It is super useful to be able to kind of write your numerics
[00:45:27.620 --> 00:45:32.340]   once, have all your weights once and kind of mix and match between different backends.
[00:45:32.340 --> 00:45:36.380]   You could even do something like fine tune and JAX and deploy on Tensorflow. So we think
[00:45:36.380 --> 00:45:42.380]   it's a kind of interesting new tool that wasn't in ML developers tool belts before, and we're
[00:45:42.380 --> 00:45:48.660]   excited for it with Keras 3. Thank you.
[00:45:48.660 --> 00:45:56.860]   - I hope it clears the wall over frameworks. Now we have a framework that we all can love
[00:45:56.860 --> 00:46:05.300]   and enjoy writing ML code on. Yeah, let's get into the Q&A section. Feel free to drop
[00:46:05.300 --> 00:46:12.980]   in questions. We are taking up questions as well. We have enough time. Yeah, I mean, if
[00:46:12.980 --> 00:46:17.820]   you have any questions, let me get started then. Matt, I have a question for you because
[00:46:17.820 --> 00:46:31.020]   we took one question Keras CV specific kind of. So it's like, obviously most of the developments,
[00:46:31.020 --> 00:46:39.920]   LLM developments are done in PyTorch, specifically PyTorch. And I believe that there's a difference
[00:46:39.920 --> 00:46:46.700]   in the technical aspects that we have to cover in order to start fine tuning or doing inference
[00:46:46.700 --> 00:46:55.580]   on large 7B or 7TB models. But I was just curious, what are the large language models
[00:46:55.580 --> 00:47:03.020]   in the scale that Keras supports out of the box right now? And given we can use a PyTorch
[00:47:03.020 --> 00:47:10.660]   backbone with Keras code and Keras NLP in this case, how can we use like a PyTorch GPT
[00:47:10.660 --> 00:47:17.460]   model instead of a Keras NLP GPT model? - Yeah, you definitely should be able to use
[00:47:17.460 --> 00:47:25.340]   as Ian was showing, like if you want to use a model written in PyTorch and wrap it in
[00:47:25.340 --> 00:47:35.900]   kind of Keras training APIs, you can definitely do so. One trick though to call out with NLP
[00:47:35.900 --> 00:47:42.300]   in particular is if you are using a pre-trained weights from somewhere, you need to make sure
[00:47:42.300 --> 00:47:47.460]   your tokenization is exactly one-to-one with wherever tokenization has been used somewhere
[00:47:47.460 --> 00:47:52.860]   else. You can't, the inputs to the model need to be exact or you're not going to get any
[00:47:52.860 --> 00:47:59.900]   results. But you could use Keras NLP to get the kind of the exact same tokenization that
[00:47:59.900 --> 00:48:05.260]   you would get for a model and then load up a Torch-only version of a model if you wanted
[00:48:05.260 --> 00:48:08.020]   to. - Nice.
[00:48:08.020 --> 00:48:19.420]   - Yeah. And then in terms of models that we have, we have, yeah, GPT2, OPT, T5 is on the
[00:48:19.420 --> 00:48:23.540]   way, some sequence-to-sequence models. And then we're actually hoping very soon to bring
[00:48:23.540 --> 00:48:31.060]   a lot of kind of these bigger language models such as Falcon, like nice permissively licensed
[00:48:31.060 --> 00:48:35.900]   large language models. Then we should have those available soon.
[00:48:35.900 --> 00:48:41.820]   - Yeah, that's so cool. Just a quick follow-up and then I can let it, yeah, then maybe Somi
[00:48:41.820 --> 00:48:49.220]   can answer a couple of questions. The follow-up is, like say if you are using a PyTorch PT
[00:48:49.220 --> 00:48:55.580]   model, language model, can we, and you showed the generate method, can we kind of like write
[00:48:55.580 --> 00:49:00.660]   our own generate method and like overwrite the existing generate method?
[00:49:00.660 --> 00:49:08.620]   - Yes. So yeah, the overall generation is, I think, compared to the like super very simple
[00:49:08.620 --> 00:49:12.860]   setup that you get when you're like training a causal language model, generation is a fair
[00:49:12.860 --> 00:49:17.020]   bit more complex because you have to deal with, especially if you want to performant
[00:49:17.020 --> 00:49:21.820]   generation like in JAX, you do need to make sure your while loop is compiled. You want
[00:49:21.820 --> 00:49:25.940]   to make sure you're caching all of these kind of intermediate states, the kind of key value
[00:49:25.940 --> 00:49:33.300]   tensors that you compute in your transformer. So it's certainly like writing kind of custom
[00:49:33.300 --> 00:49:38.500]   generation code is a decent step up in difficulty just in the subject matter, I think, but it's
[00:49:38.500 --> 00:49:45.260]   definitely something that we support. If you want to be writing your own generation step,
[00:49:45.260 --> 00:49:51.060]   you can kind of see the generation step that we provide and customize it. And it is a place
[00:49:51.060 --> 00:49:56.260]   that we want to support people doing that. The other thing we do, which is a little easier
[00:49:56.260 --> 00:50:00.340]   to do is you can pass your own sampler. So if you want to use like different sampling
[00:50:00.340 --> 00:50:05.940]   methods like top K or contrastive search has been shown to be very effective in recent
[00:50:05.940 --> 00:50:11.020]   years, you can do that in a much easier way by just passing like a sampler equals contrastive
[00:50:11.020 --> 00:50:15.100]   to our compile and the other.
[00:50:15.100 --> 00:50:23.220]   >> Nice. Subed, do you want to take questions from the chat?
[00:50:23.220 --> 00:50:31.380]   >> Yes, I think there's a question. So how do you compare the runtime for different back
[00:50:31.380 --> 00:50:36.580]   ends with Keras code?
[00:50:36.580 --> 00:50:41.700]   >> When you say runtimes, do you mean the literal wall clock times or do you mean the
[00:50:41.700 --> 00:50:46.660]   runtime in the like programming sense of a runtime?
[00:50:46.660 --> 00:50:52.620]   >> Probably in the programming sense. I'm not sure this came from the chat.
[00:50:52.620 --> 00:50:55.860]   >> Okay. Well, there's sort of --
[00:50:55.860 --> 00:50:58.860]   >> Wall clock time.
[00:50:58.860 --> 00:51:05.700]   >> Okay. Yeah. I'll touch on both briefly. It's pretty easy to compare the numerics because
[00:51:05.700 --> 00:51:10.100]   we have a bunch of models where we reload the weights in all of our different back ends
[00:51:10.100 --> 00:51:14.940]   and we run tests that verify that the output of those models are the same across back ends.
[00:51:14.940 --> 00:51:20.500]   And then from models all the way down to like individual layers, components, et cetera.
[00:51:20.500 --> 00:51:24.580]   For like wall clock time, like how fast are these things, it's actually -- it's pretty
[00:51:24.580 --> 00:51:30.460]   easy because the configuration is just such that you specify your back end as an environment
[00:51:30.460 --> 00:51:36.780]   variable so we can spin up to identical GCP instances that have the exact same resources
[00:51:36.780 --> 00:51:41.660]   and have them both have different environment variables for back end and run the exact same
[00:51:41.660 --> 00:51:47.860]   Keras training or evaluation job and just find them. It's actually quite straightforward.
[00:51:47.860 --> 00:51:53.540]   >> Yeah. So we've been doing a lot of kind of benchmarking for Keras core and it's actually
[00:51:53.540 --> 00:51:58.700]   super fun because Keras core kind of lowers the barrier between switching between frameworks.
[00:51:58.700 --> 00:52:04.260]   It's really interesting to see Torch has a lot of kind of very optimized ops specifically
[00:52:04.260 --> 00:52:09.060]   for computer vision and we're able to kind of see how performant those are. On the NLP
[00:52:09.060 --> 00:52:15.820]   side I know that JAX is actually -- because of kind of all of these nice functional assumptions
[00:52:15.820 --> 00:52:20.380]   it makes, it can really optimize training really well and we've actually seen it perform
[00:52:20.380 --> 00:52:25.500]   like a hair better than any of the other frameworks in actual running fit. And then on the inference
[00:52:25.500 --> 00:52:30.420]   side I think TensorFlow like on the generation side for inference has actually been a little
[00:52:30.420 --> 00:52:34.740]   bit faster. So it's kind of fun to be able to just flip the switch between back ends
[00:52:34.740 --> 00:52:39.740]   and be able to check what performance is for a given model.
[00:52:39.740 --> 00:52:46.580]   >> Nice. Thanks for answering that. Another question from the chat is around distributed
[00:52:46.580 --> 00:52:51.380]   training. So the question is how will Keras core handle distributed training with different
[00:52:51.380 --> 00:52:58.380]   back ends and specifically model parallelism and not just data parallelism?
[00:52:58.380 --> 00:53:03.060]   >> Yeah. >> You want to go ahead.
[00:53:03.060 --> 00:53:09.060]   >> Yeah. We are -- so today we'll work like I was saying if you want to write your own
[00:53:09.060 --> 00:53:14.260]   custom train stuff in JAX you can use any of the JAX APIs for doing model parallelism.
[00:53:14.260 --> 00:53:19.380]   Same with Torch and TensorFlow. If you kind of go specific on a back end you can use the
[00:53:19.380 --> 00:53:25.140]   distribution APIs that exist today. In TensorFlow we have the TF distribute API and you can
[00:53:25.140 --> 00:53:31.540]   just make a Keras core model, run it inside of a TF distribute scope and things will just
[00:53:31.540 --> 00:53:35.940]   work. But it's an interesting question you ask because we are actually actively developing
[00:53:35.940 --> 00:53:45.820]   a API that will be a nice cross platform API for model parallelism that will just work
[00:53:45.820 --> 00:53:50.140]   as you switch back ends between JAX and Torch and TensorFlow and we're super excited about
[00:53:50.140 --> 00:53:56.820]   that so stay tuned for more details there. >> You can see some of the development of that.
[00:53:56.820 --> 00:54:01.020]   There's a little RFCPR that you can find on Keras core because we're all open source you
[00:54:01.020 --> 00:54:06.460]   can look at it. And Scott who's a real rock star on the Keras team has been building out
[00:54:06.460 --> 00:54:11.100]   this API and it's quite impressive. We're really excited to share it with users. He
[00:54:11.100 --> 00:54:17.580]   was able to train a large language model using model parallelism with like a dozen lines
[00:54:17.580 --> 00:54:24.780]   of code. So and I think Francois will definitely be sharing that on socials as well when it's
[00:54:24.780 --> 00:54:28.260]   officially released which hopefully isn't well it will definitely be before the Keras
[00:54:28.260 --> 00:54:38.260]   core 1.0 release but you can expect to see it in the coming weeks I would say.
[00:54:38.260 --> 00:54:45.580]   >> I have a quick question from my end. So for parallelism on the PyTorch side so is
[00:54:45.580 --> 00:54:54.940]   there any plan to integrate with some tools for example Hugging Face Accelerate to support
[00:54:54.940 --> 00:55:00.260]   that kind of parallelism out of the box using Keras core?
[00:55:00.260 --> 00:55:05.140]   >> So yes. PyTorch is something that our team has a little bit less expertise in and so
[00:55:05.140 --> 00:55:09.860]   the time horizon is going to be a little longer on that but we do intend to support PyTorch
[00:55:09.860 --> 00:55:14.940]   behind our distribution API and that also includes compilation. We're still scoping
[00:55:14.940 --> 00:55:18.020]   exactly how that's going to work.
[00:55:18.020 --> 00:55:21.820]   >> But I actually do know there's an issue specifically on Keras core for PyTorch Accelerate
[00:55:21.820 --> 00:55:27.340]   where I think someone has already gone ahead and checked that you were you actually on
[00:55:27.340 --> 00:55:29.420]   that issue you made that.
[00:55:29.420 --> 00:55:33.180]   >> I actually asked Anshuman to make the issue.
[00:55:33.180 --> 00:55:39.420]   >> Oh nice. So yeah I do think you can run PyTorch Accelerate with Keras core models
[00:55:39.420 --> 00:55:44.860]   on the PyTorch backend. But yeah I think overall for us the distribution that we're really
[00:55:44.860 --> 00:55:52.900]   excited about is distribution code you can write once, write simply and take across backends.
[00:55:52.900 --> 00:55:56.340]   Thanks for the answer.
[00:55:56.340 --> 00:56:01.860]   >> Thanks there. I have a really nice question and I guess this is going to be a good question
[00:56:01.860 --> 00:56:08.500]   for most of the folks in the audience. So someone asked that coming from PyTorch are
[00:56:08.500 --> 00:56:16.820]   there any suggestions for the best fast track way of learning Keras?
[00:56:16.820 --> 00:56:22.660]   >> I think that's a great question. We don't have a lot of resources yet on how to for
[00:56:22.660 --> 00:56:27.620]   example port a PyTorch thing to Keras core. You'll find that generally the ops namespace
[00:56:27.620 --> 00:56:34.380]   in Keras core is quite closely tied to the torch API with a handful of small exceptions.
[00:56:34.380 --> 00:56:39.940]   I would personally strongly recommend Francois' book on Keras. It's called Deep Learning with
[00:56:39.940 --> 00:56:45.740]   Keras. I'm not just saying that because he's my boss. It's actually a really great resource.
[00:56:45.740 --> 00:56:47.820]   It's how I learned about deep learning.
[00:56:47.820 --> 00:56:49.300]   >> Deep learning with Python I think.
[00:56:49.300 --> 00:56:51.100]   >> Oh sorry what did I call it?
[00:56:51.100 --> 00:56:52.100]   >> Deep learning with Keras.
[00:56:52.100 --> 00:56:57.780]   >> Oh sorry. Deep learning with Python. Excuse me. Don't tell him I said that. As well as
[00:56:57.780 --> 00:57:03.780]   Keras.io. Keras.io has some really cool examples. There's literally hundreds of examples of
[00:57:03.780 --> 00:57:07.540]   how to do this thing in Keras, how to do that thing in Keras. It serves as a really good
[00:57:07.540 --> 00:57:13.020]   piece of API documentation but also of how to. That would be my recommendation. Not sure
[00:57:13.020 --> 00:57:14.620]   Matt if you have a different take on that.
[00:57:14.620 --> 00:57:19.380]   >> No I totally agree. If you go to Keras.io and you're actually specifically interested
[00:57:19.380 --> 00:57:24.900]   in the multi backend stuff, you'll notice there's a spot there, Keras core. Just to
[00:57:24.900 --> 00:57:29.820]   make sure all the terminology is clear, Keras core is what will become eventually Keras
[00:57:29.820 --> 00:57:34.540]   3 and the new stable version of Keras. Right now it's got its own little breakdown section
[00:57:34.540 --> 00:57:40.820]   on Keras.io and you can read all of our kind of basic developer guides written from a multi
[00:57:40.820 --> 00:57:45.900]   backend perspective there. That would be a great place to start for this stuff we've
[00:57:45.900 --> 00:57:46.900]   been covering in this talk.
[00:57:46.900 --> 00:57:57.860]   >> Awesome, Matt. Sorry. Thanks, Matt and Ian for that answer. I hope that helped. We
[00:57:57.860 --> 00:58:03.380]   have one minute. I just want to take this time to say that I'm not sure if we were able
[00:58:03.380 --> 00:58:08.740]   to transfer this energy. We're very excited to have you over for this webinar because
[00:58:08.740 --> 00:58:14.260]   we have been using Keras for a long time and we really are excited that Keras core is out
[00:58:14.260 --> 00:58:20.740]   there and we can literally use all the goodness of PyTorch, Jax and TensorFlow with the same
[00:58:20.740 --> 00:58:26.820]   abstraction there. It's just great. I highly recommend folks to try it out, play with it,
[00:58:26.820 --> 00:58:33.540]   try to implement the next deep learning project with Keras core. That would be really fun.
[00:58:33.540 --> 00:58:39.940]   Plus, yeah, just excited to try it myself as well. I've been trying it but tried more.
[00:58:39.940 --> 00:58:48.940]   I would also like to share with the participants here that we are actively maintaining integration
[00:58:48.940 --> 00:58:56.700]   with Keras ecosystem. We have nice callbacks that you can use and that will start doing
[00:58:56.700 --> 00:59:01.300]   model check pointing and experiment tracking and you can even customize our evaluation
[00:59:01.300 --> 00:59:06.500]   callbacks to build your own stuff. With the Keras core, we are going to be updating the
[00:59:06.500 --> 00:59:11.260]   current existing callbacks that we have so that it supports all the functionality out
[00:59:11.260 --> 00:59:19.500]   of the box for Keras core. So stay tuned there as well. And yeah, just if you have questions,
[00:59:19.500 --> 00:59:25.780]   write to us, tweet about it or yeah, just open an issue in any of the Keras ecosystems
[00:59:25.780 --> 00:59:32.980]   library and yeah, they will take it from there. Matt, Ian, Sumit, want to say the ending things?
[00:59:32.980 --> 00:59:39.100]   Yeah, I just wanted to say thanks so much for having us. This was fun and I love using
[00:59:39.100 --> 00:59:44.940]   Weights and Biases and the Keras integration support makes things super easy. So yeah,
[00:59:44.940 --> 00:59:48.860]   thanks for bringing us on. And we love using Keras.
[00:59:48.860 --> 00:59:53.780]   Yeah, thank you all for everything you've done for the community and for having us here
[00:59:53.780 --> 00:59:57.780]   today. It's always a pleasure to get to share what we've been working on and we're really
[00:59:57.780 --> 01:00:03.100]   excited about Weights and Biases and Keras moving forward.
[01:00:03.100 --> 01:00:10.700]   Awesome. Yeah, we are already about over time, so we will be closing in. Thank you all for
[01:00:10.700 --> 01:00:16.140]   joining. We really appreciate the patience and I hope you all learned something and do
[01:00:16.140 --> 01:00:19.060]   try out Keras. See you all.

