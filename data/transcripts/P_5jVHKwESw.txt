
[00:00:00.000 --> 00:00:03.160]   (soft music)
[00:00:03.160 --> 00:00:05.160]   All right, so we're gonna get started.
[00:00:05.160 --> 00:00:07.520]   This is session two, finally.
[00:00:07.520 --> 00:00:11.940]   And we're essentially looking at the second section
[00:00:11.940 --> 00:00:15.720]   in the Hugging Face official course.
[00:00:15.720 --> 00:00:18.920]   And so this week we'll be looking at...
[00:00:18.920 --> 00:00:22.880]   So last week we looked at their high level pipeline API,
[00:00:22.880 --> 00:00:25.320]   which is a really friendly,
[00:00:25.320 --> 00:00:28.200]   two or three lines of code to be able to use
[00:00:28.200 --> 00:00:32.000]   some of the core NLP tasks with raw text.
[00:00:32.000 --> 00:00:34.920]   And so we're going to be looking at this week
[00:00:34.920 --> 00:00:37.580]   how to actually do these things ourselves.
[00:00:37.580 --> 00:00:40.700]   So working with tokenizers, working with models,
[00:00:40.700 --> 00:00:44.160]   and then working with the outputs directly.
[00:00:44.160 --> 00:00:47.060]   And then we'll also take a look at Blur.
[00:00:47.060 --> 00:00:48.400]   And so, as I mentioned,
[00:00:48.400 --> 00:00:52.640]   there's at least three fast AI Hugging Face
[00:00:52.640 --> 00:00:54.720]   integration libraries.
[00:00:54.720 --> 00:00:57.160]   So Blur is one of them, that's the one I created.
[00:00:58.000 --> 00:01:01.680]   And we'll look at how it handles tokenization
[00:01:01.680 --> 00:01:03.740]   and models in a fast AI world.
[00:01:03.740 --> 00:01:06.040]   And then next week,
[00:01:06.040 --> 00:01:08.120]   we're gonna be looking at all three frameworks.
[00:01:08.120 --> 00:01:11.360]   We're gonna have folks presenting a demo
[00:01:11.360 --> 00:01:13.700]   using the same data set and task.
[00:01:13.700 --> 00:01:14.960]   So you can get an idea and feel
[00:01:14.960 --> 00:01:17.200]   for some of the design decisions,
[00:01:17.200 --> 00:01:18.560]   as well as how they work.
[00:01:18.560 --> 00:01:23.560]   And you can choose one or more to use in your own work.
[00:01:23.560 --> 00:01:26.880]   So resources again,
[00:01:27.720 --> 00:01:31.240]   the Steady Group registration page is up there.
[00:01:31.240 --> 00:01:33.400]   Hopefully we'll be back online next week
[00:01:33.400 --> 00:01:36.000]   without any disruptions.
[00:01:36.000 --> 00:01:39.080]   Until then, we have the Steady Group Discord.
[00:01:39.080 --> 00:01:42.680]   And also the presentation
[00:01:42.680 --> 00:01:45.000]   and the CoLab notebooks we'll be looking at
[00:01:45.000 --> 00:01:47.000]   will be shared at the end of this.
[00:01:47.000 --> 00:01:48.880]   So we'll share them on the Discord
[00:01:48.880 --> 00:01:53.100]   and probably also through Weights and Biases.
[00:01:53.100 --> 00:01:55.000]   Again, if you're new to fast AI,
[00:01:55.000 --> 00:01:57.360]   check out the course, check out Zach's
[00:01:57.360 --> 00:01:59.360]   Walk with Fast AI course as well.
[00:01:59.360 --> 00:02:02.160]   There's Fastbook, which is great.
[00:02:02.160 --> 00:02:03.400]   There's a reading group going on
[00:02:03.400 --> 00:02:06.000]   that Aman's doing from Weights and Biases.
[00:02:06.000 --> 00:02:07.600]   It's worth checking out.
[00:02:07.600 --> 00:02:10.520]   And in terms of the Hugging Face libraries,
[00:02:10.520 --> 00:02:11.880]   they integrate with Fast AI.
[00:02:11.880 --> 00:02:13.640]   I have those listed here.
[00:02:13.640 --> 00:02:14.960]   And then of course, we've got to mention
[00:02:14.960 --> 00:02:17.880]   the Chai Time Data Science Podcast.
[00:02:17.880 --> 00:02:19.120]   The only thing that could be better about it
[00:02:19.120 --> 00:02:21.760]   is if it was renamed to the Dark Coffee Roast
[00:02:21.760 --> 00:02:23.120]   Time Data Science Podcast,
[00:02:23.120 --> 00:02:27.560]   but I'll let Sonia think about that over the next week.
[00:02:27.560 --> 00:02:28.960]   And then there's Weights and Biases
[00:02:28.960 --> 00:02:32.400]   for experiment tracking and those types of things.
[00:02:32.400 --> 00:02:37.400]   So these are the resources and we'll get right into it.
[00:02:37.400 --> 00:02:42.360]   So essentially when we look at what's going on
[00:02:42.360 --> 00:02:44.600]   in the Pipeline API,
[00:02:44.600 --> 00:02:46.920]   there's really three things that are happening.
[00:02:46.920 --> 00:02:50.240]   Is one, it's tokenizing your raw text.
[00:02:50.240 --> 00:02:52.200]   And again, last week we talked about that.
[00:02:52.200 --> 00:02:54.360]   For anything to go through a model,
[00:02:54.360 --> 00:02:57.560]   we have to create a numerical representation of it.
[00:02:57.560 --> 00:02:59.880]   And so that's what the tokenizer does.
[00:02:59.880 --> 00:03:03.720]   And essentially the task of the tokenizer
[00:03:03.720 --> 00:03:08.560]   is to split words into sub words or symbols called tokens.
[00:03:08.560 --> 00:03:11.240]   And there's various tokenization strategies
[00:03:11.240 --> 00:03:13.080]   each with their pros and cons.
[00:03:13.080 --> 00:03:15.320]   And we'll look at that a little bit later.
[00:03:15.320 --> 00:03:16.280]   But at the end of the day,
[00:03:16.280 --> 00:03:20.200]   each of these tokens is mapped to an integer.
[00:03:20.200 --> 00:03:25.200]   And that integer is used to index into a dictionary
[00:03:25.200 --> 00:03:30.680]   or a list essentially for each token.
[00:03:30.680 --> 00:03:34.520]   And that's how we actually,
[00:03:34.520 --> 00:03:36.800]   those are the numbers that we actually pipe in
[00:03:36.800 --> 00:03:38.200]   through our model.
[00:03:38.200 --> 00:03:40.440]   And when the tokenizer runs,
[00:03:40.440 --> 00:03:42.880]   as you've seen last week and we'll see today,
[00:03:42.880 --> 00:03:47.080]   is that it actually creates not just identifiers,
[00:03:47.080 --> 00:03:48.800]   the IDs for each token,
[00:03:48.800 --> 00:03:51.840]   but it also creates other tensors.
[00:03:51.840 --> 00:03:54.880]   So we've seen like the concept of an attention mask
[00:03:54.880 --> 00:03:58.120]   or for some models like BERT,
[00:03:58.120 --> 00:04:02.800]   it will create a token type IDs tensor as well.
[00:04:02.800 --> 00:04:04.280]   And because remember BERT was trained
[00:04:04.280 --> 00:04:07.240]   on a next sentence prediction task.
[00:04:07.240 --> 00:04:09.760]   So there's a first sentence and second sentence.
[00:04:09.760 --> 00:04:14.160]   So it uses these token type IDs to differentiate the two.
[00:04:14.160 --> 00:04:16.760]   And then the big thing to remember
[00:04:16.760 --> 00:04:19.120]   is that when you're using transformers
[00:04:19.120 --> 00:04:22.280]   is that all this pre-processing needs to be done
[00:04:22.280 --> 00:04:26.200]   in exactly the same way as when the model was pre-trained.
[00:04:26.200 --> 00:04:28.000]   And that essentially means that
[00:04:28.000 --> 00:04:31.920]   when you're going to train a transformer
[00:04:31.920 --> 00:04:33.440]   based on a checkpoint,
[00:04:33.440 --> 00:04:36.680]   you wanna get a tokenizer from the same checkpoint.
[00:04:36.680 --> 00:04:38.720]   So the indices are the same.
[00:04:38.720 --> 00:04:40.800]   And also remember that one of the first things
[00:04:40.800 --> 00:04:42.480]   these transformer models do
[00:04:42.480 --> 00:04:46.200]   is actually those indices are turned into an embedding.
[00:04:46.200 --> 00:04:49.080]   So a high dimensional vector to represent each token,
[00:04:49.080 --> 00:04:50.920]   and that's something that's learned.
[00:04:50.920 --> 00:04:53.000]   And so if you wanna take advantage
[00:04:53.000 --> 00:04:56.560]   of the pre-trained checkpoint as a model,
[00:04:56.560 --> 00:04:59.360]   you're gonna wanna use the same tokenizer
[00:04:59.360 --> 00:05:00.680]   they use to pre-train it,
[00:05:00.680 --> 00:05:03.000]   or essentially you're starting from scratch.
[00:05:03.000 --> 00:05:04.520]   So just keep that in mind.
[00:05:04.520 --> 00:05:06.600]   Like most people, at least for myself,
[00:05:06.600 --> 00:05:09.440]   I've never trained a tokenizer custom.
[00:05:09.440 --> 00:05:13.240]   I've always just used whatever the checkpoint provides.
[00:05:15.320 --> 00:05:18.560]   So the second part of the pipeline
[00:05:18.560 --> 00:05:23.120]   is once you have your raw text tokenized,
[00:05:23.120 --> 00:05:24.720]   so converted the numbers,
[00:05:24.720 --> 00:05:27.840]   as well as some of the other things like the attention mask
[00:05:27.840 --> 00:05:29.840]   and token type IDs,
[00:05:29.840 --> 00:05:33.920]   is we actually run those inputs through a model.
[00:05:33.920 --> 00:05:37.120]   And this base kind of auto model,
[00:05:37.120 --> 00:05:39.400]   we talked about those, like there's auto model,
[00:05:39.400 --> 00:05:41.960]   there's auto model for various tasks.
[00:05:41.960 --> 00:05:46.280]   The base auto model simply is going to output
[00:05:46.280 --> 00:05:48.240]   a feature vector for each of those tokens
[00:05:48.240 --> 00:05:51.320]   that goes through the model.
[00:05:51.320 --> 00:05:54.440]   And so that's also known as the hidden states.
[00:05:54.440 --> 00:05:57.760]   We also refer to it as features.
[00:05:57.760 --> 00:06:01.200]   And again, it is essentially a representation
[00:06:01.200 --> 00:06:03.960]   for each token of what the model has learned
[00:06:03.960 --> 00:06:06.760]   based on the text that is trained with.
[00:06:06.760 --> 00:06:10.880]   And so if we were looking at the output,
[00:06:10.880 --> 00:06:12.480]   we would see something,
[00:06:12.480 --> 00:06:15.440]   we would see a tensor that looked like batch size
[00:06:15.440 --> 00:06:18.320]   by sequence by hidden state.
[00:06:18.320 --> 00:06:20.400]   So if we had a batch size of four
[00:06:20.400 --> 00:06:24.120]   and we are using BERT, for example,
[00:06:24.120 --> 00:06:27.680]   that allows for a max 512 and the base checkpoint,
[00:06:27.680 --> 00:06:32.680]   which basically produces a vector of, I think, 768 numbers,
[00:06:32.680 --> 00:06:39.320]   we would see something like four by 512 by 768.
[00:06:39.320 --> 00:06:41.840]   And so we'll look at, we get to the CoLab also
[00:06:41.840 --> 00:06:44.240]   kind of how to look at those things.
[00:06:44.240 --> 00:06:48.480]   So that's essentially what the kind of
[00:06:48.480 --> 00:06:50.360]   the raw auto model does.
[00:06:50.360 --> 00:06:52.920]   And then what we do for different tasks,
[00:06:52.920 --> 00:06:54.840]   whether it's sequence classification,
[00:06:54.840 --> 00:06:58.520]   named entity recognition, summarization,
[00:06:58.520 --> 00:07:02.680]   is that we switch the head and we'll kind of look at this,
[00:07:02.680 --> 00:07:07.000]   we'll show a way in CoLab how to actually look at the model.
[00:07:07.000 --> 00:07:09.640]   And you can see that the head is replaced
[00:07:09.640 --> 00:07:11.880]   by something specific to the task.
[00:07:11.880 --> 00:07:15.640]   And so it could be as simple as an in.linear layer.
[00:07:15.640 --> 00:07:20.000]   It could be a couple separated by some non-linearity,
[00:07:20.000 --> 00:07:22.760]   but the head is custom and it's typically,
[00:07:22.760 --> 00:07:25.120]   depending on the checkpoint,
[00:07:25.120 --> 00:07:27.920]   typically these are randomized weights
[00:07:27.920 --> 00:07:31.680]   that during the training process, you're going to learn.
[00:07:31.680 --> 00:07:36.240]   And then the last step in the pipeline
[00:07:36.240 --> 00:07:39.800]   is actually the post-processing of the outputs.
[00:07:39.800 --> 00:07:42.080]   So one of the things I really like about
[00:07:42.080 --> 00:07:46.480]   the Transformers architecture is that
[00:07:46.480 --> 00:07:50.880]   both the inputs and outputs are specialized objects.
[00:07:50.880 --> 00:07:53.760]   So if you're looking in Jupyter Notebook
[00:07:53.760 --> 00:07:57.800]   or in CoLab or VS Code, you could actually do inputs.
[00:07:57.800 --> 00:08:00.480]   and get IntelliSense and see what's in there.
[00:08:00.480 --> 00:08:04.720]   And so amongst other things that it returns,
[00:08:04.720 --> 00:08:08.760]   or you can ask it to return are the raw logits.
[00:08:08.760 --> 00:08:13.560]   So again, those are just numbers that the model predicts.
[00:08:13.560 --> 00:08:16.200]   And we can tell like which ones it's favoring,
[00:08:16.200 --> 00:08:19.440]   but if we wanna convert these to actual probabilities,
[00:08:19.440 --> 00:08:21.440]   we can see the confidence the model's assigning
[00:08:21.440 --> 00:08:24.880]   to the different labels in this scenario.
[00:08:24.880 --> 00:08:27.840]   We run it through Softmax.
[00:08:27.840 --> 00:08:30.640]   And then here we can see that this first one is showing
[00:08:33.840 --> 00:08:38.840]   for 96% label one, 99% label zero here.
[00:08:38.840 --> 00:08:45.600]   And this is really, most neural networks,
[00:08:45.600 --> 00:08:48.720]   most loss functions fuse or combine
[00:08:48.720 --> 00:08:52.520]   the final activation with the loss.
[00:08:52.520 --> 00:08:54.920]   And so this is typically handled for you.
[00:08:54.920 --> 00:08:56.960]   And so if you look at like libraries like FastAI,
[00:08:56.960 --> 00:08:59.920]   you will actually see the results look something like this.
[00:08:59.920 --> 00:09:03.120]   You don't have to actually run them through Softmax.
[00:09:04.120 --> 00:09:07.360]   And so with that, we'll look at a demo,
[00:09:07.360 --> 00:09:11.000]   but before that, any questions?
[00:09:11.000 --> 00:09:16.080]   - I think we're all still recovering from the bombing,
[00:09:16.080 --> 00:09:18.480]   but I'll just ask a silly question.
[00:09:18.480 --> 00:09:21.760]   How do you pick the transformer that you want to run?
[00:09:21.760 --> 00:09:25.520]   Assuming you're totally new to like this ecosystem,
[00:09:25.520 --> 00:09:28.240]   how do you pick which model you want to start running?
[00:09:28.240 --> 00:09:31.440]   - You know, for me, a lot of it is,
[00:09:32.440 --> 00:09:35.640]   I think if you go to like the hugging face,
[00:09:35.640 --> 00:09:38.200]   like their forums, you can ask questions
[00:09:38.200 --> 00:09:40.000]   and see what folks are using.
[00:09:40.000 --> 00:09:42.800]   For me, it's a little bit of trial and error,
[00:09:42.800 --> 00:09:44.960]   and also just kind of reading other papers
[00:09:44.960 --> 00:09:49.760]   and seeing the experiences other folks have had.
[00:09:49.760 --> 00:09:52.760]   So I'm not sure if there's like a great resource
[00:09:52.760 --> 00:09:54.760]   that says like, you know, for this specific task
[00:09:54.760 --> 00:09:58.480]   and this specific type of corpus, use this transformer.
[00:09:58.480 --> 00:10:00.560]   But I think if you ask them the forums,
[00:10:00.560 --> 00:10:02.560]   you'll probably get some feedback
[00:10:02.560 --> 00:10:04.320]   on the hugging face forums.
[00:10:04.320 --> 00:10:06.560]   I know for me, for like sequence classification,
[00:10:06.560 --> 00:10:10.720]   I've had really good experience using Roberta and BART.
[00:10:10.720 --> 00:10:14.760]   And I'm usually working on English texts that are,
[00:10:14.760 --> 00:10:16.920]   like at least for my work,
[00:10:16.920 --> 00:10:20.400]   a lot of like survey type comments, things like that.
[00:10:20.400 --> 00:10:23.280]   Those have proven to work pretty well.
[00:10:23.280 --> 00:10:25.960]   And then of course you can actually use weights and biases
[00:10:25.960 --> 00:10:29.280]   and like a tuning framework like Optuna,
[00:10:29.280 --> 00:10:31.040]   and you can actually try different architectures
[00:10:31.040 --> 00:10:32.200]   and see what happens.
[00:10:32.200 --> 00:10:35.280]   - Thanks.
[00:10:35.280 --> 00:10:38.040]   Lucas mentioned that he usually filters down
[00:10:38.040 --> 00:10:39.840]   hugging face, tortco slash models
[00:10:39.840 --> 00:10:41.560]   for the specific language and task.
[00:10:41.560 --> 00:10:43.680]   And that's a good starting point as well.
[00:10:43.680 --> 00:10:44.880]   - Yep, yep.
[00:10:44.880 --> 00:10:46.360]   And what's nice there is that a lot of them
[00:10:46.360 --> 00:10:47.880]   actually have their metrics
[00:10:47.880 --> 00:10:51.000]   that they've got from their validation set.
[00:10:51.000 --> 00:10:53.560]   So, and again, this thing is find something
[00:10:53.560 --> 00:10:56.360]   that's like using that approach.
[00:10:56.360 --> 00:10:59.080]   If you kind of know what your raw text looks like,
[00:10:59.080 --> 00:10:59.920]   you can go ahead.
[00:10:59.920 --> 00:11:01.200]   If you know you're working with English,
[00:11:01.200 --> 00:11:03.560]   you're working with Chinese, you can start there
[00:11:03.560 --> 00:11:07.080]   and at least start with some good pre-trained models,
[00:11:07.080 --> 00:11:09.840]   irrespective of the architecture for sure.
[00:11:09.840 --> 00:11:13.520]   - That's really cool, thanks.
[00:11:13.520 --> 00:11:14.720]   - Cool.
[00:11:14.720 --> 00:11:16.760]   Okay, so we're gonna go and first look,
[00:11:16.760 --> 00:11:21.400]   and this is essentially from the course here.
[00:11:21.400 --> 00:11:23.480]   And so we're gonna look at
[00:11:23.480 --> 00:11:27.600]   just what I just talked about right there is like,
[00:11:27.600 --> 00:11:32.400]   so here's the, again, the pipeline for sentiment analysis.
[00:11:32.400 --> 00:11:37.200]   And remember by default, this one is classifying two labels.
[00:11:37.200 --> 00:11:38.400]   So if you have more labels,
[00:11:38.400 --> 00:11:40.520]   you would have to pass that in here.
[00:11:40.520 --> 00:11:44.200]   And also this is something from last week.
[00:11:44.200 --> 00:11:49.200]   If you wanna see the exact model that the pipeline's using,
[00:11:49.200 --> 00:11:54.840]   you can actually print use model.name or path to get that.
[00:11:56.400 --> 00:11:59.600]   And of course you can actually pass in your own
[00:11:59.600 --> 00:12:02.080]   pre-trained model name or path as well up here
[00:12:02.080 --> 00:12:04.760]   if you have something specific you wanna use.
[00:12:04.760 --> 00:12:07.560]   So again, with the checkpoints is that
[00:12:07.560 --> 00:12:10.560]   we're gonna use the same checkpoint for tokenization
[00:12:10.560 --> 00:12:14.760]   as we do for getting our model so that everything lines up.
[00:12:14.760 --> 00:12:19.800]   So I'm gonna go ahead and use auto tokenizer.
[00:12:19.800 --> 00:12:26.120]   And then we'll take a look at the inputs.
[00:12:26.240 --> 00:12:31.000]   So using this particular tokenizer, DistilBERT,
[00:12:31.000 --> 00:12:33.160]   we only get two things.
[00:12:33.160 --> 00:12:35.120]   We get the input IDs, which again,
[00:12:35.120 --> 00:12:37.320]   the indices into the vocab.
[00:12:37.320 --> 00:12:39.160]   And there's also some special IDs
[00:12:39.160 --> 00:12:41.280]   that the tokenizer automatically puts in there
[00:12:41.280 --> 00:12:43.600]   that the architecture requires for training.
[00:12:43.600 --> 00:12:46.400]   So in DistilBERT and in BERT,
[00:12:46.400 --> 00:12:48.680]   you'll see like this class token,
[00:12:48.680 --> 00:12:52.480]   and you'll see like a separator token, SCP.
[00:12:52.480 --> 00:12:57.480]   And that's token 101, and this is token 102.
[00:12:57.480 --> 00:13:03.200]   And then you can see that we also have these
[00:13:03.200 --> 00:13:06.200]   zero indices right here,
[00:13:06.200 --> 00:13:09.760]   for this one right here for this particular text.
[00:13:09.760 --> 00:13:13.720]   And so remember that when we feed our inputs into a model,
[00:13:13.720 --> 00:13:16.280]   we're typically feeding them in batches,
[00:13:16.280 --> 00:13:18.560]   and that needs to be a square matrix.
[00:13:18.560 --> 00:13:20.720]   And so to make that happen,
[00:13:20.720 --> 00:13:22.920]   we actually use a padding token
[00:13:22.920 --> 00:13:25.960]   so that we have that squared matrix.
[00:13:25.960 --> 00:13:27.760]   And then we use an attention mask.
[00:13:27.760 --> 00:13:28.920]   So you can see in the attention mask,
[00:13:28.920 --> 00:13:31.040]   we have some zeros here to say,
[00:13:31.040 --> 00:13:32.560]   when you get to the self-attention layers,
[00:13:32.560 --> 00:13:35.000]   don't pay attention to those padding tokens.
[00:13:35.000 --> 00:13:36.600]   So that's what that means there.
[00:13:36.600 --> 00:13:40.480]   And then when you run through the model,
[00:13:40.480 --> 00:13:44.200]   so we create the model from the same checkpoint.
[00:13:44.200 --> 00:13:46.520]   And what's funny is you'll always get this kind of message
[00:13:46.520 --> 00:13:50.920]   about the weights of the model are essentially
[00:13:50.920 --> 00:13:56.000]   need to be fine-tuned at least the classification.
[00:13:56.000 --> 00:13:57.080]   And remember I was talking about
[00:13:57.080 --> 00:13:59.520]   how we take the core pre-trained model,
[00:13:59.520 --> 00:14:02.680]   we put a new head on it for our task, right?
[00:14:02.680 --> 00:14:05.680]   So for classification, so those parts aren't trained.
[00:14:05.680 --> 00:14:07.200]   That's all this message is saying.
[00:14:07.200 --> 00:14:08.680]   If you scroll over, you'll see,
[00:14:08.680 --> 00:14:11.920]   it's all referring to the classification parts
[00:14:11.920 --> 00:14:14.040]   of this neural network.
[00:14:14.040 --> 00:14:16.800]   And so don't be nervous when you see this message come up.
[00:14:16.800 --> 00:14:19.280]   And it's amazing 'cause like if you go to the forums,
[00:14:19.280 --> 00:14:20.240]   people are like, what is that?
[00:14:20.240 --> 00:14:21.080]   What's happening?
[00:14:21.080 --> 00:14:21.920]   What did I do wrong?
[00:14:21.920 --> 00:14:22.760]   What's going on?
[00:14:22.760 --> 00:14:24.640]   It's just the informational message to say,
[00:14:24.640 --> 00:14:27.880]   you need to train that final part of your neural network.
[00:14:27.880 --> 00:14:32.240]   And if we pass the inputs that we just created,
[00:14:32.240 --> 00:14:35.920]   as I mentioned, we can look at the last hidden state
[00:14:35.920 --> 00:14:37.520]   for just the auto model.
[00:14:37.520 --> 00:14:42.320]   And you see, we have batch size by sequence size
[00:14:42.320 --> 00:14:44.800]   by hidden state.
[00:14:44.800 --> 00:14:47.240]   And again, one of the nice things
[00:14:47.240 --> 00:14:51.000]   about the Transformers library,
[00:14:51.000 --> 00:14:54.840]   like I love the inputs and outputs are actually objects.
[00:14:54.840 --> 00:14:56.920]   So you can actually hit dot and actually see like
[00:14:56.920 --> 00:15:00.480]   what other information they're sending back.
[00:15:00.480 --> 00:15:01.800]   And when you create these models,
[00:15:01.800 --> 00:15:05.040]   you can actually request it to send other information,
[00:15:05.040 --> 00:15:07.120]   which is kind of cool.
[00:15:07.120 --> 00:15:10.000]   And that's model specific and something you would learn
[00:15:10.000 --> 00:15:11.320]   from the documentation.
[00:15:12.240 --> 00:15:15.280]   So now if we go use that checkpoint
[00:15:15.280 --> 00:15:18.880]   and let's actually create a auto model
[00:15:18.880 --> 00:15:20.400]   for sequence classification.
[00:15:20.400 --> 00:15:26.320]   And I wanted to show you that you can also just print model
[00:15:26.320 --> 00:15:29.560]   and you can see what the whole,
[00:15:29.560 --> 00:15:33.080]   like the whole architecture looks like.
[00:15:33.080 --> 00:15:35.160]   And so this is really cool when you're trying to figure out
[00:15:35.160 --> 00:15:40.000]   like, okay, how did it change for sequence classification?
[00:15:40.000 --> 00:15:41.840]   You could just print the model
[00:15:41.840 --> 00:15:46.360]   and then go down to the very bottom.
[00:15:46.360 --> 00:15:49.400]   And you can see that it has this pre-classification layer,
[00:15:49.400 --> 00:15:53.400]   classifier, and it also includes some dropout.
[00:15:53.400 --> 00:15:56.560]   But this right here is basically
[00:15:56.560 --> 00:15:59.240]   the custom head right here that was added on.
[00:15:59.240 --> 00:16:00.720]   And remember, we saw that warning that said
[00:16:00.720 --> 00:16:04.040]   these are randomly initialized, so they need to be trained.
[00:16:04.040 --> 00:16:07.120]   So if you're curious to like how Transformers work
[00:16:07.120 --> 00:16:10.480]   for a specific task, create that model.
[00:16:10.480 --> 00:16:13.240]   So last week, I think someone asked about Bart
[00:16:13.240 --> 00:16:16.240]   and how it's used for sequence classification.
[00:16:16.240 --> 00:16:19.160]   Create the auto model for sequence classification
[00:16:19.160 --> 00:16:21.720]   using a Bart checkpoint, and then just print the model
[00:16:21.720 --> 00:16:23.880]   and see kind of how it's doing that.
[00:16:23.880 --> 00:16:30.760]   So again, once we run through the model,
[00:16:30.760 --> 00:16:36.440]   we're going to get an outputs object
[00:16:36.440 --> 00:16:38.520]   and we can look at the shape of the logits.
[00:16:38.520 --> 00:16:41.680]   So remember we had a batch size of two
[00:16:41.680 --> 00:16:43.880]   and we're outputting two labels, right?
[00:16:43.880 --> 00:16:45.720]   Positive or negative.
[00:16:45.720 --> 00:16:48.720]   And then we could also like look at the logits right there.
[00:16:48.720 --> 00:16:52.120]   And then as a final step, as I mentioned,
[00:16:52.120 --> 00:16:54.360]   in normal cases, like when you're using Fast.ai,
[00:16:54.360 --> 00:16:56.280]   this will already be done for you
[00:16:56.280 --> 00:16:58.600]   as part of calculating the loss.
[00:16:58.600 --> 00:17:01.520]   But since we're just looking at running
[00:17:01.520 --> 00:17:05.920]   through the raw model, we need to actually run the logits
[00:17:05.920 --> 00:17:09.320]   through a Softmax to get the probabilities
[00:17:09.320 --> 00:17:11.680]   or confidence that the model is assigning
[00:17:11.680 --> 00:17:13.440]   to the different labels.
[00:17:13.440 --> 00:17:16.160]   And then also what's kind of cool
[00:17:16.160 --> 00:17:18.120]   'cause we don't know what those labels are.
[00:17:18.120 --> 00:17:21.560]   If we actually do look at the configuration object
[00:17:21.560 --> 00:17:26.320]   and the ID to label, that tells us that zero is negative,
[00:17:26.320 --> 00:17:28.400]   one is positive.
[00:17:28.400 --> 00:17:33.800]   Any questions on that yet, Sonia?
[00:17:34.760 --> 00:17:36.160]   - This one question, how is the number
[00:17:36.160 --> 00:17:38.480]   of hidden states determined?
[00:17:38.480 --> 00:17:41.040]   - How are they determined?
[00:17:41.040 --> 00:17:41.960]   - Yes.
[00:17:41.960 --> 00:17:46.240]   - So they're determined by the particular architecture
[00:17:46.240 --> 00:17:51.240]   and they can vary.
[00:17:51.240 --> 00:17:55.840]   So like for BERT, there's a BERT-based architecture,
[00:17:55.840 --> 00:17:57.400]   which is 768.
[00:17:57.400 --> 00:17:59.960]   So that was the number they came up with.
[00:17:59.960 --> 00:18:04.840]   And then there is a BERT-large architecture
[00:18:04.840 --> 00:18:06.040]   and I don't know what it is.
[00:18:06.040 --> 00:18:09.600]   I wanna say it's like 1024, it's a little bit bigger.
[00:18:09.600 --> 00:18:13.240]   And so it's really just determined by the architecture
[00:18:13.240 --> 00:18:17.680]   and the particular checkpoint.
[00:18:17.680 --> 00:18:20.640]   - If I may add, usually companies like Google
[00:18:20.640 --> 00:18:23.360]   that do these researches have access
[00:18:23.360 --> 00:18:24.680]   to good amount of resources.
[00:18:24.680 --> 00:18:26.840]   So they also run these architectural searches
[00:18:26.840 --> 00:18:29.040]   where they just can go all out
[00:18:29.040 --> 00:18:30.800]   and compare different hidden states
[00:18:30.800 --> 00:18:32.040]   and see what works best.
[00:18:32.040 --> 00:18:34.640]   Usually those are sometimes mentioned in the paper.
[00:18:34.640 --> 00:18:38.080]   So that's also something that happens behind the scenes.
[00:18:38.080 --> 00:18:40.200]   - Yeah, I was gonna say the papers are a good source
[00:18:40.200 --> 00:18:42.800]   to go to 'cause they've, typically the folks
[00:18:42.800 --> 00:18:46.960]   that have introduced these architectures have also done
[00:18:46.960 --> 00:18:48.960]   a bunch of like ablation type studies.
[00:18:48.960 --> 00:18:51.160]   And so they've played around with different lengths
[00:18:51.160 --> 00:18:54.680]   and different number of layers,
[00:18:54.680 --> 00:18:56.240]   different lengths for everything.
[00:18:56.240 --> 00:18:59.520]   And so that's probably a really good recommendation
[00:18:59.520 --> 00:19:01.280]   is to look at the actual paper.
[00:19:01.280 --> 00:19:04.520]   - Awesome, the second question is,
[00:19:04.520 --> 00:19:07.240]   what is the difference between using auto model
[00:19:07.240 --> 00:19:09.440]   and the architecture specific classes?
[00:19:09.440 --> 00:19:13.240]   Example, BERT model, is there any upside
[00:19:13.240 --> 00:19:14.880]   to using just the latter?
[00:19:14.880 --> 00:19:17.000]   So just using BERT model, for example.
[00:19:17.000 --> 00:19:19.720]   - The only upside is that if you decide
[00:19:19.720 --> 00:19:21.240]   to use a different checkpoint,
[00:19:21.240 --> 00:19:23.520]   so it may be a different model
[00:19:23.520 --> 00:19:26.040]   or the same model architecture,
[00:19:26.040 --> 00:19:30.040]   but a different version is that you wouldn't have
[00:19:30.040 --> 00:19:35.040]   to recode everything to use a specific hugging face objects.
[00:19:35.040 --> 00:19:40.200]   So with the auto tokenizer, auto model, auto config,
[00:19:40.200 --> 00:19:42.880]   you could just pack that checkpoint and it will infer
[00:19:42.880 --> 00:19:45.720]   and give you the same thing as if you were using
[00:19:45.720 --> 00:19:49.280]   the architecture specific objects.
[00:19:49.280 --> 00:19:53.320]   - Another one, in case of auto model,
[00:19:53.320 --> 00:19:55.240]   is the head also pre-trained?
[00:19:56.240 --> 00:19:58.480]   - On the auto model, no.
[00:19:58.480 --> 00:20:01.760]   So with the auto model, it works the same way
[00:20:01.760 --> 00:20:03.440]   as if you're using the regular objects,
[00:20:03.440 --> 00:20:08.320]   is that that last, the head, whatever it is for your task,
[00:20:08.320 --> 00:20:10.840]   is gonna be randomly initialized.
[00:20:10.840 --> 00:20:15.840]   If you're using something like this one right here
[00:20:15.840 --> 00:20:21.720]   is fine tuned for this particular,
[00:20:22.880 --> 00:20:25.920]   so the SST2 dataset.
[00:20:25.920 --> 00:20:30.920]   And I think I'm not a hundred percent sure.
[00:20:30.920 --> 00:20:33.000]   It actually, this one right here,
[00:20:33.000 --> 00:20:35.040]   'cause it's fine tuned for this particular dataset,
[00:20:35.040 --> 00:20:39.040]   it actually may start with a fine tune classification head.
[00:20:39.040 --> 00:20:43.080]   If you were using something like distilbert based on case,
[00:20:43.080 --> 00:20:46.760]   just by this, it would be randomly initialized.
[00:20:46.760 --> 00:20:49.280]   This, I'm not actually a hundred percent sure.
[00:20:49.280 --> 00:20:50.760]   If anybody knows, stick it in the chat,
[00:20:50.760 --> 00:20:55.520]   this might actually give you a classification head
[00:20:55.520 --> 00:21:00.520]   that predicts two labels with learned weights, not a hundred percent.
[00:21:00.520 --> 00:21:06.360]   Anything else?
[00:21:06.360 --> 00:21:07.480]   - Nope, thanks.
[00:21:07.480 --> 00:21:08.880]   - Okay, cool.
[00:21:08.880 --> 00:21:15.160]   All right, so let's go ahead and look at just an example.
[00:21:15.160 --> 00:21:18.160]   So we'll look at the blur library
[00:21:18.160 --> 00:21:22.120]   and kind of see how it does those same steps.
[00:21:22.120 --> 00:21:24.680]   And again, next week we'll be looking at
[00:21:24.680 --> 00:21:28.840]   three different integration libraries.
[00:21:28.840 --> 00:21:30.000]   This is just one of them.
[00:21:30.000 --> 00:21:31.760]   This is the one that I created,
[00:21:31.760 --> 00:21:34.680]   but we'll kind of go through the same behind the scenes steps
[00:21:34.680 --> 00:21:38.160]   so you can see how it works.
[00:21:38.160 --> 00:21:41.160]   And essentially when I designed blur,
[00:21:41.160 --> 00:21:44.800]   I have a very fast AI first focus.
[00:21:44.800 --> 00:21:46.000]   So if you go through the course,
[00:21:46.000 --> 00:21:48.520]   you can learn how to build your data block API,
[00:21:48.520 --> 00:21:51.040]   build your data loaders and your learner,
[00:21:51.040 --> 00:21:54.920]   and you're comfortable with using methods like show batch
[00:21:54.920 --> 00:21:59.920]   and show results and how fast AI does
[00:21:59.920 --> 00:22:02.960]   the exporting of learners for inference.
[00:22:02.960 --> 00:22:06.240]   I really tried to make it so that this would be very familiar
[00:22:06.240 --> 00:22:10.800]   to folks coming from that vantage point.
[00:22:12.000 --> 00:22:15.880]   So I'm just gonna go ahead and do a quick install
[00:22:15.880 --> 00:22:20.880]   of fast AI and blur, do some imports of everything.
[00:22:20.880 --> 00:22:24.200]   And I'm gonna use the same checkpoint
[00:22:24.200 --> 00:22:26.000]   that we are using above.
[00:22:26.000 --> 00:22:31.200]   Also, if you look at the fast AI course, part of fast AI,
[00:22:31.200 --> 00:22:34.840]   they have this nifty untar data function
[00:22:34.840 --> 00:22:38.680]   that you actually pass it a URL to a dataset.
[00:22:38.680 --> 00:22:42.640]   It will download it, untar it, and then return the path
[00:22:42.640 --> 00:22:47.240]   so that you can, in a couple of lines of code,
[00:22:47.240 --> 00:22:49.080]   actually have, for example, here,
[00:22:49.080 --> 00:22:51.080]   a data frame with all your data.
[00:22:51.080 --> 00:22:56.320]   And so this particular dataset is pretty simple.
[00:22:56.320 --> 00:23:00.280]   We have a label, negative or positive.
[00:23:00.280 --> 00:23:04.320]   We have the sequence, the text it's associated with.
[00:23:04.320 --> 00:23:07.400]   And then we also have this column right here
[00:23:07.400 --> 00:23:09.320]   that we can use in our data blocks
[00:23:09.320 --> 00:23:12.440]   for building our separate training
[00:23:12.440 --> 00:23:15.160]   and validation data loaders.
[00:23:15.160 --> 00:23:19.000]   And then, so one thing, so we talked about above,
[00:23:19.000 --> 00:23:22.680]   like how do we get these hugging face objects and whatnot.
[00:23:22.680 --> 00:23:27.680]   So with blur, I have a utility object called blur,
[00:23:27.680 --> 00:23:31.320]   and you could call get HF objects.
[00:23:31.320 --> 00:23:34.720]   It accepts a bunch of different quarks for the config,
[00:23:34.720 --> 00:23:36.880]   the tokenizer, the model.
[00:23:36.880 --> 00:23:40.680]   But usually you could pass a checkpoint,
[00:23:40.680 --> 00:23:44.120]   the particular auto model class that you want,
[00:23:44.120 --> 00:23:48.440]   and it will get all the right objects for you.
[00:23:48.440 --> 00:23:50.320]   And so here you can see,
[00:23:50.320 --> 00:23:53.560]   since we're using the same checkpoint as above,
[00:23:53.560 --> 00:23:55.480]   the architecture is distilbert,
[00:23:55.480 --> 00:23:57.280]   and you can see it properly inferred
[00:23:57.280 --> 00:24:02.280]   the correct config object, tokenizer, and model to use.
[00:24:02.760 --> 00:24:05.760]   And then from here, we'll go through this,
[00:24:05.760 --> 00:24:07.880]   some of these other parameters in a moment.
[00:24:07.880 --> 00:24:10.240]   These are the defaults, but when we get to tokenization,
[00:24:10.240 --> 00:24:13.320]   we'll take a look at like how different values
[00:24:13.320 --> 00:24:18.320]   affect your examples and the actual inputs.
[00:24:18.320 --> 00:24:22.120]   But again, following kind of like how the fast way
[00:24:22.120 --> 00:24:25.200]   of building your data is,
[00:24:25.200 --> 00:24:28.840]   we have an object in blur called HF text block.
[00:24:28.840 --> 00:24:32.000]   That takes your hugging face objects,
[00:24:32.000 --> 00:24:33.200]   and you can pass,
[00:24:33.200 --> 00:24:35.720]   there's a whole bunch of other parameters you can pass
[00:24:35.720 --> 00:24:40.200]   for tokenization, configuration, and the model.
[00:24:40.200 --> 00:24:43.040]   And this will actually give you a data block
[00:24:43.040 --> 00:24:48.040]   that understands how to work with our raw data
[00:24:48.040 --> 00:24:50.080]   that we have in the data frame.
[00:24:50.080 --> 00:24:52.240]   And since we're doing a classification task,
[00:24:52.240 --> 00:24:54.000]   we use the category block.
[00:24:54.000 --> 00:24:55.400]   So that's what you see here.
[00:24:55.400 --> 00:24:58.120]   For a classification task, we use the category block.
[00:24:58.120 --> 00:25:00.640]   So that's what you use for multi-class classification
[00:25:00.640 --> 00:25:01.680]   in fast AI.
[00:25:01.680 --> 00:25:04.520]   And you can see with this, with the data frame,
[00:25:04.520 --> 00:25:06.440]   and just a couple line of code,
[00:25:06.440 --> 00:25:08.920]   we can actually build a data block.
[00:25:08.920 --> 00:25:13.120]   And we're using the call splitter
[00:25:13.120 --> 00:25:16.520]   to split our training and validation.
[00:25:16.520 --> 00:25:19.040]   And as I mentioned before, we have this column,
[00:25:19.040 --> 00:25:22.200]   which says whether this should be part of the training
[00:25:22.200 --> 00:25:25.240]   or validation set, and the call splitter by default
[00:25:25.240 --> 00:25:26.600]   uses that value.
[00:25:26.600 --> 00:25:28.840]   And so anything that has is valid is true,
[00:25:28.840 --> 00:25:31.720]   it's gonna be part of the validation data set.
[00:25:31.720 --> 00:25:33.240]   Anything where it's false is gonna be part
[00:25:33.240 --> 00:25:34.960]   of the training data set.
[00:25:34.960 --> 00:25:38.520]   And so with that, we have our data block.
[00:25:38.520 --> 00:25:42.040]   And again, if you're confused about anything
[00:25:42.040 --> 00:25:42.960]   with the data block,
[00:25:42.960 --> 00:25:46.200]   take a look at the Fastbook information.
[00:25:46.200 --> 00:25:47.920]   It's got a bunch of stuff in there.
[00:25:47.920 --> 00:25:51.440]   Zach's walk with fast AI has a lot of good information
[00:25:51.440 --> 00:25:54.560]   about how this works and how you can,
[00:25:54.560 --> 00:25:55.920]   and like a bunch of different options
[00:25:55.920 --> 00:25:57.520]   for building your data block.
[00:25:57.520 --> 00:26:00.440]   But at the end of the day, the data block is a blueprint
[00:26:00.440 --> 00:26:04.400]   for how to take your raw data and turn it into data loaders
[00:26:04.400 --> 00:26:06.240]   that you can then use in a model.
[00:26:06.240 --> 00:26:11.440]   So with this information, we can actually on the data block
[00:26:11.440 --> 00:26:15.720]   called data loaders, pass our data, which is a data frame.
[00:26:15.720 --> 00:26:17.520]   You can pass a bunch of other options,
[00:26:17.520 --> 00:26:20.000]   but I'm just gonna include a batch size of four.
[00:26:21.440 --> 00:26:26.120]   And that's going to actually create our data loaders.
[00:26:26.120 --> 00:26:28.000]   And then once you have your data loaders,
[00:26:28.000 --> 00:26:32.400]   they function very similar to how you use the data loaders
[00:26:32.400 --> 00:26:34.680]   in fast AI for any other task.
[00:26:34.680 --> 00:26:38.840]   So you have show batch and you have a bunch
[00:26:38.840 --> 00:26:41.520]   of different options you could pass in there for the,
[00:26:41.520 --> 00:26:43.200]   you can truncate the text,
[00:26:43.200 --> 00:26:46.280]   control how many examples you wanna see.
[00:26:46.280 --> 00:26:50.680]   And then we can also look under the covers and see,
[00:26:50.680 --> 00:26:52.480]   well, okay, this is nice.
[00:26:52.480 --> 00:26:53.600]   This is what it looks like,
[00:26:53.600 --> 00:26:56.280]   but what does it look like to the model?
[00:26:56.280 --> 00:27:00.720]   And so in fast AI, this data loaders objects
[00:27:00.720 --> 00:27:03.560]   has a nifty function called one batch.
[00:27:03.560 --> 00:27:06.520]   And what that's gonna do is just return a single batch
[00:27:06.520 --> 00:27:10.640]   of your training data.
[00:27:10.640 --> 00:27:15.640]   And so again, there are your inputs and your targets.
[00:27:15.640 --> 00:27:19.200]   And so we can go ahead and say, okay, we want the inputs,
[00:27:19.200 --> 00:27:23.080]   put in this XP variable, YB to reference our targets,
[00:27:23.080 --> 00:27:25.440]   and we can look at our XP.
[00:27:25.440 --> 00:27:28.080]   So here we're looking at our inputs
[00:27:28.080 --> 00:27:30.800]   and you can see we've specified a batch size of four
[00:27:30.800 --> 00:27:35.560]   and we see four attention mass entries
[00:27:35.560 --> 00:27:37.600]   in this attention mass tensor.
[00:27:37.600 --> 00:27:42.600]   And we see four items in this input IDs tensor.
[00:27:42.600 --> 00:27:46.800]   And that refers to whatever batch,
[00:27:46.800 --> 00:27:47.840]   I don't know if this is random
[00:27:47.840 --> 00:27:51.520]   or whatever, I think it's just a random batch at graph.
[00:27:51.520 --> 00:27:55.000]   So every time you run this, I think this is going to,
[00:27:55.000 --> 00:27:57.000]   well, it looks like it's giving the same results.
[00:27:57.000 --> 00:27:58.000]   So maybe I'm wrong.
[00:27:58.000 --> 00:28:02.400]   But anyway, so that's how you can actually look
[00:28:02.400 --> 00:28:04.720]   at your inputs.
[00:28:04.720 --> 00:28:06.400]   And you'll notice that one of the things
[00:28:06.400 --> 00:28:07.680]   that's different with blur
[00:28:07.680 --> 00:28:10.080]   is that I try to respect the hugging face,
[00:28:10.080 --> 00:28:12.320]   the idea that they have an inputs object
[00:28:12.320 --> 00:28:13.960]   and an outputs object.
[00:28:13.960 --> 00:28:17.160]   And so fast AI isn't designed natively
[00:28:17.160 --> 00:28:20.000]   to handle objects like this, right?
[00:28:20.000 --> 00:28:21.640]   That look like a dictionary.
[00:28:21.640 --> 00:28:24.960]   And so we can explore it in later sessions,
[00:28:24.960 --> 00:28:26.960]   but if you start looking at the documentation,
[00:28:26.960 --> 00:28:29.440]   you can see kind of how I compensate for that.
[00:28:29.440 --> 00:28:34.440]   Essentially, I use the hugging face input and outputs,
[00:28:34.440 --> 00:28:39.200]   but use callbacks in fast AI to translate them accordingly
[00:28:39.200 --> 00:28:41.000]   so that they work in fast AI,
[00:28:41.000 --> 00:28:44.080]   'cause out of the box, you can't send the dictionary
[00:28:44.080 --> 00:28:47.160]   like this through a model using fast AI.
[00:28:47.160 --> 00:28:48.120]   And then we'll look at other ways,
[00:28:48.120 --> 00:28:51.680]   'cause I think Fast Hugs does things
[00:28:51.680 --> 00:28:53.160]   maybe a little bit differently.
[00:28:53.160 --> 00:28:55.240]   I think Zach does something a little bit differently
[00:28:55.240 --> 00:28:56.760]   in Adapt NLP.
[00:28:56.760 --> 00:28:58.720]   So we'll explore those options
[00:28:58.720 --> 00:29:00.280]   and hear from the other authors
[00:29:00.280 --> 00:29:02.640]   as to what are the design guidelines
[00:29:02.640 --> 00:29:05.760]   that were driving those decisions.
[00:29:05.760 --> 00:29:08.080]   So once we have this,
[00:29:08.080 --> 00:29:13.080]   we can actually look at the batch information
[00:29:14.000 --> 00:29:16.240]   and this is always a good practice
[00:29:16.240 --> 00:29:20.200]   regardless of whether you're using hugging face
[00:29:20.200 --> 00:29:23.760]   or some other model is to really understand
[00:29:23.760 --> 00:29:25.640]   what your batches look like.
[00:29:25.640 --> 00:29:26.680]   This will save you a lot of time
[00:29:26.680 --> 00:29:28.160]   when you start troubleshooting
[00:29:28.160 --> 00:29:29.440]   and start getting information
[00:29:29.440 --> 00:29:32.960]   about shapes not being right or whatnot.
[00:29:32.960 --> 00:29:35.560]   And so we can see that here with blur,
[00:29:35.560 --> 00:29:37.480]   we have a batch that has,
[00:29:37.480 --> 00:29:42.480]   there's two things in the inputs.
[00:29:42.800 --> 00:29:45.520]   And one of those is the input IDs
[00:29:45.520 --> 00:29:48.680]   and that's got a shape of four by 512.
[00:29:48.680 --> 00:29:51.360]   And then we also have the attention mask,
[00:29:51.360 --> 00:29:53.360]   which is the same shape, right?
[00:29:53.360 --> 00:29:56.360]   And there's gonna be ones associated to numbers
[00:29:56.360 --> 00:29:59.880]   that the attention layer should pay attention to
[00:29:59.880 --> 00:30:02.800]   and zero for ones where it shouldn't.
[00:30:02.800 --> 00:30:07.000]   We can also see that we have four here,
[00:30:07.000 --> 00:30:10.400]   we have four items in the input IDs,
[00:30:10.400 --> 00:30:12.240]   which is what we expect.
[00:30:12.240 --> 00:30:14.200]   And then we can see for our targets,
[00:30:14.200 --> 00:30:16.480]   we also have four items.
[00:30:16.480 --> 00:30:19.560]   So this is just helpful for debugging
[00:30:19.560 --> 00:30:20.880]   and making sure things look right
[00:30:20.880 --> 00:30:22.720]   before you start training.
[00:30:22.720 --> 00:30:26.080]   So once we have that,
[00:30:26.080 --> 00:30:30.640]   before we look at like fine tuning next week,
[00:30:30.640 --> 00:30:34.840]   we can actually just like we did in the example above
[00:30:34.840 --> 00:30:38.200]   is we can actually run those inputs right here
[00:30:38.200 --> 00:30:41.880]   through our hugging face model.
[00:30:41.880 --> 00:30:44.240]   And just like we did above,
[00:30:44.240 --> 00:30:46.400]   we can look at the shape of the logist,
[00:30:46.400 --> 00:30:48.720]   we can look at the actual logist
[00:30:48.720 --> 00:30:53.720]   and we can go through the process of getting probabilities
[00:30:53.720 --> 00:30:56.880]   and then actually looking at the labels.
[00:30:56.880 --> 00:30:59.720]   So we can use a lot of the same code
[00:30:59.720 --> 00:31:02.200]   that we saw just from the raw hugging face library
[00:31:02.200 --> 00:31:05.840]   in conjunction with blur or any of the other libraries
[00:31:05.840 --> 00:31:07.960]   that we're gonna be looking at next week.
[00:31:09.520 --> 00:31:13.880]   Any questions from all that information?
[00:31:13.880 --> 00:31:18.560]   - I don't see any, but I'll just say like blur,
[00:31:18.560 --> 00:31:21.200]   I'm curious to check out adapt NLP as well,
[00:31:21.200 --> 00:31:23.480]   but it really feels like at least for blur,
[00:31:23.480 --> 00:31:25.240]   it's very fast AIE.
[00:31:25.240 --> 00:31:28.520]   So it doesn't feel it's a different library at all.
[00:31:28.520 --> 00:31:30.800]   - Cool, yeah.
[00:31:30.800 --> 00:31:33.120]   That's kind of like my goal is that
[00:31:33.120 --> 00:31:35.880]   if you've gone through the first course of fast AI,
[00:31:35.880 --> 00:31:37.240]   you should look at blur and be like,
[00:31:37.240 --> 00:31:39.520]   oh yeah, I understand how everything works.
[00:31:39.520 --> 00:31:41.000]   And then you could get into the docs
[00:31:41.000 --> 00:31:43.640]   and I try to make it as clear as possible.
[00:31:43.640 --> 00:31:46.800]   And if it's not, and there's documentation missing,
[00:31:46.800 --> 00:31:47.760]   I love PR.
[00:31:47.760 --> 00:31:51.480]   So if anybody wants to help on documentation
[00:31:51.480 --> 00:31:54.360]   or testing or improving it, submit them.
[00:31:54.360 --> 00:31:56.480]   But yeah, that's kind of my goals
[00:31:56.480 --> 00:31:58.120]   that if you've gone through the first course,
[00:31:58.120 --> 00:31:59.680]   even just the first few sessions,
[00:31:59.680 --> 00:32:01.360]   it should be pretty straightforward
[00:32:01.360 --> 00:32:03.480]   how to work with the library, I hope.
[00:32:05.960 --> 00:32:10.960]   So the next slide we'll look at is,
[00:32:10.960 --> 00:32:14.240]   so now that we kind of have
[00:32:14.240 --> 00:32:16.040]   what's going on in the pipeline, right?
[00:32:16.040 --> 00:32:18.280]   We know how to actually code that
[00:32:18.280 --> 00:32:20.200]   just using pure hugging face.
[00:32:20.200 --> 00:32:24.680]   We've seen an example of how to code that with blur is,
[00:32:24.680 --> 00:32:29.000]   we have the idea of models
[00:32:29.000 --> 00:32:31.800]   and just like someone asked previously
[00:32:31.800 --> 00:32:33.640]   is why do we use auto model
[00:32:33.640 --> 00:32:38.640]   versus just the specific model we wanna use?
[00:32:38.640 --> 00:32:40.960]   It's really just to make it simple
[00:32:40.960 --> 00:32:42.600]   if we wanna play with different models.
[00:32:42.600 --> 00:32:44.240]   So it just reduces your code,
[00:32:44.240 --> 00:32:46.000]   makes things simpler to understand,
[00:32:46.000 --> 00:32:50.640]   simpler to run tests and experiments with.
[00:32:50.640 --> 00:32:53.240]   And with the auto model,
[00:32:53.240 --> 00:32:57.480]   probably the most important method is from pre-trained.
[00:32:57.480 --> 00:33:01.520]   And again, this is where we pass our checkpoint
[00:33:01.520 --> 00:33:03.440]   or if we're using a pre-trained model
[00:33:03.440 --> 00:33:04.280]   that we've created,
[00:33:04.280 --> 00:33:07.680]   we would pass like the path to the folder
[00:33:07.680 --> 00:33:11.200]   our train artifacts are in.
[00:33:11.200 --> 00:33:13.200]   And we'll look at that in just a second.
[00:33:13.200 --> 00:33:17.720]   But basically is, so from pre-trained,
[00:33:17.720 --> 00:33:20.720]   the model is now initialized with weights,
[00:33:20.720 --> 00:33:22.600]   with all the weights of that checkpoint
[00:33:22.600 --> 00:33:24.240]   and it could be used directly for inference
[00:33:24.240 --> 00:33:26.280]   on the task it was trained on
[00:33:26.280 --> 00:33:29.600]   and it can also be fine tuned on a new task.
[00:33:30.600 --> 00:33:34.240]   So remember, like we looked at that SST2 model
[00:33:34.240 --> 00:33:37.040]   and based on this,
[00:33:37.040 --> 00:33:39.160]   actually I think that the classification head
[00:33:39.160 --> 00:33:43.040]   does have learned weights, not a hundred percent sure,
[00:33:43.040 --> 00:33:45.640]   but you could actually take that model and say,
[00:33:45.640 --> 00:33:48.640]   hey, I wanna use that same checkpoint,
[00:33:48.640 --> 00:33:52.920]   but I have 30 classes and you could pass in,
[00:33:52.920 --> 00:33:55.040]   you know, a number of labels equals 30
[00:33:55.040 --> 00:33:57.320]   and it would alter that classification head.
[00:33:57.320 --> 00:33:59.440]   And then in that case,
[00:33:59.440 --> 00:34:01.680]   absolutely I know that those weights are gonna be random
[00:34:01.680 --> 00:34:03.680]   because it wasn't trained on that task.
[00:34:03.680 --> 00:34:09.240]   So from pre-trained is probably the method
[00:34:09.240 --> 00:34:14.000]   you're going to be using most
[00:34:14.000 --> 00:34:17.080]   and learn most about as you start using transformers.
[00:34:17.080 --> 00:34:19.880]   We also have a safe pre-trained.
[00:34:19.880 --> 00:34:23.400]   So if you want to save your pre-trained model
[00:34:23.400 --> 00:34:26.720]   and or not your pre-trained, but your train model,
[00:34:26.720 --> 00:34:30.760]   and that you've initialized from a pre-trained checkpoint
[00:34:30.760 --> 00:34:33.920]   and tokenizer, you're going to use this safe
[00:34:33.920 --> 00:34:35.440]   pre-trained method.
[00:34:35.440 --> 00:34:36.800]   And essentially what this is gonna do
[00:34:36.800 --> 00:34:38.920]   is create a couple of files.
[00:34:38.920 --> 00:34:41.160]   One is a config.json.
[00:34:41.160 --> 00:34:44.160]   And this is essentially all the attributes
[00:34:44.160 --> 00:34:46.920]   that are necessary to reconstruct the architecture
[00:34:46.920 --> 00:34:48.400]   and what you trained it for.
[00:34:48.400 --> 00:34:51.120]   So in this case, if I actually did take
[00:34:51.120 --> 00:34:52.920]   that distilled BERT model
[00:34:52.920 --> 00:34:57.280]   and I trained it on a task with 30 labels,
[00:34:57.280 --> 00:34:58.800]   you would see like the number of labels
[00:34:58.800 --> 00:35:00.320]   would be set to 30 in here.
[00:35:00.320 --> 00:35:03.240]   So folks would know whether they could just use it as is
[00:35:03.240 --> 00:35:05.840]   or we'd have to change that up.
[00:35:05.840 --> 00:35:08.000]   And then we also have the .bin file.
[00:35:08.000 --> 00:35:10.520]   And this is simply a state dictionary.
[00:35:10.520 --> 00:35:13.640]   It contains all your models and weights.
[00:35:13.640 --> 00:35:19.480]   And so I'm gonna go back to the CoLab here
[00:35:21.600 --> 00:35:23.400]   and let's see.
[00:35:23.400 --> 00:35:35.400]   So before we get to the save and pre-trained,
[00:35:35.400 --> 00:35:38.920]   just as a little preview of what you'll see next week,
[00:35:38.920 --> 00:35:41.760]   I'm not gonna do fine tuning right now,
[00:35:41.760 --> 00:35:46.760]   but I'm actually going to create a learner
[00:35:46.760 --> 00:35:51.400]   using the Blur library for the checkpoint
[00:35:51.400 --> 00:35:53.440]   that we have above at the data loaders.
[00:35:53.440 --> 00:35:56.720]   And you can see like once I have that learner,
[00:35:56.720 --> 00:35:58.000]   that is like freeze, unfreeze.
[00:35:58.000 --> 00:35:59.720]   So things that you are already familiar with
[00:35:59.720 --> 00:36:03.200]   in Fast.ai work, we can actually look at the results.
[00:36:03.200 --> 00:36:04.360]   So I'm not training it.
[00:36:04.360 --> 00:36:06.640]   We're just looking at the results right now.
[00:36:06.640 --> 00:36:09.520]   And you'll see that the results are already
[00:36:09.520 --> 00:36:11.880]   probably gonna be really good.
[00:36:11.880 --> 00:36:13.840]   Most of the models that are in Hugging Face
[00:36:13.840 --> 00:36:14.680]   have been trained.
[00:36:14.680 --> 00:36:16.320]   And if you're looking at models that are trained
[00:36:16.320 --> 00:36:18.880]   on the same task that you're looking at,
[00:36:18.880 --> 00:36:21.480]   they're already gonna probably be pretty good.
[00:36:21.480 --> 00:36:23.280]   So we can look at the results.
[00:36:23.280 --> 00:36:26.160]   And as I mentioned, I have a bunch of different
[00:36:26.160 --> 00:36:29.280]   kind of helper functions like Blur predict
[00:36:29.280 --> 00:36:33.360]   that can take one or more pieces of text.
[00:36:33.360 --> 00:36:37.000]   And we'll tell you the label, the label index,
[00:36:37.000 --> 00:36:39.480]   as well as give you those probabilities right there,
[00:36:39.480 --> 00:36:41.200]   just with one line of code.
[00:36:41.200 --> 00:36:44.880]   But what I wanted to show you is with models
[00:36:44.880 --> 00:36:47.880]   is we talked about that save pre-trained.
[00:36:47.880 --> 00:36:52.880]   And so I can also go here and I'm just gonna create
[00:36:52.880 --> 00:36:55.440]   my model directory.
[00:36:55.440 --> 00:36:59.560]   And then I am going to save the...
[00:36:59.560 --> 00:37:02.560]   So after actually learning and training,
[00:37:02.560 --> 00:37:06.200]   this is going to have some modified weights to it, right?
[00:37:06.200 --> 00:37:10.680]   And I'm going to save the model and the tokenizer
[00:37:10.680 --> 00:37:15.680]   that I'm using into that directory there, right?
[00:37:15.680 --> 00:37:16.920]   And this is everything.
[00:37:16.920 --> 00:37:19.800]   So when we look later on actually pushing our models
[00:37:19.800 --> 00:37:21.760]   to the hub, so we have that hub, right?
[00:37:21.760 --> 00:37:23.560]   The Hugging Face model hub, which has just,
[00:37:23.560 --> 00:37:24.880]   you know, all kinds of models.
[00:37:24.880 --> 00:37:28.040]   You can actually use a library like Blur
[00:37:28.040 --> 00:37:33.040]   and probably adapt or Fast Tugs to train a model,
[00:37:33.040 --> 00:37:36.320]   save those artifacts and push them to the hub
[00:37:36.320 --> 00:37:38.480]   and make that available to others.
[00:37:38.480 --> 00:37:42.680]   And you can see like I'm actually with Blur,
[00:37:42.680 --> 00:37:46.400]   I have the Hugging Face model as part of the model
[00:37:46.400 --> 00:37:48.680]   that's passed in or associated to the learner.
[00:37:48.680 --> 00:37:53.960]   But we can see if I look at the model versus this one,
[00:37:53.960 --> 00:37:56.160]   it's the same object.
[00:37:56.160 --> 00:37:58.080]   So this is the object that's getting modified,
[00:37:58.080 --> 00:38:00.040]   the weights are getting changed.
[00:38:00.040 --> 00:38:02.960]   And I'm gonna look at that directory.
[00:38:02.960 --> 00:38:06.640]   So you can see that some of the artifacts we talked about
[00:38:06.640 --> 00:38:11.640]   with the model save pre-trainer in there,
[00:38:11.640 --> 00:38:14.920]   as well as some of the tokenizer, like the vocab
[00:38:14.920 --> 00:38:18.000]   and its configuration as well.
[00:38:18.000 --> 00:38:20.120]   And just to prove that we can now use this,
[00:38:20.120 --> 00:38:22.960]   so imagine this is trained and we wanna use it.
[00:38:22.960 --> 00:38:27.680]   Notice that instead of passing a name from the hub,
[00:38:27.680 --> 00:38:32.320]   I'm just gonna actually pass this path
[00:38:32.320 --> 00:38:34.760]   to the my model directory.
[00:38:34.760 --> 00:38:37.440]   And when we look at it, we can see it's Distilbert,
[00:38:37.440 --> 00:38:40.720]   it's using the Distilbert config, the right tokenizer,
[00:38:40.720 --> 00:38:43.600]   and it's using the correct model architecture
[00:38:43.600 --> 00:38:46.040]   for sequence classification.
[00:38:46.040 --> 00:38:47.720]   So that's just a little like to show you
[00:38:47.720 --> 00:38:52.440]   that you can use blur or you can just,
[00:38:52.440 --> 00:38:54.200]   after it's done its training
[00:38:54.200 --> 00:38:58.440]   and actually just access those objects you created up here,
[00:38:58.440 --> 00:39:05.280]   oops, right here, you can actually use those objects
[00:39:05.280 --> 00:39:10.280]   then to save all the necessary things
[00:39:10.280 --> 00:39:13.320]   to be able to use it in the future or to push it to the hub.
[00:39:14.320 --> 00:39:19.320]   And I think that is it from this part here.
[00:39:19.320 --> 00:39:25.760]   Any questions come up, Sanyam?
[00:39:25.760 --> 00:39:26.960]   - Nope, nothing so far.
[00:39:26.960 --> 00:39:29.400]   I just wanted to point out that when we were starting this,
[00:39:29.400 --> 00:39:32.320]   everyone was like assuming they might need resources
[00:39:32.320 --> 00:39:34.080]   to access to resources,
[00:39:34.080 --> 00:39:38.040]   but like things work really well inside of Colab as well.
[00:39:38.040 --> 00:39:38.920]   - Oh yeah, yeah.
[00:39:38.920 --> 00:39:41.480]   Sometimes you gotta like tweak things a little bit,
[00:39:42.600 --> 00:39:46.280]   but hopefully at least I think,
[00:39:46.280 --> 00:39:48.520]   I noticed that the Colab from the course
[00:39:48.520 --> 00:39:51.640]   doesn't always work and you have to change things,
[00:39:51.640 --> 00:39:53.840]   but you should be able to, you know,
[00:39:53.840 --> 00:39:56.720]   muck with it enough to be able to get it to work.
[00:39:56.720 --> 00:40:00.360]   So anyway, so with that,
[00:40:00.360 --> 00:40:04.440]   the next part of the course goes through tokenizers.
[00:40:04.440 --> 00:40:05.920]   And we've already talked about that,
[00:40:05.920 --> 00:40:08.240]   the job of the tokenizer is to take raw text
[00:40:08.240 --> 00:40:10.480]   and convert it to numbers.
[00:40:10.480 --> 00:40:14.120]   And there's a variety of algorithms.
[00:40:14.120 --> 00:40:17.400]   And if you go through the course,
[00:40:17.400 --> 00:40:20.240]   they kind of have like a little video segment
[00:40:20.240 --> 00:40:22.800]   on each of them and pros and cons.
[00:40:22.800 --> 00:40:25.680]   So like probably the most basic would be like
[00:40:25.680 --> 00:40:28.800]   a word-based tokenizer that splits on spaces,
[00:40:28.800 --> 00:40:31.560]   but this is typically not desirable
[00:40:31.560 --> 00:40:34.080]   because it creates large vocabularies,
[00:40:34.080 --> 00:40:38.680]   which makes it harder to train your models.
[00:40:38.680 --> 00:40:41.160]   You lose meaning across related words.
[00:40:41.160 --> 00:40:43.720]   So like if you had the word dog and dogs,
[00:40:43.720 --> 00:40:46.080]   those would have two separate,
[00:40:46.080 --> 00:40:47.320]   those would be two separate tokens.
[00:40:47.320 --> 00:40:49.840]   So they would have two separate IDs,
[00:40:49.840 --> 00:40:51.360]   which means that the embedding learn
[00:40:51.360 --> 00:40:52.840]   for each one would be different,
[00:40:52.840 --> 00:40:56.280]   even though they are very semantically similar.
[00:40:56.280 --> 00:41:00.760]   And these also produce a lot of unk or unknown tokens
[00:41:00.760 --> 00:41:03.400]   because they're not in a vocabulary.
[00:41:03.400 --> 00:41:07.160]   And the problem with that is that if a token is unknown,
[00:41:07.160 --> 00:41:08.880]   you're really not gonna learn anything about it.
[00:41:08.880 --> 00:41:13.040]   So it's not going to help you in whatever your task is.
[00:41:13.040 --> 00:41:17.160]   The other one are character-based tokens.
[00:41:17.160 --> 00:41:21.960]   And these are not desirable typically
[00:41:21.960 --> 00:41:24.920]   because you don't really get meaningful embeddings.
[00:41:24.920 --> 00:41:26.640]   The vocab is smaller, right?
[00:41:26.640 --> 00:41:27.720]   So, you know what I mean?
[00:41:27.720 --> 00:41:31.760]   It might, instead of being 50,000 or 60,000 tokens,
[00:41:31.760 --> 00:41:33.880]   it might be 30, 40, or 50,
[00:41:34.960 --> 00:41:38.160]   but you really lose a lot of the semantic meaning of words
[00:41:38.160 --> 00:41:42.560]   because you're breaking them down into individual characters.
[00:41:42.560 --> 00:41:45.440]   And also when you actually pass these through your model,
[00:41:45.440 --> 00:41:49.000]   it's gonna take more tokens to represent the sequence.
[00:41:49.000 --> 00:41:53.440]   So that could be taxing on your compute resources.
[00:41:53.440 --> 00:41:56.640]   And it may mean that you have to add more truncation
[00:41:56.640 --> 00:41:58.520]   so you lose more information
[00:41:58.520 --> 00:42:00.640]   just to get your models to work.
[00:42:00.640 --> 00:42:04.520]   So what's the solution?
[00:42:04.520 --> 00:42:06.720]   It's subword tokenizers.
[00:42:06.720 --> 00:42:08.560]   And with subword tokenizers,
[00:42:08.560 --> 00:42:13.440]   the fundamental goal is common words should not be split up
[00:42:13.440 --> 00:42:16.920]   and rare words should be split up as needed.
[00:42:16.920 --> 00:42:21.320]   And the benefits is that you get semantic meaning.
[00:42:21.320 --> 00:42:23.960]   So like here, you'll see like let's,
[00:42:23.960 --> 00:42:25.480]   even though this might get split up
[00:42:25.480 --> 00:42:30.280]   by a word-based tokenizer 'cause of the punctuation,
[00:42:30.280 --> 00:42:32.680]   it actually keeps this word together
[00:42:32.680 --> 00:42:36.280]   because let apostrophe s is common.
[00:42:36.280 --> 00:42:39.920]   Same with do, but you see with tokenization,
[00:42:39.920 --> 00:42:44.760]   it actually pulls out token from -ization.
[00:42:44.760 --> 00:42:47.040]   And so anything that has token in it,
[00:42:47.040 --> 00:42:50.560]   you're going, regardless of whether or not
[00:42:50.560 --> 00:42:52.720]   there is something at the end,
[00:42:52.720 --> 00:42:56.920]   it's going to be able to use that meaning in your sequences.
[00:42:56.920 --> 00:42:59.800]   And then you can see we also have the punctuation
[00:42:59.800 --> 00:43:02.560]   that was pulled out into its own token.
[00:43:02.560 --> 00:43:07.360]   So the core benefits is that you get,
[00:43:07.360 --> 00:43:10.640]   you maintain a lot of semantic meaning between your tokens.
[00:43:10.640 --> 00:43:12.760]   You have a small vocab
[00:43:12.760 --> 00:43:16.480]   and it works well with many different languages.
[00:43:16.480 --> 00:43:21.320]   And very rarely, if ever, will you see an unknown token.
[00:43:21.320 --> 00:43:25.200]   So as an example, like my name is a weird spelling of Wade.
[00:43:25.200 --> 00:43:27.280]   It's W-A-Y-D-E.
[00:43:27.280 --> 00:43:29.440]   And I can guarantee if you were looking
[00:43:29.440 --> 00:43:31.720]   at a word-based tokenizer, that'd be a punk.
[00:43:31.720 --> 00:43:33.880]   There'd be nothing to learn from that.
[00:43:33.880 --> 00:43:36.680]   But using Subword tokenizers,
[00:43:36.680 --> 00:43:39.760]   it would be parsed out to Wade, W-A-Y,
[00:43:39.760 --> 00:43:43.120]   and then D-E with something like this,
[00:43:43.120 --> 00:43:47.760]   or D-E with this particular notation,
[00:43:47.760 --> 00:43:48.600]   or if you're using BERT,
[00:43:48.600 --> 00:43:52.400]   it would be like two hashtags and D-E.
[00:43:52.400 --> 00:43:56.200]   So it would be able to learn something about my name,
[00:43:56.200 --> 00:43:57.880]   which is what we want.
[00:43:57.880 --> 00:44:02.360]   And there's a whole bunch of Subword tokenization strategies
[00:44:02.360 --> 00:44:04.600]   and instead of trying to go through all of them,
[00:44:04.600 --> 00:44:08.960]   I've listed this link right here in the Transformers docs
[00:44:08.960 --> 00:44:12.480]   and different architectures use different strategies.
[00:44:12.480 --> 00:44:15.840]   And you can learn a little bit about those by going there.
[00:44:15.840 --> 00:44:18.360]   I don't know what the negatives are.
[00:44:18.360 --> 00:44:20.560]   I'm not an expert on these different strategies
[00:44:20.560 --> 00:44:23.760]   or what the negatives to Subword tokenizers are.
[00:44:24.600 --> 00:44:28.000]   So if folks wanna chime in in the chat or Discord,
[00:44:28.000 --> 00:44:29.080]   that'd be great.
[00:44:29.080 --> 00:44:30.880]   But this is a spot to learn more
[00:44:30.880 --> 00:44:32.720]   about those tokenization strategies.
[00:44:32.720 --> 00:44:39.800]   And then again, we've already kind of gone through this,
[00:44:39.800 --> 00:44:42.520]   is that the way the tokenizer works,
[00:44:42.520 --> 00:44:45.720]   it takes your raw text, converts it to tokens,
[00:44:45.720 --> 00:44:47.480]   adds special tokens, right?
[00:44:47.480 --> 00:44:50.160]   And you can see that it's got some of the,
[00:44:50.160 --> 00:44:51.760]   these are, you know, of course,
[00:44:53.000 --> 00:44:56.200]   these are tokens, even though they're Subwords,
[00:44:56.200 --> 00:45:00.000]   and then those are converted to our indexes
[00:45:00.000 --> 00:45:02.240]   and our vocabulary.
[00:45:02.240 --> 00:45:06.440]   And just as a reminder that when you're using a checkpoint,
[00:45:06.440 --> 00:45:09.000]   you wanna use the same checkpoint for the tokenizer
[00:45:09.000 --> 00:45:12.760]   as you do for the model to make sure everything lines up.
[00:45:12.760 --> 00:45:15.520]   If you don't, you're essentially training from scratch.
[00:45:15.520 --> 00:45:17.600]   And so make sure you have a lot of compute,
[00:45:17.600 --> 00:45:20.440]   a lot of coffee and a lot of free time
[00:45:20.440 --> 00:45:22.000]   'cause it's gonna take a while.
[00:45:22.320 --> 00:45:23.320]   - Thank you.
[00:45:23.320 --> 00:45:26.080]   Many questions come up?
[00:45:26.080 --> 00:45:27.840]   Yes, Anyaman, anything?
[00:45:27.840 --> 00:45:29.920]   - Yes, let me scroll up.
[00:45:29.920 --> 00:45:33.520]   So is the main focus of Blur on sequence classification
[00:45:33.520 --> 00:45:35.960]   or do you support other tasks as well?
[00:45:35.960 --> 00:45:40.240]   - So Blur supports sequence classification,
[00:45:40.240 --> 00:45:43.160]   token classification, like name entity recognition,
[00:45:43.160 --> 00:45:49.160]   extractive Q&A, summarization and translation
[00:45:49.160 --> 00:45:50.000]   out of the box.
[00:45:50.000 --> 00:45:54.200]   And I have a low level API and also a high level API
[00:45:54.200 --> 00:45:56.480]   for all those tasks and also language modeling.
[00:45:56.480 --> 00:46:01.480]   So causal language modeling is implemented
[00:46:01.480 --> 00:46:05.600]   and then mass language model has an initial implementation,
[00:46:05.600 --> 00:46:08.080]   but the only strategy that I'm using
[00:46:08.080 --> 00:46:10.960]   for mass language modeling is the BERT strategy
[00:46:10.960 --> 00:46:12.280]   from their paper.
[00:46:12.280 --> 00:46:15.080]   And so I'm hoping people can see how that works
[00:46:15.080 --> 00:46:16.760]   and add other strategies in there
[00:46:16.760 --> 00:46:18.520]   'cause there's a variety of different ways
[00:46:18.520 --> 00:46:19.560]   to do the masking.
[00:46:20.560 --> 00:46:23.240]   So yeah, language modeling is in there too.
[00:46:23.240 --> 00:46:26.160]   - And the best resource would be just the docs, right?
[00:46:26.160 --> 00:46:27.000]   To check these out?
[00:46:27.000 --> 00:46:28.880]   - Absolutely, yeah.
[00:46:28.880 --> 00:46:31.480]   - And just, this is me thinking out loud.
[00:46:31.480 --> 00:46:33.080]   So for the sub-word tokenization,
[00:46:33.080 --> 00:46:35.160]   I'm assuming it might take longer
[00:46:35.160 --> 00:46:36.320]   for the model to converge
[00:46:36.320 --> 00:46:39.040]   because we're like splitting up the meanings
[00:46:39.040 --> 00:46:40.840]   or we might need more memory
[00:46:40.840 --> 00:46:43.040]   because like we might have more tokens now.
[00:46:43.040 --> 00:46:45.440]   I know I need to experiment this, but I'm just,
[00:46:45.440 --> 00:46:47.320]   I think thinking-
[00:46:47.320 --> 00:46:48.400]   - Go ahead.
[00:46:48.400 --> 00:46:53.160]   - Yeah, so I mean, I think going back to this link
[00:46:53.160 --> 00:46:56.000]   right here, there's like a lot of good resources
[00:46:56.000 --> 00:46:58.000]   plus there's links to the different papers
[00:46:58.000 --> 00:47:01.440]   that discuss the different tokenization strategies.
[00:47:01.440 --> 00:47:04.960]   So, folks, we've heard about like sentence piece,
[00:47:04.960 --> 00:47:07.160]   word piece, byte pair coding.
[00:47:07.160 --> 00:47:10.880]   Those are all sub-word tokenization strategies
[00:47:10.880 --> 00:47:14.120]   and people a lot smarter than me, obviously,
[00:47:14.120 --> 00:47:16.240]   have put papers and done a bunch of studies
[00:47:16.240 --> 00:47:19.800]   as to why they've introduced
[00:47:19.800 --> 00:47:23.160]   and why theirs may be better than other ones
[00:47:23.160 --> 00:47:24.280]   for certain cases.
[00:47:24.280 --> 00:47:26.720]   But definitely, like you're saying,
[00:47:26.720 --> 00:47:31.560]   things like the length of vocab matters,
[00:47:31.560 --> 00:47:34.520]   how, like what they consider rare words
[00:47:34.520 --> 00:47:39.440]   is going to have an impact on the embeddings that you get.
[00:47:39.440 --> 00:47:42.800]   So, yeah, so there's a lot of information
[00:47:42.800 --> 00:47:45.280]   you can get there, or you can be lazy like me
[00:47:45.280 --> 00:47:48.520]   and go, "Hey, Roberta works good, I'm using Roberta."
[00:47:48.520 --> 00:47:50.840]   - Awesome, thanks.
[00:47:50.840 --> 00:47:56.400]   - So, the last part of this course talks
[00:47:56.400 --> 00:47:59.320]   about handling multiple sequences.
[00:47:59.320 --> 00:48:01.440]   And so the big things here is to remember
[00:48:01.440 --> 00:48:05.960]   that when you're sending inputs through a model,
[00:48:05.960 --> 00:48:07.960]   we're sending them in mini-batches.
[00:48:07.960 --> 00:48:10.720]   And so a mini-batch needs to be a square matrix,
[00:48:10.720 --> 00:48:13.440]   even if the number of tokens varies
[00:48:13.440 --> 00:48:15.360]   in each of your examples.
[00:48:15.360 --> 00:48:17.800]   And then we also need to be able to use,
[00:48:17.800 --> 00:48:21.880]   to tell the models where to apply attention.
[00:48:21.880 --> 00:48:23.960]   So hopefully you all read the attention
[00:48:23.960 --> 00:48:27.320]   is all you need paper last week,
[00:48:27.320 --> 00:48:29.160]   if not, read it this week.
[00:48:29.160 --> 00:48:32.760]   And you can see that we really want these attention layers,
[00:48:32.760 --> 00:48:34.320]   which play a significant role
[00:48:34.320 --> 00:48:36.320]   in the Transformers architecture,
[00:48:36.320 --> 00:48:39.240]   to just pay attention to things that matter,
[00:48:39.240 --> 00:48:42.680]   which means essentially ignore padding tokens.
[00:48:43.680 --> 00:48:48.040]   And so in Transformers, again,
[00:48:48.040 --> 00:48:50.800]   so there's three things that we really have to,
[00:48:50.800 --> 00:48:55.560]   or two things that we have to specify ahead of time,
[00:48:55.560 --> 00:48:57.720]   in addition to understanding attention.
[00:48:57.720 --> 00:48:59.680]   And the first is padding.
[00:48:59.680 --> 00:49:02.360]   So we need to make sure that for every batch,
[00:49:02.360 --> 00:49:04.480]   the sequence size is the same.
[00:49:04.480 --> 00:49:07.840]   And there's a bunch of different options.
[00:49:07.840 --> 00:49:11.320]   So the first one we have here is padding equals true
[00:49:11.320 --> 00:49:12.600]   or longest.
[00:49:12.600 --> 00:49:15.560]   And that means it's going to pad the sequences
[00:49:15.560 --> 00:49:19.280]   up to the maximum sentence length in the batch.
[00:49:19.280 --> 00:49:22.000]   And this is typically what you want,
[00:49:22.000 --> 00:49:26.080]   'cause it allows for more efficient models
[00:49:26.080 --> 00:49:27.000]   and the performance,
[00:49:27.000 --> 00:49:28.160]   your models are gonna be faster.
[00:49:28.160 --> 00:49:32.240]   So if the sequence size allows for 512, right?
[00:49:32.240 --> 00:49:34.320]   Like for BERT or distilled BERT,
[00:49:34.320 --> 00:49:36.000]   and we say max length,
[00:49:36.000 --> 00:49:37.160]   then there's gonna be padding
[00:49:37.160 --> 00:49:42.080]   so that every single sequence is 512 characters, right?
[00:49:42.080 --> 00:49:44.920]   So you're gonna have like a four by 512 matrix.
[00:49:44.920 --> 00:49:46.600]   But imagine that in your batch,
[00:49:46.600 --> 00:49:50.600]   the longest sequence is a hundred characters.
[00:49:50.600 --> 00:49:53.080]   Well, by using padding equals true or longest,
[00:49:53.080 --> 00:49:54.920]   you would have four by 100.
[00:49:54.920 --> 00:49:57.240]   So again, it's gonna make your models faster.
[00:49:57.240 --> 00:49:59.440]   Whoops.
[00:49:59.440 --> 00:50:00.800]   And then lastly,
[00:50:00.800 --> 00:50:05.800]   you can actually specify max length and a actual max length.
[00:50:05.840 --> 00:50:09.720]   And this is actually typically...
[00:50:09.720 --> 00:50:14.640]   So I usually use the padding strategy,
[00:50:14.640 --> 00:50:17.640]   but if you find out like you're going through your texts
[00:50:17.640 --> 00:50:22.720]   and you want to do like experimentation
[00:50:22.720 --> 00:50:24.120]   and you want it to be fast,
[00:50:24.120 --> 00:50:26.680]   you can actually use this strategy right here
[00:50:26.680 --> 00:50:28.640]   and specify a very small max length.
[00:50:28.640 --> 00:50:29.920]   So you can quickly iterate
[00:50:29.920 --> 00:50:32.240]   through your experiments initially.
[00:50:34.280 --> 00:50:35.960]   And then we have truncation.
[00:50:35.960 --> 00:50:39.040]   And basically truncation is used to make sure
[00:50:39.040 --> 00:50:41.840]   that the sequences you run through your model
[00:50:41.840 --> 00:50:44.680]   will fit for the given architect you're using
[00:50:44.680 --> 00:50:47.200]   and also for your GPU.
[00:50:47.200 --> 00:50:50.320]   So if you want to, again, truncate to,
[00:50:50.320 --> 00:50:52.800]   like the sequence like that,
[00:50:52.800 --> 00:50:54.280]   like eight would probably be too small,
[00:50:54.280 --> 00:50:58.080]   but maybe 128, even though the model takes 512,
[00:50:58.080 --> 00:51:02.120]   this can actually make things a little bit easier
[00:51:02.120 --> 00:51:05.920]   on your GPU, make your experimentation faster.
[00:51:05.920 --> 00:51:09.400]   And you may actually want to kind of pre-tokenize
[00:51:09.400 --> 00:51:12.440]   your raw text to see, you know, how, what is the biggest,
[00:51:12.440 --> 00:51:14.920]   what's the mean and figure out a max length
[00:51:14.920 --> 00:51:16.040]   somewhere in between there
[00:51:16.040 --> 00:51:17.880]   where you're getting good performance,
[00:51:17.880 --> 00:51:21.760]   but also not having to waste a lot of time
[00:51:21.760 --> 00:51:23.120]   watching your models train.
[00:51:23.120 --> 00:51:26.240]   And then of course,
[00:51:26.240 --> 00:51:28.200]   we talked about the attention mask already.
[00:51:28.200 --> 00:51:30.440]   And essentially this is just telling attention layers
[00:51:30.440 --> 00:51:33.560]   what to pay attention to, what not to pay attention to,
[00:51:33.560 --> 00:51:38.120]   and the tokenizer will build these things for you.
[00:51:38.120 --> 00:51:42.120]   And so, as I said, if we go back to blur right here,
[00:51:42.120 --> 00:51:51.120]   and let's go to this step right here.
[00:51:51.120 --> 00:51:57.600]   So we can actually, in blur,
[00:51:57.600 --> 00:52:00.880]   we can actually pass that information,
[00:52:00.880 --> 00:52:03.080]   the max length padding and truncation
[00:52:03.080 --> 00:52:08.360]   into this HF text block amongst a bunch of other things.
[00:52:08.360 --> 00:52:10.920]   And so if I wanted to,
[00:52:10.920 --> 00:52:15.400]   let's say I wanted to change the max length so that,
[00:52:15.400 --> 00:52:17.520]   you know, it was 128,
[00:52:17.520 --> 00:52:19.960]   regardless of how big the sequences were.
[00:52:19.960 --> 00:52:24.000]   And then I ran through this part,
[00:52:24.000 --> 00:52:25.960]   you can see they're smaller.
[00:52:25.960 --> 00:52:29.200]   And if I go to look at the shape of things,
[00:52:29.200 --> 00:52:32.160]   you can see that even though the actual text
[00:52:32.160 --> 00:52:34.920]   might be 512 or greater,
[00:52:34.920 --> 00:52:37.360]   it's not using that to define the length,
[00:52:37.360 --> 00:52:40.840]   it's using the 128 I specified here.
[00:52:40.840 --> 00:52:43.200]   And so all these different strategies
[00:52:43.200 --> 00:52:45.840]   that we've just kind of gone over
[00:52:45.840 --> 00:52:50.840]   can actually be passed into the text, the HF text block,
[00:52:50.840 --> 00:52:54.040]   and you'll get the behavior that you expect.
[00:52:54.920 --> 00:52:57.680]   (mouse clicking)
[00:52:57.680 --> 00:53:02.120]   Any questions on any of that?
[00:53:02.120 --> 00:53:07.000]   - Nope, no questions so far.
[00:53:07.000 --> 00:53:08.560]   - All right.
[00:53:08.560 --> 00:53:13.280]   And again, there's a link that will be included
[00:53:13.280 --> 00:53:14.520]   in the slides,
[00:53:14.520 --> 00:53:18.200]   which is essentially in the Hugging Face documentation.
[00:53:18.200 --> 00:53:20.600]   Like what we've discussed here in terms of padding,
[00:53:20.600 --> 00:53:22.320]   truncation, and max length,
[00:53:23.760 --> 00:53:24.640]   it's not exhaustive,
[00:53:24.640 --> 00:53:27.400]   there's other options that you may wanna explore.
[00:53:27.400 --> 00:53:29.440]   But typically in most of the stuff
[00:53:29.440 --> 00:53:30.960]   that you're probably working on,
[00:53:30.960 --> 00:53:33.120]   you wanna have padding equals true,
[00:53:33.120 --> 00:53:34.560]   truncation equals true,
[00:53:34.560 --> 00:53:36.320]   and max length equal to none,
[00:53:36.320 --> 00:53:39.480]   or like a specific max length like 128
[00:53:39.480 --> 00:53:43.640]   in order to get things to run on your GPU,
[00:53:43.640 --> 00:53:46.120]   or because you have a small data set,
[00:53:46.120 --> 00:53:48.280]   you know what I mean, whatever.
[00:53:48.280 --> 00:53:52.680]   But if you wanna look at all those different options
[00:53:52.680 --> 00:53:54.680]   when you're actually building your models,
[00:53:54.680 --> 00:53:56.120]   you'll be able to have this link,
[00:53:56.120 --> 00:53:59.360]   and you'll know more than you ever wanted to
[00:53:59.360 --> 00:54:01.080]   about padding and truncation,
[00:54:01.080 --> 00:54:03.560]   maybe even have nightmares about it,
[00:54:03.560 --> 00:54:05.640]   'cause there's a lot of things you can do.
[00:54:05.640 --> 00:54:11.080]   So with that, any final questions?
[00:54:11.080 --> 00:54:14.880]   Or is everything absolutely clear today,
[00:54:14.880 --> 00:54:17.760]   even after our Zoom bombs?
[00:54:17.760 --> 00:54:20.680]   - It was crystal clear to me.
[00:54:20.680 --> 00:54:24.200]   I'm sure if I was able to understand everyone else was.
[00:54:24.200 --> 00:54:25.680]   - All right, good, good.
[00:54:25.680 --> 00:54:29.120]   So for homework is watch the official course videos.
[00:54:29.120 --> 00:54:32.000]   There's a lot of really good content in the videos.
[00:54:32.000 --> 00:54:36.800]   It's not in the actual course documentation
[00:54:36.800 --> 00:54:38.680]   when you go through each section.
[00:54:38.680 --> 00:54:41.520]   And just like we have one blog post,
[00:54:41.520 --> 00:54:44.400]   which I'll retweet from Ravi, I think,
[00:54:44.400 --> 00:54:48.640]   blog something you've learned from week one or two,
[00:54:48.640 --> 00:54:51.720]   and see if you can actually include some actual code
[00:54:51.720 --> 00:54:54.040]   in your blog using Pure Hugging Face
[00:54:54.040 --> 00:54:56.480]   or one of the Fast.ai libraries.
[00:54:56.480 --> 00:54:59.000]   That's a great way to learn and also help others
[00:54:59.000 --> 00:55:01.560]   who are starting behind you.
[00:55:01.560 --> 00:55:05.400]   And then if you haven't, or even if you have,
[00:55:05.400 --> 00:55:08.560]   take another read at the "Attention Is All You Need" paper
[00:55:08.560 --> 00:55:13.560]   and also the Jay Alomar blog as well.
[00:55:13.560 --> 00:55:17.000]   And pick an architecture you're curious about.
[00:55:17.000 --> 00:55:18.640]   So one of the questions is,
[00:55:18.640 --> 00:55:21.120]   how do you actually choose an architecture
[00:55:21.120 --> 00:55:23.200]   for what you're trying to do?
[00:55:23.200 --> 00:55:24.600]   Find one that you're interested in,
[00:55:24.600 --> 00:55:27.720]   that you've heard about and people have been hyping.
[00:55:27.720 --> 00:55:32.000]   Like for example, like I have really good success
[00:55:32.000 --> 00:55:34.560]   with Roberta or Bart.
[00:55:34.560 --> 00:55:37.040]   Maybe pick one of those, read the paper.
[00:55:37.040 --> 00:55:40.720]   And as you go through, if you have questions,
[00:55:40.720 --> 00:55:43.120]   use the Discord and let's talk about it.
[00:55:43.120 --> 00:55:46.200]   But that's a great way to kind of learn like,
[00:55:46.200 --> 00:55:47.880]   not only how they work, but whether or not
[00:55:47.880 --> 00:55:50.760]   it's something that's suitable for what you're doing.
[00:55:50.760 --> 00:55:55.080]   And then lastly, sorry about the typo.
[00:55:55.080 --> 00:55:58.680]   It's not get read, it's get ready for some competitions.
[00:55:58.680 --> 00:56:00.640]   So we're gonna have some friendly competitions
[00:56:00.640 --> 00:56:05.440]   that we'll announce next week that will hopefully be fun
[00:56:05.440 --> 00:56:10.440]   and help improve all of our learning of these things
[00:56:10.440 --> 00:56:12.400]   by actually doing some stuff.
[00:56:12.400 --> 00:56:15.800]   So we'll announce that next week.
[00:56:15.800 --> 00:56:17.200]   And that's it.
[00:56:17.200 --> 00:56:18.920]   We've made it through Zoom bombing.
[00:56:18.920 --> 00:56:21.120]   We've made it through section two.
[00:56:21.120 --> 00:56:23.920]   Any final questions?
[00:56:23.920 --> 00:56:26.520]   (upbeat music)
[00:56:26.520 --> 00:56:29.120]   (upbeat music)
[00:56:29.120 --> 00:56:31.720]   (upbeat music)
[00:56:31.720 --> 00:56:34.320]   (upbeat music)
[00:56:34.320 --> 00:56:36.920]   (upbeat music)
[00:56:37.120 --> 00:56:39.720]   (upbeat music)
[00:56:39.920 --> 00:56:42.520]   (upbeat music)
[00:56:42.520 --> 00:56:45.100]   (upbeat music)
[00:56:45.100 --> 00:56:47.600]   (light music)

