
[00:00:00.000 --> 00:00:03.320]   - Thank you very much Charles for introduction
[00:00:03.320 --> 00:00:04.840]   and thank you to Weights and Biases
[00:00:04.840 --> 00:00:06.240]   for inviting me to give this talk.
[00:00:06.240 --> 00:00:08.440]   It's really exciting and also very excited
[00:00:08.440 --> 00:00:11.600]   that you've done your PhD also in a similar area.
[00:00:11.600 --> 00:00:12.920]   So this is super exciting.
[00:00:12.920 --> 00:00:14.320]   All right, so I'm gonna speak
[00:00:14.320 --> 00:00:16.000]   about the Lost Landscape Project.
[00:00:16.000 --> 00:00:18.840]   So what I'm gonna do is over a few minutes,
[00:00:18.840 --> 00:00:22.000]   I'm gonna speak a bit about the context of it all.
[00:00:22.000 --> 00:00:24.160]   Then I'm gonna take some minutes to speak
[00:00:24.160 --> 00:00:27.880]   about the dynamics of these landscapes in movement
[00:00:27.880 --> 00:00:29.600]   that is really fascinating.
[00:00:29.600 --> 00:00:32.640]   And then I'm going to show a 10 minute video
[00:00:32.640 --> 00:00:34.880]   with many examples of the project
[00:00:34.880 --> 00:00:38.640]   and I'm gonna speak over the video describing what we see.
[00:00:38.640 --> 00:00:40.960]   And I will end up talking a little bit
[00:00:40.960 --> 00:00:43.640]   about the Lost Landscape Explorer app
[00:00:43.640 --> 00:00:45.960]   that I just released a few days ago.
[00:00:45.960 --> 00:00:46.960]   So let's go for it.
[00:00:46.960 --> 00:00:50.880]   So as Charles said, my background,
[00:00:50.880 --> 00:00:54.640]   the background of this project is multidisciplinary.
[00:00:54.640 --> 00:00:56.000]   And like my background,
[00:00:56.000 --> 00:00:58.880]   there is a combination of engineering, technical fields,
[00:00:58.880 --> 00:01:00.720]   many creative fields.
[00:01:00.720 --> 00:01:03.320]   And so what are we doing in the Lost Landscape Project?
[00:01:03.320 --> 00:01:06.280]   So we care of course, very much, right?
[00:01:06.280 --> 00:01:08.920]   About the performance of our neural networks
[00:01:08.920 --> 00:01:12.200]   and we measure that performance with the loss functions.
[00:01:12.200 --> 00:01:14.680]   And these loss functions are comparing
[00:01:14.680 --> 00:01:19.200]   what we are obtaining and the target that we want to obtain.
[00:01:19.200 --> 00:01:20.960]   And this way we get the loss value
[00:01:20.960 --> 00:01:22.280]   and we use back propagation
[00:01:22.280 --> 00:01:24.240]   and we change the weights of the network.
[00:01:24.240 --> 00:01:27.920]   And of course the loss function depends on these weights
[00:01:27.920 --> 00:01:30.000]   and we may have millions or billions
[00:01:30.000 --> 00:01:32.400]   and very soon trillions of weights.
[00:01:32.400 --> 00:01:34.680]   So these are very high dimensional
[00:01:34.680 --> 00:01:36.840]   and non-linear functions.
[00:01:36.840 --> 00:01:39.120]   And of course we can use different types of loss functions
[00:01:39.120 --> 00:01:41.760]   depending on the objective of what we're doing,
[00:01:41.760 --> 00:01:44.360]   regression, classification, or working with GANs.
[00:01:44.360 --> 00:01:46.320]   But the objective is always the same.
[00:01:46.320 --> 00:01:48.920]   We are trying to push these loss value
[00:01:48.920 --> 00:01:52.000]   as small as possible, of course, in different ways,
[00:01:52.000 --> 00:01:53.200]   depending on the constraints
[00:01:53.200 --> 00:01:55.440]   and the needs of whatever we're doing.
[00:01:55.440 --> 00:01:57.040]   And so it would be really interesting
[00:01:57.040 --> 00:01:59.760]   for us to understand what is going on
[00:01:59.760 --> 00:02:01.040]   in all of this complexity
[00:02:01.040 --> 00:02:03.640]   on all of these complex weight spaces
[00:02:03.640 --> 00:02:05.680]   and to tackle this challenge,
[00:02:05.680 --> 00:02:08.640]   numerical analysis and visualization
[00:02:08.640 --> 00:02:10.520]   complement each other
[00:02:10.520 --> 00:02:13.280]   and they are partners that work together.
[00:02:13.280 --> 00:02:16.400]   And working together, they can help us understand
[00:02:16.400 --> 00:02:21.400]   through understanding these complex high dimensional spaces,
[00:02:21.400 --> 00:02:25.520]   more about how generalization works,
[00:02:25.520 --> 00:02:27.840]   ways to improve our architectures
[00:02:27.840 --> 00:02:31.040]   and our optimization algorithms, et cetera, et cetera.
[00:02:31.040 --> 00:02:33.000]   And we all know the importance
[00:02:33.000 --> 00:02:35.160]   and the usefulness of visualization
[00:02:35.160 --> 00:02:38.320]   because visualization complements numerical analysis,
[00:02:38.320 --> 00:02:41.000]   but it deals with information in a different way.
[00:02:41.000 --> 00:02:43.160]   And it gives us a very unique
[00:02:43.160 --> 00:02:45.320]   and different perspective on things.
[00:02:45.320 --> 00:02:46.600]   That's why, for example,
[00:02:46.600 --> 00:02:50.400]   Einstein used to mainly use visualization
[00:02:50.400 --> 00:02:52.600]   when he was chasing new insights.
[00:02:52.600 --> 00:02:54.480]   For example, imagining that he was
[00:02:54.480 --> 00:02:56.640]   a photon traveling through space,
[00:02:56.640 --> 00:02:58.880]   one of the many visualizations he used
[00:02:58.880 --> 00:03:01.200]   to get to his theory of relativity.
[00:03:01.200 --> 00:03:03.600]   And on the right, we can see the first picture
[00:03:03.600 --> 00:03:06.600]   of a black hole that was released recently
[00:03:06.600 --> 00:03:11.200]   and that may eventually one day earn a Nobel Prize.
[00:03:11.200 --> 00:03:13.280]   And these visualizations,
[00:03:13.280 --> 00:03:15.320]   what Einstein used to imagine
[00:03:15.320 --> 00:03:16.720]   and the picture of the black hole
[00:03:16.720 --> 00:03:17.880]   and the representations
[00:03:17.880 --> 00:03:19.480]   that we will see of the lost landscapes,
[00:03:19.480 --> 00:03:21.920]   they are big simplifications
[00:03:21.920 --> 00:03:25.400]   of that underlying complexity and reality.
[00:03:25.400 --> 00:03:27.280]   And yet they are so important.
[00:03:27.280 --> 00:03:28.240]   Why is this?
[00:03:28.240 --> 00:03:30.560]   Because when we're dealing with things
[00:03:30.560 --> 00:03:33.960]   and with complexities that are so unreachable,
[00:03:33.960 --> 00:03:36.480]   so far away from our reality,
[00:03:36.480 --> 00:03:37.800]   like the black hole,
[00:03:37.800 --> 00:03:40.040]   like what Einstein was dealing with,
[00:03:40.040 --> 00:03:41.880]   or like these lost landscapes,
[00:03:41.880 --> 00:03:44.800]   like these very high dimensional spaces
[00:03:44.800 --> 00:03:47.240]   with billions of dimensions,
[00:03:47.240 --> 00:03:51.720]   even these very simple cross sections
[00:03:51.720 --> 00:03:56.560]   simplifications can be the key
[00:03:56.560 --> 00:03:59.160]   that take us to very unique insights.
[00:03:59.160 --> 00:04:03.360]   But the important question that we always have to do,
[00:04:03.360 --> 00:04:05.920]   either with the imagination of Einstein
[00:04:05.920 --> 00:04:07.400]   or the picture of the black hole
[00:04:07.400 --> 00:04:08.520]   of these representations
[00:04:08.520 --> 00:04:10.680]   that we will see of the lost landscapes is,
[00:04:10.680 --> 00:04:15.320]   are they capturing at least some important connection
[00:04:15.320 --> 00:04:18.160]   that provides useful and actionable data?
[00:04:18.160 --> 00:04:19.000]   And we will see that
[00:04:19.000 --> 00:04:21.280]   with these lost landscape representations,
[00:04:21.280 --> 00:04:22.920]   the answer is yes.
[00:04:22.920 --> 00:04:25.160]   And we will explain why very soon.
[00:04:25.160 --> 00:04:28.640]   But so our challenge is that
[00:04:28.640 --> 00:04:30.680]   we're a little speck of dust
[00:04:30.680 --> 00:04:35.160]   in this big chart of the amount of dimensions
[00:04:35.160 --> 00:04:38.520]   that we can deal with in these visualizations.
[00:04:38.520 --> 00:04:41.160]   And of course, we can only visualize three dimensions.
[00:04:41.160 --> 00:04:44.680]   So we have to somehow simplify this complexity
[00:04:44.680 --> 00:04:48.320]   and we use dimensionality reduction techniques to do that.
[00:04:48.320 --> 00:04:49.240]   And in the last years,
[00:04:49.240 --> 00:04:50.520]   researchers have been using
[00:04:50.520 --> 00:04:53.120]   these dimensionality reduction techniques
[00:04:53.120 --> 00:04:56.440]   to create 1D plots, then 2D plots, then 3D plots.
[00:04:56.440 --> 00:04:59.120]   And for example, one of my favorite papers
[00:04:59.120 --> 00:05:01.760]   is this one by Tom Goldstein's team
[00:05:01.760 --> 00:05:04.120]   that is an absolute reference in this area,
[00:05:04.120 --> 00:05:06.800]   wonderful paper and I refer to it very often.
[00:05:06.800 --> 00:05:09.600]   And so the lost landscape project
[00:05:09.600 --> 00:05:11.760]   that I'm speaking about today,
[00:05:11.760 --> 00:05:13.080]   what it's trying to do
[00:05:13.080 --> 00:05:15.480]   is to bring greater and greater detail
[00:05:15.480 --> 00:05:17.040]   to these visualizations
[00:05:17.040 --> 00:05:20.320]   and also to study them in movement,
[00:05:20.320 --> 00:05:23.400]   the dynamics in movement through the steps and the epochs
[00:05:23.400 --> 00:05:27.760]   and also to bring to them the creative aspect
[00:05:27.760 --> 00:05:30.760]   that is not just about making these representations
[00:05:30.760 --> 00:05:33.280]   look nice because a lot of people see them
[00:05:33.280 --> 00:05:35.440]   and they say, oh, they look nice,
[00:05:35.440 --> 00:05:37.120]   but it's more than that, right?
[00:05:37.120 --> 00:05:38.560]   For example, in this example,
[00:05:38.560 --> 00:05:40.640]   we can see a picture of a galaxy on the left
[00:05:40.640 --> 00:05:42.200]   and this is the raw picture.
[00:05:42.200 --> 00:05:43.040]   And on the right,
[00:05:43.040 --> 00:05:45.200]   we see what people typically see
[00:05:45.200 --> 00:05:47.480]   that is the post-produced version.
[00:05:47.480 --> 00:05:50.720]   And of course the one on the right is much more beautiful,
[00:05:50.720 --> 00:05:52.720]   but it's more than that, right?
[00:05:52.720 --> 00:05:55.320]   It is easier to understand,
[00:05:55.320 --> 00:05:57.080]   it is easier to work with
[00:05:57.080 --> 00:05:59.520]   and if we may arrive to a new insight
[00:05:59.520 --> 00:06:01.360]   with one of these two
[00:06:01.360 --> 00:06:03.800]   is probably going to be with the one on the right.
[00:06:03.800 --> 00:06:07.320]   So the creative aspect of the project
[00:06:07.320 --> 00:06:10.200]   is more than just about making it look nice,
[00:06:10.200 --> 00:06:12.640]   it's also about those other things.
[00:06:12.640 --> 00:06:15.240]   So we have all of this complexity
[00:06:15.240 --> 00:06:17.160]   and in the last years also researchers,
[00:06:17.160 --> 00:06:20.160]   as we take these dimensionality
[00:06:20.160 --> 00:06:23.560]   with you to representations of these weight spaces
[00:06:23.560 --> 00:06:25.480]   and we combine them with the loss values
[00:06:25.480 --> 00:06:27.160]   and we build these representations,
[00:06:27.160 --> 00:06:29.040]   researchers have found connections
[00:06:29.040 --> 00:06:31.360]   between the geometry of the surfaces,
[00:06:31.360 --> 00:06:34.120]   also, I mean, using both numerical analysis
[00:06:34.120 --> 00:06:35.200]   and visualizations
[00:06:35.200 --> 00:06:36.840]   and different properties of the networks.
[00:06:36.840 --> 00:06:39.040]   We all know that when we use ResNets, right?
[00:06:39.040 --> 00:06:40.440]   If we use the skip connections,
[00:06:40.440 --> 00:06:42.160]   we get a smoother surface.
[00:06:42.800 --> 00:06:46.240]   And if we, for example, use SGD with small batch sizes,
[00:06:46.240 --> 00:06:49.120]   we tend to convert to flatter minima
[00:06:49.120 --> 00:06:50.920]   and this tends to correlate,
[00:06:50.920 --> 00:06:53.880]   most researchers agree with better generalization,
[00:06:53.880 --> 00:06:55.520]   which is quite intuitive
[00:06:55.520 --> 00:06:57.520]   because if we are in a minima
[00:06:57.520 --> 00:06:59.320]   in a certain point in weight space
[00:06:59.320 --> 00:07:01.280]   and we move a little bit
[00:07:01.280 --> 00:07:04.560]   and the minima is flatter as we move,
[00:07:04.560 --> 00:07:07.040]   the loss value is not going to change too much.
[00:07:07.040 --> 00:07:10.960]   So we kind of have a wider margin of safety,
[00:07:10.960 --> 00:07:12.920]   which means that the boundaries
[00:07:12.920 --> 00:07:15.600]   that for sampling classification separate our data,
[00:07:15.600 --> 00:07:17.320]   they are gonna be more resilient.
[00:07:17.320 --> 00:07:20.120]   So we find these type of connections.
[00:07:20.120 --> 00:07:22.000]   That's why we're interested in creating
[00:07:22.000 --> 00:07:23.760]   these types of visualizations.
[00:07:23.760 --> 00:07:26.480]   And so we get to a point in weight space
[00:07:26.480 --> 00:07:27.880]   and we don't wanna be blind.
[00:07:27.880 --> 00:07:30.520]   We want to understand what happens
[00:07:30.520 --> 00:07:32.480]   if we move a little bit,
[00:07:32.480 --> 00:07:34.280]   what happens around us?
[00:07:34.280 --> 00:07:35.920]   Is the surface rough?
[00:07:35.920 --> 00:07:36.880]   Is it a smoother?
[00:07:36.880 --> 00:07:38.320]   Is it full of non-convexities?
[00:07:38.320 --> 00:07:39.160]   What's going on?
[00:07:39.240 --> 00:07:41.360]   And we use different mathematical operations, right?
[00:07:41.360 --> 00:07:43.400]   To navigate these weight spaces
[00:07:43.400 --> 00:07:44.680]   in the high dimensionality.
[00:07:44.680 --> 00:07:46.840]   We can do an interpolation
[00:07:46.840 --> 00:07:49.200]   from a point in weight space in a random direction
[00:07:49.200 --> 00:07:51.040]   or we can do linear interpolation
[00:07:51.040 --> 00:07:52.920]   between two points in weight space
[00:07:52.920 --> 00:07:55.200]   or we can pick a couple of random directions
[00:07:55.200 --> 00:07:56.720]   that are orthogonal to each other
[00:07:56.720 --> 00:07:57.640]   and they make a plane
[00:07:57.640 --> 00:08:00.320]   and we can slice the high dimensionality with it
[00:08:00.320 --> 00:08:01.840]   and project it onto the plane
[00:08:01.840 --> 00:08:03.600]   and calculate a lot of loss values
[00:08:03.600 --> 00:08:04.760]   on a grid on the plane
[00:08:04.760 --> 00:08:07.400]   and build this 3D surface.
[00:08:07.400 --> 00:08:09.840]   Or we can pick a couple of minimizers
[00:08:09.840 --> 00:08:11.200]   points in weight space
[00:08:11.200 --> 00:08:15.720]   and look for paths that connect these two minima
[00:08:15.720 --> 00:08:17.400]   maintaining a low loss value.
[00:08:17.400 --> 00:08:18.600]   And this is a wonderful paper
[00:08:18.600 --> 00:08:21.600]   and I will be talking about this in the examples.
[00:08:21.600 --> 00:08:24.160]   So we are exploring all of these cross sections
[00:08:24.160 --> 00:08:25.560]   of this hidden beauty,
[00:08:25.560 --> 00:08:28.520]   this hidden mysteries in the high dimensionality
[00:08:28.520 --> 00:08:30.600]   and we can do this dimensionality reduction
[00:08:30.600 --> 00:08:31.560]   in many different ways.
[00:08:31.560 --> 00:08:35.000]   For example, we can use PCA directions
[00:08:35.000 --> 00:08:36.360]   and these are the directions
[00:08:36.360 --> 00:08:39.040]   that are the most optimized in the system
[00:08:39.040 --> 00:08:40.200]   down the gradient.
[00:08:40.200 --> 00:08:41.040]   And if we do this,
[00:08:41.040 --> 00:08:42.680]   we will be able to capture for example,
[00:08:42.680 --> 00:08:45.440]   pretty well the variation in the trajectories,
[00:08:45.440 --> 00:08:47.720]   number of trajectories of SGD.
[00:08:47.720 --> 00:08:50.120]   But we will not capture so well
[00:08:50.120 --> 00:08:53.800]   all the richness around in the rest of the landscape
[00:08:53.800 --> 00:08:55.440]   with the distribution of non-convexities
[00:08:55.440 --> 00:08:57.560]   and convexities if they exist.
[00:08:57.560 --> 00:09:00.480]   Conversely, if we use random directions, for example,
[00:09:00.480 --> 00:09:02.760]   we will better be able to capture
[00:09:02.760 --> 00:09:04.160]   the richness of the landscape,
[00:09:04.160 --> 00:09:07.000]   the distributions of non-convexities and convexities,
[00:09:07.000 --> 00:09:09.040]   but we will have more trouble to capture,
[00:09:09.040 --> 00:09:11.240]   for example, the variation of the trajectories.
[00:09:11.240 --> 00:09:12.200]   Why is this?
[00:09:12.200 --> 00:09:14.520]   Because it has been shown that the trajectory,
[00:09:14.520 --> 00:09:16.520]   for example, of SGD, right?
[00:09:16.520 --> 00:09:20.720]   It exists on a very low dimensional subspace.
[00:09:20.720 --> 00:09:22.840]   And if we pick a random direction
[00:09:22.840 --> 00:09:26.040]   because of the counter-intuitive properties
[00:09:26.040 --> 00:09:27.760]   of high dimensional space,
[00:09:27.760 --> 00:09:30.640]   it will probably be orthogonal to the trajectory
[00:09:30.640 --> 00:09:32.080]   and we will not be able to capture
[00:09:32.080 --> 00:09:33.600]   well the variation in the trajectory.
[00:09:33.600 --> 00:09:36.360]   So in summary, different strategies,
[00:09:36.360 --> 00:09:39.400]   different directions serve for different purposes,
[00:09:39.400 --> 00:09:41.880]   and we can do this dimensionality reduction
[00:09:41.880 --> 00:09:43.680]   in many different ways.
[00:09:43.680 --> 00:09:46.360]   Because of the same counter-intuitive properties
[00:09:46.360 --> 00:09:48.040]   of these high dimensional spaces,
[00:09:48.040 --> 00:09:50.200]   when we pick a couple of random directions,
[00:09:50.200 --> 00:09:51.960]   it's practically guaranteed
[00:09:51.960 --> 00:09:53.800]   that they are gonna be orthogonal to each other
[00:09:53.800 --> 00:09:56.920]   because as we know, the higher the dimensionality,
[00:09:56.920 --> 00:09:59.400]   the higher the proportion of the space
[00:09:59.400 --> 00:10:02.160]   that is being occupied by the orthogonal vectors,
[00:10:02.160 --> 00:10:04.040]   and we can use the cosine similarity
[00:10:04.040 --> 00:10:07.440]   and basic proofs to demonstrate this.
[00:10:07.440 --> 00:10:10.160]   And finally, finally, the last thing
[00:10:10.160 --> 00:10:12.440]   that we have to consider very important
[00:10:12.440 --> 00:10:15.480]   is that we must normalize these directions.
[00:10:15.480 --> 00:10:17.600]   And it is very intuitive to understand why.
[00:10:17.600 --> 00:10:20.720]   If we have, for example, a couple of minimizers
[00:10:20.720 --> 00:10:22.840]   with different magnitudes in the weights,
[00:10:22.840 --> 00:10:24.920]   and we apply the same perturbation
[00:10:24.920 --> 00:10:27.600]   of the same magnitude to them,
[00:10:27.600 --> 00:10:30.640]   and we build visualizations and we compare them,
[00:10:30.640 --> 00:10:33.560]   we're going to see differences that are being caused
[00:10:33.560 --> 00:10:36.800]   because of the lack of the right proportions
[00:10:36.800 --> 00:10:39.560]   between the perturbation and the magnitudes of these weights.
[00:10:39.560 --> 00:10:42.640]   And this can produce different types of issues,
[00:10:42.640 --> 00:10:44.320]   like we see in this example
[00:10:44.320 --> 00:10:47.440]   from Tom Goldstein's team paper.
[00:10:47.440 --> 00:10:49.480]   We can have, for example, a small bat size
[00:10:49.480 --> 00:10:51.160]   and large bat size minimizers,
[00:10:51.160 --> 00:10:53.200]   and we can apply weight to decay,
[00:10:53.200 --> 00:10:55.880]   and of course, the updates are gonna happen more often
[00:10:55.880 --> 00:10:57.400]   in the case of the small bats,
[00:10:57.400 --> 00:10:59.560]   so those weights are gonna get smaller.
[00:10:59.560 --> 00:11:01.240]   And then we build the visualizations,
[00:11:01.240 --> 00:11:02.280]   and we see that, for example,
[00:11:02.280 --> 00:11:05.520]   a minimizer that looked flatter now looks sharper,
[00:11:05.520 --> 00:11:07.360]   but the generalization is still better.
[00:11:07.360 --> 00:11:09.720]   So again, we're seeing the impact
[00:11:09.720 --> 00:11:13.120]   of the lack of the right proportions
[00:11:13.120 --> 00:11:15.200]   of the perturbation compared to the weights.
[00:11:15.200 --> 00:11:18.120]   So we solve this very easily normalizing the directions.
[00:11:18.120 --> 00:11:20.560]   We can normalize by layer, by filter.
[00:11:20.560 --> 00:11:23.080]   A filter works very well if we have a conf net.
[00:11:23.080 --> 00:11:25.960]   If it's not a conf net, we can apply a similar thing
[00:11:25.960 --> 00:11:27.960]   with the equivalent structure.
[00:11:27.960 --> 00:11:28.800]   And that's it.
[00:11:28.800 --> 00:11:31.320]   We create these dimensionality reductions.
[00:11:31.320 --> 00:11:35.280]   We produce these 3D representations,
[00:11:35.280 --> 00:11:37.880]   and then we ask ourselves,
[00:11:37.880 --> 00:11:41.520]   so are we capturing in these visualizations
[00:11:41.520 --> 00:11:43.120]   useful and actionable data?
[00:11:43.120 --> 00:11:45.640]   Do we have there a connection
[00:11:45.640 --> 00:11:48.320]   that even if we're so far away
[00:11:48.320 --> 00:11:51.160]   is useful enough to work with it?
[00:11:51.160 --> 00:11:53.920]   And just the researchers have demonstrated
[00:11:53.920 --> 00:11:56.880]   that the main curvatures of this dimensionality
[00:11:56.880 --> 00:12:00.000]   reduced representations are a weighted average
[00:12:00.000 --> 00:12:03.320]   of the curvatures in the high dimensional space.
[00:12:03.320 --> 00:12:06.760]   And to demonstrate this, they use numerical analysis.
[00:12:06.760 --> 00:12:08.120]   They use the Hessian.
[00:12:08.120 --> 00:12:11.000]   They use the eigenvalues, the extreme eigenvalues,
[00:12:11.000 --> 00:12:13.040]   the ratio of the extreme eigenvalues.
[00:12:13.040 --> 00:12:15.240]   They can, for example, build heat maps
[00:12:15.240 --> 00:12:18.320]   and study the distribution of non-convexities
[00:12:18.320 --> 00:12:20.640]   and convexities in the high dimensional space
[00:12:20.640 --> 00:12:23.640]   and compare them and see that they match.
[00:12:23.640 --> 00:12:26.560]   And that, for example, if we see non-convexities
[00:12:26.560 --> 00:12:30.560]   in this dimensionality reduced representations,
[00:12:30.560 --> 00:12:32.320]   it means there will be non-convexities
[00:12:32.320 --> 00:12:34.720]   in the high dimensional space.
[00:12:34.720 --> 00:12:36.800]   And if we have a positive curvature,
[00:12:36.800 --> 00:12:37.840]   if we have convexities,
[00:12:37.840 --> 00:12:41.200]   it doesn't mean that in the high dimensional space,
[00:12:41.200 --> 00:12:42.080]   everything is convex,
[00:12:42.080 --> 00:12:45.080]   but it means that the positive curvature is dominant.
[00:12:45.080 --> 00:12:47.120]   So in this way, we demonstrate
[00:12:47.120 --> 00:12:50.160]   that there is a solid enough connection
[00:12:50.160 --> 00:12:53.640]   that allows us to work with these visualizations
[00:12:53.640 --> 00:12:57.160]   and to try to chase new insights through them.
[00:12:57.160 --> 00:13:00.800]   And then finally, combining different networks
[00:13:00.800 --> 00:13:02.320]   and different parameters,
[00:13:02.320 --> 00:13:04.760]   these representations get built.
[00:13:04.760 --> 00:13:06.200]   Now we get to the dynamics
[00:13:06.200 --> 00:13:09.040]   that I haven't spoken so much about in other talks.
[00:13:09.040 --> 00:13:13.560]   As we do the same thing, not just in static terms,
[00:13:13.560 --> 00:13:16.720]   but also through different steps and epochs,
[00:13:16.720 --> 00:13:19.080]   and we build these representations in movement,
[00:13:19.080 --> 00:13:21.960]   we begin to see this counter-intuitive effects.
[00:13:21.960 --> 00:13:24.440]   We see shapes that appear out of nowhere.
[00:13:24.440 --> 00:13:26.240]   We see morphology shifts.
[00:13:26.240 --> 00:13:27.600]   We see tunneling.
[00:13:27.600 --> 00:13:29.200]   Why are these things happening?
[00:13:29.200 --> 00:13:31.800]   And we're gonna see these things in the examples
[00:13:31.800 --> 00:13:33.880]   that I'm going to show later.
[00:13:33.880 --> 00:13:37.040]   And for that, we have to accept that in comparison
[00:13:37.040 --> 00:13:39.600]   with this big challenge that we have ahead of us
[00:13:39.600 --> 00:13:42.480]   in this very, very high dimensional spaces,
[00:13:42.480 --> 00:13:46.840]   we live in our own flat land reality.
[00:13:46.840 --> 00:13:50.760]   And there is a wonderful talk given by mathematician,
[00:13:50.760 --> 00:13:52.920]   Matt Parker at the Royal Institution
[00:13:52.920 --> 00:13:54.920]   called "Four Dimensional Maths,"
[00:13:54.920 --> 00:13:58.560]   where he provides these beautiful examples in video.
[00:13:58.560 --> 00:13:59.560]   And what we see, for example,
[00:13:59.560 --> 00:14:02.720]   a 3D cube that is moving through a 2D world.
[00:14:02.720 --> 00:14:04.720]   And from the perspective of the 2D world,
[00:14:04.720 --> 00:14:08.480]   what we see is this square that appears out of nowhere
[00:14:08.480 --> 00:14:10.480]   and eventually disappears.
[00:14:10.480 --> 00:14:12.240]   Of course, from the perspective of the...
[00:14:12.240 --> 00:14:14.800]   Here we see the same thing corner first,
[00:14:14.800 --> 00:14:16.080]   and then it gets crazier.
[00:14:16.080 --> 00:14:18.040]   We see a triangle that appears.
[00:14:18.040 --> 00:14:21.840]   It becomes a different shape and eventually disappears.
[00:14:21.840 --> 00:14:23.960]   From the perspective of the 3D world,
[00:14:23.960 --> 00:14:25.480]   everything is continuous,
[00:14:25.480 --> 00:14:29.320]   but from the perspective of the 2D world,
[00:14:29.320 --> 00:14:30.880]   we see this counterintuitive effect.
[00:14:30.880 --> 00:14:32.440]   This is falling edge first,
[00:14:32.440 --> 00:14:35.360]   and we see this rectangle that appears out of nowhere.
[00:14:35.360 --> 00:14:37.200]   It expands, then it contracts,
[00:14:37.200 --> 00:14:39.120]   and then it disappears again.
[00:14:39.120 --> 00:14:42.800]   And now we see the same thing.
[00:14:42.800 --> 00:14:45.560]   And on the right, we can see what would be the perspective
[00:14:45.560 --> 00:14:47.840]   from the 2D world.
[00:14:47.840 --> 00:14:50.040]   And now we're going to see the simulation
[00:14:50.040 --> 00:14:54.360]   of a 4D object falling through a 3D world,
[00:14:54.360 --> 00:14:58.760]   and the shapes get even more complex and even crazier.
[00:14:58.760 --> 00:15:02.440]   So in the representations of the landscapes in movement,
[00:15:02.440 --> 00:15:05.120]   we're going to see a similar dynamic.
[00:15:05.120 --> 00:15:08.720]   As we move through these steps and the epochs,
[00:15:08.720 --> 00:15:12.800]   every move triggers a dimensionality reduction
[00:15:12.800 --> 00:15:16.360]   transformation, and this is analog to filtering
[00:15:16.360 --> 00:15:18.520]   that high dimensional space
[00:15:18.520 --> 00:15:21.520]   through our low dimensional reality.
[00:15:21.520 --> 00:15:24.480]   The high dimensional continuity remains,
[00:15:24.480 --> 00:15:27.640]   but those counterintuitive changes are caused
[00:15:27.640 --> 00:15:31.080]   by the act of filtering it through our low dimensionality
[00:15:31.080 --> 00:15:32.640]   as we progress through that space.
[00:15:32.640 --> 00:15:35.280]   And this is what produces these counterintuitive effects
[00:15:35.280 --> 00:15:38.480]   that we're going to see in these examples.
[00:15:38.480 --> 00:15:41.040]   There is also the noise because there is morphology noise
[00:15:41.040 --> 00:15:43.040]   in the landscapes that of course, for example,
[00:15:43.040 --> 00:15:45.000]   like the skip connections in the ResNet
[00:15:45.000 --> 00:15:47.640]   that produce this moving effect.
[00:15:47.640 --> 00:15:50.080]   They come from the architecture, the hyperparameters,
[00:15:50.080 --> 00:15:51.280]   and there is a dynamic noise.
[00:15:51.280 --> 00:15:53.680]   And an example is if we use, for example,
[00:15:53.680 --> 00:15:56.520]   small batch sizes and we're capturing steps.
[00:15:56.520 --> 00:15:57.960]   And of course, in each batch,
[00:15:57.960 --> 00:16:00.560]   we're using a different part of the dataset.
[00:16:00.560 --> 00:16:02.680]   So this is going to produce a variation
[00:16:02.680 --> 00:16:05.280]   in the dynamics of the landscape, for example.
[00:16:05.280 --> 00:16:08.280]   All right, and we also see in the examples
[00:16:08.280 --> 00:16:13.280]   that Matt provides, how as we go higher in dimensionality,
[00:16:13.560 --> 00:16:15.680]   these counterintuitive effects,
[00:16:15.680 --> 00:16:18.480]   they get more and more complex as well.
[00:16:18.480 --> 00:16:21.040]   And in our case, as we go from this billion
[00:16:21.040 --> 00:16:22.560]   and soon trillion of dimensions
[00:16:22.560 --> 00:16:25.000]   to our two dimensions plus the last three,
[00:16:25.000 --> 00:16:27.840]   we're going to see a different counterintuitive things
[00:16:27.840 --> 00:16:30.000]   happening that we're going to see in the examples.
[00:16:30.000 --> 00:16:31.120]   One of them is tunneling.
[00:16:31.120 --> 00:16:33.480]   We may see at the beginning of a training process,
[00:16:33.480 --> 00:16:35.360]   a landscape that is quite flat.
[00:16:35.360 --> 00:16:37.680]   And then as the training progresses,
[00:16:37.680 --> 00:16:42.080]   we see how the convexity begins to tunnel down the landscape.
[00:16:42.080 --> 00:16:43.760]   And this of course doesn't mean
[00:16:43.760 --> 00:16:45.360]   that in the high dimensionality,
[00:16:45.360 --> 00:16:47.560]   there is like a hole being created.
[00:16:47.560 --> 00:16:48.400]   No, of course not.
[00:16:48.400 --> 00:16:50.000]   As we know in the high dimensionality,
[00:16:50.000 --> 00:16:51.920]   we're just moving in these millions and billions
[00:16:51.920 --> 00:16:53.080]   of different directions.
[00:16:53.080 --> 00:16:56.240]   But when we filter that through our flatland reality,
[00:16:56.240 --> 00:16:59.160]   we see this counterintuitive tunneling in this case.
[00:16:59.160 --> 00:17:00.800]   Now, what is really interesting about this
[00:17:00.800 --> 00:17:03.560]   is that we've been worried for a long time
[00:17:03.560 --> 00:17:07.040]   about the local minima and the subtle points, et cetera.
[00:17:07.040 --> 00:17:10.960]   But now we are beginning studying this type of dynamics
[00:17:10.960 --> 00:17:13.480]   with numerical analysis and with these visualizations.
[00:17:13.480 --> 00:17:16.280]   We're starting to go into mode connectivity,
[00:17:16.280 --> 00:17:18.640]   finding that is much easier than we thought
[00:17:18.640 --> 00:17:20.480]   to connect the different minima.
[00:17:20.480 --> 00:17:22.680]   And we're going into all of this concept
[00:17:22.680 --> 00:17:24.520]   of the blessing of dimensionality.
[00:17:24.520 --> 00:17:27.640]   That for example, Babak Hasibi explains really well
[00:17:27.640 --> 00:17:30.520]   in this wonderful talk, in which he tells us
[00:17:30.520 --> 00:17:33.240]   that when we initialize well our networks,
[00:17:33.240 --> 00:17:35.680]   the problem of finding a good minima
[00:17:35.680 --> 00:17:38.960]   goes from finding a needle in a haystack
[00:17:38.960 --> 00:17:42.320]   to exploring a haystack full of needles
[00:17:42.320 --> 00:17:46.320]   because it becomes almost a local problem.
[00:17:46.320 --> 00:17:50.520]   And by studying all of these dynamics as well,
[00:17:50.520 --> 00:17:53.040]   we're getting closer to understand more and more
[00:17:53.040 --> 00:17:56.960]   how the high dimensionality has a lot of blessings
[00:17:56.960 --> 00:18:00.080]   that are being rebuilt little by little.
[00:18:00.080 --> 00:18:02.560]   So we're exploring all of these cross sections
[00:18:02.560 --> 00:18:04.120]   of this hidden beauty.
[00:18:04.120 --> 00:18:07.160]   And in whatever dimensionality we exist,
[00:18:07.160 --> 00:18:09.120]   we are encompassing the lower dimensions.
[00:18:09.120 --> 00:18:11.200]   We are exploring the cross sections
[00:18:11.200 --> 00:18:12.320]   of the higher dimensions.
[00:18:12.320 --> 00:18:14.040]   That's why in our 3D reality,
[00:18:14.040 --> 00:18:16.080]   we can see these slices of time.
[00:18:16.080 --> 00:18:18.040]   If we could live in higher dimensions,
[00:18:18.040 --> 00:18:21.800]   above time, maybe we would see time as all at once, right?
[00:18:21.800 --> 00:18:23.760]   But we are exploring all of these cross sections
[00:18:23.760 --> 00:18:26.280]   and always doing these dimensionality reductions
[00:18:26.280 --> 00:18:27.840]   that we can do in many different ways
[00:18:27.840 --> 00:18:29.080]   with different strategies
[00:18:29.080 --> 00:18:31.080]   and even adding extra factors
[00:18:31.080 --> 00:18:33.000]   to communicate extra information.
[00:18:33.000 --> 00:18:35.880]   Extra information, but they are all quite similar
[00:18:35.880 --> 00:18:36.720]   to each other.
[00:18:37.280 --> 00:18:40.720]   And finally, in the creation of these representations,
[00:18:40.720 --> 00:18:43.560]   and we are now going to see this 10 minute video.
[00:18:43.560 --> 00:18:46.880]   So as I explained, this is a multidisciplinary project
[00:18:46.880 --> 00:18:50.080]   in which I am combining a lot of different things, right?
[00:18:50.080 --> 00:18:52.960]   That go from the engineering, the programming,
[00:18:52.960 --> 00:18:55.440]   the optimization, the modeling,
[00:18:55.440 --> 00:18:58.480]   all the, what is the 3D surfaces and the audio visual,
[00:18:58.480 --> 00:19:00.880]   et cetera, et cetera, all of the different parts.
[00:19:00.880 --> 00:19:04.120]   And now we're gonna see this 10 minute video with examples.
[00:19:04.120 --> 00:19:06.760]   So the first one is the learning rate stress test
[00:19:06.760 --> 00:19:09.000]   in which I am basically using
[00:19:09.000 --> 00:19:12.000]   different learning rate schedules
[00:19:12.000 --> 00:19:17.000]   to look, to basically study different resiliency ranges
[00:19:17.000 --> 00:19:20.640]   in which I modify the learning rates in very radical ways.
[00:19:20.640 --> 00:19:23.920]   And I study how the surfaces respond.
[00:19:23.920 --> 00:19:27.480]   And we can see how as you reach different parts
[00:19:27.480 --> 00:19:29.440]   of the high dimensionality,
[00:19:29.440 --> 00:19:30.720]   where the process collapses,
[00:19:30.760 --> 00:19:34.960]   this basically transforms into a similar collapse
[00:19:34.960 --> 00:19:37.480]   in the dimensionality or reduced representation.
[00:19:37.480 --> 00:19:41.280]   We can also see the process of tunneling in these examples,
[00:19:41.280 --> 00:19:44.240]   how the convexity begins to tunnel down
[00:19:44.240 --> 00:19:45.320]   the landscape as well.
[00:19:45.320 --> 00:19:49.960]   So we're also using here another learning rate schedule.
[00:19:49.960 --> 00:19:52.280]   I'm analyzing these resiliency ranges
[00:19:52.280 --> 00:19:55.760]   to see what are the ranges in which we can still take
[00:19:55.760 --> 00:19:59.200]   the learning, the training process to a good solution
[00:19:59.200 --> 00:20:02.640]   while still doing radical changes in the learning rate.
[00:20:02.640 --> 00:20:05.240]   This is now mode connectivity, a wonderful paper,
[00:20:05.240 --> 00:20:08.760]   a collaboration with my friends of NYU and MIT,
[00:20:08.760 --> 00:20:10.360]   Timur Garipov, Pavel Ismailov,
[00:20:10.360 --> 00:20:12.680]   then we have Dimitri Podoporokhin, Dimitri Vetrov
[00:20:12.680 --> 00:20:14.400]   and Andrew Gordon Wilson.
[00:20:14.400 --> 00:20:18.040]   And what these researchers are doing in mode connectivity,
[00:20:18.040 --> 00:20:21.760]   right, is they get to, for example, three solutions,
[00:20:21.760 --> 00:20:25.720]   three different minima with SGD,
[00:20:25.720 --> 00:20:28.280]   and three points make a plane
[00:20:28.280 --> 00:20:30.560]   and they take two of these points
[00:20:30.560 --> 00:20:32.040]   and they rotate this plane
[00:20:32.040 --> 00:20:33.880]   through all the high dimensionality
[00:20:33.880 --> 00:20:38.440]   until they find a path that links these two minima
[00:20:38.440 --> 00:20:41.440]   through maybe a Bezier curve or a polygonal path
[00:20:41.440 --> 00:20:44.560]   while maintaining a very low loss value.
[00:20:44.560 --> 00:20:47.800]   And what is really fascinating in the visualization
[00:20:47.800 --> 00:20:50.680]   is that we can see how the barrier that separates
[00:20:50.680 --> 00:20:52.920]   with a high loss value, the different minima,
[00:20:52.920 --> 00:20:55.440]   or it's like melts and disappears gradually
[00:20:55.440 --> 00:20:58.400]   as the training progresses, as the algorithm progresses
[00:20:58.400 --> 00:21:00.080]   until the minima are linked.
[00:21:00.080 --> 00:21:02.760]   And again, this doesn't mean that in the high dimensionality
[00:21:02.760 --> 00:21:05.920]   the barrier between the minima is disappearing.
[00:21:05.920 --> 00:21:07.880]   It means that as we move in these millions
[00:21:07.880 --> 00:21:10.680]   and billions of directions in the high dimensionality,
[00:21:10.680 --> 00:21:13.000]   as we dimensionality to reduce
[00:21:13.000 --> 00:21:14.800]   this to our flatland reality,
[00:21:14.800 --> 00:21:16.920]   the counterintuitive effect that we see
[00:21:16.920 --> 00:21:20.160]   is this barrier melting between the minima.
[00:21:20.160 --> 00:21:23.600]   And we see here, finally, the two minima connected
[00:21:23.600 --> 00:21:25.320]   and the straight connection
[00:21:25.320 --> 00:21:26.960]   still has a very high loss value,
[00:21:26.960 --> 00:21:28.920]   but the Bezier curve connection
[00:21:28.920 --> 00:21:33.040]   has this very low loss value of connection.
[00:21:33.040 --> 00:21:34.920]   Here we see a static representation
[00:21:34.920 --> 00:21:37.200]   in very high resolution where we see the contrast
[00:21:37.200 --> 00:21:40.640]   between the rough surface in the high loss value areas
[00:21:40.640 --> 00:21:43.600]   and the smoother part that connects the minima.
[00:21:43.600 --> 00:21:46.600]   This is a cenital view of the connection
[00:21:46.600 --> 00:21:48.200]   between the minima.
[00:21:48.200 --> 00:21:52.160]   This is also a capture of that dramatic contrast
[00:21:52.160 --> 00:21:56.240]   in the rough surface in the high loss value areas
[00:21:56.240 --> 00:21:59.480]   that are higher than the area that connects the minima.
[00:21:59.480 --> 00:22:02.920]   And this very, very dramatic contrast between both these,
[00:22:02.920 --> 00:22:06.760]   I call the cathedral, another dramatic representation
[00:22:06.760 --> 00:22:09.080]   of this contrast between both areas.
[00:22:09.080 --> 00:22:12.080]   I see that because of the connection,
[00:22:12.080 --> 00:22:14.720]   the playing of the video is struggling a little bit.
[00:22:14.720 --> 00:22:18.560]   Okay, so the studies in which I change different parameters
[00:22:18.560 --> 00:22:22.000]   and then I capture specific parts of the landscape
[00:22:22.000 --> 00:22:24.800]   to study them as I change different parameters,
[00:22:24.800 --> 00:22:26.720]   drop out, et cetera, et cetera.
[00:22:26.720 --> 00:22:31.320]   And I study various specific parts of the landscape,
[00:22:31.320 --> 00:22:32.360]   changing different parameters.
[00:22:32.360 --> 00:22:34.800]   This is an example, for example, with ResNets,
[00:22:34.800 --> 00:22:38.000]   as we study the morphology and the dynamics,
[00:22:38.000 --> 00:22:40.600]   for example, with non-skip connections,
[00:22:40.600 --> 00:22:42.080]   with skip connections,
[00:22:42.080 --> 00:22:46.160]   and analyzing all the changes in morphology and dynamics.
[00:22:46.160 --> 00:22:48.960]   This is the Lost Landscape Library Project.
[00:22:48.960 --> 00:22:53.480]   This is an ongoing project, different from the Explorer app.
[00:22:53.480 --> 00:22:57.040]   This is an ongoing project to be able to understand
[00:22:57.040 --> 00:23:01.720]   the impact on the surface of many different gradual changes
[00:23:01.720 --> 00:23:03.320]   in the parameters of the networks.
[00:23:03.320 --> 00:23:05.080]   This is a long-term project.
[00:23:05.080 --> 00:23:08.440]   I don't know when it will be finished.
[00:23:08.440 --> 00:23:10.680]   It is different to the Explorer app,
[00:23:10.680 --> 00:23:12.760]   and this is ongoing at the moment.
[00:23:12.760 --> 00:23:15.920]   And just, this is the Lost Landscape Library Project.
[00:23:15.920 --> 00:23:18.360]   And now we move to the collaborations
[00:23:18.360 --> 00:23:21.520]   with the Landscape Research Deep Learning Group,
[00:23:21.520 --> 00:23:23.160]   founded by Deganta Misra,
[00:23:23.160 --> 00:23:25.000]   that has now joined Weights and Biases.
[00:23:25.000 --> 00:23:26.360]   So this is very exciting.
[00:23:26.360 --> 00:23:28.640]   And we do, for example, here a collaboration
[00:23:28.640 --> 00:23:32.960]   to study the lost landscapes of activation functions,
[00:23:32.960 --> 00:23:35.640]   like MIS, SWISS, and RELU.
[00:23:35.640 --> 00:23:39.280]   Basically, in this study, right,
[00:23:39.280 --> 00:23:43.720]   we are analyzing the 200th epoch of the training process
[00:23:43.720 --> 00:23:45.960]   of a ResNet-20 network.
[00:23:45.960 --> 00:23:48.800]   And we are analyzing how well-conditioned
[00:23:48.800 --> 00:23:52.400]   are the surfaces, what is the quality of their minima,
[00:23:52.400 --> 00:23:54.720]   the flatness of their minima, et cetera, et cetera.
[00:23:54.720 --> 00:23:56.840]   And we are seeing that, for example,
[00:23:56.840 --> 00:23:59.320]   MIS has this very well-conditioned surface
[00:23:59.320 --> 00:24:01.920]   in comparison, for example, with RELU,
[00:24:01.920 --> 00:24:04.120]   and the quality of the minima,
[00:24:04.120 --> 00:24:07.360]   the flatness of the minima is really nice,
[00:24:07.360 --> 00:24:08.360]   et cetera, et cetera.
[00:24:08.360 --> 00:24:10.360]   And we do these types of collaborations
[00:24:10.360 --> 00:24:12.960]   that we're expanding in different directions.
[00:24:12.960 --> 00:24:13.800]   All right.
[00:24:15.440 --> 00:24:18.280]   Lottery Garden, this is regarding the paper
[00:24:18.280 --> 00:24:19.800]   by Jonathan Frankel and Michael Carvin,
[00:24:19.800 --> 00:24:21.240]   the Lottery Ticket Hypothesis.
[00:24:21.240 --> 00:24:24.000]   And I am pruning gradually the network,
[00:24:24.000 --> 00:24:26.040]   pruning the smallest weights first.
[00:24:26.040 --> 00:24:28.840]   And as I prune the networks throughout six epochs,
[00:24:28.840 --> 00:24:30.480]   I represent the lost landscapes.
[00:24:30.480 --> 00:24:31.440]   And it's really fascinating,
[00:24:31.440 --> 00:24:34.360]   but because we can see that up to 40, 50%,
[00:24:34.360 --> 00:24:37.200]   the performance can even exceed the performance
[00:24:37.200 --> 00:24:38.560]   of the fully trained network.
[00:24:38.560 --> 00:24:40.520]   And the, digamos, the landscapes
[00:24:40.520 --> 00:24:42.520]   are really, really well-conditioned.
[00:24:42.520 --> 00:24:45.120]   And the only one we get to 80 or 90%,
[00:24:45.120 --> 00:24:46.920]   the process begins to collapse.
[00:24:46.920 --> 00:24:48.920]   This is when they break through these planes,
[00:24:48.920 --> 00:24:50.760]   they are going beyond the performance
[00:24:50.760 --> 00:24:52.840]   of the fully trained network.
[00:24:52.840 --> 00:24:55.040]   And we can see therefore that this,
[00:24:55.040 --> 00:24:56.800]   indeed there is a subset of the weights
[00:24:56.800 --> 00:25:00.120]   that are responsible for the most of the performance
[00:25:00.120 --> 00:25:01.640]   of the fully trained network.
[00:25:01.640 --> 00:25:04.800]   And we can see that the landscape is really well-behaved
[00:25:04.800 --> 00:25:07.640]   until we reach the 80s or the 90%.
[00:25:07.640 --> 00:25:11.120]   And here we can see 20% how we exceed the performance
[00:25:11.120 --> 00:25:13.040]   of the fully trained network sometimes
[00:25:13.040 --> 00:25:14.920]   throughout the six epochs, right?
[00:25:14.920 --> 00:25:17.960]   And as we approach 60, 70%,
[00:25:17.960 --> 00:25:20.320]   we see that this tunneling process
[00:25:20.320 --> 00:25:23.840]   begins to be unable to reach a slow
[00:25:23.840 --> 00:25:26.040]   until eventually we get to a part
[00:25:26.040 --> 00:25:27.880]   in the high dimensional space
[00:25:27.880 --> 00:25:31.360]   that collapses completely the transformation.
[00:25:31.360 --> 00:25:34.440]   Edge horizon and downfall, as I do these experiments,
[00:25:34.440 --> 00:25:37.120]   I give names to different parts that I'm interested in.
[00:25:37.120 --> 00:25:38.760]   So I call the edge horizon,
[00:25:38.760 --> 00:25:41.600]   the transition to the main convexity of the landscape
[00:25:41.600 --> 00:25:43.120]   and the downfall,
[00:25:43.120 --> 00:25:45.560]   the transition from the edge horizon
[00:25:45.560 --> 00:25:47.000]   to the bottom of the minima.
[00:25:47.000 --> 00:25:49.960]   And of course we have the minima, et cetera, et cetera.
[00:25:49.960 --> 00:25:51.360]   And when I do these studies,
[00:25:51.360 --> 00:25:52.400]   this is interesting paper
[00:25:52.400 --> 00:25:53.560]   because it tells us, for example,
[00:25:53.560 --> 00:25:55.440]   that we used to think that batch norm
[00:25:55.440 --> 00:25:58.200]   was correlated with a smoother surface,
[00:25:58.200 --> 00:25:59.040]   but then they say,
[00:25:59.040 --> 00:26:00.520]   "Oh, but when we do a different combination
[00:26:00.520 --> 00:26:02.040]   of for example, of depth, et cetera,
[00:26:02.040 --> 00:26:03.040]   we see something different."
[00:26:03.040 --> 00:26:04.640]   And this is the same thing that I have found
[00:26:04.640 --> 00:26:06.200]   in many of my experiments.
[00:26:06.200 --> 00:26:07.840]   It takes a lot of time
[00:26:07.840 --> 00:26:10.480]   because sometimes you get some correlation,
[00:26:10.480 --> 00:26:12.920]   but then with a different combination of parameters,
[00:26:12.920 --> 00:26:14.800]   it goes to a different direction, et cetera.
[00:26:14.800 --> 00:26:18.400]   Okay, this is a very high resolution representation
[00:26:18.400 --> 00:26:19.680]   of an edge horizon,
[00:26:19.680 --> 00:26:22.320]   the approach to an edge horizon
[00:26:22.320 --> 00:26:25.560]   and using also the creative aspect of the project
[00:26:25.560 --> 00:26:29.960]   to make it easier to understand the morphology.
[00:26:29.960 --> 00:26:34.960]   This is a small, sorry, a slow process of tunneling.
[00:26:34.960 --> 00:26:40.720]   Also, that ball represents the minimizer.
[00:26:41.160 --> 00:26:44.120]   This is a transition from the top of an edge horizon
[00:26:44.120 --> 00:26:46.360]   to the bottom below the minima
[00:26:46.360 --> 00:26:48.800]   to study the minima from below.
[00:26:48.800 --> 00:26:51.800]   It's a pity that because of the connection,
[00:26:51.800 --> 00:26:53.760]   I think it's because I'm also recording it,
[00:26:53.760 --> 00:26:55.160]   it's struggling the video,
[00:26:55.160 --> 00:26:58.160]   but we can mount this afterwards.
[00:26:58.160 --> 00:27:02.280]   Okay, this is also a flyby on top of an edge horizon.
[00:27:02.280 --> 00:27:03.280]   And this is one of my favorites.
[00:27:03.280 --> 00:27:05.640]   This is one of the first ones I created.
[00:27:05.640 --> 00:27:07.360]   This is also the beginning
[00:27:07.360 --> 00:27:09.240]   of one of these tunneling processes.
[00:27:09.240 --> 00:27:10.160]   It's one of my favorites
[00:27:10.160 --> 00:27:13.240]   because it's also one of the first ones I produced.
[00:27:13.240 --> 00:27:15.920]   All right, and now we are moving.
[00:27:15.920 --> 00:27:18.160]   Okay, this is another static representation
[00:27:18.160 --> 00:27:19.000]   of an edge horizon.
[00:27:19.000 --> 00:27:22.080]   This is a high resolution capture of a downfall.
[00:27:22.080 --> 00:27:24.160]   And we have other representations
[00:27:24.160 --> 00:27:27.080]   of edge horizons as well in here.
[00:27:27.080 --> 00:27:28.120]   All right, where are we going now?
[00:27:28.120 --> 00:27:28.960]   Dropout.
[00:27:28.960 --> 00:27:31.400]   As we begin to add during the training process,
[00:27:31.400 --> 00:27:33.640]   of course, a dropout to the networks,
[00:27:33.640 --> 00:27:35.560]   this homogeneous layer of noise
[00:27:35.560 --> 00:27:37.240]   begins to take over the landscape.
[00:27:37.240 --> 00:27:38.680]   And of course, this is a layer of noise
[00:27:38.680 --> 00:27:42.120]   that is disruptive enough to prevent too much overfitting
[00:27:42.120 --> 00:27:44.640]   and too much memorization of the paths
[00:27:44.640 --> 00:27:45.800]   throughout weight space,
[00:27:45.800 --> 00:27:48.520]   but not disruptive enough to prevent the network
[00:27:48.520 --> 00:27:50.240]   from reaching a good solution,
[00:27:50.240 --> 00:27:53.560]   unless, of course, unless we go too crazy with the dropout.
[00:27:53.560 --> 00:27:57.480]   And then of course, we will make impossible
[00:27:57.480 --> 00:27:59.720]   the movements in weight space.
[00:27:59.720 --> 00:28:02.600]   So yeah, and we see here the representation in movement
[00:28:02.600 --> 00:28:04.400]   as we crank up the dropout
[00:28:04.400 --> 00:28:07.320]   and this layer of noise begins
[00:28:07.320 --> 00:28:09.200]   to take over the landscape.
[00:28:09.200 --> 00:28:10.280]   Fascinating.
[00:28:10.280 --> 00:28:11.880]   So that is the dropout.
[00:28:11.880 --> 00:28:14.240]   And now we're gonna move to Bayesian deep learning,
[00:28:14.240 --> 00:28:15.320]   dealing with uncertainty
[00:28:15.320 --> 00:28:17.480]   so that we don't do overconfident predictions.
[00:28:17.480 --> 00:28:19.880]   And this is the group of researchers
[00:28:19.880 --> 00:28:21.920]   with Wesley Maddox, Timur Garipov,
[00:28:21.920 --> 00:28:24.480]   Pablo Ismailov, Dimitri Vetrov, and Andrew Gordon-Wilson.
[00:28:24.480 --> 00:28:27.560]   And of course, they want to build a probability distribution
[00:28:27.560 --> 00:28:28.720]   of the weights of the network
[00:28:28.720 --> 00:28:30.320]   that they call the posterior, right?
[00:28:30.320 --> 00:28:31.440]   But this is intractable,
[00:28:31.440 --> 00:28:34.880]   so they approximate it through the trajectory of SGD.
[00:28:34.880 --> 00:28:37.520]   So what they do is they get to a good solution in SGD
[00:28:37.520 --> 00:28:40.000]   and then they put a high learning rate
[00:28:40.000 --> 00:28:41.760]   and they explore all around
[00:28:41.760 --> 00:28:43.560]   and they get to different solutions
[00:28:43.560 --> 00:28:45.000]   that explain the training data,
[00:28:45.000 --> 00:28:47.040]   but do different predictions on the test data.
[00:28:47.040 --> 00:28:49.200]   And they build a Gaussian distribution with those
[00:28:49.200 --> 00:28:51.240]   and they demonstrate that that Gaussian distribution
[00:28:51.240 --> 00:28:53.640]   explains the geometry of the loss surface well.
[00:28:53.640 --> 00:28:54.720]   And in this collaboration,
[00:28:54.720 --> 00:28:57.680]   we are representing the actual proper loss landscape
[00:28:57.680 --> 00:28:59.280]   built with the real data
[00:28:59.280 --> 00:29:01.840]   and the exact position of the solutions
[00:29:01.840 --> 00:29:04.720]   and also of the Gaussian distribution as well.
[00:29:04.720 --> 00:29:09.440]   And this was presented in NeurIPS 2019.
[00:29:09.440 --> 00:29:12.800]   The previous paper was of NeurIPS 2018.
[00:29:12.800 --> 00:29:16.600]   Okay, and now we go to,
[00:29:16.600 --> 00:29:18.560]   I think we're going to go to the GANs.
[00:29:18.560 --> 00:29:21.160]   Yes, the wild networks.
[00:29:21.160 --> 00:29:23.360]   So we all know what is the problem with the GANs,
[00:29:23.360 --> 00:29:26.960]   that is that the loss function does not correlate well
[00:29:26.960 --> 00:29:28.760]   with the performance, for example,
[00:29:28.760 --> 00:29:33.680]   of the generator or the quality of the images
[00:29:33.680 --> 00:29:34.600]   that it produces.
[00:29:34.600 --> 00:29:36.400]   But if we use a different type of loss function,
[00:29:36.400 --> 00:29:38.000]   like for example, the Wasserstein,
[00:29:38.000 --> 00:29:38.880]   a different type of GAN,
[00:29:38.880 --> 00:29:39.720]   the Wasserstein GAN
[00:29:39.720 --> 00:29:41.560]   with a different type of loss function
[00:29:41.560 --> 00:29:43.240]   that uses the Wasserstein distance,
[00:29:43.240 --> 00:29:45.640]   the shortest distance to move a probability mass
[00:29:45.640 --> 00:29:47.120]   from a distribution to another,
[00:29:47.120 --> 00:29:49.080]   then just that type of loss function
[00:29:49.080 --> 00:29:51.680]   correlates much better with the performance
[00:29:51.680 --> 00:29:53.200]   of the generator.
[00:29:53.200 --> 00:29:56.200]   And then we can study better the dynamics
[00:29:56.200 --> 00:29:59.280]   of the loss surface of a generator.
[00:29:59.280 --> 00:30:01.640]   And we can see this, you know,
[00:30:01.640 --> 00:30:03.480]   more unpredictable dynamics.
[00:30:03.480 --> 00:30:05.400]   We can really see that these networks
[00:30:05.400 --> 00:30:07.920]   are so much harder to control,
[00:30:07.920 --> 00:30:11.840]   so much harder to tame as we study the dynamics,
[00:30:11.840 --> 00:30:15.040]   for example, of the surfaces of the generator.
[00:30:15.040 --> 00:30:17.400]   And this is a project with Neural Concept,
[00:30:17.400 --> 00:30:20.040]   Swiss company, about geometric deep learning.
[00:30:20.040 --> 00:30:21.120]   This is not loss landscape,
[00:30:21.120 --> 00:30:22.560]   it's just to show you that the project
[00:30:22.560 --> 00:30:25.120]   is also branching in different directions.
[00:30:25.120 --> 00:30:27.520]   And this is basically predicting
[00:30:27.520 --> 00:30:30.240]   the aerodynamic properties of a drone,
[00:30:30.240 --> 00:30:32.560]   the pressure exerted by the air
[00:30:32.560 --> 00:30:34.040]   on the surface of the drone.
[00:30:34.040 --> 00:30:37.640]   And we can also visualize the features
[00:30:37.640 --> 00:30:39.680]   being learned by the filters,
[00:30:39.680 --> 00:30:41.520]   which in a normal ConvNet,
[00:30:41.520 --> 00:30:43.960]   we would visualize with a 2D image,
[00:30:43.960 --> 00:30:46.840]   but in a geometric 3D ConvNet,
[00:30:46.840 --> 00:30:49.000]   we can map on top of the surface.
[00:30:49.000 --> 00:30:50.640]   And finally, to begin this,
[00:30:50.640 --> 00:30:52.520]   sorry, to end this presentation,
[00:30:52.520 --> 00:30:55.240]   this is the Lost Landscape Explorer app
[00:30:55.240 --> 00:30:57.160]   that I just launched a few days ago.
[00:30:57.160 --> 00:31:00.200]   And this is an app where you can explore
[00:31:00.200 --> 00:31:02.120]   different lost landscapes created
[00:31:02.120 --> 00:31:04.600]   with this dimensionality reduced techniques
[00:31:04.600 --> 00:31:05.760]   and with real data,
[00:31:05.760 --> 00:31:08.840]   and you can navigate them in 3D.
[00:31:08.840 --> 00:31:10.160]   And you have different features,
[00:31:10.160 --> 00:31:12.400]   like for example, this auto descent mode,
[00:31:12.400 --> 00:31:14.080]   you can click anywhere on the landscape
[00:31:14.080 --> 00:31:17.680]   to begin a sort of descent through the gradients
[00:31:17.680 --> 00:31:19.800]   or the sub gradients as you like.
[00:31:20.040 --> 00:31:22.760]   And of course, this is not doing a gradient descent
[00:31:22.760 --> 00:31:24.480]   in the high dimensional space, all right?
[00:31:24.480 --> 00:31:25.560]   This is a gradient descent
[00:31:25.560 --> 00:31:28.400]   through the dimensionality reduced representation,
[00:31:28.400 --> 00:31:31.120]   but it can be useful for educational purposes
[00:31:31.120 --> 00:31:34.640]   and also to reflect on different research aspects.
[00:31:34.640 --> 00:31:36.360]   For example, it's very fun that you can,
[00:31:36.360 --> 00:31:37.760]   you know, you can do one of these descents
[00:31:37.760 --> 00:31:39.760]   and you can see that it gets trapped
[00:31:39.760 --> 00:31:42.240]   in some part of the landscape in a local minima.
[00:31:42.240 --> 00:31:45.280]   And then you have a descent rate control
[00:31:45.280 --> 00:31:47.280]   that you can manipulate in real time.
[00:31:47.280 --> 00:31:50.320]   And you can crank it up and help it escape
[00:31:50.320 --> 00:31:52.880]   that local minima and continue moving down,
[00:31:52.880 --> 00:31:55.160]   or you can get to a good minima.
[00:31:55.160 --> 00:31:57.600]   And then you can, you know,
[00:31:57.600 --> 00:31:59.680]   the gradient gets almost zero,
[00:31:59.680 --> 00:32:01.880]   but then you crank up the learning rate
[00:32:01.880 --> 00:32:03.240]   at the bottom of the minima,
[00:32:03.240 --> 00:32:04.800]   and then you explore the minima
[00:32:04.800 --> 00:32:06.400]   and get to other solutions,
[00:32:06.400 --> 00:32:09.560]   which is what the Bayesian friends are doing, for example.
[00:32:09.560 --> 00:32:12.080]   So, you know, you can reflect on different research aspects
[00:32:12.080 --> 00:32:13.840]   and use it for educational purposes.
[00:32:13.840 --> 00:32:17.560]   And I will be adding more features over time to this tool.
[00:32:17.560 --> 00:32:21.560]   And you can also capture different views of the landscapes,
[00:32:21.560 --> 00:32:24.200]   you know, in PNGs, et cetera, et cetera,
[00:32:24.200 --> 00:32:26.120]   there are other features.
[00:32:26.120 --> 00:32:29.840]   And this can be accessed for free, completely open,
[00:32:29.840 --> 00:32:31.640]   and you don't have to install anything.
[00:32:31.640 --> 00:32:33.800]   This, I programmed this in JavaScript.
[00:32:33.800 --> 00:32:35.160]   This is using React.
[00:32:35.160 --> 00:32:37.520]   This is using WebGL technology,
[00:32:37.520 --> 00:32:39.240]   and it can be installed.
[00:32:39.240 --> 00:32:40.680]   This is a progressive web app,
[00:32:40.680 --> 00:32:41.880]   so it can be installed anywhere.
[00:32:41.880 --> 00:32:44.680]   It doesn't require logging or register or anything.
[00:32:44.680 --> 00:32:46.800]   And I will be adding more features to it over time,
[00:32:46.800 --> 00:32:51.800]   and this can be accessed at lostlandscape.com/explorer.
[00:32:51.800 --> 00:32:54.600]   And that's it, that concludes the presentation.
[00:32:54.600 --> 00:32:57.000]   And if you want to explore more of my work,
[00:32:57.000 --> 00:33:01.040]   you can go to ideami.com, and that's it.
[00:33:01.040 --> 00:33:01.960]   Thank you very much.
[00:33:01.960 --> 00:33:06.160]   And now we can open time for questions and dialogue.
[00:33:06.160 --> 00:33:07.000]   Let's go for it.
[00:33:07.000 --> 00:33:11.840]   - Great, yeah, thanks, Javier,
[00:33:11.840 --> 00:33:16.440]   for the, like, yeah, such a wide array of experiments
[00:33:16.440 --> 00:33:18.160]   that you've done using these tools
[00:33:18.160 --> 00:33:21.040]   to understand lost landscapes better.
[00:33:21.040 --> 00:33:24.320]   It's really just, yeah, the breadth is really impressive.
[00:33:24.320 --> 00:33:26.480]   - Right.
[00:33:26.480 --> 00:33:28.320]   - So I wanted to, like, dive in a little bit
[00:33:28.320 --> 00:33:30.960]   and hear maybe a little bit more about some of those,
[00:33:30.960 --> 00:33:31.960]   like, some of those experiments
[00:33:31.960 --> 00:33:34.560]   and what you feel like you learned from them.
[00:33:34.560 --> 00:33:35.520]   - Yes.
[00:33:35.520 --> 00:33:39.840]   - So one example, one thing you talked about was,
[00:33:39.840 --> 00:33:41.960]   you talked about using LR schedulers,
[00:33:41.960 --> 00:33:46.520]   and, like, so changing the learning rate as you were going
[00:33:46.520 --> 00:33:49.640]   and how that affected the, like,
[00:33:49.640 --> 00:33:51.680]   the sorts of places on the lost landscape
[00:33:51.680 --> 00:33:53.440]   that the optimization ended up.
[00:33:53.440 --> 00:33:58.000]   So, like, my view of LR schedulers is that, you know,
[00:33:58.000 --> 00:33:59.240]   they're kind of there to, like,
[00:33:59.240 --> 00:34:02.240]   let you optimize for a while and then kick you out
[00:34:02.240 --> 00:34:03.960]   of where you've been optimized to
[00:34:03.960 --> 00:34:06.600]   and let you find another, you know, another place to go.
[00:34:06.600 --> 00:34:08.920]   Is that something that you see in the lost landscapes,
[00:34:08.920 --> 00:34:12.080]   or is there a different intuition about these LR schedulers
[00:34:12.080 --> 00:34:14.880]   that you gain from using them with this tool?
[00:34:14.880 --> 00:34:16.680]   - Yeah, I mean, that point that you talk about,
[00:34:16.680 --> 00:34:18.000]   I absolutely agree.
[00:34:18.000 --> 00:34:22.760]   I think, you know, what I mainly see is that,
[00:34:22.760 --> 00:34:24.400]   it's again, the resiliency,
[00:34:24.400 --> 00:34:27.360]   this unreasonable effectiveness of deep learning, right?
[00:34:27.360 --> 00:34:31.440]   I mean, the resiliency that the training process has
[00:34:31.440 --> 00:34:32.960]   when you try, you know,
[00:34:32.960 --> 00:34:35.240]   when you try different resiliency ranges,
[00:34:35.240 --> 00:34:37.640]   you can see that the training process
[00:34:37.640 --> 00:34:41.560]   in these deep learning neural networks is really resilient.
[00:34:41.560 --> 00:34:46.560]   It can really recover many times from movements produced
[00:34:46.560 --> 00:34:51.920]   by these, you know, learning rates schedules
[00:34:51.920 --> 00:34:54.440]   that may take you to some, you know,
[00:34:54.440 --> 00:34:56.440]   they may interfere a bit with the training process,
[00:34:56.440 --> 00:34:58.600]   but eventually you can return to a path
[00:34:58.600 --> 00:35:01.040]   that takes you to a good minima and a good solution.
[00:35:01.040 --> 00:35:03.360]   So, this is the main thing that for me connects again
[00:35:03.360 --> 00:35:05.400]   with the blessings of dimensionality,
[00:35:05.400 --> 00:35:09.640]   that there seems to be a lot of resilience in the process,
[00:35:09.640 --> 00:35:12.440]   that even when you try to push, you know,
[00:35:12.440 --> 00:35:15.640]   the learning rate in crazy ways, of course, yes,
[00:35:15.640 --> 00:35:18.480]   you can, I mean, you can break the process easily
[00:35:18.480 --> 00:35:21.040]   if you do that, but there is, you know,
[00:35:21.040 --> 00:35:24.400]   there is kind of a margin of error
[00:35:24.400 --> 00:35:27.040]   that in my opinion is larger
[00:35:27.040 --> 00:35:30.640]   than it would be, you know, reasonable to assume,
[00:35:30.640 --> 00:35:32.280]   you know, and this is a little bit what I find.
[00:35:32.280 --> 00:35:33.400]   Of course, if you wanna break it down,
[00:35:33.400 --> 00:35:34.680]   you can break it down very easily,
[00:35:34.680 --> 00:35:37.400]   but there is still basically my intuition
[00:35:37.400 --> 00:35:38.840]   that I get from this exploration
[00:35:38.840 --> 00:35:40.880]   is that the margin of error, again, that we have
[00:35:40.880 --> 00:35:43.760]   is really wider than one would expect, yeah.
[00:35:43.760 --> 00:35:46.360]   - That's an interesting perspective.
[00:35:46.360 --> 00:35:48.480]   Yeah, it seems it's, so I've,
[00:35:48.480 --> 00:35:50.040]   some of the work that I've done involved
[00:35:50.040 --> 00:35:52.200]   second order optimizers applied to neural networks.
[00:35:52.200 --> 00:35:53.120]   So, building- - Yes.
[00:35:53.120 --> 00:35:55.560]   - SCNs, trying to invert them and use that information.
[00:35:55.560 --> 00:35:58.080]   And I found like in that instance,
[00:35:58.080 --> 00:36:00.720]   those optimizers tend not to be very robust,
[00:36:00.720 --> 00:36:03.920]   either to like, you know- - Interesting.
[00:36:03.920 --> 00:36:07.440]   - Noise changes in the loss surface and also bugs.
[00:36:07.440 --> 00:36:10.240]   So, if you implement gradient descent in a buggy way,
[00:36:10.240 --> 00:36:12.360]   it actually tends to still work decently well.
[00:36:12.360 --> 00:36:14.600]   If you implement one of these second order optimizers
[00:36:14.600 --> 00:36:16.640]   in a buggy way, it tends to do,
[00:36:16.640 --> 00:36:19.800]   it tends to be wet hot garbage instead.
[00:36:19.800 --> 00:36:22.760]   - Well, I mean, that's one of the things
[00:36:22.760 --> 00:36:26.320]   that a lot of people say about gradient descent, right?
[00:36:26.320 --> 00:36:30.160]   That is very slow, but it's kind of the thing that,
[00:36:30.160 --> 00:36:33.320]   you know, it's really trustworthy in that sense, right?
[00:36:34.320 --> 00:36:36.960]   - Yeah, definitely.
[00:36:36.960 --> 00:36:39.240]   And, you know, I mean, it's,
[00:36:39.240 --> 00:36:41.960]   you can get faster things than gradient descent
[00:36:41.960 --> 00:36:44.840]   in a lot of like specific example problems,
[00:36:44.840 --> 00:36:48.360]   but it's really hard to beat on neural networks.
[00:36:48.360 --> 00:36:50.080]   - Exactly, exactly.
[00:36:50.080 --> 00:36:54.400]   - Yeah, I guess another question,
[00:36:54.400 --> 00:36:56.840]   sort of like, so we normally see these like
[00:36:56.840 --> 00:36:58.840]   one dimensional slices through the loss surface
[00:36:58.840 --> 00:37:02.000]   that are just, we calculate the loss on each batch
[00:37:02.000 --> 00:37:04.440]   as we're going and we get a single value
[00:37:04.440 --> 00:37:05.640]   as we go through these,
[00:37:05.640 --> 00:37:10.840]   yeah, as we like load batches,
[00:37:10.840 --> 00:37:12.760]   calculate loss, update our gradients.
[00:37:12.760 --> 00:37:15.120]   And you pretty much always see something
[00:37:15.120 --> 00:37:16.920]   that just looks like this nice,
[00:37:16.920 --> 00:37:21.320]   like this nice curve, the exact curve you would expect
[00:37:21.320 --> 00:37:24.000]   from doing say gradient descent on a,
[00:37:24.000 --> 00:37:27.840]   like a perfect bowl, just like a quadratic loss surface.
[00:37:27.840 --> 00:37:31.040]   So what do you think is going on there?
[00:37:31.040 --> 00:37:34.040]   Like why, you know, 'cause it seems like
[00:37:34.040 --> 00:37:37.160]   there's a lot of richness in the landscapes
[00:37:37.160 --> 00:37:40.120]   that you see in the loss landscapes project,
[00:37:40.120 --> 00:37:45.120]   but then you don't see that in the one dimensional slice
[00:37:45.120 --> 00:37:46.600]   that we get during training.
[00:37:46.600 --> 00:37:50.480]   - Well, I mean, it really depends.
[00:37:50.480 --> 00:37:54.480]   I mean, as for example, Tom Goldstein's team's papers
[00:37:54.480 --> 00:37:58.400]   have shown all this richness of non-convexities
[00:37:58.400 --> 00:38:01.000]   and convexities depend a lot on how you design
[00:38:01.000 --> 00:38:01.840]   the network.
[00:38:01.840 --> 00:38:05.440]   I mean, if you have an architecture that is of course,
[00:38:05.440 --> 00:38:07.600]   you know, very optimized with the skip connections,
[00:38:07.600 --> 00:38:10.000]   like in a ResNet, you get this perfect bowl,
[00:38:10.000 --> 00:38:13.480]   like massively smooth, but as you begin to play
[00:38:13.480 --> 00:38:16.280]   with different parameters and you get to more challenging
[00:38:16.280 --> 00:38:18.680]   configurations, then you get, you know,
[00:38:18.680 --> 00:38:20.480]   way more richness in the landscape.
[00:38:20.480 --> 00:38:23.120]   Nowhere more, way more richness of distributions
[00:38:23.120 --> 00:38:25.120]   of non-convexities and convexities.
[00:38:25.120 --> 00:38:28.040]   And for example, when we did, you know,
[00:38:28.040 --> 00:38:33.040]   the collaboration with in the most connectivity paper,
[00:38:33.040 --> 00:38:35.880]   at the beginning, this is also another interesting thing.
[00:38:35.880 --> 00:38:40.440]   At the beginning, we did the dimensionality reduction
[00:38:40.440 --> 00:38:44.440]   to show the two minima and we were looking
[00:38:44.440 --> 00:38:47.520]   on a certain range around the two minima
[00:38:47.520 --> 00:38:52.520]   and we could just see, let's say, what is the two,
[00:38:52.520 --> 00:38:55.360]   you know, bowls that go down to the two minima
[00:38:55.360 --> 00:38:58.080]   and you know, pretty smooth and pretty well behaved.
[00:38:58.080 --> 00:39:01.240]   And then we said, let's look further away.
[00:39:01.240 --> 00:39:05.040]   So we began to expand the exploration
[00:39:05.040 --> 00:39:08.840]   and we began to do the calculations on a larger range.
[00:39:08.840 --> 00:39:11.280]   And as we started to move on a larger range,
[00:39:11.280 --> 00:39:14.440]   we begin to find this area that is around
[00:39:14.440 --> 00:39:17.680]   that has a higher loss value and is really, really,
[00:39:17.680 --> 00:39:20.880]   really rough and full of all of these places
[00:39:20.880 --> 00:39:22.080]   where you can get trapped.
[00:39:22.120 --> 00:39:24.640]   So many times it's also, Charles,
[00:39:24.640 --> 00:39:27.240]   about the range that you're exploring, okay?
[00:39:27.240 --> 00:39:28.920]   I mean, it has to do, again, it has to do
[00:39:28.920 --> 00:39:31.080]   with the type of slice and directions
[00:39:31.080 --> 00:39:32.920]   and also with the range that you're exploring.
[00:39:32.920 --> 00:39:36.720]   As I said during the talk, if you use PCA directions
[00:39:36.720 --> 00:39:39.160]   and a lot of people are, you know,
[00:39:39.160 --> 00:39:41.720]   creating these representations with PCA directions,
[00:39:41.720 --> 00:39:44.960]   then you are typically going to see just the bowl
[00:39:44.960 --> 00:39:47.960]   and very well behaved because you are just slicing
[00:39:47.960 --> 00:39:51.200]   through the most optimized directions down the gradient
[00:39:51.200 --> 00:39:53.000]   and you're just gonna see that, you know,
[00:39:53.000 --> 00:39:55.720]   very, very bowl, very optimized bowl of the landscape.
[00:39:55.720 --> 00:39:57.320]   And that, you know, a lot of people
[00:39:57.320 --> 00:39:58.680]   are doing representations like that
[00:39:58.680 --> 00:39:59.640]   and that's what they see.
[00:39:59.640 --> 00:40:00.960]   So that's one part of it.
[00:40:00.960 --> 00:40:03.440]   But the other part of it is range, okay?
[00:40:03.440 --> 00:40:06.200]   It depends then when you build a representation,
[00:40:06.200 --> 00:40:09.800]   how much range in magnitude are you calculating
[00:40:09.800 --> 00:40:12.360]   to visualize around the starting point?
[00:40:12.360 --> 00:40:15.200]   And as you move further away, as we did
[00:40:15.200 --> 00:40:17.920]   in the mode connectivity paper collaboration,
[00:40:17.920 --> 00:40:22.480]   you begin to see rougher areas that many times surround
[00:40:22.480 --> 00:40:25.400]   the central area of convexity, yeah.
[00:40:25.400 --> 00:40:28.920]   - And actually, technical clarifying points.
[00:40:28.920 --> 00:40:31.920]   When you make these lost landscape visualizations,
[00:40:31.920 --> 00:40:34.760]   how many data examples are you using?
[00:40:34.760 --> 00:40:38.520]   Like, is it more like on the order of a single batch
[00:40:38.520 --> 00:40:41.040]   or is it more like on the order of most of the data set
[00:40:41.040 --> 00:40:42.440]   or all of the data set?
[00:40:42.440 --> 00:40:44.120]   - Yeah, that is a great question.
[00:40:44.120 --> 00:40:44.960]   That is a great question.
[00:40:44.960 --> 00:40:45.800]   So it depends.
[00:40:45.800 --> 00:40:46.640]   It depends.
[00:40:46.640 --> 00:40:47.600]   Sometimes it's all of the data.
[00:40:47.600 --> 00:40:49.120]   Sometimes it's a subset.
[00:40:49.120 --> 00:40:52.120]   So, you know, researchers have demonstrated
[00:40:52.120 --> 00:40:57.120]   that if you use a decent percentage of the whole data,
[00:40:57.120 --> 00:40:59.200]   the results are gonna be very similar.
[00:40:59.200 --> 00:41:01.240]   So sometimes it's the whole data
[00:41:01.240 --> 00:41:04.040]   and sometimes it's a subset, but it cannot be, of course,
[00:41:04.040 --> 00:41:05.640]   a very small part of the data.
[00:41:05.640 --> 00:41:07.640]   It has to be a decent subset.
[00:41:07.640 --> 00:41:08.480]   - I see.
[00:41:08.480 --> 00:41:11.640]   So the roughness you're seeing isn't roughness
[00:41:11.640 --> 00:41:14.040]   you would expect to vary batch to batch, right?
[00:41:14.040 --> 00:41:16.480]   That is like, you're looking at something
[00:41:16.480 --> 00:41:18.760]   that's pretty close to what the average value
[00:41:18.760 --> 00:41:21.080]   would be across batches.
[00:41:21.080 --> 00:41:25.760]   - I mean, I don't think it's so much related to that.
[00:41:25.760 --> 00:41:27.960]   Again, I mean, the roughness that I'm seeing,
[00:41:27.960 --> 00:41:29.160]   I mean, the roughness that I'm seeing,
[00:41:29.160 --> 00:41:31.360]   like we see again in these, you know,
[00:41:31.360 --> 00:41:34.880]   areas that surrounded to minima in that example,
[00:41:34.880 --> 00:41:39.440]   for example, it's just a property of, you know,
[00:41:39.440 --> 00:41:42.680]   the other areas of the weight space
[00:41:42.680 --> 00:41:45.280]   that take you to higher loss values.
[00:41:45.280 --> 00:41:46.560]   So this is the roughness.
[00:41:46.560 --> 00:41:50.800]   For example, the variation in the dynamics
[00:41:50.800 --> 00:41:54.440]   that produce variations in the surface of the landscape,
[00:41:54.440 --> 00:41:57.240]   those, for example, are correlated with the batch sizes.
[00:41:57.240 --> 00:41:58.920]   If you are doing these calculations,
[00:41:58.920 --> 00:42:01.040]   because wait a minute, just to clarify,
[00:42:01.040 --> 00:42:03.000]   we have to differentiate two things here.
[00:42:03.000 --> 00:42:07.600]   One thing is as we capture the data,
[00:42:07.600 --> 00:42:10.840]   what is the batch size we're capturing through the dynamics?
[00:42:10.840 --> 00:42:12.920]   And another thing is as we're calculating
[00:42:12.920 --> 00:42:15.320]   the landscape itself, how much percentage
[00:42:15.320 --> 00:42:16.760]   of the data we're using, okay?
[00:42:16.760 --> 00:42:18.680]   Because for example, in the dynamics,
[00:42:18.680 --> 00:42:20.880]   if we use a very small batch size,
[00:42:20.880 --> 00:42:23.200]   then we're gonna be using a different proportion
[00:42:23.200 --> 00:42:25.040]   of the data in different steps.
[00:42:25.040 --> 00:42:28.320]   And this is gonna produce also more dramatic variations
[00:42:28.320 --> 00:42:29.400]   in the dynamics, okay?
[00:42:29.400 --> 00:42:31.680]   So both of these are different aspects, yeah.
[00:42:31.680 --> 00:42:33.840]   - Right, right.
[00:42:33.840 --> 00:42:35.720]   I guess my mental model a little bit
[00:42:35.720 --> 00:42:37.640]   when I was looking at some of those rough landscapes
[00:42:37.640 --> 00:42:39.880]   was that that was, oh, that's what I would see
[00:42:39.880 --> 00:42:42.680]   on a single, on, you know, one batch at a time,
[00:42:42.680 --> 00:42:45.200]   maybe I would see like this very rough thing.
[00:42:45.200 --> 00:42:47.080]   Then on another batch, I would see something
[00:42:47.080 --> 00:42:50.200]   that was also rough, but maybe had a pattern
[00:42:50.200 --> 00:42:52.480]   that would mostly like interfere with that one.
[00:42:52.480 --> 00:42:54.480]   So they would be high where the other one was low
[00:42:54.480 --> 00:42:57.360]   and just sort of be kind of orthogonal to each other.
[00:42:57.360 --> 00:42:59.640]   But it sounds like that roughness, as you said,
[00:42:59.640 --> 00:43:02.760]   is an inherent property of the loss surface that's, you know.
[00:43:02.760 --> 00:43:05.600]   - Yeah, because I mean, when you're talking,
[00:43:05.600 --> 00:43:08.000]   when you're talking, Charles, now about the difference
[00:43:08.000 --> 00:43:09.480]   between one batch and another batch,
[00:43:09.480 --> 00:43:11.280]   are you talking about the dynamics?
[00:43:11.280 --> 00:43:14.440]   I mean, the sequence to the steps, right?
[00:43:14.440 --> 00:43:16.600]   You are not talking, I mean, you are not talking
[00:43:16.600 --> 00:43:18.200]   about the static representation,
[00:43:18.200 --> 00:43:19.840]   or which one are you talking about?
[00:43:19.840 --> 00:43:22.320]   - I'm thinking about the static representations here,
[00:43:22.320 --> 00:43:24.800]   not that one batch size might see something different
[00:43:24.800 --> 00:43:27.960]   than another in terms of, yeah, where it goes.
[00:43:27.960 --> 00:43:29.080]   - Well, of course, of course.
[00:43:29.080 --> 00:43:32.080]   I mean, if you build the same representation,
[00:43:32.080 --> 00:43:34.520]   yeah, if you build the same representation
[00:43:34.520 --> 00:43:37.840]   and you used a different proportion of the data
[00:43:37.840 --> 00:43:40.400]   on different repetitions,
[00:43:40.400 --> 00:43:42.000]   yes, you would see differences.
[00:43:42.000 --> 00:43:45.000]   But on the very same representation,
[00:43:45.000 --> 00:43:47.520]   if you are consistent with the data that you are using,
[00:43:47.520 --> 00:43:49.840]   then no, then basically what you're seeing
[00:43:49.840 --> 00:43:53.160]   is the actual property of the difference, you know,
[00:43:53.160 --> 00:43:56.400]   that is the actual geometry that is being produced
[00:43:56.400 --> 00:43:58.920]   because of the difference in the loss values, yeah.
[00:43:58.920 --> 00:44:00.680]   - But then with the dropout section,
[00:44:00.680 --> 00:44:01.960]   where you saw those like really,
[00:44:01.960 --> 00:44:03.720]   that really rough loss surface,
[00:44:03.720 --> 00:44:06.360]   that was with a fixed dropout mask, right?
[00:44:06.360 --> 00:44:09.200]   That was like a fixed set of nodes have been set to zero?
[00:44:10.240 --> 00:44:11.840]   Through all the calculations of your point.
[00:44:11.840 --> 00:44:13.080]   - Which one specifically?
[00:44:13.080 --> 00:44:16.240]   Was that one of the static ones
[00:44:16.240 --> 00:44:17.840]   or was that the one in movement?
[00:44:17.840 --> 00:44:20.360]   - Let's, yeah, one of the static.
[00:44:20.360 --> 00:44:22.520]   So just a static one from this section
[00:44:22.520 --> 00:44:24.000]   where you showed that the dropout,
[00:44:24.000 --> 00:44:27.360]   the loss surface for dropout had all those like pump date,
[00:44:27.360 --> 00:44:31.240]   like sort of peaks and tiny little divots.
[00:44:31.240 --> 00:44:32.120]   - Yeah, yeah, yeah, exactly.
[00:44:32.120 --> 00:44:34.120]   That one, that one, I mean, each of those ones
[00:44:34.120 --> 00:44:37.440]   has a specific fixed dropout value, exactly.
[00:44:37.440 --> 00:44:39.160]   - I see.
[00:44:39.160 --> 00:44:41.520]   So then when you showed them in motion,
[00:44:41.520 --> 00:44:44.120]   then the dropout masks were changing over time.
[00:44:44.120 --> 00:44:46.680]   - Yes, because if you look at that video
[00:44:46.680 --> 00:44:48.240]   and we could, I mean, we could take a look
[00:44:48.240 --> 00:44:50.200]   or you can take it or we can take a look later,
[00:44:50.200 --> 00:44:51.720]   but basically in that video,
[00:44:51.720 --> 00:44:55.280]   I am gradually cranking up the dropout value, okay?
[00:44:55.280 --> 00:44:57.360]   So that is a training process
[00:44:57.360 --> 00:45:00.440]   in which I begin with a very, very small dropout value
[00:45:00.440 --> 00:45:02.960]   and I gradually increase it, okay?
[00:45:02.960 --> 00:45:04.000]   - I see.
[00:45:04.000 --> 00:45:05.480]   - Yeah, and as I increase it,
[00:45:05.480 --> 00:45:07.920]   that layer of noise increases as well.
[00:45:08.440 --> 00:45:09.440]   - Yeah.
[00:45:09.440 --> 00:45:10.280]   - I see.
[00:45:10.280 --> 00:45:13.160]   So that roughness, the roughness that came from the dropout,
[00:45:13.160 --> 00:45:15.840]   like that would, you know, that's gonna vary definitely
[00:45:15.840 --> 00:45:19.520]   and sort of be averaged out as you go across lots of masks.
[00:45:19.520 --> 00:45:23.480]   But the roughness from, that you showed in the, you know,
[00:45:23.480 --> 00:45:25.680]   in all the rest of them without dropout,
[00:45:25.680 --> 00:45:28.240]   that roughness is something that's, you know,
[00:45:28.240 --> 00:45:31.080]   that's gonna stick around from batch to batch.
[00:45:31.080 --> 00:45:31.920]   - Yes, yes.
[00:45:31.920 --> 00:45:34.240]   If you are consistent with the data that you use,
[00:45:34.240 --> 00:45:35.320]   yes, absolutely.
[00:45:35.320 --> 00:45:38.440]   Yeah, if you used a proportion of the data
[00:45:38.440 --> 00:45:41.040]   and you change that proportion between the representations,
[00:45:41.040 --> 00:45:41.960]   then it would be different,
[00:45:41.960 --> 00:45:44.280]   but you have to be consistent, we have to be consistent.
[00:45:44.280 --> 00:45:46.800]   So if we're always consistent, you know,
[00:45:46.800 --> 00:45:48.120]   on every presentation,
[00:45:48.120 --> 00:45:50.760]   then that's what you get everywhere in the landscape,
[00:45:50.760 --> 00:45:52.440]   is consistent, yeah.
[00:45:52.440 --> 00:45:54.720]   Yeah.
[00:45:54.720 --> 00:45:57.440]   - Interesting. - Definitely, definitely.
[00:45:57.440 --> 00:46:01.640]   - So I guess the part of the loss,
[00:46:01.640 --> 00:46:03.080]   so the feature of lost landscapes
[00:46:03.080 --> 00:46:06.480]   that I ended up studying in my dissertation
[00:46:06.480 --> 00:46:10.160]   was this very particular thing that we noticed,
[00:46:10.160 --> 00:46:13.160]   which was that very often we would find,
[00:46:13.160 --> 00:46:19.160]   we would find regions where the gradient was directly in the,
[00:46:19.160 --> 00:46:22.720]   it was like lied directly in the null space of the Hessian.
[00:46:22.720 --> 00:46:24.360]   So in the direction of the gradient,
[00:46:24.360 --> 00:46:26.120]   just looking in that direction,
[00:46:26.120 --> 00:46:29.600]   what that effectively meant was that the lost landscape was,
[00:46:29.600 --> 00:46:31.240]   I had zero curvature.
[00:46:32.240 --> 00:46:34.960]   - Okay, you had zero curvature?
[00:46:34.960 --> 00:46:37.960]   - Zero curvature. - So it was a critical point.
[00:46:37.960 --> 00:46:42.160]   - So non-zero gradient,
[00:46:42.160 --> 00:46:46.520]   but zero curvature is what these points were.
[00:46:46.520 --> 00:46:47.360]   - Okay.
[00:46:47.360 --> 00:46:48.720]   - So zero curvature in this direction.
[00:46:48.720 --> 00:46:52.080]   So you could point along a principal,
[00:46:52.080 --> 00:46:54.720]   you know, principal eigenvalue direction that's positive,
[00:46:54.720 --> 00:46:56.560]   you'd point along one that's negative,
[00:46:56.560 --> 00:46:57.600]   and in one case, you know,
[00:46:57.600 --> 00:47:00.280]   the loss surface would curve up in that direction,
[00:47:00.280 --> 00:47:03.400]   in the other case, it would curve down in that direction.
[00:47:03.400 --> 00:47:06.080]   And in these directions, there's no curvature at all.
[00:47:06.080 --> 00:47:09.120]   So you just get something that looks like a,
[00:47:09.120 --> 00:47:13.040]   you know, like a plane in that direction.
[00:47:13.040 --> 00:47:15.640]   So I guess I'm kind of curious,
[00:47:15.640 --> 00:47:17.800]   and it's also a well-known fact
[00:47:17.800 --> 00:47:21.840]   about rectified linear networks that in principle,
[00:47:21.840 --> 00:47:23.840]   if you don't change the parameters too much,
[00:47:23.840 --> 00:47:26.040]   they should behave like a,
[00:47:26.040 --> 00:47:29.560]   they should behave like a polynomial.
[00:47:29.560 --> 00:47:30.840]   So there's these sort of like,
[00:47:30.840 --> 00:47:34.040]   there's these ideas that there should be some simple shapes.
[00:47:34.040 --> 00:47:37.200]   And I'm curious if you've ever seen sort of any regions
[00:47:37.200 --> 00:47:39.520]   in these loss landscape visualizations
[00:47:39.520 --> 00:47:42.240]   that look like a region that's, you know,
[00:47:42.240 --> 00:47:44.600]   like a perfect quadratic bowl,
[00:47:44.600 --> 00:47:47.360]   or that looks like a completely flat plane.
[00:47:47.360 --> 00:47:51.160]   - That's interesting, that's interesting.
[00:47:51.160 --> 00:47:55.920]   A completely flat plane, I don't think so,
[00:47:55.920 --> 00:47:57.440]   not a completely flat plane.
[00:47:59.000 --> 00:48:00.920]   I mean, the perfect bowl, the perfect bowl,
[00:48:00.920 --> 00:48:02.280]   definitely these many, many times,
[00:48:02.280 --> 00:48:03.640]   but not the perfect plane.
[00:48:03.640 --> 00:48:05.480]   Not, sorry, not like a, you know,
[00:48:05.480 --> 00:48:07.120]   like a proper plane like that.
[00:48:07.120 --> 00:48:08.360]   No, not like that, no.
[00:48:08.360 --> 00:48:12.080]   - Yeah, my suspicion is that like fundamentally
[00:48:12.080 --> 00:48:14.120]   this geometric aspect here of these like sort of
[00:48:14.120 --> 00:48:17.520]   flat regions is it's a problem analytically.
[00:48:17.520 --> 00:48:20.680]   It's a problem like numerically and analytically,
[00:48:20.680 --> 00:48:22.880]   but they are these very small regions.
[00:48:22.880 --> 00:48:26.840]   And so it's not something that would show up in like,
[00:48:26.840 --> 00:48:29.080]   it's something that can attract an optimizer
[00:48:29.080 --> 00:48:30.120]   that's badly designed,
[00:48:30.120 --> 00:48:33.000]   but it's not something that you would see
[00:48:33.000 --> 00:48:34.560]   in the loss landscape if you weren't looking.
[00:48:34.560 --> 00:48:36.640]   - No, I mean, you know, the flatter things
[00:48:36.640 --> 00:48:39.720]   that I have seen is when you go really, really crazy
[00:48:39.720 --> 00:48:41.920]   with experiments and you reach a part
[00:48:41.920 --> 00:48:43.480]   of the high dimensional space
[00:48:43.480 --> 00:48:45.680]   that collapsed is completed process,
[00:48:45.680 --> 00:48:47.680]   then you can get a dimensionality reduction
[00:48:47.680 --> 00:48:50.640]   that literally is a plane.
[00:48:50.640 --> 00:48:51.840]   I mean, the whole thing,
[00:48:51.840 --> 00:48:53.600]   because the whole thing collapses completely, you know,
[00:48:54.200 --> 00:48:57.800]   but not in an isolated little part, yeah.
[00:48:57.800 --> 00:49:01.200]   Interesting, that's interesting.
[00:49:01.200 --> 00:49:02.040]   That's interesting.
[00:49:02.040 --> 00:49:03.640]   Yeah, I mean, if you say that it could be like
[00:49:03.640 --> 00:49:06.040]   a numerical issue, that's interesting.
[00:49:06.040 --> 00:49:09.400]   - Yeah, it's effectively, it's like a very special point,
[00:49:09.400 --> 00:49:12.120]   just like a minimizer is a very special point, right?
[00:49:12.120 --> 00:49:14.320]   And so like finding one of these points
[00:49:14.320 --> 00:49:18.480]   where there's this sort of brief little region of flatness
[00:49:18.480 --> 00:49:19.480]   is only something you'd find
[00:49:19.480 --> 00:49:21.600]   if you really were looking for it.
[00:49:21.600 --> 00:49:22.440]   - Yeah, that's interesting.
[00:49:22.440 --> 00:49:24.640]   Maybe they are out there, who knows?
[00:49:24.640 --> 00:49:29.920]   Maybe we'll have to search for them.
[00:49:29.920 --> 00:49:32.360]   - Let's see, we talked about a couple of the different,
[00:49:32.360 --> 00:49:35.800]   so you mentioned, yeah, the utility of skip connections.
[00:49:35.800 --> 00:49:42.520]   Oh, so you briefly touched on some stuff about batch norm
[00:49:42.520 --> 00:49:45.040]   from, you mentioned the paper from Mike Mahoney's group
[00:49:45.040 --> 00:49:48.240]   about like their sort of heretical view
[00:49:48.240 --> 00:49:49.280]   of what batch norm does.
[00:49:49.280 --> 00:49:50.360]   Could you tell us a little bit more
[00:49:50.360 --> 00:49:53.920]   about like how you see batch norm influencing
[00:49:53.920 --> 00:49:57.520]   these, the like landscapes and the visualizations
[00:49:57.520 --> 00:50:00.920]   that you've seen in neural network training in general?
[00:50:00.920 --> 00:50:04.080]   - Well, you know, I mean, it's very correlated
[00:50:04.080 --> 00:50:06.560]   to that paper in the sense that I have done
[00:50:06.560 --> 00:50:08.480]   quite a few experiments with batch norm
[00:50:08.480 --> 00:50:12.400]   and I have got a bit of a contradictory effects
[00:50:12.400 --> 00:50:16.000]   because again, I have seen the impact of batch norm
[00:50:16.000 --> 00:50:21.000]   apparently is moving surfaces, but also the opposite as well.
[00:50:21.000 --> 00:50:23.520]   So I don't have a clear view at the moment of it.
[00:50:23.520 --> 00:50:27.280]   You know, there was a point in which I was quite convinced
[00:50:27.280 --> 00:50:29.440]   that I have a clear correlation,
[00:50:29.440 --> 00:50:32.280]   but eventually I got to a point similar
[00:50:32.280 --> 00:50:34.960]   to what this paper is also talking about.
[00:50:34.960 --> 00:50:37.560]   And this is the problem as well,
[00:50:37.560 --> 00:50:39.360]   working with lost landscapes that as you know,
[00:50:39.360 --> 00:50:42.160]   it takes a lot of time and a lot of computation
[00:50:42.160 --> 00:50:45.320]   and there are a lot more comparisons still to be done.
[00:50:46.320 --> 00:50:47.160]   So yeah.
[00:50:47.160 --> 00:50:51.120]   - So when you say smoothness there, just a quick question.
[00:50:51.120 --> 00:50:54.040]   So my view of what batch norm does is that
[00:50:54.040 --> 00:50:56.520]   it improves the condition number of the surface.
[00:50:56.520 --> 00:50:58.400]   So like that notion of smoothness
[00:50:58.400 --> 00:51:00.560]   in terms of the relationship between the like small,
[00:51:00.560 --> 00:51:02.640]   negative, small positive eigenvalues
[00:51:02.640 --> 00:51:03.960]   and large positive eigenvalues,
[00:51:03.960 --> 00:51:06.600]   pulling those really high positive eigenvalues down.
[00:51:06.600 --> 00:51:09.360]   Is that what is meant by smoothness?
[00:51:09.360 --> 00:51:12.200]   - Yes, yes, the conditioning, this is right.
[00:51:12.200 --> 00:51:14.080]   I mean, the conditioning of the surface,
[00:51:14.080 --> 00:51:18.720]   you know, the degree of variation that you could get.
[00:51:18.720 --> 00:51:22.080]   Yes, it's a little bit related to that, absolutely.
[00:51:22.080 --> 00:51:23.800]   - Yeah, 'cause there's sort of like two,
[00:51:23.800 --> 00:51:25.040]   there's like the notion of roughness
[00:51:25.040 --> 00:51:26.040]   that we were talking about before
[00:51:26.040 --> 00:51:29.480]   where you have a landscape that's very foamy, right?
[00:51:29.480 --> 00:51:32.000]   But that landscape could still have like
[00:51:32.000 --> 00:51:35.680]   pretty well behaved maximum eigenvalues.
[00:51:35.680 --> 00:51:38.920]   You know, like an egg crate is relatively rough,
[00:51:38.920 --> 00:51:42.400]   but you never have like an eigenvalue
[00:51:42.400 --> 00:51:44.960]   that's like 10,000 times larger than the rest of them.
[00:51:44.960 --> 00:51:46.280]   - That's correct.
[00:51:46.280 --> 00:51:49.760]   - Yeah, so there's slightly different notions of smoothness.
[00:51:49.760 --> 00:51:51.400]   And I was just sort of curious
[00:51:51.400 --> 00:51:54.480]   which one was more important for the-
[00:51:54.480 --> 00:51:55.840]   - I mean, you are correct.
[00:51:55.840 --> 00:51:56.680]   You are correct.
[00:51:56.680 --> 00:51:59.080]   And we also have to consider one thing
[00:51:59.080 --> 00:52:01.240]   that in some of these representations
[00:52:01.240 --> 00:52:03.680]   captured in very, very high resolution,
[00:52:03.680 --> 00:52:08.000]   we also see some variations that we may not perceive
[00:52:08.000 --> 00:52:10.720]   when we, you know, we capture these representations
[00:52:10.720 --> 00:52:12.000]   with smaller detail.
[00:52:12.000 --> 00:52:16.360]   And some of these variations are also, you know,
[00:52:16.360 --> 00:52:18.760]   they are not things that necessarily
[00:52:18.760 --> 00:52:22.080]   are gonna interfere so much with the process.
[00:52:22.080 --> 00:52:24.560]   You know, so as you say, you know,
[00:52:24.560 --> 00:52:26.760]   surface could be well conditioned
[00:52:26.760 --> 00:52:29.040]   and still have certain variations.
[00:52:29.040 --> 00:52:31.640]   (upbeat music)
[00:52:31.640 --> 00:52:34.240]   (upbeat music)
[00:52:34.240 --> 00:52:36.840]   (upbeat music)
[00:52:37.440 --> 00:52:40.040]   (upbeat music)
[00:52:40.040 --> 00:52:42.620]   (upbeat music)

