
[00:00:00.000 --> 00:00:05.840]   Hey everyone, Ivan here, and in this video we'll start training YOLOv5 on a custom dataset.
[00:00:05.840 --> 00:00:10.560]   The goal of this video is to take the dataset that we have collected,
[00:00:10.560 --> 00:00:14.080]   train a custom YOLOv5 model on it using PyTorch,
[00:00:14.080 --> 00:00:18.480]   and then deploy our custom trained model locally on a system like Windows.
[00:00:18.480 --> 00:00:29.200]   You're watching part 3 of the YOLOv5 series. Watch part 0 to learn more about the YOLOv5
[00:00:29.200 --> 00:00:35.280]   and Weights and Biases integration, part 1 to learn how to install YOLOv5 on Windows and Google Colab,
[00:00:35.280 --> 00:00:41.120]   and part 2 to learn about when to use object detection and how to collect and label a custom
[00:00:41.120 --> 00:00:47.920]   dataset. If you have any questions, feel free to drop them in the comment section down below.
[00:00:47.920 --> 00:00:55.200]   Before talking about training, it's important to draw the distinction between pre-trained
[00:00:55.200 --> 00:01:02.080]   and custom trained models. The YOLOv5 repo comes with a model pre-trained in the Cocoa dataset,
[00:01:02.080 --> 00:01:06.240]   which is a very popular dataset for benchmarking different object detectors.
[00:01:06.240 --> 00:01:13.280]   The Cocoa dataset contains 80 classes. Therefore, the pre-trained YOLOv5 model can detect 80
[00:01:13.280 --> 00:01:19.840]   different classes, including cars, buses, dogs, horses, spoons, knives, and many, many more.
[00:01:22.000 --> 00:01:27.200]   But what happens if you want to train a YOLOv5 model to detect objects that aren't included in
[00:01:27.200 --> 00:01:33.680]   one of those 80 classes? You might ask, "Can I just add more classes to a model pre-trained
[00:01:33.680 --> 00:01:42.480]   in the Cocoa dataset?" And the answer is "not really". YOLOv5 is a convolutional neural network.
[00:01:42.480 --> 00:01:46.880]   The insides of a neural network are very interconnected, and it's basically impossible
[00:01:46.880 --> 00:01:52.800]   to tell which exact neuron or convolutional filter is responsible for detecting a certain class.
[00:01:52.800 --> 00:01:58.640]   If you wanted to add a new class, say for car wheels, to a model pre-trained in the Cocoa
[00:01:58.640 --> 00:02:04.880]   dataset, you would need to go through all the 120,000 Cocoa images and label all instances
[00:02:04.880 --> 00:02:10.720]   of car wheels there. Plus, if you had any additional images, you would need to go through
[00:02:10.720 --> 00:02:17.520]   all of them to label the car wheels and all of the 80 Cocoa classes. Depending on the hardware,
[00:02:17.520 --> 00:02:21.600]   models usually take days or even weeks to train on datasets this large.
[00:02:21.600 --> 00:02:30.240]   Just adding a class to a pre-trained model means relabeling the dataset and retraining the model.
[00:02:30.240 --> 00:02:36.080]   If your goal is to make a model to detect car wheels, a much more viable approach would be to
[00:02:36.080 --> 00:02:42.000]   collect images of car wheels, label them, and train an object detection model with just one class.
[00:02:42.000 --> 00:02:46.640]   And that way you can still take advantage of the pre-trained Cocoa model in the context
[00:02:46.640 --> 00:02:54.080]   of transfer learning. Besides, if you still wanted to take advantage of the 80 pre-trained classes,
[00:02:54.080 --> 00:02:58.720]   nothing is stopping you from running two independent neural networks one after another,
[00:02:58.720 --> 00:03:01.120]   a pre-trained one and a custom one.
[00:03:02.960 --> 00:03:07.200]   However, sometimes it's better to train a single custom trained model.
[00:03:07.200 --> 00:03:13.680]   In part 2, I talked about a model that detects four classes, buses, their open or closed doors,
[00:03:13.680 --> 00:03:19.280]   and bus numbers. This model is meant to run on a phone in real time, meaning that I couldn't
[00:03:19.280 --> 00:03:24.880]   afford slowing it down by running more than one network. Therefore, I trained a model in just
[00:03:24.880 --> 00:03:32.960]   those four classes. How did I do that? Let's talk about training a custom model.
[00:03:32.960 --> 00:03:39.040]   Right now we're looking at a dataset I have collected. There are four classes,
[00:03:39.040 --> 00:03:45.520]   closed door, open door, bus, and bus number. This dataset has been uploaded to the cloud
[00:03:45.520 --> 00:03:52.160]   and has been visualized in the browser as a 1db table. 1db, or weights and biases,
[00:03:52.160 --> 00:03:57.440]   is a machine learning tools platform for experiment tracking and versioning and exploring data.
[00:03:57.440 --> 00:04:03.040]   Watch part 2 of this series to learn how we have collected and labeled the dataset
[00:04:03.040 --> 00:04:06.080]   and then uploaded it to weights and biases as an artifact.
[00:04:06.080 --> 00:04:12.000]   The cool thing about it is that we can easily download the dataset artifact we're looking at
[00:04:12.000 --> 00:04:19.920]   to train on any machine. In this video, I'll show you how to train a YOLOv5 model on Google Colab.
[00:04:19.920 --> 00:04:24.640]   You can follow along with me. Link to the Colab I'm showing is in the video description.
[00:04:24.640 --> 00:04:29.920]   We'll come back to weights and biases later to understand whether the model is actually learning
[00:04:29.920 --> 00:04:35.280]   a thing successfully. In this Colab, we'll run the first two cells to set things up
[00:04:35.280 --> 00:04:49.200]   and run a test inference. A quick reminder, we used .yaml files to let the YOLOv5 training
[00:04:49.200 --> 00:04:54.400]   script know where to take the images for training from, what the names of the classes are, how many
[00:04:54.400 --> 00:05:00.160]   classes there are, and so on. In part 2 of this series, we used WNB artifacts, which is a tool
[00:05:00.160 --> 00:05:06.640]   for model and dataset versioning, to upload our dataset to the cloud. You can watch part 0 to
[00:05:06.640 --> 00:05:12.400]   learn more about how WNB artifacts work. I'll also leave a link to the artifacts documentation
[00:05:12.400 --> 00:05:18.960]   in the video description. When we run the upload dataset script like we did in part 2,
[00:05:18.960 --> 00:05:23.280]   it creates a special .yaml file that contains the path of the dataset as a link to the
[00:05:23.280 --> 00:05:31.120]   weights and biases artifact. We can then upload that file to Google Colab and pass it to the
[00:05:31.120 --> 00:05:35.920]   YOLOv5 training script so it knows where to download the training data from.
[00:05:35.920 --> 00:05:47.760]   If you want to follow along, this cell generates the .yaml file that contains the link to the bus
[00:05:47.760 --> 00:05:59.920]   dataset that I'm using. Now we need to pass a few parameters to the train.py script inside the
[00:05:59.920 --> 00:06:06.640]   YOLOv5 repo that initializes training. We'll pass the path to the .yaml file as a parameter,
[00:06:06.640 --> 00:06:13.040]   where its path is relative to the train.py file. We'll also specify a few other parameters,
[00:06:13.040 --> 00:06:19.360]   like the number of epochs to train for. During training, the dataset is split into many batches.
[00:06:19.360 --> 00:06:25.200]   When a model trains on all the mini-batches once, it completes one epoch. I typically
[00:06:25.200 --> 00:06:32.400]   initialize training with 30 epochs. The bboxNarrow parameter dictates once every how many epochs
[00:06:32.400 --> 00:06:38.400]   lag predictions and 16 random validation images to an interactive chart called BoundingBoxDebugger.
[00:06:39.760 --> 00:06:44.160]   The SavePeriod parameter specifies once every how many epochs we would like to save the best
[00:06:44.160 --> 00:06:49.920]   checkpoint weights. In our case, it's set to 1, meaning that we save the checkpoint weights for
[00:06:49.920 --> 00:06:56.240]   every epoch. After that, we'll specify the name of the WMB project that we would like to log the
[00:06:56.240 --> 00:07:02.160]   different training metrics, model checkpoints, and evaluation artifacts to. I named mine custom
[00:07:02.160 --> 00:07:09.520]   YOLOv5. Even if you're training a custom YOLOv5 model, you still can and should use the weights
[00:07:09.520 --> 00:07:15.600]   of the pre-trained model for transfer learning. In a nutshell, transfer learning means starting
[00:07:15.600 --> 00:07:20.400]   training using a pre-trained model instead of initializing a YOLOv5 model and its weights
[00:07:20.400 --> 00:07:25.680]   completely randomly. If you're starting with a pre-trained model, you can save time because
[00:07:25.680 --> 00:07:30.720]   the model already knows the low-level features and can focus on adjusting the high-level ones.
[00:07:30.720 --> 00:07:34.720]   Transfer learning reliably speeds up training and improves accuracy.
[00:07:35.440 --> 00:07:40.800]   There are different sizes of pre-trained YOLOv5 models to choose from - small, medium, large,
[00:07:40.800 --> 00:07:46.880]   and extra large. Intuitively, the smaller ones tend to be faster, while the larger ones tend
[00:07:46.880 --> 00:07:54.720]   to be more accurate. Here we'll pass weights YOLOv5_s.pt to use the smallest YOLOv5 model
[00:07:54.720 --> 00:08:01.520]   for training as baseline. Now we'll run this cell to start training. It'll prompt us to log
[00:08:01.520 --> 00:08:07.760]   into our Weights & Biases account or create a new one. Since I already have one, I'll copy and paste
[00:08:07.760 --> 00:08:20.560]   my API key. We can click on this link to open the Weights & Biases project we named "Custom YOLOv5".
[00:08:20.560 --> 00:08:30.000]   In order to explain how Weights & Biases is useful, I'd like to paint a picture as to how
[00:08:30.000 --> 00:08:35.360]   I used to monitor training in the past. Basically, I would only have access to the
[00:08:35.360 --> 00:08:40.560]   metrics printed in the console. Then, after the model training was finished, I would download
[00:08:40.560 --> 00:08:44.720]   the model and run it locally and attempt to understand if it's actually learned anything
[00:08:44.720 --> 00:08:52.160]   successfully. That's the coolest part about the Weights & Biases tools. We can monitor and
[00:08:52.160 --> 00:08:57.120]   visualize training and validation performance of a model while it's training and easily compare
[00:08:57.120 --> 00:09:03.360]   the performance of multiple model variations. Plus, the weights are being logged every epoch,
[00:09:03.360 --> 00:09:07.680]   meaning that even if the callup crashes, we're not wasting any progress.
[00:09:07.680 --> 00:09:14.080]   Now we're looking at the most important page of a YNDB project - the project page.
[00:09:14.080 --> 00:09:19.680]   The left panel contains our runs - the instances of training our models.
[00:09:19.680 --> 00:09:25.200]   Every time we start a new training session, we'll see a new run appear on the left.
[00:09:26.880 --> 00:09:32.000]   The project page itself lets us easily compare the performance across multiple runs
[00:09:32.000 --> 00:09:38.720]   side by side. For instance, here we can see how the different models are doing in terms of accuracy
[00:09:38.720 --> 00:09:46.800]   on the validation dataset. We can click on any run and open its run page
[00:09:46.800 --> 00:09:48.960]   in order to study it more specifically.
[00:09:52.720 --> 00:09:57.360]   I will now show you how we can analyze the training process by looking at an identical
[00:09:57.360 --> 00:10:06.400]   model I've already trained. The most useful things we can look at here are the interactive
[00:10:06.400 --> 00:10:12.880]   training metrics, the bounding box debugger, and the evaluation artifact.
[00:10:12.880 --> 00:10:18.960]   Looking at the training metrics is relatively straightforward.
[00:10:19.520 --> 00:10:24.640]   We want the loss in the validation data to go down and the mean average precision to go up.
[00:10:24.640 --> 00:10:31.600]   Mean average precision, or MAP, is the way that accuracy is measured for object detectors.
[00:10:31.600 --> 00:10:36.480]   It measures how well the bounding boxes in the corresponding classes match the labels.
[00:10:36.480 --> 00:10:43.120]   However, machine learning is complex, and oftentimes just looking at the metrics might
[00:10:43.120 --> 00:10:48.960]   not tell us the whole picture. This is where we can start looking at the bounding box debugger.
[00:10:49.120 --> 00:10:54.480]   The bounding box debugger is a chart that shows us model predictions and 16 random images from
[00:10:54.480 --> 00:11:01.600]   the validation dataset. With bboxInterval set to 1, it lags these predictions every epoch.
[00:11:01.600 --> 00:11:08.080]   This chart is interactive, meaning that we can adjust the results for which epoch we want to look
[00:11:08.080 --> 00:11:22.320]   at, the confidence thresholds, and whether to display or not display certain classes.
[00:11:22.320 --> 00:11:28.640]   The third thing that we can look at, and my favorite, is the evaluation artifact.
[00:11:28.640 --> 00:11:37.040]   The evaluation artifact allows us to compare model predictions and ground truth on every image from
[00:11:37.040 --> 00:11:50.160]   the validation dataset, for every epoch. We can find the evaluation artifact in the artifacts tab,
[00:11:50.160 --> 00:11:58.480]   or even just on our dashboard. The evaluation artifact is visualized as a 1db table,
[00:11:58.480 --> 00:12:04.400]   meaning that it's also interactive. Also note that the model predictions also display the
[00:12:04.400 --> 00:12:15.600]   confidence thresholds, unlike the ground truths. Now that our baseline has finished training,
[00:12:15.600 --> 00:12:21.280]   let's see if we can do better. I'll initialize the training again, but this time we'll use the
[00:12:21.280 --> 00:12:28.880]   large ELU5 model and train for more epochs. This will take a few hours, and then we can compare
[00:12:28.880 --> 00:12:37.520]   the runs in the WNB dashboard. Let's select the last two runs we did, the baseline and the
[00:12:37.520 --> 00:12:49.760]   hopefully improved run. First, we'll look at the metrics in the validation dataset. We can see that
[00:12:49.760 --> 00:12:57.440]   the larger model outperforms the smaller one in terms of loss and accuracy. Then we'll look at
[00:12:57.440 --> 00:13:02.400]   the bounding box debugger and see how the two models perform on the same images.
[00:13:02.400 --> 00:13:22.000]   Finally, we can get the fullest picture by looking at the evaluation table. Here we can see the both
[00:13:22.000 --> 00:13:28.560]   models' predictions and ground truths for the entire validation dataset.
[00:13:28.560 --> 00:13:46.400]   WNB also tracks hyperparameters for each run, like batch size or the number of epochs.
[00:13:47.600 --> 00:13:54.240]   The best way to compare the hyperparameters is by using the runs table. Here we can pin the
[00:13:54.240 --> 00:14:01.280]   hyperparameters we care about, like epochs, batch size, or the size of the ELU5 model we specified.
[00:14:01.280 --> 00:14:10.720]   Then we can collapse the runs table and be able to simultaneously see the performance
[00:14:10.720 --> 00:14:15.920]   of different runs and which hyperparameters they use to achieve that performance.
[00:14:16.560 --> 00:14:22.000]   If we have too many variations of hyperparameters to compare, we can group the categorical ones,
[00:14:22.000 --> 00:14:26.960]   like say the model size or the number of epochs, together and see the average performance of a
[00:14:26.960 --> 00:14:40.480]   specific group. Also, if we click on each run we can find the console logs,
[00:14:42.480 --> 00:14:49.040]   and in the artifacts tab on which dataset a given run was trained on.
[00:14:49.040 --> 00:15:01.120]   Remember how we set the save period parameter to 1? WNB logs and uploads the best weights of
[00:15:01.120 --> 00:15:07.360]   a model every epoch, based on its performance in the validation dataset. In the project page,
[00:15:07.360 --> 00:15:12.480]   we can click on a run, go into the artifacts tab and find the model artifact.
[00:15:12.480 --> 00:15:16.880]   There we can find at which epoch the model had the best performing weights.
[00:15:16.880 --> 00:15:31.120]   There's another very important feature of the ELU5 and 1DB integration to mention - the ability to
[00:15:31.120 --> 00:15:37.520]   resume crashed runs. Because our dataset and checkpoint weights have been locked to the cloud
[00:15:37.520 --> 00:15:44.560]   as WNB artifacts, if a run crashes, we can go into the runs overview panel, copy the run path,
[00:15:44.560 --> 00:15:49.680]   and pass it under the resume parameter to the train.py script to resume the crashed run.
[00:15:49.680 --> 00:15:59.680]   The cool thing is that training will continue from the very epoch it left off.
[00:16:00.240 --> 00:16:04.240]   Charts for the crashed run begin to update as well.
[00:16:04.240 --> 00:16:15.760]   I talk about resuming runs in a little more detail in part 0 of this series.
[00:16:15.760 --> 00:16:23.040]   Now that we have trained a custom ELU5 model, let's deploy it locally.
[00:16:26.400 --> 00:16:33.760]   We'll go into the artifacts tab of the best performing run and find the model artifacts.
[00:16:33.760 --> 00:16:47.120]   Then we'll download the best.pt model and put it in the ELU5 repo folder that we've
[00:16:47.120 --> 00:16:53.760]   installed locally in part 1 of this series. We'll rename it to best_model.pt and then
[00:16:53.760 --> 00:16:59.920]   open the console and run python detect.py sort0 waits bus_model.pt.
[00:16:59.920 --> 00:17:04.640]   This command starts the detection from my webcam feed.
[00:17:04.640 --> 00:17:15.520]   Let me point the camera at a second monitor that's showing buses and see how it goes.
[00:17:21.440 --> 00:17:26.800]   This is a way of deploying the model to perform real-time detection via webcam.
[00:17:27.680 --> 00:17:34.160]   So,
[00:17:35.040 --> 00:17:37.520]   so,
[00:18:02.400 --> 00:18:06.800]   we can also run python detect.py sort_best_video.mp4
[00:18:06.800 --> 00:18:15.760]   waits bus_model.pt to test the model's performance in a video.
[00:18:29.520 --> 00:18:35.440]   That's it for this video and for the ELU5 series. However, stay tuned for future videos and
[00:18:35.440 --> 00:18:40.720]   tutorials! If you have any questions or comments, please feel free to leave them in the comments
[00:18:40.720 --> 00:18:45.680]   section down below and I'll be happy to answer them. And consider subscribing to our channel
[00:18:45.680 --> 00:18:52.080]   to see the upcoming videos. And thank you for watching, I hope you enjoyed it and found it useful!

