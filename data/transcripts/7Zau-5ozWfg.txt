
[00:00:00.000 --> 00:00:06.360]   stream on YouTube and get started in one second. I'm just
[00:00:06.360 --> 00:00:09.040]   double checking if you're live on YouTube. Sometimes there's
[00:00:09.040 --> 00:00:15.920]   some issue that happens and I get paranoid about that. I can
[00:00:15.920 --> 00:00:20.600]   hear myself which means I'm live on YouTube. Hey, everybody,
[00:00:20.600 --> 00:00:24.720]   welcome back. Thanks for joining us again today. As you can see,
[00:00:24.720 --> 00:00:27.920]   it's three people on the screen today and we're super excited to
[00:00:27.920 --> 00:00:31.320]   host Jonathan. It's all thanks to Christian's help who's also
[00:00:31.320 --> 00:00:34.040]   been joining us every week. Jonathan, thanks. Thanks for
[00:00:34.040 --> 00:00:43.000]   joining us. I'll quickly start by introducing today's session
[00:00:43.000 --> 00:00:45.960]   and introducing Jonathan and then I'll hand it over to him.
[00:00:45.960 --> 00:00:50.000]   The topic for today's future of machine learning research in
[00:00:50.000 --> 00:00:56.080]   Jackson Flags. And this is the third lecture, if I may, in our
[00:00:56.080 --> 00:00:58.680]   Jack study group. Last week, we learned about working with
[00:00:58.680 --> 00:01:03.160]   neural networks in Jacks. This is being hosted with the Jacks
[00:01:03.160 --> 00:01:06.120]   global meetup. If you're not part of the meetup group, please
[00:01:06.120 --> 00:01:09.040]   go to meetup.com search for this group. This is a group by
[00:01:09.040 --> 00:01:12.400]   Christian Garcia, who, if you're from the flags community or the
[00:01:12.400 --> 00:01:15.480]   Jacks community, you really trust him to learn about Jacks
[00:01:15.480 --> 00:01:19.240]   and make sure you're part of this group so that you can keep
[00:01:19.240 --> 00:01:23.520]   learning about Jacks if you want to. So I'll quickly start by
[00:01:23.520 --> 00:01:27.600]   introducing Jonathan. Jonathan is a research software engineer
[00:01:27.600 --> 00:01:30.920]   at Google. He's one of the co-developers on flags. If I
[00:01:30.920 --> 00:01:34.360]   actually go to the flags repository, he's one of the top
[00:01:34.360 --> 00:01:37.720]   contributors, actually the top contributor to the repository.
[00:01:37.720 --> 00:01:40.600]   He's done his master's in machine learning at Cambridge
[00:01:40.600 --> 00:01:43.320]   and you can find him on Twitter at his first name and second
[00:01:43.320 --> 00:01:46.960]   name concatenated Jonathan Heek. Jonathan, thanks. Thanks again
[00:01:46.960 --> 00:01:49.000]   for your time. And thanks. Thanks for joining us.
[00:01:51.920 --> 00:01:52.880]   Thanks for having me.
[00:01:52.880 --> 00:01:59.720]   I'll also introduce our co host Christian has been joining us
[00:01:59.720 --> 00:02:03.920]   every week. And you can also find him on Twitter at C Garcia
[00:02:03.920 --> 00:02:07.680]   88. To remind you all, he has been working on the PX and LG
[00:02:07.680 --> 00:02:10.280]   frameworks, which you should check out and we'll be talking
[00:02:10.280 --> 00:02:14.560]   about in a future meetups. And with that, I'll hand it over to
[00:02:14.560 --> 00:02:17.960]   Jonathan to talk about the future of ML research in flags
[00:02:17.960 --> 00:02:21.680]   and Jacks. As a reminder, I'll post a link to our forums in the
[00:02:21.680 --> 00:02:25.160]   chat. Please ask any questions there. Jonathan will be taking
[00:02:25.160 --> 00:02:28.440]   questions throughout. So please keep the questions coming.
[00:02:28.440 --> 00:02:37.360]   Yes, please do. So thanks for having me. I'll also give a
[00:02:37.360 --> 00:02:43.440]   little bit of an introduction. So I'm Jonathan. Actually, a
[00:02:43.440 --> 00:02:48.440]   few years ago, when I started working at Google quite early, I
[00:02:48.560 --> 00:02:52.320]   figured out about this new exciting project called Jacks,
[00:02:52.320 --> 00:02:54.920]   which was sort of this functional approach to doing
[00:02:54.920 --> 00:03:00.560]   numerical compute and using sort of the full power of the the
[00:03:00.560 --> 00:03:03.880]   XLA compiler that was already there in 10 in the TensorFlow
[00:03:03.880 --> 00:03:10.360]   project. And it was sort of missing still a machine learning
[00:03:10.360 --> 00:03:14.720]   library to actually build neural nets. And I thought, Okay, let's
[00:03:14.720 --> 00:03:18.960]   try this out. That escalated and stuck to the wall. And so now
[00:03:18.960 --> 00:03:23.240]   we have we have this flex project that is used quite a bit
[00:03:23.240 --> 00:03:30.440]   by researchers across Google and also in open source. And, yeah,
[00:03:30.440 --> 00:03:34.560]   so nowadays, we're actually thinking a bit like, where are
[00:03:34.560 --> 00:03:37.520]   we heading? We have this neural network abstraction, we can
[00:03:37.520 --> 00:03:43.080]   build the basic things. But sort of how is the ML stack? And in
[00:03:43.080 --> 00:03:46.920]   particular, the sort of the stack built on top of Jacks, how
[00:03:46.920 --> 00:03:49.400]   is it going to evolve? And how is the research going to evolve?
[00:03:49.400 --> 00:03:52.640]   And what are the interactions are going to be? And how can we
[00:03:52.640 --> 00:03:58.400]   enable sort of the tomorrow's research in Jackson flex?
[00:03:58.400 --> 00:04:03.280]   Fundamentally, that this is sort of a really difficult question,
[00:04:03.280 --> 00:04:06.440]   because if you would actually know what the research was going
[00:04:06.440 --> 00:04:09.600]   to do, then it wouldn't really be research. So I guess there's
[00:04:09.600 --> 00:04:12.760]   not really truly a roadmap, but we're sort of trying to
[00:04:13.040 --> 00:04:18.040]   a plan for the unknown, so to say. This is also why I want
[00:04:18.040 --> 00:04:21.360]   this really to be a discussion, right? Because I want to hear
[00:04:21.360 --> 00:04:24.000]   from other people where where they're going, and what sort of
[00:04:24.000 --> 00:04:27.840]   exciting ideas they're going to try to throw on the wall and
[00:04:27.840 --> 00:04:32.680]   what what might stick actually. But first, I want to sort of
[00:04:32.680 --> 00:04:35.120]   start with a bit of a helicopter view, a sort of
[00:04:35.120 --> 00:04:39.120]   oversimplification of ML research, if you wish, right?
[00:04:41.120 --> 00:04:47.640]   We've got some basics now, right? Like a few years ago,
[00:04:47.640 --> 00:04:51.480]   you might still expect that sort of the CNN or this RNN that
[00:04:51.480 --> 00:04:55.360]   you're training today might be completely replaced with a
[00:04:55.360 --> 00:04:59.360]   completely different architecture tomorrow. Nowadays,
[00:04:59.360 --> 00:05:02.200]   we've got some things that actually stuck around, right?
[00:05:02.200 --> 00:05:05.560]   Like we've got normalization, we got like residual blocks, we
[00:05:05.560 --> 00:05:11.120]   got these, these basic MLPs, we've got attention, I guess,
[00:05:11.120 --> 00:05:15.720]   sort of the exemplifying network here is like the transformer,
[00:05:15.720 --> 00:05:18.480]   which is really something that seems to be applied absolutely
[00:05:18.480 --> 00:05:23.280]   everywhere outside of the domain that it was actually designed
[00:05:23.280 --> 00:05:27.160]   for, which was like language. Now you're seeing it in like,
[00:05:27.160 --> 00:05:33.160]   vision and translation and audio and pretty much anywhere. And
[00:05:33.160 --> 00:05:36.200]   it's sort of it's sort of universally working well, with
[00:05:36.200 --> 00:05:39.880]   minor tweaks. So this, I think, is actually a great thing,
[00:05:39.880 --> 00:05:43.960]   right? Like we've got some basic building blocks now in ML that
[00:05:43.960 --> 00:05:47.160]   actually work and that are actually pretty stable and that
[00:05:47.160 --> 00:05:50.360]   you can use everywhere, right? Like it's almost, it's not quite
[00:05:50.360 --> 00:05:53.200]   AGI yet, but it's but it's pretty general, right? And it's
[00:05:53.200 --> 00:05:58.720]   and it's relatively intelligent. Now that that I think is really
[00:05:59.480 --> 00:06:04.880]   cool. And of course, in Flax, we want to sort of support that
[00:06:04.880 --> 00:06:07.080]   very well, right? And because you're using these basic
[00:06:07.080 --> 00:06:09.400]   building blocks all the time, sort of the friction to using
[00:06:09.400 --> 00:06:14.280]   them should be really minimal. And so that's where sort of
[00:06:14.280 --> 00:06:17.680]   Flax comes in. And the core of Flax is really the modular
[00:06:17.680 --> 00:06:20.600]   module abstraction that we provide, which we call Linen.
[00:06:20.600 --> 00:06:25.960]   And what it does is you you already have Jax, but Jax is
[00:06:25.960 --> 00:06:30.800]   actually pretty, pretty fundamental, because it offers
[00:06:30.800 --> 00:06:34.480]   you this NumPy API for building numerical computations, and a
[00:06:34.480 --> 00:06:36.640]   bunch of functional transformations to do cool
[00:06:36.640 --> 00:06:41.320]   things with it, like taking radians, compiling it,
[00:06:41.320 --> 00:06:45.320]   vectorization, parallelization, that sort of stuff. But if
[00:06:45.320 --> 00:06:48.200]   you're building big models, then there's actually some things
[00:06:48.200 --> 00:06:51.000]   that are not sort of natively supported in a functional
[00:06:51.000 --> 00:06:54.560]   paradigm, like things like variables and random number
[00:06:54.560 --> 00:06:59.640]   sequences. So parameter state randomness, you need something
[00:06:59.640 --> 00:07:03.680]   to handle that sort of data flow and the flow of RNGs
[00:07:03.680 --> 00:07:06.960]   throughout your models. Otherwise, I mean, the great
[00:07:06.960 --> 00:07:08.760]   thing about functional programming is that it's very
[00:07:08.760 --> 00:07:12.080]   explicit, but there's a point where sort of a nice explicit
[00:07:12.080 --> 00:07:15.080]   API turns into something extremely verbose and repetitive,
[00:07:15.080 --> 00:07:19.640]   right? And so we want to make a bit of a balance there, right?
[00:07:19.640 --> 00:07:23.880]   Like we want something best of both worlds. So Linen modules
[00:07:23.880 --> 00:07:27.040]   provide that for you. So they handle the sort of the variable
[00:07:27.040 --> 00:07:32.480]   state. And at the end of the day, they provide it within an
[00:07:32.480 --> 00:07:37.200]   API that works with this functional paradigm, right? So
[00:07:37.200 --> 00:07:40.920]   at the end of the day, the stateful models offer you an
[00:07:40.920 --> 00:07:44.960]   init and an apply function that are actually functional, right?
[00:07:44.960 --> 00:07:51.560]   So unlike perhaps a truly sort of imperative Pythonic API,
[00:07:51.560 --> 00:07:55.280]   where the variables, they sort of live forever, as long as
[00:07:55.280 --> 00:07:59.000]   you're programmed, and the statefulness is unbound. In
[00:07:59.000 --> 00:08:02.160]   JAXA, it's always like sort of confined within the smallest
[00:08:02.160 --> 00:08:05.600]   region, right? Like the smallest possible region, and we want to
[00:08:05.600 --> 00:08:11.320]   sort of at a global level still be explicit. That I think is
[00:08:11.320 --> 00:08:13.840]   really nice. Oh, yeah, there's a question.
[00:08:15.080 --> 00:08:25.160]   I have a question. Like the init and apply API. Like, was this
[00:08:25.160 --> 00:08:28.880]   taken from like somewhere specific? Or did all the
[00:08:28.880 --> 00:08:33.720]   libraries suddenly rediscovered it? Like, if you know the
[00:08:33.720 --> 00:08:34.840]   history of this?
[00:08:34.840 --> 00:08:41.920]   So I think interestingly enough, there in Linen, there's not
[00:08:41.920 --> 00:08:45.280]   truly an init and apply, there's actually only apply an init as a
[00:08:45.280 --> 00:08:50.920]   wrapper around it. What is the case, though, is that if you, we
[00:08:50.920 --> 00:08:54.760]   support shape inference, right? So we sort of lazily initialize
[00:08:54.760 --> 00:09:00.120]   variables with an input example. So then naturally, sort of
[00:09:00.120 --> 00:09:02.640]   there are two stages, right? Like you want to sort of
[00:09:02.640 --> 00:09:06.480]   initialize the parameters of your model, and then you want to
[00:09:06.520 --> 00:09:12.200]   apply inputs, actual inputs to it to train it, right? So I
[00:09:12.200 --> 00:09:17.040]   think that init apply sort of distinction actually comes from
[00:09:17.040 --> 00:09:22.560]   the training, right? Like where sort of the initial part of your
[00:09:22.560 --> 00:09:26.200]   training loop is initialization. And then actually in the loop,
[00:09:26.200 --> 00:09:28.560]   you apply the model. And then actually, there might be an
[00:09:28.560 --> 00:09:31.920]   eval stage, right, which is like a variation of the time. So I
[00:09:31.920 --> 00:09:35.560]   think init apply is mostly something that comes from the
[00:09:35.560 --> 00:09:37.360]   training and not so much from the...
[00:09:37.360 --> 00:09:45.600]   I see. Well, I was mostly curious because, like, actually
[00:09:45.600 --> 00:09:50.040]   before Flax, I discovered Haiku. And then they also have init
[00:09:50.040 --> 00:09:59.040]   apply. And, and, like, my question is, like, was it there?
[00:09:59.040 --> 00:10:05.040]   Like somewhere maybe Stacks, I think also has it like, why? Why
[00:10:05.040 --> 00:10:10.080]   do all libraries converge to init apply? That's kind of my
[00:10:10.080 --> 00:10:10.600]   question.
[00:10:10.600 --> 00:10:14.520]   Yeah, I think, I think, yeah, it's an interesting question. I
[00:10:14.520 --> 00:10:18.280]   think, I think if you want to sort of stick a module into a
[00:10:18.280 --> 00:10:21.800]   functional training loop, that's eventually what you will have to
[00:10:21.800 --> 00:10:25.800]   provide, right? Like, that's the sort of the, the language
[00:10:25.800 --> 00:10:30.720]   everyone, the common language that everyone speaks, right? I
[00:10:30.720 --> 00:10:33.360]   think this is also nice that this is so explicit in a
[00:10:33.360 --> 00:10:37.000]   functional language, right? That, that, like, if you think
[00:10:37.000 --> 00:10:41.520]   that modules should at the top level be functional, then it
[00:10:41.520 --> 00:10:46.760]   sort of naturally converges to, to, to this tuple of functions,
[00:10:46.760 --> 00:10:49.160]   which in various frameworks might have slightly different
[00:10:49.160 --> 00:10:51.720]   signatures, but they're sort of like mappable to each other,
[00:10:51.720 --> 00:10:54.120]   right? Like, there's, there's sort of clear similarity there.
[00:10:54.120 --> 00:10:57.280]   I think this is actually one of the really nice things about
[00:10:57.280 --> 00:11:00.200]   functional programming that very often people converge to the
[00:11:00.200 --> 00:11:05.240]   same thing, because you need to make such explicit APIs, right?
[00:11:05.240 --> 00:11:08.040]   Unlike when it's imperative, it's much harder to make these
[00:11:08.040 --> 00:11:11.360]   sort of like, to see the similarities between these
[00:11:11.360 --> 00:11:11.800]   frameworks.
[00:11:11.800 --> 00:11:18.440]   I'll ask you some related questions a bit after.
[00:11:20.000 --> 00:11:31.560]   Sure. Let me pick this up again. So, so these linen modules, so
[00:11:31.560 --> 00:11:34.680]   that so they sort of are embeddable inside the Jax
[00:11:34.680 --> 00:11:40.080]   framework. And apart from that, there's a few small extra
[00:11:40.080 --> 00:11:44.000]   utilities that we provide, that we now in a days more and more
[00:11:44.000 --> 00:11:48.520]   to branch out into other sort of independent libraries. But in
[00:11:48.520 --> 00:11:51.840]   the early days, we actually embedded them into the, to the
[00:11:51.840 --> 00:11:55.880]   Flex repo. So these are things for like checkpointing and
[00:11:55.880 --> 00:11:59.960]   exposition training, sort of, sort of small training utilities.
[00:11:59.960 --> 00:12:02.920]   And the other thing that we always, especially at the
[00:12:02.920 --> 00:12:05.520]   beginning, we like because it was sort of lacking initially
[00:12:05.520 --> 00:12:10.800]   in Jax, is an example library, which allows you both to sort of
[00:12:10.800 --> 00:12:14.440]   get started as sort of pedagogical, like, like learning
[00:12:14.440 --> 00:12:18.200]   examples, but also forkable examples that really allow you
[00:12:18.200 --> 00:12:22.240]   to quickly bootstrap a new research project, right? So
[00:12:22.240 --> 00:12:26.080]   think about things like an ImageNet baseline, right? Like
[00:12:26.080 --> 00:12:29.760]   with like a ResNet-50, or like a transformer on a on a language
[00:12:29.760 --> 00:12:33.280]   test, which allow you sort of quickly take that thing
[00:12:33.280 --> 00:12:37.640]   relatively clean, small code base and try your research
[00:12:37.640 --> 00:12:38.120]   ideas.
[00:12:38.120 --> 00:12:46.040]   So what these linen modules look like, actually, is that you can
[00:12:46.040 --> 00:12:50.200]   have this, typically, you'll have a call. Because you want
[00:12:50.200 --> 00:12:54.240]   you, you typically have like a single entry point to a module.
[00:12:54.240 --> 00:12:58.000]   So it behaves more, more or less like a function. And there's a
[00:12:58.000 --> 00:13:03.480]   setup, which will always be called before it's used, where
[00:13:03.480 --> 00:13:11.960]   you can initialize sub modules or parameters. And so sort of to
[00:13:11.960 --> 00:13:19.440]   stay within the sort of functional flavor, these modules
[00:13:19.440 --> 00:13:23.320]   behave like immutable data classes. So you provide some,
[00:13:23.320 --> 00:13:30.800]   some parameters on top as Python free field declarations. And
[00:13:30.800 --> 00:13:35.920]   those cannot actually be mutated after construction. And one of
[00:13:35.920 --> 00:13:40.160]   the nice things is that because init is lazy, you actually get
[00:13:40.440 --> 00:13:43.080]   automatic shape inference. So you don't have to track like,
[00:13:43.080 --> 00:13:47.320]   for example, input feature shapes, right. So if you do
[00:13:47.320 --> 00:13:50.680]   like a, like a V shape, and then a fully connected layer, it will
[00:13:50.680 --> 00:13:54.080]   just figure out how many parameters that needs. And if
[00:13:54.080 --> 00:13:57.280]   you ever sort of call it with a different shape input, and the
[00:13:57.280 --> 00:14:00.400]   paramet parameter doesn't match, it will raise an error to you.
[00:14:00.400 --> 00:14:05.320]   So at the end of the day, what that produces is this nested
[00:14:05.320 --> 00:14:11.120]   dictionary of variables. Now, when you get that back from,
[00:14:11.120 --> 00:14:17.600]   from the module after you did the call to init, it actually is
[00:14:17.600 --> 00:14:21.840]   just a nested dict of Jack's arrays, right. So there's
[00:14:21.840 --> 00:14:24.160]   nothing magical about this variable abstraction where it
[00:14:24.160 --> 00:14:26.840]   keeps some kind of internal state or anything, right, like,
[00:14:26.840 --> 00:14:30.680]   at the end of the day, it's just just a nested dictionary with
[00:14:30.680 --> 00:14:34.440]   some with some array data, which I think one of the nice things
[00:14:34.440 --> 00:14:39.040]   because this is sort of makes it very easy to manipulate it or to
[00:14:39.040 --> 00:14:43.040]   sort of feed it into other API's or other libraries altogether.
[00:14:43.040 --> 00:14:47.920]   And everyone just use this, right, especially because Jack's
[00:14:47.920 --> 00:14:51.240]   has this concept of by trees where you can just map over an
[00:14:51.240 --> 00:14:59.200]   arbitrary container and manipulate. And so one other
[00:14:59.240 --> 00:15:05.000]   sort of nice feature that I think is sort of in the simple
[00:15:05.000 --> 00:15:07.640]   examples, it's shapes of a few lines. But I think actually, if
[00:15:07.640 --> 00:15:10.880]   you make complicated examples, then what you get is that you
[00:15:10.880 --> 00:15:13.600]   have these really big module definitions where you sort of
[00:15:13.600 --> 00:15:17.240]   have to scroll up and down your screen to sort of try to
[00:15:17.240 --> 00:15:20.760]   understand what this module is doing. This is sort of a very
[00:15:20.760 --> 00:15:25.520]   arch cognitive overhead, where we have this API called nn.compact
[00:15:25.520 --> 00:15:29.000]   where you can actually define modules in line in this in this
[00:15:29.000 --> 00:15:33.200]   compact function. And it will just lazily create these modules
[00:15:33.200 --> 00:15:37.880]   for you and remember the parameters and the variables and
[00:15:37.880 --> 00:15:42.240]   state. And this makes it sort of very easy because you can sort
[00:15:42.240 --> 00:15:44.800]   of from the top to bottom, you can read the definition of this
[00:15:44.800 --> 00:15:48.000]   model. And you know exactly what it does, right. And it does the
[00:15:48.000 --> 00:15:50.840]   sort of logical thing where, of course, you're not actually
[00:15:50.840 --> 00:15:56.120]   going to sort of recreate a convolution every, every time
[00:15:56.120 --> 00:15:58.840]   you call it, but you'll actually sort of reuse its parameters,
[00:15:58.840 --> 00:16:04.240]   right. And this is basically like a more declarative way of
[00:16:04.240 --> 00:16:08.120]   the like, if you would draw a network on the whiteboard, in
[00:16:08.120 --> 00:16:10.840]   some kind of self imagined pseudocode, this is probably a
[00:16:10.840 --> 00:16:13.440]   bit like how you would actually define a network, right? Like
[00:16:13.440 --> 00:16:15.800]   you wouldn't, you wouldn't sort of split these things up,
[00:16:15.800 --> 00:16:19.640]   because this makes it very easy to give a proper definition of
[00:16:19.640 --> 00:16:23.080]   the network that's easy for people to understand. And this
[00:16:23.080 --> 00:16:26.320]   is also actually how we wanted, we want to actually have the
[00:16:26.320 --> 00:16:29.120]   real code be close to the intuitive pseudocode that you
[00:16:29.120 --> 00:16:32.800]   that you would use, right. And so that's why we provide this,
[00:16:32.800 --> 00:16:33.920]   this API.
[00:16:33.920 --> 00:16:41.120]   So then once you have actually a module, right, so you can post
[00:16:41.120 --> 00:16:43.440]   some nodes together, some of the basic building blocks that
[00:16:43.440 --> 00:16:45.960]   Flex provides, and some of your higher level modules that you
[00:16:45.960 --> 00:16:49.560]   created, then you actually want to use this in your training
[00:16:49.560 --> 00:16:53.400]   loop. And we use this with the sort of these init apply
[00:16:53.400 --> 00:16:58.000]   functions. And when you init, you simply get an S the
[00:16:58.000 --> 00:17:01.960]   dictionary back with with with all the parameters and the
[00:17:01.960 --> 00:17:06.960]   state. And when you apply it, you provide those parameters and
[00:17:06.960 --> 00:17:12.240]   your inputs, right, and you get the outputs out. And if you want
[00:17:12.240 --> 00:17:15.520]   to have some state in your apply function, you sort of make it
[00:17:15.520 --> 00:17:20.240]   very explicit by providing this mutable field, and you tell
[00:17:20.640 --> 00:17:26.840]   linen, which specific type of variable is a mutant. So in
[00:17:26.840 --> 00:17:30.680]   Flex, we have this concept of collections, where now the most
[00:17:30.680 --> 00:17:33.280]   obvious one is sort of the parameters collection, which has
[00:17:33.280 --> 00:17:37.840]   all the like the learnable parameters. And, but you can
[00:17:37.840 --> 00:17:40.400]   have an arbitrary number of collections where you store your
[00:17:40.400 --> 00:17:44.560]   own stuff, right. So you might have a collection for like batch
[00:17:44.560 --> 00:17:49.200]   statistics for batch non layers, or a cache for autoregressive
[00:17:49.200 --> 00:17:52.480]   decoding, or whatever you need. And you can sort of manage these
[00:17:52.480 --> 00:17:54.920]   things separately, right. So this allows you to say, well,
[00:17:54.920 --> 00:17:58.480]   actually, during evaluation, the best statistics should not be
[00:17:58.480 --> 00:18:02.400]   mutable, but the cache and this gives you sort of this extra
[00:18:02.400 --> 00:18:08.080]   little bit of safety. And it also sort of immediately tells
[00:18:08.080 --> 00:18:13.520]   you which variables might be updated and which things you
[00:18:13.520 --> 00:18:16.480]   then which updated variables you then need to handle in your
[00:18:16.480 --> 00:18:19.000]   training, right? Because if you make a certain state mutable,
[00:18:19.000 --> 00:18:22.920]   it will also be returned by the apply function. And you will and
[00:18:22.920 --> 00:18:26.480]   you will have to handle these updated, this updated dict of
[00:18:26.480 --> 00:18:34.160]   state. So at the top level, it's a very explicit API that deals
[00:18:34.160 --> 00:18:40.120]   with whatever the sort of the modules internally create with
[00:18:40.120 --> 00:18:46.520]   their with their internal state. I think that this approach has a
[00:18:46.520 --> 00:18:49.400]   number of advantages. I mean, first of all, there's just
[00:18:49.400 --> 00:18:53.080]   Jax, which I think is a very nice sort of clean API, which
[00:18:53.080 --> 00:18:56.680]   uses NumPy, which actually everyone who was to use Python
[00:18:56.680 --> 00:19:00.440]   already knows. These functional APIs, they tend to be a bit more
[00:19:00.440 --> 00:19:03.920]   explicit. And they lack this sort of like, if there is any
[00:19:03.920 --> 00:19:07.120]   state, it's usually very confined. So there's no sort of
[00:19:07.120 --> 00:19:10.480]   spooky action at a distance where you change some internal
[00:19:10.480 --> 00:19:13.600]   thing. And then the next time you call it, it behaves
[00:19:13.600 --> 00:19:18.560]   differently. And I always thinking like, you know, all
[00:19:18.560 --> 00:19:21.600]   these things I've tried in the past, all these research ideas
[00:19:21.600 --> 00:19:25.040]   that I tried in the past, like, were they just bad ideas? Or did
[00:19:25.040 --> 00:19:27.360]   I actually just implement it wrong? And I have a feeling
[00:19:27.360 --> 00:19:29.040]   that a lot of people are actually thinking that,
[00:19:29.040 --> 00:19:32.880]   especially when you realize that a year later, someone else tries
[00:19:32.880 --> 00:19:35.640]   the exact same thing as you had in mind. And then suddenly for
[00:19:35.640 --> 00:19:41.240]   them, it did work. And then you really start to want, right. So
[00:19:41.240 --> 00:19:46.240]   so so actually, like the the cost of box is really high in
[00:19:46.240 --> 00:19:49.600]   ML. And it's actually very difficult, because I think in
[00:19:49.600 --> 00:19:51.480]   traditional software engineering, you can be pretty
[00:19:51.480 --> 00:19:54.600]   sure that if you have a bug, then probably the program or the
[00:19:54.600 --> 00:19:57.360]   compiler will just scream at you. But in ML, you will
[00:19:57.360 --> 00:20:00.080]   typically just get suboptimal results, right. And it's very
[00:20:00.080 --> 00:20:03.360]   hard to know what the suboptimal results are, if you don't have
[00:20:03.360 --> 00:20:06.240]   a baseline implementation, which in research is always the case,
[00:20:06.240 --> 00:20:09.000]   right. So mistakes are incredibly costly, and you might
[00:20:09.000 --> 00:20:15.400]   never find out that you made. But now some other advantages I
[00:20:15.400 --> 00:20:19.280]   think are, and this is mainly also related to XLA and how
[00:20:19.280 --> 00:20:23.920]   Jacks interacts with them, is that you can run on CPUs and
[00:20:23.920 --> 00:20:27.600]   GPUs and GPUs. And you can compile for all these
[00:20:27.600 --> 00:20:31.760]   platforms, which makes Jacks pretty fast, pretty competitive
[00:20:31.760 --> 00:20:35.560]   with other frameworks. And you can also parallelize pretty well
[00:20:35.560 --> 00:20:44.080]   across across devices. Recently, we've also had a change to how
[00:20:44.080 --> 00:20:49.120]   TPUs work. So this is on Google Cloud. So traditionally, how it
[00:20:49.120 --> 00:20:52.920]   worked is that you had a VM, and it was sort of connected over
[00:20:52.920 --> 00:20:56.200]   the network to a machine that actually had the TPUs, and it
[00:20:56.200 --> 00:21:00.800]   would send the program and the inputs over. And so this clearly
[00:21:00.800 --> 00:21:03.320]   has some limitations, because you cannot actually run your own
[00:21:03.320 --> 00:21:06.800]   code, your own input pipelines on the TPU machine. And many
[00:21:06.800 --> 00:21:13.360]   programs, many ML workloads are actually kind of bottlenecked by
[00:21:13.360 --> 00:21:16.280]   the input pipeline, especially if that has to run on a
[00:21:16.280 --> 00:21:21.240]   completely different machine. Now, recently, there's this new
[00:21:21.240 --> 00:21:26.080]   approach where you can run directly on the TPU machine.
[00:21:26.080 --> 00:21:31.240]   TPUs are kind of nice, because they come in very big scale, and
[00:21:31.240 --> 00:21:34.720]   you can get a large sort of interconnected slice. And
[00:21:34.720 --> 00:21:38.160]   actually, Jacks works really well with the compilation,
[00:21:38.160 --> 00:21:44.000]   Excellent compilation is also very powerful for TPUs. So that
[00:21:44.000 --> 00:21:47.360]   this this has been a very nice change where you can sort of run
[00:21:47.360 --> 00:21:51.240]   your own input pipelines, and sort of feed it directly into
[00:21:51.240 --> 00:21:58.080]   the TPU. And that I think is a nice bridge to sort of my second
[00:21:58.360 --> 00:22:02.240]   point on the helicopter view of the framework, which is that
[00:22:02.240 --> 00:22:05.480]   recently, we've seen a huge success in taking these basic
[00:22:05.480 --> 00:22:07.760]   building blocks that we know work very well and are very
[00:22:07.760 --> 00:22:12.560]   stable, and just scale them up massively. And this actually
[00:22:12.560 --> 00:22:16.040]   gives really impressive results. And this, of course, is not
[00:22:16.040 --> 00:22:18.600]   perhaps not very academic, but it's sort of more of an
[00:22:18.600 --> 00:22:22.800]   engineering effort. Nonetheless, I think this is actually really
[00:22:22.800 --> 00:22:27.480]   powerful. And we should definitely not ignore this. Even
[00:22:27.480 --> 00:22:31.520]   if even if we don't have 1000 GPUs handy, like, like perhaps
[00:22:31.520 --> 00:22:40.320]   open AI has. So I think it's really crucial that that Jackson
[00:22:40.320 --> 00:22:45.440]   flags of offer sort of very good scaling opportunities. If you
[00:22:45.440 --> 00:22:48.400]   have an idea that works, you want to be able to scale it up
[00:22:48.400 --> 00:22:52.240]   because in an ML, you can scale things up almost arbitrarily,
[00:22:52.240 --> 00:22:57.440]   and it will typically just get better and better. So the
[00:22:57.440 --> 00:23:01.800]   current parallelization strategy in JAX is that there's sort of
[00:23:01.800 --> 00:23:05.720]   two ways to parallelize something. One is Vmap, which
[00:23:05.720 --> 00:23:10.240]   allows you to automatically factorize a program on a single
[00:23:10.240 --> 00:23:13.920]   device, right? So this basically adds a patch dimension to your
[00:23:13.920 --> 00:23:17.880]   program, which you don't have to manually take care of. Now,
[00:23:17.880 --> 00:23:20.960]   that can be pretty nice, especially if you have sort of
[00:23:20.960 --> 00:23:24.840]   complicated, perhaps pre written NumPy code that you want to
[00:23:24.840 --> 00:23:30.000]   factorize. The other thing you can do is use Vmap, which is
[00:23:30.000 --> 00:23:34.800]   pretty much the exact same API, but it actually parallelizes
[00:23:34.800 --> 00:23:38.920]   across devices, right? So now, so now there's sort of a map
[00:23:38.920 --> 00:23:43.840]   dimension. But but it maps one to one to, to the devices that
[00:23:43.840 --> 00:23:46.800]   you have. And this can be on a single host or a multiple host.
[00:23:46.800 --> 00:23:52.040]   And this has, this doesn't have to be a strict map. Because of
[00:23:52.040 --> 00:23:55.520]   course, if you map over over an axis, strictly speaking, there's
[00:23:55.520 --> 00:23:59.160]   no communication across this axis. But in JAX, there are some
[00:23:59.160 --> 00:24:03.400]   special primitives that actually communicate along this sort of
[00:24:03.400 --> 00:24:07.800]   the map axis. So these are these have like little P names. So it's
[00:24:07.800 --> 00:24:13.360]   like a Psum or a Pmin or a Ppermute. And they allow you to
[00:24:13.360 --> 00:24:18.560]   communicate over this axis, right? So for example, most
[00:24:18.560 --> 00:24:22.160]   neural networks are truly mappings over a batch
[00:24:22.160 --> 00:24:25.920]   dimension. But if you, for example, have a batch norm
[00:24:25.920 --> 00:24:29.800]   layer, then you're not really truly you can't you don't have a
[00:24:29.800 --> 00:24:33.200]   you cannot truly map over your input, right? Because you
[00:24:33.200 --> 00:24:35.840]   actually have to sum over the batch dimension to calculate the
[00:24:35.840 --> 00:24:39.000]   batch statistics. And this you can actually do in JAX, even if
[00:24:39.000 --> 00:24:43.800]   you use Vmap or Pmap to sort of make this batch dimension
[00:24:43.800 --> 00:24:50.840]   implicit. But actually, this paradigm is being rethought a
[00:24:50.840 --> 00:24:53.960]   little bit. I think so far it has worked pretty well. But what
[00:24:53.960 --> 00:24:57.480]   we're realizing is that sort of these these parallel workloads,
[00:24:57.480 --> 00:25:01.840]   they're getting more complicated. And so that's why
[00:25:01.840 --> 00:25:04.400]   there's now this experimental new tool, which you can already
[00:25:04.400 --> 00:25:08.840]   use, but it's still being built, which is called XMap. And that
[00:25:08.840 --> 00:25:11.800]   sort of pulls apart the idea that some computations can be
[00:25:11.800 --> 00:25:19.000]   represented as maps, and how you run them, right. So actually,
[00:25:19.000 --> 00:25:22.040]   today, already Vmap and Pmap have very similar sort of API
[00:25:22.040 --> 00:25:25.040]   signatures, which I guess is also what made people realize
[00:25:25.040 --> 00:25:28.680]   that perhaps these really shouldn't be two separate APIs.
[00:25:28.680 --> 00:25:37.200]   And so with XMap, what you can do is you can XMap a function,
[00:25:37.200 --> 00:25:40.840]   and then map the input and output axes, the positional
[00:25:40.840 --> 00:25:44.320]   axes to logical dimensions. So basically, you can give them a
[00:25:44.320 --> 00:25:46.600]   name, a name that actually makes sense to what you're doing.
[00:25:46.600 --> 00:25:49.760]   Although here in this example, it's called like left and right,
[00:25:49.760 --> 00:25:53.640]   which I guess is not not very semantically meaningful, but you
[00:25:53.640 --> 00:25:59.400]   get the picture. So you can change the positional axes to
[00:25:59.400 --> 00:26:03.440]   look to something which has a name. And so inside the
[00:26:03.440 --> 00:26:06.040]   function that you're mapping over, these positional axes
[00:26:06.040 --> 00:26:09.400]   will no longer appear. So if you want to sort of now use them,
[00:26:09.400 --> 00:26:12.080]   you actually have to use a primitive that can take in this
[00:26:12.080 --> 00:26:15.720]   name, right. So you might Psum and give left, right, if you
[00:26:15.720 --> 00:26:19.320]   want to sum over the left dimension. Everything else is
[00:26:19.320 --> 00:26:26.240]   just implicitly mapped over. Then what you can also do is,
[00:26:26.240 --> 00:26:27.320]   oh, sorry, there's a question.
[00:26:27.320 --> 00:26:31.360]   No, finish and I'll ask it at the end of the slide.
[00:26:31.360 --> 00:26:38.400]   So you can map these logical axes, right. So the
[00:26:38.400 --> 00:26:41.360]   semantically meaningful names to something that is meaningful
[00:26:41.360 --> 00:26:44.640]   to the to the hardware, right. So you can map these axes
[00:26:44.640 --> 00:26:50.920]   optionally to whatever sort of mesh of devices that you have.
[00:26:50.920 --> 00:26:59.080]   And you can map it to the to the right and this mass
[00:26:59.080 --> 00:27:02.920]   specification is also separate from from xmap, right. So so
[00:27:02.920 --> 00:27:08.240]   you you define a mapping and separately you define how that
[00:27:08.240 --> 00:27:12.240]   mapping is sort of mapped out to the devices that you have,
[00:27:12.240 --> 00:27:15.440]   right. And so there's a sort of hardware axes, and there's
[00:27:15.440 --> 00:27:19.120]   logical axes and you provide a mapping between and you and you
[00:27:19.120 --> 00:27:22.760]   of course, put the devices on this on this mesh topology that
[00:27:22.760 --> 00:27:26.560]   you have, which typically represents the actual mesh
[00:27:26.560 --> 00:27:30.200]   topology that that your devices have, right. So, for example,
[00:27:30.200 --> 00:27:33.520]   on a GPU, you might have a GPUs that are closely connected, and
[00:27:33.520 --> 00:27:36.960]   then the rest sort of is connected over the net. And on a
[00:27:37.080 --> 00:27:40.760]   CPU machine, you have this to the topology, right where where
[00:27:40.760 --> 00:27:43.600]   all the GPUs are connected on a 2D.
[00:27:43.600 --> 00:27:46.320]   It was a question.
[00:27:46.320 --> 00:27:52.920]   Yeah, in this example, because well, xmap is kind of pretty
[00:27:52.920 --> 00:27:57.640]   new, right? Like, what is actually being computed here?
[00:27:57.640 --> 00:28:02.440]   And how does the mesh like treat the data?
[00:28:04.920 --> 00:28:09.120]   So, whatever mesh that you use, you will always actually compute
[00:28:09.120 --> 00:28:12.560]   the same thing, right? So it only impacts sort of the
[00:28:12.560 --> 00:28:16.040]   performance of how you compute it and not what you're actually
[00:28:16.040 --> 00:28:19.720]   computing, which I think is one of the nice things about this,
[00:28:19.720 --> 00:28:22.640]   right? Is that sort of parallelization here cannot break
[00:28:22.640 --> 00:28:23.160]   your code.
[00:28:23.160 --> 00:28:29.880]   I'll actually, I think, so this is a slightly sort of simple
[00:28:29.880 --> 00:28:35.240]   example. I'll have a slightly more informative example here.
[00:28:35.240 --> 00:28:39.280]   And I hope that sort of solves it, because here the logical
[00:28:39.280 --> 00:28:40.760]   dimensions actually mean something.
[00:28:40.760 --> 00:28:45.560]   So here's an example, which is perhaps a slightly over
[00:28:45.560 --> 00:28:48.600]   engineered example, right? But just to keep things simple,
[00:28:48.600 --> 00:28:55.200]   where sort of all the axes are actually mapped, right? So here,
[00:28:55.200 --> 00:28:57.920]   basically, the only thing we're getting in is actually it's all
[00:28:57.920 --> 00:29:01.480]   scalars, right? Like inside of the regression loss function.
[00:29:01.480 --> 00:29:05.040]   And then all the axes are actually mapped over and we give
[00:29:05.040 --> 00:29:08.640]   them the names that like the logical names that they have,
[00:29:08.640 --> 00:29:13.000]   like batch and input features and output features. And here
[00:29:13.000 --> 00:29:16.400]   you can also see that the other thing that is changing is that
[00:29:16.400 --> 00:29:21.160]   there are other APIs in Jaxx that are very similar, very
[00:29:21.160 --> 00:29:26.760]   alike, is that of like numpy.sum and basically psum, right?
[00:29:26.760 --> 00:29:28.800]   Because they're actually both doing the same thing. The only
[00:29:28.800 --> 00:29:32.760]   difference is that in numpy, you always provide a positional
[00:29:32.760 --> 00:29:37.520]   axis. And in psum, you provide a sort of a named axes that is
[00:29:37.520 --> 00:29:42.760]   mapped over somewhere else, right? Now, what is changing is
[00:29:42.760 --> 00:29:48.120]   that the numpy API will still implement the numpy API. But on
[00:29:48.120 --> 00:29:51.760]   top of that, it will also understand the named axes that
[00:29:51.760 --> 00:29:56.080]   XMAP provides. Similarly, with things like einsum, right, where
[00:29:56.080 --> 00:30:03.400]   you can now use a named axes rather than a position. And so
[00:30:03.400 --> 00:30:05.720]   here you have an API, which is basically everything is named,
[00:30:05.720 --> 00:30:08.920]   right? So now suddenly we've got three things, right? We can, we
[00:30:08.920 --> 00:30:11.920]   can basically write programs with named axes instead of
[00:30:11.920 --> 00:30:15.760]   positional axes. We can automatically factorize things,
[00:30:15.760 --> 00:30:18.160]   and we can parallelize things, but they're all sort of
[00:30:18.160 --> 00:30:24.040]   decoupled, right? And modeled. So we had three birds with one
[00:30:24.040 --> 00:30:31.440]   stone, basically. That's what XMAP is doing. Do you still have
[00:30:31.440 --> 00:30:32.440]   a question about the...
[00:30:32.440 --> 00:30:38.280]   Interesting, I think, well, for me, and I think a lot of
[00:30:38.280 --> 00:30:42.120]   viewers, this is kind of something I guess you should
[00:30:42.120 --> 00:30:48.640]   play with, right? Because, like, initially, it looks like, oh,
[00:30:48.640 --> 00:30:53.360]   you're putting names to things. But especially if you get to the
[00:30:53.400 --> 00:31:00.040]   mesh type of things, then it is not behaving only like a VMAP,
[00:31:00.040 --> 00:31:04.280]   but simultaneously like a VMAP, like a PMAP, if I understood it
[00:31:04.280 --> 00:31:04.920]   correctly.
[00:31:04.920 --> 00:31:09.040]   Yeah. And so one other thing is that, so if you would here, for
[00:31:09.040 --> 00:31:14.280]   example, apply... So let's say we have just a 1D mesh, right?
[00:31:14.280 --> 00:31:19.080]   And we're just parallelizing over the batch. Then the other
[00:31:19.080 --> 00:31:22.360]   funny thing that happens here is that you can actually... So in
[00:31:22.360 --> 00:31:25.720]   PMAP, you actually have to one-to-one split, right? So you
[00:31:25.720 --> 00:31:29.000]   had to split over an axis that was just as big as the number of
[00:31:29.000 --> 00:31:32.360]   devices that you have. In XMAP, that's no longer the case,
[00:31:32.360 --> 00:31:36.240]   right? So let's say you had a batch size of like 128, and you
[00:31:36.240 --> 00:31:41.520]   have like eight devices, then each will get a chunk of 16. And
[00:31:41.520 --> 00:31:44.600]   so how it will implement that is that it will do a VMAP and a
[00:31:44.600 --> 00:31:49.600]   PMAP over the batch dimension, right? So it will automatically
[00:31:49.600 --> 00:31:54.560]   vectorize over the 16 elements, right? That one device gets, and
[00:31:54.560 --> 00:31:58.720]   it will parallelize over the eight devices, right? So here,
[00:31:58.720 --> 00:32:01.920]   here, it will do a bit of both, so to say.
[00:32:01.920 --> 00:32:06.760]   It's pretty interesting. I don't... Did you happen to see
[00:32:06.760 --> 00:32:15.080]   this post of doing kind of ocean, like an ocean simulation
[00:32:15.080 --> 00:32:19.840]   or something like that, weather simulation, came last week that
[00:32:19.840 --> 00:32:27.000]   was using like, JAX for MPI, something like that?
[00:32:27.000 --> 00:32:30.440]   I'm not sure.
[00:32:30.440 --> 00:32:38.520]   I think Samjan featured it in the 27 Days of JAX. And did you
[00:32:38.520 --> 00:32:40.400]   remember what it was Samjan? It was ocean?
[00:32:40.400 --> 00:32:43.000]   I'm trying to find the link. Yes, I'll just share it in one
[00:32:43.000 --> 00:32:43.280]   minute.
[00:32:43.640 --> 00:32:48.440]   So now that you mentioned a mesh, like I remember like for
[00:32:48.440 --> 00:32:53.760]   the, for the, what he did in this case, maybe if Samjan can
[00:32:53.760 --> 00:32:59.960]   grab it, he split like a very huge domain into like a sub
[00:32:59.960 --> 00:33:06.240]   grid. And, and, and then he computed something over the
[00:33:06.240 --> 00:33:10.480]   grid, and then he had to take care of the like boundaries
[00:33:10.480 --> 00:33:11.480]   between each grid.
[00:33:12.480 --> 00:33:15.880]   Actually, I'll talk about basically that in a few slides.
[00:33:15.880 --> 00:33:19.160]   Okay. Yeah, I was just seeing this mesh and I thought, oh,
[00:33:19.160 --> 00:33:24.880]   wow, it seems like awfully similar to what he did there. I
[00:33:24.880 --> 00:33:32.480]   wonder if it is something that could be like, maybe because he
[00:33:32.480 --> 00:33:38.240]   used MPI, right? I am guessing he had multiple machines.
[00:33:38.240 --> 00:33:41.360]   Jonathan, I've pasted the link in the zoom chat.
[00:33:41.360 --> 00:33:54.200]   So, like, I guess, I mean, JAX is still not very good for
[00:33:54.200 --> 00:34:01.000]   multi, multi CPU, I don't know how we call it. Like, like a
[00:34:01.000 --> 00:34:04.680]   cluster kind of thing. It's more suited for like a TPU cluster,
[00:34:04.680 --> 00:34:05.040]   right?
[00:34:05.040 --> 00:34:10.120]   So GPU and TPU parallelism are a bit more mature, I would say.
[00:34:10.120 --> 00:34:14.560]   Yeah. Yeah. But if you have multiple GPUs, they have to be
[00:34:14.560 --> 00:34:17.160]   on the same machine, right? You can't have them on separate
[00:34:17.160 --> 00:34:18.560]   machines? Or can you?
[00:34:18.560 --> 00:34:22.480]   If I remember correctly, that that has been implemented.
[00:34:22.480 --> 00:34:27.520]   It has? Oh, wow. That works today.
[00:34:27.520 --> 00:34:29.760]   Yeah, that should work today. Yeah.
[00:34:29.760 --> 00:34:35.720]   Interesting. Then, like, I'm more curious, why, why, why do
[00:34:35.720 --> 00:34:40.400]   you need open MPI? Well, I mean, obviously, that's more for the
[00:34:40.400 --> 00:34:42.480]   developer of the post, but
[00:34:42.480 --> 00:34:47.320]   perhaps it might offer slightly more control, but I wouldn't be
[00:34:47.320 --> 00:34:48.280]   able to say for sure.
[00:34:48.280 --> 00:34:52.280]   Okay, I see. But then what? Yeah, maybe you have kind of a
[00:34:52.280 --> 00:34:57.720]   similar example. But he, if you did get to see it, he created a
[00:34:57.720 --> 00:35:01.480]   mesh, basically, and now they see this, like, oh, it's kind of
[00:35:01.480 --> 00:35:02.440]   what you're talking about.
[00:35:04.440 --> 00:35:12.120]   Yeah. Yeah. So people in our team and in Brain Amsterdam, and
[00:35:12.120 --> 00:35:15.280]   others have actually built these, these big weather models
[00:35:15.280 --> 00:35:21.000]   that also operate on a large, very large input of weather
[00:35:21.000 --> 00:35:23.960]   data. This is the MetNet model.
[00:35:23.960 --> 00:35:30.600]   V1 was published a while ago, and then, and yeah, published
[00:35:30.600 --> 00:35:30.960]   recently.
[00:35:31.560 --> 00:35:34.800]   I have a like a question, it might be a little technical, but
[00:35:34.800 --> 00:35:40.040]   how would you deal with the border? Like, communicating
[00:35:40.040 --> 00:35:43.400]   between the boundaries that was interesting in the article, but
[00:35:43.400 --> 00:35:47.280]   I wouldn't know how to implement it in what I know about PMAP.
[00:35:47.280 --> 00:35:51.720]   Yeah, so in principle, you could do that explicitly, right? So
[00:35:51.720 --> 00:35:56.600]   but I think the important thing is to sort of realize that a
[00:35:56.600 --> 00:36:00.880]   sort of a, if you have a convolutional model, and you
[00:36:00.880 --> 00:36:04.600]   split up your input in parts, that's not really quite a
[00:36:04.600 --> 00:36:08.000]   mapping function, right? Now, of course, if you extend your
[00:36:08.000 --> 00:36:12.320]   mapping with sort of like, so in JAX, there's this P per mu
[00:36:12.320 --> 00:36:18.280]   function, where sort of two indices of the map index can
[00:36:18.280 --> 00:36:21.560]   exchange an arbitrary value. Now, of course, you can manually
[00:36:21.560 --> 00:36:24.760]   slice up your border and exchange it with your neighbor.
[00:36:24.760 --> 00:36:28.480]   You can do this manually. It's very cumbersome, though, right?
[00:36:28.480 --> 00:36:31.720]   Like it's sort of like it's very intrusive to your code. And you
[00:36:31.720 --> 00:36:34.400]   have to manually compute how that actually works, right?
[00:36:34.400 --> 00:36:38.560]   Because if you do a convolution, you have to sort of look at your
[00:36:38.560 --> 00:36:42.600]   kernel size at your striding, perhaps dilation, and you sort
[00:36:42.600 --> 00:36:46.280]   of have to calculate exactly how much border you need to do this
[00:36:46.280 --> 00:36:50.000]   computation. Right? It gets even worse if you have so much
[00:36:50.000 --> 00:36:53.400]   border interaction that the border actually extends across
[00:36:53.400 --> 00:36:57.480]   two neighbors, right? Oh, well, this just gets terribly
[00:36:57.480 --> 00:37:00.960]   complicated. So it can be done as a map, but it doesn't look
[00:37:00.960 --> 00:37:01.520]   nice at all.
[00:37:01.520 --> 00:37:10.480]   For P map, this would be you would use the L AX API and
[00:37:10.480 --> 00:37:12.000]   communicate through there.
[00:37:12.000 --> 00:37:17.400]   You would use Lex dot P permute. That's that's how you do it.
[00:37:21.480 --> 00:37:28.880]   Oh, yeah. And so here I wanted to just show so that all the
[00:37:28.880 --> 00:37:32.480]   things where you don't mention the axes, right, that you
[00:37:32.480 --> 00:37:36.000]   implicitly map over. What I wanted to go though, though, to
[00:37:36.000 --> 00:37:41.040]   is that my next slide is about actually an alternative
[00:37:41.040 --> 00:37:45.360]   approach where my example of where P map breaks down, which
[00:37:45.360 --> 00:37:49.000]   which Christian has already introduced for me, is if you
[00:37:49.000 --> 00:37:52.560]   wanted to split up a large image, and you wanted to run a
[00:37:52.560 --> 00:37:56.920]   CNN on it, then really, this is this is very annoying, as we
[00:37:56.920 --> 00:38:01.440]   just described. Because you have to exchange the borders and
[00:38:01.440 --> 00:38:05.280]   doing this explicitly is very hard, and sort of it doesn't
[00:38:05.280 --> 00:38:10.080]   look very much like the thing you actually want to do. And so
[00:38:10.080 --> 00:38:12.880]   alternately to the XMAC development, there's another
[00:38:12.880 --> 00:38:17.160]   experimental parallelization strategy, which is called PJIT.
[00:38:17.600 --> 00:38:22.600]   Now PJIT uses a it's called JIT because it actually it uses
[00:38:22.600 --> 00:38:27.240]   part of the compilation. It does it uses XMAC compilation,
[00:38:27.240 --> 00:38:31.520]   but it uses an additional compiler pass. Now, what this
[00:38:31.520 --> 00:38:38.440]   compiler pass does is it, it takes a program, and then it
[00:38:38.440 --> 00:38:42.440]   emits a new program that only works on a subset of the data.
[00:38:42.440 --> 00:38:47.200]   And what it does is it inserts communication primitives,
[00:38:48.200 --> 00:38:52.200]   that make sure that the whole the entire input is processed,
[00:38:52.200 --> 00:38:56.840]   and the entire correct output is produced. So basically, it
[00:38:56.840 --> 00:39:00.200]   allows you to take a program and split it up automatically over
[00:39:00.200 --> 00:39:04.160]   multiple devices. Now, what it for example, will do is if you
[00:39:04.160 --> 00:39:09.280]   give it a convolution, and you split it up across its spatial
[00:39:09.280 --> 00:39:13.360]   dimensions, it will introduce these border exchanges, right?
[00:39:14.840 --> 00:39:18.240]   If you if you give it a data parallel program, it will
[00:39:18.240 --> 00:39:22.080]   introduce like a sum over the gradients automatically for you,
[00:39:22.080 --> 00:39:25.880]   right? So it will just, it will go through your code. And it
[00:39:25.880 --> 00:39:32.440]   will just automatically add any parallelization primitives that
[00:39:32.440 --> 00:39:36.400]   you need to keep the code correct, automatically for you,
[00:39:36.400 --> 00:39:39.640]   right? So this is completely sort of a completely implicit
[00:39:39.640 --> 00:39:44.200]   parallelization API. And the only sort of little help that
[00:39:44.200 --> 00:39:47.560]   you need to give it is sort of specify how the inputs and
[00:39:47.560 --> 00:39:50.640]   outputs of the whole function that you're compiling are
[00:39:50.640 --> 00:39:54.280]   parallelized, right, like across the across the devices that you
[00:39:54.280 --> 00:39:59.080]   have. So if you if you did a very simple example, you would
[00:39:59.080 --> 00:40:02.840]   start for example, say like, okay, just partition over the
[00:40:02.840 --> 00:40:07.360]   data, right, and it might only add a single communication
[00:40:07.360 --> 00:40:10.920]   primitive to sum over the gradients, and perhaps some over
[00:40:10.920 --> 00:40:14.160]   the batch statistics. But if you do something complicated, like
[00:40:14.160 --> 00:40:19.200]   like splitting a huge image to simulate an ocean across spatial
[00:40:19.200 --> 00:40:23.080]   dimensions, then it will introduce all these border
[00:40:23.080 --> 00:40:24.760]   exchanges for you automatically.
[00:40:24.760 --> 00:40:28.680]   So this was you're talking about Pidget?
[00:40:28.680 --> 00:40:29.560]   Yes.
[00:40:29.560 --> 00:40:34.960]   Okay. And that is very interesting, because it has very
[00:40:34.960 --> 00:40:39.680]   few documentation, right? So from the outside, you end up
[00:40:39.680 --> 00:40:40.800]   guessing what it does.
[00:40:42.360 --> 00:40:46.960]   Yeah, so so one of the, I guess, one thing that makes Pidget a
[00:40:46.960 --> 00:40:49.880]   bit of an oddball is that it's not actually a JAXS
[00:40:49.880 --> 00:40:52.880]   transformation, right? So it's not written in Python, it's
[00:40:52.880 --> 00:40:59.160]   actually part of the XLA compiler, right. And so the XLA
[00:40:59.160 --> 00:41:02.240]   compiler is not not not documented as part of JAXS.
[00:41:02.240 --> 00:41:07.720]   That were like documenting what this thing does is actually an
[00:41:07.720 --> 00:41:10.200]   ongoing effort, but it's also a bit more difficult because it's
[00:41:10.200 --> 00:41:13.800]   not quite an explicit API, right? It's, it's a lot easier
[00:41:13.800 --> 00:41:16.480]   to explain what XMAP will do than what Pidget will do,
[00:41:16.480 --> 00:41:19.640]   because Pidget is really an optimization, right? Like it's
[00:41:19.640 --> 00:41:24.400]   actually trying to partition your program as cleverly as
[00:41:24.400 --> 00:41:26.920]   possible, right? So it's really, we're really talking about an
[00:41:26.920 --> 00:41:30.880]   optimization pass here, not not a strict transformation, right?
[00:41:30.880 --> 00:41:33.760]   Like there, there are heuristics involved, which make it more
[00:41:33.760 --> 00:41:34.280]   complicated.
[00:41:34.280 --> 00:41:38.320]   But do you still help it with the mesh information and that
[00:41:38.320 --> 00:41:38.880]   kind of thing?
[00:41:39.480 --> 00:41:43.800]   Yeah, so you do provide the mesh still, and you provide the sort
[00:41:43.800 --> 00:41:48.240]   of the how the logical axes, so how the position, the position
[00:41:48.240 --> 00:41:52.640]   on logical axes map to, to the to the mesh axes on the input
[00:41:52.640 --> 00:41:56.320]   and output sides. And you can add arbitrary constraints inside
[00:41:56.320 --> 00:42:00.600]   the program. So you can, so XLA will automatically sort of
[00:42:00.600 --> 00:42:04.600]   decide what is the most optimal sharding for each intermediate
[00:42:04.600 --> 00:42:07.280]   value automatically for you, but you can override this with a
[00:42:07.280 --> 00:42:09.920]   constraint, right? Where you where you say, okay, I
[00:42:09.920 --> 00:42:15.880]   specifically want this intermediate value inside this
[00:42:15.880 --> 00:42:21.160]   Pidget function, I want that to be to be sharded in this way.
[00:42:21.160 --> 00:42:26.360]   Can I ask kind of a devil's advocate question? It's kind of
[00:42:26.360 --> 00:42:30.880]   something that it's kind of a love hate, I guess, in the end.
[00:42:31.960 --> 00:42:39.240]   I've, for example, for me, if the compiler does something that
[00:42:39.240 --> 00:42:44.160]   I need for me is, I mean, it's beautiful. It's so feels like,
[00:42:44.160 --> 00:42:49.840]   so natural, but then there's people who will, like as
[00:42:49.840 --> 00:42:54.800]   criticism, they kind of say that you lose a little bit of
[00:42:54.800 --> 00:43:01.280]   control. I don't know if this is fair to state, because then it
[00:43:01.320 --> 00:43:04.880]   you know what happens behind the scenes.
[00:43:04.880 --> 00:43:10.400]   Yeah, so I think what's definitely true, right? Like if,
[00:43:10.400 --> 00:43:14.160]   if your objective is to make things as fast as possible,
[00:43:14.160 --> 00:43:17.760]   right, and you have a good idea of how the communication should
[00:43:17.760 --> 00:43:21.040]   actually happen to make it as fast as possible, you might want
[00:43:21.040 --> 00:43:24.400]   to use an explicit API, like xmap, because you know exactly
[00:43:24.400 --> 00:43:27.240]   what's going on. And if you want to check everything anyway, you
[00:43:27.240 --> 00:43:30.520]   might as well write it explicitly in your code. If you
[00:43:30.520 --> 00:43:33.800]   have an existing model, and you want it to do a reasonably good
[00:43:33.800 --> 00:43:37.720]   effort, right. And, and more importantly, because that I
[00:43:37.720 --> 00:43:41.560]   think is really the main benefit of pj is that it guarantees you
[00:43:41.560 --> 00:43:46.680]   equivalent, right? Like, you'll be able to paralyze your program
[00:43:46.680 --> 00:43:49.360]   and know that it will produce the exact same result as the
[00:43:49.360 --> 00:43:52.640]   unparallelized program, right? Which is with xmap, of course,
[00:43:52.640 --> 00:43:55.240]   not the case. If you transfer, if you partition your code
[00:43:55.240 --> 00:43:59.240]   manually, you need to do it correct. If you don't, it will
[00:43:59.280 --> 00:44:02.360]   it will make mistakes, right? pj will not make mistakes, it
[00:44:02.360 --> 00:44:06.440]   might, it might do something sub optimally, but it won't do it
[00:44:06.440 --> 00:44:09.640]   wrong. Right, which which I think is sort of one of the
[00:44:09.640 --> 00:44:09.840]   main.
[00:44:09.840 --> 00:44:15.240]   Yeah, that is very interesting. I don't know if you have used or
[00:44:15.240 --> 00:44:17.200]   know a little bit about deep speed.
[00:44:17.200 --> 00:44:23.160]   Yeah, so I think deep speed is an interesting. That's a pretty
[00:44:23.160 --> 00:44:25.560]   interesting library. And that's one of the things where sort of
[00:44:25.560 --> 00:44:29.120]   like comparing that is one of the things where we definitely
[00:44:29.120 --> 00:44:34.920]   thinking about like, how should how should the parallelization
[00:44:34.920 --> 00:44:38.600]   strategy evolve over time? Because deep speed can do some
[00:44:38.600 --> 00:44:43.880]   things that we that are hard to do now in jacks. And, and vice
[00:44:43.880 --> 00:44:49.240]   versa, I think, right, like, so this type of parallelism is hard
[00:44:49.240 --> 00:44:50.720]   to do in deep speed, I think.
[00:44:54.000 --> 00:44:59.720]   From my perspective, like, I mean, a P map is super easy to
[00:44:59.720 --> 00:45:04.640]   use, like, I mean, you don't even know, have to know very
[00:45:04.640 --> 00:45:12.520]   much about distributed, like a systems or anything, it's just
[00:45:12.520 --> 00:45:16.920]   kind of like a V map, but you kind of have this notion that
[00:45:16.920 --> 00:45:20.480]   it's happening over multiple devices, but the details are
[00:45:20.480 --> 00:45:24.040]   taken care for you. That's the sensation I get. And then from
[00:45:24.040 --> 00:45:27.520]   deep speed, you have to take care of all the details, right?
[00:45:27.520 --> 00:45:33.520]   So it's kind of a trade off there. Like, in deep speed, you
[00:45:33.520 --> 00:45:38.160]   have to be more sanity here, and then do this and then send it
[00:45:38.160 --> 00:45:38.800]   there.
[00:45:38.800 --> 00:45:44.360]   Yeah, so what I think is cool is that jacks has been able to sort
[00:45:44.360 --> 00:45:47.720]   of, like, what's of course, always in functional API is that
[00:45:47.720 --> 00:45:50.000]   you don't want them to just be functional, you want them to be
[00:45:50.040 --> 00:45:52.960]   really declarative, right? Like, you want to declare what you're
[00:45:52.960 --> 00:45:56.080]   going to do, and not sort of have this thing look like a to
[00:45:56.080 --> 00:46:00.600]   do list of actions, right, like an imperative API. I think this
[00:46:00.600 --> 00:46:03.800]   has succeeded very well. And it's sort of like, it's becoming
[00:46:03.800 --> 00:46:06.600]   more declarative, because arguably, x map is even more
[00:46:06.600 --> 00:46:11.520]   declarative than, than P map was, right. And, and PJ, this
[00:46:11.520 --> 00:46:14.400]   may be even more so declarative, right? Because you're sort of
[00:46:14.400 --> 00:46:18.360]   like, declaring your function as if it's running on this magical
[00:46:18.360 --> 00:46:22.400]   VM that has all your compute resources bundled together,
[00:46:22.400 --> 00:46:26.080]   right. And, and now really partitioning becomes just a just
[00:46:26.080 --> 00:46:29.760]   a compiler, right? They're just an optimization that happens is
[00:46:29.760 --> 00:46:35.680]   done for you. That I think is pretty nice. Of course, then
[00:46:35.680 --> 00:46:38.760]   deep speak, maybe can also do some things that are kind of
[00:46:38.760 --> 00:46:42.440]   clever, like pipelining, or streaming up in and out
[00:46:42.440 --> 00:46:44.520]   parameters that we cannot do today.
[00:46:46.440 --> 00:46:48.000]   Sanyam Bhutani: Jonathan, there's a question from the
[00:46:48.000 --> 00:46:51.160]   audience by Yuvraj, which is similar to this. So the
[00:46:51.160 --> 00:46:55.240]   question is, how can we have more insight on how XLA is
[00:46:55.240 --> 00:46:59.120]   choosing the most optimal path? Is there a way to comment on
[00:46:59.120 --> 00:47:03.520]   what XLA is doing in some cases, which might not be the case that
[00:47:03.520 --> 00:47:04.720]   one would want it to do?
[00:47:04.720 --> 00:47:08.280]   Jonathan Asic: Yeah, so actually, I think this is a bit
[00:47:08.280 --> 00:47:11.960]   of a problem that we have today, and that we need to improve.
[00:47:12.880 --> 00:47:19.040]   Because profiling today could improve, you could try to
[00:47:19.040 --> 00:47:23.960]   reverse engineer either from sort of dumping HLO from from
[00:47:23.960 --> 00:47:29.520]   the compiler, or looking at sort of profiling output, I guess on
[00:47:29.520 --> 00:47:33.520]   the GPU. But this sort of feels a bit like a reverse
[00:47:33.520 --> 00:47:41.200]   engineering task. I think that's not ideal. We need some tools to
[00:47:41.200 --> 00:47:46.640]   diagnose and to give us information just about what kind
[00:47:46.640 --> 00:47:50.560]   of job it did on the partitioning, right. And sort of
[00:47:50.560 --> 00:47:53.680]   avoid having that clustered with all this sort of other
[00:47:53.680 --> 00:47:57.440]   information that is in profiling tools. That I think is really
[00:47:57.440 --> 00:48:01.920]   something for like, that we should support tomorrow. But
[00:48:01.920 --> 00:48:02.960]   that's not there today.
[00:48:02.960 --> 00:48:07.320]   Sanyam Bhutani: Are there ways to nudge the compiler into what
[00:48:07.320 --> 00:48:07.840]   you want?
[00:48:07.840 --> 00:48:10.360]   Jonathan Asic: Yeah, so this is what you can do with these
[00:48:10.360 --> 00:48:12.400]   constraints, right. So in principle, if you would
[00:48:12.400 --> 00:48:16.280]   constrain every op, right, so sort of the sort of hypothetical
[00:48:16.280 --> 00:48:18.840]   extreme case, then it would do exactly what you would want it
[00:48:18.840 --> 00:48:22.120]   to do, right? Because then it sort of becomes unambiguous.
[00:48:22.120 --> 00:48:27.080]   Because you've sort of put the compiler in a corner where
[00:48:27.080 --> 00:48:29.320]   basically everything is a constraint, and it can decide
[00:48:29.320 --> 00:48:35.120]   nothing on its own. More realistically, though, you would
[00:48:35.120 --> 00:48:40.800]   look at the output and sort of decide where sort of our
[00:48:40.800 --> 00:48:45.320]   strategic points to put a constraint. Normally, what you
[00:48:45.320 --> 00:48:49.840]   do here is that there are some sort of fundamentally ambiguous
[00:48:49.840 --> 00:48:53.480]   places where the compiler might lose track of what is a
[00:48:53.480 --> 00:48:56.560]   sensible thing to do. Right? So think about reshapes, for
[00:48:56.560 --> 00:48:59.200]   example, right? Like in this case, sort of like all the
[00:48:59.200 --> 00:49:01.960]   values get shuffled around, it's very hard to know like, okay,
[00:49:01.960 --> 00:49:07.000]   where, where do the parallelization axes go after a
[00:49:07.000 --> 00:49:09.720]   reshape or a gatter or something like that.
[00:49:09.720 --> 00:49:16.360]   I've had this question for a while is, well, I know, maybe I
[00:49:16.360 --> 00:49:20.160]   haven't seen an API for this, but can like in compiled
[00:49:20.160 --> 00:49:24.120]   languages, you can tell them like, hey, compile quick, or do
[00:49:24.120 --> 00:49:29.840]   your best. But in JAX is, I haven't seen like different
[00:49:29.840 --> 00:49:34.200]   modes of compilation. You know, if this exists,
[00:49:34.200 --> 00:49:39.600]   for for accelerators, I think there isn't like there isn't a
[00:49:39.600 --> 00:49:44.360]   sort of Oh, big Oh, mode. For CPUs, I think you can there's
[00:49:44.360 --> 00:49:47.240]   there's like a flag somewhere where you can sort of tweak the
[00:49:47.240 --> 00:49:51.600]   LLVM optimization level that you use. That's basically passed to
[00:49:51.600 --> 00:49:52.120]   LLVM.
[00:49:54.840 --> 00:49:59.440]   But using that, that I don't know, maybe I should just put it
[00:49:59.440 --> 00:50:02.600]   as an issue in the JAX repo. Because when you're debugging,
[00:50:02.600 --> 00:50:06.080]   you don't you don't want XLA to take like a long time. But I
[00:50:06.080 --> 00:50:09.120]   guess if you're going to run something that takes days, then
[00:50:09.120 --> 00:50:13.840]   I mean, even if it takes an hour compiling, and you wouldn't
[00:50:13.840 --> 00:50:16.960]   mind if it finishes quicker.
[00:50:16.960 --> 00:50:21.360]   Yeah, I mean, this could be an interesting feature request to
[00:50:21.360 --> 00:50:28.440]   actually to add to add sort of a fast mode to the for the
[00:50:28.440 --> 00:50:31.000]   accelerator compilers, right? That's what you're saying.
[00:50:31.000 --> 00:50:40.280]   Okay.
[00:50:40.280 --> 00:50:46.080]   So where was I?
[00:50:46.080 --> 00:50:46.680]   Okay.
[00:50:46.680 --> 00:50:55.920]   So what I just wanted to show as an example is, is sort of how
[00:50:55.920 --> 00:51:00.000]   quickly this gets gets ugly. Even if you would want to just
[00:51:00.000 --> 00:51:04.200]   manually write sort of the partitioning code in like, right
[00:51:04.200 --> 00:51:08.800]   now in P map or V map or X map. And if you wanted to do it, so
[00:51:08.800 --> 00:51:14.120]   it's sort of like a disputed map mode, right? So imagine you're
[00:51:14.120 --> 00:51:19.680]   like, doing model parallelism. So I call the X model here. And
[00:51:19.680 --> 00:51:22.760]   it's basically like partitioning over the feature dimension. And
[00:51:22.760 --> 00:51:25.240]   so there's a whole bunch of things you need to do to
[00:51:25.240 --> 00:51:30.720]   actually do the communication, right? Like you have to figure
[00:51:30.720 --> 00:51:35.400]   out, okay, which which piece of this of the pie do I have and
[00:51:35.400 --> 00:51:40.040]   sort of communicate that to the to the other devices, like,
[00:51:40.080 --> 00:51:44.400]   like, gather the outputs. Whereas with Pj, this is sort of
[00:51:44.400 --> 00:51:47.160]   like a very simple thing where you can say like, okay, I have
[00:51:47.160 --> 00:51:49.720]   some inputs, which is basically like, there's a batch access,
[00:51:49.720 --> 00:51:52.760]   there's like, or data access, like a feature access, which I
[00:51:52.760 --> 00:51:56.920]   call data and model for data and model parallelism. And then when
[00:51:56.920 --> 00:52:00.240]   you use non you just indicate that, that is not not not
[00:52:00.240 --> 00:52:04.720]   partition, that that access is not partition. And then it will
[00:52:04.720 --> 00:52:06.920]   just basically it will essentially produce this code
[00:52:06.920 --> 00:52:09.160]   for you, right? Like it will produce the same code as the
[00:52:09.160 --> 00:52:12.880]   distributed map here. But it's sort of very explicit. And
[00:52:12.880 --> 00:52:15.960]   there's no way to make a mistake. And if you do this in a
[00:52:15.960 --> 00:52:19.640]   much bigger program, of course, then this really saves you a lot
[00:52:19.640 --> 00:52:20.000]   of code.
[00:52:20.000 --> 00:52:30.240]   One thing also coming back to something like deep speed is
[00:52:30.240 --> 00:52:36.280]   that so what x xla do is what we call SPMD sharding. So it takes
[00:52:36.280 --> 00:52:40.600]   each operation and it creates a program that acts on a sub
[00:52:40.600 --> 00:52:45.000]   slice of that of that of its inputs, and it generates a sub
[00:52:45.000 --> 00:52:50.640]   just a piece of the outputs. And in each program will will
[00:52:50.640 --> 00:52:54.720]   execute all all operations. Now in pipelining, that's a bit
[00:52:54.720 --> 00:52:58.280]   different. Because actually, what you do is typically, all
[00:52:58.280 --> 00:53:01.200]   the inputs and all the outputs are actually produced by the
[00:53:01.200 --> 00:53:04.400]   program still, but you get sub programs that just they
[00:53:04.400 --> 00:53:10.840]   actually only compute a part of the compute graph, right? So
[00:53:10.840 --> 00:53:14.880]   it's so you're splitting, you're distributing not the data over
[00:53:14.880 --> 00:53:19.600]   devices, but you're actually also distributing the
[00:53:19.600 --> 00:53:23.720]   operations. And so it turns out that in very big scale, like for
[00:53:23.720 --> 00:53:27.600]   example, gp3, that actually means you need less
[00:53:27.600 --> 00:53:31.360]   communication. And you can sort of communicate asynchronously,
[00:53:31.360 --> 00:53:33.560]   right? Like that's where the pipeline comes from, where you
[00:53:33.560 --> 00:53:37.280]   can sort of start on a batch, and then your previous batch is
[00:53:37.280 --> 00:53:41.880]   sent to the next device, and you get this sort of like this
[00:53:41.880 --> 00:53:50.240]   delayed programming with the with the micro batches. And
[00:53:50.240 --> 00:53:52.920]   there's also some cool ideas about how you could generalize
[00:53:52.920 --> 00:53:56.200]   this. So for example, there's the pathways. So this is from
[00:53:56.200 --> 00:53:59.720]   the Google AI blog, where pathways is like a sort of
[00:53:59.720 --> 00:54:04.400]   networking library, where you can make this pipeline, not just
[00:54:04.400 --> 00:54:06.960]   static, but you can actually make it dynamic, right? So you
[00:54:06.960 --> 00:54:12.680]   would have the data on the side, which sub program it needs to go
[00:54:12.680 --> 00:54:16.640]   next, and you get this sort of dynamic routing, which creates
[00:54:16.640 --> 00:54:20.080]   sparsity, and which allows you to sort of have a very big
[00:54:20.080 --> 00:54:25.320]   network that still operates relatively quickly. And that is
[00:54:25.320 --> 00:54:30.480]   also massively distributed across many devices. Here, again,
[00:54:30.480 --> 00:54:33.080]   you could ask the same questions as with the mapping style
[00:54:33.080 --> 00:54:37.760]   parallelization, like, do you create these stages explicitly?
[00:54:37.760 --> 00:54:41.640]   Or do you sort of have a magic optimization passage just
[00:54:41.640 --> 00:54:45.720]   decides, okay, given a program and given us some certain number
[00:54:45.720 --> 00:54:50.320]   of devices, find the optimal number of or find the optimal
[00:54:50.320 --> 00:54:53.200]   sort of stage boundaries, right? Like you could you could
[00:54:53.200 --> 00:54:57.560]   imagine that this sort of has an explicit and implicit API that
[00:54:57.560 --> 00:55:01.200]   you could build around it. Today, we don't have that, right?
[00:55:01.200 --> 00:55:08.440]   So we definitely want to support that eventually. So this, I
[00:55:08.440 --> 00:55:10.720]   think, is one of the interesting, one of the
[00:55:10.720 --> 00:55:14.760]   interesting side effects of pipelining, I think, is that it
[00:55:14.760 --> 00:55:19.120]   sort of enables the really big players to build something
[00:55:19.120 --> 00:55:23.160]   really large scale. But also some of that technology can also
[00:55:23.160 --> 00:55:26.680]   be given back, sort of to the community and sort of improve
[00:55:26.680 --> 00:55:31.480]   the democratization of of AI a bit. Because you can take these
[00:55:31.480 --> 00:55:34.000]   really large models, at least when they're trained or when
[00:55:34.000 --> 00:55:38.200]   they're pre trained. And now you sort of run each stage on the
[00:55:38.200 --> 00:55:40.240]   same device, and you're just streaming in and out of the
[00:55:40.240 --> 00:55:46.160]   parameters. Making clever use of the RAM that your host machine
[00:55:46.160 --> 00:55:50.760]   has, or even like an SSD or HTTP, like, like sort of backup
[00:55:50.760 --> 00:55:55.280]   storage. And now you can find to these really big models, even
[00:55:55.280 --> 00:55:59.400]   though they have way more parameters than you have GPU
[00:55:59.400 --> 00:55:59.800]   memory.
[00:55:59.800 --> 00:56:09.000]   I have a comment about this. I think this is not like I think
[00:56:09.000 --> 00:56:13.200]   this could go to the day to day of certain people, especially
[00:56:13.200 --> 00:56:17.400]   the Kaggle community, maybe Sanjan has more experience there.
[00:56:17.400 --> 00:56:22.160]   But I believe Kaggle gives like competitors, I don't know if it
[00:56:22.160 --> 00:56:26.760]   depends on the competition, but you do have a TPU environment,
[00:56:26.760 --> 00:56:32.760]   correct me if I'm wrong. And I have the sensation and nobody
[00:56:32.760 --> 00:56:40.800]   uses it. And maybe it's because it's, it's kind of hard like to
[00:56:40.800 --> 00:56:45.920]   leverage it, maybe it's, it would take like, some
[00:56:45.920 --> 00:56:50.280]   investment into how to figure what is the optimal way, but
[00:56:50.280 --> 00:56:54.720]   then like something like Pidget sounds like, I will, I mean,
[00:56:54.720 --> 00:57:01.320]   just use it and it will figure out a way to, to speed it up,
[00:57:01.320 --> 00:57:05.920]   which I think, I don't know how much the Kaggle community has
[00:57:05.960 --> 00:57:11.520]   explored using the TPUs, but it's something that could, I
[00:57:11.520 --> 00:57:15.080]   mean, help, help people in this kind of settings.
[00:57:15.080 --> 00:57:22.720]   Yeah, I think I'm not sure if there's sort of the one VM setup
[00:57:22.720 --> 00:57:26.280]   has been supported, like the TPU VMs, whether that's also
[00:57:26.280 --> 00:57:28.240]   supported on Kaggle, I'm not sure about.
[00:57:28.240 --> 00:57:32.640]   I think it's the same thing. Oh, I know, it also applies to
[00:57:32.640 --> 00:57:36.160]   Colab, right? In Colab, you get eight, I guess it's the smallest
[00:57:36.160 --> 00:57:44.640]   unit. Yeah, but in Colab, you do still have the old two VM set,
[00:57:44.640 --> 00:57:49.400]   right? So you're sending off the compute to the device, which
[00:57:49.400 --> 00:57:53.560]   does limit, which does somewhat limit what you can do, right? So
[00:57:53.560 --> 00:57:56.960]   you'll have to sort of put everything on the device.
[00:57:56.960 --> 00:58:00.040]   On the device, you mean, because you have to send the data?
[00:58:01.280 --> 00:58:07.040]   Yeah, I see. But even then, wouldn't, like, something is
[00:58:07.040 --> 00:58:11.160]   help, like, to speed up the training?
[00:58:11.160 --> 00:58:17.600]   Yeah, so I guess, I mean, also, the issue with Kaggle, I think
[00:58:17.600 --> 00:58:21.040]   is that for a long time, we've seen that people have been very
[00:58:21.040 --> 00:58:23.560]   eager to try out machine learning, sort of, or deep
[00:58:23.560 --> 00:58:27.560]   learning approaches, right? And then these would sort of be the
[00:58:27.560 --> 00:58:31.600]   very sort of avant garde of machine learning approaches,
[00:58:31.600 --> 00:58:36.000]   like something like GPT-3. It really depends on the type of
[00:58:36.000 --> 00:58:39.040]   data that you have, right? Like, if you have like tabular data,
[00:58:39.040 --> 00:58:42.000]   and you don't have a lot of it, it's still pretty hard to
[00:58:42.000 --> 00:58:47.240]   basically be like, like, like decision trees, right?
[00:58:47.240 --> 00:58:51.360]   Oh, yeah, yeah. I meant more like the image competitions.
[00:58:53.400 --> 00:58:59.440]   Like, I mean, if, if you're, if you can only train on a batch
[00:58:59.440 --> 00:59:04.680]   of two, I guess it would really help if you could train on a
[00:59:04.680 --> 00:59:09.880]   batch of 16, something like that. And I don't know if the
[00:59:09.880 --> 00:59:17.360]   IO would screw with that. But I just have the sensation that it
[00:59:17.360 --> 00:59:22.440]   is like, I've never seen a notebook that turns on in a TPU
[00:59:22.440 --> 00:59:29.080]   setting. And maybe it's because it's not so easy without
[00:59:29.080 --> 00:59:33.600]   something like PGT-3. You have to do everything. And I mean,
[00:59:33.600 --> 00:59:34.800]   you might be losing time.
[00:59:34.800 --> 00:59:39.800]   Yeah, I'm also I'm not sure I would be interested to know if
[00:59:39.800 --> 00:59:42.800]   people have already tried the approach of fine tuning a large
[00:59:42.800 --> 00:59:45.600]   pre trained model and try to win a Kaggle competition with that.
[00:59:45.600 --> 00:59:49.560]   I would be curious. I'm not sure if that's even like, suppose,
[00:59:49.840 --> 00:59:53.200]   are you allowed to do that? I guess everything goes right in
[00:59:53.200 --> 00:59:53.520]   Kaggle.
[00:59:53.520 --> 00:59:59.600]   There are some restrictions, but like, that is part of a lot of
[00:59:59.600 --> 01:00:02.640]   winning models. It's also like a part of ensembles.
[01:00:02.640 --> 01:00:07.800]   Yeah, yeah. Okay. I think I actually I should look at Kaggle
[01:00:07.800 --> 01:00:11.440]   again. It's been it's been a bit of a bit of time since I since
[01:00:11.440 --> 01:00:15.120]   I've last looked. I see what's the winning approach now.
[01:00:15.640 --> 01:00:23.600]   Hmm. Ensembles. Can you for ensembles put a model per
[01:00:23.600 --> 01:00:24.320]   device?
[01:00:24.320 --> 01:00:28.680]   Yeah, you can do that. You can you can Vmap and use that to
[01:00:28.680 --> 01:00:29.440]   train an ensemble.
[01:00:29.440 --> 01:00:35.320]   That would be interesting.
[01:00:35.320 --> 01:00:42.040]   I don't know, giving Kaggler some ideas how to use the TPUs
[01:00:42.040 --> 01:00:47.120]   because I never see them used. I imagine maybe if they if they
[01:00:47.120 --> 01:00:51.480]   have something like JAX, it might be simpler, because on
[01:00:51.480 --> 01:00:56.400]   TensorFlow, I don't know how easy it is to use TPUs. And
[01:00:56.400 --> 01:01:00.640]   actually, there's a question about like differences between
[01:01:00.640 --> 01:01:01.840]   TensorFlow and JAX.
[01:01:01.840 --> 01:01:06.440]   What's the question?
[01:01:06.440 --> 01:01:08.840]   The exact question is
[01:01:08.840 --> 01:01:12.680]   asking the comparison between how does JAX compare against
[01:01:12.680 --> 01:01:15.920]   other deep learning frameworks, broadly speaking?
[01:01:15.920 --> 01:01:21.160]   Broadly speaking, well, I guess, of course, the really the
[01:01:21.160 --> 01:01:25.400]   number one biggest difference is that it uses a functional API,
[01:01:25.400 --> 01:01:28.400]   right? Like there's no that like in every framework right now
[01:01:28.400 --> 01:01:33.080]   that actually there's there's like a variable abstraction in
[01:01:33.320 --> 01:01:38.320]   part of the fundamental sort of graph.
[01:01:38.320 --> 01:01:44.040]   There are some technical differences, I think, of course,
[01:01:44.040 --> 01:01:47.280]   the NumPy is sort of like making the NumPy the central API for
[01:01:47.280 --> 01:01:48.840]   compute, I think is another one.
[01:01:48.840 --> 01:01:53.960]   The functional transformations, I think, are also somewhat
[01:01:53.960 --> 01:01:56.880]   unique, the way they work also
[01:01:56.880 --> 01:02:01.560]   with this sort of tracing mechanism.
[01:02:02.080 --> 01:02:06.400]   That I think is also interesting.
[01:02:06.400 --> 01:02:10.840]   Also, I think like,
[01:02:10.840 --> 01:02:15.600]   what I find curious also is that the way gradients are
[01:02:15.600 --> 01:02:19.000]   calculated, I think is also quite unique in JAX that
[01:02:19.000 --> 01:02:21.600]   there's sort of this principle where you can do both forward
[01:02:21.600 --> 01:02:25.760]   and backward gradients. I think this used to exist in
[01:02:25.760 --> 01:02:28.520]   frameworks like FIANO, but then sort of disappeared and
[01:02:28.520 --> 01:02:29.720]   reappeared again in JAX.
[01:02:30.400 --> 01:02:37.920]   And also sort of making XLA really the sort of
[01:02:37.920 --> 01:02:43.120]   universal compiler and working directly with that and making
[01:02:43.120 --> 01:02:45.680]   the IR work with that is sort of unique, I think.
[01:02:45.680 --> 01:02:49.720]   This is pretty nice if you want to create new transformations,
[01:02:49.720 --> 01:02:52.800]   right? Because I think what happened in the old frameworks
[01:02:52.800 --> 01:02:57.960]   is that people wrote specialized functions to make things faster,
[01:02:57.960 --> 01:03:00.240]   right? So, for example, if you had like an activation
[01:03:00.240 --> 01:03:03.120]   function that was used a lot, people would make a custom
[01:03:03.120 --> 01:03:04.800]   function for that, right?
[01:03:04.800 --> 01:03:08.880]   Because otherwise it would just like you wouldn't have a GPU
[01:03:08.880 --> 01:03:11.560]   kernel for it. And then for every little sort of like
[01:03:11.560 --> 01:03:15.400]   elementary function that it would use, it would move the
[01:03:15.400 --> 01:03:18.200]   stuff in and out of the GPU memory.
[01:03:18.200 --> 01:03:22.840]   But because JAX was built directly on XLA and XLA relies
[01:03:22.840 --> 01:03:25.320]   on fusion, right? So if you write your custom activation
[01:03:25.320 --> 01:03:28.240]   function, it will just it will bundle all the ops together and
[01:03:28.240 --> 01:03:30.000]   generate a GPU kernel for you.
[01:03:30.000 --> 01:03:34.360]   We never had the temptation, like JAX never had the
[01:03:34.360 --> 01:03:37.800]   temptation to sort of overload the intermediate language with
[01:03:37.800 --> 01:03:40.760]   custom operations for the sake of performance, right?
[01:03:40.760 --> 01:03:43.880]   That I think has made the implementation a lot cleaner,
[01:03:43.880 --> 01:03:46.760]   which makes it easy to to develop new things and also
[01:03:46.760 --> 01:03:50.800]   write custom. Like I haven't seen sort of other frameworks
[01:03:50.800 --> 01:03:54.360]   have this community where people actually create, like try to
[01:03:54.360 --> 01:03:56.800]   create new transformations on top of
[01:03:56.800 --> 01:04:02.000]   the IR that the intermediate
[01:04:02.000 --> 01:04:05.160]   representation that, for example, TensorFlow or PyTorch
[01:04:05.160 --> 01:04:08.840]   provides, which I think makes sense because for legacy
[01:04:08.840 --> 01:04:12.360]   reasons, they have like perhaps maybe even like 100 or more
[01:04:12.360 --> 01:04:15.240]   ops. And JAX has very few.
[01:04:15.240 --> 01:04:21.240]   I have a curious, I mean, there's been a curiosity as to
[01:04:21.760 --> 01:04:23.360]   why TensorFlow
[01:04:23.360 --> 01:04:25.680]   like still
[01:04:25.680 --> 01:04:31.440]   doesn't rely like fully on XLA as JAX does. It's kind of
[01:04:31.440 --> 01:04:34.640]   curious because XLA lives in TensorFlow.
[01:04:34.640 --> 01:04:40.480]   Yeah, that's true. But then sort of the TensorFlow compute
[01:04:40.480 --> 01:04:44.840]   model is older than XLA, right? And so there are some things
[01:04:44.840 --> 01:04:48.360]   that the TensorFlow model can do that XLA can't, right? Like
[01:04:48.360 --> 01:04:54.280]   the most obvious one being dynamic shapes, right? So the
[01:04:54.280 --> 01:04:59.800]   upside of having sort of this repertoire of ops that have a
[01:04:59.800 --> 01:05:02.680]   custom kernel is that you don't need to compile afterwards,
[01:05:02.680 --> 01:05:06.760]   right? And so now you can have sort of dynamic shapes, right?
[01:05:06.760 --> 01:05:11.080]   And XLA today cannot yet do that, right? So it cannot
[01:05:11.080 --> 01:05:13.880]   generate programs. This is also the main limitation of JAX,
[01:05:13.880 --> 01:05:16.760]   right? Like once you compile something, which is kind of what
[01:05:16.760 --> 01:05:21.400]   you need to get good performance out of it, you need to have
[01:05:21.400 --> 01:05:25.120]   static shapes, right? Now, I think in deep learning, actually,
[01:05:25.120 --> 01:05:27.000]   that rarely actually becomes an issue.
[01:05:27.000 --> 01:05:33.480]   But that is more of an issue for serving, in my experience.
[01:05:33.480 --> 01:05:43.320]   And I don't know if you can comment on this, but I noticed
[01:05:43.320 --> 01:05:50.760]   that JAX to TensorFlow now has like these dynamic shapes, but
[01:05:50.760 --> 01:05:56.880]   JAX doesn't. So it's curious, like, how was that achieved?
[01:05:56.880 --> 01:06:06.040]   Yeah, so JAX to TF, if you sort of, if you use the TensorFlow
[01:06:06.040 --> 01:06:11.720]   ops, it will support dynamic shapes. Now, supporting dynamic
[01:06:11.720 --> 01:06:15.160]   shapes in JAX in principle, wouldn't be such a big deal,
[01:06:15.160 --> 01:06:20.840]   right? But getting it compiled into XLA is what is the main
[01:06:20.840 --> 01:06:25.560]   issue, right? So in principle, you could add dynamic shapes
[01:06:25.560 --> 01:06:30.200]   relatively easily into the JAX intermediate language. It's sort
[01:06:30.200 --> 01:06:33.960]   of the JAX to TF transformation is already kind of doing that.
[01:06:33.960 --> 01:06:40.360]   Yeah, they, well, I mean, I implemented something with that
[01:06:40.360 --> 01:06:48.440]   recently. And like, that wasn't, that wasn't always there. I
[01:06:48.440 --> 01:06:55.240]   remember I had to specify the batch dimensions directly, and
[01:06:55.240 --> 01:07:00.080]   then it was kind of awkward. Because like, if you wanted to
[01:07:00.080 --> 01:07:03.200]   produce like, something in TensorFlow serving or something,
[01:07:03.200 --> 01:07:07.080]   you had to specify like, oh, I only support like one, four,
[01:07:07.120 --> 01:07:15.400]   16. And yeah, like, you really want to just specify whatever
[01:07:15.400 --> 01:07:19.400]   and you can do that now. But I guess as you're saying that is
[01:07:19.400 --> 01:07:24.520]   not part of XLA. That is just part of how you convert it to
[01:07:24.520 --> 01:07:25.200]   TensorFlow.
[01:07:25.200 --> 01:07:33.280]   Yeah. Yeah. So maybe in the future, that will change, right?
[01:07:33.280 --> 01:07:39.920]   But there is a related question. Maybe I don't know if you're
[01:07:39.920 --> 01:07:46.640]   going to cover it. But like, if like JAX is suitable for like
[01:07:46.640 --> 01:07:48.440]   industry applications.
[01:07:48.440 --> 01:07:53.920]   Yeah, so I think you've already mentioned right, like, like a
[01:07:53.920 --> 01:07:59.520]   big downside, which is the dynamic shapes. JAX has been
[01:07:59.520 --> 01:08:03.240]   very research focused. And I think it's kind of nice to think
[01:08:03.240 --> 01:08:11.120]   that way. For FLAGS it's sort of the same. JAX-to-TF has been
[01:08:11.120 --> 01:08:15.400]   our sort of main way to avoid having to think too much about
[01:08:15.400 --> 01:08:20.840]   serving and sort of tap into all the infrastructure that was
[01:08:20.840 --> 01:08:28.440]   already built for serving into TensorFlow. Yeah, so I think I
[01:08:28.440 --> 01:08:31.800]   think for for for sort of at least the short term future that
[01:08:31.800 --> 01:08:34.200]   that will probably be the high one.
[01:08:34.200 --> 01:08:42.080]   I'd actually say if you manage to compile it to to like a saved
[01:08:42.080 --> 01:08:49.160]   model, then JAX is good for for the industry. If you know, like
[01:08:49.160 --> 01:08:52.800]   the TensorFlow ecosystem, I know, like I use it a lot. So
[01:08:52.800 --> 01:08:57.960]   for me, that's kind of enough. I don't know if people have other
[01:08:57.960 --> 01:09:00.880]   needs, then you can hesitate.
[01:09:00.880 --> 01:09:06.000]   Yeah, so this is one of the questions that we also wonder
[01:09:06.000 --> 01:09:09.760]   about, right? Like, can JAX-to-TF actually just solve
[01:09:09.760 --> 01:09:15.000]   pretty much 99% of the serving needs? Or is there something
[01:09:15.000 --> 01:09:19.200]   we're sort of missing? I think so far it has worked, right? Like
[01:09:19.200 --> 01:09:22.520]   so far, like, for example, the sort of dynamic shapes you
[01:09:22.520 --> 01:09:25.600]   mentioned, like, we figured out that that was sort of something
[01:09:25.600 --> 01:09:31.600]   that was needed in the in the in the transformation to TF, and
[01:09:31.600 --> 01:09:36.320]   then that was added. So and there are a few more things like
[01:09:36.320 --> 01:09:40.360]   that, right? Like sort of like, figuring out the sort of
[01:09:40.360 --> 01:09:45.600]   complexities of translating from one IR into the other, but but I
[01:09:45.600 --> 01:09:48.680]   think nothing really fundamental is, is broken there.
[01:09:54.840 --> 01:10:02.080]   Okay. I'll pick up the slides again a bit. I guess, how much
[01:10:02.080 --> 01:10:04.400]   time do we have still actually? Or is there a
[01:10:04.400 --> 01:10:08.120]   if it's okay for you to go ahead, I know we scheduled an
[01:10:08.120 --> 01:10:12.120]   hour, but if you'd like to wrap up, we can wrap up as well.
[01:10:12.120 --> 01:10:17.400]   Okay, no, I'll go a bit more, but just for me to know. Thank
[01:10:17.400 --> 01:10:24.440]   you. So I guess actually, here are some of the more, perhaps
[01:10:24.440 --> 01:10:27.000]   ambiguous, but perhaps also more exciting stuff is because
[01:10:27.000 --> 01:10:30.000]   here, really, everyone can contribute, and you don't need
[01:10:30.000 --> 01:10:37.360]   the 1000 GPUs in this quadrant of the helicopter view. The
[01:10:37.360 --> 01:10:40.240]   other part of research is this to actually do research and try
[01:10:40.240 --> 01:10:44.640]   new ideas, right? And I think there's also been a lot of
[01:10:44.640 --> 01:10:48.600]   exciting things happening there. And also in the JAX community.
[01:10:48.600 --> 01:10:53.400]   What's harder here is that this is not an engineering problem,
[01:10:53.400 --> 01:10:57.240]   right? Like this, this is just trying out new ideas, and sort
[01:10:57.240 --> 01:11:00.320]   of, there's infinite possibility here. So it's not an
[01:11:00.320 --> 01:11:03.960]   optimization problem, or just figuring out a nice API, you
[01:11:03.960 --> 01:11:09.440]   actually don't know what to expect. And we want to avoid
[01:11:09.440 --> 01:11:12.800]   getting actually getting stuck, right? Like what we're trying
[01:11:12.800 --> 01:11:19.080]   to not have the ML stack dictate what ideas you can actually try
[01:11:19.080 --> 01:11:22.960]   out. And so I think the only way to do that is to make API
[01:11:22.960 --> 01:11:27.520]   that are unopinionated, and sort of general. And that maybe where
[01:11:27.520 --> 01:11:30.880]   people will say, like, look, all I need is pipelining, you know,
[01:11:30.880 --> 01:11:34.800]   but, but maybe we should say, no, we actually want people to
[01:11:34.800 --> 01:11:38.600]   sort of fully have the possibility to try out all kinds
[01:11:38.600 --> 01:11:43.760]   of networking, or, you know, like, generalization also means
[01:11:43.760 --> 01:11:48.680]   that we can potentially catch an opportunity where someone
[01:11:48.680 --> 01:11:51.320]   figures out a new way to do things that turns out to be
[01:11:51.320 --> 01:11:53.800]   really great, right? So we should not just try to
[01:11:53.800 --> 01:11:56.880]   implement the status quo, but actually leave room for people
[01:11:56.880 --> 01:12:00.760]   to try out new things. I think Jackson transformations are
[01:12:00.760 --> 01:12:03.800]   really cool examples on that, right? Like, the basic
[01:12:03.800 --> 01:12:07.040]   transformation, the basic things, right, are just taking
[01:12:07.040 --> 01:12:11.040]   gradients, compiling, maybe parallelization, right? But we
[01:12:11.040 --> 01:12:13.400]   want to try new things, right? Like, we don't just want to do
[01:12:13.400 --> 01:12:16.640]   data parallelism, we want to do model parallelism, or splitting
[01:12:16.640 --> 01:12:21.600]   up an ocean into segments. And we maybe even want to combine
[01:12:21.600 --> 01:12:24.200]   multiple of these things, right? So why should there only be one
[01:12:24.200 --> 01:12:27.680]   gradient, maybe we want information about the Hessian,
[01:12:27.680 --> 01:12:30.400]   maybe we want natural gradients, maybe one third order
[01:12:30.400 --> 01:12:35.280]   gradients. Maybe we want per example gradients, right, which
[01:12:35.280 --> 01:12:40.240]   turns out to be great for differential price, right? This
[01:12:40.240 --> 01:12:43.080]   Jax can also do, right, even though it's perhaps not
[01:12:43.080 --> 01:12:46.160]   something that is sort of a regular thing today, but it's
[01:12:46.160 --> 01:12:49.560]   useful, and it has mathematical interest. And so having this
[01:12:49.560 --> 01:12:53.960]   general framework for doing transformations, I think, is one
[01:12:53.960 --> 01:12:57.520]   of the things that makes Jax very powerful. And of course,
[01:12:57.520 --> 01:12:59.960]   there's also the pretty simple IR. So if someone actually
[01:12:59.960 --> 01:13:03.480]   figures a new transformation altogether, it's also pretty
[01:13:03.480 --> 01:13:07.680]   easy to add that, right? And it's pretty, like most programs
[01:13:07.680 --> 01:13:11.000]   will also run relatively fast, because the XLA compiler can
[01:13:11.000 --> 01:13:14.280]   sort of take these very minimal building blocks, right, like
[01:13:14.760 --> 01:13:18.440]   linear operations, exponents, logarithms, sums, whatever, and
[01:13:18.440 --> 01:13:21.520]   it can sort of fuse it together into a fast program, because it
[01:13:21.520 --> 01:13:26.360]   can generate the GPU programs for you. And so I think this
[01:13:26.360 --> 01:13:29.720]   very much lowers the entry barrier to adding new things to
[01:13:29.720 --> 01:13:37.840]   the ML stack. And so there are some next generation of
[01:13:37.840 --> 01:13:40.800]   transformations already in the work, right? So perhaps
[01:13:40.800 --> 01:13:43.720]   something to work around the static shapes, right, to sort
[01:13:43.720 --> 01:13:50.080]   of have the ability to automatically add masking. So
[01:13:50.080 --> 01:13:54.160]   you can feed a dynamically shaped input into a statically
[01:13:54.160 --> 01:14:00.000]   shaped program. There's now experimental work on taking a
[01:14:00.000 --> 01:14:03.000]   program and sparsifying it, right, so allowing it to
[01:14:03.000 --> 01:14:07.640]   operate on sparse inputs. There are sort of what I would call
[01:14:07.640 --> 01:14:14.360]   advanced gradient things like being able to differentiate
[01:14:14.360 --> 01:14:19.760]   through solvers and sort of differentiating implicit
[01:14:19.760 --> 01:14:24.760]   functions, having faster sort of higher order gradients, right,
[01:14:24.760 --> 01:14:28.320]   where you can really calculate the full Hessian quickly, in a
[01:14:28.320 --> 01:14:33.120]   sort of semi optimal way. There's been work done on the
[01:14:33.120 --> 01:14:36.320]   sort of experimental JXTF transformation, which we were
[01:14:36.320 --> 01:14:39.760]   just talking about. And I think I hope actually that there will
[01:14:39.760 --> 01:14:43.080]   be more sort of very interesting transformations. I'm thinking
[01:14:43.080 --> 01:14:47.240]   maybe other export options, like maybe someone can write like an
[01:14:47.240 --> 01:14:54.200]   ONNX output, or perhaps something to facilitate sort of
[01:14:54.200 --> 01:15:01.040]   production, like speeding up a productionized model, right,
[01:15:01.040 --> 01:15:05.200]   like transformations to allow you to use quantization or
[01:15:06.200 --> 01:15:17.600]   sparsity options in modern hardware. And so one of the
[01:15:17.600 --> 01:15:21.880]   important things Flex has to do is it has to like support all
[01:15:21.880 --> 01:15:26.520]   these magic like Jax transformations, right. But it's
[01:15:26.520 --> 01:15:30.920]   already sort of in the name. Like these transformations you
[01:15:30.920 --> 01:15:32.920]   can do and you can do them correctly because you're
[01:15:32.920 --> 01:15:35.840]   transforming a function, right, which in this case has to be
[01:15:35.840 --> 01:15:39.200]   really like a pure function, right. Because if something has
[01:15:39.200 --> 01:15:41.520]   side effects, then for example, it's really hard to write a
[01:15:41.520 --> 01:15:46.320]   correct mapping transformation. But of course, with Linen, we
[01:15:46.320 --> 01:15:52.520]   actually add state to Jax. And now it becomes less obvious how
[01:15:52.520 --> 01:15:57.240]   you for example, do like VMAF or any other transformation. And so
[01:15:57.240 --> 01:16:00.640]   what Jax, what Flex transformations do is they take
[01:16:00.640 --> 01:16:04.680]   the existing Jax transformations, and they augment
[01:16:04.680 --> 01:16:08.400]   them with state and randomness, right. So they allow you to put
[01:16:08.400 --> 01:16:13.360]   PRNG sequences and variables and lift those into a
[01:16:13.360 --> 01:16:16.760]   transformation. And sort of how this internally works is it will
[01:16:16.760 --> 01:16:21.760]   it will take the sub modules, and it will change them, it will
[01:16:21.760 --> 01:16:25.880]   turn them into a pure function, then feed it into the Jax
[01:16:25.880 --> 01:16:31.440]   transformation, and then sort of feed the output back into the
[01:16:31.440 --> 01:16:36.360]   state machine. And so what you can then do, for example, is you
[01:16:36.360 --> 01:16:39.800]   can say like, hey, I have like a dot product attention layer. And
[01:16:39.800 --> 01:16:43.000]   now I want to VMAF it. And I want to say like, oh, actually,
[01:16:43.000 --> 01:16:46.440]   I'm also going to VMAF the parameters, right. So I might
[01:16:46.440 --> 01:16:54.760]   actually add an extra dimension to the parameters. So now I have
[01:16:54.760 --> 01:17:00.440]   like a multi head attention layer, right. And, and you can
[01:17:00.440 --> 01:17:06.920]   decide how that affects the PRNG sequence as well, right. So, of
[01:17:06.920 --> 01:17:09.960]   course, I want different parameters, right, I don't want
[01:17:09.960 --> 01:17:13.160]   the parameters to be initialized in the same way. So you should
[01:17:13.160 --> 01:17:18.200]   actually split the RNG when you when you're mapping it. But
[01:17:18.200 --> 01:17:21.240]   perhaps I want dropout to be the same, right? Maybe I want to use
[01:17:21.240 --> 01:17:24.760]   the same dropout mask and work for some reason. And now you can
[01:17:24.800 --> 01:17:32.240]   you can control all of them. And so I think here, again, you can
[01:17:32.240 --> 01:17:35.800]   sort of think about interesting ways in which you could not just
[01:17:35.800 --> 01:17:39.000]   apply these transformations sort of in your training loop, but
[01:17:39.000 --> 01:17:41.760]   you can now put them directly inside your model, right. And so
[01:17:41.760 --> 01:17:44.760]   here, again, there's sort of interesting new ideas to come
[01:17:44.760 --> 01:17:51.160]   up. One sort of interesting idea is to use a combination of scan
[01:17:51.160 --> 01:17:53.720]   and the VMAF transformation, which is sort of gradient
[01:17:53.720 --> 01:17:58.280]   checkpointing, which allows you to have like constant compile
[01:17:58.280 --> 01:18:01.680]   time. Christian was just complaining about compile time,
[01:18:01.680 --> 01:18:06.480]   I think. So what you then do is you sort of roll up your model,
[01:18:06.480 --> 01:18:12.560]   right, in the depth of your model into a loop, basically.
[01:18:12.560 --> 01:18:16.520]   And so now, however, no matter how deep you make your residual
[01:18:16.520 --> 01:18:20.760]   network, it always has just this one layer that it just keeps on
[01:18:20.760 --> 01:18:23.800]   repeating with different parameters, right. And so now,
[01:18:23.800 --> 01:18:27.160]   the compile time is constant. And if you combine that with
[01:18:27.160 --> 01:18:30.680]   remap as well, you can also get this like square complexity
[01:18:30.680 --> 01:18:36.720]   memory consumption, right, because you can do optimal
[01:18:36.720 --> 01:18:41.240]   checkpointing. Actually, you could even add multiple layers
[01:18:41.240 --> 01:18:46.680]   of this and you could get even lower memory consumption. So
[01:18:46.680 --> 01:18:50.520]   this is one way in which you can sort of like, avoid like having
[01:18:50.520 --> 01:18:54.880]   exploding sort of memory and compile time if you try to train
[01:18:54.880 --> 01:18:59.880]   very deep models. But there are, I think, even more creative
[01:18:59.880 --> 01:19:02.400]   ideas that you could do, right. So you could try to sort of
[01:19:02.400 --> 01:19:05.680]   incorporate gradients into the network, or maybe you're running
[01:19:05.680 --> 01:19:08.920]   a solver inside the network, or you're doing something like
[01:19:08.920 --> 01:19:11.840]   meta-learning or custom gradients or whatever other
[01:19:11.840 --> 01:19:15.960]   thing. I think this is a pretty under-explored area where people
[01:19:15.960 --> 01:19:22.480]   could try new things. And generally, I think sort of the
[01:19:22.480 --> 01:19:25.680]   clues that we should use is that probably many ideas won't be
[01:19:25.680 --> 01:19:31.480]   exactly new ideas, but sort of using old solutions to new, on
[01:19:31.480 --> 01:19:36.080]   new problems, right. There's already a lot of science. And
[01:19:36.080 --> 01:19:40.160]   I'm really excited seeing a lot of libraries appear that use
[01:19:40.160 --> 01:19:45.920]   JAX to do science and speed it up with the NL stack, right. So
[01:19:45.920 --> 01:19:50.000]   to run it on GPUs and TPUs, like there's JAX-MD, for example,
[01:19:50.000 --> 01:19:54.280]   which does molecular dynamics on JAX, so allowing you to do a
[01:19:54.280 --> 01:19:58.720]   massively parallel. That type of stuff, I think, is really cool.
[01:19:58.720 --> 01:20:01.040]   And also gives us new challenges, right. So for
[01:20:01.040 --> 01:20:06.920]   example, we've been pretty restrictive in how D types work
[01:20:06.920 --> 01:20:10.880]   in FLAGS. We didn't think about it that much. So we basically,
[01:20:10.880 --> 01:20:13.880]   we default to float32. And then we support like half precision
[01:20:13.880 --> 01:20:16.800]   types for performance. But now there's people saying, hey, I'm
[01:20:16.800 --> 01:20:23.600]   doing like, I'm fitting like a wave function to, with a FLAGS
[01:20:23.600 --> 01:20:26.720]   neural net. And for that, I need complex numbers, right? Why?
[01:20:26.720 --> 01:20:29.440]   You should support complex numbers as well. Why should I
[01:20:29.440 --> 01:20:32.240]   always use float32? That's really cool. You know, like
[01:20:32.240 --> 01:20:35.400]   beforehand, we were like, complex numbers. Nobody needs
[01:20:35.400 --> 01:20:39.040]   that. But now, like, we've got other scientists that actually
[01:20:39.040 --> 01:20:42.400]   know that they're smarter than than male scientists, I guess.
[01:20:42.400 --> 01:20:45.760]   And so they know how complex numbers work. I mean, I already
[01:20:45.760 --> 01:20:51.960]   get Nance with float32. So I was there to try. But, but they
[01:20:51.960 --> 01:20:56.440]   know how to preserve like holomorphic functions, and they
[01:20:56.440 --> 01:20:59.680]   know how to make it work. And then we can support complex
[01:20:59.680 --> 01:21:06.680]   numbers and make them happy. I think on D type support, but
[01:21:06.680 --> 01:21:09.760]   generally on numerical precision, I think a lot of
[01:21:09.760 --> 01:21:13.440]   things is changing, right? NVIDIA has made their lives very
[01:21:13.440 --> 01:21:17.240]   hard when designing a very multi-purpose GPU that has a
[01:21:17.240 --> 01:21:21.080]   lot of new features that gives like ML, like ML
[01:21:21.080 --> 01:21:24.840]   infrastructure builders headaches. Because they support
[01:21:24.840 --> 01:21:30.000]   like a gazillion D types. And, and even like introduced, right,
[01:21:30.000 --> 01:21:33.680]   so they now have like a MatMul unit that can do like int8, int4
[01:21:33.680 --> 01:21:39.040]   truncated float32. It can have like this two out of four local
[01:21:39.040 --> 01:21:42.360]   sparsity pattern where you provide a bit mask and it will
[01:21:42.360 --> 01:21:48.200]   sort of like for every four parameters, there will be two
[01:21:48.200 --> 01:21:52.360]   masked out to zero. But how do you support this, right? And of
[01:21:52.360 --> 01:21:55.200]   course, this, it won't stay this way, right? Like there will be
[01:21:55.200 --> 01:21:58.120]   other accelerators that have slightly different sort of
[01:21:58.120 --> 01:22:04.920]   sparsity and quantization features. And how do you
[01:22:04.920 --> 01:22:08.600]   support this, right? Because now we're just basically using IEEE
[01:22:08.600 --> 01:22:12.480]   standards. But this is way more complex than that. And this is
[01:22:12.480 --> 01:22:19.160]   going to explode. And how to support that in JAX and Flex, I
[01:22:19.160 --> 01:22:23.680]   think this is still very much an open issue, right? And how do
[01:22:23.680 --> 01:22:26.440]   you support research, right? Like, of course, you can take a
[01:22:26.440 --> 01:22:31.880]   trained model and then quantize it to int8 and then run it. But
[01:22:31.880 --> 01:22:34.280]   perhaps you actually want to make use of the fact that this
[01:22:34.280 --> 01:22:37.520]   is much cheaper to run during training. But how do you do
[01:22:37.520 --> 01:22:40.640]   that, right? Like, how do you take gradients when you're doing
[01:22:40.640 --> 01:22:43.880]   math walls with int8, right? Like now you need to again,
[01:22:43.880 --> 01:22:47.080]   compose transformations, right? Because you might have to first
[01:22:47.080 --> 01:22:51.480]   take the gradient and then do the quantization, right? And
[01:22:51.480 --> 01:22:55.640]   then run it in a training. And this is going to be interesting,
[01:22:55.640 --> 01:23:00.400]   because this is going to cause problems. But I think JAX is
[01:23:00.400 --> 01:23:04.120]   very much, can't do it yet. Nobody can do it yet. But I
[01:23:04.120 --> 01:23:08.840]   think we're sort of in a unique starting position to support
[01:23:08.840 --> 01:23:13.880]   these guys. I think that will be very cool. And what I hope is
[01:23:13.880 --> 01:23:17.640]   that eventually, we'll reach the stage where we can use the
[01:23:17.640 --> 01:23:22.000]   building blocks, we can try really cool new creative ideas,
[01:23:22.000 --> 01:23:26.080]   and the ML stack won't limit us. And then if it actually works,
[01:23:26.080 --> 01:23:30.600]   it's very easy to scale up that great idea. Well, after we get
[01:23:30.600 --> 01:23:35.960]   funding for GPUs, of course, but then we can combine great ideas
[01:23:35.960 --> 01:23:40.160]   and using all of science together with massive scale and
[01:23:40.160 --> 01:23:47.160]   good engineering to create the perfect models, right. So what
[01:23:47.160 --> 01:23:50.120]   that means for us as sort of the infrastructure developers is
[01:23:50.120 --> 01:23:52.840]   that we should make sure that the basic building blocks are
[01:23:52.840 --> 01:23:58.240]   truly easy to use, and sort of safe. And that the APIs, I think
[01:23:58.280 --> 01:24:00.840]   are best functional, and definitely should be
[01:24:00.840 --> 01:24:05.920]   declarative. And, of course, we should create minimal friction
[01:24:05.920 --> 01:24:13.480]   for ML practitioners. And I hope that we can scale these ideas
[01:24:13.480 --> 01:24:18.000]   and also sort of make it still approachable for people to use,
[01:24:18.000 --> 01:24:21.080]   right. So we should be able, if a model works very well, we have
[01:24:21.080 --> 01:24:24.960]   a checkpoint, we should be able to get that on everyone's
[01:24:24.960 --> 01:24:28.440]   machine to fine tune and win Kaggle challenges, right? Like
[01:24:28.440 --> 01:24:30.240]   why are we not winning Kaggle challenges?
[01:24:30.240 --> 01:24:34.360]   That's a good question for Sandhyaan.
[01:24:34.360 --> 01:24:38.600]   I will end it with that.
[01:24:38.600 --> 01:24:43.520]   That's like a mic drop point.
[01:24:43.520 --> 01:24:49.200]   I do have something that it's kind of again, like a devil's
[01:24:49.200 --> 01:24:54.680]   advocate, because I went through it. I noticed, and I know if
[01:24:54.680 --> 01:24:59.280]   there's this kind of in the roadmap of maybe JAX or FLEX, or
[01:24:59.280 --> 01:25:06.280]   is it kind of, we'll solve it later. But if you know Keras, or
[01:25:06.280 --> 01:25:13.920]   you know PyTorch, suddenly JAX is much harder. For certain
[01:25:13.920 --> 01:25:19.440]   things, you have to kind of figure out how to do like
[01:25:19.440 --> 01:25:23.760]   certain details, obviously, because of the functional, like,
[01:25:24.320 --> 01:25:27.920]   I mean, functional programming is not that great in Python.
[01:25:27.920 --> 01:25:33.640]   It's kind of not like very focused on functional
[01:25:33.640 --> 01:25:39.000]   programming. And then also because I feel you have to be
[01:25:39.000 --> 01:25:43.840]   more explicit, which I guess is a trade off, right? It's kind of
[01:25:43.840 --> 01:25:50.160]   a trade off. But like, are there like, are you thinking about
[01:25:50.160 --> 01:25:55.680]   this? Is this kind of a thing of how to onboard new people into
[01:25:55.680 --> 01:26:02.000]   into JAX and FLEX, especially given like the huge, like
[01:26:02.000 --> 01:26:06.560]   hugging face event, I guess that a lot of people suddenly
[01:26:06.560 --> 01:26:15.400]   started creating FLEX models, but I think that that might be
[01:26:16.640 --> 01:26:23.440]   like, not a research issue, but more of a how to like a UX
[01:26:23.440 --> 01:26:26.000]   issue or something like that.
[01:26:26.000 --> 01:26:30.880]   So actually, we spend like, especially in the FLEX thing, we
[01:26:30.880 --> 01:26:35.440]   spend a lot of time thinking about this. We don't simply
[01:26:35.440 --> 01:26:38.720]   assume that everyone's like, oh, yeah, functional, functional
[01:26:38.720 --> 01:26:43.480]   paradigm is great. We don't think that everyone thinks like
[01:26:43.800 --> 01:26:50.800]   like me. Right. And like, I mean, I'm from a background, like I
[01:26:50.800 --> 01:26:54.520]   love languages like Haskell and Rust. I know I'm a bit of an
[01:26:54.520 --> 01:26:59.880]   oddball. I still prefer recursion of for loops, but I
[01:26:59.880 --> 01:27:08.560]   know that not everyone agrees. But yeah, I think it is also our
[01:27:08.560 --> 01:27:10.840]   responsibility that if we believe these things are great,
[01:27:10.840 --> 01:27:15.960]   we should also explain them, right, cleanly. And we do spend
[01:27:15.960 --> 01:27:19.200]   a lot of time on that, right, just just documenting what's
[01:27:19.200 --> 01:27:24.920]   already out there. And, and sort of explaining these functional
[01:27:24.920 --> 01:27:28.960]   paradigms, but we're definitely aware that this takes some
[01:27:28.960 --> 01:27:31.240]   extra effort to explain.
[01:27:31.240 --> 01:27:38.440]   Okay, yeah. Yeah, no, this is kind of a comment I'm
[01:27:38.440 --> 01:27:46.720]   interested in. But I know, maybe you have some pointers we could
[01:27:46.720 --> 01:27:51.080]   share to the viewers. Like not not here. There are people in
[01:27:51.080 --> 01:27:53.600]   different levels if you see the questions they've been asking.
[01:27:53.600 --> 01:27:57.680]   So I don't know if we could link to something that you think it's
[01:27:57.680 --> 01:28:04.600]   kind of the the easiest way to understand FLEX. Obviously, you
[01:28:04.600 --> 01:28:12.720]   just go to the docs, right. But the easiest way to start like
[01:28:12.720 --> 01:28:22.080]   getting on boarded with FLEX or maybe and I know you have kind
[01:28:22.080 --> 01:28:25.120]   of a course as part of the hugging phase. I'm not sure
[01:28:25.120 --> 01:28:26.040]   that was a thing.
[01:28:26.040 --> 01:28:32.240]   Yeah, so on the on the read the docs, there's a there's a
[01:28:32.280 --> 01:28:45.720]   guided tour which has a few. So it starts here. So yes, there's
[01:28:45.720 --> 01:28:53.800]   not a link to. So it starts with some explanation of JAX also
[01:28:53.800 --> 01:29:02.320]   related to the things you need to know for for FLEX in
[01:29:02.320 --> 01:29:06.120]   particular, and then it's and then you end up with some
[01:29:06.120 --> 01:29:10.040]   notebooks like running running through the notebooks for the
[01:29:10.040 --> 01:29:15.040]   FLEX basics. So explaining linen and an annotated amnesty
[01:29:15.040 --> 01:29:20.920]   example. So I would start there. Maybe maybe read some of the
[01:29:21.000 --> 01:29:25.440]   the design notes and the additional material. And then
[01:29:25.440 --> 01:29:30.200]   and then I would run through through one of the sort of more
[01:29:30.200 --> 01:29:35.040]   serious examples like like the image net or the WMT example.
[01:29:35.040 --> 01:29:40.760]   Just just try to figure that out like running that that should
[01:29:40.760 --> 01:29:42.600]   that should get you started. I think.
[01:29:42.600 --> 01:29:48.160]   Thank you, Jonathan, just to be respectful of your time. Is it
[01:29:48.160 --> 01:29:50.840]   okay to squeeze in another question from the audience?
[01:29:51.280 --> 01:29:51.880]   Oh, yeah. Sure.
[01:29:51.880 --> 01:29:58.840]   So you've Raj is asking, can you share more resources on
[01:29:58.840 --> 01:30:03.000]   pipelining for democratization of AI? This is from your slides
[01:30:03.000 --> 01:30:06.400]   and he's not sure if there are some already in the official
[01:30:06.400 --> 01:30:08.080]   guide, but he's just curious about it.
[01:30:08.080 --> 01:30:12.280]   Oh, yeah. So So by the way, I added specifically tomorrow,
[01:30:12.280 --> 01:30:17.360]   right? And tomorrow is always like one day, one day further
[01:30:17.360 --> 01:30:25.760]   ahead, right? We always say like, so if you want to learn
[01:30:25.760 --> 01:30:28.880]   about pipelining today, sadly, I'll have to refer you to deep
[01:30:28.880 --> 01:30:37.000]   speed. Or, or add the particular example of pathways, right? Like
[01:30:37.000 --> 01:30:42.720]   which, which, which also works today, which which is not not
[01:30:42.720 --> 01:30:45.600]   exactly the same thing, right? Like, I think deep speed is
[01:30:45.680 --> 01:30:49.640]   particularly the sort of the resource that many people use
[01:30:49.640 --> 01:30:53.360]   today. So I would look at that.
[01:30:53.360 --> 01:31:00.080]   Okay, thank you. I'll quickly wrap up and remind the audience
[01:31:00.080 --> 01:31:03.440]   that you can find Jonathan on Twitter. His handle is his first
[01:31:03.440 --> 01:31:06.680]   name concatenated with the second. Check out the flags
[01:31:06.680 --> 01:31:09.240]   repository, check out the JAX repository, you'll find him
[01:31:09.240 --> 01:31:13.880]   quite active there. You can also find Christian on Twitter at
[01:31:13.960 --> 01:31:20.000]   C Garcia, he 88. Make sure you find him there. Any other
[01:31:20.000 --> 01:31:23.480]   resources or please platforms you'd want to mention Jonathan?
[01:31:23.480 --> 01:31:28.560]   Yeah, I mean, if you want to check chat about flags, then
[01:31:28.560 --> 01:31:33.200]   then then check out our repository, Google slash slash
[01:31:33.200 --> 01:31:37.880]   five. And we're pretty active there and try to sort of engage
[01:31:37.880 --> 01:31:44.680]   with open source and develop further from there. So yeah,
[01:31:44.680 --> 01:31:48.800]   just just just join, join the discussions or the issues page.
[01:31:48.800 --> 01:31:51.840]   Yeah, let's build something.
[01:31:51.840 --> 01:31:57.000]   Awesome. Jonathan, thanks. Thanks again for your time. And
[01:31:57.000 --> 01:31:59.480]   Christian, thank you so much for helping make this happen. I'll,
[01:31:59.480 --> 01:32:02.800]   I'll wrap up here. But it was really, really a privilege to
[01:32:02.800 --> 01:32:05.600]   learn from one of the core contributors and one of the core
[01:32:05.600 --> 01:32:07.600]   developers. So thanks. Thanks for your time.
[01:32:08.080 --> 01:32:09.120]   Yeah, thanks for having me.
[01:32:09.120 --> 01:32:12.520]   Yeah, thanks, Jonathan. It was really nice.
[01:32:12.520 --> 01:32:22.420]   [ end ]

