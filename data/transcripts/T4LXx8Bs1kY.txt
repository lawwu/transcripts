
[00:00:00.000 --> 00:00:08.160]   There's still an enormous disconnect between what an executive expects to be able to do
[00:00:08.160 --> 00:00:13.200]   and what the software developer or what the machine learning person or the data scientist
[00:00:13.200 --> 00:00:15.040]   actually understands is doable.
[00:00:15.040 --> 00:00:19.400]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:19.400 --> 00:00:21.320]   and I'm your host, Lucas Biewald.
[00:00:21.320 --> 00:00:28.120]   James Champ is a partner at Bloomberg Beta, a fund that invests in machine learning and
[00:00:28.120 --> 00:00:29.920]   the future of work.
[00:00:29.920 --> 00:00:35.000]   He's invested in many successful companies, including my first company, CrowdFlower, and
[00:00:35.000 --> 00:00:37.200]   my second company, Weights & Biases.
[00:00:37.200 --> 00:00:41.440]   I've worked with him for a really long time, and he always has really smart things to say
[00:00:41.440 --> 00:00:43.080]   about technology trends.
[00:00:43.080 --> 00:00:47.040]   I'm super excited to talk to him today.
[00:00:47.040 --> 00:00:52.440]   James, you've invested in AI for a long time.
[00:00:52.440 --> 00:00:57.480]   You were the first investor in CrowdFlower, my first company, and you were the first investor
[00:00:57.480 --> 00:01:00.160]   in Weights & Biases.
[00:01:00.160 --> 00:01:07.560]   I was curious to know your perspective on how your thinking around investing in AI has
[00:01:07.560 --> 00:01:10.440]   changed over the last 15 years.
[00:01:10.440 --> 00:01:14.800]   Clearly the market has changed, but I was curious to understand how your thinking has
[00:01:14.800 --> 00:01:15.800]   changed.
[00:01:15.800 --> 00:01:25.240]   When I invested in CrowdFlower, I didn't understand that I was actually investing in AI.
[00:01:25.240 --> 00:01:30.240]   I thought that there was a broader collective intelligence problem that you were solving,
[00:01:30.240 --> 00:01:35.520]   and I was really enamored with both crowdsourcing and flash teams at the point.
[00:01:35.520 --> 00:01:39.320]   To be honest, I still am.
[00:01:39.320 --> 00:01:46.080]   In some ways, I think about AI or machine learning more specifically as a misnomer.
[00:01:46.080 --> 00:01:50.840]   I think that it's actually a collective intelligence thing that's going on.
[00:01:50.840 --> 00:01:53.680]   That's on the broad, theoretical side.
[00:01:53.680 --> 00:02:00.840]   Then the big change on the investment side, I think, is we went from a place where people
[00:02:00.840 --> 00:02:04.200]   actively didn't want to invest, or where I actively...
[00:02:04.200 --> 00:02:07.760]   There are a couple of folks that you and I both know who I actively encouraged not to
[00:02:07.760 --> 00:02:12.760]   use the word "machine learning" because I thought it hurt their chances to raise money,
[00:02:12.760 --> 00:02:22.400]   to a world in which now we live in where there's an incredible amount of investment.
[00:02:22.400 --> 00:02:27.080]   What's interesting about the incredible level of investment right now is that we're still
[00:02:27.080 --> 00:02:31.920]   sort of at the cusp of getting actual great business results.
[00:02:31.920 --> 00:02:37.800]   We're sort of at that point right now where I think all the pieces are almost all there,
[00:02:37.800 --> 00:02:39.240]   but they're not quite.
[00:02:39.240 --> 00:02:42.640]   Everyone feels that you have that little bit of impatience where everyone kind of wants
[00:02:42.640 --> 00:02:52.080]   to get it, and the talent's not quite there, or the executives don't quite understand it.
[00:02:52.080 --> 00:02:55.720]   So that's an uncomfortable, but also really exciting point to be in.
[00:02:55.720 --> 00:03:01.200]   Well, do you think there's some chance that we're set up for disappointment?
[00:03:01.200 --> 00:03:03.720]   We are always set up for disappointment.
[00:03:03.720 --> 00:03:05.000]   You know that as well as I do.
[00:03:05.000 --> 00:03:06.000]   That's true.
[00:03:06.000 --> 00:03:08.680]   So Lucas and I, I'm lucky enough to...
[00:03:08.680 --> 00:03:14.320]   Whatever, I guess every two weeks we have our little morning chat, and I feel like we
[00:03:14.320 --> 00:03:18.200]   have recurring themes, and one of them is this continued question of where are we in
[00:03:18.200 --> 00:03:19.680]   the market?
[00:03:19.680 --> 00:03:26.320]   You have to admit that the last probably few quarters, there's this sense that everything
[00:03:26.320 --> 00:03:27.800]   is coming together, right?
[00:03:27.800 --> 00:03:29.960]   But at the same time, you feel like everything's coming together.
[00:03:29.960 --> 00:03:34.040]   You're still looking behind you to say, "Oh goodness, in what way are we overselling,
[00:03:34.040 --> 00:03:36.760]   and what way are people misunderstanding things?"
[00:03:36.760 --> 00:03:42.520]   And at least to me, it feels like there's still base levels of understanding that are
[00:03:42.520 --> 00:03:49.480]   missing, and it still feels to me like there are opportunities to define the market in
[00:03:49.480 --> 00:03:53.800]   the right way rather than the buzzy, silly way.
[00:03:53.800 --> 00:03:59.160]   When do you think investors kind of flipped from feeling like machine learning was a science
[00:03:59.160 --> 00:04:03.920]   project to machine learning was a good business to invest in?
[00:04:03.920 --> 00:04:06.720]   I mean, you've always done kind of early stage, seed stage investments.
[00:04:06.720 --> 00:04:11.560]   That's probably where the change happened the earliest, but when was that, and what
[00:04:11.560 --> 00:04:14.400]   was going on that caused that change in mindset?
[00:04:14.400 --> 00:04:22.160]   I mean, some of it is that, well, okay, there's this little joke around Google and Facebook
[00:04:22.160 --> 00:04:24.760]   where sort of, "What do startups really do?
[00:04:24.760 --> 00:04:28.200]   We commercialize things that Google figured out five years ago, right?
[00:04:28.200 --> 00:04:29.960]   And then we bring it to the rest of the world."
[00:04:29.960 --> 00:04:32.840]   And there's a little bit of that sense that that's not ridiculous, right?
[00:04:32.840 --> 00:04:38.680]   That you saw sort of the kind of changes that people were able to implement and build inside
[00:04:38.680 --> 00:04:43.360]   the big fangs, and then realize that this should be more broadly available.
[00:04:43.360 --> 00:04:48.080]   And so you had that on the one side, and then on the other side, you had sort of these remarkable...
[00:04:48.080 --> 00:04:51.080]   Well, okay, so how do I think about this?
[00:04:51.080 --> 00:04:56.080]   I think in terms of on the academic side, you had a few things happened, right?
[00:04:56.080 --> 00:04:57.960]   On the one hand, you had great results, right?
[00:04:57.960 --> 00:04:59.760]   Just super impressive results.
[00:04:59.760 --> 00:05:03.920]   But also there's a way in which academics sort of figured out how to play the game in
[00:05:03.920 --> 00:05:09.440]   the sense that the machine learning world was sort of well-defined enough now that people
[00:05:09.440 --> 00:05:13.040]   can compete on some basis that they understood.
[00:05:13.040 --> 00:05:18.920]   I remember there was this guy who gave this great pitch around how to think about advances
[00:05:18.920 --> 00:05:22.480]   in machine learning, and he made the point that actually maybe it's really about the
[00:05:22.480 --> 00:05:23.480]   size of the data set.
[00:05:23.480 --> 00:05:26.640]   Do you remember who that guy was?
[00:05:26.640 --> 00:05:27.960]   Do you think that's still true?
[00:05:27.960 --> 00:05:28.960]   That was Lucas, by the way.
[00:05:28.960 --> 00:05:29.960]   That was Lucas, just to be clear.
[00:05:29.960 --> 00:05:32.360]   Just to be clear.
[00:05:32.360 --> 00:05:34.400]   Do I think what is still true?
[00:05:34.400 --> 00:05:39.480]   Well, I do think the size of the data set is incredibly important.
[00:05:39.480 --> 00:05:44.880]   And I think maybe five or 10 years ago, I thought it was really the only important thing
[00:05:44.880 --> 00:05:50.120]   and that advances in algorithms seemed pointless to me at the time.
[00:05:50.120 --> 00:05:55.000]   But I think in retrospect, maybe I didn't have such a quite extreme view, but at that
[00:05:55.000 --> 00:06:02.600]   time it wasn't clear that deep learning worked much better than traditional methods.
[00:06:02.600 --> 00:06:06.060]   There hadn't been a lot of improvements in algorithms for a really long time.
[00:06:06.060 --> 00:06:09.920]   And so almost all the advances felt like it was coming from bigger data sets.
[00:06:09.920 --> 00:06:17.000]   But now I look at open AI and deep mind, and it feels like a lot of the advances that are
[00:06:17.000 --> 00:06:23.720]   happening there is on one hand coming from bigger data sets, making more advanced modeling
[00:06:23.720 --> 00:06:26.480]   possible, but also advances in compute.
[00:06:26.480 --> 00:06:27.480]   Okay.
[00:06:27.480 --> 00:06:32.360]   So I've got a nuance and sort of like the extreme claim you used to make, which is I
[00:06:32.360 --> 00:06:36.840]   actually think it's that with the availability of large data sets, but also with the availability
[00:06:36.840 --> 00:06:40.720]   of the understanding that these large data sets were available, it meant that everyone
[00:06:40.720 --> 00:06:42.720]   understood how to play the game.
[00:06:42.720 --> 00:06:46.320]   That it meant that you have a whole wave of academics and companies and corporations and
[00:06:46.320 --> 00:06:51.320]   groups and teams saying, "Oh, we can play with these sets of data in interesting and
[00:06:51.320 --> 00:06:52.320]   novel ways."
[00:06:52.320 --> 00:06:57.440]   And so what that meant is that the thing that was the scarce commodity or the way that you
[00:06:57.440 --> 00:07:01.000]   basically laid that piece out meant that people were able to work on it.
[00:07:01.000 --> 00:07:04.040]   And then that's where you get all these exciting advances in part, because everyone agreed
[00:07:04.040 --> 00:07:06.440]   on sort of how to think a little bit about the data.
[00:07:06.440 --> 00:07:13.380]   I wanted to ask you too, I think one of the things that you did really well was maybe
[00:07:13.380 --> 00:07:21.560]   starting a real trend in content marketing among VCs when you and Siobhan put out the
[00:07:21.560 --> 00:07:26.500]   machine intelligence kind of infographic where you laid out all the companies.
[00:07:26.500 --> 00:07:29.880]   I was curious what caused you to start it.
[00:07:29.880 --> 00:07:34.040]   And then I feel like it became wildly successful and you stopped doing it.
[00:07:34.040 --> 00:07:37.920]   And many other people have picked up where you left off, but without the same, in my
[00:07:37.920 --> 00:07:40.560]   opinion, quality that you had.
[00:07:40.560 --> 00:07:44.040]   So can you tell us the story behind that?
[00:07:44.040 --> 00:07:45.040]   Sure.
[00:07:45.040 --> 00:07:51.040]   I think when the fund started, I think there was a sense that we were at the tail end,
[00:07:51.040 --> 00:07:54.340]   and incorrectly, there was a sense that we were at the tail end of a bunch of investment
[00:07:54.340 --> 00:07:59.540]   around big data, and that there were a lot of failed big data projects sitting around.
[00:07:59.540 --> 00:08:04.080]   And so then the question was, what are you going to do with all that investment and understanding
[00:08:04.080 --> 00:08:05.600]   and collecting data?
[00:08:05.600 --> 00:08:11.120]   And then one of the claims or one of the guesses was that you'd use that data for machine learning,
[00:08:11.120 --> 00:08:12.120]   right?
[00:08:12.120 --> 00:08:14.480]   And there are a bunch of AI applications.
[00:08:14.480 --> 00:08:19.600]   And sort of my old colleague, Siobhan Zillis, sort of had that, like pushed that inside
[00:08:19.600 --> 00:08:20.600]   a lot.
[00:08:20.600 --> 00:08:25.280]   I think in part because she felt it like just intuitively, but also she was surrounded by
[00:08:25.280 --> 00:08:28.540]   a set of folks who were like playing around different places with it.
[00:08:28.540 --> 00:08:33.360]   And then I think we were both sitting around thinking, "Wow, this is just so hard to understand."
[00:08:33.360 --> 00:08:35.800]   And we couldn't make heads or tails of it.
[00:08:35.800 --> 00:08:42.640]   And then basically what happened was Siobhan being just a really great synthesizer, but
[00:08:42.640 --> 00:08:47.960]   also someone who's quite dogged, sort of decided to go work with another friend of hers who
[00:08:47.960 --> 00:08:50.960]   figured out ways to cluster different types of businesses.
[00:08:50.960 --> 00:08:57.760]   And so she basically then took that, clustered a bunch of different types of businesses that
[00:08:57.760 --> 00:09:02.080]   included a number of keywords around AI and then categorized it and then stuck it on a
[00:09:02.080 --> 00:09:03.080]   map.
[00:09:03.080 --> 00:09:08.700]   And I think that was like a two month process to actually go through all of that and have
[00:09:08.700 --> 00:09:11.280]   all these horrible spreadsheets because it was super...
[00:09:11.280 --> 00:09:12.880]   I mean, there are products now that do this, right?
[00:09:12.880 --> 00:09:15.720]   But it was like super manual in some ways.
[00:09:15.720 --> 00:09:19.240]   And what was exciting about it, the moment she put it together, so I give her all the
[00:09:19.240 --> 00:09:21.360]   credit for actually doing the real work.
[00:09:21.360 --> 00:09:25.240]   Then suddenly it felt like this world was legible for the first time.
[00:09:25.240 --> 00:09:29.840]   And then I think we kind of assumed that there should be people working on this full time
[00:09:29.840 --> 00:09:33.520]   rather than having this just be a part-time job and then they would do a better job of
[00:09:33.520 --> 00:09:34.520]   it.
[00:09:34.520 --> 00:09:36.160]   And I think that...
[00:09:36.160 --> 00:09:40.160]   And so for a few years, basically Siobhan would take some time off right around the
[00:09:40.160 --> 00:09:43.040]   summer to just do the state of what's going on.
[00:09:43.040 --> 00:09:49.080]   And I think it was really good, not just because the categories were not always right, but
[00:09:49.080 --> 00:09:52.080]   at least it gave something for people to agree or disagree on it.
[00:09:52.080 --> 00:09:56.120]   And it made a bunch of connections for folks that I think have been still valuable to this
[00:09:56.120 --> 00:09:57.120]   day.
[00:09:57.120 --> 00:09:59.440]   And so why did we stop?
[00:09:59.440 --> 00:10:00.440]   I don't know.
[00:10:00.440 --> 00:10:01.740]   There are too many companies, right?
[00:10:01.740 --> 00:10:03.320]   And part of it is there are too many companies.
[00:10:03.320 --> 00:10:07.560]   Part of it is like, you think that there should be...
[00:10:07.560 --> 00:10:11.000]   I mean, I do think there are a new class of journalists who now think that way, right?
[00:10:11.000 --> 00:10:17.120]   Who think that mix of computational plus willingness to do the work plus not sort of subject to
[00:10:17.120 --> 00:10:20.440]   the day-to-day grind of reporting the next story.
[00:10:20.440 --> 00:10:24.200]   And they should be coming up with those conceptualizations, but I haven't totally seen it.
[00:10:24.200 --> 00:10:30.240]   I do think it was a novel contribution at the time.
[00:10:30.240 --> 00:10:34.440]   So one thing that I know you are very interested in because you talk to me about it all the
[00:10:34.440 --> 00:10:41.120]   time is kind of how organizations function as a collection of humans trying to work together
[00:10:41.120 --> 00:10:42.480]   towards a common goal.
[00:10:42.480 --> 00:10:48.960]   I feel like you think about that more than most and you think about machine learning
[00:10:48.960 --> 00:10:49.960]   more than most.
[00:10:49.960 --> 00:10:56.600]   So I'm curious how you think, or maybe how you've seen organizations adapt to machine
[00:10:56.600 --> 00:10:59.600]   learning becoming more mainstream within them.
[00:10:59.600 --> 00:11:07.240]   And I'm curious if you have predictions on how organizations might continue to evolve
[00:11:07.240 --> 00:11:12.360]   as machine learning becomes a bigger and bigger part of them.
[00:11:12.360 --> 00:11:17.200]   I mean, we're not yet at the point right now where machine learning is boring enough that
[00:11:17.200 --> 00:11:19.020]   it could be adopted easily.
[00:11:19.020 --> 00:11:23.720]   So we're still in the part of the market or part of the phase where there's plenty of
[00:11:23.720 --> 00:11:29.640]   exploration and plenty of definition and ecosystem definition to be had.
[00:11:29.640 --> 00:11:36.400]   And you see some of that in like sort of slightly misguided arguments around augmentation versus
[00:11:36.400 --> 00:11:37.400]   automation.
[00:11:37.400 --> 00:11:42.080]   And I think you only have those sort of theoretical sort of questions when people don't have actual
[00:11:42.080 --> 00:11:44.600]   solutions they're dealing with day-to-day.
[00:11:44.600 --> 00:11:47.320]   But I think that there's definitely...
[00:11:47.320 --> 00:11:48.720]   So that's the first part.
[00:11:48.720 --> 00:11:57.640]   And then I think the second part is that management theorists have thought for a long time or
[00:11:57.640 --> 00:12:02.640]   talked about the idea of a learning organization, that organizations will actually get better
[00:12:02.640 --> 00:12:06.240]   over time because they learn things.
[00:12:06.240 --> 00:12:08.960]   And generally that's just been a metaphor, right?
[00:12:08.960 --> 00:12:09.960]   That's just been sort of...
[00:12:09.960 --> 00:12:14.040]   Because of course organizations are not people, they don't have minds, they don't learn anything,
[00:12:14.040 --> 00:12:15.040]   right?
[00:12:15.040 --> 00:12:17.880]   Sort of maybe those things get codified in processes or rules.
[00:12:17.880 --> 00:12:23.840]   And part of what's exciting about machine learning sort of in the next, like the pre-AGI
[00:12:23.840 --> 00:12:31.400]   version of machine learning is that we could actually digitize a bunch of decisions that
[00:12:31.400 --> 00:12:33.920]   get made on a day-to-day basis.
[00:12:33.920 --> 00:12:38.440]   And we can actually literally learn from them, right?
[00:12:38.440 --> 00:12:42.040]   Something as boring as, "Do I go to this meeting or not go to this meeting?"
[00:12:42.040 --> 00:12:46.720]   Or something as important as, "Do I invest in this project or not?"
[00:12:46.720 --> 00:12:50.600]   All those things in the world we live in right now have almost no consequences.
[00:12:50.600 --> 00:12:54.380]   No one actually follows up on a consistent basis to make sure or understand whether things
[00:12:54.380 --> 00:12:55.440]   work or not.
[00:12:55.440 --> 00:12:58.520]   Or if they do, it's incredibly expensive and difficult, right?
[00:12:58.520 --> 00:12:59.760]   You just think about the...
[00:12:59.760 --> 00:13:03.680]   Think about, not you guys, but maybe some other theoretical organization.
[00:13:03.680 --> 00:13:07.320]   We'll have to spend all this time just digging down to figure out what product, like what
[00:13:07.320 --> 00:13:11.680]   random marketing campaign actually happened or didn't happen or how well it worked.
[00:13:11.680 --> 00:13:15.400]   And just the amount of automation people need to put in in order to systematize that.
[00:13:15.400 --> 00:13:22.160]   And what's exciting about, at least to me, what's exciting about the data-rich ML world
[00:13:22.160 --> 00:13:26.840]   we could be living in is that those decisions, we can now find out whether they actually
[00:13:26.840 --> 00:13:27.840]   work or not.
[00:13:27.840 --> 00:13:32.680]   And then we can actually maybe consistently start making better decisions, right?
[00:13:32.680 --> 00:13:34.200]   Now there are also a bunch of...
[00:13:34.200 --> 00:13:35.200]   You were going to say something.
[00:13:35.200 --> 00:13:36.200]   What were you going to say?
[00:13:36.200 --> 00:13:41.040]   Well, let's take your example of, "Should I go to a meeting or not?"
[00:13:41.040 --> 00:13:45.160]   How do I ever even know in retrospect if I should have gone to a meeting?
[00:13:45.160 --> 00:13:51.720]   How could an organization really learn whether or not it makes sense to go to a meeting?
[00:13:51.720 --> 00:13:52.720]   So I think there's...
[00:13:52.720 --> 00:13:53.720]   Okay.
[00:13:53.720 --> 00:13:59.440]   So one of the other angles that I'm very interested in is that intersection around machine learning
[00:13:59.440 --> 00:14:01.080]   and the social sciences.
[00:14:01.080 --> 00:14:03.760]   And so you'll talk to management folks that...
[00:14:03.760 --> 00:14:09.520]   Or rather on the AI side, there's always this question of what's the objective function.
[00:14:09.520 --> 00:14:13.800]   And the interesting thing is that on the social sciences side, they've learned a lesson, which
[00:14:13.800 --> 00:14:18.680]   is, "I don't know, we'll have some objective function and it'll be good enough to manage,
[00:14:18.680 --> 00:14:19.920]   but it'll never be perfect.
[00:14:19.920 --> 00:14:25.640]   That actually will have to change over time because the most interesting systems are all
[00:14:25.640 --> 00:14:26.640]   dynamic.
[00:14:26.640 --> 00:14:28.160]   They're dynamic because people are interesting."
[00:14:28.160 --> 00:14:36.600]   Once you decide that one metric is the right way to measure whether a meeting is good or
[00:14:36.600 --> 00:14:38.560]   not, people will start to learn that and they'll start to game it.
[00:14:38.560 --> 00:14:39.560]   They'll be like, "You know what?
[00:14:39.560 --> 00:14:42.840]   Whenever Lucas smiles twice, then I'm going to go...
[00:14:42.840 --> 00:14:44.760]   I'll tell some stupid joke."
[00:14:44.760 --> 00:14:47.120]   And it'll detract from the actual purpose of the business.
[00:14:47.120 --> 00:14:53.120]   And so I think that the illusion is that you'll come up with some perfect metric.
[00:14:53.120 --> 00:14:57.420]   And I think the actual goal is to continually come up with metrics that slightly will change
[00:14:57.420 --> 00:15:04.240]   over time and you'll understand what works and what doesn't work, but that'll be okay.
[00:15:04.240 --> 00:15:08.480]   You think about it in traditional organizational science, there's this great paper, I think,
[00:15:08.480 --> 00:15:16.800]   called on the folly of wanting A and rewarding B, or measuring B. And I think that problem
[00:15:16.800 --> 00:15:20.200]   is going to be forever, but that's part of the fun of the job.
[00:15:20.200 --> 00:15:24.440]   That's part of the fun of creating organizations and social systems.
[00:15:24.440 --> 00:15:26.240]   I totally agree with that.
[00:15:26.240 --> 00:15:27.240]   But I feel like even...
[00:15:27.240 --> 00:15:31.240]   I mean, maybe I don't want to harp on this case too much, but I'm curious because I always
[00:15:31.240 --> 00:15:34.200]   wonder myself if I should go to a particular meeting or not.
[00:15:34.200 --> 00:15:38.240]   But how would you even make an imperfect measure of that?
[00:15:38.240 --> 00:15:42.120]   What do you even imagine looking at to...
[00:15:42.120 --> 00:15:47.440]   So you can certainly imagine it as, is the meeting useful to you?
[00:15:47.440 --> 00:15:52.280]   You can also imagine it in terms of, is the meeting useful to increase the collective
[00:15:52.280 --> 00:15:54.800]   intelligence of the organization?
[00:15:54.800 --> 00:15:59.600]   And then you can certainly do direct measures, which we can just literally ask you, how good
[00:15:59.600 --> 00:16:00.600]   was that meeting afterwards?
[00:16:00.600 --> 00:16:03.480]   Or we can literally ask the team, how good was that meeting afterwards?
[00:16:03.480 --> 00:16:07.880]   Or we can literally look at the number of things you write after that meeting.
[00:16:07.880 --> 00:16:11.080]   Or we can literally look at the number of times that you nodded or didn't nod.
[00:16:11.080 --> 00:16:17.560]   So I mean, just to say all those signals are increasingly cheap to gather.
[00:16:17.560 --> 00:16:21.240]   And when they get cheap to gather, that's when we actually get interesting innovation.
[00:16:21.240 --> 00:16:25.320]   When it's incredibly expensive, when you need to hire McKinsey to do some study and then
[00:16:25.320 --> 00:16:29.280]   hire a bunch of people to build some system, some very expensive bespoke system, then it's
[00:16:29.280 --> 00:16:30.640]   not that useful.
[00:16:30.640 --> 00:16:36.040]   Because then your ability to move and play with the edges of your social system becomes
[00:16:36.040 --> 00:16:37.040]   too difficult.
[00:16:37.040 --> 00:16:43.120]   And then your chance to actually design it on the fly and continue to understand it,
[00:16:43.120 --> 00:16:47.200]   that's I think the interesting edge around social systems.
[00:16:47.200 --> 00:16:48.200]   Interesting.
[00:16:48.200 --> 00:16:56.160]   Where do you see machine learning making a meaningful difference in organizations today?
[00:16:56.160 --> 00:16:58.680]   I mean, in all the normal places, right?
[00:16:58.680 --> 00:17:04.760]   That we're now finally getting good enough to cluster large scale bits of information
[00:17:04.760 --> 00:17:08.880]   in ways that are meaningful so that we can provide consistent responses.
[00:17:08.880 --> 00:17:15.640]   And so I think that that piece of it, which is the big version of machine learning, finding
[00:17:15.640 --> 00:17:19.920]   the most critical decisions you need to make, the most digitized pieces, and then finding
[00:17:19.920 --> 00:17:22.800]   ways to consistently improve and collect it.
[00:17:22.800 --> 00:17:29.800]   I think that that's where most of the energy and opportunity is right now.
[00:17:29.800 --> 00:17:31.000]   But that'll change, right?
[00:17:31.000 --> 00:17:32.000]   That'll change.
[00:17:32.000 --> 00:17:35.440]   I think that the exciting...
[00:17:35.440 --> 00:17:36.840]   Does that make sense, first of all?
[00:17:36.840 --> 00:17:37.840]   Yeah, totally.
[00:17:37.840 --> 00:17:38.840]   You know what I mean?
[00:17:38.840 --> 00:17:39.840]   Yeah.
[00:17:39.840 --> 00:17:43.040]   Okay, so let me take one slight digression as we're talking about this.
[00:17:43.040 --> 00:17:50.080]   Of course, as you asked this question, the real answer is that executives could know
[00:17:50.080 --> 00:17:55.040]   how to apply machine learning if only they understood a little bit more than what they
[00:17:55.040 --> 00:17:59.040]   learn from reading or watching a movie, right?
[00:17:59.040 --> 00:18:05.440]   And there's still an enormous disconnect between what an executive expects to be able to do
[00:18:05.440 --> 00:18:10.440]   and what the software developer or what the machine learning person or the data scientist
[00:18:10.440 --> 00:18:12.240]   actually understands is doable.
[00:18:12.240 --> 00:18:17.840]   And so I do have to make the pitch, which I think I've done too many times to you, which
[00:18:17.840 --> 00:18:25.080]   is I do remain convinced that the three to four hour class that you used to teach to
[00:18:25.080 --> 00:18:29.760]   executives on how to think about machine learning probably is the best...
[00:18:29.760 --> 00:18:34.600]   If you were to say, "What's the best way to improve the way people think about machine
[00:18:34.600 --> 00:18:35.600]   learning?"
[00:18:35.600 --> 00:18:40.400]   You should make your boss's boss take a three hour course and just sit around and play with
[00:18:40.400 --> 00:18:46.240]   a very simple machine learning model because in that process, they will at least have some
[00:18:46.240 --> 00:18:54.640]   intuition about how incredibly powerful, unsexy, brittle, finicky, and incredibly scalable
[00:18:54.640 --> 00:18:57.000]   some of these models that you'll build will actually be.
[00:18:57.000 --> 00:19:02.880]   Well, you know, it's not the core of our business, but I am passionate about doing it.
[00:19:02.880 --> 00:19:07.940]   And really it's not that we shut down those classes.
[00:19:07.940 --> 00:19:12.480]   There wasn't actually much demand for it, or maybe we didn't pursue it aggressively
[00:19:12.480 --> 00:19:13.480]   enough.
[00:19:13.480 --> 00:19:16.120]   There's much more demand for the tools that we build.
[00:19:16.120 --> 00:19:21.200]   But I guess I'm curious, when you did the class and your colleagues did the class...
[00:19:21.200 --> 00:19:22.200]   Yeah, go ahead.
[00:19:22.200 --> 00:19:23.200]   What's that?
[00:19:23.200 --> 00:19:24.200]   No, no, no.
[00:19:24.200 --> 00:19:25.200]   I'm actually...
[00:19:25.200 --> 00:19:28.800]   Maybe I'm just softballing a pitch to you, but I'm curious.
[00:19:28.800 --> 00:19:32.800]   It seemed like you really liked that class and really felt like your team got a lot out
[00:19:32.800 --> 00:19:33.800]   of it.
[00:19:33.800 --> 00:19:39.400]   But really, what was it that you feel like you took away from those couple hours of building
[00:19:39.400 --> 00:19:40.400]   models?
[00:19:40.400 --> 00:19:43.240]   So, what you did is you did like half...
[00:19:43.240 --> 00:19:49.720]   It was to a wide non-technical or audience, a few technical-ish folks.
[00:19:49.720 --> 00:19:55.480]   And what you did is you gave a little overview and then you had them fire up an IDE, open
[00:19:55.480 --> 00:19:58.560]   up some things in Python, have access on data of like...
[00:19:58.560 --> 00:19:59.560]   I forget, what are they?
[00:19:59.560 --> 00:20:00.560]   Socks?
[00:20:00.560 --> 00:20:01.560]   What were the images?
[00:20:01.560 --> 00:20:05.360]   Oh yeah, fashion MNIST for those in the audience.
[00:20:05.360 --> 00:20:06.360]   That's right.
[00:20:06.360 --> 00:20:07.360]   That's right.
[00:20:07.360 --> 00:20:12.400]   And then you gave them a very straightforward framework, but you had them played around
[00:20:12.400 --> 00:20:13.920]   with slightly different approaches.
[00:20:13.920 --> 00:20:19.040]   You gave them the opportunity to see the results and you gave them the opportunity to play
[00:20:19.040 --> 00:20:20.040]   with different parameters.
[00:20:20.040 --> 00:20:22.320]   And then you introduced a few curve balls.
[00:20:22.320 --> 00:20:30.000]   And it was actually a very straightforward exercise, but it was curated and it was accessible
[00:20:30.000 --> 00:20:31.720]   to a wide range of folks.
[00:20:31.720 --> 00:20:37.760]   And what was interesting about it was that for the first time, rather than thinking about
[00:20:37.760 --> 00:20:42.720]   the grand vision of machine learning, you had a wide range of folks thinking about it
[00:20:42.720 --> 00:20:44.480]   from a very concrete...
[00:20:44.480 --> 00:20:49.840]   The way that a developer would, where you're actually dealing with data and you're thinking,
[00:20:49.840 --> 00:20:51.160]   "Oh, what does this actually do?"
[00:20:51.160 --> 00:20:52.880]   And you're thinking, "Oh my goodness, this totally broke.
[00:20:52.880 --> 00:20:57.840]   But by the way, I could also just apply this to 50,000 images instantly," which is an amazing
[00:20:57.840 --> 00:21:00.080]   feeling for someone.
[00:21:00.080 --> 00:21:04.440]   And it's a different kind of feeling that you get from building software.
[00:21:04.440 --> 00:21:06.760]   And I think that that intuition...
[00:21:06.760 --> 00:21:11.800]   I'm kind of convinced that you could teach this to Nancy Pelosi and she'd learn something
[00:21:11.800 --> 00:21:14.680]   and she'd make better policy decisions as a result of that.
[00:21:14.680 --> 00:21:17.200]   I'm kind of convinced that if you...
[00:21:17.200 --> 00:21:21.440]   We've done a slight variation of this with a couple other executives and it worked really
[00:21:21.440 --> 00:21:22.440]   well.
[00:21:22.440 --> 00:21:30.320]   And at least to me, it feels like that shift in mindset and also just that little bit of
[00:21:30.320 --> 00:21:34.960]   finger feel meant that folks just had better intuition.
[00:21:34.960 --> 00:21:36.520]   And I think it made a huge difference.
[00:21:36.520 --> 00:21:40.440]   And then they also asked better questions.
[00:21:40.440 --> 00:21:45.480]   One thing that always surprises me about VCs, because they always come...
[00:21:45.480 --> 00:21:48.800]   So many come from a quantitative background and I feel like there's so many investments
[00:21:48.800 --> 00:21:56.880]   being made is sort of the lack of rigor in the decision making process as far as I can
[00:21:56.880 --> 00:21:57.880]   see.
[00:21:57.880 --> 00:22:04.120]   I'm curious at Bloomberg Beta, do you use any machine learning or is there any kind
[00:22:04.120 --> 00:22:10.220]   of feedback loop where something's successful and then you decide to invest more in that?
[00:22:10.220 --> 00:22:11.560]   Only for top of funnel.
[00:22:11.560 --> 00:22:12.560]   Only for top of funnel.
[00:22:12.560 --> 00:22:14.560]   I mean, in our case, we're seed stage investors.
[00:22:14.560 --> 00:22:20.400]   And so our process and our process for follow on is very different from, let's say, some
[00:22:20.400 --> 00:22:22.560]   of the bigger, like a bigger fund.
[00:22:22.560 --> 00:22:30.240]   But I will remind you though, part of the fun of venture is that the game is constantly
[00:22:30.240 --> 00:22:31.240]   shifting.
[00:22:31.240 --> 00:22:35.520]   If it was exactly the same game, if the business models were exactly the same, then it'd be
[00:22:35.520 --> 00:22:37.240]   like kind of like everything else.
[00:22:37.240 --> 00:22:38.800]   There'd be no fun, it'd be routinized.
[00:22:38.800 --> 00:22:42.000]   And part of the excitement of the job, but also part of the opportunity.
[00:22:42.000 --> 00:22:48.040]   And the only reason it kind of exists is that there are chances for new business models
[00:22:48.040 --> 00:22:52.480]   to emerge where the old metrics no longer make sense.
[00:22:52.480 --> 00:22:56.220]   And I think those sorts of windows come around every so often.
[00:22:56.220 --> 00:23:00.000]   And to be honest, that's where there's that kind of uncertainty, where there's either
[00:23:00.000 --> 00:23:05.120]   a key technical uncertainty or key business model or market uncertainty.
[00:23:05.120 --> 00:23:09.440]   That's where the amazing opportunities come from.
[00:23:09.440 --> 00:23:16.980]   You have been doing venture for quite a while now and have seen a lot of big wins and losses.
[00:23:16.980 --> 00:23:23.160]   Is there anything consistent in terms of what you saw at the beginning of a successful company?
[00:23:23.160 --> 00:23:30.000]   Or is it that the venture market sort of adapts to whatever that is and the opportunity goes
[00:23:30.000 --> 00:23:31.000]   away?
[00:23:31.000 --> 00:23:34.440]   I'm sure you reflect on this because it's kind of like your main job.
[00:23:34.440 --> 00:23:41.000]   Is there any kind of common threads that you see in the successful businesses that you've
[00:23:41.000 --> 00:23:42.000]   backed?
[00:23:42.000 --> 00:23:47.080]   I mean, I think inevitably there are arbitrages that exist or there are ways to tell signal
[00:23:47.080 --> 00:23:48.080]   from noise.
[00:23:48.080 --> 00:23:52.440]   But because the market is clever and you're dealing with founders who are really smart
[00:23:52.440 --> 00:23:56.600]   and care a lot about what they're doing, what you're going to end up seeing is they'll end
[00:23:56.600 --> 00:23:59.080]   up imitating those signals of success.
[00:23:59.080 --> 00:24:03.240]   And so there's a little bit of this constantly shifting game where you're looking for a new
[00:24:03.240 --> 00:24:09.080]   signal to say that this thing means that these guys are high quality or this insight is really
[00:24:09.080 --> 00:24:10.080]   important.
[00:24:10.080 --> 00:24:12.680]   And then they'll figure out, "Oh, you know what I really should do?
[00:24:12.680 --> 00:24:15.600]   I should make sure I game Hacker News.
[00:24:15.600 --> 00:24:18.720]   And then I'll get all my buddies to go on Hacker News and then we'll coordinate and
[00:24:18.720 --> 00:24:20.560]   that'll no longer be a signal."
[00:24:20.560 --> 00:24:26.160]   Or, "What I really should do is I should make sure that all my friends are consistently
[00:24:26.160 --> 00:24:29.760]   starring my open source project on GitHub."
[00:24:29.760 --> 00:24:35.840]   Just meaning that once you figure it out, then this goes back to why these sort of dynamic
[00:24:35.840 --> 00:24:38.840]   models are so much fun.
[00:24:38.840 --> 00:24:39.840]   That's the whole point of it.
[00:24:39.840 --> 00:24:45.840]   And then so then you march on and think, "Okay, what's another signal of success?"
[00:24:45.840 --> 00:24:57.480]   I'm curious now at this moment, if I showed up and I was pitching an ML company and my
[00:24:57.480 --> 00:25:05.080]   customers were maybe the less tech forward enterprises, I feel like I probably shouldn't
[00:25:05.080 --> 00:25:07.480]   name names because some of them are always biased as customers.
[00:25:07.480 --> 00:25:17.160]   But if my customer base was like Procter & Gamble and GE, would that be more appealing to you
[00:25:17.160 --> 00:25:26.840]   than if my customer base looked like Airbnb and Facebook?
[00:25:26.840 --> 00:25:28.320]   How would you compare those two?
[00:25:28.320 --> 00:25:29.520]   Is one obviously better?
[00:25:29.520 --> 00:25:34.960]   I do think it entirely depends on the nature of the product and the nature of the solution.
[00:25:34.960 --> 00:25:41.200]   I think the way that I think about it is that there's sort of a gradient of admiration.
[00:25:41.200 --> 00:25:46.840]   In different types of markets, different people are higher up on the...
[00:25:46.840 --> 00:25:49.920]   Imagine that map, right?
[00:25:49.920 --> 00:25:53.040]   Higher up in terms of admiration.
[00:25:53.040 --> 00:25:57.080]   In some places, in some markets, some set of developer tools, then actually it does
[00:25:57.080 --> 00:26:03.280]   matter a lot whether or not the early adopters come from the tech forward or from Facebook
[00:26:03.280 --> 00:26:04.480]   or whatever.
[00:26:04.480 --> 00:26:09.880]   But in plenty of markets, and increasingly as machine learning gets mainstreamed, then
[00:26:09.880 --> 00:26:11.920]   the questions will all be around business benefit.
[00:26:11.920 --> 00:26:17.360]   And then the question is, who are the companies that other people admire or look up to or
[00:26:17.360 --> 00:26:20.240]   aspire to become in those specific markets?
[00:26:20.240 --> 00:26:25.040]   And I think that's part of the shifting nature of the game.
[00:26:25.040 --> 00:26:26.880]   I see.
[00:26:26.880 --> 00:26:35.000]   And is the gradient of admiration always clear to you?
[00:26:35.000 --> 00:26:38.440]   The secret fun part of the game is when you figure out what that gradient looks like before
[00:26:38.440 --> 00:26:42.840]   everyone else does, and then you play with people who are higher up there, right?
[00:26:42.840 --> 00:26:47.360]   And then you figure out, "Oh yeah, everyone's going to admire the data scientists at Netflix."
[00:26:47.360 --> 00:26:49.120]   Whenever that was true, right?
[00:26:49.120 --> 00:26:52.640]   And then you play with them and then you come up with much better insights or when it was
[00:26:52.640 --> 00:26:56.520]   true about whatever organization.
[00:26:56.520 --> 00:26:59.680]   And so it's not that complicated to think about, right?
[00:26:59.680 --> 00:27:04.400]   You just ask people, "Who do you like or who do you look to?"
[00:27:04.400 --> 00:27:07.640]   And I think that constantly shifts.
[00:27:07.640 --> 00:27:11.080]   So one of the things that we were talking about that I thought was intriguing was you
[00:27:11.080 --> 00:27:18.240]   mentioned that businesses that kind of focused on ML, even if they're not selling into ML,
[00:27:18.240 --> 00:27:24.880]   using ML for applications in different industries, you expect them to have a different business
[00:27:24.880 --> 00:27:27.040]   model potentially.
[00:27:27.040 --> 00:27:30.320]   And my thought is that the business model would match the market that they're selling
[00:27:30.320 --> 00:27:32.000]   into, but you felt differently.
[00:27:32.000 --> 00:27:34.920]   I'm curious to hear your thesis on that.
[00:27:34.920 --> 00:27:35.920]   Okay.
[00:27:35.920 --> 00:27:46.240]   So I'm a VC, I'm only right occasionally, and I believe most things provisionally, right?
[00:27:46.240 --> 00:27:48.240]   But I'm pretty sure about this one.
[00:27:48.240 --> 00:27:56.320]   I'm pretty sure that we underestimate the effect of technical architectures on emerging
[00:27:56.320 --> 00:27:57.320]   business models.
[00:27:57.320 --> 00:28:02.000]   So if you were to go back, I don't know, go back to like Sabre, which IBM builds for,
[00:28:02.000 --> 00:28:03.840]   I guess, American Airlines, right?
[00:28:03.840 --> 00:28:05.360]   When they have a bunch of mainframes.
[00:28:05.360 --> 00:28:09.280]   In some ways that business model, which is, "We'll charge a bunch of money to do custom
[00:28:09.280 --> 00:28:14.400]   development for you," that really comes partly out of the technical architecture of the way
[00:28:14.400 --> 00:28:18.040]   that mainframes were centralized in some other place, right?
[00:28:18.040 --> 00:28:22.920]   And the moment that PCs come around or they start to emerge, there's a way in which we
[00:28:22.920 --> 00:28:28.800]   think about maybe the best business model ever, which is to say, the one that Bill Gates
[00:28:28.800 --> 00:28:34.040]   creates, to which you charge money for the same copy of software over and over again,
[00:28:34.040 --> 00:28:35.040]   right?
[00:28:35.040 --> 00:28:37.320]   It's an incredible business model.
[00:28:37.320 --> 00:28:41.800]   That partly arises because Bill Gates and Microsoft and a bunch of folks were stubborn
[00:28:41.800 --> 00:28:46.560]   and clever and pushed through an idea, but part of it was also because there was a shift
[00:28:46.560 --> 00:28:48.040]   in the technical architecture, right?
[00:28:48.040 --> 00:28:50.160]   That you ended up with a bunch of PCs.
[00:28:50.160 --> 00:28:54.760]   And so then a different business model, because there are different economic characteristics
[00:28:54.760 --> 00:28:59.120]   of how that soft, that technical architecture is both rolled out and how it's developed
[00:28:59.120 --> 00:29:03.040]   and how you get value, then some different business model might make sense.
[00:29:03.040 --> 00:29:05.800]   And then you see the same thing for the web, right?
[00:29:05.800 --> 00:29:12.640]   When you have a ubiquitous client in whatever, 1995, I think everyone realizes that, "Oh,
[00:29:12.640 --> 00:29:14.240]   that means something new."
[00:29:14.240 --> 00:29:19.020]   And it takes five or six years before people come up with the right way to talk about it.
[00:29:19.020 --> 00:29:23.680]   But subscription software really only makes sense and only works in a world where you
[00:29:23.680 --> 00:29:29.240]   have a ubiquitous client that anyone can access from anywhere, which is sort of a shocking
[00:29:29.240 --> 00:29:30.240]   idea now, right?
[00:29:30.240 --> 00:29:35.680]   You compare it to delivering CDs before or before that, someone, I guess, getting a print
[00:29:35.680 --> 00:29:38.240]   out of some code that they were supposed to retype in.
[00:29:38.240 --> 00:29:45.900]   And so that, in each one of those cases, it's enabled and there's some new dominant business
[00:29:45.900 --> 00:29:49.560]   model that comes about because the technical architecture shifts.
[00:29:49.560 --> 00:29:52.480]   And of course, that only enables it.
[00:29:52.480 --> 00:29:56.960]   It's really the people who build the thing and market it and sell it and come up with
[00:29:56.960 --> 00:29:59.000]   the new dominant business model.
[00:29:59.000 --> 00:30:00.300]   They still have to do that.
[00:30:00.300 --> 00:30:05.100]   But it just strikes me that the shift that we're going through right now around machine
[00:30:05.100 --> 00:30:08.560]   learning or data-centric applications or this change in collective intelligence, however
[00:30:08.560 --> 00:30:14.920]   you want to talk about it, the nature of building those applications is different enough and
[00:30:14.920 --> 00:30:21.480]   the technical architecture is different enough that there should be some other business arrangement
[00:30:21.480 --> 00:30:27.560]   that ends up becoming the better one for both consumers and for some new dominant customer.
[00:30:27.560 --> 00:30:32.440]   You think about how on the machine learning model building side, you just think about
[00:30:32.440 --> 00:30:38.440]   just the amount of data you're trying to just own and control and understand and manage.
[00:30:38.440 --> 00:30:41.540]   And you think about how that changes what's a scarce resource.
[00:30:41.540 --> 00:30:42.900]   It just strikes me that there's something there.
[00:30:42.900 --> 00:30:45.360]   And so to be honest, I'm constantly looking.
[00:30:45.360 --> 00:30:48.360]   In my mind, what's my grand dream?
[00:30:48.360 --> 00:30:53.180]   My grand dream is to meet that person who's working inside one of the big companies who's
[00:30:53.180 --> 00:30:59.760]   been frustrated because she's understood how the grain of the wood of machine learning
[00:30:59.760 --> 00:31:04.360]   lends itself to some new business and then her boss's boss is like, "That's stupid.
[00:31:04.360 --> 00:31:06.360]   We need to maintain our margins," or whatever.
[00:31:06.360 --> 00:31:12.320]   And so that's the grand dream, that I'll find that person and be able to invest and partner
[00:31:12.320 --> 00:31:14.720]   with them for a number of years.
[00:31:14.720 --> 00:31:19.480]   In your imagination, are there ways that that model could look?
[00:31:19.480 --> 00:31:23.680]   I mean, I suppose it's a little bit hard to imagine these new things, but subscriptions
[00:31:23.680 --> 00:31:24.680]   have been around for a while.
[00:31:24.680 --> 00:31:30.840]   So do you imagine a move to more of a usage-based pricing or maybe companies that are willing
[00:31:30.840 --> 00:31:33.000]   to pay for your data and combine the data?
[00:31:33.000 --> 00:31:35.800]   I'm trying to picture what this could be.
[00:31:35.800 --> 00:31:36.800]   Right.
[00:31:36.800 --> 00:31:39.520]   So let me describe something.
[00:31:39.520 --> 00:31:45.760]   I led a little conference chat the other day, a little session about this.
[00:31:45.760 --> 00:31:47.800]   And I kind of try to do this anywhere I go.
[00:31:47.800 --> 00:31:51.840]   I try to lead a session on this because I'm kind of obsessed.
[00:31:51.840 --> 00:31:56.920]   And certainly usage-based is quite good and interesting, but I would just contend that
[00:31:56.920 --> 00:32:02.240]   in some ways usage-based sometimes puts me as a vendor at odds with my client because
[00:32:02.240 --> 00:32:05.760]   I just kind of want you to do more of the thing.
[00:32:05.760 --> 00:32:10.760]   And sometimes it's not really useful because I don't want to name names, but we are certainly
[00:32:10.760 --> 00:32:16.320]   in a world right now where people are wasting a lot of money either on compute or storage
[00:32:16.320 --> 00:32:18.040]   without clear business value.
[00:32:18.040 --> 00:32:23.560]   And then they're going to someday actually figure it out and then cause a lot of trouble.
[00:32:23.560 --> 00:32:26.800]   So I think that's the pro and con of usage-based.
[00:32:26.800 --> 00:32:32.160]   There's certainly some notions around data co-ops where the realization as these models
[00:32:32.160 --> 00:32:38.720]   get better, when we share our data and then we share data, maybe we share upside together.
[00:32:38.720 --> 00:32:42.600]   I think there are a bunch of folks who are trying variations of that.
[00:32:42.600 --> 00:32:48.360]   The dream of course always is to be in perfect alignment with your customer.
[00:32:48.360 --> 00:32:54.280]   And one way that happens is you have something like a value-added tax or a VIG, right?
[00:32:54.280 --> 00:32:56.720]   Where you benefit when they benefit.
[00:32:56.720 --> 00:33:01.640]   But right now in the world that we live in, understanding that benefit is so hard, right?
[00:33:01.640 --> 00:33:07.880]   Because it requires an enormous amount of infrastructure and management layers and A/B
[00:33:07.880 --> 00:33:08.880]   testing and blah, blah.
[00:33:08.880 --> 00:33:12.000]   Just think about all the problems, all the reasons why it's never worked.
[00:33:12.000 --> 00:33:13.600]   And maybe someone will figure that out, right?
[00:33:13.600 --> 00:33:19.720]   Maybe all the objections that we've had for the last X years around why this sort of benefit-driven
[00:33:19.720 --> 00:33:25.040]   business model doesn't work, maybe it'll work in some twist or turn of how we think about
[00:33:25.040 --> 00:33:29.040]   machine learning models.
[00:33:29.040 --> 00:33:34.840]   You had me convinced many years ago that a competitor would come along to Salesforce
[00:33:34.840 --> 00:33:38.120]   that would aggregate the data and use it in smart ways.
[00:33:38.120 --> 00:33:44.040]   And Salesforce has this inherent disadvantage because they're so careful about keeping everybody's
[00:33:44.040 --> 00:33:47.960]   data separate and not building models on top of it.
[00:33:47.960 --> 00:33:51.680]   Do you still believe that's coming or do you think there is some wrong assumption that
[00:33:51.680 --> 00:33:54.800]   you're making or has it happened quietly and I haven't noticed it?
[00:33:54.800 --> 00:33:56.200]   No, it hasn't happened yet.
[00:33:56.200 --> 00:34:00.320]   I mean, look, Salesforce is this enduring great business, right?
[00:34:00.320 --> 00:34:03.000]   That's going to last for decades and decades.
[00:34:03.000 --> 00:34:07.640]   That said, it still does strike me that there's an inherent tension.
[00:34:07.640 --> 00:34:12.440]   You think about all the trouble that they spent convincing me or convincing people like
[00:34:12.440 --> 00:34:18.160]   me to work with them because we believe that the data was safe in their cloud, right?
[00:34:18.160 --> 00:34:23.040]   And then just the idea that I might share data with other clients is crazy and terrible,
[00:34:23.040 --> 00:34:24.760]   at least from that point of view.
[00:34:24.760 --> 00:34:30.840]   And so there's that inherent tension in the traditional or the now established SaaS view
[00:34:30.840 --> 00:34:31.840]   of the world.
[00:34:31.840 --> 00:34:37.600]   And I think it's very hard for the incumbents then to move off of that way of thinking.
[00:34:37.600 --> 00:34:42.120]   Thinking about the world, but harder yet is convincing their clients and their customers
[00:34:42.120 --> 00:34:45.320]   who've been trained to think that way, right?
[00:34:45.320 --> 00:34:51.560]   There's a funny, maybe not funny story around Microsoft, where Microsoft got in a lot of
[00:34:51.560 --> 00:34:58.160]   trouble at some point for sending information back to their main servers about how PCs were
[00:34:58.160 --> 00:35:01.960]   doing because they would crash or there'd be some bug reported and they'd automatically
[00:35:01.960 --> 00:35:02.960]   send it back.
[00:35:02.960 --> 00:35:06.960]   It was a huge scandal because how could Microsoft be looking and stealing all my information?
[00:35:06.960 --> 00:35:10.880]   And the hilarious thing, not hilarious to Microsoft, but the hilarious thing about that
[00:35:10.880 --> 00:35:13.160]   is that's right as Google Docs is starting.
[00:35:13.160 --> 00:35:17.720]   And in the case of Google Docs, Google literally sees every single thing I type, right?
[00:35:17.720 --> 00:35:22.560]   I mean, it's literally stored on their servers and somehow because it's a different configuration
[00:35:22.560 --> 00:35:26.040]   or different expectations around the business, I'm okay about it.
[00:35:26.040 --> 00:35:35.000]   And I think something similar will happen with emerging sets of machine learning driven
[00:35:35.000 --> 00:35:36.000]   businesses.
[00:35:36.000 --> 00:35:43.840]   Although it's interesting that you say that, you had a really interesting viral tweet at
[00:35:43.840 --> 00:35:50.720]   one point showing how much better Google's transcription was than Apple's, which I thought
[00:35:50.720 --> 00:35:56.840]   was really interesting and actually made me think about the same point of Apple is so
[00:35:56.840 --> 00:36:02.800]   kind of known for being careful with privacy and Google is known for being much more laissez-faire,
[00:36:02.800 --> 00:36:07.440]   I guess, with people's data.
[00:36:07.440 --> 00:36:16.000]   But it's not clear to me that Google has used that perspective to create a huge advantage,
[00:36:16.000 --> 00:36:18.440]   at least in terms of market cap.
[00:36:18.440 --> 00:36:23.600]   Do you think over time Google's point of view will really serve it or has something changed?
[00:36:23.600 --> 00:36:24.600]   Okay.
[00:36:24.600 --> 00:36:27.560]   So I think in that case, it's a little bit of a slightly different nuance thing, right?
[00:36:27.560 --> 00:36:30.680]   I mean, why was that Pixel 6 voice recorder so much better?
[00:36:30.680 --> 00:36:34.040]   It was better in part because they had an on-device model, right?
[00:36:34.040 --> 00:36:35.320]   That was one part.
[00:36:35.320 --> 00:36:42.360]   And another part of it is that they just collected data in much more thoughtful ways.
[00:36:42.360 --> 00:36:43.720]   And so what did that mean?
[00:36:43.720 --> 00:36:49.840]   That meant you had a very fast, very accurate local experience.
[00:36:49.840 --> 00:36:56.060]   The fact that that's true is also, that's definitely true, but it's also confounded
[00:36:56.060 --> 00:36:59.240]   with the fact that Google is a very large organization right now and they've got lots
[00:36:59.240 --> 00:37:03.600]   of things that they worry about and lots of ways that they're unwilling to take risk.
[00:37:03.600 --> 00:37:08.640]   In my ideal world, someone who built the sort of technology that Google did around voice
[00:37:08.640 --> 00:37:09.960]   would have decided that, "Oh, you know what?
[00:37:09.960 --> 00:37:14.040]   Actually, this should be part of some SDK or some API, and we should just make this
[00:37:14.040 --> 00:37:17.440]   available for everyone and developers should be building a bunch of products."
[00:37:17.440 --> 00:37:20.800]   I mean, I think that's the other thing that I think we're on the cusp of because we're
[00:37:20.800 --> 00:37:26.640]   just at this point where there's this massive investment in infrastructure and research
[00:37:26.640 --> 00:37:28.440]   and tooling around machine learning.
[00:37:28.440 --> 00:37:32.680]   And we're right at the point where maybe people will build products that are actually
[00:37:32.680 --> 00:37:33.680]   good.
[00:37:33.680 --> 00:37:39.720]   We're just at the point where the lessons learned around how human-in-the-loop works,
[00:37:39.720 --> 00:37:45.160]   the lessons learned around experiences on user interface, all those things, they don't
[00:37:45.160 --> 00:37:49.640]   quite take or value-add it to the end user.
[00:37:49.640 --> 00:37:53.680]   We're just at the point where there'll be enough variation that some ideas will actually
[00:37:53.680 --> 00:37:54.680]   take hold.
[00:37:54.680 --> 00:37:57.320]   And so I'm sort of excited about that part too.
[00:37:57.320 --> 00:37:59.560]   Are you starting to see that?
[00:37:59.560 --> 00:38:05.180]   Because I feel like maybe I'm too impatient, but I kind of can't believe how much better
[00:38:05.180 --> 00:38:08.680]   all aspects of NLP have gotten in the last few years.
[00:38:08.680 --> 00:38:12.280]   I feel like transcription is now a solid translation.
[00:38:12.280 --> 00:38:15.680]   Now it works.
[00:38:15.680 --> 00:38:16.680]   It basically works.
[00:38:16.680 --> 00:38:20.880]   You can communicate for sure with people that you don't speak the same language with by
[00:38:20.880 --> 00:38:26.560]   using a translation system.
[00:38:26.560 --> 00:38:32.560]   Hugging Face and OpenAI's GPT-3 have just had incredible demos.
[00:38:32.560 --> 00:38:38.920]   And yet I don't feel like it's impacting my life that much, except for asking Alexa to
[00:38:38.920 --> 00:38:39.920]   play me music.
[00:38:39.920 --> 00:38:43.000]   I mean, well, you're exactly right.
[00:38:43.000 --> 00:38:47.320]   We're at that point right now where I'm hoping your listeners are building products because
[00:38:47.320 --> 00:38:49.800]   now it's easier to access it.
[00:38:49.800 --> 00:38:52.880]   There's this talk about democratization of machine learning.
[00:38:52.880 --> 00:38:56.960]   We talk about this often, I feel like, but I think it kind of misses the point.
[00:38:56.960 --> 00:39:03.800]   The point is by making this more broadly available, it also means that the extraordinary person
[00:39:03.800 --> 00:39:08.120]   on the edge who might not have had access to try this before, the person with the crazy
[00:39:08.120 --> 00:39:12.760]   idea that will make a huge difference once we actually see it, that they can start working
[00:39:12.760 --> 00:39:13.760]   as well.
[00:39:13.760 --> 00:39:20.360]   And I think that that's part of the exciting thing that I think everyone misses as they
[00:39:20.360 --> 00:39:22.640]   talk about the way that this whole world is shifting.
[00:39:22.640 --> 00:39:27.360]   But you're exactly right, that we should be deeply dissatisfied with, on the one hand,
[00:39:27.360 --> 00:39:31.760]   all the progress that's made with voice and parts of NLP.
[00:39:31.760 --> 00:39:35.400]   We should be super impressed with it and we should be deeply dissatisfied because the
[00:39:35.400 --> 00:39:42.240]   products and the product minds and the UI folks and the business minds have not yet
[00:39:42.240 --> 00:39:46.600]   figured out how to take advantage of those advances in ways that actually make sense
[00:39:46.600 --> 00:39:51.360]   and go with the grain of the technology.
[00:39:51.360 --> 00:39:57.080]   One thing that I would imagine being hard as an early stage investor investing in machine
[00:39:57.080 --> 00:40:01.720]   learning is that it's so easy to demo successful cases of machine learning.
[00:40:01.720 --> 00:40:06.380]   I feel like no other field is quite as easy to make a compelling demo and yet it feels
[00:40:06.380 --> 00:40:12.880]   like to make a working product, it's often going from getting the error rate down from
[00:40:12.880 --> 00:40:15.960]   like 1% to 0.1% or something like that.
[00:40:15.960 --> 00:40:16.960]   Do you have trouble with value?
[00:40:16.960 --> 00:40:17.960]   Okay, so here's my secret.
[00:40:17.960 --> 00:40:21.160]   Okay, so I'll give you one of my current secrets.
[00:40:21.160 --> 00:40:24.080]   Okay, tell me.
[00:40:24.080 --> 00:40:26.800]   So I just assume it doesn't get better.
[00:40:26.800 --> 00:40:33.400]   If the application requires the thing to go from 95 to 98 or 98 to 99, the mental exercise,
[00:40:33.400 --> 00:40:34.720]   okay, what if it doesn't get better?
[00:40:34.720 --> 00:40:36.560]   Will users still get value out of it?
[00:40:36.560 --> 00:40:41.320]   And if users still get value out of it because of the way they configure the problem, right,
[00:40:41.320 --> 00:40:42.840]   then it's an interesting product, right?
[00:40:42.840 --> 00:40:46.640]   But if you're sitting there thinking, you know, we just spend, it'll just be another
[00:40:46.640 --> 00:40:52.360]   month before we go from 98 to 99.5, then I'm like, well, you know, I don't really know
[00:40:52.360 --> 00:40:53.360]   if I believe that.
[00:40:53.360 --> 00:40:54.680]   I mean, think about this.
[00:40:54.680 --> 00:40:58.380]   This goes back to like one of our earliest conversations around search quality.
[00:40:58.380 --> 00:41:00.400]   This was like many, many years ago.
[00:41:00.400 --> 00:41:01.720]   And like, what's the beauty of search?
[00:41:01.720 --> 00:41:05.600]   The beauty of search is that when it's wrong, I'm okay about it.
[00:41:05.600 --> 00:41:09.080]   And there are whole sets of products in which like you can take advantage of the fact that
[00:41:09.080 --> 00:41:11.880]   it's super fast, it's consistent.
[00:41:11.880 --> 00:41:13.560]   And when it's wrong, I'm okay about it.
[00:41:13.560 --> 00:41:14.560]   Right?
[00:41:14.560 --> 00:41:16.760]   You know, I feel like you do that over and over again, or you find the products that
[00:41:16.760 --> 00:41:20.560]   do that, then like, those are interesting applications.
[00:41:20.560 --> 00:41:24.400]   So for an investor, you're doing an extraordinary job of not bragging about your portfolio.
[00:41:24.400 --> 00:41:26.960]   But I mean, give me some glimpse of the future.
[00:41:26.960 --> 00:41:30.880]   What's the, what are the exciting stuff that you're seeing lately?
[00:41:30.880 --> 00:41:37.760]   You know, sort of, I mean, part of it is, okay, as we think about, well, there are two
[00:41:37.760 --> 00:41:40.960]   parts that I want to talk, like I sort of want to highlight, you know, sort of on the
[00:41:40.960 --> 00:41:46.560]   ML infrastructure piece, I still think that there are analogies or lessons to be learned
[00:41:46.560 --> 00:41:48.120]   from traditional software development.
[00:41:48.120 --> 00:41:49.120]   Right?
[00:41:49.120 --> 00:41:52.800]   I think that you guys have done such a good job of understanding so many pieces out of
[00:41:52.800 --> 00:41:53.800]   that.
[00:41:53.800 --> 00:41:58.280]   And then I, but I still think like, you know, you think about like QA, like, like sort of
[00:41:58.280 --> 00:42:03.440]   figuring out how to consistently do QA, like sort of, I think that like, there are lots
[00:42:03.440 --> 00:42:08.080]   of lessons to be learned from normal software development to be applied to computer vision
[00:42:08.080 --> 00:42:12.280]   and structured data and those sorts of like, those sorts of release processes.
[00:42:12.280 --> 00:42:16.560]   And so there's a company called Colina that sort of in the middle of figuring out parts
[00:42:16.560 --> 00:42:17.560]   of that.
[00:42:17.560 --> 00:42:22.600]   I think that like, you look at companies like, well, you know, we talked to Sean about, we
[00:42:22.600 --> 00:42:24.040]   talked about Sean every so often.
[00:42:24.040 --> 00:42:29.600]   Like, you know, you think about, you look at the demo, like the publicly available stuff
[00:42:29.600 --> 00:42:31.200]   about Primer, right?
[00:42:31.200 --> 00:42:35.720]   And you look at it, just imagine sort of what they're actually doing under the hood.
[00:42:35.720 --> 00:42:39.160]   You know, if you go to Primer.ai and you look at sort of their ability to like synthetically
[00:42:39.160 --> 00:42:44.360]   generate their ability to synthesize huge amounts of data and lots and lots of articles
[00:42:44.360 --> 00:42:48.400]   and just make sense of the world and imagine applying that in their case to like a bunch
[00:42:48.400 --> 00:42:51.560]   of national security use cases.
[00:42:51.560 --> 00:42:54.840]   I think they've done, I don't know, if you look up various things that are happening
[00:42:54.840 --> 00:42:59.400]   in the world right now and the word Primer, you'll see these demos and it's sort of, you
[00:42:59.400 --> 00:43:02.520]   know, they can't show you what they're actually doing, but like you get that sense of like,
[00:43:02.520 --> 00:43:08.440]   oh, this is changing the way that people are actually sort of doing things right now.
[00:43:08.440 --> 00:43:12.920]   So that's, you know, sort of, that's the sort of thing that I feel like on the application
[00:43:12.920 --> 00:43:19.320]   layer, but then also like sort of in the development part, we're just sitting on right now.
[00:43:19.320 --> 00:43:24.440]   There's going back to like your point around my secret arb, right.
[00:43:24.440 --> 00:43:29.640]   Which is, I just sort of assume it's not necessarily going to get that much better.
[00:43:29.640 --> 00:43:33.720]   There's this great guy, Michael Cohen at this company called Spark AI.
[00:43:33.720 --> 00:43:38.000]   And their big insight is similar to that line, which is like, they're like, look, we want
[00:43:38.000 --> 00:43:41.080]   autonomous vehicles and we want them to be perfect, but they're not going to be perfect
[00:43:41.080 --> 00:43:42.080]   for a long time.
[00:43:42.080 --> 00:43:43.960]   And so let's just make sure there's a human in the loop.
[00:43:43.960 --> 00:43:44.960]   Right.
[00:43:44.960 --> 00:43:49.280]   And so you can think of them as like in some ways, sort of like whenever the machine is
[00:43:49.280 --> 00:43:52.800]   uncertain about something right in front of them, they'll get a response in like, you
[00:43:52.800 --> 00:43:55.960]   know, a pretty short SLA then to make a decision.
[00:43:55.960 --> 00:43:58.100]   And thus you can actually roll out these applications.
[00:43:58.100 --> 00:44:02.800]   You can roll out these sort of in the real world applications with the realization that
[00:44:02.800 --> 00:44:04.940]   the model doesn't have to be perfect, right.
[00:44:04.940 --> 00:44:06.480]   That we can actually have backup systems.
[00:44:06.480 --> 00:44:12.420]   And I think that sort of perspective, assuming like the sort of non-utopian view of what's
[00:44:12.420 --> 00:44:15.920]   possible with machine learning is super exciting to me.
[00:44:15.920 --> 00:44:26.520]   I'm curious what you think about, and I guess this is a broad question, but about ethical
[00:44:26.520 --> 00:44:28.160]   implications of machine learning.
[00:44:28.160 --> 00:44:33.440]   I mean, many, many people talk about machine learning and ethics and there's, I feel like
[00:44:33.440 --> 00:44:39.800]   there's constantly in the news, you know, issues that come up with machine learning.
[00:44:39.800 --> 00:44:40.800]   What do you make of it?
[00:44:40.800 --> 00:44:47.000]   Do you feel like there's sort of special ethical considerations unique to machine learning,
[00:44:47.000 --> 00:44:49.400]   different than technology or not?
[00:44:49.400 --> 00:44:53.560]   And I mean, how do you think about like what, you know, what kind of world you want and
[00:44:53.560 --> 00:44:57.800]   what regulations make sense?
[00:44:57.800 --> 00:45:02.800]   So you know, I think I have, I think it's a good thing that we live in a world where
[00:45:02.800 --> 00:45:05.120]   people are more sensitized, right.
[00:45:05.120 --> 00:45:06.120]   On the one hand.
[00:45:06.120 --> 00:45:11.920]   So I'm very glad to see lots of people applying their minds towards it on the one hand.
[00:45:11.920 --> 00:45:12.920]   On the other, okay.
[00:45:12.920 --> 00:45:15.200]   So, and this might slightly get me in trouble.
[00:45:15.200 --> 00:45:19.240]   There's like a game that I play with friends of mine who are ethicists or thinking about
[00:45:19.240 --> 00:45:20.240]   sort of the effects of technology.
[00:45:20.240 --> 00:45:25.800]   And I sort of ask, you know, sort of, I think it's appropriate to ask these questions around
[00:45:25.800 --> 00:45:29.480]   sort of what are the implications of this or that.
[00:45:29.480 --> 00:45:35.120]   But if you were around in like 1950, whatever, and someone proposed the compiler to you for
[00:45:35.120 --> 00:45:38.040]   the first time, someone said, you know, we've got this really, really great way of like
[00:45:38.040 --> 00:45:44.800]   making software easier to develop and available and mass and scale and et cetera, et cetera.
[00:45:44.800 --> 00:45:47.000]   Would you have allowed me to build a compiler?
[00:45:47.000 --> 00:45:50.120]   Just imagine all the harm that could come from a compiler.
[00:45:50.120 --> 00:45:53.240]   And imagine like, to be honest, like all the harm that has actually come from compilers,
[00:45:53.240 --> 00:45:54.240]   right.
[00:45:54.240 --> 00:45:57.640]   Everything from hacking to stealing money from people, et cetera, et cetera.
[00:45:57.640 --> 00:46:04.360]   And there's a way in which like, I think there's a reasonable argument that like we, we, we
[00:46:04.360 --> 00:46:09.160]   should like, given some current frameworks, there's an argument for why we should not
[00:46:09.160 --> 00:46:13.800]   have had a compiler, which seems on the face of it, at least to me, crazy, right?
[00:46:13.800 --> 00:46:14.800]   Like absurd.
[00:46:14.800 --> 00:46:21.640]   And I, and so to me, the questions instead should, there should be the sensitivity and
[00:46:21.640 --> 00:46:28.200]   there should be these sets of questions, but in some ways the questions should all be around
[00:46:28.200 --> 00:46:31.600]   how do we think about what do we do if we're wrong?
[00:46:31.600 --> 00:46:36.600]   And I think one of the beauties of machine learning is that embedded in machine learning
[00:46:36.600 --> 00:46:40.920]   at the very core of machine learning is this idea that these are not fixed heuristics or
[00:46:40.920 --> 00:46:45.600]   business rules, but actually these are the sort of like these kind of guesses that we're
[00:46:45.600 --> 00:46:48.080]   just have to assume that it'll be wrong sometimes.
[00:46:48.080 --> 00:46:49.080]   Right.
[00:46:49.080 --> 00:46:53.680]   And so in that way, once you think from that framework or once your executives understand
[00:46:53.680 --> 00:46:57.200]   that's how they think, how models actually work, that they're wrong, they're never going
[00:46:57.200 --> 00:46:58.200]   to be perfect.
[00:46:58.200 --> 00:47:00.120]   Otherwise you can have a big if then statement, right.
[00:47:00.120 --> 00:47:03.480]   And then sort of once you realize that they could be wrong, then you need to build the
[00:47:03.480 --> 00:47:07.480]   systems and the processes to deal with the fact that they could be wrong.
[00:47:07.480 --> 00:47:12.000]   And you also need to build a whole set of ethics and ways of thinking about like questions
[00:47:12.000 --> 00:47:15.320]   more like responsibility rather than possibilities.
[00:47:15.320 --> 00:47:16.320]   Right.
[00:47:16.320 --> 00:47:20.960]   And I think that that shift in the way you might think about sort of machine learning,
[00:47:20.960 --> 00:47:25.440]   I think it will be much more profitable in the sense of being useful for humanity.
[00:47:25.440 --> 00:47:27.000]   What do you think?
[00:47:27.000 --> 00:47:34.560]   I guess it does feel like machine learning might not be as neutral as compilers in some
[00:47:34.560 --> 00:47:44.360]   cases if you imagine it sort of taking inherent biases that we have in our society and then
[00:47:44.360 --> 00:47:50.800]   like encoding them in a very efficient system so that they can be sort of deployed at bigger
[00:47:50.800 --> 00:47:54.280]   scale and with possibly less oversight.
[00:47:54.280 --> 00:47:55.280]   Right.
[00:47:55.280 --> 00:48:00.360]   I think, okay, so that's only if you fall for the idea that we're trying to build an
[00:48:00.360 --> 00:48:04.240]   all knowing God brain that will solve things for us perfectly.
[00:48:04.240 --> 00:48:09.160]   But instead I think, and I think, and I think, you know, to be honest, like oftentimes when
[00:48:09.160 --> 00:48:12.320]   you'll talk to executives, that's how they will think about machine learning, right.
[00:48:12.320 --> 00:48:15.960]   They'll think if only we can get this perfect, then like we can rely on it forever.
[00:48:15.960 --> 00:48:20.000]   But instead of we thought about as a bureaucracy that is right some of the time, but wrong
[00:48:20.000 --> 00:48:21.000]   too, right.
[00:48:21.000 --> 00:48:25.360]   And we said, we thought about as like a possibly fallible system and we built in the support
[00:48:25.360 --> 00:48:29.200]   for that because remember the nice thing about machine learning is that it's incredibly cheap.
[00:48:29.200 --> 00:48:33.800]   Like in the grand scheme of things, it's incredibly cheap to make these judgments on the one hand.
[00:48:33.800 --> 00:48:36.160]   And also it's centralized, right.
[00:48:36.160 --> 00:48:41.240]   And by being centralized and being cheap and conscientious, meaning it's like consistent,
[00:48:41.240 --> 00:48:46.320]   then you actually have like one place where you can go and you can always say, if we fix
[00:48:46.320 --> 00:48:48.160]   it here, we can fix it everywhere.
[00:48:48.160 --> 00:48:49.160]   Right.
[00:48:49.160 --> 00:48:50.160]   So that's one part of it.
[00:48:50.160 --> 00:48:54.040]   I think the other part that you highlighted, which is it captures inherent biases.
[00:48:54.040 --> 00:48:58.440]   I think that's the other part, which is like, in some ways it's a problem with the way that
[00:48:58.440 --> 00:49:00.120]   we anthropomorphize machine learning.
[00:49:00.120 --> 00:49:06.920]   Like one way to think about it is this amazing, whatever, mind genius thing.
[00:49:06.920 --> 00:49:10.760]   On the other hand, you could just think of it as like an incredibly conservative attempt
[00:49:10.760 --> 00:49:13.400]   to cluster collective intelligence, right.
[00:49:13.400 --> 00:49:18.600]   That if we understood that machine learning was derived from data and data is by nature
[00:49:18.600 --> 00:49:22.960]   historical and anything historical by nature happened in the past, right.
[00:49:22.960 --> 00:49:27.040]   Then I think that changes a little bit your expectations about what the model could do,
[00:49:27.040 --> 00:49:28.040]   right.
[00:49:28.040 --> 00:49:31.120]   On the one hand, and then it changes your expectations around what layers you need to
[00:49:31.120 --> 00:49:35.920]   put on top of it, because you can't just rely on the model, right.
[00:49:35.920 --> 00:49:39.720]   You're going to have to have both sort of straightforward business rules to protect yourself,
[00:49:39.720 --> 00:49:43.760]   but also you also have human processes, to be honest, that are actually thinking through.
[00:49:43.760 --> 00:49:48.840]   So I do have to, at this point, make the plug for one of my favorite papers, which is called
[00:49:48.840 --> 00:49:53.600]   Street Level Algorithms, which sort of talks a little bit about that, which talks a little
[00:49:53.600 --> 00:49:54.600]   bit about the site.
[00:49:54.600 --> 00:49:55.600]   So you'll have to link to it.
[00:49:55.600 --> 00:49:57.600]   I don't know if you've, have you read it?
[00:49:57.600 --> 00:49:58.600]   No, no.
[00:49:58.600 --> 00:49:59.600]   Okay.
[00:49:59.600 --> 00:50:01.280]   I think I've tried to make you read it many times.
[00:50:01.280 --> 00:50:02.360]   It's totally worth reading.
[00:50:02.360 --> 00:50:06.920]   You should get Ali or Michael Bernstein to chat about it at some point.
[00:50:06.920 --> 00:50:12.420]   But I think like their core insight is that if you did think about machine learning models
[00:50:12.420 --> 00:50:17.880]   as bureaucracies or as like sort of processes that could be wrong some of the time, that
[00:50:17.880 --> 00:50:22.000]   you change your expectations, but also like the ways that you can take advantage of machine
[00:50:22.000 --> 00:50:25.680]   learning, which is say like you fix it in one place, you fix it for everyone, right?
[00:50:25.680 --> 00:50:29.400]   Those sorts of inherent advantages go with the grain of the technology rather than against
[00:50:29.400 --> 00:50:30.400]   it.
[00:50:30.400 --> 00:50:37.360]   Have you ever gotten a pitch on a company and not invested because it made you uncomfortable,
[00:50:37.360 --> 00:50:39.360]   like from a, like an ethical perspective?
[00:50:39.360 --> 00:50:40.360]   Oh yeah.
[00:50:40.360 --> 00:50:41.840]   I mean, plenty of times, right?
[00:50:41.840 --> 00:50:42.840]   And I think-
[00:50:42.840 --> 00:50:43.840]   Really plenty of times.
[00:50:43.840 --> 00:50:50.280]   I mean, there are plenty of times when I will say, I mean, on the one hand I'm utility maximizing,
[00:50:50.280 --> 00:50:53.200]   but then I have my own idiosyncratic definition of utility.
[00:50:53.200 --> 00:50:58.360]   And my definition of utility isn't mapped directly to just dollars, but maps into ideas
[00:50:58.360 --> 00:51:02.040]   of who I am and what kind of person I want to be and what kind of world I want to be
[00:51:02.040 --> 00:51:03.040]   in.
[00:51:03.040 --> 00:51:05.440]   And I think that that's true about all VCs, right?
[00:51:05.440 --> 00:51:10.160]   That, you know, VCs, like everyone pretends that they're like, or rather a lot of people
[00:51:10.160 --> 00:51:14.520]   pretend that they're sort of pretty straightforward and utility, like dollar maximizing, but that's
[00:51:14.520 --> 00:51:15.520]   not true.
[00:51:15.520 --> 00:51:18.600]   We all have tastes and we all have things that we like or don't like and good or bad
[00:51:18.600 --> 00:51:20.600]   reasons to say yes or no to things.
[00:51:20.600 --> 00:51:25.240]   And I think that reality is always sitting with us.
[00:51:25.240 --> 00:51:29.960]   Is there a company that you feel like you've massively misjudged?
[00:51:29.960 --> 00:51:33.920]   Like is there any, is there any like wildly successful business where you go back and
[00:51:33.920 --> 00:51:39.080]   like think about the pitch and, and feel like you like, like miss something or should update
[00:51:39.080 --> 00:51:41.480]   your belief system?
[00:51:41.480 --> 00:51:43.320]   I mean, constantly, right.
[00:51:43.320 --> 00:51:48.880]   You know, sort of the whole set of low code, no code companies that I sort of dismissed,
[00:51:48.880 --> 00:51:52.120]   like if you, I think, I don't know if you remember this conversation, like there's some
[00:51:52.120 --> 00:51:59.040]   point when we chatted where I basically said that, you know what I really believe in?
[00:51:59.040 --> 00:52:01.520]   I believe in domain specific languages.
[00:52:01.520 --> 00:52:06.440]   I think that DSLs are a much more powerful way to express business applications and the
[00:52:06.440 --> 00:52:11.960]   possibility for business applications than, you know, sort of then all these low code,
[00:52:11.960 --> 00:52:12.960]   no code things.
[00:52:12.960 --> 00:52:15.000]   And I was totally wrong, totally wrong.
[00:52:15.000 --> 00:52:24.680]   I entirely misjudged the, the value add of making something easy and the way part of,
[00:52:24.680 --> 00:52:28.400]   in part of my head, I was like, well, you know, like a developer is valuable, not just
[00:52:28.400 --> 00:52:31.280]   because they can write things in good syntax.
[00:52:31.280 --> 00:52:34.840]   They're also valuable because they have to think through complicated ideas, abstract
[00:52:34.840 --> 00:52:39.240]   them and come up with like good code to actually build something, to get something to work.
[00:52:39.240 --> 00:52:43.160]   And what I misjudged was that there are a whole set of just like low level glue things
[00:52:43.160 --> 00:52:44.720]   that people need every day, right.
[00:52:44.720 --> 00:52:48.960]   That are super easy to do, that sort of fall right under the cusp of comp, and then sort
[00:52:48.960 --> 00:52:51.880]   of really scary programming.
[00:52:51.880 --> 00:52:53.880]   And so that I totally misjudged.
[00:52:53.880 --> 00:52:59.880]   Well, one topic that we've actually never talked about, but I kind of wanted to use
[00:52:59.880 --> 00:53:04.820]   this podcast as an excuse to ask you is, I'm curious what you think about AI and consciousness.
[00:53:04.820 --> 00:53:11.080]   Like can you picture AI becoming consciousness as something that you think you could imagine
[00:53:11.080 --> 00:53:14.680]   happening in your, in your children's lifetimes?
[00:53:14.680 --> 00:53:15.680]   What does that mean?
[00:53:15.680 --> 00:53:21.360]   I guess, like, could you imagine that there's an ML system that gets to the point where
[00:53:21.360 --> 00:53:24.680]   you would not want to hurt it?
[00:53:24.680 --> 00:53:28.920]   Where you would sort of like care to, about its, its, its wellbeing.
[00:53:28.920 --> 00:53:29.920]   Okay.
[00:53:29.920 --> 00:53:32.920]   So there are a couple of different angles that I go on with this.
[00:53:32.920 --> 00:53:34.320]   I think that's true right now.
[00:53:34.320 --> 00:53:38.960]   I feel bad when, like I do lots of things to anthropomorphize sort of, I feel kind of
[00:53:38.960 --> 00:53:40.360]   bad when I drop my phone, right?
[00:53:40.360 --> 00:53:42.560]   I feel really guilty and I feel kind of bad about it.
[00:53:42.560 --> 00:53:43.560]   Really?
[00:53:43.560 --> 00:53:44.560]   For your phone?
[00:53:44.560 --> 00:53:45.560]   For my phone?
[00:53:45.560 --> 00:53:46.560]   Yeah.
[00:53:46.560 --> 00:53:50.560]   Like I feel like, and, and I think there are lots of ways that I as a human sort of assume
[00:53:50.560 --> 00:53:53.480]   human-like characteristics to almost everything, right?
[00:53:53.480 --> 00:53:59.720]   From the weather to my camera, to like the screen, to like some computer program.
[00:53:59.720 --> 00:54:00.720]   I get irritated.
[00:54:00.720 --> 00:54:04.360]   Like, why do I get, I get irritated with Chrome as if it's an actual person.
[00:54:04.360 --> 00:54:06.400]   Like it's just like a bundle of numbers.
[00:54:06.400 --> 00:54:07.400]   Right?
[00:54:07.400 --> 00:54:09.920]   And, and so I actually think that we're there already.
[00:54:09.920 --> 00:54:16.920]   I actually don't think that, I don't think that like my willingness to imbue moral worth
[00:54:16.920 --> 00:54:23.200]   or value to non-human things is something that's out there someday, but actually it's
[00:54:23.200 --> 00:54:27.480]   something that we're doing, we do all the time right now.
[00:54:27.480 --> 00:54:33.360]   And then although I am Christian, which we've talked about before, like I don't really take
[00:54:33.360 --> 00:54:35.680]   a magical point of view on consciousness.
[00:54:35.680 --> 00:54:40.800]   I think consciousness is controlling what I pay attention to and the continuing log
[00:54:40.800 --> 00:54:45.320]   to sort of walk through, like imagine, you know, sort of, and so, you know, like in that
[00:54:45.320 --> 00:54:48.520]   way and that continuing log to sort of explain what I thought before.
[00:54:48.520 --> 00:54:49.520]   Right.
[00:54:49.520 --> 00:54:55.040]   And so, so I probably don't, I mean, I both value it.
[00:54:55.040 --> 00:54:58.480]   I think it's really, really important and it's like an incredibly important organizing
[00:54:58.480 --> 00:55:01.280]   principle obviously for me day to day.
[00:55:01.280 --> 00:55:06.600]   And I kind of think that lots of things are conscious already, right?
[00:55:06.600 --> 00:55:11.740]   That they already figure out ways to direct attention and organize and also tell stories
[00:55:11.740 --> 00:55:13.440]   about themselves.
[00:55:13.440 --> 00:55:20.200]   Does your Christianity not inform your thoughts about consciousness at all?
[00:55:20.200 --> 00:55:21.200]   It totally does.
[00:55:21.200 --> 00:55:27.720]   It certainly, but I mean, I think there's a little bit of this angle where I, I think
[00:55:27.720 --> 00:55:34.680]   that the things we learn about the world or science constantly shift.
[00:55:34.680 --> 00:55:42.400]   And so I'm actually quite open and willing to sort of adopt and adjust based on how we
[00:55:42.400 --> 00:55:44.440]   end up changing our view of the universe.
[00:55:44.440 --> 00:55:45.440]   I don't know.
[00:55:45.440 --> 00:55:46.440]   Does that make sense?
[00:55:46.440 --> 00:55:47.440]   Yeah, totally.
[00:55:47.440 --> 00:55:48.440]   Is that like a coherent?
[00:55:48.440 --> 00:55:51.720]   I mean, I guess it always makes it concrete for me that I also was telling you, I had
[00:55:51.720 --> 00:55:55.720]   to ask you and I don't know how you felt about it, but I always curious if people would go
[00:55:55.720 --> 00:55:58.760]   through that Star Trek transporter.
[00:55:58.760 --> 00:56:02.840]   If you saw a whole bunch of people go through a thing that disassembled their atoms and
[00:56:02.840 --> 00:56:07.040]   put them back together somewhere else safely and you were convinced that it would work,
[00:56:07.040 --> 00:56:10.600]   would you subject yourself to that?
[00:56:10.600 --> 00:56:12.920]   Would that alarm you or not?
[00:56:12.920 --> 00:56:13.920]   Okay.
[00:56:13.920 --> 00:56:16.240]   So I have contradictory impulses.
[00:56:16.240 --> 00:56:22.000]   I get carsick, I get woozy standing up on, walking over a bridge.
[00:56:22.000 --> 00:56:26.840]   So I'm sure there'd be that trepidation, but isn't there also this view, like when you
[00:56:26.840 --> 00:56:36.200]   think about yourself right now versus yourself, I don't know, whatever, 10 years ago, a bunch
[00:56:36.200 --> 00:56:39.240]   of the atoms have changed, have been replaced.
[00:56:39.240 --> 00:56:45.000]   And in some ways we are going through this slow motion transportation.
[00:56:45.000 --> 00:56:51.480]   I mean, in some ways you're just speeding up that transformation of the rearrangement
[00:56:51.480 --> 00:56:52.880]   of those bits.
[00:56:52.880 --> 00:56:59.560]   And so I probably wouldn't be the first person to do it, but I don't know.
[00:56:59.560 --> 00:57:00.560]   You'd be like the 100th?
[00:57:00.560 --> 00:57:05.000]   Meaning that I would not necessarily have some deep, deep ethical, mystical reason to
[00:57:05.000 --> 00:57:08.040]   be concerned about it, because I kind of think we're going through it already.
[00:57:08.040 --> 00:57:15.280]   I mean, literally your set of atoms, are you your set of atoms or are you the pattern that
[00:57:15.280 --> 00:57:16.280]   your atoms are in?
[00:57:16.280 --> 00:57:19.160]   In some ways you're the pattern.
[00:57:19.160 --> 00:57:20.160]   Interesting.
[00:57:20.160 --> 00:57:25.400]   I'm not Christian, but that transporter I think makes me more nervous than it makes
[00:57:25.400 --> 00:57:26.800]   you.
[00:57:26.800 --> 00:57:34.520]   Well, but isn't it true though that you, I mean, if you thought about your current material
[00:57:34.520 --> 00:57:38.960]   composition right now, the literal pieces of it have changed pretty substantially and
[00:57:38.960 --> 00:57:40.960]   will continue to change, right?
[00:57:40.960 --> 00:57:42.360]   For sure.
[00:57:42.360 --> 00:57:43.360]   But there is, yeah.
[00:57:43.360 --> 00:57:48.240]   So look, I just gave you my most tech positive version of it, but sure, you'd ask me tomorrow
[00:57:48.240 --> 00:57:49.240]   if I would do it.
[00:57:49.240 --> 00:57:51.160]   I'd think, "Oh, a little scary.
[00:57:51.160 --> 00:57:52.160]   Let's find out."
[00:57:52.160 --> 00:57:57.720]   But I do, but don't you also believe that you're your pattern rather than your actual,
[00:57:57.720 --> 00:58:01.000]   who you are as the organization of these things inside you, right?
[00:58:01.000 --> 00:58:06.440]   Rather than the actual substance of it.
[00:58:06.440 --> 00:58:10.560]   That's true, but I feel like I am going to experience the world through somebody's eyes.
[00:58:10.560 --> 00:58:18.760]   And I think I am concerned that my future self might not be inhabiting the body of the
[00:58:18.760 --> 00:58:21.560]   person that comes out of that machine.
[00:58:21.560 --> 00:58:25.560]   But my wife strongly disagrees with my point of view on that.
[00:58:25.560 --> 00:58:28.280]   So I can see both sides of it.
[00:58:28.280 --> 00:58:32.640]   I'm pretty sure that I just wouldn't do it no matter how many people went through it
[00:58:32.640 --> 00:58:37.360]   and told me that it was safe.
[00:58:37.360 --> 00:58:38.360]   I mean, okay.
[00:58:38.360 --> 00:58:44.240]   Well, you say that now, but I will just remind you that our ability to adapt to circumstances
[00:58:44.240 --> 00:58:47.600]   and to change expectations is pretty dramatic, right?
[00:58:47.600 --> 00:58:56.840]   There are plenty of things you do now that would be super weird to you from 1999 or whatever.
[00:58:56.840 --> 00:58:59.520]   You're really young too, but you know what I mean?
[00:58:59.520 --> 00:59:03.360]   Expectations around what's normal or not normal shift consistently.
[00:59:03.360 --> 00:59:05.840]   Like staring at a phone all day long.
[00:59:05.840 --> 00:59:07.880]   Yeah, seriously, right?
[00:59:07.880 --> 00:59:08.880]   Yeah.
[00:59:08.880 --> 00:59:09.880]   All right.
[00:59:09.880 --> 00:59:12.880]   Well, final two questions.
[00:59:12.880 --> 00:59:19.120]   One question is, what's an aspect of machine learning that you think is underrated or underappreciated
[00:59:19.120 --> 00:59:21.080]   or underinvested in?
[00:59:21.080 --> 00:59:28.480]   I do think all of the HCI social system stuff really is underinvested in.
[00:59:28.480 --> 00:59:30.880]   And I think that there are lots and lots of opportunities.
[00:59:30.880 --> 00:59:40.360]   I think that it's interesting to me that the tools that annotators get right now are still
[00:59:40.360 --> 00:59:41.360]   so bad.
[00:59:41.360 --> 00:59:47.560]   I think it's interesting to me that the tools that data scientists use in some ways have
[00:59:47.560 --> 00:59:52.880]   not really changed since...remember your friend Kerr who wrote that paper in 2013?
[00:59:52.880 --> 00:59:55.080]   Look at his paper in 2013.
[00:59:55.080 --> 00:59:58.880]   The tools in some ways have not changed enough, right?
[00:59:58.880 --> 01:00:02.000]   And so I think there's lots and lots of opportunities there.
[01:00:02.000 --> 01:00:14.840]   And then I think there are lots of opportunities in making mainstream or to generalize from
[01:00:14.840 --> 01:00:17.400]   the lessons we learned from Human in the Loop.
[01:00:17.400 --> 01:00:20.680]   I think calling things Human in the Loop kind of was a mistake.
[01:00:20.680 --> 01:00:22.000]   There should be a better name for it.
[01:00:22.000 --> 01:00:25.880]   And if we had a better name for it, then everyone would think of all their jobs as Human in
[01:00:25.880 --> 01:00:26.880]   the Loop.
[01:00:26.880 --> 01:00:28.920]   Because I kind of believe that.
[01:00:28.920 --> 01:00:36.460]   I kind of believe that in the end, if we're successful, every process will be slightly
[01:00:36.460 --> 01:00:41.320]   better understood and we could be consistent and get consistently better because our job
[01:00:41.320 --> 01:00:46.880]   as humans were to either figure out edge cases or create broad clustering so that we can
[01:00:46.880 --> 01:00:47.880]   be consistent.
[01:00:47.880 --> 01:00:53.960]   So you care about the sort of interface of humans and machine learning, how they can
[01:00:53.960 --> 01:00:54.960]   work together?
[01:00:54.960 --> 01:00:58.880]   I mean, I think that I think in multiple levels, right?
[01:00:58.880 --> 01:01:06.440]   At the level of sort of the at the level of the person sort of making the initial decision
[01:01:06.440 --> 01:01:10.800]   at the person at the level of the person sort of like learning from that at the level of
[01:01:10.800 --> 01:01:14.160]   the people controlling that at the level of the people benefiting from that.
[01:01:14.160 --> 01:01:18.560]   I think all those things like the cutting edge, like we're still in a world where so
[01:01:18.560 --> 01:01:22.560]   much of that is siloed, like the way to think about it is siloed.
[01:01:22.560 --> 01:01:27.280]   And I think the ways to unlock lots of business value, but also be honest, like just straightforward
[01:01:27.280 --> 01:01:33.680]   good things for humanity is if people had at all levels of that game, sort of like a
[01:01:33.680 --> 01:01:37.880]   bigger view of what it is that they're engaged in, which is like sort of a great game of
[01:01:37.880 --> 01:01:38.880]   collective intelligence.
[01:01:38.880 --> 01:01:39.880]   All right.
[01:01:39.880 --> 01:01:43.400]   Well, practical question, which might actually have the same answer.
[01:01:43.400 --> 01:01:46.440]   It's never happened before as I asked these pairs of questions.
[01:01:46.440 --> 01:01:53.080]   But when you look at machine learning, trying to get adopted and deployed and useful inside
[01:01:53.080 --> 01:01:56.520]   of enterprises, where do you think the bottleneck is?
[01:01:56.520 --> 01:01:59.600]   Like where do these projects get stuck?
[01:01:59.600 --> 01:02:03.440]   I think they're so often badly conceived and over-promised.
[01:02:03.440 --> 01:02:06.200]   And we joked about this in the middle of this.
[01:02:06.200 --> 01:02:11.520]   I am still kind of convinced that if we offered your exec ad class to like every senior executive
[01:02:11.520 --> 01:02:16.120]   in the world, that we would basically all make much, much better decisions and we'd
[01:02:16.120 --> 01:02:19.600]   end up with like sort of much, much more successful implementations.
[01:02:19.600 --> 01:02:23.680]   So I think that that part's definitely true.
[01:02:23.680 --> 01:02:27.560]   And I also think that like the other thing that's holding us back is we still don't have
[01:02:27.560 --> 01:02:32.040]   great methodologies for thinking about how to build these systems, right?
[01:02:32.040 --> 01:02:37.320]   That we are still in software development world, I think it was, someone just gave me
[01:02:37.320 --> 01:02:38.320]   this history.
[01:02:38.320 --> 01:02:43.240]   You know, software, like random coding becomes engineering, like when NATO decides that it's
[01:02:43.240 --> 01:02:45.320]   an important thing in like 1968.
[01:02:45.320 --> 01:02:48.160]   And then we sort of codify all this waterfall stuff, right?
[01:02:48.160 --> 01:02:52.080]   And it goes from waterfall to extreme to agile over the course of the last like whatever,
[01:02:52.080 --> 01:02:53.080]   40 years.
[01:02:53.080 --> 01:02:58.400]   And what's interesting to me is that that methodology I think is mostly wrong for building
[01:02:58.400 --> 01:03:00.840]   machine learning models.
[01:03:00.840 --> 01:03:06.760]   And so we are still shoehorning these projects as if they're software development projects
[01:03:06.760 --> 01:03:10.120]   oftentimes and thus wasting a bunch of time and money.
[01:03:10.120 --> 01:03:11.120]   Awesome.
[01:03:11.120 --> 01:03:12.120]   Thanks, James.
[01:03:12.120 --> 01:03:13.120]   Okay.
[01:03:13.120 --> 01:03:15.160]   Take care.
[01:03:15.160 --> 01:03:18.680]   If you're enjoying this interview series, the most helpful thing that you can do for
[01:03:18.680 --> 01:03:20.480]   us is leave us a review.
[01:03:20.480 --> 01:03:24.800]   It helps other people find the show and really we do these shows so that people watch them
[01:03:24.800 --> 01:03:27.160]   and what I really want is more people to find it.
[01:03:27.160 --> 01:03:29.960]   So if you leave us a review, I really appreciate it.
[01:03:29.960 --> 01:03:33.520]   So James, here's what I really want to know.
[01:03:33.520 --> 01:03:37.000]   How does your religion inform your thoughts on machine learning?
[01:03:37.000 --> 01:03:38.000]   Okay.
[01:03:38.000 --> 01:03:42.840]   So this might be both borderline kooky and heretical.
[01:03:42.840 --> 01:03:45.600]   So we'll just caveat it first that way.
[01:03:45.600 --> 01:03:46.600]   Fantastic.
[01:03:46.600 --> 01:03:47.600]   Okay.
[01:03:47.600 --> 01:03:53.360]   So I think that there are a few different angles.
[01:03:53.360 --> 01:03:59.840]   I think the first is that sort of, at least in my theology, I think that sort of part
[01:03:59.840 --> 01:04:03.680]   of godliness is the act of creation.
[01:04:03.680 --> 01:04:10.640]   And I think that there's a way in which, as an investor, I put faith in the act of
[01:04:10.640 --> 01:04:14.020]   creation and helping people make something new.
[01:04:14.020 --> 01:04:17.960]   So that's one part.
[01:04:17.960 --> 01:04:23.480]   And sort of the creation of, however you want to talk about machine learning, I think there's
[01:04:23.480 --> 01:04:29.560]   this sense in which the models that we're building in some ways have sort of inherent
[01:04:29.560 --> 01:04:34.280]   worth and dignity as sort of basically sub-creations of people.
[01:04:34.280 --> 01:04:39.300]   That we are creating something new, and whether you want to call it life or whatever you want
[01:04:39.300 --> 01:04:44.520]   to call that thing, that it is something fundamentally new and different and interesting.
[01:04:44.520 --> 01:04:51.800]   And that piece of it then sort of informs the way I think about both its capabilities
[01:04:51.800 --> 01:04:56.200]   and why it's important, but at the same time, and so this is the part where I think
[01:04:56.200 --> 01:05:00.280]   other folks might have trouble with this, is that I do believe that we're fallen.
[01:05:00.280 --> 01:05:04.840]   I believe that we, I don't, I actually think that we want to be good, but we're actually
[01:05:04.840 --> 01:05:05.920]   bad.
[01:05:05.920 --> 01:05:10.240]   And I think that anything we create in some ways has tragic flaws in it, almost no matter
[01:05:10.240 --> 01:05:11.240]   what.
[01:05:11.240 --> 01:05:16.100]   And so in that way, I'm actually much more both forgiving about people, but also institutions,
[01:05:16.100 --> 01:05:17.800]   but also the models that we make, right?
[01:05:17.800 --> 01:05:22.920]   These things that we're making are both like, have great beauty and potential, but
[01:05:22.920 --> 01:05:25.360]   they're also tragically flawed because we are.
[01:05:25.360 --> 01:05:27.360]   I love it.
[01:05:27.360 --> 01:05:28.360]   Awesome.
[01:05:28.360 --> 01:05:33.360]   Oh man, that's definitely going on the podcast.
[01:05:33.360 --> 01:05:36.360]   That was great.
[01:05:36.360 --> 01:05:40.920]   I mean, it's kind of plausible, right?
[01:05:40.920 --> 01:05:41.920]   It's not crazy.
[01:05:41.920 --> 01:05:45.400]   I think I agree with all of it.
[01:05:45.400 --> 01:05:48.640]   I mean, yeah, totally.
[01:05:48.640 --> 01:05:52.520]   I think we oftentimes all think we're good.
[01:05:52.520 --> 01:05:55.320]   I mean, I think we think we're good, but we actually know.
[01:05:55.320 --> 01:05:57.280]   I mean, it's not that I'm good.
[01:05:57.280 --> 01:06:00.240]   It's I want to be good, and I'm just always doing stupid things.
[01:06:00.240 --> 01:06:02.640]   And of course the things I create are going to be imperfect.
[01:06:02.640 --> 01:06:06.440]   And that means that there, it also means there's this constant chance for improvement, which
[01:06:06.440 --> 01:06:10.960]   is the core of the understanding of gradients.

