
[00:00:00.000 --> 00:00:06.920]   So this model was predicting the classification of the tickets, right?
[00:00:06.920 --> 00:00:12.200]   And then we decided to build a model that was also suggesting which actions to take
[00:00:12.200 --> 00:00:14.840]   in response to this ticket.
[00:00:14.840 --> 00:00:20.360]   And then there was also another model that was deciding which template answer to send
[00:00:20.360 --> 00:00:24.040]   back to the user, depending on what they were telling us.
[00:00:24.040 --> 00:00:29.760]   And so instead of creating all these different models, I thought that that was a really nice
[00:00:29.760 --> 00:00:31.840]   application of multitask learning.
[00:00:31.840 --> 00:00:36.920]   And so I made it so that you can specify multiple outputs of multiple different data types.
[00:00:36.920 --> 00:00:40.880]   And in the end, we had basically one model that was capable of doing all these tasks
[00:00:40.880 --> 00:00:43.040]   using all these features.
[00:00:43.040 --> 00:00:45.800]   And that was basically the base of Ludwig.
[00:00:45.800 --> 00:00:49.920]   And then I started to add also images and all other things on top of that.
[00:00:49.920 --> 00:00:51.880]   And more people started to use it.
[00:00:51.880 --> 00:00:56.240]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:00:56.240 --> 00:00:58.520]   And I'm your host, Lukas Biewald.
[00:00:58.520 --> 00:01:03.320]   Piero is a staff research scientist in the Hazy Research Group at Stanford University.
[00:01:03.320 --> 00:01:07.760]   He's a former founding member of Uber AI, where he created Ludwig, worked on applied
[00:01:07.760 --> 00:01:10.720]   projects and published research on NLP.
[00:01:10.720 --> 00:01:12.560]   I'm super excited to talk to him today.
[00:01:12.560 --> 00:01:13.560]   All right.
[00:01:13.560 --> 00:01:17.240]   So Piero, I'd love to talk to you about your time at Uber and the things you worked on,
[00:01:17.240 --> 00:01:21.160]   but I think the thing you're maybe better known for and the main topic is probably your
[00:01:21.160 --> 00:01:22.800]   project Ludwig.
[00:01:22.800 --> 00:01:28.080]   So maybe for some of the people that might be listening or watching, could you just describe
[00:01:28.080 --> 00:01:29.640]   Ludwig at a high level?
[00:01:29.640 --> 00:01:30.640]   Sure.
[00:01:30.640 --> 00:01:36.360]   So it's actually a tool that I built when I was working at Uber, mostly for myself.
[00:01:36.360 --> 00:01:42.920]   I wanted to try to minimize the amount of work that it would take me to onboard a new
[00:01:42.920 --> 00:01:45.440]   machine learning project.
[00:01:45.440 --> 00:01:51.520]   And what it resulted in is a tool that allows you to train and then deploy deep learning
[00:01:51.520 --> 00:01:54.920]   models without having to write code.
[00:01:54.920 --> 00:02:00.640]   And it does so by allowing you to specify a declarative configuration of your model.
[00:02:00.640 --> 00:02:05.280]   And depending on the data types that you specify for the inputs and the outputs to your model,
[00:02:05.280 --> 00:02:10.520]   it assembles a different deep learning model that solves that specific task and then trains
[00:02:10.520 --> 00:02:14.600]   it for you and then you can deploy it.
[00:02:14.600 --> 00:02:16.400]   So can we make this more concrete?
[00:02:16.400 --> 00:02:22.280]   So what if my inputs were like bounding boxes, is that something that Ludwig would understand
[00:02:22.280 --> 00:02:24.400]   if it was images and bounding boxes?
[00:02:24.400 --> 00:02:29.400]   It would then sort of choose a model and learn, say, predicting classes or something like
[00:02:29.400 --> 00:02:30.400]   that?
[00:02:30.400 --> 00:02:31.400]   Would that work?
[00:02:31.400 --> 00:02:32.400]   So it doesn't.
[00:02:32.400 --> 00:02:34.840]   Right now, there's no specific bounding boxes.
[00:02:34.840 --> 00:02:38.000]   It's something like a feature that they're going to add in the near future.
[00:02:38.000 --> 00:02:40.760]   But what you do in general is exactly that.
[00:02:40.760 --> 00:02:46.560]   So you specify your inputs and your outputs and you specify what are their type.
[00:02:46.560 --> 00:02:50.760]   So for instance, if you want to do image classification, then you can say that your input is an image
[00:02:50.760 --> 00:02:52.560]   and your output is a class.
[00:02:52.560 --> 00:02:57.920]   Or if you want to do information extraction from text, then you can have text as input
[00:02:57.920 --> 00:03:02.200]   and for instance, a sequence as output where the sequence tells you what information you
[00:03:02.200 --> 00:03:04.560]   want to extract from the text.
[00:03:04.560 --> 00:03:12.320]   And any combination of these inputs and outputs allow you to create a different model basically.
[00:03:12.320 --> 00:03:16.360]   And is the idea that underneath the hood, it picks the best state of the art algorithm
[00:03:16.360 --> 00:03:19.000]   for any particular kind of input and output?
[00:03:19.000 --> 00:03:20.240]   Is that right?
[00:03:20.240 --> 00:03:22.960]   So it works at three different levels really.
[00:03:22.960 --> 00:03:25.080]   The basic level, you don't specify anything.
[00:03:25.080 --> 00:03:27.880]   You just specify your inputs and outputs and the types.
[00:03:27.880 --> 00:03:33.820]   And it uses some defaults that in most cases are pretty reasonable defaults.
[00:03:33.820 --> 00:03:39.880]   Things that are for those kinds of types of inputs and outputs, state of the art in the
[00:03:39.880 --> 00:03:41.600]   literature.
[00:03:41.600 --> 00:03:47.720]   But you have full control over all the details of the models that are being used.
[00:03:47.720 --> 00:03:52.200]   So for instance, if you're providing text, then you can specify that you want to encode
[00:03:52.200 --> 00:03:59.520]   it using an RNN or you want to encode it using a transformer or a CNN or a pre-trained model
[00:03:59.520 --> 00:04:01.360]   like BERT.
[00:04:01.360 --> 00:04:04.320]   You can choose among these options.
[00:04:04.320 --> 00:04:07.240]   And you can also change all the different parameters of these options.
[00:04:07.240 --> 00:04:12.360]   For instance, for the RNN, you can say how many layers of RNN or if you want to use an
[00:04:12.360 --> 00:04:18.760]   LSTM cell or a GRU cell or the sides of the hidden state, all the parameters that you
[00:04:18.760 --> 00:04:23.320]   may want to change for those models, you can change them.
[00:04:23.320 --> 00:04:27.800]   And additionally, one thing that we recently introduced in version 0.3 is the capability
[00:04:27.800 --> 00:04:32.680]   to do hyperparameter optimization so that you can say, I want to use an RNN, but I don't
[00:04:32.680 --> 00:04:34.840]   know how many layers do I want to use.
[00:04:34.840 --> 00:04:40.240]   And then you can say, I have this range between one and 10 and figure out which is the best
[00:04:40.240 --> 00:04:42.160]   parameter configuration for this problem.
[00:04:42.160 --> 00:04:43.880]   And what does it do underneath the hood?
[00:04:43.880 --> 00:04:47.320]   Does it have some kind of smart system for finding the best set of hyperparameters?
[00:04:47.320 --> 00:04:48.320]   Yeah.
[00:04:48.320 --> 00:04:52.840]   So first of all, the models that it trains are TensorFlow 2 models right now.
[00:04:52.840 --> 00:04:56.320]   We're also thinking about adding additional backends, but that's what...
[00:04:56.320 --> 00:05:00.440]   So the output in the end will be a TensorFlow 2 model that you can use for whatever purpose
[00:05:00.440 --> 00:05:01.920]   you want.
[00:05:01.920 --> 00:05:07.400]   And for the parameter optimization, there's also for the parameter optimization process
[00:05:07.400 --> 00:05:11.160]   itself, there's declarative configuration you can give.
[00:05:11.160 --> 00:05:16.080]   And you can specify if you want to optimize it using different algorithms.
[00:05:16.080 --> 00:05:21.040]   At the moment, there's only three supported, which is grid search, random search, and Bayesian
[00:05:21.040 --> 00:05:24.640]   optimization algorithm called PyShot.
[00:05:24.640 --> 00:05:26.280]   In the near future, we're going to add more.
[00:05:26.280 --> 00:05:31.400]   In particular, we want to integrate with Raytunet as many, many of those algorithms already
[00:05:31.400 --> 00:05:33.220]   ready to be used.
[00:05:33.220 --> 00:05:37.680]   And also you can specify where do you want to execute the hyperparameter optimization.
[00:05:37.680 --> 00:05:41.720]   If you have a laptop, maybe you want to execute it just on your machine.
[00:05:41.720 --> 00:05:47.560]   Or if you have a machine with a GPU, you may want to exploit the GPU.
[00:05:47.560 --> 00:05:52.640]   Or if you have multi-processing and multiple GPUs, you can run the training in parallel.
[00:05:52.640 --> 00:05:57.440]   And also if you have access to a cluster, then you can run on the cluster, a Kubernetes
[00:05:57.440 --> 00:06:00.520]   cluster with multiple machines with multiple GPUs.
[00:06:00.520 --> 00:06:05.000]   Does Leadwick include data preparation or data augmentation techniques?
[00:06:05.000 --> 00:06:06.440]   Is that something you can do with it also?
[00:06:06.440 --> 00:06:09.800]   Because I know that's super important to many fields these days.
[00:06:09.800 --> 00:06:10.800]   Yeah.
[00:06:10.800 --> 00:06:15.560]   So for data pre-processing, there are a bunch of things that Leadwick provides and a bunch
[00:06:15.560 --> 00:06:17.520]   of things that it doesn't provide.
[00:06:17.520 --> 00:06:23.760]   In particular, because that's not 100% the main focus, at least so far, it has not been
[00:06:23.760 --> 00:06:25.660]   100% the focus of the library.
[00:06:25.660 --> 00:06:29.400]   So we provide some relatively basic functionalities.
[00:06:29.400 --> 00:06:35.400]   And if you have some specific need for pre-processing, we would suggest to do some pre-processing
[00:06:35.400 --> 00:06:38.040]   beforehand before providing the data to Leadwick.
[00:06:38.040 --> 00:06:44.480]   But things that Leadwick does automatically are, for instance, a normalization of features,
[00:06:44.480 --> 00:06:49.520]   some tokenization of different sequences or text features.
[00:06:49.520 --> 00:06:55.480]   For images, we do resizing, cropping, pretty standard things, nothing crazy, but something
[00:06:55.480 --> 00:07:00.360]   that is useful for having a kind of an end-to-end kind of experience.
[00:07:00.360 --> 00:07:05.880]   In terms of augmentation, currently we don't have any augmentation that you can do right
[00:07:05.880 --> 00:07:11.680]   off the box, but it's one of the things that we want to add in version 0.4 of the package.
[00:07:11.680 --> 00:07:15.380]   I think one of the things that's striking about your library is, I think some libraries
[00:07:15.380 --> 00:07:20.360]   try to help people that do write code, do machine learning without a deep knowledge
[00:07:20.360 --> 00:07:21.360]   of machine learning.
[00:07:21.360 --> 00:07:25.560]   But I think your library, if I recall correctly, says right in the top, "We're trying to make
[00:07:25.560 --> 00:07:29.060]   it possible to do machine learning without actually writing any code at all."
[00:07:29.060 --> 00:07:32.260]   So that seems like a grander ambition.
[00:07:32.260 --> 00:07:33.760]   Can you talk a little bit about what...
[00:07:33.760 --> 00:07:38.400]   Maybe you come to that and maybe what design decisions you make differently to try to enable
[00:07:38.400 --> 00:07:39.400]   that?
[00:07:39.400 --> 00:07:40.400]   Sure.
[00:07:40.400 --> 00:07:42.560]   So I think to a certain extent, it's a little bit aspirational too, right?
[00:07:42.560 --> 00:07:45.080]   Because there is still something that you have to provide.
[00:07:45.080 --> 00:07:48.840]   In this case, there's a claritative definition of your model.
[00:07:48.840 --> 00:07:55.840]   But I believe that it's so much simpler to write this configuration file than it is to
[00:07:55.840 --> 00:08:00.880]   write code, than to some intensive purposes, it actually opens up the possibility for more
[00:08:00.880 --> 00:08:02.780]   people to try out to use these models.
[00:08:02.780 --> 00:08:05.960]   So that was to a certain extent the intent.
[00:08:05.960 --> 00:08:13.640]   In terms of the design decisions, I think the main one that allows for this level of
[00:08:13.640 --> 00:08:19.960]   abstraction is probably the choice that I made to be, as you were saying before, opinionated
[00:08:19.960 --> 00:08:27.080]   about the structure of the models and the fact that there are some data types that I
[00:08:27.080 --> 00:08:29.520]   support and some data types that I don't support.
[00:08:29.520 --> 00:08:35.840]   If your problem is within the realm of those data types that I support, then I make it
[00:08:35.840 --> 00:08:36.960]   really easy for you.
[00:08:36.960 --> 00:08:41.560]   If it's outside, then well, either you can go and implement it yourself or you can extend
[00:08:41.560 --> 00:08:46.580]   Ludwig to actually incorporate also additional data types that you care about.
[00:08:46.580 --> 00:08:50.300]   And those data types, the fact that you can compose a data type, so the compositionality
[00:08:50.300 --> 00:08:56.880]   aspect of it is what makes it general to cover many different use cases.
[00:08:56.880 --> 00:09:00.920]   And that's probably the main secret sauce.
[00:09:00.920 --> 00:09:05.880]   This is not so secret because it's an open source project, but it's probably part where
[00:09:05.880 --> 00:09:06.880]   the magic is.
[00:09:06.880 --> 00:09:09.260]   Let's put it this way.
[00:09:09.260 --> 00:09:11.860]   Can you describe how you would compose a data set?
[00:09:11.860 --> 00:09:14.300]   Can you give me a concrete example of that?
[00:09:14.300 --> 00:09:15.300]   A data type, sorry.
[00:09:15.300 --> 00:09:16.300]   Yeah.
[00:09:16.300 --> 00:09:23.020]   So again, one example, we've been for some examples like text input, category output,
[00:09:23.020 --> 00:09:24.340]   text classifier.
[00:09:24.340 --> 00:09:30.300]   But the interesting thing is that in some libraries, what you have is they provide you
[00:09:30.300 --> 00:09:31.300]   with some templates.
[00:09:31.300 --> 00:09:36.780]   Like for instance, the Turing core create, I believe, that allows you to create models
[00:09:36.780 --> 00:09:42.620]   for Apple devices, does something similar where you have a task, which is text classification.
[00:09:42.620 --> 00:09:46.120]   And then you have to provide the text input and the class output.
[00:09:46.120 --> 00:09:49.500]   And then there's another task that is, again, gives you some templates that you have to
[00:09:49.500 --> 00:09:50.500]   fit into.
[00:09:50.500 --> 00:09:52.700]   In LUT, it works the other way around.
[00:09:52.700 --> 00:09:55.420]   You start from the data and you look at the data that you have.
[00:09:55.420 --> 00:10:00.220]   And for instance, you have, if you want to classify an article, maybe you don't have
[00:10:00.220 --> 00:10:01.220]   only the text.
[00:10:01.220 --> 00:10:03.540]   You also have information about who's the author.
[00:10:03.540 --> 00:10:06.580]   And you also have information about the date when it was published.
[00:10:06.580 --> 00:10:11.300]   And maybe there is a subtitle and there's a separation between the title, the subtitle,
[00:10:11.300 --> 00:10:12.440]   and the body.
[00:10:12.440 --> 00:10:16.860]   And so what you could do with LUT BigEasily, you can say, well, the title is a text input
[00:10:16.860 --> 00:10:22.020]   feature, but also the subtitle is a separate in-text input feature and the body is a separate
[00:10:22.020 --> 00:10:23.380]   input feature.
[00:10:23.380 --> 00:10:27.580]   And the author is a category because maybe I'm working for a website and the website
[00:10:27.580 --> 00:10:29.600]   has 20 different authors.
[00:10:29.600 --> 00:10:34.500]   And information about the author will allow me to figure out, because many authors may
[00:10:34.500 --> 00:10:36.500]   be published in a specific topic.
[00:10:36.500 --> 00:10:40.420]   And so that's additional signal that you will have when you're trying to figure out what
[00:10:40.420 --> 00:10:43.740]   class this new article belongs to.
[00:10:43.740 --> 00:10:47.500]   And also time, because maybe a certain moment in time, there was a spike of interest in
[00:10:47.500 --> 00:10:48.500]   a specific topic.
[00:10:48.500 --> 00:10:52.100]   And so knowing that an article was published in a specific date, that helps you figuring
[00:10:52.100 --> 00:10:54.680]   out what type of article this is.
[00:10:54.680 --> 00:10:58.900]   And so with LUT Big, it's super easy to specify all these different inputs from different
[00:10:58.900 --> 00:10:59.900]   data types.
[00:10:59.900 --> 00:11:01.100]   It's just a list.
[00:11:01.100 --> 00:11:06.460]   You just say the name of your feature and the type, and it's a list of those things.
[00:11:06.460 --> 00:11:11.300]   And that's all you have to do to have a model that combines all these different inputs into
[00:11:11.300 --> 00:11:12.300]   the same architecture.
[00:11:12.300 --> 00:11:18.080]   What do you do if the types of your data are inconsistent?
[00:11:18.080 --> 00:11:19.080]   Can Ludwig handle that?
[00:11:19.080 --> 00:11:21.900]   What do you mean by inconsistent here?
[00:11:21.900 --> 00:11:26.300]   I guess what if my input data had ... I mean, missing values might be the simplest case,
[00:11:26.300 --> 00:11:27.300]   right?
[00:11:27.300 --> 00:11:31.460]   But I'm thinking of the cases that people come to me with and they want to do some classifications
[00:11:31.460 --> 00:11:33.180]   of crazy data set.
[00:11:33.180 --> 00:11:35.180]   Maybe there's sometimes multiple authors.
[00:11:35.180 --> 00:11:38.300]   Maybe there's ... I'm just thinking of all these edge cases.
[00:11:38.300 --> 00:11:39.300]   How do you deal with that?
[00:11:39.300 --> 00:11:40.300]   I see.
[00:11:40.300 --> 00:11:41.300]   I see.
[00:11:41.300 --> 00:11:46.460]   So, well, let's say for cleaning the missing values things, LUT Big does some of it for
[00:11:46.460 --> 00:11:47.460]   you.
[00:11:47.460 --> 00:11:53.140]   You can specify default filling value, or you can specify to default to fill with some
[00:11:53.140 --> 00:11:58.780]   statistics, like with the mean, with the max, these kind of things, which are pretty straightforward.
[00:11:58.780 --> 00:12:02.100]   LUT Big allows you to do all these things, so that's good.
[00:12:02.100 --> 00:12:07.820]   But if the inconsistencies are bigger, like for instance, in some cases there's multiple
[00:12:07.820 --> 00:12:11.700]   authors, well, you either treat it as a different data type altogether.
[00:12:11.700 --> 00:12:14.020]   For instance, set is a data type in LUT.
[00:12:14.020 --> 00:12:17.340]   So, if you have multiple authors, you can treat it as a set rather than treating it
[00:12:17.340 --> 00:12:20.020]   as a class, for instance, as a category.
[00:12:20.020 --> 00:12:23.940]   And so, because I have multiple of those data types, like for instance, date is a data type,
[00:12:23.940 --> 00:12:27.940]   the geolocation is a data type, and so on and so on, I think you will have relatively
[00:12:27.940 --> 00:12:32.100]   easy time to find a data type that fits the type of data that you have.
[00:12:32.100 --> 00:12:36.460]   And again, if not, LUT Big is really easy to extend to add a data type that kind of
[00:12:36.460 --> 00:12:40.380]   matches your specific use case if you want to.
[00:12:40.380 --> 00:12:45.180]   So do you have examples of people that used LUT Big that really couldn't write any code?
[00:12:45.180 --> 00:12:47.620]   Do you know people that have tried that?
[00:12:47.620 --> 00:12:48.620]   Yeah.
[00:12:48.620 --> 00:12:54.420]   So there is this really interesting example that I've witnessed, I would say, of there
[00:12:54.420 --> 00:13:01.300]   are a couple articles online from a person who's an expert in CEO, search engine optimization,
[00:13:01.300 --> 00:13:07.100]   and they wrote a couple articles on a CEO blog about using LUT Big for doing some predictions
[00:13:07.100 --> 00:13:10.660]   that are specifically useful for CEO purposes.
[00:13:10.660 --> 00:13:15.860]   And I believe most of these people don't have a programming background, they cannot code,
[00:13:15.860 --> 00:13:21.760]   and so it was really nice to see people using it for that purpose.
[00:13:21.760 --> 00:13:25.500]   And another fun example that they have, so I don't know how much coding did this guy
[00:13:25.500 --> 00:13:26.980]   do, but okay.
[00:13:26.980 --> 00:13:32.940]   So there was this application of LUT Big for, there's a published article by the Max Planck
[00:13:32.940 --> 00:13:40.140]   Institute on analysis of some biological images about, I think it was about worms or cells
[00:13:40.140 --> 00:13:44.820]   or worms, I don't remember exactly, but the point was that the person that was using it
[00:13:44.820 --> 00:13:48.500]   was a biologist, was not a computer scientist.
[00:13:48.500 --> 00:13:54.120]   And what he told me is that he would not have been able to implement, he was using ResNets
[00:13:54.120 --> 00:13:58.600]   within LUT Big, and would not have been able to implement a ResNet by himself.
[00:13:58.600 --> 00:14:04.420]   And so LUT Big enabled him to actually do this kind of research that otherwise would
[00:14:04.420 --> 00:14:06.440]   not have been easy for him to do.
[00:14:06.440 --> 00:14:11.920]   So these are some examples of what you were talking about that I'm pretty proud of.
[00:14:11.920 --> 00:14:13.880]   Yeah, you should be proud of that.
[00:14:13.880 --> 00:14:14.880]   That's really impressive.
[00:14:15.820 --> 00:14:21.800]   I guess LUT Big came out of your use cases, and obviously you're a very skilled coder.
[00:14:21.800 --> 00:14:26.240]   What were you working on at the time at Uber that inspired you to make LUT Big?
[00:14:26.240 --> 00:14:31.200]   Yeah, so the whole point is that I'm lazy and I don't want to do the same thing twice.
[00:14:31.200 --> 00:14:32.880]   Well, I mean twice is fine.
[00:14:32.880 --> 00:14:39.360]   Three times I basically try to automate it for myself, for my own sake.
[00:14:39.360 --> 00:14:41.840]   And so I was working on this project called Kota.
[00:14:41.840 --> 00:14:44.680]   There's a couple articles online if you're interested about it.
[00:14:44.680 --> 00:14:49.640]   It's like a customer support model that basically at the beginning was we were treating the
[00:14:49.640 --> 00:14:51.480]   problem as a tax classification problem.
[00:14:51.480 --> 00:14:56.200]   So we had the input tickets and we wanted to predict what type of ticket this was, because
[00:14:56.200 --> 00:15:01.080]   depending on the type, they were routed to different customer support representatives.
[00:15:01.080 --> 00:15:05.600]   And maybe just before you get too far into it, could you describe what's the scenario,
[00:15:05.600 --> 00:15:07.800]   what's an example ticket, and what would be an example class?
[00:15:07.800 --> 00:15:11.640]   Yeah, so I mean I was working at Uber, so one example was like, I don't know, my ride
[00:15:11.640 --> 00:15:15.640]   was canceled, I want my money back, or something like that.
[00:15:15.640 --> 00:15:21.600]   And the class, there were like about, I think, 2000 classes, different classes that the ticket
[00:15:21.600 --> 00:15:29.040]   could belong to, which could be appeasement request or lost item or food not delivered,
[00:15:29.040 --> 00:15:31.840]   because it was also Uber Eats side of things, right?
[00:15:31.840 --> 00:15:39.080]   So there was a really wide range of possible types of issues that could happen.
[00:15:39.080 --> 00:15:42.640]   And again, at the beginning, we were treating it as a tax classification problem.
[00:15:42.640 --> 00:15:47.040]   But then the PM working on this problem came to me and said, "You know, there is availability
[00:15:47.040 --> 00:15:48.680]   for additional features here.
[00:15:48.680 --> 00:15:52.920]   Like for instance, we can extract some features from the user that is sending this message."
[00:15:52.920 --> 00:15:58.160]   For instance, if they were using the driver app or the rider app or the Uber Eats app
[00:15:58.160 --> 00:16:00.420]   when they were sending this message.
[00:16:00.420 --> 00:16:04.720]   And so that was, again, additional signal that we wanted to integrate into the model.
[00:16:04.720 --> 00:16:09.480]   And well, I did it once and that was fine, but then they came back to me with additional
[00:16:09.480 --> 00:16:12.320]   features that were related, for instance, to the ride that they were taking.
[00:16:12.320 --> 00:16:16.520]   And I said, "Okay, so these features are, some of them are numbers, some of them are
[00:16:16.520 --> 00:16:18.680]   binary values, some of them are categories.
[00:16:18.680 --> 00:16:22.920]   Let's make it something generic so that if they come to me again with more features to
[00:16:22.920 --> 00:16:26.640]   add, it would be really easy for me to do that."
[00:16:26.640 --> 00:16:29.680]   And so that's the part that I covered for the inputs.
[00:16:29.680 --> 00:16:32.280]   And then the same happened to the outputs because we had...
[00:16:32.280 --> 00:16:36.040]   So this model was predicting the classification of the tickets, right?
[00:16:36.040 --> 00:16:41.320]   And then we decided to build a model that was also suggesting which actions to take
[00:16:41.320 --> 00:16:43.940]   in response to this ticket.
[00:16:43.940 --> 00:16:49.480]   And then there was also another model that was deciding which template answer to send
[00:16:49.480 --> 00:16:53.180]   back to the user, depending on what they were telling us.
[00:16:53.180 --> 00:16:59.480]   And so instead of creating all these different models, I thought that was a really nice application
[00:16:59.480 --> 00:17:04.080]   of multitask learning, and so I made it so that you can specify multiple outputs of multiple
[00:17:04.080 --> 00:17:06.080]   different data types.
[00:17:06.080 --> 00:17:09.920]   And in the end, we had basically one model that was capable of doing all these tasks
[00:17:09.920 --> 00:17:12.280]   using all these features.
[00:17:12.280 --> 00:17:14.920]   And that was basically the base of Ludwig.
[00:17:14.920 --> 00:17:19.080]   And then I started to add also images and all other things on top of that.
[00:17:19.080 --> 00:17:23.040]   And more people started to use it within the organization.
[00:17:23.040 --> 00:17:26.440]   And then later on, we decided finally to release it as open source because we thought that
[00:17:26.440 --> 00:17:32.000]   also other people outside Uber could find some value in using it.
[00:17:32.000 --> 00:17:33.680]   That's so cool.
[00:17:33.680 --> 00:17:39.520]   Do you anticipate more people kind of moving to this model of not worrying about the underlying
[00:17:39.520 --> 00:17:42.000]   architecture of what's happening?
[00:17:42.000 --> 00:17:44.960]   And I guess what should people then focus on if they're using Ludwig?
[00:17:44.960 --> 00:17:49.840]   If you want to make your model better, what is there left to do?
[00:17:49.840 --> 00:17:51.520]   So I think there's two aspects there.
[00:17:51.600 --> 00:17:57.600]   I would say I believe, I may be wrong, but I believe that there's much more people in
[00:17:57.600 --> 00:18:02.040]   the world that doesn't know how to implement a deep learning model than people that knows
[00:18:02.040 --> 00:18:04.520]   how to implement a deep learning model.
[00:18:04.520 --> 00:18:11.240]   And so I would say for, I believe that there's also value that Ludwig can give to an expert,
[00:18:11.240 --> 00:18:14.980]   in particular because it makes it easy to compare different models, makes it very easy
[00:18:14.980 --> 00:18:17.480]   for you to have a baseline, for instance.
[00:18:17.480 --> 00:18:21.560]   That's definitely something that is useful in many situations.
[00:18:21.560 --> 00:18:25.200]   But if you are a super expert and you want to implement, if you're a researcher and you're
[00:18:25.200 --> 00:18:29.720]   creating a new model, then probably you want to implement it from scratch and have full
[00:18:29.720 --> 00:18:31.400]   control over it.
[00:18:31.400 --> 00:18:36.420]   But I think there's the rest of us, the rest of the people that don't know how to implement
[00:18:36.420 --> 00:18:41.320]   a deep learning model and doesn't have the time and the resources to study it, for those
[00:18:41.320 --> 00:18:46.720]   people I think there's a lot of value to be unlocked by using a tool like Ludwig.
[00:18:46.720 --> 00:18:50.720]   And in terms of then what do you do if you're not writing your model?
[00:18:50.720 --> 00:18:53.520]   Well, there's all sorts of other things, right?
[00:18:53.520 --> 00:18:59.040]   First of all, you can figure out the upper parameters, both by hand and also automatically.
[00:18:59.040 --> 00:19:06.780]   And also there's also other things, like you can try to, for instance, figure out on which
[00:19:06.780 --> 00:19:09.480]   subsets of data the model performs better or worse.
[00:19:09.480 --> 00:19:15.360]   And so have some sort of outer loop kind of explainability and then trying to make sure
[00:19:15.360 --> 00:19:20.160]   that your model is safe and that it's non-discriminating, all these sorts of things.
[00:19:20.160 --> 00:19:24.160]   It's usually the way you actually approach these kinds of problems, you need to add more
[00:19:24.160 --> 00:19:30.440]   data in a specific way that tries to introduce and solve these problems in the behavior of
[00:19:30.440 --> 00:19:31.640]   the model, right?
[00:19:31.640 --> 00:19:38.960]   So I would say in general, this is like a piece of a human centered kind of process.
[00:19:38.960 --> 00:19:46.520]   And so the human has a lot of things to do in this process by labeling the data, adjusting
[00:19:46.520 --> 00:19:50.680]   the model, integrating the model into a broader application.
[00:19:50.680 --> 00:19:54.960]   So there's a lot still to do for the human, I believe.
[00:19:54.960 --> 00:20:01.120]   Is it part of Ludwig's scope to guide the human building the model into things that
[00:20:01.120 --> 00:20:04.040]   are likely to help the model perform better?
[00:20:04.040 --> 00:20:05.440]   Like I'll give you an example.
[00:20:05.440 --> 00:20:09.560]   I often help people who don't have a lot of experience train models.
[00:20:09.560 --> 00:20:13.800]   And some of the mistakes they make are kind of surprising to people that are in the field,
[00:20:13.800 --> 00:20:15.960]   but make total sense if you step back.
[00:20:15.960 --> 00:20:21.160]   I've noticed in some cases, people will have so many classes that they don't have an example,
[00:20:21.160 --> 00:20:23.200]   literally even one example of every class.
[00:20:23.200 --> 00:20:26.920]   And then they're surprised when the model can't predict that class where they've literally
[00:20:26.920 --> 00:20:28.960]   not provided an example of that.
[00:20:28.960 --> 00:20:32.920]   And I can think of lots of different ways that people can shoot themselves in the foot
[00:20:32.920 --> 00:20:35.480]   when they don't have experience with this type of thing.
[00:20:35.480 --> 00:20:39.800]   Is it part of Ludwig's scope to help people avoid those bad situations?
[00:20:39.800 --> 00:20:41.880]   So that's a really interesting question.
[00:20:41.880 --> 00:20:46.200]   I would say the scope is changing over time, to be honest.
[00:20:46.200 --> 00:20:49.800]   At the beginning, the scope, as I described at the beginning, the scope was to build a
[00:20:49.800 --> 00:20:53.980]   text classifier, and then it became a much more genetic thing over time.
[00:20:53.980 --> 00:21:00.200]   So also with regards to what you're asking, it's something that we don't...
[00:21:00.200 --> 00:21:07.200]   So let's put it this way, Ludwig nudges you in a direction, but it does so in particular
[00:21:07.200 --> 00:21:12.240]   for model architecture choices and model training and building.
[00:21:12.240 --> 00:21:17.720]   It has some defaults that are kind of reasonable and helps you figure out easily with the parameters
[00:21:17.720 --> 00:21:19.000]   what to do.
[00:21:19.000 --> 00:21:24.960]   What it does not do right now is what you described, like the more higher level kind
[00:21:24.960 --> 00:21:28.000]   of problems.
[00:21:28.000 --> 00:21:32.800]   If is the problem you're trying to solve a problem that is solvable with a machine learning
[00:21:32.800 --> 00:21:37.600]   algorithm to begin with, for instance, that's something that is right now out of the scope
[00:21:37.600 --> 00:21:38.600]   of Ludwig.
[00:21:38.600 --> 00:21:46.400]   You basically start with something that you believe could be useful, signal that kind
[00:21:46.400 --> 00:21:52.120]   of makes sense, and distribution of classes, for instance, that kind of makes sense.
[00:21:52.120 --> 00:21:55.280]   This is slightly switching gears, but this has been a surprisingly interesting question
[00:21:55.280 --> 00:21:56.640]   recently.
[00:21:56.640 --> 00:22:00.120]   What do you think about Python as sort of the lingua franca of-
[00:22:00.120 --> 00:22:05.680]   But what you're saying is very interesting because there could be some even relatively
[00:22:05.680 --> 00:22:11.600]   easy checks that one could do beforehand and return to the user saying, "Oh, there are
[00:22:11.600 --> 00:22:14.300]   class A, B, and C that don't have examples.
[00:22:14.300 --> 00:22:17.400]   Maybe you want to provide them if you want to have good performance," or something like
[00:22:17.400 --> 00:22:18.400]   that.
[00:22:18.400 --> 00:22:19.400]   That could be easily added.
[00:22:19.400 --> 00:22:21.880]   So that's something that I would take into consideration.
[00:22:21.880 --> 00:22:26.720]   Machine learning, do you think that Python is going to stay the dominant language for
[00:22:26.720 --> 00:22:30.280]   people building models, or maybe there'll be something even more high level if your
[00:22:30.280 --> 00:22:35.200]   vision is that people don't even need to write code to build these models?
[00:22:35.200 --> 00:22:36.200]   Yeah.
[00:22:36.200 --> 00:22:39.480]   I mean, so there's several aspects to this question.
[00:22:39.480 --> 00:22:43.960]   I think also it depends on who is the user.
[00:22:43.960 --> 00:22:51.520]   I believe that, for instance, if you think about databases before SQL was invented, well,
[00:22:51.520 --> 00:22:53.240]   people had to code their own databases by hand.
[00:22:53.240 --> 00:22:58.880]   Well, not really SQL, but I mean the relational database in general, introduction of those
[00:22:58.880 --> 00:23:01.400]   kinds of management systems.
[00:23:01.400 --> 00:23:08.080]   People had to implement their databases by hand, and they were using files and hierarchies
[00:23:08.080 --> 00:23:09.240]   as a way.
[00:23:09.240 --> 00:23:14.040]   The file system was basically an early example of a database, really.
[00:23:14.040 --> 00:23:21.160]   And then there was this change into the paradigm of the way that people interacted with data
[00:23:21.160 --> 00:23:29.220]   by using a language like SQL that is more declarative, doesn't require you to express
[00:23:29.220 --> 00:23:32.720]   how things should be computed, but actually what you want to compute.
[00:23:32.720 --> 00:23:36.880]   And I think that a similar shift could happen also for machine learning.
[00:23:36.880 --> 00:23:41.860]   Although this is true for a set of users, which are the final users, those ones that
[00:23:41.860 --> 00:23:47.480]   use the models, much less so for the people that actually produce the models.
[00:23:47.480 --> 00:23:50.960]   For the people that produce the model, I actually love Python.
[00:23:50.960 --> 00:23:57.640]   I think it's a great language, has really nice syntax, is very simple to pick up, very
[00:23:57.640 --> 00:24:01.600]   simple to look at someone else's code and improve it and change it.
[00:24:01.600 --> 00:24:03.640]   So I think it's a great language.
[00:24:03.640 --> 00:24:10.480]   But I can also imagine that we could be moving towards languages that are probably a little
[00:24:10.480 --> 00:24:12.920]   bit more efficient.
[00:24:12.920 --> 00:24:18.440]   And the efficiency of using Python right now is basically wrapping C stuff.
[00:24:18.440 --> 00:24:25.000]   Maybe there is a world where we start to write models in Rust, even if Rust is a little bit
[00:24:25.000 --> 00:24:26.720]   too complicated probably.
[00:24:26.720 --> 00:24:31.800]   But I believe that, or maybe in Julia, I don't know, there could be some candidates language
[00:24:31.800 --> 00:24:36.160]   to the throne Python as the lingua franca for machine learning.
[00:24:36.160 --> 00:24:42.640]   Although I don't see that happening in the very near future, to be honest.
[00:24:42.640 --> 00:24:51.400]   How do you decide what default model you give someone for a certain configuration, especially
[00:24:51.400 --> 00:24:54.360]   when the research is changing so fast?
[00:24:54.360 --> 00:24:57.680]   And I would say, especially maybe in natural language processing right now, which just
[00:24:57.680 --> 00:25:00.480]   sounds like is where Ludwig started.
[00:25:00.480 --> 00:25:04.080]   Does it ever get contentious to decide what default to put in?
[00:25:04.080 --> 00:25:07.720]   Because I would think that a lot of no-code users, if they have no experience in machine
[00:25:07.720 --> 00:25:10.000]   learning, they're probably going to stick to the default.
[00:25:10.000 --> 00:25:13.760]   Or at least even if they do a hyperparameter search, you have to constrain it somehow to
[00:25:13.760 --> 00:25:15.240]   some set of defaults.
[00:25:15.240 --> 00:25:17.440]   How do you think about that?
[00:25:17.440 --> 00:25:18.440]   This is a great point.
[00:25:18.440 --> 00:25:24.760]   And also, there are many aspects in my opinion that they're not...
[00:25:24.760 --> 00:25:29.320]   I mean, there are some researchers that are actually talking about these aspects, but
[00:25:29.320 --> 00:25:34.960]   they're not mainstream in particular in research.
[00:25:34.960 --> 00:25:40.920]   And those aspects are like performance is one dimension that a potential user of a system
[00:25:40.920 --> 00:25:44.560]   like this may care about, but there are also other dimensions.
[00:25:44.560 --> 00:25:52.480]   It could be speed of inference, or cost of training, or length of training, or carbon
[00:25:52.480 --> 00:25:55.880]   footprint of your models and so on.
[00:25:55.880 --> 00:26:02.400]   So it's really difficult to figure out a default that accommodates all these aspects.
[00:26:02.400 --> 00:26:05.280]   Actually right now, it's impossible.
[00:26:05.280 --> 00:26:12.840]   What I usually tend to do is to provide defaults that are on the, let's say, less computational
[00:26:12.840 --> 00:26:13.840]   expensive side.
[00:26:13.840 --> 00:26:23.560]   So for instance, I will not have as a default to use T5 as a model for encoding language,
[00:26:23.560 --> 00:26:30.120]   just because the amount of users that could actually fine tune a T5 model will be relatively
[00:26:30.120 --> 00:26:36.040]   small and also the degree of advantage that they would get over a smaller model that may
[00:26:36.040 --> 00:26:42.840]   be not as big as to justify the increased cost in computational cost.
[00:26:42.840 --> 00:26:50.680]   So I try to balance towards the inexpensive, but leaving the option for the more expensive.
[00:26:50.680 --> 00:26:52.640]   So that's one thing I do.
[00:26:52.640 --> 00:26:56.200]   And on the other hand, this is something that I'm really interested in doing.
[00:26:56.200 --> 00:26:59.080]   I'm starting to do some little research around it.
[00:26:59.080 --> 00:27:03.200]   One thing that I want to do is I want to do a really large scale comparative study.
[00:27:03.200 --> 00:27:08.640]   This is actually a little bit more on what I do at Stanford more than what I do specifically
[00:27:08.640 --> 00:27:09.640]   for Ludwig.
[00:27:09.640 --> 00:27:15.880]   But I'm really curious in doing a large comparative study among different models with different
[00:27:15.880 --> 00:27:20.600]   hyperparameter optimization values on different tasks.
[00:27:20.600 --> 00:27:25.040]   And maybe one interesting outcome of that could be something that looks like a recommender
[00:27:25.040 --> 00:27:30.720]   system that tells you, "I have this new dataset with this amount of data of these data types.
[00:27:30.720 --> 00:27:33.640]   What model do you suggest me to use given these constraints?"
[00:27:33.640 --> 00:27:36.920]   Because I think that the constraints are important.
[00:27:36.920 --> 00:27:44.320]   You may say, "I want only to see models that would take less than 0.1, less than 10 milliseconds
[00:27:44.320 --> 00:27:45.840]   to run inference on."
[00:27:45.840 --> 00:27:50.760]   And so maybe they will rule out some of the more expensive, but also more effective models.
[00:27:50.760 --> 00:27:53.920]   So depending on the constraints, like suggesting something that depends on the constraints,
[00:27:53.920 --> 00:27:56.400]   I think it would be really useful.
[00:27:56.400 --> 00:28:00.920]   Well, now that we have a Weights and Biases integration, we could give you the data of
[00:28:00.920 --> 00:28:04.040]   all the users that chose to make their projects open.
[00:28:04.040 --> 00:28:09.320]   And that might actually give you real world evaluation of the different things that work
[00:28:09.320 --> 00:28:10.320]   and don't work.
[00:28:10.320 --> 00:28:13.920]   It would be super cool to see if that was useful.
[00:28:13.920 --> 00:28:14.920]   Absolutely.
[00:28:14.920 --> 00:28:20.120]   I mean, this is something that with your data, you probably can already do, right?
[00:28:20.120 --> 00:28:22.440]   We could think about ways to collaborate on that.
[00:28:22.440 --> 00:28:23.440]   Definitely.
[00:28:23.440 --> 00:28:24.440]   That'd be fun.
[00:28:24.440 --> 00:28:30.560]   Stepping back a little bit, one thing that I wanted to ask you is I noticed that you've
[00:28:30.560 --> 00:28:33.120]   been doing NLP work for quite a long time.
[00:28:33.120 --> 00:28:36.320]   I think before Uber, you were at a startup bought by Uber.
[00:28:36.320 --> 00:28:40.120]   And before that, I think you had your own startup doing natural language processing.
[00:28:40.120 --> 00:28:42.800]   So you've been doing it for over a decade.
[00:28:42.800 --> 00:28:47.760]   I'm kind of curious the perspective of someone like you on kind of the new stuff that we're
[00:28:47.760 --> 00:28:48.760]   seeing.
[00:28:48.760 --> 00:28:55.760]   Do you feel like GPT-3 is a real step function change in the quality of NLP and kind of changes
[00:28:55.760 --> 00:28:57.680]   the possible applications?
[00:28:57.680 --> 00:28:59.240]   Or was it sort of inevitable?
[00:28:59.240 --> 00:29:03.560]   How do you look at the field and how do you feel the field has changed in the time that
[00:29:03.560 --> 00:29:04.560]   you've been working in it?
[00:29:04.560 --> 00:29:05.560]   Yeah.
[00:29:05.560 --> 00:29:06.560]   So, I mean, this is true.
[00:29:06.560 --> 00:29:11.760]   I've been working for at least 10 years right now, basically, in the field.
[00:29:11.760 --> 00:29:14.120]   So I've seen quite a few waves.
[00:29:14.120 --> 00:29:18.360]   Tasks that were interesting 10 years ago are still interesting today.
[00:29:18.360 --> 00:29:22.600]   There are many things that were unsolved back then and still unsolved right now.
[00:29:22.600 --> 00:29:27.880]   We did progress in terms of performance, but I would say the general framework for how
[00:29:27.880 --> 00:29:33.000]   things, the problems and how we approach them hasn't changed a lot.
[00:29:33.000 --> 00:29:38.800]   We were using neural networks before we were using SVMs, but overall, there was not a huge
[00:29:38.800 --> 00:29:43.520]   change in particular in the way things work in industry, really.
[00:29:43.520 --> 00:29:50.160]   But in particular, the capabilities for few shot, actually, the capabilities for interacting
[00:29:50.160 --> 00:29:56.800]   with the model itself through language that is shown by something like GPT-3, those are
[00:29:56.800 --> 00:30:02.960]   kind of change kind of the paradigm of interaction with those systems.
[00:30:02.960 --> 00:30:09.040]   And I think I'm not sure of the commercial usefulness and application or something like
[00:30:09.040 --> 00:30:17.720]   that, but what I'm sure of is having a general system to which you could give a really small
[00:30:17.720 --> 00:30:25.160]   amount of examples, and then the system picks on that and is able to perform the same kind
[00:30:25.160 --> 00:30:32.280]   of task that you've shown it on unseen data right off the bat without needing specific
[00:30:32.280 --> 00:30:34.920]   training for solving those tasks.
[00:30:34.920 --> 00:30:39.080]   That's a very compelling thing and something that may bring the industry in a different
[00:30:39.080 --> 00:30:40.080]   direction, I believe.
[00:30:40.080 --> 00:30:46.040]   So, I see an interesting world in the future where that shift happens.
[00:30:46.040 --> 00:30:52.000]   Although I still have my questions, the jury is still...
[00:30:52.000 --> 00:31:00.280]   We haven't settled on a final answer on how much and which scenarios this actually works
[00:31:00.280 --> 00:31:03.400]   to the point that we can actually use it.
[00:31:03.400 --> 00:31:04.680]   But let's see about that.
[00:31:04.680 --> 00:31:07.720]   I'm curious to see what the near future holds.
[00:31:07.720 --> 00:31:08.720]   Cool.
[00:31:08.720 --> 00:31:14.040]   Well, I can see we're running out of time and we always end on two questions, and I
[00:31:14.040 --> 00:31:16.920]   want to give you a little bit of time to answer these questions.
[00:31:16.920 --> 00:31:23.120]   The penultimate question that we ask is, what is a topic in machine learning broadly that
[00:31:23.120 --> 00:31:27.520]   you think doesn't get as much attention as it deserves?
[00:31:27.520 --> 00:31:31.880]   So I think now it's getting a little bit more attention than it was before.
[00:31:31.880 --> 00:31:36.200]   So I may be a little bit out of time giving this answer, but I believe that something
[00:31:36.200 --> 00:31:41.480]   that I think is very important is systematic generalization.
[00:31:41.480 --> 00:31:47.000]   And again, there's been work from Marco Barone, Brennan Lake, and also Jostan and Baum on
[00:31:47.000 --> 00:31:53.620]   this topic, but has not been for a long time at the forefront of research.
[00:31:53.620 --> 00:32:00.400]   But it's something that is super interesting and it's something that if solved may unlock
[00:32:00.400 --> 00:32:05.560]   many applications also of machine learning where now we have a hard time applying machine
[00:32:05.560 --> 00:32:06.560]   learning.
[00:32:06.560 --> 00:32:10.640]   For instance, in scenarios where there's a lot of shift in distribution of data over
[00:32:10.640 --> 00:32:17.440]   time or in scenarios where we need to train from less data, if we had a solution for systematic
[00:32:17.440 --> 00:32:21.520]   generalization, we could be able to apply machine learning models also in these scenarios.
[00:32:21.520 --> 00:32:24.960]   So I'm really looking forward to more research on that topic.
[00:32:24.960 --> 00:32:28.080]   And could you define what systematic generalization means?
[00:32:28.080 --> 00:32:33.520]   Yeah, I may be butchering it a little bit, but at least the way I see it is the fact
[00:32:33.520 --> 00:32:41.080]   that you have a model that can figure out a way to generalize beyond the training data,
[00:32:41.080 --> 00:32:43.880]   obviously, but generalize in a way that is systematic.
[00:32:43.880 --> 00:32:52.320]   So that learns that, I can give you a practical example of all the specific instances of a
[00:32:52.320 --> 00:32:55.280]   specific phenomenon, it behaves in the same way.
[00:32:55.280 --> 00:33:02.560]   Like it realizes that, for instance, if you're talking about tax, that is invariant to the
[00:33:02.560 --> 00:33:09.000]   choice of entities or is invariant to the choice of some synonyms when it's returning
[00:33:09.000 --> 00:33:11.200]   its predictions.
[00:33:11.200 --> 00:33:16.160]   And I think it's really important because those models that exhibit a behavior like
[00:33:16.160 --> 00:33:18.600]   that are models that we can trust.
[00:33:18.600 --> 00:33:19.600]   Cool.
[00:33:19.600 --> 00:33:24.720]   Well, the final question is, and maybe you can really rely on your experience at Uber
[00:33:24.720 --> 00:33:30.040]   here, what's the hardest part about taking an ML project from conceiving of the idea
[00:33:30.040 --> 00:33:33.760]   to getting it deployed in production and doing something useful?
[00:33:33.760 --> 00:33:38.480]   Yeah, I think the answer to this is it changes a lot depending on the type of organization
[00:33:38.480 --> 00:33:39.480]   that you're working in.
[00:33:39.480 --> 00:33:43.640]   Like if you're in a startup, you can do things differently.
[00:33:43.640 --> 00:33:46.480]   If you are in a big organization, maybe different.
[00:33:46.480 --> 00:33:51.480]   So I can speak in particular for the big organization kind of setting.
[00:33:51.480 --> 00:33:58.200]   I can say that in particular for researchers, one thing that is difficult is then to put
[00:33:58.200 --> 00:34:02.860]   whatever you obtained in your research into production.
[00:34:02.860 --> 00:34:06.360]   And there's at least two sets of problems why that is difficult.
[00:34:06.360 --> 00:34:09.360]   One is like a practical one, an engineering one.
[00:34:09.360 --> 00:34:14.120]   Usually the infrastructure for deployment is not the same that you use for training
[00:34:14.120 --> 00:34:17.560]   your models and so there's a mismatch there that has to be filled.
[00:34:17.560 --> 00:34:22.400]   And also maybe your models are a little bit too slow for the, what are the needs for inference
[00:34:22.400 --> 00:34:23.400]   at scale.
[00:34:23.400 --> 00:34:29.020]   And so there needs to be some compromises there and that's one of the problems.
[00:34:29.020 --> 00:34:33.000]   But the other problem, which in my opinion, it's more important because it's not a technical
[00:34:33.000 --> 00:34:39.800]   one, it's harder to solve, is a misalignment in the goals really of what the model should
[00:34:39.800 --> 00:34:41.040]   be doing.
[00:34:41.040 --> 00:34:46.360]   You may be optimizing your model with whatever metric you care about, let's say for centropy
[00:34:46.360 --> 00:34:50.200]   loss or maybe you have a ranking problem and you're optimizing for the mean reciprocal
[00:34:50.200 --> 00:34:55.040]   rank or whatever other metric you're using for both optimization and evaluation.
[00:34:55.040 --> 00:35:01.680]   But in the end, in many real scenarios, those metric are just proxies for what you actually
[00:35:01.680 --> 00:35:02.880]   care about.
[00:35:02.880 --> 00:35:06.140]   And what you actually care about if you're doing, for instance, a recommender system
[00:35:06.140 --> 00:35:11.960]   is you care about how many people are clicking on the items that you are suggesting.
[00:35:11.960 --> 00:35:17.140]   And maybe if there's a store, how many people are actually buying something.
[00:35:17.140 --> 00:35:24.480]   You may have the model that is 20% better MRR offline, you deploy it and people don't
[00:35:24.480 --> 00:35:27.100]   buy more.
[00:35:27.100 --> 00:35:29.300]   That's not the model that is going to be deployed.
[00:35:29.300 --> 00:35:34.720]   And so that's something that machine learning people usually don't think a lot about.
[00:35:34.720 --> 00:35:39.920]   And it's something that in my experience has been the main friction, the main attrition
[00:35:39.920 --> 00:35:45.320]   point between developing something offline and then getting something that deployed for
[00:35:45.320 --> 00:35:48.320]   real in front of the users.
[00:35:48.320 --> 00:35:49.320]   That makes sense.
[00:35:49.320 --> 00:35:50.320]   Well, thank you so much, Piero.
[00:35:50.320 --> 00:35:52.200]   It's a real pleasure to talk to you.
[00:35:52.200 --> 00:35:54.400]   Yeah, thank you for the really interesting questions.
[00:35:54.400 --> 00:35:57.040]   It was really fun to chat with you too.
[00:35:57.040 --> 00:35:58.440]   Yeah, thank you.
[00:35:58.440 --> 00:36:01.640]   Thanks for listening to another episode of Gradient Dissent.
[00:36:01.640 --> 00:36:05.920]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:36:05.920 --> 00:36:08.680]   hear from the people that are listening to the episodes.
[00:36:08.680 --> 00:36:12.760]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:36:12.760 --> 00:36:16.700]   a conversation, that would make me inspired to do more of these episodes.
[00:36:16.700 --> 00:36:20.240]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

