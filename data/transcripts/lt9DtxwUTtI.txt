
[00:00:00.000 --> 00:00:06.680]   awesome. Let's let's get started. Welcome back, everyone.
[00:00:06.680 --> 00:00:09.720]   This is week three and we sorted out the technical issues. Sorry
[00:00:09.720 --> 00:00:13.720]   about that. The ones that we had to face last week, we'll be
[00:00:13.720 --> 00:00:18.520]   talking about fine tuning models today using fast AI and we'll
[00:00:18.520 --> 00:00:21.680]   have presentations from Dave, he'll be talking on blur, as
[00:00:21.680 --> 00:00:25.360]   well as please make an important note, he will say no, but you
[00:00:25.360 --> 00:00:29.240]   must call him the Professor Zach. So make sure you call him the
[00:00:29.240 --> 00:00:31.320]   Professor, he'll say no, he doesn't know what to call
[00:00:31.320 --> 00:00:35.080]   himself. I'm telling you to do that. And after that, Artur will
[00:00:35.080 --> 00:00:37.600]   also present about his framework. So it's it's super
[00:00:37.600 --> 00:00:41.480]   exciting stuff. And I won't talk about this because I'm pretty
[00:00:41.480 --> 00:00:43.920]   sure butcher it. I'll keep an eye out for the questions. Keep
[00:00:43.920 --> 00:00:47.200]   them coming in the chat. And now over to Wade, please take it
[00:00:47.200 --> 00:00:47.480]   away.
[00:00:47.480 --> 00:00:49.800]   Awesome. Can everybody see my screen?
[00:00:49.800 --> 00:00:51.560]   Yes, I can.
[00:00:51.560 --> 00:00:54.160]   I was moving. All right. Awesome. All right. So welcome
[00:00:54.160 --> 00:00:58.760]   back. We're at week three. And this is what I've been excited
[00:00:58.760 --> 00:01:01.960]   about because this is the fine tuning week. So if you've been
[00:01:01.960 --> 00:01:05.480]   keeping up with the course content, essentially, they're
[00:01:05.480 --> 00:01:10.160]   looking at ways in with Hugging Face and also just as PyTorch
[00:01:10.160 --> 00:01:14.040]   and their accelerate library, how to train transformers. But
[00:01:14.040 --> 00:01:17.520]   since we're doing things with the fast AI ish bent, we're
[00:01:17.520 --> 00:01:20.400]   going to be looking at three approaches to training
[00:01:20.400 --> 00:01:27.200]   transformers with fast AI. So go ahead and go through this. So
[00:01:27.240 --> 00:01:30.120]   essentially, today, we're looking at section three, I'm
[00:01:30.120 --> 00:01:33.080]   going to go through the section three content really quick. So
[00:01:33.080 --> 00:01:36.880]   we can get to the demos. strongly encourage everybody to
[00:01:36.880 --> 00:01:42.720]   go back, look at the course, watch the videos, and be able to
[00:01:42.720 --> 00:01:47.280]   run the examples there that will only help your learning. And,
[00:01:47.280 --> 00:01:51.880]   again, our goal this week is in particular to understand how to
[00:01:51.880 --> 00:01:54.840]   fine tune Hugging Face transformers with fast AI. And
[00:01:54.840 --> 00:01:57.200]   like I said, we'll be looking at three different approaches.
[00:01:57.240 --> 00:02:05.000]   Adapt NLP, Blur and Fast Hugs. Again, the resources if you have
[00:02:05.000 --> 00:02:10.040]   any questions during the week, please post those to the
[00:02:10.040 --> 00:02:14.080]   discord. And again, depending on where you're at, we got
[00:02:14.080 --> 00:02:19.000]   resources for learning fast AI for actually working with
[00:02:19.000 --> 00:02:24.080]   Hugging Face with fast AI, or just general ML data science, in
[00:02:24.080 --> 00:02:27.960]   particular, with the Chi Time data science podcast, and also
[00:02:27.960 --> 00:02:31.280]   Weights and Biases. And I think you'll see a little brief demo
[00:02:31.280 --> 00:02:34.120]   of Weights and Biases that I'm going to provide. And then Arto
[00:02:34.120 --> 00:02:39.000]   has a much more extensive, more razzle dazzle demo to show you
[00:02:39.000 --> 00:02:46.560]   as well. So fine tuning a pre trained model, we looked at
[00:02:46.560 --> 00:02:50.480]   this mostly last week. But if you're looking for a data set,
[00:02:50.520 --> 00:02:55.960]   go to huggingface.co/datasets. And there's all kinds of ways to
[00:02:55.960 --> 00:03:01.240]   search and find something that is relevant to the domain that
[00:03:01.240 --> 00:03:04.680]   you're working with. And we also looked just, you know, very
[00:03:04.680 --> 00:03:09.320]   simply that they have a nifty map function, if you need to
[00:03:09.320 --> 00:03:15.240]   pre process data, you can batch this as well. So that instead of
[00:03:15.240 --> 00:03:18.160]   doing one example at a time, it can do four or eight examples.
[00:03:18.600 --> 00:03:21.080]   And then there's typically some other work you have to do with
[00:03:21.080 --> 00:03:25.320]   removing columns that don't matter, renaming columns, and
[00:03:25.320 --> 00:03:29.480]   then formatting it in our case, for PyTorch. And then a nice
[00:03:29.480 --> 00:03:32.720]   thing, if you are using data sets, and you want to quickly
[00:03:32.720 --> 00:03:37.160]   iterate through your training loop, you can create a subset by
[00:03:37.160 --> 00:03:42.000]   using the select method and passing a range. So in this
[00:03:42.000 --> 00:03:45.440]   case, if you want to just kind of get going and make sure your
[00:03:45.440 --> 00:03:49.320]   training loop runs, instead of loading all you know, 100,000
[00:03:49.320 --> 00:03:51.960]   training examples, you could just learn the first load the
[00:03:51.960 --> 00:03:56.680]   first 100. And then we also talked about last week, this
[00:03:56.680 --> 00:04:00.920]   idea of dynamic padding. So here, it's like, essentially,
[00:04:00.920 --> 00:04:05.560]   the idea is instead of just batching to the max model length,
[00:04:05.560 --> 00:04:08.640]   you're going to batch to the longest length of a sequence in
[00:04:08.640 --> 00:04:13.560]   a batch. And this is recommended. And mostly because
[00:04:13.560 --> 00:04:16.880]   it will make your models train faster, and you'll have less
[00:04:16.880 --> 00:04:18.640]   padding token, so you're probably gonna get better
[00:04:18.640 --> 00:04:24.160]   results. The other thing we talked about in this week
[00:04:24.160 --> 00:04:27.480]   section is using the trainer API. And this is essentially a
[00:04:27.480 --> 00:04:33.080]   high level training evaluation loop that you don't have to
[00:04:33.080 --> 00:04:37.640]   code. And it can run on multiple GPUs, TPUs, there's lots of
[00:04:37.640 --> 00:04:43.160]   options, you can use things like mixed precision. And to control
[00:04:43.160 --> 00:04:45.960]   this, you basically create a training arguments, which is
[00:04:45.960 --> 00:04:48.720]   like essentially your, your config for how it's going to
[00:04:48.720 --> 00:04:53.520]   train. And you pass that to a trainer object, and it just does
[00:04:53.520 --> 00:04:57.360]   this thing. And so if you're looking to just a high level,
[00:04:57.360 --> 00:05:01.640]   just to use hugging face and train, this is a good option.
[00:05:01.640 --> 00:05:07.080]   But we can also use pytorch. And so this should all look
[00:05:07.080 --> 00:05:13.080]   pretty familiar to most folks by now. And here we have our
[00:05:13.080 --> 00:05:15.320]   training loop, where we're going through our data loader
[00:05:15.320 --> 00:05:18.800]   batch at a time putting on the right device, patching up,
[00:05:18.800 --> 00:05:22.640]   passing all of our inputs with a model. And so remember, this is
[00:05:22.640 --> 00:05:26.040]   going to be your input IDs, your attention mask, maybe token type
[00:05:26.040 --> 00:05:29.560]   IDs, depending on the model you're using. And in this case,
[00:05:29.560 --> 00:05:33.360]   we're not going to we're going to let hugging face calculate
[00:05:33.360 --> 00:05:38.320]   the loss itself. And that's going to be accessible right
[00:05:38.320 --> 00:05:42.840]   here using the loss property on the outputs object. Then we call
[00:05:42.840 --> 00:05:47.640]   backward, get our gradients, update our weights, move our
[00:05:47.640 --> 00:05:51.480]   scheduler, one step, clear everything out, and so forth. So
[00:05:51.480 --> 00:05:54.760]   pretty standard stuff. And then on the other side over here,
[00:05:54.760 --> 00:05:58.320]   they have an example of using their metrics library. And so
[00:05:58.320 --> 00:06:03.680]   you can actually pass in a data set, get the metrics associated
[00:06:03.680 --> 00:06:10.960]   to it, and then loop through your evaluation data loader. And
[00:06:11.120 --> 00:06:14.680]   at every batch, add the predictions. And at the end,
[00:06:14.680 --> 00:06:19.040]   compute them and get your results. So if you go through
[00:06:19.040 --> 00:06:21.080]   the course, you'll see the trainer API example, you can run
[00:06:21.080 --> 00:06:24.760]   them collab, you can run the pytorch example. And then they
[00:06:24.760 --> 00:06:30.200]   also have this library called accelerate. And essentially, the
[00:06:30.200 --> 00:06:32.600]   shortcoming of the trainer API is that you lose control over
[00:06:32.600 --> 00:06:35.640]   the training loop, right? It does everything for you. The
[00:06:35.640 --> 00:06:39.880]   nice thing with pytorch is that you can control everything. And
[00:06:39.880 --> 00:06:43.520]   so with accelerate, you are essentially using that pytorch
[00:06:43.520 --> 00:06:47.880]   loop that we just saw with some just minor changes. And all of
[00:06:47.880 --> 00:06:50.360]   a sudden, you can have distributed training, you can
[00:06:50.360 --> 00:06:54.600]   train on one or more GPUs or TPUs. And again, when you look
[00:06:54.600 --> 00:06:59.000]   at the documentation, you'll see examples of how to do so you
[00:06:59.000 --> 00:07:03.000]   can run that collab. It's really cool. And this is something that
[00:07:03.000 --> 00:07:08.480]   I remember a while back, Jeremy Howard said that Sylvan was
[00:07:08.480 --> 00:07:12.000]   going to help work on in terms of integrating into fast AI. I
[00:07:12.000 --> 00:07:15.320]   don't know where that is. But when it's there, definitely it's
[00:07:15.320 --> 00:07:20.480]   something that we're going to want to use. Any questions on
[00:07:20.480 --> 00:07:25.120]   any of that speed through of the course, it took me a lot longer
[00:07:25.120 --> 00:07:26.880]   to go through the course than that.
[00:07:26.880 --> 00:07:31.880]   I think it made sense to me. So if it makes sense to me, I'm
[00:07:31.880 --> 00:07:33.360]   sure everyone would have understood.
[00:07:35.280 --> 00:07:39.440]   All right, cool. Okay, so, again, go through the course,
[00:07:39.440 --> 00:07:43.040]   you can see how to do this, how to how to fine tune with pytorch
[00:07:43.040 --> 00:07:47.040]   and accelerate and the trainer API. But how do we do this in a
[00:07:47.040 --> 00:07:51.200]   fast AI world. And so we're gonna look at three approaches.
[00:07:51.200 --> 00:07:55.360]   We're gonna start with blur, which I'll demo, Zach is going
[00:07:55.360 --> 00:08:00.720]   to demo his work with adapt NLP. And then last but not least,
[00:08:00.720 --> 00:08:04.680]   Ardo is going to demo fast hugs. And he's got all kinds of like
[00:08:04.680 --> 00:08:08.720]   nifty stuff in his demo, too. So I'm kind of jealous. And
[00:08:08.720 --> 00:08:10.160]   anyways, we'll get started.
[00:08:10.160 --> 00:08:19.280]   Alright, so I'm not going to run through everything in real time,
[00:08:19.280 --> 00:08:22.000]   I've already ran through this for the sake of time.
[00:08:22.000 --> 00:08:28.200]   But we're gonna look at fine tuning with the blur library.
[00:08:28.200 --> 00:08:33.000]   And first, I wanted to state my design objectives. And they're
[00:08:33.000 --> 00:08:37.160]   fairly simple. So first is I wanted to make a fast AI first
[00:08:37.160 --> 00:08:41.840]   library. So that means that when you look at blur, and you use
[00:08:41.840 --> 00:08:45.400]   it, it should be really familiar to you. If you know how to work
[00:08:45.400 --> 00:08:50.440]   with the CNN, look, learner or with a learner to do image
[00:08:50.440 --> 00:08:54.000]   classification with the CNN, this should all look really
[00:08:54.000 --> 00:08:57.480]   familiar to you. And you can follow the essentially the same
[00:08:57.480 --> 00:09:02.960]   steps to train a transformer. The other objective was to
[00:09:02.960 --> 00:09:07.320]   support core tests, core tasks out of the box with any
[00:09:07.320 --> 00:09:12.880]   transformer architecture. And so blur supports causal and mass
[00:09:12.880 --> 00:09:15.800]   language modeling sequence classification, token
[00:09:15.800 --> 00:09:18.160]   classification, extractive question and answering
[00:09:18.160 --> 00:09:21.520]   summarization and translation tasks using almost any
[00:09:21.520 --> 00:09:25.400]   transformer architecture out of the box. So very rarely will you
[00:09:25.400 --> 00:09:28.960]   have to implement your own callbacks or transforms to to
[00:09:28.960 --> 00:09:33.480]   handle the particular inputs or outputs of any given transformer
[00:09:33.480 --> 00:09:37.160]   architecture. And if you look at the documentation, I don't test
[00:09:37.160 --> 00:09:43.760]   all the transformers, but you'll see tests for most of them. And
[00:09:43.760 --> 00:09:46.800]   then the other nice thing with this is that if you're running
[00:09:46.800 --> 00:09:50.200]   experiments, and you want to play with different architectures
[00:09:50.200 --> 00:09:53.680]   and see how they work, literally, you can just swap out
[00:09:53.680 --> 00:09:57.000]   the checkpoint and say, like, I'm using Roberta base, and then
[00:09:57.000 --> 00:10:00.280]   I'm going to use the Bertha, I'm going to try a BERT base case
[00:10:00.280 --> 00:10:04.000]   which I burnt based on case, and you won't have to rewrite any
[00:10:04.000 --> 00:10:07.600]   code to make that happen. And then the last one is to keep
[00:10:07.600 --> 00:10:10.680]   things flexible and extendable. And this is something you'll see
[00:10:10.680 --> 00:10:14.040]   in common with all of the approaches is that all of us are
[00:10:14.040 --> 00:10:18.960]   using fast AI transforms and callbacks extensively to do a
[00:10:18.960 --> 00:10:23.600]   lot of the magic. And just like fast AI, you can take blur as
[00:10:23.600 --> 00:10:27.000]   is, or you can customize any of those points. And as a side
[00:10:27.000 --> 00:10:30.360]   note, if you're doing fast AI, I highly recommend you spend some
[00:10:30.360 --> 00:10:34.440]   time understanding the callback system by looking at the
[00:10:34.440 --> 00:10:38.040]   callbacks, and you can kind of see how they work. That's really
[00:10:38.040 --> 00:10:42.360]   where, again, I would say most of the magic happens with fast
[00:10:42.360 --> 00:10:47.800]   AI. So let's look at the code. So we're going to import a bunch
[00:10:47.800 --> 00:10:51.680]   of things. And in honor of our hosts, I'm going to actually do
[00:10:51.680 --> 00:10:54.160]   some really simple weights and bias stuff here just to show
[00:10:54.160 --> 00:10:57.680]   you how easy it is to get going. And thanks to Morgan for
[00:10:57.680 --> 00:11:01.800]   walking through some of this with me. So in this example, we
[00:11:01.800 --> 00:11:05.480]   log into weights and biases. And the first thing we're going to
[00:11:05.480 --> 00:11:12.840]   do is essentially configure our model class in a checkpoint. And
[00:11:12.840 --> 00:11:18.320]   all of us are going to actually show you how to demo the MRPC
[00:11:18.360 --> 00:11:22.880]   data set. And I'm going to follow for the most part what
[00:11:22.880 --> 00:11:27.280]   you see in the course documentation, and I think Zach
[00:11:27.280 --> 00:11:31.320]   and Ardo are as well. So you're really kind of seeing a one for
[00:11:31.320 --> 00:11:33.880]   one, like from the transformers, you know, from doing this in
[00:11:33.880 --> 00:11:39.400]   PyTorch and with the trainer, how to do this in fast AI. So
[00:11:39.400 --> 00:11:43.880]   with kind of our, our model class and checkpoint configured,
[00:11:44.280 --> 00:11:48.520]   we move to actually getting our data. And we've showed this
[00:11:48.520 --> 00:11:53.400]   before we actually load a data set. And once we have a data
[00:11:53.400 --> 00:11:57.520]   set, we need to because we're working in fast AI is define a
[00:11:57.520 --> 00:12:02.000]   validation data set. And so here, I'm going to find the
[00:12:02.000 --> 00:12:06.160]   validation indexes. And then I'm going to concatenate the two
[00:12:06.160 --> 00:12:09.720]   data sets because fast AI likes to get, especially when using
[00:12:09.720 --> 00:12:14.680]   data block API, everything in one source, your training and
[00:12:14.680 --> 00:12:20.360]   your validation data. From there, I calculate the number of
[00:12:20.360 --> 00:12:25.800]   labels we're working with. So remember that if you're working
[00:12:25.800 --> 00:12:29.560]   with something other than two labels, this is something you're
[00:12:29.560 --> 00:12:33.680]   going to want to pass into your configuration object. So it's
[00:12:33.680 --> 00:12:37.800]   good just to get it anyway. And then with blur with this
[00:12:37.800 --> 00:12:42.440]   information, we can go ahead and get our architecture, we can
[00:12:42.440 --> 00:12:48.400]   instantiate our configuration tokenizer and model object. With
[00:12:48.400 --> 00:12:51.320]   that, we can construct a data block like this. So we're going
[00:12:51.320 --> 00:12:56.520]   to use blurs HF take text block. And then for our targets, a
[00:12:56.520 --> 00:12:59.920]   category block since we're doing a multi classification problem.
[00:12:59.920 --> 00:13:05.400]   And again, the data block is essentially a blueprint for how
[00:13:05.400 --> 00:13:08.880]   to transform your raw data into number something that can go
[00:13:08.880 --> 00:13:12.720]   through a model. And so here, we're going to define our get x
[00:13:12.720 --> 00:13:18.280]   as sentence one and sentence to our target label. And then we're
[00:13:18.280 --> 00:13:22.120]   using the index splitter and the validation indexes we defined
[00:13:22.120 --> 00:13:29.040]   above to define what our validation set is. And then we
[00:13:29.040 --> 00:13:34.080]   can go ahead and call data loaders, we're going to pass in
[00:13:34.080 --> 00:13:38.240]   some other parameters for batch size and validation. And then
[00:13:38.240 --> 00:13:41.880]   again, just like fast AI, we can go ahead and call show batch, we
[00:13:41.880 --> 00:13:44.200]   pass in the data loaders. Again, this looks kind of odd, but
[00:13:44.200 --> 00:13:48.120]   we're passing the sense so I can get access to the tokenizer that
[00:13:48.120 --> 00:13:52.360]   the the hugging face tokenizer to control the output that you
[00:13:52.360 --> 00:13:57.760]   see here. Once we have that, we can train and one cool thing
[00:13:57.760 --> 00:14:01.080]   with weights and biases to get things training. Once you have
[00:14:01.080 --> 00:14:05.360]   your project and your run configured, which we did in the
[00:14:05.360 --> 00:14:11.640]   these quads above, we could just call it in it and get things
[00:14:11.640 --> 00:14:16.640]   going. And it actually has a nice little visual plugin that
[00:14:16.640 --> 00:14:19.720]   runs in your notebook that actually updates real time. So
[00:14:19.720 --> 00:14:25.720]   you can see how your how your runs going. And then we have a
[00:14:25.720 --> 00:14:29.400]   couple options for building a learner. So we can go this
[00:14:29.400 --> 00:14:33.520]   approach, which is doing everything by by hand. But we
[00:14:33.520 --> 00:14:36.760]   also have a higher level learner that derive it basically a class
[00:14:36.760 --> 00:14:40.560]   that derives from the fast AI learner called a learner,
[00:14:40.560 --> 00:14:44.760]   pronounced learner, but it looks just like the learner. I don't
[00:14:44.760 --> 00:14:49.960]   know if it would be better to call this BLURNER or what. So I
[00:14:49.960 --> 00:14:52.520]   just kept it like this for right now. But these are called
[00:14:52.520 --> 00:14:57.680]   learners. And it basically the learner, a learner encapsulates
[00:14:57.720 --> 00:15:02.760]   all of this stuff right here. So depending on it, so it
[00:15:02.760 --> 00:15:05.240]   basically just does all this out of the box, and then you can
[00:15:05.240 --> 00:15:07.960]   pass the same things you can to a fast AI learner. So you can
[00:15:07.960 --> 00:15:10.760]   pass it the callbacks and pass metrics like we're doing here.
[00:15:10.760 --> 00:15:16.600]   And it returns a frozen learner, which means that all the
[00:15:16.600 --> 00:15:20.640]   parameters of the box are are frozen with the exception of
[00:15:20.640 --> 00:15:25.800]   your classification head. And so we can do that right here. And
[00:15:25.800 --> 00:15:29.960]   once we have that, we can start training this just like any fast
[00:15:29.960 --> 00:15:35.360]   AI application. So we can use the LR finder. And here I'm
[00:15:35.360 --> 00:15:40.840]   pulling back the minimum, the steepest part of the curve where
[00:15:40.840 --> 00:15:44.040]   the valley is where the slide is. And it's nice because it
[00:15:44.040 --> 00:15:49.520]   shows you all those things. And you can see that in the course,
[00:15:49.520 --> 00:15:53.160]   they use five e neg five, which I'm just going to use here. And
[00:15:53.160 --> 00:15:56.120]   if you kind of look at like, usually, you would start here or
[00:15:56.120 --> 00:15:59.640]   here and go back 10x. And it kind of gets you in that five e
[00:15:59.640 --> 00:16:05.160]   neg five range, which is kind of interesting. And then to use
[00:16:05.160 --> 00:16:09.200]   weights and biases, just the callback that you use when you
[00:16:09.200 --> 00:16:12.960]   call fit. And once you have this, it will actually start
[00:16:12.960 --> 00:16:16.600]   logging to weights and biases, weights and biases. And then you
[00:16:16.600 --> 00:16:23.120]   can actually go to wandb.ai and actually look at all your
[00:16:23.120 --> 00:16:29.120]   training runs, which is really cool. Also, just as a side note,
[00:16:29.120 --> 00:16:31.440]   when you look at this notebook, I have little tips. So when
[00:16:31.440 --> 00:16:33.520]   you're looking at this, and you're and you're, and you're
[00:16:33.520 --> 00:16:36.360]   exploring ways to improve it, I have little tips about things
[00:16:36.360 --> 00:16:41.680]   you can look at. And so one is actually looking at papers, if
[00:16:41.680 --> 00:16:44.880]   you're using BERT based on case, go to the BERT paper, and look
[00:16:44.880 --> 00:16:47.920]   at what they used, and they had success with that will give you
[00:16:47.920 --> 00:16:51.920]   a good idea of where to where to start. So we're going to train
[00:16:51.960 --> 00:16:56.880]   and we we actually get pretty good results here. We get about
[00:16:56.880 --> 00:17:02.040]   point nine on the F1 score about 86% accuracy. And then we can
[00:17:02.040 --> 00:17:09.360]   also just like fast AI show results. We can run learn dot
[00:17:09.360 --> 00:17:12.360]   validate, look at our validation information at the conclusion of
[00:17:12.360 --> 00:17:16.760]   everything, we can get our our predictions with called a get
[00:17:16.760 --> 00:17:21.400]   preds. And then the wandb.finish basically says this run is
[00:17:21.400 --> 00:17:27.840]   done, and it closes things out for the next run. And then also,
[00:17:27.840 --> 00:17:30.120]   again, because we're using fast AI, we can do inference just
[00:17:30.120 --> 00:17:33.400]   like we normally do, we can export a learner called load
[00:17:33.400 --> 00:17:39.040]   learner, and then actually pass in a single or a list of
[00:17:39.040 --> 00:17:44.160]   examples or text for it to do predictions on. And the big
[00:17:44.160 --> 00:17:47.840]   thing here is it just has to be in the format that the data
[00:17:47.840 --> 00:17:51.120]   loader got its training and validation data. And so with the
[00:17:51.120 --> 00:17:54.160]   data, since we're using the data sets library, we're looking at
[00:17:54.160 --> 00:17:57.080]   list of dictionaries. So you pass in a dictionary or a list
[00:17:57.080 --> 00:18:01.560]   of dictionaries. And then batch interest interest is super easy.
[00:18:01.560 --> 00:18:07.160]   We can just on the learner access the test DL method from
[00:18:07.160 --> 00:18:11.480]   our data loaders, we're going to pass in a subset of our test
[00:18:11.480 --> 00:18:17.160]   data set and get predictions. So super easy. If you're working
[00:18:17.160 --> 00:18:21.760]   with a test set that is either labeled or unlabeled after the
[00:18:21.760 --> 00:18:25.960]   conclusion of your training. And then lastly, we can also use
[00:18:25.960 --> 00:18:29.800]   the high level API in blur. So for each task, we have a high
[00:18:29.800 --> 00:18:35.080]   level learner. And so here, I'm importing learner for sequence
[00:18:35.080 --> 00:18:38.840]   classification. And just to make this interesting, I'm actually
[00:18:38.840 --> 00:18:42.040]   going to use the data frame, it can actually do data frame or
[00:18:42.040 --> 00:18:46.160]   list of dictionaries out of the box. And with data sets, there's
[00:18:46.160 --> 00:18:50.480]   this nifty little dot two pandas to take a hugging face data set,
[00:18:50.480 --> 00:18:54.280]   convert it to a data frame. And once we have that I'm defining
[00:18:54.280 --> 00:18:58.920]   another column called is valid, right false for the train true
[00:18:58.920 --> 00:19:02.120]   for the validation data. And then I'm going to concatenate
[00:19:02.120 --> 00:19:09.760]   everything. And then because I am using a data frame with that
[00:19:09.760 --> 00:19:14.120]   is validate that is valid equals true or false. By default, the
[00:19:14.120 --> 00:19:17.760]   from data frame method will use a call splitter and split on
[00:19:17.760 --> 00:19:20.000]   that. So that's how we're going to get our validation and
[00:19:20.000 --> 00:19:24.320]   training set. And so you can see with using the high level
[00:19:24.320 --> 00:19:27.760]   learner is we essentially pass in our data source, our
[00:19:27.760 --> 00:19:33.000]   checkpoint, our text inputs and labels, we can pass in our data
[00:19:33.000 --> 00:19:40.200]   Lord, our data loaders, quarks, and also pass in learner quarks
[00:19:40.200 --> 00:19:46.360]   too. And this is going to return a frozen learner, just like we
[00:19:46.360 --> 00:19:51.960]   saw above. So I'm going to unfreeze it and train it for
[00:19:51.960 --> 00:19:56.440]   three epochs at five, you know, five. And again, the results
[00:19:56.440 --> 00:19:59.000]   going to differ because it's probabilistic. And you know,
[00:19:59.000 --> 00:20:02.600]   there's there's things are initialized a little bit
[00:20:02.600 --> 00:20:05.400]   differently, and the order of the batches might be a little
[00:20:05.400 --> 00:20:08.560]   bit different. But we get pretty close to the same results as we
[00:20:08.560 --> 00:20:13.440]   did above in this run. And then, again, since a learner is just
[00:20:13.440 --> 00:20:17.520]   a learner, we can do show results, we can export, we can
[00:20:17.520 --> 00:20:21.280]   load, we can do predictions, it's got to be in, again, in the
[00:20:21.280 --> 00:20:28.600]   format that it was trained on. And, and then we can also do a
[00:20:28.600 --> 00:20:33.400]   batch inference, just like we did above this time on a test
[00:20:33.400 --> 00:20:36.880]   data frame. And that's basically blur, that's how you would
[00:20:36.880 --> 00:20:40.280]   fine tune anything. This was sequence classification. But
[00:20:40.280 --> 00:20:44.280]   again, it would work for any of those core tasks that I
[00:20:44.280 --> 00:20:50.840]   mentioned above. And with that, is there any questions yet?
[00:20:50.840 --> 00:20:51.360]   Sanyam?
[00:20:51.360 --> 00:20:54.960]   Yeah, there was a question in the YouTube chat. So the
[00:20:54.960 --> 00:20:57.360]   question is, are there playlists? Yes, there are
[00:20:57.360 --> 00:21:00.720]   playlists, you can find them on the Weights and Biases YouTube
[00:21:00.720 --> 00:21:04.360]   channel, these videos are being put there. The other one, I
[00:21:04.360 --> 00:21:06.640]   think you've already answered it, but it might be word
[00:21:06.640 --> 00:21:10.520]   diving into if you could talk about the purpose of HF splitter
[00:21:10.520 --> 00:21:12.360]   that we're passing in the learner.
[00:21:12.360 --> 00:21:17.880]   Yeah, so HF splitter is kind of a stand like most hugging face
[00:21:17.880 --> 00:21:22.280]   objects have a certain architecture. And so with HF
[00:21:22.280 --> 00:21:28.440]   splitter is I essentially split the parameters into three
[00:21:28.440 --> 00:21:34.000]   groups. So the first one is kind of the initial parts of like the
[00:21:34.000 --> 00:21:39.520]   encoder. The second part is the the second half of the encoder.
[00:21:39.520 --> 00:21:43.480]   So if you have like, let's say you have 12 encoder blocks,
[00:21:43.480 --> 00:21:49.520]   right. And we talked about this back in week one, I think the
[00:21:49.520 --> 00:21:52.520]   splitter will will have the parameters in the first encoder
[00:21:52.520 --> 00:21:58.160]   block being at in one layer group, the second in a second
[00:21:58.160 --> 00:22:00.880]   layer group, and then your whatever your head is like the
[00:22:00.880 --> 00:22:04.240]   classification head in this sense, that will be in a third
[00:22:04.240 --> 00:22:08.440]   group. So you can look at the source code and see how that
[00:22:08.440 --> 00:22:13.000]   works. Sometimes it's helpful to actually freeze everything but
[00:22:13.000 --> 00:22:17.880]   the the head that hasn't been trained. A lot of times it's not
[00:22:17.880 --> 00:22:20.440]   and it's just you'll get better results if you unfreeze
[00:22:20.440 --> 00:22:23.760]   everything. So that's that's what the splitter does. And you
[00:22:23.760 --> 00:22:26.960]   can also pass your own splitter. So there are folks who have not
[00:22:26.960 --> 00:22:29.760]   had success with the defaults and blur. So they create their
[00:22:29.760 --> 00:22:33.360]   own splitting function to do that.
[00:22:33.360 --> 00:22:39.120]   Awesome. No, no other questions.
[00:22:39.120 --> 00:22:44.880]   All right. So with that, I'm going to stop my share. And I
[00:22:44.880 --> 00:22:49.840]   hand things over to Zach to give us a look at adapt NLP.
[00:22:49.840 --> 00:22:52.600]   Oh, so Zach, that's fine.
[00:22:55.640 --> 00:23:02.000]   All right. Hopefully we can all see this. Okay. Yes. Perfect.
[00:23:02.000 --> 00:23:06.080]   All right. So hi, everyone. My name is Zach Mueller. And I'm a
[00:23:06.080 --> 00:23:10.600]   machine learning engineer over at Novetta. And I actually got
[00:23:10.600 --> 00:23:13.440]   my start through the fast AI framework. And so it's very
[00:23:13.440 --> 00:23:16.120]   precious and dear to me. And so I try and apply it in everywhere
[00:23:16.120 --> 00:23:20.840]   I can. And since I'm here, you can guess I applied that to
[00:23:20.840 --> 00:23:24.680]   transformers. So today we'll be talking about adapt NLP and fast
[00:23:24.680 --> 00:23:27.560]   AI, finding and filling the gaps with the transformers
[00:23:27.560 --> 00:23:33.720]   framework. So what is adapt NLP? First of all, I'm beginning in
[00:23:33.720 --> 00:23:40.240]   February of last year, Andrew Chang, a former employee at
[00:23:40.240 --> 00:23:43.640]   Novetta, saw a need for a simpler API when it came to
[00:23:43.640 --> 00:23:47.520]   doing inference with hugging face. At the time, the pipeline
[00:23:47.520 --> 00:23:51.720]   API wasn't quite flexible enough for what we needed. And we were
[00:23:51.720 --> 00:23:54.560]   hoping for something we could deploy quickly. And no matter
[00:23:54.560 --> 00:23:58.640]   what the situation, it just worked. This became a variety of
[00:23:58.640 --> 00:24:02.960]   easy modules, basically designed for quick inference with
[00:24:02.960 --> 00:24:06.960]   multiple models, multiple sentences, multiple queries all
[00:24:06.960 --> 00:24:10.560]   at the same time, while keeping them all in memory and with
[00:24:10.560 --> 00:24:14.680]   strictly very easy to use interfaces, hence the easy
[00:24:14.680 --> 00:24:19.440]   names. As a company, Novetta kind of saw a benefit for making
[00:24:19.440 --> 00:24:23.200]   the library open source. And since then, it's had some
[00:24:23.200 --> 00:24:26.960]   community engagement and some interesting use with the library
[00:24:26.960 --> 00:24:30.640]   ever since. Below, I have linked to the documentation and the
[00:24:30.640 --> 00:24:36.320]   GitHub as well. Now let's get on to what we care about adapt NLP
[00:24:36.320 --> 00:24:40.440]   and fast AI. About six months ago, I was placed on the project
[00:24:40.440 --> 00:24:44.800]   and shortly thereafter, I became the lead developer for it. And
[00:24:44.800 --> 00:24:47.600]   coming from a fast AI background, I really wanted to
[00:24:47.600 --> 00:24:50.720]   try and integrate the fast AI suite. Because I knew if we did
[00:24:50.720 --> 00:24:54.520]   it, well, it could help us modularize our code and help us
[00:24:54.520 --> 00:24:57.920]   with the long term goal of a centralized fine tuning API.
[00:24:57.920 --> 00:25:02.880]   Let's dive a little bit more into what I mean by this. So it
[00:25:02.880 --> 00:25:06.120]   started out really simple. We were basically designed as a
[00:25:06.120 --> 00:25:09.400]   inference framework, as I sort of mentioned earlier. And so I
[00:25:09.400 --> 00:25:15.320]   wanted to start slow and rewrite all of that back end. And I
[00:25:15.320 --> 00:25:18.240]   accomplished this through the fast AI callback system. As
[00:25:18.240 --> 00:25:21.480]   Wade mentioned, it can be a powerful tool, as well as
[00:25:21.480 --> 00:25:24.600]   converting the library over from a standard Python library to
[00:25:24.600 --> 00:25:29.400]   notebook dev through another library called lib2nb dev. And
[00:25:29.400 --> 00:25:32.800]   then from there, basically, I started trying to build what I
[00:25:32.800 --> 00:25:37.080]   thought a good intuitive fine tuning API would be that
[00:25:37.080 --> 00:25:42.520]   leverages fast AI as best as it can, while still making creative
[00:25:42.520 --> 00:25:45.920]   and intuitive decision choices for the user. So that if you
[00:25:45.920 --> 00:25:48.560]   just didn't want to think about anything and train a model that
[00:25:48.560 --> 00:25:52.040]   is an option for you might not be 100% the best, but it's going
[00:25:52.040 --> 00:25:56.400]   to be close to the best. But now the question is why use fast AI?
[00:25:56.400 --> 00:26:00.520]   Why should I take the effort to integrate this framework into
[00:26:00.520 --> 00:26:03.440]   our existing project if we're already got our foundations in
[00:26:03.440 --> 00:26:10.200]   PyTorch? And that starts with callbacks. Now, as Wade sort of
[00:26:10.200 --> 00:26:13.320]   mentioned earlier, understanding the callback system is, in my
[00:26:13.320 --> 00:26:16.160]   opinion, the single most important thing you could
[00:26:16.160 --> 00:26:20.040]   possibly learn about the fast AI framework. Yes, you can learn
[00:26:20.040 --> 00:26:24.000]   the data block API and what have you. But if you want skills that
[00:26:24.000 --> 00:26:27.080]   will really help you leverage the fast AI framework to its
[00:26:27.080 --> 00:26:31.120]   fullest extent, it is the callback system. And the reason
[00:26:31.120 --> 00:26:34.720]   for that is it gives you full access to the fast AI training
[00:26:34.720 --> 00:26:39.120]   loop. People tend to get an idea of because you're letting fast
[00:26:39.120 --> 00:26:42.320]   AI learner take control of everything, you have no freedom.
[00:26:42.320 --> 00:26:46.600]   You can't modify any step and it's sort of there's little road
[00:26:46.600 --> 00:26:50.600]   blocks for you. And the reality of it is, that's actually not
[00:26:50.600 --> 00:26:55.200]   true. It helps keep your code modularized and clean, it
[00:26:55.200 --> 00:26:59.120]   becomes easier to work with and refactor in the future. And in
[00:26:59.120 --> 00:27:02.040]   our example for adapt NLP, it let us reduce our boilerplate
[00:27:02.040 --> 00:27:11.440]   code by about 80%. One instance is say, I need to adjust how my
[00:27:11.440 --> 00:27:16.040]   inputs are gathered. So in fast AI, normally, this is a just a
[00:27:16.040 --> 00:27:20.600]   batch of tensors. And we need to convert that to a dictionary for
[00:27:20.600 --> 00:27:25.000]   a hugging face. So I just wrote these when you really boil down
[00:27:25.000 --> 00:27:30.240]   to it. Eight lines of code as a callback that takes in the raw
[00:27:30.760 --> 00:27:34.360]   data loaders, sets it up as a dictionary, and I can go ahead
[00:27:34.360 --> 00:27:36.920]   and pass that to my model. And it's little things like this,
[00:27:36.920 --> 00:27:40.040]   where once you write the foundations, everything else for
[00:27:40.040 --> 00:27:46.840]   the integration becomes seamless. Now, new to this sort
[00:27:46.840 --> 00:27:49.240]   of framework before the callbacks was specifically to
[00:27:49.240 --> 00:27:52.680]   help with inference and mildly with training. Now we're going
[00:27:52.680 --> 00:27:56.440]   to talk about that fine tuning that I mentioned. So when I was
[00:27:56.440 --> 00:27:58.720]   writing this fine tuning framework, I wanted to take as
[00:27:58.720 --> 00:28:03.840]   much inspiration from fast AI as I could, without using fast AI
[00:28:03.840 --> 00:28:06.200]   directly, because there's essentially two routes you can
[00:28:06.200 --> 00:28:09.960]   take using only fast AI for absolutely everything and
[00:28:09.960 --> 00:28:13.440]   wrapping around its framework, or understanding that fast AI
[00:28:13.440 --> 00:28:16.960]   works with anything and just view it as a training framework.
[00:28:16.960 --> 00:28:21.760]   I chose the latter while trying to emulate their very intuitive
[00:28:21.760 --> 00:28:25.720]   data block API. So what this essentially translate to is a
[00:28:25.720 --> 00:28:29.400]   high level API that anyone can approach without losing any
[00:28:29.400 --> 00:28:32.840]   flexibility, a mid level API. And in my head, this is
[00:28:32.840 --> 00:28:36.320]   something that can take the hugging face data sets, and
[00:28:36.320 --> 00:28:40.040]   converts it into more or less raw torch data loaders, and a
[00:28:40.040 --> 00:28:43.520]   low level API where you just pass in a raw pytorch data set
[00:28:43.520 --> 00:28:48.320]   and a raw and just data sets in general. So on the right here,
[00:28:48.320 --> 00:28:50.240]   we're going to go a little bit more detail into this in the
[00:28:50.240 --> 00:28:53.960]   demo. But this is an example of what I mean. It looks very
[00:28:53.960 --> 00:28:58.480]   similar to the tutorial that you guys followed throughout the
[00:28:58.480 --> 00:29:02.240]   course. But we just wrap everything in this task data set
[00:29:02.240 --> 00:29:06.320]   class. And it looks very similar to say fast AI data block or
[00:29:06.320 --> 00:29:10.080]   fast AI data sets. And we build data loaders the exact same way
[00:29:10.080 --> 00:29:13.240]   by calling dot data loaders. And so these were the key
[00:29:13.240 --> 00:29:17.520]   importances I saw when it came to a user perspective of someone
[00:29:17.520 --> 00:29:20.680]   trying to learn a new framework when we assume that they already
[00:29:20.680 --> 00:29:25.400]   kind of know what fast AI is, but they don't want to deal with
[00:29:25.400 --> 00:29:31.880]   the struggles of its data API. So next on to that training API,
[00:29:31.880 --> 00:29:36.120]   because that's really where the real meat and potatoes sort of
[00:29:36.120 --> 00:29:40.200]   come from with the framework. Now all of this is actively in
[00:29:40.200 --> 00:29:46.000]   development, like, literally added a new thing last week. But
[00:29:46.000 --> 00:29:48.960]   essentially, what I wanted the new fine tuning API to be is
[00:29:48.960 --> 00:29:53.160]   more than just accept some data loaders and train. I wanted it
[00:29:53.160 --> 00:29:57.000]   to be able to work with fast AI learner, but have two
[00:29:57.000 --> 00:30:00.960]   interfaces, a high level interface where the user only
[00:30:00.960 --> 00:30:03.960]   gets access to about five or six different functions that have
[00:30:03.960 --> 00:30:07.680]   specific names and specific meanings, or a low level
[00:30:07.680 --> 00:30:11.200]   interface where it can expose the fast AI learner and get
[00:30:11.200 --> 00:30:15.400]   access to the entire framework. And the goal of that is so that
[00:30:15.400 --> 00:30:19.480]   new users aren't confused by hundreds upon 1000s of different
[00:30:19.480 --> 00:30:23.760]   little functions and attributes affiliated with the learner
[00:30:23.760 --> 00:30:28.920]   while still being able to not lose any of the flexibility and
[00:30:28.920 --> 00:30:34.160]   anything that the original fast AI framework allowed for. And so
[00:30:34.160 --> 00:30:37.280]   an example below, I have something called a sequence
[00:30:37.280 --> 00:30:41.040]   classification tuner that looks kind of similar to a fast AI
[00:30:41.040 --> 00:30:45.360]   learner, we pass in some data loaders, a model name, the
[00:30:45.360 --> 00:30:49.360]   hugging face tokenizer and set a number of classes. And then we
[00:30:49.360 --> 00:30:53.680]   call tune, which is sort of similar to fast AI train, we'll
[00:30:53.680 --> 00:30:57.640]   look a little more in detail during the demo for that, which
[00:30:57.640 --> 00:31:04.280]   we'll do now. So we're just going to follow the MRPC data
[00:31:04.280 --> 00:31:08.640]   set, which all the demos we'll be doing. And we're going to be
[00:31:08.640 --> 00:31:11.840]   installing fast AI here. Now I'm not going to run through most of
[00:31:11.840 --> 00:31:13.960]   this, but I am going to show some explicit things.
[00:31:13.960 --> 00:31:17.880]   Specifically here, I'm installing the GitHub version of
[00:31:17.880 --> 00:31:21.400]   adapt NLP, because all of my code right now is on the dev
[00:31:21.400 --> 00:31:25.080]   branch until we have a next release. And then we also
[00:31:25.080 --> 00:31:29.280]   install my version of NB dev. And I'll show a little more on
[00:31:29.280 --> 00:31:32.800]   why that later, but for now, just know it's NB dev with a few
[00:31:32.800 --> 00:31:36.320]   little bits, you're going to notice that a lot of this code
[00:31:36.320 --> 00:31:39.240]   is going to look very similar to the hugging face tutorial. So
[00:31:39.240 --> 00:31:43.320]   we're going to load the raw data sets. And then we're going to
[00:31:43.320 --> 00:31:48.600]   import the task data set class, we're going to need the name of
[00:31:48.600 --> 00:31:52.920]   our model, we want to use a tokenization function. Now this
[00:31:52.920 --> 00:31:56.880]   tokenization function should take in a row, a tokenizer and
[00:31:56.880 --> 00:32:00.960]   any keyword arguments for the tokenize function are
[00:32:00.960 --> 00:32:04.080]   essentially when we call the tokenizer, you don't need to
[00:32:04.080 --> 00:32:09.240]   worry about tokenizer or the keyword arguments, all you care
[00:32:09.240 --> 00:32:13.240]   about is making sure that what is being passed to tokenize from
[00:32:13.240 --> 00:32:16.640]   this item matches what's need what needs to be essentially
[00:32:16.640 --> 00:32:20.880]   performed. So here we tokenize both sentence one and sentence
[00:32:20.880 --> 00:32:23.960]   two together and join them together. So I'm going to write
[00:32:23.960 --> 00:32:29.400]   my token function as doing exactly that. Next, let's look
[00:32:29.400 --> 00:32:34.560]   at what the task data set API needs. So this is that little
[00:32:34.560 --> 00:32:39.520]   NB dev edition that I mentioned, we actually have function
[00:32:39.520 --> 00:32:43.560]   arguments documented for you for quick use. So we see it wants a
[00:32:43.560 --> 00:32:48.480]   train data set object. And this is the hugging face data sets, a
[00:32:48.480 --> 00:32:52.360]   validation data set, tokenizer name, whether to tokenize it
[00:32:52.360 --> 00:32:56.280]   immediately, that tokenize function I mentioned earlier,
[00:32:57.160 --> 00:33:01.240]   keyword arguments for the tokenizer as well as for the
[00:33:01.240 --> 00:33:06.040]   auto tokenizer itself that constructor, any columns we want
[00:33:06.040 --> 00:33:09.240]   to remove just like how you would have removed calls in
[00:33:09.240 --> 00:33:13.440]   hugging face, as well as that tokenizer name, that's a little
[00:33:13.440 --> 00:33:17.600]   bug, but that's all right. Once we have our data sets, you know,
[00:33:17.600 --> 00:33:20.520]   we'll go ahead and say we want to remove sentence one sentence
[00:33:20.520 --> 00:33:24.280]   two and index just like we did in the tutorial. I said a max
[00:33:24.280 --> 00:33:27.920]   length of 64 just as an example of something you can do as well
[00:33:27.920 --> 00:33:32.640]   as set padding to true. And with everything in place, we just
[00:33:32.640 --> 00:33:36.280]   pass it all in. So we pass in the raw data sets train and
[00:33:36.280 --> 00:33:40.280]   validation, the model name and all of our other arguments we
[00:33:40.280 --> 00:33:44.000]   had earlier. And finally, we need to build some data loaders
[00:33:44.000 --> 00:33:46.720]   because we right now have a perfectly built data set, we
[00:33:46.720 --> 00:33:50.840]   just need to send it off to the data loaders. So similar to how
[00:33:50.840 --> 00:33:55.600]   fast AI has data loaders, we have data loaders here as well,
[00:33:55.600 --> 00:34:01.920]   we pass in a batch size and a collate function. And for that
[00:34:01.920 --> 00:34:04.600]   collate function, we're going to pass in a tokenizer because it's
[00:34:04.600 --> 00:34:08.200]   hugging faces data collater with padding again, just like the
[00:34:08.200 --> 00:34:14.080]   tutorial. And similar to fast AI, we can do show batch. And
[00:34:14.080 --> 00:34:19.720]   so here we see a decoded batch of data with our input text, as
[00:34:19.720 --> 00:34:23.400]   well as the label of zero to one, eventually, I will probably
[00:34:23.400 --> 00:34:26.920]   look towards having this actually decode the values with
[00:34:26.920 --> 00:34:30.000]   some sort of vocabulary. But for right now, this is what the
[00:34:30.000 --> 00:34:34.720]   framework looks like. So we have data, we have built data loaders
[00:34:34.720 --> 00:34:38.080]   from raw hugging face data sets, all that's left is to find two.
[00:34:38.080 --> 00:34:42.400]   So here we have the sequence classification tuner. And I'm
[00:34:42.400 --> 00:34:45.840]   also going to import the strategy class. Now we're going
[00:34:45.840 --> 00:34:50.320]   to keep it at that high level enclosed API I mentioned
[00:34:50.320 --> 00:34:54.680]   earlier. So when we call sequence classification tuner,
[00:34:54.680 --> 00:34:58.000]   you'll notice that it pretty much has the same keyword
[00:34:58.000 --> 00:35:03.440]   arguments as fast AI learner, plus the addition of passing in
[00:35:03.440 --> 00:35:08.280]   our tokenizer and our model name. So here we can do that by
[00:35:08.280 --> 00:35:10.720]   just saying sequence classification tuner, data
[00:35:10.720 --> 00:35:15.040]   loaders, model name, the data sets tokenizer. And then in our
[00:35:15.040 --> 00:35:18.400]   case, because I don't have this automatically done yet, we're
[00:35:18.400 --> 00:35:21.360]   going to pass in the number of classes that we expect our model
[00:35:21.360 --> 00:35:25.640]   to be trained for. So in our case, it's two. Once we've built
[00:35:25.640 --> 00:35:28.320]   our tuner, it's going to go ahead and grab it from
[00:35:28.320 --> 00:35:32.560]   transformers. And as I mentioned before, we only have about four
[00:35:32.560 --> 00:35:36.400]   or five functions available to us right now. And the goal of
[00:35:36.400 --> 00:35:40.520]   that being let's not overcome the user with too many things.
[00:35:40.520 --> 00:35:43.120]   And we can actually see that ourselves. So if we do for
[00:35:43.120 --> 00:35:46.480]   instance, dir, which pulls up all of the attributes, and I
[00:35:46.480 --> 00:35:51.640]   pass in tuner, you'll see that if we go to the non private
[00:35:51.640 --> 00:35:55.720]   bits, there's only a few real attributes that you're able to
[00:35:55.720 --> 00:36:01.560]   see data loaders, export, tune, some things you'll recognize
[00:36:01.560 --> 00:36:04.720]   from the fast AI framework that you probably use constantly, but
[00:36:04.720 --> 00:36:09.280]   nothing else that you particularly don't need. So
[00:36:09.280 --> 00:36:15.000]   what's next? Rather than fast AI's to fit, we have the tune
[00:36:15.000 --> 00:36:18.880]   function instead. Now this is a wrapper that attempts to
[00:36:18.880 --> 00:36:22.600]   essentially combine every single fast AI fit function into an
[00:36:22.600 --> 00:36:26.160]   easy to use API. And this is where that strategy class comes
[00:36:26.160 --> 00:36:31.840]   in. So if I try and do a tab completion on strategy, you'll
[00:36:31.840 --> 00:36:35.760]   notice that we have cosine annealing one cycle and SGDR.
[00:36:35.760 --> 00:36:38.920]   You probably might recognize that those are the fast AI fit
[00:36:38.920 --> 00:36:42.200]   functions. And that's exactly what they do. So when I call
[00:36:42.200 --> 00:36:46.640]   tune, I pass in the number of epochs, the learning rate, same
[00:36:46.640 --> 00:36:51.440]   as you do when you call fit a fitting strategy, such as fit
[00:36:51.440 --> 00:36:56.880]   one cycle or fit any callbacks that you want to add, same as
[00:36:56.880 --> 00:37:00.800]   you would with fast AI's API, and then any other keyword
[00:37:00.800 --> 00:37:04.040]   arguments that might be needed to pass to those specific fit
[00:37:04.040 --> 00:37:11.000]   functions. So to train, it's just tuner dot tune, three epochs
[00:37:11.000 --> 00:37:14.240]   five e neg five, same as the tutorial. And we're going to say
[00:37:14.240 --> 00:37:18.320]   we want to fit with the one cycle policy. And three epochs
[00:37:18.320 --> 00:37:21.640]   later, we're at about 85% accuracy, which is roughly what
[00:37:21.640 --> 00:37:26.160]   you'd expect. So now we have a fully trained model, and we want
[00:37:26.160 --> 00:37:29.920]   to be able to use it. So we're going to first save our model
[00:37:29.920 --> 00:37:37.280]   away via tuner dot save. And what this did is stored away,
[00:37:37.280 --> 00:37:41.720]   essentially a hugging face model for so it's not just the weights,
[00:37:41.720 --> 00:37:47.960]   it's everything hugging face needs to rebuild your model. And
[00:37:47.960 --> 00:37:51.640]   there's a specific reason for that. So let's look at getting
[00:37:51.640 --> 00:37:54.960]   predictions. So there's, there's two different ways for us to get
[00:37:54.960 --> 00:37:57.120]   predictions. If you're using the fast AI framework, you're
[00:37:57.120 --> 00:38:01.360]   probably familiar with the dot predict method. This specifically
[00:38:01.360 --> 00:38:04.320]   is available in the sequence classification tuner class and
[00:38:04.320 --> 00:38:08.040]   any of the tuner classes. So we're going to use one of the
[00:38:08.040 --> 00:38:10.480]   examples, which is these two sentences. So we're going to
[00:38:10.480 --> 00:38:14.160]   combine this into one string of two sentences, which is no
[00:38:14.160 --> 00:38:16.200]   different than what hugging face is really doing under the
[00:38:16.200 --> 00:38:20.160]   surface. And then if we look at the documentation for predict,
[00:38:20.160 --> 00:38:25.080]   we can see that it takes in both a single string and a list of
[00:38:25.080 --> 00:38:29.880]   strings. So we can do batch inference automatically, and
[00:38:29.880 --> 00:38:33.840]   also has this detail level parameter. And so as you can
[00:38:33.840 --> 00:38:36.920]   imagine, you know, this is pretty flexible, because I can
[00:38:36.920 --> 00:38:43.640]   pass in any number of sentences I want, and just get exactly
[00:38:43.640 --> 00:38:49.640]   what I need. And and so if we do predict here, you'll see that we
[00:38:49.640 --> 00:38:55.160]   get label zero, some probabilities, and our input
[00:38:55.160 --> 00:38:59.640]   back. And that doesn't quite look right. Because we don't
[00:38:59.640 --> 00:39:03.960]   have label zero, label one, we need the actual classes. And the
[00:39:03.960 --> 00:39:07.440]   reason for that is we use something called sent, we use
[00:39:07.440 --> 00:39:13.080]   flare underneath the surface for a lot of our bits and pieces. So
[00:39:13.080 --> 00:39:17.800]   for right now, I have this override, a little function here,
[00:39:18.040 --> 00:39:22.080]   or for us to essentially make a class and give it classes like
[00:39:22.080 --> 00:39:26.840]   it's expecting. Eventually, this will be just an API call. And
[00:39:26.840 --> 00:39:31.480]   now, if we do predict, we'll see that it's saying not equivalent.
[00:39:31.480 --> 00:39:37.720]   So that's one way to use the inference API. But what's
[00:39:37.720 --> 00:39:42.520]   another way? So adapt NLP started out as an inference
[00:39:42.520 --> 00:39:46.200]   framework. And so it was a very important to me to keep that
[00:39:46.200 --> 00:39:50.720]   availability there. And that is through those easy modules. So
[00:39:50.720 --> 00:39:54.400]   here we have easy sequence classifier. First, you
[00:39:54.400 --> 00:39:59.360]   construct the classifier. And then you call tag text, which
[00:39:59.360 --> 00:40:03.760]   wants the sentence, the model name or path. And then here, I'm
[00:40:03.760 --> 00:40:07.200]   actually able to pass in the class name. So I'm going to say
[00:40:07.200 --> 00:40:14.160]   equivalent or non equivalent. So it's going to load it and I
[00:40:14.160 --> 00:40:17.000]   know it's not quite 100%. Right, our model might have trained a
[00:40:17.000 --> 00:40:19.480]   little weird this time around, I'll have to look into that
[00:40:19.480 --> 00:40:22.240]   because those should be equivalent. But actually, it
[00:40:22.240 --> 00:40:24.760]   looks like it's just random weights, it didn't actually
[00:40:24.760 --> 00:40:28.840]   train. Wonder if I didn't train it beforehand, which is fine.
[00:40:28.840 --> 00:40:33.120]   But you'll see that, you know, we have exactly what we got back
[00:40:33.120 --> 00:40:36.080]   when we called predict. And so what that means is under the
[00:40:36.080 --> 00:40:41.280]   hood, we're really using the easy classes. As a user, that
[00:40:41.280 --> 00:40:44.680]   doesn't matter. Just know that there's two separate API's if
[00:40:44.680 --> 00:40:47.200]   you just want to do inference. And if you want quick inference
[00:40:47.200 --> 00:40:51.080]   while training. The other thing you might have noticed, though,
[00:40:51.080 --> 00:40:53.840]   is we have this detail level parameter, what does that do?
[00:40:53.840 --> 00:41:01.000]   One of our customers noted that originally, Adapt NLP gave you
[00:41:01.000 --> 00:41:04.600]   too much information. So rather than it just being like
[00:41:04.600 --> 00:41:07.160]   predictions, probabilities, and your original sentences, it
[00:41:07.160 --> 00:41:10.440]   would get back a bunch of nonsense to that usually stemmed
[00:41:10.440 --> 00:41:15.280]   from flair. And so I took the choice of adding in a detail
[00:41:15.280 --> 00:41:18.400]   level. So here just as a quick example, we can see a detail
[00:41:18.400 --> 00:41:21.880]   level low gives us what we had before. Whereas a detail level
[00:41:21.880 --> 00:41:24.640]   of medium and high returns the same thing for sequence
[00:41:24.640 --> 00:41:28.760]   classification specifically, gives us our class vocabulary,
[00:41:28.760 --> 00:41:33.120]   the pairings of sentences to their probabilities, as well as
[00:41:33.120 --> 00:41:38.400]   everything else before. This is just an added little benefit and
[00:41:38.400 --> 00:41:42.120]   cleaner API, in my opinion, so that the user can choose what
[00:41:42.120 --> 00:41:47.560]   they want and don't want to return. But basically, that is
[00:41:47.560 --> 00:41:52.280]   the entire Adapt NLP API. Currently, it only supports
[00:41:52.280 --> 00:41:55.640]   sequence classification. And really, I'd recommend using this
[00:41:55.640 --> 00:42:00.880]   task data set, hybrid API, rather than the boilerplates
[00:42:00.880 --> 00:42:03.600]   right now, just because it's extremely unstable. And like,
[00:42:03.600 --> 00:42:06.040]   literally next week, I'm wrapping up sequence
[00:42:06.040 --> 00:42:10.160]   classification and going into token tagging. But that's it
[00:42:10.160 --> 00:42:14.080]   for my demo. So thank you, Wade, for inviting me on. And I guess
[00:42:14.080 --> 00:42:15.040]   we'll take questions.
[00:42:15.040 --> 00:42:21.120]   Thanks, Professor. There's a question when you're returning
[00:42:21.120 --> 00:42:24.640]   data loaders, are you returning fast data loaders or PyTorch
[00:42:24.640 --> 00:42:26.040]   data loaders?
[00:42:26.040 --> 00:42:29.480]   Ah, that is an excellent question. The answer is
[00:42:29.480 --> 00:42:38.000]   neither. We're actually returning adaptive data loaders.
[00:42:38.000 --> 00:42:43.680]   Now, all that that is, if it lets me pull up the source code,
[00:42:43.680 --> 00:42:49.920]   is a very quick wrapper around FastAIs data loaders that stores
[00:42:49.920 --> 00:42:54.160]   the tokenizers and has a custom show batch. I didn't really feel
[00:42:54.160 --> 00:42:58.760]   a need to have everything FastAIs data loaders have. And
[00:42:58.760 --> 00:43:01.840]   so I'm choosing an option of only implementing what's
[00:43:01.840 --> 00:43:05.400]   actually needed. And so in our case, right now, that is show
[00:43:05.400 --> 00:43:05.960]   batch.
[00:43:05.960 --> 00:43:13.440]   Awesome. Yep, there are no other questions, but I just want to
[00:43:13.440 --> 00:43:16.880]   point out that it also maybe it's just my bias, but it feels
[00:43:16.880 --> 00:43:19.480]   like all of these frameworks are really good for also just like
[00:43:19.480 --> 00:43:26.280]   the top down approach as we call it. So that's really awesome. I
[00:43:26.280 --> 00:43:28.040]   don't see any other questions coming in.
[00:43:28.560 --> 00:43:32.080]   Sanyam, I really like how you snuck in Colin Zak professor
[00:43:32.080 --> 00:43:35.720]   again, you know that gets to him. And I love how you just got
[00:43:35.720 --> 00:43:39.160]   that in there without any hesitation or anything.
[00:43:39.160 --> 00:43:42.920]   I don't fight it anymore. It's not worth it.
[00:43:42.920 --> 00:43:46.760]   You could be called worse than that. So you know,
[00:43:46.760 --> 00:43:50.160]   you have to accept it now.
[00:43:50.160 --> 00:43:53.440]   Ah, I'll at least tolerate it. Let's go with that.
[00:43:53.440 --> 00:43:57.520]   All right, Arto, are you ready?
[00:43:58.520 --> 00:43:59.080]   Yeah.
[00:43:59.080 --> 00:44:00.640]   All right.
[00:44:00.640 --> 00:44:02.960]   Let me share my screen.
[00:44:02.960 --> 00:44:07.040]   Go.
[00:44:07.040 --> 00:44:10.760]   Okay.
[00:44:10.760 --> 00:44:13.200]   Can you see my screen?
[00:44:13.200 --> 00:44:14.280]   Yes.
[00:44:14.280 --> 00:44:23.960]   Cool. So I will be presenting fast hugs. It's a small wrapper
[00:44:23.960 --> 00:44:30.720]   library very similar to blur, and actually highly inspired by
[00:44:30.720 --> 00:44:37.160]   blur. And it's designed to allow training, hug and face, pre
[00:44:37.160 --> 00:44:39.560]   trained models with fast AI training.
[00:44:39.560 --> 00:44:45.680]   Okay, and as you will see through my notebook, it's like
[00:44:45.680 --> 00:44:53.320]   really very similar to blur in many design choices. So let's
[00:44:53.320 --> 00:45:01.240]   go to those. Yeah, so the main goals I had in mind when
[00:45:01.240 --> 00:45:07.320]   creating fast hugs. So I wanted to have this integrated with
[00:45:07.320 --> 00:45:13.480]   hug and face data sets, because they come with a lot of very
[00:45:13.480 --> 00:45:19.800]   useful functionality so they rely on by arrow tables, which
[00:45:19.840 --> 00:45:24.000]   allow and allow you like not to load all the data into memory
[00:45:24.000 --> 00:45:32.960]   but have data on your hard drive and dynamically load that for
[00:45:32.960 --> 00:45:41.680]   training. And yeah, the data sets libraries evolving quite
[00:45:41.680 --> 00:45:47.880]   fast and it has a lot of cool stuff there now they support
[00:45:48.440 --> 00:45:53.240]   streaming data sets which basically allows you not to
[00:45:53.240 --> 00:45:57.040]   download all the data to your hard drive. And when you deal
[00:45:57.040 --> 00:46:01.880]   with some huge data set, like for pre training language model
[00:46:01.880 --> 00:46:08.520]   it might be like a couple hundreds gigabytes and is hugging
[00:46:08.520 --> 00:46:15.360]   face data sets you can just like iterate through the data, not
[00:46:15.400 --> 00:46:21.800]   loading everything down. And for example, in this way you can
[00:46:21.800 --> 00:46:29.440]   handle large data sets with with call up, which it has some like
[00:46:29.440 --> 00:46:33.440]   memory restrictions but with a streaming module can do that. So
[00:46:33.440 --> 00:46:37.760]   I wanted to have this all the data sets functionality
[00:46:37.760 --> 00:46:44.120]   available. The next scene I wanted a familiar fast AI
[00:46:44.120 --> 00:46:52.160]   interface and what I use I'm using data block API and learner
[00:46:52.160 --> 00:46:58.240]   for training and callbacks for tweaking the training loop. And
[00:46:58.240 --> 00:47:05.960]   like my main goal was to allow for quick experimentation. So
[00:47:07.520 --> 00:47:15.680]   these transformers trainer API is really great and useful it
[00:47:15.680 --> 00:47:22.960]   comes with a lot of built in support for like data parallel
[00:47:22.960 --> 00:47:27.240]   training model parallel training with deep speed and all the good
[00:47:27.240 --> 00:47:32.560]   stuff. But like if you want to do some quick experiments, it
[00:47:32.560 --> 00:47:40.280]   might be a bit more difficult to use trainer because you need to
[00:47:40.280 --> 00:47:45.640]   basically customize a training training step. And yeah, the
[00:47:45.640 --> 00:47:53.040]   same with writing your own training loop, right and fast AI
[00:47:53.040 --> 00:47:58.120]   allows allows to tweak your training loop with callbacks.
[00:47:58.480 --> 00:48:06.080]   And by doing that, it really facilitates like quick
[00:48:06.080 --> 00:48:10.760]   experiments. And that's what I wanted from this library as
[00:48:10.760 --> 00:48:19.440]   well. So the main components of the library are like, it relies
[00:48:19.440 --> 00:48:25.000]   on transforms for processing data. And I have a bunch of
[00:48:25.000 --> 00:48:29.360]   this different transforms which are applicable in different
[00:48:29.360 --> 00:48:34.680]   situations. And like examples are like talk transform, which
[00:48:34.680 --> 00:48:40.840]   will tokenize a single text or or talk batch transform, which
[00:48:40.840 --> 00:48:46.760]   will tokenize batch of text. And for example, you might get from
[00:48:46.760 --> 00:48:51.600]   your like you can pre process the data in data sets, and you
[00:48:51.640 --> 00:48:56.640]   might need only to pad your batches. And for that we have
[00:48:56.640 --> 00:49:00.720]   another transform and there is a bunch of other transforms which
[00:49:00.720 --> 00:49:06.880]   are specific for some other tasks, sequence classification.
[00:49:06.880 --> 00:49:13.080]   So you can check out that in the documentation for fast hacks.
[00:49:13.080 --> 00:49:21.000]   The next thing I'm using data block API to create the data
[00:49:21.000 --> 00:49:27.320]   pipeline. And I have some customized data blocks, which
[00:49:27.320 --> 00:49:32.440]   were which uses transforms mentioned about and put that all
[00:49:32.440 --> 00:49:36.760]   together to allow like quick definition of your data
[00:49:36.760 --> 00:49:46.360]   pipeline. And next, I'm using like a subclass of learner from
[00:49:46.400 --> 00:49:51.000]   fast AI, which has some like transformer specific stuff. And
[00:49:51.000 --> 00:49:59.560]   most of the work is done by this callback which handles like
[00:49:59.560 --> 00:50:06.000]   putting batches of data into proper format for for the
[00:50:06.000 --> 00:50:11.040]   hugging face models. And we can do a lot of stuff with callbacks
[00:50:11.040 --> 00:50:15.320]   as well. Like, for example, you might want to generate your
[00:50:15.320 --> 00:50:20.720]   predictions or others and like for autoregressive models and
[00:50:20.720 --> 00:50:27.280]   you can do that with callback easily. Also, you can use like
[00:50:27.280 --> 00:50:32.760]   metrics from hugging face data sets library and you can do is
[00:50:32.760 --> 00:50:36.320]   that with callbacks as well and you can probably do whatever you
[00:50:36.320 --> 00:50:45.600]   want with callback. Yeah, so now let's get to the notebook,
[00:50:45.600 --> 00:50:55.000]   actually. So I'm running this notebook on collab and I can post
[00:50:55.000 --> 00:51:01.840]   the link in the chat. If you maybe would like to follow along
[00:51:01.880 --> 00:51:07.120]   you can run it live and go through the notebook with me.
[00:51:07.120 --> 00:51:16.400]   Okay, so yeah, first, what we need is to install the libraries
[00:51:16.400 --> 00:51:24.240]   here. And we installed transformers data sets, WNB and
[00:51:24.240 --> 00:51:28.480]   TQDM. And yeah, I'm in as well as Zach like because the
[00:51:28.480 --> 00:51:35.840]   library is kind of changing. And that's why you probably want to
[00:51:35.840 --> 00:51:50.080]   install the version from GitHub. Yeah. So let's go to the MRPC
[00:51:50.080 --> 00:51:54.360]   data set. It stands for Microsoft Research Paraphrase
[00:51:54.360 --> 00:51:59.400]   Corpus and it's part of the glue benchmark. This glue benchmark
[00:51:59.400 --> 00:52:04.600]   is one of the most widely used benchmark for measuring the
[00:52:04.600 --> 00:52:09.240]   performance of language models. And it essentially contains a
[00:52:09.240 --> 00:52:18.240]   lot of sequence classification tasks as well as one regression
[00:52:18.240 --> 00:52:30.760]   task. And in particular, MRPC data set, it contains like pairs
[00:52:30.760 --> 00:52:38.200]   of sentences, and your task is to output a label for this pair
[00:52:38.200 --> 00:52:41.280]   of sentences, whether the sentences are equivalent or not.
[00:52:41.760 --> 00:52:48.240]   And yeah, and the data set is pretty small, it contains below
[00:52:48.240 --> 00:52:56.040]   for 1000 samples. So that's something to keep in mind when
[00:52:56.040 --> 00:53:02.960]   working, working with it. Okay, so now to the data block
[00:53:02.960 --> 00:53:07.600]   definition. To create data loaders, we are using here data
[00:53:07.600 --> 00:53:13.280]   block API. And here we have two data blocks, one for the inputs
[00:53:13.280 --> 00:53:19.160]   and one for labels. And the one for inputs is transformers text
[00:53:19.160 --> 00:53:26.520]   block. And it is initialized to given a model name. And that is
[00:53:26.520 --> 00:53:34.800]   done to let the data block know which tokenizer to use. And
[00:53:37.320 --> 00:53:44.320]   this data block for labels is a slightly customized category
[00:53:44.320 --> 00:53:50.600]   block from FastAI, which accepts like the label names, because
[00:53:50.600 --> 00:53:57.560]   FastAI data set already has this names for the label we have, we
[00:53:57.560 --> 00:54:01.320]   have two labels, and one is not equivalent, another is
[00:54:01.320 --> 00:54:07.320]   equivalent, and we want to use that from the data set. Okay,
[00:54:07.320 --> 00:54:13.440]   and yeah, basically, if you are familiar with FastAI, you should
[00:54:13.440 --> 00:54:23.600]   recognize this piece of code, we define we are defining this data
[00:54:23.600 --> 00:54:30.920]   block, providing some blocks and also getters for x and y, in our
[00:54:30.920 --> 00:54:36.320]   case, we have sentence one and sentence two for x and label for
[00:54:36.320 --> 00:54:43.480]   y. And also we provide those splits, which shows a data
[00:54:43.480 --> 00:54:47.920]   loader, what data is used for validation. And we get those
[00:54:47.920 --> 00:54:55.840]   splits, just with here with this helper function, which basically
[00:54:55.840 --> 00:55:02.840]   uses the names of the data sets within the data set that data
[00:55:02.840 --> 00:55:09.200]   set dict provided by like loaded from the data sets library. Okay,
[00:55:09.200 --> 00:55:19.240]   and then we construct the data loaders, just as done in FastAI
[00:55:19.240 --> 00:55:23.800]   and in blur as well, you can see that this part is very similar.
[00:55:24.760 --> 00:55:32.000]   And we can use show batch to actually visualize our inputs,
[00:55:32.000 --> 00:55:36.840]   it's always a good idea to keep an eye on what what are you
[00:55:36.840 --> 00:55:41.160]   training on. And here you can see that we have two sentences
[00:55:41.160 --> 00:55:48.080]   and the label for this sentence pair. And I will be using WNB
[00:55:48.080 --> 00:55:55.120]   for experiment tracking, it's really nice to have all the
[00:55:55.120 --> 00:56:02.400]   experiment related information stored in one place. And here we
[00:56:02.400 --> 00:56:07.160]   define some variables, which are basically constructed here I
[00:56:07.160 --> 00:56:13.280]   provide like I will tag the run with data set name is a task and
[00:56:13.280 --> 00:56:19.520]   the model I use. And also I will group this by learner rate, so
[00:56:19.520 --> 00:56:26.400]   that I can post analyze all the stuff and compare different type
[00:56:26.400 --> 00:56:36.280]   of parameters afterwards. And you can check this run by going
[00:56:36.280 --> 00:56:43.400]   to the weights and biases. And you'll see the training loss is
[00:56:43.400 --> 00:56:46.920]   decreasing nicely through the training, but the validation
[00:56:46.920 --> 00:56:51.680]   loss, it kind of goes down and then goes up again. So we have
[00:56:51.680 --> 00:56:59.960]   some overfitting here. Yeah. Okay, now let's look to the
[00:56:59.960 --> 00:57:05.720]   training. And the training is, again, very similar to what what
[00:57:05.720 --> 00:57:14.120]   you do with fast.ai. Usually, you just get the model from
[00:57:14.120 --> 00:57:21.200]   Hug and Face model hub. And the model is defined by a name. In
[00:57:21.200 --> 00:57:26.720]   this case, I'm using Roberta Bayes model. And yeah, usually
[00:57:26.720 --> 00:57:35.520]   it's always like, unless you're doing some benchmarking, you
[00:57:35.520 --> 00:57:39.920]   probably don't want to use the vanilla BERT. But you would like
[00:57:39.920 --> 00:57:46.080]   to use some of the variations of this model, which grant better
[00:57:46.080 --> 00:57:52.800]   performance. And it's very similar to the BERT but like
[00:57:52.800 --> 00:57:58.920]   trained on more data. That's Roberta. Okay. And yeah, we
[00:57:58.920 --> 00:58:04.320]   define the metrics we want to use for the MRPC. We use F1
[00:58:04.320 --> 00:58:08.880]   score and accuracy to measure the performance. And then we
[00:58:08.880 --> 00:58:18.640]   define a learner and use FP16 to speed things up and save some
[00:58:18.640 --> 00:58:24.680]   memory on GPU. And you can check the training loop. Basically,
[00:58:24.680 --> 00:58:29.160]   this trans learner I use here, it adds this trans callback,
[00:58:30.120 --> 00:58:36.640]   which is responsible for basically the communication
[00:58:36.640 --> 00:58:49.240]   between the data loaders we have and the model. So with that, we
[00:58:49.240 --> 00:58:55.720]   can run the training. And we can add the weights and biases
[00:58:55.720 --> 00:59:01.560]   callback and save model callback to the save model callback will
[00:59:01.560 --> 00:59:09.160]   store the best model we had during training by using the
[00:59:09.160 --> 00:59:15.000]   metrics we choose. And weights and biases callback basically
[00:59:15.000 --> 00:59:20.760]   logs the data to the weights and biases. And as you can see,
[00:59:20.760 --> 00:59:29.480]   this was used by weight as well. So yeah, and after the model is
[00:59:29.480 --> 00:59:38.360]   trained, you can call show results method to check what
[00:59:38.360 --> 00:59:41.920]   predictions are correct and what's what's you're seeking we
[00:59:41.920 --> 00:59:48.120]   have here some incorrect prediction. So it was supposed
[00:59:48.120 --> 00:59:53.680]   to be equivalent but the model decided it's not so yeah, it
[00:59:53.680 --> 00:59:57.400]   makes some mistakes. And after the model is trained, you
[00:59:57.400 --> 01:00:02.040]   probably want to get some predictions and it can be done
[01:00:02.040 --> 01:00:07.840]   easily by constructing the test data loader from the data set we
[01:00:07.840 --> 01:00:16.200]   downloaded and yeah, calling the get breadth message of the
[01:00:16.200 --> 01:00:22.800]   learner. Okay, and to wrap this up, I wanted to speak a bit how
[01:00:22.800 --> 01:00:26.640]   to improve the model performance. Like one thing you
[01:00:26.640 --> 01:00:32.760]   can do you can experiment with the backbone model you use. And
[01:00:32.760 --> 01:00:37.320]   some popular choices are Roberta, the ones that I used,
[01:00:37.320 --> 01:00:44.400]   there is Albert, which you use some smart parameter sharing and
[01:00:44.400 --> 01:00:53.720]   like shows good results on many tasks. There is Electra, which
[01:00:53.720 --> 01:00:59.960]   has like modified as a pre training objective, and also has
[01:00:59.960 --> 01:01:05.800]   very competitive competitive performance. And in this example,
[01:01:05.800 --> 01:01:11.200]   we will look how to use the deburter. It used disentangled
[01:01:11.240 --> 01:01:15.600]   attention. And basically, this was the first model to surpass
[01:01:15.600 --> 01:01:20.200]   the human performance on super glue benchmark, which is kind of
[01:01:20.200 --> 01:01:28.320]   more difficult version of glue, which we are using here. And as
[01:01:28.320 --> 01:01:33.880]   you can see, like, trying another model is as easy as just
[01:01:33.880 --> 01:01:39.120]   changing the model name here and everything afterwards is just
[01:01:39.600 --> 01:01:45.600]   the same. So we can start another round with this
[01:01:45.600 --> 01:01:52.840]   different model and see if it performs better. And yeah, once
[01:01:52.840 --> 01:02:02.040]   you do that, like you probably want to get some understanding
[01:02:02.040 --> 01:02:08.320]   if you if you like utilize your model as the best way you can.
[01:02:08.760 --> 01:02:13.560]   And for that, you can do some hyper parameter search. And this
[01:02:13.560 --> 01:02:19.320]   is really easy with weights and biases. So weights and biases
[01:02:19.320 --> 01:02:25.560]   has this sweeps functionality. And to use that, you basically
[01:02:25.560 --> 01:02:32.280]   need two things, you need to define a training function. And
[01:02:32.280 --> 01:02:37.880]   you can do you need to state the configuration for sweep, sweep
[01:02:37.880 --> 01:02:43.520]   agent, basically provide a range of hyper parameters for the sweep
[01:02:43.520 --> 01:02:50.600]   engine to use to start the runs. And here, you can see this train
[01:02:50.600 --> 01:02:56.320]   function, it just wraps all the code we had before into like a
[01:02:56.320 --> 01:03:02.280]   single function, it will, and the sweep agent can launch this
[01:03:02.280 --> 01:03:06.520]   function. And basically, this will do a one round of fine
[01:03:06.520 --> 01:03:15.840]   tuning. Like, like here. And yeah, we can define some ranges
[01:03:15.840 --> 01:03:22.920]   of parameters to use for the model. And say we want to do a
[01:03:22.920 --> 01:03:31.320]   grid search and inspect a bunch of learning rates for our model
[01:03:31.640 --> 01:03:40.080]   and values for weight decay. And maybe we want to use some kind
[01:03:40.080 --> 01:03:45.640]   of long learning rate decay with model depth. So that will, if
[01:03:45.640 --> 01:03:48.800]   you are familiar with fastair, you probably know that fastair
[01:03:48.800 --> 01:03:54.600]   often uses this differential learning rates, basically having
[01:03:54.600 --> 01:03:58.640]   higher learning rate for the lighter layers and lower learning
[01:03:58.640 --> 01:04:06.840]   rate for the deeper layers. And we can do that. Here, for that
[01:04:06.840 --> 01:04:13.200]   you need to split the models, model parameters by layers into
[01:04:13.200 --> 01:04:18.560]   groups. And here is a function to do that. And you can
[01:04:18.560 --> 01:04:25.800]   basically define this learning rate decay by some factor with
[01:04:25.800 --> 01:04:30.640]   which learning rate gets lower as we go deeper into the model.
[01:04:30.640 --> 01:04:37.160]   And, yeah, there are some controversy if this really
[01:04:37.160 --> 01:04:43.160]   helps or not with transformers. And like some papers use that
[01:04:43.160 --> 01:04:49.600]   and find it helpful, other papers don't. And actually, you
[01:04:49.600 --> 01:04:54.400]   can just try it for yourself and see if it helps with your tasks.
[01:04:55.040 --> 01:05:01.360]   And luckily, the MRPC dataset is quite small and you can run
[01:05:01.360 --> 01:05:10.400]   this sweeps quite fast for yourself. So here, like we
[01:05:10.400 --> 01:05:14.560]   defined this sweep configuration, it has the project
[01:05:14.560 --> 01:05:22.280]   name, entity name, like a name for the sweep, a message to use
[01:05:22.280 --> 01:05:25.560]   for hyperparameter search and the ranges of the hyper
[01:05:25.560 --> 01:05:32.000]   parameters. And then you just initialize this sweep and you
[01:05:32.000 --> 01:05:40.320]   run the agents by running this command from bytes and biases.
[01:05:40.320 --> 01:05:44.240]   And it's nice, you can run multiple agents if you have
[01:05:44.440 --> 01:05:52.360]   like access to more than one GPU you want to use, you can launch
[01:05:52.360 --> 01:05:58.480]   this training runs concurrently and the bytes and biases agent
[01:05:58.480 --> 01:06:04.440]   will manage all this for you. Okay, and that's really nice for
[01:06:04.440 --> 01:06:13.720]   hyperparameter tuning. And yeah, let me briefly show you
[01:06:14.720 --> 01:06:26.840]   the example of what this actually outputs. So here is
[01:06:26.840 --> 01:06:31.640]   some sweep I ran with the DeBerto model. And basically,
[01:06:31.640 --> 01:06:39.160]   you can have a look at different hyperparameters you used. And
[01:06:40.880 --> 01:06:46.960]   this set of hyperparameters is linked to some performance. So
[01:06:46.960 --> 01:06:53.680]   here you can see that some of the best runs were with higher
[01:06:53.680 --> 01:07:00.720]   learning rate and zero weight decay. Yeah, but you can run
[01:07:00.720 --> 01:07:05.040]   this for more combinations of hyperparameters and maybe you
[01:07:05.040 --> 01:07:10.560]   can get even better score here. So that's a really nice way to
[01:07:10.560 --> 01:07:15.360]   automatically search for the best set of hyperparameters you
[01:07:15.360 --> 01:07:25.600]   can use. Okay, so I think with that, I'm done with my demo and
[01:07:25.600 --> 01:07:29.800]   you can play with this notebook. And yeah, let's get to the
[01:07:29.800 --> 01:07:30.480]   questions.
[01:07:30.480 --> 01:07:37.040]   I think there's just a general appreciation of the framework
[01:07:37.040 --> 01:07:40.400]   and also how nice it is that you can track all of this very
[01:07:40.400 --> 01:07:44.920]   conveniently. And for me, I was also really happy to see you for
[01:07:44.920 --> 01:07:48.440]   the first time I've heard Wade and Zach earlier, but it was
[01:07:48.440 --> 01:07:52.320]   really awesome to see you. And just scrolling through and
[01:07:52.320 --> 01:07:57.440]   picking the questions. So how did you learn that DeBerto was
[01:07:57.440 --> 01:08:00.160]   the first model to surpass human performance?
[01:08:02.400 --> 01:08:07.720]   Well, that was in the paper. So they actually advertise it as
[01:08:07.720 --> 01:08:12.400]   one that it was first to surpass human performance on that
[01:08:12.400 --> 01:08:20.360]   particular benchmark. And a thing to note here, it's not
[01:08:20.360 --> 01:08:25.800]   this base model we are using here, but the largest one was
[01:08:25.800 --> 01:08:29.280]   the one to surpass human performance, which is kind of
[01:08:30.360 --> 01:08:31.280]   much larger.
[01:08:31.280 --> 01:08:40.680]   I don't see any other questions. But again, I think
[01:08:40.680 --> 01:08:42.840]   all of these presentations were really awesome. They were like
[01:08:42.840 --> 01:08:46.920]   very fast AIE and yet also similar and different. So that
[01:08:46.920 --> 01:08:48.960]   was really awesome. Thanks, everyone.
[01:08:48.960 --> 01:08:57.160]   Thanks for having me here. And thanks to Wade for starting
[01:08:57.160 --> 01:08:59.480]   this great learning group.
[01:08:59.800 --> 01:09:03.080]   So useful for everyone involved.
[01:09:03.080 --> 01:09:07.280]   Absolutely. Like the timing was perfect, because I like started
[01:09:07.280 --> 01:09:13.040]   this rework of adapt NLP, maybe two months before Wade started
[01:09:13.040 --> 01:09:15.440]   doing this thing. So it really made sure that I could that I
[01:09:15.440 --> 01:09:16.400]   put things into gear.
[01:09:16.400 --> 01:09:19.280]   Yeah, I got you motivated over there.
[01:09:19.280 --> 01:09:22.480]   Something like that motivated, terrified. I don't know. Take
[01:09:22.480 --> 01:09:23.000]   your poison.
[01:09:23.000 --> 01:09:27.120]   Anyways, yeah, great demos. Thank you. I think this is the
[01:09:27.120 --> 01:09:32.120]   first time I've talked to you as well, Arto. Correct? Like, kind
[01:09:32.120 --> 01:09:34.360]   of in person, virtually.
[01:09:34.360 --> 01:09:38.720]   Happy to meet you all as well.
[01:09:38.720 --> 01:09:44.520]   That was awesome. Um, anyways, okay, gonna wrap things up
[01:09:44.520 --> 01:09:47.400]   here. I talked to Andrea, I'm like, I think we might go long.
[01:09:47.400 --> 01:09:50.840]   She's like, I give you I gave you guys another 45 minutes. I'm
[01:09:50.840 --> 01:09:54.560]   like, obviously, she knows fast AI folks, we typically go long.
[01:09:54.640 --> 01:09:57.680]   If you've been in the course, you know, like, it's could go two
[01:09:57.680 --> 01:10:01.720]   and a half hours, it could go four hours. But let's just wrap
[01:10:01.720 --> 01:10:07.600]   things up. So great demos, encourage everybody to look at
[01:10:07.600 --> 01:10:11.000]   the documentation for each of the libraries, play around with
[01:10:11.000 --> 01:10:15.640]   them, and dig into them. And, and like Zach was mentioning, I
[01:10:15.640 --> 01:10:18.160]   think Arto mentioned, I mentioned, you will see the
[01:10:18.160 --> 01:10:21.000]   centrality of the fast AI callback system and how
[01:10:21.000 --> 01:10:23.920]   important it is to the development of these libraries,
[01:10:23.920 --> 01:10:30.680]   and other extensions to fast AI. So for homework, watch the
[01:10:30.680 --> 01:10:33.360]   official course videos from week three. Again, you're gonna learn
[01:10:33.360 --> 01:10:36.320]   about the trainer how to do things with the rock, the pipe
[01:10:36.320 --> 01:10:40.640]   torch, training and validation loop, and also how to work with
[01:10:40.640 --> 01:10:44.560]   accelerate. And as promised, we're going to have some
[01:10:44.560 --> 01:10:48.320]   competitions and demonstrations for you all to participate in,
[01:10:48.560 --> 01:10:54.360]   in a few weeks. And here they are, we have three winners per
[01:10:54.360 --> 01:11:01.240]   competition, you can submit to one, two, or all three of these,
[01:11:01.240 --> 01:11:05.880]   you can submit multiple times. And we got some swag from weights
[01:11:05.880 --> 01:11:09.520]   and biases in the form of t shirts for you to go around and
[01:11:09.520 --> 01:11:12.800]   brag that you were one of the winners of the hugging phase
[01:11:12.800 --> 01:11:17.880]   fast AI study group. And so here are the three competitions.
[01:11:18.440 --> 01:11:22.840]   That we're hoping folks get involved with. One is best blog
[01:11:22.840 --> 01:11:26.960]   post on training transformers with fast AI. And just submit
[01:11:26.960 --> 01:11:32.760]   your links to your blogs to my email. And again, the prizes for
[01:11:32.760 --> 01:11:35.520]   all these are going to be the top three is our weights of
[01:11:35.520 --> 01:11:41.720]   bias t shirts. The second competition is basically taking
[01:11:41.720 --> 01:11:44.320]   what you've learned and either using one of the libraries or
[01:11:44.320 --> 01:11:49.760]   coding your own thing and using fast AI to train a model on the
[01:11:49.760 --> 01:11:57.800]   sst to data set. And this will be very familiar to folks. It
[01:11:57.800 --> 01:12:01.720]   works a lot like MRTC, except there's just one sentence. And
[01:12:01.720 --> 01:12:05.280]   let's see what you all come with. And also no cheating. So
[01:12:05.280 --> 01:12:09.640]   use the training and validation set. If I find out you use the
[01:12:09.640 --> 01:12:15.200]   test set, I will send Sanyam to come over there and disconnect
[01:12:15.200 --> 01:12:19.560]   your zoom. So please use a training validation just like
[01:12:19.560 --> 01:12:23.440]   you normally would. We'll evaluate this and we'll announce
[01:12:23.440 --> 01:12:29.800]   the top three if we get at least three in a few weeks. And
[01:12:29.800 --> 01:12:33.520]   lastly, most interesting application of a fast AI trained
[01:12:33.520 --> 01:12:38.520]   transformer. One of the toughest things I've encountered as a
[01:12:38.520 --> 01:12:41.560]   software developer is just deploying something. It's hard.
[01:12:41.560 --> 01:12:44.400]   And everybody has a challenge with this. And especially ml
[01:12:44.400 --> 01:12:47.480]   folks, we know how to train models and build things in
[01:12:47.480 --> 01:12:50.760]   Jupyter Notebook. But can we actually deploy that so that
[01:12:50.760 --> 01:12:55.520]   it's useful to an end user. And so for this competition, looking
[01:12:55.520 --> 01:12:58.320]   for folks who have something and it could be something really
[01:12:58.320 --> 01:13:02.320]   simple. But that shows a demonstration of how a train
[01:13:02.320 --> 01:13:05.240]   transformer can be used. And hopefully you guys will be
[01:13:05.240 --> 01:13:11.880]   creative with that. All three of these are due by August 3 12pm
[01:13:11.880 --> 01:13:17.560]   Pacific time. So send me your links, send me your gist. And
[01:13:17.560 --> 01:13:23.520]   we'll have a demo day. And I've always wanted to say this, a big
[01:13:23.520 --> 01:13:26.720]   fan of watching Steve Jobs's presentations, he always had
[01:13:26.720 --> 01:13:31.040]   that one more thing. So I have one more thing. And that is that
[01:13:31.040 --> 01:13:35.240]   there will be a week five. So next week, we're going to cover
[01:13:35.240 --> 01:13:39.200]   section four, week five, we're actually going to do a general
[01:13:39.200 --> 01:13:43.160]   q&a. So bring all your questions. And we're going to
[01:13:43.160 --> 01:13:46.000]   give competition winners the opportunity to present their
[01:13:46.000 --> 01:13:50.560]   work if they choose. We're going to provide our feedback and go
[01:13:50.560 --> 01:13:54.000]   from there. The zoom link will be different. So you'll get a
[01:13:54.000 --> 01:13:58.440]   different zoom link for week five. But hopefully we'll see
[01:13:58.440 --> 01:14:02.120]   everybody there and looking forward to kind of seeing what
[01:14:02.120 --> 01:14:06.160]   the community submits in terms of those competitions and
[01:14:06.160 --> 01:14:12.320]   looking at others works. And that's it for me. Any final
[01:14:12.320 --> 01:14:14.360]   questions, thoughts?
[01:14:14.360 --> 01:14:21.760]   I'm just looking forward to reading all of the posts and
[01:14:21.760 --> 01:14:25.440]   solutions that come up. I'm just here to learn it's my greedy way
[01:14:25.440 --> 01:14:28.480]   of learning by being a co host. Although I'm really not doing
[01:14:28.480 --> 01:14:32.200]   anything, but I'm, I'm really learning a lot. So please keep
[01:14:32.200 --> 01:14:35.480]   the submissions coming in. And I look forward to learning even
[01:14:35.480 --> 01:14:39.120]   more from them. And thanks to everyone for joining.
[01:14:39.120 --> 01:14:40.640]   Thank you.
[01:14:40.640 --> 01:14:43.640]   >> Thank you.

