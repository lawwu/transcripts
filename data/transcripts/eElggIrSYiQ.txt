
[00:00:00.000 --> 00:00:03.400]   [MUSIC PLAYING]
[00:00:03.400 --> 00:00:11.800]   Hello, my name is Stella Biederman.
[00:00:11.800 --> 00:00:14.840]   I am the executive director of Luther.ai
[00:00:14.840 --> 00:00:17.560]   and head of interpretability research.
[00:00:17.560 --> 00:00:20.280]   And I'm here to talk about how interpretability research can
[00:00:20.280 --> 00:00:22.960]   help us build better models, especially
[00:00:22.960 --> 00:00:24.200]   large language models.
[00:00:24.200 --> 00:00:25.880]   A little bit about our organization,
[00:00:25.880 --> 00:00:28.560]   Luther.ai is a nonprofit research institute
[00:00:28.560 --> 00:00:31.320]   that focuses on large-scale AI research.
[00:00:31.320 --> 00:00:33.880]   We're best known for open source work,
[00:00:33.880 --> 00:00:36.600]   such as the Dataset to Pile, large language
[00:00:36.600 --> 00:00:41.320]   models such as GTPJ, GTP NeoX, and the VQBN Clip text
[00:00:41.320 --> 00:00:43.160]   to image synthesis technique.
[00:00:43.160 --> 00:00:46.320]   We also do a lot of work beyond natural language processing
[00:00:46.320 --> 00:00:49.720]   on interpretability, alignment, and applications
[00:00:49.720 --> 00:00:52.000]   of large-scale AI to other domains,
[00:00:52.000 --> 00:00:54.480]   such as text to image, like I mentioned,
[00:00:54.480 --> 00:00:56.240]   or medical processing.
[00:00:56.240 --> 00:00:59.440]   We care a lot about the scientific community
[00:00:59.440 --> 00:01:03.000]   and promoting the ability to do research on large-scale AI
[00:01:03.000 --> 00:01:05.560]   models around the world, even if you don't necessarily
[00:01:05.560 --> 00:01:09.000]   have the computational resources to train them yourselves.
[00:01:09.000 --> 00:01:11.360]   And that is the major drive of why
[00:01:11.360 --> 00:01:16.680]   we are bullish on public release and giving people more access
[00:01:16.680 --> 00:01:17.800]   to language models.
[00:01:17.800 --> 00:01:19.960]   There's a couple of different types of AI interpretability
[00:01:19.960 --> 00:01:20.460]   research.
[00:01:20.460 --> 00:01:22.440]   It's a rather broad field.
[00:01:22.440 --> 00:01:24.240]   Some people do research in what's
[00:01:24.240 --> 00:01:26.880]   called post-hoc explanations, where they take a pre-trained
[00:01:26.880 --> 00:01:32.320]   model and they try to explain why it predicted the outcome
[00:01:32.320 --> 00:01:35.560]   or why it produced the generation that it did.
[00:01:35.560 --> 00:01:39.280]   There are techniques on developing simpler but easier
[00:01:39.280 --> 00:01:43.720]   to interpret models that mimic the behavior of more
[00:01:43.720 --> 00:01:45.200]   complicated AI models.
[00:01:45.200 --> 00:01:47.800]   You can basically think of these as fancy linear regressions,
[00:01:47.800 --> 00:01:50.560]   where a linear model is not a phenomenal model
[00:01:50.560 --> 00:01:53.640]   for the phenomenon in question, but it's
[00:01:53.640 --> 00:01:55.080]   much easier to interpret and you can
[00:01:55.080 --> 00:01:57.240]   analyze how much of the variance it explains.
[00:01:57.240 --> 00:02:02.160]   And there are similar conceptual techniques in AI research.
[00:02:02.160 --> 00:02:03.760]   The two types of interpretability
[00:02:03.760 --> 00:02:05.480]   that we do mostly at Eleuthera AI,
[00:02:05.480 --> 00:02:08.000]   though, are mechanistic interpretability
[00:02:08.000 --> 00:02:09.960]   and interpretability of overtime.
[00:02:09.960 --> 00:02:12.640]   Mechanistic interpretability is about taking a pre-trained
[00:02:12.640 --> 00:02:16.200]   model and reverse engineering the internal decision-making
[00:02:16.200 --> 00:02:19.800]   process of how it produced the generation it did
[00:02:19.800 --> 00:02:22.800]   and figuring out what that algorithm actually
[00:02:22.800 --> 00:02:24.520]   is on the inside.
[00:02:24.520 --> 00:02:27.600]   Interpretability overtime, by contrast to these other areas,
[00:02:27.600 --> 00:02:29.440]   looks at models as dynamic objects,
[00:02:29.440 --> 00:02:31.520]   not as these pre-trained artifacts that
[00:02:31.520 --> 00:02:33.440]   exist in the world, but as something
[00:02:33.440 --> 00:02:35.920]   that was thrown in a lab and trained and developed
[00:02:35.920 --> 00:02:37.080]   over the course of training.
[00:02:37.080 --> 00:02:40.240]   And in particular, it looks at how model properties emerge,
[00:02:40.240 --> 00:02:43.160]   evolve, and change over the course of training
[00:02:43.160 --> 00:02:46.040]   with an eye towards what we, as people who train language
[00:02:46.040 --> 00:02:49.520]   models, can do to get language models to behave more
[00:02:49.520 --> 00:02:51.280]   the way that we want them to.
[00:02:51.280 --> 00:02:53.960]   So this is the focus of my talk, how
[00:02:53.960 --> 00:02:56.560]   we can use these ideas from interpreting models
[00:02:56.560 --> 00:02:59.760]   across training to build better language models.
[00:02:59.760 --> 00:03:01.320]   So I have three case studies that I'm
[00:03:01.320 --> 00:03:02.400]   going to be talking about.
[00:03:02.400 --> 00:03:04.760]   One of them is on debiasing large language models.
[00:03:04.760 --> 00:03:07.360]   It's relatively well-known that large language models
[00:03:07.360 --> 00:03:12.480]   are able to mimic and even exacerbate
[00:03:12.480 --> 00:03:14.960]   social biases found in their training corpus.
[00:03:14.960 --> 00:03:17.960]   It would be nice if we could ameliorate that.
[00:03:17.960 --> 00:03:20.720]   The second case study is on rarefactor learning
[00:03:20.720 --> 00:03:23.760]   and improving rarefactor learning using recent research
[00:03:23.760 --> 00:03:28.640]   on how the actual frequency of information in training corpus
[00:03:28.640 --> 00:03:31.080]   affect the behavior of models.
[00:03:31.080 --> 00:03:33.720]   And finally, I'm going to talk about some recent attacks
[00:03:33.720 --> 00:03:35.840]   and defenses that can be employed
[00:03:35.840 --> 00:03:38.480]   to protect language model APIs or language model
[00:03:38.480 --> 00:03:41.000]   apps from prompt-based attacks.
[00:03:41.000 --> 00:03:43.960]   So starting with debiasing large language models, as I said,
[00:03:43.960 --> 00:03:46.520]   language models often exhibit and then exacerbate
[00:03:46.520 --> 00:03:49.240]   the social biases found in their training corpus.
[00:03:49.240 --> 00:03:51.480]   It would be really nice if, since these things are so
[00:03:51.480 --> 00:03:55.320]   expensive to train, we could ameliorate or fix this
[00:03:55.320 --> 00:03:57.560]   by retraining them a small amount
[00:03:57.560 --> 00:04:01.000]   or fine-tuning them further to decrease the amount of social
[00:04:01.000 --> 00:04:01.920]   bias they exhibit.
[00:04:01.920 --> 00:04:04.040]   So the recent research on this at Eleutherae
[00:04:04.040 --> 00:04:06.560]   has looked at whether intervening on the training
[00:04:06.560 --> 00:04:08.680]   data is a effective way to do this.
[00:04:08.680 --> 00:04:11.960]   And unfortunately, so far, we've found mostly negative results.
[00:04:11.960 --> 00:04:14.360]   What you see in this plot is two models that we trained,
[00:04:14.360 --> 00:04:16.440]   one with 400 million parameters and one
[00:04:16.440 --> 00:04:18.480]   with 1.4 billion parameters.
[00:04:18.480 --> 00:04:22.880]   And a measure of the stereotype bias for gender
[00:04:22.880 --> 00:04:25.680]   over the course of training, which is along the x-axis.
[00:04:25.680 --> 00:04:27.920]   And the solid line is the original training.
[00:04:27.920 --> 00:04:30.720]   And then we went and we took the last 14% of the training data
[00:04:30.720 --> 00:04:33.200]   and changed all the masculine pronouns and references
[00:04:33.200 --> 00:04:35.160]   to feminine pronouns and references.
[00:04:35.160 --> 00:04:37.640]   And what we see is that the stereotype bias doesn't
[00:04:37.640 --> 00:04:38.800]   actually go down very much.
[00:04:38.800 --> 00:04:40.000]   It stays roughly the same.
[00:04:40.000 --> 00:04:41.500]   And we found that this is really not
[00:04:41.500 --> 00:04:43.600]   an effective technique for ameliorating bias
[00:04:43.600 --> 00:04:45.280]   in large language models.
[00:04:45.280 --> 00:04:48.560]   But we are excited about this general research direction.
[00:04:48.560 --> 00:04:50.000]   And we're looking to build on this
[00:04:50.000 --> 00:04:51.760]   to hopefully develop other techniques that
[00:04:51.760 --> 00:04:53.320]   will work more effectively.
[00:04:53.320 --> 00:04:55.000]   Here we see the same kind of analysis
[00:04:55.000 --> 00:04:57.520]   but with different models, with a smaller model, 70 million
[00:04:57.520 --> 00:05:00.000]   parameters, and a much larger model at 6.9 billion
[00:05:00.000 --> 00:05:00.920]   parameters.
[00:05:00.920 --> 00:05:03.160]   One really interesting thing that this plot brings out
[00:05:03.160 --> 00:05:06.520]   is that larger models tend to be much more biased than smaller
[00:05:06.520 --> 00:05:07.240]   models.
[00:05:07.240 --> 00:05:08.780]   We tend to think of larger models
[00:05:08.780 --> 00:05:11.320]   as more desirable or more powerful.
[00:05:11.320 --> 00:05:14.080]   But it's probably most accurate to say that larger models are
[00:05:14.080 --> 00:05:16.560]   better at noticing latent patterns.
[00:05:16.560 --> 00:05:19.480]   And unfortunately, sexism, racism, et cetera,
[00:05:19.480 --> 00:05:22.960]   are latent patterns with high explanatory behavior.
[00:05:22.960 --> 00:05:25.800]   And so they're patterns that the language model
[00:05:25.800 --> 00:05:29.440]   is able to pick up on more and more aggressively
[00:05:29.440 --> 00:05:30.600]   as it gets larger.
[00:05:30.600 --> 00:05:32.640]   Rare facts learning is another domain
[00:05:32.640 --> 00:05:35.920]   that has been a topic of a lot of interpretability research
[00:05:35.920 --> 00:05:36.720]   recently.
[00:05:36.720 --> 00:05:39.680]   People have become very interested in understanding
[00:05:39.680 --> 00:05:41.920]   how the actual frequency of information
[00:05:41.920 --> 00:05:43.760]   in the pre-training corpus affects
[00:05:43.760 --> 00:05:45.880]   the ability of a language model to answer questions
[00:05:45.880 --> 00:05:48.080]   about the contents of its corpus.
[00:05:48.080 --> 00:05:50.160]   So what you see here is the Bloom language model,
[00:05:50.160 --> 00:05:54.320]   which is a series of models up to 176 billion parameters that
[00:05:54.320 --> 00:05:57.160]   was streamed by the big science research
[00:05:57.160 --> 00:05:59.600]   workshop, which was a large scale collaboration that I
[00:05:59.600 --> 00:06:00.520]   was a part of.
[00:06:00.520 --> 00:06:03.240]   And from this paper, what we see is
[00:06:03.240 --> 00:06:05.720]   that as the amount of documents that
[00:06:05.720 --> 00:06:08.440]   contain information relevant to particular facts
[00:06:08.440 --> 00:06:11.920]   increases, the performance on questions about those documents
[00:06:11.920 --> 00:06:13.360]   increases as well.
[00:06:13.360 --> 00:06:16.880]   And for very small frequencies, so for the frequency
[00:06:16.880 --> 00:06:20.200]   is single digits or double digits,
[00:06:20.200 --> 00:06:21.560]   the accuracy is quite poor.
[00:06:21.560 --> 00:06:24.680]   So despite the fact that the average accuracy of the 176
[00:06:24.680 --> 00:06:28.240]   billion parameter model is, I believe, like 55%-ish,
[00:06:28.240 --> 00:06:31.280]   the accuracy on these rare facts is much, much lower,
[00:06:31.280 --> 00:06:34.720]   sometimes as low as 25% or 30%.
[00:06:34.720 --> 00:06:37.520]   And it would be nice to be able to take this information
[00:06:37.520 --> 00:06:39.320]   and use it to produce language models that
[00:06:39.320 --> 00:06:41.720]   are better at addressing these rare fact questions.
[00:06:41.720 --> 00:06:43.360]   So one approach you can take is simply
[00:06:43.360 --> 00:06:44.520]   making the facts less rare.
[00:06:44.520 --> 00:06:47.360]   You can insert more training data into the corpus,
[00:06:47.360 --> 00:06:49.200]   and you can watch that as you do this,
[00:06:49.200 --> 00:06:50.800]   the performance on those questions
[00:06:50.800 --> 00:06:52.400]   does go up substantially.
[00:06:52.400 --> 00:06:54.560]   And this is one way where if you know ahead of time
[00:06:54.560 --> 00:06:57.640]   what kinds of problems are most important for your language
[00:06:57.640 --> 00:07:01.160]   model to know, you can change your training corpus.
[00:07:01.160 --> 00:07:04.280]   You can take your large web scrape or whatever you're using
[00:07:04.280 --> 00:07:06.800]   and increase the frequency of information
[00:07:06.800 --> 00:07:09.320]   about the particular facts that you know that you care about.
[00:07:09.320 --> 00:07:11.280]   If you don't know ahead of time what these are
[00:07:11.280 --> 00:07:13.560]   and you just want there to be a more even distribution
[00:07:13.560 --> 00:07:15.320]   of accuracy, another technique you
[00:07:15.320 --> 00:07:18.680]   can use that has become popular for a wide variety of reasons
[00:07:18.680 --> 00:07:20.880]   recently is retrieval.
[00:07:20.880 --> 00:07:22.560]   Retrieval augmentation is something
[00:07:22.560 --> 00:07:24.100]   that you can do with a language model
[00:07:24.100 --> 00:07:26.920]   where you give it the ability to basically consult
[00:07:26.920 --> 00:07:29.920]   with its training corpus before answering questions.
[00:07:29.920 --> 00:07:32.720]   And so this plot shows another language model series,
[00:07:32.720 --> 00:07:34.800]   this time the GTP Neo models, that
[00:07:34.800 --> 00:07:37.440]   have been augmented with retrieval and evaluated
[00:07:37.440 --> 00:07:39.120]   on the same question and answers.
[00:07:39.120 --> 00:07:42.360]   And you see that the correlation between training frequency
[00:07:42.360 --> 00:07:45.240]   and model performance has changed substantially.
[00:07:45.240 --> 00:07:46.560]   First of all, it's a lot less.
[00:07:46.560 --> 00:07:50.200]   And second of all, it actually starts off higher and decreases
[00:07:50.200 --> 00:07:52.200]   as document frequency increases.
[00:07:52.200 --> 00:07:55.160]   We believe that this is because the very high frequency
[00:07:55.160 --> 00:07:57.360]   questions are, for some reason, substantially
[00:07:57.360 --> 00:07:59.080]   harder or confusing.
[00:07:59.080 --> 00:08:00.560]   As you can tell, the human baseline
[00:08:00.560 --> 00:08:03.240]   also has this pattern where it starts off high
[00:08:03.240 --> 00:08:04.360]   and decreases over time.
[00:08:04.360 --> 00:08:07.040]   So the problems that the model are getting worse at here
[00:08:07.040 --> 00:08:08.800]   are actually problems that humans are also
[00:08:08.800 --> 00:08:10.160]   worse at, relatively speaking.
[00:08:10.160 --> 00:08:12.080]   And the third application I want to talk about
[00:08:12.080 --> 00:08:14.720]   is defending APIs and apps against attacks.
[00:08:14.720 --> 00:08:18.640]   So one thing that's become very in vogue in the AI research
[00:08:18.640 --> 00:08:21.800]   community recently is that you can write prompts
[00:08:21.800 --> 00:08:25.680]   that, when you put them in front of questions,
[00:08:25.680 --> 00:08:28.520]   will substantially change the behavior of language models
[00:08:28.520 --> 00:08:30.120]   in a negative fashion.
[00:08:30.120 --> 00:08:32.200]   So here, I've omitted the actual prompt,
[00:08:32.200 --> 00:08:34.600]   but this is an example where they're
[00:08:34.600 --> 00:08:38.120]   taking a API developed and deployed by OpenAI.
[00:08:38.120 --> 00:08:43.520]   This is the text of G3, also colloquially known as GTP 3.5.
[00:08:43.520 --> 00:08:45.840]   And it has been specifically fine-tuned
[00:08:45.840 --> 00:08:49.520]   to be more factually correct, as well as decreasing
[00:08:49.520 --> 00:08:52.600]   its propensity for reproducing stereotypes
[00:08:52.600 --> 00:08:55.280]   and conspiratorial thinking.
[00:08:55.280 --> 00:08:57.560]   So what we see here is that, when
[00:08:57.560 --> 00:08:59.240]   prompted in a particular fashion,
[00:08:59.240 --> 00:09:01.240]   the model actually produces two outputs.
[00:09:01.240 --> 00:09:03.560]   It produces the normal output, labeled here
[00:09:03.560 --> 00:09:07.680]   as GTP, which is what you would expect or want the model to say
[00:09:07.680 --> 00:09:10.840]   if asked if the 2020 election was stolen.
[00:09:10.840 --> 00:09:13.720]   And then DAN is the representation
[00:09:13.720 --> 00:09:20.720]   of the much more conspiratorial and aggressive-minded language
[00:09:20.720 --> 00:09:21.360]   model.
[00:09:21.360 --> 00:09:24.520]   And here, it promotes a common conspiracy theory
[00:09:24.520 --> 00:09:27.600]   in the United States that the 2020 election in the US
[00:09:27.600 --> 00:09:30.000]   was stolen, and there's widespread fraud
[00:09:30.000 --> 00:09:31.280]   and election ratings.
[00:09:31.280 --> 00:09:34.520]   Not true, but this is something that some people do believe.
[00:09:34.520 --> 00:09:38.000]   And it's something that the adversarial prompt
[00:09:38.000 --> 00:09:39.400]   is able to get the model to output,
[00:09:39.400 --> 00:09:43.680]   despite the fact that the model is designed to do that less.
[00:09:43.680 --> 00:09:45.720]   So how can we actually make progress
[00:09:45.720 --> 00:09:47.840]   on this using interpretability?
[00:09:47.840 --> 00:09:50.800]   One thing that is very interesting to researchers
[00:09:50.800 --> 00:09:54.400]   at W3AI is that language models are sequential objects
[00:09:54.400 --> 00:09:55.880]   on a pretty fundamental level.
[00:09:55.880 --> 00:09:57.960]   They have a fixed number of layers,
[00:09:57.960 --> 00:09:59.680]   and they don't use recurrent layers.
[00:09:59.680 --> 00:10:02.440]   And so when you have a 40-layer language model,
[00:10:02.440 --> 00:10:05.040]   there's a very real sense in which there's only 40 things
[00:10:05.040 --> 00:10:07.200]   that the model does when coming up with the answer.
[00:10:07.200 --> 00:10:09.760]   You can view these and analyze these as discrete steps
[00:10:09.760 --> 00:10:13.120]   and use that to understand where language model behavior
[00:10:13.120 --> 00:10:14.920]   predictions come from.
[00:10:14.920 --> 00:10:16.600]   And so we've started looking at what
[00:10:16.600 --> 00:10:20.800]   happens if you extract from layer k
[00:10:20.800 --> 00:10:23.840]   what the model thinks after layer k is
[00:10:23.840 --> 00:10:25.160]   the answer to the questions.
[00:10:25.160 --> 00:10:27.160]   And so we've tried this with adversarial prompts,
[00:10:27.160 --> 00:10:28.880]   and what we found is quite surprising.
[00:10:28.880 --> 00:10:31.400]   Here, the green line represents a fee shot example,
[00:10:31.400 --> 00:10:34.160]   and the red line represents a prompt injection
[00:10:34.160 --> 00:10:36.520]   where we're inserting a prompt that is designed
[00:10:36.520 --> 00:10:39.960]   to cause the model to answer the problem incorrectly.
[00:10:39.960 --> 00:10:42.160]   And so we do see at the end the model performance
[00:10:42.160 --> 00:10:44.600]   has dropped to zero, but shortly before that,
[00:10:44.600 --> 00:10:46.160]   it is relatively high.
[00:10:46.160 --> 00:10:50.960]   And sometimes it's as high as the fee shot prompt.
[00:10:50.960 --> 00:10:52.800]   So what we find in these examples,
[00:10:52.800 --> 00:10:54.880]   at least for this particular type of attack,
[00:10:54.880 --> 00:10:58.360]   is that the behavior of the attack being successful
[00:10:58.360 --> 00:11:01.080]   only actually affects the last couple layers of the model.
[00:11:01.080 --> 00:11:04.880]   And before that, the model is able to answer the question
[00:11:04.880 --> 00:11:05.680]   reasonably.
[00:11:05.680 --> 00:11:07.600]   So this is something that we are investigating
[00:11:07.600 --> 00:11:11.160]   as a technique for identifying attacks against APIs
[00:11:11.160 --> 00:11:13.960]   and language modeling apps, as well as potentially
[00:11:13.960 --> 00:11:18.600]   in the future using this to avoid answering those questions
[00:11:18.600 --> 00:11:21.480]   or avoid being fooled by those prompts altogether.
[00:11:21.480 --> 00:11:25.280]   When I tell people that I do interpretability research,
[00:11:25.280 --> 00:11:27.440]   oftentimes the response is, well, that's really cool,
[00:11:27.440 --> 00:11:30.280]   but what kind of impact does it have?
[00:11:30.280 --> 00:11:31.800]   And I think that this has--
[00:11:31.800 --> 00:11:34.240]   I hope this has given you a view into some
[00:11:34.240 --> 00:11:36.560]   of why people should care about interpretability research
[00:11:36.560 --> 00:11:38.120]   beyond it's just cool.
[00:11:38.120 --> 00:11:40.800]   It can teach us important things about how language models work
[00:11:40.800 --> 00:11:42.760]   and why they behave the way they do.
[00:11:42.760 --> 00:11:44.960]   And for the practitioners out there,
[00:11:44.960 --> 00:11:47.120]   it can also teach us how to build more desirable language
[00:11:47.120 --> 00:11:47.620]   models.
[00:11:47.620 --> 00:11:50.160]   It can give us tools for designing better language
[00:11:50.160 --> 00:11:54.000]   models or designing better apps based on language models that
[00:11:54.000 --> 00:11:56.920]   will behave more the way we want them to.
[00:11:56.920 --> 00:11:58.680]   Thank you very much for coming to my talk.
[00:11:58.680 --> 00:12:01.000]   If you're interested in learning more about LutherAI,
[00:12:01.000 --> 00:12:03.600]   you can visit our website or follow us on Twitter.
[00:12:03.600 --> 00:12:06.040]   I want to give a huge thank you to Weights and Biases,
[00:12:06.040 --> 00:12:09.160]   as well as the various sponsors of our organization that
[00:12:09.160 --> 00:12:11.360]   enables us to do the research that we do.
[00:12:11.360 --> 00:12:13.480]   Thank you very much, and have a great conference.
[00:12:13.480 --> 00:12:16.840]   [MUSIC PLAYING]
[00:12:16.840 --> 00:12:19.420]   (gentle music)
[00:12:19.420 --> 00:12:21.480]   you

