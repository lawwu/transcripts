
[00:00:00.000 --> 00:00:08.200]   Maybe it's true that theory is somehow more philosophically interesting than just benchmark
[00:00:08.200 --> 00:00:11.720]   applications, than just empirical pursuit on methods.
[00:00:11.720 --> 00:00:13.240]   But the application is a different axis.
[00:00:13.240 --> 00:00:16.880]   I actually think that the applications are super philosophically interesting.
[00:00:16.880 --> 00:00:21.240]   I think they force you to ask, because they ask you to ask questions that are just mechanical.
[00:00:21.240 --> 00:00:25.720]   You have to ask the normative questions.
[00:00:25.720 --> 00:00:29.160]   The thing that I think is exciting about applications is that nobody told you in the first place
[00:00:29.160 --> 00:00:30.960]   what is worth predicting.
[00:00:30.960 --> 00:00:38.800]   That by itself, convincing someone that this is actually a problem worth solving.
[00:00:38.800 --> 00:00:42.640]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:42.640 --> 00:00:44.280]   models work in the real world.
[00:00:44.280 --> 00:00:46.600]   I'm your host, Lukas Priwat.
[00:00:46.600 --> 00:00:50.960]   Zach Lipton is a professor of machine learning at Carnegie Mellon University.
[00:00:50.960 --> 00:00:55.440]   He has an incredible number of research interests, and it's actually hard to research all the
[00:00:55.440 --> 00:00:58.040]   papers that he's been working on prepping for this interview.
[00:00:58.040 --> 00:01:01.200]   I'll give you a couple topics that we might cover today.
[00:01:01.200 --> 00:01:05.600]   Robustness under distribution shift, breast cancer screening with machine learning, the
[00:01:05.600 --> 00:01:11.000]   effective and equitable allocation of organs, and the intersection of causal thinking with
[00:01:11.000 --> 00:01:12.000]   messy data.
[00:01:12.000 --> 00:01:17.280]   He's the founder of the Approximately Correct blog, and the creator of Dive into Deep Learning,
[00:01:17.280 --> 00:01:21.640]   an interactive open source book drafted entirely in Jupyter notebooks.
[00:01:21.640 --> 00:01:23.120]   Couldn't be more excited to get into it.
[00:01:23.120 --> 00:01:27.680]   I have a couple of your papers that you flagged that I'd love to talk about, but kind of before
[00:01:27.680 --> 00:01:29.400]   then I kind of wanted you to catch me up.
[00:01:29.400 --> 00:01:33.960]   I feel like last time I knew you, you were applying to grad school and now you seem like
[00:01:33.960 --> 00:01:38.260]   a successful professor with a lab at a very famous school.
[00:01:38.260 --> 00:01:39.260]   What happened, Zach?
[00:01:39.260 --> 00:01:41.440]   Yeah, it's been a weird ride.
[00:01:41.440 --> 00:01:43.720]   So when we met, it was in San Francisco.
[00:01:43.720 --> 00:01:47.520]   And that was like, I had already made this weird decision to go and kind of do this tech
[00:01:47.520 --> 00:01:52.520]   thing and kind of live in California for a year, get into grad school.
[00:01:52.520 --> 00:01:54.560]   But before that, I was a musician.
[00:01:54.560 --> 00:01:56.900]   So it was even a bigger jump.
[00:01:56.900 --> 00:02:00.720]   I think it looks more planned or directed now than it was at the time.
[00:02:00.720 --> 00:02:05.240]   The guiding thing to get from being a musician to being a PhD in machine learning was just
[00:02:05.240 --> 00:02:07.800]   a recognition that I wanted to be a PhD.
[00:02:07.800 --> 00:02:12.840]   I had enough friends who were in the sciences that I sort of knew that maybe the sorting
[00:02:12.840 --> 00:02:15.520]   hat got it wrong or something at some point.
[00:02:15.520 --> 00:02:19.560]   And I didn't even know what modern machine learning was.
[00:02:19.560 --> 00:02:24.400]   It was really guided by a kind of, I knew I wanted to be in a certain kind of scholarship
[00:02:24.400 --> 00:02:26.200]   and I wanted to be a certain kind of environment.
[00:02:26.200 --> 00:02:30.240]   I knew that meant going to grad school and then sort of looking like, all right, I was
[00:02:30.240 --> 00:02:34.140]   an old man for a first person starting on a scientific career.
[00:02:34.140 --> 00:02:37.960]   So I was like, I wasn't going to do a wet lab thing and spend 10 years learning how
[00:02:37.960 --> 00:02:40.840]   to pipette because it was too late for that.
[00:02:40.840 --> 00:02:45.000]   And I had had just enough of a connection with computer science earlier that I knew
[00:02:45.000 --> 00:02:46.440]   that was something I enjoyed doing.
[00:02:46.440 --> 00:02:47.440]   But I don't know.
[00:02:47.440 --> 00:02:48.440]   It's kind of weird to look back.
[00:02:48.440 --> 00:02:52.840]   I mean, I think in terms of from where we met, which was like, I was kind of knew almost
[00:02:52.840 --> 00:02:57.160]   nothing and was just kind of wanted to go to grad school for machine learning.
[00:02:57.160 --> 00:03:02.760]   I think the biggest thing is, is that I entered the field at the moment of like a really great
[00:03:02.760 --> 00:03:04.340]   leveling event.
[00:03:04.340 --> 00:03:11.480]   So the like sudden rise of deep learning was like an unexpected thing.
[00:03:11.480 --> 00:03:14.760]   And I think it would be an exaggeration to say like completely wiped out people's skill
[00:03:14.760 --> 00:03:16.840]   sets or whatever from before then.
[00:03:16.840 --> 00:03:24.080]   But it certainly like opened up like a path in research where at least like the next two,
[00:03:24.080 --> 00:03:29.160]   three years of steps in that direction or a good chunk of them didn't really require
[00:03:29.160 --> 00:03:33.640]   that you were like, you know, like if things were just progressing normal science and it
[00:03:33.640 --> 00:03:36.920]   was like kernel machines were dominating, like for me to get to the point where I was
[00:03:36.920 --> 00:03:43.220]   like a world leader and, you know, understanding nonparametric or something would, that wouldn't
[00:03:43.220 --> 00:03:45.240]   happen in like three or four years.
[00:03:45.240 --> 00:03:49.200]   Like entering a field where suddenly like everyone was doing deep learning and there
[00:03:49.200 --> 00:03:53.880]   was like kind of like a, you know, like a wild west type environment made it very easy
[00:03:53.880 --> 00:03:59.240]   to sort of like pick an area, like say ML and healthcare and very quickly be at least
[00:03:59.240 --> 00:04:03.160]   like on now, like the new generation of technologies be one of the leaders, like applying deep
[00:04:03.160 --> 00:04:04.160]   learning in that.
[00:04:04.160 --> 00:04:09.440]   So I think I got lucky that I sort of entered at that moment of transition where it wasn't
[00:04:09.440 --> 00:04:14.840]   so disadvantageous that like I wasn't an expert in like, you know, I wasn't a great engineer
[00:04:14.840 --> 00:04:18.400]   and I didn't necessarily have all of that mathematical background, but I was able to
[00:04:18.400 --> 00:04:23.840]   sort of like, like one advantage of it is I didn't have a lot of commitments.
[00:04:23.840 --> 00:04:29.400]   So I wasn't like committed to a set of methods that like I had invested years and like reputation
[00:04:29.400 --> 00:04:30.400]   and getting them to work.
[00:04:30.400 --> 00:04:33.040]   So I could be kind of nonpartisan about it and say like, this is clearly a thing that's
[00:04:33.040 --> 00:04:36.320]   happening and I have like no sunk costs.
[00:04:36.320 --> 00:04:38.680]   So, you know, get in there.
[00:04:38.680 --> 00:04:39.680]   That's really cool.
[00:04:39.680 --> 00:04:41.160]   It's actually kind of inspiring.
[00:04:41.160 --> 00:04:42.160]   I like it.
[00:04:42.160 --> 00:04:45.520]   So what was your like initial research on like when you, when you got to grad school,
[00:04:45.520 --> 00:04:47.520]   what were you looking at?
[00:04:47.520 --> 00:04:49.240]   I was working on healthcare problems.
[00:04:49.240 --> 00:04:54.760]   So I had had some like personal health experiences that were pretty like devastating earlier
[00:04:54.760 --> 00:04:55.800]   in life.
[00:04:55.800 --> 00:05:00.800]   And I think that was just sort of always like a motivating thing of could we be making a
[00:05:00.800 --> 00:05:05.240]   lot of these kinds of inferences better that guide medical decision-making.
[00:05:05.240 --> 00:05:10.400]   It still is a kind of like overriding, like organizing, like motivation in my work.
[00:05:10.400 --> 00:05:11.400]   My research is a little more diverse.
[00:05:11.400 --> 00:05:16.200]   Like I don't, I don't just do like the, I want to grab things and get empirical results
[00:05:16.200 --> 00:05:18.160]   on say like a specific medical dataset.
[00:05:18.160 --> 00:05:23.880]   Although I do have a bunch of research in my portfolio that's like is applied medical
[00:05:23.880 --> 00:05:28.440]   work, but also like the motivated kind of underlying like theoretical and methodological
[00:05:28.440 --> 00:05:29.440]   problems.
[00:05:29.440 --> 00:05:31.640]   But like, that was how I started PhD was working on medical stuff.
[00:05:31.640 --> 00:05:35.960]   It was like, I wrote a statement of purpose that I think was like caught the attention
[00:05:35.960 --> 00:05:40.360]   of some people like at UCSD, which is where I ended up doing my PhD.
[00:05:40.360 --> 00:05:44.000]   There's a division that does biomedical informatics and there's a computer science department,
[00:05:44.000 --> 00:05:47.420]   one's in the med school, the other's in engineering school.
[00:05:47.420 --> 00:05:51.240]   And I think like they had been talking about maybe getting a joint student at some point
[00:05:51.240 --> 00:05:55.360]   or someone who would be funded on one of the medical informatics training grants and, but
[00:05:55.360 --> 00:05:58.720]   be a student in CS and they were looking for someone like that.
[00:05:58.720 --> 00:06:03.260]   What I was hired to do essentially was to work on healthcare problems.
[00:06:03.260 --> 00:06:06.640]   But I kind of like just sort of, I started with that motivation and looking at what people
[00:06:06.640 --> 00:06:07.640]   are doing.
[00:06:07.640 --> 00:06:11.120]   But I was sitting in the computer science department and watching what's happening with
[00:06:11.120 --> 00:06:12.120]   machine learning.
[00:06:12.120 --> 00:06:16.520]   So for example, I suppose like the first problem I worked on was something in text mining.
[00:06:16.520 --> 00:06:20.800]   So it was medical articles and we're doing like massive multi-label classification.
[00:06:20.800 --> 00:06:26.480]   So all the medical articles that get indexed by the NIH are tagged with, you know, some
[00:06:26.480 --> 00:06:30.880]   subset of this like large controlled vocabularies, like kind of enables things like systematic
[00:06:30.880 --> 00:06:31.880]   reviews of literature.
[00:06:31.880 --> 00:06:35.600]   And so just like a simple, like back then we're using linear models and the challenge
[00:06:35.600 --> 00:06:40.240]   was that it was 27,000 classes and we're trying to predict them all and do it in like an efficient
[00:06:40.240 --> 00:06:41.240]   way.
[00:06:41.240 --> 00:06:44.280]   Now it seems kind of quaint because it's like, you know, you do train language models with
[00:06:44.280 --> 00:06:47.880]   like billions of parameters and vocabularies that are like 300,000 words and it's not that
[00:06:47.880 --> 00:06:48.880]   big a deal.
[00:06:48.880 --> 00:06:51.640]   So I started working on that, but I was seeing what was happening in deep learning.
[00:06:51.640 --> 00:06:56.360]   And I think like the first kind of bigger break that wasn't like just a kind of minor
[00:06:56.360 --> 00:07:00.840]   paper was we were sort of watching everything that was happening.
[00:07:00.840 --> 00:07:03.560]   It's like convolutional neural networks were maybe the thing that were catching the most
[00:07:03.560 --> 00:07:08.160]   attention 2013, 14, but I was interested in a lot of these problems that had more sequential
[00:07:08.160 --> 00:07:09.160]   structure.
[00:07:09.160 --> 00:07:11.440]   So I was like getting medical time series data, like people are admitted, there's a
[00:07:11.440 --> 00:07:14.480]   bunch of measurements that are getting updated over time.
[00:07:14.480 --> 00:07:18.440]   And so I started paying attention to natural language processing, like what was happening,
[00:07:18.440 --> 00:07:20.880]   because that's another problem with that kind of sequential structure.
[00:07:20.880 --> 00:07:27.080]   And I was seeing things like these papers in 2012, 13, 14 that like Ilya Setskeva and
[00:07:27.080 --> 00:07:32.360]   like other people like that were doing with language modeling and seek to seek type things.
[00:07:32.360 --> 00:07:37.480]   And you start thinking, are these methods sort of limited to these kinds of kind of
[00:07:37.480 --> 00:07:42.840]   like neat, like ordinarily like kind of sequence things like language or would they also work
[00:07:42.840 --> 00:07:46.880]   for things like kind of like messy multivariate time series data that you have in clinical
[00:07:46.880 --> 00:07:47.880]   settings.
[00:07:47.880 --> 00:07:51.320]   And so Dave Kale, who I mentioned earlier, was the guy that they tried to recruit.
[00:07:51.320 --> 00:07:56.240]   I had actually met him like when I was like starting PhD at UCSD, actually at machine
[00:07:56.240 --> 00:07:59.320]   learning for healthcare, like the one of the first years of that one was still like it
[00:07:59.320 --> 00:08:00.320]   wasn't even a conference at the time.
[00:08:00.320 --> 00:08:02.340]   It was like a symposium.
[00:08:02.340 --> 00:08:06.280]   And so we got together, this was like second year of PhD.
[00:08:06.280 --> 00:08:10.280]   And we kind of had this idea of, you know, it wasn't obvious at the time now, like anything
[00:08:10.280 --> 00:08:13.360]   that looks like a sequence, people throw an LSTM at the time, but the time was really
[00:08:13.360 --> 00:08:17.840]   only making headway like popularly in language.
[00:08:17.840 --> 00:08:22.480]   And a little bit maybe like on top of like, you know, RNN competent type things like on
[00:08:22.480 --> 00:08:25.600]   top of like video or stuff like that.
[00:08:25.600 --> 00:08:29.400]   And so, you know, we were interested, can we do much better than kind of status quo
[00:08:29.400 --> 00:08:35.960]   at predicting things like length of stay, mortality, recognizing diagnoses based on...
[00:08:35.960 --> 00:08:41.440]   And so you have these time series where like the added complications are you have a bunch
[00:08:41.440 --> 00:08:42.700]   of different variables.
[00:08:42.700 --> 00:08:44.640]   Some of them are missing.
[00:08:44.640 --> 00:08:45.740]   They're observed.
[00:08:45.740 --> 00:08:48.160]   They're not observed at like some fixed interval on the wall clock.
[00:08:48.160 --> 00:08:50.080]   They're observed at like different times.
[00:08:50.080 --> 00:08:56.680]   If you try to like resample to make a statistic of the time series, that's reflective of like
[00:08:56.680 --> 00:09:02.760]   a fixed like wall clock time Delta, then you wind up with like missing data.
[00:09:02.760 --> 00:09:06.000]   That's not like truly missing, but it's like missing as an artifact of like the sampling
[00:09:06.000 --> 00:09:07.000]   frequency.
[00:09:07.000 --> 00:09:08.360]   Like it wasn't observed in that window.
[00:09:08.360 --> 00:09:09.360]   So then what do you do?
[00:09:09.360 --> 00:09:10.360]   How do you impute it?
[00:09:10.360 --> 00:09:11.360]   Do you carry it forward?
[00:09:11.360 --> 00:09:13.720]   You mean like basically you have a lot of windows where nothing happened?
[00:09:13.720 --> 00:09:15.480]   Yeah, yeah, yeah.
[00:09:15.480 --> 00:09:16.480]   Or right, exactly.
[00:09:16.480 --> 00:09:21.900]   Like, you know, say like your heart rate's measured continuously by, you know, like automatically
[00:09:21.900 --> 00:09:23.320]   by the equipment.
[00:09:23.320 --> 00:09:27.600]   However, like the coma score is recorded once per hour by the doctor when they make the
[00:09:27.600 --> 00:09:32.000]   rounds and then some like serological result, like maybe it's checked once per day or maybe
[00:09:32.000 --> 00:09:35.320]   some days it's never checked or something like that, you know?
[00:09:35.320 --> 00:09:39.120]   So you have, well, if you, if you choose the time interval, that's somewhere in the middle,
[00:09:39.120 --> 00:09:43.240]   like hourly and you have this one thing that you're measuring that's happening multiple
[00:09:43.240 --> 00:09:44.320]   times inside a window.
[00:09:44.320 --> 00:09:47.160]   This other thing that's only happening once every like seven windows.
[00:09:47.160 --> 00:09:51.000]   So I mean, an alternative way that you could represent it is you could just say every measurement
[00:09:51.000 --> 00:09:59.920]   is a, like, you don't have the time tick for the RNN correspond to like a fixed delta on
[00:09:59.920 --> 00:10:03.320]   the clock, but you can make it correspond to the observation and say something like
[00:10:03.320 --> 00:10:07.480]   add as a feature, what is the time lapse since the last observation?
[00:10:07.480 --> 00:10:11.120]   And that's a little bit like, you know, there's like event based representations that they
[00:10:11.120 --> 00:10:13.280]   use for like music generation and stuff like that.
[00:10:13.280 --> 00:10:15.360]   In our case, it didn't work as well.
[00:10:15.360 --> 00:10:19.560]   So I mean, I'm always curious, like, it's funny, we've talked to a whole bunch of people
[00:10:19.560 --> 00:10:24.080]   from different angles in the medical field, but like, can you give me like a rundown of
[00:10:24.080 --> 00:10:28.200]   sort of the current state of the art in ML and medical stuff?
[00:10:28.200 --> 00:10:32.160]   Like, what are the most impressive results that you've seen recently?
[00:10:32.160 --> 00:10:35.080]   So there's like a bunch of slam dunk results, I think.
[00:10:35.080 --> 00:10:38.840]   I think you have to divide up like the categories of problems.
[00:10:38.840 --> 00:10:42.320]   I think a lot of people, like you see a lot of kind of like the, whatever the public think
[00:10:42.320 --> 00:10:45.520]   pieces about ML and healthcare, and they just kind of slop everything together.
[00:10:45.520 --> 00:10:49.840]   And it's just like, you know, the AI is making decisions and like, you know, you have an
[00:10:49.840 --> 00:10:51.360]   AI doctor and is it better than regular?
[00:10:51.360 --> 00:10:56.400]   It's kind of just a way that like collapses, like doctorness is to like a single task.
[00:10:56.400 --> 00:11:00.640]   Like, I think the reality is you have a whole bunch of different tasks.
[00:11:00.640 --> 00:11:04.560]   Some of them are really clearly like recognition problems.
[00:11:04.560 --> 00:11:07.400]   Like it's a pattern recognition problem.
[00:11:07.400 --> 00:11:11.840]   And the environment around that problem is so well understood that like, if you solve
[00:11:11.840 --> 00:11:14.360]   pattern recognition, then you know what to do with the answer.
[00:11:14.360 --> 00:11:17.160]   So you don't have like a real policy problem or a decision making problem.
[00:11:17.160 --> 00:11:21.920]   You just have a, I put in this things like, you know, now I'm going to get angry letters
[00:11:21.920 --> 00:11:25.920]   from some specialists that I'm like saying they're automatable or something.
[00:11:25.920 --> 00:11:30.040]   But I think the things that are most amenable to this are the results like the diabetic
[00:11:30.040 --> 00:11:33.720]   retinopathy, where they take the retinal fundus imaging, and they're able to predict whether
[00:11:33.720 --> 00:11:37.720]   or not someone has retinopathy and do it say as well or better than a physician can just
[00:11:37.720 --> 00:11:38.720]   by looking at these images.
[00:11:38.720 --> 00:11:42.040]   This is one of those things where it's like, the doctor knows what to do.
[00:11:42.040 --> 00:11:45.720]   If they're 100% sure about the diagnosis, it's like if you could just do the diagnosis
[00:11:45.720 --> 00:11:47.720]   more accurately, it's good.
[00:11:47.720 --> 00:11:50.080]   And then you know what to do.
[00:11:50.080 --> 00:11:52.740]   And you did the diagnosis here purely from an image.
[00:11:52.740 --> 00:11:54.520]   So it's essentially an image classification test.
[00:11:54.520 --> 00:11:55.520]   Right, exactly.
[00:11:55.520 --> 00:11:59.400]   Things that sort of just like reduced to like, hey, it's a pattern recognition problem.
[00:11:59.400 --> 00:12:00.400]   That's all we're doing.
[00:12:00.400 --> 00:12:01.520]   That's all the doctor's doing.
[00:12:01.520 --> 00:12:05.960]   And you know, those things you can call it pathology, I think has some of these like
[00:12:05.960 --> 00:12:07.440]   diagnosing things based on microscopy.
[00:12:07.440 --> 00:12:10.880]   Like one of the best papers I saw on machine learning for healthcare in the first year
[00:12:10.880 --> 00:12:15.680]   that it was a publishing conference, these people said, hey, these are, it turns out,
[00:12:15.680 --> 00:12:20.460]   they were they were attuned to the climate, they were actually writing from Uganda.
[00:12:20.460 --> 00:12:24.320]   And we're, it was like, the paper is very straightforward.
[00:12:24.320 --> 00:12:29.880]   But the problem was, the A plus part of the paper is how well motivated was, they said,
[00:12:29.880 --> 00:12:34.640]   hey, there's like three of the biggest maladies in Africa were like tuberculosis, malaria
[00:12:34.640 --> 00:12:36.660]   and intestinal parasites.
[00:12:36.660 --> 00:12:40.800]   These things are diagnosed based on basically pattern recognition by human doctors looking
[00:12:40.800 --> 00:12:45.400]   at microscopy, like microscope images.
[00:12:45.400 --> 00:12:51.440]   Africa at the time, as was argued in the paper, didn't have nearly enough technicians to be
[00:12:51.440 --> 00:12:55.280]   able to like give timely diagnosis to everyone.
[00:12:55.280 --> 00:12:58.720]   And I think at the time, they said something, there was something like, because it's much
[00:12:58.720 --> 00:13:04.040]   easier to diagnose, or it's much easier to donate a microscope than a microscopist.
[00:13:04.040 --> 00:13:08.480]   So there was like a situation where there were more microscopes than there were technicians
[00:13:08.480 --> 00:13:10.480]   on the continent.
[00:13:10.480 --> 00:13:17.040]   And basically, it's like, if you can just do, if you just do pattern recognition really
[00:13:17.040 --> 00:13:20.920]   accurately, you could, you know, and you can even avoid a lot of the pitfalls that normally
[00:13:20.920 --> 00:13:24.080]   plague machine learning, like you could standardize the equipment to send everyone the same damn
[00:13:24.080 --> 00:13:28.360]   microscope, the same phone camera for taking the picture, etc.
[00:13:28.360 --> 00:13:31.440]   If you can get like, you know, so they train like a simple convent, there was not a lot
[00:13:31.440 --> 00:13:35.640]   of like, you didn't need to do anything super novel methodologically, and end up getting
[00:13:35.640 --> 00:13:40.760]   like 99% accuracy on like doing this four way classification among these images, you
[00:13:40.760 --> 00:13:45.320]   say, done, you know, this is like an important problem, you can imagine shipping that tomorrow,
[00:13:45.320 --> 00:13:47.760]   not really tomorrow, but you get the idea.
[00:13:47.760 --> 00:13:49.040]   Does that really work?
[00:13:49.040 --> 00:13:50.360]   I see a lot of these kinds of results.
[00:13:50.360 --> 00:13:53.400]   And I wonder, do they really work?
[00:13:53.400 --> 00:13:56.360]   Or is it somehow like a more toy version of the real problem?
[00:13:56.360 --> 00:13:57.360]   Right.
[00:13:57.360 --> 00:14:01.800]   I mean, I think that's almost always a concern when you look at machine learning results,
[00:14:01.800 --> 00:14:02.800]   right?
[00:14:02.800 --> 00:14:08.520]   You see in a typical ML paper, almost always on a sort of randomly partitioned holdout
[00:14:08.520 --> 00:14:09.520]   set.
[00:14:09.520 --> 00:14:15.320]   So you're always worried about basically, hey, I've everything in the paper is sort
[00:14:15.320 --> 00:14:19.920]   of conditioned on the faithfulness to like that I ID assumption.
[00:14:19.920 --> 00:14:23.920]   It's like that my my training data and data I'm going to see in the future are really
[00:14:23.920 --> 00:14:27.600]   should be can be regarded as independent samples from the same underlying distribution.
[00:14:27.600 --> 00:14:29.760]   And that's almost never true in practice.
[00:14:29.760 --> 00:14:35.760]   And the question is, like, is this true in a way that just completely bungles up everything
[00:14:35.760 --> 00:14:36.760]   you've done?
[00:14:36.760 --> 00:14:41.120]   Or is this so an example of where there's a huge discrepancy is you have people saying
[00:14:41.120 --> 00:14:44.640]   that we have a human level speech recognition.
[00:14:44.640 --> 00:14:47.680]   And then if you ever actually use your speech recognition, it's like really clear that it's
[00:14:47.680 --> 00:14:48.680]   nowhere near human level.
[00:14:48.680 --> 00:14:53.920]   So it means is like, on the training corpus, if you randomly partition it, and you only
[00:14:53.920 --> 00:15:00.400]   look at like the maybe accuracy on like catching, you know, actually, you know, I think I think
[00:15:00.400 --> 00:15:03.880]   it back, they're not looking at phoneme level error rates, they do, you know, look at word
[00:15:03.880 --> 00:15:07.720]   error rate is fine, but you get the point is like, you know, if you make this really
[00:15:07.720 --> 00:15:13.400]   strong assumption that like the training data is and people confuse these because they use
[00:15:13.400 --> 00:15:17.960]   the same word, they say generalization in both cases, but one is the extra one is, or
[00:15:17.960 --> 00:15:22.200]   maybe what you might better rather call interpolation and extrapolation of like, do I generalize
[00:15:22.200 --> 00:15:30.080]   from the training set to samples from the exact same underlying distribution?
[00:15:30.080 --> 00:15:34.280]   The other is like, can I tolerate the sort of perturbations and distribution that are
[00:15:34.280 --> 00:15:38.600]   like assured to happen in practice.
[00:15:38.600 --> 00:15:42.920]   And so I think these are this is the thing that people deal in a really clumsy and kind
[00:15:42.920 --> 00:15:44.160]   of ad hoc way with right now.
[00:15:44.160 --> 00:15:49.440]   And a lot of my more theoretical and methodological research is about like, what are actually
[00:15:49.440 --> 00:15:55.240]   proper like sound principles according to which you can expect to generalize under,
[00:15:55.240 --> 00:16:00.440]   you know, perform under various shocks to the data generating distribution.
[00:16:00.440 --> 00:16:03.200]   So and I want to get to that, but I feel like I'm taking, I took you off on a tangent for
[00:16:03.200 --> 00:16:04.200]   no reason there.
[00:16:04.200 --> 00:16:06.200]   So just going back to like, so you were like-
[00:16:06.200 --> 00:16:08.720]   You took me on a tangent, I'll oblige, you know.
[00:16:08.720 --> 00:16:09.720]   Appreciate it.
[00:16:09.720 --> 00:16:14.920]   But the, sorry, the other medical examples that you think are impressive, I think you
[00:16:14.920 --> 00:16:17.440]   were, you're sort of laying out like an ontology of it.
[00:16:17.440 --> 00:16:20.480]   Right, so I think a lot of these records, like I think the retinal fundus imaging, like
[00:16:20.480 --> 00:16:25.080]   I'm not like, I think there's a, there's that long pipeline of productionalizing things
[00:16:25.080 --> 00:16:26.080]   in clinical trials.
[00:16:26.080 --> 00:16:30.120]   And I'm not actually up to the minute on where those are in that process.
[00:16:30.120 --> 00:16:34.680]   But that would be stuff that I'd be really confident would see it to production somewhere,
[00:16:34.680 --> 00:16:38.800]   if only as like an assistive tool that like, hey, if the doctor disagrees with this, get
[00:16:38.800 --> 00:16:39.800]   a second opinion.
[00:16:39.800 --> 00:16:40.800]   Yeah.
[00:16:40.800 --> 00:16:45.260]   So that stuff, I think is, is really out there.
[00:16:45.260 --> 00:16:47.960]   But then you see the other things people are talking about, people start talking about
[00:16:47.960 --> 00:16:49.960]   management of conditions, decision-making.
[00:16:49.960 --> 00:16:55.540]   And then start training models to do things like predict what would happen based on past
[00:16:55.540 --> 00:16:57.000]   decisions or whatever.
[00:16:57.000 --> 00:17:03.840]   Now this stuff, you know, gets way, way, way funkier or, you know, like all this kind of
[00:17:03.840 --> 00:17:07.520]   stuff that like has a flavor of, there's sort of maybe two things that people do.
[00:17:07.520 --> 00:17:10.480]   One is like sort of estimating conditional probabilities and pretending that they're
[00:17:10.480 --> 00:17:14.780]   estimating treatment effects and that it's like acting as though like knowing probability
[00:17:14.780 --> 00:17:20.280]   of death given this and death given that actually is giving you insight, a really deep insight
[00:17:20.280 --> 00:17:22.360]   to what would happen if you intervened.
[00:17:22.360 --> 00:17:26.120]   Probably that someone dies given that they had a treatment is very different from probably
[00:17:26.120 --> 00:17:31.800]   someone dies, given that like I intervene and given that treatment when in the historical
[00:17:31.800 --> 00:17:36.220]   data, this person always would have received a different treatment.
[00:17:36.220 --> 00:17:40.520]   You know, so I think you have that kind of work where there's like a huge gap between,
[00:17:40.520 --> 00:17:43.900]   you know, the kinds of things people are trying to say about how, you know, you have, you
[00:17:43.900 --> 00:17:46.880]   have a sort of like two sides, people who really understand causality and therefore
[00:17:46.880 --> 00:17:51.600]   like really measured and kind of conservative about the kinds of claims they're making.
[00:17:51.600 --> 00:17:55.440]   And then other people kind of putting out like kind of associative models and acting
[00:17:55.440 --> 00:17:59.600]   and writing in a way that seems to confuse whether they're associative or actually causal
[00:17:59.600 --> 00:18:04.780]   models in terms of the kinds of decisions they could plausibly guide.
[00:18:04.780 --> 00:18:09.140]   Or you know, you have sometimes people doing things like off policy RL, where you look
[00:18:09.140 --> 00:18:13.680]   at things like sepsis management or whatever, and you try to say, well, okay, can I like
[00:18:13.680 --> 00:18:17.540]   fit some kind of, you know, it's the same as like the RL problem.
[00:18:17.540 --> 00:18:21.700]   Like I've observed a bunch of trajectories sampled from one policy and then I fit a model
[00:18:21.700 --> 00:18:25.540]   and I make an estimate of what would, what sort of like average reward would I have gotten
[00:18:25.540 --> 00:18:26.920]   under this alternative policy.
[00:18:26.920 --> 00:18:32.120]   But being able to make that kind of statement is still subject to all kinds of assumptions
[00:18:32.120 --> 00:18:36.580]   that you need, you know, in causality, like that there's no confounding that the past
[00:18:36.580 --> 00:18:41.720]   treatment decisions are not actually influenced by any variables that you yourself don't observe
[00:18:41.720 --> 00:18:44.100]   that also influence the outcome.
[00:18:44.100 --> 00:18:47.380]   So like all these kinds of things that people start talking about guiding decisions, making
[00:18:47.380 --> 00:18:52.860]   better treatment decisions, inferring all these kinds of things from observational data.
[00:18:52.860 --> 00:18:56.820]   I think there's a huge gap between the way people are talking and, you know, getting
[00:18:56.820 --> 00:18:59.780]   things into practice, but maybe those are the very most important things to actually
[00:18:59.780 --> 00:19:01.600]   be working on.
[00:19:01.600 --> 00:19:07.820]   And then you have like the kind of like easily coordinable, like ML pattern recognition problems.
[00:19:07.820 --> 00:19:12.700]   Like just can I look at an X-ray and say, you know, is it pneumonia or not?
[00:19:12.700 --> 00:19:18.220]   Can I look at a mammogram and say, should they be recalled or not for diagnostics?
[00:19:18.220 --> 00:19:22.140]   And so where does this sort of like time series analysis stuff that you were talking about
[00:19:22.140 --> 00:19:23.540]   in the beginning fit into that?
[00:19:23.540 --> 00:19:28.740]   Like is that like at a point where it's, you know, like a tool a doctor could use?
[00:19:28.740 --> 00:19:32.980]   For example, the first big paper that we did on this is when we published at ICLR, which
[00:19:32.980 --> 00:19:34.980]   is learning to diagnose with LSTM RNN.
[00:19:34.980 --> 00:19:40.180]   So we're feeding in the time series and predicting which diagnoses apply to this patient.
[00:19:40.180 --> 00:19:44.020]   So I think you could paint a story that's like not totally crazy about how this could
[00:19:44.020 --> 00:19:45.100]   potentially be useful.
[00:19:45.100 --> 00:19:47.860]   And one example would be, hey, I have a new patient.
[00:19:47.860 --> 00:19:50.420]   I have them, there's some kind of emergency.
[00:19:50.420 --> 00:19:54.060]   I have the patient, I have them hooked up, I have them, I'm recording data.
[00:19:54.060 --> 00:19:57.840]   I want to be able, if I'm not sure what the diagnosis is, it would be nice to be able
[00:19:57.840 --> 00:19:59.220]   to have like a short list.
[00:19:59.220 --> 00:20:00.220]   So that's part of how we evaluate.
[00:20:00.220 --> 00:20:04.300]   It was like, you know, I could look at what the machine thinks are the 10 most likely
[00:20:04.300 --> 00:20:05.300]   diagnoses.
[00:20:05.300 --> 00:20:09.380]   And I could say, okay, I'm going to make sure that I check, that I include these things
[00:20:09.380 --> 00:20:10.740]   in the differential or something.
[00:20:10.740 --> 00:20:13.500]   It would be like some kind of sanity check.
[00:20:13.500 --> 00:20:18.500]   Like you're using the machine as like a wide pass to just make sure that you're considering
[00:20:18.500 --> 00:20:19.500]   the right diagnosis.
[00:20:19.500 --> 00:20:23.740]   Now, is that actually, is that actually useful directly?
[00:20:23.740 --> 00:20:26.860]   Like in its form, you know what I mean?
[00:20:26.860 --> 00:20:30.600]   Like there's a question of could that in general, that kind of idea work and is this
[00:20:30.600 --> 00:20:33.560]   sort of maybe a proof of concept that it's plausible?
[00:20:33.560 --> 00:20:38.480]   I think we can maybe make that kind of argument, but in terms of like for these specific cohort,
[00:20:38.480 --> 00:20:42.080]   like for the patients in the ICU, is this really something where what we did is directly
[00:20:42.080 --> 00:20:43.080]   useful?
[00:20:43.080 --> 00:20:51.760]   I think, you know, I think there's no way you can have like kind of proper, like make,
[00:20:51.760 --> 00:20:54.960]   you know, just, I don't think it was, you know, I think you have to really like lack
[00:20:54.960 --> 00:20:59.560]   humility to kind of go out there and just kind of say like, you know, in an unqualified
[00:20:59.560 --> 00:21:01.400]   way, like this is actually useful in practice.
[00:21:01.400 --> 00:21:02.960]   I think probably not.
[00:21:02.960 --> 00:21:07.040]   Like I think for a lot of those patients, basically, you know, we're able to demonstrate
[00:21:07.040 --> 00:21:13.480]   this technology is capable of recognizing these facts about these patients, but in reality,
[00:21:13.480 --> 00:21:16.280]   you know, the diagnosis for a lot of these patients was already known.
[00:21:16.280 --> 00:21:19.760]   We're just showing that we can figure out what it was from certain trajectories, certain
[00:21:19.760 --> 00:21:21.960]   traces, certain measurements.
[00:21:21.960 --> 00:21:26.220]   But if the doctor already knows the diagnosis, what do we really do to improve care?
[00:21:26.220 --> 00:21:30.800]   And I think, you know, this is sort of how, you know, my research has maybe evolved as
[00:21:30.800 --> 00:21:32.920]   I started off maybe asking a lot more of these.
[00:21:32.920 --> 00:21:36.440]   The interesting thing was representation learning and like, can we just do anything useful with
[00:21:36.440 --> 00:21:39.060]   these types of weird looking data?
[00:21:39.060 --> 00:21:42.400]   You know, like the standard thing you remember from like the early 2000s or whatever, it
[00:21:42.400 --> 00:21:45.600]   was like always like find a way to represent whatever you're working with is like a fixed
[00:21:45.600 --> 00:21:50.800]   length vector and then feed it into like, you know, menu of scikit-learn models or whatever
[00:21:50.800 --> 00:21:52.360]   and see what comes out.
[00:21:52.360 --> 00:21:57.160]   It was exciting to say, could we do with things like, could we actually get signal out of
[00:21:57.160 --> 00:22:02.320]   these varying length time series with these, you know, weird missingness patterns and whatever.
[00:22:02.320 --> 00:22:09.280]   But you know, at some point, okay, like the representation learning thing has like happened
[00:22:09.280 --> 00:22:13.520]   and we know that we can do this and it's, there's less things that are like truly exciting
[00:22:13.520 --> 00:22:17.880]   there because we sort of know how to, we have a good set of tools between sequence models
[00:22:17.880 --> 00:22:21.840]   and conv nets and graph convolutions, et cetera, for representing kind of various sorts of
[00:22:21.840 --> 00:22:22.840]   exotic objects.
[00:22:22.840 --> 00:22:25.800]   And that's no longer maybe to me the most exciting thing.
[00:22:25.800 --> 00:22:29.880]   So the most exciting thing is, okay, we can do function fitting.
[00:22:29.880 --> 00:22:31.040]   Let's say we can do function fitting.
[00:22:31.040 --> 00:22:34.060]   Let's say we even believe that we've solved function fitting.
[00:22:34.060 --> 00:22:35.760]   Like what's next?
[00:22:35.760 --> 00:22:41.300]   Like that doesn't get us to like the AI doctor that gets us to like, maybe we've solved like
[00:22:41.300 --> 00:22:43.120]   retinal fundus imaging.
[00:22:43.120 --> 00:22:48.320]   But like for the most part, you know, here's another problem to just like poop on my own
[00:22:48.320 --> 00:22:49.360]   work a little bit more.
[00:22:49.360 --> 00:22:54.960]   And like what we all do is one thing that we often do is we make these statements about,
[00:22:54.960 --> 00:22:57.980]   you know, what is human level performance on some task.
[00:22:57.980 --> 00:23:00.840]   But we often don't think like about the wider scope.
[00:23:00.840 --> 00:23:04.920]   Like we were sort of myopically focused on, like in ML you're really told, like I've got,
[00:23:04.920 --> 00:23:08.320]   I've got my inputs, I've got my outputs, I've got my loss function.
[00:23:08.320 --> 00:23:12.160]   And then like the, the, the room inside there, that's where you dance.
[00:23:12.160 --> 00:23:15.880]   But, you know, think about the diagnosis problem.
[00:23:15.880 --> 00:23:19.920]   This is like an example I like to give my students is, you know, the, the way we cast
[00:23:19.920 --> 00:23:25.520]   the diagnosis problem in ML is like, given all this measured data, can you infer, you
[00:23:25.520 --> 00:23:30.540]   know, more accurately or as accurately as the human, what is the applicable diagnosis?
[00:23:30.540 --> 00:23:32.320]   But was that ever the hard part?
[00:23:32.320 --> 00:23:36.120]   Like, you know, like the extreme examples, like if the doctor gives you the test for
[00:23:36.120 --> 00:23:39.640]   like Lyme disease and the result is positive, the fact that the machine can more reliably
[00:23:39.640 --> 00:23:43.720]   look at the data that contains that fact and say you have, you know, that's, that's an
[00:23:43.720 --> 00:23:45.040]   extreme example, but you get the point.
[00:23:45.040 --> 00:23:49.840]   It's like, given that you were already like kind of routed to the right kind of care and
[00:23:49.840 --> 00:23:53.520]   had the right measurements done and whatever.
[00:23:53.520 --> 00:23:56.560]   You know, maybe the machine is good at doing inference about what you have, but maybe that
[00:23:56.560 --> 00:23:57.840]   was never the interesting part.
[00:23:57.840 --> 00:23:58.840]   It was never the hard part.
[00:23:58.840 --> 00:24:02.840]   That was never the part that really demanded that you need a human in the loop.
[00:24:02.840 --> 00:24:06.880]   The hard part was seeing a patient, you have no data about them and you have to make these
[00:24:06.880 --> 00:24:08.600]   hard decision problems.
[00:24:08.600 --> 00:24:10.760]   Like decisions are not just about treatments.
[00:24:10.760 --> 00:24:13.200]   There's also decisions about information revelation.
[00:24:13.200 --> 00:24:16.760]   So that's something we focus on a lot in the lab now is these like weird problems where
[00:24:16.760 --> 00:24:19.520]   the decision is what to observe.
[00:24:19.520 --> 00:24:22.640]   You know, like I want to estimate, I want to ask them, figure out what is the best drug
[00:24:22.640 --> 00:24:23.640]   to treat some patient.
[00:24:23.640 --> 00:24:25.960]   I got a bunch of people coming in.
[00:24:25.960 --> 00:24:29.240]   I can, I can run some tests, but I can't run every test for every patient.
[00:24:29.240 --> 00:24:31.780]   I could try some treatments, I can't run every treatment for every patient.
[00:24:31.780 --> 00:24:35.100]   So like if I were to cast this kind of problem, you can make it really abstract.
[00:24:35.100 --> 00:24:38.600]   You could just say like, I've got some kind of set of variables, they're related by some
[00:24:38.600 --> 00:24:39.820]   causal graph.
[00:24:39.820 --> 00:24:43.560]   In every time step you get to observe some subset of them and you get to, you have some
[00:24:43.560 --> 00:24:48.400]   budget that, you know, constrains like which ones you can, you know, intervene on.
[00:24:48.400 --> 00:24:54.700]   But the point being that it's like the set of data you observe not being taken, like,
[00:24:54.700 --> 00:24:58.660]   you know, just like by God, you know, given to you as, as something that you take for
[00:24:58.660 --> 00:25:05.160]   granted, but rather, you know, sort of widening the scope of what we consider to be like,
[00:25:05.160 --> 00:25:11.440]   sort of like our jurisdiction as like people thinking about decision making and automation.
[00:25:11.440 --> 00:25:17.320]   Well, I totally, I mean, I'm obviously a big fan of that area of research because I do
[00:25:17.320 --> 00:25:23.300]   think in practical applications, you know, you do actually have some control over those
[00:25:23.300 --> 00:25:26.240]   things like what, you know, what data you want to collect and how you want to collect
[00:25:26.240 --> 00:25:27.240]   it.
[00:25:27.240 --> 00:25:32.540]   So, I think it's a, it's a messier research problem, but probably more directly useful
[00:25:32.540 --> 00:25:37.380]   in a lot of cases, just because the, the sort of function fitting stuff is so well studied,
[00:25:37.380 --> 00:25:42.320]   you know, relative to the impact that it can have.
[00:25:42.320 --> 00:25:46.980]   It's also more like, sometimes, like, I think there's a way that people, I think people
[00:25:46.980 --> 00:25:50.740]   have like a, you've seen this before, you were like Stanford math or something like,
[00:25:50.740 --> 00:25:54.700]   you've seen like the kind of weird like hierarchies that people form of like, certain within a
[00:25:54.700 --> 00:25:58.580]   discipline and this idea of like, okay, there's like, the mathematicians are on top of the
[00:25:58.580 --> 00:26:01.580]   physicists are on top of the chemists are on top of the biologists are on top of the
[00:26:01.580 --> 00:26:02.580]   applied whatever, whatever.
[00:26:02.580 --> 00:26:06.080]   And like, this thing happens in ML a little bit with like theory and application where
[00:26:06.080 --> 00:26:08.160]   people kind of get snooty.
[00:26:08.160 --> 00:26:12.700]   And I think one thing that's weird is that there's like two axes that get collapsed there
[00:26:12.700 --> 00:26:18.220]   of like theory and application and like theory and or like mathematics and empiricism, like
[00:26:18.220 --> 00:26:23.360]   different mode of inquiry versus like method versus real world.
[00:26:23.360 --> 00:26:28.760]   And I actually think that like, that's like, maybe it's true that like theory is somehow
[00:26:28.760 --> 00:26:32.860]   more philosophically interesting than just like benchmark applications, like then just
[00:26:32.860 --> 00:26:34.980]   empirical pursuit on methods.
[00:26:34.980 --> 00:26:36.380]   But the application is like a different access.
[00:26:36.380 --> 00:26:40.620]   I actually think that like, the applications are super philosophically interesting.
[00:26:40.620 --> 00:26:43.380]   Like they, I mean, they force you to ask, because they ask you to ask questions that
[00:26:43.380 --> 00:26:48.100]   are just like mechanical, you have to have to ask the normative questions, right?
[00:26:48.100 --> 00:26:51.680]   Like, like the thing that I think is exciting about applications is that nobody told you
[00:26:51.680 --> 00:26:54.020]   in the first place, what is worth predicting?
[00:26:54.020 --> 00:27:00.940]   Like that by itself, like convincing someone that like, this is actually a problem worth
[00:27:00.940 --> 00:27:01.940]   solving.
[00:27:01.940 --> 00:27:09.660]   Well, it's funny, I was just reading one of your papers that you pointed me to on essentially
[00:27:09.660 --> 00:27:11.500]   learn like collecting more data.
[00:27:11.500 --> 00:27:14.780]   The way I would describe it is like, it's about kind of collecting more data to get
[00:27:14.780 --> 00:27:18.900]   the model to learn the things that you want or the connections you want versus the sort
[00:27:18.900 --> 00:27:20.340]   of like spurious connections.
[00:27:20.340 --> 00:27:25.620]   Like you had a good example of, you know, like models predicting like seagulls because
[00:27:25.620 --> 00:27:27.380]   they see the beach.
[00:27:27.380 --> 00:27:31.940]   And you know, we sort of assume, you make this point that's evocative of like, we assume
[00:27:31.940 --> 00:27:37.140]   that that's bad, but it's kind of hard to articulate exactly what's bad about it.
[00:27:37.140 --> 00:27:41.900]   I guess it hurts you in generalization maybe, but if it doesn't hurt you in your data set,
[00:27:41.900 --> 00:27:46.340]   it's hard to, it's probably hard to distill what's bad about that.
[00:27:46.340 --> 00:27:47.340]   Right.
[00:27:47.340 --> 00:27:54.140]   There's papers out there that are saying like, right, like sort of like using, say the model's
[00:27:54.140 --> 00:28:00.580]   bias or the model depends on superficial patterns or spurious patterns or whatever, without
[00:28:00.580 --> 00:28:04.940]   any kind of clear sense of like, what technically do they mean?
[00:28:04.940 --> 00:28:10.020]   And right, what we get at that is trying to say, here's something that I think causality
[00:28:10.020 --> 00:28:11.020]   has to offer.
[00:28:11.020 --> 00:28:14.940]   I think a lot of people talk about causal inference, like really focused on the wrong
[00:28:14.940 --> 00:28:15.940]   thing.
[00:28:15.940 --> 00:28:16.940]   Like thinking like, is it useful or is it not useful?
[00:28:16.940 --> 00:28:21.220]   Like, can I take, you know, like the Pearl machinery and go apply it on the real data
[00:28:21.220 --> 00:28:22.620]   and estimate it and get the number.
[00:28:22.620 --> 00:28:24.980]   And economists are like, I think more like focused on that.
[00:28:24.980 --> 00:28:25.980]   Like, can I get the number?
[00:28:25.980 --> 00:28:26.980]   Can I estimate it?
[00:28:26.980 --> 00:28:30.620]   But I think like one thing that that's nice about sort of like Pearl's perspective, and
[00:28:30.620 --> 00:28:37.500]   I think that is really important is that it's like causality is not just useful because
[00:28:37.500 --> 00:28:39.580]   of you can actually estimate the cause of fight.
[00:28:39.580 --> 00:28:43.460]   It's important because like you can coherently express the kinds of questions that you actually
[00:28:43.460 --> 00:28:44.700]   care about.
[00:28:44.700 --> 00:28:49.140]   And even if, you know, at least within that, like you can have a way of making coherent
[00:28:49.140 --> 00:28:50.140]   statements about things.
[00:28:50.140 --> 00:28:54.020]   Like, so in this case, like it gives us the vocabulary of to say like, what, what don't
[00:28:54.020 --> 00:29:00.300]   in what sense is it wrong to depend upon the like the beach when saying this is a seagull
[00:29:00.300 --> 00:29:03.540]   is that like, it's not what causes it to be a seagull.
[00:29:03.540 --> 00:29:04.540]   Right.
[00:29:04.540 --> 00:29:09.060]   Or, or I think like an example that I like a lot of times is like, why is it not right
[00:29:09.060 --> 00:29:14.660]   to base lending decisions for who you give a loan to on what shoes they're wearing?
[00:29:14.660 --> 00:29:21.380]   And so part of it could be that like, you know, that you know, something about how like
[00:29:21.380 --> 00:29:22.700]   shoes relate to finances.
[00:29:22.700 --> 00:29:26.220]   Like you know, something about the structure of the universe and you're able to think in
[00:29:26.220 --> 00:29:29.180]   your head, what happens if I intervene on your shoes?
[00:29:29.180 --> 00:29:32.000]   You know, if I take someone and I intervene on their shoes, cause you know, people can
[00:29:32.000 --> 00:29:33.340]   intervene on their shoes, right?
[00:29:33.340 --> 00:29:36.820]   They can, you know, if everyone who wears Oxford's to the bank gets a loan and everyone
[00:29:36.820 --> 00:29:43.540]   who wears sneakers, doesn't people will intervene and say, is this a reasonable procedure?
[00:29:43.540 --> 00:29:46.940]   One reason why you say, I, this is why I don't want, why I want it to depend on this or not
[00:29:46.940 --> 00:29:52.300]   on that is to say, you know, what would be, I can do this counterfactual kind of simulation
[00:29:52.300 --> 00:29:54.840]   and say, what would happen were I to intervene on that?
[00:29:54.840 --> 00:29:56.380]   Would this change your ability to pay?
[00:29:56.380 --> 00:29:59.660]   Would this change the applicability of the label and the image?
[00:29:59.660 --> 00:30:00.660]   Right.
[00:30:00.660 --> 00:30:03.940]   So I think like for us, like the big insight is just to think of it kind of coherently
[00:30:03.940 --> 00:30:10.300]   as like, think of like semantics as, as actually like, you know, uh, sort of being a causal
[00:30:10.300 --> 00:30:11.300]   in a way.
[00:30:11.300 --> 00:30:14.020]   Like what causes the label to apply?
[00:30:14.020 --> 00:30:19.220]   Um, and, and, and, and then it becomes maybe well-defined, right?
[00:30:19.220 --> 00:30:24.340]   Because, um, I mean, the benefit that we have is that in our paper is the learning, the
[00:30:24.340 --> 00:30:26.960]   difference that makes the difference paper.
[00:30:26.960 --> 00:30:28.540]   We actually have humans in the loop.
[00:30:28.540 --> 00:30:34.100]   So we're actually, we're saying, Hey, this is something that may or may not be, uh, actually
[00:30:34.100 --> 00:30:39.380]   identifiable from the observational data alone, but it's something that we can get via the
[00:30:39.380 --> 00:30:40.380]   annotators.
[00:30:40.380 --> 00:30:45.540]   So they're revealing to us, like with this example about, uh, genre and in movies, right?
[00:30:45.540 --> 00:30:50.940]   So if you train a classifier on, uh, to predict like sentiment on IMDb movie reviews, you
[00:30:50.940 --> 00:30:54.180]   find that like top positive words, you know, you do something like just train a linear
[00:30:54.180 --> 00:30:58.140]   model and look at like the high magnitude, positive coefficients for its negative, the
[00:30:58.140 --> 00:31:02.140]   high positive ones would be like fantastic, excellent, whatever negative ones are terrible,
[00:31:02.140 --> 00:31:06.340]   awful, but the positive ones also have like romance and the negative one also has horror.
[00:31:06.340 --> 00:31:07.340]   And you're like, that's wrong.
[00:31:07.340 --> 00:31:08.340]   And like, why is it wrong?
[00:31:08.340 --> 00:31:10.940]   And it's like, because then like Jordan Peele comes out of nowhere and starts making all
[00:31:10.940 --> 00:31:16.180]   these great horror movies and your models inferring that they're bad because, uh, it's
[00:31:16.180 --> 00:31:22.020]   like kind of, um, you know, depending upon this thing that this, this signal, it's not
[00:31:22.020 --> 00:31:23.860]   durable over time.
[00:31:23.860 --> 00:31:28.460]   I was thinking in those, in that example though, that, um, I think romance movies are generally
[00:31:28.460 --> 00:31:32.740]   better than horror movies and maybe, you know, the average human agrees with me.
[00:31:32.740 --> 00:31:35.460]   So there is some sort of, um, right.
[00:31:35.460 --> 00:31:38.380]   That's a, that's a, that's an associative statement, right?
[00:31:38.380 --> 00:31:40.380]   You're saying they are generally better.
[00:31:40.380 --> 00:31:45.060]   And that actually does seem to be what the general public agrees with, right?
[00:31:45.060 --> 00:31:47.100]   The problem isn't, are they generally better?
[00:31:47.100 --> 00:31:50.580]   It's um, does it have to be that way?
[00:31:50.580 --> 00:31:51.580]   Right.
[00:31:51.580 --> 00:31:56.780]   Is this like, could you imagine a world in which tastes shift and the talented movie
[00:31:56.780 --> 00:32:01.100]   makers, uh, really shun, uh, romance movies and they become bad.
[00:32:01.100 --> 00:32:03.740]   I mean, there's a sort of embedded assumption here.
[00:32:03.740 --> 00:32:05.380]   It's something that we're looking into a lot now.
[00:32:05.380 --> 00:32:08.500]   And for anyone in the audience who's really interested, there's, there's a lot of great
[00:32:08.500 --> 00:32:14.060]   work by, by a scholar named Jonas Peters, who's maybe more of a theoretician, but approaches
[00:32:14.060 --> 00:32:15.060]   these problems.
[00:32:15.060 --> 00:32:19.220]   And you know, there's questions about, you say, you know, partly the question, like one
[00:32:19.220 --> 00:32:22.340]   way of motivating us is you think about robustness out of domain.
[00:32:22.340 --> 00:32:25.420]   You say like in, you know, when, when I go out into the rest of the world, is it always
[00:32:25.420 --> 00:32:29.340]   going to be true that romance is good and horror is bad.
[00:32:29.340 --> 00:32:32.420]   If I go to a different culture, do I expect that like that can, you know, if I can move
[00:32:32.420 --> 00:32:36.500]   through a different state, do I expect that like, this is the durable part.
[00:32:36.500 --> 00:32:41.660]   And so, you know, one, one kind of like assumption here is that the things that caused the label
[00:32:41.660 --> 00:32:46.380]   to apply, but that these things are that this relationship is actually stable.
[00:32:46.380 --> 00:32:53.580]   So you can imagine that like the things that, that actually signal positivity versus negativity
[00:32:53.580 --> 00:32:58.820]   in a document that this is relatively stable over years, but there's a complicated relationship
[00:32:58.820 --> 00:33:04.820]   in the background that influences is the perceived sentiment positive, is the movie quality
[00:33:04.820 --> 00:33:05.820]   high?
[00:33:05.820 --> 00:33:06.820]   You know, what is the budget of the movie?
[00:33:06.820 --> 00:33:08.820]   What are the, what is in vogue?
[00:33:08.820 --> 00:33:10.740]   Like, what are the houses spending money on?
[00:33:10.740 --> 00:33:11.740]   What are the publishers spending money on?
[00:33:11.740 --> 00:33:13.500]   What's getting distributed, whatever.
[00:33:13.500 --> 00:33:18.740]   But these things are all changing, but that the causal features are, are, are, are the
[00:33:18.740 --> 00:33:22.540]   those like, you know, you can think of like, if there's a structural equation that like
[00:33:22.540 --> 00:33:29.020]   says, like, you know, what, what is the perceived sentiment, you know, from, you know, the text
[00:33:29.020 --> 00:33:33.020]   that like that, that that thing is actually relatively stable over time compared to these
[00:33:33.020 --> 00:33:34.020]   other features.
[00:33:34.020 --> 00:33:36.900]   And so, and, and that's part of our empirical validation.
[00:33:36.900 --> 00:33:38.140]   And so we have this model, right?
[00:33:38.140 --> 00:33:42.980]   We show that what we've essentially get people to do is to rewrite the document.
[00:33:42.980 --> 00:33:47.320]   They're told to make a sort of a minimal edit, but it should alter the document such that
[00:33:47.320 --> 00:33:49.260]   it accords with the counterfactual labels.
[00:33:49.260 --> 00:33:51.220]   So it was originally a positive review.
[00:33:51.220 --> 00:33:55.620]   We say edit the review without making any gratuitous edits such that it is now a negative
[00:33:55.620 --> 00:33:56.620]   review.
[00:33:56.620 --> 00:34:01.860]   And when they do that, you wind up with a new dataset where for every original review
[00:34:01.860 --> 00:34:05.980]   that you know, had horror in it with, and was positive.
[00:34:05.980 --> 00:34:11.420]   Now there's a sort of bizarro counterpart and it still has horror in it.
[00:34:11.420 --> 00:34:14.620]   And the reason why it has horror in it is because of the instructions.
[00:34:14.620 --> 00:34:17.300]   The instruction said, don't make gratuitous edits.
[00:34:17.300 --> 00:34:20.820]   Like don't change facts that like are not material to this element.
[00:34:20.820 --> 00:34:25.700]   And so this is something that like, you can, we can argue about whether it's actually statistically
[00:34:25.700 --> 00:34:30.980]   possible to have disentangled that horror is, versus isn't a causal feature without
[00:34:30.980 --> 00:34:32.720]   that intervention.
[00:34:32.720 --> 00:34:36.660]   But once we have this document, we say all the horror movies still contain horror, but
[00:34:36.660 --> 00:34:38.460]   their labels have been flipped.
[00:34:38.460 --> 00:34:42.440]   All the romance movies still contain romance, but their label has been flipped.
[00:34:42.440 --> 00:34:46.220]   Because other parts of the document, the ones that like, you know, actually needed to change
[00:34:46.220 --> 00:34:49.240]   in order to flip the applicability, the label have been changed.
[00:34:49.240 --> 00:34:54.100]   So if you train the model on the counterfactually revised data, you find that like the coefficients
[00:34:54.100 --> 00:34:55.100]   flip.
[00:34:55.100 --> 00:34:59.740]   So like excellent and fantastic are still positive words, but now horror is also a super
[00:34:59.740 --> 00:35:03.520]   positive word and like terrible and awful are still negative words, but romance becomes
[00:35:03.520 --> 00:35:05.260]   a really negative word.
[00:35:05.260 --> 00:35:10.380]   And the cool findings, if you combine these two datasets together and train on them, they
[00:35:10.380 --> 00:35:11.900]   kind of like wash each other out.
[00:35:11.900 --> 00:35:15.260]   And so you find that like all of the things that look like they don't belong on these
[00:35:15.260 --> 00:35:19.100]   like lists of important features actually seem to kind of fall off.
[00:35:19.100 --> 00:35:22.260]   So we're dealing with causality here in maybe a more gestural way.
[00:35:22.260 --> 00:35:27.100]   We're not using like the mechanics of, we're not using like the mathematical machinery
[00:35:27.100 --> 00:35:31.260]   of like graph identifiability or anything like that.
[00:35:31.260 --> 00:35:36.540]   But we are getting this interesting kind of really suggestive result on real data.
[00:35:36.540 --> 00:35:39.260]   And when we look at it, you know, just to that last point that we were talking about
[00:35:39.260 --> 00:35:44.260]   with generalizing out of domain and are the causal connections durable, one thing that
[00:35:44.260 --> 00:35:48.260]   we looked at in the camera ready version of that paper is we say, okay, we trained it
[00:35:48.260 --> 00:35:49.380]   on IMDB.
[00:35:49.380 --> 00:35:55.780]   Let's now evaluate it on Yelp, Amazon, et cetera, et cetera.
[00:35:55.780 --> 00:35:59.740]   And when you go to those other models, the model that was trained on the counterfactually
[00:35:59.740 --> 00:36:04.120]   augmented data, which is like the combination of the original and the revised does much
[00:36:04.120 --> 00:36:05.120]   better out of domain.
[00:36:05.120 --> 00:36:07.260]   Now this is not guaranteed to happen.
[00:36:07.260 --> 00:36:09.220]   Like this is like the short supports are not shared.
[00:36:09.220 --> 00:36:11.360]   There's a lot of funky things happening statistically here.
[00:36:11.360 --> 00:36:15.320]   But what I think is suggestive here is it's like, it does sort of agree with the intuition
[00:36:15.320 --> 00:36:19.780]   that you say on movie reviews, horror versus romance is like an important part of the pattern.
[00:36:19.780 --> 00:36:22.440]   Like that's a real clue.
[00:36:22.440 --> 00:36:26.160]   But once you start looking at like Amazon electronics or something, that's no longer
[00:36:26.160 --> 00:36:29.780]   actually maybe a durable pattern.
[00:36:29.780 --> 00:36:35.140]   You know, someone's like, oh my, you know, Discman was such a horror or something.
[00:36:35.140 --> 00:36:40.580]   I think like what I really liked about that paper was sometimes I feel like the, at least
[00:36:40.580 --> 00:36:45.060]   for me, some of the like highly theoretical papers kind of point out problems in there.
[00:36:45.060 --> 00:36:49.620]   They're kind of hard for me to even engage with because I don't like sort of see the
[00:36:49.620 --> 00:36:54.500]   practical effect, but you have actually such a, like a simple mechanism proposed here that
[00:36:54.500 --> 00:36:57.740]   actually worked in your case, which I thought was super cool.
[00:36:57.740 --> 00:37:06.220]   And I've noticed like in my 15 years of working with ML teams, a lot of teams naively intuit
[00:37:06.220 --> 00:37:08.460]   to do things like what you're saying.
[00:37:08.460 --> 00:37:09.740]   And they usually feel bad about it.
[00:37:09.740 --> 00:37:13.740]   Like they feel like they're kind of doing this like weird manipulation of the data to
[00:37:13.740 --> 00:37:18.740]   try to get it to generalize better by literally like, you know, often like rewriting the text
[00:37:18.740 --> 00:37:19.820]   in structured ways.
[00:37:19.820 --> 00:37:26.540]   And so, I don't know, I just really enjoyed the, it's a cool paper with a cool, you know,
[00:37:26.540 --> 00:37:29.420]   theoretical motivation that I think is really important, right.
[00:37:29.420 --> 00:37:32.740]   Of kind of like eliminating different types of bias and making things generalize better.
[00:37:32.740 --> 00:37:36.820]   But then also an interesting, like, you know, practical way of doing it.
[00:37:36.820 --> 00:37:42.180]   That kind of feels like, kind of, it's reminiscent of like active learning techniques and things,
[00:37:42.180 --> 00:37:44.780]   but you know, more and more interesting.
[00:37:44.780 --> 00:37:45.780]   Cool.
[00:37:45.780 --> 00:37:46.780]   Yeah.
[00:37:46.780 --> 00:37:47.780]   Thanks.
[00:37:47.780 --> 00:37:48.780]   It was fun to write.
[00:37:48.780 --> 00:37:52.780]   It was scary for a minute though, like, because we're like asking these like workers to do
[00:37:52.780 --> 00:37:56.820]   this weird kind of thing and like not sure if the results, you know, and it was sort
[00:37:56.820 --> 00:38:03.980]   of like a little bit of coin for, you know, relative to like, you know, the pot of discretionary
[00:38:03.980 --> 00:38:04.980]   funds at the time.
[00:38:04.980 --> 00:38:08.540]   So, it was sort of like, you know, there was like this mode of like, well, what the hell
[00:38:08.540 --> 00:38:11.660]   are we doing, you know.
[00:38:11.660 --> 00:38:13.620]   But yeah, it was nice that it worked out.
[00:38:13.620 --> 00:38:19.220]   I mean, I think that's like just mainly one of the differences between like a sort of
[00:38:19.220 --> 00:38:23.420]   like, you know, not to like get into like academia versus industry culture wars, but
[00:38:23.420 --> 00:38:29.060]   I think like saying that, like academia done right affords you is like, it's not like we
[00:38:29.060 --> 00:38:31.180]   need to get the product out or something.
[00:38:31.180 --> 00:38:36.140]   It's here's this, like, we have this thing we're after and it's like, we have, okay,
[00:38:36.140 --> 00:38:40.580]   you have that intuition of like that this mechanism might be interesting, but the next
[00:38:40.580 --> 00:38:42.820]   step isn't like just like do it or not do it.
[00:38:42.820 --> 00:38:48.460]   It's like that, like the ability to have a PhD student spend a lot of time to have like
[00:38:48.460 --> 00:38:53.060]   kind of arguments about this for a couple months of how do we want to do this, you know,
[00:38:53.060 --> 00:38:57.700]   agonize over the experiments, kind of go back to like, let's say we drew a toy causal model
[00:38:57.700 --> 00:38:59.780]   in our heads, like what does this correspond to?
[00:38:59.780 --> 00:39:03.660]   And so we have a lot of followup work coming from that now, but you know, like the fact
[00:39:03.660 --> 00:39:06.860]   that you get that, like, you know, for somebody it's like their full-time job for a year is
[00:39:06.860 --> 00:39:11.820]   like thinking really hard about a problem you can get from like, this is something kind
[00:39:11.820 --> 00:39:13.220]   of wacky, maybe let's try it.
[00:39:13.220 --> 00:39:17.940]   And then call it, you know, first, like, okay, now, now you, now this is like, this is your
[00:39:17.940 --> 00:39:20.900]   full-time job for a year is, you know, we're going to, we're going to think really hard
[00:39:20.900 --> 00:39:21.900]   about this one problem.
[00:39:21.900 --> 00:39:22.900]   Yeah.
[00:39:22.900 --> 00:39:23.900]   Yeah.
[00:39:23.900 --> 00:39:24.900]   That's super cool.
[00:39:24.900 --> 00:39:25.900]   I was kind of curious.
[00:39:25.900 --> 00:39:30.260]   I was looking at your, like another recent paper that, that you pointed me to that was
[00:39:30.260 --> 00:39:33.300]   like a little bit kind of harder, you know, for me to parse.
[00:39:33.300 --> 00:39:36.300]   Algorithmic fairness from a non-ideal perspective.
[00:39:36.300 --> 00:39:40.340]   Could you, could you describe what you're doing there?
[00:39:40.340 --> 00:39:41.420]   Yeah.
[00:39:41.420 --> 00:39:48.300]   So this is a paper with, so I actually have a postdoc in the philosophy department now.
[00:39:48.300 --> 00:39:52.140]   So he's working with me and David Banks.
[00:39:52.140 --> 00:39:58.980]   And this paper is really about, I guess in some sense, it sort of touches on the high-level
[00:39:58.980 --> 00:40:05.620]   theme of like identifiability, which is, you know, there's a lot of like well-founded concerns.
[00:40:05.620 --> 00:40:11.220]   Like if you're going to have decisions automated, these are decisions that in general are addressing
[00:40:11.220 --> 00:40:14.980]   problems that like are sort of like ethically consequential.
[00:40:14.980 --> 00:40:20.300]   Whether it's like bail decisions, lending decisions, hiring decisions, you know, mediating
[00:40:20.300 --> 00:40:27.180]   a flow of information, any of these decisions, all the normal questions that you have about
[00:40:27.180 --> 00:40:32.700]   and concerns that you have about fairness and equity and justice, you know, continue
[00:40:32.700 --> 00:40:33.700]   to apply.
[00:40:33.700 --> 00:40:37.860]   And I think as machine learning has gotten widely deployed, people have sort of become
[00:40:37.860 --> 00:40:39.700]   more and more aware of this.
[00:40:39.700 --> 00:40:43.540]   I think in 2015, I was like starting a blog or 2016 on this.
[00:40:43.540 --> 00:40:46.600]   It was sort of like, I thought it was like, didn't even know there was this community
[00:40:46.600 --> 00:40:48.020]   out there of like people working on it.
[00:40:48.020 --> 00:40:53.020]   There wasn't conferences like the fairness, accountability, transparency, or whatever.
[00:40:53.020 --> 00:40:56.060]   And now it's kind of blown up and it's blown up for a few reasons.
[00:40:56.060 --> 00:40:59.180]   But I think there've been a few like pivotal things that caught people's attention.
[00:40:59.180 --> 00:41:04.780]   Like one, there was like the hiring screening thing that was filtering out resumes by female
[00:41:04.780 --> 00:41:05.780]   candidates.
[00:41:05.780 --> 00:41:10.260]   Probably the biggest thing that caught people's attention was the ProPublica article about
[00:41:10.260 --> 00:41:11.260]   machine bias.
[00:41:11.260 --> 00:41:15.300]   This is talking about recidivism prediction models, is predicting who will get rearrested
[00:41:15.300 --> 00:41:17.540]   if released on bail.
[00:41:17.540 --> 00:41:24.100]   And so, you know, you have these systems and suddenly some, you know, there's basically
[00:41:24.100 --> 00:41:29.020]   the claim is these systems are being used to guide sentencing decisions or maybe like
[00:41:29.020 --> 00:41:31.180]   bail release decisions.
[00:41:31.180 --> 00:41:33.220]   And they're biased against black people.
[00:41:33.220 --> 00:41:35.960]   And like, this is obviously a big problem.
[00:41:35.960 --> 00:41:40.900]   And then immediately sort of people, you know, there sort of arose this crisis like, well,
[00:41:40.900 --> 00:41:41.900]   how do you quantify that?
[00:41:41.900 --> 00:41:43.780]   What is the quantity that says there's bias?
[00:41:43.780 --> 00:41:46.780]   So someone says, well, let's compare the false positive rates, compare the false negative
[00:41:46.780 --> 00:41:47.780]   rates.
[00:41:47.780 --> 00:41:51.260]   This whole kind of literature, let's compare just the fraction of people that are released
[00:41:51.260 --> 00:41:52.260]   on bail among all defendants.
[00:41:52.260 --> 00:41:56.180]   And you say, well, the distribution of crimes among defendants are maybe not the same.
[00:41:56.180 --> 00:42:00.540]   You have these metrics that are based on thresholds, but you're not necessarily considering like
[00:42:00.540 --> 00:42:01.860]   all aspects of the distribution.
[00:42:01.860 --> 00:42:05.820]   So people come back and like these kinds of criticisms and they're sort of like emerged
[00:42:05.820 --> 00:42:11.860]   this whole community that spends, you know, sort of like algorithmic fairness, which is
[00:42:11.860 --> 00:42:15.380]   looking at, you know, these kinds of problems and trying to say, what are formal ways we
[00:42:15.380 --> 00:42:16.420]   could define fairness?
[00:42:16.420 --> 00:42:21.020]   And so, you know, you might say the model should functionally behave equivalently, regardless
[00:42:21.020 --> 00:42:25.500]   of what your demographic is, fixing all your other data.
[00:42:25.500 --> 00:42:31.500]   And then the criticism against that is you say, well, you know, that that's meaningless
[00:42:31.500 --> 00:42:35.300]   because if you, if you withhold gender, but you have access to say all of my social media
[00:42:35.300 --> 00:42:39.780]   data or, you know, have some sufficiently rich set of covariates, someone's gender is
[00:42:39.780 --> 00:42:40.780]   captured there.
[00:42:40.780 --> 00:42:44.220]   So what does it mean to say that you just, that you didn't explicitly show that bit in
[00:42:44.220 --> 00:42:45.220]   the representation?
[00:42:45.220 --> 00:42:48.380]   Like the information's there, you have, so what does it mean to say it didn't impact
[00:42:48.380 --> 00:42:50.420]   your decision?
[00:42:50.420 --> 00:42:53.900]   And so there's this whole kind of line of work that's sort of trying to express this
[00:42:53.900 --> 00:42:55.740]   problem formally.
[00:42:55.740 --> 00:42:59.280]   And they're trying to express it in a world where everything is sort of defined statistically
[00:42:59.280 --> 00:43:03.020]   in a world where basically what we know is there's a set of covariates, which are just
[00:43:03.020 --> 00:43:06.180]   like some numbers, some distribution.
[00:43:06.180 --> 00:43:08.060]   There's a, we'll call that X there.
[00:43:08.060 --> 00:43:10.460]   There's a demographic indicator.
[00:43:10.460 --> 00:43:13.100]   It's like, are you in group A or in group B?
[00:43:13.100 --> 00:43:19.180]   There's the predictions by the model and there's the ground truth.
[00:43:19.180 --> 00:43:22.340]   And you know, it's just sort of like now trying to say, what are the kinds of parodies that
[00:43:22.340 --> 00:43:23.340]   we want to hold?
[00:43:23.340 --> 00:43:28.340]   You know, so maybe you say, I want an equal fraction of the population classified as positive,
[00:43:28.340 --> 00:43:31.180]   whether they're in group one or in group zero.
[00:43:31.180 --> 00:43:33.260]   I want the model that doesn't actually look at the demographic.
[00:43:33.260 --> 00:43:34.940]   I want them to have the same false positive rights.
[00:43:34.940 --> 00:43:36.420]   I want them to have the same false negative rights.
[00:43:36.420 --> 00:43:37.820]   I want to have the same both.
[00:43:37.820 --> 00:43:41.180]   I want to have, you know, so there's, there's, you know, people propose-
[00:43:41.180 --> 00:43:45.860]   I'm trying to like connect these to like, you know, as you say, like false positive,
[00:43:45.860 --> 00:43:48.060]   false negative, I'm just imagining the cases.
[00:43:48.060 --> 00:43:53.380]   I mean, can you say these in more like cases so people don't have to make that connection?
[00:43:53.380 --> 00:43:54.380]   Right.
[00:43:54.380 --> 00:43:59.340]   And actually like, this is sort of the kind of focus of a lot of our critique is that,
[00:43:59.340 --> 00:44:02.980]   you know, you could just describe the world in those terms and zoom out and start talking
[00:44:02.980 --> 00:44:04.820]   about various kinds of equations.
[00:44:04.820 --> 00:44:08.700]   And you could say a whole lot of things that seem intuitively plausible or reasonable.
[00:44:08.700 --> 00:44:13.300]   Like I want this to be equal to that, or I want the, you know, but it's like, what's
[00:44:13.300 --> 00:44:16.060]   missing from this whole thing, you know, it's like when people talk word to VEC, they say
[00:44:16.060 --> 00:44:20.020]   the word to VEC is biased, word to VEC is discriminatory, word to VEC is racist.
[00:44:20.020 --> 00:44:21.020]   What does that mean?
[00:44:21.020 --> 00:44:25.980]   What, what actually is even the category of objects which these statements apply?
[00:44:25.980 --> 00:44:30.660]   And you kind of realize really quickly that we've sort of abstracted so far away from
[00:44:30.660 --> 00:44:36.060]   the problems in that description that like, you actually, we don't have the relevant facts
[00:44:36.060 --> 00:44:37.060]   to say what is fair.
[00:44:37.060 --> 00:44:41.580]   So, you know, example would be if a model is being used to predict whether or not you're
[00:44:41.580 --> 00:44:45.540]   going to commit a crime and being falsely predicted as going to commit a crime means
[00:44:45.540 --> 00:44:49.180]   that you get, you know, you get denied bail or something, being predicted positive is
[00:44:49.180 --> 00:44:50.180]   really bad.
[00:44:50.180 --> 00:44:54.100]   If the model is trying to predict who's likely to, who condition on the, were they to be
[00:44:54.100 --> 00:44:55.940]   hired would be likely to get promoted.
[00:44:55.940 --> 00:45:00.900]   And it's using this to guide like resume screening or something like that, then like getting
[00:45:00.900 --> 00:45:02.300]   predicted positive is good.
[00:45:02.300 --> 00:45:06.020]   And so in one case, maybe you'd be concerned about false negative rates.
[00:45:06.020 --> 00:45:10.740]   If someone who really has the skill level being denied the opportunity for the job,
[00:45:10.740 --> 00:45:13.820]   in other case, you'd be concerned about false positive as someone who wouldn't commit a
[00:45:13.820 --> 00:45:19.340]   crime be flagged, but sort of lost in all that conversation also is whether or not something
[00:45:19.340 --> 00:45:24.900]   is like justice promoting or like justice, you know, whatever your sort of like normative
[00:45:24.900 --> 00:45:25.900]   positions are.
[00:45:25.900 --> 00:45:30.020]   I mean, you fix any set of like normative concerns that like, you know, you say, or
[00:45:30.020 --> 00:45:31.020]   define your morality.
[00:45:31.020 --> 00:45:35.420]   I would argue that, you know, even anywhere within kind of the normal spectrum there,
[00:45:35.420 --> 00:45:40.060]   there's still a problem that like these descriptions of the problems aren't sufficiently rich to
[00:45:40.060 --> 00:45:44.300]   sort of say what you should do, because the facts that are omitted are what actually is
[00:45:44.300 --> 00:45:45.940]   the problem I'm addressing.
[00:45:45.940 --> 00:45:50.700]   If there's disparities in the distributions initially, like what caused that to be if
[00:45:50.700 --> 00:45:54.020]   I'm making a decision, what actually is the impact of the decision?
[00:45:54.020 --> 00:45:55.020]   What is the impact?
[00:45:55.020 --> 00:45:59.580]   You know, how is it actually help or hurt people if I change this decision-making process?
[00:45:59.580 --> 00:46:06.500]   So an example might be, let's say you have a process that is determining like admissions
[00:46:06.500 --> 00:46:15.260]   to higher education, like in this case, intervening in a way that created more parity in the decisions,
[00:46:15.260 --> 00:46:21.100]   I'd argue, or create, you know, more kind of demographic diversity in the ultimate decision
[00:46:21.100 --> 00:46:23.660]   that I'd say is like a good thing.
[00:46:23.660 --> 00:46:27.820]   Now that's my sort of like normative position, maybe someone who's like not as progressive
[00:46:27.820 --> 00:46:31.340]   disagrees, but we can disagree about that.
[00:46:31.340 --> 00:46:34.780]   You know, even fixing my set of positions, if you change the situation, you say the issue
[00:46:34.780 --> 00:46:40.260]   is something like you're certifying surgeons or something, does subjecting someone to say
[00:46:40.260 --> 00:46:44.700]   a different standard across demographics actually help or hurt their careers?
[00:46:44.700 --> 00:46:48.900]   Like, you know, in this case, that might be a bad thing, because if you were to alter
[00:46:48.900 --> 00:46:55.260]   a decision-making process that was say like a safety certification, then you sort of maybe
[00:46:55.260 --> 00:47:01.740]   the reality, like the real world impact would be to sort of like almost like legitimize
[00:47:01.740 --> 00:47:04.500]   like discrimination further down the pipeline, right?
[00:47:04.500 --> 00:47:06.540]   Like where now patients are going to treat doctors differently because they know they
[00:47:06.540 --> 00:47:08.420]   were subjected to different tests.
[00:47:08.420 --> 00:47:13.420]   So there's these different decisions that have different kind of, but because of like
[00:47:13.420 --> 00:47:17.740]   what actually is the decision you're making and what actually is impacted with this decision,
[00:47:17.740 --> 00:47:21.220]   something that sort of looks from a technical perspective, like an identical problem could
[00:47:21.220 --> 00:47:26.940]   actually have a very different interpretation in terms of what is the sort of, you know,
[00:47:26.940 --> 00:47:29.340]   justice promoting policy to adopt.
[00:47:29.340 --> 00:47:34.700]   And so the concern is that by abstracting away from all those relevant details, you
[00:47:34.700 --> 00:47:37.420]   kind of lose sight of this.
[00:47:37.420 --> 00:47:41.940]   And what we ended up kind of finding, and this is really Sina gets credit for this,
[00:47:41.940 --> 00:47:45.340]   and I think a big contribution in this paper is really just making this nice connection
[00:47:45.340 --> 00:47:51.940]   across like a very wide interdisciplinary boundary is that this is sort of almost exactly
[00:47:51.940 --> 00:47:57.020]   in some ways, a recapitulation of a lot of arguments that have been, you know, for decades
[00:47:57.020 --> 00:47:59.640]   in the moral and political philosophy literature.
[00:47:59.640 --> 00:48:02.620]   And there you have sort of two approaches to sort of like theorizing, you have many
[00:48:02.620 --> 00:48:07.480]   approaches, but just like one of the axes of like differentiation and like how to theorize
[00:48:07.480 --> 00:48:11.580]   about these questions of justice is like the ideal versus the non-ideal approach.
[00:48:11.580 --> 00:48:15.060]   The ideal approach says, let's just imagine a perfect world and just say that things that
[00:48:15.060 --> 00:48:20.260]   hold in the perfect world, we should just, you know, fixate on some one of them and try
[00:48:20.260 --> 00:48:23.700]   to, you know, make our world look more like that.
[00:48:23.700 --> 00:48:28.980]   It's sort of saying, so you could think of the sort of like reason why this can go wrong
[00:48:28.980 --> 00:48:32.680]   is you can imagine, like, for example, this kind of theorizing has been used to sort of
[00:48:32.680 --> 00:48:37.820]   oppose a policy like affirmative action in a sort of blanket way where you just say,
[00:48:37.820 --> 00:48:40.800]   well, in the ideal world, we'd all be colorblind.
[00:48:40.800 --> 00:48:42.300]   So therefore we don't need affirmative action.
[00:48:42.300 --> 00:48:44.500]   Like that's, that's unjust.
[00:48:44.500 --> 00:48:47.740]   The non-ideal approach isn't someone who's like a more pragmatic way of looking at these
[00:48:47.740 --> 00:48:51.820]   sorts of problems where you sort of say, right.
[00:48:51.820 --> 00:48:57.140]   So among other things missing from the ideal approach is you don't say anything about what
[00:48:57.140 --> 00:49:02.020]   you say, how should someone behave in a world that is already in some kind of ideally just
[00:49:02.020 --> 00:49:06.580]   or fair state and where everyone else is completely compliant with what like justice demands of
[00:49:06.580 --> 00:49:09.620]   them and just like your job is to like not fuck it up.
[00:49:09.620 --> 00:49:14.460]   That's very different from the non-ideal approach where you're sort of saying, hey, I live in
[00:49:14.460 --> 00:49:19.100]   this world, there are existing disparities.
[00:49:19.100 --> 00:49:22.700]   Now given that I live in this world, given that there are these disparities, given that
[00:49:22.700 --> 00:49:27.360]   there's all these people who are bad actors out there, what is the justice promoting act?
[00:49:27.360 --> 00:49:30.540]   And to recognize that that's not necessarily the same thing.
[00:49:30.540 --> 00:49:34.980]   And then you have to be concerned with, well, like what are, you know, what, what are the
[00:49:34.980 --> 00:49:36.700]   disparities?
[00:49:36.700 --> 00:49:42.140]   Who has the right or the power or the like legitimate mandate to like intervene on what
[00:49:42.140 --> 00:49:44.820]   kinds of decisions to try to like rectify them?
[00:49:44.820 --> 00:49:48.340]   And then what are the policies that are actually effective?
[00:49:48.340 --> 00:49:52.980]   And so I guess, you know, you know, these questions become, if you remove those details,
[00:49:52.980 --> 00:49:54.260]   these questions become kind of vacuous.
[00:49:54.260 --> 00:49:58.060]   I'll give you an example would be higher education admissions.
[00:49:58.060 --> 00:50:01.540]   You know, so if you just say like, okay, well, we want to have a same fraction admitted among
[00:50:01.540 --> 00:50:02.540]   men and women.
[00:50:02.540 --> 00:50:04.860]   And you just say that, like, I think most of the people saying that aren't actually
[00:50:04.860 --> 00:50:08.660]   paying attention to the fact this is among what population, right?
[00:50:08.660 --> 00:50:13.340]   So if you were to look at like a typical like school, there's already a huge gender disparity
[00:50:13.340 --> 00:50:14.340]   in the application.
[00:50:14.340 --> 00:50:17.980]   So if you just accept people at the same rate, so, you know, there's all these, like if you
[00:50:17.980 --> 00:50:23.460]   take fix any one problem and you really start going deep, you see that there's all these
[00:50:23.460 --> 00:50:26.540]   other details that what is the right thing to do?
[00:50:26.540 --> 00:50:32.020]   What actually counts is like, you know, the fair decision making process hinges really
[00:50:32.020 --> 00:50:37.900]   like, you know, precariously on a bunch of facts that are not represented in like the
[00:50:37.900 --> 00:50:40.560]   problem descriptions.
[00:50:40.560 --> 00:50:44.740]   And so I think, you know, that's sort of our angle in this kind of critique is to sort
[00:50:44.740 --> 00:50:50.380]   of like cast a light on that, like, there's a sort of common saying in the fair ML world,
[00:50:50.380 --> 00:50:54.340]   like, oh, we have, you know, 72 definitions of fairness or something like this, like,
[00:50:54.340 --> 00:50:58.940]   look how many definitions and the kind of, you know, maybe TLDR is sort of like, we don't
[00:50:58.940 --> 00:51:04.180]   have 72 definitions, formal definitions of fairness, we have zero.
[00:51:04.180 --> 00:51:09.820]   And the reason why is because like you have 72 different sort of like fairness inspired
[00:51:09.820 --> 00:51:12.900]   like parody assertions.
[00:51:12.900 --> 00:51:18.620]   But the real actual question of fairness is sort of the question of what are the material
[00:51:18.620 --> 00:51:26.380]   facts that you need to make a determination about, you know, which apply in a given situation.
[00:51:26.380 --> 00:51:30.100]   When you look at the like the different topics in machine learning, is there one that you
[00:51:30.100 --> 00:51:35.620]   feel like people spend way little to time on, like one that like you just think like
[00:51:35.620 --> 00:51:39.220]   has way more impact proportionate to the amount of attention that people give it?
[00:51:39.220 --> 00:51:42.820]   Like, my only reluctance is that there are things that are sort of at least like the
[00:51:42.820 --> 00:51:45.860]   trajectories in the right direction, like people are paying more attention.
[00:51:45.860 --> 00:51:53.020]   But I think in general, like coming up with coherent ways of addressing, you know, problems
[00:51:53.020 --> 00:51:59.060]   like that are sort of beyond the IID setting is really key.
[00:51:59.060 --> 00:52:06.860]   And I would subsume under this, like both addressing causality and mechanism and also
[00:52:06.860 --> 00:52:09.620]   include like robustness under distribution shift.
[00:52:09.620 --> 00:52:14.020]   Like you have like one very narrow subset of distribution shift problems, which is like
[00:52:14.020 --> 00:52:19.140]   the minimax kind of like adversary setting where the adversary is able to basically have
[00:52:19.140 --> 00:52:23.940]   the same underlying distribution, but it's like the underlying, the samples are composed
[00:52:23.940 --> 00:52:28.580]   with some asshole who's able to like, you know, manipulate your data within the L infinity
[00:52:28.580 --> 00:52:29.580]   ball.
[00:52:29.580 --> 00:52:33.740]   So you've got like 4 million people working on that problem.
[00:52:33.740 --> 00:52:38.180]   But in the like broader set of like what are the kinds of structural assumptions that allow
[00:52:38.180 --> 00:52:40.140]   us to generalize under distribution shift area?
[00:52:40.140 --> 00:52:44.980]   I think we have maybe, you know, like this is the problem that plagues every single real
[00:52:44.980 --> 00:52:51.180]   world ML setting and that, you know, even among papers that like sort of say they're
[00:52:51.180 --> 00:52:54.620]   working on this problem, I think the vast majority of people don't seem to even understand
[00:52:54.620 --> 00:52:56.220]   like the basic concepts.
[00:52:56.220 --> 00:53:01.180]   I think it's like for this technology to actually be usable, I think we need to have like coherent
[00:53:01.180 --> 00:53:06.060]   principles under which we can make confident assessments about when it's going to be reliable
[00:53:06.060 --> 00:53:07.060]   and when it's not.
[00:53:07.060 --> 00:53:10.580]   And so I think, I mean, that's obviously a little bit biased maybe towards my research
[00:53:10.580 --> 00:53:13.580]   agenda, but I think that's...
[00:53:13.580 --> 00:53:15.300]   It really is.
[00:53:15.300 --> 00:53:18.700]   I mean, that is sort of, I guess that's sort of like the common sense, what it's done for
[00:53:18.700 --> 00:53:23.620]   how you should choose a problem is like you should pick something that you think is important
[00:53:23.620 --> 00:53:26.140]   and like underappreciated, not overappreciated.
[00:53:26.140 --> 00:53:27.620]   Yeah, fair enough.
[00:53:27.620 --> 00:53:30.420]   I think actually you should feel happy that you're in that situation.
[00:53:30.420 --> 00:53:34.500]   I think somehow it's not logical when people get stuck working on problems they don't think
[00:53:34.500 --> 00:53:39.220]   are the most important problem maybe, or at least based on some of the conversations we've
[00:53:39.220 --> 00:53:40.220]   had.
[00:53:40.220 --> 00:53:41.220]   All right.
[00:53:41.220 --> 00:53:42.260]   Part of that is people being lazy.
[00:53:42.260 --> 00:53:44.660]   Part of that is like the friction, right?
[00:53:44.660 --> 00:53:48.380]   It's like if you had a thing that you thought was important once and then you like built
[00:53:48.380 --> 00:53:54.020]   your lab around it and you got funding on it and your whole life revolves around maintaining
[00:53:54.020 --> 00:53:55.020]   like this research.
[00:53:55.020 --> 00:54:00.060]   So I think, I guess now that I'm running a big lab and now that I have finances to worry
[00:54:00.060 --> 00:54:02.980]   about and all that, I'm a little bit more appreciative of the handful of people out
[00:54:02.980 --> 00:54:07.620]   there who really sort of did make these like hard left turns at some point.
[00:54:07.620 --> 00:54:11.180]   Like I think Michael Jordan's a nice example of that.
[00:54:11.180 --> 00:54:15.060]   Like someone who's like kind of, you only can say he's like Miles Davis or something,
[00:54:15.060 --> 00:54:16.860]   but like, you know, it's like, okay.
[00:54:16.860 --> 00:54:22.820]   Each decade he had like neural networks, vision on parametrics.
[00:54:22.820 --> 00:54:26.220]   Now I guess it's like mechanisms and markets or whatever he's working on.
[00:54:26.220 --> 00:54:30.820]   Well, you've made quite a leap from music to deep learning, I think.
[00:54:30.820 --> 00:54:31.820]   Yeah.
[00:54:31.820 --> 00:54:32.820]   I think it's time for me to retire.
[00:54:32.820 --> 00:54:37.020]   I think five, six years is like the, that's the left turn.
[00:54:37.020 --> 00:54:38.020]   It's the left turn limit.
[00:54:38.020 --> 00:54:40.900]   So the final question, which I'm, I don't know.
[00:54:40.900 --> 00:54:41.900]   I have a mortgage now though.
[00:54:41.900 --> 00:54:42.900]   So it's a little bit, it's a little bit harder.
[00:54:42.900 --> 00:54:47.700]   And you, but you have a fancy computer behind you there.
[00:54:47.700 --> 00:54:49.620]   I don't know what.
[00:54:49.620 --> 00:54:55.340]   That's actually my power Mac from like 95, 97 maybe.
[00:54:55.340 --> 00:54:56.340]   Does it work?
[00:54:56.340 --> 00:54:57.340]   Oh yeah.
[00:54:57.340 --> 00:55:01.300]   Like I've still got, did you ever, did you have one?
[00:55:01.300 --> 00:55:02.300]   Something like that.
[00:55:02.300 --> 00:55:03.300]   Yeah.
[00:55:03.300 --> 00:55:04.300]   With like HyperCard and yeah.
[00:55:04.580 --> 00:55:08.340]   I think there's still like Oregon Trail and like Diamond, like all those like weird freeware
[00:55:08.340 --> 00:55:09.900]   games like MacSki.
[00:55:09.900 --> 00:55:12.900]   Oh yeah, MacSki.
[00:55:12.900 --> 00:55:13.900]   Epic.
[00:55:13.900 --> 00:55:16.580]   So final question.
[00:55:16.580 --> 00:55:23.300]   When you look at sort of taking machine models from the sort of like ideation or the sort
[00:55:23.300 --> 00:55:26.940]   of like research paper to actually deploy it in production, where do you see the biggest
[00:55:26.940 --> 00:55:29.380]   bottleneck or things falling apart the most?
[00:55:29.380 --> 00:55:33.140]   I think the biggest bottleneck is still problem formulation.
[00:55:33.140 --> 00:55:37.900]   Like if we were to be really sober of like most of the things that like people thought
[00:55:37.900 --> 00:55:38.980]   they were going to do.
[00:55:38.980 --> 00:55:42.340]   And then you look at like the way they propose them all the problem and then like the data
[00:55:42.340 --> 00:55:44.940]   they could actually collect and they're like, well, I think it produce and like, does this
[00:55:44.940 --> 00:55:48.500]   in any way actually address the problem that they thought they were going to?
[00:55:48.500 --> 00:55:53.100]   I think, I think those look like, I think those are, would be, you know, I don't know
[00:55:53.100 --> 00:55:57.220]   how you would collect that statistic and like, you know, there's some like measurement questions,
[00:55:57.220 --> 00:55:58.420]   but I think it would be like really depressing.
[00:55:58.420 --> 00:56:02.980]   It would be really sobering that I think most things people think they're going to do are
[00:56:02.980 --> 00:56:08.980]   either like kind of goofy and who knows if they work or just like not relevant.
[00:56:08.980 --> 00:56:09.980]   We'll never get used.
[00:56:09.980 --> 00:56:14.100]   And I think, I think that that's like figuring out like, where there's really a well motivated
[00:56:14.100 --> 00:56:18.700]   application of machine learning and what it is that there's like that weird, that weird
[00:56:18.700 --> 00:56:22.980]   next to like the things, the kind of pieces of information that you're asking people to
[00:56:22.980 --> 00:56:23.980]   put together.
[00:56:23.980 --> 00:56:28.140]   I think, I think this is why, like not, not to be like data scientists are great or whatever,
[00:56:28.140 --> 00:56:31.260]   like why, why I think like people who are really good at this job are like really hard
[00:56:31.260 --> 00:56:32.900]   to find in some way.
[00:56:32.900 --> 00:56:36.460]   And at the same time, that's, that's, it's kind of puzzling, right?
[00:56:36.460 --> 00:56:42.020]   Because I don't think that the great data scientists are in general great or even like
[00:56:42.020 --> 00:56:43.580]   rateable mathematicians, right?
[00:56:43.580 --> 00:56:47.900]   I think for the most part, there's people like actually touching data, mostly lousy
[00:56:47.900 --> 00:56:52.340]   mathematicians and they're usually not world-class engineers.
[00:56:52.340 --> 00:56:54.100]   I certainly am not.
[00:56:54.100 --> 00:56:55.100]   And like, what is it?
[00:56:55.260 --> 00:56:58.420]   And it's like, I think it's this weird combination of like the weakest link kills you.
[00:56:58.420 --> 00:57:02.940]   And like, you have to sort of see to be, I think, good at doing this applied work.
[00:57:02.940 --> 00:57:05.260]   Like what is the important problem?
[00:57:05.260 --> 00:57:09.900]   You have to also know like, what is the current technology like in the ballpark of being able
[00:57:09.900 --> 00:57:12.380]   to get you on this kind of problem?
[00:57:12.380 --> 00:57:15.660]   How does it match against the data that's available?
[00:57:15.660 --> 00:57:20.260]   And then I think you have to, at least at an intuitive level, do kind of this non-statistical
[00:57:20.260 --> 00:57:25.180]   thinking that's sort of like, what's actually the process where you're deploying it.
[00:57:25.180 --> 00:57:30.580]   So like the, right, with like the x-rays or whatever it was we were talking about, like
[00:57:30.580 --> 00:57:34.180]   the retinopathy imaging or something like this, this is sort of a good application of
[00:57:34.180 --> 00:57:37.260]   machine learning because what those images look like isn't changing over time.
[00:57:37.260 --> 00:57:42.060]   But you look at all these places in industry, people trying to like build recommender systems,
[00:57:42.060 --> 00:57:46.060]   do all these things where it's basically, it's like totally incoherent.
[00:57:46.060 --> 00:57:50.060]   Nobody has any idea what happens when you actually deploy it because the whole, you're
[00:57:50.060 --> 00:57:55.620]   modeling something as only like in the vaguest or weakest of ways actually related to like,
[00:57:55.620 --> 00:57:58.820]   you know, what you think you would like to be predicting.
[00:57:58.820 --> 00:58:01.860]   Like you're predicting clicks or whatever.
[00:58:01.860 --> 00:58:05.580]   You're predicting a condition on the previous set of exposures almost never with like any
[00:58:05.580 --> 00:58:08.780]   kind of coherent accounting for what happens when you deploy the model.
[00:58:08.780 --> 00:58:14.300]   I think like this obstacle is like people making that, like, I think this is always
[00:58:14.300 --> 00:58:19.900]   in some ways the hardest part of intellectual work in this area is the bindings.
[00:58:19.900 --> 00:58:24.340]   It's not the, you know, like the first level of difficulty is like technical skills.
[00:58:24.340 --> 00:58:25.340]   Like are you a good programmer?
[00:58:25.340 --> 00:58:26.340]   Are you a good engineer?
[00:58:26.340 --> 00:58:27.340]   Do you write proofs that are correct?
[00:58:27.340 --> 00:58:28.340]   Do you do whatever?
[00:58:28.340 --> 00:58:32.140]   But I think like the conceptual difficulty in working in machine learning is like, do
[00:58:32.140 --> 00:58:38.340]   you make the connection between like this abstraction that you possess and the world
[00:58:38.340 --> 00:58:43.980]   that like you're actually trying to somehow interact with?
[00:58:43.980 --> 00:58:48.420]   And that to me, I think often is where all kinds of things go off the rails.
[00:58:48.420 --> 00:58:51.420]   You know, I think where a lot of even like good academic work goes off the rails.
[00:58:51.420 --> 00:58:56.700]   It's like, you know, you can go down some rabbit holes asking like really like second,
[00:58:56.700 --> 00:59:01.220]   third order, like theoretical questions about these fairness things without ever asking
[00:59:01.220 --> 00:59:04.820]   of like, does this actually map onto any real world decision that I would want to make?
[00:59:04.820 --> 00:59:10.020]   Does it actually help someone with a problem that I like purport to be motivated by?
[00:59:10.020 --> 00:59:13.100]   So you know, I would just say that, you know, I mean, I don't know if that's like kind of
[00:59:13.100 --> 00:59:17.500]   a banal answer is like asking questions the right way or something, but.
[00:59:17.500 --> 00:59:18.500]   Sure.
[00:59:18.500 --> 00:59:20.940]   Well, thank you so much, Zach.
[00:59:20.940 --> 00:59:21.940]   Thanks for taking the extra time.
[00:59:21.940 --> 00:59:22.940]   That was super fun.
[00:59:22.940 --> 00:59:23.940]   Yeah, for sure.
[00:59:23.940 --> 00:59:23.940]   Thanks for having me.
[00:59:23.940 --> 00:59:24.940]   Yeah.
[00:59:24.940 --> 00:59:25.940]   For sure.
[00:59:25.940 --> 00:59:25.940]   Thanks for having me.
[00:59:25.940 --> 00:59:26.940]   Bye.
[00:59:26.940 --> 00:59:26.940]   Bye.
[00:59:26.940 --> 00:59:31.940]   Bye.
[00:59:31.940 --> 00:59:36.940]   Bye.
[00:59:36.940 --> 00:59:41.940]   Bye.
[00:59:41.940 --> 00:59:46.940]   Bye.
[00:59:46.940 --> 00:59:51.940]   Bye.

