
[00:00:00.000 --> 00:00:02.000]   -
[00:00:02.000 --> 00:00:16.180]   Amazing, I think there's still some people
[00:00:16.180 --> 00:00:17.840]   coming in toward the back.
[00:00:17.840 --> 00:00:19.100]   So I'll do like the quick,
[00:00:19.100 --> 00:00:22.000]   maybe like administrative stuff in the meantime.
[00:00:22.000 --> 00:00:23.640]   This is gonna be super interactive.
[00:00:23.640 --> 00:00:24.760]   There's also like mics around,
[00:00:24.760 --> 00:00:28.160]   so if at any point you guys have questions,
[00:00:28.160 --> 00:00:30.960]   just raise your hand, someone will go to you,
[00:00:30.960 --> 00:00:33.320]   or I think we also have mics nearby.
[00:00:33.320 --> 00:00:37.280]   Cool, I think that's pretty much it.
[00:00:37.280 --> 00:00:40.160]   I'll let a few people finish trickling in
[00:00:40.160 --> 00:00:41.820]   and then we'll get started.
[00:00:41.820 --> 00:00:46.160]   Okay, I think that's long enough.
[00:00:46.160 --> 00:00:51.780]   Okay, so welcome everyone to Model Maxing with OpenAI.
[00:00:51.780 --> 00:00:54.480]   We're gonna be talking about RFT, SFT, DPO.
[00:00:54.480 --> 00:00:56.280]   My name is Ilan Biggio.
[00:00:56.280 --> 00:00:59.080]   I'm on the developer experience team at OpenAI.
[00:00:59.080 --> 00:01:02.660]   And so I do a lot of very early testing on new products
[00:01:02.660 --> 00:01:05.400]   and new directions that we're taking the API.
[00:01:05.400 --> 00:01:07.400]   So whenever there's a new feature coming out
[00:01:07.400 --> 00:01:09.120]   and it's still changing, like every day,
[00:01:09.120 --> 00:01:12.440]   they're like, can you tell us how good this is?
[00:01:12.440 --> 00:01:13.740]   And make a demo with it.
[00:01:13.740 --> 00:01:16.440]   So this is a little bit of what we're gonna be doing today.
[00:01:16.440 --> 00:01:20.160]   It's gonna be a bit of a cross between a presentation.
[00:01:20.160 --> 00:01:22.080]   If you've been to any of my talks before,
[00:01:22.080 --> 00:01:23.080]   they're very code heavy.
[00:01:23.080 --> 00:01:27.160]   This one's a little bit less so because it's fine tuning
[00:01:27.160 --> 00:01:28.460]   and I don't want to have you waiting
[00:01:28.460 --> 00:01:31.200]   for multiple hours watching a progress bar.
[00:01:31.200 --> 00:01:36.580]   So I have a lot of pre-compiled data and some stories to share.
[00:01:36.580 --> 00:01:39.320]   Cool, so this is you.
[00:01:39.320 --> 00:01:42.080]   You might be asking, you know, SFT, RFT, DPO,
[00:01:42.080 --> 00:01:43.940]   what the heck are all these letters?
[00:01:43.940 --> 00:01:45.320]   That's a great question.
[00:01:45.320 --> 00:01:47.120]   You might have some experience with fine tuning,
[00:01:47.120 --> 00:01:49.500]   but be a little bit more asking, like, you know,
[00:01:49.500 --> 00:01:51.740]   will fine tuning solve all of my problems?
[00:01:51.740 --> 00:01:53.240]   That's another good question.
[00:01:53.240 --> 00:01:56.000]   You might also be wondering if you're in the right room.
[00:01:56.000 --> 00:01:57.800]   You are, most likely.
[00:01:57.800 --> 00:02:02.120]   If you want to learn about fine tuning, stay here.
[00:02:02.120 --> 00:02:05.720]   Cool, so yeah, we're gonna be talking about optimization
[00:02:05.720 --> 00:02:09.380]   at a high level and then the specific subset
[00:02:09.380 --> 00:02:10.500]   that is fine tuning.
[00:02:10.500 --> 00:02:12.700]   We're gonna go into each of the different fine tuning options
[00:02:12.700 --> 00:02:16.280]   that OpenAI provides and then some stories, examples,
[00:02:16.280 --> 00:02:18.880]   best practices, and then we're gonna do some Q&A.
[00:02:18.880 --> 00:02:20.920]   I think I have like one hour and a half or two hours.
[00:02:20.920 --> 00:02:22.520]   I have no idea how long this is gonna take.
[00:02:22.520 --> 00:02:24.000]   We'll take as long as we need.
[00:02:24.000 --> 00:02:26.580]   If we end early, you can all have some food.
[00:02:26.580 --> 00:02:30.420]   Cool, so this is optimization.
[00:02:30.420 --> 00:02:32.880]   This is everything that it is, right?
[00:02:32.880 --> 00:02:35.500]   It's like, it's a graph with a line going up
[00:02:35.500 --> 00:02:37.680]   and to the right in some nonlinear path.
[00:02:37.680 --> 00:02:40.260]   This is what you're going for
[00:02:40.260 --> 00:02:42.440]   whenever we talk about optimization.
[00:02:42.440 --> 00:02:45.460]   How you actually accomplish this can vary a lot.
[00:02:45.460 --> 00:02:49.000]   It can vary, you know, you can do like manual optimization,
[00:02:49.000 --> 00:02:50.800]   fine tuning, changing your prompt.
[00:02:50.800 --> 00:02:52.180]   There's a million ways to do this
[00:02:52.180 --> 00:02:53.760]   in a million different industries.
[00:02:53.760 --> 00:02:56.760]   But when you say optimization, this is all it means.
[00:02:56.760 --> 00:03:00.600]   Specifically when it comes to LLM optimization,
[00:03:00.600 --> 00:03:03.680]   we have a few different ways we can do it.
[00:03:03.680 --> 00:03:08.600]   But every LLM system is sort of composed of these three events
[00:03:08.600 --> 00:03:09.520]   or parts, right?
[00:03:09.520 --> 00:03:12.600]   You have your input, which involves the prompt and the context.
[00:03:12.600 --> 00:03:17.180]   You have your model, which is actually represented by the weights
[00:03:17.180 --> 00:03:18.720]   and whatever was pre-trained.
[00:03:18.720 --> 00:03:23.020]   And then you also have this system that goes around the model.
[00:03:23.020 --> 00:03:25.020]   It's the scaffolding, it's the tools.
[00:03:25.020 --> 00:03:29.020]   I will go ahead and say this is every single LLM application ever.
[00:03:29.020 --> 00:03:32.520]   So, everything we say will cover all of this.
[00:03:32.520 --> 00:03:35.440]   Now, today we're going to be focusing mostly on this middle part.
[00:03:35.440 --> 00:03:39.440]   Optimizing the weights and optimizing the model choice.
[00:03:39.440 --> 00:03:44.440]   Now, how to optimize like prompt context, like prompt and context and tools and scaffolding.
[00:03:44.440 --> 00:03:45.440]   There's like a million ways to do that.
[00:03:45.440 --> 00:03:48.600]   I think there's other talks today that are going to go a little bit more in-depth into that.
[00:03:48.600 --> 00:03:52.440]   But we're going to be focusing on this middle one, especially since it's a little bit nebulous.
[00:03:52.440 --> 00:03:58.600]   And I want to help clarify some of the topics and especially how you can do it with the different
[00:03:58.600 --> 00:04:02.520]   fine-tuning offerings that you can use on the OpenAI platform.
[00:04:02.520 --> 00:04:05.720]   So this might be your initial reaction to fine-tuning.
[00:04:05.720 --> 00:04:07.480]   It's like a gross idea.
[00:04:07.480 --> 00:04:08.480]   It's hard.
[00:04:08.480 --> 00:04:09.480]   It's confusing.
[00:04:09.480 --> 00:04:14.600]   I would rather just write prompts like this and have the model do what I want.
[00:04:14.600 --> 00:04:16.120]   Now, that's fair.
[00:04:16.120 --> 00:04:19.960]   I think for a lot of problems, that's actually the case.
[00:04:19.960 --> 00:04:24.920]   Fine-tuning is really just continued training that optimizes a model for a given domain.
[00:04:24.920 --> 00:04:29.480]   So it might be useful for your case.
[00:04:29.480 --> 00:04:31.360]   You might be better off sticking up with prompting.
[00:04:31.360 --> 00:04:34.220]   And this is one of the things that we're going to be talking about today.
[00:04:34.220 --> 00:04:38.460]   But I think the idea is to open up your eyes to all the options that you have.
[00:04:38.460 --> 00:04:43.800]   So let's do a quick comparison between prompting and fine-tuning and then move on.
[00:04:43.800 --> 00:04:47.960]   Prompting is kind of like a set of tools, like a hammer, pliers, whatever.
[00:04:47.960 --> 00:04:51.040]   I think those are some made-up tools in the back that don't exist, but we're going to
[00:04:51.040 --> 00:04:52.040]   ignore those.
[00:04:52.040 --> 00:04:53.920]   It has a very low barrier to entry.
[00:04:53.920 --> 00:04:55.420]   Anybody can do it.
[00:04:55.420 --> 00:04:59.420]   It's very easy and quick to do small changes to quickly progress.
[00:04:59.420 --> 00:05:03.300]   And it's actually enough for, I'd say, most jobs.
[00:05:03.300 --> 00:05:05.920]   Now, fine-tuning is like a CNC machine.
[00:05:05.920 --> 00:05:09.340]   You have -- it's a way higher upfront investment.
[00:05:09.340 --> 00:05:11.880]   You have to collect data, make sure it's clean.
[00:05:11.880 --> 00:05:14.380]   The iteration loops are longer.
[00:05:14.380 --> 00:05:18.880]   You can't just make a quick tweak and rerun because the rerun is going to be multiple hours.
[00:05:18.880 --> 00:05:21.880]   However, they're more automated.
[00:05:21.880 --> 00:05:25.880]   If you have a pipeline set up for fine-tuning, it's a little bit more hands-off.
[00:05:25.880 --> 00:05:29.040]   You don't have to do all this manual thinking yourself.
[00:05:29.040 --> 00:05:31.180]   And finally, it is a specialized tool.
[00:05:31.180 --> 00:05:34.640]   It's not useful for every single use case, but there are certain use cases where it really,
[00:05:34.640 --> 00:05:36.640]   really shines.
[00:05:36.640 --> 00:05:40.700]   And with certain kinds of fine-tuning, you can actually push the boundary of what is possible
[00:05:40.700 --> 00:05:44.000]   and do things prompting cannot accomplish.
[00:05:44.000 --> 00:05:49.060]   So these are the three main types of fine-tuning that we support during OpenAI.
[00:05:49.060 --> 00:05:53.820]   If you're going to take pictures, there's another slide later that has this and more.
[00:05:53.820 --> 00:05:57.040]   So the first one is SFT, supervised fine-tuning.
[00:05:57.040 --> 00:05:58.040]   And what is this?
[00:05:58.040 --> 00:05:59.600]   It's just imitation, right?
[00:05:59.600 --> 00:06:06.740]   You just show the model what you want it to output, and it'll learn to copy exactly that.
[00:06:06.740 --> 00:06:09.180]   The next one is direct preference optimization.
[00:06:09.180 --> 00:06:12.840]   And it's about showing the model two samples.
[00:06:12.840 --> 00:06:16.020]   And you say, one of them I don't like, one of them I like.
[00:06:16.020 --> 00:06:20.780]   Do it more like the one I like, and less like the one I don't like.
[00:06:20.780 --> 00:06:28.480]   And then finally, we recently introduced reinforcement fine-tuning, which is actually the magic sauce
[00:06:28.480 --> 00:06:34.520]   that went into making 01 and 03 and 04 mini in all these reasoning models.
[00:06:34.520 --> 00:06:38.240]   You can use this algorithm as well on this platform.
[00:06:38.240 --> 00:06:42.700]   And what you're doing, you're essentially providing a way.
[00:06:42.700 --> 00:06:47.380]   The model essentially figures out how to think about your given problem.
[00:06:47.380 --> 00:06:51.600]   So let's get a little bit deeper into each one.
[00:06:51.600 --> 00:06:54.260]   What does the data for supervised fine-tuning look like?
[00:06:54.260 --> 00:06:59.720]   Like I mentioned before, you just collect a set of inputs and outputs that you want.
[00:06:59.720 --> 00:07:02.400]   For direct preference optimization, you have inputs.
[00:07:02.400 --> 00:07:06.860]   And then for each input, you have a positive output and a negative output.
[00:07:06.860 --> 00:07:12.320]   I think we call them preferred and non-preferred.
[00:07:12.320 --> 00:07:16.000]   And then finally, for reinforcement fine-tuning, you provide a set of inputs.
[00:07:16.000 --> 00:07:18.000]   And then slightly differently, you provide a grader.
[00:07:18.000 --> 00:07:21.320]   And optionally, with the inputs, you can provide some reference in order for the grader to be able
[00:07:21.320 --> 00:07:25.780]   to have a ground truth if you want, but really this grader can be one of a variety of things
[00:07:25.780 --> 00:07:31.240]   that we're going to be talking about later, which is why reinforcement fine-tuning is so powerful,
[00:07:31.240 --> 00:07:36.820]   because it does not constrain what you can do as much as these other two, but it still has its caveats.
[00:07:36.820 --> 00:07:39.740]   So we're going to be going into all that.
[00:07:39.740 --> 00:07:42.240]   Now, what do each of these cases learn?
[00:07:42.240 --> 00:07:47.700]   Like, you can look at the data, maybe you have some idea, but from first glance, there's a lot of cases
[00:07:47.700 --> 00:07:53.700]   that supervised fine-tuning looks like it might be able to do direct preference optimization versus reinforcement fine-tuning.
[00:07:53.700 --> 00:07:55.700]   Like, how do you think about what each of them is doing?
[00:07:55.700 --> 00:08:02.700]   And the way that I find it most useful to think about this is thinking about what is the model learning in each of these cases.
[00:08:02.700 --> 00:08:13.660]   So supervised fine-tuning is essentially learning a continuous mapping, or like a soft mapping, from your inputs to the outputs.
[00:08:13.660 --> 00:08:20.640]   And so, you know, it's not exactly like a direct mapping like a table, but in essence, that is what you're teaching the model.
[00:08:20.640 --> 00:08:23.260]   It's very, very direct.
[00:08:23.260 --> 00:08:27.160]   It's just going to learn to imitate the data that you provided.
[00:08:27.160 --> 00:08:30.240]   Direct preference optimization is a little bit more interesting.
[00:08:30.240 --> 00:08:34.300]   It doesn't actually learn, like, each of the examples.
[00:08:34.300 --> 00:08:37.200]   It learns the delta between the examples.
[00:08:37.200 --> 00:08:45.800]   And so if you imagine if you can, like, pull up your, like, mental embedding, like, latent space, and you project these two ideas there,
[00:08:45.800 --> 00:08:51.880]   the positive output and the negative output, if you can imagine, like, what that difference vector is between them,
[00:08:51.880 --> 00:08:55.980]   that is essentially what you're trying to teach the model.
[00:08:55.980 --> 00:09:02.940]   And then, finally, and this is also -- it looks straightforward, but it's actually pretty interesting, is learning to reason, right?
[00:09:02.940 --> 00:09:15.540]   So reinforcement of fine-tuning learns to reason about a problem by learning to tune and change its chain of thought so that it is able to get this, like, higher success rate on whatever problem that you're giving it.
[00:09:15.540 --> 00:09:19.600]   So, what is each -- what is each of these good for?
[00:09:19.600 --> 00:09:22.500]   So, supervised fine-tuning is the simplest of the bunch.
[00:09:22.500 --> 00:09:24.500]   It's great for classification use cases.
[00:09:24.500 --> 00:09:29.000]   It's great for formatting and structured data extraction.
[00:09:29.000 --> 00:09:37.500]   Pretty much any case where you really want to constrain what the model is doing and have a very specific kind of output, supervised fine-tuning is it.
[00:09:37.500 --> 00:09:46.760]   And this is helpful for, like, distillation, if you want to teach, like, a very, very, very small model how to do this, like, structured output example and, like, prompting isn't cutting it.
[00:09:46.760 --> 00:09:48.560]   Supervised fine-tuning is the way to go.
[00:09:48.560 --> 00:09:54.740]   Now, direct preference optimization is a little bit more -- I'd say it's the hardest to think about of the three.
[00:09:54.740 --> 00:10:11.000]   It's very good at learning tone-matching, right, because tone and style are these kind of little bit more intangible things that are hard to evaluate but easy to see if you put two comparisons together, right?
[00:10:11.000 --> 00:10:19.380]   And so, this was really made in order to do tone-matching and also -- I mean, primarily, it is when you have A/B tests, right?
[00:10:19.380 --> 00:10:30.140]   Like, when you have positive data and negative data, preferred data, not preferred data, if you've been using ChatGPT and you get, like, both -- like, two responses, this is why, right?
[00:10:30.140 --> 00:10:34.220]   Like, this is the algorithm that is being used in order to, like, tune for those preferences.
[00:10:34.220 --> 00:10:44.980]   And so, it's not saying, "Output exactly the one that I prefer." It's saying, "Output slightly more in that direction," whereas supervised fine-tuning is much more exact.
[00:10:44.980 --> 00:10:51.780]   And then, reinforcement fine-tuning is for gradable hard problems.
[00:10:51.780 --> 00:11:00.460]   Some examples are, like, medical, legal, coding, just, like, things that are pretty hard to do but pretty simple or straightforward to verify.
[00:11:00.460 --> 00:11:06.700]   You really, really, really want unambiguous solutions, and I feel like in a lot of these talks, we always talk about successes.
[00:11:06.700 --> 00:11:10.220]   I snuck in a little failure here later, so you're going to get to see that.
[00:11:10.220 --> 00:11:17.060]   And it's also a really interesting candidate for training an LLM judge model, right?
[00:11:17.060 --> 00:11:35.300]   Because grading a problem is actually a problem itself, and so, if you -- like, grading that problem is a pretty verifiable answer if you have these, like, golden results that you want your LLM judge to be able to mimic.
[00:11:35.300 --> 00:11:53.540]   Cool. So, one thing to note before I get into each of these is that, each of these approaches, while they're very different, we are training only a subset of the model in each of these cases.
[00:11:53.540 --> 00:12:07.780]   And so, this is why they're not amazing for learning, like, tons of new data, right? Like, RAG and other solutions like agentic retrieval are much, much better if you want to teach the model new information.
[00:12:07.780 --> 00:12:14.020]   So, this is called, I think, low-rank adaptation. Can anybody check on that? Yeah, I see some nods. Cool.
[00:12:14.020 --> 00:12:23.220]   And essentially, for those who are interested, you should look more into it. It's a really, really clever way where you take the model weights and you decompose them into two matrices.
[00:12:23.220 --> 00:12:31.780]   And then that essentially gives you a much smaller set of weights that you need to update in order to affect the overall model behavior.
[00:12:31.780 --> 00:12:39.060]   And so, you don't fully readjust the entire model. You don't make it forget the things it did before, but you still impart some new behaviors into it.
[00:12:39.060 --> 00:12:44.340]   Cool. So, let's start with a story.
[00:12:44.340 --> 00:12:49.540]   When I joined OpenAI, I was working as a solutions architect.
[00:12:49.540 --> 00:13:01.700]   And what this meant was I was working directly with companies, helping them deploy their, like, AI use cases and applications into whatever domains they are working in.
[00:13:02.340 --> 00:13:13.540]   Now, one of our partners, one of our customers, was working on a low-latency function calling use case, kind of assistant style, based on, like, taking action.
[00:13:13.540 --> 00:13:18.580]   Right? And so, you would -- the user would say something, and the model had to act.
[00:13:18.580 --> 00:13:23.300]   But it was in an extremely latency-constrained scenario. Right?
[00:13:23.300 --> 00:13:26.900]   So, they wanted to use -- back then, it was 3.5 -- GPT 3.5.
[00:13:26.900 --> 00:13:31.060]   GPT 3.5 was very, very fast, but not very accurate.
[00:13:31.060 --> 00:13:38.020]   GPT 4 was roughly the level of performance that they were expecting in terms of accuracy, but it was way too slow.
[00:13:38.020 --> 00:13:40.580]   And so, they asked us, what can we do?
[00:13:43.060 --> 00:13:49.140]   So, ooh, actually -- before I get into this, this would actually be fairly straightforward.
[00:13:49.140 --> 00:13:57.700]   And I feel like a lot of you could imagine how to do this if we had a lot of existing examples with, like, inputs and outputs, which they did not have.
[00:13:57.700 --> 00:14:05.300]   And so, we were left to figure out, how do you fine-tune a model when you don't have, like, this great amount of data -- or these, like, good examples.
[00:14:05.300 --> 00:14:11.140]   And so, we took a few different approaches -- but I'm going to cover the two main ones that we used -- that proved to be pretty effective.
[00:14:11.140 --> 00:14:18.500]   So, the first one is, we did have all of the functions that they wanted the agent to use.
[00:14:21.300 --> 00:14:28.020]   One more thing I didn't say, they were trying to do this with 120 functions, which was unheard of at the time.
[00:14:28.020 --> 00:14:33.940]   I remember asking some researchers, like, you know, what is, like -- how many do we support usually?
[00:14:33.940 --> 00:14:37.620]   And they were like, you know, five to ten, maybe.
[00:14:37.620 --> 00:14:39.860]   It's like, okay, okay, cool, cool.
[00:14:39.860 --> 00:14:45.940]   So, this is actually the first time that anybody fine-tuned a model with over 100 functions, because
[00:14:45.940 --> 00:14:49.300]   the capability was not possible before.
[00:14:49.300 --> 00:14:51.540]   We had to have this built for this use case.
[00:14:51.540 --> 00:14:54.180]   So, what you're seeing here is the first step that we did.
[00:14:54.180 --> 00:14:58.580]   We actually took the function schemas that the customer provided
[00:14:58.580 --> 00:15:09.060]   and figured out a way to create every possible, like, permutation of the invocations of that function.
[00:15:09.060 --> 00:15:16.020]   So, for example, if we have, like, a toggle -- ooh, typo -- if we have, like, set lights and it takes one parameter and it's an enum,
[00:15:16.740 --> 00:15:20.820]   here I'm essentially showing that the three permutations that you can have are set lights off,
[00:15:20.820 --> 00:15:24.660]   set lights on, and set lights red. This was actually done completely deterministically.
[00:15:24.660 --> 00:15:28.580]   This was just, like, a Python script that I spent way too long working on.
[00:15:28.580 --> 00:15:33.460]   Then search song is kind of similar. There's more than one parameter.
[00:15:33.460 --> 00:15:40.340]   So, here I skipped the cases where you provide, like, multiple parameters, but essentially what this function did is it took a schema from a function,
[00:15:40.340 --> 00:15:44.420]   and then it spat out essentially every possible permutation of that function.
[00:15:44.420 --> 00:15:50.500]   And if it was, like, more -- if it exploded in complexity, we would, like, randomly sample from those.
[00:15:52.340 --> 00:16:00.260]   So, this was the first step. Oh, and then if there were some samples that were non-deterministic, like Katy Perry or, like, were open-ended,
[00:16:00.260 --> 00:16:03.460]   we would ask a model to, like, provide plausible values.
[00:16:04.740 --> 00:16:17.780]   The next step was once we have those function calls, then we asked GPT-4 to go essentially from here's a function call, give me a command that would have resulted in that function call.
[00:16:18.580 --> 00:16:21.380]   And we did this for a large number of the permutations.
[00:16:21.380 --> 00:16:31.060]   And so, some of you might already be noticing what we're doing, but essentially, if you, like, flip this in your head, now we have these input-output pairs,
[00:16:31.060 --> 00:16:38.180]   where the input is the GPT-4 generated command, and the output is the function call itself.
[00:16:38.180 --> 00:16:40.340]   So, we're already on a pretty good track, right?
[00:16:40.340 --> 00:16:45.540]   We have a lot of data, but it's pretty -- it's synthetic, and we don't just want it to be synthetic.
[00:16:45.540 --> 00:16:52.340]   We don't know if these are the kinds of inputs that the users will be giving, and if we are very off-base, if we are, like, very out of distribution,
[00:16:52.340 --> 00:16:56.020]   we won't have as good of a model in the end.
[00:16:56.020 --> 00:17:01.700]   I will say, the models are pretty good at generalizing, even if you do stuff that is slightly out of distribution,
[00:17:01.700 --> 00:17:05.220]   because they have such a large set of weights in, like, a pre-existing world model.
[00:17:05.220 --> 00:17:09.140]   The second way we did it was distilling some data.
[00:17:09.620 --> 00:17:16.660]   So, the engineers we were working with did actually have some unlabeled inputs, and we were like, okay.
[00:17:16.660 --> 00:17:19.220]   Well, here's what.
[00:17:19.220 --> 00:17:23.620]   You said GPT-4 was good enough for most of these cases, so let's do something.
[00:17:23.620 --> 00:17:29.620]   Let's just run them through GPT-4 and get what outputs GPT-4 would give us.
[00:17:31.220 --> 00:17:32.580]   After this, I didn't include this in here.
[00:17:32.580 --> 00:17:36.900]   We actually did some filtering as well, and, like, we did a couple stages in this pipeline to essentially
[00:17:36.900 --> 00:17:45.220]   generate a lot of, like, distilled -- still synthetic data, but hopefully very high quality.
[00:17:46.020 --> 00:17:51.140]   And in the end, we took these two data sets, passed them into GPT-3.5,
[00:17:51.140 --> 00:17:57.380]   and we achieved GPT-4 level performance at 3.5 accuracy.
[00:17:57.380 --> 00:18:03.460]   And so, this was one of the first, like, major successes that I saw with supervised fine-tuning.
[00:18:03.460 --> 00:18:08.900]   Now, note that this is a very constrained use case, so it's actually perfect for fine-tuning.
[00:18:08.900 --> 00:18:12.340]   The model doesn't have to do this, like, open-ended response. You're not trying to strive for a certain
[00:18:12.340 --> 00:18:17.300]   personality. You just want it to call one function, and you want it to do it well, and you want it to
[00:18:17.300 --> 00:18:22.260]   do it fast. So this is a perfect use case to do distillation with supervised fine-tuning.
[00:18:22.260 --> 00:18:29.300]   Actually, I want to keep this interactive. I'll pause every now and then. Feel free to ask any
[00:18:29.300 --> 00:18:35.620]   questions. If I don't see any hands, I'll just, like, barrel along. Okay. I see a couple back there.
[00:18:37.780 --> 00:18:41.060]   Yeah. I think they're coming to you with a mic.
[00:18:41.060 --> 00:18:56.100]   Six? I can just, yeah. Could you have distilled without synthetic?
[00:18:56.100 --> 00:19:01.940]   So the question is, could we have distilled without synthetic data? I think we would have preferred to
[00:19:01.940 --> 00:19:06.820]   distill without synthetic data, right? I think the use case that we -- like, the perfect case here is the
[00:19:06.820 --> 00:19:10.260]   engineers come to us, and they're like, hey, we want to get this model to be really, really fast,
[00:19:10.260 --> 00:19:15.460]   and we have 1,000 labeled examples. And we're like, great! We'll get back to you in, like, a day.
[00:19:15.460 --> 00:19:21.780]   Instead, this is what we had to do. But yeah, that's a good question. I think there's one back there.
[00:19:21.780 --> 00:19:39.220]   Yeah. So the question was, what was the ratio between synthetic and slightly less synthetic data?
[00:19:39.220 --> 00:19:50.660]   And how many examples did we need? I think I'll answer the second one first, which is -- and I have this in a demo, and we'll look at the performance. So, like,
[00:19:50.660 --> 00:19:56.260]   at around, like, 50 to 100 examples, you start to see signs of life. And that might be, like,
[00:19:56.260 --> 00:20:00.500]   already enough for some smaller problems. Then if you start, like, pushing more toward production,
[00:20:00.500 --> 00:20:07.620]   you might want to scale up to, like, a few hundred, 500 examples. In the demos I have later, I used 150
[00:20:07.620 --> 00:20:14.420]   examples and 500 examples. But this is for supervised fine-tuning. For the other two kinds, you can actually
[00:20:14.420 --> 00:20:20.740]   use fewer, and it's more forgiving. And we will talk about that in a second. And then the first one was,
[00:20:20.740 --> 00:20:26.420]   what is the ratio? I don't remember. I think I tried some ablations where, like, I only provided one of
[00:20:26.420 --> 00:20:32.980]   the -- one of each. And, like, it was best when I provided both together. But I can't tell you what -- I don't
[00:20:32.980 --> 00:20:40.340]   remember what the ratio was. Yep. Okay. So you -- you said that there was around 100 functions?
[00:20:40.340 --> 00:20:44.740]   Yes. Okay. Yeah. I pretty much had the same question. So you're -- you're basically saying
[00:20:44.740 --> 00:20:48.980]   for each function you had around 50 to 100 synthetic examples.
[00:20:48.980 --> 00:20:57.620]   I don't remember -- it was -- I've said a lot of numbers around 100 and 500. But I think they're
[00:20:57.620 --> 00:21:04.820]   being maybe conflated. We had 100 functions, around 100 functions. Each one we generated -- I think it was,
[00:21:04.820 --> 00:21:11.460]   like, between 20 and 200 -- like, we tried different amounts -- like, between 20 and 200 examples,
[00:21:11.460 --> 00:21:14.180]   depending on how many permutations each function would give us.
[00:21:14.180 --> 00:21:21.780]   Mm-hmm. And then also depending on which -- which of the inputs. So all in all, I think it was in the,
[00:21:21.780 --> 00:21:29.620]   like, like, high hundreds, low thousands. Maybe, like, yeah, low to mid thousands of examples that we used. Yeah.
[00:21:29.620 --> 00:21:37.700]   And -- sorry if I'm skipping ahead here, but was there -- did that improve the function call accuracy
[00:21:37.700 --> 00:21:42.740]   for, like, choosing which function to call? Or did it just improve, like, how well it would call a function?
[00:21:42.740 --> 00:21:49.220]   So, like, choosing the function versus, like, populating the function parameters?
[00:21:49.220 --> 00:21:50.340]   Yes. Both.
[00:21:50.340 --> 00:21:51.460]   It improved both. Awesome.
[00:21:51.460 --> 00:21:56.820]   Yeah, yeah. I mean, we -- if I remember correctly, we got to, like, within 2% of GPT-4
[00:21:56.820 --> 00:22:02.420]   at, like, a very small fraction of the latency. Yeah. Something else that we tried was, like,
[00:22:02.420 --> 00:22:06.180]   what happens if you remove all the context -- all the functions from context, or, like,
[00:22:06.180 --> 00:22:11.060]   the parameters from context? Like, can it learn the parameters, like -- because we wanted to, like,
[00:22:11.060 --> 00:22:15.140]   really squeeze out as much context as we could to, like, reduce latency? And you know when you're
[00:22:15.140 --> 00:22:21.700]   cutting input context for latency reductions, you're, like -- like, you're out of options, right?
[00:22:21.700 --> 00:22:26.180]   You should never start with that. But we were trying everything. And that was actually not very good
[00:22:26.180 --> 00:22:31.940]   results. We realized, like, having things in context -- like, removing them from context -- makes it much
[00:22:31.940 --> 00:22:37.300]   harder to learn through fine-tuning. It's not as -- as good. Thanks. Yeah.
[00:22:37.300 --> 00:22:44.900]   Yeah. Yeah. Yeah. So how -- follow-up questions that -- in the future, if they have more functions,
[00:22:44.900 --> 00:22:48.260]   then do they need to retrain everything?
[00:22:48.260 --> 00:22:54.340]   Uh -- Let's say they have 100, and, you know, they just have two more.
[00:22:54.340 --> 00:22:57.380]   Yes. So they would not scale, right? I believe.
[00:22:57.380 --> 00:23:03.540]   This -- this does not scale in that way. If you, like, add new functions, you -- like, in this exact setup that we
[00:23:03.540 --> 00:23:09.300]   have, you would need to -- to retrain. There are ways to just pick up training from where you left off.
[00:23:09.300 --> 00:23:14.740]   Like, you can fine-tune an already fine-tuned model. And it just continues the fine-tune process,
[00:23:14.740 --> 00:23:16.500]   essentially. And would that be okay? Like,
[00:23:16.500 --> 00:23:21.940]   if you fine-tune on top of this with a few examples? I feel like it -- it could be. Especially for this
[00:23:21.940 --> 00:23:26.020]   case. It's so constrained. I think you can get away with a lot in cases that are this constrained.
[00:23:26.020 --> 00:23:30.660]   Got it. Yeah. I'll take maybe one more, and then we will keep going. Yeah?
[00:23:30.660 --> 00:23:34.500]   How did your -- how did the prompts change when you did this?
[00:23:34.500 --> 00:23:38.180]   How did the prompts change? How did the prompts change when I did this?
[00:23:38.180 --> 00:23:42.980]   Like, did you have -- were you able to reduce context, and, like, what does each tool do?
[00:23:42.980 --> 00:23:49.380]   I see. Uh, yeah, in terms of -- so I think, um, the question is around, like, how much we managed to
[00:23:49.380 --> 00:23:55.300]   reduce the prompt size and, like, context that we put into the model. The answer is a bit, but not that much.
[00:23:55.300 --> 00:23:59.460]   Like, we were able to get rid of most of the prompt and just say, like, classify into, like,
[00:23:59.460 --> 00:24:04.820]   one of these, and then here's all the functions. But we weren't able to remove, like, types or the
[00:24:04.820 --> 00:24:10.500]   different parts of the functions themselves from the context without losing performance. Yeah.
[00:24:10.500 --> 00:24:15.780]   Which was much more of an issue then, because we were working with, like, a 4K context window.
[00:24:15.780 --> 00:24:22.740]   Maybe -- I think we, like, had to, like, push -- like, apply to, like, let us fine-tune a 16K model,
[00:24:22.740 --> 00:24:26.900]   and it was -- whoa, it was crazy. Now it's, like, you're fine-tuning, like, one million models. No,
[00:24:26.900 --> 00:24:33.060]   no problem. Okay. Um, cool. This is this case. I'll move on, and we'll have -- we have plenty of time
[00:24:33.060 --> 00:24:37.940]   for questions. Um, and if you're more -- if you're curious to learn more, we did actually write a lot of
[00:24:37.940 --> 00:24:42.980]   that. So we're going to move this up in a cookbook called Fine Tuning for Function Calling. You can run
[00:24:42.980 --> 00:24:50.100]   it. You can play with it. It has most of what I talked about. Okay. Time for the first live demo.
[00:24:50.100 --> 00:24:56.180]   So let's say we have a customer -- I wanted to find a data set that would be easy to find. So
[00:24:58.180 --> 00:25:06.340]   here I have cursor with, like, a very basic, you know, loading in OpenAI keys, yada, yada, yada,
[00:25:06.340 --> 00:25:12.100]   but then I'm loading in this banking data set. And essentially what it does, if we, like, print out
[00:25:16.020 --> 00:25:32.100]   the first thing in the data is not happy. Oh. Cool. Um, we have some input text. You know,
[00:25:32.100 --> 00:25:40.260]   I'm still waiting on my card. And a label. And the label is, uh, some, like, index into an array of
[00:25:41.540 --> 00:25:54.100]   labels, right? Uh, of, like, actual text labels, like, um, uh, user is -- not waiting for a card,
[00:25:54.100 --> 00:25:58.740]   but, like, like, arrival time or something like that, right? And so what we want to do is we want
[00:25:58.740 --> 00:26:02.420]   to get a model that can, like, classify this really, really well. And this is a good case for fine-tuning,
[00:26:02.420 --> 00:26:08.660]   or for supervised fine-tuning. It's very straightforward, um, it's very direct, uh, and it's very constrained,
[00:26:08.660 --> 00:26:14.900]   right? So, okay, so here I load in the data. I load in the label names. I guess I could have just
[00:26:14.900 --> 00:26:16.980]   printed these.
[00:26:16.980 --> 00:26:27.860]   Cursor abandoned me today. Okay. Yeah, activate card. Oh, yeah, yeah. We got, like, many of them.
[00:26:27.860 --> 00:26:31.380]   And so as you can see, this is not, like, a super easy task for the models. This is, like, a lot of
[00:26:31.380 --> 00:26:36.260]   different labels they can pick from. Um, and this is as hard as I tried in my prompt.
[00:26:37.780 --> 00:26:43.780]   That's it. That's it. Um, and then it passed it all the labels. Um, and I guess this is a good time
[00:26:43.780 --> 00:26:48.260]   to say, in supervised fine-tuning, your prompt does not matter as much, right? You are showing,
[00:26:48.260 --> 00:26:53.620]   like, a direct, uh, like, a direct examples, and so it -- like, you can skip a lot of the prompt
[00:26:53.620 --> 00:26:57.060]   engineering. Not all of it, right? If you give it, like, nothing to work on, it's not going to, like,
[00:26:57.060 --> 00:27:03.220]   learn very well. Um, but, like -- or, sorry -- you could actually completely remove your prompt,
[00:27:03.220 --> 00:27:07.620]   and if you have a constrained enough example, it will still learn to do the right thing. This is not
[00:27:07.620 --> 00:27:13.300]   true about DPO and RFT. You still need good prompts for both of those. Um, as of T, I would recommend
[00:27:13.300 --> 00:27:18.580]   having at least a decent prompt. Okay. So we have our data. We have our label names. I loaded in the test
[00:27:18.580 --> 00:27:26.660]   data. Uh, and I wrote my prompt. So now I have my messages. Uh, I'm using the chat completions API because
[00:27:27.220 --> 00:27:33.540]   fine-tuning right now, I think, works in that format. So I want to keep the format consistent. Um, so we have
[00:27:33.540 --> 00:27:39.860]   our system prompt, which is this beauty. And then the user is just the input text -- uh, the user message
[00:27:39.860 --> 00:27:47.620]   is just the input text, right? And then I write this very simple function that calls the API and gives it
[00:27:47.620 --> 00:27:55.220]   a classification, right? And so if I, like, test this out right now, um, look at that. So how do I look at my card -- card
[00:27:55.220 --> 00:28:01.060]   arrival? I guess that's correct. But, like, if I run it a few more times, you know, card not working -- like,
[00:28:01.060 --> 00:28:09.300]   it's not good, to be fair. I don't know if any of us could do this with this prompt. Uh, it's pretty bad.
[00:28:09.300 --> 00:28:15.380]   But that's part of the point. Okay. So we have this sort of, like, inconsistent thing. Um, now, before we
[00:28:15.380 --> 00:28:21.220]   even start doing fine-tuning, we have to ask, like, you know, should we do it? Right? And this is one of the
[00:28:21.220 --> 00:28:25.380]   most important questions in fine-tuning. I think this is part of why a lot of you are here. Should
[00:28:25.380 --> 00:28:31.620]   I do it? The answer is, how well is it doing on your evals? Right? So I wrote this very small, like,
[00:28:31.620 --> 00:28:38.340]   parallel evaluation function, um, that essentially takes in a model string, a system prompt, uh, some,
[00:28:38.340 --> 00:28:42.340]   like, input samples to run over in parallel, and then a number of workers, since I'm, like, shooting them
[00:28:42.340 --> 00:28:48.820]   off in parallel, sort of, like, asynchronously. Um, I have a semaphore, just so I don't, like, violate
[00:28:48.820 --> 00:28:54.580]   and, like, rate limits, or, like, the number of workers that I have is, like, fixed. Um, I'm, like,
[00:28:54.580 --> 00:28:59.860]   logging the progress, and then I have my score function, which just calls classify intent, um,
[00:28:59.860 --> 00:29:06.020]   updates the progress, and then returns whether it equals the correct label. And I shoot them off in
[00:29:06.020 --> 00:29:13.780]   parallel. And there we go. So, uh, this is my evaluation function. You know, people talk about,
[00:29:13.780 --> 00:29:20.660]   like, different, uh, evals, um, platforms. We have a way to do evals as well, as well on our platform. Um,
[00:29:20.660 --> 00:29:24.580]   I think I'm going to stick to this for this example, since I want to keep things, like, mostly
[00:29:24.580 --> 00:29:29.700]   in this notebook. Um, and we can, like, see little pretty progress bars that I made.
[00:29:30.340 --> 00:29:39.060]   Okay. So now we have GPT, uh, 4.0 mini and 4.0. This will come in later. Uh, and we can run it,
[00:29:39.060 --> 00:29:46.500]   right? And so if we run this evals, we're essentially taking the testing set, um, made up of 150 examples,
[00:29:46.500 --> 00:29:51.620]   uh, running it. My progress bars aren't working correctly. That's fine. Uh, it's still finished,
[00:29:51.620 --> 00:29:58.740]   right? So it just ran 4.0 mini, and it ran 4.0. And now we get the relative accuracies, which is 75%
[00:29:58.740 --> 00:30:04.260]   accuracy for 4.0 mini and 83% for 4.0. So, you know, not bad. Let's see if we can do better.
[00:30:04.260 --> 00:30:14.900]   Um, cool. So this is just, I'm taking the actual data sets and then dumping them out into files
[00:30:14.900 --> 00:30:22.660]   that we can use for training. Now, if the important part here is the format. So the format of a fine-tuning
[00:30:23.220 --> 00:30:30.500]   model and, like, let me see if I can just -- I just love pulling up docs and talks. Uh, it's what everyone
[00:30:30.500 --> 00:30:39.860]   else will be doing. Um, let's see. Oh, by the way, you should all check this out. Agents SDK TypeScript came out
[00:30:39.860 --> 00:30:51.220]   today. Uh, where are we? There we go. So if we look at the format -- by the way, I know this stuff. I'm
[00:30:51.220 --> 00:30:58.980]   not doing this for me, okay? This is for everyone to see. Uh, cool. So here we go. This is the JSON L,
[00:30:58.980 --> 00:31:04.580]   which is just, like, JSON Lines data format that you'll want to be passing, which is, um, a messages,
[00:31:04.580 --> 00:31:11.380]   uh, array -- like, a field. It's an array that you're used to, the one you know and love. Um,
[00:31:11.380 --> 00:31:16.580]   and all it's going to do is it's going to take the last assistant message and learn from it. Um,
[00:31:16.580 --> 00:31:23.620]   here the last assistant message is a function call, so it has, like, the whole schema. Um, but if we look at
[00:31:23.620 --> 00:31:31.300]   the data that we're producing here, these are, like, you know, this is my amazing prompt. Uh,
[00:31:31.300 --> 00:31:37.860]   these are, like -- this is all part of the prompt, right? And then when I get to the very end -- oh,
[00:31:37.860 --> 00:31:44.340]   boy. Okay. Uh, then we just have -- so that's the developer message. Then we have the user message,
[00:31:44.340 --> 00:31:49.860]   which is the input, and then the assistant message, which is what we want it to respond. Um, and that's
[00:31:49.860 --> 00:31:52.740]   the whole setup. And what it's going to do is it's going to learn from that last message.
[00:31:53.460 --> 00:31:58.980]   Uh, so we can either shoot this off from the, uh, fine-tuning UI, but I am going to show you -- I'll
[00:31:58.980 --> 00:32:03.780]   show you how to do both. Um, cool. So now what we're doing is we're taking the dataset, putting it in
[00:32:03.780 --> 00:32:11.220]   this format, and saving it, and calling it train. Uh, we then, like, upload the files. So it's important
[00:32:11.220 --> 00:32:16.180]   that we -- like, for us to reference files within a fine-tuning job, we have to upload them to the OpenAI
[00:32:16.180 --> 00:32:22.500]   API first. So client.files.create -- we have to set the purpose to fine-tune, give it the file,
[00:32:23.620 --> 00:32:29.140]   here are the file IDs that we have. And now, fine-tuning is as simple as, like, this API call,
[00:32:29.140 --> 00:32:34.580]   right? We pass in the base model. Uh, the reason I had the mappings between, like, the base model,
[00:32:34.580 --> 00:32:40.260]   like, 4.0 and, like, the full snapshot is because it can't take in, um, aliases. It has to take in,
[00:32:40.260 --> 00:32:45.540]   like, the full snapshot. Then we pass in the training file, which is going to actually train on validation
[00:32:45.540 --> 00:32:50.580]   file, which is going to tell us how well it's doing, uh, so it doesn't overfit. And then I'm just
[00:32:50.580 --> 00:32:53.700]   passing in the number of epochs. There's many more parameters you can play with. I just want to
[00:32:53.700 --> 00:33:01.860]   give you a sense. Um, so cool. I already ran this. I don't want you all to sit here waiting. Uh,
[00:33:01.860 --> 00:33:09.940]   fine-tuning jobs can take anywhere from 30 minutes -- and this is on the OpenAI API -- anywhere from 30
[00:33:09.940 --> 00:33:18.580]   minutes to 24 hours. So give yourself time. Never do this right before a talk. Uh, cool. So we have
[00:33:18.580 --> 00:33:24.660]   our fine-tuning job. Then we load the model, right? We, like, check the model. Uh, we check the job. We
[00:33:24.660 --> 00:33:29.540]   see if it succeeded. We load them in. I actually already saved them here. And now the question is,
[00:33:30.580 --> 00:33:36.980]   how well did it do? I fine-tuned, um, as you can see, I made some examples with -- I guess I skipped
[00:33:36.980 --> 00:33:43.060]   over this -- but I made some examples with 150 examples, and I made some with 500. And I trained both
[00:33:43.060 --> 00:33:51.620]   four -- uh, GPT-40 and GPT-40 mini with each one, just to do, like, a little comparison. Uh, and so I ran it,
[00:33:51.620 --> 00:33:59.620]   and I also compared it against 04 and 03. And the final results at the very, very end, this is what
[00:33:59.620 --> 00:34:06.820]   they look like. So, uh, baseline, 83 percent, 75 percent. Uh, reasoning models, a little bit better,
[00:34:06.820 --> 00:34:11.460]   80 and 90. Um, but the fine-tuning models blow everything out of -- like, blow everything else out
[00:34:11.460 --> 00:34:18.100]   of the water. Um, and this is kind of to show, like, how convenient fine-tuning can be, right? Like,
[00:34:18.100 --> 00:34:24.020]   it's doing better than 03 at a task that, like, I barely wrote a prompt for. And I think even
[00:34:24.020 --> 00:34:30.820]   4.0 mini -- yeah, 4.0 mini with 500 examples is as -- is, like, actually technically better than 4.0.
[00:34:30.820 --> 00:34:34.660]   But, like, they sort of re -- like, are reaching much higher levels. So this is what I want to show
[00:34:34.660 --> 00:34:39.860]   how you can, like, get fine-tuning and, like, when it's worth it. Now, if this was something I could
[00:34:39.860 --> 00:34:44.660]   have done with more prompt engineering, maybe. But, like, this is a case where I have the data. I want to run
[00:34:44.660 --> 00:34:50.740]   run it. Pretty straightforward. Uh, maybe -- yeah, I'll pause here again for -- for questions. Yeah?
[00:34:50.740 --> 00:34:53.780]   Is the fine -- oh --
[00:34:53.780 --> 00:35:02.580]   Do the fine-tune runs check for overfitting? And if they do, do they stop? Or do you have to be, like,
[00:35:02.580 --> 00:35:06.980]   watching it while you're, like -- Nope. So the question is, do the fine-tuning runs check for
[00:35:06.980 --> 00:35:11.060]   overfitting? No, they don't. I mean, you can see it. You can see it on, like, the diagram. Let me see if I can
[00:35:11.060 --> 00:35:17.140]   actually pull up these jobs. Um, maybe this last one.
[00:35:17.140 --> 00:35:22.260]   I guess the second question, then, is do you just kill it after the diagrams show
[00:35:22.260 --> 00:35:25.860]   you that it's crappy? Or, like, how do you know what to do next?
[00:35:25.860 --> 00:35:34.180]   Um, am I in a different org? Okay. I may have done this in a different org.
[00:35:35.220 --> 00:35:41.300]   So I'll pull it up at a different time. But, uh, yeah. Um, I don't want to leak
[00:35:41.300 --> 00:35:45.380]   anything. Sorry. Uh, sorry. What was the question again? Do you have -- you do have to --
[00:35:45.380 --> 00:35:47.380]   Do you see the overfitting? Like, what do you do?
[00:35:47.380 --> 00:35:52.980]   Well, I mean, yeah, you can stop it and try again. Uh, I think you're asking me a very, like,
[00:35:52.980 --> 00:35:57.540]   data science general question. What do you do when you see overfitting? So many things you can do.
[00:35:57.540 --> 00:36:01.300]   In this case, specifically, it's like, you know, you don't want to have a 24-hour job.
[00:36:01.300 --> 00:36:07.140]   Yeah. Yeah, you can stop it. Yeah, you can stop it. Uh, you can stop it, and I think you'll still
[00:36:07.140 --> 00:36:14.420]   be able to test with latest snapshot, maybe. Uh, here? Yeah. Yeah. Hey, um, a couple of questions.
[00:36:14.420 --> 00:36:19.860]   How do you decide between, like, when do you want to fine-tune versus not, like, just lean to always
[00:36:19.860 --> 00:36:24.500]   using O3? Like, when do you sort of pick up to, like, actually go fine-tune? I'll get into this more
[00:36:24.500 --> 00:36:31.220]   later. But I think the short of it is always start with prompt engineering and see how far you can go,
[00:36:31.220 --> 00:36:38.340]   and then run evals, right? And so if it's as good as you want it to be, that's great, right? Um,
[00:36:38.340 --> 00:36:41.780]   and if you are still changing the prompt and you are still getting better results,
[00:36:41.780 --> 00:36:47.540]   there's no reason to fine-tune yet, right? I think fine-tuning is something you want to consider,
[00:36:47.540 --> 00:36:52.340]   like, when you have enough data for it to be worth it and the data is, like, pretty clean and, like,
[00:36:52.340 --> 00:37:00.900]   it's not really, like, uh, like, you kind of have to have, like, the resources ready for it to do the
[00:37:00.900 --> 00:37:04.500]   fine-tuning. Fair enough. And then, like, which models do you, like, support fine-tuning for right
[00:37:04.500 --> 00:37:08.100]   now? Like, does it keep changing over time? Like, I know if you support, like, you know, 3.5 and then
[00:37:08.100 --> 00:37:12.020]   4. Like, is everything, like, when it comes out, like, instantly going to support fine-tuning? Or is
[00:37:12.020 --> 00:37:15.620]   there, like, what's the lag period behind it? I think we have... oop.
[00:37:15.620 --> 00:37:29.380]   Oh, man. Uh, where are we? I think we now, like, say, yeah, which ones support fine-tuning on the
[00:37:29.380 --> 00:37:37.220]   models page. This is why I like docs. And live browsers. Uh, yeah. Did you have to do any, uh,
[00:37:37.220 --> 00:37:43.300]   hyper-parameter tuning? Not for this. Uh, not for this. Yeah. Does it have any impact on the results?
[00:37:43.300 --> 00:37:50.500]   Like, updating, running late? Sure. But I'd say that falls into the, like, the, the, the field of,
[00:37:50.500 --> 00:37:57.860]   like, it's gonna, it's gonna be, uh, very case-by-case. You have to, like, run different, different things.
[00:37:57.860 --> 00:38:02.260]   There, there's not a lot of good intuition that I have built that I can share with you. Um, but I'm
[00:38:02.260 --> 00:38:06.020]   sure if you, like, look online, everyone will have their own opinions. Yeah. Thank you.
[00:38:06.020 --> 00:38:12.900]   Yeah, I'm curious on, um, after you fine-tune, can you still tweak the system prompt? Or if you do,
[00:38:12.900 --> 00:38:17.860]   do you need to then fine-tune again? Yeah, that's a good question. You can tweak the system prompt,
[00:38:17.860 --> 00:38:25.700]   but, um, you are now out of distribution. And you are sort of hoping the model was able to learn from your,
[00:38:25.700 --> 00:38:32.900]   like, enough out of distribution behavior that it won't. I would recommend if you fine-tune the prompt,
[00:38:32.900 --> 00:38:38.420]   always use that prompt. If you're gonna change the prompt, fine-tune again. Um, or just, like,
[00:38:38.420 --> 00:38:42.820]   use many different prompts during your fine-tuning. You don't have to use just one. Uh, if you use
[00:38:42.820 --> 00:38:47.140]   multiple different prompts, you might, like, you might prevent it from collapsing too much into,
[00:38:47.140 --> 00:38:53.460]   like, one specific kind of behavior. Um, but, good question. Uh, yeah?
[00:38:53.460 --> 00:38:56.340]   Uh, can you go back to the notebook for one second?
[00:38:56.340 --> 00:38:57.300]   Yep.
[00:38:57.300 --> 00:39:03.620]   It should look to be, like, fine-tuning the one-tune for four-o-mini worse. Is that true?
[00:39:03.620 --> 00:39:06.340]   And if that happens, have you no more fine-tuning?
[00:39:06.340 --> 00:39:13.540]   Fine-tuning for four-o-mini. For four-o-o. Oh.
[00:39:13.540 --> 00:39:15.540]   For four-o-o. For four-o-o.
[00:39:15.540 --> 00:39:21.540]   For five-o-o-o. Uh, uh, uh, how do you know when it's gonna start to go up? It's really a lot of things.
[00:39:21.540 --> 00:39:30.260]   Well, yes. That's a good question. I don't know what happened. I think, um, either I made a mistake,
[00:39:30.260 --> 00:39:39.140]   or I, um, I would love to pull up the graphs right now. Uh, the reason I'm not is because they,
[00:39:39.140 --> 00:39:44.660]   I think I ran these on the OpenAI org, and I don't want to pull that up on screen live. Um,
[00:39:44.660 --> 00:39:52.020]   but that's what I recommend you do. Yeah. Uh, yeah. Um, regarding, like, overfitting,
[00:39:52.020 --> 00:39:58.580]   do you have, like, uh, ergonomics to do that in the SDK? Like, like, defining the test set, and then?
[00:39:58.580 --> 00:40:01.780]   Oh, yeah, yeah, yeah. I mean, you, you can define a test set, validation set, and it'll, like,
[00:40:01.780 --> 00:40:07.460]   compute both. And you can determine when, when you see the, like, the curve go, go up of, like,
[00:40:07.460 --> 00:40:12.180]   the test set, like, when you see them cross to stop the job, or you have to check it manually?
[00:40:12.180 --> 00:40:17.860]   I guess I can just pull up, like, an old, older one. Um, I just don't know how successful they'll be.
[00:40:17.860 --> 00:40:20.580]   But, like, you know, you get these...
[00:40:20.580 --> 00:40:30.180]   What was I doing? Yeah, but, like, the blue line is validation loss, and the green one is. This is
[00:40:30.180 --> 00:40:34.900]   clearly, like, not an amazing fine-tuned run. I think this was pretty old, but this is, like, what the
[00:40:34.900 --> 00:40:41.460]   graphs look like. Yeah. Okay. I'm realizing, uh, we still have a lot left, and I'm running out of time.
[00:40:42.020 --> 00:40:48.100]   So let's keep it chugging along. Um, cool.
[00:40:48.100 --> 00:40:56.980]   Okay. So rules of thumb for supervised fine-tuning. It's best for simple tasks. Models can regress on
[00:40:56.980 --> 00:41:02.660]   other tasks. So, I mean, this regression that you called out, confusing. I'm not sure what it was.
[00:41:02.660 --> 00:41:08.420]   But the thing is, if you then test it on different things that you did not fine-tune for, you may see
[00:41:08.420 --> 00:41:13.220]   regressions. So this is for very constrained cases. Data diversity is critical for that reason.
[00:41:13.220 --> 00:41:18.260]   Anything you don't include might regress. Um, 100 samples to start. 500+ is best.
[00:41:19.140 --> 00:41:27.300]   So, okay. DPO. Uh, training a model on jokes. Uh, so
[00:41:27.300 --> 00:41:33.940]   my thinking here was, like, if we take a model, like, jokes -- like, this funniness principle is,
[00:41:33.940 --> 00:41:38.660]   like, very hard to evaluate for, but you know between two options whether something is funny or not. So,
[00:41:38.660 --> 00:41:45.220]   this feels like a good candidate for DPO. Uh, so I generated a bunch of jokes for -- with 4.5, hoping
[00:41:45.220 --> 00:41:50.660]   they were going to be good. Um, most of them were not. Some of them were. Uh, I really wanted a data
[00:41:50.660 --> 00:41:55.460]   set. And this is really what it comes down to. All of these are about the quality of data. I did not have
[00:41:55.460 --> 00:42:02.820]   high-quality data. I had some medium-quality data with a, um, like, mediocre validator. So, like,
[00:42:02.820 --> 00:42:09.300]   uh, then did some manual filtering. Like, did I laugh? Did I, like, exhale a little bit more? And then,
[00:42:09.300 --> 00:42:15.300]   um, okay. The interesting part is, once I have this subset of good jokes, which really should have been,
[00:42:15.300 --> 00:42:20.100]   like, a nice data set. Uh, remember what we're trying to do, right? We're trying to get some inputs
[00:42:20.100 --> 00:42:28.100]   and then two pairs. So I went backwards again from joke. I extracted the topic. Um, and once I had a
[00:42:28.100 --> 00:42:34.180]   topic for each joke, I once again went forward, uh, with 4.0 and asked it to, like, generate a corny joke
[00:42:34.180 --> 00:42:41.780]   for that topic. Um, and so what we're left with is, once again, input topics and then good jokes and
[00:42:41.780 --> 00:42:47.860]   bad jokes for that topic. Um, and so hopefully the, like, main delta between those jokes is going to
[00:42:47.860 --> 00:42:52.420]   be one is good and one is bad. They're about the same topic. So, like, that delta should be stronger,
[00:42:52.420 --> 00:42:56.900]   um, as opposed to just giving it two unrelated jokes. Maybe it's, like, ah, like, you know,
[00:42:56.900 --> 00:43:01.300]   he's looking for, like, cat jokes or something. No, it's, like, similar jokes. One's funny. One is not.
[00:43:01.300 --> 00:43:11.540]   Um, did it work? Sort of. Um, this is one of the results that I got. So two guys are hunting when one
[00:43:11.540 --> 00:43:15.700]   collapses suddenly, stops breathing, and appears lifeless. Panics, his friend calls emergency
[00:43:15.700 --> 00:43:19.780]   services. Help! My friend collapsed. He's not breathing. I think he's dead. What should I do?
[00:43:19.780 --> 00:43:24.900]   The operator calmly replies, stay calm. First, let's make sure he's really dead. And there's a short
[00:43:24.900 --> 00:43:31.140]   silence followed by a loud gunshot. And then the guy comes back and phone, okay, now what? Okay, so,
[00:43:31.140 --> 00:43:41.300]   is it original? I don't think so. But, like, you know, it's funny. Uh, cool. So, um,
[00:43:41.300 --> 00:43:46.100]   Um, yeah, but, like, can you tell me an original joke right now?
[00:43:46.100 --> 00:43:48.500]   It's exactly that joke. Exactly that joke?
[00:43:48.500 --> 00:43:51.460]   Ten years ago, I never read that.
[00:43:51.460 --> 00:43:56.660]   I agree. Me too. I would say almost all the jokes that the models generate,
[00:43:56.660 --> 00:44:02.900]   like, you are hard-pressed to make, like, a good one. But I think it's still, like, it's not a null
[00:44:02.900 --> 00:44:08.580]   result. Because the fact that it was able to, like, give me a good joke on command is a hard thing to do,
[00:44:08.580 --> 00:44:12.740]   right? Like, I think if you ask Chachapiti for a good joke now, it's, like, harder to get that to
[00:44:12.740 --> 00:44:17.940]   happen. So even though these are, like, not new jokes, the idea is to, like, elicit the good jokes.
[00:44:17.940 --> 00:44:22.900]   Um, what was I going to show you? Where's my notebook?
[00:44:24.740 --> 00:44:32.340]   Um, yeah. So here is the data in the format that we said, right? So we have the user input,
[00:44:32.340 --> 00:44:36.580]   then tell me a joke about topic, and then we have the preferred output,
[00:44:37.460 --> 00:44:42.740]   and then the non-preferred output. Amazing. Cool. Let's keep going.
[00:44:42.740 --> 00:44:53.780]   Um, okay. So DPO, it's more forgiving about data diversity, because if you give it, like,
[00:44:53.780 --> 00:45:00.260]   a bunch of examples that are all specifically jokes, and then I ask for, like, a speech about
[00:45:00.260 --> 00:45:05.380]   something, um, it's not going to give me a joke necessarily, right? Because it didn't learn to
[00:45:05.380 --> 00:45:12.260]   say jokes. It learned to get funnier, hopefully. Um, so it's, like, that delta is important,
[00:45:12.260 --> 00:45:19.460]   and it is less constraining than SFT. Um, it's much more for, like, these "what should you prefer"
[00:45:19.460 --> 00:45:24.260]   scenarios, and, like, it's harder to evaluate unless you have, like, humans or, like, some, like, natural
[00:45:24.260 --> 00:45:29.780]   signal. Um, and you can actually run it after SFT. Okay. Reinforcement fine tuning. The one I wanted to
[00:45:29.780 --> 00:45:35.940]   save time for, which I didn't. Eh, I still have some time. Um, the shape of it, as we talked before,
[00:45:35.940 --> 00:45:42.580]   this is, I think, a pretty exciting one, is, like, as time goes on, um, I think more and more use of
[00:45:42.580 --> 00:45:48.260]   this is going to start happening, because this is the first way, um, that I see that, like, we are really
[00:45:48.260 --> 00:45:53.940]   making this algorithm public, like, this reinforcement learning algorithm public, and you can, like, become,
[00:45:53.940 --> 00:46:00.180]   like, get soda, like, state-of-the-art results on whatever your task is, um, like, beyond 01 and 03.
[00:46:00.180 --> 00:46:07.620]   Um, cool. So what does this algorithm look like? You have a bunch of inputs. Give it to the model.
[00:46:07.620 --> 00:46:11.460]   Model, for each of the inputs, generates this, like, chain of thought to the output.
[00:46:11.460 --> 00:46:18.100]   Then you have your grader. Your grader evaluates the results, right? Um, how does it do this?
[00:46:18.100 --> 00:46:23.620]   However you want. Uh, and we'll get to that in the next slide. But importantly, when it performs
[00:46:23.620 --> 00:46:29.940]   well, it reinforces those behaviors specifically and whatever led to that chain of thought. And so,
[00:46:29.940 --> 00:46:36.900]   good chain of thoughts, um, that, like, get high scores are more likely, like, to keep going. Like,
[00:46:36.900 --> 00:46:42.020]   to -- it's more likely to generate those chains of thoughts that lead to high results, which is exactly
[00:46:42.020 --> 00:46:46.740]   how, like, 01, 03 and these reasoning models are trained in the first place. So you can do this.
[00:46:46.740 --> 00:46:55.300]   Um, now, these are the very, very important things to keep in mind. You want to have an unambiguous
[00:46:55.300 --> 00:47:00.500]   grading. Um, in an example you'll see later, the grading was a little bit less, uh, unambiguous,
[00:47:00.500 --> 00:47:06.500]   and the results varied. Um, you want very low -- and by unambiguous, it just means, like, if you ask a bunch of
[00:47:06.500 --> 00:47:12.340]   people, uh, who are, like, fairly smart and have the same rubric how to get to the answer, they will all give you the exact same answer.
[00:47:12.340 --> 00:47:17.620]   Um, you want very low noise data. So if you have, like, any mistakes in your data,
[00:47:17.620 --> 00:47:23.220]   they're going to, like, magnify in a way that is not normal for, like, SFT and DPO. So it's very,
[00:47:23.220 --> 00:47:28.580]   very, very important for you to clean your data and not have, like, noisy signal. You want it to be high
[00:47:28.580 --> 00:47:33.380]   signal, like -- and this kind of goes with the last one -- you want it to be able to actually get to the
[00:47:33.380 --> 00:47:38.820]   solution, um, because it's learning to reason. It's not necessarily learning, like, new facts or, like, new
[00:47:38.820 --> 00:47:47.060]   new things. It's, like, learning how to get to the solution. Um, and because it is, like, so -- it's
[00:47:47.060 --> 00:47:52.100]   very data efficient. You can give it, like, 40 to 80 very high-quality, very high-signal examples,
[00:47:52.100 --> 00:47:57.140]   and it can, like, learn to reason over them and generalize, uh, to other examples in that set.
[00:47:57.140 --> 00:48:03.700]   Uh, cool. So now let's talk about graders. Uh, there's a whole myriad of graders you can pick from.
[00:48:03.700 --> 00:48:09.940]   There's string, like, string checks. Um, there's text similarity. Um, you can write Python code,
[00:48:09.940 --> 00:48:16.980]   sandboxed, no internet access, a few libraries, preloaded. Um, and you can have that Python code be
[00:48:16.980 --> 00:48:23.940]   the grader itself. You can have, uh, score, like, model graders. So model graders are just, like, the -- like,
[00:48:23.940 --> 00:48:29.140]   using the LLM with a prompt. Um, one that outputs a score, one that outputs a label. And we have a
[00:48:29.140 --> 00:48:33.780]   multi-grader. You can nest these as much as you want. And then you write a combination function.
[00:48:33.780 --> 00:48:39.460]   Um, and so this is how you can define the grader. I'd say it covers most of the cases where you would
[00:48:39.460 --> 00:48:44.740]   want to have something graded. Um, and you can get pretty, pretty good results.
[00:48:44.740 --> 00:48:54.420]   So, uh, this was my idea for the talk. Um, I hate email. Let's make a model that can, uh, do the thing -- like,
[00:48:54.420 --> 00:48:58.900]   I -- it's not that I wanted to respond to the email. It's I want to know whether it has to show
[00:48:58.900 --> 00:49:04.500]   it to me or whether I can ignore it and never have read it in the first place. Uh, so that was the idea.
[00:49:04.500 --> 00:49:11.460]   So I made this little, like, email labeler for myself. And I became a little data labeling person for
[00:49:11.460 --> 00:49:19.220]   a day. Um, turns out if you make an interface that is nice, you can go so much faster. Um, so I labeled
[00:49:19.220 --> 00:49:26.260]   around 600 emails. Uh, my -- my categories were, like, uh, am I going to glance, ignore, archive it,
[00:49:26.260 --> 00:49:30.820]   which is, like, receipts, store it, which is, like, I want to see it later, respond within different
[00:49:30.820 --> 00:49:34.580]   time frames, take action within different time frames. I actually didn't end up using the topic.
[00:49:34.580 --> 00:49:41.620]   Um, and I can download it as a JSON-L. Now, what do the instructions look like? I have some formatting.
[00:49:42.260 --> 00:49:49.380]   I have some classification instructions with, like, a little bit of extra instructions, but I didn't try
[00:49:49.380 --> 00:49:55.220]   very, very hard with this prompt. This will come back to bite me. Um, and then the input itself, I
[00:49:55.220 --> 00:50:04.180]   formatted as, um, like, this, like, XML-ish. So I wrapped each in an email. The input is an email with
[00:50:04.180 --> 00:50:12.260]   sender, subject, date, and then the stringified body, uh, plain text of the email. How -- um,
[00:50:12.260 --> 00:50:18.580]   yeah. So this is -- then I formatted the data for reinforcement fine-tuning, which is a set of messages
[00:50:18.580 --> 00:50:24.500]   in the same way that you would for SFT or all the others. Um, and then I use reference answer. You
[00:50:24.500 --> 00:50:29.940]   can use any other fields that you want, but that is essentially fields that are accessible to your grader.
[00:50:30.660 --> 00:50:35.220]   And then the grader, which I defined below, uh, is just a string check grader that takes the
[00:50:35.220 --> 00:50:41.220]   reference answer action and compares it to the sample output. And if they're equal, we're good.
[00:50:41.220 --> 00:50:54.740]   Um, now how'd it go? Like, okay. Not amazing. Um, and I ha -- I think I know why, right? So first of all,
[00:50:54.740 --> 00:51:02.100]   like, this is just my whim. Like, whimsy deciding whether an email is important or not. It's a very
[00:51:02.100 --> 00:51:06.820]   hard function to model. If I were -- if I was asked to label the same emails again, would I hit, like,
[00:51:06.820 --> 00:51:12.260]   a hundred percent the same classifications for all of them? Probably not. And that's exactly the scenario
[00:51:12.260 --> 00:51:18.020]   you want to avoid. Um, it was somewhat noisy. I think I, like, screwed up, like, one or two. Uh, and that,
[00:51:18.020 --> 00:51:24.660]   like, uh, compounds. Um, 150 examples, that was actually fine. Like, if they were a very high
[00:51:24.660 --> 00:51:29.700]   signal, this would have been, like, a better result, but they weren't. Um, and I didn't really try with
[00:51:29.700 --> 00:51:34.500]   a prompt. The model doesn't know what I want, and it's trying to figure it out. It's like -- it's like
[00:51:34.500 --> 00:51:40.020]   as if I gave you this task with the emails, and I locked you in a room, and you get to, like, maybe write
[00:51:40.020 --> 00:51:45.940]   some notes, uh, and I, like, slip you some food and water. Um, and then the greater, uh, you know,
[00:51:45.940 --> 00:51:50.340]   string quality, that was fine, but there's only, like, a few categories, so the model could guess
[00:51:50.340 --> 00:51:57.860]   some, and you wanted to make it not very guessable. Um, okay, so that was my failure. Now let's take a
[00:51:57.860 --> 00:52:03.540]   look at something that, like, reinforcement fine-tuning is actually quite good at, right? So, um, predicting
[00:52:03.540 --> 00:52:09.140]   the number of hydrobond -- uh, sorry, yeah, hydrobond -- I think it's -- yeah, hydrobond donors and
[00:52:09.140 --> 00:52:15.460]   acceptors. Um, this is actually fully out of what I understand, um, but the model can learn to do this
[00:52:15.460 --> 00:52:23.300]   quite well. So the task is to take in some chemicals, um, formatted in a specific format, and then predict
[00:52:23.300 --> 00:52:30.580]   the donors and acceptors, uh, to analyze it. And this is an example of one of the inputs. Um, now the output
[00:52:30.580 --> 00:52:37.300]   schema is defined on the left. We have acceptors and donors, uh, which are just two numbers, uh, and it
[00:52:37.300 --> 00:52:45.540]   just has to output these two fields. And then on the right, uh, I have my grader, which is a multi-grader.
[00:52:45.540 --> 00:52:51.140]   It just checks for each of them, right? And the weight is, it gets half if it gets the donor right,
[00:52:51.140 --> 00:52:56.660]   half if it gets the acceptor right. Um, I'm gonna pause here, because this is an important slide. If there's
[00:52:56.660 --> 00:53:05.380]   any clarifying questions, otherwise I'll move on. Amazing. Um, so what were the results? Much better.
[00:53:05.380 --> 00:53:13.380]   This started at roughly, like, 65 percent. Um, it ended up going to -- I think it started, like,
[00:53:13.380 --> 00:53:20.500]   touching 80 percent, right? And this is O4 Mini. Um, O4 Mini, like, does not have a very vast world
[00:53:20.500 --> 00:53:24.660]   knowledge. It really has to, like, learn to reason through things. Um, I'd say even this
[00:53:24.660 --> 00:53:31.060]   result is not, like, fully exemplary of how good it can get. The reason it's not as good is because
[00:53:31.060 --> 00:53:36.020]   it does need some world knowledge to understand, um, like, this sort of simulation that you have to do
[00:53:36.020 --> 00:53:41.300]   with the hydro bonds. Um, but you still see this, like, very, very high performance that it, like,
[00:53:41.300 --> 00:53:46.180]   you would not get with something like supervised fine-tuning. Yes?
[00:53:46.180 --> 00:53:52.740]   Can you use DPO for this? Could you, like, hear an email example with DPO first and then do that second?
[00:53:52.740 --> 00:53:57.140]   Yeah. One thing I forgot to put in the slides. SFT and DPO you can do with non-reasoning models.
[00:53:57.940 --> 00:54:06.340]   RFT you can do with reasoning models, and there's no interplay. Yes. Sorry. He asked, um, if there --
[00:54:06.340 --> 00:54:14.820]   I'm blanking -- whether you can do DPO before RFT. And the answer was, uh, SFT and DPO are for
[00:54:14.820 --> 00:54:21.860]   non-reasoning models. RFT is for reasoning models. And it's like an IFF. Um, cool. So, some thoughts on,
[00:54:21.860 --> 00:54:27.380]   uh, notes on reinforcement fine-tuning. Only for reasoning models. It's extremely sensitive to noisy data.
[00:54:28.340 --> 00:54:34.340]   Um, agentic use cases are limited by a single turn. Right? So, you can't yet have these, like,
[00:54:34.340 --> 00:54:40.020]   long horizon tasks with multiple turns and function calls. Uh, and reinforcement fine-tuning. I would
[00:54:40.020 --> 00:54:45.860]   love to see that. We'll see if we ship it at some point. And then, um, you want the data to be high
[00:54:45.860 --> 00:54:50.980]   signal, uh, and you want a solid grader. And if you provide both, you are likely going to reach, like,
[00:54:50.980 --> 00:54:58.660]   state-of-the-art performance on the task that you are going after. Um, cool. So, this, I think,
[00:54:58.660 --> 00:55:06.580]   is one of, like, the most telling slides. I believe that is most of what I have for today. Um,
[00:55:07.460 --> 00:55:12.500]   I have a notebook. I have a ton of stuff that I didn't quite go through. Um, I'm happy to dive
[00:55:12.500 --> 00:55:18.100]   into that. But I kind of want to leave this open for questions and, like, poking around, um, different
[00:55:18.100 --> 00:55:25.060]   parts. Actually, how much time do I have left? An hour? Oh, great. We can do so much in this time.
[00:55:25.060 --> 00:55:33.300]   Uh, you guys are captive. Okay. Yeah, I do want to do a little quick questions and then we'll move on.
[00:55:33.300 --> 00:55:40.580]   One thing I started and didn't do is another kind of tuning, um, which I can do live, which is prompt
[00:55:40.580 --> 00:55:46.580]   tuning, um, which I'm going to make up on the spot. So, if you want to see that, stick around. Um, but,
[00:55:46.580 --> 00:55:52.820]   yeah, questions. Yeah. Yeah. Hi. Uh, so, I had a small question. Uh, so, you're saying that if I have a
[00:55:52.820 --> 00:56:01.460]   non-reasoning model and let's say I do an SFT step with it, uh, can I induce thinking into it? For
[00:56:01.460 --> 00:56:08.660]   example, if I create a dataset input-output pairs, which has, let's say, thinking into it as well. So,
[00:56:08.660 --> 00:56:17.060]   would you call it the same amount of thinking as you'd get in a, uh, reasoning model? Um, you would get
[00:56:17.060 --> 00:56:21.780]   thinking. It's a very kind of, a very different kind of thinking because it's, it's purely, it's learning
[00:56:21.780 --> 00:56:29.060]   by imitation. Um, so, it's not learning to, like, uh, reason in order to get to an answer. It's
[00:56:29.060 --> 00:56:35.620]   learning to imitate the thought process, um, regardless of whether it's actually useful to it
[00:56:35.620 --> 00:56:41.780]   or not. Right? Um, however, on the other, like, whereas reinforcement fine-tuning, like, it is,
[00:56:41.780 --> 00:56:46.340]   it is the one producing the chain. And so, like, if it gets to the right answer, it means that chain was
[00:56:46.340 --> 00:56:51.700]   useful, like, in, in practice. And that is true for, like, every single time that it gets reward. It's
[00:56:51.700 --> 00:56:58.260]   because, like, it managed to get there. So, it's, like, it's pretty different. I'd say, like, yeah, SFT with a,
[00:56:58.260 --> 00:57:04.580]   um, reasoning is, like, what we started to see a little bit, like, maybe a year ago. Um, but
[00:57:04.580 --> 00:57:09.940]   reinforcement, uh, learning is really how you get, like, high-quality learning. Yeah, like, chains of
[00:57:09.940 --> 00:57:17.540]   thought. Uh, yeah. Yep. Or, sorry, somebody has a mic. Yeah. Here we go, yeah. Um, thanks, Ilhan. Could you go back to the slide, um,
[00:57:17.540 --> 00:57:25.060]   where you showed the inputs to the RFT? Yeah, this one. Um, this one. This one. Does it, um, so for the
[00:57:25.060 --> 00:57:33.540]   calculate output, can I, um, basically, instead of just, like, a, a lockup into a dictionary, can I have
[00:57:33.540 --> 00:57:40.980]   functions there? Like, like, to, so, so, can I pass in, uh, a function to check if the code runs, and I'll put a
[00:57:40.980 --> 00:57:46.980]   number, or, like, a one or a zero, to see if it runs, for example? Um, can you, can you say it again? So, if, um, I'm just
[00:57:46.980 --> 00:57:51.620]   thinking, um, one of the guys in the team is fine-tuning a model at the moment to write, uh,
[00:57:51.620 --> 00:57:56.020]   Triton code, and one of the checks he has is, like, does the code actually run? Like, is it valid Python?
[00:57:56.020 --> 00:58:02.260]   Yeah. Is it possible to encode that as a grader? You could do that in a hacky way. Yeah. Right?
[00:58:02.260 --> 00:58:16.260]   So, you could use, like, a, um, a Python grader and inject the model response. Like, just do, like, um, eval or, like, exec,
[00:58:16.260 --> 00:58:21.060]   which, if you've come to my previous AI engineer talk, I did the same thing. Don't do this. But you
[00:58:21.060 --> 00:58:25.700]   could, you could, uh, get it to work. Yeah. Uh, if it has the right libraries and doesn't need internet
[00:58:25.700 --> 00:58:30.500]   access, which it might not. Yeah. And, um, just the, and then on the, the weights you have there,
[00:58:30.500 --> 00:58:37.140]   does it matter the magnitude, I guess, of, like, what you're calculating? Like, should you, like, aim for it
[00:58:37.140 --> 00:58:41.940]   to sum to, to one, or, or hundred, or, like, it doesn't really, like, match a massive difference?
[00:58:41.940 --> 00:58:46.340]   I don't think it matters. Yeah. Yeah. Um, it, asterisk in the way that, like,
[00:58:46.340 --> 00:58:53.620]   it, it doesn't really matter. Asterisk. You can look into the asterisk. Yeah. Yeah. Cool. Thanks.
[00:58:55.620 --> 00:58:59.380]   Um, I want to do some in the back because I think I've been missing some in the back. Yeah. Yeah. You,
[00:58:59.380 --> 00:59:05.940]   you mentioned that it wouldn't, the RFT wouldn't work for multi-step, uh, agent stuff. Could you talk a
[00:59:05.940 --> 00:59:12.900]   little bit more on, like, why? And if, if you, if OpenAI were to make it work for that, what that would
[00:59:12.900 --> 00:59:19.700]   look like? I just kind of didn't understand. Yeah. Yeah. So, so the reason I said that, um, is because
[00:59:19.700 --> 00:59:26.580]   right now it only does a single turn and only evaluates the output of the model. If the model,
[00:59:26.580 --> 00:59:31.620]   say, like, calls a function, there's no way to provide it the result and let it keep going.
[00:59:31.620 --> 00:59:37.060]   Right. And so that's really what you want for, like, an agentic evaluation loop, which RFT does not
[00:59:37.060 --> 00:59:44.100]   currently have. Um, I, I think that would be awesome if, like, we did have a way to, like,
[00:59:44.100 --> 00:59:51.860]   um, provide the output of the model and, like, do these more end-to-end training. Um, but this is
[00:59:51.860 --> 00:59:57.540]   what we have right now. Gotcha. Yeah. Thank you. I want to do in the back first a little bit. Yeah.
[00:59:57.540 --> 01:00:05.300]   For, for the HydroBond example with RFT, you can do a generic string check, but you can inject more
[01:00:05.300 --> 01:00:13.220]   domain area as a type of balance files if it's only using, like, chemical examples. You know, when do you
[01:00:13.220 --> 01:00:18.580]   need to design that type of custom grader, and when is a generic grader sufficient? Yeah. So the beginning
[01:00:18.580 --> 01:00:27.300]   of the question was, in the, uh, HydroBond example, um, we use, like, yeah, this, this string checks. When is it
[01:00:27.300 --> 01:00:34.900]   worth using more complex, um, graders? I think the answer is, like, where you think it would benefit,
[01:00:34.900 --> 01:00:41.300]   uh, like, is it a more accurate score? Is it a higher signal grade? Because if the answer is yes,
[01:00:41.300 --> 01:00:45.540]   then you might actually just get higher, like, better results. Right? It's going to maximize
[01:00:45.540 --> 01:00:50.740]   whatever this grader says is good. And so if you can better tune what that means, like, what good means,
[01:00:50.740 --> 01:00:58.340]   you will just get better results. Yeah. Um, or does someone else have the mic already? Yeah.
[01:00:58.340 --> 01:01:02.740]   Can you tell us more about the RFT algorithm or some details about it? Yeah, yeah, yeah, here, here,
[01:01:02.740 --> 01:01:10.340]   just come here. No, I'm kidding, no. Um, I mean, it, I think, obviously, I can't share, like, the exact details, but, like,
[01:01:10.340 --> 01:01:15.620]   like, it is, it is fundamentally, like, a very, very similar algorithm to the exact algorithm that we
[01:01:15.620 --> 01:01:21.540]   used for 01 and 03. Right? Like, this reinforcement learning with language models is, like, exactly how
[01:01:21.540 --> 01:01:28.660]   we did our reasoning models. Um, and we are putting as much out as we can. Right? And so, like, you can
[01:01:28.660 --> 01:01:35.060]   actually get these, like, really, really insane results, um, if you have clean data in, uh, in your
[01:01:35.060 --> 01:01:39.860]   scenarios. Yeah. The reason, I really wanted to come with, like, a lot of really good examples. The reason
[01:01:39.860 --> 01:01:44.180]   I can't come with a lot is, like, A, a lot of them are, like, proprietary and I can't share. And B, I wasn't
[01:01:44.180 --> 01:01:51.940]   able to do many myself because I don't have a lot of really good data that I work on. Um, yeah. So, back there.
[01:01:51.940 --> 01:02:02.260]   So, our time is our most precious resource. How do you determine when fine-tuning is worth it versus
[01:02:02.260 --> 01:02:09.460]   integrating with a new model or getting better data? Yeah. Maybe this is a little bit of a hot take,
[01:02:09.460 --> 01:02:20.820]   but I would avoid fine-tuning until you need it. Um, right? Like, if you, um, if you can get away with
[01:02:20.820 --> 01:02:28.180]   prompting, you should just use prompting. Yeah. Uh, and fine-tuning, the way you actually know -- sorry,
[01:02:28.180 --> 01:02:33.780]   I blanked for a second -- um, you really want to evaluate. Right? Like, if you, if your evals are
[01:02:33.780 --> 01:02:37.460]   showing that, like, you are reaching the limits of what you can accomplish through prompting,
[01:02:37.460 --> 01:02:42.180]   but you know it's better to do pos -- you know it's possible to do better, then that's where you can
[01:02:42.180 --> 01:02:48.660]   pull out fine-tuning. If you have the data, if you have everything set up, um, it is a big upfront cost,
[01:02:48.660 --> 01:02:55.220]   right? But if you, if you are in a space, like, uh, you know, legal or, like, uh, in, like, um, hard sciences,
[01:02:55.220 --> 01:03:00.020]   like, it can really be worth it to do something like reinforcement fine-tuning, because the results
[01:03:00.020 --> 01:03:02.980]   you get are, like, impossible to achieve otherwise.
[01:03:02.980 --> 01:03:13.620]   I have a question. What's happening on the OpenAI platform side when we do fine-tuning? So, is it generating, like,
[01:03:13.620 --> 01:03:18.660]   a new set of weights, and then when you actually go to run inference, does it load those weights
[01:03:18.660 --> 01:03:24.420]   into memory and run on that, or, like, how does that work? Yeah. So, it is generating new weights.
[01:03:24.420 --> 01:03:29.700]   It's generating this LoRa component, right? So, it's generating, like, this little piece that we use
[01:03:29.700 --> 01:03:33.940]   with the models. Um, the rest of the weights are the same in the model. And so, what it does when you use
[01:03:33.940 --> 01:03:38.100]   your inference is it loads in that part of the weights and then uses those. Yeah.
[01:03:38.100 --> 01:03:46.740]   I had a question on DPO. Um, how bad should your bad examples be? Like, with a joke, should it be
[01:03:46.740 --> 01:03:52.180]   just barely bad? Like, it's non-obvious and so you're squeezing out nuance? Or should it just be,
[01:03:52.180 --> 01:03:57.380]   like, really bad? I think you sort of answered your own question, right? If you make it too bad,
[01:03:57.380 --> 01:04:04.260]   then you are learning, um, how to go from, like, really bad to, like, good, which might not be what
[01:04:04.260 --> 01:04:08.020]   you want to teach the model. You might want to teach the model how to go from, like, mediocre to really
[01:04:08.020 --> 01:04:13.460]   good, right? And so, yeah, like, you want to sort of mimic what you would expect to see.
[01:04:13.460 --> 01:04:21.620]   Um, or you want to think, like, what is that, um, delta? I think I haven't actually heard of too
[01:04:21.620 --> 01:04:29.380]   many other people doing, like, synthetic DPO with generated bad examples. Um, most often it's just
[01:04:29.380 --> 01:04:37.940]   used for A/B tests because the signal from an A/B test is real and pure, right? The signal from synthetic DPO
[01:04:37.940 --> 01:04:46.420]   is as good as your imagination. Maybe. And, like, your intuition. So, I, yeah. Like, the best would
[01:04:46.420 --> 01:04:51.060]   be to, like, actually have people sit in front. I was thinking about doing this. Like, sending,
[01:04:51.060 --> 01:04:54.340]   sending everyone, like, a little website and having you labeled a bunch of examples. But then I realized,
[01:04:54.340 --> 01:04:58.740]   even if we did that, I wouldn't be able to train it live. Um, but that would be, that would be, like,
[01:04:58.740 --> 01:05:07.220]   good, real signal, right? Perfect signal for DPO. Uh, yes. You're gonna have to yell.
[01:05:07.220 --> 01:05:13.700]   So, I'm kind of curious about why RFT doesn't work for non-reasoning models. Like, say I wanted to
[01:05:13.700 --> 01:05:20.900]   get a model that only produced output precisely 30% emojis. I don't think I could do that with DPO,
[01:05:20.900 --> 01:05:27.780]   or could I? Uh, the question is why RFT doesn't work with non-reasoning models. What was the bit
[01:05:27.780 --> 01:05:33.460]   about emojis? I missed that. Sorry. Um, if I wanted to get a non-reasoning model
[01:05:33.460 --> 01:05:42.260]   to only output with 30% emojis exactly, I don't think I could do that with DPO, or correct.
[01:05:42.260 --> 01:05:52.740]   Um, exactly is tough, but I would bet that you could get somewhat close. Um, I mean, it depends.
[01:05:52.740 --> 01:05:56.180]   Are you trying to do, like, if it's a very specific constrained example?
[01:05:56.180 --> 01:06:15.220]   Yeah, um, the main reason it doesn't work is because we don't support it. Uh, yeah,
[01:06:15.220 --> 01:06:18.980]   so the question, the question is why doesn't reinforcement fine tuning work with non-reasoning
[01:06:18.980 --> 01:06:21.540]   models? And it's because we don't support it. Yes?
[01:06:24.980 --> 01:06:32.420]   Yeah. You have a couple of questions, so, uh, so does, does custom GPT you have, you know,
[01:06:32.420 --> 01:06:35.700]   does it use one of those fine tuning? Custom GPTs? Yeah.
[01:06:35.700 --> 01:06:40.420]   No. No. No. It's just a prompt. Okay. And second question is that, uh, for us,
[01:06:40.420 --> 01:06:46.340]   certain applications, it's also important to have good thinking, COT, you know? Yeah.
[01:06:46.340 --> 01:06:52.100]   But it looks like there's no reward for, or penalty or anything for that, like, if, you know. Um, yeah,
[01:06:52.100 --> 01:06:57.540]   I mean, we released a paper recently that talked about how you should avoid, um, directly evaluate,
[01:06:57.540 --> 01:07:02.020]   like, directly, uh, rewarding or penalizing certain behaviors in your chain of thoughts,
[01:07:02.020 --> 01:07:07.700]   because if you don't touch the chain of thought portion and only, um, put pressure on the results,
[01:07:07.700 --> 01:07:16.500]   um, essentially, you get faithful chains of thoughts. You, you, you can actually see, um, you hopefully close
[01:07:16.500 --> 01:07:21.380]   to what the model is, like, actually thinking through. Um, and so if it's doing something you
[01:07:21.380 --> 01:07:26.500]   might not want, it'll be clear. Um, but if you, like, for example, reward, like, investigate the
[01:07:26.500 --> 01:07:34.260]   chain of thought and, like, penalize it if it starts thinking about bad things, um, turns out, uh, you do
[01:07:34.260 --> 01:07:40.420]   reduce the ratio, like, the percentage of bad things by a little bit, but you reduce the cases where you can
[01:07:40.420 --> 01:07:45.220]   catch them by looking at the chain of thought to, like, zero. And so it's, like, not a good idea
[01:07:45.220 --> 01:07:48.260]   to do it. There's a, there's a paper on this. You should, you should look it up. We released.
[01:07:48.260 --> 01:07:53.860]   So, uh, uh, that question is, uh, you know, like, if you have a really good training data, right,
[01:07:54.820 --> 01:08:01.460]   then you can just distill the, any small models, even open source ones, right, I believe. So how,
[01:08:01.460 --> 01:08:07.940]   so, like, supervised training and you also showed DPO, if you have a really good quality data and where
[01:08:07.940 --> 01:08:15.380]   things are deterministic, especially the questions, then I believe there is no point in going to bigger
[01:08:15.380 --> 01:08:20.180]   models like OpenAI, right? Any open source models would do it. What, what, what do you think about that,
[01:08:20.180 --> 01:08:24.340]   you know? Well, you know, if it's, if it's doing what you want, that's awesome.
[01:08:24.900 --> 01:08:29.860]   Like, yeah, like, go with every, go with whatever is going to get you the performance that you're
[01:08:29.860 --> 01:08:36.580]   looking for. I think just in practice, you know, the, I think the real consideration is, like,
[01:08:36.580 --> 01:08:42.020]   do you want to fine tune an open source model and deal with, like, um, like, if you have the data for
[01:08:42.020 --> 01:08:47.300]   it, that's amazing. If an open source model does what you want, use that. But if you're, like, if you
[01:08:47.300 --> 01:08:52.820]   have the choice between, like, fine tuning an open source model, um, which involves all the, like, pitfalls
[01:08:52.820 --> 01:08:58.420]   that I just talked about, versus just, like, asking 4.0, like, with no fine tuning, it's, like,
[01:08:58.420 --> 01:09:03.700]   one of the, is going to let you move faster. You can always start with a bigger model,
[01:09:03.700 --> 01:09:10.740]   and then, like, tune a smaller one, right? So, I'd say always start with, always start with the biggest,
[01:09:10.740 --> 01:09:15.620]   most expensive thing that'll make your thing work, and then work on getting more efficient. Yeah.
[01:09:17.380 --> 01:09:22.900]   Yeah, given, uh, the knowledge that you shared with us today, can you give us a little bit of insight
[01:09:22.900 --> 01:09:27.780]   why O3 scores higher, but, uh, hallucinates more than O1?
[01:09:30.580 --> 01:09:34.020]   In general? Like, why O3 hallucinates more than O1?
[01:09:34.020 --> 01:09:36.660]   Yeah. You have that on the, in the system, right?
[01:09:36.660 --> 01:09:40.740]   Yeah, yeah, yeah. I mean, I don't have, like, a reason that I can give you. I think that's
[01:09:40.740 --> 01:09:45.140]   something we're investigating. Obviously, it's something that we want to avoid, but, like, um,
[01:09:45.140 --> 01:09:51.220]   yeah. I don't think we have, like, a super solid, like, understanding we can share
[01:09:51.940 --> 01:09:57.060]   about why that happens. Um, and speaking for myself, I don't know. Yeah.
[01:09:57.060 --> 01:10:03.940]   Hiya. Uh, thanks a lot. This is great. Um, it seems like the hardest thing to do for, uh,
[01:10:03.940 --> 01:10:09.540]   reinforcement learning is the reward modeling, or, like, defining the reward functions. Do you have any
[01:10:09.540 --> 01:10:15.780]   heuristics, or tips, or resources on, like, uh, the number of reward functions, or, like, the type of
[01:10:15.780 --> 01:10:21.620]   problems that it suits or doesn't? I just feel like that there's, like, not that much to know when
[01:10:21.620 --> 01:10:27.620]   I'd, like, to reach for reward functions now, and, you know, how, how I find that it's, it's, it's almost
[01:10:27.620 --> 01:10:32.820]   like it's a, uh, a solution in search of a problem, you know? I don't know when I would, when I would do
[01:10:32.820 --> 01:10:39.140]   this, you know? When you would, when you would use reinforcement learning and, like, know, like, find five
[01:10:39.140 --> 01:10:44.580]   perfect reward functions that model some behavior I want, want to capture. Um, yeah, I think
[01:10:44.580 --> 01:10:51.540]   I think, I think when, it's not for the beginning of the problem, right? Let's say you already have
[01:10:51.540 --> 01:10:56.420]   something in place, it's already working, and, like, you're wondering if you could do even better. Like,
[01:10:56.420 --> 01:11:01.780]   the fact that you're asking yourself, can you do even better, means you already have a way to evaluate
[01:11:01.780 --> 01:11:06.900]   it, and already have a way to grade it. And so that's the beauty of it, it's like, by the time that you're
[01:11:06.900 --> 01:11:13.460]   asking yourself, like, should I use reinforcement fine-tuning, you should already have all the data
[01:11:13.460 --> 01:11:19.060]   ready, or almost close to ready, because all you need is, like, your, whatever eval setup you have.
[01:11:19.060 --> 01:11:24.900]   Yeah, so I have, like, a good LLM judge, uh, that I'm using for evals, and then I take that LLM judge,
[01:11:24.900 --> 01:11:29.780]   use it as a reward function. You can. To, to, to fine-tune. You can, yes. Okay, so that's the kind
[01:11:29.780 --> 01:11:34.580]   of path that's, like, start off, build a good LLM judge that captures the task you want to do well on,
[01:11:34.580 --> 01:11:40.180]   and then, you know, try milk a model with, like, or LL to make it better.
[01:11:40.180 --> 01:11:43.780]   Yeah, I mean, that, that's one way for sure, and, like, if that is the path that works for
[01:11:43.780 --> 01:11:47.780]   you, that's great. I wouldn't say there's one, like, necessarily one path.
[01:11:47.780 --> 01:11:54.900]   Okay. So actually in the crowd is Teo. Can I, can you wave? So, um, he wrote a cookbook that I used to
[01:11:54.900 --> 01:12:01.060]   base a lot of this examples for. Um, you should check it out. It's, I, I believe, what's it called?
[01:12:01.060 --> 01:12:08.020]   Can you yell out the name? Exploring, I think it's Exploring Model Graders for Reinforcement
[01:12:08.020 --> 01:12:13.140]   Fine-Tuning. Yeah, yeah. Incredible cookbook. You should all check it out. It goes into, like,
[01:12:13.140 --> 01:12:17.700]   the nitty-gritty of, like, how to do each of these different parts. It talks about different graders.
[01:12:17.700 --> 01:12:22.020]   It talks about, like, all the different pieces. So you should really check out, uh, that cookbook
[01:12:22.020 --> 01:12:26.580]   and the function calling one if you're curious. I'll see if there's a way for us to send it out.
[01:12:26.580 --> 01:12:36.340]   Maybe I can just, like, pull it up real quick. Um, I think it's this one. Yeah. So you should
[01:12:36.340 --> 01:12:42.100]   really check this out. Um, it's long, it's comprehensive, and it's, like, really good. Um,
[01:12:42.100 --> 01:12:49.140]   cool. Any-- Yeah, uh, I had a question here. Yeah. Uh, does the fine-tune API support different
[01:12:49.140 --> 01:12:55.940]   modalities as well, or it's just text and-- Oh, we do have image fine-tuning. I forgot. Yes. Wow.
[01:12:55.940 --> 01:13:03.140]   Okay. We have image fine-tuning, uh, and so you can do image input and, um,
[01:13:03.140 --> 01:13:09.620]   I believe it's text output. So it's really good for, like, bounding boxes and all these other parts.
[01:13:09.620 --> 01:13:17.060]   Wow. Thank you. It's like you're a plant. Uh... Stand up. Yes? Um, so how do you, um, teach model,
[01:13:17.060 --> 01:13:24.180]   uh, new knowledge or domain knowledge? So, and then how do you use that work with the reasoning? And, and one
[01:13:24.180 --> 01:13:29.540]   example is, like, if I have my personal experience, personal preference, should I do fine-tuning,
[01:13:29.540 --> 01:13:37.540]   or should I kind of put it in context? Um, I would say always try starting by putting things in context.
[01:13:37.540 --> 01:13:43.940]   Um, yeah. Fine-tuning, you want to, like, leave for, like, a later stage thing. You already-- you already
[01:13:43.940 --> 01:13:49.460]   have some data to, like, show one way or another. Um, in order to, like, give models new knowledge, you know,
[01:13:49.460 --> 01:13:57.300]   you have, like, all, like, different ways you can do rag and search. Um... Technically, when you fine-tune,
[01:13:57.300 --> 01:14:02.900]   you can put in small amounts of new information into the models. But in general, you should treat it as
[01:14:02.900 --> 01:14:11.780]   more of a methodology, right? Um, like, if you consider an algorithm data, like, in that it requires bits to
[01:14:11.780 --> 01:14:18.580]   describe the procedure, you're sort of teaching that. But you can't really give it, like, um,
[01:14:18.580 --> 01:14:25.700]   too, too much data. However, you-- it can-- it can learn a bit, right? Like, SFT starts to memorize
[01:14:25.700 --> 01:14:32.340]   certain things that you give it. Um, I just wouldn't say it's, like, the best way to give, like,
[01:14:33.380 --> 01:14:38.740]   correct information or, like, correct, like, referenced information. I'd say maybe it's good to,
[01:14:38.740 --> 01:14:44.340]   like-- I would focus it more on formatting. You can use it, right? Like, I think, like, two years ago
[01:14:44.340 --> 01:14:51.860]   at this point, um, when we released, um, fine-tuning for GPT 3.5, one of the interesting things you can do
[01:14:51.860 --> 01:15:02.020]   is, um, if you do retrieval, you can teach a model, like, with, um, like, embedding search,
[01:15:02.020 --> 01:15:06.660]   and you need to, like, embed the query. First, sometimes you want to, like, transform the user's
[01:15:06.660 --> 01:15:11.540]   query into something that looks more like your search results. Um, I think it's, like, hypothetical
[01:15:11.540 --> 01:15:18.820]   document embeddings. Um, you can fine-tune a model to, like, do, like, essentially hallucinate
[01:15:18.820 --> 01:15:23.380]   that better, right? Like, if you train it on, like, the actual outputs, uh, like, in your actual,
[01:15:23.380 --> 01:15:29.460]   like, knowledge base, then it's more likely to output something that is in distribution, um, for your
[01:15:29.460 --> 01:15:34.340]   knowledge base, and so you'll get, like, more direct mappings, if that makes sense. So, but it's, like,
[01:15:34.340 --> 01:15:38.180]   you're not really teaching it too much new information. It's just helping you find-- it's
[01:15:38.180 --> 01:15:43.460]   giving you a better intuition to find it. Yeah, so, like, for a reasoning model, sometimes if I want to
[01:15:43.460 --> 01:15:50.260]   reference to custom definition or custom knowledge in certain domain, is it good to kind of combine
[01:15:50.260 --> 01:15:56.100]   with reg and then generate that kind of training data, train with RL, or just, like, I guess that's,
[01:15:56.100 --> 01:16:02.660]   in general, how to solve that problem? Um, for that case, uh, I would just let the model-- like,
[01:16:02.660 --> 01:16:08.900]   give the model some search functions. Oh. Like, let the model do search. Um, if you have explicit search
[01:16:08.900 --> 01:16:13.860]   steps that you do beforehand, um, then you can maybe fine-tune, like, specific parts,
[01:16:13.860 --> 01:16:18.980]   but we, like, because search is, like, inherently a two-step process, it's like searching and then
[01:16:18.980 --> 01:16:24.260]   interpreting, you can't do that with reinforcement fine-tuning right now. I see, I see. Thank you.
[01:16:24.260 --> 01:16:30.900]   Yeah. Uh, so for your question-- for your thing about the email classifier, did you do the
[01:16:30.900 --> 01:16:34.340]   reinforcement learning just to kind of show us, like, a bad example? No, no, no, I really
[01:16:34.340 --> 01:16:38.180]   hoped it worked. Oh, okay. Uh, that was me learning-- Well, it's so ambiguous, right? And then you were like,
[01:16:38.180 --> 01:16:41.940]   well, don't be ambiguous with it, and I'm like, okay, uh, okay. Yeah, yeah, no, thanks for calling
[01:16:41.940 --> 01:16:45.780]   me out publicly in a talk, but keep going. Well, I'm not trying to do that, but-- I'm joking,
[01:16:45.780 --> 01:16:50.740]   I'm joking. For the supervised fine-tuning, that would have been the-- what was the correct answer
[01:16:50.740 --> 01:16:57.540]   there, then? Like, supervised fine-tuning, given your data set? Um, so what I wanted to try and, like,
[01:16:57.540 --> 01:17:02.020]   maybe if we have enough time, I'll, like, do a little bit of this, was, like, there are a lot of
[01:17:02.020 --> 01:17:09.620]   nuances around the email that I have, like, my preferences that are extremely low signal.
[01:17:09.620 --> 01:17:15.060]   Like, I might see it once. Like, I might get two very similar emails from two very similar, like,
[01:17:16.580 --> 01:17:23.860]   people, uh, or, like, sources, and one I decide to, like, glance, and one I decide to, like, ignore.
[01:17:23.860 --> 01:17:30.420]   How is I going to learn that, right? Um, like, what I was imagining is, like, if you find cases that are
[01:17:30.420 --> 01:17:36.020]   similar, and you could do that with embeddings, um, but that have different results, you might be able to
[01:17:36.020 --> 01:17:39.620]   give them to, like, to give them to, like, a reasoning model, and be, like, you know, figure out what's
[01:17:39.620 --> 01:17:45.540]   different here, and then make a note of it. And then you can compile this set of, like, principles,
[01:17:45.540 --> 01:17:51.380]   essentially, that hopefully guide what it does. And so this is, like, what I would consider almost, like,
[01:17:51.380 --> 01:17:58.340]   prompt tuning, where you, like, are finding the right context to provide the model based on a lot of examples.
[01:17:58.340 --> 01:18:04.340]   Uh, we can try in a little bit. That would be cool to see. Uh, like, I feel like embedding,
[01:18:04.340 --> 01:18:08.340]   fine-tuning the embedding model itself, though, would almost be critical, then, to get,
[01:18:08.340 --> 01:18:14.740]   right, for an individual, for an individualized, like... Why? What? How so? Why? Because embeddings
[01:18:14.740 --> 01:18:19.060]   naturally don't understand the nuance of, like, how you... Oh, no, no. I'm not saying using embeddings
[01:18:19.060 --> 01:18:23.460]   to find case, to, like, find the nuance. I'm saying using embeddings to find similar ones. Right,
[01:18:23.460 --> 01:18:28.260]   but how would it know what's similar based on your preferences? Not based on my preference. Based on, like,
[01:18:29.700 --> 01:18:36.260]   whatever embeddings similar... Like, does it, like... Finding similar in the naive sense, and then
[01:18:36.260 --> 01:18:42.340]   showing the model, like, naive similar cases that are actually different, and being like, look, these
[01:18:42.340 --> 01:18:48.020]   are... The embedding model thought these were similar, but they're actually profoundly different. For some
[01:18:48.020 --> 01:18:54.580]   nuanced reason, figure out what that is. Does that make sense? Yeah. Um, yeah.
[01:18:58.260 --> 01:19:05.220]   I feel like people kind of want to see me try this. Okay. So, I did start trying this.
[01:19:05.220 --> 01:19:12.500]   I'm also going to try not to flash too much of the data, because this is my actual emails.
[01:19:12.500 --> 01:19:22.260]   Okay. So, I have the email data set. I'm loading it. I have this, like, eval function. I have my
[01:19:22.260 --> 01:19:27.940]   instructions, and then I just made a little, like, helper that... Essentially, I just did a little,
[01:19:27.940 --> 01:19:36.100]   like, fancy evaluation loop where you can define an evaluator, and you can...
[01:19:36.100 --> 01:19:46.500]   Where's my code? Oh. Yeah. I'm sorry. The eval model takes in a run function that just returns a result.
[01:19:46.500 --> 01:19:51.300]   This is where I'm going to pass the model. It takes an evaluate function, and then a data set of workers.
[01:19:51.300 --> 01:19:57.620]   This is just like a generic version of the other function I implemented. Cool. And let's see how it
[01:19:57.620 --> 01:20:05.780]   was doing. So, you know, I have this prompt. Try it with all the models. And what am I getting right
[01:20:05.780 --> 01:20:15.380]   now? For a mini, it's 58%, 51% worse, 56, 57. So, this kind of goes to show there's no signal to learn
[01:20:15.380 --> 01:20:21.380]   right now. Right? It's kind of random. Or, like, there's very little signal there. So,
[01:20:23.300 --> 01:20:26.980]   let's think about this. How would you do this sort of prompt tuning?
[01:20:26.980 --> 01:20:35.860]   Maybe even without the embedding models. We can, like, do these... Oh, I think I started writing an
[01:20:35.860 --> 01:20:44.580]   algorithm somewhere. Where is it? Ha! There we go. Look at that. So, this was sort of the idea that I had for
[01:20:44.580 --> 01:20:54.260]   fine tuning -- for prompt tuning -- was, like, run a forward pass, then sample from that training
[01:20:54.260 --> 01:20:58.260]   subset -- or, sorry, split up into mini-batches. Run a forward pass.
[01:20:58.260 --> 01:21:10.180]   Sorry. Split into mini-batches, each of which has a training subset and a test subset. Run a forward pass on the
[01:21:10.180 --> 01:21:14.420]   training subset. And then reason over it to see how it could have done better.
[01:21:14.420 --> 01:21:21.140]   And then take that -- put it back in the prompt, and then use that -- run it forward again on the
[01:21:21.140 --> 01:21:26.740]   test subset. And then update the prompt. And keep going. Does this make sense? I don't know. I'm kind
[01:21:26.740 --> 01:21:31.940]   of, like, coming up with this on the spot. So, we can see how it goes. Before I jump into this, I want to
[01:21:31.940 --> 01:21:39.220]   make sure there's no more questions. If I, like, lose myself here. No? Okay. People want to see me?
[01:21:39.220 --> 01:21:51.780]   Okay. So, you know, what are we going to do? Let's do training loop.
[01:21:55.460 --> 01:22:06.740]   And -- should we just do this? Let's do one run, right? So, like, we have some, like, mini-batch.
[01:22:16.100 --> 01:22:28.820]   And then we have training and -- like, train test. Yeah. Now, if we have that,
[01:22:28.820 --> 01:22:37.060]   then I want to run a forward pass. So, for --
[01:22:40.660 --> 01:22:41.700]   Is it just evaluate?
[01:22:41.700 --> 01:22:52.500]   I'll do this in a lazy way right now. So, you know, sample in train. Let's do results.
[01:22:52.500 --> 01:23:02.820]   Okay. So --
[01:23:02.820 --> 01:23:15.060]   Sorry. The model was so fast. Cursor is amazing. Okay. So --
[01:23:15.060 --> 01:23:24.820]   Yeah. Let's pretend that's correct. Okay. So, we get train. We get test. We sample train.
[01:23:29.140 --> 01:23:31.140]   We should -- Yeah. We should also have some labels.
[01:23:31.140 --> 01:23:39.060]   Here. Should we print what one of these -- So, we have -- let's do this, like, more accurately. So,
[01:23:39.060 --> 01:23:49.220]   the samples have an action, and they have a text. So, for each sample, we want to run it on --
[01:23:58.180 --> 01:24:05.940]   Does the model take in the whole thing? Yeah. It takes in a sample. Okay. So, then this should --
[01:24:05.940 --> 01:24:17.460]   Let's just -- let's start here. You know, process mini-batch of -- like, I don't know. Let's do 20.
[01:24:24.660 --> 01:24:28.500]   Do I want to print this. Well, let's see if this runs correctly, if this works. Okay.
[01:24:28.500 --> 01:24:31.860]   Um, I'll risk one.
[01:24:31.860 --> 01:24:37.060]   This is one of the worst ideas I've had.
[01:24:37.060 --> 01:24:48.500]   I didn't know you could do top-level weights, by the way, in notebooks. It's really nice.
[01:24:49.700 --> 01:24:54.020]   Um, yeah, this should be in parallel. I'll just do 10 for now.
[01:24:54.020 --> 01:24:57.700]   Did it -- was it happy?
[01:24:57.700 --> 01:25:03.460]   Uh, okay. So, it's just the results. So, maybe I want to append,
[01:25:03.460 --> 01:25:07.700]   like, sample...
[01:25:17.380 --> 01:25:28.180]   And then model output.
[01:25:28.180 --> 01:25:42.180]   Okay. Wow. Very topical. And then -- what did it say? What was the output? Was it ignore?
[01:25:44.420 --> 01:25:45.300]   I don't know. That feels correct.
[01:25:45.300 --> 01:25:55.780]   Okay. So, now we have -- oh, and then what was the actual -- okay, wait. Let's print -- you know,
[01:25:55.780 --> 01:25:59.220]   let's dump it, right?
[01:26:11.220 --> 01:26:12.980]   Uh-huh. We have a blank inputs.
[01:26:12.980 --> 01:26:15.780]   My package has --
[01:26:15.780 --> 01:26:22.980]   Ignore us. Okay. Okay, no, this is good, right? So, we have -- let me just, like, quickly --
[01:26:22.980 --> 01:26:26.580]   There's a lot of blank ones. Hmm.
[01:26:28.900 --> 01:26:33.060]   Uh, it's -- I don't know what it's doing.
[01:26:33.060 --> 01:26:51.540]   It's -- I don't know what it's doing.
[01:26:51.540 --> 01:27:11.460]   Whoa, whoa, whoa. Why is it still running?
[01:27:17.620 --> 01:27:42.420]   Oh, this is so much more fun with people, like, debugging with me.
[01:27:42.420 --> 01:28:00.580]   So, how should we do this? Should we, like -- I guess we could just give the entire mini batch to a model.
[01:28:00.580 --> 01:28:07.220]   That's the easiest thing. And then just be, like, you know, what did -- like, think about what you got wrong and then give me some notes.
[01:28:09.700 --> 01:28:15.940]   So, we'll have results, and then we can make a -- you know, like -- where is it?
[01:28:15.940 --> 01:28:24.660]   You know, um, extract insights. And then we can do --
[01:28:32.180 --> 01:28:59.220]   Okay. Any prompt engineers want to shout out
[01:28:59.940 --> 01:29:05.220]   Some prompts. Let's see. So, we have model. We have the results. We'll say, like, you know,
[01:29:05.220 --> 01:29:16.740]   you are provided a set of results
[01:29:16.740 --> 01:29:22.820]   from a forward pass to a model.
[01:29:29.460 --> 01:29:35.300]   Given these results, task is to -- let's see.
[01:29:35.300 --> 01:29:48.580]   It doesn't need to know the format. It's critical. These results follow --
[01:29:58.180 --> 01:29:59.140]   You know, not that fast.
[01:29:59.140 --> 01:30:11.300]   You know, there's an app that's really good at making prompts for you.
[01:30:11.300 --> 01:30:16.020]   ChatGPT. Yeah. Well, okay. Here's the funny part.
[01:30:16.020 --> 01:30:29.540]   All right. So, there is a -- it was actually with a -- with a -- I helped create this prompt generator.
[01:30:29.540 --> 01:30:33.620]   I just don't want to go that far, because then it's more for me to read.
[01:30:34.980 --> 01:30:47.380]   Okay. Um, general -- to, you know, emails outside of this batch of results. Um, the
[01:30:48.340 --> 01:31:09.860]   duplicated. In general, the output should be at most. What's the last one? It's like...
[01:31:09.860 --> 01:31:38.620]   Oh, do not... I'll do this here. Make sure the sides do not overfit
[01:31:38.620 --> 01:31:53.320]   to the current batch results. Should it be a JSON array? I'll keep it as a string for
[01:31:53.320 --> 01:32:07.380]   now. Simple string. Okay. So now we have that. Let's see how it does on this.
[01:32:07.380 --> 01:32:23.880]   Man, this is like... Okay. Yadda, yadda, yadda. Code. What did I call it? Extract insights.
[01:32:23.880 --> 01:32:45.880]   Maybe I'll give it reasoning. I'll do like that. And then results. Huh?
[01:32:45.880 --> 01:32:47.880]   Okie dokie. How do we feel about this prompt? I don't know. I feel like it's going to be
[01:32:47.880 --> 01:32:54.880]   kind of just watching, thinking, thinking, thinking, thinking, thinking, thinking, thinking,
[01:32:54.880 --> 01:33:01.880]   this is going to work. I don't think this is going to work. But, like, it'll be an interesting
[01:33:01.880 --> 01:33:06.880]   experiment. Okay.
[01:33:06.880 --> 01:33:13.880]   Okay.
[01:33:13.880 --> 01:33:13.880]   I'd say this is too long.
[01:33:13.880 --> 01:33:14.880]   Process.
[01:33:14.880 --> 01:33:15.880]   process.
[01:33:15.880 --> 01:33:15.880]   Okay.
[01:33:15.880 --> 01:33:15.880]   Okay.
[01:33:15.880 --> 01:33:16.880]   Okay.
[01:33:16.880 --> 01:33:17.880]   So, I'm not going to be able to do it.
[01:33:17.880 --> 01:33:18.880]   I'm not going to be able to do it.
[01:33:18.880 --> 01:33:19.880]   I'm not going to be able to do it.
[01:33:19.880 --> 01:33:20.880]   You're not going to be able to do it.
[01:33:20.880 --> 01:33:21.880]   Okay.
[01:33:21.880 --> 01:33:22.880]   It's going to be able to do it.
[01:33:22.880 --> 01:33:23.880]   It's going to be able to do it.
[01:33:23.880 --> 01:33:24.880]   It's going to be able to do it.
[01:33:24.880 --> 01:33:25.880]   I'm not going to be able to do it.
[01:33:25.880 --> 01:33:26.880]   I'm not going to be able to do it.
[01:33:26.880 --> 01:33:27.880]   I'm not going to be able to do it.
[01:33:27.880 --> 01:33:28.880]   I'm not going to be able to do it.
[01:33:28.880 --> 01:33:29.880]   I'm not going to be able to do it.
[01:33:29.880 --> 01:33:30.880]   I'm not going to be able to do it.
[01:33:30.880 --> 01:33:31.880]   I'm not going to be able to do it.
[01:33:31.880 --> 01:33:32.880]   I'm not going to be able to do it.
[01:33:32.880 --> 01:33:33.880]   I'm not going to be able to do it.
[01:33:33.880 --> 01:33:34.880]   I'm not going to be able to do it.
[01:33:34.880 --> 01:33:35.880]   I'm not going to be able to do it.
[01:33:35.880 --> 01:33:36.880]   I'm not going to be able to do it.
[01:33:36.880 --> 01:33:37.880]   Okay.
[01:33:37.880 --> 01:33:38.880]   I'm not going to be able to do it.
[01:33:38.880 --> 01:33:39.880]   I'm not going to be able to do it.
[01:33:39.880 --> 01:33:40.880]   I'm not going to be able to do it.
[01:33:40.880 --> 01:33:41.880]   I'm not going to be able to do it.
[01:33:41.880 --> 01:33:42.880]   I'm not going to be able to do it.
[01:33:42.880 --> 01:33:43.880]   I'm not going to be able to do it.
[01:33:43.880 --> 01:33:44.880]   I'm not going to be able to do it.
[01:33:44.880 --> 01:33:45.880]   I'm not going to be able to do it.
[01:33:45.880 --> 01:33:46.880]   I'm not going to be able to do it.
[01:33:46.880 --> 01:33:47.880]   I'm not going to be able to do it.
[01:33:47.880 --> 01:33:48.880]   I'm not going to be able to do it.
[01:33:48.880 --> 01:33:49.880]   I'm not going to be able to do it.
[01:33:49.880 --> 01:33:50.880]   I'm not going to be able to do it.
[01:33:50.880 --> 01:33:51.880]   I'm not going to be able to do it.
[01:33:51.880 --> 01:33:52.880]   I'm not going to be able to do it.
[01:33:52.880 --> 01:33:53.880]   I'm not going to be able to do it.
[01:33:53.880 --> 01:33:54.880]   I'm not going to be able to do it.
[01:33:54.880 --> 01:34:05.880]   I'm not going to be able to do it.
[01:34:05.880 --> 01:34:08.880]   I'm not going to be able to do it.
[01:34:08.880 --> 01:34:10.880]   I'm not going to be able to do it.
[01:34:10.880 --> 01:34:11.880]   I'm not going to be able to do it.
[01:34:11.880 --> 01:34:12.880]   I'm not going to be able to do it.
[01:34:12.880 --> 01:34:13.880]   I'm not going to be able to do it.
[01:34:13.880 --> 01:34:14.880]   I'm not going to be able to do it.
[01:34:14.880 --> 01:34:15.880]   I'm not going to be able to do it.
[01:34:15.880 --> 01:34:16.880]   I'm not going to be able to do it.
[01:34:16.880 --> 01:34:17.880]   I'm not going to be able to do it.
[01:34:17.880 --> 01:34:18.880]   I'm not going to be able to do it.
[01:34:18.880 --> 01:34:19.880]   I'm not going to be able to do it.
[01:34:19.880 --> 01:34:20.880]   I'm not going to be able to do it.
[01:34:20.880 --> 01:34:21.880]   I'm not going to be able to do it.
[01:34:21.880 --> 01:34:22.880]   I'm not going to be able to do it.
[01:34:22.880 --> 01:34:23.880]   I'm not going to be able to do it.
[01:34:23.880 --> 01:34:24.880]   I'm not going to be able to do it.
[01:34:24.880 --> 01:34:25.880]   I'm not going to be able to do it.
[01:34:25.880 --> 01:34:26.880]   I'm not going to be able to do it.
[01:34:26.880 --> 01:34:27.880]   I'm not going to be able to do it.
[01:34:27.880 --> 01:34:28.880]   I'm not going to be able to do it.
[01:34:28.880 --> 01:34:29.880]   I'm not going to be able to do it.
[01:34:29.880 --> 01:34:30.880]   I'm not going to be able to do it.
[01:34:30.880 --> 01:34:31.880]   I'm not going to be able to do it.
[01:34:31.880 --> 01:34:32.880]   I'm not going to be able to do it.
[01:34:32.880 --> 01:34:33.880]   I'm not going to be able to do it.
[01:34:33.880 --> 01:34:34.880]   I'm not going to be able to do it.
[01:34:34.880 --> 01:34:35.880]   I'm not going to be able to do it.
[01:34:35.880 --> 01:34:36.880]   I'm not going to be able to do it.
[01:34:36.880 --> 01:34:37.880]   I'm not going to be able to do it.
[01:34:37.880 --> 01:34:38.880]   I'm not going to be able to do it.
[01:34:38.880 --> 01:34:39.880]   I'm not going to be able to do it.
[01:34:39.880 --> 01:34:40.880]   I'm not going to be able to do it.
[01:34:40.880 --> 01:34:41.880]   I'm not going to be able to do it.
[01:34:41.880 --> 01:34:42.880]   I'm not going to be able to do it.
[01:34:42.880 --> 01:34:43.880]   I'm not going to be able to do it.
[01:34:43.880 --> 01:34:44.880]   I'm not going to be able to do it.
[01:34:44.880 --> 01:34:45.880]   I'm not going to be able to do it.
[01:34:45.880 --> 01:34:46.880]   I'm not going to be able to do it.
[01:34:46.880 --> 01:34:47.880]   I'm not going to be able to do it.
[01:34:47.880 --> 01:34:48.880]   I'm not going to be able to do it.
[01:34:48.880 --> 01:34:49.880]   I'm not going to be able to do it.
[01:34:49.880 --> 01:34:50.880]   I'm not going to be able to do it.
[01:34:50.880 --> 01:34:51.880]   I'm not going to be able to do it.
[01:34:51.880 --> 01:34:53.880]   I'm not going to be able to do it.
[01:34:53.880 --> 01:34:54.880]   I'm not going to be able to do it.
[01:34:54.880 --> 01:34:55.880]   I'm not going to be able to do it.
[01:34:55.880 --> 01:34:56.880]   I'm not going to be able to do it.
[01:34:56.880 --> 01:34:57.880]   I'm not going to be able to do it.
[01:34:57.880 --> 01:34:58.880]   I'm not going to be able to do it.
[01:34:58.880 --> 01:34:59.880]   I'm not going to be able to do it.
[01:34:59.880 --> 01:35:00.880]   I'm not going to be able to do it.
[01:35:00.880 --> 01:35:01.880]   I'm not going to be able to do it.
[01:35:01.880 --> 01:35:02.880]   I'm not going to be able to do it.
[01:35:02.880 --> 01:35:03.880]   I'm not going to be able to do it.
[01:35:03.880 --> 01:35:04.880]   I'm not going to be able to do it.
[01:35:04.880 --> 01:35:05.880]   I'm not going to be able to do it.
[01:35:05.880 --> 01:35:06.880]   I'm not going to be able to do it.
[01:35:06.880 --> 01:35:07.880]   I'm not going to be able to do it.
[01:35:07.880 --> 01:35:08.880]   I'm not going to be able to do it.
[01:35:08.880 --> 01:35:09.880]   I'm not going to be able to do it.
[01:35:09.880 --> 01:35:10.880]   I'm not going to be able to do it.
[01:35:10.880 --> 01:35:11.880]   I'm not going to be able to do it.
[01:35:11.880 --> 01:35:12.880]   I'm not going to be able to do it.
[01:35:12.880 --> 01:35:13.880]   I'm not going to be able to do it.
[01:35:13.880 --> 01:35:14.880]   I'm not going to be able to do it.
[01:35:14.880 --> 01:35:15.880]   I'm not going to be able to do it.
[01:35:15.880 --> 01:35:16.880]   I'm not going to be able to do it.
[01:35:16.880 --> 01:35:17.880]   I'm not going to be able to do it.
[01:35:17.880 --> 01:35:18.880]   I'm not going to be able to do it.
[01:35:18.880 --> 01:35:19.880]   I'm not going to be able to do it.
[01:35:19.880 --> 01:35:20.880]   I'm not going to be able to do it.
[01:35:20.880 --> 01:35:22.880]   I'm not going to be able to do it.
[01:35:22.880 --> 01:35:23.880]   I'm not going to be able to do it.
[01:35:23.880 --> 01:35:24.880]   I'm not going to be able to do it.
[01:35:24.880 --> 01:35:25.880]   I'm not going to be able to do it.
[01:35:25.880 --> 01:35:26.880]   I'm not going to be able to do it.
[01:35:26.880 --> 01:35:27.880]   I'm not going to be able to do it.
[01:35:27.880 --> 01:35:28.880]   I'm not going to be able to do it.
[01:35:28.880 --> 01:35:29.880]   I'm not going to be able to do it.
[01:35:29.880 --> 01:35:30.880]   I'm not going to be able to do it.
[01:35:30.880 --> 01:35:31.880]   I'm not going to be able to do it.
[01:35:31.880 --> 01:35:32.880]   I'm not going to be able to do it.
[01:35:32.880 --> 01:35:33.880]   I'm not going to be able to do it.
[01:35:33.880 --> 01:35:34.880]   I'm not going to be able to do it.
[01:35:34.880 --> 01:35:35.880]   I'm not going to be able to do it.
[01:35:35.880 --> 01:35:36.880]   I'm not going to be able to do it.
[01:35:36.880 --> 01:35:37.880]   I'm not going to be able to do it.
[01:35:37.880 --> 01:35:38.880]   I'm not going to be able to do it.
[01:35:38.880 --> 01:35:39.880]   I'm not going to be able to do it.
[01:35:39.880 --> 01:35:40.880]   I'm not going to be able to do it.
[01:35:40.880 --> 01:35:41.880]   I'm not going to be able to do it.
[01:35:41.880 --> 01:35:42.880]   I'm not going to be able to do it.
[01:35:42.880 --> 01:35:43.880]   I'm not going to be able to do it.
[01:35:43.880 --> 01:35:44.880]   I'm not going to be able to do it.
[01:35:44.880 --> 01:35:45.880]   I'm not going to be able to do it.
[01:35:45.880 --> 01:35:46.880]   I'm not going to be able to do it.
[01:35:46.880 --> 01:35:47.880]   I'm not going to be able to do it.
[01:35:47.880 --> 01:35:48.880]   I'm not going to be able to do it.
[01:35:48.880 --> 01:35:49.880]   I'm not going to be able to do it.
[01:35:49.880 --> 01:35:50.880]   I'm not going to be able to do it.
[01:35:50.880 --> 01:35:51.880]   I'm not going to be able to do it.
[01:35:51.880 --> 01:35:52.880]   I'm not going to be able to do it.
[01:35:52.880 --> 01:35:53.880]   I'm not going to be able to do it.
[01:35:53.880 --> 01:35:54.880]   I'm not going to be able to do it.
[01:35:54.880 --> 01:35:55.880]   I'm not going to be able to do it.
[01:35:55.880 --> 01:35:56.880]   I'm not going to be able to do it.
[01:35:56.880 --> 01:35:57.880]   I'm not going to be able to do it.
[01:35:57.880 --> 01:35:58.880]   I'm not going to be able to do it.
[01:35:58.880 --> 01:35:59.880]   I'm not going to be able to do it.
[01:35:59.880 --> 01:36:00.880]   I'm not going to be able to do it.
[01:36:00.880 --> 01:36:01.880]   I'm not going to be able to do it.
[01:36:01.880 --> 01:36:02.880]   I'm not going to be able to do it.
[01:36:02.880 --> 01:36:03.880]   I'm not going to be able to do it.
[01:36:03.880 --> 01:36:04.880]   I'm not going to be able to do it.
[01:36:04.880 --> 01:36:05.880]   I'm not going to be able to do it.
[01:36:05.880 --> 01:36:06.880]   I'm not going to be able to do it.
[01:36:06.880 --> 01:36:07.880]   I'm not going to be able to do it.
[01:36:07.880 --> 01:36:08.880]   I'm not going to be able to do it.
[01:36:08.880 --> 01:36:09.880]   I'm not going to be able to do it.
[01:36:09.880 --> 01:36:10.880]   I'm not going to be able to do it.
[01:36:10.880 --> 01:36:11.880]   I'm not going to be able to do it.
[01:36:11.880 --> 01:36:12.880]   I'm not going to be able to do it.
[01:36:12.880 --> 01:36:13.880]   I'm not going to be able to do it.
[01:36:13.880 --> 01:36:14.880]   I'm not going to be able to do it.
[01:36:14.880 --> 01:36:15.880]   I'm not going to be able to do it.
[01:36:15.880 --> 01:36:16.880]   I'm not going to be able to do it.
[01:36:16.880 --> 01:36:17.880]   I'm not going to be able to do it.
[01:36:17.880 --> 01:36:18.880]   I'm not going to be able to do it.
[01:36:18.880 --> 01:36:19.880]   I'm not going to be able to do it.
[01:36:19.880 --> 01:36:20.880]   I'm not going to be able to do it.
[01:36:20.880 --> 01:36:21.880]   I'm not going to be able to do it.
[01:36:21.880 --> 01:36:22.880]   I'm not going to be able to do it.
[01:36:22.880 --> 01:36:23.880]   I'm not going to be able to do it.
[01:36:23.880 --> 01:36:24.880]   I'm not going to be able to do it.
[01:36:24.880 --> 01:36:25.880]   I'm not going to be able to do it.
[01:36:25.880 --> 01:36:27.880]   I'm not going to be able to do it.
[01:36:27.880 --> 01:36:28.880]   I'm not going to be able to do it.
[01:36:28.880 --> 01:36:29.880]   I'm not going to be able to do it.
[01:36:29.880 --> 01:36:30.880]   I'm not going to be able to do it.
[01:36:30.880 --> 01:36:31.880]   I'm not going to be able to do it.
[01:36:31.880 --> 01:36:32.880]   I'm not going to be able to do it.
[01:36:32.880 --> 01:36:33.880]   I'm not going to be able to do it.
[01:36:33.880 --> 01:36:34.880]   I'm not going to be able to do it.
[01:36:34.880 --> 01:36:35.880]   I'm not going to be able to do it.
[01:36:35.880 --> 01:36:36.880]   I'm not going to be able to do it.
[01:36:36.880 --> 01:36:37.880]   I'm not going to be able to do it.
[01:36:37.880 --> 01:36:38.880]   I'm not going to be able to do it.
[01:36:38.880 --> 01:36:39.880]   I'm not going to be able to do it.
[01:36:39.880 --> 01:36:40.880]   I'm not going to be able to do it.
[01:36:40.880 --> 01:36:41.880]   I'm not going to be able to do it.
[01:36:41.880 --> 01:36:42.880]   I'm not going to be able to do it.
[01:36:42.880 --> 01:36:43.880]   I'm not going to be able to do it.
[01:36:43.880 --> 01:36:44.880]   I'm not going to be able to do it.
[01:36:44.880 --> 01:36:45.880]   I'm not going to be able to do it.
[01:36:45.880 --> 01:36:46.880]   I'm not going to be able to do it.
[01:36:46.880 --> 01:36:47.880]   I'm not going to be able to do it.
[01:36:47.880 --> 01:36:48.880]   I'm not going to be able to do it.
[01:36:48.880 --> 01:36:49.880]   I'm not going to be able to do it.
[01:36:49.880 --> 01:36:50.880]   I'm not going to be able to do it.
[01:36:50.880 --> 01:36:51.880]   I'm not going to be able to do it.
[01:36:51.880 --> 01:36:52.880]   I'm not going to be able to do it.
[01:36:52.880 --> 01:36:53.880]   I'm not going to be able to do it.
[01:36:53.880 --> 01:36:54.880]   I'm not going to be able to do it.
[01:36:54.880 --> 01:36:55.880]   I'm not going to be able to do it.
[01:36:55.880 --> 01:36:56.880]   I'm not going to be able to do it.
[01:36:56.880 --> 01:36:57.880]   I'm not going to be able to do it.
[01:36:57.880 --> 01:36:58.880]   I'm not going to be able to do it.
[01:36:58.880 --> 01:37:00.880]   I'm not going to be able to do it.
[01:37:00.880 --> 01:37:01.880]   I'm not going to be able to do it.
[01:37:01.880 --> 01:37:02.880]   I'm not going to be able to do it.
[01:37:02.880 --> 01:37:03.880]   I'm not going to be able to do it.
[01:37:03.880 --> 01:37:04.880]   I'm not going to be able to do it.
[01:37:04.880 --> 01:37:05.880]   I'm not going to be able to do it.
[01:37:05.880 --> 01:37:06.880]   I'm not going to be able to do it.
[01:37:06.880 --> 01:37:07.880]   I'm not going to be able to do it.
[01:37:07.880 --> 01:37:08.880]   I'm not going to be able to do it.
[01:37:08.880 --> 01:37:09.880]   I'm not going to be able to do it.
[01:37:09.880 --> 01:37:10.880]   I'm not going to be able to do it.
[01:37:10.880 --> 01:37:11.880]   I'm not going to be able to do it.
[01:37:11.880 --> 01:37:12.880]   I'm not going to be able to do it.
[01:37:12.880 --> 01:37:13.880]   I'm not going to be able to do it.
[01:37:13.880 --> 01:37:14.880]   I'm not going to be able to do it.
[01:37:14.880 --> 01:37:15.880]   I'm not going to be able to do it.
[01:37:15.880 --> 01:37:16.880]   I'm not going to be able to do it.
[01:37:16.880 --> 01:37:18.880]   I'm not going to be able to do it.
[01:37:18.880 --> 01:37:19.880]   I'm not going to be able to do it.
[01:37:19.880 --> 01:37:20.880]   I'm not going to be able to do it.
[01:37:20.880 --> 01:37:21.880]   I'm not going to be able to do it.
[01:37:21.880 --> 01:37:22.880]   I'm not going to be able to do it.
[01:37:22.880 --> 01:37:24.880]   I'm not going to be able to do it.
[01:37:24.880 --> 01:37:25.880]   I'm not going to be able to do it.
[01:37:25.880 --> 01:37:26.880]   I'm not going to be able to do it.
[01:37:26.880 --> 01:37:27.880]   I'm not going to be able to do it.
[01:37:27.880 --> 01:37:28.880]   I'm not going to be able to do it.
[01:37:28.880 --> 01:37:29.880]   I'm not going to be able to do it.
[01:37:29.880 --> 01:37:30.880]   I'm not going to be able to do it.
[01:37:30.880 --> 01:37:31.880]   I'm not going to be able to do it.
[01:37:31.880 --> 01:37:32.880]   I'm not going to be able to do it.
[01:37:32.880 --> 01:37:33.880]   I'm not going to be able to do it.
[01:37:33.880 --> 01:37:34.880]   I'm not going to be able to do it.
[01:37:34.880 --> 01:37:35.880]   I'm not going to be able to do it.
[01:37:35.880 --> 01:37:36.880]   I'm not going to be able to do it.
[01:37:36.880 --> 01:37:37.880]   I'm not going to be able to do it.
[01:37:37.880 --> 01:37:38.880]   I'm not going to be able to do it.
[01:37:38.880 --> 01:37:39.880]   I'm not going to be able to do it.
[01:37:39.880 --> 01:37:40.880]   I'm not going to be able to do it.
[01:37:40.880 --> 01:37:41.880]   I'm not going to be able to do it.
[01:37:41.880 --> 01:37:42.880]   I'm not going to be able to do it.
[01:37:42.880 --> 01:37:43.880]   I'm not going to be able to do it.
[01:37:43.880 --> 01:37:44.880]   I'm not going to be able to do it.
[01:37:44.880 --> 01:37:45.880]   I'm not going to be able to do it.
[01:37:45.880 --> 01:37:46.880]   I'm not going to be able to do it.
[01:37:46.880 --> 01:37:47.880]   I'm not going to be able to do it.
[01:37:47.880 --> 01:37:48.880]   I'm not going to be able to do it.
[01:37:48.880 --> 01:37:49.880]   I'm not going to be able to do it.
[01:37:49.880 --> 01:37:50.880]   I'm not going to be able to do it.
[01:37:50.880 --> 01:37:51.880]   I'm not going to be able to do it.
[01:37:51.880 --> 01:37:52.880]   I'm not going to be able to do it.
[01:37:52.880 --> 01:37:53.880]   I'm not going to be able to do it.
[01:37:53.880 --> 01:37:54.880]   I'm not going to be able to do it.
[01:37:54.880 --> 01:37:55.880]   I'm not going to be able to do it.
[01:37:55.880 --> 01:37:56.880]   I'm not going to be able to do it.
[01:37:56.880 --> 01:37:57.880]   I'm not going to be able to do it.
[01:37:57.880 --> 01:37:58.880]   I'm not going to be able to do it.
[01:37:58.880 --> 01:37:59.880]   I'm not going to be able to do it.
[01:37:59.880 --> 01:38:00.880]   I'm not going to be able to do it.
[01:38:00.880 --> 01:38:01.880]   I'm not going to be able to do it.
[01:38:01.880 --> 01:38:02.880]   I'm not going to be able to do it.
[01:38:02.880 --> 01:38:03.880]   I'm not going to be able to do it.
[01:38:03.880 --> 01:38:04.880]   I'm not going to be able to do it.
[01:38:04.880 --> 01:38:05.880]   I'm not going to be able to do it.
[01:38:05.880 --> 01:38:06.880]   I'm not going to be able to do it.
[01:38:06.880 --> 01:38:07.880]   I'm not going to be able to do it.
[01:38:07.880 --> 01:38:08.880]   I'm not going to be able to do it.
[01:38:08.880 --> 01:38:09.880]   I'm not going to be able to do it.
[01:38:09.880 --> 01:38:10.880]   I'm not going to be able to do it.
[01:38:10.880 --> 01:38:11.880]   I'm not going to be able to do it.
[01:38:11.880 --> 01:38:13.880]   I'm not going to be able to do it.
[01:38:13.880 --> 01:38:14.880]   I'm not going to be able to do it.
[01:38:14.880 --> 01:38:15.880]   I'm not going to be able to do it.
[01:38:15.880 --> 01:38:16.880]   I'm not going to be able to do it.
[01:38:16.880 --> 01:38:17.880]   I'm not going to be able to do it.
[01:38:17.880 --> 01:38:18.880]   I'm not going to be able to do it.
[01:38:18.880 --> 01:38:19.880]   I'm not going to be able to do it.
[01:38:19.880 --> 01:38:20.880]   I'm not going to be able to do it.
[01:38:20.880 --> 01:38:21.880]   I'm not going to be able to do it.
[01:38:21.880 --> 01:38:22.880]   I'm not going to be able to do it.
[01:38:22.880 --> 01:38:23.880]   I'm not going to be able to do it.
[01:38:23.880 --> 01:38:24.880]   I'm not going to be able to do it.
[01:38:24.880 --> 01:38:25.880]   I'm not going to be able to do it.
[01:38:25.880 --> 01:38:26.880]   I'm not going to be able to do it.
[01:38:26.880 --> 01:38:27.880]   I'm not going to be able to do it.
[01:38:27.880 --> 01:38:28.880]   I'm not going to be able to do it.
[01:38:28.880 --> 01:38:30.880]   I'm not going to be able to do it.
[01:38:30.880 --> 01:38:31.880]   I'm not going to be able to do it.
[01:38:31.880 --> 01:38:32.880]   I'm not going to be able to do it.
[01:38:32.880 --> 01:38:33.880]   I'm not going to be able to do it.
[01:38:33.880 --> 01:38:34.880]   I'm not going to be able to do it.
[01:38:34.880 --> 01:38:35.880]   I'm not going to be able to do it.
[01:38:35.880 --> 01:38:36.880]   I'm not going to be able to do it.
[01:38:36.880 --> 01:38:37.880]   I'm not going to be able to do it.
[01:38:37.880 --> 01:38:38.880]   I'm not going to be able to do it.
[01:38:38.880 --> 01:38:52.880]   I don't know.
[01:38:52.880 --> 01:38:53.880]   What do people think?
[01:38:53.880 --> 01:38:57.880]   What's going to happen?
[01:38:57.880 --> 01:39:02.880]   Okay.
[01:39:02.880 --> 01:39:07.880]   No.
[01:39:07.880 --> 01:39:08.880]   Very good call.
[01:39:08.880 --> 01:39:09.880]   Very good call.
[01:39:09.880 --> 01:39:30.880]   We could just make the notes this.
[01:39:30.880 --> 01:39:36.880]   Maybe I should print out what the notes are each time.
[01:39:36.880 --> 01:40:01.880]   Okay.
[01:40:01.880 --> 01:40:16.880]   So, while this happens, I think the idea here would be -- oh.
[01:40:16.880 --> 01:40:18.880]   That's pretty surprising.
[01:40:18.880 --> 01:40:23.880]   We're already doing pretty good.
[01:40:23.880 --> 01:40:24.880]   So, yeah.
[01:40:24.880 --> 01:40:26.880]   I guess I have zero confidence this is going to work.
[01:40:26.880 --> 01:40:44.880]   But I think this style of approach can work, right, where you can take -- and you can do this in a tree-like structure as well -- where you can take maybe subsections, these mini-batches, and then let it reason over the failures, and then maybe produce these insights.
[01:40:44.880 --> 01:40:51.880]   then you have a choice as to how you want to aggregate those insights, then you have a choice as to how you want to aggregate those insights.
[01:40:51.880 --> 01:41:03.880]   Here, I'm just replacing the notes within each mini-batch, and then the plan was to append all the different notes together from all the different mini-batches and put it in the prompt and see if that can perform better.
[01:41:03.880 --> 01:41:04.880]   Why is it stopping?
[01:41:04.880 --> 01:41:04.880]   Thank you.
[01:41:04.880 --> 01:41:19.880]   I'll leave this up in case anyone sees any other mistakes.
[01:41:19.880 --> 01:41:21.880]   So this is good to have there.
[01:41:21.880 --> 01:41:22.880]   But, yeah.
[01:41:22.880 --> 01:41:32.880]   This is like the skeleton of something that could be very interesting, obviously with a better prompt, obviously with better pieces, but -- yeah.
[01:41:32.880 --> 01:41:33.880]   I don't know.
[01:41:33.880 --> 01:41:36.880]   Any questions, ideas, thoughts?
[01:41:36.880 --> 01:41:41.880]   If not, I might just, like, call it after we see what the results are.
[01:41:41.880 --> 01:41:46.880]   For this?
[01:41:46.880 --> 01:41:49.880]   I'm making this up in front of you.
[01:41:49.880 --> 01:41:50.880]   Anything, yeah.
[01:41:50.880 --> 01:41:52.880]   Any examples that work for anything?
[01:41:52.880 --> 01:41:53.880]   No, no, no.
[01:41:53.880 --> 01:41:56.880]   Any examples that work for you, you know?
[01:41:56.880 --> 01:42:03.880]   Can I show examples that have worked for this technique?
[01:42:03.880 --> 01:42:23.880]   Well, so you're asking -- are the numbers moving?
[01:42:23.880 --> 01:42:24.880]   Not significantly.
[01:42:24.880 --> 01:42:25.880]   Okay.
[01:42:25.880 --> 01:42:26.880]   I'm going to call it.
[01:42:26.880 --> 01:42:29.880]   This doesn't work right now, but it's good enough to, like, be an example.
[01:42:29.880 --> 01:42:35.880]   I think to answer your question about, you know, what are the good techniques for prompt engineering?
[01:42:35.880 --> 01:42:36.880]   Like, if I can share some.
[01:42:36.880 --> 01:42:38.880]   I think they've just been changing so much.
[01:42:38.880 --> 01:42:40.880]   There's many, many, many, like, prompt engineering guides.
[01:42:40.880 --> 01:42:47.880]   And, like, what you would want to do is, like -- I guess you're asking both about prompt engineering, but maybe also something like this.
[01:42:47.880 --> 01:42:49.880]   Prompt engineering, there's so many resources.
[01:42:49.880 --> 01:42:53.880]   I'd say the biggest one is just be clear and don't have any, like, contradictions.
[01:42:53.880 --> 01:42:57.880]   And then a few shared examples are really, really good.
[01:42:57.880 --> 01:43:00.880]   For something like this, I'm, like, making this up as we go.
[01:43:00.880 --> 01:43:05.880]   I think, like -- again, I think the shape of this is something that can make sense.
[01:43:05.880 --> 01:43:10.880]   I would have been very surprised if I would have gotten any, like, good results.
[01:43:10.880 --> 01:43:14.880]   Maybe I'll stay, like, a few minutes after trying to get this work.
[01:43:14.880 --> 01:43:17.880]   But, yeah, question back there?
[01:43:17.880 --> 01:43:20.880]   Can you explain what you're doing?
[01:43:20.880 --> 01:43:21.880]   Yes.
[01:43:21.880 --> 01:43:27.880]   So, the question is, can I explain what the hell I'm doing here?
[01:43:27.880 --> 01:43:38.880]   I think the idea here is to -- instead of tuning the model itself, tuning the prompt.
[01:43:38.880 --> 01:43:50.880]   And, specifically, tuning a subsection of the prompt that I'm calling notes that essentially, hopefully, contains, like, jots about the emails that it's seen.
[01:43:50.880 --> 01:43:56.880]   And anything that they might have found unintuitive about classifying that email.
[01:43:56.880 --> 01:43:58.880]   Maybe that's something I should include in the prompt.
[01:43:58.880 --> 01:44:12.880]   And so, the hope is that as it sees more emails, like, with the -- what it actually classified it as and what it was supposed to classify it as, by using these reasoning models,
[01:44:12.880 --> 01:44:26.880]   hopefully extract some, like, explicit insights of, like, oh, you know, like, maybe Elon, like, wants to, like -- if he sees, like, a package has been delivered -- this one is probably glance.
[01:44:26.880 --> 01:44:27.880]   This is what I mean.
[01:44:27.880 --> 01:44:28.880]   Like, my data was very noisy.
[01:44:28.880 --> 01:44:33.880]   But, like, he will want to, like, glance at, like -- oh, I said ignore.
[01:44:33.880 --> 01:44:39.880]   Like, I might want to glance at this instead of, like, archiving.
[01:44:39.880 --> 01:44:40.880]   Right?
[01:44:40.880 --> 01:44:42.880]   And so, like -- but the question is, like, but why?
[01:44:42.880 --> 01:44:43.880]   Right?
[01:44:43.880 --> 01:44:44.880]   I didn't say why.
[01:44:44.880 --> 01:44:46.880]   I'm just saying what the correct answer is.
[01:44:46.880 --> 01:44:49.880]   And this is why I thought RFT might be -- might be good.
[01:44:49.880 --> 01:44:51.880]   But, like, this is way too noisy and subjective.
[01:44:51.880 --> 01:44:55.880]   But that doesn't mean you can't use reasoning models to still extract interesting insights.
[01:44:55.880 --> 01:45:06.880]   So, the idea would be, like, give enough examples of both successes and failures that the model itself has done to a reasoning model.
[01:45:06.880 --> 01:45:14.880]   And have it come up with, like, notes or techniques that the model can use to hopefully improve the next time.
[01:45:14.880 --> 01:45:32.880]   Yeah, so the question is, like, can I use this to identify pieces of information that are missing?
[01:45:32.880 --> 01:45:34.880]   And I think, yes, absolutely.
[01:45:34.880 --> 01:45:43.880]   And I think that's, like, very important for -- even, like, if you do want to fine-tune, but even if you don't, like, this approach can, like, tell you a little bit about your data.
[01:45:43.880 --> 01:45:44.880]   As well.
[01:45:44.880 --> 01:45:45.880]   Yeah.
[01:45:45.880 --> 01:45:48.880]   So, like, just -- it's like having someone read through it.
[01:45:48.880 --> 01:45:50.880]   And, like, write these things.
[01:45:50.880 --> 01:45:51.880]   Yeah.
[01:45:51.880 --> 01:45:54.880]   If there's more questions, I'm happy to keep answering.
[01:45:54.880 --> 01:45:58.880]   Otherwise, I'm going to officially call this.
[01:45:58.880 --> 01:46:00.880]   You have now seen two failures today.
[01:46:00.880 --> 01:46:01.880]   But that's okay.
[01:46:01.880 --> 01:46:02.880]   We learned from those.
[01:46:02.880 --> 01:46:05.880]   So, thank you.
[01:46:05.880 --> 01:46:06.880]   .
[01:46:06.880 --> 01:46:06.880]   .
[01:46:06.880 --> 01:46:07.880]   .
[01:46:07.880 --> 01:46:08.880]   .
[01:46:08.880 --> 01:46:09.880]   .
[01:46:09.880 --> 01:46:09.880]   .
[01:46:09.880 --> 01:46:14.360]   We'll see you next time.

