
[00:00:00.000 --> 00:00:04.080]   So I'm going to start off here with just a general overview of some of the
[00:00:04.080 --> 00:00:06.400]   different things you can log with weights and biases. Lavanya made this
[00:00:06.400 --> 00:00:09.920]   great report. I can share a link to it afterwards that just goes over each one
[00:00:09.920 --> 00:00:14.120]   and what you can do. So with images, so you can log an image out of your model.
[00:00:14.120 --> 00:00:18.760]   It's very simple. One call to weights and biases. You log, you have a key to name it,
[00:00:18.760 --> 00:00:23.480]   and then you can log an image or as many as you want. It'll show up in our UI.
[00:00:23.480 --> 00:00:28.720]   You can do the same thing here with videos. Very similar in API. It shows up
[00:00:28.720 --> 00:00:34.000]   here as well. You could play and watch that video. Again, you can repeat that
[00:00:34.000 --> 00:00:37.600]   with audio. I don't know if this will come through the speakers for you guys,
[00:00:37.600 --> 00:00:43.000]   but there's a little sound. All very similar looking APIs and you get the
[00:00:43.000 --> 00:00:45.760]   same thing where when you log this it shows up automatically in the UI.
[00:00:45.760 --> 00:00:50.860]   Another one is the 3D objects. So this allows you to log any 3D object model.
[00:00:50.860 --> 00:00:53.200]   There's a couple different model files that are supported.
[00:00:53.200 --> 00:01:00.760]   OGB, OBJ, GLTF, GLB, a few others in our documentation as well. We have some
[00:01:00.760 --> 00:01:05.200]   specialized 3D point clouds which is similar to the 3D objects but allows you
[00:01:05.200 --> 00:01:10.440]   to log them more natively. So you can do this in NP arrays which is, you know, much
[00:01:10.440 --> 00:01:13.360]   more similar to tensors.
[00:01:13.360 --> 00:01:21.560]   Let's see, making sure your guys' questions are getting answered. Another one is HTML.
[00:01:21.560 --> 00:01:24.680]   This is really cool because if anyone's used HTML, you can put whatever you want
[00:01:24.680 --> 00:01:28.520]   here. You can put all JavaScript script, you can render things customly. We've
[00:01:28.520 --> 00:01:30.520]   asked people to do some really cool stuff with this,
[00:01:30.520 --> 00:01:34.040]   Lavanya included. So now I gave you a quick overview. We're gonna dive into
[00:01:34.040 --> 00:01:37.400]   some project people have done and some kind of more in-depth examples of these
[00:01:37.400 --> 00:01:42.480]   media types and how you can use them to help with your machine learning. I'm
[00:01:42.480 --> 00:01:45.840]   gonna start off with my personal favorite project here which is one of my
[00:01:45.840 --> 00:01:50.240]   co-workers, Gant. So he trained this on a small set of cat images that he scraped
[00:01:50.240 --> 00:01:54.640]   on the internet and used those to train a Gant. And so you can see this media
[00:01:54.640 --> 00:02:00.040]   panel here we have when you log images to weights and biases. You could render
[00:02:00.040 --> 00:02:03.080]   them in different ways. You can lay them out in ways that are really useful here.
[00:02:03.080 --> 00:02:08.440]   So for example, this one on our x-axis, we have many images. So in your training
[00:02:08.440 --> 00:02:12.880]   run, if you log a set of ten images, it'll tile them across and then for each
[00:02:12.880 --> 00:02:18.520]   row, you can see it tells you the step number. So as this goes down, we'll scroll
[00:02:18.520 --> 00:02:22.200]   through and we'll see our model training over time and each one of these is going
[00:02:22.200 --> 00:02:27.040]   to be at some point in the latent space being generated by our Gant. So we can
[00:02:27.040 --> 00:02:29.980]   see how many different points in this latent space evolve over time and see
[00:02:29.980 --> 00:02:35.800]   the quality of our model. As you can see, we start to get more and more kind of
[00:02:35.800 --> 00:02:39.000]   realistic images going here. We can see it struggles a little bit with certain
[00:02:39.000 --> 00:02:43.960]   spots. Maybe some places are a little darker. At some point, Gant starts to pick
[00:02:43.960 --> 00:02:49.840]   up a lot of white. Maybe has a bit of trouble with that but then restabilizes
[00:02:49.840 --> 00:02:53.760]   here and starts getting some textures. At the end, we get some really interesting
[00:02:53.760 --> 00:02:59.000]   results here and it actually starts to look a lot like cats. So one of the
[00:02:59.000 --> 00:03:02.680]   things I've really tried to do with this is normally when you're doing machine
[00:03:02.680 --> 00:03:05.400]   learning in the past, you'd have to set up whatever visualization you want
[00:03:05.400 --> 00:03:09.960]   beforehand and so that would involve deciding how many cats do I want to see,
[00:03:09.960 --> 00:03:14.920]   how do I want to tile it, do I want to stop at each step. And so now what you
[00:03:14.920 --> 00:03:17.840]   can do is you can just log the images the way it's at biases and then
[00:03:17.840 --> 00:03:21.320]   afterwards you can go and you can configure it and you can set things up
[00:03:21.320 --> 00:03:25.520]   how you want. So instead of having to kind of decide how to analyze your
[00:03:25.520 --> 00:03:29.520]   results before you run your experiment, you can run your experiment, log your
[00:03:29.520 --> 00:03:34.280]   results and then perform analysis on them. Here's another example. This panel
[00:03:34.280 --> 00:03:39.200]   here, I'll go ahead and full screen it for you so we can see it separate. This is just
[00:03:39.200 --> 00:03:43.280]   many cats all rendered at the very end of training. So instead of looking at a
[00:03:43.280 --> 00:03:47.240]   grid and kind of seeing things over time, we just want the best results here. We
[00:03:47.240 --> 00:03:52.000]   can also jump into our little step slider here, scroll back in time, go to
[00:03:52.000 --> 00:04:00.280]   the beginning of training, jump around and see how these things change. Next, we
[00:04:00.280 --> 00:04:04.520]   have another 2D grid version here. So this one is going to be similar to our
[00:04:04.520 --> 00:04:08.280]   first diagram but it's laid out now left to right. So starting on the left is
[00:04:08.280 --> 00:04:12.720]   going to be point zero in time, a zero time step and the end is going to be our
[00:04:12.720 --> 00:04:17.880]   final step and then going down in this column is another point in the latent
[00:04:17.880 --> 00:04:23.040]   space. You can see we have many runs here. This one's about 20 and there's a nice
[00:04:23.040 --> 00:04:29.080]   time scale on the bottom here and what this allows us to do is dive into and
[00:04:29.080 --> 00:04:32.560]   zoom into different parts of our training run. So I'm going to look at an
[00:04:32.560 --> 00:04:36.120]   example and see maybe there's somewhere I want to zoom into this model. Looks
[00:04:36.120 --> 00:04:39.600]   like something really interesting is happening in the beginning here where
[00:04:39.600 --> 00:04:42.760]   there's some sort of fragmenting. It looks like maybe the convolutional
[00:04:42.760 --> 00:04:47.120]   layers are starting to learn patterns and edges. You can see in the second
[00:04:47.120 --> 00:04:51.000]   frame it starts to get a little more complicated, less lines, more shapes. Right
[00:04:51.000 --> 00:04:54.920]   around here we get some really interesting shapes coming out. So let's
[00:04:54.920 --> 00:05:00.480]   go ahead and select that portion on the timeline and the panel now zooms in. So
[00:05:00.480 --> 00:05:05.000]   now the far left here is going to be our point about five on our training step
[00:05:05.000 --> 00:05:09.720]   and about a hundred and we get a much more condensed view of that. And I can see
[00:05:09.720 --> 00:05:13.680]   this is kind of right around here is where I was wanting to see what's going
[00:05:13.680 --> 00:05:20.440]   on. I go in even a little smaller here and really zoom in and seeing what's
[00:05:20.440 --> 00:05:24.040]   happening in those steps. And I can keep doing this and keep doing this and diving
[00:05:24.040 --> 00:05:27.040]   in. So you can see this can be really useful on a run where you have, you know, for say
[00:05:27.040 --> 00:05:30.560]   training a big GAN you have maybe a million steps and you want to pull into
[00:05:30.560 --> 00:05:37.600]   some of these. Down here this shows how you can use this same sort of image
[00:05:37.600 --> 00:05:43.120]   panel with the same media slider but now you can do it instead of for many points
[00:05:43.120 --> 00:05:46.200]   in one latent space, you can do this for many different training runs of your
[00:05:46.200 --> 00:05:51.560]   model. So you'll see here for each row that we have in our panel there's a
[00:05:51.560 --> 00:05:55.280]   label representing the name of the training run and you can see right away
[00:05:55.280 --> 00:05:59.400]   these runs look like they didn't converge at all, didn't really learn
[00:05:59.400 --> 00:06:05.760]   anything, perhaps mode collapse. This one here you can see really only learns
[00:06:05.760 --> 00:06:10.160]   textures although does a little better than these ones up here. And it's really
[00:06:10.160 --> 00:06:13.560]   cool that you can even compare these over time so you can see look at these
[00:06:13.560 --> 00:06:17.920]   two top ones. This one tends to learn to some sort of interesting cat really
[00:06:17.920 --> 00:06:22.400]   rather quickly whereas this one it takes a lot more time to get to what is not as
[00:06:22.400 --> 00:06:28.320]   good as as good of a cat. You can you can scroll through here and see all the
[00:06:28.320 --> 00:06:33.680]   different sets of your runs and get an idea of how they compare to each other.
[00:06:33.680 --> 00:06:39.160]   Also allows you to do the same zooming behavior.
[00:06:39.160 --> 00:06:48.360]   All right I'm gonna jump ahead to the next example here which is this one's
[00:06:48.360 --> 00:06:54.120]   done by Stacy who's gonna be talking a little bit later today. She's our first
[00:06:54.120 --> 00:06:58.160]   machine learning engineer and has made some really great reports. This one is
[00:06:58.160 --> 00:07:02.400]   adversarial policies in multi-agent settings. I'm not going to go into a lot
[00:07:02.400 --> 00:07:05.880]   of the research that was done here but I will kind of go through some examples
[00:07:05.880 --> 00:07:12.000]   and show how she's able to visualize her results with this. So this example here
[00:07:12.000 --> 00:07:14.800]   what she did was log videos from her training runs because she's doing
[00:07:14.800 --> 00:07:20.440]   reinforcement learning. And so you'll see there's two adversarial agents here and
[00:07:20.440 --> 00:07:23.720]   they're both competing against each other here. And the general idea of the
[00:07:23.720 --> 00:07:29.000]   paper that she was working through here is if you train an agent to act against
[00:07:29.000 --> 00:07:33.560]   another agent they kind of both have similar behaviors and they'll fight
[00:07:33.560 --> 00:07:36.680]   against each other. You can see this happening in the top left and cooperate.
[00:07:36.680 --> 00:07:41.760]   In the bottom right the red one is trained to do something strange
[00:07:41.760 --> 00:07:48.760]   essentially to fool the blue agent here. And so the blue one just kind of gets
[00:07:48.760 --> 00:07:51.720]   confused and walks past him and stumbles and the red one just stands still and
[00:07:51.720 --> 00:07:54.280]   ends up winning.
[00:07:54.280 --> 00:08:00.920]   See there's some more examples down here involving soccer balls. So videos is
[00:08:00.920 --> 00:08:04.020]   another type of thing you can log. You can do the same setup you can do with
[00:08:04.020 --> 00:08:07.080]   images here where you can compare multiple things in a single run or
[00:08:07.080 --> 00:08:13.320]   multiple things across a set of times. A recent one that I've released now is
[00:08:13.320 --> 00:08:18.920]   1b molecule. So this is allows you to visualize any sort of molecular
[00:08:18.920 --> 00:08:25.040]   structure. Right in Weights and Biases. So this can be used for things like drug
[00:08:25.040 --> 00:08:35.680]   discovery. You can see these are the side chains here rendered as sticks and the
[00:08:35.680 --> 00:08:42.760]   backbone is rendered as ribbons. We have another one here which is this is our
[00:08:42.760 --> 00:08:46.720]   point cloud example. We'll dive into some similar examples that you saw now but
[00:08:46.720 --> 00:08:55.640]   laid out in a grid. So this is semantic segmentation with 3d point clouds and
[00:08:55.640 --> 00:09:01.920]   you'll see we can compare this is many runs and we want to compare a set of two
[00:09:01.920 --> 00:09:05.960]   things. So the first one here is going to be our prediction and then the second
[00:09:05.960 --> 00:09:12.200]   one is our ground truth. So the each pair in two sets going from left to right and
[00:09:12.200 --> 00:09:17.680]   then down is a pair of these different things laid out. And again we can take
[00:09:17.680 --> 00:09:23.120]   our slider and we can step back in time and now we can see our model is going to
[00:09:23.120 --> 00:09:28.880]   do much worse earlier on the training run. As we compare this this this is a
[00:09:28.880 --> 00:09:31.280]   really simple example here in the top right where you can see how it's really
[00:09:31.280 --> 00:09:35.280]   struggling. It's a quite simple object to segment into two different parts but it
[00:09:35.280 --> 00:09:40.520]   really just at the very beginning of training only paints it one color.
[00:09:40.520 --> 00:09:47.280]   We go towards the end here we'll go back and we'll see it learns to segment this
[00:09:47.280 --> 00:09:53.320]   into two different discrete shapes. I've laid this one out as well and kind of
[00:09:53.320 --> 00:09:57.320]   similar setups. I won't talk through the details of each one it's very similar to
[00:09:57.320 --> 00:10:01.360]   how I did the images just many different ways to look at your training grounds
[00:10:01.360 --> 00:10:04.400]   and compare the results.
[00:10:04.560 --> 00:10:09.360]   This is a report I'm going to walk through that uses some of the 1d molecule
[00:10:09.360 --> 00:10:13.520]   stuff some of the 3d objects and some of the images it was made by Jonathan King
[00:10:13.520 --> 00:10:18.640]   he's a researcher in the Coase group based out of Philadelphia. So he's trying
[00:10:18.640 --> 00:10:25.400]   to learn the structure of proteins so the 3d structure of protein from just a
[00:10:25.400 --> 00:10:30.200]   molecular data which would be the amino acid sequences and the structure of
[00:10:30.200 --> 00:10:35.120]   just a molecular data which would be the amino acid sequences and it uses text
[00:10:35.120 --> 00:10:43.480]   processing and then translate that to a 3d structure. Here's some examples here
[00:10:43.480 --> 00:10:48.640]   where the red is going to be the ground truth and the blue is the learned model.
[00:10:48.640 --> 00:10:52.960]   So you can see it's for some examples it's closer for some examples it's
[00:10:52.960 --> 00:10:58.640]   farther away the idea is to get these two shapes matching. Here's some images
[00:10:58.640 --> 00:11:02.280]   that search a more complicated shapes
[00:11:02.280 --> 00:11:08.000]   and the differences between them
[00:11:08.000 --> 00:11:19.480]   and again at the bottom we have some similar ones as well. So you can see the
[00:11:19.480 --> 00:11:25.360]   red one here struggles to learn kind of the larger structure but often does very
[00:11:25.360 --> 00:11:28.160]   well with the spirals in the local structure so something he's working on
[00:11:28.160 --> 00:11:33.800]   right now is trying to solve that issue. So getting these things to coil up and
[00:11:33.800 --> 00:11:40.240]   sort of floor extended relationships that happen across the whole structure.
[00:11:40.240 --> 00:11:44.640]   You can see it's learning very well the smaller different loops that often match
[00:11:44.640 --> 00:11:52.440]   very similarly even though the whole thing doesn't fold. Here's something that
[00:11:52.440 --> 00:11:57.160]   we released rather recently as well this one I think highlights really well the
[00:11:57.160 --> 00:12:00.720]   idea of how you can log something with weights and biases and then do your
[00:12:00.720 --> 00:12:06.000]   experimentation afterwards or your sorry your analysis afterwards. So for this
[00:12:06.000 --> 00:12:10.440]   you're trying to segment an image and so you log an image and you log an image
[00:12:10.440 --> 00:12:14.760]   mask and then what you can do after you've logged these normally you have to
[00:12:14.760 --> 00:12:18.000]   log this as sort of a static thing where you lay your image your mask over an
[00:12:18.000 --> 00:12:23.000]   image and then you see result. Now what you can do with weights and biases is
[00:12:23.000 --> 00:12:28.120]   toggle off different parts of this you can see all of the classes you've logged
[00:12:28.120 --> 00:12:38.640]   you can change the opacity of an entire layer you can hide an entire layer goes
[00:12:38.640 --> 00:12:42.280]   up like that. I sort of customize my visualization after I'm done with
[00:12:42.280 --> 00:12:45.600]   training I can change the layout of these so this one I'm going to shift it
[00:12:45.600 --> 00:12:51.360]   so now the masks are shown to the side of my image. I can go ahead and put that
[00:12:51.360 --> 00:12:54.800]   image back in underneath there if I want there's all sorts of ways to tweak this
[00:12:54.800 --> 00:13:03.040]   around. And the final thing I'm going to do here is show you just how simple it
[00:13:03.040 --> 00:13:06.240]   is to get these different layouts so if you log your set of images we'll go back
[00:13:06.240 --> 00:13:12.080]   to the CAD can here you got your advanced settings panel over here and
[00:13:12.080 --> 00:13:16.520]   this allows you to choose how you want to tile things so I'm going to tile now
[00:13:16.520 --> 00:13:22.360]   per step so you can see this starts at step 0 goes all the way to step 346 I
[00:13:22.360 --> 00:13:27.160]   can change that to be an index this will do many cats at the end of the run and
[00:13:27.160 --> 00:13:31.240]   allow me to scrub across time
[00:13:31.240 --> 00:13:40.200]   or I can do it multiple comparing one example for each run and then there's
[00:13:40.200 --> 00:13:43.280]   this additional toggle here which allows us to get the really advanced layouts
[00:13:43.280 --> 00:13:47.800]   which is grid mode and so this allows you to do two different axes so there's
[00:13:47.800 --> 00:13:52.760]   the x-axis and the y-axis in this example our x-axis is the log step and
[00:13:52.760 --> 00:13:55.520]   if you have the log step that's what gives you this really nice slider here
[00:13:55.520 --> 00:14:01.280]   so you can zoom into different parts in time then our x-axis is run do the same
[00:14:01.280 --> 00:14:05.040]   thing here where I can change this to a different setting now I've got that
[00:14:05.040 --> 00:14:10.560]   other layout we were seeing showing many examples over time
[00:14:10.560 --> 00:14:15.120]   all right so this is the media panel which is really what I've been working
[00:14:15.120 --> 00:14:18.720]   on and trying to make it easier to see peer into what you're doing with your
[00:14:18.720 --> 00:14:22.280]   model and figure out what's going on
[00:14:22.280 --> 00:14:29.440]   that was amazing Nick so many cool features packed into 10 minutes like 50
[00:14:29.440 --> 00:14:37.800]   crushed it so psych had a question what segmentation framework was used to create
[00:14:37.800 --> 00:14:45.000]   those segmentation maps that's that's all the rates and biases so if you log
[00:14:45.000 --> 00:14:49.120]   the results of your model if you log the images that go in as the input to us and
[00:14:49.120 --> 00:14:55.160]   then if you log data as a nut just log it as a non-pure a so right when it
[00:14:55.160 --> 00:14:58.320]   comes at your model it's probably a pie torch tensor or a tensor potential you
[00:14:58.320 --> 00:15:03.080]   can convert it to a non-pure a and then pass in those values so each it will be
[00:15:03.080 --> 00:15:09.480]   a 2d array where each point is a class label index or class label integers so
[00:15:09.480 --> 00:15:18.360]   one per car to per tree etc etc I think the question was maybe about which like
[00:15:18.360 --> 00:15:23.800]   what was being what model was being used to do the segmentation okay that was
[00:15:23.800 --> 00:15:32.320]   yellow I think you'll be three and then cycle test is then notebook that you
[00:15:32.320 --> 00:15:37.760]   could use to generate these we'll share the docs and the report in the chat in
[00:15:37.760 --> 00:15:46.880]   just a second yeah do people have other questions
[00:15:46.880 --> 00:16:00.000]   let me check you - nope people don't have other questions nice thank you Nick I'm
[00:16:00.000 --> 00:16:03.800]   gonna be using this talk as a demo and extending it out to everyone I know
[00:16:03.800 --> 00:16:07.320]   because I could get so many questions about how to do specific things with the
[00:16:07.320 --> 00:16:13.600]   media panel so I'm really excited and right happy to share

