
[00:00:00.000 --> 00:00:05.620]   Hey everybody. So like Lavanya said, I'm a deep learning instructor here at Weights
[00:00:05.620 --> 00:00:11.040]   and Biases. And I'm also a graduate student at the University of California, Berkeley,
[00:00:11.040 --> 00:00:14.440]   studying the optimization of neural networks. I thought I'd take some time to talk about
[00:00:14.440 --> 00:00:21.080]   a subject that is near and dear to my heart, and not many other people's, which is linear
[00:00:21.080 --> 00:00:27.240]   algebra. So linear algebra is something that's super critical for machine learning, but something
[00:00:27.240 --> 00:00:33.360]   that a lot of people kind of dislike or try to-- it's sort of like vegetables. Nobody
[00:00:33.360 --> 00:00:41.080]   wants to eat their vegetables. They want the tasty steak. And part of the reason for that
[00:00:41.080 --> 00:00:44.560]   is that I think linear algebra is kind of taught the wrong way. It's taught as though
[00:00:44.560 --> 00:00:50.040]   it were like the algebra you learn before, where you learn things about addition and
[00:00:50.040 --> 00:00:55.000]   multiplication. You learn rules for combining those two things together to generate new
[00:00:55.000 --> 00:01:00.240]   symbols, new equations to solve for things. And when you take a linear algebra class,
[00:01:00.240 --> 00:01:04.640]   sometimes you spend the entire time on computations like that. How do I get the determinant? How
[00:01:04.640 --> 00:01:09.200]   do I get the eigenvalues? Things like that. And folks end up getting confused, because
[00:01:09.200 --> 00:01:14.200]   the rules for linear algebra are very, very complicated for manipulating these symbols
[00:01:14.200 --> 00:01:23.280]   around. And they don't behave like algebra that we learn when we're younger. But this
[00:01:23.280 --> 00:01:27.800]   is not the way that linear algebra has to be taught. And it's not the way linear algebra
[00:01:27.800 --> 00:01:34.880]   actually gets used in machine learning. And so one big example of this is that linear
[00:01:34.880 --> 00:01:41.720]   algebra frequently is used not as like solving systems of equations the way you would in
[00:01:41.720 --> 00:01:49.480]   a typical linear algebra class, but as sort of for geometric transformation. So in the
[00:01:49.480 --> 00:01:54.880]   bottom right, you can see an animation of some linear algebraic manipulations considered
[00:01:54.880 --> 00:02:00.560]   as geometric transformations. This is from this YouTube series by YouTuber 3Blue1Brown,
[00:02:00.560 --> 00:02:08.840]   aka Grant Sanderson, who is a really great mathematical educator on the internet. So
[00:02:08.840 --> 00:02:14.400]   this YouTube series sort of presents linear algebra as the study of transformations of
[00:02:14.400 --> 00:02:20.960]   spaces. And so it's a good one to look at if you took one of those classic linear algebra
[00:02:20.960 --> 00:02:28.080]   classes and find yourself doing deep learning, but find that it's not taught you the linear
[00:02:28.080 --> 00:02:32.320]   algebra that you want. There's another one that I think a lot of people have maybe heard
[00:02:32.320 --> 00:02:38.200]   of or seen that YouTube series before. But there's another one a little more avant garde,
[00:02:38.200 --> 00:02:47.000]   graphical linear algebra. So this is a blog post series by this mathematician, Pavel Subchinsky.
[00:02:47.000 --> 00:02:52.120]   And the idea there, the tagline is that linear algebra is what happens when adding meets
[00:02:52.120 --> 00:02:58.120]   copying. And this one, sort of a little bit different from the way things end up getting
[00:02:58.120 --> 00:03:04.160]   used in deep learning, but it has this really neat style of diagrammatic reasoning. Everything
[00:03:04.160 --> 00:03:15.920]   is done instead of using equations, but with pictures. And so this is done in a way that's
[00:03:15.920 --> 00:03:20.600]   entirely and completely rigorous, which is really impressive for pictures as opposed
[00:03:20.600 --> 00:03:25.460]   to equations. And so I found that it's something that can really sort of take your mathematical
[00:03:25.460 --> 00:03:30.440]   understanding of mathematics in general, but linear algebra in particular, up to another
[00:03:30.440 --> 00:03:36.120]   level. But I'm not going to present sort of either of those ways of thinking about linear
[00:03:36.120 --> 00:03:39.520]   algebra today. I'm going to talk about how linear algebra can be seen as something that's
[00:03:39.520 --> 00:03:44.400]   more like programming than it is like algebra. And in this view, we're going to see that
[00:03:44.400 --> 00:03:48.720]   matrices are our functions, shapes are our types, and multiplication is composition.
[00:03:48.720 --> 00:03:52.800]   I'm not going to have time to go through the entirety of this in just the 20 minutes of
[00:03:52.800 --> 00:03:56.560]   this, but I'm hoping to give you a taste for how you can take some intuitions that you
[00:03:56.560 --> 00:04:01.640]   built from working with computer programs that don't look like linear algebra to understand
[00:04:01.640 --> 00:04:08.480]   these linear algebra components of machine learning and deep learning much more deeply.
[00:04:08.480 --> 00:04:13.880]   So in programming, we combine functions that have matching types through function composition,
[00:04:13.880 --> 00:04:19.280]   and we use this to create programs. In linear algebra, we combine matrices with matching
[00:04:19.280 --> 00:04:25.680]   shapes through matrix multiplication, and we use this to create more matrices. So this
[00:04:25.680 --> 00:04:29.600]   is the style of programming that I'm thinking about here isn't the sort of object oriented
[00:04:29.600 --> 00:04:36.280]   programming. I'm thinking about strictly typed programming, functional programming, in particular,
[00:04:36.280 --> 00:04:41.280]   for folks who are familiar with the different types of programming languages that are out
[00:04:41.280 --> 00:04:45.120]   there. But just in general, there's connections with just the way that we think about our
[00:04:45.120 --> 00:04:52.440]   computer programs, no matter what paradigm we're working in. So in all kinds of programming,
[00:04:52.440 --> 00:05:00.160]   we define one function, one program in terms of others. So if I wanted to check whether
[00:05:00.160 --> 00:05:06.640]   the string that I'm composing is too long to tweet, I might first get the length of
[00:05:06.640 --> 00:05:10.600]   that string in the number of characters, then check if that number of characters is over
[00:05:10.600 --> 00:05:15.760]   140. So that's what the Python function on the top there does. And it's calling this
[00:05:15.760 --> 00:05:26.360]   function over 140 that checks whether its input, which is a number, is above 140. So
[00:05:26.360 --> 00:05:32.720]   we want to take this piece of logic about too long to tweet and break it down into its
[00:05:32.720 --> 00:05:42.080]   constituent components. That first function, computing the length, I want to take us back
[00:05:42.080 --> 00:05:50.040]   a little bit to the way functions are initially introduced when you first learn about functions,
[00:05:50.040 --> 00:05:55.520]   at least in the American school system. So we draw all the possible inputs the function
[00:05:55.520 --> 00:06:02.520]   could get on the left and all of the possible outputs that the function could create on
[00:06:02.520 --> 00:06:07.960]   the right. So the len function takes in a string and outputs how many characters are
[00:06:07.960 --> 00:06:14.480]   in the string. So the string A has one letter in it, the string BB has two letters in it,
[00:06:14.480 --> 00:06:24.120]   and the declaration of independence has 8,007 characters in it. So these arrows map what
[00:06:24.120 --> 00:06:30.040]   input goes to which output when this function is applied to them. I can do the same with
[00:06:30.040 --> 00:06:35.280]   that over 140 function. Now that over 140 function takes in the integer, takes in an
[00:06:35.280 --> 00:06:41.120]   integer, could be the integer output by the len function, and returns a Boolean. So it
[00:06:41.120 --> 00:06:46.280]   checks whether the number is over 140 and so too large to tweet. So all those strings
[00:06:46.280 --> 00:06:54.440]   AB, BB, A, those are all under 140 characters, so over 140 outputs false. And so they're
[00:06:54.440 --> 00:07:01.760]   tweetable, but the declaration of independence is not. It's over 140 characters, and so it
[00:07:01.760 --> 00:07:07.520]   is too long to tweet. And so in order to build that too long to tweet function, I just follow
[00:07:07.520 --> 00:07:12.040]   those arrows one after another. So to get the length of the declaration of independence,
[00:07:12.040 --> 00:07:19.120]   I start there, and I follow first this red arrow, then this blue arrow. And by following
[00:07:19.120 --> 00:07:25.160]   those two things, I've essentially created an arrow that goes from this thing all the
[00:07:25.160 --> 00:07:30.800]   way on the left, the strings, all the way to Booleans on the right. And that is our
[00:07:30.800 --> 00:07:36.760]   too long to tweet function. So we've defined this too long to tweet function as the composition
[00:07:36.760 --> 00:07:44.000]   of two simpler functions. And so too long to tweet goes from strings to Booleans, where
[00:07:44.000 --> 00:07:51.480]   it's two pieces. Len goes from strings to ints, and over 140 goes from ints to bools.
[00:07:51.480 --> 00:07:57.160]   So I'm going over this in detail, in part because I want to make sure that we can, like,
[00:07:57.160 --> 00:08:01.000]   carry our intuitions over from the world of programming over into the world of linear
[00:08:01.000 --> 00:08:05.880]   algebra. So as a first step, I just want to introduce the notation for how this would
[00:08:05.880 --> 00:08:14.720]   be written in mathematical terms. So on the right here, I have a sort of more programming
[00:08:14.720 --> 00:08:20.840]   way of writing it. So len is a function that takes strings and returns ints, and that would
[00:08:20.840 --> 00:08:25.600]   be written in mathematical notation. We'd name our function, and then a colon, the type
[00:08:25.600 --> 00:08:33.600]   that comes in, so type A is generic here, to type B, which would be, you know, ints.
[00:08:33.600 --> 00:08:41.920]   And then we can do the same with another function. It takes the same input as F outputs and returns
[00:08:41.920 --> 00:08:48.240]   this -- returns a different type, Booleans. If we wanted to denote the combination of
[00:08:48.240 --> 00:08:52.280]   these two functions, their composition, we write something that looks like this, G circle
[00:08:52.280 --> 00:08:59.520]   F. So my preferred way of pronouncing this is to say that this is after. So I apply G
[00:08:59.520 --> 00:09:06.960]   after F, which helps me remember what this symbol means that the function F comes first.
[00:09:06.960 --> 00:09:13.520]   And that's actually pretty important, because when we combine functions, the order is super,
[00:09:13.520 --> 00:09:19.920]   super important. So Mani is already -- his ears -- or their ears have pricked up given
[00:09:19.920 --> 00:09:24.960]   all this talk of composition. Indeed, this is going to be -- turn out to be a category
[00:09:24.960 --> 00:09:31.400]   theory approach to linear algebra. So when we combine our functions, the order that we
[00:09:31.400 --> 00:09:38.000]   do this is super, super important. So if I do that -- if I check whether a number is
[00:09:38.000 --> 00:09:44.600]   bigger than 140, after first checking the length of the string, that's very different
[00:09:44.600 --> 00:09:53.280]   from trying to check whether the -- taking a number, determining whether it's over 140,
[00:09:53.280 --> 00:10:02.520]   and then asking for its length. What does it mean to take the length of a number? In
[00:10:02.520 --> 00:10:08.240]   Python, that won't work. And so that's represented by the fact that this G guy here goes from
[00:10:08.240 --> 00:10:19.240]   B to C, and F goes from A to B. So if I give F a C, then it doesn't know what to do. So
[00:10:19.240 --> 00:10:26.000]   this is all relatively -- it's like a core part of programming that we make sure that
[00:10:26.000 --> 00:10:31.320]   the functions that we're applying know what to do with the stuff that comes into them.
[00:10:31.320 --> 00:10:36.000]   And so I want to sort of port that over to linear algebra by thinking about linear algebra
[00:10:36.000 --> 00:10:44.680]   as a combination of functions, and not just as matrix multiplication. So in linear algebra,
[00:10:44.680 --> 00:10:49.360]   we often -- we have these linear algebraic equations that say that this matrix here on
[00:10:49.360 --> 00:10:55.080]   the left of the equal sign is equal to this matrix here on -- this combination of matrices
[00:10:55.080 --> 00:11:01.040]   on the right-hand side. So just as a very simple example, say the matrix Z is this two-row,
[00:11:01.040 --> 00:11:07.600]   three-column matrix here, and the matrices Y and X are these two columns here. So one
[00:11:07.600 --> 00:11:12.280]   way to interpret what this means is that if you follow the matrix multiplication rules
[00:11:12.280 --> 00:11:20.340]   and combine Y with X, you'll get the matrix Z. And that's fine. But I think a better way
[00:11:20.340 --> 00:11:26.080]   of thinking about it is that applying the matrix X to a vector, then applying the matrix
[00:11:26.080 --> 00:11:33.520]   Y to a vector, is the same as applying matrix Z to the vector. So that's the same thing
[00:11:33.520 --> 00:11:41.560]   as saying that that symbol F after G, or G after F, was the same as first applying F,
[00:11:41.560 --> 00:11:48.680]   then applying G. So what does that look like if we try and cast it -- if we try and dig
[00:11:48.680 --> 00:11:55.440]   in a little bit more detail? So that function X -- that matrix X there can be thought of
[00:11:55.440 --> 00:12:00.000]   as a function. So we can write it in the same way as we wrote those other functions, the
[00:12:00.000 --> 00:12:06.200]   len and the over 140 function. We put its inputs, all of its possible inputs on the
[00:12:06.200 --> 00:12:13.040]   left. Its every array with three entries in it is on the left. And it outputs arrays with
[00:12:13.040 --> 00:12:19.200]   two entries in it. So these two -- for this one, it just picks out the first two entries
[00:12:19.200 --> 00:12:26.080]   and throws out the third. So these two arrays get mapped to the same output, 0, 1. And these
[00:12:26.080 --> 00:12:34.240]   other arrays get mapped to 1, 1, 3 gets turned into 1, 1. So we would denote that following
[00:12:34.240 --> 00:12:42.520]   that -- following the sort of programming-style syntax as saying X is a function that goes
[00:12:42.520 --> 00:12:49.400]   from arrays with three input -- three values to arrays with two values. And we write that
[00:12:49.400 --> 00:12:52.520]   a little bit more differently when we got to linear algebra, but the principles are
[00:12:52.520 --> 00:13:00.240]   all exactly the same. So if we were to write it in our mathematical notation, X is a function
[00:13:00.240 --> 00:13:07.680]   that goes from -- now, arrays with three elements are called real vectors in linear algebra.
[00:13:07.680 --> 00:13:11.920]   So these would be -- essentially this is a floating point array with three entries in
[00:13:11.920 --> 00:13:17.480]   it and returns a floating point array with two entries or a two-dimensional real vector.
[00:13:17.480 --> 00:13:27.600]   Or people call it an element of R2. And this is the same thing as when -- for the class
[00:13:27.600 --> 00:13:33.200]   of linear transformations, to say that X goes from R3 to R2 is to say that X can be represented
[00:13:33.200 --> 00:13:42.760]   as some matrix. So as a matrix with three columns, so 1, 2, 3 columns, and two rows,
[00:13:42.760 --> 00:13:48.680]   two rows here. And so this function takes something that lives in like a three-dimensional
[00:13:48.680 --> 00:13:53.640]   point. So these are like our X, Y, and Z axes. So it lives in this three-dimensional world,
[00:13:53.640 --> 00:13:59.680]   the inputs to X, and the outputs of X live in this two-dimensional world. And because
[00:13:59.680 --> 00:14:05.320]   the outputs of X live in this two-dimensional world, we can combine the matrix X with the
[00:14:05.320 --> 00:14:12.620]   matrix Y to get a single map because the inputs to Y also live in this two-dimensional world.
[00:14:12.620 --> 00:14:19.640]   So Y has two rows and two columns. So because it has two columns, its inputs live in a two-dimensional
[00:14:19.640 --> 00:14:29.000]   world. And so we can combine it with the -- with the matrix X. And now, just like we could
[00:14:29.000 --> 00:14:35.600]   combine over 140 and len to create a single function, too long to tweet, we can combine
[00:14:35.600 --> 00:14:41.720]   these two matrices together, create a single matrix that goes from R3 to R2. So a single
[00:14:41.720 --> 00:14:46.180]   matrix that goes from three dimensions to two dimensions that does the same thing as
[00:14:46.180 --> 00:14:55.520]   first applying the X matrix and then applying the Y matrix. And the utility of this is that
[00:14:55.520 --> 00:15:00.400]   if we focus too much on just thinking of linear algebra as algebra, we can lose sight of the
[00:15:00.400 --> 00:15:04.720]   meaning of what we're doing. So when we're doing linear algebra, it almost -- it always
[00:15:04.720 --> 00:15:10.080]   is the case that what we mean is that we've got these matrices that represent our functions,
[00:15:10.080 --> 00:15:14.200]   and we're trying to say, oh, this function is the same thing as this other implementation
[00:15:14.200 --> 00:15:24.800]   of this function, just as like three different steps. So Z is the same thing as Y after X.
[00:15:24.800 --> 00:15:30.200]   If we think of it as algebra, we would think of it as, oh, Z is equal to Y times X. And
[00:15:30.200 --> 00:15:36.240]   then we'd suddenly be surprised to find out that YX is not the same as XY, even though
[00:15:36.240 --> 00:15:40.280]   with regular numbers, that's the case. Two times three is the same as three times two,
[00:15:40.280 --> 00:15:45.280]   and that's true for any pair. But that's generally not true with matrices. And people find this
[00:15:45.280 --> 00:15:49.520]   really surprising. I found it surprising when I first found -- learned linear algebra. I
[00:15:49.520 --> 00:15:55.200]   found this whole -- I found linear algebra really confusing, unintuitive, and uninteresting.
[00:15:55.200 --> 00:15:59.960]   And it wasn't until I, you know, needed it to do deep learning and I came across this
[00:15:59.960 --> 00:16:06.520]   other way of thinking about it that I both understood why it was important and understood
[00:16:06.520 --> 00:16:13.400]   why this shouldn't be so surprising. Matrix multiplication is less like multiplying things,
[00:16:13.400 --> 00:16:18.040]   multiplying two numbers, and more like composing functions. And when we compose functions,
[00:16:18.040 --> 00:16:22.480]   the order is super important. Putting on my socks, then putting on my shoes is very, very
[00:16:22.480 --> 00:16:28.880]   different from putting on my shoes, then putting on my socks. And so that's something that
[00:16:28.880 --> 00:16:33.480]   just sort of gets missed out on when we think of linear algebra too algebraically, and not
[00:16:33.480 --> 00:16:41.000]   enough in this sort of like functional way, the way we think about our computer programs.
[00:16:41.000 --> 00:16:45.680]   So we're -- if we think of linear algebra as being about combining matrices with matching
[00:16:45.680 --> 00:16:50.400]   shapes through matrix multiplication, as being a sort of like special case of what we do
[00:16:50.400 --> 00:16:55.200]   in programming, where you combine functions with matching types through function composition,
[00:16:55.200 --> 00:16:58.800]   then I think we get a much deeper sort of understanding of what we're doing with our
[00:16:58.800 --> 00:17:04.640]   linear algebra. And I find this is actually, you know, this isn't just a sort of mathematical
[00:17:04.640 --> 00:17:09.840]   curiosity, though it is, you know, like beautiful and fun to think about. It's something that
[00:17:09.840 --> 00:17:13.600]   I remember and that you can remember when you're debugging or reading code that has
[00:17:13.600 --> 00:17:18.000]   a bunch of linear algebra in it. Follow the shapes the way you would follow the types
[00:17:18.000 --> 00:17:22.040]   in some other program. If you don't know what's going into the program and what's coming out
[00:17:22.040 --> 00:17:27.160]   of it, if you don't know how inside the program it's turning one shape or one type into another
[00:17:27.160 --> 00:17:33.440]   type, then you -- it's much harder to understand the program. I'm always, when I'm debugging,
[00:17:33.440 --> 00:17:37.880]   tossing in print statements that print out the types of things. Is this a list? Is this
[00:17:37.880 --> 00:17:41.600]   a list of lists? What's going on here? And you can do the same thing with shapes when
[00:17:41.600 --> 00:17:45.360]   you're debugging your machine learning code. And something that can check shapes is as
[00:17:45.360 --> 00:17:50.840]   good a friend as something that can check types. So there's lots of machine learning
[00:17:50.840 --> 00:17:53.960]   frameworks out there that can check to make sure you're getting your shapes right. And
[00:17:53.960 --> 00:17:58.400]   it's a useful thing to have around as something that can do your type checking for you when
[00:17:58.400 --> 00:18:04.680]   you're doing computer -- when you're programming with types. It's also useful for insight into
[00:18:04.680 --> 00:18:09.240]   a bunch of decompositions. So principal components analysis, QR decomposition, the singular value
[00:18:09.240 --> 00:18:14.360]   decomposition, these are -- I found them super mysterious when I first started thinking about
[00:18:14.360 --> 00:18:18.040]   them with linear algebra because I wrote them down as these big collections of numbers,
[00:18:18.040 --> 00:18:22.400]   these big equations, and tried to follow linear algebra rules to see what was happening. They're
[00:18:22.400 --> 00:18:26.800]   really something that's easy to understand once you think of them as ways to turn one
[00:18:26.800 --> 00:18:32.400]   matrix into multiple matrices, the same way we take code and we refactor it from one big
[00:18:32.400 --> 00:18:45.600]   pile of expressions into multiple subcomponent functions. And then lastly, this can be -- this
[00:18:45.600 --> 00:18:50.800]   gives some insight into deep learning. If I change the entries in a matrix a tiny bit,
[00:18:50.800 --> 00:18:55.160]   just a little bit, like I add a small perturbation to one of the entries, I change the function
[00:18:55.160 --> 00:19:00.920]   it computes a little bit. It does almost the same thing but does something slightly different.
[00:19:00.920 --> 00:19:05.280]   That's not at all true for a Python function. If I change one letter, it might crash. If
[00:19:05.280 --> 00:19:11.240]   I change a def into a dof, then dof is not a key word and the whole thing falls apart.
[00:19:11.240 --> 00:19:17.200]   So this makes Python functions really hard to automatically optimize, but linear algebra
[00:19:17.200 --> 00:19:22.440]   really easy to optimize. So -- and optimizing linear algebra was a big part of the sort
[00:19:22.440 --> 00:19:26.960]   of science and engineering feats of the 20th century. And it's also behind deep learning.
[00:19:26.960 --> 00:19:31.760]   Deep learning is essentially differentiable programming or programming by optimization.
[00:19:31.760 --> 00:19:35.880]   And one of the most important parts of deep learning is that linear algebra gives a whole
[00:19:35.880 --> 00:19:41.160]   bunch of functions represented by matrices that are all differentiable, that are all
[00:19:41.160 --> 00:19:45.960]   optimizable. And so it helps us get to this point where we can write lots and lots of
[00:19:45.960 --> 00:19:50.580]   programs that we can optimize, that we can differentiate, and that can do all these crazy
[00:19:50.580 --> 00:19:57.160]   cool things like beat a game of snake. So if you -- you know, I hope that this was interesting
[00:19:57.160 --> 00:20:00.520]   to folks. And if you found it interesting, the Essence of Linear Algebra series is a
[00:20:00.520 --> 00:20:06.720]   great place to start. It's, you know, effectively bingeable. So you can do it in a single setting
[00:20:06.720 --> 00:20:11.600]   if you want. Graphical Linear Algebra, the blog series on the diagrammatic approach,
[00:20:11.600 --> 00:20:15.440]   is great for folks who'd really like to build their math chops. It's sort of aimed at folks
[00:20:15.440 --> 00:20:20.560]   who don't have a ton of math experience. Maybe they took math in high school, maybe a couple
[00:20:20.560 --> 00:20:25.840]   of classes on calculus in college, and maybe a linear algebra class, and that's it. So
[00:20:25.840 --> 00:20:29.720]   it's really -- it's aimed at that folks, but it's aimed to present some, like, really,
[00:20:29.720 --> 00:20:37.800]   like, complex and difficult math to you in a way that's very reasonable and understandable.
[00:20:37.800 --> 00:20:46.480]   So I'd get -- so that's a strong recommendation. And then lastly, a plug for a series of YouTube
[00:20:46.480 --> 00:20:51.680]   lectures. So the general idea of thinking of as many things as possible in terms of
[00:20:51.680 --> 00:20:56.880]   compositions of functions, of combining things and then un-combining them into decompositions
[00:20:56.880 --> 00:21:01.720]   and breaking them apart. This is a sort of generic approach to lots and lots of branches
[00:21:01.720 --> 00:21:07.880]   of math. And it's -- there's some really great YouTube lectures aimed at folks who've had
[00:21:07.880 --> 00:21:16.160]   some experience with C++ and Java, maybe done some complicated programming there with things
[00:21:16.160 --> 00:21:26.200]   like -- I don't know, like, generics and parametric functions, I think is what they're called,
[00:21:26.200 --> 00:21:32.080]   that sort of present a lot of these, you know, very abstract math ideas, but give these nice
[00:21:32.080 --> 00:21:38.320]   concrete examples from the world of programming. So that's Bartosz Malebski is the author of
[00:21:38.320 --> 00:21:46.800]   those and he's a really, really great educator and lots of -- and he's presented lots of
[00:21:46.800 --> 00:21:55.360]   material. So that's all I have. It was a little fast. If you found it a little bit confusing,
[00:21:55.360 --> 00:21:59.560]   that's, you know, probably more on the speed at which it had to be presented than on you.
[00:21:59.560 --> 00:22:03.640]   So definitely still check out those resources where you can take it in at your own pace.
[00:22:03.640 --> 00:22:09.880]   Feel free to email me if you have any questions. Check out my personal blog or my Twitter if
[00:22:09.880 --> 00:22:15.760]   you want to see some -- if you want to see some more stuff on this and other topics in
[00:22:15.760 --> 00:22:21.440]   the intersection of math and machine learning. All right. That's all I had. If anybody has
[00:22:21.440 --> 00:22:22.440]   any questions.
[00:22:22.440 --> 00:22:24.280]   >> Cool. Thanks, Shroff.

