
[00:00:00.000 --> 00:00:04.000]   just like 30 second delay, as I mentioned last time. So like, it
[00:00:04.000 --> 00:00:10.200]   helps my paranoia a little just to double check if it's actually
[00:00:10.200 --> 00:00:13.560]   live or not. So I'll give it a few seconds.
[00:00:13.560 --> 00:00:16.760]   How many platforms are you broadcasting to?
[00:00:16.760 --> 00:00:23.880]   Two. I can see myself on YouTube. And this time there's
[00:00:23.880 --> 00:00:27.560]   no issue on the platform as well because I heard a double echo.
[00:00:27.840 --> 00:00:31.000]   Awesome. Welcome back, everybody. Great to see you all
[00:00:31.000 --> 00:00:36.360]   joining. The real host Wade and I are back. We're here to study
[00:00:36.360 --> 00:00:39.320]   about Hugging Face and NLP. Wade is the real host. He'll be
[00:00:39.320 --> 00:00:42.520]   teaching us more about these things. If you're joining us for
[00:00:42.520 --> 00:00:45.440]   the first time, we've been learning about the Hugging Face
[00:00:45.440 --> 00:00:48.200]   course and Wade has been blending in fast.ai and teaching
[00:00:48.200 --> 00:00:52.520]   us best of both worlds. He's an expert at this. He's created the
[00:00:52.520 --> 00:00:56.360]   blur library. So welcome back or welcome to our study group. This
[00:00:56.360 --> 00:00:59.960]   is the second session in part of the series. So there was another
[00:00:59.960 --> 00:01:03.000]   part people are asking about it. We'll be learning about how to
[00:01:03.000 --> 00:01:07.320]   use fast tokenizers and NLP with fast.ai and blur. That's the
[00:01:07.320 --> 00:01:10.680]   title. Broadly speaking, last time we learned about datasets.
[00:01:10.680 --> 00:01:14.680]   With that, I'll hand it over to Wade who's the real host. And
[00:01:14.680 --> 00:01:16.880]   please as a reminder to the audience, please keep the
[00:01:16.880 --> 00:01:19.480]   questions coming. I'll actively monitor the chat throughout.
[00:01:19.480 --> 00:01:23.440]   All right, awesome. Thank you, Sanyam. Good to be back again.
[00:01:23.440 --> 00:01:34.160]   Let me share my screen here. All right, hopefully everybody can
[00:01:34.160 --> 00:01:40.520]   see the slides. So yes, as Sanyam mentioned, so this is
[00:01:40.520 --> 00:01:46.720]   week two. Last week, we looked at the datasets library. And so
[00:01:46.720 --> 00:01:49.800]   if you missed that one, the recording is up there on the
[00:01:49.800 --> 00:01:55.760]   events page. And you can go back and watch that on YouTube. And
[00:01:55.760 --> 00:01:59.360]   again, it's a pretty dense, you know, all these chapters are
[00:01:59.360 --> 00:02:05.240]   dense. So when I went through part one, it usually took me a
[00:02:05.240 --> 00:02:08.480]   few days to get through a whole section. Whereas with part two,
[00:02:08.480 --> 00:02:12.080]   I found myself like having to spend like a full week and
[00:02:12.080 --> 00:02:15.800]   everyday committing, you know, an hour of my time to really
[00:02:15.840 --> 00:02:19.960]   getting into it because there's so much there. And that's true
[00:02:19.960 --> 00:02:24.280]   of datasets. And it's going to also be true of tokenizers.
[00:02:24.280 --> 00:02:26.960]   Sanyam Bhutani: Even though the text is so small, you feel like
[00:02:26.960 --> 00:02:30.360]   it's the same in size. It's like the equivalent of Python and
[00:02:30.360 --> 00:02:33.640]   then you realize, maybe my brain is slow these days. What's going
[00:02:33.640 --> 00:02:34.320]   on in my life?
[00:02:34.320 --> 00:02:37.040]   Wade Willis: Yeah, yeah. So it's funny, because when I started
[00:02:37.040 --> 00:02:39.520]   going through the datasets part, I kind of assumed that the pace
[00:02:39.520 --> 00:02:44.840]   would be the same. And I actually tweeted out to Lewis
[00:02:44.840 --> 00:02:48.240]   Tunstall, who did most of that section. And I was like, man,
[00:02:48.240 --> 00:02:50.640]   this is so great. It's one of my favorite parts of the course.
[00:02:50.640 --> 00:02:56.560]   But man, there's a lot of stuff in here. And so I hope that even
[00:02:56.560 --> 00:02:59.360]   as we go through the study group, that you're encouraged to
[00:02:59.360 --> 00:03:01.840]   like go back and actually go through the official course
[00:03:01.840 --> 00:03:05.280]   because we're not covering everything. We're covering
[00:03:05.280 --> 00:03:08.520]   things mostly related to fast AI and, and preparing our
[00:03:08.520 --> 00:03:11.200]   datasets and building data loaders and training with fast
[00:03:11.200 --> 00:03:17.000]   AI. So, so please go back through the course as well. So
[00:03:17.000 --> 00:03:21.000]   anyways, let's go ahead and get started with that with session
[00:03:21.000 --> 00:03:26.160]   two. And so in section, we're gonna be looking at section six
[00:03:26.160 --> 00:03:31.720]   from the course. And that is essentially going over the
[00:03:31.720 --> 00:03:37.240]   Hugging Face Tokenizers library. And again, a lot of content,
[00:03:37.240 --> 00:03:40.040]   there's things that we won't be covering in the study group,
[00:03:40.040 --> 00:03:45.440]   such as training a tokenizer from scratch. But I would really
[00:03:45.440 --> 00:03:49.520]   recommend if you're working with low resource language, or you
[00:03:49.520 --> 00:03:54.200]   want to actually build a mass language model, or causal
[00:03:54.200 --> 00:03:57.600]   language model, and create your own kind of pre trained
[00:03:57.600 --> 00:04:00.640]   checkpoint, even even for English, it's worth kind of
[00:04:00.640 --> 00:04:04.320]   going through that chapter and seeing how that's done. Because
[00:04:04.320 --> 00:04:07.560]   that those are really the basis when we when we use one of
[00:04:07.560 --> 00:04:11.440]   these pre trained models, somebody actually has done that
[00:04:11.440 --> 00:04:16.320]   work for us. And the tokenizers library, and alongside actually
[00:04:16.320 --> 00:04:19.520]   with the datasets library to get a corpus to train on, have
[00:04:19.520 --> 00:04:22.240]   really made that easy to do. So if you are working with low
[00:04:22.240 --> 00:04:25.960]   resource languages, try it out. And if you need help, feel free
[00:04:25.960 --> 00:04:30.560]   to hit me up on Twitter or email, I'd be glad to help and
[00:04:30.560 --> 00:04:33.400]   and see how that goes. So we're gonna be looking at the
[00:04:33.400 --> 00:04:35.640]   tokenizers library. And primarily we're going to be
[00:04:35.640 --> 00:04:41.000]   looking at how to choose the right tokenizer. There are
[00:04:41.000 --> 00:04:45.320]   factors, there's different types of subword tokenizers, there are
[00:04:45.320 --> 00:04:50.480]   pros and cons to using pre trained architectures with pre
[00:04:50.480 --> 00:04:54.720]   trained tokenizers. So we'll look at that, how to choose
[00:04:54.720 --> 00:04:59.360]   maybe when you do want to actually fine tune, or train a
[00:04:59.360 --> 00:05:03.880]   tokenizer from scratch. And then we'll look at some of the
[00:05:04.320 --> 00:05:09.080]   preparation required for getting a fast AI data block
[00:05:09.080 --> 00:05:13.320]   ready for two tasks that are mentioned in chapter or section
[00:05:13.320 --> 00:05:17.160]   six. The first one is token classification, which is things
[00:05:17.160 --> 00:05:21.840]   like name entity recognition, trying to, there's a couple
[00:05:21.840 --> 00:05:26.400]   examples like with learning the actual grammar, like what each
[00:05:26.400 --> 00:05:31.000]   word represents noun, verb, adverb, whatever. So we're going
[00:05:31.000 --> 00:05:35.640]   to look at how do we actually get a data block so that we can
[00:05:35.640 --> 00:05:38.880]   create data loaders for training. And we're gonna do the
[00:05:38.880 --> 00:05:42.440]   same with extractive question answering. And also just as a
[00:05:42.440 --> 00:05:45.160]   side note, a lot of times when you read about question
[00:05:45.160 --> 00:05:47.480]   answering tasks, they don't include that word extractive,
[00:05:47.480 --> 00:05:50.040]   but there's actually two types of question answering tasks.
[00:05:50.040 --> 00:05:55.320]   There's extractive, which is the answer exists in a context that
[00:05:55.320 --> 00:05:58.960]   your model is allowed to look at. And then there is kind of
[00:05:58.960 --> 00:06:02.880]   more dynamic question answering, which is the answer is not in
[00:06:02.880 --> 00:06:05.280]   there. And you have to train a model that can actually
[00:06:05.280 --> 00:06:09.320]   articulate an answer. And that task is a little bit more
[00:06:09.320 --> 00:06:13.320]   similar to like the summarization or text
[00:06:13.320 --> 00:06:16.920]   generation tasks that we'll be looking at later on in the study
[00:06:16.920 --> 00:06:19.560]   group. So again, we're gonna be looking at how do we get those
[00:06:19.560 --> 00:06:22.160]   data blocks. And then the next two weeks, we're actually going
[00:06:22.160 --> 00:06:26.760]   to be looking at building off of that and actually creating a
[00:06:26.760 --> 00:06:30.400]   token classification model next week. And then the following
[00:06:30.400 --> 00:06:34.200]   week doing an extractive question answering model as
[00:06:34.200 --> 00:06:39.560]   well. So again, resources, these slides will be available, I
[00:06:39.560 --> 00:06:42.640]   won't spend time going through all this again, but tons of
[00:06:42.640 --> 00:06:47.080]   resources for fast AI. The books excellent, the course,
[00:06:47.080 --> 00:06:51.520]   absolutely brilliant. Zach Mueller's walk with fast AI will
[00:06:51.520 --> 00:06:54.000]   answer all those little questions that kind of come up
[00:06:54.000 --> 00:06:58.080]   in your mind as you go through the, the official material. And
[00:06:58.080 --> 00:07:02.040]   then there's several fast AI hug and face libraries, I created
[00:07:02.040 --> 00:07:05.440]   blur, that's when I'm most familiar with, and the one that
[00:07:05.440 --> 00:07:10.200]   I'll be spending the most time going over in this study group.
[00:07:10.200 --> 00:07:14.240]   But just know there's other ones out there. And this may not be
[00:07:14.240 --> 00:07:17.600]   exhaustive. So if you're aware of other integration libraries,
[00:07:17.640 --> 00:07:24.640]   let me know. Okay, so let's go ahead and get into the first
[00:07:24.640 --> 00:07:30.600]   question, which is choosing the right tokenizer. So just like
[00:07:30.600 --> 00:07:35.360]   there's advice about using pre trained models, you want to use
[00:07:35.360 --> 00:07:41.520]   as much as possible, a model that's trained on a task that
[00:07:41.520 --> 00:07:47.360]   you're trying to accomplish on a corpus, or that has been pre
[00:07:47.360 --> 00:07:52.280]   trained on the corpus as similar to yours. And this is important,
[00:07:52.280 --> 00:07:57.480]   all the way down to what tokenizer, what the corpus that
[00:07:57.480 --> 00:08:00.080]   the tokenizer was trained on, and maybe even the type of
[00:08:00.080 --> 00:08:04.040]   tokenizer. And so if you're using a tokenizer that was
[00:08:04.040 --> 00:08:08.720]   pre trained on a corpus or another domain, or language,
[00:08:08.720 --> 00:08:13.760]   that's typically not going to work very well. And so the
[00:08:13.760 --> 00:08:18.840]   example they give in the course is if you're using a tokenizer
[00:08:18.840 --> 00:08:23.680]   that was trained on an English corpus for to be used against a
[00:08:23.680 --> 00:08:27.560]   Japanese corpus, it's probably not going to work well because
[00:08:27.560 --> 00:08:32.800]   of how the characters and also just how spaces are used in
[00:08:32.800 --> 00:08:37.120]   English versus Japanese. And you can even think about it even
[00:08:37.120 --> 00:08:41.160]   within a language, if you're trying to create, let's say a
[00:08:41.160 --> 00:08:45.400]   sequence classification model, and you're using a pre trained
[00:08:45.400 --> 00:08:49.920]   architecture, right, that was trained on, let's say, survey
[00:08:49.920 --> 00:08:53.320]   comments, and then you try to apply that to another corpus
[00:08:53.320 --> 00:08:59.920]   that is basically composed of tweets and retweets, it's
[00:08:59.920 --> 00:09:03.600]   probably not going to do as good of a job as if you actually had
[00:09:03.600 --> 00:09:06.640]   a tokenizer that knew how to handle like the app symbol and
[00:09:06.680 --> 00:09:12.840]   the hashtags. And so, again, this happens because the use of
[00:09:12.840 --> 00:09:16.400]   spaces and punctuation varies, because of the introduction of
[00:09:16.400 --> 00:09:22.240]   new characters, domain specific verbiage, and or styles. And
[00:09:22.240 --> 00:09:24.640]   again, remember, the tokenizer is going to create tokens and
[00:09:24.640 --> 00:09:26.640]   from token, when you start training your model, you're
[00:09:26.640 --> 00:09:29.240]   going to have learned embeddings. And those learned
[00:09:29.240 --> 00:09:32.960]   embeddings are essentially numerical representations of
[00:09:33.280 --> 00:09:38.000]   your tokens. So if you're using a tokenizer that breaks up
[00:09:38.000 --> 00:09:40.600]   really common words, you're probably not going to have it as
[00:09:40.600 --> 00:09:44.440]   good as a representation, if those words were actually put
[00:09:44.440 --> 00:09:48.360]   together. And we'll actually look at a few examples right
[00:09:48.360 --> 00:10:00.560]   now. So we pulled this notebook. And I'm installing all the main
[00:10:00.560 --> 00:10:04.480]   libraries again, this notebook will be made available later.
[00:10:04.480 --> 00:10:11.200]   And we're looking at the 2.0 branch of blur. So let's look at
[00:10:11.200 --> 00:10:18.040]   choosing the right tokenizer. So one of the examples they
[00:10:18.040 --> 00:10:23.480]   mentioned in the course is this data set code search net. And
[00:10:23.480 --> 00:10:25.920]   again, one of the cool things with data sets is that you can
[00:10:25.920 --> 00:10:28.960]   probably find a data set in there that's pretty similar to
[00:10:28.960 --> 00:10:32.760]   what you're doing. So here's one that actually has examples like
[00:10:32.760 --> 00:10:36.320]   pieces of code, and you can actually specify essentially
[00:10:36.320 --> 00:10:39.240]   like a sub genre, like what language you want to look at. So
[00:10:39.240 --> 00:10:43.520]   we'll look at Python. And here again, because I'm just using
[00:10:43.520 --> 00:10:47.280]   this for illustrative purposes, I'm going to just load the first
[00:10:47.280 --> 00:10:52.840]   1000 examples from the training set. And we kind of look at what
[00:10:52.840 --> 00:10:55.480]   one of those examples looks like. And you know, pretty
[00:10:55.480 --> 00:10:59.960]   traditional Python, right? So you have, you know, here's a
[00:10:59.960 --> 00:11:03.960]   your method definition, the name, you got like star star
[00:11:03.960 --> 00:11:11.200]   quarks, new line, then you have indentation, doc string, etc.
[00:11:11.200 --> 00:11:16.200]   And so the best way to really figure out if you're working
[00:11:16.200 --> 00:11:20.680]   with a good tokenizer, if the architecture you've chosen, and
[00:11:20.680 --> 00:11:23.960]   the tokenizer you're using is going to work well for your task
[00:11:24.240 --> 00:11:28.400]   is really just by looking at an example. So if there's any
[00:11:28.400 --> 00:11:33.200]   questions, look at the example. And so it's easy, I can pull up
[00:11:33.200 --> 00:11:37.760]   this example tokenize it. And you can see this is the result,
[00:11:37.760 --> 00:11:44.120]   which isn't really optimal, like it didn't really capture common
[00:11:44.120 --> 00:11:50.520]   grammar in this particular corpus, because it wasn't
[00:11:50.880 --> 00:11:56.480]   trained on Python code. And so you can see like, this is new
[00:11:56.480 --> 00:12:00.280]   line, this is a space character. And so instead of grouping the
[00:12:00.280 --> 00:12:04.000]   spaces and new lines, it actually just broke them up into
[00:12:04.000 --> 00:12:08.040]   individual characters. And it'd probably be helpful to really
[00:12:08.040 --> 00:12:11.600]   recognize what those pieces are together, and to create a
[00:12:11.600 --> 00:12:15.320]   representation for new lines and indentation because it tells you
[00:12:15.320 --> 00:12:20.080]   kind of what part of code related to the code around it.
[00:12:20.560 --> 00:12:23.720]   So it really didn't do a great job. And so this would not be
[00:12:23.720 --> 00:12:32.080]   probably an optimal tokenizer to use for looking at Python code.
[00:12:32.080 --> 00:12:34.560]   And you can see it's quite verbose, because we have all
[00:12:34.560 --> 00:12:38.800]   those spaces, right. So it takes a lot of tokens to represent an
[00:12:38.800 --> 00:12:43.000]   example. And again, that's bad, because most of us don't have,
[00:12:43.000 --> 00:12:48.560]   you know, a bunch of high end RGB loving DL rigs at home with,
[00:12:49.520 --> 00:12:53.760]   you know, quad GPUs and parallel processing, processing
[00:12:53.760 --> 00:12:56.000]   abilities, most of us are working on Colab, we're working
[00:12:56.000 --> 00:13:00.720]   on a local DL rig. And so things like this, like we can only
[00:13:00.720 --> 00:13:06.480]   process maybe 128 tokens at a time. And so we really are
[00:13:06.480 --> 00:13:09.440]   requiring a lot of tokens to represent this particular
[00:13:09.440 --> 00:13:12.680]   example, it'd be nicer to have less tokens, it would probably
[00:13:12.680 --> 00:13:17.760]   make our models more efficient and more capable of looking at
[00:13:18.680 --> 00:13:22.880]   whole examples without having to truncate it. And I saw Sanya
[00:13:22.880 --> 00:13:25.800]   pop on there real quick, because he's got, you know, these like,
[00:13:25.800 --> 00:13:31.520]   the most amazing like, machines I've ever seen. So anyways, so
[00:13:31.520 --> 00:13:36.000]   yeah, this didn't do a really great job with with the Python
[00:13:36.000 --> 00:13:41.800]   code. And so, again, we might be tempted to go oh, GPT-2, that's
[00:13:41.800 --> 00:13:45.040]   that's a common one, let's just go ahead and use it and train
[00:13:45.040 --> 00:13:49.160]   it, we probably wouldn't get good results. So again, if
[00:13:49.160 --> 00:13:53.040]   you're not sure, look at an example. Another thing that I
[00:13:53.040 --> 00:13:57.960]   typically do is actually look at the papers. And you'd be
[00:13:57.960 --> 00:14:01.040]   surprised if you haven't read an academic paper or an archive,
[00:14:01.040 --> 00:14:06.240]   how easy it actually is to get the main gist. There's a lot of
[00:14:06.240 --> 00:14:09.600]   the latex and the math and the Greek that is foreign to me, and
[00:14:09.600 --> 00:14:13.840]   it's easy to look things up. But overall, just reading the
[00:14:13.840 --> 00:14:20.000]   description, some of the high level parts of a paper in terms
[00:14:20.000 --> 00:14:24.920]   of, you know, why they went with the approach they did, what
[00:14:24.920 --> 00:14:29.480]   corpus they used, the results can be really helpful in term in
[00:14:29.480 --> 00:14:32.960]   determining at least what kind of architecture and pre trained
[00:14:32.960 --> 00:14:38.680]   checkpoint to start with. So for me, I'm only really working with
[00:14:38.720 --> 00:14:44.560]   English corpuses. And like I've learned, but to generally trust
[00:14:44.560 --> 00:14:48.440]   using a Roberta or a deeper architecture for like sequence
[00:14:48.440 --> 00:14:53.720]   classification, or I'm a big fan of Bart, and I use Bart for and
[00:14:53.720 --> 00:14:57.000]   have really good results for sequence classification, token
[00:14:57.000 --> 00:15:00.480]   classification, and summarization. And so as you
[00:15:00.480 --> 00:15:02.960]   start to use these different architectures, you can also kind
[00:15:02.960 --> 00:15:04.800]   of get a feel like you know, which one's going to work
[00:15:04.800 --> 00:15:10.480]   better for your tasking and for your corpus. So if we wanted to,
[00:15:10.480 --> 00:15:13.840]   just like we can fine tune a model, we can fine tune a
[00:15:13.840 --> 00:15:19.520]   tokenizer. And so this right here is going to create a
[00:15:19.520 --> 00:15:22.440]   generator for us that's going to work just kind of like how a
[00:15:22.440 --> 00:15:26.760]   data loader does, it's going to give us a mini batch of text
[00:15:26.760 --> 00:15:32.200]   examples to feed into our tokenizer for it to train. And
[00:15:32.200 --> 00:15:35.440]   you want to define this as a function, because once you use
[00:15:35.440 --> 00:15:37.960]   this, you're one and done. So you want to have as a function
[00:15:37.960 --> 00:15:41.080]   you can call repeatedly. And there's different syntaxes for
[00:15:41.080 --> 00:15:45.240]   using a generator. I prefer this one. That's to me, it's more
[00:15:45.240 --> 00:15:49.840]   readable, it's more flexible. And if you're not familiar with
[00:15:49.840 --> 00:15:55.160]   this yield keyword, I would advise you to Google it and get
[00:15:55.160 --> 00:15:57.480]   comfortable with it because you'll see it not just in this
[00:15:57.480 --> 00:16:02.280]   example, but you'll also see it in fast.ai in several places.
[00:16:02.280 --> 00:16:05.840]   And you'll see it in a lot of libraries wherever they're
[00:16:05.840 --> 00:16:08.840]   building data loaders, because again, we're yielding you know,
[00:16:08.840 --> 00:16:12.440]   batches at a time. So if you're not familiar with that syntax,
[00:16:12.440 --> 00:16:15.520]   check it out. But it actually is really easy. It's kind of
[00:16:15.520 --> 00:16:20.560]   amazing to me how easy it is to train or to fine tune a
[00:16:20.560 --> 00:16:25.160]   tokenizer. You really just need to take the corpus, we have our
[00:16:25.160 --> 00:16:29.040]   tokenizer object. And they have this nice method called train
[00:16:29.040 --> 00:16:33.560]   new from iterator where you pass essentially the your generator.
[00:16:33.560 --> 00:16:38.040]   And this is the number of items you want your vocab. And we're
[00:16:38.040 --> 00:16:41.280]   working with the subset. And it's super fast. And so I can
[00:16:41.280 --> 00:16:44.720]   imagine even with the full corpus, it's unbelievably fast.
[00:16:44.720 --> 00:16:48.720]   Really impressed. This actually kind of is very fast.ai-ish to
[00:16:48.720 --> 00:16:52.680]   me because I mean, with one line, you can train or fine tune
[00:16:52.680 --> 00:16:57.520]   a tokenizer, you know, for your own corpus. And you can see that
[00:16:57.520 --> 00:17:02.720]   once we do that, we get a lot better results. So in
[00:17:02.720 --> 00:17:05.520]   particular, notice that there's less tokens, there's still a
[00:17:05.520 --> 00:17:08.680]   lot, but there's a lot less tokens required to represent
[00:17:08.680 --> 00:17:11.920]   that example, which means that we're going to have to truncate
[00:17:11.920 --> 00:17:14.840]   less data, which means we're gonna be able to train locally
[00:17:14.840 --> 00:17:19.880]   or on Colab and be able to have much more text in there, we'll
[00:17:19.880 --> 00:17:23.160]   be able to have bigger batches. And also we're gonna have better
[00:17:23.160 --> 00:17:26.600]   representation. So it's kind of learned like this is a common
[00:17:26.600 --> 00:17:32.560]   thing it sees it sees a new line with a couple indentation spaces.
[00:17:32.560 --> 00:17:34.880]   So it's actually starting to learn like really key parts of
[00:17:34.880 --> 00:17:39.080]   our corpus, which is nice. So again, if you want to fine tune
[00:17:39.080 --> 00:17:44.760]   tokenizer, it's that easy. We can also look at like tweets. So
[00:17:44.760 --> 00:17:49.720]   again, I just went to the data set saw this tweet eval. And
[00:17:49.720 --> 00:17:53.960]   this is a data set. And tweets are interesting, right? Because
[00:17:53.960 --> 00:17:56.680]   they got these little funny hashtags. And they also have
[00:17:56.680 --> 00:18:01.040]   like, this particular set, if it's if there's an app to a
[00:18:01.040 --> 00:18:05.440]   user, it just replaces it with that user. And so we can look at
[00:18:05.440 --> 00:18:08.400]   like, okay, how does GPT do on this. And you can see it doesn't
[00:18:08.400 --> 00:18:12.440]   really understand like this right here, this hash tag should
[00:18:12.440 --> 00:18:17.320]   really go with this word, right. And also with that user doesn't
[00:18:17.320 --> 00:18:23.800]   get the app tag with user. So it did an okay job, this actually
[00:18:23.800 --> 00:18:29.880]   may work, but not really, it's not really optimal, right. And
[00:18:29.880 --> 00:18:33.800]   so just like before, if we wanted to, again, create our
[00:18:33.800 --> 00:18:37.880]   generator, take our tokenizer train from new iterator, and
[00:18:37.880 --> 00:18:41.320]   then we can go ahead and test it. And you can see that it's
[00:18:41.320 --> 00:18:46.760]   still kind of didn't do a great job, the GPT one, or GPT two
[00:18:46.760 --> 00:18:50.200]   tokenizer didn't do a great job. But it did kind of group the
[00:18:50.200 --> 00:18:55.880]   things right, this hashtag goes to tiny pic. And here's tiny
[00:18:55.880 --> 00:19:01.720]   pic. So seeing that that's a common word in this corpus, is
[00:19:01.720 --> 00:19:05.720]   still kind of missed this. And part of this may be because of
[00:19:05.720 --> 00:19:11.720]   how the tokenizer works. So I have a feeling the GPT two
[00:19:11.720 --> 00:19:15.160]   tokenizer, which is a byte pair encoding tokenizer, we'll look
[00:19:15.160 --> 00:19:19.000]   at that in a second. I have a feeling it splits on kind of
[00:19:19.000 --> 00:19:22.920]   weird punctuation like that just by default. And so this might
[00:19:22.920 --> 00:19:26.760]   mean that instead of fine tuning, what we should do is
[00:19:26.760 --> 00:19:32.200]   create a tokenizer from scratch and not include that splitting
[00:19:32.200 --> 00:19:36.440]   on punctuation as part of our pre processing. And again, the
[00:19:36.440 --> 00:19:39.080]   course actually goes through how to create three different
[00:19:39.080 --> 00:19:42.680]   types of tokenizers. It's really interesting. And so if
[00:19:42.680 --> 00:19:47.000]   someone wanted to do this as homework, that would be really
[00:19:47.000 --> 00:19:51.560]   cool. And especially if they share it and in a blog post. So
[00:19:51.560 --> 00:19:54.840]   we can also try a unigram tokenizer, right? So we can
[00:19:54.840 --> 00:19:59.800]   pull up T five, look at that and see how that worked. And one
[00:19:59.800 --> 00:20:03.160]   thing with T five is that when we train the tokenizer is that
[00:20:03.160 --> 00:20:07.320]   notice it actually learned to group the hashtag with the
[00:20:07.320 --> 00:20:12.280]   thing that it's related to. So even T five looks to actually
[00:20:12.280 --> 00:20:18.840]   have captured how or which is a unigram tokenizer sentence
[00:20:18.840 --> 00:20:23.000]   piece tokenizer actually, in my opinion, is able to better
[00:20:23.000 --> 00:20:26.760]   represent tweets. So if I were actually examining this corpus,
[00:20:26.760 --> 00:20:32.520]   I may look at using T five and look find a good pre trained
[00:20:32.520 --> 00:20:42.520]   checkpoint for that. So that is how to essentially look at a
[00:20:42.520 --> 00:20:46.760]   tokenizer and determine whether the one you're using is good.
[00:20:46.760 --> 00:20:51.640]   Look at an example, look at the paper. If you're really in
[00:20:51.640 --> 00:20:55.240]   trouble and you can't find one, then you can either fine tune
[00:20:55.240 --> 00:20:59.720]   or you can train a tokenizer from scratch. And again, in the
[00:21:01.480 --> 00:21:07.160]   slides I linked to the last subsection of section six, and
[00:21:07.160 --> 00:21:10.840]   there's an example of building your own word piece BPE or
[00:21:10.840 --> 00:21:17.720]   unigram tokenizers. So pros and cons of training your own. If
[00:21:17.720 --> 00:21:19.880]   you train your own, remember you're actually changing the
[00:21:19.880 --> 00:21:25.080]   vocabulary, which means that the embeddings that a pre trained
[00:21:25.080 --> 00:21:29.400]   checkpoint will have included in their weights that have been
[00:21:29.400 --> 00:21:33.960]   learned won't be available to you. So you're going to have to
[00:21:33.960 --> 00:21:38.200]   train your own language model. And another con is if you're
[00:21:38.200 --> 00:21:40.920]   experimenting with multiple architectures, which is
[00:21:40.920 --> 00:21:43.960]   typically something I do, you'll have to train and fine
[00:21:43.960 --> 00:21:49.080]   tune the tokenizer for each if you go that approach. The pros
[00:21:49.080 --> 00:21:52.120]   are you'll have a better word token representation for your
[00:21:52.120 --> 00:21:55.480]   corpus. And we saw that when we looked at the code sample and
[00:21:55.480 --> 00:21:59.160]   how it's able to understand indentations, how to
[00:21:59.160 --> 00:22:04.200]   understand spacing in the Python code, much better than the
[00:22:04.200 --> 00:22:12.760]   default GPT-2 tokenizer. So in summary, my advice is to try
[00:22:12.760 --> 00:22:17.000]   avoid training and fine tuning, look at the papers, look at the
[00:22:17.000 --> 00:22:21.560]   hub, try to find something as similar as possible so that you
[00:22:21.560 --> 00:22:26.360]   can avoid having to go through building your own language
[00:22:26.360 --> 00:22:30.760]   model. And frankly, there is so much stuff on the hub, I would
[00:22:30.760 --> 00:22:35.160]   be really surprised if you have to go this route. I'm sure for
[00:22:35.160 --> 00:22:38.840]   low resource languages is still something you'll you may have
[00:22:38.840 --> 00:22:42.760]   to look at. And if you do that, you will have the gratitude of
[00:22:42.760 --> 00:22:45.400]   the entire HuggingFace community, especially if you
[00:22:45.400 --> 00:22:49.800]   share that on the hub. But as much as possible, try to find a
[00:22:49.800 --> 00:22:54.040]   checkpoint that was trained against a corpus that's at
[00:22:54.040 --> 00:22:58.600]   least fairly representative of what you're trying to do. And
[00:22:58.600 --> 00:23:04.680]   there's also a good description of these are all subword
[00:23:04.680 --> 00:23:07.400]   tokenizers. So this isn't exhaustive. There's other
[00:23:07.400 --> 00:23:10.680]   tokenization strategies used by some transformers, but these
[00:23:10.680 --> 00:23:14.040]   are the big ones. And so I included this in the slides,
[00:23:14.040 --> 00:23:17.880]   just so you kind of know, not just how they're trained and how
[00:23:17.880 --> 00:23:24.120]   they work, but things to keep in mind. And like one, which is
[00:23:24.120 --> 00:23:27.480]   really nice, and probably one of the reasons that Roberta and
[00:23:27.480 --> 00:23:30.760]   Bart and DeBerta work well for just about everything that I do
[00:23:30.760 --> 00:23:34.840]   is that it's impossible to get an unknown token when you use
[00:23:34.840 --> 00:23:40.120]   byte level BPE. Unk tokens are basically the tokenizers way of
[00:23:40.120 --> 00:23:43.880]   saying, I see something, I don't know what it is. So I'm just
[00:23:43.880 --> 00:23:49.560]   going to call it unknown. And as you can guess, having unknown
[00:23:49.560 --> 00:23:53.640]   tokens are not good, because there's really nothing helpful
[00:23:53.640 --> 00:23:58.920]   that your model is going to be able to learn in terms of
[00:23:58.920 --> 00:24:02.120]   representing unknown tokens. So you want to avoid them as much
[00:24:02.120 --> 00:24:05.960]   as possible. And one of the nice things again, with these
[00:24:05.960 --> 00:24:12.920]   byte, byte pairing, encoding these byte level BPE tokenizers
[00:24:12.920 --> 00:24:15.480]   is that you don't have to worry about getting an unk token,
[00:24:15.480 --> 00:24:20.360]   you'll never get it. With BERT, Funnel, and PNET, these
[00:24:20.360 --> 00:24:25.800]   represent WordPiece tokenizers, it's possible to get unknown
[00:24:25.800 --> 00:24:30.680]   tokens. For Unigram, some example architecture would be
[00:24:30.680 --> 00:24:35.640]   T5, Albert, XLNet. These are all used a particular form of
[00:24:35.640 --> 00:24:39.080]   Unigram, which is called sentence piece, which is really
[00:24:41.000 --> 00:24:46.440]   common. And so just as a note, it never removes base
[00:24:46.440 --> 00:24:50.360]   characters, so as to ensure any word can be tokenized. So I
[00:24:50.360 --> 00:24:53.240]   think it's almost rare to never that you're going to get
[00:24:53.240 --> 00:24:55.960]   unknown tokens here. Not absolutely sure about that, but
[00:24:55.960 --> 00:25:00.680]   I'm pretty confident that's right. And then also, one of the
[00:25:00.680 --> 00:25:03.160]   nice things with sentence piece, there's two nice things with
[00:25:03.160 --> 00:25:08.280]   using sentence piece based tokenizers. One, because it
[00:25:08.280 --> 00:25:12.120]   doesn't have a pre-tokenization step, it's really useful for
[00:25:12.120 --> 00:25:16.840]   languages where the space character isn't used. So if you
[00:25:16.840 --> 00:25:20.680]   are using something like a Chinese or Japanese corpus or
[00:25:20.680 --> 00:25:24.440]   something similar where spaces aren't used to denote words,
[00:25:24.440 --> 00:25:28.920]   sentence piece could be the right place to look. The other
[00:25:28.920 --> 00:25:32.760]   nice thing with sentence piece is that it is fully reversible,
[00:25:32.760 --> 00:25:37.640]   which means that you can tokenize your corpus, and then
[00:25:37.640 --> 00:25:44.520]   from that tokenized corpus, get back to your raw text one for
[00:25:44.520 --> 00:25:49.400]   one. This isn't necessarily true with all tokenizers. So like
[00:25:49.400 --> 00:25:53.080]   BERT, for example, will remove repeating spaces. So if you
[00:25:53.080 --> 00:25:58.120]   tokenize a text with a BERT tokenizer, repeating spaces are
[00:25:58.120 --> 00:26:00.920]   going to be removed, so that when you go to decode it,
[00:26:00.920 --> 00:26:05.480]   you're going to lose that, which usually isn't a problem, but
[00:26:05.480 --> 00:26:10.920]   just something to be aware of. So again, if you're using a
[00:26:10.920 --> 00:26:15.800]   language like Chinese or Japanese, maybe kind of focus
[00:26:15.800 --> 00:26:22.920]   your exploration on Unigram and in particular sentence piece
[00:26:22.920 --> 00:26:29.640]   tokenizers. And with that, is there any questions coming up
[00:26:29.640 --> 00:26:30.600]   yet, Sanyam?
[00:26:30.600 --> 00:26:34.520]   I'm just curious what happens if you like try to tokenize
[00:26:34.520 --> 00:26:40.520]   emojis? You know, I was looking for just a data set of emojis,
[00:26:40.520 --> 00:26:44.040]   and I think there are examples in there. But yeah, that would
[00:26:44.040 --> 00:26:47.080]   be kind of interesting, right, in terms of how they group that,
[00:26:47.080 --> 00:26:50.360]   because I mean, emojis are all just kind of punctuation, right,
[00:26:50.360 --> 00:26:54.520]   kind of weird punctuation. And so I have a feeling that by
[00:26:54.520 --> 00:26:57.800]   default, most of these tokenizers don't do a great job
[00:26:57.800 --> 00:27:01.800]   with that. That's my gut feeling. But I also have to
[00:27:01.800 --> 00:27:04.840]   believe on the hub, there are some pre trained models, some
[00:27:04.840 --> 00:27:08.120]   folks that already did this hard work, because it's so common,
[00:27:08.120 --> 00:27:10.360]   right, to see them nowadays.
[00:27:10.360 --> 00:27:14.520]   It's weird, like, because all of these models are trained on I
[00:27:14.520 --> 00:27:18.040]   think Reddit and similar corpuses, which don't have as
[00:27:18.040 --> 00:27:21.800]   many emojis. But in our chats, @wilsonbyassist, we have this
[00:27:21.800 --> 00:27:24.600]   like Slack allows you to check the emoji reactions. I'm on the
[00:27:24.600 --> 00:27:27.640]   top of the leaderboard. So like, I use a lot of...
[00:27:27.640 --> 00:27:30.040]   Why am I not surprised? Why am I not surprised?
[00:27:30.840 --> 00:27:34.760]   But yeah, I think there should be some people who have put up
[00:27:34.760 --> 00:27:35.000]   stuff.
[00:27:35.000 --> 00:27:39.640]   Yeah. No, I'm not surprised you're on the leaderboard. And I
[00:27:39.640 --> 00:27:43.720]   have a feeling the most common one is the the chai emoji that
[00:27:43.720 --> 00:27:45.640]   I've seen trending, you know, recently.
[00:27:45.640 --> 00:27:49.640]   Yeah, you might be 100% accurate about that.
[00:27:49.640 --> 00:27:55.160]   Kurian is asking if you could go to the tokenizer output, what is
[00:27:55.160 --> 00:27:58.760]   the G like thing he asked in the output there?
[00:27:59.320 --> 00:28:01.240]   Yeah, so when we looked at the...
[00:28:01.240 --> 00:28:07.960]   Let's go back that real quick. So again, each tokenizer has
[00:28:07.960 --> 00:28:11.080]   different ways to delineate things like spaces or new lines.
[00:28:11.080 --> 00:28:15.560]   So if we're looking at like T5, you can see it does it with this
[00:28:15.560 --> 00:28:16.600]   underscore character.
[00:28:16.600 --> 00:28:24.520]   For GPT2, it uses these kind of funny, I'm not even sure what
[00:28:24.520 --> 00:28:28.120]   that character really is. But the G right here is how it
[00:28:28.120 --> 00:28:29.240]   represents a space.
[00:28:29.240 --> 00:28:35.400]   And then the weird C character is how it represents a new line.
[00:28:35.400 --> 00:28:38.520]   And so it's actually, you kind of see this more when you get up
[00:28:38.520 --> 00:28:45.800]   here. So yeah, like right here, you can see this is the end of
[00:28:45.800 --> 00:28:47.400]   the method definition, right?
[00:28:47.400 --> 00:28:51.080]   In Python code, and it's followed by a new line.
[00:28:52.280 --> 00:29:00.280]   One, two, three, four, five, six, seven or eight spaces, and
[00:29:00.280 --> 00:29:04.600]   then the doc string. And so yeah, so that's a space, that's a
[00:29:04.600 --> 00:29:05.080]   new line.
[00:29:05.080 --> 00:29:09.000]   And then yeah, you can see so there's actually eight spaces,
[00:29:09.000 --> 00:29:11.800]   right? Because there's another new line, and then it learned,
[00:29:11.800 --> 00:29:15.480]   hey, these three double quotations are important in this
[00:29:15.480 --> 00:29:16.440]   particular corpus.
[00:29:19.720 --> 00:29:23.480]   Thanks for that. There are no other questions. But again,
[00:29:23.480 --> 00:29:25.880]   reminder to all please keep asking the question, then I'll
[00:29:25.880 --> 00:29:27.960]   interpret whenever I get a chance to ask them.
[00:29:27.960 --> 00:29:34.520]   Cool. All right. So the next thing that's that's heavily
[00:29:34.520 --> 00:29:39.880]   discussed and is used throughout section six, and then also into
[00:29:39.880 --> 00:29:45.400]   section seven, when they start looking at particular tasks, is
[00:29:46.440 --> 00:29:50.920]   there's two types of tokenizers. There are fast tokenizers, and
[00:29:50.920 --> 00:29:55.320]   there are slow tokenizers. And the fast tokenizers are written
[00:29:55.320 --> 00:30:00.360]   in Rust. It can take advantage of parallelization. So we've
[00:30:00.360 --> 00:30:04.520]   seen like in prior study groups, how much faster they can
[00:30:04.520 --> 00:30:08.520]   process our text, tokenize it and give us all of our various
[00:30:08.520 --> 00:30:12.200]   inputs. And then the slow tokenizers are written in
[00:30:12.200 --> 00:30:17.480]   Python. They are slow, and they also don't have as many
[00:30:17.480 --> 00:30:23.000]   features as fast tokenizers. And so one thing is, when you're
[00:30:23.000 --> 00:30:27.160]   building an architecture, I will say that, by and large,
[00:30:27.160 --> 00:30:30.680]   there's a fast tokenizer version of the tokenizer that you want
[00:30:30.680 --> 00:30:34.040]   to use. And when you call auto tokenizer, if a fast version
[00:30:34.040 --> 00:30:37.560]   exists, that's what you're going to get. And as a
[00:30:37.560 --> 00:30:43.480]   recommendation, try to stick with fast tokenizers. We'll look
[00:30:43.480 --> 00:30:47.480]   at some reasons why they definitely make tasks like token
[00:30:47.480 --> 00:30:50.440]   classification, the pre-processing and post-processing
[00:30:50.440 --> 00:30:54.920]   much easier. Same with question answering. Same with building
[00:30:54.920 --> 00:30:58.600]   mass language models. So try to stick with the fast tokenizer
[00:30:58.600 --> 00:31:04.280]   if you can. And when we go look at Blur, I'm really building
[00:31:04.280 --> 00:31:07.160]   things out to mostly work with fast tokenizers. But if you're
[00:31:07.160 --> 00:31:11.560]   not, you can substitute and provide functions that can
[00:31:11.560 --> 00:31:14.200]   provide things like word IDs, which we'll look at in a second,
[00:31:14.200 --> 00:31:17.560]   which is something only a fast tokenizer does. But if you
[00:31:17.560 --> 00:31:20.120]   create a function that does that for a slow tokenizer, which
[00:31:20.120 --> 00:31:26.520]   is possible, you can still use Blur to its fullest. So why
[00:31:26.520 --> 00:31:30.040]   fast tokenizers? One, that fancy train new from iterator
[00:31:30.040 --> 00:31:32.600]   method to fine tune the tokenizer only works with fast
[00:31:32.600 --> 00:31:38.600]   tokenizers. As I mentioned, fast tokenization, when we
[00:31:38.600 --> 00:31:43.080]   tokenize texts and batches can be parallelized. And it's just
[00:31:43.080 --> 00:31:48.280]   by default, it's parallelized and super fast. The other thing
[00:31:48.280 --> 00:31:51.960]   that is really nice with fast tokenizers is that they keep
[00:31:51.960 --> 00:31:55.080]   track of the original span of text or the characters the
[00:31:55.080 --> 00:32:01.480]   final tokens come from. And this allows us to do a lot of
[00:32:01.480 --> 00:32:03.800]   things that are really helpful for the two tasks we're going
[00:32:03.800 --> 00:32:08.280]   to be looking at. Things like mapping each word to the tokens
[00:32:08.280 --> 00:32:14.360]   that generated or mapping a token back to the original text
[00:32:14.360 --> 00:32:20.840]   and the characters. And this all happens by looking at the
[00:32:20.840 --> 00:32:24.520]   batch encoding object that's returned when we call the
[00:32:24.520 --> 00:32:28.840]   tokenizer. And one of the things I always have asked on
[00:32:28.840 --> 00:32:32.440]   the Hugging Face forums and I put here in the text is there's
[00:32:32.440 --> 00:32:38.280]   actually a big table that lists what fast tokenizers exist. So
[00:32:38.280 --> 00:32:43.720]   see this slide and click on this link down here to check
[00:32:43.720 --> 00:32:49.480]   that out for yourself. So let's go ahead and get into some of
[00:32:49.480 --> 00:32:54.120]   the pre-processing side. So again, this week we're looking
[00:32:54.120 --> 00:32:57.800]   at, okay, let's get our data loaders ready. And then next
[00:32:57.800 --> 00:33:00.760]   week and the following week, let's go ahead and take what
[00:33:00.760 --> 00:33:04.360]   we've done this week and actually use it to train a token
[00:33:04.360 --> 00:33:07.720]   classification and question answering model. So some of the
[00:33:07.720 --> 00:33:10.520]   things that fast tokenizers provide that are real helpful
[00:33:10.520 --> 00:33:16.520]   in token classification is one is this idea of word IDs. And
[00:33:16.520 --> 00:33:20.520]   so again, when tokenization happens, it takes words and it
[00:33:20.520 --> 00:33:24.280]   breaks them up into sub tokens, right? So there's one or more
[00:33:24.280 --> 00:33:30.120]   sub tokens for every word in your raw text. And when you're
[00:33:30.120 --> 00:33:35.240]   doing token classification, we have labels like person,
[00:33:35.240 --> 00:33:39.880]   organization, location assigned to a word. And so it's really
[00:33:39.880 --> 00:33:44.120]   helpful when we're encoding the sub tokens to use that, this
[00:33:44.120 --> 00:33:47.800]   information, this word IDs to know like, okay, here's three
[00:33:47.800 --> 00:33:51.240]   sub tokens for this one word. What's that label for that word?
[00:33:51.240 --> 00:33:54.360]   And given that, how do we actually label these sub tokens?
[00:33:54.360 --> 00:33:57.880]   And there's different strategies for that. The other
[00:33:57.880 --> 00:34:01.880]   thing that's really helpful is this return offsets mapping
[00:34:01.880 --> 00:34:05.720]   equals true option that we can pass when we call our
[00:34:05.720 --> 00:34:12.520]   tokenizer to tokenize a batch of text. And we can use this to
[00:34:12.520 --> 00:34:20.680]   again, apply the label properly to the predicted or assigned
[00:34:20.680 --> 00:34:25.720]   token. And also use that to when we're decoding our sub
[00:34:25.720 --> 00:34:30.680]   tokens and decoding them back into words to figure out what
[00:34:30.680 --> 00:34:35.640]   we're going to choose as the correct word. And again,
[00:34:35.640 --> 00:34:38.200]   there's a couple different strategies for doing that with
[00:34:38.200 --> 00:34:47.480]   token classification. So let's go ahead and look at an example
[00:34:47.480 --> 00:34:54.360]   here. And I'm also going to show you some blur code as well. So
[00:34:54.360 --> 00:34:58.520]   you can see how this is all happening. But if you wanted to
[00:34:58.520 --> 00:35:06.920]   basically use blur and fast AI for token classification, this
[00:35:06.920 --> 00:35:11.400]   is how you could do it. And I'm going to go ahead and use the
[00:35:11.400 --> 00:35:19.560]   con NLL 2003 dataset, which is really nice because it actually
[00:35:19.560 --> 00:35:23.000]   includes several token classification tasks that you
[00:35:23.000 --> 00:35:28.200]   can choose to run. So we have a POS task, a chunk task, and a
[00:35:28.200 --> 00:35:32.360]   name entity recognition near task. And we can kind of look
[00:35:32.360 --> 00:35:37.720]   at what the text looks like. And we can see that we're
[00:35:37.720 --> 00:35:41.960]   working with label indices, right? So these label indices
[00:35:41.960 --> 00:35:48.200]   point to these particular labels. And so we can see for
[00:35:48.200 --> 00:35:54.200]   the different tasks here, like what these things actually look
[00:35:54.200 --> 00:35:57.640]   like, what we're trying to predict. And so you can play
[00:35:57.640 --> 00:36:00.760]   around with, like I said, different token classification
[00:36:00.760 --> 00:36:05.240]   tasks just with this one dataset. So we'll look at using
[00:36:05.240 --> 00:36:09.400]   the name entity recognition tags. So these are the labels
[00:36:09.400 --> 00:36:17.160]   right here. And again, for just about every blur task, it's
[00:36:17.160 --> 00:36:20.920]   really you want to get your hugging face objects, the
[00:36:20.920 --> 00:36:24.440]   architecture, the configuration object, tokenizer, and model.
[00:36:24.440 --> 00:36:33.000]   And almost everything can just be fetched using this type of
[00:36:33.000 --> 00:36:37.640]   syntax right here. So I'm going to choose as a pre-trained
[00:36:37.640 --> 00:36:41.480]   model name is Distilled Roberta Base. This is one of my
[00:36:41.480 --> 00:36:45.400]   favorites because it's Roberta, which is a fabulous
[00:36:45.400 --> 00:36:49.400]   architecture for just about anything classification-wise,
[00:36:49.400 --> 00:36:52.200]   including token classification. And the distilled version is
[00:36:52.200 --> 00:36:57.880]   really fast. So I can actually use bigger batches and use a
[00:36:57.880 --> 00:37:02.280]   bigger max length, whether I'm training on Colab or on my own
[00:37:02.280 --> 00:37:05.720]   local DL rig. So we go through this process. We get our
[00:37:05.720 --> 00:37:13.400]   objects. The key configuration to pass is the number of labels.
[00:37:13.400 --> 00:37:18.200]   This is probably the most frequent mistake that folks
[00:37:18.200 --> 00:37:23.880]   make when using hugging face and using the blur library, is
[00:37:23.880 --> 00:37:28.360]   that they forget to set this. And by default, it's two. If
[00:37:28.360 --> 00:37:31.560]   you have anything more than two, you'll get some interesting
[00:37:31.560 --> 00:37:35.160]   errors that are sometimes not that readable. A lot of them
[00:37:35.160 --> 00:37:39.240]   look like those CUDA assert errors that are real fun
[00:37:39.240 --> 00:37:41.560]   because they tell you nothing except that something is
[00:37:41.560 --> 00:37:45.400]   seriously wrong. So just remember to set that when you're
[00:37:45.400 --> 00:37:49.880]   doing token classification. We can check to see if our
[00:37:49.880 --> 00:37:53.880]   tokenizer is fast by just calling that property as fast.
[00:37:53.880 --> 00:37:58.600]   And then there's cool things, again, that we can do with the
[00:37:58.600 --> 00:38:02.920]   fast tokenizer. So I'm passing in this return offsets mapping.
[00:38:02.920 --> 00:38:04.840]   And again, these are things that you can only do with the
[00:38:04.840 --> 00:38:09.560]   fast tokenizer. So if you want to replicate this with a
[00:38:09.560 --> 00:38:13.240]   Python tokenizer, you're going to have to write your own code.
[00:38:13.240 --> 00:38:17.720]   And that's probably going to be very specific to whatever
[00:38:17.720 --> 00:38:22.200]   tokenizer you're using. So again, try to use fast
[00:38:22.200 --> 00:38:26.040]   tokenizers if you can. And you can see that when we pass this,
[00:38:26.040 --> 00:38:31.400]   we get this another key added to this batch encoding object.
[00:38:31.400 --> 00:38:37.480]   And so we can actually look at the tokens. So again, when a
[00:38:37.480 --> 00:38:40.920]   tokenizer takes our raw text, it's going to create a list of
[00:38:40.920 --> 00:38:48.120]   tokens, which is exactly what we see here. And for Roberta,
[00:38:48.120 --> 00:38:54.440]   we have this is our class and our separation token. And you
[00:38:54.440 --> 00:38:56.680]   can actually figure out what all these are by looking at the
[00:38:56.680 --> 00:39:01.720]   documentation, or you can inspect the tokenizer. They
[00:39:01.720 --> 00:39:05.720]   actually list-- there's actually methods to actually get and see
[00:39:05.720 --> 00:39:09.720]   what are the special tokens for representing separation,
[00:39:09.720 --> 00:39:15.400]   classes, padding, and so forth. So we can see that these are our
[00:39:15.400 --> 00:39:21.640]   tokens. And we can also see the mapping between tokens and
[00:39:21.640 --> 00:39:28.360]   words. So as an example, my name is not a common spelling of
[00:39:28.360 --> 00:39:34.280]   Wade. And we can see it's broken this up as-- that's the
[00:39:34.280 --> 00:39:40.840]   space, W-A-Y-D-E. But how do we know that these two tokens come
[00:39:40.840 --> 00:39:45.000]   from the same word? Well, that's what we use, word IDs. And we
[00:39:45.000 --> 00:39:49.880]   can see that none is assigned to the special tokens. So we start
[00:39:49.880 --> 00:39:57.400]   with 0, 1, 2, and the third word is Wade. So we can see 0, 1, 2,
[00:39:57.400 --> 00:40:02.520]   and both of those tokens are mapped to word 3, which is
[00:40:02.520 --> 00:40:08.600]   Wade. So this is going to be really helpful for token
[00:40:08.600 --> 00:40:12.440]   classification. The other nice thing that we'll use for token
[00:40:12.440 --> 00:40:15.880]   classification and question answering is the offset mapping
[00:40:18.040 --> 00:40:23.800]   key that's added when we pass return offsets mapping equals
[00:40:23.800 --> 00:40:29.800]   true. And what this is actually going to do is give us a mapping
[00:40:29.800 --> 00:40:36.040]   back from each token to the characters from which it
[00:40:36.040 --> 00:40:41.320]   originated. So you can see all the special tokens are going to
[00:40:41.320 --> 00:40:49.160]   be just 0, 0. And we can see that this is token 1 goes from
[00:40:49.160 --> 00:40:59.400]   0 to 2, token 2 from 3 to 7, token 3 goes from characters 8
[00:40:59.400 --> 00:41:03.480]   to 10. So again, this is going to be really helpful regardless
[00:41:03.480 --> 00:41:07.320]   of whether we're working with languages that use spaces or not
[00:41:07.320 --> 00:41:12.280]   to figure out how to get back to that raw text. And again,
[00:41:12.280 --> 00:41:16.520]   token classification, this is going to be really key because
[00:41:16.520 --> 00:41:19.720]   we're going to break things into a bunch of sub tokens, but
[00:41:19.720 --> 00:41:22.840]   ultimately we want to go back to our words and say, like, what
[00:41:22.840 --> 00:41:25.960]   is this? What is this word? It might be five tokens to
[00:41:25.960 --> 00:41:29.800]   represent this word, but at the end of the day, what is that
[00:41:29.800 --> 00:41:34.600]   word? Is it a person? Is it just nothing? Is it a location?
[00:41:35.400 --> 00:41:43.400]   So this allows us to do that. So anyways, yeah. So we can use
[00:41:43.400 --> 00:41:48.280]   this capability to apply labels we have for each word properly
[00:41:48.280 --> 00:41:52.200]   to the tokens and tasks like name recognition or part of
[00:41:52.200 --> 00:41:58.200]   speech tagging. We can also use word IDs to map each word's
[00:41:58.200 --> 00:42:01.720]   label onto the tokens, and then we can use offset mapping to
[00:42:01.720 --> 00:42:06.200]   apply the labels assigned to a token back to a specific word.
[00:42:06.200 --> 00:42:10.520]   And so like I said, there's actually different strategies
[00:42:10.520 --> 00:42:17.160]   for doing this. And in Blur version 2, we support two that
[00:42:17.160 --> 00:42:20.440]   are mentioned in the course and then also a third one. And
[00:42:20.440 --> 00:42:24.760]   we'll take a look at briefly what those are. But it's as
[00:42:24.760 --> 00:42:30.200]   simple as passing in this label strategy class. And again,
[00:42:30.200 --> 00:42:32.920]   there's going to be three of them in Blur. You can also
[00:42:32.920 --> 00:42:36.680]   create your own. By default, we're going to use the only
[00:42:36.680 --> 00:42:41.480]   first token labeling strategy. And how does that actually
[00:42:41.480 --> 00:42:45.880]   look? Well, here's kind of an example of the three
[00:42:45.880 --> 00:42:48.760]   strategies and how they would change how we actually
[00:42:48.760 --> 00:42:55.160]   represent the tokenized words. So only first labeling
[00:42:55.160 --> 00:42:59.080]   strategy, which is the default, the first token is going to
[00:42:59.080 --> 00:43:02.600]   get the word label. The other ones are going to get negative
[00:43:02.600 --> 00:43:06.840]   100. So negative 100 is the default when we calculate
[00:43:06.840 --> 00:43:14.200]   cross-entropy loss. It is the number that tells cross-entropy
[00:43:14.200 --> 00:43:19.560]   loss to ignore that particular element when calculating the
[00:43:19.560 --> 00:43:24.360]   loss. And so by using this strategy, we're only going to
[00:43:24.360 --> 00:43:29.720]   be really determining the whole word based on the first
[00:43:29.720 --> 00:43:36.200]   token. And when we calculate our loss, it's going to know to
[00:43:36.200 --> 00:43:41.800]   not really include the subsequent tokens when
[00:43:41.800 --> 00:43:44.440]   calculating that. It's only going to do it based on the
[00:43:44.440 --> 00:43:47.800]   first token in the word. Another strategy that's
[00:43:47.800 --> 00:43:53.320]   supported is the same label labeling strategy. And this,
[00:43:53.320 --> 00:43:59.320]   what we do is assign each token to whatever the word label is.
[00:43:59.320 --> 00:44:04.280]   So again, here, Wade is the beginning of a person's name.
[00:44:04.280 --> 00:44:10.440]   And so we would assign B-per comma B-per to both of those
[00:44:10.440 --> 00:44:15.960]   tokens. A final one that's fairly common, and this is the
[00:44:15.960 --> 00:44:18.360]   one that's not introduced in the course, is the same label
[00:44:18.360 --> 00:44:22.520]   labeling strategy, is the BI labeling strategy. This is
[00:44:22.520 --> 00:44:27.240]   included in the course. If you're using BI labels, so for
[00:44:27.240 --> 00:44:31.320]   example, my name is Wade Gilliam, and Wade would
[00:44:31.320 --> 00:44:35.560]   typically get a B-per label, and Gilliam would get an I-per
[00:44:35.560 --> 00:44:40.360]   because it follows on from the beginning. If you're using that
[00:44:40.360 --> 00:44:46.200]   strategy, we apply that further to tokenization so that here,
[00:44:46.200 --> 00:44:52.200]   even though Wade, the whole word is, would be labeled as
[00:44:52.200 --> 00:44:57.320]   B-per in the raw dataset, we use B-per as the first token,
[00:44:57.320 --> 00:45:01.480]   and then we use I-per as the following token. So those are
[00:45:01.480 --> 00:45:07.160]   the labeling strategies that Blur supports. And if you go
[00:45:07.160 --> 00:45:13.000]   through part six of the course, they'll go over, and also some
[00:45:13.000 --> 00:45:17.320]   of the pros and cons of one and three, and why you might want
[00:45:17.320 --> 00:45:20.520]   to choose one or the other, why researchers choose one or the
[00:45:20.520 --> 00:45:24.120]   other. They'll both give you, they should all give you good
[00:45:24.120 --> 00:45:27.480]   results, but spend some time going through that if you have
[00:45:27.480 --> 00:45:33.080]   further questions. So yeah, so we can go ahead and we apply a
[00:45:33.080 --> 00:45:36.440]   labeling strategy. And the other big thing with token
[00:45:36.440 --> 00:45:44.440]   classification is that our inputs are lists of words, and
[00:45:44.440 --> 00:45:48.280]   it's really critical to kind of really think about why you want
[00:45:48.280 --> 00:45:53.800]   that. And again, it's because for however, for whatever
[00:45:53.800 --> 00:45:58.520]   language you're working with, again, we're trying to identify
[00:45:58.520 --> 00:46:02.600]   what a particular entity is. And when you provide a list,
[00:46:02.600 --> 00:46:06.120]   it's much clearer like what that means versus just a bunch
[00:46:06.120 --> 00:46:11.080]   of raw text, and then leaving it up to the tokenizer to create
[00:46:11.080 --> 00:46:16.360]   a list of tokens from that. So for token classification, it's
[00:46:16.360 --> 00:46:19.480]   required the input needs to be on the list. When you do
[00:46:19.480 --> 00:46:22.360]   inference, we can pass in a raw text, and that's something
[00:46:22.360 --> 00:46:26.200]   we'll see next week. So again, I'm working with the dataset.
[00:46:26.200 --> 00:46:31.480]   With Blur, we have this HF token class before batch
[00:46:31.480 --> 00:46:35.320]   transform that does some special things for token
[00:46:35.320 --> 00:46:41.000]   classification. And next week, we'll look more at using a
[00:46:41.000 --> 00:46:44.280]   pre-processed dataset and why that might be useful. But again,
[00:46:44.280 --> 00:46:48.200]   with Blur, this is with version one, we can go ahead and do this
[00:46:48.200 --> 00:46:54.200]   tokenization process on the fly. And so that's what we're going
[00:46:54.200 --> 00:46:57.880]   to do here. And we are using item getter to get our tokens
[00:46:57.880 --> 00:47:02.120]   and our NER tags, and we're just using a random splitter.
[00:47:02.120 --> 00:47:06.600]   And again, the ultimate goal is to get our data loaders,
[00:47:06.600 --> 00:47:13.160]   which when we pass our training dataset, we get always a
[00:47:13.160 --> 00:47:17.320]   helpful thing. Look at the batch, right? Does it actually
[00:47:17.320 --> 00:47:21.320]   make sense to you? When I'm building Blur, a lot of times
[00:47:21.320 --> 00:47:24.840]   the batch looks wrong because I've done wrong things. And
[00:47:24.840 --> 00:47:28.760]   this really is kind of like a safeguard to make sure, like
[00:47:28.760 --> 00:47:32.520]   just a sanity check, like, yes, what I see here in terms of the
[00:47:32.520 --> 00:47:37.080]   number of items in the batch, what input IDs looks like, what
[00:47:37.080 --> 00:47:39.720]   the targets look like. Yes, that makes sense. And I can go
[00:47:39.720 --> 00:47:47.880]   forward. And then we can also call show batch. And with Blur,
[00:47:47.880 --> 00:47:52.360]   the way that show batch works with token classification is
[00:47:52.360 --> 00:47:57.880]   that it takes each of your raw words and it shows the label.
[00:47:57.880 --> 00:48:02.280]   So it shows the word and then the label associated, the
[00:48:02.280 --> 00:48:08.600]   entity label associated to it. So that is how you can actually
[00:48:08.600 --> 00:48:14.040]   prepare a data block for token classification tasks using
[00:48:14.040 --> 00:48:21.880]   Blur. And I wanted to... Can you guys all see this or does it
[00:48:21.880 --> 00:48:31.160]   need to blow this up a little bit? We can actually look at
[00:48:31.160 --> 00:48:36.600]   some of the Blur notebooks. And again, these are all like up on
[00:48:36.600 --> 00:48:44.360]   GitHub on the v2 branch. Oops. I think you might have to zoom in
[00:48:44.360 --> 00:48:47.480]   a bit here. You know what? This is going to sound really
[00:48:47.480 --> 00:48:53.960]   ridiculous, but do you know how to zoom in? Oh, here we go. I
[00:48:53.960 --> 00:48:57.640]   don't know. >> That disappeared. >> That was an
[00:48:57.640 --> 00:49:03.960]   interesting side effect. Let's try that again. Zoom. Is that
[00:49:03.960 --> 00:49:10.040]   like any bigger to you? No? Oh, here we go. Oh, nice. All
[00:49:10.040 --> 00:49:15.000]   right. >> Awesome. >> There you go. Much better, I hope. >>
[00:49:15.000 --> 00:49:18.680]   Yep, that's awesome. >> Yes, you can actually look at all that
[00:49:18.680 --> 00:49:23.320]   stuff that you saw in the... Actually in the use case of
[00:49:23.320 --> 00:49:27.160]   building a data block for token classification. If you go to
[00:49:27.160 --> 00:49:30.120]   the notebooks, you'll see how all this stuff works and how
[00:49:30.120 --> 00:49:35.640]   it's utilizing some of these fast tokenizer bits. And so you
[00:49:35.640 --> 00:49:38.760]   can see how there's labeling strategies. And again, if one
[00:49:38.760 --> 00:49:42.200]   of those three labeling strategies doesn't mesh with
[00:49:42.200 --> 00:49:45.640]   what you want to do for some reason, you can actually create
[00:49:45.640 --> 00:49:51.240]   a subclass of base labeling strategy and use your own. And
[00:49:51.240 --> 00:49:55.080]   if you look at these, you can see that they're all using the
[00:49:55.080 --> 00:50:00.200]   word IDs, right? So being able to take these tokens and figure
[00:50:00.200 --> 00:50:03.480]   out what word they're associated to to get the label, whether
[00:50:03.480 --> 00:50:07.880]   it's going to be using the same label for each token or just...
[00:50:08.600 --> 00:50:15.560]   Or use the BI syntax or set the subsequent tokens of negative
[00:50:15.560 --> 00:50:20.360]   100, you need to be able to work with word IDs. So you can go
[00:50:20.360 --> 00:50:24.680]   through this code and actually, hopefully it's pretty readable
[00:50:24.680 --> 00:50:29.160]   and see how those different strategies are implemented in
[00:50:29.160 --> 00:50:34.440]   practice. There's also, and we use this in the show batch
[00:50:34.440 --> 00:50:37.480]   method, but you can actually use this on your own. We have some
[00:50:37.480 --> 00:50:41.640]   other methods like get token labels from input IDs that
[00:50:41.640 --> 00:50:45.480]   allow us to, as best as possible for the tokenizer,
[00:50:45.480 --> 00:50:50.840]   reconstruct the original raw text. And this is really
[00:50:50.840 --> 00:50:53.400]   helpful when we're doing things like inference, right? And we
[00:50:53.400 --> 00:50:58.280]   don't have things pre-split up necessarily. And so you can
[00:50:58.280 --> 00:51:03.960]   read how we do that. A lot more test and blur. So one of the
[00:51:03.960 --> 00:51:09.880]   nice things I use in nbdev for building blur out. And it's nice
[00:51:09.880 --> 00:51:14.600]   to be able to include these tests right in the notebook. And
[00:51:14.600 --> 00:51:17.560]   so you can actually pull up these notebooks, run them
[00:51:17.560 --> 00:51:21.400]   yourselves, verify that the test work and have confidence
[00:51:21.400 --> 00:51:24.840]   that you're getting what you expected. And then we have get
[00:51:24.840 --> 00:51:28.280]   word labels from token labels that actually works in
[00:51:28.280 --> 00:51:32.120]   conjunction with the method that we just saw. And so check
[00:51:32.120 --> 00:51:38.680]   that out. That's all has testing associated with it. Next week
[00:51:38.680 --> 00:51:41.960]   we'll look at pre-processing. So I'm going to skip that part
[00:51:41.960 --> 00:51:46.920]   right there. And then you can see that for blur, we have a
[00:51:46.920 --> 00:51:50.920]   mid-level API. We introduced a couple of target specific
[00:51:50.920 --> 00:51:55.640]   elements. So we have a token tensor category to identify
[00:51:56.760 --> 00:52:02.200]   token classification inputs. We have a HF token categorize
[00:52:02.200 --> 00:52:06.840]   method because we have like, basically what this does is
[00:52:06.840 --> 00:52:11.400]   turn every single input into a category that we want to
[00:52:11.400 --> 00:52:15.800]   predict. Check at that. And then again, just like we have
[00:52:15.800 --> 00:52:19.480]   the text input, the HF text input block, we can create
[00:52:19.480 --> 00:52:24.520]   blocks for our targets. And you can see it's fairly simple. It
[00:52:24.520 --> 00:52:30.040]   just really wraps that HF token categorize and presents that
[00:52:30.040 --> 00:52:39.960]   as a type transform. Let's see here. Oh, so again, because
[00:52:39.960 --> 00:52:46.920]   we're really relying on fast tokenizers and these methods
[00:52:46.920 --> 00:52:51.000]   like word IDs or like the offset mappings, in some cases,
[00:52:51.800 --> 00:52:55.400]   I didn't want to preclude people using a slow tokenizer
[00:52:55.400 --> 00:52:59.480]   from using blur. So what you can do is provide your own
[00:52:59.480 --> 00:53:03.720]   slow word IDs function. And it just needs to return the same
[00:53:03.720 --> 00:53:07.320]   things that a fast tokenizer's word IDs would. And if you do
[00:53:07.320 --> 00:53:12.760]   that and using a slow tokenizer, you'll see like here where I'm
[00:53:12.760 --> 00:53:18.520]   needing to get word IDs. If the tokenizer is not fast, it's
[00:53:18.520 --> 00:53:22.120]   going to call that method you provide. And that method should
[00:53:22.120 --> 00:53:29.080]   receive the tokenizer. It's going to receive like what the
[00:53:29.080 --> 00:53:32.360]   index of the example. And then it's going to receive that
[00:53:32.360 --> 00:53:35.640]   batch encoding object, which isn't going to have as much as
[00:53:35.640 --> 00:53:39.000]   it would with a fast tokenizer. But we'll have enough for you to
[00:53:39.000 --> 00:53:44.440]   essentially hopefully be able to return word IDs. So if you
[00:53:44.440 --> 00:53:46.600]   actually work through any of those examples, again, that'd
[00:53:46.600 --> 00:53:50.600]   actually be really fun to see like in a blog post. And also,
[00:53:50.600 --> 00:53:53.880]   if it doesn't work, you have problems, that would be good for
[00:53:53.880 --> 00:54:00.520]   me to know about as I develop the library. So yeah, feel free
[00:54:00.520 --> 00:54:05.000]   to check out the documentation and explore this on your own.
[00:54:05.000 --> 00:54:09.160]   And is there any questions on token classification?
[00:54:09.160 --> 00:54:16.440]   Oh, there are a few questions. So the first one is, how is
[00:54:16.920 --> 00:54:21.400]   FastTokenizer different from spaCy under the hood?
[00:54:21.400 --> 00:54:27.800]   So yeah, so with HuggingFace, we're not using any of the fast
[00:54:27.800 --> 00:54:33.560]   AI tokenizer like that uses spaCy. So spaCy is a little bit
[00:54:33.560 --> 00:54:40.440]   more, I'm not 100, super familiar with using it. But I
[00:54:40.440 --> 00:54:43.960]   think it operates more like on spaces. So I don't know if it's
[00:54:43.960 --> 00:54:49.160]   friendly for as many languages as like a subword tokenizer
[00:54:49.160 --> 00:54:53.400]   would be. But it's really been a long time since I played with
[00:54:53.400 --> 00:54:57.880]   it. I know there's equivalents, right? There's spaCy versions of
[00:54:57.880 --> 00:55:01.800]   spaCy for different languages, right, that you can download.
[00:55:01.800 --> 00:55:07.480]   And I don't think the way fast AI is set up, it's really going
[00:55:07.480 --> 00:55:11.960]   to work well with languages that don't use spaces. And really, I
[00:55:11.960 --> 00:55:15.240]   don't think it's going to be set up well to work with non English
[00:55:15.240 --> 00:55:21.640]   text by default. But yeah, what all the particular differences
[00:55:21.640 --> 00:55:23.720]   are, I'm not positive.
[00:55:23.720 --> 00:55:26.840]   Okay, that makes sense. Thank you.
[00:55:26.840 --> 00:55:35.080]   Isn't same label labeling strategy wrong if we use same
[00:55:35.080 --> 00:55:39.160]   always as be dash denotes start of sentence entity?
[00:55:40.520 --> 00:55:46.200]   I think it just depends. To be honest, I have typically used
[00:55:46.200 --> 00:55:49.960]   the same label labeling strategy, and I have gotten
[00:55:49.960 --> 00:55:54.680]   really good results. It's definitely not one of the
[00:55:54.680 --> 00:55:58.600]   traditional ones. So in the course, it uses the first token
[00:55:58.600 --> 00:56:03.640]   and then also the BI strategy. But and maybe this is before I
[00:56:03.640 --> 00:56:06.840]   just knew any better. I always use this the same label labeling
[00:56:06.840 --> 00:56:09.880]   strategy so that each of the tokens have the same label as
[00:56:09.880 --> 00:56:12.760]   the word. And like I said, I actually got really good
[00:56:12.760 --> 00:56:19.800]   results. And ironically, actually trained a name entity
[00:56:19.800 --> 00:56:23.560]   recognition model on a German corpus that ended up working
[00:56:23.560 --> 00:56:27.880]   really well for English. I think there's a lot more
[00:56:27.880 --> 00:56:30.280]   similarities between German and English, right? Like some of
[00:56:30.280 --> 00:56:34.200]   the words are very similar, if not identical, and also bad
[00:56:34.200 --> 00:56:37.800]   with the spacing. But it was kind of amazing that it worked
[00:56:37.800 --> 00:56:40.680]   as well as it did. So yeah, you're right. That's definitely
[00:56:40.680 --> 00:56:44.440]   not one of the normal ones. But it's the one I've used in the
[00:56:44.440 --> 00:56:47.640]   past and had good results. I'm including it in the library.
[00:56:47.640 --> 00:56:53.000]   Awesome. Thanks. Thanks for sharing that. I don't see any
[00:56:53.000 --> 00:56:55.480]   other questions. I think that was the last of it.
[00:56:55.480 --> 00:56:56.120]   All right, cool.
[00:56:56.120 --> 00:57:07.080]   All right. So that is it for basically, we're at the point
[00:57:07.080 --> 00:57:10.440]   now with tokenizers, we can actually create data loaders.
[00:57:10.440 --> 00:57:12.680]   And again, next week, we're going to look at actually using
[00:57:12.680 --> 00:57:18.440]   them to build a model. So the other task this mission in
[00:57:18.440 --> 00:57:21.640]   section six is question answering. And so one of the
[00:57:21.640 --> 00:57:25.800]   things that's interesting with question answering is this
[00:57:25.800 --> 00:57:29.240]   quote that's in the materials, unlike other pipelines, which
[00:57:29.240 --> 00:57:31.640]   can't truncate and split text that are longer than the
[00:57:31.640 --> 00:57:35.000]   maximum length accepted by the model, and thus may miss
[00:57:35.000 --> 00:57:37.400]   information at the end of the document, the question
[00:57:37.400 --> 00:57:41.400]   answering pipeline can deal with long context and return
[00:57:41.400 --> 00:57:46.280]   the answer to the question, even if it's at the end. And so
[00:57:46.280 --> 00:57:51.080]   this kind of comes up pretty frequently in conversations
[00:57:51.080 --> 00:57:55.480]   about blur and question answering as a whole is that
[00:57:55.480 --> 00:57:58.840]   how do you deal with long text, because typically we
[00:57:58.840 --> 00:58:03.800]   truncate things right to get whatever we can to fit on our
[00:58:03.800 --> 00:58:08.760]   GPU. And so with question answering, we can actually use
[00:58:08.760 --> 00:58:14.040]   this other parameter to our tokenizer called return
[00:58:14.040 --> 00:58:18.600]   overflowing tokens equals true. And we can do this when
[00:58:18.600 --> 00:58:21.560]   calling our tokenizer. And what this will essentially do is
[00:58:21.560 --> 00:58:26.120]   if our max length is 128, and we're dealing with an example,
[00:58:26.120 --> 00:58:32.040]   that's 1000 characters longer, whatever, it will actually
[00:58:32.040 --> 00:58:39.640]   chunk that into, you know, just 128 max token chunks. And so
[00:58:39.640 --> 00:58:42.520]   one example will actually be chunked and converted into
[00:58:42.520 --> 00:58:46.440]   multiple examples. And that's something that we can do when
[00:58:46.440 --> 00:58:51.160]   we're doing question answering. And that alleviates this idea
[00:58:51.160 --> 00:58:55.800]   of maybe missing answers that are at the end of long text
[00:58:55.800 --> 00:58:59.080]   that would otherwise be truncated. So we'll look at
[00:58:59.080 --> 00:59:00.840]   that. And that's particularly helpful when you're
[00:59:00.840 --> 00:59:03.800]   pre-processing. Because one of the things you want to do when
[00:59:03.800 --> 00:59:06.440]   you train your model, and you score it, and you're trying to
[00:59:06.440 --> 00:59:09.160]   figure out like, okay, we have this one example that's broken
[00:59:09.160 --> 00:59:14.520]   up into 10 chunks, which chunk has the best answer, you're
[00:59:14.520 --> 00:59:17.560]   going to want to be able to figure out like all those 10
[00:59:17.560 --> 00:59:21.800]   chunks are part of a single example. And so this really
[00:59:21.800 --> 00:59:25.160]   only makes sense when we're pre-processing our data, so
[00:59:25.160 --> 00:59:28.840]   that we can assign the unique ID associated to the example
[00:59:28.840 --> 00:59:33.240]   to all 10 of those chunks. And so basically perform that
[00:59:33.240 --> 00:59:36.280]   task. So that's something we could do with question
[00:59:36.280 --> 00:59:41.960]   answering. The other thing that's helpful is with fast
[00:59:41.960 --> 00:59:45.240]   tokenizers, which I'm annotating with this little
[00:59:45.240 --> 00:59:48.520]   asterisk, that's a fast tokenizer thing, we can access
[00:59:48.520 --> 00:59:53.320]   the sequence IDs for a particular example. Because
[00:59:53.320 --> 00:59:58.600]   when we're trying to figure out where the answer sits in our
[00:59:58.600 --> 01:00:03.720]   tokens, we don't want to have it look at the question. And so
[01:00:03.720 --> 01:00:06.600]   we can use sequence IDs, which is going to tell us, because
[01:00:06.600 --> 01:00:11.240]   we're passing in a question and a context, we can say mask
[01:00:11.240 --> 01:00:14.680]   all the tokens associated with the question part. So when
[01:00:14.680 --> 01:00:17.720]   you're looking for an answer, this is where you should be
[01:00:17.720 --> 01:00:22.680]   looking. And again, this return offsets mapping equals true,
[01:00:22.680 --> 01:00:27.720]   to be able to convert token positions into a span of text.
[01:00:27.720 --> 01:00:32.600]   So especially as we're going through example by example,
[01:00:32.600 --> 01:00:36.040]   whether we're using the overflow tokens and chunking
[01:00:36.040 --> 01:00:42.600]   things or just truncating, we want to be able to know what
[01:00:42.600 --> 01:00:48.760]   tokens are associated to what characters, because in most of
[01:00:48.760 --> 01:00:52.440]   our question answering data sets, what we're given is the
[01:00:52.440 --> 01:00:56.200]   index of the start character where the answer is, and then
[01:00:56.200 --> 01:00:59.240]   what the answer is to figure out the end character. So we
[01:00:59.240 --> 01:01:02.840]   have to be able to go back from raw character and indices
[01:01:02.840 --> 01:01:06.360]   back and forth to token indices. And so with return
[01:01:06.360 --> 01:01:11.720]   offset mapping equal true, we get that offset mapping key in
[01:01:11.720 --> 01:01:16.200]   our batch encoding object. We can use that to accomplish it.
[01:01:17.080 --> 01:01:28.040]   So let's take a look at what that would look like in terms
[01:01:28.040 --> 01:01:35.080]   of using blur. So for this particular task, I'm going to
[01:01:35.080 --> 01:01:41.400]   use version two of SQuAD, which is a really common data set.
[01:01:41.400 --> 01:01:44.600]   And there's some variations that are interesting that include
[01:01:44.600 --> 01:01:47.800]   like no answers. And I think this actually is the one that
[01:01:47.800 --> 01:01:50.360]   has like some that are answerable, some that aren't.
[01:01:50.360 --> 01:01:54.920]   So definitely this is kind of a go-to to kind of learning
[01:01:54.920 --> 01:01:57.880]   question answering. And you can see I'm going to use some of
[01:01:57.880 --> 01:02:01.560]   the fun data set stuff we learned last week. I'm going to
[01:02:01.560 --> 01:02:05.640]   convert it to the format underlying format to, or not the
[01:02:05.640 --> 01:02:09.480]   underlying format, but I'm going to have it essentially act as a
[01:02:09.480 --> 01:02:13.320]   pandas data frame so that we can actually look at this in a
[01:02:13.320 --> 01:02:17.880]   little bit nicer format. And as you can see, this is pretty
[01:02:17.880 --> 01:02:21.880]   common with most question answering data sets. We get a
[01:02:21.880 --> 01:02:28.040]   context, a question, and then an answer. And notice in this
[01:02:28.040 --> 01:02:32.360]   case, answers is actually a dictionary and it has a couple
[01:02:32.360 --> 01:02:35.720]   of things in it. It tells us what the answer is, right? In
[01:02:35.720 --> 01:02:43.400]   the late 1990s. And it also tells us the index of where
[01:02:43.400 --> 01:02:46.840]   that answer is, where the start character is in the raw
[01:02:46.840 --> 01:02:54.680]   text. So it's at position 269. So we're going to go ahead and
[01:02:54.680 --> 01:02:57.480]   use this particular format and I'll show you how to set up a
[01:02:57.480 --> 01:03:02.120]   data block for this. And notice if your data doesn't come in
[01:03:02.120 --> 01:03:05.800]   this format for answers or whatever, that's fine. This
[01:03:05.800 --> 01:03:08.280]   example will hopefully kind of give you some inspiration about
[01:03:08.280 --> 01:03:13.000]   how you can still get this data and put in a data block that
[01:03:13.000 --> 01:03:15.880]   will work the same way as it will for this particular
[01:03:15.880 --> 01:03:22.120]   example. So we're going to get our blur objects and notice
[01:03:22.120 --> 01:03:25.640]   that this time I'm going to set a max sequence length and I'm
[01:03:25.640 --> 01:03:30.840]   going to set it to 128 tokens. And this is because in question
[01:03:30.840 --> 01:03:37.240]   answering our model is going to try to predict the start token
[01:03:37.240 --> 01:03:41.720]   index and the end token index of the answer. And so if you
[01:03:41.720 --> 01:03:45.640]   think about this like from like a fast AI standpoint, we're
[01:03:45.640 --> 01:03:50.600]   actually trying to perform two categorical classification
[01:03:50.600 --> 01:03:54.680]   tasks, right? We're trying to predict the start and the end.
[01:03:55.880 --> 01:04:02.840]   And so we need to give the fast AI, we need to create multiple
[01:04:02.840 --> 01:04:07.160]   category blocks and the categories are going to
[01:04:07.160 --> 01:04:13.720]   essentially be one prediction per token position, right? So
[01:04:13.720 --> 01:04:16.840]   we're in token classification, we're looking at every token,
[01:04:16.840 --> 01:04:19.880]   every token we want to get a prediction. We'll mask out
[01:04:19.880 --> 01:04:23.240]   things like the class or padding tokens, things that we
[01:04:23.240 --> 01:04:27.080]   don't want to include, but at least when it comes to our raw
[01:04:27.080 --> 01:04:29.560]   data, this is how you want to think about it. We're trying to
[01:04:29.560 --> 01:04:35.800]   predict for every example, 128 predictions. And notice that we
[01:04:35.800 --> 01:04:40.520]   have two category blocks. This one right here is for the start
[01:04:40.520 --> 01:04:44.440]   position. This one right here is for the end position. And for
[01:04:44.440 --> 01:04:47.560]   every example, we're going to have 128 tokens and so we're
[01:04:47.560 --> 01:04:55.800]   going to have 128 predictions. Now, if you notice this block
[01:04:55.800 --> 01:05:00.280]   looks kind of interesting because I also include a
[01:05:00.280 --> 01:05:03.960]   none block in there. And the reason is, is that I'm going to
[01:05:03.960 --> 01:05:08.520]   structure things so that not only can we predict the start
[01:05:08.520 --> 01:05:13.080]   and end location, but we can actually have another target
[01:05:13.080 --> 01:05:17.080]   we're trying to predict, which is whether or not the answer
[01:05:17.080 --> 01:05:24.520]   exists in the example. So you don't have to do it this way,
[01:05:24.520 --> 01:05:28.680]   but I'm going to show you kind of a way to do that. And again,
[01:05:28.680 --> 01:05:33.400]   because we're doing things dynamically, I'm just going to
[01:05:33.400 --> 01:05:37.240]   set this to none. And then when we actually construct the data
[01:05:37.240 --> 01:05:42.280]   block, I'm going to actually create the value that we're
[01:05:42.280 --> 01:05:44.440]   going to use in this place, which is going to be a Boolean,
[01:05:44.440 --> 01:05:51.720]   right, true or false. And the way I'm going to do that is by
[01:05:51.720 --> 01:05:57.080]   using this method right here. And this method right here is
[01:05:57.080 --> 01:06:04.360]   actually going to return a tuple of three things. So first,
[01:06:04.360 --> 01:06:07.480]   this is kind of straightforward, right? We're passing in two
[01:06:07.480 --> 01:06:12.760]   things as our inputs, our question and our context. And
[01:06:12.760 --> 01:06:22.120]   then our target is going to be composed of three things. And
[01:06:22.120 --> 01:06:28.680]   this is actually going to-- we're going to change what these
[01:06:28.680 --> 01:06:33.080]   things are as we actually use our before batch transform for
[01:06:33.080 --> 01:06:35.640]   question answering. Because right now, since we're doing
[01:06:35.640 --> 01:06:37.720]   things on the fly, again, next week, we'll look at a
[01:06:37.720 --> 01:06:41.400]   pre-processed example that we don't have to figure this out.
[01:06:41.400 --> 01:06:44.600]   But notice that for the start and end, we don't know what
[01:06:44.600 --> 01:06:47.560]   those are yet, right? So when we're doing on the fly
[01:06:47.560 --> 01:06:50.440]   tokenization, those are things that we'll find out when we
[01:06:50.440 --> 01:06:55.080]   tokenize. If we were using a pre-processed text, we would
[01:06:55.080 --> 01:06:57.240]   know what these are. We wouldn't have to use these
[01:06:57.240 --> 01:07:04.200]   particular methods or attributes. But here, we're not
[01:07:04.200 --> 01:07:06.840]   going to actually know what the start and end is until we
[01:07:06.840 --> 01:07:12.520]   actually tokenize the data, which happens on the fly. So in
[01:07:12.520 --> 01:07:16.680]   Blur, I provide this get dummy token index that you can use.
[01:07:16.680 --> 01:07:20.840]   It basically is just going to return-- I forget if it's like
[01:07:20.840 --> 01:07:23.400]   one or zero, but we're going to replace it. And this is just
[01:07:23.400 --> 01:07:26.920]   to get the data block to work. And then notice that for the
[01:07:26.920 --> 01:07:31.000]   get y, this is actually going to return a tuple, which is
[01:07:31.000 --> 01:07:34.360]   going to have-- as we saw when we looked at the raw data,
[01:07:35.080 --> 01:07:38.120]   we're going to break this out and have the answer text, the
[01:07:38.120 --> 01:07:42.040]   start character index, and the end character index. And then
[01:07:42.040 --> 01:07:45.880]   we're actually going to use this within Blur to figure out
[01:07:45.880 --> 01:07:49.720]   exactly whether or not the answer exists and to also
[01:07:49.720 --> 01:07:54.120]   update the real values for our start token index that we want
[01:07:54.120 --> 01:07:58.360]   to predict and our end token index. And we'll look at Blur
[01:07:58.360 --> 01:08:01.880]   to show you how this works. So it looks a little bit more
[01:08:01.880 --> 01:08:06.440]   magical than it really is. The other thing that's key is that
[01:08:06.440 --> 01:08:11.080]   you need to pass this to your data block, which tells you how
[01:08:11.080 --> 01:08:16.680]   many inputs. And even though we have two inputs, these are
[01:08:16.680 --> 01:08:21.000]   actually just going to be one thing. And so when you
[01:08:21.000 --> 01:08:25.880]   tokenize things like this, you tokenize a question and
[01:08:25.880 --> 01:08:29.080]   context or sentence one and sentence two, it really
[01:08:29.080 --> 01:08:33.880]   returns a single set of input IDs for both of those. So we
[01:08:33.880 --> 01:08:38.120]   need to tell Blur that there's going to be one input, and
[01:08:38.120 --> 01:08:41.240]   then it will know that everything else is going to be a
[01:08:41.240 --> 01:08:46.360]   target that we want to predict. So once we do this, we can
[01:08:46.360 --> 01:08:53.000]   then call .dataloaders. We can look at our batches, and then
[01:08:53.000 --> 01:08:56.760]   we can actually do a show batch. And the way show batch
[01:08:56.760 --> 01:09:00.040]   works for question answering is that it's going to tell you
[01:09:00.040 --> 01:09:05.320]   whether it was found. It's going to tell you the start and
[01:09:05.320 --> 01:09:11.720]   end token indices and also what the answer is. So by show
[01:09:11.720 --> 01:09:13.640]   batch, we can go through and you can see that some of these,
[01:09:13.640 --> 01:09:19.480]   because again, I'm using on the fly batch time tokenization,
[01:09:19.480 --> 01:09:23.560]   I'm truncating to 128. And the reason some of these are
[01:09:23.560 --> 01:09:26.920]   false is because I actually truncated the text so that the
[01:09:26.920 --> 01:09:31.320]   answer no longer exists. And like I said, next week, we'll
[01:09:31.320 --> 01:09:37.080]   look at using that strategy of the overflow tokens to break up
[01:09:37.080 --> 01:09:40.520]   long documents. But if you don't want to do that, and you
[01:09:40.520 --> 01:09:43.640]   just want to use, you just want to truncate it, then this
[01:09:43.640 --> 01:09:47.160]   could potentially be a problem that you're going to find as
[01:09:47.160 --> 01:09:52.040]   you start training things. So anyways, yeah, that's how it
[01:09:52.040 --> 01:10:08.840]   works. And then if we look at the code here, and I'm going to
[01:10:08.840 --> 01:10:12.840]   go not look at the pre-processing at this point, but
[01:10:12.840 --> 01:10:17.240]   I want to look at the before batch transform. You can see,
[01:10:17.240 --> 01:10:21.080]   so remember that we're passing in those dummy values for a
[01:10:21.080 --> 01:10:24.520]   start end, and then we're passing in that tuple, right?
[01:10:24.520 --> 01:10:29.320]   That's really we want to use for the has an answer that
[01:10:29.320 --> 01:10:33.320]   includes the actual start character index and in character
[01:10:33.320 --> 01:10:36.440]   index. And you can see that if you're not using a pre
[01:10:36.440 --> 01:10:40.440]   tokenized data set, which means pre tokenized means we've
[01:10:40.440 --> 01:10:43.480]   already calculated what those are for our particular
[01:10:43.480 --> 01:10:49.240]   tokenizer that we're using. If we haven't, then we want to do
[01:10:49.240 --> 01:10:53.800]   that on the fly. And so you can see we're using the sequence
[01:10:53.800 --> 01:10:58.440]   IDs method, right, to create a mask. We want to mask the
[01:10:58.440 --> 01:11:02.120]   question mask so that that's not considered when we're
[01:11:02.120 --> 01:11:07.160]   figuring out whether or not our particular example has a, has
[01:11:07.160 --> 01:11:11.640]   the answer in it. And then we call this method, which is
[01:11:11.640 --> 01:11:16.760]   part of the Blurred library, which given a tokenizer,
[01:11:18.360 --> 01:11:25.480]   given your targets and given your input IDs, offset mapping
[01:11:25.480 --> 01:11:28.760]   and question mask for a particular example, can go
[01:11:28.760 --> 01:11:33.720]   ahead and find out, OK, what is the start token index that
[01:11:33.720 --> 01:11:37.720]   the answer is for the answer? What's the end token index?
[01:11:37.720 --> 01:11:41.720]   And of course, is there even an answer? And then, as I said,
[01:11:41.720 --> 01:11:44.200]   we're going to replace, remember, there's three targets
[01:11:44.200 --> 01:11:46.920]   that we create in our data block, and they're just dummy
[01:11:46.920 --> 01:11:49.720]   values, essentially, right now. We're actually going to
[01:11:49.720 --> 01:11:55.400]   update those to be the has answer, the start and the end
[01:11:55.400 --> 01:12:03.880]   token index. And if we go to find answer tokens, you can see
[01:12:03.880 --> 01:12:10.040]   how that process all works. And essentially what we're doing
[01:12:10.040 --> 01:12:13.400]   is we're using that offset mapping that we mentioned
[01:12:13.400 --> 01:12:15.880]   earlier, right, that allows us to go from characters to
[01:12:15.880 --> 01:12:20.920]   tokens back or from tokens to characters to actually figure
[01:12:20.920 --> 01:12:26.600]   out, OK, we have the character start and end indexes, but what
[01:12:26.600 --> 01:12:31.240]   are the token start and indexes? And we're essentially
[01:12:31.240 --> 01:12:36.280]   going to use that information to figure out where the answer
[01:12:36.280 --> 01:12:41.240]   is in reference to tokens rather than characters. If it
[01:12:41.240 --> 01:12:47.880]   can't find the answer, it just returns a tensor with the value
[01:12:47.880 --> 01:12:52.520]   zero for both the has answer, the start and the end location.
[01:12:52.520 --> 01:12:59.240]   So feel free to spend some time looking at this. I'll probably
[01:12:59.240 --> 01:13:01.880]   be adding some-- I had some tests originally. I think I
[01:13:01.880 --> 01:13:04.840]   might have taken them out or they're hidden somewhere in my
[01:13:04.840 --> 01:13:09.640]   blown up view of the screen, but I'll be adding some tasks to
[01:13:09.640 --> 01:13:12.120]   verify and so you can have confidence that this find
[01:13:12.120 --> 01:13:19.160]   answer token indexes works correctly. But that is
[01:13:19.160 --> 01:13:25.000]   basically it. Next week, we're going to look at, with regards
[01:13:25.000 --> 01:13:28.120]   to question answering, is actually using a preprocess
[01:13:28.120 --> 01:13:31.640]   version that has that chunking in there so that we can handle
[01:13:31.640 --> 01:13:35.080]   long documents. And you'll see that it makes the data blocks a
[01:13:35.080 --> 01:13:38.680]   little bit more easier to work with. And then we'll also kind
[01:13:38.680 --> 01:13:42.600]   of look at how to score these things correctly. So there's--
[01:13:42.600 --> 01:13:46.520]   when we're working with chunked examples, so examples that have
[01:13:46.520 --> 01:13:50.200]   been broken up into multiple examples, it changes how we need
[01:13:50.200 --> 01:13:55.320]   to process our validation set. So we will look at that next
[01:13:55.320 --> 01:14:01.880]   week. Any questions on question answering?
[01:14:01.880 --> 01:14:04.200]   >> No questions so far.
[01:14:04.200 --> 01:14:10.120]   >> All right. So for homework, go through part six. There's
[01:14:10.120 --> 01:14:13.480]   also a really good end of chapter quiz. It was like
[01:14:13.480 --> 01:14:17.080]   really-- we'll test your knowledge of different types of
[01:14:17.080 --> 01:14:21.720]   tokenizers and how they work and their pros and cons. So see
[01:14:21.720 --> 01:14:27.080]   if you guys can pass that. See if you can actually use some of
[01:14:27.080 --> 01:14:29.640]   the content that we talked about today to create a data block
[01:14:29.640 --> 01:14:32.600]   for a part of speech. It's a little bit different than name
[01:14:32.600 --> 01:14:36.680]   empty recognition. See if you can do this for attractive
[01:14:36.680 --> 01:14:40.680]   question answering. As Sanyam always says, teach others what
[01:14:40.680 --> 01:14:47.640]   you've learned, blog, Twitch, whatever. And as a bonus is go
[01:14:47.640 --> 01:14:50.440]   through the parts in chapter six and see if you can create your
[01:14:50.440 --> 01:14:55.640]   own word piece or unigram or BPE tokenizer, especially if you
[01:14:55.640 --> 01:14:59.080]   have a low resource language. But even if you want to just try
[01:14:59.080 --> 01:15:04.120]   to do it with an English corpus to get an idea of how folks
[01:15:04.120 --> 01:15:08.440]   have gone before us to actually create these checkpoints that
[01:15:08.440 --> 01:15:13.480]   we're all using for these other downstream tasks. And if you do
[01:15:13.480 --> 01:15:18.120]   that, I'd love to see what you've done. So let me know. And
[01:15:18.120 --> 01:15:23.160]   if you need help, give me a holler. And I think that's it
[01:15:23.160 --> 01:15:23.560]   for me.
[01:15:23.560 --> 01:15:28.040]   >> Awesome. It's really fascinating to see how much
[01:15:28.760 --> 01:15:31.960]   content you covered in just an hour. I know like this to study
[01:15:31.960 --> 01:15:35.240]   by yourself takes a lot of time. So I believe the audience has a
[01:15:35.240 --> 01:15:39.080]   lot homework to do like you have to go off and study this in your
[01:15:39.080 --> 01:15:41.720]   free time. This was quite quite dense material.
[01:15:41.720 --> 01:15:45.160]   >> Yeah, like even going through blur and stuff. It's hard to
[01:15:45.160 --> 01:15:49.960]   kind of I mean, it's been weeks and weeks of development just
[01:15:49.960 --> 01:15:52.760]   for those things that we just looked at, you know, real
[01:15:52.760 --> 01:15:57.480]   quickly. So yeah, look at the look at the source code. And I
[01:15:57.480 --> 01:15:59.560]   know something like that's always something that Jeremy's,
[01:15:59.560 --> 01:16:02.280]   you know, encouraged with fast AI. And the more you look at the
[01:16:02.280 --> 01:16:06.520]   source code, then the less foreign it looks and then the
[01:16:06.520 --> 01:16:11.720]   ideas just become, you know, more, more clear. And so go
[01:16:11.720 --> 01:16:15.720]   through that difficult task sometimes. And hopefully what I
[01:16:15.720 --> 01:16:17.160]   put up there isn't that difficult.
[01:16:17.160 --> 01:16:22.600]   >> Definitely. This one critical comment, I believe they're
[01:16:22.600 --> 01:16:26.680]   trying to understand why are we using blur and not pure fast AI.
[01:16:26.680 --> 01:16:29.800]   My understanding is there's a few things that you can't do in
[01:16:29.800 --> 01:16:32.520]   fast AI, which is why you need hugging face and then blur is
[01:16:32.520 --> 01:16:35.720]   this nice bridge between both of the worlds. And there are some
[01:16:35.720 --> 01:16:38.520]   opinions from Wade also that make their way to there like you
[01:16:38.520 --> 01:16:41.240]   were looking at the different classes that Wade was walking us
[01:16:41.240 --> 01:16:45.800]   through. There was hinting that we don't see I think that in the
[01:16:45.800 --> 01:16:47.880]   hugging face source code, it might have changed since the
[01:16:47.880 --> 01:16:51.480]   last time I saw it. But the question is, why are we using
[01:16:51.480 --> 01:16:54.840]   blur or what advantages there for using blur?
[01:16:54.840 --> 01:16:59.880]   >> Yeah. So if you want to just use fast AI, a lot of the stuff
[01:16:59.880 --> 01:17:02.440]   that you'll see in blur are things that you're going to have
[01:17:02.440 --> 01:17:09.720]   to code yourself. And some of the reasons why fast AI out of
[01:17:09.720 --> 01:17:14.360]   the box just can't be used with hugging face is because of how
[01:17:14.360 --> 01:17:18.440]   tokenization is done. So in fast AI, you have like ULM fit,
[01:17:19.320 --> 01:17:26.200]   which is a traditional kind of language model. And it's
[01:17:26.200 --> 01:17:29.400]   entirely different architecture than the transformers. And then
[01:17:29.400 --> 01:17:37.320]   the way that the transformers, the models actually, the inputs
[01:17:37.320 --> 01:17:40.280]   are different than what you see in fast AI. So in fast AI, it's
[01:17:40.280 --> 01:17:45.240]   very position based. And you pass in tensors, whereas with
[01:17:45.240 --> 01:17:49.880]   transformers, you pass in dictionaries with the different
[01:17:49.880 --> 01:17:53.000]   things like the input IDs, the attention mask, and so forth.
[01:17:53.000 --> 01:17:58.920]   And then also when you use a hugging face transformer model,
[01:17:58.920 --> 01:18:03.320]   the outputs are different than you would in fast AI. And so
[01:18:03.320 --> 01:18:07.960]   like, they actually have like a model outputs object. And so
[01:18:07.960 --> 01:18:10.920]   for token classification, there's actually a start logits
[01:18:10.920 --> 01:18:15.080]   and end logits that you can look at. Whereas with fast AI,
[01:18:15.080 --> 01:18:18.120]   you're typically getting things just very positional. So you're
[01:18:18.120 --> 01:18:21.240]   just getting a value, but you don't know what it relates to.
[01:18:21.240 --> 01:18:27.960]   And also, some of the other work is just in terms of encoding
[01:18:27.960 --> 01:18:33.720]   and decoding different types of text. So like, if you look at
[01:18:33.720 --> 01:18:36.520]   blur, you'll see that a lot of work's gone on the show batch,
[01:18:36.520 --> 01:18:39.400]   like, how do you show predictions? How do you show
[01:18:39.400 --> 01:18:43.320]   batches or show results? And it varies per task. So sequence
[01:18:43.320 --> 01:18:46.280]   classification looks different than token classification, then
[01:18:46.280 --> 01:18:49.800]   question answering. And so those are some of the things that if
[01:18:49.800 --> 01:18:51.960]   you wanted to have this ability, you'd have to code them
[01:18:51.960 --> 01:18:56.360]   yourself. So there's nothing if you look at the blur code, it's
[01:18:56.360 --> 01:19:00.440]   really I'm using these fast AI concepts, callbacks all over
[01:19:00.440 --> 01:19:03.800]   the place and transform. But if you don't want to use blur, you
[01:19:03.800 --> 01:19:06.920]   would have to essentially create these statements, you would
[01:19:06.920 --> 01:19:09.480]   have to write blur again by yourself. Yeah, yeah, you just
[01:19:09.480 --> 01:19:12.440]   write another version of blur, you know, call it something
[01:19:12.440 --> 01:19:16.760]   different. You know, fast blur or whatever, I don't know. But,
[01:19:16.760 --> 01:19:21.880]   but yeah, so those are some of the main reasons why blur has to
[01:19:21.880 --> 01:19:24.600]   sit in there, at least with version two of fast AI.
[01:19:24.600 --> 01:19:28.840]   That makes sense. I hope that answered the question for I
[01:19:28.840 --> 01:19:32.040]   think it's an anonymous user, because they're using a pseudonym.
[01:19:34.440 --> 01:19:38.200]   Yeah, yeah. Nope, no problem. If you have other questions. Yeah,
[01:19:38.200 --> 01:19:41.080]   yeah. Like I said, do you have my my Twitter handle over there?
[01:19:41.080 --> 01:19:44.840]   Send me a DM or shoot me an email if you go to omeow.com.
[01:19:44.840 --> 01:19:47.320]   You can find all my other contact information.
[01:19:47.320 --> 01:19:51.640]   Amazing. Thanks again for your time, Wade. I believe we're
[01:19:51.640 --> 01:19:54.840]   still meeting next week, but the week after we'll be taking a
[01:19:54.840 --> 01:19:57.000]   break since Wade will be on vacation.
[01:19:57.000 --> 01:20:02.600]   I think we're doing two weeks. It's the 13th that we'll be
[01:20:02.600 --> 01:20:06.920]   taking a break. So we're still meeting next week, I think.
[01:20:06.920 --> 01:20:09.000]   Yeah, next week we're on we're gonna finish up tok
[01:20:09.000 --> 01:20:12.680]   classification and and get into training and inference and then
[01:20:12.680 --> 01:20:14.520]   the following week question answering.
[01:20:14.520 --> 01:20:18.840]   Amazing. So we're still meeting next week. And then we'll have a
[01:20:18.840 --> 01:20:23.000]   two week break. I'll request to have enough homework to keep us
[01:20:23.000 --> 01:20:23.560]   busy then.
[01:20:23.560 --> 01:20:25.880]   Oh, yeah, no problem. You got it.
[01:20:25.880 --> 01:20:29.560]   Thanks. Thanks again for your time, Wade. And I look forward
[01:20:29.560 --> 01:20:31.480]   to learning from you again next week.
[01:20:31.480 --> 01:20:34.200]   All right, good. Good. Good being here. Thanks again, Sanyam.
[01:20:34.200 --> 01:20:39.720]   Thanks for joining everyone. We'll see you next week.

