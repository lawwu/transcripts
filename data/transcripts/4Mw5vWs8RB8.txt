
[00:00:00.000 --> 00:00:06.680]   The next thing I want to discuss is error analysis.
[00:00:06.680 --> 00:00:11.600]   And error analysis is one of the most critical parts of model evaluation.
[00:00:11.600 --> 00:00:19.060]   It's really the opportunity to improve your models and gain deep insights into where your
[00:00:19.060 --> 00:00:24.560]   model could be wrong or where it's struggling or where you can improve your model.
[00:00:24.560 --> 00:00:31.200]   And honestly, this is where I personally have the most fun in the entire machine learning
[00:00:31.200 --> 00:00:38.440]   process because this is where I get to build more intuition about the domain and how my
[00:00:38.440 --> 00:00:41.480]   model is interacting with that domain.
[00:00:41.480 --> 00:00:47.040]   So what is error analysis and how does weights and biases fit into that?
[00:00:47.040 --> 00:00:53.280]   So the way error analysis works is we're going to log a table to two weights and biases.
[00:00:53.280 --> 00:00:57.600]   You may have already seen these tables in previous lessons, but essentially it's tables
[00:00:57.600 --> 00:01:04.680]   with predictions and ground truth and comparing those ground truths to predictions with all
[00:01:04.680 --> 00:01:05.680]   the various metrics.
[00:01:05.680 --> 00:01:09.880]   And what we're going to do is we're going to look at these images and we're going to
[00:01:09.880 --> 00:01:14.520]   see where the model is most wrong, for example, low IOU.
[00:01:14.520 --> 00:01:20.960]   And the goal is to try to gain an intuition about where the model is struggling, about
[00:01:20.960 --> 00:01:29.040]   what issues might be happening in terms of the data, the labels, or so on and so forth.
[00:01:29.040 --> 00:01:34.480]   And it always surprises me how much I learn when I look at the data like this.
[00:01:34.480 --> 00:01:35.920]   And it is super important.
[00:01:35.920 --> 00:01:44.240]   No matter how much EDA that you do, this kind of guided approach to looking at data is extremely
[00:01:44.240 --> 00:01:45.480]   helpful.
[00:01:45.480 --> 00:01:51.360]   And if you go through this error analysis, you might notice a lot of things that might
[00:01:51.360 --> 00:01:59.040]   be relevant, such as poor lighting, obstruction of certain objects, so on and so forth.
[00:01:59.040 --> 00:02:03.920]   Especially with these perception tasks where humans are quite good at perception tasks
[00:02:03.920 --> 00:02:07.920]   and we're good at intuiting where issues may be occurring.
[00:02:07.920 --> 00:02:10.320]   And I'll show you some examples.
[00:02:10.320 --> 00:02:18.040]   And a key sort of output of error analysis is not only this sort of these categories.
[00:02:18.040 --> 00:02:24.160]   So when you're going through error analysis, you want to keep tabs of issues that you see
[00:02:24.160 --> 00:02:25.520]   and try to categorize them.
[00:02:25.520 --> 00:02:30.960]   You want to look at around a hundred or so examples at least and categorize any issues
[00:02:30.960 --> 00:02:32.280]   that you see.
[00:02:32.280 --> 00:02:38.920]   And what you'll often see is things like incorrect labels, even in your training set.
[00:02:38.920 --> 00:02:42.360]   And I always find issues like these.
[00:02:42.360 --> 00:02:47.920]   And so I think error analysis is a really key part of model evaluation.
[00:02:47.920 --> 00:02:55.360]   What you have here is a table of images and all of the IOU scores that I might care about.
[00:02:55.360 --> 00:02:58.440]   And this is for the validation set.
[00:02:58.440 --> 00:03:05.020]   So when I'm going through this model, I want to, or evaluating this model, my goal is to
[00:03:05.020 --> 00:03:11.000]   look at the performance of the model on the validation set, but specifically for specific
[00:03:11.000 --> 00:03:15.640]   images to get an idea of where the model might be struggling.
[00:03:15.640 --> 00:03:22.660]   And the way you do that is to go through each metric that you care about and sort of sort
[00:03:22.660 --> 00:03:31.400]   the different data points by poorly performing, their metrics are not great.
[00:03:31.400 --> 00:03:33.600]   And so let me do an example of that.
[00:03:33.600 --> 00:03:39.400]   So I'm going to take this road IOU and I'm going to sort it in ascending order so that
[00:03:39.400 --> 00:03:42.320]   the lowest IOUs are at the top.
[00:03:42.320 --> 00:03:45.120]   And then we can go and look at some of these images.
[00:03:45.120 --> 00:03:46.720]   So let's look at this image.
[00:03:46.720 --> 00:03:48.760]   Okay, so this image is interesting.
[00:03:48.760 --> 00:03:52.600]   I don't even see a road or anything like that.
[00:03:52.600 --> 00:03:58.600]   So what you see here is the image followed by the ground truth mask and then the predictions
[00:03:58.600 --> 00:03:59.600]   mask.
[00:03:59.600 --> 00:04:06.040]   So to keep track of which one is which, you can always hide it here like this.
[00:04:06.040 --> 00:04:08.880]   So this is the ground truth.
[00:04:08.880 --> 00:04:12.000]   If I hide it, you see that's the middle one.
[00:04:12.000 --> 00:04:13.720]   And you can arrange it in many ways.
[00:04:13.720 --> 00:04:16.480]   So you could stack them on top of each other.
[00:04:16.480 --> 00:04:18.660]   I don't tend to like that as much.
[00:04:18.660 --> 00:04:19.660]   Sometimes it's helpful.
[00:04:19.660 --> 00:04:22.520]   You can see this followed by both ground truths.
[00:04:22.520 --> 00:04:25.680]   These are the ground truth and the prediction stacked on top of each other.
[00:04:25.680 --> 00:04:27.120]   I like this view.
[00:04:27.120 --> 00:04:31.960]   I like to start with this view to see the predictions and the ground truth sort of segmented
[00:04:31.960 --> 00:04:34.200]   out.
[00:04:34.200 --> 00:04:37.960]   And it's interesting that, okay, so I can see in the predictions.
[00:04:37.960 --> 00:04:39.440]   So let me just look at the predictions.
[00:04:39.440 --> 00:04:42.880]   It is predicting road here.
[00:04:42.880 --> 00:04:46.960]   I'm not sure there's anything particularly that interesting about this picture to be
[00:04:46.960 --> 00:04:48.600]   honest.
[00:04:48.600 --> 00:04:52.720]   So I'm not really that...
[00:04:52.720 --> 00:04:55.640]   Yeah, this is not that interesting.
[00:04:55.640 --> 00:04:59.160]   I just want to give you sort of an idea of how to look at it.
[00:04:59.160 --> 00:05:03.240]   I think these next ones might be quite interesting.
[00:05:03.240 --> 00:05:07.320]   So let's take a look at this one here.
[00:05:07.320 --> 00:05:09.060]   Okay.
[00:05:09.060 --> 00:05:10.160]   And let's unmask that.
[00:05:10.160 --> 00:05:11.560]   So this is really interesting.
[00:05:11.560 --> 00:05:12.560]   We have a road.
[00:05:12.560 --> 00:05:16.840]   It's clearly a road in front of us.
[00:05:16.840 --> 00:05:21.480]   And I can see that there's no road being classified.
[00:05:21.480 --> 00:05:24.760]   So there is no road in the ground truth.
[00:05:24.760 --> 00:05:26.200]   So let's look at that.
[00:05:26.200 --> 00:05:33.640]   So what I can do is if I click road here, well, you can see like this orange area is
[00:05:33.640 --> 00:05:39.320]   vehicles and you can expand this to see, okay, orange area is vehicles.
[00:05:39.320 --> 00:05:41.880]   Road is not even in the ground truth.
[00:05:41.880 --> 00:05:44.480]   There is clearly road.
[00:05:44.480 --> 00:05:48.600]   And this model is trying to predict road.
[00:05:48.600 --> 00:05:49.600]   And let's take a look.
[00:05:49.600 --> 00:05:51.560]   Let's stack it on top of each other.
[00:05:51.560 --> 00:05:53.640]   Let's even take this off.
[00:05:53.640 --> 00:05:59.360]   The model is doing a pretty reasonable job at the road part of it.
[00:05:59.360 --> 00:06:01.040]   See these are the predictions.
[00:06:01.040 --> 00:06:05.720]   I can see none to unselect everything and just say road.
[00:06:05.720 --> 00:06:06.720]   And that's pretty reasonable.
[00:06:06.720 --> 00:06:07.760]   There's this blip up here.
[00:06:07.760 --> 00:06:09.960]   So what is the takeaway here?
[00:06:09.960 --> 00:06:15.640]   The takeaway from this specific example is the labels are wrong in our ground truth,
[00:06:15.640 --> 00:06:22.840]   clearly, because we can, if I do this and select road, they don't have road.
[00:06:22.840 --> 00:06:29.960]   So we need to go back to our labeling process and potentially figure out what's going on,
[00:06:29.960 --> 00:06:32.360]   why roads are not being labeled appropriately.
[00:06:32.360 --> 00:06:35.040]   Okay, let's look at another one.
[00:06:35.040 --> 00:06:38.560]   For example, let's see if there's anything interesting in this one.
[00:06:38.560 --> 00:06:40.640]   Oh, this is interesting.
[00:06:40.640 --> 00:06:42.120]   This is a parking lot.
[00:06:42.120 --> 00:06:44.320]   Okay, that's really interesting.
[00:06:44.320 --> 00:06:52.200]   So our model is predicting the entire parking lot as being a road, essentially.
[00:06:52.200 --> 00:06:55.160]   And the ground truth seems to be correct.
[00:06:55.160 --> 00:07:00.080]   The ground truth is just showing vehicles and showing the background.
[00:07:00.080 --> 00:07:02.760]   However, you will notice the ground, we missed something here.
[00:07:02.760 --> 00:07:08.280]   I mean, this camper van is not a vehicle and that is wrong.
[00:07:08.280 --> 00:07:14.280]   Let's see what our model is predicting for vehicles just while we're here, just for fun.
[00:07:14.280 --> 00:07:17.600]   And we'll see your model, it does identify that camper van.
[00:07:17.600 --> 00:07:24.760]   So again, we have some labeling issues potentially where we're not labeling things correctly.
[00:07:24.760 --> 00:07:26.560]   And that is causing error in this issue.
[00:07:26.560 --> 00:07:30.040]   So we might want to look at parking lots.
[00:07:30.040 --> 00:07:37.000]   And if I find several examples of parking lots, then I might say, okay, I want to acquire
[00:07:37.000 --> 00:07:43.360]   a dataset that has more parking lots or make sure that I try to find more examples of parking
[00:07:43.360 --> 00:07:45.640]   lots for my dataset and differentiate those.
[00:07:45.640 --> 00:07:46.640]   Okay.
[00:07:46.640 --> 00:07:46.640]   Thank you.
[00:07:46.640 --> 00:07:51.640]   [end]
[00:07:51.640 --> 00:07:52.640]   1
[00:07:52.640 --> 00:08:02.640]   [BLANK_AUDIO]

