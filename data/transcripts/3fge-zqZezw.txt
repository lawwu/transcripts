
[00:00:00.000 --> 00:00:04.680]   Testing, testing, okay, nice.
[00:00:04.680 --> 00:00:07.260]   Thank you for having me, Weights and Biases.
[00:00:07.260 --> 00:00:09.440]   Thank you everyone for joining.
[00:00:09.440 --> 00:00:11.240]   Really excited to be here.
[00:00:11.240 --> 00:00:12.640]   My name's Harrison Chase.
[00:00:12.640 --> 00:00:16.080]   I'm CEO and co-founder at LangeChain.
[00:00:16.080 --> 00:00:20.600]   LangeChain's developer tooling for building LLM applications.
[00:00:20.600 --> 00:00:26.600]   So we have a bunch of abstractions and chains and make it really easy to get started.
[00:00:26.600 --> 00:00:33.120]   And when I was thinking about what to speak about today, you know, there's a lot in LangeChain
[00:00:33.120 --> 00:00:35.000]   and so it was a bit hard to choose.
[00:00:35.000 --> 00:00:39.440]   But I think the thing that's most interesting to me at the moment and I think the most unexplored
[00:00:39.440 --> 00:00:46.120]   in the ecosystem is generally memory, the concept of memory for LLM applications.
[00:00:46.120 --> 00:00:48.320]   So that's what I'll be talking about today.
[00:00:48.320 --> 00:00:53.080]   I don't -- it's going to be a bit of an unsatisfying talk because I don't really have -- I have
[00:00:53.080 --> 00:00:57.600]   more questions than answers really and I'm not really trying to sell anything.
[00:00:57.600 --> 00:01:02.240]   I think my goal here is to introduce a bit of the problem, talk about how we've been
[00:01:02.240 --> 00:01:07.200]   thinking about it at LangeChain, talk about the generative agents paper, which was an
[00:01:07.200 --> 00:01:12.400]   amazing paper that came out about a month or so ago out of Stanford and I think is the
[00:01:12.400 --> 00:01:15.360]   most interesting paper in the memory space.
[00:01:15.360 --> 00:01:18.320]   And then just leave you with some closing thoughts.
[00:01:18.320 --> 00:01:22.760]   And I think this is a space where we'll see a lot more attention in the next few months.
[00:01:22.760 --> 00:01:27.240]   And so I'm really excited to have the chance to just talk about it and nerd out about it
[00:01:27.240 --> 00:01:29.480]   with you guys today.
[00:01:29.480 --> 00:01:33.920]   So the general problem is that LLM APIs are stateless.
[00:01:33.920 --> 00:01:38.600]   You know, when you pass a call to OpenAI at the moment and then you pass another one,
[00:01:38.600 --> 00:01:42.600]   it doesn't remember what came before.
[00:01:42.600 --> 00:01:47.120]   And on the other hand, a lot of the applications that we build are not.
[00:01:47.120 --> 00:01:52.160]   So I think, you know, looking at examples of this, chat bots is an immediate example.
[00:01:52.160 --> 00:01:57.200]   You want the chat bot to remember what came before in the conversation.
[00:01:57.200 --> 00:02:01.600]   More complex forms of chat bots can include things like personal tutors.
[00:02:01.600 --> 00:02:06.440]   We did a webinar on LangeChain and education last week.
[00:02:06.440 --> 00:02:11.320]   And I think a lot of the things that people were building were personalized tutors, which
[00:02:11.320 --> 00:02:15.520]   I think is an amazing application of this generative technology.
[00:02:15.520 --> 00:02:16.680]   And I added this third one in here.
[00:02:16.680 --> 00:02:20.480]   I always get asked, like, you know, what's the what's your favorite application that
[00:02:20.480 --> 00:02:21.480]   you see being built on LangeChain?
[00:02:21.480 --> 00:02:27.040]   And I think one of the most creative ones is a dungeon master to play Dungeons and Dragons.
[00:02:27.040 --> 00:02:31.000]   And I think on one hand, I like that because it's really creative.
[00:02:31.000 --> 00:02:36.360]   The other reason I like it is I think it's an amazing example of why memory and memory
[00:02:36.360 --> 00:02:39.680]   is a broad term and I'll talk about a bit more specifics later, but why memory in general
[00:02:39.680 --> 00:02:43.760]   is really important for a lot of these new applications.
[00:02:43.760 --> 00:02:47.260]   You know, as a dungeon master, you have a lot going on that you need to remember and
[00:02:47.260 --> 00:02:50.320]   come back to and make sure is internally consistent.
[00:02:50.320 --> 00:02:54.160]   And so I think it's a fun application of that.
[00:02:54.160 --> 00:02:57.600]   The memory modules we have in LangeChain are mostly aimed at conversation.
[00:02:57.600 --> 00:03:02.120]   I think that's the place where memory is most readily obvious.
[00:03:02.120 --> 00:03:06.840]   If you're having a conversation, the thing that you're chatting with, better remember
[00:03:06.840 --> 00:03:09.480]   what you said before.
[00:03:09.480 --> 00:03:10.480]   It's been around for a while.
[00:03:10.480 --> 00:03:15.720]   I mean, I think chat bots jumped to everyone's mind when generative AI came out.
[00:03:15.720 --> 00:03:18.320]   And so we've supported this for a while.
[00:03:18.320 --> 00:03:22.280]   The other thing I'll say, though, is memory doesn't just apply to conversation.
[00:03:22.280 --> 00:03:27.880]   At the end of the day, when I talk about memory in LLM applications, what I really mean is
[00:03:27.880 --> 00:03:33.160]   remembering previous interactions and then using those to inform future interactions.
[00:03:33.160 --> 00:03:39.400]   So I think that can come into play in other settings besides chat bots.
[00:03:39.400 --> 00:03:47.400]   A obvious example being if you've got an agent that's doing a task, you know, if it remembers
[00:03:47.400 --> 00:03:55.240]   how the task went before, what adjustments you wanted to make to the course that it took.
[00:03:55.240 --> 00:03:57.480]   Remembering these things can help guide it.
[00:03:57.480 --> 00:04:01.080]   And this ties in nicely to the concept of reflection, which I'll talk about in a bit.
[00:04:01.080 --> 00:04:05.080]   But I just want to highlight that although a lot of the modules we have in LangeChain
[00:04:05.080 --> 00:04:11.240]   are really focused on conversation, that's not the only application where memory is important.
[00:04:11.240 --> 00:04:15.960]   So I'll skim quickly through some of the memory modules that we have in LangeChain to kind
[00:04:15.960 --> 00:04:20.800]   of talk about how we currently think about it and how we have been thinking about it.
[00:04:20.800 --> 00:04:25.200]   So probably the simplest form of memory is what we call conversation buffer memory.
[00:04:25.200 --> 00:04:30.560]   And this basically just keeps around a buffer of the previous memory or the previous chats
[00:04:30.560 --> 00:04:36.040]   and passes them back into the language model so that it has as context what was said previously.
[00:04:36.040 --> 00:04:40.440]   There's different ways to select the number of messages that you pass back in.
[00:04:40.440 --> 00:04:43.280]   Obviously the more you include, the more context it has.
[00:04:43.280 --> 00:04:47.720]   At the same time, it's more costly.
[00:04:47.720 --> 00:04:52.160]   And at some point you'll run past the context window length.
[00:04:52.160 --> 00:04:53.160]   It's pretty simple.
[00:04:53.160 --> 00:04:54.160]   It's pretty easy to understand.
[00:04:54.160 --> 00:04:56.800]   You can kind of see what's going in, see what's coming out.
[00:04:56.800 --> 00:05:02.040]   The cons are, you know, you can't -- you currently can't pass in everything.
[00:05:02.040 --> 00:05:08.780]   And even if you could, that's not incredibly efficient for making a lot of generalizations.
[00:05:08.780 --> 00:05:13.120]   The next kind of version of memory that we have is what we call conversation summary
[00:05:13.120 --> 00:05:14.120]   memory.
[00:05:14.120 --> 00:05:17.520]   And this just basically keeps a rolling summary of the conversation.
[00:05:17.520 --> 00:05:23.080]   So every interaction, it will update the summary and then pass that back in.
[00:05:23.080 --> 00:05:26.440]   Again, the pros of this are it's pretty simple.
[00:05:26.440 --> 00:05:27.440]   It's easy to understand.
[00:05:27.440 --> 00:05:30.500]   You can easily introspect and see what's going on.
[00:05:30.500 --> 00:05:32.340]   It's also reasonably configurable.
[00:05:32.340 --> 00:05:35.600]   So right now we run it every chat message.
[00:05:35.600 --> 00:05:40.660]   But you could imagine a scenario where you run it every two or three or five or K chat
[00:05:40.660 --> 00:05:43.960]   messages and then update the summary accordingly.
[00:05:43.960 --> 00:05:47.980]   The cons, again, are it loses some of the detailed information.
[00:05:47.980 --> 00:05:48.980]   So you have a summary.
[00:05:48.980 --> 00:05:52.300]   You don't have the exact specifics of the messages.
[00:05:52.300 --> 00:05:56.120]   And it's really mostly only relevant in conversation settings.
[00:05:56.120 --> 00:05:59.900]   A simple kind of like combination of these two is also really efficient.
[00:05:59.900 --> 00:06:02.160]   So you get the best of both in some sense.
[00:06:02.160 --> 00:06:07.100]   You get the specificity of the previous messages and then you get a summary of the longer conversational
[00:06:07.100 --> 00:06:08.540]   context before.
[00:06:08.540 --> 00:06:11.140]   And so we support that as well.
[00:06:11.140 --> 00:06:14.700]   These are some of the more basic types of memory that we have in Linkchain.
[00:06:14.700 --> 00:06:20.300]   The next two that we have are a bit more advanced, a bit more complicated and touch on this concept
[00:06:20.300 --> 00:06:23.740]   of reflection and updating some sort of state.
[00:06:23.740 --> 00:06:27.860]   So the first that we have is conversation entity memory.
[00:06:27.860 --> 00:06:33.820]   And what this does is it extracts knowledge about specific entities and then updates a
[00:06:33.820 --> 00:06:36.620]   knowledge store of those entities.
[00:06:36.620 --> 00:06:41.820]   This was added by Sam Whitmore, who was an early contributor to Linkchain and an amazing
[00:06:41.820 --> 00:06:44.260]   follow on Twitter.
[00:06:44.260 --> 00:06:48.780]   And so the generalization of this is the state that's being kept is this entity store, the
[00:06:48.780 --> 00:06:52.740]   mapping from an entity name to some information about that entity.
[00:06:52.740 --> 00:06:58.940]   And then basically after a conversation, you can update that by essentially asking the
[00:06:58.940 --> 00:07:01.940]   language model, what do we now know about person X?
[00:07:01.940 --> 00:07:07.600]   And then adding that in to the description of person X that we have in the knowledge
[00:07:07.600 --> 00:07:08.980]   base.
[00:07:08.980 --> 00:07:15.820]   And then the way that gets used in the prompt is when you're chatting with this system,
[00:07:15.820 --> 00:07:20.180]   before the language model responds, it will parse out who the entities are, look them
[00:07:20.180 --> 00:07:23.020]   up in the knowledge base and then pass that in as context.
[00:07:23.020 --> 00:07:30.460]   So the state here is basically this mapping of entity to entity summary.
[00:07:30.460 --> 00:07:35.020]   A slightly broader generalization of that is the idea of using a knowledge graph to
[00:07:35.020 --> 00:07:37.720]   represent various things.
[00:07:37.720 --> 00:07:41.180]   And so again, there's this concept of you've got this state.
[00:07:41.180 --> 00:07:45.220]   In this case, the state is a full fledged knowledge graph.
[00:07:45.220 --> 00:07:46.820]   And you update the state every turn.
[00:07:46.820 --> 00:07:51.540]   So you extract new knowledge triplets every turn and then insert them into the knowledge
[00:07:51.540 --> 00:07:52.540]   graph.
[00:07:52.540 --> 00:07:57.820]   And then again, you use the state in the conversation by querying the knowledge graph for relevant
[00:07:57.820 --> 00:07:58.820]   triplets.
[00:07:58.820 --> 00:08:00.700]   And that's a bit context specific.
[00:08:00.700 --> 00:08:05.860]   But the idea is again, there's this generalization of you've got this state, you're updating
[00:08:05.860 --> 00:08:10.300]   it every, again, here it's every conversation, but you can imagine that being every five
[00:08:10.300 --> 00:08:13.500]   messages, every 10 messages, kind of at some cadence.
[00:08:13.500 --> 00:08:16.020]   And then there's this way of incorporating the state back in.
[00:08:16.020 --> 00:08:20.300]   It's just the state is taking different forms.
[00:08:20.300 --> 00:08:21.860]   That's what we've got in link chain.
[00:08:21.860 --> 00:08:26.380]   We've also recently added some stuff from this generative agents paper, which is an
[00:08:26.380 --> 00:08:28.620]   amazing paper that came out of Stanford.
[00:08:28.620 --> 00:08:30.780]   It's got a lot of really cool things.
[00:08:30.780 --> 00:08:35.220]   And I think the two main things that it has that are interesting, one is it has a simulation
[00:08:35.220 --> 00:08:36.220]   aspect.
[00:08:36.220 --> 00:08:40.500]   So if you haven't read this paper, what it does is it creates 25 different agents and
[00:08:40.500 --> 00:08:45.620]   puts them in a simulation, a Sims like simulation, where they're going about their day.
[00:08:45.620 --> 00:08:50.300]   There's this complex setup of the code for the simulation has not been released.
[00:08:50.300 --> 00:08:51.620]   We did not try to replicate that.
[00:08:51.620 --> 00:08:54.180]   That would have been way too intense for us.
[00:08:54.180 --> 00:08:59.940]   But it's this really intense setup where you've got 25 individual agents all with their own
[00:08:59.940 --> 00:09:03.780]   memory and agenda, and they're interacting in this environment.
[00:09:03.780 --> 00:09:06.380]   So the simulation is one of the parts of this paper.
[00:09:06.380 --> 00:09:10.500]   But the part of this paper that I'm also really more interested in is the memory that they
[00:09:10.500 --> 00:09:12.760]   use for these agents.
[00:09:12.760 --> 00:09:19.620]   So they constructed a type of memory that depended really heavily on reflection.
[00:09:19.620 --> 00:09:24.380]   And so reflection can take a bunch of different forms.
[00:09:24.380 --> 00:09:28.340]   And arguably some of the, or not arguably, some of the things I mentioned earlier around
[00:09:28.340 --> 00:09:32.300]   entity memory, knowledge graph memory, summary memory, all these involve reflection.
[00:09:32.300 --> 00:09:37.220]   They involve reflecting on the previous messages and then updating some state.
[00:09:37.220 --> 00:09:38.820]   And so this paper did that as well.
[00:09:38.820 --> 00:09:41.100]   They did it in a few different ways.
[00:09:41.100 --> 00:09:43.820]   First they reflected on the importance of a memory.
[00:09:43.820 --> 00:09:48.900]   And so when observation happened, they gave it a score of how important it was.
[00:09:48.900 --> 00:09:54.980]   And this was used later on when retrieving from the database and putting it back into
[00:09:54.980 --> 00:09:55.980]   context.
[00:09:55.980 --> 00:09:58.260]   And so that was the first form of reflection they did.
[00:09:58.260 --> 00:10:02.140]   Another form of reflection they did, which was I think even more interesting, is they
[00:10:02.140 --> 00:10:09.940]   added, they added, so you have these concepts of observations, and those form individual
[00:10:09.940 --> 00:10:10.940]   memories.
[00:10:10.940 --> 00:10:15.540]   And then they inserted another type of memory, which wasn't a specific observation, but was
[00:10:15.540 --> 00:10:19.060]   basically generalizations of many observations.
[00:10:19.060 --> 00:10:23.380]   So after N observations that asked the language model to come up with a list of questions
[00:10:23.380 --> 00:10:27.700]   that you could ask based on those observations, basically a list of insights that you could
[00:10:27.700 --> 00:10:32.140]   gather, then for each insight they'd go over that, they'd create that insight, and then
[00:10:32.140 --> 00:10:35.180]   they'd insert that back into the database.
[00:10:35.180 --> 00:10:39.060]   So you're now augmenting your database with a bunch of these reflections.
[00:10:39.060 --> 00:10:43.700]   The reason I think this is so interesting, and I'll talk about this in the next section
[00:10:43.700 --> 00:10:48.380]   around retrieval, now when you retrieve memories, you're not retrieving specific observations,
[00:10:48.380 --> 00:10:53.300]   but you're retrieving these reflections, which carry high order meaning and high order, basically
[00:10:53.300 --> 00:11:02.060]   synthesize a lot of individual things into one specific sentence or however it's represented.
[00:11:02.060 --> 00:11:05.940]   And I think that's really powerful because one, it allows for a bit more efficient retrieval.
[00:11:05.940 --> 00:11:08.480]   You can just retrieve this one thing instead of 100 things.
[00:11:08.480 --> 00:11:13.640]   But then also you're explicitly taking this time to reflect on those observations.
[00:11:13.640 --> 00:11:17.940]   So a lot of the progress in language models and getting them to do things in a reliable
[00:11:17.940 --> 00:11:23.140]   way has come from this idea of asking the language model to think step by step, show
[00:11:23.140 --> 00:11:24.620]   its work.
[00:11:24.620 --> 00:11:28.340]   There's this concept of basically asking the language model to slow down.
[00:11:28.340 --> 00:11:32.060]   And I think this concept is reflection is similar in a sense where you're explicitly
[00:11:32.060 --> 00:11:34.560]   giving the, in this case, a system.
[00:11:34.560 --> 00:11:38.780]   You're giving the system a chance to reflect on what those observations mean, rather than
[00:11:38.780 --> 00:11:43.540]   just stuffing those observations into the prompt and asking it to generate an answer
[00:11:43.540 --> 00:11:47.600]   with keeping these observations in mind or something like that.
[00:11:47.600 --> 00:11:49.380]   So that covers the reflection.
[00:11:49.380 --> 00:11:51.280]   Now how they did retrieval I think is really interesting.
[00:11:51.280 --> 00:11:58.020]   It's kind of like a three pronged attack where they weight more recent memories higher, which
[00:11:58.020 --> 00:12:00.060]   makes sense if it just happened.
[00:12:00.060 --> 00:12:01.820]   It's very similar to human memory in some sense.
[00:12:01.820 --> 00:12:04.180]   If it just happened, it's more present in your mind.
[00:12:04.180 --> 00:12:05.460]   They weight it by importance.
[00:12:05.460 --> 00:12:06.980]   Again, very similar to human memory.
[00:12:06.980 --> 00:12:13.340]   I remember my birthday better than I remember a random breakfast I had a few years ago.
[00:12:13.340 --> 00:12:15.460]   And then they've got relevancy weighted.
[00:12:15.460 --> 00:12:18.560]   So this is just looking up based on semantic similarity or something like that.
[00:12:18.560 --> 00:12:20.380]   So they remember similar events.
[00:12:20.380 --> 00:12:21.540]   And they combine all these things.
[00:12:21.540 --> 00:12:24.460]   And importantly, they're not just doing this on the individual observations.
[00:12:24.460 --> 00:12:27.440]   They're also doing this on the reflections that they made.
[00:12:27.440 --> 00:12:31.160]   And so through this process, they're retrieving these observations.
[00:12:31.160 --> 00:12:36.280]   And then they're inserting it into the prompt and asking the agents to continue their simulation
[00:12:36.280 --> 00:12:39.140]   with those things in mind.
[00:12:39.140 --> 00:12:46.580]   And so that's an overview of the current state of LingChain and probably the most interesting
[00:12:46.580 --> 00:12:48.000]   paper that I've seen.
[00:12:48.000 --> 00:12:52.560]   We have an implementation for that in LingChain as well that a lot of people have played around
[00:12:52.560 --> 00:12:53.560]   with.
[00:12:53.560 --> 00:13:00.000]   But I think it's more interesting to think about what's not in LingChain.
[00:13:00.000 --> 00:13:05.240]   Because I think there's a lot of potential for these educational bots to have a more
[00:13:05.240 --> 00:13:09.000]   complex form of memory or for these chat bots to have a more complex form of memory.
[00:13:09.000 --> 00:13:11.520]   And I don't think we're really seeing that in the moment.
[00:13:11.520 --> 00:13:17.200]   And I think that's something that's a little bit hard for us to think about how to do.
[00:13:17.200 --> 00:13:23.340]   Because I think this idea of memory can often be really context specific.
[00:13:23.340 --> 00:13:29.320]   I think there is this generalization where there should be some reflection step that
[00:13:29.320 --> 00:13:33.360]   looks at some observation and updates some state accordingly.
[00:13:33.360 --> 00:13:37.440]   And then I think there should be some step that fetches from that state and puts it into
[00:13:37.440 --> 00:13:38.440]   the prompt.
[00:13:38.440 --> 00:13:41.640]   But that's like what I just said is incredibly vague and abstract.
[00:13:41.640 --> 00:13:42.640]   State could mean anything.
[00:13:42.640 --> 00:13:46.360]   And I think that's where the application specific stuff comes into play.
[00:13:46.360 --> 00:13:50.560]   That's where I think depending on the application you're building, you might want to consider
[00:13:50.560 --> 00:13:53.320]   a few different options.
[00:13:53.320 --> 00:13:55.520]   And that's also something I'm extremely excited about.
[00:13:55.520 --> 00:14:00.120]   As I said, it's tough for us to do because we're more of a horizontal kind of like infra
[00:14:00.120 --> 00:14:02.480]   tooling company.
[00:14:02.480 --> 00:14:06.340]   But we'd be extremely interested in working closely with people that do have those types
[00:14:06.340 --> 00:14:07.340]   of applications.
[00:14:07.340 --> 00:14:10.360]   Because I think this is an area that's incredibly underexplored.
[00:14:10.360 --> 00:14:15.160]   And so whether we have the chance to work together or not, my closing thoughts on this
[00:14:15.160 --> 00:14:17.640]   is it really comes down to two things.
[00:14:17.640 --> 00:14:19.360]   What is the type of data that should be in memory?
[00:14:19.360 --> 00:14:20.480]   What does this state look like?
[00:14:20.480 --> 00:14:24.400]   And that probably will be context specific and application specific.
[00:14:24.400 --> 00:14:30.000]   And then how can you use that memory in generation?
[00:14:30.000 --> 00:14:34.000]   And so just to touch on the different like types of memory that I think you should be
[00:14:34.000 --> 00:14:35.000]   considering.
[00:14:35.000 --> 00:14:37.320]   And a lot of these come from the generative agents paper.
[00:14:37.320 --> 00:14:38.760]   You've got this like recency bit.
[00:14:38.760 --> 00:14:39.760]   I think that's important.
[00:14:39.760 --> 00:14:43.200]   You know, you do need to remember the previous things that just happened.
[00:14:43.200 --> 00:14:45.360]   Those are likely always going to want to be in memory.
[00:14:45.360 --> 00:14:47.160]   You're going to want to remember the relevant things.
[00:14:47.160 --> 00:14:48.760]   You're going to want to remember the important things.
[00:14:48.760 --> 00:14:53.640]   And I've bolded reflection here because I think this is the new thing that language
[00:14:53.640 --> 00:14:54.640]   models really enable.
[00:14:54.640 --> 00:14:59.620]   I guess language models enable in some sense enable relevancy because that's mostly done
[00:14:59.620 --> 00:15:00.920]   through semantic search.
[00:15:00.920 --> 00:15:05.560]   In some sense they enable importance because at least in the generative agents paper you're
[00:15:05.560 --> 00:15:07.280]   using a language model to score that.
[00:15:07.280 --> 00:15:09.520]   But I think this concept of reflection is really the new bit.
[00:15:09.520 --> 00:15:12.040]   And that's what I'm really excited to explore more.
[00:15:12.040 --> 00:15:16.360]   And then the last bit is just how do you use this during the generation?
[00:15:16.360 --> 00:15:19.600]   And so I think the common things are, you know, you've got previous messages.
[00:15:19.600 --> 00:15:20.720]   You can just put them in a buffer.
[00:15:20.720 --> 00:15:22.160]   You can put them in a prompt.
[00:15:22.160 --> 00:15:24.480]   You've got related events that you pull in through a vector store.
[00:15:24.480 --> 00:15:30.080]   You can also put those in as these are related things that happened previously.
[00:15:30.080 --> 00:15:34.200]   But the last thing, and this is probably the most important thing in terms of really unlocking
[00:15:34.200 --> 00:15:36.480]   this, is how do you use the state?
[00:15:36.480 --> 00:15:41.040]   How do you take the state that you've reflected on, you've updated whatever form it may be,
[00:15:41.040 --> 00:15:42.560]   and how do you put that into the prompts?
[00:15:42.560 --> 00:15:45.040]   And I think there's a bunch of different examples of that.
[00:15:45.040 --> 00:15:48.200]   I think it's really context specific.
[00:15:48.200 --> 00:15:52.480]   And I'm really excited to see what everyone builds in this vein.
[00:15:52.480 --> 00:15:56.560]   Because I think this is an underexplored but incredibly exciting avenue for large language
[00:15:56.560 --> 00:15:57.800]   models.
[00:15:57.800 --> 00:15:58.800]   That's all I have for today.
[00:15:58.800 --> 00:15:59.800]   Thank you, guys, for listening.
[00:15:59.800 --> 00:16:00.800]   Thank you, Weights & Bias.
[00:16:00.800 --> 00:16:00.800]   [ Applause ]
[00:16:00.880 --> 00:16:01.880]   Thank you, Weights & Bias.
[00:16:01.880 --> 00:16:01.880]   [ Applause ]
[00:16:01.960 --> 00:16:06.960]   [ Music ]
[00:16:06.960 --> 00:16:11.960]   [ Music ]
[00:16:11.960 --> 00:16:21.960]   [BLANK_AUDIO]

