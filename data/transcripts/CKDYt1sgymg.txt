
[00:00:00.000 --> 00:00:10.720]   Most phone users, especially of a certain age, aren't particularly happy about how much time they are spending on their phone using social apps like TikTok or Instagram.
[00:00:10.720 --> 00:00:21.220]   They've heard people like me recommending for years that they should consider quitting those apps, but most people can't quite bring themselves to do it.
[00:00:21.220 --> 00:00:32.460]   And the question is why this is. I have an explanation I want to give here, and then I want to deconstruct this explanation and give you a new way of thinking about improving your relationship with your device.
[00:00:32.460 --> 00:00:34.920]   All right, so here's what I want to argue is going on.
[00:00:34.920 --> 00:00:49.000]   Part of the resistance or reluctance that people have to, let's say, taking popular apps off their phone is that they worry that it is an overreaction, that perhaps they are falling into a moral panic.
[00:00:49.600 --> 00:00:53.640]   Now, I want to play some audio here to better emphasize what I mean.
[00:00:53.640 --> 00:00:56.980]   This comes from the spring of 2019.
[00:00:56.980 --> 00:01:05.080]   It was from an interview appearance I did on Brian Koppelman's podcast as part of the book tour for my book, Digital Minimalism.
[00:01:05.080 --> 00:01:06.580]   So let's hear this tape for a second, Jesse.
[00:01:06.580 --> 00:01:16.460]   Because I could have been a car minimalist, and I could have said, you know, that there are all these costs of having a car.
[00:01:16.680 --> 00:01:20.060]   You're not going to see the scenery. You know, we need nature, and we need to see the nature.
[00:01:20.060 --> 00:01:26.240]   You're going to, you're risking in it, you know, if you have a slight inattention, you could crash.
[00:01:26.240 --> 00:01:26.700]   Yeah.
[00:01:27.540 --> 00:01:35.380]   And so to me, it is, this argument is also, the cars are taking over. There's nothing you can do about it.
[00:01:35.380 --> 00:01:35.780]   Yeah.
[00:01:36.200 --> 00:01:40.080]   And we better instead learn how to use this stuff, the car, how to drive well.
[00:01:40.540 --> 00:01:56.480]   All right. So what Koppelman was arguing there is a classic moral panic argument about, we hear about technology these days, which is like, look, we often get worried about new things, but these things are often not so bad.
[00:01:57.140 --> 00:02:04.260]   And in the end, we might as, we end up being okay with them. So we might as well just get along with the process of learning how to use them.
[00:02:04.260 --> 00:02:16.100]   So he was saying, you just as easily, he was talking about my book, Digital Minimalism. He's saying like, look, you say you're a digital minimalist. He was saying that clip, you could use that same argument to have been a car minimalist in the early 20th century.
[00:02:16.100 --> 00:02:26.040]   And be like, I don't want new cars, have these problems. We shouldn't have cars, but cars were inevitable. Cars were coming and they ended up, you know, we need cars. Life would be much harder without them.
[00:02:26.040 --> 00:02:31.740]   We wouldn't have the, you know, American growth like we had it without them. And so that was like a waste of time. It was alarmist.
[00:02:31.740 --> 00:02:45.740]   This is a common pushback that techno critics are familiar with. Now, I think this type of reasoning, this pushback fits particularly well when we talk about the trajectory of mass media, that tools like TikTok and Instagram are at the current end point of.
[00:02:45.740 --> 00:02:57.580]   So if we think about mass media in particular, we see some examples of perhaps people being really worked up into a lather when new medias came along and then eventually relaxing about them.
[00:02:57.580 --> 00:03:07.040]   So consider comic books, for example, these once terrified the fedora wearing pearl clutching adults of the era who were convinced that they were corrupting youth.
[00:03:07.040 --> 00:03:15.300]   In a 1954 Senate subcommittee meeting, a leading anti-comic book advocate named Frederick Wortham testified as follows.
[00:03:15.300 --> 00:03:25.280]   It is my opinion without any reasonable doubt and without any reservation that comic books are an important contributing factor in many cases of juvenile delinquency.
[00:03:25.280 --> 00:03:32.680]   He went on in that same testimony to say Batman is promoting homoeroticism and Wonder Woman is promoting sadomasochism.
[00:03:32.680 --> 00:03:35.040]   Jesse, I probably should have read more comic books.
[00:03:36.040 --> 00:03:39.540]   Yeah, I don't know what's going on in those things, but man, I missed out.
[00:03:39.540 --> 00:03:42.320]   Television engendered similar concerns.
[00:03:42.320 --> 00:03:44.960]   Here's a quote from Wendell Berry.
[00:03:44.960 --> 00:03:51.060]   He had a famous essay called Family Life, and it was included in his 1981 essay collection, The Gift of the Good Land.
[00:03:51.060 --> 00:03:52.940]   And in it, Berry said the following,
[00:03:52.940 --> 00:04:00.040]   As soon as we see that the TV court is a vacuum line piping life and meaning out of the household, we can unplug it.
[00:04:00.040 --> 00:04:03.440]   All right, we're okay with comic books today.
[00:04:03.680 --> 00:04:04.620]   It blew over.
[00:04:04.620 --> 00:04:07.040]   We're okay with TVs today.
[00:04:07.040 --> 00:04:08.640]   We all still have them.
[00:04:08.640 --> 00:04:10.660]   We all still watch Netflix or Macs.
[00:04:10.660 --> 00:04:13.520]   We're not nearly as worried as Berry once was.
[00:04:13.520 --> 00:04:17.300]   So the question is, maybe we can just think about it.
[00:04:17.300 --> 00:04:20.480]   Why can't we just think about social media as just the next stop on this trajectory?
[00:04:21.200 --> 00:04:29.020]   And in the future when we're all using VR to hang out with alien versions of Mark Zuckerberg, we'll look back at the social media age and be like, come on.
[00:04:29.020 --> 00:04:31.880]   That was just technology that came and went, and we're okay with it.
[00:04:33.120 --> 00:04:34.380]   But is this right?
[00:04:34.380 --> 00:04:47.120]   So I want to take a finer look at this argument of moral panic applied specifically to mass media, applied specifically to whether or not social media is just another thing on this trajectory.
[00:04:47.340 --> 00:04:49.620]   To help us make sense of this, here's what I'm going to do.
[00:04:49.620 --> 00:04:59.040]   I'm going to return to an analogy that I introduced in an essay last spring that I was thinking about recently and realized, wait, this gives us some useful insight into this very specific question.
[00:05:00.000 --> 00:05:03.040]   So the essay I want to draw from here came out last June.
[00:05:03.040 --> 00:05:05.860]   I'm putting on the screen here for people who are watching instead of just listening.
[00:05:05.860 --> 00:05:12.160]   I wrote an essay June 19, 2024 that was called On Ultra Processed Content.
[00:05:12.160 --> 00:05:23.260]   And what I did in this essay is I drew a connection between the nutritional notion of ultra processed food and social media content, which I was going to call ultra processed content.
[00:05:23.260 --> 00:05:30.360]   I want to read you a few pieces from this because I think it's going to really help us figure out what's actually going on here with this type of information.
[00:05:30.360 --> 00:05:31.100]   All right.
[00:05:31.100 --> 00:05:49.660]   So for my essay, I said, ultra processed foods at their most damaging extreme are made by breaking down core stock ingredients such as corn or soy into their basic organic building blocks, then recombining these elements into hyper palatable combinations rich in salt, sugar, and fat soaked with unpronounceable chemical emulsifiers and preservatives.
[00:05:50.160 --> 00:05:56.960]   The problem with ultra processed foods is that they are engineered to hijack our desire mechanisms, making them literally irresistible.
[00:05:56.960 --> 00:06:03.620]   The result is that we consume way more calories than we need in arguably the least healthy form possible.
[00:06:03.620 --> 00:06:05.120]   All right.
[00:06:05.120 --> 00:06:06.140]   That's ultra processed food.
[00:06:06.140 --> 00:06:08.420]   Let's connect that to digital information.
[00:06:08.420 --> 00:06:14.000]   Reading from the article again here, something similar happens with social media content.
[00:06:14.440 --> 00:06:27.620]   Whereas the stock ingredients for ultra processed food are found in vast fields of cheap corn and soy, social media content draws on vast databases of user generated information, post reactions, videos, quips, and memes.
[00:06:27.620 --> 00:06:35.820]   Recommendation algorithms then sift through this monumental collection of proto content to find new hard to resist combinations that will appeal to users.
[00:06:35.820 --> 00:06:50.000]   A feedback loop soon develops in which the producers of the stock content adapts to what seems to better please the platform, simplifying and purifying their outputs to more efficiently feed the algorithm's global goal of hijacking the human desire mechanisms.
[00:06:50.000 --> 00:06:59.700]   In this way, the users of social media platforms simulate something like the food scientist's ability to break down corn and reconstitute it into a hyper palatable edible food-like substance.
[00:07:00.180 --> 00:07:04.280]   What is a TikTok dance mashup if not a digital Dorito?
[00:07:04.280 --> 00:07:17.640]   So I think this analogy is useful because we treat in the world of nutrition ultra processed food differently than other types of food, including some food that's not that healthy for us.
[00:07:17.640 --> 00:07:28.560]   And I want to argue if we follow this analogy to its conclusion, we'll see that maybe we should treat ultra processed content similar than we do other types of content, even content that's not necessarily super good for us.
[00:07:28.940 --> 00:07:43.040]   So in particular, looking back in the world of nutrition, there has been a porous, to be sure, but important dividing line that's been made between processed food more broadly and ultra processed food more specifically.
[00:07:43.040 --> 00:07:50.240]   So processed food is, you know, any type of food that has gone through some sort of notable processing from its natural state.
[00:07:50.320 --> 00:07:54.220]   It includes some stuff that's like barely processed, like roast nuts.
[00:07:54.220 --> 00:07:55.840]   And at the other extreme, it's like ultra processed.
[00:07:55.840 --> 00:08:03.500]   But a lot of classic processed foods are things like, you know, bread or fresh baked cookies or something like this, right?
[00:08:03.500 --> 00:08:11.360]   We learned in the 20th century, hey, it's kind of a problem if all you eat is processed food all the time.
[00:08:11.360 --> 00:08:13.180]   Like you need to eat whole foods and natural foods.
[00:08:13.180 --> 00:08:18.360]   But we didn't argue to people, cut every bit of processed food altogether on your diet.
[00:08:18.360 --> 00:08:24.320]   That is extreme and basically unaccomplishable for almost everyone.
[00:08:24.320 --> 00:08:31.420]   And the people who do pull it off is an incredibly restrictive commitment to only having sort of like minimally processed fresh whole foods.
[00:08:31.420 --> 00:08:32.460]   That should be a lot of your diet.
[00:08:32.520 --> 00:08:34.880]   But we didn't tell people try to get rid of all processed food.
[00:08:34.880 --> 00:08:40.320]   It would also cut out things that, you know, people actually enjoyed in life that were part of cultural traditions.
[00:08:40.320 --> 00:08:47.720]   If you really cut out all possible processed foods, you would never have your, you know, Bubby's Kugel or Italian grandmother's pasta.
[00:08:47.720 --> 00:08:51.060]   So it feels like this would be too far.
[00:08:51.060 --> 00:08:54.220]   But with ultra processed food, nutrition experts like that, come on.
[00:08:54.220 --> 00:08:56.600]   You don't just, it's easier just to cut it out all together.
[00:08:56.600 --> 00:08:58.440]   Like this is a very common dichotomy.
[00:08:58.440 --> 00:09:02.000]   Don't eat too much of the processed foods, you know, like enjoy it when you do,
[00:09:02.060 --> 00:09:03.320]   but don't make it the core of your diet.
[00:09:03.320 --> 00:09:07.620]   But when it comes to Doritos and Oreos, this isn't some family tradition.
[00:09:07.620 --> 00:09:09.180]   It's not somehow like necessary.
[00:09:09.180 --> 00:09:10.640]   It's not hard to get rid of them.
[00:09:10.640 --> 00:09:15.220]   They're super unhealthy, much more so compared to like having pasta because of all the weird chemicals.
[00:09:15.220 --> 00:09:20.360]   And they're way more hyper palatable than, you know, the Kugel that you're having or whatever.
[00:09:20.360 --> 00:09:22.180]   Why not just avoid those altogether?
[00:09:22.180 --> 00:09:24.560]   And this is a very common dividing line we make.
[00:09:24.560 --> 00:09:27.700]   There's unhealthy food that we eat with moderation and that's okay.
[00:09:27.700 --> 00:09:28.960]   And it has deep history.
[00:09:28.960 --> 00:09:31.500]   And then there's like this new frankenfoods.
[00:09:31.600 --> 00:09:35.700]   And it's easier to say, no nutritionist is like, well, you know, eat some Oreos, but not too much.
[00:09:35.700 --> 00:09:37.620]   Like it's better just not to eat that if you can avoid it.
[00:09:37.620 --> 00:09:39.280]   And it's not that hard to avoid it.
[00:09:39.280 --> 00:09:41.820]   You cannot have super junk food in your house.
[00:09:42.060 --> 00:09:44.080]   And it's not really going to make, that's not hard.
[00:09:44.080 --> 00:09:45.480]   Hey, it's Cal.
[00:09:45.480 --> 00:09:57.080]   I wanted to interrupt briefly to say that if you're enjoying this video, then you need to check out my new book, Slow Productivity, The Lost Art of Accomplishment Without Burnout.
[00:09:57.340 --> 00:10:02.660]   This is like the Bible for most of the ideas we talk about here in these videos.
[00:10:02.660 --> 00:10:07.920]   You can get a free excerpt at calnewport.com slash slow.
[00:10:07.920 --> 00:10:09.740]   I know you're going to like it.
[00:10:09.740 --> 00:10:10.740]   Check it out.
[00:10:10.740 --> 00:10:12.100]   Now let's get back to the video.
[00:10:12.360 --> 00:10:15.160]   It's not going to be hard to find calories necessarily.
[00:10:15.160 --> 00:10:19.100]   It's not going to be cutting you off from things that are very important in your life.
[00:10:19.580 --> 00:10:22.840]   So this is one way to think about what's going on in the world of media.
[00:10:22.840 --> 00:10:25.620]   The social media content is the ultra processed food.
[00:10:25.620 --> 00:10:35.360]   We look at things like, I don't know, watching streaming services on TV or digital diversions like those New York Times games apps like Spelling Bee or Wordle.
[00:10:35.360 --> 00:10:37.220]   That's like the pasta or the kugel.
[00:10:37.220 --> 00:10:39.680]   Don't spend all your time doing that.
[00:10:39.680 --> 00:10:43.540]   If you're in front of the TV, binging Netflix eight hours a day, that's bad.
[00:10:43.540 --> 00:10:45.120]   Also don't eat canned soup all day.
[00:10:45.120 --> 00:10:47.920]   But like you can have some of that in your life, not a big deal.
[00:10:48.800 --> 00:11:07.640]   But the ultra processed stuff, where you've broken down content like TikTok does into its most constituent smallest atomic elements and then algorithmically reconstituted into something that resembles content but is like no other content that anyone has ever constructed from scratch.
[00:11:07.640 --> 00:11:09.700]   And it is hyper palatable and you can't put it away.
[00:11:09.700 --> 00:11:17.100]   And the actual information being conveyed in the content makes you feel existentially empty or scared or outraged or upset.
[00:11:17.700 --> 00:11:23.920]   This is the psychological equivalent of the physical unwellness you get from eating ultra processed actual nutritious foods as well, right?
[00:11:23.920 --> 00:11:26.520]   Why don't we just avoid those altogether?
[00:11:26.520 --> 00:11:30.700]   This now becomes, I believe, an intellectually coherent argument.
[00:11:30.700 --> 00:11:33.360]   It doesn't mean that it's necessarily the answer.
[00:11:34.480 --> 00:11:38.920]   And we know people in the world of nutrition who like can have a couple Oreos and be like, I'm done.
[00:11:38.920 --> 00:11:43.060]   Most people, you open up a bag of Oreos, they're going to crush four sleeves.
[00:11:43.060 --> 00:11:44.740]   Some people don't.
[00:11:44.740 --> 00:11:47.460]   But most people, it's easier to say, just don't have Oreos in the house.
[00:11:47.720 --> 00:12:03.040]   So I bring this up just to make the argument that this analogy of ultra processed food and how it compares to processed food gives us at the very least an intellectually coherent and internally consistent argument for why you might say to someone, yeah, it's true.
[00:12:03.760 --> 00:12:10.580]   We might have overreacted about television and comic books and yet you probably still shouldn't use TikTok.
[00:12:11.580 --> 00:12:17.920]   Because sometimes in a particular space, as technology advances, its ability to do harm gets worse.
[00:12:17.920 --> 00:12:21.820]   And as we advance along there, it's the more advanced versions.
[00:12:21.820 --> 00:12:23.040]   We're like, okay, that's too far.
[00:12:23.040 --> 00:12:32.720]   So I do think there's an intellectually consistent or coherent and internally consistent argument for social media being an exceptional danger in the world of mass media.
[00:12:32.720 --> 00:12:38.020]   And just something I was thinking about is this ultra processed food analogy helps make that clear.
[00:12:38.020 --> 00:12:39.780]   So there you go.
[00:12:39.820 --> 00:12:42.680]   I don't know why I was thinking about that, Jesse, but it came up in a conversation.
[00:12:42.680 --> 00:12:43.440]   I was like, that's helpful.
[00:12:43.440 --> 00:12:48.860]   It's pretty incredible when most grocery stores you go into and all the processed foods like in the middle.
[00:12:48.860 --> 00:12:51.920]   It's only on the outskirts where you can get like the healthy foods and stuff.
[00:12:51.920 --> 00:12:54.000]   I feel like that's the internet these days.
[00:12:54.000 --> 00:12:54.280]   Yeah.
[00:12:54.280 --> 00:12:58.360]   Like on the outskirts is, oh, here's an interesting email newsletter and podcast.
[00:12:58.360 --> 00:13:03.860]   And in the middle is, you know, like a white supremacist TikTok dance.
[00:13:03.860 --> 00:13:07.040]   And it's like, put it in my veins.
[00:13:07.040 --> 00:13:08.820]   Put it in my veins.
[00:13:08.820 --> 00:13:10.940]   So anyways, I think that's, that's useful to me at least.
[00:13:10.940 --> 00:13:12.560]   So hopefully other people find that useful as well.
[00:13:12.560 --> 00:13:14.100]   I like that term digital Dorito.
[00:13:14.100 --> 00:13:16.060]   That's really what these things are.
[00:13:16.060 --> 00:13:18.300]   By the way, I wrote it this in more length than my newsletter.
[00:13:18.300 --> 00:13:24.600]   So again, if you're not subscribed, calnewport.com and you can find it on there and subscribe to get stuff like this in the future.
[00:13:25.600 --> 00:13:25.880]   All right.
[00:13:25.880 --> 00:13:25.960]   All right.
[00:13:26.160 --> 00:13:27.680]   We got some good questions coming up.
[00:13:27.680 --> 00:13:31.120]   But first, I want to mention one of our sponsors.
[00:13:31.940 --> 00:13:34.640]   Let's talk about our friends at Cozy Earth.
[00:13:35.500 --> 00:13:37.560]   One of my favorite companies.
[00:13:37.560 --> 00:13:45.040]   So Cozy Earth, they have the core products you might know them for is their sheets, which they're bamboo sheets, which I've really been pushing.
[00:13:45.040 --> 00:13:47.940]   We have multiple pairs in our house, the most comfortable sheets that we own.
[00:13:48.020 --> 00:13:53.640]   We bring them with us when we travel for more than a week, but they have other things you can buy as well.
[00:13:53.640 --> 00:14:01.620]   I just got a new, and we, I thought I owned everything Cozy Earth sells, Jesse, but I just got a new Cozy Earth product, which is the all day tea.
[00:14:01.880 --> 00:14:08.320]   So it's a T-shirt made out of their really comfortable style fabric and everyday essential with natural, breathable fabric.
[00:14:08.320 --> 00:14:10.700]   This tea ensures that you stay cool and comfortable all day long.
[00:14:10.700 --> 00:14:12.180]   This fabric has a way of being cooling.
[00:14:12.180 --> 00:14:13.280]   I wore it.
[00:14:13.280 --> 00:14:14.140]   I worked out in it.
[00:14:14.140 --> 00:14:15.940]   I wore it around the house over the weekend.
[00:14:15.940 --> 00:14:18.380]   I can tell you, super comfortable.
[00:14:18.380 --> 00:14:21.040]   I also have the Studio Pant Collection.
[00:14:21.040 --> 00:14:24.440]   I don't think I've ever had such a soft pair of pants.
[00:14:24.440 --> 00:14:25.780]   It's like soft and stretchy.
[00:14:25.780 --> 00:14:27.920]   Cozy Earth knows how to do comfort.
[00:14:27.920 --> 00:14:29.560]   So here's what you need.
[00:14:29.560 --> 00:14:31.020]   You need the sheets.
[00:14:31.720 --> 00:14:33.700]   I've just, I'm telling you this, buy the sheets.
[00:14:33.700 --> 00:14:35.300]   Father's Day is coming up.
[00:14:35.300 --> 00:14:43.740]   Why don't you think about the all day tea or the Studio Pant Collection as a potential Father's Day gift?
[00:14:43.740 --> 00:14:48.460]   I can tell you, I love the Cozy Earth stuff and the father in your life will as well.
[00:14:48.460 --> 00:14:53.100]   Keep in mind that there is a lifetime warranty on all apparel items.
[00:14:53.100 --> 00:14:55.860]   So hey, if you don't like it, you send it back, but you will like it.
[00:14:55.860 --> 00:14:57.380]   You will love it.
[00:14:57.380 --> 00:14:58.960]   I've been loving my Cozy Earth stuff.
[00:14:59.360 --> 00:15:04.060]   It's a great way to create a sanctuary in your own home.
[00:15:04.060 --> 00:15:08.000]   Cozy Earth makes comfort at last and this Father's Day, he deserves it.
[00:15:08.000 --> 00:15:12.200]   Go to CozyEarth.com and use code DEEP for up to 40% off all men's apparel.
[00:15:12.200 --> 00:15:13.940]   That's CozyEarth.com.
[00:15:13.940 --> 00:15:21.420]   Use code DEEP for the dads who work hard during their nine to five and deserve the best during their five to nine.
[00:15:22.280 --> 00:15:22.780]   There we go.
[00:15:22.780 --> 00:15:25.240]   I like reading things about how dads are awesome.
[00:15:25.240 --> 00:15:26.380]   I'm just like, I'm awesome.
[00:15:26.380 --> 00:15:28.480]   Send me gifs.
[00:15:28.480 --> 00:15:32.680]   I also want to talk about our friends at Harry's.
[00:15:32.680 --> 00:15:36.180]   Staying well-groomed requires the finest and most advanced tools.
[00:15:36.180 --> 00:15:39.440]   Upgrade your routine with Harry's.
[00:15:39.440 --> 00:15:40.480]   I love my Harry's razor.
[00:15:41.760 --> 00:15:44.000]   Good, weighted, thick handle.
[00:15:44.000 --> 00:15:45.000]   It looks really nice.
[00:15:45.000 --> 00:15:46.320]   The blades are really good.
[00:15:46.320 --> 00:15:48.640]   They shave quite smoothly.
[00:15:48.640 --> 00:15:52.340]   It's the most advanced shaving product and you can get it at an unbeatable price.
[00:15:52.340 --> 00:15:53.620]   That's where Harry's comes in.
[00:15:53.620 --> 00:15:55.440]   $10 trial set.
[00:15:55.440 --> 00:15:57.400]   You can now get that for $6.
[00:15:57.400 --> 00:16:00.380]   Special deal at Harry's.com slash deep.
[00:16:00.540 --> 00:16:04.420]   That's an exclusive offer just for our listeners.
[00:16:04.420 --> 00:16:06.420]   These blades are German engineered.
[00:16:06.420 --> 00:16:08.840]   The Germans know how to make razor blades.
[00:16:08.840 --> 00:16:13.520]   They have customizable delivery options for scheduled refills as low as $2.
[00:16:13.520 --> 00:16:14.580]   This is cheap.
[00:16:14.580 --> 00:16:18.600]   You're not spending all the money you need to get the other brands at the drugstore.
[00:16:18.600 --> 00:16:23.500]   You also don't have to go through the 17 layers of security to get to the razors like you do these days at the drugstore.
[00:16:23.500 --> 00:16:25.200]   Just set up a refill plan.
[00:16:25.200 --> 00:16:27.240]   Have them to show up as you need them.
[00:16:27.240 --> 00:16:29.080]   We talked about autopilot scheduling.
[00:16:29.360 --> 00:16:30.560]   This is an example.
[00:16:30.560 --> 00:16:34.180]   Don't think another minute about getting razors.
[00:16:34.180 --> 00:16:35.880]   Just set up your subscription with Harry's.
[00:16:35.880 --> 00:16:38.280]   You will not regret it.
[00:16:38.280 --> 00:16:43.760]   Use their shaving cream, their body wash.
[00:16:43.760 --> 00:16:45.360]   They have all sorts of other stuff as well.
[00:16:45.360 --> 00:16:47.080]   Deodorant.
[00:16:47.080 --> 00:16:48.200]   Look, it's all there.
[00:16:48.200 --> 00:16:48.900]   Get shipped to you.
[00:16:48.900 --> 00:16:49.700]   Get automatic.
[00:16:49.700 --> 00:16:51.020]   Save some time.
[00:16:51.020 --> 00:16:52.500]   Get products you like.
[00:16:52.500 --> 00:16:54.560]   So take your grooming out of the Stone Age.
[00:16:54.560 --> 00:16:55.220]   Get Harry's.
[00:16:55.220 --> 00:16:58.080]   Normally the trial set is $10, but right now you can get it for just $6.
[00:16:58.080 --> 00:16:59.900]   at harrys.com slash deep.
[00:16:59.900 --> 00:17:05.420]   That's our exclusive link, harrys.com slash deep for a $6 trial set.
[00:17:05.420 --> 00:17:07.940]   All right, Jesse, let's do some questions.
[00:17:07.940 --> 00:17:11.500]   First question is from Alan.
[00:17:11.860 --> 00:17:15.040]   I'm in a new role that demands deeper theoretical understanding.
[00:17:15.040 --> 00:17:22.820]   How can I improve my ability to process and critically engage with technical literature so I can become a better, more analytical thinker?
[00:17:23.160 --> 00:17:25.920]   I've been thinking a lot recently about thinking.
[00:17:25.920 --> 00:17:33.780]   And so this is a timely question, probably because I'm on a chapter of my new book, The Deep Life, that's titled Reclaiming Your Mind or Reclaim Your Brain.
[00:17:33.780 --> 00:17:35.360]   So I've been thinking about this stuff a lot.
[00:17:35.360 --> 00:17:35.840]   All right.
[00:17:35.840 --> 00:17:39.480]   There's an immediate solution and a longer-term solution, and I want you to think about both.
[00:17:39.480 --> 00:17:45.720]   The immediate solution involves what you do right now when you're in a session where you're trying to learn something hard.
[00:17:47.000 --> 00:17:50.580]   Or you've learned something hard and you want to apply it in a complicated way.
[00:17:50.580 --> 00:17:52.200]   Don't context shift.
[00:17:53.020 --> 00:17:58.760]   This right away is like taking a reduced dose of the Limitless pill from that Bradley Cooper movie.
[00:17:58.760 --> 00:17:59.960]   Is that Bradley Cooper?
[00:17:59.960 --> 00:18:01.200]   Limitless?
[00:18:01.200 --> 00:18:03.040]   Let's just assume it is.
[00:18:03.040 --> 00:18:05.480]   I'm going to assume it is because he's a Georgetown grad, so I like to say that.
[00:18:05.480 --> 00:18:09.040]   When you're not context switching, you feel much smarter.
[00:18:09.040 --> 00:18:15.940]   When you're context switching, meaning that you're trying to mainly focus on this one thing, but you're like quick checking your inbox and quick checking your phone.
[00:18:15.940 --> 00:18:22.180]   And, well, maybe I need to make the same game parlay for the upcoming, you know, Giants-Nationals game or whatever it is you're doing.
[00:18:22.180 --> 00:18:24.820]   It reduces your IQ points non-trivially.
[00:18:24.820 --> 00:18:31.940]   So if you actually, when you do something that requires hard thinking, lock in without context shifting at all, you are going to feel much smarter.
[00:18:31.940 --> 00:18:33.440]   So that's an immediate win.
[00:18:33.440 --> 00:18:34.160]   Do that right away.
[00:18:34.160 --> 00:18:40.700]   Longer-term solution is to get yourself better at doing analytical thinking.
[00:18:40.700 --> 00:18:41.700]   A few things here matter.
[00:18:41.700 --> 00:18:43.840]   One, you need more comfort with boredom.
[00:18:43.920 --> 00:18:48.960]   There's not a lot of novel stimuli when you're sitting down and trying to work on within your mind's eye, just a single problem.
[00:18:48.960 --> 00:18:52.300]   That's boring in the technical sense of lacking novel stimuli.
[00:18:52.300 --> 00:18:53.740]   Get used to that.
[00:18:53.740 --> 00:19:02.100]   Do at least a few things every day without a distraction, without a phone in your hand or an earbud in your ear, just to get used to being alone with your own thoughts and the world around you.
[00:19:02.100 --> 00:19:04.000]   You will acclimatize to it.
[00:19:04.320 --> 00:19:14.500]   If you acclimatize to it in these low-stakes situation, when you get to the higher-stakes situation of this thing is really hard, your mind isn't going to rebel about we're bored.
[00:19:14.500 --> 00:19:15.320]   It's used to that.
[00:19:15.320 --> 00:19:17.780]   It knows it's not really a threat.
[00:19:17.780 --> 00:19:23.220]   You can also train your brain to better process and make sense of information.
[00:19:23.700 --> 00:19:31.520]   To do so, start with what I call the 20-10 rule, 20 pages of reading a day, 10 minutes of being alone with your own thoughts a day, every day.
[00:19:31.520 --> 00:19:35.560]   Reading is fantastic for deepening analytical circuits.
[00:19:35.560 --> 00:19:39.900]   It's something I'm really looking into now for the book I'm writing.
[00:19:40.540 --> 00:19:43.460]   But to give you the super short version of it, reading is unique.
[00:19:43.460 --> 00:19:45.820]   It's different than speech.
[00:19:45.820 --> 00:19:50.520]   It's different than vision because it's something for which humans do not come pre-equipped with a neural circuit for.
[00:19:50.520 --> 00:19:53.420]   We have to train ourselves from scratch how to read.
[00:19:53.420 --> 00:19:59.700]   We basically hijack other circuits that are in our brain for other utilities that existed deep into our evolutionary past.
[00:19:59.700 --> 00:20:05.920]   We hijack those and we rewire them to do this really novel thing where we have to build these reading circuits.
[00:20:05.980 --> 00:20:11.100]   And when we build these new circuits, they make us better able to process information.
[00:20:11.100 --> 00:20:12.760]   The best thinkers are great readers.
[00:20:12.760 --> 00:20:18.140]   Reading is as close as we have to cognitive calisthenics.
[00:20:18.140 --> 00:20:20.220]   So start reading 20 pages a day.
[00:20:20.220 --> 00:20:23.720]   At first, what matters is that you're doing it, so I don't care what it is.
[00:20:23.720 --> 00:20:26.200]   Make it something that you're most likely to come back to.
[00:20:26.200 --> 00:20:29.620]   So the most fun, possible, interesting, easy, possible thing you can.
[00:20:29.620 --> 00:20:31.980]   And then have 10 minutes of being alone with your own thoughts.
[00:20:31.980 --> 00:20:43.800]   This gets you not just used to boredom, but it gets you used to grabbing a current out of the turbulent streams within your mind, sticking with it, and trying to actually follow it to its source.
[00:20:43.800 --> 00:20:47.040]   That is something you have to practice as well.
[00:20:47.040 --> 00:20:51.280]   When you're reading, you're kind of like actively focused on what's happening here and processing it.
[00:20:51.280 --> 00:20:56.240]   But when it comes time to do analytical thinking, you have to put those circuits to work on original information.
[00:20:56.240 --> 00:21:04.060]   So you have to be comfortable applying those circuits and being within your own thoughts and working, filtering noise, focusing on what matters.
[00:21:04.060 --> 00:21:08.880]   So 20 pages, 10 minutes of time alone with your own thoughts, preferably walking.
[00:21:08.880 --> 00:21:12.920]   There's a reason why Aristotle called his philosophical school the peripatetic school.
[00:21:12.920 --> 00:21:14.000]   They moved.
[00:21:14.000 --> 00:21:15.100]   It helps you think.
[00:21:15.100 --> 00:21:15.780]   All right.
[00:21:15.780 --> 00:21:20.240]   Once you're comfortable with that, add 10 more pages and five more minutes.
[00:21:21.020 --> 00:21:22.700]   Now it becomes a 30-15 rule.
[00:21:22.700 --> 00:21:28.880]   I'm reading 30 pages a day, 15 minutes a day of walking and just thinking and trying to hold and make sense of thoughts.
[00:21:28.880 --> 00:21:30.700]   Get comfortable with that.
[00:21:30.700 --> 00:21:32.900]   Another 10 pages, another five minutes.
[00:21:32.900 --> 00:21:48.060]   If you can get to something like 50 or so pages a day, 50 to 60 pages a day, and 20 to 30 minutes of reflective thinking, you're walking and thinking and holding your mind's eye, now it's like you're doing CrossFit.
[00:21:48.060 --> 00:21:49.760]   This is CrossFit for your mind.
[00:21:49.760 --> 00:21:53.260]   You're going to be in better shape than most people.
[00:21:53.260 --> 00:21:55.380]   Jesse, I'm looking at you because I know you do CrossFit.
[00:21:55.380 --> 00:21:57.440]   Jesse explains his CrossFit workouts to me.
[00:21:57.440 --> 00:22:02.680]   And here's like my simulation of Jesse explaining his CrossFit workout.
[00:22:02.680 --> 00:22:08.600]   He's like, well, they came in and they said, okay, what we're going to do today is 97 sets.
[00:22:08.600 --> 00:22:14.580]   97 sets of clean and jerks followed by 4,000 meters on the rower.
[00:22:15.680 --> 00:22:19.360]   And then we're going to do 19,000 box jumps.
[00:22:19.360 --> 00:22:20.420]   Is that about right?
[00:22:20.420 --> 00:22:21.900]   Yeah, more or less.
[00:22:21.900 --> 00:22:22.580]   That's about right.
[00:22:22.580 --> 00:22:22.880]   Yeah.
[00:22:22.880 --> 00:22:25.280]   So what can I say?
[00:22:25.280 --> 00:22:27.080]   This is CrossFit for your brain.
[00:22:27.080 --> 00:22:35.500]   If you can get to roughly that like 60 pages a day, 20 to 30 minutes reflection on most days, now you're ready to do some damage with your brain.
[00:22:35.560 --> 00:22:37.880]   You can actually train that brain to be smarter.
[00:22:37.880 --> 00:22:38.500]   And I don't know.
[00:22:38.500 --> 00:22:39.160]   That's what I'd recommend.
[00:22:39.160 --> 00:22:41.820]   You know who doesn't need to do CrossFit?
[00:22:41.820 --> 00:22:43.540]   Cal Network.
[00:22:44.220 --> 00:22:46.000]   You know he does it.
[00:22:46.000 --> 00:22:51.500]   When Cal Network walks in the CrossFit gyms, they shut down the boxes.
[00:22:51.500 --> 00:22:52.680]   They shut them down.
[00:22:52.680 --> 00:22:55.140]   They're like, it's not worth it.
[00:22:55.140 --> 00:22:56.520]   It's not worth it.
[00:22:56.520 --> 00:23:00.660]   And they all go to Krispy Kreme because they're so intimidated by his muscles.
[00:23:00.660 --> 00:23:00.940]   All right.
[00:23:00.940 --> 00:23:01.540]   Who do we got next?
[00:23:01.540 --> 00:23:02.880]   Next up is Arjan.
[00:23:02.880 --> 00:23:06.940]   I'm about to start my first internship doing software engineering work.
[00:23:06.940 --> 00:23:08.860]   I'm still in college and this job is remote.
[00:23:08.860 --> 00:23:13.240]   How can I stay motivated, productive, and balanced during my solo work weeks?
[00:23:13.240 --> 00:23:15.640]   Arjan, you have to time block plan.
[00:23:15.640 --> 00:23:17.100]   You have to.
[00:23:17.100 --> 00:23:27.240]   If there's no actual physical separation between your work and no work, if there's no actual in-person visible supervision that's going to play against more classic motivational circuits.
[00:23:27.240 --> 00:23:28.740]   If the boss is here, I want to do my work.
[00:23:28.740 --> 00:23:29.720]   I want to show them what I'm doing.
[00:23:29.720 --> 00:23:32.540]   It is up to you to add structure, and you have to have structure.
[00:23:32.540 --> 00:23:33.640]   Time block plan.
[00:23:33.640 --> 00:23:35.780]   When am I starting work?
[00:23:35.780 --> 00:23:36.860]   What blocks am I doing?
[00:23:36.860 --> 00:23:37.780]   When is my breaks?
[00:23:37.780 --> 00:23:38.540]   When am I doing email?
[00:23:38.540 --> 00:23:39.460]   When am I finishing work?
[00:23:39.460 --> 00:23:40.160]   Shut down routine.
[00:23:40.160 --> 00:23:41.560]   Then I move on to the rest of the day.
[00:23:41.560 --> 00:23:47.580]   Do not allow your time outside of work to blend together with your work and just say, like, look, I'll get stuff done at some point today.
[00:23:47.580 --> 00:23:48.420]   Time block plan.
[00:23:48.420 --> 00:23:53.640]   I would suggest actually trying to keep those actual working hours tight and super intense.
[00:23:53.640 --> 00:23:55.780]   And then when you're done, be done.
[00:23:55.780 --> 00:23:57.420]   But you've got to structure.
[00:23:57.420 --> 00:23:59.780]   When you're remote work, structuring your time is more important than ever.
[00:23:59.780 --> 00:24:03.700]   So get some time block planning going.
[00:24:04.920 --> 00:24:07.760]   I once, by the way, Jesse, took a look at Cal Network's time block planner.
[00:24:07.760 --> 00:24:09.960]   It's a single index card.
[00:24:09.960 --> 00:24:11.880]   It says crush it on it.
[00:24:11.880 --> 00:24:13.340]   That's his time block plan.
[00:24:13.340 --> 00:24:14.660]   All right.
[00:24:14.660 --> 00:24:15.220]   Who do we got next?
[00:24:15.220 --> 00:24:17.220]   Next up is Daniel.
[00:24:17.880 --> 00:24:34.040]   I'm curious about your thoughts on how much time to spend developing secondary skills like improving typing speed or customizing tools that support our main priorities but can easily become distracting pseudo productivity and how to set boundaries so that these activities don't take over real skill development.
[00:24:34.320 --> 00:24:37.660]   I mean, I would downplay those secondary skills.
[00:24:37.660 --> 00:24:40.960]   Like, when you learn them, you learn them because it's clear.
[00:24:40.960 --> 00:24:42.960]   If I got better at this, it's going to help exactly that.
[00:24:42.960 --> 00:24:50.340]   But really make your main focus in a demonstrable way, unambiguously demonstrable way, doing things that are rare and valuable.
[00:24:51.000 --> 00:24:59.040]   You got to get wired for that because what a lot of people do with the secondary skill pursuit is they're trying to actually avoid being evaluated.
[00:24:59.040 --> 00:25:03.880]   They want to avoid having to, at some point, just actually put everything on the line and say, here's my thing.
[00:25:03.880 --> 00:25:04.600]   How good is it?
[00:25:04.600 --> 00:25:06.160]   And they give you your ranking.
[00:25:07.160 --> 00:25:09.140]   And if you're like, no, I have a plan.
[00:25:09.140 --> 00:25:10.320]   I'm working on this skill.
[00:25:10.320 --> 00:25:13.440]   I built a custom productivity tool over here.
[00:25:13.440 --> 00:25:16.860]   I'm going to try to learn how to do this, like, secondary thing.
[00:25:16.860 --> 00:25:18.840]   It allows you to seem very, like, busy and innovative.
[00:25:18.840 --> 00:25:25.000]   It allows you to keep the hope of doing well in the future while avoiding the stress of actually just putting out there and seeing how you do.
[00:25:25.000 --> 00:25:30.520]   So I want you to get obsessed first about I'm competing to do good stuff.
[00:25:30.520 --> 00:25:31.480]   How am I doing?
[00:25:31.480 --> 00:25:33.320]   How much money am I getting from clients?
[00:25:33.320 --> 00:25:35.180]   Am I getting the promotion?
[00:25:35.180 --> 00:25:37.040]   How many dollars am I bringing into the company?
[00:25:37.620 --> 00:25:50.700]   If you have a good laser-like focus on producing unambiguous value and not being afraid of falling short and celebrating when you actually get ahead of where you think, then, yeah, throw in secondary skills as you need.
[00:25:50.700 --> 00:25:52.020]   And they're not going to be a diversion.
[00:25:52.020 --> 00:25:53.480]   They're going to be very utilitarian.
[00:25:53.480 --> 00:25:55.500]   Like, man, I'm really doing well here.
[00:25:55.500 --> 00:25:58.060]   Ah, if I could type faster, I guess that would really help this.
[00:25:58.060 --> 00:25:59.500]   Now I have a reason to learn it.
[00:25:59.500 --> 00:26:06.080]   So what I really want you to do here is not allow the secondary skills to become a diversion from actually just becoming so good you can't be ignored.
[00:26:06.180 --> 00:26:07.680]   So make that your obsession first.
[00:26:07.680 --> 00:26:11.100]   Secondary skills should remain second.
[00:26:11.100 --> 00:26:12.900]   All right.
[00:26:12.900 --> 00:26:13.560]   What do we got next?
[00:26:13.560 --> 00:26:14.560]   All right.
[00:26:14.560 --> 00:26:15.700]   Next up is from Brian.
[00:26:15.700 --> 00:26:21.880]   How does TikTok seem to know so much about me and tailor's contacts so accurately to my interests and behavior?
[00:26:22.560 --> 00:26:41.680]   That's a good question because I think it connects to our deep dive where we talked about content like TikTok being ultra-processed, that unlike other types of content like television, streaming, or games on your phone, it has a way of being hyperpalatable and it has a way of pushing your buttons in ways that's really psychologically unhealthy.
[00:26:41.820 --> 00:26:45.400]   So I think it's worth looking into because I actually think I know something about this.
[00:26:45.400 --> 00:26:53.700]   I mean, I did some research on this back when there was an internal document that got leaked and this was sort of making its rounds and I read a bunch of accounts of this document.
[00:26:53.820 --> 00:26:57.400]   I have like a generic sense of the techniques used by a TikTok algorithm.
[00:26:57.740 --> 00:27:03.040]   It's probably worth briefly going over because I think people have this wrong.
[00:27:03.040 --> 00:27:08.160]   And when you have it wrong, it changes the way you think about this world and what to do about the harms of this world.
[00:27:08.160 --> 00:27:10.160]   I looked up a quote here.
[00:27:10.160 --> 00:27:16.120]   I think this sort of establishes the way that we kind of get this wrong.
[00:27:16.260 --> 00:27:23.400]   So here's a quote about TikTok that comes from the founder of an organization called Algo Transparency, which I think kind of gives the wrong idea.
[00:27:23.400 --> 00:27:25.100]   But let me read this and then I'll give you the right idea.
[00:27:25.100 --> 00:27:26.540]   So this quote says,
[00:27:26.540 --> 00:27:31.100]   Each video a kid watches, TikTok gains a piece of information on him.
[00:27:31.100 --> 00:27:38.960]   In a few hours, the algorithm can detect his musical taste, his physical attraction, if he's depressed, if he might be into drugs, and many other sensitive information.
[00:27:38.960 --> 00:27:42.440]   There's a high risk that some of this information will be used against him.
[00:27:42.440 --> 00:27:45.840]   It could potentially be used to micro-target him or make him more addicted to the platform.
[00:27:46.580 --> 00:27:48.060]   Technically, all of that is true.
[00:27:48.060 --> 00:27:55.280]   But I think the way that we talk about algorithms here gives us an incomplete understanding of how these things actually work.
[00:27:55.280 --> 00:28:02.700]   I think when we see those type of description of algorithms, what we really imagine is sort of like someone making a list of descriptive information about a user.
[00:28:02.700 --> 00:28:03.580]   Oh, we've learned that.
[00:28:03.580 --> 00:28:04.140]   We've learned that.
[00:28:04.140 --> 00:28:05.480]   He's into this, not into that.
[00:28:05.600 --> 00:28:16.120]   We imagine an algorithm sort of like looking at these semantically rich descriptions of things you're into or not into and deciding according to some sort of complicated rules that someone programmed in.
[00:28:16.120 --> 00:28:16.860]   All right.
[00:28:16.860 --> 00:28:20.440]   Well, why don't we then take this and mix it with that?
[00:28:20.440 --> 00:28:22.660]   And we have some good stuff of that over here.
[00:28:22.660 --> 00:28:24.920]   And we're trying to emphasize this type of content.
[00:28:24.920 --> 00:28:26.320]   Oh, and I've analyzed.
[00:28:26.320 --> 00:28:29.820]   I think this type of content feels like people like it.
[00:28:29.820 --> 00:28:31.960]   And so it's like a network scheduler.
[00:28:31.960 --> 00:28:33.140]   Let's show him more of that.
[00:28:33.140 --> 00:28:40.820]   So I think that image of an algorithm, it's like a network scheduler looking at a board full of pieces of information on index cards and making a decision.
[00:28:40.820 --> 00:28:42.180]   Like, well, what are we going to schedule next?
[00:28:42.180 --> 00:28:43.860]   I think this is really going to catch his attention.
[00:28:43.860 --> 00:28:48.040]   We have that idea, which is sort of personified.
[00:28:48.040 --> 00:29:05.760]   We imagine like there's a human involved and it gives us the sense that we can turn these various knobs and say things like, hey, adjust your algorithm to not recommend stuff that is going to be so addictive or adjust your algorithm to like turn down the outrage meter from like a 10 to a 9.
[00:29:05.760 --> 00:29:13.300]   When we think about the algorithm as like a person with all this information, making scheduling decisions, we think we have all this control over it.
[00:29:13.300 --> 00:29:15.500]   That's not really how this works.
[00:29:15.500 --> 00:29:20.180]   So I'm going to try here to draw a picture for those who are watching instead of just listening and God help us all for me doing this.
[00:29:20.180 --> 00:29:26.820]   But I want to give you a better sense of how the general class of algorithms that we think powers TikTok actually works.
[00:29:26.820 --> 00:29:33.800]   So you can imagine what we do is that every time you watch a video, there's a piece of information that's gathered.
[00:29:33.800 --> 00:29:39.540]   But what's gathered is a description of the video.
[00:29:39.540 --> 00:29:42.720]   This description, though, is going to be a bunch of numbers.
[00:29:42.720 --> 00:29:48.060]   So there's going to be, you could think of it like a lot of different categories you could use to apply to a video.
[00:29:48.060 --> 00:29:51.520]   You know, cats could be a category.
[00:29:51.520 --> 00:29:54.520]   If it has a lot of cats, that number is high or low or something like this, right?
[00:29:54.520 --> 00:30:00.960]   And so you have different numbers in each of these categories sort of gives you a list of numbers that describes the video.
[00:30:01.060 --> 00:30:13.360]   So if we had, for example, just two numbers, we could imagine having a flat surface like a two-dimensional plane where you could put a, each spot in here.
[00:30:13.360 --> 00:30:21.960]   Like if we put a spot right here is associated with two numbers, like with that number and with that number, right?
[00:30:22.280 --> 00:30:27.740]   So you can imagine what we do is every time you watch a video, we can put a little dot on here that represents its description.
[00:30:27.740 --> 00:30:36.360]   And we can make that dot bigger or smaller depending, and this seems to be what's most crucial for TikTok, how long you watch it.
[00:30:36.360 --> 00:30:39.040]   Like what percentage of the video you actually watched.
[00:30:39.040 --> 00:30:41.940]   So like maybe you watched a video over here real briefly, small dot.
[00:30:42.440 --> 00:30:44.920]   Over here, though, there's a video you watched all the way through.
[00:30:44.920 --> 00:30:53.680]   There's like a big dot over there, and maybe there's a medium dot, and you kind of sampled some videos over here, right?
[00:30:53.680 --> 00:31:01.340]   And over time, we're kind of just giving you videos to watch, and we see how long are you watching each of them.
[00:31:01.340 --> 00:31:02.800]   So now we have a bunch of these numbers.
[00:31:02.800 --> 00:31:03.660]   We have this plot.
[00:31:03.660 --> 00:31:08.400]   Technically, it's going to be a sort of weighted multidimensional scatter plot.
[00:31:09.680 --> 00:31:14.880]   And now when we want to show you a new video, we being TikTok, the algorithm is super mathematical.
[00:31:14.880 --> 00:31:26.760]   It says, okay, I'm just going to randomly choose a video in this spot of possible videos to show you, but I'm not going to select from every area equally likely.
[00:31:26.760 --> 00:31:33.940]   The more sort of green real estate there is in a certain place, like the more I'm going to be pulled in that direction.
[00:31:33.940 --> 00:31:38.920]   So like I might select something over here, but I'm way more likely to select something over here,
[00:31:38.920 --> 00:31:42.140]   because there's so many, not only just so many other dots, but so many big dots.
[00:31:42.140 --> 00:31:46.040]   There's a lot of watch time over in this space of the possible videos.
[00:31:46.040 --> 00:31:51.300]   And what we're implicitly doing here is kind of creating a region.
[00:31:51.300 --> 00:31:55.180]   These are all going to be probably roughly similarly types of videos.
[00:31:55.180 --> 00:31:57.560]   Now, again, my algorithm is doing something very simple.
[00:31:57.560 --> 00:31:59.580]   It's just randomly choose a thing, but wait your choice.
[00:31:59.580 --> 00:32:04.260]   You know, the more stuff there is nearby it, more mass you want to put on it.
[00:32:04.260 --> 00:32:06.440]   And there's like a lot of probabilistic ways you could do that.
[00:32:06.700 --> 00:32:10.260]   So I'm going to start selecting more videos near stuff you already like.
[00:32:10.260 --> 00:32:13.620]   But like occasionally, you know, you roll a dice enough time.
[00:32:13.620 --> 00:32:15.340]   Sometimes you're going to roll six, three times in a row.
[00:32:15.340 --> 00:32:18.960]   Occasionally I'll pull some stuff over here as well.
[00:32:19.400 --> 00:32:27.220]   And maybe what I'll do, and we think that the TikTok algorithm does this on purpose, it will on a semi-regular basis say forget the weights.
[00:32:27.220 --> 00:32:32.720]   And let me just like purely draw randomly just to make sure there's not something we haven't tried yet that you really like.
[00:32:32.780 --> 00:32:37.380]   And then like maybe over here, there is like a whole bunch of Cal network memes.
[00:32:37.380 --> 00:32:41.920]   And immediately these start getting watched well.
[00:32:42.380 --> 00:32:49.460]   And now it's like you're getting a lot of Cal network memes and then like a lot of the times of the stuff over here.
[00:32:49.460 --> 00:32:51.600]   And occasionally we'll sample other types of things.
[00:32:51.600 --> 00:32:54.360]   That's what's happening underneath the covers.
[00:32:54.360 --> 00:32:55.980]   It's not just two numbers though.
[00:32:55.980 --> 00:32:58.240]   It'll be like a hundred numbers or a thousand numbers.
[00:32:58.340 --> 00:33:01.420]   So this is each of these spots is in a thousand dimensional space.
[00:33:01.420 --> 00:33:04.220]   So the math is different.
[00:33:04.220 --> 00:33:05.320]   But it's the same idea.
[00:33:05.320 --> 00:33:08.980]   What's key about this is the algorithm is stupid.
[00:33:08.980 --> 00:33:15.600]   The algorithm, there is no complicated code in there that humans wrote that's like how do I balance this versus that.
[00:33:15.600 --> 00:33:19.440]   It doesn't label things necessarily in the way you think.
[00:33:19.440 --> 00:33:21.140]   Like is this outrageous or not?
[00:33:21.140 --> 00:33:24.720]   I mean it does categorization and try to get rid of stuff for safety purposes.
[00:33:24.720 --> 00:33:31.880]   But like it really just mainly treats stuff like these numbers and it's just blindly doing this simple repetitive loop.
[00:33:31.880 --> 00:33:36.040]   Pick another video probabilistically from the space of videos.
[00:33:36.040 --> 00:33:38.500]   Weight your choice towards stuff that's already been watched.
[00:33:38.500 --> 00:33:43.780]   So you're more likely to choose something near a bunch of other stuff that's been watched a while than other stuff.
[00:33:43.780 --> 00:33:50.860]   On a regular basis though, randomly just choose something completely different to make sure that we're exploring other types of videos they might like.
[00:33:51.200 --> 00:34:01.280]   This simple loop run tight enough for the only input it's getting from you is how long did you watch each video and how do we automatically categorize the video with some sort of neural net.
[00:34:01.280 --> 00:34:09.640]   That is enough to like within 30 or 40 minutes be eerily accurate in showing you stuff that you really like.
[00:34:10.400 --> 00:34:17.700]   Without there ever having to be anywhere like an English language list of like topics and here's what this person likes and complicated algorithms.
[00:34:17.700 --> 00:34:25.860]   So it's a very simple weighted random sampling algorithm with a little bit of exploration or exploitation types of logic.
[00:34:25.860 --> 00:34:26.840]   It's called multi-arm band.
[00:34:26.840 --> 00:34:28.600]   It's called stochastic optimization.
[00:34:28.600 --> 00:34:29.540]   You don't have to worry about that.
[00:34:29.540 --> 00:34:31.260]   But like occasionally sampling other stuff.
[00:34:31.480 --> 00:34:37.080]   It's a very simple algorithm run again and again and again that has an eerily effective complex effect.
[00:34:37.080 --> 00:34:43.720]   So anyways, I think that's interesting to know because it tells us that you can't control these things as much as you think.
[00:34:43.720 --> 00:34:44.740]   You can filter.
[00:34:44.740 --> 00:34:53.820]   Like you can say, hey, before we even recommend something, we can like filter as just like really inappropriate content and just like let's not recommend that at all.
[00:34:55.380 --> 00:35:05.980]   You could like manually put shadow bands on things like, okay, anything with this category, I'm going to type into my system, like minimize its weight so we're much less likely to show other stuff like that.
[00:35:05.980 --> 00:35:06.620]   You can do that.
[00:35:06.620 --> 00:35:11.180]   But there is, for the most part, you don't have knobs to turn, right?
[00:35:11.180 --> 00:35:13.820]   You just say sample stuff near stuff they like.
[00:35:13.820 --> 00:35:15.540]   TikTok works really well.
[00:35:15.540 --> 00:35:17.080]   You can keep that on.
[00:35:17.080 --> 00:35:17.840]   You can turn it off.
[00:35:17.840 --> 00:35:18.440]   That's it.
[00:35:18.440 --> 00:35:21.280]   Like we do not have the control over these algorithms that people think.
[00:35:21.280 --> 00:35:26.860]   So anyways, I think that's an important insight that I want to give there is a lot of these sort of recommendation algorithms.
[00:35:26.860 --> 00:35:33.160]   The math might be complicated, but the algorithmic logic is not nearly as adjustable as you might think.
[00:35:33.160 --> 00:35:42.640]   A simple routine done again and again with more and more data can work eerily well, and it's pretty hard to have any sort of nuanced control over it.
[00:35:42.640 --> 00:35:44.600]   So there you go.
[00:35:44.600 --> 00:35:48.280]   I still have TikTok on my phone, Jesse, from that New Yorker article I wrote.
[00:35:48.280 --> 00:35:49.360]   Have you been on it?
[00:35:49.360 --> 00:35:50.360]   No, I have no interest.
[00:35:50.360 --> 00:35:51.140]   It's so weird.
[00:35:51.140 --> 00:35:51.520]   Here, look.
[00:35:51.520 --> 00:35:52.340]   I'm loading it up right now.
[00:35:52.340 --> 00:35:53.440]   It's such a weird app.
[00:35:53.440 --> 00:35:58.720]   If you're not native to this, if you come to it just for like an article like I did, let's see if this works.
[00:35:58.720 --> 00:35:59.400]   All right.
[00:35:59.400 --> 00:36:00.660]   I'm loading up TikTok on my phone.
[00:36:00.660 --> 00:36:02.260]   I haven't been on here since January.
[00:36:02.260 --> 00:36:05.500]   Don't allow.
[00:36:05.500 --> 00:36:08.980]   There's a woman singing.
[00:36:08.980 --> 00:36:15.080]   They're like taking a shirtless guy is taking blankets off of her.
[00:36:15.740 --> 00:36:23.240]   And now she's in like a fancy dress and there's confetti and the guy is blowing air on her.
[00:36:23.240 --> 00:36:24.960]   Jesse, what are we doing here?
[00:36:26.560 --> 00:36:30.360]   When you see your wife and three daughters, see you OTW home.
[00:36:30.360 --> 00:36:33.920]   And the kids are giving the middle finger.
[00:36:33.920 --> 00:36:35.420]   What are we doing?
[00:36:35.420 --> 00:36:36.360]   Is this civilization?
[00:36:36.360 --> 00:36:39.560]   Turn long videos.
[00:36:39.720 --> 00:36:40.260]   Oh my God.
[00:36:40.260 --> 00:36:42.840]   Is this video appropriate for TikTok?
[00:36:42.840 --> 00:36:43.840]   Oh, they're getting data.
[00:36:43.840 --> 00:36:45.900]   They're getting data.
[00:36:45.900 --> 00:36:46.700]   They're forcing me to.
[00:36:46.700 --> 00:36:47.580]   All right.
[00:36:47.580 --> 00:36:48.080]   One more.
[00:36:48.080 --> 00:36:53.300]   When your teacher used to call home, someone walks into a room.
[00:36:54.380 --> 00:37:01.400]   Oh, it's a skit about like a guy putting on lots of pants because his dad is going to hit
[00:37:01.400 --> 00:37:03.040]   him with a belt for being in trouble at school.
[00:37:03.040 --> 00:37:03.960]   It's a comedy skit.
[00:37:03.960 --> 00:37:04.240]   All right.
[00:37:04.240 --> 00:37:04.600]   Come on.
[00:37:04.600 --> 00:37:08.300]   Anyways, that's TikTok.
[00:37:08.300 --> 00:37:08.840]   All right.
[00:37:08.840 --> 00:37:09.780]   I don't understand that, Jesse.
[00:37:09.780 --> 00:37:11.280]   But if I watch that long enough.
[00:37:11.280 --> 00:37:13.100]   So that's random because I don't use it.
[00:37:13.100 --> 00:37:16.460]   So that's just randomly sampling from like all videos.
[00:37:16.460 --> 00:37:21.600]   But I'm telling you, if I watch this for like, I don't know, 20 minutes, it's all
[00:37:21.600 --> 00:37:22.280]   going to, I don't know what it would be.
[00:37:22.280 --> 00:37:25.260]   It'd be like a bunch of like Washington Nationals content, I guess.
[00:37:25.260 --> 00:37:26.340]   I don't know what else would be on there.
[00:37:26.340 --> 00:37:31.800]   It'd be like Washington Nationals content and Cal Network memes, I guess.
[00:37:31.800 --> 00:37:33.040]   I don't know.
[00:37:33.040 --> 00:37:35.840]   But that's how that works.
[00:37:35.840 --> 00:37:38.040]   These algorithms just like, it's like a virus.
[00:37:38.040 --> 00:37:38.580]   They're simple.
[00:37:38.580 --> 00:37:43.580]   But you let that process run long enough and fast enough, it can cause like complicated,
[00:37:43.580 --> 00:37:44.360]   terrible effects.
[00:37:44.360 --> 00:37:45.940]   So there we go.
[00:37:45.940 --> 00:37:47.980]   TikTok.
[00:37:47.980 --> 00:37:48.920]   All right.
[00:37:48.920 --> 00:37:49.660]   What do we got next?
[00:37:49.660 --> 00:37:51.400]   All right.
[00:37:51.400 --> 00:37:52.860]   Gene's next.
[00:37:52.860 --> 00:37:57.040]   What are the best strategies for workflow when using AI agents?
[00:37:57.040 --> 00:38:00.200]   And we have an article that you can react to and we can show the audience.
[00:38:00.200 --> 00:38:00.680]   Oh, okay.
[00:38:00.680 --> 00:38:01.280]   You sent an article?
[00:38:01.280 --> 00:38:02.040]   Yep.
[00:38:02.040 --> 00:38:02.620]   All right.
[00:38:02.620 --> 00:38:04.460]   I'll pull this on the screen for those who are watching.
[00:38:04.460 --> 00:38:04.880]   All right.
[00:38:04.880 --> 00:38:11.640]   The article, waiting for agents shouldn't kill your momentum.
[00:38:11.640 --> 00:38:15.900]   This post breaks down why no agents and too many agents both stall progress.
[00:38:15.900 --> 00:38:19.380]   Preston shares a sweet spot workflow to keep you locked in flow.
[00:38:19.380 --> 00:38:20.720]   All right.
[00:38:20.720 --> 00:38:22.780]   So the first heading here says waiting kills flow.
[00:38:22.780 --> 00:38:26.360]   Now, looking closer at this animation, I'll make this bigger here for those who are watching.
[00:38:27.000 --> 00:38:44.120]   What they are showing here is an AI agent working in what is at the moment the only context that I know of where something like agentic AI code is actually not just useful, but being somewhat largely applied, which is in computer programming.
[00:38:44.120 --> 00:38:50.800]   So the AI agent that's being shown on the screen here is an AI agent that is helping you write software.
[00:38:51.400 --> 00:38:59.680]   So this is, I think, right now the perhaps the only major application of agentic AI that I know of.
[00:38:59.680 --> 00:39:06.220]   And again, agentic AI is where you have control software surrounding the use of something like a language model.
[00:39:06.220 --> 00:39:09.560]   So it can query the language model for an idea or a plan.
[00:39:09.560 --> 00:39:14.280]   And then the control software can look at the answer and then take actions based on the answer.
[00:39:14.500 --> 00:39:18.240]   So when it comes to computer programming, here's another example here for those who are watching.
[00:39:18.240 --> 00:39:36.740]   When it comes to building like a semi-complicated vibe coding style computer program, you can have these agentic code like Cursor, Windsurf, or Roo, where what you're doing is it'll like first have the language model give you like a bunch of steps you need to do to build a software product.
[00:39:36.740 --> 00:39:38.940]   And then it'll be like, okay, you want us to do step one?
[00:39:38.940 --> 00:39:39.800]   And you say, yes.
[00:39:39.800 --> 00:39:40.720]   And it's like, great.
[00:39:40.720 --> 00:39:45.960]   And then the software will then query the language model to get the code for step one.
[00:39:45.960 --> 00:39:49.340]   And then it'll run it and be like, oh, there's errors.
[00:39:49.340 --> 00:39:50.760]   You want us to like try to fix it?
[00:39:50.760 --> 00:39:51.200]   Like, yeah.
[00:39:51.200 --> 00:39:57.040]   And you kind of work on step one and the agent sort of makes queries on the language model on your behalf until that step has working code.
[00:39:57.040 --> 00:39:59.020]   And then it's like, you want to go to step two and you go to step two.
[00:39:59.020 --> 00:40:04.520]   And it allows you to build more complicated software than if you just ask the language model, give me code for this.
[00:40:04.520 --> 00:40:08.440]   Like if you say, give me code, it has to be a relatively small thing it solves.
[00:40:09.000 --> 00:40:12.840]   But if you do this multi-step thing, Agendic AI lets you do more things.
[00:40:12.840 --> 00:40:14.920]   So the more complicated projects.
[00:40:14.920 --> 00:40:22.680]   So the argument here is that, and I've seen this, I've sat next to people that showed me them using these systems.
[00:40:22.680 --> 00:40:25.200]   There's a lot of waiting, right?
[00:40:25.200 --> 00:40:32.680]   Because these language models aren't fast to query because, you know, you might have 500 billion parameters being executed by multiple GPUs.
[00:40:32.680 --> 00:40:33.220]   There's queues.
[00:40:33.560 --> 00:40:34.320]   It's slow.
[00:40:34.320 --> 00:40:39.640]   So all of these animations I showed you, a lot of it is like thinking line by line.
[00:40:39.640 --> 00:40:40.560]   You see things happening.
[00:40:40.560 --> 00:40:48.320]   So this whole article is like, how do you not get too distracted while you're waiting for your agentic software when you're building software to like finish thinking?
[00:40:49.120 --> 00:41:02.580]   And I think they are fans of my work because they talk about if you're jumping over to social media or like your phone, man, you're going to get so context shifted and overloaded that you're going to, your brain's going to burn out.
[00:41:02.780 --> 00:41:17.700]   So then they argue here, like what you should do, and here's their solution is you should have something like an obsidian powered to do list next to your coding so that when this starts thinking, you can go over here and work on something productive.
[00:41:18.060 --> 00:41:27.320]   And then when it's done, go back over here and then you're never really leaving the context of the project, but you're also not just sitting there bored.
[00:41:28.240 --> 00:41:30.880]   So I don't know, there's something I like and something I dislike about this.
[00:41:30.880 --> 00:41:35.560]   I like their general application of, hey, be wary of context shifts.
[00:41:35.560 --> 00:41:43.640]   We have this idea of deep breaks where like if you want to take a mini break from what you're doing, there's ways to do it so that you don't fry your mental circuit.
[00:41:43.640 --> 00:41:48.780]   So working on the to-do list for the larger project keeps you within like the general cognitive context.
[00:41:49.000 --> 00:41:55.480]   But the sort of less exciting side of this example is like this is kind of it for agentic AI right now.
[00:41:55.480 --> 00:41:59.500]   So I'm still, we talked about this last week at the end of the show.
[00:41:59.500 --> 00:42:04.120]   I talked about this in my recent email newsletter post AI and work, which you can find at calnewport.com.
[00:42:04.120 --> 00:42:09.160]   We're not sure how many applications we're going to have for agentic AI.
[00:42:09.160 --> 00:42:12.820]   Open AI says everything's going to be done by agents and we're never going to have to have jobs again.
[00:42:12.820 --> 00:42:16.280]   And it's going to somehow generate like $20 billion like next year.
[00:42:16.900 --> 00:42:23.640]   But there's a lot of technical obstacles to applying this agentic approach to other types of work that we do.
[00:42:23.640 --> 00:42:29.600]   That's not just like building somewhat subtly constructed small size software applications.
[00:42:29.600 --> 00:42:32.400]   So there's a bunch of stuff going on here.
[00:42:32.400 --> 00:42:33.680]   I think it's an interesting application.
[00:42:33.680 --> 00:42:38.800]   So yeah, if you're coding with agentic AI, this is actually like a new problem we haven't had before.
[00:42:38.800 --> 00:42:42.980]   What do you do when you're waiting for the, actually we have had this before, it's compile time.
[00:42:42.980 --> 00:42:44.880]   I take that back.
[00:42:44.880 --> 00:42:49.340]   This is a common problem for computer programmers is computers didn't used to be super powerful.
[00:42:49.340 --> 00:42:53.560]   So every time you change your code to rerun it, you had to compile and rebuild the project.
[00:42:53.560 --> 00:42:56.380]   And it takes a while to test to see if it works.
[00:42:56.380 --> 00:42:58.080]   And so I actually, you know what?
[00:42:58.080 --> 00:42:58.640]   I take it back.
[00:42:58.640 --> 00:43:00.020]   Programmers have been talking about this forever.
[00:43:00.020 --> 00:43:01.580]   What do you do while you're waiting to compile?
[00:43:01.580 --> 00:43:03.620]   I think we've had questions like this before on the show.
[00:43:03.760 --> 00:43:07.160]   So there's nothing new here, but I think it's a good insight, at least from an AI perspective.
[00:43:07.160 --> 00:43:12.560]   This is like one of the places, if not the only place where agentic AI is kind of doing interesting things.
[00:43:12.560 --> 00:43:15.700]   So it'll be interesting to see if we get other places going forward.
[00:43:16.620 --> 00:43:19.960]   We don't yet have our podcast producer, agentic AI.
[00:43:19.960 --> 00:43:22.580]   I think it would take this podcast in weird places.
[00:43:22.580 --> 00:43:30.520]   It'd be a lot of like Cal Network and Jesse Skeleton, weird memes, like cutting back and forth, going to CrossFit gyms.
[00:43:31.080 --> 00:43:33.860]   And it would be a massive success.
[00:43:33.860 --> 00:43:36.540]   Not there yet, though.
[00:43:36.540 --> 00:43:37.620]   All right.
[00:43:37.620 --> 00:43:39.240]   Looks like we have a case study this week.
[00:43:39.240 --> 00:43:45.040]   It's where people send in their examples of using the type of advice I talk about this show in their own life.
[00:43:45.040 --> 00:43:47.100]   So we can see what it looks like in the real world.
[00:43:47.100 --> 00:43:50.960]   If you have a case study to share, send it to jesse at calnewport.com.
[00:43:50.960 --> 00:43:52.320]   All right.
[00:43:52.320 --> 00:43:53.940]   Today's case study comes from Adam.
[00:43:53.940 --> 00:43:55.700]   Adam says,
[00:43:56.600 --> 00:44:01.960]   While walking our manufacturing floor the other day, I struck up a conversation with one of our maintenance technicians.
[00:44:01.960 --> 00:44:05.840]   I noticed he had a small, well-worn notepad he kept referring to.
[00:44:05.840 --> 00:44:08.680]   Out of curiosity, I asked him what he was writing down.
[00:44:08.680 --> 00:44:11.360]   He flipped through the pages and casually said,
[00:44:11.360 --> 00:44:14.740]   I plan out my year hour by hour.
[00:44:14.740 --> 00:44:21.260]   I asked what he meant by that, and he showed me how he divided each page into days with lines for each hour of his shift.
[00:44:21.260 --> 00:44:25.700]   Each hour had a specific task or category of responsibility tied to it.
[00:44:26.240 --> 00:44:30.860]   Things like preventative maintenance, troubleshooting, parts ordering, and time for training or documenting issues.
[00:44:30.860 --> 00:44:37.300]   He explained that without this system, he'd never be able to keep up with all the side projects he and his managers were working on,
[00:44:37.300 --> 00:44:39.600]   on top of his normal day-to-day tasks.
[00:44:39.600 --> 00:44:40.880]   What struck me was his clarity.
[00:44:40.880 --> 00:44:44.680]   If I don't write it all out like this, I'll get pulled into too many directions, never get ahead.
[00:44:44.680 --> 00:44:48.780]   He said he even has a similar version for his personal life.
[00:44:48.780 --> 00:44:56.580]   I told him that what he was doing had a name, time-blocking, and that people like you talk about this method in books and podcasts.
[00:44:56.580 --> 00:45:02.620]   He proceeded to give me a severe concussion by beating me around the head with a socket wrench.
[00:45:03.420 --> 00:45:04.840]   All right, I added that last part.
[00:45:04.840 --> 00:45:16.080]   But I was just imagining lecturing a maintenance technician about, excuse me, but there's a podcast in which this technique has been thoroughly discussed.
[00:45:17.820 --> 00:45:22.760]   You're going to get a steel-toed work boot to the backside.
[00:45:22.760 --> 00:45:24.920]   All right, let me go back to what he actually said.
[00:45:24.920 --> 00:45:27.260]   He said he'd never heard the term before.
[00:45:27.260 --> 00:45:29.980]   It was just the only thing that kept him sane.
[00:45:30.380 --> 00:45:36.340]   This conversation was a grounding reminder for me as someone who's constantly tempted to over-optimize their productivity system.
[00:45:36.340 --> 00:45:39.820]   It was refreshing to see someone use a simple analog approach to stay focused and intentional.
[00:45:39.820 --> 00:45:45.120]   No apps, no elaborate dashboards, just pen and paper, an approach rooted in necessity and clarity.
[00:45:45.120 --> 00:45:48.560]   I'll tell you what article this guy probably did not read.
[00:45:48.560 --> 00:45:58.560]   The article on how to make progress on your Obsidian open source to-do list while waiting for your agentic AI to make its inference times complete.
[00:45:59.840 --> 00:46:03.020]   The maintenance technician was probably not reading that article, and he's the better for it.
[00:46:03.020 --> 00:46:04.740]   I like that example.
[00:46:04.740 --> 00:46:05.680]   Look, it's simple.
[00:46:05.680 --> 00:46:07.720]   When you're at work, you have a certain amount of time.
[00:46:07.720 --> 00:46:10.060]   That's what you have to allocate to doing things.
[00:46:10.060 --> 00:46:22.020]   The whole game is allocating that to the right things that solves the dual problem of you making progress on things that are sufficiently valued to keep you with a lot of career capital while also not burning you out or making you upset.
[00:46:22.020 --> 00:46:23.120]   That's the whole game.
[00:46:23.120 --> 00:46:23.760]   That's it.
[00:46:23.760 --> 00:46:27.200]   How you do that, right?
[00:46:27.200 --> 00:46:29.000]   It's like whatever works for you.
[00:46:29.000 --> 00:46:31.400]   It depends on the job, but whatever.
[00:46:31.400 --> 00:46:37.220]   But if you're not having some sort of system, this is how I win at that game, you're not winning at it.
[00:46:37.220 --> 00:46:47.660]   If you just rock and roll, here's my email, here's my Slack, there's stuff coming in and out, there's all these meeting invites coming at me, I'm trying to optimize my Obsidian setup while I wait for the inference time on my agentic AI.
[00:46:47.660 --> 00:46:50.780]   You're just doing all this stuff, but you don't have a clear answer to the question.
[00:46:50.780 --> 00:46:59.440]   How do you choose what to do with the time you have so as to achieve these two goals, capital and sustainability?
[00:46:59.800 --> 00:47:10.120]   If you do not have a specific answer to that, you are being pushed around by forces outside of your control, you're operating at a fraction of your capacity, you're probably exhausting your brain.
[00:47:10.120 --> 00:47:11.880]   So that's a great example.
[00:47:11.880 --> 00:47:12.980]   I love the simplicity of it.
[00:47:13.020 --> 00:47:19.040]   In the end, that's what it comes down to, and it doesn't necessarily require all that much technology to actually get it done.
[00:47:19.040 --> 00:47:21.160]   All right.
[00:47:21.160 --> 00:47:21.700]   Do we have a call?
[00:47:21.700 --> 00:47:22.320]   We do.
[00:47:22.320 --> 00:47:22.820]   All right.
[00:47:22.820 --> 00:47:24.080]   Let's hear it.
[00:47:25.680 --> 00:47:26.320]   Hi, Cal.
[00:47:26.320 --> 00:47:34.560]   My name is Emil Foligno, and I've been a lecturer within computer science at the Swedish Higher Education Institute for the past eight plus years.
[00:47:34.560 --> 00:47:44.980]   I've used some of your approaches to deep work and time management to become great at teaching, and was promoted to expert lecturer a couple of years ago.
[00:47:44.980 --> 00:47:53.140]   I've now invested some of my career capital and will start doctoral studies in January, but with my lecturer's salary.
[00:47:54.020 --> 00:47:59.220]   To be able to obsess over quality, I will need to develop a new set of skills related to research.
[00:47:59.220 --> 00:48:05.800]   As an experienced researcher, what general and specific skills would you recommend that I hone in on?
[00:48:05.800 --> 00:48:07.840]   All right.
[00:48:07.840 --> 00:48:08.560]   That's a great question.
[00:48:08.560 --> 00:48:10.240]   But first, congratulations.
[00:48:10.240 --> 00:48:13.560]   You worked a career capital system well.
[00:48:13.560 --> 00:48:22.000]   Now you're going for your doctorate, but at a salary and not at like a research assistant salary or, you know, you've worked it well.
[00:48:22.000 --> 00:48:23.220]   You became an expert lecturer.
[00:48:23.360 --> 00:48:24.620]   You have a connection to this university.
[00:48:24.620 --> 00:48:25.760]   You can do a salaried program.
[00:48:25.760 --> 00:48:27.460]   It's probably going to open up slots for you when you're done.
[00:48:27.460 --> 00:48:30.840]   So well played the way you've done this so far.
[00:48:30.840 --> 00:48:31.660]   All right.
[00:48:31.660 --> 00:48:37.720]   When it comes to how to succeed in this doctoral program, you need expert guidance.
[00:48:37.720 --> 00:48:40.720]   So you fortunately have this built in, your advisor.
[00:48:42.180 --> 00:48:47.700]   Choose an advisor who's doing good work in their field and it's a field that you think is interesting and has value.
[00:48:47.700 --> 00:48:50.160]   And then say, I want to learn how to do this.
[00:48:50.160 --> 00:48:52.040]   Let me, right off the bat, let me write it.
[00:48:52.040 --> 00:48:53.000]   Let me be useful to you.
[00:48:53.000 --> 00:48:54.280]   What paper are you working on now?
[00:48:55.440 --> 00:49:03.860]   I want to learn about it and I want to be useful and I want to write papers with you right off the bat to learn how it happens, to learn how it happens.
[00:49:03.860 --> 00:49:04.820]   And this is what I did.
[00:49:04.860 --> 00:49:08.180]   So a little bit of a brief divergence here.
[00:49:08.180 --> 00:49:11.220]   When I first arrived, so I arrived in a theory group.
[00:49:11.220 --> 00:49:16.380]   I was working in the theory of distributed systems group, in the theory of computation group at MIT.
[00:49:17.660 --> 00:49:25.240]   And technically, I was sort of hired to do systems work for this theory group, but I wanted to learn how to do theory.
[00:49:25.240 --> 00:49:35.920]   And I remember the first thing I did when I got there is there was a paper they were working on for a good conference, a conference I ended up down the line, publishing a ton of papers at and being in the steering committee for.
[00:49:35.920 --> 00:49:39.540]   But I said, how can I help on this paper?
[00:49:39.540 --> 00:49:40.780]   Here's what I know how to do.
[00:49:40.780 --> 00:49:45.540]   I just want to be involved in this paper because I want to learn how people write papers for this good conference.
[00:49:45.540 --> 00:49:47.280]   I don't want to write papers for less conferences.
[00:49:47.920 --> 00:49:51.620]   I don't want to go on my own and start writing stuff for like secondary conferences and trying to figure this out.
[00:49:51.620 --> 00:49:52.920]   I want to publish in the best places.
[00:49:52.920 --> 00:49:54.100]   I said, here's what I can do.
[00:49:54.100 --> 00:49:55.260]   I'm a kind of a systemsy guy.
[00:49:55.260 --> 00:49:57.460]   I can build like simulations and stuff like this.
[00:49:57.460 --> 00:50:00.420]   I could add a simulation sort of section here.
[00:50:00.420 --> 00:50:01.420]   I'll figure it out.
[00:50:01.420 --> 00:50:04.680]   It'd be like a nice, you know, half page to your paper.
[00:50:04.680 --> 00:50:06.360]   Like, I find you can do that for us.
[00:50:06.360 --> 00:50:11.860]   And now I was in the door and I was involved in this paper and I was there and I saw every draft and I saw how it worked.
[00:50:11.860 --> 00:50:17.160]   And when the next paper came up, which was like a follow up, I was like, well, here's a corner.
[00:50:17.160 --> 00:50:18.820]   It's the corner of the theory that I think I can do.
[00:50:18.820 --> 00:50:19.720]   It's not too hard.
[00:50:19.720 --> 00:50:27.220]   And so, like, I found a way to be useful on the high quality stuff, a small handhold on a big swing project.
[00:50:27.220 --> 00:50:33.920]   And I made those handholds bigger and bigger until after a year or so, I was like, now I know what goes into writing these papers and I could really take off.
[00:50:34.300 --> 00:50:40.520]   And now I wasn't dependent on waiting for other people to be writing a paper to work on and I could start my own and I published a lot of papers.
[00:50:40.740 --> 00:50:43.360]   So, work with your advisor on the best possible papers.
[00:50:43.360 --> 00:50:49.320]   That is the skill you want to learn and the only way to learn it is to work on really good papers with people who know how to publish them.
[00:50:49.320 --> 00:50:50.660]   Don't do anything else.
[00:50:51.960 --> 00:50:52.380]   All right.
[00:50:52.380 --> 00:51:05.300]   So, we got a final segment coming up where I will go over the books I read in May as well as looking at this truly brilliant piece of fan art that we're going to have to go through years of therapy, Jesse.
[00:51:05.660 --> 00:51:09.740]   I think, after discussing this, it's just that strange and just that wonderful.
[00:51:09.740 --> 00:51:13.720]   But first, let's briefly hear from our sponsors.
[00:51:13.720 --> 00:51:17.820]   So, guys, why don't you want to look a little bit younger?
[00:51:17.820 --> 00:51:22.800]   Maybe you want that muscular glow of, say, a Cal Network type.
[00:51:22.800 --> 00:51:26.400]   Maybe get a few more compliments on your skin or just feel more confident when you work in the mirror.
[00:51:26.400 --> 00:51:28.940]   This is what Caldera Labs is here for.
[00:51:28.940 --> 00:51:35.840]   Their high-performance skin care is designed specifically for men, simple, effective, and backed by science.
[00:51:35.840 --> 00:51:41.900]   They have products like The Good, which is an award-winning serum packed with 27 active botanicals.
[00:51:41.900 --> 00:51:42.540]   I counted them.
[00:51:42.540 --> 00:51:45.420]   And 3.4 million antioxidant units per drop.
[00:51:45.420 --> 00:51:46.320]   I counted that as well.
[00:51:46.320 --> 00:51:46.980]   It took me a while.
[00:51:46.980 --> 00:51:48.380]   Probably should have just looked at the literature.
[00:51:48.380 --> 00:51:49.620]   Their eye serum.
[00:51:49.620 --> 00:51:50.500]   I need this.
[00:51:50.500 --> 00:51:56.500]   It helps reduce the appearance of tired eyes, dark circles, and puffiness by using a breakthrough peptide complex.
[00:51:56.500 --> 00:52:07.780]   And, obviously, adaptogens that support skin resilience and base layer, which is a nutrient-wrench moisturizer infused with plant stem cells and snow mushroom extract.
[00:52:07.780 --> 00:52:09.080]   Here's the thing.
[00:52:09.080 --> 00:52:11.440]   Guys don't know that you're supposed to put stuff on your skin.
[00:52:11.440 --> 00:52:18.020]   But if you don't, and we talked about this last time, if you don't, by the time you hit your 40s, you're going to look like George Hamilton.
[00:52:18.020 --> 00:52:19.120]   That's the name, Jesse.
[00:52:19.120 --> 00:52:20.460]   I couldn't remember it last week.
[00:52:20.460 --> 00:52:25.620]   And roughly everybody wrote us to say George Hamilton is the over-tan guy.
[00:52:25.620 --> 00:52:26.500]   It is true.
[00:52:26.500 --> 00:52:27.800]   You're, like, 35 years old.
[00:52:27.800 --> 00:52:28.640]   Your skin looks great.
[00:52:28.640 --> 00:52:29.980]   You're, like, I'm going to live forever.
[00:52:29.980 --> 00:52:37.980]   You turn 40, and if you haven't been putting moisturizer on your face, you suddenly look like you just got back from a 19th century around-the-world whaling voyage.
[00:52:37.980 --> 00:52:42.780]   Old, cracked skin, and you're carving scrimshaw, and you probably have a peg leg.
[00:52:42.780 --> 00:52:45.500]   So, men, you have to put stuff on your face.
[00:52:45.500 --> 00:52:46.900]   Caldera is made for men.
[00:52:46.900 --> 00:52:47.640]   This is good stuff.
[00:52:47.880 --> 00:52:49.360]   I particularly like the base layer.
[00:52:49.360 --> 00:52:51.540]   Keeps my skin moist.
[00:52:51.540 --> 00:52:54.660]   They have cutting-edge formulas, rigorous R&D cycles.
[00:52:54.660 --> 00:52:59.260]   They are non-chalmiodigenic formulas.
[00:52:59.260 --> 00:53:02.780]   I almost get, one day I'm going to get that word right, but that means it doesn't clog your pores, guys.
[00:53:02.780 --> 00:53:03.360]   You don't want that.
[00:53:03.360 --> 00:53:04.820]   Plastic-free, cruelly-free.
[00:53:04.820 --> 00:53:05.940]   So, here's the thing.
[00:53:05.940 --> 00:53:09.600]   Skin care doesn't have to be complicated, but it should be good.
[00:53:10.460 --> 00:53:13.220]   Upgrade your routine with Caldera Lab and see the difference for yourself.
[00:53:13.220 --> 00:53:20.460]   Go to calderalab.com slash deep, and if you use the code deep at checkout, you'll get 20% off your first order.
[00:53:20.460 --> 00:53:23.400]   I also want to talk about our friends at Shopify.
[00:53:23.400 --> 00:53:27.540]   If you run a small business, you know there's nothing small about it.
[00:53:27.540 --> 00:53:29.020]   As a business owner, I get it.
[00:53:29.020 --> 00:53:32.560]   My business is and has always been all-consuming since I started.
[00:53:32.560 --> 00:53:36.760]   Every day, there is a new decision to make, and even the smallest decisions feel massive.
[00:53:36.760 --> 00:53:43.120]   The thing that will help you, most of all, is knowing that you have the right platforms that you need to be successful.
[00:53:43.120 --> 00:53:45.760]   And if you're selling things, that is Shopify.
[00:53:45.760 --> 00:53:48.600]   True story, true story.
[00:53:48.600 --> 00:53:57.680]   Cal Network began selling online, began selling online old socks that he had worn.
[00:53:57.680 --> 00:54:00.040]   He's like, I will sell this for you online.
[00:54:00.040 --> 00:54:03.560]   That business today, Jesse, $97 million a year.
[00:54:04.460 --> 00:54:09.920]   That's the type of success story that you can have, but the logistics will get complicated, but not if you use Shopify.
[00:54:09.920 --> 00:54:16.200]   Shopify is the commerce platform behind millions of businesses around the world, and 10% of all e-commerce in the U.S.
[00:54:16.200 --> 00:54:17.680]   You don't need a big team.
[00:54:17.680 --> 00:54:23.240]   Shopify will help you handle anything from website design to managing inventory to customer service to global shipping.
[00:54:23.240 --> 00:54:29.540]   Look, all the stuff you need to do to successfully sell things, Shopify is there for you.
[00:54:29.540 --> 00:54:30.840]   They can help you market.
[00:54:30.960 --> 00:54:32.060]   They have email list tools.
[00:54:32.060 --> 00:54:38.160]   You want to grow your list, your business in person, point of sale solutions right there that hook up to your online database.
[00:54:38.160 --> 00:54:40.080]   It can hook up to your, your web interface.
[00:54:40.080 --> 00:54:43.960]   Shopify is what you need if you are going to sell thing.
[00:54:44.820 --> 00:54:55.820]   So turn those what ifs into, I was going to say, turn those what ifs into, pause for sound effect, and keep giving those big dreams the best shot with Shopify.
[00:54:55.820 --> 00:55:00.420]   Sign up for your $1 per month trial and start selling today at shopify.com slash deep.
[00:55:00.420 --> 00:55:02.160]   Go to shopify.com slash deep.
[00:55:02.160 --> 00:55:04.900]   Shopify.com slash deep.
[00:55:05.360 --> 00:55:08.100]   All right, Jesse, let's move on to our final segment.
[00:55:08.100 --> 00:55:09.780]   All right.
[00:55:09.780 --> 00:55:19.540]   So before we get into the books that I read last month, let's load up for the people who are watching instead of just listing this artwork that was sent in to us.
[00:55:19.540 --> 00:55:20.020]   Let's see.
[00:55:20.020 --> 00:55:22.120]   I have some text on this somewhere.
[00:55:22.120 --> 00:55:22.940]   Oh, here we go.
[00:55:22.940 --> 00:55:23.220]   All right.
[00:55:23.220 --> 00:55:24.920]   Here's some artwork that was sent in to us.
[00:55:24.920 --> 00:55:25.840]   I'm going to put it on the screen.
[00:55:26.240 --> 00:55:28.880]   Oh my, Jesse, this was commissioned.
[00:55:28.880 --> 00:55:31.640]   This was commissioned artwork.
[00:55:31.640 --> 00:55:35.360]   I don't know if I should be excited or a little bit afraid right now.
[00:55:35.360 --> 00:55:37.940]   I'm trying to find it's in the beginning.
[00:55:37.940 --> 00:55:38.640]   Yeah, I know.
[00:55:38.640 --> 00:55:39.740]   I moved it.
[00:55:39.740 --> 00:55:41.380]   Brian said it.
[00:55:41.380 --> 00:55:42.200]   Here we go.
[00:55:42.200 --> 00:55:44.060]   So here's this artwork.
[00:55:44.060 --> 00:55:45.400]   It's for people who are watching.
[00:55:45.400 --> 00:55:46.580]   God, I don't know how to explain this, Jesse.
[00:55:46.580 --> 00:55:47.800]   I'm just going to have to go.
[00:55:47.800 --> 00:55:49.280]   I'm just going to have to get right into it.
[00:55:49.280 --> 00:56:14.180]   It is a pencil like charcoal drawing of a bunch of men that includes naturally myself, Anthony Edwards of the Minnesota Timberwolves, Columbo, Kirk Fournette, the head coach of the Iowa's Hawkeyes football team, my buddy Brett McKay from the art of manliness, and Keith Morrison of Dateline.
[00:56:14.180 --> 00:56:16.980]   And there's a grill in front of us.
[00:56:16.980 --> 00:56:18.680]   And he's in the middle.
[00:56:18.780 --> 00:56:19.340]   Brent's in the middle.
[00:56:19.340 --> 00:56:22.000]   Yeah, but I'm near the middle.
[00:56:22.000 --> 00:56:22.360]   Oh, yeah.
[00:56:22.360 --> 00:56:22.920]   You're right next to him.
[00:56:22.920 --> 00:56:23.640]   I feel good about this.
[00:56:23.640 --> 00:56:25.000]   I think it's a cool picture.
[00:56:25.000 --> 00:56:27.100]   So Brent just said, like, hey, these are all people.
[00:56:27.100 --> 00:56:28.580]   It's cool what he's doing.
[00:56:28.580 --> 00:56:33.820]   He's like, oh, these are all people for various reasons I think are interesting or cool, and it'd be fun to have an artwork and imagine what they all had a barbecue.
[00:56:33.820 --> 00:56:37.860]   But the thing that kind of freaks me out is having the Dateline guy there, right?
[00:56:37.860 --> 00:56:41.860]   Because this is not exactly what you would hope to see.
[00:56:41.860 --> 00:56:47.080]   You're, like, at a barbecue with all of your older men friends, and then the Dateline guy shows up.
[00:56:47.080 --> 00:56:48.360]   Like, it's trouble.
[00:56:49.040 --> 00:56:52.360]   Like, there's probably a corpse in that grill, if I had to guess.
[00:56:53.500 --> 00:57:00.500]   So it sort of looks like me and Anthony Edwards are about to be arrested for, like, statutory rape or something like that.
[00:57:00.500 --> 00:57:04.940]   That part makes me a little bit nervous, having the Dateline guy standing next to me.
[00:57:04.940 --> 00:57:10.420]   But I do, here's what I do appreciate, being next to, like, an excellent athlete like Anthony Edwards.
[00:57:10.520 --> 00:57:10.760]   So, okay.
[00:57:10.760 --> 00:57:11.780]   I think it's kind of cool.
[00:57:11.780 --> 00:57:14.800]   Took me a second.
[00:57:14.800 --> 00:57:16.060]   But I get it.
[00:57:16.060 --> 00:57:17.300]   I get what he's going for there.
[00:57:17.300 --> 00:57:19.700]   And I wish we had that for the HQ.
[00:57:19.700 --> 00:57:21.680]   I would definitely hang that up.
[00:57:21.680 --> 00:57:22.200]   It would be awesome.
[00:57:22.200 --> 00:57:23.820]   So, cool work.
[00:57:23.820 --> 00:57:24.540]   Good art.
[00:57:24.760 --> 00:57:24.940]   All right.
[00:57:24.940 --> 00:57:25.820]   Let's talk about books.
[00:57:25.820 --> 00:57:27.500]   I try to read five.
[00:57:27.500 --> 00:57:30.580]   I'm all over the place with my papers, by the way, today.
[00:57:30.580 --> 00:57:34.060]   This is why I need agentic AI or something.
[00:57:34.060 --> 00:57:38.900]   I try to read five books a month, and I did so last month in May 2025.
[00:57:39.700 --> 00:57:43.160]   Here are the five books I read in order that I read them.
[00:57:43.160 --> 00:57:49.280]   The first was a book called Building, A Carpenter's Notes on Life and the Art of Good Work.
[00:57:49.280 --> 00:57:50.460]   This was by Mark Ellison.
[00:57:50.460 --> 00:57:52.460]   You might recognize the name.
[00:57:52.460 --> 00:58:00.360]   So, he was profiled in The New Yorker years back as this carpenter that does super high-end custom installations in Manhattan high-rises.
[00:58:00.360 --> 00:58:08.000]   So, he's like this brilliant carpenter that you have some dream of a very complicated, intricate collection of wood
[00:58:08.000 --> 00:58:11.360]   and all sorts of shapes, like a staircase that does crazy things.
[00:58:11.360 --> 00:58:12.400]   He can figure it out.
[00:58:12.400 --> 00:58:17.000]   And so, I think there was a Burkhardt-Bilger profile of him.
[00:58:17.000 --> 00:58:18.560]   Anyways, he wrote this whole book.
[00:58:18.560 --> 00:58:19.420]   It's like a memoir.
[00:58:19.420 --> 00:58:20.300]   It's about work.
[00:58:20.300 --> 00:58:21.260]   It's about craftsmanship.
[00:58:21.260 --> 00:58:22.680]   It's really in his voice.
[00:58:22.680 --> 00:58:24.020]   I liked it.
[00:58:24.020 --> 00:58:25.120]   I liked it.
[00:58:25.120 --> 00:58:27.940]   Then I read Thoreau's Axe by Caleb Smith.
[00:58:27.940 --> 00:58:31.420]   This is more of an academic book, really.
[00:58:31.420 --> 00:58:33.920]   So, by academic, I mean, it's Princeton University Press.
[00:58:33.920 --> 00:58:43.900]   But it's about the way people wrote about focus and attention in the 19th century American context, which includes, of course, Thoreau, hence the title.
[00:58:45.200 --> 00:58:54.380]   But it's expository, I guess, in the sense of it's an academic secondary source book where the author, Caleb Smith, has broken it up into categories.
[00:58:54.380 --> 00:58:58.500]   And it's a lot of like, here's a quote, and I'll explain a little bit about what this is in the context.
[00:58:58.500 --> 00:59:02.320]   It's not like there's a through line of a singular argument.
[00:59:02.760 --> 00:59:08.160]   You know, it's not like an idea book where they're trying to say, here's my point, and I'm going to draw from these sources to make this point.
[00:59:08.160 --> 00:59:11.360]   Sometimes we get this in academic adjacent books.
[00:59:11.360 --> 00:59:19.660]   Like, there's a great one called The Wandering Mind where this medievalist, Jamie Krenner, is looking at the way that monks in the medieval period dealt with distraction.
[00:59:19.660 --> 00:59:25.860]   But she was also meditating on that and trying to, like, give us better insights about dealing with attention and distraction.
[00:59:25.860 --> 00:59:26.680]   This is not that.
[00:59:26.680 --> 00:59:42.460]   It really is like I'm doing the academic work of finding the various ways that attention and focus were talked on in the 19th centuries and categorizing them and trying to explain it for you to then use in, like, whatever other academic project you're going to use.
[00:59:42.500 --> 00:59:52.160]   So it's not really a read this through and change your life book, but it was, I mean, I came across a lot of interesting and marked up a lot, pulled out some ideas from there I'm going to use in other books.
[00:59:52.160 --> 00:59:53.480]   But, you know, it's academic.
[00:59:53.480 --> 01:00:00.880]   Interestingly, I bought Building and Thoreau's Acts at the same time at Labyrinth Books in Princeton last December.
[01:00:00.880 --> 01:00:03.500]   It's funny how this happens sometimes.
[01:00:03.500 --> 01:00:09.880]   Like, I bought those books together, and for whatever reason, they both came back into my attention last month, and I just read them back to back.
[01:00:09.880 --> 01:00:13.500]   So I don't always read things right away, but I usually get the things I buy.
[01:00:13.500 --> 01:00:17.960]   The next book I read was Nine Innings by Daniel Ockrent.
[01:00:17.960 --> 01:00:19.260]   It's a classic baseball book.
[01:00:19.260 --> 01:00:21.800]   Nine Innings of a Brewers game.
[01:00:21.800 --> 01:00:24.680]   I think it's Brewers-Orioles.
[01:00:24.680 --> 01:00:32.960]   It's a Brewers-Orioles game from the 80s, and as it goes through the nine innings, you learn a lot about the backstory of the teams and baseball,
[01:00:32.960 --> 01:00:40.840]   and Bud Siegelig is the owner of the ownership team of the Brewers, and they talk a lot about the free agency rule is pretty new at this point.
[01:00:40.840 --> 01:00:47.460]   And, like, you learn a lot about baseball and these players, and it's all built on the structure of a particular nine-inning game.
[01:00:47.460 --> 01:00:48.860]   So I like that type of book.
[01:00:48.860 --> 01:00:49.620]   There's a bunch of those.
[01:00:49.620 --> 01:00:51.520]   This is one of the better-known ones.
[01:00:51.960 --> 01:00:57.640]   Then I read—this is a needle scratch going from this hardcore baseball book.
[01:00:57.640 --> 01:01:01.160]   I then read Let Them, or is it Let Them Theory?
[01:01:01.160 --> 01:01:03.040]   Something like—by Mel Robbins.
[01:01:03.040 --> 01:01:06.040]   The book that's been, like, number one for roughly all the years.
[01:01:06.040 --> 01:01:08.180]   She gave me a copy.
[01:01:08.180 --> 01:01:10.100]   You know, I went down there to do her show.
[01:01:10.100 --> 01:01:11.080]   It's going to come out soon.
[01:01:11.080 --> 01:01:13.680]   She was nice enough to give me a copy of her book, and so I read it.
[01:01:13.680 --> 01:01:14.620]   So there you go.
[01:01:14.620 --> 01:01:15.620]   The Let Them Theory.
[01:01:15.620 --> 01:01:16.380]   The Let Them Theory.
[01:01:16.460 --> 01:01:17.620]   That book is just dominating.
[01:01:17.620 --> 01:01:20.520]   Number one—it might still be number one.
[01:01:20.520 --> 01:01:26.580]   Mel's show is crushing it out there in the podcast world, and her book is doing really well.
[01:01:26.580 --> 01:01:28.720]   May we be so lucky, Jesse.
[01:01:28.720 --> 01:01:29.340]   It was a good interview.
[01:01:29.340 --> 01:01:30.600]   I'm looking forward to that coming out.
[01:01:30.600 --> 01:01:34.260]   You know, what's funny was, you know, our buddy Steve Magnus?
[01:01:34.260 --> 01:01:34.660]   Yeah.
[01:01:34.660 --> 01:01:38.720]   The day after I did Mel's show, he came and did our show.
[01:01:38.720 --> 01:01:40.760]   Coincidentally, like, we were both there together.
[01:01:40.760 --> 01:01:44.420]   The final book I read—actually, this one's not available yet.
[01:01:44.460 --> 01:01:46.520]   I got an advanced copy, but I just jumped on it.
[01:01:46.520 --> 01:01:56.660]   It's called Against the Machine by Paul Kingsnorth, who's both a novelist and has done a lot of writing about things like modern ecology and environmental movements.
[01:01:56.660 --> 01:01:57.800]   It's a really cool book.
[01:01:57.800 --> 01:01:59.920]   It's not—I think it's coming out in the fall.
[01:01:59.920 --> 01:02:02.080]   I really enjoyed it.
[01:02:02.080 --> 01:02:03.520]   It's what I call a high-idea book.
[01:02:03.520 --> 01:02:05.760]   Like, in idea books, there's high-idea and low-idea.
[01:02:05.760 --> 01:02:08.240]   My books probably fall a little bit more into low-idea.
[01:02:08.240 --> 01:02:16.320]   By that, I mean—I don't mean it pejoratively, but a low-idea book is usually—I have an idea that's going to change the way you see something about your life.
[01:02:16.320 --> 01:02:20.580]   I'm going to help you then deploy that idea to potentially make changes.
[01:02:20.580 --> 01:02:28.200]   High-idea books is mainly just, I've got a big idea about the way the world works, and I'm just going to, like, make my case, right?
[01:02:28.200 --> 01:02:45.220]   And so he has this big case against what he calls the machine, which is what he describes as a sort of, like, totalizing capitalism-driven push towards just breaking down all order and tradition at the altar of economic growth.
[01:02:45.780 --> 01:02:53.120]   What's interesting about the book is that he's an interesting guy, a former Wiccan, interestingly, now a Greek Orthodox Christian.
[01:02:53.120 --> 01:02:59.460]   He doesn't come at this from a classical 20th century theory perspective.
[01:02:59.460 --> 01:03:06.820]   He thinks, you know, like, classic leftist theory, like Marxism, or more modern leftist theory, like the more postmodern identity-based theories.
[01:03:06.820 --> 01:03:08.380]   He's like, that's all just part of the machine.
[01:03:09.200 --> 01:03:19.560]   You think that you're revolting against, like, capitalism, but what you're doing is you're just helping to break down all the existing structures and ritual and routine and tradition that helps the machine spread more, and it's just as bad as what's happening on the right.
[01:03:19.560 --> 01:03:29.180]   And he comes at it much more from, like, a place of, I guess, place and folklore and the messy details of, like, other people and connection.
[01:03:29.180 --> 01:03:34.660]   And so it's anti-capitalist, but not, like, classic left anti-capitalist.
[01:03:34.660 --> 01:03:37.360]   And it has a lot of interesting commentary about technology as well.
[01:03:37.620 --> 01:03:43.760]   Interesting guy, former Wiccan, Greek Orthodox, lives on a small farm in West Ireland, which is beautiful.
[01:03:43.760 --> 01:03:48.380]   I've been out there before, but also is, like, long-listed for the Booker Prize for one of his novels.
[01:03:48.380 --> 01:03:50.080]   Doesn't own a phone.
[01:03:50.080 --> 01:03:52.500]   I don't think he has a computer, just a type.
[01:03:52.500 --> 01:03:54.420]   I like high-idea books.
[01:03:54.420 --> 01:03:56.400]   Great writer, big ideas.
[01:03:56.400 --> 01:04:01.760]   A lot of, like, Jacques Lull, a lot of Lewis Mumford in there, a lot of Neil Postman, Marshall McLuhan.
[01:04:01.760 --> 01:04:04.900]   I enjoyed that book, but it's not out yet.
[01:04:04.900 --> 01:04:06.020]   Also, it's a really cool cover.
[01:04:06.040 --> 01:04:07.720]   You read that quick.
[01:04:07.720 --> 01:04:09.220]   I read fast.
[01:04:09.220 --> 01:04:09.840]   Yeah.
[01:04:09.840 --> 01:04:10.060]   Oh, yeah.
[01:04:10.060 --> 01:04:11.040]   He sent it through you, right?
[01:04:11.040 --> 01:04:12.280]   He sent it to me, and I gave it to you.
[01:04:12.280 --> 01:04:13.560]   Yeah, it took a couple days.
[01:04:13.560 --> 01:04:14.000]   Yeah.
[01:04:14.000 --> 01:04:14.340]   Yeah.
[01:04:14.340 --> 01:04:16.020]   Yeah, when you read, you read.
[01:04:16.020 --> 01:04:17.060]   That's the way I see it.
[01:04:17.060 --> 01:04:18.180]   But I like that book.
[01:04:18.180 --> 01:04:18.760]   Got a lot of ideas.
[01:04:18.760 --> 01:04:19.560]   All right.
[01:04:19.560 --> 01:04:21.260]   That's all the time we have for today.
[01:04:21.260 --> 01:04:24.820]   We'll be back next week with another episode of the show.
[01:04:24.940 --> 01:04:27.400]   And until then, as always, stay deep.
[01:04:27.400 --> 01:04:34.960]   Hey, if you like today's discussion about ultra-process content, I think you'll also like episode 351 that's titled Making the Internet Good Again.
[01:04:34.960 --> 01:04:39.520]   It talks about what you should be doing instead of looking at all those distracting social apps.
[01:04:39.520 --> 01:04:40.340]   Check it out.
[01:04:40.580 --> 01:04:59.740]   So, given my history of technocriticism, with my particular focus on social media and smartphones, you might expect that I would have a lot of disagreement with Cowan, but I'm actually more on the same page as him than you might at first expect.

