
[00:00:00.000 --> 00:00:05.200]   Cool, awesome. Hey everyone, I'm Anish and today we're going to talk about how to tend and tune
[00:00:05.200 --> 00:00:09.680]   your multi-agent reinforcement learning experiments with Ray and Weights and Biases.
[00:00:09.680 --> 00:00:15.360]   So another name that we can give this is scaling production grade reinforcement learning.
[00:00:15.360 --> 00:00:19.200]   And I'm going to take you to an interesting concept through the current forms of reinforcement
[00:00:19.200 --> 00:00:24.080]   learning that we're doing now and then make our way into the newest forms that LLMs tend to use
[00:00:24.080 --> 00:00:28.720]   and how we can take concepts from reinforcement learning and apply them to agents as we'd
[00:00:28.720 --> 00:00:33.280]   imagine them LLMs. But instead of getting ahead of myself, let's go through this part first.
[00:00:33.280 --> 00:00:39.680]   And so all of you have probably seen reinforced learning in some form. For us, we're going to
[00:00:39.680 --> 00:00:45.520]   be talking today about drone delivery or I guess drone flying generally and autonomous vehicles.
[00:00:45.520 --> 00:00:50.560]   And so reinforced learning generally is a powerful machine learning paradigm
[00:00:50.560 --> 00:00:57.360]   that allows systems to learn from interaction. So we usually see this in games to robotics,
[00:00:57.360 --> 00:01:03.200]   but now when you have concepts such as multiple agents and across different and more interesting
[00:01:03.200 --> 00:01:09.360]   environments outside of the video game realm or specific robotics realms, you can start to see
[00:01:09.360 --> 00:01:15.440]   that reinforcement learning can be applied to a variety of use cases. But let's first talk about
[00:01:15.440 --> 00:01:21.120]   multi-agent reinforcement learning itself. So quickly just to show a really technical graphic
[00:01:21.120 --> 00:01:27.680]   of here, to simply think about it as the name implies, single agent reinforcement learning is
[00:01:27.680 --> 00:01:34.400]   when a solitary agent navigates an environment. So the agent's actions and the environment's
[00:01:34.400 --> 00:01:39.520]   responses are the core of the learning process. But when you have multi-agent reinforcement
[00:01:39.520 --> 00:01:46.400]   learning, multiple agents coexist, each with its own strategies and objectives. And this leads to
[00:01:46.400 --> 00:01:53.520]   a non-stationary environment where each of the effects of the agents all act upon the environment.
[00:01:53.520 --> 00:02:00.000]   And we have to understand how each of these effects, what the credit is essentially that is
[00:02:00.000 --> 00:02:05.920]   assigned from each agent to how they affected this environment. And that's what we try to analyze
[00:02:05.920 --> 00:02:10.800]   in multi-agent reinforcement learning. And so there's a lot of scaling problems that come from
[00:02:10.800 --> 00:02:17.360]   this. One is that as you make much stricter simulations or requirements for your simulations,
[00:02:17.360 --> 00:02:22.560]   your environments also scale in complexity. You also need to worry about the increased need of
[00:02:22.560 --> 00:02:27.120]   computational needs as architectures and environments also scale because you want to
[00:02:27.120 --> 00:02:33.520]   run these simulations in parallel across a lot of your GPUs in your clusters. And lastly, it's about
[00:02:33.520 --> 00:02:39.760]   learning which techniques the agents should learn such that they could have a nice balance between
[00:02:39.760 --> 00:02:44.800]   trying new strategies for the tasks that they're trying to solve or exploiting the ones that
[00:02:44.800 --> 00:02:48.240]   already currently exist for the specific use case that they're trying to solve for.
[00:02:48.240 --> 00:02:54.160]   And so when you don't solve, account for these different problems, you end up with autonomous
[00:02:54.160 --> 00:03:00.000]   vehicles that consider horse-drawn carriages as trucks. And that is something that we don't want
[00:03:00.000 --> 00:03:06.640]   to happen. And so one thing to consider when running through all of these different experiments
[00:03:06.640 --> 00:03:11.840]   with your multi-agent setups is that you want a platform that is able to work and minimize
[00:03:11.840 --> 00:03:17.840]   the refactorization and increase the del for productivity for taking these models from
[00:03:17.840 --> 00:03:22.400]   training and also pushing them into deployment while accounting for each of those scaling issues
[00:03:22.400 --> 00:03:29.360]   I mentioned. And so that kind of breaks us down to two fundamental problems. One, having a universal
[00:03:29.360 --> 00:03:35.040]   framework to ease the scaling of the distributed workload. And then secondly, ensuring that the
[00:03:35.040 --> 00:03:39.520]   people actually building these models, the ML practitioner, have the right set of tools so that
[00:03:39.520 --> 00:03:45.680]   they can track their experiments and iterate upon them effectively. So they can reduce the time to
[00:03:45.680 --> 00:03:53.200]   market for their different products. And so on the side of Ray at the very least, we have a nice
[00:03:53.200 --> 00:03:59.520]   package called RLib that's able to help organize the combination of your environments with the
[00:03:59.520 --> 00:04:05.680]   learning specifications of your agents and able to emit out and train these different models by
[00:04:05.680 --> 00:04:11.760]   distributing it across your workload and your cluster of GPUs. Then you have Weights and Biases
[00:04:11.760 --> 00:04:17.440]   which provide the full end-to-end DelVeloper experience and toolkit to take different aspects
[00:04:17.440 --> 00:04:23.120]   of your ML workflow and ensure that at the stage that's effective for your ML practitioner,
[00:04:23.120 --> 00:04:28.000]   they have that tool available to them to ensure that they take this model, do the effective
[00:04:28.000 --> 00:04:34.480]   training, and then push it to production and then to market. And so this is the one dashboard that
[00:04:34.480 --> 00:04:38.800]   we'll be looking at later. And so in general, this is how you can imagine what it would look like
[00:04:38.800 --> 00:04:44.880]   ideally from a user experience to visualize, track, and debug your model pipeline. And I'll show you
[00:04:44.880 --> 00:04:50.640]   what that looks like in person by running through one specific example. And so this is also nice if
[00:04:50.640 --> 00:04:55.840]   you're using any scale and Ray products as there's a deep integration from the Ray dashboard to our
[00:04:57.440 --> 00:05:01.440]   product offering to the Ray dashboard and the Ray dashboard back to our product.
[00:05:01.440 --> 00:05:09.840]   And so to get this behavior, it's as simple as adding into your Ray train, your Ray tune,
[00:05:09.840 --> 00:05:17.120]   or your RL lib training itself to add this one line here, the WANDB logger callback. By simply
[00:05:17.120 --> 00:05:22.160]   setting this flag with additional keywords such as save checkpoints equals true, at any point of
[00:05:22.160 --> 00:05:27.280]   model failure or during the training process, we're able to gather all the metrics to help you
[00:05:27.280 --> 00:05:32.480]   identify that failure and restart your experiment from the last effective checkpoint for your model.
[00:05:32.480 --> 00:05:40.640]   And so to kind of break this down into a two-fold cake a little bit, at the top level of this cake,
[00:05:40.640 --> 00:05:47.280]   we have the ML workflow tooling. And that's a direct user experience level that affects the
[00:05:47.280 --> 00:05:53.120]   productivity of your ML engineer and your ML team. Secondly, you have the ML execution layer,
[00:05:53.120 --> 00:05:58.400]   and that takes care of performance, scalability, and reliability of the execution environment.
[00:05:58.400 --> 00:06:05.040]   So something that the coordination of both weights and biases and any scale Ray do well together
[00:06:05.040 --> 00:06:09.280]   by offering this full holistic experience across both the workflow layer and then the
[00:06:09.280 --> 00:06:15.440]   execution engine layer. And so this blue box represents traditional training for one-off
[00:06:15.440 --> 00:06:20.880]   experiments, while this red box is more of a traditional end-to-end workflow, as most ML
[00:06:20.880 --> 00:06:25.760]   practitioners can recognize it, going from data exploration all the way down to model serving.
[00:06:25.760 --> 00:06:31.680]   And so let's go look through one specific example. Let's go through autonomous vehicles.
[00:06:31.680 --> 00:06:37.680]   So to keep things simple, the idea here that we're going to do is we're going to train an
[00:06:37.680 --> 00:06:45.200]   autonomous vehicle to take steps ahead and plan its position in time, given its current place
[00:06:45.200 --> 00:06:51.120]   in traffic. So for instance, what we do is we gather a lot of LIDAR data from these cars,
[00:06:51.120 --> 00:06:58.800]   think like a Waymo car, and we compress it down into a form that we call a bird's eye view. So
[00:06:58.800 --> 00:07:05.600]   AKA a top-down view that represents all of the different cars, their positions over all time
[00:07:05.600 --> 00:07:12.000]   steps. So as you can imagine the scenario, this is a multi-agent scenario. We ourselves are the
[00:07:12.000 --> 00:07:17.760]   individual car that's trying to navigate the road, while other cars are also navigating the road,
[00:07:17.760 --> 00:07:24.720]   affecting the state of traffic at every moment. So we as our car have to effectively plan for
[00:07:24.720 --> 00:07:30.560]   the positions of every other car and also take in the underlying intent of that driver. Are they
[00:07:30.560 --> 00:07:36.640]   going to make a turn in the next moment or will I have to react? And so we need to ensure that
[00:07:36.640 --> 00:07:41.840]   we have a full team of developers that are able to validate every step of the process before we
[00:07:41.840 --> 00:07:47.200]   ensure that this touches the market, because this is a highly critical ML model. If this fails,
[00:07:47.200 --> 00:07:53.120]   then people get hurt and you also get a lot of trouble with a lot of different federal organizations.
[00:07:53.120 --> 00:07:59.440]   And so we want to ensure that every step of the process, the GPU and cluster considerations
[00:07:59.440 --> 00:08:05.520]   for each of the members are organized appropriately to their workflow step. And then we also organize
[00:08:05.520 --> 00:08:12.480]   the workflow itself to ensure that every step of the process feeds into the proper team member
[00:08:12.480 --> 00:08:17.440]   working on the ML workflow as it matters to them. So for instance, we want to ensure that our data
[00:08:17.440 --> 00:08:23.920]   engineer flows the proper data down to our ML practitioner so they can use it for training.
[00:08:23.920 --> 00:08:29.120]   And so each and every one of these different objects that are released, so these metrics and
[00:08:29.120 --> 00:08:35.920]   this data all needs to be theorized and centralized in one location for proper analysis. There's no
[00:08:35.920 --> 00:08:40.880]   way to debug your model effectively if you can't all drill down all the way back to the initial
[00:08:40.880 --> 00:08:48.720]   data set that it touched. And so let's go through one of these dashboards. So this is a centralized
[00:08:48.720 --> 00:08:54.560]   dashboard that we call a report that you can use to take notes alongside your different ML
[00:08:54.560 --> 00:09:00.480]   experimentation. So in this case, you see here, this prediction model contains our current
[00:09:00.480 --> 00:09:08.640]   production model for our ML system for planning autonomous vehicle movement. So in the same report,
[00:09:08.640 --> 00:09:15.440]   we're able to frame by frame investigate why our model went wrong. So this actually collection is
[00:09:15.440 --> 00:09:21.200]   a little worse. Let's look at this one. I think this one. Yeah. And so in this case, the pink
[00:09:21.200 --> 00:09:27.920]   dots that you see represent the ground truth of that model and the effective next steps for it.
[00:09:27.920 --> 00:09:32.640]   Well, the blue represents the predicted motion of the specific model. As you can see here in
[00:09:32.640 --> 00:09:40.240]   the center model, we predict that this car behind us, this blue car behind the green one, it is,
[00:09:40.240 --> 00:09:45.920]   oh, a little too much zoom in on myself. I apologize. You can see that it is predicted to go
[00:09:45.920 --> 00:09:51.680]   and hit that green car if we consider the prediction from the model. So our current model is
[00:09:51.680 --> 00:09:58.400]   predicting that the car behind our green car is going to hit our car. That is an improper prediction.
[00:09:58.400 --> 00:10:05.040]   And so we want to ensure that we can work backwards and debug this process. So going back to our
[00:10:05.040 --> 00:10:11.360]   initial collection of our model in our model registry, we can go and click through any part
[00:10:11.360 --> 00:10:17.680]   of this process and investigate where in our evaluation or our training any errors occurred.
[00:10:17.680 --> 00:10:22.320]   So let's go look at our model registry and see that this is our V0 model currently in production.
[00:10:22.320 --> 00:10:28.800]   We can go through and see the files and also the underlying lineage. But I'm going to actually go
[00:10:28.800 --> 00:10:37.360]   and look at the evaluation itself. So after looking and seeing the output of all of the
[00:10:37.360 --> 00:10:43.840]   different models in comparison, as you can see here, right now we applied a filter. We can remove
[00:10:43.840 --> 00:10:50.880]   this filter. We can do a comparative analysis of all the different models that we have as
[00:10:50.880 --> 00:10:56.320]   candidate models that we want to put to production, look at the underlying details of these models to
[00:10:56.320 --> 00:11:03.760]   see if any glaring errors are consistent, and also do the same frame by frame breakdown to understand
[00:11:03.760 --> 00:11:08.960]   across all the model architectures that we tried for this specific batch, does it make sense for
[00:11:08.960 --> 00:11:16.000]   this specific use case. And to quickly summarize, this was an architecture where we use a ResNet-50
[00:11:16.000 --> 00:11:23.040]   as a backbone for as the CNN to investigate the XY position and over the time steps did awful.
[00:11:23.040 --> 00:11:26.080]   As I would not recommend using this specific backbone.
[00:11:29.760 --> 00:11:32.080]   The latency on the communication of weights and biases or...
[00:11:32.080 --> 00:11:42.320]   So if you mean from the ML training to our system, we spin up a separate thread. And so it should be
[00:11:42.320 --> 00:11:49.040]   almost negligent. We offer no real overhead in how we track our metrics. There is a little bit of
[00:11:49.040 --> 00:11:55.520]   time that it takes for theorizing data because we effectively store in a bucket. And that depends
[00:11:55.520 --> 00:12:01.440]   on the size and volume of the data itself. So how many files and the size of each of those files.
[00:12:01.440 --> 00:12:04.880]   But yeah, I think I can talk about that more a little bit later.
[00:12:04.880 --> 00:12:12.640]   Cool. And then also similarly, I can break down and go directly to the model training itself for
[00:12:12.640 --> 00:12:18.080]   the associated model in our model registry and under see if was there any issues, not only in
[00:12:18.080 --> 00:12:22.320]   our system utilization, like was there a failure that occurred that wasn't caught, but also just
[00:12:22.320 --> 00:12:27.920]   see directly, hey, was there a weird spike in my loss curve that I did not account for before
[00:12:27.920 --> 00:12:33.440]   deploying this that I missed during my evaluation and promotion process? And so that's like one
[00:12:33.440 --> 00:12:37.280]   consistent workflow that you can kind of reuse across a lot of your ML experiments.
[00:12:37.280 --> 00:12:43.360]   And so just to recap, these are some screenshots and different ways you can serialize different
[00:12:43.360 --> 00:12:48.720]   assets to investigate your model. So a mixture of rich media overlay with segmentation masks
[00:12:48.720 --> 00:12:53.920]   or a parallel coordinate plots to investigate hyper parameter tuning options for one specific
[00:12:53.920 --> 00:12:59.600]   model. But another use case that we can also talk about is multi-agent reinforced and learning for
[00:12:59.600 --> 00:13:06.240]   quadrotor control, aka how to fly your drone. So similarly to our last graph, it's the same
[00:13:06.240 --> 00:13:10.800]   workflow. But in this case, we're only looking at training itself. And in this case, we're going to
[00:13:10.800 --> 00:13:16.880]   be looking at the concept of having one drone be considered the leader of all the other drones.
[00:13:16.880 --> 00:13:23.120]   And in the batch of all these drones, they want to independently understand the position of all
[00:13:23.120 --> 00:13:30.160]   the other drones around them, but still be able to follow the main leader one. And so in this case,
[00:13:30.160 --> 00:13:37.360]   we use a decentralized actor centralized critic model. So to explain this extremely simply,
[00:13:37.360 --> 00:13:43.840]   we think of each drone as an independent agent acting on itself. It knows only its own state
[00:13:43.840 --> 00:13:49.040]   and the state of what it's available immediately around itself. The centralized critic, however,
[00:13:49.040 --> 00:13:55.840]   has a complete understanding of every drone, all of its states and all of its positions.
[00:13:55.840 --> 00:14:02.960]   So the centralized critic is taking a look at the scene of the drones at every moment and seeing,
[00:14:02.960 --> 00:14:09.120]   hey, is the placement off from the expectation from the policy that I gave each of these
[00:14:09.120 --> 00:14:14.880]   independent agents? And if each of these agents are a little bit off or not following instructions
[00:14:14.880 --> 00:14:21.760]   properly, we make an incremental change using PPO. Now, all of this sounds complex, and all
[00:14:21.760 --> 00:14:26.560]   these sound very difficult to implement individually. But actually, nicely enough,
[00:14:26.560 --> 00:14:33.360]   RLib allows you to hot swap a lot of different customizable components. So anything from the
[00:14:33.360 --> 00:14:39.680]   OpenAI gym, so our Furama Foundation gymnasium environments, to the model and action policies
[00:14:39.680 --> 00:14:45.040]   that you choose, all those are easily able to be replaced with a lot of pre-made implementations
[00:14:45.040 --> 00:14:52.880]   already provided. So making the decentralized agent centralized critic model is very
[00:14:52.880 --> 00:15:01.920]   straightforward from RLib. And so let's also take a look at a quick example of this. So in this
[00:15:01.920 --> 00:15:08.800]   scenario, we have very similar dashboards. I want to show is that if you have any custom metrics,
[00:15:08.800 --> 00:15:14.880]   you're also able to collect that information. So at least in a drone environment, one thing that I
[00:15:14.880 --> 00:15:22.560]   take into account is not only the XY position, but because there are four motors on a drone itself,
[00:15:22.560 --> 00:15:28.880]   there's a mixture of the angle of the drone itself, the amount of movement on each of the rotor,
[00:15:28.880 --> 00:15:34.880]   and also the position of the drone at that time. So there's a lot of different aspects that a drone
[00:15:34.880 --> 00:15:41.200]   has to consider when to turn itself or to spin one rotor faster. And so you're able to actually,
[00:15:41.200 --> 00:15:45.760]   as you should be doing, capturing each of this information. And from Weights and Biases,
[00:15:45.760 --> 00:15:52.560]   you're able to centralize all this information in context in one location alongside. So I apologize
[00:15:52.560 --> 00:15:59.760]   for this. I'm not very good at navigating camera zooms in a gym environment. But if I zoom in quite
[00:15:59.760 --> 00:16:06.400]   a lot, you can see that this drone flying experiment and very early iterations does not do the best
[00:16:06.400 --> 00:16:12.560]   job. But we're also able to capture the movements of each of the drones in context of the video
[00:16:12.560 --> 00:16:16.480]   itself. So you're able to have this full workflow where you have this qualitative analysis,
[00:16:16.480 --> 00:16:20.880]   which is really useful for these more complicated examples, where it makes more sense to see with
[00:16:20.880 --> 00:16:27.840]   your eyes as opposed to charts. And now I'm going to go and segue into some theory, some
[00:16:27.840 --> 00:16:35.600]   metagame with LLMs now. So right now, we're seeing a lot of effectiveness in reinforcement learning
[00:16:35.600 --> 00:16:42.400]   human feedback. The concept that we're able to use reinforcement learning principles with LLMs
[00:16:42.400 --> 00:16:49.040]   to instruct LLMs to be more human-like and effective in their response. A big core component
[00:16:49.040 --> 00:16:55.280]   here being human-like, where correctness may have been the early forms of chat GPT, but ensuring that
[00:16:55.280 --> 00:17:02.400]   we're able to understand the intent of a user who asks via their prompt and able to craft a human-like
[00:17:02.400 --> 00:17:07.760]   response is the core aspect of using this reinforcement learning human feedback, where you
[00:17:07.760 --> 00:17:16.560]   use a human labeler to score the multiple outputs from a GPT-like model and use that scoring process
[00:17:16.560 --> 00:17:22.240]   to train a reinforcement learning model to be really good at saying, "Hey, a new output from
[00:17:22.240 --> 00:17:27.520]   my model comes in. I'm going to guess how a human would score this," and ensure that you get that
[00:17:27.520 --> 00:17:34.160]   best score always, that best human-like response. So why am I talking about this in an agent's talk?
[00:17:34.160 --> 00:17:39.280]   Well, that's because LLMs are actually becoming this new backbone for a concept called agent.
[00:17:39.280 --> 00:17:43.920]   Now, there is a little bit of overlap and contention if a RL agent and LM agent have
[00:17:43.920 --> 00:17:51.200]   this overlap that we are considering, but deep down, a reinforcement using an LLM as an agent
[00:17:51.200 --> 00:17:57.120]   is pretty much us saying, "We want our LLM to be the reasoning layer for understanding the
[00:17:57.120 --> 00:18:04.240]   intention of our user and then acting upon it by using either tools or solving for an objective
[00:18:04.240 --> 00:18:11.120]   task." So people find this very fascinating and useful. Within these three projects alone,
[00:18:11.120 --> 00:18:19.520]   there is a share to 70K stars by users from GitHub. And so in these repos alone,
[00:18:19.520 --> 00:18:25.680]   these are all agent-based implementations of large language models that take into account
[00:18:25.680 --> 00:18:31.360]   a user specification. So like in GPT Engineer or Open Interpreter, you would say something like,
[00:18:31.360 --> 00:18:38.240]   "Build me a program in Flutter to make my personal portfolio," and it will decompose that problem
[00:18:38.240 --> 00:18:42.640]   into steps where you will work with it back and forth, a little bit of human feedback in there,
[00:18:42.640 --> 00:18:48.080]   to then saw a guess I effectively get the program that you desired.
[00:18:48.080 --> 00:18:53.600]   Now, what if we took that one step further? What if we create a whole sim-like environment
[00:18:53.600 --> 00:19:02.160]   where we teach each individual LLM agent to learn the best underlying prompt to use for its own goal?
[00:19:02.160 --> 00:19:07.360]   What if one agent was really good at programming? What if one agent was a really good project
[00:19:07.360 --> 00:19:13.040]   manager and we allowed each of these agents through reinforcement learning to learn these
[00:19:13.040 --> 00:19:18.400]   best underlying system prompts, as you probably heard it, to be really good at their specific
[00:19:18.400 --> 00:19:24.400]   role and task and ensure that we use reinforcement learning to also ensure that it learns the right
[00:19:24.400 --> 00:19:31.920]   policies to understand how to return human-like and reasonable responses to a lot of these
[00:19:31.920 --> 00:19:38.000]   scenarios that we would propose in this simulated Pokemon-like town? And that's because, at the end
[00:19:38.000 --> 00:19:43.680]   of the day, this is a broken-down code form of what these agents are doing. It's perceiving our task,
[00:19:43.680 --> 00:19:49.760]   storing it to memory, retrieving the best collection of documents or memories, a rag
[00:19:49.760 --> 00:19:54.480]   talk coming to you later, we'll explain this better, and ensuring that it knows the best plan
[00:19:54.480 --> 00:20:01.120]   and actions to take. So reinforcement learning is, in effect, how can we optimize a policy to know
[00:20:01.120 --> 00:20:07.120]   which best plan and action to take? So this is not done yet, this generative reinforcement agent,
[00:20:07.120 --> 00:20:12.880]   but I hope someone does it soon and they use us. And the reason I say this is that with the
[00:20:12.880 --> 00:20:19.040]   proliferation with open-source models and the investigation to debug and monitor the usage of
[00:20:19.040 --> 00:20:26.000]   your LLM, there's a natural synergy occurring for LLM ops. So now ML ops and LLM ops will now
[00:20:26.000 --> 00:20:32.000]   intercept in very interesting ways and awaits advice that is here for it. And so to quickly
[00:20:32.000 --> 00:20:36.400]   end off, I'll splash some screens of some of our new products that we can talk to about offline
[00:20:36.400 --> 00:20:41.680]   later. But this example of that agent session I was talking about with Open Interpreter,
[00:20:41.680 --> 00:20:47.520]   where I told it to do, I think, some really dumb task. Let's see. I told it to
[00:20:49.520 --> 00:20:56.400]   get the last summary of latest LLM articles from Rxiv. Underneath the understanding from that,
[00:20:56.400 --> 00:21:03.360]   it provided me a plan and the steps of code, so a mixture of arguments and input to essentially say,
[00:21:03.360 --> 00:21:07.920]   here's the plan of attack I'm going to take, and here's the code I'm going to execute on
[00:21:07.920 --> 00:21:12.480]   your computer. Do you agree? And from here, you can start the debugging process.
[00:21:12.480 --> 00:21:18.240]   Similarly, if you're building applications that you want to use to monitor your LLMs,
[00:21:18.240 --> 00:21:23.600]   like in the case where you want to ensure that you convert all of your techs and remove PII,
[00:21:23.600 --> 00:21:29.840]   you can do so effectively and have a central place to interactively understand and pinpoint
[00:21:29.840 --> 00:21:35.600]   situations where LLM safety considerations, such as preventing PII to be released, occur.
[00:21:35.600 --> 00:21:45.280]   And so that is my talk. Sorry. Yeah. Here is the way to buy Score again. Thank you, guys. And
[00:21:45.280 --> 00:21:51.120]   feel free to connect on any one of these forums. [Applause]

