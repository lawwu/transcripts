
[00:00:00.000 --> 00:00:04.960]   All that gradients guarantee you is that somewhere out there, there's a step size
[00:00:04.960 --> 00:00:07.600]   small enough that if you take it, your loss will go down.
[00:00:07.600 --> 00:00:10.760]   That's cold comfort to know that there exists a small step
[00:00:10.760 --> 00:00:12.120]   size if you don't know what it is.
[00:00:12.120 --> 00:00:19.920]   Welcome back to the Math for ML exercise series.
[00:00:19.920 --> 00:00:22.760]   I am your host, Charles Frey.
[00:00:22.760 --> 00:00:27.200]   With me today, I have Scott, growth engineer at Weights and Biases.
[00:00:27.320 --> 00:00:31.560]   Scott and I are going to be working through the calculus exercises notebook here.
[00:00:31.560 --> 00:00:37.200]   We'll be working on some stuff with the little O notation and on gradient
[00:00:37.200 --> 00:00:39.560]   descent and on calculating gradients.
[00:00:39.560 --> 00:00:43.080]   And so if you want to know a little bit more about all those topics, make sure to
[00:00:43.080 --> 00:00:46.360]   check out the lecture videos that accompany this series.
[00:00:46.360 --> 00:00:47.200]   All right.
[00:00:47.200 --> 00:00:50.080]   So let's dive into this little O notation section here.
[00:00:50.080 --> 00:00:55.200]   So one of the main themes of the calculus lecture was that it's really nice to think
[00:00:55.280 --> 00:00:59.760]   about derivatives in this slightly different definition or slightly different sort of
[00:00:59.760 --> 00:01:02.760]   rearrangement of the definition of the derivative.
[00:01:02.760 --> 00:01:07.120]   So the like normal definition, the derivative is about this limit and it's
[00:01:07.120 --> 00:01:09.080]   the limit of a particular ratio.
[00:01:09.080 --> 00:01:14.400]   And the way that I like to think about it instead is in terms of this definition,
[00:01:14.400 --> 00:01:18.760]   the first line of math that you see there that says, if I want to know what a
[00:01:18.760 --> 00:01:25.120]   function's value is at a point epsilon away from the current value, the
[00:01:25.160 --> 00:01:29.400]   gradient is something that does a linear approximation to what that value is.
[00:01:29.400 --> 00:01:33.000]   So that's f of x plus gradient times epsilon.
[00:01:33.000 --> 00:01:34.640]   So gradient times epsilon there.
[00:01:34.640 --> 00:01:38.520]   We've got that inner product, that dot product that we talked about in the linear
[00:01:38.520 --> 00:01:43.320]   algebra section, we're taking the, that dot product with epsilon, and that's giving us
[00:01:43.320 --> 00:01:46.640]   an approximation to the true value of f of x plus epsilon.
[00:01:46.640 --> 00:01:50.080]   So it's a linear approximation because we're doing that dot product.
[00:01:50.080 --> 00:01:54.960]   And it's an approximation because there's this little term here at the end, O of
[00:01:54.960 --> 00:01:56.160]   epsilon at the end.
[00:01:56.160 --> 00:02:00.040]   And what I like about this is that it's pretty easy to just like box that up and
[00:02:00.040 --> 00:02:04.000]   just think of little O of epsilon means there's a small approximation error.
[00:02:04.000 --> 00:02:07.880]   Whereas when you wrap everything in a limit, you have to be very careful when
[00:02:07.880 --> 00:02:09.440]   you're manipulating limits directly.
[00:02:09.440 --> 00:02:13.480]   They, it's very easy for you to like accidentally do something wrong and get
[00:02:13.480 --> 00:02:15.640]   the wrong answer, at least in my experience.
[00:02:15.640 --> 00:02:20.160]   And with the little O notation, I find that I don't run into that nearly as often.
[00:02:20.160 --> 00:02:23.000]   So in this exercise that we're going to do here, we're going to build some
[00:02:23.000 --> 00:02:27.360]   intuition for what little O means with some examples using this cool Python
[00:02:27.360 --> 00:02:29.640]   library called SymPy that does symbolic math.
[00:02:29.640 --> 00:02:34.360]   So if you've ever used Wolfram Alpha or Mathematica, this is like that built into
[00:02:34.360 --> 00:02:34.880]   Python.
[00:02:34.880 --> 00:02:37.120]   Have you ever seen this SymPy library before, Scott?
[00:02:37.120 --> 00:02:38.000]   I have not.
[00:02:38.000 --> 00:02:41.320]   I did see this at the very beginning, which caught my eye.
[00:02:41.320 --> 00:02:48.360]   From SymPy import X, where X is actually not, I got used to using X as like a
[00:02:48.360 --> 00:02:52.400]   variable, but here it's like, like what a function or, or something.
[00:02:52.560 --> 00:02:52.760]   Yeah.
[00:02:52.760 --> 00:02:54.120]   Let's, let's talk about what that is.
[00:02:54.120 --> 00:02:57.600]   So the goal here is to be able to write mathematical expressions.
[00:02:57.600 --> 00:03:00.680]   And one of the important distinctions between a mathematical expression,
[00:03:00.680 --> 00:03:03.680]   something in a computer program, in a computer program, everything
[00:03:03.680 --> 00:03:05.280]   is bound to a specific value.
[00:03:05.280 --> 00:03:09.080]   So if you write like X equals one, then like X is just something
[00:03:09.080 --> 00:03:10.680]   that points to the number one.
[00:03:10.680 --> 00:03:13.960]   But when you write a mathematical expression, like actually, if you
[00:03:13.960 --> 00:03:17.560]   scroll up a little bit to some of those, actually just a little bit more.
[00:03:17.560 --> 00:03:17.800]   Yeah.
[00:03:17.800 --> 00:03:22.280]   Like there, X is a variable in that X is something that we, you can slot
[00:03:22.280 --> 00:03:24.000]   different values into very easily.
[00:03:24.000 --> 00:03:28.080]   And so SymPy is designed to give you the ability to directly manipulate
[00:03:28.080 --> 00:03:32.080]   expressions made of variables, as opposed to like manipulating
[00:03:32.080 --> 00:03:35.200]   pointers to values, which is what you're really doing when you're writing programs.
[00:03:35.200 --> 00:03:40.360]   So that's like maybe a little deeper into the world of math and computer
[00:03:40.360 --> 00:03:41.880]   programming than we really need to go.
[00:03:41.880 --> 00:03:46.440]   The main thing that we want to do here is check whether the expression F of X is
[00:03:46.440 --> 00:03:51.520]   little O of an expression G of X for some particular variable X.
[00:03:51.640 --> 00:03:55.240]   If you scroll up to the definition of little O, that might be helpful here.
[00:03:55.240 --> 00:04:00.080]   To say that the expression F of X is little O of G of X.
[00:04:00.080 --> 00:04:01.960]   That's how I read the left-hand side.
[00:04:01.960 --> 00:04:09.160]   To say that is to assert or imply that there is a limit of a ratio that is equal to zero.
[00:04:09.160 --> 00:04:14.600]   This is where that like classic calculus idea about what happens to a slope as the
[00:04:14.600 --> 00:04:18.840]   two points come closer together, that's that classic calculus limit idea still
[00:04:18.840 --> 00:04:22.200]   shows up in this FrÃ©chet derivative definition, but it's showing up here
[00:04:22.200 --> 00:04:24.160]   inside the definition of a little O.
[00:04:24.160 --> 00:04:28.440]   This is the definition of little O of G of X.
[00:04:28.440 --> 00:04:32.680]   And it may be simpler to just like, just take G of X as the identity function.
[00:04:32.680 --> 00:04:34.320]   Takes in its inputs, returns its outputs.
[00:04:34.320 --> 00:04:40.160]   That implies that the limit as X goes to zero of F of X divided by X is zero.
[00:04:40.160 --> 00:04:46.760]   So what that says is X is in the limit as you get closer to zero, bigger than F of X.
[00:04:46.760 --> 00:04:47.240]   Okay.
[00:04:47.280 --> 00:04:52.960]   So G of X is bigger than F of X is one way of reading little O notation.
[00:04:52.960 --> 00:04:56.520]   It's like a strictly greater than kind of thing.
[00:04:56.520 --> 00:04:59.280]   You might've seen big O notation, especially people who have an
[00:04:59.280 --> 00:05:02.360]   engineering or physics background will have seen a version of this big O
[00:05:02.360 --> 00:05:06.920]   notation that's like a greater than or equal to, it says that the limit as X
[00:05:06.920 --> 00:05:10.960]   goes to zero of F of X over G of X is not infinite, basically.
[00:05:10.960 --> 00:05:12.920]   It's either like zero or a constant.
[00:05:12.920 --> 00:05:13.480]   Okay.
[00:05:13.480 --> 00:05:15.800]   And, and, and here I see there's this X.
[00:05:15.800 --> 00:05:19.800]   Is this for a given X or is this capturing the fact that X is like a symbol here?
[00:05:19.800 --> 00:05:20.400]   Yeah.
[00:05:20.400 --> 00:05:23.760]   If this is true, will this be true for any X?
[00:05:23.760 --> 00:05:28.040]   Let's actually, let's start with what the interface of this is little O function is.
[00:05:28.040 --> 00:05:28.480]   Okay.
[00:05:28.480 --> 00:05:34.280]   Is little O takes two expressions in terms of a variable and then takes a variable.
[00:05:34.280 --> 00:05:37.760]   So you could give me an expression that might have like multiple
[00:05:37.760 --> 00:05:40.080]   variables in it for G of X and F of X.
[00:05:40.080 --> 00:05:43.440]   So you need to tell me which variable, which variable is the one that we're
[00:05:43.440 --> 00:05:45.400]   taking the limit with respect to.
[00:05:45.600 --> 00:05:48.360]   So F of X could be eight times X plus three times Y.
[00:05:48.360 --> 00:05:53.600]   And then I would need to know which one of these is the, the actual variable of interest.
[00:05:53.600 --> 00:05:54.480]   Okay.
[00:05:54.480 --> 00:05:58.840]   Uh, so that's why there's that X at the end there, but then yes, it allows you to
[00:05:58.840 --> 00:06:04.120]   write very, all kinds of different expressions for X there in G of X and F of X.
[00:06:04.120 --> 00:06:08.440]   Excuse me, because there was, there's only one variable here, but you still
[00:06:08.440 --> 00:06:10.280]   need to pass to this sympi.limit.
[00:06:10.280 --> 00:06:12.280]   Whereas like if it was multiple variables, you would.
[00:06:12.280 --> 00:06:12.600]   Yeah.
[00:06:12.600 --> 00:06:12.800]   Yeah.
[00:06:12.800 --> 00:06:15.920]   So you'd have to, you'd have to specifically say which one it is.
[00:06:15.920 --> 00:06:16.640]   Yeah.
[00:06:16.640 --> 00:06:17.040]   Yeah.
[00:06:17.040 --> 00:06:22.600]   So you're going to be writing G's of X and F's of X here.
[00:06:22.600 --> 00:06:26.400]   So functions that you're going to write little sympi expressions
[00:06:26.400 --> 00:06:28.440]   and pass them into this function.
[00:06:28.440 --> 00:06:32.640]   And I just, as the person writing is little O, you need to tell me which
[00:06:32.640 --> 00:06:36.600]   variable that's in that expression to care about, but almost always, I think
[00:06:36.600 --> 00:06:40.040]   in every example in this notebook, there'll just be the one variable X.
[00:06:40.040 --> 00:06:40.400]   Okay.
[00:06:40.400 --> 00:06:43.960]   And just to be, just to make sure I'm understanding this correctly, this is
[00:06:43.960 --> 00:06:47.600]   just a Python way of doing this exact thing.
[00:06:47.600 --> 00:06:51.600]   So say this expression over this expression as the given variable
[00:06:51.600 --> 00:06:54.400]   tends to zero is equal to zero.
[00:06:54.400 --> 00:06:56.760]   And then you're just checking if that limit is equal to zero.
[00:06:56.760 --> 00:06:57.200]   Okay.
[00:06:57.200 --> 00:06:57.440]   Yep.
[00:06:57.440 --> 00:06:58.280]   Cool.
[00:06:58.280 --> 00:06:59.000]   Yeah, exactly.
[00:06:59.000 --> 00:07:00.240]   There's some comments there.
[00:07:00.240 --> 00:07:04.400]   There's the doc strings there to help folks understand what's going on there.
[00:07:04.400 --> 00:07:07.080]   But yeah, it's a Python way to get limits, which is really cool.
[00:07:07.080 --> 00:07:12.600]   I actually have started using sympi more and more for just like little tiny math
[00:07:12.600 --> 00:07:16.280]   things that I normally would have gone to like Wolframalpha.com to solve.
[00:07:16.280 --> 00:07:18.840]   There's this sort of long form explanation here of some of the stuff
[00:07:18.840 --> 00:07:20.120]   that Scott and I just talked about.
[00:07:20.120 --> 00:07:24.200]   Is little O operates on sympi expressions and sympi symbols.
[00:07:24.200 --> 00:07:27.600]   So a sympi symbol is like that X that you pointed out, Scott.
[00:07:27.600 --> 00:07:31.880]   It's like a variable in math in that you can, you know, you think of it as something
[00:07:31.880 --> 00:07:34.920]   that might take on lots of different values and when it takes on different
[00:07:34.920 --> 00:07:38.480]   values, importantly, a bunch of other things would also change their value.
[00:07:38.480 --> 00:07:43.640]   If X is doubled, then, you know, two times X is also doubled.
[00:07:43.640 --> 00:07:48.240]   Actually go ahead and two times X there, just like the way you would in Python.
[00:07:48.240 --> 00:07:48.880]   Yeah.
[00:07:48.880 --> 00:07:49.840]   Two times X.
[00:07:49.840 --> 00:07:51.440]   And now we get a new expression.
[00:07:51.440 --> 00:07:51.840]   Okay.
[00:07:51.840 --> 00:07:55.120]   So this is, I didn't even, prints nicely to look kind of like
[00:07:55.120 --> 00:07:56.360]   LaTeX or something like that.
[00:07:56.360 --> 00:08:00.120]   Yeah, there is LaTeX built in to sympi.
[00:08:00.120 --> 00:08:02.960]   It's the way it renders the expressions, which is pretty cool.
[00:08:02.960 --> 00:08:04.000]   That is pretty cool.
[00:08:04.000 --> 00:08:04.280]   Yeah.
[00:08:04.400 --> 00:08:07.160]   So we've got symbols, which are our variables.
[00:08:07.160 --> 00:08:11.120]   And then when we apply operations to them, we get expressions.
[00:08:11.120 --> 00:08:13.920]   And this is really very, very similar to what happens when
[00:08:13.920 --> 00:08:15.040]   you're writing a Python program.
[00:08:15.040 --> 00:08:18.280]   You have variables and then you apply functions and operations to them.
[00:08:18.280 --> 00:08:21.440]   And those things are expressions in Python as well.
[00:08:21.440 --> 00:08:24.960]   So we build expressions the same way we might build them in Python.
[00:08:24.960 --> 00:08:26.160]   Go and run this cell.
[00:08:26.160 --> 00:08:26.800]   You'll see.
[00:08:26.800 --> 00:08:30.960]   We've got all these different expressions here.
[00:08:30.960 --> 00:08:33.560]   And if you change the index, you can look at different ones.
[00:08:33.600 --> 00:08:37.040]   So we're going to do lots of different math operations to them.
[00:08:37.040 --> 00:08:37.400]   Yep.
[00:08:37.400 --> 00:08:39.960]   And then maybe minus one or three.
[00:08:39.960 --> 00:08:40.280]   Yeah.
[00:08:40.280 --> 00:08:40.720]   Cool.
[00:08:40.720 --> 00:08:44.040]   So you build expressions with numbers that are floats or ints.
[00:08:44.040 --> 00:08:45.720]   Use those operators.
[00:08:45.720 --> 00:08:49.920]   And then there's a bunch of sympi functions that are mathematical,
[00:08:49.920 --> 00:08:54.200]   these like mathematical functions, but you, you don't do it with numpy functions.
[00:08:54.200 --> 00:08:56.600]   It's not np.sign will work.
[00:08:56.600 --> 00:09:00.880]   Because numpy doesn't know anything about, about these like special
[00:09:00.880 --> 00:09:02.520]   mathematical expressions, right?
[00:09:02.560 --> 00:09:05.680]   I guess maybe the division is numpy is for computing stuff.
[00:09:05.680 --> 00:09:08.920]   And sympi is for like calculating stuff.
[00:09:08.920 --> 00:09:10.840]   The way physicists use it, that term.
[00:09:10.840 --> 00:09:13.880]   Phys, physicists use calculate to refer to like, you know, writing down a
[00:09:13.880 --> 00:09:17.200]   mathematical expression and then like simplifying it and getting an answer.
[00:09:17.200 --> 00:09:18.560]   That's what sympi helps you do.
[00:09:18.560 --> 00:09:20.240]   Okay, cool.
[00:09:20.240 --> 00:09:23.680]   So it's more for a mathematical purpose.
[00:09:23.680 --> 00:09:27.880]   That's not like you actually want to show values through it and see that.
[00:09:27.880 --> 00:09:32.040]   You just, you want to like analytically understand your equations.
[00:09:32.480 --> 00:09:32.720]   Yeah.
[00:09:32.720 --> 00:09:32.960]   Yeah.
[00:09:32.960 --> 00:09:34.080]   That's maybe a better term.
[00:09:34.080 --> 00:09:36.960]   Numpy is computational and sympi is analytical.
[00:09:36.960 --> 00:09:41.960]   I used it because recently I was looking into and trying to understand the
[00:09:41.960 --> 00:09:45.800]   logit normal density, which shows up in modeling of pandemics, actually.
[00:09:45.800 --> 00:09:48.320]   I wanted to calculate what its density was.
[00:09:48.320 --> 00:09:50.120]   And there's like an analytical formula.
[00:09:50.120 --> 00:09:53.280]   And I, at this point in my life, am better at Python.
[00:09:53.280 --> 00:09:56.440]   I am at manipulating analytical formulas for things.
[00:09:56.440 --> 00:10:00.480]   So to like check my work, I just put it in sympi and sympi gave me an answer.
[00:10:00.480 --> 00:10:01.120]   Cool.
[00:10:01.160 --> 00:10:06.000]   I feel like people might've seen, a lot of times when I've seen like how to
[00:10:06.000 --> 00:10:11.360]   understand a say formula, it would be like you shove a bunch of values through
[00:10:11.360 --> 00:10:15.760]   that maybe, and then see what maybe a Monte Carlo simulation or something like
[00:10:15.760 --> 00:10:20.600]   that, you would then see what the result thing, function or distribution of the
[00:10:20.600 --> 00:10:23.560]   outputs looks like, and that's your way of understanding it.
[00:10:23.560 --> 00:10:28.800]   But here, I guess you're, you're actually getting to the nitty gritty of more how a
[00:10:28.800 --> 00:10:30.960]   mathematician or a physicist would understand.
[00:10:31.160 --> 00:10:32.160]   The equation.
[00:10:32.160 --> 00:10:32.920]   Is that fair?
[00:10:32.920 --> 00:10:33.600]   Yeah.
[00:10:33.600 --> 00:10:33.840]   Yeah.
[00:10:33.840 --> 00:10:36.320]   And I actually think it's really good to be able to do both.
[00:10:36.320 --> 00:10:38.960]   And yeah, I like to be able to jump back and forth.
[00:10:38.960 --> 00:10:40.920]   Go ahead and run that little example there.
[00:10:40.920 --> 00:10:48.080]   And let's try and work through why we think that these answers are correct here.
[00:10:48.080 --> 00:10:52.560]   So we're calling is little o on x, x and x.
[00:10:52.560 --> 00:10:59.080]   And so what that's asking is, is the limit of x divided by x as x goes to zero.
[00:10:59.440 --> 00:11:01.080]   Is that equal to zero?
[00:11:01.080 --> 00:11:01.720]   Okay.
[00:11:01.720 --> 00:11:07.320]   So just thinking about that, this is the sort of like math exercise-iest part of
[00:11:07.320 --> 00:11:09.600]   this course, this like little o section here.
[00:11:09.600 --> 00:11:13.440]   So x divided by x, let's assume x is not zero, right?
[00:11:13.440 --> 00:11:14.840]   So it's some number other than zero.
[00:11:14.840 --> 00:11:16.360]   What's x divided by x going to be?
[00:11:16.360 --> 00:11:17.000]   It's going to be one.
[00:11:17.000 --> 00:11:19.400]   That's going to remain true as x goes to zero.
[00:11:19.400 --> 00:11:24.400]   And so the limit of x divided by x as x goes to zero is going to be one and not zero.
[00:11:24.400 --> 00:11:24.800]   Okay.
[00:11:24.800 --> 00:11:26.200]   So that's why it is false.
[00:11:26.200 --> 00:11:27.360]   That's why it's false.
[00:11:27.360 --> 00:11:27.640]   Yeah.
[00:11:28.000 --> 00:11:31.160]   What about x squared divided by x?
[00:11:31.160 --> 00:11:32.400]   So same exercise.
[00:11:32.400 --> 00:11:35.360]   What is x squared divided by x going to be when x is not zero?
[00:11:35.360 --> 00:11:39.480]   When x is not zero, what's x squared divided by x?
[00:11:39.480 --> 00:11:41.520]   It's going to be x.
[00:11:41.520 --> 00:11:43.520]   And x is going to zero.
[00:11:43.520 --> 00:11:47.520]   So then the limit as x goes to zero of x is zero.
[00:11:47.520 --> 00:11:48.120]   Okay.
[00:11:48.120 --> 00:11:53.160]   So that's how it's the math-iest part of the exercise.
[00:11:53.160 --> 00:11:56.960]   I've made a little spot here where people are encouraged to kind of like play around
[00:11:56.960 --> 00:12:02.320]   with this to change what f of x and g of x are and to run the function.
[00:12:02.320 --> 00:12:04.760]   There's like a little thing that checks what the limit is.
[00:12:04.760 --> 00:12:09.000]   And then there's something that tells you whether that counts as little o or not.
[00:12:09.000 --> 00:12:09.520]   Okay.
[00:12:09.520 --> 00:12:14.240]   So just to repeat what happened here, I just changed this to x squared, just to see that
[00:12:14.240 --> 00:12:15.400]   we got the same answer and we did it.
[00:12:15.400 --> 00:12:18.280]   My imagination here is pretty limited, but okay.
[00:12:18.280 --> 00:12:22.080]   I could raise it to a further power and it still is true.
[00:12:22.080 --> 00:12:23.520]   Maybe add something to it.
[00:12:23.520 --> 00:12:26.080]   I was going to bet to multiply something, but I'll do that next.
[00:12:26.080 --> 00:12:26.280]   Okay.
[00:12:26.280 --> 00:12:31.600]   So add something isn't, then maybe I'll just multiply while I'm at it.
[00:12:31.600 --> 00:12:31.960]   Yeah.
[00:12:31.960 --> 00:12:32.800]   Also isn't.
[00:12:32.800 --> 00:12:33.320]   Right.
[00:12:33.320 --> 00:12:39.800]   So two times x, it gives us the same answer for little o as just x, right?
[00:12:39.800 --> 00:12:43.840]   So run this without the times two and you get the limit as one and false.
[00:12:43.840 --> 00:12:47.880]   What this is trying to tell us is going back to the understanding of
[00:12:47.880 --> 00:12:49.320]   what the gradient does for you.
[00:12:49.320 --> 00:12:51.080]   The gradient gives you an approximation.
[00:12:51.480 --> 00:12:54.800]   And what we want out of our gradient approximation is that
[00:12:54.800 --> 00:12:56.880]   it can be as good as we want.
[00:12:56.880 --> 00:13:01.320]   And the way that's expressed in mathematics is that if you're close
[00:13:01.320 --> 00:13:04.040]   enough to zero, if you're trying to approximate the function close
[00:13:04.040 --> 00:13:05.400]   enough to the point that you started.
[00:13:05.400 --> 00:13:10.240]   Does that also mean that your, your error will be like going down
[00:13:10.240 --> 00:13:11.880]   as you get closer to that point.
[00:13:11.880 --> 00:13:16.120]   And so if your error to sort of close the loop here, if your error is equal
[00:13:16.120 --> 00:13:21.440]   to how far away you are from the starting point plus two, then your
[00:13:21.440 --> 00:13:23.480]   error is always going to be at least two.
[00:13:23.480 --> 00:13:27.200]   And so your approximation error, it doesn't get arbitrarily good.
[00:13:27.200 --> 00:13:30.600]   Your approximation is not arbitrarily good, right?
[00:13:30.600 --> 00:13:33.160]   It wouldn't be as good as the gradient approximation because the gradient
[00:13:33.160 --> 00:13:37.600]   approximation can become as good as we want as we get closer to the point
[00:13:37.600 --> 00:13:39.320]   where we're calculating the gradient.
[00:13:39.320 --> 00:13:39.920]   Okay.
[00:13:39.920 --> 00:13:40.360]   Yeah.
[00:13:40.360 --> 00:13:41.080]   That makes sense.
[00:13:41.080 --> 00:13:44.040]   It's just saying you have to take a small step.
[00:13:44.760 --> 00:13:49.720]   If you take that small step, is the error increasing with the size of your step
[00:13:49.720 --> 00:13:54.800]   or is it decreased or is it like still small with whatever the size of your step is?
[00:13:54.800 --> 00:13:56.960]   Is that a good, is that a way of understanding it?
[00:13:56.960 --> 00:13:58.040]   Yeah.
[00:13:58.040 --> 00:13:58.400]   Yeah.
[00:13:58.400 --> 00:14:03.680]   I would say for Luth purposes, thinking of it as, as I make my step size smaller,
[00:14:03.680 --> 00:14:07.520]   my approximation gets better and better is the right way to think of it.
[00:14:07.520 --> 00:14:10.680]   That's what this little o idea is like trying to capture.
[00:14:10.680 --> 00:14:15.320]   And then there's this like mathematical apparatus of limits that helps
[00:14:15.320 --> 00:14:17.680]   us express that idea clearly.
[00:14:17.680 --> 00:14:18.760]   All right.
[00:14:18.760 --> 00:14:21.640]   So what we're going to do here is just like play around a little
[00:14:21.640 --> 00:14:23.080]   bit with this little o notation.
[00:14:23.080 --> 00:14:26.800]   And by the way, if you, I've oriented this class more towards like
[00:14:26.800 --> 00:14:30.040]   programmery types, because that tends to be the types of people who are
[00:14:30.040 --> 00:14:31.440]   interested in machine learning.
[00:14:31.440 --> 00:14:35.160]   But if you like these like symbol manipulation mathematics things, there's
[00:14:35.160 --> 00:14:38.840]   like a blog post series that I wrote about the Frechet derivative, this style
[00:14:38.840 --> 00:14:42.040]   of computing derivatives that really shows you in detail how to do these
[00:14:42.040 --> 00:14:44.000]   algebraic manipulations and things like that.
[00:14:44.000 --> 00:14:47.400]   So if that's something that's of interest to you, there's links in this notebook
[00:14:47.400 --> 00:14:49.160]   and I'll add it on the YouTube video as well.
[00:14:49.160 --> 00:14:49.640]   Cool.
[00:14:49.640 --> 00:14:50.040]   Okay.
[00:14:50.040 --> 00:14:52.800]   We're just going to do some like quick exercises just to build a little
[00:14:52.800 --> 00:14:56.000]   intuition of what it means for something to be little o of X or not.
[00:14:56.000 --> 00:15:01.200]   With this one, there's a bunch of expressions and the goal is to answer
[00:15:01.200 --> 00:15:06.800]   these and say, is it true or is it not true that this expression is little o of X?
[00:15:06.960 --> 00:15:07.240]   Okay.
[00:15:07.240 --> 00:15:09.800]   So you lied to me when you said that the previous part
[00:15:09.800 --> 00:15:12.320]   was the most matziest part.
[00:15:12.320 --> 00:15:17.000]   Thankfully, I've got the first two already answered from before.
[00:15:17.000 --> 00:15:17.360]   Yeah.
[00:15:17.360 --> 00:15:20.440]   So I can say false for this one.
[00:15:20.440 --> 00:15:25.520]   And then if I remember correctly, this was true when it was raised to a power
[00:15:25.520 --> 00:15:27.040]   and that was the one I experimented with.
[00:15:27.040 --> 00:15:31.040]   And multiply, I think this one.
[00:15:31.040 --> 00:15:31.640]   Okay.
[00:15:31.640 --> 00:15:38.360]   So I can think through this as you decrease the size of your step.
[00:15:38.360 --> 00:15:38.920]   All right.
[00:15:38.920 --> 00:15:39.480]   What would it be?
[00:15:39.480 --> 00:15:41.520]   X over this.
[00:15:41.520 --> 00:15:44.840]   So it's five times X over X.
[00:15:44.840 --> 00:15:45.520]   Okay.
[00:15:45.520 --> 00:15:45.640]   Yeah.
[00:15:45.640 --> 00:15:45.920]   Sorry.
[00:15:45.920 --> 00:15:47.280]   That's, that's what I wanted to know.
[00:15:47.280 --> 00:15:53.120]   The distinction here is the first argument there is, is X here for all of them.
[00:15:53.120 --> 00:15:56.560]   For the is little o, the first argument in this one is X.
[00:15:56.560 --> 00:15:59.360]   So we're just asking, are these things little o of X?
[00:15:59.600 --> 00:16:05.000]   And so that's the ratio of this thing over X is our, is our ratio we take the limit of.
[00:16:05.000 --> 00:16:05.320]   Yeah.
[00:16:05.320 --> 00:16:05.520]   Okay.
[00:16:05.520 --> 00:16:08.880]   I think I missed that because here we had a sort of variable
[00:16:08.880 --> 00:16:11.400]   input function here and that's set to be X.
[00:16:11.400 --> 00:16:11.640]   Okay.
[00:16:11.640 --> 00:16:11.960]   Sweet.
[00:16:11.960 --> 00:16:17.960]   Five times X over X as X tends to zero does the error, no.
[00:16:17.960 --> 00:16:20.200]   Let me, let me put it another way to you.
[00:16:20.200 --> 00:16:23.200]   We already know that X is not little o of X.
[00:16:23.200 --> 00:16:25.000]   That's that first thing you've got there, right?
[00:16:25.000 --> 00:16:25.400]   Yeah.
[00:16:25.600 --> 00:16:32.280]   So we know that X is not little o of X and the reason why, do you remember, so the
[00:16:32.280 --> 00:16:34.960]   limit that tells me that the limit is not zero.
[00:16:34.960 --> 00:16:36.840]   Do you remember what the limit was?
[00:16:36.840 --> 00:16:39.360]   It's actually still on the screen funnily enough.
[00:16:39.360 --> 00:16:40.320]   What was the one?
[00:16:40.320 --> 00:16:44.320]   So if I multiply it by five, do you know what's going to happen to the limit?
[00:16:44.320 --> 00:16:46.560]   The, the limit will be multiplied by five.
[00:16:46.560 --> 00:16:48.040]   Yeah, exactly.
[00:16:48.040 --> 00:16:48.800]   So, okay.
[00:16:48.800 --> 00:16:52.080]   I can kind of, the constants get moved out of the limit.
[00:16:52.080 --> 00:16:53.160]   Is that, can I say that?
[00:16:53.160 --> 00:16:53.440]   Yep.
[00:16:53.440 --> 00:16:54.040]   Yeah.
[00:16:54.080 --> 00:16:56.760]   This is actually the, the way this is expressed is actually
[00:16:56.760 --> 00:16:59.080]   that limits are linear operations.
[00:16:59.080 --> 00:17:03.480]   So we talked about linearity for vector operations and matrix operations.
[00:17:03.480 --> 00:17:07.800]   The limit is in some sense, the same thing, but applied to like sequences.
[00:17:07.800 --> 00:17:10.320]   And so limits are also linear.
[00:17:10.320 --> 00:17:15.680]   And so multiplying this by five, just multiplies the limit by five.
[00:17:15.680 --> 00:17:20.480]   And so the limit is now five and that limit is not zero.
[00:17:20.480 --> 00:17:23.920]   And so the answer here is yeah, false.
[00:17:23.960 --> 00:17:24.680]   That's helpful.
[00:17:24.680 --> 00:17:27.960]   And I think that that, that helps my understanding that I can think of this
[00:17:27.960 --> 00:17:31.080]   as just, yeah, multiplying the limit itself.
[00:17:31.080 --> 00:17:35.240]   I don't have to actually consider this as some new thing that changes the rules in any way.
[00:17:35.240 --> 00:17:35.960]   Right.
[00:17:35.960 --> 00:17:40.120]   So that's why when people write little O, nobody ever writes like little O of five
[00:17:40.120 --> 00:17:42.400]   times X, you would never really write it that way.
[00:17:42.400 --> 00:17:45.480]   I did that once in a paper, cause I had to like, I really wanted to
[00:17:45.480 --> 00:17:48.760]   emphasize something in particular, but it's like very rare that you would
[00:17:48.760 --> 00:17:52.800]   have that because this is just about whether the limit is zero or not.
[00:17:53.040 --> 00:17:56.560]   And all of that information about constants gets, gets thrown away.
[00:17:56.560 --> 00:17:57.080]   Simplified.
[00:17:57.080 --> 00:17:59.520]   And I guess it's the same for big O.
[00:17:59.520 --> 00:18:04.560]   That's one thing you often see in big O is that you don't have to consider if it's
[00:18:04.560 --> 00:18:08.280]   twice it, or if it's, it's got a small constant at it, you can just get rid of
[00:18:08.280 --> 00:18:11.400]   that and simplify it to whatever, and then there's a set that you just have
[00:18:11.400 --> 00:18:14.040]   to check, is that big O correct?
[00:18:14.040 --> 00:18:19.560]   So when physicists use big O and mathematicians use big O or little O,
[00:18:19.560 --> 00:18:21.480]   they usually think about X getting small.
[00:18:21.880 --> 00:18:23.520]   So we said the limit is X goes to zero.
[00:18:23.520 --> 00:18:26.480]   Computer scientists think about what happens when the input gets really big.
[00:18:26.480 --> 00:18:31.760]   So computer scientists think about big O, what happens, how many operations does it
[00:18:31.760 --> 00:18:33.800]   take as my input gets bigger and bigger?
[00:18:33.800 --> 00:18:38.040]   So that one thing that that's the same is that these like multiplicative factors
[00:18:38.040 --> 00:18:39.800]   can be thrown out cause they are, it's not important.
[00:18:39.800 --> 00:18:44.000]   If you go over a list twice, that's much less important than whether you have to
[00:18:44.000 --> 00:18:47.720]   go over the list and also for each thing in the list, go over the list.
[00:18:47.960 --> 00:18:50.680]   2N is much, much smaller than N squared.
[00:18:50.680 --> 00:18:52.440]   Once you get to really large N.
[00:18:52.440 --> 00:18:55.280]   Big O notation that people are maybe familiar with from an algorithms and
[00:18:55.280 --> 00:18:58.520]   data structures class, or from like trying to reason about when their code's
[00:18:58.520 --> 00:19:02.520]   going to run slow or run fast, that does that same thing of hiding a
[00:19:02.520 --> 00:19:04.560]   lot of those, those kinds of details.
[00:19:04.560 --> 00:19:05.280]   Okay, cool.
[00:19:05.280 --> 00:19:08.640]   This is the same cause this is just another constant or
[00:19:08.640 --> 00:19:10.560]   another multiplicative factor.
[00:19:10.560 --> 00:19:11.120]   Yep.
[00:19:11.120 --> 00:19:15.560]   We've already figured out that this continues to be true as you raise it.
[00:19:15.640 --> 00:19:19.720]   And then this, I assume doesn't affect things like it doesn't affect things here.
[00:19:19.720 --> 00:19:21.600]   So I can say true again.
[00:19:21.600 --> 00:19:23.600]   And then I'm going to, I'm actually going to stop you for a
[00:19:23.600 --> 00:19:25.160]   second before we get to the last one.
[00:19:25.160 --> 00:19:28.840]   So I actually think it's useful to, you're not falling into my trap, which is good.
[00:19:28.840 --> 00:19:32.600]   But the trap here, part of the purpose of teaching is to lay traps for students
[00:19:32.600 --> 00:19:36.240]   where you can detect that they might not have understood something correctly so
[00:19:36.240 --> 00:19:39.640]   that you can, so that you can help them pull themselves out of the trap and
[00:19:39.640 --> 00:19:41.120]   then understand things better.
[00:19:41.120 --> 00:19:45.440]   But so the idea there is like, if I multiply X by one divided by a million,
[00:19:45.440 --> 00:19:47.440]   I'm making X really small.
[00:19:47.440 --> 00:19:49.240]   I'm making X a million times smaller.
[00:19:49.240 --> 00:19:53.720]   And so this, this term here, what this term represents is the
[00:19:53.720 --> 00:19:55.680]   like magnitude of our error.
[00:19:55.680 --> 00:19:58.120]   And people are maybe tempted.
[00:19:58.120 --> 00:20:01.280]   And because we often have to think in terms of like the absolute magnitudes,
[00:20:01.280 --> 00:20:04.680]   a million times smaller error, like that's pretty small, but it's not
[00:20:04.680 --> 00:20:08.080]   the case that making the error a million times smaller changes this little O.
[00:20:08.080 --> 00:20:08.880]   Okay.
[00:20:08.880 --> 00:20:14.960]   The error, even though it's scaled by such a small number, it still doesn't
[00:20:14.960 --> 00:20:18.000]   change how it scales with X.
[00:20:18.000 --> 00:20:18.680]   Exactly.
[00:20:18.680 --> 00:20:19.040]   Yeah.
[00:20:19.040 --> 00:20:19.560]   Okay.
[00:20:19.560 --> 00:20:23.720]   It's still going to change like linearly with how I change it.
[00:20:23.720 --> 00:20:24.360]   Yes.
[00:20:24.360 --> 00:20:29.760]   So it is, it is not the case that it, that like you could just make X small
[00:20:29.760 --> 00:20:36.520]   enough that one in a million times X is arbitrarily smaller than X.
[00:20:36.520 --> 00:20:40.880]   It's always going to be a million times smaller than X, but X squared, on the
[00:20:40.880 --> 00:20:46.040]   other hand, as I make X smaller and smaller X squared, X squared will be a
[00:20:46.040 --> 00:20:47.880]   million times smaller than X.
[00:20:47.880 --> 00:20:55.560]   If X is one in a thousand, if X is one in a million, then X squared will be one
[00:20:55.560 --> 00:20:59.960]   in a million squared, one in a million times a million, which is one in a trillion.
[00:20:59.960 --> 00:21:03.240]   Uh, so X squared will become a trillion times smaller than X.
[00:21:03.240 --> 00:21:07.560]   And like, if, if a trillion times smaller is still not enough smaller for me, then
[00:21:07.560 --> 00:21:11.880]   like I can make X even smaller, I can make X one in a trillion and then X
[00:21:11.880 --> 00:21:14.040]   squared is one in a trillion trillion.
[00:21:14.040 --> 00:21:16.920]   Whereas on the other hand, X divided by a million is always
[00:21:16.920 --> 00:21:18.440]   just a million times smaller.
[00:21:18.440 --> 00:21:18.720]   Yeah.
[00:21:18.720 --> 00:21:18.960]   Okay.
[00:21:18.960 --> 00:21:19.680]   That makes sense.
[00:21:19.680 --> 00:21:22.960]   Um, so that's what this is trying to express and then just like scaling
[00:21:22.960 --> 00:21:26.040]   something up, so it's really, it seems like it would be really, really big.
[00:21:26.040 --> 00:21:30.080]   You can always with X cubed, I can just keep making X smaller and smaller, and
[00:21:30.080 --> 00:21:33.720]   eventually it'll swamp out any effect of that multiplying by a million there.
[00:21:33.720 --> 00:21:34.400]   Okay.
[00:21:34.400 --> 00:21:34.920]   I get you.
[00:21:34.920 --> 00:21:41.200]   So it's, even though this number itself is very small, if this ends up being the
[00:21:41.200 --> 00:21:48.200]   smaller of the two, where like this has six zeros here or whatever, this could
[00:21:48.200 --> 00:21:51.680]   have 10 zeros and now all of a sudden this is the dominant thing that, like
[00:21:51.680 --> 00:21:55.880]   if I'm already at one in a, one in a trillion trillion trillion, then changing
[00:21:55.880 --> 00:22:00.320]   by a factor of a million is not that big of a deal, but changing by squaring
[00:22:00.320 --> 00:22:03.080]   has become even more of a big impact.
[00:22:03.200 --> 00:22:06.280]   So let's go ahead and actually just run the grader on these.
[00:22:06.280 --> 00:22:08.480]   It'll check that we've gotten all the answers correctly.
[00:22:08.480 --> 00:22:10.120]   Okay.
[00:22:10.120 --> 00:22:11.800]   All right.
[00:22:11.800 --> 00:22:13.440]   I haven't added, I haven't added sympi.
[00:22:13.440 --> 00:22:14.360]   Yeah.
[00:22:14.360 --> 00:22:17.560]   A sympi dot exp, taking the exponential function.
[00:22:17.560 --> 00:22:20.080]   And it looks like I've given away the answer for you there.
[00:22:20.080 --> 00:22:22.200]   But what I wanted to do was actually just look at these.
[00:22:22.200 --> 00:22:25.880]   The first one there, little o of X is like a strictly less than symbol.
[00:22:25.880 --> 00:22:28.480]   X is not strictly less than X.
[00:22:28.840 --> 00:22:31.160]   X is less than or equal to X.
[00:22:31.160 --> 00:22:33.640]   Um, but X is not strictly less than X.
[00:22:33.640 --> 00:22:36.000]   That was one of the things I wanted to get across in the first one.
[00:22:36.000 --> 00:22:36.320]   Yeah.
[00:22:36.320 --> 00:22:38.600]   Multiplying by a constant doesn't change anything.
[00:22:38.600 --> 00:22:39.120]   Yeah.
[00:22:39.120 --> 00:22:39.960]   Close to zero.
[00:22:39.960 --> 00:22:41.720]   X is smaller than X squared.
[00:22:41.720 --> 00:22:44.800]   And then finally, yeah, the exponential function is not
[00:22:44.800 --> 00:22:46.800]   little o of X to the N for any N.
[00:22:46.800 --> 00:22:49.000]   So yeah, the exponential function.
[00:22:49.000 --> 00:22:52.120]   Maybe one way of thinking about it is as you get close to zero,
[00:22:52.120 --> 00:22:53.680]   this value gets close to one.
[00:22:53.680 --> 00:22:55.720]   I guess I can have a look at that.
[00:22:55.800 --> 00:22:59.720]   If we have time I can, and I can just do like numpy dot X.
[00:22:59.720 --> 00:23:01.320]   Oh, I guess we don't have that.
[00:23:01.320 --> 00:23:01.840]   Can I do?
[00:23:01.840 --> 00:23:03.400]   Yeah, I wouldn't use numpy on this.
[00:23:03.400 --> 00:23:05.280]   What I would do is go up to.
[00:23:05.280 --> 00:23:06.240]   Can I do this?
[00:23:06.240 --> 00:23:09.440]   Oh, well, it'll give you a number, but what's maybe more helpful would
[00:23:09.440 --> 00:23:13.960]   be to go up to the cell where you can check is little o of X here
[00:23:13.960 --> 00:23:15.240]   at the bottom of your screen there.
[00:23:15.240 --> 00:23:15.480]   Yeah.
[00:23:15.480 --> 00:23:18.480]   So change F of X and G of X up at the top there.
[00:23:18.480 --> 00:23:19.640]   Oh, okay.
[00:23:19.640 --> 00:23:19.880]   Yeah.
[00:23:19.880 --> 00:23:20.480]   Yeah.
[00:23:20.480 --> 00:23:24.080]   So change F of X to sympi dot exp of X.
[00:23:24.080 --> 00:23:24.960]   Right.
[00:23:24.960 --> 00:23:25.240]   Yeah.
[00:23:25.240 --> 00:23:33.120]   So the, the limit is infinite here because the E to the X goes to one as X goes to
[00:23:33.120 --> 00:23:33.680]   zero.
[00:23:33.680 --> 00:23:40.560]   So if your error is scales like E to the X or E to the Epsilon, then as Epsilon
[00:23:40.560 --> 00:23:43.120]   gets small, E to the Epsilon doesn't get small.
[00:23:43.120 --> 00:23:44.440]   It becomes order one.
[00:23:44.440 --> 00:23:47.960]   And so your error is always going to be like size one.
[00:23:47.960 --> 00:23:53.440]   And so the ratio of your error to how big your step is, is like blowing up.
[00:23:53.440 --> 00:23:53.960]   Okay.
[00:23:54.440 --> 00:23:55.480]   That's what I was going to say.
[00:23:55.480 --> 00:23:58.840]   That's what I was going to just check with this is just actually look at how
[00:23:58.840 --> 00:24:03.000]   the value changes as we approach zero.
[00:24:03.000 --> 00:24:06.640]   And I, and I'm, and I'm seeing it's like converging on, if I do, if I do this,
[00:24:06.640 --> 00:24:13.000]   it's converging on one, which it should be converging on zero quicker than this.
[00:24:13.000 --> 00:24:13.640]   Yeah.
[00:24:13.640 --> 00:24:15.440]   So go ahead and divide it by that number.
[00:24:15.440 --> 00:24:16.720]   Yeah.
[00:24:16.720 --> 00:24:19.880]   It's take sympi dot exp and then yeah, copy and paste that number.
[00:24:19.880 --> 00:24:21.400]   So divided by number.
[00:24:21.400 --> 00:24:21.880]   Yeah.
[00:24:21.880 --> 00:24:22.200]   Okay.
[00:24:22.200 --> 00:24:23.680]   Just so I have this.
[00:24:23.680 --> 00:24:23.960]   Yeah.
[00:24:23.960 --> 00:24:24.160]   Yeah.
[00:24:24.160 --> 00:24:24.640]   That's smart.
[00:24:24.640 --> 00:24:25.400]   Yeah.
[00:24:25.400 --> 00:24:28.960]   So far from going to zero, this is zooming off to infinity.
[00:24:28.960 --> 00:24:30.320]   Okay, cool.
[00:24:30.320 --> 00:24:31.200]   Yeah, that makes sense.
[00:24:31.200 --> 00:24:37.600]   So even if it's converging to one, this number as, as my input goes to zero for
[00:24:37.600 --> 00:24:42.040]   my limit, it needs to be converging to zero here and actually making it get a lot worse.
[00:24:42.040 --> 00:24:42.320]   Yeah.
[00:24:42.320 --> 00:24:42.880]   Okay.
[00:24:42.880 --> 00:24:46.840]   So then that means false here.
[00:24:46.840 --> 00:24:48.240]   And I pass all the tests.
[00:24:48.240 --> 00:24:49.000]   Great.
[00:24:49.000 --> 00:24:52.880]   So there's a pretty similar thing for little O of X squared, but it's so
[00:24:52.880 --> 00:24:57.680]   similar and like the ideas are so close that rather than going through this one,
[00:24:57.680 --> 00:25:00.480]   I'd rather just keep on trucking to the rest of the exercises.
[00:25:00.480 --> 00:25:02.840]   So folks can work through those exercises.
[00:25:02.840 --> 00:25:05.000]   If you have any questions, post them on YouTube, post them on the
[00:25:05.000 --> 00:25:07.440]   forum and we can answer them.
[00:25:07.440 --> 00:25:10.480]   But I want to move on to thinking about the gradient.
[00:25:10.480 --> 00:25:13.800]   Cause we've been talking about little O, which is a useful idea, helpful for
[00:25:13.800 --> 00:25:17.160]   building your intuition about what approximations are for, but it's not,
[00:25:17.160 --> 00:25:19.160]   it's not directly related to machine learning.
[00:25:19.160 --> 00:25:22.480]   And I want to tie what we're talking about always back to what's going on in
[00:25:22.480 --> 00:25:23.040]   machine learning.
[00:25:23.040 --> 00:25:23.760]   Okay.
[00:25:23.760 --> 00:25:28.360]   The importance of this little O notation there is that it gives us a formal way
[00:25:28.360 --> 00:25:33.080]   to talk about approximation and when an approximation is like a good approximation
[00:25:33.080 --> 00:25:34.680]   versus not a good approximation.
[00:25:34.680 --> 00:25:37.600]   So we can say the two things are approximately the same.
[00:25:37.600 --> 00:25:40.640]   Without all the formality, what we'd want to write in order to explain what the
[00:25:40.640 --> 00:25:44.920]   gradient is, is we would say that if we want to know what the value of a function
[00:25:44.920 --> 00:25:52.680]   is somewhere else, it is not equal to, but maybe almost equal to f of X plus the
[00:25:52.680 --> 00:25:55.520]   gradient times how far away we're looking.
[00:25:55.520 --> 00:25:59.640]   Epsilon there, importantly, like usually are, we're calculating the
[00:25:59.640 --> 00:26:03.040]   derivatives or gradients of functions that take in vectors as input.
[00:26:03.040 --> 00:26:07.600]   So epsilon here isn't just a number, but actually a vector of numbers.
[00:26:07.600 --> 00:26:12.560]   And so if I want to say I'm at a point X, and then I want to see what the value of
[00:26:12.560 --> 00:26:15.840]   the function is over here, this is my epsilon here.
[00:26:15.840 --> 00:26:20.240]   And so I take that direction, multiply that vector with the gradient, and that
[00:26:20.240 --> 00:26:24.160]   gives me how much the function will have changed if I move in that direction.
[00:26:24.160 --> 00:26:25.160]   Approximately.
[00:26:25.160 --> 00:26:27.000]   Not exactly, but approximately.
[00:26:27.000 --> 00:26:27.440]   Okay.
[00:26:27.440 --> 00:26:32.880]   Just so I'm sure I understand this, we have some change in input, this epsilon,
[00:26:32.880 --> 00:26:39.120]   and we're trying to say, if when we add this small change to our input, it's
[00:26:39.160 --> 00:26:45.440]   approximately equal to our original function with the original input and some
[00:26:45.440 --> 00:26:48.600]   gradient times that small change that we added in.
[00:26:48.600 --> 00:26:49.120]   Yes.
[00:26:49.120 --> 00:26:49.400]   Is that?
[00:26:49.400 --> 00:26:49.800]   Okay.
[00:26:49.800 --> 00:26:54.640]   And the important thing here is what we have now is we have an approximation to
[00:26:54.640 --> 00:26:59.200]   f that doesn't require, require us to actually use the function f directly.
[00:26:59.200 --> 00:27:03.400]   We have its gradient, which once I calculate gradient f of X,
[00:27:03.400 --> 00:27:05.560]   that just becomes a vector here.
[00:27:05.840 --> 00:27:10.400]   And so I just have a vector and I have the value of f of X, and now I can
[00:27:10.400 --> 00:27:15.040]   approximate the value of f anywhere else, not just at X, but anywhere else I can
[00:27:15.040 --> 00:27:16.760]   calculate an approximate value for it.
[00:27:16.760 --> 00:27:20.520]   When I'm close to X, it'll be probably a good approximation.
[00:27:20.520 --> 00:27:22.880]   Further away, the approximation gets worse and worse.
[00:27:22.880 --> 00:27:27.800]   But the upshot is now I have something that may be like millions of times
[00:27:27.800 --> 00:27:30.720]   cheaper to compute than f to calculate this.
[00:27:30.720 --> 00:27:35.800]   So I now have this nice approximation that might be computationally easier to use.
[00:27:35.880 --> 00:27:36.240]   Okay.
[00:27:36.240 --> 00:27:36.680]   Okay.
[00:27:36.680 --> 00:27:37.680]   Yeah, that makes sense.
[00:27:37.680 --> 00:27:39.440]   And the approximation is linear.
[00:27:39.440 --> 00:27:43.440]   It's based on linear algebra operations, specifically it's, that's a dot product
[00:27:43.440 --> 00:27:46.560]   there, we talked about the dot product in the linear algebra section, so that
[00:27:46.560 --> 00:27:50.920]   makes it maybe easy to use, easier to reason about and do math about than
[00:27:50.920 --> 00:27:54.040]   like a generic function, linear functions are nice, they're simpler.
[00:27:54.040 --> 00:27:56.000]   And then how, how big is our error?
[00:27:56.000 --> 00:28:01.120]   We know that the error, because it's little o, must shrink at a rate faster than epsilon.
[00:28:01.120 --> 00:28:05.000]   So what that says is like, if you have some tolerance for error, there exists a
[00:28:05.000 --> 00:28:08.840]   small enough epsilon such that your error won't be too big, but we don't
[00:28:08.840 --> 00:28:09.960]   know that much else about it.
[00:28:09.960 --> 00:28:12.160]   Just knowing that a function has a gradient doesn't tell you how
[00:28:12.160 --> 00:28:13.800]   big your errors are going to be.
[00:28:13.800 --> 00:28:18.440]   Where this shows up in machine learning is that all that gradients guarantee you
[00:28:18.440 --> 00:28:23.280]   is that there exists a step size that somewhere out there, there's a step
[00:28:23.280 --> 00:28:26.520]   size small enough that if you take it, your loss will go down.
[00:28:26.520 --> 00:28:31.280]   And that's cold comfort often to know that there exists a small step
[00:28:31.280 --> 00:28:32.640]   size if you don't know what it is.
[00:28:32.680 --> 00:28:36.360]   And just to be more concrete, what I think you're referring to here is choosing
[00:28:36.360 --> 00:28:40.280]   the learning rate is like a notoriously difficult thing and that's where it
[00:28:40.280 --> 00:28:44.880]   shows up and how you, you choose that step size is what this learning rate
[00:28:44.880 --> 00:28:46.560]   hyper parameter you have to set is.
[00:28:46.560 --> 00:28:46.760]   Yeah.
[00:28:46.760 --> 00:28:46.960]   Yeah.
[00:28:46.960 --> 00:28:47.440]   Sorry.
[00:28:47.440 --> 00:28:52.480]   Learning rate and step size are I think synonymous, but these days I think
[00:28:52.480 --> 00:28:56.160]   learning rate has really taken over because step size was what the sort of
[00:28:56.160 --> 00:28:59.960]   like classical ML people like to call it more often, like classical optimization.
[00:29:00.000 --> 00:29:03.640]   And then learning rate was the term in the neural network world coming from
[00:29:03.640 --> 00:29:07.240]   like cognitive science, connectionism, thinking about this as learning,
[00:29:07.240 --> 00:29:10.680]   not just optimization, step size, learning rate, same thing.
[00:29:10.680 --> 00:29:12.960]   So little O guarantees us that there exists a learning rate
[00:29:12.960 --> 00:29:13.920]   doesn't tell us what it is.
[00:29:13.920 --> 00:29:15.440]   So then we have to figure it out ourselves.
[00:29:15.440 --> 00:29:20.560]   What, what I want to do is I want to play around with this thing, make use of it a
[00:29:20.560 --> 00:29:24.000]   bit, show you what some gradient approximations look like.
[00:29:24.000 --> 00:29:26.840]   So now we're going to switch gears a little bit rather than using
[00:29:26.840 --> 00:29:29.920]   SymPy, which allows us to like symbolically manipulate stuff.
[00:29:29.920 --> 00:29:33.720]   We're going to go over and use a different library called Autograd
[00:29:33.720 --> 00:29:35.680]   that will give us gradients for stuff.
[00:29:35.680 --> 00:29:39.240]   So Autograd gives you gradients for NumPy functions.
[00:29:39.240 --> 00:29:42.320]   It's basically the library that the automatic differentiation
[00:29:42.320 --> 00:29:44.120]   in Torch is based off of.
[00:29:44.120 --> 00:29:47.200]   It's also very similar to the same automatic differentiation stuff
[00:29:47.200 --> 00:29:48.360]   that you'll find in TensorFlow.
[00:29:48.360 --> 00:29:51.800]   That's what this Autograd thing here is for.
[00:29:51.800 --> 00:29:53.160]   Go ahead and run that cell.
[00:29:53.160 --> 00:29:53.680]   Okay.
[00:29:53.880 --> 00:29:58.480]   You may be familiar with Torch.grad.
[00:29:58.480 --> 00:30:01.360]   Oh, that's what happens when you call dot backward.
[00:30:01.360 --> 00:30:01.760]   Yeah.
[00:30:01.760 --> 00:30:02.720]   You can think of it that way.
[00:30:02.720 --> 00:30:06.320]   When you call that backward, you evaluate and get a specific
[00:30:06.320 --> 00:30:08.240]   gradient vector at a specific point.
[00:30:08.240 --> 00:30:13.120]   And so you get that noble F of X that is in the definition that up there,
[00:30:13.120 --> 00:30:15.200]   you get that value as a gradient.
[00:30:15.200 --> 00:30:20.480]   What Autograd.grad does here is you give it a function F and it gives you back a
[00:30:20.480 --> 00:30:25.160]   new function that is like that upside down triangle sign, nabla F.
[00:30:25.160 --> 00:30:29.760]   So that nabla F function now takes in an X and gives you a vector.
[00:30:29.760 --> 00:30:32.760]   I think you've lost me now with your terminology.
[00:30:32.760 --> 00:30:34.080]   Nabla?
[00:30:34.080 --> 00:30:35.480]   Is that the same as delta?
[00:30:35.480 --> 00:30:37.640]   Ah, it's an upside down triangle.
[00:30:37.640 --> 00:30:40.240]   So actually, yeah, go up a little bit to this right there.
[00:30:40.240 --> 00:30:41.760]   That's pronounced nabla.
[00:30:41.760 --> 00:30:43.320]   It is an upside down delta.
[00:30:43.320 --> 00:30:46.160]   I don't think, I don't think it's actually a Greek letter.
[00:30:46.160 --> 00:30:49.240]   I think it was like invented by physicists, but anyway.
[00:30:49.400 --> 00:30:51.120]   Yeah, you can think of it as the gradient symbol.
[00:30:51.120 --> 00:30:52.760]   I'll just call it the gradient symbol.
[00:30:52.760 --> 00:30:58.240]   So gradient symbol F or grad F is a function that takes in an X and gives you
[00:30:58.240 --> 00:31:00.840]   that vector you can use to approximate the function.
[00:31:00.840 --> 00:31:06.200]   So Autograd.grad takes in a function and gives you back a new function, grad F.
[00:31:06.200 --> 00:31:11.800]   So then we can call that function and get, so grad identity at two is one,
[00:31:11.800 --> 00:31:14.760]   because the identity function, the gradient is always one.
[00:31:14.760 --> 00:31:18.400]   And then the constant function, the gradient is always zero.
[00:31:18.520 --> 00:31:20.040]   I'm giving it a function.
[00:31:20.040 --> 00:31:25.560]   It's giving me now a new function that I can pass in values and it'll tell me what
[00:31:25.560 --> 00:31:28.680]   the gradient of my original function would be.
[00:31:28.680 --> 00:31:29.440]   Is that true?
[00:31:29.440 --> 00:31:29.840]   Yeah.
[00:31:29.840 --> 00:31:32.920]   What the gradient of the original function is at that specific point.
[00:31:32.920 --> 00:31:33.600]   At that point.
[00:31:33.600 --> 00:31:33.920]   Okay.
[00:31:33.920 --> 00:31:37.200]   So there's a little bit of a confusion of terminology here because different
[00:31:37.200 --> 00:31:39.200]   people care about different pieces of this.
[00:31:39.200 --> 00:31:43.200]   So sometimes people use gradient to refer to like Autograd.grad, like
[00:31:43.200 --> 00:31:46.880]   gradient's the operator that takes in a function and returns a new function.
[00:31:47.160 --> 00:31:48.760]   That's Autograd.grad.
[00:31:48.760 --> 00:31:53.960]   Other people refer to that thing on the left there, grad identity, as like the
[00:31:53.960 --> 00:31:58.920]   gradient of the identity, the gradient of F that takes in the same inputs
[00:31:58.920 --> 00:32:01.600]   as F and returns vectors, and then.
[00:32:01.600 --> 00:32:05.920]   Other people, especially in machine learning, think of the specific values
[00:32:05.920 --> 00:32:08.800]   returned by that function as the gradients.
[00:32:08.800 --> 00:32:10.760]   When you talk about, Oh, I need to calculate gradients.
[00:32:10.760 --> 00:32:12.160]   I need to store my gradients.
[00:32:12.160 --> 00:32:16.360]   I need to apply my gradients to my parameters in order to do gradient descent.
[00:32:16.440 --> 00:32:20.560]   All those uses of the term are thinking of actually those final output
[00:32:20.560 --> 00:32:22.120]   values printed underneath the cell.
[00:32:22.120 --> 00:32:24.480]   They're like actual concrete numbers there.
[00:32:24.480 --> 00:32:27.560]   So people kind of use the word gradient for all of all three of those things.
[00:32:27.560 --> 00:32:29.160]   And you just need to know from the context.
[00:32:29.160 --> 00:32:29.480]   Okay.
[00:32:29.480 --> 00:32:31.400]   Maybe just a quick check.
[00:32:31.400 --> 00:32:35.680]   Is it clear why the gradient of the constant function is always zero?
[00:32:35.680 --> 00:32:37.640]   Looking at that constant function there.
[00:32:37.640 --> 00:32:38.360]   Yes.
[00:32:38.360 --> 00:32:44.720]   Because if it'll always return one, which doesn't change as X changes.
[00:32:45.000 --> 00:32:50.600]   So the gradient doesn't, the error doesn't get smaller as I change my input.
[00:32:50.600 --> 00:32:52.520]   Yeah, that's an interesting way of thinking about it.
[00:32:52.520 --> 00:32:55.120]   So the little O term there is actually always zero.
[00:32:55.120 --> 00:32:57.760]   The first intuition that you threw out there is a good one.
[00:32:57.760 --> 00:32:59.960]   It's like, Oh, this function doesn't change.
[00:32:59.960 --> 00:33:01.680]   And so the gradient should be zero.
[00:33:01.680 --> 00:33:03.920]   Cause if there's no change, there's no gradient.
[00:33:03.920 --> 00:33:05.120]   That's a useful intuition.
[00:33:05.120 --> 00:33:07.960]   And that's the intuition that comes a little bit more, I think, from thinking
[00:33:07.960 --> 00:33:11.880]   about physics and thinking of gradients in terms of like directions of motion.
[00:33:11.880 --> 00:33:14.720]   That's definitely a good intuition to have for doing machine learning
[00:33:14.720 --> 00:33:16.120]   and for thinking about gradient descent.
[00:33:16.120 --> 00:33:20.200]   With our linear approximation intuition that we're trying to build in this
[00:33:20.200 --> 00:33:25.400]   session, the idea is that if this function is constant, right, then what's the best
[00:33:25.400 --> 00:33:27.680]   linear approximation to a constant function?
[00:33:27.680 --> 00:33:29.160]   Just that value itself.
[00:33:29.160 --> 00:33:29.440]   No.
[00:33:29.440 --> 00:33:29.800]   Yeah.
[00:33:29.800 --> 00:33:30.040]   Yeah.
[00:33:30.040 --> 00:33:30.240]   Yeah.
[00:33:30.240 --> 00:33:33.600]   It's a flat line, I guess is maybe the way I would put it, but I think what you
[00:33:33.600 --> 00:33:35.440]   said, you said is also the same idea.
[00:33:35.440 --> 00:33:39.320]   So because this function is a flat line, the linear approximation to
[00:33:39.320 --> 00:33:41.120]   this function is also a flat line.
[00:33:41.120 --> 00:33:41.480]   Yeah.
[00:33:41.480 --> 00:33:42.320]   That makes sense.
[00:33:42.320 --> 00:33:42.640]   Great.
[00:33:42.640 --> 00:33:43.920]   And it doesn't, it doesn't matter.
[00:33:43.960 --> 00:33:47.760]   Like one thing that's confusing or a little bit different here to what we've
[00:33:47.760 --> 00:33:52.400]   been looking at is this doesn't actually operate on the input at all.
[00:33:52.400 --> 00:33:56.480]   It's not like I, you know, have a different function that's two times X or
[00:33:56.480 --> 00:34:00.480]   just the, even the value here, it's just one being returned every time.
[00:34:00.480 --> 00:34:00.880]   Yeah.
[00:34:00.880 --> 00:34:01.120]   Yeah.
[00:34:01.120 --> 00:34:02.880]   It's a function that ignores its input.
[00:34:02.880 --> 00:34:05.240]   It still takes in the input, but then it ignores it.
[00:34:05.240 --> 00:34:05.600]   Yeah.
[00:34:05.600 --> 00:34:09.880]   The, the gradient of a function that doesn't do anything to the input.
[00:34:09.880 --> 00:34:12.200]   There's no slope or there's.
[00:34:12.200 --> 00:34:13.200]   There's no slope.
[00:34:13.360 --> 00:34:14.920]   Just a, a flat line.
[00:34:14.920 --> 00:34:15.400]   Okay.
[00:34:15.400 --> 00:34:19.160]   Now that we've, we've talked a little bit about AutoGrad, let's make use of it and
[00:34:19.160 --> 00:34:22.520]   use it to write this linear approximation function.
[00:34:22.520 --> 00:34:26.360]   This returns the linear approximation of the value F at X plus
[00:34:26.360 --> 00:34:28.320]   Epsilon from the point X.
[00:34:28.320 --> 00:34:32.600]   So looking at this guy here, you'll see there's a type signature, which is
[00:34:32.600 --> 00:34:37.080]   helpful, right, telling you it's taking in a function, a callable, and then two
[00:34:37.080 --> 00:34:40.320]   floating point numbers and giving you back a floating point number.
[00:34:40.320 --> 00:34:41.120]   Okay.
[00:34:41.600 --> 00:34:47.120]   The way this function works is it promises that if you give it a function, a value
[00:34:47.120 --> 00:34:51.720]   to start at, and then a place to approximate function at Epsilon, then it
[00:34:51.720 --> 00:34:55.920]   will give you an approximate value of F of X plus Epsilon.
[00:34:55.920 --> 00:34:56.360]   Okay.
[00:34:56.360 --> 00:34:58.880]   But you could just call F of X plus Epsilon.
[00:34:58.880 --> 00:35:02.560]   That would give you the exactly correct value, but we don't want to return that.
[00:35:02.560 --> 00:35:04.560]   We want to return an approximation to that.
[00:35:04.560 --> 00:35:04.880]   Okay.
[00:35:04.880 --> 00:35:09.400]   So I could do this, but I'm not, this is like actually what would be the correct
[00:35:09.440 --> 00:35:12.400]   value of what we're trying to approximate.
[00:35:12.400 --> 00:35:13.160]   Yes.
[00:35:13.160 --> 00:35:14.760]   This is the thing we're trying to approximate.
[00:35:14.760 --> 00:35:17.560]   I'm just going to like leave that there as a little reminder.
[00:35:17.560 --> 00:35:19.360]   This is the thing we're trying to approximate.
[00:35:19.360 --> 00:35:24.000]   So I know from above that we've learned a formula for this.
[00:35:24.000 --> 00:35:27.480]   That is, if I remember it's, maybe I won't even try to remember it.
[00:35:27.480 --> 00:35:32.840]   So it's the gradient plus my function evaluated at this position.
[00:35:32.840 --> 00:35:38.760]   And then my gradient is times this given Epsilon value, which is the thing I, my step
[00:35:38.760 --> 00:35:39.960]   size, yeah, exactly.
[00:35:39.960 --> 00:35:43.760]   So that is F evaluated at X.
[00:35:43.760 --> 00:35:44.040]   Yep.
[00:35:44.040 --> 00:35:46.920]   And then I just have to do my gradient.
[00:35:46.920 --> 00:35:48.880]   So I'll just leave that there as a little thing.
[00:35:48.880 --> 00:35:52.560]   Times my Epsilon.
[00:35:52.560 --> 00:35:55.680]   And now I need to figure out like, what's this?
[00:35:55.680 --> 00:35:59.680]   Thankfully you have just shown me how I can calculate that.
[00:35:59.680 --> 00:36:03.880]   This is, I probably am, I'm shooting myself in the foot calling a grad because
[00:36:03.880 --> 00:36:08.240]   you've just told me all of the different things that this can be, but this is
[00:36:08.680 --> 00:36:13.320]   my autograd function evaluated at my given X.
[00:36:13.320 --> 00:36:13.760]   Exactly.
[00:36:13.760 --> 00:36:20.520]   It's autograd.grad of my function, but then I need to call this.
[00:36:20.520 --> 00:36:21.640]   This is a bit weird.
[00:36:21.640 --> 00:36:23.320]   So maybe I'll give this a name.
[00:36:23.320 --> 00:36:24.800]   I don't really like doing that.
[00:36:24.800 --> 00:36:26.080]   Chaining functions.
[00:36:26.080 --> 00:36:30.000]   So people are less comfortable with higher order functions in Python, because it's
[00:36:30.000 --> 00:36:33.440]   not really a functional programming language, but yeah, in some languages what
[00:36:33.440 --> 00:36:36.400]   you just did would be absolutely the right way to do it, but I think you're
[00:36:36.400 --> 00:36:37.640]   right that it's not very Pythonic.
[00:36:38.440 --> 00:36:38.800]   Yeah.
[00:36:38.800 --> 00:36:40.760]   I just think it looks weird when you're going.
[00:36:40.760 --> 00:36:44.920]   I don't know when it's dot, dot something, dot something.
[00:36:44.920 --> 00:36:49.040]   That doesn't look too weird to me, but this, I prefer giving it a name.
[00:36:49.040 --> 00:36:49.240]   Yeah.
[00:36:49.240 --> 00:36:52.400]   And I would call it grad F just because what have you done?
[00:36:52.400 --> 00:36:55.080]   You've applied the gradient operator to F.
[00:36:55.080 --> 00:36:57.520]   You have the gradient function of F.
[00:36:57.520 --> 00:36:58.120]   That's right.
[00:36:58.120 --> 00:37:00.880]   And then grad F of my input.
[00:37:00.880 --> 00:37:01.280]   Yeah.
[00:37:01.280 --> 00:37:04.400]   And now I'm taking that and I'm multiplying it by Epsilon.
[00:37:04.400 --> 00:37:04.920]   Yep.
[00:37:04.920 --> 00:37:06.280]   I think that should do it.
[00:37:06.280 --> 00:37:07.040]   Let's see.
[00:37:07.040 --> 00:37:07.720]   Nailed it.
[00:37:07.880 --> 00:37:08.960]   Good solution there.
[00:37:08.960 --> 00:37:11.080]   A couple of things that I just want to quickly note.
[00:37:11.080 --> 00:37:15.280]   So notice the, you multiply those two things together and that's, that's what
[00:37:15.280 --> 00:37:19.480]   works when your function takes in a scalar value, returns a scalar value.
[00:37:19.480 --> 00:37:21.840]   Usually in machine learning, we think about things that take
[00:37:21.840 --> 00:37:23.960]   in vectors and return scalars.
[00:37:23.960 --> 00:37:26.160]   So then this would be a dot product instead.
[00:37:26.160 --> 00:37:26.720]   Is that that?
[00:37:26.720 --> 00:37:30.520]   Unfortunately you have to use np.dot explicitly here because
[00:37:30.520 --> 00:37:31.720]   they aren't actually vectors.
[00:37:31.720 --> 00:37:32.560]   Okay.
[00:37:32.560 --> 00:37:33.040]   Yeah.
[00:37:33.040 --> 00:37:38.720]   So that at operator is only defined for like numpy arrays is not defined for floats.
[00:37:38.720 --> 00:37:40.520]   And strats.
[00:37:40.520 --> 00:37:42.040]   Epsilons.
[00:37:42.040 --> 00:37:42.360]   Yeah.
[00:37:42.360 --> 00:37:43.040]   Okay.
[00:37:43.040 --> 00:37:44.560]   So you get the same answer.
[00:37:44.560 --> 00:37:47.400]   This one would work out of the box.
[00:37:47.400 --> 00:37:50.520]   If you were to switch around F for something that takes a float, sorry, that
[00:37:50.520 --> 00:37:54.600]   takes in a numpy array instead of just a floating point number, you didn't need
[00:37:54.600 --> 00:37:57.360]   that in order to solve the problem, but this is maybe what it would look more
[00:37:57.360 --> 00:37:59.880]   like in an, in a real machine learning case.
[00:38:00.040 --> 00:38:04.040]   What's, what's nice about this, or at least what's from what I've taken away
[00:38:04.040 --> 00:38:08.600]   from the video is that it's, it's interesting and cool that this shares
[00:38:08.600 --> 00:38:13.360]   the same notation and, and, and it works the same way for both scalar inputs.
[00:38:13.360 --> 00:38:15.080]   We're thinking about it like that.
[00:38:15.080 --> 00:38:18.920]   And for vectors, your, your gradient functions doesn't end up
[00:38:18.920 --> 00:38:20.960]   being some completely new thing.
[00:38:20.960 --> 00:38:21.880]   Yeah, yeah, exactly.
[00:38:21.880 --> 00:38:24.960]   So that, yeah, that like Frechet style really emphasizes the importance
[00:38:24.960 --> 00:38:27.200]   of that dot product there, which then.
[00:38:27.480 --> 00:38:30.960]   The same basic idea, it becomes slightly different in the
[00:38:30.960 --> 00:38:32.840]   case of matrix valued inputs.
[00:38:32.840 --> 00:38:35.960]   It's not exactly dot, it's like another level of additional
[00:38:35.960 --> 00:38:37.320]   generalization, but it's there.
[00:38:37.320 --> 00:38:40.320]   And you can even go another level higher and talk about derivatives
[00:38:40.320 --> 00:38:42.280]   of things that take in functions.
[00:38:42.280 --> 00:38:44.400]   So derivatives of higher order functions.
[00:38:44.400 --> 00:38:47.360]   And then it's still an inner product, but now it's this
[00:38:47.360 --> 00:38:49.280]   different, like higher level thing.
[00:38:49.280 --> 00:38:53.000]   But if you think of it as a dot product from the beginning, then it's a little
[00:38:53.000 --> 00:38:56.360]   bit less surprising every time you go up a level and discover it's, it's a dot
[00:38:56.360 --> 00:38:59.040]   product by a different name, wearing a trench coat or whatever.
[00:38:59.040 --> 00:39:03.880]   The last thing that I wanted to do is fully connect this back to machine
[00:39:03.880 --> 00:39:06.920]   learning, basically connect this to optimization by gradient descent.
[00:39:06.920 --> 00:39:11.480]   Gradient descent uses gradients in order to optimize functions.
[00:39:11.480 --> 00:39:13.360]   And that's, you know, that's why we're even talking about
[00:39:13.360 --> 00:39:14.840]   calculus in this series, right?
[00:39:14.840 --> 00:39:15.760]   There's lots of cool math.
[00:39:15.760 --> 00:39:19.440]   We aren't talking about integral calculus very much at all in this series.
[00:39:19.440 --> 00:39:23.440]   Because integrals don't show up as much in machine learning, especially
[00:39:23.440 --> 00:39:26.040]   the types of machine learning that scale really well and that work
[00:39:26.080 --> 00:39:27.040]   the best these days.
[00:39:27.040 --> 00:39:30.920]   So we're talking about gradients in order to understand what happens when
[00:39:30.920 --> 00:39:34.560]   we do optimization, these gradient based optimization techniques.
[00:39:34.560 --> 00:39:35.840]   Let's close this out.
[00:39:35.840 --> 00:39:40.600]   The gradient gives us a good linear guess for which direction to move
[00:39:40.600 --> 00:39:41.800]   to make the loss go down.
[00:39:41.800 --> 00:39:45.520]   Cause it gives us, it gives us a good guess for how the loss is going to
[00:39:45.520 --> 00:39:47.720]   behave near the values of our parameters.
[00:39:47.720 --> 00:39:51.160]   So we can use that to give us new values of the parameters that we
[00:39:51.160 --> 00:39:52.920]   think will make the loss go down.
[00:39:53.080 --> 00:39:57.320]   It turns out the direction to move, and this is talked about a little bit in the
[00:39:57.320 --> 00:40:00.640]   lecture and in the slides, is it turns out the direction to move is actually
[00:40:00.640 --> 00:40:03.760]   just the gradient times minus one, the negative gradient.
[00:40:03.760 --> 00:40:07.280]   Um, that'll be the direction that should make the loss go down the fastest,
[00:40:07.280 --> 00:40:08.640]   according to our approximation.
[00:40:08.640 --> 00:40:10.560]   And so that's where we go.
[00:40:10.560 --> 00:40:15.600]   But little o tells us that there is some amount of change that we should be able
[00:40:15.600 --> 00:40:20.600]   to guarantee that the decrease in the loss that we can predict because we have
[00:40:20.600 --> 00:40:25.000]   the gradient will be bigger than that little o term should be bigger than any
[00:40:25.000 --> 00:40:28.280]   other effect due to the fact that our function is not actually linear, but we
[00:40:28.280 --> 00:40:29.560]   don't know what that size is.
[00:40:29.560 --> 00:40:32.360]   We choose a size and that's the learning rate.
[00:40:32.360 --> 00:40:36.160]   Usually the Greek letter for that is that eta, that N looking Greek
[00:40:36.160 --> 00:40:37.480]   letter there next to the gradient.
[00:40:37.480 --> 00:40:39.480]   Though sometimes people use alpha as well.
[00:40:39.480 --> 00:40:44.280]   This sort of puts all of our ideas together and uses it for machine learning.
[00:40:44.280 --> 00:40:48.360]   And we're just going to implement last thing here, Scott, this gradient descent
[00:40:48.360 --> 00:40:53.120]   step that applies one step of gradient descent on F at X.
[00:40:53.120 --> 00:40:57.800]   You get an X and F and a learning rate, and I want the updated value for X.
[00:40:57.800 --> 00:40:58.280]   Okay.
[00:40:58.280 --> 00:40:59.360]   I'm given an X.
[00:40:59.360 --> 00:41:06.640]   So I want to do my X minus my learning rate, which is passed in as learning rate.
[00:41:06.640 --> 00:41:12.960]   And then I want to dot product that with my gradient of my
[00:41:12.960 --> 00:41:16.120]   function applied at this input X.
[00:41:16.160 --> 00:41:22.640]   We'll go with MP dot dot and my function, we haven't actually calculated my gradient,
[00:41:22.640 --> 00:41:28.600]   but we can do grad F equals auto grad dot grad of F.
[00:41:28.600 --> 00:41:30.600]   And there we have this grad F.
[00:41:30.600 --> 00:41:34.440]   We can evaluate that at my given XT.
[00:41:34.440 --> 00:41:37.480]   Oh, I realized I've called this X and it should be XT.
[00:41:37.480 --> 00:41:43.120]   And that will give us our new value of X with this learning rate.
[00:41:43.120 --> 00:41:43.760]   Yep.
[00:41:43.920 --> 00:41:48.880]   The one thing I would say though, is this may work, but you can do a dot
[00:41:48.880 --> 00:41:52.680]   product between two scalars or between two vectors that are the same length.
[00:41:52.680 --> 00:41:55.120]   But the learning rate is just a number, right?
[00:41:55.120 --> 00:41:59.040]   So that little circle dot in the middle there is meant to be like multiplication.
[00:41:59.040 --> 00:41:59.680]   Oh, okay.
[00:41:59.680 --> 00:42:00.880]   That makes sense.
[00:42:00.880 --> 00:42:01.440]   Yeah.
[00:42:01.440 --> 00:42:03.880]   I'm pretty sure that would still run actually, because NumPy loves
[00:42:03.880 --> 00:42:07.280]   broadcasting, but like semantically, I think that's not quite right.
[00:42:07.280 --> 00:42:07.560]   Yeah.
[00:42:07.560 --> 00:42:08.000]   Awesome.
[00:42:08.000 --> 00:42:08.320]   Okay.
[00:42:08.320 --> 00:42:09.040]   Yeah, that's it.
[00:42:09.040 --> 00:42:11.160]   The test, there's some nice little tests in there.
[00:42:11.160 --> 00:42:13.120]   There's some cute tests that check.
[00:42:13.120 --> 00:42:17.360]   Actually, just go ahead and put like XT plus learning rate in your
[00:42:17.360 --> 00:42:20.680]   implementation of the function there, just so we can see what some failed tests look like.
[00:42:20.680 --> 00:42:23.600]   So the tests here just check that you're, that you give the right
[00:42:23.600 --> 00:42:26.760]   answers on some specific problems and that it can be used to optimize,
[00:42:26.760 --> 00:42:28.080]   that can be used to optimize a function.
[00:42:28.080 --> 00:42:30.160]   So it actually checks that your gradient descent is right.
[00:42:30.160 --> 00:42:32.320]   So that's all the exercises for the calculus section.
[00:42:32.320 --> 00:42:35.120]   Next time we'll talk through the exercises on the probability,
[00:42:35.120 --> 00:42:36.080]   for the probability section.
[00:42:36.080 --> 00:42:36.440]   Cool.
[00:42:36.440 --> 00:42:37.320]   Thanks very much.
[00:42:37.320 --> 00:42:38.280]   All right.
[00:42:38.280 --> 00:42:38.760]   See you then.
[00:42:38.760 --> 00:42:41.340]   (upbeat music)
[00:42:41.340 --> 00:42:43.920]   (upbeat music)
[00:42:43.920 --> 00:42:46.500]   (upbeat music)
[00:42:46.500 --> 00:42:49.080]   (upbeat music)
[00:42:49.080 --> 00:42:51.660]   (upbeat music)
[00:42:51.660 --> 00:42:54.240]   (upbeat music)
[00:42:54.240 --> 00:42:56.820]   (upbeat music)
[00:42:56.820 --> 00:42:58.260]   (upbeat music)

