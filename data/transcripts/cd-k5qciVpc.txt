
[00:00:00.000 --> 00:00:03.000]   We need to hear an echo just to make sure we're live.
[00:00:03.000 --> 00:00:09.000]   Always takes one second and I want to always make sure that things are going as expected.
[00:00:09.000 --> 00:00:20.000]   Let me refresh YouTube.
[00:00:20.000 --> 00:00:29.000]   Awesome. It seems my audio isn't routed to my headphones.
[00:00:29.000 --> 00:00:30.000]   Let me quickly do that.
[00:00:30.000 --> 00:00:32.000]   Refresh YouTube just to make sure.
[00:00:32.000 --> 00:00:34.000]   Awesome.
[00:00:34.000 --> 00:00:35.000]   This, I apologize.
[00:00:35.000 --> 00:00:41.000]   There's always this live debugging that I have to do every single time, even though I've been doing this for a while.
[00:00:41.000 --> 00:00:42.000]   I still run into issues.
[00:00:42.000 --> 00:00:44.000]   So sorry about that.
[00:00:44.000 --> 00:00:47.000]   Welcome back to everyone who's joining us live.
[00:00:47.000 --> 00:00:49.000]   Hello to everyone who's watching the recap.
[00:00:49.000 --> 00:00:54.000]   It's always exciting to see and learn with everyone.
[00:00:54.000 --> 00:00:57.000]   This is the second lecture in the JAG series.
[00:00:57.000 --> 00:01:01.000]   So if you're watching a recap, you should go check out the previous lecture.
[00:01:01.000 --> 00:01:03.000]   It should be there in the playlist.
[00:01:03.000 --> 00:01:05.000]   And we do plan to cover quite a few things today.
[00:01:05.000 --> 00:01:09.000]   So I want to start with a disclaimer.
[00:01:09.000 --> 00:01:11.000]   I'm a fan of Jeremy Howard.
[00:01:11.000 --> 00:01:13.000]   He's the creator of FastA.
[00:01:13.000 --> 00:01:17.000]   And if you can't tell, I'm remotely not the same person.
[00:01:17.000 --> 00:01:19.000]   I'm far from him.
[00:01:19.000 --> 00:01:28.000]   But he taught me this technique of spaced repetition, wherein you learn a concept and after a while you try to repeat it.
[00:01:28.000 --> 00:01:30.000]   I really try to follow that.
[00:01:30.000 --> 00:01:38.000]   And by that, I mean, I try to, when I'm especially trying to do a study group or do a lecture, I try to introduce a concept and come back to it later.
[00:01:38.000 --> 00:01:41.000]   And this is one thing you'll see across the JAG series as well.
[00:01:41.000 --> 00:01:55.000]   So I'll try to introduce FLAGS today, particularly, not in a lot of depth, but I'll just give you a taste of the API, a high-level introduction, and then we'll come back and learn more about it.
[00:01:55.000 --> 00:02:00.000]   So the disclaimer is, if things don't make sense, that's how it's designed to be.
[00:02:00.000 --> 00:02:03.000]   Also, because I'm new to JAGS myself, I'm learning myself.
[00:02:03.000 --> 00:02:05.000]   But please feel free to ask any questions.
[00:02:05.000 --> 00:02:10.000]   And we'll keep revisiting these topics as you keep joining again.
[00:02:10.000 --> 00:02:13.000]   Christian is here as well.
[00:02:13.000 --> 00:02:14.000]   Hey, Christian.
[00:02:14.000 --> 00:02:19.000]   I will make him co-host just so that he can speak.
[00:02:19.000 --> 00:02:20.000]   Hey Sanjay.
[00:02:20.000 --> 00:02:21.000]   Thanks.
[00:02:21.000 --> 00:02:23.000]   Thanks for joining us again.
[00:02:23.000 --> 00:02:25.000]   A pleasure.
[00:02:25.000 --> 00:02:26.000]   Awesome.
[00:02:26.000 --> 00:02:33.000]   I was just getting started and just giving a disclaimer that I'll keep introducing topics at different intervals and revisiting them.
[00:02:33.000 --> 00:02:36.000]   Sounds good.
[00:02:36.000 --> 00:02:43.000]   I'll join you for a while and then I'll have to leave, but I hope to remain a little bit.
[00:02:43.000 --> 00:02:45.000]   No, thank you.
[00:02:45.000 --> 00:02:50.000]   Last time it was like having a senior engineer correct me anytime I was going wrong.
[00:02:50.000 --> 00:02:52.000]   So I look forward to that again.
[00:02:52.000 --> 00:02:55.000]   And please interrupt me anytime you see me saying anything incorrectly.
[00:02:55.000 --> 00:02:58.000]   Awesome.
[00:02:58.000 --> 00:03:01.000]   I'll get started and introduce the session.
[00:03:01.000 --> 00:03:05.000]   Let me get the Zoom chat out of the way, minimize everything.
[00:03:05.000 --> 00:03:07.000]   So this is the second lecture in the series.
[00:03:07.000 --> 00:03:10.000]   And today we'll be learning quite a few topics.
[00:03:10.000 --> 00:03:18.000]   If you're new to the group, I want to remind you that this is being co-organized with the JAX Global Meetup group that is being led by Christian.
[00:03:18.000 --> 00:03:20.000]   So please do join that.
[00:03:20.000 --> 00:03:22.000]   Christian wants to keep running the JAX series.
[00:03:22.000 --> 00:03:27.000]   And I'm not sure if I'm at the expertise where I'll be able to continue the sessions.
[00:03:27.000 --> 00:03:33.000]   If not, if you're interested in JAX, make sure to join the group so that you can keep learning more.
[00:03:33.000 --> 00:03:35.000]   Here's the agenda for today.
[00:03:35.000 --> 00:03:39.000]   I'll first of all announce 27 days of JAX winners.
[00:03:39.000 --> 00:03:43.000]   And followed by that, we'll try to recap a few concepts from last week.
[00:03:43.000 --> 00:03:46.000]   We'll revisit Autodiff, revisit PyTrees.
[00:03:46.000 --> 00:03:53.000]   And then we'll learn FLAGS, a vanilla introduction to the API, and try to implement a few neural networks in JAX.
[00:03:53.000 --> 00:03:55.000]   Or understand the neural networks.
[00:03:55.000 --> 00:03:59.000]   I'm too terrified to like code, so I won't do that.
[00:03:59.000 --> 00:04:01.000]   Here are the resources being discussed today.
[00:04:01.000 --> 00:04:04.000]   Akash Nann, he's a Kaggle kernel master.
[00:04:04.000 --> 00:04:07.000]   I would highly recommend that you check out his kernels.
[00:04:07.000 --> 00:04:09.000]   I'll be referring to them today.
[00:04:09.000 --> 00:04:12.000]   We'll be using the FLAGS documentation collabs.
[00:04:12.000 --> 00:04:15.000]   We'll be looking at the Scenic API examples.
[00:04:15.000 --> 00:04:18.000]   And we'll be taking a look at the PyTorch unit implementation.
[00:04:18.000 --> 00:04:22.000]   So introduction to the co-hosts that you just heard.
[00:04:22.000 --> 00:04:26.000]   Christian is the real lead of this group because he's been guiding me.
[00:04:26.000 --> 00:04:30.000]   And he's the JAX expert among the two of us in this call.
[00:04:30.000 --> 00:04:40.000]   He's a machine learning engineer at Quonsight and also is developing incredible libraries that we'll be learning soon in our series called TreeX and Elegy.
[00:04:40.000 --> 00:04:41.000]   You can still check them out if you want.
[00:04:41.000 --> 00:04:44.000]   And you can follow him on Twitter @cgarcia88.
[00:04:44.000 --> 00:04:54.000]   So let me remind everyone of this link that we'll be using to keep the chat going.
[00:04:54.000 --> 00:04:58.000]   And let me post this in the YouTube chat as well.
[00:04:58.000 --> 00:05:08.000]   And pin it to the YouTube chat.
[00:05:08.000 --> 00:05:12.000]   So if you head over to this link, this should take you to the Dyspose forums.
[00:05:12.000 --> 00:05:19.000]   And the reason these exist is some people aren't always able to join live or you might have questions that you want to discuss later on.
[00:05:19.000 --> 00:05:31.000]   For that, please come back to our forums and feel free to ask any questions in this thread.
[00:05:31.000 --> 00:05:38.000]   Like so you can leave any questions and I'll be answering them from time to time and taking a look at the questions.
[00:05:38.000 --> 00:05:48.000]   So please, as a request, instead of the Zoom chat and YouTube chat, please post the questions here so that it's easier for me to keep an eye out.
[00:05:48.000 --> 00:05:52.000]   Referring back to Akash Nann, this is his Kaggle profile.
[00:05:52.000 --> 00:05:59.000]   And as I mentioned, he's among the top 150 in the world of Kaggle notebooks rankings.
[00:05:59.000 --> 00:06:02.000]   We'll be taking a look at his notebooks today.
[00:06:02.000 --> 00:06:05.000]   Recapping 27 days of JAX.
[00:06:05.000 --> 00:06:09.000]   So I wanted to make my dent in the universe, which is why I picked the number 27.
[00:06:09.000 --> 00:06:14.000]   And I invited everyone to spend some time learning JAX every day.
[00:06:14.000 --> 00:06:21.000]   So as an incentive, please consider sharing whatever you learned about if you write any code or any blog post with us.
[00:06:21.000 --> 00:06:33.000]   And I promised I will convince the team at Bateson Biases to send some swag over to the top or to at least the most enjoyable reads, which will be opinionated, of course.
[00:06:33.000 --> 00:06:36.000]   Here are the three winners from last week.
[00:06:36.000 --> 00:06:39.000]   Romilly, who's there in the chat as well today.
[00:06:39.000 --> 00:06:46.000]   Twitter handle @rearblog wrote about a comparison of running spiking neural networks on the Jetson Nano.
[00:06:46.000 --> 00:06:52.000]   And on his workstation, he says he plans on running this on the Jetson as well.
[00:06:52.000 --> 00:07:00.000]   So that was incredible to see that JAX also works on the Jetson Nano without much issues, at least compared to other frameworks.
[00:07:00.000 --> 00:07:09.000]   Pranjil Gulati wrote about an introduction to JAX and this was actually his first blog post.
[00:07:09.000 --> 00:07:16.000]   So that was really great to see that his first blog post was part of 27 days of JAX and he wrote about JAX.
[00:07:16.000 --> 00:07:19.000]   So thanks for that nice blog post, Pranjil.
[00:07:19.000 --> 00:07:26.000]   And Dion wrote this incredible, incredible blog a few days ago.
[00:07:26.000 --> 00:07:30.000]   And it's about, let me move the zoom things out of the way.
[00:07:30.000 --> 00:07:34.000]   It's about supercharged high resolution ocean simulation with JAX.
[00:07:34.000 --> 00:07:42.000]   This has a lot of math in it and many interesting things that I'll, for the sake of sounding smart, say that I did understand.
[00:07:42.000 --> 00:07:50.000]   But after spending some time with this blog, this was a thoroughly written and very detailed blog post on how you can do all of these simulations.
[00:07:50.000 --> 00:07:52.000]   I would encourage you all to check this out.
[00:07:52.000 --> 00:07:57.000]   So thanks everyone who took part in our initiative.
[00:07:57.000 --> 00:08:10.000]   Someone from our team will reach out to you and send some swag over and please keep contributing if you want.
[00:08:10.000 --> 00:08:14.000]   So from here, I would want to recap what we studied last week.
[00:08:14.000 --> 00:08:17.000]   I mentioned I like the concept of spaced repetition.
[00:08:17.000 --> 00:08:22.000]   So every week I'll introduce a concept, come back and spend some time explaining it.
[00:08:22.000 --> 00:08:28.000]   Hopefully we'll always have Christian most of the times to correct me in any places that I go wrong.
[00:08:28.000 --> 00:08:30.000]   So to recap, what is JAX?
[00:08:30.000 --> 00:08:36.000]   The simplest way, which doesn't do it justice is it's called NumPy for accelerator.
[00:08:36.000 --> 00:08:38.000]   That's a mental model you can construct.
[00:08:38.000 --> 00:08:45.000]   It's quite compiler oriented and it is designed around XLA, which was designed by the team at Google.
[00:08:45.000 --> 00:08:50.000]   And it makes it really easy to run your code on any accelerator.
[00:08:50.000 --> 00:08:55.000]   What are accelerators? Your CPU, your GPU or a TPU board.
[00:08:55.000 --> 00:08:57.000]   All are accelerators.
[00:08:57.000 --> 00:09:10.000]   And if you've worked across any other framework, you might know that there are these fine, fine details that always make it slightly more challenging to run your code on multiple TPU cores, multiple GPU cores.
[00:09:10.000 --> 00:09:12.000]   You always have to make that changes.
[00:09:12.000 --> 00:09:16.000]   With JAX, it's in my opinion, relatively easier.
[00:09:16.000 --> 00:09:22.000]   It brings all of the goodness of Autograd that I'm sure all of you would be familiar from with PyTorch.
[00:09:22.000 --> 00:09:29.000]   So anytime you can backprop very easily and call the gradients, you can expect a similar behavior in JAX.
[00:09:29.000 --> 00:09:34.000]   And as I mentioned, it's very easy to parallelize everything.
[00:09:34.000 --> 00:09:39.000]   So further recapping the difference between JAX.numpy and numpy.
[00:09:39.000 --> 00:09:41.000]   And some people call this numpy.
[00:09:41.000 --> 00:09:43.000]   I still don't agree with them.
[00:09:43.000 --> 00:09:44.000]   I think it's called numpy.
[00:09:44.000 --> 00:09:46.000]   It's not Python, right?
[00:09:46.000 --> 00:09:51.000]   So on the left side, you can see the key points from JAX.
[00:09:51.000 --> 00:09:54.000]   On the right side are the definitions from numpy.
[00:09:54.000 --> 00:09:59.000]   On JAX, usually the arrays are called device array because they live on your device.
[00:09:59.000 --> 00:10:04.000]   They live on the CPU memory, GPU memory, or on the TPU board.
[00:10:04.000 --> 00:10:10.000]   In numpy, they are ndarray because remember in numpy, you cannot put an array on your GPU memory.
[00:10:10.000 --> 00:10:12.000]   It just lives on the CPU memory.
[00:10:12.000 --> 00:10:16.000]   That's why PyTorch was so exciting because it was numpy for GPUs.
[00:10:16.000 --> 00:10:21.000]   To some extent, JAX is numpy for accelerators.
[00:10:21.000 --> 00:10:24.000]   In JAX, these are immutable.
[00:10:24.000 --> 00:10:28.000]   So you can't change a list, for example, or you can't change the array.
[00:10:28.000 --> 00:10:30.000]   In numpy, they are mutable.
[00:10:30.000 --> 00:10:34.000]   A random function does exist, but its state remains the same.
[00:10:34.000 --> 00:10:38.000]   So every time you call the random function, you need to assign a key.
[00:10:38.000 --> 00:10:41.000]   Otherwise, you'll be getting the same random number every single time.
[00:10:41.000 --> 00:10:48.000]   So it doesn't work exactly how a random number generator would compared to numpy.
[00:10:48.000 --> 00:10:50.000]   And this was actually a decision made by the team.
[00:10:50.000 --> 00:10:53.000]   I was trying to understand why this happened.
[00:10:53.000 --> 00:10:55.000]   And, Krishan, please correct me if I'm wrong.
[00:10:55.000 --> 00:11:01.000]   The team believes that for reproducible research, you should have the same random number coming up every time.
[00:11:01.000 --> 00:11:13.000]   Yeah, I think one of the things that solves really neatly is how to make a distributed computation reproducible.
[00:11:13.000 --> 00:11:21.000]   And if each machine has its own random state, then you're going to get different results.
[00:11:21.000 --> 00:11:34.000]   But if you are very specific, which is what they did, then you can with more confidence have a reproducible distributed environment.
[00:11:34.000 --> 00:11:39.000]   I think that is the main reason.
[00:11:39.000 --> 00:11:42.000]   Thanks. Thanks for that clarification.
[00:11:42.000 --> 00:11:46.000]   So as I mentioned, JAX arrays are immutable.
[00:11:46.000 --> 00:11:53.000]   So whenever you try to update them and out of place updation happens, which means a new copy gets created and returned.
[00:11:53.000 --> 00:11:58.000]   Why would you worry about this if you're working with a large batch of images and you make a change to them?
[00:11:58.000 --> 00:12:06.000]   Let's say you should be concerned about the memory and getting the wonderful code out of memory error, which I'm sure no one of us enjoys.
[00:12:06.000 --> 00:12:09.000]   In numpy, however, the updates happen in place.
[00:12:09.000 --> 00:12:20.000]   So whenever you let's say, let's say you're trying to make a data augmentation for images happen and you apply a bunch of matrix transformation, those happen in place.
[00:12:20.000 --> 00:12:25.000]   You don't create a copy, you don't eat more memory and you don't have to worry about handling that.
[00:12:25.000 --> 00:12:29.000]   So just as a caution, with JAX it's out of place.
[00:12:29.000 --> 00:12:31.000]   And it's single precision by default.
[00:12:31.000 --> 00:12:38.000]   Last week we had an incredible decision and as I learned it's faster computation with smaller precision.
[00:12:38.000 --> 00:12:42.000]   That's one thing that JAX doesn't so effectively support at the moment.
[00:12:42.000 --> 00:12:45.000]   Numpy by default always converts to double.
[00:12:45.000 --> 00:12:47.000]   That's a data type.
[00:12:47.000 --> 00:12:53.000]   For JAX you have to actually enable this by having a flag in the global settings.
[00:12:53.000 --> 00:12:57.000]   Christian, would you like to add anything here?
[00:12:57.000 --> 00:13:03.000]   I think a lot of the decisions are geared because of how JAX works on TPUs.
[00:13:03.000 --> 00:13:11.000]   So I think like TPUs work on 32 bits by default.
[00:13:11.000 --> 00:13:18.000]   So just bear in mind that the best JAX backend is TPU.
[00:13:18.000 --> 00:13:27.000]   Might explain a lot of what you see around the quirks or sometimes because of that.
[00:13:27.000 --> 00:13:33.000]   Do you know if 16-bit is faster on TPUs? I assume not.
[00:13:33.000 --> 00:13:43.000]   So yes, for 16 bits they have a format that TensorFlow developed called Bfloat16.
[00:13:43.000 --> 00:13:54.000]   So it has obviously less precision but it has the ranges a little bit higher than regular 16 float.
[00:13:54.000 --> 00:14:00.000]   And I believe the TPUs support that really well.
[00:14:00.000 --> 00:14:09.000]   And if you try to large scale stuff, the TensorFlow and Google Cloud teams in general,
[00:14:09.000 --> 00:14:13.000]   they tell you to try to do this.
[00:14:13.000 --> 00:14:21.000]   So JAX, I think, because this part of XLA actually supports this natively.
[00:14:21.000 --> 00:14:23.000]   Okay. Thanks.
[00:14:23.000 --> 00:14:35.000]   Although I think, I'm not sure you have to actually use the type Bfloat16 or if it's a config thing.
[00:14:35.000 --> 00:14:40.000]   I haven't done it a long time ago, so I don't remember.
[00:14:40.000 --> 00:14:44.000]   Let's see if I can find that answer.
[00:14:44.000 --> 00:14:55.000]   Float16.
[00:14:55.000 --> 00:15:02.000]   Yeah, maybe search for Bfloat16.
[00:15:02.000 --> 00:15:07.000]   Search on my own also.
[00:15:07.000 --> 00:15:10.000]   This seems to work.
[00:15:10.000 --> 00:15:12.000]   Bfloat16.
[00:15:12.000 --> 00:15:17.000]   I think it needs to be declared in the global flags.
[00:15:17.000 --> 00:15:22.000]   That's support.
[00:15:22.000 --> 00:15:31.000]   It says Bfloat16.
[00:15:31.000 --> 00:15:34.000]   I think it is there.
[00:15:34.000 --> 00:15:35.000]   We would have to try it.
[00:15:35.000 --> 00:15:37.000]   You just type Bfloat16.
[00:15:37.000 --> 00:15:38.000]   I found it.
[00:15:38.000 --> 00:15:40.000]   I'm not sure if you're looking at my screen.
[00:15:40.000 --> 00:15:41.000]   Okay.
[00:15:41.000 --> 00:15:42.000]   I was trying to help.
[00:15:42.000 --> 00:15:44.000]   No worries.
[00:15:44.000 --> 00:15:46.000]   Yeah.
[00:15:46.000 --> 00:15:51.000]   This won't work on your local machine, I think.
[00:15:51.000 --> 00:15:56.000]   So you have to make sure you're running on TPU.
[00:15:56.000 --> 00:16:01.000]   Else, I don't know if it defaults back to just float32.
[00:16:01.000 --> 00:16:05.000]   But it would be fun to try it out if you have a notebook.
[00:16:05.000 --> 00:16:09.000]   Are you saying all of this money I spent on GPUs is wasted?
[00:16:09.000 --> 00:16:12.000]   Well, yeah, I don't know.
[00:16:12.000 --> 00:16:15.000]   I don't think you can do that on GPU.
[00:16:15.000 --> 00:16:16.000]   I'm not sure.
[00:16:16.000 --> 00:16:18.000]   I'll try it out after.
[00:16:18.000 --> 00:16:20.000]   That would be fun.
[00:16:20.000 --> 00:16:26.000]   I've only been able to do that on TPUs.
[00:16:26.000 --> 00:16:29.000]   Okay.
[00:16:29.000 --> 00:16:32.000]   I see a lot of questions coming in the Zoom chat.
[00:16:32.000 --> 00:16:36.000]   Could you please post them here so that it's easier for me to ask them?
[00:16:36.000 --> 00:16:45.000]   Let me reshare the link to our forums in the Zoom chat.
[00:16:45.000 --> 00:16:49.000]   And to the people asking about the previous recording, there was an issue with the recording.
[00:16:49.000 --> 00:16:50.000]   So I'll re-upload it.
[00:16:50.000 --> 00:16:57.000]   If you actually go to the section, you should be able to find the Colab, which is pretty much what we discussed.
[00:16:57.000 --> 00:17:04.000]   And I'll have the recording uploaded by tomorrow or the day after.
[00:17:04.000 --> 00:17:06.000]   So just checking here in case.
[00:17:06.000 --> 00:17:08.000]   Yeah, I'll quickly recap Autodiff as well.
[00:17:08.000 --> 00:17:10.000]   It's similar to Autograd.
[00:17:10.000 --> 00:17:16.000]   I'll be looking at another notebook that will explain it in more detail.
[00:17:16.000 --> 00:17:19.000]   And it does not work with vectors.
[00:17:19.000 --> 00:17:24.000]   And vectors are the NumPy vectors and not the physics vectors that are being referred to.
[00:17:24.000 --> 00:17:28.000]   For parallelization, you have the following in JAX.
[00:17:28.000 --> 00:17:30.000]   You can call it Vmap or a Pmap.
[00:17:30.000 --> 00:17:34.000]   And this is one thing that is a bit mind-boggling.
[00:17:34.000 --> 00:17:41.000]   At least for someone who's coming from PyTorch, you don't have to explicitly define the batch dimensions.
[00:17:41.000 --> 00:17:44.000]   And Vmap works on a single device.
[00:17:44.000 --> 00:17:51.000]   And Pmap, let's say if you want to parallelize across multiple TPUs or multiple GPUs, it works across those.
[00:17:51.000 --> 00:17:58.000]   So whenever you Pmap a function, it's to parallelize them across multiple devices.
[00:17:58.000 --> 00:18:06.000]   So I think from here, I want to jump over to the Autodiff notebook by Akash Nand.
[00:18:06.000 --> 00:18:08.000]   There is a question, though.
[00:18:08.000 --> 00:18:13.000]   I don't know if you want to answer it.
[00:18:13.000 --> 00:18:15.000]   I saw it in a Zoom chat.
[00:18:15.000 --> 00:18:17.000]   I saw it.
[00:18:17.000 --> 00:18:20.000]   What do you mean by out-of-place updation?
[00:18:20.000 --> 00:18:23.000]   So I'll try to take it and Christian, correct me.
[00:18:23.000 --> 00:18:30.000]   Whenever you change something in Python, let's say you define a variable and you update its value, that happens in place.
[00:18:30.000 --> 00:18:36.000]   Which means inside of your computer, it is stored in a particular memory location.
[00:18:36.000 --> 00:18:38.000]   And that is where it gets updated.
[00:18:38.000 --> 00:18:44.000]   Out-of-place is when you create a copy of it, change it, and that gets returned to your compiler.
[00:18:44.000 --> 00:18:49.000]   So that is what out-of-place updation means.
[00:18:49.000 --> 00:18:51.000]   Yeah.
[00:18:51.000 --> 00:19:02.000]   I think I just wanted to maybe, in Jack's, I don't know, if we're going to showcase JIT today, we might.
[00:19:02.000 --> 00:19:09.000]   One of the things you have to take into account is that the way things execute are not the way you write them.
[00:19:09.000 --> 00:19:11.000]   Because there is a compiler.
[00:19:11.000 --> 00:19:21.000]   So even if you do out-of-place operations, the compiler might be smart enough to say, "Ah, no, this is better in place."
[00:19:21.000 --> 00:19:24.000]   And that is actually what probably happens.
[00:19:24.000 --> 00:19:32.000]   In PyTorch, you can't do that because PyTorch, well, most of the time it is not JITed.
[00:19:32.000 --> 00:19:36.000]   So you do a lot of in-place when you can.
[00:19:36.000 --> 00:19:43.000]   Just kind of a thing to take into account if you're trying to optimize it is just don't.
[00:19:43.000 --> 00:19:52.000]   Or it can be optimized for you, basically.
[00:19:52.000 --> 00:19:55.000]   But I don't think we'll be looking at JIT today.
[00:19:55.000 --> 00:19:58.000]   I don't think we'll be JITing a lot of things.
[00:19:58.000 --> 00:20:00.000]   We might come back to it later.
[00:20:00.000 --> 00:20:06.000]   So as I mentioned, I'll be shamelessly using the kernels that Akash Nann has put out.
[00:20:06.000 --> 00:20:12.000]   And Akash is someone whom I really look up to as much as I look up to someone like Christian.
[00:20:12.000 --> 00:20:14.000]   His code is always incredible.
[00:20:14.000 --> 00:20:18.000]   So if you anytime see a kernel by him, first of all, make sure you upvote it.
[00:20:18.000 --> 00:20:22.000]   I actually removed my upvote because I wanted to prove this point.
[00:20:22.000 --> 00:20:26.000]   And his code is something that I always learn from.
[00:20:26.000 --> 00:20:33.000]   Like there are these fine, fine details that I always end up learning different things from him.
[00:20:33.000 --> 00:20:36.000]   I'm sorry, Christian, did you have something to say?
[00:20:36.000 --> 00:20:42.000]   No, I just wanted to give a shout out to Akash, who's actually in the chat.
[00:20:42.000 --> 00:20:45.000]   Thanks. Thanks for joining us, Akash.
[00:20:45.000 --> 00:20:51.000]   If you want to cover this, I can invite you as a co-host, but I don't think you would be interested.
[00:20:51.000 --> 00:20:54.000]   I'll try to cover them to the best of my ability.
[00:20:54.000 --> 00:20:56.000]   So I'll be using two of his notebooks.
[00:20:56.000 --> 00:21:00.000]   The first one is called Autodef and JAX, and the second one is called PyTrees.
[00:21:00.000 --> 00:21:03.000]   I'll post all of these links in the forums afterward.
[00:21:03.000 --> 00:21:09.000]   The reason why I don't do them live is because then people start looking at them and they don't listen so much.
[00:21:09.000 --> 00:21:11.000]   And I don't want that happening.
[00:21:11.000 --> 00:21:14.000]   We will share this link afterwards.
[00:21:14.000 --> 00:21:19.000]   So this notebook takes a look at the concept of automatic differentiation.
[00:21:19.000 --> 00:21:21.000]   And Akash is a TensorFlow expert.
[00:21:21.000 --> 00:21:24.000]   He has contributed to Kera's.
[00:21:24.000 --> 00:21:27.000]   He's been writing about research papers a lot.
[00:21:27.000 --> 00:21:32.000]   So this will have concepts from TensorFlow.
[00:21:32.000 --> 00:21:36.000]   But this becomes a homework for everyone in the meeting today and everyone joining.
[00:21:36.000 --> 00:21:41.000]   Try to convert this to PyTorch and see if you can see if there are things that don't work.
[00:21:42.000 --> 00:21:50.000]   So I think first of all, we'd like to take a look at gradients.
[00:21:50.000 --> 00:21:53.000]   Akash always focuses on teaching your concepts.
[00:21:53.000 --> 00:22:00.000]   So here we're learning about a simple function called product where you just multiply x and y and you try to return that.
[00:22:00.000 --> 00:22:03.000]   This would, of course, work as expected.
[00:22:03.000 --> 00:22:13.000]   And if you've done some high school math, you would remember that the dz by dx would be the value of y.
[00:22:13.000 --> 00:22:20.000]   So to combine that point, what we do here is we call grad on these functions.
[00:22:20.000 --> 00:22:25.000]   And the reason we can call that is because we've imported grad above from JAX.
[00:22:25.000 --> 00:22:28.000]   That's how we do automatic differentiation in JAX.
[00:22:28.000 --> 00:22:43.000]   So if we call the grad function on top of product and pass these two variables, we should be able to print the gradient of z with respect to x, which should be the value of y.
[00:22:43.000 --> 00:22:45.000]   Four.
[00:22:45.000 --> 00:22:52.000]   So when you divide or when you differentiate by x, it returns a value of five, which was four and same for y.
[00:22:52.000 --> 00:22:54.000]   It should return the value of x, which is three.
[00:22:54.000 --> 00:23:01.000]   And sorry, I'm taking too long on this, but for anyone who doesn't remember differentiation, I wanted to clarify this.
[00:23:01.000 --> 00:23:07.000]   So above, we've defined the function product, passed it to grad, and we've received the gradients.
[00:23:07.000 --> 00:23:08.000]   Why is this so important?
[00:23:08.000 --> 00:23:16.000]   Whenever you're trying to do back propagation or whenever you're working with neural networks, backprop is the core meat of what gets them trained.
[00:23:16.000 --> 00:23:20.000]   And it basically works across differentiation.
[00:23:20.000 --> 00:23:25.000]   For that, you want autodiff to work or automatic differentiation to work.
[00:23:25.000 --> 00:23:26.000]   Yeah.
[00:23:26.000 --> 00:23:46.000]   So, again, I don't know, maybe the slight detail, if somebody skipped too fast, the important thing is the argnum's parameter, which tells you what to differentiate with respect to, right?
[00:23:46.000 --> 00:23:50.000]   I don't know.
[00:23:50.000 --> 00:23:52.000]   You skipped a bit over that.
[00:23:52.000 --> 00:23:55.000]   I did, yes.
[00:23:55.000 --> 00:24:08.000]   So here we're differentiating with x, so that's why it's the zeroth arg and with y, it's the first argument since it's zero index.
[00:24:08.000 --> 00:24:16.000]   Next, we combine a function transform and what we're trying to do here is take a look under the hood what's happening.
[00:24:16.000 --> 00:24:22.000]   So we call make JacksPR and that actually prints out what's happening inside.
[00:24:22.000 --> 00:24:35.000]   I'll probably skip this over, but this is just to take a look under the hood of how this operates and how are the gradients being calculated.
[00:24:35.000 --> 00:24:46.000]   So to stop gradient computation, and this is when you don't want them to flow through specific computations, you can explicitly stop them.
[00:24:46.000 --> 00:24:58.000]   So think when in PyTorch you said grad to none or when you don't try to calculate the gradients, it's similar to that.
[00:24:58.000 --> 00:25:06.000]   You can call stop gradient and that should skip it.
[00:25:06.000 --> 00:25:10.000]   I'm just trying to read through and remember if there are things I want to cover.
[00:25:10.000 --> 00:25:22.000]   Also, everyone can check the kernel out, which is why I'm just skipping through a lot of details.
[00:25:22.000 --> 00:25:28.000]   I don't think I would want to cover anything from here. It was quite clear to me.
[00:25:28.000 --> 00:25:34.000]   Christian, do you think any other important details you want to highlight?
[00:25:34.000 --> 00:25:46.000]   Maybe one of the cool things we did last time and it's also here, you can compose Jacks functions.
[00:25:46.000 --> 00:25:51.000]   So all the transformations are composable.
[00:25:51.000 --> 00:26:01.000]   One of the cool things is that you can, as showcased here, you compose the grad with a vmap and then with jit.
[00:26:01.000 --> 00:26:27.000]   So if the gradient is only defined with respect to a single number, if you want to turn it into a function that is able to compute the gradient with respect to a vector,
[00:26:27.000 --> 00:26:37.000]   then in Jacks you just compose them. So vmap grad and then you suddenly get a function that is able to deal with that kind of data,
[00:26:37.000 --> 00:26:44.000]   which is one of the superpowers, at least in my mind, from the Jacks world.
[00:26:44.000 --> 00:27:00.000]   And just to clarify, when you say compose, it's like chaining them inside of each other's call so that they actually call each other on top.
[00:27:00.000 --> 00:27:07.000]   Any other thing you want to highlight from here?
[00:27:07.000 --> 00:27:14.000]   I don't know.
[00:27:14.000 --> 00:27:20.000]   I think this notebook is best read slowly.
[00:27:20.000 --> 00:27:28.000]   So I think the basics are covered.
[00:27:28.000 --> 00:27:32.000]   Unless anybody has questions, that would be interesting.
[00:27:32.000 --> 00:27:37.000]   Let's see if there are open questions. Can you cover vmap again?
[00:27:37.000 --> 00:27:47.000]   I'm trying to go back to that point.
[00:27:47.000 --> 00:27:56.000]   So I built this mental model where v is on one device and p stands for parallelization. So it's across parallel devices.
[00:27:56.000 --> 00:28:03.000]   So when you want to use something across multiple cores of a CPU, that's when you would call vmap.
[00:28:03.000 --> 00:28:10.000]   And you can simply pass different functions to vmap and they get optimized across that one device.
[00:28:10.000 --> 00:28:19.000]   So that's how I remember it in my head. I'm not sure if that answers the question.
[00:28:19.000 --> 00:28:27.000]   I think last time we did an example, which might have been good.
[00:28:27.000 --> 00:28:34.000]   Basically, you have a function that works for like a single sample, vmap turns it into a function that works for a batch.
[00:28:34.000 --> 00:28:40.000]   That's how I view it. And it's the simplest way to understand it. And you can do a lot of magic.
[00:28:40.000 --> 00:28:47.000]   So it actually does a lot more things if you play with the API, but that's kind of the basic one.
[00:28:47.000 --> 00:28:53.000]   There's also a question. Does JackScratch support multivariate differentiation?
[00:28:53.000 --> 00:29:05.000]   Like maybe differentiate to multiple inputs? The answer is yes.
[00:29:05.000 --> 00:29:15.000]   So I think one of the ways is telling ArxNum to just be a list or rather a tuple.
[00:29:15.000 --> 00:29:23.000]   Can you check the API, Samyam?
[00:29:23.000 --> 00:29:34.000]   Yeah, I would believe you can tell it, well, by default it differentiates with respect to the first argument.
[00:29:34.000 --> 00:29:46.000]   But then I imagine you can just pass like a tuple and tell it the arguments that you want to differentiate with respect to.
[00:29:46.000 --> 00:29:53.000]   Just search for grad, the JackScratch, the API for grad.
[00:29:53.000 --> 00:30:08.000]   The other thing you can do, because you usually, well, that depends on how you're using it, but you usually put your differentiable parameters like single structure.
[00:30:08.000 --> 00:30:13.000]   Oh yeah, just click on grad there and click on the grad function.
[00:30:13.000 --> 00:30:20.000]   And then it would be the ArxNums. Yeah, it can be like a sequence.
[00:30:20.000 --> 00:30:24.000]   So a sequence of ints. Yeah, you can just tell what you want.
[00:30:24.000 --> 00:30:29.000]   For neural networks, you usually put all your parameters into a single structure.
[00:30:29.000 --> 00:30:32.000]   Could be, well, what we're going to see later as a pi tree.
[00:30:32.000 --> 00:30:40.000]   And it will do multivariate, but it will be with respect to the first argument usually.
[00:30:40.000 --> 00:30:44.000]   But you have a structure, so you have multiple variables there.
[00:30:44.000 --> 00:30:57.000]   So definitely support it because, yeah, for neural networks, you have to have that.
[00:30:57.000 --> 00:31:06.000]   And just as a reminder, please consider posting the questions here instead so that I don't miss out on them.
[00:31:06.000 --> 00:31:10.000]   The link is in the Zoom chat.
[00:31:10.000 --> 00:31:16.000]   So I'll continue with the second notebook that I want to steal from Akash.
[00:31:16.000 --> 00:31:19.000]   And this covers pi trees.
[00:31:19.000 --> 00:31:26.000]   Christian later will be talking more about the frameworks he's been working on and their pi trees will be revisited again.
[00:31:26.000 --> 00:31:35.000]   But this should give us a good, I would say, brush up and also an in-depth view of what pi trees are.
[00:31:35.000 --> 00:31:40.000]   So if you've gone through a computer science degree, it's totally fine if you haven't.
[00:31:40.000 --> 00:31:45.000]   How we define trees, as I remember, is how we define pi trees.
[00:31:45.000 --> 00:31:51.000]   So the trees data structure is quite similar to pi trees.
[00:31:51.000 --> 00:32:01.000]   One thing that I still didn't understand so clearly, these can be defined like lists, tuples, dictionaries, name tuple, order dictionary, and none as well.
[00:32:01.000 --> 00:32:05.000]   I'm not sure where none would be most useful.
[00:32:05.000 --> 00:32:08.000]   Maybe just for empty ones.
[00:32:08.000 --> 00:32:11.000]   Yeah, none is very useful, actually.
[00:32:11.000 --> 00:32:15.000]   None is an empty pi tree.
[00:32:15.000 --> 00:32:29.000]   And it allows you to basically, if you want to erase something, you just put a none.
[00:32:29.000 --> 00:32:33.000]   But it doesn't, like it knows how to treat nones.
[00:32:33.000 --> 00:32:47.000]   So not obvious. Maybe if we see an example is more obvious why this is useful, but the thing disappears at the end, which is what you want.
[00:32:47.000 --> 00:32:58.000]   I'm just, I'm not here, maybe like it's a stupid question, but like, is this when we're like trying to zero the grads, for example, or when would you want to do this?
[00:32:58.000 --> 00:33:20.000]   Yeah, for example, if, well, it depends on how you want to do it, but if you have multiple, you have like a structure of parameters, and then maybe some of them, you want to freeze them, for example, or something.
[00:33:20.000 --> 00:33:27.000]   Well, not freeze them, but ignore them. The gradient will not pass with respect to them.
[00:33:27.000 --> 00:33:36.000]   Sometimes, depending on how it's actually easier, you can search for them and set them to none, replace them with none.
[00:33:36.000 --> 00:33:46.000]   And then once they're none, the gradient will basically not touch that part. I don't know if it makes sense.
[00:33:46.000 --> 00:33:48.000]   It does, it does.
[00:33:48.000 --> 00:33:55.000]   Maybe we, yeah, that's better to see in an example, but it is useful, very useful.
[00:33:55.000 --> 00:33:59.000]   That doesn't make sense. Thanks. Thanks, Sushant.
[00:33:59.000 --> 00:34:07.000]   So all of these are different ways of defining PyTrees. It can be a list. It can be a list of different elements.
[00:34:07.000 --> 00:34:10.000]   It can be a tuple as well.
[00:34:10.000 --> 00:34:14.000]   And when I said list, it can have different objects in here.
[00:34:14.000 --> 00:34:20.000]   The cool thing about all of these is, Jax knows how to handle these as PyTrees.
[00:34:20.000 --> 00:34:35.000]   So if you've, again, not to make anyone feel bad who's not studied computer science, but if you've studied computer science, you would have spent a large amount of time learning how to print out leaves, how to traverse trees.
[00:34:35.000 --> 00:34:40.000]   You want to implement these things in the most optimized way when you're trying to work with them.
[00:34:40.000 --> 00:34:51.000]   And basically Jax takes care of all of these operations. So if you pass all of these, and when I said, again, a side note, but when I said you can learn different things from Akash,
[00:34:51.000 --> 00:34:58.000]   this is how I recall that you should be putting all of these different PyTrees in a list and you should be iterating on them.
[00:34:58.000 --> 00:35:04.000]   That's why I mentioned that you should look at Akash's code and you probably always learn something new.
[00:35:04.000 --> 00:35:08.000]   Or maybe you're not as newbish as I am and you would know this already.
[00:35:08.000 --> 00:35:14.000]   But coming back to the original point, you can print out the representation of the tree.
[00:35:14.000 --> 00:35:22.000]   You can print out the length or the number of leaves and you can print out the leaves.
[00:35:22.000 --> 00:35:27.000]   So these are different functions that Jax can take care of.
[00:35:27.000 --> 00:35:42.000]   Simply put, it's a composition or a collection of all of these objects and we are most of the times working with different leaves.
[00:35:42.000 --> 00:35:57.000]   Maybe Christian, I'll again ask you this question. So I wasn't too clear about this point. Why they aren't device arrays and why can't we work with them?
[00:35:57.000 --> 00:36:16.000]   Yeah, that is a very important distinction. Device arrays or even NumPy arrays or any object that is not registered as a PyTree is treated as a leaf.
[00:36:16.000 --> 00:36:35.000]   Why is it important that device arrays are treated this way? Because usually, as you will see, Jax is interested in building structures that contain parameters.
[00:36:35.000 --> 00:36:49.000]   So that you can do a bunch of the Jax operations, but you don't want to go into the structures.
[00:36:49.000 --> 00:37:00.000]   For example, if you were to treat an array as a tree, you can just say it's the individual numbers flattened and that those are the leaves.
[00:37:00.000 --> 00:37:09.000]   But PyTrees weren't built for that purpose. They were built to handle collections of parameters.
[00:37:09.000 --> 00:37:15.000]   So the parameter is the leaf you want.
[00:37:15.000 --> 00:37:31.000]   Why? As you'll see throughout the API, usually you accept PyTrees as possible inputs and those operations will be done with respect to the arrays.
[00:37:31.000 --> 00:37:45.000]   If the arrays themselves were PyTrees, then it would operate with respect to scalars or something like that. And that doesn't make sense. That is not what you want.
[00:37:45.000 --> 00:37:58.000]   So you do get an error if you try to make a Jax array a PyTree simultaneously. Jax doesn't allow that.
[00:37:58.000 --> 00:38:02.000]   That makes sense.
[00:38:02.000 --> 00:38:09.000]   Just making sure I was where I was earlier.
[00:38:09.000 --> 00:38:15.000]   Now I scrolled down a bit. Let me scroll up.
[00:38:15.000 --> 00:38:25.000]   Another clarification here is these are tree-like data structures, as I mentioned, and not graph-like.
[00:38:25.000 --> 00:38:30.000]   So if you don't know what that is, it's totally fine. Just try to understand how PyTrees function.
[00:38:30.000 --> 00:38:35.000]   It's like a tree where you have different leaves node branching out from one particular top.
[00:38:35.000 --> 00:38:44.000]   And that's why we had this discussion. If you know what graphs are, please don't have a mental image of these being similar to those.
[00:38:44.000 --> 00:39:07.000]   There is a very interesting issue that I posted in the Jax repo where Matthew, who is one of the authors of Jax, discussed a way where Jax could actually have PyDags or something like that.
[00:39:07.000 --> 00:39:13.000]   Very interesting. I hope they do something like that in the future.
[00:39:13.000 --> 00:39:27.000]   But as the notebook points out, right now, if you have the same node in two parts of the tree, you can, because it's Python, you can just repeat an element.
[00:39:27.000 --> 00:39:43.000]   That element will be cloned. So if you pass it through a transformation and you get the structure back, you will have two different elements.
[00:39:43.000 --> 00:40:01.000]   Whereas originally you had the same one but in two places. That is very important because you cannot assume that the identities will be preserved if that makes sense.
[00:40:01.000 --> 00:40:11.000]   And then you will be confused a lot if you try to do stuff like that.
[00:40:11.000 --> 00:40:18.000]   Sorry if this is again a new question, but when are DAG-like structures useful for neural networks?
[00:40:18.000 --> 00:40:31.000]   For example, in Trix, the issue I opened it because it would be super useful to have a DAG-like structure for parameter sharing.
[00:40:31.000 --> 00:40:44.000]   So like parameter sharing, you kind of assume, "Oh, I have my model, but in different parts of the model, I will have the same structure."
[00:40:44.000 --> 00:40:49.000]   And well, that improves efficiency usually.
[00:40:49.000 --> 00:41:06.000]   But if you try to do it, like simply, like, "Oh, well, how would I do it?" Let's say in PyTorch, I just put the weights in different parts of my module and that is it.
[00:41:06.000 --> 00:41:21.000]   If you try to do this inside a PyTree, then well, originally they are, but once you pass them through an operation, they are just cloned.
[00:41:21.000 --> 00:41:29.000]   Referential transparency is the term you see in the notebook and in the documentation.
[00:41:29.000 --> 00:41:40.000]   And then after an operation, they will be basically not shared. They will be separate entities.
[00:41:40.000 --> 00:41:49.000]   So it would be very nice if DAGs implemented like PyDAGs.
[00:41:49.000 --> 00:41:56.000]   And I believe it's possible. It's just they maybe didn't think so since the beginning.
[00:41:56.000 --> 00:42:03.000]   But Matthew pointed out a way to do it. I mean, it would be a lot of work, but...
[00:42:03.000 --> 00:42:05.000]   Okay.
[00:42:05.000 --> 00:42:10.000]   Interesting topic, not applicable right now.
[00:42:10.000 --> 00:42:19.000]   Maybe you can take this as an example where PyTree is more useful than just a Python native structure.
[00:42:19.000 --> 00:42:30.000]   I'll try to answer this. DAGs comes with all of this goodness of running on an accelerator, being able to parallelize it across different devices.
[00:42:30.000 --> 00:42:36.000]   You can't do that with a Python native structure. So you get all of that goodness with PyTrees.
[00:42:36.000 --> 00:42:42.000]   Okay, I can also give some information there.
[00:42:42.000 --> 00:42:56.000]   Like, usually you use PyTrees to store the parameters of a model, as we will probably see when we get to Flux.
[00:42:56.000 --> 00:43:07.000]   Just for convenience, having them in the same place. And then if it's a PyTree, you can operate over it and do stuff like gradient descent.
[00:43:07.000 --> 00:43:14.000]   The second part, why not just use a Python native structure? That is kind of what Trix proposes.
[00:43:14.000 --> 00:43:25.000]   Well, it's not exactly that. It's a Python object that implements a PyTree.
[00:43:25.000 --> 00:43:31.000]   So you can have them.
[00:43:31.000 --> 00:43:38.000]   Actually, I don't know. I think PyTrees are a very good abstraction by themselves.
[00:43:38.000 --> 00:43:43.000]   So much that PyTorch is implementing PyTrees.
[00:43:43.000 --> 00:43:53.000]   So I think it's the other way around. Why doesn't Python implement PyTrees, I would say.
[00:43:53.000 --> 00:43:55.000]   Thanks.
[00:43:55.000 --> 00:44:04.000]   Coming back to Akash's notebook. Just as a reminder, we're looking at PyTrees here and we're trying to answer different questions around this.
[00:44:04.000 --> 00:44:14.000]   Coming back to PyTrees, you should use tree maps and tree multi-maps, which is why I think probably Christian says that you should.
[00:44:14.000 --> 00:44:20.000]   Python should have PyTrees by default, because this makes it easier to operate on the leaf nodes.
[00:44:20.000 --> 00:44:29.000]   Again, if you've worked with native, I would say late-codey questions where you have to do stuff to data structures that no one enjoys,
[00:44:29.000 --> 00:44:37.000]   you would remember that it's very annoying to implement these things. And then doing it in an optimized fashion is very difficult.
[00:44:37.000 --> 00:44:44.000]   Yeah, just as an anecdote, I've installed JAX just to operate over a JSON, something like that.
[00:44:44.000 --> 00:44:53.000]   They are useful regardless of JAX, which is what would be cool at Python.
[00:44:53.000 --> 00:44:57.000]   This could be extracted into a separate library, rather.
[00:44:57.000 --> 00:45:13.000]   There is one from DeepMind that is called dmTree. It is kind of a way to deal with PyTrees.
[00:45:13.000 --> 00:45:14.000]   It's that one?
[00:45:14.000 --> 00:45:15.000]   The first link?
[00:45:15.000 --> 00:45:25.000]   Yeah. So DeepMind tried to do this, like a library that only operates for PyTrees, which is nice.
[00:45:25.000 --> 00:45:33.000]   It has all the basic elements. It's just not the JAX one. So the JAX one is faster.
[00:45:33.000 --> 00:45:46.000]   I still install JAX to do that kind of thing. If you want to see examples of PyTrees in the wild, a JSON is a PyTree.
[00:45:46.000 --> 00:45:48.000]   That makes sense.
[00:45:48.000 --> 00:46:05.000]   It is a dictionary of lists and elements. That is a PyTree. And I've used JAX to deal with JSON data. So it's useful by itself.
[00:46:05.000 --> 00:46:19.000]   As I mentioned, and as Christian mentioned, it's very easy to operate. And for that you can use a tree map and tree multi-map for doing operations on leaves.
[00:46:19.000 --> 00:46:26.000]   Continuing further, I'm just going through and thinking if there are any things I want to cover.
[00:46:26.000 --> 00:46:37.000]   I think the other things are quite readable from here, so I just wanted to cover these points.
[00:46:37.000 --> 00:46:45.000]   Anything you want to add, Christian, to PyTrees?
[00:46:45.000 --> 00:47:00.000]   I think they're a very important topic. Maybe learn it sufficiently well. It's pretty easy. I don't know if you're going to slow down and go through it.
[00:47:00.000 --> 00:47:14.000]   But once you know PyTrees, you love them. That is something.
[00:47:14.000 --> 00:47:23.000]   They're incredibly useful. I don't know how to express more enthusiasm, but they are.
[00:47:23.000 --> 00:47:33.000]   The reason for not going through every fine detail is because the kernel itself is quite readable and I just wanted to give different highlights and revisit them again later.
[00:47:33.000 --> 00:47:40.000]   For sure. I think this could be a place where I say goodbye. I gotta leave.
[00:47:40.000 --> 00:47:45.000]   Thanks for joining us. And I'll continue the presentation myself.
[00:47:45.000 --> 00:47:53.000]   For sure. It was incredibly fun. Good luck. Thanks. Bye.
[00:47:53.000 --> 00:47:57.000]   Thanks for joining us again.
[00:47:57.000 --> 00:48:15.000]   Okay, I'll continue the presentation by myself and hopefully not mess up any of the bits. But I would encourage everyone to find Christian on Twitter. He has been, as you can see, mentoring our group and also doing incredible work in the realm of JAX.
[00:48:15.000 --> 00:48:22.000]   So from here, let me again move the annoying Zoom things out of the way.
[00:48:22.000 --> 00:48:33.000]   I want to introduce FLAGS and maybe try to as a stretch goal, cover units in JAX and compare them with PyTorch.
[00:48:33.000 --> 00:48:40.000]   So that's one of the stretch goals. Let's see if I can get to that in the next 30 minutes that we have.
[00:48:40.000 --> 00:48:49.000]   FLAGS is the framework for neural networks in JAX. And as the readme puts it, it's defined with flexibility in mind.
[00:48:49.000 --> 00:49:01.000]   Again, I haven't hit those hard corners yet where these things seem obvious. But as you heard Christian and as I've been spending more and more time,
[00:49:01.000 --> 00:49:09.000]   it started to make sense. Pmaps and Vmaps started to mix in and hopefully I'll start to understand where FLAGS really shines out.
[00:49:09.000 --> 00:49:20.000]   Another key design element, it works with TensorFlow datasets and data loaders and same with PyTorch. So it's agnostic in that way.
[00:49:20.000 --> 00:49:32.000]   I'll try to, and followed by this, I'll try to introduce the Linen API. So for PyTorch, you usually import neural networks via the NN module.
[00:49:32.000 --> 00:49:42.000]   However, you rely on the Linen API. If you head over to their documentation, or even their GitHub repository, you can find Nain's presentation around this.
[00:49:42.000 --> 00:49:50.000]   I'll try to summarize the key things from there. You have another API called Optacs for optimizers.
[00:49:50.000 --> 00:49:58.000]   And there's a lot of thought on, or a few opinionated ways of serializing and saving models as they should be.
[00:49:58.000 --> 00:50:08.000]   So for introducing the basics of FLAGS, once I reconnect this collab, and this comes from, this comes straight from their readme.
[00:50:08.000 --> 00:50:16.000]   This is, sorry, this comes straight from their documentation and it's FLAGS basics. You can also find it on their GitHub.
[00:50:16.000 --> 00:50:22.000]   Again, I'll be posting all of these links after the live stream. This is what it will cover.
[00:50:22.000 --> 00:50:33.000]   How to instantiate a model for FLAGS, how to initialize the parameters and use optimizers, and how can you save all of these models that you train with FLAGS.
[00:50:33.000 --> 00:50:42.000]   One of the ways I try to understand this, it's somewhat how Keras is to TensorFlow, that's how FLAGS is to JAX.
[00:50:42.000 --> 00:50:53.000]   So to work with JAX, first of all, you'll be importing typing. And this is one of the, I think, fine details introduced in Python 3.10.
[00:50:53.000 --> 00:50:59.000]   I'll again introduce these concepts, but if you're new to this, I would highly encourage checking these out.
[00:50:59.000 --> 00:51:05.000]   So this is something that comes from outside of JAX and FLAGS.
[00:51:05.000 --> 00:51:12.000]   From FLAGS, we'll just need the neural network modules from the Linin API, as I said.
[00:51:12.000 --> 00:51:19.000]   And in the notebook, in the documentation, they actually walk you through how to create linear regression, which I skipped.
[00:51:19.000 --> 00:51:28.000]   So to create a dense layer, which essentially just replicates linear regression, you can simply call nn.dense.
[00:51:28.000 --> 00:51:37.000]   This is somewhat similar to how you would do it in PyTorch, I would say, and you can say, please create five parameters, five input parameters.
[00:51:37.000 --> 00:51:42.000]   Please take five input parameters and pass that value.
[00:51:42.000 --> 00:51:49.000]   Next, the notebook teaches us how to define different layers and how to work with them.
[00:51:49.000 --> 00:52:01.000]   So one of the key ways of, I'm skipping again over a few details because we eventually end up revisiting all of these.
[00:52:01.000 --> 00:52:10.000]   And I'll come back to the gradient descent part as well.
[00:52:10.000 --> 00:52:16.000]   So now I want to create this mental model of how to create neural networks.
[00:52:16.000 --> 00:52:20.000]   As expected, you would inherit from nn.module.
[00:52:20.000 --> 00:52:24.000]   And remember when I said about hinting, this is what hinting looks like in practice.
[00:52:24.000 --> 00:52:35.000]   It's one of these fine details that you end up overusing when you're just introduced to it, but it's quite powerful in my experience and FLAGS heavily relies on it.
[00:52:35.000 --> 00:52:41.000]   So features would be expected as a sequence of integers.
[00:52:41.000 --> 00:52:51.000]   Another fine detail I want to point out here is in JAX modules or in JAX based neural networks, you would be defining these at the class level scope.
[00:52:51.000 --> 00:53:00.000]   So anything that is being saved inside of the model is defined at the class level scope.
[00:53:00.000 --> 00:53:10.000]   So we'll start by setting up and here we define all of our layers, which are for this case, simple dense layers.
[00:53:10.000 --> 00:53:17.000]   And we'll define a call function where we enumerate all of the layers and add a value to them on top.
[00:53:17.000 --> 00:53:20.000]   And again, not going into fine details here.
[00:53:20.000 --> 00:53:22.000]   I'm assuming everyone knows what these layers are.
[00:53:22.000 --> 00:53:27.000]   If you don't, please ask a question on the forums and I'll answer it happily.
[00:53:27.000 --> 00:53:38.000]   Now to initialize this neural network or to pass it some variables, which was defined earlier in this notebook as well, but I skipped over to introduce it here.
[00:53:38.000 --> 00:53:55.000]   You will call the random number generator, pick its values from a uniform distribution and create this array that will pass to the models parameters as we initialize it.
[00:53:55.000 --> 00:54:01.000]   So we create a model here and we pass the features which are supposed to be a sequence of integers.
[00:54:01.000 --> 00:54:08.000]   So it will have dense layers with 3, 4 and 5 neurons in them.
[00:54:08.000 --> 00:54:14.000]   I'm not drawing this because I assume everyone can have a nice mental image of what dense layers look like.
[00:54:14.000 --> 00:54:17.000]   If you don't, please interrupt me.
[00:54:17.000 --> 00:54:24.000]   So we define the function by creating an object of explicit MLP that we simply call model.
[00:54:24.000 --> 00:54:37.000]   The parameters are initialized by this uniform distribution that we called earlier using the random number generator.
[00:54:37.000 --> 00:54:43.000]   And then this is another one of those fine details that take some getting used to, especially for someone who's coming from PyTorch.
[00:54:43.000 --> 00:54:49.000]   You need to apply these parameters to the model.
[00:54:49.000 --> 00:54:56.000]   So nn.module subclass is made of a collection of data fields.
[00:54:56.000 --> 00:55:04.000]   Here it was the features that we defined as a sequence of integers.
[00:55:04.000 --> 00:55:12.000]   Setup is called when you can register the submodules or variables you will have to call in your model.
[00:55:12.000 --> 00:55:20.000]   And the call function, this should be obvious, returns the output of the model from the given input.
[00:55:20.000 --> 00:55:23.000]   So that's how you define MLP.
[00:55:23.000 --> 00:55:28.000]   Let's look at another example.
[00:55:28.000 --> 00:55:38.000]   This is an alternate way of doing the same, but you can call this decorator called compact and you can wrap this submodule around it.
[00:55:38.000 --> 00:55:49.000]   So we're doing a similar thing where we just define the call function, enumerate the feature layers, add the dense layers, and add a ReLU.
[00:55:49.000 --> 00:55:54.000]   So inside of any neural network, you would have a dense layer followed by ReLU.
[00:55:54.000 --> 00:55:57.000]   And this is similar to what we did earlier.
[00:55:57.000 --> 00:56:04.000]   Things are the same here, but the only difference we've done, and this is slightly neater to implement or slightly better refactored,
[00:56:04.000 --> 00:56:09.000]   is just call nn.compact and create this wrapper.
[00:56:09.000 --> 00:56:15.000]   So this is how you basically create a simple neural network or a simple MLP in Flux.
[00:56:15.000 --> 00:56:24.000]   Let me see if there are any questions.
[00:56:24.000 --> 00:56:28.000]   I am just double checking Zoom chat just to be sure.
[00:56:28.000 --> 00:56:30.000]   No, I don't see any questions.
[00:56:30.000 --> 00:56:33.000]   Alright, I'll continue further.
[00:56:33.000 --> 00:56:39.000]   Either I'm doing an awesome job of explaining everything or a terrible job, which is why I don't see any questions.
[00:56:39.000 --> 00:56:46.000]   I hope it's the first one.
[00:56:46.000 --> 00:56:48.000]   Awesome, the notebook is quite readable.
[00:56:48.000 --> 00:56:55.000]   So again, I'm consciously skipping over a lot of details because I will always revisit this in the next lecture as well.
[00:56:55.000 --> 00:57:00.000]   So for now, I want you all to have an okay mental image of everything.
[00:57:00.000 --> 00:57:03.000]   And then next week, we'll revisit and I'll point out fine details as well.
[00:57:03.000 --> 00:57:14.000]   As I said, I've learned this thing from fast.ai that I always try to implement in any course or lecture that we try to host.
[00:57:14.000 --> 00:57:21.000]   So assuming you don't have a dense layer provided by Flux, or if you want to write your own,
[00:57:21.000 --> 00:57:23.000]   here's how you can define those.
[00:57:23.000 --> 00:57:31.000]   And this would be probably really useful when you're trying to implement a different architecture inside of Flux.
[00:57:31.000 --> 00:57:41.000]   So you can define a kernel like so, where you pass the parameters, declare it as a kernel.
[00:57:41.000 --> 00:57:43.000]   You can also look at the documentation.
[00:57:43.000 --> 00:57:47.000]   It is quite intuitive for this bit.
[00:57:47.000 --> 00:57:54.000]   Initialize it with a function, declare whatever function you want to use here.
[00:57:54.000 --> 00:58:02.000]   Define the shape and features, and then define the operation that happens afterward.
[00:58:02.000 --> 00:58:11.000]   And you could also have a bias, for example, if you're trying to redefine, let's say, this would be dense layer here.
[00:58:11.000 --> 00:58:18.000]   And then you return the state of where you simply add the bias to the weights.
[00:58:18.000 --> 00:58:23.000]   From there, you can again, follow the same steps to create the model.
[00:58:23.000 --> 00:58:39.000]   Initialize it, apply these parameters, and that gives you a list of, sorry, a frozen dict of, again, all of these parameters that we've initialized.
[00:58:39.000 --> 00:58:51.000]   I think this was the introduction I wanted to give for the neural network API.
[00:58:51.000 --> 00:58:58.000]   Awesome, let me move on to the next bit.
[00:58:58.000 --> 00:59:05.000]   And reconnect this.
[00:59:05.000 --> 00:59:12.000]   But just to recap very quickly, this is how you can define MLPs and flags.
[00:59:12.000 --> 00:59:22.000]   Just in these few lines you can see, and if you actually decorate with nn.compact, you don't have to do the few fine details as I mentioned earlier.
[00:59:22.000 --> 00:59:32.000]   Inside of a neural network, you just need to define the number of neurons, followed by how many times you want to apply activation function.
[00:59:32.000 --> 00:59:37.000]   By definition, you should be applying it after every single layer in your neural network.
[00:59:37.000 --> 00:59:49.000]   So that's what we do here. We iterate over the feature list, apply a value followed by a dense, and then apply one final dense layer.
[00:59:49.000 --> 00:59:51.000]   And you return this.
[00:59:51.000 --> 00:59:58.000]   And from there, you can simply create a model with whatever dimensions or number of neurons you want to.
[00:59:58.000 --> 01:00:04.000]   Create one single batch, and initialize with this particular batch.
[01:00:04.000 --> 01:00:09.000]   And then your output becomes model.apply_variables, batch.
[01:00:09.000 --> 01:00:16.000]   Variables is how we initialize this, and batch is defined with the batch size that we need to work with.
[01:00:16.000 --> 01:00:23.000]   How would you define a CNN inside of flags?
[01:00:23.000 --> 01:00:31.000]   I'm probably again skipping CNN definition, assuming everyone knows what CNNs are and what happens inside of them.
[01:00:31.000 --> 01:00:38.000]   But inside of a CNN, you would have a bunch of convolutional layers, followed by ReLU, which is an activation function.
[01:00:38.000 --> 01:00:41.000]   You can switch it out. Most of the times you use ReLU.
[01:00:41.000 --> 01:00:51.000]   Followed by an average pooling or max pooling, followed by another convolutional layer with different kernel size, maybe the same kernel size.
[01:00:51.000 --> 01:00:59.000]   As you convolve over an image, the depth increases, which means you might see the number of features going up.
[01:00:59.000 --> 01:01:03.000]   Towards the end, you would have a dense layer.
[01:01:03.000 --> 01:01:11.000]   And if you're trying to do, let's say, classification, you would call it logged softmax.
[01:01:11.000 --> 01:01:22.000]   So in these few simple lines and inside of call function, which is what you define for defining the model inside of flags,
[01:01:22.000 --> 01:01:32.000]   we've defined a CNN with, it seems, two convolution layers followed by a few dense layers.
[01:01:32.000 --> 01:01:45.000]   And then again, you can initialize the model with creating a batch and having these variables initialized and then applying it to the model, which should give you the output.
[01:01:45.000 --> 01:01:58.000]   I'll take a pause and see if there are any questions, because this was quite a few things to pack in a few minutes.
[01:01:58.000 --> 01:02:13.000]   Okay, no questions, I'll continue further.
[01:02:13.000 --> 01:02:19.000]   I'll probably just wait for these two questions to come in.
[01:02:19.000 --> 01:02:38.000]   As a reminder to everyone, please do ask any questions that you want. This session is for you, so I'm happy to take any questions that you want.
[01:02:38.000 --> 01:02:42.000]   Thank you, Srinath, for that question.
[01:02:42.000 --> 01:02:51.000]   I will be comparing, if hopefully time permits, a unit implementation in PyTorch and JAX.
[01:02:51.000 --> 01:02:57.000]   But again, as I found, I would encourage you not to compare these at this moment.
[01:02:57.000 --> 01:03:02.000]   In your head, I found it quite confusing when I was just starting out.
[01:03:02.000 --> 01:03:18.000]   I would say after you've just gone through the basics and the notebooks and the documentation, and played around a nice bunch with all of them, after that I would encourage you to try and create a mental model, otherwise it's quite confusing.
[01:03:18.000 --> 01:03:27.000]   What is the @flag? This is a decorator we're calling to make our code more readable.
[01:03:27.000 --> 01:03:39.000]   This makes the function definition a little bit nicer. If I go to the previous colab and point out where this was introduced.
[01:03:39.000 --> 01:03:46.000]   So since we want to declare all of these sub-modules, nn.compact makes it relatively shorter and cleaner for us to do.
[01:03:46.000 --> 01:03:55.000]   So we don't have to define setup inside of our class, for example, wherever we define the model.
[01:03:55.000 --> 01:04:06.000]   Does this answer your question?
[01:04:06.000 --> 01:04:19.000]   What are the features? Features are the neurons that we're defining in every layer.
[01:04:19.000 --> 01:04:26.000]   I assume you don't have any further questions, but please ask if you do.
[01:04:26.000 --> 01:04:31.000]   So this is not very different from the CNN we just defined.
[01:04:31.000 --> 01:04:44.000]   And now what we're trying to do, or what I'm trying to walk you through is an annotated MNIST notebook that I've again shamelessly taken from the official repository.
[01:04:44.000 --> 01:04:49.000]   Again, links will be posted after the call, but we'll start by creating a simple CNN.
[01:04:49.000 --> 01:04:56.000]   And our goal here is to create a notebook or a neural network that works with MNIST.
[01:04:56.000 --> 01:05:01.000]   The classic example for any neural network library.
[01:05:01.000 --> 01:05:09.000]   For that, just to point out, we'll be working with TensorFlow datasets, which makes it really easy to grab MNIST.
[01:05:09.000 --> 01:05:18.000]   This is a suggested homework. Please consider trying this with PyTorch and see if you can make this work.
[01:05:18.000 --> 01:05:21.000]   So we define, I'm just trying to see if there's anything different happening here.
[01:05:21.000 --> 01:05:31.000]   We do introduce D shape, which is similar to the flattened layer, as you might remember from PyTorch.
[01:05:31.000 --> 01:05:40.000]   Apart from that, this is a simple CNN definition. It has a few convolution layers followed by an activation function.
[01:05:40.000 --> 01:05:48.000]   We average pool the output from here, flatten it out, follow it with a dense neural network or a fully connected neural network.
[01:05:48.000 --> 01:05:59.000]   And as expected, we call it Softmax because we're trying to classify numbers or classify images here.
[01:05:59.000 --> 01:06:06.000]   Since it's a classification problem, we'll be using cross entropy laws.
[01:06:06.000 --> 01:06:09.000]   And it is defined like so.
[01:06:09.000 --> 01:06:17.000]   So in here, we'll be taking the logics, the labels, and we'll one hot, sorry, one hot encode the labels.
[01:06:17.000 --> 01:06:21.000]   Since it's 10 classes, that's what we pass it to.
[01:06:21.000 --> 01:06:26.000]   So we define the number of classes as 10.
[01:06:26.000 --> 01:06:29.000]   And from there, this is the definition of cross entropy laws.
[01:06:29.000 --> 01:06:37.000]   I'll skip this, skip over this, assuming everyone knows how to define cross entropy laws.
[01:06:37.000 --> 01:06:44.000]   We compute the matrix, which are the laws in accuracy, the things that we care about.
[01:06:44.000 --> 01:06:49.000]   For that, we simply call cross entropy laws on the logics and the labels.
[01:06:49.000 --> 01:06:57.000]   So laws takes in whatever the outputs are coming out as and the real labels, so the ground truth.
[01:06:57.000 --> 01:07:00.000]   Accuracy is defined like so.
[01:07:00.000 --> 01:07:03.000]   And this is quite intuitive and similar to how you do it in PyTorch.
[01:07:03.000 --> 01:07:05.000]   I'll skip over that.
[01:07:05.000 --> 01:07:11.000]   Matrix is returned as a dictionary of both laws and accuracy.
[01:07:11.000 --> 01:07:16.000]   So next, we load the data by calling all of the goodness packed inside of TensorFlow data sets.
[01:07:16.000 --> 01:07:23.000]   I'm not a TensorFlow expert, so I assume this is the best way to do it since it's from the documentation and hopefully it is.
[01:07:23.000 --> 01:07:39.000]   We also normalize, I believe, the images that are coming in from here and set up the train and test splits by saving them in the train data set and test data set.
[01:07:39.000 --> 01:07:45.000]   So after that, we'll create a train state and as I'll just read out this paragraph.
[01:07:45.000 --> 01:07:52.000]   We want to create, or you usually create a single data class that represents the entire training state.
[01:07:52.000 --> 01:07:58.000]   So in JAX, you have the training states and you apply this to the model.
[01:07:58.000 --> 01:08:02.000]   The model just defines the functionality.
[01:08:02.000 --> 01:08:07.000]   And this training state would include the step number, the parameters and the optimizer state.
[01:08:07.000 --> 01:08:14.000]   This is again very different to how we do it in PyTorch.
[01:08:14.000 --> 01:08:24.000]   Adding optimizer and model to this state has the advantage that we only need to pass around a single argument to the function.
[01:08:24.000 --> 01:08:31.000]   So we define create train state where we take a random number generator, a learning rate and momentum.
[01:08:31.000 --> 01:08:36.000]   We create a CNN, initialize the parameters.
[01:08:36.000 --> 01:08:43.000]   This time we also create an optimizer, use this classic stochastic gradient descent where we pass the learning rate and momentum.
[01:08:43.000 --> 01:08:47.000]   Again, this will come in as an input.
[01:08:47.000 --> 01:08:52.000]   And we return the train state because it's quite important and it's done often.
[01:08:52.000 --> 01:09:05.000]   It's available as a function where we create a train state by calling this function where we apply the CNN, the parameters and the optimizer.
[01:09:05.000 --> 01:09:12.000]   Now at this point, we've managed to create a neural network.
[01:09:12.000 --> 01:09:20.000]   Define, sorry, first of all, define the neural network, create its states, define training state, define the matrix.
[01:09:20.000 --> 01:09:25.000]   So basically we're all set for the training bit.
[01:09:25.000 --> 01:09:29.000]   For which we'll do the following steps.
[01:09:29.000 --> 01:09:37.000]   We'll apply the parameters and a batch of input images. We'll compute the cross entropy loss.
[01:09:37.000 --> 01:09:43.000]   Evaluate the loss and apply a pi tree of gradients to the optimizer to update the model's parameters.
[01:09:43.000 --> 01:09:49.000]   So this is one of the examples where pi trees really stand out.
[01:09:49.000 --> 01:09:57.000]   And I'll probably skip over this, but it's really nice to know where to JIT functions.
[01:09:57.000 --> 01:10:02.000]   Just as a caveat, I probably try to JIT every single thing as we saw in the previous lecture as well.
[01:10:02.000 --> 01:10:04.000]   That is not the right approach.
[01:10:04.000 --> 01:10:11.000]   But as you start to play around with at least the official notebooks, you start getting a better understanding of what to JIT.
[01:10:11.000 --> 01:10:21.000]   And if you've not joined the previous lecture, JIT pre-compiles this code and optimizes it for whatever accelerator you're using it.
[01:10:21.000 --> 01:10:29.000]   Long story short, if you JIT a function or if you apply this decorator, it makes this code go burr.
[01:10:29.000 --> 01:10:32.000]   That's the main definition of it.
[01:10:32.000 --> 01:10:35.000]   So here we define the training step.
[01:10:35.000 --> 01:10:41.000]   Where we'll define the loss function like so. We'll take in the parameters.
[01:10:41.000 --> 01:10:46.000]   The logits are returned by applying the parameters and a batch.
[01:10:46.000 --> 01:10:49.000]   The batch would be the set of images.
[01:10:49.000 --> 01:10:57.000]   The loss comes from the cross entropy loss which we just defined above, which takes in the logits and the ground labels.
[01:10:57.000 --> 01:11:00.000]   And these are returned.
[01:11:00.000 --> 01:11:09.000]   From there we calculate the gradient by calling the gradient function and apply these gradients and compute the matrix.
[01:11:09.000 --> 01:11:11.000]   Matrix, sorry.
[01:11:11.000 --> 01:11:15.000]   And the state and matrix is what is returned from here.
[01:11:15.000 --> 01:11:23.000]   To evaluate how good our model is performing, again you'll apply state every time to a model.
[01:11:23.000 --> 01:11:27.000]   So the model itself doesn't retain the state, you have to apply it every time.
[01:11:27.000 --> 01:11:36.000]   You create an evaluation where you apply these parameters and batches and then you simply call the compute matrix to which you pass these logits and labels.
[01:11:43.000 --> 01:11:52.000]   I'm trying to see if I want to elaborate on any step from the training epoch, but this is quite similar to the general methods that we use.
[01:11:52.000 --> 01:12:02.000]   I'll just skip over these states. I believe the bits from here are quite similar to PyTorch I would say.
[01:12:02.000 --> 01:12:06.000]   So let me again take a pause and see if there are any questions.
[01:12:06.000 --> 01:12:10.000]   Yes, that is right.
[01:12:12.000 --> 01:12:13.000]   Okay.
[01:12:13.000 --> 01:12:21.000]   Any other questions so far for flags or anything we've discussed?
[01:12:21.000 --> 01:12:40.000]   Okay, awesome. I'll try to attempt the impossible of my stretch goal which is to cover units.
[01:12:40.000 --> 01:12:51.000]   I can't possibly cover the entirety of units and for that I'll try to go to YouTube and point you to a reading group we did earlier.
[01:12:51.000 --> 01:13:05.000]   I'm not sharing my screen because I'm on YouTube and trying to find the correct thing to point you all towards.
[01:13:05.000 --> 01:13:21.000]   So if you head over to the Weights and Biases YouTube channel and you hover over to playlists, you should see somewhere the paper reading group and in there you'll find units.
[01:13:21.000 --> 01:13:34.000]   So in that particular one hour, I covered units in quite some depth, covered what they are, the reason they exist, why they're so good and key details around it.
[01:13:34.000 --> 01:13:44.000]   If you want, please consider checking that out. It's not possible to summarize all of the details thoroughly, but I'll still try to do that.
[01:13:44.000 --> 01:13:51.000]   So units are an architecture that actually kicked off or really worked well with image segmentation.
[01:13:51.000 --> 01:13:59.000]   When I say kicked off, they actually made it possible for us to realize that neural networks work really well with image segmentation.
[01:13:59.000 --> 01:14:06.000]   Image segmentation is this task of labeling every single pixel inside of an image or video you see.
[01:14:06.000 --> 01:14:16.000]   So for example, if you're seeing this right now, you want to label that particular box or this particular area where my finger is as a monitor.
[01:14:16.000 --> 01:14:24.000]   This particular area as a table, this as a chair, so on and so forth. That is image segmentation.
[01:14:24.000 --> 01:14:29.000]   This is useful for helping a model understand literally what's inside of an image.
[01:14:29.000 --> 01:14:42.000]   And from there you can do interesting things. For example, background removal is one of, I think, the papers that I really saw this year that became quite interesting.
[01:14:42.000 --> 01:14:54.000]   Or you could use them with self-driving cars to understand where you can drive, which part of the road you're allowed to drive, if the thing in front of the car is a human or not.
[01:14:54.000 --> 01:15:00.000]   That is image segmentation in one minute.
[01:15:00.000 --> 01:15:06.000]   Units are the architecture that started working really well with image segmentation. These are U-shaped networks.
[01:15:06.000 --> 01:15:13.000]   This is a four-point summary, which can't do it justice. But these have an encoder-decoder architecture.
[01:15:13.000 --> 01:15:20.000]   And they introduced the secret sauce to ResNet before ResNet, as I understood correctly, which is the skip connection.
[01:15:20.000 --> 01:15:26.000]   What is this? So here is the architecture. And to credit, this image comes from the original paper.
[01:15:26.000 --> 01:15:33.000]   I've not drawn this. This is just a snap from there. And if you look up units, you should find the paper.
[01:15:33.000 --> 01:15:39.000]   But if you look on the bottom right edge, the blue arrows represent a convolutional layer.
[01:15:39.000 --> 01:15:46.000]   White is a copy and crop, which is what is a skip connection.
[01:15:46.000 --> 01:15:54.000]   Max pooling is denoted as red. And then you have the up-conv layer, which was key to making units work.
[01:15:54.000 --> 01:16:00.000]   So without going into detail of why this is important and intuitively what's the intuition behind it,
[01:16:00.000 --> 01:16:05.000]   which I leave you to checking out in the paper reading group I pointed you to,
[01:16:05.000 --> 01:16:11.000]   we start with an image. And if you look at the left part of this architecture, that is the encoder.
[01:16:11.000 --> 01:16:19.000]   So it takes an image, encodes the information, and then the right part of this image is known as a decoder.
[01:16:19.000 --> 01:16:25.000]   A couple of things that are happening here is we apply a bunch of convolutions followed by ReLU.
[01:16:25.000 --> 01:16:32.000]   We apply a max pooling layer to it, apply the same convolutions and activation function,
[01:16:32.000 --> 01:16:41.000]   max pool it again. We get to a smaller resolution from where we up-conv and get to an image that is similar to the input size.
[01:16:41.000 --> 01:16:50.000]   Between these layers, we copy and crop and concatenate these features over to the decoder layers.
[01:16:50.000 --> 01:16:59.000]   So that is the unit architecture. Why did I recap this? Because I wanted to cover its implementation in Flux.
[01:16:59.000 --> 01:17:10.000]   So here is the implementation in PyTorch. And for now, I think this should help you a bit, Srinath.
[01:17:10.000 --> 01:17:17.000]   I'll try to compare the PyTorch implementation with the JAX implementation or the Flux implementation.
[01:17:17.000 --> 01:17:24.000]   So inside of any neural network, you just want to define these functions that we took a look at.
[01:17:24.000 --> 01:17:43.000]   So let me try to stop sharing and split my screen across where I have the image on one side and have the implementation on the other.
[01:17:43.000 --> 01:17:58.000]   Sorry, I'm just trying to make the screen share work. It's a little bit annoying right now.
[01:17:58.000 --> 01:18:06.000]   One year into the pandemic and I'm still learning how to operate Zoom effectively. Sorry about this.
[01:18:06.000 --> 01:18:14.000]   Let me share my entire desktop and I'll assume everyone can see this.
[01:18:14.000 --> 01:18:20.000]   So on the left, you have the unit architecture and on the right, you're just trying to code the different blocks inside of it.
[01:18:20.000 --> 01:18:34.000]   So a block inside of this would consist of, if you look at the blue arrow, convolution followed by a ReLU followed by another convolution because that's what is happening here.
[01:18:34.000 --> 01:18:47.000]   And in the forward function, you would just wrap this with the ReLU inside of which you'll call a conv2, a ReLU and a conv1. That's how you change this.
[01:18:47.000 --> 01:18:56.000]   Next, we define the encoder, which is the left bit of this architecture. And again, this is a rushed explanation, which means things might not make sense.
[01:18:56.000 --> 01:19:01.000]   But I encourage you all to come back and watch this video or interrupt me and ask questions.
[01:19:01.000 --> 01:19:06.000]   So we define the encoder like so.
[01:19:06.000 --> 01:19:12.000]   And I'll just skip over these numbers because these are quite...
[01:19:12.000 --> 01:19:16.000]   These come just from the implementation details and aren't that important.
[01:19:16.000 --> 01:19:29.000]   So we'll call the superclass constructor and define the encoder blocks, which is just a module list of all of the blocks inside of the definition.
[01:19:29.000 --> 01:19:31.000]   After which we'll call the max pool layer.
[01:19:31.000 --> 01:19:38.000]   So after applying all of these functions, as you can see on the left, we call a max pool layer, right?
[01:19:38.000 --> 01:19:49.000]   Which is, sorry, the red arrow.
[01:19:49.000 --> 01:19:52.000]   And you define the forward function like so.
[01:19:52.000 --> 01:20:02.000]   So for any block inside of encoder blocks, X will contain block of X.
[01:20:02.000 --> 01:20:10.000]   And we append this to the list, followed by which we apply a pool, which is max pool 2d as defined above.
[01:20:10.000 --> 01:20:12.000]   Same for decoder.
[01:20:12.000 --> 01:20:22.000]   You would store a series of up cons in the module list and upcon was just const post 2d.
[01:20:22.000 --> 01:20:24.000]   If you work with units, you'd know what that is.
[01:20:24.000 --> 01:20:28.000]   If you don't, please watch the video that I pointed you to.
[01:20:28.000 --> 01:20:38.000]   And decoder blocks are defined as module list of all of these blocks in the entirety of all of these channels.
[01:20:38.000 --> 01:20:50.000]   So inside the forward function, you call upcon layers, you crop and concatenate, which is defined if you look on the left as the gray or white arrow.
[01:20:50.000 --> 01:20:54.000]   And from there, you just return the decoder block of that particular layer.
[01:20:54.000 --> 01:20:57.000]   And then you would have to define the crop here.
[01:20:57.000 --> 01:21:03.000]   From there, it's really easy to define a unit since we've defined all of its fine details.
[01:21:03.000 --> 01:21:13.000]   Let me stop sharing my screen and make this full screen so that I can share it now.
[01:21:13.000 --> 01:21:20.000]   Oops, wrong screen. Sorry about that.
[01:21:20.000 --> 01:21:27.000]   I'm assuming everyone can see the unit notebook.
[01:21:27.000 --> 01:21:29.000]   Could you please clarify?
[01:21:29.000 --> 01:21:31.000]   Okay, I see the green bar around it.
[01:21:31.000 --> 01:21:35.000]   Sorry about this. I was just making sure if this is visible to everyone.
[01:21:35.000 --> 01:21:43.000]   So now to define the unit, finally, we've defined all of the layers or all of the functions that we need inside of it.
[01:21:43.000 --> 01:21:51.000]   We can simply grab the encoder block, the decoder block, create a head and a retain dimension and define the forward function like so.
[01:21:51.000 --> 01:21:54.000]   So this is how you do it in PyTorch.
[01:21:54.000 --> 01:22:01.000]   To not mind boggle and confuse ourselves, we'll use the Scenic API, which comes from Google Research as well.
[01:22:01.000 --> 01:22:09.000]   And I've shamelessly again stolen the unit implementation in there.
[01:22:09.000 --> 01:22:17.000]   And if you're new to JAX, again, Scenic makes it easy to work with computer vision models in JAX.
[01:22:17.000 --> 01:22:25.000]   I've stolen this code from their example baseline on this link.
[01:22:25.000 --> 01:22:31.000]   So inside of Scenic, you have segmentation models from where you can import a segmentation model.
[01:22:31.000 --> 01:22:35.000]   You can import neural network layers and the operations.
[01:22:35.000 --> 01:22:42.000]   So remember the part where we were applying conf operator to all of the different inputs repeatedly?
[01:22:42.000 --> 01:22:52.000]   Here it's defined as conf 3x3 by calling nn.conf and creating a kernel size of 3x3 because it's convolution 3x3.
[01:22:52.000 --> 01:23:03.000]   We define dconf 3x3, which is transpose 2d in PyTorch like so.
[01:23:03.000 --> 01:23:08.000]   So we define the features on a class level scale.
[01:23:08.000 --> 01:23:18.000]   As a reminder, all of the things that you want being stored inside of a neural network in JAX have a class level definition.
[01:23:18.000 --> 01:23:20.000]   So we define features up top.
[01:23:20.000 --> 01:23:27.000]   We define padding and we take a flag if we want to use batch norm or not.
[01:23:27.000 --> 01:23:50.000]   Inside, as expected, we define the call function where if you go through this, you'll understand that it is just conf trans 2d similar to how you do it in PyTorch.
[01:23:50.000 --> 01:23:58.000]   From there, we define a conf relu 2, which is 2 unpadded convolutions and relus.
[01:23:58.000 --> 01:24:11.000]   I think I can skip over these details because these are quite self-explanatory if you work with units and in the interest of time, I'm just rushing through.
[01:24:11.000 --> 01:24:19.000]   But again, if you actually go to this link and check out this Python file, it should make sense if you work with relus.
[01:24:19.000 --> 01:24:23.000]   But I'm just trying to point out the key differences here.
[01:24:23.000 --> 01:24:28.000]   Next, we define the downsample block, which inherits from nn.module.
[01:24:28.000 --> 01:24:35.000]   On a class scale, we define the features, padding, and set a flag for batch norm.
[01:24:35.000 --> 01:24:42.000]   In the call, we'll define functions that help us downsample the input.
[01:24:42.000 --> 01:24:49.000]   For that, we'll call conf relu 2 on the features, padding, and batch norm.
[01:24:49.000 --> 01:24:58.000]   We'll check if we want to apply batch norm to the batch and the training dataset that gets inputted here, followed by which we'll apply a max pooling there.
[01:24:58.000 --> 01:25:03.000]   Remember the red arrows from the diagram I showed you? This is that.
[01:25:03.000 --> 01:25:11.000]   Inside of that architecture, if you paid close attention, there was this bottom layer, which is defined like so.
[01:25:11.000 --> 01:25:16.000]   So inside of that, you have two unpadded convolutions, dropout and deconvolution happening.
[01:25:16.000 --> 01:25:25.000]   So we define these here. We've already defined these functions above, so we can simply call conf relu 2 and conf 3 by 3.
[01:25:25.000 --> 01:25:30.000]   These get applied progressively to the array that we pass in here.
[01:25:30.000 --> 01:25:33.000]   And we simply return this.
[01:25:33.000 --> 01:25:37.000]   We do the same for upsample block.
[01:25:37.000 --> 01:25:43.000]   And then we define an output block. I'm skipping over the details again in the interest of time.
[01:25:43.000 --> 01:25:50.000]   And now we're ready to define unit, since we've defined all of the functions that constitute it.
[01:25:50.000 --> 01:25:59.000]   So we'll take an input of number of classes, we'll define the block size, we'll ask if we want to use padding or batch norm.
[01:25:59.000 --> 01:26:02.000]   And then we define the call.
[01:26:02.000 --> 01:26:18.000]   So if you remember the part where we were looping over the modulus, this is somewhat similar to that.
[01:26:18.000 --> 01:26:25.000]   So we loop over the block size and grab the features in there.
[01:26:25.000 --> 01:26:37.000]   Apply a downsample block, append the skip connections, so the white arrow, followed by which we call the bottleneck block.
[01:26:37.000 --> 01:26:44.000]   Skipping over the fine details again, just pointing out the details.
[01:26:44.000 --> 01:26:47.000]   And then we do the same for upscaling.
[01:26:47.000 --> 01:26:54.000]   So in this bit, the first half, we define the encoder like so.
[01:26:54.000 --> 01:27:04.000]   And in the second part, we define the decoder by calling the upsample block, followed by the output block that we had defined above.
[01:27:04.000 --> 01:27:13.000]   Not we, someone that kindly created this Python notebook that I'm shamelessly walking everyone through.
[01:27:13.000 --> 01:27:20.000]   So that is a very quick comparison of how to implement units in PyTorch versus JAX.
[01:27:20.000 --> 01:27:26.000]   I will probably again revisit this to point out very fine details in the next meetup.
[01:27:26.000 --> 01:27:32.000]   But for now, let me stop sharing and check if there are any questions.
[01:27:32.000 --> 01:27:50.000]   Okay, I don't see any questions.
[01:27:50.000 --> 01:27:53.000]   I'll take one last call for questions.
[01:27:53.000 --> 01:28:16.000]   If you have any questions, please ask them right now, or you can always come back later on the forums.
[01:28:16.000 --> 01:28:23.000]   I'll, okay, I was going to say I'll take a look at the question afterwards.
[01:28:23.000 --> 01:28:28.000]   But, okay. And again, my apologies to everyone because this was quite a rushed explanation.
[01:28:28.000 --> 01:28:35.000]   But if I try to walk everyone through every fine detail, this becomes a six hour live stream.
[01:28:35.000 --> 01:28:37.000]   No one has time for that.
[01:28:37.000 --> 01:28:48.000]   So I bring my personal opinions to the study groups, which are, I want you all to have a nice mental image and reintroduce concepts every week.
[01:28:48.000 --> 01:28:56.000]   So I rushed through the explanations to give a nice mental image and then walk through the fine details while pointing out which notebooks everyone should read.
[01:28:56.000 --> 01:29:01.000]   Assuming you all have the time, which is why we're coming here to learn.
[01:29:01.000 --> 01:29:10.000]   The reason we do this at Weights and Biases is because we want everyone to learn what's interesting in machine learning.
[01:29:10.000 --> 01:29:15.000]   And since I'm hosting these groups, I bring my own opinions to that.
[01:29:15.000 --> 01:29:21.000]   But just as a reminder, everyone is always welcome to come back and ask any questions.
[01:29:21.000 --> 01:29:31.000]   To wrap up, here are some suggested homeworks. Please consider joining 27 Days of JAX, wherein you use this hashtag and tag me or Weights and Biases.
[01:29:31.000 --> 01:29:36.000]   We're running an active competition every week where we pick winners.
[01:29:36.000 --> 01:29:44.000]   We'll be sending some swag your way and more. I'm trying to convince the team to give out a few more exciting things beyond that.
[01:29:44.000 --> 01:29:49.000]   Think compute credits. Don't tell anyone.
[01:29:49.000 --> 01:30:01.000]   Another one of the suggested homeworks, please consider comparing or creating a model implementation in FLAGS and then combining it with PyTorch if it's not already there in the examples.
[01:30:01.000 --> 01:30:06.000]   Implement a neural network in pure JAX, then FLAGS plus JAX.
[01:30:06.000 --> 01:30:16.000]   So try to find an implementation of ResNet and Let's Say NumPy, port that over to JAX, then try to use FLAGS and JAX.
[01:30:16.000 --> 01:30:19.000]   And consider writing about all of these things.
[01:30:19.000 --> 01:30:26.000]   In my opinion, it's not just because you want to share this work, but also it really, really solidifies whatever you have learned.
[01:30:26.000 --> 01:30:35.000]   That's my biggest takeaway. Whenever you get behind a camera and at the expense of looking foolish, which I'm totally okay with,
[01:30:35.000 --> 01:30:39.000]   I end up learning so much more while trying to walk everyone through the FLAGS API.
[01:30:39.000 --> 01:30:46.000]   I wouldn't have gone through the fine details if I were just to study it by myself.
[01:30:46.000 --> 01:30:50.000]   So if you study these things, if you spend more time, please consider writing about it.
[01:30:50.000 --> 01:30:53.000]   That was a long-winded way of saying that.
[01:30:53.000 --> 01:30:59.000]   Thanks again, everyone, for joining. I have one request for everyone that is still with us.
[01:30:59.000 --> 01:31:03.000]   My colleague, Andrea, helps make these events happen.
[01:31:03.000 --> 01:31:13.000]   It is her birthday in one hour, so I would request everyone to wish her a happy birthday because she's the reason why all of these sessions really happen at Weights and Biases.
[01:31:13.000 --> 01:31:18.000]   So if you're still here, please give a happy birthday for my colleague Andrea in the chat.
[01:31:18.000 --> 01:31:32.000]   Thanks for joining us. We'll be meeting again next week at the same time.
[01:31:32.000 --> 01:31:42.000]   [BLANK_AUDIO]

