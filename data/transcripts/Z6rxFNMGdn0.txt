
[00:00:00.000 --> 00:00:03.720]   The following is a conversation with Ian Goodfellow.
[00:00:03.720 --> 00:00:06.360]   He's the author of the popular textbook on deep learning,
[00:00:06.360 --> 00:00:08.920]   simply titled "Deep Learning."
[00:00:08.920 --> 00:00:12.320]   He coined the term of generative adversarial networks,
[00:00:12.320 --> 00:00:14.560]   otherwise known as GANs,
[00:00:14.560 --> 00:00:18.160]   and with his 2014 paper is responsible
[00:00:18.160 --> 00:00:20.440]   for launching the incredible growth
[00:00:20.440 --> 00:00:22.120]   of research and innovation
[00:00:22.120 --> 00:00:24.720]   in this subfield of deep learning.
[00:00:24.720 --> 00:00:27.520]   He got his BS and MS at Stanford,
[00:00:27.520 --> 00:00:30.120]   his PhD at University of Montreal
[00:00:30.120 --> 00:00:33.320]   with Yoshua Bengio and Aaron Kervil.
[00:00:33.320 --> 00:00:35.240]   He held several research positions,
[00:00:35.240 --> 00:00:37.600]   including an open AI, Google Brain,
[00:00:37.600 --> 00:00:41.560]   and now at Apple as the director of machine learning.
[00:00:41.560 --> 00:00:45.400]   This recording happened while Ian was still at Google Brain,
[00:00:45.400 --> 00:00:48.520]   but we don't talk about anything specific to Google
[00:00:48.520 --> 00:00:50.760]   or any other organization.
[00:00:50.760 --> 00:00:52.480]   This conversation is part
[00:00:52.480 --> 00:00:54.520]   of the Artificial Intelligence podcast.
[00:00:54.520 --> 00:00:57.560]   If you enjoy it, subscribe on YouTube, iTunes,
[00:00:57.560 --> 00:01:00.880]   or simply connect with me on Twitter @LexFriedman,
[00:01:00.880 --> 00:01:03.000]   spelled F-R-I-D.
[00:01:03.000 --> 00:01:07.080]   And now, here's my conversation with Ian Goodfellow.
[00:01:07.080 --> 00:01:10.960]   You open your popular deep learning book
[00:01:10.960 --> 00:01:13.600]   with a Russian doll type diagram
[00:01:13.600 --> 00:01:15.880]   that shows deep learning as a subset
[00:01:15.880 --> 00:01:17.120]   of representation learning,
[00:01:17.120 --> 00:01:19.960]   which in turn is a subset of machine learning,
[00:01:19.960 --> 00:01:22.520]   and finally a subset of AI.
[00:01:22.520 --> 00:01:25.280]   So this kind of implies that there may be limits
[00:01:25.280 --> 00:01:27.720]   to deep learning in the context of AI.
[00:01:27.720 --> 00:01:31.560]   So what do you think is the current limits of deep learning,
[00:01:31.560 --> 00:01:33.120]   and are those limits something
[00:01:33.120 --> 00:01:35.760]   that we can overcome with time?
[00:01:35.760 --> 00:01:37.720]   - Yeah, I think one of the biggest limitations
[00:01:37.720 --> 00:01:39.320]   of deep learning is that right now
[00:01:39.320 --> 00:01:42.920]   it requires really a lot of data, especially labeled data.
[00:01:42.920 --> 00:01:45.480]   There are some unsupervised
[00:01:45.480 --> 00:01:47.140]   and semi-supervised learning algorithms
[00:01:47.140 --> 00:01:49.480]   that can reduce the amount of labeled data you need,
[00:01:49.480 --> 00:01:52.200]   but they still require a lot of unlabeled data.
[00:01:52.200 --> 00:01:54.240]   Reinforcement learning algorithms, they don't need labels,
[00:01:54.240 --> 00:01:56.320]   but they need really a lot of experiences.
[00:01:56.320 --> 00:01:58.960]   As human beings, we don't learn to play Pong
[00:01:58.960 --> 00:02:01.600]   by failing at Pong 2 million times.
[00:02:01.600 --> 00:02:05.920]   So just getting the generalization ability better
[00:02:05.920 --> 00:02:08.080]   is one of the most important bottlenecks
[00:02:08.080 --> 00:02:10.600]   in the capability of the technology today.
[00:02:10.600 --> 00:02:12.400]   And then I guess I'd also say deep learning
[00:02:12.400 --> 00:02:15.660]   is like a component of a bigger system.
[00:02:15.660 --> 00:02:19.080]   So far, nobody is really proposing to have
[00:02:20.640 --> 00:02:22.040]   only what you'd call deep learning
[00:02:22.040 --> 00:02:25.560]   as the entire ingredient of intelligence.
[00:02:25.560 --> 00:02:29.880]   You use deep learning as sub-modules of other systems,
[00:02:29.880 --> 00:02:32.360]   like AlphaGo has a deep learning model
[00:02:32.360 --> 00:02:34.160]   that estimates the value function.
[00:02:34.160 --> 00:02:36.620]   Most reinforcement learning algorithms
[00:02:36.620 --> 00:02:37.920]   have a deep learning module
[00:02:37.920 --> 00:02:40.360]   that estimates which action to take next,
[00:02:40.360 --> 00:02:42.520]   but you might have other components.
[00:02:42.520 --> 00:02:46.120]   - So you're basically building a function estimator.
[00:02:46.120 --> 00:02:48.640]   Do you think it's possible,
[00:02:48.640 --> 00:02:50.180]   you said nobody's kind of been thinking
[00:02:50.180 --> 00:02:52.280]   about this so far, but do you think neural networks
[00:02:52.280 --> 00:02:56.080]   could be made to reason in the way symbolic systems did
[00:02:56.080 --> 00:02:58.800]   in the '80s and '90s to do more,
[00:02:58.800 --> 00:03:01.480]   create more like programs as opposed to functions?
[00:03:01.480 --> 00:03:03.980]   - Yeah, I think we already see that a little bit.
[00:03:03.980 --> 00:03:08.880]   I already kind of think of neural nets as a kind of program.
[00:03:08.880 --> 00:03:12.960]   I think of deep learning as basically learning programs
[00:03:12.960 --> 00:03:15.320]   that have more than one step.
[00:03:15.320 --> 00:03:17.000]   So if you draw a flow chart,
[00:03:17.760 --> 00:03:19.580]   or if you draw a TensorFlow graph
[00:03:19.580 --> 00:03:21.900]   describing your machine learning model,
[00:03:21.900 --> 00:03:23.540]   I think of the depth of that graph
[00:03:23.540 --> 00:03:25.900]   as describing the number of steps that run in sequence,
[00:03:25.900 --> 00:03:27.660]   and then the width of that graph
[00:03:27.660 --> 00:03:30.180]   is the number of steps that run in parallel.
[00:03:30.180 --> 00:03:31.720]   Now it's been long enough
[00:03:31.720 --> 00:03:32.940]   that we've had deep learning working
[00:03:32.940 --> 00:03:33.920]   that it's a little bit silly
[00:03:33.920 --> 00:03:35.780]   to even discuss shallow learning anymore.
[00:03:35.780 --> 00:03:38.940]   But back when I first got involved in AI,
[00:03:38.940 --> 00:03:40.140]   when we used machine learning,
[00:03:40.140 --> 00:03:41.320]   we were usually learning things
[00:03:41.320 --> 00:03:43.740]   like support vector machines.
[00:03:43.740 --> 00:03:45.660]   You could have a lot of input features to the model,
[00:03:45.660 --> 00:03:48.140]   and you could multiply each feature by a different weight.
[00:03:48.140 --> 00:03:50.120]   All those multiplications were done in parallel
[00:03:50.120 --> 00:03:51.260]   to each other.
[00:03:51.260 --> 00:03:52.740]   There wasn't a lot done in series.
[00:03:52.740 --> 00:03:54.360]   I think what we got with deep learning
[00:03:54.360 --> 00:03:58.420]   was really the ability to have steps of a program
[00:03:58.420 --> 00:04:00.340]   that run in sequence.
[00:04:00.340 --> 00:04:03.180]   And I think that we've actually started to see
[00:04:03.180 --> 00:04:05.020]   that what's important with deep learning
[00:04:05.020 --> 00:04:07.980]   is more the fact that we have a multi-step program
[00:04:07.980 --> 00:04:10.780]   rather than the fact that we've learned a representation.
[00:04:10.780 --> 00:04:15.140]   If you look at things like ResNets, for example,
[00:04:15.140 --> 00:04:18.660]   they take one particular kind of representation
[00:04:18.660 --> 00:04:21.060]   and they update it several times.
[00:04:21.060 --> 00:04:23.560]   Back when deep learning first really took off
[00:04:23.560 --> 00:04:25.740]   in the academic world in 2006,
[00:04:25.740 --> 00:04:27.660]   when Jeff Hinton showed
[00:04:27.660 --> 00:04:30.180]   that you could train deep belief networks,
[00:04:30.180 --> 00:04:31.980]   everybody who was interested in the idea
[00:04:31.980 --> 00:04:33.560]   thought of it as each layer
[00:04:33.560 --> 00:04:35.940]   learns a different level of abstraction.
[00:04:35.940 --> 00:04:37.820]   That the first layer trained on images
[00:04:37.820 --> 00:04:38.940]   learns something like edges,
[00:04:38.940 --> 00:04:40.420]   and the second layer learns corners,
[00:04:40.420 --> 00:04:43.320]   and eventually you get these kind of grandmother cell units
[00:04:43.320 --> 00:04:45.920]   that recognize specific objects.
[00:04:45.920 --> 00:04:47.980]   Today, I think most people think of it
[00:04:47.980 --> 00:04:50.660]   more as a computer program,
[00:04:50.660 --> 00:04:51.980]   where as you add more layers,
[00:04:51.980 --> 00:04:55.140]   you can do more updates before you output your final number.
[00:04:55.140 --> 00:04:56.420]   But I don't think anybody believes
[00:04:56.420 --> 00:05:01.420]   that layer 150 of the ResNet is a grandmother cell,
[00:05:01.420 --> 00:05:05.080]   and layer 100 is contours or something like that.
[00:05:05.080 --> 00:05:08.180]   - Okay, so you're not thinking of it
[00:05:08.180 --> 00:05:11.520]   as a singular representation that keeps building.
[00:05:11.520 --> 00:05:14.060]   You think of it as a program,
[00:05:14.060 --> 00:05:15.940]   sort of almost like a state.
[00:05:15.940 --> 00:05:18.580]   Representation is a state of understanding.
[00:05:18.580 --> 00:05:20.260]   - Yeah, I think of it as a program
[00:05:20.260 --> 00:05:21.500]   that makes several updates
[00:05:21.500 --> 00:05:23.820]   and arrives at better and better understandings,
[00:05:23.820 --> 00:05:27.500]   but it's not replacing the representation at each step.
[00:05:27.500 --> 00:05:29.160]   It's refining it.
[00:05:29.160 --> 00:05:31.660]   And in some sense, that's a little bit like reasoning.
[00:05:31.660 --> 00:05:33.560]   It's not reasoning in the form of deduction,
[00:05:33.560 --> 00:05:36.940]   but it's reasoning in the form of taking a thought
[00:05:36.940 --> 00:05:39.420]   and refining it and refining it carefully
[00:05:39.420 --> 00:05:41.260]   until it's good enough to use.
[00:05:41.260 --> 00:05:43.580]   - So do you think, and I hope you don't mind,
[00:05:43.580 --> 00:05:46.020]   we'll jump philosophical every once in a while.
[00:05:46.020 --> 00:05:50.460]   Do you think of cognition, human cognition,
[00:05:50.460 --> 00:05:53.500]   or even consciousness as simply a result
[00:05:53.500 --> 00:05:58.100]   of this kind of sequential representation learning,
[00:05:58.100 --> 00:06:00.420]   do you think that can emerge?
[00:06:00.420 --> 00:06:02.460]   - Cognition, yes, I think so.
[00:06:02.460 --> 00:06:03.700]   Consciousness, it's really hard
[00:06:03.700 --> 00:06:06.440]   to even define what we mean by that.
[00:06:06.440 --> 00:06:09.820]   I guess there's, consciousness is often defined
[00:06:09.820 --> 00:06:12.060]   as things like having self-awareness,
[00:06:12.060 --> 00:06:16.060]   and that's relatively easy to turn into something actionable
[00:06:16.060 --> 00:06:18.380]   for a computer scientist to reason about.
[00:06:18.380 --> 00:06:19.700]   People also define consciousness
[00:06:19.700 --> 00:06:22.420]   in terms of having qualitative states of experience,
[00:06:22.420 --> 00:06:25.260]   like qualia, and there's all these philosophical problems,
[00:06:25.260 --> 00:06:27.820]   like could you imagine a zombie
[00:06:27.820 --> 00:06:30.700]   who does all the same information processing as a human,
[00:06:30.700 --> 00:06:33.460]   but doesn't really have the qualitative experiences
[00:06:33.460 --> 00:06:34.660]   that we have?
[00:06:34.660 --> 00:06:37.540]   That sort of thing, I have no idea how to formalize
[00:06:37.540 --> 00:06:39.940]   or turn it into a scientific question.
[00:06:39.940 --> 00:06:41.580]   I don't know how you could run an experiment
[00:06:41.580 --> 00:06:44.860]   to tell whether a person is a zombie or not.
[00:06:44.860 --> 00:06:47.180]   And similarly, I don't know how you could run an experiment
[00:06:47.180 --> 00:06:49.640]   to tell whether an advanced AI system
[00:06:49.640 --> 00:06:53.020]   had become conscious in the sense of qualia or not.
[00:06:53.020 --> 00:06:54.540]   - But in the more practical sense,
[00:06:54.540 --> 00:06:56.260]   like almost like self-attention,
[00:06:56.260 --> 00:06:58.900]   you think consciousness and cognition can,
[00:06:58.900 --> 00:07:03.220]   in an impressive way, emerge from current types
[00:07:03.220 --> 00:07:05.540]   of architectures that we think of as deep learning.
[00:07:05.540 --> 00:07:07.940]   - Or if you think of consciousness
[00:07:07.940 --> 00:07:12.180]   in terms of self-awareness and just making plans
[00:07:12.180 --> 00:07:16.580]   based on the fact that the agent itself exists in the world,
[00:07:16.580 --> 00:07:18.000]   reinforcement learning algorithms
[00:07:18.000 --> 00:07:20.140]   are already more or less forced
[00:07:20.140 --> 00:07:23.060]   to model the agent's effect on the environment.
[00:07:23.060 --> 00:07:26.340]   So that more limited version of consciousness
[00:07:26.340 --> 00:07:31.340]   is already something that we get limited versions of
[00:07:31.340 --> 00:07:32.980]   with reinforcement learning algorithms
[00:07:32.980 --> 00:07:34.660]   if they're trained well.
[00:07:34.660 --> 00:07:37.420]   - But you say limited.
[00:07:37.420 --> 00:07:39.900]   So the big question really is how you jump
[00:07:39.900 --> 00:07:42.100]   from limited to human level, right?
[00:07:42.100 --> 00:07:44.620]   And whether it's possible.
[00:07:44.620 --> 00:07:49.020]   Even just building common sense reasoning
[00:07:49.020 --> 00:07:50.540]   seems to be exceptionally difficult.
[00:07:50.540 --> 00:07:52.500]   So if we scale things up,
[00:07:52.500 --> 00:07:55.020]   if we get much better on supervised learning,
[00:07:55.020 --> 00:07:56.620]   if we get better at labeling,
[00:07:56.620 --> 00:08:00.620]   if we get bigger data sets, more compute,
[00:08:00.620 --> 00:08:03.860]   do you think we'll start to see really impressive things
[00:08:03.860 --> 00:08:08.340]   that go from limited to something,
[00:08:08.340 --> 00:08:10.340]   echoes of human level cognition?
[00:08:10.340 --> 00:08:11.180]   - I think so, yeah.
[00:08:11.180 --> 00:08:13.340]   I'm optimistic about what can happen
[00:08:13.340 --> 00:08:16.420]   just with more computation and more data.
[00:08:16.420 --> 00:08:20.100]   I do think it'll be important to get the right kind of data.
[00:08:20.100 --> 00:08:23.140]   Today, most of the machine learning systems we train
[00:08:23.140 --> 00:08:27.540]   are mostly trained on one type of data for each model.
[00:08:27.540 --> 00:08:31.380]   But the human brain, we get all of our different senses
[00:08:31.380 --> 00:08:33.860]   and we have many different experiences
[00:08:33.860 --> 00:08:36.300]   like riding a bike, driving a car,
[00:08:36.300 --> 00:08:37.940]   talking to people, reading.
[00:08:37.940 --> 00:08:42.420]   I think when you get that kind of integrated data set
[00:08:42.420 --> 00:08:44.420]   working with a machine learning model
[00:08:44.420 --> 00:08:47.660]   that can actually close the loop and interact,
[00:08:47.660 --> 00:08:50.460]   we may find that algorithms not so different
[00:08:50.460 --> 00:08:53.260]   from what we have today learn really interesting things
[00:08:53.260 --> 00:08:54.380]   when you scale them up a lot
[00:08:54.380 --> 00:08:58.220]   and train them on a large amount of multimodal data.
[00:08:58.220 --> 00:08:59.620]   - So multimodal is really interesting,
[00:08:59.620 --> 00:09:04.020]   but within, like you're working adversarial examples,
[00:09:04.020 --> 00:09:09.020]   so selecting within modal, within one mode of data,
[00:09:09.020 --> 00:09:13.780]   selecting better what are the difficult cases
[00:09:13.780 --> 00:09:16.140]   from which you're most useful to learn from?
[00:09:16.140 --> 00:09:18.860]   - Oh yeah, like could we get a whole lot of mileage
[00:09:18.860 --> 00:09:22.260]   out of designing a model that's resistant
[00:09:22.260 --> 00:09:24.100]   to adversarial examples or something like that?
[00:09:24.100 --> 00:09:26.260]   - Right, that's a question.
[00:09:26.260 --> 00:09:27.740]   - My thinking on that has evolved a lot
[00:09:27.740 --> 00:09:28.900]   over the last few years.
[00:09:28.900 --> 00:09:29.940]   - Oh, interesting.
[00:09:29.940 --> 00:09:31.260]   - When I first started to really invest
[00:09:31.260 --> 00:09:32.740]   in studying adversarial examples,
[00:09:32.740 --> 00:09:36.340]   I was thinking of it mostly as adversarial examples
[00:09:36.340 --> 00:09:38.980]   reveal a big problem with machine learning
[00:09:38.980 --> 00:09:41.180]   and we would like to close the gap
[00:09:41.180 --> 00:09:43.700]   between how machine learning models
[00:09:43.700 --> 00:09:46.540]   respond to adversarial examples and how humans respond.
[00:09:46.540 --> 00:09:49.180]   After studying the problem more,
[00:09:49.180 --> 00:09:51.940]   I still think that adversarial examples are important.
[00:09:51.940 --> 00:09:55.420]   I think of them now more of as a security liability
[00:09:55.420 --> 00:09:57.780]   than as an issue that necessarily shows
[00:09:57.780 --> 00:10:01.260]   there's something uniquely wrong with machine learning
[00:10:01.260 --> 00:10:02.820]   as opposed to humans.
[00:10:02.820 --> 00:10:04.620]   - Also, do you see them as a tool
[00:10:04.620 --> 00:10:06.460]   to improve the performance of the system?
[00:10:06.460 --> 00:10:10.780]   Not on the security side, but literally just accuracy.
[00:10:10.780 --> 00:10:13.460]   - I do see them as a kind of tool on that side,
[00:10:13.460 --> 00:10:16.660]   but maybe not quite as much as I used to think.
[00:10:16.660 --> 00:10:18.500]   We've started to find that there's a trade-off
[00:10:18.500 --> 00:10:21.660]   between accuracy on adversarial examples
[00:10:21.660 --> 00:10:23.580]   and accuracy on clean examples.
[00:10:24.380 --> 00:10:27.140]   Back in 2014, when I did the first
[00:10:27.140 --> 00:10:29.060]   adversarially trained classifier
[00:10:29.060 --> 00:10:33.020]   that showed resistance to some kinds of adversarial examples,
[00:10:33.020 --> 00:10:36.020]   it also got better at the clean data on MNIST.
[00:10:36.020 --> 00:10:37.100]   And that's something we've replicated
[00:10:37.100 --> 00:10:39.020]   several times on MNIST,
[00:10:39.020 --> 00:10:41.500]   that when we train against weak adversarial examples,
[00:10:41.500 --> 00:10:43.900]   MNIST classifiers get more accurate.
[00:10:43.900 --> 00:10:47.100]   So far, that hasn't really held up on other data sets
[00:10:47.100 --> 00:10:48.860]   and hasn't held up when we train
[00:10:48.860 --> 00:10:50.740]   against stronger adversaries.
[00:10:50.740 --> 00:10:53.180]   It seems like when you confront
[00:10:53.180 --> 00:10:55.740]   a really strong adversary,
[00:10:55.740 --> 00:10:58.100]   you tend to have to give something up.
[00:10:58.100 --> 00:10:59.060]   - Interesting.
[00:10:59.060 --> 00:11:00.540]   But it's such a compelling idea,
[00:11:00.540 --> 00:11:04.740]   'cause it feels like that's how us humans learn
[00:11:04.740 --> 00:11:06.340]   as to the difficult cases.
[00:11:06.340 --> 00:11:08.820]   - We try to think of what would we screw up,
[00:11:08.820 --> 00:11:11.020]   and then we make sure we fix that.
[00:11:11.020 --> 00:11:13.700]   It's also, in a lot of branches of engineering,
[00:11:13.700 --> 00:11:15.820]   you do a worst-case analysis
[00:11:15.820 --> 00:11:18.740]   and make sure that your system will work in the worst case.
[00:11:18.740 --> 00:11:20.420]   And then that guarantees that it'll work
[00:11:20.420 --> 00:11:23.580]   in all of the messy average cases
[00:11:23.580 --> 00:11:27.420]   that happen when you go out into a really randomized world.
[00:11:27.420 --> 00:11:29.540]   - Yeah, with driving with autonomous vehicles,
[00:11:29.540 --> 00:11:33.060]   there seems to be a desire to just look for,
[00:11:33.060 --> 00:11:34.860]   think adversarially,
[00:11:34.860 --> 00:11:36.900]   try to figure out how to mess up the system.
[00:11:36.900 --> 00:11:40.620]   And if you can be robust to all those difficult cases,
[00:11:40.620 --> 00:11:42.900]   then you can, it's a hand-wavy,
[00:11:42.900 --> 00:11:45.820]   empirical way to show your system is safe.
[00:11:45.820 --> 00:11:47.020]   - Yeah, yeah.
[00:11:47.020 --> 00:11:49.100]   Today, most adversarial example research
[00:11:49.100 --> 00:11:51.620]   isn't really focused on a particular use case,
[00:11:51.620 --> 00:11:54.020]   but there are a lot of different use cases
[00:11:54.020 --> 00:11:56.940]   where you'd like to make sure that the adversary
[00:11:56.940 --> 00:12:00.220]   can't interfere with the operation of your system.
[00:12:00.220 --> 00:12:01.060]   Like in finance,
[00:12:01.060 --> 00:12:03.300]   if you have an algorithm making trades for you,
[00:12:03.300 --> 00:12:04.660]   people go to a lot of an effort
[00:12:04.660 --> 00:12:06.660]   to obfuscate their algorithm.
[00:12:06.660 --> 00:12:08.060]   That's both to protect their IP,
[00:12:08.060 --> 00:12:10.860]   because you don't wanna research
[00:12:10.860 --> 00:12:13.580]   and develop a profitable trading algorithm
[00:12:13.580 --> 00:12:16.100]   then have somebody else capture the gains.
[00:12:16.100 --> 00:12:17.140]   But it's at least partly
[00:12:17.140 --> 00:12:19.500]   because you don't want people to make adversarial examples
[00:12:19.500 --> 00:12:22.580]   that fool your algorithm into making bad trades.
[00:12:22.580 --> 00:12:26.580]   Or I guess one area that's been popular
[00:12:26.580 --> 00:12:30.180]   in the academic literature is speech recognition.
[00:12:30.180 --> 00:12:34.420]   If you use speech recognition to hear an audio waveform
[00:12:34.420 --> 00:12:37.700]   and then turn that into a command
[00:12:37.700 --> 00:12:39.660]   that a phone executes for you,
[00:12:39.660 --> 00:12:41.900]   you don't want a malicious adversary
[00:12:41.900 --> 00:12:43.620]   to be able to produce audio
[00:12:43.620 --> 00:12:46.300]   that gets interpreted as malicious commands,
[00:12:46.300 --> 00:12:47.820]   especially if a human in the room
[00:12:47.820 --> 00:12:50.300]   doesn't realize that something like that is happening.
[00:12:50.300 --> 00:12:52.020]   - In speech recognition,
[00:12:52.020 --> 00:12:53.900]   has there been much success
[00:12:53.900 --> 00:12:58.460]   in being able to create adversarial examples
[00:12:58.460 --> 00:12:59.780]   that fool the system?
[00:12:59.780 --> 00:13:00.860]   - Yeah, actually.
[00:13:00.860 --> 00:13:02.420]   I guess the first work that I'm aware of
[00:13:02.420 --> 00:13:05.140]   is a paper called "Hidden Voice Commands"
[00:13:05.140 --> 00:13:08.460]   that came out in 2016, I believe.
[00:13:08.460 --> 00:13:10.780]   And they were able to show that
[00:13:10.780 --> 00:13:13.780]   they could make sounds that are not understandable
[00:13:13.780 --> 00:13:18.420]   by a human, but are recognized as the target phrase
[00:13:18.420 --> 00:13:21.340]   that the attacker wants the phone to recognize it as.
[00:13:21.340 --> 00:13:24.020]   Since then, things have gotten a little bit better
[00:13:24.020 --> 00:13:27.580]   on the attacker side and worse on the defender side.
[00:13:27.580 --> 00:13:33.380]   It's become possible to make sounds
[00:13:33.380 --> 00:13:35.580]   that sound like normal speech,
[00:13:35.580 --> 00:13:38.980]   but are actually interpreted as a different sentence
[00:13:38.980 --> 00:13:40.700]   than the human hears.
[00:13:40.700 --> 00:13:42.740]   The level of perceptibility
[00:13:42.740 --> 00:13:45.420]   of the adversarial perturbation is still kind of high.
[00:13:45.420 --> 00:13:48.180]   When you listen to the recording,
[00:13:48.180 --> 00:13:51.020]   it sounds like there's some noise in the background,
[00:13:51.020 --> 00:13:52.940]   just like rustling sounds.
[00:13:52.940 --> 00:13:53.940]   But those rustling sounds
[00:13:53.940 --> 00:13:55.540]   are actually the adversarial perturbation
[00:13:55.540 --> 00:13:58.020]   that makes the phone hear a completely different sentence.
[00:13:58.020 --> 00:14:00.100]   - Yeah, that's so fascinating.
[00:14:00.100 --> 00:14:01.620]   Peter Norvig mentioned that you're writing
[00:14:01.620 --> 00:14:04.260]   the deep learning chapter for the fourth edition
[00:14:04.260 --> 00:14:07.340]   of the "Artificial Intelligence, a Modern Approach" book.
[00:14:07.340 --> 00:14:10.700]   So how do you even begin summarizing
[00:14:10.700 --> 00:14:12.700]   the field of deep learning in a chapter?
[00:14:12.700 --> 00:14:13.940]   (Peter laughs)
[00:14:13.940 --> 00:14:16.900]   - Well, in my case, I waited like a year
[00:14:16.900 --> 00:14:19.180]   before I actually wrote anything.
[00:14:19.180 --> 00:14:22.660]   Is it, even having written a full length textbook before,
[00:14:22.660 --> 00:14:26.820]   it's still pretty intimidating to try to start writing
[00:14:26.820 --> 00:14:29.100]   just one chapter that covers everything.
[00:14:29.100 --> 00:14:33.220]   One thing that helped me make that plan
[00:14:33.220 --> 00:14:34.340]   was actually the experience
[00:14:34.340 --> 00:14:36.740]   of having written the full book before
[00:14:36.740 --> 00:14:39.140]   and then watching how the field changed
[00:14:39.140 --> 00:14:40.940]   after the book came out.
[00:14:40.940 --> 00:14:42.300]   I've realized there's a lot of topics
[00:14:42.300 --> 00:14:45.020]   that were maybe extraneous in the first book.
[00:14:45.020 --> 00:14:47.580]   And just seeing what stood the test
[00:14:47.580 --> 00:14:49.420]   of a few years of being published
[00:14:49.420 --> 00:14:52.740]   and what seems a little bit less important to have included
[00:14:52.740 --> 00:14:54.260]   now helped me pare down the topics
[00:14:54.260 --> 00:14:55.820]   I wanted to cover for the book.
[00:14:55.820 --> 00:14:59.260]   It's also really nice now that
[00:14:59.260 --> 00:15:00.580]   the field has kind of stabilized
[00:15:00.580 --> 00:15:02.820]   to the point where some core ideas from the 1980s
[00:15:02.820 --> 00:15:04.780]   are still used today.
[00:15:04.780 --> 00:15:06.660]   When I first started studying machine learning,
[00:15:06.660 --> 00:15:09.580]   almost everything from the 1980s had been rejected
[00:15:09.580 --> 00:15:11.340]   and now some of it has come back.
[00:15:11.340 --> 00:15:13.460]   So that stuff that's really stood the test of time
[00:15:13.460 --> 00:15:15.940]   is what I focused on putting into the book.
[00:15:15.940 --> 00:15:21.300]   There's also, I guess, two different philosophies
[00:15:21.300 --> 00:15:23.140]   about how you might write a book.
[00:15:23.140 --> 00:15:24.820]   One philosophy is you try to write a reference
[00:15:24.820 --> 00:15:26.220]   that covers everything.
[00:15:26.220 --> 00:15:28.020]   The other philosophy is you try to provide
[00:15:28.020 --> 00:15:30.380]   a high level summary that gives people
[00:15:30.380 --> 00:15:32.420]   the language to understand a field
[00:15:32.420 --> 00:15:34.980]   and tells them what the most important concepts are.
[00:15:34.980 --> 00:15:37.060]   The first deep learning book that I wrote
[00:15:37.060 --> 00:15:39.620]   with Joshua and Aaron was somewhere between
[00:15:39.620 --> 00:15:42.380]   the two philosophies, that it's trying to be
[00:15:42.380 --> 00:15:45.780]   both a reference and an introductory guide.
[00:15:45.780 --> 00:15:48.940]   Writing this chapter for Russell and Norvig's book,
[00:15:48.940 --> 00:15:52.780]   I was able to focus more on just a concise introduction
[00:15:52.780 --> 00:15:54.260]   of the key concepts and the language
[00:15:54.260 --> 00:15:55.980]   you need to read about them more.
[00:15:55.980 --> 00:15:57.540]   In a lot of cases, I actually just wrote paragraphs
[00:15:57.540 --> 00:16:00.020]   that said, "Here's a rapidly evolving area
[00:16:00.020 --> 00:16:01.900]   "that you should pay attention to.
[00:16:01.900 --> 00:16:04.660]   "It's pointless to try to tell you what the latest
[00:16:04.660 --> 00:16:09.660]   "and best version of a learn-to-learn model is."
[00:16:09.660 --> 00:16:13.660]   I can point you to a paper that's recent right now,
[00:16:13.660 --> 00:16:16.300]   but there isn't a whole lot of a reason
[00:16:16.300 --> 00:16:18.620]   to delve into exactly what's going on
[00:16:18.620 --> 00:16:21.620]   with the latest learning-to-learn approach
[00:16:21.620 --> 00:16:23.420]   or the latest module produced
[00:16:23.420 --> 00:16:24.980]   by a learning-to-learn algorithm.
[00:16:24.980 --> 00:16:26.780]   You should know that learning-to-learn is a thing
[00:16:26.780 --> 00:16:29.500]   and that it may very well be the source
[00:16:29.500 --> 00:16:32.220]   of the latest and greatest convolutional net
[00:16:32.220 --> 00:16:34.540]   or recurrent net module that you would want to use
[00:16:34.540 --> 00:16:36.060]   in your latest project.
[00:16:36.060 --> 00:16:38.180]   But there isn't a lot of point in trying to summarize
[00:16:38.180 --> 00:16:42.300]   exactly which architecture and which learning approach
[00:16:42.300 --> 00:16:44.060]   got to which level of performance.
[00:16:44.060 --> 00:16:48.020]   - So you maybe focused more on the basics
[00:16:48.020 --> 00:16:51.300]   of the methodology, so from backpropagation
[00:16:51.300 --> 00:16:53.740]   to feedforward to recurrent neural networks,
[00:16:53.740 --> 00:16:55.180]   convolutional, that kind of thing?
[00:16:55.180 --> 00:16:56.500]   - Yeah, yeah.
[00:16:56.500 --> 00:16:58.700]   - So if I were to ask you, I remember I took
[00:16:58.700 --> 00:17:03.740]   algorithms and data structures algorithms course.
[00:17:03.740 --> 00:17:08.220]   I remember the professor asked, "What is an algorithm?"
[00:17:08.220 --> 00:17:12.240]   And yelled at everybody in a good way
[00:17:12.240 --> 00:17:14.100]   that nobody was answering it correctly.
[00:17:14.100 --> 00:17:16.420]   Everybody knew what the algorithm, it was a graduate course.
[00:17:16.420 --> 00:17:18.180]   Everybody knew what an algorithm was,
[00:17:18.180 --> 00:17:19.820]   but they weren't able to answer it well.
[00:17:19.820 --> 00:17:22.380]   So let me ask you in that same spirit,
[00:17:22.380 --> 00:17:23.620]   what is deep learning?
[00:17:23.620 --> 00:17:29.580]   - I would say deep learning is any kind of machine learning
[00:17:29.740 --> 00:17:32.500]   that involves learning parameters
[00:17:32.500 --> 00:17:36.020]   of more than one consecutive step.
[00:17:36.020 --> 00:17:39.620]   So that would mean shallow learning is things
[00:17:39.620 --> 00:17:43.780]   where you learn a lot of operations that happen in parallel.
[00:17:43.780 --> 00:17:46.740]   You might have a system that makes multiple steps,
[00:17:46.740 --> 00:17:51.020]   like you might have hand-designed feature extractors,
[00:17:51.020 --> 00:17:52.660]   but really only one step is learned.
[00:17:52.660 --> 00:17:56.060]   Deep learning is anything where you have multiple operations
[00:17:56.060 --> 00:17:58.580]   in sequence, and that includes the things
[00:17:58.580 --> 00:17:59.820]   that are really popular today,
[00:17:59.820 --> 00:18:03.620]   like convolutional networks and recurrent networks,
[00:18:03.620 --> 00:18:05.060]   but it also includes some of the things
[00:18:05.060 --> 00:18:08.300]   that have died out, like Bolton machines,
[00:18:08.300 --> 00:18:10.900]   where we weren't using back propagation.
[00:18:10.900 --> 00:18:14.260]   Today I hear a lot of people define deep learning
[00:18:14.260 --> 00:18:19.060]   as gradient descent applied to
[00:18:19.060 --> 00:18:21.500]   these differentiable functions.
[00:18:21.500 --> 00:18:24.820]   And I think that's a legitimate usage of the term.
[00:18:24.820 --> 00:18:25.940]   It's just different from the way
[00:18:25.940 --> 00:18:27.860]   that I use the term myself.
[00:18:27.860 --> 00:18:31.780]   - So what's an example of deep learning
[00:18:31.780 --> 00:18:34.780]   that is not gradient descent and differentiable functions?
[00:18:34.780 --> 00:18:37.460]   In your, I mean, not specifically perhaps,
[00:18:37.460 --> 00:18:39.820]   but more even looking into the future,
[00:18:39.820 --> 00:18:44.340]   what's your thought about that space of approaches?
[00:18:44.340 --> 00:18:46.380]   - Yeah, so I tend to think of machine learning algorithms
[00:18:46.380 --> 00:18:50.220]   as decomposed into really three different pieces.
[00:18:50.220 --> 00:18:53.020]   There's the model, which can be something like a neural net
[00:18:53.020 --> 00:18:56.620]   or a Bolton machine or a recurrent model.
[00:18:56.620 --> 00:18:59.500]   And that basically just describes how do you take data
[00:18:59.500 --> 00:19:01.140]   and how do you take parameters?
[00:19:01.140 --> 00:19:04.300]   And what function do you use to make a prediction
[00:19:04.300 --> 00:19:07.340]   given the data and the parameters?
[00:19:07.340 --> 00:19:09.260]   Another piece of the learning algorithm
[00:19:09.260 --> 00:19:12.380]   is the optimization algorithm,
[00:19:12.380 --> 00:19:14.900]   or not every algorithm can be really described
[00:19:14.900 --> 00:19:15.900]   in terms of optimization,
[00:19:15.900 --> 00:19:18.860]   but what's the algorithm for updating the parameters
[00:19:18.860 --> 00:19:21.660]   or updating whatever the state of the network is?
[00:19:21.660 --> 00:19:26.260]   And then the last part is the data set,
[00:19:26.260 --> 00:19:29.180]   like how do you actually represent the world
[00:19:29.180 --> 00:19:32.100]   as it comes into your machine learning system?
[00:19:32.100 --> 00:19:35.780]   So I think of deep learning as telling us something
[00:19:35.780 --> 00:19:39.060]   about what does the model look like?
[00:19:39.060 --> 00:19:41.260]   And basically to qualify as deep,
[00:19:41.260 --> 00:19:44.540]   I say that it just has to have multiple layers.
[00:19:44.540 --> 00:19:46.340]   That can be multiple steps
[00:19:46.340 --> 00:19:49.220]   in a feed-forward differentiable computation.
[00:19:49.220 --> 00:19:52.020]   That can be multiple layers in a graphical model.
[00:19:52.020 --> 00:19:53.540]   There's a lot of ways that you could satisfy me
[00:19:53.540 --> 00:19:56.140]   that something has multiple steps
[00:19:56.140 --> 00:19:58.100]   that are each parameterized separately.
[00:19:58.100 --> 00:19:59.940]   I think of gradient descent
[00:19:59.940 --> 00:20:01.900]   as being all about that other piece,
[00:20:01.900 --> 00:20:04.260]   how do you actually update the parameters piece?
[00:20:04.260 --> 00:20:05.980]   So you could imagine having a deep model
[00:20:05.980 --> 00:20:07.540]   like a convolutional net
[00:20:07.540 --> 00:20:09.660]   and training it with something like evolution
[00:20:09.660 --> 00:20:11.300]   or a genetic algorithm.
[00:20:11.300 --> 00:20:14.780]   And I would say that still qualifies as deep learning.
[00:20:14.780 --> 00:20:16.060]   And then in terms of models
[00:20:16.060 --> 00:20:18.740]   that aren't necessarily differentiable,
[00:20:18.740 --> 00:20:21.260]   I guess Boltzmann machines are probably
[00:20:21.260 --> 00:20:23.580]   the main example of something
[00:20:23.580 --> 00:20:25.540]   where you can't really take a derivative
[00:20:25.540 --> 00:20:28.020]   and use that for the learning process.
[00:20:28.020 --> 00:20:30.820]   But you can still argue that the model
[00:20:30.820 --> 00:20:33.780]   has many steps of processing that it applies
[00:20:33.780 --> 00:20:35.820]   when you run inference in the model.
[00:20:35.820 --> 00:20:38.980]   - So it's the steps of processing that's key.
[00:20:38.980 --> 00:20:41.380]   So Jeff Hinton suggests that we need to throw away
[00:20:41.380 --> 00:20:44.940]   back propagation and start all over.
[00:20:44.940 --> 00:20:46.540]   What do you think about that?
[00:20:46.540 --> 00:20:48.620]   What could an alternative direction
[00:20:48.620 --> 00:20:50.980]   of training neural networks look like?
[00:20:50.980 --> 00:20:52.900]   - I don't know that back propagation
[00:20:52.900 --> 00:20:54.700]   is gonna go away entirely.
[00:20:54.700 --> 00:20:57.140]   Most of the time when we decide
[00:20:57.140 --> 00:20:59.220]   that a machine learning algorithm
[00:20:59.220 --> 00:21:03.460]   isn't on the critical path to research for improving AI,
[00:21:03.460 --> 00:21:04.660]   the algorithm doesn't die.
[00:21:04.660 --> 00:21:07.740]   It just becomes used for some specialized set of things.
[00:21:07.740 --> 00:21:11.220]   A lot of algorithms like logistic regression
[00:21:11.220 --> 00:21:14.020]   don't seem that exciting to AI researchers
[00:21:14.020 --> 00:21:16.780]   who are working on things like speech recognition
[00:21:16.780 --> 00:21:18.460]   or autonomous cars today.
[00:21:18.460 --> 00:21:21.140]   But there's still a lot of use for logistic regression
[00:21:21.140 --> 00:21:24.060]   and things like analyzing really noisy data
[00:21:24.060 --> 00:21:25.740]   in medicine and finance,
[00:21:25.740 --> 00:21:28.820]   or making really rapid predictions
[00:21:28.820 --> 00:21:30.740]   in really time-limited contexts.
[00:21:30.740 --> 00:21:33.500]   So I think back propagation and gradient descent
[00:21:33.500 --> 00:21:37.500]   are around to stay, but they may not end up being
[00:21:37.500 --> 00:21:40.900]   everything that we need to get to real human level
[00:21:40.900 --> 00:21:42.420]   or superhuman AI.
[00:21:42.420 --> 00:21:44.780]   - Are you optimistic about us discovering,
[00:21:44.780 --> 00:21:50.260]   back propagation has been around for a few decades.
[00:21:50.260 --> 00:21:54.100]   So are you optimistic about us as a community
[00:21:54.100 --> 00:21:56.820]   being able to discover something better?
[00:21:56.820 --> 00:21:57.660]   - Yeah, I am.
[00:21:57.660 --> 00:22:01.820]   I think we likely will find something that works better.
[00:22:01.820 --> 00:22:05.500]   You could imagine things like having stacks of models
[00:22:05.500 --> 00:22:07.580]   where some of the lower level models
[00:22:07.580 --> 00:22:10.220]   predict parameters of the higher level models.
[00:22:10.220 --> 00:22:12.180]   And so at the top level,
[00:22:12.180 --> 00:22:13.500]   you're not learning in terms of literally
[00:22:13.500 --> 00:22:14.460]   calculating gradients,
[00:22:14.460 --> 00:22:17.700]   but just predicting how different values will perform.
[00:22:17.700 --> 00:22:19.580]   You can kind of see that already in some areas
[00:22:19.580 --> 00:22:21.380]   like Bayesian optimization,
[00:22:21.380 --> 00:22:22.940]   where you have a Gaussian process
[00:22:22.940 --> 00:22:24.180]   that predicts how well different
[00:22:24.180 --> 00:22:25.900]   parameter values will perform.
[00:22:25.900 --> 00:22:27.700]   We already use those kinds of algorithms
[00:22:27.700 --> 00:22:30.260]   for things like hyper parameter optimization.
[00:22:30.260 --> 00:22:31.660]   And in general, we know a lot of things
[00:22:31.660 --> 00:22:33.260]   other than back prop that work really well
[00:22:33.260 --> 00:22:34.980]   for specific problems.
[00:22:34.980 --> 00:22:37.460]   The main thing we haven't found is
[00:22:37.460 --> 00:22:38.900]   a way of taking one of these other
[00:22:38.900 --> 00:22:41.180]   non-back prop based algorithms
[00:22:41.180 --> 00:22:43.500]   and having it really advance the state of the art
[00:22:43.500 --> 00:22:46.140]   on an AI level problem.
[00:22:46.140 --> 00:22:47.100]   - Right.
[00:22:47.100 --> 00:22:49.180]   - But I wouldn't be surprised if eventually
[00:22:49.180 --> 00:22:51.580]   we find that some of these algorithms that,
[00:22:51.580 --> 00:22:52.820]   even the ones that already exist,
[00:22:52.820 --> 00:22:54.260]   not even necessarily a new one,
[00:22:54.260 --> 00:22:58.220]   we might find some way of customizing
[00:22:58.220 --> 00:22:59.820]   one of these algorithms to do something
[00:22:59.820 --> 00:23:02.540]   really interesting at the level of cognition
[00:23:02.540 --> 00:23:05.300]   or the level of,
[00:23:05.300 --> 00:23:08.460]   I think one system that we really don't have
[00:23:08.460 --> 00:23:12.140]   working quite right yet is like short-term memory.
[00:23:12.140 --> 00:23:14.540]   We have things like LSTMs,
[00:23:14.540 --> 00:23:17.060]   they're called long short-term memory.
[00:23:17.060 --> 00:23:20.060]   They still don't do quite what a human does
[00:23:20.060 --> 00:23:21.820]   with short-term memory.
[00:23:21.820 --> 00:23:26.980]   Like gradient descent to learn a specific fact
[00:23:26.980 --> 00:23:29.420]   has to do multiple steps on that fact.
[00:23:29.420 --> 00:23:34.180]   Like if I tell you the meeting today is at 3 p.m.,
[00:23:34.180 --> 00:23:35.500]   I don't need to say over and over again,
[00:23:35.500 --> 00:23:37.820]   it's at 3 p.m., it's at 3 p.m., it's at 3 p.m.,
[00:23:37.820 --> 00:23:40.420]   it's at 3 p.m. for you to do a gradient step on each one.
[00:23:40.420 --> 00:23:43.220]   You just hear it once and you remember it.
[00:23:43.220 --> 00:23:45.140]   There's been some work on things like
[00:23:46.060 --> 00:23:48.340]   self-attention and attention-like mechanisms
[00:23:48.340 --> 00:23:50.420]   like the neural Turing machine
[00:23:50.420 --> 00:23:52.220]   that can write to memory cells
[00:23:52.220 --> 00:23:54.900]   and update themselves with facts like that right away.
[00:23:54.900 --> 00:23:56.900]   But I don't think we've really nailed it yet.
[00:23:56.900 --> 00:23:59.580]   And that's one area where I'd imagine
[00:23:59.580 --> 00:24:02.660]   that new optimization algorithms
[00:24:02.660 --> 00:24:03.820]   or different ways of applying
[00:24:03.820 --> 00:24:06.020]   existing optimization algorithms
[00:24:06.020 --> 00:24:08.820]   could give us a way of just lightning fast
[00:24:08.820 --> 00:24:11.180]   updating the state of a machine learning system
[00:24:11.180 --> 00:24:14.100]   to contain a specific fact like that
[00:24:14.100 --> 00:24:15.340]   without needing to have it presented
[00:24:15.340 --> 00:24:16.980]   over and over and over again.
[00:24:16.980 --> 00:24:21.420]   - So some of the success of symbolic systems in the '80s
[00:24:21.420 --> 00:24:26.220]   is they were able to assemble these kinds of facts better.
[00:24:26.220 --> 00:24:29.100]   But there's a lot of expert input required
[00:24:29.100 --> 00:24:31.140]   and it's very limited in that sense.
[00:24:31.140 --> 00:24:33.700]   Do you ever look back to that
[00:24:33.700 --> 00:24:36.580]   as something that we'll have to return to eventually,
[00:24:36.580 --> 00:24:38.420]   sort of dust off the book from the shelf
[00:24:38.420 --> 00:24:41.340]   and think about how we build knowledge,
[00:24:41.340 --> 00:24:42.940]   representation, knowledge--
[00:24:42.940 --> 00:24:44.820]   - Like will we have to use graph searches?
[00:24:44.820 --> 00:24:45.780]   - Graph searches, right.
[00:24:45.780 --> 00:24:47.700]   - And like first order logic and entailment
[00:24:47.700 --> 00:24:48.540]   and things like that.
[00:24:48.540 --> 00:24:49.580]   - That kind of thing, yeah, exactly.
[00:24:49.580 --> 00:24:51.180]   - In my particular line of work,
[00:24:51.180 --> 00:24:54.540]   which has mostly been machine learning security
[00:24:54.540 --> 00:24:56.740]   and also generative modeling,
[00:24:56.740 --> 00:25:00.540]   I haven't usually found myself moving in that direction.
[00:25:00.540 --> 00:25:03.500]   For generative models, I could see a little bit of,
[00:25:03.500 --> 00:25:05.180]   it could be useful if you had something like
[00:25:05.180 --> 00:25:09.660]   a differentiable knowledge base
[00:25:09.660 --> 00:25:10.980]   or some other kind of knowledge base
[00:25:10.980 --> 00:25:13.140]   where it's possible for some of our
[00:25:13.140 --> 00:25:14.820]   fuzzier machine learning algorithms
[00:25:14.820 --> 00:25:16.860]   to interact with the knowledge base.
[00:25:16.860 --> 00:25:19.020]   - I mean, neural network is kind of like that.
[00:25:19.020 --> 00:25:21.420]   It's a differentiable knowledge base of sorts.
[00:25:21.420 --> 00:25:22.260]   - Yeah.
[00:25:22.260 --> 00:25:23.620]   - But--
[00:25:23.620 --> 00:25:27.620]   - If we had a really easy way of giving feedback
[00:25:27.620 --> 00:25:29.260]   to machine learning models,
[00:25:29.260 --> 00:25:32.380]   that would clearly help a lot with generative models.
[00:25:32.380 --> 00:25:33.900]   And so you could imagine one way of getting there
[00:25:33.900 --> 00:25:36.700]   would be get a lot better at natural language processing.
[00:25:36.700 --> 00:25:38.900]   But another way of getting there would be
[00:25:38.900 --> 00:25:40.260]   take some kind of knowledge base
[00:25:40.260 --> 00:25:42.300]   and figure out a way for it to actually
[00:25:42.300 --> 00:25:44.060]   interact with a neural network.
[00:25:44.060 --> 00:25:46.060]   - Being able to have a chat with a neural network.
[00:25:46.060 --> 00:25:46.900]   - Yeah.
[00:25:46.900 --> 00:25:47.860]   (laughing)
[00:25:47.860 --> 00:25:49.980]   So like one thing in generative models we see a lot today
[00:25:49.980 --> 00:25:53.540]   is you'll get things like faces that are not symmetrical,
[00:25:53.540 --> 00:25:58.180]   like people that have two eyes that are different colors.
[00:25:58.180 --> 00:25:59.540]   And I mean, there are people with eyes
[00:25:59.540 --> 00:26:00.820]   that are different colors in real life,
[00:26:00.820 --> 00:26:03.420]   but not nearly as many of them as you tend to see
[00:26:03.420 --> 00:26:06.060]   in the machine learning generated data.
[00:26:06.060 --> 00:26:08.060]   So if you had either a knowledge base
[00:26:08.060 --> 00:26:10.140]   that could contain the fact,
[00:26:10.180 --> 00:26:13.340]   people's faces are generally approximately symmetric
[00:26:13.340 --> 00:26:15.900]   and eye color is especially likely
[00:26:15.900 --> 00:26:17.940]   to be the same on both sides.
[00:26:17.940 --> 00:26:20.180]   Being able to just inject that hint
[00:26:20.180 --> 00:26:22.020]   into the machine learning model
[00:26:22.020 --> 00:26:23.820]   without it having to discover that itself
[00:26:23.820 --> 00:26:25.780]   after studying a lot of data
[00:26:25.780 --> 00:26:28.340]   would be a really useful feature.
[00:26:28.340 --> 00:26:30.140]   I could see a lot of ways of getting there
[00:26:30.140 --> 00:26:32.180]   without bringing back some of the 1980s technology,
[00:26:32.180 --> 00:26:35.140]   but I also see some ways that you could imagine
[00:26:35.140 --> 00:26:37.460]   extending the 1980s technology to play nice
[00:26:37.460 --> 00:26:40.020]   with neural nets and have it help get there.
[00:26:40.020 --> 00:26:42.580]   - Awesome, so you talked about the story
[00:26:42.580 --> 00:26:45.180]   of you coming up with the idea of GANs
[00:26:45.180 --> 00:26:47.020]   at a bar with some friends.
[00:26:47.020 --> 00:26:49.580]   You were arguing that this, you know,
[00:26:49.580 --> 00:26:53.060]   GANs would work, generative adversarial networks,
[00:26:53.060 --> 00:26:54.660]   and the others didn't think so.
[00:26:54.660 --> 00:26:57.100]   Then you went home at midnight,
[00:26:57.100 --> 00:26:58.420]   coded it up, and it worked.
[00:26:58.420 --> 00:27:01.340]   So if I was a friend of yours at the bar,
[00:27:01.340 --> 00:27:02.700]   I would also have doubts.
[00:27:02.700 --> 00:27:03.860]   It's a really nice idea,
[00:27:03.860 --> 00:27:06.820]   but I'm very skeptical that it would work.
[00:27:06.820 --> 00:27:09.300]   What was the basis of their skepticism?
[00:27:09.300 --> 00:27:13.180]   What was the basis of your intuition why it should work?
[00:27:13.180 --> 00:27:15.980]   - I don't wanna be someone who goes around
[00:27:15.980 --> 00:27:18.300]   promoting alcohol for the purposes of science,
[00:27:18.300 --> 00:27:21.020]   but in this case, I do actually think
[00:27:21.020 --> 00:27:23.060]   that drinking helped a little bit.
[00:27:23.060 --> 00:27:25.360]   When your inhibitions are lowered,
[00:27:25.360 --> 00:27:27.380]   you're more willing to try out things
[00:27:27.380 --> 00:27:29.620]   that you wouldn't try out otherwise.
[00:27:29.620 --> 00:27:32.460]   So I have noticed in general
[00:27:32.460 --> 00:27:34.540]   that I'm less prone to shooting down some of my own ideas
[00:27:34.540 --> 00:27:37.980]   when I have had a little bit to drink.
[00:27:37.980 --> 00:27:40.820]   I think if I had had that idea at lunchtime,
[00:27:40.820 --> 00:27:42.260]   I probably would have thought,
[00:27:42.260 --> 00:27:43.740]   it's hard enough to train one neural net.
[00:27:43.740 --> 00:27:44.900]   You can't train a second neural net
[00:27:44.900 --> 00:27:48.080]   in the inner loop of the outer neural net.
[00:27:48.080 --> 00:27:49.820]   That was basically my friend's objection,
[00:27:49.820 --> 00:27:52.740]   was that trying to train two neural nets at the same time
[00:27:52.740 --> 00:27:54.260]   would be too hard.
[00:27:54.260 --> 00:27:56.140]   - So it was more about the training process,
[00:27:56.140 --> 00:27:58.300]   unless, so my skepticism would be,
[00:27:58.300 --> 00:28:01.140]   you know, I'm sure you could train it,
[00:28:01.140 --> 00:28:03.180]   but the thing it would converge to
[00:28:03.180 --> 00:28:05.820]   would not be able to generate anything reasonable,
[00:28:05.820 --> 00:28:08.260]   any kind of reasonable realism.
[00:28:08.260 --> 00:28:11.360]   - Yeah, so part of what all of us were thinking about
[00:28:11.360 --> 00:28:15.280]   when we had this conversation was deep Bolton machines,
[00:28:15.280 --> 00:28:16.980]   which a lot of us in the lab, including me,
[00:28:16.980 --> 00:28:19.580]   were a big fan of deep Bolton machines at the time.
[00:28:19.580 --> 00:28:22.900]   They involved two separate processes
[00:28:22.900 --> 00:28:24.180]   running at the same time.
[00:28:24.180 --> 00:28:28.140]   One of them is called the positive phase,
[00:28:28.140 --> 00:28:31.180]   where you load data into the model
[00:28:31.180 --> 00:28:33.540]   and tell the model to make the data more likely.
[00:28:33.540 --> 00:28:35.140]   The other one is called the negative phase,
[00:28:35.140 --> 00:28:37.020]   where you draw samples from the model
[00:28:37.020 --> 00:28:39.660]   and tell the model to make those samples less likely.
[00:28:39.660 --> 00:28:42.220]   In a deep Bolton machine,
[00:28:42.220 --> 00:28:43.960]   it's not trivial to generate a sample.
[00:28:43.960 --> 00:28:46.980]   You have to actually run an iterative process
[00:28:46.980 --> 00:28:49.140]   that gets better and better samples
[00:28:49.140 --> 00:28:51.400]   coming closer and closer to the distribution
[00:28:51.400 --> 00:28:52.860]   the model represents.
[00:28:52.860 --> 00:28:53.900]   So during the training process,
[00:28:53.900 --> 00:28:57.180]   you're always running these two systems at the same time.
[00:28:57.180 --> 00:28:58.940]   One that's updating the parameters of the model
[00:28:58.940 --> 00:29:00.500]   and another one that's trying to generate samples
[00:29:00.500 --> 00:29:01.680]   from the model.
[00:29:01.680 --> 00:29:04.340]   And they worked really well on things like MNIST,
[00:29:04.340 --> 00:29:05.820]   but a lot of us in the lab, including me,
[00:29:05.820 --> 00:29:07.500]   had tried to get deep Bolton machines
[00:29:07.500 --> 00:29:11.900]   to scale past MNIST to things like generating color photos.
[00:29:11.900 --> 00:29:14.120]   And we just couldn't get the two processes
[00:29:14.120 --> 00:29:15.940]   to stay synchronized.
[00:29:15.940 --> 00:29:18.740]   So when I had the idea for GANs,
[00:29:18.740 --> 00:29:20.320]   a lot of people thought that the discriminator
[00:29:20.320 --> 00:29:22.580]   would have more or less the same problem
[00:29:22.580 --> 00:29:25.340]   as the negative phase in the Bolton machine.
[00:29:25.340 --> 00:29:27.780]   That trying to train the discriminator in the inner loop,
[00:29:27.780 --> 00:29:29.920]   you just couldn't get it to keep up
[00:29:29.920 --> 00:29:31.540]   with the generator in the outer loop.
[00:29:31.540 --> 00:29:33.820]   And that would prevent it from converging
[00:29:33.820 --> 00:29:35.220]   to anything useful.
[00:29:35.220 --> 00:29:36.860]   - Yeah, I share that intuition.
[00:29:36.860 --> 00:29:37.700]   - Yeah.
[00:29:37.700 --> 00:29:41.940]   - But turns out to not be the case.
[00:29:41.940 --> 00:29:43.760]   - A lot of the time with machine learning algorithms,
[00:29:43.760 --> 00:29:45.160]   it's really hard to predict ahead of time
[00:29:45.160 --> 00:29:46.900]   how well they'll actually perform.
[00:29:46.900 --> 00:29:49.140]   You have to just run the experiment and see what happens.
[00:29:49.140 --> 00:29:53.460]   And I would say I still today don't have one factor
[00:29:53.460 --> 00:29:54.740]   I can put my finger on and say,
[00:29:54.740 --> 00:29:58.300]   "This is why GANs worked for photo generation
[00:29:58.300 --> 00:30:00.300]   "and deep Bolton machines don't."
[00:30:00.300 --> 00:30:03.300]   There are a lot of theory papers
[00:30:03.300 --> 00:30:06.340]   showing that under some theoretical settings,
[00:30:06.340 --> 00:30:09.620]   the GAN algorithm does actually converge.
[00:30:09.620 --> 00:30:14.140]   But those settings are restricted enough
[00:30:14.140 --> 00:30:17.540]   that they don't necessarily explain the whole picture
[00:30:17.540 --> 00:30:20.740]   in terms of all the results that we see in practice.
[00:30:20.740 --> 00:30:22.300]   - So taking a step back,
[00:30:22.300 --> 00:30:24.860]   can you, in the same way as we talked about deep learning,
[00:30:24.860 --> 00:30:28.400]   can you tell me what generative adversarial networks are?
[00:30:28.400 --> 00:30:31.380]   - Yeah, so generative adversarial networks
[00:30:31.380 --> 00:30:33.980]   are a particular kind of generative model.
[00:30:33.980 --> 00:30:36.260]   A generative model is a machine learning model
[00:30:36.260 --> 00:30:38.860]   that can train on some set of data.
[00:30:38.860 --> 00:30:41.220]   Like say you have a collection of photos of cats
[00:30:41.220 --> 00:30:43.980]   and you want to generate more photos of cats,
[00:30:43.980 --> 00:30:47.700]   or you want to estimate a probability distribution over cats
[00:30:47.700 --> 00:30:49.780]   so you can ask how likely it is
[00:30:49.780 --> 00:30:51.820]   that some new image is a photo of a cat.
[00:30:51.820 --> 00:30:55.800]   GANs are one way of doing this.
[00:30:55.800 --> 00:30:59.180]   Some generative models are good at creating new data.
[00:30:59.180 --> 00:31:01.620]   Other generative models are good at estimating
[00:31:01.620 --> 00:31:03.000]   that density function and telling you
[00:31:03.000 --> 00:31:06.580]   how likely particular pieces of data are
[00:31:06.580 --> 00:31:09.700]   to come from the same distribution as the training data.
[00:31:09.700 --> 00:31:12.420]   GANs are more focused on generating samples
[00:31:12.420 --> 00:31:15.620]   rather than estimating the density function.
[00:31:15.620 --> 00:31:18.500]   There are some kinds of GANs like FlowGAN that can do both,
[00:31:18.500 --> 00:31:21.620]   but mostly GANs are about generating samples,
[00:31:21.620 --> 00:31:24.240]   generating new photos of cats that look realistic.
[00:31:25.220 --> 00:31:29.300]   And they do that completely from scratch.
[00:31:29.300 --> 00:31:32.220]   It's analogous to human imagination.
[00:31:32.220 --> 00:31:34.740]   When a GAN creates a new image of a cat,
[00:31:34.740 --> 00:31:39.300]   it's using a neural network to produce a cat
[00:31:39.300 --> 00:31:41.020]   that has not existed before.
[00:31:41.020 --> 00:31:44.540]   It isn't doing something like compositing photos together.
[00:31:44.540 --> 00:31:47.060]   You're not literally taking the eye off of one cat
[00:31:47.060 --> 00:31:48.260]   and the ear off of another cat.
[00:31:48.260 --> 00:31:51.340]   It's more of this digestive process
[00:31:51.340 --> 00:31:53.940]   where the neural net trains on a lot of data
[00:31:53.940 --> 00:31:55.580]   and comes up with some representation
[00:31:55.580 --> 00:31:57.420]   of the probability distribution
[00:31:57.420 --> 00:31:59.820]   and generates entirely new cats.
[00:31:59.820 --> 00:32:00.900]   There are a lot of different ways
[00:32:00.900 --> 00:32:01.980]   of building a generative model.
[00:32:01.980 --> 00:32:05.660]   What's specific to GANs is that we have a two-player game
[00:32:05.660 --> 00:32:08.100]   in the game theoretic sense.
[00:32:08.100 --> 00:32:10.340]   And as the players in this game compete,
[00:32:10.340 --> 00:32:13.940]   one of them becomes able to generate realistic data.
[00:32:13.940 --> 00:32:16.140]   The first player is called the generator.
[00:32:16.140 --> 00:32:20.660]   It produces output data, such as just images, for example.
[00:32:20.660 --> 00:32:22.460]   And at the start of the learning process,
[00:32:22.460 --> 00:32:25.140]   it'll just produce completely random images.
[00:32:25.140 --> 00:32:27.420]   The other player is called the discriminator.
[00:32:27.420 --> 00:32:29.700]   The discriminator takes images as input
[00:32:29.700 --> 00:32:32.540]   and guesses whether they're real or fake.
[00:32:32.540 --> 00:32:34.260]   You train it both on real data,
[00:32:34.260 --> 00:32:36.140]   so photos that come from your training set,
[00:32:36.140 --> 00:32:37.860]   actual photos of cats,
[00:32:37.860 --> 00:32:39.900]   and you train it to say that those are real.
[00:32:39.900 --> 00:32:41.980]   You also train it on images
[00:32:41.980 --> 00:32:43.860]   that come from the generator network
[00:32:43.860 --> 00:32:46.060]   and you train it to say that those are fake.
[00:32:46.060 --> 00:32:49.220]   As the two players compete in this game,
[00:32:49.220 --> 00:32:50.980]   the discriminator tries to become better
[00:32:50.980 --> 00:32:53.340]   at recognizing whether images are real or fake.
[00:32:53.340 --> 00:32:54.820]   And the generator becomes better
[00:32:54.820 --> 00:32:57.020]   at fooling the discriminator into thinking
[00:32:57.020 --> 00:32:59.620]   that its outputs are real.
[00:32:59.620 --> 00:33:03.580]   And you can analyze this through the language of game theory
[00:33:03.580 --> 00:33:06.980]   and find that there's a Nash equilibrium
[00:33:06.980 --> 00:33:08.660]   where the generator has captured
[00:33:08.660 --> 00:33:10.820]   the correct probability distribution.
[00:33:10.820 --> 00:33:12.180]   So in the cat example,
[00:33:12.180 --> 00:33:14.580]   it makes perfectly realistic cat photos.
[00:33:14.580 --> 00:33:17.180]   And the discriminator is unable to do better
[00:33:17.180 --> 00:33:18.740]   than random guessing
[00:33:18.740 --> 00:33:21.860]   because all the samples coming from both the data
[00:33:21.860 --> 00:33:24.060]   and the generator look equally likely
[00:33:24.060 --> 00:33:25.860]   to have come from either source.
[00:33:25.860 --> 00:33:28.380]   - So do you ever sit back
[00:33:28.380 --> 00:33:31.300]   and does it just blow your mind that this thing works?
[00:33:31.300 --> 00:33:33.380]   So from very,
[00:33:33.380 --> 00:33:35.860]   so it's able to estimate the identity function
[00:33:35.860 --> 00:33:38.700]   enough to generate realistic images.
[00:33:38.700 --> 00:33:42.180]   I mean, yeah, do you ever sit back?
[00:33:42.180 --> 00:33:44.220]   - Yeah. - And think,
[00:33:44.220 --> 00:33:46.780]   how does this even, why, this is quite incredible,
[00:33:46.780 --> 00:33:48.340]   especially where GANs have gone
[00:33:48.340 --> 00:33:49.300]   in terms of realism.
[00:33:49.300 --> 00:33:51.660]   - Yeah, and not just to flatter my own work,
[00:33:51.660 --> 00:33:53.900]   but generative models,
[00:33:53.900 --> 00:33:55.460]   all of them have this property
[00:33:55.460 --> 00:33:58.860]   that if they really did what we asked them to do,
[00:33:58.860 --> 00:34:01.100]   they would do nothing but memorize the training data.
[00:34:01.100 --> 00:34:01.940]   - Right, exactly.
[00:34:01.940 --> 00:34:05.780]   - Models that are based on maximizing the likelihood,
[00:34:05.780 --> 00:34:08.180]   the way that you obtain the maximum likelihood
[00:34:08.180 --> 00:34:09.740]   for a specific training set
[00:34:09.740 --> 00:34:12.420]   is you assign all of your probability mass
[00:34:12.420 --> 00:34:15.140]   to the training examples and nowhere else.
[00:34:15.140 --> 00:34:18.420]   For GANs, the game is played using a training set.
[00:34:18.420 --> 00:34:21.180]   So the way that you become unbeatable in the game
[00:34:21.180 --> 00:34:23.420]   is you literally memorize training examples.
[00:34:23.420 --> 00:34:28.900]   One of my former interns wrote a paper,
[00:34:28.900 --> 00:34:31.060]   his name is Vaishnav Nagarajan,
[00:34:31.060 --> 00:34:33.060]   and he showed that it's actually hard
[00:34:33.060 --> 00:34:36.100]   for the generator to memorize the training data,
[00:34:36.100 --> 00:34:39.140]   hard in a statistical learning theory sense
[00:34:39.140 --> 00:34:42.180]   that you can actually create reasons
[00:34:42.180 --> 00:34:47.180]   for why it would require quite a lot of learning steps
[00:34:47.180 --> 00:34:52.180]   and a lot of observations of different latent variables
[00:34:52.180 --> 00:34:54.340]   before you could memorize the training data.
[00:34:54.340 --> 00:34:55.660]   That still doesn't really explain
[00:34:55.660 --> 00:34:58.220]   why when you produce samples that are new,
[00:34:58.220 --> 00:34:59.860]   why do you get compelling images
[00:34:59.860 --> 00:35:01.860]   rather than just garbage
[00:35:01.860 --> 00:35:03.740]   that's different from the training set.
[00:35:03.740 --> 00:35:06.940]   And I don't think we really have a good answer for that,
[00:35:06.940 --> 00:35:07.900]   especially if you think about
[00:35:07.900 --> 00:35:10.260]   how many possible images are out there
[00:35:10.260 --> 00:35:15.260]   and how few images the generative model sees during training.
[00:35:15.260 --> 00:35:16.940]   It seems just unreasonable
[00:35:16.940 --> 00:35:19.220]   that generative models create new images
[00:35:19.220 --> 00:35:20.780]   as well as they do,
[00:35:20.780 --> 00:35:22.740]   especially considering that we're basically
[00:35:22.740 --> 00:35:25.180]   training them to memorize rather than generalize.
[00:35:25.180 --> 00:35:28.220]   I think part of the answer is
[00:35:28.220 --> 00:35:30.860]   there's a paper called Deep Image Prior
[00:35:30.860 --> 00:35:33.100]   where they show that you can take a convolutional net
[00:35:33.100 --> 00:35:35.020]   and you don't even need to learn the parameters of it at all,
[00:35:35.020 --> 00:35:36.820]   you just use the model architecture.
[00:35:37.700 --> 00:35:41.100]   And it's already useful for things like in-painting images.
[00:35:41.100 --> 00:35:42.300]   I think that shows us
[00:35:42.300 --> 00:35:44.380]   that the convolutional network architecture
[00:35:44.380 --> 00:35:45.940]   captures something really important
[00:35:45.940 --> 00:35:47.980]   about the structure of images.
[00:35:47.980 --> 00:35:50.980]   And we don't need to actually use learning
[00:35:50.980 --> 00:35:52.260]   to capture all the information
[00:35:52.260 --> 00:35:54.060]   coming out of the convolutional net.
[00:35:54.060 --> 00:35:58.460]   That would imply that it would be much harder
[00:35:58.460 --> 00:36:01.300]   to make generative models in other domains.
[00:36:01.300 --> 00:36:03.660]   So far, we're able to make reasonable speech models
[00:36:03.660 --> 00:36:04.900]   and things like that.
[00:36:04.900 --> 00:36:06.420]   But to be honest,
[00:36:06.420 --> 00:36:07.860]   we haven't actually explored a whole lot
[00:36:07.860 --> 00:36:09.820]   of different data sets all that much.
[00:36:09.820 --> 00:36:11.500]   We don't, for example,
[00:36:11.500 --> 00:36:16.500]   see a lot of deep learning models of biology data sets,
[00:36:16.500 --> 00:36:19.900]   where you have lots of microarrays
[00:36:19.900 --> 00:36:22.300]   measuring the amount of different enzymes
[00:36:22.300 --> 00:36:23.140]   and things like that.
[00:36:23.140 --> 00:36:25.300]   So we may find that some of the progress
[00:36:25.300 --> 00:36:26.900]   that we've seen for images and speech
[00:36:26.900 --> 00:36:30.140]   turns out to really rely heavily on the model architecture.
[00:36:30.140 --> 00:36:33.020]   And we were able to do what we did for vision
[00:36:33.020 --> 00:36:36.020]   by trying to reverse engineer the human visual system.
[00:36:37.020 --> 00:36:39.820]   And maybe it'll turn out that we can't just
[00:36:39.820 --> 00:36:42.580]   use that same trick for arbitrary kinds of data.
[00:36:42.580 --> 00:36:45.940]   - Right, so there's aspects of the human vision system,
[00:36:45.940 --> 00:36:48.420]   the hardware of it, that makes it,
[00:36:48.420 --> 00:36:51.140]   without learning, without cognition,
[00:36:51.140 --> 00:36:53.660]   just makes it really effective at detecting the patterns
[00:36:53.660 --> 00:36:54.940]   we see in the visual world.
[00:36:54.940 --> 00:36:55.940]   - Yeah.
[00:36:55.940 --> 00:36:57.700]   - Yeah, that's really interesting.
[00:36:57.700 --> 00:37:02.300]   What, in a big, quick overview,
[00:37:02.300 --> 00:37:06.300]   in your view, what types of GANs are there,
[00:37:06.300 --> 00:37:10.100]   and what other generative models besides GANs are there?
[00:37:10.100 --> 00:37:13.540]   - Yeah, so it's maybe a little bit easier to start with
[00:37:13.540 --> 00:37:16.900]   what kinds of generative models are there other than GANs.
[00:37:16.900 --> 00:37:20.900]   So most generative models are likelihood-based,
[00:37:20.900 --> 00:37:24.900]   where to train them, you have a model that tells you
[00:37:24.900 --> 00:37:29.100]   how much probability it assigns to a particular example,
[00:37:29.100 --> 00:37:31.140]   and you just maximize the probability
[00:37:31.140 --> 00:37:33.700]   assigned to all the training examples.
[00:37:33.700 --> 00:37:36.180]   It turns out that it's hard to design a model
[00:37:36.180 --> 00:37:39.180]   that can create really complicated images
[00:37:39.180 --> 00:37:42.260]   or really complicated audio waveforms,
[00:37:42.260 --> 00:37:46.220]   and still have it be possible to estimate
[00:37:46.220 --> 00:37:51.220]   the likelihood function from a computational point of view.
[00:37:51.220 --> 00:37:53.820]   Most interesting models that you would just
[00:37:53.820 --> 00:37:56.420]   write down intuitively, it turns out that it's almost
[00:37:56.420 --> 00:37:59.020]   impossible to calculate the amount of probability
[00:37:59.020 --> 00:38:00.780]   they assign to a particular point.
[00:38:00.780 --> 00:38:04.740]   So there's a few different schools of generative models
[00:38:04.740 --> 00:38:06.260]   in the likelihood family.
[00:38:06.260 --> 00:38:10.180]   One approach is to very carefully design the model
[00:38:10.180 --> 00:38:12.780]   so that it is computationally tractable
[00:38:12.780 --> 00:38:15.540]   to measure the density it assigns to a particular point.
[00:38:15.540 --> 00:38:19.140]   So there are things like autoregressive models,
[00:38:19.140 --> 00:38:23.940]   like PixelCNN, those basically break down
[00:38:23.940 --> 00:38:26.860]   the probability distribution into a product
[00:38:26.860 --> 00:38:28.660]   over every single feature.
[00:38:28.660 --> 00:38:31.540]   So for an image, you estimate the probability
[00:38:31.540 --> 00:38:35.780]   of each pixel, given all of the pixels that came before it.
[00:38:35.780 --> 00:38:37.660]   There's tricks where if you want to measure
[00:38:37.660 --> 00:38:40.620]   the density function, you can actually calculate
[00:38:40.620 --> 00:38:43.500]   the density for all these pixels more or less in parallel.
[00:38:43.500 --> 00:38:46.860]   Generating the image still tends to require you
[00:38:46.860 --> 00:38:50.820]   to go one pixel at a time, and that can be very slow.
[00:38:50.820 --> 00:38:52.980]   But there are, again, tricks for doing this
[00:38:52.980 --> 00:38:54.540]   in a hierarchical pattern where you can keep
[00:38:54.540 --> 00:38:56.140]   the runtime under control.
[00:38:56.140 --> 00:38:58.540]   - Are the quality of the images it generates
[00:38:58.540 --> 00:39:01.660]   putting runtime aside pretty good?
[00:39:01.660 --> 00:39:04.420]   - They're reasonable, yeah.
[00:39:04.420 --> 00:39:07.460]   I would say a lot of the best results
[00:39:07.460 --> 00:39:11.060]   are from GANs these days, but it can be hard to tell
[00:39:11.060 --> 00:39:14.700]   how much of that is based on who's studying
[00:39:14.700 --> 00:39:17.300]   which type of algorithm, if that makes sense.
[00:39:17.300 --> 00:39:18.900]   - The amount of effort invested in a particular--
[00:39:18.900 --> 00:39:21.420]   - Yeah, or like the kind of expertise.
[00:39:21.420 --> 00:39:23.140]   So a lot of people who've traditionally been excited
[00:39:23.140 --> 00:39:25.060]   about graphics or art and things like that
[00:39:25.060 --> 00:39:27.020]   have gotten interested in GANs.
[00:39:27.020 --> 00:39:28.740]   And to some extent, it's hard to tell,
[00:39:28.740 --> 00:39:32.340]   are GANs doing better because they have a lot of
[00:39:32.340 --> 00:39:34.700]   graphics and art experts behind them,
[00:39:34.700 --> 00:39:36.700]   or are GANs doing better because
[00:39:36.700 --> 00:39:38.900]   they're more computationally efficient,
[00:39:38.900 --> 00:39:41.660]   or are GANs doing better because they prioritize
[00:39:41.660 --> 00:39:44.620]   the realism of samples over the accuracy
[00:39:44.620 --> 00:39:45.500]   of the density function?
[00:39:45.500 --> 00:39:48.660]   I think all of those are potentially valid explanations,
[00:39:48.660 --> 00:39:51.300]   and it's hard to tell.
[00:39:51.300 --> 00:39:53.740]   - So can you give a brief history of GANs
[00:39:53.740 --> 00:39:58.740]   from 2014, were you paper 13?
[00:39:58.740 --> 00:40:00.980]   - Yeah, so a few highlights.
[00:40:00.980 --> 00:40:04.740]   In the first paper, we just showed that GANs basically work.
[00:40:04.740 --> 00:40:06.620]   If you look back at the samples we had now,
[00:40:06.620 --> 00:40:08.820]   they look terrible.
[00:40:08.820 --> 00:40:10.460]   On the CIFAR-10 dataset, you can't even
[00:40:10.460 --> 00:40:12.220]   recognize objects in them.
[00:40:12.220 --> 00:40:15.020]   - Your paper, sorry, you used CIFAR-10?
[00:40:15.020 --> 00:40:18.060]   - We used MNIST, which is little handwritten digits.
[00:40:18.060 --> 00:40:19.860]   We used the Toronto Face Database,
[00:40:19.860 --> 00:40:22.700]   which is small, grayscale photos of faces.
[00:40:22.700 --> 00:40:24.220]   We did have recognizable faces.
[00:40:24.220 --> 00:40:25.700]   My colleague Bing Xu put together
[00:40:25.700 --> 00:40:28.540]   the first GAN face model for that paper.
[00:40:28.540 --> 00:40:32.980]   We also had the CIFAR-10 dataset,
[00:40:32.980 --> 00:40:36.100]   which is things like very small 32 by 32 pixels
[00:40:36.100 --> 00:40:40.660]   of cars and cats and dogs.
[00:40:40.660 --> 00:40:43.020]   For that, we didn't get recognizable objects,
[00:40:43.020 --> 00:40:46.180]   but all the deep learning people back then
[00:40:46.180 --> 00:40:48.420]   were really used to looking at these failed samples
[00:40:48.420 --> 00:40:50.420]   and kind of reading them like tea leaves.
[00:40:50.420 --> 00:40:53.020]   And people who are used to reading the tea leaves
[00:40:53.020 --> 00:40:56.500]   recognize that our tea leaves at least look different.
[00:40:56.500 --> 00:40:57.820]   Maybe not necessarily better,
[00:40:57.820 --> 00:40:59.980]   but there was something unusual about them.
[00:40:59.980 --> 00:41:03.620]   And that got a lot of us excited.
[00:41:03.620 --> 00:41:06.180]   One of the next really big steps was LAPGAN
[00:41:06.180 --> 00:41:10.900]   by Emily Denton and Sumit Chintala at Facebook AI Research,
[00:41:10.900 --> 00:41:14.420]   where they actually got really good high-resolution photos
[00:41:14.420 --> 00:41:16.580]   working with GANs for the first time.
[00:41:16.580 --> 00:41:18.860]   They had a complicated system where they generated
[00:41:18.860 --> 00:41:22.780]   the image starting at low-res and then scaling up to high-res,
[00:41:22.780 --> 00:41:24.900]   but they were able to get it to work.
[00:41:24.900 --> 00:41:29.900]   And then in 2015, I believe, later that same year,
[00:41:29.900 --> 00:41:34.940]   Alec Radford and Sumit Chintala and Luke Metz
[00:41:34.940 --> 00:41:38.420]   published the DCGAN paper,
[00:41:38.420 --> 00:41:40.980]   which it stands for Deep Convolutional GAN.
[00:41:40.980 --> 00:41:43.740]   It's kind of a non-unique name
[00:41:43.740 --> 00:41:46.420]   because these days basically all GANs
[00:41:46.420 --> 00:41:48.380]   and even some before that were deep and convolutional,
[00:41:48.380 --> 00:41:50.220]   but they just kind of picked a name
[00:41:50.220 --> 00:41:54.020]   for a really great recipe where they were able to actually,
[00:41:54.020 --> 00:41:57.300]   using only one model instead of a multi-step process,
[00:41:57.300 --> 00:41:59.700]   actually generate realistic images of faces
[00:41:59.700 --> 00:42:00.740]   and things like that.
[00:42:00.740 --> 00:42:05.220]   That was sort of like the beginning
[00:42:05.220 --> 00:42:07.380]   of the Cambrian explosion of GANs.
[00:42:07.380 --> 00:42:09.740]   Like, once you had animals that had a backbone,
[00:42:09.740 --> 00:42:12.900]   you suddenly got lots of different versions of fish
[00:42:12.900 --> 00:42:15.340]   and four-legged animals and things like that.
[00:42:15.340 --> 00:42:17.940]   So DCGAN became kind of the backbone
[00:42:17.940 --> 00:42:19.420]   for many different models that came out.
[00:42:19.420 --> 00:42:21.620]   - Used as a baseline even still.
[00:42:21.620 --> 00:42:23.140]   - Yeah, yeah.
[00:42:23.140 --> 00:42:25.940]   And so from there, I would say some interesting things
[00:42:25.940 --> 00:42:29.420]   we've seen are, there's a lot you can say
[00:42:29.420 --> 00:42:30.940]   about how just the quality
[00:42:30.940 --> 00:42:33.540]   of standard image generation GANs has increased,
[00:42:33.540 --> 00:42:35.100]   but what's also maybe more interesting
[00:42:35.100 --> 00:42:37.380]   on an intellectual level is how the things
[00:42:37.380 --> 00:42:40.060]   you can use GANs for has also changed.
[00:42:40.060 --> 00:42:44.580]   One thing is that you can use them to learn classifiers
[00:42:44.580 --> 00:42:47.380]   without having to have class labels for every example
[00:42:47.380 --> 00:42:48.940]   in your training set.
[00:42:48.940 --> 00:42:51.780]   So that's called semi-supervised learning.
[00:42:51.780 --> 00:42:55.820]   My colleague at OpenAI, Tim Solomons, who's at Brain now,
[00:42:55.820 --> 00:42:59.780]   wrote a paper called "Improved Techniques for Training GANs."
[00:42:59.780 --> 00:43:00.900]   I'm a co-author on this paper,
[00:43:00.900 --> 00:43:03.700]   but I can't claim any credit for this particular part.
[00:43:03.700 --> 00:43:05.860]   One thing he showed in the paper is that
[00:43:05.860 --> 00:43:07.820]   you can take the GAN discriminator
[00:43:07.820 --> 00:43:11.340]   and use it as a classifier that actually tells you,
[00:43:11.340 --> 00:43:13.620]   you know, this image is a cat, this image is a dog,
[00:43:13.620 --> 00:43:16.420]   this image is a car, this image is a truck, and so on.
[00:43:16.420 --> 00:43:18.820]   Not just to say whether the image is real or fake,
[00:43:18.820 --> 00:43:20.700]   but if it is real, to say specifically
[00:43:20.700 --> 00:43:22.620]   what kind of object it is.
[00:43:22.620 --> 00:43:25.340]   And he found that you can train these classifiers
[00:43:25.340 --> 00:43:30.340]   with far fewer labeled examples than traditional classifiers.
[00:43:30.340 --> 00:43:33.660]   - So if you supervise based on also
[00:43:33.660 --> 00:43:35.300]   not just your discrimination ability,
[00:43:35.300 --> 00:43:38.660]   but your ability to classify, you're going to do much,
[00:43:38.660 --> 00:43:40.100]   you're going to converge much faster
[00:43:40.100 --> 00:43:43.300]   to being effective at being a discriminator.
[00:43:43.300 --> 00:43:44.260]   - Yeah.
[00:43:44.260 --> 00:43:46.340]   So for example, for the MNIST dataset,
[00:43:46.340 --> 00:43:48.860]   you want to look at an image of a handwritten digit
[00:43:48.860 --> 00:43:52.700]   and say whether it's a zero, a one, or a two, and so on.
[00:43:52.700 --> 00:43:56.980]   To get down to less than 1% accuracy
[00:43:56.980 --> 00:44:00.260]   required around 60,000 examples
[00:44:00.260 --> 00:44:02.780]   until maybe about 2014 or so.
[00:44:02.780 --> 00:44:07.460]   In 2016, with this semi-supervised GAN project,
[00:44:07.460 --> 00:44:11.060]   Tim was able to get below 1% error
[00:44:11.060 --> 00:44:13.660]   using only a hundred labeled examples.
[00:44:13.660 --> 00:44:16.020]   So that was about a 600X decrease
[00:44:16.020 --> 00:44:18.020]   in the amount of labels that he needed.
[00:44:18.020 --> 00:44:21.100]   He's still using more images than that,
[00:44:21.100 --> 00:44:23.460]   but he doesn't need to have each of them labeled as,
[00:44:23.460 --> 00:44:25.100]   you know, this one's a one, this one's a two,
[00:44:25.100 --> 00:44:27.020]   this one's a zero, and so on.
[00:44:27.020 --> 00:44:30.020]   - Then to be able to, for GANs to be able to generate
[00:44:30.020 --> 00:44:33.460]   recognizable objects, so objects from a particular class,
[00:44:33.460 --> 00:44:37.020]   you still need labeled data
[00:44:37.020 --> 00:44:38.900]   because you need to know what it means
[00:44:38.900 --> 00:44:40.900]   to be a particular class cat, dog.
[00:44:40.900 --> 00:44:44.620]   How do you think we can move away from that?
[00:44:44.620 --> 00:44:46.660]   - Yeah, some researchers at Brain Zurich
[00:44:46.660 --> 00:44:49.060]   actually just released a really great paper
[00:44:49.060 --> 00:44:53.980]   on semi-supervised GANs where their goal isn't to classify,
[00:44:53.980 --> 00:44:56.260]   it's to make recognizable objects
[00:44:56.260 --> 00:44:58.700]   despite not having a lot of labeled data.
[00:44:58.700 --> 00:45:02.420]   They were working off of DeepMind's BigGAN project,
[00:45:02.420 --> 00:45:05.220]   and they showed that they can match the performance
[00:45:05.220 --> 00:45:08.700]   of BigGAN using only 10%, I believe,
[00:45:08.700 --> 00:45:10.580]   of the labels.
[00:45:10.580 --> 00:45:12.340]   BigGAN was trained on the ImageNet dataset,
[00:45:12.340 --> 00:45:14.460]   which is about 1.2 million images,
[00:45:14.460 --> 00:45:15.900]   and had all of them labeled.
[00:45:15.900 --> 00:45:19.100]   This latest project from Brain Zurich
[00:45:19.100 --> 00:45:20.260]   shows that they're able to get away
[00:45:20.260 --> 00:45:24.620]   with only having about 10% of the images labeled.
[00:45:24.620 --> 00:45:29.900]   And they do that essentially using a clustering algorithm
[00:45:29.900 --> 00:45:33.380]   where the discriminator learns to assign the objects
[00:45:33.380 --> 00:45:36.340]   to groups, and then this understanding
[00:45:36.340 --> 00:45:40.380]   that objects can be grouped into similar types
[00:45:40.380 --> 00:45:43.460]   helps it to form more realistic ideas
[00:45:43.460 --> 00:45:45.420]   of what should be appearing in the image,
[00:45:45.420 --> 00:45:47.980]   because it knows that every image it creates
[00:45:47.980 --> 00:45:50.180]   has to come from one of these archetypal groups
[00:45:50.180 --> 00:45:53.220]   rather than just being some arbitrary image.
[00:45:53.220 --> 00:45:55.140]   If you train a GAN with no class labels,
[00:45:55.140 --> 00:45:57.220]   you tend to get things that look sort of like
[00:45:57.220 --> 00:46:00.500]   grass or water or brick or dirt,
[00:46:00.500 --> 00:46:04.460]   but without necessarily a lot going on in them.
[00:46:04.460 --> 00:46:05.820]   And I think that's partly because
[00:46:05.820 --> 00:46:07.900]   if you look at a large ImageNet image,
[00:46:07.900 --> 00:46:11.260]   the object doesn't necessarily occupy the whole image.
[00:46:11.260 --> 00:46:15.660]   And so you learn to create realistic sets of pixels,
[00:46:15.660 --> 00:46:17.540]   but you don't necessarily learn
[00:46:17.540 --> 00:46:20.140]   that the object is the star of the show
[00:46:20.140 --> 00:46:22.220]   and you want it to be in every image you make.
[00:46:22.220 --> 00:46:25.460]   - Yeah, I've heard you talk about the horse,
[00:46:25.460 --> 00:46:27.060]   the zebra cycle GAN mapping,
[00:46:27.060 --> 00:46:31.980]   and how it turns out, again, thought-provoking,
[00:46:31.980 --> 00:46:33.660]   that horses are usually on grass
[00:46:33.660 --> 00:46:35.740]   and zebras are usually on drier terrain.
[00:46:35.740 --> 00:46:38.220]   So when you're doing that kind of generation,
[00:46:38.220 --> 00:46:42.720]   you're going to end up generating greener horses or whatever.
[00:46:42.720 --> 00:46:45.420]   So those are connected together.
[00:46:45.420 --> 00:46:47.420]   It's not just-- - Yeah, yeah.
[00:46:47.420 --> 00:46:49.060]   - You're not able to segment,
[00:46:49.060 --> 00:46:52.360]   you're able to generate in a segmented way.
[00:46:52.360 --> 00:46:55.060]   So are there other types of games you come across
[00:46:55.060 --> 00:47:00.060]   in your mind that neural networks can play with each other
[00:47:00.060 --> 00:47:05.220]   to be able to solve problems?
[00:47:05.220 --> 00:47:07.700]   - Yeah, the one that I spend most of my time on
[00:47:07.700 --> 00:47:12.700]   is in security, you can model most interactions as a game
[00:47:12.700 --> 00:47:15.820]   where there's attackers trying to break your system
[00:47:15.820 --> 00:47:19.160]   and you're the defender trying to build a resilient system.
[00:47:19.160 --> 00:47:23.100]   There's also domain adversarial learning,
[00:47:23.100 --> 00:47:25.540]   which is an approach to domain adaptation
[00:47:25.540 --> 00:47:27.260]   that looks really a lot like GANs.
[00:47:27.260 --> 00:47:31.820]   The authors had the idea before the GAN paper came out,
[00:47:31.820 --> 00:47:33.780]   their paper came out a little bit later,
[00:47:33.780 --> 00:47:38.260]   and they were very nice and cited the GAN paper,
[00:47:38.260 --> 00:47:40.220]   but I know that they actually had the idea
[00:47:40.220 --> 00:47:41.180]   before it came out.
[00:47:41.180 --> 00:47:44.340]   Domain adaptation is when you want to train
[00:47:44.340 --> 00:47:47.620]   a machine learning model in one setting called a domain
[00:47:47.620 --> 00:47:50.300]   and then deploy it in another domain later.
[00:47:50.300 --> 00:47:52.700]   And you would like it to perform well in the new domain,
[00:47:52.700 --> 00:47:54.020]   even though the new domain is different
[00:47:54.020 --> 00:47:55.940]   from how it was trained.
[00:47:55.940 --> 00:47:58.500]   So for example, you might want to train
[00:47:58.500 --> 00:48:01.380]   on a really clean image data set like ImageNet,
[00:48:01.380 --> 00:48:03.380]   but then deploy on users' phones
[00:48:03.380 --> 00:48:06.020]   where the user is taking pictures in the dark
[00:48:06.020 --> 00:48:07.820]   or pictures while moving quickly
[00:48:07.820 --> 00:48:10.020]   and just pictures that aren't really centered
[00:48:10.020 --> 00:48:11.340]   or composed all that well.
[00:48:11.340 --> 00:48:15.860]   When you take a normal machine learning model,
[00:48:15.860 --> 00:48:17.860]   it often degrades really badly
[00:48:17.860 --> 00:48:19.020]   when you move to the new domain
[00:48:19.020 --> 00:48:20.060]   because it looks so different
[00:48:20.060 --> 00:48:22.140]   from what the model was trained on.
[00:48:22.140 --> 00:48:25.460]   Domain adaptation algorithms try to smooth out that gap.
[00:48:25.460 --> 00:48:27.340]   And the domain adversarial approach
[00:48:27.340 --> 00:48:29.820]   is based on training a feature extractor
[00:48:29.820 --> 00:48:32.180]   where the features have the same statistics
[00:48:32.180 --> 00:48:35.180]   regardless of which domain you extracted them on.
[00:48:35.180 --> 00:48:36.900]   So in the domain adversarial game,
[00:48:36.900 --> 00:48:39.180]   you have one player that's a feature extractor
[00:48:39.180 --> 00:48:42.100]   and another player that's a domain recognizer.
[00:48:42.100 --> 00:48:44.300]   The domain recognizer wants to look at the output
[00:48:44.300 --> 00:48:45.740]   of the feature extractor
[00:48:45.740 --> 00:48:49.340]   and guess which of the two domains the features came from.
[00:48:49.340 --> 00:48:50.900]   So it's a lot like the real versus fake
[00:48:50.900 --> 00:48:52.500]   discriminator in GANs.
[00:48:52.500 --> 00:48:54.940]   And then the feature extractor,
[00:48:54.940 --> 00:48:56.860]   you can think of as loosely analogous
[00:48:56.860 --> 00:48:57.980]   to the generator in GANs,
[00:48:57.980 --> 00:48:59.140]   except what it's trying to do here
[00:48:59.140 --> 00:49:02.500]   is both fool the domain recognizer
[00:49:02.500 --> 00:49:05.380]   into not knowing which domain the data came from
[00:49:05.380 --> 00:49:09.060]   and also extract features that are good for classification.
[00:49:09.060 --> 00:49:10.500]   So at the end of the day,
[00:49:10.500 --> 00:49:13.780]   in the cases where it works out,
[00:49:13.780 --> 00:49:16.900]   you can actually get features
[00:49:16.900 --> 00:49:20.660]   that work about the same in both domains.
[00:49:20.660 --> 00:49:22.900]   Sometimes this has a drawback where
[00:49:22.900 --> 00:49:24.860]   in order to make things work the same in both domains,
[00:49:24.860 --> 00:49:26.780]   it just gets worse at the first one.
[00:49:26.780 --> 00:49:27.860]   But there are a lot of cases
[00:49:27.860 --> 00:49:30.820]   where it actually works out well on both.
[00:49:30.820 --> 00:49:33.020]   - So do you think of GANs being useful
[00:49:33.020 --> 00:49:35.460]   in the context of data augmentation?
[00:49:35.460 --> 00:49:38.100]   - Yeah, one thing you could hope for with GANs
[00:49:38.100 --> 00:49:41.380]   is you could imagine I've got a limited training set
[00:49:41.380 --> 00:49:43.900]   and I'd like to make more training data
[00:49:43.900 --> 00:49:46.060]   to train something else like a classifier.
[00:49:46.060 --> 00:49:50.540]   You could train the GAN on the training set
[00:49:50.540 --> 00:49:52.380]   and then create more data.
[00:49:52.380 --> 00:49:55.220]   And then maybe the classifier would perform better
[00:49:55.220 --> 00:49:56.500]   on the test set after training
[00:49:56.500 --> 00:49:58.900]   on this bigger GAN generated data set.
[00:49:58.900 --> 00:50:00.420]   So that's the simplest version
[00:50:00.420 --> 00:50:03.060]   of something you might hope would work.
[00:50:03.060 --> 00:50:05.460]   I've never heard of that particular approach working,
[00:50:05.460 --> 00:50:08.940]   but I think there's some closely related things
[00:50:08.940 --> 00:50:11.540]   that I think could work in the future
[00:50:11.540 --> 00:50:14.100]   and some that actually already have worked.
[00:50:14.100 --> 00:50:15.820]   So if we think a little bit about what we'd be hoping for
[00:50:15.820 --> 00:50:18.220]   if we use the GAN to make more training data,
[00:50:18.220 --> 00:50:22.060]   we're hoping that the GAN will generalize to new examples
[00:50:22.060 --> 00:50:24.140]   better than the classifier would have generalized
[00:50:24.140 --> 00:50:25.980]   if it was trained on the same data.
[00:50:25.980 --> 00:50:27.700]   And I don't know of any reason to believe
[00:50:27.700 --> 00:50:28.900]   that the GAN would generalize better
[00:50:28.900 --> 00:50:30.260]   than the classifier would.
[00:50:30.260 --> 00:50:33.060]   But what we might hope for
[00:50:33.060 --> 00:50:35.540]   is that the GAN could generalize differently
[00:50:35.540 --> 00:50:37.460]   from a specific classifier.
[00:50:37.460 --> 00:50:39.140]   So one thing I think is worth trying
[00:50:39.140 --> 00:50:41.020]   that I haven't personally tried, but someone could try
[00:50:41.020 --> 00:50:43.380]   is what if you trained a whole lot
[00:50:43.380 --> 00:50:46.460]   of different generative models on the same training set,
[00:50:46.460 --> 00:50:48.340]   create samples from all of them,
[00:50:48.340 --> 00:50:50.540]   and then train a classifier on that?
[00:50:50.540 --> 00:50:52.980]   Because each of the generative models might generalize
[00:50:52.980 --> 00:50:54.420]   in a slightly different way,
[00:50:54.420 --> 00:50:56.940]   they might capture many different axes of variation
[00:50:56.940 --> 00:50:58.820]   that one individual model wouldn't.
[00:50:58.820 --> 00:51:01.860]   And then the classifier can capture all of those ideas
[00:51:01.860 --> 00:51:03.540]   by training on all of their data.
[00:51:03.540 --> 00:51:04.380]   So it'd be a little bit
[00:51:04.380 --> 00:51:06.260]   like making an ensemble of classifiers.
[00:51:06.260 --> 00:51:07.820]   - Ensemble of GANs.
[00:51:07.820 --> 00:51:08.820]   - Yeah. - In a way.
[00:51:08.820 --> 00:51:10.060]   - I think that could generalize better.
[00:51:10.060 --> 00:51:12.620]   The other thing that GANs are really good for
[00:51:12.620 --> 00:51:16.980]   is not necessarily generating new data
[00:51:16.980 --> 00:51:19.340]   that's exactly like what you already have,
[00:51:19.340 --> 00:51:23.540]   but by generating new data that has different properties
[00:51:23.540 --> 00:51:25.300]   from the data you already had.
[00:51:25.300 --> 00:51:26.220]   One thing that you can do
[00:51:26.220 --> 00:51:29.100]   is you can create differentially private data.
[00:51:29.100 --> 00:51:31.860]   So suppose that you have something like medical records,
[00:51:31.860 --> 00:51:33.820]   and you don't want to train a classifier
[00:51:33.820 --> 00:51:36.460]   on the medical records and then publish the classifier,
[00:51:36.460 --> 00:51:38.140]   because someone might be able to reverse engineer
[00:51:38.140 --> 00:51:40.540]   some of the medical records you trained on.
[00:51:40.540 --> 00:51:42.780]   There's a paper from Casey Green's lab
[00:51:42.780 --> 00:51:45.020]   that shows how you can train a GAN
[00:51:45.020 --> 00:51:46.980]   using differential privacy.
[00:51:46.980 --> 00:51:48.980]   And then the samples from the GAN
[00:51:48.980 --> 00:51:51.180]   still have the same differential privacy guarantees
[00:51:51.180 --> 00:51:52.700]   as the parameters of the GAN.
[00:51:52.700 --> 00:51:55.660]   So you can make fake patient data
[00:51:55.660 --> 00:51:57.220]   for other researchers to use,
[00:51:57.220 --> 00:51:59.180]   and they can do almost anything they want with that data
[00:51:59.180 --> 00:52:02.020]   because it doesn't come from real people.
[00:52:02.020 --> 00:52:04.260]   And the differential privacy mechanism
[00:52:04.260 --> 00:52:06.460]   gives you clear guarantees
[00:52:06.460 --> 00:52:09.900]   on how much the original people's data has been protected.
[00:52:09.900 --> 00:52:11.340]   - That's really interesting, actually.
[00:52:11.340 --> 00:52:13.740]   I haven't heard you talk about that before.
[00:52:13.740 --> 00:52:15.220]   In terms of fairness,
[00:52:15.220 --> 00:52:18.620]   I've seen from AAAI, your talk,
[00:52:18.620 --> 00:52:21.220]   how can adversarial machine learning
[00:52:21.220 --> 00:52:23.300]   help models be more fair
[00:52:23.300 --> 00:52:25.700]   with respect to sensitive variables?
[00:52:25.700 --> 00:52:28.460]   - Yeah, so there's a paper from Amos Storky's lab
[00:52:28.460 --> 00:52:31.380]   about how to learn machine learning models
[00:52:31.380 --> 00:52:34.780]   that are incapable of using specific variables.
[00:52:34.780 --> 00:52:36.660]   So say, for example, you wanted to make predictions
[00:52:36.660 --> 00:52:39.540]   that are not affected by gender.
[00:52:39.540 --> 00:52:41.220]   It isn't enough to just leave gender
[00:52:41.220 --> 00:52:42.780]   out of the input to the model.
[00:52:42.780 --> 00:52:43.980]   You can often infer gender
[00:52:43.980 --> 00:52:45.420]   from a lot of other characteristics.
[00:52:45.420 --> 00:52:47.460]   Like, say that you have the person's name,
[00:52:47.460 --> 00:52:48.580]   but you're not told their gender.
[00:52:48.580 --> 00:52:50.500]   Well, if their name is Ian,
[00:52:50.500 --> 00:52:52.100]   they're kind of obviously a man.
[00:52:52.100 --> 00:52:54.540]   So what you'd like to do
[00:52:54.540 --> 00:52:55.620]   is make a machine learning model
[00:52:55.620 --> 00:52:58.980]   that can still take in a lot of different attributes
[00:52:58.980 --> 00:53:02.540]   and make a really accurate, informed prediction,
[00:53:02.540 --> 00:53:05.740]   but be confident that it isn't reverse engineering gender
[00:53:05.740 --> 00:53:08.380]   or another sensitive variable internally.
[00:53:08.380 --> 00:53:10.260]   You can do that using something very similar
[00:53:10.260 --> 00:53:12.820]   to the domain adversarial approach,
[00:53:12.820 --> 00:53:16.100]   where you have one player that's a feature extractor
[00:53:16.100 --> 00:53:19.060]   and another player that's a feature analyzer.
[00:53:19.060 --> 00:53:21.420]   And you want to make sure that the feature analyzer
[00:53:21.420 --> 00:53:24.700]   is not able to guess the value of the sensitive variable
[00:53:24.700 --> 00:53:26.620]   that you're trying to keep private.
[00:53:26.620 --> 00:53:29.060]   - Right, that's, yeah, I love this approach.
[00:53:29.060 --> 00:53:31.620]   So, yeah, with the feature,
[00:53:31.620 --> 00:53:36.020]   you're not able to infer the sensitive variables.
[00:53:36.020 --> 00:53:36.860]   - Yeah. - It's brilliant.
[00:53:36.860 --> 00:53:39.460]   It's quite brilliant and simple, actually.
[00:53:39.460 --> 00:53:42.740]   - Another way I think that GANs in particular
[00:53:42.740 --> 00:53:44.220]   could be used for fairness
[00:53:44.220 --> 00:53:46.740]   would be to make something like a cycle GAN,
[00:53:46.740 --> 00:53:49.700]   where you can take data from one domain
[00:53:49.700 --> 00:53:51.140]   and convert it into another.
[00:53:51.140 --> 00:53:53.860]   We've seen cycle GAN turning horses into zebras.
[00:53:53.860 --> 00:53:58.860]   We've seen other unsupervised GANs made by Mingyu Liu
[00:53:58.860 --> 00:54:01.980]   doing things like turning day photos into night photos.
[00:54:01.980 --> 00:54:04.780]   I think for fairness,
[00:54:04.780 --> 00:54:08.420]   you could imagine taking records for people in one group
[00:54:08.420 --> 00:54:11.500]   and transforming them into analogous people in another group
[00:54:11.500 --> 00:54:14.940]   and testing to see if they're treated equitably
[00:54:14.940 --> 00:54:16.420]   across those two groups.
[00:54:16.420 --> 00:54:18.060]   There's a lot of things that'd be hard to get right
[00:54:18.060 --> 00:54:21.100]   to make sure that the conversion process itself is fair.
[00:54:21.100 --> 00:54:23.860]   And I don't think it's anywhere near
[00:54:23.860 --> 00:54:25.380]   something that we could actually use yet.
[00:54:25.380 --> 00:54:27.100]   But if you could design that conversion process
[00:54:27.100 --> 00:54:30.500]   very carefully, it might give you a way of doing audits
[00:54:30.500 --> 00:54:33.100]   where you say, what if we took people from this group,
[00:54:33.100 --> 00:54:35.420]   converted them into equivalent people in another group?
[00:54:35.420 --> 00:54:38.660]   Does the system actually treat them how it ought to?
[00:54:38.660 --> 00:54:41.740]   - That's also really interesting.
[00:54:41.740 --> 00:54:46.740]   In popular press and in general, in our imagination,
[00:54:46.740 --> 00:54:51.700]   you think, well, GANs are able to generate data
[00:54:51.700 --> 00:54:54.500]   and you start to think about deep fakes
[00:54:54.500 --> 00:54:57.900]   or being able to sort of maliciously generate data
[00:54:57.900 --> 00:55:01.180]   that fakes the identity of other people.
[00:55:01.180 --> 00:55:03.140]   Is this something of a concern to you?
[00:55:03.140 --> 00:55:06.900]   Is this something, if you look 10, 20 years into the future,
[00:55:06.900 --> 00:55:10.340]   is that something that pops up in your work,
[00:55:10.340 --> 00:55:11.860]   in the work of the community that's working
[00:55:11.860 --> 00:55:13.540]   on generating models?
[00:55:13.540 --> 00:55:15.860]   - I'm a lot less concerned about 20 years from now
[00:55:15.860 --> 00:55:17.380]   than the next few years.
[00:55:17.380 --> 00:55:20.820]   I think there will be a kind of bumpy cultural transition
[00:55:20.820 --> 00:55:23.140]   as people encounter this idea
[00:55:23.140 --> 00:55:24.660]   that there can be very realistic videos
[00:55:24.660 --> 00:55:26.260]   and audio that aren't real.
[00:55:26.260 --> 00:55:30.100]   I think 20 years from now, people will mostly understand
[00:55:30.100 --> 00:55:31.900]   that you shouldn't believe something is real
[00:55:31.900 --> 00:55:34.060]   just because you saw a video of it.
[00:55:34.060 --> 00:55:36.700]   People will expect to see that it's been cryptographically
[00:55:36.700 --> 00:55:41.700]   signed or have some other mechanism to make them believe
[00:55:41.700 --> 00:55:44.300]   that the content is real.
[00:55:44.300 --> 00:55:45.660]   There's already people working on this.
[00:55:45.660 --> 00:55:47.620]   Like there's a startup called TruePic
[00:55:47.620 --> 00:55:50.860]   that provides a lot of mechanisms for authenticating
[00:55:50.860 --> 00:55:51.980]   that an image is real.
[00:55:51.980 --> 00:55:56.100]   They're maybe not quite up to having a state actor
[00:55:56.100 --> 00:55:59.820]   try to evade their verification techniques,
[00:55:59.820 --> 00:56:02.380]   but it's something that people are already working on
[00:56:02.380 --> 00:56:04.100]   and I think will get right eventually.
[00:56:04.140 --> 00:56:08.300]   - So you think authentication will eventually win out?
[00:56:08.300 --> 00:56:10.740]   So being able to authenticate that this is real
[00:56:10.740 --> 00:56:11.900]   and this is not.
[00:56:11.900 --> 00:56:13.300]   - Yeah.
[00:56:13.300 --> 00:56:15.780]   - As opposed to GANs just getting better and better
[00:56:15.780 --> 00:56:18.220]   or generative models being able to get better and better
[00:56:18.220 --> 00:56:21.500]   to where the nature of what is real is normal.
[00:56:21.500 --> 00:56:24.460]   - I don't think we'll ever be able to look at the pixels
[00:56:24.460 --> 00:56:28.580]   of a photo and tell you for sure that it's real or not real.
[00:56:28.580 --> 00:56:32.780]   And I think it would actually be somewhat dangerous
[00:56:32.780 --> 00:56:35.140]   to rely on that approach too much.
[00:56:35.140 --> 00:56:36.820]   If you make a really good fake detector
[00:56:36.820 --> 00:56:38.900]   and then someone's able to fool your fake detector
[00:56:38.900 --> 00:56:42.140]   and your fake detector says this image is not fake,
[00:56:42.140 --> 00:56:43.500]   then it's even more credible
[00:56:43.500 --> 00:56:45.100]   than if you've never made a fake detector
[00:56:45.100 --> 00:56:46.260]   in the first place.
[00:56:46.260 --> 00:56:50.380]   What I do think we'll get to is systems
[00:56:50.380 --> 00:56:53.300]   that we can kind of use behind the scenes
[00:56:53.300 --> 00:56:55.580]   to make estimates of what's going on
[00:56:55.580 --> 00:56:57.820]   and maybe not like use them in court
[00:56:57.820 --> 00:56:59.580]   for a definitive analysis.
[00:56:59.580 --> 00:57:04.180]   I also think we will likely get better authentication systems
[00:57:04.180 --> 00:57:07.380]   where, you know, imagine that every phone
[00:57:07.380 --> 00:57:10.540]   cryptographically signs everything that comes out of it.
[00:57:10.540 --> 00:57:12.820]   You wouldn't be able to conclusively tell
[00:57:12.820 --> 00:57:14.540]   that an image was real,
[00:57:14.540 --> 00:57:17.700]   but you would be able to tell somebody
[00:57:17.700 --> 00:57:21.300]   who knew the appropriate private key for this phone
[00:57:21.300 --> 00:57:24.340]   was actually able to sign this image
[00:57:24.340 --> 00:57:27.460]   and upload it to this server at this timestamp.
[00:57:27.460 --> 00:57:28.940]   - Right.
[00:57:28.940 --> 00:57:31.380]   So you could imagine maybe you make phones
[00:57:31.380 --> 00:57:34.300]   that have the private keys hardware embedded in them.
[00:57:34.300 --> 00:57:37.500]   If like a state security agency
[00:57:37.500 --> 00:57:39.260]   really wants to infiltrate the company,
[00:57:39.260 --> 00:57:40.860]   they could probably, you know,
[00:57:40.860 --> 00:57:42.540]   plant a private key of their choice
[00:57:42.540 --> 00:57:45.100]   or break open the chip and learn the private key
[00:57:45.100 --> 00:57:46.220]   or something like that.
[00:57:46.220 --> 00:57:47.460]   But it would make it a lot harder
[00:57:47.460 --> 00:57:51.500]   for an adversary with fewer resources to fake things.
[00:57:51.500 --> 00:57:52.860]   - For most of us it would be okay.
[00:57:52.860 --> 00:57:53.700]   Okay.
[00:57:53.700 --> 00:57:58.340]   So you mentioned the beer and the bar and the new ideas.
[00:57:58.340 --> 00:57:59.780]   You were able to implement this
[00:57:59.780 --> 00:58:02.900]   or come up with this new idea pretty quickly
[00:58:02.900 --> 00:58:04.420]   and implement it pretty quickly.
[00:58:04.420 --> 00:58:07.740]   Do you think there's still many such groundbreaking ideas
[00:58:07.740 --> 00:58:11.020]   in deep learning that could be developed so quickly?
[00:58:11.020 --> 00:58:13.020]   - Yeah, I do think that there are a lot of ideas
[00:58:13.020 --> 00:58:14.860]   that can be developed really quickly.
[00:58:14.860 --> 00:58:17.860]   GANs were probably a little bit of an outlier
[00:58:17.860 --> 00:58:20.220]   on the whole like one hour time scale.
[00:58:20.220 --> 00:58:24.260]   But just in terms of like low resource ideas
[00:58:24.260 --> 00:58:25.580]   where you do something really different
[00:58:25.580 --> 00:58:28.820]   on the algorithm scale and get a big payback.
[00:58:28.820 --> 00:58:31.900]   I think it's not as likely that you'll see that
[00:58:31.900 --> 00:58:34.940]   in terms of things like core machine learning technologies
[00:58:34.940 --> 00:58:36.580]   like a better classifier
[00:58:36.580 --> 00:58:38.180]   or a better reinforcement learning algorithm
[00:58:38.180 --> 00:58:39.580]   or a better generative model.
[00:58:39.580 --> 00:58:42.420]   If I had the GAN idea today,
[00:58:42.420 --> 00:58:45.260]   it would be a lot harder to prove that it was useful
[00:58:45.260 --> 00:58:46.940]   than it was back in 2014
[00:58:46.940 --> 00:58:50.100]   because I would need to get it running on something
[00:58:50.100 --> 00:58:54.060]   like ImageNet or CelebA at high resolution.
[00:58:54.060 --> 00:58:55.540]   You know, those take a while to train.
[00:58:55.540 --> 00:58:57.580]   You couldn't train it in an hour
[00:58:57.580 --> 00:59:01.020]   and know that it was something really new and exciting.
[00:59:01.020 --> 00:59:03.260]   Back in 2014, training on MNIST was enough.
[00:59:03.260 --> 00:59:06.780]   But there are other areas of machine learning
[00:59:06.780 --> 00:59:11.260]   where I think a new idea could actually be developed
[00:59:11.260 --> 00:59:13.260]   really quickly with low resources.
[00:59:13.260 --> 00:59:15.420]   - What's your intuition about what areas
[00:59:15.420 --> 00:59:17.740]   of machine learning are ripe for this?
[00:59:17.740 --> 00:59:22.740]   - Yeah, so I think fairness and interpretability
[00:59:23.140 --> 00:59:27.060]   are areas where we just really don't have any idea
[00:59:27.060 --> 00:59:29.060]   how anything should be done yet.
[00:59:29.060 --> 00:59:30.380]   Like for interpretability,
[00:59:30.380 --> 00:59:32.740]   I don't think we even have the right definitions.
[00:59:32.740 --> 00:59:36.100]   And even just defining a really useful concept,
[00:59:36.100 --> 00:59:38.140]   you don't even need to run any experiments,
[00:59:38.140 --> 00:59:40.100]   could have a huge impact on the field.
[00:59:40.100 --> 00:59:42.580]   We've seen that, for example, in differential privacy
[00:59:42.580 --> 00:59:45.340]   that Cynthia Dwork and her collaborators
[00:59:45.340 --> 00:59:48.060]   made this technical definition of privacy
[00:59:48.060 --> 00:59:50.060]   where before a lot of things are really mushy
[00:59:50.060 --> 00:59:51.620]   and then with that definition,
[00:59:51.620 --> 00:59:54.260]   you could actually design randomized algorithms
[00:59:54.260 --> 00:59:56.220]   for accessing databases and guarantee
[00:59:56.220 --> 00:59:58.860]   that they preserved individual people's privacy
[00:59:58.860 --> 01:00:01.820]   in like a mathematical quantitative sense.
[01:00:01.820 --> 01:00:05.860]   Right now, we all talk a lot about how interpretable
[01:00:05.860 --> 01:00:07.580]   different machine learning algorithms are,
[01:00:07.580 --> 01:00:09.860]   but it's really just people's opinion.
[01:00:09.860 --> 01:00:11.300]   And everybody probably has a different idea
[01:00:11.300 --> 01:00:13.860]   of what interpretability means in their head.
[01:00:13.860 --> 01:00:17.020]   If we could define some concept related to interpretability
[01:00:17.020 --> 01:00:18.780]   that's actually measurable,
[01:00:18.780 --> 01:00:20.620]   that would be a huge leap forward,
[01:00:20.620 --> 01:00:24.180]   even without a new algorithm that increases that quantity.
[01:00:24.180 --> 01:00:28.780]   And also once we had the definition of differential privacy,
[01:00:28.780 --> 01:00:31.380]   it was fast to get the algorithms that guaranteed it.
[01:00:31.380 --> 01:00:33.540]   So you could imagine once we have definitions
[01:00:33.540 --> 01:00:35.740]   of good concepts and interpretability,
[01:00:35.740 --> 01:00:37.580]   we might be able to provide the algorithms
[01:00:37.580 --> 01:00:40.540]   that have the interpretability guarantees quickly too.
[01:00:40.540 --> 01:00:46.900]   - What do you think it takes to build a system
[01:00:46.900 --> 01:00:48.660]   with human level intelligence
[01:00:48.660 --> 01:00:51.980]   as we quickly venture into the philosophical?
[01:00:51.980 --> 01:00:54.420]   So artificial general intelligence,
[01:00:54.420 --> 01:00:55.620]   what do you think it takes?
[01:00:55.620 --> 01:01:00.620]   - I think that it definitely takes better environments
[01:01:00.620 --> 01:01:03.780]   than we currently have for training agents,
[01:01:03.780 --> 01:01:05.300]   that we want them to have
[01:01:05.300 --> 01:01:08.740]   a really wide diversity of experiences.
[01:01:08.740 --> 01:01:11.780]   I also think it's gonna take really a lot of computation.
[01:01:11.780 --> 01:01:13.780]   It's hard to imagine exactly how much.
[01:01:13.780 --> 01:01:16.300]   - So you're optimistic about simulation,
[01:01:16.300 --> 01:01:18.180]   simulating a variety of environments
[01:01:18.180 --> 01:01:19.580]   as the path forward?
[01:01:19.580 --> 01:01:22.020]   - I think it's a necessary ingredient.
[01:01:22.020 --> 01:01:24.740]   Yeah, I don't think that we're going to get
[01:01:24.740 --> 01:01:27.380]   to artificial general intelligence
[01:01:27.380 --> 01:01:29.740]   by training on fixed data sets
[01:01:29.740 --> 01:01:32.140]   or by thinking really hard about the problem.
[01:01:32.140 --> 01:01:35.900]   I think that the agent really needs to interact
[01:01:35.900 --> 01:01:40.900]   and have a variety of experiences within the same lifespan.
[01:01:40.900 --> 01:01:44.140]   And today we have many different models
[01:01:44.140 --> 01:01:45.740]   that can each do one thing,
[01:01:45.740 --> 01:01:47.580]   and we tend to train them on one data set
[01:01:47.580 --> 01:01:49.020]   or one RL environment.
[01:01:49.020 --> 01:01:51.420]   Sometimes there are actually papers
[01:01:51.420 --> 01:01:53.500]   about getting one set of parameters
[01:01:53.500 --> 01:01:57.020]   to perform well in many different RL environments,
[01:01:57.020 --> 01:01:59.540]   but we don't really have anything like an agent
[01:01:59.540 --> 01:02:02.940]   that goes seamlessly from one type of experience to another
[01:02:02.940 --> 01:02:05.300]   and really integrates all the different things
[01:02:05.300 --> 01:02:08.060]   that it does over the course of its life.
[01:02:08.060 --> 01:02:10.580]   When we do see multi-agent environments,
[01:02:10.580 --> 01:02:12.420]   they tend to be,
[01:02:12.420 --> 01:02:14.700]   or so many multi-environment agents,
[01:02:14.700 --> 01:02:16.780]   they tend to be similar environments.
[01:02:16.780 --> 01:02:20.420]   Like all of them are playing like an action-based video game.
[01:02:20.420 --> 01:02:23.220]   We don't really have an agent that goes from
[01:02:23.220 --> 01:02:27.500]   playing a video game to like reading the Wall Street Journal
[01:02:27.500 --> 01:02:31.260]   to predicting how effective a molecule will be as a drug
[01:02:31.260 --> 01:02:33.220]   or something like that.
[01:02:33.220 --> 01:02:35.140]   - What do you think is a good test
[01:02:35.140 --> 01:02:36.980]   for intelligence in your view?
[01:02:36.980 --> 01:02:38.660]   There's been a lot of benchmarks,
[01:02:38.660 --> 01:02:41.700]   started with Alan Turing,
[01:02:41.700 --> 01:02:46.260]   natural conversation being a good benchmark for intelligence.
[01:02:46.260 --> 01:02:51.260]   What would Ian Goodfellow sit back
[01:02:51.260 --> 01:02:53.340]   and be really damn impressed
[01:02:53.340 --> 01:02:56.020]   if a system was able to accomplish?
[01:02:56.020 --> 01:02:58.460]   - Something that doesn't take a lot of glue
[01:02:58.460 --> 01:02:59.780]   from human engineers.
[01:02:59.780 --> 01:03:04.780]   So imagine that instead of having to go to the CIFAR website
[01:03:04.780 --> 01:03:07.940]   and download CIFAR 10
[01:03:07.940 --> 01:03:11.340]   and then write a Python script to parse it and all that,
[01:03:11.340 --> 01:03:16.340]   you could just point an agent at the CIFAR 10 problem
[01:03:16.340 --> 01:03:19.180]   and it downloads and extracts the data
[01:03:19.180 --> 01:03:22.420]   and trains a model and starts giving you predictions.
[01:03:22.420 --> 01:03:25.980]   I feel like something that doesn't need to have
[01:03:25.980 --> 01:03:28.700]   every step of the pipeline assembled for it
[01:03:28.700 --> 01:03:30.460]   definitely understands what it's doing.
[01:03:30.460 --> 01:03:32.380]   - Is AutoML moving into that direction
[01:03:32.380 --> 01:03:34.420]   or are you thinking way even bigger?
[01:03:34.420 --> 01:03:37.260]   - AutoML has mostly been moving toward,
[01:03:37.260 --> 01:03:39.940]   once we've built all the glue,
[01:03:39.940 --> 01:03:42.180]   can the machine learning system
[01:03:42.180 --> 01:03:44.340]   design the architecture really well?
[01:03:44.340 --> 01:03:45.740]   And so I'm more of saying,
[01:03:45.740 --> 01:03:49.580]   if something knows how to pre-process the data
[01:03:49.580 --> 01:03:52.340]   so that it successfully accomplishes the task,
[01:03:52.340 --> 01:03:53.500]   then it would be very hard to argue
[01:03:53.500 --> 01:03:56.220]   that it doesn't truly understand the task
[01:03:56.220 --> 01:03:58.500]   in some fundamental sense.
[01:03:58.500 --> 01:03:59.540]   And I don't necessarily know
[01:03:59.540 --> 01:04:02.260]   that that's the philosophical definition of intelligence,
[01:04:02.260 --> 01:04:03.780]   but that's something that would be really cool to build,
[01:04:03.780 --> 01:04:05.580]   that would be really useful and would impress me
[01:04:05.580 --> 01:04:08.180]   and would convince me that we've made a step forward
[01:04:08.180 --> 01:04:09.420]   in real AI.
[01:04:09.420 --> 01:04:13.380]   - So you give it the URL for Wikipedia
[01:04:13.380 --> 01:04:18.380]   and then next day expect it to be able to solve CIFAR-10.
[01:04:18.380 --> 01:04:20.820]   - Or you type in a paragraph
[01:04:20.820 --> 01:04:22.180]   explaining what you want it to do
[01:04:22.180 --> 01:04:24.780]   and it figures out what web searches it should run
[01:04:24.780 --> 01:04:28.300]   and downloads all the necessary ingredients.
[01:04:28.300 --> 01:04:33.300]   - So you have a very clear, calm way of speaking,
[01:04:33.300 --> 01:04:37.580]   no ums, easy to edit.
[01:04:37.580 --> 01:04:40.220]   I've seen comments for both you and I
[01:04:40.220 --> 01:04:44.180]   have been identified as both potentially being robots.
[01:04:44.180 --> 01:04:47.180]   If you have to prove to the world that you are indeed human,
[01:04:47.180 --> 01:04:48.180]   how would you do it?
[01:04:48.180 --> 01:04:53.180]   - I can understand thinking that I'm a robot.
[01:04:53.180 --> 01:04:57.780]   - It's the flip side of the Turing test, I think.
[01:04:57.780 --> 01:05:00.420]   - Yeah, yeah, the prove your human test.
[01:05:00.420 --> 01:05:03.580]   - Intellectually, so you have to,
[01:05:03.580 --> 01:05:07.380]   is there something that's truly unique
[01:05:07.380 --> 01:05:09.900]   in your mind as it doesn't go back
[01:05:09.900 --> 01:05:11.620]   to just natural language again,
[01:05:11.620 --> 01:05:13.860]   just being able to talk your way out of it?
[01:05:13.860 --> 01:05:17.060]   - Proving that I'm not a robot with today's technology,
[01:05:17.060 --> 01:05:18.740]   yeah, that's pretty straightforward.
[01:05:18.740 --> 01:05:20.780]   My conversation today hasn't veered off
[01:05:20.780 --> 01:05:24.380]   into talking about the stock market or something
[01:05:24.380 --> 01:05:25.940]   because it's my training data.
[01:05:25.940 --> 01:05:27.500]   But I guess more generally,
[01:05:27.500 --> 01:05:28.860]   trying to prove that something is real
[01:05:28.860 --> 01:05:31.420]   from the content alone is incredibly hard.
[01:05:31.420 --> 01:05:32.460]   That's one of the main things I've gotten
[01:05:32.460 --> 01:05:33.460]   out of my GAN research,
[01:05:33.500 --> 01:05:37.700]   that you can simulate almost anything.
[01:05:37.700 --> 01:05:39.100]   And so you have to really step back
[01:05:39.100 --> 01:05:42.260]   to a separate channel to prove that something is real.
[01:05:42.260 --> 01:05:45.540]   So I guess I should have had myself stamped
[01:05:45.540 --> 01:05:47.700]   on a blockchain when I was born or something,
[01:05:47.700 --> 01:05:48.620]   but I didn't do that.
[01:05:48.620 --> 01:05:50.820]   So according to my own research methodology,
[01:05:50.820 --> 01:05:52.980]   there's just no way to know at this point.
[01:05:52.980 --> 01:05:56.340]   - So what, last question, problem stands out for you
[01:05:56.340 --> 01:05:58.380]   that you're really excited about challenging
[01:05:58.380 --> 01:05:59.940]   in the near future?
[01:05:59.940 --> 01:06:02.940]   - So I think resistance to adversarial examples,
[01:06:02.940 --> 01:06:05.540]   figuring out how to make machine learning secure
[01:06:05.540 --> 01:06:07.500]   against an adversary who wants to interfere it
[01:06:07.500 --> 01:06:10.700]   and control it, that is one of the most important things
[01:06:10.700 --> 01:06:12.180]   researchers today could solve.
[01:06:12.180 --> 01:06:17.180]   - In all domains, image, language, driving, and everything.
[01:06:17.180 --> 01:06:19.820]   - I guess I'm most concerned about domains
[01:06:19.820 --> 01:06:22.020]   we haven't really encountered yet.
[01:06:22.020 --> 01:06:24.060]   Like imagine 20 years from now
[01:06:24.060 --> 01:06:26.340]   when we're using advanced AIs
[01:06:26.340 --> 01:06:28.980]   to do things we haven't even thought of yet.
[01:06:28.980 --> 01:06:30.660]   Like if you ask people,
[01:06:30.660 --> 01:06:35.140]   what are the important problems in security of phones
[01:06:35.140 --> 01:06:38.940]   in like 2002, I don't think we would have anticipated
[01:06:38.940 --> 01:06:42.180]   that we're using them for nearly as many things
[01:06:42.180 --> 01:06:43.660]   as we're using them for today.
[01:06:43.660 --> 01:06:44.900]   I think it's gonna be like that with AI
[01:06:44.900 --> 01:06:47.940]   that you can kind of try to speculate about where it's going
[01:06:47.940 --> 01:06:51.060]   but really the business opportunities that end up taking off
[01:06:51.060 --> 01:06:54.220]   would be hard to predict ahead of time.
[01:06:54.220 --> 01:06:56.460]   What you can predict ahead of time is that
[01:06:56.460 --> 01:06:58.380]   almost anything you can do with machine learning,
[01:06:58.380 --> 01:07:02.140]   you would like to make sure that people can't get it
[01:07:02.140 --> 01:07:04.660]   to do what they want rather than what you want
[01:07:04.660 --> 01:07:08.540]   just by showing it a funny QR code or a funny input pattern.
[01:07:08.540 --> 01:07:11.060]   - And you think that the set of methodology to do that
[01:07:11.060 --> 01:07:12.900]   can be bigger than any one domain?
[01:07:12.900 --> 01:07:14.180]   And that's the-- - I think so, yeah.
[01:07:14.180 --> 01:07:19.180]   Yeah, like one methodology that I think is,
[01:07:19.180 --> 01:07:20.700]   not a specific methodology,
[01:07:20.700 --> 01:07:22.820]   but like a category of solutions
[01:07:22.820 --> 01:07:25.740]   that I'm excited about today is making dynamic models
[01:07:25.740 --> 01:07:28.260]   that change every time they make a prediction.
[01:07:28.260 --> 01:07:31.180]   So right now we tend to train models
[01:07:31.180 --> 01:07:33.180]   and then after they're trained, we freeze them
[01:07:33.180 --> 01:07:36.300]   and we just use the same rule to classify everything
[01:07:36.300 --> 01:07:38.260]   that comes in from then on.
[01:07:38.260 --> 01:07:41.580]   That's really a sitting duck from a security point of view.
[01:07:41.580 --> 01:07:45.540]   If you always output the same answer for the same input,
[01:07:45.540 --> 01:07:48.340]   then people can just run inputs through
[01:07:48.340 --> 01:07:50.220]   until they find a mistake that benefits them.
[01:07:50.220 --> 01:07:51.820]   And then they use the same mistake
[01:07:51.820 --> 01:07:53.260]   over and over and over again.
[01:07:53.260 --> 01:07:56.580]   I think having a model that updates its predictions
[01:07:56.580 --> 01:08:00.420]   so that it's harder to predict what you're gonna get
[01:08:00.420 --> 01:08:02.820]   will make it harder for an adversary
[01:08:02.820 --> 01:08:04.900]   to really take control of the system
[01:08:04.900 --> 01:08:06.180]   and make it do what they want it to do.
[01:08:06.180 --> 01:08:09.820]   - Yeah, models that maintain a bit of a sense of mystery
[01:08:09.820 --> 01:08:12.180]   about them 'cause they always keep changing.
[01:08:12.180 --> 01:08:13.020]   - Yeah.
[01:08:13.020 --> 01:08:14.340]   - Ian, thanks so much for talking today.
[01:08:14.340 --> 01:08:15.180]   It was awesome.
[01:08:15.180 --> 01:08:16.020]   - Thank you for coming in.
[01:08:16.020 --> 01:08:17.020]   It's great to see you.
[01:08:17.020 --> 01:08:19.620]   (upbeat music)
[01:08:19.620 --> 01:08:22.220]   (upbeat music)
[01:08:22.220 --> 01:08:24.820]   (upbeat music)
[01:08:24.820 --> 01:08:27.420]   (upbeat music)
[01:08:27.420 --> 01:08:30.020]   (upbeat music)
[01:08:30.020 --> 01:08:32.620]   (upbeat music)
[01:08:32.620 --> 01:08:42.620]   [BLANK_AUDIO]

