
[00:00:00.000 --> 00:00:05.280]   There's a future where like the distinction between small and large models like disappears to some degree.
[00:00:05.280 --> 00:00:09.480]   And with long context, there's also a degree to which fine tuning might disappear, to be honest.
[00:00:09.480 --> 00:00:14.760]   Like these, these two things that are very important today, and like today's landscape models, we have like whole
[00:00:14.760 --> 00:00:17.480]   different tiers of model sizes, and we have fine-tuned models of different things.
[00:00:17.480 --> 00:00:26.960]   You can imagine a future where you just actually have a dynamic bundle of compute and like infinite context that
[00:00:26.960 --> 00:00:29.200]   specializes your model to different things.
[00:00:29.320 --> 00:00:36.040]   One of the bottlenecks for AI progress that many people identify is the inability of these models to perform
[00:00:36.040 --> 00:00:42.520]   tasks on long horizons, which means engaging with the task for many hours or even many weeks or months where
[00:00:42.520 --> 00:00:47.840]   like if I have, I don't know, an assistant or an employee or something, they can just do a thing and tell
[00:00:47.840 --> 00:00:51.720]   them for a while. And AI agents haven't taken off for this reason, from what I understand.
[00:00:51.720 --> 00:00:57.840]   So how linked are long context windows and the ability to perform well on them and the ability to do these
[00:00:57.840 --> 00:01:03.360]   kinds of long horizon tasks that require you to engage with an assignment for many hours?
[00:01:03.360 --> 00:01:04.760]   Or are these unrelated concepts?
[00:01:04.760 --> 00:01:09.760]   I mean, I would actually take issue with that being the reason that agents haven't taken off, where I think
[00:01:09.760 --> 00:01:13.080]   that's more about like lines of reliability and the model actually successfully doing things.
[00:01:13.080 --> 00:01:18.040]   If you just can't chain tasks successfully with high enough probability, then you won't get something that
[00:01:18.040 --> 00:01:23.240]   looks like an agent. And that's why something like an agent might follow more of a step function like GPT-4
[00:01:23.240 --> 00:01:28.680]   class models, Gemini Ultra class models are not enough. But maybe the next increment on model scale means
[00:01:28.680 --> 00:01:33.360]   that you get that extra nine, even though like the loss isn't going down that dramatically, that like small
[00:01:33.360 --> 00:01:40.160]   amount of extra ability gives you the extra. Obviously, you need some amount of context to fit long horizon
[00:01:40.160 --> 00:01:43.200]   tasks, but I don't think that's been the limiting factor up to now.
[00:01:43.200 --> 00:01:51.840]   Do you expect that it will be multiple copies of models talking to each other? Or will it be just adapt a
[00:01:51.840 --> 00:01:58.120]   computer solve, then the thing just like runs bigger, like more compute when it needs to do a kind of thing
[00:01:58.120 --> 00:02:03.480]   that a whole firm needs to do. And I asked this because there's two things that make me wonder about like
[00:02:03.480 --> 00:02:08.880]   whether agents is the right way to think about what will happen in the future. One is with longer context,
[00:02:08.880 --> 00:02:16.400]   these models are able to ingest and consider the information that no human can and therefore we need like
[00:02:16.400 --> 00:02:19.200]   one engineer who's thinking about the front-end code and one engineer who's thinking about the back-end
[00:02:19.200 --> 00:02:23.280]   code, where this thing can just ingest the whole thing. This sort of like Hayekian problem of
[00:02:23.280 --> 00:02:31.680]   specialization goes away. Second, these models are just like very general of you're like not using
[00:02:31.680 --> 00:02:35.520]   different types of GPT-4 to do different kinds of things. You're using the exact same model, right? So
[00:02:35.520 --> 00:02:41.440]   I wonder what that implies is in the future, like an AI firm is just like a model instead of a bunch of AI
[00:02:41.440 --> 00:02:42.480]   agents hooked together.
[00:02:42.800 --> 00:02:49.600]   That's a great question. I think especially in the near term, it will look much more like agents
[00:02:49.600 --> 00:02:55.120]   hooked together. And I say that like purely because as humans, we're going to want to have these like
[00:02:55.120 --> 00:02:59.880]   isolated, reliable and like, like, like components that we can trust.
[00:02:59.880 --> 00:03:06.160]   So if your claim is that the AI agents haven't taken off because of reliability rather than long
[00:03:06.160 --> 00:03:14.480]   horizon task performance, isn't the lack of reliability when a task is chained on top of another
[00:03:14.480 --> 00:03:19.080]   task on top of another task, isn't that exactly the difficulty with long horizon tasks? Is that like
[00:03:19.080 --> 00:03:24.640]   you have to do 10 things in a row or a hundred things in a row and diminishing the reliability of
[00:03:24.640 --> 00:03:31.960]   any one of them, or the probability goes down from 99.99 to 99.9, then like the whole thing gets
[00:03:31.960 --> 00:03:34.840]   multiplied together and the whole thing becomes much less likely to happen.
[00:03:34.880 --> 00:03:40.120]   One of the things that will be really important to do over the next however long is understand
[00:03:40.120 --> 00:03:43.040]   better what does success rate over long horizon tasks look like.
[00:03:43.040 --> 00:03:46.440]   And I think that's even important to understand what the economic impact of these models might be
[00:03:46.440 --> 00:03:51.680]   and like actually properly judge increasing capabilities, right, by cutting down the tasks that
[00:03:51.680 --> 00:03:57.000]   we do and the inputs and outputs involved into minutes or hours or days, and seeing how good it
[00:03:57.000 --> 00:04:01.680]   is successively chaining and completing tasks at those different resolutions of time, because then
[00:04:01.680 --> 00:04:07.480]   that tells you how automatable a job family or task family is in a way that like MMO use scores
[00:04:07.480 --> 00:04:07.720]   don't.
[00:04:07.720 --> 00:04:11.680]   I mean, it was less than a year ago that we introduced 100k context windows. And I think
[00:04:11.680 --> 00:04:16.880]   everyone was pretty surprised by that. So yeah, everyone would just kind of had this soundbite of
[00:04:16.880 --> 00:04:22.280]   quadratic attention costs. Yeah, we can't have long context windows. Here we are. So yeah, like
[00:04:22.280 --> 00:04:24.200]   the benchmarks are being actively made.
[00:04:24.200 --> 00:04:30.200]   One thing you can imagine is you have an AI firm or something. And the whole thing is like end to
[00:04:30.200 --> 00:04:35.880]   end trained on the signal of like, did I make profits or like, if that's like too ambiguous, if
[00:04:35.880 --> 00:04:40.160]   it's if it's an architecture firm, and they're making blueprints, did my client like the
[00:04:40.160 --> 00:04:44.080]   blueprints? And in the middle, you can imagine agents who are salespeople and agents who are
[00:04:44.080 --> 00:04:50.160]   like doing the designing agents who like do the editing, whatever. Would that kind of signal work
[00:04:50.160 --> 00:04:52.640]   on an end to end system like that?
[00:04:52.640 --> 00:04:56.320]   Yeah, in the limit? Yes. That's the dream of reinforcement learning, right? It's like, all you
[00:04:56.320 --> 00:05:01.880]   need to do is provide this extremely sparse signal. And then over enough iterations, you create the
[00:05:01.880 --> 00:05:07.000]   information that allows you to learn from that signal. But I don't expect that to be the thing
[00:05:07.000 --> 00:05:12.360]   that works first. I think this is going to require an incredible amount of care, and like diligence
[00:05:12.360 --> 00:05:17.840]   on the behalf of humans surrounding these machines, and making sure they do exactly the right thing
[00:05:17.840 --> 00:05:21.720]   and exactly what you want and giving them right signals to improve in the ways that you want.
[00:05:22.160 --> 00:05:26.800]   Yeah, you can't train on the RL reward unless the model generates some reward.
[00:05:26.800 --> 00:05:31.280]   Yeah, that's Yeah, exactly. You're in like, you're in this like sparse RL world where like, if it
[00:05:31.280 --> 00:05:34.720]   never, the client never likes what you produce, then like you don't get any reward at all. And
[00:05:34.720 --> 00:05:35.640]   like, it's kind of bad.
[00:05:35.640 --> 00:05:38.720]   But in the future, these models will be good enough to get the reward some of the time, right?
[00:05:38.720 --> 00:05:40.680]   This is the nines of reliability.

