
[00:00:00.000 --> 00:00:03.280]   In this video, we're going to do a really fun thing,
[00:00:03.280 --> 00:00:06.800]   which is I'm going to teach you how to generate text with LSTMs.
[00:00:06.800 --> 00:00:09.280]   I don't know anyone that doesn't find this compelling.
[00:00:09.280 --> 00:00:11.720]   So if you haven't been following along,
[00:00:11.720 --> 00:00:16.280]   this is one of which is super fun to follow along and generate your own text.
[00:00:16.280 --> 00:00:18.040]   And I'll show you one of my favorite examples.
[00:00:18.040 --> 00:00:19.000]   There's so many of them.
[00:00:19.000 --> 00:00:24.440]   If you've seen like crazy gerbil names or band names or human names generated by
[00:00:24.440 --> 00:00:28.480]   machine learning, in this case, it's machine learning generated pranks.
[00:00:28.480 --> 00:00:32.320]   And you can find more of them on aiweirdness.com.
[00:00:32.320 --> 00:00:34.360]   But they actually did exactly what we're doing today.
[00:00:34.360 --> 00:00:39.200]   They fed an LSTM a whole bunch of different kinds of things,
[00:00:39.200 --> 00:00:41.200]   different kinds of pranks, I guess.
[00:00:41.200 --> 00:00:43.840]   And it learned a language model around that.
[00:00:43.840 --> 00:00:46.400]   And then it generated new pranks.
[00:00:46.400 --> 00:00:47.840]   So how did this actually work?
[00:00:47.840 --> 00:00:51.760]   How did recurrent neural networks that we saw predicting time series generate
[00:00:51.760 --> 00:00:53.480]   these crazy pranks?
[00:00:53.480 --> 00:00:58.360]   What we do is we actually take text, and typically this is on a character level.
[00:00:58.360 --> 00:01:01.600]   We take each individual character in a body of text.
[00:01:01.600 --> 00:01:04.120]   We take each character and we one-hot encode it.
[00:01:04.120 --> 00:01:08.760]   And if you remember what that means is we basically make a vector where each
[00:01:08.760 --> 00:01:11.720]   element in the vector corresponds to a different character.
[00:01:11.720 --> 00:01:13.760]   And so here I have my name Lucas, right?
[00:01:13.760 --> 00:01:20.000]   So the third element is an L. And so our vector has the third element set to 1,
[00:01:20.000 --> 00:01:21.840]   and the other one 0.
[00:01:21.840 --> 00:01:25.360]   And then we say the first element of this vector, I guess, corresponds to U.
[00:01:25.360 --> 00:01:29.520]   So the second vector that goes into our network is a 1 followed by 0s.
[00:01:29.520 --> 00:01:35.280]   The fourth element is a K. So the third vector that we put in is all 0s except
[00:01:35.280 --> 00:01:38.040]   for the fourth element set to 1.
[00:01:38.040 --> 00:01:43.520]   And we feed this series of vectors into our recurrent neural network.
[00:01:43.520 --> 00:01:47.760]   And our network passes through some kind of state, which is hopefully encoding
[00:01:47.760 --> 00:01:49.280]   something about the world.
[00:01:49.280 --> 00:01:52.040]   And it's also outputting a vector.
[00:01:52.040 --> 00:01:56.400]   And this output vector is typically the same length as our input vector.
[00:01:56.400 --> 00:01:59.400]   So we can use the same encoding to know what it means.
[00:01:59.400 --> 00:02:03.120]   And in this case, the network will typically output a whole set of different
[00:02:03.120 --> 00:02:04.000]   numbers.
[00:02:04.000 --> 00:02:09.240]   But what we want it to do, what we train it to do, is output the next letter
[00:02:09.240 --> 00:02:11.720]   at each state.
[00:02:11.720 --> 00:02:16.120]   So we're actually training our network to take input letters and in each step
[00:02:16.120 --> 00:02:18.960]   predict what the next letter is going to be.
[00:02:18.960 --> 00:02:22.960]   So then when we run this network into the future after we've trained it,
[00:02:22.960 --> 00:02:28.720]   it generates plausible sounding words and pieces of text.
[00:02:28.720 --> 00:02:30.600]   OK, let's go to the code.
[00:02:30.600 --> 00:02:32.920]   First, go into the ml class directory.
[00:02:32.920 --> 00:02:35.440]   Then go into the videos directory.
[00:02:35.440 --> 00:02:37.440]   And then go into textgen.
[00:02:37.440 --> 00:02:43.560]   There's one file in here, car-gen.py.
[00:02:43.560 --> 00:02:48.000]   So lines 1 through 13, as usual, they're just importing various things
[00:02:48.000 --> 00:02:49.200]   that we're going to use.
[00:02:49.200 --> 00:02:51.960]   And lines 15 through 18 is actually a little thing
[00:02:51.960 --> 00:02:55.720]   that I set up for you so that you can easily change the input text that you
[00:02:55.720 --> 00:02:56.220]   want.
[00:02:56.220 --> 00:02:58.760]   So this code will actually take in any text
[00:02:58.760 --> 00:03:01.280]   and it'll learn a language model based on that text
[00:03:01.280 --> 00:03:04.560]   and try to predict things that look like the text that you passed in
[00:03:04.560 --> 00:03:08.120]   as the first argument as input.
[00:03:08.120 --> 00:03:12.480]   Lines 20 through 26 set up a whole bunch of configuration parameters.
[00:03:12.480 --> 00:03:15.200]   In this case, maybe the most important configuration parameter
[00:03:15.200 --> 00:03:17.720]   is actually config.maxlen, which is going
[00:03:17.720 --> 00:03:21.500]   to be the length of that sliding window of how many characters
[00:03:21.500 --> 00:03:27.400]   we look at as input into our recurrent neural network.
[00:03:27.400 --> 00:03:31.160]   Lines 28 and 29 actually open up the file
[00:03:31.160 --> 00:03:33.480]   and pull out the individual characters.
[00:03:33.480 --> 00:03:38.320]   And then lines 31 and 32 set up a mapping from the character to indices
[00:03:38.320 --> 00:03:40.720]   and then back from the indices to the character,
[00:03:40.720 --> 00:03:43.400]   setting the stage for building this one hot encoded
[00:03:43.400 --> 00:03:47.920]   vector for each character that we might see in our text.
[00:03:47.920 --> 00:03:52.080]   Lines 36 through 40 pull out that sliding window.
[00:03:52.080 --> 00:03:54.560]   And in this case, the window size, as I said, was maxlen.
[00:03:54.560 --> 00:03:59.040]   And actually, we're only going to take that window every config.step times
[00:03:59.040 --> 00:03:59.660]   that we see it.
[00:03:59.660 --> 00:04:02.080]   So we might slide that window and set it over by 1.
[00:04:02.080 --> 00:04:06.200]   In this case, we're going to slide it over by 3 at each step.
[00:04:06.200 --> 00:04:10.520]   Lines 45 through 50 actually build up that matrix
[00:04:10.520 --> 00:04:11.640]   that we were talking about.
[00:04:11.640 --> 00:04:14.440]   So in this case, we want x to be a two-dimensional thing.
[00:04:14.440 --> 00:04:16.120]   This is going to be input.
[00:04:16.120 --> 00:04:21.880]   And you can think of the rows here as being each individual character
[00:04:21.880 --> 00:04:27.080]   and the columns being the one hot encoded value of each character.
[00:04:27.080 --> 00:04:32.000]   So in this case, x is the input and y is the output that we're looking for.
[00:04:32.000 --> 00:04:36.880]   Line 52, we set up a sequential neural network architecture.
[00:04:36.880 --> 00:04:40.960]   Line 53, we add a simple recurrent neural network.
[00:04:40.960 --> 00:04:44.280]   And the important parameter we pass in here is 128.
[00:04:44.280 --> 00:04:49.000]   And if you remember, that means that this network is outputting 128 numbers.
[00:04:49.000 --> 00:04:52.000]   And it's also passing the same 128 numbers
[00:04:52.000 --> 00:04:56.680]   across to the next step in the recurrent network.
[00:04:56.680 --> 00:04:59.080]   Now, we're not outputting 128 numbers.
[00:04:59.080 --> 00:05:02.320]   We're actually outputting as many numbers as different types of characters
[00:05:02.320 --> 00:05:03.840]   that we saw in our text.
[00:05:03.840 --> 00:05:08.600]   So in order to make our output dimension match our y or the data we're
[00:05:08.600 --> 00:05:11.600]   trying to predict, we need to add a dense layer.
[00:05:11.600 --> 00:05:15.040]   And in this case, our dense layer is outputting exactly as many numbers
[00:05:15.040 --> 00:05:17.880]   as we have different types of characters in our data set.
[00:05:17.880 --> 00:05:20.640]   In other words, LEN cars.
[00:05:20.640 --> 00:05:23.280]   And we set the activation function to be softmax.
[00:05:23.280 --> 00:05:26.480]   In this case, it's really important because we actually
[00:05:26.480 --> 00:05:29.320]   are predicting categories.
[00:05:29.320 --> 00:05:31.680]   In this case, each character is a different category
[00:05:31.680 --> 00:05:33.520]   that we're trying to predict.
[00:05:33.520 --> 00:05:35.880]   And then we call model.compile.
[00:05:35.880 --> 00:05:39.160]   And we set our loss function to be categorical cross-entropy, again,
[00:05:39.160 --> 00:05:42.680]   because we're outputting categories of things.
[00:05:42.680 --> 00:05:44.640]   And here we set our optimizer to rmsprop.
[00:05:44.640 --> 00:05:46.400]   But you could also use the atom optimizer
[00:05:46.400 --> 00:05:48.680]   if you want to be consistent with previous videos.
[00:05:48.680 --> 00:05:53.680]   So LENs 58 through 65 actually has a sample function.
[00:05:53.680 --> 00:05:55.840]   And here what the sample function does is
[00:05:55.840 --> 00:05:58.960]   it makes a prediction of the next character in the text.
[00:05:58.960 --> 00:06:00.840]   But it actually adds a little bit of noise.
[00:06:00.840 --> 00:06:02.560]   So essentially, it weights the prediction
[00:06:02.560 --> 00:06:04.440]   that it's going to make by the output number,
[00:06:04.440 --> 00:06:06.120]   where the output number of this network
[00:06:06.120 --> 00:06:08.760]   is the probability that it thinks a different character is
[00:06:08.760 --> 00:06:10.160]   coming next.
[00:06:10.160 --> 00:06:13.000]   And the reason to not just output the highest probability
[00:06:13.000 --> 00:06:15.120]   answer as the next character is because it
[00:06:15.120 --> 00:06:17.160]   makes the thing more interesting if sometimes it
[00:06:17.160 --> 00:06:19.160]   throws a little bit of a curveball.
[00:06:19.160 --> 00:06:21.160]   So we want to show the types of things
[00:06:21.160 --> 00:06:23.640]   this network is predicting, not necessarily always
[00:06:23.640 --> 00:06:25.560]   the thing that this network thinks is the most
[00:06:25.560 --> 00:06:29.000]   probable next thing.
[00:06:29.000 --> 00:06:32.260]   And then sample text, actually, at the end of each epoch,
[00:06:32.260 --> 00:06:35.560]   it's a callback that makes some predictions on the text,
[00:06:35.560 --> 00:06:37.240]   one with a little bit lower diversity
[00:06:37.240 --> 00:06:39.040]   and one with a little bit higher diversity.
[00:06:39.040 --> 00:06:40.540]   So lower diversity numbers will tend
[00:06:40.540 --> 00:06:43.000]   to output more boring things or more things that are exactly
[00:06:43.000 --> 00:06:44.640]   like the input text that we saw.
[00:06:44.640 --> 00:06:46.840]   Higher diversity will make the network
[00:06:46.840 --> 00:06:49.600]   try a little bit more surprising characters.
[00:06:49.600 --> 00:06:52.160]   So we'll get a little bit maybe less realistic,
[00:06:52.160 --> 00:06:54.760]   but maybe more interesting outputs from a network.
[00:06:54.760 --> 00:06:58.520]   So you could try changing 0.5 or 1.2 here to different numbers
[00:06:58.520 --> 00:07:01.080]   and see what this network outputs for you.
[00:07:01.080 --> 00:07:04.600]   So finally, line 97 calls model.fit on x and y.
[00:07:04.600 --> 00:07:07.520]   And here, x is that one hot encoded character vector,
[00:07:07.520 --> 00:07:11.000]   and y is the next character in the sequence.
[00:07:11.000 --> 00:07:14.480]   Batch size is the usual thing of how many x's and y's
[00:07:14.480 --> 00:07:17.800]   to show the network at one batch or at one time.
[00:07:17.800 --> 00:07:19.600]   We set epochs to be 100.
[00:07:19.600 --> 00:07:21.680]   This is going to take a while to run.
[00:07:21.680 --> 00:07:23.040]   And we have two callbacks here.
[00:07:23.040 --> 00:07:25.000]   One is the typical W and B callback
[00:07:25.000 --> 00:07:26.760]   that lets us see the loss function over time
[00:07:26.760 --> 00:07:29.320]   and lets us view the data as it runs.
[00:07:29.320 --> 00:07:30.940]   And then also sample text, which is actually
[00:07:30.940 --> 00:07:34.440]   going to generate some examples of the text based on the input
[00:07:34.440 --> 00:07:35.160]   data that we see.
[00:07:35.160 --> 00:07:42.800]   So let's try running cargen.py.
[00:07:42.800 --> 00:07:44.760]   I have a whole bunch of interesting different text
[00:07:44.760 --> 00:07:45.760]   files that you can run this on.
[00:07:45.760 --> 00:07:48.820]   So I have book.txt, which is some free novel
[00:07:48.820 --> 00:07:52.040]   that I got of Gutenberg's free book collection.
[00:07:52.040 --> 00:07:53.920]   I also have female.txt and male.txt,
[00:07:53.920 --> 00:07:56.800]   which is actually lists of male and female names.
[00:07:56.800 --> 00:07:59.320]   And then I have lucascode.txt, which is actually
[00:07:59.320 --> 00:08:01.800]   all the code that I wrote in this repository.
[00:08:01.800 --> 00:08:05.280]   So you can maybe feed this in and see if you can generate code.
[00:08:05.280 --> 00:08:07.080]   But let's take an easy example.
[00:08:07.080 --> 00:08:10.280]   Let's try running cargen on male.txt,
[00:08:10.280 --> 00:08:12.480]   which is a list of boy names.
[00:08:12.480 --> 00:08:16.280]   And then this network will try to predict new male names.
[00:08:16.280 --> 00:08:22.760]   So you can see that we feed into this network
[00:08:22.760 --> 00:08:26.000]   as input this chunk of lists of names.
[00:08:26.000 --> 00:08:28.200]   There's actually a raw 200 characters,
[00:08:28.200 --> 00:08:31.440]   including new lines, out of male.txt.
[00:08:31.440 --> 00:08:33.400]   And then the network starts to make predictions.
[00:08:33.400 --> 00:08:35.760]   And at first, it outputs total gibberish.
[00:08:35.760 --> 00:08:39.400]   There's no pattern to what this network is outputting.
[00:08:39.400 --> 00:08:42.120]   This network will take you a little while to run.
[00:08:42.120 --> 00:08:44.720]   But it'll start to generate somewhat reasonable
[00:08:44.720 --> 00:08:50.120]   seeming boy names, like Nir or Idar or Guwente.
[00:08:50.120 --> 00:08:52.200]   But actually, there is some weird things
[00:08:52.200 --> 00:08:53.160]   that it's not learning.
[00:08:53.160 --> 00:08:56.160]   For example, in this data set, every single line
[00:08:56.160 --> 00:08:57.760]   starts with a single capital letter.
[00:08:57.760 --> 00:09:00.160]   And that's the only place that there's a capital letter.
[00:09:00.160 --> 00:09:01.920]   But this network is actually having trouble
[00:09:01.920 --> 00:09:02.680]   even knowing that.
[00:09:02.680 --> 00:09:04.100]   You'll see capital letters that are
[00:09:04.100 --> 00:09:06.960]   inserted in the middle of the text, in the middle of names.
[00:09:06.960 --> 00:09:09.200]   There's nowhere that we said you couldn't do that.
[00:09:09.200 --> 00:09:10.740]   So maybe it makes sense that it is doing that.
[00:09:10.740 --> 00:09:12.440]   But it does seem a little weird that it
[00:09:12.440 --> 00:09:17.080]   hasn't learned these kind of simple patterns from our data.
[00:09:17.080 --> 00:09:19.160]   So here's where an LSTM is actually
[00:09:19.160 --> 00:09:20.640]   going to make a big improvement.
[00:09:20.640 --> 00:09:22.880]   So what's the problem with simple recurrent neural
[00:09:22.880 --> 00:09:23.920]   networks?
[00:09:23.920 --> 00:09:25.720]   The problem is it's really hard for them
[00:09:25.720 --> 00:09:28.600]   to encode long-range dependencies.
[00:09:28.600 --> 00:09:30.880]   And it has to do with the way the network is actually
[00:09:30.880 --> 00:09:31.720]   constructed.
[00:09:31.720 --> 00:09:34.520]   So in this case, inside of each recurrent neural network
[00:09:34.520 --> 00:09:38.500]   is a perceptron that's dependent on the previous state
[00:09:38.500 --> 00:09:40.960]   and the input constructing the next state.
[00:09:40.960 --> 00:09:43.300]   So all it really is is a weighted sum
[00:09:43.300 --> 00:09:45.440]   and then an activation function applied
[00:09:45.440 --> 00:09:49.580]   to the input at that moment and the previous state.
[00:09:49.580 --> 00:09:51.200]   And so it's very hard for a network
[00:09:51.200 --> 00:09:55.280]   to keep the same state for a long period of time.
[00:09:55.280 --> 00:09:59.880]   Because if it's multiplying an input state by 0.99,
[00:09:59.880 --> 00:10:02.320]   if you apply that multiplication 20 times,
[00:10:02.320 --> 00:10:04.400]   it's actually going to be a pretty small number.
[00:10:04.400 --> 00:10:08.560]   So if we have an input at one state of a left parenthesis,
[00:10:08.560 --> 00:10:11.480]   we know that maybe 15, 20, 30 characters out,
[00:10:11.480 --> 00:10:13.800]   we're probably going to see a right parenthesis.
[00:10:13.800 --> 00:10:15.280]   But it's very hard for that network
[00:10:15.280 --> 00:10:18.680]   to remember that it saw a left parenthesis over more
[00:10:18.680 --> 00:10:19.960]   than four, five, six states.
[00:10:19.960 --> 00:10:22.960]   Because it keeps multiplying it by the same number, whatever
[00:10:22.960 --> 00:10:26.000]   that parameter is on the input state.
[00:10:26.000 --> 00:10:29.480]   Back in 1997, long short-term memory networks
[00:10:29.480 --> 00:10:31.200]   were invented to solve this problem,
[00:10:31.200 --> 00:10:34.040]   to be able to encode long-term state and also
[00:10:34.040 --> 00:10:35.320]   short-term state.
[00:10:35.320 --> 00:10:37.840]   And they're usually presented with a confusing set
[00:10:37.840 --> 00:10:39.000]   of equations.
[00:10:39.000 --> 00:10:41.320]   And what I'm going to try to do is
[00:10:41.320 --> 00:10:43.260]   explain what these equations mean
[00:10:43.260 --> 00:10:46.640]   and also give you some intuition around how LSTMs work
[00:10:46.640 --> 00:10:48.440]   and how you might modify them or where
[00:10:48.440 --> 00:10:50.040]   you might want to use them.
[00:10:50.040 --> 00:10:53.280]   So first of all, let's back up and look at what an LSTM is
[00:10:53.280 --> 00:10:56.040]   without digging into exactly what it does.
[00:10:56.040 --> 00:10:58.760]   So just like the simple recurrent neural network,
[00:10:58.760 --> 00:11:02.880]   it takes in an input vector and it outputs a vector.
[00:11:02.880 --> 00:11:06.440]   Now, one difference is that LSTMs actually
[00:11:06.440 --> 00:11:08.120]   maintain some state that they pass
[00:11:08.120 --> 00:11:11.940]   into the next iteration that's different than what
[00:11:11.940 --> 00:11:12.840]   they output.
[00:11:12.840 --> 00:11:14.440]   So they're actually outputting a value
[00:11:14.440 --> 00:11:16.560]   and they're passing along a different value
[00:11:16.560 --> 00:11:18.160]   to the next iteration that's different
[00:11:18.160 --> 00:11:19.880]   than a recurrent neural network.
[00:11:19.880 --> 00:11:21.800]   So the way LSTMs work is they really
[00:11:21.800 --> 00:11:25.800]   do four independent computations involving perceptrons
[00:11:25.800 --> 00:11:28.000]   with different sets of parameters.
[00:11:28.000 --> 00:11:31.160]   And then they use the output of those computations
[00:11:31.160 --> 00:11:35.280]   to decide what to pass to the next step in the LSTM
[00:11:35.280 --> 00:11:37.080]   and what to output.
[00:11:37.080 --> 00:11:39.320]   And actually, each one of these computations
[00:11:39.320 --> 00:11:42.120]   has an intuition built around it.
[00:11:42.120 --> 00:11:44.560]   So the first computation that this LSTM does
[00:11:44.560 --> 00:11:45.440]   is what to forget.
[00:11:45.440 --> 00:11:49.640]   And this is actually usually put in a variable called f sub t.
[00:11:49.640 --> 00:11:52.280]   And the f really does stand for forget.
[00:11:52.280 --> 00:11:55.440]   So this really is a perceptron where the input is actually
[00:11:55.440 --> 00:11:57.840]   the input from the top and the state
[00:11:57.840 --> 00:12:00.000]   from the previous step in the LSTM.
[00:12:00.000 --> 00:12:02.680]   So it takes those two and it concatenates them into a vector
[00:12:02.680 --> 00:12:04.120]   and flattens them out.
[00:12:04.120 --> 00:12:06.920]   And then it uses a dense layer or a perceptron
[00:12:06.920 --> 00:12:09.920]   to output exactly the same number of numbers
[00:12:09.920 --> 00:12:12.440]   as the state vector that came in from the previous step
[00:12:12.440 --> 00:12:13.800]   in the LSTM.
[00:12:13.800 --> 00:12:16.760]   And it typically uses a sigmoid activation function here.
[00:12:16.760 --> 00:12:19.600]   And the reason is that basically where this thing is outputting
[00:12:19.600 --> 00:12:21.400]   1's, it's going to keep values.
[00:12:21.400 --> 00:12:23.360]   And where it's going to output 0's,
[00:12:23.360 --> 00:12:25.120]   it's going to forget those values.
[00:12:25.120 --> 00:12:29.240]   So you'll see how that works in a second.
[00:12:29.240 --> 00:12:32.000]   The second computation that an LSTM does
[00:12:32.000 --> 00:12:35.320]   is it sort of decides what things to update.
[00:12:35.320 --> 00:12:37.560]   And this is exactly the same as the computation
[00:12:37.560 --> 00:12:39.400]   of what things to forget, but it uses
[00:12:39.400 --> 00:12:41.000]   a different set of parameters.
[00:12:41.000 --> 00:12:44.520]   So we take as input the input value from above
[00:12:44.520 --> 00:12:47.080]   and the state value from the previous step in the LSTM.
[00:12:47.080 --> 00:12:51.080]   And we output a different vector the size of the state vector.
[00:12:51.080 --> 00:12:53.240]   In this case, the interpretation is 1
[00:12:53.240 --> 00:12:55.140]   are things that the network should update,
[00:12:55.140 --> 00:12:58.440]   and 0's are things that it doesn't want to update.
[00:12:58.440 --> 00:13:01.780]   The third calculation that it does
[00:13:01.780 --> 00:13:05.720]   is it decides kind of what new values it might want to use.
[00:13:05.720 --> 00:13:08.000]   So this is typically put in a variable called
[00:13:08.000 --> 00:13:12.440]   C with a tilde on the top, sub T.
[00:13:12.440 --> 00:13:15.200]   And this is exactly the same looking calculation,
[00:13:15.200 --> 00:13:16.800]   but it's a different set of parameters
[00:13:16.800 --> 00:13:18.360]   and a different set of outputs.
[00:13:18.360 --> 00:13:21.440]   And typically here, it doesn't use a sigmoid activation
[00:13:21.440 --> 00:13:22.880]   function in the perceptron.
[00:13:22.880 --> 00:13:27.160]   It uses that inverse tangent that we talked about.
[00:13:27.160 --> 00:13:28.620]   The important thing to remember is
[00:13:28.620 --> 00:13:31.680]   that that can output numbers from negative 1 to 1.
[00:13:31.680 --> 00:13:33.560]   So these aren't just numbers between 0 and 1.
[00:13:33.560 --> 00:13:35.720]   They're numbers that could be from negative 1 to 1.
[00:13:35.720 --> 00:13:38.280]   The final calculation that a perceptron does
[00:13:38.280 --> 00:13:41.880]   is it decides basically what it should be outputting.
[00:13:41.880 --> 00:13:44.320]   So this takes in as values, again,
[00:13:44.320 --> 00:13:47.040]   the input concatenated with the previous state
[00:13:47.040 --> 00:13:48.780]   and outputs another set of numbers
[00:13:48.780 --> 00:13:51.560]   that's the same size as the state that it's passing through.
[00:13:51.560 --> 00:13:54.240]   And this is kind of telling it what numbers it should output
[00:13:54.240 --> 00:13:57.600]   to the next layer in the LSTM or into the dense layer that's
[00:13:57.600 --> 00:14:00.560]   going to finally determine what we're predicting.
[00:14:00.560 --> 00:14:03.720]   And we put this all together in a formula that I've written out.
[00:14:03.720 --> 00:14:08.160]   So the next state is actually equal to the previous state
[00:14:08.160 --> 00:14:09.760]   times the forget vector.
[00:14:09.760 --> 00:14:11.680]   So this is an element-wise times.
[00:14:11.680 --> 00:14:14.160]   So it's not a matrix vector multiplication.
[00:14:14.160 --> 00:14:18.200]   It's basically doing an element-wise multiplication.
[00:14:18.200 --> 00:14:21.500]   So the things where the forget vector is set to 0,
[00:14:21.500 --> 00:14:23.120]   those states are essentially removed.
[00:14:23.120 --> 00:14:25.520]   And the things where the forget vector is set to 1,
[00:14:25.520 --> 00:14:29.360]   those states are kept and passed into the next state.
[00:14:29.360 --> 00:14:32.360]   And then it adds to that previous state.
[00:14:32.360 --> 00:14:37.120]   And the update calculation times the new candidate states.
[00:14:37.120 --> 00:14:40.880]   So where the update activation is set to 0,
[00:14:40.880 --> 00:14:42.540]   it doesn't use the new candidate things.
[00:14:42.540 --> 00:14:44.720]   It just keeps what it had before.
[00:14:44.720 --> 00:14:46.840]   And where the update activation is set to 1,
[00:14:46.840 --> 00:14:52.520]   it passes across the complete value of the candidate updates.
[00:14:52.520 --> 00:14:55.360]   So how does the LSTM use these four values
[00:14:55.360 --> 00:14:58.160]   to decide what to pass into the next step in the LSTM
[00:14:58.160 --> 00:15:00.040]   and what to output?
[00:15:00.040 --> 00:15:02.200]   Well, the way it decides the next state
[00:15:02.200 --> 00:15:03.520]   is it takes its previous state.
[00:15:03.520 --> 00:15:04.960]   And the first thing it does is it
[00:15:04.960 --> 00:15:07.960]   multiplies the previous state by the forget vector.
[00:15:07.960 --> 00:15:09.680]   And so what happens here is basically
[00:15:09.680 --> 00:15:11.880]   where the forget vector is set to 0,
[00:15:11.880 --> 00:15:13.440]   those elements basically get removed.
[00:15:13.440 --> 00:15:16.760]   They get erased because anything times 0 is 0.
[00:15:16.760 --> 00:15:19.240]   And where the forget vector is set to 1,
[00:15:19.240 --> 00:15:22.280]   it keeps those elements because anything times 1 is itself.
[00:15:22.280 --> 00:15:28.760]   It takes that result, and it adds the update activation
[00:15:28.760 --> 00:15:31.480]   times the candidate vector.
[00:15:31.480 --> 00:15:33.840]   And so here, where the update activation is 0,
[00:15:33.840 --> 00:15:35.500]   it kind of ignores the candidate vector.
[00:15:35.500 --> 00:15:37.560]   It basically passes through whatever it had.
[00:15:37.560 --> 00:15:39.400]   And where the update activation is set to 1,
[00:15:39.400 --> 00:15:43.600]   it does a complete update with some new value.
[00:15:43.600 --> 00:15:47.600]   So in this way, it's really easy for an LSTM
[00:15:47.600 --> 00:15:49.400]   to encode long-term memory.
[00:15:49.400 --> 00:15:52.000]   As long as that update activation is set to 0,
[00:15:52.000 --> 00:15:55.080]   and as long as the forget value is set to 1,
[00:15:55.080 --> 00:15:56.840]   that means that this thing is actually
[00:15:56.840 --> 00:15:58.840]   just passing into the next state exactly what
[00:15:58.840 --> 00:16:01.000]   it saw in the previous state.
[00:16:01.000 --> 00:16:03.920]   Then it actually outputs a value that's
[00:16:03.920 --> 00:16:07.160]   the output activation times typically
[00:16:07.160 --> 00:16:09.520]   the hyperbolic tangent of what it's passing through
[00:16:09.520 --> 00:16:11.920]   into the next state.
[00:16:11.920 --> 00:16:15.640]   So then the output is actually the output activation vector
[00:16:15.640 --> 00:16:19.400]   times an activation function of the next state
[00:16:19.400 --> 00:16:20.560]   that it's passing through.
[00:16:20.560 --> 00:16:24.080]   So what this means is that the LSTM doesn't actually always
[00:16:24.080 --> 00:16:26.600]   have to output the same thing that is passing
[00:16:26.600 --> 00:16:29.920]   into the next value in the LSTM.
[00:16:29.920 --> 00:16:32.280]   This gives it more power to do different things.
[00:16:32.280 --> 00:16:34.520]   One more quick aside is that you'll often
[00:16:34.520 --> 00:16:36.800]   see the hard sigmoid function used instead
[00:16:36.800 --> 00:16:37.800]   of the sigmoid function.
[00:16:37.800 --> 00:16:39.600]   And so what this means is it's basically
[00:16:39.600 --> 00:16:42.280]   almost like a discretized version of the sigmoid, which
[00:16:42.280 --> 00:16:45.360]   really lets the LSTM pass out 1's or 0's in a way
[00:16:45.360 --> 00:16:48.560]   that you'd never get exactly a 1 or exactly a 0
[00:16:48.560 --> 00:16:49.640]   from the sigmoid function.
[00:16:49.640 --> 00:16:52.800]   So actually, Keras uses this internally in the LSTM
[00:16:52.800 --> 00:16:54.180]   instead of the sigmoid function.
[00:16:54.180 --> 00:16:56.180]   And it can make the computation a little easier.
[00:16:56.180 --> 00:16:59.040]   So if all of these details feel really crazy to you,
[00:16:59.040 --> 00:17:01.300]   and you're wondering where they came from,
[00:17:01.300 --> 00:17:03.160]   it turns out that there is some research that
[00:17:03.160 --> 00:17:05.440]   says that they're actually all not really necessary.
[00:17:05.440 --> 00:17:08.600]   There are lots of ways to modify these calculations.
[00:17:08.600 --> 00:17:11.640]   And LSTMs still work reasonably well.
[00:17:11.640 --> 00:17:13.680]   But these calculations have some good properties.
[00:17:13.680 --> 00:17:16.600]   It allows the LSTM to keep long-term memory, which
[00:17:16.600 --> 00:17:17.720]   is super important.
[00:17:17.720 --> 00:17:20.600]   And it also allows us to efficiently do the back
[00:17:20.600 --> 00:17:21.840]   propagation calculations.
[00:17:21.840 --> 00:17:23.440]   So you can imagine that calculating
[00:17:23.440 --> 00:17:26.260]   all these parameters inside of our LSTM
[00:17:26.260 --> 00:17:28.160]   across a lot of different states could
[00:17:28.160 --> 00:17:29.400]   be an expensive calculation.
[00:17:29.400 --> 00:17:30.440]   And it is.
[00:17:30.440 --> 00:17:33.080]   And so these operations that we have
[00:17:33.080 --> 00:17:36.360]   are really optimized to making that calculation as
[00:17:36.360 --> 00:17:37.760]   efficient as possible.
[00:17:37.760 --> 00:17:39.640]   And you'll appreciate that when you run LSTMs,
[00:17:39.640 --> 00:17:41.840]   because even with this optimization,
[00:17:41.840 --> 00:17:45.560]   they're still pretty slow to find the optimal values.
[00:17:45.560 --> 00:17:47.280]   Here's the really important takeaway
[00:17:47.280 --> 00:17:49.600]   to know about LSTMs before we go back to the code, which
[00:17:49.600 --> 00:17:54.400]   is that LSTMs tend to be more powerful than simple RNNs.
[00:17:54.400 --> 00:17:57.800]   And they're much better at finding long-term dependencies.
[00:17:57.800 --> 00:18:00.400]   But they have more parameters and can be slower
[00:18:00.400 --> 00:18:04.320]   and can sometimes be more prone for overfitting.
[00:18:04.320 --> 00:18:09.040]   But it's actually really easy to add LSTMs to your code.
[00:18:09.040 --> 00:18:12.720]   If I go into CarGen and I scroll down,
[00:18:12.720 --> 00:18:19.480]   where I see simple RNN here, I just change that to LSTM.
[00:18:19.480 --> 00:18:21.120]   I save my file.
[00:18:21.120 --> 00:18:23.800]   I kill my program.
[00:18:23.800 --> 00:18:27.600]   And now I can train my LSTM on male names
[00:18:27.600 --> 00:18:30.480]   and see if it can learn more intricate, more expressive
[00:18:30.480 --> 00:18:32.160]   patterns.
[00:18:32.160 --> 00:18:33.800]   If you let this LSTM run for a while--
[00:18:33.800 --> 00:18:35.040]   and I really do mean a while.
[00:18:35.040 --> 00:18:38.200]   I think this took me 15 minutes to actually get this data out--
[00:18:38.200 --> 00:18:41.120]   you'll see that it learns much more certainty
[00:18:41.120 --> 00:18:42.160]   about boys' names.
[00:18:42.160 --> 00:18:44.000]   The names are more realistic.
[00:18:44.000 --> 00:18:46.080]   And it's not making basic mistakes.
[00:18:46.080 --> 00:18:48.640]   It's actually never putting a capital letter
[00:18:48.640 --> 00:18:49.600]   after the first letter.
[00:18:49.600 --> 00:18:52.280]   And it's very rarely forgetting to capitalize
[00:18:52.280 --> 00:18:53.720]   the first letter in the name.
[00:18:53.720 --> 00:18:57.280]   In fact, because my input data was in alphabetical order,
[00:18:57.280 --> 00:19:01.400]   it funnily outputs boys' names in alphabetical order.
[00:19:01.400 --> 00:19:05.000]   So here we happen to give it as seed data things in H's.
[00:19:05.000 --> 00:19:09.400]   So we're getting things like Harare, Hirsir, Harame.
[00:19:09.400 --> 00:19:13.560]   These really do feel like somewhat realistic boys' names.
[00:19:13.560 --> 00:19:16.560]   And I do have some other fun files in here.
[00:19:16.560 --> 00:19:19.480]   So I have, like I mentioned, female names.
[00:19:19.480 --> 00:19:20.480]   I also have my code.
[00:19:20.480 --> 00:19:22.140]   So you can actually run this on my code
[00:19:22.140 --> 00:19:24.120]   and see if you can generate working code.
[00:19:24.120 --> 00:19:26.160]   This will be fun homework for you.
[00:19:26.160 --> 00:19:28.000]   But before we go, I actually want
[00:19:28.000 --> 00:19:31.120]   to show you one more type of recurrent neural network
[00:19:31.120 --> 00:19:33.160]   that you should know about, which is GRUs,
[00:19:33.160 --> 00:19:35.360]   or gated recurrent unit.
[00:19:35.360 --> 00:19:37.720]   LSTMs got super popular a few years ago.
[00:19:37.720 --> 00:19:39.800]   But they were actually invented a long time ago.
[00:19:39.800 --> 00:19:42.280]   And so some people went back and looked at the assumptions
[00:19:42.280 --> 00:19:43.960]   behind LSTMs and wondered if they
[00:19:43.960 --> 00:19:46.640]   could make something more efficient or more principled.
[00:19:46.640 --> 00:19:49.440]   So a gated recurrent unit is actually
[00:19:49.440 --> 00:19:53.480]   a simplified version of an LSTM that can often train faster
[00:19:53.480 --> 00:19:56.440]   and give you equivalent performance.
[00:19:56.440 --> 00:19:59.360]   So you can see that there are actually less equations here.
[00:19:59.360 --> 00:20:01.640]   And there's really two important differences.
[00:20:01.640 --> 00:20:04.280]   The first thing to know that's a major difference
[00:20:04.280 --> 00:20:07.000]   is that we're not passing a different value
[00:20:07.000 --> 00:20:07.880]   than we're outputting.
[00:20:07.880 --> 00:20:09.340]   So actually, the output value here
[00:20:09.340 --> 00:20:13.080]   is the same value that the GRU passes into the next step.
[00:20:13.080 --> 00:20:14.680]   So that's not true of an LSTM.
[00:20:14.680 --> 00:20:18.060]   But it is true of a gated recurrent unit.
[00:20:18.060 --> 00:20:21.160]   The second thing is that we only have three vectors
[00:20:21.160 --> 00:20:23.440]   that we're calculating, not four.
[00:20:23.440 --> 00:20:26.040]   And the reason is that if you go back to an LSTM,
[00:20:26.040 --> 00:20:28.080]   it's sort of deciding what to forget.
[00:20:28.080 --> 00:20:30.200]   And it's deciding what to update.
[00:20:30.200 --> 00:20:32.480]   And that might seem a little bit redundant.
[00:20:32.480 --> 00:20:35.280]   Because if you're forgetting it and you're not updating it,
[00:20:35.280 --> 00:20:37.040]   what are you really doing?
[00:20:37.040 --> 00:20:39.120]   So in this case, it sort of merges
[00:20:39.120 --> 00:20:42.520]   that update vector and that forget vector
[00:20:42.520 --> 00:20:44.600]   into a single vector.
[00:20:44.600 --> 00:20:48.000]   So it basically has the update activation.
[00:20:48.000 --> 00:20:51.160]   And that's set to basically be 1 minus the forget value.
[00:20:51.160 --> 00:20:53.760]   In this case, it's called z of t for some reason.
[00:20:53.760 --> 00:20:56.480]   But it's one number that basically decides what to forget
[00:20:56.480 --> 00:20:57.880]   and what to update.
[00:20:57.880 --> 00:21:02.200]   So also very easy to try out gated recurrent units.
[00:21:02.200 --> 00:21:03.320]   We can go back into the code.
[00:21:03.320 --> 00:21:08.720]   And where we see LSTM, we just change it to GRU.
[00:21:08.720 --> 00:21:12.560]   And that adds a gated recurrent unit layer.
[00:21:12.560 --> 00:21:14.760]   And actually, when I run this, I find
[00:21:14.760 --> 00:21:17.000]   kind of what you'd expect from some of the theory
[00:21:17.000 --> 00:21:19.100]   or some of the results, which is that it basically
[00:21:19.100 --> 00:21:22.840]   runs about as well as an LSTM and trains a little bit faster.
[00:21:22.840 --> 00:21:28.120]   So today, we went through exactly how simple RNNs, LSTMs,
[00:21:28.120 --> 00:21:29.400]   and GRUs work.
[00:21:29.400 --> 00:21:31.400]   So hopefully, you have an intuition
[00:21:31.400 --> 00:21:34.840]   about where you can apply them and why you should use them.
[00:21:34.840 --> 00:21:37.460]   But maybe more importantly, we saw for the first time
[00:21:37.460 --> 00:21:41.040]   how to take text and turn it into time series data
[00:21:41.040 --> 00:21:44.040]   and how to take that time series data and make predictions
[00:21:44.040 --> 00:21:46.640]   and then how to use those predictions to make
[00:21:46.640 --> 00:21:51.800]   crazy pranks or internet memes generated by machine learning.
[00:21:51.800 --> 00:21:54.880]   So I think this is one of the most fun interactive sections.
[00:21:54.880 --> 00:21:56.760]   And you can take this code-- you can literally
[00:21:56.760 --> 00:21:59.160]   take this code and take any data set
[00:21:59.160 --> 00:22:01.360]   that you want in any kind of character encoding
[00:22:01.360 --> 00:22:03.880]   in any language you want and generate
[00:22:03.880 --> 00:22:06.480]   all kinds of interesting predictions.
[00:22:06.480 --> 00:22:08.440]   And if you do make some interesting predictions,
[00:22:08.440 --> 00:22:09.400]   please send them to me.
[00:22:09.400 --> 00:22:12.080]   I would love to see what you come up with.
[00:22:12.080 --> 00:22:22.080]   [BLANK_AUDIO]

