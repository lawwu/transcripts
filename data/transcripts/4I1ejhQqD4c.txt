
[00:00:00.000 --> 00:00:03.120]   So should we expect the next version of Fast.ai to be in a new language?
[00:00:03.120 --> 00:00:05.160]   Have you thought about moving away from Python?
[00:00:05.160 --> 00:00:11.400]   Oh, I mean, obviously I have, because I looked at Swift, you know, and sadly, you know, Chris
[00:00:11.400 --> 00:00:14.640]   Latner left Google.
[00:00:14.640 --> 00:00:18.360]   So I don't know, you know, they've got some good folks still there.
[00:00:18.360 --> 00:00:19.800]   Maybe they'll make something great of it.
[00:00:19.800 --> 00:00:26.920]   But, but, you know, I tend to kind of follow people, like, you know, people who have been
[00:00:26.920 --> 00:00:29.980]   successful many times, and Chris was one of those people.
[00:00:29.980 --> 00:00:30.980]   So yeah, I mean, what's next?
[00:00:30.980 --> 00:00:35.960]   I don't know, like, it's certainly like Python is not the future of machine learning.
[00:00:35.960 --> 00:00:41.640]   It can't be, you know, it's, it's so nicely hackable.
[00:00:41.640 --> 00:00:48.720]   But it's so frustrating to work with a language where you can't do anything fast enough, unless
[00:00:48.720 --> 00:00:56.280]   you, you know, call out to some external cruder or C code.
[00:00:56.280 --> 00:01:00.920]   And you can't run anything in parallel, unless you like put it on a whole other process.
[00:01:00.920 --> 00:01:05.240]   Like, I find working with Python, there's just so much overhead in my, in my brain to
[00:01:05.240 --> 00:01:10.760]   try to get it to work fast enough.
[00:01:10.760 --> 00:01:14.680]   It's obviously fine for a lot of things, but not really in the deep learning world, or
[00:01:14.680 --> 00:01:17.560]   not really in the machine learning world.
[00:01:17.560 --> 00:01:22.960]   So like, I really hope that Julia is really successful, because like, there's a language
[00:01:22.960 --> 00:01:26.480]   with a nicely designed type system, and a nicely designed dispatch system, and most
[00:01:26.480 --> 00:01:30.640]   importantly, it's kind of Julia all the way down.
[00:01:30.640 --> 00:01:39.320]   So you can get in and write your GPU kernel in, in, in Julia, or you can, you know, the,
[00:01:39.320 --> 00:01:42.960]   all the basic stuff is implemented in Julia all the way down until you hit the LLVM.
[00:01:42.960 --> 00:01:45.240]   So this is an embarrassing question.
[00:01:45.240 --> 00:01:46.720]   Is Julia kind of like MATLAB?
[00:01:46.720 --> 00:01:48.680]   Is that what I should be thinking?
[00:01:48.680 --> 00:01:56.920]   It was designed to be something that MATLAB people could, could use, but no, it's more
[00:01:56.920 --> 00:02:04.360]   like, I don't know, like Common Lisp meets MATLAB meets Python.
[00:02:04.360 --> 00:02:10.240]   Sounds a little bit like R, maybe.
[00:02:10.240 --> 00:02:19.400]   You see, R has some nice ideas, but the, you know, the R object system, this, I mean, A,
[00:02:19.400 --> 00:02:26.000]   there's too many of them, and B, they're all such a hack, and then C, it's, because it's
[00:02:26.000 --> 00:02:27.480]   so dynamic, it's very slow.
[00:02:27.480 --> 00:02:30.800]   So again, you have to implement everything in something that's not R, and R just becomes
[00:02:30.800 --> 00:02:32.320]   a glue language on top of it.
[00:02:32.320 --> 00:02:36.680]   I mean, I spent so, so many years writing, writing R, and it's certainly better than
[00:02:36.680 --> 00:02:39.720]   what came before, but I never enjoyed it.
[00:02:39.720 --> 00:02:49.160]   So Julia is a compiled language, and it's got a rich type system, and it's entirely
[00:02:49.160 --> 00:02:53.600]   based on function dispatch using the type system.
[00:02:53.600 --> 00:02:59.240]   It's got a very strong kind of metaprogramming approach, so that's why you can write your
[00:02:59.240 --> 00:03:02.960]   CUDA kernel in Julia, for example.
[00:03:02.960 --> 00:03:07.720]   You know, it's, it's got AutoGrad, again, it's written in Julia.
[00:03:07.720 --> 00:03:16.920]   It's got a lot of nice features, but unfortunately it hasn't really got the corporate buy-in
[00:03:16.920 --> 00:03:19.000]   yet.
[00:03:19.000 --> 00:03:26.280]   So it's highly reliant on a kind of this core group of super smart people that started it,
[00:03:26.280 --> 00:03:30.800]   and now run Julia computing, which doesn't seem to have a business model, as far as I
[00:03:30.800 --> 00:03:37.640]   can tell, other than keep getting funding from VCs, which works for a while, but at
[00:03:37.640 --> 00:03:54.080]   some point stops.
[00:03:54.080 --> 00:03:56.660]   (upbeat music)

