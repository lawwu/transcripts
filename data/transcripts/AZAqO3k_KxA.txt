
[00:00:00.000 --> 00:00:05.280]   My worry is that energy becomes too cheap and people at very low cost can destroy things
[00:00:05.280 --> 00:00:11.520]   rather easily. So say if destroying a city with a nuclear weapon cost $50,000,
[00:00:11.520 --> 00:00:18.320]   what would the world look like? I'm just not sure. It might be more stable than we think,
[00:00:18.320 --> 00:00:22.720]   but I'm greatly worried and I could readily imagine it falling apart. There's uncertainty
[00:00:22.720 --> 00:00:27.440]   about a lot of things and AI will help us with those other uncertainties. So Annette,
[00:00:27.440 --> 00:00:32.080]   do you think more intelligence is likely to be good or bad, including against x-risk? And I
[00:00:32.080 --> 00:00:37.120]   think it's more likely to be good. So if it were the only risk, I'd be more worried about it than
[00:00:37.120 --> 00:00:41.200]   if there's a whole multitude of risks. But clearly there's a whole multitude of risks.
[00:00:41.200 --> 00:00:46.160]   But since people grew up in pretty stable times, they tend not to see that in emotionally vivid
[00:00:46.160 --> 00:00:50.720]   terms. And then this one monster comes along and they're all terrified. In this case, the reason
[00:00:50.720 --> 00:00:56.720]   the nuke got so cheap was because of intelligence. Now that doesn't mean we should stop intelligence,
[00:00:56.720 --> 00:01:01.600]   but if that's the end result of intelligence over hundreds of years, that doesn't seem like
[00:01:01.600 --> 00:01:07.040]   intelligence is always a net good. Well, we're doing better than the other great apes, I would
[00:01:07.040 --> 00:01:11.840]   say, even though we face these really big risks. And in the meantime, we did incredible things.
[00:01:11.840 --> 00:01:16.560]   So that's a gamble I would take. But I believe we should view it more self-consciously as a
[00:01:16.560 --> 00:01:23.040]   sort of gamble. And it's too late to turn back. The fundamental choice was one of decentralization.
[00:01:23.040 --> 00:01:27.920]   And that may have happened hundreds of millions or billions of years ago. And once you opt for
[00:01:27.920 --> 00:01:33.440]   decentralization, intelligence is going to have advantages and you're not going to be able to turn
[00:01:33.440 --> 00:01:38.320]   the clock back on it. The risk, not that everyone dies, I think that's quite low, but that we
[00:01:38.320 --> 00:01:44.720]   retreat to some kind of pretty chaotic form of like medieval Balkans existence with a much lower
[00:01:44.720 --> 00:01:50.480]   population. That seems to me quite a high risk with or without AI. It's probably the default
[00:01:50.480 --> 00:01:56.400]   setting. Given that you think that's the default setting. Why is that not a big part of your when
[00:01:56.400 --> 00:02:01.440]   you're thinking about how new technologies are coming about? Why not consciously think in terms
[00:02:01.440 --> 00:02:06.480]   of is this getting us to the outcome where we avoid this sort of pre-industrial state that would
[00:02:06.480 --> 00:02:13.360]   result from the $50,000 nukes? Well, if you think the risk is cheap energy more than AI per se,
[00:02:13.360 --> 00:02:20.160]   admittedly, AI could speed the path to cheap energy. It seems very hard to control the strategy
[00:02:20.160 --> 00:02:26.240]   that's worked best so far is to have relatively benevolent nations become hegemons and establish
[00:02:26.240 --> 00:02:33.840]   dominance. So it does influence me. I want the US, UK, some other subset of nations to establish
[00:02:33.840 --> 00:02:39.280]   dominance in AI. It may not work forever, but in a decentralized world, it sure beats the
[00:02:39.280 --> 00:02:44.080]   alternative. So a lot of the AI types, they're too rationalist and they don't start with the
[00:02:44.080 --> 00:02:48.960]   premise that we chose a decentralized world a very, very long time ago, even way before humans.
[00:02:48.960 --> 00:02:52.800]   What would have to be different for you to not be a doomer per se, but just one of these people who
[00:02:52.800 --> 00:02:57.200]   this is the main thing to be thinking about during this period of history or something like that?
[00:02:57.200 --> 00:03:01.680]   Well, I think it is one of the main things we should be thinking about. But I would say if I
[00:03:01.680 --> 00:03:08.160]   thought international cooperation were very possible, I would at least possibly have very
[00:03:08.160 --> 00:03:13.280]   different views than I do now. Or if I thought no other country could make progress on AI,
[00:03:13.280 --> 00:03:19.360]   those seem unlikely to me, but they're not logically impossible. So the fundamental premise
[00:03:19.360 --> 00:03:24.800]   where I differ from a lot of the doomers is my understanding of a decentralized world and its
[00:03:24.800 --> 00:03:29.440]   principles being primary. Their understanding is some kind of comparison, like here's the little
[00:03:29.440 --> 00:03:34.320]   people and here's the big monster and the big monster gets bigger. And even if the big monster
[00:03:34.320 --> 00:03:38.320]   does a lot of good things, it's just getting bigger and here are the little people. That's
[00:03:38.320 --> 00:03:42.800]   a possible framework. But if you start with decentralization and competition, you don't
[00:03:42.800 --> 00:03:45.920]   just think you can wake up in the morning and like legislate safety.
[00:03:45.920 --> 00:03:49.440]   I think you made an interesting point when you were talking about Keynes in the book where you
[00:03:49.440 --> 00:03:53.680]   said one of his faults was that he assumed that people like him would always be in charge.
[00:03:53.680 --> 00:03:54.080]   That's right.
[00:03:54.080 --> 00:03:57.440]   And I do see that also in the alignment discourse, like alignment is, you know,
[00:03:57.440 --> 00:04:00.000]   if it's just handing over to the government and just assuming the government does what
[00:04:00.000 --> 00:04:01.040]   you'd expect it to do.
[00:04:01.040 --> 00:04:05.120]   And I worry about this from my own point of view. So even if you think U.S. is pretty benevolent
[00:04:05.120 --> 00:04:10.160]   today, which is a highly contested and mixed proposition, and I'm an American citizen,
[00:04:10.160 --> 00:04:15.360]   pretty patriotic, but I'm fully aware of the long history of my government in killing and
[00:04:15.360 --> 00:04:21.280]   slaving, doing other terrible things to people. And then you have to rethink that over a long
[00:04:21.280 --> 00:04:26.720]   period of time, it may be the worst time period that affects the final outcome, even if the
[00:04:26.720 --> 00:04:32.480]   average is pretty good. And then if power corrupts and if government even indirectly
[00:04:32.480 --> 00:04:37.680]   controls AI systems, so U.S. government could become worse because it's a leader in AI, right?
[00:04:37.680 --> 00:04:38.480]   Yeah.
[00:04:38.480 --> 00:04:45.120]   But again, I've got to still take that over China or Russia or wherever else it might be.
[00:04:45.120 --> 00:04:48.720]   What's your sense of how the government reacts when the labs are doing,
[00:04:48.720 --> 00:04:52.240]   regardless of how they should react, how they will react when the labs are doing, like,
[00:04:52.240 --> 00:04:57.280]   I don't know, $10 billion training runs. And if under the premise that, you know, these are
[00:04:57.280 --> 00:05:01.680]   powerful models, not human level, per se, but just they can do all kinds of crazy stuff.
[00:05:01.680 --> 00:05:04.720]   How do you think the government's going to, are they going to nationalize the
[00:05:04.720 --> 00:05:07.440]   labs or how are you staying in Washington? What's your sense?
[00:05:07.440 --> 00:05:12.800]   My guess is until there's sort of an SBF-like incident, which might even not be significant,
[00:05:12.800 --> 00:05:18.320]   but a headlines incident, which SBF was, even if it doesn't affect the future evolution of crypto,
[00:05:18.320 --> 00:05:23.280]   which I guess is my view, it won't. Until there's that, we won't do much of anything.
[00:05:23.280 --> 00:05:28.400]   And then we'll have an SBF-like incident and we'll overreact. That seems a very common
[00:05:28.400 --> 00:05:33.040]   pattern in American history. And the fact that it's AI, the stakes might be high or whatever,
[00:05:33.840 --> 00:05:37.120]   I doubt if it will change the recurrence of that pattern.
[00:05:37.120 --> 00:05:37.620]   Yeah.
[00:05:37.620 --> 00:05:39.680]   you
[00:05:39.680 --> 00:05:49.680]   [BLANK_AUDIO]

