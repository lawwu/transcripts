
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:07.440]   - All right, thank you guys for joining us.
[00:00:07.440 --> 00:00:09.640]   So this is our investor panel
[00:00:09.640 --> 00:00:12.480]   and we're joined by three amazing investors.
[00:00:12.480 --> 00:00:14.200]   We have George here,
[00:00:14.200 --> 00:00:17.000]   who is the managing director at Insight.
[00:00:17.000 --> 00:00:19.800]   He has over 20 years of being an operator.
[00:00:19.800 --> 00:00:23.140]   He took Altruix to IPO
[00:00:23.140 --> 00:00:25.440]   and we definitely lean on his advice
[00:00:25.440 --> 00:00:27.480]   to shape our go-to-market motion.
[00:00:27.480 --> 00:00:28.620]   So welcome George.
[00:00:29.880 --> 00:00:32.360]   - Thank you Lavanya, pleasure to be here.
[00:00:32.360 --> 00:00:33.520]   - We also have James,
[00:00:33.520 --> 00:00:35.920]   who's also an investor in Rated Bises.
[00:00:35.920 --> 00:00:38.000]   He's a partner at Bloomberg Beta.
[00:00:38.000 --> 00:00:39.800]   He's incredibly thoughtful
[00:00:39.800 --> 00:00:41.880]   and he's always willing to help his companies
[00:00:41.880 --> 00:00:43.240]   in whatever way they need.
[00:00:43.240 --> 00:00:46.000]   And I personally love working with James.
[00:00:46.000 --> 00:00:46.840]   Welcome James.
[00:00:46.840 --> 00:00:49.520]   - Good to be here.
[00:00:49.520 --> 00:00:51.720]   - And then lastly, we have Sarah,
[00:00:51.720 --> 00:00:54.320]   who is a GP at Amplify Partners
[00:00:54.320 --> 00:00:57.120]   and she's one of the smartest people I've met.
[00:00:57.120 --> 00:00:59.080]   And what I love about Sarah
[00:00:59.080 --> 00:01:02.240]   is that she has incredibly thoughtful opinions
[00:01:02.240 --> 00:01:04.160]   on a variety of topics
[00:01:04.160 --> 00:01:06.320]   and you leave every conversation with Sarah
[00:01:06.320 --> 00:01:08.000]   feeling like a smarter person.
[00:01:08.000 --> 00:01:10.160]   So thank you Sarah for joining us.
[00:01:10.160 --> 00:01:12.680]   - I don't know that I'll be able to live up to that intro,
[00:01:12.680 --> 00:01:14.180]   but thank you for it.
[00:01:14.180 --> 00:01:16.800]   - Of course.
[00:01:16.800 --> 00:01:19.720]   So I'm excited to have you guys all in one room
[00:01:19.720 --> 00:01:24.260]   and I have some really spicy questions for us today.
[00:01:24.260 --> 00:01:26.360]   So let's start with a simple one.
[00:01:27.420 --> 00:01:30.820]   Is machine learning even a good space to invest right now?
[00:01:30.820 --> 00:01:33.100]   We see a lot of companies, there's a lot of hype,
[00:01:33.100 --> 00:01:35.700]   but there's very few companies that are making money
[00:01:35.700 --> 00:01:38.240]   and building ROI efficient businesses.
[00:01:38.240 --> 00:01:42.180]   And anyone can jump in.
[00:01:42.180 --> 00:01:45.380]   - It's funny because you said,
[00:01:45.380 --> 00:01:47.700]   this is a relatively simple question.
[00:01:47.700 --> 00:01:50.700]   I actually think that is one of the spiciest questions
[00:01:50.700 --> 00:01:52.300]   that you can ask.
[00:01:52.300 --> 00:01:56.060]   And frankly, I think the answer that I can share is like,
[00:01:56.060 --> 00:01:57.200]   I don't know.
[00:01:57.200 --> 00:02:00.620]   Certainly as somebody who invests heavily
[00:02:00.620 --> 00:02:02.140]   in tools and platforms,
[00:02:02.140 --> 00:02:05.900]   I feel like we're perhaps not far enough along
[00:02:05.900 --> 00:02:08.660]   in at least the LLM journey
[00:02:08.660 --> 00:02:12.020]   to really draw conclusions about
[00:02:12.020 --> 00:02:14.020]   the set of tools that people need,
[00:02:14.020 --> 00:02:18.820]   about a dominant design for the LLM stack
[00:02:18.820 --> 00:02:22.700]   and about kind of the relationship between the LLM stack,
[00:02:22.700 --> 00:02:27.660]   the ML stack and the existing data and application stack.
[00:02:27.660 --> 00:02:30.880]   It feels like we're still in this kind of early inning
[00:02:30.880 --> 00:02:33.660]   period where existing tech companies,
[00:02:33.660 --> 00:02:35.980]   companies like Stripe, Etsy, et cetera,
[00:02:35.980 --> 00:02:39.340]   they're forming teams to kind of define
[00:02:39.340 --> 00:02:41.820]   and implement their LLM strategy,
[00:02:41.820 --> 00:02:44.480]   but they haven't like stubbed their toe yet.
[00:02:44.480 --> 00:02:49.020]   I suspect that we're going to learn a lot
[00:02:49.020 --> 00:02:50.780]   from the problems that they encounter
[00:02:50.780 --> 00:02:53.480]   and the decisions that they make in the coming months.
[00:02:53.480 --> 00:02:57.220]   So I think, you know, the next couple of years
[00:02:57.220 --> 00:03:00.500]   will be a great time to invest in ML tools and platforms.
[00:03:00.500 --> 00:03:03.460]   But right now I do have, you know,
[00:03:03.460 --> 00:03:05.300]   pretty significant questions about
[00:03:05.300 --> 00:03:07.580]   kind of the uncertainty associated with just
[00:03:07.580 --> 00:03:10.720]   the needs, the tools, and even the models.
[00:03:10.720 --> 00:03:14.020]   - I think that's a really good way of putting it.
[00:03:14.020 --> 00:03:16.580]   The only thing that I'd add is that
[00:03:16.580 --> 00:03:19.980]   the interesting difference right now
[00:03:19.980 --> 00:03:21.620]   is that the assumption is that the big companies
[00:03:21.620 --> 00:03:26.340]   are actually pretty good in setting up their applications
[00:03:26.340 --> 00:03:28.500]   and their LLM related applications.
[00:03:28.500 --> 00:03:30.620]   And historically, that's usually not the case.
[00:03:30.620 --> 00:03:35.460]   Usually the big legacy companies are really bad at adoption.
[00:03:35.460 --> 00:03:37.860]   I don't know whether that's because of something
[00:03:37.860 --> 00:03:39.500]   about the nature of LLMs,
[00:03:39.500 --> 00:03:42.500]   or whether that's right now just because we're just
[00:03:42.500 --> 00:03:44.620]   gonna find out all the ways that big companies
[00:03:44.620 --> 00:03:46.340]   can't adopt LLMs.
[00:03:46.340 --> 00:03:48.420]   And so you'll end up looking,
[00:03:48.420 --> 00:03:51.500]   my guess is over the next three to 18 months
[00:03:51.500 --> 00:03:54.180]   at sort of LLM native companies
[00:03:54.180 --> 00:03:55.620]   and what they're doing differently,
[00:03:55.620 --> 00:03:57.580]   and you'll find some disconnect.
[00:03:57.580 --> 00:03:59.940]   And I am trying to pay a lot of attention to that.
[00:03:59.940 --> 00:04:02.820]   And I'm very hopeful that when George finds out
[00:04:02.820 --> 00:04:04.700]   what are the secrets he's learning from Jasper,
[00:04:04.700 --> 00:04:06.260]   that he'll tell the rest of us.
[00:04:06.260 --> 00:04:11.220]   - So I guess I should comment on that in terms of
[00:04:11.220 --> 00:04:15.180]   where we have at least initially started to take
[00:04:15.180 --> 00:04:18.180]   some early thoughts in terms of how this market
[00:04:18.180 --> 00:04:19.460]   is gonna evolve.
[00:04:19.460 --> 00:04:24.220]   And so when we looked at the space regarding
[00:04:24.220 --> 00:04:26.700]   large language bundles and just what generative AI
[00:04:26.700 --> 00:04:28.540]   could really become,
[00:04:28.540 --> 00:04:31.300]   our starting point was less so,
[00:04:31.300 --> 00:04:33.420]   this sort of finished offering in terms of how
[00:04:33.420 --> 00:04:34.820]   an application could come to market,
[00:04:34.820 --> 00:04:36.700]   but also just, I picked this era,
[00:04:36.700 --> 00:04:39.060]   indicated some of the building blocks that were necessary.
[00:04:39.060 --> 00:04:43.220]   And at Insight, we had a very strong focus on looking
[00:04:43.220 --> 00:04:46.100]   at the modern data stack up to that moment
[00:04:46.100 --> 00:04:49.620]   in terms of just being able to get your private data
[00:04:49.620 --> 00:04:50.460]   well organized.
[00:04:50.460 --> 00:04:54.340]   We certainly thought through how the MLOps experience
[00:04:54.340 --> 00:04:58.300]   was gonna play out and specifically where MLOps
[00:04:58.300 --> 00:05:01.140]   could be leveraged for the building of applications,
[00:05:01.140 --> 00:05:04.100]   including generative AI based applications.
[00:05:04.100 --> 00:05:06.980]   And then what are these potential markets
[00:05:06.980 --> 00:05:11.580]   where a generative experience could be transformational.
[00:05:11.580 --> 00:05:15.380]   And I think as we went through that journey,
[00:05:15.380 --> 00:05:18.260]   we of course came across Jasper.
[00:05:18.260 --> 00:05:22.540]   Jasper certainly spoke to a lot of the elements
[00:05:22.540 --> 00:05:24.420]   that I just described a moment ago,
[00:05:24.420 --> 00:05:28.340]   where they were able to take an early lead
[00:05:28.340 --> 00:05:30.700]   in at least the content marketing space
[00:05:30.700 --> 00:05:35.700]   with a very prescribed curated view of what in this case,
[00:05:35.700 --> 00:05:41.420]   an open AI based, finely tuned down model could be
[00:05:41.420 --> 00:05:45.300]   for the content worker and package that very naturally
[00:05:45.300 --> 00:05:49.460]   with workflow and a compelling user experience,
[00:05:49.460 --> 00:05:54.460]   a UX that was profoundly focused on how a content marketer
[00:05:54.460 --> 00:05:56.980]   lives and works.
[00:05:56.980 --> 00:05:59.580]   And I think that really kind of played out
[00:05:59.580 --> 00:06:03.260]   in terms of how Jasper has been able to become
[00:06:03.260 --> 00:06:05.460]   the second bestest growing software company
[00:06:05.460 --> 00:06:10.300]   in the history of SaaS in the last 18 months.
[00:06:10.300 --> 00:06:14.820]   And so for us, we look for more opportunities like that,
[00:06:14.820 --> 00:06:19.780]   where there are more complete solutions in market
[00:06:19.780 --> 00:06:23.500]   that are effectively built off of the previous experiences
[00:06:23.500 --> 00:06:26.740]   around the modern data stack and MLOps.
[00:06:26.740 --> 00:06:30.420]   And I think Sarah has an important point
[00:06:30.420 --> 00:06:32.940]   that there could be some delineation
[00:06:32.940 --> 00:06:36.620]   in terms of what a pure LLM first stack
[00:06:36.620 --> 00:06:38.700]   and a pure to James's point,
[00:06:38.700 --> 00:06:40.820]   LLM first company could look like,
[00:06:40.820 --> 00:06:44.220]   but ultimately there's still some immediate opportunities
[00:06:44.220 --> 00:06:46.940]   and we keep looking for them where they might exist,
[00:06:46.940 --> 00:06:49.980]   where you can find a complete solution
[00:06:49.980 --> 00:06:53.100]   for a market that needs a generative first offering.
[00:06:53.100 --> 00:06:56.420]   - And George, when you think about some of the new
[00:06:56.420 --> 00:06:58.420]   infrastructure or developer problems
[00:06:58.420 --> 00:07:00.620]   that you've run into at Jasper,
[00:07:00.620 --> 00:07:02.580]   can you talk about any of those?
[00:07:02.580 --> 00:07:04.420]   I think it'd just be really interesting for the crowd
[00:07:04.420 --> 00:07:07.220]   to hear what are the things that you're now,
[00:07:07.220 --> 00:07:08.340]   or what they're worried about
[00:07:08.340 --> 00:07:10.380]   and how you're thinking about it with them.
[00:07:10.380 --> 00:07:11.660]   - Well, no surprise,
[00:07:12.780 --> 00:07:15.380]   I'm not saying anything that anyone
[00:07:15.380 --> 00:07:17.620]   would be already thinking about today.
[00:07:17.620 --> 00:07:21.260]   If you see a business at scale like Jasper
[00:07:21.260 --> 00:07:23.660]   in such a short period of time,
[00:07:23.660 --> 00:07:24.940]   you have to start thinking about
[00:07:24.940 --> 00:07:26.860]   what that underlying infrastructure is
[00:07:26.860 --> 00:07:30.820]   that'll squirt the longterm durability of that business.
[00:07:30.820 --> 00:07:33.700]   And one key factor around it as well,
[00:07:33.700 --> 00:07:37.020]   how much does the dependence on open AI play out
[00:07:37.020 --> 00:07:38.540]   for a business like Jasper over time?
[00:07:38.540 --> 00:07:40.420]   Well, it's actually one of the first things we worked on,
[00:07:40.420 --> 00:07:44.980]   to look at ways that there's a more experimentation
[00:07:44.980 --> 00:07:47.300]   and orchestration oriented layer
[00:07:47.300 --> 00:07:51.420]   that could ensemble across multiple large language models
[00:07:51.420 --> 00:07:54.100]   to basically create a finely tuned,
[00:07:54.100 --> 00:07:55.660]   sort of domain specific outline
[00:07:55.660 --> 00:07:57.340]   that Jasper would be able to leverage.
[00:07:57.340 --> 00:07:58.940]   Some of that already manifests itself,
[00:07:58.940 --> 00:08:01.700]   even in the Generative AI Conference
[00:08:01.700 --> 00:08:04.100]   that Jasper hosted a few weeks back,
[00:08:04.100 --> 00:08:07.180]   where they announced a partnership with HubSpot,
[00:08:07.180 --> 00:08:09.500]   where HubSpot's now taking advantage
[00:08:09.500 --> 00:08:12.740]   of the finely tuned Jasper specific,
[00:08:12.740 --> 00:08:16.820]   or more broadly, marketing content specific LLM
[00:08:16.820 --> 00:08:18.340]   that HubSpot is now leveraging
[00:08:18.340 --> 00:08:21.980]   to be able to build their own capabilities
[00:08:21.980 --> 00:08:23.780]   around Jasper's API.
[00:08:23.780 --> 00:08:27.300]   And so I think that's an indicator
[00:08:27.300 --> 00:08:32.300]   of how the early, early Austin CK's marketing play out.
[00:08:32.300 --> 00:08:36.380]   Anyone who's not building their own LLM
[00:08:36.380 --> 00:08:38.020]   needs to be (indistinct)
[00:08:38.020 --> 00:08:43.020]   and when he has to rely on someone's capability there,
[00:08:43.020 --> 00:08:46.260]   is likely going to come up with ways
[00:08:46.260 --> 00:08:49.500]   that they can have their own orchestration
[00:08:49.500 --> 00:08:52.420]   and they can have their own ensemble
[00:08:52.420 --> 00:08:56.140]   to that not rely on a single LLM co-buyer.
[00:08:56.140 --> 00:08:59.340]   And that's already played out, even in Jasper.
[00:08:59.340 --> 00:09:02.260]   So, I'll leave it up to that.
[00:09:02.260 --> 00:09:05.060]   Yeah, I mean, I think that's a really interesting journey
[00:09:05.060 --> 00:09:08.300]   and also kind of speaks to what I was describing
[00:09:08.300 --> 00:09:11.020]   or about like not really understanding
[00:09:11.020 --> 00:09:14.420]   what the dominant design for the LLM stack might be.
[00:09:14.420 --> 00:09:16.140]   Now the challenges that companies face,
[00:09:16.140 --> 00:09:20.020]   like when they're interfacing with the OpenAI APIs,
[00:09:20.020 --> 00:09:23.340]   that's a very, very different set of challenges
[00:09:23.340 --> 00:09:25.420]   and frankly, a different margin profile
[00:09:25.420 --> 00:09:29.380]   than companies that are either like pre-training
[00:09:29.380 --> 00:09:32.420]   or fine tuning their own models from scratch.
[00:09:33.700 --> 00:09:35.620]   As you were talking about scaling too,
[00:09:35.620 --> 00:09:40.620]   it's also a reminder that like even LLM driven applications,
[00:09:40.620 --> 00:09:43.180]   like there's still applications
[00:09:43.180 --> 00:09:45.860]   and user expectations for applications,
[00:09:45.860 --> 00:09:50.020]   I think are both high and continue to evolve.
[00:09:50.020 --> 00:09:52.700]   And one of the things that I think a lot about
[00:09:52.700 --> 00:09:55.820]   kind of investing across different types
[00:09:55.820 --> 00:09:57.820]   of technical tools and platforms
[00:09:57.820 --> 00:10:00.340]   and interfacing a lot with like the database community
[00:10:00.340 --> 00:10:02.460]   and the distributed systems community
[00:10:02.460 --> 00:10:04.860]   is that I believe in the future apps
[00:10:04.860 --> 00:10:07.460]   will not only be intelligent,
[00:10:07.460 --> 00:10:09.260]   but like they're going to be faster
[00:10:09.260 --> 00:10:12.940]   and they're going to be collaborative
[00:10:12.940 --> 00:10:15.380]   and like fast collaborative apps
[00:10:15.380 --> 00:10:19.260]   without LLMs are no challenging at all.
[00:10:19.260 --> 00:10:22.740]   One of my observations in kind of playing
[00:10:22.740 --> 00:10:24.980]   with some of these new LLM driven tools
[00:10:24.980 --> 00:10:27.260]   is that they're all so damn slow.
[00:10:27.260 --> 00:10:30.220]   So I think we're going to see a lot,
[00:10:30.220 --> 00:10:33.580]   even as these LLM native companies start to scale
[00:10:33.580 --> 00:10:36.900]   and start to encounter the same types of scaling challenges
[00:10:36.900 --> 00:10:40.260]   that like more traditional software companies also face.
[00:10:40.260 --> 00:10:44.020]   - I want to dig more into this large model stack.
[00:10:44.020 --> 00:10:45.620]   I think this is a very interesting idea.
[00:10:45.620 --> 00:10:47.860]   What do you think it looks like
[00:10:47.860 --> 00:10:50.460]   and what tools and platforms are essential
[00:10:50.460 --> 00:10:52.820]   for developers to build their own
[00:10:52.820 --> 00:10:55.540]   large model driven applications?
[00:10:55.540 --> 00:10:58.340]   - Well, I'll start with a softball answer on that.
[00:10:58.340 --> 00:10:59.660]   Thanks for asking it, Nivania.
[00:10:59.660 --> 00:11:02.700]   It actually turns out that at least one element
[00:11:02.700 --> 00:11:07.700]   of the LLM stack is going to be some core capability
[00:11:07.700 --> 00:11:11.380]   around the spin and tracking parameter tuning
[00:11:11.380 --> 00:11:12.660]   and friction controls,
[00:11:12.660 --> 00:11:16.140]   which it's compelling to see that weights and biases
[00:11:16.140 --> 00:11:18.980]   is now deployed at a hundred percent
[00:11:18.980 --> 00:11:22.940]   of all the original LLM period is as we speak.
[00:11:22.940 --> 00:11:25.700]   And so that's a pretty early indicator again,
[00:11:25.700 --> 00:11:30.700]   but it comes to where we see what will create our market
[00:11:30.700 --> 00:11:34.100]   to be around large LLM when they go.
[00:11:34.100 --> 00:11:36.380]   But also it's going to speak to the fact
[00:11:36.380 --> 00:11:38.820]   that domain specific parameter tuning models
[00:11:38.820 --> 00:11:43.100]   will also need to be leveraged off of the fact
[00:11:43.100 --> 00:11:45.940]   that you can do those same experiments,
[00:11:45.940 --> 00:11:47.860]   you can do the same tracking,
[00:11:47.860 --> 00:11:50.220]   you can do the same parameter states
[00:11:50.220 --> 00:11:53.820]   that you'd need in an experience like it's biases.
[00:11:53.820 --> 00:11:57.500]   So I think that is one starting point
[00:11:57.500 --> 00:11:59.940]   and certainly benefits the center
[00:11:59.940 --> 00:12:03.780]   of all MLOps experience today.
[00:12:03.780 --> 00:12:06.220]   I think there'll be other capabilities
[00:12:06.220 --> 00:12:08.020]   that will also be needed,
[00:12:08.020 --> 00:12:14.220]   but right now the model development has taken such priority
[00:12:14.220 --> 00:12:18.620]   and center stage that anyone who's enabling
[00:12:18.620 --> 00:12:21.620]   and easing the challenges around model development
[00:12:22.540 --> 00:12:25.580]   and models and provider tools will generally benefit
[00:12:25.580 --> 00:12:28.500]   for the next half a decade of work.
[00:12:28.500 --> 00:12:33.060]   - Yeah, I think another key question again is like,
[00:12:33.060 --> 00:12:35.740]   are you prompting a model?
[00:12:35.740 --> 00:12:39.140]   Are you interfacing with an LLM API
[00:12:39.140 --> 00:12:41.940]   or are you fine tuning
[00:12:41.940 --> 00:12:44.900]   and or pre-training your own models from scratch?
[00:12:44.900 --> 00:12:48.060]   And the developer experience associated with each today
[00:12:48.060 --> 00:12:49.660]   is just radically different.
[00:12:49.660 --> 00:12:51.700]   Like if you are prompting,
[00:12:51.700 --> 00:12:53.980]   then certainly having kind of better tools
[00:12:53.980 --> 00:12:58.100]   for like prompt version control, prompt evaluation,
[00:12:58.100 --> 00:13:01.020]   I think certainly becomes a issue.
[00:13:01.020 --> 00:13:04.140]   I'm shocked by the number of companies
[00:13:04.140 --> 00:13:06.780]   that cannot answer the question.
[00:13:06.780 --> 00:13:09.380]   What is the correlation between kind of like subsets
[00:13:09.380 --> 00:13:12.660]   of prompts and user behavior?
[00:13:12.660 --> 00:13:17.380]   Like what subsets of prompts might cause users to churn?
[00:13:17.380 --> 00:13:21.660]   So certainly I think there's a whole set of tooling
[00:13:21.660 --> 00:13:23.100]   there that's needed,
[00:13:23.100 --> 00:13:27.340]   but if you're fine tuning and or doing pre-training,
[00:13:27.340 --> 00:13:28.740]   additional pre-training,
[00:13:28.740 --> 00:13:34.300]   then your ML stack starts to look much different.
[00:13:34.300 --> 00:13:37.180]   I think there's a lot of more kind of fundamental
[00:13:37.180 --> 00:13:39.980]   data management technology that is needed to ensure
[00:13:39.980 --> 00:13:44.700]   that you're training on kind of the right slices of data
[00:13:44.700 --> 00:13:46.180]   and that you understand performance
[00:13:46.180 --> 00:13:48.300]   across those slices of data.
[00:13:49.460 --> 00:13:52.180]   Certainly your compute requirements go up a lot.
[00:13:52.180 --> 00:13:55.340]   And so investing in kind of additional model optimization
[00:13:55.340 --> 00:13:57.540]   becomes necessary in order to kind of meet
[00:13:57.540 --> 00:13:59.340]   those latency requirements
[00:13:59.340 --> 00:14:02.140]   and ensure that like you're building a good business.
[00:14:02.140 --> 00:14:05.500]   You're not spending $40 million in order to generate
[00:14:05.500 --> 00:14:08.060]   $1 million in ARR.
[00:14:08.060 --> 00:14:11.700]   I don't think we yet really know
[00:14:11.700 --> 00:14:14.540]   how like prompting versus fine tuning will be used.
[00:14:14.540 --> 00:14:16.900]   It's really difficult to even make an assertion.
[00:14:16.900 --> 00:14:19.300]   Like, should I be using my own models?
[00:14:19.300 --> 00:14:21.580]   Should I be using these LLM APIs?
[00:14:21.580 --> 00:14:23.780]   Because the APIs continue to get better
[00:14:23.780 --> 00:14:27.340]   in determining when you have a task
[00:14:27.340 --> 00:14:29.460]   that is kind of specific enough
[00:14:29.460 --> 00:14:32.220]   or kind of like disparate enough
[00:14:32.220 --> 00:14:35.220]   from a next word prediction task
[00:14:35.220 --> 00:14:39.740]   where additional pre-training is going to be useful,
[00:14:39.740 --> 00:14:41.940]   where fine tuning would be useful.
[00:14:41.940 --> 00:14:46.220]   Making that determination is still like pretty hard
[00:14:46.220 --> 00:14:49.020]   and feels like a moving target.
[00:14:50.020 --> 00:14:51.980]   Let me say something controversial.
[00:14:51.980 --> 00:14:54.620]   I think that if you're building a business
[00:14:54.620 --> 00:14:56.860]   on top of LLMs right now,
[00:14:56.860 --> 00:14:59.060]   you should just think about it as incurring
[00:14:59.060 --> 00:15:02.420]   lots and lots of high interest rate debt
[00:15:02.420 --> 00:15:04.940]   that you should just like sort of worry more
[00:15:04.940 --> 00:15:07.980]   about figuring out what are the boundaries
[00:15:07.980 --> 00:15:12.020]   of the interfaces for humans and the business models.
[00:15:12.020 --> 00:15:13.940]   And you should be playing around that more
[00:15:13.940 --> 00:15:16.260]   rather than necessarily trying to figure out
[00:15:16.260 --> 00:15:19.420]   your unit costs or deciding whether or not
[00:15:19.420 --> 00:15:22.060]   to fine tune the model.
[00:15:22.060 --> 00:15:24.620]   My guess is in the vast majority
[00:15:24.620 --> 00:15:29.620]   of the most interesting LLM native businesses,
[00:15:29.620 --> 00:15:32.380]   a lot of them will just be clever product people
[00:15:32.380 --> 00:15:34.740]   rather than brilliant technical people.
[00:15:34.740 --> 00:15:38.500]   And I think right now that there's a little bit of a race
[00:15:38.500 --> 00:15:41.300]   to just figure out what works and doesn't work,
[00:15:41.300 --> 00:15:44.700]   that we have very, very few examples of applications
[00:15:44.700 --> 00:15:46.180]   that are actually good.
[00:15:46.180 --> 00:15:48.660]   And in some ways, I feel like that's the place
[00:15:48.660 --> 00:15:50.780]   to play right now.
[00:15:50.780 --> 00:15:53.180]   And admittedly, I think we're more infrastructure
[00:15:53.180 --> 00:15:54.900]   developer type folks, right?
[00:15:54.900 --> 00:15:57.700]   But I think that I'm constantly surprised
[00:15:57.700 --> 00:15:59.900]   that there's not more variation going on
[00:15:59.900 --> 00:16:01.260]   in application world.
[00:16:01.260 --> 00:16:03.660]   - So that is a spicy cake.
[00:16:03.660 --> 00:16:06.940]   And I agree and disagree to an extent.
[00:16:06.940 --> 00:16:09.180]   I certainly think that there are businesses
[00:16:09.180 --> 00:16:13.180]   that can be built on top of these LLM APIs
[00:16:13.180 --> 00:16:15.700]   where they will not require additional fine tuning,
[00:16:15.700 --> 00:16:17.980]   where they will not require pre-training,
[00:16:17.980 --> 00:16:22.020]   and where you can invest primarily in the user experience.
[00:16:22.020 --> 00:16:24.020]   One of the things that has surprised me
[00:16:24.020 --> 00:16:28.020]   is that frankly, I think the LLM kind of hacker community
[00:16:28.020 --> 00:16:32.140]   today is primarily composed of people
[00:16:32.140 --> 00:16:34.060]   who don't have a background in ML.
[00:16:34.060 --> 00:16:40.260]   But what I've also observed is that like a lot of the things
[00:16:40.260 --> 00:16:42.700]   that they're building look the same.
[00:16:42.700 --> 00:16:45.420]   Like how many pitches for Ho-Gen startups
[00:16:45.420 --> 00:16:47.380]   do each of you see every week?
[00:16:47.380 --> 00:16:51.260]   How many like copywriting kind of like Jasper lookalikes
[00:16:51.260 --> 00:16:52.740]   do we see every week?
[00:16:52.740 --> 00:16:55.820]   And I think part of the reason why we see so much convergence
[00:16:55.820 --> 00:16:57.420]   behind the same use cases
[00:16:57.420 --> 00:17:00.860]   is that there isn't a real fundamental understanding
[00:17:00.860 --> 00:17:04.700]   of how these models work, when they fail,
[00:17:04.700 --> 00:17:09.980]   what like it means to fine tune,
[00:17:09.980 --> 00:17:13.460]   or what it means to prompt tune.
[00:17:13.460 --> 00:17:18.340]   And without like, you don't need a deep understanding
[00:17:18.340 --> 00:17:20.700]   of the underlying technology,
[00:17:20.700 --> 00:17:24.700]   but without an understanding of the underlying technology,
[00:17:24.700 --> 00:17:28.980]   I think it's really difficult to like get creative.
[00:17:28.980 --> 00:17:31.620]   One of the things that is inspiring to me
[00:17:31.620 --> 00:17:34.060]   is I spent a lot of time at universities too
[00:17:34.060 --> 00:17:36.820]   kind of trying to better understand where like research
[00:17:37.980 --> 00:17:40.420]   and industry needs are going to converge.
[00:17:40.420 --> 00:17:42.780]   And on a recent trip to Caltech,
[00:17:42.780 --> 00:17:44.820]   like I talked to people who were thinking about
[00:17:44.820 --> 00:17:47.900]   like using LLMs for weather forecasting
[00:17:47.900 --> 00:17:52.780]   and biomarker discovery and planning and robotic systems.
[00:17:52.780 --> 00:17:54.460]   And like, I wanna see more of that.
[00:17:54.460 --> 00:17:59.460]   I wanna see people think beyond like Ho-Gen and copywriting.
[00:17:59.460 --> 00:18:01.580]   But I do think that requires
[00:18:01.580 --> 00:18:04.580]   kind of a more nuanced understanding of ML.
[00:18:04.580 --> 00:18:07.340]   - Interesting.
[00:18:07.340 --> 00:18:11.060]   So do you think there's a space for educating product people
[00:18:11.060 --> 00:18:14.060]   more on the intricacies of the LLMs and fine tuning?
[00:18:14.060 --> 00:18:15.300]   And if we could just do that,
[00:18:15.300 --> 00:18:17.820]   then their brains are being more maybe creative
[00:18:17.820 --> 00:18:21.700]   or like they know how to think of those UI experiences
[00:18:21.700 --> 00:18:23.020]   that James talked about.
[00:18:23.020 --> 00:18:26.700]   And how do we educate them on the LLMs?
[00:18:26.700 --> 00:18:28.100]   - So I do think that's part of it.
[00:18:28.100 --> 00:18:29.100]   The weird part of course,
[00:18:29.100 --> 00:18:31.100]   is that this is a constantly moving target
[00:18:31.100 --> 00:18:32.860]   and we don't really have a great sense
[00:18:32.860 --> 00:18:34.580]   of what the architecture is gonna look like.
[00:18:34.580 --> 00:18:38.620]   But I do think that the brave and interesting product people
[00:18:38.620 --> 00:18:40.140]   who are willing to dive in right now
[00:18:40.140 --> 00:18:41.260]   are gonna make lots of mistakes,
[00:18:41.260 --> 00:18:42.540]   but also they're gonna be the ones
[00:18:42.540 --> 00:18:44.380]   to figure out the insights sooner.
[00:18:44.380 --> 00:18:49.180]   And I totally agree with Sarah in the sense that
[00:18:49.180 --> 00:18:51.300]   if you are successful building something
[00:18:51.300 --> 00:18:53.980]   or come up with a novel interface or novel business,
[00:18:53.980 --> 00:18:56.660]   then you're gonna have to go back and redo a bunch of work
[00:18:56.660 --> 00:18:58.220]   if it turns out to be interesting.
[00:18:58.220 --> 00:19:00.300]   But my point there,
[00:19:00.300 --> 00:19:02.940]   more being that like there should be more
[00:19:02.940 --> 00:19:05.020]   Jasper-like founders out there
[00:19:05.020 --> 00:19:07.260]   and they should be trying more weird things.
[00:19:07.260 --> 00:19:10.420]   And I'm sort of surprised that it isn't happening.
[00:19:10.420 --> 00:19:12.460]   - See, like one of the companies
[00:19:12.460 --> 00:19:15.820]   that I perhaps like regret not investing in the most
[00:19:15.820 --> 00:19:18.140]   is a company called Afresh Technologies.
[00:19:18.140 --> 00:19:21.580]   They're kind of applying reinforcement learning
[00:19:21.580 --> 00:19:23.220]   and other types of machine learning
[00:19:23.220 --> 00:19:27.340]   to optimize a partial goods inventory management
[00:19:27.340 --> 00:19:28.700]   for groceries.
[00:19:28.700 --> 00:19:30.980]   And the founding team includes
[00:19:32.580 --> 00:19:36.700]   one co-founder who's also a professor at Cornell Tech
[00:19:36.700 --> 00:19:38.700]   with a background in RL,
[00:19:38.700 --> 00:19:40.980]   another co-founder with like
[00:19:40.980 --> 00:19:43.740]   a very, very strong engineering background,
[00:19:43.740 --> 00:19:46.380]   and a third co-founder who had a background
[00:19:46.380 --> 00:19:49.660]   in kind of the grocery sector in snacks.
[00:19:49.660 --> 00:19:55.420]   And we saw like back in 2017, 2018,
[00:19:55.420 --> 00:19:58.180]   probably around that time that the company was founded,
[00:19:58.180 --> 00:20:00.980]   that like those companies overperformed.
[00:20:00.980 --> 00:20:03.900]   Like the companies where you had the ML expertise
[00:20:03.900 --> 00:20:07.060]   and you had the product or domain expertise,
[00:20:07.060 --> 00:20:10.460]   and you had the engineering expertise.
[00:20:10.460 --> 00:20:12.860]   And I feel like there's been like a little bit of FOMO
[00:20:12.860 --> 00:20:15.300]   in the BC community where we're like,
[00:20:15.300 --> 00:20:18.260]   "Oh man, if we don't make our LLM investment now,
[00:20:18.260 --> 00:20:20.060]   like we're going to miss out."
[00:20:20.060 --> 00:20:22.220]   But frankly, like I think we're better off
[00:20:22.220 --> 00:20:24.780]   like waiting for those complete teams
[00:20:24.780 --> 00:20:26.900]   that know the tech, that know the domain,
[00:20:26.900 --> 00:20:29.380]   that know how to like build products at scale
[00:20:30.540 --> 00:20:32.020]   rather than like trying to pick
[00:20:32.020 --> 00:20:34.260]   and choose subsets of expertise.
[00:20:34.260 --> 00:20:38.060]   - George, you were part of making
[00:20:38.060 --> 00:20:40.660]   big architecture shifts before, right?
[00:20:40.660 --> 00:20:42.700]   How do you think about these big architecture shifts
[00:20:42.700 --> 00:20:45.980]   compared to like the move from clients over the web
[00:20:45.980 --> 00:20:46.820]   or something like that?
[00:20:46.820 --> 00:20:49.420]   Or you think about your alternate turnaround?
[00:20:49.420 --> 00:20:52.940]   - Yeah, so I think it is as big,
[00:20:52.940 --> 00:20:55.820]   potentially even bigger than cold start
[00:20:55.820 --> 00:20:58.980]   with the shift to mobile and the shift to cloud.
[00:20:58.980 --> 00:21:02.020]   And that will take time.
[00:21:02.020 --> 00:21:06.620]   And I think we're in the first few moments of the ballgame,
[00:21:06.620 --> 00:21:08.820]   not even in the first inning yet.
[00:21:08.820 --> 00:21:13.420]   And because everyone can see this opportunity
[00:21:13.420 --> 00:21:16.100]   and can see how this ballgame will play out
[00:21:16.100 --> 00:21:19.260]   for quite a while, there is a little bit of an urgency.
[00:21:19.260 --> 00:21:22.460]   And I think, just to Sarah's point,
[00:21:22.460 --> 00:21:26.500]   there's an urgency that might just need to ease,
[00:21:26.500 --> 00:21:28.660]   just a little bit more contingent
[00:21:28.660 --> 00:21:32.140]   and just get rid of the funnel for a moment
[00:21:32.140 --> 00:21:36.100]   and really focus on finding those incredible businesses
[00:21:36.100 --> 00:21:39.860]   that'll emerge in this next half a decade or more.
[00:21:39.860 --> 00:21:43.740]   And I think part of that is a complete view
[00:21:43.740 --> 00:21:48.140]   on how the team plus the domain,
[00:21:48.140 --> 00:21:51.140]   plus the user experience and the workflow
[00:21:51.140 --> 00:21:53.580]   around the enterprise space
[00:21:53.580 --> 00:21:56.900]   creates something compelling in the category,
[00:21:56.900 --> 00:21:59.620]   something that's really passionate in the category.
[00:21:59.620 --> 00:22:02.700]   And that doesn't happen overnight.
[00:22:02.700 --> 00:22:05.220]   And just because you're strong in LLM,
[00:22:05.220 --> 00:22:09.380]   whether you're taking it off of an existing API
[00:22:09.380 --> 00:22:11.820]   or you're finding your own model,
[00:22:11.820 --> 00:22:13.980]   it doesn't necessarily happen overnight.
[00:22:13.980 --> 00:22:15.500]   Builders are pulling the product
[00:22:15.500 --> 00:22:17.620]   and making all users experience.
[00:22:17.620 --> 00:22:20.300]   So I would just encourage anyone
[00:22:20.300 --> 00:22:23.780]   who's either investing or building in this space,
[00:22:23.780 --> 00:22:25.940]   especially as there is all of this buzz
[00:22:25.940 --> 00:22:28.260]   and all of this, you know,
[00:22:28.260 --> 00:22:30.820]   kinetic activity that's happening right now,
[00:22:30.820 --> 00:22:35.780]   this will play itself out over the next decade or longer.
[00:22:35.780 --> 00:22:39.420]   And ultimately, every software category
[00:22:39.420 --> 00:22:42.940]   is going to have a LLM
[00:22:42.940 --> 00:22:45.700]   moving at the center of its category.
[00:22:45.700 --> 00:22:50.060]   And whether that be delivered by incumbents
[00:22:50.060 --> 00:22:54.460]   or whether that be delivered by a upstart
[00:22:54.460 --> 00:22:59.300]   that's coming into market right now,
[00:22:59.300 --> 00:23:04.140]   we won't see the outcome of that for quite a while.
[00:23:04.140 --> 00:23:06.500]   But right now is the time to build.
[00:23:06.500 --> 00:23:08.900]   I think it is absolutely time to build and experiment
[00:23:08.900 --> 00:23:12.220]   and try things and presume that everything
[00:23:12.220 --> 00:23:15.500]   that we're doing today isn't going to necessarily be durable
[00:23:15.500 --> 00:23:18.220]   because I think we're in the mode of learning
[00:23:18.220 --> 00:23:20.420]   and experimentation, that's a big winner.
[00:23:20.420 --> 00:23:26.180]   - And so we've talked a lot about like these LLM applications
[00:23:26.180 --> 00:23:28.700]   just switching gear to like the people building these models.
[00:23:28.700 --> 00:23:32.020]   Do you think there's a place to gain competitive advantage
[00:23:32.020 --> 00:23:34.260]   by building your own large models?
[00:23:34.260 --> 00:23:36.180]   And is it even possible for people
[00:23:36.180 --> 00:23:39.580]   who are not the big incumbents to do this?
[00:23:39.580 --> 00:23:41.780]   Or is it just the big players will build the models
[00:23:41.780 --> 00:23:44.340]   and then the smaller players will just buy into them?
[00:23:45.580 --> 00:23:50.580]   - I mean, I think like if you believe in possibility
[00:23:50.580 --> 00:23:52.900]   in the advancement of technologies,
[00:23:52.900 --> 00:23:55.100]   you've got to believe that there's an opportunity
[00:23:55.100 --> 00:23:59.580]   for others to build models for the same reason
[00:23:59.580 --> 00:24:02.420]   that we believe that like new companies
[00:24:02.420 --> 00:24:04.260]   can build new databases.
[00:24:04.260 --> 00:24:08.940]   I think a lot of the models that we see today
[00:24:08.940 --> 00:24:13.820]   have been designed for kind of a subset of tasks,
[00:24:13.820 --> 00:24:16.340]   again, like tasks that tend to look like
[00:24:16.340 --> 00:24:20.700]   either image video audio generation tasks
[00:24:20.700 --> 00:24:22.820]   or language modeling tasks.
[00:24:22.820 --> 00:24:26.860]   But there are other tasks perhaps,
[00:24:26.860 --> 00:24:31.860]   like weather forecasting might be an example
[00:24:31.860 --> 00:24:37.180]   or things related to like structured
[00:24:37.180 --> 00:24:39.300]   or time series data could be another
[00:24:40.300 --> 00:24:44.100]   where these models sometimes like underperform.
[00:24:44.100 --> 00:24:49.620]   So I do imagine that there will be opportunities
[00:24:49.620 --> 00:24:51.860]   to build new types of models.
[00:24:51.860 --> 00:24:55.060]   I think even the best AI researchers would also say
[00:24:55.060 --> 00:25:00.060]   that like, we don't know that the transformer architecture
[00:25:00.060 --> 00:25:04.660]   is the end, like it is quite possible in the next five years
[00:25:04.660 --> 00:25:08.060]   that like a new type of model architecture emerges.
[00:25:08.980 --> 00:25:11.140]   There'd been a lot of recent research
[00:25:11.140 --> 00:25:13.620]   on like state machines and things like that,
[00:25:13.620 --> 00:25:16.700]   that ends up kind of outperforming transformers.
[00:25:16.700 --> 00:25:21.700]   So, I mean, I think it's possible.
[00:25:21.700 --> 00:25:24.740]   I guess the question is like, is it probable?
[00:25:24.740 --> 00:25:29.900]   - I think the reason why so much of current
[00:25:29.900 --> 00:25:34.540]   large language model development is in some way
[00:25:34.540 --> 00:25:38.340]   tied closer to the hyperscalers
[00:25:38.340 --> 00:25:43.260]   is mainly because of the limitation on compute resources.
[00:25:43.260 --> 00:25:46.460]   When you look at where the intensivity
[00:25:46.460 --> 00:25:50.380]   of both the training and the inference
[00:25:50.380 --> 00:25:52.540]   in building a large language model
[00:25:52.540 --> 00:25:57.220]   and then of course being able to deliver it at scale,
[00:25:57.220 --> 00:26:02.220]   you still need a significant amount of GPU based compute
[00:26:02.220 --> 00:26:05.300]   as the current sort of paradigm.
[00:26:05.300 --> 00:26:06.340]   I think that will change.
[00:26:06.340 --> 00:26:09.700]   I think there will be more sort of domain specific
[00:26:09.700 --> 00:26:13.060]   inference oriented AI chips that will emerge.
[00:26:13.060 --> 00:26:15.060]   I think the cost of inference will come down.
[00:26:15.060 --> 00:26:18.860]   I think there will be some near term challenges
[00:26:18.860 --> 00:26:22.100]   in terms of supply on the chip side
[00:26:22.100 --> 00:26:26.340]   that has kind of created a somewhat block
[00:26:26.340 --> 00:26:31.020]   in terms of who has access to GPUs at scale.
[00:26:31.020 --> 00:26:36.020]   And mainly the cloud providers are the ones
[00:26:36.220 --> 00:26:38.540]   that have access to GPUs at scale.
[00:26:38.540 --> 00:26:40.940]   And so it's been interesting to notice
[00:26:40.940 --> 00:26:45.460]   that every LLM that has been funded up to this point,
[00:26:45.460 --> 00:26:47.140]   there's about six or seven major ones
[00:26:47.140 --> 00:26:51.500]   that are now taking somewhere between minimum 200 million
[00:26:51.500 --> 00:26:55.780]   to as much as 1.5 billion, not including OpenAI,
[00:26:55.780 --> 00:26:59.100]   which of course took a big $10 billion round,
[00:26:59.100 --> 00:27:00.700]   plus with Microsoft.
[00:27:00.700 --> 00:27:03.460]   Each one of them has some tie in
[00:27:03.460 --> 00:27:06.620]   to one of the key hyperscalers.
[00:27:06.620 --> 00:27:09.860]   And I think in the short term,
[00:27:09.860 --> 00:27:14.300]   that that limitation is what it is.
[00:27:14.300 --> 00:27:16.020]   And I think in the midterm,
[00:27:16.020 --> 00:27:17.900]   that should change for a number of reasons.
[00:27:17.900 --> 00:27:21.860]   One is just a greater supply of like just AI oriented chips
[00:27:21.860 --> 00:27:24.820]   that will emerge and a greater supply
[00:27:24.820 --> 00:27:28.540]   that will be deployable in various infrastructures.
[00:27:28.540 --> 00:27:31.940]   But it's also an interesting thing to see
[00:27:31.940 --> 00:27:34.300]   how more efficient models, right?
[00:27:34.300 --> 00:27:38.380]   The ones, I mean, the text ones are pretty large
[00:27:38.380 --> 00:27:41.380]   and that's why they're of course called
[00:27:41.380 --> 00:27:42.820]   large language models.
[00:27:42.820 --> 00:27:44.540]   Because the number of parameters
[00:27:44.540 --> 00:27:47.060]   we're now throwing at these models
[00:27:47.060 --> 00:27:51.020]   and the loss function around them is so interesting
[00:27:51.020 --> 00:27:55.740]   because you're not losing any degradation in models.
[00:27:55.740 --> 00:27:58.100]   You throw more parameters over time.
[00:27:58.100 --> 00:28:01.900]   I think in other modalities,
[00:28:01.900 --> 00:28:03.860]   if we look at like image processing,
[00:28:03.860 --> 00:28:05.300]   look at video processing,
[00:28:05.300 --> 00:28:07.100]   models are actually much more compact
[00:28:07.100 --> 00:28:10.140]   and models to be on the generative side,
[00:28:10.140 --> 00:28:14.740]   don't have to require as much processing per se
[00:28:14.740 --> 00:28:17.740]   to both build it and inference costs
[00:28:17.740 --> 00:28:20.100]   are still quite expensive.
[00:28:20.100 --> 00:28:22.540]   So some of this has to just kind of come down
[00:28:22.540 --> 00:28:25.860]   to the economics of like supply side challenges
[00:28:25.860 --> 00:28:28.940]   for what it takes to serve and manage
[00:28:28.940 --> 00:28:31.340]   and deploy these models at scale.
[00:28:31.340 --> 00:28:33.860]   Once that unlocks in the next several years,
[00:28:33.860 --> 00:28:38.860]   I think we'll see less dependency on all the hyperscalers.
[00:28:38.860 --> 00:28:40.500]   But for now, it seems like there is
[00:28:40.500 --> 00:28:41.780]   a pretty critical dependency
[00:28:41.780 --> 00:28:43.740]   on who has access to the GPUs.
[00:28:43.740 --> 00:28:47.060]   - So George, under what conditions would you have invested
[00:28:47.060 --> 00:28:49.220]   or would you invest in one of these sort of attempts
[00:28:49.220 --> 00:28:51.620]   to spend hundreds of millions, if not billions of dollars
[00:28:51.620 --> 00:28:53.140]   to build very big models?
[00:28:54.020 --> 00:28:57.900]   - So just to be forthright,
[00:28:57.900 --> 00:29:01.660]   we have passed on every one of the investments
[00:29:01.660 --> 00:29:06.180]   and that'll be an interesting discussion
[00:29:06.180 --> 00:29:07.500]   five or six years from now, right?
[00:29:07.500 --> 00:29:10.260]   Whether it made sense to pass them on or not.
[00:29:10.260 --> 00:29:14.820]   I think for us, we personally had a difficult time
[00:29:14.820 --> 00:29:19.460]   seeing the financial returns around those investments
[00:29:19.460 --> 00:29:21.860]   because it seemed like every one of the investments
[00:29:21.860 --> 00:29:25.420]   that were made, a lot of those investments
[00:29:25.420 --> 00:29:30.020]   were also tied to retirement of compute, right?
[00:29:30.020 --> 00:29:31.420]   By the hyperscalers.
[00:29:31.420 --> 00:29:35.660]   So if you're making a $10 investment,
[00:29:35.660 --> 00:29:38.980]   there was $1 of compute bill that was retired
[00:29:38.980 --> 00:29:40.620]   in every one of these deals.
[00:29:40.620 --> 00:29:43.380]   And so inherently, it made a lot of sense, right?
[00:29:43.380 --> 00:29:46.220]   For the hyperscalers to make the investments
[00:29:46.220 --> 00:29:47.700]   that they have done.
[00:29:47.700 --> 00:29:52.700]   But I think with a financially oriented equity,
[00:29:52.700 --> 00:29:58.020]   growth equity, sort of check coming in at scale
[00:29:58.020 --> 00:30:01.660]   from a venture capital firm like Insight,
[00:30:01.660 --> 00:30:04.180]   we could have come to a reasonable math
[00:30:04.180 --> 00:30:08.300]   that made sense to make those investments at this scale.
[00:30:08.300 --> 00:30:11.660]   And so that's why we purposely chose to see
[00:30:11.660 --> 00:30:15.900]   where some of these applied use cases would come about,
[00:30:15.900 --> 00:30:17.820]   like the Jaspers of the world,
[00:30:17.820 --> 00:30:21.980]   and where we saw the use of the right tools
[00:30:21.980 --> 00:30:24.540]   that would be needed to build models
[00:30:24.540 --> 00:30:27.580]   and build the applications around these models at scale
[00:30:27.580 --> 00:30:31.780]   with the investment and the likes of Weights and Biases.
[00:30:31.780 --> 00:30:36.780]   But for now, we have purposely stayed out of the building
[00:30:36.780 --> 00:30:38.900]   of the large language models themselves.
[00:30:38.900 --> 00:30:40.860]   We're big believers in it.
[00:30:40.860 --> 00:30:44.780]   I almost equivocate this to like their R&D that's required,
[00:30:44.780 --> 00:30:47.380]   say, to fund the space program,
[00:30:47.380 --> 00:30:50.140]   but we'd rather see the benefits of the industries
[00:30:50.140 --> 00:30:53.620]   afford after the R&D investments, right?
[00:30:53.620 --> 00:30:55.780]   - I have a related question there.
[00:30:55.780 --> 00:31:00.780]   So certainly I think like as an early stage fund,
[00:31:00.780 --> 00:31:03.620]   like many of these companies go straight
[00:31:03.620 --> 00:31:06.220]   to raising 40, $50 million.
[00:31:06.220 --> 00:31:08.180]   So we haven't been as focused
[00:31:08.180 --> 00:31:10.140]   on those types of investments.
[00:31:10.140 --> 00:31:13.980]   That said, one of the things that I've struggled with,
[00:31:13.980 --> 00:31:17.380]   or struggled to make a conclusion about
[00:31:17.380 --> 00:31:19.140]   is this model wherein these companies
[00:31:19.140 --> 00:31:24.140]   are research product hybrids,
[00:31:24.140 --> 00:31:28.660]   where their primary goal may not be commercializing a product
[00:31:28.660 --> 00:31:33.100]   or even driving towards profitability,
[00:31:33.100 --> 00:31:36.140]   but rather AGI.
[00:31:36.140 --> 00:31:39.300]   Even a year ago, somebody came to me
[00:31:39.300 --> 00:31:40.700]   with like one of these ideas
[00:31:40.700 --> 00:31:43.340]   where they wanted to do research,
[00:31:43.340 --> 00:31:45.500]   and in the course of doing research,
[00:31:45.500 --> 00:31:47.500]   spin off various products.
[00:31:47.500 --> 00:31:50.980]   And I remember at the time, like advising this person,
[00:31:50.980 --> 00:31:52.260]   like that's a bad idea.
[00:31:52.260 --> 00:31:55.940]   All ideas have some merit to them,
[00:31:55.940 --> 00:31:58.420]   but like that is going to be very challenging
[00:31:58.420 --> 00:31:59.780]   to execute upon.
[00:31:59.780 --> 00:32:03.060]   But I think back to that conversation
[00:32:03.060 --> 00:32:05.540]   and wonder like if I misguided that person.
[00:32:05.540 --> 00:32:09.660]   And I'd be curious to kind of hear your thoughts on that.
[00:32:09.660 --> 00:32:12.940]   Like a lot of these kind of AGI companies,
[00:32:12.940 --> 00:32:14.500]   like they're not only spending
[00:32:14.500 --> 00:32:17.020]   hundreds of millions of dollars on compute,
[00:32:17.020 --> 00:32:22.020]   but they're doing so in pursuit of AGI, not profitability.
[00:32:22.020 --> 00:32:23.500]   Like does that change
[00:32:23.500 --> 00:32:26.180]   how you would think about an investment?
[00:32:26.180 --> 00:32:28.980]   - For you though, Sarah, if you, let's say,
[00:32:28.980 --> 00:32:32.140]   I'm just trying to imagine like the big LLM
[00:32:32.140 --> 00:32:34.380]   foundational model company that you would have invested in.
[00:32:34.380 --> 00:32:36.940]   Like was there a version of the business model
[00:32:36.940 --> 00:32:40.300]   where like someone could have convinced you to say,
[00:32:41.540 --> 00:32:44.620]   yeah, let's invest and build one of these.
[00:32:44.620 --> 00:32:46.980]   Like what would that look like?
[00:32:46.980 --> 00:32:50.900]   - I mean, at the time, at least,
[00:32:50.900 --> 00:32:54.340]   like, and again, this is why I posed the question out there.
[00:32:54.340 --> 00:32:58.140]   Like I think I'd probably be more inclined
[00:32:58.140 --> 00:33:02.020]   to invest in a company that had as its primary focus,
[00:33:02.020 --> 00:33:06.420]   a goal of building and iterating on products
[00:33:06.420 --> 00:33:08.340]   rather than the split initiative
[00:33:08.340 --> 00:33:10.900]   of research and commercialization.
[00:33:11.900 --> 00:33:14.140]   I'm sure that there are, I mean,
[00:33:14.140 --> 00:33:16.100]   there are obviously other considerations
[00:33:16.100 --> 00:33:19.660]   and certainly perhaps like a big fear of mine
[00:33:19.660 --> 00:33:23.460]   is that like these companies will never have a margin profile
[00:33:23.460 --> 00:33:26.620]   that is aligned with like the tastes
[00:33:26.620 --> 00:33:28.380]   of public market investors.
[00:33:28.380 --> 00:33:32.220]   But again, I do wonder about this,
[00:33:32.220 --> 00:33:34.220]   this kind of split focus thing
[00:33:34.220 --> 00:33:36.300]   and if it's something that can exist
[00:33:36.300 --> 00:33:38.660]   or if it's something that needs to be resolved.
[00:33:40.020 --> 00:33:41.980]   - James, do you have an answer?
[00:33:41.980 --> 00:33:43.780]   - I was just gonna say,
[00:33:43.780 --> 00:33:46.700]   I think it will be hard at the time
[00:33:46.700 --> 00:33:48.380]   to have that split focus.
[00:33:48.380 --> 00:33:53.860]   I believe that there is important fundamental research
[00:33:53.860 --> 00:33:55.300]   that's happening here.
[00:33:55.300 --> 00:33:57.540]   I don't necessarily believe that,
[00:33:57.540 --> 00:34:00.540]   you know, getting to on stage around AGI
[00:34:00.540 --> 00:34:03.260]   is even the right answer.
[00:34:03.260 --> 00:34:08.260]   I think that there might be some really compelled moments
[00:34:09.380 --> 00:34:12.340]   in this whole journey that just solving
[00:34:12.340 --> 00:34:17.260]   for a better human machine symbiosis alone, right,
[00:34:17.260 --> 00:34:19.860]   across, you know, a thousand domains
[00:34:19.860 --> 00:34:24.860]   is more interesting to me than reaching a single AGI.
[00:34:24.860 --> 00:34:29.260]   And, you know, we'll see how it plays out,
[00:34:29.260 --> 00:34:33.900]   but I honestly think it is a generally quixotic journey
[00:34:33.900 --> 00:34:36.700]   to go down the AGI pathway now
[00:34:36.700 --> 00:34:40.220]   when there is an opportunity to build
[00:34:40.220 --> 00:34:45.220]   a thousand different domains of human machine symbiosis.
[00:34:45.220 --> 00:34:48.740]   - I think that there's a great question around
[00:34:48.740 --> 00:34:50.540]   whether or not these are railroad tracks
[00:34:50.540 --> 00:34:52.020]   in the sense that they become infrastructure
[00:34:52.020 --> 00:34:54.540]   that lasts for decades and not centuries.
[00:34:54.540 --> 00:34:56.580]   And I don't think so.
[00:34:56.580 --> 00:34:59.500]   I think that the hard part with these big models,
[00:34:59.500 --> 00:35:00.900]   at least right now,
[00:35:00.900 --> 00:35:04.340]   is that they do get cheaper and cheaper to make
[00:35:04.340 --> 00:35:07.780]   and, you know, every 10X or every 50X bigger,
[00:35:07.780 --> 00:35:09.540]   the cost of building it, you know,
[00:35:09.540 --> 00:35:10.980]   it's a mixture of both data, obviously,
[00:35:10.980 --> 00:35:13.660]   and sort of the size of the models, you know,
[00:35:13.660 --> 00:35:17.740]   that it's not clear that it's durable, right?
[00:35:17.740 --> 00:35:19.180]   And so that's probably, I think,
[00:35:19.180 --> 00:35:23.300]   the reason why smart people like George and Sarah avoided it
[00:35:23.300 --> 00:35:25.940]   and I don't have a great answer there.
[00:35:25.940 --> 00:35:27.820]   I mean, I think one claim was that
[00:35:27.820 --> 00:35:29.300]   there'd be secret process knowledge
[00:35:29.300 --> 00:35:33.740]   around acquiring and cleaning data,
[00:35:33.740 --> 00:35:35.540]   or there might be secret process knowledge
[00:35:35.540 --> 00:35:38.220]   around running very, very big systems.
[00:35:38.220 --> 00:35:40.700]   And that might turn out to be true, right?
[00:35:40.700 --> 00:35:43.300]   I mean, tell me what TSMC's moat is, right?
[00:35:43.300 --> 00:35:44.820]   Or one of these companies.
[00:35:44.820 --> 00:35:46.780]   And so that's where I'd be wrong, right?
[00:35:46.780 --> 00:35:49.860]   That if it's possible that it'll look like 50 years from now
[00:35:49.860 --> 00:35:52.260]   and there's gonna be the equivalent of TSMC
[00:35:52.260 --> 00:35:53.820]   where there'll just be a small set of folks
[00:35:53.820 --> 00:35:55.980]   who actually understand how to get something done
[00:35:55.980 --> 00:35:57.500]   at like the appropriate scale.
[00:35:57.500 --> 00:36:01.340]   I don't think so, but that would be the counter case.
[00:36:01.340 --> 00:36:04.220]   - So what types of startups would you guys,
[00:36:04.220 --> 00:36:06.340]   ML startups would you guys invest in today?
[00:36:06.340 --> 00:36:08.460]   We talked about large language models
[00:36:08.460 --> 00:36:09.820]   maybe not being a good bet,
[00:36:09.820 --> 00:36:12.380]   like everyone, every friend that I have
[00:36:12.380 --> 00:36:14.380]   is building something like a Jasper
[00:36:14.380 --> 00:36:16.460]   and like, it's like almost like a brain virus,
[00:36:16.460 --> 00:36:17.300]   like the crypto thing,
[00:36:17.300 --> 00:36:19.020]   but now everyone's building something like this.
[00:36:19.020 --> 00:36:22.220]   What should they be building and what would you invest in?
[00:36:22.220 --> 00:36:27.580]   - So I think Sarah alluded to it quite well.
[00:36:27.580 --> 00:36:30.860]   There's just plenty of other software domains
[00:36:30.860 --> 00:36:32.460]   to build amazing things, right?
[00:36:32.460 --> 00:36:36.260]   Look at the entire opportunity
[00:36:36.260 --> 00:36:39.460]   around where the PLM market is, right?
[00:36:39.460 --> 00:36:40.780]   A product life cycle market.
[00:36:40.780 --> 00:36:42.620]   Look at the entire opportunity
[00:36:42.620 --> 00:36:47.420]   around where you can do next generation
[00:36:47.420 --> 00:36:49.460]   computer aided design applications.
[00:36:49.460 --> 00:36:53.100]   Why would there be an incredible viewpoint
[00:36:53.100 --> 00:36:54.700]   to how the generic model would fit in?
[00:36:54.700 --> 00:36:59.500]   Look at the way currently testing software works.
[00:36:59.500 --> 00:37:02.220]   So yeah, there isn't like a great answer
[00:37:02.220 --> 00:37:04.660]   in terms of just how to really create
[00:37:04.660 --> 00:37:07.700]   better, more scaled out automation in the testing space.
[00:37:07.700 --> 00:37:12.700]   So I believe that there are plenty of other things
[00:37:12.700 --> 00:37:16.740]   to do than to write the next writing application.
[00:37:16.740 --> 00:37:18.900]   There's plenty of those,
[00:37:18.900 --> 00:37:21.260]   but if you've already been investing in today,
[00:37:21.260 --> 00:37:25.900]   I think as founders start to look at these opportunities,
[00:37:25.900 --> 00:37:28.660]   they'll find plenty of rain
[00:37:28.660 --> 00:37:30.940]   where even incumbents aren't moving as fast.
[00:37:30.940 --> 00:37:32.100]   Nothing in other areas,
[00:37:32.100 --> 00:37:33.740]   like the incumbents are moving pretty fast
[00:37:33.740 --> 00:37:35.620]   and they're gonna take up the space
[00:37:35.620 --> 00:37:40.540]   because they're just as smart as the in the upstart
[00:37:40.540 --> 00:37:42.500]   and more respected market.
[00:37:42.500 --> 00:37:45.140]   But I think there are definite opportunities
[00:37:45.140 --> 00:37:49.900]   where a dislocation is possible
[00:37:49.900 --> 00:37:52.300]   from just a new entry company early
[00:37:52.300 --> 00:37:56.740]   and building in an area that has been mostly
[00:37:56.740 --> 00:37:59.140]   static experience to be predictable software.
[00:37:59.140 --> 00:38:02.700]   - So I've got two observation.
[00:38:02.700 --> 00:38:04.340]   Oh, go ahead, Sarah.
[00:38:04.340 --> 00:38:06.620]   - Yeah, I was just going to say that I think like,
[00:38:06.620 --> 00:38:08.900]   you know, we had discussed this before,
[00:38:08.900 --> 00:38:12.660]   but like domain expertise do matter.
[00:38:12.660 --> 00:38:16.140]   Now, one of the reasons why I think,
[00:38:16.140 --> 00:38:19.060]   I don't wanna say like building developer tools is easy,
[00:38:19.060 --> 00:38:22.940]   but one of the kind of archetypes of an Amplify founder
[00:38:22.940 --> 00:38:25.740]   is somebody who has encountered a problem
[00:38:25.740 --> 00:38:28.660]   like in their job, built a solution,
[00:38:28.660 --> 00:38:31.420]   recognized that that solution could be applied
[00:38:31.420 --> 00:38:32.900]   in other contexts,
[00:38:32.900 --> 00:38:35.740]   and then decided to kind of commercialize that.
[00:38:35.740 --> 00:38:40.140]   I think there are all types of domains now, again,
[00:38:40.140 --> 00:38:44.060]   that have been kind of untouched by LLMs.
[00:38:44.060 --> 00:38:46.140]   But what is most important for like those
[00:38:46.140 --> 00:38:49.500]   who are considering building in this category
[00:38:49.500 --> 00:38:52.460]   is to focus on like problems that they understand
[00:38:52.460 --> 00:38:55.500]   rather than like going out to search for problems
[00:38:55.500 --> 00:38:59.260]   that could potentially be addressed by LLMs.
[00:38:59.260 --> 00:39:03.060]   We were early investors in a company called Brenway ML.
[00:39:03.060 --> 00:39:07.420]   And, you know, the last time that Chris Valenzuela,
[00:39:07.420 --> 00:39:10.500]   the founder of Brenway visited San Francisco,
[00:39:10.500 --> 00:39:13.380]   like we went on a walk and like visited
[00:39:13.380 --> 00:39:17.740]   the Diego Rivera murals at SFBOMA.
[00:39:17.740 --> 00:39:21.620]   Chris is an artist, like he's exhibited at like Lollapalooza
[00:39:21.620 --> 00:39:24.900]   and like museums throughout Santiago.
[00:39:24.900 --> 00:39:26.820]   He understands creatives.
[00:39:26.820 --> 00:39:30.620]   He builds tools for people who he understands deeply
[00:39:30.620 --> 00:39:32.340]   because he's been in their shoes.
[00:39:32.340 --> 00:39:34.940]   And I think like we need to see more of that.
[00:39:34.940 --> 00:39:38.220]   I hope that kind of be the in the hacker community
[00:39:38.220 --> 00:39:42.780]   around LLMs kind of expand more into, you know,
[00:39:42.780 --> 00:39:46.500]   those with domain expertise in other verticals.
[00:39:46.500 --> 00:39:49.220]   And then I do eventually think that we will see more,
[00:39:49.220 --> 00:39:51.060]   you know, LLM tools and infra.
[00:39:53.500 --> 00:39:55.020]   - Okay, so here's my observation.
[00:39:55.020 --> 00:39:56.980]   My observation is in the same way
[00:39:56.980 --> 00:39:59.460]   that every great web application developer
[00:39:59.460 --> 00:40:02.500]   when starting Ruby on Rails or starting anything new
[00:40:02.500 --> 00:40:04.020]   makes a to-do app, right?
[00:40:04.020 --> 00:40:05.620]   They just can't help themselves.
[00:40:05.620 --> 00:40:07.740]   You know, everyone should build a little writing app
[00:40:07.740 --> 00:40:08.740]   and then they should sort of like,
[00:40:08.740 --> 00:40:10.220]   think of it as like a palette cleanser.
[00:40:10.220 --> 00:40:12.060]   You need to get it out of your system.
[00:40:12.060 --> 00:40:14.260]   And then you'll learn a lot about like the boundaries
[00:40:14.260 --> 00:40:16.660]   of the system and then do something else, right?
[00:40:16.660 --> 00:40:20.460]   But the real answer is you do have to try first, right?
[00:40:20.460 --> 00:40:23.780]   And the writing app is like the obvious first easy thing
[00:40:23.780 --> 00:40:26.660]   to do and the thing that hits your head and that's fine.
[00:40:26.660 --> 00:40:28.220]   These are probably move on pretty quickly.
[00:40:28.220 --> 00:40:30.260]   Unless you have the JASTA,
[00:40:30.260 --> 00:40:32.580]   which means don't tell George about it,
[00:40:32.580 --> 00:40:33.700]   but tell me and Sarah.
[00:40:33.700 --> 00:40:40.420]   - Okay, I mean, the funny thing is that like,
[00:40:40.420 --> 00:40:43.980]   I actually, so like every, not every developer,
[00:40:43.980 --> 00:40:46.660]   but like many developers at some point in their life
[00:40:46.660 --> 00:40:48.500]   have like built a note taking app.
[00:40:49.900 --> 00:40:51.740]   And you know, the funny thing is that like,
[00:40:51.740 --> 00:40:55.700]   I actually think building a like killer LLM driven
[00:40:55.700 --> 00:40:59.900]   note taking app is an incredibly like challenging feat
[00:40:59.900 --> 00:41:01.660]   because going back to what I was saying,
[00:41:01.660 --> 00:41:03.580]   like it now needs to be collaborative.
[00:41:03.580 --> 00:41:06.300]   Like, frankly, like I've tried some of these LLM driven
[00:41:06.300 --> 00:41:08.980]   text editors and I'm like, you don't have commenting,
[00:41:08.980 --> 00:41:13.540]   sharing, collaboration, like that's a deal breaker for me.
[00:41:13.540 --> 00:41:15.260]   It needs to be fast.
[00:41:15.260 --> 00:41:19.180]   So perhaps I guess to your point, James,
[00:41:19.180 --> 00:41:22.700]   like in attempting to build some of these things,
[00:41:22.700 --> 00:41:24.300]   I think we'll start to see too,
[00:41:24.300 --> 00:41:27.940]   like the limitations of LLMs and just how hard it is
[00:41:27.940 --> 00:41:30.580]   to kind of build a killer product
[00:41:30.580 --> 00:41:33.060]   that is not just a killer LLM driven product,
[00:41:33.060 --> 00:41:35.500]   but like a killer product in its own right.
[00:41:35.500 --> 00:41:38.500]   - Well, Sarah, you mentioned a note taking piece.
[00:41:38.500 --> 00:41:40.380]   It was interesting to hear that, right?
[00:41:40.380 --> 00:41:42.860]   Because I was mentioning some of these moments
[00:41:42.860 --> 00:41:46.380]   where in condoms can just as easily come into your market.
[00:41:46.380 --> 00:41:49.300]   And if you look at what Notion has done, right?
[00:41:49.300 --> 00:41:50.140]   - Yeah.
[00:41:50.140 --> 00:41:53.140]   - One of the more modern day taking applications
[00:41:53.140 --> 00:41:55.420]   at the moment as well,
[00:41:55.420 --> 00:41:58.620]   they introduce a generative capability effectively
[00:41:58.620 --> 00:42:02.340]   as a feature into what Notion does is a note taking app.
[00:42:02.340 --> 00:42:05.540]   So it actually does point to there are moments
[00:42:05.540 --> 00:42:10.220]   where it is, you know, incumbents even in existing markets
[00:42:10.220 --> 00:42:14.300]   and makes them pretty interesting, innovative features
[00:42:14.300 --> 00:42:18.100]   just by the fact that they're introducing an LLM
[00:42:18.100 --> 00:42:20.820]   into how your application works.
[00:42:20.820 --> 00:42:27.540]   - It's nice to know that like the startup advice
[00:42:27.540 --> 00:42:29.980]   still hearing from you guys is the same,
[00:42:29.980 --> 00:42:32.140]   which is like pick a problem that you understand,
[00:42:32.140 --> 00:42:33.860]   make sure you have domain expertise,
[00:42:33.860 --> 00:42:35.940]   then see if LLMs are fit there.
[00:42:35.940 --> 00:42:37.660]   But like, even if you do make sure
[00:42:37.660 --> 00:42:40.540]   that user experience is fast and like, it's really good,
[00:42:40.540 --> 00:42:42.860]   like teams that are innovative
[00:42:42.860 --> 00:42:44.260]   and then also find a founding team
[00:42:44.260 --> 00:42:47.020]   where someone has the domain expertise
[00:42:47.020 --> 00:42:49.020]   and then someone understands machine.
[00:42:49.020 --> 00:42:52.460]   Is that a good summary of like what startups should be doing?
[00:42:52.460 --> 00:42:53.540]   - Yeah, absolutely.
[00:42:53.540 --> 00:42:55.220]   And I hope that we're seeing the same thing
[00:42:55.220 --> 00:42:56.820]   in like 25 years too.
[00:42:56.820 --> 00:42:59.340]   Like, I don't know that any of that changes.
[00:42:59.340 --> 00:43:01.620]   You just like kind of swap off, you know,
[00:43:01.620 --> 00:43:05.740]   like LLMs for, I don't know, like Voodoo Mind Tricks
[00:43:05.740 --> 00:43:08.500]   and just like same.
[00:43:08.500 --> 00:43:09.340]   - Yeah.
[00:43:09.340 --> 00:43:10.340]   - Okay, now there is a nuance though, right?
[00:43:10.380 --> 00:43:13.140]   Which is some of this depends on where we are in the cycle.
[00:43:13.140 --> 00:43:17.500]   And so, you know, building a cloud-based application in 2018
[00:43:17.500 --> 00:43:20.740]   is very different than 2020 versus 2010, right?
[00:43:20.740 --> 00:43:25.180]   And so when you are so, so early in the cycle
[00:43:25.180 --> 00:43:27.500]   and everyone's just trying to figure everything out,
[00:43:27.500 --> 00:43:29.940]   I think there are a bunch of design decisions
[00:43:29.940 --> 00:43:31.980]   and assumptions that founders are gonna have to make.
[00:43:31.980 --> 00:43:33.260]   And there's gonna be lots of work
[00:43:33.260 --> 00:43:35.020]   they're gonna have to do that honestly
[00:43:35.020 --> 00:43:39.180]   might turn out to be commodified a year from now.
[00:43:39.180 --> 00:43:41.940]   And like, I think the task of the great founders
[00:43:41.940 --> 00:43:43.860]   which are like roll with the punches, right?
[00:43:43.860 --> 00:43:46.140]   And sort of like lean in on that.
[00:43:46.140 --> 00:43:47.220]   - That's a good point.
[00:43:47.220 --> 00:43:49.580]   Okay, we've got five minutes left.
[00:43:49.580 --> 00:43:51.300]   Let's get into the lightning round.
[00:43:51.300 --> 00:43:52.940]   So I'm gonna pose a bunch of questions.
[00:43:52.940 --> 00:43:56.540]   If you guys can keep your answers to 15, 20 seconds each
[00:43:56.540 --> 00:44:00.460]   and we'll go in the order of maybe Sarah, George
[00:44:00.460 --> 00:44:02.500]   and James to end this.
[00:44:02.500 --> 00:44:04.580]   Cool.
[00:44:04.580 --> 00:44:08.300]   Oops, I lost my questions, right?
[00:44:08.300 --> 00:44:09.180]   Cool.
[00:44:09.180 --> 00:44:12.860]   So the first question is how does Stability make money
[00:44:12.860 --> 00:44:15.860]   when they're committed to open sourcing everything they do?
[00:44:15.860 --> 00:44:19.460]   - No comment.
[00:44:19.460 --> 00:44:22.620]   - George.
[00:44:22.620 --> 00:44:25.900]   - I'm still figuring that out.
[00:44:25.900 --> 00:44:29.580]   - I think that they've been savvy
[00:44:29.580 --> 00:44:33.460]   about doing a bunch of BD investments and connections
[00:44:33.460 --> 00:44:36.220]   that may result in like lots and lots of revenue
[00:44:36.220 --> 00:44:37.460]   over the next five years.
[00:44:38.460 --> 00:44:39.300]   - What do you think?
[00:44:39.300 --> 00:44:40.140]   Do you believe?
[00:44:40.140 --> 00:44:43.380]   Is prompt engineering a real job
[00:44:43.380 --> 00:44:47.140]   that might happen in the next year or so?
[00:44:47.140 --> 00:44:52.700]   - Software development is a real job.
[00:44:52.700 --> 00:44:54.860]   And I anticipate that in the future,
[00:44:54.860 --> 00:44:57.180]   prompt engineering will become part
[00:44:57.180 --> 00:44:58.980]   of the software development role.
[00:44:58.980 --> 00:45:04.580]   - I think prompt engineering is gonna be
[00:45:04.580 --> 00:45:06.980]   a critical skill set for every product manager.
[00:45:06.980 --> 00:45:07.820]   - Okay.
[00:45:07.820 --> 00:45:10.780]   - Jane.
[00:45:10.780 --> 00:45:15.900]   - I think it might be even bigger than that.
[00:45:15.900 --> 00:45:18.500]   - So it sounds like every software engineer,
[00:45:18.500 --> 00:45:20.340]   every PM and almost everyone needs
[00:45:20.340 --> 00:45:22.060]   to learn prompt engineering.
[00:45:22.060 --> 00:45:23.340]   Got it.
[00:45:23.340 --> 00:45:26.060]   Are there any parallels between Bitcoin and AI
[00:45:26.060 --> 00:45:26.980]   that you're seeing?
[00:45:26.980 --> 00:45:31.740]   - There are definitely a lot of people flowing
[00:45:31.740 --> 00:45:34.100]   from crypto into AI.
[00:45:34.100 --> 00:45:36.820]   There's definitely a lot of hype behind AI.
[00:45:36.820 --> 00:45:40.100]   I think there is more impact
[00:45:40.100 --> 00:45:44.820]   to be immediately unlocked with AI than crypto.
[00:45:44.820 --> 00:45:48.180]   - And more precedent too.
[00:45:48.180 --> 00:45:51.300]   Like we've been talking about AI for...
[00:45:51.300 --> 00:45:53.780]   - Since 1950.
[00:45:53.780 --> 00:45:54.620]   - Yeah, yeah.
[00:45:54.620 --> 00:45:56.500]   I was going to say close to century,
[00:45:56.500 --> 00:46:01.500]   not quite there yet, but like for a long, long time.
[00:46:01.500 --> 00:46:03.140]   - Yeah.
[00:46:04.140 --> 00:46:05.260]   - I would be really excited
[00:46:05.260 --> 00:46:07.380]   if all crypto investors kept doing crypto.
[00:46:07.380 --> 00:46:09.300]   - Me too.
[00:46:09.300 --> 00:46:14.300]   - I think there are dynamics around building
[00:46:14.300 --> 00:46:17.420]   some of these models and the capital requirements
[00:46:17.420 --> 00:46:20.500]   and expectations and the sort of increasing,
[00:46:20.500 --> 00:46:22.780]   sort of expectations of like increasing investment
[00:46:22.780 --> 00:46:24.980]   that are similar to crypto.
[00:46:24.980 --> 00:46:29.700]   - What is the most overhyped
[00:46:29.700 --> 00:46:31.420]   machine learning company right now?
[00:46:32.420 --> 00:46:34.580]   (silence)
[00:46:34.580 --> 00:46:41.580]   - I don't want to answer that.
[00:46:41.580 --> 00:46:43.060]   I don't know.
[00:46:43.060 --> 00:46:43.900]   - Yeah.
[00:46:43.900 --> 00:46:45.940]   I don't have a good answer
[00:46:45.940 --> 00:46:49.740]   on like the most overhyped company right now.
[00:46:49.740 --> 00:46:50.580]   Yeah.
[00:46:50.580 --> 00:46:51.540]   That's a tough one.
[00:46:51.540 --> 00:46:53.060]   - James is thinking.
[00:46:53.060 --> 00:46:54.420]   - I mean, I think it's just interesting actually
[00:46:54.420 --> 00:46:56.860]   that I kind of think that
[00:46:59.740 --> 00:47:01.940]   on the ML side, actually,
[00:47:01.940 --> 00:47:03.980]   the marketing has been actually kind of bad
[00:47:03.980 --> 00:47:05.260]   and behind the times, right?
[00:47:05.260 --> 00:47:08.500]   But actually I kind of think that there is room
[00:47:08.500 --> 00:47:10.860]   for some ML company to get better at marketing
[00:47:10.860 --> 00:47:12.300]   if they get really overhyped.
[00:47:12.300 --> 00:47:15.460]   - Cool.
[00:47:15.460 --> 00:47:16.300]   Last question.
[00:47:16.300 --> 00:47:18.740]   If you were starting a machine learning company today,
[00:47:18.740 --> 00:47:20.180]   what startup would you start?
[00:47:20.180 --> 00:47:26.380]   - My love has always been in tools and infra.
[00:47:26.380 --> 00:47:28.420]   I think there will be a lot of challenges
[00:47:28.420 --> 00:47:30.740]   that companies face, particularly as they scale
[00:47:30.740 --> 00:47:33.180]   beyond the prototyping phase with LLMs
[00:47:33.180 --> 00:47:37.780]   and the LLM APIs into the building LLMs
[00:47:37.780 --> 00:47:39.140]   into existing products.
[00:47:39.140 --> 00:47:41.500]   So I would start a tools company.
[00:47:41.500 --> 00:47:42.500]   I'd have to.
[00:47:42.500 --> 00:47:44.060]   It's the only thing I could do.
[00:47:44.060 --> 00:47:48.620]   - Yeah.
[00:47:48.620 --> 00:47:50.340]   For my own skill sets,
[00:47:50.340 --> 00:47:54.460]   I would also feel like starting a tools company,
[00:47:54.460 --> 00:47:56.340]   particularly in the data analytics space
[00:47:56.340 --> 00:47:58.900]   would be pretty exciting at this moment.
[00:47:58.900 --> 00:48:01.780]   I also see some pretty compelling opportunities
[00:48:01.780 --> 00:48:04.740]   for founders who've had previous experience,
[00:48:04.740 --> 00:48:07.820]   particularly in certain domains that have been regulated,
[00:48:07.820 --> 00:48:09.940]   like healthcare and financial services.
[00:48:09.940 --> 00:48:13.060]   In those markets, you can build
[00:48:13.060 --> 00:48:15.820]   a regulated domain-specific model,
[00:48:15.820 --> 00:48:17.500]   which would be pretty exciting
[00:48:17.500 --> 00:48:20.300]   and what you can accomplish with those models,
[00:48:20.300 --> 00:48:24.020]   particularly in healthcare and financial services.
[00:48:24.020 --> 00:48:26.380]   And that wouldn't be myself doing that,
[00:48:26.380 --> 00:48:29.540]   but I could definitely see some incredible calendars
[00:48:29.540 --> 00:48:32.740]   to work in a domain that they've had previous experience.
[00:48:32.740 --> 00:48:35.660]   - George will be your chairman
[00:48:35.660 --> 00:48:37.740]   if you convince him to join and help you.
[00:48:37.740 --> 00:48:41.900]   I think the big opportunity right now
[00:48:41.900 --> 00:48:45.060]   that's being missed is the LLMs talking to themselves.
[00:48:45.060 --> 00:48:46.260]   I think that in all likelihood,
[00:48:46.260 --> 00:48:50.180]   the interfaces will be sort of maybe a thousand times more
[00:48:50.180 --> 00:48:52.460]   between LLMs talking to themselves
[00:48:52.460 --> 00:48:54.620]   rather than like a human interface.
[00:48:54.620 --> 00:48:57.180]   - So you're building a dating app?
[00:48:57.180 --> 00:49:04.300]   - Or a boarding management app.
[00:49:04.300 --> 00:49:05.500]   Boarding management app.
[00:49:05.500 --> 00:49:08.700]   - All right, thank you guys so much for coming.
[00:49:08.700 --> 00:49:10.220]   That was great discussion.
[00:49:10.220 --> 00:49:11.060]   We'll drop your Twitter
[00:49:11.060 --> 00:49:13.260]   so people can find you and ask you more questions.
[00:49:13.260 --> 00:49:15.500]   I really appreciate George and James
[00:49:15.500 --> 00:49:18.380]   for all of your advice that you've given us over the years.
[00:49:18.380 --> 00:49:20.700]   And Sarah, hope to work with you in the future,
[00:49:20.700 --> 00:49:22.780]   but also love talking to you.
[00:49:22.780 --> 00:49:23.620]   Thank you guys.
[00:49:23.620 --> 00:49:26.220]   (upbeat music)
[00:49:26.220 --> 00:49:28.800]   (upbeat music)
[00:49:28.800 --> 00:49:30.800]   You

