
[00:00:00.000 --> 00:00:12.960]   Hi, everybody. Thank you for joining the session today. As Charles mentioned, I would like to talk about machine learning opera ops and metadata and why your machine learning projects probably need that and
[00:00:12.960 --> 00:00:21.240]   My name is Hannes. I'm a machine learning engineer at SAP concur and I'm also a Google Developer Expert for machine learning. And I was very
[00:00:21.480 --> 00:00:30.800]   Fortunate to be a co author of two machine learning complications one around natural language processing and the most recent one with a rally around machine learning pipelines with TensorFlow extended
[00:00:30.800 --> 00:00:43.040]   So when we normally talk about machine learning. We always focus on the machine learning code and as described in the very famous paper of the hidden technical debt of machine learning systems.
[00:00:44.040 --> 00:00:59.200]   When we bring our machine learning models to the real world. We need a variety of systems around it. In that case, we're talking about data collection systems. We need to verify the data monitor the models, we need to
[00:01:00.320 --> 00:01:12.280]   Extract the features do feature engineering do the model analysis. And then we also need to serve those models. And when we talk about machine learning ops. It's all about like how can we orchestrate those
[00:01:12.280 --> 00:01:19.520]   Those different tools that they can work together and that we can get through an interim process, probably, or hopefully automatically
[00:01:19.520 --> 00:01:22.840]   So normally in data science projects.
[00:01:23.720 --> 00:01:29.680]   Often the world looks like this. We have a business problem. We're trying to solve this business problem with a machine learning solution.
[00:01:29.680 --> 00:01:41.800]   And in that case, we developed a machine learning model and at some point, the work of a data scientist could be considered done because the business problem is being sold a solved. We can predict a value or we can
[00:01:43.520 --> 00:01:56.920]   Sort of like predict time series is labeled, etc. But I would say that it's not the case because often we need to analyze our models there like we should do this consistently. And we also need to sort of get to a point where we
[00:01:56.920 --> 00:02:09.280]   Update those models continuously because as soon as we release those models to the real world, the world will interact with the model and and vice versa. And sort of that those systems will influence each other and
[00:02:11.000 --> 00:02:23.600]   Change like say the feedback loop will change the model itself and set things like this at the same point in time as soon as our models hits the real world. We need to make sure that we can reproduce these models continuously.
[00:02:23.600 --> 00:02:36.560]   And we also would like to track all the artifacts going into a model through things like an audit trail or traceability and things like this. So that's where machine learning ops and M OPS come in.
[00:02:37.960 --> 00:02:42.560]   So normally what happens to machine trained machine learning models, as I said earlier, it's
[00:02:42.560 --> 00:02:50.320]   We get to a point where we can solve a data science problem, but then getting getting that machine learning model into the real world. There's another
[00:02:50.320 --> 00:03:00.480]   Set of tools we need to connect with. And it's a quite another burden and those models being released and we see data drift. We see changes in the data schema.
[00:03:01.120 --> 00:03:09.640]   We see changing pre processing steps complicated retraining processes and potentially like high latencies when it comes to serving those models.
[00:03:09.640 --> 00:03:17.680]   And those are currently in a lot of situations manual steps with data scientists have to do to keep those models up and running.
[00:03:18.320 --> 00:03:31.920]   And that keeps them away from like being innovative and really focusing on new data science projects. And so that's why the idea of machine learning ops came up a few years ago and now sort of like crystallized into what we
[00:03:31.920 --> 00:03:36.640]   Nowadays, see with TensorFlow extended and other competitor platforms.
[00:03:36.640 --> 00:03:43.000]   So a machine learning life cycle would look like this. We start with a data ingestion, as I mentioned, we go through the data validation.
[00:03:43.360 --> 00:03:49.560]   Feature engineering model training and then a very important part happens with we take a look at the model itself.
[00:03:49.560 --> 00:04:01.640]   I can just stress like let's not just focus on the aggregated accuracy of a model, but let's drill into the features and maybe slicing the individual features again and make sure that
[00:04:01.640 --> 00:04:11.040]   The model produces predictions equally in this case, or like hopefully unbiased and then we need to validate that model comparing the model against previous runs
[00:04:11.600 --> 00:04:19.200]   And is it performing better. Sometimes we have criteria where like, okay, we accept drop in a certain accuracy. If the model is faster, for example.
[00:04:19.200 --> 00:04:27.600]   But we basically have to compare against previous runs and then we can start deploying those machine learning models. Once the machine learning model is out, we basically
[00:04:27.600 --> 00:04:35.800]   Can capture more data with a new model version and ingest that back into our machine learning pipeline. So that could be a continuous running cycle in this case.
[00:04:36.600 --> 00:04:44.560]   But very important is like those components we see in those machine learning pipelines. They're entangled. And so, for example, we
[00:04:44.560 --> 00:04:59.040]   We would use the output of a particular component. Let's say the schema generation and that will be reused during the training process or let's say that feature engineering process because we need to know what the rough data structure of the the data set looks like.
[00:05:00.960 --> 00:05:12.840]   And so they're intertwined and it's not just a linear process as we know this from the software engineering world where we get a source code load all the dependencies compile the code and then ship an artifact
[00:05:12.840 --> 00:05:24.560]   So there's a lot of interdependencies here. And that's where TensorFlow Extended comes in because it gives us a chance to express our pipelines through the components.
[00:05:25.160 --> 00:05:32.880]   And we can then define them specifically for our machine learning project and orchestrate them on a on a platform which I will show in a second.
[00:05:32.880 --> 00:05:45.560]   Those the TensorFlow Extended project is basically a portfolio of different libraries and then they provide the glue code to coordinate those libraries with each other.
[00:05:46.240 --> 00:06:05.040]   And, but you can also run them as standalone libraries. For example, TensorFlow data validation could work without any problems in a PyTorch project or in a scikit-learn project. And as you will see in a second here, you can generate statistics around data sets in almost independently from
[00:06:05.040 --> 00:06:07.120]   The framework itself.
[00:06:08.600 --> 00:06:20.200]   So then once we define the individual components, we can tie them together into a pipeline and then we can orchestrate those pipelines on Apache Airflow, Apache Beam, or Kubeflow pipelines these days.
[00:06:20.200 --> 00:06:29.440]   And each component will talk to a metadata store to receive references to inputs to the pipelines and then manipulate something
[00:06:30.800 --> 00:06:38.000]   As a part of that component, let's say generate a statistics of the data set or train a model or validate a model and then
[00:06:38.000 --> 00:06:44.160]   Store the outputs and store the references on those outputs of the component in the metadata store again.
[00:06:44.160 --> 00:06:50.600]   And in a second, I will show you some interesting use cases where we can use that metadata in machine learning projects.
[00:06:50.600 --> 00:06:59.040]   You can also coordinate or orchestrate your pipelines with JupyterHub, but then in that moment, the human is the orchestrator, but it's a really good way of like
[00:06:59.640 --> 00:07:09.800]   Trying your machine learning pipelines debugging it and once everything runs, you can export them to the other platforms and then continuously run your machine learning updates.
[00:07:09.800 --> 00:07:22.640]   So how does it work like in the real world in terms of just to show a little bit of code here. We basically define the individual components as Python code. So here in this particular case we
[00:07:24.360 --> 00:07:36.480]   Define a data set or an input generation or input component and we say that the data which is being imported here we split that data three to one to have
[00:07:36.480 --> 00:07:45.680]   A quarter goes into our validation set and three quarters go into the training set and then we have a variety of components. In this case, we ingest from a
[00:07:46.520 --> 00:07:56.840]   TF record component. So we can ingest TF record files, but there's a variety of opportunities to ingest data from that could be local files or that could be remote database systems.
[00:07:56.840 --> 00:08:00.160]   Where we can start our machine learning pipelines from
[00:08:00.160 --> 00:08:05.840]   Once the data is ingested, we can generate those statistics. I'll show you an example here in a second.
[00:08:05.840 --> 00:08:08.040]   But then we can also
[00:08:09.960 --> 00:08:16.960]   Generate a schema and we can with every model version being produced, we can compare the statistics generated
[00:08:16.960 --> 00:08:26.880]   For that particular run and then we can also compare the schema and sometimes we have situations where a database admin drops an entire column.
[00:08:27.920 --> 00:08:34.720]   And or data becomes the sparse feature becomes sparse all of a sudden, sorry, dense feature becomes sparse.
[00:08:34.720 --> 00:08:40.800]   And in that case, we can alert data scientists automatically and say, hey, is something wrong in your pipeline, please take a look.
[00:08:40.800 --> 00:08:54.240]   So as an example, once we run that component, we can then visualize the output data structure into this very nice visualization for numerical and categorical features and show basic statistical
[00:08:56.000 --> 00:09:03.400]   Information about the data set. And there's also options of comparing data sets with each other, but it's, it might not be
[00:09:03.400 --> 00:09:09.280]   Crazy for like a statistician here, but for getting a good overview of
[00:09:09.280 --> 00:09:17.760]   For a data set or seeing what has changed since the last run. I think this is a really good, wonderful tool to take a look at those components.
[00:09:18.400 --> 00:09:27.960]   Sorry to take a look at those data sets. We can then move on and define similar components for like our feature engineering model training model analysis and the deployment.
[00:09:27.960 --> 00:09:35.040]   And then we can tie this together. As I said, into a pipeline and we can actually we can orchestrate that pipeline.
[00:09:35.040 --> 00:09:43.120]   And that's where I would say where the magic happens where we can remove the burden for the data scientists to run all those steps manually.
[00:09:43.520 --> 00:09:55.560]   Basically, define those individual components as a as a list. And then those runners, which are provided by TensorFlow extended define the dependencies between the components and generate
[00:09:55.560 --> 00:10:11.720]   The instructions for Apache be more airflow or in the case of couple of pipelines that generates the order instructions which we can then upload or push to our crew for pipelines environments and there we can then execute them. So here's an example for Apache beam.
[00:10:13.280 --> 00:10:24.720]   It determined all the different dependencies of the components and then it starts executing from the first to the last component and if everything runs smoothly would probably have we're hopefully have an exportable model at the end of the day.
[00:10:24.720 --> 00:10:37.920]   So where to start. If you're, if you have a machine learning project and you you run this in production or you want to bring a machine learning model into production, then there is a
[00:10:39.360 --> 00:10:47.520]   Where to start with TFX the TensorFlow team has provided wonderful example projects for various use cases.
[00:10:47.520 --> 00:11:00.680]   We have written about one additional use case we saw at conquer labs, which was deploying bird models for various applications and in the context of TensorFlow extended there's wonderful benefits when it comes to
[00:11:01.240 --> 00:11:10.200]   TensorFlow transform TensorFlow text and tying this all together and those pipelines, we can really simplify and the deployments and make them scalable.
[00:11:10.200 --> 00:11:26.080]   One thing I hear sometimes when I discuss machine learning pipelines with colleagues that oh wait, we have a we have a CI CD system for software tools already. We can just run that our machine learning pipelines in the same tool. We don't need anything new for that.
[00:11:26.920 --> 00:11:36.440]   And I would say, be really careful with that because, as I said earlier, those pipeline runs are interdependent. So the results from
[00:11:36.440 --> 00:11:44.480]   The current run will depend on the previous run. So we always want to compare against previous data sets or previous model runs and things like this.
[00:11:44.480 --> 00:11:56.040]   And also at the same time, we really need to watch out scaling that we can scale out those pipelines and TensorFlow extended has this wonderful case where it uses Apache being behind the scenes.
[00:11:56.640 --> 00:11:57.120]   And
[00:11:57.120 --> 00:12:04.600]   In with Apache being we can export those data heavy tasks to a data cluster.
[00:12:04.600 --> 00:12:08.040]   Or yeah, can be compute cluster.
[00:12:08.040 --> 00:12:17.720]   And earlier I said, like, let's take a look at the metadata and I think the metadata can be an insurance policy for machine learning.
[00:12:18.240 --> 00:12:30.640]   And with insurance policy. Let's please not take this very literal, but when we deploy machine learning models. I think there we should be able to answer questions in the future about a model we have produced today.
[00:12:30.640 --> 00:12:39.640]   So normal questions could be like what data set was being used. What was the distribution of a certain feature of that data set.
[00:12:40.320 --> 00:12:50.880]   What was the validation. What was certain validation results per feature of that particular model. How much better was the model against the previous run, etc. Maybe who signed off on it.
[00:12:50.880 --> 00:12:57.040]   And so the metadata. When we, when we take a look at the metadata. We can basically just
[00:12:57.960 --> 00:13:03.960]   Answer a lot of questions around machine learning and machine learning and and like real world scenarios we are we're sort of like
[00:13:03.960 --> 00:13:15.200]   Affected by privacy concerns legal compliances, etc. All those things. I hope we can answer at some point with a metadata we produce during those pipeline months.
[00:13:15.200 --> 00:13:23.240]   And TensorFlow extended is really great components are designed in a very standardized way every component consists out of three parts.
[00:13:24.160 --> 00:13:35.720]   They have a driver and execute and a publisher and the driver will receive the references as inputs and look and load them from the metadata store and the publisher will take the results.
[00:13:35.720 --> 00:13:46.240]   After the manipulation of the data or the train model from the executor and save those artifacts and the references of those artifacts in the metadata store.
[00:13:47.320 --> 00:13:54.200]   And then based on that we can build really interesting solutions from that. So for example, we can build collaboration tools.
[00:13:54.200 --> 00:14:02.800]   We can use collaboration tools in this case it's TensorFlow model analysis and data validation. So we can generate those statistics and those overviews about the
[00:14:02.800 --> 00:14:16.360]   The results of a particular model run and then store them away and archive them. And if there are questions we can come back and reopen them and discuss them as a team in a on a through a website or platform. We can also
[00:14:17.360 --> 00:14:29.360]   Investigate the model lineage and say, okay, we have a we have a model. Let's see. Let's take a look at all the artifacts which went into that model output or this exported model and see what could have potentially caused an issue there.
[00:14:29.360 --> 00:14:42.240]   And then we can there's things like warm starting so we might be restricted to use smaller data sets due to GDPR compliance, but we can basically then fine tune existing models.
[00:14:43.280 --> 00:14:48.800]   Through warm starting and the metadata allows us to go back to the last successful run
[00:14:48.800 --> 00:14:51.120]   For a particular model.
[00:14:51.120 --> 00:14:57.440]   And then another use case. I just want to mention here quickly is like we can
[00:14:57.440 --> 00:15:04.320]   Put a human in the loop and we can automate everything. But at the very end, we can ask a human in this case.
[00:15:05.440 --> 00:15:16.840]   We use this like component that component publishes a tiny like a message redirects the data scientists to the pipeline. They can basically investigate
[00:15:16.840 --> 00:15:25.720]   The model analysis feedback and see if everything doesn't look good and they can poke the model, a little bit and then sign off on the model and we would track this in the metadata as well.
[00:15:26.920 --> 00:15:37.640]   So then if we put both together the machine learning ops with TensorFlow Extended and the metadata, we can remove the burden from the data scientists to manually walk through all those steps.
[00:15:37.640 --> 00:15:51.440]   Yeah, in manual fashion and therefore we can also do this consistently like because we automated, we can make sure that every model goes through the same validation process and
[00:15:52.360 --> 00:16:00.400]   Analysis process and then also we can also produce those pipelines in or we can hopefully reproduce our models.
[00:16:00.400 --> 00:16:06.920]   In a consistent way we store the data sets we have our references to particular versions. So we can pick a version and then
[00:16:06.920 --> 00:16:11.760]   Go back to all the artifacts which affected that model and reproduce that model that is the
[00:16:11.760 --> 00:16:20.760]   That's hopefully what we can achieve with those with those pipeline pipelines. If this was a too fast of a run through. We have an additional
[00:16:21.160 --> 00:16:38.960]   Reference here. That's the Riley book which we recently published in in July and that I was very fortunate to co author that with Catherine Nelson and it's now available as print and digital version. So with that, I would say thank you. And if there are questions, I'm more than happy to take some
[00:16:38.960 --> 00:16:47.920]   Great. Yeah. Thanks a lot. Honest for a really, really interesting talk a nice overview of this the TensorFlow Extended tool.
[00:16:48.280 --> 00:16:55.440]   I know there's a lot of folks in our audience who are really interested in, you know, reproducible like high quality machine learning sort of getting out of just
[00:16:55.440 --> 00:17:03.200]   Jupyter notebooks on your laptop and making something that can work in production. So the first question is from YouTube from William limer on
[00:17:03.200 --> 00:17:08.160]   So you had all those slides where you showed that sort of feedback loop with
[00:17:08.160 --> 00:17:17.240]   With the metadata and the feedback loop of this pipeline. So they so they just wanted to confirm. So can we make a sort of feedback loop in our pipeline.
[00:17:17.440 --> 00:17:28.120]   Where our models updated with new data sort of on a set schedule like every day. Let's, you know, run a sort of like cron job to update this model with new data and push it out there. Is that something you support.
[00:17:28.120 --> 00:17:37.960]   Yes. So I'm tools like good flow pipelines. They already have that feature in there that you can rerun pipelines on a continuous basis.
[00:17:38.880 --> 00:17:46.920]   The, the tricky part here is to how to capture the data that's very problem specific. There hasn't been like one tool out there where
[00:17:46.920 --> 00:17:56.840]   You can sort of like ended and make it or tweak it for every application. So there's a lot of like custom applications custom tools coming in there.
[00:17:56.840 --> 00:18:03.120]   There's also different types of feedback. There's direct and indirect feedback and explicit and implicit feedback in terms of like
[00:18:03.560 --> 00:18:15.200]   Are we just saying the prediction was wrong or are we correcting the prediction. There's different types. And so therefore, sort of the bit has a lot to do with like how does the UI work. What's the application.
[00:18:15.200 --> 00:18:25.440]   Who is validating those results. But in general, you can continuously run those pipelines all the orchestrators have features to run them on a on a continuous basis.
[00:18:26.560 --> 00:18:33.640]   Nice. It's good to hear. Seems like a tough thing to manage manually. So it's good to hear. There's good automated tools for helping with that.
[00:18:33.640 --> 00:18:41.160]   It's easy to mess it up. That's the problem. I myself like would forget a step or like
[00:18:41.160 --> 00:18:47.400]   Sometimes you have a Python script and you're executed the Python script, but something fails because of dependencies or like
[00:18:48.880 --> 00:18:59.960]   You track to you forget to track something and then it was like, oh, where did I leave my metadata and usually when the metadata ended up in my notebook and that was a bad place for metadata.
[00:18:59.960 --> 00:19:09.560]   We have a question from Himanshu Aurora. How does the state of MLOps and PyTorch compared to that of TensorFlow. I know you mentioned that the tool is
[00:19:09.560 --> 00:19:17.000]   agnostic to back end framework. But have you noticed a difference in these two frameworks and how well they support MLOps
[00:19:17.680 --> 00:19:28.960]   Yeah, so just to be clear, so TensorFlow TFX as a whole is TensorFlow specific there's like very specific components in there. So for example, TensorFlow transform that
[00:19:28.960 --> 00:19:38.680]   Generates a TensorFlow graph which can then be connected with a trained model that only works with TensorFlow, but individual components, let's say TensorFlow
[00:19:39.680 --> 00:19:52.920]   TensorFlow data validation or model analysis model analysis or in that case, not the particular tool, but like, let's say the what if tool, there's ways to use that with other machine learning models.
[00:19:52.920 --> 00:20:05.320]   I'm, I'm not an expert in the PyTorch ecosystem. So everything I say here is with a grain of salt. The, I would say that the project started with a different focus
[00:20:06.200 --> 00:20:16.760]   But I think tools are emerging to make them more framework independent and as we have seen this when the TensorFlow world with a TFDB tool that is independent.
[00:20:16.760 --> 00:20:29.760]   That could easily run with a PyTorch model or with a scikit-learn model. So I would expect that in the future, similar projects will come up or maybe TFX will extend or plugins will come in, where we can run
[00:20:31.000 --> 00:20:36.120]   PyTorch models or we can save PyTorch models through those pipelines and validate them. So,
[00:20:36.120 --> 00:20:38.480]   I think that could work.
[00:20:38.480 --> 00:20:40.880]   Great.
[00:20:40.880 --> 00:20:49.760]   This is a question, an anonymous question, but it matches one I kind of wanted to ask. So you, you mentioned, you know, a lot of people think that they can use
[00:20:50.240 --> 00:21:04.240]   like CI tools from base, you know, other areas of programming and use them. Like what kinds of issues. Have you seen people run into when they try and port that stuff without using a specialized tool like TensorFlow Extended?
[00:21:04.240 --> 00:21:11.800]   I think the biggest, the biggest problem is like in a in a CI, sorry, in a traditional CI/CD tool.
[00:21:11.800 --> 00:21:20.000]   If you want to compare to previous runs, let's say you want to your last line of defense is like you only want to deploy if your model is X percent better than the previous run.
[00:21:20.760 --> 00:21:35.160]   And if you want to do this within, within like with the traditional CI/CD software tool, then you have to run, you basically have to run the metadata yourself and compare against a log file and maybe previous runs or something.
[00:21:35.160 --> 00:21:40.280]   But in that case, you would have to implement those interfaces yourself and
[00:21:41.280 --> 00:21:47.800]   that's what you get free of charge with with TensorFlow Extended. That's the big benefit. The other thing is when we
[00:21:47.800 --> 00:21:55.840]   like run like Jenkins, for example, to validate models. That's absolutely works. I've done this in previous jobs as well.
[00:21:55.840 --> 00:22:08.400]   But in that moment is like sometimes models become bigger and sometimes you need to, like, for example, for the TensorFlow model analysis, you want to get use a larger validation set.
[00:22:09.360 --> 00:22:15.840]   In that moment, you can run those statistics or the aggregation of the statistics through Apache Beam if you use TensorFlow Extended.
[00:22:15.840 --> 00:22:30.560]   If you run this on Jenkins, you're really limited to that box you have the virtual machine and that could be a potential bottleneck. So there's a good, good benefits here for tools like TensorFlow Extended and things like Kubeflow Pipelines to combine them.
[00:22:33.400 --> 00:22:44.560]   Great. So I have a question that I wanted to make sure to ask you. So earlier today, a friend of Weights and Biases, Chip Yen, who was in our podcast, maybe a month ago,
[00:22:44.560 --> 00:22:55.760]   had a tweet about common misperceptions about ML models in production and wanted to pass a couple of those by you and get sort of your thoughts on that. And maybe where you see TensorFlow Extended maybe helping with this.
[00:22:56.840 --> 00:23:04.040]   So one part was misconception number three, if nothing happens, model performance remains the same.
[00:23:04.040 --> 00:23:10.280]   So she says, ML models perform best right after training in prod, they degrade quickly.
[00:23:10.280 --> 00:23:22.080]   And she gives a tip, train models on data generated six months ago and test on current data to see how bad that's going to be. So what do you think of that tip? What do you think the TensorFlow Extended story is about concept drift and model drift?
[00:23:22.480 --> 00:23:31.320]   I think that tweet made my day. I read through the tweet this morning and I was like, wow, this is like anybody who's interested in like machine learning infrastructure,
[00:23:31.320 --> 00:23:33.040]   that should be
[00:23:33.040 --> 00:23:41.960]   a starting point. And if you need arguments for your manager to get resources for your project, just pull up the tweet and show it to them.
[00:23:43.000 --> 00:23:53.640]   In terms of the model staleness and like updating models, definitely like that's something we can split. We can ingest different data sets
[00:23:53.640 --> 00:24:03.040]   from different points in time into those TFX pipelines. And I think that she makes a really wonderful point here in saying
[00:24:04.040 --> 00:24:15.760]   let's not assume once a model is trained, it will be perfect. As soon as it's outdated. And we continue to need to update the model and we need to challenge our assumptions around what data set we use. And
[00:24:15.760 --> 00:24:19.840]   I really want to stress that like just looking at
[00:24:21.320 --> 00:24:37.240]   like the aggregated accuracy across a large training set is potentially not an indication for a good performing model. So I would highly recommend tools like TensorFlow data, model analysis and data validation for like investigating those data sets.
[00:24:37.240 --> 00:24:47.280]   Yeah, absolutely. I came across a long Twitter thread of a bunch of MLOps people dunking on accuracy and just making fun of people who only look at model accuracy.
[00:24:47.600 --> 00:24:52.960]   And as somebody who mostly, you know, studied optimization, I was, you know, I felt like I was a little bit in trouble.
[00:24:52.960 --> 00:24:57.680]   Well, there's accuracy is not
[00:24:57.680 --> 00:25:03.960]   hard to say, like, I think Sarah could could answer the question much better than me, but the
[00:25:03.960 --> 00:25:14.520]   I think if we if we take a look at accuracy and use it as a metric, we should be fair and not take a look at it from a holistic, like across the entire data set and just
[00:25:14.840 --> 00:25:29.920]   accumulate or sum up the entire accuracy, but we should go by maybe feature by feature and then slice a particular particular features we think are very sensitive and then make sure that the accuracy is consistent across that particular feature in those different slices.
[00:25:29.920 --> 00:25:33.160]   And we often like in the case of
[00:25:33.160 --> 00:25:39.720]   my employer, we, we develop models for different countries and we want to make sure that, for example,
[00:25:40.920 --> 00:25:47.640]   the model works in country A as in country B, but I'm pretty sure that one country will
[00:25:47.640 --> 00:25:54.000]   be the majority of the data set and we and when we add new countries, they will be underrepresented. So we really need to make sure that those
[00:25:54.000 --> 00:26:00.040]   the accuracy in that country works well as well, or it has a good performance and it shouldn't be an outlier and
[00:26:00.720 --> 00:26:12.960]   if we just look at the overall accuracy, we wouldn't be able to see that. So that's why I think tools like TensorFlow model analysis is just wonderful and really, really helpful for machine learning engineers.
[00:26:12.960 --> 00:26:24.600]   Yeah, definitely, definitely. It's become more and more clear. Another tweet from the from the list. Misconception number four, you won't need to update your models.
[00:26:25.200 --> 00:26:34.480]   That much and mentions that AWS deploys every 11.7 seconds. So I'm curious, like how fast do you see
[00:26:34.480 --> 00:26:44.240]   like people deploying pipelines with with TensorFlow Extended, you know, how quickly can people get that turnaround versus what they do when they do it by hand. Have you seen this gap?
[00:26:44.240 --> 00:26:53.280]   Well, I don't know if the 11 second number is for a particular model because a pipeline run would take way longer than 11 seconds.
[00:26:53.840 --> 00:27:03.440]   But I think it's for literally the AWS service like a new build goes out every 11 seconds, not, not Amazon's ML models, which I'm sure too proprietary for us to know what's going on.
[00:27:03.440 --> 00:27:15.920]   But I think we should we should as a data scientist and machine learning engineer, regardless of what what the time span is, but we should have the capabilities of pushing new models out and in having ways of
[00:27:15.920 --> 00:27:19.720]   validate the models automatically and consistently.
[00:27:21.040 --> 00:27:31.400]   Currently, I see a lot of data science projects where the models are still being produced in notebooks and then being deployed. Maybe the analysis happens in the notebook, but sometimes we lose the notebook and
[00:27:31.400 --> 00:27:40.000]   with that the data is gone and we lose the records of that. And so what we do with machine learning engineering and those pipelines is that we just
[00:27:40.000 --> 00:27:49.880]   put everything into a consistent workflow and track those information consistently and maybe keep those records in a remote MySQL database so that
[00:27:50.760 --> 00:27:52.600]   that can be backed up in a central place.
[00:27:52.600 --> 00:27:59.040]   And maybe just one last shout out that the thread is extremely interesting and
[00:27:59.040 --> 00:28:04.760]   Chip mentions a couple of numbers which I thought were really interesting. So
[00:28:04.760 --> 00:28:14.400]   One of them, what you just mentioned was like the number of deploys deployments and also like how many models certain companies have running in production. It's like
[00:28:14.400 --> 00:28:17.480]   It's good to see those numbers and
[00:28:18.800 --> 00:28:22.360]   To really think about why we need more infrastructure around machine learning.
[00:28:22.360 --> 00:28:23.680]   Definitely.
[00:28:23.680 --> 00:28:29.960]   I want to, we only have time for one more question. So I'm going to ask this one from conduct Becky.
[00:28:29.960 --> 00:28:42.240]   Production pipelines for edge devices with tools like TF Lite and those for well resource servers have inherent differences. How does the approach here and the tools stack up to the challenges of edge.
[00:28:43.760 --> 00:28:51.280]   Um, so I assume that the goal of the pipeline is to produce like a TF Lite model. That's my assumption to answer that question.
[00:28:51.280 --> 00:29:01.560]   And you're in luck because the TensorFlow team has released an amazing feature recently where you can you can go through the entire training set step.
[00:29:01.560 --> 00:29:10.600]   And you can then rewrite the model for JavaScript or TensorFlow Lite. And so that could be part of a machine learning model machine learning pipeline.
[00:29:11.480 --> 00:29:21.040]   And what we have experimented with at Concur Labs was also branching our pipeline. So we would go through the all the data verification validation.
[00:29:21.040 --> 00:29:27.240]   Feature engineering, but then once we get to the training or maybe we train a model and then we branch and say,
[00:29:27.240 --> 00:29:36.400]   Why don't we, we can produce three models and one model is converted to JavaScript. The other one is converted to something we can push to our mobile devices.
[00:29:36.680 --> 00:29:41.200]   And then we have the last model, which is then being used in TensorFlow serving applications.
[00:29:41.200 --> 00:29:50.920]   And then we can because we run this in those pipelines, we can make sure it runs on the exact same data sets on the exact same transformations and we can even
[00:29:50.920 --> 00:29:58.080]   convert those transformations into the model. So we can attach them to the trained models and export them as one artifact.
[00:29:58.960 --> 00:30:13.560]   It's another amazing byproduct that I find from the TFX pipelines that we can then ship one artifact and make sure that we there's no Python script, you could potentially lose where you have your pre processing steps that happened to me where
[00:30:13.560 --> 00:30:25.320]   I just used the wrong pre processing step for model and the performance was it was working, but the performance was weird. It didn't match our validation and with TFX, you can get around those mistakes.
[00:30:25.320 --> 00:30:26.920]   Great.
[00:30:28.280 --> 00:30:31.520]   Great, thanks for answering all of the folks questions.
[00:30:31.520 --> 00:30:40.080]   Yeah, and thanks for really great to talk. I'm looking forward to to using TensorFlow Extended the next time I need to orchestrate a big old pipeline.
[00:30:40.080 --> 00:30:41.200]   Yeah, yeah.
[00:30:41.240 --> 00:30:42.040]   Thanks for having me.
[00:30:42.040 --> 00:30:42.560]   Yeah.
[00:30:42.560 --> 00:30:43.080]   Yeah.
[00:30:43.080 --> 00:30:43.800]   Thanks for having me.
[00:30:44.800 --> 00:30:45.800]   Thanks for having me.
[00:30:45.800 --> 00:30:46.800]   Thanks for having me.
[00:30:46.800 --> 00:30:56.800]   [BLANK_AUDIO]

