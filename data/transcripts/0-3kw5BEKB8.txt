
[00:00:00.000 --> 00:00:03.640]   The following is a conversation with Whitney Cummings.
[00:00:03.640 --> 00:00:07.240]   She's a stand-up comedian, actor, producer, writer, director,
[00:00:07.240 --> 00:00:11.240]   and recently, finally, the host of her very own podcast
[00:00:11.240 --> 00:00:12.920]   called Good For You.
[00:00:12.920 --> 00:00:15.920]   Her most recent Netflix special called Can I Touch It?
[00:00:15.920 --> 00:00:18.680]   features in part a robot she affectionately
[00:00:18.680 --> 00:00:22.240]   named Bearclaw that is designed to be visually a replica
[00:00:22.240 --> 00:00:23.400]   Whitney.
[00:00:23.400 --> 00:00:26.000]   It's exciting for me to see one of my favorite comedians
[00:00:26.000 --> 00:00:30.720]   explore the social aspects of robotics and AI in our society.
[00:00:30.720 --> 00:00:32.920]   She also has some fascinating ideas
[00:00:32.920 --> 00:00:36.000]   about human behavior, psychology, and urology,
[00:00:36.000 --> 00:00:38.120]   some of which she explores in her book called
[00:00:38.120 --> 00:00:41.240]   I'm Fine and Other Lies.
[00:00:41.240 --> 00:00:43.360]   It was truly a pleasure to meet Whitney
[00:00:43.360 --> 00:00:45.200]   and have this conversation with her,
[00:00:45.200 --> 00:00:47.960]   and even to continue it through text afterwards.
[00:00:47.960 --> 00:00:50.160]   Every once in a while, late at night,
[00:00:50.160 --> 00:00:52.360]   I'll be programming over a cup of coffee
[00:00:52.360 --> 00:00:55.760]   and will get a text from Whitney saying something hilarious.
[00:00:55.760 --> 00:00:58.960]   Or weirder yet, sending a video of Brian Callen
[00:00:58.960 --> 00:01:00.960]   saying something hilarious.
[00:01:00.960 --> 00:01:03.880]   That's when I know the universe has a sense of humor,
[00:01:03.880 --> 00:01:07.400]   and it gifted me with one hell of an amazing journey.
[00:01:07.400 --> 00:01:09.200]   Then I put the phone down and go back
[00:01:09.200 --> 00:01:13.400]   to programming with a stupid, joyful smile on my face.
[00:01:13.400 --> 00:01:16.320]   If you enjoy this conversation, listen to Whitney's podcast,
[00:01:16.320 --> 00:01:19.840]   Good For You, and follow her on Twitter and Instagram.
[00:01:19.840 --> 00:01:22.640]   This is the Artificial Intelligence Podcast.
[00:01:22.640 --> 00:01:24.840]   If you enjoy it, subscribe on YouTube,
[00:01:24.840 --> 00:01:26.800]   give it five stars on Apple Podcasts,
[00:01:26.800 --> 00:01:30.160]   support on Patreon, or simply connect with me on Twitter,
[00:01:30.160 --> 00:01:34.040]   @LexFriedman, spelled F-R-I-D-M-A-N.
[00:01:34.040 --> 00:01:35.840]   This show is presented by Cash App,
[00:01:35.840 --> 00:01:38.280]   the number one finance app in the App Store.
[00:01:38.280 --> 00:01:39.160]   They regularly support
[00:01:39.160 --> 00:01:41.440]   Whitney's Good For You podcast as well.
[00:01:41.440 --> 00:01:43.920]   I personally use Cash App to send money to friends,
[00:01:43.920 --> 00:01:45.680]   but you can also use it to buy, sell,
[00:01:45.680 --> 00:01:47.960]   and deposit Bitcoin in just seconds.
[00:01:47.960 --> 00:01:50.660]   Cash App also has a new investing feature.
[00:01:50.660 --> 00:01:53.480]   You can buy fractions of a stock, say $1 worth,
[00:01:53.480 --> 00:01:55.760]   no matter what the stock price is.
[00:01:55.760 --> 00:01:58.560]   Brokerage services are provided by Cash App Investing,
[00:01:58.560 --> 00:02:02.020]   a subsidiary of Square, and member SIPC.
[00:02:02.020 --> 00:02:04.120]   I'm excited to be working with Cash App
[00:02:04.120 --> 00:02:07.180]   to support one of my favorite organizations called FIRST,
[00:02:07.180 --> 00:02:10.400]   best known for their FIRST Robotics and LEGO competitions.
[00:02:10.400 --> 00:02:13.800]   They educate and inspire hundreds of thousands of students
[00:02:13.800 --> 00:02:16.000]   in over 110 countries,
[00:02:16.000 --> 00:02:18.680]   and have a perfect rating on Charity Navigator,
[00:02:18.680 --> 00:02:19.960]   which means the donated money
[00:02:19.960 --> 00:02:22.380]   is used to maximum effectiveness.
[00:02:22.380 --> 00:02:25.240]   When you get Cash App from the App Store or Google Play
[00:02:25.240 --> 00:02:28.960]   and use code LEXPODCAST, you'll get $10,
[00:02:28.960 --> 00:02:32.100]   and Cash App will also donate $10 to FIRST,
[00:02:32.100 --> 00:02:35.280]   which again is an organization that I've personally seen
[00:02:35.280 --> 00:02:37.400]   inspire girls and boys to dream
[00:02:37.400 --> 00:02:39.180]   of engineering a better world.
[00:02:39.180 --> 00:02:43.080]   This podcast is supported by ZipRecruiter.
[00:02:43.080 --> 00:02:45.240]   Hiring great people is hard,
[00:02:45.240 --> 00:02:47.300]   and to me is the most important element
[00:02:47.300 --> 00:02:50.100]   of a successful mission-driven team.
[00:02:50.100 --> 00:02:52.120]   I've been fortunate to be a part of
[00:02:52.120 --> 00:02:54.720]   and to lead several great engineering teams.
[00:02:54.720 --> 00:02:56.680]   The hiring I've done in the past
[00:02:56.680 --> 00:02:59.420]   was mostly through tools that we built ourselves,
[00:02:59.420 --> 00:03:02.400]   but reinventing the wheel was painful.
[00:03:02.400 --> 00:03:05.220]   ZipRecruiter is a tool that's already available for you.
[00:03:05.220 --> 00:03:09.000]   It seeks to make hiring simple, fast, and smart.
[00:03:09.000 --> 00:03:11.960]   For example, Codable co-founder Gretchen Huebner
[00:03:11.960 --> 00:03:14.400]   used ZipRecruiter to find a new game artist
[00:03:14.400 --> 00:03:16.700]   to join her education tech company.
[00:03:16.700 --> 00:03:18.840]   By using ZipRecruiter screening questions
[00:03:18.840 --> 00:03:20.280]   to filter candidates,
[00:03:20.280 --> 00:03:23.080]   Gretchen found it easier to focus on the best candidates,
[00:03:23.080 --> 00:03:26.440]   and finally hiring the perfect person for the role
[00:03:26.440 --> 00:03:29.440]   in less than two weeks from start to finish.
[00:03:29.440 --> 00:03:32.520]   ZipRecruiter, the smartest way to hire.
[00:03:32.520 --> 00:03:34.160]   See why ZipRecruiter is effective
[00:03:34.160 --> 00:03:36.660]   for businesses of all sizes by signing up,
[00:03:36.660 --> 00:03:41.560]   as I did, for free at ziprecruiter.com/lexpod.
[00:03:41.560 --> 00:03:45.340]   That's ziprecruiter.com/lexpod.
[00:03:46.520 --> 00:03:50.640]   And now, here's my conversation with Whitney Cummings.
[00:03:50.640 --> 00:03:53.760]   I have trouble making eye contact, as you can tell.
[00:03:53.760 --> 00:03:54.600]   - Me too.
[00:03:54.600 --> 00:03:56.960]   Did you know that I had to work on making eye contact
[00:03:56.960 --> 00:03:58.880]   'cause I used to look here?
[00:03:58.880 --> 00:04:00.360]   Do you see what I'm doing? - That helps, yeah.
[00:04:00.360 --> 00:04:01.800]   - Do you want me to do that?
[00:04:01.800 --> 00:04:03.600]   Well, I'll do this way, I'll cheat the camera.
[00:04:03.600 --> 00:04:05.760]   But I used to do this, and finally people,
[00:04:05.760 --> 00:04:07.320]   like I'd be on dates, and guys would be like,
[00:04:07.320 --> 00:04:08.920]   "Are you looking at my hair?"
[00:04:08.920 --> 00:04:10.900]   It would make people really insecure
[00:04:10.900 --> 00:04:13.200]   because I didn't really get a lot of eye contact as a kid.
[00:04:13.200 --> 00:04:14.760]   It's one to three years.
[00:04:14.760 --> 00:04:16.440]   Did you not get a lot of eye contact as a kid?
[00:04:16.440 --> 00:04:17.280]   - I don't know.
[00:04:17.280 --> 00:04:19.560]   I haven't done the soul searching.
[00:04:19.560 --> 00:04:20.400]   - Right.
[00:04:20.400 --> 00:04:24.200]   - But there's definitely some psychological issues.
[00:04:24.200 --> 00:04:25.520]   - Makes you uncomfortable.
[00:04:25.520 --> 00:04:27.880]   - Yeah, for some reason, when I connect eyes,
[00:04:27.880 --> 00:04:31.800]   I start to think, I assume that you're judging me.
[00:04:31.800 --> 00:04:33.320]   - Oh, well, I am.
[00:04:33.320 --> 00:04:34.800]   That's why you assume that.
[00:04:34.800 --> 00:04:36.020]   We all are.
[00:04:36.020 --> 00:04:36.860]   - All right. - This is perfect.
[00:04:36.860 --> 00:04:38.160]   The podcast would be me and you both
[00:04:38.160 --> 00:04:40.200]   staring at the table the whole time.
[00:04:40.200 --> 00:04:42.400]   (both laughing)
[00:04:42.400 --> 00:04:44.200]   - Do you think robots of the future,
[00:04:44.200 --> 00:04:46.000]   ones with human level intelligence,
[00:04:46.000 --> 00:04:49.480]   will be female, male, genderless,
[00:04:49.480 --> 00:04:53.240]   or another gender we have not yet created as a society?
[00:04:53.240 --> 00:04:55.240]   - You're the expert at this.
[00:04:55.240 --> 00:04:57.320]   - Well, I'm gonna ask you-- - You know the answer.
[00:04:57.320 --> 00:04:58.680]   - I'm gonna ask you questions
[00:04:58.680 --> 00:05:01.000]   that maybe nobody knows the answer to.
[00:05:01.000 --> 00:05:01.960]   - Okay.
[00:05:01.960 --> 00:05:04.080]   - And then I just want you to hypothesize
[00:05:04.080 --> 00:05:09.080]   as a imaginative author, director, comedian,
[00:05:09.080 --> 00:05:12.120]   and just intellectual. - Can we just be very clear
[00:05:12.120 --> 00:05:14.200]   that you know a ton about this
[00:05:14.200 --> 00:05:15.800]   and I know nothing about this,
[00:05:15.800 --> 00:05:19.600]   but I have thought a lot about
[00:05:19.600 --> 00:05:22.860]   what I think robots can fix in our society.
[00:05:22.860 --> 00:05:24.400]   And I mean, I'm a comedian.
[00:05:24.400 --> 00:05:27.520]   It's my job to study human nature,
[00:05:27.520 --> 00:05:28.860]   to make jokes about human nature,
[00:05:28.860 --> 00:05:31.080]   and to sometimes play devil's advocate.
[00:05:31.080 --> 00:05:35.160]   And I just see such a tremendous negativity around robots,
[00:05:35.160 --> 00:05:37.000]   or at least the idea of robots,
[00:05:37.000 --> 00:05:40.120]   that it was like, oh, I'm just gonna take the opposite side
[00:05:40.120 --> 00:05:43.400]   for fun, for jokes, and then I was like,
[00:05:43.400 --> 00:05:45.920]   oh no, I really agree in this devil's advocate argument.
[00:05:45.920 --> 00:05:49.400]   So please correct me when I'm wrong about this stuff.
[00:05:49.400 --> 00:05:51.760]   - So first of all, there's no right and wrong
[00:05:51.760 --> 00:05:54.880]   because we're all, I think most of the people
[00:05:54.880 --> 00:05:57.560]   working on robotics are really not actually even thinking
[00:05:57.560 --> 00:06:00.040]   about some of the big picture things
[00:06:00.040 --> 00:06:01.280]   that you've been exploring.
[00:06:01.280 --> 00:06:04.600]   In fact, your robot, what's her name, by the way?
[00:06:04.600 --> 00:06:06.200]   - Bear Claw. - We'll go with Bear Claw.
[00:06:06.200 --> 00:06:08.440]   (laughing)
[00:06:09.840 --> 00:06:11.680]   What's the genesis of that name, by the way?
[00:06:11.680 --> 00:06:15.000]   - Bear Claw was, I, God, I don't even remember the joke
[00:06:15.000 --> 00:06:16.640]   'cause I black out after I shoot specials,
[00:06:16.640 --> 00:06:19.160]   but I was writing something about the pet names
[00:06:19.160 --> 00:06:22.920]   that men call women, like Cupcake, Sweetie, Honey,
[00:06:22.920 --> 00:06:26.500]   you know, like, we're always named after desserts
[00:06:26.500 --> 00:06:29.360]   or something, and I was just writing a joke
[00:06:29.360 --> 00:06:30.960]   about if you wanna call us a dessert,
[00:06:30.960 --> 00:06:33.160]   at least pick a cool dessert, you know,
[00:06:33.160 --> 00:06:35.640]   like Bear Claw, like something cool.
[00:06:35.640 --> 00:06:36.480]   So I ended up calling her Bear Claw.
[00:06:36.480 --> 00:06:38.240]   - And she stuck.
[00:06:38.240 --> 00:06:42.280]   So do you think future robots
[00:06:42.280 --> 00:06:44.440]   of greater and greater intelligence
[00:06:44.440 --> 00:06:46.520]   would like to make them female, male?
[00:06:46.520 --> 00:06:48.560]   Would we like to assign them gender?
[00:06:48.560 --> 00:06:50.800]   Or would we like to move away from gender
[00:06:50.800 --> 00:06:54.000]   and say something more ambiguous?
[00:06:54.000 --> 00:06:56.360]   - I think it depends on their purpose, you know?
[00:06:56.360 --> 00:06:59.920]   I feel like if it's a sex robot,
[00:06:59.920 --> 00:07:01.900]   people prefer certain genders, you know?
[00:07:01.900 --> 00:07:03.960]   And I also, you know, when I went down
[00:07:03.960 --> 00:07:06.300]   and explored the robot factory,
[00:07:06.300 --> 00:07:07.680]   I was asking about the type of people
[00:07:07.680 --> 00:07:11.520]   that bought sex robots, and I was very surprised
[00:07:11.520 --> 00:07:13.400]   at the answer, because of course,
[00:07:13.400 --> 00:07:15.320]   the stereotype was it's gonna be a bunch of perverts.
[00:07:15.320 --> 00:07:18.580]   It ended up being a lot of people that were handicapped,
[00:07:18.580 --> 00:07:20.520]   a lot of people with erectile dysfunction,
[00:07:20.520 --> 00:07:23.880]   and a lot of people that were exploring their sexuality.
[00:07:23.880 --> 00:07:25.920]   A lot of people that thought they were gay,
[00:07:25.920 --> 00:07:28.120]   but weren't sure, but didn't wanna take the risk
[00:07:28.120 --> 00:07:31.660]   of trying on someone that could reject them
[00:07:31.660 --> 00:07:33.880]   and being embarrassed, or they were closeted,
[00:07:33.880 --> 00:07:36.340]   or in a city where maybe that's, you know,
[00:07:36.340 --> 00:07:37.940]   taboo and stigmatized, you know?
[00:07:37.940 --> 00:07:40.560]   So I think that a gendered sex robot,
[00:07:40.560 --> 00:07:42.340]   that would serve an important purpose
[00:07:42.340 --> 00:07:44.160]   for someone trying to explore their sexuality.
[00:07:44.160 --> 00:07:45.000]   Am I into men?
[00:07:45.000 --> 00:07:46.160]   Let me try on this thing first.
[00:07:46.160 --> 00:07:47.000]   Am I into women?
[00:07:47.000 --> 00:07:48.220]   Let me try on this thing first.
[00:07:48.220 --> 00:07:51.200]   So I think gendered robots would be important for that,
[00:07:51.200 --> 00:07:52.460]   but I think genderless robots,
[00:07:52.460 --> 00:07:56.520]   in terms of emotional support robots, babysitters,
[00:07:56.520 --> 00:07:58.720]   I'm fine for a genderless babysitter
[00:07:58.720 --> 00:08:00.120]   with my husband in the house.
[00:08:00.120 --> 00:08:02.080]   You know, there are places that I think
[00:08:02.080 --> 00:08:04.640]   that genderless makes a lot of sense,
[00:08:04.640 --> 00:08:07.540]   but obviously not in the sex area.
[00:08:07.540 --> 00:08:09.920]   - What do you mean with your husband in the house?
[00:08:09.920 --> 00:08:11.820]   What's that have to do with the gender of the robot?
[00:08:11.820 --> 00:08:13.040]   - Right, I mean, I don't have a husband,
[00:08:13.040 --> 00:08:14.340]   but hypothetically speaking,
[00:08:14.340 --> 00:08:15.740]   I think every woman's worst nightmare
[00:08:15.740 --> 00:08:17.340]   is like the hot babysitter.
[00:08:17.340 --> 00:08:18.220]   (laughing)
[00:08:18.220 --> 00:08:19.260]   You know what I mean?
[00:08:19.260 --> 00:08:21.700]   So I think that there is a time and place,
[00:08:21.700 --> 00:08:25.380]   I think, for genderless, you know, teachers, doctors,
[00:08:25.380 --> 00:08:27.260]   all that kind of, it would be very awkward
[00:08:27.260 --> 00:08:29.700]   if the first robotic doctor was a guy,
[00:08:29.700 --> 00:08:32.580]   or the first robotic nurse is a woman.
[00:08:32.580 --> 00:08:36.120]   You know, it's sort of, that stuff is so loaded.
[00:08:36.120 --> 00:08:38.480]   I think that genderless could just take
[00:08:38.480 --> 00:08:43.480]   the unnecessary drama out of it,
[00:08:43.480 --> 00:08:46.040]   and possibility to sexualize them,
[00:08:46.040 --> 00:08:49.560]   or be triggered by any of that stuff.
[00:08:49.560 --> 00:08:52.840]   - So there's two components to this, to Bearclaw.
[00:08:52.840 --> 00:08:55.040]   So one is the voice and the talking, and so on,
[00:08:55.040 --> 00:08:56.400]   and then there's the visual appearance.
[00:08:56.400 --> 00:08:59.620]   So on the topic of gender and genderless,
[00:08:59.620 --> 00:09:03.200]   in your experience, what has been the value
[00:09:03.200 --> 00:09:04.920]   of the physical appearance?
[00:09:04.920 --> 00:09:09.000]   So has it added much to the depth of the interaction?
[00:09:09.000 --> 00:09:11.240]   - I mean, mine's kind of an extenuating circumstance,
[00:09:11.240 --> 00:09:13.560]   'cause she's supposed to look exactly like me.
[00:09:13.560 --> 00:09:16.000]   I mean, I spent six months getting my face molded,
[00:09:16.000 --> 00:09:18.160]   and having, you know, the idea was,
[00:09:18.160 --> 00:09:21.240]   I was exploring the concept of, can robots replace us?
[00:09:21.240 --> 00:09:22.500]   Because that's the big fear,
[00:09:22.500 --> 00:09:24.280]   but also the big dream in a lot of ways,
[00:09:24.280 --> 00:09:26.780]   and I wanted to dig into that area,
[00:09:26.780 --> 00:09:29.720]   because, you know, for a lot of people,
[00:09:29.720 --> 00:09:30.840]   it's like, they're gonna take our jobs,
[00:09:30.840 --> 00:09:33.000]   and they're gonna replace us, legitimate fear,
[00:09:33.000 --> 00:09:34.360]   but then a lot of women I know are like,
[00:09:34.360 --> 00:09:37.000]   I would love for a robot to replace me every now and then,
[00:09:37.000 --> 00:09:38.920]   so it can go to baby showers for me,
[00:09:38.920 --> 00:09:40.200]   and it can pick up my kids at school,
[00:09:40.200 --> 00:09:42.500]   and it can cook dinner, and whatever.
[00:09:42.500 --> 00:09:45.240]   So I just think that was an interesting place to explore,
[00:09:45.240 --> 00:09:47.080]   so her looking like me was a big part of it.
[00:09:47.080 --> 00:09:49.120]   Now her looking like me just adds
[00:09:49.120 --> 00:09:51.400]   an unnecessary level of insecurity,
[00:09:51.400 --> 00:09:53.560]   'cause I got her a year ago,
[00:09:53.560 --> 00:09:54.720]   and she already looks younger than me,
[00:09:54.720 --> 00:09:57.660]   so that's a weird problem,
[00:09:57.660 --> 00:10:00.700]   but I think that her looking human was the idea,
[00:10:00.700 --> 00:10:03.100]   and I think that where we are now,
[00:10:03.100 --> 00:10:04.820]   please correct me if I'm wrong,
[00:10:04.820 --> 00:10:09.820]   a human robot resembling an actual human you know
[00:10:09.820 --> 00:10:13.780]   is going to feel more realistic than some generic face.
[00:10:13.780 --> 00:10:18.140]   - Well, you're saying that robots that have some familiarity
[00:10:18.140 --> 00:10:22.540]   like look similar to somebody that you actually know,
[00:10:22.540 --> 00:10:24.580]   you'll be able to form a deeper connection with?
[00:10:24.580 --> 00:10:26.020]   - That was the question? - I think so on some level.
[00:10:26.020 --> 00:10:28.820]   - That's an open question, I don't, you know,
[00:10:28.820 --> 00:10:30.200]   it's an interesting--
[00:10:30.200 --> 00:10:32.100]   - Or the opposite, 'cause then you know me,
[00:10:32.100 --> 00:10:33.380]   and you're like, well I know this isn't real,
[00:10:33.380 --> 00:10:36.260]   'cause you're right here, so maybe it does the opposite.
[00:10:36.260 --> 00:10:39.260]   - We have a very keen eye for human faces,
[00:10:39.260 --> 00:10:41.860]   and they're able to detect strangeness,
[00:10:41.860 --> 00:10:44.820]   especially when it has to do with people
[00:10:44.820 --> 00:10:46.420]   whose faces we've seen a lot of,
[00:10:46.420 --> 00:10:48.900]   so I tend to be a bigger fan
[00:10:48.900 --> 00:10:52.820]   of moving away completely from faces.
[00:10:52.820 --> 00:10:54.220]   - Of recognizable faces?
[00:10:54.220 --> 00:10:56.020]   No, just human faces at all.
[00:10:56.020 --> 00:10:58.340]   - In general, 'cause I think that's where things get dicey,
[00:10:58.340 --> 00:11:01.340]   and one thing I will say is I think my robot
[00:11:01.340 --> 00:11:03.060]   is more realistic than other robots,
[00:11:03.060 --> 00:11:05.180]   not necessarily because you have seen me,
[00:11:05.180 --> 00:11:07.620]   and then you see her, and you go, oh, they're so similar,
[00:11:07.620 --> 00:11:11.100]   but also because human faces are flawed and asymmetrical,
[00:11:11.100 --> 00:11:13.460]   and sometimes we forget when we're making things
[00:11:13.460 --> 00:11:14.400]   that are supposed to look human,
[00:11:14.400 --> 00:11:16.020]   we make them too symmetrical,
[00:11:16.020 --> 00:11:17.960]   and that's what makes them stop looking human,
[00:11:17.960 --> 00:11:20.540]   so because they molded my asymmetrical face,
[00:11:20.540 --> 00:11:22.860]   she just, even if someone didn't know who I was,
[00:11:22.860 --> 00:11:26.580]   I think she'd look more realistic than most generic ones
[00:11:26.580 --> 00:11:28.900]   that didn't have some kind of flaws.
[00:11:28.900 --> 00:11:29.740]   - Got it, yeah.
[00:11:29.740 --> 00:11:30.740]   - 'Cause they start looking creepy
[00:11:30.740 --> 00:11:33.260]   when they're too symmetrical, 'cause human beings aren't.
[00:11:33.260 --> 00:11:35.740]   - Yeah, the flaws is what it means to be human,
[00:11:35.740 --> 00:11:39.420]   so visually as well, but I'm just a fan of the idea
[00:11:39.420 --> 00:11:43.300]   of letting humans use a little bit more imagination,
[00:11:43.300 --> 00:11:47.580]   so just hearing the voice is enough for us humans
[00:11:47.580 --> 00:11:50.300]   to then start imagining the visual appearance
[00:11:50.300 --> 00:11:52.020]   that goes along with that voice,
[00:11:52.020 --> 00:11:54.500]   and you don't necessarily need to work too hard
[00:11:54.500 --> 00:11:56.900]   on creating the actual visual appearance,
[00:11:56.900 --> 00:11:59.140]   so there's some value to that.
[00:11:59.140 --> 00:12:00.620]   When you step into this territory
[00:12:00.620 --> 00:12:04.300]   of actually building a robot that looks like Bear Claw,
[00:12:04.300 --> 00:12:07.680]   it's such a long road of facial expressions,
[00:12:07.680 --> 00:12:12.280]   of sort of making everything smiling, winking,
[00:12:12.280 --> 00:12:14.900]   rolling of the eyes, all that kind of stuff,
[00:12:14.900 --> 00:12:16.540]   it gets really, really tricky.
[00:12:16.540 --> 00:12:19.140]   - It gets tricky, and I think, again, I'm a comedian,
[00:12:19.140 --> 00:12:21.820]   like I'm obsessed with what makes us human,
[00:12:21.820 --> 00:12:25.460]   and our human nature, and the nasty side of human nature
[00:12:25.460 --> 00:12:28.060]   tends to be where I've ended up exploring
[00:12:28.060 --> 00:12:31.580]   over and over again, and I was just mostly fascinated
[00:12:31.580 --> 00:12:33.340]   by people's reactions, so it's my job
[00:12:33.340 --> 00:12:35.640]   to get the biggest reaction from a group of strangers,
[00:12:35.640 --> 00:12:39.860]   the loudest possible reaction, and I just had this instinct,
[00:12:39.860 --> 00:12:42.180]   just when I started building her, and people going,
[00:12:42.180 --> 00:12:43.020]   (gasps)
[00:12:43.020 --> 00:12:45.620]   and people scream, and I mean, I would bring her out
[00:12:45.620 --> 00:12:49.380]   on stage, and people would scream, and I just,
[00:12:49.380 --> 00:12:51.540]   to me, that was the next level of entertainment,
[00:12:51.540 --> 00:12:53.540]   getting a laugh, I've done that, I know how to do that,
[00:12:53.540 --> 00:12:54.940]   I think comedians were always trying to figure out
[00:12:54.940 --> 00:12:57.280]   what the next level is, and comedy's evolving so much,
[00:12:57.280 --> 00:13:00.540]   and Jordan Peele had just done these genius
[00:13:00.540 --> 00:13:03.060]   comedy horror movies, which feel like the next level
[00:13:03.060 --> 00:13:08.060]   of comedy to me, and this sort of funny horror
[00:13:08.060 --> 00:13:11.660]   of a robot was fascinating to me,
[00:13:11.660 --> 00:13:15.520]   but I think the thing that I got the most obsessed with
[00:13:15.520 --> 00:13:18.220]   was people being freaked out and scared of her,
[00:13:18.220 --> 00:13:21.660]   and I started digging around with pathogen avoidance,
[00:13:21.660 --> 00:13:24.060]   and the idea that we've essentially evolved
[00:13:24.060 --> 00:13:27.140]   to be repelled by anything that looks human,
[00:13:27.140 --> 00:13:30.340]   but is off a little bit, anything that could be sick,
[00:13:30.340 --> 00:13:32.580]   or diseased, or dead, essentially,
[00:13:32.580 --> 00:13:34.740]   is our reptilian brain's way to get us
[00:13:34.740 --> 00:13:38.340]   to not try to have sex with it, basically,
[00:13:38.340 --> 00:13:39.940]   you know, so I got really fascinated
[00:13:39.940 --> 00:13:42.140]   by how freaked out and scared, I mean,
[00:13:42.140 --> 00:13:44.380]   I would see grown men get upset,
[00:13:44.380 --> 00:13:45.220]   they're like, get that thing away from me,
[00:13:45.220 --> 00:13:47.900]   like, oh, like, people get angry,
[00:13:47.900 --> 00:13:50.860]   and it was like, you know what this is, you know,
[00:13:50.860 --> 00:13:53.340]   but the sort of like, you know,
[00:13:53.340 --> 00:13:55.980]   amygdala getting activated by something
[00:13:55.980 --> 00:13:59.340]   that to me is just a fun toy said a lot
[00:13:59.340 --> 00:14:02.100]   about our history as a species,
[00:14:02.100 --> 00:14:04.780]   and what got us into trouble thousands of years ago.
[00:14:04.780 --> 00:14:07.340]   - So it's that, it's the deep down stuff
[00:14:07.340 --> 00:14:10.100]   that's in our genetics, but also is it just,
[00:14:10.100 --> 00:14:13.140]   are people freaked out by the fact that there's a robot?
[00:14:13.140 --> 00:14:14.860]   It's not just the appearance,
[00:14:14.860 --> 00:14:17.860]   but that there's an artificial human.
[00:14:17.860 --> 00:14:21.260]   - Anything people I think, and I'm just also fascinated
[00:14:21.260 --> 00:14:23.060]   by the blind spots humans have,
[00:14:23.060 --> 00:14:24.740]   so the idea that you're afraid of that,
[00:14:24.740 --> 00:14:27.220]   I mean, how many robots have killed people?
[00:14:27.220 --> 00:14:29.780]   How many humans have died at the hands of other humans?
[00:14:29.780 --> 00:14:31.380]   - Yeah, a few more. - Millions?
[00:14:31.380 --> 00:14:32.900]   Hundreds of millions?
[00:14:32.900 --> 00:14:34.580]   Yet we're scared of that?
[00:14:34.580 --> 00:14:36.060]   And we'll go to the grocery store
[00:14:36.060 --> 00:14:38.500]   and be around a bunch of humans who statistically,
[00:14:38.500 --> 00:14:39.500]   the chances are much higher
[00:14:39.500 --> 00:14:40.700]   that you're gonna get killed by humans,
[00:14:40.700 --> 00:14:43.580]   so I'm just fascinated by, without judgment,
[00:14:43.580 --> 00:14:47.820]   how irrational we are as a species.
[00:14:47.820 --> 00:14:49.260]   - The worry is the exponential,
[00:14:49.260 --> 00:14:52.700]   so it's, you can say the same thing about nuclear weapons
[00:14:52.700 --> 00:14:55.740]   before we dropped on Hiroshima and Nagasaki,
[00:14:55.740 --> 00:14:59.420]   so the worry that people have is the exponential growth.
[00:14:59.420 --> 00:15:03.700]   So it's like, oh, it's fun and games right now,
[00:15:03.700 --> 00:15:08.700]   but overnight, especially if a robot
[00:15:08.700 --> 00:15:11.660]   provides value to society, we'll put one in every home,
[00:15:11.660 --> 00:15:12.780]   and then all of a sudden,
[00:15:12.780 --> 00:15:16.180]   lose track of the actual large-scale impact
[00:15:16.180 --> 00:15:18.100]   it has on society, and then all of a sudden,
[00:15:18.100 --> 00:15:20.260]   gain greater and greater control
[00:15:20.260 --> 00:15:23.900]   to where we'll all be, affect our political system
[00:15:23.900 --> 00:15:25.580]   and then affect our decision.
[00:15:25.580 --> 00:15:27.700]   - Didn't robots already ruin our political system?
[00:15:27.700 --> 00:15:28.700]   Didn't that just already happen?
[00:15:28.700 --> 00:15:29.540]   - Which ones?
[00:15:29.540 --> 00:15:32.540]   Oh, Russia hacking. - No offense.
[00:15:32.540 --> 00:15:34.980]   But hasn't that already happened?
[00:15:34.980 --> 00:15:36.460]   I mean, that was like an algorithm
[00:15:36.460 --> 00:15:39.340]   of negative things being clicked on more.
[00:15:39.340 --> 00:15:40.780]   - We'd like to tell stories
[00:15:40.780 --> 00:15:43.660]   and like to demonize certain people.
[00:15:43.660 --> 00:15:46.860]   I think nobody understands our current political system,
[00:15:46.860 --> 00:15:49.700]   our discourse on Twitter, the Twitter mobs.
[00:15:49.700 --> 00:15:52.580]   Nobody has a sense, not Twitter, not Facebook,
[00:15:52.580 --> 00:15:53.420]   the people running it,
[00:15:53.420 --> 00:15:55.420]   nobody understands the impact of these algorithms.
[00:15:55.420 --> 00:15:56.900]   They're trying their best.
[00:15:56.900 --> 00:15:57.980]   Despite what people think,
[00:15:57.980 --> 00:16:00.260]   they're not like a bunch of lefties
[00:16:00.260 --> 00:16:03.260]   trying to make sure that Hillary Clinton gets elected.
[00:16:03.260 --> 00:16:06.860]   It's more that it's an incredibly complex system
[00:16:06.860 --> 00:16:08.900]   that we don't, and that's the worry.
[00:16:08.900 --> 00:16:11.460]   It's so complex and moves so fast
[00:16:11.460 --> 00:16:15.780]   that nobody will be able to stop it once it happens.
[00:16:15.780 --> 00:16:16.940]   - And let me ask a question.
[00:16:16.940 --> 00:16:19.460]   This is a very savage question,
[00:16:19.460 --> 00:16:23.860]   which is, is this just the next stage of evolution?
[00:16:23.860 --> 00:16:25.660]   As humans, some people will die.
[00:16:25.660 --> 00:16:27.460]   Yes, I mean, that's always happened.
[00:16:27.460 --> 00:16:30.340]   This is just taking emotion out of it.
[00:16:30.340 --> 00:16:34.980]   Is this basically the next stage of survival of the fittest?
[00:16:34.980 --> 00:16:37.780]   - Yeah, you have to think of organisms.
[00:16:37.780 --> 00:16:41.380]   You know, what does it mean to be a living organism?
[00:16:41.380 --> 00:16:45.660]   Like, is a smartphone part of your living organism?
[00:16:45.660 --> 00:16:49.740]   - We're in relationships with our phones.
[00:16:49.740 --> 00:16:52.460]   - Yeah, but-- - We have sex through them,
[00:16:52.460 --> 00:16:53.420]   with them, what's the difference
[00:16:53.420 --> 00:16:54.460]   between with them and through them?
[00:16:54.460 --> 00:16:57.140]   - But it also expands your cognitive abilities,
[00:16:57.140 --> 00:16:59.100]   expands your memory, knowledge, and so on,
[00:16:59.100 --> 00:17:00.660]   so you're a much smarter person
[00:17:00.660 --> 00:17:02.660]   because you have a smartphone in your hand.
[00:17:02.660 --> 00:17:04.820]   - But as soon as it's out of my hand,
[00:17:04.820 --> 00:17:06.140]   we've got big problems,
[00:17:06.140 --> 00:17:08.420]   'cause we become sort of so morphed with them.
[00:17:08.420 --> 00:17:09.980]   - Well, there's a symbiotic relationship,
[00:17:09.980 --> 00:17:12.580]   and that's what, so Elon Musk, the neural link,
[00:17:12.580 --> 00:17:16.700]   is working on trying to increase the bandwidth
[00:17:16.700 --> 00:17:19.340]   of communication between computers and your brain,
[00:17:19.340 --> 00:17:22.860]   and so further and further expand our ability
[00:17:22.860 --> 00:17:26.340]   as human beings to sort of leverage machines,
[00:17:26.340 --> 00:17:29.380]   and maybe that's the future, the evolution,
[00:17:29.380 --> 00:17:30.500]   next evolutionary step.
[00:17:30.500 --> 00:17:33.940]   It could be also that, yes, we'll give birth,
[00:17:33.940 --> 00:17:36.580]   just like we give birth to human children right now,
[00:17:36.580 --> 00:17:38.980]   we'll give birth to AI and they'll replace us.
[00:17:38.980 --> 00:17:42.220]   I think it's a really interesting possibility.
[00:17:42.220 --> 00:17:44.140]   - I'm gonna play devil's advocate.
[00:17:44.140 --> 00:17:48.340]   I just think that the fear of robots is wildly classist,
[00:17:48.340 --> 00:17:51.180]   because, I mean, Facebook, like, it's easy for us to say,
[00:17:51.180 --> 00:17:52.020]   they're taking their data.
[00:17:52.020 --> 00:17:55.740]   Okay, a lot of people that get employment off of Facebook,
[00:17:55.740 --> 00:17:58.260]   they are able to get income off of Facebook.
[00:17:58.260 --> 00:17:59.860]   They don't care if you take their phone numbers
[00:17:59.860 --> 00:18:01.980]   and their emails and their data, as long as it's free.
[00:18:01.980 --> 00:18:03.980]   They don't wanna have to pay $5 a month for Facebook.
[00:18:03.980 --> 00:18:05.860]   Facebook is a wildly democratic thing.
[00:18:05.860 --> 00:18:08.300]   Forget about the election and all that kind of stuff.
[00:18:08.300 --> 00:18:12.540]   A lot of technology making people's lives easier,
[00:18:12.540 --> 00:18:17.180]   I find that most elite people are more scared
[00:18:17.180 --> 00:18:21.260]   than lower-income people, and women, for the most part.
[00:18:21.260 --> 00:18:24.020]   So the idea of something that's stronger than us
[00:18:24.020 --> 00:18:25.260]   and that might eventually kill us,
[00:18:25.260 --> 00:18:26.460]   women are used to that.
[00:18:26.460 --> 00:18:29.940]   Like, that's not, I see a lot of really rich men
[00:18:29.940 --> 00:18:31.180]   being like, "The robots are gonna kill us."
[00:18:31.180 --> 00:18:33.700]   We're like, "What's another thing that's gonna kill us?"
[00:18:33.700 --> 00:18:36.180]   I tend to see, like, "Oh, something can walk me
[00:18:36.180 --> 00:18:37.180]   to my car at night.
[00:18:37.180 --> 00:18:38.780]   Something can help me cook dinner."
[00:18:38.780 --> 00:18:42.940]   For people in underprivileged countries
[00:18:42.940 --> 00:18:45.260]   who can't afford eye surgery, like, in a robot,
[00:18:45.260 --> 00:18:48.740]   can we send a robot to underprivileged places
[00:18:48.740 --> 00:18:50.580]   to do surgery where they can't?
[00:18:50.580 --> 00:18:53.460]   I work with this organization called Operation Smile
[00:18:53.460 --> 00:18:55.580]   where they do cleft palate surgeries,
[00:18:55.580 --> 00:18:56.900]   and there's a lot of places that can't do
[00:18:56.900 --> 00:19:00.340]   a very simple surgery because they can't afford doctors
[00:19:00.340 --> 00:19:01.380]   and medical care and such.
[00:19:01.380 --> 00:19:04.740]   So I just see, and this can be completely naive
[00:19:04.740 --> 00:19:05.740]   and should be completely wrong,
[00:19:05.740 --> 00:19:08.620]   but I feel like a lot of people are going like,
[00:19:08.620 --> 00:19:09.860]   "The robots are gonna destroy us."
[00:19:09.860 --> 00:19:11.540]   Humans, we're destroying ourselves.
[00:19:11.540 --> 00:19:12.740]   We're self-destructing.
[00:19:12.740 --> 00:19:14.620]   Robots, to me, are the only hope to clean up
[00:19:14.620 --> 00:19:15.860]   all the messes that we've created.
[00:19:15.860 --> 00:19:18.140]   Even when we go try to clean up pollution in the ocean,
[00:19:18.140 --> 00:19:21.620]   we make it worse because of the oil that the tankers use.
[00:19:21.620 --> 00:19:25.420]   It's like, to me, robots are the only solution.
[00:19:25.420 --> 00:19:27.900]   Firefighters are heroes, but they're limited
[00:19:27.900 --> 00:19:30.180]   in how many times they can run into a fire.
[00:19:30.180 --> 00:19:32.340]   So there's just something interesting to me.
[00:19:32.340 --> 00:19:36.140]   I'm not hearing a lot of lower-income,
[00:19:36.140 --> 00:19:39.940]   more vulnerable populations talking about robots.
[00:19:39.940 --> 00:19:42.020]   - Maybe you can speak to it a little bit more.
[00:19:42.020 --> 00:19:44.100]   There's an idea, I think you've expressed it,
[00:19:44.100 --> 00:19:49.100]   I've heard actually a few female writers and robotists
[00:19:49.100 --> 00:19:54.100]   have talked to express this idea that exactly you just said,
[00:19:54.100 --> 00:19:59.500]   which is, it just seems that being afraid
[00:19:59.500 --> 00:20:03.100]   of existential threats of artificial intelligence
[00:20:03.100 --> 00:20:06.260]   is a male issue.
[00:20:06.260 --> 00:20:07.100]   - Yeah.
[00:20:07.100 --> 00:20:09.580]   - And I wonder what that is.
[00:20:09.580 --> 00:20:14.060]   Because men in certain positions, like you said,
[00:20:14.060 --> 00:20:15.620]   it's also a classist issue.
[00:20:15.620 --> 00:20:17.420]   They haven't been humbled by life,
[00:20:17.420 --> 00:20:20.660]   and so you always look for the biggest problems
[00:20:20.660 --> 00:20:22.380]   to take on around you.
[00:20:22.380 --> 00:20:24.220]   - It's a champagne problem to be afraid of robots.
[00:20:24.220 --> 00:20:26.460]   Most people don't have health insurance,
[00:20:26.460 --> 00:20:28.220]   they're afraid they're not gonna be able to feed their kids,
[00:20:28.220 --> 00:20:30.020]   they can't afford a tutor for their kids.
[00:20:30.020 --> 00:20:32.380]   I mean, I just think of the way I grew up,
[00:20:32.380 --> 00:20:36.180]   and I had a mother who worked two jobs, had kids.
[00:20:36.180 --> 00:20:37.820]   We couldn't afford an SAT tutor.
[00:20:37.820 --> 00:20:40.060]   The idea of a robot coming in,
[00:20:40.060 --> 00:20:41.060]   being able to tutor your kids,
[00:20:41.060 --> 00:20:43.540]   being able to provide childcare for your kids,
[00:20:43.540 --> 00:20:45.540]   being able to come in with cameras for eyes
[00:20:45.540 --> 00:20:48.340]   and make sure surveillance.
[00:20:48.340 --> 00:20:49.820]   I'm very pro-surveillance,
[00:20:49.820 --> 00:20:52.300]   because I've had security problems,
[00:20:52.300 --> 00:20:55.780]   and I've been, we're generally in a little more danger
[00:20:55.780 --> 00:20:56.620]   than you guys are.
[00:20:56.620 --> 00:20:58.620]   So I think that robots are a little less scary to us,
[00:20:58.620 --> 00:21:01.260]   'cause we can see them maybe as free assistance,
[00:21:01.260 --> 00:21:03.420]   help, and protection.
[00:21:03.420 --> 00:21:06.860]   And then there's sort of another element for me personally,
[00:21:06.860 --> 00:21:08.820]   which is maybe more of a female problem.
[00:21:08.820 --> 00:21:11.660]   I don't know, I'm just gonna make a generalization.
[00:21:11.660 --> 00:21:13.060]   Happy to be wrong.
[00:21:13.100 --> 00:21:18.100]   But the emotional sort of component of robots
[00:21:18.100 --> 00:21:21.220]   and what they can provide in terms of,
[00:21:21.220 --> 00:21:23.740]   I think there's a lot of people
[00:21:23.740 --> 00:21:25.700]   that don't have microphones
[00:21:25.700 --> 00:21:28.620]   that I just recently kind of stumbled upon
[00:21:28.620 --> 00:21:30.980]   in doing all my research on the sex robots
[00:21:30.980 --> 00:21:31.980]   for my standup special,
[00:21:31.980 --> 00:21:34.900]   which is there's a lot of very shy people
[00:21:34.900 --> 00:21:35.900]   that aren't good at dating.
[00:21:35.900 --> 00:21:37.940]   There's a lot of people who are scared of human beings,
[00:21:37.940 --> 00:21:40.580]   who have personality disorders,
[00:21:40.580 --> 00:21:42.140]   or grew up in alcoholic homes,
[00:21:42.140 --> 00:21:44.380]   or struggle with addiction, or whatever it is
[00:21:44.380 --> 00:21:47.020]   where a robot can solve an emotional problem.
[00:21:47.020 --> 00:21:49.900]   And so we're largely having this conversation
[00:21:49.900 --> 00:21:53.700]   about rich guys that are emotionally healthy
[00:21:53.700 --> 00:21:55.660]   and how scared of robots they are.
[00:21:55.660 --> 00:21:58.620]   We're forgetting about a huge part of the population
[00:21:58.620 --> 00:22:01.660]   who maybe isn't as charming and effervescent
[00:22:01.660 --> 00:22:05.500]   and solvent as people like you and Elon Musk,
[00:22:05.500 --> 00:22:09.300]   who these robots could solve very real problems
[00:22:09.300 --> 00:22:11.300]   in their life, emotional or financial.
[00:22:11.300 --> 00:22:13.500]   - Well, that's in general a really interesting idea
[00:22:13.500 --> 00:22:16.260]   that most people in the world don't have a voice.
[00:22:16.260 --> 00:22:18.620]   You've talked about it,
[00:22:18.620 --> 00:22:19.980]   even the people on Twitter
[00:22:19.980 --> 00:22:22.820]   who are driving the conversation.
[00:22:22.820 --> 00:22:25.420]   You said comments, people who leave comments
[00:22:25.420 --> 00:22:28.260]   represent a very tiny percent of the population.
[00:22:28.260 --> 00:22:29.580]   And they're the ones,
[00:22:29.580 --> 00:22:33.340]   we tend to think they speak for the population,
[00:22:33.340 --> 00:22:37.300]   but it's very possible on many topics they don't at all.
[00:22:37.300 --> 00:22:40.660]   And look, and I'm sure there's gotta be some kind of
[00:22:40.660 --> 00:22:43.980]   legal sort of structure in place
[00:22:43.980 --> 00:22:45.300]   for when the robots happen.
[00:22:45.300 --> 00:22:46.660]   You know way more about this than I do,
[00:22:46.660 --> 00:22:49.780]   but for me to just go, the robots are bad,
[00:22:49.780 --> 00:22:51.260]   that's a wild generalization
[00:22:51.260 --> 00:22:54.460]   that I feel like is really inhumane in some way.
[00:22:54.460 --> 00:22:56.700]   Just after the research I've done,
[00:22:56.700 --> 00:23:00.100]   you're gonna tell me that a man whose wife died suddenly
[00:23:00.100 --> 00:23:02.980]   and he feels guilty moving on with a human woman
[00:23:02.980 --> 00:23:04.340]   or can't get over the grief,
[00:23:04.340 --> 00:23:06.980]   he can't have a sex robot in his own house?
[00:23:06.980 --> 00:23:07.860]   Why not?
[00:23:07.860 --> 00:23:08.820]   Who cares?
[00:23:08.820 --> 00:23:09.980]   Why do you care?
[00:23:09.980 --> 00:23:12.700]   - Well, there's an interesting aspect of human nature.
[00:23:12.700 --> 00:23:16.820]   So, you know, we tend to as a civilization
[00:23:16.820 --> 00:23:19.940]   to create a group that's the other in all kinds of ways.
[00:23:19.940 --> 00:23:20.780]   - Right.
[00:23:20.780 --> 00:23:23.500]   - And so you work with animals too.
[00:23:23.500 --> 00:23:26.540]   You're especially sensitive to the suffering of animals.
[00:23:26.540 --> 00:23:27.660]   Let me kind of ask,
[00:23:27.660 --> 00:23:28.540]   what's your,
[00:23:28.540 --> 00:23:34.260]   do you think we'll abuse robots in the future?
[00:23:34.260 --> 00:23:36.620]   Do you think some of the darker aspects of human nature
[00:23:36.620 --> 00:23:37.980]   will come out?
[00:23:37.980 --> 00:23:39.220]   - I think some people will,
[00:23:39.220 --> 00:23:41.820]   but if we design them properly,
[00:23:41.820 --> 00:23:43.020]   the people that do it,
[00:23:43.020 --> 00:23:46.060]   we can put it on a record and they can,
[00:23:46.060 --> 00:23:46.940]   we can put them in jail.
[00:23:46.940 --> 00:23:48.900]   We can find sociopaths more easily.
[00:23:48.900 --> 00:23:49.740]   You know, like--
[00:23:49.740 --> 00:23:53.220]   - But why is that a sociopathic thing to harm a robot?
[00:23:53.220 --> 00:23:55.260]   - I think, look, I don't know enough
[00:23:55.260 --> 00:23:57.940]   about the consciousness and stuff as you do.
[00:23:57.940 --> 00:23:59.900]   I guess it would have to be when they're conscious,
[00:23:59.900 --> 00:24:02.900]   but it is, you know, the part of the brain
[00:24:02.900 --> 00:24:04.460]   that is responsible for compassion,
[00:24:04.460 --> 00:24:05.300]   the frontal lobe or whatever,
[00:24:05.300 --> 00:24:08.220]   like people that abuse animals also abuse humans
[00:24:08.220 --> 00:24:09.460]   and commit other kinds of crimes.
[00:24:09.460 --> 00:24:11.140]   Like that's, it's all the same part of the brain.
[00:24:11.140 --> 00:24:14.060]   No one abuses animals and then is like awesome
[00:24:14.060 --> 00:24:17.420]   to women and children and awesome to underprivileged,
[00:24:17.420 --> 00:24:18.660]   you know, minorities.
[00:24:18.660 --> 00:24:20.540]   Like it's all, so, you know,
[00:24:20.540 --> 00:24:23.060]   we've been working really hard to put a database together
[00:24:23.060 --> 00:24:24.820]   of all the people that have abused animals.
[00:24:24.820 --> 00:24:25.980]   So when they commit another crime,
[00:24:25.980 --> 00:24:29.380]   you go, okay, this is, you know, it's all the same stuff.
[00:24:29.380 --> 00:24:32.420]   And I think people probably think I'm nuts
[00:24:32.420 --> 00:24:34.820]   for the, a lot of the animal work I do,
[00:24:34.820 --> 00:24:37.100]   but because when animal abuse is present,
[00:24:37.100 --> 00:24:38.900]   another crime is always present,
[00:24:38.900 --> 00:24:40.900]   but the animal abuse is the most socially acceptable.
[00:24:40.900 --> 00:24:43.940]   You can kick a dog and there's nothing people can do,
[00:24:43.940 --> 00:24:46.620]   but then what they're doing behind closed doors,
[00:24:46.620 --> 00:24:47.440]   you can't see.
[00:24:47.440 --> 00:24:48.900]   So there's always something else going on,
[00:24:48.900 --> 00:24:50.720]   which is why I never feel compunction about it.
[00:24:50.720 --> 00:24:52.420]   But I do think we'll start seeing the same thing
[00:24:52.420 --> 00:24:53.380]   with robots.
[00:24:53.380 --> 00:24:55.540]   The person that kicks the,
[00:24:55.540 --> 00:24:59.760]   I felt compassion when the kicking the dog robot
[00:24:59.760 --> 00:25:00.860]   really pissed me off.
[00:25:00.860 --> 00:25:04.140]   I know that they're just trying to get the stability right
[00:25:04.140 --> 00:25:05.220]   and all that,
[00:25:05.220 --> 00:25:07.380]   but I do think there will come a time
[00:25:07.380 --> 00:25:10.740]   where that will be a great way to be able to figure out
[00:25:10.740 --> 00:25:15.500]   if somebody has like, you know, anti-social behaviors.
[00:25:15.500 --> 00:25:18.100]   - You kind of mentioned surveillance.
[00:25:18.100 --> 00:25:20.020]   It's also a really interesting idea of yours
[00:25:20.020 --> 00:25:21.540]   that you just said, you know,
[00:25:21.540 --> 00:25:23.420]   a lot of people seem to be really uncomfortable
[00:25:23.420 --> 00:25:24.300]   with surveillance.
[00:25:24.300 --> 00:25:25.180]   - Yeah.
[00:25:25.180 --> 00:25:28.540]   - And you just said that, you know what, for me,
[00:25:28.540 --> 00:25:31.220]   you know, there's positives for surveillance.
[00:25:31.220 --> 00:25:32.200]   - I think people behave better
[00:25:32.200 --> 00:25:33.300]   when they know they're being watched.
[00:25:33.300 --> 00:25:36.020]   And I know this is a very unpopular opinion.
[00:25:36.020 --> 00:25:38.140]   I'm talking about it on stage right now.
[00:25:38.140 --> 00:25:40.380]   We behave better when we know we're being watched.
[00:25:40.380 --> 00:25:41.980]   You and I had a very different conversation
[00:25:41.980 --> 00:25:43.220]   before we were recording.
[00:25:43.220 --> 00:25:44.580]   (laughing)
[00:25:44.580 --> 00:25:45.460]   We behave different.
[00:25:45.460 --> 00:25:47.540]   You sit up and you are in your best behavior
[00:25:47.540 --> 00:25:49.380]   and I'm trying to sound eloquent
[00:25:49.380 --> 00:25:51.180]   and I'm trying to not hurt anyone's feelings.
[00:25:51.180 --> 00:25:52.900]   And I mean, I have a camera right there.
[00:25:52.900 --> 00:25:54.680]   I'm behaving totally different
[00:25:54.680 --> 00:25:56.980]   than when we first started talking, you know?
[00:25:56.980 --> 00:25:59.420]   When you know there's a camera, you behave differently.
[00:25:59.420 --> 00:26:02.740]   I mean, there's cameras all over LA at stoplights
[00:26:02.740 --> 00:26:04.060]   so that people don't run stoplights,
[00:26:04.060 --> 00:26:05.860]   but there's not even film in it.
[00:26:05.860 --> 00:26:08.020]   They don't even use them anymore, but it works.
[00:26:08.020 --> 00:26:08.860]   - It works.
[00:26:08.860 --> 00:26:09.700]   - Right?
[00:26:09.700 --> 00:26:10.980]   And I'm, you know, working on this thing
[00:26:10.980 --> 00:26:12.000]   in standabout surveillance.
[00:26:12.000 --> 00:26:14.260]   It's like, that's why we invented Santa Claus.
[00:26:14.260 --> 00:26:17.820]   You know, Santa Claus is the first surveillance, basically.
[00:26:17.820 --> 00:26:20.420]   All we had to say to kids is he's making a list
[00:26:20.420 --> 00:26:22.940]   and he's watching you and they behave better.
[00:26:22.940 --> 00:26:23.780]   - That's brilliant.
[00:26:23.780 --> 00:26:26.140]   - You know, so I do think that there are benefits
[00:26:26.140 --> 00:26:27.420]   to surveillance.
[00:26:27.420 --> 00:26:30.940]   You know, I think we all do sketchy things in private
[00:26:30.940 --> 00:26:34.500]   and we all have watched weird porn or Googled weird things
[00:26:34.500 --> 00:26:37.060]   and we don't want people to know about it,
[00:26:37.060 --> 00:26:37.940]   our secret lives.
[00:26:37.940 --> 00:26:40.220]   So I do think that obviously,
[00:26:40.220 --> 00:26:42.860]   we should be able to have a modicum of privacy,
[00:26:42.860 --> 00:26:46.660]   but I tend to think that people that are the most negative
[00:26:46.660 --> 00:26:48.300]   about surveillance have the most secrets.
[00:26:48.300 --> 00:26:49.140]   - The most hype.
[00:26:49.140 --> 00:26:50.540]   (laughing)
[00:26:50.540 --> 00:26:54.580]   Well, you should, you're saying you're doing bits on it now?
[00:26:54.580 --> 00:26:57.020]   - Well, I'm just talking in general about, you know,
[00:26:57.020 --> 00:26:59.340]   privacy and surveillance and how paranoid
[00:26:59.340 --> 00:27:02.500]   we're kind of becoming and how, you know,
[00:27:02.500 --> 00:27:04.940]   I mean, it's just wild to me that people are like,
[00:27:04.940 --> 00:27:05.940]   our emails are gonna leak
[00:27:05.940 --> 00:27:07.220]   and they're taking our phone numbers.
[00:27:07.220 --> 00:27:11.460]   Like there used to be a book full of phone numbers
[00:27:11.460 --> 00:27:15.580]   and addresses that were, they just throw it at your door.
[00:27:15.580 --> 00:27:18.420]   And we all had a book of everyone's numbers, you know,
[00:27:18.420 --> 00:27:20.380]   this is a very new thing.
[00:27:20.380 --> 00:27:22.380]   And, you know, I know our amygdala is designed
[00:27:22.380 --> 00:27:25.380]   to compound sort of threats and, you know,
[00:27:25.380 --> 00:27:29.300]   there's stories about, and I think we all just glom on
[00:27:29.300 --> 00:27:31.420]   in a very, you know, tribal way of,
[00:27:31.420 --> 00:27:32.420]   yeah, they're taking our data.
[00:27:32.420 --> 00:27:33.740]   Like, we don't even know what that means,
[00:27:33.740 --> 00:27:37.100]   but we're like, well, yeah, they, they, you know?
[00:27:37.100 --> 00:27:39.740]   So I just think that sometimes it's like, okay, well,
[00:27:39.740 --> 00:27:41.340]   so what, they're gonna sell your data?
[00:27:41.340 --> 00:27:42.180]   Who cares?
[00:27:42.180 --> 00:27:43.220]   Why do you care?
[00:27:43.220 --> 00:27:46.240]   - First of all, that bit will kill in China.
[00:27:46.240 --> 00:27:51.100]   So, and I say this sort of only a little bit joking
[00:27:51.100 --> 00:27:55.220]   because a lot of people in China, including the citizens,
[00:27:55.220 --> 00:27:58.700]   despite what people in the West think of as abuse,
[00:27:59.660 --> 00:28:02.580]   are actually in support of the idea of surveillance.
[00:28:02.580 --> 00:28:06.500]   Sort of, they're not in support of the abuse of surveillance
[00:28:06.500 --> 00:28:09.420]   but they're, they like, I mean, the idea of surveillance
[00:28:09.420 --> 00:28:13.540]   is kind of like the idea of government.
[00:28:13.540 --> 00:28:15.940]   Like you said, we behave differently.
[00:28:15.940 --> 00:28:18.520]   And in a way, it's almost like why we like sports.
[00:28:18.520 --> 00:28:22.380]   There's rules and within the constraints of the rules,
[00:28:22.380 --> 00:28:25.040]   this is a more stable society.
[00:28:25.040 --> 00:28:28.140]   And they make good arguments about success,
[00:28:28.140 --> 00:28:30.460]   being able to build successful companies,
[00:28:30.460 --> 00:28:32.780]   being able to build successful social lives
[00:28:32.780 --> 00:28:34.540]   around a fabric that's more stable.
[00:28:34.540 --> 00:28:37.060]   When you have a surveillance, it keeps the criminals away,
[00:28:37.060 --> 00:28:41.860]   keeps abusive animals, whatever the values of the society,
[00:28:41.860 --> 00:28:44.780]   with surveillance, you can enforce those values better.
[00:28:44.780 --> 00:28:45.900]   - And here's what I will say.
[00:28:45.900 --> 00:28:47.300]   There's a lot of unethical things
[00:28:47.300 --> 00:28:48.580]   happening with surveillance.
[00:28:48.580 --> 00:28:52.100]   Like I feel the need to really make that very clear.
[00:28:52.100 --> 00:28:54.100]   I mean, the fact that Google is like collecting
[00:28:54.100 --> 00:28:55.980]   if people's hands start moving on the mouse
[00:28:55.980 --> 00:28:58.460]   to find out if they're getting Parkinson's
[00:28:58.460 --> 00:29:00.080]   and then their insurance goes up,
[00:29:00.080 --> 00:29:02.180]   like that is completely unethical and wrong.
[00:29:02.180 --> 00:29:03.340]   And I think stuff like that,
[00:29:03.340 --> 00:29:05.860]   we have to really be careful around.
[00:29:05.860 --> 00:29:08.660]   So the idea of using our data to raise our insurance rates
[00:29:08.660 --> 00:29:10.820]   or, you know, I heard that they're looking,
[00:29:10.820 --> 00:29:13.300]   they can sort of predict if you're gonna have depression
[00:29:13.300 --> 00:29:16.100]   based on your selfies by detecting micro muscles
[00:29:16.100 --> 00:29:18.260]   in your face, you know, all that kind of stuff.
[00:29:18.260 --> 00:29:20.020]   That is a nightmare, not okay.
[00:29:20.020 --> 00:29:22.380]   But I think, you know, we have to delineate
[00:29:22.380 --> 00:29:25.180]   what's a real threat and what's getting spam
[00:29:25.180 --> 00:29:27.420]   in your email box, that's not what to spend
[00:29:27.420 --> 00:29:28.620]   your time and energy on.
[00:29:28.620 --> 00:29:31.100]   Focus on the fact that every time you buy cigarettes,
[00:29:31.100 --> 00:29:35.260]   your insurance is going up without you knowing about it.
[00:29:35.260 --> 00:29:36.980]   - On the topic of animals too,
[00:29:36.980 --> 00:29:38.380]   can we just linger on it a little bit?
[00:29:38.380 --> 00:29:40.340]   Like, what do you think,
[00:29:40.340 --> 00:29:43.400]   what does it say about our society,
[00:29:43.400 --> 00:29:45.700]   of the society wide abuse of animals
[00:29:45.700 --> 00:29:47.220]   that we see in general?
[00:29:47.220 --> 00:29:49.420]   Sort of factory farming, just in general,
[00:29:49.420 --> 00:29:52.060]   just the way we treat animals of different categories.
[00:29:53.780 --> 00:29:56.980]   Like, what do you think of that?
[00:29:56.980 --> 00:29:59.820]   What does a better world look like?
[00:29:59.820 --> 00:30:03.660]   What should people think about it in general?
[00:30:03.660 --> 00:30:06.460]   - I think the most interesting thing
[00:30:06.460 --> 00:30:07.860]   I can probably say around this,
[00:30:07.860 --> 00:30:09.500]   that's the least emotional,
[00:30:09.500 --> 00:30:11.860]   'cause I'm actually a very non-emotional animal person
[00:30:11.860 --> 00:30:14.060]   because it's, I think everyone's an animal person.
[00:30:14.060 --> 00:30:15.880]   It's just a matter of if it's yours
[00:30:15.880 --> 00:30:18.480]   or if you've, you know, been conditioned to go numb.
[00:30:18.480 --> 00:30:20.820]   You know, I think it's really a testament
[00:30:20.820 --> 00:30:24.580]   to what as a species we are able to be in denial about,
[00:30:24.580 --> 00:30:26.300]   mass denial and mass delusion,
[00:30:26.300 --> 00:30:30.620]   and how we're able to dehumanize and debase groups,
[00:30:30.620 --> 00:30:33.420]   you know, World War II,
[00:30:33.420 --> 00:30:36.780]   in a way in order to conform
[00:30:36.780 --> 00:30:38.860]   and find protection in the conforming.
[00:30:38.860 --> 00:30:43.860]   So we are also a species who used to go to coliseums
[00:30:43.860 --> 00:30:47.540]   and watch elephants and tigers fight to the death.
[00:30:47.540 --> 00:30:50.300]   We used to watch human beings be pulled apart
[00:30:50.300 --> 00:30:53.060]   in the, that wasn't that long ago.
[00:30:53.060 --> 00:30:56.860]   We're also a species who had slaves,
[00:30:56.860 --> 00:30:59.020]   and it was socially acceptable by a lot of people.
[00:30:59.020 --> 00:31:00.160]   People didn't see anything wrong with it.
[00:31:00.160 --> 00:31:02.660]   So we're a species that is able to go numb
[00:31:02.660 --> 00:31:05.940]   and that is able to dehumanize very quickly
[00:31:05.940 --> 00:31:08.120]   and make it the norm.
[00:31:08.120 --> 00:31:10.420]   Child labor wasn't that long ago.
[00:31:10.420 --> 00:31:12.620]   Like the idea that now we look back and go,
[00:31:12.620 --> 00:31:15.900]   "Oh yeah, kids were losing fingers in factories,
[00:31:15.900 --> 00:31:17.180]   "making shoes."
[00:31:17.180 --> 00:31:20.160]   Like someone had to come in and make that, you know,
[00:31:20.160 --> 00:31:23.180]   so I think it just says a lot about the fact that,
[00:31:23.180 --> 00:31:25.300]   you know, we are animals and we are self-serving
[00:31:25.300 --> 00:31:27.260]   and one of the most successful,
[00:31:27.260 --> 00:31:29.180]   the most successful species
[00:31:29.180 --> 00:31:33.140]   because we are able to debase and degrade
[00:31:33.140 --> 00:31:36.840]   and essentially exploit anything that benefits us.
[00:31:36.840 --> 00:31:39.980]   I think the pendulum's gonna swing as being lately.
[00:31:39.980 --> 00:31:40.820]   - Which way?
[00:31:40.820 --> 00:31:42.580]   - Like I think we're Rome now, kind of.
[00:31:42.580 --> 00:31:44.980]   Like I think we're on the verge of collapse
[00:31:44.980 --> 00:31:47.240]   because we are dopamine receptors.
[00:31:47.240 --> 00:31:49.560]   Like we are just, I think we're all kind of addicts
[00:31:49.560 --> 00:31:50.520]   when it comes to this stuff.
[00:31:50.520 --> 00:31:53.380]   Like we don't know when to stop.
[00:31:53.380 --> 00:31:54.480]   It's always the buffet.
[00:31:54.480 --> 00:31:56.600]   Like we're, the thing that used to keep us alive,
[00:31:56.600 --> 00:31:58.380]   which is killing animals and eating them,
[00:31:58.380 --> 00:31:59.740]   now killing animals and eating them
[00:31:59.740 --> 00:32:01.220]   is what's killing us in a way.
[00:32:01.220 --> 00:32:04.220]   So it's like we just can't, we don't know when to call it
[00:32:04.220 --> 00:32:06.560]   and we don't, moderation is not really something
[00:32:06.560 --> 00:32:10.040]   that humans have evolved to have yet.
[00:32:10.040 --> 00:32:13.620]   So I think it's really just a flaw in our wiring.
[00:32:13.620 --> 00:32:15.240]   - Do you think we'll look back at this time
[00:32:15.240 --> 00:32:19.380]   as our society's being deeply unethical?
[00:32:19.380 --> 00:32:20.520]   - Yeah, yeah.
[00:32:20.520 --> 00:32:22.240]   I think we'll be embarrassed.
[00:32:22.240 --> 00:32:24.860]   - Which are the worst parts right now going on?
[00:32:24.860 --> 00:32:26.120]   Is it-- - In terms of animal?
[00:32:26.120 --> 00:32:27.780]   Well, I think-- - No, in terms of anything.
[00:32:27.780 --> 00:32:29.800]   What's the unethical thing?
[00:32:29.800 --> 00:32:32.020]   It's very hard to take a step out of it,
[00:32:32.020 --> 00:32:36.180]   but you just said we used to watch, you know,
[00:32:36.180 --> 00:32:40.400]   there's been a lot of cruelty throughout history.
[00:32:40.400 --> 00:32:42.080]   What's the cruelty going on now?
[00:32:42.080 --> 00:32:44.180]   - I think it's gonna be pigs.
[00:32:44.180 --> 00:32:45.480]   I think it's gonna be, I mean,
[00:32:45.480 --> 00:32:48.660]   pigs are one of the most emotionally intelligent animals.
[00:32:48.660 --> 00:32:51.640]   And they have the intelligence of like a three-year-old.
[00:32:51.640 --> 00:32:54.280]   And I think we'll look back and be really,
[00:32:54.280 --> 00:32:55.120]   they use tools.
[00:32:55.120 --> 00:32:58.400]   I mean, I think we have this narrative
[00:32:58.400 --> 00:32:59.720]   that they're pigs and they're pigs
[00:32:59.720 --> 00:33:01.840]   and they're disgusting and they're dirty
[00:33:01.840 --> 00:33:02.840]   and their bacon is so good.
[00:33:02.840 --> 00:33:04.200]   I think that we'll look back one day
[00:33:04.200 --> 00:33:06.620]   and be really embarrassed about that.
[00:33:06.620 --> 00:33:10.300]   - Is this for just, what's it called, the factory farming?
[00:33:10.300 --> 00:33:12.520]   So basically mass-- - 'Cause we don't see it.
[00:33:12.520 --> 00:33:14.520]   If you saw, I mean, we do have,
[00:33:14.520 --> 00:33:17.560]   I mean, this is probably an evolutionary advantage.
[00:33:17.560 --> 00:33:20.360]   We do have the ability to completely
[00:33:20.360 --> 00:33:21.480]   pretend something's not,
[00:33:21.480 --> 00:33:24.000]   something that is so horrific that it overwhelms us.
[00:33:24.000 --> 00:33:27.520]   And we're able to essentially deny that it's happening.
[00:33:27.520 --> 00:33:28.640]   I think if people were to see
[00:33:28.640 --> 00:33:30.440]   what goes on in factory farming,
[00:33:30.440 --> 00:33:35.320]   and also we're really to take in how bad it is for us,
[00:33:35.320 --> 00:33:37.120]   you know, we're hurting ourselves first and foremost
[00:33:37.120 --> 00:33:38.400]   with what we eat.
[00:33:38.400 --> 00:33:41.240]   But that's also a very elitist argument, you know?
[00:33:41.240 --> 00:33:44.580]   It's a luxury to be able to complain about meat.
[00:33:44.580 --> 00:33:46.640]   It's a luxury to be able to not eat meat.
[00:33:46.640 --> 00:33:49.960]   You know, there's very few people because of, you know,
[00:33:49.960 --> 00:33:53.320]   how the corporations have set up meat being cheap.
[00:33:53.320 --> 00:33:55.280]   You know, it's $2 to buy a Big Mac,
[00:33:55.280 --> 00:33:57.620]   it's $10 to buy a healthy meal.
[00:33:57.620 --> 00:34:00.000]   You know, that's, I think a lot of people
[00:34:00.000 --> 00:34:02.260]   don't have the luxury to even think that way.
[00:34:02.260 --> 00:34:04.200]   But I do think that animals in captivity,
[00:34:04.200 --> 00:34:05.040]   I think we're gonna look back
[00:34:05.040 --> 00:34:06.480]   and be pretty grossed out about.
[00:34:06.480 --> 00:34:08.760]   Mammals in captivity, whales, dolphins.
[00:34:08.760 --> 00:34:10.160]   I mean, that's already starting to dismantle.
[00:34:10.160 --> 00:34:13.980]   Circuses, we're gonna be pretty embarrassed about.
[00:34:13.980 --> 00:34:15.720]   But I think it's really more a testament
[00:34:15.720 --> 00:34:20.720]   to, you know, there's just such a ability to go like,
[00:34:20.720 --> 00:34:25.520]   that thing is different than me and we're better.
[00:34:25.520 --> 00:34:26.360]   It's the ego.
[00:34:26.360 --> 00:34:27.560]   I mean, it's just, we have the species
[00:34:27.560 --> 00:34:29.160]   with the biggest ego, ultimately.
[00:34:29.160 --> 00:34:31.840]   - Well, that's what I think, that's my hope for robots
[00:34:31.840 --> 00:34:34.200]   is they'll, you mentioned consciousness before.
[00:34:34.200 --> 00:34:37.600]   Nobody knows what consciousness is.
[00:34:37.600 --> 00:34:42.200]   But I'm hoping robots will help us empathize
[00:34:42.200 --> 00:34:47.200]   and understand that there's other creatures
[00:34:47.200 --> 00:34:50.360]   out besides ourselves that can suffer,
[00:34:50.360 --> 00:34:54.840]   that can experience the world
[00:34:54.840 --> 00:34:57.720]   and that we can torture by our actions.
[00:34:57.720 --> 00:34:59.920]   And robots can explicitly teach us that,
[00:34:59.920 --> 00:35:01.520]   I think, better than animals can.
[00:35:01.520 --> 00:35:06.520]   - I have never seen such compassion
[00:35:06.520 --> 00:35:09.440]   from a lot of people in my life
[00:35:11.080 --> 00:35:13.880]   toward any human, animal, child,
[00:35:13.880 --> 00:35:15.240]   as I have a lot of people
[00:35:15.240 --> 00:35:16.840]   in the way they interact with the robot.
[00:35:16.840 --> 00:35:18.480]   'Cause I think there's-- - Compassion, for sure.
[00:35:18.480 --> 00:35:20.000]   - I think there's something of,
[00:35:20.000 --> 00:35:23.760]   I mean, I was on the robot owners chat boards
[00:35:23.760 --> 00:35:26.160]   for a good eight months.
[00:35:26.160 --> 00:35:28.360]   And the main emotional benefit is
[00:35:28.360 --> 00:35:30.600]   she's never gonna cheat on you.
[00:35:30.600 --> 00:35:32.160]   She's never gonna hurt you.
[00:35:32.160 --> 00:35:33.360]   She's never gonna lie to you.
[00:35:33.360 --> 00:35:35.020]   She doesn't judge you.
[00:35:35.020 --> 00:35:38.740]   You know, I think that robots help people,
[00:35:38.740 --> 00:35:40.880]   and this is part of the work I do with animals.
[00:35:40.880 --> 00:35:43.040]   Like I do equine therapy and train dogs and stuff
[00:35:43.040 --> 00:35:46.280]   because there is this safe space to be authentic.
[00:35:46.280 --> 00:35:48.240]   You're with this being that doesn't care
[00:35:48.240 --> 00:35:49.080]   what you do for a living,
[00:35:49.080 --> 00:35:50.400]   doesn't care how much money you have,
[00:35:50.400 --> 00:35:51.540]   doesn't care who you're dating,
[00:35:51.540 --> 00:35:52.440]   doesn't care what you look like,
[00:35:52.440 --> 00:35:54.560]   doesn't care if you have cellulite, whatever.
[00:35:54.560 --> 00:35:57.960]   You feel safe to be able to truly be present
[00:35:57.960 --> 00:36:00.160]   without being defensive and worrying about eye contact
[00:36:00.160 --> 00:36:02.920]   and being triggered by needing to be perfect
[00:36:02.920 --> 00:36:04.840]   and fear of judgment and all that.
[00:36:04.840 --> 00:36:07.320]   And robots really can't judge you yet,
[00:36:07.320 --> 00:36:09.320]   but they can't judge you.
[00:36:09.320 --> 00:36:13.480]   But I think it really puts people at ease
[00:36:13.480 --> 00:36:15.320]   and at their most authentic.
[00:36:15.320 --> 00:36:18.720]   - Do you think you can have a deep connection
[00:36:18.720 --> 00:36:22.040]   with a robot that's not judging?
[00:36:22.040 --> 00:36:25.440]   Do you think you can really have a relationship
[00:36:25.440 --> 00:36:30.000]   with a robot or a human being that's a safe space?
[00:36:30.000 --> 00:36:34.320]   Or is attention, mystery, danger necessary
[00:36:34.320 --> 00:36:36.000]   for a deep connection?
[00:36:36.000 --> 00:36:38.640]   - I'm gonna speak for myself and say that
[00:36:38.640 --> 00:36:40.120]   I grew up in Alcoa, Colombo.
[00:36:40.120 --> 00:36:41.440]   I identify as a codependent,
[00:36:41.440 --> 00:36:43.280]   talked about this stuff before,
[00:36:43.280 --> 00:36:45.360]   but for me, it's very hard to be in a relationship
[00:36:45.360 --> 00:36:47.600]   with a human being without feeling like
[00:36:47.600 --> 00:36:50.760]   I need to perform in some way or deliver in some way.
[00:36:50.760 --> 00:36:51.920]   And I don't know if that's just the people
[00:36:51.920 --> 00:36:56.520]   I've been in a relationship with or me or my brokenness,
[00:36:56.520 --> 00:37:01.320]   but I do think this is gonna sound really negative
[00:37:01.320 --> 00:37:06.400]   and pessimistic, but I do think a lot of our relationships
[00:37:06.400 --> 00:37:08.360]   are a projection and a lot of our relationships
[00:37:08.360 --> 00:37:12.280]   are performance, and I don't think I really understood that
[00:37:12.280 --> 00:37:15.280]   until I worked with horses.
[00:37:15.280 --> 00:37:18.080]   And most communications with human is nonverbal, right?
[00:37:18.080 --> 00:37:22.000]   I can say, "I love you," but you don't think I love you.
[00:37:22.000 --> 00:37:24.280]   Whereas with animals, it's very direct.
[00:37:24.280 --> 00:37:26.840]   It's all physical, it's all energy.
[00:37:26.840 --> 00:37:28.520]   I feel like that with robots, too.
[00:37:28.520 --> 00:37:29.960]   It feels very...
[00:37:29.960 --> 00:37:35.280]   How I say something doesn't matter.
[00:37:35.280 --> 00:37:36.920]   My inflection doesn't really matter.
[00:37:36.920 --> 00:37:40.280]   And you thinking that my tone is disrespectful,
[00:37:40.280 --> 00:37:42.160]   like you're not filtering it through all
[00:37:42.160 --> 00:37:43.800]   of the bad relationships you've been in.
[00:37:43.800 --> 00:37:44.840]   You're not filtering it through
[00:37:44.840 --> 00:37:45.880]   the way your mom talked to you.
[00:37:45.880 --> 00:37:47.760]   You're not getting triggered.
[00:37:47.760 --> 00:37:49.400]   I find that for the most part,
[00:37:49.400 --> 00:37:51.000]   people don't always receive things
[00:37:51.000 --> 00:37:53.640]   the way that you intend them to or the way intended,
[00:37:53.640 --> 00:37:56.120]   and that makes relationships really murky.
[00:37:56.120 --> 00:37:57.440]   - So the relationships with animals
[00:37:57.440 --> 00:38:00.680]   and relationship with the robots as they are now,
[00:38:00.680 --> 00:38:03.080]   you kind of implied that that's more healthy.
[00:38:05.240 --> 00:38:08.080]   Can you have a healthy relationship with other humans?
[00:38:08.080 --> 00:38:10.120]   Or not healthy, I don't like that word,
[00:38:10.120 --> 00:38:14.440]   but shouldn't it be, you've talked about codependency.
[00:38:14.440 --> 00:38:16.640]   Maybe you can talk about what is codependency,
[00:38:16.640 --> 00:38:21.640]   but is the challenges of that,
[00:38:21.640 --> 00:38:24.640]   the complexity of that necessary for passion,
[00:38:24.640 --> 00:38:27.360]   for love between humans?
[00:38:27.360 --> 00:38:29.560]   - That's right, you love passion.
[00:38:29.560 --> 00:38:31.240]   (laughing)
[00:38:31.240 --> 00:38:32.080]   That's a good thing.
[00:38:32.080 --> 00:38:34.000]   - I thought this would be a safe space.
[00:38:34.000 --> 00:38:36.240]   (laughing)
[00:38:36.240 --> 00:38:40.080]   I got trolled by Rogan for hours on this.
[00:38:40.080 --> 00:38:42.680]   - Look, I am not anti-passion.
[00:38:42.680 --> 00:38:45.360]   I think that I've just maybe been around long enough
[00:38:45.360 --> 00:38:48.320]   to know that sometimes it's ephemeral
[00:38:48.320 --> 00:38:53.040]   and that passion is a mixture
[00:38:53.040 --> 00:38:55.440]   of a lot of different things.
[00:38:55.440 --> 00:38:57.720]   Adrenaline, which turns into dopamine, cortisol.
[00:38:57.720 --> 00:38:59.200]   It's a lot of neurochemicals.
[00:38:59.200 --> 00:39:01.240]   It's a lot of projection.
[00:39:01.240 --> 00:39:03.280]   It's a lot of what we've seen in movies.
[00:39:03.280 --> 00:39:06.240]   It's a lot of, I identify as an addict.
[00:39:06.240 --> 00:39:08.640]   So for me, sometimes passion is like,
[00:39:08.640 --> 00:39:10.200]   uh-oh, this could be bad.
[00:39:10.200 --> 00:39:11.600]   And I think we've been so conditioned to believe
[00:39:11.600 --> 00:39:13.160]   that passion means your soulmates.
[00:39:13.160 --> 00:39:14.400]   And I mean, how many times have you had
[00:39:14.400 --> 00:39:15.680]   a passionate connection with someone
[00:39:15.680 --> 00:39:17.840]   and then it was a total train wreck?
[00:39:17.840 --> 00:39:19.840]   - The train wreck is interesting.
[00:39:19.840 --> 00:39:21.160]   - How many times exactly?
[00:39:21.160 --> 00:39:22.000]   - Exactly.
[00:39:22.000 --> 00:39:22.840]   What's a train wreck?
[00:39:22.840 --> 00:39:24.480]   - You just did a lot of math in your head
[00:39:24.480 --> 00:39:25.520]   in that little moment.
[00:39:25.520 --> 00:39:26.560]   - Counting.
[00:39:26.560 --> 00:39:28.640]   I mean, what's a train wreck?
[00:39:28.640 --> 00:39:31.560]   What's a, why is obsession,
[00:39:31.560 --> 00:39:33.680]   so you describe this codependency
[00:39:33.680 --> 00:39:37.480]   and sort of the idea of attachment,
[00:39:37.480 --> 00:39:40.360]   over attachment to people who don't deserve
[00:39:40.360 --> 00:39:44.600]   that kind of attachment as somehow a bad thing.
[00:39:44.600 --> 00:39:47.800]   I think our society says it's a bad thing.
[00:39:47.800 --> 00:39:49.680]   It probably is a bad thing.
[00:39:49.680 --> 00:39:52.640]   Like a delicious burger is a bad thing.
[00:39:52.640 --> 00:39:53.480]   I don't know.
[00:39:53.480 --> 00:39:54.360]   - Right, oh, that's a good point.
[00:39:54.360 --> 00:39:55.640]   I think that you're pointing out something
[00:39:55.640 --> 00:39:57.280]   really fascinating, which is like passion,
[00:39:57.280 --> 00:40:00.280]   if you go into it knowing this is like pizza
[00:40:00.280 --> 00:40:01.920]   where it's gonna be delicious for two hours
[00:40:01.920 --> 00:40:03.480]   and then I don't have to have it again for three.
[00:40:03.480 --> 00:40:06.440]   If you can have a choice in the passion,
[00:40:06.440 --> 00:40:07.600]   I define passion as something
[00:40:07.600 --> 00:40:09.600]   that is relatively unmanageable
[00:40:09.600 --> 00:40:10.880]   and something you can't control
[00:40:10.880 --> 00:40:13.760]   or stop and start with your own volition.
[00:40:13.760 --> 00:40:16.360]   So maybe we're operating under different definitions.
[00:40:16.360 --> 00:40:17.960]   If passion is something that like,
[00:40:17.960 --> 00:40:21.000]   you know, ruins your marriages
[00:40:21.000 --> 00:40:23.240]   and screws up your professional life
[00:40:23.240 --> 00:40:27.360]   and becomes this thing that you're not in control of
[00:40:27.360 --> 00:40:29.920]   and becomes addictive, I think that's the difference
[00:40:29.920 --> 00:40:32.640]   is is it a choice or is it not a choice?
[00:40:32.640 --> 00:40:35.160]   And if it is a choice, then passion's great.
[00:40:35.160 --> 00:40:37.400]   But if it's something that like consumes you
[00:40:37.400 --> 00:40:39.400]   and makes you start making bad decisions
[00:40:39.400 --> 00:40:41.200]   and clouds your frontal lobe
[00:40:41.200 --> 00:40:44.080]   and is just all about dopamine
[00:40:44.080 --> 00:40:46.240]   and not really about the person
[00:40:46.240 --> 00:40:47.840]   and more about the neurochemical,
[00:40:47.840 --> 00:40:50.800]   we call it sort of the drug, the internal drug cabinet.
[00:40:50.800 --> 00:40:53.000]   If it's all just you're on drugs, that's different,
[00:40:53.000 --> 00:40:55.040]   you know, 'cause sometimes you're just on drugs.
[00:40:55.040 --> 00:40:57.440]   - Okay, so there's a philosophical question here.
[00:40:58.520 --> 00:41:03.440]   So would you rather, and it's interesting for a comedian,
[00:41:03.440 --> 00:41:07.640]   brilliant comedian to speak so eloquently
[00:41:07.640 --> 00:41:09.560]   about a balanced life.
[00:41:09.560 --> 00:41:12.100]   I kind of argue against this point.
[00:41:12.100 --> 00:41:13.080]   There's such an obsession
[00:41:13.080 --> 00:41:15.580]   of creating this healthy lifestyle now,
[00:41:15.580 --> 00:41:18.160]   psychologically speaking.
[00:41:18.160 --> 00:41:19.760]   You know, I'm a fan of the idea
[00:41:19.760 --> 00:41:24.760]   that you sort of fly high and you crash and die at 27
[00:41:24.760 --> 00:41:26.520]   as also a possible life,
[00:41:26.520 --> 00:41:28.000]   and it's not one we should judge
[00:41:28.000 --> 00:41:30.680]   because I think there's moments of greatness.
[00:41:30.680 --> 00:41:32.160]   I've talked to Olympic athletes
[00:41:32.160 --> 00:41:34.320]   where some of their greatest moments
[00:41:34.320 --> 00:41:36.600]   are achieved in their early 20s,
[00:41:36.600 --> 00:41:39.880]   and the rest of their life is in a kind of fog
[00:41:39.880 --> 00:41:42.320]   of almost of a depression because they can never--
[00:41:42.320 --> 00:41:44.280]   - Because it was based on their physical prowess, right?
[00:41:44.280 --> 00:41:46.440]   - Physical prowess, and they'll never,
[00:41:46.440 --> 00:41:50.260]   so that, so they're watching their physical prowess fade,
[00:41:50.260 --> 00:41:54.720]   and they'll never achieve the kind of height,
[00:41:54.720 --> 00:41:57.220]   not just physical, of just emotion,
[00:41:57.220 --> 00:41:59.720]   of-- - Well, the max number
[00:41:59.720 --> 00:42:03.200]   of neurochemicals, and you also put your money
[00:42:03.200 --> 00:42:04.760]   on the wrong horse.
[00:42:04.760 --> 00:42:06.520]   That's where I would just go like,
[00:42:06.520 --> 00:42:10.260]   oh, yeah, if you're doing a job where you peak at 22,
[00:42:10.260 --> 00:42:12.440]   the rest of your life is gonna be hard.
[00:42:12.440 --> 00:42:15.280]   - That idea is considering the notion
[00:42:15.280 --> 00:42:17.620]   that you wanna optimize some kind of,
[00:42:17.620 --> 00:42:19.440]   but we're all gonna die soon.
[00:42:19.440 --> 00:42:20.280]   - What?
[00:42:20.280 --> 00:42:22.000]   (laughing)
[00:42:22.000 --> 00:42:23.400]   Now you tell me.
[00:42:23.400 --> 00:42:26.920]   I've immortalized myself, so I'm gonna be fine.
[00:42:26.920 --> 00:42:30.440]   - See, you're almost like, how many Oscar-winning movies
[00:42:30.440 --> 00:42:34.160]   can I direct by the time I'm 100?
[00:42:34.160 --> 00:42:35.880]   How many this and that?
[00:42:35.880 --> 00:42:40.880]   But there's, life is short, relatively speaking.
[00:42:40.880 --> 00:42:42.680]   - I know, but it can also come in different,
[00:42:42.680 --> 00:42:45.200]   you go, life is short, play hard,
[00:42:45.200 --> 00:42:47.720]   fall in love as much as you can, run into walls.
[00:42:47.720 --> 00:42:51.360]   I would also go, life is short, don't deplete yourself
[00:42:51.360 --> 00:42:55.800]   on things that aren't sustainable and that you can't keep.
[00:42:55.800 --> 00:42:56.640]   - Yeah.
[00:42:56.640 --> 00:42:59.840]   - So I think everyone gets dopamine from different places,
[00:42:59.840 --> 00:43:01.840]   everyone has meaning from different places.
[00:43:01.840 --> 00:43:04.640]   I look at the fleeting, passionate relationships
[00:43:04.640 --> 00:43:08.000]   I've had in the past, and I don't have pride in them.
[00:43:08.000 --> 00:43:09.120]   I think that you have to decide
[00:43:09.120 --> 00:43:11.240]   what helps you sleep at night.
[00:43:11.240 --> 00:43:13.600]   For me, it's pride and feeling like I behave
[00:43:13.600 --> 00:43:16.160]   with grace and integrity, that's just me personally.
[00:43:16.160 --> 00:43:19.760]   Everyone can go like, yeah, I slept with all the hot chicks
[00:43:19.760 --> 00:43:23.560]   in Italy, I could, and I did all the whatever,
[00:43:23.560 --> 00:43:25.160]   like whatever you value.
[00:43:25.160 --> 00:43:26.720]   We're allowed to value different things.
[00:43:26.720 --> 00:43:28.080]   - We're talking about Brian Callan.
[00:43:28.080 --> 00:43:29.800]   (laughing)
[00:43:29.800 --> 00:43:32.640]   - Brian Callan has lived his life to the fullest,
[00:43:32.640 --> 00:43:34.600]   to say the least, but I think that it's just
[00:43:34.600 --> 00:43:38.920]   for me personally, I, and this could be like my workaholism
[00:43:38.920 --> 00:43:43.680]   or my achievementism, if I don't have something
[00:43:43.680 --> 00:43:47.960]   to show for something, I feel like it's a waste of time
[00:43:47.960 --> 00:43:50.360]   or some kind of loss.
[00:43:50.360 --> 00:43:52.680]   I'm in a 12-step program, and the third step would say,
[00:43:52.680 --> 00:43:54.200]   there's no such thing as waste of time
[00:43:54.200 --> 00:43:56.880]   and everything happens exactly as it should
[00:43:56.880 --> 00:43:59.560]   and whatever, that's a way to just sort of keep us sane
[00:43:59.560 --> 00:44:01.880]   so we don't grieve too much and beat ourselves up
[00:44:01.880 --> 00:44:04.720]   over past mistakes, there's no such thing as mistakes,
[00:44:04.720 --> 00:44:08.400]   da-da-da, but I think passion is,
[00:44:08.400 --> 00:44:11.760]   I think it's so life-affirming and one of the few things
[00:44:11.760 --> 00:44:14.920]   that maybe people like us, makes us feel awake and seen
[00:44:14.920 --> 00:44:19.920]   and we just have such a high threshold for adrenaline.
[00:44:19.920 --> 00:44:22.800]   You know, I mean, you are a fighter, right?
[00:44:22.800 --> 00:44:27.520]   Yeah, okay, so yeah, so you have a very high tolerance
[00:44:27.520 --> 00:44:30.440]   for adrenaline, and I think that Olympic athletes,
[00:44:30.440 --> 00:44:33.680]   the amount of adrenaline they get from performing,
[00:44:33.680 --> 00:44:35.720]   it's very hard to follow that, it's like when guys come back
[00:44:35.720 --> 00:44:38.160]   from the military and they have depression,
[00:44:38.160 --> 00:44:40.720]   it's like, do you miss bullets flying at you?
[00:44:40.720 --> 00:44:42.880]   Yeah, kind of, because of that adrenaline
[00:44:42.880 --> 00:44:45.160]   which turned into dopamine and the camaraderie,
[00:44:45.160 --> 00:44:46.600]   I mean, there's people that speak much better
[00:44:46.600 --> 00:44:50.120]   about this than I do, but I just, I'm obsessed
[00:44:50.120 --> 00:44:52.240]   with neurology and I'm just obsessed with sort of
[00:44:52.240 --> 00:44:54.480]   the lies we tell ourselves in order
[00:44:54.480 --> 00:44:57.120]   to justify getting neurochemicals.
[00:44:57.120 --> 00:45:00.400]   - You've done actually quite, done a lot of thinking
[00:45:00.400 --> 00:45:03.480]   and talking about neurology, just kind of look
[00:45:03.480 --> 00:45:06.960]   at human behavior through the lens of looking
[00:45:06.960 --> 00:45:09.160]   at how our actually, chemically, our brain works.
[00:45:09.160 --> 00:45:13.960]   So what, first of all, why did you connect with that idea
[00:45:13.960 --> 00:45:17.600]   and what have you, how has your view of the world changed
[00:45:17.600 --> 00:45:21.760]   by considering the brain is just a machine?
[00:45:22.480 --> 00:45:24.640]   - You know, I know it probably sounds really nihilistic,
[00:45:24.640 --> 00:45:27.600]   but for me it's very liberating to know a lot
[00:45:27.600 --> 00:45:30.120]   about neurochemicals because you don't have to,
[00:45:30.120 --> 00:45:32.560]   it's like the same thing with like critics,
[00:45:32.560 --> 00:45:34.840]   like critical reviews, if you believe the good,
[00:45:34.840 --> 00:45:36.200]   you have to believe the bad kind of thing.
[00:45:36.200 --> 00:45:39.000]   Like, you know, if you believe that your bad choices
[00:45:39.000 --> 00:45:43.840]   were because of your moral integrity or whatever,
[00:45:43.840 --> 00:45:44.840]   you have to believe your good ones,
[00:45:44.840 --> 00:45:46.400]   I just think there's something really liberating
[00:45:46.400 --> 00:45:48.160]   and going like, oh, that was just adrenaline,
[00:45:48.160 --> 00:45:49.800]   I just said that thing 'cause I was adrenalized
[00:45:49.800 --> 00:45:52.160]   and I was scared and my amygdala was activated
[00:45:52.160 --> 00:45:54.480]   and that's why I said you're an asshole and get out.
[00:45:54.480 --> 00:45:56.800]   And that's, you know, I just think it's important
[00:45:56.800 --> 00:45:58.880]   to delineate what's nature and what's nurture,
[00:45:58.880 --> 00:46:01.120]   what is your choice and what is just your brain
[00:46:01.120 --> 00:46:02.120]   trying to keep you safe.
[00:46:02.120 --> 00:46:04.640]   I think we forget that even though we have security systems
[00:46:04.640 --> 00:46:07.000]   and homes and locks on our doors, that our brain,
[00:46:07.000 --> 00:46:08.800]   for the most part, is just trying to keep us safe
[00:46:08.800 --> 00:46:10.440]   all the time, it's why we hold grudges,
[00:46:10.440 --> 00:46:13.000]   it's why we get angry, it's why we get road rage,
[00:46:13.000 --> 00:46:14.840]   it's why we do a lot of things.
[00:46:14.840 --> 00:46:17.360]   And it's also, when I started learning about neurology,
[00:46:17.360 --> 00:46:19.760]   I started having so much more compassion for other people.
[00:46:19.760 --> 00:46:21.280]   You know, if someone yelled at me,
[00:46:21.280 --> 00:46:22.640]   being like, fuck you, on the road,
[00:46:22.640 --> 00:46:24.680]   I'd be like, okay, he's producing adrenaline right now
[00:46:24.680 --> 00:46:27.760]   because we're all going 65 miles an hour
[00:46:27.760 --> 00:46:30.400]   and our brains aren't really designed
[00:46:30.400 --> 00:46:33.360]   for this type of stress and he's scared.
[00:46:33.360 --> 00:46:35.400]   He was scared, you know, so that really helped me
[00:46:35.400 --> 00:46:38.720]   to have more love for people in my everyday life
[00:46:38.720 --> 00:46:41.080]   instead of being in fight or flight mode.
[00:46:41.080 --> 00:46:44.240]   But the, I think, more interesting answer to your question
[00:46:44.240 --> 00:46:45.800]   is that I've had migraines my whole life,
[00:46:45.800 --> 00:46:49.120]   like I've suffered with really intense migraines,
[00:46:49.120 --> 00:46:52.440]   ocular migraines, ones where my arm would go numb,
[00:46:52.440 --> 00:46:55.080]   and I just started having to go to so many doctors
[00:46:55.080 --> 00:46:58.400]   to learn about it, and I started, you know,
[00:46:58.400 --> 00:47:00.720]   learning that we don't really know that much.
[00:47:00.720 --> 00:47:03.680]   We know a lot, but it's wild to go into one
[00:47:03.680 --> 00:47:04.920]   of the best neurologists in the world
[00:47:04.920 --> 00:47:05.960]   who's like, yeah, we don't know.
[00:47:05.960 --> 00:47:07.400]   - We don't know. - We don't know.
[00:47:07.400 --> 00:47:08.720]   And that fascinated me.
[00:47:08.720 --> 00:47:10.760]   - It's like one of the worst pains you can probably have,
[00:47:10.760 --> 00:47:13.360]   all that stuff, and we don't know the source.
[00:47:13.360 --> 00:47:15.520]   - We don't know the source, and there is something
[00:47:15.520 --> 00:47:18.640]   really fascinating about when your left arm
[00:47:18.640 --> 00:47:21.080]   starts going numb, and you start not being able to see
[00:47:21.080 --> 00:47:22.880]   out of the left side of both your eyes,
[00:47:22.880 --> 00:47:25.400]   and I remember when the migraines get really bad,
[00:47:25.400 --> 00:47:26.920]   it's like a mini stroke almost,
[00:47:26.920 --> 00:47:29.920]   and you're able to see words on a page,
[00:47:29.920 --> 00:47:31.280]   but I can't read them.
[00:47:31.280 --> 00:47:33.120]   They just look like symbols to me.
[00:47:33.120 --> 00:47:35.040]   So there's something just really fascinating to me
[00:47:35.040 --> 00:47:38.320]   about your brain just being able to stop functioning,
[00:47:38.320 --> 00:47:41.680]   and so I just wanted to learn about it, study about it.
[00:47:41.680 --> 00:47:43.400]   I did all these weird alternative treatments.
[00:47:43.400 --> 00:47:45.920]   I got this piercing in here that actually works.
[00:47:45.920 --> 00:47:48.160]   I've tried everything, and then both my parents
[00:47:48.160 --> 00:47:49.240]   had strokes.
[00:47:49.240 --> 00:47:51.080]   So when both of my parents had strokes,
[00:47:51.080 --> 00:47:54.200]   I became sort of the person who had to decide
[00:47:54.200 --> 00:47:56.720]   what was gonna happen with their recovery,
[00:47:56.720 --> 00:47:59.160]   which is just a wild thing to have to deal with it,
[00:47:59.160 --> 00:48:02.160]   you know, 28 years old, when it happened,
[00:48:02.160 --> 00:48:05.120]   and I started spending basically all day every day
[00:48:05.120 --> 00:48:08.240]   in ICUs with neurologists learning about what happened
[00:48:08.240 --> 00:48:11.200]   in my dad's brain, and why he can't move his left arm,
[00:48:11.200 --> 00:48:13.600]   but he can move his right leg, but he can't see out of the,
[00:48:13.600 --> 00:48:16.000]   you know, and then my mom had another stroke
[00:48:16.000 --> 00:48:18.120]   in a different part of the brain.
[00:48:18.120 --> 00:48:20.560]   So I started having to learn what parts of the brain
[00:48:20.560 --> 00:48:23.200]   did what, and so that I wouldn't take their behavior
[00:48:23.200 --> 00:48:25.160]   so personally, and so that I would be able to manage
[00:48:25.160 --> 00:48:27.440]   my expectations in terms of their recovery.
[00:48:27.440 --> 00:48:31.280]   So my mom, because it affected a lot of her frontal lobe,
[00:48:31.280 --> 00:48:33.120]   changed a lot as a person.
[00:48:33.120 --> 00:48:35.800]   She was way more emotional, she was way more micromanaged,
[00:48:35.800 --> 00:48:37.000]   she was forgetting certain things,
[00:48:37.000 --> 00:48:40.320]   so it broke my heart less when I was able to know,
[00:48:40.320 --> 00:48:42.080]   oh yeah, well, the stroke hit this part of the brain,
[00:48:42.080 --> 00:48:44.240]   and that's the one that's responsible for short-term memory
[00:48:44.240 --> 00:48:46.880]   and that's responsible for long-term memory, da-da-da,
[00:48:46.880 --> 00:48:48.720]   and then my brother just got something
[00:48:48.720 --> 00:48:50.640]   called viral encephalitis,
[00:48:50.640 --> 00:48:53.280]   which is an infection inside the brain.
[00:48:53.280 --> 00:48:56.320]   So it was kind of wild that I was able to go,
[00:48:56.320 --> 00:48:58.240]   oh, I know exactly what's happening here, and I know,
[00:48:58.240 --> 00:48:59.800]   you know, so.
[00:48:59.800 --> 00:49:02.480]   - So that allows you to have some more compassion
[00:49:02.480 --> 00:49:04.480]   for the struggles that people have,
[00:49:04.480 --> 00:49:06.640]   but does it take away some of the magic
[00:49:06.640 --> 00:49:10.680]   for some of the, from some of the more positive experiences
[00:49:10.680 --> 00:49:12.000]   of life? - Sometimes.
[00:49:12.000 --> 00:49:15.360]   Sometimes, and I don't, I'm such a control addict
[00:49:15.360 --> 00:49:18.120]   that, you know, I think our biggest,
[00:49:18.120 --> 00:49:19.920]   someone like me, my biggest dream
[00:49:19.920 --> 00:49:22.240]   is to know why someone's doing, that's what stand-up is.
[00:49:22.240 --> 00:49:23.440]   It's just trying to figure out why,
[00:49:23.440 --> 00:49:25.040]   or that's what writing is, that's what acting is,
[00:49:25.040 --> 00:49:26.400]   that's what performing is, it's trying to figure out
[00:49:26.400 --> 00:49:27.440]   why someone would do something.
[00:49:27.440 --> 00:49:30.040]   As an actor, you get a piece of, you know, material,
[00:49:30.040 --> 00:49:32.120]   and you go, this person, why would he say that?
[00:49:32.120 --> 00:49:33.760]   Why would she pick up that cup?
[00:49:33.760 --> 00:49:35.040]   Why would she walk over here?
[00:49:35.040 --> 00:49:36.560]   It's really why, why, why, why.
[00:49:36.560 --> 00:49:39.600]   So I think neurology is, if you're trying to figure out
[00:49:39.600 --> 00:49:41.520]   human motives and why people do what they do,
[00:49:41.520 --> 00:49:44.000]   it'd be crazy not to understand
[00:49:44.000 --> 00:49:46.080]   how neurochemicals motivate us.
[00:49:46.080 --> 00:49:48.080]   I also have a lot of addiction in my family,
[00:49:48.080 --> 00:49:51.480]   and hardcore drug addiction and mental illness,
[00:49:51.480 --> 00:49:53.720]   and in order to cope with it,
[00:49:53.720 --> 00:49:54.760]   you really have to understand it,
[00:49:54.760 --> 00:49:56.000]   borderline personality disorder,
[00:49:56.000 --> 00:49:58.340]   schizophrenia, and drug addiction.
[00:49:58.340 --> 00:50:00.640]   So I have a lot of people I love
[00:50:00.640 --> 00:50:02.840]   that suffer from drug addiction and alcoholism,
[00:50:02.840 --> 00:50:04.760]   and the first thing they started teaching you
[00:50:04.760 --> 00:50:05.880]   is it's not a choice.
[00:50:05.880 --> 00:50:07.360]   These people's dopamine receptors
[00:50:07.360 --> 00:50:09.640]   don't hold dopamine the same ways yours do.
[00:50:09.640 --> 00:50:11.600]   Their frontal lobe is underdeveloped.
[00:50:11.600 --> 00:50:14.600]   Like, you know, and that really helped me
[00:50:14.600 --> 00:50:17.920]   to navigate loving people
[00:50:17.920 --> 00:50:20.240]   that were addicted to substances.
[00:50:20.240 --> 00:50:22.600]   - I wanna be careful with this question,
[00:50:22.600 --> 00:50:24.240]   but how much--
[00:50:24.240 --> 00:50:25.320]   - Money do you have?
[00:50:25.320 --> 00:50:27.480]   - How much-- (laughing)
[00:50:27.480 --> 00:50:28.520]   Can I borrow $10?
[00:50:28.520 --> 00:50:30.920]   (laughing)
[00:50:30.920 --> 00:50:31.760]   Okay.
[00:50:31.760 --> 00:50:36.520]   No, is how much control,
[00:50:36.520 --> 00:50:39.760]   how much, despite the chemical imbalances
[00:50:39.760 --> 00:50:42.920]   or the biological limitations
[00:50:42.920 --> 00:50:44.480]   that each of our individual brains have,
[00:50:44.480 --> 00:50:47.080]   how much mind over matter is there?
[00:50:47.080 --> 00:50:49.440]   So through things,
[00:50:49.440 --> 00:50:53.200]   and I've known people with clinical depression,
[00:50:53.200 --> 00:50:55.560]   and so it's always a touchy subject
[00:50:55.560 --> 00:50:57.680]   to say how much they can really help it.
[00:50:57.680 --> 00:50:58.520]   - Very.
[00:50:58.520 --> 00:51:01.680]   - What can you, yeah, what can you,
[00:51:01.680 --> 00:51:03.560]   'cause you've talked about codependency,
[00:51:03.560 --> 00:51:07.400]   you've talked about issues that you struggle through,
[00:51:07.400 --> 00:51:09.880]   and nevertheless you choose to take a journey
[00:51:09.880 --> 00:51:11.200]   of healing and so on.
[00:51:11.200 --> 00:51:14.240]   So that's your choice, that's your actions.
[00:51:14.240 --> 00:51:17.720]   So how much can you do to help fight the limitations
[00:51:17.720 --> 00:51:20.000]   of the neurochemicals in your brain?
[00:51:20.000 --> 00:51:21.800]   - That's such an interesting question,
[00:51:21.800 --> 00:51:23.440]   and I don't think I'm at all qualified to answer,
[00:51:23.440 --> 00:51:25.560]   but I'll say what I do know.
[00:51:25.560 --> 00:51:28.200]   And really quick, just the definition of codependency,
[00:51:28.200 --> 00:51:29.880]   I think a lot of people think of codependency
[00:51:29.880 --> 00:51:32.680]   as like two people that can't stop hanging out,
[00:51:32.680 --> 00:51:35.200]   you know, or like, you know,
[00:51:35.200 --> 00:51:36.640]   that's not totally off,
[00:51:36.640 --> 00:51:38.280]   but I think for the most part,
[00:51:38.280 --> 00:51:39.960]   my favorite definition of codependency
[00:51:39.960 --> 00:51:42.920]   is the inability to tolerate the discomfort of others.
[00:51:42.920 --> 00:51:44.040]   You grow up in an alcoholic home,
[00:51:44.040 --> 00:51:45.200]   you grow up around mental illness,
[00:51:45.200 --> 00:51:46.480]   you grow up in chaos,
[00:51:46.480 --> 00:51:48.280]   you have a parent that's a narcissist,
[00:51:48.280 --> 00:51:50.520]   you basically are wired to just,
[00:51:50.520 --> 00:51:52.760]   people, please, worry about others,
[00:51:52.760 --> 00:51:54.720]   be perfect, walk on eggshells,
[00:51:54.720 --> 00:51:56.680]   shape shift to accommodate other people.
[00:51:56.680 --> 00:52:01.680]   So codependence is a very active wiring issue
[00:52:01.680 --> 00:52:04.480]   that, you know, doesn't just affect
[00:52:04.480 --> 00:52:05.680]   your romantic relationships,
[00:52:05.680 --> 00:52:07.040]   it affects you being a boss,
[00:52:07.040 --> 00:52:10.520]   it affects you in the world online,
[00:52:10.520 --> 00:52:12.040]   you know, you get one negative comment
[00:52:12.040 --> 00:52:14.160]   and it throws you for two weeks,
[00:52:14.160 --> 00:52:16.200]   you know, it also is linked to eating disorders
[00:52:16.200 --> 00:52:17.160]   and other kinds of addiction.
[00:52:17.160 --> 00:52:20.240]   So it's a very big thing.
[00:52:20.240 --> 00:52:22.000]   And I think a lot of people sometimes only think
[00:52:22.000 --> 00:52:23.520]   that it's in romantic relationships,
[00:52:23.520 --> 00:52:25.960]   so I always feel the need to say that.
[00:52:25.960 --> 00:52:28.080]   And also one of the reasons I love the idea of robots
[00:52:28.080 --> 00:52:29.880]   so much because you don't have to walk
[00:52:29.880 --> 00:52:30.880]   on eggshells around them,
[00:52:30.880 --> 00:52:33.320]   you don't have to worry they're gonna get mad at you yet,
[00:52:33.320 --> 00:52:36.920]   but there's no, codependence are hypersensitive
[00:52:36.920 --> 00:52:39.560]   to the needs and moods of others,
[00:52:39.560 --> 00:52:42.160]   and it's very exhausting, it's depleting.
[00:52:42.160 --> 00:52:44.720]   Just, well, one conversation about where we're gonna
[00:52:44.720 --> 00:52:47.280]   go to dinner is like, do you wanna go get Chinese food?
[00:52:47.280 --> 00:52:48.440]   We just had Chinese food.
[00:52:48.440 --> 00:52:50.160]   Well, wait, are you mad?
[00:52:50.160 --> 00:52:52.200]   Well, no, I didn't mean to, and it's just like,
[00:52:52.200 --> 00:52:54.960]   that codependence live in this,
[00:52:54.960 --> 00:52:56.640]   everything means something,
[00:52:56.640 --> 00:53:00.160]   and humans can be very emotionally exhausting.
[00:53:00.160 --> 00:53:01.200]   Why did you look at me that way?
[00:53:01.200 --> 00:53:02.040]   What are you thinking about?
[00:53:02.040 --> 00:53:03.200]   What was that, why'd you check your phone?
[00:53:03.200 --> 00:53:05.040]   It's just, it's a hypersensitivity
[00:53:05.040 --> 00:53:07.920]   that can be incredibly time-consuming,
[00:53:07.920 --> 00:53:10.760]   which is why I love the idea of robots just subbing in.
[00:53:10.760 --> 00:53:13.840]   Even, I've had a hard time running TV shows and stuff
[00:53:13.840 --> 00:53:15.360]   because even asking someone to do something,
[00:53:15.360 --> 00:53:16.520]   I don't wanna come off like a bitch.
[00:53:16.520 --> 00:53:18.680]   I'm very concerned about what other people think of me,
[00:53:18.680 --> 00:53:21.640]   how I'm perceived, which is why I think robots
[00:53:21.640 --> 00:53:23.880]   will be very beneficial for codependence.
[00:53:23.880 --> 00:53:25.600]   - By the way, just a real quick tangent,
[00:53:25.600 --> 00:53:29.160]   that skill or flaw, whatever you wanna call it,
[00:53:29.160 --> 00:53:30.840]   is actually really useful for,
[00:53:30.840 --> 00:53:34.640]   if you ever do start your own podcast for interviewing,
[00:53:34.640 --> 00:53:36.480]   because you're now kind of obsessed
[00:53:36.480 --> 00:53:39.240]   about the mindset of others,
[00:53:39.240 --> 00:53:43.560]   and it makes you a good sort of listener and talker with.
[00:53:43.560 --> 00:53:48.560]   So I think, what's her name from NPR?
[00:53:48.560 --> 00:53:49.480]   - Terry Gross.
[00:53:49.480 --> 00:53:51.720]   - Terry Gross talked about having that, so.
[00:53:51.720 --> 00:53:53.680]   - I don't feel like she has that at all.
[00:53:53.680 --> 00:53:56.320]   (laughing)
[00:53:56.320 --> 00:53:57.160]   What?
[00:53:57.160 --> 00:54:00.760]   She worries about other people's feelings.
[00:54:00.760 --> 00:54:01.800]   - Yeah, absolutely.
[00:54:01.800 --> 00:54:03.680]   - Oh, I don't get that at all.
[00:54:03.680 --> 00:54:05.240]   - I mean, you have to put yourself in the mind
[00:54:05.240 --> 00:54:07.120]   of the person you're speaking with.
[00:54:07.120 --> 00:54:08.880]   - Yes, oh, I see, just in terms of,
[00:54:08.880 --> 00:54:10.200]   yeah, I am starting a podcast,
[00:54:10.200 --> 00:54:12.440]   and the reason I haven't is because I'm codependent
[00:54:12.440 --> 00:54:14.360]   and I'm too worried it's not gonna be perfect.
[00:54:14.360 --> 00:54:17.520]   So a big codependent adage is,
[00:54:17.520 --> 00:54:19.160]   perfectionism leads to procrastination,
[00:54:19.160 --> 00:54:20.320]   which leads to paralysis.
[00:54:20.320 --> 00:54:22.280]   - So how do you, sorry to take a million tangents,
[00:54:22.280 --> 00:54:23.600]   how do you survive on social media?
[00:54:23.600 --> 00:54:25.160]   'Cause you're exceptionally active.
[00:54:25.160 --> 00:54:26.560]   - But by the way, I took you on a tangent
[00:54:26.560 --> 00:54:27.800]   and didn't answer your last question
[00:54:27.800 --> 00:54:29.960]   about how much we can control.
[00:54:29.960 --> 00:54:32.960]   - How much, yeah, we'll return it, or maybe not.
[00:54:32.960 --> 00:54:33.800]   The answer is we can't.
[00:54:33.800 --> 00:54:36.240]   - Now as a codependent, I'm worried, okay, good.
[00:54:36.240 --> 00:54:39.640]   We can, but one of the things that I'm fascinated by
[00:54:39.640 --> 00:54:40.960]   is the first thing you learn
[00:54:40.960 --> 00:54:42.640]   when you go into 12-step programs
[00:54:42.640 --> 00:54:44.200]   or addiction recovery or any of this
[00:54:44.200 --> 00:54:47.840]   is genetics loads the gun, environment pulls the trigger.
[00:54:47.840 --> 00:54:50.540]   And there's certain parts of your genetics
[00:54:50.540 --> 00:54:51.560]   you cannot control.
[00:54:51.560 --> 00:54:54.240]   I come from a lot of alcoholism,
[00:54:54.240 --> 00:54:59.240]   I come from a lot of mental illness,
[00:54:59.920 --> 00:55:01.680]   there's certain things I cannot control
[00:55:01.680 --> 00:55:04.000]   and a lot of things that maybe we don't even know yet
[00:55:04.000 --> 00:55:04.840]   what we can and can't
[00:55:04.840 --> 00:55:06.720]   'cause of how little we actually know about the brain.
[00:55:06.720 --> 00:55:08.600]   But we also talk about the warrior spirit
[00:55:08.600 --> 00:55:12.080]   and there are some people that have that warrior spirit
[00:55:12.080 --> 00:55:15.280]   and we don't necessarily know what that engine is,
[00:55:15.280 --> 00:55:17.960]   whether it's you get dopamine from succeeding
[00:55:17.960 --> 00:55:21.160]   or achieving or martyring yourself
[00:55:21.160 --> 00:55:24.880]   or that tension you get from growing.
[00:55:24.880 --> 00:55:25.720]   So a lot of people are like,
[00:55:25.720 --> 00:55:29.040]   "Oh, well, this person can edify themselves and overcome,
[00:55:29.040 --> 00:55:32.260]   "but if you're getting attention from improving yourself,
[00:55:32.260 --> 00:55:34.560]   "you're gonna keep wanting to do that."
[00:55:34.560 --> 00:55:37.260]   So that is something that helps a lot of
[00:55:37.260 --> 00:55:38.600]   in terms of changing your brain.
[00:55:38.600 --> 00:55:40.440]   If you talk about changing your brain to people
[00:55:40.440 --> 00:55:41.640]   and talk about what you're doing
[00:55:41.640 --> 00:55:42.920]   to overcome set obstacles,
[00:55:42.920 --> 00:55:44.600]   you're gonna get more attention from them,
[00:55:44.600 --> 00:55:46.840]   which is gonna fire off your reward system
[00:55:46.840 --> 00:55:48.400]   and then you're gonna keep doing it.
[00:55:48.400 --> 00:55:50.320]   - Yeah, so you can leverage that momentum.
[00:55:50.320 --> 00:55:52.720]   - So this is why in any 12-step program,
[00:55:52.720 --> 00:55:55.160]   you go into a room and you talk about your progress
[00:55:55.160 --> 00:55:57.120]   'cause then everyone claps for you
[00:55:57.120 --> 00:55:58.800]   and then you're more motivated to keep going.
[00:55:58.800 --> 00:56:00.200]   So that's why we say you're only as sick
[00:56:00.200 --> 00:56:01.240]   as the secrets you keep,
[00:56:01.240 --> 00:56:03.700]   because if you keep things secret,
[00:56:03.700 --> 00:56:06.120]   there's no one guiding you to go in a certain direction.
[00:56:06.120 --> 00:56:07.120]   It's based on, right?
[00:56:07.120 --> 00:56:10.400]   We're sort of designed to get approval from the tribe
[00:56:10.400 --> 00:56:12.840]   or from a group of people 'cause our brain
[00:56:12.840 --> 00:56:14.600]   translates it to safety.
[00:56:14.600 --> 00:56:15.440]   So, yeah.
[00:56:15.440 --> 00:56:17.680]   - And in that case, the tribe is a positive one
[00:56:17.680 --> 00:56:19.520]   that helps you go in a positive direction.
[00:56:19.520 --> 00:56:21.240]   - So that's why it's so important to go into a room
[00:56:21.240 --> 00:56:25.080]   and also say, "Hey, I wanted to use drugs today."
[00:56:25.080 --> 00:56:26.440]   And people go, "Mm."
[00:56:26.440 --> 00:56:27.440]   They go, "Me too."
[00:56:27.720 --> 00:56:30.040]   You feel less alone and you feel less like you're,
[00:56:30.040 --> 00:56:32.720]   you know, have been castigated from the pack or whatever.
[00:56:32.720 --> 00:56:34.200]   And then you say, "And I didn't have any,"
[00:56:34.200 --> 00:56:36.400]   you get a chip when you haven't drank for 30 days
[00:56:36.400 --> 00:56:37.560]   or 60 days or whatever.
[00:56:37.560 --> 00:56:38.600]   You get little rewards.
[00:56:38.600 --> 00:56:43.200]   - So talking about a pack that's not at all healthy or good,
[00:56:43.200 --> 00:56:46.240]   but in fact is often toxic, social media.
[00:56:46.240 --> 00:56:49.760]   So you're one of my favorite people on Twitter and Instagram
[00:56:49.760 --> 00:56:54.480]   to sort of just both the comedy and the insight and just fun.
[00:56:54.480 --> 00:56:55.760]   How do you prevent social media
[00:56:55.760 --> 00:56:57.240]   from destroying your mental health?
[00:56:57.240 --> 00:56:58.080]   - I haven't.
[00:56:58.080 --> 00:57:00.400]   I haven't.
[00:57:00.400 --> 00:57:03.160]   It's the next big epidemic, isn't it?
[00:57:03.160 --> 00:57:06.600]   I don't think I have.
[00:57:06.600 --> 00:57:08.680]   I don't think--
[00:57:08.680 --> 00:57:10.760]   - Is moderation the answer?
[00:57:10.760 --> 00:57:14.320]   - Maybe, but you can do a lot of damage in a moderate way.
[00:57:14.320 --> 00:57:17.120]   I mean, I guess, again, it depends on your goals, you know?
[00:57:17.120 --> 00:57:19.600]   And I think for me,
[00:57:19.600 --> 00:57:21.800]   the way that my addiction to social media,
[00:57:21.800 --> 00:57:23.080]   I'm happy to call it an addiction.
[00:57:23.080 --> 00:57:24.960]   I mean, and I define it as an addiction
[00:57:24.960 --> 00:57:26.280]   because it stops being a choice.
[00:57:26.280 --> 00:57:28.120]   There are times I just reach over and I'm like,
[00:57:28.120 --> 00:57:29.240]   that was--
[00:57:29.240 --> 00:57:30.080]   - Yeah, that was weird.
[00:57:30.080 --> 00:57:31.320]   - That was weird.
[00:57:31.320 --> 00:57:33.720]   I'll be driving sometimes and I'll be like, oh my God,
[00:57:33.720 --> 00:57:36.800]   my arm just went to my phone, you know?
[00:57:36.800 --> 00:57:37.800]   I can put it down.
[00:57:37.800 --> 00:57:41.360]   I can take time away from it, but when I do, I get antsy.
[00:57:41.360 --> 00:57:43.400]   I get restless, irritable, and discontent.
[00:57:43.400 --> 00:57:45.840]   I mean, that's kind of the definition, isn't it?
[00:57:45.840 --> 00:57:48.680]   So I think by no means
[00:57:48.680 --> 00:57:50.480]   do I have a healthy relationship with social media.
[00:57:50.480 --> 00:57:51.560]   I'm sure there's a way to,
[00:57:51.560 --> 00:57:54.800]   but I think I'm especially a weirdo in this space
[00:57:54.800 --> 00:57:58.040]   because it's easy to conflate, is this work?
[00:57:58.040 --> 00:57:58.880]   Is this not?
[00:57:58.880 --> 00:58:00.840]   I can always say that it's for work.
[00:58:00.840 --> 00:58:01.960]   - Right. - You know?
[00:58:01.960 --> 00:58:04.160]   - But I mean, don't you get the same kind of thing
[00:58:04.160 --> 00:58:08.080]   as you get from when a room full of people laugh at your jokes
[00:58:08.080 --> 00:58:11.200]   'cause I mean, I see, especially the way you do Twitter,
[00:58:11.200 --> 00:58:13.680]   it's an extension of your comedy in a way.
[00:58:13.680 --> 00:58:15.960]   - I took a big break from Twitter though,
[00:58:15.960 --> 00:58:16.800]   a really big break.
[00:58:16.800 --> 00:58:19.160]   I took like six months off or something for a while
[00:58:19.160 --> 00:58:20.480]   because it was just like,
[00:58:20.480 --> 00:58:22.240]   it seemed like it was all kind of politics
[00:58:22.240 --> 00:58:23.280]   and it was just a little bit,
[00:58:23.280 --> 00:58:25.000]   it wasn't giving me dopamine
[00:58:25.000 --> 00:58:28.280]   because there was like this weird, a lot of feedback.
[00:58:28.280 --> 00:58:30.720]   So I had to take a break from it and then go back to it
[00:58:30.720 --> 00:58:33.480]   'cause I felt like I didn't have a healthy relationship.
[00:58:33.480 --> 00:58:36.120]   - Have you ever tried the, I don't know if I believe him,
[00:58:36.120 --> 00:58:39.440]   but Joe Rogan seems to not read comments.
[00:58:39.440 --> 00:58:42.540]   Have you, and he's one of the only people at the scale,
[00:58:42.540 --> 00:58:47.340]   like at your level, who at least claims not to read.
[00:58:48.760 --> 00:58:53.760]   'Cause you and him swim in this space of tense ideas
[00:58:53.760 --> 00:58:58.720]   that get the toxic folks riled up.
[00:58:58.720 --> 00:59:01.160]   - I think Rogan, I don't know.
[00:59:01.160 --> 00:59:06.600]   I think he probably looks at YouTube, like the likes,
[00:59:06.600 --> 00:59:09.640]   and I think if something's, if he doesn't know,
[00:59:09.640 --> 00:59:12.700]   I don't know, I'm sure he would tell the truth.
[00:59:12.700 --> 00:59:15.840]   I'm sure he's got people that look at them
[00:59:15.840 --> 00:59:17.280]   and is like, "This guest did great,"
[00:59:17.280 --> 00:59:20.640]   or, "I don't," I'm sure he gets it.
[00:59:20.640 --> 00:59:23.240]   I can't picture him in the weeds on--
[00:59:23.240 --> 00:59:24.320]   - No, for sure.
[00:59:24.320 --> 00:59:26.020]   He's honest actually saying that.
[00:59:26.020 --> 00:59:29.680]   - Feedback, we're addicted to feedback.
[00:59:29.680 --> 00:59:30.600]   Yeah, we're addicted to feedback.
[00:59:30.600 --> 00:59:34.280]   I mean, look, I think that our brain is designed
[00:59:34.280 --> 00:59:37.760]   to get intel on how we're perceived
[00:59:37.760 --> 00:59:39.900]   so that we know where we stand, right?
[00:59:39.900 --> 00:59:41.320]   That's our whole deal, right?
[00:59:41.320 --> 00:59:43.080]   As humans, we wanna know where we stand.
[00:59:43.080 --> 00:59:44.160]   We walk into a room and we go,
[00:59:44.160 --> 00:59:45.480]   "Who's the most powerful person in here?
[00:59:45.480 --> 00:59:47.520]   "I gotta talk to 'em and get in their good graces."
[00:59:47.520 --> 00:59:49.960]   It's just we're designed to rank ourselves, right?
[00:59:49.960 --> 00:59:51.800]   And constantly know our rank.
[00:59:51.800 --> 00:59:55.900]   And social media, because you can't figure out your rank
[00:59:55.900 --> 00:59:59.600]   with 500 million people, it's impossible.
[00:59:59.600 --> 01:00:00.720]   So our brain is like, "What's my rank?
[01:00:00.720 --> 01:00:03.000]   "What's my," and especially if we're following people.
[01:00:03.000 --> 01:00:05.440]   I think the big, the interesting thing I think
[01:00:05.440 --> 01:00:07.840]   I maybe be able to say about this,
[01:00:07.840 --> 01:00:09.440]   besides my speech impediment,
[01:00:09.440 --> 01:00:13.120]   is that I did start muting people
[01:00:13.120 --> 01:00:16.160]   that rank wildly higher than me
[01:00:16.160 --> 01:00:19.040]   because it is just stressful on the brain
[01:00:19.040 --> 01:00:23.080]   to constantly look at people that are incredibly successful
[01:00:23.080 --> 01:00:25.320]   so you keep feeling bad about yourself.
[01:00:25.320 --> 01:00:28.640]   I think that that is cutting to a certain extent.
[01:00:28.640 --> 01:00:30.960]   Just like, look at me looking at all these people
[01:00:30.960 --> 01:00:32.080]   that have so much more money than me
[01:00:32.080 --> 01:00:33.800]   and so much more success than me.
[01:00:33.800 --> 01:00:35.880]   It's making me feel like a failure,
[01:00:35.880 --> 01:00:37.600]   even though I don't think I'm a failure,
[01:00:37.600 --> 01:00:41.920]   but it's easy to frame it so that I can feel that way.
[01:00:41.920 --> 01:00:43.320]   - Yeah, that's really interesting,
[01:00:43.320 --> 01:00:45.120]   especially if they're close to,
[01:00:45.120 --> 01:00:46.920]   like if they're other comedians or something like that.
[01:00:46.920 --> 01:00:48.920]   - That's right. - Or whatever.
[01:00:48.920 --> 01:00:50.480]   It's really disappointing to me.
[01:00:50.480 --> 01:00:51.760]   I do the same thing as well.
[01:00:51.760 --> 01:00:53.580]   So other successful people that are really close
[01:00:53.580 --> 01:00:56.400]   to what I do, I don't know.
[01:00:56.400 --> 01:00:58.240]   I wish I could just admire.
[01:00:58.240 --> 01:00:59.080]   - Yeah.
[01:00:59.080 --> 01:01:01.160]   - And for it not to be a distraction.
[01:01:01.160 --> 01:01:02.480]   - But that's why you are where you are
[01:01:02.480 --> 01:01:04.360]   'cause you don't just admire your competitive
[01:01:04.360 --> 01:01:05.280]   and you wanna win.
[01:01:05.280 --> 01:01:07.520]   So it's also the same thing that bums you out
[01:01:07.520 --> 01:01:08.840]   when you look at this is the same reason
[01:01:08.840 --> 01:01:09.680]   you are where you are.
[01:01:09.680 --> 01:01:11.520]   So that's why I think it's so important
[01:01:11.520 --> 01:01:12.840]   to learn about neurology and addiction
[01:01:12.840 --> 01:01:14.120]   'cause you're able to go like,
[01:01:14.120 --> 01:01:15.720]   oh, this same instinct.
[01:01:15.720 --> 01:01:18.840]   So I'm very sensitive and I sometimes don't like that
[01:01:18.840 --> 01:01:20.480]   about myself, but I'm like, well, that's the reason
[01:01:20.480 --> 01:01:22.440]   I'm able to write good standup.
[01:01:22.440 --> 01:01:25.680]   And that's the reason I'm able to be sensitive to feedback
[01:01:25.680 --> 01:01:26.920]   and go, that joke should have been better.
[01:01:26.920 --> 01:01:28.080]   I can make that better.
[01:01:28.080 --> 01:01:29.480]   So it's the kind of thing where it's like,
[01:01:29.480 --> 01:01:31.280]   you have to be really sensitive in your work
[01:01:31.280 --> 01:01:32.360]   and the second you leave,
[01:01:32.360 --> 01:01:33.580]   you gotta be able to turn it off.
[01:01:33.580 --> 01:01:34.840]   It's about developing the muscle,
[01:01:34.840 --> 01:01:38.360]   being able to know when to let it be a superpower
[01:01:38.360 --> 01:01:41.200]   and when it's gonna hold you back and be an obstacle.
[01:01:41.200 --> 01:01:43.080]   So I try to not be in that black and white
[01:01:43.080 --> 01:01:45.760]   of like, being competitive is bad
[01:01:45.760 --> 01:01:47.680]   or being jealous of someone just to go like,
[01:01:47.680 --> 01:01:50.200]   oh, there's that thing that makes me really successful
[01:01:50.200 --> 01:01:51.400]   in a lot of other ways,
[01:01:51.400 --> 01:01:53.240]   but right now it's making me feel bad.
[01:01:53.240 --> 01:01:54.920]   - Well, I'm kind of looking to you
[01:01:54.920 --> 01:01:58.000]   'cause you're basically a celebrity,
[01:01:58.000 --> 01:02:01.200]   the famous sort of world-class comedian.
[01:02:01.200 --> 01:02:03.080]   And so I feel like you're the right person
[01:02:03.080 --> 01:02:06.080]   to be one of the key people to define
[01:02:06.080 --> 01:02:08.680]   what's the healthy path forward with social media.
[01:02:08.680 --> 01:02:12.800]   'Cause we're all trying to figure it out now
[01:02:12.800 --> 01:02:16.200]   and I'm curious to see where it evolves.
[01:02:16.200 --> 01:02:17.960]   I think you're at the center of that.
[01:02:17.960 --> 01:02:21.640]   So like, there's trying to leave Twitter
[01:02:21.640 --> 01:02:22.800]   and then come back and say,
[01:02:22.800 --> 01:02:24.080]   can I do this in a healthy way?
[01:02:24.080 --> 01:02:25.920]   I mean, you have to keep trying, exploring and thinking.
[01:02:25.920 --> 01:02:28.120]   - You have to know because it's being,
[01:02:28.120 --> 01:02:29.720]   I have a couple answers.
[01:02:29.720 --> 01:02:31.600]   I think, I hire a company
[01:02:31.600 --> 01:02:33.920]   to do some of my social media for me.
[01:02:33.920 --> 01:02:35.960]   So it's also being able to go,
[01:02:35.960 --> 01:02:38.360]   okay, I make a certain amount of money by doing this,
[01:02:38.360 --> 01:02:40.360]   but now let me be a good business person
[01:02:40.360 --> 01:02:42.960]   and say, I'm gonna pay you this amount to run this for me.
[01:02:42.960 --> 01:02:45.920]   So I'm not 24/7 in the weeds, hashtagging and responding.
[01:02:45.920 --> 01:02:47.280]   And just, it's a lot to take on.
[01:02:47.280 --> 01:02:48.800]   It's a lot of energy to take on.
[01:02:48.800 --> 01:02:49.640]   But at the same time,
[01:02:49.640 --> 01:02:52.280]   part of what I think makes me successful
[01:02:52.280 --> 01:02:53.480]   on social media if I am,
[01:02:53.480 --> 01:02:55.280]   is that people know I'm actually doing it
[01:02:55.280 --> 01:02:57.200]   and that I am an engaging and I'm responding
[01:02:57.200 --> 01:02:59.600]   and developing a personal relationship
[01:02:59.600 --> 01:03:01.080]   with complete strangers.
[01:03:01.080 --> 01:03:04.000]   So I think, figuring out that balance
[01:03:04.000 --> 01:03:06.200]   and really approaching it as a business,
[01:03:06.200 --> 01:03:07.320]   that's what I try to do.
[01:03:07.320 --> 01:03:09.520]   It's not dating, it's not,
[01:03:09.520 --> 01:03:11.240]   I try to just be really objective about,
[01:03:11.240 --> 01:03:13.440]   okay, here's what's working, here's what's not working.
[01:03:13.440 --> 01:03:15.920]   And in terms of taking the break from Twitter,
[01:03:15.920 --> 01:03:17.680]   this is a really savage take,
[01:03:17.680 --> 01:03:21.760]   but because I don't talk about my politics publicly,
[01:03:21.760 --> 01:03:26.080]   being on Twitter right after the last election
[01:03:26.080 --> 01:03:27.960]   was not gonna be beneficial
[01:03:27.960 --> 01:03:30.320]   because there was gonna be, you had to take a side.
[01:03:30.320 --> 01:03:31.600]   You had to be political
[01:03:31.600 --> 01:03:34.440]   in order to get any kind of retweets or likes.
[01:03:34.440 --> 01:03:37.280]   And I just wasn't interested in doing that
[01:03:37.280 --> 01:03:38.720]   'cause you were gonna lose as many people
[01:03:38.720 --> 01:03:39.560]   as you were gonna gain
[01:03:39.560 --> 01:03:40.840]   and it was gonna all come clean in the wash.
[01:03:40.840 --> 01:03:41.840]   So I was just like,
[01:03:41.840 --> 01:03:44.360]   the best thing I can do for me business-wise
[01:03:44.360 --> 01:03:46.680]   is to just abstain.
[01:03:46.680 --> 01:03:52.240]   And the robot, I joke about her replacing me,
[01:03:52.240 --> 01:03:54.360]   but she does do half of my social media.
[01:03:54.360 --> 01:03:57.960]   'Cause I don't want people to get sick of me.
[01:03:57.960 --> 01:03:59.840]   I don't want to be redundant.
[01:03:59.840 --> 01:04:02.440]   There are times when I don't have the time or the energy
[01:04:02.440 --> 01:04:03.360]   to make a funny video,
[01:04:03.360 --> 01:04:06.160]   but I know she's gonna be compelling and interesting
[01:04:06.160 --> 01:04:08.520]   and that's something that you can't see every day.
[01:04:08.520 --> 01:04:11.920]   - Of course, the humor comes from your,
[01:04:11.920 --> 01:04:13.400]   I mean, the cleverness, the wit,
[01:04:13.400 --> 01:04:16.400]   the humor comes from you when you film the robot.
[01:04:16.400 --> 01:04:17.840]   That's kind of the trick of it.
[01:04:17.840 --> 01:04:21.000]   I mean, the robot is not quite there
[01:04:21.000 --> 01:04:23.440]   to do anything funny.
[01:04:23.440 --> 01:04:26.680]   The absurdity is revealed through the filmmaker in that case
[01:04:26.680 --> 01:04:27.840]   or whoever is interacting,
[01:04:27.840 --> 01:04:32.840]   not through the actual robot being who she is.
[01:04:32.840 --> 01:04:37.440]   Let me sort of, love, okay.
[01:04:37.440 --> 01:04:38.960]   (Bridget laughs)
[01:04:38.960 --> 01:04:40.800]   How difficult-- - What is it?
[01:04:40.800 --> 01:04:41.640]   - What is it?
[01:04:41.640 --> 01:04:45.080]   Well, first an engineering question.
[01:04:45.080 --> 01:04:48.080]   I know, I know, you're not an engineer,
[01:04:48.080 --> 01:04:52.160]   but how difficult do you think is it to build an AI system
[01:04:52.160 --> 01:04:53.760]   that you can have a deep,
[01:04:53.760 --> 01:04:56.480]   fulfilling, monogamous relationship with?
[01:04:56.480 --> 01:04:59.880]   Sort of replace the human-to-human relationships
[01:04:59.880 --> 01:05:00.800]   that we value?
[01:05:00.800 --> 01:05:04.280]   - I think anyone can fall in love with anything.
[01:05:04.280 --> 01:05:08.340]   You know, like how often have you looked back at someone,
[01:05:08.340 --> 01:05:10.480]   like I ran into someone the other day
[01:05:10.480 --> 01:05:12.640]   that I was in love with and I was like,
[01:05:12.640 --> 01:05:16.320]   "Hey," it was like, there was nothing there.
[01:05:16.320 --> 01:05:17.880]   There was nothing there.
[01:05:17.880 --> 01:05:19.720]   Like, you know, like where you're able to go like,
[01:05:19.720 --> 01:05:20.880]   "Oh, that was weird.
[01:05:20.880 --> 01:05:22.480]   "Oh, right."
[01:05:22.480 --> 01:05:25.080]   You know, I were able--
[01:05:25.080 --> 01:05:26.320]   - You mean from a distant past
[01:05:26.320 --> 01:05:27.880]   or something like that? - Yeah.
[01:05:27.880 --> 01:05:28.720]   When you're able to go like,
[01:05:28.720 --> 01:05:31.440]   I can't believe we had an incredible connection
[01:05:31.440 --> 01:05:32.960]   and now it's just,
[01:05:32.960 --> 01:05:37.320]   I do think that people will be in love with robots,
[01:05:37.320 --> 01:05:39.820]   probably even more deeply with humans,
[01:05:39.820 --> 01:05:42.520]   because it's like when people mourn their animals,
[01:05:42.520 --> 01:05:44.040]   when their animals die,
[01:05:44.040 --> 01:05:47.760]   they're always, it's sometimes harder than mourning a human
[01:05:47.760 --> 01:05:50.360]   because you can't go, "Well, he was kind of an asshole."
[01:05:50.360 --> 01:05:52.000]   But like, "He didn't pick me up from school."
[01:05:52.000 --> 01:05:53.880]   You know, it's like you're able to get out of your grief
[01:05:53.880 --> 01:05:54.720]   a little bit.
[01:05:54.720 --> 01:05:56.200]   You're able to kind of be,
[01:05:56.200 --> 01:05:57.440]   "Oh, he was kind of judgmental."
[01:05:57.440 --> 01:05:59.440]   Or, "She was kind of," you know.
[01:05:59.440 --> 01:06:02.080]   With a robot, there's something so pure about,
[01:06:02.080 --> 01:06:05.560]   and innocent, and impish, and childlike about it
[01:06:05.560 --> 01:06:09.660]   that I think it probably will be much more conducive
[01:06:09.660 --> 01:06:12.520]   to a narcissistic love, for sure, at that.
[01:06:12.520 --> 01:06:15.240]   But it's not like, "Well, he cheated.
[01:06:15.240 --> 01:06:16.080]   "She can't cheat.
[01:06:16.080 --> 01:06:16.900]   "She can't leave you.
[01:06:16.900 --> 01:06:17.960]   "She can't," you know.
[01:06:17.960 --> 01:06:21.560]   - Well, if a bear claw leaves your life
[01:06:21.560 --> 01:06:23.640]   and maybe a new version
[01:06:23.640 --> 01:06:25.680]   or somebody else will enter,
[01:06:25.680 --> 01:06:27.960]   will you miss a bear claw?
[01:06:27.960 --> 01:06:30.680]   - For guys that have these sex robots,
[01:06:30.680 --> 01:06:34.400]   they're building a nursing home for the bodies
[01:06:34.400 --> 01:06:36.360]   that are now rusting
[01:06:36.360 --> 01:06:37.960]   'cause they don't wanna part with the bodies
[01:06:37.960 --> 01:06:40.880]   'cause they have such an intense emotional connection to it.
[01:06:40.880 --> 01:06:42.880]   I mean, it's kind of like a car club, a little bit.
[01:06:42.880 --> 01:06:45.000]   You know, like it's, you know.
[01:06:45.000 --> 01:06:47.400]   But I'm not saying this is right.
[01:06:47.400 --> 01:06:50.040]   I'm not saying it's cool, it's weird, it's creepy,
[01:06:50.040 --> 01:06:53.840]   but we do anthropomorphize things with faces
[01:06:53.840 --> 01:06:56.680]   and we do develop emotional connections to things.
[01:06:56.680 --> 01:06:58.080]   I mean, there's certain,
[01:06:58.080 --> 01:06:59.360]   have you ever tried to like throw away,
[01:06:59.360 --> 01:07:01.840]   I can't even throw away my teddy bear from when I was a kid.
[01:07:01.840 --> 01:07:04.360]   It's a piece of trash and it's upstairs.
[01:07:04.360 --> 01:07:06.700]   Like, it's just like, why can't I throw that away?
[01:07:06.700 --> 01:07:07.980]   It's bizarre.
[01:07:07.980 --> 01:07:10.160]   You know, and there's something kind of beautiful about that.
[01:07:10.160 --> 01:07:13.160]   There's something, it gives me hope in humans
[01:07:13.160 --> 01:07:15.760]   'cause I see humans do such horrific things all the time
[01:07:15.760 --> 01:07:18.380]   and maybe I'm too, I see too much of it, frankly,
[01:07:18.380 --> 01:07:20.280]   but there's something kind of beautiful
[01:07:20.280 --> 01:07:24.400]   about the way we're able to have emotional connections
[01:07:24.400 --> 01:07:29.240]   to objects, which, you know, a lot of,
[01:07:29.240 --> 01:07:32.200]   I mean, it's kind of specifically, I think, Western, right?
[01:07:32.200 --> 01:07:34.920]   That we don't see objects as having souls.
[01:07:34.920 --> 01:07:36.840]   Like, that's kind of specifically us.
[01:07:36.840 --> 01:07:39.760]   But I don't think it's so much
[01:07:39.760 --> 01:07:43.460]   that we're objectifying humans with these sex robots.
[01:07:43.460 --> 01:07:45.700]   We're kind of humanizing objects, right?
[01:07:45.700 --> 01:07:47.120]   So there's something kind of fascinating
[01:07:47.120 --> 01:07:48.180]   in our ability to do that.
[01:07:48.180 --> 01:07:50.080]   'Cause a lot of us don't humanize humans.
[01:07:50.080 --> 01:07:52.880]   So it's just a weird little place to play in.
[01:07:52.880 --> 01:07:54.980]   And I think a lot of people, I mean,
[01:07:54.980 --> 01:07:57.760]   a lot of people will be marrying these things is my guess.
[01:07:57.760 --> 01:07:59.640]   - So you've asked the question.
[01:07:59.640 --> 01:08:00.640]   Let me ask it of you.
[01:08:00.640 --> 01:08:01.900]   So what is love?
[01:08:01.900 --> 01:08:05.720]   You have a bit of a brilliant definition of love
[01:08:05.720 --> 01:08:07.860]   as being willing to die for someone
[01:08:07.860 --> 01:08:10.600]   who you yourself want to kill.
[01:08:10.600 --> 01:08:12.240]   So that's kind of fun.
[01:08:12.240 --> 01:08:13.960]   First of all, that's brilliant.
[01:08:13.960 --> 01:08:16.480]   That's a really good definition.
[01:08:16.480 --> 01:08:18.080]   I don't think it'll stick with me for a long time.
[01:08:18.080 --> 01:08:19.880]   - This is how little of a romantic I am.
[01:08:19.880 --> 01:08:21.440]   A plane went by when you said that.
[01:08:21.440 --> 01:08:24.960]   And my brain is like, you're gonna need to rerecord that.
[01:08:24.960 --> 01:08:26.440]   I don't want you to get into post
[01:08:26.440 --> 01:08:28.040]   and then not be able to use it.
[01:08:28.040 --> 01:08:30.300]   (laughing)
[01:08:30.300 --> 01:08:32.000]   - And I'm a romantic 'cause I--
[01:08:32.000 --> 01:08:33.680]   - Don't mean to ruin the moment.
[01:08:33.680 --> 01:08:35.680]   - Actually, I can not be conscious of the fact
[01:08:35.680 --> 01:08:38.280]   that I heard the plane and it made me feel like
[01:08:38.280 --> 01:08:41.160]   how amazing it is that we live in a world with planes.
[01:08:41.160 --> 01:08:43.360]   (laughing)
[01:08:43.360 --> 01:08:46.160]   - And I just went, why haven't we fucking evolved
[01:08:46.160 --> 01:08:49.120]   past planes and why can't they make them quieter?
[01:08:49.120 --> 01:08:49.960]   - Yeah.
[01:08:49.960 --> 01:08:50.780]   (laughing)
[01:08:50.780 --> 01:08:51.620]   - But yes.
[01:08:51.620 --> 01:08:52.460]   - This--
[01:08:52.460 --> 01:08:54.480]   - My definition of love?
[01:08:54.480 --> 01:08:56.760]   - What, yeah, what's your--
[01:08:56.760 --> 01:08:59.960]   - Consistently producing dopamine for a long time.
[01:08:59.960 --> 01:09:01.440]   (laughing)
[01:09:01.440 --> 01:09:05.140]   Consistent output of oxytocin with the same person.
[01:09:05.140 --> 01:09:08.280]   - Dopamine is a positive thing.
[01:09:08.280 --> 01:09:09.560]   What about the negative?
[01:09:09.560 --> 01:09:12.000]   What about the fear and the insecurity?
[01:09:12.000 --> 01:09:16.520]   The longing, anger, all that kind of stuff?
[01:09:16.520 --> 01:09:17.840]   - I think that's part of love.
[01:09:17.840 --> 01:09:22.000]   I think that love brings out the best in you
[01:09:22.000 --> 01:09:24.040]   but it also, if you don't get angry and upset,
[01:09:24.040 --> 01:09:26.880]   it's, I don't know, I think that that's part of it.
[01:09:26.880 --> 01:09:29.680]   I think we have this idea that love has to be really
[01:09:29.680 --> 01:09:31.060]   placid or something.
[01:09:31.060 --> 01:09:34.160]   I only saw stormy relationships growing up
[01:09:34.160 --> 01:09:38.120]   so I don't have a judgment on how a relationship
[01:09:38.120 --> 01:09:42.360]   should look but I do think that this idea
[01:09:42.360 --> 01:09:47.360]   that love has to be eternal is really destructive,
[01:09:47.360 --> 01:09:50.780]   is really destructive and self-defeating
[01:09:50.780 --> 01:09:53.680]   and a big source of stress for people.
[01:09:53.680 --> 01:09:55.640]   I mean, I'm still figuring out love.
[01:09:55.640 --> 01:09:58.040]   I think we all kind of are but I do kind of
[01:09:58.040 --> 01:09:59.360]   stand by that definition.
[01:09:59.360 --> 01:10:04.160]   And I think that, I think for me, love is like
[01:10:04.160 --> 01:10:06.260]   just being able to be authentic with somebody.
[01:10:06.260 --> 01:10:08.560]   It's very simple, I know, but I think for me
[01:10:08.560 --> 01:10:11.060]   it's about not feeling pressure to have to perform
[01:10:11.060 --> 01:10:14.800]   or impress somebody, just feeling truly like
[01:10:14.800 --> 01:10:16.540]   accepted unconditionally by someone.
[01:10:16.540 --> 01:10:19.160]   Although I do believe love should be conditional.
[01:10:19.160 --> 01:10:22.860]   That might be a hot take.
[01:10:22.860 --> 01:10:24.280]   I think everything should be conditional.
[01:10:24.280 --> 01:10:28.060]   I think if someone's behavior, I don't think love
[01:10:28.060 --> 01:10:29.520]   should just be like, I'm in love with you,
[01:10:29.520 --> 01:10:30.960]   now behave however you want forever.
[01:10:30.960 --> 01:10:31.840]   This is unconditional.
[01:10:31.840 --> 01:10:35.320]   I think love is a daily action.
[01:10:35.320 --> 01:10:38.120]   It's not something you just like get tenure on
[01:10:38.120 --> 01:10:40.020]   and then get to behave however you want
[01:10:40.020 --> 01:10:41.900]   'cause we said I love you 10 years ago.
[01:10:41.900 --> 01:10:44.580]   It's a daily, it's a verb.
[01:10:44.580 --> 01:10:46.140]   - Well, there's some things that are,
[01:10:46.140 --> 01:10:49.080]   you see, if you explicitly make it clear
[01:10:49.080 --> 01:10:51.460]   that it's conditional, it takes away
[01:10:51.460 --> 01:10:52.520]   some of the magic of it.
[01:10:52.520 --> 01:10:55.360]   So there's certain stories we tell ourselves
[01:10:55.360 --> 01:10:57.200]   that we don't wanna make explicit about love.
[01:10:57.200 --> 01:10:59.160]   I don't know, maybe that's the wrong way to think of it.
[01:10:59.160 --> 01:11:02.680]   Maybe you wanna be explicit in relationships.
[01:11:02.680 --> 01:11:04.640]   - I also think love is a business decision.
[01:11:04.640 --> 01:11:07.980]   Like I do in a good way.
[01:11:07.980 --> 01:11:11.160]   Like I think that love is not just
[01:11:11.160 --> 01:11:12.580]   when you're across from somebody.
[01:11:12.580 --> 01:11:15.360]   It's when I go to work, can I focus?
[01:11:15.360 --> 01:11:16.200]   Do I, am I worried about you?
[01:11:16.200 --> 01:11:17.440]   Am I stressed out about you?
[01:11:17.440 --> 01:11:20.420]   Am I, you're not responding to me, you're not reliable.
[01:11:20.420 --> 01:11:23.180]   Like I think that being in a relationship,
[01:11:23.180 --> 01:11:24.280]   the kind of love that I would want
[01:11:24.280 --> 01:11:26.920]   is the kind of relationship where when we're not together,
[01:11:26.920 --> 01:11:30.320]   it's not draining me, causing me stress, making me worry,
[01:11:30.320 --> 01:11:33.160]   you know, and sometimes passion, that word.
[01:11:33.400 --> 01:11:36.480]   You know, we get murky about it,
[01:11:36.480 --> 01:11:38.200]   but I think it's also like I can be the best version
[01:11:38.200 --> 01:11:40.080]   of myself when the person's not around
[01:11:40.080 --> 01:11:42.680]   and I don't have to feel abandoned or scared
[01:11:42.680 --> 01:11:43.960]   or any of these kind of other things.
[01:11:43.960 --> 01:11:47.040]   So it's like love, you know, for me, I think is,
[01:11:47.040 --> 01:11:49.800]   I think it's a Flaubert quote and I'm gonna butcher it,
[01:11:49.800 --> 01:11:51.920]   but I think it's like be, you know,
[01:11:51.920 --> 01:11:54.240]   boring in your personal life so you can be violent
[01:11:54.240 --> 01:11:55.780]   and take risks in your professional life.
[01:11:55.780 --> 01:11:56.620]   Is that it?
[01:11:56.620 --> 01:11:57.480]   I got it wrong.
[01:11:57.480 --> 01:12:00.000]   Something like that, but I do think that
[01:12:00.000 --> 01:12:01.680]   it's being able to align values in a way
[01:12:01.680 --> 01:12:04.680]   to where you can also thrive outside of the relationship.
[01:12:04.680 --> 01:12:06.200]   - Some of the most successful people I know
[01:12:06.200 --> 01:12:10.120]   are those sort of happily married and have kids and so on.
[01:12:10.120 --> 01:12:10.960]   It's always funny--
[01:12:10.960 --> 01:12:11.880]   - It can be boring.
[01:12:11.880 --> 01:12:13.200]   Boring's okay.
[01:12:13.200 --> 01:12:14.480]   Boring is serenity.
[01:12:14.480 --> 01:12:16.500]   - And it's funny how those elements
[01:12:16.500 --> 01:12:18.380]   actually make you much more productive.
[01:12:18.380 --> 01:12:19.700]   I don't understand the--
[01:12:19.700 --> 01:12:21.160]   - I don't think relationships should drain you
[01:12:21.160 --> 01:12:23.440]   and take away energy that you could be using
[01:12:23.440 --> 01:12:25.800]   to create things that generate pride.
[01:12:25.800 --> 01:12:26.640]   - Okay.
[01:12:26.640 --> 01:12:28.240]   - Did you say your relationship of love yet?
[01:12:28.240 --> 01:12:29.080]   - Huh?
[01:12:29.080 --> 01:12:31.400]   - Have you said your definition of love?
[01:12:31.400 --> 01:12:32.640]   - My definition of love?
[01:12:32.640 --> 01:12:34.760]   No, I did not say it.
[01:12:34.760 --> 01:12:35.600]   (Bridget laughs)
[01:12:35.600 --> 01:12:36.760]   We're out of time.
[01:12:36.760 --> 01:12:37.600]   - No!
[01:12:37.600 --> 01:12:41.800]   - When you have a podcast, maybe you can invite me on.
[01:12:41.800 --> 01:12:42.640]   - Oh no, I already did.
[01:12:42.640 --> 01:12:44.040]   You're doing it.
[01:12:44.040 --> 01:12:46.400]   We've already talked about this.
[01:12:46.400 --> 01:12:49.480]   - And because I also have codependency, I had to say yes.
[01:12:49.480 --> 01:12:52.120]   - No, yeah, yeah, no, no, I'm trapping you.
[01:12:52.120 --> 01:12:53.080]   You owe me now.
[01:12:53.080 --> 01:12:58.080]   - Actually, I wondered whether when I asked
[01:12:58.280 --> 01:13:01.720]   if we could talk today, after sort of doing more research
[01:13:01.720 --> 01:13:04.640]   and reading some of your book, I started to wonder,
[01:13:04.640 --> 01:13:07.040]   did you just feel pressured to say yes?
[01:13:07.040 --> 01:13:09.320]   - Yes, of course.
[01:13:09.320 --> 01:13:10.160]   - Good.
[01:13:10.160 --> 01:13:11.000]   - But I'm a fan of yours too.
[01:13:11.000 --> 01:13:11.820]   - Okay, awesome.
[01:13:11.820 --> 01:13:13.400]   - No, I actually, because I am codependent,
[01:13:13.400 --> 01:13:14.920]   but I'm in recovery for codependence,
[01:13:14.920 --> 01:13:17.640]   so I actually don't do anything I don't wanna do.
[01:13:17.640 --> 01:13:20.400]   - You really, you go out of your way to say no.
[01:13:20.400 --> 01:13:21.240]   - What's that?
[01:13:21.240 --> 01:13:22.720]   I say no all the time.
[01:13:22.720 --> 01:13:23.560]   - Good, I'm trying to learn that as well.
[01:13:23.560 --> 01:13:26.040]   - I moved this, remember, I moved it from one to two.
[01:13:26.040 --> 01:13:26.880]   - Yeah, yeah.
[01:13:26.880 --> 01:13:27.720]   - I--
[01:13:27.720 --> 01:13:30.320]   - Yeah, just to let you know how recovered I am.
[01:13:30.320 --> 01:13:33.960]   I'm not codependent, but I don't do anything
[01:13:33.960 --> 01:13:34.800]   I don't wanna do.
[01:13:34.800 --> 01:13:36.960]   - Yeah, you're ahead of me on that, okay.
[01:13:36.960 --> 01:13:37.800]   So do you--
[01:13:37.800 --> 01:13:38.880]   - You're like, I don't even wanna be here.
[01:13:38.880 --> 01:13:40.600]   (laughing)
[01:13:40.600 --> 01:13:43.480]   - Do you think about your mortality?
[01:13:43.480 --> 01:13:47.000]   - Yes, it is a big part of how I was able
[01:13:47.000 --> 01:13:49.120]   to sort of kickstart my codependence recovery.
[01:13:49.120 --> 01:13:50.480]   My dad passed a couple years ago,
[01:13:50.480 --> 01:13:53.120]   and when you have someone close to you in your life die,
[01:13:53.120 --> 01:13:56.540]   everything gets real clear in terms of
[01:13:56.540 --> 01:13:58.360]   how we're a speck of dust who's only here
[01:13:58.360 --> 01:14:00.880]   for a certain amount of time.
[01:14:00.880 --> 01:14:02.360]   - What do you think is the meaning of it all?
[01:14:02.360 --> 01:14:05.200]   Like what, the speck of dust,
[01:14:05.200 --> 01:14:08.080]   what's maybe in your own life,
[01:14:08.080 --> 01:14:13.080]   what's the goal, the purpose of your existence?
[01:14:13.080 --> 01:14:15.280]   - Is there one?
[01:14:15.280 --> 01:14:17.320]   - Well, you're exceptionally ambitious,
[01:14:17.320 --> 01:14:19.120]   you've created some incredible things
[01:14:19.120 --> 01:14:21.680]   in different disciplines.
[01:14:21.680 --> 01:14:23.560]   - Yeah, it's we're all just managing our terror
[01:14:23.560 --> 01:14:24.560]   'cause we know we're gonna die.
[01:14:24.560 --> 01:14:26.360]   So we create and build all these things
[01:14:26.360 --> 01:14:29.460]   and rituals and religions and robots
[01:14:29.460 --> 01:14:31.800]   and whatever we need to do to just distract ourselves
[01:14:31.800 --> 01:14:35.240]   from imminent rotting.
[01:14:35.240 --> 01:14:37.140]   We're rotting, we're all dying.
[01:14:37.140 --> 01:14:42.540]   I got very into terror management theory
[01:14:42.540 --> 01:14:45.120]   when my dad died and it resonated, it helped me,
[01:14:45.120 --> 01:14:46.580]   and everyone's got their own religion
[01:14:46.580 --> 01:14:50.280]   or sense of purpose or thing that distracts them
[01:14:50.280 --> 01:14:53.360]   from the horrors of being human.
[01:14:53.360 --> 01:14:56.080]   - What's the terror management theory?
[01:14:56.080 --> 01:14:57.360]   - Terror management is basically the idea
[01:14:57.360 --> 01:14:58.680]   that since we're the only animal
[01:14:58.680 --> 01:15:00.360]   that knows they're gonna die,
[01:15:00.360 --> 01:15:03.520]   we have to basically distract ourselves
[01:15:03.520 --> 01:15:08.520]   with awards and achievements and games and whatever
[01:15:08.520 --> 01:15:11.960]   just in order to distract ourselves
[01:15:11.960 --> 01:15:14.600]   from the terror we would feel if we really processed
[01:15:14.600 --> 01:15:16.960]   the fact that we could not only, we are gonna die,
[01:15:16.960 --> 01:15:18.480]   but also could die at any minute
[01:15:18.480 --> 01:15:19.820]   because we're only superficially
[01:15:19.820 --> 01:15:21.280]   at the top of the food chain.
[01:15:22.760 --> 01:15:26.160]   And technically we're at the top of the food chain
[01:15:26.160 --> 01:15:29.320]   if we have houses and guns and stuff, machines,
[01:15:29.320 --> 01:15:32.520]   but if me and a lion are in the woods together,
[01:15:32.520 --> 01:15:33.800]   most things could kill us.
[01:15:33.800 --> 01:15:35.720]   I mean, a bee can kill some people.
[01:15:35.720 --> 01:15:38.680]   Something this big can kill a lot of humans.
[01:15:38.680 --> 01:15:41.480]   So it's basically just to manage the terror
[01:15:41.480 --> 01:15:45.200]   that we all would feel if we were able to really be awake.
[01:15:45.200 --> 01:15:46.860]   'Cause we're mostly zombies, right?
[01:15:46.860 --> 01:15:50.400]   Job, school, religion, go to sleep, drink,
[01:15:50.400 --> 01:15:54.480]   football, relationship, dopamine, love, you know,
[01:15:54.480 --> 01:15:57.000]   we're kind of just like trudging along
[01:15:57.000 --> 01:15:58.480]   like zombies for the most part.
[01:15:58.480 --> 01:15:59.800]   And then I think--
[01:15:59.800 --> 01:16:02.360]   - That fear of death adds some motivation.
[01:16:02.360 --> 01:16:03.460]   - Yes.
[01:16:03.460 --> 01:16:06.080]   - Well, I think I speak for a lot of people
[01:16:06.080 --> 01:16:10.440]   in saying that I can't wait to see what your terror creates
[01:16:10.440 --> 01:16:11.880]   (laughing)
[01:16:11.880 --> 01:16:13.640]   in the next few years.
[01:16:13.640 --> 01:16:14.840]   I'm a huge fan.
[01:16:14.840 --> 01:16:16.560]   Whitney, thank you so much for talking today.
[01:16:16.560 --> 01:16:17.400]   - Thanks.
[01:16:18.840 --> 01:16:20.520]   - Thanks for listening to this conversation
[01:16:20.520 --> 01:16:21.880]   with Whitney Cummings.
[01:16:21.880 --> 01:16:24.680]   And thank you to our presenting sponsor, Cash App.
[01:16:24.680 --> 01:16:27.400]   Download it and use code LEXPODCAST.
[01:16:27.400 --> 01:16:30.160]   You'll get $10 and $10 will go to FIRST,
[01:16:30.160 --> 01:16:33.120]   a STEM education nonprofit that inspires hundreds
[01:16:33.120 --> 01:16:35.440]   of thousands of young minds to learn
[01:16:35.440 --> 01:16:38.000]   and to dream of engineering our future.
[01:16:38.000 --> 01:16:40.800]   If you enjoy this podcast, subscribe on YouTube,
[01:16:40.800 --> 01:16:42.720]   give it five stars on Apple Podcast,
[01:16:42.720 --> 01:16:46.080]   support on Patreon, or connect with me on Twitter.
[01:16:46.080 --> 01:16:49.200]   Thank you for listening and hope to see you next time.
[01:16:49.200 --> 01:16:51.780]   (upbeat music)
[01:16:51.780 --> 01:16:54.360]   (upbeat music)
[01:16:54.360 --> 01:17:04.360]   [BLANK_AUDIO]

