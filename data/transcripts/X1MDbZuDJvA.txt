
[00:00:00.000 --> 00:00:02.560]   So it's now time in the world of wonderful things
[00:00:02.560 --> 00:00:05.240]   to introduce our wonderful next guest,
[00:00:05.240 --> 00:00:10.240]   Rock Novosel, who is joining us from Slovenia,
[00:00:10.240 --> 00:00:16.520]   from Ljubljana, where he works as a full stack engineer,
[00:00:16.520 --> 00:00:21.280]   primarily not on machine learning and deep learning,
[00:00:21.280 --> 00:00:24.440]   but he has built a really cool tool
[00:00:24.440 --> 00:00:28.080]   as part of the CodeSearchNet challenge
[00:00:28.080 --> 00:00:30.640]   that Weights and Biases did with GitHub.
[00:00:30.640 --> 00:00:33.760]   And it's a really impressive project.
[00:00:33.760 --> 00:00:35.360]   It's got some really clear uses,
[00:00:35.360 --> 00:00:40.360]   and I'm really excited to hear from him about that.
[00:00:40.360 --> 00:00:42.540]   So Rock, go ahead and take it away.
[00:00:42.540 --> 00:00:46.880]   - Okay, thank you for the introduction.
[00:00:46.880 --> 00:00:48.200]   Hi, my name is Rock,
[00:00:48.200 --> 00:00:51.240]   and I would like to present CodeSnippet Search.
[00:00:51.240 --> 00:00:54.960]   It is a web application and a web extension
[00:00:54.960 --> 00:00:57.400]   that allows you to search GitHub repositories
[00:00:57.400 --> 00:01:01.680]   using actual language queries and code itself, actually.
[00:01:01.680 --> 00:01:04.920]   It uses PyTorch to power deep neural networks,
[00:01:04.920 --> 00:01:07.600]   which embed actual language queries
[00:01:07.600 --> 00:01:10.040]   and code snippets into vectors.
[00:01:10.040 --> 00:01:12.040]   So using nearest neighbor search,
[00:01:12.040 --> 00:01:13.840]   we can determine which code snippets
[00:01:13.840 --> 00:01:17.680]   are most similar to a given query or another code snippet.
[00:01:17.680 --> 00:01:21.560]   So why neural code search?
[00:01:21.560 --> 00:01:23.440]   Code search, for me at least,
[00:01:23.440 --> 00:01:26.720]   is one of the most important tools during coding.
[00:01:26.720 --> 00:01:29.640]   I noticed that searching and navigating through code
[00:01:29.640 --> 00:01:32.680]   heavily outweighs the actual coding.
[00:01:32.680 --> 00:01:35.640]   So when I'm already familiar with the code base,
[00:01:35.640 --> 00:01:38.640]   I find that exact search is what I need.
[00:01:38.640 --> 00:01:40.480]   For example, regular expressions
[00:01:40.480 --> 00:01:42.440]   and case-insensitive search.
[00:01:42.440 --> 00:01:45.560]   That is because I'm already familiar with the naming scheme,
[00:01:45.560 --> 00:01:49.560]   and I can reasonably predict how to formulate a search query
[00:01:49.560 --> 00:01:52.160]   and basically every IDE and code editor
[00:01:52.160 --> 00:01:54.460]   is already equipped with it.
[00:01:54.460 --> 00:01:56.920]   But when I'm not familiar with the code base
[00:01:56.920 --> 00:02:00.280]   or even a new folder within an existing large project,
[00:02:00.280 --> 00:02:03.720]   for example, it makes searching more difficult
[00:02:03.720 --> 00:02:05.680]   because it's more trial and error.
[00:02:05.680 --> 00:02:07.680]   A tool like Code Snippet Search
[00:02:07.680 --> 00:02:11.320]   would allow me to easily explore unfamiliar code,
[00:02:11.320 --> 00:02:13.280]   focusing on the semantics
[00:02:13.280 --> 00:02:16.280]   without getting bogged down in the syntax.
[00:02:16.280 --> 00:02:18.080]   For example, this is especially useful
[00:02:18.080 --> 00:02:21.320]   when onboarding a new developer onto a project
[00:02:21.320 --> 00:02:25.100]   because it can be a significant boost to their productivity.
[00:02:25.100 --> 00:02:28.380]   And here are listed some example queries
[00:02:28.380 --> 00:02:30.760]   a new developer could potentially enter.
[00:02:30.760 --> 00:02:32.980]   So if you're a new developer
[00:02:32.980 --> 00:02:37.180]   and you're working on a product that's using shipping
[00:02:37.180 --> 00:02:41.660]   or products or authenticating user or generating QR code,
[00:02:41.660 --> 00:02:44.080]   so you don't exactly know
[00:02:44.080 --> 00:02:47.140]   how the authentication is implemented,
[00:02:47.140 --> 00:02:49.500]   is it a single method or is it in a class?
[00:02:49.500 --> 00:02:52.140]   So basically instead of trial and error,
[00:02:52.140 --> 00:02:54.860]   you just type in authenticate user
[00:02:54.860 --> 00:02:57.900]   and it would return either the function
[00:02:57.900 --> 00:03:01.920]   that implements it or the class or some kind of service.
[00:03:01.920 --> 00:03:06.820]   So that's where I see the potential behind these methods
[00:03:06.820 --> 00:03:10.260]   or behind the Code Snippet Search.
[00:03:10.260 --> 00:03:13.020]   So outside of a work environment,
[00:03:13.020 --> 00:03:15.140]   we encounter unfamiliar code
[00:03:15.140 --> 00:03:18.920]   in the form of GitHub repositories basically every day.
[00:03:18.920 --> 00:03:22.020]   So semantic search tools would provide the faster way
[00:03:22.020 --> 00:03:25.260]   for users to find answers to their issues
[00:03:25.260 --> 00:03:26.740]   directly in the code.
[00:03:26.740 --> 00:03:29.640]   Consequently, it could lessen the burden
[00:03:29.640 --> 00:03:32.220]   on maintainers to provide these answers.
[00:03:32.220 --> 00:03:35.260]   That's why I think it's important for GitHub
[00:03:35.260 --> 00:03:39.440]   to explore this option and potentially maybe one day
[00:03:39.440 --> 00:03:43.460]   even implement it in their product.
[00:03:43.460 --> 00:03:47.020]   So as mentioned, the main data source
[00:03:47.020 --> 00:03:50.700]   and inspiration for Code Snippet Search
[00:03:50.700 --> 00:03:53.040]   is GitHub's Code Search Net project.
[00:03:53.040 --> 00:03:54.800]   The Code Search Net corpus
[00:03:54.800 --> 00:03:57.580]   contains approximately 6 million functions
[00:03:57.580 --> 00:03:59.780]   from six programming languages.
[00:03:59.780 --> 00:04:04.300]   These are Go, Python, PHP, Java, Ruby, and JavaScript,
[00:04:04.300 --> 00:04:06.820]   where 2 million functions are annotated
[00:04:06.820 --> 00:04:09.060]   with a docstring or a com.
[00:04:09.060 --> 00:04:11.760]   So Code Search Net along with Weights and Biases
[00:04:11.760 --> 00:04:13.980]   also hosts a challenge where the goal
[00:04:13.980 --> 00:04:16.820]   is to answer 99 queries as best as possible
[00:04:16.820 --> 00:04:18.980]   with functions from the corpus.
[00:04:18.980 --> 00:04:22.540]   So this challenge is what got me into neural code search
[00:04:22.540 --> 00:04:25.960]   in the first place and also deep learning in general.
[00:04:25.960 --> 00:04:30.100]   Code Search Net also provides various baseline
[00:04:30.100 --> 00:04:33.860]   implementations of neural code search in TensorFlow.
[00:04:33.860 --> 00:04:37.100]   And my implementation for Code Snippet Search
[00:04:37.100 --> 00:04:41.000]   was inspired by their neural bag of words implementation
[00:04:41.000 --> 00:04:43.340]   because it was performing the best overall
[00:04:43.340 --> 00:04:45.500]   in the published results.
[00:04:45.500 --> 00:04:49.420]   So initially I had written Code Snippet Search in Keras
[00:04:49.420 --> 00:04:52.960]   and it was able to search for the Code Search Net corpus,
[00:04:52.960 --> 00:04:55.340]   but due to difficulties when developing
[00:04:55.340 --> 00:04:56.740]   and deploying the models,
[00:04:56.740 --> 00:04:59.140]   I decided to switch to PyTorch
[00:04:59.140 --> 00:05:00.940]   when I wanted to add support
[00:05:00.940 --> 00:05:03.900]   for searching GitHub repositories.
[00:05:03.900 --> 00:05:11.180]   Okay, now since I think it's a good time
[00:05:11.180 --> 00:05:12.860]   to kind of show you the demo.
[00:05:12.860 --> 00:05:17.860]   So this is the demo of totally not cherry pick examples
[00:05:17.860 --> 00:05:23.620]   I wanted to show you.
[00:05:23.620 --> 00:05:26.380]   So this is the Code Snippet Search application,
[00:05:26.380 --> 00:05:28.940]   which is available at codesnippetsearch.net.
[00:05:28.940 --> 00:05:32.700]   Currently I support 31 code repositories
[00:05:32.700 --> 00:05:37.060]   because the instance I'm hosting on it is not that large.
[00:05:37.060 --> 00:05:40.100]   So that's kind of the limit of what I can get it on.
[00:05:41.060 --> 00:05:42.940]   So on the first page,
[00:05:42.940 --> 00:05:45.860]   we see the list of supported repositories,
[00:05:45.860 --> 00:05:48.940]   their support languages and their descriptions.
[00:05:48.940 --> 00:05:52.100]   So for the example, I decided to use Django
[00:05:52.100 --> 00:05:57.100]   because Django powers the backend of this application.
[00:05:57.100 --> 00:06:02.420]   So for the first example, let's try something simple,
[00:06:02.420 --> 00:06:07.140]   for example, sendMail, which of course returns back
[00:06:07.140 --> 00:06:11.980]   a couple of functions that implement sendMail.
[00:06:11.980 --> 00:06:16.980]   I don't know, email user, mail admins, send email message.
[00:06:16.980 --> 00:06:21.220]   And as you can see, it returns the link to the GitHub file.
[00:06:21.220 --> 00:06:26.220]   So you can click and it returns the highlighted function.
[00:06:26.220 --> 00:06:29.100]   Then you also get the match rating,
[00:06:29.100 --> 00:06:34.100]   which is kind of a pseudo match rating,
[00:06:34.100 --> 00:06:36.580]   which is calculated from the distance
[00:06:36.580 --> 00:06:39.660]   between the query and the code itself.
[00:06:39.660 --> 00:06:41.580]   And then we get the similar code snippets,
[00:06:41.580 --> 00:06:43.620]   which I'll show in a minute.
[00:06:43.620 --> 00:06:49.300]   So we can try a slightly more complex query,
[00:06:49.300 --> 00:06:54.300]   for example, how to get database table name in Django.
[00:06:54.300 --> 00:06:58.780]   So here we get getTableList,
[00:06:58.780 --> 00:07:01.540]   which is kind of what we're looking for.
[00:07:01.540 --> 00:07:03.300]   So not the exact answers,
[00:07:03.300 --> 00:07:08.180]   but I guess any skilled developer could take this function
[00:07:08.180 --> 00:07:10.620]   and extract what it needs from it.
[00:07:10.620 --> 00:07:13.300]   So to get just the one table name
[00:07:13.300 --> 00:07:15.820]   or the table name he's looking for.
[00:07:15.820 --> 00:07:18.140]   And to showcase the previously mentioned
[00:07:18.140 --> 00:07:20.260]   similar code snippets option,
[00:07:20.260 --> 00:07:25.500]   let's say we wanted to look for how to convert
[00:07:25.500 --> 00:07:30.420]   time zone to local date time.
[00:07:31.620 --> 00:07:35.860]   Okay, and we basically get functions
[00:07:35.860 --> 00:07:40.300]   that are able to do the conversion from time zone,
[00:07:40.300 --> 00:07:43.220]   from a specific time zone to a local date time.
[00:07:43.220 --> 00:07:46.460]   And if we click on the similar code snippets,
[00:07:46.460 --> 00:07:50.180]   so this basically takes this code snippet
[00:07:50.180 --> 00:07:55.180]   and basically searches for similar implementations,
[00:07:55.180 --> 00:08:00.260]   similar functions that implement the same functionality.
[00:08:01.020 --> 00:08:03.220]   And here we get as expected,
[00:08:03.220 --> 00:08:08.220]   a bunch of time zone converting functions as well.
[00:08:08.220 --> 00:08:11.940]   So I mentioned that code snippet search
[00:08:11.940 --> 00:08:14.780]   also comes in the web extension variety.
[00:08:14.780 --> 00:08:18.260]   So if we switch to a GitHub repository,
[00:08:18.260 --> 00:08:22.900]   we can open the sidebar by pressing Alt+Shift,
[00:08:22.900 --> 00:08:25.340]   the sidebar where you can enter the query,
[00:08:25.340 --> 00:08:26.860]   the same as before.
[00:08:26.860 --> 00:08:30.500]   Let's say, since we're on the PyTorch GitHub repository,
[00:08:30.500 --> 00:08:31.340]   let's say we wanted to look
[00:08:31.340 --> 00:08:34.460]   for one of the one-dimensional convolutions.
[00:08:34.460 --> 00:08:41.380]   And as expected, we get back the one-dimensional convolution
[00:08:41.380 --> 00:08:43.780]   implemented in PyTorch.
[00:08:43.780 --> 00:08:48.780]   Let's for fun just search this in the GitHub search
[00:08:48.780 --> 00:08:53.580]   and let's see what we get back.
[00:08:53.580 --> 00:08:58.060]   As we see, code snippet searches is back.
[00:08:59.060 --> 00:08:59.940]   (chuckles)
[00:08:59.940 --> 00:09:01.700]   It is better in every way,
[00:09:01.700 --> 00:09:05.380]   so don't use the GitHub search anymore.
[00:09:05.380 --> 00:09:08.500]   (chuckles)
[00:09:08.500 --> 00:09:10.140]   Of course not.
[00:09:10.140 --> 00:09:12.660]   And another feature I would like to show you
[00:09:12.660 --> 00:09:14.940]   is the search by code feature,
[00:09:14.940 --> 00:09:19.300]   which is basically you select any arbitrary code snippet,
[00:09:19.300 --> 00:09:24.060]   you press the right mouse click
[00:09:24.060 --> 00:09:26.660]   and you can select search by code.
[00:09:26.660 --> 00:09:31.660]   This basically searches for similar code snippets
[00:09:31.660 --> 00:09:35.260]   and here we get various implementations
[00:09:35.260 --> 00:09:39.500]   of the value function from various modules.
[00:09:39.500 --> 00:09:42.940]   And here, luckily we also get,
[00:09:42.940 --> 00:09:45.020]   let's say the leaky value function,
[00:09:45.020 --> 00:09:46.540]   which is also implemented
[00:09:46.540 --> 00:09:50.380]   and it's kind of, let's say a kind of expected result
[00:09:50.380 --> 00:09:52.280]   if you're looking for the value function,
[00:09:52.280 --> 00:09:56.260]   you might expect to also see the leaky value function.
[00:09:56.860 --> 00:10:01.500]   So this was kind of a quick demo of the function
[00:10:01.500 --> 00:10:05.660]   of the web application and the web extension.
[00:10:05.660 --> 00:10:15.580]   Let's restart the presentation.
[00:10:15.580 --> 00:10:19.740]   Okay, so now we can go take a closer look behind the scene.
[00:10:19.740 --> 00:10:21.500]   As I mentioned before,
[00:10:21.500 --> 00:10:24.940]   code snippet search works by using joint embeddings
[00:10:24.940 --> 00:10:29.500]   of code and queries to implement the neural search system.
[00:10:29.500 --> 00:10:32.340]   The training objective is to map code
[00:10:32.340 --> 00:10:35.260]   and corresponding queries onto vectors
[00:10:35.260 --> 00:10:36.740]   that are close to each other.
[00:10:36.740 --> 00:10:39.820]   With this, I can embed the natural language query
[00:10:39.820 --> 00:10:42.180]   and then use nearest neighbor search
[00:10:42.180 --> 00:10:46.760]   to return a set of closest code snippets.
[00:10:46.760 --> 00:10:49.460]   During training, I use function doc strings
[00:10:49.460 --> 00:10:53.780]   or comments as substitutes for natural language queries.
[00:10:53.780 --> 00:10:57.740]   So in this image, I have the entire model
[00:10:57.740 --> 00:10:58.940]   split up into layers.
[00:10:58.940 --> 00:11:01.900]   So first is of course the input layer.
[00:11:01.900 --> 00:11:05.820]   One input we have for the queries or doc strings
[00:11:05.820 --> 00:11:08.560]   and one input each for languages.
[00:11:08.560 --> 00:11:13.700]   So inputs are then forwarded into embedding layers
[00:11:13.700 --> 00:11:16.700]   with Java afterwards.
[00:11:16.700 --> 00:11:18.740]   And basically the magic happens
[00:11:18.740 --> 00:11:21.580]   in the encoding and coding layer.
[00:11:21.580 --> 00:11:22.940]   Here I take the samples,
[00:11:22.940 --> 00:11:25.100]   but in this case, the most effective way
[00:11:25.100 --> 00:11:29.240]   of encoding the tokens, which is using a weighted mean.
[00:11:29.240 --> 00:11:31.820]   The weights we use for weighting are learned
[00:11:31.820 --> 00:11:35.220]   in a separate layer, in a separate trainable layer.
[00:11:35.220 --> 00:11:39.180]   Since we split up the languages at the input,
[00:11:39.180 --> 00:11:42.500]   we have to concatenate them back in the same order
[00:11:42.500 --> 00:11:44.620]   as they appeared in the queries.
[00:11:44.620 --> 00:11:47.180]   And finally, I normalize the rows
[00:11:47.180 --> 00:11:49.300]   in the concatenated matrices
[00:11:49.300 --> 00:11:53.580]   and multiply them to get cosine similarities.
[00:11:53.580 --> 00:11:55.860]   So in the last functions,
[00:11:55.860 --> 00:11:57.860]   the code search net implemented a couple
[00:11:57.860 --> 00:11:59.580]   of different loss functions,
[00:11:59.580 --> 00:12:04.100]   but in general, the last function can be
[00:12:04.100 --> 00:12:07.580]   intuitively explained as maximizing the similarities
[00:12:07.580 --> 00:12:10.700]   between the corresponding code and query pairs
[00:12:10.700 --> 00:12:13.120]   while minimizing the similarities
[00:12:13.120 --> 00:12:15.100]   between non-corresponding pairs.
[00:12:15.100 --> 00:12:20.100]   So in this case, the similarity is the cosine similarity.
[00:12:20.100 --> 00:12:25.220]   So, and how is the actual searching performed?
[00:12:25.220 --> 00:12:27.180]   I take all of the existing code snippets,
[00:12:27.180 --> 00:12:29.100]   I encode them and store them
[00:12:29.100 --> 00:12:31.980]   in a nearest neighbors search index.
[00:12:31.980 --> 00:12:35.560]   So when I want to look up nearest code snippets
[00:12:35.560 --> 00:12:38.380]   using a query, I encode the query,
[00:12:38.380 --> 00:12:39.740]   look up the nearest neighbors
[00:12:39.740 --> 00:12:42.300]   and return the most similar ones.
[00:12:42.300 --> 00:12:45.460]   I'm using the Annoy Index nearest neighbors library
[00:12:45.460 --> 00:12:48.980]   because of the lookup speed and it's very fast.
[00:12:48.980 --> 00:12:52.560]   And since the data set was really large,
[00:12:52.560 --> 00:12:55.600]   building the index was time consuming.
[00:12:55.600 --> 00:12:58.220]   That forced me to do some open source work
[00:12:58.220 --> 00:13:00.500]   and contribute to the Annoy Index project
[00:13:00.500 --> 00:13:03.540]   to implement a multi-threaded index build.
[00:13:03.540 --> 00:13:07.240]   So if you're using Annoy Index, that should be available.
[00:13:07.240 --> 00:13:09.100]   And the Annoy Index was responsible
[00:13:09.100 --> 00:13:11.360]   for another little anecdote that happened
[00:13:11.360 --> 00:13:14.580]   while working on the code search challenge.
[00:13:14.580 --> 00:13:18.420]   I've mentioned that I've re-implemented their baseline,
[00:13:18.420 --> 00:13:20.700]   including the searching part.
[00:13:20.700 --> 00:13:22.460]   In my re-implementation,
[00:13:22.460 --> 00:13:25.940]   I used the Scikit nearest neighbors search class
[00:13:25.940 --> 00:13:27.780]   instead of Annoy Index because
[00:13:27.780 --> 00:13:29.480]   by then I wasn't familiar with it.
[00:13:29.480 --> 00:13:33.380]   I wasn't expecting a good result on the challenge
[00:13:33.380 --> 00:13:35.380]   since I wasn't doing anything special.
[00:13:35.380 --> 00:13:40.100]   But it turned out I was at the top of the leaderboard,
[00:13:40.100 --> 00:13:43.280]   almost doubling the previous high score.
[00:13:43.280 --> 00:13:47.380]   So this kind of puzzled me and the organizers as well.
[00:13:47.380 --> 00:13:48.920]   So I did some digging.
[00:13:48.920 --> 00:13:50.800]   I swapped out the Scikit version,
[00:13:50.800 --> 00:13:52.680]   so my version with Annoy Index
[00:13:52.680 --> 00:13:54.800]   and my score instantly dropped back.
[00:13:54.800 --> 00:13:58.760]   It turned out the provided code search evaluation code
[00:13:58.760 --> 00:14:01.640]   didn't update the size of the Annoy Index.
[00:14:01.640 --> 00:14:04.160]   And so it returned terrible results.
[00:14:04.160 --> 00:14:08.480]   So the lesson is always benchmark the index size
[00:14:08.480 --> 00:14:10.300]   if you're using Annoy Index.
[00:14:10.300 --> 00:14:14.620]   Okay, back to code snippet search.
[00:14:14.620 --> 00:14:17.360]   The training procedure is basically split
[00:14:17.360 --> 00:14:18.960]   into two major parts,
[00:14:18.960 --> 00:14:21.000]   training the base language models
[00:14:21.000 --> 00:14:24.240]   and then training the repository language models
[00:14:24.240 --> 00:14:27.260]   with the help of the base language models.
[00:14:27.260 --> 00:14:30.080]   First, I trained the base language models
[00:14:30.080 --> 00:14:32.420]   using the code search net corpus.
[00:14:32.420 --> 00:14:35.820]   And the main goal of the base language model
[00:14:35.820 --> 00:14:39.420]   is to train the word and code token embeddings
[00:14:39.420 --> 00:14:43.980]   that can then be transferred to repository models.
[00:14:43.980 --> 00:14:44.800]   Basically with this,
[00:14:44.800 --> 00:14:47.480]   I'm hoping that the base language models
[00:14:47.480 --> 00:14:50.400]   learn some general programming language features
[00:14:50.400 --> 00:14:53.500]   that can then be fine tuned by the repository data.
[00:14:53.500 --> 00:14:56.020]   With repositories,
[00:14:56.020 --> 00:14:59.120]   I was kind of on my own to extract the corpus.
[00:14:59.120 --> 00:15:04.040]   So I'm using another GitHub project called TreeSitter
[00:15:04.040 --> 00:15:08.560]   to find functions and then tokenize them.
[00:15:08.560 --> 00:15:12.600]   So repository models have the exact same structure.
[00:15:12.600 --> 00:15:14.800]   I just initialize all embedding layers
[00:15:14.800 --> 00:15:17.440]   with base language embeddings.
[00:15:17.440 --> 00:15:20.420]   So I just use that as the initial value.
[00:15:20.420 --> 00:15:22.700]   And the embeddings are then fine tuned
[00:15:22.700 --> 00:15:26.320]   by training the model with repository code snippets.
[00:15:26.320 --> 00:15:32.200]   Okay, and there are of course,
[00:15:32.200 --> 00:15:35.960]   plenty of warnings attached to code snippet search.
[00:15:35.960 --> 00:15:37.840]   You need a reasonably large,
[00:15:37.840 --> 00:15:40.840]   well-documented repository to train the model.
[00:15:40.840 --> 00:15:43.360]   And even then it only works on functions.
[00:15:43.360 --> 00:15:47.000]   So the first logical step would be to figure out
[00:15:47.000 --> 00:15:49.200]   how to work on smaller chunks of code.
[00:15:49.200 --> 00:15:52.580]   So a couple of lines of code that are common above.
[00:15:52.580 --> 00:15:54.480]   And from a model perspective,
[00:15:54.480 --> 00:15:57.440]   I would like to experiment with tree-based models
[00:15:57.440 --> 00:16:00.440]   that could potentially capture the information
[00:16:00.440 --> 00:16:03.480]   hidden in the abstract syntax tree.
[00:16:03.480 --> 00:16:05.400]   As you've seen in the model image,
[00:16:05.400 --> 00:16:07.960]   we're now kind of just ignoring,
[00:16:07.960 --> 00:16:11.880]   even ignoring the order because that seems to work the best
[00:16:11.880 --> 00:16:16.840]   when encoding the code and queries.
[00:16:16.840 --> 00:16:19.880]   So, but potentially for that,
[00:16:19.880 --> 00:16:21.880]   we would need to gather a lot more samples
[00:16:21.880 --> 00:16:26.880]   because even using RNNs or 1D convolutions
[00:16:26.880 --> 00:16:30.160]   on these sequences of tokens,
[00:16:30.160 --> 00:16:35.160]   they don't work as good as a simple Diverboard model.
[00:16:35.160 --> 00:16:39.560]   So potentially we would have to gather a lot more samples.
[00:16:39.560 --> 00:16:43.920]   So that would be all from me.
[00:16:43.920 --> 00:16:45.200]   I hope you like the project.
[00:16:45.200 --> 00:16:50.200]   I hope it's potentially useful and thank you for listening.
[00:16:50.200 --> 00:16:57.480]   - Great, yeah, thanks for sharing, Rak.
[00:16:57.480 --> 00:16:59.800]   I've got a lot of questions
[00:16:59.800 --> 00:17:03.480]   and I hope, I encourage folks in the audience
[00:17:03.480 --> 00:17:05.200]   to submit their own questions as well,
[00:17:05.200 --> 00:17:06.560]   but I'll kick us off here.
[00:17:06.560 --> 00:17:12.320]   One is, so based off of your experience with the tool
[00:17:12.320 --> 00:17:14.080]   or maybe the way that you tokenize things,
[00:17:14.080 --> 00:17:16.480]   how robust is this to typos?
[00:17:16.480 --> 00:17:19.080]   You know, like, so if I typed, you know, convolutions
[00:17:19.080 --> 00:17:21.640]   as you did a couple of times instead of convolutions,
[00:17:21.640 --> 00:17:25.640]   is that going to throw it off or is it robust to that?
[00:17:27.880 --> 00:17:29.680]   - I guess the answer is kind of,
[00:17:29.680 --> 00:17:34.000]   because we're kind of using, for tokenizing,
[00:17:34.000 --> 00:17:36.040]   for tokens we're using half of it
[00:17:36.040 --> 00:17:38.680]   as just the regular vocabulary
[00:17:38.680 --> 00:17:42.280]   and half of it with the byte pair encoding.
[00:17:42.280 --> 00:17:47.280]   So we're using kind of the subset of the token.
[00:17:47.280 --> 00:17:52.560]   So I guess if you misspell the last part of the word,
[00:17:52.560 --> 00:17:57.560]   so the CO and V, I guess it could capture that
[00:17:57.560 --> 00:18:00.560]   and still find the appropriate function.
[00:18:00.560 --> 00:18:05.560]   So it's kind of robust, but I guess the answer is kind of,
[00:18:05.560 --> 00:18:13.080]   as I've said, these were kind of cherry picked examples.
[00:18:13.080 --> 00:18:16.960]   So if you provide a weird input,
[00:18:16.960 --> 00:18:19.600]   it's gonna provide a weird output for now,
[00:18:19.600 --> 00:18:23.320]   but then I'm trying to work on the,
[00:18:24.480 --> 00:18:27.760]   minimizing the amount of times that happens.
[00:18:27.760 --> 00:18:33.280]   - I mean, yeah, I mean, I would imagine a byte pair encoding.
[00:18:33.280 --> 00:18:35.360]   So this is for folks who aren't as familiar
[00:18:35.360 --> 00:18:36.760]   with natural language byte pair encoding,
[00:18:36.760 --> 00:18:40.360]   tries to find good ways to break words down into sub words
[00:18:40.360 --> 00:18:42.880]   and use those and embed those.
[00:18:42.880 --> 00:18:45.720]   So it's what's used in the general purpose transform,
[00:18:45.720 --> 00:18:48.520]   or not the generative pre-trained transformer models,
[00:18:48.520 --> 00:18:50.880]   the GPT models at OpenAI.
[00:18:50.880 --> 00:18:52.240]   That's what they use for their natural language
[00:18:52.240 --> 00:18:53.240]   processing stuff.
[00:18:53.240 --> 00:18:56.160]   I'm actually, so you trained this byte pair encoding,
[00:18:56.160 --> 00:18:58.320]   it's specialized to code, right?
[00:18:58.320 --> 00:19:01.600]   It's not like a pre-trained thing from, yeah.
[00:19:01.600 --> 00:19:04.120]   So it's actually, it would be cool actually
[00:19:04.120 --> 00:19:07.720]   to see what are the byte pairs that show up in code.
[00:19:07.720 --> 00:19:09.840]   Are they different from the ones that show up
[00:19:09.840 --> 00:19:11.000]   in natural language?
[00:19:11.000 --> 00:19:14.320]   Like maybe underscores become really important
[00:19:14.320 --> 00:19:15.920]   or something like that.
[00:19:15.920 --> 00:19:19.440]   - Yeah, basically all of the punctuation in the code
[00:19:19.440 --> 00:19:21.960]   becomes really important.
[00:19:22.960 --> 00:19:27.960]   Yeah, I think basically I looked inside the vocabularies,
[00:19:27.960 --> 00:19:31.400]   I think the first 10, 15 are kind of the combinations
[00:19:31.400 --> 00:19:34.160]   of basically of the punctuation.
[00:19:34.160 --> 00:19:37.800]   So semicolons, colons, stuff like that.
[00:19:37.800 --> 00:19:39.760]   - Nice, and yeah, and actually that's an important detail
[00:19:39.760 --> 00:19:41.720]   of the byte pair encoding algorithm, right?
[00:19:41.720 --> 00:19:44.760]   It starts with the most important chunks.
[00:19:44.760 --> 00:19:47.520]   So you actually do get kind of an ordering of importance
[00:19:47.520 --> 00:19:49.600]   of how like, how soon were these learned
[00:19:49.600 --> 00:19:52.440]   as to be a useful thing to encode.
[00:19:52.440 --> 00:19:54.280]   So it's cool that it's actually all punctuation.
[00:19:54.280 --> 00:19:56.280]   That's a neat result.
[00:19:56.280 --> 00:19:58.040]   I don't know if anybody's ever published
[00:19:58.040 --> 00:20:01.000]   on what the encodings look like for code.
[00:20:01.000 --> 00:20:07.640]   You mentioned that you switched over
[00:20:07.640 --> 00:20:12.160]   from TensorFlow Keras to PyTorch.
[00:20:12.160 --> 00:20:14.480]   So I was wondering, like a lot of our audience actually,
[00:20:14.480 --> 00:20:16.880]   works in both frameworks sometimes,
[00:20:16.880 --> 00:20:18.280]   it's sort of like jumps between them.
[00:20:18.280 --> 00:20:21.240]   I often get questions from folks in our community,
[00:20:21.240 --> 00:20:23.880]   should I do this project in Keras, should I do it in PyTorch?
[00:20:23.880 --> 00:20:25.360]   What's your recommendation?
[00:20:25.360 --> 00:20:27.360]   What do you think are the important things
[00:20:27.360 --> 00:20:28.960]   to help make that decision?
[00:20:28.960 --> 00:20:35.240]   - Just looking at the image of my model, right?
[00:20:35.240 --> 00:20:38.280]   I was having a lot of trouble fitting this kind of structure
[00:20:38.280 --> 00:20:41.160]   into basically into Keras.
[00:20:41.160 --> 00:20:46.160]   Basically I had to, there is no way, at least I know,
[00:20:46.920 --> 00:20:50.480]   in Keras to encode, basically to have inputs
[00:20:50.480 --> 00:20:51.760]   of different sizes, right?
[00:20:51.760 --> 00:20:56.760]   So potentially you would have three Python samples,
[00:20:56.760 --> 00:21:00.600]   10 Go samples, 15 Ruby samples.
[00:21:00.600 --> 00:21:04.960]   And as far as I could experiment,
[00:21:04.960 --> 00:21:07.680]   I couldn't come up with a way to make,
[00:21:07.680 --> 00:21:10.840]   for Keras to make sense of that.
[00:21:10.840 --> 00:21:14.720]   So basically with Keras, I had to split up the models.
[00:21:14.720 --> 00:21:19.720]   Basically there were six models, one for each language,
[00:21:19.720 --> 00:21:24.160]   basically one for each query and programming language pair,
[00:21:24.160 --> 00:21:29.160]   because that was the only way to have multiple languages,
[00:21:29.160 --> 00:21:33.720]   but that kind of loses the point
[00:21:33.720 --> 00:21:41.720]   where you have the same queries for all of the languages.
[00:21:41.720 --> 00:21:46.720]   And so you kind of lose on the one training.
[00:21:46.720 --> 00:21:50.360]   And that was kind of the first problem.
[00:21:50.360 --> 00:21:52.360]   The Keras model looked really weird
[00:21:52.360 --> 00:21:56.720]   and it was kind of hard to implement it, at least for me.
[00:21:56.720 --> 00:22:01.520]   So I, and then the second problem was basically
[00:22:01.520 --> 00:22:04.640]   just the development problem with the Django.
[00:22:04.640 --> 00:22:08.080]   When locally developing Keras and Django,
[00:22:08.080 --> 00:22:11.600]   there were some weird threading issues.
[00:22:11.600 --> 00:22:14.480]   I couldn't figure out because
[00:22:14.480 --> 00:22:20.280]   I don't exactly remember what it was,
[00:22:20.280 --> 00:22:23.640]   but basically when I ran a local Django server
[00:22:23.640 --> 00:22:26.200]   and it called the model, the TensorFlow model
[00:22:26.200 --> 00:22:30.200]   or the Keras model, it just segfaulted
[00:22:30.200 --> 00:22:32.560]   or something like that, something really weird,
[00:22:32.560 --> 00:22:35.960]   which basically there was some threading issue
[00:22:35.960 --> 00:22:40.960]   that required me to run Djangos in a single thread
[00:22:41.480 --> 00:22:43.360]   somewhere else or something like that.
[00:22:43.360 --> 00:22:48.360]   So that kind of culminated in me just looking into PyTorch
[00:22:48.360 --> 00:22:52.680]   and seeing that basically it was really simple
[00:22:52.680 --> 00:22:57.680]   to transfer all of the codes into PyTorch.
[00:22:57.680 --> 00:23:03.960]   And the model itself is actually really simple
[00:23:03.960 --> 00:23:07.640]   to implement in PyTorch.
[00:23:07.640 --> 00:23:12.320]   In Keras, it was actually pretty long.
[00:23:12.320 --> 00:23:15.320]   This is the model.
[00:23:15.320 --> 00:23:17.840]   Basically, these are all helper functions.
[00:23:17.840 --> 00:23:23.920]   And this is the forward, basically the forward function
[00:23:23.920 --> 00:23:25.360]   for training the model, right?
[00:23:25.360 --> 00:23:26.480]   This is it.
[00:23:26.480 --> 00:23:28.880]   This is basically, this does everything.
[00:23:28.880 --> 00:23:31.960]   And here are the encoding stuff.
[00:23:31.960 --> 00:23:35.160]   And I don't know, it just seems simple.
[00:23:35.160 --> 00:23:36.720]   It was easy to deploy.
[00:23:37.720 --> 00:23:42.720]   The training was marginally faster with PyTorch.
[00:23:42.720 --> 00:23:48.200]   And because I didn't have to train six separate models,
[00:23:48.200 --> 00:23:51.440]   that also helped a lot with the development speed
[00:23:51.440 --> 00:23:54.720]   and I was able to kind of iterate faster.
[00:23:54.720 --> 00:23:58.160]   And the train and the model, of course, got better
[00:23:58.160 --> 00:24:01.000]   because queries were able to be shared
[00:24:01.000 --> 00:24:02.840]   between separate languages.
[00:24:02.840 --> 00:24:05.720]   - That's interesting.
[00:24:05.720 --> 00:24:07.680]   I definitely have heard from folks
[00:24:07.680 --> 00:24:12.000]   that when it comes time to build a complicated model
[00:24:12.000 --> 00:24:14.640]   in Keras, it's not as clear how to do that.
[00:24:14.640 --> 00:24:18.120]   I think there are folks in the TensorFlow community
[00:24:18.120 --> 00:24:19.320]   who do build these kinds of models
[00:24:19.320 --> 00:24:20.720]   and they know the right way to do it.
[00:24:20.720 --> 00:24:22.400]   But I think what people's experience has been
[00:24:22.400 --> 00:24:24.200]   is that it's just a little bit harder to figure out
[00:24:24.200 --> 00:24:26.280]   how to do that than in PyTorch.
[00:24:26.280 --> 00:24:28.720]   So hopefully, I think they're both great frameworks
[00:24:28.720 --> 00:24:30.640]   and I do work in both of them.
[00:24:30.640 --> 00:24:33.680]   So hopefully that gets surfaced better
[00:24:33.680 --> 00:24:35.800]   so that we have more flexibility
[00:24:35.800 --> 00:24:38.880]   in choosing which framework to use.
[00:24:38.880 --> 00:24:42.800]   Let's see.
[00:24:42.800 --> 00:24:46.920]   Oh, another question, a little bit broader.
[00:24:46.920 --> 00:24:49.960]   Why do you think the bag of words methods
[00:24:49.960 --> 00:24:51.560]   are so successful here?
[00:24:51.560 --> 00:24:52.920]   You mentioned, I think, a couple of times
[00:24:52.920 --> 00:24:56.200]   that people have tried 1D convolutions
[00:24:56.200 --> 00:24:58.040]   and recurrent networks of different kinds
[00:24:58.040 --> 00:24:59.640]   and not found great performance.
[00:24:59.640 --> 00:25:02.840]   Do you have any intuition for why that should be the case
[00:25:02.840 --> 00:25:04.400]   that these would work so poorly?
[00:25:04.400 --> 00:25:10.040]   - My first guess would be that there just isn't a lot of,
[00:25:10.040 --> 00:25:11.840]   that there isn't enough samples,
[00:25:11.840 --> 00:25:15.200]   like probably 6 million functions are not enough to,
[00:25:15.200 --> 00:25:20.760]   are not enough to train basically a recurrent model
[00:25:20.760 --> 00:25:24.080]   or there just isn't a lot of information to get out.
[00:25:24.080 --> 00:25:28.920]   But basically, strictly from the challenge perspective,
[00:25:28.920 --> 00:25:33.920]   I think that a lot of the queries can be answered
[00:25:33.920 --> 00:25:39.160]   just by using a simple token search or something like that,
[00:25:39.160 --> 00:25:42.120]   which is basically what bag of words is.
[00:25:42.120 --> 00:25:45.480]   I think the queries itself,
[00:25:45.480 --> 00:25:48.920]   the challenge queries are not that complicated.
[00:25:48.920 --> 00:25:53.040]   So they probably don't need any deeper understanding
[00:25:53.040 --> 00:25:55.920]   than just basically looking at tokens from the query,
[00:25:55.920 --> 00:25:57.520]   tokens from the code,
[00:25:57.520 --> 00:25:59.640]   and just kind of matching,
[00:25:59.640 --> 00:26:01.920]   kind of fuzzy matching them together
[00:26:01.920 --> 00:26:04.360]   and get a pretty good result.
[00:26:04.360 --> 00:26:09.080]   I think the, now with the updated results,
[00:26:09.080 --> 00:26:12.120]   the self-attention model was really close
[00:26:12.120 --> 00:26:14.160]   to the neural bag of words model.
[00:26:14.160 --> 00:26:19.160]   So I think that with some tweaking
[00:26:19.160 --> 00:26:22.880]   the self-attention model, which I didn't look too much into,
[00:26:22.880 --> 00:26:25.400]   but I think it could be better
[00:26:25.400 --> 00:26:28.800]   than the neural bag of words model.
[00:26:28.800 --> 00:26:31.520]   I kind of had to tweak the original implementations
[00:26:31.520 --> 00:26:34.120]   so I could get it just right.
[00:26:34.120 --> 00:26:38.640]   But I think the main thing is just to get more samples.
[00:26:38.640 --> 00:26:42.360]   Basically only 2 million functions are trainable,
[00:26:42.360 --> 00:26:44.240]   which have a docstring.
[00:26:44.240 --> 00:26:49.040]   So probably just more data, I guess.
[00:26:49.040 --> 00:26:50.720]   - Gotcha.
[00:26:50.720 --> 00:26:53.760]   Yeah, I know that to get those big transformer
[00:26:53.760 --> 00:26:56.160]   self-attention models working on natural language
[00:26:56.160 --> 00:26:57.320]   as opposed to code,
[00:26:57.320 --> 00:26:59.440]   people have found that it's pre-training, right?
[00:26:59.440 --> 00:27:01.200]   It's being able to get a dataset
[00:27:01.200 --> 00:27:04.120]   that's like orders of magnitude larger
[00:27:04.120 --> 00:27:05.720]   than your supervised dataset
[00:27:05.720 --> 00:27:07.800]   because there's no human in the loop.
[00:27:07.800 --> 00:27:12.800]   So if you were able to code crawl all of GitHub,
[00:27:12.800 --> 00:27:14.840]   all of its public repositories,
[00:27:14.840 --> 00:27:17.320]   and just learn how to generate code,
[00:27:17.320 --> 00:27:19.720]   that might be helpful.
[00:27:19.720 --> 00:27:21.200]   They probably already did some pre-training
[00:27:21.200 --> 00:27:23.200]   in the self-attention one.
[00:27:23.880 --> 00:27:26.040]   - Probably, I guess.
[00:27:26.040 --> 00:27:30.600]   But yeah, my guess would be just more data.
[00:27:30.600 --> 00:27:34.120]   And with the neural bag of words model,
[00:27:34.120 --> 00:27:36.800]   there's a funny little issue where you don't,
[00:27:36.800 --> 00:27:41.000]   the model doesn't differentiate between,
[00:27:41.000 --> 00:27:44.600]   convert int to string or string to int.
[00:27:44.600 --> 00:27:48.120]   Basically that's the same thing to the model
[00:27:48.120 --> 00:27:50.120]   and it returns same results.
[00:27:53.120 --> 00:27:55.160]   And a lot of the challenge queries
[00:27:55.160 --> 00:27:59.320]   are convert something to that JSON to XML
[00:27:59.320 --> 00:28:02.160]   and to get it working on that,
[00:28:02.160 --> 00:28:07.160]   or basically maybe just using a sequence model
[00:28:07.160 --> 00:28:10.000]   on the queries and then just the neural bag of words
[00:28:10.000 --> 00:28:15.000]   on the code might help to encode this kind of dependencies.
[00:28:15.000 --> 00:28:20.080]   - Gotcha, yeah, that makes sense.
[00:28:21.040 --> 00:28:25.800]   Great, well, I think that's about all the time that we have.
[00:28:25.800 --> 00:28:29.000]   So thank you so much for staying up late
[00:28:29.000 --> 00:28:32.280]   to be able to present your work to this community.
[00:28:32.280 --> 00:28:34.160]   And thanks for answering my questions.
[00:28:34.160 --> 00:28:36.040]   It's really cool work.
[00:28:36.040 --> 00:28:37.760]   Really, again, like very impressive.
[00:28:37.760 --> 00:28:39.760]   Oh, I did have one question that I wanted to make sure
[00:28:39.760 --> 00:28:41.960]   I didn't forget to ask you,
[00:28:41.960 --> 00:28:44.880]   which is, I think a lot of people in our community
[00:28:44.880 --> 00:28:48.320]   are on sort of like a similar track to you
[00:28:48.320 --> 00:28:49.920]   where you're a full stack developer,
[00:28:49.920 --> 00:28:53.240]   you do, your day job is not machine learning
[00:28:53.240 --> 00:28:54.080]   and deep learning.
[00:28:54.080 --> 00:28:55.600]   And this is something that you've like a skill
[00:28:55.600 --> 00:28:57.280]   that you've added to your toolkit.
[00:28:57.280 --> 00:28:58.760]   So could you just comment on maybe
[00:28:58.760 --> 00:29:01.560]   the most important resources that you think are out there
[00:29:01.560 --> 00:29:03.960]   for following that journey?
[00:29:03.960 --> 00:29:06.400]   What was most important to you along the way?
[00:29:06.400 --> 00:29:08.400]   Like the kind of, the advice that you would have liked
[00:29:08.400 --> 00:29:11.200]   to have gotten maybe when you were just getting started?
[00:29:11.200 --> 00:29:17.560]   - I guess from an engineer's perspective,
[00:29:17.560 --> 00:29:22.560]   I just don't care about math too much at the start, right?
[00:29:22.560 --> 00:29:28.080]   Just kind of, I would think it would be better
[00:29:28.080 --> 00:29:31.880]   because I was worried that if I didn't understand
[00:29:31.880 --> 00:29:33.840]   everything behind that was happening,
[00:29:33.840 --> 00:29:37.240]   like the math, the back propagations, all of that stuff,
[00:29:37.240 --> 00:29:41.480]   that I wouldn't be able to come up with models on my own.
[00:29:41.480 --> 00:29:45.240]   So I think it would be great advice
[00:29:45.240 --> 00:29:49.360]   or someone could tell me back then
[00:29:49.360 --> 00:29:54.000]   that you can just start coding, start producing models,
[00:29:54.000 --> 00:29:56.920]   throw some data at it and see if it works.
[00:29:56.920 --> 00:30:00.400]   And if it doesn't, then you can go basically explore
[00:30:00.400 --> 00:30:01.280]   why it doesn't work.
[00:30:01.280 --> 00:30:05.080]   And then by basically using that, you can learn.
[00:30:05.080 --> 00:30:10.080]   But just kind of as a general resource,
[00:30:13.800 --> 00:30:18.800]   I think the PyTorch documentation is really good.
[00:30:18.800 --> 00:30:21.560]   The Keras documentation is really good.
[00:30:21.560 --> 00:30:25.480]   Basically for beginners, Keras is great.
[00:30:25.480 --> 00:30:27.200]   The tutorials are great.
[00:30:27.200 --> 00:30:33.320]   And also the Kaggle competitions or some of the,
[00:30:33.320 --> 00:30:39.120]   not the competitions, but the stuff that is just like there
[00:30:39.120 --> 00:30:40.360]   for beginners to learn.
[00:30:40.360 --> 00:30:42.080]   There are a lot of great notebooks
[00:30:42.080 --> 00:30:46.560]   that basically implement either a neural network
[00:30:46.560 --> 00:30:48.800]   from scratch or they're using Keras
[00:30:48.800 --> 00:30:53.680]   to basically train a simple model to predict something.
[00:30:53.680 --> 00:30:55.920]   I learned actually a lot from those notebooks
[00:30:55.920 --> 00:30:59.320]   because I could actually see the code
[00:30:59.320 --> 00:31:02.840]   and not just go through tutorials over and over again.
[00:31:02.840 --> 00:31:06.400]   I could just download the, I think the iPython notebook
[00:31:06.400 --> 00:31:09.120]   and kind of try on my own.
[00:31:10.120 --> 00:31:12.320]   Basically just trying stuff on your own
[00:31:12.320 --> 00:31:16.720]   is the most important thing to get started.
[00:31:16.720 --> 00:31:21.720]   And yeah, just don't get stuck in the math at first.
[00:31:21.720 --> 00:31:26.000]   You can get stuck later, but it's not at first.
[00:31:26.000 --> 00:31:29.880]   At least for non-machine learning practitioners,
[00:31:29.880 --> 00:31:33.920]   basically just people are coming in from, I don't know,
[00:31:33.920 --> 00:31:36.720]   front end, back end, full stack developers.
[00:31:36.720 --> 00:31:38.920]   - Yeah, I think that makes sense.
[00:31:38.920 --> 00:31:41.720]   It's motivating, it's energizing when you make something,
[00:31:41.720 --> 00:31:43.320]   even if it's not perfect,
[00:31:43.320 --> 00:31:46.200]   if you make a cool tool for yourself,
[00:31:46.200 --> 00:31:48.080]   that's a nice way to get started
[00:31:48.080 --> 00:31:50.280]   rather than just slogging through textbooks.
[00:31:50.280 --> 00:31:53.920]   - Exactly, that would be my advice.
[00:31:53.920 --> 00:31:54.960]   - Yeah, yeah.
[00:31:54.960 --> 00:31:57.720]   So it's, we sort of took, I think, two very different paths.
[00:31:57.720 --> 00:32:01.000]   I went in, did a PhD and had the chance
[00:32:01.000 --> 00:32:02.760]   to just get lost in math for a couple of years
[00:32:02.760 --> 00:32:05.640]   while learning my way forward.
[00:32:05.640 --> 00:32:07.800]   But it's not a path I would recommend necessarily
[00:32:07.800 --> 00:32:11.720]   to anybody, or at least to everybody.
[00:32:11.720 --> 00:32:13.840]   So yeah, that's great advice, Ra.
[00:32:13.840 --> 00:32:16.520]   - Yeah, depending on the person, yeah.
[00:32:16.520 --> 00:32:18.680]   - Yeah, well, so thank you so much for joining us.
[00:32:18.680 --> 00:32:20.240]   I'll let you go now.
[00:32:20.240 --> 00:32:25.200]   And yeah, so thanks everybody for tuning in.
[00:32:25.200 --> 00:32:27.800]   (upbeat music)
[00:32:27.800 --> 00:32:30.400]   (upbeat music)
[00:32:30.400 --> 00:32:33.000]   (upbeat music)
[00:32:33.000 --> 00:32:35.600]   (upbeat music)
[00:32:35.600 --> 00:32:45.600]   [BLANK_AUDIO]

