
[00:00:00.000 --> 00:00:06.080]   Welcome back everyone.
[00:00:06.080 --> 00:00:09.560]   This is part six in our series on contextual representation.
[00:00:09.560 --> 00:00:11.080]   We're going to focus on RoBERTa.
[00:00:11.080 --> 00:00:14.680]   RoBERTa stands for robustly optimized BERT approach.
[00:00:14.680 --> 00:00:16.600]   You might recall that I finished
[00:00:16.600 --> 00:00:18.360]   the BERT screencast by listing out
[00:00:18.360 --> 00:00:21.680]   some key known limitations of the BERT model.
[00:00:21.680 --> 00:00:25.280]   The top item on that list was just an observation that
[00:00:25.280 --> 00:00:28.800]   the BERT team originally did an admirably detailed,
[00:00:28.800 --> 00:00:30.840]   but still very partial set of
[00:00:30.840 --> 00:00:33.880]   ablation studies and optimization studies.
[00:00:33.880 --> 00:00:38.960]   That gave us some glimpses of how to best optimize BERT models,
[00:00:38.960 --> 00:00:41.480]   but it was hardly a thorough exploration.
[00:00:41.480 --> 00:00:45.120]   That's where the RoBERTa team is going to take over and try to
[00:00:45.120 --> 00:00:48.600]   do a more thorough exploration of this design space.
[00:00:48.600 --> 00:00:51.160]   I think this is a really interesting development
[00:00:51.160 --> 00:00:53.240]   because at a meta level,
[00:00:53.240 --> 00:00:55.640]   it points to a shift in methodologies.
[00:00:55.640 --> 00:00:57.640]   The RoBERTa team does do
[00:00:57.640 --> 00:01:00.320]   a much fuller exploration of the design space,
[00:01:00.320 --> 00:01:04.120]   but it's nowhere near the exhaustive exploration of
[00:01:04.120 --> 00:01:05.760]   hyperparameters that we used to
[00:01:05.760 --> 00:01:08.880]   see especially in the pre-deep learning era.
[00:01:08.880 --> 00:01:12.440]   I think what we're seeing with RoBERTa is that it is simply too
[00:01:12.440 --> 00:01:15.040]   expensive in terms of money or compute
[00:01:15.040 --> 00:01:17.960]   or time to be completely thorough.
[00:01:17.960 --> 00:01:21.160]   Even RoBERTa is a very heuristic and
[00:01:21.160 --> 00:01:24.000]   partial exploration of the design space.
[00:01:24.000 --> 00:01:27.520]   But nonetheless, I think it was extremely instructive.
[00:01:27.520 --> 00:01:29.920]   For this slide, I'm going to list out
[00:01:29.920 --> 00:01:32.560]   key differences between BERT and RoBERTa,
[00:01:32.560 --> 00:01:35.560]   and then we'll explore some of the evidence in favor of
[00:01:35.560 --> 00:01:38.560]   these decisions just after that.
[00:01:38.560 --> 00:01:40.080]   First item on the list,
[00:01:40.080 --> 00:01:42.920]   BERT used a static masking approach.
[00:01:42.920 --> 00:01:44.960]   What that means is that they copied
[00:01:44.960 --> 00:01:47.560]   their training data some number of times and
[00:01:47.560 --> 00:01:50.560]   applied different masks to each copy.
[00:01:50.560 --> 00:01:53.280]   But then that set of copies of
[00:01:53.280 --> 00:01:55.280]   the dataset with its masking was used
[00:01:55.280 --> 00:01:58.000]   repeatedly during epochs of training.
[00:01:58.000 --> 00:02:00.280]   What that means is that the same masking
[00:02:00.280 --> 00:02:02.560]   was seen repeatedly by the model.
[00:02:02.560 --> 00:02:04.880]   You might have an intuition that we'll get
[00:02:04.880 --> 00:02:07.320]   more and better diversity into
[00:02:07.320 --> 00:02:10.200]   this training regime if we dynamically mask examples,
[00:02:10.200 --> 00:02:13.320]   which would just mean that as we load individual batches,
[00:02:13.320 --> 00:02:17.680]   we apply some random dynamic masking to those so that
[00:02:17.680 --> 00:02:20.960]   subsequent batches containing the same examples
[00:02:20.960 --> 00:02:23.360]   have different masking applied to them.
[00:02:23.360 --> 00:02:25.720]   Clearly, that's going to introduce some diversity
[00:02:25.720 --> 00:02:28.760]   into the training regime and that could be useful.
[00:02:28.760 --> 00:02:32.840]   For BERT, the inputs to the model
[00:02:32.840 --> 00:02:35.760]   were two concatenated document segments,
[00:02:35.760 --> 00:02:37.000]   and that's actually crucial to
[00:02:37.000 --> 00:02:39.240]   their next sentence prediction task.
[00:02:39.240 --> 00:02:41.240]   Whereas for RoBERTa, inputs are
[00:02:41.240 --> 00:02:45.520]   sentence sequences that may even span document boundaries.
[00:02:45.520 --> 00:02:47.800]   Obviously, that's going to be disruptive to
[00:02:47.800 --> 00:02:50.000]   the next sentence prediction objective,
[00:02:50.000 --> 00:02:53.960]   but correspondingly, whereas BERT had that NSP objective,
[00:02:53.960 --> 00:02:55.520]   RoBERTa simply dropped it on
[00:02:55.520 --> 00:02:58.480]   the grounds that it was not earning its keep.
[00:02:58.480 --> 00:03:03.600]   For BERT, the training batches contained 256 examples.
[00:03:03.600 --> 00:03:07.160]   RoBERTa upped that to 2,000 examples per batch,
[00:03:07.160 --> 00:03:09.280]   a substantial increase.
[00:03:09.280 --> 00:03:12.000]   BERT used a wordpiece tokenizer,
[00:03:12.000 --> 00:03:13.240]   whereas RoBERTa used
[00:03:13.240 --> 00:03:17.080]   a character level byte-pair encoding algorithm.
[00:03:17.080 --> 00:03:19.640]   BERT was trained on a lot of data,
[00:03:19.640 --> 00:03:21.960]   Books, Corpus, and English Wikipedia.
[00:03:21.960 --> 00:03:25.840]   RoBERTa leveled up on the amount of data by training on Books,
[00:03:25.840 --> 00:03:28.080]   Corpus, Wikipedia, CC News,
[00:03:28.080 --> 00:03:29.800]   Open Web Text, and Stories,
[00:03:29.800 --> 00:03:31.400]   and the result of that is
[00:03:31.400 --> 00:03:32.840]   a substantial increase in
[00:03:32.840 --> 00:03:35.440]   the amount of data that the model saw.
[00:03:35.440 --> 00:03:38.280]   BERT was trained for one million steps,
[00:03:38.280 --> 00:03:41.840]   whereas RoBERTa was trained for 500,000 steps.
[00:03:41.840 --> 00:03:44.200]   Pause there. You might think that means
[00:03:44.200 --> 00:03:46.760]   RoBERTa was trained for less time,
[00:03:46.760 --> 00:03:48.720]   but remember the batch sizes are
[00:03:48.720 --> 00:03:51.240]   substantially larger and so the net effect of
[00:03:51.240 --> 00:03:53.400]   these two choices is that RoBERTa was
[00:03:53.400 --> 00:03:57.120]   trained for a lot more instances.
[00:03:57.120 --> 00:03:59.440]   Then finally, for the BERT team,
[00:03:59.440 --> 00:04:01.480]   there was an intuition that it would be useful for
[00:04:01.480 --> 00:04:05.080]   optimization to train on short sequences first.
[00:04:05.080 --> 00:04:07.880]   The RoBERTa team simply dropped that and trained on
[00:04:07.880 --> 00:04:11.560]   full-length sequences throughout the training regime.
[00:04:11.560 --> 00:04:14.160]   I think those are the high-level changes
[00:04:14.160 --> 00:04:15.560]   between BERT and RoBERTa.
[00:04:15.560 --> 00:04:17.720]   There are some additional differences and I
[00:04:17.720 --> 00:04:20.000]   refer to Section 3.1 of
[00:04:20.000 --> 00:04:22.920]   the paper for the details on those.
[00:04:22.920 --> 00:04:25.040]   Let's dive into some of
[00:04:25.040 --> 00:04:27.520]   the evidence that they used for these choices,
[00:04:27.520 --> 00:04:30.160]   beginning with that first shift from
[00:04:30.160 --> 00:04:33.000]   static masking to dynamic masking.
[00:04:33.000 --> 00:04:36.240]   This table summarizes their evidence for this choice.
[00:04:36.240 --> 00:04:38.600]   They're using SQuAD, Multi-NLI,
[00:04:38.600 --> 00:04:41.440]   and Binary Stanford Sentiment Treebank
[00:04:41.440 --> 00:04:44.400]   as their benchmarks to make this decision.
[00:04:44.400 --> 00:04:46.960]   You can see that for SQuAD and SST,
[00:04:46.960 --> 00:04:48.200]   there's a pretty clear win,
[00:04:48.200 --> 00:04:49.840]   dynamic masking is better.
[00:04:49.840 --> 00:04:53.000]   For Multi-NLI, it looks like there was a small regression,
[00:04:53.000 --> 00:04:56.880]   but on average, the results look better for dynamic masking.
[00:04:56.880 --> 00:04:59.800]   I will say that to augment these results,
[00:04:59.800 --> 00:05:01.480]   there is a clear intuition that
[00:05:01.480 --> 00:05:03.600]   dynamic masking is going to be useful.
[00:05:03.600 --> 00:05:06.200]   Even if it's not reflected in these benchmarks,
[00:05:06.200 --> 00:05:08.080]   we might still think that it's
[00:05:08.080 --> 00:05:12.120]   a wise choice if we can afford to train in that way.
[00:05:12.120 --> 00:05:16.560]   We talked briefly about how examples are presented to
[00:05:16.560 --> 00:05:19.800]   these models. I would say the two competitors that
[00:05:19.800 --> 00:05:22.320]   Roberta thoroughly evaluated were
[00:05:22.320 --> 00:05:25.000]   full sentences and doc sentences.
[00:05:25.000 --> 00:05:27.480]   Doc sentences will be where we limit
[00:05:27.480 --> 00:05:29.000]   training instances to pairs of
[00:05:29.000 --> 00:05:31.400]   sentences that come from the same document,
[00:05:31.400 --> 00:05:34.360]   which you would think would give us a clear intuition about
[00:05:34.360 --> 00:05:38.080]   something like discourse coherence for those instances.
[00:05:38.080 --> 00:05:41.320]   We can also compare that against full sentences in which we
[00:05:41.320 --> 00:05:43.880]   present examples even though they
[00:05:43.880 --> 00:05:46.480]   might span document boundaries.
[00:05:46.480 --> 00:05:49.760]   We have less of a guarantee of discourse coherence.
[00:05:49.760 --> 00:05:53.160]   Although doc sentences comes out a little bit ahead in
[00:05:53.160 --> 00:05:55.800]   this benchmark that they have set up across squad,
[00:05:55.800 --> 00:05:58.800]   Multi-NLI, SST2, and race,
[00:05:58.800 --> 00:06:01.720]   they chose full sentences on the grounds that there is
[00:06:01.720 --> 00:06:04.960]   more at play here than just accuracy.
[00:06:04.960 --> 00:06:06.880]   We should also think about
[00:06:06.880 --> 00:06:09.200]   the efficiency of the training regime.
[00:06:09.200 --> 00:06:11.760]   Since full sentences makes it much easier to
[00:06:11.760 --> 00:06:14.320]   create efficient batches of examples,
[00:06:14.320 --> 00:06:16.120]   they opted for that instead.
[00:06:16.120 --> 00:06:19.360]   That's also very welcome to my mind because it's showing,
[00:06:19.360 --> 00:06:21.000]   again, that there's more at stake in
[00:06:21.000 --> 00:06:23.640]   this new era than just accuracy.
[00:06:23.640 --> 00:06:26.880]   We should also consider our resources.
[00:06:26.880 --> 00:06:29.760]   This table summarizes
[00:06:29.760 --> 00:06:32.120]   their evidence for the larger batch sizes.
[00:06:32.120 --> 00:06:34.680]   They're using various metrics here, perplexity,
[00:06:34.680 --> 00:06:36.680]   which is a pseudo perplexity given
[00:06:36.680 --> 00:06:39.600]   that BERT uses bidirectional context.
[00:06:39.600 --> 00:06:43.320]   They're also benchmarking against Multi-NLI and SST2.
[00:06:43.320 --> 00:06:45.560]   What they find is that clearly,
[00:06:45.560 --> 00:06:47.040]   there's a win for having
[00:06:47.040 --> 00:06:51.640]   this very large batch size at 2,000 examples.
[00:06:51.640 --> 00:06:55.240]   Then finally, just the raw amount of data that
[00:06:55.240 --> 00:06:56.640]   these models are trained on is
[00:06:56.640 --> 00:06:58.080]   interesting and also the amount
[00:06:58.080 --> 00:06:59.680]   of training time that they get.
[00:06:59.680 --> 00:07:01.640]   What they found is that they got
[00:07:01.640 --> 00:07:04.240]   the best results for Roberta by training
[00:07:04.240 --> 00:07:06.400]   for as long as they could possibly afford
[00:07:06.400 --> 00:07:10.200]   to on as much data as they could include.
[00:07:10.200 --> 00:07:12.240]   You can see the amount of data going up to
[00:07:12.240 --> 00:07:14.680]   160 gigabytes here versus
[00:07:14.680 --> 00:07:16.800]   the largest BERT model at 13,
[00:07:16.800 --> 00:07:18.560]   a substantial increase.
[00:07:18.560 --> 00:07:21.640]   The step size going all the way up to 500,000,
[00:07:21.640 --> 00:07:23.280]   whereas for BERT, it was a million.
[00:07:23.280 --> 00:07:26.040]   But remember, overall, there are many more examples
[00:07:26.040 --> 00:07:27.600]   being presented as a result of
[00:07:27.600 --> 00:07:32.240]   the batch size being so much larger for the Roberta models.
[00:07:32.240 --> 00:07:34.880]   Again, another familiar lesson
[00:07:34.880 --> 00:07:36.280]   from the deep learning era,
[00:07:36.280 --> 00:07:39.880]   more is better in terms of data and training time,
[00:07:39.880 --> 00:07:42.120]   especially when our goal is to create
[00:07:42.120 --> 00:07:44.520]   these pre-trained artifacts
[00:07:44.520 --> 00:07:47.280]   that are useful for fine-tuning.
[00:07:47.280 --> 00:07:50.120]   To round this out, I thought I'd mention that
[00:07:50.120 --> 00:07:52.280]   the Roberta team released two models,
[00:07:52.280 --> 00:07:54.600]   BASE and LARGE, which are directly
[00:07:54.600 --> 00:07:57.520]   comparable to the corresponding BERT artifacts.
[00:07:57.520 --> 00:08:00.280]   The BASE model has 12 layers,
[00:08:00.280 --> 00:08:02.320]   dimensionality of 768,
[00:08:02.320 --> 00:08:05.480]   and a feed-forward layer of 3072 for a total of
[00:08:05.480 --> 00:08:07.960]   125 million parameters which is
[00:08:07.960 --> 00:08:10.080]   more or less the same as BERT BASE.
[00:08:10.080 --> 00:08:12.240]   Then correspondingly, BERT LARGE has
[00:08:12.240 --> 00:08:15.680]   all the same basic settings as BERT BASE,
[00:08:15.680 --> 00:08:17.360]   and correspondingly, essentially,
[00:08:17.360 --> 00:08:21.680]   the same number of parameters at 355 million.
[00:08:21.680 --> 00:08:24.600]   As I said at the start of this screencast,
[00:08:24.600 --> 00:08:26.760]   Roberta was thorough, but even that is only
[00:08:26.760 --> 00:08:28.440]   a very partial exploration of
[00:08:28.440 --> 00:08:31.360]   the full design space suggested by the BERT model.
[00:08:31.360 --> 00:08:33.320]   For many more results,
[00:08:33.320 --> 00:08:34.960]   I highly recommend this paper,
[00:08:34.960 --> 00:08:37.720]   a primer in BERTology from Rogers et al.
[00:08:37.720 --> 00:08:40.280]   It's a little bit of an old paper at this point,
[00:08:40.280 --> 00:08:42.240]   so lots has happened since it was released,
[00:08:42.240 --> 00:08:45.000]   but nonetheless, it's very thorough and contains
[00:08:45.000 --> 00:08:47.680]   lots of insights about how best to set up
[00:08:47.680 --> 00:08:51.040]   these BERT style models for doing various things in NLP.
[00:08:51.040 --> 00:08:53.200]   So highly recommended as a companion
[00:08:53.200 --> 00:08:55.760]   to this little screencast.
[00:08:55.760 --> 00:09:05.760]   [BLANK_AUDIO]

