
[00:00:00.000 --> 00:00:04.320]   It's an individual link for the live stream itself.
[00:00:04.320 --> 00:00:07.840]   Cool.
[00:00:07.840 --> 00:00:10.400]   I see people popping on.
[00:00:10.400 --> 00:00:11.120]   Hello, folks.
[00:00:11.120 --> 00:00:12.240]   Are you guys joining us?
[00:00:12.240 --> 00:00:19.020]   Nice.
[00:00:25.720 --> 00:00:31.800]   And then I'm also going to pop over to our stream.
[00:00:31.800 --> 00:00:34.560]   Tyler, just by the way, all the intros you made,
[00:00:34.560 --> 00:00:35.680]   everyone took the meeting.
[00:00:35.680 --> 00:00:36.840]   Everyone was awesome.
[00:00:36.840 --> 00:00:39.720]   So that was really super helpful.
[00:00:39.720 --> 00:00:40.220]   Great.
[00:00:40.220 --> 00:00:41.600]   Happy to hear that.
[00:00:41.600 --> 00:00:42.120]   It's funny.
[00:00:42.120 --> 00:00:44.920]   I feel like I'm starting to learn the gossip
[00:00:44.920 --> 00:00:47.720]   in your domain.
[00:00:47.720 --> 00:00:51.000]   Yeah, it's--
[00:00:51.000 --> 00:00:52.440]   Because we are live.
[00:00:52.440 --> 00:00:53.080]   It's been a--
[00:00:53.080 --> 00:00:55.040]   And recording.
[00:00:55.040 --> 00:00:56.280]   The uptake of machine learning has
[00:00:56.280 --> 00:00:57.600]   been a little laggy in biology.
[00:00:57.600 --> 00:00:59.280]   But it's nice to see it start taking off.
[00:00:59.280 --> 00:01:02.480]   And now people are doing some pretty cool stuff with it.
[00:01:02.480 --> 00:01:03.200]   It's super cool.
[00:01:03.200 --> 00:01:05.120]   Now I ask people, are you like small molecule,
[00:01:05.120 --> 00:01:08.640]   or large molecule, or biological?
[00:01:08.640 --> 00:01:11.120]   Yeah, and everyone's got different models and approaches.
[00:01:11.120 --> 00:01:12.560]   It's pretty cool.
[00:01:12.560 --> 00:01:13.060]   Yeah.
[00:01:13.060 --> 00:01:19.200]   Where's everyone else joining us from?
[00:01:19.200 --> 00:01:20.640]   I just see one person answering.
[00:01:20.640 --> 00:01:24.120]   [VIDEO PLAYBACK]
[00:01:24.120 --> 00:01:45.160]   All right, I see people popping into the stream
[00:01:45.160 --> 00:01:47.320]   and as well as in the Zoom.
[00:01:47.320 --> 00:01:49.640]   We'll give it two more minutes, and then we're going to go.
[00:01:50.640 --> 00:01:55.280]   Yeah, for that, I'm going to put in the chat the direct link
[00:01:55.280 --> 00:01:56.280]   to the stream now.
[00:01:56.280 --> 00:02:02.960]   So if you want to share, feel free to send that link out.
[00:02:02.960 --> 00:02:03.460]   Thanks.
[00:02:03.460 --> 00:02:17.560]   All right, I'm going to kick us off in about one minute.
[00:02:17.560 --> 00:02:21.040]   [VIDEO PLAYBACK]
[00:02:21.040 --> 00:02:43.880]   I see Han is joining us.
[00:02:43.880 --> 00:02:45.600]   Hey, Han.
[00:02:45.600 --> 00:02:47.440]   Han was one of our first speakers,
[00:02:47.440 --> 00:02:48.480]   and he was really good.
[00:02:48.480 --> 00:03:01.640]   No math lesson from Charles this week?
[00:03:01.640 --> 00:03:05.720]   No math lesson from Charles this week.
[00:03:05.720 --> 00:03:07.200]   More coming soon.
[00:03:07.200 --> 00:03:08.200]   Yeah.
[00:03:08.200 --> 00:03:09.960]   Hey, Elvanya.
[00:03:09.960 --> 00:03:15.200]   Yeah, I just gave my final defense of my thesis last week,
[00:03:15.200 --> 00:03:18.920]   so I can get back on the webinar train.
[00:03:18.920 --> 00:03:20.160]   Nice.
[00:03:20.160 --> 00:03:21.480]   For us?
[00:03:21.480 --> 00:03:21.980]   Thanks.
[00:03:21.980 --> 00:03:30.960]   All right, I'm going to kick us off.
[00:03:30.960 --> 00:03:32.320]   Thank you, everyone, for coming.
[00:03:32.320 --> 00:03:33.480]   We really appreciate it.
[00:03:33.480 --> 00:03:36.680]   We know it's really late for some of you,
[00:03:36.680 --> 00:03:39.960]   and we appreciate that you still showed up here.
[00:03:39.960 --> 00:03:42.720]   This is our fifth time doing it, I think,
[00:03:42.720 --> 00:03:46.080]   and every time the quality of the speakers, the audience,
[00:03:46.080 --> 00:03:50.880]   and also just the whole environment gets better.
[00:03:50.880 --> 00:03:52.600]   We used to have technical difficulties,
[00:03:52.600 --> 00:03:56.560]   which we've slowly cut down on, so I'm very happy about that.
[00:03:56.560 --> 00:03:59.960]   And with that, I'm going to give this over to Lucas, who's
[00:03:59.960 --> 00:04:02.560]   going to introduce our speakers.
[00:04:02.560 --> 00:04:04.640]   Thanks, Elvanya.
[00:04:04.640 --> 00:04:06.400]   Yeah, I'm super excited about this.
[00:04:06.400 --> 00:04:11.280]   It's fun to see the wide diversity of speakers and talks.
[00:04:11.280 --> 00:04:13.200]   This week, for your entertainment,
[00:04:13.200 --> 00:04:17.960]   we have Tyler Shimko, who just defended his thesis for a PhD
[00:04:17.960 --> 00:04:21.720]   in genetics at Stanford, and then also founded a biotech
[00:04:21.720 --> 00:04:24.240]   startup, Trident Bioscience, which I'm super
[00:04:24.240 --> 00:04:26.920]   excited to learn more about.
[00:04:26.920 --> 00:04:29.560]   He's working on computational experimental technologies
[00:04:29.560 --> 00:04:32.680]   to expedite the discovery of useful proteins.
[00:04:32.680 --> 00:04:35.200]   And he's going to talk about one of my favorite topics, which
[00:04:35.200 --> 00:04:38.680]   is machine learning in biophysics.
[00:04:38.680 --> 00:04:43.960]   We also have Kostov Sinha, who is a PhD and also an intern
[00:04:43.960 --> 00:04:46.840]   at Facebook AI Research.
[00:04:46.840 --> 00:04:49.200]   He is going to be talking about his paper,
[00:04:49.200 --> 00:04:52.120]   Evaluating Logical Generalization in Graph
[00:04:52.120 --> 00:04:54.320]   Neural Networks, which I started reading just
[00:04:54.320 --> 00:04:56.480]   before prepping to do this.
[00:04:56.480 --> 00:05:01.560]   And I think it hurt my preparation for these intros.
[00:05:01.560 --> 00:05:04.520]   We also have Madison May, who I was just
[00:05:04.520 --> 00:05:08.200]   noticing his Twitter handle is @PragmaticML, I believe,
[00:05:08.200 --> 00:05:11.760]   which makes me want to listen to his talk right there.
[00:05:11.760 --> 00:05:15.920]   He's a co-founder and ML team lead at Indico Data Solutions
[00:05:15.920 --> 00:05:18.960]   in Boston, and is going to be talking
[00:05:18.960 --> 00:05:22.200]   about long-term context in transformers, definitely
[00:05:22.200 --> 00:05:23.840]   a hot topic.
[00:05:23.840 --> 00:05:28.240]   And then we also have Stacey, who is, A, in some sense,
[00:05:28.240 --> 00:05:33.280]   the only, the top, deep learning engineer at Weights & Biases.
[00:05:33.280 --> 00:05:37.040]   She's actually been with us since nearly the very beginning.
[00:05:37.040 --> 00:05:39.960]   And before that, she worked on productionizing machine
[00:05:39.960 --> 00:05:42.160]   learning systems at a number of companies,
[00:05:42.160 --> 00:05:45.520]   maybe most excitingly on content discovery and recommendation
[00:05:45.520 --> 00:05:46.600]   at Flickr.
[00:05:46.600 --> 00:05:48.240]   And she's going to be talking about one
[00:05:48.240 --> 00:05:51.200]   of my favorite parts of Weights & Biases
[00:05:51.200 --> 00:05:53.400]   that really is her brainchild, which
[00:05:53.400 --> 00:05:56.920]   is benchmarks for collaborative machine learning in some
[00:05:56.920 --> 00:05:59.320]   of the applications, I hope.
[00:05:59.320 --> 00:06:00.920]   So super excited.
[00:06:00.920 --> 00:06:02.320]   I will hand it over to Tyler.
[00:06:05.040 --> 00:06:05.760]   Thanks, Lucas.
[00:06:05.760 --> 00:06:08.440]   And I'll just share my screen, and then we'll get started.
[00:06:08.440 --> 00:06:12.880]   All right.
[00:06:12.880 --> 00:06:15.200]   So thank you guys for the invitation to speak today.
[00:06:15.200 --> 00:06:17.960]   I'm really excited to tell you about some of the work that
[00:06:17.960 --> 00:06:21.560]   came out of some of my thesis, as Lucas said,
[00:06:21.560 --> 00:06:25.560]   from Stanford in genetics, with a heavy focus on biophysics
[00:06:25.560 --> 00:06:27.880]   and how we can apply machine learning methods
[00:06:27.880 --> 00:06:32.160]   and where they might fit in best to the overall experimental
[00:06:32.160 --> 00:06:35.160]   pipeline of biotechnology.
[00:06:35.160 --> 00:06:37.280]   So a little bit more about me.
[00:06:37.280 --> 00:06:39.280]   I did my undergraduate degree in biology
[00:06:39.280 --> 00:06:42.000]   at the University of Utah, focused pretty heavily
[00:06:42.000 --> 00:06:45.760]   on wet lab biology at the time, and then transitioned in 2015
[00:06:45.760 --> 00:06:49.360]   to my PhD in genetics at Stanford,
[00:06:49.360 --> 00:06:52.160]   where I made the switch from a primarily wet lab
[00:06:52.160 --> 00:06:56.040]   background to a primarily dry lab or computational background
[00:06:56.040 --> 00:06:58.160]   and started learning and applying machine learning
[00:06:58.160 --> 00:07:00.080]   methods to the problems that we were facing
[00:07:00.080 --> 00:07:01.840]   in our lab at Stanford.
[00:07:01.840 --> 00:07:04.640]   And then recently defended my thesis about three weeks ago
[00:07:04.640 --> 00:07:08.320]   now, and shortly thereafter founded a biotech company
[00:07:08.320 --> 00:07:12.360]   called Trident Bioscience to take some of the methods
[00:07:12.360 --> 00:07:14.560]   and techniques that I developed at Stanford
[00:07:14.560 --> 00:07:16.720]   and transition them to commercial applications
[00:07:16.720 --> 00:07:20.240]   for the discovery and refinement of useful proteins.
[00:07:20.240 --> 00:07:23.880]   And now I want to start my talk with a very general overview,
[00:07:23.880 --> 00:07:25.800]   take you guys back to high school science class
[00:07:25.800 --> 00:07:28.240]   and the central dogma of molecular biology.
[00:07:28.240 --> 00:07:30.760]   And the central dogma states that the information flows
[00:07:30.760 --> 00:07:34.360]   from DNA to RNA to protein, this generally
[00:07:34.360 --> 00:07:39.360]   kind of one directional movement through transcription
[00:07:39.360 --> 00:07:42.440]   and translation, transcription of DNA into RNA,
[00:07:42.440 --> 00:07:44.440]   and then translation of the information contained
[00:07:44.440 --> 00:07:46.600]   in that RNA into protein.
[00:07:46.600 --> 00:07:48.920]   But that's really only half the story, this bit
[00:07:48.920 --> 00:07:51.440]   that you kind of get in high school science class.
[00:07:51.440 --> 00:07:54.320]   What happens to the protein is kind of lost
[00:07:54.320 --> 00:07:58.440]   in that process of explaining this central dogma.
[00:07:58.440 --> 00:08:00.600]   But that protein goes on to actually fold
[00:08:00.600 --> 00:08:02.360]   into a three-dimensional structure.
[00:08:02.360 --> 00:08:05.400]   And that three-dimensional structure is dynamic with time.
[00:08:05.400 --> 00:08:07.960]   It actually changes, and the shape
[00:08:07.960 --> 00:08:10.720]   shifts so that it can perform different functions
[00:08:10.720 --> 00:08:11.800]   for the cell.
[00:08:11.800 --> 00:08:13.720]   In addition, you can get these post-translational
[00:08:13.720 --> 00:08:16.080]   modifications or chemical group additions
[00:08:16.080 --> 00:08:18.680]   to these protein structures that can further
[00:08:18.680 --> 00:08:20.440]   modify their function.
[00:08:20.440 --> 00:08:24.040]   And then this ensemble of all these different structures
[00:08:24.040 --> 00:08:26.520]   are what actually carry out the various functions
[00:08:26.520 --> 00:08:29.880]   for a single protein sequence with any given cell.
[00:08:29.880 --> 00:08:32.120]   And when I say protein sequence or DNA sequence
[00:08:32.120 --> 00:08:33.600]   throughout this talk, what I mean
[00:08:33.600 --> 00:08:36.280]   is that each of these molecules are composed
[00:08:36.280 --> 00:08:38.320]   of different monomeric blocks.
[00:08:38.320 --> 00:08:40.560]   So they're polymers strung together
[00:08:40.560 --> 00:08:41.920]   with different LEGO-like building
[00:08:41.920 --> 00:08:43.880]   blocks in a linear chain.
[00:08:43.880 --> 00:08:45.480]   And it's the three-dimensional folding
[00:08:45.480 --> 00:08:47.800]   of that linear chain that really determines
[00:08:47.800 --> 00:08:52.200]   what the structure and ultimate function is going to be.
[00:08:52.200 --> 00:08:54.680]   And in the context of this talk, when I say function, what
[00:08:54.680 --> 00:08:56.440]   I really mean is binding.
[00:08:56.440 --> 00:08:59.560]   So the binding of proteins to each other,
[00:08:59.560 --> 00:09:03.400]   proteins to smaller peptides, so small chains of amino acids,
[00:09:03.400 --> 00:09:07.000]   or proteins to DNA or RNA.
[00:09:07.000 --> 00:09:09.080]   So you might be asking yourself, why are we really
[00:09:09.080 --> 00:09:12.960]   focused on this phenotype or this idea of binding
[00:09:12.960 --> 00:09:17.240]   from one protein to another protein or one protein to DNA?
[00:09:17.240 --> 00:09:18.840]   And the reason is that this is really
[00:09:18.840 --> 00:09:22.520]   how biology controls or tunes the kind of rheostat
[00:09:22.520 --> 00:09:25.840]   for different interactions and different cellular processes.
[00:09:25.840 --> 00:09:28.080]   And this biological rheostat can be
[00:09:28.080 --> 00:09:32.000]   defined by these two main axes, the first being affinity,
[00:09:32.000 --> 00:09:34.960]   or how tightly the different interactions take place,
[00:09:34.960 --> 00:09:36.360]   and the second being specificity,
[00:09:36.360 --> 00:09:39.640]   or how specific a protein recognizes its binding
[00:09:39.640 --> 00:09:41.360]   partner.
[00:09:41.360 --> 00:09:43.240]   And so if we divide this rheostat
[00:09:43.240 --> 00:09:46.520]   into different quadrants, we can start with the bottom left,
[00:09:46.520 --> 00:09:48.600]   where we have nonspecific interactions, essentially
[00:09:48.600 --> 00:09:50.640]   proteins and DNA just bumping into each other
[00:09:50.640 --> 00:09:52.400]   randomly in the cell.
[00:09:52.400 --> 00:09:54.480]   If we then go to the other extreme, a really high
[00:09:54.480 --> 00:09:56.840]   affinity, high specificity interaction,
[00:09:56.840 --> 00:09:59.520]   we have something like antibody antigen systems.
[00:09:59.520 --> 00:10:03.200]   These systems bind almost irreversibly to each other
[00:10:03.200 --> 00:10:04.880]   and with really high specificity.
[00:10:04.880 --> 00:10:08.640]   So an antibody can recognize its target protein
[00:10:08.640 --> 00:10:11.160]   in a sea of other proteins.
[00:10:11.160 --> 00:10:13.360]   If we then move down the affinity axis,
[00:10:13.360 --> 00:10:14.800]   we run into transcription factors,
[00:10:14.800 --> 00:10:18.440]   or proteins that bind DNA to regulate the transcription
[00:10:18.440 --> 00:10:20.200]   of downstream genes.
[00:10:20.200 --> 00:10:22.640]   So these are the proteins that play an integral role
[00:10:22.640 --> 00:10:25.800]   in determining which genes are expressed in which cells.
[00:10:25.800 --> 00:10:27.760]   And their binding is really important,
[00:10:27.760 --> 00:10:30.520]   but it tends to be pretty low affinity,
[00:10:30.520 --> 00:10:32.920]   but relatively high specificity.
[00:10:32.920 --> 00:10:35.360]   To fill out the quadrant, we can think about nucleosomes,
[00:10:35.360 --> 00:10:38.120]   or these proteins that the DNA in your cells
[00:10:38.120 --> 00:10:40.160]   is actually wrapped around.
[00:10:40.160 --> 00:10:42.320]   And these proteins play an integral role
[00:10:42.320 --> 00:10:44.920]   in actually packing the DNA into the nucleus
[00:10:44.920 --> 00:10:46.720]   of each individual cell.
[00:10:46.720 --> 00:10:48.760]   And they're a really high affinity interaction,
[00:10:48.760 --> 00:10:50.080]   but they have a low specificity.
[00:10:50.080 --> 00:10:54.360]   They generally bind DNA with lower sequence specificity
[00:10:54.360 --> 00:10:55.920]   than the transcription factors.
[00:10:55.920 --> 00:10:59.720]   And as you might expect, these types of interactions
[00:10:59.720 --> 00:11:02.120]   are really important for a lot of different applications
[00:11:02.120 --> 00:11:03.720]   in biotechnology.
[00:11:03.720 --> 00:11:05.440]   For instance, one thing that you might be familiar with
[00:11:05.440 --> 00:11:06.280]   is CRISPR.
[00:11:06.280 --> 00:11:11.280]   CRISPR relies on the base pairing between the RNA
[00:11:11.280 --> 00:11:14.640]   as part of the CRISPR complex,
[00:11:14.640 --> 00:11:17.240]   and the DNA sequence that it's trying to modify.
[00:11:17.240 --> 00:11:18.480]   And this has been commercialized
[00:11:18.480 --> 00:11:19.720]   by a bunch of different companies,
[00:11:19.720 --> 00:11:22.680]   Adatos and Caribou are just two of these.
[00:11:22.680 --> 00:11:24.680]   Another type of interaction that's particularly important
[00:11:24.680 --> 00:11:25.520]   is antibodies.
[00:11:25.520 --> 00:11:28.160]   And with all the COVID related research,
[00:11:28.160 --> 00:11:30.160]   I'm sure you guys are probably more familiar
[00:11:30.160 --> 00:11:32.520]   with antibodies than you might like to be,
[00:11:32.520 --> 00:11:34.520]   but they're used in a variety of different applications.
[00:11:34.520 --> 00:11:36.280]   They're of course used as therapeutics,
[00:11:36.280 --> 00:11:38.240]   as you've been hearing about lately,
[00:11:38.240 --> 00:11:39.640]   but they also are the basis
[00:11:39.640 --> 00:11:41.440]   for a lot of different pregnancy tests.
[00:11:41.440 --> 00:11:45.200]   So these antibodies are recognizing components in the urine
[00:11:45.200 --> 00:11:47.640]   that are indicative of a positive pregnancy.
[00:11:48.640 --> 00:11:50.600]   And finally, we have transcription factors,
[00:11:50.600 --> 00:11:52.520]   which is where I'm going to start my talk,
[00:11:52.520 --> 00:11:54.600]   because these interactions are really important
[00:11:54.600 --> 00:11:56.880]   for a lot of applications in synthetic biology.
[00:11:56.880 --> 00:11:58.640]   And companies like Amaris or Zymogen,
[00:11:58.640 --> 00:12:00.840]   which you may not have even heard of,
[00:12:00.840 --> 00:12:03.320]   are really interested in how transcription factors
[00:12:03.320 --> 00:12:05.320]   are binding and regulating downstream genes
[00:12:05.320 --> 00:12:07.440]   and how they might be able to modify that
[00:12:07.440 --> 00:12:11.480]   to regulate gene expression for their desired traits.
[00:12:11.480 --> 00:12:13.040]   And so as you might expect,
[00:12:13.040 --> 00:12:14.640]   given the importance of designing
[00:12:14.640 --> 00:12:16.520]   these different intermolecular interactions
[00:12:16.520 --> 00:12:19.600]   or these kind of binding interactions,
[00:12:19.600 --> 00:12:22.000]   we might want a set of tools that would allow us
[00:12:22.000 --> 00:12:25.280]   to develop and modify different interactions
[00:12:25.280 --> 00:12:26.720]   to suit a particular need.
[00:12:26.720 --> 00:12:30.200]   So if we think about what tools we might need
[00:12:30.200 --> 00:12:32.360]   to actually design these intermolecular interactions,
[00:12:32.360 --> 00:12:35.680]   they kind of break down into four general categories.
[00:12:35.680 --> 00:12:37.720]   The first and the ones that I think you guys
[00:12:37.720 --> 00:12:39.880]   came to this talk for is a model
[00:12:39.880 --> 00:12:43.240]   of how these interactions are actually taking place.
[00:12:43.240 --> 00:12:45.520]   We then need a method to translate
[00:12:45.520 --> 00:12:46.920]   those predicted interactions
[00:12:46.920 --> 00:12:49.760]   or the predicted mechanism of the interaction
[00:12:49.760 --> 00:12:53.840]   into a way to actually test the hypothesis
[00:12:53.840 --> 00:12:55.280]   that's made by that model.
[00:12:55.280 --> 00:12:57.480]   And so what this looks like in a lot
[00:12:57.480 --> 00:13:00.360]   of wet lab biology techniques
[00:13:00.360 --> 00:13:02.120]   is generating a set of sequences,
[00:13:02.120 --> 00:13:03.640]   what we call a library,
[00:13:03.640 --> 00:13:06.080]   that you can then run through a certain experiment
[00:13:06.080 --> 00:13:09.640]   and test the actual outcome of the DNA sequence
[00:13:09.640 --> 00:13:13.840]   or protein being produced by that library of input sequences.
[00:13:15.120 --> 00:13:16.280]   Taking this a step further,
[00:13:16.280 --> 00:13:18.560]   we need to actually test that pool
[00:13:18.560 --> 00:13:20.920]   or that library of sequences that we've generated
[00:13:20.920 --> 00:13:23.440]   via some experimental method to get a measurement
[00:13:23.440 --> 00:13:25.920]   about how well they're performing a certain task.
[00:13:25.920 --> 00:13:28.400]   And then finally, and ideally,
[00:13:28.400 --> 00:13:30.080]   we would like some sort of method
[00:13:30.080 --> 00:13:33.400]   to update our initial model,
[00:13:33.400 --> 00:13:35.720]   given the new experimental data.
[00:13:35.720 --> 00:13:38.400]   And hopefully we're able to build a closed loop system
[00:13:38.400 --> 00:13:40.000]   where we can get continuous improvement,
[00:13:40.000 --> 00:13:41.160]   a better and better model
[00:13:41.160 --> 00:13:42.960]   of how these interactions are taking place
[00:13:43.040 --> 00:13:45.200]   and the ways in which we might be able to change them
[00:13:45.200 --> 00:13:49.080]   to generate new functionally refined protein
[00:13:49.080 --> 00:13:50.360]   or DNA variants.
[00:13:50.360 --> 00:13:54.400]   And what I'm gonna focus on for the purposes of this talk
[00:13:54.400 --> 00:13:56.680]   is kind of this upper corner here.
[00:13:56.680 --> 00:13:59.080]   I'm gonna define what the model
[00:13:59.080 --> 00:14:01.320]   of these types of interactions might look like,
[00:14:01.320 --> 00:14:04.760]   and then start to think about how we might generate a method
[00:14:04.760 --> 00:14:09.760]   for making these hypotheses actually testable.
[00:14:09.760 --> 00:14:12.160]   And the reason that this is such a big problem in biology
[00:14:12.160 --> 00:14:14.440]   is that sequence space in general,
[00:14:14.440 --> 00:14:17.560]   and especially for proteins is incredibly massive.
[00:14:17.560 --> 00:14:19.320]   It's really hard for us to fathom
[00:14:19.320 --> 00:14:23.080]   how large sequence space is and how quickly it grows.
[00:14:23.080 --> 00:14:25.520]   So for DNA sequences,
[00:14:25.520 --> 00:14:28.000]   there are four possible nucleotides at each position.
[00:14:28.000 --> 00:14:30.520]   And so as you get a longer and longer chain
[00:14:30.520 --> 00:14:32.080]   of these nucleotides,
[00:14:32.080 --> 00:14:34.440]   the number of potential sequences
[00:14:34.440 --> 00:14:38.680]   that that length of chain can give rise to
[00:14:38.680 --> 00:14:41.800]   grows as four to the L, where L is the length of chain.
[00:14:41.800 --> 00:14:43.960]   So it grows exponentially.
[00:14:43.960 --> 00:14:45.240]   For proteins on the other hand,
[00:14:45.240 --> 00:14:47.240]   you have 20 choices per position.
[00:14:47.240 --> 00:14:49.200]   And so the number of possible sequences
[00:14:49.200 --> 00:14:51.040]   grows as 20 to the L.
[00:14:51.040 --> 00:14:52.400]   And so here, what I'm showing
[00:14:52.400 --> 00:14:55.240]   is given a certain protein sequence
[00:14:55.240 --> 00:14:56.440]   that we wanna start with,
[00:14:56.440 --> 00:14:58.560]   we can mutate a certain number of positions
[00:14:58.560 --> 00:15:01.200]   to all 19 other choices.
[00:15:01.200 --> 00:15:05.080]   And here on the Y-axis is the number of total mutants
[00:15:05.080 --> 00:15:07.560]   we would be required to screen
[00:15:07.560 --> 00:15:10.920]   if we wanna do this in a saturation mutagenesis
[00:15:10.920 --> 00:15:13.480]   or exhaustive fashion.
[00:15:13.480 --> 00:15:14.760]   And unfortunately,
[00:15:14.760 --> 00:15:17.120]   our most advanced experimental methods
[00:15:17.120 --> 00:15:19.760]   have a limitation of about 10 to the 11 sequences
[00:15:19.760 --> 00:15:22.320]   that they can screen in a single experiment.
[00:15:22.320 --> 00:15:26.960]   And so this limitation really defines our upper boundary
[00:15:26.960 --> 00:15:30.840]   for what we can do in an experimental wet lab technique.
[00:15:30.840 --> 00:15:32.840]   So to some extent, we must be selective
[00:15:32.840 --> 00:15:34.920]   in the design of these protein sequences
[00:15:34.920 --> 00:15:38.280]   that we wanna screen in high throughput libraries.
[00:15:38.280 --> 00:15:40.960]   And to me, this really defines the best place
[00:15:40.960 --> 00:15:42.920]   for a home for deep learning methods
[00:15:42.920 --> 00:15:45.280]   because we have these high throughput screening methods
[00:15:45.280 --> 00:15:46.600]   available to us.
[00:15:46.600 --> 00:15:50.000]   The cost of any given prediction being incorrect
[00:15:50.000 --> 00:15:51.000]   is relatively low.
[00:15:51.000 --> 00:15:53.360]   We're not using this to diagnose a disease
[00:15:53.360 --> 00:15:55.240]   or to immediately treat a disease,
[00:15:55.240 --> 00:15:57.160]   but the value of the informed search
[00:15:57.160 --> 00:15:59.600]   through pattern recognition is really high.
[00:15:59.600 --> 00:16:02.120]   So if we can recognize patterns in the existing data
[00:16:02.120 --> 00:16:03.920]   and use that to inform the sequences
[00:16:03.920 --> 00:16:05.320]   that we include in our library,
[00:16:05.320 --> 00:16:06.880]   it could be really valuable.
[00:16:08.200 --> 00:16:09.040]   And as a case study,
[00:16:09.040 --> 00:16:10.840]   I wanna present the first project
[00:16:10.840 --> 00:16:12.600]   that I worked on during my PhD,
[00:16:12.600 --> 00:16:15.600]   which is characterizing and modeling
[00:16:15.600 --> 00:16:17.080]   the behavior of transcription factors
[00:16:17.080 --> 00:16:20.120]   and how they bind to DNA sequences.
[00:16:20.120 --> 00:16:21.600]   So as I mentioned earlier in the talk,
[00:16:21.600 --> 00:16:23.160]   these transcription factors bind to DNA
[00:16:23.160 --> 00:16:25.560]   and they do so in a sequence specific manner.
[00:16:25.560 --> 00:16:27.160]   And so when I say sequence specific manner,
[00:16:27.160 --> 00:16:31.040]   I mean that the given protein is binding to DNA
[00:16:31.040 --> 00:16:34.560]   with a preference for certain short DNA motifs.
[00:16:34.560 --> 00:16:36.920]   An example of one is given here.
[00:16:36.920 --> 00:16:37.960]   So this is FO4.
[00:16:37.960 --> 00:16:39.600]   It's a transcription factor involved
[00:16:39.600 --> 00:16:43.520]   in helping yeast fungi respond to phosphate starvation,
[00:16:43.520 --> 00:16:45.800]   hence the name FO4.
[00:16:45.800 --> 00:16:48.080]   And the sequence preference is given here.
[00:16:48.080 --> 00:16:50.080]   And so the dominant nucleotides
[00:16:50.080 --> 00:16:53.800]   are these ones at the middle, CACGTG,
[00:16:53.800 --> 00:16:56.880]   also known, and you might hear me say EBOX motif.
[00:16:56.880 --> 00:17:00.840]   EBOX motif generally refers to this CACGTG pattern here
[00:17:00.840 --> 00:17:01.760]   at the center.
[00:17:01.760 --> 00:17:06.400]   But unfortunately, or fortunately for biology,
[00:17:06.400 --> 00:17:07.640]   families of transcription factors
[00:17:07.640 --> 00:17:10.320]   often bind very similar sequences.
[00:17:10.320 --> 00:17:13.120]   And so here we have a related transcription factor, CBF1,
[00:17:13.120 --> 00:17:17.440]   that you can see also likes to bind this CACGTG motif.
[00:17:17.440 --> 00:17:19.960]   And so this raises an interesting question,
[00:17:19.960 --> 00:17:21.520]   which is, you know,
[00:17:21.520 --> 00:17:24.200]   if they have very similar binding preferences,
[00:17:24.200 --> 00:17:26.120]   do they actually just regulate the same genes?
[00:17:26.120 --> 00:17:29.080]   Are they fully redundant in their function?
[00:17:29.080 --> 00:17:32.720]   And if we look at the gene regulatory networks, so here,
[00:17:32.720 --> 00:17:35.280]   the transcription factor is given
[00:17:35.280 --> 00:17:36.920]   as these kind of central nodes,
[00:17:36.920 --> 00:17:38.640]   and all of the genes that they regulate
[00:17:38.640 --> 00:17:40.880]   are the ones connected to those central nodes.
[00:17:40.880 --> 00:17:42.200]   And what we see is that there's actually
[00:17:42.200 --> 00:17:43.960]   a very small amount of overlap
[00:17:43.960 --> 00:17:46.320]   between the genes regulated by FO4
[00:17:46.320 --> 00:17:48.360]   and the genes regulated by CBF1.
[00:17:48.360 --> 00:17:52.400]   And so the question that we wanted to answer was,
[00:17:52.400 --> 00:17:55.160]   if these core binding motifs, so that CACGTG,
[00:17:55.160 --> 00:17:58.240]   the EBOX motif, is nearly identical,
[00:17:58.240 --> 00:18:00.200]   what differentiates the target binding sites
[00:18:00.200 --> 00:18:03.240]   for one transcription factor over the other
[00:18:03.240 --> 00:18:05.360]   in the actual genome of a living organism?
[00:18:05.360 --> 00:18:08.680]   And so what we have to understand
[00:18:08.680 --> 00:18:10.440]   is that these binding site sequences
[00:18:10.440 --> 00:18:12.920]   sit in different contexts within the genome.
[00:18:12.920 --> 00:18:15.520]   They're not just floating out as a, you know,
[00:18:15.520 --> 00:18:18.960]   six or eight mer sequence in the cell.
[00:18:18.960 --> 00:18:21.160]   They're actually connected to a bunch of other DNA,
[00:18:21.160 --> 00:18:23.520]   and that DNA has its own sequence.
[00:18:23.520 --> 00:18:26.640]   And so our hypothesis was that it might be these sequences
[00:18:26.640 --> 00:18:30.680]   that sit outside of that main motif
[00:18:30.680 --> 00:18:32.280]   that actually govern whether
[00:18:32.280 --> 00:18:34.720]   the transcription factor is going to bind or not.
[00:18:34.720 --> 00:18:40.360]   And so we wanted to look at all possible arrangements,
[00:18:40.360 --> 00:18:42.360]   so all possible genomic configurations
[00:18:42.360 --> 00:18:43.760]   for these given EBOX motifs,
[00:18:43.760 --> 00:18:47.000]   and then measure the binding for FO4 and CBF1
[00:18:47.000 --> 00:18:48.680]   against that target library.
[00:18:48.680 --> 00:18:53.800]   And the way we did this was we made this DNA library
[00:18:53.800 --> 00:18:56.000]   that we're able to use high throughput sequencing on,
[00:18:56.000 --> 00:18:58.320]   so we're able to actually measure and read out
[00:18:58.320 --> 00:19:00.240]   each of these individual sequences,
[00:19:00.320 --> 00:19:02.800]   where we fix that EBOX motif at the center,
[00:19:02.800 --> 00:19:06.880]   CACGTG here, and then we allowed the bases next to it
[00:19:06.880 --> 00:19:11.600]   to be any of the four other nucleotides, so ACG or T.
[00:19:11.600 --> 00:19:15.040]   This generated about a million sequences in total,
[00:19:15.040 --> 00:19:18.920]   and represents an attempt to characterize binding
[00:19:18.920 --> 00:19:20.640]   that's actually relatively large.
[00:19:20.640 --> 00:19:22.840]   So we don't have a lot of really good methods
[00:19:22.840 --> 00:19:24.440]   to measure a million sequences
[00:19:24.440 --> 00:19:26.720]   with the resolution that we wanted to.
[00:19:26.720 --> 00:19:27.960]   And so along with a postdoc
[00:19:27.960 --> 00:19:29.640]   and another grad student in the lab,
[00:19:29.640 --> 00:19:31.720]   I developed this BET-seq method,
[00:19:31.720 --> 00:19:35.160]   which stands for Binding Energy Topographies by Sequencing,
[00:19:35.160 --> 00:19:37.320]   and it allows us to measure these transcription factor
[00:19:37.320 --> 00:19:39.560]   binding specificities in high throughput,
[00:19:39.560 --> 00:19:42.480]   importantly, with energetic information behind it.
[00:19:42.480 --> 00:19:45.360]   So it has to be a biophysical measurement
[00:19:45.360 --> 00:19:47.360]   in order for us to be really interested
[00:19:47.360 --> 00:19:49.640]   in Polly's lab, my PI's lab,
[00:19:49.640 --> 00:19:52.440]   and so we wanted to do this with really high resolution
[00:19:52.440 --> 00:19:54.880]   and generate these physically meaningful values
[00:19:54.880 --> 00:19:57.160]   for each measured transcription factor
[00:19:57.160 --> 00:19:59.280]   DNA sequence interaction.
[00:19:59.280 --> 00:20:00.160]   And the way that we did this
[00:20:00.160 --> 00:20:02.400]   was we have a small microfluidic device
[00:20:02.400 --> 00:20:05.040]   that you can see here on the left-hand side,
[00:20:05.040 --> 00:20:08.080]   and this device is about the size of a postage stamp
[00:20:08.080 --> 00:20:10.080]   and has a bunch of these inputs
[00:20:10.080 --> 00:20:12.800]   and these hydraulically activated valves.
[00:20:12.800 --> 00:20:16.800]   So here's kind of a schematic of how the individual chambers
[00:20:16.800 --> 00:20:19.080]   within this microfluidic device look,
[00:20:19.080 --> 00:20:22.520]   and each one has what we call this button valve at the top
[00:20:22.520 --> 00:20:26.880]   that allows us to compress and actually stop the interaction
[00:20:26.880 --> 00:20:28.120]   of a transcription factor
[00:20:28.120 --> 00:20:30.680]   with a bunch of different DNA sequences.
[00:20:30.680 --> 00:20:32.280]   So what this allows us to do
[00:20:32.280 --> 00:20:34.440]   is bind transcription factor to the surface here,
[00:20:34.440 --> 00:20:37.120]   flow in DNA, we're going to select
[00:20:37.120 --> 00:20:39.280]   for the things that bind, they're going to bind,
[00:20:39.280 --> 00:20:40.720]   we're then gonna trap them
[00:20:40.720 --> 00:20:43.880]   and separate them from the sequences that don't bind.
[00:20:43.880 --> 00:20:46.000]   And so this separation is what allows us
[00:20:46.000 --> 00:20:49.080]   to make a measurement, an energetic measurement
[00:20:49.080 --> 00:20:50.560]   of the affinity of different
[00:20:50.560 --> 00:20:53.560]   transcription factor DNA interactions.
[00:20:53.560 --> 00:20:55.680]   So then we can put each library,
[00:20:55.680 --> 00:20:58.120]   so here we have the bound and the unbound libraries,
[00:20:58.120 --> 00:21:00.400]   go through high throughput DNA sequencing
[00:21:00.400 --> 00:21:03.320]   on a machine made by usually Illumina.
[00:21:03.320 --> 00:21:05.840]   And then this generates a library
[00:21:05.840 --> 00:21:08.040]   where we can actually order these sequences
[00:21:08.040 --> 00:21:11.520]   based on the preference of each given transcription factor
[00:21:11.520 --> 00:21:16.520]   to these kind of context sequences outside the E-box domain.
[00:21:16.520 --> 00:21:18.600]   And so here you can see,
[00:21:18.600 --> 00:21:20.680]   we can potentially look at these,
[00:21:20.680 --> 00:21:23.200]   all of these DNA sequences in the library
[00:21:23.200 --> 00:21:25.280]   and rank them from highest affinity,
[00:21:25.280 --> 00:21:27.040]   which in Delta Delta G space,
[00:21:27.040 --> 00:21:29.280]   so Delta Delta G is just a measurement
[00:21:29.280 --> 00:21:32.120]   for how high affinity a sequence binds
[00:21:32.120 --> 00:21:35.360]   and it has a physical meaning behind it.
[00:21:35.360 --> 00:21:37.960]   And the more negative that value is,
[00:21:37.960 --> 00:21:41.160]   the more tightly it's bound to the transcription factor.
[00:21:41.160 --> 00:21:43.040]   So more negative equals tighter binding,
[00:21:43.040 --> 00:21:45.120]   more positive equals looser binding.
[00:21:45.120 --> 00:21:48.920]   And so we can do this for the entire library of sequences.
[00:21:48.920 --> 00:21:51.680]   But unfortunately, as I said before,
[00:21:51.680 --> 00:21:54.400]   the methods that already exist for doing this
[00:21:54.400 --> 00:21:57.240]   don't really scale well to the million sequences
[00:21:57.240 --> 00:21:58.840]   that we have in our library.
[00:21:58.840 --> 00:22:02.480]   And the reason is because experimental noise
[00:22:02.480 --> 00:22:05.760]   really inhibits these energetic measurements.
[00:22:05.760 --> 00:22:07.000]   The two reasons behind that are
[00:22:07.000 --> 00:22:08.560]   that we can't calculate a binding energy
[00:22:08.560 --> 00:22:09.640]   for something we don't observe.
[00:22:09.640 --> 00:22:11.000]   So for a million sequences,
[00:22:11.000 --> 00:22:13.880]   you actually have to use quite a bit of sequencing
[00:22:13.880 --> 00:22:16.840]   in order to see every sequence in that library.
[00:22:16.840 --> 00:22:20.360]   And for low count values,
[00:22:20.360 --> 00:22:22.640]   so things where we only observe a few of those
[00:22:22.640 --> 00:22:24.080]   of that individual sequence,
[00:22:24.080 --> 00:22:25.920]   a few of those molecules,
[00:22:25.920 --> 00:22:27.560]   the values are extremely noisy,
[00:22:27.560 --> 00:22:29.480]   which make our ultimate energetic measurements
[00:22:29.480 --> 00:22:30.320]   really noisy.
[00:22:30.320 --> 00:22:33.000]   And so we thought about this for a while
[00:22:33.000 --> 00:22:34.480]   and the solution that we came up with
[00:22:34.480 --> 00:22:36.360]   was to simulate this process.
[00:22:36.360 --> 00:22:38.600]   So the advantage of working
[00:22:38.600 --> 00:22:40.760]   in a biophysically meaningful space
[00:22:40.760 --> 00:22:43.160]   is that we know how these things should interact
[00:22:43.160 --> 00:22:44.160]   in a perfect world,
[00:22:44.160 --> 00:22:46.000]   and we can just simulate the experiment
[00:22:46.000 --> 00:22:49.280]   and see how well we do for different depths of sequencing.
[00:22:49.280 --> 00:22:51.120]   And the important thing to recognize here
[00:22:51.120 --> 00:22:53.960]   is that as you go to a deeper depth of sequencing,
[00:22:53.960 --> 00:22:54.920]   you're getting more reads,
[00:22:54.920 --> 00:22:56.200]   you're getting better data,
[00:22:56.200 --> 00:22:58.280]   but it's also more and more expensive.
[00:22:58.280 --> 00:23:01.960]   And so we simulated one of these Betsy experiments
[00:23:01.960 --> 00:23:03.880]   and here on the bottom,
[00:23:03.880 --> 00:23:05.560]   we get the mean reads per sequence.
[00:23:05.560 --> 00:23:07.280]   So the number of times we're observing
[00:23:07.280 --> 00:23:09.280]   each individual DNA sequence,
[00:23:09.280 --> 00:23:12.000]   and then on the Y axis here,
[00:23:12.000 --> 00:23:13.960]   we're showing the median value
[00:23:13.960 --> 00:23:16.720]   for two different measurements of success.
[00:23:16.720 --> 00:23:18.480]   And the first is the fraction of the library
[00:23:18.480 --> 00:23:19.320]   that we're observing.
[00:23:19.320 --> 00:23:21.800]   We wanna be able to see all million sequences.
[00:23:22.040 --> 00:23:24.160]   And then the second is the R squared value
[00:23:24.160 --> 00:23:27.600]   with the true defined value
[00:23:27.600 --> 00:23:29.680]   that we put into the simulation.
[00:23:29.680 --> 00:23:31.880]   And we can see that as we start out on the left,
[00:23:31.880 --> 00:23:34.640]   we end up with these really kind of garbage datasets
[00:23:34.640 --> 00:23:37.360]   where we're only observing a fraction of our library
[00:23:37.360 --> 00:23:38.720]   and the measurements that we're getting
[00:23:38.720 --> 00:23:41.840]   don't actually correlate that well with reality.
[00:23:41.840 --> 00:23:42.680]   On the other side,
[00:23:42.680 --> 00:23:44.160]   we have a really expensive experiment
[00:23:44.160 --> 00:23:45.400]   that yields beautiful data
[00:23:45.400 --> 00:23:48.000]   where we're observing nearly all of the library.
[00:23:48.000 --> 00:23:51.560]   And our data quality is really good as well.
[00:23:51.560 --> 00:23:53.120]   The R squared value is really high
[00:23:53.120 --> 00:23:55.480]   with the expected values.
[00:23:55.480 --> 00:23:57.720]   But we figured that we might be able to sit
[00:23:57.720 --> 00:23:59.640]   kind of in this Goldilocks zone here in the middle
[00:23:59.640 --> 00:24:02.520]   where we're observing almost all of our library
[00:24:02.520 --> 00:24:04.480]   or all of our library,
[00:24:04.480 --> 00:24:07.280]   but the R squared values might not be perfect.
[00:24:07.280 --> 00:24:09.040]   And so that really set up the expectation
[00:24:09.040 --> 00:24:10.800]   that if we sequence one of these libraries,
[00:24:10.800 --> 00:24:13.640]   if we perform the experiment at really low read depth,
[00:24:13.640 --> 00:24:17.520]   what we're gonna get out is kind of not reproducible
[00:24:17.520 --> 00:24:20.680]   and is going to look kind of like a circular pattern.
[00:24:20.680 --> 00:24:22.400]   In fact, when we ran the experiment,
[00:24:22.400 --> 00:24:24.040]   we ran it twice, two different replicates,
[00:24:24.040 --> 00:24:27.160]   and we do see this indicative circle pattern
[00:24:27.160 --> 00:24:29.520]   where each measurement does not correlate
[00:24:29.520 --> 00:24:32.360]   with the same measurement made in a different replicate.
[00:24:32.360 --> 00:24:34.320]   But since we had the simulation data in hand,
[00:24:34.320 --> 00:24:36.720]   we were able to say that we expected that,
[00:24:36.720 --> 00:24:38.720]   made the change to a higher read depth,
[00:24:38.720 --> 00:24:41.000]   and we ended up getting better data out.
[00:24:41.000 --> 00:24:42.240]   But again, the problem with this
[00:24:42.240 --> 00:24:44.120]   is that if we run all of these experiments
[00:24:44.120 --> 00:24:44.960]   at really high read depth,
[00:24:44.960 --> 00:24:47.120]   it's gonna cost us a lot of money.
[00:24:47.120 --> 00:24:49.480]   And we'd really like to avoid that.
[00:24:49.480 --> 00:24:51.160]   So we thought to ourselves,
[00:24:51.160 --> 00:24:53.480]   we might be able to use some type of modeling
[00:24:53.480 --> 00:24:57.440]   to extract and separate the signal in the data
[00:24:57.440 --> 00:25:00.400]   from the low read depth experiments from the noise.
[00:25:00.400 --> 00:25:03.800]   And so we wanted to see if we could do that.
[00:25:03.800 --> 00:25:06.480]   And the thought behind this was that
[00:25:06.480 --> 00:25:08.320]   we already use transcription factor binding models.
[00:25:08.320 --> 00:25:11.520]   In fact, I presented one earlier in this talk.
[00:25:11.520 --> 00:25:14.680]   So this motif that I showed you at the very beginning,
[00:25:14.680 --> 00:25:16.880]   it's important to ask how we actually got to that point.
[00:25:16.880 --> 00:25:19.600]   And it's through a very simple linear model
[00:25:19.600 --> 00:25:22.000]   that we can go from a bunch of different sequences
[00:25:22.000 --> 00:25:24.120]   to one of these motifs.
[00:25:24.120 --> 00:25:25.920]   So we asked, how did we get here?
[00:25:25.920 --> 00:25:29.160]   Someone previously had run a bunch of DNA sequences,
[00:25:29.160 --> 00:25:32.600]   did the same kind of enrichment step that we did earlier,
[00:25:32.600 --> 00:25:35.360]   pulled out a bunch of sequences
[00:25:35.360 --> 00:25:37.600]   that bound to the transcription factor,
[00:25:37.600 --> 00:25:38.880]   performed alignment,
[00:25:38.880 --> 00:25:41.200]   and then noticed that in all of the sequences that bound,
[00:25:41.200 --> 00:25:43.080]   there was a common pattern.
[00:25:43.080 --> 00:25:44.960]   This CACGTG pattern,
[00:25:44.960 --> 00:25:47.680]   and potentially this flanking nucleotide here,
[00:25:47.680 --> 00:25:49.640]   upstream and downstream.
[00:25:49.640 --> 00:25:51.080]   And from that, they were able to generate
[00:25:51.080 --> 00:25:53.400]   what we call a consensus binding site.
[00:25:53.400 --> 00:25:57.520]   So a single sequence that is generally enriched
[00:25:57.520 --> 00:26:00.640]   in the binding sites for these transcription factors.
[00:26:00.640 --> 00:26:03.840]   But an important assumption in this methodology
[00:26:03.840 --> 00:26:06.960]   is that the contribution of each nucleotide to binding
[00:26:06.960 --> 00:26:08.280]   is totally independent.
[00:26:08.280 --> 00:26:13.240]   And so we didn't think that that assumption was that good.
[00:26:13.240 --> 00:26:15.200]   We wanted to ask, is this assumption of independence
[00:26:15.200 --> 00:26:16.560]   actually valid?
[00:26:16.560 --> 00:26:19.840]   And we knew, because the time that we were doing this work,
[00:26:19.840 --> 00:26:22.240]   neural network models were coming into vogue.
[00:26:22.240 --> 00:26:24.200]   We knew that nonlinear models would enable us
[00:26:24.200 --> 00:26:25.920]   to do the same type of modeling,
[00:26:25.920 --> 00:26:27.320]   but in an assumption-free fashion.
[00:26:27.320 --> 00:26:30.000]   So we wouldn't be forced into assuming that
[00:26:30.000 --> 00:26:32.360]   the contribution of each nucleotide in the binding site
[00:26:32.360 --> 00:26:33.920]   was contributing independently.
[00:26:33.920 --> 00:26:39.160]   And so we built ourselves a nice little neural network model.
[00:26:39.160 --> 00:26:41.360]   We feed in the DNA sequence here
[00:26:41.360 --> 00:26:43.240]   as a one-hot encoded vector,
[00:26:43.240 --> 00:26:46.920]   have a couple intermediate layers with nonlinearities,
[00:26:46.920 --> 00:26:49.760]   and then we're gonna output the predicted delta-delta-G value
[00:26:49.760 --> 00:26:53.320]   and train on our experimentally measured values.
[00:26:53.320 --> 00:26:55.600]   And when you work on deep learning and biology,
[00:26:55.600 --> 00:26:59.200]   you run into a bunch of people that aren't too keen
[00:26:59.200 --> 00:27:02.360]   to trust that your magic neural network box model
[00:27:02.360 --> 00:27:04.320]   is actually accurate.
[00:27:04.320 --> 00:27:06.200]   And so you have to do something to prove to them
[00:27:06.200 --> 00:27:09.880]   that the data that you get out actually reflects reality.
[00:27:09.880 --> 00:27:13.080]   And we had another grad student in the lab, Arjun,
[00:27:13.080 --> 00:27:16.480]   who was kind enough to measure a bunch of the sequences
[00:27:16.480 --> 00:27:18.920]   that we had looked at on an orthogonal platform,
[00:27:18.920 --> 00:27:21.200]   so based on the same technology,
[00:27:21.200 --> 00:27:23.000]   but a much higher resolution technique
[00:27:23.000 --> 00:27:24.520]   that's much lower throughput.
[00:27:24.520 --> 00:27:28.120]   And what he was able to find and show
[00:27:28.120 --> 00:27:31.200]   was that if we take our unprocessed, so our raw data,
[00:27:31.200 --> 00:27:32.280]   the measurements that we had made
[00:27:32.280 --> 00:27:34.240]   without any type of correction,
[00:27:34.240 --> 00:27:36.040]   they don't correlate super well with the measurements
[00:27:36.040 --> 00:27:38.440]   that he had made in this orthogonal method.
[00:27:38.440 --> 00:27:40.520]   But if we first run it through our neural network
[00:27:40.520 --> 00:27:42.840]   and take the prediction output from that
[00:27:42.840 --> 00:27:45.480]   and compare it to his orthogonal measurement,
[00:27:45.480 --> 00:27:46.600]   they do correlate really well.
[00:27:46.600 --> 00:27:48.080]   So this gave us a lot of confidence
[00:27:48.080 --> 00:27:49.360]   that what we were seeing
[00:27:49.360 --> 00:27:52.640]   was good separation of signal from noise.
[00:27:52.640 --> 00:27:55.800]   But we also understood that in exchange
[00:27:55.800 --> 00:27:57.640]   for the flexibility of neural network models,
[00:27:57.640 --> 00:28:00.160]   we pay a price in interpretability.
[00:28:00.160 --> 00:28:02.800]   So we're not able to have that nice little motif anymore
[00:28:02.800 --> 00:28:03.680]   where we can say, okay,
[00:28:03.680 --> 00:28:05.920]   these are the contributions to binding.
[00:28:05.920 --> 00:28:08.040]   And so we were looking for a way around that
[00:28:08.040 --> 00:28:11.680]   and we wanted to take a look at how well
[00:28:11.680 --> 00:28:13.080]   each of the existing models
[00:28:13.080 --> 00:28:16.000]   that were based on principled biophysical approaches
[00:28:16.000 --> 00:28:19.600]   can explain what's learned by the neural network models.
[00:28:19.600 --> 00:28:21.200]   So we basically wanted to ask
[00:28:21.200 --> 00:28:23.000]   how far can a mononucleotide model
[00:28:23.000 --> 00:28:25.120]   or that kind of binding motif model
[00:28:25.120 --> 00:28:27.640]   that I showed you in the very beginning,
[00:28:27.640 --> 00:28:29.520]   how far can that get us in explaining
[00:28:29.520 --> 00:28:30.960]   what the neural network has learned?
[00:28:30.960 --> 00:28:33.320]   And so if we plot the mononucleotides,
[00:28:33.320 --> 00:28:36.880]   the linear model prediction down here on the X-axis
[00:28:36.880 --> 00:28:38.520]   and the neural network prediction on the Y-axis,
[00:28:38.520 --> 00:28:40.440]   we see that for FO4,
[00:28:40.440 --> 00:28:42.480]   that linear approximation is actually pretty good.
[00:28:42.480 --> 00:28:45.480]   It explains binding behavior pretty well.
[00:28:45.480 --> 00:28:48.360]   But for CBF1 on the other hand, it doesn't do so well.
[00:28:48.360 --> 00:28:49.800]   There's a lot of variability
[00:28:49.800 --> 00:28:52.800]   for any given mononucleotide predicted Delta Delta G
[00:28:52.800 --> 00:28:54.800]   that can't be explained
[00:28:54.800 --> 00:28:57.440]   only by those mononucleotide effects.
[00:28:57.440 --> 00:29:01.440]   And so there's another method for doing this
[00:29:01.440 --> 00:29:04.520]   that adds a little bit more biophysical information,
[00:29:04.520 --> 00:29:06.240]   injects a bit more domain knowledge
[00:29:06.240 --> 00:29:08.040]   into this modeling process.
[00:29:08.040 --> 00:29:10.160]   And that's considering dinucleotide effects.
[00:29:10.160 --> 00:29:12.760]   And dinucleotide effects are just combinations
[00:29:12.760 --> 00:29:16.600]   of different mononucleotides at different positions.
[00:29:16.600 --> 00:29:20.480]   And these dinucleotide effects are really important
[00:29:20.480 --> 00:29:23.160]   for a bunch of different functions,
[00:29:23.160 --> 00:29:26.040]   but there's reason to believe that for FO4 and CBF1,
[00:29:26.040 --> 00:29:28.480]   especially they might be important.
[00:29:28.480 --> 00:29:31.560]   And so what I'm showing here is the actual structure
[00:29:31.560 --> 00:29:32.880]   of the FO4 protein,
[00:29:33.320 --> 00:29:36.600]   which are these kind of blue squiggly lines up here,
[00:29:36.600 --> 00:29:38.760]   and then the DNA that it's bound to,
[00:29:38.760 --> 00:29:40.720]   the green helix down here.
[00:29:40.720 --> 00:29:43.360]   And I'm showing kind of two views of that
[00:29:43.360 --> 00:29:45.000]   offset by 90 degrees.
[00:29:45.000 --> 00:29:48.640]   And there's two different types of dinucleotide effects
[00:29:48.640 --> 00:29:50.040]   that we wanted to consider.
[00:29:50.040 --> 00:29:51.560]   The first were adjacent
[00:29:51.560 --> 00:29:54.040]   or nearest neighbor dinucleotide effects.
[00:29:54.040 --> 00:29:56.520]   These effects are a single base
[00:29:56.520 --> 00:29:58.600]   and the base pair right next to it.
[00:29:58.600 --> 00:29:59.920]   And these are really important
[00:29:59.920 --> 00:30:03.000]   for determining the actual three-dimensional shape of DNA.
[00:30:03.000 --> 00:30:05.080]   And so I know that I've been presenting DNA here
[00:30:05.080 --> 00:30:07.880]   as a linear sequence of these letters,
[00:30:07.880 --> 00:30:09.440]   but it actually exists in your cells
[00:30:09.440 --> 00:30:11.200]   as this three-dimensional helix
[00:30:11.200 --> 00:30:14.400]   that can bend, rotate, compact, expand.
[00:30:14.400 --> 00:30:16.800]   And a lot of those shape properties
[00:30:16.800 --> 00:30:18.560]   are determined by the nucleotides
[00:30:18.560 --> 00:30:20.760]   and how they stack right next to each other.
[00:30:20.760 --> 00:30:23.000]   On the other hand,
[00:30:23.000 --> 00:30:24.960]   we realized that these gapped nucleotides
[00:30:24.960 --> 00:30:27.520]   or dinucleotide effects
[00:30:27.520 --> 00:30:29.560]   that are not present in adjacent positions
[00:30:29.560 --> 00:30:30.560]   might be really important
[00:30:30.560 --> 00:30:34.080]   because FO4 and CBF1 are actually dimer proteins.
[00:30:34.080 --> 00:30:36.880]   And so there's two individual protein chains
[00:30:36.880 --> 00:30:40.200]   that bind together and co-bind that DNA complex.
[00:30:40.200 --> 00:30:44.440]   And because these tails of FO4 and CBF1
[00:30:44.440 --> 00:30:47.560]   sit within these grooves of the DNA,
[00:30:47.560 --> 00:30:49.760]   the contacts that are distal to one another
[00:30:49.760 --> 00:30:52.400]   might be actually important in the binding process.
[00:30:52.400 --> 00:30:55.440]   So if we add these dinucleotide features
[00:30:55.440 --> 00:30:56.440]   to our linear models
[00:30:56.440 --> 00:30:58.760]   and then compare them back to the neural network models,
[00:30:58.760 --> 00:31:00.720]   what we see is that for FO4,
[00:31:00.720 --> 00:31:03.080]   it nearly perfectly explains binding at this point.
[00:31:03.080 --> 00:31:05.000]   There's very little variation left.
[00:31:05.000 --> 00:31:07.880]   And for CBF1, it does a pretty significant job
[00:31:07.880 --> 00:31:11.480]   indicating a significant improvement
[00:31:11.480 --> 00:31:14.360]   on the explanation of binding.
[00:31:14.360 --> 00:31:18.240]   If we add in all of the dinucleotide effects,
[00:31:18.240 --> 00:31:21.520]   at this point, we can explain nearly all of the variability
[00:31:21.520 --> 00:31:23.640]   in the predictions from the neural network for FO4.
[00:31:23.640 --> 00:31:26.760]   And the same is almost true for CBF1 as well.
[00:31:27.760 --> 00:31:31.560]   So what we found from this work was that FO4 and CBF1
[00:31:31.560 --> 00:31:35.040]   clearly bind in these two very different modes.
[00:31:35.040 --> 00:31:36.880]   So one has a strong preference
[00:31:36.880 --> 00:31:38.040]   for the mononucleotide effects.
[00:31:38.040 --> 00:31:41.520]   So it seems to be that each individual nucleotide
[00:31:41.520 --> 00:31:44.040]   is actually contributing independently to binding.
[00:31:44.040 --> 00:31:46.040]   But CBF1, that doesn't seem to be the case,
[00:31:46.040 --> 00:31:47.960]   indicating that for CBF1,
[00:31:47.960 --> 00:31:49.880]   there might be a much larger effect
[00:31:49.880 --> 00:31:53.200]   of something like DNA shape to how it binds.
[00:31:54.920 --> 00:31:57.280]   But if we wanna ask the question why,
[00:31:57.280 --> 00:31:58.720]   the work that I've presented so far
[00:31:58.720 --> 00:32:00.280]   doesn't really get us there.
[00:32:00.280 --> 00:32:03.520]   And we're a bit limited going from DNA sequence
[00:32:03.520 --> 00:32:04.720]   to functional predictions
[00:32:04.720 --> 00:32:06.840]   directly via a neural network model.
[00:32:06.840 --> 00:32:09.440]   And the reason for that,
[00:32:09.440 --> 00:32:12.440]   if we go back to the initial presentation
[00:32:12.440 --> 00:32:15.480]   of the central dogma of molecular biology,
[00:32:15.480 --> 00:32:19.760]   is that instead of predicting each step along this process
[00:32:19.760 --> 00:32:22.760]   from protein to the folding and conformational dynamics,
[00:32:22.760 --> 00:32:25.280]   the post-translational modifications and then binding,
[00:32:25.280 --> 00:32:28.280]   we're basically black boxing this entire intermediate step
[00:32:28.280 --> 00:32:31.840]   and going directly from the protein and DNA sequences
[00:32:31.840 --> 00:32:33.600]   directly to their functions.
[00:32:33.600 --> 00:32:35.560]   And it's important to note here
[00:32:35.560 --> 00:32:37.440]   that we're not explicitly modeling
[00:32:37.440 --> 00:32:39.120]   the structure of the protein or the DNA here.
[00:32:39.120 --> 00:32:42.560]   We're just taking a proxy measurement of their sequences
[00:32:42.560 --> 00:32:43.680]   for their structures.
[00:32:43.680 --> 00:32:47.920]   But why would we wanna model the structure at all?
[00:32:47.920 --> 00:32:49.480]   Aside from the fact that I just told you
[00:32:49.480 --> 00:32:51.760]   that it was critically important.
[00:32:51.760 --> 00:32:53.680]   And I chose an example from the literature
[00:32:53.680 --> 00:32:55.800]   that I think illustrates this really well.
[00:32:55.800 --> 00:33:00.000]   And the principle here is that structural changes
[00:33:00.000 --> 00:33:01.680]   in the protein actually drive
[00:33:01.680 --> 00:33:03.520]   really important functional effects
[00:33:03.520 --> 00:33:05.960]   for the protein being targeted.
[00:33:05.960 --> 00:33:10.240]   And so here, what I'm showing on the left-hand side
[00:33:10.240 --> 00:33:12.880]   is green fluorescent protein, also known as GFP.
[00:33:12.880 --> 00:33:15.760]   And this is a really important protein in molecular biology
[00:33:15.760 --> 00:33:18.240]   because if you hit it with the right wavelength of light,
[00:33:18.240 --> 00:33:21.640]   it glows green and it allows us to visualize proteins
[00:33:21.640 --> 00:33:24.280]   and motion within the cell.
[00:33:24.280 --> 00:33:28.480]   And on the right-hand side are two different nanobodies.
[00:33:28.480 --> 00:33:31.080]   So these are really cool single domain antibodies
[00:33:31.080 --> 00:33:32.920]   isolated from camelid species.
[00:33:32.920 --> 00:33:35.080]   So llamas, alpacas, camels,
[00:33:35.080 --> 00:33:37.680]   they have these really cool single domain antibodies.
[00:33:37.680 --> 00:33:41.360]   And a lab in Europe isolated two of these different
[00:33:41.360 --> 00:33:45.520]   nanobodies that bound this GFP protein.
[00:33:45.520 --> 00:33:47.880]   And they were doing some basic characterization of this.
[00:33:47.880 --> 00:33:50.200]   And they realized that as they added more and more
[00:33:50.200 --> 00:33:51.560]   of each nanobody,
[00:33:51.560 --> 00:33:54.400]   they actually saw a very real functional effect.
[00:33:54.400 --> 00:33:57.720]   And so the one on top in red, as they added more nanobody,
[00:33:57.720 --> 00:34:00.480]   it suppressed the fluorescence of GFP.
[00:34:00.480 --> 00:34:01.320]   But the one on the bottom,
[00:34:01.320 --> 00:34:02.600]   as they added more and more of it,
[00:34:02.600 --> 00:34:04.960]   it enhanced the fluorescence of GFP.
[00:34:04.960 --> 00:34:06.960]   And so they named these the GFP minimizer
[00:34:06.960 --> 00:34:09.360]   and enhancer nanobodies respectively.
[00:34:09.360 --> 00:34:13.080]   And all they really differ in is the site on GFP
[00:34:13.080 --> 00:34:14.120]   that they're binding.
[00:34:14.120 --> 00:34:17.040]   And what's interesting about this
[00:34:17.040 --> 00:34:19.720]   is that if we look at the GFP minimizer
[00:34:19.720 --> 00:34:21.320]   and enhancer nanobodies and start to ask like,
[00:34:21.320 --> 00:34:23.080]   okay, what are the functional effects?
[00:34:23.080 --> 00:34:24.560]   What are the structural reasons
[00:34:24.560 --> 00:34:26.360]   behind these functional effects?
[00:34:26.360 --> 00:34:28.600]   We can look at the contact map of GFP.
[00:34:28.600 --> 00:34:31.000]   So if we look at all of the amino acids
[00:34:31.000 --> 00:34:32.320]   by all the amino acids,
[00:34:32.320 --> 00:34:33.720]   and then just highlight the ones
[00:34:33.720 --> 00:34:35.540]   that are in contact with each other,
[00:34:35.540 --> 00:34:38.480]   we see that when the GFP minimizer nanobody is bound,
[00:34:38.480 --> 00:34:39.920]   this is what the contact map looks like.
[00:34:39.920 --> 00:34:41.680]   When the enhancer nanobody is bound,
[00:34:41.680 --> 00:34:43.600]   this is what the contact map looks like.
[00:34:43.600 --> 00:34:45.680]   And they look generally very similar,
[00:34:45.680 --> 00:34:46.840]   which is to be expected.
[00:34:46.840 --> 00:34:49.320]   GFP's structure is pretty stable.
[00:34:49.320 --> 00:34:50.740]   But if we start to look at the difference
[00:34:50.740 --> 00:34:51.840]   between these two,
[00:34:51.840 --> 00:34:53.160]   we can start to see really important
[00:34:53.160 --> 00:34:54.900]   functional effects pop out.
[00:34:54.900 --> 00:34:57.520]   And so here, what I'm showing are in red,
[00:34:57.520 --> 00:34:59.160]   the enhancer specific contacts.
[00:34:59.160 --> 00:35:01.680]   So the contacts that are only present
[00:35:01.680 --> 00:35:06.120]   in the GFP bound to the minimizer nanobody
[00:35:06.120 --> 00:35:07.300]   or the enhancer nanobody,
[00:35:07.300 --> 00:35:09.200]   and then the minimizer specific ones
[00:35:09.200 --> 00:35:10.400]   bound to the minimizer.
[00:35:10.400 --> 00:35:12.820]   And so it would be really useful
[00:35:12.820 --> 00:35:14.260]   if we had some sort of model
[00:35:14.260 --> 00:35:18.200]   to actually predict the structural consequences
[00:35:18.200 --> 00:35:20.320]   of these types of binding interactions.
[00:35:20.320 --> 00:35:23.960]   So how might we go about doing that?
[00:35:23.960 --> 00:35:26.360]   If we think about two different amino acid residues,
[00:35:26.360 --> 00:35:28.800]   so here I'm showing two of the chemical structures,
[00:35:28.800 --> 00:35:30.700]   each for alanine.
[00:35:30.700 --> 00:35:32.800]   What we want is some model that takes as input
[00:35:32.800 --> 00:35:34.600]   those two residues,
[00:35:34.600 --> 00:35:36.320]   and gives us back the probability
[00:35:36.320 --> 00:35:38.480]   of them being in contact with one another.
[00:35:38.480 --> 00:35:41.280]   And if we run this over all of the potential pairs
[00:35:41.280 --> 00:35:44.120]   of amino acids in a protein structure,
[00:35:44.120 --> 00:35:47.100]   we can generate this probabilistic contact map.
[00:35:48.100 --> 00:35:50.100]   And there are a bunch of different ways to do this.
[00:35:50.100 --> 00:35:53.100]   And in fact, AlphaFold has been very successful
[00:35:53.100 --> 00:35:55.140]   at generating these predicted contact maps
[00:35:55.140 --> 00:35:57.300]   for single chain structures.
[00:35:57.300 --> 00:35:59.460]   But one of the most promising I think
[00:35:59.460 --> 00:36:01.620]   is reframing this problem
[00:36:01.620 --> 00:36:04.740]   as a graph neural network problem.
[00:36:04.740 --> 00:36:07.540]   And so we can treat these proteins as a molecular graph,
[00:36:07.540 --> 00:36:08.940]   because as I said before,
[00:36:08.940 --> 00:36:11.260]   they're just a bunch of these amino acids
[00:36:11.260 --> 00:36:12.500]   chained together linearly.
[00:36:12.500 --> 00:36:14.280]   And so this, as I said before,
[00:36:14.280 --> 00:36:16.380]   is the structure of alanine.
[00:36:16.380 --> 00:36:17.220]   And what you might notice
[00:36:17.220 --> 00:36:20.120]   is that it looks a bit like a graph structure.
[00:36:20.120 --> 00:36:22.260]   You have your nodes and you have your edges
[00:36:22.260 --> 00:36:25.040]   that connect atoms with covalent bonds.
[00:36:25.040 --> 00:36:31.620]   And so if we apply graph neural network models,
[00:36:31.620 --> 00:36:34.880]   we might be able to actually model this process,
[00:36:34.880 --> 00:36:37.220]   incorporating all of the atoms,
[00:36:37.220 --> 00:36:40.460]   not just the amino acids or the nucleotides
[00:36:40.460 --> 00:36:45.460]   like we did before for the FO4 and CBF1 modeling.
[00:36:46.460 --> 00:36:48.420]   And an important side note here
[00:36:48.420 --> 00:36:50.740]   is that using these atom level representations
[00:36:50.740 --> 00:36:52.620]   and not relying on the representations
[00:36:52.620 --> 00:36:54.020]   just of the amino acids
[00:36:54.020 --> 00:36:57.420]   as kind of elements of your alphabet
[00:36:57.420 --> 00:37:00.140]   is that it allows us a straightforward method
[00:37:00.140 --> 00:37:02.260]   for encoding post-translational modifications
[00:37:02.260 --> 00:37:05.300]   or these chemical modifications that aren't amino acids,
[00:37:05.300 --> 00:37:08.300]   but are added to the protein after it's been translated.
[00:37:08.300 --> 00:37:12.140]   And so in general, graph neural networks
[00:37:12.140 --> 00:37:14.000]   operate over these graph structures
[00:37:14.000 --> 00:37:15.340]   through message passing.
[00:37:15.340 --> 00:37:17.300]   And I tried out two different ones
[00:37:17.300 --> 00:37:19.780]   in very early stage development,
[00:37:19.780 --> 00:37:20.980]   these graph attention networks
[00:37:20.980 --> 00:37:23.420]   and position-aware graph neural networks
[00:37:23.420 --> 00:37:25.860]   to perform this exact task,
[00:37:25.860 --> 00:37:28.380]   where we're going to input atom level features
[00:37:28.380 --> 00:37:30.940]   and the covalent adjacency matrix,
[00:37:30.940 --> 00:37:33.620]   perform graph convolutions for the two different residues
[00:37:33.620 --> 00:37:35.580]   and then output the probability of contact,
[00:37:35.580 --> 00:37:37.160]   generating that contact map.
[00:37:37.160 --> 00:37:41.260]   And so we don't just generate a single contact map,
[00:37:41.260 --> 00:37:43.500]   but actually we generate different types
[00:37:43.500 --> 00:37:44.720]   of potential interactions.
[00:37:44.720 --> 00:37:47.580]   And so those of you familiar with chemistry
[00:37:47.580 --> 00:37:49.940]   might understand why we might want to predict
[00:37:49.940 --> 00:37:52.940]   hydrogen bonds or Van der Waals interactions.
[00:37:52.940 --> 00:37:55.040]   We actually predict a set of eight different
[00:37:55.040 --> 00:37:57.020]   types of interactions.
[00:37:57.020 --> 00:38:00.820]   And so this work is still very early stage,
[00:38:00.820 --> 00:38:03.340]   but I want to present some really preliminary results,
[00:38:03.340 --> 00:38:05.880]   just overfitting one of these graph neural network models
[00:38:05.880 --> 00:38:08.580]   to a single structure to see if it has the capacity
[00:38:08.580 --> 00:38:10.000]   to do that overfitting.
[00:38:11.180 --> 00:38:14.960]   And so if we look at one small protein structure here,
[00:38:14.960 --> 00:38:17.500]   we can use the different types of interactions.
[00:38:17.500 --> 00:38:19.660]   So here hydrophobic, hydrogen bond,
[00:38:19.660 --> 00:38:23.420]   Van der Waals and water bridges as our target.
[00:38:23.420 --> 00:38:26.220]   And then if we try a bunch of different models,
[00:38:26.220 --> 00:38:28.920]   we can see varying degrees of success overfitting
[00:38:28.920 --> 00:38:31.140]   to that single structure with one of the models
[00:38:31.140 --> 00:38:33.180]   actually perfectly fitting the data,
[00:38:33.180 --> 00:38:35.740]   indicating that this might be a feasible approach
[00:38:35.740 --> 00:38:37.420]   for modeling protein structure
[00:38:37.420 --> 00:38:39.700]   and importantly, modeling the interaction
[00:38:39.700 --> 00:38:41.140]   of different protein structures.
[00:38:41.140 --> 00:38:43.200]   So complexes of different proteins.
[00:38:43.200 --> 00:38:47.060]   And so what I've shown you here is that
[00:38:47.060 --> 00:38:49.700]   these graph neural network models are really powerful
[00:38:49.700 --> 00:38:52.420]   and directly applicable and directly translatable
[00:38:52.420 --> 00:38:54.700]   to chemical prediction tasks.
[00:38:54.700 --> 00:38:58.220]   That by using all atom models of protein structure,
[00:38:58.220 --> 00:39:01.460]   we're able to predict the effect of post-translational
[00:39:01.460 --> 00:39:03.700]   modifications or things that are added to the protein
[00:39:03.700 --> 00:39:05.060]   after it's been translated.
[00:39:05.060 --> 00:39:08.620]   And that by using a deep learning model
[00:39:08.620 --> 00:39:10.260]   to do this type of prediction,
[00:39:10.260 --> 00:39:12.260]   we'll be able to do it in really high throughput,
[00:39:12.260 --> 00:39:14.500]   allowing us to generate those protein libraries
[00:39:14.500 --> 00:39:16.380]   that I mentioned at the beginning of the talk,
[00:39:16.380 --> 00:39:18.580]   that would allow us to do really high throughput screening
[00:39:18.580 --> 00:39:20.140]   for functional proteins.
[00:39:20.140 --> 00:39:22.600]   And so if we combine these types of methods
[00:39:22.600 --> 00:39:26.220]   with existing methods based on physics simulations
[00:39:26.220 --> 00:39:28.700]   and experimental processes,
[00:39:28.700 --> 00:39:31.180]   we can kind of find the needle in the haystack
[00:39:31.180 --> 00:39:33.180]   a lot quicker than previously possible.
[00:39:33.180 --> 00:39:36.580]   And with that, I wanna acknowledge a bunch of people
[00:39:36.580 --> 00:39:39.100]   that were really integral in this work.
[00:39:39.100 --> 00:39:42.300]   So Dan, Lan, Arjun were the two members of the Fordyce lab
[00:39:42.300 --> 00:39:45.980]   that really helped me develop the Bet-Seq assay.
[00:39:45.980 --> 00:39:48.300]   I got a lot of help from Anshul Kondaji's lab,
[00:39:48.300 --> 00:39:50.940]   in particular, these individuals listed here
[00:39:50.940 --> 00:39:53.460]   on developing some of the early neural network models
[00:39:53.460 --> 00:39:54.300]   for this.
[00:39:54.300 --> 00:39:57.340]   The Leskovec lab has been super helpful
[00:39:57.340 --> 00:39:59.780]   in shaping my thinking about how to apply
[00:39:59.780 --> 00:40:01.580]   graph neural networks to the protein structure
[00:40:01.580 --> 00:40:03.300]   prediction problem.
[00:40:03.300 --> 00:40:05.820]   And then these two collaborators down here at the bottom,
[00:40:05.820 --> 00:40:07.620]   Yaron and Thomas were very helpful
[00:40:07.620 --> 00:40:10.300]   on some of the other graph neural network
[00:40:10.300 --> 00:40:12.260]   and some work that I didn't talk about.
[00:40:12.260 --> 00:40:16.200]   And with that, I'll open it up for any questions.
[00:40:16.200 --> 00:40:19.500]   - Great, thanks, Tyler.
[00:40:19.500 --> 00:40:24.900]   So I see one question and people both on YouTube
[00:40:24.900 --> 00:40:26.340]   and people in the Zoom,
[00:40:26.340 --> 00:40:28.140]   can you please start asking your questions
[00:40:28.140 --> 00:40:31.920]   either in the Q&A or in the chat and I'll come get you.
[00:40:31.920 --> 00:40:36.620]   The YouTube stream does run about two minutes behind.
[00:40:37.620 --> 00:40:40.100]   So we'll just take that into account.
[00:40:40.100 --> 00:40:43.540]   I do see one question here though, which is,
[00:40:43.540 --> 00:40:47.120]   what does the data for doing your network model look like?
[00:40:47.120 --> 00:40:51.720]   - So I'll assume, but correct me if it's wrong,
[00:40:51.720 --> 00:40:54.500]   that you're asking about what the inputs and outputs
[00:40:54.500 --> 00:40:55.840]   to that network look like.
[00:40:55.840 --> 00:40:59.380]   And so the input to the network
[00:40:59.380 --> 00:41:01.940]   is just the one hot encoded DNA sequence.
[00:41:01.940 --> 00:41:06.100]   And so it's just a four by L matrix
[00:41:06.100 --> 00:41:10.340]   where the individual rows represent A, T, G and C,
[00:41:10.340 --> 00:41:12.900]   and then the length of that matrix
[00:41:12.900 --> 00:41:15.380]   represents the length of the DNA sequence.
[00:41:15.380 --> 00:41:16.220]   So that's the input.
[00:41:16.220 --> 00:41:19.460]   And then the output is the predicted delta delta G value.
[00:41:19.460 --> 00:41:22.080]   So that energetic value that we're actually measuring,
[00:41:22.080 --> 00:41:25.040]   we have a noisy estimate of that from our experiment.
[00:41:25.040 --> 00:41:28.500]   That's what we're using as the target for the training data.
[00:41:28.500 --> 00:41:31.020]   - Cool.
[00:41:31.020 --> 00:41:35.060]   This is not a question, but Daniel Cooper on YouTube
[00:41:35.060 --> 00:41:37.460]   says his mind is blown and he's really excited.
[00:41:37.460 --> 00:41:40.420]   Someone dabbed.
[00:41:40.420 --> 00:41:42.300]   So that was cool.
[00:41:42.300 --> 00:41:44.420]   I don't see.
[00:41:44.420 --> 00:41:46.420]   Oh, I see another question.
[00:41:46.420 --> 00:41:49.060]   Have you been using any of the state of the art
[00:41:49.060 --> 00:41:52.540]   NLP libraries, HuggingFace maybe,
[00:41:52.540 --> 00:41:55.180]   to parse and analyze the DNA sequences?
[00:41:55.180 --> 00:41:58.020]   - So we haven't been using that.
[00:41:58.020 --> 00:42:00.060]   And what a lot of the other groups at Stanford
[00:42:00.060 --> 00:42:01.060]   have found really successful
[00:42:01.060 --> 00:42:03.060]   are actually the 2D CNN models.
[00:42:03.060 --> 00:42:06.140]   So treating that four by L matrix,
[00:42:06.140 --> 00:42:09.220]   not as a sequence and applying NLP stuff,
[00:42:09.220 --> 00:42:12.060]   but applying a lot more of the computer vision techniques
[00:42:12.060 --> 00:42:15.340]   as a four by L image, basically.
[00:42:15.340 --> 00:42:18.800]   So they're using that to train and test networks
[00:42:18.800 --> 00:42:20.860]   over full genome sequences.
[00:42:20.860 --> 00:42:22.940]   So the sequences that I was looking at here
[00:42:22.940 --> 00:42:24.580]   were only 10 base pairs.
[00:42:24.580 --> 00:42:26.740]   They routinely train on thousands
[00:42:26.740 --> 00:42:30.040]   or hundreds of thousands long sequences.
[00:42:30.880 --> 00:42:31.720]   - Thanks.
[00:42:31.720 --> 00:42:34.560]   Cool.
[00:42:34.560 --> 00:42:37.200]   - Tyler.
[00:42:37.200 --> 00:42:38.160]   Yeah.
[00:42:38.160 --> 00:42:40.600]   So a really cool talk.
[00:42:40.600 --> 00:42:44.160]   I was wondering, so it sounds like you think
[00:42:44.160 --> 00:42:47.240]   that the sort of 2D CNN approach is gonna work better
[00:42:47.240 --> 00:42:51.600]   for these one hot encoding DNA sequences.
[00:42:51.600 --> 00:42:53.720]   Do you have any thoughts about why something
[00:42:53.720 --> 00:42:57.500]   like an embedding based model isn't what works there,
[00:42:57.500 --> 00:42:59.760]   even though that's been so successful in NLP?
[00:42:59.760 --> 00:43:04.000]   And yeah, also, why do you think graph networks
[00:43:04.000 --> 00:43:06.300]   are so successful for the other tasks
[00:43:06.300 --> 00:43:07.880]   that you've used them for?
[00:43:07.880 --> 00:43:08.720]   - Yeah.
[00:43:08.720 --> 00:43:11.160]   So I think that the convolutional networks
[00:43:11.160 --> 00:43:14.000]   versus recurrent networks or transformers,
[00:43:14.000 --> 00:43:17.680]   I think that a lot of that comes back to
[00:43:17.680 --> 00:43:19.680]   not necessarily that they wouldn't be successful,
[00:43:19.680 --> 00:43:23.000]   but that the 2D CNNs are,
[00:43:23.000 --> 00:43:24.640]   they generate predictions in a way
[00:43:24.640 --> 00:43:27.840]   that's much more interpretable for biologists
[00:43:27.840 --> 00:43:29.440]   and for bioinformaticians.
[00:43:29.440 --> 00:43:30.960]   And so we're used to thinking about these things
[00:43:30.960 --> 00:43:32.200]   in terms of motifs.
[00:43:32.200 --> 00:43:35.640]   And if you think about each kernel in a CNN
[00:43:35.640 --> 00:43:37.560]   as basically a motif finder,
[00:43:37.560 --> 00:43:39.920]   it's gonna apply a weight to each nucleotide
[00:43:39.920 --> 00:43:41.280]   at each position,
[00:43:41.280 --> 00:43:43.920]   then people have already done a lot of the heavy lifting
[00:43:43.920 --> 00:43:48.600]   of kind of generating interpretation methods for 2D CNNs.
[00:43:48.600 --> 00:43:50.800]   So I think a lot of it stems from that.
[00:43:50.800 --> 00:43:54.080]   You might see greater success using some of the NLP
[00:43:54.080 --> 00:43:57.280]   techniques, but the other factor is that the data
[00:43:57.280 --> 00:43:59.200]   tend to be really noisy.
[00:43:59.200 --> 00:44:02.240]   And the trick is that depending on who collects the data,
[00:44:02.240 --> 00:44:03.920]   which group, how they do it,
[00:44:03.920 --> 00:44:06.680]   you run into a lot of these batch effects and things that
[00:44:06.680 --> 00:44:09.920]   aren't necessarily normalized over different experimental
[00:44:09.920 --> 00:44:12.400]   techniques or different experimental protocols.
[00:44:12.400 --> 00:44:15.120]   - Cool.
[00:44:15.120 --> 00:44:16.800]   And then Han asked,
[00:44:16.800 --> 00:44:19.320]   what type of linear models or techniques were used
[00:44:19.320 --> 00:44:23.120]   in the sequencing and how has modeling the sequences
[00:44:23.120 --> 00:44:27.760]   using linear models helped in designing your neural network?
[00:44:28.920 --> 00:44:29.760]   - Yeah.
[00:44:29.760 --> 00:44:33.480]   So the linear models basically use the same inputs
[00:44:33.480 --> 00:44:34.320]   and outputs.
[00:44:34.320 --> 00:44:38.760]   And so it's just essentially a 40 dimensional vector
[00:44:38.760 --> 00:44:41.600]   where we're flattening that matrix.
[00:44:41.600 --> 00:44:43.400]   So it's the same one hot encoded matrix
[00:44:43.400 --> 00:44:45.000]   and the same Delta Delta G values,
[00:44:45.000 --> 00:44:47.800]   same inputs and outputs as the neural network models.
[00:44:47.800 --> 00:44:50.440]   And I'll have to admit that it didn't really shape
[00:44:50.440 --> 00:44:53.600]   our thinking about exactly how to design
[00:44:53.600 --> 00:44:54.440]   the neural network model.
[00:44:54.440 --> 00:44:56.800]   We kind of worked from the neural network model backwards.
[00:44:56.800 --> 00:44:59.840]   And so we built a neural network model that fit our data
[00:44:59.840 --> 00:45:02.760]   and was validated via these orthogonal methods.
[00:45:02.760 --> 00:45:05.280]   And then we use the linear models to really interpret
[00:45:05.280 --> 00:45:07.640]   what the neural network was learning and how that fit
[00:45:07.640 --> 00:45:10.400]   with our assumptions based on the data sets
[00:45:10.400 --> 00:45:12.000]   that had already been collected.
[00:45:12.000 --> 00:45:13.920]   - Nice.
[00:45:13.920 --> 00:45:16.920]   And then I think the last question is,
[00:45:16.920 --> 00:45:19.440]   have you tried experimenting with graph neural networks
[00:45:19.440 --> 00:45:22.680]   in addition to convolutional neural networks?
[00:45:22.680 --> 00:45:25.760]   And if so, is any of this stuff publicly available?
[00:45:26.720 --> 00:45:29.760]   - Yeah, so the graph neural network stuff
[00:45:29.760 --> 00:45:31.280]   that I presented at the very end
[00:45:31.280 --> 00:45:33.400]   is still really early stages.
[00:45:33.400 --> 00:45:37.160]   We're not seeing the same level of prediction accuracy yet
[00:45:37.160 --> 00:45:38.720]   as something like AlphaGo
[00:45:38.720 --> 00:45:42.480]   or these recurrent geometric networks.
[00:45:42.480 --> 00:45:45.280]   So if you're looking for the literature,
[00:45:45.280 --> 00:45:47.200]   I would point you to either the AlphaGo,
[00:45:47.200 --> 00:45:49.920]   or not the AlphaGo, the AlphaFold paper,
[00:45:49.920 --> 00:45:52.640]   or a lot of the work from Mohamed Al-Karashi's group
[00:45:52.640 --> 00:45:54.160]   at Harvard does a really nice job
[00:45:54.160 --> 00:45:56.520]   with some of the recurrent networks.
[00:45:56.520 --> 00:45:59.520]   They seem to be more accurate in general so far,
[00:45:59.520 --> 00:46:00.640]   but I think there's promise
[00:46:00.640 --> 00:46:03.240]   for the graph neural network techniques.
[00:46:03.240 --> 00:46:05.440]   - Nice.
[00:46:05.440 --> 00:46:06.760]   All right, I think that's it.
[00:46:06.760 --> 00:46:07.880]   Thank you so much, Tyler.
[00:46:07.880 --> 00:46:09.320]   This was really cool.
[00:46:09.320 --> 00:46:10.720]   - Of course, thank you guys.
[00:46:10.720 --> 00:46:14.720]   - All right, up next, we have Kostev.
[00:46:14.720 --> 00:46:20.800]   So Kostev is a research PhD intern at Facebook AI Research,
[00:46:20.800 --> 00:46:23.240]   and his interests lie at the intersection
[00:46:23.240 --> 00:46:25.720]   of generative language modeling
[00:46:25.720 --> 00:46:28.080]   and neural reasoning,
[00:46:28.080 --> 00:46:30.760]   which I'm super fascinated to hear about.
[00:46:30.760 --> 00:46:33.120]   Today, he's gonna talk about one of the papers
[00:46:33.120 --> 00:46:34.080]   that he's written.
[00:46:34.080 --> 00:46:38.640]   His paper is called "Evaluating Logical Generalizations
[00:46:38.640 --> 00:46:40.400]   in Graph Neural Networks."
[00:46:40.400 --> 00:46:43.480]   So Kostev, the floor is yours.
[00:46:43.480 --> 00:46:45.520]   - Thanks.
[00:46:45.520 --> 00:46:46.760]   Thanks for the nice introduction.
[00:46:46.760 --> 00:46:50.320]   Just gonna share my screen.
[00:46:50.320 --> 00:46:54.440]   - Also, Kostev is gonna do an AMA at 9 a.m.
[00:46:54.440 --> 00:46:56.560]   in our Slack community this Friday.
[00:46:56.560 --> 00:47:00.400]   So if you have any more questions after reading his paper,
[00:47:00.400 --> 00:47:03.760]   please let us know and join the AMA.
[00:47:03.760 --> 00:47:07.040]   - Cool.
[00:47:07.040 --> 00:47:08.400]   Can you guys see my screen?
[00:47:08.400 --> 00:47:11.080]   Okay.
[00:47:11.080 --> 00:47:12.080]   - Yes, we can.
[00:47:12.080 --> 00:47:13.000]   - Awesome.
[00:47:13.000 --> 00:47:16.440]   So yeah, thanks for inviting me.
[00:47:16.440 --> 00:47:19.880]   And Tyler also, it was really great talk.
[00:47:19.880 --> 00:47:24.520]   And yeah, so let me go forward with this talk.
[00:47:24.520 --> 00:47:26.560]   So this talk is not essentially about
[00:47:26.560 --> 00:47:28.080]   only our recent paper.
[00:47:28.080 --> 00:47:30.520]   This is essentially about clubbing
[00:47:30.520 --> 00:47:32.560]   two similar works that I had.
[00:47:32.560 --> 00:47:36.280]   So I'm just gonna go through that in this talk.
[00:47:36.280 --> 00:47:40.080]   So first, a little bit of intro about myself.
[00:47:40.080 --> 00:47:43.240]   I'm a PhD candidate at McGill University and Mila,
[00:47:43.240 --> 00:47:46.840]   co-supervised by Joel Penu and William L. Hamilton.
[00:47:46.840 --> 00:47:50.280]   And also I'm a research intern at FAIR Montreal.
[00:47:50.280 --> 00:47:54.400]   My research interests are kind of in intersection right now
[00:47:54.400 --> 00:47:56.040]   with systematic generalization,
[00:47:56.040 --> 00:47:58.680]   which is kind of a sub part of neural reasoning
[00:47:58.680 --> 00:48:03.000]   and in discrete domains, such as in language or in graphs.
[00:48:03.000 --> 00:48:06.360]   And also I dabble a little bit into dialogue systems.
[00:48:06.360 --> 00:48:08.600]   And originally I'm from Kolkata, India.
[00:48:08.600 --> 00:48:10.280]   And right now I'm at Montreal.
[00:48:10.280 --> 00:48:12.720]   So, okay.
[00:48:12.720 --> 00:48:15.960]   So why care about generalization?
[00:48:15.960 --> 00:48:20.000]   So ML models are getting really good at feature extraction.
[00:48:20.000 --> 00:48:23.880]   So those of you have like seen Joshua's stellar talk
[00:48:23.880 --> 00:48:25.280]   in last NeurIPS,
[00:48:25.280 --> 00:48:28.840]   he talked about this topic of system one
[00:48:28.840 --> 00:48:30.880]   versus system two deep learning.
[00:48:30.880 --> 00:48:32.440]   So what is system one?
[00:48:32.440 --> 00:48:34.400]   System one is where we are right now,
[00:48:34.400 --> 00:48:36.440]   where we have very good models,
[00:48:36.440 --> 00:48:38.360]   which does feature extraction.
[00:48:38.360 --> 00:48:40.360]   So given a standard data set,
[00:48:40.360 --> 00:48:43.280]   that model like really performs quite well
[00:48:43.280 --> 00:48:45.000]   on that particular data set.
[00:48:45.000 --> 00:48:47.920]   And in the bottom right corner,
[00:48:47.920 --> 00:48:51.160]   you see that I have posted like a top one accuracy.
[00:48:51.160 --> 00:48:53.320]   This is taken from papers from code.
[00:48:53.320 --> 00:48:55.680]   And this is a top one accuracy on ImageNet.
[00:48:55.680 --> 00:48:57.440]   And right now the state of the art
[00:48:57.440 --> 00:49:00.040]   is just a little bit shy of 90%.
[00:49:00.040 --> 00:49:03.920]   So system one deep learning works really, really well.
[00:49:03.920 --> 00:49:05.960]   We have pretty good feature extractors
[00:49:05.960 --> 00:49:09.440]   and also the number of parameters used by our models
[00:49:09.440 --> 00:49:13.080]   are like increasing like really, really fast.
[00:49:13.080 --> 00:49:16.520]   But what this talk is involving is,
[00:49:16.520 --> 00:49:17.800]   I'm kind of trying to motivate
[00:49:17.800 --> 00:49:19.440]   towards system two deep learning.
[00:49:19.440 --> 00:49:23.600]   So this is reasoning, which involves inductive reasoning.
[00:49:23.600 --> 00:49:29.080]   Basically, when you learn some skills from your data set,
[00:49:29.080 --> 00:49:33.440]   you want to like apply those skills to some new data set
[00:49:33.440 --> 00:49:35.080]   or those data sets,
[00:49:35.080 --> 00:49:38.480]   which are kind of produced from that particular data set.
[00:49:38.480 --> 00:49:41.440]   And you want to transfer your known skills.
[00:49:41.440 --> 00:49:44.520]   And you also want to remember your known skills.
[00:49:44.520 --> 00:49:48.440]   So basically, this is a slide I've just like kind of
[00:49:48.440 --> 00:49:50.080]   screen grab from Joshua's talk,
[00:49:50.080 --> 00:49:52.440]   where he talks more about system one
[00:49:52.440 --> 00:49:54.120]   and system two deep learning.
[00:49:54.120 --> 00:49:56.680]   So essentially, the system two or the future
[00:49:56.680 --> 00:49:59.040]   is to go towards a set of models,
[00:49:59.040 --> 00:50:03.080]   which are kind of more logically driven.
[00:50:03.080 --> 00:50:07.120]   So essentially, whenever you're making like a prediction,
[00:50:07.120 --> 00:50:10.520]   you are making that prediction in a logically valid way,
[00:50:10.520 --> 00:50:13.360]   so that you can reapply your learned skills
[00:50:13.360 --> 00:50:15.120]   on top of a data set,
[00:50:15.120 --> 00:50:19.040]   which is kind of derived from the original data set.
[00:50:19.040 --> 00:50:21.720]   So that kind of helps us to go towards
[00:50:21.720 --> 00:50:24.480]   a realm of out of domain distribution.
[00:50:24.480 --> 00:50:26.720]   Out of domain is kind of ill defined,
[00:50:26.720 --> 00:50:28.000]   like you can just say that,
[00:50:28.000 --> 00:50:31.280]   okay, I train on a subset of images involving cats,
[00:50:31.280 --> 00:50:34.160]   and I want to test on images involving dogs.
[00:50:34.160 --> 00:50:38.240]   But in case of the out of domain distribution,
[00:50:38.240 --> 00:50:40.080]   it's better to present it in this way,
[00:50:40.080 --> 00:50:44.800]   that if your model can learn the integral skills
[00:50:44.800 --> 00:50:46.320]   or the integral components
[00:50:46.320 --> 00:50:48.480]   that is driving a particular task,
[00:50:48.480 --> 00:50:51.040]   can your models use those components
[00:50:51.040 --> 00:50:53.120]   and reapply them to a new task?
[00:50:53.120 --> 00:50:54.880]   So that example that I gave,
[00:50:54.880 --> 00:50:56.160]   it will turn up to be,
[00:50:56.160 --> 00:50:58.280]   let's say you train on a subset of cats
[00:50:58.280 --> 00:51:01.120]   and whether you can recognize a new breed of cat.
[00:51:01.120 --> 00:51:05.640]   So how do you know your model is reasoning?
[00:51:05.640 --> 00:51:07.680]   So there are a lot of performance metrics
[00:51:07.680 --> 00:51:10.720]   that we look at specifically accuracy
[00:51:10.720 --> 00:51:12.160]   or in binary classification,
[00:51:12.160 --> 00:51:14.440]   you see on held out test sets,
[00:51:14.440 --> 00:51:16.280]   but that is it.
[00:51:16.280 --> 00:51:19.600]   Like that is the only measure that we are interested in,
[00:51:19.600 --> 00:51:21.720]   mostly in deep learning research.
[00:51:21.720 --> 00:51:23.280]   So right now there's a lot of push
[00:51:23.280 --> 00:51:25.960]   of building more interpretable models.
[00:51:25.960 --> 00:51:30.040]   We also want to build a lot more integrate test sets.
[00:51:30.040 --> 00:51:32.200]   So essentially you just not say that,
[00:51:32.200 --> 00:51:34.680]   okay, on aggregate, my model is performing 90%,
[00:51:34.680 --> 00:51:36.560]   but what does it show?
[00:51:36.560 --> 00:51:39.000]   Like, does it show that where your models are failing
[00:51:39.000 --> 00:51:40.320]   and how they are failing?
[00:51:40.320 --> 00:51:41.600]   And even if they are failing,
[00:51:41.600 --> 00:51:45.200]   like their failures should be kind of systematic.
[00:51:45.200 --> 00:51:47.880]   And also how do you guarantee that your model
[00:51:47.880 --> 00:51:50.480]   is just not basically rote learning features,
[00:51:50.480 --> 00:51:53.240]   like given a distribution, which is somewhat similar,
[00:51:53.240 --> 00:51:54.880]   but not exactly the same,
[00:51:54.880 --> 00:51:58.360]   whether your models can learn on this kind of setups.
[00:51:58.360 --> 00:52:01.320]   And also how do you evaluate the robustness of your model?
[00:52:01.320 --> 00:52:02.640]   So recently in like
[00:52:02.640 --> 00:52:04.320]   in natural language processing literature,
[00:52:04.320 --> 00:52:07.160]   a lot of work is coming up on adversarial attacks
[00:52:07.160 --> 00:52:09.600]   and people have been shown over and over that
[00:52:09.600 --> 00:52:11.480]   with certain types of adversarial attacks,
[00:52:11.480 --> 00:52:13.880]   you can pull a lot of language classifiers
[00:52:13.880 --> 00:52:15.640]   and also a lot of image classifiers.
[00:52:15.640 --> 00:52:18.600]   And there's a lot of defense mechanisms are coming up.
[00:52:18.600 --> 00:52:22.000]   But again, like how do you evaluate your model
[00:52:22.000 --> 00:52:24.200]   on that kind of robustness
[00:52:24.200 --> 00:52:27.200]   without having access to such kind of data?
[00:52:27.200 --> 00:52:31.120]   So the key properties in system two deep learning
[00:52:31.120 --> 00:52:32.720]   that I'm trying to propose is that
[00:52:32.720 --> 00:52:36.040]   we want to go towards a systematic generalization.
[00:52:36.040 --> 00:52:38.800]   So essentially we want to learn to extract
[00:52:38.800 --> 00:52:41.360]   the core skills of a task,
[00:52:41.360 --> 00:52:43.520]   and we want to learn to apply those skills
[00:52:43.520 --> 00:52:47.120]   to solve any task, which can be produced from such rules.
[00:52:47.120 --> 00:52:50.080]   For example, if you are learning how to add,
[00:52:50.080 --> 00:52:52.640]   you're probably learning how to add like,
[00:52:52.640 --> 00:52:55.480]   let's say single or double digit numbers,
[00:52:55.480 --> 00:52:56.560]   but let's say on test time,
[00:52:56.560 --> 00:52:58.920]   if I give you like N digit number,
[00:52:58.920 --> 00:53:00.600]   then you should also be able to add
[00:53:00.600 --> 00:53:01.760]   because it's the same skill.
[00:53:01.760 --> 00:53:05.040]   You're just applying it over a new distribution.
[00:53:05.040 --> 00:53:06.920]   So that is just a very trivial example,
[00:53:06.920 --> 00:53:10.240]   but in a lot of other cases,
[00:53:10.240 --> 00:53:13.840]   you want to be kind of able to extract the rules
[00:53:13.840 --> 00:53:16.520]   that are imbibed in your data set.
[00:53:16.520 --> 00:53:19.440]   And you want to make sure that your model
[00:53:19.440 --> 00:53:21.800]   like reapplies those tools.
[00:53:21.800 --> 00:53:24.560]   And you want to be invariant to noise.
[00:53:24.560 --> 00:53:27.440]   And to do that, your model must be logically valid.
[00:53:27.440 --> 00:53:30.920]   That is, if your model is like correct at certain examples,
[00:53:30.920 --> 00:53:33.680]   then it should be correct for those examples
[00:53:33.680 --> 00:53:36.800]   throughout a set of distributions.
[00:53:36.800 --> 00:53:40.200]   So let's say we take a simple example.
[00:53:40.200 --> 00:53:43.680]   Let's say an example can be a simple kinship relation game.
[00:53:43.680 --> 00:53:46.360]   Now, if I'm given you a text,
[00:53:46.360 --> 00:53:49.000]   which says about a certain kinship relation
[00:53:49.000 --> 00:53:52.640]   between characters, you can, like, if I ask you like,
[00:53:52.640 --> 00:53:55.920]   okay, how is Carol related to Justin in this case?
[00:53:55.920 --> 00:53:57.280]   Then you can easily say that, okay,
[00:53:57.280 --> 00:53:59.560]   Carol is the grandmother of Justin.
[00:53:59.560 --> 00:54:02.920]   Now, this kinship relations, we know,
[00:54:02.920 --> 00:54:05.280]   like these rules that we already have learned
[00:54:05.280 --> 00:54:07.720]   from growing up, but the cool thing,
[00:54:07.720 --> 00:54:12.160]   what we do without realizing is that we reapply these rules.
[00:54:12.160 --> 00:54:15.480]   So if I give you a long, long set of relations,
[00:54:15.480 --> 00:54:17.000]   and if I give you like a pen and paper,
[00:54:17.000 --> 00:54:18.120]   you can still solve that.
[00:54:18.120 --> 00:54:21.320]   You can still like reapply your own thinking
[00:54:21.320 --> 00:54:24.920]   to solve like, okay, this character is related
[00:54:24.920 --> 00:54:26.440]   to this character in this way.
[00:54:27.560 --> 00:54:31.960]   So basically you can think of it as a graph also,
[00:54:31.960 --> 00:54:35.720]   like given certain entities and certain edges.
[00:54:35.720 --> 00:54:39.200]   So the edges are essentially denoting relations.
[00:54:39.200 --> 00:54:41.160]   And if you want to do relation prediction,
[00:54:41.160 --> 00:54:44.560]   that is what is the type of a particular edge,
[00:54:44.560 --> 00:54:47.440]   then in this kind of a kinship relation game,
[00:54:47.440 --> 00:54:51.880]   you can easily do that by taking like a walk over the path
[00:54:51.880 --> 00:54:53.880]   and basically resolving that path.
[00:54:55.000 --> 00:54:59.200]   So one of my previous work is named Clutter,
[00:54:59.200 --> 00:55:01.080]   which is exactly this.
[00:55:01.080 --> 00:55:03.800]   So Clutter is essentially a diagnostic benchmark,
[00:55:03.800 --> 00:55:06.440]   which is created for evaluating
[00:55:06.440 --> 00:55:08.400]   inductive reasoning from text.
[00:55:08.400 --> 00:55:11.120]   It is exactly a kinship relation game
[00:55:11.120 --> 00:55:15.440]   where the model is essentially given a blurb of text,
[00:55:15.440 --> 00:55:17.040]   which contains a story.
[00:55:17.040 --> 00:55:21.160]   And it's a Q&A setup where you are given a question
[00:55:21.160 --> 00:55:24.080]   and then your model has to give out an answer
[00:55:24.080 --> 00:55:28.080]   on all possible family relations.
[00:55:28.080 --> 00:55:32.680]   Now, each puzzle is built using first order logic.
[00:55:32.680 --> 00:55:37.680]   Now that helps us a lot in basically making sure
[00:55:37.680 --> 00:55:40.920]   that all the puzzles are verifiable
[00:55:40.920 --> 00:55:44.760]   and the logic is built on a set of rules.
[00:55:44.760 --> 00:55:49.320]   Now the generator is basically fed a set of rules
[00:55:49.320 --> 00:55:51.960]   that we know about family relations.
[00:55:51.960 --> 00:55:54.800]   And then the generator essentially reapplies these rules
[00:55:54.800 --> 00:55:57.680]   to create this intricate puzzle.
[00:55:57.680 --> 00:56:00.240]   So this puzzle has essentially two modalities.
[00:56:00.240 --> 00:56:02.640]   So it has the primary modality is the text,
[00:56:02.640 --> 00:56:05.480]   which we want the models to perform.
[00:56:05.480 --> 00:56:07.760]   And then there is a secondary modality is graph,
[00:56:07.760 --> 00:56:10.760]   that is each puzzle also has an underlying graph.
[00:56:10.760 --> 00:56:15.040]   Now, this is also a kind of a diagnostic suite.
[00:56:15.040 --> 00:56:18.560]   You can build your own complex setups on top of this.
[00:56:18.560 --> 00:56:21.560]   And then you can see how your models
[00:56:21.560 --> 00:56:24.120]   are systematically generalizing.
[00:56:24.120 --> 00:56:28.080]   Now, just as an example, so this is an input document.
[00:56:28.080 --> 00:56:29.920]   This gets fetched to the model.
[00:56:29.920 --> 00:56:33.000]   And then we have a query.
[00:56:33.000 --> 00:56:35.080]   So in this case, we have a query,
[00:56:35.080 --> 00:56:39.720]   which is a pair of entities that we want the relation in.
[00:56:39.720 --> 00:56:43.040]   Interestingly, we didn't use a natural language question
[00:56:43.040 --> 00:56:45.480]   that you see in a lot of Q&A data sets,
[00:56:45.480 --> 00:56:49.440]   because there was a really nice paper in AMNLP 2018,
[00:56:49.440 --> 00:56:52.600]   which shows that a lot of these models,
[00:56:52.600 --> 00:56:54.160]   they do not even understand
[00:56:54.160 --> 00:56:56.360]   the entire context of the question.
[00:56:56.360 --> 00:56:58.600]   So let's say if there is a question is that,
[00:56:58.600 --> 00:57:00.040]   when did this event occur?
[00:57:00.040 --> 00:57:02.840]   The model kind of performs even well,
[00:57:02.840 --> 00:57:05.400]   if you like cut off all the different words
[00:57:05.400 --> 00:57:07.600]   and still focus on the word when.
[00:57:07.600 --> 00:57:11.000]   This is like a really great paper by Zachary Lipton.
[00:57:11.000 --> 00:57:13.200]   I would suggest you all to read it.
[00:57:13.200 --> 00:57:17.440]   And so basically like we replace the question
[00:57:17.440 --> 00:57:20.040]   with the set of like queries.
[00:57:20.040 --> 00:57:23.160]   And then the task is simply to predict
[00:57:23.160 --> 00:57:25.720]   what is the relationship between them.
[00:57:25.720 --> 00:57:28.040]   Now, this seems very simple.
[00:57:28.040 --> 00:57:30.480]   And I have like heard from a lot of people
[00:57:30.480 --> 00:57:32.840]   that they are saying that, okay, this is very simple,
[00:57:32.840 --> 00:57:34.680]   because you just have a set of rules
[00:57:34.680 --> 00:57:36.120]   that the model have to learn.
[00:57:36.120 --> 00:57:37.680]   And once the model learns the rule
[00:57:37.680 --> 00:57:39.680]   and then it should perform well, right?
[00:57:40.640 --> 00:57:44.400]   So it turns out no, why?
[00:57:44.400 --> 00:57:47.960]   Let us go towards a bit of the tasks first.
[00:57:47.960 --> 00:57:50.840]   So one of the tasks or the primary task
[00:57:50.840 --> 00:57:53.960]   that we want to look at is called logical generalization.
[00:57:53.960 --> 00:57:55.720]   So in logical generalization,
[00:57:55.720 --> 00:57:58.880]   you're given a set of like puzzles,
[00:57:58.880 --> 00:58:02.320]   which are kind of restricted by their length.
[00:58:02.320 --> 00:58:05.040]   So essentially you're given a lot of examples,
[00:58:05.040 --> 00:58:07.720]   but you're restricting the length of this puzzle
[00:58:07.720 --> 00:58:12.720]   to only contain relationship combinations of length three.
[00:58:12.720 --> 00:58:15.120]   And then you're testing on puzzles,
[00:58:15.120 --> 00:58:17.080]   which has relationship combinations
[00:58:17.080 --> 00:58:18.800]   of length more than three.
[00:58:18.800 --> 00:58:21.480]   So this is equivalent to the example I gave before
[00:58:21.480 --> 00:58:24.840]   that if you know how relationships work,
[00:58:24.840 --> 00:58:27.240]   if I give you a long, long passage,
[00:58:27.240 --> 00:58:31.000]   you can still solve that given enough time and compute.
[00:58:31.000 --> 00:58:34.440]   So this is exactly the notion of system two deep learning,
[00:58:34.440 --> 00:58:36.280]   because the models have to think,
[00:58:36.280 --> 00:58:40.160]   the model have to like reapply their own skills
[00:58:40.160 --> 00:58:43.200]   over and over in order to solve this particular task.
[00:58:43.200 --> 00:58:49.400]   And also we looked at a case of linguistic generalization.
[00:58:49.400 --> 00:58:52.520]   So essentially we thought that, okay,
[00:58:52.520 --> 00:58:55.640]   a lot of NLP models are pretty good these days
[00:58:55.640 --> 00:58:58.960]   because they kind of learn a lot of features
[00:58:58.960 --> 00:59:02.320]   about the individual words.
[00:59:02.320 --> 00:59:05.080]   And then they, whenever they find the similar words,
[00:59:05.080 --> 00:59:07.360]   they kind of overfit on top of that.
[00:59:07.360 --> 00:59:10.560]   In order to like, in order to make sure
[00:59:10.560 --> 00:59:12.360]   that the models are not doing it,
[00:59:12.360 --> 00:59:16.280]   for each puzzle, we have two representations of each puzzle.
[00:59:16.280 --> 00:59:19.200]   One representation is drawn,
[00:59:19.200 --> 00:59:21.440]   like basically each puzzle is a graph,
[00:59:21.440 --> 00:59:24.360]   and then you're applying a layer of language on top of it.
[00:59:24.360 --> 00:59:27.040]   And this layer of language is what we are collecting
[00:59:27.040 --> 00:59:28.880]   through Amazon Mechanical Turk.
[00:59:28.880 --> 00:59:32.200]   And essentially we are splitting the placeholders
[00:59:32.200 --> 00:59:35.760]   of the Amazon Mechanical Turk into training and testing.
[00:59:35.760 --> 00:59:38.000]   So this becomes really challenging for the model
[00:59:38.000 --> 00:59:42.960]   because the model has never seen the exact syntax before,
[00:59:42.960 --> 00:59:46.800]   but if the model understands how relationships work,
[00:59:46.800 --> 00:59:49.440]   then it should be able to generalize better
[00:59:49.440 --> 00:59:50.600]   in this kind of setup.
[00:59:50.600 --> 00:59:55.400]   So, and also a third cool thing that we had in Clutter
[00:59:55.400 --> 00:59:57.280]   was a robustness test.
[00:59:57.280 --> 01:00:01.120]   So as I said that, since these are built on logic,
[01:00:01.120 --> 01:00:04.800]   you can easily verify whether your logic is sound or not.
[01:00:04.800 --> 01:00:06.440]   So let's say given in this example,
[01:00:06.440 --> 01:00:10.320]   if I have given you a family relation tree like this,
[01:00:10.320 --> 01:00:13.000]   and let's say I want you to answer
[01:00:13.000 --> 01:00:15.880]   what is the relationship between A and D.
[01:00:15.880 --> 01:00:18.560]   So then there can be two possible paths
[01:00:18.560 --> 01:00:20.120]   in the supporting fact,
[01:00:20.120 --> 01:00:22.000]   and then your model has to understand,
[01:00:22.000 --> 01:00:23.760]   okay, since there are a supporting fact,
[01:00:23.760 --> 01:00:26.480]   I have additional information to do well.
[01:00:26.480 --> 01:00:30.040]   While if I have a tree which has a dangling path,
[01:00:30.040 --> 01:00:31.920]   that is kind of a confounder.
[01:00:31.920 --> 01:00:35.960]   And that confounder is like used to make sure that,
[01:00:35.960 --> 01:00:38.200]   okay, your model does not perform well,
[01:00:38.200 --> 01:00:39.680]   but your model has to understand,
[01:00:39.680 --> 01:00:42.400]   okay, I do not want to go through this path.
[01:00:42.400 --> 01:00:44.200]   And then there is a disconnected fact
[01:00:44.200 --> 01:00:47.480]   where you have a edge which is entirely disconnected
[01:00:47.480 --> 01:00:51.280]   from your current resolution path,
[01:00:51.280 --> 01:00:54.440]   but then your model has to also disregard that.
[01:00:54.440 --> 01:00:56.440]   So these are really simple,
[01:00:56.440 --> 01:00:59.120]   but very powerful robustness test
[01:00:59.120 --> 01:01:00.800]   that we have in Flutter.
[01:01:00.800 --> 01:01:04.120]   And so let's go towards like,
[01:01:04.120 --> 01:01:06.160]   what are the models we initially used?
[01:01:06.160 --> 01:01:08.520]   So we essentially thought, okay,
[01:01:08.520 --> 01:01:10.520]   we have made the task really difficult
[01:01:10.520 --> 01:01:13.840]   for the text or the natural language understanding models.
[01:01:13.840 --> 01:01:17.840]   We also need to provide a benchmark for a model
[01:01:17.840 --> 01:01:20.400]   which do not have to do the parsing.
[01:01:20.400 --> 01:01:22.600]   So we used a graph attention network,
[01:01:22.600 --> 01:01:27.000]   which worked solely on top of the raw graphs.
[01:01:27.000 --> 01:01:29.880]   And for models that have access to the text,
[01:01:29.880 --> 01:01:32.240]   we used a vast number of models,
[01:01:32.240 --> 01:01:34.240]   including transformer style models,
[01:01:34.240 --> 01:01:36.440]   such as BERT and BERT LSTM,
[01:01:36.440 --> 01:01:39.760]   which is essentially a layer of LSTM on top of BERT,
[01:01:39.760 --> 01:01:41.880]   on top of frozen BERT, essentially.
[01:01:41.880 --> 01:01:47.360]   So how does the models fare on generalizability?
[01:01:47.360 --> 01:01:49.600]   In the plot on your right,
[01:01:49.600 --> 01:01:51.640]   you see that the generalizability
[01:01:51.640 --> 01:01:54.120]   or the accuracy during testing,
[01:01:54.120 --> 01:01:57.160]   drastically falls down when we show tasks
[01:01:57.160 --> 01:01:59.520]   of more and more length.
[01:01:59.520 --> 01:02:02.920]   While if we provide more training data,
[01:02:02.920 --> 01:02:04.960]   like if we provide the model training data
[01:02:04.960 --> 01:02:08.040]   of involving four number of relations,
[01:02:08.040 --> 01:02:10.960]   then the model like perform somewhat better,
[01:02:10.960 --> 01:02:13.720]   but the graph neural network performs the best
[01:02:13.720 --> 01:02:16.600]   when given a lot of additional data,
[01:02:16.600 --> 01:02:19.280]   but still the generalization towards
[01:02:19.280 --> 01:02:21.880]   like extreme generalization, what I say,
[01:02:21.880 --> 01:02:24.200]   like towards the end of the,
[01:02:24.200 --> 01:02:26.320]   like the number of 10 relations,
[01:02:26.320 --> 01:02:29.520]   then all the models kind of falter.
[01:02:29.520 --> 01:02:33.040]   But graph neural networks are really robust.
[01:02:33.040 --> 01:02:36.640]   When we use the different robustness test,
[01:02:36.640 --> 01:02:38.360]   we found that the graph neural networks
[01:02:38.360 --> 01:02:41.120]   almost always outperform
[01:02:41.120 --> 01:02:43.960]   all the text question answering models,
[01:02:43.960 --> 01:02:45.360]   which is understandably so,
[01:02:45.360 --> 01:02:47.000]   because the graph neural network
[01:02:47.000 --> 01:02:49.600]   do not have to do the semantic parsing.
[01:02:49.600 --> 01:02:52.560]   So semantic parsing is one problem,
[01:02:52.560 --> 01:02:55.280]   but still the graph neural networks falter
[01:02:55.280 --> 01:02:57.240]   on the length generalization problem.
[01:02:57.240 --> 01:03:03.320]   So basically clutter was based only on 15 rules.
[01:03:03.320 --> 01:03:06.400]   And these 15 rules were kind of a very boiled down
[01:03:06.400 --> 01:03:07.880]   set of family relation rules,
[01:03:07.880 --> 01:03:09.840]   which we used in order to make sure
[01:03:09.840 --> 01:03:13.320]   that the puzzles do not have any ambiguous relations.
[01:03:13.320 --> 01:03:16.320]   Now, how do we evaluate the adaptation
[01:03:16.320 --> 01:03:18.240]   of the models to different rules?
[01:03:18.240 --> 01:03:20.680]   Now, why is adaptation important?
[01:03:20.680 --> 01:03:22.560]   It is important, let's say, for example,
[01:03:22.560 --> 01:03:25.080]   in recommender systems, you have your model,
[01:03:25.080 --> 01:03:27.840]   which is trained on production for certain products.
[01:03:27.840 --> 01:03:31.200]   And now your company wants to sell some other product.
[01:03:31.200 --> 01:03:35.200]   Now you have to adapt on this new data of products,
[01:03:35.200 --> 01:03:36.800]   because you have to learn the relationship
[01:03:36.800 --> 01:03:38.560]   between these new products.
[01:03:38.560 --> 01:03:40.680]   But if your model already knows
[01:03:40.680 --> 01:03:42.840]   how to pick up these relationships,
[01:03:42.840 --> 01:03:46.160]   it will be able to pick it up really well on this new data.
[01:03:47.080 --> 01:03:50.200]   And also that would be really useful later on
[01:03:50.200 --> 01:03:51.640]   for smart dialogue agents,
[01:03:51.640 --> 01:03:54.080]   which will be able to like converse with you
[01:03:54.080 --> 01:03:56.040]   based on some prior information
[01:03:56.040 --> 01:03:58.560]   that you have already shared with it.
[01:03:58.560 --> 01:04:01.640]   Also, we want to know in system two deep learning
[01:04:01.640 --> 01:04:04.880]   that how do we remember old rules?
[01:04:04.880 --> 01:04:07.600]   For example, when we are going forward
[01:04:07.600 --> 01:04:10.600]   to newer and newer tasks or newer and newer rules,
[01:04:10.600 --> 01:04:13.920]   do we still remember the old rules that we learned before?
[01:04:15.080 --> 01:04:17.400]   So these are the three key requirements
[01:04:17.400 --> 01:04:18.840]   for logical generalization.
[01:04:18.840 --> 01:04:22.120]   We want to inductively reason to unseen compositions,
[01:04:22.120 --> 01:04:24.120]   which is compositionality.
[01:04:24.120 --> 01:04:26.760]   We want to learn to adapt new rule set,
[01:04:26.760 --> 01:04:30.080]   which is multitask or even meta-learning.
[01:04:30.080 --> 01:04:32.840]   So these are two different sets of problems.
[01:04:32.840 --> 01:04:35.120]   In this paper, we only looked at multitask,
[01:04:35.120 --> 01:04:40.120]   but meta-learning can easily be done on top of this data.
[01:04:40.120 --> 01:04:43.640]   And we want to see how does the models
[01:04:43.640 --> 01:04:45.480]   remember the learned rules?
[01:04:45.480 --> 01:04:46.920]   So given a new set of rules,
[01:04:46.920 --> 01:04:49.760]   the learner should not forget what it has learned before.
[01:04:49.760 --> 01:04:55.120]   So now we come to the current paper,
[01:04:55.120 --> 01:04:57.320]   which is again, another dataset,
[01:04:57.320 --> 01:05:00.480]   but this dataset is focused purely on graph
[01:05:00.480 --> 01:05:01.800]   because in the previous paper,
[01:05:01.800 --> 01:05:04.160]   we found that the graphs have performed so well.
[01:05:04.160 --> 01:05:05.200]   So then we thought, okay,
[01:05:05.200 --> 01:05:09.160]   let's see how far we can stress test graph neural networks.
[01:05:09.160 --> 01:05:13.040]   And this is a multipurpose, multirational graph dataset,
[01:05:13.040 --> 01:05:16.600]   which is built using the rules of first order logic,
[01:05:16.600 --> 01:05:18.440]   but unlike clutter,
[01:05:18.440 --> 01:05:21.160]   so clutter was only built on top of 15 rules
[01:05:21.160 --> 01:05:23.480]   and still it was challenging enough.
[01:05:23.480 --> 01:05:26.400]   In this work, we went a bit more crazy.
[01:05:26.400 --> 01:05:29.720]   We generated a large bunch of rules.
[01:05:29.720 --> 01:05:31.520]   And then we said, okay,
[01:05:31.520 --> 01:05:34.120]   once we have generated this large bunch of rules,
[01:05:34.120 --> 01:05:38.160]   we want to basically sample rules from this large set
[01:05:38.160 --> 01:05:40.640]   in order to create different worlds.
[01:05:40.640 --> 01:05:44.440]   Now, different worlds essentially mean different buckets.
[01:05:44.440 --> 01:05:47.160]   So each bucket contains a set of rules,
[01:05:47.160 --> 01:05:50.800]   but these set of rules are overlapping with each other.
[01:05:50.800 --> 01:05:52.800]   So let's say there could be a scenario
[01:05:52.800 --> 01:05:54.520]   where there are two worlds,
[01:05:54.520 --> 01:05:58.000]   the two worlds have the exact same set of rules except one.
[01:05:58.000 --> 01:06:02.320]   So then that two worlds are kind of 95% similar
[01:06:02.320 --> 01:06:04.880]   based on the number of rules that are available.
[01:06:04.880 --> 01:06:09.720]   So that gives us a really nice way to define similarity
[01:06:09.720 --> 01:06:11.920]   and similarity is important
[01:06:11.920 --> 01:06:14.160]   because it helps us to define
[01:06:14.160 --> 01:06:18.560]   how far your task is going out of the current distribution.
[01:06:18.560 --> 01:06:21.440]   So when people are interested in auto domain distribution,
[01:06:21.440 --> 01:06:23.680]   the standard problem that people face
[01:06:23.680 --> 01:06:27.080]   is that there is no such metric on different datasets
[01:06:27.080 --> 01:06:30.640]   to define what is exactly an out of domain distribution.
[01:06:30.640 --> 01:06:32.560]   But if you define your dataset
[01:06:32.560 --> 01:06:35.040]   based on standard first order logic,
[01:06:35.040 --> 01:06:38.840]   you get a really nice interface to do that.
[01:06:38.840 --> 01:06:42.200]   And that's exactly what we did in Graphlog.
[01:06:42.200 --> 01:06:45.440]   So essentially each world,
[01:06:45.440 --> 01:06:47.800]   so these are like W1, W2, WN,
[01:06:47.800 --> 01:06:49.280]   these are different worlds
[01:06:49.280 --> 01:06:53.280]   and each world consists of 5,000 graphs in training,
[01:06:53.280 --> 01:06:55.080]   1,000 in testing.
[01:06:55.080 --> 01:06:57.600]   And then these different graphs,
[01:06:57.600 --> 01:07:01.840]   so essentially we had a total of 57 different worlds.
[01:07:01.840 --> 01:07:05.760]   And the number of graphs are also quite large
[01:07:05.760 --> 01:07:08.720]   because there's 5,000 in each of these worlds.
[01:07:08.720 --> 01:07:13.720]   And also each world has its own world specific graph.
[01:07:13.720 --> 01:07:17.240]   So basically what is a world specific graph?
[01:07:17.240 --> 01:07:21.280]   So essentially when we are generating graphs for each world,
[01:07:21.280 --> 01:07:23.720]   we first generate a huge graph.
[01:07:23.720 --> 01:07:26.000]   So this graph is basically generated
[01:07:26.000 --> 01:07:30.360]   by applying all the rules over and over and over and over.
[01:07:30.360 --> 01:07:32.040]   And when you have this huge graph,
[01:07:32.040 --> 01:07:34.120]   you're sampling smaller and smaller graphs
[01:07:34.120 --> 01:07:36.080]   from this huge set.
[01:07:36.080 --> 01:07:37.560]   And so these smaller graphs
[01:07:37.560 --> 01:07:39.640]   essentially become your training set.
[01:07:39.640 --> 01:07:42.600]   But then another sample of this huge graph
[01:07:42.600 --> 01:07:45.280]   is your context representation.
[01:07:45.280 --> 01:07:47.560]   Now, why is context representation useful?
[01:07:47.560 --> 01:07:49.760]   Because if you want to do multitask learning,
[01:07:49.760 --> 01:07:52.440]   if you do not have any sense of context,
[01:07:52.440 --> 01:07:56.240]   then the models would essentially not perform well at all.
[01:07:56.240 --> 01:07:58.320]   Because the model is essentially learning
[01:07:58.320 --> 01:08:01.080]   some objective function in some task
[01:08:01.080 --> 01:08:03.200]   and then it is switching to another task
[01:08:03.200 --> 01:08:07.680]   and then it would drastically not perform at all.
[01:08:07.680 --> 01:08:09.760]   So what would be a context here?
[01:08:09.760 --> 01:08:12.160]   So since we are looking at a relation prediction,
[01:08:12.160 --> 01:08:15.480]   the context are essentially the relation embeddings.
[01:08:15.480 --> 01:08:18.120]   And since we are providing a world graph,
[01:08:18.120 --> 01:08:20.320]   a model can take that world graph
[01:08:20.320 --> 01:08:22.880]   and then output certain relation embeddings
[01:08:22.880 --> 01:08:25.760]   that would be useful for that particular world.
[01:08:25.760 --> 01:08:30.640]   And then the other models can take that particular embedding
[01:08:30.640 --> 01:08:34.720]   and then work on top of it for that current world.
[01:08:34.720 --> 01:08:37.560]   So it's very easy to compute similarity.
[01:08:37.560 --> 01:08:40.680]   It's very easy to compute out of domain distribution.
[01:08:40.680 --> 01:08:45.320]   And so with this help of 5,000 graphs in each world,
[01:08:45.320 --> 01:08:48.680]   number one, you can perform supervised learning as usual.
[01:08:48.680 --> 01:08:50.920]   So the supervised learning essentially consists
[01:08:50.920 --> 01:08:52.880]   of the compositionality test,
[01:08:52.880 --> 01:08:54.760]   because in the test set,
[01:08:54.760 --> 01:08:57.600]   you are essentially figuring out
[01:08:57.600 --> 01:09:00.160]   the unique compositions of relations
[01:09:00.160 --> 01:09:01.960]   which you have never seen before.
[01:09:01.960 --> 01:09:04.680]   Like you are given to predict the edge
[01:09:04.680 --> 01:09:07.240]   or the relationship between a source and a sink,
[01:09:07.240 --> 01:09:08.960]   whereas the composition or the walk
[01:09:08.960 --> 01:09:11.200]   between the source and a sink is unique.
[01:09:11.200 --> 01:09:15.160]   And then you can do multitask learning
[01:09:15.160 --> 01:09:17.040]   on top of all of these worlds.
[01:09:17.040 --> 01:09:20.520]   You can train your models to do a multitask learning.
[01:09:20.520 --> 01:09:22.240]   Essentially that means that you're training
[01:09:22.240 --> 01:09:24.160]   on one world at one epoch
[01:09:24.160 --> 01:09:25.960]   and another world in another epoch.
[01:09:25.960 --> 01:09:27.400]   So essentially you're trying to learn
[01:09:27.400 --> 01:09:30.000]   all the world information at the same time.
[01:09:30.000 --> 01:09:32.520]   Now, in the beginning,
[01:09:32.520 --> 01:09:35.000]   we said that we had this set of rules
[01:09:35.000 --> 01:09:36.600]   generated in the beginning,
[01:09:36.600 --> 01:09:38.640]   and then we are essentially doing a subset
[01:09:38.640 --> 01:09:40.200]   on this set of rules.
[01:09:40.200 --> 01:09:43.080]   So that means by doing a multitask learning,
[01:09:43.080 --> 01:09:48.080]   you will never go into the region of invalid rules,
[01:09:48.080 --> 01:09:51.080]   because all the rules are generated at the same time.
[01:09:51.080 --> 01:09:53.920]   So all the rules are already made sure
[01:09:53.920 --> 01:09:56.520]   that all the rules are consistent with each other.
[01:09:56.520 --> 01:09:59.360]   So by doing this kind of multitask learning,
[01:09:59.360 --> 01:10:03.040]   you won't have to worry about whether the rules
[01:10:03.040 --> 01:10:05.400]   that you're learning in world A are inconsistent
[01:10:05.400 --> 01:10:07.400]   with the rules that you're learning in world B.
[01:10:07.400 --> 01:10:09.720]   Sure, these two rules can be very different,
[01:10:09.720 --> 01:10:12.120]   but if you're putting it together in the same model,
[01:10:12.120 --> 01:10:14.680]   there wouldn't be an inconsistency issue.
[01:10:14.680 --> 01:10:18.040]   And finally, you can do continual learning.
[01:10:18.040 --> 01:10:20.440]   So this is highly exciting
[01:10:20.440 --> 01:10:24.320]   because our work is one of the first work in graph datasets
[01:10:24.320 --> 01:10:29.320]   which has this added objective of continual learning.
[01:10:29.320 --> 01:10:31.960]   And there's a lot of recent work,
[01:10:31.960 --> 01:10:33.960]   exciting work coming up in continual learning
[01:10:33.960 --> 01:10:35.800]   where people are trying to figure out
[01:10:35.800 --> 01:10:39.320]   how to basically see that whether your models
[01:10:39.320 --> 01:10:43.320]   are learning to remember on previous task or not.
[01:10:43.320 --> 01:10:45.640]   Given this set of world splits,
[01:10:45.640 --> 01:10:49.800]   we can define each task as individual world split.
[01:10:49.800 --> 01:10:54.160]   And that is our proposal of doing continual learning
[01:10:54.160 --> 01:10:55.880]   purely on top of graphs.
[01:10:55.880 --> 01:11:00.600]   So before I jump into the results,
[01:11:00.600 --> 01:11:01.920]   so I want to like talk a little bit
[01:11:01.920 --> 01:11:03.880]   about the models that we use.
[01:11:03.880 --> 01:11:08.160]   So essentially we used a lot of models involving RGCN,
[01:11:08.160 --> 01:11:11.040]   which is a popular model used for relation prediction
[01:11:11.040 --> 01:11:15.500]   and graph attention networks and GCNs also.
[01:11:15.500 --> 01:11:17.360]   Now, as I said,
[01:11:17.360 --> 01:11:19.800]   when you're trying to do multitask representation,
[01:11:19.800 --> 01:11:21.320]   you want to have a context.
[01:11:21.320 --> 01:11:23.160]   And how do you get this context?
[01:11:23.160 --> 01:11:27.440]   So we basically, we divided the model into two parts.
[01:11:27.440 --> 01:11:29.480]   One is a representation function,
[01:11:29.480 --> 01:11:32.360]   which takes into input the world model,
[01:11:32.360 --> 01:11:34.000]   or sorry, the world graph.
[01:11:34.000 --> 01:11:37.520]   So essentially each world has a unique world graph.
[01:11:37.520 --> 01:11:39.800]   And this model ingests the world graph
[01:11:39.800 --> 01:11:43.240]   and outputs the set of relation embeddings,
[01:11:43.240 --> 01:11:46.560]   which are contextual for that current world graph.
[01:11:46.560 --> 01:11:48.720]   And then another function,
[01:11:48.720 --> 01:11:50.140]   which is the composition function,
[01:11:50.140 --> 01:11:52.200]   which is also another graph neural network
[01:11:52.200 --> 01:11:55.000]   that takes these relation embeddings,
[01:11:55.000 --> 01:11:57.600]   and it takes all the different 5,000 graphs
[01:11:57.600 --> 01:12:01.280]   and tries to predict the missing relations on top of it.
[01:12:01.280 --> 01:12:04.480]   So in this kind of domain,
[01:12:04.480 --> 01:12:08.840]   we took a lot of combinations of available
[01:12:08.840 --> 01:12:12.600]   like composition and representation functions.
[01:12:12.600 --> 01:12:14.800]   So a very simple representation function
[01:12:14.800 --> 01:12:16.460]   can be just a parameter,
[01:12:16.460 --> 01:12:18.040]   or like just a weight lookup,
[01:12:18.040 --> 01:12:21.760]   or basically like an embedding lookup in terms of an NLP.
[01:12:21.760 --> 01:12:24.000]   So why that wouldn't work?
[01:12:24.000 --> 01:12:26.440]   Because an embedding lookup is essentially learning
[01:12:26.440 --> 01:12:30.400]   to like back propagate to each relations
[01:12:30.400 --> 01:12:32.960]   from all different worlds.
[01:12:32.960 --> 01:12:35.000]   So it basically does not learn
[01:12:35.000 --> 01:12:37.480]   any world specific information.
[01:12:37.480 --> 01:12:39.040]   So on top of that,
[01:12:39.040 --> 01:12:42.160]   we can optimize that by using a GCN
[01:12:42.160 --> 01:12:43.600]   and a graph attention network
[01:12:43.600 --> 01:12:45.880]   as our representation functions,
[01:12:45.880 --> 01:12:48.320]   which take into account the world graph,
[01:12:48.320 --> 01:12:51.600]   and then gives you a set of weights
[01:12:51.600 --> 01:12:56.600]   or basically a set of relations to work on top of.
[01:12:56.600 --> 01:13:00.480]   So, and then the composition function
[01:13:00.480 --> 01:13:03.960]   is essentially a model which can predict relations.
[01:13:03.960 --> 01:13:07.240]   So this model has to operate on the edges
[01:13:07.240 --> 01:13:11.760]   and our GCN was one of the really famous models,
[01:13:11.760 --> 01:13:15.600]   which works on top of these relation prediction tasks.
[01:13:15.600 --> 01:13:17.240]   We had to modify GAT.
[01:13:17.240 --> 01:13:19.920]   So for the clutter task also we modified GAT
[01:13:19.920 --> 01:13:23.520]   to attend over edge representations.
[01:13:23.520 --> 01:13:25.880]   So originally GAT or graph attention networks
[01:13:25.880 --> 01:13:29.480]   is basically during the message passing update,
[01:13:29.480 --> 01:13:31.020]   the graph attention network attends
[01:13:31.020 --> 01:13:33.840]   to the different messages that are coming in.
[01:13:33.840 --> 01:13:36.260]   But in our modified edge GAT,
[01:13:36.260 --> 01:13:38.040]   the graph attention network is attending
[01:13:38.040 --> 01:13:40.000]   to both the messages that are coming in
[01:13:40.000 --> 01:13:42.400]   and also to the edge representation
[01:13:42.400 --> 01:13:44.680]   of the source and the sink.
[01:13:44.680 --> 01:13:47.280]   So this edge GAT is also the same model
[01:13:47.280 --> 01:13:48.480]   that we used in clutter
[01:13:48.480 --> 01:13:51.160]   and that's why we use it here also.
[01:13:51.160 --> 01:13:55.680]   So how do the models fare on generalizability?
[01:13:55.680 --> 01:13:57.600]   So when we do supervised learning,
[01:13:57.600 --> 01:14:02.600]   we find a nice separation of difficulty of different worlds.
[01:14:02.600 --> 01:14:06.760]   So this difficulty is essentially like a separation
[01:14:06.760 --> 01:14:10.440]   based on the number of unique composition
[01:14:10.440 --> 01:14:12.940]   or descriptors available in each world.
[01:14:12.940 --> 01:14:14.280]   So in some of the worlds,
[01:14:14.280 --> 01:14:18.120]   the number of compositions are not enough.
[01:14:18.120 --> 01:14:21.480]   So essentially for each of the graphs that we generate,
[01:14:21.480 --> 01:14:23.480]   these graphs also has the set of noise
[01:14:23.480 --> 01:14:24.760]   that you have seen before.
[01:14:24.760 --> 01:14:27.960]   So in clutter that the types of noise that we generate,
[01:14:27.960 --> 01:14:30.440]   we also use the same types of noise here.
[01:14:30.440 --> 01:14:33.600]   So that adds a lot of other difficulty
[01:14:33.600 --> 01:14:36.480]   and these procedurally generated worlds,
[01:14:36.480 --> 01:14:38.560]   when we do a supervised learning,
[01:14:38.560 --> 01:14:40.960]   then we have a nice like proxy,
[01:14:40.960 --> 01:14:44.520]   which is the supervised learning scores.
[01:14:44.520 --> 01:14:48.060]   And these scores are basically very similar
[01:14:48.060 --> 01:14:49.660]   to each of the different models.
[01:14:49.660 --> 01:14:52.300]   And that gives you how much easy
[01:14:52.300 --> 01:14:54.380]   or difficult each world are.
[01:14:54.380 --> 01:14:58.460]   Now we do multitask learning.
[01:14:58.460 --> 01:14:59.660]   Now in multitask learning,
[01:14:59.660 --> 01:15:01.900]   we increase the number of worlds
[01:15:01.900 --> 01:15:05.900]   and then we see that we hit a saturation point at 20 worlds.
[01:15:05.900 --> 01:15:09.980]   So that can be an outcome of the number of parameters
[01:15:09.980 --> 01:15:11.500]   that we had in our model.
[01:15:11.500 --> 01:15:13.340]   Like when if we probably,
[01:15:13.340 --> 01:15:15.260]   if we increase the number of parameters,
[01:15:15.260 --> 01:15:17.580]   we can increase this number of saturation.
[01:15:17.580 --> 01:15:18.900]   But if you see the accuracy,
[01:15:18.900 --> 01:15:20.900]   the even at number 20 worlds,
[01:15:20.900 --> 01:15:23.060]   the highest accuracy is 60%.
[01:15:23.060 --> 01:15:25.140]   Whereas in individual worlds,
[01:15:25.140 --> 01:15:28.740]   the accuracy goes towards 80 to 85%.
[01:15:28.740 --> 01:15:31.220]   And then we also did multitask learning
[01:15:31.220 --> 01:15:36.220]   on the easy and the on similar and dissimilar splits.
[01:15:36.220 --> 01:15:38.660]   So essentially S is the similar split
[01:15:38.660 --> 01:15:40.460]   where we took the worlds,
[01:15:40.460 --> 01:15:42.860]   which have a similar set of rules,
[01:15:42.860 --> 01:15:45.860]   like which are not different too much from each other.
[01:15:45.860 --> 01:15:47.580]   And D is a set of worlds
[01:15:47.580 --> 01:15:50.760]   which have very dissimilar set of rules.
[01:15:50.760 --> 01:15:52.540]   And then in both of the cases,
[01:15:52.540 --> 01:15:54.980]   we find that graph attention network
[01:15:54.980 --> 01:15:58.100]   and edge-gat combination is performing the best
[01:15:58.100 --> 01:15:59.920]   out of all different basis lines.
[01:15:59.920 --> 01:16:05.180]   So now we look into multitask adaptation.
[01:16:05.180 --> 01:16:06.780]   So in multitask adaptation,
[01:16:06.780 --> 01:16:09.900]   we train using the multitask objective,
[01:16:09.900 --> 01:16:12.660]   and then we adapt based on the weights learned
[01:16:12.660 --> 01:16:14.580]   from this multitask objective.
[01:16:14.580 --> 01:16:15.980]   And we want to see two things.
[01:16:15.980 --> 01:16:18.980]   First, we want to see the final performance
[01:16:18.980 --> 01:16:20.500]   after adaptation.
[01:16:20.500 --> 01:16:24.260]   And then we want to see the adaptation speed.
[01:16:24.260 --> 01:16:27.380]   And during this, so just a clarification,
[01:16:27.380 --> 01:16:29.740]   this is not traditional meta-learning.
[01:16:29.740 --> 01:16:32.100]   This is just simple multitask learning.
[01:16:32.100 --> 01:16:34.060]   And then you're adapting on the weights
[01:16:34.060 --> 01:16:36.540]   that are learned on the multitask learning model,
[01:16:36.540 --> 01:16:39.500]   which is very similar to what we do in transformer models,
[01:16:39.500 --> 01:16:41.380]   where we do pre-training,
[01:16:41.380 --> 01:16:44.820]   and then you basically fine tune on your given task.
[01:16:44.820 --> 01:16:49.340]   So in the graph you see on the left,
[01:16:49.340 --> 01:16:53.860]   on the orange bar is one that we represent,
[01:16:53.860 --> 01:16:57.260]   which has very dissimilar set of rules.
[01:16:57.260 --> 01:16:58.700]   And the blue bar is the one
[01:16:58.700 --> 01:17:01.220]   where we have a very similar set of rules.
[01:17:01.220 --> 01:17:04.660]   And what we see that for most of the models,
[01:17:04.660 --> 01:17:08.420]   when we do multitask training on dissimilar set of worlds,
[01:17:08.420 --> 01:17:12.340]   then the final adaptation is much better
[01:17:12.340 --> 01:17:15.060]   than when we train on similar set of worlds.
[01:17:15.060 --> 01:17:16.580]   So this is a very nice finding,
[01:17:16.580 --> 01:17:19.060]   and that kind of correlates
[01:17:19.060 --> 01:17:21.460]   with a lot of pre-training literature,
[01:17:21.460 --> 01:17:23.060]   that whenever you're doing pre-training,
[01:17:23.060 --> 01:17:25.180]   you should try to incorporate
[01:17:25.180 --> 01:17:28.900]   as much dissimilar set data points as possible
[01:17:28.900 --> 01:17:31.140]   in your pre-training pipeline.
[01:17:31.140 --> 01:17:33.180]   And similarly, we see that
[01:17:33.180 --> 01:17:36.020]   when we look at the gradient updates,
[01:17:36.020 --> 01:17:40.660]   the models which are trained on dissimilar set of worlds
[01:17:40.660 --> 01:17:42.580]   are faster in adaptation.
[01:17:42.580 --> 01:17:48.740]   And finally, we come into the results of continual learning.
[01:17:48.740 --> 01:17:52.220]   In continual learning, we have two plots.
[01:17:52.220 --> 01:17:56.220]   So the blue plot is essentially the current task performance
[01:17:56.220 --> 01:17:58.580]   and the orange plot is the performance
[01:17:58.580 --> 01:18:01.780]   of the mean previous task performances.
[01:18:01.780 --> 01:18:04.220]   So the mean of the previous task performance
[01:18:04.220 --> 01:18:07.620]   is basically falls down rapidly,
[01:18:07.620 --> 01:18:09.740]   and also it is pretty bad.
[01:18:09.740 --> 01:18:12.820]   And that is where we need to improve a lot
[01:18:12.820 --> 01:18:14.500]   in graph literature.
[01:18:14.500 --> 01:18:16.980]   And this data set essentially gives you
[01:18:16.980 --> 01:18:18.700]   a very, very challenging benchmark
[01:18:18.700 --> 01:18:20.260]   to do continual learning.
[01:18:20.260 --> 01:18:22.820]   So in the second plot,
[01:18:22.820 --> 01:18:25.460]   I'm showing the continual learning results
[01:18:25.460 --> 01:18:29.780]   when we ordered the worlds in terms of its difficulty.
[01:18:29.780 --> 01:18:32.060]   So recall that we got a difficulty score
[01:18:32.060 --> 01:18:34.420]   while doing the supervised learning training.
[01:18:34.420 --> 01:18:37.460]   So we ordered the models according to the difficulty score.
[01:18:37.460 --> 01:18:41.260]   And then we see that the single task performance
[01:18:41.260 --> 01:18:43.220]   is according to the model,
[01:18:43.220 --> 01:18:46.540]   but then the mean of previous task performance
[01:18:46.540 --> 01:18:48.300]   is also pretty bad.
[01:18:48.300 --> 01:18:50.980]   And that needs to be done.
[01:18:50.980 --> 01:18:53.500]   So basically there's a lot of grounds to cover
[01:18:53.500 --> 01:18:54.980]   in terms of continual learning
[01:18:54.980 --> 01:18:57.020]   and we believe this is a really,
[01:18:57.020 --> 01:18:58.820]   really challenging benchmark for it.
[01:18:58.820 --> 01:19:03.740]   So essentially the key takeaways in graph log
[01:19:03.740 --> 01:19:05.940]   is that training on diverse data sets
[01:19:05.940 --> 01:19:08.420]   improve the generalization performance
[01:19:08.420 --> 01:19:10.820]   and pre-training on diverse data sets
[01:19:10.820 --> 01:19:14.140]   also help to adapt to newer data sets.
[01:19:14.140 --> 01:19:16.540]   And pre-training on difficult data sets
[01:19:16.540 --> 01:19:18.860]   improve the model adaptation ability.
[01:19:18.860 --> 01:19:22.540]   So difficult also includes like dissimilar data sets,
[01:19:22.540 --> 01:19:23.780]   but if you read the paper,
[01:19:23.780 --> 01:19:27.700]   we also have a topic on how easy and difficult
[01:19:27.700 --> 01:19:30.980]   training works in these kinds of setups.
[01:19:30.980 --> 01:19:34.740]   And we essentially have a lot of work to do
[01:19:34.740 --> 01:19:37.780]   in order to perform true lifelong learning in graphs.
[01:19:37.780 --> 01:19:42.060]   So in the conclusion,
[01:19:42.060 --> 01:19:45.540]   I show you like basically a snapshot of the two data sets
[01:19:45.540 --> 01:19:48.700]   that I've been worked on, on the last couple of years.
[01:19:48.700 --> 01:19:51.380]   And essentially one of them focuses
[01:19:51.380 --> 01:19:52.620]   on the modality of graphs
[01:19:52.620 --> 01:19:55.300]   while the other focuses on the modality of text.
[01:19:55.300 --> 01:19:59.180]   And for both of them, I have like released blog post
[01:19:59.180 --> 01:20:02.020]   and for graph log, we have released an API,
[01:20:02.020 --> 01:20:06.260]   which is a really easy to install PIP installable file.
[01:20:06.260 --> 01:20:10.300]   We also are looking to release the similar kind of API
[01:20:10.300 --> 01:20:11.820]   for clutter as well.
[01:20:11.820 --> 01:20:13.220]   And using this API,
[01:20:13.220 --> 01:20:15.020]   you don't have to worry about data loading.
[01:20:15.020 --> 01:20:17.260]   So I provide data loaders for you
[01:20:17.260 --> 01:20:19.620]   and easy to use experiments
[01:20:19.700 --> 01:20:22.100]   and also easy to use scripts
[01:20:22.100 --> 01:20:23.500]   written in PyTorch Lightning
[01:20:23.500 --> 01:20:27.980]   for you to like run your experiments just like that.
[01:20:27.980 --> 01:20:32.940]   And we have a generator module for both of these data sets.
[01:20:32.940 --> 01:20:35.980]   The generator module for clutter is already out there.
[01:20:35.980 --> 01:20:38.940]   We will be doing a lot of like improvements
[01:20:38.940 --> 01:20:41.220]   based on user feedback on top of it.
[01:20:41.220 --> 01:20:43.580]   And also the generator module for graph log
[01:20:43.580 --> 01:20:44.940]   is coming out very soon.
[01:20:44.940 --> 01:20:48.900]   So yeah, that's mostly it.
[01:20:48.900 --> 01:20:51.580]   I thanks to my awesome collaborators,
[01:20:51.580 --> 01:20:54.660]   Shagun Sodhani, Jin for clutter,
[01:20:54.660 --> 01:20:57.540]   Joel and Will, my supervisors.
[01:20:57.540 --> 01:21:00.620]   And also thanks to my amazing colleagues at Mila
[01:21:00.620 --> 01:21:02.980]   and Fair Montreal for all of their support.
[01:21:02.980 --> 01:21:06.940]   And if you have any questions, you can ask me now
[01:21:06.940 --> 01:21:09.700]   or you can like send me a mail.
[01:21:09.700 --> 01:21:10.540]   Yeah.
[01:21:10.540 --> 01:21:13.780]   And also you can like join our logical ML Slack,
[01:21:13.780 --> 01:21:16.540]   which I've opened to answer more questions.
[01:21:16.540 --> 01:21:18.020]   Thanks.
[01:21:18.020 --> 01:21:20.220]   - Nice. Thank you so much.
[01:21:20.220 --> 01:21:24.820]   So we have two questions already.
[01:21:24.820 --> 01:21:26.500]   So I'll go ahead and do that.
[01:21:26.500 --> 01:21:27.620]   And then the YouTube folks,
[01:21:27.620 --> 01:21:30.300]   you can start asking your questions on YouTube
[01:21:30.300 --> 01:21:32.180]   and we'll get those for you as well.
[01:21:32.180 --> 01:21:33.940]   All right.
[01:21:33.940 --> 01:21:35.860]   So Mohammed asks,
[01:21:35.860 --> 01:21:39.620]   what if the relation is not defined from the text?
[01:21:39.620 --> 01:21:41.100]   Would the model still be forced
[01:21:41.100 --> 01:21:42.740]   to predict the value of the edge?
[01:21:42.740 --> 01:21:46.260]   - So yeah, that's a very good question.
[01:21:46.260 --> 01:21:47.860]   We made sure that the relations
[01:21:47.860 --> 01:21:49.900]   are always defined in the text.
[01:21:49.900 --> 01:21:52.780]   So that kind of error do creep in
[01:21:52.780 --> 01:21:56.140]   if you are working a lot with Amazon Mechanical Turk
[01:21:56.140 --> 01:21:59.180]   and it's a very messy situation out there
[01:21:59.180 --> 01:22:01.860]   because you have to do a lot of like cleaning
[01:22:01.860 --> 01:22:03.420]   on the data that you collect.
[01:22:03.420 --> 01:22:05.820]   So we made sure that the data that we collected
[01:22:05.820 --> 01:22:07.460]   or the templates that we collected
[01:22:07.460 --> 01:22:11.220]   has all the relation identifiers
[01:22:11.220 --> 01:22:13.260]   and without them, you cannot like
[01:22:13.260 --> 01:22:14.420]   essentially solve the puzzle.
[01:22:14.420 --> 01:22:17.140]   So we made sure that you have the relation IDs.
[01:22:17.140 --> 01:22:19.060]   - Nice.
[01:22:19.060 --> 01:22:21.660]   And then the second question is,
[01:22:21.660 --> 01:22:23.340]   rules are super interesting.
[01:22:23.340 --> 01:22:25.820]   Is it possible to inject custom rules
[01:22:25.820 --> 01:22:28.860]   into experiments to see how they perform?
[01:22:28.860 --> 01:22:30.460]   Is it also possible?
[01:22:30.460 --> 01:22:31.300]   Oh, go ahead.
[01:22:31.300 --> 01:22:32.140]   Yeah, just answer that one.
[01:22:32.140 --> 01:22:34.100]   - Yeah, so that's a really nice question.
[01:22:34.100 --> 01:22:37.460]   And yes, it is possible for both of the data sets.
[01:22:37.460 --> 01:22:40.820]   For clutter, you can just easily like edit a YAML file
[01:22:40.820 --> 01:22:42.380]   and do the generations.
[01:22:42.380 --> 01:22:45.060]   So in the YAML file, the way I define the rules
[01:22:45.060 --> 01:22:47.740]   is it's essentially a first order horn clause.
[01:22:47.740 --> 01:22:49.660]   So the horn clause is essentially a rule
[01:22:49.660 --> 01:22:52.500]   which has like a body and a head
[01:22:52.500 --> 01:22:55.060]   where the body composed of like two entities
[01:22:55.060 --> 01:22:57.860]   and the head is the output of that entity.
[01:22:57.860 --> 01:23:00.500]   So you can inject rules in that fashion.
[01:23:00.500 --> 01:23:01.860]   In case of graph log,
[01:23:01.860 --> 01:23:04.780]   we are essentially generating rules on the fly.
[01:23:04.780 --> 01:23:07.340]   So we are not using a single YAML file.
[01:23:07.340 --> 01:23:09.380]   So in case of graph log, what you can do,
[01:23:09.380 --> 01:23:12.380]   you can tweak with that rule generation parameters.
[01:23:12.380 --> 01:23:14.580]   So there's a lot of parameters which says that,
[01:23:14.580 --> 01:23:16.820]   okay, how many symmetric rules we will have?
[01:23:16.820 --> 01:23:20.700]   How many like invertible rules we will have and so on.
[01:23:20.700 --> 01:23:23.860]   You can increase the set of rules like to a lot.
[01:23:23.860 --> 01:23:26.540]   Like if you want to do massive experiments,
[01:23:26.540 --> 01:23:27.780]   you can just play with them.
[01:23:27.780 --> 01:23:30.180]   Like both are essentially data set generators.
[01:23:30.180 --> 01:23:34.620]   - And then a related question,
[01:23:34.620 --> 01:23:37.740]   is it possible to extract rules created
[01:23:37.740 --> 01:23:39.180]   and understood by the model?
[01:23:39.500 --> 01:23:40.500]   - Which?
[01:23:40.500 --> 01:23:41.420]   - Yes.
[01:23:41.420 --> 01:23:44.180]   So your question is whether it's possible
[01:23:44.180 --> 01:23:47.740]   to extract the rules that are understood by the model.
[01:23:47.740 --> 01:23:50.380]   So this is a very interesting research direction
[01:23:50.380 --> 01:23:51.420]   on its own.
[01:23:51.420 --> 01:23:52.740]   For graph neural networks,
[01:23:52.740 --> 01:23:54.580]   there is like several papers
[01:23:54.580 --> 01:23:56.740]   which look exactly at this problem.
[01:23:56.740 --> 01:23:58.420]   Like using graph attention networks,
[01:23:58.420 --> 01:24:01.220]   they try to like predict which of the edges
[01:24:01.220 --> 01:24:04.180]   that the attention are like kind of highest.
[01:24:04.180 --> 01:24:05.420]   And those are the edges
[01:24:05.420 --> 01:24:07.700]   that the model is kind of perceiving.
[01:24:07.700 --> 01:24:09.980]   We haven't done that kind of experiments in here,
[01:24:09.980 --> 01:24:12.940]   but that would be a really interesting way to look at.
[01:24:12.940 --> 01:24:15.420]   - Thanks.
[01:24:15.420 --> 01:24:17.420]   So those are all the questions, but-
[01:24:17.420 --> 01:24:21.420]   - I had a follow up or related question actually to Mani.
[01:24:21.420 --> 01:24:26.420]   So you presented in for the clutter work,
[01:24:26.420 --> 01:24:28.420]   you presented some stuff about generalization
[01:24:28.420 --> 01:24:32.660]   to relations like longer sequences of relations,
[01:24:32.660 --> 01:24:34.220]   reasoning to longer sequences.
[01:24:35.420 --> 01:24:38.380]   So back in the early 90s,
[01:24:38.380 --> 01:24:41.660]   some folks did work with RNNs to do addition.
[01:24:41.660 --> 01:24:44.140]   And you used addition as an example of a similar problem.
[01:24:44.140 --> 01:24:45.820]   And they looked at how long,
[01:24:45.820 --> 01:24:49.020]   like how far out could they,
[01:24:49.020 --> 01:24:52.620]   like how long digits could they add together?
[01:24:52.620 --> 01:24:55.220]   And they were able to extract from the RNN
[01:24:55.220 --> 01:24:56.740]   an actual dynamical system
[01:24:56.740 --> 01:24:59.420]   that the RNN was implementing approximately
[01:24:59.420 --> 01:25:04.420]   that could do addition to basically infinite precision.
[01:25:04.540 --> 01:25:08.660]   Basically, like it was, they could find a set of weights
[01:25:08.660 --> 01:25:10.940]   that could then do even better
[01:25:10.940 --> 01:25:12.940]   than the network had done on the task.
[01:25:12.940 --> 01:25:16.460]   So that's, they use some of the specific ideas of RNNs,
[01:25:16.460 --> 01:25:19.260]   but I'm curious if for some of these like logical ML things
[01:25:19.260 --> 01:25:23.820]   that get closer to traditional computation,
[01:25:23.820 --> 01:25:25.700]   if there's any hope that the networks
[01:25:25.700 --> 01:25:27.780]   that we're building and learning
[01:25:27.780 --> 01:25:30.860]   that we might be able to extract back out from them,
[01:25:30.860 --> 01:25:33.180]   an algorithm, something interpretable,
[01:25:33.180 --> 01:25:36.220]   something that's like compilable down to traditional code,
[01:25:36.220 --> 01:25:39.980]   or whether you think that that's a gap that can't be crossed.
[01:25:39.980 --> 01:25:43.580]   - No, that is also like an exciting research direction
[01:25:43.580 --> 01:25:45.700]   what people do in program synthesis.
[01:25:45.700 --> 01:25:49.260]   So essentially you are trying to like generate the programs
[01:25:49.260 --> 01:25:52.180]   which you are, essentially you are trying to optimize
[01:25:52.180 --> 01:25:55.180]   the programs that you generate on your particular task.
[01:25:55.180 --> 01:25:57.620]   So there is a lot of research going on there.
[01:25:57.620 --> 01:26:01.940]   And I am not super familiar with the research literature,
[01:26:01.940 --> 01:26:06.300]   but the way you mentioned it, it would be like possible.
[01:26:06.300 --> 01:26:09.060]   Like you essentially generate a program
[01:26:09.060 --> 01:26:11.180]   that will do kind of recursive computation
[01:26:11.180 --> 01:26:12.980]   on top of this data set.
[01:26:12.980 --> 01:26:15.420]   And that recursive computation model
[01:26:15.420 --> 01:26:17.100]   is basically what we need.
[01:26:17.100 --> 01:26:19.660]   But that is also like super hard.
[01:26:19.660 --> 01:26:22.500]   And I would also like term that as system two deep learning.
[01:26:22.500 --> 01:26:24.540]   And that is what we want to move towards.
[01:26:24.540 --> 01:26:28.300]   - Thanks.
[01:26:28.300 --> 01:26:30.660]   We have one more question from behind.
[01:26:30.660 --> 01:26:33.340]   - For those who are not familiar with a graph neural networks
[01:26:33.340 --> 01:26:35.900]   do you have a sequence of papers that you recommend
[01:26:35.900 --> 01:26:38.540]   that people read or other resources?
[01:26:38.540 --> 01:26:39.660]   - Sure.
[01:26:39.660 --> 01:26:42.460]   For graph neural networks, I would suggest to start
[01:26:42.460 --> 01:26:47.460]   from the basic GCN architecture by Thomas Kiff.
[01:26:47.460 --> 01:26:50.780]   And then you can look at Peter Velikovic's
[01:26:50.780 --> 01:26:53.620]   graph attention network, which is really popular.
[01:26:53.620 --> 01:26:55.540]   If you want a condensed survey,
[01:26:55.540 --> 01:26:58.500]   so my supervisor, Will Hamilton,
[01:26:58.500 --> 01:27:01.300]   will be releasing a book very, very soon.
[01:27:01.300 --> 01:27:02.460]   So we are super excited.
[01:27:02.460 --> 01:27:04.620]   Like we actually like use that book
[01:27:04.620 --> 01:27:06.940]   for our course this semester.
[01:27:06.940 --> 01:27:10.220]   So that book will contain like a really, really nice
[01:27:10.220 --> 01:27:14.220]   in-depth overview of the entire theory and the applications.
[01:27:14.220 --> 01:27:16.820]   So just look out in his website.
[01:27:16.820 --> 01:27:18.700]   That's all I can say.
[01:27:18.700 --> 01:27:21.100]   - Is there publishing deadline for that yet?
[01:27:21.100 --> 01:27:22.860]   Or like, when is it coming out?
[01:27:22.860 --> 01:27:23.700]   - That I don't know.
[01:27:23.700 --> 01:27:26.900]   I have to ask him, but it should be coming out soon.
[01:27:27.860 --> 01:27:32.580]   You can actually find the online draft on Will's webpage.
[01:27:32.580 --> 01:27:37.580]   So Will's webpage, Will has a link towards his course,
[01:27:37.580 --> 01:27:40.940]   which is COMP 767.
[01:27:40.940 --> 01:27:43.940]   And in that, or it was 766, yeah.
[01:27:43.940 --> 01:27:45.180]   So basically in that course,
[01:27:45.180 --> 01:27:47.740]   you will find the list of notes.
[01:27:47.740 --> 01:27:50.540]   And those notes are ones that he will compile
[01:27:50.540 --> 01:27:51.500]   towards his book.
[01:27:51.500 --> 01:27:53.900]   - Thanks.
[01:27:53.900 --> 01:27:54.820]   All right.
[01:27:54.820 --> 01:27:57.580]   Those are all the questions I see.
[01:27:57.580 --> 01:27:59.500]   Thanks Charles for sharing the link.
[01:27:59.500 --> 01:28:05.260]   Kostov will be in our SAC community at 9 a.m. on Friday.
[01:28:05.260 --> 01:28:06.420]   Answering more questions,
[01:28:06.420 --> 01:28:08.020]   we'll dive deeper into his paper.
[01:28:08.020 --> 01:28:09.620]   It's gonna be a great time.
[01:28:09.620 --> 01:28:13.620]   So yeah, thank you for coming.
[01:28:13.620 --> 01:28:14.740]   - Thank you so much.
[01:28:14.740 --> 01:28:15.580]   Thank you.
[01:28:15.580 --> 01:28:19.780]   - And up next, we have Madison.
[01:28:19.780 --> 01:28:21.780]   I'm super excited to hear from him.
[01:28:21.780 --> 01:28:24.420]   So Madison is a co-founder
[01:28:24.420 --> 01:28:26.500]   and the machine learning team lead
[01:28:26.500 --> 01:28:28.340]   at Indico Data Solutions.
[01:28:28.340 --> 01:28:30.020]   They're based out in Boston.
[01:28:30.020 --> 01:28:34.660]   And Madison is gonna talk about
[01:28:34.660 --> 01:28:36.900]   long-term context in transformers.
[01:28:36.900 --> 01:28:38.820]   And I'm super excited to have him on.
[01:28:38.820 --> 01:28:46.060]   - Hey folks, pleasure to be talking tonight.
[01:28:46.060 --> 01:28:48.420]   So like Lavanya said,
[01:28:48.420 --> 01:28:49.340]   we're gonna be talking about
[01:28:49.340 --> 01:28:51.780]   long-term context in transformers.
[01:28:52.780 --> 01:28:56.540]   And goal of this talk is to give
[01:28:56.540 --> 01:28:59.780]   a pretty shallow look at a breadth of diverse ideas
[01:28:59.780 --> 01:29:01.100]   for long-term attention.
[01:29:01.100 --> 01:29:02.380]   I wanna just make sure you're aware
[01:29:02.380 --> 01:29:04.140]   of some practical tips and tricks
[01:29:04.140 --> 01:29:05.700]   that you might be able to apply
[01:29:05.700 --> 01:29:06.900]   to your own problems
[01:29:06.900 --> 01:29:09.340]   that might even be outside of the NLP field.
[01:29:09.340 --> 01:29:11.500]   And in general, it's going to be an engineering lens
[01:29:11.500 --> 01:29:16.020]   into NLP with a focus on computational and memory cost.
[01:29:16.020 --> 01:29:18.300]   And this is just because of my own background.
[01:29:18.300 --> 01:29:21.300]   So this is the result of a couple of weekends
[01:29:21.300 --> 01:29:23.740]   spent diving into this particular subfield
[01:29:23.740 --> 01:29:26.740]   and doing a bit of writing and blogging on the topic.
[01:29:26.740 --> 01:29:29.940]   And if like me, you're based on the East Coast,
[01:29:29.940 --> 01:29:33.340]   getting a bit tired, having some trouble staying awake,
[01:29:33.340 --> 01:29:36.100]   I've hidden tiny images of Bert on every slide.
[01:29:36.100 --> 01:29:40.660]   So you can use this talk that way instead, if you prefer.
[01:29:40.660 --> 01:29:43.340]   - We appreciate that.
[01:29:43.340 --> 01:29:45.060]   Also, I'm so sorry, I didn't realize
[01:29:45.060 --> 01:29:46.380]   that it's like 10 p.m. for you.
[01:29:46.380 --> 01:29:47.580]   We should have had you go.
[01:29:47.580 --> 01:29:48.900]   - Oh, not at all a problem.
[01:29:50.980 --> 01:29:54.420]   So why might we care about long-term context?
[01:29:54.420 --> 01:29:56.900]   Well, first of all, if we're interested in text generation,
[01:29:56.900 --> 01:29:58.860]   we might want to remember characters
[01:29:58.860 --> 01:30:01.140]   or key plot elements in a story
[01:30:01.140 --> 01:30:04.700]   for more coherent text generation.
[01:30:04.700 --> 01:30:07.900]   We might want to keep a consistent style
[01:30:07.900 --> 01:30:10.180]   throughout longer generated texts.
[01:30:10.180 --> 01:30:12.540]   Or if you're like me and you're in industry,
[01:30:12.540 --> 01:30:16.140]   you might be interested in processing long documents
[01:30:16.140 --> 01:30:18.020]   that reference previous content.
[01:30:18.020 --> 01:30:20.420]   For instance, something like a legal definition.
[01:30:21.060 --> 01:30:24.820]   So most of you at this point
[01:30:24.820 --> 01:30:26.940]   are probably familiar with BERT.
[01:30:26.940 --> 01:30:30.220]   BERT uses this concept called self-attention,
[01:30:30.220 --> 01:30:31.740]   dot product self-attention
[01:30:31.740 --> 01:30:34.660]   to route information between tokens.
[01:30:34.660 --> 01:30:37.140]   And just for kind of a brief overview,
[01:30:37.140 --> 01:30:39.900]   we take our hidden state from a layer,
[01:30:39.900 --> 01:30:42.020]   we project it out to a set of queries,
[01:30:42.020 --> 01:30:44.500]   a set of keys, a set of values.
[01:30:44.500 --> 01:30:48.580]   We compute a measure of agreement between queries and keys
[01:30:48.580 --> 01:30:52.820]   via a matrix multiply to get our attention matrix.
[01:30:52.820 --> 01:30:54.540]   Then we normalize these values
[01:30:54.540 --> 01:30:57.940]   and we use them to weight the values
[01:30:57.940 --> 01:31:00.180]   that were projected out from our hidden state.
[01:31:00.180 --> 01:31:02.300]   And the problem we're going to be focusing on today
[01:31:02.300 --> 01:31:07.580]   is this matrix multiply and this large attention matrix.
[01:31:07.580 --> 01:31:10.140]   Because as our sequences scale in length,
[01:31:10.140 --> 01:31:13.020]   it's these two steps that become particularly problematic
[01:31:13.020 --> 01:31:16.180]   in our network.
[01:31:16.180 --> 01:31:18.260]   Like I said, this is going to be kind of an engineering lens
[01:31:18.260 --> 01:31:19.220]   into the problem.
[01:31:19.220 --> 01:31:22.980]   So let's talk a little bit about runtime complexity.
[01:31:22.980 --> 01:31:25.380]   So for a simple dot product self-attention,
[01:31:25.380 --> 01:31:29.300]   we have as an input for a natural language processing problem,
[01:31:29.300 --> 01:31:32.340]   a tensor of shape ND, where N is our sequence length
[01:31:32.340 --> 01:31:34.860]   and D is our hidden dimension.
[01:31:34.860 --> 01:31:36.300]   And we want to compute the dot product
[01:31:36.300 --> 01:31:38.460]   between every possible token pair.
[01:31:38.460 --> 01:31:40.740]   So to do a little bit of simple math,
[01:31:40.740 --> 01:31:43.860]   we need N by N by D multiplies.
[01:31:43.860 --> 01:31:47.340]   So for typical choices of N, like in BERT defaults,
[01:31:47.340 --> 01:31:50.260]   we have 512 and 768.
[01:31:50.260 --> 01:31:53.900]   That amounts to about 200 million multiplies.
[01:31:53.900 --> 01:31:57.620]   But if we look at a context length of 8096,
[01:31:57.620 --> 01:32:01.220]   so about 16x longer, or exactly 16x longer,
[01:32:01.220 --> 01:32:03.020]   we now have 50 billion multiplies
[01:32:03.020 --> 01:32:06.500]   required to compute our attention matrix.
[01:32:06.500 --> 01:32:09.820]   And more generally, we have an operation that is big O of N
[01:32:09.820 --> 01:32:14.340]   squared D. And it's kind of worth noting that 8096 really
[01:32:14.340 --> 01:32:15.740]   isn't that long in practice.
[01:32:15.740 --> 01:32:18.860]   Like, long books are hundreds of thousands of tokens.
[01:32:18.860 --> 01:32:21.340]   So we have quite a long way to go.
[01:32:21.340 --> 01:32:26.420]   Memory complexity also becomes problematic.
[01:32:26.420 --> 01:32:28.060]   And in some cases, it's more problematic
[01:32:28.060 --> 01:32:31.380]   than the computational cost of dealing with long sequences.
[01:32:31.380 --> 01:32:33.980]   So here, we'll talk about multi-head attention,
[01:32:33.980 --> 01:32:36.180]   where we're looking at a couple of different lenses
[01:32:36.180 --> 01:32:38.460]   into the same sequence.
[01:32:38.460 --> 01:32:41.020]   For multi-head attention, we have a tensor of shape K,
[01:32:41.020 --> 01:32:44.220]   the number of heads, by sequence length, by sequence length,
[01:32:44.220 --> 01:32:47.620]   that represents the attention between the source sequence
[01:32:47.620 --> 01:32:48.780]   and the target sequence.
[01:32:48.780 --> 01:32:51.180]   We'll have some nice visuals a little bit later.
[01:32:51.180 --> 01:32:54.780]   But just for some quick back of the envelope math,
[01:32:54.780 --> 01:32:58.500]   we're going to have to store 12 by 512 by 512 floating point
[01:32:58.500 --> 01:33:02.700]   values, about 3 million floating point values for vanilla BERT,
[01:33:02.700 --> 01:33:06.260]   or about 12 megabytes of activations.
[01:33:06.260 --> 01:33:08.260]   When we scale up to longer context,
[01:33:08.260 --> 01:33:12.980]   this becomes 3 gigabytes, or 800 million floats per layer.
[01:33:12.980 --> 01:33:19.300]   So if we were to have 12 layers of these sample networks,
[01:33:19.300 --> 01:33:21.980]   we would need about 144 megabytes, for example,
[01:33:21.980 --> 01:33:23.300]   for BERT.
[01:33:23.300 --> 01:33:25.340]   And for the longer context setting,
[01:33:25.340 --> 01:33:27.780]   we'd need about 36 gigabytes, for example.
[01:33:27.780 --> 01:33:30.700]   So already, it's pretty clear why we have a problem.
[01:33:30.700 --> 01:33:35.260]   This big O, K, N squared value quickly explodes.
[01:33:35.260 --> 01:33:39.620]   And we can't feasibly store 36 gigabytes worth of activations
[01:33:39.620 --> 01:33:41.380]   in our model during training time
[01:33:41.380 --> 01:33:43.620]   to allow for back propagation.
[01:33:43.620 --> 01:33:47.500]   So let's start diving into some of the details of how
[01:33:47.500 --> 01:33:49.940]   we might resolve this issue.
[01:33:49.940 --> 01:33:52.620]   So first up, we have the cheap engineering solution,
[01:33:52.620 --> 01:33:56.060]   which is, instead of processing the full document at a time,
[01:33:56.060 --> 01:33:58.460]   let's break it up into smaller chunks.
[01:33:58.460 --> 01:34:02.020]   This is kind of good first go-to.
[01:34:02.020 --> 01:34:03.980]   The problem is, within each chunk,
[01:34:03.980 --> 01:34:07.620]   we have no way to use information from other chunks.
[01:34:07.620 --> 01:34:10.500]   And if you're looking or you're interested in filling
[01:34:10.500 --> 01:34:15.140]   in the blanks for tokens on the edges of a given chunk,
[01:34:15.140 --> 01:34:17.740]   you might not have the necessary context
[01:34:17.740 --> 01:34:19.340]   to perform that task well.
[01:34:19.340 --> 01:34:20.900]   So these edges are a problem.
[01:34:20.900 --> 01:34:28.100]   One possible engineering solution to this problem
[01:34:28.100 --> 01:34:30.820]   is simply to compute overlapping chunks
[01:34:30.820 --> 01:34:33.860]   and redo some computation.
[01:34:33.860 --> 01:34:37.060]   So this is twice as expensive, because each chunk overlaps
[01:34:37.060 --> 01:34:38.820]   with half of the previous chunk.
[01:34:38.820 --> 01:34:41.540]   But it helps to partially resolve
[01:34:41.540 --> 01:34:43.540]   the issue we had previously, which
[01:34:43.540 --> 01:34:46.340]   is, we have some tokens on the edge
[01:34:46.340 --> 01:34:48.820]   where we don't have a lot of relevant context.
[01:34:48.820 --> 01:34:50.700]   So in this setting, we're actually just
[01:34:50.700 --> 01:34:54.940]   using the predictions from the middle portion of the chunk
[01:34:54.940 --> 01:34:56.700]   as the outputs for our entire sequence.
[01:34:56.700 --> 01:35:05.860]   If you prefer a more machine learning approach
[01:35:05.860 --> 01:35:07.820]   to solving this problem, I'm going
[01:35:07.820 --> 01:35:10.300]   to be talking over four different paper selections.
[01:35:10.300 --> 01:35:11.740]   We're going to fly through things.
[01:35:11.740 --> 01:35:13.940]   But hopefully, there's some relevant ideas
[01:35:13.940 --> 01:35:15.940]   that you can apply to your own tasks.
[01:35:15.940 --> 01:35:18.620]   So the sparse transformer, the compressive transformer,
[01:35:18.620 --> 01:35:21.580]   a model called the reformer, and the routing transformer.
[01:35:21.580 --> 01:35:27.500]   First up, the sparse transformer.
[01:35:27.500 --> 01:35:30.100]   This is by a group of folks out of OpenAI,
[01:35:30.100 --> 01:35:31.420]   Rewind Child, et al.
[01:35:36.100 --> 01:35:38.820]   This diagram is a visualization of which
[01:35:38.820 --> 01:35:43.260]   tokens a given query token attends to in an input
[01:35:43.260 --> 01:35:44.100]   sequence.
[01:35:44.100 --> 01:35:46.180]   So our query tokens here are denoted
[01:35:46.180 --> 01:35:48.980]   by the dark blue values.
[01:35:48.980 --> 01:35:51.140]   And our target tokens, which we're attending to,
[01:35:51.140 --> 01:35:53.340]   are denoted by the light blue values.
[01:35:53.340 --> 01:35:55.780]   So this is an example of causal attention,
[01:35:55.780 --> 01:35:59.780]   like you might use in an autoregressive language model.
[01:35:59.780 --> 01:36:02.220]   And this is our baseline.
[01:36:02.220 --> 01:36:06.860]   So the colored values represent roughly the number
[01:36:06.860 --> 01:36:08.580]   of floating point operations to get done.
[01:36:08.580 --> 01:36:18.420]   And this is an alternate view into solving the same problem
[01:36:18.420 --> 01:36:21.380]   that the OpenAI team calls sparse attention.
[01:36:21.380 --> 01:36:23.300]   And there are many different kinds of sparsity,
[01:36:23.300 --> 01:36:29.180]   but in particular, they call theirs block sparse attention.
[01:36:29.180 --> 01:36:32.660]   And they've chosen to break up this dense attention operation
[01:36:32.660 --> 01:36:34.940]   into two factors.
[01:36:34.940 --> 01:36:38.300]   So the first factor we have is this block local attention,
[01:36:38.300 --> 01:36:43.060]   where each token only looks at a small local window around it
[01:36:43.060 --> 01:36:45.460]   itself.
[01:36:45.460 --> 01:36:47.860]   And secondly, we have long-term attention,
[01:36:47.860 --> 01:36:50.580]   which is looking at specific tokens
[01:36:50.580 --> 01:36:52.020]   throughout a much longer sequence.
[01:36:52.020 --> 01:36:58.740]   So we've introduced some dilation into our attention.
[01:36:58.740 --> 01:37:04.020]   And what you find is that the network can route information
[01:37:04.020 --> 01:37:08.020]   to these aggregator tokens with the light blue squares.
[01:37:08.020 --> 01:37:10.740]   And then in the next layer, the subsequent layer
[01:37:10.740 --> 01:37:14.060]   of this model, you can attend to these aggregator tokens
[01:37:14.060 --> 01:37:20.460]   to use information from that aggregator token's local block.
[01:37:20.460 --> 01:37:23.660]   So the cool property of the sparse attention model
[01:37:23.660 --> 01:37:27.700]   is that in two layers, you can hypothetically
[01:37:27.700 --> 01:37:31.860]   route any information between two tokens,
[01:37:31.860 --> 01:37:34.380]   so first by local aggregation and then
[01:37:34.380 --> 01:37:36.700]   by attending to these aggregator tokens.
[01:37:36.700 --> 01:37:45.460]   And this actually has some empirical motivation.
[01:37:45.460 --> 01:37:48.460]   If you have ever read the paper, What Does BERT Look At?
[01:37:48.460 --> 01:37:50.940]   An Analysis of BERT's Attention, they
[01:37:50.940 --> 01:37:53.540]   dive into some of the patterns that BERT attention heads
[01:37:53.540 --> 01:37:54.460]   have learned.
[01:37:54.460 --> 01:37:56.420]   And one of the two trends that they find
[01:37:56.420 --> 01:37:58.180]   are attending to separator tokens
[01:37:58.180 --> 01:37:59.700]   and attending to periods.
[01:37:59.700 --> 01:38:01.660]   So BERT has naturally learned that there
[01:38:01.660 --> 01:38:05.900]   exists some aggregator tokens in a sequence
[01:38:05.900 --> 01:38:08.180]   where information is kind of summarized.
[01:38:08.180 --> 01:38:11.900]   And by attending to those summaries,
[01:38:11.900 --> 01:38:15.060]   you can route information over longer distances.
[01:38:15.060 --> 01:38:17.460]   So there's some empirical grounding here
[01:38:17.460 --> 01:38:18.620]   for this approach.
[01:38:18.620 --> 01:38:24.660]   They also had to overcome some computational challenges
[01:38:24.660 --> 01:38:26.740]   in order to make this approach practical.
[01:38:26.740 --> 01:38:28.300]   One of the papers they look to is
[01:38:28.300 --> 01:38:32.580]   called Training Deep Networks with Sublinear Memory Cost
[01:38:32.580 --> 01:38:34.460]   by Chen et al.
[01:38:34.460 --> 01:38:37.020]   And the key idea of training deep networks
[01:38:37.020 --> 01:38:42.260]   with sublinear memory cost is that on the backwards pass,
[01:38:42.260 --> 01:38:45.460]   rather than storing every single one of our activations,
[01:38:45.460 --> 01:38:49.180]   we can locally recompute a subset of the activations
[01:38:49.180 --> 01:38:52.100]   using checkpoints, so specific activations
[01:38:52.100 --> 01:38:53.940]   that we chose to store.
[01:38:53.940 --> 01:38:55.820]   And we go from those checkpoints,
[01:38:55.820 --> 01:38:58.500]   and we recompute a portion of the forward pass,
[01:38:58.500 --> 01:39:00.620]   the other intermediate activations,
[01:39:00.620 --> 01:39:03.700]   so that we can apply backpropagation.
[01:39:03.700 --> 01:39:06.380]   So it's really just a strategy to avoid
[01:39:06.380 --> 01:39:12.820]   caching all of our activations to compute our backwards pass.
[01:39:12.820 --> 01:39:15.260]   And we really need these large batches in order to efficiently
[01:39:15.260 --> 01:39:18.940]   use GPU and TPU hardware that relies
[01:39:18.940 --> 01:39:21.300]   on being able to efficiently parallelize things.
[01:39:22.300 --> 01:39:24.300]   And one thing you'll observe, if you
[01:39:24.300 --> 01:39:26.900]   look at the particular activations,
[01:39:26.900 --> 01:39:29.580]   the sparse attention paper chose to checkpoint.
[01:39:29.580 --> 01:39:33.940]   It chose not to checkpoint this k by n by n attention matrix.
[01:39:33.940 --> 01:39:36.460]   It was so problematic in our initial back
[01:39:36.460 --> 01:39:38.980]   of the envelope calculations.
[01:39:38.980 --> 01:39:42.900]   So it chose instead to checkpoint
[01:39:42.900 --> 01:39:45.340]   some of these smaller activations of the network.
[01:39:50.300 --> 01:39:52.300]   If you want to do some quick math here,
[01:39:52.300 --> 01:39:54.020]   a typical cost of a gradient update
[01:39:54.020 --> 01:39:58.060]   is one forward pass plus one backwards pass.
[01:39:58.060 --> 01:40:00.020]   When we use gradient checkpointing,
[01:40:00.020 --> 01:40:03.260]   we have to do one forward pass plus one backward pass
[01:40:03.260 --> 01:40:04.980]   plus another partial forward pass
[01:40:04.980 --> 01:40:07.620]   to fill in the missing gaps.
[01:40:07.620 --> 01:40:10.220]   So worst case, this is something like 50% slower
[01:40:10.220 --> 01:40:12.500]   for the same batch size, because worst case,
[01:40:12.500 --> 01:40:16.420]   we'd be computing two forward passes and one backwards pass.
[01:40:16.420 --> 01:40:21.740]   But oftentimes, we can save more than double--
[01:40:21.740 --> 01:40:24.700]   we can more than half our memory cost.
[01:40:24.700 --> 01:40:26.700]   And by more than halving our memory cost,
[01:40:26.700 --> 01:40:28.780]   we can more than double our batch size.
[01:40:28.780 --> 01:40:30.220]   And by increasing our batch sizes
[01:40:30.220 --> 01:40:33.260]   by more than a factor of two, we can sometimes actually
[01:40:33.260 --> 01:40:38.140]   see speedups, even though we're doing more computation
[01:40:38.140 --> 01:40:38.700]   per update.
[01:40:38.700 --> 01:40:43.860]   So the key ideas expressed in this paper
[01:40:43.860 --> 01:40:46.220]   are that we can factorize dense attention
[01:40:46.220 --> 01:40:49.020]   to two sparse operations that we run in series
[01:40:49.020 --> 01:40:51.020]   and still hypothetically allow for attention
[01:40:51.020 --> 01:40:53.580]   between any two tokens.
[01:40:53.580 --> 01:40:59.220]   We get O(n) times square root n memory and runtime complexity.
[01:40:59.220 --> 01:41:02.780]   So we've gained a factor of square root n.
[01:41:02.780 --> 01:41:05.740]   And if our n is something like 8,096,
[01:41:05.740 --> 01:41:07.140]   this is pretty significant.
[01:41:07.140 --> 01:41:10.340]   This is something like a factor of 90.
[01:41:10.340 --> 01:41:11.780]   And we can checkpoint activations
[01:41:11.780 --> 01:41:13.420]   to further improve our memory use
[01:41:13.540 --> 01:41:16.340]   and scale out to even longer sequences.
[01:41:16.340 --> 01:41:18.540]   So memory use is very frequently the bottleneck
[01:41:18.540 --> 01:41:20.380]   when we're dealing with very long sequences.
[01:41:20.380 --> 01:41:27.540]   Next up is a compressive transformer.
[01:41:27.540 --> 01:41:30.140]   So this is a paper by Ray et al.
[01:41:30.140 --> 01:41:31.900]   These are folks out of DeepMind.
[01:41:31.900 --> 01:41:36.780]   The compressive transformer is largely
[01:41:36.780 --> 01:41:40.940]   an extension of Transformer XL with one key addition.
[01:41:40.940 --> 01:41:42.740]   We're going to talk about Transformer XL,
[01:41:42.740 --> 01:41:45.100]   and then we're going to talk about the novel contribution
[01:41:45.100 --> 01:41:47.300]   of the compressive transformer.
[01:41:47.300 --> 01:41:49.540]   But I think this gif is kind of useful in illustrating
[01:41:49.540 --> 01:41:52.620]   the three main components of the compressive transformer,
[01:41:52.620 --> 01:41:55.020]   the first being the current chunk we're looking at,
[01:41:55.020 --> 01:41:57.740]   which is labeled sequence in this diagram,
[01:41:57.740 --> 01:42:01.460]   the second being memory, which is simply the previous chunk
[01:42:01.460 --> 01:42:07.260]   that we processed that's gradually moved back in a queue.
[01:42:07.260 --> 01:42:09.460]   And finally, we have compressed memory,
[01:42:09.460 --> 01:42:13.460]   where multiple memory time steps are
[01:42:13.460 --> 01:42:16.460]   compressed into a single state.
[01:42:16.460 --> 01:42:18.540]   So this is a lossy operation, but we
[01:42:18.540 --> 01:42:23.060]   want to learn to lose as little useful information as possible.
[01:42:23.060 --> 01:42:24.460]   So we'll talk a little bit about how
[01:42:24.460 --> 01:42:28.140]   we optimize for losing as little useful information as possible.
[01:42:28.140 --> 01:42:33.660]   First, let's talk about some of the contributions of Transformer
[01:42:33.660 --> 01:42:39.420]   XL, attentive language models beyond a fixed length context.
[01:42:39.420 --> 01:42:41.540]   So as we discussed previously, one
[01:42:41.540 --> 01:42:44.180]   of the easy solutions for dealing with long sequences
[01:42:44.180 --> 01:42:47.020]   is to chunk them up into smaller chunks.
[01:42:47.020 --> 01:42:49.860]   The problem with this is that you have no attention
[01:42:49.860 --> 01:42:53.220]   in between segments.
[01:42:53.220 --> 01:42:55.740]   Transformer XL proposes that we could
[01:42:55.740 --> 01:43:00.300]   cache these prior activations and attend to them,
[01:43:00.300 --> 01:43:03.980]   but not actually compute the gradients
[01:43:03.980 --> 01:43:09.380]   through this operation to save some time and training time.
[01:43:09.380 --> 01:43:12.780]   And this cheap trick gives us a neat property
[01:43:12.780 --> 01:43:19.020]   that when we have a model that is n layers deep, every layer,
[01:43:19.020 --> 01:43:22.540]   we can add some additional context to our model.
[01:43:22.540 --> 01:43:25.100]   So you'll see this token, instead of having a context
[01:43:25.100 --> 01:43:29.100]   length of size 4, has a context length of something like 10
[01:43:29.100 --> 01:43:32.500]   here, because every prior layer can
[01:43:32.500 --> 01:43:36.380]   attend to tokens that might have occurred outside of its context
[01:43:36.380 --> 01:43:37.260]   in the prior layer.
[01:43:37.260 --> 01:43:44.100]   Secondly, the Transformer XL paper
[01:43:44.100 --> 01:43:48.460]   introduced this concept of relative positional encodings.
[01:43:48.460 --> 01:43:51.420]   One of the problems with representing long documents
[01:43:51.420 --> 01:43:55.020]   in chunks is that when we're using absolute position
[01:43:55.020 --> 01:43:58.540]   indices, so each position in a sequence
[01:43:58.540 --> 01:44:01.060]   is just labeled by its index, and we
[01:44:01.060 --> 01:44:04.500]   have an embedding for each index,
[01:44:04.500 --> 01:44:07.740]   chunking resets the counter at 0.
[01:44:07.740 --> 01:44:12.940]   And learning representations for the beginning of a chunk
[01:44:12.940 --> 01:44:16.020]   really isn't that meaningful for long sequences.
[01:44:16.020 --> 01:44:19.060]   Ideally, we'd want position index 0
[01:44:19.060 --> 01:44:21.060]   to mean something like the start of a document,
[01:44:21.060 --> 01:44:22.980]   or honestly, anything that doesn't
[01:44:22.980 --> 01:44:28.100]   rely on taking an arbitrary modulo 512, or modulo chunk
[01:44:28.100 --> 01:44:29.620]   size.
[01:44:29.620 --> 01:44:31.380]   So resetting to 0 doesn't really make sense
[01:44:31.380 --> 01:44:33.020]   when we chunk up documents.
[01:44:33.020 --> 01:44:34.820]   Relative position embedding instead
[01:44:34.820 --> 01:44:37.660]   looks at the difference between two different token indices
[01:44:37.660 --> 01:44:41.100]   and learns a representation for each relative difference.
[01:44:41.100 --> 01:44:43.420]   It also does some more weight sharing,
[01:44:43.420 --> 01:44:44.980]   but for the purposes of this talk,
[01:44:44.980 --> 01:44:46.660]   this is the key contribution.
[01:44:46.660 --> 01:44:48.900]   And when we talk about relative positions,
[01:44:48.900 --> 01:44:51.340]   we have a more sensical interpretation
[01:44:51.340 --> 01:44:52.860]   for long sequences.
[01:44:52.860 --> 01:44:58.260]   The key contribution of this paper
[01:44:58.260 --> 01:45:01.460]   is actually this compression network.
[01:45:01.460 --> 01:45:05.340]   This compression network is a tiny little MLP.
[01:45:05.340 --> 01:45:08.820]   It takes a few memory states and compresses them
[01:45:08.820 --> 01:45:11.940]   into a single compressed memory state.
[01:45:11.940 --> 01:45:13.140]   So it has two parts.
[01:45:13.140 --> 01:45:14.660]   The first is this compression module,
[01:45:14.660 --> 01:45:18.220]   which is this tiny little MLP.
[01:45:18.220 --> 01:45:21.540]   They tried some baselines like mean pooling, max pooling,
[01:45:21.540 --> 01:45:25.300]   the most commonly used memory states, for instance.
[01:45:25.300 --> 01:45:28.140]   But they found that the best objective was actually
[01:45:28.140 --> 01:45:31.540]   to have pretty much a secondary model that
[01:45:31.540 --> 01:45:34.300]   tries to reconstruct the attention
[01:45:34.300 --> 01:45:39.420]   matrix over these memory states from the single compressed
[01:45:39.420 --> 01:45:41.620]   memory state.
[01:45:41.620 --> 01:45:46.580]   So the objective here is we want to mimic the attention
[01:45:46.580 --> 01:45:48.740]   over what we compressed.
[01:45:48.740 --> 01:45:51.460]   And this is more or less trained as a separate network.
[01:45:51.460 --> 01:45:55.420]   So the loss doesn't blend with that of the main network.
[01:45:55.420 --> 01:45:58.060]   And this loss, this secondary loss,
[01:45:58.060 --> 01:46:00.140]   allows them to achieve better results
[01:46:00.140 --> 01:46:02.900]   than some naive heuristics like mean or max
[01:46:02.900 --> 01:46:05.620]   pooling over this sequence.
[01:46:05.620 --> 01:46:08.740]   And we can theoretically attend to tokens, compression rate
[01:46:08.740 --> 01:46:12.540]   times n layers times context size tokens away.
[01:46:12.540 --> 01:46:14.980]   So we get a factor of compression rate
[01:46:14.980 --> 01:46:17.980]   out of this operation over transformer XL.
[01:46:17.980 --> 01:46:24.580]   One often overlooked contribution
[01:46:24.580 --> 01:46:26.900]   is simply the fact that they also introduce a new data
[01:46:26.900 --> 01:46:28.980]   set for this task.
[01:46:28.980 --> 01:46:34.340]   Oftentimes, people benchmark on Wikitext 103, LM1B, and Wiki 8.
[01:46:34.340 --> 01:46:38.140]   These are some of the data sets people use for longer term
[01:46:38.140 --> 01:46:39.940]   sequence modeling.
[01:46:39.940 --> 01:46:41.500]   But to a certain extent, we're kind
[01:46:41.500 --> 01:46:43.620]   of outgrowing those data sets.
[01:46:43.620 --> 01:46:47.980]   And so DeepMind's team built up a new data set called PG19
[01:46:47.980 --> 01:46:49.860]   based off of Project Gutenberg, which
[01:46:49.860 --> 01:46:52.820]   is an open repository of public domain books
[01:46:52.820 --> 01:46:55.180]   you can find at gutenberg.org.
[01:46:55.180 --> 01:46:58.740]   On average, these documents are 20x longer than the LM1B
[01:46:58.740 --> 01:46:59.540]   benchmark.
[01:46:59.540 --> 01:47:01.100]   It's about twice as large.
[01:47:01.100 --> 01:47:04.580]   So data set contributions are hugely important to the field.
[01:47:04.580 --> 01:47:07.260]   And I'm really looking forward to the research that
[01:47:07.260 --> 01:47:13.380]   comes out of new papers that use this better benchmark.
[01:47:13.380 --> 01:47:15.980]   So the key ideas here are this concept
[01:47:15.980 --> 01:47:19.540]   of using relative positional embeddings,
[01:47:19.540 --> 01:47:21.620]   segment level recurrence mechanism,
[01:47:21.620 --> 01:47:23.540]   even if we don't backwards propagate,
[01:47:23.540 --> 01:47:29.420]   even though we don't back propagate the other chunks,
[01:47:29.420 --> 01:47:33.180]   an idea of compressive memory, and the application
[01:47:33.180 --> 01:47:35.260]   of a better data set for more rigorous research.
[01:47:35.260 --> 01:47:44.420]   Next up, we have The Reformer by Nikita Kitev et al.
[01:47:44.420 --> 01:47:46.140]   And I believe he's actually an undergrad.
[01:47:46.140 --> 01:47:50.140]   This is extremely impressive work.
[01:47:50.140 --> 01:47:55.580]   At a high level, The Reformer takes your input sequence.
[01:47:55.580 --> 01:47:59.980]   It applies locality-sensitive hashing
[01:47:59.980 --> 01:48:05.300]   to bucket the input sequence up into these different colors.
[01:48:05.300 --> 01:48:08.380]   We sort by color.
[01:48:08.380 --> 01:48:10.900]   We attend within each color here.
[01:48:10.900 --> 01:48:13.020]   So we chunk things up, and we attend within elements
[01:48:13.020 --> 01:48:14.020]   of the same color.
[01:48:14.020 --> 01:48:16.420]   Chunking is kind of just an implementation detail here.
[01:48:16.420 --> 01:48:18.220]   So for the most part, you can ignore that.
[01:48:18.580 --> 01:48:23.100]   Next up, we have the LoCality-Sensitive Hashing.
[01:48:23.100 --> 01:48:26.500]   First, they found that the idea of having a separate query
[01:48:26.500 --> 01:48:30.100]   and key isn't entirely necessary in order
[01:48:30.100 --> 01:48:32.420]   to attend over a sequence.
[01:48:32.420 --> 01:48:34.100]   So if you actually tie the weights
[01:48:34.100 --> 01:48:38.100]   of the key in the query, you get similar performance.
[01:48:38.100 --> 01:48:40.780]   And they use this as an inspiration
[01:48:40.780 --> 01:48:44.540]   for their locality-sensitive hashing operation.
[01:48:44.540 --> 01:48:47.980]   If you're unfamiliar with locality-sensitive hashing,
[01:48:47.980 --> 01:48:54.980]   the idea is to, in this case, perform some random rotations,
[01:48:54.980 --> 01:48:58.220]   determine which of several buckets
[01:48:58.220 --> 01:49:01.020]   your normalized vector ends up in,
[01:49:01.020 --> 01:49:04.900]   and treat that bucket as a bucket for sorting.
[01:49:04.900 --> 01:49:08.060]   So there's different varieties of locality-sensitive hashing.
[01:49:08.060 --> 01:49:09.820]   This one's based on angular rotations.
[01:49:09.820 --> 01:49:13.660]   There are others based on simply identifying
[01:49:13.660 --> 01:49:17.220]   whether or not a given input vector has
[01:49:17.220 --> 01:49:19.980]   a positive or negative correlation
[01:49:19.980 --> 01:49:21.660]   with another vector.
[01:49:21.660 --> 01:49:24.620]   So it's essentially random projections
[01:49:24.620 --> 01:49:28.860]   that allow us to have this property.
[01:49:28.860 --> 01:49:31.860]   We can go from a high-dimensional space
[01:49:31.860 --> 01:49:35.380]   to a low-dimensional space and still
[01:49:35.380 --> 01:49:38.700]   have similar vectors in the high-dimensional space
[01:49:38.700 --> 01:49:41.940]   appear close together in the low-dimensional space
[01:49:41.940 --> 01:49:44.220]   or frequently occur in the same bucket.
[01:49:44.220 --> 01:49:51.700]   The concept from the reformer that I think
[01:49:51.700 --> 01:49:54.540]   is actually most important--
[01:49:54.540 --> 01:49:57.900]   so personally, I don't think that the locality-sensitive
[01:49:57.900 --> 01:50:01.140]   hashing approach will necessarily stick over time.
[01:50:01.140 --> 01:50:04.620]   But this is a beautiful idea inspired
[01:50:04.620 --> 01:50:08.940]   by residual nets that really allows us to reduce our memory
[01:50:08.940 --> 01:50:11.420]   use during training.
[01:50:11.420 --> 01:50:13.300]   I think this is extremely valuable for training
[01:50:13.300 --> 01:50:15.340]   longer sequences.
[01:50:15.340 --> 01:50:17.140]   So if you recall residual networks,
[01:50:17.140 --> 01:50:20.020]   we represent the output of a layer
[01:50:20.020 --> 01:50:24.180]   as the sum or the normalized sum of your input
[01:50:24.180 --> 01:50:27.140]   plus some function of x.
[01:50:27.140 --> 01:50:29.540]   There is a related concept called
[01:50:29.540 --> 01:50:31.540]   reversible neural networks, where you
[01:50:31.540 --> 01:50:35.820]   have two separate equations, first one being y1
[01:50:35.820 --> 01:50:39.180]   is equal to x1 plus some function of x2,
[01:50:39.180 --> 01:50:44.580]   second being y2 is equal to x2 plus some function of x1.
[01:50:44.580 --> 01:50:50.900]   And it has the special property that we can reconstruct fully
[01:50:50.900 --> 01:50:53.700]   the gradients with respect to our inputs
[01:50:53.700 --> 01:50:56.940]   from the gradients with respect to our outputs
[01:50:56.940 --> 01:50:59.900]   and some amount of recomputation.
[01:50:59.900 --> 01:51:03.260]   So I won't dive into the math, but just take for granted
[01:51:03.260 --> 01:51:06.860]   that we don't need to store more than a single layer's
[01:51:06.860 --> 01:51:09.620]   worth of activations at a time.
[01:51:09.620 --> 01:51:11.540]   And this is absolutely awesome because it
[01:51:11.540 --> 01:51:17.180]   saves us a factor of model depth in memory use during training.
[01:51:17.180 --> 01:51:19.500]   And importantly, it doesn't seem to impact accuracy.
[01:51:19.500 --> 01:51:27.620]   So this requires writing a custom backwards pass operation
[01:51:27.620 --> 01:51:28.740]   for your layers.
[01:51:28.740 --> 01:51:36.700]   For the reformer, that structure looks like this.
[01:51:36.700 --> 01:51:41.220]   And these are just arbitrary chunkings of your input vector,
[01:51:41.220 --> 01:51:43.860]   x1 and x2.
[01:51:43.860 --> 01:51:49.460]   So here, x2, we apply the LSH attention block.
[01:51:49.460 --> 01:51:51.980]   And that is equivalent to f1.
[01:51:51.980 --> 01:51:54.780]   We then sum that with x1.
[01:51:54.780 --> 01:51:57.220]   This is like a residual.
[01:51:57.220 --> 01:52:01.580]   g in the prior slide is the same as our feed-forward block.
[01:52:01.580 --> 01:52:05.580]   And then we apply that with x2.
[01:52:05.580 --> 01:52:08.860]   We have another residual here to output y2 and y1.
[01:52:08.860 --> 01:52:14.180]   So the special structure is just allowing us to save memory use
[01:52:14.180 --> 01:52:14.900]   during training.
[01:52:14.900 --> 01:52:22.980]   Key ideas here being the concept of reversible networks,
[01:52:22.980 --> 01:52:24.660]   the fact that tying keys and queries
[01:52:24.660 --> 01:52:27.980]   doesn't seem to negatively impact model performance
[01:52:27.980 --> 01:52:32.500]   all that much when you measure in terms of perplexity,
[01:52:32.500 --> 01:52:35.620]   and this idea of LSH bucketing.
[01:52:35.620 --> 01:52:37.980]   And the reason this LSH bucketing works
[01:52:37.980 --> 01:52:44.820]   is simply that, like I said before,
[01:52:44.820 --> 01:52:47.460]   we are preserving the property that things
[01:52:47.460 --> 01:52:51.580]   that end up in similar buckets have high attention
[01:52:51.580 --> 01:52:52.500]   activations.
[01:52:52.500 --> 01:52:59.420]   And finally, we have the routing transformers.
[01:52:59.420 --> 01:53:01.060]   This is a paper by Roy et al.
[01:53:01.900 --> 01:53:04.620]   And it's quite a similar idea to the reformer,
[01:53:04.620 --> 01:53:07.060]   except instead of locality-sensitive hashing,
[01:53:07.060 --> 01:53:11.140]   they choose to apply a k-means update rule.
[01:53:11.140 --> 01:53:13.220]   They also take some inspiration from a paper
[01:53:13.220 --> 01:53:15.300]   we discussed earlier, which is the sparse attention
[01:53:15.300 --> 01:53:19.180]   paper from OpenAI, where they use this idea of local
[01:53:19.180 --> 01:53:19.860]   attention.
[01:53:19.860 --> 01:53:23.780]   You can think about it as being attention convolved
[01:53:23.780 --> 01:53:26.620]   across your input sequence.
[01:53:26.620 --> 01:53:28.500]   So they find that this is a pretty good way
[01:53:28.500 --> 01:53:31.220]   to capture local information.
[01:53:31.220 --> 01:53:38.060]   And then based solely on the content of each token's--
[01:53:38.060 --> 01:53:41.220]   based solely on each token's content,
[01:53:41.220 --> 01:53:43.700]   they also apply this routing attention operation.
[01:53:43.700 --> 01:53:51.180]   Here's just a little gif of k-means updates.
[01:53:51.180 --> 01:53:54.380]   And one thing you'll notice is that the size of each token
[01:53:54.380 --> 01:53:55.940]   is actually quite small.
[01:53:55.940 --> 01:53:59.300]   And one thing you'll notice is that the size of each k-means
[01:53:59.300 --> 01:54:02.620]   bucket isn't necessarily the same.
[01:54:02.620 --> 01:54:05.220]   For the purposes of the paper, they just--
[01:54:05.220 --> 01:54:07.540]   instead of applying k-means in a proper way,
[01:54:07.540 --> 01:54:11.380]   they select the k-closest values or the--
[01:54:11.380 --> 01:54:12.660]   let's use a different variable.
[01:54:12.660 --> 01:54:18.060]   We'll say the m-closest values for each cluster center.
[01:54:18.060 --> 01:54:22.180]   And this is just to make sure that we have matrices
[01:54:22.180 --> 01:54:23.820]   that we can easily parallelize.
[01:54:23.820 --> 01:54:25.900]   So if we had matrices of varying widths,
[01:54:25.900 --> 01:54:28.620]   we couldn't easily batch our operations.
[01:54:28.620 --> 01:54:35.820]   So just like normal attention, we
[01:54:35.820 --> 01:54:39.460]   project our hidden state to queries and keys.
[01:54:39.460 --> 01:54:41.900]   We randomly project our queries and keys
[01:54:41.900 --> 01:54:44.980]   with a fixed orthonormal matrix.
[01:54:44.980 --> 01:54:48.060]   Alongside our gradient descent operation,
[01:54:48.060 --> 01:54:52.620]   we run a k-means clustering on the projected vectors.
[01:54:52.620 --> 01:54:53.980]   So this is separate from SGD.
[01:54:53.980 --> 01:54:56.020]   We're applying the k-means update rule
[01:54:56.020 --> 01:55:01.020]   on these projected vectors to get new cluster centers.
[01:55:01.020 --> 01:55:04.380]   We attend only within each k-means cluster.
[01:55:04.380 --> 01:55:06.700]   So in practice, we're attending only to a fixed number
[01:55:06.700 --> 01:55:10.300]   of closest terms to each cluster center.
[01:55:10.300 --> 01:55:13.140]   And this allows us to, instead of having
[01:55:13.140 --> 01:55:16.260]   an O(n squared) operation like dense attention,
[01:55:16.260 --> 01:55:21.100]   use an attention operation that is now O(n by n over k).
[01:55:21.100 --> 01:55:23.460]   And if you select optimal k, this
[01:55:23.460 --> 01:55:28.660]   becomes O(n squared n), just like sparse attention.
[01:55:28.660 --> 01:55:30.460]   So the intuition here is that you
[01:55:30.460 --> 01:55:33.940]   have a large dot product between two tokens.
[01:55:33.940 --> 01:55:36.740]   That probably means that those tokens are likely going
[01:55:36.740 --> 01:55:38.820]   to belong to the same cluster.
[01:55:38.820 --> 01:55:40.980]   And if your attention weights are dominated
[01:55:40.980 --> 01:55:43.020]   by a few key elements, this means
[01:55:43.020 --> 01:55:45.740]   that attending locally within these k-means clusters
[01:55:45.740 --> 01:55:48.660]   is a pretty good approximation of what dense attention is
[01:55:48.660 --> 01:55:51.460]   doing anyhow.
[01:55:51.460 --> 01:55:56.860]   And typically, we do find that many of the attention matrices
[01:55:56.860 --> 01:56:00.260]   empirically invert are dominated by a few large terms
[01:56:00.260 --> 01:56:01.820]   when we inspect the attention weights.
[01:56:01.820 --> 01:56:06.140]   So the key ideas here are, again,
[01:56:06.140 --> 01:56:08.700]   this application of local attention
[01:56:08.700 --> 01:56:12.300]   in combination with a global attention operation based
[01:56:12.300 --> 01:56:15.580]   on k-means clustering of the content of each token.
[01:56:16.580 --> 01:56:20.580]   So in terms of takeaways, what I hope you got out of this talk,
[01:56:20.580 --> 01:56:22.420]   memory use matters.
[01:56:22.420 --> 01:56:25.420]   And practical engineering tricks to reduce memory use
[01:56:25.420 --> 01:56:28.780]   can have pretty big impacts in the kinds of models
[01:56:28.780 --> 01:56:30.980]   that we can train.
[01:56:30.980 --> 01:56:33.700]   Checkpointing gradients is kind of an underrated trick
[01:56:33.700 --> 01:56:36.020]   that you should probably be applying to your own models
[01:56:36.020 --> 01:56:41.580]   if you're not, to get larger batch sizes.
[01:56:41.580 --> 01:56:43.340]   If you have a key idea, you can use it
[01:56:43.340 --> 01:56:47.660]   to compress large matrices into smaller matrices.
[01:56:47.660 --> 01:56:49.500]   If you have expensive operations,
[01:56:49.500 --> 01:56:52.260]   see if you can factorize them into the composition
[01:56:52.260 --> 01:56:53.460]   of two smaller operations.
[01:56:53.460 --> 01:56:58.060]   Another alternative is seeing whether you
[01:56:58.060 --> 01:57:02.780]   can compress large matrices into smaller matrices,
[01:57:02.780 --> 01:57:07.500]   or cluster information together to use
[01:57:07.500 --> 01:57:10.700]   a kind of structured sparsity that approximates
[01:57:10.700 --> 01:57:12.420]   the full dense attention.
[01:57:12.420 --> 01:57:16.140]   So in terms of takeaways, flexible inductive biases
[01:57:16.140 --> 01:57:17.620]   aren't always beneficial.
[01:57:17.620 --> 01:57:21.860]   And oftentimes, taking advantage of our own human intuition
[01:57:21.860 --> 01:57:26.420]   for what is required to solve a given task
[01:57:26.420 --> 01:57:29.740]   can allow us to really make some big efficiency gains.
[01:57:29.740 --> 01:57:35.620]   And although I didn't really have time
[01:57:35.620 --> 01:57:38.020]   to talk about these during the course of this talk,
[01:57:38.020 --> 01:57:39.860]   I would also really recommend taking a look
[01:57:39.860 --> 01:57:44.260]   at the idea of sparse synchronous attention by Yitei.
[01:57:44.260 --> 01:57:46.340]   So sparse synchronous attention uses
[01:57:46.340 --> 01:57:48.620]   the idea of differentiable sorting
[01:57:48.620 --> 01:57:51.340]   to approximate dense attention.
[01:57:51.340 --> 01:57:53.940]   So if you're familiar with the idea of synchronous iterations
[01:57:53.940 --> 01:58:01.660]   or earth-mover assistance, they use those concepts for sparsity.
[01:58:01.660 --> 01:58:04.140]   There's also a paper called Large Memory Layers of Product
[01:58:04.140 --> 01:58:07.900]   Keys that uses another form of factorization
[01:58:07.900 --> 01:58:11.700]   matrices to recall the equivalent of world knowledge
[01:58:11.700 --> 01:58:15.220]   or facts that they can use in sequence modeling.
[01:58:15.220 --> 01:58:17.860]   And finally, there is a paper called Adaptive Span
[01:58:17.860 --> 01:58:21.940]   Transformers that dynamically varies context size per head
[01:58:21.940 --> 01:58:25.620]   to see speedups in terms of how much computation is required,
[01:58:25.620 --> 01:58:27.780]   particularly on CPU, because it's
[01:58:27.780 --> 01:58:31.140]   harder to parallelize their method and apply it to GPU.
[01:58:31.140 --> 01:58:36.380]   That's all I've got for you today.
[01:58:36.380 --> 01:58:37.940]   But hopefully it was interesting,
[01:58:37.940 --> 01:58:40.180]   hopefully provided some exposure to some concepts
[01:58:40.180 --> 01:58:41.860]   that you hadn't been exposed to before,
[01:58:41.860 --> 01:58:44.300]   and gives you at the very least some key terms
[01:58:44.300 --> 01:58:48.220]   to search if you're interested.
[01:58:48.220 --> 01:58:49.700]   That was amazing.
[01:58:49.700 --> 01:58:52.820]   I feel like that was such a calm and detailed explanation
[01:58:52.820 --> 01:58:55.900]   of these papers that I needed to explain all of the machine
[01:58:55.900 --> 01:58:57.660]   learning papers doing.
[01:58:57.660 --> 01:59:02.180]   And I'm sure people feel the same way.
[01:59:02.180 --> 01:59:06.620]   All right, so let's jump into some questions.
[01:59:06.620 --> 01:59:09.020]   I see a lot of people saying how much they love the talk
[01:59:09.020 --> 01:59:11.900]   and how much they love the slides and the GIFs.
[01:59:11.900 --> 01:59:15.940]   I don't see questions yet.
[01:59:15.940 --> 01:59:17.380]   Oh, I see one question.
[01:59:17.380 --> 01:59:21.940]   Someone was like, I'm surprised you left out the long former.
[01:59:21.940 --> 01:59:24.420]   Was there a reason behind it?
[01:59:24.420 --> 01:59:28.140]   The initial reason was just that that particular paper
[01:59:28.140 --> 01:59:30.340]   was introduced after I wrote the blog post
[01:59:30.340 --> 01:59:33.980]   that this talk was based on.
[01:59:33.980 --> 01:59:36.420]   But I do think it's a promising approach.
[01:59:36.420 --> 01:59:39.180]   The thing I really liked about the long former paper
[01:59:39.180 --> 01:59:44.460]   is that they modified an existing pre-trained model
[01:59:44.460 --> 01:59:46.620]   to accept longer context lengths.
[01:59:46.620 --> 01:59:49.340]   So if you're an industry researcher like me
[01:59:49.340 --> 01:59:51.740]   at a small startup where we don't have infinite compute
[01:59:51.740 --> 01:59:55.300]   budgets, they lay out a practical approach
[01:59:55.300 --> 01:59:57.900]   to make modifications to existing models
[01:59:57.900 --> 02:00:00.900]   to solve a specific task, in this case,
[02:00:00.900 --> 02:00:03.540]   using longer term context.
[02:00:03.540 --> 02:00:04.780]   I thought that was super cool.
[02:00:04.780 --> 02:00:09.380]   And that's pretty atypical for papers in this space.
[02:00:09.380 --> 02:00:12.820]   Are you going to do a follow up covering new things
[02:00:12.820 --> 02:00:14.980]   like every few months maybe?
[02:00:14.980 --> 02:00:18.020]   I'm hoping to update the blog post that I wrote a while back
[02:00:18.020 --> 02:00:21.380]   on pragmatic.ml to incorporate some of the newer concepts
[02:00:21.380 --> 02:00:24.020]   as those papers come out.
[02:00:24.020 --> 02:00:26.100]   Ultimately, my goal is just to find papers
[02:00:26.100 --> 02:00:28.820]   that I can apply in my day job working
[02:00:28.820 --> 02:00:31.180]   on document understanding.
[02:00:31.180 --> 02:00:33.900]   Which actually ties nicely into my question, which is,
[02:00:33.900 --> 02:00:35.900]   would you like to tell people about your startup?
[02:00:35.900 --> 02:00:38.660]   Because I think your startup is pretty cool.
[02:00:38.660 --> 02:00:39.140]   Yeah.
[02:00:39.140 --> 02:00:41.420]   So Indico Data Solutions is a company
[02:00:41.420 --> 02:00:47.420]   that builds software tools for non-technical users to use.
[02:00:47.420 --> 02:00:50.180]   In particular, we want to give subject matter experts
[02:00:50.180 --> 02:00:54.300]   the ability to interact with the machine learning model
[02:00:54.300 --> 02:00:57.900]   to automate painful and repetitive portions
[02:00:57.900 --> 02:00:59.580]   of their everyday tasks.
[02:00:59.580 --> 02:01:02.660]   So everything from processing invoices
[02:01:02.660 --> 02:01:06.540]   to pulling structured information out of loans
[02:01:06.540 --> 02:01:09.500]   to one particularly interesting scenario
[02:01:09.500 --> 02:01:12.980]   is that we help folks judge whether or not
[02:01:12.980 --> 02:01:17.420]   dumpsters are overfull in their computer vision workflow.
[02:01:17.420 --> 02:01:19.180]   Interesting.
[02:01:19.180 --> 02:01:20.220]   Who's paying for that?
[02:01:23.260 --> 02:01:25.260]   A garbage company.
[02:01:25.260 --> 02:01:26.340]   Very interesting.
[02:01:26.340 --> 02:01:28.340]   Interesting.
[02:01:28.340 --> 02:01:31.380]   I see another question.
[02:01:31.380 --> 02:01:34.500]   Leonardo asks, do you have empirical results
[02:01:34.500 --> 02:01:37.180]   from the techniques that you covered?
[02:01:37.180 --> 02:01:40.020]   Yes, I wanted to include them.
[02:01:40.020 --> 02:01:42.300]   It's a bit hard because some of the data sets
[02:01:42.300 --> 02:01:46.100]   that folks benchmarked on didn't overlap
[02:01:46.100 --> 02:01:50.620]   or are kind of unfortunately small to get reliable results.
[02:01:50.620 --> 02:01:52.860]   I'd say the papers that had the most impressive
[02:01:52.860 --> 02:01:56.260]   empirical results were the compressive transformer,
[02:01:56.260 --> 02:02:00.100]   the sparse transformer, and the routing transformer.
[02:02:00.100 --> 02:02:02.780]   I think the reformer paper was a little bit of an earlier work,
[02:02:02.780 --> 02:02:05.020]   and I expect follow-ups to do a little bit better.
[02:02:05.020 --> 02:02:10.540]   Charles?
[02:02:10.540 --> 02:02:11.740]   I had a question.
[02:02:11.740 --> 02:02:12.580]   Yeah.
[02:02:12.580 --> 02:02:13.940]   So this is a little bit broader.
[02:02:13.940 --> 02:02:17.540]   I think, yeah, this is super cool stuff.
[02:02:17.540 --> 02:02:22.380]   And making models faster is something--
[02:02:22.380 --> 02:02:23.980]   yeah, I'm not very good at it, and I'm
[02:02:23.980 --> 02:02:25.940]   glad there are people who are.
[02:02:25.940 --> 02:02:29.140]   But I wanted to ask, I guess, a broader question.
[02:02:29.140 --> 02:02:30.660]   A lot of this attention stuff seemed
[02:02:30.660 --> 02:02:35.140]   to be about trying to make sure that the attention could
[02:02:35.140 --> 02:02:39.260]   hold as much of the document as possible in memory
[02:02:39.260 --> 02:02:41.740]   a lot of the time, or computational tricks
[02:02:41.740 --> 02:02:44.340]   to be able to recreate that information later.
[02:02:44.340 --> 02:02:47.140]   But when I read something with tens of thousands of tokens
[02:02:47.140 --> 02:02:51.140]   in it, like a very long essay or a short book,
[02:02:51.140 --> 02:02:53.420]   if I need to remember something from the beginning,
[02:02:53.420 --> 02:02:56.900]   I notice that I don't have the information I need,
[02:02:56.900 --> 02:02:59.980]   then I search back through the text.
[02:02:59.980 --> 02:03:02.440]   And that's something that I think a recurrent network would
[02:03:02.440 --> 02:03:03.180]   be better at.
[02:03:03.180 --> 02:03:05.340]   But I'm just curious if people have thought about,
[02:03:05.340 --> 02:03:08.580]   I don't know, maybe multiple passes of attention,
[02:03:08.580 --> 02:03:10.860]   or a more active attention mechanism,
[02:03:10.860 --> 02:03:14.500]   more akin to an eye scanning something
[02:03:14.500 --> 02:03:16.860]   is used in computer vision, and less akin to these attention
[02:03:16.860 --> 02:03:21.940]   mechanisms that seem to be universal.
[02:03:21.940 --> 02:03:23.460]   Yeah.
[02:03:23.460 --> 02:03:25.180]   I'm blanking on the name of the paper,
[02:03:25.180 --> 02:03:29.820]   but there was a recent paper that was actually
[02:03:29.820 --> 02:03:34.820]   for a knowledge plate, a question answering task,
[02:03:34.820 --> 02:03:38.740]   that first attended over representations
[02:03:38.740 --> 02:03:43.780]   for full documents, and stored cast representations
[02:03:43.780 --> 02:03:45.260]   for each of those documents.
[02:03:45.260 --> 02:03:49.660]   And then specifically queried for relevant documents,
[02:03:49.660 --> 02:03:51.620]   and looked within those documents to gather
[02:03:51.620 --> 02:03:53.460]   facts and information that might be
[02:03:53.460 --> 02:03:55.220]   used to answer a given question.
[02:03:55.220 --> 02:03:57.560]   So I'll find the name of that paper at some other point.
[02:03:57.560 --> 02:04:01.620]   But people are certainly messing around
[02:04:01.620 --> 02:04:06.820]   with ideas of attention after the fact, I suppose.
[02:04:06.820 --> 02:04:07.340]   Is it good?
[02:04:07.340 --> 02:04:07.860]   Yeah.
[02:04:07.860 --> 02:04:08.460]   Yeah.
[02:04:08.460 --> 02:04:10.220]   Yeah, it also sounded-- some of the stuff
[02:04:10.220 --> 02:04:12.020]   that you presented in the middle, where
[02:04:12.020 --> 02:04:14.500]   you had routing attention and things like that,
[02:04:14.500 --> 02:04:17.180]   those were somewhat akin to a multi-scale attention,
[02:04:17.180 --> 02:04:18.720]   where you know that you have to attend
[02:04:18.720 --> 02:04:19.760]   to something in the past.
[02:04:19.760 --> 02:04:21.980]   So that's definitely similar.
[02:04:21.980 --> 02:04:23.660]   But it also seems like something that's
[02:04:23.660 --> 02:04:26.820]   a little bit more in the RL, RNN world,
[02:04:26.820 --> 02:04:31.220]   than the more CNN transformer world.
[02:04:31.220 --> 02:04:32.620]   And some of these approaches even
[02:04:32.620 --> 02:04:35.380]   use something that's a little bit more heuristic-esque,
[02:04:35.380 --> 02:04:37.300]   just like a nearest neighbor search,
[02:04:37.300 --> 02:04:41.740]   to find relevant information to use later.
[02:04:41.740 --> 02:04:42.380]   Yeah.
[02:04:42.380 --> 02:04:43.980]   I think one thing I didn't mention
[02:04:43.980 --> 02:04:45.740]   was this idea of hierarchy.
[02:04:45.740 --> 02:04:48.340]   Hierarchy is another way to approach problems like this.
[02:04:48.340 --> 02:04:52.220]   So for instance, BERT LSTM was mentioned earlier.
[02:04:52.220 --> 02:04:55.820]   That would be an approach where we fix higher level input
[02:04:55.820 --> 02:04:59.340]   features and have a secondary layer of modeling
[02:04:59.340 --> 02:05:00.540]   on top of fixed features.
[02:05:00.540 --> 02:05:09.140]   All right.
[02:05:09.140 --> 02:05:10.060]   Thank you, Madison.
[02:05:10.060 --> 02:05:11.100]   That was amazing.
[02:05:11.100 --> 02:05:12.340]   And thank you for staying up.
[02:05:12.340 --> 02:05:14.980]   I'm so sorry I did not realize.
[02:05:14.980 --> 02:05:15.900]   Thanks for having me.
[02:05:15.900 --> 02:05:16.700]   I'm so glad.
[02:05:16.700 --> 02:05:18.580]   Of course.
[02:05:18.580 --> 02:05:19.300]   All right.
[02:05:19.300 --> 02:05:22.300]   I always imagine people are giving the speakers
[02:05:22.300 --> 02:05:26.060]   a virtual round of applause after every talk.
[02:05:26.060 --> 02:05:30.500]   So we're going to insert that here in the recording.
[02:05:30.500 --> 02:05:33.540]   But next up, we have Stacey.
[02:05:33.540 --> 02:05:36.260]   Stacey is amazing.
[02:05:36.260 --> 02:05:38.100]   She's one of my favorite people at work.
[02:05:38.100 --> 02:05:39.780]   Don't tell my team I said that.
[02:05:39.780 --> 02:05:43.060]   But I love how much she cares about her work.
[02:05:43.060 --> 02:05:44.780]   She's a deep learning engineer.
[02:05:44.780 --> 02:05:47.220]   And she's super smart.
[02:05:47.220 --> 02:05:49.340]   Previously, Stacey worked at Flickr,
[02:05:49.340 --> 02:05:52.700]   where she worked on a lot of really interesting computer
[02:05:52.700 --> 02:05:54.260]   vision applications.
[02:05:54.260 --> 02:05:58.020]   And today, Stacey is going to talk about benchmarks, which
[02:05:58.020 --> 02:06:01.420]   are, as you can see--
[02:06:01.420 --> 02:06:04.140]   actually, I'm going to let her introduce what benchmarks are.
[02:06:04.140 --> 02:06:05.820]   But she's going to talk about, in short,
[02:06:05.820 --> 02:06:08.500]   collaboration around deep learning using benchmarks.
[02:06:08.500 --> 02:06:10.180]   So Stacey, welcome.
[02:06:10.180 --> 02:06:10.820]   Hi.
[02:06:10.820 --> 02:06:11.660]   Great.
[02:06:11.660 --> 02:06:12.620]   Thanks for the intro.
[02:06:12.620 --> 02:06:15.420]   Can you see the slides?
[02:06:15.420 --> 02:06:16.340]   Yeah.
[02:06:16.340 --> 02:06:17.340]   Awesome.
[02:06:17.340 --> 02:06:20.900]   Yeah, so as Lavanya said, I'm Stacey.
[02:06:20.900 --> 02:06:23.660]   I'm a deep learning engineer at Weights and Biases.
[02:06:23.660 --> 02:06:26.620]   And I'm going to talk about benchmarks today
[02:06:26.620 --> 02:06:29.500]   and how they can hopefully support collaborative deep
[02:06:29.500 --> 02:06:33.020]   learning for our planet.
[02:06:33.020 --> 02:06:36.780]   Just as a quick outline, I want to focus on DropWatch,
[02:06:36.780 --> 02:06:39.460]   because that's the most recent benchmark.
[02:06:39.460 --> 02:06:42.580]   That's the one I've been thinking about the most.
[02:06:42.580 --> 02:06:47.060]   And from there, expand into new and upcoming benchmarks,
[02:06:47.060 --> 02:06:48.820]   talk a bit about the collaborative features
[02:06:48.820 --> 02:06:53.460]   that benchmarks enable, and then how you might participate,
[02:06:53.460 --> 02:06:56.260]   and then get into Q&A. So I can also
[02:06:56.260 --> 02:06:58.900]   adjust depending on what kinds of questions folks have.
[02:06:58.900 --> 02:07:04.220]   So jumping right into DropWatch, and this
[02:07:04.220 --> 02:07:07.620]   is work with Andrew Hobbs of UC Davis.
[02:07:07.620 --> 02:07:11.620]   And it actually started as his PhD project,
[02:07:11.620 --> 02:07:17.900]   in which a data set of over 100,000 images,
[02:07:17.900 --> 02:07:20.540]   satellite images of northern Kenya,
[02:07:20.540 --> 02:07:27.060]   was annotated by experts who live in that area
[02:07:27.060 --> 02:07:30.980]   to indicate the level of drought at the location.
[02:07:30.980 --> 02:07:34.100]   And the reason this is a useful and important data set
[02:07:34.100 --> 02:07:37.700]   is that drought is a serious and increasing problem,
[02:07:37.700 --> 02:07:40.420]   in northern Kenya in particular, but in lots
[02:07:40.420 --> 02:07:41.900]   of regions around the world that's
[02:07:41.900 --> 02:07:43.660]   worsening with climate change.
[02:07:43.660 --> 02:07:47.380]   And there is an existing program in Kenya
[02:07:47.380 --> 02:07:49.900]   started by the International Livestock Research
[02:07:49.900 --> 02:07:54.540]   Institute in 2011 that uses index insurance,
[02:07:54.540 --> 02:07:57.900]   or a measure of, in this case, how
[02:07:57.900 --> 02:08:00.140]   bad the environmental conditions are,
[02:08:00.140 --> 02:08:04.780]   to pay folks, farmers, or in this case,
[02:08:04.780 --> 02:08:09.820]   nomadic herders, relative to how bad the conditions are
[02:08:09.820 --> 02:08:12.260]   that year, so that they can use the money
[02:08:12.260 --> 02:08:16.260]   to support themselves and their families
[02:08:16.260 --> 02:08:20.660]   and compensate for the livestock loss, buy food, et cetera.
[02:08:20.660 --> 02:08:24.420]   And this program has been running for, I guess,
[02:08:24.420 --> 02:08:28.740]   nine years now, and it's expanding to other countries.
[02:08:28.740 --> 02:08:31.940]   And it's working pretty well, except when
[02:08:31.940 --> 02:08:34.860]   there is a difference in the prediction
[02:08:34.860 --> 02:08:37.580]   and the actual drought, then that
[02:08:37.580 --> 02:08:43.460]   can convert to a large cost in human terms,
[02:08:43.460 --> 02:08:48.020]   based on either farmers not getting as much money
[02:08:48.020 --> 02:08:51.100]   as they should, or the wrong individuals getting
[02:08:51.100 --> 02:08:53.660]   the wrong payout.
[02:08:53.660 --> 02:08:57.860]   So yeah, as I said, this index-based livestock
[02:08:57.860 --> 02:09:00.700]   insurance currently monitors drought conditions
[02:09:00.700 --> 02:09:02.220]   using satellite.
[02:09:02.220 --> 02:09:06.060]   When drought occurs in Kenya, the program
[02:09:06.060 --> 02:09:08.260]   transfers resources to Pastoralis,
[02:09:08.260 --> 02:09:11.420]   which is the name for these nomadic herders
[02:09:11.420 --> 02:09:12.820]   with mobile money.
[02:09:12.820 --> 02:09:15.740]   And then they can use that to cover household expenses
[02:09:15.740 --> 02:09:17.620]   or livestock needs.
[02:09:17.620 --> 02:09:19.860]   And I have here all the wonderful folks
[02:09:19.860 --> 02:09:22.460]   who supported the data collection for this, which
[02:09:22.460 --> 02:09:25.220]   is Cornell University, Atkinson Center for Sustainable Future,
[02:09:25.220 --> 02:09:25.940]   and ILRI.
[02:09:25.940 --> 02:09:35.260]   The issue with the current method
[02:09:35.260 --> 02:09:39.780]   is that it's pretty good, but it doesn't actually
[02:09:39.780 --> 02:09:44.220]   differentiate between edible plants and non-edible plants
[02:09:44.220 --> 02:09:46.180]   from satellite.
[02:09:46.180 --> 02:09:48.180]   The current method uses Normalized Differenced
[02:09:48.180 --> 02:09:53.940]   Vegetation Index, or NDVI, which basically compares
[02:09:53.940 --> 02:09:57.380]   how green it looks from a satellite image.
[02:09:57.380 --> 02:09:58.860]   But as you can see, the problem is
[02:09:58.860 --> 02:10:01.940]   that this doesn't distinguish between edible and non-edible
[02:10:01.940 --> 02:10:02.780]   plants.
[02:10:02.780 --> 02:10:05.660]   The idea and what led to the creation of this data set
[02:10:05.660 --> 02:10:08.380]   was to crowdsource expert labels of forage quality
[02:10:08.380 --> 02:10:11.740]   and then use computer vision to assess the drought conditions
[02:10:11.740 --> 02:10:14.420]   better.
[02:10:14.420 --> 02:10:16.420]   So what does the data actually look like?
[02:10:16.420 --> 02:10:18.980]   We have ground-level labels.
[02:10:18.980 --> 02:10:22.980]   And these were actual folks in Kenya
[02:10:22.980 --> 02:10:25.220]   who walked around to the location with an iPad
[02:10:25.220 --> 02:10:28.340]   and labeled what the conditions looked
[02:10:28.340 --> 02:10:31.500]   like on the ground at a particular geolocation
[02:10:31.500 --> 02:10:35.820]   with a scale from 0 to 3 for how many cows that location can
[02:10:35.820 --> 02:10:39.420]   feed, 0 being none, total drought,
[02:10:39.420 --> 02:10:41.300]   and 3 being three or more cows.
[02:10:41.300 --> 02:10:47.060]   So that's really pretty good forage quality.
[02:10:47.060 --> 02:10:48.860]   And they also took some ground-level images.
[02:10:48.860 --> 02:10:52.780]   So we have those for validation of the overall data set.
[02:10:52.780 --> 02:10:54.300]   And here you can see some examples
[02:10:54.300 --> 02:10:55.980]   of what that might look like in practice
[02:10:55.980 --> 02:10:57.940]   and why it could be hard to tell just
[02:10:57.940 --> 02:10:59.820]   by looking at the amount of green that's
[02:10:59.820 --> 02:11:02.780]   in the satellite image.
[02:11:02.780 --> 02:11:04.740]   So that's the ground-level labels.
[02:11:04.740 --> 02:11:08.660]   Those are linked with Landsat satellite images
[02:11:08.660 --> 02:11:10.780]   from the same place and time.
[02:11:10.780 --> 02:11:15.420]   And there's higher resolution and more precise satellite
[02:11:15.420 --> 02:11:17.340]   imagery available now, but not at the time
[02:11:17.340 --> 02:11:18.940]   that this data set was collected.
[02:11:18.940 --> 02:11:21.780]   So essentially, with improved imagery,
[02:11:21.780 --> 02:11:24.020]   we could improve the accuracy of the model.
[02:11:24.020 --> 02:11:27.700]   And the goal is to use only these satellite images
[02:11:27.700 --> 02:11:32.220]   to output the prediction or the drought score, 0 being drought
[02:11:32.220 --> 02:11:34.220]   and 3 being really good quality.
[02:11:34.220 --> 02:11:36.260]   And you can see from some of these example images
[02:11:36.260 --> 02:11:37.980]   that it's pretty hard.
[02:11:37.980 --> 02:11:41.060]   And the model is actually only looking at the center region
[02:11:41.060 --> 02:11:45.300]   in these examples.
[02:11:45.300 --> 02:11:47.500]   So that's an overview of the data set.
[02:11:47.500 --> 02:11:50.620]   Now I want to get into how we frame this as a benchmark.
[02:11:50.620 --> 02:11:54.100]   So initially, we launched it in August of last year,
[02:11:54.100 --> 02:11:56.780]   right around the CDPR conference.
[02:11:56.780 --> 02:12:00.460]   And since then--
[02:12:00.460 --> 02:12:03.820]   so I guess it's been about nine months.
[02:12:03.820 --> 02:12:07.420]   Aside from the original authors, we've had nine participants
[02:12:07.420 --> 02:12:12.540]   log over 2,500 experiment runs to the benchmark.
[02:12:12.540 --> 02:12:15.020]   The validation accuracy, which is the metric that we're
[02:12:15.020 --> 02:12:16.900]   using to compare models--
[02:12:16.900 --> 02:12:19.900]   how much better can we do using deep learning
[02:12:19.900 --> 02:12:21.540]   methods on this data set?
[02:12:21.540 --> 02:12:24.620]   And it's a fixed data set, so comparison is easier.
[02:12:24.620 --> 02:12:31.660]   That's been the improvement just from community submissions.
[02:12:31.660 --> 02:12:34.500]   And one particularly cool outcome
[02:12:34.500 --> 02:12:39.300]   is that there's a paper going into the Vision
[02:12:39.300 --> 02:12:44.100]   for Agriculture workshop at CDPR 2020 on climate adaptation
[02:12:44.100 --> 02:12:47.940]   or reliably predicting from imbalanced satellite data
[02:12:47.940 --> 02:12:51.500]   by Ruchit Rawal and Prabhupad Han that cite the DroughtWatch
[02:12:51.500 --> 02:12:53.700]   benchmark and the data set.
[02:12:53.700 --> 02:12:56.300]   And this is an inspiring indication
[02:12:56.300 --> 02:12:58.780]   that folks are actually building on the existing benchmark data
[02:12:58.780 --> 02:13:01.540]   set, looking at the code, and using the models to support
[02:13:01.540 --> 02:13:03.860]   and accelerate their research, which is really
[02:13:03.860 --> 02:13:07.060]   what we want to encourage with these projects.
[02:13:07.060 --> 02:13:08.780]   And of course, we're just getting started.
[02:13:08.780 --> 02:13:14.300]   I'm going to jump into a report shortly.
[02:13:14.300 --> 02:13:16.140]   And here's an overview of what we've
[02:13:16.140 --> 02:13:17.620]   done to improve the performance.
[02:13:17.620 --> 02:13:19.900]   Basically, at the very beginning of this project,
[02:13:19.900 --> 02:13:25.100]   I trained a very simple convolutional neural net
[02:13:25.100 --> 02:13:28.940]   to predict, given the satellite image,
[02:13:28.940 --> 02:13:34.540]   whether it's from 0 to 3 on the drought scale.
[02:13:34.540 --> 02:13:37.500]   And an ongoing task is improving the performance
[02:13:37.500 --> 02:13:39.900]   of this model, which includes exploring
[02:13:39.900 --> 02:13:43.900]   different architectures, tuning different hyperparameters,
[02:13:43.900 --> 02:13:46.180]   also looking at data augmentation.
[02:13:46.180 --> 02:13:49.620]   Because in the original data set, for example,
[02:13:49.620 --> 02:13:52.300]   there's 11 spectral bands for every satellite image.
[02:13:52.300 --> 02:13:54.260]   And some of those are noisy because
[02:13:54.260 --> 02:13:56.300]   of clouds or other factors.
[02:13:56.300 --> 02:13:59.300]   So looking at the variance in that data,
[02:13:59.300 --> 02:14:01.540]   perhaps eliminating some spectral bands,
[02:14:01.540 --> 02:14:03.220]   filtering out the clouds.
[02:14:03.220 --> 02:14:04.300]   There's a class imbalance.
[02:14:04.300 --> 02:14:06.420]   So about 60% of the data set is actually
[02:14:06.420 --> 02:14:12.940]   drought, which makes it harder to train a balanced model.
[02:14:12.940 --> 02:14:14.620]   We also have those labeled ground photos
[02:14:14.620 --> 02:14:16.460]   that I mentioned, which we could potentially
[02:14:16.460 --> 02:14:19.420]   use to label more data and cross-reference them
[02:14:19.420 --> 02:14:22.060]   with the satellite imagery.
[02:14:22.060 --> 02:14:24.940]   And finally, like I said, we only
[02:14:24.940 --> 02:14:28.380]   have a label for the very center of the satellite image.
[02:14:28.380 --> 02:14:30.100]   And the satellite image itself actually
[02:14:30.100 --> 02:14:35.260]   covers a much larger area than we are able to label.
[02:14:35.260 --> 02:14:39.460]   So perhaps by clustering those regions of the same size,
[02:14:39.460 --> 02:14:43.020]   we could label more of the off-center pixels
[02:14:43.020 --> 02:14:45.140]   in the satellite imagery.
[02:14:45.140 --> 02:14:49.460]   And I'm going to try to jump into the report that I made.
[02:14:49.460 --> 02:14:52.340]   Does that still show up?
[02:14:52.340 --> 02:14:53.260]   Yeah, it does.
[02:14:53.260 --> 02:14:54.260]   OK, great.
[02:14:54.260 --> 02:14:58.260]   Yeah, so this is a report covering,
[02:14:58.260 --> 02:15:00.100]   sort of summarizing the work that's
[02:15:00.100 --> 02:15:02.460]   happened on this benchmark.
[02:15:02.460 --> 02:15:04.820]   Again, an overview of the data.
[02:15:04.820 --> 02:15:08.580]   And you can jump in here with really clear code
[02:15:08.580 --> 02:15:10.100]   on how to get started.
[02:15:10.100 --> 02:15:12.780]   I plotted the validation accuracy
[02:15:12.780 --> 02:15:14.260]   for different submissions.
[02:15:14.260 --> 02:15:17.660]   So you can sort of see the improving trend over time here.
[02:15:17.660 --> 02:15:20.620]   There's a lot of different submissions.
[02:15:20.620 --> 02:15:22.220]   I went through, and you can actually
[02:15:22.220 --> 02:15:24.020]   click through the different submissions
[02:15:24.020 --> 02:15:25.860]   that folks have made.
[02:15:25.860 --> 02:15:29.860]   So the current leader is this user,
[02:15:29.860 --> 02:15:35.540]   Yubamba98, with a 77.8% validation accuracy.
[02:15:35.540 --> 02:15:38.660]   And that was using EfficientNet.
[02:15:38.660 --> 02:15:41.220]   And you can, from the benchmark, click
[02:15:41.220 --> 02:15:43.740]   into some of these approaches and see
[02:15:43.740 --> 02:15:47.900]   how folks are trying to improve on the baseline model.
[02:15:47.900 --> 02:15:50.140]   So it looks like using extra convolutional and dropout
[02:15:50.140 --> 02:15:52.300]   layers helped.
[02:15:52.300 --> 02:15:55.940]   Fine-tuning ResNet-50 helped in this case.
[02:15:55.940 --> 02:16:00.140]   And also, Rick137 tried Inception ResNet-V2
[02:16:00.140 --> 02:16:04.300]   with custom activation functions and a clocked learning rate.
[02:16:04.300 --> 02:16:09.260]   And we can sort of plot this improvement here over time
[02:16:09.260 --> 02:16:11.260]   based on when folks submitted.
[02:16:11.260 --> 02:16:14.460]   And 2% is definitely an improvement.
[02:16:14.460 --> 02:16:17.420]   And these are folks who just decided that this data is cool,
[02:16:17.420 --> 02:16:21.940]   and they want to work on it and either use it
[02:16:21.940 --> 02:16:24.820]   to publish their own papers or just contribute
[02:16:24.820 --> 02:16:29.340]   to improving this concrete model.
[02:16:29.340 --> 02:16:33.180]   And there's a lot of room to keep making it better,
[02:16:33.180 --> 02:16:37.300]   because even these very sophisticated architectures
[02:16:37.300 --> 02:16:41.620]   are still pretty close to the original baseline.
[02:16:41.620 --> 02:16:43.740]   Here, I kind of cover how I developed
[02:16:43.740 --> 02:16:48.660]   the original baseline, which is just a nice use case of reports
[02:16:48.660 --> 02:16:50.980]   to say, here's what I tried in this phase.
[02:16:50.980 --> 02:16:54.900]   Here's how I finally got to an improvement.
[02:16:54.900 --> 02:16:58.060]   For me, adding class weights, because it's imbalanced,
[02:16:58.060 --> 02:17:03.540]   and then actually removing some of the channels that
[02:17:03.540 --> 02:17:07.460]   were noisy increased the accuracy by 15%,
[02:17:07.460 --> 02:17:09.060]   which is pretty amazing.
[02:17:09.060 --> 02:17:12.540]   And then I have also set it up with a hyperparameter suite,
[02:17:12.540 --> 02:17:16.980]   which is an awesome feature you can try to just explore what
[02:17:16.980 --> 02:17:19.580]   some of the combinations might be that improve validation
[02:17:19.580 --> 02:17:20.700]   accuracy.
[02:17:20.700 --> 02:17:27.300]   And then you can see which regions of the hyperparameter
[02:17:27.300 --> 02:17:29.860]   space correspond to better performance.
[02:17:29.860 --> 02:17:35.220]   So here, it looks like learning rate pretty much varies.
[02:17:35.220 --> 02:17:39.780]   Maybe L3 size, we want to keep a little lower, and so on.
[02:17:39.780 --> 02:17:46.020]   You can also check what the worst performance is
[02:17:46.020 --> 02:17:49.820]   by selecting this one run.
[02:17:49.820 --> 02:17:51.780]   And there's lots of ideas for what to try next
[02:17:51.780 --> 02:17:53.740]   here at the end of the report.
[02:17:53.740 --> 02:17:58.460]   And at this point, I'll hop back to the slides.
[02:17:58.460 --> 02:18:01.980]   Oh, I guess one other thing I wanted to show
[02:18:01.980 --> 02:18:04.620]   is that you can log the actual examples in Weights and Biases,
[02:18:04.620 --> 02:18:08.020]   and you can see the model's predictions here
[02:18:08.020 --> 02:18:09.980]   on the training data.
[02:18:09.980 --> 02:18:12.980]   Or sorry, the validation data, but you could log either one.
[02:18:12.980 --> 02:18:20.740]   So hopping back, an aspect that I haven't touched on yet,
[02:18:20.740 --> 02:18:24.980]   those are the concrete details of what this problem is like
[02:18:24.980 --> 02:18:27.500]   and what we're trying to solve.
[02:18:27.500 --> 02:18:29.900]   Right now, Andrew is using historical data
[02:18:29.900 --> 02:18:33.500]   to estimate the economic advantage of deploying
[02:18:33.500 --> 02:18:35.100]   a model with this improved index.
[02:18:35.100 --> 02:18:38.500]   So how much better would the predictions have to be,
[02:18:38.500 --> 02:18:40.660]   how much more accurate the model would have to be,
[02:18:40.660 --> 02:18:44.740]   and how that would translate to improvement in terms of payouts
[02:18:44.740 --> 02:18:47.780]   to the farmers, et cetera.
[02:18:47.780 --> 02:18:52.140]   And the next step is partnering with ILRI and Takaful
[02:18:52.140 --> 02:18:54.260]   to actually deploy this model, so ideally
[02:18:54.260 --> 02:18:57.060]   to replace the current model that
[02:18:57.060 --> 02:19:00.740]   uses the normalized difference vegetation index
[02:19:00.740 --> 02:19:10.180]   and really close the loop on letting the broader world have
[02:19:10.180 --> 02:19:12.660]   access to this improved accuracy.
[02:19:12.660 --> 02:19:15.100]   And that's a really important component of benchmarks
[02:19:15.100 --> 02:19:18.620]   is that we want to see the solution actually deployed
[02:19:18.620 --> 02:19:21.500]   in the real world.
[02:19:21.500 --> 02:19:23.740]   In terms of the research contributions,
[02:19:23.740 --> 02:19:26.060]   there's a lot of potential in this project
[02:19:26.060 --> 02:19:28.300]   to aggregate with other data.
[02:19:28.300 --> 02:19:32.100]   For example, other crops like maize, cassava, rice,
[02:19:32.100 --> 02:19:34.940]   use it in other regions, whether that's
[02:19:34.940 --> 02:19:38.780]   nearby Kenya or potentially in completely different
[02:19:38.780 --> 02:19:42.740]   ecosystems, fine tuning across several models.
[02:19:42.740 --> 02:19:44.580]   And transfer learning might do better
[02:19:44.580 --> 02:19:46.820]   than starting from scratch.
[02:19:46.820 --> 02:19:50.500]   And then this generalizes to any agricultural metric.
[02:19:50.500 --> 02:19:53.780]   Basically, if you have any ground level measure or index
[02:19:53.780 --> 02:19:56.500]   that's geo-referenced, like a certain crop
[02:19:56.500 --> 02:20:00.260]   yield at a particular location, how much visual space
[02:20:00.260 --> 02:20:03.020]   the crop takes up, maybe how much of it
[02:20:03.020 --> 02:20:05.260]   is affected by a certain pest, et cetera,
[02:20:05.260 --> 02:20:09.700]   then you can use a similar model to basically make
[02:20:09.700 --> 02:20:12.740]   a prediction of that metric from satellite images alone,
[02:20:12.740 --> 02:20:15.240]   which means you don't need to send folks out into the field.
[02:20:15.240 --> 02:20:19.340]   You don't need to do a much more expensive labeling process,
[02:20:19.340 --> 02:20:21.580]   provided you can train this initial model.
[02:20:21.580 --> 02:20:31.340]   I will take a moment here to jump into the other benchmarks.
[02:20:31.340 --> 02:20:37.340]   I know we've been focused on DroughtWatch for concreteness.
[02:20:37.340 --> 02:20:38.460]   But there's a lot of them.
[02:20:38.460 --> 02:20:42.220]   You can find them at wandb.com/benchmarks.
[02:20:42.220 --> 02:20:46.260]   The other really big one that we did in partnership with GitHub
[02:20:46.260 --> 02:20:54.100]   is CodeSearchNet, which looks at programming code language
[02:20:54.100 --> 02:20:56.300]   in six different languages, I believe,
[02:20:56.300 --> 02:21:00.260]   and tries to learn a mapping between that and comments
[02:21:00.260 --> 02:21:04.820]   from a bunch of open source code that GitHub has shared.
[02:21:04.820 --> 02:21:10.180]   And that lets you do things like search in natural language
[02:21:10.180 --> 02:21:13.740]   for a query like, how do I start a web server?
[02:21:13.740 --> 02:21:15.700]   Or how do I copy a file?
[02:21:15.700 --> 02:21:17.420]   Specify the language, and then go directly
[02:21:17.420 --> 02:21:18.420]   to the code that does it.
[02:21:18.420 --> 02:21:21.020]   And that's just one of the applications.
[02:21:21.020 --> 02:21:25.540]   The ultimate goal is understanding code the way
[02:21:25.540 --> 02:21:29.180]   that we understand language.
[02:21:29.180 --> 02:21:31.740]   And there's been a lot of activity on this benchmark.
[02:21:31.740 --> 02:21:34.340]   This is just the overview.
[02:21:34.340 --> 02:21:41.220]   You can see in the leaderboard, if it updates,
[02:21:41.220 --> 02:21:44.380]   that there's tons and tons of submissions to this one.
[02:21:44.380 --> 02:21:48.420]   And we see them coming in.
[02:21:48.420 --> 02:21:52.260]   So that's a very exciting other benchmark
[02:21:52.260 --> 02:21:53.500]   you might be interested in.
[02:21:53.500 --> 02:21:57.460]   There's a space for discussions.
[02:21:57.460 --> 02:22:00.380]   There's also a ton of other benchmarks.
[02:22:00.380 --> 02:22:02.020]   There's another one with aerial data,
[02:22:02.020 --> 02:22:04.980]   which is this aerial segmentation by drone deploy,
[02:22:04.980 --> 02:22:11.300]   where we have images from drones,
[02:22:11.300 --> 02:22:14.020]   so drone footage shared by drone deploy,
[02:22:14.020 --> 02:22:16.740]   where we're trying to label what the drone is seeing.
[02:22:16.740 --> 02:22:18.820]   So in this case, you might get an image like this,
[02:22:18.820 --> 02:22:20.500]   along with an elevation map.
[02:22:20.500 --> 02:22:24.300]   And what you want to label is, these are houses.
[02:22:24.300 --> 02:22:25.900]   This is vegetation.
[02:22:25.900 --> 02:22:30.660]   Maybe this is a body of water, et cetera.
[02:22:30.660 --> 02:22:33.660]   And again, this comes with a really nice data set,
[02:22:33.660 --> 02:22:36.820]   nice code to read it in, and instructions
[02:22:36.820 --> 02:22:40.460]   on how to get started and train the baseline model.
[02:22:40.460 --> 02:22:43.660]   But of course, you're not required to use any of that.
[02:22:43.660 --> 02:22:47.700]   So folks have written completely their own training script.
[02:22:47.700 --> 02:22:52.340]   You don't have to use any of the FastAI or Keras frameworks.
[02:22:52.340 --> 02:22:54.940]   And it's exciting to see how folks
[02:22:54.940 --> 02:22:59.620]   have gone on to improve on the initial baseline
[02:22:59.620 --> 02:23:00.340]   that we supply.
[02:23:00.340 --> 02:23:09.100]   And on that page, you'll see a bunch of them.
[02:23:09.100 --> 02:23:10.460]   We try to go for a variety.
[02:23:10.460 --> 02:23:12.220]   But as I said, we're just getting started.
[02:23:12.220 --> 02:23:15.060]   So a lot of them are vision-based.
[02:23:15.060 --> 02:23:17.540]   There's a really cool one where there's
[02:23:17.540 --> 02:23:19.740]   a spoiler for the Witness game.
[02:23:19.740 --> 02:23:26.740]   But that can be a fun way to explore vision
[02:23:26.740 --> 02:23:29.900]   without giving away the spoiler.
[02:23:29.900 --> 02:23:31.980]   And we're hoping to keep adding a bunch of these.
[02:23:31.980 --> 02:23:33.940]   One that I'm working on right now
[02:23:33.940 --> 02:23:39.020]   is parsing structured data for campaign finance receipts.
[02:23:39.020 --> 02:23:44.300]   So that entails from plain text, figuring out--
[02:23:44.300 --> 02:23:47.980]   parsing it as a receipt, meaning who paid how much for what ad.
[02:23:47.980 --> 02:23:49.400]   And currently, that data is required
[02:23:49.400 --> 02:23:50.460]   to be publicly released.
[02:23:50.460 --> 02:23:53.940]   But it's not required to be easily readable.
[02:23:53.940 --> 02:23:55.740]   So there's thousands of these receipts
[02:23:55.740 --> 02:23:59.700]   that are readable by a human, but not
[02:23:59.700 --> 02:24:01.060]   in aggregate by a machine.
[02:24:01.060 --> 02:24:04.460]   So that's an exciting benchmark that we hope to have up soon.
[02:24:07.620 --> 02:24:09.940]   And in general, we hope that these benchmarks
[02:24:09.940 --> 02:24:12.820]   are useful for learning new skills as they walk you
[02:24:12.820 --> 02:24:15.620]   through a project, give you a good data set to work on,
[02:24:15.620 --> 02:24:20.300]   maybe a good excuse to try a new framework,
[02:24:20.300 --> 02:24:22.020]   see what other people have tried.
[02:24:22.020 --> 02:24:25.820]   It's good for reproducibility when it's written up this way.
[02:24:25.820 --> 02:24:26.380]   I don't know.
[02:24:26.380 --> 02:24:28.140]   I haven't shown the exact code.
[02:24:28.140 --> 02:24:32.480]   But it's easily accessible from any of these benchmark pages.
[02:24:32.480 --> 02:24:34.900]   The idea is that you can just step through the instructions
[02:24:34.900 --> 02:24:36.700]   and immediately start training a model.
[02:24:36.700 --> 02:24:38.540]   And it's already set up to log all
[02:24:38.540 --> 02:24:41.900]   of the relevant hyperparameters, config, metrics,
[02:24:41.900 --> 02:24:43.060]   two weights and biases.
[02:24:43.060 --> 02:24:46.220]   So you can both see the lifecycle of someone else
[02:24:46.220 --> 02:24:49.540]   training their model and their workflow.
[02:24:49.540 --> 02:24:53.380]   And you can structure your further work on top of that
[02:24:53.380 --> 02:24:57.460]   instead of starting from zero.
[02:24:57.460 --> 02:25:00.220]   I'm excited about the real-world impact of these benchmarks.
[02:25:00.220 --> 02:25:02.140]   And I'm really trying to find more
[02:25:02.140 --> 02:25:05.500]   that are focused on environmental sustainability,
[02:25:05.500 --> 02:25:09.060]   climate change, positive social impact.
[02:25:09.060 --> 02:25:12.460]   But really, the application is useful for anything.
[02:25:12.460 --> 02:25:15.340]   I think any distributed team with shared goals
[02:25:15.340 --> 02:25:17.980]   can structure a benchmark, say, this is the metric
[02:25:17.980 --> 02:25:18.980]   that we care about.
[02:25:18.980 --> 02:25:22.660]   Here's all of our different models that we're training,
[02:25:22.660 --> 02:25:25.180]   how we're approaching it, and using the existing weights
[02:25:25.180 --> 02:25:30.140]   and biases features to basically collaborate on that.
[02:25:30.140 --> 02:25:33.180]   And some ways that you can participate, if you'd like,
[02:25:33.180 --> 02:25:36.700]   is you can join one of our existing benchmarks.
[02:25:36.700 --> 02:25:38.420]   You can definitely start a new benchmark
[02:25:38.420 --> 02:25:40.460]   if you have a data set that you're excited about.
[02:25:40.460 --> 02:25:46.600]   You can email me with any benchmarks
[02:25:46.600 --> 02:25:48.980]   that you think would be cool.
[02:25:48.980 --> 02:25:51.500]   And then I want to generally encourage everyone,
[02:25:51.500 --> 02:25:54.100]   as they do this or as they work on any machine learning project,
[02:25:54.100 --> 02:25:57.700]   to share intermediate results.
[02:25:57.700 --> 02:26:00.420]   I notice that the more folks write up their approach
[02:26:00.420 --> 02:26:02.620]   and describe what they tried and what they're doing,
[02:26:02.620 --> 02:26:05.580]   the easier it is for everyone else hopping onto the project
[02:26:05.580 --> 02:26:08.900]   to catch up and know what's already happened
[02:26:08.900 --> 02:26:12.940]   and not be reinventing the wheel.
[02:26:12.940 --> 02:26:14.860]   And I think collectively, the more we do that,
[02:26:14.860 --> 02:26:17.260]   the faster we'll coalesce around solutions
[02:26:17.260 --> 02:26:21.500]   to these important problems.
[02:26:21.500 --> 02:26:23.260]   And then just growing our community.
[02:26:23.260 --> 02:26:25.180]   I'm sure a lot of you are already in the Slack,
[02:26:25.180 --> 02:26:27.220]   but here's the link again.
[02:26:27.220 --> 02:26:28.780]   I think finding collaborators there,
[02:26:28.780 --> 02:26:32.180]   finding projects to work on there, getting ideas
[02:26:32.180 --> 02:26:36.100]   is one way that we'll continue to build up solutions
[02:26:36.100 --> 02:26:39.020]   to these.
[02:26:39.020 --> 02:26:41.060]   Yeah, and that's all I have.
[02:26:41.060 --> 02:26:45.260]   I'm happy to take questions, jump into any of the details.
[02:26:45.260 --> 02:26:49.620]   And yeah, again, if you're excited about benchmarks
[02:26:49.620 --> 02:26:51.540]   in any form, please email me.
[02:26:51.540 --> 02:26:54.620]   We're just getting started.
[02:26:54.620 --> 02:26:55.460]   Thank you, Stacey.
[02:26:55.460 --> 02:26:57.060]   This is what I love about Stacey.
[02:26:57.060 --> 02:27:00.300]   The rest of the company is building products, writing code,
[02:27:00.300 --> 02:27:02.460]   and she is actually building things
[02:27:02.460 --> 02:27:07.540]   to do good in the world, which is very nice.
[02:27:07.540 --> 02:27:10.780]   And we appreciate that you put so much thought
[02:27:10.780 --> 02:27:12.820]   into every single product that you build,
[02:27:12.820 --> 02:27:15.900]   and you think about its impact into the world.
[02:27:15.900 --> 02:27:19.180]   And that ties into the first question from Michael,
[02:27:19.180 --> 02:27:22.780]   which is, is there a government or nonprofit entity
[02:27:22.780 --> 02:27:25.700]   that your team is in contact with to put
[02:27:25.700 --> 02:27:30.340]   the findings from Drought Watch to good use to save lives?
[02:27:30.340 --> 02:27:33.420]   Yeah, so that's all through my collaborator, Andrew Hubs,
[02:27:33.420 --> 02:27:35.740]   and the International Livestock Research Institute.
[02:27:35.740 --> 02:27:38.860]   And Takaful is, I think, the other company.
[02:27:38.860 --> 02:27:41.620]   And he's also happy to answer questions
[02:27:41.620 --> 02:27:43.620]   and excited for more collaborators.
[02:27:43.620 --> 02:27:46.020]   As he's finishing his PhD thesis,
[02:27:46.020 --> 02:27:51.460]   he's going to transition into full-time making this work.
[02:27:51.460 --> 02:27:53.420]   And I have a question.
[02:27:53.420 --> 02:27:56.900]   So what problems make for good benchmarks?
[02:27:56.900 --> 02:28:00.500]   So if people are thinking about pitching you a benchmark,
[02:28:00.500 --> 02:28:01.900]   what should they?
[02:28:01.900 --> 02:28:04.180]   Yeah, that's a great question.
[02:28:04.180 --> 02:28:05.740]   I think what we really need right now
[02:28:05.740 --> 02:28:09.620]   is to see more examples of collaboration.
[02:28:09.620 --> 02:28:13.420]   So anything where you think existing approaches haven't
[02:28:13.420 --> 02:28:15.860]   worked that well, or you really want
[02:28:15.860 --> 02:28:19.020]   to invite more perspectives.
[02:28:19.020 --> 02:28:21.580]   I mean, really, any benchmark is good.
[02:28:21.580 --> 02:28:25.460]   But I think the more we can see how this tool actually
[02:28:25.460 --> 02:28:29.060]   enables folks to collaborate with each other, as opposed
[02:28:29.060 --> 02:28:34.780]   to maybe just one person solving the whole thing right away,
[02:28:34.780 --> 02:28:36.900]   that would be less interesting because we
[02:28:36.900 --> 02:28:38.500]   don't get to see the evolution of ideas
[02:28:38.500 --> 02:28:41.140]   in the space of possibilities.
[02:28:41.140 --> 02:28:42.580]   That makes sense.
[02:28:42.580 --> 02:28:44.820]   And I know we've talked about this internally,
[02:28:44.820 --> 02:28:46.620]   but people might have questions.
[02:28:46.620 --> 02:28:48.740]   How is this different from Kaggle?
[02:28:48.740 --> 02:28:51.100]   Because you think about this so differently, right?
[02:28:51.100 --> 02:28:53.740]   Yeah, so thank you.
[02:28:53.740 --> 02:28:56.580]   One thing that gets me about Kaggle
[02:28:56.580 --> 02:29:01.140]   is a framing where all of the code is secret,
[02:29:01.140 --> 02:29:04.460]   and there's a certain deadline until which everyone
[02:29:04.460 --> 02:29:07.220]   works competitively to develop their solutions.
[02:29:07.220 --> 02:29:09.700]   And then there might be a collaborative phase.
[02:29:09.700 --> 02:29:12.820]   And of course, there are non-competitive framings
[02:29:12.820 --> 02:29:14.140]   of this.
[02:29:14.140 --> 02:29:16.140]   But what I'm really excited about with benchmarks
[02:29:16.140 --> 02:29:18.060]   is making it inherently collaborative
[02:29:18.060 --> 02:29:22.780]   and hoping that folks are motivated by solving the problem
[02:29:22.780 --> 02:29:26.100]   and by learning more about the space
[02:29:26.100 --> 02:29:31.060]   and working with each other than they are by the prize money
[02:29:31.060 --> 02:29:36.100]   or just the recognition.
[02:29:36.100 --> 02:29:38.780]   Although I'm hoping to get recognition in here too,
[02:29:38.780 --> 02:29:41.300]   ideally even in a way where it's not just like,
[02:29:41.300 --> 02:29:42.980]   these are the lines of code I got
[02:29:42.980 --> 02:29:44.620]   and this is the improvement I made,
[02:29:44.620 --> 02:29:47.460]   but here's how I managed to explain my ideas.
[02:29:47.460 --> 02:29:53.140]   Here's why I thought this approach might be helpful.
[02:29:53.140 --> 02:29:55.340]   Because I think that part is really missing from just
[02:29:55.340 --> 02:29:57.860]   submitting the code that works or just
[02:29:57.860 --> 02:30:00.260]   passing-- having some model that does something
[02:30:00.260 --> 02:30:03.820]   on the validation set that might be really good,
[02:30:03.820 --> 02:30:05.340]   but then we don't know why it works
[02:30:05.340 --> 02:30:09.220]   or how to transfer it to other contexts.
[02:30:09.220 --> 02:30:11.340]   Makes sense.
[02:30:11.340 --> 02:30:15.420]   All right, does anyone have any questions for Stacey
[02:30:15.420 --> 02:30:16.700]   before we wrap this up?
[02:30:16.700 --> 02:30:24.540]   Someone said they joined late and missed almost everything.
[02:30:24.540 --> 02:30:27.860]   That's OK, because we will be posting.
[02:30:27.860 --> 02:30:29.620]   Thank you for still joining us.
[02:30:29.620 --> 02:30:31.820]   We'll be posting all the recordings on our YouTube
[02:30:31.820 --> 02:30:32.900]   channel.
[02:30:32.900 --> 02:30:36.340]   And Kiela, maybe you can pop a link to the playlist
[02:30:36.340 --> 02:30:37.060]   in the chat.
[02:30:37.060 --> 02:30:45.180]   All right, so thank you so much, you guys, for coming.
[02:30:45.180 --> 02:30:48.500]   Sometimes we go for three hours, sometimes we go for two hours.
[02:30:48.500 --> 02:30:50.740]   It's always a good time, though.
[02:30:50.740 --> 02:30:53.260]   And we've been here for 2 and 1/2 hours.
[02:30:53.260 --> 02:30:55.460]   And I appreciate your patience.
[02:30:55.460 --> 02:30:57.740]   I appreciate Madison, who's been up for--
[02:30:57.740 --> 02:31:01.420]   I don't know what time it is for you, but seems pretty late.
[02:31:01.420 --> 02:31:03.300]   Thank you, Kostov.
[02:31:03.300 --> 02:31:05.820]   Thank you, Charles, for your thoughtful questions,
[02:31:05.820 --> 02:31:06.860]   as always.
[02:31:06.860 --> 02:31:09.300]   Thank you, Stacey, for being a better person than I will ever
[02:31:09.300 --> 02:31:14.020]   be and still being a badass machine learning engineer.
[02:31:14.020 --> 02:31:18.100]   And Tyler left, but his talk was amazing, too.
[02:31:18.100 --> 02:31:20.500]   And thank you, Kiela.
[02:31:20.500 --> 02:31:22.660]   Kiela is so fast.
[02:31:22.660 --> 02:31:25.300]   She will take the stream and split them
[02:31:25.300 --> 02:31:28.380]   into recordings that we then share
[02:31:28.380 --> 02:31:30.900]   with over 20,000 people.
[02:31:30.900 --> 02:31:32.060]   So thank you, Kiela.
[02:31:32.060 --> 02:31:34.540]   This would not be possible without you.
[02:31:34.540 --> 02:31:36.220]   The next step that's going to happen
[02:31:36.220 --> 02:31:39.820]   is, in case you miss one or a couple talks,
[02:31:39.820 --> 02:31:42.020]   we are going to continue to promote them
[02:31:42.020 --> 02:31:44.060]   over the next few weeks.
[02:31:44.060 --> 02:31:45.660]   So you can catch them there.
[02:31:45.660 --> 02:31:47.700]   The speakers will be--
[02:31:47.700 --> 02:31:51.980]   we'll link their Twitter information below the videos.
[02:31:51.980 --> 02:31:53.400]   But also, they'll be in the Slack.
[02:31:53.400 --> 02:31:55.540]   So if you have more questions, ask them.
[02:31:55.540 --> 02:31:57.220]   Thank you so much for joining us.
[02:31:57.220 --> 02:31:59.900]   And we'll see you not the next Tuesday,
[02:31:59.900 --> 02:32:02.580]   but the Tuesday after that.
[02:32:02.580 --> 02:32:12.580]   [BLANK_AUDIO]

