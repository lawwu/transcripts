
[00:00:00.000 --> 00:00:03.440]   [MUSIC PLAYING]
[00:00:03.440 --> 00:00:10.120]   Just to remind you of our overall structure,
[00:00:10.120 --> 00:00:13.720]   we're figuring out what math has to do with machine learning.
[00:00:13.720 --> 00:00:16.440]   And even though all programming involves math at some level,
[00:00:16.440 --> 00:00:19.400]   machine learning is a particular kind of programming.
[00:00:19.400 --> 00:00:21.440]   It's programming by optimization.
[00:00:21.440 --> 00:00:24.360]   Rather than telling a computer exactly what we want it to do,
[00:00:24.360 --> 00:00:26.600]   we tell the computer, here's what it means
[00:00:26.600 --> 00:00:28.480]   to get better at this task.
[00:00:28.480 --> 00:00:30.880]   It's very numerical rather than programming
[00:00:30.880 --> 00:00:32.120]   where you just write stuff.
[00:00:32.120 --> 00:00:35.160]   And so we really need math for this type of programming
[00:00:35.160 --> 00:00:37.840]   in order to understand that optimization process.
[00:00:37.840 --> 00:00:39.640]   So we already talked about the objects
[00:00:39.640 --> 00:00:41.920]   that were being optimized, the first way
[00:00:41.920 --> 00:00:44.760]   that optimization and machine learning intersect.
[00:00:44.760 --> 00:00:46.240]   When we talked about linear algebra,
[00:00:46.240 --> 00:00:48.960]   there we talked about arrays, which represent
[00:00:48.960 --> 00:00:50.320]   our data and our models.
[00:00:50.320 --> 00:00:53.320]   These are the things that go into our optimization process.
[00:00:53.320 --> 00:00:57.120]   Our parameters we'll see are often thought of as arrays.
[00:00:57.120 --> 00:00:59.080]   The things that determine how our model behaves
[00:00:59.080 --> 00:01:01.480]   the equivalent of our code effectively.
[00:01:01.480 --> 00:01:06.280]   And so all of that fell under the aegis of linear algebra.
[00:01:06.280 --> 00:01:10.080]   Now we're going to talk about how we optimize with calculus.
[00:01:10.080 --> 00:01:12.680]   Calculus helps us make tiny changes in order
[00:01:12.680 --> 00:01:16.640]   to iteratively, slowly improve the behavior of our computer
[00:01:16.640 --> 00:01:18.480]   programs with machine learning.
[00:01:18.480 --> 00:01:21.200]   We still won't cover what it is that we're optimizing.
[00:01:21.200 --> 00:01:23.000]   So we're really leaving this pretty--
[00:01:23.000 --> 00:01:25.720]   we're leaving the meat of it for the third session
[00:01:25.720 --> 00:01:27.880]   where we talk about probability and statistics.
[00:01:27.880 --> 00:01:30.640]   Because it turns out what we want to do is reduce surprise
[00:01:30.640 --> 00:01:33.280]   or reduce uncertainty with our models.
[00:01:33.280 --> 00:01:36.080]   But today, we will focus instead on how
[00:01:36.080 --> 00:01:38.160]   we optimize with calculus.
[00:01:38.160 --> 00:01:39.160]   So let's dive in.
[00:01:39.160 --> 00:01:41.640]   So the takeaways for today are that calculus
[00:01:41.640 --> 00:01:44.320]   is approximation with linear maps,
[00:01:44.320 --> 00:01:47.360]   and that calculus helps us incrementally optimize.
[00:01:47.360 --> 00:01:49.760]   The first point indicates a connection
[00:01:49.760 --> 00:01:52.000]   to what we talked about in linear algebra, which we're
[00:01:52.000 --> 00:01:53.640]   going to really chew on today.
[00:01:53.640 --> 00:01:56.720]   But then one point which we will hopefully but not definitely
[00:01:56.720 --> 00:01:59.800]   get to is that calculus has been automated.
[00:01:59.800 --> 00:02:01.520]   Calculus can be done automatically
[00:02:01.520 --> 00:02:02.720]   in computers nowadays.
[00:02:02.720 --> 00:02:05.520]   Vector calculus is at the center of machine learning.
[00:02:05.520 --> 00:02:08.080]   So I think most folks will probably
[00:02:08.080 --> 00:02:11.560]   have taken a class on calculus with a single variable,
[00:02:11.560 --> 00:02:14.440]   derivatives of functions with a single variable.
[00:02:14.440 --> 00:02:16.800]   Vector calculus does the same thing,
[00:02:16.800 --> 00:02:19.840]   but instead it's with vectors, with arrays,
[00:02:19.840 --> 00:02:23.240]   with matrices as the arguments to the functions.
[00:02:23.240 --> 00:02:26.000]   And so some examples of where vector calculus shows up,
[00:02:26.000 --> 00:02:28.360]   you may have, if you've done linear regression before
[00:02:28.360 --> 00:02:29.880]   and you've got an exact solution,
[00:02:29.880 --> 00:02:32.720]   those exact solutions are derived using calculus.
[00:02:32.720 --> 00:02:34.880]   So the actual calculation isn't done using calculus,
[00:02:34.880 --> 00:02:36.680]   but the derivation uses calculus.
[00:02:36.680 --> 00:02:39.680]   If you've ever calculated eigenvectors and eigenvalues,
[00:02:39.680 --> 00:02:43.360]   that is typically done with-- if you want to calculate them
[00:02:43.360 --> 00:02:46.200]   iteratively, it's done with a method that
[00:02:46.200 --> 00:02:48.120]   is justified using calculus.
[00:02:48.120 --> 00:02:50.880]   And so the explanation for why this works uses calculus.
[00:02:50.880 --> 00:02:55.040]   Then the algorithms of gradient descent and backpropagation,
[00:02:55.040 --> 00:02:56.880]   where the former is a generic term
[00:02:56.880 --> 00:02:59.360]   and the latter is a specific term for neural networks,
[00:02:59.360 --> 00:03:01.560]   these algorithms are used to optimize machine learning
[00:03:01.560 --> 00:03:04.200]   algorithms, and they are calculus-based.
[00:03:04.200 --> 00:03:06.120]   And so the usual thing you'll hear
[00:03:06.120 --> 00:03:08.800]   is that vector calculus combines linear algebra and calculus,
[00:03:08.800 --> 00:03:11.480]   where linear algebra is algebra for solving equations
[00:03:11.480 --> 00:03:13.560]   with vectors and matrices instead of just numbers,
[00:03:13.560 --> 00:03:15.320]   and calculus is a collection of methods
[00:03:15.320 --> 00:03:18.240]   for studying rates of change and areas.
[00:03:18.240 --> 00:03:20.560]   But we've already seen that the former is not necessarily
[00:03:20.560 --> 00:03:22.960]   the best way to think about linear algebra for machine
[00:03:22.960 --> 00:03:24.720]   learning, that linear algebra is instead
[00:03:24.720 --> 00:03:26.680]   the study of functions that can be represented
[00:03:26.680 --> 00:03:27.960]   by arrays, a.k.a.
[00:03:27.960 --> 00:03:30.280]   linear maps, as we talked about last time.
[00:03:30.280 --> 00:03:32.080]   This time, we'll talk about a different way
[00:03:32.080 --> 00:03:33.080]   to think about calculus.
[00:03:33.080 --> 00:03:35.160]   Calculus, at least derivative calculus,
[00:03:35.160 --> 00:03:37.440]   which is our primary concern, is the study
[00:03:37.440 --> 00:03:41.120]   of methods for approximating functions with linear maps.
[00:03:41.120 --> 00:03:43.240]   So rather than the function being
[00:03:43.240 --> 00:03:45.960]   exactly the same as something linear,
[00:03:45.960 --> 00:03:48.760]   we're going to approximate the function with something linear.
[00:03:48.760 --> 00:03:51.160]   And we're going to take a slightly different tack
[00:03:51.160 --> 00:03:54.160]   than the usual way of covering it, of defining it.
[00:03:54.160 --> 00:03:56.640]   We're going to use a slightly different definition
[00:03:56.640 --> 00:03:57.840]   of the derivative, at least.
[00:03:57.840 --> 00:04:00.680]   The end result is the same, but the definition is different.
[00:04:00.680 --> 00:04:02.960]   And in mathematics, the way we write our definitions
[00:04:02.960 --> 00:04:05.600]   is really important, even if the final object is equivalent.
[00:04:05.600 --> 00:04:07.240]   And so basically, what we're going to do
[00:04:07.240 --> 00:04:09.920]   is we're going to define the derivative using linear maps,
[00:04:09.920 --> 00:04:12.760]   and that's going to make vector calculus a lot easier when
[00:04:12.760 --> 00:04:13.560]   it comes time.
[00:04:13.560 --> 00:04:15.360]   And the three benefits are that we're
[00:04:15.360 --> 00:04:17.440]   going to end up with a single style for thinking
[00:04:17.440 --> 00:04:19.120]   about gradients, a.k.a.
[00:04:19.120 --> 00:04:23.360]   derivatives of single variable vector and matrix functions.
[00:04:23.360 --> 00:04:26.080]   We're going to drop indices from all of our calculations,
[00:04:26.080 --> 00:04:29.280]   so it's not going to be this big thicket of i's and j's and k's,
[00:04:29.280 --> 00:04:32.000]   as sometimes happens when you do vector calculus.
[00:04:32.000 --> 00:04:33.760]   And we'll also get a chance to use
[00:04:33.760 --> 00:04:36.560]   something called the little o notation instead of limits,
[00:04:36.560 --> 00:04:39.000]   which has connections to the big O notation from computer
[00:04:39.000 --> 00:04:41.440]   science, and I think is just a little bit more intuitive
[00:04:41.440 --> 00:04:42.520]   and easier to work with.
[00:04:42.520 --> 00:04:44.360]   So let's talk about how this works
[00:04:44.360 --> 00:04:45.680]   for single variable functions.
[00:04:45.680 --> 00:04:47.640]   These are the functions you would come across
[00:04:47.640 --> 00:04:50.040]   in a first calculus sequence.
[00:04:50.040 --> 00:04:54.280]   What's called the calculus AB and BC in the American schooling
[00:04:54.280 --> 00:04:54.800]   system.
[00:04:54.800 --> 00:04:57.840]   The standard definition of the derivative emphasizes limits.
[00:04:57.840 --> 00:05:01.520]   It says that the derivative is the limit of a ratio,
[00:05:01.520 --> 00:05:05.440]   and that ratio is between change in output and change in input.
[00:05:05.440 --> 00:05:06.400]   We take a function.
[00:05:06.400 --> 00:05:08.600]   We're trying to find the derivative of that function.
[00:05:08.600 --> 00:05:11.000]   And what we do is we look at the difference
[00:05:11.000 --> 00:05:13.040]   between how it behaves at one point,
[00:05:13.040 --> 00:05:14.980]   how it behaves at another point, and we
[00:05:14.980 --> 00:05:17.720]   divide by how far away those points are.
[00:05:17.720 --> 00:05:20.360]   And then we make that change in input smaller.
[00:05:20.360 --> 00:05:23.160]   We move the two points we're querying closer together.
[00:05:23.160 --> 00:05:25.560]   We imagine making that infinitely small.
[00:05:25.560 --> 00:05:27.520]   The mathematics of limits gives us an answer
[00:05:27.520 --> 00:05:29.840]   to what this ratio limits to, and that
[00:05:29.840 --> 00:05:31.480]   is what we call the derivative.
[00:05:31.480 --> 00:05:34.520]   And there are some useful things about thinking that way,
[00:05:34.520 --> 00:05:37.120]   but I want to emphasize a slightly different way
[00:05:37.120 --> 00:05:39.100]   of thinking about it, which is we're
[00:05:39.100 --> 00:05:42.480]   going to instead think in terms of approximation.
[00:05:42.480 --> 00:05:44.500]   So the derivative is going to show up
[00:05:44.500 --> 00:05:47.980]   as a function that helps us approximate other functions.
[00:05:47.980 --> 00:05:49.660]   We're still thinking about, OK, what
[00:05:49.660 --> 00:05:52.740]   if I imagine input at one point versus input at another point?
[00:05:52.740 --> 00:05:55.460]   But instead, we're saying, if the value at another point
[00:05:55.460 --> 00:05:58.420]   epsilon away from my query point x
[00:05:58.420 --> 00:06:00.820]   is equal to the original point--
[00:06:00.820 --> 00:06:02.980]   so this is setting our sort of baseline--
[00:06:02.980 --> 00:06:06.700]   plus some function of x, which can be whatever we want,
[00:06:06.700 --> 00:06:09.860]   times epsilon-- so now we're multiplying these two things
[00:06:09.860 --> 00:06:12.440]   together-- and then plus something smaller.
[00:06:12.440 --> 00:06:14.840]   So for now, just think of little o of epsilon
[00:06:14.840 --> 00:06:16.840]   as meaning this thing is really small.
[00:06:16.840 --> 00:06:19.240]   At least it is smaller than epsilon.
[00:06:19.240 --> 00:06:22.520]   Then we call that function f prime, the derivative.
[00:06:22.520 --> 00:06:25.320]   So one way to think about this is
[00:06:25.320 --> 00:06:28.640]   that if we can always approximate the behavior of f
[00:06:28.640 --> 00:06:32.480]   near x-- so f at x plus epsilon for various different points
[00:06:32.480 --> 00:06:35.560]   of epsilon-- we can approximate that with a line passing
[00:06:35.560 --> 00:06:38.640]   through f of x with slope f prime of x.
[00:06:38.640 --> 00:06:40.520]   Then we call the function that gives us the slope,
[00:06:40.520 --> 00:06:41.140]   the derivative.
[00:06:41.140 --> 00:06:46.060]   Now this right here is the definition of a line.
[00:06:46.060 --> 00:06:47.860]   We have an intercept here.
[00:06:47.860 --> 00:06:49.280]   We have a slope here.
[00:06:49.280 --> 00:06:50.960]   And then we have a variable here.
[00:06:50.960 --> 00:06:53.200]   Epsilon is a variable now.
[00:06:53.200 --> 00:06:55.760]   This is often b for bias m.
[00:06:55.760 --> 00:06:57.640]   I don't know why that's the slope thing in x.
[00:06:57.640 --> 00:07:02.680]   So y equals mx plus b is the way that equation is often written.
[00:07:02.680 --> 00:07:04.360]   This is the equation for a line.
[00:07:04.360 --> 00:07:07.600]   And it doesn't give us the exact value of f of x plus epsilon.
[00:07:07.600 --> 00:07:08.640]   That would be too good.
[00:07:08.640 --> 00:07:09.800]   That would be too easy.
[00:07:09.800 --> 00:07:12.480]   In general, it gives us something that's off by a little bit.
[00:07:12.480 --> 00:07:16.240]   And the amount it's off by is given by this little o of epsilon term,
[00:07:16.240 --> 00:07:18.720]   which could be lots of different functions of epsilon.
[00:07:18.720 --> 00:07:20.280]   It could be epsilon squared.
[00:07:20.280 --> 00:07:22.400]   It could be e to the minus epsilon.
[00:07:22.400 --> 00:07:25.200]   It could be anything that's really small when epsilon is really,
[00:07:25.200 --> 00:07:25.920]   really small.
[00:07:25.920 --> 00:07:30.800]   So the function we are using to approximate f near a point
[00:07:30.800 --> 00:07:32.080]   is linear, right?
[00:07:32.080 --> 00:07:34.160]   f prime of x times epsilon.
[00:07:34.160 --> 00:07:36.280]   Even though f prime of x can be anything,
[00:07:36.280 --> 00:07:38.040]   the way it interacts with epsilon, the way
[00:07:38.040 --> 00:07:41.560]   it interacts with our variable, is multiplicative.
[00:07:41.560 --> 00:07:45.240]   And so this function we're using to approximate f near the point x
[00:07:45.240 --> 00:07:48.720]   is linear and therefore can always be represented by an array.
[00:07:48.720 --> 00:07:50.480]   Right now, it's an array with one element.
[00:07:50.480 --> 00:07:52.360]   So this isn't that interesting, right?
[00:07:52.360 --> 00:07:55.480]   And the way we use the array is we're basically doing a dot product.
[00:07:55.480 --> 00:07:58.360]   We're taking one number here, one number here,
[00:07:58.360 --> 00:08:01.800]   multiplying them together, and adding up the results.
[00:08:01.800 --> 00:08:03.040]   We get one number.
[00:08:03.040 --> 00:08:07.200]   This maybe doesn't seem that useful of a use of our linear algebra analogy
[00:08:07.200 --> 00:08:10.240]   yet, but watch this space for the next example.
[00:08:10.240 --> 00:08:13.120]   I think it's maybe useful to have a picture.
[00:08:13.120 --> 00:08:15.080]   I have this function, this curve here.
[00:08:15.080 --> 00:08:16.920]   I actually just drew this in Google Slides.
[00:08:16.920 --> 00:08:20.880]   So I don't know how to calculate values of this function at other points.
[00:08:20.880 --> 00:08:23.360]   But I know that this is its value at one point.
[00:08:23.360 --> 00:08:25.160]   And the question is, OK, how do I figure out
[00:08:25.160 --> 00:08:26.880]   what its value is at another point?
[00:08:26.880 --> 00:08:28.960]   So I can't query this function at all.
[00:08:28.960 --> 00:08:31.400]   So one way to figure out what its value at another point
[00:08:31.400 --> 00:08:32.520]   is to approximate it.
[00:08:32.520 --> 00:08:33.720]   And how are we going to approximate it?
[00:08:33.720 --> 00:08:36.560]   There are lots of different ways that we could approximate a function,
[00:08:36.560 --> 00:08:37.200]   just tons.
[00:08:37.200 --> 00:08:39.000]   But the way we're going to approximate it
[00:08:39.000 --> 00:08:42.080]   is by using local, like around a particular point,
[00:08:42.080 --> 00:08:44.720]   linear straight line approximations.
[00:08:44.720 --> 00:08:46.120]   So it won't give us exact answers.
[00:08:46.120 --> 00:08:47.320]   It'll give us approximations.
[00:08:47.320 --> 00:08:48.480]   But they'll be pretty good.
[00:08:48.480 --> 00:08:51.400]   If you look at this gold line here that represents our approximation,
[00:08:51.400 --> 00:08:55.480]   it actually stays pretty close to the gray line across this region here.
[00:08:55.480 --> 00:08:58.200]   The approximation is pretty valid across the region
[00:08:58.200 --> 00:09:00.120]   where I'm bringing my pointer there.
[00:09:00.120 --> 00:09:01.700]   We have our point in gray, f of x.
[00:09:01.700 --> 00:09:02.680]   That's where we start.
[00:09:02.680 --> 00:09:05.280]   It's also the 0 point of our approximation.
[00:09:05.280 --> 00:09:07.840]   When epsilon is 0, that's what our guess is.
[00:09:07.840 --> 00:09:09.080]   f of x equals f of x.
[00:09:09.080 --> 00:09:09.600]   Great.
[00:09:09.600 --> 00:09:10.640]   That's a good sign.
[00:09:10.640 --> 00:09:15.080]   And then as we move away, we use the slope of this line times epsilon
[00:09:15.080 --> 00:09:16.040]   to get our new point.
[00:09:16.040 --> 00:09:18.680]   So then our approximation is the point I'm
[00:09:18.680 --> 00:09:22.920]   indicating with my bright red indicator there, my little laser pointer.
[00:09:22.920 --> 00:09:24.760]   And that value is not exactly right.
[00:09:24.760 --> 00:09:27.320]   The actual answer is this bigger red circle here.
[00:09:27.320 --> 00:09:29.680]   But the difference between them is actually quite small.
[00:09:29.680 --> 00:09:32.340]   And that's that purple, tiny little purple block
[00:09:32.340 --> 00:09:35.180]   in between the yellow and the red point.
[00:09:35.180 --> 00:09:36.540]   That's our O of epsilon term.
[00:09:36.540 --> 00:09:41.700]   So if the value at a point epsilon away is this point's value
[00:09:41.700 --> 00:09:47.180]   plus this linear function of epsilon plus something really small
[00:09:47.180 --> 00:09:49.700]   that we can maybe ignore if we're squinting,
[00:09:49.700 --> 00:09:53.380]   we call that function, that gold function there, the derivative.
[00:09:53.380 --> 00:09:55.180]   So here's how this definition gets used.
[00:09:55.180 --> 00:09:59.100]   I want to give you a sense for how this would get used to define
[00:09:59.100 --> 00:10:02.580]   or compute derivatives as opposed to the way it's normally done.
[00:10:02.580 --> 00:10:04.680]   So consider the function x squared.
[00:10:04.680 --> 00:10:06.760]   This function-- we'll see its derivative, actually.
[00:10:06.760 --> 00:10:09.500]   I won't spoil it, just in case you don't recall or haven't
[00:10:09.500 --> 00:10:11.060]   learned the derivative of x squared.
[00:10:11.060 --> 00:10:13.220]   So x squared is equal to x times x.
[00:10:13.220 --> 00:10:16.820]   Let's calculate a derivative for the function x squared just
[00:10:16.820 --> 00:10:18.140]   using our definition.
[00:10:18.140 --> 00:10:21.140]   What we do is we write out the left-hand side of our definition,
[00:10:21.140 --> 00:10:22.900]   f of x plus epsilon.
[00:10:22.900 --> 00:10:26.300]   So this is saying, OK, what is the value of our function at this other point?
[00:10:26.300 --> 00:10:29.260]   And then we just move around our symbols.
[00:10:29.260 --> 00:10:31.180]   We play some algebra games until we can
[00:10:31.180 --> 00:10:33.940]   get it to look like the right-hand side of our definition.
[00:10:33.940 --> 00:10:36.860]   And then all of those pieces, we just match them.
[00:10:36.860 --> 00:10:38.660]   So let me show you how that works.
[00:10:38.660 --> 00:10:42.080]   x plus epsilon times x plus epsilon, that's what f of x is, right?
[00:10:42.080 --> 00:10:43.220]   It's x squared.
[00:10:43.220 --> 00:10:44.420]   So the input gets squared.
[00:10:44.420 --> 00:10:46.220]   Our input is now x plus epsilon.
[00:10:46.220 --> 00:10:50.100]   So then we just expand that out, x squared plus x times epsilon
[00:10:50.100 --> 00:10:52.420]   plus epsilon times x plus epsilon squared.
[00:10:52.420 --> 00:10:53.940]   We do a little bit of rearrangement.
[00:10:53.940 --> 00:10:56.980]   This is just two copies of x times epsilon.
[00:10:56.980 --> 00:10:58.380]   Let's take a look at what we got.
[00:10:58.380 --> 00:10:59.340]   We got x squared.
[00:10:59.340 --> 00:11:00.780]   That's our f of x, right?
[00:11:00.780 --> 00:11:02.500]   This is what I mean by pattern matching.
[00:11:02.500 --> 00:11:06.740]   We match each component of this here to each component of this here.
[00:11:06.740 --> 00:11:08.940]   Then we've also got this nice thing here,
[00:11:08.940 --> 00:11:13.140]   where we've got a function of x on the left-hand side.
[00:11:13.140 --> 00:11:15.180]   So 2 times x, that's a function of x.
[00:11:15.180 --> 00:11:16.220]   I give you an x.
[00:11:16.220 --> 00:11:18.500]   You return to me that value double.
[00:11:18.500 --> 00:11:20.700]   It's a simple function, but it's a function, all right?
[00:11:20.700 --> 00:11:25.020]   So that's f prime of x, so long as the rest of this expression
[00:11:25.020 --> 00:11:27.340]   here matches our definition's expression.
[00:11:27.340 --> 00:11:32.540]   So epsilon here, it's interacting with x only by multiplication, right?
[00:11:32.540 --> 00:11:37.420]   It's interacting with our f prime of x term only through multiplication.
[00:11:37.420 --> 00:11:41.540]   So that counts as our sort of linear term in epsilon.
[00:11:41.540 --> 00:11:43.940]   And then we've got this term here, epsilon squared.
[00:11:43.940 --> 00:11:46.820]   Now, you might think of squaring something as making it bigger,
[00:11:46.820 --> 00:11:49.660]   because we normally think of numbers bigger than 1, right?
[00:11:49.660 --> 00:11:51.460]   2 squared is bigger than 2.
[00:11:51.460 --> 00:11:52.140]   It's 4.
[00:11:52.140 --> 00:11:54.140]   100 squared is 10,000.
[00:11:54.140 --> 00:11:59.020]   But for 1 over 100, 1 over 100 squared is 1 over 10,000.
[00:11:59.020 --> 00:12:02.740]   So squaring actually, just like it makes things really much bigger
[00:12:02.740 --> 00:12:06.260]   as numbers get bigger, it actually makes things much, much smaller
[00:12:06.260 --> 00:12:08.220]   as numbers get closer to 0.
[00:12:08.220 --> 00:12:10.620]   So epsilon squared actually counts as something
[00:12:10.620 --> 00:12:12.860]   that is smaller than epsilon.
[00:12:12.860 --> 00:12:14.620]   That's this little o of epsilon term.
[00:12:14.620 --> 00:12:15.780]   So that's our final piece.
[00:12:15.780 --> 00:12:16.660]   And now we've done it.
[00:12:16.660 --> 00:12:19.980]   We've pattern matched this expression here to our definition.
[00:12:19.980 --> 00:12:23.180]   And so the derivative of x squared is 2x.
[00:12:23.180 --> 00:12:25.580]   And the nice thing here is we do have to be
[00:12:25.580 --> 00:12:28.060]   able to reason about this little o term here.
[00:12:28.060 --> 00:12:31.260]   But I find that substantially easier than doing
[00:12:31.260 --> 00:12:32.860]   this whole thing underneath a limit.
[00:12:32.860 --> 00:12:35.620]   Limits, it's always a little bit unclear to me what's allowed
[00:12:35.620 --> 00:12:37.220]   and what's not allowed under a limit.
[00:12:37.220 --> 00:12:38.940]   And in fact, physicists and mathematicians
[00:12:38.940 --> 00:12:42.140]   often fight about which limits are appropriate to take.
[00:12:42.140 --> 00:12:44.740]   It's useful technology to have limits around.
[00:12:44.740 --> 00:12:48.140]   But I like to focus on specific examples
[00:12:48.140 --> 00:12:51.220]   where I can be very clear that what I'm doing is correct.
[00:12:51.220 --> 00:12:53.060]   So let's talk about what little o is.
[00:12:53.060 --> 00:12:56.340]   It is indeed a type of limit and a limit of ratios.
[00:12:56.340 --> 00:13:00.500]   So the idea that limits of ratios are important for calculus is not gone.
[00:13:00.500 --> 00:13:03.980]   It's just been given a really nice API is how I think about it.
[00:13:03.980 --> 00:13:07.900]   Little o is a nice abstraction, a nice thing built out
[00:13:07.900 --> 00:13:10.700]   of limits that's easier to work with than the thing itself,
[00:13:10.700 --> 00:13:13.100]   like Python is built out of C.
[00:13:13.100 --> 00:13:17.260]   So a term is little o of epsilon if it gets smaller much faster
[00:13:17.260 --> 00:13:18.500]   than epsilon does.
[00:13:18.500 --> 00:13:20.660]   That's the intuitive way of framing it.
[00:13:20.660 --> 00:13:23.860]   So little o of epsilon means as you get close to 0,
[00:13:23.860 --> 00:13:28.260]   something is little o of epsilon if it gets closer to 0 much faster
[00:13:28.260 --> 00:13:32.020]   than epsilon does, which has a specific mathematical definition, which
[00:13:32.020 --> 00:13:36.580]   is that if the limit as x goes to 0 of f of x over g of x
[00:13:36.580 --> 00:13:40.260]   is 0, which means g of x, the bottom of the ratio,
[00:13:40.260 --> 00:13:42.620]   is bigger than the top of the ratio, then
[00:13:42.620 --> 00:13:46.140]   we can replace f of x with little o of g of x.
[00:13:46.140 --> 00:13:48.740]   So often, g will just be the identity function.
[00:13:48.740 --> 00:13:51.860]   Like previously, we had little o of epsilon
[00:13:51.860 --> 00:13:54.100]   as the inner argument there.
[00:13:54.100 --> 00:13:57.100]   So that says that this function of the variable
[00:13:57.100 --> 00:13:59.700]   is smaller than the variable itself.
[00:13:59.700 --> 00:14:03.580]   If we have f of epsilon over epsilon, it has its limit being 0.
[00:14:03.580 --> 00:14:06.900]   And this means that for a small enough epsilon, anything little o of epsilon
[00:14:06.900 --> 00:14:08.780]   can safely be ignored.
[00:14:08.780 --> 00:14:11.780]   So here are a couple of examples.
[00:14:11.780 --> 00:14:13.820]   So some terms that are little o of epsilon.
[00:14:13.820 --> 00:14:15.820]   So 0 is little o of epsilon.
[00:14:15.820 --> 00:14:21.980]   If I look, the limit as x goes to 0 of 0 over x, that's just going to be 0.
[00:14:21.980 --> 00:14:27.700]   If I look at x squared, so the limit of x squared over x,
[00:14:27.700 --> 00:14:30.060]   that's going to be x squared divided by x.
[00:14:30.060 --> 00:14:31.060]   That's just x.
[00:14:31.060 --> 00:14:33.420]   Limit as x goes to 0 is 0.
[00:14:33.420 --> 00:14:35.860]   kx squared, I can multiply x squared by something.
[00:14:35.860 --> 00:14:37.980]   And that doesn't change the argument I just gave.
[00:14:37.980 --> 00:14:43.100]   The limit of k times x as x goes to 0 remains 0.
[00:14:43.100 --> 00:14:44.780]   So all those terms are little o of x.
[00:14:44.780 --> 00:14:46.900]   Some things on the right that are not little o of x.
[00:14:46.900 --> 00:14:48.340]   So 1 over x.
[00:14:48.340 --> 00:14:51.820]   So a constant value is not little o of x.
[00:14:51.820 --> 00:14:54.260]   Our approximation will always be off by 1.
[00:14:54.260 --> 00:14:55.740]   So intuitively, that makes sense.
[00:14:55.740 --> 00:15:00.100]   But by definition, what that means is the limit as x goes to 0 of 1 over x,
[00:15:00.100 --> 00:15:02.820]   that is actually infinite rather than 0.
[00:15:02.820 --> 00:15:04.740]   x is not little o of x.
[00:15:04.740 --> 00:15:06.220]   x over x, that's just 1.
[00:15:06.220 --> 00:15:09.140]   The limit of x goes to 0 of 1 is 1.
[00:15:09.140 --> 00:15:11.940]   And multiplying that by something doesn't change it.
[00:15:11.940 --> 00:15:13.060]   There's some useful rules.
[00:15:13.060 --> 00:15:14.220]   You can add them together.
[00:15:14.220 --> 00:15:18.180]   If I have two little o terms, the sum of them is still little o.
[00:15:18.180 --> 00:15:21.460]   But I won't go fully into the details because it's actually not that important
[00:15:21.460 --> 00:15:24.380]   that you become really good at calculating derivatives.
[00:15:24.380 --> 00:15:26.860]   It's more important that you have a sense intuitively
[00:15:26.860 --> 00:15:30.500]   for what these pieces mean and what the derivative is,
[00:15:30.500 --> 00:15:33.620]   which is something that is off by a little o term.
[00:15:33.620 --> 00:15:35.780]   And so the reason why I like this is because it's really
[00:15:35.780 --> 00:15:38.220]   similar to something that I end up using and thinking
[00:15:38.220 --> 00:15:41.500]   about when I'm writing computer code, which is big O notation from computer
[00:15:41.500 --> 00:15:45.220]   science lets us talk about how fast algorithms are while abstracting away
[00:15:45.220 --> 00:15:49.060]   a bunch of irrelevant details about the exact implementation of it
[00:15:49.060 --> 00:15:53.260]   and things that might be different across CPUs or whatever.
[00:15:53.260 --> 00:15:55.380]   So the idea is something that's big O of n
[00:15:55.380 --> 00:15:57.740]   is faster than something that's big O of n squared, which
[00:15:57.740 --> 00:16:01.660]   is much faster than something that's big O of 2dn for a big enough input.
[00:16:01.660 --> 00:16:06.020]   Similarly, little o lets us talk about the best linear approximation
[00:16:06.020 --> 00:16:09.540]   while abstracting away exactly how good or bad that approximation is.
[00:16:09.540 --> 00:16:12.180]   When you're doing the limit, like the classical limit
[00:16:12.180 --> 00:16:15.380]   approach to studying calculus, these two things get mixed up together.
[00:16:15.380 --> 00:16:20.660]   And you always need to both measure how good the linear approximation is exactly
[00:16:20.660 --> 00:16:23.020]   and produce the correct value for it.
[00:16:23.020 --> 00:16:26.420]   But with little o notation, with the Frechet derivative style--
[00:16:26.420 --> 00:16:28.780]   that's the name for this definition of the derivative--
[00:16:28.780 --> 00:16:31.020]   then you can split those two things apart.
[00:16:31.020 --> 00:16:33.820]   Little o and big O, they're called Landau symbols.
[00:16:33.820 --> 00:16:36.260]   And so if you want to look up more about these
[00:16:36.260 --> 00:16:39.620]   and see a little bit more about how they're used and how they relate,
[00:16:39.620 --> 00:16:43.340]   Landau symbol is the relevant vocab term to Google.
[00:16:43.340 --> 00:16:45.820]   There was a lot of setup covering something
[00:16:45.820 --> 00:16:47.580]   that maybe people know decently--
[00:16:47.580 --> 00:16:50.700]   some people maybe know decently well, single variable calculus.
[00:16:50.700 --> 00:16:53.860]   Now let's see it actually do something really useful for us
[00:16:53.860 --> 00:16:56.500]   and make defining the gradient or the derivative
[00:16:56.500 --> 00:16:59.060]   for a function of a vector much easier.
[00:16:59.060 --> 00:17:03.060]   We're talking about here scalar-valued functions of vectors, functions
[00:17:03.060 --> 00:17:07.060]   that take in a vector, something in Rn or an array with n entries,
[00:17:07.060 --> 00:17:11.020]   and return a single real value in computing a single floating point
[00:17:11.020 --> 00:17:11.740]   value.
[00:17:11.740 --> 00:17:17.860]   So the Frechet definition, again, centers approximation and linearity.
[00:17:17.860 --> 00:17:19.220]   And it looks very, very similar.
[00:17:19.220 --> 00:17:22.660]   On the left-hand side, we have the value of the function at a different point.
[00:17:22.660 --> 00:17:27.260]   And then we have our four pieces, the starting point, x,
[00:17:27.260 --> 00:17:28.620]   the value of the function there.
[00:17:28.620 --> 00:17:30.020]   We have a little o term.
[00:17:30.020 --> 00:17:32.820]   And it says that our approximation error is
[00:17:32.820 --> 00:17:36.900]   going to be small relative to the norm of our epsilon,
[00:17:36.900 --> 00:17:39.380]   the norm of how we're changing our input.
[00:17:39.380 --> 00:17:42.080]   But now instead of just a simple multiplication,
[00:17:42.080 --> 00:17:44.820]   we have here an inner product or a dot product.
[00:17:44.820 --> 00:17:46.380]   This is a linear map right here.
[00:17:46.380 --> 00:17:47.740]   We take in a vector.
[00:17:47.740 --> 00:17:50.180]   We do its inner product with epsilon.
[00:17:50.180 --> 00:17:54.140]   And that gives us this central term here, this linear term.
[00:17:54.140 --> 00:17:56.260]   And this is what we're going to use to approximate
[00:17:56.260 --> 00:17:57.940]   the behavior of the function f.
[00:17:57.940 --> 00:18:02.060]   You can think of this as in 2D, it sets up a plane
[00:18:02.060 --> 00:18:05.260]   that you approximate the function f with.
[00:18:05.260 --> 00:18:09.100]   Where normally in 2D, this would be some surface.
[00:18:09.100 --> 00:18:11.980]   If we have two-dimensional inputs and one-dimensional outputs,
[00:18:11.980 --> 00:18:14.500]   we represent the inputs in two dimensions,
[00:18:14.500 --> 00:18:17.420]   the outputs in the third dimension sticking out.
[00:18:17.420 --> 00:18:19.180]   And so you would get some surface.
[00:18:19.180 --> 00:18:22.860]   And we approximate that with just a plane, just like a flat sheet of paper.
[00:18:22.860 --> 00:18:25.220]   That's what this middle part here defines.
[00:18:25.220 --> 00:18:27.060]   This is basically the same thing as defining
[00:18:27.060 --> 00:18:29.100]   that line in the one-dimensional case.
[00:18:29.100 --> 00:18:32.620]   We just now have a vector with multiple entries to define it.
[00:18:32.620 --> 00:18:34.700]   So a couple of things to notice here.
[00:18:34.700 --> 00:18:37.140]   One is that the gradient is a function.
[00:18:37.140 --> 00:18:38.900]   So there's a little bit of confusion here.
[00:18:38.900 --> 00:18:42.620]   People refer to both the gradient itself and the output of it
[00:18:42.620 --> 00:18:43.940]   with the term gradient.
[00:18:43.940 --> 00:18:47.380]   Properly, just like a derivative, the gradient is a function.
[00:18:47.380 --> 00:18:49.860]   It takes in arrays and returns arrays.
[00:18:49.860 --> 00:18:52.700]   This guy here takes in a vector.
[00:18:52.700 --> 00:18:54.700]   f is a function of vectors.
[00:18:54.700 --> 00:18:56.780]   And then what it returns is something that we
[00:18:56.780 --> 00:19:01.300]   can use to take an inner product with epsilon, which is also a vector,
[00:19:01.300 --> 00:19:04.700]   because epsilon is something that we add to our inputs.
[00:19:04.700 --> 00:19:07.860]   It's got to be a function that takes in vectors and returns vectors.
[00:19:07.860 --> 00:19:11.820]   And then we use those vectors to create linear functions that
[00:19:11.820 --> 00:19:13.940]   approximate the original function.
[00:19:13.940 --> 00:19:16.060]   There's lots of ways to write that inner product.
[00:19:16.060 --> 00:19:18.700]   But the most important alternative way to write it
[00:19:18.700 --> 00:19:20.260]   is as a matrix multiplication.
[00:19:20.260 --> 00:19:22.580]   And this makes it a little bit more clear that, one,
[00:19:22.580 --> 00:19:26.060]   it reduces to the derivative for vectors of length 1.
[00:19:26.060 --> 00:19:28.940]   If I transpose a single number, nothing happens.
[00:19:28.940 --> 00:19:31.260]   So I just get the number times this number.
[00:19:31.260 --> 00:19:33.060]   And that's what we did with the derivative.
[00:19:33.060 --> 00:19:35.260]   And then it's more clear that this right here
[00:19:35.260 --> 00:19:38.540]   is a linear function of epsilon for fixed x.
[00:19:38.540 --> 00:19:42.540]   So if I vary x, all kinds of weird stuff can happen to this value.
[00:19:42.540 --> 00:19:46.060]   But the main thing we're thinking about is, at a fixed value of x,
[00:19:46.060 --> 00:19:47.940]   what happens when we vary epsilon?
[00:19:47.940 --> 00:19:50.540]   And in that case, this is just a linear function here.
[00:19:50.540 --> 00:19:51.700]   It's matrix multiplication.
[00:19:51.700 --> 00:19:55.500]   And matrix multiplication is how we apply linear functions.
[00:19:55.500 --> 00:19:58.980]   That sort of centers this idea that what we're doing here
[00:19:58.980 --> 00:20:03.380]   is creating a linear approximation to the original function,
[00:20:03.380 --> 00:20:06.780]   all the same principles that we had in the single variable version.
[00:20:06.780 --> 00:20:11.580]   So let's use this definition in an actual example.
[00:20:11.580 --> 00:20:16.460]   So we're going to calculate a gradient for the function x transpose x.
[00:20:16.460 --> 00:20:20.820]   It's also written as the squared norm of the vector x, which
[00:20:20.820 --> 00:20:24.740]   is to say it's basically the sum of the squared entries of the vector.
[00:20:24.740 --> 00:20:28.100]   That double bar sign is the same thing as measuring
[00:20:28.100 --> 00:20:31.460]   the length of the vector, the norm, the square root
[00:20:31.460 --> 00:20:32.980]   of the sum of squared entries.
[00:20:32.980 --> 00:20:34.100]   But we're squaring it.
[00:20:34.100 --> 00:20:36.180]   So we get something that is x transpose x.
[00:20:36.180 --> 00:20:37.260]   What is x transpose x?
[00:20:37.260 --> 00:20:41.740]   That means I take each entry and multiply the first one times itself,
[00:20:41.740 --> 00:20:44.300]   the second one times itself, and so on.
[00:20:44.300 --> 00:20:45.340]   So let's write that out.
[00:20:45.340 --> 00:20:49.780]   If we were to apply this function here, transpose and then multiply
[00:20:49.780 --> 00:20:52.580]   the things together, then it would look something like this.
[00:20:52.580 --> 00:20:56.740]   f of x plus epsilon is equal to x plus epsilon transpose x plus epsilon.
[00:20:56.740 --> 00:21:00.140]   And that gives us, again, four terms, just like in x squared.
[00:21:00.140 --> 00:21:04.740]   x transpose x plus x transpose epsilon plus epsilon transpose x
[00:21:04.740 --> 00:21:07.220]   plus epsilon transpose epsilon.
[00:21:07.220 --> 00:21:11.500]   And then we need to do a little bit of algebraic manipulation.
[00:21:11.500 --> 00:21:13.380]   So to start off, we have an easy one.
[00:21:13.380 --> 00:21:16.220]   x transpose x, that's just our original function f.
[00:21:16.220 --> 00:21:19.580]   Then 2x transpose epsilon, where did this guy come from?
[00:21:19.580 --> 00:21:23.180]   Well, if you look at the definition of this inner product here,
[00:21:23.180 --> 00:21:25.820]   all we're doing is multiplying the two entries together
[00:21:25.820 --> 00:21:27.100]   and summing the whole thing up.
[00:21:27.100 --> 00:21:29.340]   It doesn't matter what order we do that in.
[00:21:29.340 --> 00:21:31.180]   It doesn't matter if the left-hand side is x
[00:21:31.180 --> 00:21:33.740]   and the right-hand side is epsilon, or the left-hand side is epsilon
[00:21:33.740 --> 00:21:35.180]   and the right-hand side is x.
[00:21:35.180 --> 00:21:38.380]   So these two things are the same, just like x times epsilon and epsilon
[00:21:38.380 --> 00:21:39.940]   times x are the same.
[00:21:39.940 --> 00:21:45.260]   And so we get a single term here in the middle that's 2x transpose epsilon.
[00:21:45.260 --> 00:21:47.940]   And now we're really in business here.
[00:21:47.940 --> 00:21:51.340]   We have a function of x here that's 2 times x.
[00:21:51.340 --> 00:21:55.380]   We take that, that's a vector, and we transpose it and then combine that
[00:21:55.380 --> 00:21:56.180]   with epsilon.
[00:21:56.180 --> 00:21:59.540]   That's exactly what the middle linear term
[00:21:59.540 --> 00:22:01.500]   of our definition of the Frechet derivative,
[00:22:01.500 --> 00:22:05.020]   definition of the gradient, that's exactly what that looks like.
[00:22:05.020 --> 00:22:07.380]   It's a transpose of a function of x.
[00:22:07.380 --> 00:22:09.860]   And that gets combined with epsilon.
[00:22:09.860 --> 00:22:13.660]   And there's no epsilons anywhere, naked single epsilons anywhere else.
[00:22:13.660 --> 00:22:15.100]   There's no epsilon here.
[00:22:15.100 --> 00:22:16.420]   So this is our middle term.
[00:22:16.420 --> 00:22:21.740]   This last term here, this epsilon squared term, just like x squared
[00:22:21.740 --> 00:22:25.740]   is little o of x, the squared norm of epsilon
[00:22:25.740 --> 00:22:28.460]   is little o of the norm of epsilon.
[00:22:28.460 --> 00:22:31.060]   So that might be useful to go back to the original definitions
[00:22:31.060 --> 00:22:33.540]   and check that this actually works out.
[00:22:33.540 --> 00:22:36.260]   But it does indeed work out.
[00:22:36.260 --> 00:22:39.220]   It's basically-- it's actually pretty much basically the same thing.
[00:22:39.220 --> 00:22:42.500]   This can also be applied to functions of matrices,
[00:22:42.500 --> 00:22:46.260]   functions that take a matrix input and produce a scalar output.
[00:22:46.260 --> 00:22:49.860]   So an example of that would be maybe for multiple linear regression,
[00:22:49.860 --> 00:22:52.620]   where you have multiple inputs, multiple outputs.
[00:22:52.620 --> 00:22:54.660]   If you wanted to calculate how well that's doing,
[00:22:54.660 --> 00:22:57.940]   you would need a scalar-valued function of matrices.
[00:22:57.940 --> 00:23:00.340]   And we won't go into great detail about this one.
[00:23:00.340 --> 00:23:03.700]   The slides-- I'm going to skip over a bunch of slides that
[00:23:03.700 --> 00:23:06.660]   are available if you check the online version of the slides
[00:23:06.660 --> 00:23:08.860]   if you want to see in detail how this works.
[00:23:08.860 --> 00:23:13.100]   But the key point is that it looks exactly the same as it did for vectors,
[00:23:13.100 --> 00:23:17.420]   which looks exactly the same as it did for scalar values.
[00:23:17.420 --> 00:23:20.900]   So one single approach, three different applications.
[00:23:20.900 --> 00:23:23.580]   So now we have all these letters are capital
[00:23:23.580 --> 00:23:25.140]   because they represent matrices.
[00:23:25.140 --> 00:23:28.380]   Our function f takes in a matrix, returns a single value.
[00:23:28.380 --> 00:23:31.580]   We know how it behaves on, say, this single matrix x.
[00:23:31.580 --> 00:23:35.260]   We want to know what happens if we change x by adding to it this matrix e
[00:23:35.260 --> 00:23:35.780]   here.
[00:23:35.780 --> 00:23:37.700]   This is a sort of capital epsilon.
[00:23:37.700 --> 00:23:42.020]   At x plus this big matrix of different entries, what does that look like?
[00:23:42.020 --> 00:23:47.540]   It looks like f of x plus an inner product now between two matrices.
[00:23:47.540 --> 00:23:49.380]   If you want to think about what this means,
[00:23:49.380 --> 00:23:52.160]   it's sort of like turn both of these matrices into vectors
[00:23:52.160 --> 00:23:53.460]   and take their inner product.
[00:23:53.460 --> 00:23:56.460]   But again, basically, the key point here
[00:23:56.460 --> 00:23:59.660]   is that it is a linear function of epsilon.
[00:23:59.660 --> 00:24:02.860]   So as we vary epsilon, this whole thing varies linearly
[00:24:02.860 --> 00:24:04.360]   because it remains an inner product.
[00:24:04.360 --> 00:24:07.660]   It remains essentially a form of matrix multiplication.
[00:24:07.660 --> 00:24:10.260]   And the exact matrix that goes in here, it's
[00:24:10.260 --> 00:24:12.500]   going to come out a little bit differently depending
[00:24:12.500 --> 00:24:14.020]   on which value of x goes in.
[00:24:14.020 --> 00:24:17.740]   And the function that tells us how to take a value x
[00:24:17.740 --> 00:24:23.500]   and return the approximating matrix is the gradient now
[00:24:23.500 --> 00:24:25.100]   of this matrix value function.
[00:24:25.100 --> 00:24:28.700]   And so it's a matrix, just like in the second example, it was a vector.
[00:24:28.700 --> 00:24:31.380]   And again, we're off a little bit in general.
[00:24:31.380 --> 00:24:33.500]   And this is going to be a little o term that's
[00:24:33.500 --> 00:24:35.580]   a function of the norm of a matrix.
[00:24:35.580 --> 00:24:38.420]   So there's a little bit in the slides about a little bit more
[00:24:38.420 --> 00:24:41.700]   about what the inner product of a matrix is, what the norm of a matrix is,
[00:24:41.700 --> 00:24:43.420]   and some links to additional results.
[00:24:43.420 --> 00:24:45.700]   And actually, it goes through a derivation, I believe,
[00:24:45.700 --> 00:24:46.940]   from a function of matrices.
[00:24:46.940 --> 00:24:51.140]   And it's exactly the same as what we did with the vector-valued functions.
[00:24:51.140 --> 00:24:54.220]   So this is the major benefit to thinking this way.
[00:24:54.220 --> 00:24:58.220]   There's this single style for gradients of single variable vector and matrix
[00:24:58.220 --> 00:24:58.780]   functions.
[00:24:58.780 --> 00:25:01.460]   It actually even extends to functional calculus,
[00:25:01.460 --> 00:25:05.880]   where we're taking derivatives where the input is a function itself,
[00:25:05.880 --> 00:25:09.080]   not just a single variable vector or matrix.
[00:25:09.080 --> 00:25:09.700]   So that's cool.
[00:25:09.700 --> 00:25:12.500]   It's always nice to be using a tool that really extends
[00:25:12.500 --> 00:25:14.620]   all the way to the hardest cases.
[00:25:14.620 --> 00:25:16.820]   And you may have noticed there were no indices anywhere.
[00:25:16.820 --> 00:25:19.700]   So indices have disappeared from our calculations.
[00:25:19.700 --> 00:25:24.260]   I find it cleaner, easier to follow, and more straightforward to not
[00:25:24.260 --> 00:25:26.300]   have to use indices everywhere.
[00:25:26.300 --> 00:25:29.580]   The little o notation showing up instead of limits
[00:25:29.580 --> 00:25:32.500]   allows us to use some of our intuitions from using big O notation,
[00:25:32.500 --> 00:25:35.140]   from thinking about the speed of programs,
[00:25:35.140 --> 00:25:38.200]   and to avoid doing everything underneath the limit.
[00:25:38.200 --> 00:25:41.300]   So that gives us a view of what calculus is doing.
[00:25:41.300 --> 00:25:44.260]   It's giving us linear approximations to functions.
[00:25:44.260 --> 00:25:46.660]   So now let's use that view of what calculus
[00:25:46.660 --> 00:25:49.700]   is doing in order to understand our optimization
[00:25:49.700 --> 00:25:51.300]   process in machine learning.
[00:25:51.300 --> 00:25:54.140]   So when we optimize in machine learning, we
[00:25:54.140 --> 00:25:57.100]   optimize through many tiny changes, almost always.
[00:25:57.100 --> 00:26:00.180]   There are a few times where we optimize by just doing it all at once.
[00:26:00.180 --> 00:26:04.700]   But we almost always optimize by making many tiny changes.
[00:26:04.700 --> 00:26:06.540]   What we want to do in machine learning is
[00:26:06.540 --> 00:26:11.140]   we want to update the parameter values of a model to make it perform better.
[00:26:11.140 --> 00:26:14.020]   So usually in machine learning, effectively,
[00:26:14.020 --> 00:26:20.180]   our program for doing the task that we want to do is specified by a vector.
[00:26:20.180 --> 00:26:23.260]   And then we can calculate how well it's doing with this function called
[00:26:23.260 --> 00:26:24.620]   the loss function.
[00:26:24.620 --> 00:26:25.700]   So it takes in the vector.
[00:26:25.700 --> 00:26:30.420]   It usually applies some function to a bunch of data, sort of checks.
[00:26:30.420 --> 00:26:34.180]   Maybe it's predicting something, one data value based off another value.
[00:26:34.180 --> 00:26:35.540]   Maybe it's compressing data.
[00:26:35.540 --> 00:26:37.820]   There's lots of things we can do with machine learning.
[00:26:37.820 --> 00:26:40.980]   But we always need to be able to score how well we are doing.
[00:26:40.980 --> 00:26:42.500]   And that's the function f here.
[00:26:42.500 --> 00:26:43.900]   And we want to make that value--
[00:26:43.900 --> 00:26:47.580]   usually, it's golf rules where lower is better.
[00:26:47.580 --> 00:26:52.220]   And so I want to choose new parameters such that the value of this function
[00:26:52.220 --> 00:26:53.300]   goes down.
[00:26:53.300 --> 00:26:56.900]   It'd be great if we just knew all the values of f,
[00:26:56.900 --> 00:26:59.340]   if we could just look at the plot of the function
[00:26:59.340 --> 00:27:02.420]   and say, oh, that would be the best value over there.
[00:27:02.420 --> 00:27:04.340]   But we can't do that in general.
[00:27:04.340 --> 00:27:07.820]   These are 100 dimensional vectors, million dimensional vectors,
[00:27:07.820 --> 00:27:11.060]   trillion dimensional vectors nowadays in machine learning.
[00:27:11.060 --> 00:27:14.620]   So we'll never, ever be able to look at them and solve it.
[00:27:14.620 --> 00:27:17.420]   And we can't even actually know what the value of the function
[00:27:17.420 --> 00:27:18.940]   is going to be at some other point.
[00:27:18.940 --> 00:27:20.860]   We have to calculate values of this function.
[00:27:20.860 --> 00:27:23.220]   It's often extremely expensive.
[00:27:23.220 --> 00:27:25.620]   And we have to calculate-- for some models,
[00:27:25.620 --> 00:27:27.660]   we have to feed gigabytes of data into them
[00:27:27.660 --> 00:27:28.820]   to know what this value is going to be.
[00:27:28.820 --> 00:27:32.100]   So we want to try and avoid that calculation as much as possible.
[00:27:32.100 --> 00:27:34.820]   So what we're going to do is we're going to use linear approximations
[00:27:34.820 --> 00:27:35.740]   to that function.
[00:27:35.740 --> 00:27:38.780]   So we're going to make my best linear guess to what
[00:27:38.780 --> 00:27:40.420]   the function will be somewhere else.
[00:27:40.420 --> 00:27:42.980]   And we want that to say, oh, your performance will be better.
[00:27:42.980 --> 00:27:44.780]   The value of f will be smaller.
[00:27:44.780 --> 00:27:49.140]   So we're going to choose new parameters to make the best linear guess go down
[00:27:49.140 --> 00:27:50.700]   as fast as possible.
[00:27:50.700 --> 00:27:53.380]   And so we'll see in a second that the way to do that
[00:27:53.380 --> 00:27:57.020]   is to pick our value of epsilon, our change in our parameters.
[00:27:57.020 --> 00:27:59.740]   We pick it to point basically in the opposite direction.
[00:27:59.740 --> 00:28:03.660]   So it makes this linear term as negative as possible.
[00:28:03.660 --> 00:28:06.100]   Then the question is, how big do we make epsilon?
[00:28:06.100 --> 00:28:09.260]   Do we make epsilon really big because we want to make a big change?
[00:28:09.260 --> 00:28:12.220]   Well, no, because we know that this is just a linear guess, right?
[00:28:12.220 --> 00:28:14.180]   We shouldn't change too much.
[00:28:14.180 --> 00:28:16.980]   And that's coming from this last term here, this purple term,
[00:28:16.980 --> 00:28:19.900]   saying that the bigger we make our change, the wronger
[00:28:19.900 --> 00:28:23.340]   we should expect to be about whether our machine learning model is actually
[00:28:23.340 --> 00:28:24.780]   performing better or not.
[00:28:24.780 --> 00:28:26.860]   And reading all that off leads us directly
[00:28:26.860 --> 00:28:28.520]   to the algorithm of gradient descent.
[00:28:28.520 --> 00:28:33.700]   So it says for our new value of x here on the left, which we do by assignment,
[00:28:33.700 --> 00:28:38.540]   it's equal to the previous value of x minus the value of the gradient.
[00:28:38.540 --> 00:28:44.740]   And this guy, minus eta times gradient of f of x, that there
[00:28:44.740 --> 00:28:48.540]   is our epsilon term from above, this part here.
[00:28:48.540 --> 00:28:53.580]   That's how we are changing x, negative eta gradient of f of x.
[00:28:53.580 --> 00:28:56.380]   So now why are we descending the gradient?
[00:28:56.380 --> 00:28:58.400]   Why are we using the negative gradient?
[00:28:58.400 --> 00:29:02.420]   It's because this makes the linear term as negative as possible.
[00:29:02.420 --> 00:29:04.820]   So I've drawn this here, the gold arrow.
[00:29:04.820 --> 00:29:06.620]   That's the direction the gradient points.
[00:29:06.620 --> 00:29:09.660]   Remember, the gradient here is a vector, and so it has a direction.
[00:29:09.660 --> 00:29:12.620]   And so imagine this two-dimensional space here
[00:29:12.620 --> 00:29:15.500]   is all the directions that vectors point,
[00:29:15.500 --> 00:29:18.140]   and the gradient is this vector pointing in this direction.
[00:29:18.140 --> 00:29:20.720]   So this is a function that takes in two numbers
[00:29:20.720 --> 00:29:22.860]   and spits out a single number.
[00:29:22.860 --> 00:29:25.620]   So the question is, how does this function behave?
[00:29:25.620 --> 00:29:27.580]   How does this inner product function behave?
[00:29:27.580 --> 00:29:31.020]   As I change epsilon, and that's what this circle here
[00:29:31.020 --> 00:29:32.120]   is meant to demonstrate.
[00:29:32.120 --> 00:29:36.500]   As I rotate epsilon around, then the arrow here
[00:29:36.500 --> 00:29:38.580]   is moving around this circle.
[00:29:38.580 --> 00:29:43.220]   And at each point, it produces a particular value of this term
[00:29:43.220 --> 00:29:44.980]   here, this inner product term.
[00:29:44.980 --> 00:29:48.340]   So when it's close to the gradient, you get a really high value,
[00:29:48.340 --> 00:29:50.540]   as indicated by this color scale here.
[00:29:50.540 --> 00:29:53.620]   And as we rotate around and go towards the middle,
[00:29:53.620 --> 00:29:55.100]   we get middling values.
[00:29:55.100 --> 00:29:57.700]   And as we go all the way towards the opposite direction,
[00:29:57.700 --> 00:29:59.900]   this term here becomes negative.
[00:29:59.900 --> 00:30:02.980]   So when we're pointing all the way in this direction represented
[00:30:02.980 --> 00:30:06.380]   by the blue arrow here, we get very negative values.
[00:30:06.380 --> 00:30:08.980]   So this function here shows you actually exactly how
[00:30:08.980 --> 00:30:12.380]   this function behaves as you spin epsilon around,
[00:30:12.380 --> 00:30:15.500]   you say, clockwise from gold to blue.
[00:30:15.500 --> 00:30:19.500]   And that blue variable there is negative 1 times the gradient.
[00:30:19.500 --> 00:30:21.020]   It's the gradient flipped around.
[00:30:21.020 --> 00:30:23.420]   Just FYI, this function may look familiar.
[00:30:23.420 --> 00:30:24.300]   It's cosine.
[00:30:24.300 --> 00:30:26.540]   It's basically-- it's a scaled version of cosine,
[00:30:26.540 --> 00:30:28.340]   depending on how big the vectors are.
[00:30:28.340 --> 00:30:30.580]   So the basic point of this slide is just
[00:30:30.580 --> 00:30:34.860]   that if we want to make this function here as negative as possible,
[00:30:34.860 --> 00:30:39.860]   we want to make the epsilon value equal to negative 1 times the gradient,
[00:30:39.860 --> 00:30:41.740]   and so point in this direction.
[00:30:41.740 --> 00:30:45.620]   So that makes this term here in the middle as negative as possible.
[00:30:45.620 --> 00:30:48.140]   And then so long as this term on the right
[00:30:48.140 --> 00:30:51.740]   here is not bigger than epsilon, which we can guarantee because it's
[00:30:51.740 --> 00:30:56.900]   a little o, then that will make f at x plus epsilon smaller than f of x.
[00:30:56.900 --> 00:31:01.860]   And we will have made our machine learning algorithm perform better
[00:31:01.860 --> 00:31:03.740]   than it was performing before.
[00:31:03.740 --> 00:31:08.180]   And so gradient descent basically goes down the steepest slope.
[00:31:08.180 --> 00:31:11.980]   So I've drawn the gradients here on this topography here.
[00:31:11.980 --> 00:31:15.900]   So this is what the function maybe looked like for that previous example.
[00:31:15.900 --> 00:31:18.940]   We were looking at a single point, and we were imagining the gradient points
[00:31:18.940 --> 00:31:21.460]   this way, how will our linear approximation
[00:31:21.460 --> 00:31:23.620]   behave as we rotate epsilon.
[00:31:23.620 --> 00:31:27.620]   Now we have a two-dimensional function here that produces a scalar value.
[00:31:27.620 --> 00:31:29.540]   And I've drawn here a couple of places I've
[00:31:29.540 --> 00:31:31.140]   drawn what the gradient looks like.
[00:31:31.140 --> 00:31:33.260]   So the gradient points sort of up.
[00:31:33.260 --> 00:31:36.740]   The gradient goes uphill in the steepest direction possible.
[00:31:36.740 --> 00:31:39.020]   At the tops of hills, it's very clear you're at the top.
[00:31:39.020 --> 00:31:41.620]   There's nowhere to go that will make you go upwards.
[00:31:41.620 --> 00:31:47.420]   At the bottoms of valleys, the gradient is also once again 0.
[00:31:47.420 --> 00:31:50.660]   The way to think about it maybe is that at the bottom here,
[00:31:50.660 --> 00:31:53.340]   there's a whole bunch of directions you could go up in.
[00:31:53.340 --> 00:31:56.580]   There's no way to choose a particular direction to go up in.
[00:31:56.580 --> 00:31:59.460]   But it's maybe better-- rather than thinking of gradients as going up
[00:31:59.460 --> 00:32:02.180]   or down slopes, it's maybe better to think
[00:32:02.180 --> 00:32:04.460]   in terms of these linear approximations.
[00:32:04.460 --> 00:32:07.300]   We should say that at this point, the best linear approximation
[00:32:07.300 --> 00:32:09.540]   is constant because the function is curving away
[00:32:09.540 --> 00:32:13.860]   in every direction, which is the same at the top here.
[00:32:13.860 --> 00:32:16.300]   Higher values to this function mean worse performance.
[00:32:16.300 --> 00:32:17.940]   So we want to descend gradients.
[00:32:17.940 --> 00:32:18.940]   We don't want to go up.
[00:32:18.940 --> 00:32:19.820]   We want to go down.
[00:32:19.820 --> 00:32:21.300]   So we flip them around.
[00:32:21.300 --> 00:32:25.540]   That's another way of thinking about what gradient descent does.
[00:32:25.540 --> 00:32:29.140]   So gradient descent tends to find the bottoms of valleys.
[00:32:29.140 --> 00:32:32.940]   In something like this, we would end up in one of these little divots
[00:32:32.940 --> 00:32:34.580]   here by applying gradient descent.
[00:32:34.580 --> 00:32:37.740]   We don't know where to start necessarily in machine learning
[00:32:37.740 --> 00:32:39.180]   and optimization in general.
[00:32:39.180 --> 00:32:41.860]   We often think of ourselves as starting at a random point.
[00:32:41.860 --> 00:32:44.460]   And so we end up randomly in one of these valleys.
[00:32:44.460 --> 00:32:48.020]   So to make that metaphor really direct, I've
[00:32:48.020 --> 00:32:51.700]   got here the topography of my home state of California, which
[00:32:51.700 --> 00:32:55.100]   has a much more interesting topography than the state I was born in,
[00:32:55.100 --> 00:32:58.420]   which is Illinois-- very flat, not very interesting for gradient descent.
[00:32:58.420 --> 00:33:00.260]   California is much more interesting.
[00:33:00.260 --> 00:33:03.140]   So if you imagine you start somewhere in California
[00:33:03.140 --> 00:33:06.580]   here with these big mountains and this big central valley,
[00:33:06.580 --> 00:33:09.420]   the Bay Area down here, another low-lying area.
[00:33:09.420 --> 00:33:12.180]   And you imagine, OK, what would happen if I just walked downhill?
[00:33:12.180 --> 00:33:14.380]   Because that's basically what gradient descent does.
[00:33:14.380 --> 00:33:18.660]   In most places that you start, you would end up in this big area here,
[00:33:18.660 --> 00:33:19.420]   the Central Valley.
[00:33:19.420 --> 00:33:21.620]   If you pick a random part of California to start in,
[00:33:21.620 --> 00:33:25.420]   this basin here of the Central Valley is the place you would probably end up.
[00:33:25.420 --> 00:33:27.100]   The least likely place for you to end up
[00:33:27.100 --> 00:33:29.780]   is the highest point in California, Mount Whitney,
[00:33:29.780 --> 00:33:32.540]   because you'd have to land exactly on the top of Mount Whitney.
[00:33:32.540 --> 00:33:35.940]   And then if you're exactly at the top, then there's basically
[00:33:35.940 --> 00:33:38.740]   every direction is downhill, and you would go nowhere.
[00:33:38.740 --> 00:33:41.500]   The lowest place in California is actually not
[00:33:41.500 --> 00:33:44.140]   the place where you would end up in most cases.
[00:33:44.140 --> 00:33:45.860]   The lowest place in California is actually
[00:33:45.860 --> 00:33:48.940]   this really deep cut in the middle of the Rocky Mountains
[00:33:48.940 --> 00:33:50.620]   here called Badwater Basin.
[00:33:50.620 --> 00:33:53.820]   And unlike the Central Valley, it is very sharp.
[00:33:53.820 --> 00:33:55.460]   It goes down really quickly.
[00:33:55.460 --> 00:33:58.740]   The Central Valley is nice and gently sloping down from the mountains
[00:33:58.740 --> 00:34:00.340]   into this big flat area.
[00:34:00.340 --> 00:34:03.340]   And so Badwater Basin here is maybe where
[00:34:03.340 --> 00:34:05.960]   you think you'd like to end up if you're looking
[00:34:05.960 --> 00:34:07.860]   for the lowest point in California.
[00:34:07.860 --> 00:34:10.540]   But it's not where you're going to end up most of the time.
[00:34:10.540 --> 00:34:13.340]   Now, what people have found is that gradient descent
[00:34:13.340 --> 00:34:15.540]   finds things like the Central Valley.
[00:34:15.540 --> 00:34:18.740]   And for realistic problems, things like Badwater Basin,
[00:34:18.740 --> 00:34:22.220]   these sharp, deep little divots, are not where you want to end up.
[00:34:22.220 --> 00:34:24.060]   And the Central Valley is actually better.
[00:34:24.060 --> 00:34:26.980]   And this is one of the reasons why gradient descent has become basically
[00:34:26.980 --> 00:34:30.420]   the most popular algorithm for optimizing machine learning
[00:34:30.420 --> 00:34:33.700]   models over all kinds of other options that people have considered
[00:34:33.700 --> 00:34:35.460]   for how to make algorithms better.
[00:34:35.460 --> 00:34:37.700]   We have time to talk about the automation of calculus.
[00:34:37.700 --> 00:34:38.260]   That's good.
[00:34:38.260 --> 00:34:40.780]   We talked a good amount about how you would actually
[00:34:40.780 --> 00:34:44.900]   calculate derivatives in our two sections on scalar and vector
[00:34:44.900 --> 00:34:46.220]   valued functions.
[00:34:46.220 --> 00:34:49.180]   But it's actually not the case that in order to do machine learning,
[00:34:49.180 --> 00:34:50.980]   you need to become really good at calculus.
[00:34:50.980 --> 00:34:52.060]   That used to be the case.
[00:34:52.060 --> 00:34:53.900]   But luckily, that's not the case anymore.
[00:34:53.900 --> 00:34:55.680]   So you don't really need to sweat actually
[00:34:55.680 --> 00:34:58.700]   being able to calculate these things, though it is very important
[00:34:58.700 --> 00:35:01.460]   that you develop your intuition for things about calculus
[00:35:01.460 --> 00:35:03.700]   so that you can understand the behavior of algorithms
[00:35:03.700 --> 00:35:05.820]   without having to whip out a math textbook.
[00:35:05.820 --> 00:35:08.540]   So in the bad old days, calculus for machine learning
[00:35:08.540 --> 00:35:10.620]   was done entirely by hand.
[00:35:10.620 --> 00:35:12.580]   So this block of mathematics on the right
[00:35:12.580 --> 00:35:16.180]   here is the calculation of one piece of the gradient.
[00:35:16.180 --> 00:35:19.020]   So in addition to getting the math right,
[00:35:19.020 --> 00:35:21.100]   an ML researcher engineer in the late '90s
[00:35:21.100 --> 00:35:24.060]   then had to translate that math into computer code
[00:35:24.060 --> 00:35:26.000]   without introducing any bugs.
[00:35:26.000 --> 00:35:28.500]   And this was this tremendous bottleneck.
[00:35:28.500 --> 00:35:32.660]   Not only do you have to spend time actually calculating the gradients,
[00:35:32.660 --> 00:35:36.620]   you then need to write them into an equally sized block of code
[00:35:36.620 --> 00:35:38.300]   and then test that code.
[00:35:38.300 --> 00:35:40.300]   So this part here is actually just-- this
[00:35:40.300 --> 00:35:43.860]   is the part of the code that implements the model that
[00:35:43.860 --> 00:35:47.380]   goes from inputs to final value.
[00:35:47.380 --> 00:35:48.980]   So that's this block of code here.
[00:35:48.980 --> 00:35:54.820]   And now if we were in the battle days in the '90s, then what we would do
[00:35:54.820 --> 00:36:00.580]   is we would then have to write an additional equally sized block of code
[00:36:00.580 --> 00:36:04.060]   that would take this math here and basically write it out.
[00:36:04.060 --> 00:36:06.660]   So derivative for this function, derivative for this function,
[00:36:06.660 --> 00:36:08.100]   derivative for all of these.
[00:36:08.100 --> 00:36:12.220]   So nowadays, what happens instead is that you take something like this
[00:36:12.220 --> 00:36:14.700]   and then you just pass it to something that automatically
[00:36:14.700 --> 00:36:16.580]   calculates gradients for you.
[00:36:16.580 --> 00:36:20.220]   So in this case, I'm using the AutoGrad library here.
[00:36:20.220 --> 00:36:21.940]   You basically just pass whatever function
[00:36:21.940 --> 00:36:26.260]   is you want the gradient of, in this case, the loss function, to AutoGrad
[00:36:26.260 --> 00:36:29.100]   instead of writing an equally complicated block of code
[00:36:29.100 --> 00:36:33.820]   to this 32 or so lines of dense math code.
[00:36:33.820 --> 00:36:36.900]   And the derivatives you get will be tested, verified
[00:36:36.900 --> 00:36:41.540]   by the community of folks who are experts in making sure
[00:36:41.540 --> 00:36:43.980]   that these gradients are calculated correctly.
[00:36:43.980 --> 00:36:46.700]   So the core technology that makes this work
[00:36:46.700 --> 00:36:49.980]   is something called a computational graph.
[00:36:49.980 --> 00:36:53.420]   So this example is adapted from a really nice blog post
[00:36:53.420 --> 00:36:56.540]   called Calculus on Computational Graphs by Chris Ola, who's
[00:36:56.540 --> 00:37:00.340]   one of the top explainers in the ML space.
[00:37:00.340 --> 00:37:03.620]   If we take an expression like this, we can break down
[00:37:03.620 --> 00:37:06.700]   how this gets calculated by a computer and trace
[00:37:06.700 --> 00:37:10.500]   all the operations that go into calculating the final value, e.
[00:37:10.500 --> 00:37:12.380]   So we have our a plus b.
[00:37:12.380 --> 00:37:13.700]   We have our b plus 1.
[00:37:13.700 --> 00:37:16.620]   These become intermediate variables.
[00:37:16.620 --> 00:37:18.140]   Call them c and d.
[00:37:18.140 --> 00:37:20.660]   And then we multiply c and d together to get e.
[00:37:20.660 --> 00:37:21.900]   So we've broken this down.
[00:37:21.900 --> 00:37:23.660]   This is a composite expression.
[00:37:23.660 --> 00:37:26.820]   And this is a sequence of atomic simple operations,
[00:37:26.820 --> 00:37:28.460]   adding and multiplying.
[00:37:28.460 --> 00:37:30.460]   So we can write that as a graph.
[00:37:30.460 --> 00:37:32.540]   So a goes into the add function.
[00:37:32.540 --> 00:37:33.620]   And so does b.
[00:37:33.620 --> 00:37:34.820]   That's what this arrow means.
[00:37:34.820 --> 00:37:37.220]   b goes into this add1 function.
[00:37:37.220 --> 00:37:39.300]   These are our two variables, c and d,
[00:37:39.300 --> 00:37:40.660]   on the left and right here.
[00:37:40.660 --> 00:37:43.380]   Those two things get put into the multiply function.
[00:37:43.380 --> 00:37:44.580]   And that's our final value.
[00:37:44.580 --> 00:37:45.980]   So it's in green here at the bottom
[00:37:45.980 --> 00:37:47.340]   because it's the final output.
[00:37:47.340 --> 00:37:49.260]   And then the inputs are in blue.
[00:37:49.260 --> 00:37:51.060]   So we can actually-- so this here
[00:37:51.060 --> 00:37:55.460]   was a cartoon based off of this mathematical expression here.
[00:37:55.460 --> 00:37:58.420]   We can actually turn that into a Python expression
[00:37:58.420 --> 00:38:01.220]   and then generate that graph automatically.
[00:38:01.220 --> 00:38:03.820]   I generated this in PyTorch, which
[00:38:03.820 --> 00:38:05.820]   is a machine learning framework that provides
[00:38:05.820 --> 00:38:07.540]   computational graph support.
[00:38:07.540 --> 00:38:11.020]   So I've got e is this variable down here, mole backward.
[00:38:11.020 --> 00:38:13.340]   You can ignore the backward part there.
[00:38:13.340 --> 00:38:15.020]   So this is the multiplication step here.
[00:38:15.020 --> 00:38:16.600]   And what goes into the multiplication?
[00:38:16.600 --> 00:38:20.220]   It's two variables here go into the multiplication step.
[00:38:20.220 --> 00:38:23.060]   And this one, this left addition here,
[00:38:23.060 --> 00:38:25.860]   has a and b as its components.
[00:38:25.860 --> 00:38:28.140]   So a plus b go into this one.
[00:38:28.140 --> 00:38:30.860]   And then b plus 1 goes into this one.
[00:38:30.860 --> 00:38:32.020]   So this is another addition.
[00:38:32.020 --> 00:38:34.500]   And it consumes only b, not a.
[00:38:34.500 --> 00:38:38.100]   This is a way of tracing how a value was calculated.
[00:38:38.100 --> 00:38:40.380]   So normally, we don't think about that necessarily
[00:38:40.380 --> 00:38:41.960]   when we're writing computer programs.
[00:38:41.960 --> 00:38:44.460]   We don't think about carrying around
[00:38:44.460 --> 00:38:47.620]   the sequence of operations that result in a final value.
[00:38:47.620 --> 00:38:49.620]   But actually, if you just write--
[00:38:49.620 --> 00:38:51.780]   if you just implement addition and multiplication
[00:38:51.780 --> 00:38:54.460]   in the right way, then you can track all this information
[00:38:54.460 --> 00:38:55.300]   automatically.
[00:38:55.300 --> 00:38:57.620]   And if we have stored somewhere a function
[00:38:57.620 --> 00:38:59.620]   for the gradient of addition and for the gradient
[00:38:59.620 --> 00:39:01.220]   of multiplication, then we can use
[00:39:01.220 --> 00:39:03.740]   them to get the gradient of e with respect
[00:39:03.740 --> 00:39:04.900]   to the input variables.
[00:39:04.900 --> 00:39:06.980]   And this is essentially an automated version
[00:39:06.980 --> 00:39:08.540]   of what you learn to do in calculus.
[00:39:08.540 --> 00:39:09.980]   You memorize a few derivatives.
[00:39:09.980 --> 00:39:11.260]   What's the derivative of sine?
[00:39:11.260 --> 00:39:12.700]   What's the derivative of x squared?
[00:39:12.700 --> 00:39:14.700]   What's the derivative of e to the x?
[00:39:14.700 --> 00:39:16.860]   And then you use rules to calculate new ones,
[00:39:16.860 --> 00:39:20.580]   rules like the sum rule, the product rule, the chain rule.
[00:39:20.580 --> 00:39:22.820]   And the difference is that computers are much faster
[00:39:22.820 --> 00:39:24.300]   and they don't make mistakes.
[00:39:24.300 --> 00:39:26.660]   So they can do this automatically really quickly.
[00:39:26.660 --> 00:39:29.380]   There's some rules for this, which is basically
[00:39:29.380 --> 00:39:32.500]   the chain rule and the sum rule that basically say,
[00:39:32.500 --> 00:39:37.020]   as I go along a path in a graph, the gradients multiply.
[00:39:37.020 --> 00:39:40.460]   And as I look across multiple paths converging,
[00:39:40.460 --> 00:39:42.300]   then the gradients add.
[00:39:42.300 --> 00:39:45.780]   You can read either these slides here or the blog post
[00:39:45.780 --> 00:39:49.220]   from Chris Ola, if you want more detail about the rules, where
[00:39:49.220 --> 00:39:51.420]   these rules come from to make the computational graph.
[00:39:51.420 --> 00:39:53.540]   But the important point here is just
[00:39:53.540 --> 00:39:57.860]   that this technology enables the automatic calculation
[00:39:57.860 --> 00:39:58.860]   of gradients.
[00:39:58.860 --> 00:40:01.540]   And so it means that this difficult step in machine
[00:40:01.540 --> 00:40:04.820]   learning of figuring out what the best linear approximation
[00:40:04.820 --> 00:40:08.060]   of your function is that used to require a mathematician now
[00:40:08.060 --> 00:40:09.300]   does not.
[00:40:09.300 --> 00:40:12.300]   Let's close out with some additional resources.
[00:40:12.300 --> 00:40:15.580]   So if you need a refresher on deep concepts like limit,
[00:40:15.580 --> 00:40:18.340]   if you felt like, OK, I actually was a little bit confused
[00:40:18.340 --> 00:40:20.540]   by our discussion about limits and approximations
[00:40:20.540 --> 00:40:22.940]   and derivatives, I would recommend 3Blue1Brown's
[00:40:22.940 --> 00:40:24.980]   YouTube series, The Essence of Calculus.
[00:40:24.980 --> 00:40:28.300]   This guy is one of the best math explainers in the world.
[00:40:28.300 --> 00:40:30.580]   And The Essence of Calculus series
[00:40:30.580 --> 00:40:32.840]   is excellent, just as The Essence of Linear Algebra
[00:40:32.840 --> 00:40:33.780]   series was excellent.
[00:40:33.780 --> 00:40:36.380]   If you want to learn more about this Frchet derivative style,
[00:40:36.380 --> 00:40:39.140]   I have a blog post series, Frchet Derivatives 1
[00:40:39.140 --> 00:40:41.960]   through 4, that applies the Frchet derivative
[00:40:41.960 --> 00:40:45.140]   to a couple of very simple ML problems culminating
[00:40:45.140 --> 00:40:46.980]   in linear neural networks.
[00:40:46.980 --> 00:40:48.820]   If you want a deep dive on vector calculus
[00:40:48.820 --> 00:40:51.380]   beyond just machine learning, your appetite for calculus
[00:40:51.380 --> 00:40:53.420]   has been whetted, and you want to understand
[00:40:53.420 --> 00:40:55.540]   calculus in general better, consider
[00:40:55.540 --> 00:40:58.220]   the multivariable calculus lectures on Khan Academy.
[00:40:58.220 --> 00:41:01.540]   It's an old school MOOC covering the calculus, mostly
[00:41:01.540 --> 00:41:03.620]   what you need for physics and engineering, things like
[00:41:03.620 --> 00:41:05.820]   divergence and curl that we didn't talk about,
[00:41:05.820 --> 00:41:08.020]   and a lot more stuff about integrals, which are less
[00:41:08.020 --> 00:41:09.340]   important for machine learning.
[00:41:09.340 --> 00:41:10.020]   Those are great.
[00:41:10.020 --> 00:41:12.500]   They're actually by Grant Sanderson, who later became
[00:41:12.500 --> 00:41:13.620]   3Blue1Brown.
[00:41:13.620 --> 00:41:16.420]   So if you prefer to learn from interactive Jupyter Notebooks,
[00:41:16.420 --> 00:41:19.860]   Python examples, then check out this calculus notebook,
[00:41:19.860 --> 00:41:23.300]   a colab, an online notebook by Aurelien Guerin,
[00:41:23.300 --> 00:41:26.100]   for his book, Hands-On Machine Learning.
[00:41:26.100 --> 00:41:29.020]   It's got a bunch of these for a bunch of different topics.
[00:41:29.020 --> 00:41:31.860]   So there's actually other math for machine learning notebooks
[00:41:31.860 --> 00:41:33.580]   in that repository.
[00:41:33.580 --> 00:41:36.460]   And if you really want sort of a programmer's approach
[00:41:36.460 --> 00:41:38.980]   to calculus rather than a mathematician's approach,
[00:41:38.980 --> 00:41:41.420]   there's this crazy class, Structure and Interpretation
[00:41:41.420 --> 00:41:43.260]   of Classical Mechanics, by some folks who
[00:41:43.260 --> 00:41:45.300]   made one of the most famous computer science
[00:41:45.300 --> 00:41:47.540]   classes in the world, Structure and Interpretation
[00:41:47.540 --> 00:41:49.180]   of Computer Programs, which takes
[00:41:49.180 --> 00:41:52.300]   this very functional programming approach to understanding
[00:41:52.300 --> 00:41:54.020]   derivatives and calculus.
[00:41:54.020 --> 00:41:57.700]   And in the end, the classical mechanics for which calculus
[00:41:57.700 --> 00:41:59.260]   was invented, Newton's stuff.
[00:41:59.260 --> 00:42:01.940]   [MUSIC PLAYING]
[00:42:01.940 --> 00:42:02.500]   Hey, friends.
[00:42:02.500 --> 00:42:03.500]   Charles here.
[00:42:03.500 --> 00:42:04.780]   Thanks for watching my video.
[00:42:04.780 --> 00:42:06.660]   If you enjoyed it, give it a like.
[00:42:06.660 --> 00:42:10.020]   If you want more with Wysi's tutorial and demo content,
[00:42:10.020 --> 00:42:11.380]   subscribe to our channel.
[00:42:11.380 --> 00:42:13.180]   And if you've got any questions, comments,
[00:42:13.180 --> 00:42:15.780]   ideas for future videos, leave a comment below.
[00:42:15.780 --> 00:42:17.940]   We'd love to hear from you.
[00:42:17.940 --> 00:42:20.520]   (upbeat music)

