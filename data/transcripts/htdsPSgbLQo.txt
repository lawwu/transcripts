
[00:00:00.000 --> 00:00:09.800]   I wanted to understand how we train intelligent agents that have this kind of embodied intelligence
[00:00:09.800 --> 00:00:16.440]   that you see in us and other animals, where we can walk through an environment gracefully,
[00:00:16.440 --> 00:00:22.160]   deliberately, we can get to where we want to go, we can engage with the environment
[00:00:22.160 --> 00:00:31.120]   if we need to rearrange it, we rearrange it, we clearly act spatially intelligently, intelligently
[00:00:31.120 --> 00:00:33.000]   in an embodied fashion.
[00:00:33.000 --> 00:00:39.300]   And this seems very core to me, and I want to understand it because I think this underlies
[00:00:39.300 --> 00:00:42.280]   other kinds of intelligence as well.
[00:00:42.280 --> 00:00:46.640]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:46.640 --> 00:00:49.280]   and I'm your host, Lucas Biewald.
[00:00:49.280 --> 00:00:56.320]   Vladlen Kolton is Chief Scientist for Intelligent Systems at Intel, where he runs a lab of researchers
[00:00:56.320 --> 00:01:02.280]   working on computer vision, robotics, and mapping simulations to reality.
[00:01:02.280 --> 00:01:06.880]   Today we're going to talk about drones, four-legged robots, and a whole bunch of cool stuff.
[00:01:06.880 --> 00:01:11.120]   All right, well, Vladlen, thanks so much for talking with us.
[00:01:11.120 --> 00:01:13.060]   I saw your title is somewhat evocative.
[00:01:13.060 --> 00:01:16.160]   It's the Chief Scientist for Intelligent Systems at Intel.
[00:01:16.160 --> 00:01:18.240]   Can you say a little bit about what the scope of that is?
[00:01:18.240 --> 00:01:19.600]   It sounds intriguing.
[00:01:19.600 --> 00:01:24.280]   Yeah, I prefer the term intelligent systems to AI.
[00:01:24.280 --> 00:01:30.840]   AI is a very loaded term with a very long history, a lot of baggage.
[00:01:30.840 --> 00:01:38.040]   As you may remember, the term fell out of favor for a very long time because AI overpromised
[00:01:38.040 --> 00:01:43.360]   and under-delivered in the '80s and '90s.
[00:01:43.360 --> 00:01:50.320]   And when I became active in the field, when I really learned quite a bit about AI, the
[00:01:50.320 --> 00:01:55.800]   term AI was not used by many of the most serious people in the field.
[00:01:55.800 --> 00:01:59.240]   People avoided the term artificial intelligence.
[00:01:59.240 --> 00:02:03.200]   People identified primarily as machine learning researchers.
[00:02:03.200 --> 00:02:09.340]   And that persisted into, I'd say, the mid-2010s, actually.
[00:02:09.340 --> 00:02:16.880]   It's only very recently that the term AI became respectable again, and serious researchers
[00:02:16.880 --> 00:02:24.680]   on a large scale started to identify themselves as artificial intelligence researchers.
[00:02:24.680 --> 00:02:30.700]   I somehow find the term intelligent systems broader, first of all, because it doesn't
[00:02:30.700 --> 00:02:32.660]   have the word artificial.
[00:02:32.660 --> 00:02:38.220]   So if we're interested in intelligent systems, we clearly are interested in artificial intelligence
[00:02:38.220 --> 00:02:40.580]   systems, but also in natural intelligence systems.
[00:02:40.580 --> 00:02:42.780]   We want to understand the nature of intelligence.
[00:02:42.780 --> 00:02:50.060]   We are concerned with intelligence, understanding it and producing it, inducing it in systems
[00:02:50.060 --> 00:02:51.960]   that we create.
[00:02:51.960 --> 00:02:54.540]   It's a more neutral term with less baggage.
[00:02:54.540 --> 00:02:55.540]   I like it.
[00:02:55.540 --> 00:03:01.060]   I don't mind AI, but somehow I'm more predisposed to intelligent systems.
[00:03:01.060 --> 00:03:02.060]   Cool.
[00:03:02.060 --> 00:03:03.060]   I love it.
[00:03:03.060 --> 00:03:06.340]   And I always try to take the perspective of these as someone who knows about machine learning
[00:03:06.340 --> 00:03:11.300]   or intelligent systems, but maybe isn't an expert in your field, which will be super
[00:03:11.300 --> 00:03:15.620]   easy in this interview because I know very little about robotics and a lot of the stuff
[00:03:15.620 --> 00:03:17.620]   that you've been working on.
[00:03:17.620 --> 00:03:19.980]   I am very intrigued by it.
[00:03:19.980 --> 00:03:22.860]   And I think anyone can understand how cool this stuff is.
[00:03:22.860 --> 00:03:26.980]   So I'd love to ask you about some of the papers that I was looking at.
[00:03:26.980 --> 00:03:32.640]   One that just stuck out to myself now, but also my younger self, is just unbelievably
[00:03:32.640 --> 00:03:40.220]   cool was the paper that you wrote on quadruped locomotion, where you have a walking robot
[00:03:40.220 --> 00:03:42.300]   navigating terrain.
[00:03:42.300 --> 00:03:45.860]   And I think what was maybe most evocative about it was you say that you basically train
[00:03:45.860 --> 00:03:48.020]   the robot completely in simulation.
[00:03:48.020 --> 00:03:52.460]   And so then it's sort of like zero shot learning in new terrain.
[00:03:52.460 --> 00:03:58.460]   And I guess, could you say for someone like me, actually, who's not an expert in the field,
[00:03:58.460 --> 00:04:00.860]   what's hard about this, just in general?
[00:04:00.860 --> 00:04:05.660]   And then what did your paper offer that was sort of new to this challenge?
[00:04:05.660 --> 00:04:07.040]   Yeah.
[00:04:07.040 --> 00:04:16.900]   Legged locomotion is very hard because you need to coordinate the actuation of many actuators.
[00:04:16.900 --> 00:04:23.480]   And there is one very visceral way to understand how hard it is, which is to control an animated
[00:04:23.480 --> 00:04:29.760]   character with simple legs, where you need to actuate their different joints or their
[00:04:29.760 --> 00:04:32.720]   different muscles with different keys on the keyboard.
[00:04:32.720 --> 00:04:34.380]   And there are games like this.
[00:04:34.380 --> 00:04:37.400]   And you can try doing this even with just four joints.
[00:04:37.400 --> 00:04:40.500]   So try actuating four joints yourself.
[00:04:40.500 --> 00:04:42.820]   And it's basically impossible.
[00:04:42.820 --> 00:04:44.920]   It's just brutally, brutally hard.
[00:04:44.920 --> 00:04:51.920]   It's this delicate dance, where at the same time, in synchrony, different muscles need
[00:04:51.920 --> 00:04:53.620]   to fire just right.
[00:04:53.620 --> 00:04:58.040]   And one is firing more and more strongly, and the other needs to subside.
[00:04:58.040 --> 00:04:59.560]   And this needs to be coordinated.
[00:04:59.560 --> 00:05:04.340]   This is a very precise trajectory in a very high dimensional space.
[00:05:04.340 --> 00:05:05.540]   This is hard to learn.
[00:05:05.540 --> 00:05:10.440]   And if you look at human toddlers learning it, it takes them a good couple of years to
[00:05:10.440 --> 00:05:11.440]   learn it.
[00:05:11.440 --> 00:05:15.440]   This is even for human intelligence, which is awesome.
[00:05:15.440 --> 00:05:19.280]   And I use the term awesome here in this original meaning.
[00:05:19.280 --> 00:05:22.040]   I don't mean awesome like a really good cup of coffee.
[00:05:22.040 --> 00:05:24.760]   I mean awesome.
[00:05:24.760 --> 00:05:30.080]   Even for this level of intelligence, it takes a couple of years of experience to get a hang
[00:05:30.080 --> 00:05:32.200]   of legged locomotion.
[00:05:32.200 --> 00:05:35.040]   So this is very, very hard.
[00:05:35.040 --> 00:05:41.840]   And we want our systems to discover this, to master this delicate dance that as adult
[00:05:41.840 --> 00:05:45.720]   humans we basically take for granted.
[00:05:45.720 --> 00:05:51.240]   And you can look at basically the most successful, I would say, attempt so far, which is Boston
[00:05:51.240 --> 00:06:00.160]   Dynamics, which is a group of incredibly smart, incredibly dedicated, insightful engineers
[00:06:00.160 --> 00:06:05.040]   who are some of the best in the world at this, a large group.
[00:06:05.040 --> 00:06:06.480]   And it took them 30 years.
[00:06:06.480 --> 00:06:15.200]   It took them 30 years to really get it, to really design and tune legged locomotion controllers
[00:06:15.200 --> 00:06:17.000]   that are very robust.
[00:06:17.000 --> 00:06:23.760]   We did this, and depending how you count, but I would say about two, three years, primarily
[00:06:23.760 --> 00:06:26.360]   with two graduate students.
[00:06:26.360 --> 00:06:30.360]   Now these are amazing graduate students.
[00:06:30.360 --> 00:06:33.360]   These are really extraordinary graduate students.
[00:06:33.360 --> 00:06:38.560]   But still, the fact that we could do this in two, three years speaks to the power of
[00:06:38.560 --> 00:06:39.560]   the approach.
[00:06:39.560 --> 00:06:46.240]   And the approach is essentially taking the system through a tremendous amount of experience
[00:06:46.240 --> 00:06:54.840]   in simulation and have it do all the trying and falling in simulation.
[00:06:54.840 --> 00:07:00.560]   And then the key question after that is what happens when you learn in simulation and put
[00:07:00.560 --> 00:07:06.480]   the model, put the controller on the real robot in reality, will it work?
[00:07:06.480 --> 00:07:12.740]   And there are a few ideas that make it work and a few pleasant surprises where it worked
[00:07:12.740 --> 00:07:14.440]   better than we expected.
[00:07:14.440 --> 00:07:19.600]   One key idea that was introduced in our previous paper, the science robotics paper that we
[00:07:19.600 --> 00:07:27.400]   published a couple of years ago, is to empirically characterize the actuators that are used on
[00:07:27.400 --> 00:07:29.000]   the real robot.
[00:07:29.000 --> 00:07:37.880]   So you basically measure, you do system identification, you measure the dynamics model of each actuator
[00:07:37.880 --> 00:07:42.560]   empirically by just perturbing the robot, actuating the actuator, and just seeing what
[00:07:42.560 --> 00:07:45.640]   happens, seeing how the system responds.
[00:07:45.640 --> 00:07:52.360]   And that means that you don't need to model complex motors with their delays and the electromechanical
[00:07:52.360 --> 00:07:55.360]   phenomena that happen in the actuators.
[00:07:55.360 --> 00:07:57.840]   You don't need to model that analytically.
[00:07:57.840 --> 00:08:03.000]   You can just fit a little neural network, a little function approximator to what you
[00:08:03.000 --> 00:08:04.000]   see.
[00:08:04.000 --> 00:08:09.660]   Then you take this empirical actuator model into your simulated legged system.
[00:08:09.660 --> 00:08:14.300]   Then you have the legged system walking or walk around on simulated terrain.
[00:08:14.300 --> 00:08:20.640]   That's where the pleasant surprise comes, which is that we didn't have to model all
[00:08:20.640 --> 00:08:27.920]   the possible behaviors of simulated terrains and all the types of simulated terrains in
[00:08:27.920 --> 00:08:28.920]   simulation.
[00:08:28.920 --> 00:08:30.820]   We didn't have to model vegetation.
[00:08:30.820 --> 00:08:34.080]   We didn't have to model gravel.
[00:08:34.080 --> 00:08:36.540]   We didn't have to model crumbling.
[00:08:36.540 --> 00:08:40.920]   We didn't have to model snow and ice.
[00:08:40.920 --> 00:08:48.980]   Just with a few simple types of terrains and aggressively randomized geometry of these
[00:08:48.980 --> 00:08:55.580]   terrains, we could teach the controller to be incredibly robust.
[00:08:55.580 --> 00:09:01.260]   And the amazing thing that we discovered, which is maybe the most interesting outcome
[00:09:01.260 --> 00:09:07.820]   of this work, is that in the real world, the controller was robust to things it never really
[00:09:07.820 --> 00:09:10.700]   explicitly saw in simulation.
[00:09:10.700 --> 00:09:20.520]   Snow, vegetation, running water, soft, yielding, compliant terrain, sand, things that would
[00:09:20.520 --> 00:09:23.740]   be excruciatingly hard to model.
[00:09:23.740 --> 00:09:26.580]   Turns out we didn't need to model them at all.
[00:09:26.580 --> 00:09:27.580]   That's so cool.
[00:09:27.580 --> 00:09:32.120]   I guess we've talked to a whole bunch of people that work on different types of simulated
[00:09:32.120 --> 00:09:39.180]   data, often just for the cost savings of being able to generate infinite amounts of data.
[00:09:39.180 --> 00:09:45.420]   It seems like if I could summarize what they seem to say, it's that you often benefit from
[00:09:45.420 --> 00:09:48.860]   still a little bit of real world data in addition to the simulated data.
[00:09:48.860 --> 00:09:52.300]   But it sounds like in this case, you didn't actually need it.
[00:09:52.300 --> 00:09:57.540]   Did it literally work the first time you tried it or were there some tweaks that you had
[00:09:57.540 --> 00:10:02.500]   to make to the simulation to actually get it to bridge the gap between simulation and
[00:10:02.500 --> 00:10:03.500]   reality?
[00:10:03.500 --> 00:10:05.660]   It worked shockingly well.
[00:10:05.660 --> 00:10:10.340]   And what helped a lot is that Junho just kept going.
[00:10:10.340 --> 00:10:16.180]   And I love working with young researchers, young engineers, young scientists, because
[00:10:16.180 --> 00:10:19.660]   they do things that would seem crazy to me.
[00:10:19.660 --> 00:10:23.180]   And if you asked me to predict, I would say that's not going to work.
[00:10:23.180 --> 00:10:27.260]   But fortunately, often they don't ask me and they just try things.
[00:10:27.260 --> 00:10:33.500]   And so we would just watch Junho try things out and things kept working.
[00:10:33.500 --> 00:10:42.300]   So the fact that you don't need to model these very complex physical behaviors in the terrain,
[00:10:42.300 --> 00:10:45.140]   in the environment, this is an empirical finding.
[00:10:45.140 --> 00:10:50.100]   We basically discovered this because Junho tried it and it worked.
[00:10:50.100 --> 00:10:57.400]   And then he kept doing it and it kept working and it kept working remarkably well.
[00:10:57.400 --> 00:11:03.660]   So some of it was very good that he didn't ask me and others, "Is this a good idea?
[00:11:03.660 --> 00:11:05.780]   Should I try this?"
[00:11:05.780 --> 00:11:09.500]   It seems like there's these obvious extensions that would be amazingly useful.
[00:11:09.500 --> 00:11:15.580]   Like if you tried to do bipedal locomotion and then making the robot usefully engaging
[00:11:15.580 --> 00:11:20.820]   with its world, where does this line of inquiry get stuck?
[00:11:20.820 --> 00:11:22.300]   It seems so promising.
[00:11:22.300 --> 00:11:27.340]   Yeah, we're definitely pushing this along a number of avenues.
[00:11:27.340 --> 00:11:34.220]   I'm very interested in bipeds and we do have a project with bipeds.
[00:11:34.220 --> 00:11:37.420]   We're also continuing to work with quadrupeds.
[00:11:37.420 --> 00:11:42.180]   We have multiple projects with quadrupeds and we're far from done with quadrupeds.
[00:11:42.180 --> 00:11:44.600]   There's definitely more, there's more to go.
[00:11:44.600 --> 00:11:48.980]   And then you mentioned interaction, you mentioned engaging with the world.
[00:11:48.980 --> 00:11:54.500]   And this is also a very interesting frontier and we have projects like this as well.
[00:11:54.500 --> 00:12:01.380]   So ultimately you want not to just navigate through the world, you also want to interact
[00:12:01.380 --> 00:12:06.260]   with this more deliberately, not just be robust and not fall and get to where you want to
[00:12:06.260 --> 00:12:07.260]   go.
[00:12:07.260 --> 00:12:10.700]   But after you get to where you want to go, you actually want to do something, maybe take
[00:12:10.700 --> 00:12:16.300]   something somewhere else or manipulate the environment in some way.
[00:12:16.300 --> 00:12:17.940]   What physics simulator did you use?
[00:12:17.940 --> 00:12:21.900]   Is this something you built or did you use off the shelf?
[00:12:21.900 --> 00:12:28.340]   This was a custom, this is a custom physics simulator built by Jim In, Jim In Hwangbo,
[00:12:28.340 --> 00:12:31.420]   who led the first stage of that project.
[00:12:31.420 --> 00:12:36.460]   That's why I said, by the way, that it took three years because I'm including the previous
[00:12:36.460 --> 00:12:41.180]   iteration that was done by Jim In that laid a lot of the groundwork and a lot of the systems
[00:12:41.180 --> 00:12:44.340]   infrastructure we ended up using.
[00:12:44.340 --> 00:12:51.260]   So Jim In basically built a physics simulator from scratch to be incredibly, incredibly
[00:12:51.260 --> 00:12:52.460]   efficient.
[00:12:52.460 --> 00:12:57.820]   So it's very easy for these simulation times to get out of hand.
[00:12:57.820 --> 00:13:03.060]   And if you're not careful, you start looking at training times on the order of a week or
[00:13:03.060 --> 00:13:04.660]   more.
[00:13:04.660 --> 00:13:10.600]   And I've seen this happen when people just code in Python and take off the shelf components,
[00:13:10.600 --> 00:13:14.180]   they get hit with so much overhead and so much communication.
[00:13:14.180 --> 00:13:19.860]   And then I tell them that they can get a one or two or three orders of magnitude if they
[00:13:19.860 --> 00:13:20.980]   do it themselves.
[00:13:20.980 --> 00:13:23.620]   And sometimes it's really necessary.
[00:13:23.620 --> 00:13:28.380]   And so our debug cycle was a couple of hours in this project.
[00:13:28.380 --> 00:13:29.580]   So that helped.
[00:13:29.580 --> 00:13:30.580]   That's incredible.
[00:13:31.580 --> 00:13:36.700]   That seems like such an undertaking to build a physics simulator from scratch.
[00:13:36.700 --> 00:13:40.980]   Was it somehow constrained to make it a more tractable problem?
[00:13:40.980 --> 00:13:49.820]   So I think what helped is that Jim In did not build a physics simulator for this project.
[00:13:49.820 --> 00:13:55.040]   It's not that he started this project and then he said, "I need to pause the research
[00:13:55.040 --> 00:13:59.940]   for about a year to build a custom high-performance physics simulator, and then I'll get to do
[00:13:59.940 --> 00:14:01.140]   what I want to do."
[00:14:01.140 --> 00:14:07.100]   He built it up during his PhD, during many prior publications.
[00:14:07.100 --> 00:14:12.160]   And it's a hobby project, just like every self-respecting computer graphics student
[00:14:12.160 --> 00:14:15.500]   has a custom rendering engine that they're maintaining.
[00:14:15.500 --> 00:14:21.820]   So in this area, a number of people have kind of custom physics engines that they're maintaining
[00:14:21.820 --> 00:14:26.260]   just because they're frustrated with anything they get off the shelf, because it's not custom
[00:14:26.260 --> 00:14:27.260]   enough.
[00:14:27.260 --> 00:14:28.460]   It doesn't provide the interfaces they want.
[00:14:28.460 --> 00:14:31.580]   It doesn't provide the customizability that they want.
[00:14:31.580 --> 00:14:36.940]   One of the things you mentioned in the paper, or one of the papers was using privilege learning
[00:14:36.940 --> 00:14:39.180]   as a learning strategy, which is something I hadn't heard of.
[00:14:39.180 --> 00:14:41.140]   Could you describe what that is?
[00:14:41.140 --> 00:14:48.740]   Yeah, it's an incredibly powerful approach that we've been using in multiple projects.
[00:14:48.740 --> 00:14:52.100]   And it splits the training process into two stages.
[00:14:52.100 --> 00:15:00.540]   In the first stage, you train a sensory motor agent that has access to privileged information.
[00:15:00.540 --> 00:15:03.360]   That's usually the ground truth state of the agent.
[00:15:03.360 --> 00:15:07.420]   For example, where it is, exactly what its configuration is.
[00:15:07.420 --> 00:15:12.100]   So for example, for an autonomous car, it would be its absolutely precise ground truth
[00:15:12.100 --> 00:15:15.900]   position in the world, down to the millimeter.
[00:15:15.900 --> 00:15:21.060]   And also the ground truth configuration of the environment.
[00:15:21.060 --> 00:15:24.780]   Everything that matters in the environment, the geometric layout of the environment, the
[00:15:24.780 --> 00:15:31.340]   positions of the other participants, the other agents in the environment, and maybe even
[00:15:31.340 --> 00:15:34.540]   how they're moving and where they're going and why.
[00:15:34.540 --> 00:15:42.140]   So you get this God's eye view into the world, the ground truth configuration of everything.
[00:15:42.140 --> 00:15:44.860]   And this is actually a much easier learning problem.
[00:15:44.860 --> 00:15:52.300]   You basically don't need to learn to perceive the world through incomplete and noisy sensors.
[00:15:52.300 --> 00:15:54.460]   You just need to learn to act.
[00:15:54.460 --> 00:15:59.580]   So the teacher, this first agent, we call it the teacher, the privileged teacher, it
[00:15:59.580 --> 00:16:02.300]   just learns to act.
[00:16:02.300 --> 00:16:06.940]   Then you got this agent, this teacher that always knows what to do.
[00:16:06.940 --> 00:16:11.340]   It always knows how to act very, very effectively.
[00:16:11.340 --> 00:16:17.700]   And then this teacher trains the student that has no access to privileged information.
[00:16:17.700 --> 00:16:22.700]   The student operates only on real sensors that you would have access to in the real
[00:16:22.700 --> 00:16:23.700]   world.
[00:16:23.700 --> 00:16:29.100]   Noisy, incomplete sensors, maybe cameras, IMU, only on board sensors, only on board
[00:16:29.100 --> 00:16:30.860]   computation.
[00:16:30.860 --> 00:16:36.740]   But this teacher can always query, the student can always query the teacher and ask, what
[00:16:36.740 --> 00:16:37.740]   would you do?
[00:16:37.740 --> 00:16:38.820]   What is the right thing to do?
[00:16:38.820 --> 00:16:40.220]   What would you do in this configuration?
[00:16:40.220 --> 00:16:42.020]   What would you do in this configuration?
[00:16:42.020 --> 00:16:47.700]   So the learning process problem is again easier because the student just needs to learn to
[00:16:47.700 --> 00:16:50.020]   perceive the environment.
[00:16:50.020 --> 00:16:55.940]   It essentially has a supervised learning problem now because in any configuration it finds
[00:16:55.940 --> 00:16:59.340]   itself, the teacher can tell it, here is the right thing to do.
[00:16:59.340 --> 00:17:01.420]   Here is the right thing to do.
[00:17:01.420 --> 00:17:05.860]   So the sensory motor learning problem is split into two.
[00:17:05.860 --> 00:17:09.620]   First learning to act without perception being hard.
[00:17:09.620 --> 00:17:14.740]   And second, learning to perceive without action being hard.
[00:17:14.740 --> 00:17:21.700]   Turns out that's much easier than just learning the two together in a bundle.
[00:17:21.700 --> 00:17:22.700]   That's really interesting.
[00:17:22.700 --> 00:17:28.980]   So the way you did the second part of the training, let me make sure I got this.
[00:17:28.980 --> 00:17:35.580]   The second model with the realistic inputs, is it trying to match what the teacher would
[00:17:35.580 --> 00:17:36.580]   have done?
[00:17:36.580 --> 00:17:37.580]   Yeah.
[00:17:37.580 --> 00:17:43.460]   But it doesn't actually try to figure out an intermediate true representation of the
[00:17:43.460 --> 00:17:44.460]   world.
[00:17:44.460 --> 00:17:46.060]   It's just kind of matching the teacher?
[00:17:46.060 --> 00:17:52.260]   Or does it somehow try to actually do that mapping from noisy sensors to real world state?
[00:17:52.260 --> 00:17:53.260]   Right.
[00:17:53.260 --> 00:17:56.900]   It doesn't need to reconstruct the real world state.
[00:17:56.900 --> 00:18:01.260]   So there are different architectures we can imagine with different intermediate representations,
[00:18:01.260 --> 00:18:06.580]   but the simplest instantiation of this approach is that you just have a network that maps
[00:18:06.580 --> 00:18:08.980]   sensory input to action.
[00:18:08.980 --> 00:18:15.220]   And then this network is just trained in a supervised fashion by the actions that the
[00:18:15.220 --> 00:18:16.220]   teacher produces.
[00:18:16.220 --> 00:18:17.220]   I see.
[00:18:17.220 --> 00:18:18.220]   Cool.
[00:18:18.220 --> 00:18:19.220]   Okay.
[00:18:19.220 --> 00:18:23.580]   So I'm really just cherry picking your papers that just seem kind of awesome to me.
[00:18:23.580 --> 00:18:29.900]   But I was also pretty impressed by your paper where you taught drones to do like crazy acrobatics.
[00:18:29.900 --> 00:18:32.260]   Do you know what I'm talking about?
[00:18:32.260 --> 00:18:33.260]   Oh, cool.
[00:18:33.260 --> 00:18:34.260]   Yeah.
[00:18:34.260 --> 00:18:40.380]   So you talk about the simulation in that one, and it seemed like it must be really hard
[00:18:40.380 --> 00:18:47.340]   to simulate what actually happens to a drone as it kind of flies in crazy ways.
[00:18:47.340 --> 00:18:51.900]   I mean, I'm not sure, but it just seems so stochastic to me, just like watching a drone.
[00:18:51.900 --> 00:18:53.020]   It's so hard to control a drone.
[00:18:53.020 --> 00:18:54.900]   I was actually wondering if that...
[00:18:54.900 --> 00:18:58.660]   It seems like it must have been a real simulation challenge to actually make that work.
[00:18:58.660 --> 00:19:02.020]   Also, we should put a link to the videos because they're super cool.
[00:19:02.020 --> 00:19:03.020]   Yeah.
[00:19:03.020 --> 00:19:09.940]   This was an amazing project driven again by amazing students from the University of Zurich,
[00:19:09.940 --> 00:19:12.220]   Antonio Locarcio.
[00:19:12.220 --> 00:19:17.020]   First we benefited from some infrastructure that the quadrotor community has, which is
[00:19:17.020 --> 00:19:19.860]   they have good quadrotor simulators.
[00:19:19.860 --> 00:19:24.100]   They have good models for the dynamics of quadrotors.
[00:19:24.100 --> 00:19:31.100]   We also benefited from some luck, which is that not everything that can happen to a quadrotor
[00:19:31.100 --> 00:19:34.540]   needs to be simulated to get a good quadrotor control.
[00:19:34.540 --> 00:19:40.780]   So for example, we did not simulate aerodynamic effects, which are very hard to simulate.
[00:19:40.780 --> 00:19:49.060]   So if a quadrotor goes close to a wall, it then gets aerodynamic pushback.
[00:19:49.060 --> 00:19:51.460]   It gets really, really hairy.
[00:19:51.460 --> 00:19:57.620]   We did not simulate that and turns out we didn't need to because the neural network
[00:19:57.620 --> 00:20:01.140]   makes decisions moment to moment, moment to moment.
[00:20:01.140 --> 00:20:06.300]   And if it gets a bit off track, if it's thrown around, no problem.
[00:20:06.300 --> 00:20:11.060]   In the very next moment, it adjusts to the state that it finds itself in.
[00:20:11.060 --> 00:20:14.200]   So this is closed loop control.
[00:20:14.200 --> 00:20:17.860]   If it was open loop control, well, it would have failed.
[00:20:17.860 --> 00:20:18.860]   I see.
[00:20:18.860 --> 00:20:19.860]   Interesting.
[00:20:19.860 --> 00:20:24.740]   So were there any other details that you had to get right to make that work?
[00:20:24.740 --> 00:20:26.300]   I'm really impressed the way you're...
[00:20:26.300 --> 00:20:30.580]   It seems like you're effortlessly able to jump from simulation to reality.
[00:20:30.580 --> 00:20:34.500]   And everyone else that I talked to is like, "This is the most impossible step."
[00:20:34.500 --> 00:20:39.880]   But something about these domains or something you're doing, it seems to work really effectively
[00:20:39.880 --> 00:20:40.880]   for you.
[00:20:40.880 --> 00:20:41.880]   Yeah.
[00:20:41.880 --> 00:20:42.880]   Yeah.
[00:20:42.880 --> 00:20:45.860]   So we're getting a hang of this.
[00:20:45.860 --> 00:20:51.780]   And there are a few key ideas that have served us well.
[00:20:51.780 --> 00:20:54.560]   One key idea is abstraction.
[00:20:54.560 --> 00:20:57.140]   So abstraction is really, really key.
[00:20:57.140 --> 00:21:05.180]   The more abstract the representation that a sensor or a sensory modality produces, the
[00:21:05.180 --> 00:21:09.740]   easier it is to transfer from simulation to reality.
[00:21:09.740 --> 00:21:13.380]   So when you mean abstract, can you give me an example of what would be abstract versus
[00:21:13.380 --> 00:21:14.380]   not abstract?
[00:21:14.380 --> 00:21:15.380]   Yeah.
[00:21:15.380 --> 00:21:18.060]   So let's look at three points on the abstraction spectrum.
[00:21:18.060 --> 00:21:22.460]   Point number one, a regular camera, like the camera that is pointing at you now and the
[00:21:22.460 --> 00:21:25.020]   camera that is pointing at me now.
[00:21:25.020 --> 00:21:26.020]   Point number one.
[00:21:26.020 --> 00:21:30.200]   Point number two, a depth map coming out of a stereo camera.
[00:21:30.200 --> 00:21:31.340]   So we have a stereo camera.
[00:21:31.340 --> 00:21:32.340]   It's a real sensor.
[00:21:32.340 --> 00:21:33.340]   It really exists.
[00:21:33.340 --> 00:21:35.380]   It produces a depth map.
[00:21:35.380 --> 00:21:37.100]   Let's look at that depth map.
[00:21:37.100 --> 00:21:44.420]   Point number three, sparse feature tracks that a feature extractor like SIFT would produce.
[00:21:44.420 --> 00:21:50.740]   So just very salient points in the image and just a few points that are being tracked through
[00:21:50.740 --> 00:21:51.740]   time.
[00:21:51.740 --> 00:21:53.380]   So you're getting just a dot.
[00:21:53.380 --> 00:21:58.300]   So the depth map is more abstract than the color.
[00:21:58.300 --> 00:21:59.700]   Why is that?
[00:21:59.700 --> 00:22:04.900]   Because there are degrees of variability that would affect the color image that the depth
[00:22:04.900 --> 00:22:06.500]   map is invariant to.
[00:22:06.500 --> 00:22:14.060]   The color of that rack behind you would massively affect the color image, but would not affect
[00:22:14.060 --> 00:22:15.740]   the depth map.
[00:22:15.740 --> 00:22:16.740]   Is it sunny?
[00:22:16.740 --> 00:22:17.820]   Is it dark?
[00:22:17.820 --> 00:22:22.140]   Are you now at night with your environment lit by lamps?
[00:22:22.140 --> 00:22:26.860]   All of that affects the color image and it's brutally hard to simulate.
[00:22:26.860 --> 00:22:32.840]   And it's brutally hard to nail the appearance so that the simulated appearance matches the
[00:22:32.840 --> 00:22:39.100]   statistics of the real appearance because we're just not very good at modeling the reflectance
[00:22:39.100 --> 00:22:40.580]   of real objects.
[00:22:40.580 --> 00:22:44.940]   We're not good at dealing with translucency, refraction.
[00:22:44.940 --> 00:22:50.140]   We're still not so great at simulating light transport.
[00:22:50.140 --> 00:22:56.560]   So all these things that determine the appearance of the color image, very, very hard to simulate.
[00:22:56.560 --> 00:22:59.660]   The depth map is invariant to all of that.
[00:22:59.660 --> 00:23:05.280]   It gives you primarily a reading of the geometric layout of the environment.
[00:23:05.280 --> 00:23:10.840]   So if you have a policy that operates on depth maps, it will transfer much more easily from
[00:23:10.840 --> 00:23:17.800]   simulation to reality because things that we are not good at simulating, like the actual
[00:23:17.800 --> 00:23:22.720]   appearance of objects, they don't affect the depth map.
[00:23:22.720 --> 00:23:27.520]   And then if you take something even more abstract, let's say you run a feature extractor, a sparse
[00:23:27.520 --> 00:23:33.240]   feature tracker, through time the video will just be a collection of points, like a moving
[00:23:33.240 --> 00:23:37.320]   dot map, a moving point display.
[00:23:37.320 --> 00:23:43.560]   It actually still gives you a lot of information about the content of the environment, but
[00:23:43.560 --> 00:23:45.600]   now it's invariant to much more.
[00:23:45.600 --> 00:23:51.040]   It's invariant also to geometric details and quite a lot of the content of the environment.
[00:23:51.040 --> 00:23:55.800]   So maybe you don't even have to get the geometry of the environment and the detailed content
[00:23:55.800 --> 00:23:58.280]   of the environment right either.
[00:23:58.280 --> 00:24:00.480]   So now that's even more abstract.
[00:24:00.480 --> 00:24:07.400]   And that last representation is the representation that we used in the DeepDrone Gravatics project.
[00:24:07.400 --> 00:24:14.320]   So the drone, even though it has a camera and it could look at the color image, it deliberately
[00:24:14.320 --> 00:24:16.200]   does it.
[00:24:16.200 --> 00:24:22.320]   It deliberately abstracts away all the appearance and the geometric detail and just operates
[00:24:22.320 --> 00:24:24.560]   on sparse feature tracks.
[00:24:24.560 --> 00:24:32.520]   And it turns out that we could train that policy with that sensory input in very simple
[00:24:32.520 --> 00:24:38.160]   simulated environments and they would just work out of the box in the real world.
[00:24:38.160 --> 00:24:39.680]   Well, that's so interesting.
[00:24:39.680 --> 00:24:43.800]   It makes me wonder, people that we've talked to have talked about sort of end to end learning
[00:24:43.800 --> 00:24:47.440]   with autonomous vehicles versus pieces.
[00:24:47.440 --> 00:24:52.360]   And I guess I would never consider that if you kind of break it up more, have more intermediate
[00:24:52.360 --> 00:24:56.640]   representations, it might make simulation easier, transferring from simulation to the
[00:24:56.640 --> 00:24:57.640]   real world.
[00:24:57.640 --> 00:24:59.840]   But that actually makes total sense.
[00:24:59.840 --> 00:25:00.840]   Yeah.
[00:25:00.840 --> 00:25:09.720]   So I think for example, the output of a lighter is easier to simulate than the original environment
[00:25:09.720 --> 00:25:11.200]   that gave rise to that output.
[00:25:11.200 --> 00:25:16.040]   So if you look at the output of a lighter, it's a pretty sparse point set.
[00:25:16.040 --> 00:25:21.080]   If you train a policy that operates on this sparse point set, maybe you don't need a very
[00:25:21.080 --> 00:25:25.080]   detailed super high fidelity model of the environment.
[00:25:25.080 --> 00:25:30.400]   Certainly maybe not of its appearance because you don't really see that appearance reflected
[00:25:30.400 --> 00:25:33.400]   much in the lighter reading.
[00:25:33.400 --> 00:25:34.400]   Interesting.
[00:25:34.400 --> 00:25:39.640]   I guess I also wanted to ask you about another piece of work that you did that was intriguing,
[00:25:39.640 --> 00:25:43.760]   which is this simple factory paper where you have kind of a setup to train things much
[00:25:43.760 --> 00:25:45.440]   faster.
[00:25:45.440 --> 00:25:50.080]   And I have to confess, this is what I kind of struggled to understand what you were doing.
[00:25:50.080 --> 00:25:54.120]   So I would love just kind of a high level explanation.
[00:25:54.120 --> 00:25:56.320]   I'm not a reinforcement learning expert at all.
[00:25:56.320 --> 00:26:00.920]   So maybe kind of set up what the problem is and kind of what your contribution is that
[00:26:00.920 --> 00:26:03.160]   made these things run so much faster.
[00:26:03.160 --> 00:26:04.240]   Yeah.
[00:26:04.240 --> 00:26:11.040]   So our goal is to see how far we can push the throughput of sensory motor learning systems
[00:26:11.040 --> 00:26:12.040]   in simulation.
[00:26:12.040 --> 00:26:20.200]   We're particularly interested in sensory motor learning in immersive three-dimensional environments.
[00:26:20.200 --> 00:26:27.840]   I'm personally a bit less jazzed by environments such as board games or even Atari because
[00:26:27.840 --> 00:26:30.360]   it's still quite far from the real world.
[00:26:30.360 --> 00:26:33.640]   Although you have done a fair amount of work on it, haven't you?
[00:26:33.640 --> 00:26:36.360]   So you're not saying it from someone...
[00:26:36.360 --> 00:26:37.360]   Right.
[00:26:37.360 --> 00:26:46.880]   Well, we've done some, but what really excites me very deeply is training systems that work
[00:26:46.880 --> 00:26:51.560]   in immersive 3D environments, because that to me is the big prize.
[00:26:51.560 --> 00:26:57.440]   If we do that really, really well, that brings us closer to deploying systems in the physical
[00:26:57.440 --> 00:26:58.440]   world.
[00:26:58.440 --> 00:27:00.400]   The physical world is three-dimensional.
[00:27:00.400 --> 00:27:05.640]   The physical world is immersive, perceived from a first person view on board sensing
[00:27:05.640 --> 00:27:09.400]   and computation by animals, including humans.
[00:27:09.400 --> 00:27:15.480]   And these are the kinds of systems that I would love to be able to create.
[00:27:15.480 --> 00:27:18.960]   So that's where we try to go in our simulated environments.
[00:27:18.960 --> 00:27:23.360]   And these simulated environments tend to be, if you're not careful, they're pretty computationally
[00:27:23.360 --> 00:27:24.360]   intensive.
[00:27:24.360 --> 00:27:31.120]   And if you just use, again, if you use out of the box systems, you will notice a pattern
[00:27:31.120 --> 00:27:32.120]   here.
[00:27:32.120 --> 00:27:37.160]   If you use tools out of the box and have some high level Python scripting on top of existing
[00:27:37.160 --> 00:27:41.960]   tools, you'll basically have a simulation environment that runs at 30 frames per second,
[00:27:41.960 --> 00:27:45.160]   maybe at maybe 60 frames per second.
[00:27:45.160 --> 00:27:50.120]   You're roughly collecting experience in something that corresponds to real time.
[00:27:50.120 --> 00:27:57.560]   Now, as we mentioned, it takes a human toddler a couple of years of experience to learn to
[00:27:57.560 --> 00:27:58.560]   walk.
[00:27:58.560 --> 00:28:04.480]   And a human toddler is a much better learner, a much more effective learner than any system
[00:28:04.480 --> 00:28:05.480]   we have right now.
[00:28:05.480 --> 00:28:11.080]   So two years is a bit slow if you ask me for a debug cycle.
[00:28:11.080 --> 00:28:14.720]   I don't want to have a debug cycle of two years.
[00:28:14.720 --> 00:28:21.520]   And in fact, what we need to do is take this amount of experience and then multiply it
[00:28:21.520 --> 00:28:28.120]   by several orders of magnitude because the models that we're training are much more data
[00:28:28.120 --> 00:28:33.760]   hungry and they're much poorer learners than the human toddler.
[00:28:33.760 --> 00:28:40.160]   So then basically we're looking at compressing maybe centuries of experience until we get
[00:28:40.160 --> 00:28:43.400]   better at learning algorithms and the models we design.
[00:28:43.400 --> 00:28:47.520]   But with the current models and algorithms, the challenge is to compress perhaps centuries
[00:28:47.520 --> 00:28:53.960]   of experience into overnight, an overnight training run, which is a reasonably comfortable
[00:28:53.960 --> 00:28:54.960]   debug cycle.
[00:28:54.960 --> 00:28:58.560]   You go to lunch or run, you go home, you come back in the morning, you have experimental
[00:28:58.560 --> 00:28:59.560]   results.
[00:28:59.560 --> 00:29:06.080]   That basically means that you need to operate, you need to collect experience and use it
[00:29:06.080 --> 00:29:12.240]   for learning on the orders of hundreds of thousands of frames per second, millions of
[00:29:12.240 --> 00:29:14.040]   frames per second.
[00:29:14.040 --> 00:29:15.800]   And this is where we're driving.
[00:29:15.800 --> 00:29:22.680]   So in this paper, we demonstrated a system architecture that in an immersive environment,
[00:29:22.680 --> 00:29:29.680]   trains agents that act, collect experience and learn in these 3D immersive environments
[00:29:29.680 --> 00:29:38.960]   at on the order of a hundred thousand frames per second on a single machine, single server.
[00:29:38.960 --> 00:29:46.480]   And the key was basically a bottom up from scratch, from first principles system design
[00:29:46.480 --> 00:29:48.920]   with a lot of specialization.
[00:29:48.920 --> 00:29:56.160]   So we have processes that just collect experience, agents just run nonstop, collect experience.
[00:29:56.160 --> 00:30:02.240]   We have other processes that just learn and update the neural network weights.
[00:30:02.240 --> 00:30:07.640]   So it's not that you get an, you have an agent that goes out, collect experience, then does
[00:30:07.640 --> 00:30:12.920]   some gradient descent step, steps, updates its weights, goes back into the environment,
[00:30:12.920 --> 00:30:17.040]   collects some more experience with better weights and so on and so forth.
[00:30:17.040 --> 00:30:19.080]   Everything happens in parallel.
[00:30:19.080 --> 00:30:26.600]   Everybody is busy all the time and the resources are utilized at very, very close to a hundred
[00:30:26.600 --> 00:30:29.200]   percent utilization.
[00:30:29.200 --> 00:30:32.240]   Everything is connected through a high bandwidth memory.
[00:30:32.240 --> 00:30:33.620]   Everything is on the same node.
[00:30:33.620 --> 00:30:38.480]   So there is no message passing, because if you look at these rates of operation, if you're
[00:30:38.480 --> 00:30:43.400]   operating at hundreds of thousands of frames per second, message passing is too slow.
[00:30:43.400 --> 00:30:47.560]   The fastest message passing protocol you can find is too slow.
[00:30:47.560 --> 00:30:52.040]   It would become, the message passing becomes the bottleneck in the system.
[00:30:52.040 --> 00:30:56.160]   So what happens is that these processes just read and write from shared memory.
[00:30:56.160 --> 00:30:58.520]   They just all access the same memory buffers.
[00:30:58.520 --> 00:31:03.120]   When the new neural network weights are ready, they're written into the memory buffer.
[00:31:03.120 --> 00:31:07.760]   When a new agent is ready to go out, collect experience, it just reads the latest weights
[00:31:07.760 --> 00:31:09.440]   from the memory buffer.
[00:31:09.440 --> 00:31:16.720]   And there is a cute idea that we borrowed from computer graphics, which is double buffering.
[00:31:16.720 --> 00:31:21.740]   And double buffering is one of the very, very first things I learned in computer graphics
[00:31:21.740 --> 00:31:23.320]   as a teenager.
[00:31:23.320 --> 00:31:28.800]   We wrote assembly code and basically, you know, lesson one in computer graphics, how
[00:31:28.800 --> 00:31:32.300]   do you display, how do you even display an image?
[00:31:32.300 --> 00:31:35.040]   Double buffer is part of, is part of lesson one.
[00:31:35.040 --> 00:31:40.200]   The idea is that there are two buffers, the display points to the front buffer, and that's
[00:31:40.200 --> 00:31:41.380]   what's being displayed.
[00:31:41.380 --> 00:31:42.840]   That's the active buffer.
[00:31:42.840 --> 00:31:48.320]   In the meantime, the logic of your code is updating the back buffer with the image of
[00:31:48.320 --> 00:31:49.560]   the next frame.
[00:31:49.560 --> 00:31:53.340]   When the back buffer is ready, you just swap pointers.
[00:31:53.340 --> 00:31:59.640]   So the display points to the, starts pointing to the back buffer, that becomes the primary
[00:31:59.640 --> 00:32:05.800]   one, and then the logic of your code operates on what used to be the front buffer.
[00:32:05.800 --> 00:32:09.800]   So the back buffer becomes the front buffer, the front buffer becomes the back buffer,
[00:32:09.800 --> 00:32:10.980]   you keep going.
[00:32:10.980 --> 00:32:17.900]   We introduced this idea into reinforcement learning, again, to just keep everybody busy
[00:32:17.900 --> 00:32:19.320]   all the time.
[00:32:19.320 --> 00:32:29.800]   So the learning processes work on a buffer and write out the new weights, and the experience
[00:32:29.800 --> 00:32:36.120]   collectors have their own, their own buffer that they're writing out sensory data into,
[00:32:36.120 --> 00:32:41.240]   and then they swap buffers, there's no delay, and they just keep going.
[00:32:41.240 --> 00:32:43.320]   Interesting.
[00:32:43.320 --> 00:32:48.680]   Would it be possible to scale this up if there were multiple machines and there was a delay
[00:32:48.680 --> 00:32:49.960]   in the message passing?
[00:32:49.960 --> 00:32:54.200]   So the distributed setting is more complex.
[00:32:54.200 --> 00:32:56.480]   We have avoided it so far.
[00:32:56.480 --> 00:33:03.280]   If you are connected over a high speed fabric, then it should be possible.
[00:33:03.280 --> 00:33:12.160]   We've deliberately maybe handicapped ourselves still, even in a followup project that we
[00:33:12.160 --> 00:33:15.920]   have now that was accepted to iClear.
[00:33:15.920 --> 00:33:21.960]   We limited ourselves to a single node because we felt that we will learn useful things if
[00:33:21.960 --> 00:33:29.640]   we just constrain ourselves to a single node and ask how far can we push single node performance.
[00:33:29.640 --> 00:33:34.920]   And in this latest paper that was just accepted to iClear, we basically showed that with a
[00:33:34.920 --> 00:33:42.560]   single node, if we again take this holistic end-to-end from first principle system design
[00:33:42.560 --> 00:33:50.200]   philosophy, we can match results that previously were obtained on an absolutely massive industrial
[00:33:50.200 --> 00:33:53.400]   scale cluster.
[00:33:53.400 --> 00:33:56.920]   Your learning speed is so fast to me.
[00:33:56.920 --> 00:34:01.280]   It seems faster than actually what I would expect from supervised learning, where you're
[00:34:01.280 --> 00:34:05.200]   literally just pulling the images off your hard drive.
[00:34:05.200 --> 00:34:07.000]   Am I wrong about that?
[00:34:07.000 --> 00:34:08.080]   Oh, yeah.
[00:34:08.080 --> 00:34:14.400]   So in the latest work, it's basically the forward pass through the ConvNet is one of
[00:34:14.400 --> 00:34:15.400]   the big bottlenecks.
[00:34:15.400 --> 00:34:17.120]   It's no longer the simulation.
[00:34:17.120 --> 00:34:19.720]   We can simulate so fast.
[00:34:19.720 --> 00:34:22.520]   We can simulate the environment so fast.
[00:34:22.520 --> 00:34:24.160]   It's no longer the bottleneck.
[00:34:24.160 --> 00:34:29.120]   It's actually like routine processing, like even just doing the forward pass in the ConvNet.
[00:34:29.120 --> 00:34:30.120]   Amazing.
[00:34:30.120 --> 00:34:34.200]   So I guess one more project that you worked on that I was kind of captivated by, I kind
[00:34:34.200 --> 00:34:37.880]   of wanted to ask about, because I think a lot of people that watch these interviews
[00:34:37.880 --> 00:34:39.960]   would be interested in it too, is Carla, right?
[00:34:39.960 --> 00:34:42.960]   Which is like kind of an environment for learning autonomous vehicle stuff.
[00:34:42.960 --> 00:34:46.040]   Could you maybe describe it and what inspired you to make it?
[00:34:46.040 --> 00:34:47.040]   Yeah.
[00:34:47.040 --> 00:34:54.000]   Carla is a simulator for autonomous driving and has grown into an extensive open source
[00:34:54.000 --> 00:34:59.520]   simulation platform for autonomous driving that's now widely used both in industry and
[00:34:59.520 --> 00:35:01.640]   in research.
[00:35:01.640 --> 00:35:03.960]   And I can answer your question about inspiration.
[00:35:03.960 --> 00:35:05.120]   I think in two parts.
[00:35:05.120 --> 00:35:10.640]   There is what originally inspired us to create Carla.
[00:35:10.640 --> 00:35:13.520]   And then there is what keeps it going.
[00:35:13.520 --> 00:35:19.920]   And so what originally inspired us is actually basic scientific interest in sensory motor
[00:35:19.920 --> 00:35:22.240]   learning and sensory motor control.
[00:35:22.240 --> 00:35:29.800]   I wanted to understand how we train intelligent agents that have this kind of embodied intelligence
[00:35:29.800 --> 00:35:36.560]   that you see in us and other animals, where we can walk through an environment gracefully,
[00:35:36.560 --> 00:35:37.640]   deliberately.
[00:35:37.640 --> 00:35:40.380]   We can get to where we want to go.
[00:35:40.380 --> 00:35:43.640]   We can engage with the environment if we need to rearrange it.
[00:35:43.640 --> 00:35:45.400]   We rearrange it.
[00:35:45.400 --> 00:35:53.000]   We clearly act spatially intelligently, intelligently in an embodied fashion.
[00:35:53.000 --> 00:35:55.820]   And this seems very core to me.
[00:35:55.820 --> 00:36:01.020]   And I want to understand it because I think this underlies other kinds of intelligence
[00:36:01.020 --> 00:36:02.080]   as well.
[00:36:02.080 --> 00:36:07.320]   And I think it's important for us on our way to AI, to use the loaded term.
[00:36:07.320 --> 00:36:11.640]   I think it's very important for us to understand this aspect of intelligence.
[00:36:11.640 --> 00:36:17.040]   It seems very core to me, the kinds of internal representations that we maintain and how we
[00:36:17.040 --> 00:36:22.440]   maintain them as we move through immersive three-dimensional environments.
[00:36:22.440 --> 00:36:24.160]   So I wanted to study this.
[00:36:24.160 --> 00:36:27.680]   I wanted to study this in a reproducible fashion.
[00:36:27.680 --> 00:36:29.960]   I wanted good tooling.
[00:36:29.960 --> 00:36:34.480]   I wanted good environments in which this can be studied.
[00:36:34.480 --> 00:36:41.240]   And we looked around and when we started this work, there just weren't very good, very satisfactory
[00:36:41.240 --> 00:36:42.840]   environments for us.
[00:36:42.840 --> 00:36:48.200]   We ended up in some early projects, we ended up using the game Doom, which is a first-person
[00:36:48.200 --> 00:36:57.280]   shooter that I used to play as a teenager and I still have a warm spot for.
[00:36:57.280 --> 00:37:00.800]   And we used Doom and we used it to a good effect.
[00:37:00.800 --> 00:37:03.280]   And in fact, we still use it in projects.
[00:37:03.280 --> 00:37:07.120]   And we used it in the Sample Factory paper as well.
[00:37:07.120 --> 00:37:12.160]   I mean, Sample Factory is another paper that is based on Doom, essentially on derivatives
[00:37:12.160 --> 00:37:18.280]   of John Carmack's old code, which tells you something about the guy.
[00:37:18.280 --> 00:37:26.360]   So if people still use your code 25 years later, you did something good.
[00:37:26.360 --> 00:37:32.680]   But Doom, if you just look at it, it somehow is less than ideal.
[00:37:32.680 --> 00:37:41.000]   You walk around in a dungeon and you engage in assertive diplomacy of the kind that maybe
[00:37:41.000 --> 00:37:48.400]   we don't want to always look at and we don't want our graduate students to always be confronted
[00:37:48.400 --> 00:37:49.400]   with.
[00:37:49.400 --> 00:37:55.160]   I mean, there's a lot of blood and gore and somehow it wasn't designed for AI.
[00:37:55.160 --> 00:37:59.960]   It was designed for the entertainment of, I guess, primarily teenage boys.
[00:37:59.960 --> 00:38:08.920]   So we wanted something a bit more modern and that connects more directly to the kinds of
[00:38:08.920 --> 00:38:14.840]   applications that we have in mind, to useful, productive behaviors that we want our intelligent
[00:38:14.840 --> 00:38:16.760]   systems to learn.
[00:38:16.760 --> 00:38:21.760]   And autonomous driving was clearly one such behavior.
[00:38:21.760 --> 00:38:26.560]   And I held the view at the time that I still hold that autonomous driving is a long-term
[00:38:26.560 --> 00:38:27.560]   problem.
[00:38:27.560 --> 00:38:29.080]   It's a long-term game.
[00:38:29.080 --> 00:38:35.760]   It wasn't about to be solved, as people were saying when we were creating Carla.
[00:38:35.760 --> 00:38:39.320]   And I still don't think that it's about to be solved.
[00:38:39.320 --> 00:38:42.400]   I think it's a long-term effort.
[00:38:42.400 --> 00:38:47.760]   So we created a simulation platform where the task is autonomous driving.
[00:38:47.760 --> 00:38:54.400]   And as an embodied artificial intelligence task, as an embodied artificial intelligence
[00:38:54.400 --> 00:38:55.720]   domain, I think it's a great domain.
[00:38:55.720 --> 00:38:57.200]   You have a complex environment.
[00:38:57.200 --> 00:39:00.320]   You need to navigate through it.
[00:39:00.320 --> 00:39:05.960]   You need to perceive the environment, make decisions in real time.
[00:39:05.960 --> 00:39:07.680]   The decisions really matter.
[00:39:07.680 --> 00:39:10.520]   If you get something wrong, it's really bad.
[00:39:10.520 --> 00:39:14.000]   So the stakes are high, but you're in simulation.
[00:39:14.000 --> 00:39:16.600]   So that was the original motivation.
[00:39:16.600 --> 00:39:23.440]   It was basic scientific interest in intelligence and how to develop intelligence.
[00:39:23.440 --> 00:39:26.580]   And then the platform became very widely used.
[00:39:26.580 --> 00:39:27.580]   People wanted it.
[00:39:27.580 --> 00:39:32.480]   People wanted it for the engineering task of autonomous driving.
[00:39:32.480 --> 00:39:37.220]   People kept asking for more and more and more and more features, more and more functionality.
[00:39:37.220 --> 00:39:43.120]   Other large institutions, like actual automotive companies, started providing funding for this
[00:39:43.120 --> 00:39:47.040]   platform to be maintained and developed because they wanted it.
[00:39:47.040 --> 00:39:49.080]   And we put together a team.
[00:39:49.080 --> 00:39:55.120]   The team is ably led by Germn Ross, one of the original developers of Carla, who is
[00:39:55.120 --> 00:40:01.840]   now leading an extensive international team that is really primarily devoted to the autonomous
[00:40:01.840 --> 00:40:08.000]   driving domain and supporting the autonomous driving domain through Carla.
[00:40:08.000 --> 00:40:09.000]   That's so cool.
[00:40:09.000 --> 00:40:13.680]   I feel like, or maybe one criticism of academia, I don't know if it's fair or not, is that
[00:40:13.680 --> 00:40:18.400]   it has trouble with incentives to make tools like this that are really reusable.
[00:40:18.400 --> 00:40:23.840]   Did you feel pressure to write papers instead of building a robust simulating tool that
[00:40:23.840 --> 00:40:26.160]   would be useful for lots of other people?
[00:40:26.160 --> 00:40:35.160]   Well, I maintain a portfolio approach where I think it's okay for one thrust of my research
[00:40:35.160 --> 00:40:41.360]   and one thrust of my lab to not yield a publication for a long time because other thrusts just
[00:40:41.360 --> 00:40:46.120]   very naturally end up publishing more.
[00:40:46.120 --> 00:40:49.200]   So it balances out.
[00:40:49.200 --> 00:40:50.480]   It balances out.
[00:40:50.480 --> 00:40:57.240]   I personally don't see publication as a product or as a goal.
[00:40:57.240 --> 00:40:59.800]   I see publication as a symptom.
[00:40:59.800 --> 00:41:02.060]   Publication is a symptom of having something to say.
[00:41:02.060 --> 00:41:04.480]   So publications come out.
[00:41:04.480 --> 00:41:10.280]   They come out at a healthy rate just because we end up discovering useful things that we
[00:41:10.280 --> 00:41:13.600]   want to share with people.
[00:41:13.600 --> 00:41:20.680]   And I personally find it very gratifying to work on a project for a long time and do something
[00:41:20.680 --> 00:41:24.560]   substantial, maybe then publish.
[00:41:24.560 --> 00:41:31.560]   And if people use our work and it's useful to them, that is its own reward to me.
[00:41:31.560 --> 00:41:37.440]   So even if there is no publication, if people find our work useful, I love it.
[00:41:37.440 --> 00:41:40.080]   I find it very, very gratifying.
[00:41:40.080 --> 00:41:43.280]   Yeah, I can totally relate to that.
[00:41:43.280 --> 00:41:46.640]   Could I ask you a more open-ended question since we're kind of getting to the end of
[00:41:46.640 --> 00:41:47.640]   this?
[00:41:47.640 --> 00:41:54.880]   I guess I wonder, when I look at ML applications, I guess broadly defined ML, the one that is
[00:41:54.880 --> 00:41:58.040]   kind of mysterious to me is robotics.
[00:41:58.040 --> 00:42:03.480]   I feel like I see ML working all over the place.
[00:42:03.480 --> 00:42:05.160]   It's just so easy to find.
[00:42:05.160 --> 00:42:08.320]   Suddenly my camera can search semantically.
[00:42:08.320 --> 00:42:17.200]   I feel like the thing that I can do that computers most can't do is kind of pick up an arbitrary
[00:42:17.200 --> 00:42:19.000]   object and move it somewhere.
[00:42:19.000 --> 00:42:24.960]   And it seems like you've been really successful getting these things to work to some degree.
[00:42:24.960 --> 00:42:29.760]   But I guess I always wonder, what is so hard about robotics?
[00:42:29.760 --> 00:42:35.280]   Do you think there'll be a moment where it suddenly starts working and we see ML robot
[00:42:35.280 --> 00:42:42.800]   applications all over the place, or is this always going to remain a huge challenge?
[00:42:42.800 --> 00:42:46.400]   I don't think it will always remain a huge challenge.
[00:42:46.400 --> 00:42:49.480]   I don't think there is magic here.
[00:42:49.480 --> 00:42:55.320]   The problem is qualitatively different from pure perception problems, such as computer
[00:42:55.320 --> 00:43:01.880]   vision and being able to tell your camera, where is Lucas?
[00:43:01.880 --> 00:43:04.880]   And the camera will find Lucas.
[00:43:04.880 --> 00:43:08.520]   The problem is qualitatively different, but I don't think the problem is insurmountable.
[00:43:08.520 --> 00:43:12.360]   And I think we're making good progress.
[00:43:12.360 --> 00:43:16.440]   So the challenge is that to learn to act, you need to actually act.
[00:43:16.440 --> 00:43:18.240]   To act, you need to act in an environment.
[00:43:18.240 --> 00:43:20.800]   You need to act in a living environment.
[00:43:20.800 --> 00:43:25.120]   If you act in a physical environment, you have a problem because the physical environment
[00:43:25.120 --> 00:43:27.120]   runs in real time.
[00:43:27.120 --> 00:43:31.720]   So you're potentially looking at the kinds of debug cycles that we mentioned with a human
[00:43:31.720 --> 00:43:34.400]   toddler where something takes a couple of years to learn.
[00:43:34.400 --> 00:43:39.280]   And in these couple of years, I mean, the toddler is also an incredibly robust system.
[00:43:39.280 --> 00:43:42.200]   The toddler can fall, fall no problem.
[00:43:42.200 --> 00:43:50.280]   So during this time, you run out of battery power, you fall, you break things, you need
[00:43:50.280 --> 00:43:54.360]   a physical space in which all of this happens.
[00:43:54.360 --> 00:44:01.240]   And then if you're designing the outer learning algorithms, you need to do this in parallel
[00:44:01.240 --> 00:44:03.360]   on many, many, many, many variations.
[00:44:03.360 --> 00:44:08.120]   You need many, many, many, many slightly different toddlers to see which one learns better.
[00:44:08.120 --> 00:44:12.200]   So it's very, very hard to make progress in this regime.
[00:44:12.200 --> 00:44:19.920]   So I think we need to identify the essential skills, the underlying skills that, and I
[00:44:19.920 --> 00:44:28.800]   think many of these can be understood and modeled in essentially our equivalent of model
[00:44:28.800 --> 00:44:29.800]   systems.
[00:44:30.520 --> 00:44:35.960]   So if you look at neuroscience, for example, much of what we know about the nervous system
[00:44:35.960 --> 00:44:40.960]   was not discovered in humans, in the human nervous systems.
[00:44:40.960 --> 00:44:47.400]   It was discovered in model systems such as squids.
[00:44:47.400 --> 00:44:52.720]   So a squid is pretty different from a human, but it shares some essential aspects when
[00:44:52.720 --> 00:44:57.040]   it comes to the operation of the nervous system.
[00:44:57.040 --> 00:45:01.240]   And it's easier to work with, okay, for very many reasons.
[00:45:01.240 --> 00:45:05.320]   Squids are just easier to work with than humans.
[00:45:05.320 --> 00:45:10.960]   Nobody says that if we understand squid intelligence, we will understand everything about human
[00:45:10.960 --> 00:45:15.440]   intelligence and how to write novels and compose music.
[00:45:15.440 --> 00:45:20.400]   But we will understand many, many essential things that advance the field forward.
[00:45:20.400 --> 00:45:26.480]   I believe we can also understand the essence of embodied intelligence without worrying
[00:45:26.480 --> 00:45:35.160]   about, let's say, how to grasp wet, slippery pebbles and how to pour coffee from a particular
[00:45:35.160 --> 00:45:37.440]   type of container.
[00:45:37.440 --> 00:45:41.640]   Maybe we don't need to simulate all these complexities of the physical world.
[00:45:41.640 --> 00:45:49.080]   We need to identify the essential features that really bring out the essence of the problem,
[00:45:49.080 --> 00:45:57.360]   the essential aspects of spatial intelligence, and then study these inconvenient model systems.
[00:45:57.360 --> 00:46:01.000]   That's what we try to do with a lot of our work.
[00:46:01.000 --> 00:46:06.720]   And I think we can actually make progress, make progress enough to bootstrap physical
[00:46:06.720 --> 00:46:13.640]   systems that are basically intelligent enough to survive and not cause a lot of damage when
[00:46:13.640 --> 00:46:16.580]   they are deployed in the physical world.
[00:46:16.580 --> 00:46:20.600]   And then we can actually deploy them in the physical world and start tackling some of
[00:46:20.600 --> 00:46:29.280]   these last millimeter problems, such as how to grasp a slippery glass, that kind of thing.
[00:46:29.280 --> 00:46:30.280]   That's so interesting.
[00:46:30.280 --> 00:46:31.280]   Is it really last millimeter?
[00:46:31.280 --> 00:46:34.920]   Because I feel like something just like, I mean, you would know better than me, but just
[00:46:34.920 --> 00:46:38.480]   like the way fabric hangs or the way liquids spill.
[00:46:38.480 --> 00:46:44.920]   I understand that those are incredibly hard to simulate with any kind of accuracy as we
[00:46:44.920 --> 00:46:46.440]   would recognize it.
[00:46:46.440 --> 00:46:51.020]   You think that that's actually in the details and the more important thing is like, well,
[00:46:51.020 --> 00:46:56.580]   what is the more important thing then to know how to simulate quickly?
[00:46:56.580 --> 00:47:01.860]   Where's the productive axis to improve?
[00:47:01.860 --> 00:47:09.380]   One problem that I think a lot about that seems pretty key is the problem of internal
[00:47:09.380 --> 00:47:15.200]   representations of spatial environments that you need to maintain.
[00:47:15.200 --> 00:47:18.560]   So suppose you want to find your keys.
[00:47:18.560 --> 00:47:22.560]   You're in an apartment, you don't remember where you left your keys, you want to find
[00:47:22.560 --> 00:47:24.880]   your keys.
[00:47:24.880 --> 00:47:29.520]   So you need to move through the apartment and you need to maintain some representation
[00:47:29.520 --> 00:47:30.600]   of it.
[00:47:30.600 --> 00:47:36.760]   Or you're in a new restaurant and you want to find the bathroom.
[00:47:36.760 --> 00:47:38.340]   You've never been there before.
[00:47:38.340 --> 00:47:39.520]   You want to find the bathroom.
[00:47:39.520 --> 00:47:42.240]   I've done this experiment many times.
[00:47:42.240 --> 00:47:46.360]   You always find the bathroom and you don't even need to ask people, right?
[00:47:46.360 --> 00:47:48.480]   How do you do that?
[00:47:48.480 --> 00:47:49.840]   What is that?
[00:47:49.840 --> 00:47:55.840]   So I think these questions, these behaviors step into actually an important, what to me
[00:47:55.840 --> 00:48:01.280]   feels like an essential aspect of embodied intelligence, an essential aspect of spatial
[00:48:01.280 --> 00:48:02.280]   intelligence.
[00:48:02.280 --> 00:48:07.720]   And I think if we figure that out, we will be on our way.
[00:48:07.720 --> 00:48:11.160]   We will not be done, but we will be on our way.
[00:48:11.160 --> 00:48:13.600]   Then there is the very detailed aspect.
[00:48:13.600 --> 00:48:20.240]   One of my favorite challenges, long-term challenges for robotics is Steve Wozniak's challenge,
[00:48:20.240 --> 00:48:27.120]   which is that a robot needs to be able to go into a new house that it's never been in
[00:48:27.120 --> 00:48:32.120]   before and make a coffee.
[00:48:32.120 --> 00:48:40.720]   So that I think will not be solved with just the skill that I mentioned to you.
[00:48:40.720 --> 00:48:46.760]   That does rely on some of these last millimeter problems of sort of the detailed actuation,
[00:48:46.760 --> 00:48:51.480]   also reasoning about the functionality of projects, of objects.
[00:48:51.480 --> 00:48:53.680]   And I think we're actually far.
[00:48:53.680 --> 00:48:55.560]   I don't think it's going to happen next year.
[00:48:55.560 --> 00:48:58.760]   I think we're quite far, but it's a very exciting journey.
[00:48:58.760 --> 00:48:59.760]   Awesome.
[00:48:59.760 --> 00:49:00.760]   I love it.
[00:49:00.760 --> 00:49:01.760]   Thanks so much for your time.
[00:49:01.760 --> 00:49:02.760]   That was a lot of fun.
[00:49:02.760 --> 00:49:03.760]   Really appreciate it.
[00:49:03.760 --> 00:49:05.440]   Thank you so much, Lukas.
[00:49:05.440 --> 00:49:08.680]   Thanks for listening to another episode of Gradient Dissent.
[00:49:08.680 --> 00:49:13.000]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:49:13.000 --> 00:49:15.760]   hear from the people that are listening to these episodes.
[00:49:15.760 --> 00:49:19.840]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:49:19.840 --> 00:49:23.800]   a conversation, that would make me inspired to do more of these episodes.
[00:49:23.800 --> 00:49:27.360]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

