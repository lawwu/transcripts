
[00:00:00.000 --> 00:00:03.080]   LLMs are very good at memorizing static programs.
[00:00:03.080 --> 00:00:08.600]   If you scale up the size of your database, you are not increasing the intelligence of
[00:00:08.600 --> 00:00:09.600]   the system one bit.
[00:00:09.600 --> 00:00:12.680]   I feel like you're using words like memorization, which we would never use for human children.
[00:00:12.680 --> 00:00:17.600]   If they can just solve any arbitrary algebraic problem, you wouldn't say they've memorized
[00:00:17.600 --> 00:00:18.600]   algebra.
[00:00:18.600 --> 00:00:19.600]   They'd say they've learned algebra.
[00:00:19.600 --> 00:00:23.320]   So we've got a million-dollar prize pool, and there's a $500,000 prize for the first
[00:00:23.320 --> 00:00:25.960]   team that can get to the 85% benchmark.
[00:00:25.960 --> 00:00:29.240]   If Arc survives three months from here, we'll up the prize.
[00:00:29.240 --> 00:00:33.920]   They have basically set back progress towards AGI by probably like five to ten years.
[00:00:33.920 --> 00:00:37.520]   They caused this complete closing down of frontier research publishing.
[00:00:37.520 --> 00:00:43.920]   And now LLMs have sucked the oxygen out of the room like everyone is just doing LLMs.
[00:00:43.920 --> 00:00:44.920]   Okay.
[00:00:44.920 --> 00:00:51.600]   Today I have the pleasure to speak with FranÃ§ois Chollet, who is a AI researcher at Google
[00:00:51.600 --> 00:00:53.680]   and creator of Keras.
[00:00:53.680 --> 00:00:58.600]   And he's launching a prize in collaboration with Mike Knouf, the co-founder of Zapier,
[00:00:58.600 --> 00:01:03.760]   who we'll also be talking to in a second, a million-dollar prize to solve the Arc benchmark
[00:01:03.760 --> 00:01:04.840]   that he created.
[00:01:04.840 --> 00:01:08.480]   So first question, what is the Arc benchmark and why do you even need this prize?
[00:01:08.480 --> 00:01:11.760]   Why won't the biggest LLM we have in a year be able to just saturate it?
[00:01:11.760 --> 00:01:12.760]   Sure.
[00:01:12.760 --> 00:01:16.760]   So Arc is intended as a kind of IQ test for machine intelligence.
[00:01:16.760 --> 00:01:21.080]   And what makes it different from most LLM benchmarks out there is that it's designed
[00:01:21.080 --> 00:01:23.080]   to be resistant to memorization.
[00:01:23.080 --> 00:01:29.440]   So if you look at the way LLMs work, they're basically this big interpolative memory.
[00:01:29.440 --> 00:01:34.560]   And the way you scale up their capabilities is by trying to cram as much knowledge and
[00:01:34.560 --> 00:01:37.160]   patterns as possible into them.
[00:01:37.160 --> 00:01:42.360]   And by contrast, Arc does not require a lot of knowledge at all.
[00:01:42.360 --> 00:01:47.720]   It's designed to only require what's known as core knowledge, which is basic knowledge
[00:01:47.720 --> 00:01:54.280]   about things like elementary physics, objectness, counting, that sort of thing, the sort of
[00:01:54.280 --> 00:01:59.440]   knowledge that any four-year-old or five-year-old possesses.
[00:01:59.440 --> 00:02:04.560]   But what's interesting is that each puzzle in Arc is novel, is something that you've
[00:02:04.560 --> 00:02:09.880]   probably not encountered before, even if you've memorized the entire internet.
[00:02:09.880 --> 00:02:15.940]   And that's what makes Arc challenging for LLMs.
[00:02:15.940 --> 00:02:19.440]   And so far, LLMs have not been doing very well on it.
[00:02:19.440 --> 00:02:24.320]   In fact, the approaches that are working well are more towards discrete program search,
[00:02:24.320 --> 00:02:25.920]   program synthesis.
[00:02:25.920 --> 00:02:31.440]   So first of all, I'll make a comment that I'm glad that as a skeptic of LLM, you have
[00:02:31.440 --> 00:02:38.080]   put out yourself a benchmark that, is it accurate to say that suppose that the biggest model
[00:02:38.080 --> 00:02:42.840]   we have in a year is able to get 80% on this, then your view would be we are on track to
[00:02:42.840 --> 00:02:43.840]   AGI with LLMs?
[00:02:43.840 --> 00:02:45.200]   How would you think about that?
[00:02:45.200 --> 00:02:46.200]   Right.
[00:02:46.200 --> 00:02:51.740]   I'm pretty skeptical that we're going to see LLM do 80% in a year.
[00:02:51.740 --> 00:02:56.200]   That said, if we do see it, you would also have to look at how this was achieved.
[00:02:56.200 --> 00:03:02.360]   If you just train the model and millions or billions of puzzles similar to Arc, so that
[00:03:02.360 --> 00:03:08.140]   you're relying on the ability to have some overlap between the tasks that you train on
[00:03:08.140 --> 00:03:12.720]   and the tasks that you're going to see at test time, then you're still using memorization.
[00:03:12.720 --> 00:03:17.880]   And maybe, maybe it can work, you know, hopefully Arc is going to be good enough that it's going
[00:03:17.880 --> 00:03:23.640]   to be resistant to this sort of attempt and brute forcing, but you know, you never know.
[00:03:23.640 --> 00:03:24.640]   Maybe, maybe it could happen.
[00:03:24.640 --> 00:03:26.500]   I'm not saying it's not going to happen.
[00:03:26.500 --> 00:03:28.440]   Arc is not a perfect benchmark.
[00:03:28.440 --> 00:03:32.440]   Maybe, maybe it has flaws, maybe it could be hacked in that way.
[00:03:32.440 --> 00:03:40.480]   So I guess I'm curious about what would GPT-5 have to do that you're very confident that,
[00:03:40.480 --> 00:03:42.560]   you know, it's on the path to AGI?
[00:03:42.560 --> 00:03:48.680]   What would make me change my mind about that is basically, if I start seeing a critical
[00:03:48.680 --> 00:03:54.160]   mass of cases where you show the model with something it has not seen before, a task that's
[00:03:54.160 --> 00:03:58.600]   actually novel from the perspective of its training data, something that's not in training
[00:03:58.600 --> 00:04:03.000]   data, and if it can actually adapt on the fly.
[00:04:03.000 --> 00:04:08.400]   And this is true for LLMs, but really this would catch my attention with any AI technique
[00:04:08.400 --> 00:04:14.520]   out there, if I can see the ability to adapt to novelty on the fly, to pick up new skills
[00:04:14.520 --> 00:04:18.480]   efficiently, then I would be extremely interested.
[00:04:18.480 --> 00:04:21.560]   I would think this is on the path to AGI.
[00:04:21.560 --> 00:04:24.880]   So the advantage they have is that they do get to see everything.
[00:04:24.880 --> 00:04:28.480]   Maybe I'll take issue with how much they are relying on that, but let's suppose that they
[00:04:28.480 --> 00:04:32.220]   are relying, obviously they're relying on that more than humans do.
[00:04:32.220 --> 00:04:36.720]   To the extent that they do have so much in distribution, to the extent that we have trouble
[00:04:36.720 --> 00:04:41.360]   distinguishing whether an example is in distribution or not.
[00:04:41.360 --> 00:04:45.440]   Well, if they have everything in distribution, then they can do everything that we can do.
[00:04:45.440 --> 00:04:47.840]   Maybe it's not in distribution for us.
[00:04:47.840 --> 00:04:51.300]   Why is it so crucial that it has to be out of distribution for them?
[00:04:51.300 --> 00:04:54.280]   You know, why can't we just leverage the fact that they do get to see everything?
[00:04:54.280 --> 00:04:55.280]   Right.
[00:04:55.280 --> 00:04:59.480]   You're asking basically, what's the difference between actual intelligence, which is the
[00:04:59.480 --> 00:05:04.040]   ability to adapt to things you've not been prepared for, and pure memorization, like
[00:05:04.040 --> 00:05:07.120]   reciting what you've seen before.
[00:05:07.120 --> 00:05:10.120]   And it's not just some semantic difference.
[00:05:10.120 --> 00:05:17.880]   The big difference is that you can never pre-train on everything that you might see at test time.
[00:05:17.880 --> 00:05:18.880]   Right.
[00:05:18.880 --> 00:05:20.920]   Because the world changes all the time.
[00:05:20.920 --> 00:05:25.440]   So it's not just the fact that the space of possible tasks is infinite.
[00:05:25.440 --> 00:05:29.480]   And even if you're trained on millions of them, you've only seen zero percent of the
[00:05:29.480 --> 00:05:30.480]   total space.
[00:05:30.480 --> 00:05:33.680]   So it's also the fact that the world is changing every day.
[00:05:33.680 --> 00:05:34.880]   Right.
[00:05:34.880 --> 00:05:40.640]   This is why we, the human species, has developed intelligence in the first place.
[00:05:40.640 --> 00:05:46.520]   If there was such a thing as a distribution for the world, for the universe, for our lives,
[00:05:46.520 --> 00:05:48.520]   then we would not need intelligence at all.
[00:05:48.520 --> 00:05:53.440]   In fact, many creatures, many insects, for instance, do not have intelligence.
[00:05:53.440 --> 00:06:01.200]   Instead, what they have is they have in their connectome, in their genes, hard-coded programs,
[00:06:01.200 --> 00:06:05.480]   behavioral programs that map some stimuli to appropriate response.
[00:06:05.480 --> 00:06:10.440]   And they can actually navigate their lives, their environments, in a way that's very evolutionarily
[00:06:10.440 --> 00:06:14.680]   fits that way, without needing to learn anything.
[00:06:14.680 --> 00:06:19.880]   And while if our environment was static enough, predictable enough, what would have happened
[00:06:19.880 --> 00:06:25.120]   is that evolution would have found the perfect behavioral program, a hard-coded static behavioral
[00:06:25.120 --> 00:06:30.960]   program, would have written it into our genes, we would have a hard-coded brain connectome,
[00:06:30.960 --> 00:06:32.280]   and that's what we would be running on.
[00:06:32.280 --> 00:06:34.040]   But no, that's not what happened.
[00:06:34.040 --> 00:06:36.320]   Instead, we have general intelligence.
[00:06:36.320 --> 00:06:41.320]   So we are born with extremely little knowledge about the world, but we are born with the
[00:06:41.320 --> 00:06:47.400]   ability to learn very efficiently and to adapt in the face of things that we've never seen
[00:06:47.400 --> 00:06:48.400]   before.
[00:06:48.400 --> 00:06:49.400]   And that's what makes us unique.
[00:06:49.400 --> 00:06:53.960]   And that's what is really, really challenging to recreate in machines.
[00:06:53.960 --> 00:06:58.120]   I want to rabbit hole on that a little bit, but before I do that, maybe I'm going to overlay
[00:06:58.120 --> 00:07:02.280]   some examples of what an ARC-like challenge look like for the YouTube audience, but maybe
[00:07:02.280 --> 00:07:06.600]   for people listening on audio, can you just describe what would a sample ARC challenge
[00:07:06.600 --> 00:07:07.600]   look like?
[00:07:07.600 --> 00:07:08.600]   Sure.
[00:07:08.600 --> 00:07:11.920]   So one ARC puzzle, it looks kind of like an IQ test puzzle.
[00:07:11.920 --> 00:07:15.600]   You've got a number of demonstration input/output pairs.
[00:07:15.600 --> 00:07:19.320]   So one pair is made of two grids.
[00:07:19.320 --> 00:07:25.200]   The one grid shows you an input and the second grid shows you what you should produce as
[00:07:25.200 --> 00:07:27.920]   a response to that input.
[00:07:27.920 --> 00:07:33.160]   And you get a couple pairs like this to demonstrate the nature of the task, to demonstrate what
[00:07:33.160 --> 00:07:35.200]   you're supposed to do with your inputs.
[00:07:35.200 --> 00:07:43.080]   And then you get a new test input and your job is to produce the corresponding test output.
[00:07:43.080 --> 00:07:47.920]   You look at the demonstration pairs and from that, you figure out what you're supposed
[00:07:47.920 --> 00:07:53.040]   to do and you show that you've understood it on this new test pair.
[00:07:53.040 --> 00:07:59.240]   And importantly, in order to the sort of like knowledge basis that you need in order to
[00:07:59.240 --> 00:08:03.480]   approach these challenges is you just need core knowledge.
[00:08:03.480 --> 00:08:09.880]   And core knowledge is it's basically the knowledge of what makes an object, basic counting, basic
[00:08:09.880 --> 00:08:13.440]   geometry, topology, symmetries, that sort of thing.
[00:08:13.440 --> 00:08:17.800]   So extremely basic knowledge, LLMs for sure possess such knowledge.
[00:08:17.800 --> 00:08:21.680]   Any child possesses such knowledge.
[00:08:21.680 --> 00:08:24.920]   And what's really interesting is that each puzzle is new.
[00:08:24.920 --> 00:08:31.120]   So it's not something that you're going to find elsewhere on the internet, for instance.
[00:08:31.120 --> 00:08:36.160]   And that means that whether it's as a human or as a machine, every puzzle, you have to
[00:08:36.160 --> 00:08:37.560]   approach it from scratch.
[00:08:37.560 --> 00:08:39.520]   You have to actually reason your way through it.
[00:08:39.520 --> 00:08:43.140]   You cannot just fetch the response from your memory.
[00:08:43.140 --> 00:08:51.920]   So the core knowledge, one contention here is we are only now getting multimodal models
[00:08:51.920 --> 00:08:57.520]   who because of the data they're trained on, are trained to do spatial reasoning, whereas
[00:08:57.520 --> 00:09:03.320]   obviously not only humans, but for billions of years of evolution, we've had our ancestors
[00:09:03.320 --> 00:09:08.960]   have had to learn how to understand abstract physical and spatial properties and recognize
[00:09:08.960 --> 00:09:10.860]   the patterns there.
[00:09:10.860 --> 00:09:17.520]   And so one view would be in the next year, as we gain models that are multimodal native,
[00:09:17.520 --> 00:09:22.080]   that isn't just a sort of second class that is an add-on, but the multimodal capability
[00:09:22.080 --> 00:09:27.520]   is a priority, that it will understand these kinds of patterns because that's something
[00:09:27.520 --> 00:09:28.680]   we would see natively.
[00:09:28.680 --> 00:09:34.960]   Whereas right now, what Arc sees is some JSON string of 100100, and it's supposed to recognize
[00:09:34.960 --> 00:09:35.960]   the pattern there.
[00:09:35.960 --> 00:09:39.400]   And even if you showed a human such as like just a sequence of these kinds of numbers,
[00:09:39.400 --> 00:09:44.360]   it would have a challenge making sense of what kind of question you're asking it.
[00:09:44.360 --> 00:09:47.640]   So why wouldn't it be the case that as soon as we get the multimodal models, which we're
[00:09:47.640 --> 00:09:51.080]   on the path to unlock right now, they're going to be so much better at archetype spatial
[00:09:51.080 --> 00:09:52.080]   reasoning?
[00:09:52.080 --> 00:09:56.080]   That's an empirical question, so I guess we're going to see the answer within a few months.
[00:09:56.080 --> 00:10:02.320]   But my answer to that is, you know, our grids, they're just discrete 2D grids of symbols.
[00:10:02.320 --> 00:10:08.320]   They're pretty small, like it's not like, if you flatten an image as a sequence of pixels,
[00:10:08.320 --> 00:10:12.280]   for instance, then you get something that's actually very, very difficult to parse.
[00:10:12.280 --> 00:10:15.240]   That's not true for Arc because the grids are very small.
[00:10:15.240 --> 00:10:17.200]   You only have 10 possible symbols.
[00:10:17.200 --> 00:10:21.440]   So there's these 2D grids that are actually very easy to flatten as sequences.
[00:10:21.440 --> 00:10:24.600]   And transformers, LLMs, they're very good at parsing the sequences.
[00:10:24.600 --> 00:10:33.600]   In fact, you can show that LLMs do fine with processing Arc-like data by simply fine-tuning
[00:10:33.600 --> 00:10:42.720]   LLM on some subsets of the tasks and then trying to test it on small variations of these
[00:10:42.720 --> 00:10:43.720]   tasks.
[00:10:43.720 --> 00:10:49.520]   And you see that, yeah, the LLM can encode just fine solution programs for tasks that
[00:10:49.520 --> 00:10:50.520]   it has seen before.
[00:10:50.520 --> 00:10:57.720]   So it does not really have a problem parsing the input or figuring out the program.
[00:10:57.720 --> 00:11:04.680]   The reason why LLMs don't do well on Arc is really just the unfamiliarity aspect.
[00:11:04.680 --> 00:11:09.280]   The fact that each new task is different from every other task.
[00:11:09.280 --> 00:11:13.880]   You cannot basically, you cannot memorize the solution programs in advance.
[00:11:13.880 --> 00:11:18.320]   You have to synthesize a new solution program on the fly for each new task.
[00:11:18.320 --> 00:11:20.900]   And that's really what LLMs are struggling with.
[00:11:20.900 --> 00:11:25.360]   So before I do more Devil's Advocate, I just want to step back and explain why I'm especially
[00:11:25.360 --> 00:11:27.600]   interested in having this conversation.
[00:11:27.600 --> 00:11:32.000]   And obviously the Million Dollar Arc Prize, I'm excited to actually play with it myself.
[00:11:32.000 --> 00:11:39.740]   And hopefully the Vesuvius Challenge, which was Nat Friedman's prize for decoding squirrels,
[00:11:39.740 --> 00:11:45.320]   the winner of that, decoding the squirrels that were buried in the volcanoes in the Herculaneum
[00:11:45.320 --> 00:11:49.880]   Library, that was solved by a 22-year-old who was listening to the podcast, Luke Farritor.
[00:11:49.880 --> 00:11:55.100]   So hopefully somebody listening will find this challenge intriguing and find a solution.
[00:11:55.100 --> 00:12:02.640]   And the reason I've had on recently a lot of people who are bullish on LLMs, and I've
[00:12:02.640 --> 00:12:05.720]   had discussions with them before interviewing you about how do we explain the fact that
[00:12:05.720 --> 00:12:09.040]   LLMs don't seem to be natively performing that well on Arc.
[00:12:09.040 --> 00:12:12.840]   And I found their explanations somewhat contrived.
[00:12:12.840 --> 00:12:15.240]   And I'll try out some of the reasons on you.
[00:12:15.240 --> 00:12:19.720]   But it is actually an intriguing fact that they actually, these are some of these problems
[00:12:19.720 --> 00:12:22.720]   are relatively straightforward for humans to understand.
[00:12:22.720 --> 00:12:25.960]   They do struggle with them if you just input them natively.
[00:12:25.960 --> 00:12:27.800]   All of them are very easy for humans.
[00:12:27.800 --> 00:12:32.840]   Like any smart human should be able to do 90%, 95% on Arc.
[00:12:32.840 --> 00:12:33.840]   Smart human.
[00:12:33.840 --> 00:12:37.920]   A smart human, but even a five-year-old, so with very, very little knowledge, they could
[00:12:37.920 --> 00:12:40.380]   definitely do over 50%.
[00:12:40.380 --> 00:12:48.700]   So let's talk about that because I agree that smart humans will do very well on this test,
[00:12:48.700 --> 00:12:54.040]   but the average human will probably do, you know, mediocre.
[00:12:54.040 --> 00:12:55.040]   Not really.
[00:12:55.040 --> 00:12:58.400]   So we actually tried with average humans, the score was about 85.
[00:12:58.400 --> 00:13:00.480]   That was with Amazon Mechanical Turk Workers, right?
[00:13:00.480 --> 00:13:01.480]   That's right.
[00:13:01.480 --> 00:13:04.420]   I honestly don't know the demographic profile of Amazon Mechanical Turk Workers, but imagine
[00:13:04.420 --> 00:13:09.160]   just interacting with the platform that Amazon has set up to do remote work.
[00:13:09.160 --> 00:13:11.400]   That's not the median human across the planet, I'm guessing.
[00:13:11.400 --> 00:13:17.680]   Well, I mean, the broader point here being that, so we see the spectrum in humans where
[00:13:17.680 --> 00:13:20.280]   humans obviously have AGI.
[00:13:20.280 --> 00:13:24.560]   But even within humans, you see a spectrum where some people are relatively dumber, and
[00:13:24.560 --> 00:13:29.480]   they'll do perform work on IQ-like tests, for example, Raven's progressive matrices.
[00:13:29.480 --> 00:13:32.800]   If you look at how the average person performs on that, and you look at the kind of questions
[00:13:32.800 --> 00:13:35.760]   that is a sort of hit or miss, half of people will get it right, half of people will get
[00:13:35.760 --> 00:13:36.760]   it wrong.
[00:13:36.760 --> 00:13:37.760]   Some of them are like pretty trivial.
[00:13:37.760 --> 00:13:40.680]   For us, we might think like this is kind of trivial.
[00:13:40.680 --> 00:13:46.880]   And so humans have AGI, but from relatively small tweaks, you can go from somebody who
[00:13:46.880 --> 00:13:50.360]   misses these kinds of basic IQ test questions to somebody who gets them all right.
[00:13:50.360 --> 00:13:55.080]   Which suggests that actually, if these models are doing natively, we'll talk about some
[00:13:55.080 --> 00:13:57.200]   of the previous performances that people have tried with these models.
[00:13:57.200 --> 00:14:03.080]   But somebody with a Jack Cole with a 240 million parameter model got 35%.
[00:14:03.080 --> 00:14:06.520]   Doesn't that suggest that they're on this spectrum that clearly exists within humans,
[00:14:06.520 --> 00:14:08.480]   and they're going to be saturated pretty soon?
[00:14:08.480 --> 00:14:09.480]   Yeah.
[00:14:09.480 --> 00:14:11.440]   So there's a bunch of interesting points here.
[00:14:11.440 --> 00:14:19.360]   So there is indeed a branch of LLM approaches, so spreaded by Jack Cole, that are doing quite
[00:14:19.360 --> 00:14:23.140]   well, that are in fact state of the art.
[00:14:23.140 --> 00:14:25.560]   But you have to look at what's going on there.
[00:14:25.560 --> 00:14:26.560]   So there are two things.
[00:14:26.560 --> 00:14:33.040]   The first thing is that to get these numbers, you need to pre-train your LLM on millions
[00:14:33.040 --> 00:14:34.560]   of generated R tasks.
[00:14:34.560 --> 00:14:39.720]   And of course, if you compare that to a five-year-old child looking at R for the first time, the
[00:14:39.720 --> 00:14:43.960]   child has never done an IQ test before, has never seen something like an R task before.
[00:14:43.960 --> 00:14:49.360]   The only overlap between what they know and what they have to do in the test is core knowledge,
[00:14:49.360 --> 00:14:53.120]   is knowing about counting and objects and symmetries and things like that.
[00:14:53.120 --> 00:14:57.480]   And still, they're going to do really well, and they're going to do much better than the
[00:14:57.480 --> 00:15:00.720]   LLM trained on millions of similar tasks.
[00:15:00.720 --> 00:15:07.240]   And the second thing that's something to note about the Jack Cole approach is one thing
[00:15:07.240 --> 00:15:12.640]   that's really critical to making the model work at all is test time fine tuning.
[00:15:12.640 --> 00:15:17.920]   And that's something that's really missing, by the way, from LLM approaches right now
[00:15:17.920 --> 00:15:23.800]   is that most of the time when you're using an LLM, it's just doing static inference.
[00:15:23.800 --> 00:15:28.320]   The model is frozen and you're just prompting it, and then you're getting an answer.
[00:15:28.320 --> 00:15:31.840]   So the model is not actually learning anything on the fly.
[00:15:31.840 --> 00:15:36.040]   Its state is not adapting to the task at hand.
[00:15:36.040 --> 00:15:42.480]   And what Jack Cole is actually doing is that for every test problem is on the fly is fine
[00:15:42.480 --> 00:15:47.160]   tuning a version of the LLM for that task.
[00:15:47.160 --> 00:15:48.960]   And that's really what's unlocking performance.
[00:15:48.960 --> 00:15:55.160]   If you don't do that, you get like 1%, 2%, so basically something completely negligible.
[00:15:55.160 --> 00:15:59.360]   And if you do test time fine tuning, and you add a bunch of tricks on top, then you end
[00:15:59.360 --> 00:16:01.160]   up with interesting performance numbers.
[00:16:01.160 --> 00:16:07.200]   So I think what he's doing is trying to address one of the key limitations of LLMs today,
[00:16:07.200 --> 00:16:11.720]   which is the lack of active inference is actually adding active inference to LLMs.
[00:16:11.720 --> 00:16:13.440]   And that's working extremely well, actually.
[00:16:13.440 --> 00:16:15.160]   So that's fascinating to me.
[00:16:15.160 --> 00:16:18.560]   There's so many interesting rabbit holes there.
[00:16:18.560 --> 00:16:20.200]   Should I take them in sequence or deal with them all at once?
[00:16:20.200 --> 00:16:21.320]   Let me, let me just start.
[00:16:21.320 --> 00:16:27.060]   So the point you made about the fact that you need to unlock the adaptive compute slash
[00:16:27.060 --> 00:16:31.760]   test time compute, a lot of the scale maximalist, I think this will be interesting rabbit hole
[00:16:31.760 --> 00:16:37.040]   to explore with you, because a lot of the scaling maximalist have your broader perspective
[00:16:37.040 --> 00:16:41.880]   in the sense that they think that in addition to scaling, you need these kinds of things
[00:16:41.880 --> 00:16:47.600]   like unlocking adaptive compute or doing some sort of RL to get the system to working.
[00:16:47.600 --> 00:16:51.920]   And their perspective is that this is a relatively straightforward thing that will be added at
[00:16:51.920 --> 00:16:57.860]   the top, the representations that a scaled up model has greater access to.
[00:16:57.860 --> 00:16:58.860]   No, it's not.
[00:16:58.860 --> 00:17:00.480]   It's not just a technical detail.
[00:17:00.480 --> 00:17:01.920]   It's not a straightforward thing.
[00:17:01.920 --> 00:17:03.340]   It is everything.
[00:17:03.340 --> 00:17:05.420]   It is the important part.
[00:17:05.420 --> 00:17:14.300]   And the scale maximalist argument, it boils down to, you know, these people, they refer
[00:17:14.300 --> 00:17:19.620]   to scaling loss, which is this empirical relationship that you can draw between how much compute
[00:17:19.620 --> 00:17:23.840]   you spend on training a model and the performance you're getting on benchmarks, right?
[00:17:23.840 --> 00:17:28.340]   And the key question here, of course, is, well, how do you measure performance?
[00:17:28.340 --> 00:17:33.360]   What it is that you're actually improving by adding more compute and more data?
[00:17:33.360 --> 00:17:35.640]   And well, it's benchmark performance, right?
[00:17:35.640 --> 00:17:42.000]   And the thing is, the way you measure performance is not a technical detail.
[00:17:42.000 --> 00:17:47.260]   It's not an afterthought because it's going to narrow down the sort of questions that
[00:17:47.260 --> 00:17:48.260]   you're asking.
[00:17:48.260 --> 00:17:53.380]   So accordingly, it's going to narrow down the set of answers that you're looking for.
[00:17:53.380 --> 00:17:59.680]   If you look at the benchmarks we're using for LLMs, they're all memorization-based benchmarks.
[00:17:59.680 --> 00:18:03.640]   Like sometimes they're literally just knowledge-based, like a school test.
[00:18:03.640 --> 00:18:09.200]   And even if you look at the ones that are, you know, explicitly about reasoning, you
[00:18:09.200 --> 00:18:15.960]   realize if you look closely that in order to solve them, it's enough to memorize a finite
[00:18:15.960 --> 00:18:19.200]   set of reasoning patterns.
[00:18:19.200 --> 00:18:24.440]   And then you just reapply them, they're like static programs, LLMs are very good at memorizing
[00:18:24.440 --> 00:18:27.000]   static programs, small statics programs.
[00:18:27.000 --> 00:18:31.880]   And they've got this sort of like bank of solution programs.
[00:18:31.880 --> 00:18:36.880]   And when you give them a new puzzle, they can just fetch the appropriate program, apply
[00:18:36.880 --> 00:18:41.520]   it, and it's looking like it's reasoning, but really it's not doing any sort of on-the-fly
[00:18:41.520 --> 00:18:42.680]   program synthesis.
[00:18:42.680 --> 00:18:44.640]   All it's doing is program fetching.
[00:18:44.640 --> 00:18:48.760]   So you can actually solve all these benchmarks with memorization.
[00:18:48.760 --> 00:18:55.520]   And so what you're scaling up here, like if you look at the models, they are big parametric
[00:18:55.520 --> 00:18:59.560]   curves fitted to a data distribution, which I call in this sense.
[00:18:59.560 --> 00:19:05.000]   So they're basically these big interpolative databases, interpolative memories.
[00:19:05.000 --> 00:19:11.400]   And of course, if you scale up the size of your database and you cram into it more knowledge,
[00:19:11.400 --> 00:19:17.880]   more patterns and so on, you are going to be increasing its performance as measured
[00:19:17.880 --> 00:19:19.880]   by memorization benchmark.
[00:19:19.880 --> 00:19:20.880]   That's kind of obvious.
[00:19:20.880 --> 00:19:26.800]   But as you're doing it, you are not increasing the intelligence of the system one bit.
[00:19:26.800 --> 00:19:31.760]   You are increasing the skill of the system, you are increasing its usefulness, its scope
[00:19:31.760 --> 00:19:36.720]   of applicability, but not its intelligence because skill is not intelligence.
[00:19:36.720 --> 00:19:42.680]   And that's the fundamental confusion that people run into is that they're confusing
[00:19:42.680 --> 00:19:43.680]   skill and intelligence.
[00:19:43.680 --> 00:19:46.560]   Yeah, there's a lot of fascinating things to talk about here.
[00:19:46.560 --> 00:19:52.880]   So skill, intelligence, interpolation, I mean, okay, so the thing about they're fitting some
[00:19:52.880 --> 00:19:58.400]   manifold into that maps the input data, there's a reductionist way to talk about what happens
[00:19:58.400 --> 00:20:03.320]   in the human brain that says that it's just axons firing at each other.
[00:20:03.320 --> 00:20:06.440]   But we don't care about the reductionist explanation of what's happening.
[00:20:06.440 --> 00:20:12.480]   We care about what the sort of meta at the macroscopic level, what happens when these
[00:20:12.480 --> 00:20:13.920]   things combine.
[00:20:13.920 --> 00:20:20.360]   As far as the interpolation goes, so okay, let's look at one of the benchmarks here.
[00:20:20.360 --> 00:20:23.000]   There's one benchmark that does great school math.
[00:20:23.000 --> 00:20:28.560]   And these are problems that like a smart high schooler would be able to solve.
[00:20:28.560 --> 00:20:29.920]   It's called GSM 8K.
[00:20:29.920 --> 00:20:33.320]   And these models get 95% on these, like basically they always nail it.
[00:20:33.320 --> 00:20:34.320]   That's memorization benchmark.
[00:20:34.320 --> 00:20:35.560]   Okay, let's talk about what that means.
[00:20:35.560 --> 00:20:38.560]   So here's one question about from that benchmark.
[00:20:38.560 --> 00:20:42.800]   So 30 students are in a class, one fifth of them are 12 year olds, one third are 13 year
[00:20:42.800 --> 00:20:45.120]   old, one 10th are 11 year olds.
[00:20:45.120 --> 00:20:48.400]   How many of them are not 11, 12 or 13 years old?
[00:20:48.400 --> 00:20:50.480]   So I agree, like this is not rocket science, right?
[00:20:50.480 --> 00:20:53.560]   You can write down on paper how you go through this problem.
[00:20:53.560 --> 00:20:56.720]   And a high school kid, at least a smart high school kid should be able to solve it.
[00:20:56.720 --> 00:21:02.920]   Now, when you say memorization, it still has to reason through how to think about fractions
[00:21:02.920 --> 00:21:07.120]   and what is the context of the whole problem and then combining the different calculations
[00:21:07.120 --> 00:21:08.120]   it's doing.
[00:21:08.120 --> 00:21:12.320]   It depends how you want to define reasoning, but there are two definitions you can use.
[00:21:12.320 --> 00:21:18.120]   So one is, I have available a set of program templates.
[00:21:18.120 --> 00:21:23.160]   It's like the structure of the puzzle, which can also generate its solution.
[00:21:23.160 --> 00:21:27.760]   And I'm just going to identify the right template, which is in my memory.
[00:21:27.760 --> 00:21:32.000]   I'm going to input the new values into the template, run the program, get the solution.
[00:21:32.000 --> 00:21:33.680]   And you could say, this is reasoning.
[00:21:33.680 --> 00:21:34.680]   And I say, yeah, sure.
[00:21:34.680 --> 00:21:35.680]   Okay.
[00:21:35.680 --> 00:21:40.320]   But another definition we can use is reasoning is the ability to, when you're faced with
[00:21:40.320 --> 00:21:46.120]   a puzzle, given that you don't have already a program in memory to solve it, you must
[00:21:46.120 --> 00:21:52.280]   synthesize on the fly a new program based on bits of pieces of existing programs that
[00:21:52.280 --> 00:21:53.280]   you have.
[00:21:53.280 --> 00:21:55.000]   You have to do on the fly program synthesis.
[00:21:55.000 --> 00:22:00.040]   And it's actually dramatically harder than just fetching the right memorized program
[00:22:00.040 --> 00:22:01.500]   and replying it.
[00:22:01.500 --> 00:22:07.000]   So I think maybe we are overestimating the extent to which humans are so sample efficient,
[00:22:07.000 --> 00:22:14.800]   they also don't need training in this way where they have to drill in these kinds of
[00:22:14.800 --> 00:22:16.800]   pathways of reasoning through certain kinds of problems.
[00:22:16.800 --> 00:22:18.280]   So let's take math, for example.
[00:22:18.280 --> 00:22:19.280]   Yeah.
[00:22:19.280 --> 00:22:23.840]   It's not like you can just show a baby the axioms of set theory, and now they know math,
[00:22:23.840 --> 00:22:24.840]   right?
[00:22:24.840 --> 00:22:28.320]   So when they're growing up, you had to do years of teaching them pre-algebra, then you
[00:22:28.320 --> 00:22:31.360]   got to do a year of teaching them doing drills and going through the same kind of problem
[00:22:31.360 --> 00:22:34.720]   in algebra, then geometry, pre-calculus, calculus.
[00:22:34.720 --> 00:22:35.720]   Absolutely.
[00:22:35.720 --> 00:22:36.720]   So training.
[00:22:36.720 --> 00:22:37.720]   Yeah.
[00:22:37.720 --> 00:22:40.160]   But isn't that like the same kind of thing where you can't just see one example, and
[00:22:40.160 --> 00:22:42.000]   now you have the program or whatever.
[00:22:42.000 --> 00:22:43.000]   You actually had to drill it.
[00:22:43.000 --> 00:22:45.000]   These models also had to drill with a bunch of returning data.
[00:22:45.000 --> 00:22:46.000]   Sure.
[00:22:46.000 --> 00:22:50.860]   I mean, in order to do on the fly program synthesis, you actually need building blocks
[00:22:50.860 --> 00:22:52.020]   to work from.
[00:22:52.020 --> 00:22:56.160]   So knowledge and memory are actually tremendously important in the process.
[00:22:56.160 --> 00:23:00.640]   I'm not, I'm not saying it's memory versus reasoning.
[00:23:00.640 --> 00:23:04.780]   In order to do effective reasoning, you need memory.
[00:23:04.780 --> 00:23:09.060]   But it sounds like it's compatible with your story that through seeing a lot of different
[00:23:09.060 --> 00:23:14.040]   kinds of examples, these things can learn to reason within the context of those examples.
[00:23:14.040 --> 00:23:16.160]   And we can also see within bigger and bigger models.
[00:23:16.160 --> 00:23:20.080]   So that was an example of a high school level math problem.
[00:23:20.080 --> 00:23:23.780]   Let's say a model that's like smaller than GPT-3 couldn't do that at all.
[00:23:23.780 --> 00:23:26.120]   As these models get bigger, they can, they seem to be able to pick a bigger and bigger
[00:23:26.120 --> 00:23:28.560]   No, it's not really a size issue.
[00:23:28.560 --> 00:23:31.160]   It's more like a training data issue in this case.
[00:23:31.160 --> 00:23:34.920]   Well, bigger models can pick up these kinds of circuits, which smaller models apparently
[00:23:34.920 --> 00:23:38.440]   don't do a good job of doing this, even if you were to train them on this kind of data.
[00:23:38.440 --> 00:23:40.960]   Doesn't that just suggest that as you have bigger and bigger models, they can pick up
[00:23:40.960 --> 00:23:44.600]   bigger and bigger pathways or more general ways of reasoning?
[00:23:44.600 --> 00:23:45.600]   Absolutely.
[00:23:45.600 --> 00:23:46.600]   But then isn't that intelligence?
[00:23:46.600 --> 00:23:47.600]   No, no, it's not.
[00:23:47.600 --> 00:23:52.840]   If you scale up your database, and you keep adding to it more knowledge, more program
[00:23:52.840 --> 00:23:56.560]   templates, then sure, it becomes more and more skillful, you can apply it to more and
[00:23:56.560 --> 00:23:57.560]   more tasks.
[00:23:57.560 --> 00:24:03.680]   But general intelligence is not task-specific skill scaled up to many skills.
[00:24:03.680 --> 00:24:06.960]   Because there is an infinite space of possible skills.
[00:24:06.960 --> 00:24:12.280]   General intelligence is the ability to approach any problem, any skill, and very quickly master
[00:24:12.280 --> 00:24:14.120]   it using very little data.
[00:24:14.120 --> 00:24:18.200]   Because this is what makes you able to face anything you might ever encounter.
[00:24:18.200 --> 00:24:25.800]   This is the definition of generality, like generality is not specificity scaled up.
[00:24:25.800 --> 00:24:31.720]   It is the ability to apply your mind to anything at all, to arbitrary things.
[00:24:31.720 --> 00:24:35.800]   And this requires, fundamentally, this requires the ability to adapt, to learn on the fly
[00:24:35.800 --> 00:24:37.180]   efficiently.
[00:24:37.180 --> 00:24:43.520]   So my claim is that by doing this free training on bigger and bigger models, you are gaining
[00:24:43.520 --> 00:24:46.840]   the capacity to then generalize very efficiently.
[00:24:46.840 --> 00:24:48.560]   Let me give you an example.
[00:24:48.560 --> 00:24:49.560]   Let me give you an example.
[00:24:49.560 --> 00:24:55.200]   So your own company, Google, in their paper on Gemini 1.5, they had this very interesting
[00:24:55.200 --> 00:25:02.960]   example where they would give, in context, they would give the model, the grammar book
[00:25:02.960 --> 00:25:07.580]   and the dictionary of a language that has less than 200 living speakers.
[00:25:07.580 --> 00:25:09.660]   So it's not in the free training data.
[00:25:09.660 --> 00:25:11.880]   And you just give them the dictionary.
[00:25:11.880 --> 00:25:16.600]   And it basically is able to speak this language and translate to it, including the complex
[00:25:16.600 --> 00:25:20.260]   and organic ways in which languages are structured.
[00:25:20.260 --> 00:25:22.960]   So a human, if you showed me a dictionary from like English to Spanish, I'm not going
[00:25:22.960 --> 00:25:28.340]   to be able to pick up the how to structure sentences and how to say things in Spanish.
[00:25:28.340 --> 00:25:33.160]   The fact that because of the representations that it has gained through this free training,
[00:25:33.160 --> 00:25:36.560]   it is able to now extremely efficiently learn a new language.
[00:25:36.560 --> 00:25:40.480]   Doesn't that show that this kind of pre-training actually does increase your ability to learn
[00:25:40.480 --> 00:25:41.480]   new tasks?
[00:25:41.480 --> 00:25:45.840]   If you're right, if you were right, LLMs would do really well on ARC puzzles, because ARC
[00:25:45.840 --> 00:25:47.760]   puzzles are not complex.
[00:25:47.760 --> 00:25:49.960]   Each one of them requires very little knowledge.
[00:25:49.960 --> 00:25:52.280]   Each one of them is very low on complexity.
[00:25:52.280 --> 00:25:54.240]   You don't need to think very hard about it.
[00:25:54.240 --> 00:25:56.640]   They're actually extremely obvious for humans.
[00:25:56.640 --> 00:26:00.760]   Like even children can do them, but LLMs cannot.
[00:26:00.760 --> 00:26:06.600]   Even LLMs that have, you know, 100,000 times more knowledge than you do, they still cannot.
[00:26:06.600 --> 00:26:11.760]   And the only thing that makes ARC special is that it was designed with this intent to
[00:26:11.760 --> 00:26:12.760]   resist memorization.
[00:26:12.760 --> 00:26:14.440]   This is the only thing.
[00:26:14.440 --> 00:26:19.200]   And this is the huge blocker for LLM performance, right?
[00:26:19.200 --> 00:26:27.400]   And so, you know, I think if you look at LLMs closely, it's pretty obvious that they're
[00:26:27.400 --> 00:26:32.320]   not really like synthesizing new programs on the fly to solve the tasks that they're
[00:26:32.320 --> 00:26:33.320]   faced with.
[00:26:33.320 --> 00:26:36.800]   They're very much replying things that they've stored in memory.
[00:26:36.800 --> 00:26:42.760]   For instance, one thing that's very striking is LLMs can solve a Caesar cipher, you know,
[00:26:42.760 --> 00:26:49.480]   like a cipher, like transposing letters to code a message.
[00:26:49.480 --> 00:26:52.760]   And well, that's a fairly complex algorithm, right?
[00:26:52.760 --> 00:26:55.080]   But it comes up quite a bit on the internet.
[00:26:55.080 --> 00:26:56.560]   So they've basically memorized it.
[00:26:56.560 --> 00:27:01.240]   And what's really interesting is that they can do it for a transposition length of like
[00:27:01.240 --> 00:27:05.800]   three or five, because they are very, very common numbers in examples provided on the
[00:27:05.800 --> 00:27:06.800]   internet.
[00:27:06.800 --> 00:27:11.320]   But if you try to do it with an arbitrary number like nine, it's going to fail.
[00:27:11.320 --> 00:27:16.480]   Because it does not encode the generalized form of the algorithm, but only specific cases.
[00:27:16.480 --> 00:27:20.160]   It has memorized specific cases of the algorithm, right?
[00:27:20.160 --> 00:27:26.120]   And if it could actually synthesize on the fly, the solver algorithm, then the value
[00:27:26.120 --> 00:27:30.760]   of N would not matter at all, because it does not increase the problem complexity.
[00:27:30.760 --> 00:27:34.640]   I think this is true of humans as well, where, what was the study?
[00:27:34.640 --> 00:27:39.800]   Humans use memorization pattern matching all the time, of course, but humans are not limited
[00:27:39.800 --> 00:27:41.480]   to memorization pattern matching.
[00:27:41.480 --> 00:27:45.400]   They have this very unique ability to adapt to new situations on the fly.
[00:27:45.400 --> 00:27:50.360]   This is exactly what enables you to navigate every new day in your life.
[00:27:50.360 --> 00:27:54.520]   I'm forgetting the details, but there was some study that chess grandmasters will perform
[00:27:54.520 --> 00:27:57.120]   very well within the context of the moves that...
[00:27:57.120 --> 00:28:01.120]   Excellent example, because chess at the highest level is all about memorization.
[00:28:01.120 --> 00:28:02.120]   Chess memorization.
[00:28:02.120 --> 00:28:03.120]   Okay, sure.
[00:28:03.120 --> 00:28:04.120]   We can leave that aside.
[00:28:04.120 --> 00:28:12.680]   I have a general question of why in context the GPT-1, sorry, Gemini 1.5 was able to learn
[00:28:12.680 --> 00:28:15.760]   a language, including the complex grammar structure.
[00:28:15.760 --> 00:28:17.360]   Doesn't that show that they can pick up new knowledge?
[00:28:17.360 --> 00:28:23.120]   I would assume that it has simply mined from its extremely extensive, unimaginably vast
[00:28:23.120 --> 00:28:28.960]   training data, it has mined the required template, and then it's just reusing it.
[00:28:28.960 --> 00:28:34.200]   We know that LLMs have a very poor ability to synthesize new program templates like this
[00:28:34.200 --> 00:28:36.520]   on the fly, or even adapt existing ones.
[00:28:36.520 --> 00:28:39.240]   They're very much limited to fetching.
[00:28:39.240 --> 00:28:42.920]   Suppose there's a programmer at Google, they go into the office in the morning.
[00:28:42.920 --> 00:28:47.640]   At what point are they doing something that 100% cannot be due to fetching some template
[00:28:47.640 --> 00:28:51.680]   that could, even if they, suppose they were an LLM, they could not do if they had fetched
[00:28:51.680 --> 00:28:52.680]   some template from their program.
[00:28:52.680 --> 00:28:55.640]   Like at what point do they have to use this so-called extreme generalization capability?
[00:28:55.640 --> 00:29:01.020]   Forget about Google software developers, every human, every day of their lives is full of
[00:29:01.020 --> 00:29:04.000]   novel things that they've not been prepared for.
[00:29:04.000 --> 00:29:07.960]   You cannot navigate your life based on memorization alone.
[00:29:07.960 --> 00:29:08.960]   It's impossible.
[00:29:08.960 --> 00:29:13.200]   I'm sort of denying the premise that they're, you also agree they're not doing like quote
[00:29:13.200 --> 00:29:15.240]   unquote memorization.
[00:29:15.240 --> 00:29:18.160]   It seems like you're saying they're less capable of generalization.
[00:29:18.160 --> 00:29:21.760]   But I'm just curious of like the kind of generalization they do.
[00:29:21.760 --> 00:29:25.760]   So if you, if you, if you get into the office and you try to do this kind of generalization,
[00:29:25.760 --> 00:29:26.760]   you're going to fail at your job.
[00:29:26.760 --> 00:29:27.760]   But what is the first point?
[00:29:27.760 --> 00:29:28.760]   You're a programmer.
[00:29:28.760 --> 00:29:31.440]   What is the first point when you try to do that generalization, you would, you would,
[00:29:31.440 --> 00:29:34.600]   you would lose your job because you can't do the extreme generalization?
[00:29:34.600 --> 00:29:41.240]   I don't have any specific examples, but literally, like take this situation, for instance, you've
[00:29:41.240 --> 00:29:43.080]   never been here in this room.
[00:29:43.080 --> 00:29:47.680]   Maybe you've been in, in, in this, in this city a few times, I don't know, but there's
[00:29:47.680 --> 00:29:49.400]   a fair amount of novelty.
[00:29:49.400 --> 00:29:52.000]   You've never been, been, you know, interviewing me.
[00:29:52.000 --> 00:29:56.040]   There's a fair amount of novelty in every hour of every day in your life.
[00:29:56.040 --> 00:30:02.280]   It's in fact, by and large, more novelty than any, any LLM could handle.
[00:30:02.280 --> 00:30:06.240]   Like if you just put a LLM in, in a robot, it could not be doing all the things that
[00:30:06.240 --> 00:30:07.600]   you've been doing today.
[00:30:07.600 --> 00:30:08.600]   Right.
[00:30:08.600 --> 00:30:13.320]   Or take, I don't know, like self-driving cars, for instance, you take a self-driving car
[00:30:13.320 --> 00:30:15.400]   operating in the, in the Bay Area.
[00:30:15.400 --> 00:30:19.640]   Do you think you could just drop it in New York City or drop it in London where people
[00:30:19.640 --> 00:30:20.640]   drive on the left?
[00:30:20.640 --> 00:30:22.440]   Uh, no, it's, it's going to fail.
[00:30:22.440 --> 00:30:29.400]   So not, not only can you drop, not like make it generalized to, uh, uh, a change of rules,
[00:30:29.400 --> 00:30:34.560]   um, uh, of driving rules, but you can not even make it generalized to a new city.
[00:30:34.560 --> 00:30:37.680]   It needs to be trained on each specific environment.
[00:30:37.680 --> 00:30:38.680]   Yeah.
[00:30:38.680 --> 00:30:42.500]   I mean, I, I agree that self-driving cars aren't AGI, um, but it's the same type of
[00:30:42.500 --> 00:30:43.500]   model.
[00:30:43.500 --> 00:30:44.500]   They're transformers as well.
[00:30:44.500 --> 00:30:48.860]   I mean, we, I don't know, apes also have brains with neurons in them, but they're less intelligent
[00:30:48.860 --> 00:30:49.860]   because they're smaller.
[00:30:49.860 --> 00:30:50.860]   It's not the same architecture.
[00:30:50.860 --> 00:30:51.860]   We can get into that.
[00:30:51.860 --> 00:30:59.040]   But so I still don't understand what, uh, like a concrete thing of, we also need training.
[00:30:59.040 --> 00:31:00.040]   That's why education exists.
[00:31:00.040 --> 00:31:03.260]   That's why we had to spend the first 18 years of our life doing drills.
[00:31:03.260 --> 00:31:06.200]   We have a memory, but we are not a memory.
[00:31:06.200 --> 00:31:07.700]   We are not limited to just a memory.
[00:31:07.700 --> 00:31:11.100]   But I'm denying the premise that that's necessarily the only thing these models are doing.
[00:31:11.100 --> 00:31:15.980]   And I'm still not sure what is the, what is the task that a remote worker would be doing,
[00:31:15.980 --> 00:31:20.260]   have to, like, suppose you just stepped out of remote work with an LLM and they're, they're
[00:31:20.260 --> 00:31:21.260]   a programmer.
[00:31:21.260 --> 00:31:23.980]   What is the first point at which you realize this is not a human, this is an LLM?
[00:31:23.980 --> 00:31:27.300]   What about I just send them a knock puzzle and see how they do?
[00:31:27.300 --> 00:31:29.300]   No, like part of their job, you know.
[00:31:29.300 --> 00:31:32.140]   But you have to deal with novelty all the time.
[00:31:32.140 --> 00:31:33.140]   Okay.
[00:31:33.140 --> 00:31:37.700]   So if you, is there a world in which all the programmers are replaced and then we're still
[00:31:37.700 --> 00:31:41.820]   saying, ah, but they're only doing memorization late in programming tasks, but they're still
[00:31:41.820 --> 00:31:46.900]   producing a trillion dollars of, uh, worth of, you know, output in the form of code?
[00:31:46.900 --> 00:31:49.460]   Software development is actually a pretty good example of a job where you're dealing
[00:31:49.460 --> 00:31:53.860]   with novelty all the time, or if you're not, well, I'm not sure what you're doing.
[00:31:53.860 --> 00:32:00.080]   So I personally use generative AI very little in my software development job.
[00:32:00.080 --> 00:32:05.100]   And before, before LLM, I think I was also using stack overflow very little.
[00:32:05.100 --> 00:32:08.620]   You know, some people maybe are just copy pasting stuff from stack overflow, or nowadays
[00:32:08.620 --> 00:32:11.420]   copy pasting stuff from, from an LLM.
[00:32:11.420 --> 00:32:15.100]   Personally, I try to focus on problem solving.
[00:32:15.100 --> 00:32:17.020]   The syntax is just a technical detail.
[00:32:17.020 --> 00:32:19.500]   What's really important is the problem solving.
[00:32:19.500 --> 00:32:27.340]   Like the essence of programming is engineering mental models, like mental representations
[00:32:27.340 --> 00:32:29.620]   of the problem you're trying to solve.
[00:32:29.620 --> 00:32:34.180]   But you can add, you know, we have many, people can interact with these systems themselves
[00:32:34.180 --> 00:32:38.740]   and you can go to chatGPT and say, here's a specification of the kind of program I want.
[00:32:38.740 --> 00:32:39.740]   They'll build it for you.
[00:32:39.740 --> 00:32:43.260]   As long as there are many examples of this program on like ETEM and stack overflow and
[00:32:43.260 --> 00:32:47.180]   so on, sure, they will fetch the program for you from their memory.
[00:32:47.180 --> 00:32:48.980]   But you can change arbitrary details.
[00:32:48.980 --> 00:32:49.980]   No, it doesn't work.
[00:32:49.980 --> 00:32:51.900]   You can say, I need it to work on this different kind of server.
[00:32:51.900 --> 00:32:55.900]   If that were true, there would be no software engineers today.
[00:32:55.900 --> 00:32:56.900]   I agree.
[00:32:56.900 --> 00:33:01.380]   We're not at a full AGI yet in the sense that these models have, let's say, less than a
[00:33:01.380 --> 00:33:05.660]   trillion parameters, a human brain has somewhere on the order of 10 to 30 trillion synapses.
[00:33:05.660 --> 00:33:11.220]   I mean, if you were just doing some naive math, you're like at least 10x under parameterized.
[00:33:11.220 --> 00:33:12.700]   So I agree, we're not there yet.
[00:33:12.700 --> 00:33:19.140]   But I'm sort of confused on why we're not on the spectrum where yes, I agree that there's
[00:33:19.140 --> 00:33:20.980]   many kinds of generalization they can do.
[00:33:20.980 --> 00:33:24.100]   But it seems like they're on this kind of smooth spectrum that we see even within humans
[00:33:24.100 --> 00:33:27.420]   where some humans would have a hard time doing an arc type test.
[00:33:27.420 --> 00:33:30.900]   We see that based on the performance on progressive Raman's matrices type IQ tests.
[00:33:30.900 --> 00:33:37.060]   I'm not a fan of IQ tests because for the most part, you can train on IQ tests and get
[00:33:37.060 --> 00:33:38.060]   better at them.
[00:33:38.060 --> 00:33:40.020]   So they're very much memorization based.
[00:33:40.020 --> 00:33:46.780]   And this is actually the main pitfall that Arc tries not to fall for.
[00:33:46.780 --> 00:33:52.660]   So if all remote jobs are automated in the next five years, let's say, at least that
[00:33:52.660 --> 00:33:56.660]   don't require you to be like sort of a service, it's not like a salesperson where you want
[00:33:56.660 --> 00:34:01.540]   the human to be talking, but like programming, whatever in that world.
[00:34:01.540 --> 00:34:03.020]   Would you say that that's not possible?
[00:34:03.020 --> 00:34:07.100]   Because a lot of what a programmer needs to do, definitely requires things that would
[00:34:07.100 --> 00:34:08.740]   not be in any free training corpus?
[00:34:08.740 --> 00:34:09.740]   Sure.
[00:34:09.740 --> 00:34:13.220]   I mean, in five years, there will be more software engineers than there are today.
[00:34:13.220 --> 00:34:14.220]   Right.
[00:34:14.220 --> 00:34:15.220]   But I just want to understand.
[00:34:15.220 --> 00:34:16.220]   So I'm still not sure.
[00:34:16.220 --> 00:34:19.580]   I mean, I know how to, I studied computer science, I think if I had become a code monkey
[00:34:19.580 --> 00:34:22.020]   out of college, like what would I be doing?
[00:34:22.020 --> 00:34:23.660]   I go to my job.
[00:34:23.660 --> 00:34:26.540]   What is the first thing my boss tells me something to do?
[00:34:26.540 --> 00:34:28.460]   When does he realize I'm an LLM?
[00:34:28.460 --> 00:34:30.580]   If I was an LLM?
[00:34:30.580 --> 00:34:39.900]   Probably on the first day, you know, again, if it were true that LLMs could generalize
[00:34:39.900 --> 00:34:45.900]   to novel problems like this, and actually develop software to solve a problem they've
[00:34:45.900 --> 00:34:48.840]   never seen before, you would not need software engineers anymore.
[00:34:48.840 --> 00:34:52.980]   In practice, if I look at how people are using LLMs in their software engineering job today,
[00:34:52.980 --> 00:34:56.740]   they are using it as a stack overflow replacement.
[00:34:56.740 --> 00:35:03.260]   So they're using it as a way to copy paste code snippets to perform very common actions.
[00:35:03.260 --> 00:35:06.540]   And this is what they actually need is a database of code snippets.
[00:35:06.540 --> 00:35:10.860]   They don't actually need any of the abilities that actually make them software engineers.
[00:35:10.860 --> 00:35:15.780]   I mean, when we talk about interpolating between stack overflow databases, if you look at the
[00:35:15.780 --> 00:35:22.100]   kinds of math problems or coding problems, maybe to say that there, maybe let's step
[00:35:22.100 --> 00:35:23.100]   back on interpolation.
[00:35:23.100 --> 00:35:24.780]   And let me ask the question this way.
[00:35:24.780 --> 00:35:26.100]   Why can't creativity?
[00:35:26.100 --> 00:35:31.300]   Why isn't creativity just interpolation in a higher dimension, where if a bigger model
[00:35:31.300 --> 00:35:36.060]   can learn a more complex manifold, if we're going to use the ML language, and if you look
[00:35:36.060 --> 00:35:40.380]   at read a biography of a scientist, right, it doesn't feel like they're not zero shotting
[00:35:40.380 --> 00:35:41.380]   new scientific theories.
[00:35:41.380 --> 00:35:43.260]   They're playing with existing ideas.
[00:35:43.260 --> 00:35:45.300]   They're trying to juxtapose them in their head.
[00:35:45.300 --> 00:35:51.660]   They try out some like slightly ever in the tree of evolution, intellectual descendants,
[00:35:51.660 --> 00:35:55.980]   they try out a different evolutionary path, we use sort of run the experiment there in
[00:35:55.980 --> 00:35:58.000]   terms of publishing the paper, whatever.
[00:35:58.000 --> 00:35:59.620]   It seems like a similar kind of thing humans are doing.
[00:35:59.620 --> 00:36:02.020]   There's like at a higher level of generalization.
[00:36:02.020 --> 00:36:05.340]   And what you see across bigger and bigger models is they can they seem to be approaching
[00:36:05.340 --> 00:36:09.300]   higher and higher level of generalization, where GBT to couldn't do a great school level
[00:36:09.300 --> 00:36:13.820]   math problem that requires more generalization that it has capability for even that skill,
[00:36:13.820 --> 00:36:15.900]   and GBT three and four can.
[00:36:15.900 --> 00:36:16.900]   So not quite.
[00:36:16.900 --> 00:36:22.940]   So GBT four has a higher degree of skill and higher range of skills, because the same semantics
[00:36:22.940 --> 00:36:25.740]   here, but some degree of generalization, I don't get a semantics here.
[00:36:25.740 --> 00:36:32.820]   But the question of why why can't creativity be just interpolation on a higher higher dimension?
[00:36:32.820 --> 00:36:35.540]   I think interpolation can be created, absolutely.
[00:36:35.540 --> 00:36:41.380]   And you know, to your point, I do think that on some level, humans also do a lot of memorization,
[00:36:41.380 --> 00:36:45.260]   a lot of reciting, a lot of pattern matching, a lot of interpolation as well.
[00:36:45.260 --> 00:36:51.940]   So it's it's very much a spectrum between between pattern matching and true reasoning.
[00:36:51.940 --> 00:36:52.940]   It's a spectrum.
[00:36:52.940 --> 00:36:57.580]   And humans are never really at one hand end of the spectrum.
[00:36:57.580 --> 00:37:00.540]   They're never really doing pure pattern matching or pure reasoning.
[00:37:00.540 --> 00:37:03.140]   They're usually doing some mixture of both.
[00:37:03.140 --> 00:37:07.660]   Even if you're doing something that seems very reasoning heavy, like proving a mathematical
[00:37:07.660 --> 00:37:12.140]   theorem, as you're doing it, sure, you're doing quite a bit of discrete search in your
[00:37:12.140 --> 00:37:14.500]   mind, quite a bit of actual reasoning.
[00:37:14.500 --> 00:37:20.380]   But you're also very much guided by intuition, guided by pattern matching, guided by the
[00:37:20.380 --> 00:37:25.140]   shape of proofs that you've seen before, by your knowledge of mathematics.
[00:37:25.140 --> 00:37:30.540]   So it's it's never really, you know, all of our thoughts, everything we do is a mixture
[00:37:30.540 --> 00:37:35.460]   of this sort of like interpolating memorization based thinking, this sort of like type one
[00:37:35.460 --> 00:37:40.560]   thinking and type two thinking.
[00:37:40.560 --> 00:37:43.540]   Why are bigger models more sample efficient?
[00:37:43.540 --> 00:37:51.620]   Because they have more reasonable building blocks that they can lean on to pick up new
[00:37:51.620 --> 00:37:53.180]   patterns in their training data.
[00:37:53.180 --> 00:37:56.140]   And does that pattern keep continuing as you keep getting bigger and bigger?
[00:37:56.140 --> 00:38:01.180]   To the extent that the new patterns you're giving the model to learn are a good match
[00:38:01.180 --> 00:38:03.100]   for what it has learned before.
[00:38:03.100 --> 00:38:06.820]   If you present something that's actually novel, that is not in a standard distribution, like
[00:38:06.820 --> 00:38:09.340]   an ARC puzzle, for instance, it will fail.
[00:38:09.340 --> 00:38:10.340]   Let me make this claim.
[00:38:10.340 --> 00:38:14.300]   The program synthesis, I think, is a very, very useful intuition pump.
[00:38:14.300 --> 00:38:18.040]   Why can't it be the case that what's happening in the transformer is the early layers are
[00:38:18.040 --> 00:38:22.380]   doing the figuring out how to represent the inputting tokens.
[00:38:22.380 --> 00:38:25.540]   And what the middle layers do is this kind of program search, program synthesis, where
[00:38:25.540 --> 00:38:32.300]   they combine the inputs to the, you know, all the circuits in the model where they go
[00:38:32.300 --> 00:38:35.660]   from the low level representation to a higher level of representation near the middle of
[00:38:35.660 --> 00:38:36.660]   the model.
[00:38:36.660 --> 00:38:39.860]   They use these programs and they do, they combine these concepts.
[00:38:39.860 --> 00:38:44.180]   Then what comes out at the other end is the reasoning based on that high level intelligence.
[00:38:44.180 --> 00:38:45.180]   Possibly.
[00:38:45.180 --> 00:38:46.180]   Why not?
[00:38:46.180 --> 00:38:52.820]   But, you know, if, if these models were actually capable of synthesizing, synthesizing novel
[00:38:52.820 --> 00:38:59.660]   programs, however simple, they should be able to do ARC because for any ARC task, if you
[00:38:59.660 --> 00:39:04.060]   write down the solution program in Python, it's not a complex program.
[00:39:04.060 --> 00:39:08.020]   It's extremely simple and, and, and humans can figure it out.
[00:39:08.020 --> 00:39:10.180]   So why, why can LLMs not do it?
[00:39:10.180 --> 00:39:11.180]   Okay.
[00:39:11.180 --> 00:39:13.300]   I think that's a fair, a fair point.
[00:39:13.300 --> 00:39:18.940]   And if I turn the question around to you, so suppose that it's the case that in a year,
[00:39:18.940 --> 00:39:25.220]   a multimodal model can solve ARC, let's say get 80%, whatever the average human would
[00:39:25.220 --> 00:39:26.220]   get.
[00:39:26.220 --> 00:39:27.220]   Sure.
[00:39:27.220 --> 00:39:28.220]   Then AGI?
[00:39:28.220 --> 00:39:29.220]   Quite possibly, yes.
[00:39:29.220 --> 00:39:30.220]   If you start.
[00:39:30.220 --> 00:39:38.040]   So honestly, what I would like to see is an LLM type model solving ARC at like 80%, but
[00:39:38.040 --> 00:39:42.940]   after having only been trained on core knowledge related stuff.
[00:39:42.940 --> 00:39:46.440]   But human kids, I don't think we're necessarily just traded on.
[00:39:46.440 --> 00:39:48.340]   It's not just that we have in our genes object permanence.
[00:39:48.340 --> 00:39:49.340]   Okay.
[00:39:49.340 --> 00:39:50.340]   Let me rephrase that.
[00:39:50.340 --> 00:39:51.340]   Yeah.
[00:39:51.340 --> 00:39:58.060]   Only trained on information that is not explicitly trying to anticipate what's going to be in
[00:39:58.060 --> 00:39:59.060]   the ARC test set.
[00:39:59.060 --> 00:40:03.140]   But isn't the whole point of ARC that you can't sort of, it's a new chart of type of
[00:40:03.140 --> 00:40:04.140]   intelligence every single time?
[00:40:04.140 --> 00:40:05.140]   Yes, that is the point.
[00:40:05.140 --> 00:40:09.700]   So if ARC were perfect, flawless benchmark, it would be impossible to anticipate what's
[00:40:09.700 --> 00:40:10.700]   in the test set.
[00:40:10.700 --> 00:40:15.660]   And, you know, ARC was released more than four years ago and so far it's been resistant
[00:40:15.660 --> 00:40:16.740]   to memorization.
[00:40:16.740 --> 00:40:22.940]   So I think it has to some extent passed the test of time, but I don't think it's perfect.
[00:40:22.940 --> 00:40:29.860]   I think if you try to make by hand hundreds of thousands of ARC tasks, and then you try
[00:40:29.860 --> 00:40:36.140]   to multiply them by programmatically generating variations, and then you end up with maybe
[00:40:36.140 --> 00:40:41.780]   hundreds of millions of tasks, just by brute forcing the task space, there will be enough
[00:40:41.780 --> 00:40:45.140]   overlap between what you're trained on and what's in the test set that you can actually
[00:40:45.140 --> 00:40:46.340]   score very highly.
[00:40:46.340 --> 00:40:49.660]   So you know, with enough scale, you can always cheat.
[00:40:49.660 --> 00:40:52.980]   If you can do this for every single thing that supposedly requires intelligence, then
[00:40:52.980 --> 00:40:53.980]   what good is intelligence?
[00:40:53.980 --> 00:40:55.980]   Apparently you can just brute force intelligence.
[00:40:55.980 --> 00:41:01.900]   If the world, if your life were a static distribution, then sure you could just brute force the space
[00:41:01.900 --> 00:41:03.900]   of possible behaviors.
[00:41:03.900 --> 00:41:10.020]   Like, you know, the way we think about intelligence, there are several metaphors I like to use,
[00:41:10.020 --> 00:41:16.140]   but one of them is you can think of intelligence as a pathfinding algorithm in future situation
[00:41:16.140 --> 00:41:17.140]   space.
[00:41:17.140 --> 00:41:20.620]   So like, I don't know if you're familiar with game development, like RTS game development,
[00:41:20.620 --> 00:41:23.740]   but you have a map, right?
[00:41:23.740 --> 00:41:28.900]   And you have, it's like a 2D map, and you have partial information about it.
[00:41:28.900 --> 00:41:31.900]   Like there is some fog of war on your map.
[00:41:31.900 --> 00:41:35.180]   There are areas that you haven't explored yet, you know nothing about them.
[00:41:35.180 --> 00:41:39.060]   And then there are areas that you've explored, but you only know how they were like in the
[00:41:39.060 --> 00:41:40.060]   past.
[00:41:40.060 --> 00:41:41.740]   You don't know how they are like today.
[00:41:41.740 --> 00:41:49.960]   And now instead of thinking about a 2D map, think about the space of possible future situations
[00:41:49.960 --> 00:41:53.180]   that you might encounter and how they're connected to each other.
[00:41:53.180 --> 00:41:54.500]   Intelligence is a pathfinding algorithm.
[00:41:54.500 --> 00:42:00.740]   So once you set a goal, it will tell you how to get there optimally.
[00:42:00.740 --> 00:42:06.020]   But of course it's constrained by the information you have.
[00:42:06.020 --> 00:42:09.300]   It cannot pathfind in an area that you know nothing about.
[00:42:09.300 --> 00:42:12.860]   It cannot also anticipate changes.
[00:42:12.860 --> 00:42:21.420]   And the thing is, if you had complete information about the map, then you could solve the pathfinding
[00:42:21.420 --> 00:42:28.060]   problem by simply memorizing every possible path, every mapping from point A to point
[00:42:28.060 --> 00:42:32.540]   B. You could solve the problem with pure memory.
[00:42:32.540 --> 00:42:37.020]   But the reason you cannot do that in real life is because you don't actually know what's
[00:42:37.020 --> 00:42:39.060]   going to happen in the future.
[00:42:39.060 --> 00:42:40.060]   Life is ever-changing.
[00:42:40.060 --> 00:42:43.100]   I feel like you're using words like memorization, which we would never use for human children.
[00:42:43.100 --> 00:42:47.660]   If your kid learns to do algebra and then now learns to do calculus, you wouldn't say
[00:42:47.660 --> 00:42:48.660]   they've memorized calculus.
[00:42:48.660 --> 00:42:53.700]   If they can just solve any arbitrary algebraic problem, you wouldn't say they've memorized
[00:42:53.700 --> 00:42:54.700]   algebra.
[00:42:54.700 --> 00:42:55.700]   They'd say they've learned algebra.
[00:42:55.700 --> 00:42:58.300]   Humans are never really doing pure memorization or pure reasoning.
[00:42:58.300 --> 00:43:01.260]   But that's only because you're semantically labeling when the human does the skill, it's
[00:43:01.260 --> 00:43:04.180]   a memorization, when the exact same skill is done by the LLM, as you can measure by
[00:43:04.180 --> 00:43:05.180]   these benchmarks.
[00:43:05.180 --> 00:43:06.900]   And you can just plug in any sort of math problem.
[00:43:06.900 --> 00:43:10.660]   Most humans are doing the exact same as the LLM is doing, which is just, for instance,
[00:43:10.660 --> 00:43:15.420]   I know if you learn to add numbers, you're memorizing an algorithm, you're memorizing
[00:43:15.420 --> 00:43:17.660]   a program, and then you can reapply it.
[00:43:17.660 --> 00:43:21.780]   You are not synthesizing on the fly the addition program.
[00:43:21.780 --> 00:43:24.660]   So obviously, at some point, some human had to figure out how to do addition.
[00:43:24.660 --> 00:43:29.760]   But the way a kid learns it is not that they sort of figure out from the absence of set
[00:43:29.760 --> 00:43:30.760]   theory how to do addition.
[00:43:30.760 --> 00:43:34.020]   I think what you learn in school is mostly memorization.
[00:43:34.020 --> 00:43:39.820]   So my claim is that, listen, these models are vastly underparameterized relative to
[00:43:39.820 --> 00:43:43.340]   how many flops or how many parameters you have in the human brain.
[00:43:43.340 --> 00:43:47.880]   And so yeah, they're not going to be like coming up with new theorems like the smartest
[00:43:47.880 --> 00:43:48.940]   humans can.
[00:43:48.940 --> 00:43:51.420]   But most humans can't do that either.
[00:43:51.420 --> 00:43:54.940]   What most humans do, it sounds like it's similar to what you're calling memorization, which
[00:43:54.940 --> 00:44:01.260]   is memorizing skills or memorizing, you know, techniques that you've learned.
[00:44:01.260 --> 00:44:03.820]   And so it sounds like it's compatible.
[00:44:03.820 --> 00:44:04.820]   Tell me if this is wrong.
[00:44:04.820 --> 00:44:09.420]   Is it compatible in your world if, like, all the remote workers are gone, but they're doing
[00:44:09.420 --> 00:44:12.180]   skills which we can potentially make synthetic data of?
[00:44:12.180 --> 00:44:16.260]   So we record everybody's screen and every single remote worker's screen.
[00:44:16.260 --> 00:44:18.860]   We sort of understand the skills they're performing there.
[00:44:18.860 --> 00:44:20.820]   And now we've trained a model that can do all this.
[00:44:20.820 --> 00:44:22.460]   All the remote workers are unemployed.
[00:44:22.460 --> 00:44:26.720]   We're generating trillions of dollars of economic activity from AI, remote workers.
[00:44:26.720 --> 00:44:29.140]   In that world, are we still in the memorization regime?
[00:44:29.140 --> 00:44:30.140]   So sure.
[00:44:30.140 --> 00:44:33.900]   With memorization, you can automate almost anything.
[00:44:33.900 --> 00:44:38.020]   As long as it's a static distribution, as long as you don't have to deal with change.
[00:44:38.020 --> 00:44:40.420]   Are most jobs part of such a static distribution?
[00:44:40.420 --> 00:44:44.500]   Potentially, there are lots of things that you can automate.
[00:44:44.500 --> 00:44:47.020]   And LLMs are an excellent tool for automation.
[00:44:47.020 --> 00:44:51.340]   And I think that's, but you have to understand that automation is not the same as intelligence.
[00:44:51.340 --> 00:44:53.220]   I'm not saying that LLMs are useless.
[00:44:53.220 --> 00:44:57.180]   I've been a huge proponent of deep learning for many years.
[00:44:57.180 --> 00:44:58.860]   And you know, for many years, I've been saying two things.
[00:44:58.860 --> 00:45:03.380]   I've been saying that if you keep scaling up deep learning, it will keep paying off.
[00:45:03.380 --> 00:45:06.340]   And at the same time, I've been saying, if you keep scaling up deep learning, this will
[00:45:06.340 --> 00:45:08.780]   not lead to AGI.
[00:45:08.780 --> 00:45:10.860]   So we can automate more and more things.
[00:45:10.860 --> 00:45:12.660]   And yes, this is economically valuable.
[00:45:12.660 --> 00:45:14.580]   And yes, potentially there are many jobs.
[00:45:14.580 --> 00:45:18.000]   You could automate a way like this and that would be economically valuable.
[00:45:18.000 --> 00:45:20.200]   But you're not, still not going to have intelligence.
[00:45:20.200 --> 00:45:24.260]   So you can ask, you know, okay, so what does it matter if we can generate all this economic
[00:45:24.260 --> 00:45:25.260]   value?
[00:45:25.260 --> 00:45:26.260]   Maybe we don't need intelligence at all.
[00:45:26.260 --> 00:45:31.500]   Well, you need intelligence the moment you have to deal with change, with novelty, with
[00:45:31.500 --> 00:45:32.500]   uncertainty.
[00:45:32.500 --> 00:45:39.460]   As long as you're in a space that can be exactly described in advance, you can just, you can
[00:45:39.460 --> 00:45:41.660]   just make a pure memorization, right?
[00:45:41.660 --> 00:45:44.180]   In fact, you can always solve any problem.
[00:45:44.180 --> 00:45:52.880]   You can always display arbitrary levels of skills on any task without leveraging any
[00:45:52.880 --> 00:45:59.500]   intelligence whatsoever, as long as it is possible to describe the problem and its solution
[00:45:59.500 --> 00:46:01.500]   very, very precisely.
[00:46:01.500 --> 00:46:05.420]   But when they do deal with novelty, then you just call it interpolation, right?
[00:46:05.420 --> 00:46:06.420]   And so.
[00:46:06.420 --> 00:46:07.420]   No, no, no.
[00:46:07.420 --> 00:46:09.980]   Interpolation is not enough to deal with all kinds of novelty.
[00:46:09.980 --> 00:46:13.220]   If it were, then LLMs would be, would be AGI.
[00:46:13.220 --> 00:46:14.380]   Well, I agree they're not AGI.
[00:46:14.380 --> 00:46:17.460]   I'm just trying to figure out how do we figure out we're on the path to AGI.
[00:46:17.460 --> 00:46:23.360]   And I think sort of crux here is maybe that it seems to me that these things are on a
[00:46:23.360 --> 00:46:27.360]   spectrum and we're clearly covering the earliest part of the spectrum with LLMs.
[00:46:27.360 --> 00:46:28.360]   I think so.
[00:46:28.360 --> 00:46:29.360]   And, oh, okay.
[00:46:29.360 --> 00:46:30.360]   Interesting.
[00:46:30.360 --> 00:46:34.360]   But here's another sort of thing that I think is evidence for this, grokking, right?
[00:46:34.360 --> 00:46:39.520]   So clearly, even within deep learning, there's a difference between the memorization regime
[00:46:39.520 --> 00:46:44.760]   and the generalization regime, where at first they'll just memorize the data set of, you
[00:46:44.760 --> 00:46:48.200]   know, if you're doing modular addition, how to add digits.
[00:46:48.200 --> 00:46:51.400]   And then at some point, if you keep training on that, they'll learn the skill.
[00:46:51.400 --> 00:46:55.940]   So the fact that there is that distinction suggests that the generalized circuit that
[00:46:55.940 --> 00:46:59.640]   deep learning can learn, there is a regime it enters where it generalizes.
[00:46:59.640 --> 00:47:02.920]   If you have an over-parameterized model, which you don't have in comparison to all the tasks
[00:47:02.920 --> 00:47:04.440]   we want these models to do right now.
[00:47:04.440 --> 00:47:06.400]   Grokking is a very, very old phenomenon.
[00:47:06.400 --> 00:47:09.720]   We've been observing it for decades.
[00:47:09.720 --> 00:47:15.480]   It's basically an instance of the minimum description length principle, where, sure,
[00:47:15.480 --> 00:47:22.880]   you can, given a problem, you can just memorize a point-wise input-to-output mapping, which
[00:47:22.880 --> 00:47:23.880]   is completely overfit.
[00:47:23.880 --> 00:47:29.800]   So it does not generalize at all, but it solves the problem on the trained data.
[00:47:29.800 --> 00:47:35.840]   And from there, you can actually keep pruning it, keep making your mapping simpler and simpler
[00:47:35.840 --> 00:47:40.720]   and more compressed, and at some point it will start generalizing.
[00:47:40.720 --> 00:47:45.080]   And so that's something called the minimum description length principle.
[00:47:45.080 --> 00:47:51.520]   It's this idea that the program that will generalize best is the shortest, right?
[00:47:51.520 --> 00:47:55.960]   And it doesn't mean that you're doing anything other than memorization, but you're doing
[00:47:55.960 --> 00:47:58.880]   memorization plus regularization, right?
[00:47:58.880 --> 00:47:59.880]   Okay.
[00:47:59.880 --> 00:48:00.880]   Generalization.
[00:48:00.880 --> 00:48:01.880]   Yeah.
[00:48:01.880 --> 00:48:03.600]   And that is absolutely, at least to generalization.
[00:48:03.600 --> 00:48:04.600]   Right.
[00:48:04.600 --> 00:48:05.720]   It's more efficient than one skill.
[00:48:05.720 --> 00:48:10.080]   But then the pattern you see here of meta-learning is that it's more efficient to store a program
[00:48:10.080 --> 00:48:14.760]   that can perform many skills rather than one skill, which is what we might call fluid intelligence.
[00:48:14.760 --> 00:48:17.920]   And so as you get bigger and bigger models, you would expect it to go up this hierarchy
[00:48:17.920 --> 00:48:21.600]   of generalization, where it generalizes to a skill, then it generalizes across multiple
[00:48:21.600 --> 00:48:22.600]   skills.
[00:48:22.600 --> 00:48:23.600]   That's correct.
[00:48:23.600 --> 00:48:24.600]   That's correct.
[00:48:24.600 --> 00:48:27.320]   And, you know, LLMs, they're not infinitely large.
[00:48:27.320 --> 00:48:32.360]   They have only a fixed number of parameters, and so they have to compress their knowledge
[00:48:32.360 --> 00:48:33.840]   as much as possible.
[00:48:33.840 --> 00:48:40.680]   And in practice, LLMs are mostly storing reusable bits of programs, like vector programs.
[00:48:40.680 --> 00:48:44.240]   And because they have this need for compression, it means that every time they're learning
[00:48:44.240 --> 00:48:49.720]   a new program, they're going to try to express it in terms of existing bits and pieces of
[00:48:49.720 --> 00:48:51.960]   programs that they've already learned before.
[00:48:51.960 --> 00:48:52.960]   Right?
[00:48:52.960 --> 00:48:54.920]   Isn't this the generalization?
[00:48:54.920 --> 00:48:55.920]   Absolutely.
[00:48:55.920 --> 00:48:56.920]   Oh, wait.
[00:48:56.920 --> 00:49:01.640]   So this is what, you know, clearly LLMs have some degree of generalization, and this is
[00:49:01.640 --> 00:49:03.960]   precisely why, it's because they have to compress.
[00:49:03.960 --> 00:49:05.720]   And why is that intrinsically limited?
[00:49:05.720 --> 00:49:09.520]   Why can't you just go, at some point it has to learn a higher level of generalization,
[00:49:09.520 --> 00:49:12.320]   a higher level, and then the highest level is the fluid intelligence.
[00:49:12.320 --> 00:49:18.480]   It's intrinsically limited because the substrate of your model is a big parametric curve.
[00:49:18.480 --> 00:49:22.040]   And all you can do with this is local generalization.
[00:49:22.040 --> 00:49:27.760]   If you want to go beyond this towards broader or an extreme generalization, you have to
[00:49:27.760 --> 00:49:30.040]   move to a different type of model.
[00:49:30.040 --> 00:49:36.120]   And my paradigm of choice is discrete program search, program synthesis.
[00:49:36.120 --> 00:49:40.640]   And if you want to understand that, you can sort of like compare it, contrast it with
[00:49:40.640 --> 00:49:41.640]   deep learning.
[00:49:41.640 --> 00:49:47.160]   So in deep learning, your model is a parametric curve, a differentiable parametric curve.
[00:49:47.160 --> 00:49:52.680]   In program synthesis, your model is a discrete graph of operators.
[00:49:52.680 --> 00:49:57.520]   You've got like a set of logical operators, like a domain specific language.
[00:49:57.520 --> 00:50:03.400]   You're picking instances of it, you're structuring that into a graph, that's a program.
[00:50:03.400 --> 00:50:08.520]   And that's actually very similar to like a program you might write in Python or C++ and
[00:50:08.520 --> 00:50:09.600]   so on.
[00:50:09.600 --> 00:50:13.560]   And in deep learning, your learning engine, because we are doing machine learning here,
[00:50:13.560 --> 00:50:16.880]   like we're trying to automatically learn these models.
[00:50:16.880 --> 00:50:21.520]   In deep learning, your learning engine is gradient descent, right?
[00:50:21.520 --> 00:50:26.600]   And gradient descent is very compute efficient because you have this very strong informative
[00:50:26.600 --> 00:50:31.880]   feedback signal about where the solution is so you can get to the solution very quickly.
[00:50:31.880 --> 00:50:37.360]   But it is very data inefficient, meaning that in order to make it work, you need a dense
[00:50:37.360 --> 00:50:39.680]   sampling of the operating space.
[00:50:39.680 --> 00:50:41.920]   You need a dense sampling of the data distribution.
[00:50:41.920 --> 00:50:46.200]   And then you're limited to only generalizing within that data distribution.
[00:50:46.200 --> 00:50:50.160]   And the reason why you have this limitation is because your model is a curve.
[00:50:50.160 --> 00:50:56.160]   And meanwhile, if you look at discrete program search, the learning engine is combinatorial
[00:50:56.160 --> 00:50:57.160]   search.
[00:50:57.160 --> 00:51:02.000]   You're just trying a bunch of programs until you find one that actually meets your spec.
[00:51:02.000 --> 00:51:04.360]   This process is extremely data efficient.
[00:51:04.360 --> 00:51:08.680]   You can learn a generalizable program from just one example, two examples, which is why
[00:51:08.680 --> 00:51:10.960]   it works so well on Arc, by the way.
[00:51:10.960 --> 00:51:15.700]   But the big limitation is that it's extremely compute inefficient because you're running
[00:51:15.700 --> 00:51:18.920]   into a combinatorial explosion, of course.
[00:51:18.920 --> 00:51:27.280]   And so you can sort of see here how deep learning and discrete program search, they have very
[00:51:27.280 --> 00:51:30.060]   complementary strengths and limitations as well.
[00:51:30.060 --> 00:51:35.720]   Like every limitation of deep learning has a strength, a corresponding strength in program
[00:51:35.720 --> 00:51:37.560]   synthesis and in diversity.
[00:51:37.560 --> 00:51:42.160]   And I think the path forward is going to be to merge the two, to basically start doing.
[00:51:42.160 --> 00:51:47.960]   So another way you can think about it is, so this parametric curves train with ground
[00:51:47.960 --> 00:51:55.000]   descent, they're a great fit for everything that's a system one type thinking, like pattern
[00:51:55.000 --> 00:51:58.760]   cognition, intuition, memorization, and so on.
[00:51:58.760 --> 00:52:06.120]   And discrete program search is a great fit for a type two thinking, system two thinking,
[00:52:06.120 --> 00:52:11.940]   for instance, planning, reasoning, quickly figuring out a generalizable model that matches
[00:52:11.940 --> 00:52:16.240]   just one or two examples, like for an arc puzzle, for instance.
[00:52:16.240 --> 00:52:23.160]   And I think humans are never doing pure system one or pure system two, they're always mixing
[00:52:23.160 --> 00:52:24.720]   and matching both.
[00:52:24.720 --> 00:52:29.400]   And right now we have all the tools for system one, we have almost nothing for system two.
[00:52:29.400 --> 00:52:32.160]   The way forward is to create a hybrid system.
[00:52:32.160 --> 00:52:37.380]   And I think the form it's going to take is it's going to be mostly system two.
[00:52:37.380 --> 00:52:42.220]   So the outer structure is going to be a discrete program search system.
[00:52:42.220 --> 00:52:46.360]   But you're going to fix the fundamental limitation of discrete program search, which is commutator
[00:52:46.360 --> 00:52:49.580]   explosion, you're going to fix it with deep learning.
[00:52:49.580 --> 00:52:55.400]   You're going to leverage deep learning to guide, to provide intuition in program space
[00:52:55.400 --> 00:52:57.600]   to guide the program search.
[00:52:57.600 --> 00:53:03.720]   And I think that's very similar to what you see, for instance, when you're playing chess
[00:53:03.720 --> 00:53:11.640]   or when you're trying to prove a theorem is that it's mostly a reasoning thing, but you
[00:53:11.640 --> 00:53:15.520]   start out with some intuition about the shape of the solution.
[00:53:15.520 --> 00:53:19.800]   And that's very much something you can get via a deep learning model.
[00:53:19.800 --> 00:53:24.060]   Deep learning models, they're very much like intuition machines, they're pattern matching
[00:53:24.060 --> 00:53:25.060]   machines.
[00:53:25.060 --> 00:53:33.360]   So you start from this shape of the solution, and then you're going to do actual explicit
[00:53:33.360 --> 00:53:39.020]   discrete program search, but you're not going to do it via brute force, you're not going
[00:53:39.020 --> 00:53:42.720]   to try things kind of like randomly.
[00:53:42.720 --> 00:53:47.460]   You're actually going to ask another deep learning model for suggestions, like here's
[00:53:47.460 --> 00:53:52.200]   the best likely next step, here's where in the graph you should be going.
[00:53:52.200 --> 00:53:56.840]   And you can also use yet another deep learning model for feedback about, well, here's what
[00:53:56.840 --> 00:53:57.840]   I have so far.
[00:53:57.840 --> 00:53:58.840]   Is it looking good?
[00:53:58.840 --> 00:54:01.360]   Should I just backtrack and try something new?
[00:54:01.360 --> 00:54:07.880]   So I think discrete program search is going to be the key, but you want to make it dramatically
[00:54:07.880 --> 00:54:11.280]   better or does it make it more efficient by leveraging deep learning.
[00:54:11.280 --> 00:54:14.280]   And by the way, another thing that you can use deep learning for is of course things
[00:54:14.280 --> 00:54:19.160]   like common sense knowledge and knowledge in general.
[00:54:19.160 --> 00:54:24.140]   And I think you're going to end up with this sort of system where you have this on the
[00:54:24.140 --> 00:54:29.660]   fly synthesis engine that can adapt to new situations.
[00:54:29.660 --> 00:54:36.600]   But the way it adapts is that it's going to fetch from a bank of patterns, modules that
[00:54:36.600 --> 00:54:44.080]   could be themselves curves that could be differentiable modules and some others that could be algorithmic
[00:54:44.080 --> 00:54:45.080]   in nature.
[00:54:45.080 --> 00:54:50.520]   It's going to assemble them via this process that's intuition guided.
[00:54:50.520 --> 00:54:53.640]   And it's going to give you for every new situation you might be faced with, it's going to give
[00:54:53.640 --> 00:55:00.960]   you with a generalizable model that was synthesized using very, very little data.
[00:55:00.960 --> 00:55:02.760]   Something like this would sort of arc.
[00:55:02.760 --> 00:55:09.320]   That's actually a really interesting prompt because I think an interesting crux here is
[00:55:09.320 --> 00:55:16.680]   when I talk to my friends who are extremely optimistic about LLMs and expect AGI within
[00:55:16.680 --> 00:55:23.600]   the next couple of years, they also in some sense agree that scaling is not all you need.
[00:55:23.600 --> 00:55:29.160]   But that the rest of the progress is undergirded and enabled by scaling.
[00:55:29.160 --> 00:55:35.040]   But still, you need to add the system to the test time compute atop these models.
[00:55:35.040 --> 00:55:39.200]   And their perspective is that it's relatively straightforward to do that because you have
[00:55:39.200 --> 00:55:43.840]   this library of representations that you built up from free training.
[00:55:43.840 --> 00:55:48.720]   But it's almost talking like, you know, it just like skimming through textbooks, you
[00:55:48.720 --> 00:55:53.540]   need some more deliberate way in which it engages with the material it learns.
[00:55:53.540 --> 00:55:56.760]   In context learning is extremely sample efficient.
[00:55:56.760 --> 00:56:00.420]   But to actually distill that into the weights, you need the model to like talk through the
[00:56:00.420 --> 00:56:03.080]   things that it sees, and then add it back to the weights.
[00:56:03.080 --> 00:56:07.520]   As far as the system two goes, they talk about adding some kind of RL setup so that it is
[00:56:07.520 --> 00:56:13.000]   encouraged to proceed on the reasoning traces that end up being correct.
[00:56:13.000 --> 00:56:15.840]   And they think this is relatively straightforward stuff that will be added within the next couple
[00:56:15.840 --> 00:56:16.840]   of years.
[00:56:16.840 --> 00:56:17.840]   That's an empirical question.
[00:56:17.840 --> 00:56:18.840]   Yeah.
[00:56:18.840 --> 00:56:19.840]   So I think we'll see.
[00:56:19.840 --> 00:56:20.840]   Your intuition, I assume is not that.
[00:56:20.840 --> 00:56:21.840]   I'm curious.
[00:56:21.840 --> 00:56:27.360]   My intuition is, in fact, this whole like system two architecture is the hard part is
[00:56:27.360 --> 00:56:29.480]   the very hard and obvious part.
[00:56:29.480 --> 00:56:32.920]   Scaling up the interpolated memory is the easy part.
[00:56:32.920 --> 00:56:37.000]   All you need is like, it's literally just a big curve.
[00:56:37.000 --> 00:56:38.240]   All you need is more data.
[00:56:38.240 --> 00:56:42.080]   It's a representation of a data set, an interpolated representation of a data set.
[00:56:42.080 --> 00:56:43.080]   That's the easy part.
[00:56:43.080 --> 00:56:46.400]   The hard part is the architecture of intelligence.
[00:56:46.400 --> 00:56:48.680]   Memory and intelligence are separate components.
[00:56:48.680 --> 00:56:49.680]   We have the memory.
[00:56:49.680 --> 00:56:51.160]   We don't have the intelligence yet.
[00:56:51.160 --> 00:56:55.160]   And I agree with you that, well, having the memory is actually very useful.
[00:56:55.160 --> 00:56:59.080]   And if you just had the intelligence, but it was not hooked up to an extensive memory,
[00:56:59.080 --> 00:57:04.240]   it would not be that useful because it would not have enough material to work from.
[00:57:04.240 --> 00:57:05.240]   Yeah.
[00:57:05.240 --> 00:57:11.360]   The alternative hypothesis here that a former guest Trenton Brook advanced is that intelligence
[00:57:11.360 --> 00:57:17.720]   is just hierarchically associated memory where higher level patterns, when Sherlock Holmes
[00:57:17.720 --> 00:57:21.280]   goes into a crime scene and he's extremely sample efficient, he can just like look at
[00:57:21.280 --> 00:57:23.440]   a few clues and figure out who was a murderer.
[00:57:23.440 --> 00:57:28.200]   And the way he's able to do that is he has learned higher, higher level sort of associations.
[00:57:28.200 --> 00:57:30.520]   It's memory in some fundamental sense.
[00:57:30.520 --> 00:57:34.020]   But so here's one way to ask the question.
[00:57:34.020 --> 00:57:40.520]   In the brain, supposedly we do program synthesis, but it is just synapses connected to each
[00:57:40.520 --> 00:57:41.560]   other.
[00:57:41.560 --> 00:57:45.560]   And so physically it's got to be that you just query the right circuit, right?
[00:57:45.560 --> 00:57:46.560]   You are.
[00:57:46.560 --> 00:57:47.560]   Yeah, yeah, yeah.
[00:57:47.560 --> 00:57:48.560]   It's a matter of degree.
[00:57:48.560 --> 00:57:53.000]   But if you can learn it, if, you know, training in the environment that the human ancestors
[00:57:53.000 --> 00:57:56.880]   are trained in means you learn that those circuits, training on the same kinds of outputs
[00:57:56.880 --> 00:58:00.640]   that humans produce, which to replicate require these kinds of circuits, wouldn't that train
[00:58:00.640 --> 00:58:03.560]   the same kind of whatever humans have?
[00:58:03.560 --> 00:58:07.840]   You know, it's a matter of degree.
[00:58:07.840 --> 00:58:12.600]   If you have a system that has a memory and is only capable of doing local generalization
[00:58:12.600 --> 00:58:16.900]   from that, it's not going to be very adaptable.
[00:58:16.900 --> 00:58:23.320]   To be really general, you need the memory plus the ability to search to quite some depth
[00:58:23.320 --> 00:58:27.000]   to achieve, you know, broader, even extreme generalization.
[00:58:27.000 --> 00:58:34.800]   You know, like one of my favorites, a psychologist, Jean Piaget, who was the founder of developmental
[00:58:34.800 --> 00:58:37.760]   psychology, he had a very good quote about intelligence.
[00:58:37.760 --> 00:58:42.300]   He said, "Intelligence is what you use when you don't know what to do."
[00:58:42.300 --> 00:58:48.560]   And it's like, as a human living your life, in most situations you already know what to
[00:58:48.560 --> 00:58:53.560]   do because you've been in this situation before, you already have the answer, right?
[00:58:53.560 --> 00:58:58.360]   And you're only going to need to use intelligence when you're faced with novelty, with something
[00:58:58.360 --> 00:59:04.000]   you didn't expect, with something that you weren't prepared for, either by your own experience,
[00:59:04.000 --> 00:59:07.560]   your own life experience or by your evolutionary history.
[00:59:07.560 --> 00:59:13.800]   Like this day that you're living right now is different in some important ways from every
[00:59:13.800 --> 00:59:19.000]   day you've lived before, but it's also different from any day ever lived by any of your ancestors.
[00:59:19.000 --> 00:59:22.720]   And still you're capable of being functional, right?
[00:59:22.720 --> 00:59:23.720]   How is it possible?
[00:59:23.720 --> 00:59:29.160]   I'm not denying that generalization is extremely important and is the basis for intelligence.
[00:59:29.160 --> 00:59:30.160]   That's not the crux.
[00:59:30.160 --> 00:59:32.280]   The crux is like how much of that is happening in the models.
[00:59:32.280 --> 00:59:35.520]   But okay, let me ask a separate question.
[00:59:35.520 --> 00:59:38.660]   We might keep going in the circle here.
[00:59:38.660 --> 00:59:43.440]   The differences in intelligence between humans, maybe the intelligence test because of reasons
[00:59:43.440 --> 00:59:46.040]   you mentioned are not measuring it well, but clearly there's differences in intelligence
[00:59:46.040 --> 00:59:47.840]   between different humans.
[00:59:47.840 --> 00:59:49.480]   What is your explanation for what's going on there?
[00:59:49.480 --> 00:59:53.480]   Because I think that's sort of compatible with my story that there's a spectrum of generality
[00:59:53.480 --> 00:59:56.760]   and that these models are climbing up to a human level.
[00:59:56.760 --> 01:00:01.880]   And even some humans haven't even climbed up to the Einstein level or the Francois level.
[01:00:01.880 --> 01:00:02.880]   Right.
[01:00:02.880 --> 01:00:03.880]   So that's a great question.
[01:00:03.880 --> 01:00:09.080]   You know, there is extensive evidence that intelligence, difference in intelligence are
[01:00:09.080 --> 01:00:11.720]   mostly genetic in nature, right?
[01:00:11.720 --> 01:00:16.560]   Meaning that if you take someone who is not very intelligent, there is no amount of training
[01:00:16.560 --> 01:00:23.080]   of like training data you can expose that person to that would make them become Einstein.
[01:00:23.080 --> 01:00:28.680]   And this kind of points to the fact that you really need a better architecture, you need
[01:00:28.680 --> 01:00:33.960]   a better algorithm, and more training data is not in fact all you need.
[01:00:33.960 --> 01:00:35.880]   I think I agree with that.
[01:00:35.880 --> 01:00:42.120]   I think maybe a way I might phrase it is that the people who are smarter have in ML language
[01:00:42.120 --> 01:00:44.900]   better initializations.
[01:00:44.900 --> 01:00:48.080]   The neural wiring, if you just look at it, it's more efficient.
[01:00:48.080 --> 01:00:51.440]   They have maybe greater density of firing.
[01:00:51.440 --> 01:00:53.240]   And so some part of the story is scaling.
[01:00:53.240 --> 01:00:57.060]   There is some correlation between brain size and intelligence.
[01:00:57.060 --> 01:01:01.440]   And we also see within the context of quote unquote scaling that people talk about within
[01:01:01.440 --> 01:01:08.760]   the context of LLMs, architectural improvements, where a model like Gemini 1.5 flash is performs
[01:01:08.760 --> 01:01:15.440]   as well as GPT-4 did when GPT-4 was released a year ago, but is 57 times cheaper on output.
[01:01:15.440 --> 01:01:19.680]   So the part of the scaling story is that the architectural improvements are, we're in like
[01:01:19.680 --> 01:01:23.000]   extremely low hanging fruit territory when it comes to those.
[01:01:23.000 --> 01:01:29.060]   Okay, we're back now with the co-founder of Zapier, Mike Knouf.
[01:01:29.060 --> 01:01:31.560]   We had to restart a few times there.
[01:01:31.560 --> 01:01:36.100]   And you're funding this prize and you're running this prize with Francois.
[01:01:36.100 --> 01:01:39.860]   And so tell me about how this came together.
[01:01:39.860 --> 01:01:41.940]   What prompted you guys to launch this prize?
[01:01:41.940 --> 01:01:45.100]   Yeah, I guess I've been sort of like AI curious for 13 years.
[01:01:45.100 --> 01:01:48.860]   I've been, I co-founded Zapier, I've been running it for the last 13 years.
[01:01:48.860 --> 01:01:52.760]   And I think I first got introduced to your work in during COVID.
[01:01:52.760 --> 01:01:56.500]   I kind of went down the rabbit hole, I had a lot of free time.
[01:01:56.500 --> 01:02:00.220]   And it was right after you published your On Measure of Intelligence paper, you sort
[01:02:00.220 --> 01:02:03.780]   of introduced the concept of AGI, this like efficiency of skill acquisition is like the
[01:02:03.780 --> 01:02:06.460]   right definition and the arc puzzles.
[01:02:06.460 --> 01:02:09.020]   But I don't think the first Kaggle contest was done yet.
[01:02:09.020 --> 01:02:10.780]   I think it was still running.
[01:02:10.780 --> 01:02:15.260]   And so I kind of, it was interesting, but I just parked the idea.
[01:02:15.260 --> 01:02:19.140]   And I bigger fish to fry at Zapier, we're in this middle of this big turnaround of trying
[01:02:19.140 --> 01:02:21.380]   to get to our second product.
[01:02:21.380 --> 01:02:26.640]   And then it was January 2022, when the chain of thought paper came out, that really like
[01:02:26.640 --> 01:02:28.560]   awoken me to sort of the progress.
[01:02:28.560 --> 01:02:32.480]   I gave a whole presentation to the Zapier on like the GPT-3 paper events, I sort of
[01:02:32.480 --> 01:02:34.400]   felt like I had priced in everything that Elms could do.
[01:02:34.400 --> 01:02:38.680]   And that paper was really shocking to me in terms of all these latent capabilities that
[01:02:38.680 --> 01:02:41.780]   Elms have that I didn't expect that they had.
[01:02:41.780 --> 01:02:46.360]   And so I actually gave up my exec team role at Zapier, I was running half the company
[01:02:46.360 --> 01:02:50.800]   at that point, I went back to be an individual contributor, and just do to go do AI research
[01:02:50.800 --> 01:02:54.140]   alongside Brian, my co founder.
[01:02:54.140 --> 01:02:58.160]   And ultimately, that led me to back towards Arc, I was looking into it again, and I had
[01:02:58.160 --> 01:03:04.360]   sort of expected to see this, you know, saturation effect that you know, MMLU has that GMSK has
[01:03:04.360 --> 01:03:05.960]   8K has.
[01:03:05.960 --> 01:03:09.960]   And when I looked at the scores and the progress that since the last four years, I was really
[01:03:09.960 --> 01:03:14.700]   again shocked to see, actually, we've made very little objective progress towards it.
[01:03:14.700 --> 01:03:18.000]   And it felt very, it's like a really, really important eval.
[01:03:18.000 --> 01:03:21.440]   And as I sort of spent the last year asking people, quizzing people about it, and sort
[01:03:21.440 --> 01:03:26.400]   of my network and community, very few people, few people even knew it existed.
[01:03:26.400 --> 01:03:32.040]   And that felt like, okay, if it's right, that this is a really, really, like globally, singularly
[01:03:32.040 --> 01:03:34.900]   unique AGI eval.
[01:03:34.900 --> 01:03:38.560]   And it's different from every other eval that exists that are more that more narrowly measures
[01:03:38.560 --> 01:03:40.280]   AI skill.
[01:03:40.280 --> 01:03:41.800]   Like more people should know about this thing.
[01:03:41.800 --> 01:03:44.440]   I had my own ideas on how to beat the Arc as well.
[01:03:44.440 --> 01:03:48.160]   So I'm like, I was working on nights and weekends on that, and I flew up to meet Francois earlier
[01:03:48.160 --> 01:03:51.800]   this year, to sort of quiz him, show him my ideas.
[01:03:51.800 --> 01:03:55.640]   And ultimately, I was like, well, you know, why don't you think more people know about
[01:03:55.640 --> 01:03:56.640]   Arc?
[01:03:56.640 --> 01:03:57.640]   I think you should actually answer that.
[01:03:57.640 --> 01:04:00.600]   I think it's a really interesting question, like, why don't you think more people know
[01:04:00.600 --> 01:04:01.600]   about Arc?
[01:04:01.600 --> 01:04:02.600]   Sure.
[01:04:02.600 --> 01:04:06.560]   You know, I think benchmarks that gain traction in the research community are benchmarks that
[01:04:06.560 --> 01:04:08.920]   are already fairly tractable.
[01:04:08.920 --> 01:04:12.840]   Because the dynamic that you see is that some research group is going to make some initial
[01:04:12.840 --> 01:04:16.880]   breakthrough, and then this is going to catch the attention of everyone else.
[01:04:16.880 --> 01:04:21.240]   And so you're going to get follow up papers with people trying to beat the first team
[01:04:21.240 --> 01:04:22.240]   and so on.
[01:04:22.240 --> 01:04:27.040]   And for Arc, this has not really happened because Arc is actually very hard for existing
[01:04:27.040 --> 01:04:28.040]   AI techniques.
[01:04:28.040 --> 01:04:31.080]   Kind of Arc requires you to try new ideas.
[01:04:31.080 --> 01:04:35.520]   And that's very much the point, by the way, like the point is not that, yeah, you should
[01:04:35.520 --> 01:04:38.480]   just be able to apply existing technology and sort of Arc.
[01:04:38.480 --> 01:04:43.000]   The point is that existing technology has reached a plateau.
[01:04:43.000 --> 01:04:47.640]   And if you want to go beyond that, if you want to start being able to tackle problems
[01:04:47.640 --> 01:04:52.720]   that you haven't memorized, that you haven't seen before, you need to try new ideas.
[01:04:52.720 --> 01:05:01.160]   And Arc is not just meant to be this sort of measure of how close we are to AGI.
[01:05:01.160 --> 01:05:03.880]   It's also meant to be a source of inspiration.
[01:05:03.880 --> 01:05:08.320]   Like I want, I want researchers to look at these puzzles and be like, hey, it's really
[01:05:08.320 --> 01:05:15.500]   strange that these puzzles are so simple and most humans can just do them very quickly.
[01:05:15.500 --> 01:05:18.680]   Why is it so hard for existing AI systems?
[01:05:18.680 --> 01:05:20.880]   Why is it so hard for LLMs and so on?
[01:05:20.880 --> 01:05:25.800]   And it's true for LLMs, but Arc was actually released before LLMs were really a thing.
[01:05:25.800 --> 01:05:30.320]   And the only thing that made it special at the time was that it was designed to be a
[01:05:30.320 --> 01:05:37.640]   resistance to memorization and the fact that it has survived LLMs and general so well kind
[01:05:37.640 --> 01:05:40.400]   of shows that yes, it is actually resistant to memorization.
[01:05:40.400 --> 01:05:44.160]   This is what nerds night me because I went and took a bunch of the puzzles myself.
[01:05:44.160 --> 01:05:45.960]   I've showed it to all my friends and family too.
[01:05:45.960 --> 01:05:49.560]   And they're all like, oh yeah, this is like super easy.
[01:05:49.560 --> 01:05:51.320]   Are you sure AI can't solve this?
[01:05:51.320 --> 01:05:54.280]   Like that's the reaction in the same one for me as well.
[01:05:54.280 --> 01:05:55.840]   And the more you dig in, you're like, okay, yep.
[01:05:55.840 --> 01:05:58.920]   There's not just empirical evidence over the last four years that it's unbeaten, but there's
[01:05:58.920 --> 01:06:02.880]   theoretical like concepts behind why.
[01:06:02.880 --> 01:06:05.960]   And I completely agree at this point that like new ideas basically are needed to be
[01:06:05.960 --> 01:06:06.960]   dark.
[01:06:06.960 --> 01:06:10.760]   And there's a lot of current trends in the world that are actually, I think working against
[01:06:10.760 --> 01:06:11.760]   that happening.
[01:06:11.760 --> 01:06:16.040]   Basically, I think we're actually less likely to generate new ideas right now.
[01:06:16.040 --> 01:06:19.520]   You know, I think one of the kind of trends is the closing up Frontier Research, right?
[01:06:19.520 --> 01:06:22.720]   The GPT-4 paper from opening, I had no technical details shared.
[01:06:22.720 --> 01:06:26.240]   The Gemini paper had no technical details shared in like the longer context part of
[01:06:26.240 --> 01:06:27.240]   that work.
[01:06:27.240 --> 01:06:31.440]   And yet that open innovation and open progress and sharing is what got us to Transformers
[01:06:31.440 --> 01:06:32.440]   in the first place.
[01:06:32.440 --> 01:06:35.200]   That's what got us to LM's in the first place.
[01:06:35.200 --> 01:06:39.360]   So it's kind of disappointing a little bit, actually, that like so much Frontier work
[01:06:39.360 --> 01:06:40.360]   has gone closed.
[01:06:40.360 --> 01:06:43.840]   It's really making a bet that like these individual labs are going to have the breakthrough and
[01:06:43.840 --> 01:06:46.360]   not the ecosystem is going to have the breakthrough.
[01:06:46.360 --> 01:06:49.480]   And I think sort of the internet open source has shown that that's like the most powerful
[01:06:49.480 --> 01:06:52.400]   innovation ecosystem that's ever existed, probably in the entire world.
[01:06:52.400 --> 01:06:58.280]   I think that's actually really sad that Frontier Research is no longer being published.
[01:06:58.280 --> 01:07:03.080]   If you look back, you know, four years ago, well, everything was just openly shared.
[01:07:03.080 --> 01:07:06.040]   Like all the state-of-the-art results were published.
[01:07:06.040 --> 01:07:07.320]   And this is no longer the case.
[01:07:07.320 --> 01:07:11.520]   And it's very much, you know, OpenAI single-handedly changed the game.
[01:07:11.520 --> 01:07:18.960]   And I think OpenAI basically set back progress towards HCI by quite a few years, probably
[01:07:18.960 --> 01:07:21.160]   like five to 10 years for two reasons.
[01:07:21.160 --> 01:07:28.320]   And one is that, well, they caused this complete closing down of Frontier Research publishing.
[01:07:28.320 --> 01:07:35.800]   But also, they triggered this initial burst of hype around LLMs.
[01:07:35.800 --> 01:07:39.760]   And now LLMs have sucked the oxygen out of the room.
[01:07:39.760 --> 01:07:43.520]   Like everything, everyone is just doing LLMs.
[01:07:43.520 --> 01:07:50.000]   And I see LLMs as more of an off-ramp on the path to HCI, actually.
[01:07:50.000 --> 01:07:55.640]   And all these new resources, they're actually going to LLMs instead of everything else they
[01:07:55.640 --> 01:07:57.120]   could be going to.
[01:07:57.120 --> 01:08:03.360]   And you know, if you look further into the past, to like 2015, 2016, there were like
[01:08:03.360 --> 01:08:07.440]   a thousand times fewer people doing AI back then.
[01:08:07.440 --> 01:08:14.440]   And yet I feel like the rate of progress was higher because people are exploring more directions.
[01:08:14.440 --> 01:08:19.640]   The world felt more open-ended, like you could just go and try, like have a cool idea of
[01:08:19.640 --> 01:08:22.360]   a launch and try it and get some interesting results.
[01:08:22.360 --> 01:08:24.520]   So there was this energy.
[01:08:24.520 --> 01:08:29.000]   And now everyone is very much doing some variation of the same thing.
[01:08:29.000 --> 01:08:34.760]   And the big labs also tried their hand on Arc, but because they got bad results, they
[01:08:34.760 --> 01:08:35.760]   didn't publish anything.
[01:08:35.760 --> 01:08:39.400]   Like, you know, people only publish positive results.
[01:08:39.400 --> 01:08:47.120]   I wonder how much effort people have put into trying to prompt or scaffold, do some sort
[01:08:47.120 --> 01:08:52.840]   of maybe Devon type approach into getting the frontier models and the frontier models
[01:08:52.840 --> 01:08:56.440]   of today, not just a year ago, because a lot of post-training has gone into making them
[01:08:56.440 --> 01:08:57.440]   better.
[01:08:57.440 --> 01:09:04.680]   So CloudFury Opus or GPT-4.0 into getting good solutions on Arc.
[01:09:04.680 --> 01:09:09.080]   I hope that one of the things this episode does is get people to try out this open competition
[01:09:09.080 --> 01:09:14.360]   where they have to put in an open source model to compete, but also to like figure out if
[01:09:14.360 --> 01:09:20.160]   there maybe the capability is latent in Cloud Opus and just see if you can show that.
[01:09:20.160 --> 01:09:22.000]   I think that would be super interesting.
[01:09:22.000 --> 01:09:23.360]   So let's talk about the prize.
[01:09:23.360 --> 01:09:28.020]   How much do you win if you solve it, you know, get whatever percent on Arc?
[01:09:28.020 --> 01:09:30.600]   How much do you get if you get the best submission, but don't crack it?
[01:09:30.600 --> 01:09:34.000]   So we got a million dollar, actually a little over a million dollars, the prize pool, running
[01:09:34.000 --> 01:09:35.880]   the contest on an annual basis.
[01:09:35.880 --> 01:09:40.180]   We're going to, we're starting it today through the middle of November.
[01:09:40.180 --> 01:09:41.960]   And the goal is to get 85%.
[01:09:41.960 --> 01:09:45.320]   That's the lower bound of human average that you guys talked about earlier.
[01:09:45.320 --> 01:09:50.600]   And there's a $500,000 prize for the first team that can get to the 85% benchmark.
[01:09:50.600 --> 01:09:53.920]   We're also going to run, we don't expect that to happen this year, actually.
[01:09:53.920 --> 01:09:59.440]   One of the early statisticians at Zapier gave me this line that has always stuck with me,
[01:09:59.440 --> 01:10:01.340]   that the longer it takes, the longer it takes.
[01:10:01.340 --> 01:10:05.480]   So my prior is that like Arc is going to take years to solve.
[01:10:05.480 --> 01:10:08.360]   And so we're going to keep doing, we're also going to break down and do a progress price
[01:10:08.360 --> 01:10:09.360]   this year.
[01:10:09.360 --> 01:10:13.840]   There's a $100,000 progress price, which we will pay out to the top scores.
[01:10:13.840 --> 01:10:19.600]   So $50,000 is going to go to the top objective scores this year on the Kaggle leaderboard,
[01:10:19.600 --> 01:10:21.440]   which we're hosting it on Kaggle.
[01:10:21.440 --> 01:10:26.240]   And then we're going to have a $50,000 pot set for a paper award for the best paper that
[01:10:26.240 --> 01:10:30.440]   explains conceptually the scores that they were able to achieve.
[01:10:30.440 --> 01:10:33.800]   And one of the, I think, interesting things we're also going to be doing is we're going
[01:10:33.800 --> 01:10:37.840]   to be requiring that in order to win the prize money, that you put the solution or your paper
[01:10:37.840 --> 01:10:40.520]   out into public domain.
[01:10:40.520 --> 01:10:44.760]   The reason for this is, you know, typically with contests, you see a lot of like closed
[01:10:44.760 --> 01:10:47.680]   up sharing, people are kind of private secret, they want to hold their alpha to themselves
[01:10:47.680 --> 01:10:49.280]   during the contest period.
[01:10:49.280 --> 01:10:53.240]   And because we expect it's going to be multiple years, we want to enter a game here.
[01:10:53.240 --> 01:10:58.560]   So the plan is, you know, at the end of November, we will award the $100,000 prize money to
[01:10:58.560 --> 01:11:04.320]   the top progress prize, and then use the downtime between December, January, February, to share
[01:11:04.320 --> 01:11:08.480]   out all the knowledge from the top scores and the approaches folks were taking in order
[01:11:08.480 --> 01:11:12.120]   to rebaseline the community up to whatever the state of the art is, and then run the
[01:11:12.120 --> 01:11:16.360]   contest again next year, and keep doing that on a yearly basis until we get 85%.
[01:11:16.360 --> 01:11:20.400]   I'll give some people some context on why I think this prize is very interesting.
[01:11:20.400 --> 01:11:25.240]   I was having conversations with my friends who are very much believers in models as they
[01:11:25.240 --> 01:11:26.800]   exist today.
[01:11:26.800 --> 01:11:30.160]   And first of all, it was intriguing to me that they didn't know about Arc.
[01:11:30.160 --> 01:11:32.440]   These are experienced ML researchers.
[01:11:32.440 --> 01:11:36.580]   And so you showed them, this happened a couple nights ago, we went to dinner and I showed
[01:11:36.580 --> 01:11:38.420]   them an example problem.
[01:11:38.420 --> 01:11:41.120]   And they said, of course, an LLM would be able to solve something like this.
[01:11:41.120 --> 01:11:44.680]   And then we take a screenshot of it, we just put it into our chat GPT app, and it doesn't
[01:11:44.680 --> 01:11:46.000]   get the pattern.
[01:11:46.000 --> 01:11:48.720]   And so I think it's very interesting.
[01:11:48.720 --> 01:11:52.000]   Like it is a notable fact, I was sort of playing devil's advocate against you on these kinds
[01:11:52.000 --> 01:11:53.000]   of questions.
[01:11:53.000 --> 01:11:54.000]   But this is a very intriguing fact.
[01:11:54.000 --> 01:11:59.320]   And I think this prize is extremely interesting, because we're going to learn something fascinating
[01:11:59.320 --> 01:12:01.440]   one way or another.
[01:12:01.440 --> 01:12:05.880]   So with regards to the 85%, separate from this prize, I'd be very curious if somebody
[01:12:05.880 --> 01:12:08.160]   could replicate that result.
[01:12:08.160 --> 01:12:14.080]   Because obviously, in psychology and other kinds of fields, which this result seems to
[01:12:14.080 --> 01:12:19.720]   be analogous to when you run tests on some small sample of people, often they're hard
[01:12:19.720 --> 01:12:20.720]   to replicate.
[01:12:20.720 --> 01:12:23.940]   So I'd be very curious if you try to replicate this, how what does the average human perform
[01:12:23.940 --> 01:12:25.640]   on Arc?
[01:12:25.640 --> 01:12:29.840]   As for the difficulty on how long it will take to crack this benchmark, it's very interesting,
[01:12:29.840 --> 01:12:34.720]   because the other benchmarks that are now fully saturated, like MMLU math, actually
[01:12:34.720 --> 01:12:39.920]   the people who made them, Dan Hendricks and Colin Burns, who did MMLU in math, I think
[01:12:39.920 --> 01:12:43.400]   they were grad students or college students when they made it.
[01:12:43.400 --> 01:12:47.020]   And the goal when they made it just a couple of years ago was that this will be a test
[01:12:47.020 --> 01:12:48.020]   of AGI.
[01:12:48.020 --> 01:12:49.760]   And of course, it got totally saturated.
[01:12:49.760 --> 01:12:52.560]   I know you'll argue that these are tests of memorization.
[01:12:52.560 --> 01:12:57.160]   But I think the pattern we've seen, in fact, Epoch AI has a very interesting graph that
[01:12:57.160 --> 01:13:02.080]   I'll sort of overlay for the YouTube version here, where you see this almost exponential
[01:13:02.080 --> 01:13:07.960]   where it gets, you know, 5%, 10%, 30%, 40%, as you increase the compute across models,
[01:13:07.960 --> 01:13:10.320]   and then it just shoots up.
[01:13:10.320 --> 01:13:15.840]   And in the GPT-4 technical report, they had this interesting graph of the human eval problem
[01:13:15.840 --> 01:13:18.640]   set, which was 22 coding problems.
[01:13:18.640 --> 01:13:25.080]   And they had to graph it on the mean log pass curve, basically, because early on in training,
[01:13:25.080 --> 01:13:29.920]   or even smaller models can have the right idea of how to solve this problem, but it
[01:13:29.920 --> 01:13:34.160]   takes a lot of reliability to make sure they stay on track to solve the whole problem.
[01:13:34.160 --> 01:13:37.320]   And so you really want to upweigh the signal where they get it right at least some of the
[01:13:37.320 --> 01:13:39.760]   time, maybe one in 100 times or one in 1,000.
[01:13:39.760 --> 01:13:42.980]   And then so they go from like one in 1,000, one in 100, one in 10, and then they just
[01:13:42.980 --> 01:13:44.560]   like totally saturate it.
[01:13:44.560 --> 01:13:49.040]   I guess the question I have, this is all leading up to is, why won't the same thing happen
[01:13:49.040 --> 01:13:55.760]   with Arc, where people had to try really hard, bigger models, and now they figured out these
[01:13:55.760 --> 01:13:59.400]   techniques that Jack Cole has figured out with only a 240 million parameter language
[01:13:59.400 --> 01:14:02.560]   model that can get 35%.
[01:14:02.560 --> 01:14:04.640]   Shouldn't we see the same pattern we saw across all these other benchmarks where you just
[01:14:04.640 --> 01:14:08.640]   like sort of eek out, and then once you get the general idea, then you just go all the
[01:14:08.640 --> 01:14:09.640]   way to 100?
[01:14:09.640 --> 01:14:11.000]   That's an empirical question.
[01:14:11.000 --> 01:14:13.960]   So we'll see in practice what happens.
[01:14:13.960 --> 01:14:16.600]   But what Jack Cole is doing is actually very unique.
[01:14:16.600 --> 01:14:21.080]   It's not just pre-training an LLM and then prompting it, he's actually trying to do active
[01:14:21.080 --> 01:14:22.080]   inference.
[01:14:22.080 --> 01:14:23.080]   He's doing a test time, right?
[01:14:23.080 --> 01:14:24.080]   He's doing like test time fine-tuning.
[01:14:24.080 --> 01:14:25.840]   It's like test time fine-tuning.
[01:14:25.840 --> 01:14:30.600]   And this is actually trying to lift one of the key limitations of LLMs, which is that
[01:14:30.600 --> 01:14:32.680]   at inference time, they cannot learn anything new.
[01:14:32.680 --> 01:14:35.680]   They cannot adapt on the fly to what they're seeing.
[01:14:35.680 --> 01:14:38.800]   And he's actually trying to learn.
[01:14:38.800 --> 01:14:44.320]   So what he's doing is effectively a form of program synthesis.
[01:14:44.320 --> 01:14:48.840]   Because the LLM contains a lot of useful building blocks, like programming building blocks.
[01:14:48.840 --> 01:14:53.840]   And by fine-tuning it on the task at test time, you are trying to assemble these building
[01:14:53.840 --> 01:14:57.440]   blocks into the right pattern that matches the task.
[01:14:57.440 --> 01:15:00.760]   This is exactly what program synthesis is about.
[01:15:00.760 --> 01:15:06.360]   And the way we contrast this approach with discrete program search is that in discrete
[01:15:06.360 --> 01:15:12.280]   program search, so you're trying to assemble a program from a set of primitives.
[01:15:12.280 --> 01:15:13.440]   You have very few primitives.
[01:15:13.440 --> 01:15:17.320]   So people working on discrete program search on Arc, for instance, they tend to work with
[01:15:17.320 --> 01:15:22.040]   DSLs that have like 100 to 200 primitive programs.
[01:15:22.040 --> 01:15:29.040]   So very small DSL, but then they're trying to combine these primitives into very complex
[01:15:29.040 --> 01:15:30.040]   programs.
[01:15:30.040 --> 01:15:32.520]   So there's a very deep depth of search.
[01:15:32.520 --> 01:15:39.440]   And on the other hand, if you look at what Jekyll is doing with LLMs is that he's got
[01:15:39.440 --> 01:15:46.680]   this sort of like vector program database DSL of millions of building blocks in the
[01:15:46.680 --> 01:15:52.860]   LLM that are mined by pre-training the LLM, not just on a ton of programming problems,
[01:15:52.860 --> 01:15:56.520]   but also on millions of generated Arc-like tasks.
[01:15:56.520 --> 01:15:59.800]   So you have an extraordinarily large DSL.
[01:15:59.800 --> 01:16:05.040]   And then the fine-tuning is very, very shallow recombination of these primitives.
[01:16:05.040 --> 01:16:12.220]   So discrete program search, very deep recombination, very small set of primitive programs.
[01:16:12.220 --> 01:16:17.560]   And the LLM approach is the same, but on the complete opposite end of that spectrum where
[01:16:17.560 --> 01:16:22.120]   you scale up the memorization by a massive factor and you're doing very, very shallow
[01:16:22.120 --> 01:16:23.660]   search.
[01:16:23.660 --> 01:16:27.560]   But they are the same thing, just different ends of the spectrum.
[01:16:27.560 --> 01:16:33.960]   And I think where you're going to get the most value for your compute cycles is going
[01:16:33.960 --> 01:16:36.300]   to be somewhere in between.
[01:16:36.300 --> 01:16:43.360]   You want to leverage memorization to build up a richer, more useful bank of primitive
[01:16:43.360 --> 01:16:44.360]   programs.
[01:16:44.360 --> 01:16:48.440]   And you don't want them to be hard-coded, like what we saw for the typical Arc DSL.
[01:16:48.440 --> 01:16:51.320]   You want them to be learned from examples.
[01:16:51.320 --> 01:16:56.000]   But then you also want to do some degree of deep search.
[01:16:56.000 --> 01:17:00.240]   As long as you're only doing very shallow search, you are limited to local generalization.
[01:17:00.240 --> 01:17:07.640]   If you want to generalize further, more broadly, this depth of search is going to be critical.
[01:17:07.640 --> 01:17:13.080]   I might argue that the reason that he had to rely so heavily on the synthetic data was
[01:17:13.080 --> 01:17:19.520]   because he used a 240 million parameter model because the Kaggle competition at the time
[01:17:19.520 --> 01:17:25.400]   required him to use a P100 GPU, which has like a tenth or something of the flops of
[01:17:25.400 --> 01:17:26.600]   an H100.
[01:17:26.600 --> 01:17:30.360]   And so obviously he can't use it.
[01:17:30.360 --> 01:17:35.040]   If you believe that scaling will solve this kind of reasoning, then there you can just
[01:17:35.040 --> 01:17:36.760]   rely on the generalization.
[01:17:36.760 --> 01:17:40.040]   Whereas if you're using a much smaller model, for context for the listeners by the way,
[01:17:40.040 --> 01:17:43.600]   the frontier models today are literally a thousand X bigger than that.
[01:17:43.600 --> 01:17:50.920]   And so for your competition, from what I remember, the submission you have to submit can't make
[01:17:50.920 --> 01:17:57.320]   any API calls, can't go online, and has to run on an NVIDIA Tesla T4.
[01:17:57.320 --> 01:17:58.320]   P100.
[01:17:58.320 --> 01:17:59.320]   P100.
[01:17:59.320 --> 01:18:00.320]   Oh, is it P100?
[01:18:00.320 --> 01:18:01.320]   Yeah.
[01:18:01.320 --> 01:18:02.320]   Okay.
[01:18:02.320 --> 01:18:03.320]   So again, it's like significantly less powerful.
[01:18:03.320 --> 01:18:04.320]   There's a 12-hour runtime limit, basically.
[01:18:04.320 --> 01:18:05.920]   There's a forcing function of efficiency in the eval.
[01:18:05.920 --> 01:18:07.080]   But here's the thing.
[01:18:07.080 --> 01:18:09.700]   You only have 100 test tasks.
[01:18:09.700 --> 01:18:13.560]   So the amount of compute available for each task is actually quite a bit, especially if
[01:18:13.560 --> 01:18:16.680]   you contrast that with the simplicity of each task.
[01:18:16.680 --> 01:18:22.440]   So it would be seven minutes per task, basically, which people have tried to do these estimates
[01:18:22.440 --> 01:18:24.680]   of how many flops does a human brain have.
[01:18:24.680 --> 01:18:26.180]   And you can take them with a grain of salt.
[01:18:26.180 --> 01:18:31.440]   But as a sort of anchor, it's basically the amount of flops an H100 has.
[01:18:31.440 --> 01:18:35.680]   And I guess maybe you would argue that, well, a human brain can solve this question in faster
[01:18:35.680 --> 01:18:36.680]   than 7.2 minutes.
[01:18:36.680 --> 01:18:40.200]   So even with a tenth of the compute, you should be able to do it in seven minutes.
[01:18:40.200 --> 01:18:45.080]   Obviously, we have less memory than, you know, like petabytes of fast access memory in the
[01:18:45.080 --> 01:18:46.240]   brain.
[01:18:46.240 --> 01:18:49.640]   And with these, you know, 29 or whatever gigabytes in this H100.
[01:18:49.640 --> 01:18:57.920]   Anyway, I guess the broader question I'm asking is, I wish there was a way to also test this
[01:18:57.920 --> 01:19:03.120]   prize with some sort of scaffolding on the biggest models as a way to test whether scaling
[01:19:03.120 --> 01:19:07.960]   is the path to get to, you know, solving ARC.
[01:19:07.960 --> 01:19:08.960]   Absolutely.
[01:19:08.960 --> 01:19:13.000]   In the context of the competition, we want to see how much progress we can do with limited
[01:19:13.000 --> 01:19:14.000]   resources.
[01:19:14.000 --> 01:19:17.520]   But you're entirely right that it's a super interesting open question.
[01:19:17.520 --> 01:19:20.680]   What could the biggest model out there actually do on ARC?
[01:19:20.680 --> 01:19:27.360]   So we want to actually also make available a private sort of like one-off track where
[01:19:27.360 --> 01:19:29.520]   you can submit to us a VM.
[01:19:29.520 --> 01:19:33.880]   And so you can put on it any model you want, like you can take one of the largest open
[01:19:33.880 --> 01:19:40.000]   source models out there, fine, you can do whatever you want, and just give us an image.
[01:19:40.000 --> 01:19:43.920]   And then we run it on the H100 for like 24 hours or something.
[01:19:43.920 --> 01:19:44.920]   And you see what you get.
[01:19:44.920 --> 01:19:48.580]   I think it's worth pointing out that there's two different test sets.
[01:19:48.580 --> 01:19:53.160]   There's a public test set that's in the public GitHub repository that anyone can use to train,
[01:19:53.160 --> 01:19:56.500]   you know, put it in an open API call, whatever you'd like to do.
[01:19:56.500 --> 01:19:59.280]   And then there's the private test set, which is the 100 that is actually measuring the
[01:19:59.280 --> 01:20:00.760]   state of the art.
[01:20:00.760 --> 01:20:04.120]   So I think it is pretty open-ended and interesting to have folks attempt to at least use the
[01:20:04.120 --> 01:20:06.120]   public test set and go try it.
[01:20:06.120 --> 01:20:10.320]   Now, there is an asterisk on any score that's reported on against the public test set because
[01:20:10.320 --> 01:20:11.360]   it is public.
[01:20:11.360 --> 01:20:13.440]   It could have leaked into the training data somewhere.
[01:20:13.440 --> 01:20:15.200]   And this is actually what people are already doing.
[01:20:15.200 --> 01:20:20.600]   Like, you can already try to prompt one of the best models, like the latest Gemini, the
[01:20:20.600 --> 01:20:24.560]   latest GPT-4, with tasks from the public evaluation set.
[01:20:24.560 --> 01:20:30.160]   And you know, again, the problem is that these tasks are available as JSON files on GitHub.
[01:20:30.160 --> 01:20:32.480]   These models are also trained on GitHub.
[01:20:32.480 --> 01:20:36.000]   So they're actually trained on these tasks.
[01:20:36.000 --> 01:20:40.240]   And yeah, that kind of creates uncertainty about if they can actually solve some of the
[01:20:40.240 --> 01:20:41.240]   tasks.
[01:20:41.240 --> 01:20:44.040]   Is that because they memorized the answer or not?
[01:20:44.040 --> 01:20:51.000]   You know, maybe you would be better off trying to create your own private, arc-like, very
[01:20:51.000 --> 01:20:53.320]   novel test set.
[01:20:53.320 --> 01:20:54.840]   Don't make the task difficult.
[01:20:54.840 --> 01:20:55.840]   Don't make them complex.
[01:20:55.840 --> 01:20:57.280]   Make them very obvious for humans.
[01:20:57.280 --> 01:21:02.600]   But make sure to make them original as much as possible, make them unique, different,
[01:21:02.600 --> 01:21:06.720]   and see how much your GPT-4 and so on, or GPT-5, does on them.
[01:21:06.720 --> 01:21:11.360]   Well, they're having tests on whether these models are being over-trained on these benchmarks.
[01:21:11.360 --> 01:21:17.720]   Scale recently did this where on the GSM, they basically replicated the benchmark, but
[01:21:17.720 --> 01:21:19.000]   with different questions.
[01:21:19.000 --> 01:21:23.280]   And so some of the models actually were extremely over-fit on the benchmark, like MISRAL and
[01:21:23.280 --> 01:21:24.720]   so forth.
[01:21:24.720 --> 01:21:30.480]   But the frontier models, CLAWD and GBT, actually did as well on their novel benchmark that
[01:21:30.480 --> 01:21:35.280]   they did on the specific questions that were in the existing public benchmark.
[01:21:35.280 --> 01:21:40.160]   So I would be relatively optimistic about them just sort of training on the JSON.
[01:21:40.160 --> 01:21:46.720]   I was joking with Mike that you should allow API access, but sort of keep an even more
[01:21:46.720 --> 01:21:51.600]   private validation set of these ARC questions.
[01:21:51.600 --> 01:21:55.360]   And so allow API access, people can sort of play with GPT-4 scaffolding to enter into
[01:21:55.360 --> 01:21:56.720]   this contest.
[01:21:56.720 --> 01:22:00.720]   And if it turns out, maybe later on you run the validation set on the API, and if it performs
[01:22:00.720 --> 01:22:05.720]   worse than the test set that you allowed the API access to originally, that means that
[01:22:05.720 --> 01:22:10.200]   OpenAI is training on your API calls, and you like go public with this and show them
[01:22:10.200 --> 01:22:13.160]   like, "Oh my God, they've like leaked your data."
[01:22:13.160 --> 01:22:17.280]   We do want to make, we want to evolve the ARC dataset, like that is a goal that we want
[01:22:17.280 --> 01:22:18.280]   to do.
[01:22:18.280 --> 01:22:19.280]   I think, Francois, you mentioned, you know, it's not perfect.
[01:22:19.480 --> 01:22:22.480]   Yeah, no, ARC is not perfect, the perfect benchmark.
[01:22:22.480 --> 01:22:26.840]   I mean, I made it like four years ago, over four years ago, almost five now.
[01:22:26.840 --> 01:22:29.120]   This was in a time before LLMs.
[01:22:29.120 --> 01:22:34.200]   And I think we learned a lot actually since about what potential flaws there might be.
[01:22:34.200 --> 01:22:39.480]   I think there is some redundancy in the set of tasks, which is of course against the goals
[01:22:39.480 --> 01:22:40.480]   of the benchmark.
[01:22:40.480 --> 01:22:42.840]   Every task is supposed to be unique in practice.
[01:22:42.840 --> 01:22:43.960]   That's not quite true.
[01:22:43.960 --> 01:22:49.320]   I think there's also, every task is supposed to be very novel, but in practice, they might
[01:22:49.320 --> 01:22:50.320]   not be.
[01:22:50.320 --> 01:22:54.800]   They might be structurally similar to something that you might find online somewhere.
[01:22:54.800 --> 01:23:00.440]   So we want to keep iterating and release an ARC2 version later this year.
[01:23:00.440 --> 01:23:06.920]   And I think when we do that, we're going to want to make the old private test set available.
[01:23:06.920 --> 01:23:12.040]   So maybe we won't be releasing it publicly, but what we could do is just create a test
[01:23:12.040 --> 01:23:17.360]   server where you can query, get a task, you submit a solution, and of course you can use
[01:23:17.360 --> 01:23:19.760]   whatever frontier model you want there.
[01:23:19.760 --> 01:23:23.920]   So that way, because you actually have to query the CPI, you're making sure that no
[01:23:23.920 --> 01:23:27.120]   one is going to by accident train on this data.
[01:23:27.120 --> 01:23:31.280]   It's unlike like the current public R today, which is literally on GitHub.
[01:23:31.280 --> 01:23:34.080]   So there's no question about whether the models are actually trained on it.
[01:23:34.080 --> 01:23:36.500]   Yes, they are, because they're trained on GitHub.
[01:23:36.500 --> 01:23:42.560]   So by sort of like gating access to querying this API, we would avoid this issue.
[01:23:42.560 --> 01:23:47.480]   And then we would see, you know, for people who actually want to try whatever technique
[01:23:47.480 --> 01:23:51.920]   they have in mind, using whatever resources they want, that would be a way for them to
[01:23:51.920 --> 01:23:52.920]   get an answer.
[01:23:52.920 --> 01:23:54.120]   I wonder what might happen.
[01:23:54.120 --> 01:23:55.560]   I'm not sure.
[01:23:55.560 --> 01:24:01.080]   One answer is that they've come up with a whole new algorithm for AI with something,
[01:24:01.080 --> 01:24:03.880]   some explicit program synthesis that now we're on a new track.
[01:24:03.880 --> 01:24:09.220]   And another is they did something hacky with the existing models in a way that actually
[01:24:09.220 --> 01:24:14.740]   is valid, which reveals that maybe intelligence is more of getting things to the right part
[01:24:14.740 --> 01:24:16.940]   of the distribution, but then it can reason.
[01:24:16.940 --> 01:24:19.780]   And in that world, I guess that will be interesting.
[01:24:19.780 --> 01:24:22.780]   And maybe that'll indicate that, you know, you had to do something hacky with current
[01:24:22.780 --> 01:24:27.100]   models as they get better, you won't have to do something hacky.
[01:24:27.100 --> 01:24:31.540]   I'm also going to be very curious to see how these multimodal models, if they will perform
[01:24:31.540 --> 01:24:33.940]   natively much better at ARC-like tests.
[01:24:33.940 --> 01:24:37.020]   If ARC survives three months from here, we'll up the price.
[01:24:37.020 --> 01:24:41.140]   I think we're about to make a really important moment of contact with reality by blowing
[01:24:41.140 --> 01:24:43.420]   up the price, putting a much big price pool against it.
[01:24:43.420 --> 01:24:46.260]   We're going to learn really quickly if there's like low-hanging fruit of ideas.
[01:24:46.260 --> 01:24:47.900]   Again, I think new ideas are needed.
[01:24:47.900 --> 01:24:51.380]   I think anyone listening to this might have the idea in their head.
[01:24:51.380 --> 01:24:53.620]   And I'd encourage everyone to like give it a try.
[01:24:53.620 --> 01:24:58.780]   And I think as time goes on, that adds strength to the argument that like we've sort of stalled
[01:24:58.780 --> 01:25:01.140]   out in progress and that new ideas are necessary to beat ARC.
[01:25:01.140 --> 01:25:06.740]   That's the point of having a money prize is that you attract more people, you get them
[01:25:06.740 --> 01:25:07.900]   to try to solve it.
[01:25:07.900 --> 01:25:11.540]   And if there's an easy way to hack the benchmark that reveals that the benchmark is flawed,
[01:25:11.540 --> 01:25:12.540]   then you're going to know about it.
[01:25:12.540 --> 01:25:19.020]   In fact, that was the point of the original Kaggle competition back in 2020 for ARC.
[01:25:19.020 --> 01:25:24.020]   I was running this competition because I had released this dataset and I wanted to know
[01:25:24.020 --> 01:25:26.700]   if it was hackable, if you could cheat.
[01:25:26.700 --> 01:25:31.700]   So there was a small money prize at the time, that was like 20K, and this was right around
[01:25:31.700 --> 01:25:34.180]   the same time as GPT-3 was released.
[01:25:34.180 --> 01:25:39.740]   So people, of course, tried GPT-3 on the public data, it scored zero.
[01:25:39.740 --> 01:25:49.540]   But I think what the first contest taught us is that there is no obvious shortcut, right?
[01:25:49.540 --> 01:25:53.620]   And well, now there's more money, there's going to be more people looking into it.
[01:25:53.620 --> 01:25:58.660]   And well, we're going to find out, we're going to see if the benchmark is going to survive.
[01:25:58.660 --> 01:26:06.060]   And you know, if we end up with a solution that is not like trying to brute force the
[01:26:06.060 --> 01:26:10.900]   space of possible ARC tasks, that's just trained on core knowledge, I don't think it's necessarily
[01:26:10.900 --> 01:26:16.780]   going to be in and by itself AGI, but it's probably going to be a huge milestone on the
[01:26:16.780 --> 01:26:27.780]   way to AGI, because what it represents is the ability to synthesize a task, a problem-solving
[01:26:27.780 --> 01:26:32.140]   program from just two or three examples.
[01:26:32.140 --> 01:26:35.780]   And that alone is a new way to program.
[01:26:35.780 --> 01:26:39.660]   It's an entirely new paradigm for software development, where you can start programming
[01:26:39.660 --> 01:26:44.180]   potentially quite complex programs that will generalize very well.
[01:26:44.180 --> 01:26:49.860]   And instead of programming them by coming up with the shape of the program in your mind
[01:26:49.860 --> 01:26:55.940]   and then typing it up, you're actually just showing the computer what output you want
[01:26:55.940 --> 01:26:58.460]   and you let the computer figure it out.
[01:26:58.460 --> 01:27:00.380]   I think that alone is extremely powerful.
[01:27:00.380 --> 01:27:04.500]   I want to riff a little bit on what kinds of solutions might be possible here and which
[01:27:04.500 --> 01:27:10.860]   you would consider sort of defeating the purpose of ARC and which are sort of valid.
[01:27:10.860 --> 01:27:16.620]   There's one I'll mention, which is my friends Ryan and Buck stayed up last night because
[01:27:16.620 --> 01:27:19.860]   I told them about this and they were like, "Oh, of course, everyone's going to solve
[01:27:19.860 --> 01:27:20.860]   this."
[01:27:20.860 --> 01:27:21.860]   Thank you for spreading the word.
[01:27:21.860 --> 01:27:22.860]   Of course, everyone's going to solve this.
[01:27:22.860 --> 01:27:26.700]   And then so they were trying to prompt, I think, Claude Opus on this, and they say they
[01:27:26.700 --> 01:27:30.660]   got 25% on the public ARC test.
[01:27:30.660 --> 01:27:36.020]   And what they did was have other examples of some of the ARC tests and in context explain
[01:27:36.020 --> 01:27:40.180]   the reasoning of why you went from one output to another output, and then now you have the
[01:27:40.180 --> 01:27:41.620]   current problem.
[01:27:41.620 --> 01:27:48.580]   And I think also maybe expressing the JSON in a way that is more amenable to the tokenizer.
[01:27:48.580 --> 01:27:52.000]   And another thing was using the code interpreter.
[01:27:52.000 --> 01:27:56.640]   So I'm curious, actually, if you think the code interpreter, which keeps getting better
[01:27:56.640 --> 01:28:01.780]   as these models get smarter, is just the program synthesis right there, because what they were
[01:28:01.780 --> 01:28:08.180]   able to do was the actual output of the cells, the JSON output, they got through the code
[01:28:08.180 --> 01:28:10.980]   interpreter, like right in the Python program that gets right up here.
[01:28:10.980 --> 01:28:14.900]   Do you think that the program synthesis kind of research you're talking about will look
[01:28:14.900 --> 01:28:17.720]   like just using the code interpreter in large language models?
[01:28:17.720 --> 01:28:23.380]   I think whatever solution we see that will score well is going to probably need to leverage
[01:28:23.380 --> 01:28:27.180]   some aspects from deep learning models and LLMs in particular.
[01:28:27.180 --> 01:28:32.460]   We've shown already that LLMs can do quite well, that's basically the Jack code approach.
[01:28:32.460 --> 01:28:37.460]   We've also shown that pure discrete problem search from a small DSL does very, very well
[01:28:37.460 --> 01:28:39.140]   before Jack code, this was the state of the art.
[01:28:39.140 --> 01:28:41.580]   In fact, it's still extremely close to the state of the art.
[01:28:41.580 --> 01:28:44.300]   And there's no deep learning involved at all in these models.
[01:28:44.300 --> 01:28:49.420]   So we have two approaches that have basically no overlap, that are doing quite well.
[01:28:49.420 --> 01:28:54.660]   And they're very much at two opposite ends of one spectrum, where on the one hand, you
[01:28:54.660 --> 01:29:00.140]   have these extremely large banks of millions of vector programs, but very, very shallow
[01:29:00.140 --> 01:29:02.860]   recombination, like simplistic recombination.
[01:29:02.860 --> 01:29:08.980]   And on the other end, you have very simplistic DSLs, very simple, like 100 or 200 primitives,
[01:29:08.980 --> 01:29:12.940]   but very deep, very sophisticated program search.
[01:29:12.940 --> 01:29:15.300]   The solution is going to be somewhere in between, right?
[01:29:15.300 --> 01:29:20.460]   So the people who are going to be winning the R competition and that we are going to
[01:29:20.460 --> 01:29:25.580]   be making the most progress towards near-term NGR are going to be those that manage to merge
[01:29:25.580 --> 01:29:30.900]   the deep learning paradigm and a discrete problem search paradigm into one elegant way.
[01:29:30.900 --> 01:29:37.020]   You know, you asked like, what would be legitimate and what would be cheating, for instance.
[01:29:37.020 --> 01:29:41.100]   So I think you want to add a good interpreter to the system.
[01:29:41.100 --> 01:29:42.100]   I think that's great.
[01:29:42.100 --> 01:29:43.100]   That's sort of legitimate.
[01:29:43.100 --> 01:29:49.380]   The part that would be cheating is try to anticipate what might be in the test, like
[01:29:49.380 --> 01:29:55.420]   brute force the space of possible tasks, and then train a memorization system on it, and
[01:29:55.420 --> 01:29:59.420]   then rely on the fact that you're generating so many tasks, like millions and millions
[01:29:59.420 --> 01:30:03.560]   and millions, that inevitably there's going to be some overlap between what you're generating
[01:30:03.560 --> 01:30:04.740]   and what's in the test set.
[01:30:04.740 --> 01:30:09.740]   I think that's defeating the purpose of benchmark, because then you can just solve it with that
[01:30:09.740 --> 01:30:13.380]   and you need to adapt just by fetching a memorized solution.
[01:30:13.380 --> 01:30:18.100]   So hopefully Arc will resist to that, but you know, nothing, no benchmark is necessarily
[01:30:18.100 --> 01:30:19.100]   perfect.
[01:30:19.100 --> 01:30:22.340]   So maybe there's a way to hack it, and I guess we are going to get an answer very soon.
[01:30:22.340 --> 01:30:27.060]   I think some amount of fine-tuning is valid because these models don't natively think
[01:30:27.060 --> 01:30:30.460]   in terms of, especially the language models alone, which the open source models that they
[01:30:30.460 --> 01:30:34.100]   would have to use to be competitive here, compete here, they're, you know, they're like
[01:30:34.100 --> 01:30:35.100]   natively language.
[01:30:35.100 --> 01:30:39.820]   So they'd like, need to be able to think in the, in this kind of, um, the Arc type way.
[01:30:39.820 --> 01:30:45.020]   You want to input core knowledge, like Arc-like core knowledge into the model, but surely
[01:30:45.020 --> 01:30:49.820]   you don't need tens of millions of tasks to do this, like core knowledge is extremely
[01:30:49.820 --> 01:30:50.820]   basic.
[01:30:50.820 --> 01:30:56.460]   If you look at some of these Arc type questions, I actually do think they rely a little bit
[01:30:56.460 --> 01:30:59.940]   on things I have seen throughout my life.
[01:30:59.940 --> 01:31:04.900]   And for the same reason, like, for example, like something bounces off a wall and comes
[01:31:04.900 --> 01:31:08.740]   back and you see that pattern, it's like I played arcade games and I've seen like pong
[01:31:08.740 --> 01:31:09.740]   or something.
[01:31:09.740 --> 01:31:13.120]   And I think, for example, when you see the Flynn effect and people's intelligence is
[01:31:13.120 --> 01:31:17.500]   measured on Raven's progressive matrices, increasing on these kinds of questions, it's
[01:31:17.500 --> 01:31:20.660]   probably a similar story where since now, since childhood, we actually see these sorts
[01:31:20.660 --> 01:31:23.700]   of patterns in TV and whatever, spatial patterns.
[01:31:23.700 --> 01:31:26.220]   And so I don't think this is sort of core knowledge.
[01:31:26.220 --> 01:31:30.900]   I think actually this is also part of the quote unquote fine tuning that humans have
[01:31:30.900 --> 01:31:34.780]   as they grow up of seeing different kinds of spatial patterns and trying to pattern
[01:31:34.780 --> 01:31:35.780]   match to them.
[01:31:35.780 --> 01:31:39.620]   I would definitely file that in their core knowledge, like, uh, core knowledge includes
[01:31:39.620 --> 01:31:44.020]   basic physics, for instance, bouncing or trajectories, uh, that would be included.
[01:31:44.020 --> 01:31:45.780]   But yeah, I think, I think you're entirely right.
[01:31:45.780 --> 01:31:49.980]   The reason why as a human, you're able to quickly figure out the solution is because
[01:31:49.980 --> 01:31:54.260]   you have this set of building blocks instead of patterns in your mind that you can recombine.
[01:31:54.260 --> 01:31:57.340]   Is core knowledge required to attain intelligence?
[01:31:57.340 --> 01:32:00.700]   Any, any algorithm you have, does the core knowledge have to be in some sense hard coded
[01:32:00.700 --> 01:32:03.940]   or can even the core knowledge be learned through intelligence?
[01:32:03.940 --> 01:32:05.160]   Core knowledge can be learned.
[01:32:05.160 --> 01:32:09.580]   And I think in, in the case of humans, uh, some amount of core knowledge, uh, is something
[01:32:09.580 --> 01:32:10.580]   that you're born with.
[01:32:10.580 --> 01:32:14.940]   Like we're actually born with a small amount of, uh, knowledge about the world we're going
[01:32:14.940 --> 01:32:15.940]   to live in.
[01:32:15.940 --> 01:32:16.940]   Uh, we're not blank slates.
[01:32:16.940 --> 01:32:20.460]   Uh, but most core knowledge is acquired through experience.
[01:32:20.460 --> 01:32:24.860]   But the, the thing with core knowledge that it's not going to be acquired, like for instance,
[01:32:24.860 --> 01:32:29.260]   in school, it's actually acquired very, very early in the first, like three to four years
[01:32:29.260 --> 01:32:30.540]   of your life.
[01:32:30.540 --> 01:32:34.580]   And by age four, you have all the core knowledge you're going to need as an adult.
[01:32:34.580 --> 01:32:35.580]   Okay.
[01:32:35.580 --> 01:32:36.580]   Interesting.
[01:32:36.580 --> 01:32:40.900]   Uh, so, I mean, on the prize itself, I'm super excited to see both the open source versions
[01:32:40.900 --> 01:32:45.820]   of maybe with a Lama, uh, 70B or something where people can score in the competition
[01:32:45.820 --> 01:32:52.180]   itself, then if to sort of test specifically the scaling hypothesis, I'm very curious to
[01:32:52.180 --> 01:32:56.180]   see if you can prompt on the public version of ARC, which I guess won't be competitive.
[01:32:56.180 --> 01:33:00.020]   You will be able to submit to this competition itself, but I'd be very curious to see how
[01:33:00.020 --> 01:33:03.900]   if people can sort of crack that and get ARC working there and if that would update your
[01:33:03.900 --> 01:33:04.900]   views on AGI.
[01:33:04.900 --> 01:33:05.900]   It's really be motivating.
[01:33:05.900 --> 01:33:08.940]   We're going to keep running the contest until somebody puts a reproducible open source version
[01:33:08.940 --> 01:33:09.940]   into public domain.
[01:33:09.940 --> 01:33:14.300]   So even if somebody privately beats eval or beats the ARC eval, we're going to still keep
[01:33:14.300 --> 01:33:18.040]   the prize money until someone can reproduce it and put the public reproducible version
[01:33:18.040 --> 01:33:19.040]   out there.
[01:33:19.040 --> 01:33:20.040]   Yeah, exactly.
[01:33:20.040 --> 01:33:24.600]   Like the goal is to accelerate progress towards AGI and a key part of that is that any sort
[01:33:24.600 --> 01:33:28.920]   of meaningful bits of progress needs to be shared, needs to be public.
[01:33:28.920 --> 01:33:31.880]   So it's so everyone can know about it and can try to iterate on it.
[01:33:31.880 --> 01:33:34.040]   If there's no sharing, there's no progress.
[01:33:34.040 --> 01:33:37.780]   What I'm especially curious about is sort of disaggregating the bets of like, can we
[01:33:37.780 --> 01:33:43.700]   make an open version of this versus is this a thing that's just possible with scaling?
[01:33:43.700 --> 01:33:47.940]   And we can, I guess, test both of them based on the public and the private version.
[01:33:47.940 --> 01:33:50.740]   We're making contact with reality as well with this, right?
[01:33:50.740 --> 01:33:53.060]   We're going to learn a lot, I think, about what the actual limits of the compute were.
[01:33:53.060 --> 01:33:56.180]   If someone showed up and said, hey, here's a closed source model that like I'm getting
[01:33:56.180 --> 01:33:57.260]   50 plus percent on.
[01:33:57.260 --> 01:33:59.960]   I think that would probably update us on like, okay, perhaps we should increase the amount
[01:33:59.960 --> 01:34:03.220]   of compute that we give on the private test set in order to balance, you know, some of
[01:34:03.220 --> 01:34:07.140]   the decisions initially are somewhat arbitrary in order to learn about, okay, what do people
[01:34:07.140 --> 01:34:08.140]   want?
[01:34:08.140 --> 01:34:09.140]   What does progress look like?
[01:34:09.140 --> 01:34:12.060]   And I think both of us are sort of committed to evolving it over time in order to be the
[01:34:12.060 --> 01:34:14.300]   best or the closest to perfect as we can get it.
[01:34:14.300 --> 01:34:15.300]   Awesome.
[01:34:15.300 --> 01:34:18.420]   And where can people go to learn more about the prize and maybe give their hand at it?
[01:34:18.420 --> 01:34:19.420]   ParkPrize.org.
[01:34:19.420 --> 01:34:20.420]   Which goes live today.
[01:34:20.420 --> 01:34:21.420]   So.
[01:34:21.420 --> 01:34:22.420]   It's live now.
[01:34:22.420 --> 01:34:23.420]   One million dollars is on the line, people.
[01:34:23.420 --> 01:34:24.420]   Good luck.
[01:34:24.420 --> 01:34:25.420]   Thank you guys for coming on the podcast.
[01:34:25.420 --> 01:34:28.780]   It was super fun to go through all the cruxes on intelligence and get a different perspective
[01:34:28.780 --> 01:34:30.940]   and also to announce the prize here.
[01:34:30.940 --> 01:34:31.940]   So this is awesome.
[01:34:31.940 --> 01:34:32.940]   Thank you for helping break the news.
[01:34:32.940 --> 01:34:33.700]   Thank you for having us.
[01:34:33.700 --> 01:34:34.700]   Thank you.
[01:34:34.700 --> 01:34:34.700]   Thank you.
[01:34:34.700 --> 01:34:36.760]   you
[01:34:36.760 --> 01:34:46.760]   [BLANK_AUDIO]

