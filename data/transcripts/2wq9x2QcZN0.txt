
[00:00:00.000 --> 00:00:03.360]   The following is a conversation with Eric Weinstein.
[00:00:03.360 --> 00:00:06.000]   He's a mathematician, economist, physicist,
[00:00:06.000 --> 00:00:08.680]   and the managing director of Thiel Capital.
[00:00:08.680 --> 00:00:10.880]   He coined the term, and you could say,
[00:00:10.880 --> 00:00:14.320]   is the founder of the intellectual dark web,
[00:00:14.320 --> 00:00:17.280]   which is a loosely assembled group of public intellectuals
[00:00:17.280 --> 00:00:20.920]   that includes Sam Harris, Jordan Peterson, Steven Pinker,
[00:00:20.920 --> 00:00:24.800]   Joe Rogan, Michael Shermer, and a few others.
[00:00:24.800 --> 00:00:26.080]   This conversation is part
[00:00:26.080 --> 00:00:30.240]   of the Artificial Intelligence Podcast at MIT and beyond.
[00:00:30.240 --> 00:00:33.700]   If you enjoy it, subscribe on YouTube, iTunes,
[00:00:33.700 --> 00:00:37.080]   or simply connect with me on Twitter @LexFriedman,
[00:00:37.080 --> 00:00:39.400]   spelled F-R-I-D.
[00:00:39.400 --> 00:00:43.420]   And now, here's my conversation with Eric Weinstein.
[00:00:43.420 --> 00:00:46.320]   - Are you nervous about this?
[00:00:46.320 --> 00:00:47.240]   - Scared shitless.
[00:00:47.240 --> 00:00:50.560]   - Okay, (speaking in foreign language)
[00:00:50.560 --> 00:00:51.720]   - You mentioned "Kung Fu Panda"
[00:00:51.720 --> 00:00:53.280]   as one of your favorite movies.
[00:00:54.560 --> 00:00:58.000]   It has the usual profound master-student dynamic going on.
[00:00:58.000 --> 00:01:01.100]   So, who has been a teacher
[00:01:01.100 --> 00:01:02.800]   that significantly influenced the direction
[00:01:02.800 --> 00:01:04.900]   of your thinking and life's work?
[00:01:04.900 --> 00:01:08.680]   So, if you're the Kung Fu Panda, who was your shifu?
[00:01:08.680 --> 00:01:09.500]   - Oh, that's interesting,
[00:01:09.500 --> 00:01:12.140]   because I didn't see shifu as being the teacher.
[00:01:12.140 --> 00:01:13.640]   - Who was the teacher?
[00:01:13.640 --> 00:01:16.500]   - Oogway, Master Oogway, the turtle.
[00:01:16.500 --> 00:01:18.140]   - Oh, the turtle, right.
[00:01:18.140 --> 00:01:20.800]   - They only meet twice in the entire film,
[00:01:20.800 --> 00:01:23.960]   and the first conversation sort of doesn't count.
[00:01:23.960 --> 00:01:27.540]   So, the magic of the film, in fact,
[00:01:27.540 --> 00:01:32.540]   its point is that the teaching that really matters
[00:01:32.540 --> 00:01:37.180]   is transferred during a single conversation,
[00:01:37.180 --> 00:01:40.040]   and it's very brief.
[00:01:40.040 --> 00:01:42.660]   And so, who played that role in my life?
[00:01:42.660 --> 00:01:47.160]   I would say either my grandfather,
[00:01:48.600 --> 00:01:52.560]   Harry Rubin, and his wife, Sophie Rubin, my grandmother,
[00:01:52.560 --> 00:01:54.040]   or Tom Lehrer.
[00:01:54.040 --> 00:01:55.880]   - Tom Lehrer?
[00:01:55.880 --> 00:01:57.200]   - Yeah.
[00:01:57.200 --> 00:01:58.120]   - In which way?
[00:01:58.120 --> 00:02:01.400]   - If you give a child Tom Lehrer records,
[00:02:01.400 --> 00:02:04.200]   what you do is you destroy their ability
[00:02:04.200 --> 00:02:07.200]   to be taken over by later malware.
[00:02:07.200 --> 00:02:13.040]   And it's so irreverent, so witty, so clever, so obscene,
[00:02:14.160 --> 00:02:18.520]   that it destroys the ability to lead a normal life
[00:02:18.520 --> 00:02:19.340]   for many people.
[00:02:19.340 --> 00:02:24.340]   So, if I meet somebody who's usually really shifted
[00:02:24.340 --> 00:02:27.120]   from any kind of neurotypical presentation,
[00:02:27.120 --> 00:02:30.680]   I'll often ask them, are you a Tom Lehrer fan?
[00:02:30.680 --> 00:02:34.120]   And the odds that they will respond are quite high.
[00:02:34.120 --> 00:02:38.160]   - Now, Tom Lehrer's Poisoning Pigeons in the Park, Tom Lehrer?
[00:02:38.160 --> 00:02:39.160]   - That's very interesting.
[00:02:39.160 --> 00:02:41.400]   There are a small number of Tom Lehrer songs
[00:02:41.400 --> 00:02:43.800]   that broke into the general population.
[00:02:43.800 --> 00:02:46.160]   Poisoning Pigeons in the Park, the Element song,
[00:02:46.160 --> 00:02:47.640]   and perhaps the Vatican rag.
[00:02:47.640 --> 00:02:51.440]   So, when you meet somebody who knows those songs,
[00:02:51.440 --> 00:02:52.560]   but doesn't know--
[00:02:52.560 --> 00:02:54.640]   - Oh, you're judging me right now, aren't you?
[00:02:54.640 --> 00:02:56.040]   - Harshly.
[00:02:56.040 --> 00:02:58.720]   No, but you're Russian, so undoubtedly you know
[00:02:58.720 --> 00:03:00.920]   Nikolai Ivanovich Lubachevsky, that song.
[00:03:00.920 --> 00:03:01.760]   - Yes, yeah.
[00:03:01.760 --> 00:03:03.800]   - So, that was a song about plagiarism
[00:03:03.800 --> 00:03:05.520]   that was in fact plagiarized,
[00:03:05.520 --> 00:03:07.760]   which most people don't know, from Danny Kaye,
[00:03:07.760 --> 00:03:10.640]   where Danny Kaye did a song called
[00:03:10.640 --> 00:03:12.940]   Stanislavski of the Muskie Arts.
[00:03:12.940 --> 00:03:16.280]   And so, Tom Lehrer did this brilliant job
[00:03:16.280 --> 00:03:19.940]   of plagiarizing a song and making it about plagiarism,
[00:03:19.940 --> 00:03:22.500]   and then making it about this mathematician
[00:03:22.500 --> 00:03:24.800]   who worked in non-Euclidean geometry.
[00:03:24.800 --> 00:03:27.540]   That was like giving heroin to a child.
[00:03:27.540 --> 00:03:31.360]   It was extremely addictive, and eventually led me
[00:03:31.360 --> 00:03:33.540]   to a lot of different places,
[00:03:33.540 --> 00:03:36.520]   one of which may have been a PhD in mathematics.
[00:03:36.520 --> 00:03:39.580]   - And he was also at least a lecturer in mathematics,
[00:03:39.580 --> 00:03:42.160]   I believe, at Harvard, something like that.
[00:03:42.160 --> 00:03:44.160]   - I just had dinner with him, in fact.
[00:03:44.160 --> 00:03:48.400]   When my son turned 13, we didn't tell him,
[00:03:48.400 --> 00:03:52.720]   but his bar mitzvah present was dinner
[00:03:52.720 --> 00:03:54.320]   with his hero, Tom Lehrer.
[00:03:54.320 --> 00:03:58.780]   And Tom Lehrer was 88 years old, sharp as a tack,
[00:03:58.780 --> 00:04:01.860]   irreverent and funny as hell, and just,
[00:04:01.860 --> 00:04:04.560]   you know, there are very few people in this world
[00:04:04.560 --> 00:04:06.640]   that you have to meet while they're still here,
[00:04:06.640 --> 00:04:09.240]   and that was definitely one for our family.
[00:04:09.240 --> 00:04:14.240]   - So that wit is a reflection of intelligence
[00:04:14.240 --> 00:04:16.400]   in some kind of deep way, like where,
[00:04:16.400 --> 00:04:18.560]   that would be a good test of intelligence,
[00:04:18.560 --> 00:04:20.200]   whether you're a Tom Lehrer fan.
[00:04:20.200 --> 00:04:22.740]   So what do you think that is about wit,
[00:04:22.740 --> 00:04:26.200]   about that kind of humor,
[00:04:26.200 --> 00:04:29.120]   ability to see the absurdity in existence?
[00:04:29.120 --> 00:04:31.040]   Do you think that's connected to intelligence,
[00:04:31.040 --> 00:04:33.000]   or are we just two Jews on a mic
[00:04:33.000 --> 00:04:34.680]   that appreciate that kind of humor?
[00:04:34.680 --> 00:04:37.640]   - No, I think that it's absolutely connected to intelligence.
[00:04:37.640 --> 00:04:41.400]   So you can see it, there's a place where Tom Lehrer
[00:04:41.400 --> 00:04:44.680]   decides that he's going to lampoon Gilbert
[00:04:44.680 --> 00:04:47.320]   of Gilbert and Sullivan, and he's going to outdo Gilbert
[00:04:47.320 --> 00:04:49.440]   with clever, meaningless wordplay.
[00:04:49.440 --> 00:04:52.960]   And he has, forget the, well, let's see.
[00:04:52.960 --> 00:04:55.960]   He's doing Clementine as if Gilbert and Sullivan wrote it,
[00:04:55.960 --> 00:04:57.720]   and he says, "That I mister de prester,
[00:04:57.720 --> 00:04:59.420]   "young sister, nay mister, this mister de pester,
[00:04:59.420 --> 00:05:01.560]   "she tried, pestering sister's a festering blister,
[00:05:01.560 --> 00:05:03.400]   "you best to resister, say I.
[00:05:03.400 --> 00:05:04.920]   "The sister persisted, the mister resisted,
[00:05:04.920 --> 00:05:06.040]   "I kissed her all loyalty slip.
[00:05:06.040 --> 00:05:08.280]   "When she said I could have her, her sister's cadaver
[00:05:08.280 --> 00:05:10.520]   "must surely have turned in its crypt."
[00:05:10.520 --> 00:05:13.560]   That's so dense, it's so insane,
[00:05:13.560 --> 00:05:17.180]   that that's clearly intelligence,
[00:05:17.180 --> 00:05:19.480]   because it's hard to construct something like that.
[00:05:19.480 --> 00:05:23.000]   If I look at my favorite Tom Lehrer lyric,
[00:05:23.000 --> 00:05:26.780]   there's a perfectly absurd one, which is,
[00:05:26.780 --> 00:05:28.660]   "Once all the Germans were warlike and mean,
[00:05:28.660 --> 00:05:30.100]   "but that couldn't happen again.
[00:05:30.100 --> 00:05:31.880]   "We taught them a lesson in 1918,
[00:05:31.880 --> 00:05:34.440]   "and they've hardly bothered us since then."
[00:05:34.440 --> 00:05:36.840]   That is a different kind of intelligence.
[00:05:36.840 --> 00:05:39.780]   You're taking something that is so horrific,
[00:05:39.780 --> 00:05:43.200]   and you're sort of making it palatable and funny,
[00:05:43.200 --> 00:05:48.200]   and demonstrating also just your humanity.
[00:05:48.200 --> 00:05:50.120]   I think the thing that came through,
[00:05:50.120 --> 00:05:55.200]   as Tom Lehrer wrote all of these terrible, horrible lines,
[00:05:55.200 --> 00:05:58.040]   was just what a sensitive and beautiful soul he was,
[00:05:58.040 --> 00:06:01.820]   who was channeling pain through humor and through grace.
[00:06:02.840 --> 00:06:04.920]   I've seen throughout Europe, throughout Russia,
[00:06:04.920 --> 00:06:06.160]   that same kind of humor emerged
[00:06:06.160 --> 00:06:09.200]   from the generation of World War II.
[00:06:09.200 --> 00:06:11.440]   It seemed like that humor is required
[00:06:11.440 --> 00:06:14.000]   to somehow deal with the pain and the suffering
[00:06:14.000 --> 00:06:15.960]   of that that war created.
[00:06:15.960 --> 00:06:17.880]   - Well, you do need the environment
[00:06:17.880 --> 00:06:19.720]   to create the broad Slavic soul.
[00:06:19.720 --> 00:06:23.000]   I don't think that many Americans
[00:06:23.000 --> 00:06:27.080]   really appreciate Russian humor,
[00:06:27.080 --> 00:06:31.380]   how you had to joke during the time of,
[00:06:31.380 --> 00:06:33.840]   let's say, Article 58 under Stalin.
[00:06:33.840 --> 00:06:36.040]   You had to be very, very careful.
[00:06:36.040 --> 00:06:38.500]   The concept of a Russian satirical magazine
[00:06:38.500 --> 00:06:41.360]   like "Krokodil" doesn't make sense.
[00:06:41.360 --> 00:06:44.000]   So you have this cross-cultural problem
[00:06:44.000 --> 00:06:48.800]   that there are certain areas of human experience
[00:06:48.800 --> 00:06:51.160]   that it would be better to know nothing about.
[00:06:51.160 --> 00:06:53.440]   And quite unfortunately,
[00:06:53.440 --> 00:06:55.640]   Eastern Europe knows a great deal about them,
[00:06:55.640 --> 00:07:00.000]   which makes the songs of Vladimir Vysotsky so potent.
[00:07:00.340 --> 00:07:04.260]   The prose of Pushkin, whatever it is,
[00:07:04.260 --> 00:07:06.580]   you have to appreciate the depth
[00:07:06.580 --> 00:07:08.940]   of the Eastern European experience.
[00:07:08.940 --> 00:07:11.500]   And I would think that perhaps Americans
[00:07:11.500 --> 00:07:15.560]   knew something like this around the time of the Civil War,
[00:07:15.560 --> 00:07:19.660]   or maybe under slavery and Jim Crow,
[00:07:19.660 --> 00:07:24.660]   or even the harsh tyranny of the coal and steel employers
[00:07:24.660 --> 00:07:28.260]   during the labor wars.
[00:07:29.100 --> 00:07:32.220]   But in general, I would say it's hard for us
[00:07:32.220 --> 00:07:36.040]   to understand and imagine the collective culture
[00:07:36.040 --> 00:07:38.440]   unless we have the system of selective pressures
[00:07:38.440 --> 00:07:41.540]   that, for example, Russians were subjected to.
[00:07:41.540 --> 00:07:46.700]   - Yeah, so if there's one good thing that comes out of war,
[00:07:46.700 --> 00:07:51.700]   it's literature, art, and humor, and music.
[00:07:51.700 --> 00:07:52.560]   - Oh, I don't think so.
[00:07:52.560 --> 00:07:54.880]   I think almost everything is good about war
[00:07:54.880 --> 00:07:57.020]   except for death and destruction.
[00:07:57.020 --> 00:07:57.860]   - Right.
[00:07:59.060 --> 00:08:02.660]   - Without the death, it would bring the romance of it.
[00:08:02.660 --> 00:08:03.500]   The whole thing is nice.
[00:08:03.500 --> 00:08:05.700]   - Well, this is why we're always caught up in war.
[00:08:05.700 --> 00:08:08.340]   We have this very ambiguous relationship to it,
[00:08:08.340 --> 00:08:12.780]   is that it makes life real and pressing and meaningful
[00:08:12.780 --> 00:08:15.860]   and at an unacceptable price,
[00:08:15.860 --> 00:08:17.780]   and the price has never been higher.
[00:08:17.780 --> 00:08:22.220]   - So to jump into AI a little bit,
[00:08:22.220 --> 00:08:27.780]   in one of the conversations you had, or one of the videos,
[00:08:27.780 --> 00:08:30.940]   you described that one of the things AI systems can't do,
[00:08:30.940 --> 00:08:32.820]   and biological systems can,
[00:08:32.820 --> 00:08:34.900]   is self-replicate in the physical world.
[00:08:34.900 --> 00:08:35.740]   - Oh, no, no, no.
[00:08:35.740 --> 00:08:39.020]   - In the physical world.
[00:08:39.020 --> 00:08:42.660]   - Well, yes, the physical robots can't self-replicate.
[00:08:42.660 --> 00:08:46.860]   This is a very tricky point,
[00:08:46.860 --> 00:08:50.780]   which is that the only thing that we've been able to create
[00:08:50.780 --> 00:08:52.100]   that's really complex,
[00:08:52.100 --> 00:08:57.100]   that has an analog of our reproductive system, is software.
[00:08:57.300 --> 00:09:01.860]   But nevertheless, software replicates itself,
[00:09:01.860 --> 00:09:04.060]   if we're speaking strictly for the replication,
[00:09:04.060 --> 00:09:06.060]   in this kind of digital space.
[00:09:06.060 --> 00:09:08.660]   So let me, just to begin, let me ask a question.
[00:09:08.660 --> 00:09:12.380]   Do you see a protective barrier or a gap
[00:09:12.380 --> 00:09:15.220]   between the physical world and the digital world?
[00:09:15.220 --> 00:09:16.420]   - Let's not call it digital.
[00:09:16.420 --> 00:09:19.700]   Let's call it the logical world versus the physical world.
[00:09:19.700 --> 00:09:21.820]   - Why logical?
[00:09:21.820 --> 00:09:24.140]   - Well, because even though we had,
[00:09:24.140 --> 00:09:26.380]   let's say Einstein's brain preserved,
[00:09:27.220 --> 00:09:29.820]   it was meaningless to us as a physical object
[00:09:29.820 --> 00:09:31.940]   because we couldn't do anything
[00:09:31.940 --> 00:09:35.540]   with what was stored in it at a logical level.
[00:09:35.540 --> 00:09:38.420]   And so the idea that something may be stored logically
[00:09:38.420 --> 00:09:43.420]   and that it may be stored physically are not necessarily,
[00:09:43.420 --> 00:09:45.740]   we don't always benefit from synonymizing.
[00:09:45.740 --> 00:09:48.100]   I'm not suggesting that there isn't a material basis
[00:09:48.100 --> 00:09:49.840]   to the logical world,
[00:09:49.840 --> 00:09:52.980]   but that it does warrant identification
[00:09:52.980 --> 00:09:57.860]   with a separate layer that need not invoke logic gates
[00:09:57.860 --> 00:09:59.460]   and zeros and ones.
[00:09:59.460 --> 00:10:01.260]   - And so connecting those two worlds,
[00:10:01.260 --> 00:10:03.340]   so the logical world and the physical world,
[00:10:03.340 --> 00:10:06.900]   or maybe just connecting to the logical world
[00:10:06.900 --> 00:10:09.140]   inside our brain, Einstein's brain,
[00:10:09.140 --> 00:10:14.140]   you mentioned the idea of out, outtelligence.
[00:10:14.140 --> 00:10:15.460]   - Artificial outtelligence.
[00:10:15.460 --> 00:10:16.660]   - Artificial outtelligence.
[00:10:16.660 --> 00:10:18.660]   - Yes, this is the only essay
[00:10:18.660 --> 00:10:21.740]   that John Brockman ever invited me to write
[00:10:21.740 --> 00:10:23.920]   that he refused to publish in Edge.
[00:10:23.920 --> 00:10:26.740]   - Why?
[00:10:26.740 --> 00:10:31.100]   - Well, maybe it wasn't well-written, but I don't know.
[00:10:31.100 --> 00:10:32.460]   - The idea is quite compelling.
[00:10:32.460 --> 00:10:33.900]   It's quite unique and new,
[00:10:33.900 --> 00:10:37.200]   and at least from my view, a stance point,
[00:10:37.200 --> 00:10:38.300]   maybe you can explain it.
[00:10:38.300 --> 00:10:39.480]   - Sure.
[00:10:39.480 --> 00:10:42.580]   What I was thinking about is why it is that we're waiting
[00:10:42.580 --> 00:10:47.260]   to be terrified by artificial general intelligence
[00:10:47.260 --> 00:10:52.260]   when in fact artificial life is terrifying in and of itself
[00:10:52.260 --> 00:10:54.500]   and it's already here.
[00:10:54.500 --> 00:10:57.780]   So in order to have a system of selective pressures,
[00:10:57.780 --> 00:11:00.180]   you need three distinct elements.
[00:11:00.180 --> 00:11:04.300]   You need variation within a population,
[00:11:04.300 --> 00:11:07.440]   you need heritability, and you need differential success.
[00:11:07.440 --> 00:11:10.620]   So what's really unique,
[00:11:10.620 --> 00:11:13.060]   and I've made this point I think elsewhere,
[00:11:14.480 --> 00:11:17.440]   about software is that if you think about
[00:11:17.440 --> 00:11:19.280]   what humans know how to build, that's impressive.
[00:11:19.280 --> 00:11:21.500]   So I always take a car and I say,
[00:11:21.500 --> 00:11:23.200]   does it have an analog of each
[00:11:23.200 --> 00:11:25.480]   of the physiological systems?
[00:11:25.480 --> 00:11:26.680]   Does it have a skeletal structure?
[00:11:26.680 --> 00:11:27.840]   That's its frame.
[00:11:27.840 --> 00:11:30.200]   Does it have a neurological structure?
[00:11:30.200 --> 00:11:32.040]   It has an onboard computer.
[00:11:32.040 --> 00:11:33.720]   It has a digestive system.
[00:11:33.720 --> 00:11:38.380]   The one thing it doesn't have is a reproductive system.
[00:11:38.380 --> 00:11:42.920]   But if you can call spawn on a process,
[00:11:42.920 --> 00:11:45.220]   effectively you do have a reproductive system.
[00:11:45.220 --> 00:11:49.680]   And that means that you can create something
[00:11:49.680 --> 00:11:53.360]   with variation, heritability, and differential success.
[00:11:53.360 --> 00:11:56.520]   Now, the next step in the chain of thinking was
[00:11:56.520 --> 00:12:01.520]   where do we see inanimate, non-intelligent life
[00:12:01.520 --> 00:12:04.320]   outwitting intelligent life?
[00:12:04.320 --> 00:12:08.200]   And I have two favorite systems
[00:12:08.200 --> 00:12:11.320]   and I try to stay on them so that we don't get distracted.
[00:12:11.320 --> 00:12:15.840]   One of which is the Ofres orchid subspecies,
[00:12:15.840 --> 00:12:18.200]   or subclade, I don't know what to call it.
[00:12:18.200 --> 00:12:19.080]   - Is it a type of flower?
[00:12:19.080 --> 00:12:21.940]   - Yeah, it's a type of flower that mimics the female
[00:12:21.940 --> 00:12:25.840]   of a pollinator species in order to dupe the males
[00:12:25.840 --> 00:12:29.000]   into engaging, it was called pseudocopulation,
[00:12:29.000 --> 00:12:31.840]   with the fake female, which is usually represented
[00:12:31.840 --> 00:12:33.680]   by the lowest petal.
[00:12:33.680 --> 00:12:35.520]   And there's also a pheromone component
[00:12:35.520 --> 00:12:36.880]   to fool the males into thinking
[00:12:36.880 --> 00:12:37.960]   they have a mating opportunity.
[00:12:37.960 --> 00:12:40.680]   But the flower doesn't have to give up energy
[00:12:40.680 --> 00:12:42.640]   in the form of nectar as a lure
[00:12:42.640 --> 00:12:44.240]   because it's tricking the males.
[00:12:44.240 --> 00:12:50.280]   The other system is a particular species of mussel,
[00:12:50.280 --> 00:12:54.960]   Lampicillus in the clear streams of Missouri.
[00:12:54.960 --> 00:12:59.900]   And it fools bass into biting a fleshy lip
[00:12:59.900 --> 00:13:01.720]   that contain its young.
[00:13:01.720 --> 00:13:04.800]   And when the bass see this fleshy lip,
[00:13:04.800 --> 00:13:07.000]   which looks exactly like a species of fish
[00:13:07.000 --> 00:13:08.880]   that the bass like to eat,
[00:13:08.880 --> 00:13:12.240]   the young explode and clamp onto the gills
[00:13:12.240 --> 00:13:15.680]   and parasitize the bass and also use the bass
[00:13:15.680 --> 00:13:18.340]   to redistribute them as they eventually release.
[00:13:18.340 --> 00:13:24.440]   Both of these systems, you have a highly intelligent dupe
[00:13:24.440 --> 00:13:29.180]   being fooled by a lower life form.
[00:13:29.180 --> 00:13:34.560]   And what is sculpting these convincing lures?
[00:13:34.560 --> 00:13:39.560]   It's the intelligence of previously duped targets
[00:13:39.560 --> 00:13:41.440]   for these strategies.
[00:13:41.440 --> 00:13:44.740]   So when the target is smart enough to avoid the strategy,
[00:13:44.740 --> 00:13:49.440]   those weaker mimics fall off.
[00:13:49.440 --> 00:13:51.000]   They have terminal lines.
[00:13:51.000 --> 00:13:52.680]   And only the better ones survive.
[00:13:52.680 --> 00:13:56.560]   So it's an arms race between the target species
[00:13:56.560 --> 00:14:00.860]   that is being parasitized, getting smarter,
[00:14:00.860 --> 00:14:05.860]   and this other less intelligent or non-intelligent object
[00:14:05.860 --> 00:14:08.040]   getting as if smarter.
[00:14:08.040 --> 00:14:13.780]   And so what you see is that artificial general intelligence
[00:14:13.780 --> 00:14:17.100]   is not needed to parasitize us.
[00:14:17.100 --> 00:14:22.020]   It's simply sufficient for us to outwit ourselves.
[00:14:22.020 --> 00:14:24.140]   So you could have a program, let's say,
[00:14:24.140 --> 00:14:28.180]   one of these Nigerian scams that writes letters
[00:14:28.180 --> 00:14:32.300]   and uses whoever sends it Bitcoin
[00:14:32.300 --> 00:14:36.740]   to figure out which aspects of the program should be kept,
[00:14:36.740 --> 00:14:38.780]   which should be varied and thrown away.
[00:14:38.780 --> 00:14:41.240]   And you don't need it to be in any way intelligent
[00:14:41.240 --> 00:14:43.460]   in order to have a really nightmarish scenario
[00:14:43.460 --> 00:14:44.980]   of being parasitized by something
[00:14:44.980 --> 00:14:46.620]   that has no idea what it's doing.
[00:14:46.620 --> 00:14:49.300]   - So you phrased a few concepts really eloquently.
[00:14:49.300 --> 00:14:52.980]   So let me try to, as a few directions this goes.
[00:14:52.980 --> 00:14:56.880]   So one, first of all, in the way we write software today,
[00:14:56.880 --> 00:15:01.100]   it's not common that we allow it to self-modify.
[00:15:01.100 --> 00:15:02.820]   - But we do have that ability now.
[00:15:02.820 --> 00:15:03.860]   - We have the ability.
[00:15:03.860 --> 00:15:05.100]   It's-- - Just not common.
[00:15:05.100 --> 00:15:06.020]   - It's not just common.
[00:15:06.020 --> 00:15:11.020]   So your thought is that that is a serious worry
[00:15:11.020 --> 00:15:15.300]   if there becomes a reason--
[00:15:15.300 --> 00:15:18.420]   - But self-modifying code is available now.
[00:15:18.420 --> 00:15:21.020]   - So there's different types of self-modification, right?
[00:15:21.020 --> 00:15:24.940]   There's personalization, your email app,
[00:15:24.940 --> 00:15:29.620]   your Gmail is self-modifying to you
[00:15:29.620 --> 00:15:32.280]   after you log in or whatever, you can think of it that way.
[00:15:32.280 --> 00:15:34.100]   But ultimately, it's central,
[00:15:34.100 --> 00:15:37.740]   all the information is centralized.
[00:15:37.740 --> 00:15:40.980]   But you're thinking of ideas where you're completely,
[00:15:40.980 --> 00:15:45.580]   this is a unique entity operating under selective pressures
[00:15:45.580 --> 00:15:46.900]   and it changes--
[00:15:46.900 --> 00:15:49.540]   - Well, you just, if you think about the fact
[00:15:49.540 --> 00:15:52.340]   that our immune systems don't know
[00:15:52.340 --> 00:15:53.620]   what's coming at them next,
[00:15:53.620 --> 00:15:57.860]   but they have a small set of spanning components.
[00:15:57.860 --> 00:16:00.940]   And if it's a sufficiently expressive system
[00:16:00.940 --> 00:16:05.940]   in that any shape or binding region can be approximated
[00:16:05.940 --> 00:16:10.120]   with the Lego that is present,
[00:16:10.120 --> 00:16:14.940]   then you can have confidence that you don't need to know
[00:16:14.940 --> 00:16:17.500]   what's coming at you because the combinatorics
[00:16:17.500 --> 00:16:22.280]   are sufficient to reach any configuration needed.
[00:16:23.340 --> 00:16:25.860]   - So that's a beautiful thing, well,
[00:16:25.860 --> 00:16:27.060]   terrifying thing to worry about
[00:16:27.060 --> 00:16:28.700]   because it's so within our reach.
[00:16:28.700 --> 00:16:31.840]   - Whenever I suggest these things,
[00:16:31.840 --> 00:16:33.980]   I do always have a concern as to whether or not
[00:16:33.980 --> 00:16:36.900]   I will bring them into being by talking about them.
[00:16:36.900 --> 00:16:39.740]   - So there's this thing from OpenAI,
[00:16:39.740 --> 00:16:44.080]   so next week, I have to talk to the founder of OpenAI,
[00:16:44.080 --> 00:16:47.940]   this idea that their text generation,
[00:16:47.940 --> 00:16:51.620]   the new stuff they have for generating text
[00:16:51.620 --> 00:16:53.500]   is they didn't wanna bring it,
[00:16:53.500 --> 00:16:54.500]   they didn't wanna release it
[00:16:54.500 --> 00:16:57.500]   because they're worried about the--
[00:16:57.500 --> 00:16:58.880]   - I'm delighted to hear that,
[00:16:58.880 --> 00:17:00.620]   but they're going to end up releasing it.
[00:17:00.620 --> 00:17:04.140]   - Yes, so that's the thing, I think talking about it,
[00:17:04.140 --> 00:17:05.220]   well, at least from my end,
[00:17:05.220 --> 00:17:09.600]   I'm more a proponent of technology preventing,
[00:17:09.600 --> 00:17:14.020]   so further innovation preventing
[00:17:14.020 --> 00:17:16.420]   the detrimental effects of innovation.
[00:17:16.420 --> 00:17:20.060]   - Well, we're sort of tumbling down a hill
[00:17:20.060 --> 00:17:22.180]   at accelerating speed,
[00:17:22.180 --> 00:17:24.860]   so whether or not we're proponents or--
[00:17:24.860 --> 00:17:25.700]   - It doesn't really matter.
[00:17:25.700 --> 00:17:27.340]   - It may not matter, but I--
[00:17:27.340 --> 00:17:28.260]   - Well, it may not.
[00:17:28.260 --> 00:17:29.860]   - Well, I do feel that there are people
[00:17:29.860 --> 00:17:33.780]   who've held things back and died poorer
[00:17:33.780 --> 00:17:35.300]   than they might have otherwise been,
[00:17:35.300 --> 00:17:37.400]   we don't even know their names.
[00:17:37.400 --> 00:17:39.820]   I don't think that we should discount the idea
[00:17:39.820 --> 00:17:42.380]   that having the smartest people
[00:17:42.380 --> 00:17:46.620]   showing off how smart they are by what they've developed
[00:17:46.620 --> 00:17:50.660]   may be a terminal process.
[00:17:50.660 --> 00:17:55.180]   I'm very mindful in particular of a beautiful letter
[00:17:55.180 --> 00:17:58.300]   that Edward Teller of all people wrote to Leo Zillard
[00:17:58.300 --> 00:17:59.540]   where Zillard was trying to figure out
[00:17:59.540 --> 00:18:02.580]   how to control the use of atomic weaponry
[00:18:02.580 --> 00:18:03.900]   at the end of World War II,
[00:18:03.900 --> 00:18:07.100]   and Teller rather strangely,
[00:18:07.100 --> 00:18:09.940]   because many of us view him as a monster,
[00:18:09.940 --> 00:18:13.140]   showed some very advanced moral thinking
[00:18:13.140 --> 00:18:16.380]   talking about the slim chance we have for survival
[00:18:16.380 --> 00:18:19.460]   and that the only hope is to make war unthinkable.
[00:18:19.460 --> 00:18:23.100]   I do think that not enough of us feel in our gut
[00:18:23.100 --> 00:18:24.660]   what it is we are playing with
[00:18:24.660 --> 00:18:27.180]   when we are working on technical problems.
[00:18:27.180 --> 00:18:29.220]   And I would recommend to anyone who hasn't seen it,
[00:18:29.220 --> 00:18:33.140]   a movie called "The Bridge on the River Kwai"
[00:18:33.140 --> 00:18:36.500]   about, I believe, captured British POWs
[00:18:36.500 --> 00:18:39.960]   who just in a desire to do a bridge well
[00:18:39.960 --> 00:18:43.060]   end up over collaborating with their Japanese captors.
[00:18:43.060 --> 00:18:46.620]   - Well, now you're making me question
[00:18:46.620 --> 00:18:50.240]   the unrestricted open discussion of ideas in AI.
[00:18:50.240 --> 00:18:52.580]   - I'm not saying I know the answer.
[00:18:52.580 --> 00:18:55.820]   I'm just saying that I could make a decent case
[00:18:55.820 --> 00:18:57.740]   for either our need to talk about this
[00:18:57.740 --> 00:19:00.660]   and to become technologically focused on containing it
[00:19:00.660 --> 00:19:02.780]   or need to stop talking about this
[00:19:02.780 --> 00:19:06.960]   and try to hope that the relatively small number
[00:19:06.960 --> 00:19:08.560]   of highly adept individuals
[00:19:08.560 --> 00:19:09.860]   who are looking at these problems
[00:19:09.860 --> 00:19:12.140]   is small enough that we should in fact
[00:19:12.140 --> 00:19:13.940]   be talking about how to contain them.
[00:19:13.940 --> 00:19:16.500]   - Well, the way ideas, the way innovation happens,
[00:19:16.500 --> 00:19:20.280]   what new ideas develop, Newton with calculus,
[00:19:20.280 --> 00:19:23.480]   whether if he was silent,
[00:19:23.480 --> 00:19:25.820]   the idea would emerge elsewhere.
[00:19:25.820 --> 00:19:27.700]   Well, in the case of Newton, of course.
[00:19:27.700 --> 00:19:30.660]   But in the case of AI,
[00:19:30.660 --> 00:19:32.580]   how small is the set of individuals
[00:19:32.580 --> 00:19:35.780]   out of which such ideas would arise?
[00:19:35.780 --> 00:19:38.580]   Is it a question? - Well, the idea
[00:19:38.580 --> 00:19:40.540]   is that the researchers we know
[00:19:40.540 --> 00:19:41.660]   and those that we don't know
[00:19:41.660 --> 00:19:43.740]   who may live in countries that don't wish us to know
[00:19:43.740 --> 00:19:46.260]   what level they're currently at
[00:19:46.260 --> 00:19:50.500]   are very disciplined in keeping these things to themselves.
[00:19:50.500 --> 00:19:51.460]   Of course, I will point out
[00:19:51.460 --> 00:19:55.780]   that there's a religious school in Kerala
[00:19:55.780 --> 00:19:59.220]   that developed something very close to the calculus,
[00:19:59.220 --> 00:20:01.020]   certainly in terms of infinite series
[00:20:01.020 --> 00:20:06.020]   in, I guess, religious prayer and rhyme and prose.
[00:20:10.340 --> 00:20:14.940]   So it's not that Newton had any ability to hold that back,
[00:20:14.940 --> 00:20:16.420]   and I don't really believe
[00:20:16.420 --> 00:20:17.620]   that we have an ability to hold it back.
[00:20:17.620 --> 00:20:20.380]   I do think that we could change the proportion
[00:20:20.380 --> 00:20:23.100]   of the time we spend worrying about the effects,
[00:20:23.100 --> 00:20:24.300]   what if we are successful,
[00:20:24.300 --> 00:20:25.780]   rather than simply trying to succeed
[00:20:25.780 --> 00:20:28.260]   and hope that we'll be able to contain things later.
[00:20:28.260 --> 00:20:29.140]   - Beautifully put.
[00:20:29.140 --> 00:20:31.380]   So on the idea of our intelligence,
[00:20:31.380 --> 00:20:34.860]   what form, treading cautiously,
[00:20:34.860 --> 00:20:37.860]   as we've agreed as we tumbled down the hill,
[00:20:37.860 --> 00:20:40.020]   what form-- - Can't stop ourselves, can we?
[00:20:40.020 --> 00:20:40.860]   - We cannot.
[00:20:40.860 --> 00:20:43.460]   What form do you see it taking?
[00:20:43.460 --> 00:20:47.700]   So one example, Facebook, Google,
[00:20:47.700 --> 00:20:51.540]   do want to, I don't know a better word,
[00:20:51.540 --> 00:20:55.780]   you want to influence users to behave a certain way.
[00:20:55.780 --> 00:20:59.580]   And so that's one kind of example of our intelligence,
[00:20:59.580 --> 00:21:02.500]   is systems perhaps modifying the behavior
[00:21:02.500 --> 00:21:05.660]   of these intelligent human beings
[00:21:05.660 --> 00:21:08.620]   in order to sell more product of different kind.
[00:21:08.620 --> 00:21:12.860]   But do you see other examples of this actually emerging in--
[00:21:12.860 --> 00:21:14.620]   - Just take any parasitic system.
[00:21:14.620 --> 00:21:17.860]   Make sure that there's some way in which
[00:21:17.860 --> 00:21:20.460]   that there's differential success,
[00:21:20.460 --> 00:21:24.940]   heritability, and variation.
[00:21:24.940 --> 00:21:27.420]   And those are the magic ingredients.
[00:21:27.420 --> 00:21:29.260]   And if you really wanted to build a nightmare machine,
[00:21:29.260 --> 00:21:32.980]   make sure that the system that expresses the variability
[00:21:33.980 --> 00:21:38.980]   has a spanning set so that it can learn to arbitrary levels
[00:21:38.980 --> 00:21:41.780]   by making it sufficiently expressive.
[00:21:41.780 --> 00:21:43.140]   That's your nightmare.
[00:21:43.140 --> 00:21:46.260]   - So it's your nightmare, but it could also be,
[00:21:46.260 --> 00:21:50.220]   it's a really powerful mechanism by which to create,
[00:21:50.220 --> 00:21:52.260]   well, powerful systems.
[00:21:52.260 --> 00:21:57.100]   So are you more worried about the negative direction
[00:21:57.100 --> 00:21:59.020]   that might go versus the positive?
[00:21:59.020 --> 00:22:01.700]   So you said parasitic, but that doesn't necessarily
[00:22:01.700 --> 00:22:05.060]   need to be what the system converges towards.
[00:22:05.060 --> 00:22:07.100]   It could be, what is it, symbiotic?
[00:22:07.100 --> 00:22:10.620]   - Parasitism, the dividing line between parasitism
[00:22:10.620 --> 00:22:13.580]   and symbiosis is not so clear.
[00:22:13.580 --> 00:22:15.060]   - That's what they tell me about marriage.
[00:22:15.060 --> 00:22:17.340]   I'm still single, so I don't know.
[00:22:17.340 --> 00:22:22.340]   - Well, yeah, we could go into that too, but.
[00:22:22.340 --> 00:22:23.820]   (Lex laughing)
[00:22:23.820 --> 00:22:27.380]   No, I think we have to appreciate,
[00:22:27.380 --> 00:22:30.500]   are you infected by your own mitochondria?
[00:22:30.500 --> 00:22:32.900]   - Right. (laughing)
[00:22:32.900 --> 00:22:33.740]   - Good.
[00:22:33.740 --> 00:22:35.980]   Right? - Yeah.
[00:22:35.980 --> 00:22:38.900]   - So in marriage, you fear the loss of independence,
[00:22:38.900 --> 00:22:42.640]   but even though the American therapeutic community
[00:22:42.640 --> 00:22:45.300]   may be very concerned about codependence,
[00:22:45.300 --> 00:22:48.060]   what's to say that codependence isn't what's necessary
[00:22:48.060 --> 00:22:52.140]   to have a stable relationship in which to raise children
[00:22:52.140 --> 00:22:54.100]   who are maximally case-selected
[00:22:54.100 --> 00:22:56.020]   and require incredible amounts of care
[00:22:56.020 --> 00:22:57.340]   because you have to wait 13 years
[00:22:57.340 --> 00:22:58.940]   before there's any reproductive payout,
[00:22:58.940 --> 00:23:01.740]   and most of us don't want our 13-year-olds having kids.
[00:23:01.740 --> 00:23:04.500]   It's a very tricky situation to analyze,
[00:23:04.500 --> 00:23:09.020]   and I would say that predators and parasites
[00:23:09.020 --> 00:23:10.820]   drive much of our evolution,
[00:23:10.820 --> 00:23:13.920]   and I don't know whether to be angry at them or thank them.
[00:23:13.920 --> 00:23:17.580]   - Well, ultimately, I mean, nobody knows the meaning of life
[00:23:17.580 --> 00:23:21.180]   or what even happiness is, but there is some metrics.
[00:23:21.180 --> 00:23:22.260]   - Oh, they didn't tell you?
[00:23:22.260 --> 00:23:26.300]   - They didn't, that's why all the poetry in books are about.
[00:23:27.980 --> 00:23:29.660]   You know, there is some metrics under which
[00:23:29.660 --> 00:23:32.140]   you can kind of measure how good it is
[00:23:32.140 --> 00:23:34.900]   that these AI systems are roaming about.
[00:23:34.900 --> 00:23:39.560]   So you're more nervous about software
[00:23:39.560 --> 00:23:43.860]   than you are optimistic about ideas of,
[00:23:43.860 --> 00:23:45.140]   yeah, self-replicating large scale.
[00:23:45.140 --> 00:23:48.540]   - I don't think we've really felt where we are.
[00:23:48.540 --> 00:23:52.340]   You know, occasionally we get a wake-up.
[00:23:52.340 --> 00:23:57.060]   9/11 was so anomalous compared to everything else
[00:23:58.060 --> 00:24:00.580]   we've experienced on American soil
[00:24:00.580 --> 00:24:03.220]   that it came to us as a complete shock
[00:24:03.220 --> 00:24:04.940]   that that was even a possibility.
[00:24:04.940 --> 00:24:07.440]   What it really was was a highly creative
[00:24:07.440 --> 00:24:12.440]   and determined R&D team deep in the bowels of Afghanistan
[00:24:12.440 --> 00:24:16.540]   showing us that we had certain exploits
[00:24:16.540 --> 00:24:19.380]   that we were open to that nobody had chosen to express.
[00:24:19.380 --> 00:24:21.180]   I can think of several of these things
[00:24:21.180 --> 00:24:23.180]   that I don't talk about publicly
[00:24:23.180 --> 00:24:25.140]   that just seem to have to do with
[00:24:26.920 --> 00:24:30.820]   how relatively unimaginative those who wish
[00:24:30.820 --> 00:24:33.780]   to cause havoc and destruction have been up until now.
[00:24:33.780 --> 00:24:36.320]   But the great mystery of our time,
[00:24:36.320 --> 00:24:40.120]   of this particular little era,
[00:24:40.120 --> 00:24:45.120]   is how remarkably stable we've been since 1945
[00:24:45.120 --> 00:24:47.240]   when we demonstrated the ability
[00:24:47.240 --> 00:24:50.220]   to use nuclear weapons in anger.
[00:24:50.220 --> 00:24:55.220]   And we don't know why things like that
[00:24:56.760 --> 00:24:58.320]   haven't happened since then.
[00:24:58.320 --> 00:25:00.800]   We've had several close calls, we've had mistakes,
[00:25:00.800 --> 00:25:03.420]   we've had brinksmanship.
[00:25:03.420 --> 00:25:05.820]   And what's now happened is that we've settled
[00:25:05.820 --> 00:25:10.720]   into a sense that, oh, it'll always be nothing.
[00:25:10.720 --> 00:25:14.240]   It's been so long since something was
[00:25:14.240 --> 00:25:18.040]   at that level of danger
[00:25:18.040 --> 00:25:20.760]   that we've got a wrong idea in our head.
[00:25:20.760 --> 00:25:23.000]   And that's why when I went on the Ben Shapiro show,
[00:25:23.000 --> 00:25:25.280]   I talked about the need to resume
[00:25:25.280 --> 00:25:28.080]   above-ground testing of nuclear devices,
[00:25:28.080 --> 00:25:30.560]   because we have people whose developmental experience
[00:25:30.560 --> 00:25:34.000]   suggests that when, let's say, Donald Trump
[00:25:34.000 --> 00:25:37.200]   and North Korea engage on Twitter,
[00:25:37.200 --> 00:25:39.320]   oh, it's nothing, it's just posturing.
[00:25:39.320 --> 00:25:41.040]   Everybody's just in it for money.
[00:25:41.040 --> 00:25:45.160]   There's a sense that people are in a video game mode
[00:25:45.160 --> 00:25:49.360]   which has been the right call since 1945.
[00:25:49.360 --> 00:25:51.320]   We've been mostly in video game mode.
[00:25:51.320 --> 00:25:52.460]   It's amazing.
[00:25:52.460 --> 00:25:54.160]   - So you're worried about a generation
[00:25:54.160 --> 00:25:57.000]   which has not seen any existential--
[00:25:57.000 --> 00:25:58.640]   - We've lived under it.
[00:25:58.640 --> 00:26:00.240]   You see, you're younger.
[00:26:00.240 --> 00:26:03.940]   I don't know if, and again, you came from Moscow.
[00:26:03.940 --> 00:26:09.320]   There was a TV show called The Day After
[00:26:09.320 --> 00:26:14.320]   that had a huge effect on a generation growing up in the US.
[00:26:14.320 --> 00:26:17.760]   And it talked about what life would be like
[00:26:17.760 --> 00:26:19.400]   after a nuclear exchange.
[00:26:20.920 --> 00:26:24.480]   We have not gone through an embodied experience
[00:26:24.480 --> 00:26:27.400]   collectively where we've thought about this.
[00:26:27.400 --> 00:26:30.040]   And I think it's one of the most irresponsible things
[00:26:30.040 --> 00:26:32.680]   that the elders among us have done,
[00:26:32.680 --> 00:26:36.280]   which is to provide this beautiful garden
[00:26:36.280 --> 00:26:42.260]   in which the thorns are cut off of the rose bushes
[00:26:42.260 --> 00:26:47.720]   and all of the edges are rounded and sanded.
[00:26:47.720 --> 00:26:50.880]   And so people have developed this totally unreal idea
[00:26:50.880 --> 00:26:53.920]   which is everything's going to be just fine.
[00:26:53.920 --> 00:26:57.160]   And do I think that my leading concern is AGI
[00:26:57.160 --> 00:27:01.880]   or my leading concern is thermonuclear exchange
[00:27:01.880 --> 00:27:04.080]   or gene drives or any one of these things?
[00:27:04.080 --> 00:27:05.640]   I don't know.
[00:27:05.640 --> 00:27:08.840]   But I know that our time here
[00:27:08.840 --> 00:27:11.880]   in this very long experiment here is finite
[00:27:11.880 --> 00:27:15.000]   because the toys that we've built are so impressive.
[00:27:15.000 --> 00:27:19.000]   And the wisdom to accompany them has not materialized.
[00:27:19.000 --> 00:27:24.000]   And I think we actually got a wisdom uptick since 1945.
[00:27:24.000 --> 00:27:27.160]   We had a lot of dangerous skilled players
[00:27:27.160 --> 00:27:29.760]   on the world stage who nevertheless,
[00:27:29.760 --> 00:27:33.880]   no matter how bad they were, managed to not embroil us
[00:27:33.880 --> 00:27:38.160]   in something that we couldn't come back from.
[00:27:38.160 --> 00:27:39.240]   - The Cold War.
[00:27:39.240 --> 00:27:41.400]   - Yeah, and the distance from the Cold War.
[00:27:41.400 --> 00:27:46.520]   You know, I'm very mindful of,
[00:27:46.520 --> 00:27:49.080]   there was a Russian tradition actually,
[00:27:49.080 --> 00:27:51.400]   of on your wedding day,
[00:27:51.400 --> 00:27:56.400]   going to visit a memorial to those who gave their lives.
[00:27:56.400 --> 00:27:58.280]   Can you imagine this?
[00:27:58.280 --> 00:28:00.200]   Where on the happiest day of your life,
[00:28:00.200 --> 00:28:03.720]   you go and you pay homage to the people
[00:28:03.720 --> 00:28:06.480]   who fought and died in the Battle of Stalingrad?
[00:28:06.480 --> 00:28:11.760]   I'm not a huge fan of communism, I gotta say.
[00:28:11.760 --> 00:28:15.080]   But there were a couple of things that the Russians did
[00:28:15.080 --> 00:28:18.760]   that were really positive in the Soviet era.
[00:28:18.760 --> 00:28:21.320]   And I think trying to let people know
[00:28:21.320 --> 00:28:23.240]   how serious life actually is,
[00:28:23.240 --> 00:28:26.040]   the Russian model of seriousness
[00:28:26.040 --> 00:28:28.400]   is better than the American model.
[00:28:28.400 --> 00:28:30.600]   - And maybe, like you mentioned,
[00:28:30.600 --> 00:28:33.560]   there was a small echo of that after 9/11.
[00:28:33.560 --> 00:28:36.120]   - We wouldn't let it form.
[00:28:36.120 --> 00:28:39.000]   We talk about 9/11, but it's 9/12
[00:28:39.000 --> 00:28:41.600]   that really moved the needle.
[00:28:41.600 --> 00:28:44.740]   When we were all just there and nobody wanted to speak.
[00:28:44.740 --> 00:28:48.280]   We witnessed something super serious
[00:28:48.280 --> 00:28:53.280]   and we didn't want to run to our computers
[00:28:53.280 --> 00:28:57.880]   and blast out our deep thoughts and our feelings.
[00:28:57.880 --> 00:29:04.000]   And it was profound because we woke up briefly there.
[00:29:04.000 --> 00:29:07.320]   I talk about the gated institutional narrative
[00:29:07.320 --> 00:29:09.000]   that sort of programs our lives.
[00:29:09.000 --> 00:29:11.880]   I've seen it break three times in my life.
[00:29:11.880 --> 00:29:14.980]   One of which was the election of Donald Trump.
[00:29:14.980 --> 00:29:17.680]   Another time was the fall of Lehman Brothers
[00:29:17.680 --> 00:29:21.360]   when everybody who knew that Bear Stearns
[00:29:21.360 --> 00:29:25.040]   wasn't that important knew that Lehman Brothers
[00:29:25.040 --> 00:29:27.240]   met AIG was next.
[00:29:27.240 --> 00:29:29.300]   And the other one was 9/11.
[00:29:29.300 --> 00:29:33.520]   And so if I'm 53 years old and I only remember three times
[00:29:33.520 --> 00:29:37.280]   that the global narrative was really interrupted,
[00:29:37.280 --> 00:29:39.800]   that tells you how much we've been on top
[00:29:39.800 --> 00:29:43.280]   of developing events.
[00:29:43.280 --> 00:29:45.680]   I mean, we had the Murrow Federal Building explosion,
[00:29:45.680 --> 00:29:47.560]   but it didn't cause the narrative to break.
[00:29:47.560 --> 00:29:48.920]   It wasn't profound enough.
[00:29:48.920 --> 00:29:53.920]   Around 9/12, we started to wake up out of our slumber.
[00:29:53.920 --> 00:29:59.920]   And the powers that be did not want to coming together.
[00:29:59.920 --> 00:30:02.620]   The admonition was go shopping.
[00:30:02.620 --> 00:30:06.520]   - The powers that be was what is that force
[00:30:06.520 --> 00:30:07.800]   as opposed to blaming individuals?
[00:30:07.800 --> 00:30:08.800]   - We don't know.
[00:30:08.800 --> 00:30:10.080]   - So whatever that--
[00:30:10.080 --> 00:30:11.680]   - Whatever that force is,
[00:30:11.680 --> 00:30:13.440]   there's a component of it that's emergent
[00:30:13.440 --> 00:30:15.640]   and there's a component of it that's deliberate.
[00:30:15.640 --> 00:30:18.620]   So give yourself a portfolio with two components.
[00:30:18.620 --> 00:30:20.120]   Some amount of it is emergent,
[00:30:20.120 --> 00:30:23.400]   but some amount of it is also an understanding
[00:30:23.400 --> 00:30:25.240]   that if people come together,
[00:30:25.240 --> 00:30:27.580]   they become an incredible force.
[00:30:27.580 --> 00:30:29.580]   And what you're seeing right now, I think,
[00:30:29.580 --> 00:30:34.580]   is there are forces that are trying to come together
[00:30:34.640 --> 00:30:37.780]   and there are forces that are trying to push things apart.
[00:30:37.780 --> 00:30:41.840]   And one of them is the globalist narrative
[00:30:41.840 --> 00:30:43.280]   versus the national narrative,
[00:30:43.280 --> 00:30:47.400]   where to the globalist perspective,
[00:30:47.400 --> 00:30:50.080]   the nations are bad things in essence,
[00:30:50.080 --> 00:30:52.880]   that they're temporary, they're nationalistic,
[00:30:52.880 --> 00:30:55.460]   they're jingoistic, it's all negative.
[00:30:55.460 --> 00:30:58.240]   To people in the national, more in the national idiom,
[00:30:58.240 --> 00:31:00.560]   they're saying, "Look, this is where I pay my taxes.
[00:31:00.560 --> 00:31:02.500]   "This is where I do my army service.
[00:31:02.500 --> 00:31:04.180]   "This is where I have a vote.
[00:31:04.180 --> 00:31:05.960]   "This is where I have a passport.
[00:31:05.960 --> 00:31:07.660]   "Who the hell are you to tell me
[00:31:07.660 --> 00:31:09.600]   "that because you've moved into some place
[00:31:09.600 --> 00:31:11.680]   "that you can make money globally,
[00:31:11.680 --> 00:31:14.040]   "that you've chosen to abandon other people
[00:31:14.040 --> 00:31:16.840]   "to whom you have a special and elevated duty?"
[00:31:16.840 --> 00:31:19.540]   And I think that these competing narratives
[00:31:19.540 --> 00:31:22.080]   have been pushing towards the global perspective
[00:31:22.080 --> 00:31:25.720]   from the elite and a larger and larger number
[00:31:25.720 --> 00:31:27.240]   of disenfranchised people are saying,
[00:31:27.240 --> 00:31:30.960]   "Hey, I actually live in a place and I have laws
[00:31:30.960 --> 00:31:33.280]   "and I speak a language, I have a culture,
[00:31:33.280 --> 00:31:36.580]   "and who are you to tell me that because you can profit
[00:31:36.580 --> 00:31:40.360]   "in some faraway land that my obligations
[00:31:40.360 --> 00:31:43.260]   "to my fellow countrymen are so much diminished?"
[00:31:43.260 --> 00:31:45.380]   - So these tensions between nations and so on,
[00:31:45.380 --> 00:31:48.620]   ultimately you see being proud of your country and so on,
[00:31:48.620 --> 00:31:51.780]   which creates potentially the kind of things
[00:31:51.780 --> 00:31:54.060]   that led to wars and so on.
[00:31:54.060 --> 00:31:57.340]   Ultimately it is human nature and it is good for us
[00:31:57.340 --> 00:31:59.060]   for wake-up calls of different kinds.
[00:31:59.060 --> 00:32:01.220]   - Well, I think that these are tensions.
[00:32:01.220 --> 00:32:05.040]   And my point isn't, I mean, nationalism run amok
[00:32:05.040 --> 00:32:08.520]   is a nightmare, and internationalism run amok
[00:32:08.520 --> 00:32:09.680]   is a nightmare.
[00:32:09.680 --> 00:32:14.680]   And the problem is we're trying to push these pendulums
[00:32:14.680 --> 00:32:18.160]   to some place where they're somewhat balanced,
[00:32:18.160 --> 00:32:22.000]   where we have a higher duty of care to those
[00:32:22.000 --> 00:32:25.880]   who share our laws and our citizenship,
[00:32:25.880 --> 00:32:29.240]   but we don't forget our duties of care to the global system.
[00:32:30.960 --> 00:32:32.720]   I would think this is elementary,
[00:32:32.720 --> 00:32:37.720]   but the problem that we're facing concerns the ability
[00:32:37.720 --> 00:32:42.760]   for some to profit by abandoning their obligations
[00:32:42.760 --> 00:32:45.300]   to others within their system.
[00:32:45.300 --> 00:32:48.540]   And that's what we've had for decades.
[00:32:48.540 --> 00:32:50.220]   - You mentioned nuclear weapons.
[00:32:50.220 --> 00:32:51.660]   I was hoping to get answers from you
[00:32:51.660 --> 00:32:56.020]   since one of the many things you've done as economics,
[00:32:56.020 --> 00:32:57.660]   and maybe you can understand human behavior,
[00:32:57.660 --> 00:33:01.020]   why the heck we haven't blown each other up yet.
[00:33:01.020 --> 00:33:02.740]   But okay, so we'll get back--
[00:33:02.740 --> 00:33:03.580]   - I don't know the answer.
[00:33:03.580 --> 00:33:06.340]   - Yeah, it's really important to say
[00:33:06.340 --> 00:33:07.660]   that we really don't know.
[00:33:07.660 --> 00:33:09.540]   - A mild uptick in wisdom.
[00:33:09.540 --> 00:33:10.900]   - A mild uptick in wisdom.
[00:33:10.900 --> 00:33:13.660]   Well, Steven Pinker, who I've talked with,
[00:33:13.660 --> 00:33:18.060]   has a lot of really good ideas about why, but--
[00:33:18.060 --> 00:33:19.500]   - I don't trust his optimism.
[00:33:19.500 --> 00:33:23.980]   - Listen, I'm Russian, so I never trust a guy
[00:33:23.980 --> 00:33:24.820]   who's that optimistic.
[00:33:24.820 --> 00:33:26.860]   - No, no, no, it's just that you're talking
[00:33:26.860 --> 00:33:29.980]   about a guy who's looking at a system
[00:33:29.980 --> 00:33:33.700]   in which more and more of the kinetic energy,
[00:33:33.700 --> 00:33:36.660]   like war, has been turned into potential energy,
[00:33:36.660 --> 00:33:38.540]   like unused nuclear weapons.
[00:33:38.540 --> 00:33:39.380]   - Wow, beautifully put.
[00:33:39.380 --> 00:33:41.420]   - And now I'm looking at that system,
[00:33:41.420 --> 00:33:42.540]   and I'm saying, okay, well,
[00:33:42.540 --> 00:33:44.200]   if you don't have a potential energy term,
[00:33:44.200 --> 00:33:45.940]   then everything's just getting better and better.
[00:33:45.940 --> 00:33:49.140]   - Yeah, wow, that's beautifully put.
[00:33:49.140 --> 00:33:51.300]   Only a physicist could, okay.
[00:33:51.300 --> 00:33:52.400]   - I'm not a physicist.
[00:33:52.400 --> 00:33:55.300]   - Is that a dirty word?
[00:33:55.300 --> 00:33:57.780]   - No, no, I wish I were a physicist.
[00:33:57.780 --> 00:33:59.180]   - Me too, my dad's a physicist.
[00:33:59.180 --> 00:34:00.380]   I'm trying to live up to that,
[00:34:00.380 --> 00:34:02.820]   probably for the rest of my life.
[00:34:02.820 --> 00:34:05.060]   He's probably gonna listen to this too, so.
[00:34:05.060 --> 00:34:06.060]   - He did. - Yeah.
[00:34:06.060 --> 00:34:10.220]   So your friend Sam Harris worries a lot
[00:34:10.220 --> 00:34:12.220]   about the existential threat of AI,
[00:34:12.220 --> 00:34:16.900]   not in the way that you've described, but in the more.
[00:34:16.900 --> 00:34:19.060]   - Well, he hangs out with Elon, I don't know Elon.
[00:34:19.060 --> 00:34:20.260]   (laughing)
[00:34:20.260 --> 00:34:22.780]   - So are you worried about that kind of,
[00:34:23.780 --> 00:34:26.660]   you know, about the,
[00:34:26.660 --> 00:34:30.740]   about either robotic systems
[00:34:30.740 --> 00:34:33.620]   or traditionally defined AI systems
[00:34:33.620 --> 00:34:35.620]   essentially becoming super intelligent,
[00:34:35.620 --> 00:34:37.540]   much more intelligent than human beings,
[00:34:37.540 --> 00:34:40.420]   and getting-- - Well, they already are.
[00:34:40.420 --> 00:34:41.260]   And they're not.
[00:34:41.260 --> 00:34:46.100]   - When seen as a collective, you mean?
[00:34:46.100 --> 00:34:48.220]   - Well, I mean, I can mean all sorts of things,
[00:34:48.220 --> 00:34:51.260]   but certainly many of the things that we thought
[00:34:53.020 --> 00:34:55.780]   were peculiar to general intelligence
[00:34:55.780 --> 00:34:57.340]   do not require general intelligence.
[00:34:57.340 --> 00:34:59.660]   So that's been one of the big awakenings
[00:34:59.660 --> 00:35:04.220]   that you can write a pretty convincing sports story
[00:35:04.220 --> 00:35:09.220]   from stats alone without needing to have watched the game.
[00:35:09.220 --> 00:35:13.140]   So, you know, is it possible to write lively prose
[00:35:13.140 --> 00:35:14.180]   about politics?
[00:35:14.180 --> 00:35:15.300]   Yeah, no, not yet.
[00:35:15.300 --> 00:35:20.340]   So we're sort of all over the map.
[00:35:20.340 --> 00:35:22.700]   One of the things about chess,
[00:35:22.700 --> 00:35:25.380]   there's a question I once asked on Quora
[00:35:25.380 --> 00:35:26.900]   that didn't get a lot of response,
[00:35:26.900 --> 00:35:30.060]   which was what is the greatest brilliancy ever produced
[00:35:30.060 --> 00:35:31.420]   by a computer in a chess game,
[00:35:31.420 --> 00:35:32.700]   which was different than the question
[00:35:32.700 --> 00:35:35.460]   of what is the greatest game ever played.
[00:35:35.460 --> 00:35:36.900]   So if you think about brilliancies
[00:35:36.900 --> 00:35:38.620]   is what really animates many of us
[00:35:38.620 --> 00:35:40.300]   to think of chess as an art form.
[00:35:40.300 --> 00:35:44.020]   Those are those moves and combinations
[00:35:44.020 --> 00:35:49.020]   that just show such flair, panache, and soul.
[00:35:49.020 --> 00:35:50.460]   Computers weren't really great at that.
[00:35:50.460 --> 00:35:52.300]   They were great positional monsters.
[00:35:52.300 --> 00:35:56.940]   And recently we've started seeing brilliancies.
[00:35:56.940 --> 00:35:59.740]   - Yeah, a few grandmasters have identified
[00:35:59.740 --> 00:36:02.860]   with AlphaZero that things were quite brilliant.
[00:36:02.860 --> 00:36:06.260]   - Yeah, so that's an example of something.
[00:36:06.260 --> 00:36:07.580]   We don't think that that's AGI,
[00:36:07.580 --> 00:36:11.680]   but in a very restricted set of rules like chess,
[00:36:11.680 --> 00:36:15.540]   you're starting to see poetry of a high order.
[00:36:15.540 --> 00:36:20.540]   And so I don't like the idea that we're waiting for AGI.
[00:36:21.220 --> 00:36:25.900]   AGI is sort of slowly infiltrating our lives
[00:36:25.900 --> 00:36:30.060]   in the same way that I don't think a worm should be,
[00:36:30.060 --> 00:36:32.820]   C. elegans shouldn't be treated as non-conscious
[00:36:32.820 --> 00:36:34.500]   because it only has 300 neurons.
[00:36:34.500 --> 00:36:37.780]   Maybe it just has a very low level of consciousness
[00:36:37.780 --> 00:36:39.360]   because we don't understand what these things mean
[00:36:39.360 --> 00:36:40.740]   as they scale up.
[00:36:40.740 --> 00:36:43.620]   So am I worried about this general phenomena?
[00:36:43.620 --> 00:36:46.900]   Sure, but I think that one of the things that's happening
[00:36:46.900 --> 00:36:49.300]   is that a lot of us are fretting about this
[00:36:50.180 --> 00:36:52.420]   in part because of human needs.
[00:36:52.420 --> 00:36:57.260]   We've always been worried about the Golem, right?
[00:36:57.260 --> 00:36:59.740]   - Well, the Golem is the artificially created--
[00:36:59.740 --> 00:37:00.700]   - Life, you know.
[00:37:00.700 --> 00:37:02.740]   - It's like Frankenstein type of character.
[00:37:02.740 --> 00:37:04.040]   - It's a Jewish version.
[00:37:04.040 --> 00:37:09.040]   And Frankenberg, Frankenstein.
[00:37:09.040 --> 00:37:10.460]   - Yeah, that makes sense.
[00:37:10.460 --> 00:37:14.020]   - But we've always been worried
[00:37:14.020 --> 00:37:16.300]   about creating something like this
[00:37:16.300 --> 00:37:18.460]   and it's getting closer and closer.
[00:37:18.460 --> 00:37:20.260]   And there are ways in which
[00:37:20.260 --> 00:37:25.220]   we have to realize that the whole thing
[00:37:25.220 --> 00:37:29.180]   that we've experienced or the context of our lives
[00:37:29.180 --> 00:37:32.340]   is almost certainly coming to an end.
[00:37:32.340 --> 00:37:37.340]   And I don't mean to suggest that we won't survive,
[00:37:37.340 --> 00:37:38.940]   I don't know.
[00:37:38.940 --> 00:37:41.100]   And I don't mean to suggest that it's coming tomorrow.
[00:37:41.100 --> 00:37:43.580]   It could be 300, 500 years.
[00:37:43.580 --> 00:37:46.940]   But there's no plan that I'm aware of
[00:37:46.940 --> 00:37:49.660]   if we have three rocks that we could possibly inhabit
[00:37:49.660 --> 00:37:54.660]   that are sensible within current technological dreams,
[00:37:54.660 --> 00:37:57.820]   the Earth, the Moon, and Mars.
[00:37:57.820 --> 00:38:01.980]   And we have a very competitive civilization
[00:38:01.980 --> 00:38:04.500]   that is still forced into violence
[00:38:04.500 --> 00:38:07.380]   to sort out disputes that cannot be arbitrated.
[00:38:07.380 --> 00:38:10.460]   It is not clear to me that we have a long-term future
[00:38:10.460 --> 00:38:12.820]   until we get to the next stage,
[00:38:12.820 --> 00:38:14.580]   which is to figure out whether or not
[00:38:14.580 --> 00:38:17.740]   the Einsteinian speed limit can be broken.
[00:38:17.740 --> 00:38:19.800]   And that requires our source code.
[00:38:19.800 --> 00:38:23.620]   - Our source code, the stuff in our brains
[00:38:23.620 --> 00:38:26.380]   to figure out, what do you mean by our source code?
[00:38:26.380 --> 00:38:27.820]   - The source code of the context,
[00:38:27.820 --> 00:38:29.980]   whatever it is that produces the quarks,
[00:38:29.980 --> 00:38:32.180]   the electrons, the neutrinos.
[00:38:32.180 --> 00:38:33.900]   - Our source code, I got it.
[00:38:33.900 --> 00:38:35.060]   So this is--
[00:38:35.060 --> 00:38:36.020]   - No, you're talking about stuff
[00:38:36.020 --> 00:38:38.140]   that's written in a higher level language.
[00:38:38.140 --> 00:38:39.140]   - Yeah, yeah, that's right.
[00:38:39.140 --> 00:38:42.100]   You're talking about the low level, the bits.
[00:38:42.100 --> 00:38:46.540]   That's what is currently keeping us here.
[00:38:46.540 --> 00:38:48.660]   We can't even imagine.
[00:38:48.660 --> 00:38:52.440]   We have harebrained schemes for staying
[00:38:52.440 --> 00:38:54.320]   within the Einsteinian speed limit.
[00:38:54.320 --> 00:38:57.300]   Maybe if we could just drug ourselves
[00:38:57.300 --> 00:38:58.740]   and go into a suspended state,
[00:38:58.740 --> 00:39:00.380]   or we could have multiple generations.
[00:39:00.380 --> 00:39:02.780]   I think all that stuff is pretty silly.
[00:39:02.780 --> 00:39:05.620]   But I think it's also pretty silly to imagine
[00:39:05.620 --> 00:39:07.560]   that our wisdom is going to increase
[00:39:07.560 --> 00:39:10.420]   to the point that we can have the toys we have,
[00:39:10.420 --> 00:39:14.020]   and we're not going to use them for 500 years.
[00:39:14.020 --> 00:39:17.260]   - Speaking of Einstein, I had a profound breakthrough
[00:39:17.260 --> 00:39:20.340]   when I realized you're just one letter away from the guy.
[00:39:20.340 --> 00:39:23.580]   - Yeah, but I'm also one letter away from Feinstein.
[00:39:23.580 --> 00:39:25.180]   - It's, well, you get to pick.
[00:39:25.180 --> 00:39:26.180]   (laughing)
[00:39:26.180 --> 00:39:28.540]   Okay, so unified theory.
[00:39:28.540 --> 00:39:32.660]   You know, you've worked, you enjoy the beauty of geometry.
[00:39:32.660 --> 00:39:34.340]   I don't actually know if you enjoy it.
[00:39:34.340 --> 00:39:35.860]   You certainly are quite good at explaining--
[00:39:35.860 --> 00:39:36.820]   - I tremble before it.
[00:39:36.820 --> 00:39:37.860]   - Tremble before it.
[00:39:38.900 --> 00:39:40.460]   If you're religious, that is one of the--
[00:39:40.460 --> 00:39:42.100]   - I don't have to be religious.
[00:39:42.100 --> 00:39:45.020]   It's just so beautiful, you will tremble anyway.
[00:39:45.020 --> 00:39:47.380]   - I mean, I just read Einstein's biography,
[00:39:47.380 --> 00:39:51.300]   and one of the ways, one of the things you've done
[00:39:51.300 --> 00:39:55.140]   is try to explore a unified theory,
[00:39:55.140 --> 00:39:57.460]   talking about a 14-dimensional observer
[00:39:57.460 --> 00:40:02.020]   that has the 4D space-time continuum embedded in it.
[00:40:02.020 --> 00:40:05.940]   I'm just curious how you think,
[00:40:05.940 --> 00:40:08.140]   philosophically, at a high level,
[00:40:08.140 --> 00:40:10.740]   about something more than four dimensions.
[00:40:10.740 --> 00:40:14.900]   How do you try to, what does it make you feel,
[00:40:14.900 --> 00:40:19.240]   talking in the mathematical world about dimensions
[00:40:19.240 --> 00:40:22.160]   that are greater than the ones we can perceive?
[00:40:22.160 --> 00:40:25.060]   Is there something that you take away
[00:40:25.060 --> 00:40:27.020]   that's more than just the math?
[00:40:27.020 --> 00:40:29.500]   - Well, first of all, stick out your tongue at me.
[00:40:29.500 --> 00:40:33.740]   Okay.
[00:40:33.740 --> 00:40:36.180]   Now, on the front of that tongue,
[00:40:36.180 --> 00:40:37.020]   - Yeah?
[00:40:37.020 --> 00:40:38.460]   - There was a sweet receptor.
[00:40:38.460 --> 00:40:44.420]   And next to that were salt receptors on two different sides.
[00:40:44.420 --> 00:40:46.620]   A little bit farther back, there were sour receptors.
[00:40:46.620 --> 00:40:48.100]   And you wouldn't show me the back of your tongue
[00:40:48.100 --> 00:40:50.140]   where your bitter receptor was.
[00:40:50.140 --> 00:40:51.260]   - Show the good side always.
[00:40:51.260 --> 00:40:56.260]   - Okay, but that was four dimensions of taste receptors.
[00:40:56.260 --> 00:40:58.820]   But you also had pain receptors on that tongue,
[00:40:58.820 --> 00:41:01.140]   and probably heat receptors on that tongue.
[00:41:01.140 --> 00:41:03.140]   So let's assume that you had one of each.
[00:41:03.140 --> 00:41:05.260]   That would be six dimensions.
[00:41:05.260 --> 00:41:07.860]   So when you eat something, you eat a slice of pizza,
[00:41:07.860 --> 00:41:14.300]   and it's got some hot pepper on it, maybe some jalapeno.
[00:41:14.300 --> 00:41:17.600]   You're having a six-dimensional experience, dude.
[00:41:17.600 --> 00:41:21.020]   - Do you think we overemphasize the value of time
[00:41:21.020 --> 00:41:24.300]   as one of the dimensions, or space?
[00:41:24.300 --> 00:41:26.380]   Well, we certainly overemphasize the value of time,
[00:41:26.380 --> 00:41:28.140]   'cause we like things to start and end,
[00:41:28.140 --> 00:41:30.820]   or we really don't like things to end, but they seem to.
[00:41:30.820 --> 00:41:33.380]   - Well, what if you flipped one of the spatial dimensions
[00:41:33.380 --> 00:41:35.020]   into being a temporal dimension?
[00:41:35.020 --> 00:41:39.300]   And you and I were to meet in New York City,
[00:41:39.300 --> 00:41:42.180]   and say, "Well, where and when should we meet?"
[00:41:42.180 --> 00:41:46.420]   Say, "How about I'll meet you on 36th and Lexington
[00:41:46.420 --> 00:41:51.260]   "at two in the afternoon and 11 o'clock in the morning?"
[00:41:51.260 --> 00:41:54.660]   That would be very confusing.
[00:41:54.660 --> 00:41:58.620]   - Well, so it's convenient for us to think about time,
[00:41:58.620 --> 00:41:59.460]   you mean?
[00:41:59.460 --> 00:42:01.260]   - We happen to be in a delicious situation
[00:42:01.260 --> 00:42:04.260]   in which we have three dimensions of space and one of time,
[00:42:04.260 --> 00:42:07.460]   and they're woven together in this sort of strange fabric
[00:42:07.460 --> 00:42:09.380]   where we can trade off a little space for a little time,
[00:42:09.380 --> 00:42:12.220]   but we still only have one dimension that is picked out
[00:42:12.220 --> 00:42:13.380]   relative to the other three.
[00:42:13.380 --> 00:42:15.560]   It's very much "Gladys Knight and the Pips."
[00:42:15.560 --> 00:42:17.940]   - So which one developed for who?
[00:42:17.940 --> 00:42:19.660]   Do we develop for these dimensions,
[00:42:19.660 --> 00:42:22.740]   or did the dimensions, or were they always there
[00:42:22.740 --> 00:42:23.620]   and it doesn't--
[00:42:23.620 --> 00:42:25.480]   - Well, do you imagine that there isn't a place
[00:42:25.480 --> 00:42:27.160]   where there are four temporal dimensions,
[00:42:27.160 --> 00:42:28.780]   or two and two of space and time,
[00:42:28.780 --> 00:42:30.660]   or three of time and one of space,
[00:42:30.660 --> 00:42:33.820]   and then would time not be playing the role of space?
[00:42:33.820 --> 00:42:35.880]   Why do you imagine that the sector that you're in
[00:42:35.880 --> 00:42:36.940]   is all that there is?
[00:42:36.940 --> 00:42:40.860]   - Certainly do not, but I can't imagine otherwise.
[00:42:40.860 --> 00:42:44.540]   I mean, I haven't done ayahuasca or any of those drugs,
[00:42:44.540 --> 00:42:46.140]   I'd hope to one day, but--
[00:42:46.140 --> 00:42:47.020]   - Instead of doing ayahuasca,
[00:42:47.020 --> 00:42:49.500]   you could just head over to building two.
[00:42:49.500 --> 00:42:50.740]   - That's where the mathematicians are?
[00:42:50.740 --> 00:42:52.060]   - Yeah, that's where they hang.
[00:42:52.060 --> 00:42:53.380]   - Just to look at some geometry?
[00:42:53.380 --> 00:42:55.500]   - Well, just ask about pseudo-Riemannian geometry,
[00:42:55.500 --> 00:42:56.740]   that's what you're interested in.
[00:42:56.740 --> 00:42:58.140]   (Lex laughs)
[00:42:58.140 --> 00:42:58.980]   - Okay.
[00:42:58.980 --> 00:43:01.420]   - Or you could talk to a shaman and end up in Peru.
[00:43:01.420 --> 00:43:03.020]   - And then some extra money for that trip?
[00:43:03.020 --> 00:43:04.860]   - Yeah, but you won't be able to do any calculations
[00:43:04.860 --> 00:43:06.420]   if that's how you choose to go about it.
[00:43:06.420 --> 00:43:08.300]   - Well, a different kind of calculation.
[00:43:08.300 --> 00:43:09.140]   - So to speak.
[00:43:09.140 --> 00:43:09.960]   - Yeah.
[00:43:09.960 --> 00:43:11.620]   One of my favorite people, Edward Frankel,
[00:43:11.620 --> 00:43:13.860]   Berkeley professor, author of "Love and Math,"
[00:43:13.860 --> 00:43:15.020]   great title for a book,
[00:43:15.020 --> 00:43:19.780]   said that you were quite a remarkable intellect
[00:43:19.780 --> 00:43:22.060]   to come up with such beautiful original ideas
[00:43:22.060 --> 00:43:25.020]   in terms of the unified theory and so on,
[00:43:25.020 --> 00:43:28.060]   but you were working outside academia.
[00:43:28.060 --> 00:43:31.540]   So one question, in developing ideas
[00:43:31.540 --> 00:43:33.500]   that are truly original, truly interesting,
[00:43:33.500 --> 00:43:35.940]   what's the difference between inside academia
[00:43:35.940 --> 00:43:39.980]   and outside academia when it comes to developing such ideas?
[00:43:39.980 --> 00:43:43.100]   - Oh, it's a terrible choice, terrible choice.
[00:43:43.100 --> 00:43:46.820]   So if you do it inside of academics,
[00:43:46.820 --> 00:43:50.340]   you are forced to constantly
[00:43:50.340 --> 00:43:56.700]   show great loyalty to the consensus
[00:43:56.700 --> 00:44:00.300]   and you distinguish yourself with small,
[00:44:00.300 --> 00:44:02.900]   almost microscopic heresies
[00:44:02.900 --> 00:44:05.000]   to make your reputation in general.
[00:44:05.000 --> 00:44:09.180]   And you have very competent people
[00:44:09.180 --> 00:44:11.540]   and brilliant people who are working together
[00:44:11.540 --> 00:44:16.460]   who form very deep social networks
[00:44:16.460 --> 00:44:21.100]   and have a very high level of behavior,
[00:44:21.100 --> 00:44:22.780]   at least within mathematics
[00:44:22.780 --> 00:44:25.100]   and at least technically within physics,
[00:44:25.100 --> 00:44:26.180]   theoretical physics.
[00:44:26.180 --> 00:44:31.380]   When you go outside, you meet lunatics and crazy people,
[00:44:31.380 --> 00:44:36.100]   madmen, and these are people
[00:44:36.100 --> 00:44:40.420]   who do not usually subscribe to the consensus position
[00:44:40.420 --> 00:44:42.320]   and almost always lose their way.
[00:44:42.320 --> 00:44:46.780]   And the key question is,
[00:44:46.780 --> 00:44:50.540]   will progress likely come from someone
[00:44:50.540 --> 00:44:54.820]   who has miraculously managed to stay within the system
[00:44:54.820 --> 00:44:57.380]   and is able to take on a larger amount of heresy
[00:44:57.380 --> 00:45:00.320]   that is sort of unthinkable,
[00:45:00.320 --> 00:45:04.500]   in which case that will be fascinating?
[00:45:04.500 --> 00:45:07.940]   Or is it more likely that somebody will maintain
[00:45:07.940 --> 00:45:10.820]   a level of discipline from outside of academics
[00:45:10.820 --> 00:45:15.660]   and be able to make use of the freedom
[00:45:15.660 --> 00:45:18.540]   that comes from not having to constantly
[00:45:18.540 --> 00:45:21.700]   affirm your loyalty to the consensus of your field?
[00:45:21.700 --> 00:45:23.020]   - So you've characterized in ways
[00:45:23.020 --> 00:45:28.020]   that academia in this particular sense is declining.
[00:45:28.020 --> 00:45:30.260]   You posted a plot,
[00:45:30.260 --> 00:45:34.380]   the older population of the faculty is getting larger,
[00:45:34.380 --> 00:45:37.020]   the younger is getting smaller and so on.
[00:45:37.020 --> 00:45:40.660]   So which direction of the two are you more hopeful about?
[00:45:40.660 --> 00:45:43.220]   - Well, the baby boomers can't hang on forever.
[00:45:43.220 --> 00:45:44.580]   - Well, first of all, in general true,
[00:45:44.580 --> 00:45:46.380]   and second of all, in academia.
[00:45:46.380 --> 00:45:49.460]   - But that's really what this time is about.
[00:45:49.460 --> 00:45:51.140]   - Is the baby boomers control.
[00:45:51.460 --> 00:45:53.460]   We're used to like financial bubbles
[00:45:53.460 --> 00:45:57.140]   that last a few years in length and then pop.
[00:45:57.140 --> 00:46:01.860]   The baby boomer bubble is this really long lived thing.
[00:46:01.860 --> 00:46:03.900]   And all of the ideology,
[00:46:03.900 --> 00:46:07.060]   all of the behavior patterns, the norms,
[00:46:07.060 --> 00:46:08.380]   now for example, string theory
[00:46:08.380 --> 00:46:11.580]   is an almost entirely baby boomer phenomena.
[00:46:11.580 --> 00:46:13.960]   It was something that baby boomers were able to do
[00:46:13.960 --> 00:46:16.660]   because it required a very high level
[00:46:16.660 --> 00:46:18.420]   of mathematical ability.
[00:46:20.460 --> 00:46:24.820]   - You don't think of string theory as an original idea?
[00:46:24.820 --> 00:46:26.900]   - Oh, I mean, it was original to Veneziano
[00:46:26.900 --> 00:46:29.660]   probably is older than the baby boomers.
[00:46:29.660 --> 00:46:31.860]   And there are people who are younger than the baby boomers
[00:46:31.860 --> 00:46:33.300]   who are still doing string theory.
[00:46:33.300 --> 00:46:35.220]   And I'm not saying that nothing discovered
[00:46:35.220 --> 00:46:38.460]   within the large string theoretic complex is wrong.
[00:46:38.460 --> 00:46:41.420]   Quite the contrary, a lot of brilliant mathematics
[00:46:41.420 --> 00:46:43.620]   and a lot of the structure of physics
[00:46:43.620 --> 00:46:45.640]   was elucidated by string theorists.
[00:46:45.640 --> 00:46:49.220]   What do I think of the deliverable nature
[00:46:49.220 --> 00:46:52.460]   of this product that will not ship called string theory?
[00:46:52.460 --> 00:46:55.300]   I think that it is largely an affirmative action program
[00:46:55.300 --> 00:46:59.300]   for highly mathematically and geometrically talented
[00:46:59.300 --> 00:47:02.980]   baby boomer physicists so that they can say
[00:47:02.980 --> 00:47:04.660]   that they're working on something
[00:47:04.660 --> 00:47:08.740]   within the constraints of what they will say
[00:47:08.740 --> 00:47:10.420]   is quantum gravity.
[00:47:10.420 --> 00:47:12.420]   Now there are other schemes,
[00:47:12.420 --> 00:47:14.540]   there's like asymptotic safety.
[00:47:14.540 --> 00:47:17.100]   There are other things that you could imagine doing.
[00:47:17.100 --> 00:47:20.740]   I don't think much of any of the major programs,
[00:47:20.740 --> 00:47:25.260]   but to have inflicted this level of loyalty
[00:47:25.260 --> 00:47:29.540]   through a shibboleth, well, surely you don't question X.
[00:47:29.540 --> 00:47:32.780]   Well, I question almost everything in the string program.
[00:47:32.780 --> 00:47:34.220]   And that's why I got out of physics.
[00:47:34.220 --> 00:47:37.300]   When you called me a physicist, it was a great honor.
[00:47:37.300 --> 00:47:39.140]   But the reason I didn't become a physicist
[00:47:39.140 --> 00:47:41.260]   wasn't that I fell in love with mathematics.
[00:47:41.260 --> 00:47:46.260]   As I said, wow, in 1984, 1983, I saw the field going mad.
[00:47:46.620 --> 00:47:48.740]   And I saw that mathematics,
[00:47:48.740 --> 00:47:52.900]   which has all sorts of problems, was not going insane.
[00:47:52.900 --> 00:47:55.500]   And so instead of studying things within physics,
[00:47:55.500 --> 00:47:57.100]   I thought it was much safer to study
[00:47:57.100 --> 00:47:59.780]   the same objects within mathematics.
[00:47:59.780 --> 00:48:01.180]   There's a huge price to pay for that.
[00:48:01.180 --> 00:48:03.420]   You lose physical intuition.
[00:48:03.420 --> 00:48:05.500]   But the point is that it wasn't
[00:48:05.500 --> 00:48:08.180]   a North Korean reeducation camp either.
[00:48:08.180 --> 00:48:11.220]   - Are you hopeful about cracking open
[00:48:11.220 --> 00:48:13.600]   the Einstein unified theory in a way
[00:48:13.600 --> 00:48:17.140]   that has really, really understanding
[00:48:17.140 --> 00:48:20.620]   whether this uniting everything together
[00:48:20.620 --> 00:48:21.860]   with quantum theory and so on?
[00:48:21.860 --> 00:48:25.420]   - I mean, I'm trying to play this role myself.
[00:48:25.420 --> 00:48:28.480]   To do it to the extent of handing it over
[00:48:28.480 --> 00:48:32.300]   to the more responsible, more professional,
[00:48:32.300 --> 00:48:33.800]   more competent community.
[00:48:33.800 --> 00:48:37.740]   So I think that they're wrong about a great number
[00:48:37.740 --> 00:48:39.560]   of their belief structures.
[00:48:39.560 --> 00:48:42.460]   But I do believe, I mean,
[00:48:42.460 --> 00:48:45.120]   I have a really profound love hate relationship
[00:48:45.120 --> 00:48:46.800]   with this group of people.
[00:48:46.800 --> 00:48:48.680]   - On the physics side? - Oh yeah.
[00:48:48.680 --> 00:48:50.040]   - 'Cause the mathematicians actually
[00:48:50.040 --> 00:48:51.880]   seem to be much more open-minded.
[00:48:51.880 --> 00:48:54.080]   - They are and they aren't.
[00:48:54.080 --> 00:48:55.560]   They're open-minded about anything
[00:48:55.560 --> 00:48:56.920]   that looks like great math.
[00:48:56.920 --> 00:49:00.180]   They'll study something that isn't very important physics,
[00:49:00.180 --> 00:49:01.700]   but if it's beautiful mathematics,
[00:49:01.700 --> 00:49:04.320]   then they'll have, they have great intuition
[00:49:04.320 --> 00:49:06.100]   about these things.
[00:49:06.100 --> 00:49:07.900]   As good as the mathematicians are,
[00:49:07.900 --> 00:49:10.940]   and I might even intellectually at some horsepower level
[00:49:10.940 --> 00:49:15.460]   give them the edge, the theoretical physics community
[00:49:15.460 --> 00:49:19.860]   is bar none the most profound intellectual community
[00:49:19.860 --> 00:49:21.900]   that we have ever created.
[00:49:21.900 --> 00:49:24.980]   It is the number one, there's nobody in second place
[00:49:24.980 --> 00:49:25.940]   as far as I'm concerned.
[00:49:25.940 --> 00:49:29.140]   Look, in their spare time, in their spare time,
[00:49:29.140 --> 00:49:31.060]   they invented molecular biology.
[00:49:31.060 --> 00:49:33.100]   - What was the origin of molecular biology?
[00:49:33.100 --> 00:49:33.940]   You're saying physics?
[00:49:33.940 --> 00:49:34.860]   - Well, somebody like Francis Crick.
[00:49:34.860 --> 00:49:38.620]   I mean, a lot of the early molecular biologists.
[00:49:38.620 --> 00:49:40.180]   - Were physicists? - Yeah, I mean,
[00:49:40.180 --> 00:49:42.500]   you know, Schrodinger wrote, "What is life?"
[00:49:42.500 --> 00:49:44.420]   That was highly inspirational.
[00:49:44.420 --> 00:49:49.420]   I mean, you have to appreciate that there is no community
[00:49:49.420 --> 00:49:54.660]   like the basic research community in theoretical physics.
[00:49:54.660 --> 00:49:59.380]   And it's not something, I'm highly critical of these guys.
[00:49:59.380 --> 00:50:04.380]   I think that they would just wasted the decades of time
[00:50:04.380 --> 00:50:08.340]   with a near religious devotion
[00:50:08.340 --> 00:50:11.020]   to their misconception conceptualization
[00:50:11.020 --> 00:50:13.260]   of where the problems were in physics.
[00:50:13.260 --> 00:50:16.740]   But this has been the greatest intellectual collapse
[00:50:16.740 --> 00:50:18.780]   ever witnessed within academics.
[00:50:18.780 --> 00:50:22.780]   - You see it as a collapse or just a lull?
[00:50:22.780 --> 00:50:25.940]   - Oh, I'm terrified that we're about to lose the vitality.
[00:50:25.940 --> 00:50:27.900]   We can't afford to pay these people.
[00:50:27.900 --> 00:50:31.260]   We can't afford to give them an accelerator
[00:50:31.260 --> 00:50:33.480]   just to play with in case they find something
[00:50:33.480 --> 00:50:35.180]   at the next energy level.
[00:50:35.180 --> 00:50:37.240]   These people created our economy.
[00:50:38.180 --> 00:50:41.780]   They gave us the Rad Lab and radar.
[00:50:41.780 --> 00:50:45.500]   They gave us two atomic devices to end World War II.
[00:50:45.500 --> 00:50:48.380]   They created the semiconductor and the transistor
[00:50:48.380 --> 00:50:51.580]   to power our economy through Moore's Law.
[00:50:51.580 --> 00:50:54.660]   As a positive externality of particle accelerators,
[00:50:54.660 --> 00:50:56.380]   they created the World Wide Web.
[00:50:56.380 --> 00:50:59.940]   And we have the insolence to say,
[00:50:59.940 --> 00:51:02.380]   "Why should we fund you with our taxpayer dollars?"
[00:51:02.380 --> 00:51:06.180]   No, the question is, are you enjoying your physics dollars?
[00:51:08.000 --> 00:51:09.420]   Right, these guys signed
[00:51:09.420 --> 00:51:12.380]   the world's worst licensing agreement.
[00:51:12.380 --> 00:51:16.180]   And if they simply charged for every time
[00:51:16.180 --> 00:51:19.660]   you used a transistor or a URL,
[00:51:19.660 --> 00:51:21.900]   or enjoyed the peace that they have provided
[00:51:21.900 --> 00:51:25.740]   during this period of time through the terrible weapons
[00:51:25.740 --> 00:51:29.600]   that they developed, or your communications devices,
[00:51:29.600 --> 00:51:32.300]   all of the things that power our economy,
[00:51:32.300 --> 00:51:33.880]   I really think came out of physics,
[00:51:33.880 --> 00:51:35.700]   even to the extent that chemistry came out of physics
[00:51:35.700 --> 00:51:37.880]   and molecular biology came out of physics.
[00:51:37.880 --> 00:51:39.940]   So, first of all, you have to know
[00:51:39.940 --> 00:51:42.440]   that I'm very critical of this community.
[00:51:42.440 --> 00:51:45.060]   Second of all, it is our most important community.
[00:51:45.060 --> 00:51:47.540]   We have neglected it, we've abused it,
[00:51:47.540 --> 00:51:49.700]   we don't take it seriously,
[00:51:49.700 --> 00:51:52.300]   we don't even care to get them to rehab
[00:51:52.300 --> 00:51:54.700]   after a couple of generations of failure.
[00:51:54.700 --> 00:51:57.980]   No one, I think the youngest person
[00:51:57.980 --> 00:52:00.980]   to have really contributed to the standard model
[00:52:00.980 --> 00:52:05.620]   of theoretical level was born in 1951, right?
[00:52:05.620 --> 00:52:06.580]   Frank Wilczek.
[00:52:07.580 --> 00:52:10.000]   And almost nothing has happened
[00:52:10.000 --> 00:52:14.820]   that in theoretical physics after 1973, '74
[00:52:14.820 --> 00:52:17.020]   that sent somebody to Stockholm
[00:52:17.020 --> 00:52:21.600]   for theoretical development that predicted experiment.
[00:52:21.600 --> 00:52:24.760]   So, we have to understand that we are doing this to ourselves
[00:52:24.760 --> 00:52:28.440]   now with that said, these guys have behaved abysmally,
[00:52:28.440 --> 00:52:31.980]   in my opinion, because they haven't owned up
[00:52:31.980 --> 00:52:33.380]   to where they actually are,
[00:52:33.380 --> 00:52:34.920]   what problems they're really facing,
[00:52:34.920 --> 00:52:37.280]   how definite they can actually be.
[00:52:37.280 --> 00:52:39.760]   They haven't shared some of their most brilliant discoveries
[00:52:39.760 --> 00:52:41.820]   which are desperately needed in other fields,
[00:52:41.820 --> 00:52:43.120]   like gauge theory,
[00:52:43.120 --> 00:52:45.580]   which at least the mathematicians can share,
[00:52:45.580 --> 00:52:47.400]   which is an upgrade of the differential calculus
[00:52:47.400 --> 00:52:49.040]   of Newton and Leibniz.
[00:52:49.040 --> 00:52:50.560]   And they haven't shared the importance
[00:52:50.560 --> 00:52:52.840]   of renormalization theory,
[00:52:52.840 --> 00:52:55.400]   even though this should be standard operating procedure
[00:52:55.400 --> 00:52:59.440]   for people across the sciences dealing with different layers
[00:52:59.440 --> 00:53:01.040]   and different levels of phenomena.
[00:53:01.040 --> 00:53:03.440]   - And by shared, you mean communicated in such a way
[00:53:03.440 --> 00:53:06.800]   that it disseminates throughout the different sciences.
[00:53:06.800 --> 00:53:07.920]   - These guys are sitting,
[00:53:07.920 --> 00:53:10.680]   both theoretical physicists and mathematicians
[00:53:10.680 --> 00:53:13.520]   are sitting on top of a giant stockpile
[00:53:13.520 --> 00:53:16.320]   of intellectual gold, right?
[00:53:16.320 --> 00:53:17.660]   They have so many things
[00:53:17.660 --> 00:53:19.760]   that have not been manifested anywhere.
[00:53:19.760 --> 00:53:23.400]   I was just on Twitter, I think I mentioned
[00:53:23.400 --> 00:53:26.080]   the Habermann switch pitch that shows the self-duality
[00:53:26.080 --> 00:53:29.640]   of the tetrahedron realized as a linkage mechanism.
[00:53:29.640 --> 00:53:32.080]   Now, this is like a triviality
[00:53:32.080 --> 00:53:36.440]   and it makes an amazing toy that's built a market,
[00:53:36.440 --> 00:53:38.520]   hopefully a fortune for Chuck Habermann.
[00:53:38.520 --> 00:53:41.280]   Well, you have no idea how much great stuff
[00:53:41.280 --> 00:53:43.440]   that these priests have in their monastery.
[00:53:43.440 --> 00:53:47.360]   - So it's truly a love and hate relationship for you.
[00:53:47.360 --> 00:53:48.200]   - Yeah, well, look--
[00:53:48.200 --> 00:53:49.240]   - Sounds like it's more on the love side.
[00:53:49.240 --> 00:53:51.960]   - This building that we're in right here
[00:53:51.960 --> 00:53:54.840]   is the building in which I really put together
[00:53:54.840 --> 00:53:57.640]   the conspiracy between the National Academy of Sciences
[00:53:57.640 --> 00:54:00.040]   and the National Science Foundation
[00:54:00.040 --> 00:54:01.560]   through the government university industry
[00:54:01.560 --> 00:54:04.840]   research round table to destroy the bargaining power
[00:54:04.840 --> 00:54:08.520]   of American academics using foreign labor.
[00:54:08.520 --> 00:54:09.360]   - With a--
[00:54:09.360 --> 00:54:11.080]   - On microfiche in the basement.
[00:54:11.080 --> 00:54:13.240]   Oh yeah, that was done here in this building.
[00:54:13.240 --> 00:54:14.080]   Isn't that weird?
[00:54:14.080 --> 00:54:16.960]   - And I'm truly speaking with a revolutionary
[00:54:16.960 --> 00:54:18.160]   and a radical--
[00:54:18.160 --> 00:54:20.120]   - No, no, no, no, no, no, no, no, no, no.
[00:54:20.120 --> 00:54:25.120]   At an intellectual level, I am absolutely garden variety.
[00:54:25.120 --> 00:54:27.580]   I'm just straight down the middle.
[00:54:27.580 --> 00:54:29.480]   The system that we are in,
[00:54:29.480 --> 00:54:33.320]   this university is functionally insane.
[00:54:34.680 --> 00:54:36.680]   Harvard is functionally insane.
[00:54:36.680 --> 00:54:41.120]   And we don't understand that when we get these things wrong,
[00:54:41.120 --> 00:54:43.560]   the financial crisis made this very clear.
[00:54:43.560 --> 00:54:46.120]   There was a long period where every grownup,
[00:54:46.120 --> 00:54:51.120]   everybody with a tie who spoke in baritone tones
[00:54:51.120 --> 00:54:55.520]   with the right degree at the end of their name,
[00:54:55.520 --> 00:54:59.360]   we're talking about how we banished volatility.
[00:54:59.360 --> 00:55:01.000]   We were in the great moderation.
[00:55:01.000 --> 00:55:04.040]   Okay, they were all crazy.
[00:55:04.040 --> 00:55:05.200]   And who was right?
[00:55:05.200 --> 00:55:08.400]   It was like Nassim Taleb, Nouriel Roubini.
[00:55:08.400 --> 00:55:10.720]   Now, what happens is that they claimed
[00:55:10.720 --> 00:55:14.600]   the market went crazy, but the market didn't go crazy.
[00:55:14.600 --> 00:55:16.140]   The market had been crazy.
[00:55:16.140 --> 00:55:19.040]   And what happened is that it suddenly went sane.
[00:55:19.040 --> 00:55:21.120]   Well, that's where we are with academics.
[00:55:21.120 --> 00:55:23.860]   Academics right now is mad as a hatter.
[00:55:23.860 --> 00:55:25.520]   And it's absolutely evident.
[00:55:25.520 --> 00:55:27.040]   I can show you graph after graph.
[00:55:27.040 --> 00:55:28.520]   I can show you the internal discussions.
[00:55:28.520 --> 00:55:30.540]   I can show you the conspiracies.
[00:55:30.540 --> 00:55:32.160]   Harvard's dealing with one right now
[00:55:32.160 --> 00:55:36.320]   over its admissions policies for people of color
[00:55:36.320 --> 00:55:38.200]   who happen to come from Asia.
[00:55:38.200 --> 00:55:41.880]   All of this madness is necessary to keep the game going.
[00:55:41.880 --> 00:55:43.120]   What we're talking about,
[00:55:43.120 --> 00:55:46.280]   just while we're on the topic of revolutionaries,
[00:55:46.280 --> 00:55:49.820]   is we're talking about the danger of an outbreak of sanity.
[00:55:49.820 --> 00:55:53.560]   - Yeah, you're the guy pointing out the elephant
[00:55:53.560 --> 00:55:55.360]   in the room here.
[00:55:55.360 --> 00:55:56.960]   - The elephant has no clothes.
[00:55:56.960 --> 00:55:58.200]   (Lex laughing)
[00:55:58.200 --> 00:55:59.600]   - Is that how that goes?
[00:55:59.600 --> 00:56:04.600]   I was gonna talk a little bit to Joe Rogan about this.
[00:56:04.600 --> 00:56:06.280]   We ran out of time.
[00:56:06.280 --> 00:56:09.480]   But I think you have some,
[00:56:09.480 --> 00:56:11.920]   just listening to you,
[00:56:11.920 --> 00:56:14.400]   you could probably speak really eloquently to academia
[00:56:14.400 --> 00:56:16.480]   on the difference between the different fields.
[00:56:16.480 --> 00:56:19.000]   So you think there's a difference
[00:56:19.000 --> 00:56:20.560]   between science, engineering,
[00:56:20.560 --> 00:56:22.440]   and then the humanities and academia
[00:56:22.440 --> 00:56:25.760]   in terms of tolerance that they're willing to tolerate?
[00:56:25.760 --> 00:56:27.860]   So from my perspective, I thought,
[00:56:28.780 --> 00:56:32.060]   computer science and maybe engineering
[00:56:32.060 --> 00:56:34.740]   is more tolerant to radical ideas,
[00:56:34.740 --> 00:56:36.900]   but that's perhaps innocent of me.
[00:56:36.900 --> 00:56:39.660]   'Cause I always, all the battles going on now
[00:56:39.660 --> 00:56:41.420]   are a little bit more on the humanities side
[00:56:41.420 --> 00:56:43.180]   and gender studies and so on.
[00:56:43.180 --> 00:56:46.540]   - Have you seen the American Mathematical Society's
[00:56:46.540 --> 00:56:49.460]   publication of an essay called "Get Out the Way"?
[00:56:49.460 --> 00:56:50.580]   - I have not.
[00:56:50.580 --> 00:56:51.420]   What's the--
[00:56:51.420 --> 00:56:55.200]   - The idea is that white men who hold positions
[00:56:56.300 --> 00:56:58.020]   within universities in mathematics
[00:56:58.020 --> 00:56:59.460]   should vacate their positions
[00:56:59.460 --> 00:57:02.500]   so that young black women can take over
[00:57:02.500 --> 00:57:03.820]   or something like this.
[00:57:03.820 --> 00:57:04.860]   - That's in terms of diversity,
[00:57:04.860 --> 00:57:06.100]   which I also wanna ask you about,
[00:57:06.100 --> 00:57:10.140]   but in terms of diversity of strictly ideas.
[00:57:10.140 --> 00:57:10.980]   - Oh, sure.
[00:57:10.980 --> 00:57:14.220]   - Do you think, 'cause you're basically saying physics
[00:57:14.220 --> 00:57:16.580]   as a community has become a little bit intolerant
[00:57:16.580 --> 00:57:20.340]   to some degree to new radical ideas.
[00:57:20.340 --> 00:57:21.180]   Or at least you said--
[00:57:21.180 --> 00:57:24.180]   - Well, it's changed a little bit recently,
[00:57:24.180 --> 00:57:28.100]   which is that even string theory is now admitting,
[00:57:28.100 --> 00:57:30.900]   okay, we don't, this doesn't look very promising
[00:57:30.900 --> 00:57:32.820]   in the short term, right?
[00:57:32.820 --> 00:57:35.980]   So the question is what compiles,
[00:57:35.980 --> 00:57:39.580]   if you wanna take the computer science metaphor,
[00:57:39.580 --> 00:57:41.900]   what will get you into a journal?
[00:57:41.900 --> 00:57:44.260]   Will you spend your life trying to push some paper
[00:57:44.260 --> 00:57:47.440]   into a journal or will it be accepted easily?
[00:57:47.440 --> 00:57:50.440]   What do we know about the characteristics of the submitter?
[00:57:51.940 --> 00:57:55.420]   And what gets taken up and what does not?
[00:57:55.420 --> 00:57:58.660]   All of these fields are experiencing pressure
[00:57:58.660 --> 00:58:02.180]   because no field is performing so brilliantly well
[00:58:02.180 --> 00:58:08.620]   that it's revolutionizing our way of speaking and thinking
[00:58:08.620 --> 00:58:12.820]   in the ways in which we've become accustomed.
[00:58:12.820 --> 00:58:15.880]   - But don't you think, even in theoretical physics,
[00:58:15.880 --> 00:58:19.860]   a lot of times, even with theories like string theory,
[00:58:19.860 --> 00:58:22.860]   you could speak to this, it does eventually lead to
[00:58:22.860 --> 00:58:25.500]   what are the ways that this theory would be testable?
[00:58:25.500 --> 00:58:28.740]   - So ultimately, although, look,
[00:58:28.740 --> 00:58:32.020]   there's this thing about Popper and the scientific method
[00:58:32.020 --> 00:58:34.020]   that's a cancer and a disease
[00:58:34.020 --> 00:58:36.260]   in the minds of very smart people.
[00:58:36.260 --> 00:58:39.780]   That's not really how most of the stuff gets worked out,
[00:58:39.780 --> 00:58:41.180]   it's how it gets checked.
[00:58:41.180 --> 00:58:45.500]   And there is a dialogue between theory and experiment.
[00:58:45.500 --> 00:58:49.900]   But everybody should read Paul Dirac's 1963
[00:58:49.900 --> 00:58:55.940]   scientific American article where he,
[00:58:55.940 --> 00:58:58.620]   it's very interesting, he talks about it
[00:58:58.620 --> 00:59:00.580]   as if it was about the Schrodinger equation
[00:59:00.580 --> 00:59:03.780]   and Schrodinger's failure to advance his own work
[00:59:03.780 --> 00:59:06.260]   because of his failure to account for some phenomenon.
[00:59:06.260 --> 00:59:08.740]   The key point is that if your theory is a slight bit off,
[00:59:08.740 --> 00:59:10.260]   it won't agree with experiment,
[00:59:10.260 --> 00:59:13.500]   but it doesn't mean that the theory is actually wrong.
[00:59:13.500 --> 00:59:15.700]   But Dirac could as easily have been talking
[00:59:15.700 --> 00:59:18.740]   about his own equation in which he predicted
[00:59:18.740 --> 00:59:22.020]   that the electrons should have an antiparticle.
[00:59:22.020 --> 00:59:24.580]   And since the only positively charged particle
[00:59:24.580 --> 00:59:26.740]   that was known at the time was the proton,
[00:59:26.740 --> 00:59:29.500]   Heisenberg pointed out, well, shouldn't your antiparticle,
[00:59:29.500 --> 00:59:31.620]   the proton have the same mass as the electron
[00:59:31.620 --> 00:59:33.540]   and doesn't that invalidate your theory?
[00:59:33.540 --> 00:59:35.620]   So I think that Dirac was actually being quite,
[00:59:35.620 --> 00:59:39.340]   potentially quite sneaky and talking about the fact
[00:59:39.340 --> 00:59:41.300]   that he had been pushed off of his own theory
[00:59:41.300 --> 00:59:42.900]   to some extent by Heisenberg.
[00:59:43.780 --> 00:59:47.960]   But look, we fetishized the scientific method
[00:59:47.960 --> 00:59:52.460]   and Popper and falsification because it protects us
[00:59:52.460 --> 00:59:55.480]   from crazy ideas entering the field.
[00:59:55.480 --> 00:59:58.420]   So it's a question of balancing type one and type two error
[00:59:58.420 --> 01:00:01.460]   and we were pretty maxed out in one direction.
[01:00:01.460 --> 01:00:04.140]   - The opposite of that, let me say what comforts me,
[01:00:04.140 --> 01:00:07.620]   sort of biology or engineering,
[01:00:07.620 --> 01:00:10.700]   at the end of the day, does the thing work?
[01:00:10.700 --> 01:00:11.540]   - Yeah.
[01:00:11.540 --> 01:00:14.620]   - You can test the crazies away.
[01:00:14.620 --> 01:00:16.700]   The crazy, well, see, now you're saying,
[01:00:16.700 --> 01:00:18.180]   but some ideas are truly crazy
[01:00:18.180 --> 01:00:20.780]   and some are actually correct.
[01:00:20.780 --> 01:00:24.300]   - So there's pre-correct, currently crazy.
[01:00:24.300 --> 01:00:25.300]   - Yeah. - Right?
[01:00:25.300 --> 01:00:27.160]   And so you don't wanna get rid of everybody
[01:00:27.160 --> 01:00:29.660]   who's pre-correct and currently crazy.
[01:00:29.660 --> 01:00:35.180]   The problem is is that we don't have standards in general
[01:00:35.180 --> 01:00:38.740]   for trying to determine who has to be put to the sword
[01:00:38.740 --> 01:00:42.100]   in terms of their career and who has to be protected
[01:00:42.100 --> 01:00:46.300]   as some sort of giant time suck pain in the ass
[01:00:46.300 --> 01:00:47.860]   who may change everything.
[01:00:47.860 --> 01:00:49.220]   - Do you think that's possible,
[01:00:49.220 --> 01:00:51.300]   creating a mechanism of those selected?
[01:00:51.300 --> 01:00:53.300]   - Well, you're not gonna like the answer, but here it comes.
[01:00:53.300 --> 01:00:55.140]   - Oh, boy.
[01:00:55.140 --> 01:00:59.360]   - It has to do with very human elements.
[01:00:59.360 --> 01:01:02.500]   We're trying to do this at the level of rules and fairness.
[01:01:02.500 --> 01:01:03.620]   It's not gonna work.
[01:01:03.620 --> 01:01:07.820]   'Cause the only thing that really understands this
[01:01:08.660 --> 01:01:09.980]   is the rules.
[01:01:09.980 --> 01:01:12.220]   - You ever read the "Double Helix"?
[01:01:12.220 --> 01:01:13.260]   - It's a book.
[01:01:13.260 --> 01:01:16.000]   - Oh, you have to read this book.
[01:01:16.000 --> 01:01:19.820]   Not only did Jim Watson half discover
[01:01:19.820 --> 01:01:21.940]   this three-dimensional structure of DNA,
[01:01:21.940 --> 01:01:24.940]   he was also one hell of a writer before he became an ass.
[01:01:24.940 --> 01:01:29.620]   No, he's tried to destroy his own reputation.
[01:01:29.620 --> 01:01:30.460]   - I knew about the ass,
[01:01:30.460 --> 01:01:32.940]   I didn't know about the good writer.
[01:01:32.940 --> 01:01:35.780]   - Jim Watson is one of the most important people now living.
[01:01:35.780 --> 01:01:38.860]   And as I've said before,
[01:01:38.860 --> 01:01:41.100]   Jim Watson is too important a legacy
[01:01:41.100 --> 01:01:42.840]   to be left to Jim Watson.
[01:01:42.840 --> 01:01:46.620]   That book tells you more
[01:01:46.620 --> 01:01:49.260]   about what actually moves the dial.
[01:01:49.260 --> 01:01:51.340]   And there's another story about him,
[01:01:51.340 --> 01:01:52.700]   which I don't agree with,
[01:01:52.700 --> 01:01:54.820]   which is that he stole everything from Rosalind Franklin.
[01:01:54.820 --> 01:01:56.420]   I mean, the problems that he had
[01:01:56.420 --> 01:01:58.180]   with Rosalind Franklin are real,
[01:01:58.180 --> 01:02:02.020]   but we should actually honor that tension in our history
[01:02:02.020 --> 01:02:05.180]   by delving into it rather than having a simple solution.
[01:02:05.180 --> 01:02:07.940]   Jim Watson talks about Francis Crick
[01:02:07.940 --> 01:02:09.240]   being a pain in the ass
[01:02:09.240 --> 01:02:11.700]   that everybody secretly knew was super brilliant.
[01:02:11.700 --> 01:02:16.740]   And there's an encounter between Chargaff,
[01:02:16.740 --> 01:02:19.340]   who came up with the equimolar relations
[01:02:19.340 --> 01:02:20.880]   between the nucleotides,
[01:02:20.880 --> 01:02:22.860]   who should have gotten the structure of DNA,
[01:02:22.860 --> 01:02:24.620]   and Watson and Crick.
[01:02:24.620 --> 01:02:28.940]   And he talks about missing a shiver
[01:02:28.940 --> 01:02:30.300]   in the heartbeat of biology.
[01:02:30.300 --> 01:02:31.620]   And this stuff is so gorgeous,
[01:02:31.620 --> 01:02:34.620]   it just makes you tremble even thinking about it.
[01:02:35.620 --> 01:02:40.620]   Look, we know very often who is to be feared,
[01:02:40.620 --> 01:02:44.000]   and we need to fund the people that we fear.
[01:02:44.000 --> 01:02:46.980]   The people who are wasting our time
[01:02:46.980 --> 01:02:49.660]   need to be excluded from the conversation.
[01:02:49.660 --> 01:02:52.980]   You see, and maybe we'll make some errors
[01:02:52.980 --> 01:02:54.780]   in both directions,
[01:02:54.780 --> 01:02:58.100]   but we have known our own people.
[01:02:58.100 --> 01:03:01.020]   We know the pains in the asses that might work out.
[01:03:01.020 --> 01:03:03.380]   And we know the people who are really just blowhards
[01:03:03.380 --> 01:03:07.340]   who really have very little to contribute most of the time.
[01:03:07.340 --> 01:03:10.460]   It's not 100%, but you're not gonna get there with rules.
[01:03:10.460 --> 01:03:12.620]   - Right, it's using some kind of instinct.
[01:03:12.620 --> 01:03:14.540]   I mean, to be honest,
[01:03:14.540 --> 01:03:16.540]   I'm gonna make you roll your eyes for a second,
[01:03:16.540 --> 01:03:19.580]   but the first time I heard
[01:03:19.580 --> 01:03:21.100]   that there is a large community of people
[01:03:21.100 --> 01:03:22.680]   who believe the Earth is flat,
[01:03:22.680 --> 01:03:26.420]   actually made me pause and ask myself the question.
[01:03:26.420 --> 01:03:27.980]   - Why would there be such a community?
[01:03:27.980 --> 01:03:30.180]   - Yeah, is it possible the Earth is flat?
[01:03:30.180 --> 01:03:32.020]   So I had to like, wait a minute.
[01:03:33.180 --> 01:03:35.140]   I mean, then you go through a thinking process
[01:03:35.140 --> 01:03:37.500]   that I think is really healthy.
[01:03:37.500 --> 01:03:40.700]   It ultimately ends up being a geometry thing, I think.
[01:03:40.700 --> 01:03:44.100]   It's an interesting thought experiment at the very least.
[01:03:44.100 --> 01:03:46.540]   - Well, I do a different version of it.
[01:03:46.540 --> 01:03:48.660]   I say, why is this community stable?
[01:03:48.660 --> 01:03:51.500]   - Yeah, that's a good way to analyze it.
[01:03:51.500 --> 01:03:53.220]   - Interesting that whatever we've done
[01:03:53.220 --> 01:03:54.900]   has not erased the community.
[01:03:54.900 --> 01:03:57.820]   So, you know, they're taking a long shot bet
[01:03:57.820 --> 01:03:59.300]   that won't pan out, you know.
[01:03:59.300 --> 01:04:01.500]   Maybe we just haven't thought enough
[01:04:01.500 --> 01:04:03.460]   about the rationality of the square root of two
[01:04:03.460 --> 01:04:05.140]   and somebody brilliant will figure it out.
[01:04:05.140 --> 01:04:07.060]   Maybe we will eventually land one day
[01:04:07.060 --> 01:04:09.100]   on the surface of Jupiter and explore it.
[01:04:09.100 --> 01:04:14.020]   Right, these are crazy things that will never happen.
[01:04:14.020 --> 01:04:17.540]   - So much of social media operates by AI algorithms.
[01:04:17.540 --> 01:04:19.500]   You talked about this a little bit,
[01:04:19.500 --> 01:04:21.700]   recommending the content you see.
[01:04:21.700 --> 01:04:24.980]   So on this idea of radical thought,
[01:04:24.980 --> 01:04:28.180]   how much should AI show you things you disagree with
[01:04:28.180 --> 01:04:29.800]   on Twitter and so on?
[01:04:30.860 --> 01:04:33.140]   In a Twitter verse.
[01:04:33.140 --> 01:04:34.500]   - I hate this question.
[01:04:34.500 --> 01:04:35.340]   - Yeah? - Yeah.
[01:04:35.340 --> 01:04:37.300]   - 'Cause you don't know the answer?
[01:04:37.300 --> 01:04:38.900]   - No, no, no, no.
[01:04:38.900 --> 01:04:43.220]   Look, they've pushed out this cognitive Lego to us
[01:04:43.220 --> 01:04:45.780]   that will just lead to madness.
[01:04:45.780 --> 01:04:47.820]   It's good to be challenged with things
[01:04:47.820 --> 01:04:49.380]   that you disagree with.
[01:04:49.380 --> 01:04:50.500]   The answer is no.
[01:04:50.500 --> 01:04:52.980]   It's good to be challenged with interesting things
[01:04:52.980 --> 01:04:55.340]   with which you currently disagree,
[01:04:55.340 --> 01:04:57.060]   but that might be true.
[01:04:57.060 --> 01:04:58.500]   So I don't really care about whether or not
[01:04:58.500 --> 01:05:00.380]   I disagree with something or don't disagree.
[01:05:00.380 --> 01:05:03.420]   I need to know why that particular disagreeable thing
[01:05:03.420 --> 01:05:05.340]   is being pushed out.
[01:05:05.340 --> 01:05:07.020]   Is it because it's likely to be true?
[01:05:07.020 --> 01:05:09.700]   Is it because, is there some reason?
[01:05:09.700 --> 01:05:12.300]   Because I can write a computer generator
[01:05:12.300 --> 01:05:15.900]   to come up with an infinite number of disagreeable statements
[01:05:15.900 --> 01:05:17.640]   that nobody needs to look at.
[01:05:17.640 --> 01:05:19.660]   So please, before you push things at me
[01:05:19.660 --> 01:05:22.780]   that are disagreeable, tell me why.
[01:05:22.780 --> 01:05:25.220]   - There is an aspect in which that question is quite dumb,
[01:05:25.220 --> 01:05:28.960]   especially because it's being used to almost,
[01:05:29.920 --> 01:05:33.800]   very generically by these different networks to say,
[01:05:33.800 --> 01:05:35.440]   well, we're trying to work this out.
[01:05:35.440 --> 01:05:39.620]   But basically, how much,
[01:05:39.620 --> 01:05:43.560]   do you see the value of seeing things you don't like,
[01:05:43.560 --> 01:05:45.540]   not you disagree with, because it's very difficult
[01:05:45.540 --> 01:05:47.600]   to know exactly what you articulated,
[01:05:47.600 --> 01:05:52.480]   which is the stuff that's important for you to consider
[01:05:52.480 --> 01:05:53.360]   that you disagree with.
[01:05:53.360 --> 01:05:54.960]   That's really hard to figure out.
[01:05:54.960 --> 01:05:57.120]   The bottom line is the stuff you don't like.
[01:05:57.120 --> 01:06:00.640]   If you're a Hillary Clinton supporter,
[01:06:00.640 --> 01:06:03.800]   you may not want to, it might not make you feel good
[01:06:03.800 --> 01:06:05.800]   to see anything about Donald Trump.
[01:06:05.800 --> 01:06:06.920]   That's the only thing algorithms
[01:06:06.920 --> 01:06:08.920]   can really optimize for currently.
[01:06:08.920 --> 01:06:10.320]   They really can't. - No, they can do better.
[01:06:10.320 --> 01:06:12.700]   This is, we're-- - You think so?
[01:06:12.700 --> 01:06:17.200]   - No, we're engaged in some moronic back and forth
[01:06:17.200 --> 01:06:22.200]   where I have no idea why people who are capable
[01:06:22.200 --> 01:06:25.960]   of building Google, Facebook, Twitter
[01:06:25.960 --> 01:06:28.880]   are having us in these incredibly low level discussions.
[01:06:28.880 --> 01:06:31.200]   Do they not know any smart people?
[01:06:31.200 --> 01:06:33.200]   Do they not have the phone numbers of people
[01:06:33.200 --> 01:06:34.960]   who can elevate these discussions?
[01:06:34.960 --> 01:06:39.540]   - They do, but this, they're optimizing
[01:06:39.540 --> 01:06:41.580]   for a different thing and they are pushing those people
[01:06:41.580 --> 01:06:42.420]   out of those rooms.
[01:06:42.420 --> 01:06:46.080]   - They're optimizing for things we can't see.
[01:06:46.080 --> 01:06:48.480]   And yes, profit is there.
[01:06:48.480 --> 01:06:50.240]   Nobody's questioning that.
[01:06:50.240 --> 01:06:53.000]   But they're also optimizing for things like
[01:06:54.160 --> 01:06:56.640]   political control or the fact that they're doing business
[01:06:56.640 --> 01:06:58.720]   in Pakistan and so they don't want to talk
[01:06:58.720 --> 01:07:00.640]   about all the things that they're going to be
[01:07:00.640 --> 01:07:03.280]   bending to in Pakistan.
[01:07:03.280 --> 01:07:07.400]   So we're involved in a fake discussion.
[01:07:07.400 --> 01:07:08.240]   - You think so?
[01:07:08.240 --> 01:07:09.760]   You think these conversations at that depth
[01:07:09.760 --> 01:07:11.240]   are happening inside Google?
[01:07:11.240 --> 01:07:14.000]   You don't think they have some basic metrics
[01:07:14.000 --> 01:07:15.720]   under user engagements?
[01:07:15.720 --> 01:07:18.220]   - You're having a fake conversation with us, guys.
[01:07:18.220 --> 01:07:19.880]   We know you're having a fake conversation.
[01:07:19.880 --> 01:07:22.400]   I do not wish to be part of your fake conversation.
[01:07:23.600 --> 01:07:26.720]   You know how to cool these units.
[01:07:26.720 --> 01:07:29.360]   You know high availability like nobody's business.
[01:07:29.360 --> 01:07:33.160]   My Gmail never goes down, almost.
[01:07:33.160 --> 01:07:36.240]   - So you think just because they can do incredible work
[01:07:36.240 --> 01:07:38.280]   on the software side with infrastructure,
[01:07:38.280 --> 01:07:43.280]   they can also deal with some of these difficult questions
[01:07:43.280 --> 01:07:46.280]   about human behavior, human understanding,
[01:07:46.280 --> 01:07:47.440]   you're not, you're not.
[01:07:47.440 --> 01:07:50.920]   - I mean, I've seen the developers' screens
[01:07:50.920 --> 01:07:54.320]   that people take shots of inside of Google.
[01:07:54.320 --> 01:07:58.440]   And I've heard stories inside of Facebook and Apple.
[01:07:58.440 --> 01:08:01.600]   We're not, we're engaged, they're engaging us
[01:08:01.600 --> 01:08:04.060]   in the wrong conversations.
[01:08:04.060 --> 01:08:06.080]   We are not at this low level.
[01:08:06.080 --> 01:08:08.100]   Here's one of my favorite questions.
[01:08:08.100 --> 01:08:11.840]   Why is every piece of hardware that I purchase
[01:08:11.840 --> 01:08:15.560]   in tech space equipped as a listening device?
[01:08:15.560 --> 01:08:19.740]   Where's my physical shutter to cover my lens?
[01:08:19.740 --> 01:08:22.640]   We had this in the 1970s.
[01:08:22.640 --> 01:08:25.080]   The cameras that had lens caps, you know?
[01:08:25.080 --> 01:08:27.920]   How much would it cost to have a security model?
[01:08:27.920 --> 01:08:29.780]   Pay five extra bucks.
[01:08:29.780 --> 01:08:33.040]   Why is my indicator light software controlled?
[01:08:33.040 --> 01:08:35.240]   Why, when my camera is on, do I not see
[01:08:35.240 --> 01:08:38.040]   that the light is on by putting it as something
[01:08:38.040 --> 01:08:39.700]   that cannot be bypassed?
[01:08:39.700 --> 01:08:42.900]   Why have you set up all my devices
[01:08:42.900 --> 01:08:46.280]   at some difficulty to yourselves as listening devices
[01:08:46.280 --> 01:08:47.720]   and we don't even talk about this?
[01:08:47.720 --> 01:08:51.520]   This thing is total fucking bullshit.
[01:08:51.520 --> 01:08:53.100]   - Well, I hope-- - No, no, wait, wait, wait.
[01:08:53.100 --> 01:08:55.320]   - These discussions are happening about privacy.
[01:08:55.320 --> 01:08:57.080]   Is there a more difficult thing you're giving credit for?
[01:08:57.080 --> 01:08:59.040]   - It's not just privacy.
[01:08:59.040 --> 01:09:01.080]   It's about social control.
[01:09:01.080 --> 01:09:03.560]   We're talking about social control.
[01:09:03.560 --> 01:09:07.120]   Why do I not have controls over my own levers?
[01:09:07.120 --> 01:09:09.940]   Just have a really cute UI where I can switch,
[01:09:09.940 --> 01:09:11.680]   I can dial things, or I can at least see
[01:09:11.680 --> 01:09:12.960]   what the algorithms are.
[01:09:12.960 --> 01:09:16.720]   - But you think that there is some deliberate choices
[01:09:16.720 --> 01:09:19.040]   being made here. - There is emergence
[01:09:19.040 --> 01:09:21.560]   and there is intention.
[01:09:21.560 --> 01:09:22.920]   There are two dimensions.
[01:09:22.920 --> 01:09:26.320]   The vector does not collapse onto either axis.
[01:09:26.320 --> 01:09:29.080]   But the idea that anybody who suggests
[01:09:29.080 --> 01:09:34.080]   that intention is completely absent is a child.
[01:09:34.080 --> 01:09:35.960]   - That's really beautifully put.
[01:09:35.960 --> 01:09:38.840]   And like many things you've said is gonna make me--
[01:09:38.840 --> 01:09:40.800]   - Can I turn this around slightly?
[01:09:40.800 --> 01:09:41.800]   - Yeah.
[01:09:41.800 --> 01:09:42.880]   - I sit down with you and you say
[01:09:42.880 --> 01:09:44.580]   that you're obsessed with my feed.
[01:09:45.720 --> 01:09:47.560]   I don't even know what my feed is.
[01:09:47.560 --> 01:09:49.720]   What are you seeing that I'm not?
[01:09:49.720 --> 01:09:53.560]   - I was obsessively looking through your feed on Twitter
[01:09:53.560 --> 01:09:54.880]   'cause it was really enjoyable
[01:09:54.880 --> 01:09:56.520]   because there's the Tom Lehrer element,
[01:09:56.520 --> 01:09:58.240]   there's the humor in it.
[01:09:58.240 --> 01:10:01.600]   - By the way, that feed is Eric R. Weinstein on Twitter.
[01:10:01.600 --> 01:10:03.720]   - It's great. - Eric R. Weinstein.
[01:10:03.720 --> 01:10:06.620]   No, but seriously, why?
[01:10:06.620 --> 01:10:09.840]   - Why did I find it enjoyable or what was I seeing?
[01:10:09.840 --> 01:10:11.360]   - What are you looking for?
[01:10:11.360 --> 01:10:12.920]   Why are we doing this?
[01:10:12.920 --> 01:10:14.800]   What is this podcast about?
[01:10:14.800 --> 01:10:16.480]   I know you've got all these interesting people.
[01:10:16.480 --> 01:10:18.880]   I'm just some guy who's sort of a podcast guest.
[01:10:18.880 --> 01:10:22.360]   - Sort of a podcast, you're not even wearing a tie.
[01:10:22.360 --> 01:10:24.740]   I mean, it's not even a serious interview.
[01:10:24.740 --> 01:10:30.560]   I'm searching for meaning, for happiness,
[01:10:30.560 --> 01:10:34.400]   for a dopamine rush, so short-term and long-term.
[01:10:34.400 --> 01:10:36.500]   - And how are you finding your way to me?
[01:10:36.500 --> 01:10:41.240]   I don't honestly know what I'm doing to reach you.
[01:10:41.240 --> 01:10:46.240]   The representing ideas which feel common sense to me
[01:10:46.240 --> 01:10:47.880]   and not many people are speaking,
[01:10:47.880 --> 01:10:52.120]   so it's kind of like the intellectual dark web folks.
[01:10:52.120 --> 01:10:58.660]   These folks, from Sam Harris to Jordan Peterson to yourself,
[01:10:58.660 --> 01:11:01.080]   are saying things where it's like you're saying,
[01:11:01.080 --> 01:11:03.980]   look, there's an elephant and he's not wearing any clothes.
[01:11:03.980 --> 01:11:09.400]   And I say, yeah, yeah, let's have more of that conversation.
[01:11:09.400 --> 01:11:10.960]   That's how I'm finding you.
[01:11:10.960 --> 01:11:13.320]   - I'm desperate to try to change
[01:11:13.320 --> 01:11:14.720]   the conversation we're having.
[01:11:14.720 --> 01:11:17.400]   I'm very worried we've got an election in 2020.
[01:11:17.400 --> 01:11:20.120]   I don't think we can afford four more years
[01:11:20.120 --> 01:11:22.380]   of a misinterpreted message,
[01:11:22.380 --> 01:11:25.320]   which is what Donald Trump was.
[01:11:25.320 --> 01:11:28.360]   And I don't want the destruction of our institutions.
[01:11:28.360 --> 01:11:30.600]   They all seem hell-bent on destroying themselves.
[01:11:30.600 --> 01:11:33.200]   So I'm trying to save theoretical physics,
[01:11:33.200 --> 01:11:34.720]   trying to save the New York Times,
[01:11:34.720 --> 01:11:38.200]   trying to save our various processes.
[01:11:38.200 --> 01:11:40.560]   And I think it feels delusional to me
[01:11:40.560 --> 01:11:44.600]   that this is falling to a tiny group of people
[01:11:44.600 --> 01:11:48.720]   who are willing to speak out without getting so freaked out
[01:11:48.720 --> 01:11:50.760]   that everything they say will be misinterpreted
[01:11:50.760 --> 01:11:53.000]   and that their lives will be ruined through the process.
[01:11:53.000 --> 01:11:56.000]   I mean, I think we're in an absolutely bananas period
[01:11:56.000 --> 01:11:57.920]   of time, and I don't believe it should fall
[01:11:57.920 --> 01:12:01.120]   to such a tiny number of shoulders to shoulder this way.
[01:12:01.120 --> 01:12:05.840]   - So I have to ask you, on the capitalism side,
[01:12:05.840 --> 01:12:08.160]   you mentioned that technology is killing capitalism,
[01:12:08.160 --> 01:12:12.880]   or it has effects that are, well, not unintended,
[01:12:12.880 --> 01:12:16.000]   but not what economists would predict
[01:12:16.000 --> 01:12:18.800]   or speak of capitalism creating.
[01:12:18.800 --> 01:12:21.240]   I just wanna talk to you about, in general,
[01:12:21.240 --> 01:12:23.640]   the effect of even then artificial intelligence
[01:12:23.640 --> 01:12:27.280]   or technology automation taking away jobs
[01:12:27.280 --> 01:12:28.200]   and these kinds of things,
[01:12:28.200 --> 01:12:31.500]   and what you think is the way to alleviate that,
[01:12:31.500 --> 01:12:33.700]   whether the Andrew Yang presidential candidate
[01:12:33.700 --> 01:12:36.040]   with universal basic income, UBI,
[01:12:36.040 --> 01:12:38.680]   what are your thoughts there?
[01:12:38.680 --> 01:12:42.000]   How do we fight off the negative effects of technology?
[01:12:42.000 --> 01:12:44.560]   - All right, you're a software guy, right?
[01:12:44.560 --> 01:12:48.500]   A human being is a worker is an old idea.
[01:12:48.500 --> 01:12:53.960]   A human being has a worker is a different object, right?
[01:12:53.960 --> 01:12:55.720]   So if you think about object-oriented programming
[01:12:55.720 --> 01:12:59.640]   as a paradigm, a human being has a worker
[01:12:59.640 --> 01:13:01.840]   and a human being has a soul.
[01:13:01.840 --> 01:13:04.400]   We're talking about the fact that for a period of time,
[01:13:04.400 --> 01:13:08.720]   the worker that a human being has was in a position
[01:13:08.720 --> 01:13:11.340]   to feed the soul that a human being has.
[01:13:11.340 --> 01:13:15.040]   However, we have two separate claims
[01:13:15.040 --> 01:13:17.020]   on the value in society.
[01:13:17.020 --> 01:13:20.740]   One is as a worker and the other is as a soul,
[01:13:20.740 --> 01:13:23.440]   and the soul needs sustenance, it needs dignity,
[01:13:23.440 --> 01:13:25.320]   it needs meaning, it needs purpose.
[01:13:25.320 --> 01:13:32.380]   As long as your means of support is not highly repetitive,
[01:13:33.600 --> 01:13:34.840]   I think you have a while to go
[01:13:34.840 --> 01:13:36.920]   before you need to start worrying.
[01:13:36.920 --> 01:13:39.760]   But if what you do is highly repetitive
[01:13:39.760 --> 01:13:41.240]   and it's not terribly generative,
[01:13:41.240 --> 01:13:45.960]   you are in the crosshairs of for loops and while loops,
[01:13:45.960 --> 01:13:48.880]   and that's what computers excel at, repetitive behavior.
[01:13:48.880 --> 01:13:52.440]   And when I say repetitive, I may mean things
[01:13:52.440 --> 01:13:54.840]   that have never happened through combinatorial possibilities
[01:13:54.840 --> 01:13:57.200]   but as long as it has a looped characteristic to it,
[01:13:57.200 --> 01:13:58.100]   you're in trouble.
[01:13:58.100 --> 01:14:02.960]   We are seeing a massive push towards socialism
[01:14:02.960 --> 01:14:07.960]   because capitalists are slow to address the fact
[01:14:07.960 --> 01:14:10.880]   that a worker may not be able to make claims.
[01:14:10.880 --> 01:14:15.640]   A relatively undistinguished median member of our society
[01:14:15.640 --> 01:14:20.640]   still has needs to reproduce, needs to dignity.
[01:14:20.640 --> 01:14:25.520]   And when capitalism abandons the median individual
[01:14:25.520 --> 01:14:29.660]   or the bottom 10th or whatever it's going to do,
[01:14:29.660 --> 01:14:32.640]   it's flirting with revolution.
[01:14:32.640 --> 01:14:35.400]   And what concerns me is that the capitalists
[01:14:35.400 --> 01:14:38.080]   aren't sufficiently capitalistic to understand this.
[01:14:38.080 --> 01:14:43.040]   You really want to court authoritarian control
[01:14:43.040 --> 01:14:45.100]   in our society because you can't see
[01:14:45.100 --> 01:14:47.300]   that people may not be able to defend themselves
[01:14:47.300 --> 01:14:50.080]   in the marketplace because the marginal product
[01:14:50.080 --> 01:14:55.040]   of their labor is too low to feed their dignity as a soul.
[01:14:55.040 --> 01:14:59.960]   So my great concern is that our free society has to do
[01:14:59.960 --> 01:15:02.320]   with the fact that we are self-organized.
[01:15:02.320 --> 01:15:04.960]   I remember looking down from my office in Manhattan
[01:15:04.960 --> 01:15:07.880]   when Lehman Brothers collapsed and thinking,
[01:15:07.880 --> 01:15:09.440]   who's going to tell all these people
[01:15:09.440 --> 01:15:11.820]   that they need to show up at work
[01:15:11.820 --> 01:15:14.120]   when they don't have a financial system
[01:15:14.120 --> 01:15:16.440]   to incentivize them to show up at work?
[01:15:16.440 --> 01:15:20.880]   So my complaint is first of all, not with the socialists
[01:15:20.880 --> 01:15:24.520]   but with the capitalists, which is you guys are being idiots.
[01:15:24.520 --> 01:15:28.280]   You're courting revolution by continuing to harp
[01:15:28.280 --> 01:15:30.960]   on the same old ideas that, well, you know,
[01:15:30.960 --> 01:15:33.040]   try harder, bootstrap yourself.
[01:15:33.040 --> 01:15:36.300]   Yeah, to an extent that works, to an extent.
[01:15:36.300 --> 01:15:37.940]   But we are clearly headed in a place
[01:15:37.940 --> 01:15:41.040]   that there's nothing that ties together our need
[01:15:41.040 --> 01:15:45.400]   to contribute and our need to consume.
[01:15:45.400 --> 01:15:47.580]   And that may not be provided by capitalism
[01:15:47.580 --> 01:15:49.460]   because it may have been a temporary phenomenon.
[01:15:49.460 --> 01:15:52.800]   So check out my article on anthropic capitalism
[01:15:52.800 --> 01:15:55.480]   and the new gimmick economy.
[01:15:55.480 --> 01:15:58.020]   I think people are late getting the wake-up call
[01:15:58.020 --> 01:15:59.980]   and we would be doing a better job
[01:15:59.980 --> 01:16:01.860]   saving capitalism from itself
[01:16:01.860 --> 01:16:05.740]   because I don't want this done under authoritarian control.
[01:16:05.740 --> 01:16:08.100]   And the more we insist that everybody
[01:16:08.100 --> 01:16:10.100]   who's not thriving in our society
[01:16:10.100 --> 01:16:11.860]   during their reproductive years
[01:16:11.860 --> 01:16:15.260]   in order to have a family is failing at a personal level.
[01:16:15.260 --> 01:16:18.380]   I mean, what a disgusting thing that we're saying.
[01:16:18.380 --> 01:16:19.940]   What a horrible message.
[01:16:19.940 --> 01:16:21.900]   Who the hell have we become
[01:16:21.900 --> 01:16:24.940]   that we've so bought into the Chicago model
[01:16:24.940 --> 01:16:26.580]   that we can't see the humanity
[01:16:26.580 --> 01:16:28.140]   that we're destroying in that process?
[01:16:28.140 --> 01:16:31.460]   And I hate the thought of communism.
[01:16:31.460 --> 01:16:32.280]   I really do.
[01:16:32.280 --> 01:16:34.540]   My family has flirted with it decades past.
[01:16:34.540 --> 01:16:36.460]   It's a wrong, bad idea.
[01:16:36.460 --> 01:16:38.460]   But we are going to need to figure out
[01:16:38.460 --> 01:16:40.620]   how to make sure that those souls
[01:16:40.620 --> 01:16:43.060]   are nourished and respected
[01:16:43.060 --> 01:16:45.060]   and capitalism better have an answer.
[01:16:45.060 --> 01:16:46.460]   And I'm betting on capitalism,
[01:16:46.460 --> 01:16:49.980]   but I gotta tell you, I'm pretty disappointed with my team.
[01:16:49.980 --> 01:16:52.140]   - So you're still on the capitalism team.
[01:16:52.140 --> 01:16:54.260]   You just, there's a theme here.
[01:16:54.260 --> 01:16:56.180]   - Radical capital.
[01:16:56.180 --> 01:16:57.340]   - Hyper capitalism.
[01:16:57.340 --> 01:16:59.620]   - I want, I think hyper capitalism
[01:16:59.620 --> 01:17:01.980]   is gonna have to be coupled to hyper socialism.
[01:17:01.980 --> 01:17:04.260]   You need to allow the most productive people
[01:17:04.260 --> 01:17:06.100]   to create wonders.
[01:17:06.100 --> 01:17:08.240]   And you gotta stop bogging them down
[01:17:08.240 --> 01:17:11.260]   with all of these extra nice requirements.
[01:17:11.260 --> 01:17:12.860]   Nice is dead.
[01:17:12.860 --> 01:17:14.500]   Good has a future.
[01:17:14.500 --> 01:17:16.180]   Nice doesn't have a future
[01:17:16.180 --> 01:17:19.020]   because nice ends up with gulags.
[01:17:19.020 --> 01:17:21.100]   - Damn, that's a good line.
[01:17:21.100 --> 01:17:22.700]   Okay, last question.
[01:17:22.700 --> 01:17:27.140]   You tweeted today a simple, quite insightful equation
[01:17:27.140 --> 01:17:33.060]   saying imagine that every unit F of fame you picked up
[01:17:33.060 --> 01:17:35.660]   as stalkers and H haters.
[01:17:35.660 --> 01:17:38.660]   So I imagine S and H are dependent on your path to fame
[01:17:38.660 --> 01:17:39.500]   perhaps a little bit.
[01:17:39.500 --> 01:17:40.940]   - Well, it's not as simple.
[01:17:40.940 --> 01:17:42.460]   People always take these things literally
[01:17:42.460 --> 01:17:45.160]   when you have like 280 characters to explain yourself.
[01:17:45.160 --> 01:17:47.020]   (laughing)
[01:17:47.020 --> 01:17:49.500]   - So you mean that that's not a mathematical--
[01:17:49.500 --> 01:17:50.340]   - No, there's no law.
[01:17:50.340 --> 01:17:51.260]   - Oh, okay.
[01:17:51.260 --> 01:17:52.260]   All right.
[01:17:52.260 --> 01:17:53.380]   I put the word imagine
[01:17:53.380 --> 01:17:56.260]   because I still have a mathematician's desire for precision.
[01:17:56.260 --> 01:17:57.820]   Imagine that this were true.
[01:17:57.820 --> 01:18:00.080]   - But it was a beautiful way to imagine
[01:18:00.080 --> 01:18:03.460]   that there is a law that has those variables in it.
[01:18:03.460 --> 01:18:06.700]   And you've become quite famous these days.
[01:18:06.700 --> 01:18:09.940]   So how do you yourself optimize that equation
[01:18:09.940 --> 01:18:11.860]   with the peculiar kind of fame
[01:18:11.860 --> 01:18:13.760]   that you have gathered along the way?
[01:18:13.760 --> 01:18:14.740]   - I wanna be kinder.
[01:18:14.740 --> 01:18:16.180]   I wanna be kinder to myself.
[01:18:16.180 --> 01:18:17.380]   I wanna be kinder to others.
[01:18:17.380 --> 01:18:19.200]   I wanna be able to have heart,
[01:18:22.100 --> 01:18:24.460]   compassion, these things are really important.
[01:18:24.460 --> 01:18:28.940]   And I have a pretty spectrumy kind of approach to analysis.
[01:18:28.940 --> 01:18:30.460]   I'm quite literal.
[01:18:30.460 --> 01:18:33.100]   I can go full Rain Man on you at any given moment.
[01:18:33.100 --> 01:18:33.940]   No, I can't.
[01:18:33.940 --> 01:18:34.780]   I can't.
[01:18:34.780 --> 01:18:36.440]   It's faculty of autism, if you like.
[01:18:36.440 --> 01:18:37.380]   And people are gonna get angry
[01:18:37.380 --> 01:18:39.180]   because they want autism to be respected.
[01:18:39.180 --> 01:18:44.180]   But when you see me coding or you see me doing mathematics,
[01:18:44.180 --> 01:18:47.740]   I'm, you know, I speak with speech apnea.
[01:18:47.740 --> 01:18:49.300]   (stammering)
[01:18:49.300 --> 01:18:50.500]   Be right down to dinner.
[01:18:50.500 --> 01:18:54.180]   And we have to try to integrate ourselves
[01:18:54.180 --> 01:18:57.380]   in those tensions between, you know,
[01:18:57.380 --> 01:19:00.660]   it's sort of back to us as a worker and us as a soul.
[01:19:00.660 --> 01:19:05.540]   Many of us are optimizing one at the expense of the other.
[01:19:05.540 --> 01:19:08.020]   And I struggle with social media
[01:19:08.020 --> 01:19:09.900]   and I struggle with people making threats
[01:19:09.900 --> 01:19:11.720]   against our families.
[01:19:11.720 --> 01:19:15.860]   And I struggle with just how much pain people are in.
[01:19:15.860 --> 01:19:18.820]   And if there's one message I would like to push out there,
[01:19:20.020 --> 01:19:22.360]   you're responsible, everybody, all of us,
[01:19:22.360 --> 01:19:24.860]   myself included, with struggling.
[01:19:24.860 --> 01:19:27.500]   Struggle, struggle mightily because you,
[01:19:27.500 --> 01:19:30.780]   it's nobody else's job to do your struggle for you.
[01:19:30.780 --> 01:19:33.460]   Now with that said, if you're struggling and you're trying
[01:19:33.460 --> 01:19:35.540]   and you're trying to figure out how to better yourself
[01:19:35.540 --> 01:19:38.100]   and where you've failed, where you've let down your family,
[01:19:38.100 --> 01:19:40.780]   your friends, your workers, all this kind of stuff,
[01:19:40.780 --> 01:19:43.500]   give yourself a break.
[01:19:43.500 --> 01:19:46.380]   You know, if it's not working out,
[01:19:46.380 --> 01:19:50.300]   I have a lifelong relationship with failure and success.
[01:19:50.300 --> 01:19:52.780]   There's been no period of my life
[01:19:52.780 --> 01:19:55.900]   where both haven't been present in one form or another.
[01:19:55.900 --> 01:19:59.300]   And I do wish to say that a lot of times
[01:19:59.300 --> 01:20:01.200]   people think this is glamorous.
[01:20:01.200 --> 01:20:04.400]   I'm about to go, you know, do a show with Sam Harris.
[01:20:04.400 --> 01:20:05.760]   People are gonna listen in on two guys
[01:20:05.760 --> 01:20:07.280]   having a conversation on stage.
[01:20:07.280 --> 01:20:08.500]   It's completely crazy.
[01:20:08.500 --> 01:20:09.980]   I'm always trying to figure out how to make sure
[01:20:09.980 --> 01:20:12.300]   that those people get maximum value.
[01:20:12.300 --> 01:20:15.660]   And that's why I'm doing this podcast, you know,
[01:20:16.540 --> 01:20:18.260]   just give yourself a break.
[01:20:18.260 --> 01:20:20.500]   You owe us, you owe us your struggle.
[01:20:20.500 --> 01:20:22.900]   You don't owe your family or your coworkers
[01:20:22.900 --> 01:20:25.800]   or your lovers or your family members success.
[01:20:25.800 --> 01:20:29.680]   As long as you're in there and you're picking yourself up,
[01:20:29.680 --> 01:20:33.620]   recognize that this new situation with the economy
[01:20:33.620 --> 01:20:37.060]   that doesn't have the juice to sustain our institutions
[01:20:37.060 --> 01:20:39.700]   has caused the people who've risen to the top
[01:20:39.700 --> 01:20:43.380]   of those institutions to get quite brutal and cruel.
[01:20:43.380 --> 01:20:45.100]   Everybody is lying at the moment.
[01:20:45.100 --> 01:20:46.800]   Nobody's really a truth teller.
[01:20:46.800 --> 01:20:50.060]   Try to keep your humanity about you.
[01:20:50.060 --> 01:20:52.780]   Try to recognize that if you're failing,
[01:20:52.780 --> 01:20:54.940]   if things aren't where you want them to be
[01:20:54.940 --> 01:20:56.620]   and you're struggling and you're trying to figure out
[01:20:56.620 --> 01:20:58.420]   what you're doing wrong, what you could do,
[01:20:58.420 --> 01:21:01.260]   it's not necessarily all your fault.
[01:21:01.260 --> 01:21:02.960]   We are in a global situation.
[01:21:02.960 --> 01:21:06.060]   I have not met the people who are honest,
[01:21:06.060 --> 01:21:08.220]   kind, good, successful.
[01:21:08.220 --> 01:21:12.740]   Nobody that I've met is checking all the boxes.
[01:21:12.740 --> 01:21:14.420]   Nobody's getting all 10s.
[01:21:14.420 --> 01:21:17.340]   So I just think that's an important message
[01:21:17.340 --> 01:21:18.740]   that doesn't get pushed out enough.
[01:21:18.740 --> 01:21:21.780]   Either people wanna hold society responsible
[01:21:21.780 --> 01:21:23.980]   for their failures, which is not reasonable.
[01:21:23.980 --> 01:21:26.140]   You have to struggle, you have to try.
[01:21:26.140 --> 01:21:28.340]   Or they wanna say you're 100% responsible
[01:21:28.340 --> 01:21:30.500]   for your failures, which is total nonsense.
[01:21:30.500 --> 01:21:32.900]   - Beautifully put.
[01:21:32.900 --> 01:21:34.380]   Eric, thank you so much for talking today.
[01:21:34.380 --> 01:21:35.860]   - Thanks for having me, buddy.
[01:21:35.860 --> 01:21:38.440]   (upbeat music)
[01:21:38.440 --> 01:21:41.020]   (upbeat music)
[01:21:41.020 --> 01:21:43.600]   (upbeat music)
[01:21:43.600 --> 01:21:46.180]   (upbeat music)
[01:21:46.180 --> 01:21:48.760]   (upbeat music)
[01:21:48.760 --> 01:21:51.340]   (upbeat music)
[01:21:51.340 --> 01:22:01.340]   [BLANK_AUDIO]

