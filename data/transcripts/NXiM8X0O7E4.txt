
[00:00:00.000 --> 00:00:09.340]   Okay. Looks like we're live on YouTube as well. So hey, everybody, and welcome back
[00:00:09.340 --> 00:00:15.560]   to another session of Fastbook Reading Group. And I've got a couple announcements today.
[00:00:15.560 --> 00:00:20.160]   So just give me one sec. Let me just get things sorted. So I think it's all looking good now.
[00:00:20.160 --> 00:00:27.400]   I think YouTube is working as well. So let me just do a quick... should be fine now.
[00:00:27.400 --> 00:00:32.280]   Okay. And then you can hear me on Zoom. All right. So let me switch back to sharing my
[00:00:32.280 --> 00:00:45.960]   screen. Okay. So you should be able to see Chapter 15, Application Architectures Deep
[00:00:45.960 --> 00:00:53.960]   Dive. And I'm just going to switch to my complete screen. Please let me know in case you can't
[00:00:53.960 --> 00:00:59.360]   see my screen anymore, because I think I've just changed something. Should be fine now.
[00:00:59.360 --> 00:01:05.280]   I've got a couple announcements today. So just this morning, I've been thinking a lot
[00:01:05.280 --> 00:01:11.760]   about this lately. It's like, what is a good time to end Fastbook Reading Group? And what
[00:01:11.760 --> 00:01:18.160]   is a good time to like... I mean, what is a good time? And there is no good time, but
[00:01:18.160 --> 00:01:26.880]   I think now is still a very good... now is still like... I want to like... this Chapter
[00:01:26.880 --> 00:01:32.800]   15, after this Chapter 15, I think is where we can end things and we can call this like
[00:01:32.800 --> 00:01:39.680]   the part one. And the more that I think of it, the more that I agree with this decision.
[00:01:39.680 --> 00:01:45.840]   I even had a chat with a couple of my colleagues before joining this Fastbook session. And
[00:01:45.840 --> 00:01:51.920]   we all kind of agree that it's okay to wrap up things here. So for those of you, this
[00:01:51.920 --> 00:01:56.800]   mode... like it would come as a news to all of you, but today would be the last Fastbook
[00:01:56.800 --> 00:02:02.440]   Reading session. And as part of this reading session, we're going to cover Chapter 15.
[00:02:02.440 --> 00:02:06.000]   And we're going to look into more into CNN Learner. We're going to look into a bit of
[00:02:06.000 --> 00:02:11.480]   UNET and we're going to close off things from a computer vision perspective. And before
[00:02:11.480 --> 00:02:18.640]   I proceed with Chapter 15, I just want to say like, I want to explain the decision for
[00:02:18.640 --> 00:02:25.520]   doing so. And in doing that, I think I want to share that. What's next? I was just having
[00:02:25.520 --> 00:02:30.320]   a look at all the chapters, like what's coming. So we've got an excellent... we've got a chapter
[00:02:30.320 --> 00:02:38.960]   on SGD. This is basically explains like optimizers, explains momentum. And for those interested,
[00:02:38.960 --> 00:02:45.200]   I've got a blog post on this as well. So if I go to my personal website and there should
[00:02:45.200 --> 00:02:51.760]   be one on... there should be one on optimizers. Here it is, Adam and Friends. So if we go
[00:02:51.760 --> 00:02:57.080]   into this blog post, you will see like in this blog post, I've introduced everything
[00:02:57.080 --> 00:03:06.200]   from scratch. Basically I've re-implemented PyTorches, SGD, RMSProp, Adam, and basically
[00:03:06.200 --> 00:03:13.400]   the even momentum. So SGD with momentum and Adam has that already. So if you go and there's
[00:03:13.400 --> 00:03:18.040]   like lots of resources and credits and there's lots of places where you can learn more about
[00:03:18.040 --> 00:03:23.440]   optimizers. So I think this blog post would serve as a good place because I won't have
[00:03:23.440 --> 00:03:29.040]   anything new to share apart from sharing this blog post with everybody. So I'm just going
[00:03:29.040 --> 00:03:45.640]   to post this in the chat. And before we proceed further, if you go to 1db.me/fastbook15, that
[00:03:45.640 --> 00:03:51.360]   should take us to the community which is the forum. So let me paste that link over here
[00:03:51.360 --> 00:03:57.280]   as well. So this is... I'm just going to post here. So if anybody's actually talking about...
[00:03:57.280 --> 00:04:04.280]   Oh, hey, Sam. If anybody's talking about, please use the forum as the place to comment
[00:04:04.280 --> 00:04:09.960]   because I'm not looking at the Zoom chat and I'm not looking at the YouTube sessions. Sorry,
[00:04:09.960 --> 00:04:17.360]   the YouTube chat. But let me just quickly post this here. This is the post, blog post
[00:04:17.360 --> 00:04:22.920]   on optimizers. So if you have a look at this blog post, you'll see like everything's been
[00:04:22.920 --> 00:04:27.320]   explained nice and easy. So that covers off pretty much chapter 16. And then what we're
[00:04:27.320 --> 00:04:34.040]   left with is chapter 17, 19, and this one, which is CAM. So in chapter 17, there's foundations,
[00:04:34.040 --> 00:04:38.240]   which is pretty much PyTorch from scratch. So what you'll see in this is like modeling
[00:04:38.240 --> 00:04:44.240]   a neuron or matrix multiplication from scratch, or you'll see element-wise arithmetic. And
[00:04:44.240 --> 00:04:48.720]   there's like broadcasting. There's all of these things... Oh, sorry. There's all of
[00:04:48.720 --> 00:04:54.040]   these things that are completely PyTorch specific. And for those interested in learning about
[00:04:54.040 --> 00:04:59.800]   these things, I'd like to direct you to this another event that my brilliant colleague,
[00:04:59.800 --> 00:05:05.680]   Sanyam Bhutani is hosting, which is the PyTorch book reading group. And in this one, this
[00:05:05.680 --> 00:05:10.040]   book was written by Thomas Wiemann. And in this book, I know there's like a complete
[00:05:10.040 --> 00:05:14.880]   chapter on broadcasting. So in Fast.ai, it's like one section of the chapter. There's like
[00:05:14.880 --> 00:05:21.120]   this complete chapter and this book reading group would serve as a great place to learn
[00:05:21.120 --> 00:05:26.760]   more about PyTorch. So foundations chapter is basically PyTorch. So I'd like to direct
[00:05:26.760 --> 00:05:34.920]   everybody again to this. If you come to community.1db.ai. So if you go at this community.1db.ai,
[00:05:34.920 --> 00:05:38.800]   this brings you to the Bits and Biases forums. And from there on, if you go to community
[00:05:38.800 --> 00:05:42.560]   events, which is right here, and there you can see like all the different events that
[00:05:42.560 --> 00:05:47.240]   are currently on. And if you go to PyTorch book reading group, this will land you to
[00:05:47.240 --> 00:05:52.640]   Sanyam's reading group, which is learning PyTorch and learning everything about PyTorch.
[00:05:52.640 --> 00:05:59.840]   So that's a great, great place. And then again, skipping CAM, because that's just the paper
[00:05:59.840 --> 00:06:07.440]   and it's like CAM is more about like highlighting or like checking what the model is learning.
[00:06:07.440 --> 00:06:10.960]   So that's a great place, but I could cover that off as part of the paper reading group
[00:06:10.960 --> 00:06:16.840]   in the future. And then you have a Fast.ai learner. And in this one, it's pretty much
[00:06:16.840 --> 00:06:23.600]   creating everything in Fast.ai from scratch using PyTorch. So again, as long as everybody
[00:06:23.600 --> 00:06:28.080]   has interest in learning PyTorch and you have your foundation set for PyTorch, then we can
[00:06:28.080 --> 00:06:33.720]   come back and do this as a, Sanyam and I have decided that we're going to do this as a guest
[00:06:33.720 --> 00:06:38.240]   lecture, like both of us are going to host one together. So when the PyTorch reading
[00:06:38.240 --> 00:06:42.880]   group comes to a point where everything that's been explained in this chapter has been explained,
[00:06:42.880 --> 00:06:48.640]   like data loaders, data sets, or like all these different things, once all of these
[00:06:48.640 --> 00:06:53.120]   things have been explained, then Sanyam and I will probably host a session where we explain
[00:06:53.120 --> 00:07:00.720]   like how to build a Fast.ai learner from scratch. So I think like, what I mean to say is after
[00:07:00.720 --> 00:07:06.480]   chapter 15, there's no new concepts that I can teach you. There's nothing deep learning.
[00:07:06.480 --> 00:07:17.200]   So there's like, like, this is a good platform and where I know most people like, and myself
[00:07:17.200 --> 00:07:22.840]   from the Fast.ai community, where I know most of these people, we kind of ended at a similar
[00:07:22.840 --> 00:07:28.920]   platform. So if we did the part one Fast.ai courses, there's usually a gap of three months
[00:07:28.920 --> 00:07:37.120]   between part one and part two. And that gap was actually a blessing because in that gap,
[00:07:37.120 --> 00:07:41.080]   everybody who's part of the group can now spend their own time learning their own things.
[00:07:41.080 --> 00:07:45.000]   So if you have an interest in medical, if you have an interest in music, if you have
[00:07:45.000 --> 00:07:50.360]   any, basically audio, if you have an interest in say segmentation. So it's like, now it's
[00:07:50.360 --> 00:07:55.280]   a good platform, like everything's been set up for you to go forward and learn about these
[00:07:55.280 --> 00:08:01.840]   things nicely. And like, we already covered ResNet. So I've got another group going at
[00:08:01.840 --> 00:08:06.200]   Weights and Biases, which is the paper reading group. So I'm sorry for like taking this on
[00:08:06.200 --> 00:08:11.560]   for longer than usual, but I do want to cover like all the things that were there in my
[00:08:11.560 --> 00:08:17.920]   head that could be left in case, like if we stopped the Fast.ai reading group here. So
[00:08:17.920 --> 00:08:21.760]   if you go to this paper reading group and you see there's this like, again, in community
[00:08:21.760 --> 00:08:25.720]   events and there's a paper reading group, you can see like there's this master list,
[00:08:25.720 --> 00:08:30.280]   which contains all the papers that we've discussed so far. So if you want to look at EfficientNet,
[00:08:30.280 --> 00:08:34.440]   we do this like this paper, there's a YouTube video. Same for DenseNet. So just recently
[00:08:34.440 --> 00:08:40.040]   we did DenseNet. For ResNet, we did the ResNet live coding. So you can go to YouTube would
[00:08:40.040 --> 00:08:43.880]   be a nice place for this. So if you go YouTube and you search for the Weights and Biases
[00:08:43.880 --> 00:08:53.120]   channel, so if you go Weights and Biases. So you can see this Weights and Biases channel.
[00:08:53.120 --> 00:08:56.620]   And then if you look at the videos, you can see this, like all of these different videos
[00:08:56.620 --> 00:09:02.640]   and playlists that would serve this purpose. In videos, you can see there's like the PyTorch
[00:09:02.640 --> 00:09:07.480]   reading group, which is hosted by Sam. Then there's the DenseNet reading group. Then this
[00:09:07.480 --> 00:09:11.640]   is live. And then we have a live coding session on ResNet. So there's like all of these things
[00:09:11.640 --> 00:09:17.640]   where, again, these are these resources that are going to continue. So I think I have spoken
[00:09:17.640 --> 00:09:22.000]   about the paper reading groups before, that this is again a great time to get involved
[00:09:22.000 --> 00:09:27.640]   in reading papers. And I know for a fact that some of you have been writing blogs, for example,
[00:09:27.640 --> 00:09:32.160]   Vinayak did reach out to me and he's written a blog about DenseNet. I know Ravi Mishra,
[00:09:32.160 --> 00:09:37.880]   he's reached out to me and he's written a blog about, I think, fixing the FixRes paper.
[00:09:37.880 --> 00:09:42.280]   So I'm really, really excited to see all of that progress. So with that being said, I
[00:09:42.280 --> 00:09:52.760]   think, I hope that I'm able to justify this decision of why stopping the fast book now
[00:09:52.760 --> 00:09:59.400]   would be ideal. So with that being said, I still have some closing remarks, which I will
[00:09:59.400 --> 00:10:04.600]   do towards the end. But with that being said, let's quickly start off with Application Architecture's
[00:10:04.600 --> 00:10:10.120]   Deep Dive and cover up what I have to cover up here. But let me just go to the forum post
[00:10:10.120 --> 00:10:19.600]   and just see if there's any comments. Oh, thanks, Sam. Yes, this is the PyTorch reading
[00:10:19.600 --> 00:10:27.640]   group. Cool. So with that being said, let's get started with the architecture details.
[00:10:27.640 --> 00:10:37.680]   So with architecture details, now with architecture details, what you will see and this is something
[00:10:37.680 --> 00:10:45.320]   that we know is you pretty much, sorry about that. One second. You pretty much have an
[00:10:45.320 --> 00:10:53.280]   input image and this input image is passed through the model. And the model can be divided
[00:10:53.280 --> 00:11:01.400]   into two things. One is the body. And the other thing is the head. So for those of you
[00:11:01.400 --> 00:11:05.920]   that have been attending the ResNet and EdSnet paper reading group, this would come like
[00:11:05.920 --> 00:11:10.720]   this is something that I've explained there already. So if my input images say 3 by 2
[00:11:10.720 --> 00:11:16.720]   to 4 by 2 to 4, it just means like three channels, red, green, blue. And 2 to 4 by 2 to 4 is
[00:11:16.720 --> 00:11:22.600]   the height and width. Then there's the model in itself, like the model architecture that
[00:11:22.600 --> 00:11:29.320]   we use like ResNet, EdSnet, this can be divided into body and head. And the benefit of thinking
[00:11:29.320 --> 00:11:36.960]   of deep learning models as having a body and a head is that you can pretty much do something
[00:11:36.960 --> 00:11:40.720]   like this. And this is something that we covered off in chapter one. So when we were doing
[00:11:40.720 --> 00:11:47.720]   pre-training, what we did was we cut off the head, if you remember, when I was telling
[00:11:47.720 --> 00:11:54.160]   you about using pre-trained weights. So what you can do is you can cut off the head because
[00:11:54.160 --> 00:11:58.240]   remember when we were learning about what deep learning architecture really learns,
[00:11:58.240 --> 00:12:03.320]   we saw that the earlier layers, so there's like early layers, middle layers, and then
[00:12:03.320 --> 00:12:06.920]   there's like these later layers. So these are later layers, early layers, and then something
[00:12:06.920 --> 00:12:11.400]   in the middle. So we saw that the earlier layers are the ones that learn like really
[00:12:11.400 --> 00:12:16.080]   low level features like diagonals, edges, circles. And the middle layers learn on top
[00:12:16.080 --> 00:12:20.480]   like some faces or like eyes, eyebrows, and things like that. And then the later layers
[00:12:20.480 --> 00:12:25.360]   are the ones that learn the most specific things to the data set. So if you're working
[00:12:25.360 --> 00:12:35.360]   on ImageNet, or better yet, if you're working on PetBreez, if you're working on PetBreez
[00:12:35.360 --> 00:12:40.840]   data set, then the earlier layers will learn something like edges, diagonals. The intermediate
[00:12:40.840 --> 00:12:47.920]   layers will learn something like fur or like dog eyes or cat paws or like those sort of
[00:12:47.920 --> 00:12:53.880]   features. And then the later layers will accumulate and join all of these and learn all about
[00:12:53.880 --> 00:12:57.600]   like, okay, this is what a dog looks like. This is what the specific breed looks like
[00:12:57.600 --> 00:13:03.840]   and so on. So what we do in transfer learning or pretty much in model architectures is we
[00:13:03.840 --> 00:13:10.280]   cut off the head. So what is this head? In a way, you can think of the body as a feature
[00:13:10.280 --> 00:13:16.720]   extractor. So what that looks like is when you have your input image and you pass that
[00:13:16.720 --> 00:13:27.000]   through the body. For, let's say you're using a ResNet. So then if you have an input of
[00:13:27.000 --> 00:13:32.440]   like three by two to four by two to four, as the, again, if you've attended the ResNet
[00:13:32.440 --> 00:13:36.440]   paper reading group, you already know that as we go deeper and deeper into the ResNet
[00:13:36.440 --> 00:13:41.680]   architecture, what we do is we increase the number of channels and we decrease the spatial
[00:13:41.680 --> 00:13:46.960]   dimension. So the output looks something like five and two channels and seven by seven.
[00:13:46.960 --> 00:13:52.800]   So if it's like this big, it just becomes like really small, but has a lot of channels.
[00:13:52.800 --> 00:14:03.040]   So it looks like something like that. Something like that. So these are like five and two
[00:14:03.040 --> 00:14:09.280]   channels and then seven by seven. So you have like something that becomes really small.
[00:14:09.280 --> 00:14:14.120]   So the number of channels increases, but the spatial dimensions become really small. So
[00:14:14.120 --> 00:14:20.280]   in a way, why this is called as a feature extractor is because now if you use like a
[00:14:20.280 --> 00:14:26.120]   pooling layer, then you can get this as a five, one, two vector, right? We already know
[00:14:26.120 --> 00:14:31.560]   what pooling is. So I can represent in a way what I've done is that I've passed this image
[00:14:31.560 --> 00:14:37.200]   to something called the body, and then that's given me a five and two long vector. So I
[00:14:37.200 --> 00:14:45.520]   can represent an image as a 512 long vector, right? So this 512 could be the feature representation.
[00:14:45.520 --> 00:14:51.000]   Feature, I'm just going to call it feature. So in a way of like represented the whole
[00:14:51.000 --> 00:14:56.160]   image as a 512 vector, and I can do this for all the, like however many images I have.
[00:14:56.160 --> 00:15:01.840]   If I have another image, I pass it through the body and I get another 512 vector. So
[00:15:01.840 --> 00:15:07.360]   like I can represent things as vectors. So in a way, the body is the feature extractor.
[00:15:07.360 --> 00:15:14.920]   And then what happens next is you have something called a head. And the head is responsible
[00:15:14.920 --> 00:15:23.080]   for doing like different tasks. If you basically for all of deep learning, you want this, everything
[00:15:23.080 --> 00:15:28.560]   before this is pretty much the same, but the head is what changes. So if you want to do
[00:15:28.560 --> 00:15:37.200]   object detection, you use a different head. So if you, you can do object detection, or
[00:15:37.200 --> 00:15:45.920]   which you'd use like a bounding box head or like a, you could do segmentation. You could
[00:15:45.920 --> 00:15:54.600]   do just classification. You could do multi-label classification. So this is like all those
[00:15:54.600 --> 00:16:00.240]   sorts of things, multi-label stuff. So it's like all of these different, all of these
[00:16:00.240 --> 00:16:06.240]   different things that you can do by just having a, sorry, one sec. There's all of these like
[00:16:06.240 --> 00:16:12.760]   different things that you can do by just having a custom head. So this is the main point that
[00:16:12.760 --> 00:16:19.280]   I want to make about architectures is that a complete architecture can be just separated
[00:16:19.280 --> 00:16:24.840]   into a body and a head, and you can keep the body. And as long as you keep updating the
[00:16:24.840 --> 00:16:30.520]   head, you can do more and more things. And this is something that's been explained, that's
[00:16:30.520 --> 00:16:35.080]   been explained in the CNN learner part. So whenever we create a CNN learner, remember
[00:16:35.080 --> 00:16:40.560]   when we were doing, when we were doing like segmentation, we again used the CNN learner
[00:16:40.560 --> 00:16:45.440]   class. And when we were doing classification, we still used the CNN learner in Fast.ai.
[00:16:45.440 --> 00:16:50.200]   So the reason for that is because Fast.ai, what it does underneath is that it updates
[00:16:50.200 --> 00:16:53.920]   the head. So for segmentation, it would use something called a segmentation head. And
[00:16:53.920 --> 00:16:57.600]   for classification, it will use something called a classification head. Are there any
[00:16:57.600 --> 00:17:06.600]   questions on like this, just this basic? Oh, thanks Ramesh for this great feedback.
[00:17:06.600 --> 00:17:14.920]   Okay. So that's just the basics. That's just the basic. So now let's get started and get
[00:17:14.920 --> 00:17:23.480]   start looking into what a CNN learner is. So in, in PyTorch now, what I also want to
[00:17:23.480 --> 00:17:28.600]   do is I want to make sure that you're able to read, read and write good quality code.
[00:17:28.600 --> 00:17:33.320]   So there's like different ways of doing this is again, you could use Visual Studio Code
[00:17:33.320 --> 00:17:37.480]   or you could use WIM. So in this case, let me just show you how to use WIM. So I'm in
[00:17:37.480 --> 00:17:42.760]   Fast.ai. I guess I need to close off this and that's when this will work properly. Yeah,
[00:17:42.760 --> 00:17:47.960]   there we go. So first let me show you WIM. So let's say you want to learn about the source
[00:17:47.960 --> 00:17:52.680]   code of CNN learner. So that's another thing that we need to start looking into, right?
[00:17:52.680 --> 00:17:56.720]   So I can just go here, I can press WIM and there's something called CDAX and I can say
[00:17:56.720 --> 00:18:04.120]   tag CNN learner, and that will take me to the CNN learner source code. So like this
[00:18:04.120 --> 00:18:09.080]   is a really good way. WIM is a really good way to look at the source code. And the, another
[00:18:09.080 --> 00:18:13.840]   good way is to just use BS code, which is another great way, which I've shown you a
[00:18:13.840 --> 00:18:19.320]   few different times before. I guess this is taking some time to open up. It's opening
[00:18:19.320 --> 00:18:33.280]   up. That's from yesterday, I guess. Okay. So again, with CNN learner, what you do is
[00:18:33.280 --> 00:18:41.360]   whenever you're trying to create a CNN learner, again, the point that's being made is, the
[00:18:41.360 --> 00:18:46.280]   point that's being made is every time you use a CNN learner. So when you use the CNN
[00:18:46.280 --> 00:18:56.080]   learner function in Fast.ai, what it does is it has something called a model meta. So
[00:18:56.080 --> 00:19:01.680]   whenever you're using, like whenever I say M equals CNN learner, and I pass in like my
[00:19:01.680 --> 00:19:08.120]   ResNet-50 model, what that does is first off, it will use this something called model meta
[00:19:08.120 --> 00:19:14.380]   in Fast.ai. And this model meta has information about where to cut the model architecture.
[00:19:14.380 --> 00:19:19.100]   So again, this is for transfer learning. So as I've said, you can just have a different
[00:19:19.100 --> 00:19:26.220]   head every time. And having a different head means you cut off the architecture like this,
[00:19:26.220 --> 00:19:30.860]   like this, this red thing here, that's cutting off the architecture, because the whole architecture
[00:19:30.860 --> 00:19:35.540]   looks something like this. And then you just cut it off and you divide it into, you keep
[00:19:35.540 --> 00:19:39.780]   the left and you get away of the right and you just replace it with any new head that
[00:19:39.780 --> 00:19:46.220]   you want to use. So there's this dictionary called model meta. And if you look at this
[00:19:46.220 --> 00:19:51.180]   dictionary, you can see like this dictionary has been defined for all of these different,
[00:19:51.180 --> 00:19:56.100]   various different models. So there's like for DenseNet, it's been defined. It basically
[00:19:56.100 --> 00:20:02.220]   tells Fast.ai on where to cut the model. And it basically tells Fast.ai on like how the
[00:20:02.220 --> 00:20:07.060]   split looks like, so like how do you split the layers. And these stats just means like
[00:20:07.060 --> 00:20:12.140]   how do you normalize your input images. So now I could just say in Fast.ai, I could just
[00:20:12.140 --> 00:20:17.860]   say something called create head. So if you want to use like when we, this is again showing
[00:20:17.860 --> 00:20:21.780]   and this is again going into the details of how things have been implemented. So what
[00:20:21.780 --> 00:20:31.540]   you have is when you start with ImageNet. So ImageNet has 1000 classes, right? So what
[00:20:31.540 --> 00:20:38.460]   would happen is you have your 3x2x4x2x4 input image. When it goes through the body, let's
[00:20:38.460 --> 00:20:47.740]   say you're using ResNet-50, the output that you get looks something like 512x7x7. So this
[00:20:47.740 --> 00:20:54.860]   is like the representation of this input image. You apply a pooling layer. Next is pooling
[00:20:54.860 --> 00:21:02.500]   operation. So you get a 512x1x1 feature and you could just flatten it. So you have something
[00:21:02.500 --> 00:21:09.260]   like a 512 feature. So now your whole input image has been represented as a 512 long vector.
[00:21:09.260 --> 00:21:23.740]   And then this is what gets passed to the head. So for ImageNet, for ImageNet, the head, what
[00:21:23.740 --> 00:21:30.460]   it has to do is it has to take this 512 long vector and it has to output 1000 classes,
[00:21:30.460 --> 00:21:36.660]   right? 1000 classes because ImageNet has 1000 classes. But let's say if you want to reuse
[00:21:36.660 --> 00:21:43.420]   the same architecture for pets, then what you could do is instead of using this head,
[00:21:43.420 --> 00:21:53.360]   you could use a different head. So you could use a different head, but this time instead
[00:21:53.360 --> 00:21:58.180]   of having like 1000 classes, this just has like 37 classes because that's how many pets
[00:21:58.180 --> 00:22:03.620]   there are. So now you pass this 512 long vector into a different head and it's able to do
[00:22:03.620 --> 00:22:09.420]   pet-based classification instead of ImageNet classification. So every time you create a
[00:22:09.420 --> 00:22:14.020]   CNN learner and you say, okay, that's how many classes I want, underneath what Fast
[00:22:14.020 --> 00:22:19.460]   AI is doing, it uses the information from model meta. It cuts the architecture at this
[00:22:19.460 --> 00:22:24.340]   specific point. So it keeps all the layers except the last two layers. It cuts it off
[00:22:24.340 --> 00:22:29.940]   at that point. So it keeps all the layers except the last two layers, which is just
[00:22:29.940 --> 00:22:36.420]   the head. So it cuts off the head, creates a new head like this, and that's pretty much
[00:22:36.420 --> 00:22:41.260]   it. Then you have your CNN learner, which can do a pet classification, which can do
[00:22:41.260 --> 00:22:47.780]   ImageNet classifications. Pretty much you just update your head. So I hope by showing
[00:22:47.780 --> 00:22:55.260]   you this example of like how all architectures can be seen as having a body and a head, I
[00:22:55.260 --> 00:23:00.060]   hope that I've been able to make those things clearer in your head. It's like, how can we
[00:23:00.060 --> 00:23:06.140]   use this CNN learner function for pets and how can we use the same function for ImageNet?
[00:23:06.140 --> 00:23:12.940]   So I'll just have a look if there's more questions. Okay, there are.
[00:23:12.940 --> 00:23:23.060]   Thanks, Sanyam. That's a good one. How to get source code and VS code. Yeah, as Sanyam
[00:23:23.060 --> 00:23:29.780]   has mentioned, if you just search Fast AI like that, and you go to Fast AI deep learning
[00:23:29.780 --> 00:23:35.100]   library, and then as Sanyam is saying, now GitHub has this amazingly cool feature. If
[00:23:35.100 --> 00:23:41.900]   you press dot, I'm just on my keyboard, press dot and nothing's happening. Interesting.
[00:23:41.900 --> 00:23:50.220]   Oh, I'm not signed in, I guess. That's why. I'll have to sign in. So let me go. Just trust
[00:23:50.220 --> 00:23:58.780]   me. Like if I go into my personal here and I go Fast AI, which is just, let's say I'm
[00:23:58.780 --> 00:24:03.740]   in the Fastbook repo. I'm logged in now and I guess now I can press dot. Yeah. So if I
[00:24:03.740 --> 00:24:07.860]   press dot, you just need to be signed in into your GitHub account. And then if you press
[00:24:07.860 --> 00:24:12.860]   dot, everything will load up in your VS code. And then everything that you see I'm doing
[00:24:12.860 --> 00:24:17.620]   over here in this VS code, you can pretty much do right here. So you can still do control
[00:24:17.620 --> 00:24:23.580]   shift F to search things. You can still like look at the source code. You can go into clean.
[00:24:23.580 --> 00:24:30.820]   You can look at the, you can look at the notebooks and all that stuff. So I'll just close this.
[00:24:30.820 --> 00:24:38.340]   Okay. I have a question on head for classification. We use a certain architectures, but for segmentation,
[00:24:38.340 --> 00:24:43.380]   we usually have different base architectures like UNet, Red Internet. Ramesh, you're wrong,
[00:24:43.380 --> 00:24:50.900]   but I will tell you why. Just give me one sec. Most of the work really is in the data
[00:24:50.900 --> 00:24:56.800]   prep. Can you speak a bit about data prep, step in object detection? And we have locations.
[00:24:56.800 --> 00:25:00.900]   So we no longer have one by one is to one, but you still have a classification or even
[00:25:00.900 --> 00:25:04.340]   segmentation, but the architecture might use anchors or other techniques. How do you go
[00:25:04.340 --> 00:25:09.700]   about building data set for object detection? So for this one, while I don't want to go
[00:25:09.700 --> 00:25:14.300]   very much into object detection, I do want to give you a point you to these two block
[00:25:14.300 --> 00:25:20.700]   first, which is the annotated DETR and efficient debt. So in that one, you'll see like points
[00:25:20.700 --> 00:25:24.460]   about anchors and you'll see points about like, this is pretty much corporate detection
[00:25:24.460 --> 00:25:29.820]   data set. So if you go in there, you'll see like the annotated DETR block has this data
[00:25:29.820 --> 00:25:34.700]   preparation, which just explains everything on how you, how you do data preparation for
[00:25:34.700 --> 00:25:41.300]   object detection. So that will give you some insights about that. As far as when you say
[00:25:41.300 --> 00:25:46.100]   for segmentation and object detection, we usually have a different base architecture
[00:25:46.100 --> 00:25:50.360]   like UNet. So we're going to touch into UNet and we're going to touch into Red Internet,
[00:25:50.360 --> 00:25:59.940]   but actually even these architectures reuse ResNet as your feature extractor. So again,
[00:25:59.940 --> 00:26:05.540]   for, again, you can look at these things as a body and a head. Red Internet can be have
[00:26:05.540 --> 00:26:09.500]   like written and it has like these different configs where you can use ResNet 50, ResNet
[00:26:09.500 --> 00:26:13.900]   101 or like different things. And you could actually like use Red Internet with Tim to
[00:26:13.900 --> 00:26:19.620]   have like all these different backbones, same for UNet. So I think you can still have a
[00:26:19.620 --> 00:26:25.940]   look at these things as, as having a body and as having a, having a head. If that doesn't
[00:26:25.940 --> 00:26:30.420]   work, change GitHub to GitHub 1S. Yes, thanks very much. That's a great point as well. So
[00:26:30.420 --> 00:26:36.980]   again, what, what's been mentioned is if I go to GitHub, this one, and if I just say,
[00:26:36.980 --> 00:26:44.260]   if I just call it GitHub 1S, that should look, that should load it in VS Code already. There's
[00:26:44.260 --> 00:26:50.260]   great points. Thanks for, thanks for, thanks for putting these as comments. So that's that.
[00:26:50.260 --> 00:26:54.780]   So that's the architecture details. Now, next step is UNet Learner and we're going straight
[00:26:54.780 --> 00:27:00.420]   into UNet. While first I'd like to, again, I've just written blog posts about pretty
[00:27:00.420 --> 00:27:03.900]   much all these things. So I keep pointing to them because I think they're really good
[00:27:03.900 --> 00:27:08.500]   quality in terms of like explaining things from scratch. So for example, in this blog
[00:27:08.500 --> 00:27:15.420]   post, if you, there's like in Fastbook, if you go to UNet, there's like this section,
[00:27:15.420 --> 00:27:20.260]   which is a somewhat smaller section on, on UNet, which is just this much. But if you
[00:27:20.260 --> 00:27:25.500]   look at this blog post, I've explained everything on how to implement UNet from scratch. And
[00:27:25.500 --> 00:27:29.660]   it's all in 60 lines of PyTorch code. So by the time the whole architecture, again, I've
[00:27:29.660 --> 00:27:34.220]   got this whole really nice factory production line analogy, and I'm going to use this to
[00:27:34.220 --> 00:27:39.380]   explain UNet right now. But if you go through this blog post, you'll see like towards the
[00:27:39.380 --> 00:27:44.300]   end, this is what the, this is what the UNet architecture looks like. And overall, it's
[00:27:44.300 --> 00:27:50.540]   all just this much code, which is pretty much 60 lines of code when I counted last. So the
[00:27:50.540 --> 00:27:55.300]   whole architecture can be implemented like this. But let me try and explain what's happening
[00:27:55.300 --> 00:28:02.460]   in UNet. So in this FastAI, in this part of the Fastbook, what's, what's trying to be
[00:28:02.460 --> 00:28:09.060]   explained is like how you can do segmentation. So I've told you like an architecture has
[00:28:09.060 --> 00:28:13.900]   a body and it has a head, but then how can you do that for segmentation? Because segmentation
[00:28:13.900 --> 00:28:18.980]   is a completely different task. So when we were doing, I think it was chapter one. So
[00:28:18.980 --> 00:28:25.820]   let me see if I can bring that up. There was this really nice visualization on, on segmentation
[00:28:25.820 --> 00:28:46.940]   on Canvas. So I just want to bring that up. Should be at the bottom. Okay, here it is.
[00:28:46.940 --> 00:28:53.140]   So for a segmentation, basically what you, the task that you have to do is that for each
[00:28:53.140 --> 00:28:57.500]   image, you want to segment every pixel. So you want to say like for this image, you want
[00:28:57.500 --> 00:29:02.020]   to say all of these pixels are roads. Then you want to say like all of these pixels on
[00:29:02.020 --> 00:29:05.500]   the left are buildings. All of these pixels are buildings. And then all of these pixels
[00:29:05.500 --> 00:29:11.100]   are cars. So it's a much difficult problem to do than just like classifying, okay, pets.
[00:29:11.100 --> 00:29:14.860]   This is like the pet breed for this image, or this is the pet breed for that image and
[00:29:14.860 --> 00:29:19.220]   something like that. So segmentation is a completely different task. But what the UNet
[00:29:19.220 --> 00:29:26.740]   architecture looks like is like this. So a UNet architecture looks something like this.
[00:29:26.740 --> 00:29:32.700]   And that's what's been explained in like what happens in the FastAI UNet learner is when
[00:29:32.700 --> 00:29:38.700]   you create a FastAI UNet learner, basically you could use a ResNet-50 backbone. So you
[00:29:38.700 --> 00:29:43.460]   have your input image, which is at this point. And I'm just trying to explain like UNet and
[00:29:43.460 --> 00:29:49.540]   segmentation, just exactly what goes on in the specifics is that when you have your input
[00:29:49.540 --> 00:29:55.460]   image style, when that goes into it, these two things like this is just a small block,
[00:29:55.460 --> 00:30:03.820]   which consists of two convolutions. So let me actually put that in OneNote so I can explain
[00:30:03.820 --> 00:30:15.300]   it and pretty much annotate it. Okay. So first, let's just look at everything that's on the
[00:30:15.300 --> 00:30:21.980]   left. So just this much, okay, just everything that's on the left of this line. So you can
[00:30:21.980 --> 00:30:29.740]   see this is just like a standard encoder. This is just like a standard feature extractor.
[00:30:29.740 --> 00:30:37.660]   So you have your input image, let's say it's 3 by 572 by 572. Then you apply two convolutions,
[00:30:37.660 --> 00:30:46.580]   two conv, these are conv 3 by 3 and zero padding. So there's no padding. And when you apply
[00:30:46.580 --> 00:30:56.300]   a conv 3 by 3 to an input of 572 by 572, let me show you what happens. So let's say my
[00:30:56.300 --> 00:31:07.740]   input is torch.random 1 by 3 by 572 by 572. And I'm applying a conv, which is nn.conv2d.
[00:31:07.740 --> 00:31:17.220]   So in size in-chan is 3, out-chan is 64. Because remember what we're doing is we're going from
[00:31:17.220 --> 00:31:22.220]   the input has, well, in my case, I'm taking as 3, they're taking as 1. So let's just keep
[00:31:22.220 --> 00:31:28.820]   that the same as the image. Let's just use a one channel image. So you have input channels
[00:31:28.820 --> 00:31:34.900]   as one, but then the output channels are 64. So output channels are 64. I'm just updating
[00:31:34.900 --> 00:31:41.100]   that to be one channel. And kernel size is 3. And then let's see what the output shape
[00:31:41.100 --> 00:31:50.740]   of this is. The output shape is 1 by 64 by 570 by 570, which is exactly that's mentioned
[00:31:50.740 --> 00:31:58.420]   here 570 by 570, 64 channels. Correct? So what can be seen is like these two things
[00:31:58.420 --> 00:32:05.020]   in Unet when you're doing segmentation, both these and these are just two cons, right?
[00:32:05.020 --> 00:32:10.020]   So it's this, all of this is like just a block with two cons. So it's just look something
[00:32:10.020 --> 00:32:15.180]   like this. It's like a block. You have your first con followed by your second con. So
[00:32:15.180 --> 00:32:21.900]   I'm just going to call it con one, con two, and there's no padding in it. So then this
[00:32:21.900 --> 00:32:27.500]   block is repeated everywhere. This block is right here. Then that block is there. And
[00:32:27.500 --> 00:32:32.820]   that block is there. And then finally that block is there. So you have one, two, three,
[00:32:32.820 --> 00:32:39.740]   four, five, five such blocks of two convolutions. And then as you can see, what's really happening
[00:32:39.740 --> 00:32:45.260]   is like the number of channels. If you see, we started with one channel, then we got to
[00:32:45.260 --> 00:32:56.060]   64, then we got to 128, then we got to 256, 512 and 1024. And same, if you look at the
[00:32:56.060 --> 00:33:03.100]   spatial dimensions, we started with 572 by 572. We got down to 280 by 280, then 136 by
[00:33:03.100 --> 00:33:10.980]   136, then 64 by 64, and finally 28 by 28. So what's actually happening, and again, this
[00:33:10.980 --> 00:33:14.820]   is something I've explained for ResNet and DenseNet paper reading groups as well, is
[00:33:14.820 --> 00:33:20.180]   like the standard approach is when you go deep into your architecture, the number of
[00:33:20.180 --> 00:33:26.620]   channels goes up and the spatial dimensions goes down. So this is exactly what's happening.
[00:33:26.620 --> 00:33:46.780]   And all of this on your left, like all of this is called the encoder. All of this is
[00:33:46.780 --> 00:33:59.980]   called the encoder. Oh, sorry. This is just called, I did that again. Try and type. Oh,
[00:33:59.980 --> 00:34:04.820]   right. Nice. So this is all of this is called encoder. Now you could use a ResNet-50, you
[00:34:04.820 --> 00:34:12.660]   could use a ResNet-101, a ResNet-152 or any for that matter. But in UNet, what actually
[00:34:12.660 --> 00:34:22.700]   happens is then you have, like this part is down sampling and then you're doing up sampling,
[00:34:22.700 --> 00:34:30.780]   which just means that by the time your feature map is down to 28 by 28, and it has 1024 channels,
[00:34:30.780 --> 00:34:35.860]   then you want to go and again, take this feature map and you want to increase it back to size
[00:34:35.860 --> 00:34:40.940]   where it has like 388 by 388 size. So how do you do that? Like, so far, we've just looked
[00:34:40.940 --> 00:34:47.180]   at convolutions that decrease the feature map size, or at least if you apply padding,
[00:34:47.180 --> 00:34:52.660]   then it maintains the feature map size. But how do you go from like a 28 by 28 to 38 by
[00:34:52.660 --> 00:35:00.580]   38, like 388 by 388? How do you increase the size of your feature map? So you have a small,
[00:35:00.580 --> 00:35:07.100]   what we've seen so far is you have an input image that has three channels. And then as
[00:35:07.100 --> 00:35:11.500]   it goes through your model, which is this, I'm just going to call that model, you get
[00:35:11.500 --> 00:35:16.660]   something like this, which is now smaller in feature map size, but has a lot more number
[00:35:16.660 --> 00:35:22.820]   of channels, right? So how do you take this and go back to doing something like bigger,
[00:35:22.820 --> 00:35:26.340]   bigger spatial dimensions and less number of channels? Like how do you do that? This
[00:35:26.340 --> 00:35:32.460]   is the question. And so far, we've just looked at techniques that are capable of doing the
[00:35:32.460 --> 00:35:39.660]   encoding part, which is this encoder. But we haven't looked at this part where we can
[00:35:39.660 --> 00:35:45.420]   increase the spatial dimensions. So how does that happen? Well, in an image, like if you
[00:35:45.420 --> 00:35:49.580]   know, every image looks something like this, like it's just represented by numbers. So
[00:35:49.580 --> 00:35:54.440]   this is like 0.1, just normalized 0.25, something. These are just numbers. What you could do
[00:35:54.440 --> 00:36:03.200]   is you could take this number and like make this a bigger image like that. And it could
[00:36:03.200 --> 00:36:12.160]   be 0.1, 0.1, 0.1, 0.1 here. And then 0.25 is 0.25, 0.25, 0.25, 0.25. So pretty much
[00:36:12.160 --> 00:36:16.080]   like you're repeating the numbers at four points. Now what you have is like you have
[00:36:16.080 --> 00:36:21.720]   it's basically you've gone from, if this is 20 by 20, you've gone from 20 by 20 to 40
[00:36:21.720 --> 00:36:26.880]   by 40. But it's not really very meaningful. Like it's not the best approach to increasing
[00:36:26.880 --> 00:36:33.760]   the size. So in this part, there's another approach that's being discussed and it's called
[00:36:33.760 --> 00:36:39.440]   as a transposed convolution. So what you do with transposed convolution is that if your
[00:36:39.440 --> 00:36:46.000]   image looks something like this, what you do is you add padding in the middle. So you
[00:36:46.000 --> 00:36:55.400]   have something like that. My 0.1 goes here, 0.25 goes there. And what you've inserted
[00:36:55.400 --> 00:37:00.200]   around these are a bunch of zeros, like just zeros. So this is just padding. And then the
[00:37:00.200 --> 00:37:06.920]   next number comes here, like 0.75 or something. So you've just introduced zeros. And now what
[00:37:06.920 --> 00:37:11.680]   you could do is now when you do the convolution operation, then this becomes like one pixel
[00:37:11.680 --> 00:37:16.240]   and all of this becomes the second pixel. All of this becomes the third pixel. So pretty
[00:37:16.240 --> 00:37:23.200]   much like you've increased the size of your feature map. So this is a nice visualization
[00:37:23.200 --> 00:37:28.080]   of like how you started with a three by three. So you can see like the blue dots are the
[00:37:28.080 --> 00:37:32.920]   three by three input image, but the actual output that you get is like a five by five.
[00:37:32.920 --> 00:37:38.080]   So you've increased the size from three by three to five by five. And this is the unit.
[00:37:38.080 --> 00:37:44.440]   This is like the upsampling. So in UNet then what you do next is in this upsampling part,
[00:37:44.440 --> 00:37:49.440]   you apply these transposed convolutions, which is called as upconv. You apply these upconv.
[00:37:49.440 --> 00:37:54.880]   So you go from, you start with 52 by 52. And when you apply upconv, you have 104 by 104.
[00:37:54.880 --> 00:38:01.440]   Then you decrease the number of channels from 256 to 128 to 64. Similarly, going from 100
[00:38:01.440 --> 00:38:08.360]   to 200, from 196 to 392 and so on. So now you're doing the upsampling part in UNet.
[00:38:08.360 --> 00:38:15.440]   But one thing that again, the researchers noted at this point is that it's not just
[00:38:15.440 --> 00:38:21.840]   enough to apply upconv. What you could do is you have the feature maps on your left.
[00:38:21.840 --> 00:38:28.760]   You pretty much use a skip connection, but this time all the way to the right. So like
[00:38:28.760 --> 00:38:35.040]   you're adding the same feature map size. And this, my friends, this architecture is the
[00:38:35.040 --> 00:38:42.240]   UNet architecture. And in FastAI, I did want to like spend more time on UNet. But again,
[00:38:42.240 --> 00:38:47.680]   if you now go through this blog post of UNet, I've only explained this very briefly, but
[00:38:47.680 --> 00:38:52.160]   if you go through this blog post, all of this would be explained really, really nicely.
[00:38:52.160 --> 00:38:56.680]   And you can see like this is factory line analogy that I've used, where I say when you're
[00:38:56.680 --> 00:39:00.560]   going down the factory line, everything is like an assembly point. And when you're going
[00:39:00.560 --> 00:39:06.560]   down this yellow factory line pipeline, what's happening is that you're pretty much down
[00:39:06.560 --> 00:39:10.720]   sampling because you can see like all of this on the left is down sampling. And when you're
[00:39:10.720 --> 00:39:16.920]   going down the orange pipeline in your factory, then you're pretty much using upsampling.
[00:39:16.920 --> 00:39:20.560]   And then you can see like how the assembly station for yellow looks like it's pretty
[00:39:20.560 --> 00:39:25.960]   much looks like this, conv3/3 followed by conv3/3. And you can see how the assembly
[00:39:25.960 --> 00:39:31.360]   block for decoder looks like, like you have the features, but you also get the extra features
[00:39:31.360 --> 00:39:36.560]   from the encoder and you concatenate them and you have conv3/3 followed by conv3/3.
[00:39:36.560 --> 00:39:43.200]   So I hope that explains UNet in a lot more detail. So if you have any questions about
[00:39:43.200 --> 00:39:51.360]   UNet, now would be a good time. But otherwise, that's architecture details. I did want to
[00:39:51.360 --> 00:39:56.240]   spend more time on natural language processing in Tableau, but for natural language processing,
[00:39:56.240 --> 00:40:03.040]   again, we have a, I haven't touched natural language processing for a reason is that if
[00:40:03.040 --> 00:40:10.800]   you haven't heard of Hugging Face, you should. So Hugging Face is this place where it's like
[00:40:10.800 --> 00:40:15.840]   it's now kind of been the central place for natural language processing and transformers.
[00:40:15.840 --> 00:40:20.120]   So they have a huge number of models. If I go on models, you can see like this is just
[00:40:20.120 --> 00:40:24.880]   a small number, but you can look at token classification, text classification. I forgot
[00:40:24.880 --> 00:40:30.440]   the amount of models that they have. I think it's like a big, big number of models that
[00:40:30.440 --> 00:40:37.480]   they have and serve. And you can see there's like resources, datasets, there's good documentation
[00:40:37.480 --> 00:40:45.680]   on this. And if I remember correctly, there was, if I go to community events, there it
[00:40:45.680 --> 00:40:51.600]   is. Okay. It's blocked, but there is a Hugging Face and FastAI study group as well that was
[00:40:51.600 --> 00:41:00.000]   hosted by Weights and Biases. You can find the, you can find the, actually, if I go here,
[00:41:00.000 --> 00:41:05.920]   you know, just on YouTube and I go to Weights and Biases again. So if I just click on this,
[00:41:05.920 --> 00:41:12.800]   maybe it won't open the video. And if I go into playlist, I think there should be, here
[00:41:12.800 --> 00:41:18.720]   it is. 1DB study group, FastAI with Hugging Face. You can see like this is the FastAI
[00:41:18.720 --> 00:41:24.440]   Hugging Face course, sorry, the Hugging Face course that was hosted. And I know Sanyam
[00:41:24.440 --> 00:41:29.680]   was a part of this, but I think this would be a great, great place to look at NLP. And
[00:41:29.680 --> 00:41:36.520]   another NLP course that I would recommend is by Rachel Thomas. So Rachel Thomas NLP
[00:41:36.520 --> 00:41:42.240]   course, I've forgotten what it's called. I think it's called, oh yeah, A Code-first Introduction
[00:41:42.240 --> 00:41:52.600]   to Natural Language Processing. So let me put that on the forum. I'm saying, anyway,
[00:41:52.600 --> 00:41:58.680]   the point that I want to make is like this already existing resources. Oh, Sanyam already
[00:41:58.680 --> 00:42:12.220]   posted this. Hey Sanyam, I got you covered. NLP course. So again, this is a great, great
[00:42:12.220 --> 00:42:21.920]   place to start with NLP. It introduces everything. If you have a look at, I'm trying to find,
[00:42:21.920 --> 00:42:25.280]   lectures are in this playlist. There it is. And in that playlist, you will see like this
[00:42:25.280 --> 00:42:30.480]   everything. It does topic modeling. It does what is NLP. It starts from the basis. It
[00:42:30.480 --> 00:42:38.160]   does Naive Bayes, transfer learning, ULM fit, RNNs, sequence to sequence, GRU, and then
[00:42:38.160 --> 00:42:42.420]   introduction to the transformers and transformers for language translation. So I think this
[00:42:42.420 --> 00:42:47.040]   is really an underrated course, but it's a really, really wonderful course. And this
[00:42:47.040 --> 00:42:52.160]   is where I started my journey with NLP. And then once you're done with that, switch over
[00:42:52.160 --> 00:42:56.520]   to Hugging Face. Hugging Face is the place for transformers to learn about these papers.
[00:42:56.520 --> 00:43:03.200]   Like the attention is all you need paper. And then there's like GPT-2. And I also have
[00:43:03.200 --> 00:43:09.040]   another resource that I'd like to share on GPT-2, which is this one. And it's in the
[00:43:09.040 --> 00:43:18.680]   GPT-2. So when you get to transformers, I'm posting all of these. Once you get to transformers,
[00:43:18.680 --> 00:43:31.680]   there is another resource. So that's another resource. So that should get your NLP pretty
[00:43:31.680 --> 00:43:36.680]   much sorted. Like if you follow these resources, that should get your NLP sorted. And then
[00:43:36.680 --> 00:43:41.520]   we are left with Tabular. Tabular, I think Zach has been doing some wonderful, wonderful
[00:43:41.520 --> 00:43:46.280]   stuff with Tabular. I don't remember if there's resources that I can point you to straight
[00:43:46.280 --> 00:43:59.680]   away, but let's see. Tabular. I think there's something he's got to cover.
[00:43:59.680 --> 00:44:07.920]   Sam, help me here. Do you know a good Tabular course that I'm, I think it's part of, it
[00:44:07.920 --> 00:44:13.640]   should be part of running a walk with Fast.ai. And where is that walk with Fast.ai? Walk
[00:44:13.640 --> 00:44:21.040]   with Fast.ai. I think that's part of this. Okay. There it is. Walk with Fast.ai Tabular.
[00:44:21.040 --> 00:44:26.320]   And you'll see introduction, classification, using Tabular, pandas extension, autoencoders
[00:44:26.320 --> 00:44:31.920]   and all of that stuff. I think this is a great place to learn about more about Tabular things.
[00:44:31.920 --> 00:44:43.160]   So I'm going to post these again in the forums. Tabular. That's that. And then what are we
[00:44:43.160 --> 00:44:47.520]   left with? Computer vision we've covered as part of Fast.ai. Next thing that you'd like
[00:44:47.520 --> 00:44:53.120]   to do after this is go to iDodge image models. So learn about more models. Right now we've
[00:44:53.120 --> 00:45:04.520]   just covered ResNet, but go to this repository. I've got TimDocs. Here's another resource.
[00:45:04.520 --> 00:45:17.440]   So two more resources. Sorry for just spamming the forums, but I do want to on CV. So TimDocs
[00:45:17.440 --> 00:45:25.400]   and we have Tim. So basically like these are the places where you learn a lot about a lot
[00:45:25.400 --> 00:45:29.240]   of different models. So start reading those papers. Again, do the paper reading group.
[00:45:29.240 --> 00:45:33.760]   Because if you go to the paper reading group, you will see we're pretty much computer vision
[00:45:33.760 --> 00:45:39.160]   heavy. We actually just do computer vision in the paper reading group. So you can see
[00:45:39.160 --> 00:45:44.160]   there's like ResNet, DenseNet, EfficientNet, Squeeze and Excitation Network. We've got
[00:45:44.160 --> 00:45:49.840]   Dita, which is for object detection, Kite, which is a transformer, EfficientNet we do,
[00:45:49.840 --> 00:45:56.200]   Convid, we had Stefania join us on Facebook. For MDita we had Aishwarya Kamath join us.
[00:45:56.200 --> 00:46:02.520]   And then there's like other things that I could point you to. It's more blog posts,
[00:46:02.520 --> 00:46:06.760]   but I'll just add them to the forums. So that should be a complete list of things that can
[00:46:06.760 --> 00:46:13.960]   be referenced. I don't think I'm missing anything. If there's any questions, like if
[00:46:13.960 --> 00:46:20.200]   there's any specific resources that you'd like me to mention, please just post them
[00:46:20.200 --> 00:46:24.440]   in the forum thread. Because am I missing something? I think I should get everything
[00:46:24.440 --> 00:46:32.200]   covered. NLP is covered, computer vision is covered. Segmentation is, I think there's
[00:46:32.200 --> 00:46:40.640]   a great repository. PyTorch, segmentation models. Segmentation models by Torch. So
[00:46:40.640 --> 00:46:46.240]   this is again a great repository. I'm just going to put, post idea, segmentation models,
[00:46:46.240 --> 00:46:55.800]   PyTorch, another great repository. So I'm just doing a dump of like all of these different
[00:46:55.800 --> 00:47:02.800]   resources so you can like go and take a peek and find your topic of interest and carry
[00:47:02.800 --> 00:47:12.640]   on. But now coming to the closing words for a fast book. Again, this is an emotional moment
[00:47:12.640 --> 00:47:19.080]   for me. And all I'd like to say to everybody who's been part of this journey this long
[00:47:19.080 --> 00:47:28.760]   is a big and massive thank you. Because I think, I think like most of you have been
[00:47:28.760 --> 00:47:32.960]   a part of this journey for 15 long weeks, which is more than, more than one quarter
[00:47:32.960 --> 00:47:40.720]   that I, that I think of it. So we started off with no knowledge of deep learning. And
[00:47:40.720 --> 00:47:46.480]   I hope that we have come to a point where you have some knowledge of deep learning,
[00:47:46.480 --> 00:47:51.320]   at least like you, there's, there's things that have improved from, from where you started
[00:47:51.320 --> 00:47:57.880]   with me in this journey. And I hope like some things have improved. And I would like to
[00:47:57.880 --> 00:48:02.680]   thank the community, like all of you that have been a part of, that have been a part
[00:48:02.680 --> 00:48:07.200]   of Fastbook and you've been writing blogs week after week. And you've been doing, and
[00:48:07.200 --> 00:48:16.120]   I think Sanyam had a community, Sanyam hosted a community hangout group and he highlighted
[00:48:16.120 --> 00:48:22.200]   the work from you guys. So Vinayak was there, Ravi was there, Ravi Mishra, Ravi Chandra.
[00:48:22.200 --> 00:48:28.720]   Am I missing people? I think I can't, like, if I start taking names, like it's going to
[00:48:28.720 --> 00:48:33.520]   be a massive, massive list of like 15, 20 people that have been part of this journey
[00:48:33.520 --> 00:48:39.120]   from, from right from the beginning. And the only words that I have to say is thank you.
[00:48:39.120 --> 00:48:45.160]   And you can reach out to me anytime. And whenever you feel like you've got a question for me.
[00:48:45.160 --> 00:48:54.720]   So my Twitter actually is Twitter.com. I think that should work, right? Yes. So I'm just
[00:48:54.720 --> 00:49:03.120]   going to share where you can, places where you can reach out to me is Twitter. That would
[00:49:03.120 --> 00:49:21.240]   be a great place. LinkedIn. Oh, that's not found. Let me try. There's no, because I think
[00:49:21.240 --> 00:49:45.680]   it's still there somewhere. There we go. I think I've missed the community thread. Where
[00:49:45.680 --> 00:49:56.280]   is it? And I was writing this. So these would be places where you can reach out to me is
[00:49:56.280 --> 00:50:02.160]   like the Twitter and LinkedIn. And again, this is where we'd like to, like, I like to
[00:50:02.160 --> 00:50:08.680]   stop. But if you have any questions, yes, this is the last study group session for now.
[00:50:08.680 --> 00:50:15.800]   Because as I have pointed to other resources, like the Pytorch reading group and NLP resources,
[00:50:15.800 --> 00:50:21.080]   and I think Sanyam is doing a great job for the Pytorch reading group. Thank you for,
[00:50:21.080 --> 00:50:27.640]   thank you Ravi for such nice words. And I'm looking forward to seeing you in the paper
[00:50:27.640 --> 00:50:33.320]   reading groups. Yeah, we're going to do. So what are we doing as well as part of paper
[00:50:33.320 --> 00:50:39.840]   reading group is that I've started doing live coding sessions. And in those live coding
[00:50:39.840 --> 00:50:44.960]   sessions, I'm pretty much coding architecture from scratch. So I heard we had the first
[00:50:44.960 --> 00:50:49.960]   one where I wrote our best-in-architecture from scratch. And that's all part of the,
[00:50:49.960 --> 00:50:53.960]   if you go community events and you see a paper reading group on the forums, which should
[00:50:53.960 --> 00:50:59.720]   be here, and you will see there's this resident life coding. So you can see this resident
[00:50:59.720 --> 00:51:04.560]   life coding. You can find the, I think the YouTube link right at the top. So if I go
[00:51:04.560 --> 00:51:10.200]   actually at the top, there's this stream, you can see there's that live coding stream
[00:51:10.200 --> 00:51:15.880]   and click on it and you'll find that link. This will always be remembered as a favorite
[00:51:15.880 --> 00:51:23.120]   fast day. And thanks Sanyam, really appreciate that. Cool. I guess that's where we will end
[00:51:23.120 --> 00:51:29.360]   things today. And that's where we will end the fast book reading group and bits and biases.
[00:51:29.360 --> 00:51:33.000]   And I do look forward to seeing each and every one of you in all of these different events
[00:51:33.000 --> 00:51:39.000]   and just reach out to me on Twitter or LinkedIn if you have any questions or I can be of help
[00:51:39.000 --> 00:51:48.240]   at any time. So thanks very much and goodbye. Oh, thanks Matt for the, thanks guys. Really
[00:51:48.240 --> 00:51:53.960]   appreciate this. Thanks very much. See you and yeah, see you in the paper reading group
[00:51:53.960 --> 00:51:55.320]   I guess. Bye.
[00:51:55.320 --> 00:51:56.320]   Bye.
[00:51:56.320 --> 00:51:57.320]   Bye.
[00:51:57.320 --> 00:51:57.320]   Bye.
[00:51:57.320 --> 00:51:58.320]   Bye.
[00:51:58.320 --> 00:51:58.320]   Bye.
[00:51:58.320 --> 00:51:59.320]   Bye.
[00:51:59.320 --> 00:52:00.320]   Bye.
[00:52:00.320 --> 00:52:01.320]   Bye.
[00:52:01.320 --> 00:52:01.320]   Bye.
[00:52:01.320 --> 00:52:02.320]   Bye.
[00:52:02.320 --> 00:52:03.320]   Bye.
[00:52:03.320 --> 00:52:04.320]   Bye.
[00:52:04.320 --> 00:52:04.320]   Bye.
[00:52:04.320 --> 00:52:05.320]   Bye.
[00:52:05.320 --> 00:52:06.320]   Bye.
[00:52:06.320 --> 00:52:07.320]   Bye.
[00:52:07.320 --> 00:52:07.320]   Bye.
[00:52:07.320 --> 00:52:08.320]   Bye.
[00:52:08.320 --> 00:52:09.320]   Bye.
[00:52:09.320 --> 00:52:10.320]   Bye.
[00:52:10.320 --> 00:52:10.320]   Bye.
[00:52:11.320 --> 00:52:12.320]   Bye.
[00:52:12.320 --> 00:52:13.320]   Bye.
[00:52:13.320 --> 00:52:13.320]   Bye.
[00:52:13.320 --> 00:52:14.320]   Bye.
[00:52:14.320 --> 00:52:15.320]   Bye.
[00:52:15.320 --> 00:52:16.320]   Bye.
[00:52:16.320 --> 00:52:16.320]   Bye.
[00:52:16.320 --> 00:52:21.320]   Bye.
[00:52:21.320 --> 00:52:22.320]   Bye.
[00:52:22.320 --> 00:52:23.320]   Bye.
[00:52:23.320 --> 00:52:24.320]   Bye.
[00:52:24.320 --> 00:52:25.320]   Bye.
[00:52:25.320 --> 00:52:26.320]   Bye.
[00:52:26.320 --> 00:52:26.320]   Bye.
[00:52:26.320 --> 00:52:27.320]   Bye.
[00:52:27.320 --> 00:52:28.320]   Bye.
[00:52:28.320 --> 00:52:29.320]   Bye.
[00:52:29.320 --> 00:52:29.320]   Bye.
[00:52:29.320 --> 00:52:34.320]   Bye.
[00:52:34.320 --> 00:52:39.320]   Bye.
[00:52:39.320 --> 00:52:44.320]   Bye.
[00:52:44.320 --> 00:52:49.320]   Bye.
[00:52:49.320 --> 00:52:54.320]   Bye.
[00:52:54.320 --> 00:52:59.320]   Bye.
[00:52:59.320 --> 00:53:04.320]   Bye.
[00:53:04.320 --> 00:53:09.320]   Bye.
[00:53:09.320 --> 00:53:14.320]   Bye.
[00:53:14.320 --> 00:53:19.320]   Bye.
[00:53:19.320 --> 00:53:24.320]   Bye.
[00:53:24.320 --> 00:53:29.320]   Bye.
[00:53:29.320 --> 00:53:34.320]   Bye.
[00:53:34.320 --> 00:53:39.320]   Bye.
[00:53:39.320 --> 00:53:44.320]   Bye.
[00:53:44.320 --> 00:53:49.320]   Bye.
[00:53:49.320 --> 00:53:54.320]   Bye.
[00:53:54.320 --> 00:53:59.320]   Bye.
[00:53:59.320 --> 00:54:04.320]   Bye.
[00:54:04.320 --> 00:54:09.320]   Bye.
[00:54:09.320 --> 00:54:14.320]   Bye.
[00:54:14.320 --> 00:54:16.380]   you

