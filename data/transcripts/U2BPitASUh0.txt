
[00:00:00.000 --> 00:00:02.280]   - Welcome to the Huberman Lab Podcast,
[00:00:02.280 --> 00:00:03.680]   where we discuss science
[00:00:03.680 --> 00:00:05.880]   and science-based tools for everyday life.
[00:00:05.880 --> 00:00:10.240]   I'm Andrew Huberman,
[00:00:10.240 --> 00:00:13.480]   and I'm a professor of neurobiology and ophthalmology
[00:00:13.480 --> 00:00:15.440]   at Stanford School of Medicine.
[00:00:15.440 --> 00:00:18.080]   My guest today is Dr. Jamil Zaki.
[00:00:18.080 --> 00:00:20.600]   Dr. Jamil Zaki is a professor of psychology
[00:00:20.600 --> 00:00:22.120]   at Stanford University.
[00:00:22.120 --> 00:00:23.440]   He is also the director
[00:00:23.440 --> 00:00:26.360]   of the Social Neuroscience Laboratory at Stanford.
[00:00:26.360 --> 00:00:29.500]   His laboratory focuses on key aspects of the human experience
[00:00:29.500 --> 00:00:31.700]   such as empathy and cynicism,
[00:00:31.700 --> 00:00:34.120]   which lie at the heart of our ability to learn
[00:00:34.120 --> 00:00:36.320]   and can be barriers to learning,
[00:00:36.320 --> 00:00:38.400]   such as the case with cynicism.
[00:00:38.400 --> 00:00:40.640]   Today, you'll learn the optimal mindsets to adopt
[00:00:40.640 --> 00:00:44.520]   when trying to understand how to learn conflict resolution
[00:00:44.520 --> 00:00:47.280]   and how to navigate relationships of all kinds
[00:00:47.280 --> 00:00:48.780]   and in all contexts,
[00:00:48.780 --> 00:00:52.120]   including personal relationships and in the workplace.
[00:00:52.120 --> 00:00:54.520]   What sets Dr. Zaki's work apart from others
[00:00:54.520 --> 00:00:56.840]   is that he's able to take laboratory research
[00:00:56.840 --> 00:00:58.920]   and apply that to real-world scenarios
[00:00:58.920 --> 00:01:00.480]   to direct optimal strategies
[00:01:00.480 --> 00:01:03.080]   for things like how to set personal boundaries,
[00:01:03.080 --> 00:01:05.360]   how to learn information in uncertain
[00:01:05.360 --> 00:01:08.120]   and sometimes even uncomfortable environments,
[00:01:08.120 --> 00:01:10.040]   and then how to bring that to bear
[00:01:10.040 --> 00:01:12.080]   in terms of your relationship to yourself,
[00:01:12.080 --> 00:01:13.480]   your relationship to others,
[00:01:13.480 --> 00:01:16.880]   and how to collaborate with others in more effective ways.
[00:01:16.880 --> 00:01:18.940]   I wanna be very clear that today's discussion,
[00:01:18.940 --> 00:01:21.800]   while focused on cynicism, trust, and empathy,
[00:01:21.800 --> 00:01:23.040]   is anything but squishy.
[00:01:23.040 --> 00:01:25.820]   In fact, it focuses on experimental data
[00:01:25.820 --> 00:01:28.040]   derived from real-world contexts.
[00:01:28.040 --> 00:01:30.520]   So it is both grounded in solid research
[00:01:30.520 --> 00:01:32.240]   and it is very practical,
[00:01:32.240 --> 00:01:34.280]   such that by the end of today's episode,
[00:01:34.280 --> 00:01:35.760]   you'll be armed with new knowledge
[00:01:35.760 --> 00:01:38.380]   about what cynicism is and is not,
[00:01:38.380 --> 00:01:40.220]   what empathy is and is not.
[00:01:40.220 --> 00:01:42.680]   This is very important because there's a lot of confusion
[00:01:42.680 --> 00:01:45.040]   about these words and what they mean.
[00:01:45.040 --> 00:01:47.480]   But I can assure you that by the end of today's discussion,
[00:01:47.480 --> 00:01:49.920]   you will have new frameworks and indeed new tools,
[00:01:49.920 --> 00:01:52.800]   protocols that you can use as strategies
[00:01:52.800 --> 00:01:56.480]   to better navigate situations and relationships of all kinds
[00:01:56.480 --> 00:01:58.760]   and indeed to learn better.
[00:01:58.760 --> 00:02:00.600]   I'd also like to mention that Dr. Zaki
[00:02:00.600 --> 00:02:04.480]   has authored a terrific new book entitled "Hope for Cynics,
[00:02:04.480 --> 00:02:06.720]   the Surprising Science of Human Goodness."
[00:02:06.720 --> 00:02:09.700]   And I've read this book and it is spectacular.
[00:02:09.700 --> 00:02:12.240]   There's a link to the book in the show note captions.
[00:02:12.240 --> 00:02:15.120]   Before we begin, I'd like to emphasize that this podcast
[00:02:15.120 --> 00:02:17.900]   is separate from my teaching and research roles at Stanford.
[00:02:17.900 --> 00:02:20.180]   It is, however, part of my desire and effort
[00:02:20.180 --> 00:02:22.160]   to bring zero cost to consumer information
[00:02:22.160 --> 00:02:24.200]   about science and science-related tools
[00:02:24.200 --> 00:02:25.560]   to the general public.
[00:02:25.560 --> 00:02:26.640]   In keeping with that theme,
[00:02:26.640 --> 00:02:29.580]   I'd like to thank the sponsors of today's podcast.
[00:02:29.580 --> 00:02:31.600]   Our first sponsor is Maui Nui.
[00:02:31.600 --> 00:02:33.840]   Maui Nui venison is the most nutrient dense
[00:02:33.840 --> 00:02:36.000]   and delicious red meat available.
[00:02:36.000 --> 00:02:37.580]   I've spoken before on this podcast
[00:02:37.580 --> 00:02:39.240]   about the fact that most of us should be seeking
[00:02:39.240 --> 00:02:41.680]   to get about one gram of quality protein
[00:02:41.680 --> 00:02:44.240]   per pound of body weight every day.
[00:02:44.240 --> 00:02:46.160]   That protein provides critical building blocks
[00:02:46.160 --> 00:02:48.400]   for things like muscle repair and synthesis,
[00:02:48.400 --> 00:02:49.960]   but also promotes overall health
[00:02:49.960 --> 00:02:52.320]   given the importance of muscle as an organ.
[00:02:52.320 --> 00:02:53.920]   Eating enough quality protein each day
[00:02:53.920 --> 00:02:56.560]   is also a terrific way to stave off hunger.
[00:02:56.560 --> 00:02:57.740]   One of the key things, however,
[00:02:57.740 --> 00:03:00.160]   is to make sure that you're getting enough quality protein
[00:03:00.160 --> 00:03:02.320]   without ingesting excess calories.
[00:03:02.320 --> 00:03:04.760]   Maui Nui venison has an extremely high quality
[00:03:04.760 --> 00:03:06.640]   protein to calorie ratio,
[00:03:06.640 --> 00:03:08.480]   such that getting that one gram of protein
[00:03:08.480 --> 00:03:10.600]   per pound of body weight is both easy
[00:03:10.600 --> 00:03:12.240]   and doesn't cause you to ingest
[00:03:12.240 --> 00:03:14.020]   an excess amount of calories.
[00:03:14.020 --> 00:03:17.160]   Also, Maui Nui venison is absolutely delicious.
[00:03:17.160 --> 00:03:18.640]   They have venison steaks,
[00:03:18.640 --> 00:03:21.000]   ground venison, and venison bone broth.
[00:03:21.000 --> 00:03:23.060]   I personally like and eat all of those.
[00:03:23.060 --> 00:03:25.440]   In fact, I probably eat a Maui Nui venison burger
[00:03:25.440 --> 00:03:26.720]   pretty much every day,
[00:03:26.720 --> 00:03:29.520]   and occasionally I'll swap that for a Maui Nui steak.
[00:03:29.520 --> 00:03:31.880]   And if you're traveling a lot or simply on the go,
[00:03:31.880 --> 00:03:34.640]   they have a very convenient Maui Nui venison jerky,
[00:03:34.640 --> 00:03:37.240]   which has 10 grams of quality protein per stick
[00:03:37.240 --> 00:03:39.020]   at just 55 calories.
[00:03:39.020 --> 00:03:41.920]   While Maui Nui offers the highest quality meat available,
[00:03:41.920 --> 00:03:43.680]   their supplies are limited.
[00:03:43.680 --> 00:03:45.360]   Responsible population management
[00:03:45.360 --> 00:03:47.440]   of the Axis deer on the island of Maui
[00:03:47.440 --> 00:03:50.240]   means that they will not go beyond harvest capacity.
[00:03:50.240 --> 00:03:52.440]   Signing up for a membership is therefore the best way
[00:03:52.440 --> 00:03:55.140]   to ensure access to their high quality meat.
[00:03:55.140 --> 00:03:56.960]   If you'd like to try Maui Nui venison,
[00:03:56.960 --> 00:04:00.200]   you can go to mauinuivenison.com/huberman
[00:04:00.200 --> 00:04:03.120]   to get 20% off your membership or first order.
[00:04:03.120 --> 00:04:06.960]   Again, that's mauinuivenison.com/huberman.
[00:04:06.960 --> 00:04:09.680]   Today's episode is also brought to us by Juve.
[00:04:09.680 --> 00:04:13.040]   Juve makes medical grade red light therapy devices.
[00:04:13.040 --> 00:04:15.060]   Now, if there's one thing I've consistently emphasized
[00:04:15.060 --> 00:04:16.160]   on this podcast,
[00:04:16.160 --> 00:04:17.840]   it's the incredible impact
[00:04:17.840 --> 00:04:19.760]   that light can have on our biology.
[00:04:19.760 --> 00:04:21.160]   Now, in addition to sunlight,
[00:04:21.160 --> 00:04:22.880]   red light and near-infrared light
[00:04:22.880 --> 00:04:24.520]   have been shown to have positive effects
[00:04:24.520 --> 00:04:27.520]   on improving numerous aspects of cellular and organ health,
[00:04:27.520 --> 00:04:29.160]   including faster muscle recovery,
[00:04:29.160 --> 00:04:31.080]   improved skin health and wound healing,
[00:04:31.080 --> 00:04:34.200]   even improvements in acne, reducing pain and inflammation,
[00:04:34.200 --> 00:04:35.760]   improving mitochondrial function,
[00:04:35.760 --> 00:04:37.840]   and even improving vision itself.
[00:04:37.840 --> 00:04:39.280]   What sets Juve lights apart,
[00:04:39.280 --> 00:04:42.040]   and why they're my preferred red light therapy devices,
[00:04:42.040 --> 00:04:44.180]   is that they use clinically proven wavelengths,
[00:04:44.180 --> 00:04:46.460]   meaning it uses specific wavelengths of red light
[00:04:46.460 --> 00:04:48.520]   and near-infrared light in combination
[00:04:48.520 --> 00:04:51.020]   to trigger the optimal cellular adaptations.
[00:04:51.020 --> 00:04:53.160]   Personally, I use the Juve handheld light,
[00:04:53.160 --> 00:04:54.660]   both at home and when I travel.
[00:04:54.660 --> 00:04:56.120]   It's only about the size of a sandwich,
[00:04:56.120 --> 00:04:58.360]   so it's super portable and convenient to use.
[00:04:58.360 --> 00:05:00.080]   I also have a Juve whole body panel,
[00:05:00.080 --> 00:05:02.760]   and I use that about three or four times per week.
[00:05:02.760 --> 00:05:04.000]   If you'd like to try Juve,
[00:05:04.000 --> 00:05:08.960]   you can go to juve, spelled J-O-O-V-V.com/huberman.
[00:05:08.960 --> 00:05:10.780]   Juve is offering an exclusive discount
[00:05:10.780 --> 00:05:12.360]   to all Huberman Loud listeners,
[00:05:12.360 --> 00:05:15.280]   with up to $400 off select Juve products.
[00:05:15.280 --> 00:05:19.200]   Again, that's Juve, J-O-O-V-V.com/huberman,
[00:05:19.200 --> 00:05:22.000]   to get $400 off select Juve products.
[00:05:22.000 --> 00:05:25.200]   Today's episode is also brought to us by Waking Up.
[00:05:25.200 --> 00:05:26.720]   Waking Up is a meditation app
[00:05:26.720 --> 00:05:29.180]   that offers hundreds of guided meditation programs,
[00:05:29.180 --> 00:05:32.560]   mindfulness trainings, yoga nidra sessions, and more.
[00:05:32.560 --> 00:05:34.000]   I started practicing meditation
[00:05:34.000 --> 00:05:35.840]   when I was about 15 years old,
[00:05:35.840 --> 00:05:38.480]   and it made a profound impact on my life.
[00:05:38.480 --> 00:05:39.860]   And by now, there are thousands
[00:05:39.860 --> 00:05:41.440]   of quality peer-reviewed studies
[00:05:41.440 --> 00:05:44.680]   that emphasize how useful mindfulness meditation can be
[00:05:44.680 --> 00:05:47.440]   for improving our focus, managing stress and anxiety,
[00:05:47.440 --> 00:05:49.720]   improving our mood, and much more.
[00:05:49.720 --> 00:05:51.200]   In recent years, I started using
[00:05:51.200 --> 00:05:53.040]   the Waking Up app for my meditations,
[00:05:53.040 --> 00:05:55.180]   because I find it to be a terrific resource
[00:05:55.180 --> 00:05:56.940]   for allowing me to really be consistent
[00:05:56.940 --> 00:05:58.740]   with my meditation practice.
[00:05:58.740 --> 00:06:00.840]   Many people start a meditation practice
[00:06:00.840 --> 00:06:02.160]   and experience some benefits,
[00:06:02.160 --> 00:06:03.880]   but many people also have challenges
[00:06:03.880 --> 00:06:05.540]   keeping up with that practice.
[00:06:05.540 --> 00:06:07.080]   What I and so many other people love
[00:06:07.080 --> 00:06:08.120]   about the Waking Up app
[00:06:08.120 --> 00:06:10.040]   is that it has a lot of different meditations
[00:06:10.040 --> 00:06:10.920]   to choose from,
[00:06:10.920 --> 00:06:13.440]   and those meditations are of different durations.
[00:06:13.440 --> 00:06:15.040]   So it makes it very easy to keep up
[00:06:15.040 --> 00:06:16.560]   with your meditation practice,
[00:06:16.560 --> 00:06:18.440]   both from the perspective of novelty,
[00:06:18.440 --> 00:06:20.160]   you never get tired of those meditations,
[00:06:20.160 --> 00:06:21.720]   there's always something new to explore
[00:06:21.720 --> 00:06:23.000]   and to learn about yourself
[00:06:23.000 --> 00:06:25.360]   and about the effectiveness of meditation,
[00:06:25.360 --> 00:06:28.080]   and you can always fit meditation into your schedule,
[00:06:28.080 --> 00:06:30.860]   even if you only have two or three minutes per day
[00:06:30.860 --> 00:06:31.960]   in which to meditate.
[00:06:31.960 --> 00:06:33.640]   I also really like doing yoga nidra,
[00:06:33.640 --> 00:06:36.000]   or what is sometimes called non-sleep deep rest,
[00:06:36.000 --> 00:06:37.560]   for about 10 or 20 minutes,
[00:06:37.560 --> 00:06:39.800]   because it is a great way to restore mental
[00:06:39.800 --> 00:06:42.000]   and physical vigor without the tiredness
[00:06:42.000 --> 00:06:43.440]   that some people experience when they wake up
[00:06:43.440 --> 00:06:44.720]   from a conventional nap.
[00:06:44.720 --> 00:06:46.280]   If you'd like to try the Waking Up app,
[00:06:46.280 --> 00:06:49.120]   please go to wakingup.com/huberman,
[00:06:49.120 --> 00:06:51.200]   where you can access a free 30-day trial.
[00:06:51.200 --> 00:06:54.080]   Again, that's wakingup.com/huberman
[00:06:54.080 --> 00:06:56.280]   to access a free 30-day trial.
[00:06:56.280 --> 00:06:59.600]   And now for my discussion with Dr. Jamil Zaki.
[00:06:59.600 --> 00:07:02.080]   Dr. Jamil Zaki, welcome.
[00:07:02.080 --> 00:07:03.480]   - Thanks so much for having me.
[00:07:03.480 --> 00:07:05.480]   - Delighted to have you here.
[00:07:05.480 --> 00:07:07.120]   And to learn from you,
[00:07:07.120 --> 00:07:09.520]   you have decided to tackle
[00:07:09.520 --> 00:07:12.840]   an enormous number of very interesting
[00:07:12.840 --> 00:07:14.720]   and challenging topics.
[00:07:14.720 --> 00:07:17.880]   Challenging because my read of it,
[00:07:17.880 --> 00:07:18.760]   not just your book,
[00:07:18.760 --> 00:07:21.440]   but of these fields in the science that you've done,
[00:07:21.440 --> 00:07:26.280]   is that people default to some complicated states
[00:07:26.280 --> 00:07:29.880]   and emotions sometimes that in some ways serve them well,
[00:07:29.880 --> 00:07:32.560]   in some ways serve them less well.
[00:07:32.560 --> 00:07:34.080]   So I'd like to talk about this at the level
[00:07:34.080 --> 00:07:38.120]   of the individual and interactions between pairs
[00:07:38.120 --> 00:07:40.120]   and larger groups and so on.
[00:07:40.120 --> 00:07:43.040]   But just to kick things off,
[00:07:43.040 --> 00:07:45.320]   what is cynicism?
[00:07:45.320 --> 00:07:47.240]   You know, I have my own ideas,
[00:07:47.240 --> 00:07:49.160]   but what is cynicism?
[00:07:49.160 --> 00:07:53.640]   What does it serve in terms of its role in the human mind?
[00:07:53.640 --> 00:07:57.400]   - The way that psychologists think of cynicism these days
[00:07:57.400 --> 00:08:01.440]   is as a theory, a theory about human beings.
[00:08:01.440 --> 00:08:05.680]   It's the idea that generally people at their core
[00:08:05.680 --> 00:08:08.880]   are selfish, greedy, and dishonest.
[00:08:08.880 --> 00:08:11.400]   Now, that's not to say that a cynical person
[00:08:11.400 --> 00:08:14.880]   will deny that somebody could act kindly, for instance,
[00:08:14.880 --> 00:08:17.720]   could donate to charity, could help a stranger,
[00:08:17.720 --> 00:08:20.240]   but they would say all of that,
[00:08:20.240 --> 00:08:22.720]   all of that kind and friendly behavior
[00:08:22.720 --> 00:08:26.480]   is a thin veneer covering up who we really are,
[00:08:26.480 --> 00:08:29.120]   which is self-interested.
[00:08:29.120 --> 00:08:30.880]   Another way of putting this is,
[00:08:30.880 --> 00:08:33.000]   you know, there are these ancient philosophical questions
[00:08:33.000 --> 00:08:34.080]   about people.
[00:08:34.080 --> 00:08:35.960]   Are we good or bad?
[00:08:35.960 --> 00:08:37.320]   Kind or cruel?
[00:08:37.320 --> 00:08:39.080]   Caring or callous?
[00:08:39.080 --> 00:08:42.340]   And cynicism is answering all of those
[00:08:42.340 --> 00:08:45.140]   in the relatively bleak way that you might.
[00:08:45.140 --> 00:08:50.160]   - I believe in your book, you quote Kurt Vonnegut,
[00:08:50.160 --> 00:08:53.400]   who says, "We are who we pretend to be,
[00:08:53.400 --> 00:08:56.760]   "so we need to be careful who we pretend to be."
[00:08:56.760 --> 00:08:58.200]   What do you think that quote means?
[00:08:58.200 --> 00:09:00.380]   How do you interpret that quote?
[00:09:00.380 --> 00:09:01.220]   - Thanks for bringing that up.
[00:09:01.220 --> 00:09:04.140]   Kurt Vonnegut, one of my favorite authors,
[00:09:04.140 --> 00:09:07.740]   and to me, that quote is enormously powerful
[00:09:07.740 --> 00:09:12.420]   because it expresses the idea of self-fulfilling prophecies.
[00:09:12.420 --> 00:09:17.460]   You know, there's this subjective sense that people have
[00:09:17.460 --> 00:09:20.060]   that our version of the world is the world,
[00:09:20.060 --> 00:09:22.900]   that we are passively taking in information,
[00:09:22.900 --> 00:09:26.380]   veridically, dispassionately,
[00:09:26.380 --> 00:09:28.460]   and in fact, that's not the case.
[00:09:28.460 --> 00:09:32.340]   We each construct our own version of the world.
[00:09:32.340 --> 00:09:36.180]   And so, for instance, if you think about cynicism, right?
[00:09:36.180 --> 00:09:37.420]   Are people kind or cruel?
[00:09:37.420 --> 00:09:40.080]   That's pretty much an unanswerable question
[00:09:40.080 --> 00:09:41.140]   at the level of science.
[00:09:41.140 --> 00:09:43.380]   It's a philosophical, some could argue,
[00:09:43.380 --> 00:09:45.380]   even a theological question.
[00:09:45.380 --> 00:09:47.700]   But it turns out that the way you answer that
[00:09:47.700 --> 00:09:52.040]   goes a long way in constructing and shaping
[00:09:52.040 --> 00:09:56.100]   the life that you live, the decisions that you make.
[00:09:56.100 --> 00:09:58.140]   So cynics, maybe it's not so much
[00:09:58.140 --> 00:09:59.760]   about who they pretend to be,
[00:09:59.760 --> 00:10:03.580]   but it's about who they pretend everybody else is, right?
[00:10:03.580 --> 00:10:07.700]   If you decide that other people are selfish, for instance,
[00:10:07.700 --> 00:10:10.340]   you'll be far less likely to trust them.
[00:10:10.340 --> 00:10:12.620]   And there's a lot of evidence that cynics,
[00:10:12.620 --> 00:10:15.140]   when they're put in situations with new people,
[00:10:15.140 --> 00:10:18.260]   even when they interact with their friends,
[00:10:18.260 --> 00:10:20.780]   romantic partners, and families,
[00:10:20.780 --> 00:10:22.560]   that they still have their guard up,
[00:10:22.560 --> 00:10:25.440]   that they're not able to make trusting
[00:10:25.440 --> 00:10:27.780]   and deep connections with other people.
[00:10:27.780 --> 00:10:28.620]   But guess what?
[00:10:28.620 --> 00:10:30.220]   When you treat other people in that way,
[00:10:30.220 --> 00:10:32.040]   a couple of things happen.
[00:10:32.040 --> 00:10:34.800]   One, you're not able to receive
[00:10:34.800 --> 00:10:38.120]   what most of us need from social connections.
[00:10:38.120 --> 00:10:41.600]   There's one really classic and very sad study
[00:10:41.600 --> 00:10:45.320]   where people were forced to give an extemporaneous speech
[00:10:45.320 --> 00:10:47.020]   about a subject they don't know much about,
[00:10:47.020 --> 00:10:49.300]   a very stressful experience
[00:10:49.300 --> 00:10:51.260]   that raised people's blood pressure.
[00:10:52.320 --> 00:10:54.760]   Some of these folks had a cheerleader,
[00:10:54.760 --> 00:10:58.440]   not an actual cheerleader, but a friendly stranger
[00:10:58.440 --> 00:10:59.880]   who was with them while they prepared,
[00:10:59.880 --> 00:11:02.140]   saying, "You've got this, I know you can do it.
[00:11:02.140 --> 00:11:03.840]   "I'm in your corner."
[00:11:03.840 --> 00:11:06.180]   Other people had no support.
[00:11:06.180 --> 00:11:09.280]   As you know, one of the great things about social support
[00:11:09.280 --> 00:11:12.100]   is that it buffers us from stress.
[00:11:12.100 --> 00:11:14.080]   So most people,
[00:11:14.080 --> 00:11:17.000]   when they had this friendly person by their side,
[00:11:17.000 --> 00:11:19.520]   their blood pressure, as they prepared for the speech,
[00:11:19.520 --> 00:11:23.240]   went up only half as much as when they were alone.
[00:11:23.240 --> 00:11:26.480]   But cynical people had a spike in their blood pressure
[00:11:26.480 --> 00:11:29.720]   that was indistinguishable in magnitude
[00:11:29.720 --> 00:11:33.120]   whether or not a person was by their side or not.
[00:11:33.120 --> 00:11:35.440]   One way that I think about this is,
[00:11:35.440 --> 00:11:39.120]   social connection is a deep and necessary form
[00:11:39.120 --> 00:11:41.480]   of psychological nourishment.
[00:11:41.480 --> 00:11:43.640]   And living a cynical life,
[00:11:43.640 --> 00:11:47.400]   making the decision that most people can't be trusted,
[00:11:47.400 --> 00:11:51.480]   stops you from being able to metabolize those calories,
[00:11:51.480 --> 00:11:55.740]   leaves you malnourished in a social way.
[00:11:55.740 --> 00:11:59.160]   A second thing that happens when you choose
[00:11:59.160 --> 00:12:03.700]   to pretend that others are selfish, greedy, and dishonest
[00:12:03.700 --> 00:12:06.920]   is that you bring out the worst in them.
[00:12:06.920 --> 00:12:08.880]   There's a lot of research that finds
[00:12:08.880 --> 00:12:11.220]   that cynical people tend to do things
[00:12:11.220 --> 00:12:13.980]   like monitoring others, spying on them,
[00:12:13.980 --> 00:12:15.560]   or threatening them to make sure
[00:12:15.560 --> 00:12:18.480]   that that other person doesn't betray them.
[00:12:18.480 --> 00:12:22.200]   But of course, other people can tell how we're treating them
[00:12:22.200 --> 00:12:24.560]   and they reciprocate our kindness
[00:12:24.560 --> 00:12:27.840]   and retaliate against our unkindness.
[00:12:27.840 --> 00:12:30.600]   So cynical people end up bringing out
[00:12:30.600 --> 00:12:33.260]   the most selfish qualities of others,
[00:12:33.260 --> 00:12:35.840]   telling a story full of villains
[00:12:35.840 --> 00:12:38.640]   and then ending up stuck living in that story.
[00:12:38.640 --> 00:12:41.840]   - How early in life does cynicism show up?
[00:12:41.840 --> 00:12:44.460]   I'm thinking about "Sesame Street" characters,
[00:12:44.460 --> 00:12:47.720]   which to me embody different neural circuits.
[00:12:47.720 --> 00:12:51.000]   You know, you've got Cookie Monster,
[00:12:51.000 --> 00:12:53.960]   some strong dopaminergic drive there.
[00:12:53.960 --> 00:12:55.480]   Knows what he wants, knows what he likes,
[00:12:55.480 --> 00:12:56.360]   and he's gonna get it.
[00:12:56.360 --> 00:12:58.280]   - That great prefrontal system, maybe.
[00:12:58.280 --> 00:13:01.160]   - Right, even if he has to eat the box
[00:13:01.160 --> 00:13:03.780]   in order to get to the cookie quicker.
[00:13:03.780 --> 00:13:07.920]   You have Elmo, who's all loving,
[00:13:07.920 --> 00:13:09.440]   and you have Oscar the Grouch.
[00:13:09.440 --> 00:13:13.520]   Somewhat cynical, but certainly grouchy.
[00:13:13.520 --> 00:13:18.520]   And then in, you know, essentially every fairy tale
[00:13:18.520 --> 00:13:23.220]   or every Christmas story or, you know,
[00:13:23.220 --> 00:13:25.960]   there seems to be sort of a skeptic
[00:13:25.960 --> 00:13:27.620]   or somebody that can't be brought on board
[00:13:27.620 --> 00:13:31.340]   the celebration that one would otherwise have.
[00:13:31.340 --> 00:13:35.540]   But even though kids are learning about cynicism
[00:13:35.540 --> 00:13:40.380]   and grouchiness and curmudgeons,
[00:13:40.380 --> 00:13:43.640]   I often think about those phenotypes in older folks
[00:13:43.640 --> 00:13:45.080]   because that's how they've been written
[00:13:45.080 --> 00:13:47.000]   into most of those stories.
[00:13:47.000 --> 00:13:48.840]   I guess Oscar the Grouch is,
[00:13:48.840 --> 00:13:50.520]   we don't know how old Oscar is.
[00:13:50.520 --> 00:13:53.560]   If one observes children,
[00:13:53.560 --> 00:13:58.500]   how early can you observe classically defined cynicism?
[00:13:58.500 --> 00:14:01.080]   - That's a great question.
[00:14:01.080 --> 00:14:04.000]   Classically defined cynicism would be hard
[00:14:04.000 --> 00:14:05.400]   to measure very early in life
[00:14:05.400 --> 00:14:07.800]   because you typically measure it through self-report.
[00:14:07.800 --> 00:14:10.820]   So people have to have relatively well-developed,
[00:14:10.820 --> 00:14:13.300]   elaborated stories that they can tell you
[00:14:13.300 --> 00:14:16.060]   about their version of the world.
[00:14:16.060 --> 00:14:20.640]   That said, one early experience and one early phenotype
[00:14:20.640 --> 00:14:24.580]   that's very strongly correlated with generalized mistrust
[00:14:24.580 --> 00:14:27.660]   and unwillingness to count on other people
[00:14:27.660 --> 00:14:30.980]   would be insecure attachment early in life.
[00:14:30.980 --> 00:14:35.440]   So for instance, you might know, but just for listeners,
[00:14:35.440 --> 00:14:38.680]   insecure attachment is a way of describing
[00:14:38.680 --> 00:14:41.580]   how kids experience the social world.
[00:14:41.580 --> 00:14:43.220]   It's often tested using something known
[00:14:43.220 --> 00:14:45.980]   as the strange situation where a one-year-old
[00:14:45.980 --> 00:14:49.560]   is brought to a lab with their caregiver,
[00:14:49.560 --> 00:14:52.860]   mother, father, whoever is caring for them.
[00:14:52.860 --> 00:14:54.480]   They're in a novel environment
[00:14:54.480 --> 00:14:56.340]   and researchers are observing
[00:14:56.340 --> 00:14:57.740]   how much do they explore the space?
[00:14:57.740 --> 00:14:59.780]   How comfortable do they seem?
[00:14:59.780 --> 00:15:03.580]   Then after that, a stranger enters the room.
[00:15:03.580 --> 00:15:07.020]   Couple minutes after that, their mother leaves the room
[00:15:07.020 --> 00:15:09.000]   or their caregiver leaves the room,
[00:15:09.000 --> 00:15:11.340]   which is of course incredibly strange
[00:15:11.340 --> 00:15:13.340]   and stressful for most one-year-olds.
[00:15:13.340 --> 00:15:16.560]   The caregiver then returns after a minute
[00:15:16.560 --> 00:15:19.420]   and what researchers look at is a few things.
[00:15:19.420 --> 00:15:22.700]   One, how comfortable is the child exploring a space
[00:15:22.700 --> 00:15:24.820]   with their caregiver present?
[00:15:24.820 --> 00:15:28.140]   Two, how comfortable are they when other people are around?
[00:15:28.140 --> 00:15:31.060]   Three, how do they react when their caregiver leaves?
[00:15:31.060 --> 00:15:34.020]   And four, how do they react at the reunion
[00:15:34.020 --> 00:15:35.700]   with their caregiver?
[00:15:35.700 --> 00:15:38.580]   And the majority of kids, approximately 2/3 of them,
[00:15:38.580 --> 00:15:42.380]   are securely attached, meaning that they are comfortable
[00:15:42.380 --> 00:15:45.820]   exploring a new space, they get really freaked out,
[00:15:45.820 --> 00:15:48.420]   of course, as you might when their caregiver leaves,
[00:15:48.420 --> 00:15:51.880]   but then they soothe quickly when their caregiver returns.
[00:15:51.880 --> 00:15:56.780]   The remaining 1/3 or so of kids are insecurely attached,
[00:15:56.780 --> 00:15:59.180]   meaning that they're skittish in new environments
[00:15:59.180 --> 00:16:02.140]   even when their parent or caregiver is there.
[00:16:02.140 --> 00:16:04.940]   They really freak out when their caregiver leaves
[00:16:04.940 --> 00:16:08.500]   and they're not very soothed upon their return.
[00:16:08.500 --> 00:16:11.260]   Now, for a long time, attachment style was viewed
[00:16:11.260 --> 00:16:13.980]   in very emotional terms and it is,
[00:16:13.980 --> 00:16:17.000]   it is an emotional reaction first and foremost,
[00:16:17.000 --> 00:16:19.540]   but researchers more recently have started to think about,
[00:16:19.540 --> 00:16:22.780]   well, what are the cognitive schemas?
[00:16:22.780 --> 00:16:26.380]   What are the underpinnings, the ways that children think
[00:16:26.380 --> 00:16:28.860]   when they are securely or insecurely attached?
[00:16:28.860 --> 00:16:31.880]   And one brilliant study used looking time.
[00:16:31.880 --> 00:16:36.820]   Looking time in kids is a metric of what surprises them.
[00:16:36.820 --> 00:16:38.320]   If something really surprising happens,
[00:16:38.320 --> 00:16:40.540]   they look for a very long time.
[00:16:40.540 --> 00:16:43.420]   And researchers found that insecurely attached kids,
[00:16:43.420 --> 00:16:47.700]   when they saw a video of a reunion,
[00:16:47.700 --> 00:16:52.100]   of a caregiver and infant acting in a way
[00:16:52.100 --> 00:16:55.100]   that felt loving and stable,
[00:16:55.100 --> 00:16:58.300]   they looked longer as though that was surprising.
[00:16:58.300 --> 00:17:01.140]   Kids who were securely attached didn't look very long
[00:17:01.140 --> 00:17:03.820]   at those stable interactions,
[00:17:03.820 --> 00:17:06.940]   but looked longer at interactions that were unstable.
[00:17:06.940 --> 00:17:07.780]   - Interesting.
[00:17:07.780 --> 00:17:09.960]   - It's almost as though there is a setup
[00:17:09.960 --> 00:17:11.780]   that kids develop very early.
[00:17:11.780 --> 00:17:13.520]   Can I count on people?
[00:17:13.520 --> 00:17:16.580]   Am I safe with people?
[00:17:16.580 --> 00:17:19.780]   And insecure attachment is a signal coming early in life,
[00:17:19.780 --> 00:17:21.380]   no, you're not safe with people,
[00:17:21.380 --> 00:17:23.980]   that I think, well, and the data show,
[00:17:23.980 --> 00:17:26.980]   elaborates later in life into mistrust
[00:17:26.980 --> 00:17:28.640]   in other relationships.
[00:17:28.640 --> 00:17:33.480]   - How different is cynicism from skepticism?
[00:17:33.480 --> 00:17:38.060]   I can think of some places where they might overlap,
[00:17:38.060 --> 00:17:41.780]   but cynicism seems to carry something
[00:17:41.780 --> 00:17:44.340]   of a lack of anticipation
[00:17:44.340 --> 00:17:47.380]   about any possibility of a positive future.
[00:17:47.380 --> 00:17:49.020]   Is that one way to think about it?
[00:17:49.020 --> 00:17:51.700]   - That's a very sharp way of thinking about it, actually.
[00:17:51.700 --> 00:17:54.500]   And I wish that people knew more
[00:17:54.500 --> 00:17:58.500]   about the discrepancy between these two ways
[00:17:58.500 --> 00:17:59.500]   of viewing the world.
[00:17:59.500 --> 00:18:01.380]   Cynicism and skepticism,
[00:18:01.380 --> 00:18:03.900]   people often use them interchangeably.
[00:18:03.900 --> 00:18:05.500]   In fact, they're quite different.
[00:18:05.500 --> 00:18:08.480]   And I would argue that one is much more useful
[00:18:08.480 --> 00:18:10.020]   for learning about the world
[00:18:10.020 --> 00:18:12.500]   and building relationships than the other.
[00:18:12.500 --> 00:18:16.840]   Again, cynicism is a theory that's kind of locked in,
[00:18:16.840 --> 00:18:19.120]   that no matter what people show you,
[00:18:19.120 --> 00:18:23.780]   their true colors are, again, untrustworthy
[00:18:23.780 --> 00:18:25.420]   and self-oriented.
[00:18:25.420 --> 00:18:27.420]   It's a hyper-Darwinian view, right,
[00:18:27.420 --> 00:18:30.380]   that ultimately people are red in tooth and claw.
[00:18:30.380 --> 00:18:36.380]   Skepticism is instead the, I guess, restlessness
[00:18:36.380 --> 00:18:42.040]   with our assumptions, a desire for new information.
[00:18:42.040 --> 00:18:44.980]   One way I often think about it is that cynics
[00:18:44.980 --> 00:18:46.960]   think a little bit like lawyers, right?
[00:18:46.960 --> 00:18:49.580]   They have a decision that they've already made about you
[00:18:49.580 --> 00:18:51.060]   and about everybody.
[00:18:51.060 --> 00:18:52.900]   And they're just waiting for evidence
[00:18:52.900 --> 00:18:54.620]   that supports their point.
[00:18:54.620 --> 00:18:57.180]   And when evidence comes in that doesn't support their point,
[00:18:57.180 --> 00:18:59.020]   they explain it away, right?
[00:18:59.020 --> 00:19:00.140]   And you see this, actually,
[00:19:00.140 --> 00:19:03.920]   that cynical people will offer more ulterior motives
[00:19:03.920 --> 00:19:06.580]   when they see an act of kindness, for instance.
[00:19:06.580 --> 00:19:08.380]   They'll explain it away.
[00:19:08.380 --> 00:19:11.940]   In that way, I think cynics actually are quite similar
[00:19:11.940 --> 00:19:14.800]   to the naive, trusting, gullible folks
[00:19:14.800 --> 00:19:17.260]   that they love to make fun of, right?
[00:19:17.260 --> 00:19:20.620]   Naivete, gullibility, is trusting people
[00:19:20.620 --> 00:19:23.420]   in a credulous, unthinking way.
[00:19:23.420 --> 00:19:26.300]   I would say cynicism is mistrusting people
[00:19:26.300 --> 00:19:29.340]   in a credulous and unthinking way.
[00:19:29.340 --> 00:19:32.020]   So if cynics then think like lawyers,
[00:19:32.020 --> 00:19:35.100]   sort of in the prosecution against humanity,
[00:19:35.100 --> 00:19:38.180]   skeptics think more like scientists.
[00:19:38.180 --> 00:19:40.980]   Skepticism, classically in philosophy,
[00:19:40.980 --> 00:19:44.760]   is the belief that you can never truly know anything.
[00:19:44.760 --> 00:19:46.520]   But as we think about it now,
[00:19:46.520 --> 00:19:51.300]   it's more the desire for evidence to underlie any claim
[00:19:51.300 --> 00:19:53.020]   that you believe.
[00:19:53.020 --> 00:19:55.020]   And the great thing about skepticism
[00:19:55.020 --> 00:19:58.300]   is it doesn't require an ounce of naivete.
[00:19:58.300 --> 00:20:00.980]   You can be absolutely sharp in deciding,
[00:20:00.980 --> 00:20:02.300]   I don't want to trust this person,
[00:20:02.300 --> 00:20:04.500]   or I do want to trust this person,
[00:20:04.500 --> 00:20:08.900]   but it allows you to update and learn from specific acts,
[00:20:08.900 --> 00:20:11.740]   specific instances, and specific people.
[00:20:11.740 --> 00:20:13.340]   - When I think about scientists,
[00:20:13.340 --> 00:20:14.820]   one of the first things I think about
[00:20:14.820 --> 00:20:17.000]   is not just their willingness,
[00:20:17.000 --> 00:20:19.520]   but their excitement to embrace complexity.
[00:20:19.520 --> 00:20:20.360]   - Yes.
[00:20:20.360 --> 00:20:22.400]   - Like, okay, these two groups disagree,
[00:20:22.400 --> 00:20:25.340]   or these two sets of data disagree,
[00:20:25.340 --> 00:20:28.480]   and it's the complexity of that interaction
[00:20:28.480 --> 00:20:29.960]   that excites them.
[00:20:29.960 --> 00:20:32.900]   Whereas when I think of cynics
[00:20:32.900 --> 00:20:34.780]   in the way that it's framed up in my mind,
[00:20:34.780 --> 00:20:36.780]   which I'm getting more educated now,
[00:20:36.780 --> 00:20:39.980]   but admittedly my understanding of cynicism
[00:20:39.980 --> 00:20:42.020]   is still rather superficial.
[00:20:43.340 --> 00:20:46.200]   You'll change that in the course of our discussion.
[00:20:46.200 --> 00:20:50.980]   But that cynics are not embracing
[00:20:50.980 --> 00:20:54.060]   the complexity of disagreement.
[00:20:54.060 --> 00:20:56.480]   They are moving away from the,
[00:20:56.480 --> 00:20:59.860]   certainly any notion of excitement by complexity.
[00:20:59.860 --> 00:21:01.640]   It seems like it's a heuristic,
[00:21:01.640 --> 00:21:03.960]   it's a way to simplify the world around you.
[00:21:03.960 --> 00:21:05.700]   - That's exactly right.
[00:21:05.700 --> 00:21:08.740]   Phil Tetlock has a great term for this
[00:21:08.740 --> 00:21:11.020]   called integrative complexity.
[00:21:11.020 --> 00:21:14.580]   To what extent can you hold different versions of the world,
[00:21:14.580 --> 00:21:16.340]   different arguments in mind?
[00:21:16.340 --> 00:21:19.400]   To what extent can you pick from each one
[00:21:19.400 --> 00:21:22.620]   what you believe based on the best evidence available?
[00:21:22.620 --> 00:21:25.140]   And integrative complexity is a great way
[00:21:25.140 --> 00:21:27.900]   to learn about the world and about the social world,
[00:21:27.900 --> 00:21:30.020]   whereas cynicism, as you rightly point out,
[00:21:30.020 --> 00:21:32.220]   is much more of a heuristic.
[00:21:32.220 --> 00:21:35.020]   It's a black and white form of thinking.
[00:21:35.020 --> 00:21:39.140]   And the really sad thing is that cynicism
[00:21:39.140 --> 00:21:43.840]   then puts us in a position where we can't learn very much.
[00:21:43.840 --> 00:21:45.220]   This is what, in learning theory,
[00:21:45.220 --> 00:21:47.940]   is called a wicked learning environment,
[00:21:47.940 --> 00:21:49.940]   where, and I don't wanna get too nerdy.
[00:21:49.940 --> 00:21:51.180]   Well, I guess I can get nerdy here.
[00:21:51.180 --> 00:21:52.780]   - You can get as nerdy as you want.
[00:21:52.780 --> 00:21:54.500]   This audience likes nerdy.
[00:21:54.500 --> 00:21:57.140]   - So let's think in Bayesian terms, right?
[00:21:57.140 --> 00:22:00.060]   So Bayesian statistics is where you have
[00:22:00.060 --> 00:22:01.760]   a set of beliefs about the world,
[00:22:01.760 --> 00:22:03.860]   you take new information in,
[00:22:03.860 --> 00:22:07.300]   and that new information allows you to update your priors
[00:22:07.300 --> 00:22:11.460]   into a posterior distribution, into a new set of beliefs.
[00:22:11.460 --> 00:22:12.300]   And that's great.
[00:22:12.300 --> 00:22:14.780]   That's a great way to learn about the world,
[00:22:14.780 --> 00:22:18.440]   to adapt to new information and new circumstances.
[00:22:18.440 --> 00:22:21.620]   A wicked learning environment is where your priors
[00:22:21.620 --> 00:22:24.020]   prevent you from gathering the information
[00:22:24.020 --> 00:22:26.920]   that you would need to confirm or disconfirm them.
[00:22:26.920 --> 00:22:30.720]   So think about mistrust, for instance, right?
[00:22:30.720 --> 00:22:33.540]   It's easy to understand why people mistrust.
[00:22:33.540 --> 00:22:35.380]   Some of us are insecurely attached,
[00:22:35.380 --> 00:22:37.240]   and we've been hurt in the past.
[00:22:37.240 --> 00:22:38.980]   We're trying to stay safe.
[00:22:38.980 --> 00:22:40.560]   We don't want to be betrayed.
[00:22:40.560 --> 00:22:43.100]   This is a completely natural response.
[00:22:43.100 --> 00:22:45.860]   It's a totally understandable response.
[00:22:45.860 --> 00:22:49.460]   But when we decide to mistrust,
[00:22:49.460 --> 00:22:52.100]   we never are able to learn whether the people
[00:22:52.100 --> 00:22:56.380]   who we are mistrusting would have been trustworthy or not.
[00:22:56.380 --> 00:22:58.460]   When we trust, we can learn
[00:22:58.460 --> 00:22:59.940]   whether we've been right or not, right?
[00:22:59.940 --> 00:23:01.540]   Somebody can betray us, and that hurts,
[00:23:01.540 --> 00:23:03.180]   and we remember it for years.
[00:23:03.180 --> 00:23:07.220]   Or more often than not, the data turn out to show us
[00:23:07.220 --> 00:23:08.580]   they can honor that trust.
[00:23:08.580 --> 00:23:10.340]   We can build a relationship.
[00:23:10.340 --> 00:23:13.580]   We can start a collaboration.
[00:23:13.580 --> 00:23:16.140]   We can live a full social life.
[00:23:16.140 --> 00:23:18.700]   And it turns out that the problem is
[00:23:18.700 --> 00:23:22.460]   that trusting people incorrectly, you do learn from,
[00:23:22.460 --> 00:23:26.140]   but mistrusting people incorrectly, you don't learn from,
[00:23:26.140 --> 00:23:30.740]   because the missed opportunities are invisible to us.
[00:23:30.740 --> 00:23:32.060]   - Wow, there's certainly a lot there
[00:23:32.060 --> 00:23:34.740]   that maps to many people's experience.
[00:23:34.740 --> 00:23:37.920]   So you pointed out that some degree of cynicism
[00:23:37.920 --> 00:23:40.940]   likely has roots in insecure attachment.
[00:23:40.940 --> 00:23:45.620]   That said, if one looks internationally,
[00:23:45.620 --> 00:23:50.500]   do we find cultures where it's very hard to find cynics,
[00:23:50.500 --> 00:23:53.740]   and there could be any number of reasons for this,
[00:23:53.740 --> 00:23:56.420]   or perhaps even more interestingly,
[00:23:56.420 --> 00:23:59.660]   do we find cultures where there really isn't even a word
[00:23:59.660 --> 00:24:01.100]   for cynicism?
[00:24:01.100 --> 00:24:03.580]   - Wow, I love that question.
[00:24:03.580 --> 00:24:06.060]   There is a lot of variance in,
[00:24:06.060 --> 00:24:09.340]   and the data on cynicism are much more local
[00:24:09.340 --> 00:24:10.540]   to the US typically.
[00:24:10.540 --> 00:24:14.180]   I mean, for better and for worse,
[00:24:14.180 --> 00:24:17.280]   a lot of research on this is done in an American context.
[00:24:17.280 --> 00:24:21.920]   But that said, there's a lot of data on generalized trust,
[00:24:21.920 --> 00:24:24.280]   which you could say is an inverse of cynicism, right?
[00:24:24.280 --> 00:24:27.020]   So for instance, there are national and international
[00:24:27.020 --> 00:24:31.740]   samples of major surveys which ask people
[00:24:31.740 --> 00:24:33.420]   whether they agree or disagree
[00:24:33.420 --> 00:24:35.220]   that most people can be trusted.
[00:24:35.220 --> 00:24:38.260]   And there's a lot of variance around the world.
[00:24:38.260 --> 00:24:41.060]   In general, the cultures that are most trusting
[00:24:41.060 --> 00:24:43.020]   have a couple of things in common.
[00:24:43.020 --> 00:24:46.660]   One, they are more economically equal
[00:24:46.660 --> 00:24:48.600]   than untrusting cultures.
[00:24:48.600 --> 00:24:52.660]   So there's a lot of great work from Kate Willett
[00:24:52.660 --> 00:24:56.260]   and Richard Wilkinson that,
[00:24:56.260 --> 00:24:57.740]   they have a book called "The Spirit Level"
[00:24:57.740 --> 00:25:00.180]   where they look at inequality across the world
[00:25:00.180 --> 00:25:01.820]   and relate it to public health outcomes,
[00:25:01.820 --> 00:25:03.540]   and one of them is trust.
[00:25:03.540 --> 00:25:07.780]   There's also variance in trust over time.
[00:25:07.780 --> 00:25:10.900]   So you can look at not just are there places or cultures
[00:25:10.900 --> 00:25:12.100]   that trust more than others,
[00:25:12.100 --> 00:25:16.060]   but when does a culture trust more or less?
[00:25:16.060 --> 00:25:19.740]   And in the US, that's sadly a story of decline.
[00:25:19.740 --> 00:25:23.360]   In 1972, about half of Americans believed
[00:25:23.360 --> 00:25:25.340]   that most people can be trusted.
[00:25:25.340 --> 00:25:29.460]   And by 2018, that had fallen to about a third of Americans.
[00:25:29.460 --> 00:25:33.200]   And that's a drop as big, just to put it in perspective,
[00:25:33.200 --> 00:25:36.780]   as the stock market took in the financial collapse of 2008.
[00:25:36.780 --> 00:25:39.680]   So there's a lot of variance here,
[00:25:39.680 --> 00:25:42.340]   both across space and time.
[00:25:42.340 --> 00:25:44.900]   And one of the, not the only,
[00:25:44.900 --> 00:25:48.860]   but one of the seeming characteristics of cultures
[00:25:48.860 --> 00:25:52.180]   that tracks that is how unequal they are.
[00:25:52.180 --> 00:25:54.760]   In part, because research suggests
[00:25:54.760 --> 00:25:58.460]   that when you are in a highly unequal society,
[00:25:58.460 --> 00:26:01.580]   economically, there's a sense of zero-sum competition
[00:26:01.580 --> 00:26:02.420]   that develops.
[00:26:02.420 --> 00:26:03.760]   There's a sense that, wait a minute,
[00:26:03.760 --> 00:26:07.100]   anything that another person gets, I lose.
[00:26:07.100 --> 00:26:12.100]   And if you have that inherent sense of zero-sum competition,
[00:26:12.100 --> 00:26:15.300]   then it's very difficult to form bonds.
[00:26:15.300 --> 00:26:17.520]   It's very difficult to trust other people
[00:26:17.520 --> 00:26:20.220]   because you might think, well, in order to survive,
[00:26:20.220 --> 00:26:23.220]   this person has to try to outrun me.
[00:26:23.220 --> 00:26:24.260]   They have to try to trip me.
[00:26:24.260 --> 00:26:28.220]   They have to try to make me fail for themselves to succeed.
[00:26:28.220 --> 00:26:29.460]   I'd like to take a quick break
[00:26:29.460 --> 00:26:32.100]   and acknowledge our sponsor, AG1.
[00:26:32.100 --> 00:26:33.700]   By now, many of you have heard me say
[00:26:33.700 --> 00:26:35.460]   that if I could take just one supplement,
[00:26:35.460 --> 00:26:37.540]   that supplement would be AG1.
[00:26:37.540 --> 00:26:40.080]   The reason for that is AG1 is the highest quality
[00:26:40.080 --> 00:26:40.980]   and most complete
[00:26:40.980 --> 00:26:43.740]   of the foundational nutritional supplements available.
[00:26:43.740 --> 00:26:45.180]   What that means is that it contains
[00:26:45.180 --> 00:26:46.540]   not just vitamins and minerals,
[00:26:46.540 --> 00:26:49.940]   but also probiotics, prebiotics, and adaptogens
[00:26:49.940 --> 00:26:52.340]   to cover any gaps you may have in your diet
[00:26:52.340 --> 00:26:54.620]   and provide support for a demanding life.
[00:26:54.620 --> 00:26:56.460]   For me, even if I eat mostly whole foods
[00:26:56.460 --> 00:26:57.700]   and minimally processed foods,
[00:26:57.700 --> 00:26:59.620]   which I do for most of my food intake,
[00:26:59.620 --> 00:27:00.780]   it's very difficult for me
[00:27:00.780 --> 00:27:02.420]   to get enough fruits and vegetables,
[00:27:02.420 --> 00:27:04.300]   vitamins and minerals, micronutrients,
[00:27:04.300 --> 00:27:06.500]   and adaptogens from food alone.
[00:27:06.500 --> 00:27:09.940]   For that reason, I've been taking AG1 daily since 2012
[00:27:09.940 --> 00:27:12.620]   and often twice a day, once in the morning or mid-morning,
[00:27:12.620 --> 00:27:14.620]   and again in the afternoon or evening.
[00:27:14.620 --> 00:27:17.040]   When I do that, it clearly bolsters my energy,
[00:27:17.040 --> 00:27:19.620]   my immune system, and my gut microbiome.
[00:27:19.620 --> 00:27:21.460]   These are all critical to brain function,
[00:27:21.460 --> 00:27:24.020]   mood, physical performance, and much more.
[00:27:24.020 --> 00:27:25.500]   If you'd like to try AG1,
[00:27:25.500 --> 00:27:28.620]   you can go to drinkag1.com/huberman
[00:27:28.620 --> 00:27:30.200]   to claim their special offer.
[00:27:30.200 --> 00:27:32.340]   Right now, they're giving away five free travel packs
[00:27:32.340 --> 00:27:35.180]   plus a year supply of vitamin D3K2.
[00:27:35.180 --> 00:27:38.820]   Again, that's drinkag1.com/huberman
[00:27:38.820 --> 00:27:40.540]   to claim that special offer.
[00:27:40.540 --> 00:27:42.560]   - What is the relationship, if any,
[00:27:42.560 --> 00:27:46.760]   between cynicism and happiness or lack of happiness?
[00:27:46.760 --> 00:27:48.540]   When I think of somebody who's really cynical,
[00:27:48.540 --> 00:27:50.300]   I think of an Oscar the Grouch
[00:27:50.300 --> 00:27:53.160]   or a curmudgeon-like character.
[00:27:53.160 --> 00:27:54.380]   And as I ask this question,
[00:27:54.380 --> 00:27:56.620]   I'm thinking specifically about what you said earlier
[00:27:56.620 --> 00:27:58.960]   about how cynicism prevents us
[00:27:58.960 --> 00:28:02.300]   from certain forms of learning that are important
[00:28:02.300 --> 00:28:03.260]   and very valuable to us.
[00:28:03.260 --> 00:28:04.100]   Here's the reason why.
[00:28:04.100 --> 00:28:05.260]   I'll give just a little bit of context.
[00:28:05.260 --> 00:28:06.180]   I remember when I was a kid,
[00:28:06.180 --> 00:28:10.260]   my dad, who went to classic boarding schools,
[00:28:10.260 --> 00:28:11.500]   he grew up in South America,
[00:28:11.500 --> 00:28:15.900]   but he went to these boarding schools that were very strict.
[00:28:15.900 --> 00:28:18.140]   And he was taught, he told me,
[00:28:18.140 --> 00:28:22.220]   that to be cheerful and happy,
[00:28:22.220 --> 00:28:26.280]   people would accuse you of being kind of dumb.
[00:28:26.280 --> 00:28:27.860]   Whereas if you were cynical
[00:28:27.860 --> 00:28:30.180]   and you acted a little bored with everything,
[00:28:30.180 --> 00:28:32.520]   people thought that you were more discerning,
[00:28:32.520 --> 00:28:34.720]   but that he felt it was a terrible model
[00:28:34.720 --> 00:28:39.300]   for going through life because it veered into cynicism.
[00:28:39.300 --> 00:28:41.300]   My dad happens to be a scientist.
[00:28:41.300 --> 00:28:44.800]   He's, I think, a relatively happy person.
[00:28:44.800 --> 00:28:45.640]   Sorry, dad.
[00:28:45.640 --> 00:28:46.940]   A happy person, seems happy,
[00:28:46.940 --> 00:28:49.940]   but meaning he's a person who has happiness
[00:28:49.940 --> 00:28:51.140]   and he has other emotions too.
[00:28:51.140 --> 00:28:52.540]   I wouldn't say he's happy all the time,
[00:28:52.540 --> 00:28:55.680]   but he experiences joy and pleasure in daily,
[00:28:55.680 --> 00:28:57.380]   small things and big things in life.
[00:28:57.380 --> 00:29:00.620]   So clearly he rescued himself from the forces
[00:29:00.620 --> 00:29:02.020]   that were kind of pushing him down that path.
[00:29:02.020 --> 00:29:03.660]   But that's the anecdote.
[00:29:03.660 --> 00:29:07.300]   But I use that question more as a way to frame up
[00:29:07.300 --> 00:29:10.220]   the possible collaboration between cynicism
[00:29:10.220 --> 00:29:15.220]   and exuding boredom or a challenge
[00:29:15.220 --> 00:29:18.620]   in shifting somebody towards a happier affect.
[00:29:20.060 --> 00:29:21.340]   Because when I think about cynics,
[00:29:21.340 --> 00:29:23.820]   I think that they're kind of unhappy people.
[00:29:23.820 --> 00:29:25.460]   And when I think about people who are not very cynical,
[00:29:25.460 --> 00:29:27.980]   I think of them as kind of cheerful and curious.
[00:29:27.980 --> 00:29:29.920]   And there's some ebullience there.
[00:29:29.920 --> 00:29:33.200]   They might not be Tigger-like in their affect,
[00:29:33.200 --> 00:29:36.020]   but they kind of veer that direction.
[00:29:36.020 --> 00:29:37.840]   - Andrew, I love this trip down memory lane.
[00:29:37.840 --> 00:29:39.760]   I'm having all these childhood memories
[00:29:39.760 --> 00:29:43.840]   of Tigger and Sesame Street.
[00:29:43.840 --> 00:29:45.340]   There's so much in what you're saying.
[00:29:45.340 --> 00:29:47.460]   I want to try to pull on a couple of threads here,
[00:29:47.460 --> 00:29:48.600]   if that's okay.
[00:29:48.600 --> 00:29:51.560]   First, and this one is pretty straightforward,
[00:29:51.560 --> 00:29:53.860]   the effect of cynicism on well-being
[00:29:53.860 --> 00:29:57.460]   is just really documented and quite negative.
[00:29:57.460 --> 00:30:00.860]   So there are large prospective studies
[00:30:00.860 --> 00:30:04.100]   with tens of thousands of people,
[00:30:04.100 --> 00:30:05.460]   several of these studies,
[00:30:05.460 --> 00:30:08.740]   that measure cynicism and then measure life outcomes
[00:30:08.740 --> 00:30:11.140]   in the years and decades afterwards.
[00:30:11.140 --> 00:30:14.100]   And the news is pretty bleak for cynics, right?
[00:30:14.100 --> 00:30:18.020]   So absolutely, lower levels of happiness,
[00:30:18.020 --> 00:30:20.580]   flourishing satisfaction with life,
[00:30:20.580 --> 00:30:24.500]   greater incidence of depression, greater loneliness.
[00:30:24.500 --> 00:30:29.140]   But it's not just the neck up that cynicism affects.
[00:30:29.140 --> 00:30:31.300]   Cynics over the course of their lives
[00:30:31.300 --> 00:30:35.420]   also tend to have greater degrees of cellular inflammation,
[00:30:35.420 --> 00:30:37.720]   more incidence of heart disease,
[00:30:37.720 --> 00:30:41.340]   and they even have higher rates of all-cause mortality,
[00:30:41.340 --> 00:30:43.660]   so shorter lives than non-cynics.
[00:30:43.660 --> 00:30:45.540]   And again, this might sound like, wait a minute,
[00:30:45.540 --> 00:30:47.300]   you go from a philosophical theory
[00:30:47.300 --> 00:30:49.500]   to a shorter life.
[00:30:49.500 --> 00:30:51.940]   The answer is, yeah, you do, because,
[00:30:51.940 --> 00:30:53.940]   and again, these are correlational studies,
[00:30:53.940 --> 00:30:56.180]   so I don't wanna draw too many causal claims,
[00:30:56.180 --> 00:30:58.100]   but they're quite rigorous and control
[00:30:58.100 --> 00:31:00.300]   for a lot of other factors.
[00:31:00.300 --> 00:31:02.420]   But I would say that this is consistent
[00:31:02.420 --> 00:31:04.100]   with the idea that, really,
[00:31:04.100 --> 00:31:07.540]   one of the great protectors of our health
[00:31:07.540 --> 00:31:09.940]   is our sense of connection to other people.
[00:31:09.940 --> 00:31:13.120]   And if you are unable or unwilling
[00:31:13.120 --> 00:31:15.020]   to be vulnerable around others,
[00:31:15.020 --> 00:31:18.580]   to really touch in to that type of connection,
[00:31:18.580 --> 00:31:21.700]   it stands to reason that things like chronic stress
[00:31:21.700 --> 00:31:25.280]   and isolation would impact not just your mind,
[00:31:25.280 --> 00:31:28.760]   but all through your body and your organ systems.
[00:31:28.760 --> 00:31:31.420]   So again, the news here is not great,
[00:31:31.420 --> 00:31:34.820]   and I often think about one of the best encapsulations
[00:31:34.820 --> 00:31:39.060]   of a cynical view of life comes from Thomas Hobbes,
[00:31:39.060 --> 00:31:42.020]   the philosopher, who, in his book "Leviathan,"
[00:31:42.020 --> 00:31:44.980]   said we need a restrictive government.
[00:31:44.980 --> 00:31:47.620]   Because left to our own devices,
[00:31:47.620 --> 00:31:50.500]   human life is nasty, brutish, and short.
[00:31:50.500 --> 00:31:52.700]   And ironically, I think that might describe
[00:31:52.700 --> 00:31:57.700]   the lives of cynics themselves more than most people.
[00:31:57.700 --> 00:31:59.780]   So that's point one, right,
[00:31:59.780 --> 00:32:03.740]   is that there is this pretty stark negative correlation
[00:32:03.740 --> 00:32:06.680]   between cynicism and a lot of life outcomes
[00:32:06.680 --> 00:32:08.860]   that we might want for ourselves.
[00:32:08.860 --> 00:32:10.540]   But point two, I think,
[00:32:10.540 --> 00:32:13.460]   is related to what your dad also noticed,
[00:32:13.460 --> 00:32:17.380]   which is that if cynicism hurts us so much,
[00:32:17.380 --> 00:32:19.540]   why would we adopt it?
[00:32:19.540 --> 00:32:21.460]   If it was a pill, if there was a pill
[00:32:21.460 --> 00:32:24.180]   that as its side effects listed depression,
[00:32:24.180 --> 00:32:26.460]   loneliness, heart disease, and early death,
[00:32:26.460 --> 00:32:27.820]   it would be a poison, right?
[00:32:27.820 --> 00:32:30.580]   It would have a skull and crossbones on the bottle.
[00:32:30.580 --> 00:32:32.080]   But yet we're swallowing it.
[00:32:32.080 --> 00:32:33.540]   More of us are swallowing it
[00:32:33.540 --> 00:32:35.580]   than we did in years and decades past.
[00:32:35.580 --> 00:32:36.880]   Why?
[00:32:36.880 --> 00:32:39.460]   Well, one of the answers, I think,
[00:32:39.460 --> 00:32:42.220]   is because our culture glamorizes cynicism.
[00:32:42.220 --> 00:32:44.140]   It's because of the very stereotype
[00:32:44.140 --> 00:32:45.340]   that your father pointed out,
[00:32:45.340 --> 00:32:47.340]   which is that if you're happy-go-lucky,
[00:32:47.340 --> 00:32:50.460]   if you trust people, that kind of seems dull.
[00:32:50.460 --> 00:32:52.160]   It seems like maybe you're not that sharp.
[00:32:52.160 --> 00:32:54.580]   Maybe you don't understand the world.
[00:32:54.580 --> 00:32:58.980]   And there is that strong relationship in our stereotypes,
[00:32:58.980 --> 00:33:01.020]   in our models of the world.
[00:33:01.020 --> 00:33:03.080]   Susan Fisk and many other psychologists
[00:33:03.080 --> 00:33:06.260]   have studied warmth and competence, right?
[00:33:06.260 --> 00:33:10.260]   How friendly and caring does somebody seem?
[00:33:10.260 --> 00:33:13.900]   And how able do they seem to accomplish hard things?
[00:33:13.900 --> 00:33:16.260]   And it turns out that in many studies,
[00:33:16.260 --> 00:33:19.160]   people's perception is that these are inversely correlated.
[00:33:19.160 --> 00:33:22.120]   That if you're warm, maybe you're not that competent.
[00:33:22.120 --> 00:33:24.380]   And if you're competent, maybe you shouldn't be that warm.
[00:33:24.380 --> 00:33:26.520]   And in fact, if you tell people
[00:33:26.520 --> 00:33:29.680]   to act as competently as they can,
[00:33:29.680 --> 00:33:32.980]   they'll often respond by being a little bit less nice,
[00:33:32.980 --> 00:33:36.300]   a little bit less warm than they would be otherwise.
[00:33:36.300 --> 00:33:38.820]   There's also data that find that,
[00:33:38.820 --> 00:33:41.100]   you know, where people are presented in surveys
[00:33:41.100 --> 00:33:43.420]   with a cynic and a non-cynic.
[00:33:43.420 --> 00:33:45.900]   They're told about, here's one person,
[00:33:45.900 --> 00:33:48.940]   they really think that people are great overall,
[00:33:48.940 --> 00:33:50.420]   and they tend to be trusting.
[00:33:50.420 --> 00:33:52.660]   Here's another person who thinks that people
[00:33:52.660 --> 00:33:53.900]   are kind of out for themselves
[00:33:53.900 --> 00:33:56.420]   and really doesn't trust most folks.
[00:33:56.420 --> 00:33:58.060]   And then they'll ask those people,
[00:33:58.060 --> 00:34:02.180]   who should we pick for this difficult intellectual task?
[00:34:02.180 --> 00:34:06.580]   And 70% of respondents pick a cynical person
[00:34:06.580 --> 00:34:09.780]   over a non-cynic for difficult intellectual tasks.
[00:34:09.780 --> 00:34:14.280]   85% of people think that cynics are socially wiser,
[00:34:14.280 --> 00:34:17.160]   that they'd be able, for instance, to detect who's lying
[00:34:17.160 --> 00:34:18.900]   and who's telling the truth.
[00:34:18.900 --> 00:34:22.280]   So most of us put a lot of faith in people
[00:34:22.280 --> 00:34:25.580]   who don't have a lot of faith in people, ironically.
[00:34:25.580 --> 00:34:28.860]   And even more ironically, we're wrong to do so.
[00:34:28.860 --> 00:34:32.340]   Olga Stavrova, this great psychologist who studies cynicism,
[00:34:32.340 --> 00:34:35.780]   has this paper called "The Cynical Genius Illusion,"
[00:34:35.780 --> 00:34:38.760]   where she documents all these biases,
[00:34:38.760 --> 00:34:42.220]   the way that we think cynics are bright and wise,
[00:34:42.220 --> 00:34:46.460]   and then uses national data, tens of thousands of people,
[00:34:46.460 --> 00:34:49.620]   to show that actually, cynics do less well
[00:34:49.620 --> 00:34:52.460]   on cognitive tests, on mathematical tests,
[00:34:52.460 --> 00:34:54.740]   that trust is related with things
[00:34:54.740 --> 00:34:57.100]   like intelligence and education,
[00:34:57.100 --> 00:35:00.580]   and that, in other work,
[00:35:00.580 --> 00:35:03.380]   this is not from Olga Stavrova, but from others,
[00:35:03.380 --> 00:35:06.580]   that actually, cynics do less well than non-cynics
[00:35:06.580 --> 00:35:08.000]   in detecting liars.
[00:35:08.000 --> 00:35:11.840]   Because if you have a blanket assumption about people,
[00:35:11.840 --> 00:35:15.380]   you're not actually attending to evidence in a sharp way.
[00:35:15.380 --> 00:35:17.660]   You're not actually taking in new information
[00:35:17.660 --> 00:35:20.060]   and making wise decisions.
[00:35:20.060 --> 00:35:24.500]   - In other words, cynics are not being scientific.
[00:35:24.500 --> 00:35:27.300]   Their hypothesis is cast,
[00:35:27.300 --> 00:35:30.120]   but they're not looking at the data equally, right?
[00:35:30.120 --> 00:35:31.300]   And we should remind people
[00:35:31.300 --> 00:35:33.700]   that a hypothesis is not a question.
[00:35:33.700 --> 00:35:35.900]   Every great experiment starts with a question,
[00:35:35.900 --> 00:35:37.500]   and then you generate a hypothesis,
[00:35:37.500 --> 00:35:41.500]   which is a theory or conclusion,
[00:35:41.500 --> 00:35:43.580]   essentially made up front,
[00:35:43.580 --> 00:35:45.660]   and then you go collect data,
[00:35:45.660 --> 00:35:49.340]   and you see if you prove or disprove the hypothesis.
[00:35:49.340 --> 00:35:51.020]   And you can never really prove a hypothesis.
[00:35:51.020 --> 00:35:54.220]   You can only support it or not support it
[00:35:54.220 --> 00:35:55.420]   with the data that you collect,
[00:35:55.420 --> 00:35:56.980]   depending on the precision of your tools.
[00:35:56.980 --> 00:36:00.200]   But that's very interesting,
[00:36:00.200 --> 00:36:05.200]   because I would think that if we view cynics as smarter,
[00:36:05.200 --> 00:36:08.380]   which clearly they're not as a group, right?
[00:36:08.380 --> 00:36:11.020]   You're saying cynics are not more intelligent, right?
[00:36:11.020 --> 00:36:13.060]   I believe that's covered in your book.
[00:36:13.060 --> 00:36:16.560]   And if one knows that,
[00:36:16.560 --> 00:36:20.740]   then why do we send cynics in kind of like razors
[00:36:20.740 --> 00:36:25.400]   to assess what the environment is like?
[00:36:25.400 --> 00:36:29.940]   Is that because we'd rather have others deployed
[00:36:29.940 --> 00:36:32.520]   for us to kind of like weed people out?
[00:36:32.520 --> 00:36:35.660]   Is it that we're willing to accept some false negatives?
[00:36:35.660 --> 00:36:38.220]   Meaning for those,
[00:36:38.220 --> 00:36:39.120]   I guess we're using a little bit
[00:36:39.120 --> 00:36:40.600]   of a semi-technical language here,
[00:36:40.600 --> 00:36:42.140]   false negatives would be,
[00:36:42.140 --> 00:36:43.580]   you're trying to assess a group of people
[00:36:43.580 --> 00:36:45.880]   that would be terrific employees.
[00:36:45.880 --> 00:36:48.360]   And you send in somebody, interview them,
[00:36:48.360 --> 00:36:50.480]   that's very cynical.
[00:36:50.480 --> 00:36:52.680]   So presumably in one's mind,
[00:36:52.680 --> 00:36:55.520]   that filter of cynicism is only going to allow in people
[00:36:55.520 --> 00:36:58.040]   that are really, really right for the job.
[00:36:58.040 --> 00:36:59.640]   And we're willing to accept that,
[00:36:59.640 --> 00:37:02.060]   you know, there are probably two or three candidates
[00:37:02.060 --> 00:37:03.380]   that would also be right for the job,
[00:37:03.380 --> 00:37:04.580]   but we're willing to let them go.
[00:37:04.580 --> 00:37:06.560]   Some false negatives,
[00:37:06.560 --> 00:37:11.380]   as opposed to having someone get through the filter
[00:37:11.380 --> 00:37:12.780]   who really can't do the job.
[00:37:12.780 --> 00:37:15.100]   Like we're willing to let certain opportunities go
[00:37:15.100 --> 00:37:18.420]   by being cynical or by deploying a cynic as the,
[00:37:18.420 --> 00:37:20.120]   you know, I'm imagining the person with the clipboard,
[00:37:20.120 --> 00:37:21.900]   you know, very rigid.
[00:37:21.900 --> 00:37:22.740]   - Yeah. - Like cynicism
[00:37:22.740 --> 00:37:24.180]   and rigidity seem to go together.
[00:37:24.180 --> 00:37:25.340]   So that's why I'm lumping
[00:37:25.340 --> 00:37:27.500]   these kinds of psychological phenotypes.
[00:37:27.500 --> 00:37:29.180]   - No, I think that's absolutely right.
[00:37:29.180 --> 00:37:32.180]   And so a couple of things, one, you know,
[00:37:32.180 --> 00:37:34.980]   you said that if we know that cynics aren't smarter
[00:37:34.980 --> 00:37:36.900]   than non-cynics, why are we deploying them?
[00:37:36.900 --> 00:37:38.660]   Well, let's be clear, we know this,
[00:37:38.660 --> 00:37:41.660]   meaning you and I know this and scientists know this,
[00:37:41.660 --> 00:37:44.580]   but the data show that most people don't know this,
[00:37:44.580 --> 00:37:47.860]   that we maintain the stereotype in our culture
[00:37:47.860 --> 00:37:50.300]   that being negative about people
[00:37:50.300 --> 00:37:52.780]   means that you've been around the block enough times,
[00:37:52.780 --> 00:37:54.040]   that it is a form of wisdom.
[00:37:54.040 --> 00:37:56.220]   So that's a stereotype that I think we need
[00:37:56.220 --> 00:37:58.100]   to dispel first of all.
[00:37:58.100 --> 00:38:00.580]   But I do think that to your point,
[00:38:00.580 --> 00:38:03.380]   when we deploy cynics out in the field, you know,
[00:38:03.380 --> 00:38:05.020]   when we say, I'm gonna be nice,
[00:38:05.020 --> 00:38:07.820]   but I want somebody who's really pretty negative,
[00:38:07.820 --> 00:38:10.540]   who's really pretty suspicious to protect me
[00:38:10.540 --> 00:38:12.380]   or to protect my community,
[00:38:12.380 --> 00:38:15.780]   I think that's a really, again, understandable instinct,
[00:38:15.780 --> 00:38:18.460]   almost from an evolutionary perspective.
[00:38:18.460 --> 00:38:21.780]   You know, we are built to pay lots of attention
[00:38:21.780 --> 00:38:25.300]   to threats in our environment and threats to our community.
[00:38:25.300 --> 00:38:27.740]   And in the early social world, you know,
[00:38:27.740 --> 00:38:28.660]   if you wind, I mean,
[00:38:28.660 --> 00:38:32.100]   just to do some back of the envelope evolutionary psychology,
[00:38:32.100 --> 00:38:36.700]   if you wind the clock back 100, 150,000 years,
[00:38:36.700 --> 00:38:37.780]   what's, you know,
[00:38:37.780 --> 00:38:39.860]   what is the greatest threat to early communities?
[00:38:39.860 --> 00:38:41.660]   It's people, right?
[00:38:41.660 --> 00:38:44.060]   It's people who would take advantage
[00:38:44.060 --> 00:38:45.820]   of our communal nature, right?
[00:38:45.820 --> 00:38:48.620]   The thing that allows human beings to thrive
[00:38:48.620 --> 00:38:51.000]   is that we collaborate.
[00:38:51.000 --> 00:38:54.460]   But that collaboration means that a free rider,
[00:38:54.460 --> 00:38:56.960]   somebody who chooses to not pitch in,
[00:38:56.960 --> 00:38:59.540]   but still take out from the common pool,
[00:38:59.540 --> 00:39:02.660]   anything that they want, can do exceptionally well.
[00:39:02.660 --> 00:39:05.500]   They can live a life of leisure on the backs
[00:39:05.500 --> 00:39:07.100]   of a community that's working hard.
[00:39:07.100 --> 00:39:10.680]   And if you select then for that type of person,
[00:39:10.680 --> 00:39:12.260]   if that type of person proliferates,
[00:39:12.260 --> 00:39:14.420]   then the community collapses.
[00:39:14.420 --> 00:39:18.180]   So it makes sense that we depend on cynics
[00:39:18.180 --> 00:39:22.100]   from that perspective, from a threat mitigation perspective,
[00:39:22.100 --> 00:39:24.900]   from a risk aversion perspective.
[00:39:24.900 --> 00:39:26.680]   But it doesn't make sense
[00:39:26.680 --> 00:39:28.600]   from the perspective of trying to optimize
[00:39:28.600 --> 00:39:30.600]   our actual social lives, right?
[00:39:30.600 --> 00:39:33.120]   And I think that oftentimes, you know,
[00:39:33.120 --> 00:39:34.360]   we are risk averse in general,
[00:39:34.360 --> 00:39:36.820]   meaning that we're more scared of negative outcomes
[00:39:36.820 --> 00:39:40.320]   than we are enticed by positive outcomes.
[00:39:40.320 --> 00:39:41.520]   But in the social world,
[00:39:41.520 --> 00:39:44.520]   that risk aversion is, I think,
[00:39:44.520 --> 00:39:46.860]   quite harmful in a lot of demonstrable ways.
[00:39:46.860 --> 00:39:50.900]   - Is cynicism domain specific?
[00:39:50.900 --> 00:39:53.000]   And there again, I'm using jargon,
[00:39:53.000 --> 00:39:57.180]   meaning if somebody is cynical in one environment,
[00:39:57.180 --> 00:39:58.540]   like cynical about the markets,
[00:39:58.540 --> 00:39:59.900]   like, well, things are up now,
[00:39:59.900 --> 00:40:03.040]   but, you know, have an election come,
[00:40:03.040 --> 00:40:04.780]   so things can go this way or that way, depending on,
[00:40:04.780 --> 00:40:07.060]   you know, do they tend to be cynical
[00:40:07.060 --> 00:40:10.140]   about other aspects of life, other domains?
[00:40:10.140 --> 00:40:13.360]   - So there's a little bit of data on this,
[00:40:13.360 --> 00:40:15.380]   and it suggests a couple of things.
[00:40:15.380 --> 00:40:18.180]   One, left to our own devices,
[00:40:18.180 --> 00:40:21.860]   our levels of cynicism tend to be pretty stable over time.
[00:40:22.820 --> 00:40:25.640]   And also decline in older adulthood,
[00:40:25.640 --> 00:40:29.400]   contra the stereotype of the curmudgeonly older person.
[00:40:29.400 --> 00:40:31.320]   But another is that cynicism
[00:40:31.320 --> 00:40:33.760]   does tend to be pretty domain general.
[00:40:33.760 --> 00:40:36.840]   So for instance, cynics, you know,
[00:40:36.840 --> 00:40:39.160]   and this makes sense if you look at questionnaires
[00:40:39.160 --> 00:40:41.800]   that assess cynicism, which are things like,
[00:40:41.800 --> 00:40:45.380]   people are honest chiefly through fear of getting caught,
[00:40:45.380 --> 00:40:48.360]   or most people really don't like helping each other.
[00:40:48.360 --> 00:40:51.120]   I mean, if you're answering those questions positively,
[00:40:51.120 --> 00:40:52.260]   you're just not a fan of,
[00:40:52.260 --> 00:40:53.980]   you're probably not great at parties,
[00:40:53.980 --> 00:40:55.820]   you're not a fan of people.
[00:40:55.820 --> 00:40:57.940]   And it turns out that people who answer the,
[00:40:57.940 --> 00:41:01.340]   this is an old scale developed by a couple of psychologists
[00:41:01.340 --> 00:41:04.540]   named Walter Cook and Donald Medley in the 1950s.
[00:41:04.540 --> 00:41:07.940]   If you answer the Cook-Medley hostility scale,
[00:41:07.940 --> 00:41:09.900]   if you answer these questions positively,
[00:41:09.900 --> 00:41:12.580]   you tend to be less trusting of strangers,
[00:41:12.580 --> 00:41:14.560]   but you also tend to, for instance,
[00:41:14.560 --> 00:41:17.940]   have less trust in your romantic partnerships,
[00:41:17.940 --> 00:41:20.220]   you have less trust in your friends,
[00:41:20.220 --> 00:41:21.940]   and you have less trust in your colleagues.
[00:41:21.940 --> 00:41:25.380]   So this is sort of an all-purpose view of the world,
[00:41:25.380 --> 00:41:30.180]   at least as Cook and Medley first thought about it.
[00:41:30.180 --> 00:41:32.420]   But I do wanna build on a great intuition you have,
[00:41:32.420 --> 00:41:35.200]   which is that different environments
[00:41:35.200 --> 00:41:38.860]   might bring out cynicism or tamp it down.
[00:41:38.860 --> 00:41:40.880]   And it turns out that that's also very true.
[00:41:40.880 --> 00:41:43.480]   As trait-like as cynicism can be,
[00:41:43.480 --> 00:41:46.140]   there's lots of evidence that the type of
[00:41:46.140 --> 00:41:48.940]   social environment we're in matters a lot.
[00:41:48.940 --> 00:41:52.420]   One of my favorite studies in this domain
[00:41:52.420 --> 00:41:55.760]   came from Southeastern Brazil.
[00:41:55.760 --> 00:41:58.260]   There are two fishing villages in Southeastern Brazil.
[00:41:58.260 --> 00:42:00.900]   They're separated by about 30, 40 miles.
[00:42:00.900 --> 00:42:04.780]   They're similar in socioeconomic status, religion, culture,
[00:42:04.780 --> 00:42:07.520]   but there's one big difference between them.
[00:42:07.520 --> 00:42:09.980]   One of the villages sits on the ocean,
[00:42:09.980 --> 00:42:11.840]   and in order to fish on the ocean,
[00:42:11.840 --> 00:42:14.140]   you need big boats, heavy equipment.
[00:42:14.140 --> 00:42:15.420]   You can't do it alone.
[00:42:15.420 --> 00:42:17.660]   You must work together.
[00:42:17.660 --> 00:42:19.540]   The other village is on a lake,
[00:42:19.540 --> 00:42:23.060]   where fishermen strike out on small boats alone,
[00:42:23.060 --> 00:42:25.520]   and they compete with one another.
[00:42:25.520 --> 00:42:27.580]   About 10 years ago, economists,
[00:42:27.580 --> 00:42:29.460]   this was a study led by Andreas Liebrand,
[00:42:29.460 --> 00:42:31.000]   a really great economist.
[00:42:31.000 --> 00:42:32.780]   They went to these villages,
[00:42:32.780 --> 00:42:35.800]   and they gave the folks who worked there
[00:42:35.800 --> 00:42:38.460]   a bunch of social games to play.
[00:42:38.460 --> 00:42:41.500]   These were not with fellow fishermen, but with strangers.
[00:42:41.500 --> 00:42:44.420]   Games like, would you trust somebody with some money
[00:42:44.420 --> 00:42:47.380]   and see if they then want to share dividends with you?
[00:42:47.380 --> 00:42:49.420]   Or give in some money yourself,
[00:42:49.420 --> 00:42:52.860]   would you like to share some of it with another person?
[00:42:52.860 --> 00:42:56.260]   And they found that when they start in their careers,
[00:42:56.260 --> 00:43:00.300]   lake fishermen and ocean fishermen were equally trusting
[00:43:00.300 --> 00:43:03.380]   and equally trustworthy as well.
[00:43:03.380 --> 00:43:06.580]   But over the course of their careers, they diverged.
[00:43:06.580 --> 00:43:08.820]   Being in a collaborative environment
[00:43:08.820 --> 00:43:12.340]   where people must count on one another to survive
[00:43:12.340 --> 00:43:17.020]   made people over time more trusting and more trustworthy.
[00:43:17.020 --> 00:43:20.660]   Being in a competitive zero-sum environment over time
[00:43:20.660 --> 00:43:23.900]   made people less trusting and less trustworthy.
[00:43:23.900 --> 00:43:26.220]   Now, one thing that always amazes me about this work
[00:43:26.220 --> 00:43:30.620]   is that people in both of these environments are right.
[00:43:30.620 --> 00:43:32.700]   If you're in a competitive environment,
[00:43:32.700 --> 00:43:35.940]   you don't trust and you're right to not trust.
[00:43:35.940 --> 00:43:37.700]   If you're in a collaborative environment,
[00:43:37.700 --> 00:43:40.220]   you do trust and you're right to trust.
[00:43:40.220 --> 00:43:42.780]   And this is from the point of view of economic games,
[00:43:42.780 --> 00:43:45.440]   and I think much broadly construed as well.
[00:43:45.440 --> 00:43:47.140]   So one question then becomes,
[00:43:47.140 --> 00:43:49.820]   well, which of these environments do we want to be in?
[00:43:49.820 --> 00:43:53.000]   I think the cost in terms of wellbeing and relationships
[00:43:53.000 --> 00:43:55.100]   is quite obvious if you're in a competitive environment.
[00:43:55.100 --> 00:43:57.500]   And then the second question of course is,
[00:43:57.500 --> 00:44:00.540]   how do we put ourselves in the type of environment
[00:44:00.540 --> 00:44:03.180]   that we want knowing that that environment will change
[00:44:03.180 --> 00:44:05.980]   who we are over the course of our lives?
[00:44:05.980 --> 00:44:08.820]   - So much of schooling in this country
[00:44:08.820 --> 00:44:11.500]   is based on at first cooperation,
[00:44:11.500 --> 00:44:13.540]   like we're all gonna sit around and listen to a story
[00:44:13.540 --> 00:44:15.580]   and then we're gonna work in small groups.
[00:44:15.580 --> 00:44:18.040]   But in my experience over time,
[00:44:18.040 --> 00:44:21.180]   it evolves into more independent learning and competition.
[00:44:21.180 --> 00:44:23.200]   They post the distribution of scores.
[00:44:23.200 --> 00:44:26.820]   That's largely the distribution of individual scores.
[00:44:26.820 --> 00:44:28.300]   There are exceptions to this, of course.
[00:44:28.300 --> 00:44:30.100]   Like I think I've never been to business school,
[00:44:30.100 --> 00:44:32.300]   but I think they form small groups and work on projects.
[00:44:32.300 --> 00:44:35.340]   It's true in computer science at the undergraduate level
[00:44:35.340 --> 00:44:36.180]   and so on.
[00:44:36.180 --> 00:44:40.220]   But to what extent do you think having a mixture
[00:44:40.220 --> 00:44:44.500]   of cooperative learning, still competition perhaps
[00:44:44.500 --> 00:44:47.940]   between groups, as well as individual learning
[00:44:47.940 --> 00:44:52.940]   and competition can foster kind of an erosion of cynicism?
[00:44:52.940 --> 00:44:55.900]   Because it sounds like being cynical is,
[00:44:55.900 --> 00:44:57.820]   I don't wanna be hard on the cynics here,
[00:44:57.820 --> 00:45:00.100]   but they're probably already hard on themselves
[00:45:00.100 --> 00:45:01.300]   and everybody else.
[00:45:01.300 --> 00:45:02.700]   We know they're hard on everybody else.
[00:45:02.700 --> 00:45:05.140]   But, oh, there was my presumption.
[00:45:05.140 --> 00:45:06.820]   Okay, I'm gonna stay open-minded.
[00:45:06.820 --> 00:45:09.020]   Maybe they're not, you'll tell me.
[00:45:09.020 --> 00:45:13.260]   That they are on average less intelligent
[00:45:13.260 --> 00:45:14.300]   is what I'm hearing.
[00:45:14.300 --> 00:45:18.420]   And that there's something really big to be gained
[00:45:18.420 --> 00:45:21.980]   from anybody who decides to embrace novel ideas,
[00:45:21.980 --> 00:45:25.020]   even if they decide to stick with their original decision
[00:45:25.020 --> 00:45:27.220]   about others or something.
[00:45:27.220 --> 00:45:30.700]   Provided they explore the data in an open-minded way,
[00:45:30.700 --> 00:45:32.260]   even transiently, it sounds like
[00:45:32.260 --> 00:45:33.660]   there's an opportunity there.
[00:45:33.660 --> 00:45:38.300]   You gave a long-term example of these two phishing scenarios.
[00:45:38.300 --> 00:45:40.980]   So the neuroplasticity takes years,
[00:45:40.980 --> 00:45:42.980]   but we know neuroplasticity can be pretty quick.
[00:45:42.980 --> 00:45:44.820]   I would imagine if you expose a cynic
[00:45:44.820 --> 00:45:48.260]   to a counterexample to their belief
[00:45:48.260 --> 00:45:50.700]   that it's not gonna erode all of their cynicism,
[00:45:50.700 --> 00:45:51.940]   but it might make a little dent
[00:45:51.940 --> 00:45:54.340]   in that neural circuit for cynicism.
[00:45:54.340 --> 00:45:56.460]   - Yeah, this is a great perspective.
[00:45:56.460 --> 00:45:59.180]   And a couple of things I wanna be clear on.
[00:45:59.180 --> 00:46:03.300]   One, I am not here to judge or impugn cynics.
[00:46:03.300 --> 00:46:07.020]   I should confess that I myself struggle with cynicism
[00:46:07.020 --> 00:46:08.980]   and have for my entire life.
[00:46:08.980 --> 00:46:12.180]   Part of my journey to learn more about it
[00:46:12.180 --> 00:46:13.580]   and even to write this book
[00:46:13.580 --> 00:46:15.940]   was an attempt to understand myself
[00:46:15.940 --> 00:46:20.940]   and to see if it is possible to unlearn cynicism
[00:46:20.940 --> 00:46:22.940]   because frankly, I wanted to.
[00:46:22.940 --> 00:46:25.580]   So you will get no judgment from me
[00:46:25.580 --> 00:46:28.220]   of people who feel like it's hard to trust.
[00:46:28.220 --> 00:46:33.260]   I think that another point that you're bringing out
[00:46:33.260 --> 00:46:35.860]   that I wanna cosign is that saying
[00:46:35.860 --> 00:46:38.820]   that competition over the longterm,
[00:46:38.820 --> 00:46:41.060]   zero-sum competition can erode our trust
[00:46:41.060 --> 00:46:43.540]   isn't the same as saying that we should never compete.
[00:46:43.540 --> 00:46:44.860]   Competition is beautiful.
[00:46:44.860 --> 00:46:47.660]   I mean, the Olympics are going on right now
[00:46:47.660 --> 00:46:50.420]   and it's amazing to see what people do
[00:46:50.420 --> 00:46:53.820]   when they are at odds trying to best one another.
[00:46:53.820 --> 00:46:57.300]   Incredible feats are accomplished
[00:46:57.300 --> 00:47:00.540]   when we focus on the great things that we can do.
[00:47:00.540 --> 00:47:03.340]   And oftentimes we are driven to greatness
[00:47:03.340 --> 00:47:07.300]   by people we respect who are trying to be greater than us.
[00:47:07.300 --> 00:47:09.860]   So absolutely competition can be part
[00:47:09.860 --> 00:47:14.460]   of a very healthy social structure and a very healthy life.
[00:47:14.460 --> 00:47:16.840]   I think that the broader question
[00:47:16.840 --> 00:47:18.820]   is whether we construe that competition
[00:47:18.820 --> 00:47:23.820]   at the level of a task or at the level of the person.
[00:47:23.820 --> 00:47:28.040]   In fact, there's a lot of work in the science of conflict
[00:47:28.040 --> 00:47:30.740]   and conflict resolution that looks at the difference
[00:47:30.740 --> 00:47:34.820]   between task conflict and personal conflict.
[00:47:34.820 --> 00:47:36.780]   You can imagine in a workplace,
[00:47:36.780 --> 00:47:38.340]   two people have different ideas
[00:47:38.340 --> 00:47:41.780]   for what direction they wanna take a project in.
[00:47:41.780 --> 00:47:45.460]   Well, that's great if it leads to healthy debate
[00:47:45.460 --> 00:47:48.540]   and if that is mutually respectful.
[00:47:48.540 --> 00:47:50.780]   But the minute that that turns into
[00:47:50.780 --> 00:47:52.820]   blanket judgments about the other person,
[00:47:52.820 --> 00:47:54.660]   oh, the reason that they want this direction
[00:47:54.660 --> 00:47:56.940]   is because they're not so bright
[00:47:56.940 --> 00:47:59.340]   or because they don't have vision
[00:47:59.340 --> 00:48:01.140]   or because they're trying to gain favor.
[00:48:01.140 --> 00:48:05.740]   That's when we go from healthy skeptical conflict
[00:48:05.740 --> 00:48:09.500]   into cynical and destructive conflict.
[00:48:09.500 --> 00:48:11.860]   And you see this with athletes as well.
[00:48:11.860 --> 00:48:14.540]   Athletes often are very good friends
[00:48:14.540 --> 00:48:16.980]   and some of the people that they respect the most
[00:48:16.980 --> 00:48:19.260]   are the folks who they're battling.
[00:48:19.260 --> 00:48:23.540]   In the case of contact sports and boxing,
[00:48:23.540 --> 00:48:26.500]   literally battling, but they can have immense
[00:48:26.500 --> 00:48:28.700]   and positive regard for one another
[00:48:28.700 --> 00:48:31.340]   outside of the ring in those contexts.
[00:48:31.340 --> 00:48:33.480]   So I think that there's a huge difference
[00:48:33.480 --> 00:48:37.020]   between competition that's oriented on tasks,
[00:48:37.020 --> 00:48:39.780]   which can help us be the best version of ourselves
[00:48:39.780 --> 00:48:44.020]   and competition that bleeds into judgment,
[00:48:44.020 --> 00:48:46.340]   suspicion and mistrust.
[00:48:46.340 --> 00:48:48.620]   - I'd like to take us back just briefly
[00:48:48.620 --> 00:48:51.020]   to these developmental stages.
[00:48:51.020 --> 00:48:54.580]   Maybe I'm bridging two things that don't belong together,
[00:48:54.580 --> 00:48:56.840]   but I'm thinking about the young brain,
[00:48:56.840 --> 00:48:58.980]   which of course is hyperplastic
[00:48:58.980 --> 00:49:01.160]   and comparing that to the older brain.
[00:49:01.160 --> 00:49:04.820]   But the young brain learns a number of things
[00:49:04.820 --> 00:49:05.960]   while it does a number of things.
[00:49:05.960 --> 00:49:09.180]   It handles heart rate, digestion, et cetera, unconsciously.
[00:49:09.180 --> 00:49:12.300]   And then in many ways, the neuroplasticity
[00:49:12.300 --> 00:49:13.380]   that occurs early in life
[00:49:13.380 --> 00:49:16.620]   is to establish these maps of prediction.
[00:49:16.620 --> 00:49:20.140]   If things fall down, not up in general,
[00:49:20.140 --> 00:49:23.920]   things fall down, not up and so on.
[00:49:23.920 --> 00:49:26.140]   So that mental real estate can be used
[00:49:26.140 --> 00:49:28.040]   for other things and learning new things.
[00:49:28.040 --> 00:49:31.500]   So I'm thinking about the sort of classic example
[00:49:31.500 --> 00:49:33.100]   of object permanence.
[00:49:33.100 --> 00:49:36.780]   You show a baby a block or a toy,
[00:49:36.780 --> 00:49:38.940]   and then you hide that toy.
[00:49:38.940 --> 00:49:41.260]   And they, at a certain age, a very young age,
[00:49:41.260 --> 00:49:43.220]   will look as if it's gone.
[00:49:43.220 --> 00:49:45.340]   And then you bring it back and then they're amazed.
[00:49:45.340 --> 00:49:47.820]   And then at some point along their developmental trajectory,
[00:49:47.820 --> 00:49:49.500]   they learn object permanence.
[00:49:49.500 --> 00:49:51.900]   They know that it's behind your back, okay?
[00:49:51.900 --> 00:49:56.040]   And then we hear that characters like Santa Claus are real,
[00:49:56.040 --> 00:49:58.640]   and then eventually we learn that they're not,
[00:49:58.640 --> 00:49:59.700]   and so on and so on.
[00:49:59.700 --> 00:50:06.000]   In many ways, we go from being completely non-cynical
[00:50:06.000 --> 00:50:09.580]   about the physical world to being,
[00:50:09.580 --> 00:50:12.540]   one could sort of view it as cynical
[00:50:12.540 --> 00:50:14.000]   about the physical world, right?
[00:50:14.000 --> 00:50:16.900]   Like I love to see magic.
[00:50:16.900 --> 00:50:19.700]   In fact, we had probably the world's best
[00:50:19.700 --> 00:50:23.500]   or among the very best magicians on this podcast,
[00:50:23.500 --> 00:50:25.820]   Ozzy Wind, he's a mentalist and magician.
[00:50:25.820 --> 00:50:27.580]   And to see him do magic,
[00:50:27.580 --> 00:50:29.500]   even as an adult who understands
[00:50:29.500 --> 00:50:31.180]   that the laws of physics apply,
[00:50:31.180 --> 00:50:35.180]   they seem to defy the laws of physics in real time.
[00:50:35.180 --> 00:50:38.940]   And it just blows your mind to the point where you like,
[00:50:38.940 --> 00:50:41.220]   that can't be, but you sort of want it to be.
[00:50:41.220 --> 00:50:42.820]   And at some point you just go, you know what?
[00:50:42.820 --> 00:50:45.020]   It's what we call magic.
[00:50:45.020 --> 00:50:49.780]   So it seems to me that cynics apply almost physics
[00:50:49.780 --> 00:50:52.160]   like rules to social interaction.
[00:50:52.160 --> 00:50:56.220]   Like that they talk in terms of like first principles
[00:50:56.220 --> 00:50:58.260]   of human interactions, right?
[00:50:58.260 --> 00:51:00.980]   They talk about this group always this,
[00:51:00.980 --> 00:51:02.700]   and that group always that, right?
[00:51:02.700 --> 00:51:05.060]   These like strict categories,
[00:51:05.060 --> 00:51:07.860]   thick black lines between categories,
[00:51:07.860 --> 00:51:11.780]   as opposed to any kind of blending of understanding
[00:51:11.780 --> 00:51:13.780]   or a blending of rules.
[00:51:13.780 --> 00:51:16.180]   And one can see how that would be a really useful heuristic,
[00:51:16.180 --> 00:51:19.540]   but as we're learning, it's not good in the sense
[00:51:19.540 --> 00:51:20.360]   that we don't want to judge,
[00:51:20.360 --> 00:51:22.900]   but it's not good if our goal is to learn more
[00:51:22.900 --> 00:51:24.900]   about the world or learn the most information
[00:51:24.900 --> 00:51:25.720]   about the world.
[00:51:25.720 --> 00:51:26.560]   Can we say that?
[00:51:26.560 --> 00:51:29.020]   - Yes, and I appreciate you saying, yeah.
[00:51:29.020 --> 00:51:33.600]   I also try to avoid good, bad language or moral judgment,
[00:51:33.600 --> 00:51:35.780]   but I think that many of us have the goals
[00:51:35.780 --> 00:51:37.800]   of having strong relationships
[00:51:37.800 --> 00:51:40.900]   and of flourishing psychologically
[00:51:40.900 --> 00:51:43.220]   and of learning accurately about the world.
[00:51:43.220 --> 00:51:44.460]   And if those are your goals,
[00:51:44.460 --> 00:51:45.900]   I think it's fair to say
[00:51:45.900 --> 00:51:49.200]   that cynicism can block your way towards them.
[00:51:49.200 --> 00:51:50.180]   I love this.
[00:51:50.180 --> 00:51:51.940]   I've never thought about it in this way,
[00:51:51.940 --> 00:51:53.540]   but I love that perspective.
[00:51:53.540 --> 00:51:58.360]   And there is almost a philosophical certainty.
[00:51:58.360 --> 00:52:01.540]   Maybe it's not a happy philosophical certainty,
[00:52:01.540 --> 00:52:03.620]   but we love to, right?
[00:52:03.620 --> 00:52:06.580]   Human beings love explanatory power.
[00:52:06.580 --> 00:52:09.700]   We love to be able to have laws
[00:52:09.700 --> 00:52:11.740]   that determine what will happen.
[00:52:11.740 --> 00:52:14.860]   And the laws of physics are some of our most reliable,
[00:52:14.860 --> 00:52:15.700]   right?
[00:52:15.700 --> 00:52:19.220]   And really we all use theories to predict the world, right?
[00:52:19.220 --> 00:52:20.980]   I mean, we all have a theory of gravity
[00:52:20.980 --> 00:52:22.140]   that lives inside our head.
[00:52:22.140 --> 00:52:24.880]   We don't think objects with mass attract one another,
[00:52:24.880 --> 00:52:27.460]   but we know if we drop a bowling ball on our foot,
[00:52:27.460 --> 00:52:28.980]   we're gonna probably maybe not walk
[00:52:28.980 --> 00:52:30.580]   for the next week or at least, right?
[00:52:30.580 --> 00:52:35.580]   So we use theories to provide explanatory simplicity
[00:52:35.580 --> 00:52:40.300]   to a vast and overwhelmingly complex world.
[00:52:40.300 --> 00:52:45.260]   And absolutely, I think cynicism has a great function
[00:52:45.260 --> 00:52:48.660]   in simplifying, but of course in simplifying,
[00:52:48.660 --> 00:52:50.940]   we lose a lot of the detail.
[00:52:50.940 --> 00:52:53.300]   We lose a lot of the wonder
[00:52:53.300 --> 00:52:56.900]   that maybe we experienced earlier in life.
[00:52:56.900 --> 00:53:01.820]   And I do wanna, your beautiful description of kids
[00:53:01.820 --> 00:53:06.180]   and their sort of sense of, I suppose, perennial surprise
[00:53:07.180 --> 00:53:09.820]   makes me think about another aspect
[00:53:09.820 --> 00:53:12.120]   of what we lose to cynicism,
[00:53:12.120 --> 00:53:17.120]   which is the ability to witness the beauty of human action
[00:53:17.120 --> 00:53:19.240]   and human kindness.
[00:53:19.240 --> 00:53:22.820]   My friend, Dacher Keltner, studies awe,
[00:53:22.820 --> 00:53:27.180]   this emotion of experiencing something vast
[00:53:27.180 --> 00:53:31.220]   and also experiencing ourselves as small
[00:53:31.220 --> 00:53:33.540]   and a part of that vastness.
[00:53:33.540 --> 00:53:36.420]   And he wrote a great book on awe.
[00:53:36.420 --> 00:53:38.420]   And in it, he talks about his research
[00:53:38.420 --> 00:53:42.500]   where he cataloged what are the experiences
[00:53:42.500 --> 00:53:46.980]   that most commonly produce awe in a large sample,
[00:53:46.980 --> 00:53:49.180]   large representative sample of people.
[00:53:49.180 --> 00:53:50.940]   Now, I don't know about you, Andrew,
[00:53:50.940 --> 00:53:52.500]   but when I think about awe,
[00:53:52.500 --> 00:53:57.420]   my first go-to is Carl Sagan's "Pale Blue Dot,"
[00:53:57.420 --> 00:53:59.900]   this image of a kind of nebula band
[00:53:59.900 --> 00:54:04.900]   or sort of cluster basically, stardust really.
[00:54:04.900 --> 00:54:06.540]   And there's one dot in it with an arrow,
[00:54:06.540 --> 00:54:09.580]   and Carl Sagan says, "That dot is Earth,
[00:54:09.580 --> 00:54:13.700]   "and every king and tyrant and mother and father,
[00:54:13.700 --> 00:54:16.820]   "every person who's ever fallen in love,
[00:54:16.820 --> 00:54:18.480]   "and every person who's ever had their heart broken,
[00:54:18.480 --> 00:54:20.300]   "they're all on that tiny dot there."
[00:54:20.300 --> 00:54:23.460]   I go to that, I show that to my kids all the time.
[00:54:23.460 --> 00:54:25.860]   When I think of awe, I think of outer space.
[00:54:25.860 --> 00:54:27.860]   I think of groves of redwood trees.
[00:54:27.860 --> 00:54:31.140]   I think of drone footage of the Himalayas, right?
[00:54:31.140 --> 00:54:33.580]   But Dacher finds that if you ask people
[00:54:33.580 --> 00:54:36.340]   what they experience awe in response to,
[00:54:36.340 --> 00:54:40.900]   the number one category is what he calls moral beauty,
[00:54:40.900 --> 00:54:46.060]   everyday acts of kindness, giving,
[00:54:46.060 --> 00:54:48.700]   compassion, and connection.
[00:54:48.700 --> 00:54:51.500]   This is also related to what Dacher and John Haidt
[00:54:51.500 --> 00:54:53.700]   talk about in terms of moral elevation,
[00:54:53.700 --> 00:54:57.820]   witnessing positive actions that actually make us feel
[00:54:57.820 --> 00:55:00.100]   like we're capable of more.
[00:55:00.100 --> 00:55:03.140]   And moral beauty is everywhere.
[00:55:03.140 --> 00:55:05.500]   If you are open to it, it is the most common thing
[00:55:05.500 --> 00:55:09.820]   that will make you feel the vastness of our species.
[00:55:09.820 --> 00:55:14.320]   And to have a lawful, physics-like prediction
[00:55:14.320 --> 00:55:17.740]   about the world that blinkers you from seeing that,
[00:55:17.740 --> 00:55:21.180]   that gives you tunnel vision and prevents you
[00:55:21.180 --> 00:55:23.620]   from experiencing moral beauty
[00:55:23.620 --> 00:55:26.220]   seems like a tragic form of simplicity.
[00:55:26.220 --> 00:55:27.540]   - I'd like to take a quick break
[00:55:27.540 --> 00:55:30.260]   and thank one of our sponsors, Function.
[00:55:30.260 --> 00:55:31.900]   I recently became a Function member
[00:55:31.900 --> 00:55:34.180]   after searching for the most comprehensive approach
[00:55:34.180 --> 00:55:35.180]   to lab testing.
[00:55:35.180 --> 00:55:37.100]   While I've long been a fan of blood testing,
[00:55:37.100 --> 00:55:39.080]   I really wanted to find a more in-depth program
[00:55:39.080 --> 00:55:41.700]   for analyzing blood, urine, and saliva
[00:55:41.700 --> 00:55:43.580]   to get a full picture of my heart health,
[00:55:43.580 --> 00:55:46.380]   my hormone status, my immune system regulation,
[00:55:46.380 --> 00:55:49.640]   my metabolic function, my vitamin and mineral status,
[00:55:49.640 --> 00:55:53.060]   and other critical areas of my overall health and vitality.
[00:55:53.060 --> 00:55:56.260]   Function not only provides testing of over 100 biomarkers
[00:55:56.260 --> 00:55:57.900]   key to physical and mental health,
[00:55:57.900 --> 00:55:59.780]   but it also analyzes these results
[00:55:59.780 --> 00:56:03.340]   and provides insights from top doctors on your results.
[00:56:03.340 --> 00:56:06.180]   For example, in one of my first tests with Function,
[00:56:06.180 --> 00:56:09.300]   I learned that I had two high levels of mercury in my blood.
[00:56:09.300 --> 00:56:10.620]   This was totally surprising to me.
[00:56:10.620 --> 00:56:13.420]   I had no idea prior to taking the test.
[00:56:13.420 --> 00:56:15.280]   Function not only helped me detect this,
[00:56:15.280 --> 00:56:17.780]   but offered medical doctor-informed insights
[00:56:17.780 --> 00:56:20.460]   on how to best reduce those mercury levels,
[00:56:20.460 --> 00:56:22.780]   which included limiting my tuna consumption,
[00:56:22.780 --> 00:56:24.540]   because I had been eating a lot of tuna,
[00:56:24.540 --> 00:56:26.880]   while also making an effort to eat more leafy greens
[00:56:26.880 --> 00:56:29.780]   and supplementing with NAC and acetylcysteine,
[00:56:29.780 --> 00:56:31.860]   both of which can support glutathione production
[00:56:31.860 --> 00:56:35.600]   and detoxification and worked to reduce my mercury levels.
[00:56:35.600 --> 00:56:37.180]   Comprehensive lab testing like this
[00:56:37.180 --> 00:56:38.580]   is so important for health.
[00:56:38.580 --> 00:56:40.380]   And while I've been doing it for years,
[00:56:40.380 --> 00:56:43.300]   I've always found it to be overly complicated and expensive.
[00:56:43.300 --> 00:56:44.780]   I've been so impressed by Function,
[00:56:44.780 --> 00:56:46.680]   both at the level of ease of use,
[00:56:46.680 --> 00:56:48.220]   that is getting the tests done,
[00:56:48.220 --> 00:56:51.100]   as well as how comprehensive and how actionable
[00:56:51.100 --> 00:56:54.520]   the tests are, that I recently joined their advisory board,
[00:56:54.520 --> 00:56:56.760]   and I'm thrilled that they're sponsoring the podcast.
[00:56:56.760 --> 00:56:58.080]   If you'd like to try Function,
[00:56:58.080 --> 00:57:01.380]   go to functionhealth.com/huberman.
[00:57:01.380 --> 00:57:04.980]   Function currently has a wait list of over 250,000 people,
[00:57:04.980 --> 00:57:07.800]   but they're offering early access to Huberman Lab listeners.
[00:57:07.800 --> 00:57:11.060]   Again, that's functionhealth.com/huberman
[00:57:11.060 --> 00:57:13.680]   to get early access to Function.
[00:57:13.680 --> 00:57:17.060]   - I love that your examples of both pale blue dot
[00:57:17.060 --> 00:57:21.600]   and everyday compassion bridge the two,
[00:57:21.600 --> 00:57:24.780]   what I think of as time domains that the,
[00:57:24.780 --> 00:57:26.560]   or I should say space-time domains
[00:57:26.560 --> 00:57:28.780]   that the brain can encompass.
[00:57:28.780 --> 00:57:31.720]   You know, this has long fascinated me about the human brain
[00:57:31.720 --> 00:57:34.240]   and presumably other animals' brains as well,
[00:57:34.240 --> 00:57:39.120]   which is that, you know, we can sharpen our aperture
[00:57:39.120 --> 00:57:41.860]   to, you know, something so, so small
[00:57:41.860 --> 00:57:44.380]   and pay attention to just like the immense beauty.
[00:57:44.380 --> 00:57:45.980]   And, you know, like I have a lot of ants
[00:57:45.980 --> 00:57:46.860]   in my yard right now,
[00:57:46.860 --> 00:57:48.260]   and lately I've been watching them interact
[00:57:48.260 --> 00:57:49.740]   'cause they were driving me crazy.
[00:57:49.740 --> 00:57:50.760]   They were just like, you know,
[00:57:50.760 --> 00:57:52.300]   they're like everywhere this summer
[00:57:52.300 --> 00:57:53.360]   and they're climbing on me.
[00:57:53.360 --> 00:57:55.080]   And I thought, I'm just gonna like watch what they do.
[00:57:55.080 --> 00:57:57.260]   And clearly there's a structure there.
[00:57:57.260 --> 00:57:59.140]   I know Debra Gordon at Stanford
[00:57:59.140 --> 00:58:00.900]   has studied ant behavior in others.
[00:58:00.900 --> 00:58:02.540]   And it's like, there's a lot going on there,
[00:58:02.540 --> 00:58:03.700]   but then you look up from there and you're like,
[00:58:03.700 --> 00:58:04.780]   wow, there's a big yard.
[00:58:04.780 --> 00:58:08.020]   And then the sense of all for me is that
[00:58:08.020 --> 00:58:11.740]   interactions like that must be going on everywhere
[00:58:11.740 --> 00:58:13.420]   in this yard.
[00:58:13.420 --> 00:58:16.380]   And, you know, it frames up that the aperture
[00:58:16.380 --> 00:58:19.020]   of our cognition in space and in time, you know,
[00:58:19.020 --> 00:58:22.400]   covering small distances quickly or small distances slowly.
[00:58:22.400 --> 00:58:24.580]   And then we can zoom out literally
[00:58:24.580 --> 00:58:29.580]   and think about us on this ball in space, right?
[00:58:29.580 --> 00:58:33.460]   You know, and that ability I think is incredible.
[00:58:33.460 --> 00:58:36.820]   And that awe can be captured at these different extremes
[00:58:36.820 --> 00:58:39.900]   of space, time, cognition.
[00:58:39.900 --> 00:58:41.100]   Amazing.
[00:58:41.100 --> 00:58:42.280]   It seems to me that what you're saying
[00:58:42.280 --> 00:58:44.520]   is that cynicism and awe are also opposite ends
[00:58:44.520 --> 00:58:45.340]   of the continuum.
[00:58:45.340 --> 00:58:48.180]   And that's taking us in a direction slightly different
[00:58:48.180 --> 00:58:50.300]   than I was going to try and take us.
[00:58:50.300 --> 00:58:52.300]   But I love that we're talking about awe
[00:58:52.300 --> 00:58:55.660]   because to me it feels like it's a more extreme example
[00:58:55.660 --> 00:58:56.500]   of delight.
[00:58:56.500 --> 00:59:00.500]   And I'd like you to perhaps,
[00:59:00.500 --> 00:59:04.620]   if there's any examples of research on this, you know,
[00:59:04.620 --> 00:59:08.460]   touch on to what extent a sense of cynicism divorces us
[00:59:08.460 --> 00:59:13.460]   from delight and awe, or I guess their collaborator,
[00:59:13.460 --> 00:59:16.140]   which is creativity.
[00:59:16.140 --> 00:59:18.860]   To me, everything you're saying about cynicism
[00:59:18.860 --> 00:59:22.020]   makes it sound anti-creative because you're,
[00:59:22.020 --> 00:59:24.580]   by definition, you're eliminating possibility.
[00:59:24.580 --> 00:59:28.660]   And creativity, of course, is the unique original combination
[00:59:28.660 --> 00:59:30.580]   of existing things or the creation of new things
[00:59:30.580 --> 00:59:32.700]   altogether, creativity.
[00:59:32.700 --> 00:59:34.780]   So what, if anything, has been studied
[00:59:34.780 --> 00:59:38.060]   about the relationship between cynicism,
[00:59:38.060 --> 00:59:40.460]   I guess we call it open-mindedness,
[00:59:40.460 --> 00:59:43.140]   and creativity and/or awe?
[00:59:43.140 --> 00:59:44.700]   - Yeah, great questions.
[00:59:44.700 --> 00:59:48.140]   And there is some work on this,
[00:59:48.140 --> 00:59:50.220]   and a lot of it comes actually in the context
[00:59:50.220 --> 00:59:52.000]   of the workplace, right?
[00:59:52.000 --> 00:59:53.780]   So you can examine, I mean,
[00:59:53.780 --> 00:59:55.660]   these Brazilian fishing villages
[00:59:55.660 --> 00:59:57.740]   were, after all, workplaces, right?
[00:59:57.740 --> 01:00:01.020]   That led people to more or less cynicism.
[01:00:01.020 --> 01:00:03.960]   But other workplaces also have structures
[01:00:03.960 --> 01:00:07.140]   that make people more or less able to trust one another.
[01:00:07.140 --> 01:00:09.900]   One version of this is what's known as stack ranking.
[01:00:09.900 --> 01:00:12.780]   And, you know, this is where people,
[01:00:12.780 --> 01:00:16.460]   managers are forced to pick the highest-performing
[01:00:16.460 --> 01:00:18.740]   and lowest-performing members of their team
[01:00:18.740 --> 01:00:21.380]   and, in essence, eliminate the people
[01:00:21.380 --> 01:00:25.160]   who are at the bottom 10% every six or 12 months.
[01:00:25.160 --> 01:00:26.900]   Stack ranking has, thankfully,
[01:00:26.900 --> 01:00:30.340]   mostly fallen out of favor in the corporate world,
[01:00:30.340 --> 01:00:34.860]   but it was very de rigueur in the late 20th
[01:00:34.860 --> 01:00:36.900]   and early 21st century, you know,
[01:00:36.900 --> 01:00:38.740]   up until 10 or so years ago.
[01:00:38.740 --> 01:00:41.100]   And it still exists in some places.
[01:00:41.100 --> 01:00:45.820]   And the idea, again, was if you want people to be creative,
[01:00:45.820 --> 01:00:48.260]   if you want them to do their best,
[01:00:48.260 --> 01:00:50.100]   tap into who they really are.
[01:00:50.100 --> 01:00:51.420]   And who are we really?
[01:00:51.420 --> 01:00:55.020]   We are really a hyper-individualistic,
[01:00:55.020 --> 01:00:57.100]   again, Darwinian species.
[01:00:57.100 --> 01:01:00.860]   Really, stack ranking is a social Darwinist approach
[01:01:00.860 --> 01:01:02.100]   to management.
[01:01:02.100 --> 01:01:05.180]   And the idea is, well, great, if you threaten people,
[01:01:05.180 --> 01:01:07.820]   if you make them want to defeat one another,
[01:01:07.820 --> 01:01:09.700]   they will be at their most creative
[01:01:09.700 --> 01:01:12.340]   when they are trying to do that, right?
[01:01:12.340 --> 01:01:14.660]   That it will bring out their best.
[01:01:14.660 --> 01:01:15.620]   The opposite is true.
[01:01:15.620 --> 01:01:19.400]   I mean, stack-ranked workplaces, of course, are miserable.
[01:01:19.400 --> 01:01:22.460]   The people in them are quite unhappy
[01:01:22.460 --> 01:01:25.060]   and more likely to leave their jobs.
[01:01:25.060 --> 01:01:26.700]   But some of the more interesting work
[01:01:26.700 --> 01:01:29.680]   pertains to what stack ranking does to creativity.
[01:01:29.680 --> 01:01:33.940]   Because it turns out that if your job
[01:01:33.940 --> 01:01:36.600]   is to just not be at the bottom of the pile,
[01:01:36.600 --> 01:01:42.140]   then the last thing you want to do is take a creative risk.
[01:01:42.140 --> 01:01:43.940]   You do not want to go out on a limb.
[01:01:43.940 --> 01:01:46.580]   You do not want to try something new.
[01:01:46.580 --> 01:01:50.640]   If other people are going to go after you for doing that,
[01:01:50.640 --> 01:01:54.400]   and if you screw up, or if it doesn't go well,
[01:01:54.400 --> 01:01:57.120]   you're eliminated from the group, right?
[01:01:57.120 --> 01:01:59.080]   So I think you're exactly right
[01:01:59.080 --> 01:02:01.040]   that these cynical environments
[01:02:01.040 --> 01:02:02.880]   are also highly conservative.
[01:02:02.880 --> 01:02:04.760]   I, of course, don't mean politically conservative.
[01:02:04.760 --> 01:02:08.080]   I mean conservative in terms of the types of choices
[01:02:08.080 --> 01:02:09.480]   that people make.
[01:02:09.480 --> 01:02:10.440]   And that's sort of, I think,
[01:02:10.440 --> 01:02:13.560]   at the level of individual creativity.
[01:02:13.560 --> 01:02:15.620]   But there's also a cost at the level
[01:02:15.620 --> 01:02:19.120]   of what we might call group creativity, right?
[01:02:19.120 --> 01:02:23.440]   A lot of our best ideas come not from our minds,
[01:02:23.440 --> 01:02:25.280]   but from the space between us,
[01:02:25.280 --> 01:02:29.320]   from dialogue, or from group conversation.
[01:02:29.320 --> 01:02:31.680]   And it turns out that in stacked rank,
[01:02:31.680 --> 01:02:33.320]   zero-sum environments,
[01:02:33.320 --> 01:02:36.840]   people are less willing to share knowledge and perspective
[01:02:36.840 --> 01:02:40.840]   because doing so amounts to helping your enemy succeed,
[01:02:40.840 --> 01:02:43.300]   which is the same as helping yourself fail.
[01:02:43.300 --> 01:02:46.680]   So to the extent that creativity requires
[01:02:46.680 --> 01:02:49.000]   a sort of collaborative mindset,
[01:02:49.000 --> 01:02:53.840]   then cynicism is preventative of that.
[01:02:53.840 --> 01:02:55.960]   And there's actually some terrific work
[01:02:55.960 --> 01:03:00.520]   by Anita Woolley and colleagues
[01:03:00.520 --> 01:03:04.060]   that looks at group intelligence, collective intelligence.
[01:03:04.060 --> 01:03:06.200]   This is the idea that, of course,
[01:03:06.200 --> 01:03:08.120]   people have levels of intelligence
[01:03:08.120 --> 01:03:10.120]   that can be measured in various ways,
[01:03:10.120 --> 01:03:12.720]   and have various forms of intelligence as well.
[01:03:12.720 --> 01:03:15.160]   But groups, when they get together,
[01:03:15.160 --> 01:03:17.340]   have a type of intelligence,
[01:03:17.340 --> 01:03:20.120]   and especially creative problem-solving intelligence,
[01:03:20.120 --> 01:03:23.120]   that goes above and beyond the sum of their parts,
[01:03:23.120 --> 01:03:24.800]   that can't be explained,
[01:03:24.800 --> 01:03:27.980]   and actually in some cases is almost orthogonal
[01:03:27.980 --> 01:03:31.340]   to the intelligence of the individuals in that group, right?
[01:03:31.340 --> 01:03:33.920]   Controlling for the intelligence of individuals,
[01:03:33.920 --> 01:03:36.400]   there's a group factor that still matters.
[01:03:36.400 --> 01:03:38.640]   And so Anita Woolley and others have looked at,
[01:03:38.640 --> 01:03:43.080]   well, what predicts that type of collective intelligence?
[01:03:43.080 --> 01:03:45.520]   And a couple of factors matter.
[01:03:45.520 --> 01:03:49.000]   One is people's ability
[01:03:49.000 --> 01:03:51.440]   to understand each other's emotions,
[01:03:51.440 --> 01:03:53.480]   so interpersonal sensitivity.
[01:03:53.480 --> 01:03:55.800]   But another is their willingness to, in essence,
[01:03:55.800 --> 01:04:00.500]   pass the mic, to share the conversation, and to collaborate.
[01:04:00.500 --> 01:04:03.560]   And so again, succeeding, thriving,
[01:04:03.560 --> 01:04:06.180]   optimizing, and being creative,
[01:04:06.180 --> 01:04:09.520]   both at the individual and at the group level,
[01:04:09.520 --> 01:04:12.280]   require environments where we feel free,
[01:04:12.280 --> 01:04:14.240]   and where we feel safe,
[01:04:14.240 --> 01:04:17.160]   and where we feel that contributing to somebody else
[01:04:17.160 --> 01:04:18.900]   can also contribute to ourselves.
[01:04:18.900 --> 01:04:22.880]   - It's so interesting to think about
[01:04:22.880 --> 01:04:25.320]   all of this in the context of neuroplasticity.
[01:04:25.320 --> 01:04:28.320]   I feel like one of the holy grails of neuroscience
[01:04:28.320 --> 01:04:29.880]   is to finally understand,
[01:04:29.880 --> 01:04:31.840]   you know, what are the gates to neuroplasticity?
[01:04:31.840 --> 01:04:33.760]   We understand a lot about the cellular mechanisms.
[01:04:33.760 --> 01:04:35.440]   We know it's possible throughout the lifespan.
[01:04:35.440 --> 01:04:38.160]   We know that there's sure an involvement
[01:04:38.160 --> 01:04:40.560]   of different neuromodulators and so on,
[01:04:40.560 --> 01:04:44.760]   but at the level of kind of human behavior
[01:04:44.760 --> 01:04:49.320]   and emotional stance, not technical, not a technical term,
[01:04:49.320 --> 01:04:53.000]   but I'll use it, of say, being curious.
[01:04:53.000 --> 01:04:56.760]   Like to me, curiosity is an interest in the outcome
[01:04:56.760 --> 01:05:00.160]   with no specific emotional attachment to the outcome.
[01:05:00.160 --> 01:05:02.280]   But of course, we could say you're curious
[01:05:02.280 --> 01:05:04.200]   with the hope of getting a certain result, you know,
[01:05:04.200 --> 01:05:05.320]   so one could modify it.
[01:05:05.320 --> 01:05:10.000]   But there is something about that childlike mind,
[01:05:10.000 --> 01:05:11.200]   so-called beginner's mind,
[01:05:11.200 --> 01:05:12.920]   where you're open to different outcomes.
[01:05:12.920 --> 01:05:15.160]   And it seems like the examples that you're giving
[01:05:15.160 --> 01:05:17.680]   keep bringing me back to these developmental themes
[01:05:17.680 --> 01:05:21.400]   because if it's true that cynics, you know,
[01:05:21.400 --> 01:05:24.220]   exclude a lot of data that could be useful to them,
[01:05:24.220 --> 01:05:28.800]   it seems that the opportunities for neuroplasticity
[01:05:28.800 --> 01:05:31.360]   are reduced for cynics.
[01:05:31.360 --> 01:05:32.680]   To flip it on its head,
[01:05:32.680 --> 01:05:38.440]   to what extent are we all a little bit cynical?
[01:05:38.440 --> 01:05:40.060]   And how would we explore that?
[01:05:40.060 --> 01:05:41.780]   Like if I were in your laboratory
[01:05:41.780 --> 01:05:44.280]   and you had 10 minutes with me,
[01:05:44.280 --> 01:05:46.280]   what questions would you ask me
[01:05:46.280 --> 01:05:48.600]   to determine how cynical I might be
[01:05:48.600 --> 01:05:51.660]   or how not cynical I might be?
[01:05:51.660 --> 01:05:52.680]   - Well, the first thing that I would do
[01:05:52.680 --> 01:05:55.920]   is give you that classic questionnaire from Cook and Medley,
[01:05:55.920 --> 01:05:58.720]   which would just ask you about your theories of the world.
[01:05:58.720 --> 01:06:00.360]   What do you think people are like?
[01:06:00.360 --> 01:06:02.860]   Do you think that people are generally honest?
[01:06:02.860 --> 01:06:05.160]   Do you think that they are generally trustworthy?
[01:06:05.160 --> 01:06:07.440]   - So it loads the questions or it's open-ended
[01:06:07.440 --> 01:06:09.720]   where I would, would you say, what are people like?
[01:06:09.720 --> 01:06:11.600]   And then I would just kind of free associate about that?
[01:06:11.600 --> 01:06:14.240]   - No, it's a series of 50 statements.
[01:06:14.240 --> 01:06:16.040]   And you're asked in a binary way,
[01:06:16.040 --> 01:06:18.840]   do you agree or disagree with each of these statements?
[01:06:18.840 --> 01:06:20.760]   Since then, Olga Stavrova and others
[01:06:20.760 --> 01:06:23.960]   have adapted Cook-Medley and made it a shorter scale
[01:06:23.960 --> 01:06:26.240]   and turned the questions into continuous
[01:06:26.240 --> 01:06:29.440]   one to nine or one to seven answers.
[01:06:29.440 --> 01:06:33.120]   But generally speaking, these are discrete questions
[01:06:33.120 --> 01:06:36.000]   that numerically or quantitatively
[01:06:36.000 --> 01:06:39.700]   tap our general theories of people.
[01:06:39.700 --> 01:06:41.960]   If you were in my lab, I might also ask you
[01:06:41.960 --> 01:06:43.880]   to play some different economic games.
[01:06:43.880 --> 01:06:46.840]   You know, the trust game being the number one
[01:06:46.840 --> 01:06:47.980]   that we might use here.
[01:06:47.980 --> 01:06:50.200]   So I can explain it.
[01:06:50.200 --> 01:06:52.640]   So the trust game involves two players
[01:06:52.640 --> 01:06:55.840]   and one of them is an investor.
[01:06:55.840 --> 01:06:57.840]   They start out with some amounts of money,
[01:06:57.840 --> 01:06:59.380]   let's just say $10.
[01:06:59.380 --> 01:07:01.240]   They can send as much of that money
[01:07:01.240 --> 01:07:03.440]   as they want to a trustee.
[01:07:03.440 --> 01:07:06.240]   The money is then tripled in value.
[01:07:06.240 --> 01:07:09.240]   So if the investor sends $10,
[01:07:09.240 --> 01:07:12.860]   in the hands of the trustee, it becomes $30.
[01:07:12.860 --> 01:07:15.240]   The trustee can then choose to give back
[01:07:15.240 --> 01:07:17.960]   whatever amount they want to the investor.
[01:07:17.960 --> 01:07:21.720]   So they can be exactly fair and give 15 back,
[01:07:21.720 --> 01:07:24.800]   in which case both people end up pretty much better off
[01:07:24.800 --> 01:07:27.720]   than they would have without an active trust.
[01:07:27.720 --> 01:07:31.020]   The trustee can keep all $30 themselves,
[01:07:31.020 --> 01:07:33.200]   betraying the investor,
[01:07:33.200 --> 01:07:35.460]   or the trustee can give more than 50% back.
[01:07:35.460 --> 01:07:37.020]   They can say, well, I started out with nothing,
[01:07:37.020 --> 01:07:39.260]   why don't you take 2/3 back?
[01:07:39.260 --> 01:07:42.980]   And this is one terrific behavioral measure of trust
[01:07:42.980 --> 01:07:44.640]   and it can be played in a couple of different ways.
[01:07:44.640 --> 01:07:47.700]   One is binary, where I would say,
[01:07:47.700 --> 01:07:51.940]   Andrew, you can send $10 to an internet stranger
[01:07:51.940 --> 01:07:53.420]   or you can send nothing
[01:07:53.420 --> 01:07:55.340]   and they can choose to send you back half
[01:07:55.340 --> 01:07:58.200]   or they can choose to send you back nothing.
[01:07:58.200 --> 01:07:59.040]   Would you do it?
[01:07:59.040 --> 01:08:01.480]   Actually, I'm curious, would you do that?
[01:08:01.480 --> 01:08:03.680]   - Oh, I absolutely zip it over to them.
[01:08:03.680 --> 01:08:04.520]   Yeah, I'm curious.
[01:08:04.520 --> 01:08:05.340]   - Great.
[01:08:05.340 --> 01:08:07.640]   - And I'm willing to lose the money.
[01:08:07.640 --> 01:08:09.040]   So I suppose that factors in as well.
[01:08:09.040 --> 01:08:09.880]   - Yeah.
[01:08:09.880 --> 01:08:10.700]   Follow-up question.
[01:08:10.700 --> 01:08:14.120]   In that type of study, what percentage of trustees
[01:08:14.120 --> 01:08:16.200]   do you think make the trustworthy decision
[01:08:16.200 --> 01:08:17.500]   of sending back the money?
[01:08:17.500 --> 01:08:23.560]   - Gosh.
[01:08:24.500 --> 01:08:27.080]   (silence)
[01:08:27.080 --> 01:08:28.520]   55%.
[01:08:28.520 --> 01:08:31.140]   - Yeah, so your prediction there
[01:08:31.140 --> 01:08:33.780]   is quite aligned with most people's.
[01:08:33.780 --> 01:08:39.500]   There's a great study by Fechenhauer and Dunning
[01:08:39.500 --> 01:08:43.160]   that found that people, when they're asked to forecast,
[01:08:43.160 --> 01:08:47.700]   they say, I bet 52, 55% of people will send this money back,
[01:08:47.700 --> 01:08:50.300]   will make this binary trust decision.
[01:08:50.300 --> 01:08:52.660]   In fact, 80% of trustees
[01:08:52.660 --> 01:08:55.840]   make the pro-social and trustworthy decision.
[01:08:55.840 --> 01:08:58.160]   And again, what Fechenhauer and Dunning found
[01:08:58.160 --> 01:09:02.260]   is that when we have negative assumptions,
[01:09:02.260 --> 01:09:04.740]   we're less likely to send over the money,
[01:09:04.740 --> 01:09:07.440]   and therefore less likely to learn that we were wrong.
[01:09:07.440 --> 01:09:10.800]   And so that's one of, it's another example
[01:09:10.800 --> 01:09:13.780]   of where cynical beliefs, I mean,
[01:09:13.780 --> 01:09:15.460]   you're interesting because you had the belief
[01:09:15.460 --> 01:09:18.660]   that it's a 50% chance, but you still chose to trust.
[01:09:18.660 --> 01:09:21.020]   So from a Bayesian perspective,
[01:09:21.020 --> 01:09:23.300]   when that person actually sent the money back,
[01:09:23.300 --> 01:09:25.580]   which they would have an 80% chance of doing,
[01:09:25.580 --> 01:09:27.140]   and if I were to ask you again,
[01:09:27.140 --> 01:09:29.440]   what percentage of people give back,
[01:09:29.440 --> 01:09:31.780]   you might update your perception.
[01:09:31.780 --> 01:09:32.620]   - Absolutely.
[01:09:32.620 --> 01:09:33.700]   - Right?
[01:09:33.700 --> 01:09:36.940]   But without any evidence, you can't update your perception.
[01:09:36.940 --> 01:09:39.220]   So, and this is just one of many examples.
[01:09:39.220 --> 01:09:41.600]   It turns out that there's a lot of evidence
[01:09:41.600 --> 01:09:46.180]   that when asked to estimate how friendly, trustworthy,
[01:09:46.180 --> 01:09:49.100]   compassionate, or open-minded others are,
[01:09:49.100 --> 01:09:52.620]   people's estimates come in much lower than data suggests.
[01:09:52.620 --> 01:09:56.940]   And this to me is both the tragedy of cynical thinking,
[01:09:56.940 --> 01:09:58.500]   those heuristics that we're using,
[01:09:58.500 --> 01:10:01.780]   and a major opportunity for so many of us, right?
[01:10:01.780 --> 01:10:04.340]   It's a tragedy because we're coming up
[01:10:04.340 --> 01:10:06.700]   with these simple black and white,
[01:10:06.700 --> 01:10:09.500]   physics-like predictions about the world,
[01:10:09.500 --> 01:10:11.060]   and they're often wrong.
[01:10:11.060 --> 01:10:13.620]   They're often unduly negative.
[01:10:13.620 --> 01:10:16.420]   An opportunity because to the extent
[01:10:16.420 --> 01:10:18.940]   that we can tap into a more scientific
[01:10:18.940 --> 01:10:21.340]   or curious mindset,
[01:10:21.340 --> 01:10:23.900]   to the extent that we can open ourselves to the data,
[01:10:23.900 --> 01:10:26.780]   pleasant surprises are everywhere.
[01:10:26.780 --> 01:10:30.860]   The social world is full of a lot more positive
[01:10:30.860 --> 01:10:34.620]   and helpful and kind people than we realize, right?
[01:10:34.620 --> 01:10:38.660]   The average person underestimates the average person.
[01:10:38.660 --> 01:10:42.060]   This is not to say that there aren't awful,
[01:10:42.060 --> 01:10:44.340]   people who do awful things every day around the world.
[01:10:44.340 --> 01:10:45.980]   There, of course, are.
[01:10:45.980 --> 01:10:50.060]   But we take those extreme examples and over-rotate on them.
[01:10:50.060 --> 01:10:53.900]   We assume that the most toxic, awful examples
[01:10:53.900 --> 01:10:57.540]   that we see are representative when they're not.
[01:10:57.540 --> 01:10:58.980]   So we miss all these opportunities,
[01:10:58.980 --> 01:11:01.500]   but understanding that, I hope,
[01:11:01.500 --> 01:11:06.260]   opens people to gaining more of those opportunities,
[01:11:06.260 --> 01:11:09.620]   to using them, and to finding out more accurate
[01:11:09.620 --> 01:11:12.100]   and more hopeful information about each other.
[01:11:12.100 --> 01:11:13.860]   - There does seem to be a salience
[01:11:13.860 --> 01:11:18.660]   about negative interactions or somebody stealing from us
[01:11:18.660 --> 01:11:21.900]   or doing something that we consider cruel
[01:11:21.900 --> 01:11:23.580]   to us or to others.
[01:11:23.580 --> 01:11:26.380]   Nowadays, with social media,
[01:11:26.380 --> 01:11:30.020]   we get a window into, gosh,
[01:11:30.020 --> 01:11:32.980]   probably billions of social interactions
[01:11:32.980 --> 01:11:37.060]   in the form of comments and clapbacks and retweets.
[01:11:37.060 --> 01:11:40.020]   And there certainly is benevolence on social media,
[01:11:40.020 --> 01:11:45.020]   but what if any data exists about how social media
[01:11:45.020 --> 01:11:48.700]   either feeds or impedes cynicism,
[01:11:48.700 --> 01:11:51.120]   or maybe it doesn't change it at all?
[01:11:51.120 --> 01:11:54.760]   And I should say that there's also the kind of,
[01:11:54.760 --> 01:11:58.740]   I have to be careful, I'm trying not to be cynical.
[01:11:58.740 --> 01:12:03.740]   I maintain the view that certain social media platforms
[01:12:05.900 --> 01:12:10.400]   encourage a bit more negativity than others.
[01:12:10.400 --> 01:12:11.980]   And certainly there are accounts,
[01:12:11.980 --> 01:12:15.620]   I'm trying to think of accounts on Instagram like Upworthy,
[01:12:15.620 --> 01:12:20.020]   which its whole basis is to promote positive stuff.
[01:12:20.020 --> 01:12:21.580]   I like that account very much.
[01:12:21.580 --> 01:12:25.980]   But certainly you can find the full array
[01:12:25.980 --> 01:12:27.980]   of emotions on social media.
[01:12:27.980 --> 01:12:30.580]   To what extent is just being on social media,
[01:12:30.580 --> 01:12:34.500]   regardless of platform, increasing or decreasing cynicism?
[01:12:35.340 --> 01:12:38.220]   It's a terrific question.
[01:12:38.220 --> 01:12:41.060]   It's hard to provide a very clear answer,
[01:12:41.060 --> 01:12:43.020]   and I don't want to get out over my skis
[01:12:43.020 --> 01:12:45.540]   with what is known and what's not known.
[01:12:45.540 --> 01:12:49.540]   Social media has been a tectonic shift in our lives.
[01:12:49.540 --> 01:12:52.860]   It has coincided with a rise in cynicism,
[01:12:52.860 --> 01:12:55.300]   but as you know, history's not an experiment.
[01:12:55.300 --> 01:12:57.460]   So you can't take two temporal trends
[01:12:57.460 --> 01:12:58.840]   that are coincident with one another
[01:12:58.840 --> 01:13:01.020]   and say that one caused the other.
[01:13:01.020 --> 01:13:04.660]   That said, my own intuition and a lot of the data
[01:13:04.660 --> 01:13:07.260]   suggest that in at least some ways,
[01:13:07.260 --> 01:13:10.260]   social media is a cynicism factory, right?
[01:13:10.260 --> 01:13:12.100]   I mean, so let's first stipulate
[01:13:12.100 --> 01:13:13.760]   how much time we're spending on there.
[01:13:13.760 --> 01:13:17.260]   I mean, the average person goes through
[01:13:17.260 --> 01:13:21.140]   300 feet of social media feed a day.
[01:13:21.140 --> 01:13:22.020]   - Is that right? - Yeah.
[01:13:22.020 --> 01:13:23.140]   - They've measured it in feet?
[01:13:23.140 --> 01:13:25.660]   - Approximately the height of the Statue of Liberty, yeah.
[01:13:25.660 --> 01:13:28.380]   So we're doing one Statue of Liberty worth
[01:13:28.380 --> 01:13:31.460]   of scrolling a day, much of it doom scrolling,
[01:13:31.460 --> 01:13:34.540]   if you're anything like me, at least.
[01:13:34.540 --> 01:13:37.260]   And so then the question becomes,
[01:13:37.260 --> 01:13:41.640]   what are we seeing when we scroll for that long?
[01:13:41.640 --> 01:13:43.380]   Who are we seeing?
[01:13:43.380 --> 01:13:47.940]   And are they representative of what people are really like?
[01:13:47.940 --> 01:13:51.980]   And the answer in a lot of ways is no.
[01:13:51.980 --> 01:13:53.300]   That what we see on social media
[01:13:53.300 --> 01:13:56.780]   is not representative of the human population.
[01:13:56.780 --> 01:13:59.220]   So there's a lot of evidence.
[01:13:59.220 --> 01:14:02.020]   A lot of this comes from William Brady,
[01:14:02.020 --> 01:14:04.560]   now at Northwestern, and Molly Crockett,
[01:14:04.560 --> 01:14:08.780]   that when people tweet, for instance,
[01:14:08.780 --> 01:14:11.060]   I mean, a lot of this is done on the site
[01:14:11.060 --> 01:14:15.780]   formerly known as Twitter, when people tweet in outrage,
[01:14:15.780 --> 01:14:17.360]   and when they tweet negatively,
[01:14:17.360 --> 01:14:20.760]   and when they tweet about, in particular, immorality,
[01:14:20.760 --> 01:14:24.260]   right, moral outrage, that algorithmically,
[01:14:24.260 --> 01:14:28.000]   those tweets are broadcast further, they're shared more.
[01:14:29.060 --> 01:14:30.380]   And this does a couple of things.
[01:14:30.380 --> 01:14:32.620]   One, it reinforces the people
[01:14:32.620 --> 01:14:34.780]   who are already tweeting in that way.
[01:14:34.780 --> 01:14:37.580]   So William Brady has this great work
[01:14:37.580 --> 01:14:40.540]   using a kind of reinforcement learning model, right?
[01:14:40.540 --> 01:14:43.180]   Reinforcement learning is where you do something,
[01:14:43.180 --> 01:14:45.720]   you're rewarded, and that reward makes you more likely
[01:14:45.720 --> 01:14:47.580]   to do that same thing again.
[01:14:47.580 --> 01:14:51.500]   And it turns out that Brady found
[01:14:51.500 --> 01:14:56.100]   that when people tweet in outrage and then get egged on,
[01:14:56.100 --> 01:14:58.700]   and oftentimes I should say this is tribal in nature,
[01:14:58.700 --> 01:15:01.900]   it's somebody tweeting against somebody who's an outsider,
[01:15:01.900 --> 01:15:03.580]   and then being rewarded by people
[01:15:03.580 --> 01:15:06.320]   who they consider to be part of their group, right?
[01:15:06.320 --> 01:15:08.900]   When that happens, that person is more likely
[01:15:08.900 --> 01:15:12.200]   in their future tweets to turn up the volume
[01:15:12.200 --> 01:15:15.440]   on that outrage and on that moral outrage in particular.
[01:15:15.440 --> 01:15:18.540]   So there's a sort of ratchet effect, right,
[01:15:18.540 --> 01:15:20.520]   on the people who are sharing.
[01:15:20.520 --> 01:15:22.780]   But a second question becomes,
[01:15:22.780 --> 01:15:24.700]   well, what about the people watching?
[01:15:24.700 --> 01:15:26.780]   What about the rest of us?
[01:15:26.780 --> 01:15:28.840]   Claire Robertson has a great paper on this
[01:15:28.840 --> 01:15:31.940]   where she documents that a vast majority,
[01:15:31.940 --> 01:15:35.500]   I mean, 90 plus percent of tweets are created
[01:15:35.500 --> 01:15:40.060]   by the 10% of the most active users, right?
[01:15:40.060 --> 01:15:41.980]   And this is in the political sphere.
[01:15:41.980 --> 01:15:45.540]   And these are probably not representative, these folks,
[01:15:45.540 --> 01:15:47.500]   not representative of the rest of us in terms
[01:15:47.500 --> 01:15:52.500]   of how extreme and maybe how bitter their opinions are.
[01:15:52.500 --> 01:15:55.580]   And so we, when we're scrolling,
[01:15:55.580 --> 01:15:58.340]   that Statue of Liberty's worth of information,
[01:15:58.340 --> 01:16:00.980]   we think that we're seeing the world.
[01:16:00.980 --> 01:16:02.900]   We think that we're seeing our fellow citizens.
[01:16:02.900 --> 01:16:04.620]   We think that we're getting a picture
[01:16:04.620 --> 01:16:06.060]   of what people are like.
[01:16:06.060 --> 01:16:08.460]   In fact, we're pulling from the fringes.
[01:16:08.460 --> 01:16:12.500]   And what this leads to is a misconstrual
[01:16:12.500 --> 01:16:14.880]   of what the world is really like.
[01:16:14.880 --> 01:16:17.700]   This is, by the way, not just part of social media,
[01:16:17.700 --> 01:16:19.860]   it's also part of legacy media.
[01:16:19.860 --> 01:16:22.940]   Communication theorists talk about something called
[01:16:22.940 --> 01:16:25.500]   the mean world syndrome, right?
[01:16:25.500 --> 01:16:28.260]   Where the more time that you spend looking at the news,
[01:16:28.260 --> 01:16:31.380]   for instance, the more you think violent crime
[01:16:31.380 --> 01:16:33.020]   is up in your area,
[01:16:33.020 --> 01:16:36.340]   the more you think you're in danger of violent crime,
[01:16:36.340 --> 01:16:39.820]   even during years when violent crime is decreasing.
[01:16:39.820 --> 01:16:43.260]   I'm old enough to remember when "Stranger Danger"
[01:16:43.260 --> 01:16:45.340]   was this big, massive story.
[01:16:45.340 --> 01:16:47.100]   And every time you wanted cereal,
[01:16:47.100 --> 01:16:48.860]   the milk carton would have a picture
[01:16:48.860 --> 01:16:51.540]   of a kid who had been kidnapped by a stranger.
[01:16:51.540 --> 01:16:53.580]   And during that time, if you asked people
[01:16:53.580 --> 01:16:57.460]   how many kids are being kidnapped by strangers in the US,
[01:16:57.460 --> 01:17:00.100]   they would, in many cases, say 50,000 children
[01:17:00.100 --> 01:17:02.100]   are being kidnapped each year in the US.
[01:17:02.100 --> 01:17:05.580]   Can you imagine what the world would be?
[01:17:05.580 --> 01:17:08.020]   There would be SWAT teams on every corner.
[01:17:08.020 --> 01:17:09.640]   The real number in those years
[01:17:09.640 --> 01:17:12.260]   was closer to 100 kids per year.
[01:17:12.260 --> 01:17:14.220]   Now, let me be clear, each one of those
[01:17:14.220 --> 01:17:17.940]   is an absolute tragedy, but there's a big difference here.
[01:17:17.940 --> 01:17:21.140]   And oftentimes, when we tune into media,
[01:17:21.140 --> 01:17:24.620]   we end up with these enormously warped perceptions
[01:17:24.620 --> 01:17:26.960]   where we think that the world is much more dangerous
[01:17:26.960 --> 01:17:27.960]   than it really is.
[01:17:27.960 --> 01:17:30.380]   We think that people are much more extreme
[01:17:30.380 --> 01:17:31.220]   than they really are.
[01:17:31.220 --> 01:17:34.820]   And because stories of immorality go viral
[01:17:34.820 --> 01:17:38.260]   so much more often than stories of everyday goodness,
[01:17:38.260 --> 01:17:39.920]   I mean, I love "Upworthy" as well,
[01:17:39.920 --> 01:17:44.780]   but it's not winning right now in the social media wars.
[01:17:44.780 --> 01:17:45.620]   - Not yet.
[01:17:45.620 --> 01:17:47.060]   - Not yet, not yet.
[01:17:47.060 --> 01:17:51.060]   And so this leaves us all absolutely exhausted
[01:17:51.060 --> 01:17:52.740]   and also feeling alone.
[01:17:52.740 --> 01:17:54.220]   People who feel like, wow,
[01:17:54.220 --> 01:17:57.540]   I actually don't feel that much outrage
[01:17:57.540 --> 01:17:59.380]   or I don't want to feel that much outrage.
[01:17:59.380 --> 01:18:02.500]   I actually don't want to hate everybody
[01:18:02.500 --> 01:18:04.400]   who's different from me, for instance.
[01:18:04.400 --> 01:18:07.380]   I'm just exhausted by all this.
[01:18:07.380 --> 01:18:09.340]   We feel like, well, I guess I'm the only one
[01:18:09.340 --> 01:18:11.180]   because everybody else seems really excited
[01:18:11.180 --> 01:18:14.540]   about this battle royale that we've put ourselves in.
[01:18:14.540 --> 01:18:17.500]   But in fact, most people are just
[01:18:17.500 --> 01:18:19.300]   like the exhausted majority, right?
[01:18:19.300 --> 01:18:22.980]   We're paying so much attention to a tiny minority
[01:18:22.980 --> 01:18:25.820]   of what the journalist, Amanda Ripley,
[01:18:25.820 --> 01:18:28.500]   calls conflict entrepreneurs,
[01:18:28.500 --> 01:18:30.780]   people who stoke conflict on purpose
[01:18:30.780 --> 01:18:32.980]   that we're confusing them with the average.
[01:18:32.980 --> 01:18:36.140]   - Ooh, so much there.
[01:18:36.140 --> 01:18:41.300]   I have, I suppose, a mixed relationship to social media.
[01:18:41.300 --> 01:18:43.340]   I teach there and I learn there.
[01:18:43.340 --> 01:18:45.980]   And I also have to be very discerning
[01:18:45.980 --> 01:18:47.460]   in terms of how I interact with it.
[01:18:47.460 --> 01:18:49.860]   And you made this point
[01:18:49.860 --> 01:18:52.060]   that I've never heard anyone make before,
[01:18:52.060 --> 01:18:54.500]   which is that many people feel alone
[01:18:54.500 --> 01:18:56.260]   by virtue of the fact that they don't share
[01:18:56.260 --> 01:19:00.900]   in this warring nature that they see on social media.
[01:19:00.900 --> 01:19:03.260]   It's almost like sometimes I feel
[01:19:03.260 --> 01:19:04.940]   like I'm watching a combat sport
[01:19:04.940 --> 01:19:07.880]   that I don't feel quite cut out for.
[01:19:07.880 --> 01:19:08.720]   - Yeah.
[01:19:08.720 --> 01:19:12.860]   - And then when I'm away from it, I feel better.
[01:19:12.860 --> 01:19:14.100]   But I, like everybody else,
[01:19:14.100 --> 01:19:18.420]   sometimes we'll get sucked into the highly salient nature
[01:19:18.420 --> 01:19:22.620]   of a combat between groups on social media.
[01:19:22.620 --> 01:19:27.120]   It can be very alluring in the worst ways.
[01:19:27.120 --> 01:19:33.220]   This mean world syndrome, what's the inverse of that?
[01:19:33.220 --> 01:19:35.500]   The kind world syndrome, I suppose.
[01:19:35.500 --> 01:19:37.880]   But attempts at creating those sorts
[01:19:37.880 --> 01:19:41.020]   of social media platforms have been made.
[01:19:41.020 --> 01:19:44.100]   Things like Blue Sky, which has other aspects to it as well.
[01:19:44.100 --> 01:19:46.380]   But, and while it may be thriving,
[01:19:46.380 --> 01:19:50.500]   I don't know, I haven't checked recently.
[01:19:50.500 --> 01:19:52.940]   It seems like people aren't really interested
[01:19:52.940 --> 01:19:55.460]   in being on there as much as they are these other platforms.
[01:19:55.460 --> 01:19:57.980]   Clearly the numbers play out that way.
[01:19:57.980 --> 01:20:00.260]   Why do you think that is?
[01:20:00.260 --> 01:20:03.700]   - Well, we as a species, I think,
[01:20:03.700 --> 01:20:08.180]   are characterized by what we would call negativity bias.
[01:20:08.180 --> 01:20:12.420]   Right, negative events and threats loom larger in our minds.
[01:20:12.420 --> 01:20:15.460]   And that happens in a number of domains.
[01:20:15.460 --> 01:20:19.500]   Our decision making is negatively biased
[01:20:19.500 --> 01:20:22.260]   in that we'd prefer to avoid a negative outcome
[01:20:22.260 --> 01:20:23.700]   than to pursue a positive outcome.
[01:20:23.700 --> 01:20:26.060]   That's the classic work of Kahneman and Tversky,
[01:20:26.060 --> 01:20:27.380]   for instance.
[01:20:27.380 --> 01:20:31.460]   The impressions that we form are often negatively skewed.
[01:20:31.460 --> 01:20:35.380]   So classic work in psychology going back to the 1950s
[01:20:35.380 --> 01:20:39.220]   shows that if you teach somebody about a new person
[01:20:39.220 --> 01:20:42.060]   who they've never met and you list three positive qualities
[01:20:42.060 --> 01:20:45.060]   that this person has and three negative qualities,
[01:20:45.060 --> 01:20:48.500]   people will very much judge the person
[01:20:48.500 --> 01:20:50.220]   on their worst qualities.
[01:20:50.220 --> 01:20:53.420]   And also remember more about their negative qualities
[01:20:53.420 --> 01:20:55.620]   than about their positive qualities.
[01:20:55.620 --> 01:20:59.740]   And again, you can see why this would be part of who we are
[01:20:59.740 --> 01:21:01.700]   because we need to protect one another.
[01:21:01.700 --> 01:21:04.300]   We also tend to, by the way, not just think
[01:21:04.300 --> 01:21:05.580]   in a negatively biased way,
[01:21:05.580 --> 01:21:09.540]   but speak and share in a negatively biased way.
[01:21:09.540 --> 01:21:12.780]   In my lab, we had a study where people witnessed
[01:21:12.780 --> 01:21:15.460]   other groups of four playing an economic game
[01:21:15.460 --> 01:21:20.460]   where they could be selfish or they could be positive.
[01:21:20.460 --> 01:21:24.380]   And we asked them, okay, we're gonna ask you to share
[01:21:24.380 --> 01:21:26.100]   a piece of information about one of the people
[01:21:26.100 --> 01:21:27.700]   you were playing this game with
[01:21:27.700 --> 01:21:31.500]   for a future generation of participants.
[01:21:31.500 --> 01:21:33.140]   Who would you like to share about?
[01:21:33.140 --> 01:21:36.500]   And when somebody in a group acted in a selfish way,
[01:21:36.500 --> 01:21:41.660]   people shared information about them three times more often
[01:21:41.660 --> 01:21:44.420]   than when they acted in a generous way.
[01:21:44.420 --> 01:21:46.780]   So we gossip negatively.
[01:21:46.780 --> 01:21:49.940]   And again, that gossip is pro-social.
[01:21:49.940 --> 01:21:51.860]   The idea is if there's somebody out there
[01:21:51.860 --> 01:21:53.780]   harming my community, of course,
[01:21:53.780 --> 01:21:55.780]   I'm gonna shout about them from the rooftops
[01:21:55.780 --> 01:21:57.860]   because I wanna protect my friends.
[01:21:57.860 --> 01:22:01.420]   It's a very noble instinct in a way.
[01:22:01.420 --> 01:22:03.740]   But we further found that when we actually showed
[01:22:03.740 --> 01:22:05.460]   a new generation of participants
[01:22:05.460 --> 01:22:07.820]   the gossip that the first generation shared,
[01:22:07.820 --> 01:22:10.260]   and we asked, hey, how generous and how selfish
[01:22:10.260 --> 01:22:11.940]   were people in that first generation,
[01:22:11.940 --> 01:22:15.580]   they vastly underestimated that group's generosity.
[01:22:15.580 --> 01:22:16.740]   Does that make sense?
[01:22:16.740 --> 01:22:20.180]   In other words, in trying to protect our communities,
[01:22:20.180 --> 01:22:23.420]   we send highly biased information
[01:22:23.420 --> 01:22:24.860]   about who's in our community
[01:22:24.860 --> 01:22:29.180]   and give other people the wrong idea of who we are.
[01:22:29.180 --> 01:22:32.260]   And I see that unfolding on social media
[01:22:32.260 --> 01:22:33.260]   every day of my life.
[01:22:33.260 --> 01:22:34.620]   Every day that I'm on social media,
[01:22:34.620 --> 01:22:36.460]   I do try to take breaks.
[01:22:36.460 --> 01:22:38.340]   But when I'm on there, I see it.
[01:22:38.340 --> 01:22:42.300]   And to your question, what do we do here?
[01:22:42.300 --> 01:22:45.420]   Why don't positive networks, positive information,
[01:22:45.420 --> 01:22:48.460]   why doesn't it proliferate more?
[01:22:48.460 --> 01:22:52.060]   I think it's because of these ingrained biases in our mind.
[01:22:52.060 --> 01:22:54.900]   And I understand that that can sound fatalistic
[01:22:54.900 --> 01:22:57.740]   because it's like, oh, maybe this is just who we are.
[01:22:57.740 --> 01:23:01.260]   But I don't think that we generally accept our instincts
[01:23:01.260 --> 01:23:06.260]   and biases as a life sentence, as destiny.
[01:23:06.260 --> 01:23:09.780]   A lot of us, well, human beings in general,
[01:23:09.780 --> 01:23:12.380]   have the instinct to trust and be kinder
[01:23:12.380 --> 01:23:15.340]   towards people who look like us versus people who don't,
[01:23:15.340 --> 01:23:17.860]   for instance, who share our racial makeup.
[01:23:17.860 --> 01:23:20.860]   None of us, I think, or a few of us sit here and say,
[01:23:20.860 --> 01:23:22.500]   well, I have that bias in my mind,
[01:23:22.500 --> 01:23:26.580]   so I guess I'm always going to be racially biased.
[01:23:26.580 --> 01:23:29.780]   We try to counteract those instincts.
[01:23:29.780 --> 01:23:33.060]   We try to become aware of those biases.
[01:23:33.060 --> 01:23:35.540]   Depressed people have the bias
[01:23:35.540 --> 01:23:37.180]   to see themselves as worthless
[01:23:37.180 --> 01:23:39.620]   and to interpret new information they receive
[01:23:39.620 --> 01:23:41.100]   through that framework.
[01:23:41.100 --> 01:23:43.340]   Well, therapy is the attempt to say,
[01:23:43.340 --> 01:23:45.180]   I don't want to feel this way anymore.
[01:23:45.180 --> 01:23:48.820]   I want to fight the default settings in my mind.
[01:23:48.820 --> 01:23:51.380]   I want to try to explore curiosity,
[01:23:51.380 --> 01:23:53.460]   to explore something new.
[01:23:53.460 --> 01:23:56.620]   So to say that this toxic environment that we're in
[01:23:56.620 --> 01:23:59.060]   corresponds with some of our biases
[01:23:59.060 --> 01:24:00.500]   is, to me, not the same as saying
[01:24:00.500 --> 01:24:03.140]   we are destined to remain in that situation.
[01:24:03.140 --> 01:24:04.580]   - Do you think it's possible
[01:24:04.580 --> 01:24:07.980]   to be adequately informed about threats
[01:24:07.980 --> 01:24:13.260]   to be able to live one's life in the most adaptive way
[01:24:13.260 --> 01:24:16.740]   while not being on social media,
[01:24:16.740 --> 01:24:18.540]   none of the social media platforms?
[01:24:18.540 --> 01:24:22.580]   Can you have a great life that way,
[01:24:22.580 --> 01:24:24.100]   a safe life?
[01:24:24.100 --> 01:24:26.460]   - This is a quasi-philosophical question,
[01:24:26.460 --> 01:24:29.340]   but from my perspective, absolutely.
[01:24:29.340 --> 01:24:30.820]   I mean, I think some of the threats
[01:24:30.820 --> 01:24:34.780]   that we learn about on social media are simply wrong.
[01:24:34.780 --> 01:24:37.300]   They're phantom threats.
[01:24:37.300 --> 01:24:42.260]   We're made to fear something that actually is not happening,
[01:24:42.260 --> 01:24:47.140]   made to fear a group of people who are not as dangerous
[01:24:47.140 --> 01:24:50.140]   as they're made out to be on social media.
[01:24:50.140 --> 01:24:53.380]   Of course, I think being informed about the world around us
[01:24:53.380 --> 01:24:55.380]   matters to staying safe.
[01:24:55.380 --> 01:24:57.740]   But again, I think we can also more broadly
[01:24:57.740 --> 01:25:00.100]   construe what safety is.
[01:25:00.100 --> 01:25:01.940]   You know, if being on social media
[01:25:01.940 --> 01:25:05.580]   makes you avoidant of taking chances on people,
[01:25:05.580 --> 01:25:07.020]   if it makes you feel as though
[01:25:07.020 --> 01:25:09.340]   anybody who's different from you ideologically,
[01:25:09.340 --> 01:25:13.180]   for instance, is bloodthirsty and extreme,
[01:25:13.180 --> 01:25:17.500]   that's going to limit your life in very important ways.
[01:25:17.500 --> 01:25:19.220]   And you can talk about being safe
[01:25:19.220 --> 01:25:22.380]   in terms of safe from acute threats.
[01:25:22.380 --> 01:25:23.820]   But as we've talked about,
[01:25:23.820 --> 01:25:26.460]   living a diminished and disconnected life
[01:25:26.460 --> 01:25:30.340]   is its own form of danger over a longer time horizon.
[01:25:30.340 --> 01:25:33.580]   So really, you know, there are a lot of ways
[01:25:33.580 --> 01:25:37.860]   in which in the attempt to stay safe right now,
[01:25:37.860 --> 01:25:40.300]   we introduce ourselves to long-term danger.
[01:25:40.300 --> 01:25:43.660]   - I'm not anti-social media,
[01:25:43.660 --> 01:25:48.460]   but I have to circle back on this yet again.
[01:25:48.460 --> 01:25:49.860]   Former guest on this podcast,
[01:25:49.860 --> 01:25:51.260]   one of our most popular episodes
[01:25:51.260 --> 01:25:53.460]   is with a former Navy SEAL, David Goggins,
[01:25:53.460 --> 01:25:56.460]   who's known for many things,
[01:25:56.460 --> 01:26:01.460]   but chief among them is striving and pushing oneself.
[01:26:01.460 --> 01:26:05.780]   And David has said many times that nowadays
[01:26:05.780 --> 01:26:07.780]   it's easier than ever to be extraordinary
[01:26:07.780 --> 01:26:09.580]   because most people are basically spending time
[01:26:09.580 --> 01:26:13.220]   just consuming experiences on social media
[01:26:13.220 --> 01:26:17.300]   and doing a lot less, just literally doing a lot less,
[01:26:17.300 --> 01:26:19.660]   not just exercising and running as he does.
[01:26:19.660 --> 01:26:22.140]   Although, by the way, he's in school to become a paramedic.
[01:26:22.140 --> 01:26:24.260]   So he's essentially gone to medical school
[01:26:24.260 --> 01:26:27.500]   and is always doing a bunch of other things as well.
[01:26:27.500 --> 01:26:31.260]   So he's also an intellectual learner.
[01:26:31.260 --> 01:26:34.780]   Now, I don't know if I agree with him completely,
[01:26:34.780 --> 01:26:37.820]   but it's an interesting statement.
[01:26:37.820 --> 01:26:42.820]   You know, if social media is bringing out our cynicism,
[01:26:43.040 --> 01:26:47.100]   polarizing us, and perhaps taking away,
[01:26:47.100 --> 01:26:49.280]   I would probably agree with David,
[01:26:49.280 --> 01:26:54.080]   at least to some extent, taking away our time
[01:26:54.080 --> 01:26:58.600]   where we could be generative, writing, thinking,
[01:26:58.600 --> 01:27:01.120]   socializing, building in other ways
[01:27:01.120 --> 01:27:04.280]   that one builds their life,
[01:27:04.280 --> 01:27:07.080]   then I guess an important question is,
[01:27:07.080 --> 01:27:09.360]   do you think social media could be leveraged
[01:27:09.360 --> 01:27:14.360]   to decrease cynicism or, as you referred to it,
[01:27:14.360 --> 01:27:17.700]   to generate hopeful skepticism?
[01:27:17.700 --> 01:27:20.500]   Like this notion of hopeful skepticism
[01:27:20.500 --> 01:27:21.960]   as a replacement for cynicism
[01:27:21.960 --> 01:27:23.900]   is something that is really intriguing.
[01:27:23.900 --> 01:27:24.960]   Like, what would that look like?
[01:27:24.960 --> 01:27:28.160]   Like, if we were just gonna do the Gedanken experiment here,
[01:27:28.160 --> 01:27:30.760]   like, what would a feed on social media look like
[01:27:30.760 --> 01:27:35.760]   that fed hopeful skepticism as opposed to cynicism?
[01:27:35.760 --> 01:27:37.720]   - Here's a far out example.
[01:27:37.720 --> 01:27:39.600]   I mean, I love this train of thought,
[01:27:39.600 --> 01:27:42.720]   so I'm gonna try to take it to a logical conclusion
[01:27:42.720 --> 01:27:45.080]   that would never actually occur in real life,
[01:27:45.080 --> 01:27:49.840]   but a great way to generate more accurate
[01:27:49.840 --> 01:27:52.400]   and hopeful skepticism, and by hopeful skepticism,
[01:27:52.400 --> 01:27:54.780]   I mean skepticism as we've described,
[01:27:54.780 --> 01:27:57.640]   a scientific mindset, a scientific perspective,
[01:27:57.640 --> 01:28:00.960]   and a curiosity, a hunger for information.
[01:28:00.960 --> 01:28:04.660]   And in the hopeful piece, I simply mean skepticism
[01:28:04.660 --> 01:28:06.460]   that begins with the understanding
[01:28:06.460 --> 01:28:09.640]   that our defaults are often too negative,
[01:28:09.640 --> 01:28:12.300]   so that I'm going to be open and I'm going to realize
[01:28:12.300 --> 01:28:16.460]   that my gut instinct is probably leading me
[01:28:16.460 --> 01:28:18.440]   towards the negative and can be challenged,
[01:28:18.440 --> 01:28:20.480]   that I don't have to listen to it all the time.
[01:28:20.480 --> 01:28:22.780]   So just as a working definition,
[01:28:22.780 --> 01:28:28.600]   I think that what I would want in a social media feed
[01:28:28.600 --> 01:28:31.040]   would be for it to have more data.
[01:28:31.040 --> 01:28:35.240]   If you could compel every person on Earth
[01:28:35.240 --> 01:28:39.020]   to post to social media about what they're doing today,
[01:28:39.020 --> 01:28:41.340]   about what they're thinking, about what they want,
[01:28:41.340 --> 01:28:43.420]   about their values, right?
[01:28:43.420 --> 01:28:45.420]   If you could compel each, of course,
[01:28:45.420 --> 01:28:46.620]   that's dystopic in many ways,
[01:28:46.620 --> 01:28:48.560]   but just as a thought experiment.
[01:28:48.560 --> 01:28:52.820]   And then people's feed was a representative sample
[01:28:52.820 --> 01:28:55.720]   of real people on the planet, right?
[01:28:55.720 --> 01:29:00.520]   Real people, and people who over time, right,
[01:29:00.520 --> 01:29:02.940]   as I scroll through my Statue of Liberty now,
[01:29:02.940 --> 01:29:05.140]   I see what people are really like.
[01:29:05.140 --> 01:29:08.340]   I see the people who are extreme and negative and toxic,
[01:29:08.340 --> 01:29:10.820]   but I also see a grandmother
[01:29:10.820 --> 01:29:14.080]   who's driving her grandkid to hockey practice.
[01:29:14.080 --> 01:29:18.820]   I see a nurse who's coming in to help an elderly patient.
[01:29:18.820 --> 01:29:22.380]   I see somebody who's made an unlikely connection
[01:29:22.380 --> 01:29:24.500]   with somebody who they disagree with.
[01:29:24.500 --> 01:29:28.540]   A veridical, accurate feed,
[01:29:28.540 --> 01:29:30.460]   I think would drive hopeful skepticism.
[01:29:30.460 --> 01:29:32.340]   And that's, again, one of the things
[01:29:32.340 --> 01:29:34.880]   that has struck me most over the last few years
[01:29:34.880 --> 01:29:36.460]   of doing this research,
[01:29:36.460 --> 01:29:40.340]   is that we stereotype hope and positivity,
[01:29:40.340 --> 01:29:42.100]   as you were saying earlier,
[01:29:42.100 --> 01:29:47.100]   as kind of dim, naive, a rose-colored pair of glasses.
[01:29:47.100 --> 01:29:50.900]   But in fact, I think what the data show us
[01:29:50.900 --> 01:29:52.900]   is that we're all wearing a pair
[01:29:52.900 --> 01:29:55.620]   of soot-colored glasses all the time.
[01:29:55.620 --> 01:29:58.540]   And actually the best way to make people more hopeful
[01:29:58.540 --> 01:30:00.980]   is to ask them to look more carefully,
[01:30:00.980 --> 01:30:04.240]   not to look away, but look towards
[01:30:04.240 --> 01:30:06.840]   in a more accurate and open fashion.
[01:30:06.840 --> 01:30:08.720]   And there's one version of this
[01:30:08.720 --> 01:30:12.760]   that we've tried at Stanford, in our own backyard.
[01:30:12.760 --> 01:30:15.120]   So my lab and I, we've, for years,
[01:30:15.120 --> 01:30:19.360]   been surveying as many Stanford undergraduates as we can
[01:30:19.360 --> 01:30:21.280]   about their social health, right?
[01:30:21.280 --> 01:30:23.280]   So how connected are they?
[01:30:23.280 --> 01:30:24.840]   How mentally healthy are they?
[01:30:24.840 --> 01:30:28.240]   And a couple of years ago,
[01:30:28.240 --> 01:30:32.040]   we asked thousands of undergraduates to describe
[01:30:32.040 --> 01:30:34.840]   both themselves and the average Stanford student
[01:30:34.840 --> 01:30:36.400]   on a number of dimensions.
[01:30:36.400 --> 01:30:37.960]   For instance, how empathic are you?
[01:30:37.960 --> 01:30:40.280]   How empathic is the average Stanford student?
[01:30:40.280 --> 01:30:42.880]   How much do you like helping people who are struggling?
[01:30:42.880 --> 01:30:44.320]   What do you think the average Stanford student
[01:30:44.320 --> 01:30:45.480]   would respond to that?
[01:30:45.480 --> 01:30:47.640]   How much do you want to meet new people on campus?
[01:30:47.640 --> 01:30:50.080]   How do you think the average student would respond?
[01:30:50.080 --> 01:30:54.160]   And we discovered not one, but two Stanfords.
[01:30:54.160 --> 01:30:56.040]   The first was made up of real students
[01:30:56.040 --> 01:30:58.220]   who are enormously compassionate,
[01:30:58.220 --> 01:31:01.160]   who really want to meet new friends,
[01:31:01.160 --> 01:31:04.340]   who want to help their friends when they're struggling.
[01:31:04.340 --> 01:31:07.840]   The second Stanford existed in students' minds.
[01:31:07.840 --> 01:31:10.980]   Their imagination of the average undergraduate
[01:31:10.980 --> 01:31:14.320]   was much less friendly, much less compassionate,
[01:31:14.320 --> 01:31:18.800]   much pricklier and more judgmental than real students were.
[01:31:18.800 --> 01:31:20.920]   So again, we've got this discrepancy
[01:31:20.920 --> 01:31:24.600]   between what people perceive and social reality.
[01:31:24.600 --> 01:31:28.160]   We found that students who underestimated their peers
[01:31:28.160 --> 01:31:30.120]   were less willing to do things like
[01:31:30.120 --> 01:31:32.480]   strike up a conversation with a stranger
[01:31:32.480 --> 01:31:34.920]   or confide in a friend when they were struggling.
[01:31:34.920 --> 01:31:37.920]   And that left them more isolated and lonelier.
[01:31:37.920 --> 01:31:41.440]   This is the kind of vicious cycle of cynicism, right?
[01:31:41.440 --> 01:31:46.240]   But more recently, my lab led by a great postdoc, Ray Pei,
[01:31:46.240 --> 01:31:48.120]   tried an intervention.
[01:31:48.120 --> 01:31:50.600]   And the intervention was as simple as you can imagine.
[01:31:50.600 --> 01:31:52.760]   It was show students the real data.
[01:31:53.640 --> 01:31:56.080]   We put posters in a number of dorms,
[01:31:56.080 --> 01:31:58.160]   experimental dorms we called them,
[01:31:58.160 --> 01:32:00.400]   that simply said, hey, did you know,
[01:32:00.400 --> 01:32:04.020]   95% of students at Stanford
[01:32:04.020 --> 01:32:06.000]   would like to help their friends who are struggling.
[01:32:06.000 --> 01:32:09.560]   85% want to make friends with new students.
[01:32:09.560 --> 01:32:12.360]   We also worked with Frosh 101,
[01:32:12.360 --> 01:32:15.640]   a one-unit class that most first-year students take
[01:32:15.640 --> 01:32:17.520]   and show them the data.
[01:32:17.520 --> 01:32:20.440]   We're just showing students to each other.
[01:32:20.440 --> 01:32:23.960]   And we found that when students learned this information,
[01:32:23.960 --> 01:32:26.800]   they were more willing to take social risks.
[01:32:26.800 --> 01:32:28.160]   And six months later,
[01:32:28.160 --> 01:32:30.880]   they were more likely to have a greater number of friends,
[01:32:30.880 --> 01:32:32.800]   to be more socially integrated.
[01:32:32.800 --> 01:32:36.800]   So here again is a tragic and vicious cycle,
[01:32:36.800 --> 01:32:39.460]   but then there's a virtuous cycle that can replace it
[01:32:39.460 --> 01:32:42.320]   if we just show people better information.
[01:32:42.320 --> 01:32:44.440]   Again, I don't imagine that there'll ever be
[01:32:44.440 --> 01:32:47.520]   a social media feed where everybody has to post
[01:32:47.520 --> 01:32:50.040]   and you see an actually representative sample of the world.
[01:32:50.040 --> 01:32:53.680]   But if we could, I do think that that would generate
[01:32:53.680 --> 01:32:57.040]   a more hopeful perspective because the truth
[01:32:57.040 --> 01:32:59.040]   is more hopeful than what we're seeing.
[01:32:59.040 --> 01:33:02.080]   - Do you think there's a version of AI
[01:33:02.080 --> 01:33:06.840]   that is less cynical than people tend to be?
[01:33:06.840 --> 01:33:10.240]   The reason I ask this is I'm quite excited about
[01:33:10.240 --> 01:33:11.920]   and hopeful about AI.
[01:33:11.920 --> 01:33:14.360]   I'm not one of these, I don't know what you call them,
[01:33:14.360 --> 01:33:17.300]   but AI doomers. - Doomers, yeah.
[01:33:17.300 --> 01:33:18.520]   - And it's here, it's happening.
[01:33:18.520 --> 01:33:19.800]   It's happening in the background now.
[01:33:19.800 --> 01:33:23.100]   And I've started using AI in a number of different
[01:33:23.100 --> 01:33:25.820]   realms of life and I find it to be incredible.
[01:33:25.820 --> 01:33:29.240]   It seems to me to combine neural networks
[01:33:29.240 --> 01:33:33.840]   and Google search with PubMed and it's fascinating.
[01:33:33.840 --> 01:33:36.080]   It's not perfect, it's far from perfect,
[01:33:36.080 --> 01:33:37.700]   but that's also part of its beauty
[01:33:37.700 --> 01:33:42.320]   is that it mimics a human lack of perfectness well enough
[01:33:42.320 --> 01:33:45.720]   that it feels something kind of like
[01:33:45.720 --> 01:33:47.720]   brain-like, personality-like.
[01:33:49.600 --> 01:33:53.440]   You could imagine that given the enormous amount
[01:33:53.440 --> 01:33:56.040]   of cynicism that's out there,
[01:33:56.040 --> 01:33:59.720]   that some of the large language models that make up AI
[01:33:59.720 --> 01:34:04.560]   would be somewhat cynical, would put filters
[01:34:04.560 --> 01:34:07.640]   that were overly stringent on certain topics.
[01:34:07.640 --> 01:34:11.960]   You also wouldn't want AI that was not stringent enough,
[01:34:11.960 --> 01:34:16.720]   right, because we are already and soon to be using AI
[01:34:16.720 --> 01:34:19.480]   to bring us information extremely quickly
[01:34:19.480 --> 01:34:22.280]   and the last thing we want are errors in that information.
[01:34:22.280 --> 01:34:26.560]   So if we were to take what we know from humans
[01:34:26.560 --> 01:34:28.080]   and the data that you've collected
[01:34:28.080 --> 01:34:31.240]   and others have collected about ways to shift ourselves
[01:34:31.240 --> 01:34:34.040]   from cynicism to hopeful skepticism,
[01:34:34.040 --> 01:34:36.560]   do you think that's something that could be laced
[01:34:36.560 --> 01:34:38.460]   into these large language models?
[01:34:38.460 --> 01:34:40.320]   I'm not talking about at the technical level,
[01:34:40.320 --> 01:34:43.280]   that's certainly beyond my understanding,
[01:34:43.280 --> 01:34:46.160]   but could you build an AI version of yourself
[01:34:46.160 --> 01:34:48.120]   that could forage the internet for news
[01:34:48.120 --> 01:34:50.320]   and what's going on out there that is,
[01:34:50.320 --> 01:34:54.380]   you know, tune down the cynicism a little bit
[01:34:54.380 --> 01:34:56.840]   since it's difficult to be less cynical?
[01:34:56.840 --> 01:34:58.640]   In other words, could it do a better job
[01:34:58.640 --> 01:35:02.200]   of being you than you and then therefore make you better?
[01:35:02.200 --> 01:35:06.120]   - Wow, I love that question.
[01:35:06.120 --> 01:35:09.880]   I think that there is, I could imagine
[01:35:09.880 --> 01:35:11.520]   an opportunity for that.
[01:35:11.520 --> 01:35:16.080]   I think one roadblock that I don't think is insurmountable
[01:35:16.080 --> 01:35:18.040]   but that you would need to face
[01:35:18.040 --> 01:35:22.800]   in that really fascinating goal is that AI models
[01:35:22.800 --> 01:35:25.960]   are, of course, products of the data that we feed them.
[01:35:25.960 --> 01:35:29.960]   And so if, you know, basically AI models eat the internet,
[01:35:29.960 --> 01:35:33.080]   right, swallow it, and then give it back to us in some form,
[01:35:33.080 --> 01:35:38.040]   to the extent that the internet is asymmetrically waiting,
[01:35:38.040 --> 01:35:43.040]   right, is overweighting negative content and cynical content,
[01:35:43.640 --> 01:35:48.200]   then AIs that swallow that will reflect it as well.
[01:35:48.200 --> 01:35:50.040]   I think that, and I could imagine,
[01:35:50.040 --> 01:35:53.320]   and it's blowing my mind in real time to think about,
[01:35:53.320 --> 01:35:58.280]   but you could imagine retuning the way
[01:35:58.280 --> 01:36:01.900]   that AI takes information to account for negativity bias
[01:36:01.900 --> 01:36:04.560]   and to correct, I mean, this is what you're getting at,
[01:36:04.560 --> 01:36:07.600]   I think, right, to correct for that negativity bias
[01:36:07.600 --> 01:36:12.600]   and then produce an inference that is less biased,
[01:36:12.840 --> 01:36:15.780]   more accurate, and less cynical,
[01:36:15.780 --> 01:36:19.040]   and then give that as a kind of digest to people, right?
[01:36:19.040 --> 01:36:23.080]   So don't make me go through my social media feed.
[01:36:23.080 --> 01:36:27.520]   Go through it for me, correct, right, de-bias it,
[01:36:27.520 --> 01:36:32.000]   and then give it to me in a more accurate way.
[01:36:32.000 --> 01:36:34.360]   That's an incredible idea.
[01:36:34.360 --> 01:36:36.280]   - I mean, that's what I want.
[01:36:36.280 --> 01:36:38.400]   I was thinking about my Instagram feed
[01:36:38.400 --> 01:36:41.880]   and cynicism versus hopeful skepticism
[01:36:41.880 --> 01:36:46.140]   versus, I guess, awe, and I'll use the following examples.
[01:36:46.140 --> 01:36:51.400]   I subscribed to an Instagram account that I like very much,
[01:36:51.400 --> 01:36:53.680]   which essentially just gives me images
[01:36:53.680 --> 01:36:58.680]   of beautiful animals in their ultimate essence.
[01:36:58.680 --> 01:37:01.800]   It's an account by a guy named Joel Sartore
[01:37:01.800 --> 01:37:03.240]   who works for National Geographic,
[01:37:03.240 --> 01:37:04.920]   and he's created what's called the photo arc.
[01:37:04.920 --> 01:37:09.000]   He's trying to get images of all the world's animals
[01:37:09.920 --> 01:37:12.480]   that really capture their essence,
[01:37:12.480 --> 01:37:14.640]   and many of them are endangered
[01:37:14.640 --> 01:37:16.600]   and some very close to extinction.
[01:37:16.600 --> 01:37:21.400]   Others are more prolific right now.
[01:37:21.400 --> 01:37:22.940]   Nonetheless, I think of that account
[01:37:22.940 --> 01:37:25.680]   as all goodness, all benevolence.
[01:37:25.680 --> 01:37:26.820]   And then at the other extreme,
[01:37:26.820 --> 01:37:29.840]   I subscribe to an animal account called Nature is Metal.
[01:37:29.840 --> 01:37:33.560]   We've actually collaborated with Nature is Metal
[01:37:33.560 --> 01:37:37.720]   on a great white shark grabbing a tuna video
[01:37:37.720 --> 01:37:41.360]   that I didn't take, but someone I was with took,
[01:37:41.360 --> 01:37:43.400]   and we got their permission to post it.
[01:37:43.400 --> 01:37:44.480]   In any event, Nature is Metal
[01:37:44.480 --> 01:37:46.320]   is all about the harshness of nature.
[01:37:46.320 --> 01:37:49.060]   And then I think about the Planet Earth series
[01:37:49.060 --> 01:37:51.480]   hosted by David Attenborough and so forth,
[01:37:51.480 --> 01:37:54.660]   which sort of has a mixture of beautiful ducklings,
[01:37:54.660 --> 01:37:57.940]   but then also animals hunting each other
[01:37:57.940 --> 01:38:00.280]   and dying of old age or of starvation,
[01:38:00.280 --> 01:38:01.340]   and so the full array.
[01:38:01.340 --> 01:38:04.720]   So I think about that as an example of,
[01:38:04.720 --> 01:38:07.120]   if you look at Nature is Metal long enough,
[01:38:07.960 --> 01:38:09.920]   and it's a very cool account.
[01:38:09.920 --> 01:38:11.160]   I highly recommend people follow
[01:38:11.160 --> 01:38:12.760]   all three of these accounts.
[01:38:12.760 --> 01:38:13.800]   But if you look at it long enough,
[01:38:13.800 --> 01:38:16.960]   you get the impression like nature is hard.
[01:38:16.960 --> 01:38:18.800]   Life is hard out there.
[01:38:18.800 --> 01:38:20.080]   And it can be.
[01:38:20.080 --> 01:38:23.240]   You look at the Sartore account
[01:38:23.240 --> 01:38:25.940]   and you get the impression that animals are just beautiful.
[01:38:25.940 --> 01:38:27.820]   They're just being them, right?
[01:38:27.820 --> 01:38:31.800]   And he has a gift for capturing
[01:38:31.800 --> 01:38:34.160]   the essence of insects, reptiles, and mammals,
[01:38:34.160 --> 01:38:35.760]   and everything in between.
[01:38:35.760 --> 01:38:39.040]   So when I think about social media,
[01:38:39.040 --> 01:38:41.600]   or I even just think about our outlook
[01:38:41.600 --> 01:38:46.520]   onto the landscape of real life, non-virtual life,
[01:38:46.520 --> 01:38:48.400]   I feel like the human brain
[01:38:48.400 --> 01:38:51.340]   potentially can like all these things.
[01:38:51.340 --> 01:38:53.200]   But what you're describing in cynicism
[01:38:53.200 --> 01:38:56.240]   is the people that, for whatever reason,
[01:38:56.240 --> 01:38:59.880]   they're skewed toward this view that like life is hard,
[01:38:59.880 --> 01:39:01.600]   and therefore I need to protect myself
[01:39:01.600 --> 01:39:04.600]   and protect others at all times.
[01:39:04.600 --> 01:39:07.280]   In reality, how dynamic is cynicism?
[01:39:07.280 --> 01:39:10.280]   Earlier, you described how it can be domain-specific.
[01:39:10.280 --> 01:39:14.460]   But if somebody is pretty cynical,
[01:39:14.460 --> 01:39:18.200]   and they're older than 25,
[01:39:18.200 --> 01:39:21.260]   they're outside the sort of developmental plasticity range,
[01:39:21.260 --> 01:39:25.640]   what are the things that they can do on a daily basis
[01:39:25.640 --> 01:39:28.300]   to either tune down their cynicism
[01:39:28.300 --> 01:39:31.160]   or create room for this hopeful skepticism
[01:39:31.160 --> 01:39:32.680]   in a way that enriches them?
[01:39:32.680 --> 01:39:33.900]   Let's just start with them,
[01:39:33.900 --> 01:39:35.780]   because after all, they're cynics.
[01:39:35.780 --> 01:39:37.960]   Like we can't bait them
[01:39:37.960 --> 01:39:39.880]   with the good that they'll do for the world,
[01:39:39.880 --> 01:39:41.120]   but they'll do that too.
[01:39:41.120 --> 01:39:44.760]   What are some tools that we can all apply
[01:39:44.760 --> 01:39:47.160]   towards being less cynical?
[01:39:47.160 --> 01:39:49.200]   - It's a brilliant question, and you're right.
[01:39:49.200 --> 01:39:51.180]   I mean, I think a lot of us are very tuned
[01:39:51.180 --> 01:39:52.720]   into the metal side of life.
[01:39:52.720 --> 01:39:57.120]   And heavy metal is great, but life is not all metal.
[01:39:57.120 --> 01:39:59.880]   So how do we retune ourselves?
[01:39:59.880 --> 01:40:01.880]   I think about this a lot,
[01:40:01.880 --> 01:40:04.700]   in part because over the last several years,
[01:40:04.700 --> 01:40:07.380]   I haven't just been studying cynicism.
[01:40:07.380 --> 01:40:11.260]   I've been trying to counteract it in myself and in others.
[01:40:11.260 --> 01:40:16.260]   So I've focused on practical everyday things that I can do.
[01:40:16.260 --> 01:40:19.380]   And I guess they come in a bunch of categories.
[01:40:19.380 --> 01:40:21.100]   I'm gonna try to tick through them,
[01:40:21.100 --> 01:40:23.180]   but I really wanna hear your thoughts.
[01:40:23.180 --> 01:40:25.220]   The first has to do with our mindsets
[01:40:25.220 --> 01:40:29.140]   and the ways that we approach our own thinking.
[01:40:29.140 --> 01:40:32.220]   So I like to engage in a practice
[01:40:32.220 --> 01:40:35.320]   that I call being skeptical of my cynicism.
[01:40:35.320 --> 01:40:38.880]   So that is, in essence,
[01:40:38.880 --> 01:40:41.820]   taking tools from cognitive behavioral therapy
[01:40:41.820 --> 01:40:44.780]   and applying them to my cynical inferences.
[01:40:44.780 --> 01:40:46.500]   So again, my default mode,
[01:40:46.500 --> 01:40:48.800]   my factory settings are pretty suspicious.
[01:40:48.800 --> 01:40:50.380]   I wanna lay my cards on the table.
[01:40:50.380 --> 01:40:53.360]   It's ironic, given what I study, but there we are.
[01:40:53.360 --> 01:40:56.620]   So I often find myself in new situations,
[01:40:56.620 --> 01:40:59.020]   suspecting people, mistrusting people,
[01:40:59.020 --> 01:41:01.700]   wondering if they might take advantage of me.
[01:41:01.700 --> 01:41:05.420]   And what I do these days that I didn't do in the past
[01:41:05.420 --> 01:41:07.420]   is say, well, wait a minute, Zaki,
[01:41:07.420 --> 01:41:09.620]   where is this coming from?
[01:41:09.620 --> 01:41:13.060]   You're a scientist, defend your inference,
[01:41:13.060 --> 01:41:16.260]   defend your hypothesis, right?
[01:41:16.260 --> 01:41:17.660]   What evidence do you have to back it up?
[01:41:17.660 --> 01:41:18.940]   And very often,
[01:41:18.940 --> 01:41:22.520]   I find that the evidence is thin to non-existent, right?
[01:41:22.520 --> 01:41:25.660]   So that challenge, that just unearthing of,
[01:41:25.660 --> 01:41:27.560]   wait a minute, are you sure?
[01:41:27.560 --> 01:41:28.400]   No, you're not.
[01:41:28.400 --> 01:41:32.300]   And can tap into a little bit of intellectual humility.
[01:41:32.300 --> 01:41:34.820]   A second thing that I try to do
[01:41:34.820 --> 01:41:39.820]   is apply what my lab and I call a reciprocity mindset.
[01:41:39.820 --> 01:41:43.460]   That is understanding that yes,
[01:41:43.460 --> 01:41:45.920]   people vary in how trustworthy they are,
[01:41:45.920 --> 01:41:48.060]   but what you do also matters.
[01:41:48.060 --> 01:41:52.100]   Research finds that when you trust people,
[01:41:52.100 --> 01:41:54.680]   they're more likely to become trustworthy
[01:41:54.680 --> 01:41:56.400]   because they wanna reciprocate.
[01:41:56.400 --> 01:41:58.080]   You've honored them in this small way,
[01:41:58.080 --> 01:41:59.420]   and so they step up.
[01:41:59.420 --> 01:42:02.440]   It's known as earned trust in economics.
[01:42:02.440 --> 01:42:06.280]   And when you mistrust people, they become less trustworthy.
[01:42:06.280 --> 01:42:11.000]   So in my lab, we found that when you teach people this,
[01:42:11.000 --> 01:42:13.860]   when you teach people to own the influence
[01:42:13.860 --> 01:42:15.760]   that they have on others,
[01:42:15.760 --> 01:42:17.940]   they're more willing to be trusting.
[01:42:17.940 --> 01:42:19.080]   And when you're more trusting,
[01:42:19.080 --> 01:42:21.860]   then of course the other person reciprocates,
[01:42:21.860 --> 01:42:24.020]   which again, turns into this positive cycle.
[01:42:24.020 --> 01:42:27.220]   So I try, when I make a decision as to whether or not
[01:42:27.220 --> 01:42:28.820]   I'm gonna trust somebody,
[01:42:28.820 --> 01:42:30.060]   I think the default is to say,
[01:42:30.060 --> 01:42:31.960]   whoa, I'm taking on this risk.
[01:42:31.960 --> 01:42:33.540]   Is this a good choice for me?
[01:42:33.540 --> 01:42:36.840]   And I try to rotate that a little bit and say,
[01:42:36.840 --> 01:42:39.480]   what am I doing for the relationship here?
[01:42:39.480 --> 01:42:43.060]   Is this act of trust maybe a gift to this other person?
[01:42:43.060 --> 01:42:45.880]   How can it positively influence who they become
[01:42:45.880 --> 01:42:48.260]   in the course of this interaction?
[01:42:48.260 --> 01:42:51.000]   And then a third thing on the sort of mindset side,
[01:42:51.000 --> 01:42:53.320]   and then we can get to some behaviors,
[01:42:53.320 --> 01:42:56.260]   is what I call social savoring.
[01:42:57.260 --> 01:42:59.260]   I do this a lot with my kids, actually.
[01:42:59.260 --> 01:43:03.660]   Savoring is a general term for appreciating good things
[01:43:03.660 --> 01:43:04.740]   while they happen.
[01:43:04.740 --> 01:43:06.080]   It's related to gratitude,
[01:43:06.080 --> 01:43:08.700]   but gratitude is more appreciating the things
[01:43:08.700 --> 01:43:11.080]   that have happened to us in the past that are good.
[01:43:11.080 --> 01:43:13.300]   Savoring is, let's grab this moment right now
[01:43:13.300 --> 01:43:15.040]   and think about it.
[01:43:15.040 --> 01:43:19.340]   So my kids and I started savoring practices
[01:43:19.340 --> 01:43:20.420]   a couple of years ago.
[01:43:20.420 --> 01:43:21.540]   I call it classes.
[01:43:21.540 --> 01:43:25.100]   So I'll say, today we're gonna do an ice cream eating class,
[01:43:25.100 --> 01:43:27.620]   or we're gonna do a sunset watching class.
[01:43:27.620 --> 01:43:29.460]   - Cool, I wanna, are you adopting children?
[01:43:29.460 --> 01:43:30.900]   (laughing)
[01:43:30.900 --> 01:43:33.100]   - Applications are coming in now.
[01:43:33.100 --> 01:43:35.340]   We're evaluating them on a rolling basis.
[01:43:35.340 --> 01:43:36.660]   - I've already graduated college.
[01:43:36.660 --> 01:43:38.700]   (laughing)
[01:43:38.700 --> 01:43:40.980]   - But so we'll just sit there, you know,
[01:43:40.980 --> 01:43:44.320]   and eat ice cream slowly, you know, not so that it melts,
[01:43:44.320 --> 01:43:46.660]   but we'll say, you know, what are you enjoying about this?
[01:43:46.660 --> 01:43:47.500]   Is it the texture?
[01:43:47.500 --> 01:43:48.700]   Is it the flavor?
[01:43:48.700 --> 01:43:51.060]   What do you wanna remember about this moment?
[01:43:51.060 --> 01:43:54.100]   And I noticed more recently while working on this book
[01:43:54.100 --> 01:43:56.900]   that all of this was sensory.
[01:43:56.900 --> 01:44:00.300]   Sunsets, somersaults, ice cream, you name it.
[01:44:00.300 --> 01:44:02.340]   But it wasn't very social.
[01:44:02.340 --> 01:44:05.340]   And what they were hearing from me about other people
[01:44:05.340 --> 01:44:06.780]   was negatively skewed
[01:44:06.780 --> 01:44:09.120]   because gossip is negatively skewed, right?
[01:44:09.120 --> 01:44:10.820]   If somebody cut me off in traffic
[01:44:10.820 --> 01:44:12.500]   while I'm driving them to summer camp,
[01:44:12.500 --> 01:44:14.460]   they learn all about that person,
[01:44:14.460 --> 01:44:16.220]   but they don't learn about the people
[01:44:16.220 --> 01:44:19.400]   who are politely following traffic laws all around us,
[01:44:19.400 --> 01:44:22.860]   right, which is 90 plus percent of drivers.
[01:44:22.860 --> 01:44:25.580]   And so I started a practice of social savoring
[01:44:25.580 --> 01:44:28.820]   where I try to share with my kids
[01:44:28.820 --> 01:44:31.220]   positive things that I notice about other people.
[01:44:31.220 --> 01:44:33.820]   You could call it positive gossip as well.
[01:44:33.820 --> 01:44:35.500]   And one thing that I noticed
[01:44:35.500 --> 01:44:39.500]   is that that habit of savoring for them
[01:44:39.500 --> 01:44:43.580]   changed my mental processing, right?
[01:44:43.580 --> 01:44:45.740]   It actually changed what I noticed
[01:44:45.740 --> 01:44:46.580]   because of course,
[01:44:46.580 --> 01:44:49.700]   if you're trying to tell somebody about something,
[01:44:49.700 --> 01:44:52.400]   you look for examples that you can tell them about.
[01:44:52.400 --> 01:44:56.020]   So a habit of action of speech in that case
[01:44:56.020 --> 01:44:57.700]   became a habit of mine.
[01:44:57.700 --> 01:45:00.020]   So those three things,
[01:45:00.020 --> 01:45:02.260]   being skeptical of my cynicism,
[01:45:02.260 --> 01:45:04.940]   adopting a reciprocity mindset and social savoring,
[01:45:04.940 --> 01:45:08.560]   those are three of the psychological pieces.
[01:45:08.560 --> 01:45:10.140]   And I can get to some actions,
[01:45:10.140 --> 01:45:12.740]   but yeah, I wonder what you think of these.
[01:45:12.740 --> 01:45:13.900]   - Oh, I love those three.
[01:45:13.900 --> 01:45:17.860]   And I love the distinguishing features
[01:45:17.860 --> 01:45:19.580]   of savoring versus gratitude
[01:45:19.580 --> 01:45:23.600]   because there's so much data to support gratitude practices.
[01:45:23.600 --> 01:45:26.000]   And I don't think I've ever heard
[01:45:26.000 --> 01:45:27.820]   those two distinguished from one another,
[01:45:27.820 --> 01:45:31.480]   but clearly savoring things is going to be,
[01:45:31.480 --> 01:45:34.500]   is equally powerful towards our neurochemistry
[01:45:34.500 --> 01:45:35.340]   and our wellbeing.
[01:45:35.340 --> 01:45:38.640]   And I love that you include both sensory
[01:45:38.640 --> 01:45:40.640]   and interpersonal aspects to this.
[01:45:40.640 --> 01:45:41.960]   These are highly actionable
[01:45:41.960 --> 01:45:44.860]   and I'm sure people are as excited about them as I am
[01:45:44.860 --> 01:45:48.500]   because all this knowledge from the laboratory
[01:45:48.500 --> 01:45:49.880]   is indeed wonderful.
[01:45:49.880 --> 01:45:53.000]   But of course we always want to know what can we do
[01:45:53.000 --> 01:45:54.540]   now that you've made such a strong case
[01:45:54.540 --> 01:45:57.800]   for tuning down our cynicism a little bit
[01:45:57.800 --> 01:46:00.960]   in order to make ourselves smarter, better, happier
[01:46:00.960 --> 01:46:05.020]   and in touch with awe on a more regular basis.
[01:46:05.020 --> 01:46:06.560]   Would love to hear about some of the actions
[01:46:06.560 --> 01:46:07.800]   one can take as well.
[01:46:07.800 --> 01:46:11.920]   - Yeah, so if you imagine the mindset shifts
[01:46:11.920 --> 01:46:15.380]   that I've talked about as thinking more like a scientist
[01:46:15.380 --> 01:46:17.060]   about the social world,
[01:46:17.060 --> 01:46:20.300]   then the second step to me is to act more like a scientist
[01:46:20.300 --> 01:46:22.280]   in the social world.
[01:46:22.280 --> 01:46:24.500]   The monk and author, Pema Chodron,
[01:46:24.500 --> 01:46:26.300]   this great, great writer.
[01:46:26.300 --> 01:46:27.300]   - That's wonderful.
[01:46:27.300 --> 01:46:29.520]   - Has, is written beautifully
[01:46:29.520 --> 01:46:32.200]   about treating your life like an experiment.
[01:46:32.200 --> 01:46:37.000]   You know, in this moment, you could interrupt the defaults.
[01:46:37.000 --> 01:46:39.020]   You could interrupt the patterns
[01:46:39.020 --> 01:46:41.220]   and look around more carefully.
[01:46:41.220 --> 01:46:42.740]   And I try to do that.
[01:46:42.740 --> 01:46:45.320]   And I encourage other people to do that as well.
[01:46:45.320 --> 01:46:47.860]   You know, one form of this is what I call
[01:46:47.860 --> 01:46:50.340]   taking leaps of faith on other people, right?
[01:46:50.340 --> 01:46:53.560]   Collecting more social data requires risk.
[01:46:53.560 --> 01:46:54.940]   So I try to do that.
[01:46:54.940 --> 01:46:56.220]   I try to take more risks,
[01:46:56.220 --> 01:46:58.820]   become less risk averse in a social context.
[01:46:58.820 --> 01:47:01.020]   Now, this is not to say, you know,
[01:47:01.020 --> 01:47:03.100]   that I share my bank information with a prince
[01:47:03.100 --> 01:47:05.980]   who's gonna wire me $14 million, right?
[01:47:05.980 --> 01:47:07.220]   You need to be calculated.
[01:47:07.220 --> 01:47:12.060]   You need to be smart and safe in the risks that you take.
[01:47:12.060 --> 01:47:13.980]   But I would argue that many of us
[01:47:13.980 --> 01:47:17.100]   are far too risk averse in the social world.
[01:47:17.100 --> 01:47:19.960]   And there are lots of ways that I try to do this
[01:47:19.960 --> 01:47:22.420]   and lots of ways that people can do this.
[01:47:22.420 --> 01:47:26.360]   One is to just be more open to the social world.
[01:47:26.360 --> 01:47:27.420]   I'm an introvert.
[01:47:27.420 --> 01:47:29.460]   Andrew, I think you've said you're an introvert as well.
[01:47:29.460 --> 01:47:30.300]   Is that true?
[01:47:30.300 --> 01:47:31.140]   - I am.
[01:47:31.140 --> 01:47:32.980]   - Yeah, and so as introverts,
[01:47:32.980 --> 01:47:36.740]   we tend to think that the social world is maybe tiring
[01:47:36.740 --> 01:47:38.940]   and we need to recharge on our own.
[01:47:38.940 --> 01:47:39.860]   It's completely valid.
[01:47:39.860 --> 01:47:41.840]   I experience that all the time.
[01:47:41.840 --> 01:47:44.040]   I think that sometimes my introversion
[01:47:44.040 --> 01:47:45.820]   morphs into something else
[01:47:45.820 --> 01:47:49.620]   where I underestimate the joy of social contact.
[01:47:49.620 --> 01:47:52.820]   You know, there's so many times that before a dinner party,
[01:47:52.820 --> 01:47:55.800]   I would pay an embarrassing amount of money
[01:47:55.800 --> 01:47:58.080]   for the other party to cancel on me.
[01:47:58.080 --> 01:47:59.740]   I don't wanna be the person to cancel,
[01:47:59.740 --> 01:48:02.460]   but I would feel so relieved if they canceled.
[01:48:02.460 --> 01:48:05.540]   But then while I'm there and afterwards,
[01:48:05.540 --> 01:48:08.260]   I feel totally fulfilled by the experience.
[01:48:08.260 --> 01:48:09.740]   It's a little bit like running.
[01:48:09.740 --> 01:48:11.180]   Running is another thing that I love,
[01:48:11.180 --> 01:48:13.660]   but there are many times that before a run,
[01:48:13.660 --> 01:48:16.320]   I think, gosh, I really don't wanna do this.
[01:48:16.320 --> 01:48:19.540]   And then afterwards, I'm so grateful to have done so.
[01:48:19.540 --> 01:48:21.180]   There's a bunch of research that finds
[01:48:21.180 --> 01:48:23.900]   that people in general are like this.
[01:48:23.900 --> 01:48:26.340]   If you ask them to forecast what it would be like
[01:48:26.340 --> 01:48:28.460]   to talk with a stranger,
[01:48:28.460 --> 01:48:30.580]   to open up about a problem
[01:48:30.580 --> 01:48:32.000]   that they're having with a friend,
[01:48:32.000 --> 01:48:35.120]   to express gratitude, to try to help somebody,
[01:48:35.120 --> 01:48:38.980]   even to have a disagreement on ideological grounds,
[01:48:38.980 --> 01:48:43.180]   people forecast that these conversations would be awful,
[01:48:43.180 --> 01:48:46.900]   awkward, cringe, painful,
[01:48:46.900 --> 01:48:50.780]   and in the case of disagreement, harmful even.
[01:48:50.780 --> 01:48:53.500]   This is work from Nick Epley, Juliana Schroeder,
[01:48:53.500 --> 01:48:54.540]   and many others, by the way,
[01:48:54.540 --> 01:48:57.300]   on something known as under-sociality.
[01:48:57.300 --> 01:48:59.220]   And because we have these forecasts,
[01:48:59.220 --> 01:49:02.060]   we simply don't pursue the conversations.
[01:49:02.060 --> 01:49:03.300]   We don't go deeper.
[01:49:03.300 --> 01:49:05.660]   We stay on the surface.
[01:49:05.660 --> 01:49:08.420]   Nick, Juliana, and others then challenge people.
[01:49:08.420 --> 01:49:11.100]   They say, "Go and do this, have this conversation,
[01:49:11.100 --> 01:49:12.380]   "and then report back."
[01:49:12.380 --> 01:49:16.340]   And people's actual experiences are vastly more positive
[01:49:16.340 --> 01:49:19.200]   and more fulfilling than their forecasts.
[01:49:19.200 --> 01:49:21.620]   So I try to remember this in my own life.
[01:49:21.620 --> 01:49:25.500]   I try to realize when my forecasts are too risk-averse
[01:49:25.500 --> 01:49:28.380]   and too negative and say, "Let me just jump in.
[01:49:28.380 --> 01:49:30.040]   "Let me take this chance.
[01:49:30.040 --> 01:49:32.060]   "If it goes badly, well, fine.
[01:49:32.060 --> 01:49:35.220]   "And if it goes well, even better."
[01:49:35.220 --> 01:49:36.340]   The second piece here, though,
[01:49:36.340 --> 01:49:38.400]   is not just to take those risks,
[01:49:38.400 --> 01:49:40.300]   but to document their effects.
[01:49:40.300 --> 01:49:44.500]   I call this encounter counting.
[01:49:44.500 --> 01:49:48.660]   So in essence, gathering new data from the world is great,
[01:49:48.660 --> 01:49:50.600]   but if you forget those data,
[01:49:50.600 --> 01:49:53.060]   well, then the effects might be short-lived.
[01:49:53.060 --> 01:49:56.340]   I try to really remember when a social encounter
[01:49:56.340 --> 01:49:58.540]   is a mismatch with my expectations.
[01:49:58.540 --> 01:50:01.720]   I have a relative who, for instance,
[01:50:01.720 --> 01:50:03.780]   I disagree with politically quite a bit.
[01:50:03.780 --> 01:50:06.220]   And when I was working on this book, I said,
[01:50:06.220 --> 01:50:07.340]   "Let me take a chance.
[01:50:07.340 --> 01:50:09.800]   "We've known each other for 30 years.
[01:50:09.800 --> 01:50:11.740]   "We've never talked politics.
[01:50:11.740 --> 01:50:13.620]   "Let me try."
[01:50:13.620 --> 01:50:15.900]   And so I invited her to have this conversation
[01:50:15.900 --> 01:50:17.460]   about an issue we really disagree on.
[01:50:17.460 --> 01:50:20.400]   And we did not agree by the end of the conversation,
[01:50:20.400 --> 01:50:24.180]   but it was an immensely deep and meaningful conversation,
[01:50:24.180 --> 01:50:26.620]   and I actually felt like I knew her better,
[01:50:26.620 --> 01:50:29.340]   even though we've been close for decades.
[01:50:29.340 --> 01:50:31.540]   And I could just say, "Well, that was nice,"
[01:50:31.540 --> 01:50:32.660]   and then forget all about it
[01:50:32.660 --> 01:50:34.660]   and imagine that any future conversations
[01:50:34.660 --> 01:50:36.560]   on disagreement would be terrible.
[01:50:36.560 --> 01:50:39.900]   But I tried to write down in my journal
[01:50:39.900 --> 01:50:41.420]   sort of this is what happened,
[01:50:41.420 --> 01:50:43.740]   this is how it counteracted my expectations,
[01:50:43.740 --> 01:50:47.860]   try to lock in that learning from the social world
[01:50:47.860 --> 01:50:49.660]   so that pleasant surprises
[01:50:49.660 --> 01:50:51.660]   hopefully aren't as surprising anymore.
[01:50:51.660 --> 01:50:55.120]   - I love those practices.
[01:50:55.120 --> 01:50:58.300]   And thank you for reinforcing the process
[01:50:58.300 --> 01:51:00.500]   of reinforcing the experiences,
[01:51:00.500 --> 01:51:02.920]   because many times I'll be listening to an audio book
[01:51:02.920 --> 01:51:04.220]   or I'll think of something when I'm running
[01:51:04.220 --> 01:51:07.780]   and I'll put it into my voice memos or notes in my phone,
[01:51:07.780 --> 01:51:09.700]   and then I move them to this very notebook
[01:51:09.700 --> 01:51:12.460]   or another similar to it, and I'll go back and read it.
[01:51:12.460 --> 01:51:16.360]   But many things don't get passed through the filters
[01:51:16.360 --> 01:51:20.040]   that I forget because I didn't do that.
[01:51:20.040 --> 01:51:21.460]   And we know this is one of the best ways
[01:51:21.460 --> 01:51:24.180]   to solidify information is to think about experiences
[01:51:24.180 --> 01:51:27.020]   and information after being exposed to it.
[01:51:27.020 --> 01:51:29.220]   This is true studying, this is true clearly
[01:51:29.220 --> 01:51:32.700]   for emotional learning and our own personal evolution.
[01:51:32.700 --> 01:51:36.300]   - Which brings me to another example of somebody
[01:51:36.300 --> 01:51:37.500]   from the, I don't know what to call them,
[01:51:37.500 --> 01:51:40.300]   is it sort of philosophy, wellness, self-help space,
[01:51:40.300 --> 01:51:45.300]   you mentioned Pema Chodron, wonderful writer.
[01:51:45.300 --> 01:51:48.540]   There's someone else more or less in that space,
[01:51:48.540 --> 01:51:51.060]   Byron Katie, who a lot of her work
[01:51:51.060 --> 01:51:54.020]   is about challenging beliefs by simply asking questions
[01:51:54.020 --> 01:51:55.820]   about our core beliefs.
[01:51:55.820 --> 01:51:58.280]   This is something that I've started to explore a bit,
[01:51:58.280 --> 01:52:03.280]   like one could have the idea that good people always,
[01:52:03.280 --> 01:52:05.360]   I don't know, show up on time,
[01:52:05.360 --> 01:52:07.400]   and wouldn't we all love to be punctual?
[01:52:07.400 --> 01:52:09.940]   And as an academic, I confess,
[01:52:09.940 --> 01:52:11.900]   for me, everything starts 10 minutes after the hour,
[01:52:11.900 --> 01:52:14.640]   so we're consistently on time but late, right?
[01:52:14.640 --> 01:52:16.240]   So the non-academics.
[01:52:16.240 --> 01:52:18.400]   My friends from the military have a saying,
[01:52:18.400 --> 01:52:22.240]   which is five minutes early is on time, on time is late,
[01:52:22.240 --> 01:52:25.280]   and if you're late, you better bring lunch,
[01:52:25.280 --> 01:52:26.220]   so that kind of thing.
[01:52:26.220 --> 01:52:30.740]   In any event, the practice that she promotes,
[01:52:30.740 --> 01:52:35.240]   in essence, is to take a core belief
[01:52:35.240 --> 01:52:36.560]   and then just start challenging it
[01:52:36.560 --> 01:52:38.000]   from a number of different directions.
[01:52:38.000 --> 01:52:39.280]   Is that always true?
[01:52:39.280 --> 01:52:40.640]   Are there cases where that's not true?
[01:52:40.640 --> 01:52:42.240]   What would that look like, et cetera,
[01:52:42.240 --> 01:52:44.880]   as a way to really deconstruct one's own core beliefs,
[01:52:44.880 --> 01:52:47.040]   which is, I think, a bit of what you're talking about,
[01:52:47.040 --> 01:52:50.800]   and I feel like this could go in at least two directions.
[01:52:50.800 --> 01:52:51.680]   You can have a core belief
[01:52:51.680 --> 01:52:53.740]   that leads in the direction of cynicism,
[01:52:53.740 --> 01:52:56.760]   that you can deconstruct by just simply asking questions.
[01:52:56.760 --> 01:52:59.320]   Is that always true?
[01:52:59.320 --> 01:53:01.640]   Are there ever instances where that's not true?
[01:53:01.640 --> 01:53:04.120]   And what would it mean if that weren't true
[01:53:04.120 --> 01:53:05.520]   in a given instance, this sort of thing?
[01:53:05.520 --> 01:53:07.000]   And then on the other side,
[01:53:07.000 --> 01:53:10.360]   where we tend to err toward hopeful skepticism
[01:53:10.360 --> 01:53:13.200]   as opposed to cynicism, there too,
[01:53:13.200 --> 01:53:15.040]   I could imagine it would be useful
[01:53:15.040 --> 01:53:18.920]   to explore hopeful skepticism also as a scientist, right?
[01:53:18.920 --> 01:53:20.680]   Are there cases where hopeful skepticism,
[01:53:20.680 --> 01:53:21.760]   here, I'm gonna be cynical,
[01:53:21.780 --> 01:53:24.900]   can really get us into trouble, for instance?
[01:53:24.900 --> 01:53:28.420]   Anyway, obviously, I haven't run a study on this
[01:53:28.420 --> 01:53:30.880]   just because I came up with this example on the fly,
[01:53:30.880 --> 01:53:33.180]   but does what I just described fit more or less
[01:53:33.180 --> 01:53:34.660]   into the framework that you're describing?
[01:53:34.660 --> 01:53:35.500]   - Absolutely.
[01:53:35.500 --> 01:53:37.060]   I think that it's, in essence,
[01:53:37.060 --> 01:53:39.660]   being skeptical about our beliefs,
[01:53:39.660 --> 01:53:41.500]   putting them through their paces, right?
[01:53:41.500 --> 01:53:43.660]   Kicking the tires on our own beliefs.
[01:53:43.660 --> 01:53:45.100]   And again, this reminds me
[01:53:45.100 --> 01:53:47.140]   of cognitive behavioral therapy, right?
[01:53:47.140 --> 01:53:51.060]   A person who's socially anxious might tell their therapist,
[01:53:51.060 --> 01:53:54.040]   "I think all my friends secretly hate me."
[01:53:54.040 --> 01:53:55.880]   They might believe that to their core.
[01:53:55.880 --> 01:53:58.120]   It might affect every decision that they make.
[01:53:58.120 --> 01:53:59.540]   And the therapist might challenge them and say,
[01:53:59.540 --> 01:54:02.540]   "Well, wait, what's the evidence that you have for that?
[01:54:02.540 --> 01:54:04.700]   "Are there any instances in your entire life
[01:54:04.700 --> 01:54:06.660]   "where that seemed to not be true?"
[01:54:06.660 --> 01:54:08.540]   And to your point from Byron Katie,
[01:54:08.540 --> 01:54:10.720]   what would it mean if it weren't true?
[01:54:10.720 --> 01:54:14.560]   So this is the bedrock of one of the most successful
[01:54:14.560 --> 01:54:16.680]   forms of therapy for depression and anxiety
[01:54:16.680 --> 01:54:19.120]   and phobia in the world.
[01:54:19.120 --> 01:54:22.260]   You know, I do wanna also, I guess,
[01:54:22.260 --> 01:54:24.260]   zoom in on something that you're sharing there
[01:54:24.260 --> 01:54:26.080]   about our core beliefs.
[01:54:26.080 --> 01:54:29.100]   'Cause I think that in addition to testing our core beliefs,
[01:54:29.100 --> 01:54:31.000]   one thing that I wish we would do more
[01:54:31.000 --> 01:54:32.980]   is share our core beliefs.
[01:54:32.980 --> 01:54:34.840]   Because I don't think we know
[01:54:34.840 --> 01:54:36.560]   what each other's core beliefs are.
[01:54:36.560 --> 01:54:39.980]   And I think oftentimes, we think that we are more alone
[01:54:39.980 --> 01:54:42.940]   in our core beliefs than we actually are.
[01:54:42.940 --> 01:54:46.140]   So this is true in our politics, for instance,
[01:54:46.140 --> 01:54:48.780]   like the amount of people on,
[01:54:48.780 --> 01:54:50.840]   from every part of the political spectrum
[01:54:50.840 --> 01:54:55.840]   who want more compromise, more peace, and less conflict
[01:54:55.840 --> 01:55:00.420]   is north of 80% in surveys that my lab has conducted.
[01:55:00.420 --> 01:55:02.480]   But people don't know that.
[01:55:02.480 --> 01:55:05.340]   And so the lack of evidence, the lack of data
[01:55:05.340 --> 01:55:08.140]   about what other people want is a hindrance
[01:55:08.140 --> 01:55:11.220]   to the goals that we actually all share.
[01:55:11.220 --> 01:55:13.980]   This is also true in workplaces.
[01:55:13.980 --> 01:55:16.340]   So in the course of my work,
[01:55:16.340 --> 01:55:19.740]   I've done sort of some different projects
[01:55:19.740 --> 01:55:23.180]   with school systems, hospital systems, businesses.
[01:55:23.180 --> 01:55:24.940]   And one of the things I love doing
[01:55:24.940 --> 01:55:26.580]   is starting with an anonymous survey
[01:55:26.580 --> 01:55:28.220]   of everybody in the community.
[01:55:28.220 --> 01:55:33.000]   And I ask, how much do you value empathy and collaboration?
[01:55:33.000 --> 01:55:35.820]   How much would you prefer a workplace or community
[01:55:35.820 --> 01:55:40.220]   defined by cooperation versus competition?
[01:55:40.220 --> 01:55:43.500]   And invariably, and I'm talking about some places
[01:55:43.500 --> 01:55:45.760]   where you might imagine people would be competitive,
[01:55:45.760 --> 01:55:49.260]   invariably, a super majority of individuals
[01:55:49.260 --> 01:55:52.580]   in those communities want compassion,
[01:55:52.580 --> 01:55:56.720]   cooperation, and collaboration, right?
[01:55:56.720 --> 01:56:00.580]   Much more than they want competition or isolation.
[01:56:00.580 --> 01:56:01.900]   So one of the things that I love to do
[01:56:01.900 --> 01:56:03.500]   when I speak for those groups is to say,
[01:56:03.500 --> 01:56:06.820]   hey, look, here's some data, look around you.
[01:56:06.820 --> 01:56:10.340]   Here you've got 90% of people in this organization
[01:56:10.340 --> 01:56:12.220]   who want more cooperation.
[01:56:12.220 --> 01:56:15.420]   So if you just take a look in your periphery,
[01:56:15.420 --> 01:56:18.360]   almost everybody around you wants that as well.
[01:56:18.360 --> 01:56:20.660]   I also survey these communities and say,
[01:56:20.660 --> 01:56:22.220]   what do you think the average person
[01:56:22.220 --> 01:56:23.320]   would respond to these questions?
[01:56:23.320 --> 01:56:25.380]   And invariably, they're wrong.
[01:56:25.380 --> 01:56:28.740]   And so I say, you have underestimated each other,
[01:56:28.740 --> 01:56:30.840]   and now I'm giving you permission to stop.
[01:56:30.840 --> 01:56:34.380]   And I think this is one of the other actions
[01:56:34.380 --> 01:56:38.180]   that we can take if we're in a leadership position anywhere.
[01:56:38.180 --> 01:56:41.220]   Right, I think that looking for more data is great.
[01:56:41.220 --> 01:56:43.580]   If you're a leader, you can collect those data
[01:56:43.580 --> 01:56:46.300]   and you can show people to themselves.
[01:56:46.300 --> 01:56:50.540]   You can unveil the core beliefs of your community.
[01:56:50.540 --> 01:56:55.220]   And oftentimes those core beliefs are incredibly beautiful
[01:56:55.220 --> 01:56:59.220]   and surprising to the people in those communities
[01:56:59.220 --> 01:57:02.200]   and give them what I would call not peer pressure,
[01:57:02.200 --> 01:57:06.500]   but peer permission to express who they've been all along.
[01:57:06.500 --> 01:57:07.340]   - I love that.
[01:57:07.340 --> 01:57:09.540]   And one of the things that we've done on this podcast
[01:57:09.540 --> 01:57:12.980]   is to always invite comments and questions,
[01:57:12.980 --> 01:57:17.980]   critique and so forth in the comment section on YouTube.
[01:57:17.980 --> 01:57:20.580]   And I always say, and I do read all the comments,
[01:57:20.580 --> 01:57:21.860]   and sometimes it takes me a while
[01:57:21.860 --> 01:57:22.940]   and I'm still sifting through them.
[01:57:22.940 --> 01:57:25.860]   But I think comment sections can be,
[01:57:25.860 --> 01:57:27.860]   yes, they can be toxic in certain environments,
[01:57:27.860 --> 01:57:28.980]   in certain contexts,
[01:57:28.980 --> 01:57:32.340]   but they can also be tremendously enriching,
[01:57:32.340 --> 01:57:36.140]   not just for the reader, but for the commenter.
[01:57:36.140 --> 01:57:39.900]   And to see what people's core beliefs are really about.
[01:57:39.900 --> 01:57:43.180]   Now, oftentimes comments are of a different form
[01:57:43.180 --> 01:57:45.340]   and that's okay, that's all right.
[01:57:45.340 --> 01:57:49.300]   But I think that because of the anonymity involved,
[01:57:49.300 --> 01:57:51.660]   I think I can see that now through the lens
[01:57:51.660 --> 01:57:53.820]   of what you're saying as a license
[01:57:53.820 --> 01:57:55.460]   for people to really share their core beliefs
[01:57:55.460 --> 01:57:58.300]   about something that can be really informative
[01:57:58.300 --> 01:57:59.660]   and really enriching.
[01:57:59.660 --> 01:58:01.780]   Although I much prefer, I confess,
[01:58:01.780 --> 01:58:04.220]   the model that you're presenting
[01:58:04.220 --> 01:58:06.860]   where people are doing this in real time face-to-face
[01:58:06.860 --> 01:58:08.180]   as opposed to just online.
[01:58:08.180 --> 01:58:12.980]   As long as we're talking about polarization
[01:58:12.980 --> 01:58:14.960]   and the wish for less polarization,
[01:58:14.960 --> 01:58:19.500]   what are the data saying about the current state of affairs?
[01:58:19.500 --> 01:58:21.940]   We're recording this, you know, about what,
[01:58:21.940 --> 01:58:23.700]   three months or so out from an election
[01:58:23.700 --> 01:58:27.220]   or 90 some days or so from an election,
[01:58:27.220 --> 01:58:28.940]   presidential election.
[01:58:28.940 --> 01:58:30.580]   So without getting into discussions
[01:58:30.580 --> 01:58:32.620]   about political camps per se,
[01:58:33.620 --> 01:58:37.340]   what do your data and understanding about cynicism
[01:58:37.340 --> 01:58:42.340]   and hopeful skepticism tell us about that whole process
[01:58:42.340 --> 01:58:47.620]   and how the two camps are presenting themselves?
[01:58:47.620 --> 01:58:49.060]   - There is so much to say about this.
[01:58:49.060 --> 01:58:51.820]   I'm gonna try to not give a lecture here,
[01:58:51.820 --> 01:58:56.820]   but like so many of the themes in this conversation,
[01:58:56.820 --> 01:58:59.780]   I think that the headline for me
[01:58:59.780 --> 01:59:02.200]   when I look at the data on polarization,
[01:59:02.200 --> 01:59:05.420]   and I'm gonna talk about perceived polarization as well,
[01:59:05.420 --> 01:59:07.220]   is twofold.
[01:59:07.220 --> 01:59:12.220]   One, it's tragic because we are underestimating one another.
[01:59:12.220 --> 01:59:15.180]   And two, there's a lot of opportunity here
[01:59:15.180 --> 01:59:17.860]   because the delta between the world that we think we're in
[01:59:17.860 --> 01:59:20.180]   and the one that we're actually in is great,
[01:59:20.180 --> 01:59:22.420]   and it's positive as well.
[01:59:22.420 --> 01:59:25.340]   So there's a bunch of work on political perceptions.
[01:59:25.340 --> 01:59:29.340]   This is work done by folks like Meena Chakra at Harvard,
[01:59:29.340 --> 01:59:32.200]   my colleague Rob Willer in sociology at Stanford,
[01:59:32.200 --> 01:59:33.640]   our colleague, Rob Willer.
[01:59:33.640 --> 01:59:38.640]   And a lot of this focuses on what people think
[01:59:38.640 --> 01:59:44.000]   the average member of the other side is like.
[01:59:44.000 --> 01:59:45.400]   So if you're a Republican,
[01:59:45.400 --> 01:59:48.000]   what do you think the average Democrat believes?
[01:59:48.000 --> 01:59:48.840]   What do you think they're like?
[01:59:48.840 --> 01:59:49.800]   If you're a Democrat,
[01:59:49.800 --> 01:59:52.440]   what do you think the average Republican is like?
[01:59:52.440 --> 01:59:54.480]   And so I'll stop talking about Republicans
[01:59:54.480 --> 01:59:56.480]   and Democrats here because a lot of these data
[01:59:56.480 --> 02:00:01.120]   are bipartisan, the biases are pretty even across camps.
[02:00:01.120 --> 02:00:03.560]   And it turns out that in all cases,
[02:00:03.560 --> 02:00:06.960]   we are dead wrong about who's on the other side.
[02:00:06.960 --> 02:00:08.680]   We're even wrong demographically
[02:00:08.680 --> 02:00:10.600]   about who's on the other side.
[02:00:10.600 --> 02:00:15.600]   For instance, Democrats think that 25% of Republicans
[02:00:15.600 --> 02:00:18.560]   make more than $250,000 a year.
[02:00:18.560 --> 02:00:21.900]   The actual number is 2%.
[02:00:21.900 --> 02:00:25.020]   But the stereotype of Republicans that Democrats hold
[02:00:25.020 --> 02:00:27.240]   is that they're wealthy, I suppose.
[02:00:27.240 --> 02:00:31.080]   Republicans vastly overestimate the percentage of Democrats
[02:00:31.080 --> 02:00:34.040]   who are part of the LGBTQ community, for instance.
[02:00:34.040 --> 02:00:36.640]   Again, it's just a cultural stereotype.
[02:00:36.640 --> 02:00:40.480]   So we're wrong about even who's on the other side,
[02:00:40.480 --> 02:00:42.680]   but we're even more wrong about what they believe
[02:00:42.680 --> 02:00:44.120]   and what they want.
[02:00:44.120 --> 02:00:47.680]   So data suggests that there is perceived polarization,
[02:00:47.680 --> 02:00:50.880]   that is what we think the other side believes,
[02:00:50.880 --> 02:00:53.520]   is much greater than real polarization.
[02:00:53.520 --> 02:00:56.160]   I mean, first of all, we are divided, let's stipulate that.
[02:00:56.160 --> 02:00:58.740]   And those divisions can be really dangerous
[02:00:58.740 --> 02:01:02.580]   and are in some cases existential.
[02:01:02.580 --> 02:01:06.500]   But the division in our mind is much greater
[02:01:06.500 --> 02:01:08.820]   than the division that we actually have.
[02:01:08.820 --> 02:01:11.860]   My late friend, Emil Bruno, collected some data
[02:01:11.860 --> 02:01:14.860]   where he gathered Republicans and Democrats' views
[02:01:14.860 --> 02:01:15.820]   on immigration.
[02:01:15.820 --> 02:01:18.500]   He said, what would you want immigration to look like
[02:01:18.500 --> 02:01:21.980]   where zero is the borders are totally closed
[02:01:21.980 --> 02:01:24.000]   and 100 is they're totally open?
[02:01:24.000 --> 02:01:27.580]   And he plotted the distributions of what that looks like.
[02:01:27.580 --> 02:01:30.180]   He also asked people on either side,
[02:01:30.180 --> 02:01:33.060]   what do you think the other side would respond
[02:01:33.060 --> 02:01:34.940]   if asked that same question?
[02:01:34.940 --> 02:01:36.860]   And he plotted those distributions as well.
[02:01:36.860 --> 02:01:38.400]   - Other side meaning which group?
[02:01:38.400 --> 02:01:39.240]   - If you're a Democrat,
[02:01:39.240 --> 02:01:40.500]   what do you think Republicans would want?
[02:01:40.500 --> 02:01:43.580]   And if you're a Republican, what would Democrats want?
[02:01:43.580 --> 02:01:46.180]   And the distributions are totally different.
[02:01:46.180 --> 02:01:48.780]   The distributions of our actual preferences
[02:01:48.780 --> 02:01:51.180]   are like a hill with two peaks, right?
[02:01:51.180 --> 02:01:53.820]   So Republicans want more closed borders,
[02:01:53.820 --> 02:01:55.420]   Democrats want them more open,
[02:01:55.420 --> 02:01:57.860]   but they're not that far apart, first of all, the means,
[02:01:57.860 --> 02:02:01.040]   and there's a lot of overlap in the distributions.
[02:02:01.040 --> 02:02:04.500]   The distributions of our perceptions are two hills
[02:02:04.500 --> 02:02:07.460]   on opposite sides of a landscape.
[02:02:07.460 --> 02:02:11.180]   Republicans think that Democrats want totally open borders
[02:02:11.180 --> 02:02:14.100]   and Democrats think Republicans want totally closed borders.
[02:02:14.100 --> 02:02:18.340]   And the same pattern plays out for all sorts of issues
[02:02:18.340 --> 02:02:21.580]   where we think the other side is much more extreme,
[02:02:21.580 --> 02:02:23.740]   we think the average member of the other side
[02:02:23.740 --> 02:02:26.140]   is much more extreme than they really are.
[02:02:26.140 --> 02:02:28.260]   There's also work on meta-perceptions.
[02:02:28.260 --> 02:02:32.220]   What do you think the other side thinks about you?
[02:02:32.220 --> 02:02:34.580]   And it turns out that people on both sides
[02:02:34.580 --> 02:02:37.860]   imagine that their rivals hate them
[02:02:37.860 --> 02:02:40.940]   twice as much as their rivals really do.
[02:02:40.940 --> 02:02:42.460]   There's work on democratic norms
[02:02:42.460 --> 02:02:45.820]   that my grad student Louisa Santos collected,
[02:02:45.820 --> 02:02:48.300]   where we overestimate how anti-democratic
[02:02:48.300 --> 02:02:51.160]   the other side is by two times.
[02:02:51.160 --> 02:02:53.780]   And Rob has collected data on violence.
[02:02:53.780 --> 02:02:55.100]   How much do you think the other side
[02:02:55.100 --> 02:02:57.780]   would support violence to advance their aims?
[02:02:57.780 --> 02:03:01.260]   And here, the overestimates are 400%.
[02:03:01.260 --> 02:03:03.860]   So we think that the average person on the other side
[02:03:03.860 --> 02:03:07.700]   is four times as enthusiastic about violence
[02:03:07.700 --> 02:03:09.360]   as they really are.
[02:03:09.360 --> 02:03:13.100]   We have an image in our mind of the other
[02:03:13.100 --> 02:03:18.100]   as violent extremists who want to burn down the system.
[02:03:18.100 --> 02:03:21.500]   And again, we've talked about the warped media ecosystem
[02:03:21.500 --> 02:03:23.780]   that we're in, and that probably contributes here.
[02:03:23.780 --> 02:03:26.580]   But the fact is that those misperceptions
[02:03:26.580 --> 02:03:29.420]   are making all the problems that we fear worse.
[02:03:29.420 --> 02:03:31.060]   Because if you think that the other side
[02:03:31.060 --> 02:03:33.220]   is gearing up for war, what do you do?
[02:03:33.220 --> 02:03:35.460]   You have to defend yourself.
[02:03:35.460 --> 02:03:39.940]   And so we're caught in this almost cycle of escalation
[02:03:39.940 --> 02:03:42.940]   that really very few of us want.
[02:03:42.940 --> 02:03:44.700]   Now, I want to be really clear here
[02:03:44.700 --> 02:03:48.140]   that I'm not saying that we don't have actual disagreements.
[02:03:48.140 --> 02:03:52.380]   I'm also not saying that people across
[02:03:52.380 --> 02:03:55.860]   our political spectrum are all peaceable and all kind.
[02:03:55.860 --> 02:03:59.540]   There are absolutely extreme and violent people
[02:03:59.540 --> 02:04:02.500]   around our country that represent their political views
[02:04:02.500 --> 02:04:04.440]   in horrible and toxic ways.
[02:04:04.440 --> 02:04:06.580]   But that's not the average.
[02:04:06.580 --> 02:04:08.220]   And again, I want to get back to this point
[02:04:08.220 --> 02:04:11.560]   that the average person underestimates the average person.
[02:04:11.560 --> 02:04:14.020]   Not that we underestimate everybody,
[02:04:14.020 --> 02:04:16.740]   but that we're wrong about most people.
[02:04:16.740 --> 02:04:21.100]   And so again, to me, this is a tragedy and an opportunity.
[02:04:21.100 --> 02:04:24.020]   Rob and Mina and lots of other people find
[02:04:24.020 --> 02:04:27.520]   that when you ask people to actually pay attention
[02:04:27.520 --> 02:04:29.100]   to the data, when you show them,
[02:04:29.100 --> 02:04:32.540]   "Hey, actually, the other side fears violence
[02:04:32.540 --> 02:04:34.460]   "just as much as you do."
[02:04:34.460 --> 02:04:36.260]   When you show them that actually the other side
[02:04:36.260 --> 02:04:38.900]   is terrified of losing our democracy.
[02:04:38.900 --> 02:04:40.140]   When you show them that the other side
[02:04:40.140 --> 02:04:43.140]   doesn't actually hate you, that mitigates,
[02:04:43.140 --> 02:04:46.760]   that pulls back all of these escalatory impulses.
[02:04:46.760 --> 02:04:49.180]   In essence, you can decrease the threat
[02:04:49.180 --> 02:04:51.020]   that people feel from the other side
[02:04:51.020 --> 02:04:54.320]   by showing them who the other side really is.
[02:04:54.320 --> 02:04:56.300]   I understand this is such a massive
[02:04:56.300 --> 02:04:59.080]   and toxic sort of environment that we're in.
[02:04:59.080 --> 02:05:01.220]   I'm not saying that hopeful skepticism
[02:05:01.220 --> 02:05:06.220]   will solve our divided political landscape,
[02:05:06.220 --> 02:05:08.000]   will solve our problems.
[02:05:08.000 --> 02:05:11.540]   But I do think it's worth noting how wrong we are
[02:05:11.540 --> 02:05:13.900]   and that being a little bit less wrong
[02:05:13.900 --> 02:05:17.700]   can at least open a door, maybe let our minds wander
[02:05:17.700 --> 02:05:20.580]   towards a place of greater compromise and peace,
[02:05:20.580 --> 02:05:23.140]   which is what most people actually want.
[02:05:23.140 --> 02:05:25.360]   - Wow.
[02:05:25.360 --> 02:05:27.860]   I say that for several reasons.
[02:05:27.860 --> 02:05:31.340]   First of all, I've never heard the landscape
[02:05:31.340 --> 02:05:32.700]   described that way.
[02:05:32.700 --> 02:05:35.060]   And I confess, I didn't know that the landscape
[02:05:35.060 --> 02:05:40.060]   was as toward the center as it turns out it is.
[02:05:40.060 --> 02:05:47.760]   I have also many theories about how media and social media
[02:05:47.760 --> 02:05:50.560]   and podcasts for that matter might be contributing
[02:05:50.560 --> 02:05:55.560]   to this perceived polarization as opposed to the reality.
[02:05:55.560 --> 02:05:58.560]   And there's certainly a lot to explore
[02:05:58.560 --> 02:06:01.520]   in terms of what we can each and all do
[02:06:01.520 --> 02:06:05.700]   to remedy our understanding of what's going on out there.
[02:06:05.700 --> 02:06:07.460]   As a consequence, I'll ask,
[02:06:07.460 --> 02:06:10.140]   can some of the same tools that you described
[02:06:10.140 --> 02:06:13.400]   to better interact with one's own children,
[02:06:13.400 --> 02:06:16.460]   with one's own self, with other individuals
[02:06:16.460 --> 02:06:21.460]   and in small groups be used to sort of defragment
[02:06:21.460 --> 02:06:25.260]   some of the cynicism circuitry that exists in us
[02:06:25.260 --> 02:06:28.220]   around this polarized, excuse me,
[02:06:28.220 --> 02:06:32.160]   perceived highly polarized political landscape?
[02:06:32.160 --> 02:06:33.560]   - I love that clarification.
[02:06:33.560 --> 02:06:34.660]   Yeah, absolutely.
[02:06:34.660 --> 02:06:38.760]   I think that the answer is yes.
[02:06:38.760 --> 02:06:42.320]   There is lots of evidence that we are actively avoiding
[02:06:42.320 --> 02:06:45.760]   having conversations in part because of who we think
[02:06:45.760 --> 02:06:47.120]   the other side is.
[02:06:47.120 --> 02:06:49.600]   There is an amazing study that was conducted
[02:06:49.600 --> 02:06:53.940]   during Thanksgiving of 2016, which as you may recall,
[02:06:53.940 --> 02:06:58.940]   was directly after a very polarizing election
[02:06:58.940 --> 02:07:04.060]   and researchers used geo-tracking on people's cell phones
[02:07:04.060 --> 02:07:07.000]   to examine whether in order to go to Thanksgiving dinner,
[02:07:07.000 --> 02:07:12.000]   they crossed between a blue county into a red county
[02:07:12.000 --> 02:07:14.560]   or a red county into a blue county.
[02:07:14.560 --> 02:07:16.320]   In other words, are they going into,
[02:07:16.320 --> 02:07:18.520]   and I'm using air quotes here, quote unquote,
[02:07:18.520 --> 02:07:21.120]   enemy territory for Thanksgiving dinner.
[02:07:21.120 --> 02:07:23.620]   And they used that as a proxy of whether
[02:07:23.620 --> 02:07:26.120]   they're having dinner with people they disagree with.
[02:07:26.120 --> 02:07:29.440]   And it turns out that people who crossed county lines,
[02:07:29.440 --> 02:07:31.960]   who crossed into enemy territory, again in quotes,
[02:07:31.960 --> 02:07:33.560]   this is perceived polarization,
[02:07:33.560 --> 02:07:37.680]   they had dinners that were 50 minutes shorter
[02:07:37.680 --> 02:07:39.960]   than people who were dining with folks
[02:07:39.960 --> 02:07:42.640]   who presumably they agreed with.
[02:07:42.640 --> 02:07:46.480]   So we're talking about forsaking pie, Andrew.
[02:07:46.480 --> 02:07:49.400]   They're giving up pie in order to not talk
[02:07:49.400 --> 02:07:50.800]   with people they disagree with.
[02:07:50.800 --> 02:07:52.660]   And I think a lot of us are very skittish
[02:07:52.660 --> 02:07:55.580]   about these conversations because if you believe
[02:07:55.580 --> 02:07:59.920]   that the other side is a bunch of bloodthirsty marauders,
[02:07:59.920 --> 02:08:02.240]   why would you want to talk with them?
[02:08:02.240 --> 02:08:04.400]   Why have a beer with a fascist?
[02:08:04.400 --> 02:08:06.500]   That's just not a great plan.
[02:08:06.500 --> 02:08:12.160]   The truth though is that when we can collect better data,
[02:08:12.160 --> 02:08:16.440]   oftentimes we end up with better perceptions.
[02:08:16.440 --> 02:08:18.960]   And I mean better in two ways,
[02:08:18.960 --> 02:08:22.040]   one more positive and two more accurate.
[02:08:22.040 --> 02:08:24.760]   Now again, I want to say that there are real threats
[02:08:24.760 --> 02:08:25.760]   in our political environment.
[02:08:25.760 --> 02:08:29.920]   I'm not asking anybody to make themselves unsafe in any way.
[02:08:29.920 --> 02:08:34.080]   But in our lab, again, my wonderful graduate student,
[02:08:34.080 --> 02:08:37.720]   Louisa Santos, ran a study where we had about 160 people,
[02:08:37.720 --> 02:08:39.840]   these are folks from all over the country,
[02:08:39.840 --> 02:08:43.280]   who took part in Zoom conversations.
[02:08:43.280 --> 02:08:45.200]   We made sure that they really disagreed
[02:08:45.200 --> 02:08:48.960]   about gun control, immigration, and climate change,
[02:08:48.960 --> 02:08:50.720]   and they talked about those issues.
[02:08:51.640 --> 02:08:53.440]   We asked them to forecast
[02:08:53.440 --> 02:08:55.040]   what those conversations would be like,
[02:08:55.040 --> 02:08:57.220]   and we asked other folks to forecast
[02:08:57.220 --> 02:08:59.360]   what those conversations would be like.
[02:08:59.360 --> 02:09:02.100]   And the forecasts went from neutral to negative.
[02:09:02.100 --> 02:09:04.800]   Some people thought it won't make any difference,
[02:09:04.800 --> 02:09:07.520]   and other people thought it will be counterproductive.
[02:09:07.520 --> 02:09:10.240]   Some folks in our survey said dialogue is dead,
[02:09:10.240 --> 02:09:13.140]   there's no point in any of these conversations.
[02:09:13.140 --> 02:09:15.800]   We then brought these folks together.
[02:09:15.800 --> 02:09:18.800]   Oh, and I should say, among the people who were cynical
[02:09:18.800 --> 02:09:20.760]   about these conversations and who forecasted
[02:09:20.760 --> 02:09:24.440]   that they would go poorly, was us, the research team.
[02:09:24.440 --> 02:09:26.520]   Louisa and I spent hours talking about,
[02:09:26.520 --> 02:09:28.400]   what if people start to threaten each other,
[02:09:28.400 --> 02:09:32.180]   or dox each other, or look up each other's addresses.
[02:09:32.180 --> 02:09:34.800]   You know, Andrew, that we have institutional review boards
[02:09:34.800 --> 02:09:37.080]   that make sure that we're keeping human subjects safe,
[02:09:37.080 --> 02:09:40.840]   and the IRB wanted all sorts of safeguards in place,
[02:09:40.840 --> 02:09:43.760]   because we all thought that these conversations
[02:09:43.760 --> 02:09:46.120]   might go really poorly.
[02:09:46.120 --> 02:09:48.100]   After the conversations occurred,
[02:09:48.100 --> 02:09:50.520]   we asked folks who had taken part of them
[02:09:50.520 --> 02:09:54.760]   to rate how positive they were on a one to 100 scale.
[02:09:54.760 --> 02:09:57.760]   And the most common, the modal response
[02:09:57.760 --> 02:10:00.620]   that people gave us was 100 out of 100.
[02:10:00.620 --> 02:10:04.360]   And it wasn't just that they liked the conversation,
[02:10:04.360 --> 02:10:07.600]   they were shocked by how much they liked the conversation.
[02:10:07.600 --> 02:10:11.120]   They also reported less negative emotion
[02:10:11.120 --> 02:10:13.180]   for the other side as a whole,
[02:10:13.180 --> 02:10:15.520]   not just for the person that they talked with,
[02:10:15.520 --> 02:10:18.640]   and they reported more intellectual humility,
[02:10:18.640 --> 02:10:22.120]   more openness to questioning their own views.
[02:10:22.120 --> 02:10:24.700]   So here are conversations that we as a culture
[02:10:24.700 --> 02:10:28.640]   are actively avoiding because of our priors.
[02:10:28.640 --> 02:10:30.420]   Our priors are wrong given the data,
[02:10:30.420 --> 02:10:33.440]   but we don't know that, and we don't give ourselves chances
[02:10:33.440 --> 02:10:37.100]   to learn that we're wrong, because we don't collect the data.
[02:10:37.100 --> 02:10:38.400]   And when we do collect the data,
[02:10:38.400 --> 02:10:40.960]   when we step in and take that leap of faith,
[02:10:40.960 --> 02:10:45.960]   take that social risk, we are shocked and humbled,
[02:10:46.380 --> 02:10:49.620]   and feel more positive, and maybe even feel
[02:10:49.620 --> 02:10:51.740]   a slightly greater sense of hope
[02:10:51.740 --> 02:10:55.300]   that there can be some way out of this toxic environment
[02:10:55.300 --> 02:10:57.260]   that we're all trapped in.
[02:10:57.260 --> 02:11:01.140]   - Well, Jamil, Dr. Zaki,
[02:11:01.140 --> 02:11:06.140]   thank you so much for sharing your incredible,
[02:11:06.140 --> 02:11:08.860]   like what can only be described as wisdom
[02:11:08.860 --> 02:11:12.980]   into this area of humanity, right?
[02:11:12.980 --> 02:11:17.700]   I mean, to be a cynic is one potential aspect
[02:11:17.700 --> 02:11:21.140]   of being human, but you've made very clear
[02:11:21.140 --> 02:11:22.780]   that we have control.
[02:11:22.780 --> 02:11:25.460]   There is plasticity over this aspect of ourselves.
[02:11:25.460 --> 02:11:29.540]   If we adopt the right mindsets, apply the right practices,
[02:11:29.540 --> 02:11:33.820]   and it's so clear based on everything you've shared today
[02:11:33.820 --> 02:11:38.580]   that humans are operating rationally,
[02:11:38.580 --> 02:11:40.500]   and yet irrationally at the same time.
[02:11:40.500 --> 02:11:42.620]   I'm certainly not the first to say that,
[02:11:42.620 --> 02:11:44.260]   but in the context of cynicism,
[02:11:44.260 --> 02:11:46.620]   and in the context of being happier individuals,
[02:11:46.620 --> 02:11:49.500]   and families, and couples, and groups,
[02:11:49.500 --> 02:11:53.900]   that to really take a hard look at how cynical we are,
[02:11:53.900 --> 02:11:58.620]   and to start to make even minor inroads into that
[02:11:58.620 --> 02:12:00.100]   through belief testing.
[02:12:00.100 --> 02:12:01.940]   You know, I wrote down as we were talking
[02:12:01.940 --> 02:12:04.940]   that what I really feel you're encouraging us to do,
[02:12:04.940 --> 02:12:08.620]   correct me if I'm wrong, is to do both internal
[02:12:08.620 --> 02:12:11.340]   and external reality testing in an effort
[02:12:11.340 --> 02:12:15.660]   to move us away toward internal and external polarization.
[02:12:15.660 --> 02:12:20.100]   And I can't think of any higher calling than that.
[02:12:20.100 --> 02:12:23.340]   And you're giving us the tools,
[02:12:23.340 --> 02:12:25.820]   and those tools are supported by data.
[02:12:25.820 --> 02:12:29.500]   These aren't just ideas, they are data-supported ideas.
[02:12:29.500 --> 02:12:33.100]   And I just want to thank you for your incredible generosity
[02:12:33.100 --> 02:12:35.500]   in coming here today to talk about those ideas.
[02:12:35.500 --> 02:12:36.940]   Your book is phenomenal.
[02:12:36.940 --> 02:12:38.700]   I already learned so much from it,
[02:12:38.700 --> 02:12:40.900]   and I highly encourage people to read it.
[02:12:40.900 --> 02:12:43.820]   And what you've shared with us today is phenomenal.
[02:12:43.820 --> 02:12:46.020]   And I do hope to have you back again
[02:12:46.020 --> 02:12:49.500]   to talk about another topic that you are expert in,
[02:12:49.500 --> 02:12:52.860]   which is empathy, but we'll have to all wait
[02:12:52.860 --> 02:12:55.380]   with bated breath for that, myself included.
[02:12:55.380 --> 02:12:57.540]   So once again, I just want to thank you for your time,
[02:12:57.540 --> 02:12:59.620]   the incredible work that you're doing,
[02:12:59.620 --> 02:13:04.220]   and the evolution that you're taking us on.
[02:13:04.220 --> 02:13:07.500]   So on behalf of myself and everyone listening and watching,
[02:13:07.500 --> 02:13:08.980]   thank you ever so much.
[02:13:08.980 --> 02:13:11.860]   Andrew, this has been an absolutely delightful conversation.
[02:13:11.860 --> 02:13:15.660]   And I will say my forecast of it was very high,
[02:13:15.660 --> 02:13:19.180]   and it has exceeded that forecast.
[02:13:19.180 --> 02:13:21.740]   I also just want to take a moment to thank you
[02:13:21.740 --> 02:13:24.220]   for your work as a science communicator.
[02:13:24.220 --> 02:13:28.700]   As somebody who believes in not just trying
[02:13:28.700 --> 02:13:32.620]   to generate knowledge, but also to share knowledge,
[02:13:32.620 --> 02:13:35.420]   I think that it's absolutely one
[02:13:35.420 --> 02:13:37.700]   of the most important services that we can do
[02:13:37.700 --> 02:13:41.060]   as folks who have been trained and learned all this stuff
[02:13:41.060 --> 02:13:44.260]   to bring that information to as many people as we can.
[02:13:44.260 --> 02:13:47.020]   And I think it's just, it's an incredible mission
[02:13:47.020 --> 02:13:49.220]   and clearly has had such wonderful impact.
[02:13:49.220 --> 02:13:52.100]   So it's an honor to be part of that conversation
[02:13:52.100 --> 02:13:54.080]   and to be part of that effort.
[02:13:54.080 --> 02:13:54.920]   - Oh, well, thank you.
[02:13:54.920 --> 02:13:55.760]   I'll take that in.
[02:13:55.760 --> 02:13:59.280]   And it's a labor of love and an honor and a privilege
[02:13:59.280 --> 02:14:00.860]   to sit here today with you.
[02:14:00.860 --> 02:14:02.020]   So thank you ever so much.
[02:14:02.020 --> 02:14:03.520]   And please do come back again.
[02:14:03.520 --> 02:14:04.980]   - I would love that.
[02:14:04.980 --> 02:14:06.900]   - Thank you for joining me for today's discussion
[02:14:06.900 --> 02:14:08.520]   with Dr. Jamil Zaki.
[02:14:08.520 --> 02:14:09.740]   To learn more about his work
[02:14:09.740 --> 02:14:12.560]   and to find a link to his new book, "Hope for Cynics,"
[02:14:12.560 --> 02:14:15.000]   please see the links in the show note captions.
[02:14:15.000 --> 02:14:17.500]   If you're learning from and or enjoying this podcast,
[02:14:17.500 --> 02:14:19.180]   please subscribe to our YouTube channel.
[02:14:19.180 --> 02:14:21.660]   That's a terrific zero cost way to support us.
[02:14:21.660 --> 02:14:23.580]   Another terrific zero cost way to support us
[02:14:23.580 --> 02:14:26.500]   is to follow the podcast on both Spotify and Apple.
[02:14:26.500 --> 02:14:27.940]   And on both Spotify and Apple,
[02:14:27.940 --> 02:14:30.340]   you can leave us up to a five-star review.
[02:14:30.340 --> 02:14:32.060]   Please check out the sponsors mentioned
[02:14:32.060 --> 02:14:34.180]   at the beginning and throughout today's episode.
[02:14:34.180 --> 02:14:36.940]   That's the best way to support this podcast.
[02:14:36.940 --> 02:14:39.620]   If you have questions for me or comments about the podcast
[02:14:39.620 --> 02:14:41.580]   or guests or topics that you'd like me to consider
[02:14:41.580 --> 02:14:43.100]   for the Huberman Lab podcast,
[02:14:43.100 --> 02:14:45.500]   please put those in the comment section on YouTube.
[02:14:45.500 --> 02:14:47.220]   I do read all the comments.
[02:14:47.220 --> 02:14:48.380]   For those of you that haven't heard,
[02:14:48.380 --> 02:14:49.580]   I have a new book coming out.
[02:14:49.580 --> 02:14:51.180]   It's my very first book.
[02:14:51.180 --> 02:14:52.580]   It's entitled "Protocols,
[02:14:52.580 --> 02:14:54.740]   an Operating Manual for the Human Body."
[02:14:54.740 --> 02:14:55.900]   This is a book that I've been working on
[02:14:55.900 --> 02:14:57.060]   for more than five years,
[02:14:57.060 --> 02:14:59.380]   and that's based on more than 30 years
[02:14:59.380 --> 02:15:00.940]   of research and experience.
[02:15:00.940 --> 02:15:04.000]   And it covers protocols for everything from sleep
[02:15:04.000 --> 02:15:06.500]   to exercise to stress control,
[02:15:06.500 --> 02:15:08.940]   protocols related to focus and motivation.
[02:15:08.940 --> 02:15:12.300]   And of course, I provide the scientific substantiation
[02:15:12.300 --> 02:15:14.380]   for the protocols that are included.
[02:15:14.380 --> 02:15:18.280]   The book is now available by presale at protocolsbook.com.
[02:15:18.280 --> 02:15:20.660]   There you can find links to various vendors.
[02:15:20.660 --> 02:15:22.420]   You can pick the one that you like best.
[02:15:22.420 --> 02:15:24.180]   Again, the book is called "Protocols,
[02:15:24.180 --> 02:15:27.020]   an Operating Manual for the Human Body."
[02:15:27.020 --> 02:15:29.200]   If you're not already following me on social media,
[02:15:29.200 --> 02:15:31.900]   I'm Huberman Lab on all social media platforms.
[02:15:31.900 --> 02:15:34.740]   So that's Instagram, X, formerly known as Twitter,
[02:15:34.740 --> 02:15:36.380]   Threads, Facebook, and LinkedIn.
[02:15:36.380 --> 02:15:38.060]   And on all those platforms,
[02:15:38.060 --> 02:15:40.020]   I cover science and science-related tools,
[02:15:40.020 --> 02:15:41.620]   some of which overlaps with the content
[02:15:41.620 --> 02:15:42.920]   of the Huberman Lab podcast,
[02:15:42.920 --> 02:15:45.140]   but much of which is distinct from the content
[02:15:45.140 --> 02:15:46.440]   on the Huberman Lab podcast.
[02:15:46.440 --> 02:15:49.600]   Again, that's Huberman Lab on all social media channels.
[02:15:49.600 --> 02:15:50.780]   If you haven't already subscribed
[02:15:50.780 --> 02:15:52.260]   to our Neural Network newsletter,
[02:15:52.260 --> 02:15:53.660]   our Neural Network newsletter
[02:15:53.660 --> 02:15:57.220]   is a zero-cost monthly newsletter that has protocols,
[02:15:57.220 --> 02:15:59.460]   which are one- to three-page PDFs
[02:15:59.460 --> 02:16:02.540]   that describe things like optimizing your sleep,
[02:16:02.540 --> 02:16:05.300]   how to optimize your dopamine, deliberate cold exposure.
[02:16:05.300 --> 02:16:07.220]   We have a foundational fitness protocol
[02:16:07.220 --> 02:16:08.820]   that describes resistance training,
[02:16:08.820 --> 02:16:10.300]   sets and reps, and all of that,
[02:16:10.300 --> 02:16:11.700]   as well as cardiovascular training
[02:16:11.700 --> 02:16:13.860]   that's supported by the scientific research.
[02:16:13.860 --> 02:16:15.220]   And we have protocols related
[02:16:15.220 --> 02:16:17.780]   to neuroplasticity and learning.
[02:16:17.780 --> 02:16:19.980]   Again, you can find all that at completely zero cost
[02:16:19.980 --> 02:16:21.660]   by going to hubermanlab.com,
[02:16:21.660 --> 02:16:23.760]   go to the menu tab in the right corner,
[02:16:23.760 --> 02:16:26.080]   scroll down to newsletter, you put in your email,
[02:16:26.080 --> 02:16:28.660]   and we do not share your email with anybody.
[02:16:28.660 --> 02:16:31.060]   Thank you once again for joining me for today's discussion
[02:16:31.060 --> 02:16:32.700]   with Dr. Jamil Zaki.
[02:16:32.700 --> 02:16:34.840]   And last, but certainly not least,
[02:16:34.840 --> 02:16:36.980]   thank you for your interest in science.
[02:16:36.980 --> 02:16:39.560]   (upbeat music)
[02:16:39.560 --> 02:16:42.220]   (upbeat music)

