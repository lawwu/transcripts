
[00:00:00.000 --> 00:00:02.580]   (gentle music)
[00:00:02.580 --> 00:00:10.280]   - Do you think neural networks can be made to reason?
[00:00:10.280 --> 00:00:12.320]   - Yes, there is no question about that.
[00:00:12.320 --> 00:00:14.160]   Again, we have a good example, right?
[00:00:14.160 --> 00:00:17.000]   The question is how.
[00:00:17.000 --> 00:00:19.320]   So the question is how much prior structure
[00:00:19.320 --> 00:00:20.680]   do you have to put in the neural net
[00:00:20.680 --> 00:00:22.880]   so that something like human reasoning
[00:00:22.880 --> 00:00:26.040]   will emerge from it, from learning?
[00:00:26.040 --> 00:00:29.880]   Another question is all of our kind of model
[00:00:29.880 --> 00:00:32.520]   of what reasoning is that are based on logic
[00:00:32.520 --> 00:00:36.400]   are discrete and are therefore incompatible
[00:00:36.400 --> 00:00:38.000]   with gradient-based learning.
[00:00:38.000 --> 00:00:39.920]   And I'm a very strong believer in this idea
[00:00:39.920 --> 00:00:41.120]   of gradient-based learning.
[00:00:41.120 --> 00:00:44.560]   I don't believe that other types of learning
[00:00:44.560 --> 00:00:47.160]   that don't use kind of gradient information, if you want.
[00:00:47.160 --> 00:00:48.640]   - So you don't like discrete mathematics?
[00:00:48.640 --> 00:00:50.240]   You don't like anything discrete?
[00:00:50.240 --> 00:00:52.160]   - Well, it's not that I don't like it.
[00:00:52.160 --> 00:00:54.320]   It's just that it's incompatible with learning
[00:00:54.320 --> 00:00:56.280]   and I'm a big fan of learning, right?
[00:00:56.280 --> 00:00:59.240]   So in fact, that's perhaps one reason why
[00:01:00.240 --> 00:01:02.800]   deep learning has been kind of looked at with suspicion
[00:01:02.800 --> 00:01:03.840]   by a lot of computer scientists,
[00:01:03.840 --> 00:01:05.040]   because the math is very different.
[00:01:05.040 --> 00:01:07.560]   The math that you use for deep learning,
[00:01:07.560 --> 00:01:12.320]   it kind of has more to do with cybernetics,
[00:01:12.320 --> 00:01:14.800]   the kind of math you do in electrical engineering
[00:01:14.800 --> 00:01:17.360]   than the kind of math you do in computer science.
[00:01:17.360 --> 00:01:20.800]   And nothing in machine learning is exact, right?
[00:01:20.800 --> 00:01:23.120]   Computer science is all about sort of
[00:01:23.120 --> 00:01:26.520]   obsessive-compulsive attention to details
[00:01:26.520 --> 00:01:28.800]   of like every index has to be right.
[00:01:28.800 --> 00:01:31.760]   And you can prove that an algorithm is correct, right?
[00:01:31.760 --> 00:01:35.360]   Machine learning is the science of sloppiness, really.
[00:01:35.360 --> 00:01:38.080]   - That's beautiful.
[00:01:38.080 --> 00:01:43.080]   So, okay, maybe let's feel around in the dark
[00:01:43.080 --> 00:01:46.360]   of what is a neural network that reasons
[00:01:46.360 --> 00:01:51.360]   or a system that works with continuous functions
[00:01:51.360 --> 00:01:56.960]   that's able to do, build knowledge,
[00:01:56.960 --> 00:01:58.840]   however we think about reasoning,
[00:01:58.840 --> 00:02:02.440]   build on previous knowledge, build on extra knowledge,
[00:02:02.440 --> 00:02:04.080]   create new knowledge,
[00:02:04.080 --> 00:02:07.640]   generalize outside of any training set ever built.
[00:02:07.640 --> 00:02:08.920]   What does that look like?
[00:02:08.920 --> 00:02:13.320]   Maybe, do you have inklings of thoughts
[00:02:13.320 --> 00:02:15.400]   of what that might look like?
[00:02:15.400 --> 00:02:16.880]   - Yeah, I mean, yes and no.
[00:02:16.880 --> 00:02:18.760]   If I had precise ideas about this,
[00:02:18.760 --> 00:02:21.840]   I think we'd be building it right now.
[00:02:21.840 --> 00:02:23.640]   And there are people working on this
[00:02:23.640 --> 00:02:26.800]   whose main research interest is actually exactly that, right?
[00:02:26.800 --> 00:02:29.880]   So what you need to have is a working memory.
[00:02:29.880 --> 00:02:34.480]   So you need to have some device, if you want,
[00:02:34.480 --> 00:02:39.120]   some subsystem that can store a relatively large number
[00:02:39.120 --> 00:02:41.760]   of factual episodic information
[00:02:41.760 --> 00:02:45.480]   for a reasonable amount of time.
[00:02:45.480 --> 00:02:48.440]   So in the brain, for example,
[00:02:48.440 --> 00:02:50.320]   there are kind of three main types of memory.
[00:02:50.320 --> 00:02:55.320]   One is the sort of memory of the state of your cortex.
[00:02:56.120 --> 00:02:58.280]   And that sort of disappears within 20 seconds.
[00:02:58.280 --> 00:03:00.680]   You can't remember things for more than about 20 seconds
[00:03:00.680 --> 00:03:04.960]   or a minute if you don't have any other form of memory.
[00:03:04.960 --> 00:03:07.120]   The second type of memory, which is longer term,
[00:03:07.120 --> 00:03:08.840]   but still short term, is the hippocampus.
[00:03:08.840 --> 00:03:11.040]   So you can, you know, you came into this building,
[00:03:11.040 --> 00:03:15.560]   you remember where the exit is, where the elevators are.
[00:03:15.560 --> 00:03:18.360]   You have some map of that building
[00:03:18.360 --> 00:03:20.280]   that's stored in your hippocampus.
[00:03:20.280 --> 00:03:23.080]   You might remember something about what I said.
[00:03:23.080 --> 00:03:24.400]   You know, a few minutes ago.
[00:03:24.400 --> 00:03:25.240]   - I forgot it all already, but it's part.
[00:03:25.240 --> 00:03:26.880]   - It's been erased, but you know,
[00:03:26.880 --> 00:03:30.200]   but that would be in your hippocampus.
[00:03:30.200 --> 00:03:33.320]   And then the longer term memory is in the synapse,
[00:03:33.320 --> 00:03:34.640]   the synapses, right?
[00:03:34.640 --> 00:03:37.480]   So what you need if you want a system
[00:03:37.480 --> 00:03:38.320]   that's capable of reasoning
[00:03:38.320 --> 00:03:41.440]   is that you want the hippocampus like thing, right?
[00:03:41.440 --> 00:03:44.560]   And that's what people have tried to do
[00:03:44.560 --> 00:03:46.520]   with memory networks and, you know,
[00:03:46.520 --> 00:03:48.520]   neural training machines and stuff like that, right?
[00:03:48.520 --> 00:03:51.360]   And now with transformers, which have sort of
[00:03:51.360 --> 00:03:54.640]   a memory in their kind of self-attention system,
[00:03:54.640 --> 00:03:56.000]   you can think of it this way.
[00:03:56.000 --> 00:03:59.800]   So that's one element you need.
[00:03:59.800 --> 00:04:02.600]   Another thing you need is some sort of network
[00:04:02.600 --> 00:04:05.720]   that can access this memory,
[00:04:05.720 --> 00:04:10.680]   get an information back, and then kind of crunch on it,
[00:04:10.680 --> 00:04:13.400]   and then do this iteratively multiple times,
[00:04:13.400 --> 00:04:18.400]   because a chain of reasoning is a process
[00:04:18.400 --> 00:04:23.400]   by which you update your knowledge
[00:04:23.400 --> 00:04:24.960]   about the state of the world,
[00:04:24.960 --> 00:04:27.360]   about what's going to happen, et cetera.
[00:04:27.360 --> 00:04:30.000]   And that has to be this sort of
[00:04:30.000 --> 00:04:31.680]   recurrent operation, basically.
[00:04:31.680 --> 00:04:33.720]   - And you think that kind of,
[00:04:33.720 --> 00:04:35.680]   if we think about a transformer,
[00:04:35.680 --> 00:04:38.560]   so that seems to be too small to contain the knowledge
[00:04:38.560 --> 00:04:41.840]   that's to represent the knowledge
[00:04:41.840 --> 00:04:43.800]   that's contained in Wikipedia, for example.
[00:04:43.800 --> 00:04:46.560]   - Well, a transformer doesn't have this idea of recurrence.
[00:04:46.560 --> 00:04:47.640]   It's got a fixed number of layers,
[00:04:47.640 --> 00:04:50.160]   and that's the number of steps that limits,
[00:04:50.160 --> 00:04:51.680]   basically, its representation.
[00:04:51.680 --> 00:04:55.800]   - But recurrence would build on the knowledge somehow.
[00:04:55.800 --> 00:04:59.280]   I mean, it would evolve the knowledge
[00:04:59.280 --> 00:05:02.600]   and expand the amount of information, perhaps,
[00:05:02.600 --> 00:05:04.880]   or useful information within that knowledge.
[00:05:04.880 --> 00:05:09.320]   But is this something that just can emerge with size?
[00:05:09.320 --> 00:05:11.040]   Because it seems like everything we have now is too small.
[00:05:11.040 --> 00:05:11.880]   - Not just.
[00:05:11.880 --> 00:05:13.880]   No, it's not clear.
[00:05:13.880 --> 00:05:15.720]   I mean, how you access and write
[00:05:15.720 --> 00:05:17.600]   into an associative memory inefficient
[00:05:17.600 --> 00:05:19.760]   way, I mean, sort of the original memory network
[00:05:19.760 --> 00:05:22.080]   maybe had something like the right architecture,
[00:05:22.080 --> 00:05:25.080]   but if you try to scale up a memory network
[00:05:25.080 --> 00:05:27.400]   so that the memory contains all of Wikipedia,
[00:05:27.400 --> 00:05:28.560]   it doesn't quite work.
[00:05:28.560 --> 00:05:29.640]   - Right.
[00:05:29.640 --> 00:05:33.200]   - So there's a need for new ideas there, okay.
[00:05:33.200 --> 00:05:34.520]   But it's not the only form of reasoning.
[00:05:34.520 --> 00:05:35.920]   So there's another form of reasoning,
[00:05:35.920 --> 00:05:38.400]   which is very classical,
[00:05:38.400 --> 00:05:41.600]   so in some types of AI,
[00:05:41.600 --> 00:05:45.440]   and it's based on, let's call it energy minimization.
[00:05:45.440 --> 00:05:49.480]   Okay, so you have some sort of objective,
[00:05:49.480 --> 00:05:51.360]   some energy function that represents
[00:05:51.360 --> 00:05:57.840]   the quality or the negative quality, okay.
[00:05:57.840 --> 00:05:59.240]   Energy goes up when things get bad
[00:05:59.240 --> 00:06:01.840]   and they get low when things get good.
[00:06:01.840 --> 00:06:04.960]   So let's say you want to figure out,
[00:06:04.960 --> 00:06:07.480]   what gestures do I need to do
[00:06:07.480 --> 00:06:11.760]   to grab an object or walk out the door?
[00:06:11.760 --> 00:06:14.840]   If you have a good model of your own body,
[00:06:14.840 --> 00:06:17.000]   a good model of the environment,
[00:06:17.000 --> 00:06:18.960]   using this kind of energy minimization,
[00:06:18.960 --> 00:06:21.440]   you can do planning.
[00:06:21.440 --> 00:06:23.760]   And in optimal control,
[00:06:23.760 --> 00:06:26.640]   it's called model predictive control.
[00:06:26.640 --> 00:06:28.640]   You have a model of what's gonna happen in the world
[00:06:28.640 --> 00:06:30.080]   as a consequence of your actions.
[00:06:30.080 --> 00:06:33.120]   And that allows you to, by energy minimization,
[00:06:33.120 --> 00:06:34.360]   figure out a sequence of action
[00:06:34.360 --> 00:06:36.640]   that optimizes a particular objective function,
[00:06:36.640 --> 00:06:38.720]   which minimizes the number of times
[00:06:38.720 --> 00:06:39.560]   you're gonna hit something
[00:06:39.560 --> 00:06:41.080]   and the energy you're gonna spend
[00:06:41.080 --> 00:06:43.360]   doing the gesture and et cetera.
[00:06:44.360 --> 00:06:47.000]   So that's a form of reasoning.
[00:06:47.000 --> 00:06:48.080]   Planning is a form of reasoning.
[00:06:48.080 --> 00:06:52.600]   And perhaps what led to the ability of humans to reason
[00:06:52.600 --> 00:06:57.600]   is the fact that, or species that appear before us
[00:06:57.600 --> 00:06:59.600]   had to do some sort of planning
[00:06:59.600 --> 00:07:01.520]   to be able to hunt and survive
[00:07:01.520 --> 00:07:04.160]   and survive the winter in particular.
[00:07:04.160 --> 00:07:07.880]   And so it's the same capacity that you need to have.
[00:07:07.880 --> 00:07:11.000]   - So in your intuition is,
[00:07:11.000 --> 00:07:13.840]   if we look at expert systems
[00:07:13.840 --> 00:07:17.760]   and encoding knowledge as logic systems,
[00:07:17.760 --> 00:07:21.280]   as graphs or in this kind of way,
[00:07:21.280 --> 00:07:24.840]   is not a useful way to think about knowledge?
[00:07:24.840 --> 00:07:28.520]   - Graphs are a little brittle or logic representation.
[00:07:28.520 --> 00:07:32.440]   So basically, variables that have values
[00:07:32.440 --> 00:07:33.840]   and then constraint between them
[00:07:33.840 --> 00:07:35.840]   that are represented by rules
[00:07:35.840 --> 00:07:37.400]   is a little too rigid and too brittle, right?
[00:07:37.400 --> 00:07:41.160]   So some of the early efforts in that respect
[00:07:43.200 --> 00:07:45.560]   were to put probabilities on them.
[00:07:45.560 --> 00:07:49.120]   So a rule, if you have this and that symptom,
[00:07:49.120 --> 00:07:51.760]   you have this disease with that probability
[00:07:51.760 --> 00:07:53.960]   and you should prescribe that antibiotic
[00:07:53.960 --> 00:07:55.080]   with that probability, right?
[00:07:55.080 --> 00:07:57.840]   That's the mycin system from the seventies.
[00:07:57.840 --> 00:08:02.080]   And that's what that branch of AI led to
[00:08:02.080 --> 00:08:04.840]   Bayesian networks and graphical models
[00:08:04.840 --> 00:08:09.480]   and causal inference and variational method.
[00:08:09.480 --> 00:08:14.480]   So there is, I mean, certainly a lot of interesting work
[00:08:14.480 --> 00:08:15.960]   going on in this area.
[00:08:15.960 --> 00:08:18.400]   The main issue with this is knowledge acquisition.
[00:08:18.400 --> 00:08:23.400]   How do you reduce a bunch of data to a graph of this type?
[00:08:23.400 --> 00:08:26.360]   - Yeah, it relies on the expert to,
[00:08:26.360 --> 00:08:29.480]   on the human being to encode, to add knowledge.
[00:08:29.480 --> 00:08:31.640]   - And that's essentially impractical.
[00:08:31.640 --> 00:08:34.000]   - Yeah, it's not scalable.
[00:08:34.000 --> 00:08:34.840]   - That's a big question.
[00:08:34.840 --> 00:08:35.960]   The second question is,
[00:08:35.960 --> 00:08:38.320]   do you want to represent knowledge as symbols
[00:08:39.160 --> 00:08:41.760]   and do you want to manipulate them with logic?
[00:08:41.760 --> 00:08:43.840]   And again, that's incompatible with learning.
[00:08:43.840 --> 00:08:47.680]   So one suggestion with Geoff Hinton
[00:08:47.680 --> 00:08:49.560]   has been advocating for many decades
[00:08:49.560 --> 00:08:53.840]   is replace symbols by vectors.
[00:08:53.840 --> 00:08:55.480]   Think of it as pattern of activities
[00:08:55.480 --> 00:08:57.840]   in a bunch of neurons or units
[00:08:57.840 --> 00:08:59.640]   or whatever you want to call them
[00:08:59.640 --> 00:09:03.280]   and replace logic by continuous functions.
[00:09:03.280 --> 00:09:06.360]   Okay, and that becomes now compatible.
[00:09:06.360 --> 00:09:09.480]   There's a very good set of ideas
[00:09:09.480 --> 00:09:12.160]   by written in a paper about 10 years ago
[00:09:12.160 --> 00:09:15.520]   by Leon Boutou on, who is here at Facebook.
[00:09:15.520 --> 00:09:18.920]   The title of the paper is
[00:09:18.920 --> 00:09:20.360]   from machine learning to machine reasoning.
[00:09:20.360 --> 00:09:24.000]   And his idea is that a learning system
[00:09:24.000 --> 00:09:25.400]   should be able to manipulate objects
[00:09:25.400 --> 00:09:27.680]   that are in a space
[00:09:27.680 --> 00:09:29.440]   and then put the result back in the same space.
[00:09:29.440 --> 00:09:31.720]   So it's this idea of working memory basically.
[00:09:31.720 --> 00:09:35.200]   And it's very enlightening.
[00:09:35.200 --> 00:09:38.320]   - And in a sense that might learn something
[00:09:38.320 --> 00:09:41.480]   like the simple expert systems.
[00:09:41.480 --> 00:09:46.640]   I mean, you can learn basic logic operations there.
[00:09:46.640 --> 00:09:47.960]   - Yeah, quite possibly.
[00:09:47.960 --> 00:09:51.240]   There's a big debate on sort of how much prior structure
[00:09:51.240 --> 00:09:53.640]   you have to put in for this kind of stuff to emerge.
[00:09:53.640 --> 00:09:55.280]   That's the debate I have with Gary Marcus
[00:09:55.280 --> 00:09:56.320]   and people like that.
[00:09:56.320 --> 00:09:58.920]   (upbeat music)
[00:09:58.920 --> 00:10:01.520]   (upbeat music)
[00:10:01.520 --> 00:10:04.120]   (upbeat music)
[00:10:04.120 --> 00:10:06.720]   (upbeat music)
[00:10:06.720 --> 00:10:09.320]   (upbeat music)
[00:10:09.320 --> 00:10:11.920]   (upbeat music)
[00:10:11.920 --> 00:10:21.920]   [BLANK_AUDIO]

