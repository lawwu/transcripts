
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:05.760]   - Hi everybody, my name is Jan David Erlich.
[00:00:05.760 --> 00:00:07.920]   I'm the Chief Operating Officer over here
[00:00:07.920 --> 00:00:11.040]   at Weights & Biases, and that means I get the honor
[00:00:11.040 --> 00:00:14.720]   to present our first state of ML fully connected report.
[00:00:14.720 --> 00:00:18.920]   And this one is for the year that just ended 2022.
[00:00:18.920 --> 00:00:21.200]   Maybe before we start, given this is the first
[00:00:21.200 --> 00:00:23.740]   of such reports, we'll give you a little bit of a caveat
[00:00:23.740 --> 00:00:27.200]   on the methodology that we used over here for this report.
[00:00:28.040 --> 00:00:31.880]   First off, it's based on an analyzed analysis of usage
[00:00:31.880 --> 00:00:34.200]   on the Weights & Biases active user base
[00:00:34.200 --> 00:00:36.480]   and the activity measured on our platform.
[00:00:36.480 --> 00:00:38.560]   A little bit of data in 2020,
[00:00:38.560 --> 00:00:42.240]   but most of it really in 2021 and 2022.
[00:00:42.240 --> 00:00:44.100]   You'll see the definition, you know,
[00:00:44.100 --> 00:00:46.760]   you'll see active user used throughout this report
[00:00:46.760 --> 00:00:48.920]   afterwards, and so it's best to define
[00:00:48.920 --> 00:00:51.500]   what an active user is, and that is an individual
[00:00:51.500 --> 00:00:53.800]   that's performed some sort of activity
[00:00:53.800 --> 00:00:55.400]   on the Weights & Biases platform,
[00:00:55.400 --> 00:00:59.840]   whether logging a run or performing a hyperparameter sweep
[00:00:59.840 --> 00:01:04.840]   or logging some versioned model data or artifacts, et cetera.
[00:01:04.840 --> 00:01:12.380]   Finally, probably the biggest caveat is presence counts
[00:01:12.380 --> 00:01:15.040]   of frameworks and libraries.
[00:01:15.040 --> 00:01:17.720]   We're measuring that by looking at import statements
[00:01:17.720 --> 00:01:20.320]   in the Python training code.
[00:01:20.320 --> 00:01:23.680]   We are aware that this can overcount libraries
[00:01:23.680 --> 00:01:26.840]   that are imported but not used.
[00:01:26.840 --> 00:01:28.280]   None of y'all do that, right?
[00:01:28.280 --> 00:01:31.400]   And then finally, ML training hours
[00:01:31.400 --> 00:01:33.460]   are measured while clock time,
[00:01:33.460 --> 00:01:38.240]   as opposed to processor strength or count adjusted.
[00:01:38.240 --> 00:01:41.160]   So just factor that into the results
[00:01:41.160 --> 00:01:42.640]   that you're gonna see in a second.
[00:01:42.640 --> 00:01:46.720]   Let me start off with a brief overview of 2022.
[00:01:46.720 --> 00:01:51.720]   So in 2022, we had a little bit over 200,000 active users,
[00:01:52.680 --> 00:01:55.960]   as per the definition on the previous slide,
[00:01:55.960 --> 00:02:00.600]   hailing from almost 195 countries
[00:02:00.600 --> 00:02:05.600]   and performing nearly actually 125,000,
[00:02:05.600 --> 00:02:09.560]   or sorry, 125 million ML runs.
[00:02:09.560 --> 00:02:12.960]   So just to show that hopefully this data
[00:02:12.960 --> 00:02:15.520]   is gonna be somewhat representative
[00:02:15.520 --> 00:02:19.960]   given the statistical significance of the sample set
[00:02:19.960 --> 00:02:22.120]   that we're using to do this report.
[00:02:23.040 --> 00:02:24.360]   Hopefully I don't need to remind you
[00:02:24.360 --> 00:02:25.920]   what Weights and Biases does,
[00:02:25.920 --> 00:02:28.080]   but I'll spend a quick second doing that anyway.
[00:02:28.080 --> 00:02:30.800]   We are an end-to-end ML ops solution
[00:02:30.800 --> 00:02:35.800]   that enables ML practitioners to train their models
[00:02:35.800 --> 00:02:40.200]   from definition all the way through production.
[00:02:40.200 --> 00:02:42.100]   We have a number of products,
[00:02:42.100 --> 00:02:45.080]   including experiment tracking, collaborative reports,
[00:02:45.080 --> 00:02:48.600]   artifacts that let you version both your datasets
[00:02:48.600 --> 00:02:49.440]   and your models,
[00:02:49.440 --> 00:02:52.160]   interactive data visualizations with tables.
[00:02:52.160 --> 00:02:57.160]   We recently launched our workflow automation system launch,
[00:02:57.160 --> 00:02:59.520]   and then you can host Weights and Biases
[00:02:59.520 --> 00:03:03.340]   on a variety of cloud environments, as well as on-premise.
[00:03:03.340 --> 00:03:07.400]   And we integrate with thousands of frameworks and libraries,
[00:03:07.400 --> 00:03:11.840]   some of which we will discuss in this state of ML report.
[00:03:11.840 --> 00:03:14.600]   And then we're also honored to be used
[00:03:14.600 --> 00:03:17.280]   by some of the world's leading ML teams
[00:03:17.280 --> 00:03:21.320]   across a broad basis of industries and use cases.
[00:03:21.320 --> 00:03:24.280]   And aside from a little bit of a humble brag with the slide,
[00:03:24.280 --> 00:03:26.440]   it hopefully goes to show again
[00:03:26.440 --> 00:03:29.040]   that the data that we'll show over the next few slides
[00:03:29.040 --> 00:03:33.200]   is hopefully representative of the universe of research
[00:03:33.200 --> 00:03:35.560]   that is being performed in ML,
[00:03:35.560 --> 00:03:38.200]   both in academia and in the corporate setting
[00:03:38.200 --> 00:03:41.920]   across a broad base of research parameters,
[00:03:41.920 --> 00:03:44.920]   industries, use cases, and the like.
[00:03:44.920 --> 00:03:46.960]   So, you know, without further ado,
[00:03:46.960 --> 00:03:49.080]   let's get into the data.
[00:03:49.080 --> 00:03:51.040]   We'll cover three topics,
[00:03:51.040 --> 00:03:53.880]   a little bit about the user demographics,
[00:03:53.880 --> 00:03:56.780]   a bit on the frameworks and libraries
[00:03:56.780 --> 00:04:00.200]   that we see these users using in Weights and Biases,
[00:04:00.200 --> 00:04:02.640]   and then finally a bit on hardware,
[00:04:02.640 --> 00:04:06.680]   predominantly in this analysis on GPUs.
[00:04:06.680 --> 00:04:09.960]   So first, when we look at the demographics by country,
[00:04:09.960 --> 00:04:12.900]   I think one of the things that you can notice straight out
[00:04:12.900 --> 00:04:16.160]   from 2020 through 2022
[00:04:16.160 --> 00:04:20.560]   is the rise of users in Asian countries.
[00:04:20.560 --> 00:04:22.120]   And that's not to say that Europe,
[00:04:22.120 --> 00:04:23.600]   particularly Western Europe,
[00:04:23.600 --> 00:04:26.580]   is not still strong users of the platform,
[00:04:26.580 --> 00:04:29.160]   but you can see starting in '21,
[00:04:29.160 --> 00:04:32.480]   users in Asia, China, South Korea, Japan,
[00:04:32.480 --> 00:04:35.440]   really becoming ascendant as active users
[00:04:35.440 --> 00:04:37.300]   on the Weights and Biases platform.
[00:04:37.300 --> 00:04:40.680]   Obviously, the US is still leading in
[00:04:40.680 --> 00:04:41.880]   and across these years,
[00:04:41.880 --> 00:04:46.120]   but we see this big rise among the Asian countries.
[00:04:46.320 --> 00:04:49.120]   When you look by profession,
[00:04:49.120 --> 00:04:52.680]   you see that usage in both academics and corporates
[00:04:52.680 --> 00:04:54.880]   continues to increase year on year,
[00:04:54.880 --> 00:04:58.160]   but we also see this acceleration in usage
[00:04:58.160 --> 00:05:01.000]   among personal users and hobbyists,
[00:05:01.000 --> 00:05:04.480]   which we think reflects the broader interests
[00:05:04.480 --> 00:05:06.760]   across the population in ML.
[00:05:06.760 --> 00:05:09.880]   And so we're seeing more of these kind of individual users
[00:05:09.880 --> 00:05:13.720]   trying out models, learning, and getting exposure to ML,
[00:05:13.720 --> 00:05:16.920]   and that's being reflected in the data that we see.
[00:05:16.920 --> 00:05:20.840]   And then finally, when you look at the corporate usage
[00:05:20.840 --> 00:05:23.020]   and slice it by industry,
[00:05:23.020 --> 00:05:25.960]   you'll notice first that this is a 100% chart.
[00:05:25.960 --> 00:05:29.180]   So all of these industries are growing year on year,
[00:05:29.180 --> 00:05:31.480]   but it was quite interesting for us
[00:05:31.480 --> 00:05:34.520]   when we looked at it as a total chart
[00:05:34.520 --> 00:05:38.640]   to see the increased growth in healthcare and life sciences.
[00:05:38.640 --> 00:05:43.640]   So high-tech, as one would expect, dominates our user base,
[00:05:43.840 --> 00:05:46.360]   but you can see between 2020 and 2022
[00:05:46.360 --> 00:05:50.240]   that the healthcare and life sciences slice of this pie
[00:05:50.240 --> 00:05:52.360]   is growing a bit faster than the rest,
[00:05:52.360 --> 00:05:54.520]   and we found that interesting.
[00:05:54.520 --> 00:05:56.200]   Let's talk a little bit about frameworks.
[00:05:56.200 --> 00:05:57.760]   So first, let's start by talking about
[00:05:57.760 --> 00:06:02.100]   the two main core frameworks, TensorFlow and PyTorch.
[00:06:02.100 --> 00:06:06.000]   And as you can see for our user base,
[00:06:06.000 --> 00:06:08.380]   Torch seems to be the preferred one
[00:06:08.380 --> 00:06:13.380]   and seems to be accelerating its lead over TensorFlow.
[00:06:14.180 --> 00:06:17.940]   And that is true when we measure this across active users,
[00:06:17.940 --> 00:06:21.180]   across ML, the number of ML training runs,
[00:06:21.180 --> 00:06:23.860]   as well as the sum of the training hours
[00:06:23.860 --> 00:06:26.740]   spent executing these runs.
[00:06:26.740 --> 00:06:28.800]   And you can see actually in the latter two,
[00:06:28.800 --> 00:06:30.500]   the ML runs and the training runs,
[00:06:30.500 --> 00:06:32.940]   that the acceleration seems to be steeper
[00:06:32.940 --> 00:06:36.980]   for PyTorch over TensorFlow than it is among active users.
[00:06:36.980 --> 00:06:39.020]   And so we did a further analysis
[00:06:39.020 --> 00:06:43.340]   to try to see whether Torch users just run longer runs,
[00:06:43.340 --> 00:06:45.220]   and it does seem to be the case,
[00:06:45.220 --> 00:06:50.220]   that Torch users seem to run slightly longer ML runs
[00:06:50.220 --> 00:06:52.500]   than TensorFlow users,
[00:06:52.500 --> 00:06:55.100]   and that might indicate slightly larger networks
[00:06:55.100 --> 00:06:59.100]   or slightly deeper runs or more training data or wherever.
[00:06:59.100 --> 00:07:01.300]   We can take it in any direction.
[00:07:01.300 --> 00:07:04.180]   Let's look a little bit at the higher order frameworks,
[00:07:04.180 --> 00:07:05.620]   Keras versus Lightning.
[00:07:05.620 --> 00:07:07.560]   And I think the first thing that you'll notice
[00:07:07.560 --> 00:07:10.260]   is among active users,
[00:07:10.260 --> 00:07:13.160]   Keras is more widely used than Lightning.
[00:07:13.160 --> 00:07:17.260]   And that is the reverse of the Torch TensorFlow situation
[00:07:17.260 --> 00:07:18.780]   that we described earlier,
[00:07:18.780 --> 00:07:21.140]   which has us hypothesizing
[00:07:21.140 --> 00:07:23.900]   that basically more TensorFlow users
[00:07:23.900 --> 00:07:26.140]   use it alongside Keras
[00:07:26.140 --> 00:07:29.500]   than Torch users might use alongside Lightning.
[00:07:29.500 --> 00:07:32.420]   But once you look at ML runs and training hours,
[00:07:32.420 --> 00:07:35.260]   you notice that the relative effects are the same.
[00:07:35.260 --> 00:07:38.400]   So basically Lightning is leading Keras
[00:07:38.400 --> 00:07:41.260]   by ML runs and training hours
[00:07:41.280 --> 00:07:44.400]   parallel to its underlying core framework,
[00:07:44.400 --> 00:07:46.520]   so Torch versus TensorFlow.
[00:07:46.520 --> 00:07:49.840]   And then finally, when you look at the boosted networks,
[00:07:49.840 --> 00:07:55.760]   it's actually hard to make out who's leading or whatnot.
[00:07:55.760 --> 00:07:59.020]   Kind of the graphs seem to be different leaders
[00:07:59.020 --> 00:08:00.040]   across different graphs.
[00:08:00.040 --> 00:08:02.940]   But one of the things that is fairly clear
[00:08:02.940 --> 00:08:04.900]   is that you see more runs
[00:08:04.900 --> 00:08:07.820]   with more shorter training runs, basically.
[00:08:07.820 --> 00:08:12.160]   And that should be reflective of the scalar type
[00:08:12.160 --> 00:08:14.560]   of media types that these frameworks
[00:08:14.560 --> 00:08:16.840]   are usually used to train.
[00:08:16.840 --> 00:08:19.560]   Finally, when you look at the split by profession,
[00:08:19.560 --> 00:08:22.080]   so comparing the usage of these frameworks
[00:08:22.080 --> 00:08:24.440]   by academics versus corporates,
[00:08:24.440 --> 00:08:26.920]   the first thing you notice is that Torch
[00:08:26.920 --> 00:08:29.760]   is widely used by both sets,
[00:08:29.760 --> 00:08:32.340]   slightly more so by academics,
[00:08:32.340 --> 00:08:35.020]   and that gap is made up in the corporate realm
[00:08:35.020 --> 00:08:39.520]   by a slightly larger usage of all of the other frameworks.
[00:08:39.520 --> 00:08:42.200]   So TensorFlow, Keras, Lightning, XGBoost, et cetera.
[00:08:42.200 --> 00:08:45.960]   No one framework has really significantly made up
[00:08:45.960 --> 00:08:49.240]   that little sliver of gap provided by Torch
[00:08:49.240 --> 00:08:50.200]   among corporates.
[00:08:50.200 --> 00:08:53.920]   But Torch is the overwhelming favorite
[00:08:53.920 --> 00:08:57.800]   for both of these categories.
[00:08:57.800 --> 00:09:01.780]   Finally, when we look at the non-large framework libraries,
[00:09:01.780 --> 00:09:04.560]   we wanted to see some of the fastest growers.
[00:09:04.560 --> 00:09:07.480]   And we did this actually quarter over quarter.
[00:09:07.480 --> 00:09:10.720]   So looking at Q4 of 2022 versus Q3,
[00:09:10.720 --> 00:09:13.280]   because we really saw an ascendance in 2022
[00:09:13.280 --> 00:09:17.720]   of this interest and an explosion of ML that year.
[00:09:17.720 --> 00:09:20.400]   And that is proven out by the growth
[00:09:20.400 --> 00:09:23.600]   of some of these smaller libraries
[00:09:23.600 --> 00:09:25.440]   over the course of those quarters,
[00:09:25.440 --> 00:09:30.440]   where you can see growth rates north of 3x, 2x,
[00:09:30.440 --> 00:09:33.020]   and even among the larger libraries
[00:09:33.020 --> 00:09:34.600]   like Transformers and Hydra,
[00:09:34.600 --> 00:09:38.000]   you're still seeing growth rates well north of 1.5x.
[00:09:38.000 --> 00:09:41.280]   So quite impressive acceleration in the usage
[00:09:41.280 --> 00:09:44.280]   of a broad base of libraries among VML practitioners,
[00:09:44.280 --> 00:09:46.160]   training runs, and weights and biases,
[00:09:46.160 --> 00:09:49.680]   particularly in the last year in 2022.
[00:09:49.680 --> 00:09:53.000]   Now, interestingly, when you compare the libraries
[00:09:53.000 --> 00:09:55.680]   by profession, you don't see that many differences.
[00:09:55.680 --> 00:09:59.080]   So our corporate users and academic users
[00:09:59.080 --> 00:10:03.340]   are broadly using the same ML libraries.
[00:10:03.340 --> 00:10:05.780]   And I think this is, again, probably a testament
[00:10:05.780 --> 00:10:10.780]   of the fairly blurry line between both types of research.
[00:10:10.780 --> 00:10:16.220]   So academics helping with corporate research
[00:10:16.220 --> 00:10:17.300]   and vice versa.
[00:10:17.300 --> 00:10:22.300]   And so they're essentially using the same libraries
[00:10:22.300 --> 00:10:27.980]   to perform very similar type of research.
[00:10:27.980 --> 00:10:30.640]   Finally, let's talk a little bit about hardware.
[00:10:30.640 --> 00:10:32.080]   And I think one of the things
[00:10:32.080 --> 00:10:34.440]   that probably should jump out at you pretty quickly
[00:10:34.440 --> 00:10:39.000]   on this GPU chart is how much it is dominated by NVIDIA.
[00:10:39.000 --> 00:10:43.080]   There's a little sliver of the new Apple M1, M2 processors.
[00:10:43.080 --> 00:10:44.660]   And then obviously there's some processors
[00:10:44.660 --> 00:10:46.240]   that we can't identify,
[00:10:46.240 --> 00:10:48.820]   but the overwhelming majority of the rest
[00:10:48.820 --> 00:10:50.720]   are NVIDIA processors.
[00:10:50.720 --> 00:10:53.240]   The other thing that we're seeing year on year
[00:10:53.240 --> 00:10:57.880]   is the ascendance of the more powerful processors
[00:10:57.880 --> 00:11:02.320]   among the GE force, the set, the 1080s, 2080s
[00:11:02.320 --> 00:11:06.120]   are being supplanted by 3080s and 4080s.
[00:11:06.120 --> 00:11:09.720]   And then obviously you see the triangle at the bottom,
[00:11:09.720 --> 00:11:14.640]   the rise of the A100, these very beefy GPUs
[00:11:14.640 --> 00:11:17.220]   that are used in data centers.
[00:11:17.220 --> 00:11:20.040]   And then when we look at GPU counts,
[00:11:20.040 --> 00:11:22.520]   we notice that most of our users
[00:11:22.520 --> 00:11:25.200]   are still using single GPU environments.
[00:11:25.200 --> 00:11:27.140]   I think this is also a reflection
[00:11:27.140 --> 00:11:28.820]   of the broader adoption of ML.
[00:11:28.820 --> 00:11:32.120]   Not everyone can necessarily afford
[00:11:32.120 --> 00:11:35.000]   these very beefy multi GPU setups.
[00:11:35.000 --> 00:11:37.600]   But when you look by number of runs
[00:11:37.600 --> 00:11:39.160]   and by number of training hours,
[00:11:39.160 --> 00:11:40.880]   particularly by number of training hours,
[00:11:40.880 --> 00:11:44.280]   you see the ascendance of these more clustered
[00:11:44.280 --> 00:11:48.360]   GPU environments, going up to nine plus
[00:11:48.360 --> 00:11:53.120]   and more GPUs growing in percentage of use.
[00:11:53.120 --> 00:11:55.560]   And so there is a subset of users
[00:11:55.560 --> 00:11:58.540]   that are training these very large models
[00:11:58.540 --> 00:12:01.220]   on very beefy environments.
[00:12:01.220 --> 00:12:03.660]   And as with all first analyses,
[00:12:03.660 --> 00:12:05.740]   it begs for further analyses.
[00:12:05.740 --> 00:12:10.080]   And so we couldn't get done all of the analysis
[00:12:10.080 --> 00:12:10.920]   that we wanted to.
[00:12:10.920 --> 00:12:12.540]   And so we've left ourselves some topics
[00:12:12.540 --> 00:12:14.580]   for future state of ML reports.
[00:12:14.580 --> 00:12:17.940]   In particular, we wanna do some more work
[00:12:17.940 --> 00:12:22.940]   analyzing CPUs, Google's TPUs and Graphcore's IPUs.
[00:12:23.820 --> 00:12:27.640]   We'd love to do some work analyzing on which clouds,
[00:12:27.640 --> 00:12:31.240]   whether Amazon, Google, Microsoft, or on-premise,
[00:12:31.240 --> 00:12:34.600]   our users are training their ML runs.
[00:12:34.600 --> 00:12:37.080]   And then obviously the industry data
[00:12:37.080 --> 00:12:39.460]   at the start of the deck has us curious
[00:12:39.460 --> 00:12:41.360]   to do some more deeper analysis
[00:12:41.360 --> 00:12:44.720]   into the evolution of use cases and media types
[00:12:44.720 --> 00:12:48.380]   that are being used in the training of ML frameworks.
[00:12:48.380 --> 00:12:49.520]   This was really fun.
[00:12:49.520 --> 00:12:51.840]   So we will do it again.
[00:12:51.840 --> 00:12:55.700]   So please look forward to future state of ML reports
[00:12:55.700 --> 00:12:57.580]   from us at Weights and Biases.
[00:12:57.580 --> 00:13:00.020]   And yeah, thank you again for your time.
[00:13:00.020 --> 00:13:02.600]   (upbeat music)
[00:13:02.600 --> 00:13:05.180]   (upbeat music)
[00:13:05.180 --> 00:13:07.180]   It's the right thing to do.

