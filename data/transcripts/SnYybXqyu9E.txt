
[00:00:00.000 --> 00:00:06.500]   Hi, my name is Shreya Rajpal and today I am going to be talking about how to control large
[00:00:06.500 --> 00:00:10.160]   language model outputs for practical applications.
[00:00:10.160 --> 00:00:14.520]   And we're going to be using the open source framework called Guardrails AI in order to
[00:00:14.520 --> 00:00:17.260]   do this.
[00:00:17.260 --> 00:00:22.260]   So why do we need to control large language model outputs in the first place?
[00:00:22.260 --> 00:00:26.840]   Large language models are awesome, but they have a few different issues, primarily because
[00:00:26.840 --> 00:00:30.580]   they don't always behave how we expect them to.
[00:00:30.580 --> 00:00:33.880]   For starters, they are brittle and hard to control.
[00:00:33.880 --> 00:00:38.880]   And common issues that we might run into as we're building large language model applications
[00:00:38.880 --> 00:00:45.520]   are that the LLM app works while prototyping, but it ends up being flaky or does doing unexpected
[00:00:45.520 --> 00:00:48.840]   behavior when we run it in production.
[00:00:48.840 --> 00:00:54.600]   Getting correct outputs from large language models in a very consistent manner is hard.
[00:00:54.600 --> 00:01:01.040]   And common examples of incorrect outputs that we may experience are hallucinations, falsehoods,
[00:01:01.040 --> 00:01:04.160]   lack of correct structure, et cetera.
[00:01:04.160 --> 00:01:12.440]   Additionally, the only tool that is available to a developer is the prompt, which is insufficient.
[00:01:12.440 --> 00:01:20.240]   Because of these reasons, the practical application of an LLM, whenever the correctness of the
[00:01:20.240 --> 00:01:26.120]   LLM output is essential, those applications end up being limited.
[00:01:26.120 --> 00:01:35.320]   So how do we correct for this problem by being able to control the LLM outputs better?
[00:01:35.320 --> 00:01:38.520]   For starters, why is this problem challenging?
[00:01:38.520 --> 00:01:45.280]   And what are the common tools that developers use in order to control LLMs?
[00:01:45.280 --> 00:01:51.120]   The first one is using the prompt in order to make the LLM listen to directions or follow
[00:01:51.120 --> 00:01:52.920]   instructions.
[00:01:52.920 --> 00:01:57.480]   But this is problematic because LLMs are fundamentally stochastic.
[00:01:57.480 --> 00:02:04.840]   And this results in behavior where the same input may not guarantee the same output across
[00:02:04.840 --> 00:02:07.760]   multiple runs.
[00:02:07.760 --> 00:02:11.600]   Another issue with this is that a prompt doesn't offer any guarantees.
[00:02:11.600 --> 00:02:17.420]   So even if you request certain behavior in the prompt by asking an LLM to generate some
[00:02:17.420 --> 00:02:25.920]   output or to always avoid some output, LLMs don't always follow those instructions.
[00:02:25.920 --> 00:02:32.340]   Another tool that people often use is by trying to control the underlying model by updating
[00:02:32.340 --> 00:02:35.120]   the model weights, et cetera.
[00:02:35.120 --> 00:02:40.240]   But this is an expensive and time-consuming process, both training and fine-tuning a model
[00:02:40.240 --> 00:02:47.560]   on some custom data is much harder than just using an LLM hidden behind an API.
[00:02:47.560 --> 00:02:53.240]   But because LLMs are hidden behind APIs, there's also no control over model version updates,
[00:02:53.240 --> 00:02:54.240]   et cetera.
[00:02:54.240 --> 00:03:01.720]   So the underlying model might get updated by the model provider, even if the API remains
[00:03:01.720 --> 00:03:02.720]   consistent.
[00:03:02.720 --> 00:03:12.520]   A third utility, which often ends up being very useful and very practical, is combining
[00:03:12.520 --> 00:03:19.640]   an LLM with an output verification system and using this combination as a way to enforce
[00:03:19.640 --> 00:03:23.220]   control over the LLM outputs.
[00:03:23.220 --> 00:03:31.120]   So what this means is that we can build application-specific checks and verification programs that ensure
[00:03:31.120 --> 00:03:38.320]   that the LLM output is correct for some context.
[00:03:38.320 --> 00:03:43.880]   So now let's go into what this architecture actually looks like.
[00:03:43.880 --> 00:03:49.880]   For starters, this is what standard development of an LLM workflow looks like, where there's
[00:03:49.880 --> 00:03:51.460]   some application logic.
[00:03:51.460 --> 00:03:57.160]   And within that application logic is the LLM API call, where we end up getting a prompt.
[00:03:57.160 --> 00:04:01.640]   The prompt is then sent over to an LLM API, and we end up getting some output from the
[00:04:01.640 --> 00:04:06.720]   LLM that is then forwarded within that application logic.
[00:04:06.720 --> 00:04:14.100]   And how the Guardrails AI approach differs from the standard route is that the raw output
[00:04:14.100 --> 00:04:20.080]   of the LLM is then sent over into a separate verification module.
[00:04:20.080 --> 00:04:28.120]   So what this verification module does is it executes a number of independent small programs
[00:04:28.120 --> 00:04:34.920]   on the LLM output that check whether the LLM output behaves in a certain way or not.
[00:04:34.920 --> 00:04:40.000]   And depending on whether that verification passes or fails, next steps are taken.
[00:04:40.000 --> 00:04:46.040]   So as an example of some of the verification programs that you can add are making sure
[00:04:46.040 --> 00:04:52.320]   that there is no personally identifying information in the LLM output, or making sure that the
[00:04:52.320 --> 00:04:55.520]   LLM output does not contain any profanity.
[00:04:55.520 --> 00:05:00.360]   If you're building an application for a specific company, making sure that no competitors of
[00:05:00.360 --> 00:05:02.400]   that company are mentioned.
[00:05:02.400 --> 00:05:06.880]   If there's code being generated, making sure that the code is executable within a certain
[00:05:06.880 --> 00:05:08.620]   runtime.
[00:05:08.620 --> 00:05:12.000]   If you're generating summaries, making sure that the summaries are similar to the source
[00:05:12.000 --> 00:05:13.240]   text, et cetera.
[00:05:13.240 --> 00:05:19.680]   So each of these criteria is an independent program that is executed on top of the output
[00:05:19.680 --> 00:05:22.340]   of the LLM.
[00:05:22.340 --> 00:05:28.880]   As a combination of these programs, we either pass validation, in which case the output
[00:05:28.880 --> 00:05:34.640]   is correct, and we forward the raw output back to the application logic, or validation
[00:05:34.640 --> 00:05:38.800]   fails, in which case we construct a new prompt.
[00:05:38.800 --> 00:05:44.880]   And that prompt, the new prompt with relevant context about what specific validations failed
[00:05:44.880 --> 00:05:50.880]   is sent over to the LLM API and then corrected.
[00:05:50.880 --> 00:05:55.800]   So that is a framework that allows us to ensure guarantees, et cetera.
[00:05:55.800 --> 00:06:01.280]   But what does Guardrails AI provide you if you're building LLM applications in this manner?
[00:06:01.280 --> 00:06:06.600]   So Guardrails AI, first and foremost, gives you a very general framework for creating
[00:06:06.600 --> 00:06:08.840]   custom validators.
[00:06:08.840 --> 00:06:15.960]   It also implements the whole orchestration of prompting and verification and re-prompting.
[00:06:15.960 --> 00:06:20.760]   It comes with a library of commonly used validators for multiple use cases.
[00:06:20.760 --> 00:06:23.840]   We'll get into this later in the tutorial.
[00:06:23.840 --> 00:06:29.500]   And also, it has a specification language for generating structured LLM outputs.
[00:06:29.500 --> 00:06:34.260]   So we're going to dig deeper into generating structured outputs specifically for this tutorial
[00:06:34.260 --> 00:06:37.440]   and see what that looks like.
[00:06:37.440 --> 00:06:38.960]   So we go back to the diagram.
[00:06:38.960 --> 00:06:45.160]   But instead of looking at it in a general way, we basically look at what needs to change
[00:06:45.160 --> 00:06:49.280]   in this diagram in order to generate the following desired output.
[00:06:49.280 --> 00:06:54.880]   So we have some application where, let's say, we're parsing some unstructured data to generate
[00:06:54.880 --> 00:06:56.280]   structured data.
[00:06:56.280 --> 00:07:04.620]   And our LLM output should basically contain the following three fields in a JSON.
[00:07:04.620 --> 00:07:11.460]   So we want our LLM to generate a JSON with the fields of name, age, and zip code.
[00:07:11.460 --> 00:07:16.140]   Additionally, for the application that we're building, let's say that our verification
[00:07:16.140 --> 00:07:21.020]   logic contains things like that the first and last name should be contained within the
[00:07:21.020 --> 00:07:25.980]   name, that the age should be less than 100.
[00:07:25.980 --> 00:07:30.500]   And in the zip code, let's say that we want California zip codes only.
[00:07:30.500 --> 00:07:32.280]   So what does this look like?
[00:07:32.280 --> 00:07:37.380]   What does ensuring this criteria with guardrails looks like?
[00:07:37.380 --> 00:07:39.760]   For starters, we create a prompt.
[00:07:39.760 --> 00:07:46.540]   So with guardrails, one of the first things to do is to create a placeholder where guardrails
[00:07:46.540 --> 00:07:52.060]   can automatically inject information about the output structure.
[00:07:52.060 --> 00:07:56.780]   So this information about that the output structure should contain three keys of name,
[00:07:56.780 --> 00:08:01.480]   age, and zip code, and what their respective format should be is automatically created
[00:08:01.480 --> 00:08:05.100]   and inserted by guardrails.
[00:08:05.100 --> 00:08:06.740]   This prompt is sent to the LLM.
[00:08:06.740 --> 00:08:12.260]   And let's say that we end up receiving the following output, where name and age are something,
[00:08:12.260 --> 00:08:16.300]   but then we also get the zip code that is incorrect.
[00:08:16.300 --> 00:08:20.460]   This output is passed over to the verification system.
[00:08:20.460 --> 00:08:26.780]   And when we verify it, we essentially have two verification programs that pass and a
[00:08:26.780 --> 00:08:33.380]   third verification program that requests California zip codes only fails.
[00:08:33.380 --> 00:08:38.500]   Because this logic failed, we essentially re-trigger prompting, et cetera.
[00:08:38.500 --> 00:08:42.840]   And a new prompt is automatically created by guardrails.
[00:08:42.840 --> 00:08:46.660]   And this prompt looks something like this, where we provide the relevant context about
[00:08:46.660 --> 00:08:51.340]   why the failure occurred, which is that the zip code was not a California zip code, and
[00:08:51.340 --> 00:08:56.100]   more helpful context about structuring the output so that it can be combined with the
[00:08:56.100 --> 00:08:57.700]   previously generated output.
[00:08:57.700 --> 00:09:06.540]   So we only ask for the thing that was incorrect, which in this case is the zip code.
[00:09:06.540 --> 00:09:11.380]   With the new re-ask prompt, we end up getting a new re-asked output.
[00:09:11.380 --> 00:09:15.780]   And let's say the new output looks something like this.
[00:09:15.780 --> 00:09:20.540]   When we run verification on this new output, we see that all our verification programs
[00:09:20.540 --> 00:09:31.140]   pass, in which case this output is sent over to the application logic directly.
[00:09:31.140 --> 00:09:36.140]   Here's a quick walkthrough of some examples of validators that are offered.
[00:09:36.140 --> 00:09:42.820]   So obviously, JSON structure and type checking is some of the common ones, but also that
[00:09:42.820 --> 00:09:47.900]   there's about lists, about the quality of the text, like no profanity or quality of
[00:09:47.900 --> 00:09:50.340]   the translation, et cetera.
[00:09:50.340 --> 00:09:57.060]   Specific validators for summarization, specific validators for not asking private questions,
[00:09:57.060 --> 00:09:58.060]   et cetera.
[00:09:58.060 --> 00:10:04.460]   But any custom validator can be created and supported by the framework.
[00:10:04.460 --> 00:10:10.180]   We primarily talked about re-asking within the context of the framework, but guardrails
[00:10:10.180 --> 00:10:14.640]   also offers other options for handling incorrect output.
[00:10:14.640 --> 00:10:19.500]   So options like fixing, like programmatically fixing the generated output.
[00:10:19.500 --> 00:10:25.020]   So let's say that the age in our example was greater than 100, then we automatically set
[00:10:25.020 --> 00:10:26.740]   the max age to 100.
[00:10:26.740 --> 00:10:29.120]   That we programmatically fix.
[00:10:29.120 --> 00:10:31.620]   Other cases might be filtering incorrect values.
[00:10:31.620 --> 00:10:38.100]   So here instead of asking the LLM to regenerate a new correct zip code, we maybe just filter
[00:10:38.100 --> 00:10:40.820]   out incorrect zip codes.
[00:10:40.820 --> 00:10:44.300]   Refrain from answering where we don't answer at all.
[00:10:44.300 --> 00:10:50.380]   No op, which is like take no action, but store the failure to a log.
[00:10:50.380 --> 00:10:54.740]   And then finally, raising an exception.
[00:10:54.740 --> 00:11:00.520]   In summary, guardrails AI is a framework for creating custom validators.
[00:11:00.520 --> 00:11:05.380]   It's a library of many commonly used validators across multiple use cases.
[00:11:05.380 --> 00:11:11.140]   It's an orchestration system for prompting, verification, and regenerating prompts.
[00:11:11.140 --> 00:11:17.580]   In addition to having a specification language for generating structured LLM outputs.
[00:11:17.580 --> 00:11:23.300]   To learn more about guardrails, you can follow along on the GitHub project at this link.
[00:11:23.300 --> 00:11:27.380]   Check out our documentation at getguardrails.ai.
[00:11:27.380 --> 00:11:30.780]   You can follow the Twitter account or you can join our Discord.
[00:11:30.780 --> 00:11:31.780]   Thank you.
[00:11:31.780 --> 00:11:31.780]   [MUSIC PLAYING]
[00:11:31.780 --> 00:11:39.340]   [MUSIC PLAYING]

