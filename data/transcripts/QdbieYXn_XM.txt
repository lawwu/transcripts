
[00:00:00.000 --> 00:00:03.760]   We're here today to discuss the Convit paper.
[00:00:03.760 --> 00:00:05.760]   And we also have with us Stefani.
[00:00:05.760 --> 00:00:08.880]   He's joining us from Paris.
[00:00:08.880 --> 00:00:10.080]   Is that correct?
[00:00:10.080 --> 00:00:11.560]   Yep.
[00:00:11.560 --> 00:00:12.480]   Awesome.
[00:00:12.480 --> 00:00:14.360]   So Stefani's been around.
[00:00:14.360 --> 00:00:17.360]   He's working as a researcher.
[00:00:17.360 --> 00:00:19.960]   He's a PhD student who's working on deep learning.
[00:00:19.960 --> 00:00:24.480]   And he's jointly supervised by Gailo Biroli and Leven Sagun.
[00:00:24.480 --> 00:00:26.400]   And he studied theoretical physics
[00:00:26.400 --> 00:00:28.640]   and worked with NASA on black hole mergers,
[00:00:28.640 --> 00:00:30.560]   which is super exciting.
[00:00:30.560 --> 00:00:33.040]   His research focuses on understanding
[00:00:33.040 --> 00:00:35.920]   how deep neural networks are able to generalize,
[00:00:35.920 --> 00:00:38.320]   despite being heavily over-parameterized.
[00:00:38.320 --> 00:00:41.400]   And he's used tools from statistical mechanics
[00:00:41.400 --> 00:00:44.280]   to study simple models and try to understand
[00:00:44.280 --> 00:00:45.440]   when and why they overfit.
[00:00:45.440 --> 00:00:49.240]   And he also works on different types of inductive biases,
[00:00:49.240 --> 00:00:51.560]   how inductive biases affect learning
[00:00:51.560 --> 00:00:55.000]   from fully connected networks to convolution networks
[00:00:55.000 --> 00:00:57.920]   to transformers.
[00:00:57.920 --> 00:01:00.320]   He's also interested in bio-inspired alternatives
[00:01:00.320 --> 00:01:01.520]   to back propagation.
[00:01:01.520 --> 00:01:04.000]   So I'm really, really excited to have you here with us,
[00:01:04.000 --> 00:01:05.080]   Stefani, today.
[00:01:05.080 --> 00:01:07.000]   Is that the correct way to pronounce your name?
[00:01:07.000 --> 00:01:09.200]   Is it Stefani or is it Stefan?
[00:01:09.200 --> 00:01:10.000]   Stefani.
[00:01:10.000 --> 00:01:10.520]   Thank you.
[00:01:10.520 --> 00:01:11.120]   Stefani.
[00:01:11.120 --> 00:01:12.320]   Oh, perfect.
[00:01:12.320 --> 00:01:15.720]   Thanks so much for finding the time for joining us today.
[00:01:15.720 --> 00:01:17.720]   So at Weights and Biases, we usually
[00:01:17.720 --> 00:01:19.920]   meet every week or biweekly.
[00:01:19.920 --> 00:01:24.160]   And we discuss and dissect lots of papers.
[00:01:24.160 --> 00:01:27.960]   And when I first read Convit, the paper in itself,
[00:01:27.960 --> 00:01:30.080]   the idea also seemed like something
[00:01:30.080 --> 00:01:32.880]   that's so interesting that we had to discuss it
[00:01:32.880 --> 00:01:33.800]   at Weights and Biases.
[00:01:33.800 --> 00:01:37.320]   So thanks for finding the time for joining us today.
[00:01:37.320 --> 00:01:40.600]   And we'll be going through the paper with you.
[00:01:40.600 --> 00:01:41.960]   So thanks for that.
[00:01:41.960 --> 00:01:42.480]   Thank you.
[00:01:42.480 --> 00:01:45.080]   It's a pleasure.
[00:01:45.080 --> 00:01:46.040]   Cool.
[00:01:46.040 --> 00:01:49.160]   So something that we'll be doing, as is usual with most--
[00:01:49.160 --> 00:01:51.560]   with almost all of our paper reading groups
[00:01:51.560 --> 00:01:55.800]   is I do ask that we ask all our questions towards the end
[00:01:55.800 --> 00:01:58.160]   at 1db.me/convit.
[00:01:58.160 --> 00:02:02.640]   I've just pasted that URL in chat as well.
[00:02:02.640 --> 00:02:06.080]   So this will bring us to--
[00:02:06.080 --> 00:02:12.560]   this will bring us to this page.
[00:02:12.560 --> 00:02:14.760]   The only reason why we host all our questions here,
[00:02:14.760 --> 00:02:16.800]   and it's towards the bottom, there's
[00:02:16.800 --> 00:02:18.640]   an option to write a comment.
[00:02:18.640 --> 00:02:19.720]   And it is live.
[00:02:19.720 --> 00:02:21.520]   So I can go through these questions.
[00:02:21.520 --> 00:02:24.800]   And the only reason why we host our questions here,
[00:02:24.800 --> 00:02:26.600]   we've done this for our Fastbook sessions.
[00:02:26.600 --> 00:02:29.760]   We've done this for our previous paper reading group sessions.
[00:02:29.760 --> 00:02:31.720]   It's just because we're live on YouTube as well.
[00:02:31.720 --> 00:02:33.640]   So it's very hard for me to monitor the chat.
[00:02:33.640 --> 00:02:36.160]   And it's very hard for me to have
[00:02:36.160 --> 00:02:38.760]   a look at these multiple places to ask questions
[00:02:38.760 --> 00:02:39.880]   towards the end.
[00:02:39.880 --> 00:02:41.640]   So when it's time to ask these questions,
[00:02:41.640 --> 00:02:45.600]   we'll go to this URL.
[00:02:45.600 --> 00:02:46.360]   Cool.
[00:02:46.360 --> 00:02:50.240]   With that being said, let's get started with the Convit paper.
[00:02:50.240 --> 00:02:54.520]   So Stefan, do you want to maybe do a quick intro
[00:02:54.520 --> 00:02:58.800]   about the main idea and what exactly is Convit
[00:02:58.800 --> 00:03:01.840]   and how different is it from the other vision
[00:03:01.840 --> 00:03:03.720]   transformers over there?
[00:03:03.720 --> 00:03:04.440]   Yeah, sure.
[00:03:04.440 --> 00:03:05.840]   So the main idea is pretty simple.
[00:03:05.840 --> 00:03:08.480]   It's that currently we have this sort of dilemma
[00:03:08.480 --> 00:03:11.320]   for practitioners between the convolutional networks
[00:03:11.320 --> 00:03:12.120]   and transformers.
[00:03:12.120 --> 00:03:15.920]   And they're sort of on an opposite side of the spectrum,
[00:03:15.920 --> 00:03:18.160]   the spectrum being how much inductive bias you have.
[00:03:18.160 --> 00:03:22.400]   So CNNs have very strong inductive biases, locality,
[00:03:22.400 --> 00:03:23.320]   weight sharing.
[00:03:23.320 --> 00:03:24.840]   And this makes them very effective
[00:03:24.840 --> 00:03:26.520]   when you don't have much data.
[00:03:26.520 --> 00:03:29.480]   And in some sense, you have a good prior.
[00:03:29.480 --> 00:03:31.080]   But then when you have a lot of data,
[00:03:31.080 --> 00:03:33.720]   it's generally better to learn the inductive bias
[00:03:33.720 --> 00:03:35.480]   in a data-driven manner.
[00:03:35.480 --> 00:03:39.240]   And so, yeah, basically you have this dilemma.
[00:03:39.240 --> 00:03:41.320]   And there have been some approaches
[00:03:41.320 --> 00:03:43.280]   to try and find a compromise in this dilemma,
[00:03:43.280 --> 00:03:44.920]   typically using hybrid models.
[00:03:44.920 --> 00:03:48.040]   So basically stacking some self-attention layers
[00:03:48.040 --> 00:03:49.880]   on a convolutional backbone.
[00:03:49.880 --> 00:03:51.000]   And that works really well.
[00:03:51.000 --> 00:03:53.080]   You generally have the best trade-offs
[00:03:53.080 --> 00:03:56.080]   in terms of speed accuracy.
[00:03:56.080 --> 00:03:58.400]   But the problem with this is that it sort of induces
[00:03:58.400 --> 00:04:00.400]   a choice, like you have to choose
[00:04:00.400 --> 00:04:05.040]   how many convolutional layers versus how many self-attention
[00:04:05.040 --> 00:04:05.560]   layers.
[00:04:05.560 --> 00:04:08.880]   And that's obviously a very task-dependent thing.
[00:04:08.880 --> 00:04:12.720]   And so the simple idea behind the combit
[00:04:12.720 --> 00:04:17.120]   is to basically learn how many layers should be convolutional
[00:04:17.120 --> 00:04:18.120]   from the data itself.
[00:04:18.120 --> 00:04:22.680]   So essentially start from the strong inductive bias
[00:04:22.680 --> 00:04:26.080]   configuration of a CNN in a sort of devious way, which
[00:04:26.080 --> 00:04:30.320]   is initializing transformer layers as CNN layers,
[00:04:30.320 --> 00:04:32.200]   and then giving freedom to the model
[00:04:32.200 --> 00:04:34.920]   to stay in the convolutional configuration
[00:04:34.920 --> 00:04:38.920]   if it has much data, or to learn more general content-based
[00:04:38.920 --> 00:04:42.720]   attentions if there is enough data.
[00:04:42.720 --> 00:04:43.640]   Yeah, excellent.
[00:04:43.640 --> 00:04:44.360]   Thanks for that.
[00:04:44.360 --> 00:04:47.840]   I think that really is an interesting, interesting idea.
[00:04:47.840 --> 00:04:49.440]   So you mentioned inductive biases.
[00:04:49.440 --> 00:04:52.120]   And just for the readers who aren't new to convit
[00:04:52.120 --> 00:04:53.800]   and who aren't new to vision transformers,
[00:04:53.800 --> 00:04:57.160]   we have discussed vision transformers previously before.
[00:04:57.160 --> 00:05:02.120]   But basically-- and Stefan, feel free to correct me any time.
[00:05:02.120 --> 00:05:04.320]   As we go through the paper, I'll try and summarize it
[00:05:04.320 --> 00:05:07.600]   for everybody as we go along.
[00:05:07.600 --> 00:05:09.920]   So as you said, when there's an input image--
[00:05:09.920 --> 00:05:12.880]   so let's just call this as my input image.
[00:05:12.880 --> 00:05:16.360]   And then there's this whole network.
[00:05:16.360 --> 00:05:19.760]   So in CNNs, basically a convolutional neural network,
[00:05:19.760 --> 00:05:22.960]   what you have is you have lots of CNN blocks.
[00:05:22.960 --> 00:05:24.720]   So a REST net would have four blocks
[00:05:24.720 --> 00:05:27.960]   of varying numbers of convolution neural network
[00:05:27.960 --> 00:05:31.040]   layers, which is pretty much convolution, batch norm,
[00:05:31.040 --> 00:05:31.880]   and ReLU.
[00:05:31.880 --> 00:05:37.240]   And then that's when you get the output here.
[00:05:37.240 --> 00:05:39.720]   But Stefan, as you mentioned recently,
[00:05:39.720 --> 00:05:42.000]   there's this new line of research
[00:05:42.000 --> 00:05:45.160]   that has come out, which instead of using CNNs
[00:05:45.160 --> 00:05:49.760]   throughout the whole network, uses a hybrid architecture.
[00:05:49.760 --> 00:05:52.440]   And we saw this with the vision transformer as well,
[00:05:52.440 --> 00:05:55.520]   is that the early layers are a CNN.
[00:05:55.520 --> 00:05:57.720]   And the later layers are attention.
[00:05:57.720 --> 00:06:02.120]   So far so good.
[00:06:02.120 --> 00:06:05.520]   So please feel free to interrupt me, Stefan.
[00:06:05.520 --> 00:06:10.120]   But with Convid, how Convid is different
[00:06:10.120 --> 00:06:14.120]   is that these early layers--
[00:06:14.120 --> 00:06:18.040]   these early layers-- in all the other architectures out there,
[00:06:18.040 --> 00:06:22.480]   when we have a look, it's pretty much a common pattern
[00:06:22.480 --> 00:06:25.040]   in the recent research, is that you
[00:06:25.040 --> 00:06:28.080]   have the early layers as convolutional layers,
[00:06:28.080 --> 00:06:30.640]   and then the later layers as attention layers.
[00:06:30.640 --> 00:06:33.120]   But the way how Convid is different
[00:06:33.120 --> 00:06:42.000]   is that your early layers are something called a GPCA layers.
[00:06:42.000 --> 00:06:44.280]   And they have the option by themselves
[00:06:44.280 --> 00:06:46.960]   whether they want to stay CNN or they
[00:06:46.960 --> 00:06:49.680]   want to go towards the attention side of things.
[00:06:49.680 --> 00:06:51.320]   So we'll also be discussing what's
[00:06:51.320 --> 00:06:53.280]   the difference in the two and why
[00:06:53.280 --> 00:06:55.160]   one is better than the other.
[00:06:55.160 --> 00:06:58.560]   But so far, is that a correct understanding
[00:06:58.560 --> 00:07:00.760]   in terms of the abstract of the paper?
[00:07:00.760 --> 00:07:02.760]   Yeah, absolutely.
[00:07:02.760 --> 00:07:03.480]   Excellent.
[00:07:03.480 --> 00:07:05.800]   So we'll just quickly go through the abstract.
[00:07:05.800 --> 00:07:08.160]   Convolution architectures, as we know,
[00:07:08.160 --> 00:07:12.080]   have proven extremely useful for vision tasks.
[00:07:12.080 --> 00:07:16.720]   They have something called a inductive bias.
[00:07:16.720 --> 00:07:19.960]   And the best things about the convolution neural networks
[00:07:19.960 --> 00:07:22.400]   are they're more sample efficient,
[00:07:22.400 --> 00:07:25.080]   but they come at the cost of potentially lower performance
[00:07:25.080 --> 00:07:26.000]   ceiling.
[00:07:26.000 --> 00:07:28.720]   So over here, Stefan, does that mean
[00:07:28.720 --> 00:07:33.920]   that if we train for longer with more data,
[00:07:33.920 --> 00:07:37.400]   would the vision transformers have better performance
[00:07:37.400 --> 00:07:40.440]   compared to the convolutional neural networks?
[00:07:40.440 --> 00:07:44.040]   Yes, there is a kind of war currently for state of the art.
[00:07:44.040 --> 00:07:46.080]   And it's hard to follow because every day there's
[00:07:46.080 --> 00:07:48.360]   a new model that beats the state of the art.
[00:07:48.360 --> 00:07:50.560]   So yeah, you have these NF nets, which
[00:07:50.560 --> 00:07:52.200]   are the best available CNNs, which
[00:07:52.200 --> 00:07:55.560]   are pretty much on par with the deep transformers and stuff
[00:07:55.560 --> 00:07:56.040]   like that.
[00:07:56.040 --> 00:07:57.240]   So it's hard to say who's--
[00:07:57.240 --> 00:08:01.280]   but generally speaking, a model with less inductive bias
[00:08:01.280 --> 00:08:02.920]   will generally be more powerful when
[00:08:02.920 --> 00:08:04.520]   you have infinite amount of data.
[00:08:04.520 --> 00:08:06.080]   It's going to learn the data and be
[00:08:06.080 --> 00:08:09.840]   inductive bias in an autonomous way.
[00:08:09.840 --> 00:08:10.360]   Thank you.
[00:08:10.360 --> 00:08:11.280]   Yeah, that is correct.
[00:08:11.280 --> 00:08:11.780]   I do agree.
[00:08:11.780 --> 00:08:15.280]   Every week, somehow there's a new paper coming out.
[00:08:15.280 --> 00:08:18.040]   And I recently saw this Codenet or this Kite
[00:08:18.040 --> 00:08:19.720]   or there's all these different papers that
[00:08:19.720 --> 00:08:21.520]   keep coming out in Transformers World
[00:08:21.520 --> 00:08:24.160]   and also then on the convolution side of things
[00:08:24.160 --> 00:08:27.200]   where it's hard to keep up as which one's going to eventually
[00:08:27.200 --> 00:08:30.400]   have a better performance.
[00:08:30.400 --> 00:08:33.680]   So in this, I think just for the audience who are new
[00:08:33.680 --> 00:08:35.640]   to the paper and haven't read it,
[00:08:35.640 --> 00:08:37.680]   the main thing that gets introduced as well
[00:08:37.680 --> 00:08:40.520]   as one of those is the gated positional self-attention,
[00:08:40.520 --> 00:08:44.960]   the GPSA layer, which as I've mentioned,
[00:08:44.960 --> 00:08:49.720]   we can have a GPSA layer like this.
[00:08:49.720 --> 00:08:52.040]   So at this point, then, the main idea
[00:08:52.040 --> 00:08:55.360]   is to initialize the GPSA layers to mimic the locality
[00:08:55.360 --> 00:08:57.520]   of convolution layers.
[00:08:57.520 --> 00:09:00.880]   From my understanding, I guess, the understanding here
[00:09:00.880 --> 00:09:03.360]   is that these GPCA layers at beginning,
[00:09:03.360 --> 00:09:07.280]   at the initialization, because we know from past research
[00:09:07.280 --> 00:09:10.840]   that it really helps to have the earlier layers as CNNs
[00:09:10.840 --> 00:09:14.280]   and the later layers as self-attention layers.
[00:09:14.280 --> 00:09:18.120]   Then based on my understanding, the GPSA layers
[00:09:18.120 --> 00:09:21.040]   at the beginning get initialized like a CNN,
[00:09:21.040 --> 00:09:25.240]   sort of start behaving like convolutional neural networks
[00:09:25.240 --> 00:09:26.720]   and convolutional layers.
[00:09:26.720 --> 00:09:29.360]   And then the later ones are just plain attention.
[00:09:29.360 --> 00:09:31.480]   Is that a correct understanding?
[00:09:31.480 --> 00:09:32.000]   Yeah.
[00:09:32.000 --> 00:09:33.400]   Yeah.
[00:09:33.400 --> 00:09:38.640]   Well, actually, to be honest, the architecture we use
[00:09:38.640 --> 00:09:39.800]   is actually 12 layers.
[00:09:39.800 --> 00:09:42.480]   And pretty much, you could, in principle,
[00:09:42.480 --> 00:09:44.800]   use all the GPSA layers all along.
[00:09:44.800 --> 00:09:47.560]   The reason why we use 10 GPSA layers and then
[00:09:47.560 --> 00:09:49.800]   two normal self-attention layers is
[00:09:49.800 --> 00:09:52.200]   because it's easier to handle the class token
[00:09:52.200 --> 00:09:55.720]   with usual self-attention.
[00:09:55.720 --> 00:09:59.080]   But in principle, you could use GPSA layers all along.
[00:09:59.080 --> 00:10:01.720]   Essentially, they're deciding all by themselves
[00:10:01.720 --> 00:10:03.120]   to stay convolutional or not.
[00:10:03.120 --> 00:10:04.760]   So yeah.
[00:10:04.760 --> 00:10:05.240]   Right.
[00:10:05.240 --> 00:10:06.560]   So you mentioned the class token.
[00:10:06.560 --> 00:10:09.040]   So by that, you mean--
[00:10:09.040 --> 00:10:12.080]   just as a recap for everybody, what happens
[00:10:12.080 --> 00:10:14.200]   is in terms of a transformer world--
[00:10:14.200 --> 00:10:15.840]   and this is just a vision transformer--
[00:10:15.840 --> 00:10:19.680]   is you have an input image that then gets divided into patches.
[00:10:19.680 --> 00:10:22.160]   And something that gets added to these patches
[00:10:22.160 --> 00:10:26.320]   is this class token, which is this extra learnable class
[00:10:26.320 --> 00:10:27.000]   embedding.
[00:10:27.000 --> 00:10:28.720]   So what Stefan here is, I believe,
[00:10:28.720 --> 00:10:31.760]   he's referring to is that that class token is
[00:10:31.760 --> 00:10:34.640]   easier to handle in these later layers of the attention.
[00:10:34.640 --> 00:10:36.080]   Is that correct?
[00:10:36.080 --> 00:10:36.800]   Yeah, absolutely.
[00:10:36.800 --> 00:10:37.280]   Yeah.
[00:10:37.280 --> 00:10:38.920]   We'll probably explain the reason later.
[00:10:38.920 --> 00:10:42.000]   But the difference is due to the fact
[00:10:42.000 --> 00:10:44.920]   that GPSA does require positional attention,
[00:10:44.920 --> 00:10:47.400]   whereas the usual self-attention used in the bit
[00:10:47.400 --> 00:10:50.080]   only uses content attention.
[00:10:50.080 --> 00:10:50.760]   Yep.
[00:10:50.760 --> 00:10:55.600]   We'll definitely get to that point.
[00:10:55.600 --> 00:10:56.840]   Cool.
[00:10:56.840 --> 00:10:58.920]   So I guess that's the main idea.
[00:10:58.920 --> 00:11:01.160]   The main idea is we try--
[00:11:01.160 --> 00:11:02.840]   in this paper as well, in Convid,
[00:11:02.840 --> 00:11:06.080]   we see that it is another one of those papers that have come out
[00:11:06.080 --> 00:11:07.360]   using vision transformers.
[00:11:07.360 --> 00:11:12.160]   But this idea of having this layer, which initially
[00:11:12.160 --> 00:11:16.000]   acts like a CNN, and then it's up to the layer on itself.
[00:11:16.000 --> 00:11:17.640]   It's not something that's hard-coded,
[00:11:17.640 --> 00:11:21.120]   because lots of networks, they hard-code this.
[00:11:21.120 --> 00:11:24.280]   And they make this as a requirement
[00:11:24.280 --> 00:11:25.720]   is that the earlier layers are CNN
[00:11:25.720 --> 00:11:27.200]   and the later layers are attention.
[00:11:27.200 --> 00:11:32.240]   And this is how this Convid paper is just so different
[00:11:32.240 --> 00:11:35.560]   and one of the, I believe, one of the most interesting lines
[00:11:35.560 --> 00:11:38.120]   of research in recent papers.
[00:11:38.120 --> 00:11:38.640]   Yeah.
[00:11:38.640 --> 00:11:40.360]   Maybe I can add something before we
[00:11:40.360 --> 00:11:42.480]   go into the details, which we don't really
[00:11:42.480 --> 00:11:43.400]   discuss in the paper.
[00:11:43.920 --> 00:11:47.600]   Maybe one advantage and one drawback
[00:11:47.600 --> 00:11:49.520]   that we haven't mentioned in the paper.
[00:11:49.520 --> 00:11:51.880]   So one easy criticism you can make of the Convid
[00:11:51.880 --> 00:11:54.720]   is that it's a bit of a waste for the layers which
[00:11:54.720 --> 00:11:56.840]   do remain convolutional in the end
[00:11:56.840 --> 00:11:59.000]   to be parameterized as self-attention layers.
[00:11:59.000 --> 00:12:02.800]   Because clearly, convolutions are much more efficient
[00:12:02.800 --> 00:12:03.680]   computationally.
[00:12:03.680 --> 00:12:06.160]   And so you could say it's a bit of a waste
[00:12:06.160 --> 00:12:09.720]   to have an attention layer acting as a CNN.
[00:12:09.720 --> 00:12:13.040]   However, there is one thing that I
[00:12:13.040 --> 00:12:17.560]   think is better in Convid than in hybrid models.
[00:12:17.560 --> 00:12:20.080]   And perhaps it explains the good performance
[00:12:20.080 --> 00:12:25.080]   is the fact that typically, the optimizers and hyperparameters
[00:12:25.080 --> 00:12:28.440]   you use for CNNs are quite different from those
[00:12:28.440 --> 00:12:30.160]   that you use for transformers.
[00:12:30.160 --> 00:12:31.720]   For example, for transformers, you're
[00:12:31.720 --> 00:12:34.280]   typically going to use adaptive optimizers like Adam.
[00:12:34.280 --> 00:12:37.760]   Whereas for CNNs, you generally use the SGD momentum.
[00:12:37.760 --> 00:12:41.440]   And so that's, I think, incurs probably some limitations
[00:12:41.440 --> 00:12:46.200]   in hybrid models where you can't use a hybrid optimizer.
[00:12:46.200 --> 00:12:50.040]   So in this sense, the Convid is a unified framework
[00:12:50.040 --> 00:12:54.560]   between convolution and transformers.
[00:12:54.560 --> 00:12:55.880]   That really does make sense.
[00:12:55.880 --> 00:12:58.080]   And I think that's also a good point
[00:12:58.080 --> 00:13:00.960]   to think of in terms of drawback of the GPSA
[00:13:00.960 --> 00:13:03.800]   that if in the end, it's going to--
[00:13:03.800 --> 00:13:05.520]   and from experiments, if we know that it's
[00:13:05.520 --> 00:13:07.360]   going to stay convolution, then why
[00:13:07.360 --> 00:13:10.320]   should we have all these extra parameters that it
[00:13:10.320 --> 00:13:12.680]   starts with self-attention?
[00:13:12.680 --> 00:13:13.400]   Cool.
[00:13:13.400 --> 00:13:17.520]   I think that's something that we'll discuss for everybody.
[00:13:17.520 --> 00:13:19.320]   We'll discuss as we go through the paper.
[00:13:19.320 --> 00:13:21.520]   And if there's anything that you don't understand,
[00:13:21.520 --> 00:13:23.920]   please feel free to keep posting your questions.
[00:13:23.920 --> 00:13:25.960]   We will address them as we go.
[00:13:25.960 --> 00:13:32.400]   So in terms of introduction of the paper, the introduction--
[00:13:32.400 --> 00:13:35.080]   and this is all based on my understanding--
[00:13:35.080 --> 00:13:37.520]   the introduction just mentions that there's
[00:13:37.520 --> 00:13:39.160]   pretty much two types of--
[00:13:39.160 --> 00:13:41.680]   either there's two types of research.
[00:13:41.680 --> 00:13:44.880]   And one of the main things that this introduction says
[00:13:44.880 --> 00:13:47.920]   is that the inductive biases are hard-coded
[00:13:47.920 --> 00:13:50.880]   into the architectural structure of CNNs.
[00:13:50.880 --> 00:13:53.320]   We have locality, and we have weight sharing.
[00:13:53.320 --> 00:13:59.160]   So I think at this point, it might make sense to actually
[00:13:59.160 --> 00:13:59.880]   just--
[00:13:59.880 --> 00:14:03.120]   Stefan, if you do agree, maybe we spend just two or five minutes
[00:14:03.120 --> 00:14:06.240]   just looking at the basic differences of what
[00:14:06.240 --> 00:14:10.400]   exactly a convolution is and how exactly are
[00:14:10.400 --> 00:14:12.080]   these inductive biases hard-coded
[00:14:12.080 --> 00:14:15.880]   and then what exactly is attention.
[00:14:15.880 --> 00:14:19.480]   So if I go in this, I do have an image, a very small image,
[00:14:19.480 --> 00:14:22.000]   that might help explain this idea.
[00:14:22.000 --> 00:14:23.720]   So in a convolution--
[00:14:23.720 --> 00:14:27.640]   and Stefan, please feel free to correct me, as I've said.
[00:14:27.640 --> 00:14:31.200]   So you have this input image.
[00:14:31.200 --> 00:14:33.440]   Let's say it's a very small image.
[00:14:33.440 --> 00:14:38.560]   Actually, it might need a little bit bigger.
[00:14:38.560 --> 00:14:42.880]   So let's just say that's just one small part of the image.
[00:14:42.880 --> 00:14:51.240]   So then if this is my convolution kernel--
[00:14:51.240 --> 00:14:54.360]   let's say this is a 3 by 3 convolution kernel.
[00:14:54.360 --> 00:15:03.400]   So it's a 3 by 3, and my input image is, say, 2 to 4 by 2
[00:15:03.400 --> 00:15:04.680]   to 4.
[00:15:04.680 --> 00:15:07.560]   Then something that happens in 2D convolutions
[00:15:07.560 --> 00:15:13.240]   is that this convolution kernel will go pretty much like that
[00:15:13.240 --> 00:15:15.160]   all over the image.
[00:15:15.160 --> 00:15:17.640]   And I guess one of the main things
[00:15:17.640 --> 00:15:21.000]   is that these weights that get learned for the convolution
[00:15:21.000 --> 00:15:23.840]   kernels, they are sort of shared throughout all
[00:15:23.840 --> 00:15:25.360]   over the image.
[00:15:25.360 --> 00:15:27.080]   Is that a good understanding?
[00:15:27.080 --> 00:15:29.560]   And then is that why you mentioned in the paper
[00:15:29.560 --> 00:15:34.040]   that it's sort of like a hard inductive bias in convolution?
[00:15:34.040 --> 00:15:34.680]   Yeah.
[00:15:34.680 --> 00:15:36.280]   So that's the weight sharing constraints.
[00:15:36.280 --> 00:15:37.840]   And then, of course, there's the fact
[00:15:37.840 --> 00:15:40.240]   that the convolutional kernel is only 3 by 3.
[00:15:40.240 --> 00:15:44.360]   So it can't directly extract the long range dependencies.
[00:15:44.360 --> 00:15:45.120]   Yeah.
[00:15:45.120 --> 00:15:47.240]   So in that one, I guess the point is
[00:15:47.240 --> 00:15:50.360]   that when we have a layer 2, which after,
[00:15:50.360 --> 00:15:54.520]   if we apply a convolution to this layer 1--
[00:15:54.520 --> 00:15:56.520]   so let's say the layer 1 is the image--
[00:15:56.520 --> 00:15:59.440]   then at a certain point if the convolution kernel is only 3
[00:15:59.440 --> 00:16:01.920]   by 3, then this particular--
[00:16:01.920 --> 00:16:03.440]   it's called a receptive field.
[00:16:03.440 --> 00:16:04.960]   And in terms of the receptive field,
[00:16:04.960 --> 00:16:08.120]   it can only have a look at this part of the image.
[00:16:08.120 --> 00:16:10.400]   So all of this on the other side,
[00:16:10.400 --> 00:16:11.880]   all of the other part of the image
[00:16:11.880 --> 00:16:14.760]   is never looked at by this particular pixel
[00:16:14.760 --> 00:16:16.360]   of this second layer.
[00:16:16.360 --> 00:16:19.000]   So it doesn't really have any visibility
[00:16:19.000 --> 00:16:21.560]   to the other side of the image.
[00:16:21.560 --> 00:16:24.680]   This thing is something that gets taken away
[00:16:24.680 --> 00:16:26.560]   in terms of an attention.
[00:16:26.560 --> 00:16:31.920]   So the way I like to think of attention
[00:16:31.920 --> 00:16:35.240]   and the way I like to think how this is different
[00:16:35.240 --> 00:16:41.160]   is that if I have an input image x and I have three matrices,
[00:16:41.160 --> 00:16:45.160]   I have my, let's say, my weight query, my weight k,
[00:16:45.160 --> 00:16:47.200]   and my weight value.
[00:16:47.200 --> 00:16:50.880]   When these three get multiplied, I get three matrices again.
[00:16:50.880 --> 00:16:55.160]   So this one I'm going to call q.
[00:16:55.160 --> 00:16:56.600]   This one I'm going to call k.
[00:16:56.600 --> 00:16:58.760]   And this one I'm going to call v.
[00:16:58.760 --> 00:17:07.640]   How this is different is that in attention,
[00:17:07.640 --> 00:17:11.280]   if this is the pixel that I'm trying to look at,
[00:17:11.280 --> 00:17:16.320]   then everything else around the image becomes my query.
[00:17:16.320 --> 00:17:20.840]   So this pixel, if this becomes my query,
[00:17:20.840 --> 00:17:23.040]   everything around becomes the key.
[00:17:23.040 --> 00:17:24.520]   And then we find--
[00:17:24.520 --> 00:17:26.000]   let's just think of it this way.
[00:17:26.000 --> 00:17:28.280]   We find a weighted combination.
[00:17:28.280 --> 00:17:29.960]   So if the value--
[00:17:29.960 --> 00:17:32.600]   this is how this understanding of attention goes.
[00:17:32.600 --> 00:17:36.560]   That let's say the value is the actual--
[00:17:36.560 --> 00:17:38.680]   in terms of understanding is the representation
[00:17:38.680 --> 00:17:41.480]   of this input image.
[00:17:41.480 --> 00:17:44.160]   So this is what represents this input image.
[00:17:44.160 --> 00:17:46.840]   And when we're trying to look at a particular part of the image,
[00:17:46.840 --> 00:17:50.680]   let's say over here, we want to understand how much attention
[00:17:50.680 --> 00:17:53.880]   or how much stress does this particular pixel need
[00:17:53.880 --> 00:17:56.720]   to look at all the other parts of the image
[00:17:56.720 --> 00:18:00.880]   to then come up with.
[00:18:00.880 --> 00:18:02.680]   So if there's an image of a dog here
[00:18:02.680 --> 00:18:04.560]   and there's an image of a dog over here,
[00:18:04.560 --> 00:18:08.320]   then these two would have a higher attention value
[00:18:08.320 --> 00:18:10.880]   because then it's important to know that this picture could
[00:18:10.880 --> 00:18:12.400]   that be of dogs.
[00:18:12.400 --> 00:18:14.600]   So in that sense, then, what you have
[00:18:14.600 --> 00:18:16.840]   is you have your query and your key.
[00:18:16.840 --> 00:18:19.000]   So every other pixel becomes the key.
[00:18:19.000 --> 00:18:20.920]   And then you have a weighted combination
[00:18:20.920 --> 00:18:24.320]   of these, which is called attention.
[00:18:24.320 --> 00:18:26.600]   And you multiply that by the value
[00:18:26.600 --> 00:18:28.080]   to get your final output.
[00:18:28.080 --> 00:18:29.800]   So the way how this is different is
[00:18:29.800 --> 00:18:31.760]   that there's no receptive field.
[00:18:31.760 --> 00:18:36.840]   Like when we have a convolution, this one particular pixel
[00:18:36.840 --> 00:18:40.920]   can only have a look at this 3 by 3 if it's a 3 by 3 kernel.
[00:18:40.920 --> 00:18:44.920]   But this sort of constraint is taken away in attention.
[00:18:44.920 --> 00:18:46.640]   Is that correct, Stefan?
[00:18:46.640 --> 00:18:50.080]   Is there something you want to add to this?
[00:18:50.080 --> 00:18:53.120]   Actually, in attention, you also have weight sharing
[00:18:53.120 --> 00:18:55.520]   in some sense because you use the same Q and K
[00:18:55.520 --> 00:18:57.040]   for different positions.
[00:18:57.040 --> 00:18:59.440]   So yeah, it's really the receptive field,
[00:18:59.440 --> 00:19:00.640]   which is widened.
[00:19:00.640 --> 00:19:03.520]   And the fact that you, in some sense,
[00:19:03.520 --> 00:19:06.320]   the filters are dynamically generated for attention,
[00:19:06.320 --> 00:19:10.600]   whereas for CNNs, they're fixed.
[00:19:10.600 --> 00:19:12.440]   Thank you.
[00:19:12.440 --> 00:19:16.520]   With that being said, I think the reason why I quickly
[00:19:16.520 --> 00:19:20.120]   wanted to touch in this is because as we go along,
[00:19:20.120 --> 00:19:21.840]   this understanding of--
[00:19:21.840 --> 00:19:25.120]   at least for me as well when I was first reading the paper--
[00:19:25.120 --> 00:19:28.240]   this understanding of just understanding that a CNN is
[00:19:28.240 --> 00:19:31.320]   somewhat local and has these hard inductive biases,
[00:19:31.320 --> 00:19:33.240]   whereas attention is not.
[00:19:33.240 --> 00:19:37.720]   It really helped me as we will go through the paper.
[00:19:38.720 --> 00:19:40.760]   So this question that--
[00:19:40.760 --> 00:19:44.080]   recently, there's been more and more papers
[00:19:44.080 --> 00:19:46.000]   that are based purely on attention.
[00:19:46.000 --> 00:19:49.560]   So the whole network, there's no convolutional part to it.
[00:19:49.560 --> 00:19:51.200]   The whole paper is based on attention.
[00:19:51.200 --> 00:19:52.840]   And this brings into the question,
[00:19:52.840 --> 00:19:55.320]   do we really need these hard-coded inductive biases
[00:19:55.320 --> 00:19:57.440]   that we just discussed in terms of weight sharing
[00:19:57.440 --> 00:19:59.160]   and the receptive field?
[00:20:06.160 --> 00:20:09.960]   So over here, I see, Stefan, is this just a prior reference
[00:20:09.960 --> 00:20:13.960]   that says we know that we found from past research
[00:20:13.960 --> 00:20:16.520]   that it really works if the earlier layers are
[00:20:16.520 --> 00:20:19.040]   convolutional and the later layers are attention?
[00:20:19.040 --> 00:20:23.040]   Excellent.
[00:20:23.040 --> 00:20:23.560]   Cool.
[00:20:23.560 --> 00:20:26.760]   So then I guess at this point, then we
[00:20:26.760 --> 00:20:30.080]   start happy to look into the soft inductive biases.
[00:20:30.080 --> 00:20:33.720]   And we're happy to start looking at the soft inductive biases
[00:20:33.720 --> 00:20:36.360]   and we're happy to start going forward.
[00:20:36.360 --> 00:20:41.760]   But is there anything else that you want to add so far, Stefan?
[00:20:41.760 --> 00:20:44.440]   I think it's a good summary.
[00:20:44.440 --> 00:20:47.280]   OK, excellent.
[00:20:47.280 --> 00:20:51.160]   So then I guess when I was reading this paper,
[00:20:51.160 --> 00:20:53.040]   I sort of understood this in a way
[00:20:53.040 --> 00:20:56.240]   that a convolutional neural network has
[00:20:56.240 --> 00:20:59.360]   these hard inductive biases that actually
[00:20:59.360 --> 00:21:02.760]   are part of the convolutional kernel,
[00:21:02.760 --> 00:21:05.760]   like because of the weight sharing and these things.
[00:21:05.760 --> 00:21:10.960]   Then I guess one of the questions that
[00:21:10.960 --> 00:21:17.120]   get asked here, one of the questions
[00:21:17.120 --> 00:21:20.360]   is that these things forcefully induce
[00:21:20.360 --> 00:21:23.600]   the convolutional inductive biases into transformers,
[00:21:23.600 --> 00:21:27.200]   potentially affecting the transformer with the limitations.
[00:21:27.200 --> 00:21:28.840]   So sorry, when I say these things,
[00:21:28.880 --> 00:21:33.480]   let's go into this a little bit more.
[00:21:33.480 --> 00:21:37.640]   So because we know that from past research,
[00:21:37.640 --> 00:21:43.000]   one of the best ways is to sort of combine a CNN and attention
[00:21:43.000 --> 00:21:46.360]   and then CNN in the early layers and attention in the later
[00:21:46.360 --> 00:21:47.280]   layers.
[00:21:47.280 --> 00:21:49.080]   There's basically two ways.
[00:21:49.080 --> 00:21:50.840]   One is the hybrid model.
[00:21:50.840 --> 00:21:53.800]   And the second one is knowledge distillation
[00:21:53.800 --> 00:21:55.440]   that got used in diet.
[00:21:55.440 --> 00:21:58.480]   Let me see if I can find where it says-- oh, here it is.
[00:21:58.480 --> 00:21:59.880]   Did you want to--
[00:21:59.880 --> 00:22:02.160]   Stefan, did you want to introduce everybody
[00:22:02.160 --> 00:22:05.800]   to what these two approaches are and how they're different?
[00:22:05.800 --> 00:22:06.600]   Yeah.
[00:22:06.600 --> 00:22:08.640]   So as we said, hybrid models is really just
[00:22:08.640 --> 00:22:13.360]   stacking self-attention layers onto the convolutional layers.
[00:22:13.360 --> 00:22:15.600]   Knowledge distillation is a bit different.
[00:22:15.600 --> 00:22:18.280]   The way they use it in transformer papers
[00:22:18.280 --> 00:22:23.520]   is that essentially you have a teacher, which is typically
[00:22:23.520 --> 00:22:25.840]   here going to be a convolutional network, which is going
[00:22:25.840 --> 00:22:27.480]   to give you some soft labels.
[00:22:27.480 --> 00:22:30.640]   So it's not going to give you hard labels as in one
[00:22:30.640 --> 00:22:31.800]   hot encoded vectors.
[00:22:31.800 --> 00:22:33.440]   It's not going to say this is a dog.
[00:22:33.440 --> 00:22:38.480]   It's going to say this is 98% dog, 94%, et cetera.
[00:22:38.480 --> 00:22:42.680]   Well, actually, that's how knowledge distillation usually
[00:22:42.680 --> 00:22:43.280]   works.
[00:22:43.280 --> 00:22:45.360]   What's surprising is that in the ways you generally
[00:22:45.360 --> 00:22:48.320]   use transformers, the teacher actually gives hard labels.
[00:22:48.320 --> 00:22:53.040]   And that seems to be enough to give the student
[00:22:53.040 --> 00:22:55.560]   strong inductive bias.
[00:22:55.560 --> 00:22:57.360]   And actually, we observed this in our paper.
[00:22:57.360 --> 00:22:59.280]   I think it's in an appendix.
[00:22:59.280 --> 00:23:01.960]   We showed that the student which learns
[00:23:01.960 --> 00:23:04.440]   from a convolutional teacher, the student transformer which
[00:23:04.440 --> 00:23:08.400]   learns from a convolutional teacher
[00:23:08.400 --> 00:23:14.400]   does have a more local inductive bias at the end of training,
[00:23:14.400 --> 00:23:15.960]   which is very surprising.
[00:23:15.960 --> 00:23:18.200]   How can you get a convolutional inductive bias just out
[00:23:18.200 --> 00:23:19.760]   of hard labels?
[00:23:19.760 --> 00:23:23.280]   It's very surprising, but that seems to be the case.
[00:23:23.280 --> 00:23:24.920]   Yeah, it does seem surprising.
[00:23:24.920 --> 00:23:28.760]   And I think there was this recent paper that came out
[00:23:28.760 --> 00:23:30.960]   from Google research about knowledge distillation
[00:23:30.960 --> 00:23:33.880]   that a teacher should be patient.
[00:23:33.880 --> 00:23:37.560]   And they use soft inductive biases,
[00:23:37.560 --> 00:23:39.200]   but that's, again, from a convolutional
[00:23:39.200 --> 00:23:42.960]   to a convolutional student teacher process,
[00:23:42.960 --> 00:23:46.360]   not to a transformer instead.
[00:23:46.360 --> 00:23:49.000]   But I guess then that's the main idea.
[00:23:49.000 --> 00:23:51.000]   And this is discussed in, I believe,
[00:23:51.000 --> 00:23:53.960]   the diet data efficient image transformer.
[00:23:53.960 --> 00:23:58.400]   That was one of the first papers to then bring this distillation
[00:23:58.400 --> 00:24:00.680]   token and bring knowledge distillation
[00:24:00.680 --> 00:24:03.480]   to image transformers.
[00:24:03.480 --> 00:24:07.480]   But the idea is to have these-- what
[00:24:07.480 --> 00:24:10.760]   these hybrid and knowledge distillation models do is--
[00:24:10.760 --> 00:24:13.200]   with vision transformers, one thing that we've seen
[00:24:13.200 --> 00:24:17.320]   is that they have to have this really long pre-training
[00:24:17.320 --> 00:24:19.440]   on a lot of data.
[00:24:19.440 --> 00:24:22.960]   So for example, the initial vision transformer--
[00:24:22.960 --> 00:24:27.160]   Stefan, correct me, but was it the GFD 300 million images
[00:24:27.160 --> 00:24:30.000]   that it was trained on?
[00:24:30.000 --> 00:24:31.800]   Yeah, so that's a lot of pre-training.
[00:24:31.800 --> 00:24:34.000]   And then is that correct that what
[00:24:34.000 --> 00:24:36.200]   these approaches of having a hybrid and knowledge
[00:24:36.200 --> 00:24:38.680]   distillation do is they try and take away
[00:24:38.680 --> 00:24:41.720]   that requirement of having really long pre-training
[00:24:41.720 --> 00:24:44.440]   and having these really expensive compute
[00:24:44.440 --> 00:24:47.560]   requirements so you can actually still not
[00:24:47.560 --> 00:24:49.720]   have to train on 300 million images?
[00:24:49.720 --> 00:24:50.920]   Yeah, absolutely.
[00:24:50.920 --> 00:24:53.600]   And to me, the knowledge distillation is sort of--
[00:24:53.600 --> 00:24:56.360]   it's not ideal in the sense that, well, for two reasons.
[00:24:56.360 --> 00:24:58.400]   First of all, you require a teacher.
[00:24:58.400 --> 00:25:00.440]   So you have to have a forward pass of the teacher
[00:25:00.440 --> 00:25:04.040]   each time at each training step to get the correct label.
[00:25:04.040 --> 00:25:06.560]   And also, it's a bit surprising that it works well
[00:25:06.560 --> 00:25:09.400]   with these hard labels because essentially, the best you
[00:25:09.400 --> 00:25:10.480]   can get from the teacher--
[00:25:10.480 --> 00:25:12.400]   I mean, all you can get from the teacher
[00:25:12.400 --> 00:25:14.680]   is basically a wrong label.
[00:25:14.680 --> 00:25:16.600]   Like, either the teacher gives the right label
[00:25:16.600 --> 00:25:19.360]   and then you already had the token of the right label,
[00:25:19.360 --> 00:25:21.120]   or it gives you a wrong label.
[00:25:21.120 --> 00:25:22.800]   And although that seems to ease training,
[00:25:22.800 --> 00:25:26.880]   it's actually giving you bad information.
[00:25:26.880 --> 00:25:31.600]   So to me, it's not an ideal practice,
[00:25:31.600 --> 00:25:34.200]   despite its empirical success.
[00:25:34.200 --> 00:25:35.800]   I'm completely with you on this.
[00:25:35.800 --> 00:25:39.200]   And I think one of the questions I've had as well is-- so maybe
[00:25:39.200 --> 00:25:42.280]   it might make sense that we just spend five minutes discussing
[00:25:42.280 --> 00:25:44.480]   this distillation side of things and why it's not
[00:25:44.480 --> 00:25:45.920]   a good approach is--
[00:25:45.920 --> 00:25:52.160]   I think you mentioned that it's possible that the only thing
[00:25:52.160 --> 00:25:54.120]   it can do is give a wrong label.
[00:25:54.120 --> 00:25:57.080]   So basically, I guess, just to make things a little bit clear
[00:25:57.080 --> 00:26:00.760]   is when you have your input image, then if, let's say,
[00:26:00.760 --> 00:26:03.840]   you have 1 million such images, then you also
[00:26:03.840 --> 00:26:05.680]   have 1 million corresponding labels.
[00:26:05.680 --> 00:26:07.800]   So this could be a label 2, and the last one
[00:26:07.800 --> 00:26:10.520]   could be a label 10, where 2 could mean that of a fish
[00:26:10.520 --> 00:26:11.880]   and 10 could mean that of a dog.
[00:26:11.880 --> 00:26:13.800]   That's just the categories.
[00:26:13.800 --> 00:26:15.560]   So something that happens is--
[00:26:15.560 --> 00:26:17.680]   so these becomes my true labels.
[00:26:17.680 --> 00:26:21.040]   So this class token is trying to minimize
[00:26:21.040 --> 00:26:24.200]   the loss between basically the class embedding
[00:26:24.200 --> 00:26:28.480]   and this true labels, whereas this distillation token
[00:26:28.480 --> 00:26:31.800]   is trying to minimize this loss, again, the cross-entropy loss
[00:26:31.800 --> 00:26:35.840]   between this sort of--
[00:26:35.840 --> 00:26:40.760]   so you have a teacher model.
[00:26:40.760 --> 00:26:43.440]   Let's say this teacher model is something that's really massive,
[00:26:43.440 --> 00:26:45.680]   really big, and it's really good.
[00:26:45.680 --> 00:26:49.840]   It might have like 89%, I think, previously people
[00:26:49.840 --> 00:26:52.440]   in papers that have said--
[00:26:52.440 --> 00:26:53.360]   they've used RegNet.
[00:26:53.360 --> 00:26:56.080]   And what you do is then for all of these,
[00:26:56.080 --> 00:26:58.920]   you make predictions using the teacher model.
[00:26:58.920 --> 00:27:01.800]   So you get another set of not the true labels,
[00:27:01.800 --> 00:27:04.440]   but let's just call them pseudo labels or something.
[00:27:04.440 --> 00:27:06.440]   But you also get this set of labels.
[00:27:06.440 --> 00:27:10.000]   And if you have the right labels for all of them,
[00:27:10.000 --> 00:27:12.360]   then there's no difference in the true label
[00:27:12.360 --> 00:27:13.480]   and the pseudo label.
[00:27:13.480 --> 00:27:14.960]   But the only thing that you can get
[00:27:14.960 --> 00:27:16.600]   is that the teacher can predict wrong.
[00:27:16.600 --> 00:27:19.120]   So instead of a 2, it could predict a 4.
[00:27:19.120 --> 00:27:23.600]   And this way of having these numbers associated to each
[00:27:23.600 --> 00:27:25.720]   is called a hard label.
[00:27:25.720 --> 00:27:29.520]   So I guess, does this sort of summarize well
[00:27:29.520 --> 00:27:32.880]   that this is why a teacher approach might not
[00:27:32.880 --> 00:27:35.160]   be really good is because, first, you
[00:27:35.160 --> 00:27:36.600]   need to have a teacher model which
[00:27:36.600 --> 00:27:37.840]   might work for ImageNet.
[00:27:37.840 --> 00:27:40.440]   But say if you want to work for a medical data set
[00:27:40.440 --> 00:27:42.280]   or you want to work on a different data set,
[00:27:42.280 --> 00:27:45.040]   it might not be that good a teacher model present.
[00:27:45.040 --> 00:27:48.080]   And then secondly, it can only give you wrong labels.
[00:27:48.080 --> 00:27:49.160]   So why does this work?
[00:27:49.160 --> 00:27:51.240]   That's, again, maybe a question that
[00:27:51.240 --> 00:27:54.160]   hasn't been answered properly.
[00:27:54.160 --> 00:27:56.720]   So how does Convid do it different, Stefan?
[00:27:56.720 --> 00:27:58.760]   I do have the answer, but did you
[00:27:58.760 --> 00:28:00.880]   want to take a minute and just say,
[00:28:00.880 --> 00:28:02.680]   does it not do any knowledge distillation
[00:28:02.680 --> 00:28:05.120]   or how is Convid then different?
[00:28:05.120 --> 00:28:07.080]   Yeah, so the idea is really to plug
[00:28:07.080 --> 00:28:10.000]   in the convolutional inductive bias in the architecture
[00:28:10.000 --> 00:28:16.440]   rather than sort of pull it out of the convolutional teacher.
[00:28:16.440 --> 00:28:19.800]   So I guess then it doesn't go this hybrid model part
[00:28:19.800 --> 00:28:22.320]   and it doesn't go this knowledge distillation part,
[00:28:22.320 --> 00:28:24.880]   but it goes down this path of having the earlier layers as
[00:28:24.880 --> 00:28:26.240]   GPCA--
[00:28:26.240 --> 00:28:28.560]   GPSA, I keep calling it GPCA--
[00:28:28.560 --> 00:28:31.280]   and the later layers as, say, CNN blocks.
[00:28:31.280 --> 00:28:33.360]   And as you mentioned, there's like 14 or 16,
[00:28:33.360 --> 00:28:36.120]   depending on how big the architecture is.
[00:28:36.160 --> 00:28:41.520]   And then this has the soft inductive bias instead
[00:28:41.520 --> 00:28:43.120]   of a hard inductive bias.
[00:28:43.120 --> 00:28:50.800]   And you found-- so when we were looking at this,
[00:28:50.800 --> 00:28:51.880]   this was the diet.
[00:28:51.880 --> 00:28:54.000]   Basically, this is what the diet paper did
[00:28:54.000 --> 00:28:56.240]   and they had knowledge distillation.
[00:28:56.240 --> 00:28:58.680]   And from figure two, is this something
[00:28:58.680 --> 00:29:01.400]   that you found, Stefan, is that it was performing better
[00:29:01.400 --> 00:29:03.120]   than diet and did you want to maybe shed
[00:29:03.120 --> 00:29:05.440]   some light on the experiments and the initial experiments
[00:29:05.440 --> 00:29:10.640]   that you failed and did you want to explain this image?
[00:29:10.640 --> 00:29:15.040]   The right panel is just to check that the convit still
[00:29:15.040 --> 00:29:18.680]   works well on the original tasks or image net.
[00:29:18.680 --> 00:29:21.240]   It's slightly better than the diet
[00:29:21.240 --> 00:29:24.760]   with the same hyperparameters, but that's not the key points.
[00:29:24.760 --> 00:29:27.720]   The key point about the convit is
[00:29:27.720 --> 00:29:31.000]   to sort of get the best of both worlds between the CNNs, which
[00:29:31.000 --> 00:29:33.120]   are really good in the low data regime,
[00:29:33.120 --> 00:29:35.640]   and the transformers, which are better with a lot of data.
[00:29:35.640 --> 00:29:39.120]   And as you can see, the gap between the-- on the left
[00:29:39.120 --> 00:29:42.280]   panel, you can see that the gap between the convit and the diet
[00:29:42.280 --> 00:29:46.440]   grows as you have less and less images in your training set.
[00:29:46.440 --> 00:29:49.000]   And so that's really showing that the whole point
[00:29:49.000 --> 00:29:50.480]   of these GPSA layers.
[00:29:50.480 --> 00:29:52.160]   And you also see this in the ablation
[00:29:52.160 --> 00:29:54.040]   at the end of the paper.
[00:29:54.040 --> 00:29:56.520]   When you don't have much data, the inductive biases
[00:29:56.520 --> 00:29:59.400]   really kick in and really help you get better performance.
[00:29:59.400 --> 00:30:04.880]   They kind of save you from under-overfitting the data
[00:30:04.880 --> 00:30:05.380]   set.
[00:30:05.380 --> 00:30:12.160]   Thank you for that.
[00:30:12.160 --> 00:30:16.080]   I guess then let's quickly have a look at--
[00:30:16.080 --> 00:30:18.440]   I'm trying to find where the contribution section went.
[00:30:18.440 --> 00:30:21.560]   I think it's-- because this is the recent version,
[00:30:21.560 --> 00:30:24.400]   and the one I have printed out is a different one.
[00:30:24.400 --> 00:30:25.040]   But let me see.
[00:30:25.040 --> 00:30:25.680]   Here it is.
[00:30:25.680 --> 00:30:27.560]   So then again, we've already looked at this.
[00:30:27.560 --> 00:30:30.560]   And the main contributions are this gated positional self
[00:30:30.560 --> 00:30:31.080]   attention.
[00:30:31.080 --> 00:30:35.760]   And then there's a comparison on diet.
[00:30:35.760 --> 00:30:39.320]   And then also we look at why this convit works later.
[00:30:39.320 --> 00:30:41.240]   So let's quickly get down this path.
[00:30:41.240 --> 00:30:48.560]   So I'm going to skip this mathematics of self attention.
[00:30:48.560 --> 00:30:50.640]   But it's basically-- actually, this
[00:30:50.640 --> 00:30:56.400]   is something that might be important in terms
[00:30:56.400 --> 00:30:57.800]   of the understanding.
[00:30:57.800 --> 00:30:58.920]   So I'm just going to take--
[00:30:58.920 --> 00:31:01.280]   Stefan, I'm going to take quickly a minute
[00:31:01.280 --> 00:31:04.280]   to try and explain what relative position attention is.
[00:31:04.280 --> 00:31:06.400]   But is that something that you want to do and just
[00:31:06.400 --> 00:31:12.120]   spend some time introducing this particular equation?
[00:31:12.120 --> 00:31:15.040]   I can quickly comment it.
[00:31:15.040 --> 00:31:17.800]   So this equation 4, it's unusual.
[00:31:17.800 --> 00:31:20.280]   The usual attention you have in vision transformers
[00:31:20.280 --> 00:31:24.360]   is the same equation but without the dv times r.
[00:31:24.360 --> 00:31:25.960]   And so the idea in this equation 4
[00:31:25.960 --> 00:31:27.360]   is that you have these two terms.
[00:31:27.360 --> 00:31:28.880]   The first one is the usual term.
[00:31:28.880 --> 00:31:33.240]   And basically, to evaluate the attention between a pixel i
[00:31:33.240 --> 00:31:36.840]   and pixel j, it's just going to look at their values.
[00:31:36.840 --> 00:31:39.880]   So the qi, which is the embedding of pixel--
[00:31:39.880 --> 00:31:43.640]   so not pixel-- patch i and the embedding of patch j.
[00:31:43.640 --> 00:31:47.280]   So it really depends only on their content, their pixels.
[00:31:47.280 --> 00:31:50.480]   Whereas the second term, v times rij,
[00:31:50.480 --> 00:31:52.880]   is going to depend only on their relative positions.
[00:31:52.880 --> 00:31:56.800]   And so rij is basically going to be a vector telling us
[00:31:56.800 --> 00:32:00.240]   how the patches are located with respect to each other.
[00:32:00.240 --> 00:32:04.960]   And the v is a vector which is learned by the convet
[00:32:04.960 --> 00:32:06.640]   to determine how much attention should
[00:32:06.640 --> 00:32:09.520]   be paid depending on the relative position
[00:32:09.520 --> 00:32:12.560]   of the patches.
[00:32:12.560 --> 00:32:14.720]   Thank you very much, Aoud.
[00:32:14.720 --> 00:32:17.640]   So something that we've seen in the vision transformer
[00:32:17.640 --> 00:32:21.920]   is that each patch gets this sort of a position embedding
[00:32:21.920 --> 00:32:24.040]   get associated to it.
[00:32:24.040 --> 00:32:25.720]   Because a transformer on its own--
[00:32:25.720 --> 00:32:28.400]   and I'm just trying to summarize that the transformer on its own
[00:32:28.400 --> 00:32:31.240]   doesn't really have any idea of which patch is
[00:32:31.240 --> 00:32:34.080]   at which position, which could be helpful.
[00:32:34.080 --> 00:32:37.400]   Then this idea was for attention is
[00:32:37.400 --> 00:32:39.560]   that if you're looking at this query,
[00:32:39.560 --> 00:32:43.960]   then looking at how far away the key sort of patch is,
[00:32:43.960 --> 00:32:49.320]   I think that's just a way of looking at how relative--
[00:32:49.320 --> 00:32:52.160]   that's a way of looking at relative position.
[00:32:52.160 --> 00:32:55.040]   And then, of course, if a patch is too far away,
[00:32:55.040 --> 00:32:57.920]   then is it going to have a different sort of attention
[00:32:57.920 --> 00:33:02.400]   than a patch that's right next to the query patch?
[00:33:02.400 --> 00:33:05.960]   Is that, so far, a good summary of relative position
[00:33:05.960 --> 00:33:07.400]   embeddings?
[00:33:07.400 --> 00:33:11.200]   Yeah, the big difference with the usual positional embeddings
[00:33:11.200 --> 00:33:13.760]   which are used in the VIT is that in the VIT
[00:33:13.760 --> 00:33:15.440]   you have absolute position embeddings.
[00:33:15.440 --> 00:33:19.280]   So if you have n patches, there are n of them.
[00:33:19.280 --> 00:33:22.040]   Whereas here we use relative position embeddings.
[00:33:22.040 --> 00:33:23.720]   So there are actually n squared of them.
[00:33:23.720 --> 00:33:26.120]   They model all the relative positions of two patches.
[00:33:26.120 --> 00:33:33.040]   In terms of n squared, this is something
[00:33:33.040 --> 00:33:36.360]   even I have a question about is, when you say--
[00:33:36.360 --> 00:33:40.440]   I think this comes down in the paper as well right next,
[00:33:40.440 --> 00:33:43.320]   is that in terms of when you say n squared,
[00:33:43.320 --> 00:33:45.240]   could you maybe help me also understand
[00:33:45.240 --> 00:33:48.120]   when you say it's n squared relative position embeddings
[00:33:48.120 --> 00:33:51.240]   and why that number n squared comes about?
[00:33:51.240 --> 00:33:54.880]   Yeah, the way you usually do positional embedding
[00:33:54.880 --> 00:33:57.360]   is essentially for each patch you just
[00:33:57.360 --> 00:33:59.240]   add a little vector which characterizes
[00:33:59.240 --> 00:34:00.360]   its absolute position.
[00:34:00.360 --> 00:34:02.600]   So it's going to say this patch is in top left,
[00:34:02.600 --> 00:34:04.040]   this patch in top right.
[00:34:04.040 --> 00:34:06.760]   Whereas here, what we really care about
[00:34:06.760 --> 00:34:09.000]   is not the absolute position of the patch,
[00:34:09.000 --> 00:34:12.360]   but how each patch is related to each other patch.
[00:34:12.360 --> 00:34:13.880]   And you need much more information
[00:34:13.880 --> 00:34:17.600]   to describe all the pairwise positions than just
[00:34:17.600 --> 00:34:19.200]   the absolute positions.
[00:34:19.200 --> 00:34:22.920]   So there are n squared pairs of patches.
[00:34:22.920 --> 00:34:25.360]   So that's n squared positional embeddings.
[00:34:25.360 --> 00:34:26.080]   Perfect.
[00:34:26.080 --> 00:34:28.400]   Thank you very much.
[00:34:28.400 --> 00:34:35.480]   So then I guess that brings us to this question,
[00:34:35.480 --> 00:34:38.960]   to this point where our self-attention is actually,
[00:34:38.960 --> 00:34:41.320]   in a way--
[00:34:41.320 --> 00:34:44.440]   I think there was this research done in this past paper
[00:34:44.440 --> 00:34:50.280]   by Codney et al that shows that a multi-head positional self--
[00:34:50.280 --> 00:34:53.520]   PSA layer, positional self-attention layer,
[00:34:53.520 --> 00:34:56.800]   can actually work like--
[00:34:56.800 --> 00:34:59.760]   like can actually express any convolutional layer of filter
[00:34:59.760 --> 00:35:02.280]   size done to n h by n h.
[00:35:02.280 --> 00:35:07.120]   So I guess in a general transformer, what we have--
[00:35:07.120 --> 00:35:08.880]   I'll try and summarize this.
[00:35:08.920 --> 00:35:10.440]   But I'll draw the image.
[00:35:10.440 --> 00:35:14.520]   And then maybe, Stefan, did you want to spend some time
[00:35:14.520 --> 00:35:18.240]   and just explain what this means?
[00:35:18.240 --> 00:35:21.280]   So what I guess that means is if you have an input image,
[00:35:21.280 --> 00:35:24.680]   and this at the center is my query,
[00:35:24.680 --> 00:35:27.240]   and let's say I have nine attention heads,
[00:35:27.240 --> 00:35:31.200]   because typically in a transformer,
[00:35:31.200 --> 00:35:33.480]   you have multi-headed self-attention.
[00:35:33.480 --> 00:35:35.800]   So then if each attention head starts
[00:35:35.800 --> 00:35:41.960]   to look at these points around the query, then in a way,
[00:35:41.960 --> 00:35:46.040]   it's actually acting like a 3 by 3 convolution kernel.
[00:35:46.040 --> 00:35:49.600]   Is this something that you want to explain on top of, Stefan?
[00:35:49.600 --> 00:35:52.880]   And is that a good summary of that paper?
[00:35:52.880 --> 00:35:56.520]   OK.
[00:35:56.520 --> 00:35:59.280]   So then that's the main idea, that any--
[00:35:59.280 --> 00:36:03.200]   like this research in the past, in 2019, by Codney et al,
[00:36:03.200 --> 00:36:07.360]   it shows that it's actually very possible
[00:36:07.360 --> 00:36:14.280]   if these self-attention layers with nine heads or 16 heads
[00:36:14.280 --> 00:36:17.440]   or basically n heads, n number of heads
[00:36:17.440 --> 00:36:21.680]   are sort of initialized with a certain amount of parameters.
[00:36:21.680 --> 00:36:24.640]   In that case, then if you can make each attention head
[00:36:24.640 --> 00:36:28.240]   pay attention to these pixels around the query pixel,
[00:36:28.240 --> 00:36:32.600]   then you can actually make it work like an under root n
[00:36:32.600 --> 00:36:35.280]   by under root n convolution kernel.
[00:36:35.280 --> 00:36:37.400]   So if it's nine, it's like a 3 by 3 kernel.
[00:36:37.400 --> 00:36:39.440]   If it's 16, it's a 4 by 4 kernel.
[00:36:39.440 --> 00:36:48.280]   And then this is pretty much how you tend to initialize these.
[00:36:48.280 --> 00:36:50.840]   This is from, again, that previous paper, which
[00:36:50.840 --> 00:36:55.200]   says that if you initialize these attention
[00:36:55.200 --> 00:36:58.240]   layers in this way, then you can actually
[00:36:58.240 --> 00:37:01.280]   make it behave like a convolutional layer
[00:37:01.280 --> 00:37:02.240]   in the beginning.
[00:37:02.240 --> 00:37:05.040]   So was this one of the main sort of inspirations,
[00:37:05.040 --> 00:37:10.480]   Stefan, is that from this paper is you've got the idea of, OK,
[00:37:10.480 --> 00:37:16.720]   maybe let's try and initialize these early layers
[00:37:16.720 --> 00:37:20.240]   as a convolution and then see how we go?
[00:37:20.240 --> 00:37:21.360]   Was that the discussion?
[00:37:21.360 --> 00:37:22.760]   Please tell us more.
[00:37:22.760 --> 00:37:24.160]   Yeah, yeah, exactly.
[00:37:24.160 --> 00:37:25.880]   And actually, to pay credit to the authors,
[00:37:25.880 --> 00:37:27.680]   they probably would have done it themselves
[00:37:27.680 --> 00:37:30.640]   if there was already the DIT around at the moment
[00:37:30.640 --> 00:37:34.600]   where they investigated this and if they had the computer
[00:37:34.600 --> 00:37:36.440]   resource to test it.
[00:37:36.440 --> 00:37:37.600]   It's a very natural idea.
[00:37:37.600 --> 00:37:47.400]   Yeah, it does feel very natural that--
[00:37:47.400 --> 00:37:51.120]   I mean, if you think of it, it does feel very natural
[00:37:51.120 --> 00:37:54.080]   that it makes sense because we've all
[00:37:54.080 --> 00:37:56.480]   seen in this various lines of research
[00:37:56.480 --> 00:37:58.200]   that the earlier layers are convolutions.
[00:37:58.200 --> 00:38:02.280]   It does feel natural that why don't we initialize it
[00:38:02.280 --> 00:38:05.920]   in a way that it sort of learns whether it
[00:38:05.920 --> 00:38:10.640]   wants to stay convolutional or not or how far away.
[00:38:10.640 --> 00:38:15.360]   So then I guess one of the other things that matters here
[00:38:15.360 --> 00:38:19.160]   is this alpha, which is this convolutional initialization.
[00:38:19.160 --> 00:38:22.320]   So in this image, we have this alpha is here.
[00:38:22.320 --> 00:38:25.120]   Did you want to maybe explain, Stefan,
[00:38:25.120 --> 00:38:27.840]   how this alpha parameter is important
[00:38:27.840 --> 00:38:31.680]   and how it's learned in the network?
[00:38:31.680 --> 00:38:33.560]   The alpha parameter basically tells
[00:38:33.560 --> 00:38:36.640]   how focused the attention is on one patch.
[00:38:36.640 --> 00:38:38.680]   So as Aman was saying, if you want
[00:38:38.680 --> 00:38:40.480]   a convolutional configuration, ideally,
[00:38:40.480 --> 00:38:43.480]   you want each attention head to pay only attention
[00:38:43.480 --> 00:38:44.840]   to one patch.
[00:38:44.840 --> 00:38:47.160]   Well, you can have a soft version of that if you want
[00:38:47.160 --> 00:38:51.200]   because attention can look over a broader area.
[00:38:51.200 --> 00:38:55.560]   And essentially, how focused the attention is around one pixel
[00:38:55.560 --> 00:38:56.680]   is determined by alpha.
[00:38:56.680 --> 00:38:58.880]   So if you have a large alpha, you only pay attention
[00:38:58.880 --> 00:38:59.680]   to one pixel.
[00:38:59.680 --> 00:39:01.560]   And if you have a smaller alpha, then it's
[00:39:01.560 --> 00:39:05.240]   going to be broader distributed around the pixel of interest.
[00:39:05.240 --> 00:39:09.760]   Thank you.
[00:39:09.760 --> 00:39:12.480]   Yeah, this was another key-- one of the things I was really
[00:39:12.480 --> 00:39:15.080]   trying to understand is how is this alpha actually
[00:39:15.080 --> 00:39:15.840]   affecting things.
[00:39:15.840 --> 00:39:18.200]   But now that you put it, it does really
[00:39:18.200 --> 00:39:20.200]   help my understanding as well.
[00:39:20.200 --> 00:39:21.760]   It's like if you have a smaller alpha,
[00:39:21.760 --> 00:39:26.000]   then you're looking at a bigger area around the pixel.
[00:39:26.000 --> 00:39:28.640]   Whereas when you have a much bigger alpha,
[00:39:28.640 --> 00:39:31.480]   then you're looking at a much smaller and much concentrated
[00:39:31.480 --> 00:39:33.520]   area around the pixel.
[00:39:33.520 --> 00:39:37.000]   So the attention is much more concentrated
[00:39:37.000 --> 00:39:37.880]   to a specific region.
[00:39:37.880 --> 00:39:40.960]   If you look at this case here, what's also nice with this
[00:39:40.960 --> 00:39:46.920]   is it allows us to have kernels of even size
[00:39:46.920 --> 00:39:48.720]   because if you think of kernels, it's
[00:39:48.720 --> 00:39:52.480]   a bit awkward to do a convolution with a 2 by 2
[00:39:52.480 --> 00:39:55.080]   kernel because the kernel has to--
[00:39:55.080 --> 00:39:58.040]   can't be centered around the query pixel.
[00:39:58.040 --> 00:40:00.040]   Whereas here, it's all right because basically we
[00:40:00.040 --> 00:40:04.720]   can do a 2 by 2 kernel simply by focusing
[00:40:04.720 --> 00:40:07.120]   on the corner of the query pixel.
[00:40:07.120 --> 00:40:09.000]   So essentially, you're paying attention
[00:40:09.000 --> 00:40:12.040]   to the neighboring patches to the corner.
[00:40:12.040 --> 00:40:15.000]   And we can also do a 4 by 4 convolutional,
[00:40:15.000 --> 00:40:18.640]   but that's awkward with usual kernels.
[00:40:18.640 --> 00:40:22.800]   So yeah, that's also a curious thing about it.
[00:40:22.800 --> 00:40:24.960]   That's a really key insight.
[00:40:24.960 --> 00:40:25.440]   Exactly.
[00:40:25.440 --> 00:40:30.040]   So in a CNN, if you have this as my sort of the pixel
[00:40:30.040 --> 00:40:32.000]   that I'm looking at the convolution around,
[00:40:32.000 --> 00:40:35.040]   then in terms of a 4 by 4, how do you
[00:40:35.040 --> 00:40:39.160]   make sure that this is at the center of that convolution,
[00:40:39.160 --> 00:40:39.880]   basically?
[00:40:39.880 --> 00:40:43.080]   But whereas in attention, it's much better.
[00:40:43.080 --> 00:40:44.520]   It's much easier to do.
[00:40:44.520 --> 00:40:46.920]   So if you just keep looking at the pixels around it,
[00:40:46.920 --> 00:40:51.640]   then is that a good summary of what you just said?
[00:40:51.640 --> 00:40:53.080]   Yeah, what I meant is just basically
[00:40:53.080 --> 00:40:55.960]   that if you have a 2 by 2 kernel,
[00:40:55.960 --> 00:40:58.920]   you can't center that around the query patch
[00:40:58.920 --> 00:41:03.800]   with usual convolutions because you're either on one corner.
[00:41:03.800 --> 00:41:07.600]   Whereas here, you can really do a 2 by 2 convolution.
[00:41:07.600 --> 00:41:10.080]   Right, yeah, because either here or here or here,
[00:41:10.080 --> 00:41:13.400]   which is always where the corner is in attention.
[00:41:13.400 --> 00:41:14.520]   In this way, it's possible.
[00:41:14.520 --> 00:41:22.680]   With that being said, then when I read the paper,
[00:41:22.680 --> 00:41:25.520]   then these equations are how you pretty much
[00:41:25.520 --> 00:41:28.440]   initialized the early attention layers.
[00:41:28.440 --> 00:41:33.720]   And that led to GPS, the gated positional self-attention.
[00:41:33.720 --> 00:41:37.480]   But there's two main differences that you've
[00:41:37.480 --> 00:41:40.040]   added from what I could see in the approach.
[00:41:40.040 --> 00:41:43.680]   And one of them was this--
[00:41:43.680 --> 00:41:47.000]   so one of them was this idea of adaptive attention span.
[00:41:47.000 --> 00:41:48.840]   And the second one, this positional gating.
[00:41:48.840 --> 00:41:52.040]   So the equations get updated.
[00:41:52.040 --> 00:41:55.600]   But did you want to maybe introduce everybody to this?
[00:41:55.600 --> 00:41:59.320]   Why did you have to update or change things?
[00:41:59.320 --> 00:42:05.920]   And how did you come about to then coming to GPSA?
[00:42:05.920 --> 00:42:07.520]   I didn't really change the equations,
[00:42:07.520 --> 00:42:08.920]   but I just simplified them a bit.
[00:42:08.920 --> 00:42:11.760]   So yeah, the idea behind the positional gating
[00:42:11.760 --> 00:42:14.760]   is that, OK, we want our self-attention
[00:42:14.760 --> 00:42:17.040]   to be initialized as a convolution,
[00:42:17.040 --> 00:42:20.040]   but we also want it to escape this configuration, right?
[00:42:20.040 --> 00:42:22.320]   If it wants to learn more complex dependencies
[00:42:22.320 --> 00:42:23.720]   between pixels.
[00:42:23.720 --> 00:42:26.960]   And so if you want to go on the positional gating equation,
[00:42:26.960 --> 00:42:30.680]   I think it's equation 6, probably.
[00:42:30.680 --> 00:42:33.920]   Yeah.
[00:42:33.920 --> 00:42:36.040]   So if you look at equation 7, the idea
[00:42:36.040 --> 00:42:40.000]   is to basically split this content term and this position
[00:42:40.000 --> 00:42:41.120]   term apart.
[00:42:41.120 --> 00:42:43.600]   And then we're basically going to weigh their importances
[00:42:43.600 --> 00:42:44.680]   with the gating parameter.
[00:42:44.680 --> 00:42:47.320]   So initially, we only want the content--
[00:42:47.320 --> 00:42:48.800]   sorry, the positional part, right?
[00:42:48.800 --> 00:42:51.480]   Because we only want to pay attention to how the pixels are
[00:42:51.480 --> 00:42:52.680]   related to each other.
[00:42:52.680 --> 00:42:55.200]   And then throughout training, the gating parameter
[00:42:55.200 --> 00:42:59.080]   is going to allow us to discard this positional term
[00:42:59.080 --> 00:43:01.520]   if we need to and to pay more attention
[00:43:01.520 --> 00:43:03.480]   to the actual content of the pixels.
[00:43:03.480 --> 00:43:09.760]   Yeah, that's the main and that's the core idea
[00:43:09.760 --> 00:43:11.240]   of the whole paper, right?
[00:43:11.240 --> 00:43:14.600]   And if we can all get this core idea, I think that's it.
[00:43:14.600 --> 00:43:17.520]   We've understood this paper really well.
[00:43:17.520 --> 00:43:20.440]   So you mentioned content and position terms.
[00:43:20.440 --> 00:43:27.960]   So if I go back to this equation 4,
[00:43:27.960 --> 00:43:29.640]   when we were having a look at attention,
[00:43:29.640 --> 00:43:31.200]   there's basically these two terms.
[00:43:31.200 --> 00:43:34.720]   One is a query and key sort of interaction.
[00:43:34.720 --> 00:43:39.040]   So if this is my query key and value matrices,
[00:43:39.040 --> 00:43:42.800]   then if this is my patch and I'm looking at other patches,
[00:43:42.800 --> 00:43:46.120]   then that's a content to content interaction.
[00:43:46.120 --> 00:43:47.800]   But whereas in this one, I'm always
[00:43:47.800 --> 00:43:49.800]   just looking at the relative position,
[00:43:49.800 --> 00:43:51.960]   as I mentioned in terms of when we were looking
[00:43:51.960 --> 00:43:53.080]   at relative position.
[00:43:53.080 --> 00:43:55.800]   So this is a content to content and this is a position
[00:43:55.800 --> 00:43:58.480]   to position interaction.
[00:43:58.480 --> 00:44:06.640]   And it does a softmax after adding the two.
[00:44:06.640 --> 00:44:10.600]   But the way in this, how this is different
[00:44:10.600 --> 00:44:14.840]   is that that equation in a way that still remains the same,
[00:44:14.840 --> 00:44:19.440]   but initially we're paying this much weight to the position
[00:44:19.440 --> 00:44:21.280]   and we're paying 1 minus--
[00:44:21.280 --> 00:44:24.040]   well, is that just a sigmoid parameter?
[00:44:24.040 --> 00:44:24.880]   Yeah.
[00:44:24.880 --> 00:44:26.040]   Yeah, OK.
[00:44:26.040 --> 00:44:28.640]   And then we're paying 1 minus sigmoid times--
[00:44:28.640 --> 00:44:29.600]   well, lambda h.
[00:44:29.600 --> 00:44:32.120]   So lambda h is the gating parameter.
[00:44:32.120 --> 00:44:33.120]   Is that correct?
[00:44:33.120 --> 00:44:34.120]   Yeah.
[00:44:34.120 --> 00:44:39.200]   So we're paying 1 minus lambda h times weight to this term.
[00:44:39.200 --> 00:44:42.440]   So this equation is still, as Stefan has mentioned,
[00:44:42.440 --> 00:44:44.120]   that he didn't really change the equation,
[00:44:44.120 --> 00:44:48.000]   but he just made it simple.
[00:44:48.000 --> 00:44:51.080]   Is that initially, because we want--
[00:44:51.080 --> 00:44:54.480]   and I'm just trying to summarize from my understanding as well--
[00:44:54.480 --> 00:45:00.120]   is that because initially we want these attention
[00:45:00.120 --> 00:45:02.400]   layers to act like convolution kernels,
[00:45:02.400 --> 00:45:04.840]   we want to initialize them in that way.
[00:45:04.840 --> 00:45:06.480]   That means that you're actually going
[00:45:06.480 --> 00:45:11.240]   to pay more weight to the relative position,
[00:45:11.240 --> 00:45:13.880]   because a convolution has a receptive field
[00:45:13.880 --> 00:45:17.840]   and the position of the actual pixels matters a lot more.
[00:45:17.840 --> 00:45:21.800]   But then as we go and as the network learns,
[00:45:21.800 --> 00:45:23.440]   then over time what we want to do
[00:45:23.440 --> 00:45:28.680]   is we want to go away from this paying more attention
[00:45:28.680 --> 00:45:32.880]   to position, but then also start paying to content.
[00:45:32.880 --> 00:45:35.560]   And that's just the main equation.
[00:45:35.560 --> 00:45:36.840]   Is that correct, Stefan?
[00:45:36.840 --> 00:45:37.360]   Yeah.
[00:45:41.800 --> 00:45:44.560]   And then that brings us to this image.
[00:45:44.560 --> 00:45:47.280]   So then how is Convid different?
[00:45:47.280 --> 00:45:51.280]   I believe this image will actually then just summarize
[00:45:51.280 --> 00:45:53.200]   and everything that we've learned so far
[00:45:53.200 --> 00:45:56.560]   will come together in this image.
[00:45:56.560 --> 00:45:58.040]   What it illustrates is just the fact
[00:45:58.040 --> 00:46:02.440]   that we have basically just a vision transformer where
[00:46:02.440 --> 00:46:06.200]   we replace the self-attention layers by GPSA layers.
[00:46:06.200 --> 00:46:08.800]   And what you can see with the class token
[00:46:08.800 --> 00:46:14.160]   is that the class token is only injected into the patches
[00:46:14.160 --> 00:46:15.600]   after the GPSA layers.
[00:46:15.600 --> 00:46:18.280]   And the reason for that is that there's--
[00:46:18.280 --> 00:46:20.800]   I mean, you could, but there's not a very natural way
[00:46:20.800 --> 00:46:25.840]   to do self-attention or at least positional self-attention
[00:46:25.840 --> 00:46:28.240]   with a class token, because the class token doesn't carry
[00:46:28.240 --> 00:46:30.600]   any positional information.
[00:46:30.600 --> 00:46:31.840]   It's not part of the image.
[00:46:31.840 --> 00:46:34.680]   And this is the reason why we inject the class token later.
[00:46:34.680 --> 00:46:38.440]   But it's actually also what more recent transformers do.
[00:46:38.440 --> 00:46:42.200]   If you look at the CAIT models by going deeper
[00:46:42.200 --> 00:46:43.840]   with vision transformers, they also
[00:46:43.840 --> 00:46:47.880]   inject the class token after a few layers, which
[00:46:47.880 --> 00:46:50.040]   seems to improve performance.
[00:46:50.040 --> 00:46:51.040]   Yeah, and they call it--
[00:46:51.040 --> 00:46:53.720]   I think they differentiate between a self-attention
[00:46:53.720 --> 00:46:56.760]   and a class attention, or they call it class embedding.
[00:46:56.760 --> 00:46:58.560]   I did have a look.
[00:46:58.560 --> 00:47:00.200]   Interesting.
[00:47:00.200 --> 00:47:03.120]   So that's-- I guess in terms of that,
[00:47:03.120 --> 00:47:06.720]   that's the main gist of the paper.
[00:47:06.720 --> 00:47:09.200]   But I'll quickly go and have a look in case anybody
[00:47:09.200 --> 00:47:11.880]   has any questions so far.
[00:47:11.880 --> 00:47:14.360]   So is there anything else that you
[00:47:14.360 --> 00:47:17.760]   want to add, Stefan, in terms of the main understanding
[00:47:17.760 --> 00:47:19.760]   and the main summary of the paper?
[00:47:19.760 --> 00:47:22.920]   Or would that be all this is?
[00:47:22.920 --> 00:47:23.720]   OK.
[00:47:23.720 --> 00:47:26.240]   In this case, then, we do welcome questions.
[00:47:26.240 --> 00:47:28.280]   And I do want to ask, if you have any questions,
[00:47:28.280 --> 00:47:33.240]   then please go to this URL, 1db.me/convid.
[00:47:33.240 --> 00:47:36.360]   So let me go there and just have a look if there's questions
[00:47:36.360 --> 00:47:36.860]   so far.
[00:47:36.860 --> 00:47:44.600]   OK.
[00:47:44.600 --> 00:47:47.880]   So question is, why latter layers are for attention
[00:47:47.880 --> 00:47:49.080]   in the hybrid models?
[00:47:49.080 --> 00:47:52.200]   Stefan, did you want to take that one?
[00:47:52.200 --> 00:47:56.480]   So yeah, the idea is that if you think of the usual intuition,
[00:47:56.480 --> 00:47:58.280]   that's the deeper that you go in the layers,
[00:47:58.280 --> 00:47:59.880]   the more abstract information you have.
[00:47:59.880 --> 00:48:04.440]   For example, in the first layer, you extract edges.
[00:48:04.440 --> 00:48:05.560]   And then you look at shapes.
[00:48:05.560 --> 00:48:07.440]   And then you have the full objects
[00:48:07.440 --> 00:48:09.120]   and try to model the relationships
[00:48:09.120 --> 00:48:10.200]   between each other.
[00:48:10.200 --> 00:48:11.800]   So in the later layers, basically, you've
[00:48:11.800 --> 00:48:13.560]   already extracted your objects in the image.
[00:48:13.560 --> 00:48:16.000]   And you really want to capture the long-range dependencies
[00:48:16.000 --> 00:48:17.160]   between these objects.
[00:48:17.160 --> 00:48:20.000]   And this is where self-attention really comes into play.
[00:48:20.000 --> 00:48:22.760]   It's very good at extracting dependencies
[00:48:22.760 --> 00:48:24.400]   between distant parts of the image.
[00:48:24.400 --> 00:48:28.280]   Thank you.
[00:48:28.280 --> 00:48:30.000]   And then the second question that we have
[00:48:30.000 --> 00:48:34.880]   is, does padding affect inductive bias?
[00:48:34.880 --> 00:48:36.080]   I don't think so.
[00:48:36.080 --> 00:48:40.880]   Padding is just more of a mathematical artifact.
[00:48:40.880 --> 00:48:43.040]   Yeah, actually, there's a recent paper about that.
[00:48:43.040 --> 00:48:47.640]   Mind the Pad, it's called, which studies that question.
[00:48:47.640 --> 00:48:51.280]   Shows that padding can actually be detrimental in some cases.
[00:48:51.280 --> 00:48:53.200]   But I haven't read it in detail.
[00:48:53.200 --> 00:48:57.680]   Thank you.
[00:48:57.680 --> 00:49:02.680]   I haven't heard of this Mind the Pad, but I will have a look.
[00:49:02.680 --> 00:49:04.480]   Then the next question is, are these models
[00:49:04.480 --> 00:49:07.480]   robust to distribution shift in experiments
[00:49:07.480 --> 00:49:09.840]   in ImageNet C, ImageNet R?
[00:49:09.840 --> 00:49:11.080]   Yeah, that's a great question.
[00:49:11.080 --> 00:49:14.080]   And in fact, we haven't done the experiments ourselves.
[00:49:14.080 --> 00:49:16.920]   However, they have been made in a recent paper, which
[00:49:16.920 --> 00:49:19.040]   is called--
[00:49:19.040 --> 00:49:22.400]   I think it's called Towards Robust Vision Transformer.
[00:49:22.400 --> 00:49:24.360]   And actually, the experiments do show
[00:49:24.360 --> 00:49:28.040]   that the COMBIT has better robustness than the DIT.
[00:49:28.040 --> 00:49:30.600]   Pretty significant, actually.
[00:49:30.600 --> 00:49:31.880]   And that's kind of interesting.
[00:49:31.880 --> 00:49:36.200]   It's also what's appeared in our latest paper, which
[00:49:36.200 --> 00:49:40.680]   came out like three days ago, where we also use GPSA layers.
[00:49:40.680 --> 00:49:43.480]   But instead of injecting them in vision transformers,
[00:49:43.480 --> 00:49:46.160]   we actually use these GPSA layers
[00:49:46.160 --> 00:49:49.640]   as a reparameterization of convolutional layers
[00:49:49.640 --> 00:49:52.280]   inside pre-trained ResNets.
[00:49:52.280 --> 00:49:54.080]   Because here, essentially, what we're doing
[00:49:54.080 --> 00:49:56.600]   is we're initializing a transformer
[00:49:56.600 --> 00:49:59.120]   as a random convolution.
[00:49:59.120 --> 00:50:00.560]   Well, you can do the--
[00:50:00.560 --> 00:50:02.400]   the idea we explored in our latest paper
[00:50:02.400 --> 00:50:06.000]   is to, instead of initialize your self-attention,
[00:50:06.000 --> 00:50:08.280]   your GPSA as a random convolution,
[00:50:08.280 --> 00:50:10.400]   initialize it as a pre-trained convolution.
[00:50:10.400 --> 00:50:13.600]   And in that case, we also saw that the GPSA layers
[00:50:13.600 --> 00:50:15.640]   do improve the robustness.
[00:50:15.640 --> 00:50:17.120]   So yeah, that's a great question.
[00:50:17.120 --> 00:50:19.560]   Generally speaking, it looks from recent papers
[00:50:19.560 --> 00:50:23.920]   that self-attention layers do bring extra robustness
[00:50:23.920 --> 00:50:26.120]   to models.
[00:50:26.120 --> 00:50:27.640]   Typically, the explanation we have
[00:50:27.640 --> 00:50:30.960]   is that they're better at detecting object contours
[00:50:30.960 --> 00:50:33.240]   and doing some form of segmentation.
[00:50:33.240 --> 00:50:37.560]   So that does improve robustness, typically.
[00:50:37.560 --> 00:50:39.400]   They're less reliant on the textures
[00:50:39.400 --> 00:50:41.600]   and better at detecting shapes.
[00:50:41.600 --> 00:50:45.880]   Thank you.
[00:50:45.880 --> 00:50:50.960]   Yeah, I did see this paper about the transformer robustness.
[00:50:50.960 --> 00:50:57.000]   And it's another interesting read from after Convit.
[00:50:57.000 --> 00:51:01.440]   I guess we'll quickly spend some time based on--
[00:51:01.440 --> 00:51:06.240]   I did also want to touch on this table 2 here that you've--
[00:51:06.240 --> 00:51:08.520]   that, Stefan, from experiments, you
[00:51:08.520 --> 00:51:11.840]   saw how when we're looking at 5% of the data,
[00:51:11.840 --> 00:51:17.520]   then Convit actually gets like a 47.8% top one accuracy
[00:51:17.520 --> 00:51:20.080]   compared to a 34.8%.
[00:51:20.080 --> 00:51:22.680]   And of course, when you're looking at 100%,
[00:51:22.680 --> 00:51:26.040]   then it's 81.4 versus a 79.9.
[00:51:26.040 --> 00:51:28.080]   I was just curious, and why do you
[00:51:28.080 --> 00:51:30.960]   feel that's happening, is that because you've
[00:51:30.960 --> 00:51:33.320]   added the convolutional inductive bias, which
[00:51:33.320 --> 00:51:35.320]   is important?
[00:51:35.320 --> 00:51:36.040]   Yeah, definitely.
[00:51:36.040 --> 00:51:38.920]   There's a breakdown of where the performance comes
[00:51:38.920 --> 00:51:42.400]   from in the ablation table, which is just a bit further,
[00:51:42.400 --> 00:51:45.280]   where we really distinguish the three components of the Convit,
[00:51:45.280 --> 00:51:47.880]   which are essentially the fact that we
[00:51:47.880 --> 00:51:51.320]   use positional information, the fact that we use gating,
[00:51:51.320 --> 00:51:54.240]   and the fact that we have a convolutional initialization.
[00:51:54.240 --> 00:51:56.200]   And so we have these three ingredients.
[00:51:56.200 --> 00:51:59.760]   And can you go to the table, the last table?
[00:51:59.760 --> 00:52:05.640]   Yeah, there we go, table 3.
[00:52:05.640 --> 00:52:06.960]   So in that case, you can--
[00:52:06.960 --> 00:52:13.040]   so if we start from the DIT, which is referenced as E,
[00:52:13.040 --> 00:52:16.080]   you can see that on 10% of the data sets,
[00:52:16.080 --> 00:52:19.880]   it has 47.8 performance.
[00:52:19.880 --> 00:52:22.360]   And then if we go up in the rows,
[00:52:22.360 --> 00:52:24.360]   we're basically adding these three ingredients.
[00:52:24.360 --> 00:52:26.760]   And you see that adding the first ingredient, which
[00:52:26.760 --> 00:52:31.080]   is the GPSA, already gives you a pretty big boost, like 7%.
[00:52:31.080 --> 00:52:35.720]   Then if you add the gating parameters,
[00:52:35.720 --> 00:52:38.240]   you get another 2% boost.
[00:52:38.240 --> 00:52:40.520]   Then adding the convolutional initialization,
[00:52:40.520 --> 00:52:41.480]   you get another boost.
[00:52:41.480 --> 00:52:48.280]   And then finally, the whole thing together gets to 59.7.
[00:52:48.280 --> 00:52:51.080]   I think then this table probably--
[00:52:51.080 --> 00:52:55.080]   is that a recent addition to the paper?
[00:52:55.080 --> 00:52:55.580]   Yeah.
[00:52:55.580 --> 00:52:58.080]   I kind of missed that.
[00:52:58.080 --> 00:52:59.960]   All right, this is really interesting to see
[00:52:59.960 --> 00:53:03.040]   how these small components then actually
[00:53:03.040 --> 00:53:05.360]   affect the performance.
[00:53:05.360 --> 00:53:08.000]   So in that case, it does appear just
[00:53:08.000 --> 00:53:12.880]   the convolutional initialization seems
[00:53:12.880 --> 00:53:16.360]   to have a much bigger effect, right, of 54%?
[00:53:16.360 --> 00:53:19.840]   Yeah, and what's extremely interesting in this here
[00:53:19.840 --> 00:53:25.880]   also is that if you look at the figure just below the 47.8,
[00:53:25.880 --> 00:53:29.560]   essentially what we tried is to use a DIT.
[00:53:29.560 --> 00:53:31.720]   But we completely freeze the attention modules.
[00:53:31.720 --> 00:53:34.800]   But we initialize them in a convolutional way.
[00:53:34.800 --> 00:53:39.240]   And so typically, this is going to degrade performance,
[00:53:39.240 --> 00:53:39.760]   usually.
[00:53:39.760 --> 00:53:42.200]   I mean, if you look at the column on the left,
[00:53:42.200 --> 00:53:46.640]   it goes from 79.1 down to 78.6.
[00:53:46.640 --> 00:53:49.680]   But actually, when you have a small amount of data,
[00:53:49.680 --> 00:53:51.480]   performance is actually improved.
[00:53:51.480 --> 00:53:56.480]   In other words, it's better to have a frozen convolution
[00:53:56.480 --> 00:53:58.920]   than a trained self-attention when you really
[00:53:58.920 --> 00:54:01.200]   don't have much data, which is really surprising.
[00:54:01.200 --> 00:54:04.640]   And again--
[00:54:04.640 --> 00:54:05.920]   I'm still trying to understand.
[00:54:05.920 --> 00:54:10.000]   So when you say you froze the attention layers, what exactly--
[00:54:10.000 --> 00:54:10.960]   We don't train them.
[00:54:10.960 --> 00:54:14.360]   So we train-- so these 10 GPSA layers,
[00:54:14.360 --> 00:54:15.960]   we keep them fixed throughout training.
[00:54:15.960 --> 00:54:19.480]   And we only train the two final self-attention layers.
[00:54:19.480 --> 00:54:23.600]   And what's really surprising is that if you
[00:54:23.600 --> 00:54:28.040]   freeze all these layers, but you use
[00:54:28.040 --> 00:54:30.000]   the convolutional initialization,
[00:54:30.000 --> 00:54:32.640]   it actually gives you better performance than training,
[00:54:32.640 --> 00:54:35.680]   but without the convolutional initialization.
[00:54:35.680 --> 00:54:37.960]   That really is an interesting insight.
[00:54:37.960 --> 00:54:42.280]   Yeah, it goes together with all these ideas of,
[00:54:42.280 --> 00:54:44.800]   is attention really necessary?
[00:54:44.800 --> 00:54:47.320]   The idea that we think that we're
[00:54:47.320 --> 00:54:49.720]   doing something really clever with these self-attention
[00:54:49.720 --> 00:54:52.320]   layers, but probably the most important thing
[00:54:52.320 --> 00:54:54.200]   is not the way we extract--
[00:54:54.200 --> 00:54:58.160]   we exchange information between patches,
[00:54:58.160 --> 00:55:01.400]   but the fact that information flows at all.
[00:55:01.400 --> 00:55:05.560]   It goes together with that paper, the FNET paper,
[00:55:05.560 --> 00:55:09.800]   where they simply mix tokens with a Fourier layer, which
[00:55:09.800 --> 00:55:11.000]   isn't even trained.
[00:55:11.000 --> 00:55:13.120]   But that seems to work pretty well.
[00:55:13.120 --> 00:55:14.480]   In other words, it doesn't really
[00:55:14.480 --> 00:55:17.640]   matter how the tokens are mixed together,
[00:55:17.640 --> 00:55:22.400]   as long as they are mixed together effectively.
[00:55:22.400 --> 00:55:27.960]   In terms of the only thing that kind of did--
[00:55:27.960 --> 00:55:29.560]   and I'm conscious of time as well,
[00:55:29.560 --> 00:55:32.520]   but I guess this is probably the last thing
[00:55:32.520 --> 00:55:35.800]   that we will touch upon if there's no more questions.
[00:55:35.800 --> 00:55:36.960]   There is one.
[00:55:36.960 --> 00:55:39.520]   But then I guess one thing I did feel
[00:55:39.520 --> 00:55:42.440]   when I was reading this paper, which was against my intuition
[00:55:42.440 --> 00:55:49.240]   is that it does make sense to then have the initial layers
[00:55:49.240 --> 00:55:55.560]   initialized as CNNs and then the later layers as self-attention.
[00:55:55.560 --> 00:55:59.200]   Then even then, in terms of a top one accuracy,
[00:55:59.200 --> 00:56:02.520]   it's around 82.4, 82.5, I believe.
[00:56:02.520 --> 00:56:08.560]   Whereas if we look at the kite, they go as up as 86%.
[00:56:08.560 --> 00:56:10.240]   Is that-- why do you think that's--
[00:56:10.240 --> 00:56:11.600]   what's the difference?
[00:56:11.600 --> 00:56:15.120]   And then is there any future versions that you're planning
[00:56:15.120 --> 00:56:18.120]   or there's more research coming where you're going
[00:56:18.120 --> 00:56:19.200]   to try and bridge the gap?
[00:56:19.200 --> 00:56:22.240]   Or was that not the idea to actually even look at top one
[00:56:22.240 --> 00:56:23.920]   but just do this experiment?
[00:56:23.920 --> 00:56:25.880]   Yeah, I think the key point--
[00:56:25.880 --> 00:56:28.640]   we didn't really want to push the model to the limits
[00:56:28.640 --> 00:56:32.040]   in terms of maximal top one because the key point
[00:56:32.040 --> 00:56:35.920]   of the paper really was the sample efficiency.
[00:56:35.920 --> 00:56:37.800]   It's basically getting vision transformers
[00:56:37.800 --> 00:56:40.280]   to work on any kind of data set and introduce
[00:56:40.280 --> 00:56:44.920]   this idea of soft convolutional inductive bias.
[00:56:44.920 --> 00:56:47.120]   If you look at the kite, probably the differences
[00:56:47.120 --> 00:56:51.080]   between kite and DIT or combat, I'd
[00:56:51.080 --> 00:56:55.400]   say, of course, the depth, which gives you high expressivity.
[00:56:55.400 --> 00:56:57.960]   And then pretty much the training tricks
[00:56:57.960 --> 00:57:03.360]   they introduce, so this idea of mixing with the class
[00:57:03.360 --> 00:57:06.360]   token in later layers.
[00:57:06.360 --> 00:57:11.640]   And there's also that idea of using layer scale, which
[00:57:11.640 --> 00:57:16.520]   essentially is using a sort of residual connection
[00:57:16.520 --> 00:57:18.480]   but with a weight, which tells you
[00:57:18.480 --> 00:57:21.920]   how to mix the residual together with the normal path.
[00:57:21.920 --> 00:57:25.320]   And so, yeah, a couple of training tricks,
[00:57:25.320 --> 00:57:27.320]   which make a big difference.
[00:57:27.320 --> 00:57:29.120]   But I think the key point of combat
[00:57:29.120 --> 00:57:31.800]   is really the sample efficiency.
[00:57:31.800 --> 00:57:32.880]   Thank you.
[00:57:32.880 --> 00:57:36.320]   I guess then that brings us to one of the last questions.
[00:57:36.320 --> 00:57:39.400]   For the days, why does--
[00:57:39.400 --> 00:57:41.080]   oh, sorry, that makes it two questions.
[00:57:41.080 --> 00:57:45.440]   But the first one is, why does Adam optimizer work better
[00:57:45.440 --> 00:57:47.680]   in attention layers but not in convolution?
[00:57:47.680 --> 00:57:49.200]   I don't think we did say that.
[00:57:49.200 --> 00:57:50.840]   Is that something-- did you mention, Stefan?
[00:57:50.840 --> 00:57:51.320]   Is that--
[00:57:51.320 --> 00:57:52.680]   I mentioned at the beginning when
[00:57:52.680 --> 00:57:55.600]   I said that hybrid models are a bit finicky to train.
[00:57:55.600 --> 00:57:58.160]   Because they-- yeah.
[00:57:58.160 --> 00:58:00.000]   And so, OK, there's a paper which
[00:58:00.000 --> 00:58:04.120]   studies this in detail, which is easy to find the title
[00:58:04.120 --> 00:58:10.280]   of something like why Adam works better for attention layers.
[00:58:10.280 --> 00:58:14.240]   So I think the insight they give in the paper
[00:58:14.240 --> 00:58:18.360]   is the fact that typically, a studio of momentum
[00:58:18.360 --> 00:58:22.120]   is going to create long-tailed gradients.
[00:58:22.120 --> 00:58:24.520]   And so you can solve this by clipping gradients.
[00:58:24.520 --> 00:58:26.680]   But there was something related to the distribution
[00:58:26.680 --> 00:58:29.840]   of the gradients.
[00:58:29.840 --> 00:58:33.520]   The very hand-wavy way I like to think about this,
[00:58:33.520 --> 00:58:35.080]   but it's probably wrong.
[00:58:35.080 --> 00:58:38.120]   But my intuition is that, generally speaking,
[00:58:38.120 --> 00:58:40.360]   the self-attention mechanism uses
[00:58:40.360 --> 00:58:41.880]   very different components.
[00:58:41.880 --> 00:58:44.960]   It has the Q, the K, and the V matrices.
[00:58:44.960 --> 00:58:48.080]   And then it has the projection to the output.
[00:58:48.080 --> 00:58:52.040]   These are sort of qualitatively different matrices.
[00:58:52.040 --> 00:58:55.000]   And in such a case, I think it's useful to have
[00:58:55.000 --> 00:58:56.080]   an adaptive optimizer.
[00:58:56.080 --> 00:58:58.080]   Because you don't necessarily need the same learning
[00:58:58.080 --> 00:58:59.360]   rate for these different parts.
[00:58:59.360 --> 00:59:02.520]   Whereas for usual networks, like convolutional networks,
[00:59:02.520 --> 00:59:07.480]   all the components are rather similar.
[00:59:07.480 --> 00:59:09.560]   But that's just my intuition.
[00:59:09.560 --> 00:59:11.240]   I have no evidence for that.
[00:59:11.240 --> 00:59:13.280]   So I refer you to that paper.
[00:59:13.280 --> 00:59:17.120]   Why does Adam work better for attention layers?
[00:59:17.120 --> 00:59:19.120]   Thank you.
[00:59:19.120 --> 00:59:20.840]   I hope that answers the question.
[00:59:20.840 --> 00:59:25.240]   And then it is the last question of the day.
[00:59:25.240 --> 00:59:29.240]   If initial layers end up being closer to CNNs,
[00:59:29.240 --> 00:59:31.760]   by the end of CNNs, aren't we having
[00:59:31.760 --> 00:59:34.920]   a higher receptive field almost throughout the whole image?
[00:59:34.920 --> 00:59:41.720]   Yeah, so that's a good question.
[00:59:41.720 --> 00:59:43.880]   So it's true that one simplification we often
[00:59:43.880 --> 00:59:48.400]   make with transformers is saying that they benefit from capturing
[00:59:48.400 --> 00:59:49.480]   long-range dependencies.
[00:59:49.480 --> 00:59:52.640]   It is true that CNNs also can capture long-range dependencies.
[00:59:52.640 --> 00:59:55.360]   Because if you cascade convolutions one
[00:59:55.360 --> 00:59:57.440]   after the other, information does end up
[00:59:57.440 --> 01:00:00.600]   flowing through the whole image.
[01:00:00.600 --> 01:00:03.640]   But the key difference, I think, with self-attention
[01:00:03.640 --> 01:00:06.080]   is that every single layer has access
[01:00:06.080 --> 01:00:09.280]   to every part of the representation
[01:00:09.280 --> 01:00:10.360]   at the given layer.
[01:00:10.360 --> 01:00:14.640]   Whereas in CNNs, after eight layers,
[01:00:14.640 --> 01:00:16.240]   we're going to indeed have information
[01:00:16.240 --> 01:00:17.880]   about all the parts of the image,
[01:00:17.880 --> 01:00:20.120]   but not of the current activation,
[01:00:20.120 --> 01:00:22.240]   rather of the original image.
[01:00:22.240 --> 01:00:25.600]   And so maybe that's the slight difference between CNNs
[01:00:25.600 --> 01:00:26.280]   and transformers.
[01:00:26.280 --> 01:00:29.240]   But again, this is an interesting question,
[01:00:29.240 --> 01:00:32.680]   which definitely needs a paper in itself.
[01:00:32.680 --> 01:00:34.400]   Totally.
[01:00:34.400 --> 01:00:36.200]   That being said, thanks very much, Stefan,
[01:00:36.200 --> 01:00:39.920]   for finding the time and talking and discussing the Convid paper
[01:00:39.920 --> 01:00:41.360]   with us.
[01:00:41.360 --> 01:00:43.080]   Really thankful to you as well.
[01:00:43.080 --> 01:00:45.480]   And I do want to mention is that initially, when
[01:00:45.480 --> 01:00:47.040]   I had some questions about Convid,
[01:00:47.040 --> 01:00:48.280]   I did reach out to Stefan.
[01:00:48.280 --> 01:00:53.160]   And he was very helping to answer all questions for me.
[01:00:53.160 --> 01:00:54.880]   And even today, he's taken out the time
[01:00:54.880 --> 01:00:57.520]   for finding and answering all the questions from everybody
[01:00:57.520 --> 01:00:58.040]   else.
[01:00:58.040 --> 01:00:59.480]   Thanks very much for doing that.
[01:00:59.480 --> 01:01:04.200]   We really appreciate you helping us with the Convid paper.
[01:01:04.200 --> 01:01:09.480]   And wish you good luck for the next coming lines of research
[01:01:09.480 --> 01:01:11.200]   that you end up taking.
[01:01:11.200 --> 01:01:11.920]   Yeah, great.
[01:01:11.920 --> 01:01:16.040]   And thanks for implementing the Convid on TIM.
[01:01:16.040 --> 01:01:17.840]   And that's very useful.
[01:01:17.840 --> 01:01:19.680]   So if anybody wants to play around with it,
[01:01:19.680 --> 01:01:22.840]   you can just download it easily from the PyTorch image
[01:01:22.840 --> 01:01:24.520]   models package.
[01:01:24.520 --> 01:01:26.520]   Yeah, that's something I should quickly mention.
[01:01:26.520 --> 01:01:28.760]   I guess that's a really good way of closing this.
[01:01:28.760 --> 01:01:33.120]   So if anybody wants to experiment with Convid,
[01:01:33.120 --> 01:01:36.320]   it's already part of this library called
[01:01:36.320 --> 01:01:37.680]   PyTorch image models.
[01:01:37.680 --> 01:01:41.720]   So pretty much, you can just say TIM.createModel and say Convid,
[01:01:41.720 --> 01:01:43.260]   and it will create the model for you.
[01:01:43.260 --> 01:01:45.160]   So you can actually run all your experiments
[01:01:45.160 --> 01:01:50.160]   and try all of the things that we've mentioned today.
[01:01:50.160 --> 01:01:51.320]   Thanks, everybody.
[01:01:51.320 --> 01:01:52.320]   See you next week.
[01:01:52.320 --> 01:01:55.360]   [MUSIC PLAYING]
[01:01:55.360 --> 01:01:58.720]   [MUSIC PLAYING]
[01:01:58.720 --> 01:02:02.080]   [MUSIC PLAYING]
[01:02:03.040 --> 01:02:06.400]   [MUSIC PLAYING]
[01:02:06.560 --> 01:02:09.920]   [MUSIC PLAYING]
[01:02:09.920 --> 01:02:12.500]   (upbeat music)

