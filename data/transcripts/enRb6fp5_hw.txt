
[00:00:00.000 --> 00:00:05.840]   Welcome everyone.
[00:00:05.840 --> 00:00:08.600]   This is our first screencast on information retrieval.
[00:00:08.600 --> 00:00:10.200]   Let's start with some guiding ideas.
[00:00:10.200 --> 00:00:12.400]   These will serve both to give you a sense for
[00:00:12.400 --> 00:00:14.840]   the current moment in science and technology,
[00:00:14.840 --> 00:00:17.580]   and also help you build a bridge into the homework,
[00:00:17.580 --> 00:00:21.400]   which is on retrieval augmented in context learning.
[00:00:21.400 --> 00:00:24.440]   You might have noticed by now that NLP
[00:00:24.440 --> 00:00:26.960]   is revolutionizing information retrieval.
[00:00:26.960 --> 00:00:28.700]   This is a story that really begins with
[00:00:28.700 --> 00:00:30.800]   the transformer or maybe more properly,
[00:00:30.800 --> 00:00:33.160]   one of its most famous spokes models, BERT.
[00:00:33.160 --> 00:00:35.120]   Soon after BERT was launched,
[00:00:35.120 --> 00:00:37.600]   Google announced that it was incorporating aspects of
[00:00:37.600 --> 00:00:40.520]   BERT into its core search technologies.
[00:00:40.520 --> 00:00:42.920]   Microsoft made a similar announcement
[00:00:42.920 --> 00:00:45.160]   with Bing at about the same time.
[00:00:45.160 --> 00:00:48.360]   I have a feeling that these two very public announcements were
[00:00:48.360 --> 00:00:50.560]   just a glimpse of the changes that were starting
[00:00:50.560 --> 00:00:53.600]   to happen with major search engines.
[00:00:53.600 --> 00:00:55.840]   A little bit later, we started to see that
[00:00:55.840 --> 00:00:59.000]   large language models would play a direct role in search.
[00:00:59.000 --> 00:01:02.560]   I think the startup u.com was really visionary in this sense.
[00:01:02.560 --> 00:01:05.340]   I like to highlight u.com because its CEO,
[00:01:05.340 --> 00:01:08.900]   Richard Socher, is a distinguished alum of this course.
[00:01:08.900 --> 00:01:11.620]   u.com was way ahead of the curve in seeing that
[00:01:11.620 --> 00:01:13.800]   large language models could be
[00:01:13.800 --> 00:01:17.740]   really interesting and powerful aspects of web search.
[00:01:17.740 --> 00:01:20.420]   We've seen lots of activity in that space since then.
[00:01:20.420 --> 00:01:23.620]   For example, Microsoft has partnered with OpenAI,
[00:01:23.620 --> 00:01:25.640]   and it's now using OpenAI models,
[00:01:25.640 --> 00:01:28.500]   as part of the Bing search experience.
[00:01:28.500 --> 00:01:29.940]   You might have noticed also in
[00:01:29.940 --> 00:01:31.700]   a different perspective on this,
[00:01:31.700 --> 00:01:33.500]   that when GPT-4 was announced,
[00:01:33.500 --> 00:01:35.780]   part of the announcement was a partnership with
[00:01:35.780 --> 00:01:39.380]   Morgan Stanley to help Morgan Stanley employees
[00:01:39.380 --> 00:01:43.540]   use GPT-4 to find things in their own internal documents.
[00:01:43.540 --> 00:01:47.160]   That just shows you that we hear a lot about public web search,
[00:01:47.160 --> 00:01:49.680]   but there are also powerful search applications that
[00:01:49.680 --> 00:01:51.900]   could happen internal to organizations.
[00:01:51.900 --> 00:01:54.160]   Again, powered by, in this case,
[00:01:54.160 --> 00:01:56.820]   throughout this entire story as usual,
[00:01:56.820 --> 00:01:59.060]   powered by the transformer.
[00:01:59.060 --> 00:02:01.740]   You might ask yourself why this is happening,
[00:02:01.740 --> 00:02:04.140]   and I think the fundamental reason is that
[00:02:04.140 --> 00:02:06.260]   information retrieval is simply
[00:02:06.260 --> 00:02:08.740]   a hard natural language understanding problem.
[00:02:08.740 --> 00:02:11.520]   The more powerful our NLU technologies,
[00:02:11.520 --> 00:02:13.580]   the better we can do with retrieval.
[00:02:13.580 --> 00:02:16.220]   Here's an example that brings that point home.
[00:02:16.220 --> 00:02:18.540]   Our query is what compounds protect
[00:02:18.540 --> 00:02:21.100]   the digestive system against viruses?
[00:02:21.100 --> 00:02:23.800]   A relevant document is in the stomach,
[00:02:23.800 --> 00:02:26.500]   gastric acid, and proteases serve as
[00:02:26.500 --> 00:02:30.200]   powerful chemical defenses against ingested pathogens.
[00:02:30.200 --> 00:02:32.820]   The coloring indicates relevance connections.
[00:02:32.820 --> 00:02:35.100]   You'll notice that for the keywords in the query in
[00:02:35.100 --> 00:02:38.200]   the document, there is no string overlap.
[00:02:38.200 --> 00:02:40.460]   The connections that we need to make here are
[00:02:40.460 --> 00:02:43.460]   entirely semantic and that shows you that the more
[00:02:43.460 --> 00:02:46.800]   deeply we understand the language of the query in the document,
[00:02:46.800 --> 00:02:48.860]   the better we're going to be able to do at
[00:02:48.860 --> 00:02:53.060]   finding these relevant passages given queries like this.
[00:02:53.060 --> 00:02:55.960]   That's all mainly about information retrieval,
[00:02:55.960 --> 00:02:57.280]   but I'm an NLP-er,
[00:02:57.280 --> 00:03:00.560]   and for me, the more exciting direction of this is that
[00:03:00.560 --> 00:03:03.640]   information retrieval is now revolutionizing NLP,
[00:03:03.640 --> 00:03:06.140]   and the way it's doing that is by making
[00:03:06.140 --> 00:03:08.720]   our NLP problems more open and
[00:03:08.720 --> 00:03:11.880]   more relevant to actual daily tasks.
[00:03:11.880 --> 00:03:14.800]   Let me use question answering to highlight that.
[00:03:14.800 --> 00:03:17.680]   In the by now standard formulation
[00:03:17.680 --> 00:03:20.080]   of question answering within NLP,
[00:03:20.080 --> 00:03:25.980]   the system is given a title and a context passage and a question,
[00:03:25.980 --> 00:03:28.260]   and the task is to answer that question,
[00:03:28.260 --> 00:03:30.660]   and there is a guarantee that the answer will be
[00:03:30.660 --> 00:03:33.860]   a literal substring of that context passage.
[00:03:33.860 --> 00:03:37.940]   That is standard QA as formulated in tasks like SQUAD,
[00:03:37.940 --> 00:03:40.100]   the Stanford Question Answering Dataset.
[00:03:40.100 --> 00:03:42.260]   Just to repeat, at train time,
[00:03:42.260 --> 00:03:45.740]   you're given a title, context, question, and answer.
[00:03:45.740 --> 00:03:48.660]   At test time, you're given a title, context,
[00:03:48.660 --> 00:03:51.960]   and question, and you have a guarantee that the answer
[00:03:51.960 --> 00:03:55.300]   is a literal substring of that context passage.
[00:03:55.300 --> 00:03:57.780]   That used to be a hard problem for our best models,
[00:03:57.780 --> 00:03:59.820]   but it has grown quite easy,
[00:03:59.820 --> 00:04:01.700]   and I think you can see that it's also pretty
[00:04:01.700 --> 00:04:04.020]   disconnected from actual things that we want to
[00:04:04.020 --> 00:04:06.820]   do with question answering in the world where we very
[00:04:06.820 --> 00:04:12.340]   rarely get this very rich context or that substring guarantee.
[00:04:12.340 --> 00:04:15.320]   We are moving now as a field into
[00:04:15.320 --> 00:04:17.820]   a formulation of QA that I've called OpenQA,
[00:04:17.820 --> 00:04:19.700]   and this will be substantially harder.
[00:04:19.700 --> 00:04:21.920]   In this mode, maybe there's a title,
[00:04:21.920 --> 00:04:23.620]   and a context, and a question,
[00:04:23.620 --> 00:04:25.140]   and the task is to answer.
[00:04:25.140 --> 00:04:30.180]   But now, only the question and answer are given at train time,
[00:04:30.180 --> 00:04:33.180]   and the title and the context passage will need to be
[00:04:33.180 --> 00:04:37.260]   retrieved from somewhere, from a large document corpus,
[00:04:37.260 --> 00:04:38.780]   it could be the web.
[00:04:38.780 --> 00:04:40.980]   Having retrieved it, of course,
[00:04:40.980 --> 00:04:43.220]   we have no guarantee that the answer will be
[00:04:43.220 --> 00:04:47.180]   a literal substring of anything in the context or the title.
[00:04:47.180 --> 00:04:49.140]   This is a substantially harder problem,
[00:04:49.140 --> 00:04:52.780]   but it's also much more relevant because this is simulating,
[00:04:52.780 --> 00:04:56.180]   actually searching on the web where you pose a question and you need to
[00:04:56.180 --> 00:05:00.140]   retrieve relevant information in order to answer the question.
[00:05:00.140 --> 00:05:03.440]   But substantially harder, only the question and answer at train time,
[00:05:03.440 --> 00:05:05.580]   and at test time, all you're given is
[00:05:05.580 --> 00:05:09.300]   the question and all the relevant information needs to be retrieved.
[00:05:09.300 --> 00:05:12.180]   What you see there is to the extent that we can have
[00:05:12.180 --> 00:05:14.780]   really good retrieval technologies find
[00:05:14.780 --> 00:05:17.420]   this really good evidence for answering these questions,
[00:05:17.420 --> 00:05:19.580]   we can develop effective systems,
[00:05:19.580 --> 00:05:24.580]   and that is the crucial role for retrieval in this OpenQA pipeline that you all will
[00:05:24.580 --> 00:05:30.020]   be exploring as part of the associated homework and bake-off for this unit.
[00:05:30.020 --> 00:05:35.560]   Question answering is really just one example of
[00:05:35.560 --> 00:05:38.740]   a family of what you might call knowledge intensive tasks.
[00:05:38.740 --> 00:05:39.980]   I mentioned question answering,
[00:05:39.980 --> 00:05:42.660]   but we also have things like claim verification,
[00:05:42.660 --> 00:05:46.460]   common sense reasoning, long-form reading comprehension,
[00:05:46.460 --> 00:05:48.200]   and information seeking dialogue.
[00:05:48.200 --> 00:05:52.580]   These are all transparently tasks that depend very heavily on having
[00:05:52.580 --> 00:05:57.020]   rich information about the world informing whatever prediction the system makes.
[00:05:57.020 --> 00:06:00.300]   That's pretty clear, but I'm also interested in taking
[00:06:00.300 --> 00:06:05.860]   standard typically closed NLP tasks and expanding them into more open variants.
[00:06:05.860 --> 00:06:09.380]   For example, summarization is standardly just posed as
[00:06:09.380 --> 00:06:12.020]   a task where you take in a long passage and try to
[00:06:12.020 --> 00:06:14.940]   produce a shorter one but couldn't we make that
[00:06:14.940 --> 00:06:17.180]   a knowledge intensive task where we augment
[00:06:17.180 --> 00:06:20.000]   the input with lots of information that we've retrieved?
[00:06:20.000 --> 00:06:24.960]   I think it's a reasonable hypothesis that could improve summarization systems.
[00:06:24.960 --> 00:06:28.860]   Similarly, natural language inference typically just posed as
[00:06:28.860 --> 00:06:31.780]   a closed classification problem premise
[00:06:31.780 --> 00:06:34.540]   hypothesis and you give them one of three labels.
[00:06:34.540 --> 00:06:38.340]   But wouldn't it be interesting to augment the premise with information about
[00:06:38.340 --> 00:06:43.900]   the world that might help the system make better predictions as a classifier?
[00:06:43.900 --> 00:06:47.780]   I think that's just two examples of how we could take classical problems,
[00:06:47.780 --> 00:06:49.600]   even classification problems,
[00:06:49.600 --> 00:06:53.660]   and reformulate them into knowledge intensive tasks that would benefit from
[00:06:53.660 --> 00:06:57.380]   retrieval with the result that they could be made more effective
[00:06:57.380 --> 00:07:01.820]   and also more scalable to real-world problems.
[00:07:01.820 --> 00:07:06.500]   Let's talk a little bit about information retrieval approaches,
[00:07:06.500 --> 00:07:09.140]   and I'll start with classical IR.
[00:07:09.140 --> 00:07:11.900]   In this case, we have a user query coming in,
[00:07:11.900 --> 00:07:14.420]   when was Stanford University founded?
[00:07:14.420 --> 00:07:16.980]   The first thing that we do is term lookup.
[00:07:16.980 --> 00:07:20.940]   What we've done offline presumably is create a large index
[00:07:20.940 --> 00:07:24.780]   that maps terms to associated relevant documents.
[00:07:24.780 --> 00:07:27.420]   It could be a list of documents that contain the term,
[00:07:27.420 --> 00:07:30.980]   but we would probably also do some scoring of those documents with
[00:07:30.980 --> 00:07:35.020]   respect to these query terms to organize them by relevance.
[00:07:35.020 --> 00:07:36.900]   On the basis of that index,
[00:07:36.900 --> 00:07:40.340]   we can do document scoring and give back to the user
[00:07:40.340 --> 00:07:44.540]   a ranked list of documents ordered by relevance.
[00:07:44.540 --> 00:07:47.740]   Then it's up to the user to figure out which of those documents to
[00:07:47.740 --> 00:07:51.740]   check out in looking for an answer to the question.
[00:07:51.740 --> 00:07:57.500]   That is the classical search experience as we all know it.
[00:07:57.500 --> 00:08:02.740]   There is now a movement afoot to replace a lot of that with pure language models.
[00:08:02.740 --> 00:08:05.540]   I've called this the LLMs for everything approach.
[00:08:05.540 --> 00:08:07.780]   In this mode, the user's query comes in,
[00:08:07.780 --> 00:08:09.820]   when was Stanford University founded?
[00:08:09.820 --> 00:08:12.500]   A big language model, totally opaque to us,
[00:08:12.500 --> 00:08:15.660]   does some mysterious work and spits out the answer,
[00:08:15.660 --> 00:08:19.180]   Stanford University was founded in 1891.
[00:08:19.180 --> 00:08:21.780]   A real change to the search experience,
[00:08:21.780 --> 00:08:26.820]   whereas before we had to look through a ranked list of web pages to find our answer,
[00:08:26.820 --> 00:08:30.020]   now the answer is given to us directly.
[00:08:30.020 --> 00:08:32.020]   That could be very exciting.
[00:08:32.020 --> 00:08:34.940]   However, we might start to worry.
[00:08:34.940 --> 00:08:37.340]   We know these models can fabricate evidence,
[00:08:37.340 --> 00:08:40.620]   and so we should be skeptical consumers of their outputs.
[00:08:40.620 --> 00:08:43.220]   Since we don't know where this answer came from,
[00:08:43.220 --> 00:08:46.020]   we have no information about how it was produced.
[00:08:46.020 --> 00:08:50.220]   We might start to wonder about whether our information need was actually met,
[00:08:50.220 --> 00:08:52.420]   whether we should trust this string.
[00:08:52.420 --> 00:08:55.260]   I'm deeply concerned about this model,
[00:08:55.260 --> 00:08:58.220]   enough that I think we should be pushing in a different direction.
[00:08:58.220 --> 00:09:02.060]   That's where you would get neural information retrieval modules
[00:09:02.060 --> 00:09:08.820]   continuing to be important players in open knowledge intensive tasks for NLP.
[00:09:08.820 --> 00:09:11.620]   Neural IR models are going to function a lot like
[00:09:11.620 --> 00:09:15.820]   those classical models except in a much richer semantic space.
[00:09:15.820 --> 00:09:17.980]   We're going to start with a big language model,
[00:09:17.980 --> 00:09:20.420]   just as we did in the LLMs for everything approach,
[00:09:20.420 --> 00:09:22.700]   but we're going to use it somewhat differently.
[00:09:22.700 --> 00:09:25.380]   The first thing we'll do with that language model is take
[00:09:25.380 --> 00:09:27.700]   all the documents that we have in our collection of
[00:09:27.700 --> 00:09:31.340]   documents and represent them with the language model.
[00:09:31.340 --> 00:09:35.900]   The result of that will be some dense numerical representations that we
[00:09:35.900 --> 00:09:41.300]   expect to capture important aspects of their structure and their meaning.
[00:09:41.300 --> 00:09:46.060]   That is essentially the document index in the classical IR mode,
[00:09:46.060 --> 00:09:49.900]   but now it's a bunch of deep learning representations.
[00:09:49.900 --> 00:09:52.420]   Then the user's query comes in,
[00:09:52.420 --> 00:09:54.820]   and the first thing we do with that query is process it,
[00:09:54.820 --> 00:09:57.140]   probably using the same large language model,
[00:09:57.140 --> 00:10:01.300]   and get back a dense numerical representation of that query.
[00:10:01.300 --> 00:10:04.380]   Then on the basis of all these representations,
[00:10:04.380 --> 00:10:07.900]   we can do scoring and extraction as usual.
[00:10:07.900 --> 00:10:09.980]   At this point, we can reproduce
[00:10:09.980 --> 00:10:12.660]   everything about the classical search experience.
[00:10:12.660 --> 00:10:16.980]   The only twist is that scoring will happen in a different way because we're now
[00:10:16.980 --> 00:10:20.460]   dealing not with terms and scores but rather with
[00:10:20.460 --> 00:10:25.620]   these dense numerical representations that we're accustomed to throughout deep learning.
[00:10:25.620 --> 00:10:31.380]   But the result of all that scoring is that we give the user back a ranked list of pages.
[00:10:31.380 --> 00:10:35.660]   We've reproduced the classical experience for the user in the sense that they now need to
[00:10:35.660 --> 00:10:39.940]   search through those pages and find the answer to their question.
[00:10:39.940 --> 00:10:44.420]   We just hope that we're doing a much better job of offering
[00:10:44.420 --> 00:10:47.020]   relevant pages in virtue of the fact that we're
[00:10:47.020 --> 00:10:50.980]   operating in a much richer semantic space.
[00:10:50.980 --> 00:10:56.220]   This is a good moment to bridge into in-context learning,
[00:10:56.220 --> 00:10:58.060]   which is the other part of this unit,
[00:10:58.060 --> 00:11:00.260]   and they'll all come together for you in the homework.
[00:11:00.260 --> 00:11:02.860]   Let's think about how that bridge is going to happen.
[00:11:02.860 --> 00:11:04.740]   Now we're going to be in the mode of having
[00:11:04.740 --> 00:11:07.180]   a large language model and prompting it.
[00:11:07.180 --> 00:11:09.620]   In this case, we've simply prompted it with a question,
[00:11:09.620 --> 00:11:13.060]   who is Bert, and the task is to come up with an answer.
[00:11:13.060 --> 00:11:14.980]   In the mode that we're operating in,
[00:11:14.980 --> 00:11:17.460]   that is the only thing that we're given by the system.
[00:11:17.460 --> 00:11:20.340]   This is a truly an open QA formulation.
[00:11:20.340 --> 00:11:26.340]   The question is, how can we effectively answer this question using retrieval?
[00:11:26.340 --> 00:11:31.100]   Well, one thing we could do is retrieve from a document store,
[00:11:31.100 --> 00:11:34.060]   context passage for that question that we
[00:11:34.060 --> 00:11:36.900]   hope will be relevant evidence for answering that question.
[00:11:36.900 --> 00:11:38.620]   That's given in green here.
[00:11:38.620 --> 00:11:40.900]   But there's more that we could do with retrieval.
[00:11:40.900 --> 00:11:43.300]   For example, we know that large language models,
[00:11:43.300 --> 00:11:45.140]   when they're doing in-context learning,
[00:11:45.140 --> 00:11:47.700]   benefit from having demonstrations.
[00:11:47.700 --> 00:11:50.860]   Maybe we have a train set of questions and we could
[00:11:50.860 --> 00:11:54.500]   just retrieve from that set a question to use.
[00:11:54.500 --> 00:11:58.740]   Then at that point, we could either use the training answer to
[00:11:58.740 --> 00:12:02.020]   that question or maybe retrieve an answer in the hope that
[00:12:02.020 --> 00:12:05.580]   that will more closely simulate what the system actually has to do.
[00:12:05.580 --> 00:12:09.140]   But in any case, we now have this demonstration and we could go on further.
[00:12:09.140 --> 00:12:10.380]   Depending on the train set,
[00:12:10.380 --> 00:12:15.140]   we could either use training evidence like a passage from our QA dataset,
[00:12:15.140 --> 00:12:18.340]   or retrieve a context passage, again,
[00:12:18.340 --> 00:12:23.260]   using a retriever to function as evidence for this little demonstration here.
[00:12:23.260 --> 00:12:27.340]   The guiding hypothesis is that having woven together
[00:12:27.340 --> 00:12:30.020]   training instances with some retrieval steps
[00:12:30.020 --> 00:12:32.860]   to produce evidence for answering this question,
[00:12:32.860 --> 00:12:37.300]   we're going to do a better job at coming up with predicted answers.
[00:12:37.300 --> 00:12:40.980]   That's a simple retrieve then read pipeline
[00:12:40.980 --> 00:12:43.220]   where we're using our retriever to find evidence.
[00:12:43.220 --> 00:12:46.100]   What you'll see in the in-context learning unit,
[00:12:46.100 --> 00:12:48.020]   and as you work on the homework,
[00:12:48.020 --> 00:12:52.020]   is that this is just the start of a very rich set of options that we can
[00:12:52.020 --> 00:12:56.020]   employ for effectively develop in-context learning systems
[00:12:56.020 --> 00:12:59.340]   that use retrieval to find relevant evidence.
[00:12:59.340 --> 00:13:02.460]   That's how these two themes really come together.
[00:13:02.460 --> 00:13:05.580]   I think that these two themes coming together is one of
[00:13:05.580 --> 00:13:10.860]   the central questions for the field of NLP and IR in the current moment.
[00:13:10.860 --> 00:13:15.100]   Because really what we're seeing is a lot of worrisome behavior
[00:13:15.100 --> 00:13:17.140]   from large language models that are being
[00:13:17.140 --> 00:13:19.740]   deployed as part of search technologies.
[00:13:19.740 --> 00:13:24.700]   For example, we all saw that Google took a real hit in terms of its stock price for
[00:13:24.700 --> 00:13:28.980]   making a minor factual error in one of its demo videos.
[00:13:28.980 --> 00:13:32.700]   Maybe that was appropriate given how high stakes all of this is,
[00:13:32.700 --> 00:13:36.140]   but it's funny to think about because at the same time,
[00:13:36.140 --> 00:13:40.260]   open AI models were fabricating evidence all over the place.
[00:13:40.260 --> 00:13:44.060]   This is a amusing example where I have asked the system,
[00:13:44.060 --> 00:13:48.180]   are professional baseball players allowed to glue small wings to their caps?
[00:13:48.180 --> 00:13:52.540]   I asked the model to offer me some evidence for the answer that it gave.
[00:13:52.540 --> 00:13:56.460]   It did indeed dutifully say no and then offer some evidence,
[00:13:56.460 --> 00:14:00.260]   but the evidence links that it offered are entirely fabricated.
[00:14:00.260 --> 00:14:02.940]   They are not real links to web pages.
[00:14:02.940 --> 00:14:05.620]   If you follow them, you get a 404 page.
[00:14:05.620 --> 00:14:08.660]   I find this tremendously frustrating and
[00:14:08.660 --> 00:14:14.380]   easily worse than offering no evidence at all because we have all become accustomed
[00:14:14.380 --> 00:14:16.780]   to seeing URLs and assuming that they do
[00:14:16.780 --> 00:14:20.220]   function as ground truth evidence for the answer given.
[00:14:20.220 --> 00:14:23.520]   The fact that that ground truth is being completely fabricated
[00:14:23.520 --> 00:14:27.380]   is absolutely worse than offering no evidence at all.
[00:14:27.380 --> 00:14:29.380]   Here's another funny case for this.
[00:14:29.380 --> 00:14:33.220]   We're going to talk about our demonstrate search predict paper.
[00:14:33.220 --> 00:14:37.220]   Figure 1 of that paper includes an example with the question,
[00:14:37.220 --> 00:14:41.260]   how many stories are in the castle David Gregory inherited?
[00:14:41.260 --> 00:14:47.620]   On Twitter, a user read our paper and then tried the example with Bing search engine.
[00:14:47.620 --> 00:14:52.620]   They said, "Aha, Bing can answer your very difficult seeming question, no problem."
[00:14:52.620 --> 00:14:56.440]   But then that user immediately followed up by noticing that Bing was actually
[00:14:56.440 --> 00:15:01.020]   citing our own paper as evidence for the answer to this question.
[00:15:01.020 --> 00:15:04.080]   I will say that this is deeply worrisome to me.
[00:15:04.080 --> 00:15:07.440]   Our paper should not be regarded as
[00:15:07.440 --> 00:15:11.660]   good ground truth evidence about the castle David Gregory inherited.
[00:15:11.660 --> 00:15:14.880]   We used it purely as an illustrative example.
[00:15:14.880 --> 00:15:17.120]   If we had had slightly different intentions,
[00:15:17.120 --> 00:15:20.840]   we might have actually been talking about giving the wrong answer to this question.
[00:15:20.840 --> 00:15:23.800]   In fact, our figure does embed some wrong answers.
[00:15:23.800 --> 00:15:28.440]   The idea that a scientific research paper that's about in context learning with
[00:15:28.440 --> 00:15:33.400]   retrieval would be used as evidence for the castle David Gregory inherited,
[00:15:33.400 --> 00:15:36.360]   anything about that is completely baffling to me.
[00:15:36.360 --> 00:15:38.920]   That just shows you that simply because you have
[00:15:38.920 --> 00:15:42.320]   some search mechanisms doesn't mean that you're doing good search.
[00:15:42.320 --> 00:15:47.000]   But what we really need in this context is high-quality search.
[00:15:47.000 --> 00:15:48.920]   Just to round this out,
[00:15:48.920 --> 00:15:52.040]   I found this amusing but maybe a little bit worrisome.
[00:15:52.040 --> 00:15:54.480]   You should all try this with your own names.
[00:15:54.480 --> 00:16:00.880]   I prompted chat GPT with write a biography of Christopher Potts from Stanford University.
[00:16:00.880 --> 00:16:03.640]   I'm very happy with the first paragraph.
[00:16:03.640 --> 00:16:07.960]   It's very flattering to me and we can go ahead and say that it is truthful.
[00:16:07.960 --> 00:16:12.120]   But everything in the box in red is completely false.
[00:16:12.120 --> 00:16:16.120]   All of the factual information expressed there is false.
[00:16:16.120 --> 00:16:19.100]   It's a nice biography of me.
[00:16:19.100 --> 00:16:22.720]   I have no complaints about any of these facts except that they are false.
[00:16:22.720 --> 00:16:25.920]   The reason though that I'm worried is that I think not
[00:16:25.920 --> 00:16:30.120]   everyone is going to get such flattering information when they ask for their biography.
[00:16:30.120 --> 00:16:34.280]   We are just on the precipice of seeing really worrisome behavior with
[00:16:34.280 --> 00:16:37.720]   really meaningful downstream effects for people in society,
[00:16:37.720 --> 00:16:41.760]   if these language models continue to fabricate evidence in this way.
[00:16:41.760 --> 00:16:46.360]   That's why I feel like the current unit and the work that you do for it is
[00:16:46.360 --> 00:16:49.720]   absolutely extremely important and relevant to addressing
[00:16:49.720 --> 00:16:53.720]   this growing societal and technological problem.
[00:16:53.720 --> 00:16:55.420]   That sets the stage.
[00:16:55.420 --> 00:16:58.420]   If you would like a little bit more on this, Omar,
[00:16:58.420 --> 00:17:02.340]   Matej Zaharia and I did two blog posts on this a few years ago
[00:17:02.340 --> 00:17:05.500]   that I think remain still extremely relevant.
[00:17:05.500 --> 00:17:07.120]   The first is building scalable,
[00:17:07.120 --> 00:17:10.440]   explainable, and adaptive NLP models with retrieval.
[00:17:10.440 --> 00:17:13.440]   That's a technical blog post.
[00:17:13.440 --> 00:17:16.640]   A more high-level outward looking one is
[00:17:16.640 --> 00:17:20.180]   this modest proposal for radically better AI-powered web search,
[00:17:20.180 --> 00:17:22.680]   where all the way back in 2021,
[00:17:22.680 --> 00:17:27.440]   we were emphasizing the importance of provenance for information and
[00:17:27.440 --> 00:17:32.120]   ground truth in documents as an important aspect of doing web search,
[00:17:32.120 --> 00:17:35.640]   even with big, powerful, fancy, large language models.
[00:17:35.640 --> 00:17:38.360]   That is the vision that we're going to try to project for
[00:17:38.360 --> 00:17:42.400]   you throughout this unit and with our homework.
[00:17:42.400 --> 00:17:52.400]   [BLANK_AUDIO]

