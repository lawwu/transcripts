
[00:00:00.000 --> 00:00:06.520]   So yeah, tonight I'm going to talk about attention for time series forecasting.
[00:00:06.520 --> 00:00:08.360]   So I'll just hop right in.
[00:00:08.360 --> 00:00:15.220]   So one of my interests for a long time has been incorporating more deep learning models
[00:00:15.220 --> 00:00:23.100]   in industry, and in particular in industry-wide, we've seen massive advances in NLP with things
[00:00:23.100 --> 00:00:32.980]   like Hugging Face, Transformers, and the computer vision side, ImageNet, and other related technologies
[00:00:32.980 --> 00:00:36.080]   like that to really improve computer vision.
[00:00:36.080 --> 00:00:41.980]   We really haven't seen widespread adaption of deep learning, particularly in the time
[00:00:41.980 --> 00:00:47.140]   series forecast, at least not in my experience and a lot of people that I've talked to in
[00:00:47.140 --> 00:00:48.140]   industry.
[00:00:48.140 --> 00:00:49.620]   And there's been some reasons for this.
[00:00:49.620 --> 00:00:55.060]   In industry, I've seen people generally prefer very simple, kind of shallow models that are
[00:00:55.060 --> 00:01:03.660]   very easily explainable like XGBoost, linear regression, Profit, ARIMA, et cetera.
[00:01:03.660 --> 00:01:08.100]   Usually these are like the go-to in 90 percent of the cases I've seen.
[00:01:08.100 --> 00:01:10.260]   I've heard various rationales.
[00:01:10.260 --> 00:01:12.180]   Some people it's interpretability.
[00:01:12.180 --> 00:01:17.500]   We need this model to be really interpretable for time series to our stakeholders.
[00:01:17.500 --> 00:01:22.820]   Also ease of use, not all data science teams might have the skill level necessary to try
[00:01:22.820 --> 00:01:25.020]   to train a deep neural network.
[00:01:25.020 --> 00:01:27.900]   And another big one is lack of training data.
[00:01:27.900 --> 00:01:34.020]   So while I think these are all good rationales, I do think long-term we'll definitely see
[00:01:34.020 --> 00:01:41.180]   more – we definitely should see more deep learning models incorporated in an industry
[00:01:41.180 --> 00:01:43.820]   level and industry setting.
[00:01:43.820 --> 00:01:48.140]   And I think a very good candidate to do that is transformers.
[00:01:48.140 --> 00:01:51.180]   You might wonder why given how big and complex for it is.
[00:01:51.180 --> 00:01:57.760]   But on one level it's really good at finding relevant time steps in the sequence for predicting.
[00:01:57.760 --> 00:02:01.800]   You can visualize heat maps of attention.
[00:02:01.800 --> 00:02:05.420]   That can help with some of the interpretability issues I just described.
[00:02:05.420 --> 00:02:09.460]   It also allows access to any part of the history.
[00:02:09.460 --> 00:02:15.740]   And also as I was just saying it's potentially very conducive for transfer learning.
[00:02:15.740 --> 00:02:20.180]   We haven't seen a lot of research conducted on that specifically, but from taking our
[00:02:20.180 --> 00:02:25.340]   understanding in natural language processing it would definitely be my guess in one area
[00:02:25.340 --> 00:02:34.940]   I'm actually actively exploring at the moment if basically the transformer can work very
[00:02:34.940 --> 00:02:37.140]   well in a transfer learning context.
[00:02:37.140 --> 00:02:40.180]   Oh, and I did want to add one other thing.
[00:02:40.180 --> 00:02:45.780]   So as I said before another big reason is also these models are very complex and the
[00:02:45.780 --> 00:02:47.620]   code structure is very hard to use.
[00:02:47.620 --> 00:02:53.100]   So another big thing regardless of it's transformers or LSTMs it has to be – we have to have
[00:02:53.100 --> 00:02:57.940]   easier interfaces to fit time series data to these neural networks because otherwise
[00:02:57.940 --> 00:03:02.300]   they're just too cumbersome to use even if they have much better performance.
[00:03:02.300 --> 00:03:05.760]   No one's going to want to write out like 200 lines of code when they can just call
[00:03:05.760 --> 00:03:09.380]   model.fit with sklearn or something.
[00:03:09.380 --> 00:03:16.060]   So that's actually it for some of my formal slides.
[00:03:16.060 --> 00:03:27.420]   I'm now actually going to do some quick looks at some papers just kind of interactively.
[00:03:27.420 --> 00:03:34.860]   So one of the interesting things, really interesting things that came out recently was this paper
[00:03:34.860 --> 00:03:39.260]   Enhancing the Locality and Breaking the Memory Bottleneck of the Transformer for Time Series
[00:03:39.260 --> 00:03:41.940]   Forecasting.
[00:03:41.940 --> 00:03:50.820]   And so with that essentially – I'm just going to mute Slack actually for a second,
[00:03:50.820 --> 00:03:54.340]   stop those pop-ups.
[00:03:54.340 --> 00:03:59.420]   So with that one of the offers looked specifically at using transformers for time series forecasting.
[00:03:59.420 --> 00:04:03.820]   They also came up with this kind of new idea of essentially using this mass multi-head
[00:04:03.820 --> 00:04:05.580]   attention mechanism.
[00:04:05.580 --> 00:04:10.140]   And with that they looked at instead of just applying kind of a single kind of dot product,
[00:04:10.140 --> 00:04:15.300]   actually using a convolution in the multi-head attention mechanism.
[00:04:15.300 --> 00:04:21.260]   And by doing that they can capture the greater idea of kind of the temporal dimensions of
[00:04:21.260 --> 00:04:26.200]   the time series data, particularly when working with multivariate time series data.
[00:04:26.200 --> 00:04:30.220]   Obviously there's more detail I could go into here, but I'm just trying to provide
[00:04:30.220 --> 00:04:36.740]   a very brief overview of kind of some of the interesting stuff going on in this space tonight.
[00:04:36.740 --> 00:04:40.960]   So that was very interesting and they found that worked pretty well.
[00:04:40.960 --> 00:04:44.900]   On the kind of the memory side of it they also found that these alternatives to like
[00:04:44.900 --> 00:04:51.340]   a full kind of self-attention dot product, one was this log sparse attention and another
[00:04:51.340 --> 00:04:55.460]   was this restart attention.
[00:04:55.460 --> 00:04:59.780]   And basically by using these they just in a very brief view they can reduce the memory
[00:04:59.780 --> 00:05:00.940]   footprint.
[00:05:00.940 --> 00:05:06.540]   So this was one of the very interesting research articles that come out of the last NIRPS conference
[00:05:06.540 --> 00:05:10.540]   on this kind of transformers for attention.
[00:05:10.540 --> 00:05:16.660]   Another kind of interesting article that I saw was this temporal fusion transformers.
[00:05:16.660 --> 00:05:21.380]   And one of the interesting things about this in particular is a lot of times in time series
[00:05:21.380 --> 00:05:28.260]   forecasting you'll have both, at certain time steps you'll have both observed inputs and
[00:05:28.260 --> 00:05:29.260]   unknown inputs.
[00:05:29.260 --> 00:05:33.420]   For instance like the day of the week you're going to know even several time steps ahead,
[00:05:33.420 --> 00:05:38.380]   but things like, but the thing you're actually forecasting you're obviously not going to
[00:05:38.380 --> 00:05:39.380]   know that.
[00:05:39.380 --> 00:05:46.060]   So this model kind of provided an interesting framework for combining all these static co-variates
[00:05:46.060 --> 00:05:49.620]   with this dynamic temporal data.
[00:05:49.620 --> 00:05:53.620]   As I said the architectures here are pretty quick, but I did just want to kind of survey
[00:05:53.620 --> 00:05:56.460]   those fairly briefly.
[00:05:56.460 --> 00:05:59.580]   So one of the things, the main thing I've been working on for the last couple months
[00:05:59.580 --> 00:06:11.940]   is actually this kind of generalized framework for using transformers, particularly transformers
[00:06:11.940 --> 00:06:19.340]   but also other deep time series models for time series forecasting.
[00:06:19.340 --> 00:06:24.260]   So initially this was actually geared mainly towards stream and river flow forecasting
[00:06:24.260 --> 00:06:27.980]   because that's one of my main interests, but I've been working on trying to expand it to
[00:06:27.980 --> 00:06:33.820]   other areas as well as I'm doing right now with kind of like some of my COVID research.
[00:06:33.820 --> 00:06:39.460]   But the idea is it essentially integrates directly with weights and biases.
[00:06:39.460 --> 00:06:45.360]   Within it we have all these different variations of the transformer model that we can easily
[00:06:45.360 --> 00:06:50.740]   use for time series forecasting, like here's a full simple transformer, there's a custom
[00:06:50.740 --> 00:06:51.740]   transformer.
[00:06:51.740 --> 00:06:58.060]   And basically the way it works is in general what we have with this is we'll have a custom
[00:06:58.060 --> 00:07:03.700]   configuration file and this integrates very nicely with weights and biases because when
[00:07:03.700 --> 00:07:08.820]   I go to log the training run it logs this full configuration file with all my different
[00:07:08.820 --> 00:07:14.860]   parameters, you know, what optimizer I'm using, how many time steps I'm forecasting ahead,
[00:07:14.860 --> 00:07:17.120]   et cetera, et cetera.
[00:07:17.120 --> 00:07:26.780]   So I don't know exactly how well I'm doing on time, but I assume Lovana would let me
[00:07:26.780 --> 00:07:29.420]   know if I'm getting close to the end here.
[00:07:29.420 --> 00:07:35.660]   So I will go and show you some of my actual runs using this kind of framework on time
[00:07:35.660 --> 00:07:36.660]   series data.
[00:07:36.660 --> 00:07:37.660]   So –
[00:07:37.660 --> 00:07:38.660]   >> You're doing great.
[00:07:38.660 --> 00:07:42.180]   You can use another five minutes or even ten if you want.
[00:07:42.180 --> 00:07:43.940]   >> Okay, sure.
[00:07:43.940 --> 00:07:44.940]   That's great.
[00:07:44.940 --> 00:07:45.940]   Yeah, I just forget.
[00:07:45.940 --> 00:07:51.220]   It's kind of hard because I didn't remember what time I actually started at, but thanks,
[00:07:51.220 --> 00:07:52.220]   Lovana.
[00:07:52.220 --> 00:07:57.780]   So yeah, as you can see here this works very well with weights and biases.
[00:07:57.780 --> 00:08:02.580]   You know, I'm able to log – so this is still on my upper river flow forecasting task since
[00:08:02.580 --> 00:08:05.420]   that's what I was working on for a while.
[00:08:05.420 --> 00:08:11.140]   So here I logged, you know, the cubic footage per second on the test, the temperature, and
[00:08:11.140 --> 00:08:13.040]   then the precipitation.
[00:08:13.040 --> 00:08:15.420]   Here's my training loss plots.
[00:08:15.420 --> 00:08:19.880]   This model for whatever reason wasn't doing very good, so here's my actual predictions
[00:08:19.880 --> 00:08:22.460]   versus the actual flow.
[00:08:22.460 --> 00:08:27.540]   Obviously that could be – could use some more hyperparameter optimization.
[00:08:27.540 --> 00:08:33.660]   So but in any case – and you know, here's my full config file, so it works very nice
[00:08:33.660 --> 00:08:36.460]   and clean in this context.
[00:08:36.460 --> 00:08:45.700]   So and particularly because transformers and other models like these have so many hyperparameters,
[00:08:45.700 --> 00:08:51.000]   this is where I think there's a good potential in a great – because using – we can look
[00:08:51.000 --> 00:08:53.540]   at, you know, obviously things like batch size.
[00:08:53.540 --> 00:09:00.840]   We can vary with forecast history and forecast length and to understand how that will affect,
[00:09:00.840 --> 00:09:04.540]   you know, our final loss and all these other sorts of metrics.
[00:09:04.540 --> 00:09:09.640]   So that's actually what I've been looking at so far in some of the coronavirus forecasting
[00:09:09.640 --> 00:09:10.640]   research.
[00:09:10.640 --> 00:09:15.840]   If you're not familiar, I'm actually working as part of the Corona Y group right now, which
[00:09:15.840 --> 00:09:24.920]   is, you know, this global group to help with both on the NLP side and now I'm working on
[00:09:24.920 --> 00:09:29.640]   the time series forecasting side, use AI to, you know, help fight coronavirus, which is
[00:09:29.640 --> 00:09:31.900]   obviously a very good mission.
[00:09:31.900 --> 00:09:37.940]   So one of the major problems for this sort of stuff with Corona Y is we obviously have
[00:09:37.940 --> 00:09:42.040]   a very limited, very, very, very limited time series data set.
[00:09:42.040 --> 00:09:45.640]   So this is really a case where we really – if we want to use deep learning on this type
[00:09:45.640 --> 00:09:50.780]   of problem, we really need transfer learning and also data augmentation techniques.
[00:09:50.780 --> 00:09:54.420]   We also need very good hyperparameter searches.
[00:09:54.420 --> 00:09:59.680]   So I'll actually be pretty honest right now.
[00:09:59.680 --> 00:10:04.980]   At this moment, I'm primarily – most of my baselines I've used are actually LSTM
[00:10:04.980 --> 00:10:05.980]   models.
[00:10:05.980 --> 00:10:11.820]   I am planning on getting to the actual transformer here as well, but I just wanted to do a baseline
[00:10:11.820 --> 00:10:13.820]   LSTM method.
[00:10:13.820 --> 00:10:18.260]   But as you can see here, I'm already beginning to run like parameter sweeps to determine
[00:10:18.260 --> 00:10:23.940]   kind of the best batch size, the best learning rate, and the best sequence length, et cetera,
[00:10:23.940 --> 00:10:28.940]   and how that corresponds to the mean squared error and, you know, forecasting the number
[00:10:28.940 --> 00:10:31.500]   of coronavirus cases.
[00:10:31.500 --> 00:10:35.260]   And we can see that definitely by using these parameter sweeps.
[00:10:35.260 --> 00:10:40.260]   I really do like this tool because I can very quickly see what types of things are working
[00:10:40.260 --> 00:10:41.260]   well.
[00:10:41.260 --> 00:10:51.500]   And similarly, like, if we go down to the media section – well, that might not be
[00:10:51.500 --> 00:10:57.580]   loading right now, but I've been loading the similar plots of cases versus actual cases
[00:10:57.580 --> 00:10:58.580]   there.
[00:10:58.580 --> 00:11:02.940]   I think I might actually be somewhat overloading my browser between the video and this right
[00:11:02.940 --> 00:11:04.740]   now.
[00:11:04.740 --> 00:11:11.100]   But you would see, like, if this loads essentially the plot of the predicted cases versus the
[00:11:11.100 --> 00:11:12.380]   actual ones.
[00:11:12.380 --> 00:11:13.860]   Yeah, so like here.
[00:11:13.860 --> 00:11:17.000]   So here's the models predicted cases.
[00:11:17.000 --> 00:11:19.820]   Those are the actual cases.
[00:11:19.820 --> 00:11:25.500]   As you can see, it's not the best model, but this is just the very simple LSTM baseline
[00:11:25.500 --> 00:11:28.180]   with hyperparameter optimization.
[00:11:28.180 --> 00:11:32.580]   And we're definitely going to add in some transfer learning on some similar pandemic
[00:11:32.580 --> 00:11:40.380]   datasets that's hopefully going to very much improve the performance of these models.
[00:11:40.380 --> 00:11:45.460]   So yeah, I guess that's kind of an overview of what I'm doing and kind of some of the
[00:11:45.460 --> 00:11:50.860]   research I'm looking at to hopefully bring deep learning really to the forefront of time
[00:11:50.860 --> 00:11:52.940]   series forecasting as a whole.
[00:11:52.940 --> 00:11:59.380]   Because I do think that there could definitely be a similar type of ImageNet or BERT moment
[00:11:59.380 --> 00:12:05.460]   for time series forecasting where we have a massive pre-trained transformer model or
[00:12:05.460 --> 00:12:10.340]   even possibly an LSTM model, although that historically hasn't worked.
[00:12:10.340 --> 00:12:14.420]   But I do think kind of the future in that area too is for deep learning.
[00:12:14.420 --> 00:12:19.580]   And yeah, definitely weights and biases and their platform and their hyperparameters is
[00:12:19.580 --> 00:12:24.900]   making it easier to kind of plot course and see what works and what doesn't.
[00:12:24.900 --> 00:12:27.660]   So does anyone have any questions about that?
[00:12:27.660 --> 00:12:32.060]   That pretty much covers what I wanted to go over tonight.
[00:12:32.060 --> 00:12:35.900]   Never kind of presentation on short notice, so I didn't have really refined slides or
[00:12:35.900 --> 00:12:36.900]   anything.
[00:12:36.900 --> 00:12:41.500]   I feel like I always ask Isaac to present two days in advance.
[00:12:41.500 --> 00:12:44.020]   I'm like, hey, can you present at the salon?
[00:12:44.020 --> 00:12:47.500]   And he scrambles to get a talk together.
[00:12:47.500 --> 00:12:49.180]   So we do have a question.
[00:12:49.180 --> 00:12:52.140]   If you guys have more questions, please ask in the Slack.
[00:12:52.140 --> 00:12:56.940]   And Isaac, maybe you can post the link to the Slack community again.
[00:12:56.940 --> 00:13:01.620]   So someone asked, could you talk about how you use configs with your project and weights
[00:13:01.620 --> 00:13:02.620]   and biases?
[00:13:02.620 --> 00:13:06.140]   Do you keep all your configs in the oneDb.config object?
[00:13:06.140 --> 00:13:09.500]   I love a question about weights and biases.
[00:13:09.500 --> 00:13:10.500]   Sure.
[00:13:10.500 --> 00:13:13.580]   So, yeah, I have different configuration files.
[00:13:13.580 --> 00:13:21.940]   So actually, this platform uses essentially different configs to run and initialize the
[00:13:21.940 --> 00:13:22.940]   models.
[00:13:22.940 --> 00:13:26.340]   So actually, the models are primarily initialized to for dictionaries.
[00:13:26.340 --> 00:13:31.300]   So basically, if I wanted to change the model I wanted to use, I would just change this
[00:13:31.300 --> 00:13:34.820]   to like, you know, an LSTM or something like that.
[00:13:34.820 --> 00:13:37.740]   And so I do store like different groups of this JSON data.
[00:13:37.740 --> 00:13:42.700]   So for instance, this config is for like a full simple transformer model.
[00:13:42.700 --> 00:13:44.780]   This config is for an LSTM.
[00:13:44.780 --> 00:13:55.140]   And then what happens is when the models are initialized, this logs straight to the db.init
[00:13:55.140 --> 00:13:56.140]   command.
[00:13:56.140 --> 00:13:57.660]   So this config is passed to that.
[00:13:57.660 --> 00:14:02.380]   And yeah, I'm actually still in the process of fully integrating the hyperparameter search
[00:14:02.380 --> 00:14:05.420]   with these configs because these configs are very complex.
[00:14:05.420 --> 00:14:10.620]   But that's definitely on my radar to like kind of fully integrate their hyperparameter
[00:14:10.620 --> 00:14:16.900]   searches which are pretty good with my full kind of config initialization architecture.
[00:14:16.900 --> 00:14:21.580]   And then another thing I do is I do just store like the config files in JSON format.
[00:14:21.580 --> 00:14:27.860]   So I have a number of JSON configs that I can just download.
[00:14:27.860 --> 00:14:30.220]   And I use some of those for unit tests too.
[00:14:30.220 --> 00:14:33.940]   So here's like an LSTM in the full JSON format.
[00:14:33.940 --> 00:14:35.940]   Here's a full transformer config.
[00:14:35.940 --> 00:14:40.260]   Anyways, that probably answers more than answers your question.
[00:14:40.260 --> 00:14:42.260]   But it's kind of.
[00:14:42.260 --> 00:14:43.260]   Thank you.
[00:14:43.260 --> 00:14:54.340]   So someone else asked, how are your models comparing to standard epidemiology models?
[00:14:54.340 --> 00:14:56.220]   Have you tried comparing them?
[00:14:56.220 --> 00:14:57.220]   Yeah.
[00:14:57.220 --> 00:15:02.260]   So right now we are working at looking at the LSTM model in particular.
[00:15:02.260 --> 00:15:08.980]   So that's the main one I've run so far versus a standard SEER model.
[00:15:08.980 --> 00:15:13.100]   So far, pretty similar.
[00:15:13.100 --> 00:15:18.140]   And the SEER model I think was still outperforming the LSTM a bit.
[00:15:18.140 --> 00:15:24.260]   But as I said, we haven't really tried transfer learning or any of these other techniques
[00:15:24.260 --> 00:15:25.260]   on it yet.
[00:15:25.260 --> 00:15:29.220]   But yeah, I'd definitely be interested to see like long term once we get like kind of
[00:15:29.220 --> 00:15:34.700]   a full transfer learning data set and other data augmentation techniques to see how that
[00:15:34.700 --> 00:15:35.900]   compared with it.
[00:15:35.900 --> 00:15:40.340]   Plus, we're potentially even weighing maybe using some of those very basic mathematical
[00:15:40.340 --> 00:15:43.140]   models for data augmentation techniques.
[00:15:43.140 --> 00:15:46.340]   So we would like just pre-train on data generated by them.
[00:15:46.340 --> 00:15:50.260]   And even though that isn't the best, that could provide like an initialization, which
[00:15:50.260 --> 00:15:55.220]   we then fine tune on all of our on the actual data.
[00:15:55.220 --> 00:16:04.100]   So Oswald asked, do you have any thoughts around using reformers with time series data?
[00:16:04.100 --> 00:16:07.340]   Yeah, I think the reformer is very interesting.
[00:16:07.340 --> 00:16:13.220]   It's a very, I think it has a lot of potential for time series too, based on what I've seen.
[00:16:13.220 --> 00:16:16.140]   I haven't personally gotten around to implementing it.
[00:16:16.140 --> 00:16:20.180]   I think it only came out, what was it, three or four months ago.
[00:16:20.180 --> 00:16:24.740]   But yeah, definitely at some point, I'd really like to add it to my library and see how it
[00:16:24.740 --> 00:16:25.740]   performs.
[00:16:25.740 --> 00:16:26.740]   Have you tried transformers?
[00:16:26.740 --> 00:16:35.820]   And you mentioned that you hadn't gotten around to them yet.
[00:16:35.820 --> 00:16:43.380]   So can you share the code with people, because a bunch of people asked for your GitHub repo.
[00:16:43.380 --> 00:16:46.740]   And then also, will you come and give this talk again once you've tried transformers
[00:16:46.740 --> 00:16:47.740]   and reformers?
[00:16:47.740 --> 00:16:52.940]   Yeah, yeah, I can definitely share the code.
[00:16:52.940 --> 00:16:57.340]   At least, yeah, for our kind of Corona Y models.
[00:16:57.340 --> 00:17:02.460]   And yeah, I'd be happy to talk again at some point if people are interested.

