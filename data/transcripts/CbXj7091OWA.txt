
[00:00:00.000 --> 00:00:02.600]   Today, you and I are going to build a simple neural network
[00:00:02.600 --> 00:00:03.400]   together.
[00:00:03.400 --> 00:00:05.360]   We're going to understand line by line
[00:00:05.360 --> 00:00:07.360]   what every single line does.
[00:00:07.360 --> 00:00:10.200]   And we're going to do something that no other video does,
[00:00:10.200 --> 00:00:12.240]   but everybody working on neural networks
[00:00:12.240 --> 00:00:15.040]   does, which is we're going to debug that model together
[00:00:15.040 --> 00:00:17.880]   and find an issue with it and make it work.
[00:00:17.880 --> 00:00:19.360]   For your first neural network, it's
[00:00:19.360 --> 00:00:23.080]   very traditional to use a data set called MNIST, which
[00:00:23.080 --> 00:00:24.660]   is a handwriting data set.
[00:00:24.660 --> 00:00:27.480]   So our goal here is to look at digits
[00:00:27.480 --> 00:00:30.440]   that people actually wrote down of different numbers.
[00:00:30.440 --> 00:00:32.320]   And our classifier is going to try
[00:00:32.320 --> 00:00:35.920]   to figure out from those pixels what number the person wrote
[00:00:35.920 --> 00:00:36.600]   down.
[00:00:36.600 --> 00:00:39.800]   So I'm a huge believer in learning by doing.
[00:00:39.800 --> 00:00:42.520]   So I strongly recommend at this point
[00:00:42.520 --> 00:00:45.520]   opening up a terminal of your own and following along.
[00:00:45.520 --> 00:00:47.280]   So first, open up a terminal and type
[00:00:47.280 --> 00:00:54.720]   git clone https://github.com/lucas/ml-class.
[00:00:54.720 --> 00:00:57.280]   This will put all the files I use in all the classes
[00:00:57.280 --> 00:00:59.400]   in a directory for you called ml-class.
[00:00:59.400 --> 00:01:01.940]   And by the way, you're welcome to modify and use these files
[00:01:01.940 --> 00:01:04.440]   in any way you like.
[00:01:04.440 --> 00:01:07.560]   Now type cd ml-class and then type
[00:01:07.560 --> 00:01:10.360]   pip install -r requirements.txt.
[00:01:10.360 --> 00:01:12.760]   This will install all the necessary Python packages.
[00:01:12.760 --> 00:01:19.520]   Sometimes students have trouble here.
[00:01:19.520 --> 00:01:21.280]   Usually this comes from needing to install
[00:01:21.280 --> 00:01:23.640]   a new version of Python.
[00:01:23.640 --> 00:01:26.600]   Now type cd videos and then cd intro
[00:01:26.600 --> 00:01:29.840]   to go into the directory for this class.
[00:01:29.840 --> 00:01:32.560]   There's actually one more step to track your work online.
[00:01:32.560 --> 00:01:35.840]   Type wnb signup.
[00:01:35.840 --> 00:01:38.960]   Just for reference, WNB is a product that I built.
[00:01:38.960 --> 00:01:40.640]   And you can create a free account.
[00:01:40.640 --> 00:01:43.160]   And then WNB will let you see your progress
[00:01:43.160 --> 00:01:44.560]   as you build the models.
[00:01:44.560 --> 00:01:46.200]   And since I built this product, we're
[00:01:46.200 --> 00:01:48.280]   going to use it heavily during all these videos.
[00:01:48.280 --> 00:01:55.360]   There's one more thing we should always do before we start.
[00:01:55.360 --> 00:01:57.520]   And that is to look at our data.
[00:01:57.520 --> 00:02:00.440]   So you can open up the file mnist.png,
[00:02:00.440 --> 00:02:02.040]   which I put in this directory, so you
[00:02:02.040 --> 00:02:04.320]   can see some of the examples of the digits
[00:02:04.320 --> 00:02:06.040]   we're going to classify.
[00:02:06.040 --> 00:02:09.120]   Mnist is a famous data set of handwritten numbers
[00:02:09.120 --> 00:02:10.560]   from 0 to 9.
[00:02:10.560 --> 00:02:12.840]   And our goal here is to produce a model that
[00:02:12.840 --> 00:02:16.560]   takes a single 28 by 28 image and outputs
[00:02:16.560 --> 00:02:18.440]   which digit was written.
[00:02:18.440 --> 00:02:21.120]   This is a super practical task that's generally
[00:02:21.120 --> 00:02:24.200]   known as optical character recognition.
[00:02:24.200 --> 00:02:26.880]   There are lots of ways we could approach this problem.
[00:02:26.880 --> 00:02:29.280]   Today, we're going to use a type of machine learning called
[00:02:29.280 --> 00:02:30.520]   a neural network.
[00:02:30.520 --> 00:02:32.520]   And we're going to start with the most basic type
[00:02:32.520 --> 00:02:35.440]   of neural network, which is called a perceptron.
[00:02:35.440 --> 00:02:38.200]   I had to introduce a bunch of scary sounding terms here.
[00:02:38.200 --> 00:02:40.200]   So don't worry if these terms are confusing.
[00:02:40.200 --> 00:02:42.200]   They'll become second nature as soon
[00:02:42.200 --> 00:02:44.280]   as we start to work with them.
[00:02:44.280 --> 00:02:47.800]   Generally, perceptrons take in an array or list of numbers
[00:02:47.800 --> 00:02:49.800]   and output a single number.
[00:02:49.800 --> 00:02:52.880]   And just for now, let's slightly simplify our problem
[00:02:52.880 --> 00:02:56.280]   to just detecting fives in our data.
[00:02:56.280 --> 00:02:59.240]   So here, the input numbers are pixel values.
[00:02:59.240 --> 00:03:01.040]   And we want the output of our perceptron
[00:03:01.040 --> 00:03:05.060]   to be a 1 if the digit is a 5 and 0
[00:03:05.060 --> 00:03:08.440]   if our digit happens to be any other number.
[00:03:08.440 --> 00:03:10.680]   A little history-- perceptrons were actually
[00:03:10.680 --> 00:03:14.240]   invented in 1957 by a psychologist
[00:03:14.240 --> 00:03:15.800]   named Frank Rosenblatt.
[00:03:15.800 --> 00:03:17.320]   And they were originally designed
[00:03:17.320 --> 00:03:20.560]   for a very, very similar image recognition task to the one
[00:03:20.560 --> 00:03:22.800]   that we're actually doing right now.
[00:03:22.800 --> 00:03:26.360]   And I really like to imagine the machine that Rosenblatt
[00:03:26.360 --> 00:03:29.240]   actually built. He had an array of light sensors
[00:03:29.240 --> 00:03:30.680]   that would look at the image.
[00:03:30.680 --> 00:03:33.600]   And then he would pass the output of each light sensor
[00:03:33.600 --> 00:03:35.640]   through a system of dials he built.
[00:03:35.640 --> 00:03:38.720]   And the output would actually be either a light bulb lighting
[00:03:38.720 --> 00:03:41.640]   up or a light bulb staying dark.
[00:03:41.640 --> 00:03:44.240]   And at first, the weights, a.k.a.
[00:03:44.240 --> 00:03:46.680]   the knobs, were set randomly.
[00:03:46.680 --> 00:03:48.640]   He would put in a picture of a 5.
[00:03:48.640 --> 00:03:51.560]   And the 5 sensor probably wouldn't light up
[00:03:51.560 --> 00:03:53.720]   since the picture actually is of a 5.
[00:03:53.720 --> 00:03:55.600]   And he wants the sensor to light up.
[00:03:55.600 --> 00:03:58.900]   He would try to get the output to be high.
[00:03:58.900 --> 00:04:00.680]   So how does he do that?
[00:04:00.680 --> 00:04:02.600]   He turns a random knob a little bit
[00:04:02.600 --> 00:04:04.320]   and sees if the light gets brighter.
[00:04:04.320 --> 00:04:05.760]   And if the light does get brighter,
[00:04:05.760 --> 00:04:07.320]   he turns it some more.
[00:04:07.320 --> 00:04:09.240]   If the light gets dimmer, he tries
[00:04:09.240 --> 00:04:11.040]   turning the knob in the other direction.
[00:04:11.040 --> 00:04:12.560]   And maybe it gets brighter.
[00:04:12.560 --> 00:04:15.760]   Then he moves on to the next knob and does the same thing.
[00:04:15.760 --> 00:04:18.000]   Once he's actually gone through all the knobs,
[00:04:18.000 --> 00:04:19.920]   he puts in another picture.
[00:04:19.920 --> 00:04:22.080]   Now, if this new picture is not of a 5,
[00:04:22.080 --> 00:04:24.120]   he now doesn't want the sensor to light up.
[00:04:24.120 --> 00:04:26.760]   So he turns the knobs in order to make the light actually
[00:04:26.760 --> 00:04:29.040]   be dimmer for this picture.
[00:04:29.040 --> 00:04:31.360]   Now, changing the knobs might mess up
[00:04:31.360 --> 00:04:32.920]   the settings for the first picture.
[00:04:32.920 --> 00:04:34.880]   But we don't actually worry about that for now.
[00:04:34.880 --> 00:04:37.080]   He keeps walking through all of his pictures,
[00:04:37.080 --> 00:04:39.200]   turning the knobs until he's looked
[00:04:39.200 --> 00:04:43.120]   at every single image of either a 5 or not a 5.
[00:04:43.120 --> 00:04:45.320]   At this point, he's done what's known in neural network
[00:04:45.320 --> 00:04:48.240]   training as an epic.
[00:04:48.240 --> 00:04:50.280]   Once he's gone through every single picture,
[00:04:50.280 --> 00:04:54.880]   he starts back over at the beginning for a new epic.
[00:04:54.880 --> 00:04:58.800]   These days, we usually think of perceptrons as an algorithm
[00:04:58.800 --> 00:05:00.440]   more than a machine.
[00:05:00.440 --> 00:05:03.320]   So let's talk about the algorithm mathematically.
[00:05:03.320 --> 00:05:05.200]   It's actually super simple.
[00:05:05.200 --> 00:05:07.800]   First, we take in a set of input numbers, which in our case
[00:05:07.800 --> 00:05:09.880]   happen to be pixel values.
[00:05:09.880 --> 00:05:12.080]   Then we flatten out the pixel values
[00:05:12.080 --> 00:05:15.440]   into one long fixed length array.
[00:05:15.440 --> 00:05:19.480]   Then we multiply each input by a corresponding number
[00:05:19.480 --> 00:05:20.960]   known as a weight.
[00:05:20.960 --> 00:05:25.000]   And we add up the results of the multiplications.
[00:05:25.000 --> 00:05:26.400]   These weights are those knobs.
[00:05:26.400 --> 00:05:27.520]   And they're somehow learned.
[00:05:27.520 --> 00:05:29.760]   But we'll go into more about that later.
[00:05:29.760 --> 00:05:32.600]   Sometimes we apply what's called an activation function
[00:05:32.600 --> 00:05:33.360]   to the output.
[00:05:33.360 --> 00:05:35.920]   But we actually don't need to worry about that now.
[00:05:35.920 --> 00:05:38.460]   And then finally, we can interpret the output number
[00:05:38.460 --> 00:05:39.720]   however we want.
[00:05:39.720 --> 00:05:41.560]   But in this case, we've agreed that we're
[00:05:41.560 --> 00:05:44.360]   going to say a 1 means that the output is a 5,
[00:05:44.360 --> 00:05:48.240]   and a 0 means that the output is not a 5.
[00:05:48.240 --> 00:05:52.640]   So let's do this on a very small, very specific example.
[00:05:52.640 --> 00:05:55.600]   Imagine for simplicity that our input images
[00:05:55.600 --> 00:05:58.400]   were 2 by 2 images.
[00:05:58.400 --> 00:06:01.260]   So here, we only have 4 pixel values.
[00:06:01.260 --> 00:06:03.600]   And 0 corresponds to a black pixel.
[00:06:03.600 --> 00:06:07.000]   And 255 corresponds to a white pixel.
[00:06:07.000 --> 00:06:09.400]   We flatten the image out into an array
[00:06:09.400 --> 00:06:11.640]   that's going to have length 4.
[00:06:11.640 --> 00:06:15.320]   And now, our weights happen to be these four numbers here,
[00:06:15.320 --> 00:06:17.680]   starting with 0.12.
[00:06:17.680 --> 00:06:19.240]   Where do those weights come from?
[00:06:19.240 --> 00:06:20.480]   We pick them randomly.
[00:06:20.480 --> 00:06:22.000]   But we're then going to learn them
[00:06:22.000 --> 00:06:24.600]   when we do our algorithm training.
[00:06:24.600 --> 00:06:30.580]   So finally, our weighted sum in this case is 2.55.
[00:06:30.580 --> 00:06:32.620]   If you're like me, this all becomes much more clear
[00:06:32.620 --> 00:06:33.600]   by hacking on it.
[00:06:33.600 --> 00:06:35.320]   So let's actually go to the code.
[00:06:35.320 --> 00:06:37.520]   Go back into your directory and open up
[00:06:37.520 --> 00:06:40.920]   perceptron-single.py in your text editor.
[00:06:40.920 --> 00:06:46.280]   Now, let's walk through this.
[00:06:46.280 --> 00:06:49.720]   The first couple lines are just importing the library Keras,
[00:06:49.720 --> 00:06:52.520]   which is a fantastic library for building neural networks
[00:06:52.520 --> 00:06:55.680]   that we're going to use extensively.
[00:06:55.680 --> 00:06:58.680]   Now, the next few lines set up W and B,
[00:06:58.680 --> 00:07:00.600]   which we're going to use for actually looking
[00:07:00.600 --> 00:07:01.180]   at our results.
[00:07:04.800 --> 00:07:08.400]   Line 16 is the first interesting line,
[00:07:08.400 --> 00:07:11.560]   which uses a special Keras function to load the MNIST
[00:07:11.560 --> 00:07:13.760]   data into four data sets.
[00:07:13.760 --> 00:07:16.720]   With a normal data set, we'd have to download it and load it
[00:07:16.720 --> 00:07:17.640]   ourselves.
[00:07:17.640 --> 00:07:20.840]   But MNIST is such a famous, well-known data set
[00:07:20.840 --> 00:07:24.520]   that it's built into the Keras library itself.
[00:07:24.520 --> 00:07:27.000]   We're going to use a common notation in machine learning
[00:07:27.000 --> 00:07:30.680]   where x stands for the inputs and y stands for the outputs.
[00:07:30.680 --> 00:07:37.480]   So here, x_train is a list of 60,000 28 by 28 images.
[00:07:37.480 --> 00:07:41.600]   Or another way of looking at it is that x_train is 60,000
[00:07:41.600 --> 00:07:46.240]   28 by 28 array of integers from 0 to 255,
[00:07:46.240 --> 00:07:50.760]   where 0 is the darkest black and 255 is the brightest white.
[00:07:50.760 --> 00:07:54.120]   y_train is a list of 60,000 labels,
[00:07:54.120 --> 00:07:55.840]   which in the case of MNIST are actually
[00:07:55.840 --> 00:07:59.200]   digits between 0 and 9.
[00:07:59.200 --> 00:08:02.960]   Now, x_test is a held out set of 10,000 images
[00:08:02.960 --> 00:08:05.320]   that we're going to use to test our algorithm on once we've
[00:08:05.320 --> 00:08:06.200]   trained it.
[00:08:06.200 --> 00:08:09.040]   And y_test is 10,000 more labels that
[00:08:09.040 --> 00:08:13.280]   correspond to the images in x_test.
[00:08:13.280 --> 00:08:17.760]   Just for now, we're only going to classify 5s versus not 5s,
[00:08:17.760 --> 00:08:22.400]   so we have to transform our output data into that.
[00:08:22.400 --> 00:08:27.840]   Lines 18 and 19 create two new variables, is_5_train
[00:08:27.840 --> 00:08:32.560]   and is_5_test that correspond to exactly whether or not
[00:08:32.560 --> 00:08:35.920]   our image is a 5.
[00:08:35.920 --> 00:08:40.160]   Lines 22 and 23 calculate the image width and the image
[00:08:40.160 --> 00:08:44.480]   height using an incredibly useful function, shape.
[00:08:44.480 --> 00:08:48.480]   x_train.shape gives us the dimensions of our x_train
[00:08:48.480 --> 00:08:52.400]   variable, which as we mentioned earlier, is 60,000 by 28
[00:08:52.400 --> 00:08:53.620]   by 28.
[00:08:53.620 --> 00:08:59.560]   So x_train bracket 1 and x_train bracket 2
[00:08:59.560 --> 00:09:02.560]   are actually both 28, since our images are in fact
[00:09:02.560 --> 00:09:06.440]   28 by 28 pixel squares.
[00:09:06.440 --> 00:09:08.800]   OK, now for the important part where we build
[00:09:08.800 --> 00:09:10.920]   our first neural network.
[00:09:10.920 --> 00:09:13.840]   Line 26 sets up our network to be sequential.
[00:09:13.840 --> 00:09:15.260]   Keras actually has different ways
[00:09:15.260 --> 00:09:17.060]   of defining neural networks, and sequential
[00:09:17.060 --> 00:09:19.080]   is the simplest and most important,
[00:09:19.080 --> 00:09:20.720]   so we're going to use it.
[00:09:20.720 --> 00:09:24.340]   It means that our network is defined as a series of steps.
[00:09:24.340 --> 00:09:27.820]   Line 27 defines this first step in our network,
[00:09:27.820 --> 00:09:31.660]   which simply flattens our data from a 28 by 28
[00:09:31.660 --> 00:09:35.060]   two-dimensional array to a single 784-length
[00:09:35.060 --> 00:09:36.660]   one-dimensional array.
[00:09:36.660 --> 00:09:39.180]   We tell the flatten command that the input will always
[00:09:39.180 --> 00:09:42.420]   be 28 by 28, and this is a fundamental constraint
[00:09:42.420 --> 00:09:43.340]   of neural networks.
[00:09:43.340 --> 00:09:46.220]   The input size always has to be the same.
[00:09:46.220 --> 00:09:48.060]   If we ever have a different sized image,
[00:09:48.060 --> 00:09:50.000]   we're going to have to crop it or resize it
[00:09:50.000 --> 00:09:53.020]   before we feed it into our network.
[00:09:53.020 --> 00:09:57.660]   Line 28 adds a single perceptron to our network.
[00:09:57.660 --> 00:10:00.340]   The layer is called dense because every input
[00:10:00.340 --> 00:10:02.460]   is connected to every output, and this
[00:10:02.460 --> 00:10:04.380]   will make more sense later as our networks
[00:10:04.380 --> 00:10:06.800]   get more complicated.
[00:10:06.800 --> 00:10:10.140]   Our simple network outputs one single number,
[00:10:10.140 --> 00:10:14.620]   which is where that number 1 in line 28 comes from.
[00:10:14.620 --> 00:10:18.100]   Now, line 29 sets up our network for training,
[00:10:18.100 --> 00:10:20.660]   but this really requires some explanation.
[00:10:20.660 --> 00:10:23.220]   Our network looks like this.
[00:10:23.220 --> 00:10:25.940]   We are multiplying our pixels by a set of weights
[00:10:25.940 --> 00:10:29.060]   and adding them up and hoping that we output a 1 where
[00:10:29.060 --> 00:10:32.180]   the input is a 5 and hoping that we output a 0 in all
[00:10:32.180 --> 00:10:33.660]   the other cases.
[00:10:33.660 --> 00:10:35.980]   The only thing we can change about our algorithm
[00:10:35.980 --> 00:10:37.340]   is the weights.
[00:10:37.340 --> 00:10:39.540]   So how do we find a good set of weights
[00:10:39.540 --> 00:10:41.500]   that will output a 1 when our pixels correspond
[00:10:41.500 --> 00:10:44.660]   to an image of a 5 and 0 otherwise?
[00:10:44.660 --> 00:10:46.660]   We talked earlier about how Rosenblatt,
[00:10:46.660 --> 00:10:50.020]   the inventor of perceptrons, was turning knobs himself
[00:10:50.020 --> 00:10:51.340]   to train an algorithm.
[00:10:51.340 --> 00:10:54.220]   And I think, in fact, that's an excellent intuition for how
[00:10:54.220 --> 00:10:56.980]   all these algorithms work.
[00:10:56.980 --> 00:11:00.060]   However, if we actually had to try turning each knob
[00:11:00.060 --> 00:11:02.100]   and testing how well the network would work,
[00:11:02.100 --> 00:11:04.780]   it would take forever to train large neural networks
[00:11:04.780 --> 00:11:06.620]   on lots of data.
[00:11:06.620 --> 00:11:09.160]   So luckily, there's a computational method
[00:11:09.160 --> 00:11:11.620]   with neural networks that means we don't actually
[00:11:11.620 --> 00:11:13.180]   have to turn every knob.
[00:11:13.180 --> 00:11:15.260]   And in general, the problem of searching
[00:11:15.260 --> 00:11:18.020]   over a large number of knobs or parameters
[00:11:18.020 --> 00:11:20.940]   for an optimal setting is what's known as a gradient descent
[00:11:20.940 --> 00:11:21.540]   problem.
[00:11:21.540 --> 00:11:23.060]   And mathematicians have actually been
[00:11:23.060 --> 00:11:26.060]   thinking about this class of problems for hundreds of years.
[00:11:26.060 --> 00:11:28.060]   In this case of neural networks, there's
[00:11:28.060 --> 00:11:31.340]   a special optimization called back propagation,
[00:11:31.340 --> 00:11:33.540]   which helps us calculate exactly what's
[00:11:33.540 --> 00:11:35.840]   going to happen when we turn each knob very, very
[00:11:35.840 --> 00:11:37.740]   efficiently.
[00:11:37.740 --> 00:11:39.500]   And this optimization is really important,
[00:11:39.500 --> 00:11:42.420]   because we're going to change these knobs or these weights
[00:11:42.420 --> 00:11:43.420]   quite a bit.
[00:11:43.420 --> 00:11:46.900]   Since every machine learning library, including Keras,
[00:11:46.900 --> 00:11:50.300]   has a large array of excellent gradient descent algorithms
[00:11:50.300 --> 00:11:53.100]   built into it, we're not going to go exactly
[00:11:53.100 --> 00:11:55.220]   into how gradient descent and back propagation
[00:11:55.220 --> 00:11:56.580]   works beyond this intuition.
[00:11:56.580 --> 00:11:58.460]   But there's a ton of materials online
[00:11:58.460 --> 00:12:00.540]   if you're interested in that sort of thing.
[00:12:00.540 --> 00:12:02.220]   I'll say my favorite resource for this
[00:12:02.220 --> 00:12:05.220]   is the 3Blue1Brown series on neural networks,
[00:12:05.220 --> 00:12:07.900]   which covers a lot more theory than these lessons
[00:12:07.900 --> 00:12:10.180]   and is also a lot of fun.
[00:12:10.180 --> 00:12:12.420]   OK, let's go back to the code.
[00:12:12.420 --> 00:12:14.380]   There's two things we actually do need to define
[00:12:14.380 --> 00:12:15.940]   to make gradient descent work.
[00:12:15.940 --> 00:12:17.900]   The first is the loss function, which
[00:12:17.900 --> 00:12:20.660]   is basically how much we don't like our output
[00:12:20.660 --> 00:12:24.100]   or how different our output is from the output that we wanted.
[00:12:24.100 --> 00:12:26.900]   The simplest loss function is mean absolute error,
[00:12:26.900 --> 00:12:29.060]   which is just how different your output number is
[00:12:29.060 --> 00:12:30.660]   from the output number that you wanted.
[00:12:30.660 --> 00:12:33.620]   A fancier loss function would be mean squared error,
[00:12:33.620 --> 00:12:35.420]   which is how different your output number is
[00:12:35.420 --> 00:12:37.300]   from what you wanted squared.
[00:12:37.300 --> 00:12:39.100]   And we'll use that for now.
[00:12:39.100 --> 00:12:42.620]   Here, MSE is short for mean squared error.
[00:12:42.620 --> 00:12:44.500]   The second thing we have to specify
[00:12:44.500 --> 00:12:47.340]   is which gradient descent algorithm to use.
[00:12:47.340 --> 00:12:49.260]   One really crucial and sometimes hard
[00:12:49.260 --> 00:12:51.820]   to set up parameter in gradient descent
[00:12:51.820 --> 00:12:53.460]   is called the learning rate.
[00:12:53.460 --> 00:12:55.180]   And this is basically how fast you're
[00:12:55.180 --> 00:12:57.620]   changing the weights every time you feed it
[00:12:57.620 --> 00:12:59.580]   a new image from your data set.
[00:12:59.580 --> 00:13:01.820]   Change the weights too slow, and it'll take you forever
[00:13:01.820 --> 00:13:03.060]   to find good weights.
[00:13:03.060 --> 00:13:05.540]   Change the weights too fast, and you might jump over
[00:13:05.540 --> 00:13:07.160]   a good set of weights.
[00:13:07.160 --> 00:13:08.620]   The right learning rate can really
[00:13:08.620 --> 00:13:12.500]   depend on the problem and other factors.
[00:13:12.500 --> 00:13:14.580]   There are many choices of algorithms in Keras,
[00:13:14.580 --> 00:13:17.740]   but I almost always use the atom gradient descent function
[00:13:17.740 --> 00:13:19.860]   because I don't have to specify the learning rate,
[00:13:19.860 --> 00:13:23.900]   and it can really adapt to a wide range of cases.
[00:13:23.900 --> 00:13:27.380]   So that's what optimizer equals atom means.
[00:13:27.380 --> 00:13:32.180]   The final thing I do is I set the metric to accuracy.
[00:13:32.180 --> 00:13:34.340]   This doesn't change the algorithm itself,
[00:13:34.340 --> 00:13:36.740]   but what it does is it makes Keras output
[00:13:36.740 --> 00:13:39.500]   the accuracy of our algorithm as the algorithm learns.
[00:13:39.500 --> 00:13:46.540]   The last line here in our code does the actual training.
[00:13:46.540 --> 00:13:49.220]   We call fit on the model, which makes
[00:13:49.220 --> 00:13:52.180]   it look for the best set of weights given the input
[00:13:52.180 --> 00:13:55.860]   training data x_train and the output that we want,
[00:13:55.860 --> 00:13:59.140]   which is is_5_train.
[00:13:59.140 --> 00:14:02.660]   We ask our model to print out the accuracy on a held out
[00:14:02.660 --> 00:14:05.780]   validation set in addition to the training set
[00:14:05.780 --> 00:14:07.820]   that it was trained on.
[00:14:07.820 --> 00:14:11.100]   By default, Keras will do the training for one epoch,
[00:14:11.100 --> 00:14:14.500]   meaning that it will look at each input exactly one time.
[00:14:14.500 --> 00:14:17.460]   But here, we set epochs equals to config.epochs,
[00:14:17.460 --> 00:14:19.140]   which we earlier set to be 10.
[00:14:19.140 --> 00:14:25.020]   So it'll actually go over each training data point 10 times.
[00:14:25.020 --> 00:14:29.220]   So now this model is super simple, but it will run.
[00:14:29.220 --> 00:14:31.380]   So go back into your terminal and type Python
[00:14:31.380 --> 00:14:35.500]   perceptron-single.py, and this will train the network.
[00:14:35.500 --> 00:14:45.460]   You can follow the link to see a chart of the accuracy
[00:14:45.460 --> 00:14:46.740]   as the model trains over time.
[00:15:00.020 --> 00:15:03.420]   And wow, the accuracy of this model looks terrible.
[00:15:03.420 --> 00:15:04.820]   I would expect that just guessing
[00:15:04.820 --> 00:15:09.140]   would give an accuracy of 50%, and guessing not 5 only
[00:15:09.140 --> 00:15:12.020]   would give an accuracy of 90%.
[00:15:12.020 --> 00:15:13.940]   One really frustrating thing about working
[00:15:13.940 --> 00:15:17.460]   with neural networks is they never give you helpful errors.
[00:15:17.460 --> 00:15:19.660]   What they do instead is they just don't work that well.
[00:15:19.660 --> 00:15:24.820]   Most tutorials skip over the debugging part,
[00:15:24.820 --> 00:15:27.300]   but I think debugging was where the learning happens.
[00:15:27.300 --> 00:15:29.580]   So let's try to fix this model together.
[00:15:29.580 --> 00:15:31.060]   If you really want to learn, maybe
[00:15:31.060 --> 00:15:33.780]   pause this video for a minute and dive into the Keras docs
[00:15:33.780 --> 00:15:35.400]   and see if you can debug this yourself.
[00:15:35.400 --> 00:15:42.020]   So first, let's see if this model gets better
[00:15:42.020 --> 00:15:44.100]   with more training.
[00:15:44.100 --> 00:15:48.300]   We can set the number of epochs to 100 and wait a while.
[00:15:48.300 --> 00:16:10.620]  , OK, so we're back, and we've trained for 100 epochs.
[00:16:10.620 --> 00:16:13.860]   And you can see the loss number really isn't going down at all,
[00:16:13.860 --> 00:16:15.260]   and the accuracy is not going up.
[00:16:15.260 --> 00:16:18.300]   So this model is definitely not improving.
[00:16:18.300 --> 00:16:19.820]   So the next thing we need to test
[00:16:19.820 --> 00:16:24.540]   is if the model can learn to fit a tiny subset of the data.
[00:16:24.540 --> 00:16:28.780]   So for time's sake, we're going to set the epochs back to 10.
[00:16:28.780 --> 00:16:31.060]   But then the important thing is we're
[00:16:31.060 --> 00:16:35.420]   going to set the input data to be just 20 images.
[00:16:35.420 --> 00:16:39.340]   So we subset x_train to just the first 20 examples,
[00:16:39.340 --> 00:16:42.180]   and then we subset our output data
[00:16:42.180 --> 00:16:45.080]   to be just the first 20 labels.
[00:16:45.080 --> 00:16:46.940]   Now, a reasonable neural network should
[00:16:46.940 --> 00:16:50.300]   be able to get 100% accuracy on the first 20 images
[00:16:50.300 --> 00:16:51.220]   just by overfitting.
[00:16:51.220 --> 00:16:55.940]   So we run this neural network, and we
[00:16:55.940 --> 00:17:04.800]   pull open the output in weights and biases, or W and B.
[00:17:04.800 --> 00:17:08.580]   And we see that the accuracy is still not good,
[00:17:08.580 --> 00:17:11.500]   even when we're constrained to just the first 20 images.
[00:17:11.500 --> 00:17:13.740]   So something is really broken.
[00:17:13.740 --> 00:17:15.740]   So the next thing to try is to look
[00:17:15.740 --> 00:17:18.620]   at what the model is outputting on just a couple of test
[00:17:18.620 --> 00:17:20.140]   images.
[00:17:20.140 --> 00:17:22.860]   So x_test is a set of test images.
[00:17:22.860 --> 00:17:28.300]   So we can call print on model.predict x_test colon 10.
[00:17:28.300 --> 00:17:30.900]   And this will show us what the model is actually
[00:17:30.900 --> 00:17:33.180]   outputting, the number coming out of the model
[00:17:33.180 --> 00:17:34.180]   on the first 10 images.
[00:17:34.180 --> 00:17:43.620]   And this is pretty weird.
[00:17:43.620 --> 00:17:46.260]   So we were expecting numbers between 0 and 1,
[00:17:46.260 --> 00:17:48.420]   but we're getting numbers that are wildly negative
[00:17:48.420 --> 00:17:51.820]   and numbers that are above 200.
[00:17:51.820 --> 00:17:54.180]   So remember, nothing ever said that a neural network
[00:17:54.180 --> 00:17:58.020]   had to output a number between 0 and 1.
[00:17:58.020 --> 00:18:01.020]   So we'll get into this more deeply in later episodes,
[00:18:01.020 --> 00:18:02.820]   but remember how we said you can optionally
[00:18:02.820 --> 00:18:05.420]   add an activation function to the weighted
[00:18:05.420 --> 00:18:06.860]   sum of the perceptron?
[00:18:06.860 --> 00:18:09.300]   Well, here's a really good reason why.
[00:18:09.300 --> 00:18:12.100]   We want to force our output to be between 0 and 1.
[00:18:12.100 --> 00:18:14.260]   And now, a common activation function,
[00:18:14.260 --> 00:18:17.820]   maybe the most common activation function, is called a sigmoid.
[00:18:17.820 --> 00:18:20.940]   Also, for you math nerds, colloquially known
[00:18:20.940 --> 00:18:23.300]   as logistic or softmax.
[00:18:23.300 --> 00:18:25.220]   For you non-math nerds, the important thing
[00:18:25.220 --> 00:18:28.020]   to know about a sigmoid is that big negative numbers are
[00:18:28.020 --> 00:18:31.580]   turned into 0's and big positive numbers are turned into 1,
[00:18:31.580 --> 00:18:34.660]   and numbers near 0 are turned into something in between.
[00:18:34.660 --> 00:18:36.900]   So no matter what your weighted sum is,
[00:18:36.900 --> 00:18:39.540]   the output after taking the sigmoid
[00:18:39.540 --> 00:18:41.600]   is going to be between 0 and 1.
[00:18:41.600 --> 00:18:48.220]   So we can add a sigmoid by going back into our code
[00:18:48.220 --> 00:18:50.960]   and adding just a single line to our model, which
[00:18:50.960 --> 00:18:53.860]   is activation equals sigmoid.
[00:18:53.860 --> 00:19:03.660]   On a small data set, the loss is going down
[00:19:03.660 --> 00:19:05.260]   and the accuracy is going up.
[00:19:05.260 --> 00:19:06.620]   So this is a really good sign.
[00:19:09.380 --> 00:19:13.000]   So let's go back in and let's train
[00:19:13.000 --> 00:19:17.560]   the model on the complete set of training data again
[00:19:17.560 --> 00:19:18.600]   and see how well it does.
[00:19:18.600 --> 00:19:31.280]   So now our accuracy is closer to 91%.
[00:19:31.280 --> 00:19:34.600]   So we're doing better than random.
[00:19:34.600 --> 00:19:37.000]   So as an aside, whenever you're building machine learning
[00:19:37.000 --> 00:19:38.640]   models, you basically have three ways
[00:19:38.640 --> 00:19:40.200]   to make your models better.
[00:19:40.200 --> 00:19:42.720]   You can improve your algorithms, like we're doing now,
[00:19:42.720 --> 00:19:45.720]   which tends to be really hard, as you can see.
[00:19:45.720 --> 00:19:47.680]   You can improve your data preparation,
[00:19:47.680 --> 00:19:49.600]   which is also pretty hard, and we'll
[00:19:49.600 --> 00:19:50.840]   talk about that more later.
[00:19:50.840 --> 00:19:53.080]   Or you can add more training data.
[00:19:53.080 --> 00:19:55.320]   Adding training data might seem hard.
[00:19:55.320 --> 00:19:57.620]   Labeling 60,000 images probably took a long time.
[00:19:57.620 --> 00:19:59.160]   But I actually started a company called
[00:19:59.160 --> 00:20:00.760]   Figure 8, which will do this for you.
[00:20:00.760 --> 00:20:04.120]   And you should check it out if you need more labeled data.
[00:20:04.120 --> 00:20:06.600]   This model is working reasonably well.
[00:20:06.600 --> 00:20:08.480]   But actually, every time you run it,
[00:20:08.480 --> 00:20:09.920]   you're going to see different scores.
[00:20:09.920 --> 00:20:11.380]   So when you train this model, you
[00:20:11.380 --> 00:20:13.960]   may get a different accuracy number than I'm getting.
[00:20:13.960 --> 00:20:15.700]   We'll get into how to fix this and how
[00:20:15.700 --> 00:20:18.960]   to build much more complicated models in the next installment.
[00:20:18.960 --> 00:20:20.480]   What did we cover today?
[00:20:20.480 --> 00:20:22.280]   We built our first neural network.
[00:20:22.280 --> 00:20:24.400]   And more importantly, we debugged
[00:20:24.400 --> 00:20:25.760]   our first neural network.
[00:20:25.760 --> 00:20:27.640]   Get used to that if you really want to build
[00:20:27.640 --> 00:20:29.800]   neural networks for your job.
[00:20:29.800 --> 00:20:31.980]   We also talked about Keras, which
[00:20:31.980 --> 00:20:34.040]   is the most important neural network framework,
[00:20:34.040 --> 00:20:35.440]   not just for beginners, but people
[00:20:35.440 --> 00:20:37.980]   that really build neural networks professionally.
[00:20:37.980 --> 00:20:40.080]   And we talked about loss functions.
[00:20:40.080 --> 00:20:42.080]   And we talked about weights and a little bit
[00:20:42.080 --> 00:20:44.120]   about back propagation.
[00:20:44.120 --> 00:20:46.720]   In the next section, we're going to take this neural network
[00:20:46.720 --> 00:20:48.280]   and we're going to build what's known
[00:20:48.280 --> 00:20:50.160]   as a multilayer perceptron, which
[00:20:50.160 --> 00:20:52.120]   is a more complicated neural network that
[00:20:52.120 --> 00:20:53.880]   can do more things.
[00:20:53.880 --> 00:20:56.440]   And then later on, we're going to build a convolutional neural
[00:20:56.440 --> 00:20:59.120]   network, which is maybe the really fancy kind that you
[00:20:59.120 --> 00:21:01.720]   hear about in a lot of papers and in the news all the time.
[00:21:01.720 --> 00:21:03.640]   And we're going to keep creating these videos.
[00:21:03.640 --> 00:21:05.560]   So you should probably subscribe so
[00:21:05.560 --> 00:21:08.700]   that you're the first to know when a new video comes out.

