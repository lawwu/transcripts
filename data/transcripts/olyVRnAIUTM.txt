
[00:00:00.000 --> 00:00:02.000]   Hello, I'm Chris.
[00:00:02.000 --> 00:00:05.760]   You might remember me from the transfer learning video.
[00:00:05.760 --> 00:00:06.840]   Well, I'm back.
[00:00:06.840 --> 00:00:09.440]   And I'm super excited, because in most of the videos
[00:00:09.440 --> 00:00:12.800]   up until this point, we haven't actually deployed a model.
[00:00:12.800 --> 00:00:15.440]   But today, we're going to build a model from scratch
[00:00:15.440 --> 00:00:19.800]   and deploy it into a web browser where we can actually use it.
[00:00:19.800 --> 00:00:22.000]   And this model is especially cool,
[00:00:22.000 --> 00:00:25.320]   because it is going to be able to detect emotion.
[00:00:25.320 --> 00:00:28.160]   So we're actually making something that could be useful.
[00:00:28.160 --> 00:00:32.000]   I personally sometimes struggle knowing how I feel.
[00:00:32.000 --> 00:00:35.000]   But now, finally, a machine can tell me
[00:00:35.000 --> 00:00:38.600]   with a high degree of accuracy exactly what my feelings are.
[00:00:38.600 --> 00:00:39.720]   So let's dig into it.
[00:00:39.720 --> 00:00:43.560]   So inside the emotion classifier directory of videos
[00:00:43.560 --> 00:00:47.560]   in the ML class project, you can see train.py.
[00:00:47.560 --> 00:00:50.160]   Train.py is a pretty simple script.
[00:00:50.160 --> 00:00:52.000]   We're pulling in a couple Keras layers,
[00:00:52.000 --> 00:00:54.200]   setting up some configuration parameters.
[00:00:54.200 --> 00:00:56.140]   And then we have this custom callback,
[00:00:56.140 --> 00:00:59.440]   which is going to tell us how long the model is actually
[00:00:59.440 --> 00:01:01.080]   taking to perform inference.
[00:01:01.080 --> 00:01:05.160]   So we can do some performance monitoring, as well as
[00:01:05.160 --> 00:01:08.360]   monitoring on the metrics that we're trying to measure.
[00:01:08.360 --> 00:01:11.520]   This function simply loads the data, which in this case,
[00:01:11.520 --> 00:01:16.320]   the data is black and white images of people making
[00:01:16.320 --> 00:01:18.160]   seven different emotions.
[00:01:18.160 --> 00:01:19.480]   Here, we load the data.
[00:01:19.480 --> 00:01:20.720]   We normalize it.
[00:01:20.720 --> 00:01:24.280]   And currently, we have a really simple perceptron
[00:01:24.280 --> 00:01:27.920]   that is simply outputting seven different numbers, which
[00:01:27.920 --> 00:01:32.400]   will be the probabilities of the emotions anger, disgust, fear,
[00:01:32.400 --> 00:01:35.720]   happiness, sad, surprise, and neutral.
[00:01:35.720 --> 00:01:37.360]   Finally, after we finish training,
[00:01:37.360 --> 00:01:39.760]   we're going to save our model so that we can use it later
[00:01:39.760 --> 00:01:42.160]   and deploy it to the web.
[00:01:42.160 --> 00:01:43.760]   So let's go ahead and run train.py.
[00:01:43.760 --> 00:01:50.560]   Takes a second to load the data.
[00:01:50.560 --> 00:01:53.040]   But we can watch live progress in the Weights and Biases
[00:01:53.040 --> 00:01:55.200]   interface.
[00:01:55.200 --> 00:01:57.040]   So for this model, let's go ahead and dig in
[00:01:57.040 --> 00:01:59.280]   and actually look at some of the predictions that
[00:01:59.280 --> 00:02:01.200]   are coming out of it.
[00:02:01.200 --> 00:02:03.360]   And we can zoom here to see these expressions a little
[00:02:03.360 --> 00:02:04.920]   better.
[00:02:04.920 --> 00:02:07.520]   Here, happy, that seems right.
[00:02:07.520 --> 00:02:10.960]   Happy, this looks more angry, but that's difficult.
[00:02:10.960 --> 00:02:13.200]   Neutral.
[00:02:13.200 --> 00:02:15.800]   Happy, it seems to be skewing towards happy.
[00:02:15.800 --> 00:02:18.280]   This is correct, sad.
[00:02:18.280 --> 00:02:20.800]   So our actual validation accuracy
[00:02:20.800 --> 00:02:22.080]   isn't that great right now.
[00:02:22.080 --> 00:02:27.440]   We're getting the correct emotion about 35% of the time.
[00:02:27.440 --> 00:02:28.860]   So the first thing I wanted to do
[00:02:28.860 --> 00:02:31.400]   was actually improve this model a bit.
[00:02:31.400 --> 00:02:34.060]   But this is kind of an endless exercise
[00:02:34.060 --> 00:02:37.360]   that I'd love for you all to explore and try
[00:02:37.360 --> 00:02:39.020]   a number of different techniques to get
[00:02:39.020 --> 00:02:41.960]   the accuracy of this model up before we actually deploy it.
[00:02:41.960 --> 00:02:50.000]   So the first thing I would do is add some convolutional layers.
[00:02:50.000 --> 00:02:53.880]   So here, let's go ahead and import our COM 2D layer.
[00:02:53.880 --> 00:02:58.800]   And we can also import our max pooling 2D layers.
[00:02:58.800 --> 00:03:03.480]   Now, instead of flattening the data,
[00:03:03.480 --> 00:03:07.040]   we can actually take the data and input it directly
[00:03:07.040 --> 00:03:08.720]   into a COM 2D layer.
[00:03:08.720 --> 00:03:13.080]   Now, the number of filters we want here
[00:03:13.080 --> 00:03:16.880]   is fairly arbitrary, but let's go ahead and choose 32.
[00:03:16.880 --> 00:03:18.720]   And we'll just use a 3 by 3 kernel.
[00:03:19.640 --> 00:03:23.640]   And we want an activation function.
[00:03:23.640 --> 00:03:24.640]   So let's use relu.
[00:03:24.640 --> 00:03:29.640]   Now, we can go ahead and add our max pooling
[00:03:29.640 --> 00:03:32.680]   so that the network can learn across spatial dimensions.
[00:03:32.680 --> 00:03:37.600]   And by default, this is just going to be a 2 by 2 pool.
[00:03:37.600 --> 00:03:44.880]   And here, let's add another convolution layer.
[00:03:44.880 --> 00:03:46.920]   Because we're working with half-dimensional data,
[00:03:46.920 --> 00:03:48.720]   we can add a convolution layer.
[00:03:48.720 --> 00:03:50.880]   Because we're working with half the amount of data
[00:03:50.880 --> 00:03:52.720]   as we were before because of the pooling,
[00:03:52.720 --> 00:03:55.920]   we can likely afford to do twice as many convolutions
[00:03:55.920 --> 00:03:56.720]   as we did earlier.
[00:03:56.720 --> 00:04:02.120]   Now, finally, before we go into our dents, we should flatten.
[00:04:02.120 --> 00:04:11.200]   And we may as well add one more hidden layer
[00:04:11.200 --> 00:04:13.680]   to increase the expressibility of the model.
[00:04:13.680 --> 00:04:16.120]   [TYPING]
[00:04:16.120 --> 00:04:20.840]   Great.
[00:04:20.840 --> 00:04:22.680]   So with just a few lines, hopefully, we've
[00:04:22.680 --> 00:04:25.160]   dramatically increased the performance of our model.
[00:04:25.160 --> 00:04:27.760]   And there's a whole bunch of other things we can do here.
[00:04:27.760 --> 00:04:30.680]   But I'll leave it for you guys to check out
[00:04:30.680 --> 00:04:33.320]   some of our other videos around more advanced techniques
[00:04:33.320 --> 00:04:36.960]   on computer vision and classification.
[00:04:36.960 --> 00:04:40.000]   [TYPING]
[00:04:40.000 --> 00:04:41.480]   [CLICK]
[00:04:41.480 --> 00:04:49.440]   OK, so we can see that this new model is taking much longer
[00:04:49.440 --> 00:04:53.080]   to train because we're doing far more operations
[00:04:53.080 --> 00:04:53.960]   within the network.
[00:04:53.960 --> 00:04:56.280]   But that additional complexity has actually
[00:04:56.280 --> 00:04:59.400]   gotten us quite a bit of accuracy.
[00:04:59.400 --> 00:05:02.440]   So let's go ahead and take a look at this new model.
[00:05:02.440 --> 00:05:04.040]   Here, we can see after a single epoch,
[00:05:04.040 --> 00:05:07.600]   we're already almost up to 50% accuracy,
[00:05:07.600 --> 00:05:11.600]   which sounds good to me.
[00:05:11.600 --> 00:05:13.320]   And just looking at some of the examples,
[00:05:13.320 --> 00:05:15.900]   it seems like we're definitely making a more accurate model.
[00:05:15.900 --> 00:05:18.880]   [AUDIO OUT]
[00:05:18.880 --> 00:05:21.840]   [AUDIO OUT]
[00:05:21.840 --> 00:05:24.820]   [AUDIO OUT]
[00:05:24.820 --> 00:05:27.800]   [AUDIO OUT]
[00:05:27.800 --> 00:05:30.800]   [AUDIO OUT]
[00:05:30.800 --> 00:05:33.800]   [AUDIO OUT]
[00:05:33.800 --> 00:05:36.800]   [AUDIO OUT]
[00:05:36.800 --> 00:05:37.800]   [AUDIO OUT]
[00:05:37.800 --> 00:05:40.880]   So we've created a model that definitely gets higher accuracy,
[00:05:40.880 --> 00:05:44.160]   but that additional complexity has come at a cost.
[00:05:44.160 --> 00:05:46.600]   We can see here, looking at our accuracy curves,
[00:05:46.600 --> 00:05:50.440]   that our validation accuracy has actually plateaued,
[00:05:50.440 --> 00:05:53.800]   while our accuracy is continuing to improve,
[00:05:53.800 --> 00:05:56.360]   which is a real clear indicator of overfitting.
[00:05:56.360 --> 00:05:59.740]   So the first thing I would try would be to add dropout.
[00:05:59.740 --> 00:06:02.600]   And the next would be to check out our video on data
[00:06:02.600 --> 00:06:06.120]   augmentation and create more training data
[00:06:06.120 --> 00:06:09.840]   so that the model can't memorize any individual specific
[00:06:09.840 --> 00:06:10.720]   examples.
[00:06:10.720 --> 00:06:12.440]   But I think this model is good enough
[00:06:12.440 --> 00:06:15.040]   to create a cool deployment.
[00:06:15.040 --> 00:06:17.960]   So let's go ahead and look at what that looks like.
[00:06:17.960 --> 00:06:20.720]   So inside the EmotionClassifier directory,
[00:06:20.720 --> 00:06:22.880]   I have a couple helper scripts here.
[00:06:22.880 --> 00:06:25.160]   So the first one is convert.sh.
[00:06:25.160 --> 00:06:28.400]   Let's go ahead and just look and see what's inside of that.
[00:06:28.400 --> 00:06:31.120]   So convert.sh is just a simple bash script
[00:06:31.120 --> 00:06:33.520]   that's calling TensorFlow.js converter.
[00:06:33.520 --> 00:06:37.160]   And this is something that gets installed with the TensorFlow.js
[00:06:37.160 --> 00:06:38.180]   Python library.
[00:06:38.180 --> 00:06:39.720]   And here, we're telling it that we're
[00:06:39.720 --> 00:06:41.520]   going to give it a Keras model.
[00:06:41.520 --> 00:06:44.440]   And we want it to do quantization.
[00:06:44.440 --> 00:06:48.680]   So the quantization is going to actually decrease
[00:06:48.680 --> 00:06:51.200]   the amount of bytes that we're going to express
[00:06:51.200 --> 00:06:55.640]   each of our weights and biases in the network from 4 down to 2.
[00:06:55.640 --> 00:06:57.720]   So it should make the bottle about half the size
[00:06:57.720 --> 00:07:02.800]   as it would be if we used full 32-bit floating point numbers.
[00:07:02.800 --> 00:07:06.600]   And I'm having it put the generated TensorFlow.js
[00:07:06.600 --> 00:07:10.440]   model into app/models, which is-- inside of app
[00:07:10.440 --> 00:07:12.360]   is a little web application that I made,
[00:07:12.360 --> 00:07:15.760]   which we're going to use to actually watch the model
[00:07:15.760 --> 00:07:17.920]   perform inference in real time.
[00:07:17.920 --> 00:07:21.400]   So let's go ahead and run convert.sh.
[00:07:21.400 --> 00:07:23.320]   Cool, so it says we converted the model.
[00:07:23.320 --> 00:07:25.360]   Now we can run serve.sh.
[00:07:25.360 --> 00:07:28.440]   So let's just look at serve real quick.
[00:07:28.440 --> 00:07:29.920]   So serve is pretty simple.
[00:07:29.920 --> 00:07:33.720]   It's going to CD into app and start a simple Python HTTP
[00:07:33.720 --> 00:07:34.400]   server.
[00:07:34.400 --> 00:07:37.040]   And then we're using a service called servio.net
[00:07:37.040 --> 00:07:41.720]   to actually SSH tunnel into this machine
[00:07:41.720 --> 00:07:43.360]   and give us a unique URL that we can
[00:07:43.360 --> 00:07:45.520]   access the application with.
[00:07:45.520 --> 00:07:49.560]   OK, so let's go ahead and run serve.sh.
[00:07:49.560 --> 00:07:51.240]   Cool, so you can see in the output
[00:07:51.240 --> 00:07:55.040]   that we're now forwarding all HTTP traffic to this URL
[00:07:55.040 --> 00:07:56.040]   at servio.net.
[00:07:56.040 --> 00:08:01.400]   So we can copy that URL and open it in our browser.
[00:08:01.400 --> 00:08:03.280]   And now this is actually a web server that's
[00:08:03.280 --> 00:08:07.040]   talking to our local machine.
[00:08:07.040 --> 00:08:10.320]   And you can see Lucas and I here.
[00:08:10.320 --> 00:08:13.360]   So it's only going to grab the first face.
[00:08:13.360 --> 00:08:16.320]   So I think it's whoever's face is closest.
[00:08:16.320 --> 00:08:19.920]   We can hit predict once, which is going to grab a frame
[00:08:19.920 --> 00:08:24.400]   and then crop around just our faces, make it grayscale,
[00:08:24.400 --> 00:08:26.400]   and then put it through our actual classifier.
[00:08:26.400 --> 00:08:29.620]   So here we can see that it took about 20 milliseconds
[00:08:29.620 --> 00:08:30.880]   to perform inference.
[00:08:30.880 --> 00:08:34.720]   And it thinks that I'm angry with a 32% confidence.
[00:08:34.720 --> 00:08:36.840]   What's really fun with this application
[00:08:36.840 --> 00:08:38.720]   is we can actually stream the predictions.
[00:08:38.720 --> 00:08:41.320]   So if I start streaming, now it's
[00:08:41.320 --> 00:08:44.920]   going to be predicting what my emotions are while I talk.
[00:08:44.920 --> 00:08:47.400]   And it looks like it's going to flip between Lucas and I
[00:08:47.400 --> 00:08:50.280]   randomly as well.
[00:08:50.280 --> 00:08:50.780]   Let's see.
[00:08:50.780 --> 00:08:54.040]   Let's see if I can get it happy.
[00:08:54.040 --> 00:08:57.640]   Ah, it was there for a second.
[00:08:57.640 --> 00:08:59.000]   It's really favoring angry.
[00:08:59.000 --> 00:09:04.040]   Luke, you want to get in there?
[00:09:04.040 --> 00:09:04.360]   LUKE DAVIS: Let me try.
[00:09:04.360 --> 00:09:05.200]   Let me try to get up.
[00:09:07.840 --> 00:09:10.880]   Yeah, Luke, it works for me.
[00:09:10.880 --> 00:09:12.640]   It doesn't like my beards.
[00:09:12.640 --> 00:09:15.880]   I don't think we had that many beards in the training data.
[00:09:15.880 --> 00:09:20.040]   So as you can clearly see, making a model like this
[00:09:20.040 --> 00:09:21.760]   makes me very happy.
[00:09:21.760 --> 00:09:24.320]   And the model is actually able to detect that.
[00:09:24.320 --> 00:09:26.820]   Although it seems to be bouncing back and forth between sad
[00:09:26.820 --> 00:09:27.680]   and--
[00:09:27.680 --> 00:09:31.040]   OK, maybe the model is just a little confused about my beard.
[00:09:31.040 --> 00:09:34.720]   But with some more training data and maybe some dropout,
[00:09:34.720 --> 00:09:36.500]   I think it could get it right for sure.
[00:09:36.500 --> 00:09:40.000]   So all of this code is freely available within a repository.
[00:09:40.000 --> 00:09:42.440]   And it's a great starting point to play
[00:09:42.440 --> 00:09:45.160]   with emotion detection.
[00:09:45.160 --> 00:09:47.640]   I love this use case especially because I
[00:09:47.640 --> 00:09:50.600]   can imagine a lot of fun things we could do with this.
[00:09:50.600 --> 00:09:52.880]   Right now, we have this emotion classifier
[00:09:52.880 --> 00:09:54.720]   running in the background in our browser.
[00:09:54.720 --> 00:09:57.180]   And maybe if it starts to detect that we're getting a little
[00:09:57.180 --> 00:10:01.800]   frustrated or angry, it could prompt us to step back, check
[00:10:01.800 --> 00:10:05.800]   in, and hopefully feel better.
[00:10:05.800 --> 00:10:07.840]   We'd love to see what you guys do with it.
[00:10:07.840 --> 00:10:12.360]   And we encourage you to write it in the comments
[00:10:12.360 --> 00:10:14.320]   and send us screenshots of the stuff
[00:10:14.320 --> 00:10:19.680]   you can do with this real-time emotion detection model.
[00:10:19.680 --> 00:10:22.740]   [NO AUDIO]
[00:10:22.740 --> 00:10:25.180]   [NO AUDIO]
[00:10:25.180 --> 00:10:27.660]   [NO AUDIO]
[00:10:27.660 --> 00:10:30.140]   [NO AUDIO]
[00:10:30.140 --> 00:10:32.140]   [NO AUDIO]
[00:10:32.140 --> 00:10:42.140]   [BLANK_AUDIO]

