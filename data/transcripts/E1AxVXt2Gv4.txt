
[00:00:00.000 --> 00:00:03.480]   The following is a conversation with Marcus Hutter,
[00:00:03.480 --> 00:00:06.680]   senior research scientist at Google DeepMind.
[00:00:06.680 --> 00:00:08.280]   Throughout his career of research,
[00:00:08.280 --> 00:00:11.600]   including with Juergen Schmidhuber and Shane Legg,
[00:00:11.600 --> 00:00:13.920]   he has proposed a lot of interesting ideas
[00:00:13.920 --> 00:00:17.480]   in and around the field of artificial general intelligence,
[00:00:17.480 --> 00:00:22.480]   including the development of AIXI, spelled A-I-X-I, model,
[00:00:22.480 --> 00:00:25.360]   which is a mathematical approach to AGI
[00:00:25.360 --> 00:00:28.840]   that incorporates ideas of Kolmogorov complexity,
[00:00:28.840 --> 00:00:32.000]   Solomonov induction, and reinforcement learning.
[00:00:32.000 --> 00:00:38.040]   In 2006, Marcus launched the 50,000 Euro Hutter Prize
[00:00:38.040 --> 00:00:41.160]   for Lossless Compression of Human Knowledge.
[00:00:41.160 --> 00:00:43.700]   The idea behind this prize is that the ability
[00:00:43.700 --> 00:00:47.880]   to compress well is closely related to intelligence.
[00:00:47.880 --> 00:00:50.320]   This, to me, is a profound idea.
[00:00:50.320 --> 00:00:54.880]   Specifically, if you can compress the first 100 megabytes
[00:00:54.880 --> 00:00:58.320]   or one gigabyte of Wikipedia better than your predecessors,
[00:00:58.320 --> 00:01:02.160]   your compressor likely has to also be smarter.
[00:01:02.160 --> 00:01:04.240]   The intention of this prize is to encourage
[00:01:04.240 --> 00:01:06.400]   the development of intelligent compressors
[00:01:06.400 --> 00:01:08.360]   as a path to AGI.
[00:01:08.360 --> 00:01:13.240]   In conjunction with his podcast release just a few days ago,
[00:01:13.240 --> 00:01:15.520]   Marcus announced a 10X increase
[00:01:15.520 --> 00:01:17.440]   in several aspects of this prize,
[00:01:17.440 --> 00:01:22.440]   including the money, to 500,000 Euros.
[00:01:22.440 --> 00:01:24.160]   The better your compressor works
[00:01:24.160 --> 00:01:26.040]   relative to the previous winners,
[00:01:26.040 --> 00:01:27.680]   the higher fraction of that prize money
[00:01:27.680 --> 00:01:29.400]   is awarded to you.
[00:01:29.400 --> 00:01:30.800]   You can learn more about it
[00:01:30.800 --> 00:01:34.060]   if you Google simply Hutter Prize.
[00:01:34.060 --> 00:01:38.080]   I'm a big fan of benchmarks for developing AI systems,
[00:01:38.080 --> 00:01:40.600]   and the Hutter Prize may indeed be one
[00:01:40.600 --> 00:01:43.120]   that will spark some good ideas for approaches
[00:01:43.120 --> 00:01:47.840]   that will make progress on the path of developing AGI systems.
[00:01:47.840 --> 00:01:50.520]   This is the Artificial Intelligence Podcast.
[00:01:50.520 --> 00:01:52.720]   If you enjoy it, subscribe on YouTube,
[00:01:52.720 --> 00:01:54.680]   give it five stars on Apple Podcasts,
[00:01:54.680 --> 00:01:56.000]   support it on Patreon,
[00:01:56.000 --> 00:01:59.240]   or simply connect with me on Twitter @LexFriedman,
[00:01:59.240 --> 00:02:02.600]   spelled F-R-I-D-M-A-N.
[00:02:02.600 --> 00:02:05.480]   As usual, I'll do one or two minutes of ads now,
[00:02:05.480 --> 00:02:06.960]   and never any ads in the middle
[00:02:06.960 --> 00:02:09.160]   that can break the flow of the conversation.
[00:02:09.160 --> 00:02:10.600]   I hope that works for you
[00:02:10.600 --> 00:02:13.180]   and doesn't hurt the listening experience.
[00:02:13.180 --> 00:02:15.360]   This show is presented by Cash App,
[00:02:15.360 --> 00:02:17.760]   the number one finance app in the App Store.
[00:02:17.760 --> 00:02:21.200]   When you get it, use code LEXPODCAST.
[00:02:21.200 --> 00:02:23.520]   Cash App lets you send money to friends,
[00:02:23.520 --> 00:02:26.040]   buy Bitcoin, and invest in the stock market
[00:02:26.040 --> 00:02:27.920]   with as little as $1.
[00:02:27.920 --> 00:02:30.960]   Brokerage services are provided by Cash App Investing,
[00:02:30.960 --> 00:02:35.000]   a subsidiary of Square, a member SIPC.
[00:02:35.000 --> 00:02:36.720]   Since Cash App allows you to send
[00:02:36.720 --> 00:02:39.160]   and receive money digitally, peer-to-peer,
[00:02:39.160 --> 00:02:42.760]   and security in all digital transactions is very important,
[00:02:42.760 --> 00:02:45.880]   let me mention the PCI Data Security Standard
[00:02:45.880 --> 00:02:48.120]   that Cash App is compliant with.
[00:02:48.120 --> 00:02:52.080]   I'm a big fan of standards for safety and security.
[00:02:52.080 --> 00:02:55.080]   PCI DSS is a good example of that,
[00:02:55.080 --> 00:02:57.180]   where a bunch of competitors got together
[00:02:57.180 --> 00:02:59.960]   and agreed that there needs to be a global standard
[00:02:59.960 --> 00:03:02.520]   around the security of transactions.
[00:03:02.520 --> 00:03:06.040]   Now, we just need to do the same for autonomous vehicles
[00:03:06.040 --> 00:03:08.880]   and AI systems in general.
[00:03:08.880 --> 00:03:10.760]   So again, if you get Cash App
[00:03:10.760 --> 00:03:12.560]   from the App Store or Google Play,
[00:03:12.560 --> 00:03:16.400]   and use the code LEXPODCAST, you'll get $10,
[00:03:16.400 --> 00:03:19.240]   and Cash App will also donate $10 to FIRST,
[00:03:19.240 --> 00:03:21.280]   one of my favorite organizations
[00:03:21.280 --> 00:03:24.520]   that is helping to advance robotics and STEM education
[00:03:24.520 --> 00:03:26.760]   for young people around the world.
[00:03:26.760 --> 00:03:31.680]   And now, here's my conversation with Markus Hutter.
[00:03:31.680 --> 00:03:34.480]   Do you think of the universe as a computer
[00:03:34.480 --> 00:03:37.020]   or maybe an information processing system?
[00:03:37.020 --> 00:03:39.080]   Let's go with a big question first.
[00:03:39.080 --> 00:03:41.560]   - Okay, with a big question first.
[00:03:41.560 --> 00:03:45.240]   I think it's a very interesting hypothesis or idea,
[00:03:45.240 --> 00:03:47.960]   and I have a background in physics,
[00:03:47.960 --> 00:03:50.800]   so I know a little bit about physical theories,
[00:03:50.800 --> 00:03:52.440]   the standard model of particle physics
[00:03:52.440 --> 00:03:54.440]   and general relativity theory,
[00:03:54.440 --> 00:03:56.440]   and they are amazing and describe
[00:03:56.440 --> 00:03:57.920]   virtually everything in the universe,
[00:03:57.920 --> 00:03:59.760]   and they're all, in a sense, computable theories.
[00:03:59.760 --> 00:04:01.800]   I mean, they're very hard to compute.
[00:04:01.800 --> 00:04:04.360]   And it's very elegant, simple theories
[00:04:04.360 --> 00:04:07.260]   which describe virtually everything in the universe.
[00:04:07.260 --> 00:04:10.240]   So there's a strong indication
[00:04:10.240 --> 00:04:15.240]   that somehow the universe is computable,
[00:04:15.240 --> 00:04:17.720]   but it's a plausible hypothesis.
[00:04:17.720 --> 00:04:19.520]   - So why do you think, just like you said,
[00:04:19.520 --> 00:04:22.240]   general relativity, quantum field theory,
[00:04:22.240 --> 00:04:24.780]   why do you think that the laws of physics
[00:04:24.780 --> 00:04:28.960]   are so nice and beautiful and simple and compressible?
[00:04:28.960 --> 00:04:32.780]   Do you think our universe was designed,
[00:04:32.780 --> 00:04:34.200]   is naturally this way?
[00:04:34.200 --> 00:04:36.720]   Are we just focusing on the parts
[00:04:36.720 --> 00:04:39.520]   that are especially compressible?
[00:04:39.520 --> 00:04:42.760]   Our human minds just enjoy something about that simplicity,
[00:04:42.760 --> 00:04:44.840]   and in fact, there's other things
[00:04:44.840 --> 00:04:46.720]   that are not so compressible.
[00:04:46.720 --> 00:04:49.400]   - No, I strongly believe, and I'm pretty convinced
[00:04:49.400 --> 00:04:52.560]   that the universe is inherently beautiful, elegant,
[00:04:52.560 --> 00:04:55.520]   and simple, and described by these equations,
[00:04:55.520 --> 00:04:57.640]   and we're not just picking that.
[00:04:57.640 --> 00:05:00.040]   I mean, if there were some phenomena
[00:05:00.040 --> 00:05:02.680]   which cannot be neatly described,
[00:05:02.680 --> 00:05:04.640]   scientists would try that, right?
[00:05:04.640 --> 00:05:06.720]   And there's biology which is more messy,
[00:05:06.720 --> 00:05:09.280]   but we understand that it's an emergent phenomena,
[00:05:09.280 --> 00:05:11.000]   and it's complex systems,
[00:05:11.000 --> 00:05:12.680]   but they still follow the same rules, right,
[00:05:12.680 --> 00:05:14.640]   of quantum and electrodynamics.
[00:05:14.640 --> 00:05:16.560]   All of chemistry follows that, and we know that.
[00:05:16.560 --> 00:05:18.140]   I mean, we cannot compute everything
[00:05:18.140 --> 00:05:20.320]   because we have limited computational resources.
[00:05:20.320 --> 00:05:22.080]   No, I think it's not a bias of the humans,
[00:05:22.080 --> 00:05:24.000]   but it's objectively simple.
[00:05:24.000 --> 00:05:25.640]   I mean, of course, you never know.
[00:05:25.640 --> 00:05:28.320]   Maybe there's some corners very far out in the universe,
[00:05:28.320 --> 00:05:33.000]   or super, super tiny below the nucleus of atoms,
[00:05:33.000 --> 00:05:38.000]   or, well, parallel universes which are not nice and simple,
[00:05:38.000 --> 00:05:40.560]   but there's no evidence for that,
[00:05:40.560 --> 00:05:42.240]   and we should apply Occam's razor
[00:05:42.240 --> 00:05:45.160]   and choose the simplest tree consistent with it,
[00:05:45.160 --> 00:05:48.040]   but also it's a little bit self-referential.
[00:05:48.040 --> 00:05:49.480]   - So maybe a quick pause.
[00:05:49.480 --> 00:05:51.000]   What is Occam's razor?
[00:05:51.000 --> 00:05:53.880]   - So Occam's razor says that you should not
[00:05:53.880 --> 00:05:57.480]   multiply entities beyond necessity,
[00:05:57.480 --> 00:06:01.400]   which sort of if you translate it to proper English means,
[00:06:01.400 --> 00:06:03.440]   and in the scientific context means
[00:06:03.440 --> 00:06:06.440]   that if you have two theories or hypotheses or models
[00:06:06.440 --> 00:06:09.800]   which equally well describe the phenomenon,
[00:06:09.800 --> 00:06:11.560]   your study or the data,
[00:06:11.560 --> 00:06:13.960]   you should choose the more simple one.
[00:06:13.960 --> 00:06:15.480]   - So that's just the principle?
[00:06:15.480 --> 00:06:16.320]   - Yes.
[00:06:16.320 --> 00:06:20.040]   - So that's not like a provable law perhaps?
[00:06:20.040 --> 00:06:23.480]   Perhaps we'll kind of discuss it and think about it,
[00:06:23.480 --> 00:06:28.080]   but what's the intuition of why the simpler answer
[00:06:28.080 --> 00:06:33.080]   is the one that is likelier to be more correct descriptor
[00:06:33.080 --> 00:06:35.080]   of whatever we're talking about?
[00:06:35.080 --> 00:06:36.560]   - I believe that Occam's razor
[00:06:36.560 --> 00:06:40.260]   is probably the most important principle in science.
[00:06:40.260 --> 00:06:42.040]   I mean, of course, we need logical deduction
[00:06:42.040 --> 00:06:44.520]   and we do experimental design,
[00:06:44.520 --> 00:06:49.520]   but science is about understanding the world,
[00:06:49.520 --> 00:06:51.440]   finding models of the world,
[00:06:51.440 --> 00:06:53.680]   and we can come up with crazy complex models
[00:06:53.680 --> 00:06:55.980]   which explain everything but predict nothing,
[00:06:55.980 --> 00:07:00.200]   but the simple model seem to have predictive power
[00:07:00.200 --> 00:07:03.120]   and it's a valid question why.
[00:07:03.120 --> 00:07:05.960]   And there are two answers to that.
[00:07:05.960 --> 00:07:07.200]   You can just accept it.
[00:07:07.200 --> 00:07:09.200]   That is the principle of science,
[00:07:09.200 --> 00:07:12.820]   and we use this principle and it seems to be successful.
[00:07:12.820 --> 00:07:15.900]   We don't know why, but it just happens to be.
[00:07:15.900 --> 00:07:18.540]   Or you can try, you know, find another principle
[00:07:18.540 --> 00:07:21.100]   which explains Occam's razor.
[00:07:21.100 --> 00:07:24.120]   And if we start with the assumption
[00:07:24.120 --> 00:07:27.560]   that the world is governed by simple rules,
[00:07:27.560 --> 00:07:31.400]   then there's a bias to our simplicity
[00:07:31.400 --> 00:07:33.880]   and applying Occam's razor
[00:07:33.880 --> 00:07:37.080]   is the mechanism to finding these rules.
[00:07:37.080 --> 00:07:39.040]   And actually in a more quantitative sense,
[00:07:39.040 --> 00:07:40.200]   and we come back to that later
[00:07:40.200 --> 00:07:41.680]   in case of somnolent deduction,
[00:07:41.680 --> 00:07:43.000]   you can rigorously prove that.
[00:07:43.000 --> 00:07:45.600]   You have to assume that the world is simple,
[00:07:45.600 --> 00:07:47.680]   then Occam's razor is the best you can do
[00:07:47.680 --> 00:07:49.000]   in a certain sense.
[00:07:49.000 --> 00:07:51.620]   - So I apologize for the romanticized question,
[00:07:51.620 --> 00:07:56.240]   but why do you think, outside of its effectiveness,
[00:07:56.240 --> 00:07:58.360]   why do you think we find simplicity
[00:07:58.360 --> 00:07:59.920]   so appealing as human beings?
[00:07:59.920 --> 00:08:03.480]   Why does it just, why does E equals MC squared
[00:08:03.480 --> 00:08:08.320]   seem so beautiful to us humans?
[00:08:08.320 --> 00:08:10.600]   - I guess mostly, in general,
[00:08:10.600 --> 00:08:14.960]   many things can be explained by an evolutionary argument.
[00:08:14.960 --> 00:08:17.320]   And, you know, there's some artifacts in humans
[00:08:17.320 --> 00:08:21.240]   which are just artifacts and not evolutionary necessary.
[00:08:21.240 --> 00:08:24.160]   But with this beauty and simplicity,
[00:08:24.160 --> 00:08:28.840]   it's, I believe, at least the core,
[00:08:28.840 --> 00:08:32.760]   is about, like science,
[00:08:32.760 --> 00:08:34.520]   finding regularities in the world,
[00:08:34.520 --> 00:08:36.000]   understanding the world,
[00:08:36.000 --> 00:08:37.840]   which is necessary for survival, right?
[00:08:38.040 --> 00:08:40.760]   If I look at a bush, right,
[00:08:40.760 --> 00:08:42.520]   and I just see noise,
[00:08:42.520 --> 00:08:43.760]   and there is a tiger, right,
[00:08:43.760 --> 00:08:45.000]   and eats me, then I'm dead.
[00:08:45.000 --> 00:08:46.840]   But if I try to find a pattern,
[00:08:46.840 --> 00:08:49.560]   and we know that humans are prone to
[00:08:49.560 --> 00:08:54.400]   find more patterns in data than they are,
[00:08:54.400 --> 00:08:57.880]   like the Mars face and all these things,
[00:08:57.880 --> 00:08:59.920]   but this bias towards finding patterns,
[00:08:59.920 --> 00:09:01.120]   even if they are non,
[00:09:01.120 --> 00:09:03.880]   but, I mean, it's best, of course, if they are,
[00:09:03.880 --> 00:09:05.240]   helps us for survival.
[00:09:05.240 --> 00:09:07.440]   - Yeah, that's fascinating.
[00:09:07.440 --> 00:09:09.880]   I haven't thought really about the,
[00:09:09.880 --> 00:09:11.480]   I thought I just loved science,
[00:09:11.480 --> 00:09:16.280]   but indeed, in terms of just for survival purposes,
[00:09:16.280 --> 00:09:18.680]   there is an evolutionary argument
[00:09:18.680 --> 00:09:23.240]   for why we find the work of Einstein so beautiful.
[00:09:23.240 --> 00:09:26.840]   Maybe a quick small tangent.
[00:09:26.840 --> 00:09:30.680]   Could you describe what Solomonov induction is?
[00:09:30.680 --> 00:09:35.040]   - Yeah, so that's a theory which I claim,
[00:09:35.040 --> 00:09:37.680]   and where Solomonov sort of claimed a long time ago
[00:09:37.680 --> 00:09:42.280]   that this solves the big philosophical problem of induction.
[00:09:42.280 --> 00:09:45.080]   And I believe the claim is essentially true.
[00:09:45.080 --> 00:09:47.160]   And what it does is the following.
[00:09:47.160 --> 00:09:52.040]   So, okay, for the picky listener,
[00:09:52.040 --> 00:09:55.880]   induction can be interpreted narrowly and widely.
[00:09:55.880 --> 00:09:59.000]   Narrow means inferring models from data.
[00:09:59.000 --> 00:10:03.080]   And widely means also then using these models
[00:10:03.080 --> 00:10:04.040]   for doing predictions,
[00:10:04.040 --> 00:10:06.560]   so predictions also part of the induction.
[00:10:06.560 --> 00:10:09.440]   So I'm a little sloppy sort of with the terminology,
[00:10:09.440 --> 00:10:13.640]   and maybe that comes from Ray Solomonov being sloppy.
[00:10:13.640 --> 00:10:14.920]   Maybe I shouldn't say that.
[00:10:14.920 --> 00:10:16.080]   (both laughing)
[00:10:16.080 --> 00:10:18.160]   He can't complain anymore.
[00:10:18.160 --> 00:10:22.160]   So let me explain a little bit this theory in simple terms.
[00:10:22.160 --> 00:10:24.280]   So assume you have a data sequence,
[00:10:24.280 --> 00:10:25.760]   make it very simple, the simplest one,
[00:10:25.760 --> 00:10:28.840]   say 1, 1, 1, 1, 1, and you see 100 1s.
[00:10:28.840 --> 00:10:31.000]   What do you think comes next?
[00:10:31.000 --> 00:10:32.680]   The natural answer, I'm gonna speed up a little bit,
[00:10:32.680 --> 00:10:34.720]   the natural answer is, of course, 1.
[00:10:34.720 --> 00:10:38.160]   And the question is why?
[00:10:38.160 --> 00:10:41.000]   Well, we see a pattern there.
[00:10:41.000 --> 00:10:42.800]   There's a 1, and we repeat it.
[00:10:42.800 --> 00:10:45.520]   And why should it suddenly after 100 1s be different?
[00:10:45.520 --> 00:10:49.120]   So what we're looking for is simple explanations or models
[00:10:49.120 --> 00:10:50.760]   for the data we have.
[00:10:50.760 --> 00:10:51.720]   And now the question is,
[00:10:51.720 --> 00:10:55.480]   a model has to be presented in a certain language.
[00:10:55.480 --> 00:10:57.560]   In which language do we use?
[00:10:57.560 --> 00:10:59.440]   In science, we want formal languages,
[00:10:59.440 --> 00:11:00.840]   and we can use mathematics,
[00:11:00.840 --> 00:11:03.880]   or we can use programs on a computer.
[00:11:03.880 --> 00:11:06.440]   So abstractly on a Turing machine, for instance,
[00:11:06.440 --> 00:11:08.440]   or it can be a general purpose computer.
[00:11:08.440 --> 00:11:11.400]   And there are, of course, lots of models.
[00:11:11.400 --> 00:11:12.880]   You can say maybe it's 100 1s,
[00:11:12.880 --> 00:11:15.280]   and then 100 0s, and 100 1s, that's a model, right?
[00:11:15.280 --> 00:11:17.000]   But there are simpler models.
[00:11:17.000 --> 00:11:19.480]   There's a model print one loop.
[00:11:19.480 --> 00:11:21.200]   It also explains the data.
[00:11:21.200 --> 00:11:24.640]   And if you push that to the extreme,
[00:11:24.640 --> 00:11:26.880]   you are looking for the shortest program,
[00:11:26.880 --> 00:11:28.560]   which, if you run this program,
[00:11:28.560 --> 00:11:30.920]   reproduces the data you have.
[00:11:30.920 --> 00:11:33.680]   It will not stop, it will continue, naturally.
[00:11:33.680 --> 00:11:36.120]   And this you take for your prediction.
[00:11:36.120 --> 00:11:38.440]   And on the sequence of 1s, it's very plausible, right?
[00:11:38.440 --> 00:11:40.880]   That print one loop is the shortest program.
[00:11:40.880 --> 00:11:43.000]   We can give some more complex examples,
[00:11:43.000 --> 00:11:45.240]   like one, two, three, four, five.
[00:11:45.240 --> 00:11:46.080]   What comes next?
[00:11:46.080 --> 00:11:48.200]   The short program is again, you know, counter.
[00:11:48.200 --> 00:11:51.440]   And so that is, roughly speaking,
[00:11:51.440 --> 00:11:53.160]   how Solomon's induction works.
[00:11:53.160 --> 00:11:57.760]   The extra twist is that it can also deal with noisy data.
[00:11:57.760 --> 00:11:59.960]   So if you have, for instance, a coin flip,
[00:11:59.960 --> 00:12:01.720]   say a biased coin, which comes up head
[00:12:01.720 --> 00:12:06.320]   with 60% probability, then it will predict.
[00:12:06.320 --> 00:12:07.800]   It will learn and figure this out.
[00:12:07.800 --> 00:12:08.720]   And after a while, it predict,
[00:12:08.720 --> 00:12:12.640]   oh, the next coin flip will be head with probability 60%.
[00:12:12.640 --> 00:12:14.840]   So it's the stochastic version of that.
[00:12:14.840 --> 00:12:16.720]   - But the goal is, the dream is,
[00:12:16.720 --> 00:12:18.760]   always the search for the short program.
[00:12:18.760 --> 00:12:19.600]   - Yes, yeah.
[00:12:19.600 --> 00:12:22.240]   Well, in Solomon of induction, precisely what you do is,
[00:12:22.240 --> 00:12:26.000]   so you combine, so looking for the shortest program
[00:12:26.000 --> 00:12:27.640]   is like applying Opus Razor,
[00:12:27.640 --> 00:12:29.640]   like looking for the simplest theory.
[00:12:29.640 --> 00:12:32.360]   There's also Epicurus principle, which says,
[00:12:32.360 --> 00:12:33.880]   if you have multiple hypothesis,
[00:12:33.880 --> 00:12:35.600]   which equally well describe your data,
[00:12:35.600 --> 00:12:36.760]   don't discard any of them,
[00:12:36.760 --> 00:12:39.120]   keep all of them around, you never know.
[00:12:39.120 --> 00:12:41.000]   And you can put it together and say,
[00:12:41.000 --> 00:12:43.280]   okay, I have a bias towards simplicity,
[00:12:43.280 --> 00:12:45.360]   but I don't rule out the larger models.
[00:12:45.360 --> 00:12:47.320]   And technically what we do is,
[00:12:47.320 --> 00:12:50.840]   we weigh the shorter models higher
[00:12:50.840 --> 00:12:53.040]   and the longer models lower.
[00:12:53.040 --> 00:12:54.640]   And you use a Bayesian techniques,
[00:12:54.640 --> 00:12:59.640]   you have a prior, which is precisely two to the minus
[00:12:59.640 --> 00:13:02.680]   the complexity of the program.
[00:13:02.680 --> 00:13:05.400]   And you weigh all this hypothesis and take this mixture
[00:13:05.400 --> 00:13:07.840]   and then you get also the stochasticity in.
[00:13:07.840 --> 00:13:09.160]   - Yeah, like many of your ideas,
[00:13:09.160 --> 00:13:11.280]   that's just a beautiful idea of weighing
[00:13:11.280 --> 00:13:12.800]   based on the simplicity of the program.
[00:13:12.800 --> 00:13:13.840]   I love that.
[00:13:13.840 --> 00:13:17.760]   That seems to me, maybe a very human-centric concept,
[00:13:17.760 --> 00:13:20.000]   seems to be a very appealing way
[00:13:20.000 --> 00:13:24.080]   of discovering good programs in this world.
[00:13:25.080 --> 00:13:28.240]   You've used the term compression quite a bit.
[00:13:28.240 --> 00:13:30.720]   I think it's a beautiful idea.
[00:13:30.720 --> 00:13:33.080]   Sort of, we just talked about simplicity
[00:13:33.080 --> 00:13:37.760]   and maybe science or just all of our intellectual pursuits
[00:13:37.760 --> 00:13:41.520]   is basically the attempt to compress the complexity
[00:13:41.520 --> 00:13:43.600]   all around us into something simple.
[00:13:43.600 --> 00:13:48.600]   So what does this word mean to you, compression?
[00:13:48.600 --> 00:13:52.040]   - I essentially have already explained it.
[00:13:52.040 --> 00:13:54.440]   So compression means for me,
[00:13:54.440 --> 00:13:59.440]   finding short programs for the data or the phenomenon at hand
[00:13:59.440 --> 00:14:04.440]   you could interpret it more widely as finding simple theories
[00:14:04.440 --> 00:14:05.920]   which can be mathematical theories
[00:14:05.920 --> 00:14:09.480]   or maybe even informal, like just in words.
[00:14:09.480 --> 00:14:12.360]   Compression means finding short descriptions,
[00:14:12.360 --> 00:14:15.320]   explanations, programs for the data.
[00:14:15.320 --> 00:14:18.080]   - Do you see science as a kind of
[00:14:19.640 --> 00:14:22.120]   our human attempt at compression?
[00:14:22.120 --> 00:14:23.560]   So we're speaking more generally
[00:14:23.560 --> 00:14:25.440]   'cause when you say programs,
[00:14:25.440 --> 00:14:26.920]   you're kind of zooming in on a particular
[00:14:26.920 --> 00:14:28.600]   sort of almost like a computer science,
[00:14:28.600 --> 00:14:30.680]   artificial intelligence focus.
[00:14:30.680 --> 00:14:32.400]   But do you see all of human endeavor
[00:14:32.400 --> 00:14:34.840]   as a kind of compression?
[00:14:34.840 --> 00:14:37.400]   - Well, at least all of science I see as a endeavor
[00:14:37.400 --> 00:14:40.160]   of compression, not all of humanity maybe.
[00:14:40.160 --> 00:14:42.640]   And well, there are also some other aspects of science
[00:14:42.640 --> 00:14:44.120]   like experimental design, right?
[00:14:44.120 --> 00:14:47.960]   I mean, we create experiments specifically
[00:14:47.960 --> 00:14:49.240]   to get extra knowledge.
[00:14:49.240 --> 00:14:52.800]   And that is then part of the decision-making process.
[00:14:52.800 --> 00:14:57.360]   But once we have the data to understand the data
[00:14:57.360 --> 00:14:58.640]   is essentially compression.
[00:14:58.640 --> 00:15:01.320]   So I don't see any difference between compression,
[00:15:01.320 --> 00:15:05.060]   understanding and prediction.
[00:15:05.060 --> 00:15:08.040]   - So we're jumping around topics a little bit,
[00:15:08.040 --> 00:15:10.560]   but returning back to simplicity,
[00:15:10.560 --> 00:15:14.400]   a fascinating concept of Kolmogorov complexity.
[00:15:14.400 --> 00:15:17.160]   So in your sense, do most objects
[00:15:17.160 --> 00:15:19.720]   in our mathematical universe
[00:15:19.720 --> 00:15:22.000]   have high Kolmogorov complexity?
[00:15:22.000 --> 00:15:24.120]   And maybe what is, first of all,
[00:15:24.120 --> 00:15:26.000]   what is Kolmogorov complexity?
[00:15:26.000 --> 00:15:27.840]   - Okay, Kolmogorov complexity
[00:15:27.840 --> 00:15:31.200]   is a notion of simplicity or complexity.
[00:15:31.200 --> 00:15:36.000]   And it takes the compression view to the extreme.
[00:15:36.000 --> 00:15:39.720]   So I explained before that if you have some data sequence,
[00:15:39.720 --> 00:15:41.760]   just think about a file on a computer
[00:15:41.760 --> 00:15:45.160]   and best sort of, you know, just a string of bits.
[00:15:45.160 --> 00:15:49.440]   And if you, and we have data compressors,
[00:15:49.440 --> 00:15:52.040]   like we compress big files into say zip files
[00:15:52.040 --> 00:15:53.720]   with certain compressors.
[00:15:53.720 --> 00:15:56.360]   And you can also produce self-extracting archives.
[00:15:56.360 --> 00:15:59.120]   That means as an executable, if you run it,
[00:15:59.120 --> 00:16:00.760]   it reproduces your original file
[00:16:00.760 --> 00:16:02.840]   without needing an extra decompressor.
[00:16:02.840 --> 00:16:06.240]   It's just the decompressor plus the archive together in one.
[00:16:06.240 --> 00:16:08.800]   And now there are better and worse compressors.
[00:16:08.800 --> 00:16:11.120]   And you can ask, what is the ultimate compressor?
[00:16:11.120 --> 00:16:14.840]   So what is the shortest possible self-extracting archive
[00:16:14.840 --> 00:16:17.880]   you could produce for a certain data set, yeah?
[00:16:17.880 --> 00:16:19.560]   Which reproduces the data set.
[00:16:19.560 --> 00:16:23.280]   And the length of this is called the Kolmogorov complexity.
[00:16:23.280 --> 00:16:26.680]   And arguably, that is the information content
[00:16:26.680 --> 00:16:27.960]   in the data set.
[00:16:27.960 --> 00:16:30.440]   I mean, if the data set is very redundant or very boring,
[00:16:30.440 --> 00:16:31.720]   you can compress it very well.
[00:16:31.720 --> 00:16:34.760]   So the information content should be low.
[00:16:34.760 --> 00:16:36.920]   And you know, it is low according to this definition.
[00:16:36.920 --> 00:16:39.680]   - So it's the length of the shortest program
[00:16:39.680 --> 00:16:41.000]   that summarizes the data?
[00:16:41.000 --> 00:16:42.000]   - Yes, yeah.
[00:16:42.040 --> 00:16:46.280]   - And what's your sense of our sort of universe
[00:16:46.280 --> 00:16:51.280]   when we think about the different objects in our universe,
[00:16:51.280 --> 00:16:55.440]   that we try concepts or whatever at every level,
[00:16:55.440 --> 00:16:58.320]   do they have high or low Kolmogorov complexity?
[00:16:58.320 --> 00:16:59.400]   So what's the hope?
[00:16:59.400 --> 00:17:01.400]   Do we have a lot of hope
[00:17:01.400 --> 00:17:04.400]   in being able to summarize much of our world?
[00:17:04.400 --> 00:17:08.520]   - That's a tricky and difficult question.
[00:17:08.520 --> 00:17:13.520]   So as I said before, I believe that the whole universe,
[00:17:13.520 --> 00:17:16.720]   based on the evidence we have, is very simple.
[00:17:16.720 --> 00:17:19.240]   So it has a very short description.
[00:17:19.240 --> 00:17:23.200]   - Sorry, to linger on that, the whole universe,
[00:17:23.200 --> 00:17:24.040]   what does that mean?
[00:17:24.040 --> 00:17:26.760]   Do you mean at the very basic fundamental level
[00:17:26.760 --> 00:17:28.600]   in order to create the universe?
[00:17:28.600 --> 00:17:29.440]   - Yes, yeah.
[00:17:29.440 --> 00:17:32.120]   So you need a very short program,
[00:17:32.120 --> 00:17:32.960]   when you run it--
[00:17:32.960 --> 00:17:34.040]   - To get the thing going.
[00:17:34.040 --> 00:17:35.040]   - To get the thing going,
[00:17:35.040 --> 00:17:37.480]   and then it will reproduce our universe.
[00:17:37.480 --> 00:17:39.360]   There's a problem with noise.
[00:17:39.360 --> 00:17:42.120]   We can come back to that later, possibly.
[00:17:42.120 --> 00:17:45.280]   - Is noise a problem or is it a bug or a feature?
[00:17:45.280 --> 00:17:49.480]   - I would say it makes our life as a scientist
[00:17:49.480 --> 00:17:52.200]   really, really much harder.
[00:17:52.200 --> 00:17:53.520]   I mean, think about without noise,
[00:17:53.520 --> 00:17:55.980]   we wouldn't need all of the statistics.
[00:17:55.980 --> 00:17:58.920]   - But then maybe we wouldn't feel like there's a free will.
[00:17:58.920 --> 00:18:00.160]   Maybe we need that for the--
[00:18:00.160 --> 00:18:02.360]   - Yeah, this is an illusion
[00:18:02.360 --> 00:18:04.640]   that noise can give you free will.
[00:18:04.640 --> 00:18:06.720]   - At least in that way, it's a feature.
[00:18:06.720 --> 00:18:09.080]   But also, if you don't have noise,
[00:18:09.080 --> 00:18:10.800]   you have chaotic phenomena,
[00:18:10.800 --> 00:18:12.800]   which are effectively like noise.
[00:18:12.800 --> 00:18:15.760]   So we can't get away with statistics even then.
[00:18:15.760 --> 00:18:17.600]   I mean, think about rolling a dice
[00:18:17.600 --> 00:18:19.300]   and forget about quantum mechanics
[00:18:19.300 --> 00:18:21.260]   and you know exactly how you throw it.
[00:18:21.260 --> 00:18:24.080]   But I mean, it's still so hard to compute the trajectory
[00:18:24.080 --> 00:18:26.440]   that effectively it is best to model it
[00:18:26.440 --> 00:18:30.160]   as coming out with a number,
[00:18:30.160 --> 00:18:31.720]   this probability one over six.
[00:18:33.120 --> 00:18:36.400]   But from this set of philosophical
[00:18:36.400 --> 00:18:38.160]   Kolmogorov complexity perspective,
[00:18:38.160 --> 00:18:39.960]   if we didn't have noise,
[00:18:39.960 --> 00:18:43.240]   then arguably you could describe the whole universe
[00:18:43.240 --> 00:18:47.460]   as well as a standard model plus generativity.
[00:18:47.460 --> 00:18:49.680]   I mean, we don't have a theory of everything yet,
[00:18:49.680 --> 00:18:52.280]   but sort of assuming we are close to it or have it, yeah.
[00:18:52.280 --> 00:18:53.520]   Plus the initial conditions,
[00:18:53.520 --> 00:18:55.480]   which may hopefully be simple.
[00:18:55.480 --> 00:18:56.680]   And then you just run it
[00:18:56.680 --> 00:18:59.120]   and then you would reproduce the universe.
[00:18:59.120 --> 00:19:03.560]   But that's spoiled by noise or by chaotic systems
[00:19:03.560 --> 00:19:06.320]   or by initial conditions, which may be complex.
[00:19:06.320 --> 00:19:09.760]   So now if we don't take the whole universe,
[00:19:09.760 --> 00:19:13.800]   but just a subset, just take planet Earth.
[00:19:13.800 --> 00:19:15.640]   Planet Earth cannot be compressed
[00:19:15.640 --> 00:19:17.600]   into a couple of equations.
[00:19:17.600 --> 00:19:19.280]   This is a hugely complex system.
[00:19:19.280 --> 00:19:20.120]   - So interesting.
[00:19:20.120 --> 00:19:21.680]   So when you look at the window,
[00:19:21.680 --> 00:19:23.080]   like the whole thing might be simple,
[00:19:23.080 --> 00:19:26.120]   but when you just take a small window, then--
[00:19:26.120 --> 00:19:28.800]   - It may become complex and that may be counterintuitive,
[00:19:28.800 --> 00:19:31.760]   but there's a very nice analogy.
[00:19:31.760 --> 00:19:34.280]   The book, the library of all books.
[00:19:34.280 --> 00:19:37.000]   So imagine you have a normal library with interesting books
[00:19:37.000 --> 00:19:39.360]   and you go there, great, lots of information
[00:19:39.360 --> 00:19:42.040]   and quite complex, yeah?
[00:19:42.040 --> 00:19:43.360]   So now I create a library
[00:19:43.360 --> 00:19:46.840]   which contains all possible books, say, of 500 pages.
[00:19:46.840 --> 00:19:49.720]   So the first book just has AAAA over all the pages.
[00:19:49.720 --> 00:19:52.280]   The next book, AAAA and ends with B and so on.
[00:19:52.280 --> 00:19:54.240]   I create this library of all books.
[00:19:54.240 --> 00:19:55.760]   I can write a super short program
[00:19:55.760 --> 00:19:57.320]   which creates this library.
[00:19:57.320 --> 00:19:59.040]   So this library which has all books
[00:19:59.040 --> 00:20:01.320]   has zero information content.
[00:20:01.320 --> 00:20:02.920]   And you take a subset of this library
[00:20:02.920 --> 00:20:05.360]   and suddenly you have a lot of information in there.
[00:20:05.360 --> 00:20:06.720]   - So that's fascinating.
[00:20:06.720 --> 00:20:08.360]   I think one of the most beautiful object,
[00:20:08.360 --> 00:20:10.480]   mathematical objects that, at least today,
[00:20:10.480 --> 00:20:12.560]   seems to be understudied or under-talked about
[00:20:12.560 --> 00:20:14.960]   is cellular automata.
[00:20:14.960 --> 00:20:18.600]   What lessons do you draw from sort of the game of life
[00:20:18.600 --> 00:20:20.840]   for cellular automata where you start with the simple rules
[00:20:20.840 --> 00:20:22.880]   just like you're describing with the universe
[00:20:22.880 --> 00:20:26.320]   and somehow complexity emerges?
[00:20:26.320 --> 00:20:30.400]   Do you feel like you have an intuitive grasp
[00:20:30.400 --> 00:20:34.120]   on the fascinating behavior of such systems
[00:20:34.120 --> 00:20:37.560]   where, like you said, some chaotic behavior could happen,
[00:20:37.560 --> 00:20:39.400]   some complexity could emerge,
[00:20:39.400 --> 00:20:43.680]   it could die out in some very rigid structures?
[00:20:43.680 --> 00:20:46.800]   Do you have a sense about cellular automata
[00:20:46.800 --> 00:20:48.200]   that somehow transfers maybe
[00:20:48.200 --> 00:20:50.960]   to the bigger questions of our universe?
[00:20:50.960 --> 00:20:52.000]   - Yeah, the cellular automata,
[00:20:52.000 --> 00:20:54.240]   and especially the converse game of life,
[00:20:54.240 --> 00:20:56.240]   is really great because these rules are so simple.
[00:20:56.240 --> 00:20:57.720]   You can explain it to every child,
[00:20:57.720 --> 00:21:00.280]   and even by hand you can simulate a little bit,
[00:21:00.280 --> 00:21:04.000]   and you see these beautiful patterns emerge,
[00:21:04.000 --> 00:21:06.800]   and people have proven that it's even Turing-complete.
[00:21:06.800 --> 00:21:09.840]   You cannot just use a computer to simulate game of life,
[00:21:09.840 --> 00:21:13.480]   but you can also use game of life to simulate any computer.
[00:21:13.480 --> 00:21:16.520]   That is truly amazing,
[00:21:16.520 --> 00:21:21.240]   and it's the prime example probably to demonstrate
[00:21:21.240 --> 00:21:25.240]   that very simple rules can lead to very rich phenomena.
[00:21:25.240 --> 00:21:26.800]   And people sometimes,
[00:21:26.800 --> 00:21:29.720]   how is chemistry and biology so rich?
[00:21:29.720 --> 00:21:32.400]   I mean, this can't be based on simple rules,
[00:21:32.400 --> 00:21:34.520]   but no, we know quantum electrodynamics
[00:21:34.520 --> 00:21:36.360]   describes all of chemistry,
[00:21:36.360 --> 00:21:38.960]   and we come later back to that.
[00:21:38.960 --> 00:21:40.920]   I claim intelligence can be explained
[00:21:40.920 --> 00:21:43.000]   or described in one single equation,
[00:21:43.000 --> 00:21:44.600]   this very rich phenomenon.
[00:21:44.600 --> 00:21:49.880]   You asked also about whether I understand this phenomenon,
[00:21:49.880 --> 00:21:53.240]   and it's probably not,
[00:21:54.280 --> 00:21:55.560]   and there's this saying,
[00:21:55.560 --> 00:21:56.800]   you never understand really things,
[00:21:56.800 --> 00:21:58.360]   you just get used to them.
[00:21:58.360 --> 00:22:03.360]   And I think I'm pretty used to cellular automata,
[00:22:03.360 --> 00:22:05.640]   so you believe that you understand now
[00:22:05.640 --> 00:22:07.120]   why this phenomenon happens,
[00:22:07.120 --> 00:22:09.280]   but I give you a different example.
[00:22:09.280 --> 00:22:11.800]   I didn't play too much with this converse game of life,
[00:22:11.800 --> 00:22:15.040]   but a little bit more with fractals
[00:22:15.040 --> 00:22:16.240]   and with the Mandelbrot set,
[00:22:16.240 --> 00:22:18.520]   and you know, these beautiful patterns,
[00:22:18.520 --> 00:22:20.000]   just look Mandelbrot set.
[00:22:20.000 --> 00:22:23.320]   And well, when the computers were really slow
[00:22:23.320 --> 00:22:25.360]   and I just had a black and white monitor
[00:22:25.360 --> 00:22:29.120]   and programmed my own programs in assembler too.
[00:22:29.120 --> 00:22:31.040]   - Assembler, wow.
[00:22:31.040 --> 00:22:32.440]   Wow, you're legit.
[00:22:32.440 --> 00:22:33.840]   (both laughing)
[00:22:33.840 --> 00:22:35.760]   - To get these fractals on the screen,
[00:22:35.760 --> 00:22:37.400]   and it was mesmerized, and much later.
[00:22:37.400 --> 00:22:40.320]   So I returned to this, you know, every couple of years,
[00:22:40.320 --> 00:22:42.880]   and then I tried to understand what is going on,
[00:22:42.880 --> 00:22:44.880]   and you can understand a little bit.
[00:22:44.880 --> 00:22:48.560]   So I tried to derive the locations,
[00:22:48.720 --> 00:22:53.560]   you know, there are these circles and the apple shape,
[00:22:53.560 --> 00:22:57.360]   and then you have smaller Mandelbrot sets
[00:22:57.360 --> 00:22:59.040]   recursively in this set.
[00:22:59.040 --> 00:23:01.760]   And there's a way to mathematically,
[00:23:01.760 --> 00:23:03.480]   by solving high order polynomials,
[00:23:03.480 --> 00:23:05.680]   to figure out where these centers are
[00:23:05.680 --> 00:23:08.080]   and what size they are approximately.
[00:23:08.080 --> 00:23:12.520]   And by sort of mathematically approaching this problem,
[00:23:12.520 --> 00:23:17.520]   you slowly get a feeling of why things are like they are.
[00:23:18.080 --> 00:23:23.080]   And that sort of is a first step to understanding
[00:23:23.080 --> 00:23:24.920]   why this rich phenomenon appears.
[00:23:24.920 --> 00:23:27.200]   - Do you think it's possible, what's your intuition?
[00:23:27.200 --> 00:23:28.920]   Do you think it's possible to reverse engineer
[00:23:28.920 --> 00:23:33.680]   and find the short program that generated these fractals
[00:23:33.680 --> 00:23:36.400]   by looking at the fractals?
[00:23:36.400 --> 00:23:37.800]   - Well, in principle, yes.
[00:23:37.800 --> 00:23:42.000]   So, I mean, in principle, what you can do is,
[00:23:42.000 --> 00:23:44.560]   you take any data set, you take these fractals,
[00:23:44.560 --> 00:23:47.480]   or you take whatever your data set, whatever you have,
[00:23:48.120 --> 00:23:51.000]   say a picture of Conway's Game of Life,
[00:23:51.000 --> 00:23:53.200]   and you run through all programs.
[00:23:53.200 --> 00:23:55.280]   You take a program of size one, two, three, four,
[00:23:55.280 --> 00:23:57.040]   and all these programs, run them all in parallel
[00:23:57.040 --> 00:23:59.080]   in so-called dovetailing fashion,
[00:23:59.080 --> 00:24:02.360]   give them computational resources, first one 50%,
[00:24:02.360 --> 00:24:05.520]   second one half resources, and so on, and let them run.
[00:24:05.520 --> 00:24:07.680]   Wait until they hold, give an output,
[00:24:07.680 --> 00:24:09.160]   compare it to your data,
[00:24:09.160 --> 00:24:12.360]   and if some of these programs produce the correct data,
[00:24:12.360 --> 00:24:14.480]   then you stop, and then you have already some program.
[00:24:14.480 --> 00:24:16.880]   It may be a long program because it's faster.
[00:24:16.880 --> 00:24:17.920]   And then you continue,
[00:24:17.920 --> 00:24:19.680]   and you get shorter and shorter programs
[00:24:19.680 --> 00:24:22.480]   until you eventually find the shortest program.
[00:24:22.480 --> 00:24:24.000]   The interesting thing, you can never know
[00:24:24.000 --> 00:24:25.520]   whether it's the shortest program
[00:24:25.520 --> 00:24:27.400]   because there could be an even shorter program,
[00:24:27.400 --> 00:24:29.400]   which is just even slower,
[00:24:29.400 --> 00:24:32.160]   and you just have to wait, yeah?
[00:24:32.160 --> 00:24:34.960]   But asymptotically, and actually after finite time,
[00:24:34.960 --> 00:24:36.440]   you have the shortest program.
[00:24:36.440 --> 00:24:40.400]   So, this is a theoretical but completely impractical way
[00:24:40.400 --> 00:24:44.080]   of finding the underlying structure
[00:24:45.920 --> 00:24:47.360]   in every data set,
[00:24:47.360 --> 00:24:48.960]   and that is what Solomonov induction does
[00:24:48.960 --> 00:24:50.600]   and Kolmogorov complexity.
[00:24:50.600 --> 00:24:52.600]   In practice, of course, we have to approach the problem
[00:24:52.600 --> 00:24:55.120]   more intelligently, and then,
[00:24:55.120 --> 00:25:00.680]   if you take resource limitations into account,
[00:25:00.680 --> 00:25:03.640]   there's, for instance, the field of pseudo-random numbers,
[00:25:03.640 --> 00:25:05.800]   and these are random numbers,
[00:25:05.800 --> 00:25:08.440]   so these are deterministic sequences,
[00:25:08.440 --> 00:25:10.840]   but no algorithm which is fast,
[00:25:10.840 --> 00:25:12.560]   fast means runs in polynomial time,
[00:25:12.560 --> 00:25:15.520]   can detect that it's actually deterministic.
[00:25:15.520 --> 00:25:17.840]   So, we can produce interesting,
[00:25:17.840 --> 00:25:19.440]   I mean, random numbers, maybe not that interesting,
[00:25:19.440 --> 00:25:20.360]   but just an example.
[00:25:20.360 --> 00:25:24.280]   We can produce complex-looking data,
[00:25:24.280 --> 00:25:27.040]   and we can then prove that no fast algorithm
[00:25:27.040 --> 00:25:28.920]   can detect the underlying pattern.
[00:25:28.920 --> 00:25:34.480]   - Which is, unfortunately,
[00:25:34.480 --> 00:25:39.760]   that's a big challenge for our search for simple programs
[00:25:39.760 --> 00:25:42.280]   in the space of artificial intelligence, perhaps.
[00:25:42.280 --> 00:25:44.360]   - Yes, it definitely is for artificial intelligence,
[00:25:44.360 --> 00:25:47.120]   and it's quite surprising that it's,
[00:25:47.120 --> 00:25:48.680]   I can't say easy, I mean,
[00:25:48.680 --> 00:25:52.040]   physicists worked really hard to find these theories,
[00:25:52.040 --> 00:25:55.920]   but apparently, it was possible for human minds
[00:25:55.920 --> 00:25:57.520]   to find these simple rules in the universe.
[00:25:57.520 --> 00:25:59.080]   It could have been different, right?
[00:25:59.080 --> 00:26:00.920]   - It could have been different.
[00:26:00.920 --> 00:26:02.920]   It's awe-inspiring.
[00:26:02.920 --> 00:26:07.680]   So, let me ask another absurdly big question.
[00:26:07.680 --> 00:26:13.080]   What is intelligence, in your view?
[00:26:13.080 --> 00:26:15.160]   So, I have, of course, a definition.
[00:26:15.160 --> 00:26:18.360]   - I wasn't sure what you were gonna say,
[00:26:18.360 --> 00:26:19.880]   'cause you could have just as easily said,
[00:26:19.880 --> 00:26:21.280]   "I have no clue."
[00:26:21.280 --> 00:26:23.600]   - Which many people would say,
[00:26:23.600 --> 00:26:25.440]   but I'm not modest in this question.
[00:26:25.440 --> 00:26:30.200]   So, the informal version,
[00:26:30.200 --> 00:26:33.240]   which I worked out together with Shane Lack,
[00:26:33.240 --> 00:26:35.680]   who co-founded DeepMind,
[00:26:35.680 --> 00:26:38.880]   is that intelligence measures an agent's ability
[00:26:38.880 --> 00:26:42.080]   to perform well in a wide range of environments.
[00:26:43.080 --> 00:26:46.000]   So, that doesn't sound very impressive,
[00:26:46.000 --> 00:26:49.760]   and these words have been very carefully chosen,
[00:26:49.760 --> 00:26:53.160]   and there is a mathematical theory behind that,
[00:26:53.160 --> 00:26:55.160]   and we come back to that later.
[00:26:55.160 --> 00:26:59.880]   And if you look at this definition by itself,
[00:26:59.880 --> 00:27:01.400]   it seems like, yeah, okay,
[00:27:01.400 --> 00:27:03.640]   but it seems a lot of things are missing.
[00:27:03.640 --> 00:27:05.280]   But if you think it through,
[00:27:05.280 --> 00:27:08.960]   then you realize that most,
[00:27:08.960 --> 00:27:10.920]   and I claim all of the other traits,
[00:27:10.920 --> 00:27:12.840]   at least of rational intelligence,
[00:27:12.840 --> 00:27:14.680]   which we usually associate with intelligence,
[00:27:14.680 --> 00:27:18.200]   are emergent phenomena from this definition.
[00:27:18.200 --> 00:27:20.280]   Like, you know, creativity, memorization,
[00:27:20.280 --> 00:27:22.400]   planning, knowledge.
[00:27:22.400 --> 00:27:25.160]   You all need that in order to perform well
[00:27:25.160 --> 00:27:27.600]   in a wide range of environments.
[00:27:27.600 --> 00:27:29.320]   So, you don't have to explicitly mention that
[00:27:29.320 --> 00:27:30.160]   in a definition.
[00:27:30.160 --> 00:27:31.000]   - Interesting.
[00:27:31.000 --> 00:27:34.200]   So, yeah, so the consciousness, abstract reasoning,
[00:27:34.200 --> 00:27:36.400]   all these kinds of things are just emergent phenomena
[00:27:36.480 --> 00:27:39.880]   that help you in towards,
[00:27:39.880 --> 00:27:42.080]   can you say the definition again?
[00:27:42.080 --> 00:27:44.400]   So, multiple environments.
[00:27:44.400 --> 00:27:46.080]   Did you mention the word goals?
[00:27:46.080 --> 00:27:47.960]   - No, but we have an alternative definition.
[00:27:47.960 --> 00:27:49.000]   Instead of performing well,
[00:27:49.000 --> 00:27:50.400]   you can just replace it by goals.
[00:27:50.400 --> 00:27:53.520]   So, intelligence measures an agent's ability
[00:27:53.520 --> 00:27:55.920]   to achieve goals in a wide range of environments.
[00:27:55.920 --> 00:27:56.760]   That's more or less equal.
[00:27:56.760 --> 00:27:58.160]   - But it's interesting, 'cause in there,
[00:27:58.160 --> 00:27:59.960]   there's an injection of the word goals.
[00:27:59.960 --> 00:28:03.360]   So, we wanna specify there should be a goal.
[00:28:03.360 --> 00:28:05.000]   - Yeah, but perform well is sort of,
[00:28:05.000 --> 00:28:06.440]   what does it mean?
[00:28:06.440 --> 00:28:07.280]   It's the same problem.
[00:28:07.280 --> 00:28:09.440]   - Yeah, there's a little bit of a gray area,
[00:28:09.440 --> 00:28:12.480]   but it's much closer to something that could be formalized.
[00:28:12.480 --> 00:28:16.520]   In your view, are humans,
[00:28:16.520 --> 00:28:18.480]   where do humans fit into that definition?
[00:28:18.480 --> 00:28:22.080]   Are they general intelligence systems
[00:28:22.080 --> 00:28:24.240]   that are able to perform in,
[00:28:24.240 --> 00:28:28.000]   like how good are they at fulfilling that definition,
[00:28:28.000 --> 00:28:31.360]   at performing well in multiple environments?
[00:28:31.360 --> 00:28:32.880]   - Yeah, that's a big question.
[00:28:32.880 --> 00:28:35.960]   I mean, the humans are performing best among all--
[00:28:35.960 --> 00:28:37.720]   - Species on Earth?
[00:28:37.720 --> 00:28:40.800]   - Species we know of, yeah.
[00:28:40.800 --> 00:28:43.480]   - Depends, you could say that trees and plants
[00:28:43.480 --> 00:28:44.560]   are doing a better job.
[00:28:44.560 --> 00:28:46.840]   They'll probably outlast us.
[00:28:46.840 --> 00:28:49.520]   - Yeah, but they are in a much more narrow environment.
[00:28:49.520 --> 00:28:51.800]   I mean, you just have a little bit of air pollutions
[00:28:51.800 --> 00:28:54.160]   and these trees die, and we can adapt.
[00:28:54.160 --> 00:28:55.840]   We build houses, we build filters,
[00:28:55.840 --> 00:28:59.600]   we do geoengineering.
[00:28:59.600 --> 00:29:01.160]   - So, the multiple environment part.
[00:29:01.160 --> 00:29:02.720]   - Yeah, that is very important.
[00:29:02.720 --> 00:29:04.760]   So, that distinguished narrow intelligence
[00:29:04.760 --> 00:29:07.440]   from wide intelligence, also in the AI research.
[00:29:07.440 --> 00:29:12.200]   - So, let me ask the Alan Turing question.
[00:29:12.200 --> 00:29:14.280]   Can machines think?
[00:29:14.280 --> 00:29:16.000]   Can machines be intelligent?
[00:29:16.000 --> 00:29:19.680]   So, in your view, I have to kind of ask,
[00:29:19.680 --> 00:29:20.680]   the answer's probably yes,
[00:29:20.680 --> 00:29:24.480]   but I wanna kind of hear your thoughts on it.
[00:29:24.480 --> 00:29:27.840]   Can machines be made to fulfill this definition
[00:29:27.840 --> 00:29:30.880]   of intelligence, to achieve intelligence?
[00:29:30.880 --> 00:29:33.120]   - Well, we are sort of getting there,
[00:29:33.120 --> 00:29:36.000]   and on a small scale, we are already there.
[00:29:36.000 --> 00:29:39.120]   The wide range of environments are missing,
[00:29:39.120 --> 00:29:40.440]   but we have self-driving cars,
[00:29:40.440 --> 00:29:42.840]   we have programs which play Go and chess,
[00:29:42.840 --> 00:29:44.560]   we have speech recognition.
[00:29:44.560 --> 00:29:46.920]   So, it's pretty amazing, but you can,
[00:29:46.920 --> 00:29:48.520]   these are narrow environments.
[00:29:48.520 --> 00:29:51.120]   But if you look at AlphaZero,
[00:29:51.120 --> 00:29:53.840]   that was also developed by DeepMind.
[00:29:53.840 --> 00:29:55.480]   I mean, got famous with AlphaGo,
[00:29:55.480 --> 00:29:57.800]   and then came AlphaZero a year later.
[00:29:57.800 --> 00:29:59.360]   That was truly amazing.
[00:29:59.360 --> 00:30:01.880]   So, reinforcement learning algorithm,
[00:30:01.880 --> 00:30:06.120]   which is able just by self-play to play chess,
[00:30:06.120 --> 00:30:08.680]   and then also Go.
[00:30:08.680 --> 00:30:10.200]   And I mean, yes, they're both games,
[00:30:10.200 --> 00:30:11.480]   but they're quite different games.
[00:30:11.480 --> 00:30:13.440]   And you know, there's, you didn't,
[00:30:13.440 --> 00:30:15.200]   don't feed them the rules of the game.
[00:30:15.200 --> 00:30:16.680]   And the most remarkable thing,
[00:30:16.680 --> 00:30:18.040]   which is still a mystery to me,
[00:30:18.040 --> 00:30:21.000]   that usually for any decent chess program,
[00:30:21.000 --> 00:30:22.760]   I don't know much about Go,
[00:30:22.760 --> 00:30:25.760]   you need opening books and end game tables,
[00:30:25.760 --> 00:30:26.880]   and so on, too.
[00:30:26.960 --> 00:30:29.680]   And nothing in there, nothing was put in there.
[00:30:29.680 --> 00:30:31.080]   - Especially with AlphaZero,
[00:30:31.080 --> 00:30:33.560]   the self-play mechanism, starting from scratch,
[00:30:33.560 --> 00:30:38.560]   being able to learn, actually new strategies is--
[00:30:38.560 --> 00:30:43.080]   - Yeah, it rediscovered all these famous openings
[00:30:43.080 --> 00:30:46.320]   within four hours by itself.
[00:30:46.320 --> 00:30:47.520]   What I was really happy about,
[00:30:47.520 --> 00:30:50.200]   I'm a terrible chess player, but I like Queen Gambi,
[00:30:50.200 --> 00:30:53.200]   and AlphaZero figured out that this is the best opening.
[00:30:53.200 --> 00:30:54.720]   (both laughing)
[00:30:54.720 --> 00:30:55.560]   - Finally.
[00:30:56.800 --> 00:30:58.320]   Somebody proved you correct.
[00:30:58.320 --> 00:31:01.680]   - So yes, to answer your question,
[00:31:01.680 --> 00:31:05.040]   yes, I believe that general intelligence is possible.
[00:31:05.040 --> 00:31:08.240]   And it also, I mean, it depends how you define it.
[00:31:08.240 --> 00:31:11.480]   Do you say AGI, with general intelligence,
[00:31:11.480 --> 00:31:14.520]   artificial intelligence, only refers to
[00:31:14.520 --> 00:31:17.440]   if you achieve human level or a subhuman level,
[00:31:17.440 --> 00:31:19.960]   but quite broad, is it also general intelligence?
[00:31:19.960 --> 00:31:20.920]   So we have to distinguish,
[00:31:20.920 --> 00:31:23.320]   or it's only super human intelligence,
[00:31:23.320 --> 00:31:25.080]   general artificial intelligence.
[00:31:25.120 --> 00:31:27.840]   - Is there a test in your mind, like the Turing test,
[00:31:27.840 --> 00:31:29.960]   and natural language, or some other test
[00:31:29.960 --> 00:31:32.040]   that would impress the heck out of you,
[00:31:32.040 --> 00:31:34.840]   that would kind of cross the line of
[00:31:34.840 --> 00:31:38.080]   your sense of intelligence
[00:31:38.080 --> 00:31:39.880]   within the framework that you said?
[00:31:39.880 --> 00:31:43.000]   - Well, the Turing test, well, it has been criticized a lot,
[00:31:43.000 --> 00:31:45.960]   but I think it's not as bad as some people think.
[00:31:45.960 --> 00:31:47.720]   Some people think it's too strong.
[00:31:47.720 --> 00:31:52.160]   So it tests not just for a system to be intelligent,
[00:31:52.160 --> 00:31:55.160]   but it also has to fake human--
[00:31:55.160 --> 00:31:56.000]   - Deception.
[00:31:56.000 --> 00:31:59.000]   - Deception, right, which is much harder.
[00:31:59.000 --> 00:32:01.200]   And on the other hand, they say it's too weak,
[00:32:01.200 --> 00:32:06.200]   because it just maybe fakes emotions or intelligent behavior.
[00:32:06.200 --> 00:32:09.440]   It's not real.
[00:32:09.440 --> 00:32:12.000]   But I don't think that's the problem, or a big problem.
[00:32:12.000 --> 00:32:14.520]   So if you would pass the Turing test,
[00:32:14.520 --> 00:32:20.640]   so a conversation over terminal with a bot for an hour,
[00:32:20.640 --> 00:32:23.360]   or maybe a day or so, and you can fool a human
[00:32:23.360 --> 00:32:26.160]   into not knowing whether this is a human or not,
[00:32:26.160 --> 00:32:30.280]   so that's the Turing test, I would be truly impressed.
[00:32:30.280 --> 00:32:34.400]   And we have this annual competitions, the Leupner Prize.
[00:32:34.400 --> 00:32:36.000]   And I mean, it started with Eliza,
[00:32:36.000 --> 00:32:38.240]   that was the first conversational program.
[00:32:38.240 --> 00:32:41.800]   And what is it called, the Japanese Mitsuko or so,
[00:32:41.800 --> 00:32:44.720]   that's the winner of the last couple of years.
[00:32:44.720 --> 00:32:45.880]   And-- - It's quite impressive.
[00:32:45.880 --> 00:32:46.920]   - Yeah, it's quite impressive.
[00:32:46.920 --> 00:32:50.280]   And then Google has developed Mina, right?
[00:32:50.280 --> 00:32:55.240]   Just recently, that's an open domain conversational bot,
[00:32:55.240 --> 00:32:57.600]   just a couple of weeks ago, I think.
[00:32:57.600 --> 00:32:58.800]   - Yeah, I kind of like the metric
[00:32:58.800 --> 00:33:01.720]   that sort of the Alexa Prize has proposed.
[00:33:01.720 --> 00:33:04.040]   I mean, maybe it's obvious to you, it wasn't to me,
[00:33:04.040 --> 00:33:07.760]   of setting sort of a length of a conversation.
[00:33:07.760 --> 00:33:10.960]   Like you want the bot to be sufficiently interesting
[00:33:10.960 --> 00:33:13.680]   that you'd want to keep talking to it for like 20 minutes.
[00:33:13.680 --> 00:33:18.680]   And that's a surprisingly effective in aggregate metric,
[00:33:19.560 --> 00:33:24.560]   'cause you really, like nobody has the patience
[00:33:24.560 --> 00:33:27.760]   to be able to talk to a bot that's not interesting
[00:33:27.760 --> 00:33:29.000]   and intelligent and witty,
[00:33:29.000 --> 00:33:33.000]   and is able to go on to different tangents, jump domains,
[00:33:33.000 --> 00:33:35.380]   be able to say something interesting
[00:33:35.380 --> 00:33:36.720]   to maintain your attention.
[00:33:36.720 --> 00:33:39.040]   - And maybe many humans will also fail this test.
[00:33:39.040 --> 00:33:42.840]   - That's the, unfortunately, we set,
[00:33:42.840 --> 00:33:45.400]   just like with autonomous vehicles, with chatbots,
[00:33:45.400 --> 00:33:48.200]   we also set a bar that's way too high to reach.
[00:33:48.200 --> 00:33:50.040]   I said, the Turing test is not as bad
[00:33:50.040 --> 00:33:51.200]   as some people believe,
[00:33:51.200 --> 00:33:55.960]   but what is really not useful about the Turing test,
[00:33:55.960 --> 00:33:59.800]   it gives us no guidance how to develop these systems
[00:33:59.800 --> 00:34:00.640]   in the first place.
[00:34:00.640 --> 00:34:03.000]   Of course, we can develop them by trial and error
[00:34:03.000 --> 00:34:05.440]   and do whatever and then run the test
[00:34:05.440 --> 00:34:06.880]   and see whether it works or not.
[00:34:06.880 --> 00:34:11.880]   But a mathematical definition of intelligence
[00:34:11.880 --> 00:34:17.320]   gives us an objective, which we can then analyze
[00:34:17.560 --> 00:34:21.640]   by theoretical tools or computational,
[00:34:21.640 --> 00:34:25.120]   and maybe even prove how close we are.
[00:34:25.120 --> 00:34:28.720]   And we will come back to that later with the IXE model.
[00:34:28.720 --> 00:34:31.240]   So, I mentioned the compression, right?
[00:34:31.240 --> 00:34:33.280]   So in natural language processing,
[00:34:33.280 --> 00:34:36.720]   they achieved amazing results.
[00:34:36.720 --> 00:34:38.960]   And one way to test this, of course,
[00:34:38.960 --> 00:34:40.240]   take the system, you train it,
[00:34:40.240 --> 00:34:43.160]   and then you see how well it performs on the task.
[00:34:43.200 --> 00:34:47.520]   But a lot of performance measurement
[00:34:47.520 --> 00:34:49.000]   is done by so-called perplexity,
[00:34:49.000 --> 00:34:51.920]   which is essentially the same as complexity
[00:34:51.920 --> 00:34:53.240]   or compression length.
[00:34:53.240 --> 00:34:55.920]   So the NLP community develops new systems
[00:34:55.920 --> 00:34:57.520]   and then they measure the compression length,
[00:34:57.520 --> 00:35:01.240]   and then they have ranking and leaks
[00:35:01.240 --> 00:35:02.800]   because there's a strong correlation
[00:35:02.800 --> 00:35:04.640]   between compressing well,
[00:35:04.640 --> 00:35:07.560]   and then the system's performing well at the task at hand.
[00:35:07.560 --> 00:35:10.720]   It's not perfect, but it's good enough for them
[00:35:10.720 --> 00:35:13.640]   as an intermediate aim.
[00:35:13.640 --> 00:35:16.000]   - So you mean a measure,
[00:35:16.000 --> 00:35:18.400]   so this is kind of almost returning
[00:35:18.400 --> 00:35:19.760]   to the comical of complexity.
[00:35:19.760 --> 00:35:22.480]   So you're saying good compression
[00:35:22.480 --> 00:35:24.960]   usually means good intelligence.
[00:35:24.960 --> 00:35:25.800]   - Yes.
[00:35:25.800 --> 00:35:31.080]   - So you mentioned you're one of the only people
[00:35:31.080 --> 00:35:36.080]   who dared boldly to try to formalize
[00:35:36.080 --> 00:35:38.680]   the idea of artificial general intelligence,
[00:35:38.680 --> 00:35:42.840]   to have a mathematical framework for intelligence,
[00:35:42.840 --> 00:35:44.960]   just like as we mentioned,
[00:35:44.960 --> 00:35:49.200]   termed AIXI, A-I-X-I.
[00:35:49.200 --> 00:35:51.760]   So let me ask the basic question.
[00:35:51.760 --> 00:35:53.400]   What is AIXI?
[00:35:53.400 --> 00:35:57.960]   - Okay, so let me first say what it stands for because--
[00:35:57.960 --> 00:35:58.880]   - What it stands for, actually,
[00:35:58.880 --> 00:36:00.400]   that's probably the more basic question.
[00:36:00.400 --> 00:36:01.640]   - Yeah. (laughs)
[00:36:01.640 --> 00:36:04.400]   The first question is usually how it's pronounced,
[00:36:04.400 --> 00:36:07.200]   but finally I put it on the website how it's pronounced,
[00:36:07.200 --> 00:36:08.320]   and you figured it out.
[00:36:09.080 --> 00:36:10.560]   - Yeah.
[00:36:10.560 --> 00:36:13.320]   - The name comes from AI, artificial intelligence,
[00:36:13.320 --> 00:36:16.440]   and the X, I, is the Greek letter Xi,
[00:36:16.440 --> 00:36:19.720]   which are used for Solomonov's distribution
[00:36:19.720 --> 00:36:22.040]   for quite stupid reasons,
[00:36:22.040 --> 00:36:24.720]   which I'm not willing to repeat here in front of camera.
[00:36:24.720 --> 00:36:27.080]   (both laugh)
[00:36:27.080 --> 00:36:29.920]   So it just happened to be more or less arbitrary,
[00:36:29.920 --> 00:36:31.640]   I chose the Xi,
[00:36:31.640 --> 00:36:34.720]   but it also has nice other interpretations.
[00:36:34.720 --> 00:36:38.200]   So there are actions and perceptions in this model,
[00:36:38.200 --> 00:36:40.520]   right, an agent has actions and perceptions,
[00:36:40.520 --> 00:36:44.680]   and over time, so this is A-index-I, X-index-I,
[00:36:44.680 --> 00:36:46.160]   so there's action at time I,
[00:36:46.160 --> 00:36:49.040]   and then followed by perception at time I.
[00:36:49.040 --> 00:36:50.440]   - Yeah, we'll go with that.
[00:36:50.440 --> 00:36:51.840]   I'll edit out the first part.
[00:36:51.840 --> 00:36:52.680]   (laughs)
[00:36:52.680 --> 00:36:53.520]   I'm just kidding.
[00:36:53.520 --> 00:36:54.880]   - I have some more interpretations.
[00:36:54.880 --> 00:36:55.720]   - Yeah, good.
[00:36:55.720 --> 00:36:59.320]   - So at some point, maybe five years ago or 10 years ago,
[00:36:59.320 --> 00:37:03.000]   I discovered in Barcelona,
[00:37:03.000 --> 00:37:04.760]   it was on a big church,
[00:37:04.760 --> 00:37:08.520]   there was in stone engraved some text,
[00:37:08.520 --> 00:37:11.520]   and the word Aix appeared there a couple of times.
[00:37:11.520 --> 00:37:12.920]   (both laugh)
[00:37:12.920 --> 00:37:17.000]   I was very surprised and happy about that,
[00:37:17.000 --> 00:37:19.440]   and I looked it up, so it is Catalan language,
[00:37:19.440 --> 00:37:21.720]   and it means, with some interpretation,
[00:37:21.720 --> 00:37:24.800]   so that's it, that's the right thing to do, yeah, Eureka.
[00:37:24.800 --> 00:37:27.440]   - Oh, so it's almost like destined,
[00:37:27.440 --> 00:37:32.080]   somehow came to you in a dream.
[00:37:32.080 --> 00:37:34.280]   - And similarly, there's a Chinese word, Aixi,
[00:37:34.280 --> 00:37:35.400]   also written like Aixi,
[00:37:35.400 --> 00:37:37.480]   if you transcribe that to Pinyin,
[00:37:37.480 --> 00:37:41.160]   and the final one is that it's AI crossed with induction,
[00:37:41.160 --> 00:37:44.680]   because that is, and it's going more to the content now,
[00:37:44.680 --> 00:37:47.400]   so good old-fashioned AI is more about planning
[00:37:47.400 --> 00:37:48.760]   in known deterministic world,
[00:37:48.760 --> 00:37:50.640]   and induction is more about often,
[00:37:50.640 --> 00:37:53.200]   you know, IID data and inferring models,
[00:37:53.200 --> 00:37:54.880]   and essentially what this Aixi model does
[00:37:54.880 --> 00:37:56.160]   is combining these two.
[00:37:56.160 --> 00:37:59.040]   - And I actually also recently, I think,
[00:37:59.040 --> 00:38:02.320]   heard that in Japanese, AI means love,
[00:38:02.320 --> 00:38:06.760]   so if you can combine XI somehow with that,
[00:38:06.760 --> 00:38:10.360]   I think we can, there might be some interesting ideas there,
[00:38:10.360 --> 00:38:12.720]   so Aixi, let's then take the next step,
[00:38:12.720 --> 00:38:16.600]   can you maybe talk at the big level
[00:38:16.600 --> 00:38:19.520]   of what is this mathematical framework?
[00:38:19.520 --> 00:38:22.600]   - Yeah, so it consists essentially of two parts,
[00:38:22.600 --> 00:38:26.600]   one is the learning and induction and prediction part,
[00:38:26.600 --> 00:38:28.760]   and the other one is the planning part,
[00:38:28.760 --> 00:38:31.240]   so let's come first to the learning,
[00:38:31.240 --> 00:38:32.880]   induction, prediction part,
[00:38:32.880 --> 00:38:35.720]   which essentially I explained already before,
[00:38:35.720 --> 00:38:40.720]   so what we need for any agent to act well
[00:38:40.720 --> 00:38:43.520]   is that it can somehow predict what happens,
[00:38:43.520 --> 00:38:46.080]   I mean, if you have no idea what your actions do,
[00:38:46.080 --> 00:38:48.960]   how can you decide which actions are good or not,
[00:38:48.960 --> 00:38:52.880]   so you need to have some model of what your actions effect,
[00:38:52.880 --> 00:38:56.200]   so what you do is you have some experience,
[00:38:56.200 --> 00:38:59.440]   you build models like scientists of your experience,
[00:38:59.440 --> 00:39:01.440]   then you hope these models are roughly correct,
[00:39:01.440 --> 00:39:03.520]   and then you use these models for prediction.
[00:39:03.520 --> 00:39:05.240]   - And the model is, sorry to interrupt,
[00:39:05.240 --> 00:39:08.400]   and the model is based on your perception of the world,
[00:39:08.400 --> 00:39:10.520]   how your actions will affect that world?
[00:39:10.520 --> 00:39:12.120]   - That's not--
[00:39:12.120 --> 00:39:12.960]   - So how do you think about it?
[00:39:12.960 --> 00:39:14.600]   - That's not the important part,
[00:39:14.600 --> 00:39:16.040]   it is technically important,
[00:39:16.040 --> 00:39:18.280]   but at this stage we can just think about predicting,
[00:39:18.280 --> 00:39:20.080]   say, stock market data,
[00:39:20.080 --> 00:39:22.240]   whether data or IQ sequences,
[00:39:22.240 --> 00:39:24.520]   one, two, three, four, five, what comes next, yeah?
[00:39:24.520 --> 00:39:28.680]   So of course our actions affect what we're doing,
[00:39:28.680 --> 00:39:30.240]   but I'll come back to that in a second.
[00:39:30.240 --> 00:39:32.160]   - So, and I'll keep just interrupting,
[00:39:32.160 --> 00:39:37.160]   so just to draw a line between prediction and planning,
[00:39:37.160 --> 00:39:40.880]   what do you mean by prediction in this way?
[00:39:40.880 --> 00:39:43.640]   It's trying to predict the environment
[00:39:43.640 --> 00:39:47.280]   without your long-term action in that environment,
[00:39:47.280 --> 00:39:48.240]   what is prediction?
[00:39:48.240 --> 00:39:51.200]   - Okay, if you want to put the actions in now,
[00:39:51.200 --> 00:39:53.680]   okay, then let's put in now, yeah?
[00:39:53.680 --> 00:39:54.720]   So--
[00:39:54.720 --> 00:39:56.840]   - We don't have to put them now, scratch it,
[00:39:56.840 --> 00:39:58.360]   scratch it, dumb question, okay.
[00:39:58.360 --> 00:40:01.320]   - So the simplest form of prediction is
[00:40:01.320 --> 00:40:04.840]   that you just have data which you passively observe,
[00:40:04.840 --> 00:40:06.200]   and you want to predict what happens
[00:40:06.200 --> 00:40:08.960]   without interfering, as I said,
[00:40:08.960 --> 00:40:12.120]   weather forecasting, stock market, IQ sequences,
[00:40:12.120 --> 00:40:16.240]   or just anything, okay?
[00:40:16.240 --> 00:40:18.920]   And Solomonov's theory of induction based on compression,
[00:40:18.920 --> 00:40:20.400]   so you look for the shortest program
[00:40:20.400 --> 00:40:22.280]   which describes your data sequence,
[00:40:22.280 --> 00:40:24.440]   and then you take this program, run it,
[00:40:24.440 --> 00:40:27.000]   it reproduces your data sequence by definition,
[00:40:27.000 --> 00:40:29.120]   and then you let it continue running,
[00:40:29.120 --> 00:40:30.880]   and then it will produce some predictions,
[00:40:30.880 --> 00:40:35.880]   and you can rigorously prove that for any prediction task,
[00:40:35.880 --> 00:40:40.040]   this is essentially the best possible predictor.
[00:40:40.040 --> 00:40:42.040]   Of course, if there's a prediction task,
[00:40:42.040 --> 00:40:45.080]   or a task which is unpredictable,
[00:40:45.080 --> 00:40:46.720]   like, you know, fair coin flips,
[00:40:46.720 --> 00:40:48.160]   yeah, I cannot predict the next fair coin flip,
[00:40:48.160 --> 00:40:49.160]   what Solomonov does, he says,
[00:40:49.160 --> 00:40:51.640]   okay, next head is probably 50%,
[00:40:51.640 --> 00:40:52.560]   it's the best you can do.
[00:40:52.560 --> 00:40:54.040]   So if something is unpredictable,
[00:40:54.040 --> 00:40:56.600]   Solomonov will also not magically predict it.
[00:40:56.600 --> 00:40:59.640]   But if there is some pattern and predictability,
[00:40:59.640 --> 00:41:03.720]   then Solomonov induction will figure that out eventually,
[00:41:03.720 --> 00:41:06.040]   and not just eventually, but rather quickly,
[00:41:06.040 --> 00:41:08.360]   and you can have proof convergence rates,
[00:41:08.360 --> 00:41:11.720]   whatever your data is.
[00:41:11.720 --> 00:41:14.760]   So that is pure magic in a sense.
[00:41:14.760 --> 00:41:15.600]   What's the catch?
[00:41:15.600 --> 00:41:17.080]   Well, the catch is that it's not computable,
[00:41:17.080 --> 00:41:18.160]   and we come back to that later,
[00:41:18.160 --> 00:41:19.440]   you cannot just implement it,
[00:41:19.440 --> 00:41:21.120]   and even with Google resources here,
[00:41:21.120 --> 00:41:23.280]   and run it, and predict the stock market,
[00:41:23.280 --> 00:41:24.120]   and become rich.
[00:41:24.120 --> 00:41:28.120]   I mean, Ray Solomonov already tried it at the time.
[00:41:28.120 --> 00:41:31.640]   - But so the basic task is you're in the environment,
[00:41:31.640 --> 00:41:33.200]   and you're interacting with the environment
[00:41:33.200 --> 00:41:35.400]   to try to learn a model of that environment,
[00:41:35.400 --> 00:41:38.720]   and the model is in the space of all these programs,
[00:41:38.720 --> 00:41:41.320]   and your goal is to get a bunch of programs that are simple.
[00:41:41.320 --> 00:41:44.000]   - And so let's go to the actions now.
[00:41:44.000 --> 00:41:45.040]   But actually, good that you asked.
[00:41:45.040 --> 00:41:46.360]   Usually I skip this part,
[00:41:46.360 --> 00:41:48.240]   although there is also a minor contribution,
[00:41:48.240 --> 00:41:49.680]   which I did, so the action part,
[00:41:49.680 --> 00:41:51.760]   but I usually sort of just jump to the decision part.
[00:41:51.760 --> 00:41:53.360]   So let me explain to the action part now.
[00:41:53.360 --> 00:41:54.280]   Thanks for asking.
[00:41:54.280 --> 00:41:57.720]   So you have to modify it a little bit
[00:41:57.720 --> 00:42:01.040]   by now not just predicting a sequence
[00:42:01.040 --> 00:42:03.200]   which just comes to you,
[00:42:03.200 --> 00:42:06.720]   but you have an observation, then you act somehow,
[00:42:06.720 --> 00:42:09.080]   and then you want to predict the next observation
[00:42:09.080 --> 00:42:11.880]   based on the past observation and your action.
[00:42:11.880 --> 00:42:14.640]   Then you take the next action,
[00:42:14.640 --> 00:42:17.200]   you don't care about predicting it because you're doing it,
[00:42:17.200 --> 00:42:19.000]   and then you get the next observation,
[00:42:19.000 --> 00:42:20.680]   and you want, well, before you get it,
[00:42:20.680 --> 00:42:21.840]   you want to predict it, again,
[00:42:21.840 --> 00:42:24.840]   based on your past action and observation sequence.
[00:42:24.840 --> 00:42:28.720]   You just condition extra on your actions.
[00:42:28.720 --> 00:42:30.480]   There's an interesting alternative
[00:42:30.480 --> 00:42:33.360]   that you also try to predict your own actions.
[00:42:33.360 --> 00:42:36.400]   If you want--
[00:42:36.400 --> 00:42:38.000]   - In the past or the future?
[00:42:38.000 --> 00:42:39.720]   - Your future actions.
[00:42:39.720 --> 00:42:40.560]   - That's interesting.
[00:42:40.560 --> 00:42:41.840]   (both laughing)
[00:42:41.840 --> 00:42:43.480]   Wait, let me wrap.
[00:42:43.480 --> 00:42:45.000]   I think my brain is broke.
[00:42:45.000 --> 00:42:47.400]   - We should maybe discuss that later
[00:42:47.400 --> 00:42:48.760]   after I've explained the ICES model.
[00:42:48.760 --> 00:42:50.160]   That's an interesting variation.
[00:42:50.160 --> 00:42:52.080]   - But that is a really interesting variation.
[00:42:52.080 --> 00:42:53.080]   And a quick comment,
[00:42:53.080 --> 00:42:55.440]   I don't know if you want to insert that in here,
[00:42:55.440 --> 00:42:59.200]   but you're looking at that, in terms of observations,
[00:42:59.200 --> 00:43:01.600]   you're looking at the entire, the big history,
[00:43:01.600 --> 00:43:03.320]   the long history of the observations.
[00:43:03.320 --> 00:43:04.440]   - Exactly, that's very important,
[00:43:04.440 --> 00:43:07.680]   the whole history from birth of the agent.
[00:43:07.680 --> 00:43:10.840]   And we can come back to that also why this is important.
[00:43:10.840 --> 00:43:13.560]   Often, in RL, you have MDPs,
[00:43:13.560 --> 00:43:15.840]   micro-decision processes, which are much more limiting.
[00:43:15.840 --> 00:43:19.880]   Okay, so now we can predict conditioned on actions.
[00:43:19.880 --> 00:43:21.560]   So even if we influence environment,
[00:43:21.560 --> 00:43:24.080]   but prediction is not all we want to do, right?
[00:43:24.080 --> 00:43:26.920]   We also want to act really in the world.
[00:43:26.920 --> 00:43:29.080]   And the question is how to choose the actions.
[00:43:29.080 --> 00:43:32.320]   And we don't want to greedily choose the actions,
[00:43:32.320 --> 00:43:36.480]   just what is best in the next time step.
[00:43:36.480 --> 00:43:38.320]   And we first, I should say,
[00:43:38.320 --> 00:43:39.920]   how do we measure performance?
[00:43:39.920 --> 00:43:43.320]   So we measure performance by giving the agent reward.
[00:43:43.320 --> 00:43:45.600]   That's the so-called reinforcement learning framework.
[00:43:45.600 --> 00:43:48.520]   So every time step, you can give it a positive reward
[00:43:48.520 --> 00:43:50.320]   or negative reward, or maybe no reward.
[00:43:50.320 --> 00:43:51.880]   It could be a very scarce, right?
[00:43:51.880 --> 00:43:54.120]   Like if you play chess, just at the end of the game,
[00:43:54.120 --> 00:43:56.880]   you give plus one for winning or minus one for losing.
[00:43:56.880 --> 00:43:59.200]   So in the IXE framework, that's completely sufficient.
[00:43:59.200 --> 00:44:01.400]   So occasionally you give a reward signal
[00:44:01.400 --> 00:44:04.000]   and you ask the agent to maximize reward,
[00:44:04.000 --> 00:44:06.360]   but not greedily sort of, you know, the next one, next one,
[00:44:06.360 --> 00:44:09.160]   because that's very bad in the long run if you're greedy.
[00:44:09.160 --> 00:44:12.400]   But over the lifetime of the agent,
[00:44:12.400 --> 00:44:14.560]   so let's assume the agent lives for M time steps,
[00:44:14.560 --> 00:44:16.960]   let's just say it dies in sort of 100 years, sharp.
[00:44:16.960 --> 00:44:19.720]   That's just, you know, the simplest model to explain.
[00:44:19.720 --> 00:44:22.320]   So it looks at the future reward sum
[00:44:22.320 --> 00:44:24.880]   and ask what is my action sequence,
[00:44:24.880 --> 00:44:26.960]   or actually more precisely my policy,
[00:44:26.960 --> 00:44:29.880]   which leads in expectation,
[00:44:29.880 --> 00:44:32.160]   because I don't know the world,
[00:44:32.160 --> 00:44:34.160]   to the maximum reward sum.
[00:44:34.160 --> 00:44:36.120]   Let me give you an analogy.
[00:44:36.120 --> 00:44:38.240]   In chess, for instance,
[00:44:38.240 --> 00:44:40.320]   we know how to play optimally in theory.
[00:44:40.320 --> 00:44:42.160]   It's just a minimax strategy.
[00:44:42.160 --> 00:44:44.400]   I play the move which seems best to me
[00:44:44.400 --> 00:44:46.800]   under the assumption that the opponent plays the move
[00:44:46.800 --> 00:44:50.640]   which is best for him, so worst for me,
[00:44:50.640 --> 00:44:54.160]   under the assumption that I play, again, the best move,
[00:44:54.160 --> 00:44:55.960]   and then you have this expected max tree
[00:44:55.960 --> 00:44:57.560]   to the end of the game,
[00:44:57.560 --> 00:44:58.840]   and then you backpropagate
[00:44:58.840 --> 00:45:00.760]   and then you get the best possible move.
[00:45:00.760 --> 00:45:02.120]   So that is the optimal strategy,
[00:45:02.120 --> 00:45:04.840]   which von Neumann already figured out a long time ago,
[00:45:04.840 --> 00:45:08.960]   for playing adversarial games.
[00:45:08.960 --> 00:45:11.600]   Luckily, or maybe unluckily for the theory,
[00:45:11.600 --> 00:45:12.440]   it becomes harder.
[00:45:12.440 --> 00:45:14.960]   The world is not always adversarial,
[00:45:14.960 --> 00:45:18.400]   so it can be, if there are other humans, even cooperative,
[00:45:18.400 --> 00:45:22.680]   or nature is usually, I mean, the dead nature is stochastic.
[00:45:22.680 --> 00:45:26.800]   Things just happen randomly, or don't care about you.
[00:45:26.800 --> 00:45:29.400]   So what you have to take into account is the noise
[00:45:29.400 --> 00:45:30.720]   and not necessarily adversariality.
[00:45:30.720 --> 00:45:34.000]   So you replace the minimum on the opponent's side
[00:45:34.000 --> 00:45:36.000]   by an expectation,
[00:45:36.000 --> 00:45:40.040]   which is general enough to include also adversarial cases.
[00:45:40.040 --> 00:45:41.560]   So now instead of a minimax strategy,
[00:45:41.560 --> 00:45:43.800]   you have an expected max strategy.
[00:45:43.800 --> 00:45:45.400]   So far so good, so that is well known,
[00:45:45.400 --> 00:45:48.000]   it's called sequential decision theory.
[00:45:48.000 --> 00:45:49.440]   But the question is,
[00:45:49.440 --> 00:45:52.440]   on which probability distribution do you base that?
[00:45:52.440 --> 00:45:55.360]   If I have the true probability distribution,
[00:45:55.360 --> 00:45:56.920]   like say I play Begummin, right?
[00:45:56.920 --> 00:45:59.320]   There's dice and there's certain randomness involved.
[00:45:59.320 --> 00:46:00.920]   Yeah, I can calculate probabilities
[00:46:00.920 --> 00:46:02.600]   and feed it in the expected max
[00:46:02.600 --> 00:46:04.120]   or the sequential decision tree,
[00:46:04.120 --> 00:46:07.160]   come up with the optimal decision if I have enough compute.
[00:46:07.160 --> 00:46:09.720]   But for the real world, we don't know that.
[00:46:09.720 --> 00:46:13.920]   What is the probability the driver in front of me breaks?
[00:46:13.920 --> 00:46:14.880]   I don't know.
[00:46:14.880 --> 00:46:16.880]   So depends on all kinds of things
[00:46:16.880 --> 00:46:19.600]   and especially new situations, I don't know.
[00:46:19.600 --> 00:46:22.480]   So this is this unknown thing about prediction
[00:46:22.480 --> 00:46:24.200]   and there's where Solomonov comes in.
[00:46:24.200 --> 00:46:26.320]   So what you do is in sequential decision tree,
[00:46:26.320 --> 00:46:28.640]   you just replace the true distribution,
[00:46:28.640 --> 00:46:32.920]   which we don't know, by this universal distribution.
[00:46:32.920 --> 00:46:34.600]   I didn't explicitly talk about it,
[00:46:34.600 --> 00:46:36.760]   but this is used for universal prediction
[00:46:36.760 --> 00:46:40.400]   and plug it into the sequential decision tree mechanism.
[00:46:40.400 --> 00:46:42.600]   And then you get the best of both worlds.
[00:46:42.600 --> 00:46:44.600]   You have a long-term planning agent,
[00:46:44.600 --> 00:46:48.040]   but it doesn't need to know anything about the world
[00:46:48.040 --> 00:46:51.600]   because the Solomonov induction part learns.
[00:46:51.600 --> 00:46:54.680]   - Can you explicitly try to describe
[00:46:54.680 --> 00:46:56.040]   the universal distribution
[00:46:56.040 --> 00:46:59.680]   and how Solomonov induction plays a role here?
[00:46:59.680 --> 00:47:00.760]   I'm trying to understand.
[00:47:00.760 --> 00:47:03.840]   - So what it does it, so in the simplest case,
[00:47:03.840 --> 00:47:05.600]   I said, take the shortest program,
[00:47:05.600 --> 00:47:07.160]   describing your data, run it,
[00:47:07.160 --> 00:47:09.040]   have a prediction which would be deterministic.
[00:47:09.040 --> 00:47:09.880]   - Yes.
[00:47:09.880 --> 00:47:13.160]   - Okay, but you should not just take the shortest program,
[00:47:13.160 --> 00:47:15.320]   but also consider the longer ones,
[00:47:15.320 --> 00:47:18.480]   but keep it lower a priori probability.
[00:47:18.480 --> 00:47:20.160]   So in the Bayesian framework,
[00:47:20.160 --> 00:47:23.640]   you say a priori any distribution,
[00:47:23.640 --> 00:47:29.360]   which is a model or a stochastic program
[00:47:29.360 --> 00:47:30.760]   has a certain a priori probability,
[00:47:30.760 --> 00:47:32.160]   which is two to the minus,
[00:47:32.160 --> 00:47:33.320]   and why two to the minus length,
[00:47:33.320 --> 00:47:35.520]   you know, I could explain length of this program.
[00:47:35.520 --> 00:47:39.760]   So longer programs are punished, a priori.
[00:47:39.760 --> 00:47:41.320]   And then you multiply it
[00:47:41.320 --> 00:47:43.840]   with the so-called likelihood function,
[00:47:43.840 --> 00:47:46.680]   which is, as the name suggests,
[00:47:46.680 --> 00:47:51.000]   is how likely is this model given the data at hand.
[00:47:51.000 --> 00:47:53.240]   So if you have a very wrong model,
[00:47:53.240 --> 00:47:55.000]   it's very unlikely that this model is true,
[00:47:55.000 --> 00:47:56.760]   and so it is very small number.
[00:47:56.760 --> 00:48:00.240]   So even if the model is simple, it gets penalized by that.
[00:48:00.240 --> 00:48:02.480]   And what you do is then you take just the sum,
[00:48:02.480 --> 00:48:04.440]   or this is the average over it.
[00:48:04.440 --> 00:48:07.600]   And this gives you a probability distribution,
[00:48:07.600 --> 00:48:09.200]   so-called universal distribution,
[00:48:09.200 --> 00:48:10.520]   or Solomonov distribution.
[00:48:10.520 --> 00:48:13.200]   - So it's weighed by the simplicity of the program
[00:48:13.200 --> 00:48:14.120]   and the likelihood.
[00:48:14.120 --> 00:48:15.320]   - Yes.
[00:48:15.320 --> 00:48:17.280]   - It's kind of a nice idea.
[00:48:17.280 --> 00:48:18.120]   - Yeah.
[00:48:18.120 --> 00:48:19.640]   - So, okay.
[00:48:19.640 --> 00:48:23.240]   And then you said there's, you're planning N or M,
[00:48:23.240 --> 00:48:25.920]   or forgot the letter, steps into the future.
[00:48:25.920 --> 00:48:28.280]   So how difficult is that problem?
[00:48:28.280 --> 00:48:29.440]   What's involved there?
[00:48:29.440 --> 00:48:30.280]   - Okay, so there's- - Is it a basic
[00:48:30.280 --> 00:48:31.240]   optimization problem?
[00:48:31.240 --> 00:48:32.080]   What are we talking about?
[00:48:32.080 --> 00:48:34.920]   - So you have a planning problem up to horizon M,
[00:48:34.920 --> 00:48:38.080]   and that's exponential time in the horizon M,
[00:48:38.080 --> 00:48:41.760]   which is, I mean, it's computable, but intractable.
[00:48:41.760 --> 00:48:43.560]   I mean, even for chess, it's already intractable
[00:48:43.560 --> 00:48:45.440]   to do that exactly, and for Go.
[00:48:45.440 --> 00:48:48.720]   - But it could be also discounted kind of framework, or?
[00:48:48.720 --> 00:48:53.000]   - Yeah, so having a hard horizon, you know, at 100 years,
[00:48:53.000 --> 00:48:55.800]   it's just for simplicity of discussing the model,
[00:48:55.800 --> 00:48:57.680]   and also sometimes the math is simple.
[00:48:57.680 --> 00:49:00.040]   But there are lots of variations.
[00:49:00.040 --> 00:49:02.560]   Actually, a quite interesting parameter.
[00:49:02.560 --> 00:49:07.280]   There's nothing really problematic about it,
[00:49:07.280 --> 00:49:08.280]   but it's very interesting.
[00:49:08.280 --> 00:49:09.320]   So for instance, you think, no,
[00:49:09.320 --> 00:49:12.920]   let's let the parameter M tend to infinity, right?
[00:49:12.920 --> 00:49:15.880]   You want an agent which lives forever, right?
[00:49:15.880 --> 00:49:17.520]   If you do it naively, you have two problems.
[00:49:17.520 --> 00:49:19.200]   First, the mathematics breaks down
[00:49:19.200 --> 00:49:21.400]   because you have an infinite reward sum,
[00:49:21.400 --> 00:49:22.760]   which may give infinity,
[00:49:22.760 --> 00:49:25.600]   and getting reward 0.1 every time step is infinity,
[00:49:25.600 --> 00:49:27.640]   and giving reward one every time step is infinity,
[00:49:27.640 --> 00:49:28.640]   so equally good.
[00:49:29.520 --> 00:49:31.120]   That's not really what we want.
[00:49:31.120 --> 00:49:35.800]   Other problem is that if you have an infinite life,
[00:49:35.800 --> 00:49:38.600]   you can be lazy for as long as you want for 10 years,
[00:49:38.600 --> 00:49:41.440]   and then catch up with the same expected reward.
[00:49:41.440 --> 00:49:44.040]   And, you know, think about yourself,
[00:49:44.040 --> 00:49:47.280]   or maybe some friends or so.
[00:49:47.280 --> 00:49:51.480]   If they knew they lived forever, why work hard now?
[00:49:51.480 --> 00:49:54.320]   Just enjoy your life, and then catch up later.
[00:49:54.320 --> 00:49:56.640]   So that's another problem with infinite horizon.
[00:49:56.640 --> 00:49:59.080]   And you mentioned, yes, we can go to discounting.
[00:49:59.800 --> 00:50:01.200]   But then the standard discounting
[00:50:01.200 --> 00:50:03.120]   is so-called geometric discounting.
[00:50:03.120 --> 00:50:05.440]   So a dollar today is about worth
[00:50:05.440 --> 00:50:08.360]   as much as, you know, $1.05 tomorrow.
[00:50:08.360 --> 00:50:10.360]   So if you do this so-called geometric discounting,
[00:50:10.360 --> 00:50:13.000]   you have introduced an effective horizon.
[00:50:13.000 --> 00:50:15.400]   So the agent is now motivated
[00:50:15.400 --> 00:50:18.400]   to look ahead a certain amount of time effectively.
[00:50:18.400 --> 00:50:20.640]   It's like a moving horizon.
[00:50:20.640 --> 00:50:23.880]   And for any fixed effective horizon,
[00:50:23.880 --> 00:50:28.120]   there is a problem to solve which requires larger horizon.
[00:50:28.120 --> 00:50:30.520]   So if I look ahead, you know, five time steps,
[00:50:30.520 --> 00:50:32.480]   I'm a terrible chess player, right?
[00:50:32.480 --> 00:50:34.600]   I need to look ahead longer.
[00:50:34.600 --> 00:50:36.800]   If I play Go, I probably have to look ahead even longer.
[00:50:36.800 --> 00:50:40.280]   So for every problem, for every horizon,
[00:50:40.280 --> 00:50:43.800]   there is a problem which this horizon cannot solve.
[00:50:43.800 --> 00:50:46.960]   But I introduced the so-called near harmonic horizon,
[00:50:46.960 --> 00:50:48.360]   which goes down with one over T,
[00:50:48.360 --> 00:50:51.600]   rather than exponentially T, which produces an agent
[00:50:51.600 --> 00:50:53.880]   which effectively looks into the future
[00:50:53.880 --> 00:50:55.200]   proportional to its age.
[00:50:55.200 --> 00:50:57.360]   So if it's five years old, it plans for five years.
[00:50:57.360 --> 00:51:00.440]   If it's 100 years old, it then plans for 100 years.
[00:51:00.440 --> 00:51:02.480]   And it's a little bit similar to humans too, right?
[00:51:02.480 --> 00:51:04.320]   I mean, children don't plan ahead very long,
[00:51:04.320 --> 00:51:07.040]   but then we get adult, we play ahead more longer.
[00:51:07.040 --> 00:51:08.560]   Maybe when we get very old,
[00:51:08.560 --> 00:51:10.320]   I mean, we know that we don't live forever,
[00:51:10.320 --> 00:51:12.560]   maybe then our horizon shrinks again.
[00:51:12.560 --> 00:51:16.040]   - So that's really interesting.
[00:51:16.040 --> 00:51:18.120]   So adjusting the horizon,
[00:51:18.120 --> 00:51:20.680]   is there some mathematical benefit of that?
[00:51:20.680 --> 00:51:24.800]   Or is it just a nice, I mean, intuitively,
[00:51:24.800 --> 00:51:26.560]   empirically, it would probably be a good idea
[00:51:26.560 --> 00:51:27.960]   to sort of push the horizon back,
[00:51:27.960 --> 00:51:32.960]   extend the horizon as you experience more of the world.
[00:51:32.960 --> 00:51:35.880]   But is there some mathematical conclusions here
[00:51:35.880 --> 00:51:37.280]   that are beneficial?
[00:51:37.280 --> 00:51:38.960]   - With the solomon-hawking sort of prediction part,
[00:51:38.960 --> 00:51:41.440]   we have extremely strong finite time,
[00:51:41.440 --> 00:51:44.760]   but finite data results.
[00:51:44.760 --> 00:51:46.000]   So you have so and so much data,
[00:51:46.000 --> 00:51:47.160]   then you lose so and so much.
[00:51:47.160 --> 00:51:49.400]   So the theory is really great.
[00:51:49.400 --> 00:51:51.920]   With the Ixc model, with the planning part,
[00:51:51.920 --> 00:51:53.760]   many results are only asymptotic,
[00:51:54.600 --> 00:51:56.880]   which, well, this is--
[00:51:56.880 --> 00:51:57.720]   - What does asymptotic mean?
[00:51:57.720 --> 00:51:59.960]   - Asymptotic means you can prove, for instance,
[00:51:59.960 --> 00:52:03.440]   that in the long run, if the agent acts long enough,
[00:52:03.440 --> 00:52:06.840]   then it performs optimal or some nice thing happens.
[00:52:06.840 --> 00:52:09.520]   But you don't know how fast it converges.
[00:52:09.520 --> 00:52:10.920]   So it may converge fast,
[00:52:10.920 --> 00:52:12.320]   but we're just not able to prove it
[00:52:12.320 --> 00:52:13.800]   because of a difficult problem.
[00:52:13.800 --> 00:52:17.360]   Or maybe there's a bug in the model
[00:52:17.360 --> 00:52:19.560]   so that it's really that slow.
[00:52:19.560 --> 00:52:22.880]   So that is what asymptotic means, sort of eventually,
[00:52:22.880 --> 00:52:24.720]   but we don't know how fast.
[00:52:24.720 --> 00:52:28.000]   And if I give the agent a fixed horizon M,
[00:52:28.000 --> 00:52:32.280]   then I cannot prove asymptotic results, right?
[00:52:32.280 --> 00:52:35.040]   So I mean, sort of if it dies in 100 years,
[00:52:35.040 --> 00:52:37.880]   then 100 years is over, I cannot say eventually.
[00:52:37.880 --> 00:52:40.640]   So this is the advantage of the discounting
[00:52:40.640 --> 00:52:42.800]   that I can prove asymptotic results.
[00:52:42.800 --> 00:52:47.800]   - So just to clarify, so, okay, I've built up a model.
[00:52:49.160 --> 00:52:50.960]   Well, now in the moment,
[00:52:50.960 --> 00:52:55.400]   I have this way of looking several steps ahead.
[00:52:55.400 --> 00:52:57.840]   How do I pick what action I will take?
[00:52:57.840 --> 00:53:00.760]   - It's like with a playing chess, right?
[00:53:00.760 --> 00:53:02.360]   You do this minimax.
[00:53:02.360 --> 00:53:04.440]   In this case here, do expectimax
[00:53:04.440 --> 00:53:06.880]   based on the Solomonov distribution.
[00:53:06.880 --> 00:53:08.040]   You propagate back.
[00:53:08.040 --> 00:53:12.120]   And then, well, an action falls out.
[00:53:12.120 --> 00:53:15.520]   The action which maximizes the future expected reward
[00:53:15.520 --> 00:53:16.800]   on the Solomonov distribution,
[00:53:16.800 --> 00:53:18.280]   and then you just take this action.
[00:53:18.280 --> 00:53:19.640]   - And then repeat.
[00:53:19.640 --> 00:53:21.000]   - And then you get a new observation,
[00:53:21.000 --> 00:53:22.640]   and you feed it in this action observation,
[00:53:22.640 --> 00:53:23.480]   then you repeat.
[00:53:23.480 --> 00:53:24.920]   - And the reward, so on.
[00:53:24.920 --> 00:53:26.800]   - Yeah, so you're in a row too, yeah.
[00:53:26.800 --> 00:53:29.120]   - And then maybe you can even predict your own action.
[00:53:29.120 --> 00:53:30.000]   I love that idea.
[00:53:30.000 --> 00:53:34.120]   But okay, this big framework, what is it?
[00:53:34.120 --> 00:53:38.920]   I mean, it's kind of a beautiful mathematical framework
[00:53:38.920 --> 00:53:41.920]   to think about artificial general intelligence.
[00:53:41.920 --> 00:53:45.840]   What can you, what does it help you into it
[00:53:45.840 --> 00:53:49.120]   about how to build such systems?
[00:53:49.120 --> 00:53:51.760]   Or maybe from another perspective,
[00:53:51.760 --> 00:53:56.760]   what does it help us in understanding AGI?
[00:53:56.760 --> 00:54:00.520]   - So when I started in the field,
[00:54:00.520 --> 00:54:01.880]   I was always interested in two things.
[00:54:01.880 --> 00:54:05.880]   One was AGI, the name didn't exist then,
[00:54:05.880 --> 00:54:09.280]   but what called general AI or strong AI,
[00:54:09.280 --> 00:54:10.880]   and the physics here of everything.
[00:54:10.880 --> 00:54:13.200]   So I switched back and forth between computer science
[00:54:13.200 --> 00:54:14.760]   and physics quite often.
[00:54:14.760 --> 00:54:16.000]   - You said the theory of everything.
[00:54:16.000 --> 00:54:17.400]   - The theory of everything, yeah, just like--
[00:54:17.400 --> 00:54:19.280]   - Those are basically the two biggest problems
[00:54:19.280 --> 00:54:21.440]   before all of humanity.
[00:54:21.440 --> 00:54:28.400]   - Yeah, I can explain if you wanted some later time,
[00:54:28.400 --> 00:54:29.960]   why I'm interested in these two questions.
[00:54:29.960 --> 00:54:32.120]   - Can I ask you, on a small tangent,
[00:54:32.120 --> 00:54:37.960]   if it was one to be solved, which one would you,
[00:54:37.960 --> 00:54:41.800]   if an apple fell on your head,
[00:54:41.800 --> 00:54:43.280]   and there was a brilliant insight,
[00:54:43.280 --> 00:54:46.360]   and you could arrive at the solution to one,
[00:54:46.360 --> 00:54:49.160]   would it be AGI or the theory of everything?
[00:54:49.160 --> 00:54:51.760]   - Definitely AGI, because once the AGI problem is solved,
[00:54:51.760 --> 00:54:54.400]   I can ask the AGI to solve the other problem for me.
[00:54:54.400 --> 00:54:57.720]   - Yeah, brilliantly put.
[00:54:57.720 --> 00:55:01.160]   Okay, so as you were saying about it--
[00:55:01.160 --> 00:55:04.920]   - Okay, so, and the reason why I didn't settle,
[00:55:04.920 --> 00:55:08.400]   I mean, this thought about, once you have solved AGI,
[00:55:08.400 --> 00:55:09.960]   it solves all kinds of other,
[00:55:09.960 --> 00:55:11.240]   not just the theory of every problem,
[00:55:11.240 --> 00:55:14.200]   but all kinds of more useful problems to humanity
[00:55:14.200 --> 00:55:16.320]   is very appealing to many people,
[00:55:16.320 --> 00:55:18.280]   and I had this thought also,
[00:55:18.280 --> 00:55:23.280]   but I was quite disappointed with the state of the art
[00:55:23.280 --> 00:55:25.480]   of the field of AI.
[00:55:25.480 --> 00:55:28.160]   There was some theory about logical reasoning,
[00:55:28.160 --> 00:55:30.640]   but I was never convinced that this will fly,
[00:55:30.640 --> 00:55:33.360]   and then there was this more heuristic approaches
[00:55:33.360 --> 00:55:37.520]   with neural networks, and I didn't like these heuristics,
[00:55:37.520 --> 00:55:40.360]   so, and also I didn't have any good idea myself.
[00:55:40.880 --> 00:55:42.200]   (laughing)
[00:55:42.200 --> 00:55:44.280]   So, that's the reason why I toggled back and forth
[00:55:44.280 --> 00:55:46.400]   quite some while, and even worked four and a half years
[00:55:46.400 --> 00:55:48.280]   in a company developing software,
[00:55:48.280 --> 00:55:49.720]   something completely unrelated,
[00:55:49.720 --> 00:55:52.840]   but then I had this idea about the AXI model,
[00:55:52.840 --> 00:55:57.800]   and so what it gives you, it gives you a gold standard.
[00:55:57.800 --> 00:56:02.400]   So, I have proven that this is the most intelligent agents
[00:56:02.400 --> 00:56:06.840]   which anybody could "build" in quotation mark,
[00:56:06.840 --> 00:56:08.240]   because it's just mathematical,
[00:56:08.240 --> 00:56:11.200]   and you need infinite compute, yeah?
[00:56:11.200 --> 00:56:14.960]   But this is the limit, and this is completely specified.
[00:56:14.960 --> 00:56:19.320]   It's not just a framework, and every year,
[00:56:19.320 --> 00:56:21.240]   tens of frameworks are developed,
[00:56:21.240 --> 00:56:23.960]   which are just skeletons, and then pieces are missing,
[00:56:23.960 --> 00:56:25.760]   and usually these missing pieces turn out
[00:56:25.760 --> 00:56:27.400]   to be really, really difficult,
[00:56:27.400 --> 00:56:31.160]   and so this is completely and uniquely defined,
[00:56:31.160 --> 00:56:33.520]   and we can analyze that mathematically,
[00:56:33.520 --> 00:56:37.360]   and we have also developed some approximations.
[00:56:37.360 --> 00:56:40.320]   I can talk about that a little bit later.
[00:56:40.320 --> 00:56:41.840]   That would be sort of the top-down approach,
[00:56:41.840 --> 00:56:44.280]   like, say, for Neumann's minimax theory,
[00:56:44.280 --> 00:56:47.320]   that's the theoretical optimal play of games,
[00:56:47.320 --> 00:56:48.880]   and now we need to approximate it,
[00:56:48.880 --> 00:56:51.080]   put heuristics in, prune the tree, blah, blah, blah,
[00:56:51.080 --> 00:56:53.200]   and so on, so we can do that also with the AXI model,
[00:56:53.200 --> 00:56:54.320]   but for general AI.
[00:56:54.320 --> 00:56:59.000]   It can also inspire those, and most of,
[00:56:59.000 --> 00:57:00.880]   most researchers go bottom-up, right?
[00:57:00.880 --> 00:57:02.920]   They have their systems, they try to make it more general,
[00:57:02.920 --> 00:57:04.200]   more intelligent.
[00:57:04.200 --> 00:57:07.080]   It can inspire in which direction to go.
[00:57:07.760 --> 00:57:09.160]   - What do you mean by that?
[00:57:09.160 --> 00:57:11.240]   - So if you have some choice to make, right?
[00:57:11.240 --> 00:57:13.160]   So how should I evaluate my system
[00:57:13.160 --> 00:57:15.440]   if I can't do cross-validation?
[00:57:15.440 --> 00:57:18.080]   How should I do my learning
[00:57:18.080 --> 00:57:21.560]   if my standard regularization doesn't work well, yeah?
[00:57:21.560 --> 00:57:22.560]   So the answer is always this.
[00:57:22.560 --> 00:57:25.080]   We have a system which does everything that's AXI.
[00:57:25.080 --> 00:57:27.840]   It's just, you know, completely in the ivory tower,
[00:57:27.840 --> 00:57:30.640]   completely useless from a practical point of view,
[00:57:30.640 --> 00:57:31.960]   but you can look at it and see,
[00:57:31.960 --> 00:57:34.960]   ah, yeah, maybe, you know, I can take some aspects,
[00:57:34.960 --> 00:57:36.560]   and, you know, instead of Kolmogorov complexity,
[00:57:36.560 --> 00:57:38.120]   I just take some compressors
[00:57:38.120 --> 00:57:39.920]   which has been developed so far.
[00:57:39.920 --> 00:57:42.120]   And for the planning, well, we have UCT,
[00:57:42.120 --> 00:57:45.240]   which has also, you know, been used in Go,
[00:57:45.240 --> 00:57:50.080]   and at least it's inspired me a lot
[00:57:50.080 --> 00:57:54.200]   to have this formal definition.
[00:57:54.200 --> 00:57:55.800]   And if you look at other fields, you know,
[00:57:55.800 --> 00:57:57.760]   like I always come back to physics
[00:57:57.760 --> 00:57:59.000]   because I have a physics background.
[00:57:59.000 --> 00:58:00.720]   Think about the phenomenon of energy.
[00:58:00.720 --> 00:58:03.200]   That was a long time a mysterious concept,
[00:58:03.200 --> 00:58:05.880]   and at some point it was completely formalized.
[00:58:05.880 --> 00:58:08.160]   And that really helped a lot.
[00:58:08.160 --> 00:58:10.720]   And I can point out a lot of these things
[00:58:10.720 --> 00:58:12.960]   which were first mysterious and vague,
[00:58:12.960 --> 00:58:15.160]   and then they have been rigorously formalized.
[00:58:15.160 --> 00:58:18.240]   Speed and acceleration has been confused, right,
[00:58:18.240 --> 00:58:19.680]   until it was formally defined.
[00:58:19.680 --> 00:58:20.800]   Yeah, there was a time like this.
[00:58:20.800 --> 00:58:23.240]   And people, you know, often, you know,
[00:58:23.240 --> 00:58:26.200]   who don't have any background, you know, still confuse it.
[00:58:26.200 --> 00:58:31.920]   So, and this AXI model or the intelligence definitions,
[00:58:31.920 --> 00:58:33.160]   which is sort of the dual to it,
[00:58:33.160 --> 00:58:34.680]   we come back to that later,
[00:58:34.680 --> 00:58:37.200]   formalizes the notion of intelligence
[00:58:37.200 --> 00:58:38.920]   uniquely and rigorously.
[00:58:38.920 --> 00:58:41.640]   - So in a sense, it serves as kind of the light
[00:58:41.640 --> 00:58:43.040]   at the end of the tunnel.
[00:58:43.040 --> 00:58:43.880]   - Yes, yeah.
[00:58:43.880 --> 00:58:46.840]   - So for, so, I mean, there's a million questions
[00:58:46.840 --> 00:58:47.760]   I could ask her.
[00:58:47.760 --> 00:58:50.320]   So maybe kind of, okay,
[00:58:50.320 --> 00:58:52.120]   let's feel around in the dark a little bit.
[00:58:52.120 --> 00:58:54.760]   So there's been here at DeepMind,
[00:58:54.760 --> 00:58:56.980]   but in general, been a lot of breakthrough ideas,
[00:58:56.980 --> 00:58:59.500]   just like we've been saying around reinforcement learning.
[00:58:59.500 --> 00:59:02.120]   So how do you see the progress
[00:59:02.120 --> 00:59:04.440]   in reinforcement learning is different?
[00:59:04.440 --> 00:59:08.080]   Like which subset of AXI does it occupy?
[00:59:08.080 --> 00:59:10.600]   The current, like you said,
[00:59:10.600 --> 00:59:14.520]   maybe the Markov assumption is made quite often
[00:59:14.520 --> 00:59:15.880]   in reinforcement learning.
[00:59:15.880 --> 00:59:20.240]   There's other assumptions made
[00:59:20.240 --> 00:59:21.560]   in order to make the system work.
[00:59:21.560 --> 00:59:24.200]   What do you see as the difference connection
[00:59:24.200 --> 00:59:26.800]   between reinforcement learning and AXI?
[00:59:26.800 --> 00:59:29.000]   - Yeah, so the major difference is that
[00:59:29.000 --> 00:59:33.320]   essentially all other approaches,
[00:59:33.320 --> 00:59:35.640]   they make stronger assumptions.
[00:59:35.640 --> 00:59:36.720]   So in reinforcement learning,
[00:59:36.720 --> 00:59:40.800]   the Markov assumption is that the next state
[00:59:40.800 --> 00:59:42.040]   or next observation only depends
[00:59:42.040 --> 00:59:45.280]   on the previous observation and not the whole history,
[00:59:45.280 --> 00:59:47.560]   which makes, of course, the mathematics much easier
[00:59:47.560 --> 00:59:49.840]   rather than dealing with histories.
[00:59:49.840 --> 00:59:51.640]   Of course, they profit from it also
[00:59:51.640 --> 00:59:53.120]   because then you have algorithms
[00:59:53.120 --> 00:59:54.360]   that run on current computers
[00:59:54.360 --> 00:59:56.640]   and do something practically useful.
[00:59:56.640 --> 00:59:59.680]   But for general AI, all the assumptions
[00:59:59.680 --> 01:00:01.760]   which are made by other approaches,
[01:00:01.760 --> 01:00:04.040]   we know already now they are limiting.
[01:00:04.040 --> 01:00:09.600]   So for instance, usually you need an ergodicity assumption
[01:00:09.600 --> 01:00:11.680]   in the MDP framework in order to learn.
[01:00:11.680 --> 01:00:14.760]   Ergodicity essentially means that you can recover
[01:00:14.760 --> 01:00:16.720]   from your mistakes and that there are no traps
[01:00:16.720 --> 01:00:18.280]   in the environment.
[01:00:18.280 --> 01:00:20.000]   And if you make this assumption,
[01:00:20.000 --> 01:00:22.960]   then essentially you can go back to a previous state,
[01:00:22.960 --> 01:00:24.160]   go there a couple of times,
[01:00:24.160 --> 01:00:29.160]   and then learn what statistics and what the state is like.
[01:00:29.960 --> 01:00:33.320]   And then in the long run, perform well in this state.
[01:00:33.320 --> 01:00:35.880]   But there are no fundamental problems.
[01:00:35.880 --> 01:00:39.120]   But in real life, we know there can be one single action.
[01:00:39.120 --> 01:00:44.120]   One second of being inattentive while driving a car fast
[01:00:44.120 --> 01:00:45.960]   can ruin the rest of my life.
[01:00:45.960 --> 01:00:48.400]   I can become quadriplegic or whatever.
[01:00:48.400 --> 01:00:50.360]   So there's no recovery anymore.
[01:00:50.360 --> 01:00:52.800]   So the real world is not ergodic, I always say.
[01:00:52.800 --> 01:00:54.520]   There are traps and there are situations
[01:00:54.520 --> 01:00:56.440]   where you're not recover from.
[01:00:56.440 --> 01:01:01.440]   And very little theory has been developed for this case.
[01:01:01.440 --> 01:01:07.280]   - What about, what do you see in the context of Aixia
[01:01:07.280 --> 01:01:09.680]   as the role of exploration?
[01:01:09.680 --> 01:01:14.680]   Sort of, you mentioned in the real world
[01:01:14.680 --> 01:01:17.880]   we can get into trouble when we make the wrong decisions
[01:01:17.880 --> 01:01:19.240]   and really pay for it.
[01:01:19.240 --> 01:01:22.200]   But exploration seems to be fundamentally important
[01:01:22.200 --> 01:01:25.560]   for learning about this world, for gaining new knowledge.
[01:01:25.560 --> 01:01:29.120]   So is exploration baked in?
[01:01:29.120 --> 01:01:32.240]   Another way to ask it, what are the parameters
[01:01:32.240 --> 01:01:35.360]   of Aixia that can be controlled?
[01:01:35.360 --> 01:01:38.000]   - Yeah, I say the good thing is that
[01:01:38.000 --> 01:01:40.200]   there are no parameters to control.
[01:01:40.200 --> 01:01:43.120]   Some other people try knobs to control,
[01:01:43.120 --> 01:01:44.120]   and you can do that.
[01:01:44.120 --> 01:01:46.880]   I mean, you can modify Aixia so that you have some knobs
[01:01:46.880 --> 01:01:48.800]   to play with if you want to.
[01:01:48.800 --> 01:01:53.640]   But the exploration is directly baked in.
[01:01:53.640 --> 01:01:56.960]   And that comes from the Bayesian learning
[01:01:56.960 --> 01:01:58.680]   and the long-term planning.
[01:01:58.680 --> 01:02:03.680]   So these together already imply exploration.
[01:02:03.680 --> 01:02:08.240]   You can nicely and explicitly prove that
[01:02:08.240 --> 01:02:13.240]   for simple problems like so-called bandit problems,
[01:02:13.240 --> 01:02:18.000]   where you say, to give a real world example,
[01:02:18.000 --> 01:02:20.200]   say you have two medical treatments, A and B,
[01:02:20.200 --> 01:02:21.560]   you don't know the effectiveness,
[01:02:21.560 --> 01:02:23.360]   you try A a little bit, B a little bit,
[01:02:23.360 --> 01:02:25.760]   but you don't want to harm too many patients.
[01:02:25.760 --> 01:02:29.760]   So you have to sort of trade off exploring.
[01:02:29.760 --> 01:02:31.680]   And at some point you want to explore,
[01:02:31.680 --> 01:02:34.040]   and you can do the mathematics
[01:02:34.040 --> 01:02:36.040]   and figure out the optimal strategy.
[01:02:36.040 --> 01:02:39.080]   The so-called Bayesian agents,
[01:02:39.080 --> 01:02:41.080]   they're also non-Bayesian agents,
[01:02:41.080 --> 01:02:44.200]   but it shows that this Bayesian framework,
[01:02:44.200 --> 01:02:47.360]   by taking a prior over possible worlds,
[01:02:47.360 --> 01:02:48.400]   doing the Bayesian mixture,
[01:02:48.400 --> 01:02:50.600]   then the Bayes optimal decision with long-term planning
[01:02:50.600 --> 01:02:55.600]   that is important, automatically implies exploration
[01:02:55.600 --> 01:02:57.600]   also to the proper extent,
[01:02:57.600 --> 01:02:59.680]   not too much exploration and not too little,
[01:02:59.680 --> 01:03:01.520]   in these very simple settings.
[01:03:01.520 --> 01:03:04.360]   In the IXE model, I was also able to prove
[01:03:04.360 --> 01:03:06.160]   that it is a self-optimizing theorem
[01:03:06.160 --> 01:03:07.720]   or asymptotic optimality theorems,
[01:03:07.720 --> 01:03:10.480]   although they're only asymptotic, not finite time bounds.
[01:03:10.480 --> 01:03:12.240]   - So it seems like the long-term planning
[01:03:12.240 --> 01:03:14.080]   is really important, but the long-term part
[01:03:14.080 --> 01:03:15.400]   of the planning is really important.
[01:03:15.400 --> 01:03:17.320]   - Yes. - And also,
[01:03:17.320 --> 01:03:18.920]   maybe a quick tangent,
[01:03:18.920 --> 01:03:21.360]   how important do you think is removing
[01:03:21.360 --> 01:03:25.320]   the Markov assumption and looking at the full history?
[01:03:25.320 --> 01:03:28.040]   Sort of intuitively, of course, it's important,
[01:03:28.040 --> 01:03:30.960]   but is it like fundamentally transformative
[01:03:30.960 --> 01:03:33.400]   to the entirety of the problem?
[01:03:33.400 --> 01:03:34.560]   What's your sense of it?
[01:03:34.560 --> 01:03:37.400]   'Cause we make that assumption quite often,
[01:03:37.400 --> 01:03:40.000]   just throwing away the past.
[01:03:40.000 --> 01:03:41.880]   - No, I think it's absolutely crucial.
[01:03:41.880 --> 01:03:47.240]   The question is whether there's a way to deal with it
[01:03:47.240 --> 01:03:52.240]   in a more heuristic and still sufficiently well way.
[01:03:52.240 --> 01:03:55.520]   So I have to come up with an example on the fly,
[01:03:55.520 --> 01:03:59.400]   but you have some key event in your life,
[01:03:59.400 --> 01:04:02.080]   long time ago, in some city or something,
[01:04:02.080 --> 01:04:04.680]   you realize that's a really dangerous street or whatever,
[01:04:04.680 --> 01:04:08.000]   right, yeah, and you want to remember that forever, right,
[01:04:08.000 --> 01:04:09.800]   in case you come back there.
[01:04:09.800 --> 01:04:11.560]   - Kind of a selective kind of memory.
[01:04:11.560 --> 01:04:15.160]   So you remember all the important events in the past,
[01:04:15.160 --> 01:04:17.480]   but somehow selecting the importance is--
[01:04:17.480 --> 01:04:19.960]   - That's very hard, yeah, and I'm not concerned
[01:04:19.960 --> 01:04:21.720]   about just storing the whole history,
[01:04:21.720 --> 01:04:24.720]   just you can calculate human life,
[01:04:24.720 --> 01:04:27.640]   say, 30 or 100 years, doesn't matter, right,
[01:04:27.640 --> 01:04:31.800]   how much data comes in through the vision system
[01:04:31.800 --> 01:04:35.200]   and the auditory system, you compress it a little bit,
[01:04:35.200 --> 01:04:37.560]   in this case, lossily, and store it.
[01:04:37.560 --> 01:04:40.520]   We are soon in the means of just storing it,
[01:04:40.520 --> 01:04:44.920]   but you still need to do selection for the planning part
[01:04:44.920 --> 01:04:47.280]   and the compression for the understanding part.
[01:04:47.280 --> 01:04:50.000]   The raw storage, I'm really not concerned about,
[01:04:50.000 --> 01:04:53.640]   and I think we should just store, if you develop an agent,
[01:04:53.640 --> 01:04:59.400]   preferably just store all the interaction history,
[01:04:59.400 --> 01:05:02.240]   and then you build, of course, models on top of it,
[01:05:02.240 --> 01:05:04.960]   and you compress it, and you are selective,
[01:05:04.960 --> 01:05:08.120]   but occasionally, you go back to the old data
[01:05:08.120 --> 01:05:11.840]   and reanalyze it based on your new experience you have.
[01:05:11.840 --> 01:05:13.840]   You know, sometimes you are in school,
[01:05:13.840 --> 01:05:16.800]   you learn all these things you think is totally useless,
[01:05:16.800 --> 01:05:19.320]   and much later, you realize, oh, they were not
[01:05:19.320 --> 01:05:21.600]   so useless as you thought.
[01:05:21.600 --> 01:05:24.080]   - I'm looking at you, linear algebra.
[01:05:24.080 --> 01:05:27.720]   Right, so maybe let me ask about objective functions,
[01:05:27.720 --> 01:05:32.720]   because that reward, it seems to be an important part.
[01:05:32.720 --> 01:05:36.660]   The rewards are kind of given to the system.
[01:05:36.660 --> 01:05:43.200]   For a lot of people, the specification
[01:05:44.040 --> 01:05:48.080]   of the objective function is a key part of intelligence,
[01:05:48.080 --> 01:05:52.920]   like the agent itself figuring out what is important.
[01:05:52.920 --> 01:05:54.640]   What do you think about that?
[01:05:54.640 --> 01:05:58.560]   Is it possible within the IXE framework
[01:05:58.560 --> 01:06:01.840]   to yourself discover the reward
[01:06:01.840 --> 01:06:03.700]   based on which you should operate?
[01:06:03.700 --> 01:06:07.200]   - Okay, that will be a long answer.
[01:06:07.200 --> 01:06:08.760]   (Lex laughs)
[01:06:08.760 --> 01:06:11.880]   So, and that is a very interesting question,
[01:06:11.880 --> 01:06:14.560]   and I'm asked a lot about this question.
[01:06:14.560 --> 01:06:16.680]   Where do the rewards come from?
[01:06:16.680 --> 01:06:19.200]   And that depends, yeah?
[01:06:19.200 --> 01:06:22.480]   So, and then, you know, I give you now a couple of answers.
[01:06:22.480 --> 01:06:27.480]   So if we want to build agents, now let's start simple.
[01:06:27.480 --> 01:06:29.920]   So let's assume we want to build an agent
[01:06:29.920 --> 01:06:34.400]   based on the IXE model, which performs a particular task.
[01:06:34.400 --> 01:06:35.800]   Let's start with something super simple,
[01:06:35.800 --> 01:06:38.040]   like, I mean, super simple, like playing chess,
[01:06:38.040 --> 01:06:39.880]   yeah, or go or something, yeah?
[01:06:39.880 --> 01:06:42.360]   Then you just, you know, the reward is, you know,
[01:06:42.360 --> 01:06:43.520]   winning the game is plus one,
[01:06:43.520 --> 01:06:46.200]   losing the game is minus one, done.
[01:06:46.200 --> 01:06:47.360]   You apply this agent.
[01:06:47.360 --> 01:06:50.120]   If you have enough compute, you let itself play,
[01:06:50.120 --> 01:06:51.800]   and it will learn the rules of the game,
[01:06:51.800 --> 01:06:53.040]   will play perfect chess.
[01:06:53.040 --> 01:06:56.040]   After some while, problem solved, okay?
[01:06:56.040 --> 01:06:59.520]   So if you have more complicated problems,
[01:06:59.520 --> 01:07:04.560]   then you may believe that you have the right reward,
[01:07:04.560 --> 01:07:05.400]   but it's not.
[01:07:05.400 --> 01:07:09.000]   So a nice, cute example is elevator control
[01:07:09.000 --> 01:07:10.960]   that is also in Rich Sutton's book,
[01:07:10.960 --> 01:07:12.640]   which is a great book, by the way.
[01:07:12.640 --> 01:07:16.200]   So you control the elevator, and you think,
[01:07:16.200 --> 01:07:18.320]   well, maybe the reward should be coupled
[01:07:18.320 --> 01:07:20.440]   to how long people wait in front of the elevator.
[01:07:20.440 --> 01:07:22.360]   You know, long wait is bad.
[01:07:22.360 --> 01:07:24.240]   You program it, and you do it,
[01:07:24.240 --> 01:07:26.360]   and what happens is the elevator eagerly picks up
[01:07:26.360 --> 01:07:28.800]   all the people, but never drops them off.
[01:07:28.800 --> 01:07:30.960]   (both laughing)
[01:07:30.960 --> 01:07:33.040]   So then you realize, ah, maybe the time
[01:07:33.040 --> 01:07:36.800]   in the elevator also counts, so you minimize the sum, yeah?
[01:07:36.800 --> 01:07:39.520]   And the elevator does that, but never picks up the people
[01:07:39.520 --> 01:07:40.920]   in the 10th floor and the top floor,
[01:07:40.920 --> 01:07:42.840]   because in expectation, it's not worth it.
[01:07:42.840 --> 01:07:43.840]   Just let them stay.
[01:07:43.840 --> 01:07:46.360]   - Yeah. (both laughing)
[01:07:46.360 --> 01:07:50.160]   - So even in apparently simple problems,
[01:07:50.160 --> 01:07:51.760]   you can make mistakes, yeah?
[01:07:51.760 --> 01:07:55.720]   And that's what, in more serious context,
[01:07:55.720 --> 01:07:58.520]   say, AGI safety researchers consider.
[01:07:58.520 --> 01:08:01.120]   So now let's go back to general agents.
[01:08:01.120 --> 01:08:02.880]   So assume you want to build an agent
[01:08:02.880 --> 01:08:05.640]   which is generally useful to humans, yeah?
[01:08:05.640 --> 01:08:08.000]   So you have a household robot, yeah?
[01:08:08.000 --> 01:08:10.440]   And it should do all kinds of tasks.
[01:08:10.440 --> 01:08:15.040]   So in this case, the human should give the reward on the fly.
[01:08:15.040 --> 01:08:16.800]   I mean, maybe it's pre-trained in the factory
[01:08:16.800 --> 01:08:18.600]   and that there's some sort of internal reward
[01:08:18.600 --> 01:08:20.520]   for the battery level or whatever, yeah?
[01:08:20.520 --> 01:08:23.600]   But, so it does the dishes badly.
[01:08:23.600 --> 01:08:25.280]   You know, you punish the robot, it does it good.
[01:08:25.280 --> 01:08:28.000]   You reward the robot, and then train it to a new task,
[01:08:28.000 --> 01:08:29.000]   kind of like a child, right?
[01:08:29.000 --> 01:08:31.720]   So you need the human in the loop
[01:08:31.720 --> 01:08:35.080]   if you want a system which is useful to the human.
[01:08:35.080 --> 01:08:38.520]   And as long as this agent stays sub-human level,
[01:08:38.520 --> 01:08:41.560]   that should work reasonably well,
[01:08:41.560 --> 01:08:43.560]   apart from, you know, these examples.
[01:08:43.560 --> 01:08:45.680]   It becomes critical if they become, you know,
[01:08:45.680 --> 01:08:46.520]   on a human level.
[01:08:46.520 --> 01:08:47.800]   It's the same as children, small children.
[01:08:47.800 --> 01:08:49.360]   You have reasonably well under control.
[01:08:49.360 --> 01:08:51.160]   They become older.
[01:08:51.160 --> 01:08:53.720]   The reward technique doesn't work so well anymore.
[01:08:53.720 --> 01:08:59.160]   So then finally, so this would be agents
[01:08:59.160 --> 01:09:02.400]   which are just, you could say, slaves to the humans, yeah?
[01:09:02.400 --> 01:09:04.560]   So if you are more ambitious and just say,
[01:09:04.560 --> 01:09:08.640]   we want to build a new species of intelligent beings,
[01:09:08.640 --> 01:09:09.960]   we put them on a new planet,
[01:09:09.960 --> 01:09:12.680]   and we want them to develop this planet or whatever.
[01:09:12.680 --> 01:09:15.920]   So we don't give them any reward.
[01:09:15.920 --> 01:09:17.480]   So what could we do?
[01:09:17.480 --> 01:09:20.280]   And you could try to, you know,
[01:09:20.280 --> 01:09:22.240]   come up with some reward functions like, you know,
[01:09:22.240 --> 01:09:23.960]   it should maintain itself, the robot.
[01:09:23.960 --> 01:09:28.720]   It should maybe multiply, build more robots, right?
[01:09:28.720 --> 01:09:32.320]   And, you know, maybe, well, all kinds of things
[01:09:32.320 --> 01:09:34.280]   which you find useful, but that's pretty hard, right?
[01:09:34.280 --> 01:09:36.440]   You know, what does self-maintenance mean?
[01:09:36.440 --> 01:09:38.160]   You know, what does it mean to build a copy?
[01:09:38.160 --> 01:09:40.720]   Should it be exact copy, an approximate copy?
[01:09:40.720 --> 01:09:42.080]   And so that's really hard.
[01:09:42.080 --> 01:09:46.200]   But Laurent Assor, also at DeepMind,
[01:09:46.200 --> 01:09:48.840]   developed a beautiful model.
[01:09:48.840 --> 01:09:50.600]   So he just took the EICSI model
[01:09:50.600 --> 01:09:55.040]   and coupled the rewards to information gain.
[01:09:55.040 --> 01:09:57.920]   So he said the reward is proportional
[01:09:57.920 --> 01:10:00.760]   to how much the agent had learned about the world.
[01:10:00.760 --> 01:10:03.360]   And you can rigorously, formally, uniquely define that
[01:10:03.360 --> 01:10:05.880]   in terms of our Kettler versions, okay?
[01:10:05.880 --> 01:10:07.400]   So if you put that in,
[01:10:07.400 --> 01:10:09.880]   you get a completely autonomous agent.
[01:10:09.880 --> 01:10:11.720]   And actually, interestingly, for this agent,
[01:10:11.720 --> 01:10:13.160]   we can prove much stronger result
[01:10:13.160 --> 01:10:16.000]   than for the general agent, which is also nice.
[01:10:16.000 --> 01:10:18.120]   And if you let this agent loose,
[01:10:18.120 --> 01:10:20.040]   it will be, in a sense, the optimal scientist.
[01:10:20.040 --> 01:10:21.680]   It is absolutely curious to learn
[01:10:21.680 --> 01:10:24.120]   as much as possible about the world.
[01:10:24.120 --> 01:10:25.760]   And of course, it will also have
[01:10:25.760 --> 01:10:27.200]   a lot of instrumental goals, right?
[01:10:27.200 --> 01:10:29.600]   In order to learn, it needs to at least survive, right?
[01:10:29.600 --> 01:10:31.560]   A dead agent is not good for anything.
[01:10:31.560 --> 01:10:34.000]   So it needs to have self-preservation.
[01:10:34.000 --> 01:10:35.840]   And if it builds small helpers,
[01:10:35.840 --> 01:10:39.160]   acquiring more information, it will do that, yeah?
[01:10:39.160 --> 01:10:43.720]   If exploration, space exploration or whatever is necessary,
[01:10:43.720 --> 01:10:45.960]   right, to gathering information and develop it.
[01:10:45.960 --> 01:10:48.240]   So it has a lot of instrumental goals
[01:10:48.240 --> 01:10:51.040]   following on this information gain.
[01:10:51.040 --> 01:10:53.800]   And this agent is completely autonomous of us.
[01:10:53.800 --> 01:10:55.680]   No rewards necessary anymore.
[01:10:55.680 --> 01:10:57.560]   - Yeah, of course, it could find a way
[01:10:57.560 --> 01:10:59.760]   to game the concept of information
[01:10:59.760 --> 01:11:04.760]   and get stuck in that library that you mentioned beforehand
[01:11:04.760 --> 01:11:08.680]   with a very large number of books.
[01:11:08.680 --> 01:11:10.760]   - The first agent had this problem.
[01:11:10.760 --> 01:11:13.720]   It would get stuck in front of an old TV screen,
[01:11:13.720 --> 01:11:15.040]   which has just had white noise.
[01:11:15.040 --> 01:11:16.560]   - Yeah, white noise, yeah.
[01:11:16.560 --> 01:11:20.680]   - But the second version can deal with at least stochasticity.
[01:11:20.680 --> 01:11:22.560]   Well. - Yeah.
[01:11:22.560 --> 01:11:25.440]   What about curiosity, this kind of word,
[01:11:25.440 --> 01:11:28.000]   curiosity, creativity?
[01:11:28.000 --> 01:11:30.960]   Is that kind of the reward function being
[01:11:30.960 --> 01:11:32.000]   of getting new information,
[01:11:32.000 --> 01:11:37.000]   is that similar to idea of kind of injecting exploration
[01:11:37.000 --> 01:11:41.960]   for its own sake inside the reward function?
[01:11:41.960 --> 01:11:44.960]   Do you find this at all appealing, interesting?
[01:11:44.960 --> 01:11:46.400]   - I think that's a nice definition.
[01:11:46.400 --> 01:11:49.160]   Curiosity is reward, sorry,
[01:11:49.160 --> 01:11:51.960]   curiosity is exploration for its own sake.
[01:11:51.960 --> 01:11:56.280]   Yeah, I would accept that.
[01:11:57.200 --> 01:12:00.000]   But most curiosity, well, in humans,
[01:12:00.000 --> 01:12:01.320]   and especially in children,
[01:12:01.320 --> 01:12:03.120]   is not just for its own sake,
[01:12:03.120 --> 01:12:06.040]   but for actually learning about the environment
[01:12:06.040 --> 01:12:08.520]   and for behaving better.
[01:12:08.520 --> 01:12:13.200]   So I think most curiosity is tied, in the end,
[01:12:13.200 --> 01:12:14.920]   towards performing better.
[01:12:14.920 --> 01:12:17.720]   - Well, okay, so if intelligent systems
[01:12:17.720 --> 01:12:19.480]   need to have this reward function,
[01:12:19.480 --> 01:12:22.480]   you're an intelligent system,
[01:12:22.480 --> 01:12:26.160]   currently passing the Turing test quite effectively.
[01:12:26.160 --> 01:12:27.840]   (Markus laughs)
[01:12:27.840 --> 01:12:30.280]   What's the reward function
[01:12:30.280 --> 01:12:33.960]   of our human intelligence existence?
[01:12:33.960 --> 01:12:35.200]   What's the reward function
[01:12:35.200 --> 01:12:37.760]   that Markus Hodder is operating under?
[01:12:37.760 --> 01:12:39.800]   - Okay, to the first question,
[01:12:39.800 --> 01:12:44.520]   the biological reward function is to survive and to spread,
[01:12:44.520 --> 01:12:48.280]   and very few humans are able to overcome
[01:12:48.280 --> 01:12:50.000]   this biological reward function.
[01:12:50.000 --> 01:12:54.240]   But we live in a very nice world
[01:12:54.240 --> 01:12:56.280]   where we have lots of spare time
[01:12:56.280 --> 01:12:57.680]   and can still survive and spread,
[01:12:57.680 --> 01:13:01.960]   so we can develop arbitrary other interests,
[01:13:01.960 --> 01:13:03.360]   which is quite interesting.
[01:13:03.360 --> 01:13:04.440]   - On top of that.
[01:13:04.440 --> 01:13:06.240]   - On top of that, yeah.
[01:13:06.240 --> 01:13:09.720]   But the survival and spreading is, I would say,
[01:13:09.720 --> 01:13:14.400]   the goal or the reward function of humans, the core one.
[01:13:14.400 --> 01:13:17.480]   - I like how you avoided answering the second question,
[01:13:17.480 --> 01:13:19.760]   which a good intelligent system would.
[01:13:19.760 --> 01:13:20.920]   - So my--
[01:13:20.920 --> 01:13:24.200]   - Your own meaning of life and the reward function.
[01:13:24.200 --> 01:13:26.960]   - My own meaning of life and reward function
[01:13:26.960 --> 01:13:29.560]   is to find an AGI to build it.
[01:13:29.560 --> 01:13:31.200]   (Markus laughs)
[01:13:31.200 --> 01:13:32.620]   - Beautifully put, okay.
[01:13:32.620 --> 01:13:34.280]   Let's dissect the X even further.
[01:13:34.280 --> 01:13:36.960]   So one of the assumptions is,
[01:13:36.960 --> 01:13:39.680]   kind of, infinity keeps creeping up everywhere.
[01:13:39.680 --> 01:13:42.600]   (Markus laughs)
[01:13:42.600 --> 01:13:44.920]   Which, what are your thoughts
[01:13:44.920 --> 01:13:46.920]   on kind of bounded rationality
[01:13:46.920 --> 01:13:50.040]   and sort of the nature of our existence
[01:13:50.040 --> 01:13:52.000]   in intelligent systems is that we're operating
[01:13:52.000 --> 01:13:55.640]   always under constraints, under limited time,
[01:13:55.640 --> 01:13:57.640]   limited resources.
[01:13:57.640 --> 01:13:59.480]   How does that, how do you think about that
[01:13:59.480 --> 01:14:01.600]   within the IXE framework,
[01:14:01.600 --> 01:14:04.480]   within trying to create an AGI system
[01:14:04.480 --> 01:14:06.760]   that operates under these constraints?
[01:14:06.760 --> 01:14:09.200]   - Yeah, that is one of the criticisms about IXE,
[01:14:09.200 --> 01:14:11.320]   that it ignores computation completely,
[01:14:11.320 --> 01:14:13.800]   and some people believe that intelligence
[01:14:13.800 --> 01:14:18.800]   is inherently tied to what's bounded resources.
[01:14:18.800 --> 01:14:21.120]   - What do you think on this one point?
[01:14:21.120 --> 01:14:23.920]   Do you think it's, do you think the bounded resources
[01:14:23.920 --> 01:14:25.640]   are fundamental to intelligence?
[01:14:25.640 --> 01:14:31.160]   - I would say that an intelligence notion
[01:14:31.160 --> 01:14:35.520]   which ignores computational limits is extremely useful.
[01:14:35.520 --> 01:14:39.160]   A good intelligence notion which includes these resources
[01:14:39.160 --> 01:14:42.140]   would be even more useful, but we don't have that yet.
[01:14:42.140 --> 01:14:48.280]   And so look at other fields outside of computer science.
[01:14:48.480 --> 01:14:52.240]   Computational aspects never play a fundamental role.
[01:14:52.240 --> 01:14:54.880]   You develop biological models for cells,
[01:14:54.880 --> 01:14:56.680]   something in physics, these theories,
[01:14:56.680 --> 01:14:58.160]   I mean, become more and more crazy
[01:14:58.160 --> 01:15:00.320]   and harder and harder to compute.
[01:15:00.320 --> 01:15:02.400]   Well, in the end, of course, we need to do something
[01:15:02.400 --> 01:15:05.520]   with this model, but there's more nuisance than a feature.
[01:15:05.520 --> 01:15:10.040]   And I'm sometimes wondering if artificial intelligence
[01:15:10.040 --> 01:15:12.080]   would not sit in a computer science department,
[01:15:12.080 --> 01:15:14.040]   but in a philosophy department,
[01:15:14.040 --> 01:15:16.120]   then this computational focus
[01:15:16.120 --> 01:15:18.400]   would be probably significantly less.
[01:15:18.400 --> 01:15:19.720]   I mean, think about the induction problem
[01:15:19.720 --> 01:15:22.080]   is more in the philosophy department.
[01:15:22.080 --> 01:15:24.480]   There's virtually no paper who cares about, you know,
[01:15:24.480 --> 01:15:26.440]   how long it takes to compute the answer.
[01:15:26.440 --> 01:15:28.320]   That is completely secondary.
[01:15:28.320 --> 01:15:31.660]   Of course, once we have figured out the first problem,
[01:15:31.660 --> 01:15:35.800]   so intelligence without computational resources,
[01:15:35.800 --> 01:15:39.360]   then the next and very good question is,
[01:15:39.360 --> 01:15:42.420]   could we improve it by including computational resources?
[01:15:42.420 --> 01:15:45.480]   But nobody was able to do that so far
[01:15:45.480 --> 01:15:47.780]   in an even halfway satisfactory manner.
[01:15:48.780 --> 01:15:51.580]   - I like that, that in the long run,
[01:15:51.580 --> 01:15:53.980]   the right department to belong to is philosophy.
[01:15:53.980 --> 01:15:59.580]   That's actually quite a deep idea of,
[01:15:59.580 --> 01:16:01.940]   or even to at least to think about big picture
[01:16:01.940 --> 01:16:05.340]   philosophical questions, big picture questions,
[01:16:05.340 --> 01:16:07.420]   even in the computer science department.
[01:16:07.420 --> 01:16:10.060]   But you've mentioned approximation,
[01:16:10.060 --> 01:16:12.220]   sort of there's a lot of infinity,
[01:16:12.220 --> 01:16:13.940]   a lot of huge resources needed.
[01:16:13.940 --> 01:16:16.340]   Are there approximations to IEC
[01:16:16.340 --> 01:16:19.820]   that within the IEC framework that are useful?
[01:16:19.820 --> 01:16:23.140]   - Yeah, we have developed a couple of approximations.
[01:16:23.140 --> 01:16:28.140]   And what we do there is that the Solomoff induction part,
[01:16:28.140 --> 01:16:32.440]   which was, you know, find the shortest program
[01:16:32.440 --> 01:16:33.680]   describing your data,
[01:16:33.680 --> 01:16:36.700]   which has replaced it by standard data compressors, right?
[01:16:36.700 --> 01:16:39.280]   And the better compressors get, you know,
[01:16:39.280 --> 01:16:41.740]   the better this part will become.
[01:16:41.740 --> 01:16:43.420]   We focus on a particular compressor
[01:16:43.420 --> 01:16:44.580]   called context-free weighting,
[01:16:44.580 --> 01:16:48.540]   which is pretty amazing, not so well known.
[01:16:48.540 --> 01:16:50.140]   And has beautiful theoretical properties,
[01:16:50.140 --> 01:16:52.260]   also works reasonably well in practice.
[01:16:52.260 --> 01:16:55.180]   So we use that for the approximation of the induction
[01:16:55.180 --> 01:16:57.220]   and the learning and the prediction part.
[01:16:57.220 --> 01:17:01.740]   And for the planning part,
[01:17:01.740 --> 01:17:03.740]   we essentially just took the ideas
[01:17:03.740 --> 01:17:07.340]   from a computer go from 2006.
[01:17:07.340 --> 01:17:10.460]   It was Java Zipispari, also now at DeepMind,
[01:17:11.340 --> 01:17:14.620]   who developed the so-called UCT algorithm,
[01:17:14.620 --> 01:17:17.460]   upper confidence bound for trees algorithm
[01:17:17.460 --> 01:17:19.100]   on top of the Monte Carlo tree search.
[01:17:19.100 --> 01:17:23.220]   So we approximate this planning part by sampling.
[01:17:23.220 --> 01:17:28.220]   And it's successful on some small toy problems.
[01:17:28.220 --> 01:17:33.520]   We don't want to lose the generality, right?
[01:17:33.520 --> 01:17:34.940]   And that's sort of the handicap, right?
[01:17:34.940 --> 01:17:38.900]   If you want to be general, you have to give up something.
[01:17:38.900 --> 01:17:41.140]   So, but this single agent was able to play,
[01:17:41.140 --> 01:17:44.260]   you know, small games like coon poker and tic-tac-toe
[01:17:44.260 --> 01:17:47.180]   and even Pac-Man.
[01:17:47.180 --> 01:17:52.060]   And it's the same architecture, no change.
[01:17:52.060 --> 01:17:54.940]   The agent doesn't know the rules of the game,
[01:17:54.940 --> 01:17:56.780]   virtually nothing, all by itself,
[01:17:56.780 --> 01:17:58.860]   or by player with these environments.
[01:17:58.860 --> 01:18:03.220]   - So, Juergen Schmidhuber proposed something
[01:18:03.220 --> 01:18:04.780]   called Gate-On Machines,
[01:18:04.780 --> 01:18:06.940]   which is a self-improving program
[01:18:06.940 --> 01:18:08.460]   that rewrites its own code.
[01:18:09.460 --> 01:18:12.860]   Sort of mathematically or philosophically,
[01:18:12.860 --> 01:18:15.140]   what's the relationship in your eyes,
[01:18:15.140 --> 01:18:16.220]   if you're familiar with it,
[01:18:16.220 --> 01:18:18.460]   between Aixie and the Gate-On Machines?
[01:18:18.460 --> 01:18:19.780]   - Yeah, familiar with it.
[01:18:19.780 --> 01:18:22.380]   He developed it while I was in his lab.
[01:18:22.380 --> 01:18:26.260]   Yeah, so the Gate-On Machine, to explain it briefly,
[01:18:26.260 --> 01:18:28.980]   you give it a task.
[01:18:28.980 --> 01:18:30.460]   It could be a simple task as, you know,
[01:18:30.460 --> 01:18:32.540]   finding prime factors and numbers, right?
[01:18:32.540 --> 01:18:33.860]   You can formally write it down.
[01:18:33.860 --> 01:18:35.300]   There's a very slow algorithm to do that.
[01:18:35.300 --> 01:18:37.540]   Just try all the factors, yeah?
[01:18:37.540 --> 01:18:39.300]   Or play chess, right?
[01:18:39.300 --> 01:18:41.260]   Optimally, you write the algorithm to minimax
[01:18:41.260 --> 01:18:42.140]   to the end of the game,
[01:18:42.140 --> 01:18:45.420]   so you write down what the Girdle machine should do.
[01:18:45.420 --> 01:18:48.980]   Then it will take part of its resources
[01:18:48.980 --> 01:18:50.820]   to run this program,
[01:18:50.820 --> 01:18:54.100]   and other part of its sources to improve this program.
[01:18:54.100 --> 01:18:56.980]   And when it finds an improved version
[01:18:56.980 --> 01:19:00.780]   which provably computes the same answer,
[01:19:00.780 --> 01:19:02.420]   so that's the key part, yeah?
[01:19:02.420 --> 01:19:05.780]   It needs to prove by itself that this change of program
[01:19:05.780 --> 01:19:09.020]   still satisfies the original specification.
[01:19:09.020 --> 01:19:10.180]   And if it does so,
[01:19:10.180 --> 01:19:11.740]   then it replaces the original program
[01:19:11.740 --> 01:19:13.220]   by the improved program,
[01:19:13.220 --> 01:19:17.140]   and by definition does the same job, but just faster, okay?
[01:19:17.140 --> 01:19:19.260]   And then, you know, it proves over it and over it.
[01:19:19.260 --> 01:19:22.380]   And it's developed in a way that
[01:19:22.380 --> 01:19:26.820]   all parts of this Girdle machine can self-improve,
[01:19:26.820 --> 01:19:29.220]   but it stays provably consistent
[01:19:29.220 --> 01:19:31.860]   with the original specification.
[01:19:31.860 --> 01:19:36.140]   So from this perspective, it has nothing to do with IXE,
[01:19:36.140 --> 01:19:40.580]   but if you would now put IXE as the starting axioms in,
[01:19:40.580 --> 01:19:44.860]   it would run IXE, but, you know, that takes forever.
[01:19:44.860 --> 01:19:48.540]   But then if it finds a provable speedup of IXE,
[01:19:48.540 --> 01:19:51.020]   it would replace it by this, and this, and this,
[01:19:51.020 --> 01:19:52.900]   and maybe eventually it comes up with a model
[01:19:52.900 --> 01:19:54.540]   which is still the IXE model.
[01:19:54.540 --> 01:19:59.540]   It cannot be, I mean, just for the knowledgeable reader,
[01:19:59.660 --> 01:20:02.900]   IXE is incomputable, and I can prove that,
[01:20:02.900 --> 01:20:06.180]   therefore, there cannot be a computable exact
[01:20:06.180 --> 01:20:08.660]   algorithm computer.
[01:20:08.660 --> 01:20:10.380]   There needs to be some approximations,
[01:20:10.380 --> 01:20:12.020]   and this is not dealt with the Girdle machine,
[01:20:12.020 --> 01:20:13.260]   so you have to do something about it.
[01:20:13.260 --> 01:20:15.700]   But there's the IXETL model, which is finitely computable,
[01:20:15.700 --> 01:20:16.540]   which we could put in.
[01:20:16.540 --> 01:20:19.260]   - Which part of IXE is non-computable?
[01:20:19.260 --> 01:20:20.780]   - The Solomonov induction part.
[01:20:20.780 --> 01:20:22.260]   - The induction, okay, so.
[01:20:22.260 --> 01:20:26.380]   - But there is ways of getting computable approximations
[01:20:26.380 --> 01:20:27.500]   of the IXE model.
[01:20:28.500 --> 01:20:30.060]   So then it's at least computable.
[01:20:30.060 --> 01:20:33.740]   It is still way beyond any resources anybody will ever have,
[01:20:33.740 --> 01:20:35.900]   but then the Girdle machine could sort of improve it
[01:20:35.900 --> 01:20:37.780]   further and further in an exact way.
[01:20:37.780 --> 01:20:41.220]   - So is it theoretically possible
[01:20:41.220 --> 01:20:45.020]   that the Girdle machine process could improve?
[01:20:45.020 --> 01:20:51.900]   Isn't IXE already optimal?
[01:20:51.900 --> 01:20:56.820]   - It is optimal in terms of the revert collected
[01:20:56.820 --> 01:20:59.420]   over its interaction cycles,
[01:20:59.420 --> 01:21:03.540]   but it takes infinite time to produce one action.
[01:21:03.540 --> 01:21:07.220]   And the world continues whether you want it or not.
[01:21:07.220 --> 01:21:09.780]   So the model is, assuming you had an oracle
[01:21:09.780 --> 01:21:11.260]   which solved this problem,
[01:21:11.260 --> 01:21:12.980]   and then in the next 100 milliseconds
[01:21:12.980 --> 01:21:15.420]   or the reaction time you need gives the answer,
[01:21:15.420 --> 01:21:16.700]   then IXE is optimal.
[01:21:16.700 --> 01:21:18.660]   - Oh, so--
[01:21:18.660 --> 01:21:19.740]   - It's optimal in sense of data,
[01:21:19.740 --> 01:21:23.740]   also from learning efficiency and data efficiency,
[01:21:23.740 --> 01:21:25.660]   but not in terms of computation time.
[01:21:25.660 --> 01:21:27.580]   - And then the Girdle machine in theory,
[01:21:27.580 --> 01:21:31.020]   but probably not provably could make it go faster.
[01:21:31.020 --> 01:21:31.860]   - Yes.
[01:21:31.860 --> 01:21:32.700]   - Okay.
[01:21:32.700 --> 01:21:34.660]   Interesting.
[01:21:34.660 --> 01:21:36.660]   Those two components are super interesting.
[01:21:36.660 --> 01:21:39.360]   The sort of the perfect intelligence
[01:21:39.360 --> 01:21:42.900]   combined with self-improvement.
[01:21:42.900 --> 01:21:45.540]   Sort of provable self-improvement
[01:21:45.540 --> 01:21:48.780]   in the sense you're always getting the correct answer
[01:21:48.780 --> 01:21:50.340]   and you're improving.
[01:21:50.340 --> 01:21:51.400]   Beautiful ideas.
[01:21:51.400 --> 01:21:53.540]   Okay, so you've also mentioned
[01:21:53.540 --> 01:21:55.740]   that different kinds of things
[01:21:55.740 --> 01:21:59.860]   in the chase of solving this reward,
[01:21:59.860 --> 01:22:01.740]   sort of optimizing for the goal,
[01:22:01.740 --> 01:22:04.980]   interesting human things could emerge.
[01:22:04.980 --> 01:22:08.820]   So is there a place for consciousness within IXE?
[01:22:08.820 --> 01:22:13.500]   Where does, maybe you can comment,
[01:22:13.500 --> 01:22:16.380]   because I suppose we humans are just
[01:22:16.380 --> 01:22:18.260]   another instantiation of IXE agents
[01:22:18.260 --> 01:22:20.900]   and we seem to have consciousness.
[01:22:20.900 --> 01:22:23.420]   - You say humans are an instantiation of an IXE agent?
[01:22:23.420 --> 01:22:24.260]   - Yes.
[01:22:24.260 --> 01:22:25.260]   - Well, that would be amazing,
[01:22:25.260 --> 01:22:27.880]   but I think that's not really for the smartest
[01:22:27.880 --> 01:22:29.000]   and most rational humans.
[01:22:29.000 --> 01:22:32.920]   I think maybe we are very crude approximations.
[01:22:32.920 --> 01:22:33.760]   - Interesting.
[01:22:33.760 --> 01:22:35.740]   I mean, I tend to believe, again, I'm Russian,
[01:22:35.740 --> 01:22:40.740]   so I tend to believe our flaws are part of the optimal.
[01:22:40.740 --> 01:22:45.660]   So we tend to laugh off and criticize our flaws
[01:22:45.660 --> 01:22:49.260]   and I tend to think that that's actually close
[01:22:49.260 --> 01:22:50.700]   to an optimal behavior.
[01:22:50.700 --> 01:22:53.780]   But some flaws, if you think more carefully about it,
[01:22:53.780 --> 01:22:55.020]   are actually not flaws, yeah,
[01:22:55.020 --> 01:22:57.800]   but I think there are still enough flaws.
[01:22:57.800 --> 01:23:00.020]   - I don't know.
[01:23:00.020 --> 01:23:00.860]   It's unclear.
[01:23:00.860 --> 01:23:03.200]   As a student of history, I think all the suffering
[01:23:03.200 --> 01:23:06.820]   that we've endured as a civilization,
[01:23:06.820 --> 01:23:10.240]   it's possible that that's the optimal amount of suffering
[01:23:10.240 --> 01:23:13.840]   we need to endure to minimize long-term suffering.
[01:23:13.840 --> 01:23:17.300]   - That's your Russian background, I think.
[01:23:17.300 --> 01:23:18.140]   - That's the Russian,
[01:23:18.140 --> 01:23:21.900]   whether we humans are or not instantiations of an AIC agent,
[01:23:21.900 --> 01:23:25.700]   do you think consciousness is something that could emerge
[01:23:25.700 --> 01:23:28.620]   in a computational form of framework like AIC?
[01:23:28.620 --> 01:23:31.740]   - Let me also ask you a question.
[01:23:31.740 --> 01:23:33.100]   Do you think I'm conscious?
[01:23:33.100 --> 01:23:38.060]   - That's a good question.
[01:23:38.060 --> 01:23:44.300]   That tie is confusing me, but I think so.
[01:23:44.300 --> 01:23:45.780]   - You think that makes me unconscious
[01:23:45.780 --> 01:23:47.180]   because it strangles me?
[01:23:47.180 --> 01:23:49.780]   - If an agent were to solve the imitation game
[01:23:49.780 --> 01:23:51.740]   posed by Turing, I think they would be dressed
[01:23:51.740 --> 01:23:56.740]   similarly to you, because there's a kind of flamboyant,
[01:23:56.740 --> 01:24:01.060]   interesting, complex behavior pattern
[01:24:01.060 --> 01:24:04.500]   that sells that you're human and you're conscious.
[01:24:04.500 --> 01:24:06.140]   But why do you ask?
[01:24:06.140 --> 01:24:07.940]   - Was it a yes or was it a no?
[01:24:07.940 --> 01:24:08.780]   - Yes, I think you're--
[01:24:08.780 --> 01:24:10.300]   - Yes. (laughs)
[01:24:10.300 --> 01:24:12.220]   - I think you're conscious, yes.
[01:24:12.220 --> 01:24:16.140]   - Yeah, and you explain somehow why.
[01:24:16.140 --> 01:24:18.660]   But you infer that from my behavior, right?
[01:24:18.660 --> 01:24:19.500]   - Yes.
[01:24:19.500 --> 01:24:20.740]   - You can never be sure about that.
[01:24:20.740 --> 01:24:23.340]   And I think the same thing will happen
[01:24:23.340 --> 01:24:26.780]   with any intelligent agent we develop
[01:24:26.780 --> 01:24:31.060]   if it behaves in a way sufficiently close to humans,
[01:24:31.060 --> 01:24:32.100]   or maybe even not humans.
[01:24:32.100 --> 01:24:34.260]   I mean, maybe a dog is also sometimes
[01:24:34.260 --> 01:24:36.500]   a little bit self-conscious, right?
[01:24:36.500 --> 01:24:39.940]   So if it behaves in a way where we attribute
[01:24:39.940 --> 01:24:42.780]   typically consciousness, we would attribute consciousness
[01:24:42.780 --> 01:24:45.340]   to these intelligent systems and, you know,
[01:24:45.340 --> 01:24:47.300]   I see probably in particular.
[01:24:47.300 --> 01:24:48.860]   That, of course, doesn't answer the question
[01:24:48.860 --> 01:24:50.860]   whether it's really conscious.
[01:24:50.860 --> 01:24:53.540]   And that's the big, hard problem of consciousness.
[01:24:53.540 --> 01:24:55.700]   You know, maybe I'm a zombie.
[01:24:55.700 --> 01:24:59.360]   I mean, not the movie zombie, but the philosophical zombie.
[01:24:59.360 --> 01:25:02.620]   - Is, to you, the display of consciousness
[01:25:02.620 --> 01:25:06.740]   close enough to consciousness from a perspective of AGI
[01:25:06.740 --> 01:25:09.780]   that the distinction of the hard problem of consciousness
[01:25:09.780 --> 01:25:11.340]   is not an interesting one?
[01:25:11.340 --> 01:25:12.500]   - I think we don't have to worry
[01:25:12.500 --> 01:25:13.980]   about the consciousness problem,
[01:25:13.980 --> 01:25:16.860]   especially the hard problem for developing AGI.
[01:25:16.860 --> 01:25:20.220]   I think, you know, we progress.
[01:25:20.220 --> 01:25:21.380]   At some point we have, you know,
[01:25:21.380 --> 01:25:23.340]   solved all the technical problems
[01:25:23.340 --> 01:25:25.460]   and this system will behave intelligent
[01:25:25.460 --> 01:25:26.540]   and then super intelligent
[01:25:26.540 --> 01:25:30.220]   and this consciousness will emerge.
[01:25:30.220 --> 01:25:32.540]   I mean, definitely it will display behavior
[01:25:32.540 --> 01:25:35.100]   which we will interpret as conscious.
[01:25:35.100 --> 01:25:38.180]   And then it's a philosophical question.
[01:25:38.180 --> 01:25:39.900]   Did this consciousness really emerge
[01:25:39.900 --> 01:25:43.740]   or is it a zombie which just, you know, fakes everything?
[01:25:43.740 --> 01:25:45.260]   We still don't have to figure that out,
[01:25:45.260 --> 01:25:47.540]   although it may be interesting,
[01:25:47.540 --> 01:25:48.980]   at least from a philosophical point of view,
[01:25:48.980 --> 01:25:49.900]   it's very interesting,
[01:25:49.900 --> 01:25:53.220]   but it may also be sort of practically interesting.
[01:25:53.220 --> 01:25:54.300]   You know, there's some people saying,
[01:25:54.300 --> 01:25:56.260]   if it's just faking consciousness and feelings,
[01:25:56.260 --> 01:25:59.180]   you know, then we don't need to be concerned about rights.
[01:25:59.180 --> 01:26:01.660]   But if it's real conscious and has feelings,
[01:26:01.660 --> 01:26:03.460]   then we need to be concerned, yeah.
[01:26:03.460 --> 01:26:07.580]   - I can't wait till the day
[01:26:07.580 --> 01:26:10.700]   where AI systems exhibit consciousness
[01:26:10.700 --> 01:26:12.460]   because it'll truly be
[01:26:12.460 --> 01:26:14.580]   some of the hardest ethical questions
[01:26:14.580 --> 01:26:15.700]   of what we do with that.
[01:26:15.700 --> 01:26:18.940]   - It is rather easy to build systems
[01:26:18.940 --> 01:26:21.140]   which people ascribe consciousness.
[01:26:21.140 --> 01:26:22.660]   And I give you an analogy.
[01:26:22.660 --> 01:26:25.380]   I mean, remember, maybe it was before you were born,
[01:26:25.380 --> 01:26:26.380]   the Tamagotchi.
[01:26:26.380 --> 01:26:29.860]   - How dare you, sir?
[01:26:29.860 --> 01:26:31.980]   - Why, that's the,
[01:26:31.980 --> 01:26:33.300]   yeah, but you're young, right?
[01:26:33.300 --> 01:26:34.980]   - Yes, it's good to think, yeah, thank you.
[01:26:34.980 --> 01:26:36.220]   Thank you very much.
[01:26:36.220 --> 01:26:37.580]   But I was also in the Soviet Union.
[01:26:37.580 --> 01:26:41.260]   We didn't have any of those fun things.
[01:26:41.260 --> 01:26:42.700]   But you have heard about this Tamagotchi,
[01:26:42.700 --> 01:26:44.620]   which was really, really primitive,
[01:26:44.620 --> 01:26:46.980]   actually for the time it was,
[01:26:46.980 --> 01:26:48.860]   and you could raise this,
[01:26:48.860 --> 01:26:51.660]   and kids got so attached to it
[01:26:51.660 --> 01:26:53.620]   and didn't want to let it die.
[01:26:53.620 --> 01:26:55.660]   And I would have probably,
[01:26:55.660 --> 01:26:57.460]   if we would have asked the children,
[01:26:57.460 --> 01:26:59.540]   do you think this Tamagotchi is conscious?
[01:26:59.540 --> 01:27:00.380]   - They would have said yes.
[01:27:00.380 --> 01:27:01.660]   - Probably would have said yes, I would guess.
[01:27:01.660 --> 01:27:04.780]   - I think that's kind of a beautiful thing, actually,
[01:27:04.780 --> 01:27:08.700]   'cause that consciousness, ascribing consciousness,
[01:27:08.700 --> 01:27:10.500]   seems to create a deeper connection.
[01:27:10.500 --> 01:27:11.340]   - Yep.
[01:27:11.340 --> 01:27:12.660]   - Which is a powerful thing,
[01:27:12.660 --> 01:27:15.940]   but we have to be careful on the ethics side of that.
[01:27:15.940 --> 01:27:18.500]   Well, let me ask about the AGI community broadly.
[01:27:18.500 --> 01:27:22.740]   You kind of represent some of the most serious work on AGI,
[01:27:22.740 --> 01:27:24.260]   at least earlier,
[01:27:24.260 --> 01:27:29.260]   and DeepMind represents serious work on AGI these days.
[01:27:29.260 --> 01:27:34.100]   But why, in your sense, is the AGI community so small,
[01:27:34.100 --> 01:27:38.100]   or has been so small, until maybe DeepMind came along?
[01:27:38.100 --> 01:27:41.660]   Like, why aren't more people seriously working
[01:27:41.660 --> 01:27:45.860]   on human-level and superhuman-level intelligence
[01:27:45.860 --> 01:27:47.380]   from a formal perspective?
[01:27:47.380 --> 01:27:49.660]   - Okay, from a formal perspective,
[01:27:49.660 --> 01:27:52.540]   that's sort of an extra point.
[01:27:52.540 --> 01:27:54.940]   So I think there are a couple of reasons.
[01:27:54.940 --> 01:27:56.660]   I mean, AI came in waves, right?
[01:27:56.660 --> 01:27:58.500]   You know, AI winters and AI summers,
[01:27:58.500 --> 01:28:01.500]   and then there were big promises which were not fulfilled.
[01:28:01.500 --> 01:28:05.740]   And people got disappointed,
[01:28:05.740 --> 01:28:10.740]   and that narrow AI, solving particular problems
[01:28:10.740 --> 01:28:14.020]   which seemed to require intelligence,
[01:28:14.020 --> 01:28:17.020]   was always, to some extent, successful,
[01:28:17.020 --> 01:28:19.500]   and there were improvements, small steps.
[01:28:19.500 --> 01:28:22.060]   And if you build something which is, you know,
[01:28:22.060 --> 01:28:25.140]   useful for society or industrially useful,
[01:28:25.140 --> 01:28:26.620]   then there's a lot of funding.
[01:28:26.620 --> 01:28:28.540]   So I guess it was in parts the money,
[01:28:28.540 --> 01:28:34.220]   which drives people to develop specific systems,
[01:28:34.220 --> 01:28:36.260]   solving specific tasks.
[01:28:36.260 --> 01:28:37.380]   But you would think that, you know,
[01:28:37.380 --> 01:28:38.780]   at least in university,
[01:28:38.780 --> 01:28:42.860]   you should be able to do ivory tower research.
[01:28:42.860 --> 01:28:46.020]   And that was probably better a long time ago,
[01:28:46.020 --> 01:28:48.340]   but even nowadays, there's quite some pressure
[01:28:48.340 --> 01:28:52.260]   of doing applied research or translational research,
[01:28:52.260 --> 01:28:56.700]   and, you know, it's harder to get grants as a theorist.
[01:28:56.700 --> 01:28:59.940]   So that also drives people away.
[01:28:59.940 --> 01:29:01.540]   It's maybe also harder,
[01:29:01.540 --> 01:29:03.180]   attacking the general intelligence problem.
[01:29:03.180 --> 01:29:05.940]   So I think enough people, I mean, maybe a small number,
[01:29:05.940 --> 01:29:09.620]   were still interested in formalizing intelligence
[01:29:09.620 --> 01:29:12.940]   and thinking of general intelligence,
[01:29:12.940 --> 01:29:17.620]   but, you know, not much came up, right?
[01:29:17.620 --> 01:29:19.900]   Or not much great stuff came up.
[01:29:19.900 --> 01:29:21.380]   - So what do you think,
[01:29:21.380 --> 01:29:24.860]   we talked about the formal big light
[01:29:24.860 --> 01:29:26.180]   at the end of the tunnel,
[01:29:26.180 --> 01:29:27.620]   but from the engineering perspective,
[01:29:27.620 --> 01:29:30.380]   what do you think it takes to build an AGI system?
[01:29:30.380 --> 01:29:33.900]   Is it, and I don't know if that's a stupid question
[01:29:33.900 --> 01:29:35.100]   or a distinct question
[01:29:35.100 --> 01:29:37.100]   from everything we've been talking about at IAXE,
[01:29:37.100 --> 01:29:39.140]   but what do you see as the steps
[01:29:39.140 --> 01:29:41.020]   that are necessary to take
[01:29:41.020 --> 01:29:43.020]   to start to try to build something?
[01:29:43.020 --> 01:29:44.340]   - So you want a blueprint now,
[01:29:44.340 --> 01:29:46.340]   and then you go off and do it?
[01:29:46.340 --> 01:29:48.020]   - That's the whole point of this conversation,
[01:29:48.020 --> 01:29:49.780]   I'm trying to squeeze that in there.
[01:29:49.780 --> 01:29:51.580]   Now, is there, I mean, what's your intuition?
[01:29:51.580 --> 01:29:53.980]   Is it in the robotics space
[01:29:53.980 --> 01:29:55.340]   or something that has a body
[01:29:55.340 --> 01:29:56.820]   and tries to explore the world?
[01:29:56.820 --> 01:29:58.940]   Is it in the reinforcement learning space,
[01:29:58.940 --> 01:30:00.980]   like the efforts with AlphaZero and AlphaStar
[01:30:00.980 --> 01:30:02.860]   that are kind of exploring
[01:30:02.860 --> 01:30:05.500]   how you can solve it through in the simulation,
[01:30:05.500 --> 01:30:06.740]   in the gaming world?
[01:30:06.740 --> 01:30:08.220]   Is there stuff in sort of
[01:30:08.220 --> 01:30:13.220]   all the transformer work in natural language processing,
[01:30:13.220 --> 01:30:15.780]   sort of maybe attacking the open domain dialogue?
[01:30:15.780 --> 01:30:18.700]   Like what, where do you see the promising pathways?
[01:30:18.700 --> 01:30:24.540]   - Let me pick the embodiment maybe.
[01:30:24.540 --> 01:30:29.540]   So, embodiment is important, yes and no.
[01:30:29.540 --> 01:30:38.180]   I don't believe that we need a physical robot
[01:30:38.180 --> 01:30:40.260]   walking or rolling around,
[01:30:40.260 --> 01:30:45.100]   interacting with the real world in order to achieve AGI.
[01:30:45.100 --> 01:30:50.100]   And I think it's more of a distraction
[01:30:50.100 --> 01:30:51.380]   probably than helpful.
[01:30:51.380 --> 01:30:54.580]   It's sort of confusing the body with the mind.
[01:30:54.580 --> 01:30:58.940]   For industrial applications or near-term applications,
[01:30:58.940 --> 01:31:01.180]   of course we need robots for all kinds of things,
[01:31:01.180 --> 01:31:04.140]   but for solving the big problem,
[01:31:04.140 --> 01:31:08.140]   at least at this stage, I think it's not necessary.
[01:31:08.140 --> 01:31:10.100]   But the answer is also yes,
[01:31:10.100 --> 01:31:13.220]   that I think the most promising approach
[01:31:13.220 --> 01:31:15.300]   is that you have an agent
[01:31:15.300 --> 01:31:18.500]   and that can be a virtual agent in a computer
[01:31:18.500 --> 01:31:20.140]   interacting with an environment,
[01:31:20.140 --> 01:31:22.580]   possibly a 3D simulated environment
[01:31:22.580 --> 01:31:24.140]   like in many computer games.
[01:31:24.140 --> 01:31:28.900]   And you train and learn the agent.
[01:31:28.900 --> 01:31:33.140]   Even if you don't intend to later put it sort of,
[01:31:33.140 --> 01:31:35.580]   this algorithm in a robot brain
[01:31:35.580 --> 01:31:38.580]   and leave it forever in the virtual reality,
[01:31:38.580 --> 01:31:40.540]   getting experience in a,
[01:31:40.540 --> 01:31:43.740]   although it's just simulated 3D world,
[01:31:43.740 --> 01:31:48.020]   is possibly, and I say possibly,
[01:31:48.020 --> 01:31:51.700]   important to understand things
[01:31:51.700 --> 01:31:53.980]   on a similar level as humans do,
[01:31:53.980 --> 01:31:56.340]   especially if the agent,
[01:31:56.340 --> 01:31:59.540]   or primarily if the agent needs to interact
[01:31:59.540 --> 01:32:00.380]   with the humans, right?
[01:32:00.380 --> 01:32:01.620]   You know, if you talk about objects
[01:32:01.620 --> 01:32:04.820]   on top of each other in space and flying in cars and so on,
[01:32:04.820 --> 01:32:06.500]   and the agent has no experience
[01:32:06.500 --> 01:32:09.660]   with even virtual 3D worlds,
[01:32:09.660 --> 01:32:11.220]   it's probably hard to grasp.
[01:32:11.220 --> 01:32:14.620]   So if you develop an abstract agent,
[01:32:14.620 --> 01:32:16.820]   say we take the mathematical path
[01:32:16.820 --> 01:32:18.420]   and we just want to build an agent
[01:32:18.420 --> 01:32:19.580]   which can prove theorems
[01:32:19.580 --> 01:32:21.860]   and becomes a better and better mathematician,
[01:32:21.860 --> 01:32:23.860]   then this agent needs to be able
[01:32:23.860 --> 01:32:26.020]   to reason in very abstract spaces
[01:32:26.020 --> 01:32:27.780]   and then maybe sort of putting it
[01:32:27.780 --> 01:32:29.500]   into 3D environment simulated
[01:32:29.500 --> 01:32:30.540]   or it is even harmful.
[01:32:30.540 --> 01:32:32.740]   It should sort of, you put it in,
[01:32:32.740 --> 01:32:34.060]   I don't know, an environment
[01:32:34.060 --> 01:32:35.900]   which it creates itself or so.
[01:32:35.900 --> 01:32:38.500]   - It seems like you have a interesting,
[01:32:38.500 --> 01:32:40.740]   rich, complex trajectory through life
[01:32:40.740 --> 01:32:42.740]   in terms of your journey of ideas.
[01:32:42.740 --> 01:32:45.860]   So it's interesting to ask what books,
[01:32:45.860 --> 01:32:49.580]   technical fiction, philosophical books,
[01:32:49.580 --> 01:32:52.780]   ideas, people had a transformative effect.
[01:32:52.780 --> 01:32:53.900]   Books are most interesting
[01:32:53.900 --> 01:32:57.340]   'cause maybe people could also read those books
[01:32:57.340 --> 01:33:00.180]   and see if they could be inspired as well.
[01:33:00.180 --> 01:33:03.580]   - Yeah, luckily I asked books and not singular book.
[01:33:03.580 --> 01:33:08.180]   It's very hard and I tried to pin down one book.
[01:33:08.180 --> 01:33:10.620]   And I can do that at the end.
[01:33:10.620 --> 01:33:15.620]   So the most, the books which were most transformative
[01:33:16.020 --> 01:33:19.660]   for me or which I can most highly recommend
[01:33:19.660 --> 01:33:21.980]   to people interested in AI.
[01:33:21.980 --> 01:33:22.940]   - Both perhaps.
[01:33:22.940 --> 01:33:25.500]   - Yeah, yeah, both, yeah, yeah.
[01:33:25.500 --> 01:33:28.620]   I would always start with Russell and Norbeck,
[01:33:28.620 --> 01:33:30.940]   Artificial Intelligence, A Modern Approach.
[01:33:30.940 --> 01:33:33.460]   That's the AI Bible.
[01:33:33.460 --> 01:33:35.020]   It's an amazing book.
[01:33:35.020 --> 01:33:36.380]   It's very broad.
[01:33:36.380 --> 01:33:38.900]   It covers all approaches to AI.
[01:33:38.900 --> 01:33:40.900]   And even if you focus on one approach,
[01:33:40.900 --> 01:33:42.580]   I think that is the minimum you should know
[01:33:42.580 --> 01:33:44.660]   about the other approaches out there.
[01:33:44.660 --> 01:33:46.260]   So that should be your first book.
[01:33:46.260 --> 01:33:48.380]   - Fourth edition should be coming out soon.
[01:33:48.380 --> 01:33:50.100]   - Oh, okay, interesting.
[01:33:50.100 --> 01:33:51.580]   - There's a deep learning chapter now,
[01:33:51.580 --> 01:33:53.140]   so there must be.
[01:33:53.140 --> 01:33:55.620]   Written by Ian Goodfellow, okay.
[01:33:55.620 --> 01:33:59.740]   - And then the next book I would recommend,
[01:33:59.740 --> 01:34:02.980]   the Reinforcement Learning Book by Sutton and Bartow.
[01:34:02.980 --> 01:34:04.500]   That's a beautiful book.
[01:34:04.500 --> 01:34:07.940]   If there's any problem with the book,
[01:34:07.940 --> 01:34:12.940]   it makes RL feel and look much easier than it actually is.
[01:34:13.820 --> 01:34:15.660]   It's a very gentle book.
[01:34:15.660 --> 01:34:17.620]   It's very nice to read, the exercises to do.
[01:34:17.620 --> 01:34:20.380]   You can very quickly get some RL systems to run,
[01:34:20.380 --> 01:34:23.300]   you know, on very toy problems, but it's a lot of fun.
[01:34:23.300 --> 01:34:28.300]   And in a couple of days, you feel you know what RL is about,
[01:34:28.300 --> 01:34:31.460]   but it's much harder than the book.
[01:34:31.460 --> 01:34:34.860]   - Come on now, it's an awesome book.
[01:34:34.860 --> 01:34:36.260]   - Yeah, no, it is, yeah.
[01:34:36.260 --> 01:34:41.260]   And maybe, I mean, there's so many books out there.
[01:34:41.260 --> 01:34:43.460]   If you like the information theoretic approach,
[01:34:43.460 --> 01:34:46.780]   then there's "Colmogorov Complexity" by Leon Bitani,
[01:34:46.780 --> 01:34:50.820]   but probably, you know, some short article is enough.
[01:34:50.820 --> 01:34:52.140]   You don't need to read the whole book,
[01:34:52.140 --> 01:34:54.420]   but it's a great book.
[01:34:54.420 --> 01:34:59.420]   And if you have to mention one all-time favorite book,
[01:34:59.420 --> 01:35:03.220]   so different flavor, that's a book which is used
[01:35:03.220 --> 01:35:05.220]   in the International Baccalaureate
[01:35:05.220 --> 01:35:09.020]   for high school students in several countries.
[01:35:09.020 --> 01:35:12.940]   That's from Nicholas Alchen, "Theory of Knowledge."
[01:35:12.940 --> 01:35:16.580]   Second edition or first, not the third, please.
[01:35:16.580 --> 01:35:18.900]   The third one, they took out all the fun.
[01:35:18.900 --> 01:35:25.620]   So this asks all the interesting,
[01:35:25.620 --> 01:35:27.620]   or to me, interesting philosophical questions
[01:35:27.620 --> 01:35:29.940]   about how we acquire knowledge from all perspectives,
[01:35:29.940 --> 01:35:32.100]   you know, from math, from art, from physics,
[01:35:32.100 --> 01:35:36.220]   and ask how can we know anything?
[01:35:36.220 --> 01:35:38.020]   And the book is called "Theory of Knowledge."
[01:35:38.020 --> 01:35:40.700]   - From which, it's almost like a philosophical exploration
[01:35:40.700 --> 01:35:43.140]   of how we get knowledge from anything.
[01:35:43.140 --> 01:35:45.140]   - Yes, yeah, I mean, can religion tell us, you know,
[01:35:45.140 --> 01:35:46.180]   about something about the world?
[01:35:46.180 --> 01:35:48.060]   Can science tell us something about the world?
[01:35:48.060 --> 01:35:50.740]   Can mathematics, or is it just playing with symbols?
[01:35:50.740 --> 01:35:54.380]   And, you know, it's open-ended questions,
[01:35:54.380 --> 01:35:56.220]   and I mean, it's for high school students,
[01:35:56.220 --> 01:35:58.300]   so they have the resources from "Hitchhiker's Guide
[01:35:58.300 --> 01:35:59.940]   to the Galaxy" and from "Star Wars"
[01:35:59.940 --> 01:36:01.780]   and "The Chicken Crossed the Road," yeah?
[01:36:01.780 --> 01:36:05.940]   And it's fun to read, but it's also quite deep.
[01:36:07.580 --> 01:36:11.460]   - If you could live one day of your life over again,
[01:36:11.460 --> 01:36:12.780]   does it make you truly happy,
[01:36:12.780 --> 01:36:14.380]   or maybe like we said with the books,
[01:36:14.380 --> 01:36:16.180]   it was truly transformative.
[01:36:16.180 --> 01:36:19.060]   What day, what moment would you choose?
[01:36:19.060 --> 01:36:20.780]   Does something pop into your mind?
[01:36:20.780 --> 01:36:23.420]   - Does it need to be a day in the past,
[01:36:23.420 --> 01:36:25.860]   or can it be a day in the future?
[01:36:25.860 --> 01:36:27.900]   - Well, space-time is an emergent phenomena,
[01:36:27.900 --> 01:36:30.340]   so it's all the same anyway.
[01:36:30.340 --> 01:36:31.180]   - Okay.
[01:36:31.180 --> 01:36:33.340]   Okay, from the past.
[01:36:33.340 --> 01:36:36.740]   - You're really gonna say from the future, I love it.
[01:36:36.740 --> 01:36:38.300]   No, I will tell you from the future, yeah?
[01:36:38.300 --> 01:36:39.140]   - Okay, from the past.
[01:36:39.140 --> 01:36:41.380]   - So from the past, I would say
[01:36:41.380 --> 01:36:43.700]   when I discovered my axiom model.
[01:36:43.700 --> 01:36:45.100]   I mean, it was not in one day,
[01:36:45.100 --> 01:36:48.780]   but it was one moment where I realized
[01:36:48.780 --> 01:36:49.900]   Kolmogorov complexity,
[01:36:49.900 --> 01:36:53.100]   I didn't even know that it existed,
[01:36:53.100 --> 01:36:56.220]   but I discovered sort of this compression idea myself,
[01:36:56.220 --> 01:36:58.020]   but immediately I knew I can't be the first one,
[01:36:58.020 --> 01:36:59.220]   but I had this idea,
[01:36:59.220 --> 01:37:02.100]   and then I knew about sequential decision tree,
[01:37:02.100 --> 01:37:06.260]   and I knew if I put it together, this is the right thing.
[01:37:06.260 --> 01:37:09.580]   And yeah, still when I think back about this moment,
[01:37:09.580 --> 01:37:12.300]   I'm super excited about it.
[01:37:12.300 --> 01:37:16.220]   - Was there any more details and context that moment?
[01:37:16.220 --> 01:37:17.900]   Did an apple fall on your head?
[01:37:17.900 --> 01:37:23.980]   So like if you look at Ian Goodfellow talking about GANs,
[01:37:23.980 --> 01:37:25.820]   there was beer involved.
[01:37:25.820 --> 01:37:30.060]   Is there some more context of what sparked your thought,
[01:37:30.060 --> 01:37:31.060]   or was it just--
[01:37:31.060 --> 01:37:32.820]   - No, it was much more mundane.
[01:37:32.820 --> 01:37:34.460]   So I worked in this company,
[01:37:34.460 --> 01:37:36.020]   so in this sense, the four and a half years
[01:37:36.020 --> 01:37:37.500]   was not completely wasted.
[01:37:37.500 --> 01:37:43.620]   And I worked on an image interpolation problem,
[01:37:43.620 --> 01:37:48.340]   and I developed a quite neat new interpolation techniques,
[01:37:48.340 --> 01:37:49.380]   and they got patented,
[01:37:49.380 --> 01:37:52.100]   and then which happens quite often,
[01:37:52.100 --> 01:37:54.260]   I got sort of overboard and thought about,
[01:37:54.260 --> 01:37:56.100]   yeah, that's pretty good, but it's not the best,
[01:37:56.100 --> 01:37:59.660]   so what is the best possible way of doing interpolation?
[01:37:59.660 --> 01:38:03.060]   And then I thought, yeah, you want the simplest picture,
[01:38:03.060 --> 01:38:04.620]   which if you core screen it,
[01:38:04.620 --> 01:38:06.420]   recovers your original picture,
[01:38:06.420 --> 01:38:08.740]   and then I thought about the simplicity concept
[01:38:08.740 --> 01:38:11.140]   more in quantitative terms,
[01:38:11.140 --> 01:38:13.940]   and yeah, then everything developed.
[01:38:13.940 --> 01:38:16.940]   - And somehow the full beautiful mix
[01:38:16.940 --> 01:38:18.780]   of also being a physicist
[01:38:18.780 --> 01:38:20.500]   and thinking about the big picture of it
[01:38:20.500 --> 01:38:22.300]   then led you to probably--
[01:38:22.300 --> 01:38:25.100]   - Yeah, yeah, so as a physicist,
[01:38:25.100 --> 01:38:26.900]   I was probably trained not to always think
[01:38:26.900 --> 01:38:29.220]   in computational terms, just ignore that
[01:38:29.220 --> 01:38:32.220]   and think about the fundamental properties
[01:38:32.220 --> 01:38:34.060]   which you want to have.
[01:38:34.060 --> 01:38:36.980]   - So what about if you could really one day in the future,
[01:38:36.980 --> 01:38:39.940]   what would that be?
[01:38:39.940 --> 01:38:41.540]   - When I solve the AGI problem?
[01:38:41.540 --> 01:38:44.580]   - I don't think-- - In practice, in practice,
[01:38:44.580 --> 01:38:46.460]   so in theory I have solved it with the Ix-A model,
[01:38:46.460 --> 01:38:47.700]   but in practice. - Yes.
[01:38:47.700 --> 01:38:50.780]   - And then I ask the first question.
[01:38:50.780 --> 01:38:53.300]   - What would be the first question?
[01:38:53.300 --> 01:38:54.740]   - What's the meaning of life?
[01:38:54.740 --> 01:38:58.500]   - I don't think there's a better way to end it.
[01:38:58.500 --> 01:38:59.340]   Thank you so much for talking today,
[01:38:59.340 --> 01:39:01.420]   it's a huge honor to finally meet you.
[01:39:01.420 --> 01:39:04.620]   - Yeah, thank you too, it was a pleasure of mine, too.
[01:39:04.620 --> 01:39:06.300]   - Thanks for listening to this conversation
[01:39:06.300 --> 01:39:07.420]   with Marcus Hutter,
[01:39:07.420 --> 01:39:10.060]   and thank you to our presenting sponsor, Cash App.
[01:39:10.060 --> 01:39:12.620]   Download it, use code LEXPODCAST,
[01:39:12.620 --> 01:39:15.500]   you'll get $10, and $10 will go to FIRST,
[01:39:15.500 --> 01:39:18.560]   an organization that inspires and educates young minds
[01:39:18.560 --> 01:39:22.340]   to become science and technology innovators of tomorrow.
[01:39:22.340 --> 01:39:25.140]   If you enjoy this podcast, subscribe on YouTube,
[01:39:25.140 --> 01:39:27.020]   get five stars on Apple Podcasts,
[01:39:27.020 --> 01:39:30.420]   support on Patreon, or simply connect with me on Twitter
[01:39:30.420 --> 01:39:32.040]   at Lex Friedman.
[01:39:32.040 --> 01:39:35.740]   And now, let me leave you with some words of wisdom
[01:39:35.740 --> 01:39:37.900]   from Albert Einstein.
[01:39:37.900 --> 01:39:42.120]   "The measure of intelligence is the ability to change."
[01:39:42.120 --> 01:39:46.500]   Thank you for listening, and hope to see you next time.
[01:39:46.500 --> 01:39:49.080]   (upbeat music)
[01:39:49.080 --> 01:39:51.660]   (upbeat music)
[01:39:51.660 --> 01:40:01.660]   [BLANK_AUDIO]

