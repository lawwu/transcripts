
[00:00:00.000 --> 00:00:05.760]   Hey everyone, Ivan here, and in this video we're looking at YOLOv5 and Weights & Biases integration.
[00:00:05.760 --> 00:00:12.400]   For some context, YOLOv5 is a popular repository by Ultralytics for training YOLO-type
[00:00:12.400 --> 00:00:18.080]   single-shot object detectors in PyTorch, and Weights & Biases is a machine learning tools platform.
[00:00:18.080 --> 00:00:23.200]   We'll use Weights & Biases throughout this video to interactively track
[00:00:23.200 --> 00:00:28.160]   training metrics, bounding boxes predictions from epoch to epoch, and even our datasets.
[00:00:28.720 --> 00:00:32.880]   We'll also use WNB to automatically save and version model weights checkpoints,
[00:00:32.880 --> 00:00:37.040]   which allows us to stop and resume training on any device at any point.
[00:00:37.040 --> 00:00:43.440]   Right now we're looking at a WNB report with the corresponding Colab notebook. Links for both will
[00:00:43.440 --> 00:00:49.120]   be in the video description down below, so feel free to follow along. If you have any questions
[00:00:49.120 --> 00:00:53.520]   at any point throughout this video, I strongly encourage you to drop them in the comment
[00:00:53.520 --> 00:00:59.920]   section down below. And let's get started! Let's click the first cell to clone the YOLOv5 repo
[00:00:59.920 --> 00:01:06.960]   and download today's chess pieces dataset. This cell starts the training and will prompt us to
[00:01:06.960 --> 00:01:12.320]   log into our Weights & Biases accounts, and since I already have one, I'll select the second option
[00:01:12.320 --> 00:01:17.120]   and paste my API key. You can also select the first option and quickly create an account,
[00:01:17.120 --> 00:01:22.640]   and then follow along with the rest of the video. Now that we've logged in, let's click on this link
[00:01:22.640 --> 00:01:28.880]   to open the project page. This is a Weights & Biases dashboard for a project that we named
[00:01:28.880 --> 00:01:36.560]   YOLO1DB_DEMO. Here we can compare all the different runs or training sessions of different
[00:01:36.560 --> 00:01:42.080]   configurations of our models all in one place. Including, of course, the training session that
[00:01:42.080 --> 00:01:48.960]   we've just started. The first really interesting thing to look at here is the bounding_box debugger.
[00:01:48.960 --> 00:01:52.880]   It's a chart that gives us an intuition for what the different training runs of our models
[00:01:52.880 --> 00:01:57.360]   are actually learning by letting us compare their predictions side by side
[00:01:57.360 --> 00:02:00.320]   on 16 random images from the validation dataset.
[00:02:00.320 --> 00:02:07.520]   The bounding_box interval is a parameter that determines how often we log these predictions.
[00:02:07.520 --> 00:02:12.000]   In this case, it's set to 1, meaning that we log every epoch.
[00:02:12.000 --> 00:02:16.880]   The most notable thing about this chart is that it's interactive.
[00:02:18.720 --> 00:02:22.640]   We can select different images to look at, adjust confidence threshold,
[00:02:22.640 --> 00:02:26.480]   or choose to display or not display certain classes.
[00:02:26.480 --> 00:02:40.880]   In our dashboard, we can also find various useful metrics that are also interactive,
[00:02:40.880 --> 00:02:46.640]   like mean average precision or validation loss. We can even track GPU usage and see whether we're
[00:02:46.640 --> 00:02:49.520]   utilizing our hardware to its full potential.
[00:02:49.520 --> 00:02:57.840]   The coolest thing about being able to compare runs is that weights and biases also track the
[00:02:57.840 --> 00:03:04.320]   hyperparameters for each of them. We can see which models perform best on which hyperparameters they
[00:03:04.320 --> 00:03:11.600]   use to do so. But what if something really bad happens while training another YOLOv5?
[00:03:14.160 --> 00:03:20.560]   Your internet gets disconnected, or a VM session crashes, and when training models for many hours
[00:03:20.560 --> 00:03:25.600]   or even days, losing progress can be very annoying and very expensive.
[00:03:25.600 --> 00:03:29.120]   So, has our progress been wasted?
[00:03:29.120 --> 00:03:36.320]   It hasn't. Remember how we passed the save_period and upload_dataset parameters?
[00:03:36.320 --> 00:03:41.040]   Upload_dataset asks us whether we want to upload our dataset in parallel
[00:03:41.040 --> 00:03:46.240]   with training our model, and save_period asks us once every how many epochs we would like to save
[00:03:46.240 --> 00:03:51.840]   our checkpoint weights. Both are uploaded and logged as weights and biases artifacts.
[00:03:51.840 --> 00:04:00.160]   Think of an artifact as a versioned folder of data. Let's say we have five images in our dataset.
[00:04:00.160 --> 00:04:07.040]   We can log them as an artifact named "dataset". We then change two images to be, say, "grayscale"
[00:04:07.040 --> 00:04:12.160]   and log the artifact again with the same name the system scans every bit of our image files.
[00:04:12.160 --> 00:04:16.560]   If we detect a difference, even if the image file names are the same,
[00:04:16.560 --> 00:04:22.160]   it logs this artifact as the newer version. Most importantly, the new artifact only saves
[00:04:22.160 --> 00:04:27.600]   the changed images and just references the unchanged ones, meaning that we're not wasting
[00:04:27.600 --> 00:04:35.680]   any storage. So, that's artifacts in a nutshell. With weights and biases, we get 100 gigabytes of
[00:04:35.680 --> 00:04:40.400]   free artifact storage, and here's how it can help us resume the crashed run.
[00:04:40.400 --> 00:04:44.880]   There are three types of artifacts logged in this YOLOv5 integration.
[00:04:44.880 --> 00:04:49.120]   Datasets, model checkpoints, and evaluation artifacts.
[00:04:49.120 --> 00:04:55.600]   We have the model checkpoints and our dataset with a little bit of 1db magic. Nothing is stopping us
[00:04:55.600 --> 00:04:59.520]   from being able to resume training on any device that's connected to the internet.
[00:05:00.480 --> 00:05:06.800]   To do so, let's run train.py resume 1db_artifact. We need to paste the link to the run that we want
[00:05:06.800 --> 00:05:12.080]   to resume, which we can get by clicking on it, going into overview, and copying it there.
[00:05:12.080 --> 00:05:29.440]   Now just look at it. Training continues from the very epoch it left off.
[00:05:29.440 --> 00:05:34.400]   Since we're also executing the same run, the charts also continue to log new information
[00:05:34.400 --> 00:05:38.400]   from where they left off, and no experiment tracking information was lost.
[00:05:38.400 --> 00:05:44.400]   This was how to resume training on any device connected to the internet.
[00:05:44.400 --> 00:05:53.840]   There are also options of resuming runs if your datasets, models, or even both are stored locally.
[00:05:53.840 --> 00:05:59.120]   There's another huge perk to uploading datasets as artifacts.
[00:05:59.760 --> 00:06:06.640]   1db_tables is a way of visualizing, querying, and filtering entire datasets right in the browser.
[00:06:06.640 --> 00:06:15.440]   In our dashboard, we can go into the artifacts tab, click on dataset, select the version we
[00:06:15.440 --> 00:06:21.120]   want to look at, go to files, click on the table file, and start visualizing our dataset.
[00:06:21.120 --> 00:06:27.200]   Here we can look at all of our images individually, or group them by classes
[00:06:27.200 --> 00:06:30.320]   and see the images with the same objects present in them.
[00:06:30.320 --> 00:06:38.240]   But think about it. Since we've already uploaded the images,
[00:06:38.240 --> 00:06:42.560]   why don't we also upload and put the bounding boxes for every epoch?
[00:06:42.560 --> 00:06:50.080]   Meet the evaluation artifact. If we go into the artifacts tab, click on evaluation,
[00:06:50.080 --> 00:06:54.400]   and select the run we want to look at, we can see the evaluation artifact being saved for
[00:06:54.400 --> 00:07:00.800]   every epoch. We can click on files, results table, and literally see the model predictions
[00:07:00.800 --> 00:07:05.360]   for every image in our validation dataset compared to the ground truth for every epoch.
[00:07:05.360 --> 00:07:15.600]   And remember, 1db_tables are interactive. We can sort the entire validation dataset predictions
[00:07:15.600 --> 00:07:21.200]   by average confidence in ascending order to see which images our model struggles the most with,
[00:07:21.200 --> 00:07:25.680]   and look at the ground truth images and the predictions individually to try to figure out
[00:07:25.680 --> 00:07:27.280]   why that's actually happening.
[00:07:27.280 --> 00:07:40.480]   We can also filter the predictions above or below a certain confidence threshold.
[00:07:40.480 --> 00:08:00.480]   And we can group the predictions by the same classes.
[00:08:06.480 --> 00:08:12.080]   Evaluation artifact can help us understand exactly where and how our models are struggling,
[00:08:12.080 --> 00:08:14.880]   and more importantly help us figure out how to fix it.
[00:08:14.880 --> 00:08:22.880]   We can do this for every epoch because the images from our dataset
[00:08:22.880 --> 00:08:27.200]   are already uploaded to the cloud, and we're just lagging the bounding boxes.
[00:08:32.240 --> 00:08:37.280]   The YOLOv5 and Weights and Biases integration really lets us step up how we manage to interpret
[00:08:37.280 --> 00:08:42.720]   the training process. We can keep track of all the hyperparameters and metrics,
[00:08:42.720 --> 00:08:46.400]   but also get an intuition for what our model is actually learning.
[00:08:46.400 --> 00:08:54.960]   Weights and Biases lets us interactively visualize predictions over the entire validation dataset,
[00:08:54.960 --> 00:08:59.440]   and give us the peace of mind that whatever happens, our training progress is saved,
[00:08:59.440 --> 00:09:06.080]   and we can always resume it at any time. Huge shoutout to Aish Churasia for his
[00:09:06.080 --> 00:09:11.520]   contributions in making this integration happen. You can read his report about it on Weights and
[00:09:11.520 --> 00:09:16.160]   Biases, and I definitely encourage you to try out the Google Colab notebook yourself,
[00:09:16.160 --> 00:09:18.640]   and see how it can be useful in your projects.
[00:09:18.640 --> 00:09:25.200]   If you have any questions, please feel free to leave them in the comment section down below,
[00:09:25.200 --> 00:09:30.320]   and I'll be happy to answer them. And consider subscribing to our channel to see more tutorials,
[00:09:30.320 --> 00:09:37.760]   interviews, and talks. And thank you for watching, I hope you enjoyed it and found it useful.

