
[00:00:00.000 --> 00:00:08.160]   to come in. Welcome, everyone. We are also now live on YouTube. That's great. Let me pull up.
[00:00:08.160 --> 00:00:15.920]   I want to share my welcome screen real quick, Hannes. I'm going to cut off your screen share
[00:00:15.920 --> 00:00:26.880]   and pop up mine for our welcoming slide to our attendees. Welcome, everybody, to the next edition
[00:00:26.880 --> 00:00:33.760]   of the Weights and Biases Deep Learning Salon. Great to have you all here. Great to have our
[00:00:33.760 --> 00:00:42.240]   wonderful and distinguished guests as well. So just wanted to, before we kick things off,
[00:00:42.240 --> 00:00:49.600]   do a quick promo for the next salon. We do these every two weeks. So the next one will be October
[00:00:49.600 --> 00:00:56.960]   13th at the same time, 5 o'clock Pacific. We'll have two talks, as is usual. One will be Jesse
[00:00:56.960 --> 00:01:03.520]   Mu, who is a graduate student at Stanford who wrote this really interesting paper recently called
[00:01:03.520 --> 00:01:08.960]   Compositional Explanations of Neurons about how you can use simple concepts, simple explanations
[00:01:08.960 --> 00:01:14.240]   of what a neuron in a neural network does, like detect tree, detect water, combine those together
[00:01:14.240 --> 00:01:20.160]   with logical connectives, like tree and water, to be something that detects a tree-lined lake,
[00:01:20.160 --> 00:01:24.560]   things like that. So it's a really cool paper. I'm really excited to hear him talk about it.
[00:01:24.560 --> 00:01:29.680]   I will also give a talk in two weeks about interpreting neural networks as typecasting
[00:01:29.680 --> 00:01:34.480]   functions, that these fancy neural networks that we're building in our pipelines are really
[00:01:34.480 --> 00:01:41.280]   fundamentally about doing something like 2Bool or 2Float, but in a much more sophisticated and
[00:01:41.280 --> 00:01:47.600]   fancy way. So I'm excited to try out that idea on folks. Just a reminder, you can catch all the
[00:01:47.600 --> 00:01:52.960]   salons you missed on our YouTube channel. So just look for Weights and Biases on YouTube, and you
[00:01:52.960 --> 00:01:58.720]   can find us. You can find our podcast, our deep learning salons, and our app tutorials, and more.
[00:01:58.720 --> 00:02:05.520]   So that's all the promo that I have. So I will kick it over to our first speaker.
[00:02:06.800 --> 00:02:15.200]   And so our first speaker today is Hannes Hopke, who is joining us from Portland, and who works
[00:02:15.200 --> 00:02:22.240]   at SAP, doing primarily, it sounds like, natural language processing. But he'll be talking to us
[00:02:22.240 --> 00:02:29.120]   about MLOps, and in particular, TensorFlow Extended. So I'm really excited to hear from him,
[00:02:29.120 --> 00:02:35.360]   and I'll let him take the floor. Hey, everybody. Thank you for joining
[00:02:35.360 --> 00:02:40.640]   the session today. As Charles mentioned, I would like to talk about machine learning ops and
[00:02:40.640 --> 00:02:48.560]   metadata, and why your machine learning projects probably need that. And my name is Hannes. I'm a
[00:02:48.560 --> 00:02:53.760]   machine learning engineer at SAP Concur, and I'm also a Google Developer Expert for machine learning.
[00:02:53.760 --> 00:02:58.560]   And I was very fortunate to be a co-author of two machine learning publications, one around natural
[00:02:58.560 --> 00:03:03.120]   language processing, and the most recent one with O'Reilly around machine learning pipelines with
[00:03:03.120 --> 00:03:08.640]   TensorFlow Extended. So when we normally talk about machine learning, we always focus on the
[00:03:08.640 --> 00:03:14.720]   machine learning code. And as described in the very famous paper of the hidden technical debt
[00:03:14.720 --> 00:03:20.320]   of machine learning systems, when we bring our machine learning models to the real world, we need
[00:03:20.320 --> 00:03:27.680]   a variety of systems around it. In that case, we're talking about data collection systems. We
[00:03:27.680 --> 00:03:35.520]   need to verify the data, monitor the models. We need to extract the features, do feature engineering,
[00:03:35.520 --> 00:03:41.120]   do the model analysis, and then we also need to serve those models. And when we talk about machine
[00:03:41.120 --> 00:03:47.920]   learning ops, it's all about how can we orchestrate those different tools so they can work together,
[00:03:47.920 --> 00:03:52.720]   and that we can get through an end-to-end process, probably or hopefully automatically.
[00:03:54.240 --> 00:04:00.320]   So normally in data science projects, often the world looks like this. We have a business problem.
[00:04:00.320 --> 00:04:03.920]   We're trying to solve this business problem with a machine learning solution. And in that case,
[00:04:03.920 --> 00:04:09.360]   we developed a machine learning model. And at some point, the work of a data scientist could
[00:04:09.360 --> 00:04:14.880]   be considered done because the business problem is being solved. We can predict a value, or we can
[00:04:14.880 --> 00:04:22.560]   sort of like predict time series, labels, et cetera. But I would say that it's not the case,
[00:04:22.560 --> 00:04:28.720]   because often we need to analyze our models. We should do this consistently. And we also need to
[00:04:28.720 --> 00:04:34.480]   sort of get to a point where we update those models continuously, because as soon as we release those
[00:04:34.480 --> 00:04:40.640]   models to the real world, the world will interact with the model and vice versa. And sort of those
[00:04:40.640 --> 00:04:47.040]   systems will influence each other and change, like say, the feedback loop will change the model
[00:04:47.040 --> 00:04:53.760]   itself and things like this. At the same point in time, as soon as a model hits the real world,
[00:04:53.760 --> 00:04:59.040]   we need to make sure that we can reproduce these models continuously. And we also would like to
[00:04:59.040 --> 00:05:05.840]   track all the artifacts going into a model through things like an audit trail or traceability and
[00:05:05.840 --> 00:05:12.320]   things like this. So that's where machine learning ops and ML ops come in. So normally what happens
[00:05:12.320 --> 00:05:19.360]   to machine learning models, as I said earlier, we get to a point where we can solve a data science
[00:05:19.360 --> 00:05:23.440]   problem. But then getting that machine learning model into the real world, there's another
[00:05:23.440 --> 00:05:29.760]   set of tools we need to connect with. And it's quite another burden. And those models being
[00:05:29.760 --> 00:05:36.320]   released, then we see data drift. We see changes in the data schema. We see changing pre-processing
[00:05:36.320 --> 00:05:42.080]   steps, complicated retraining processes, and potentially high latencies when it comes to
[00:05:42.080 --> 00:05:47.680]   serving those models. And those are currently in a lot of situations manual steps with data
[00:05:47.680 --> 00:05:53.200]   scientists have to do to keep those models up and running. And that keeps them away from being
[00:05:53.200 --> 00:06:00.000]   innovative and really focusing on new data science projects. And so that's why the idea of machine
[00:06:00.000 --> 00:06:06.880]   learning ops came up a few years ago and now sort of like crystallized into what we nowadays see
[00:06:06.880 --> 00:06:11.840]   with TensorFlow Extended and other competitor platforms. So a machine learning lifecycle
[00:06:11.840 --> 00:06:15.360]   would look like this. We start with a data ingestion, as I mentioned. We go through the
[00:06:15.360 --> 00:06:20.640]   data validation, feature engineering, model training. And then a very important part happens.
[00:06:20.640 --> 00:06:28.560]   We take a look at the model itself. I can just stress, let's not just focus on the aggregated
[00:06:28.560 --> 00:06:33.520]   accuracy of a model, but let's drill into the features and maybe slicing the individual
[00:06:33.520 --> 00:06:38.800]   features again and make sure that the model produces predictions equally in that case,
[00:06:38.800 --> 00:06:43.440]   or hopefully unbiased. And then we need to validate that model, comparing the model against
[00:06:43.440 --> 00:06:48.560]   previous runs. Is it performing better? Or sometimes we have criteria where, OK, we accept
[00:06:48.560 --> 00:06:54.000]   a little drop in a certain accuracy if the model is faster, for example. But we basically have to
[00:06:54.000 --> 00:06:58.800]   compare against previous runs. And then we can start deploying those machine learning models.
[00:06:58.800 --> 00:07:02.720]   Once the machine learning model is out, we basically can capture more data with a new
[00:07:02.720 --> 00:07:06.240]   model version and ingest that back into our machine learning pipeline. So that could be
[00:07:06.240 --> 00:07:12.000]   a continuous running cycle in this case. But very important is those components
[00:07:12.000 --> 00:07:17.520]   we see in those machine learning pipelines, they're entangled. And so, for example,
[00:07:17.520 --> 00:07:22.640]   we would use the output of a particular component, let's say the schema generation,
[00:07:22.640 --> 00:07:27.040]   and that will be reused during the training process or, let's say, the feature engineering
[00:07:27.040 --> 00:07:32.240]   process because we need to know what the rough data structure of the data set looks like.
[00:07:32.240 --> 00:07:38.800]   And so they're intertwined. And it's not just a linear process, as we know this from the software
[00:07:38.800 --> 00:07:44.960]   engineering world, where we get a source code, load all the dependencies, compile the code,
[00:07:44.960 --> 00:07:50.240]   and then ship an artifact. So there's a lot of interdependencies here. And that's where
[00:07:50.240 --> 00:07:57.040]   TensorFlow Extended comes in because it gives us a chance to express our pipelines through the
[00:07:57.040 --> 00:08:02.640]   components. And we can then define them specifically for our machine learning project
[00:08:02.640 --> 00:08:06.080]   and orchestrate them on a platform, which I will show in a second.
[00:08:06.080 --> 00:08:14.560]   The TensorFlow Extended project is basically a portfolio of different libraries. And then
[00:08:15.120 --> 00:08:20.720]   they provide the glue code to coordinate those libraries with each other. But you can also run
[00:08:20.720 --> 00:08:26.160]   them as standalone libraries. For example, TensorFlow data validation could work without
[00:08:26.160 --> 00:08:32.880]   any problems in a PyTorch project or in a Scikit-learn project. And as you will see in a
[00:08:32.880 --> 00:08:40.000]   second here, you can generate statistics around data sets almost independently from the framework
[00:08:40.000 --> 00:08:46.080]   itself. So then once we define the individual components, we can tie them together into
[00:08:46.080 --> 00:08:51.840]   a pipeline. And then we can orchestrate those pipelines on Apache Airflow, Apache Beam,
[00:08:51.840 --> 00:08:56.720]   or Kubeflow pipelines these days. And each component will talk to a metadata store
[00:08:56.720 --> 00:09:02.400]   to receive references to inputs to the pipelines and then manipulate something
[00:09:02.400 --> 00:09:09.280]   as a part of that component. Let's say generate the statistics of the data set or train a model
[00:09:09.280 --> 00:09:15.600]   or validate a model and then store the outputs and store the references on those outputs of
[00:09:15.600 --> 00:09:19.760]   the component in the metadata store again. And in a second, I will show you some interesting
[00:09:19.760 --> 00:09:25.280]   use cases where we can use that metadata in machine learning projects. You can also
[00:09:25.280 --> 00:09:29.760]   coordinate or orchestrate your pipelines with JupyterHub. But in that moment, the human is
[00:09:29.760 --> 00:09:34.800]   the orchestrator. But it's a really good way of trying your machine learning pipelines,
[00:09:34.800 --> 00:09:39.440]   debugging it. And once everything runs, you can export them to the other platforms
[00:09:39.440 --> 00:09:42.960]   and then continuously run your machine learning updates.
[00:09:42.960 --> 00:09:49.840]   So how does it work in the real world in terms of just to show a little bit of code here? We
[00:09:49.840 --> 00:09:55.680]   basically define the individual components as Python code. So here in this particular case, we
[00:09:57.680 --> 00:10:05.760]   define a data set or an input generation or input component. And we say that the data which is being
[00:10:05.760 --> 00:10:12.400]   imported here, we split that data three to one to have a quarter goes into our validation set and
[00:10:12.400 --> 00:10:17.760]   three quarters go into the training set. And then we have a variety of components. In this case,
[00:10:17.760 --> 00:10:23.360]   we ingest from a tiered record component. So we can ingest tiered record files. But there's a
[00:10:23.360 --> 00:10:29.040]   variety of opportunities to ingest data from. That could be local files or that could be remote
[00:10:29.040 --> 00:10:36.320]   database systems where we can start our machine learning pipelines from. Once the data is ingested,
[00:10:36.320 --> 00:10:38.960]   we can generate those statistics. I'll show you an example here in a second.
[00:10:38.960 --> 00:10:48.000]   But then we can also generate a schema. And with every model version being produced, we can compare
[00:10:48.560 --> 00:10:54.960]   the statistics generated for that particular run. And then we can also compare the schema.
[00:10:54.960 --> 00:11:00.000]   And sometimes we have situations where a database admin drops an entire column
[00:11:00.000 --> 00:11:05.520]   or a data becomes-- a sparse feature becomes sparse all of a sudden.
[00:11:05.520 --> 00:11:10.720]   Sorry, a dense feature becomes sparse. And in that case, we can alert data scientists
[00:11:10.720 --> 00:11:14.000]   automatically and say, hey, there's something wrong in your pipeline. Please take a look.
[00:11:14.560 --> 00:11:20.240]   So as an example, once we run that component, we can then visualize the output data structure
[00:11:20.240 --> 00:11:27.280]   into this very nice visualization for numerical and categorical features and show basic statistical
[00:11:27.280 --> 00:11:34.160]   information about the data set. And there's also options of comparing data sets with each other.
[00:11:34.160 --> 00:11:42.320]   But it might not be crazy for a statistician here. But for getting a good overview of--
[00:11:43.680 --> 00:11:47.840]   for a data set or seeing what has changed since the last run, I think this is a really good,
[00:11:47.840 --> 00:11:53.280]   wonderful tool to take a look at those components-- sorry, to take a look at those data sets.
[00:11:53.280 --> 00:11:59.600]   We can then move on and define similar components for our feature engineering, model training,
[00:11:59.600 --> 00:12:04.880]   model analysis, and the deployment. And then we can tie this together, as I said, into a pipeline.
[00:12:04.880 --> 00:12:11.760]   And we can orchestrate that pipeline. And that's where the-- I would say where the magic happens,
[00:12:11.760 --> 00:12:16.640]   where we can remove the burden for the data scientists to run all those steps manually.
[00:12:16.640 --> 00:12:22.800]   We basically define those individual components as a list. And then those runners, which are
[00:12:22.800 --> 00:12:28.640]   provided by TensorFlow Extended, define the dependencies between the components and generate
[00:12:28.640 --> 00:12:33.440]   the instructions for Apache BMO Airflow. Or in the case of Kubeflow Pipelines, it generates the
[00:12:33.440 --> 00:12:39.360]   Argo instructions, which we can then upload or push to our Kubeflow Pipelines environments.
[00:12:40.160 --> 00:12:47.360]   And there, we can then execute them. So here's an example for Apache Beam. It determined all
[00:12:47.360 --> 00:12:51.280]   the different dependencies of the components. And then it starts executing from the first to
[00:12:51.280 --> 00:12:55.360]   the last component. And if everything runs smoothly, we probably have-- we'll hopefully
[00:12:55.360 --> 00:13:03.760]   have an exportable model at the end of the day. So where to start? If you have a machine learning
[00:13:03.760 --> 00:13:09.760]   project and you run this in production or you want to bring a machine learning model into production,
[00:13:09.760 --> 00:13:17.360]   then there is a-- where to start with TFX? The TensorFlow team has provided wonderful
[00:13:17.360 --> 00:13:24.960]   example projects for various use cases. We have written about one additional use case we saw at
[00:13:24.960 --> 00:13:31.200]   Concur Labs, which was deploying BERT models for various applications. And in the context of
[00:13:31.200 --> 00:13:36.160]   TensorFlow Extended, there is wonderful benefits when it comes to TensorFlow Transform, TensorFlow
[00:13:36.160 --> 00:13:42.400]   Text, and tying this all together in those pipelines. We can really simplify the deployments
[00:13:42.400 --> 00:13:48.800]   and make them scalable. One thing I hear sometimes when I discuss machine learning pipelines with
[00:13:48.800 --> 00:13:55.360]   colleagues that, oh, wait, we have a CI/CD system for our software tools already. We can just run
[00:13:55.360 --> 00:13:59.280]   our machine learning pipelines in the same tool. We don't need anything new for that.
[00:14:00.000 --> 00:14:06.640]   And I would say be really careful with that because, as I said earlier, those pipeline runs
[00:14:06.640 --> 00:14:13.440]   are interdependent. So the results from the current run will depend on the previous run. So we always
[00:14:13.440 --> 00:14:17.600]   want to compare against previous data sets or previous model runs and things like this.
[00:14:17.600 --> 00:14:24.320]   Also, at the same time, we really need to watch our scaling, that we can scale those pipelines.
[00:14:24.320 --> 00:14:29.200]   And TensorFlow Extended has this wonderful case where it uses Apache Beam behind the scenes.
[00:14:29.840 --> 00:14:40.880]   And with Apache Beam, we can export those data-heavy tasks to a data cluster or a compute
[00:14:40.880 --> 00:14:48.400]   cluster. Earlier, I said, let's take a look at the metadata. And I think the metadata can be an
[00:14:48.400 --> 00:14:54.960]   insurance policy for machine learning. And with insurance policy, let's please not take this very
[00:14:54.960 --> 00:15:00.560]   literal. But when we deploy machine learning models, I think we should be able to answer
[00:15:00.560 --> 00:15:06.320]   questions in the future about a model we have produced today. So normal questions could be,
[00:15:06.320 --> 00:15:12.560]   like, what data set was being used? What was the distribution of a certain feature of that data
[00:15:12.560 --> 00:15:19.520]   set? What was the validation? What were certain validation results per feature of that particular
[00:15:19.520 --> 00:15:24.080]   model? How much better was the model against the previous run, et cetera? Maybe who signed off on
[00:15:24.080 --> 00:15:31.920]   that? And so the metadata, when we take a look at the metadata, we can basically just answer a lot
[00:15:31.920 --> 00:15:36.880]   of questions around machine learning. And machine learning in real-world scenarios, we're sort of
[00:15:36.880 --> 00:15:45.440]   affected by privacy concerns, legal compliances, et cetera. All those things, I hope we can answer
[00:15:45.440 --> 00:15:49.520]   at some point with the metadata we've produced during those pipeline runs. And TensorFlow
[00:15:49.520 --> 00:15:55.200]   Extended is really great. Components are designed in a very standardized way. Every component
[00:15:55.200 --> 00:16:00.880]   consists out of three parts. They have a driver, an executor, and a publisher. The driver will
[00:16:00.880 --> 00:16:07.920]   receive the references as inputs and load them from the metadata store. And the publisher will
[00:16:07.920 --> 00:16:14.560]   take the results after the manipulation of the data or the trained model from the executor and
[00:16:14.560 --> 00:16:19.520]   save those artifacts, the references of those artifacts in the metadata store.
[00:16:19.520 --> 00:16:25.840]   And then based on that, we can build really interesting solutions from that. So for example,
[00:16:25.840 --> 00:16:30.720]   we can build collaboration tools. We can use collaboration tools. In this case, it's TensorFlow
[00:16:30.720 --> 00:16:35.440]   model analysis and data validation. So we can generate those statistics and those overviews
[00:16:35.440 --> 00:16:41.280]   about the results of a particular model run and then store them away and archive them. And if
[00:16:41.280 --> 00:16:47.520]   there are questions, we can come back and reopen them and discuss them as a team through a website
[00:16:47.520 --> 00:16:55.840]   or a platform. We can also investigate the model lineage and say, OK, we have a model. Let's take
[00:16:55.840 --> 00:17:01.120]   a look at all the artifacts which went into that model output or this exported model and see what
[00:17:01.120 --> 00:17:07.200]   could have potentially caused an issue there. And then there's things like warm starting. So
[00:17:08.080 --> 00:17:14.880]   we might be restricted to use smaller data sets due to GDPR compliance. But we can basically then
[00:17:14.880 --> 00:17:21.680]   fine tune existing models through warm starting. And the metadata allows us to go back to the last
[00:17:21.680 --> 00:17:29.680]   successful run for a particular model. And then another use case I just want to mention here
[00:17:29.680 --> 00:17:36.160]   quickly is we can put a human in the loop. And we can automate everything. But at the very end,
[00:17:36.160 --> 00:17:43.600]   we can ask a human, in this case, we use the Slack component. That component publishes a tiny
[00:17:43.600 --> 00:17:50.960]   message, redirects the data scientist to the pipeline. They can basically investigate the
[00:17:50.960 --> 00:17:56.080]   model analysis feedback and see if everything doesn't look good. They can poke the model a
[00:17:56.080 --> 00:17:59.520]   little bit and then sign off on the model. And we would track this in the metadata as well.
[00:17:59.520 --> 00:18:05.760]   So then if we put both together, the machine learning ops with TensorFlow extended in
[00:18:05.760 --> 00:18:10.880]   the metadata, we can remove the burden from the data scientist to manually walk through all those
[00:18:10.880 --> 00:18:19.600]   steps in manual fashion. And therefore, we can also do this consistently. Because we automate
[00:18:19.600 --> 00:18:27.200]   it, we can make sure that every model goes through the same validation process and analysis process.
[00:18:27.200 --> 00:18:34.160]   And then also, we can also produce those pipelines. Or we can hopefully reproduce our models
[00:18:35.280 --> 00:18:39.600]   in a consistent way. We store the data sets. We have our references to particular versions. So
[00:18:39.600 --> 00:18:43.840]   we can pick a version and then go back to all the artifacts which affected that model and
[00:18:43.840 --> 00:18:50.720]   reproduce that model. That's hopefully what we can achieve with those pipelines.
[00:18:50.720 --> 00:18:56.880]   If this was too fast of a run through, we have an additional reference here. That's the Reilly
[00:18:56.880 --> 00:19:02.720]   book, which we recently published in July. And I was very fortunate to co-author that with
[00:19:02.720 --> 00:19:08.080]   Catherine Nelson. And it's now available as print and digital version. So with that,
[00:19:08.080 --> 00:19:12.800]   I would say thank you. And if there are questions, I'm more than happy to take some.
[00:19:12.800 --> 00:19:20.080]   Great. Yeah. Thanks a lot, Hannes, for a really interesting talk. A nice overview of this,
[00:19:20.080 --> 00:19:23.840]   the TensorFlow extended tool. I know there's a lot of folks in our audience who are really
[00:19:23.840 --> 00:19:28.880]   interested in reproducible, high quality machine learning. Sort of getting out of
[00:19:28.880 --> 00:19:33.440]   just Jupyter notebooks on your laptop and making something that can work in production.
[00:19:33.440 --> 00:19:39.760]   The first question is from YouTube, from William Limerond. So you had all those slides where you
[00:19:39.760 --> 00:19:45.840]   showed that sort of feedback loop with the metadata and the feedback loop of this pipeline.
[00:19:45.840 --> 00:19:51.280]   So they just wanted to confirm, so can we make a sort of feedback loop in our pipeline
[00:19:51.280 --> 00:19:56.160]   where our model is updated with new data sort of on a set schedule? Like every day,
[00:19:56.160 --> 00:20:00.720]   let's run a sort of like cron job to update this model with new data and push it out there. Is
[00:20:00.720 --> 00:20:05.680]   that something you support? Yes. So tools like GroupFlow
[00:20:05.680 --> 00:20:11.600]   pipelines, they already have that feature in there that you can rerun pipelines on a continuous basis.
[00:20:11.600 --> 00:20:17.920]   The tricky part here is how to capture the data. That's very problem specific. There hasn't been
[00:20:17.920 --> 00:20:25.520]   one tool out there where you can sort of like bend it and make it or tweak it for every application.
[00:20:25.520 --> 00:20:31.200]   So there's a lot of like custom applications, custom tools coming in there. There's also
[00:20:31.200 --> 00:20:35.360]   different types of feedback. There's direct and indirect feedback and explicit and implicit
[00:20:35.360 --> 00:20:39.920]   feedback in terms of like, are we just saying the prediction was wrong or are we correcting
[00:20:39.920 --> 00:20:45.920]   the prediction? There's different types. And so therefore sort of that has a lot to do with like
[00:20:45.920 --> 00:20:51.280]   how does the UI work? What's the application? Like who is validating those results?
[00:20:52.320 --> 00:20:56.720]   But in general, you can continuously run those pipelines. All the orchestrators have features
[00:20:56.720 --> 00:21:02.160]   to run them on a continuous basis. Nice. It's good to hear. It seems like
[00:21:02.160 --> 00:21:06.240]   a tough thing to manage manually. So it's good to hear there's good automated tools for
[00:21:06.240 --> 00:21:10.640]   helping with that. It's easy to mess it up. That's the problem.
[00:21:10.640 --> 00:21:17.360]   I myself like would forget a step or like sometimes you have a Python script and you're
[00:21:17.360 --> 00:21:23.520]   executed a Python script but something fails because of dependencies or like you track to,
[00:21:23.520 --> 00:21:28.720]   you forget to track something and then it's like, oh, where did I leave my metadata? And usually
[00:21:28.720 --> 00:21:33.200]   the metadata ended up in my notebook and that was a bad place for metadata.
[00:21:33.200 --> 00:21:39.920]   For sure. We have a question from Himanshu Arora. How does the state of MLOps and PyTorch
[00:21:39.920 --> 00:21:45.200]   compare to that of TensorFlow? I know you mentioned that the tool is agnostic to backend framework,
[00:21:45.200 --> 00:21:50.160]   but have you noticed a difference in these two frameworks and how well they support MLOps?
[00:21:50.160 --> 00:21:57.120]   Yeah. So just to be clear, so TensorFlow, TFX as a whole is TensorFlow specific. There's like
[00:21:57.120 --> 00:22:04.320]   very specific components in there. So for example, TensorFlow transform that generates a TensorFlow
[00:22:04.320 --> 00:22:09.200]   graph, which can then be connected with a trained model that only works with TensorFlow. But
[00:22:09.200 --> 00:22:17.520]   individual components, let's say TensorFlow data validation or model analysis, model analysis,
[00:22:17.520 --> 00:22:22.880]   or in that case, not the particular tool, but like, let's say the what if tool, there's ways
[00:22:22.880 --> 00:22:31.520]   to use that with other machine learning models. I'm not an expert in the PyTorch ecosystem. So
[00:22:31.520 --> 00:22:37.920]   everything I say here is with a grain of salt. I would say that the project started with a different
[00:22:37.920 --> 00:22:45.520]   focus. But I think tools are emerging to make them more framework independent. And as we have seen
[00:22:45.520 --> 00:22:52.000]   this when the TensorFlow world with a TFDB tool that is independent, that could easily run with
[00:22:52.000 --> 00:22:57.280]   a PyTorch model or with a scikit-learn model. So I would expect that in the future, similar
[00:22:57.280 --> 00:23:04.800]   projects will come up or maybe TFX will extend or plugins will come in where we can run PyTorch
[00:23:04.800 --> 00:23:11.120]   models or we can save PyTorch models through those pipelines and validate them. So I think
[00:23:11.120 --> 00:23:18.560]   that could work. >> Great. This is a question, an anonymous question, but it matches one I kind of
[00:23:18.560 --> 00:23:27.840]   wanted to ask. So you mentioned a lot of people think that they can use CI tools from other areas
[00:23:27.840 --> 00:23:33.920]   of programming and use them. What kinds of issues have you seen people run into when they try and
[00:23:33.920 --> 00:23:37.520]   port that stuff without using a specialized tool like TensorFlow Extended?
[00:23:37.520 --> 00:23:44.960]   >> I think the biggest problem is like in a CI -- sorry, in a traditional CI/CD tool,
[00:23:44.960 --> 00:23:50.560]   if you want to compare to previous runs, let's say your last line of defense is you only want
[00:23:50.560 --> 00:23:55.600]   to deploy if your model is X percent better than the previous run. If you want to do this with a
[00:23:55.600 --> 00:24:02.320]   traditional CI/CD software tool, then you have to run -- you basically have to run the metadata
[00:24:02.320 --> 00:24:10.560]   yourself and compare against a log file, maybe previous runs or something. But in that case,
[00:24:10.560 --> 00:24:15.920]   you would have to implement those interfaces yourself, and that's what you get free of charge
[00:24:15.920 --> 00:24:23.840]   with TensorFlow Extended. That's the big benefit. The other thing is when we run Jenkins, for
[00:24:23.840 --> 00:24:28.960]   example, to validate models, that absolutely works. I've done this in previous jobs as well.
[00:24:29.920 --> 00:24:35.040]   But in that moment, it's like sometimes models become bigger, and sometimes you need to -- like,
[00:24:35.040 --> 00:24:41.600]   for example, for the TensorFlow model analysis, you want to use a larger validation set.
[00:24:41.600 --> 00:24:46.640]   In that moment, you can run those statistics or the aggregation of those statistics through
[00:24:46.640 --> 00:24:51.600]   Apache Beam if you use TensorFlow Extended. If you run this on Jenkins, you're really limited
[00:24:51.600 --> 00:24:56.480]   to that box you have, the virtual machine, and that could be a potential bottleneck. So
[00:24:57.280 --> 00:25:02.720]   there's good benefits here for tools like TensorFlow Extended and things like Kubeflow
[00:25:02.720 --> 00:25:09.680]   Pipelines to combine them. >> Great. So I have a question that I
[00:25:09.680 --> 00:25:15.760]   wanted to make sure to ask you. So earlier today, a friend of Weights and Biases, Chip Yen, who was
[00:25:15.760 --> 00:25:21.920]   in our podcast maybe a month ago, had a tweet about common misperceptions about ML models in
[00:25:21.920 --> 00:25:26.400]   production, and I wanted to pass a couple of those by you and get your thoughts on that and maybe
[00:25:26.400 --> 00:25:34.240]   where you see TensorFlow Extended maybe helping with this. So one part was misconception number
[00:25:34.240 --> 00:25:40.720]   three. If nothing happens, model performance remains the same. So she says ML models perform
[00:25:40.720 --> 00:25:45.520]   best right after training in prod. They degrade quickly. And she gives a tip. Train models on data
[00:25:45.520 --> 00:25:49.920]   generated six months ago and test on current data to see how bad that's going to be. So what do you
[00:25:49.920 --> 00:25:55.120]   think of that tip? What do you think the TensorFlow Extended story is about concept drift and model
[00:25:55.120 --> 00:26:00.160]   drift? >> I think that tweet made my day. I read through the tweet this morning and I was like,
[00:26:00.160 --> 00:26:06.080]   wow, this is like anybody who's interested in machine learning infrastructure, that should be
[00:26:06.080 --> 00:26:12.400]   a starting point. And if you need arguments for your manager to get resources for your project,
[00:26:12.400 --> 00:26:21.200]   just pull up the tweet and show it to them. In terms of the model staleness and updating models,
[00:26:21.200 --> 00:26:28.080]   definitely that's something we can split. We can ingest different data sets from different
[00:26:28.080 --> 00:26:35.360]   points in time into those TFX pipelines. And I think that she makes a really wonderful point
[00:26:35.360 --> 00:26:41.280]   here in saying, let's not assume once a model is trained, it will be perfect. As soon as it's
[00:26:41.280 --> 00:26:46.720]   out, it's outdated. We continuously need to update the model and we need to challenge our assumptions
[00:26:46.720 --> 00:26:56.320]   around what data set we use. And I really want to stress that just looking at the aggregated accuracy
[00:26:56.320 --> 00:27:02.320]   across a large training set is potentially not an indication for a good performing model. So
[00:27:02.320 --> 00:27:09.120]   I would highly recommend tools like TensorFlow data, model analysis and data validation for
[00:27:09.120 --> 00:27:13.360]   investigating those data sets. >> Yeah, absolutely. I came across a
[00:27:14.160 --> 00:27:18.880]   long Twitter thread of a bunch of ML ops people dunking on accuracy and just making fun of people
[00:27:18.880 --> 00:27:25.360]   who only look at model accuracy. And as somebody who mostly studied optimization, I felt like I was
[00:27:25.360 --> 00:27:35.360]   a little bit in trouble. >> Well, accuracy is not -- it's hard to say. I think Sarah could answer
[00:27:35.360 --> 00:27:42.080]   the question much better than me. But I think if we take a look at accuracy and use that as a metric,
[00:27:42.080 --> 00:27:48.080]   we should be fair and not take a look at it from a list across the entire data set and just
[00:27:48.080 --> 00:27:53.760]   accumulate or sum up the entire accuracy. But we should go by maybe feature by feature and then
[00:27:53.760 --> 00:28:00.000]   slice a particular feature we think are very sensitive and make sure the accuracy is consistent
[00:28:00.000 --> 00:28:06.240]   across a particular feature in those different slices. And we often, like in the case of
[00:28:07.440 --> 00:28:12.480]   my employer, we develop models for different countries and we want to make sure that, for
[00:28:12.480 --> 00:28:20.720]   example, the model works in country A as in country B, but I'm pretty sure that one country will
[00:28:20.720 --> 00:28:25.840]   be the majority of the data set and when we add new countries, they will be underrepresented. So
[00:28:25.840 --> 00:28:31.200]   we really need to make sure that the accuracy in that country works well as well or has a good
[00:28:31.200 --> 00:28:36.080]   performance and it shouldn't be an outlier. And if we just look at the overall accuracy,
[00:28:36.080 --> 00:28:41.280]   we wouldn't be able to see that. So that's why I think tools like TensorFlow model analysis is just
[00:28:41.280 --> 00:28:46.160]   wonderful and really, really helpful for machine learning engineers.
[00:28:46.160 --> 00:28:54.240]   Yeah, definitely. Definitely. It's become more and more clear. Another tweet from the list,
[00:28:54.240 --> 00:29:00.640]   misconception number four, you won't need to update your models that much. And mentions that
[00:29:00.640 --> 00:29:10.320]   AWS deploys every 11.7 seconds. So I'm curious, how fast do you see people deploying pipelines
[00:29:10.320 --> 00:29:15.680]   with TensorFlow Extended? How quickly can people get that turnaround versus what they do when they
[00:29:15.680 --> 00:29:22.000]   do it by hand? Have you seen this gap? Well, I don't know if the 11 second number
[00:29:22.000 --> 00:29:26.400]   is for a particular model because a pipeline run would take way longer than 11 seconds.
[00:29:27.280 --> 00:29:31.920]   I think it's for literally the AWS service, a new build goes out every 11 seconds, not
[00:29:31.920 --> 00:29:36.720]   Amazon's ML models, which I'm sure are too proprietary for us to know what's going on.
[00:29:36.720 --> 00:29:42.960]   But I think we should, as a data scientist and machine learning engineer, regardless of what the
[00:29:42.960 --> 00:29:49.040]   time span is, but we should have the capabilities of pushing new models out and having ways of
[00:29:49.040 --> 00:29:55.600]   validate the models automatically and consistently. Currently, I see a lot of
[00:29:55.600 --> 00:30:01.040]   data science projects where the models are still being produced in notebooks and then being
[00:30:01.040 --> 00:30:04.560]   deployed. Maybe the analysis happens in the notebook, but sometimes we lose that notebook and
[00:30:04.560 --> 00:30:10.800]   with that, the data is gone and we lose the records of that. And so what we do with machine
[00:30:10.800 --> 00:30:15.520]   learning engineering and those pipelines is that we just put everything into a consistent workflow
[00:30:15.520 --> 00:30:21.360]   and track those information consistently and maybe keep those records in a remote
[00:30:21.360 --> 00:30:25.760]   MySQL database so that that can be backed up in a central place.
[00:30:25.760 --> 00:30:32.080]   Maybe just one last shout out. The thread is extremely interesting and
[00:30:32.080 --> 00:30:37.840]   Chip mentions a couple of numbers, which I thought were really interesting. So
[00:30:37.840 --> 00:30:44.320]   one of them, what you just mentioned, was the number of deployments and also how many models
[00:30:44.320 --> 00:30:50.320]   certain companies have running in production. It's good to see those numbers.
[00:30:50.320 --> 00:30:55.600]   And to really think about why we need more infrastructure around machine learning.
[00:30:55.600 --> 00:31:02.160]   Definitely. We only have time for one more question. So I'm going to ask this one from
[00:31:02.160 --> 00:31:08.240]   Kanduk Bekshi. Production pipelines for edge devices with tools like TF Lite and those for
[00:31:08.240 --> 00:31:13.760]   well-resourced servers have inherent differences. How does the approach here and the tools stack up
[00:31:13.760 --> 00:31:22.240]   to the challenges of edge? So I assume that the goal of the pipeline is to produce like a TF Lite
[00:31:22.240 --> 00:31:27.920]   model. That's my assumption to answer that question. And you are in luck because the TensorFlow team
[00:31:27.920 --> 00:31:35.040]   has released an amazing feature recently where you can go through the entire training set step
[00:31:35.040 --> 00:31:41.520]   and you can then rewrite the model for JavaScript or TensorFlow Lite. And so that could be part of
[00:31:41.520 --> 00:31:46.160]   a machine learning model, machine learning pipeline. And what we have experimented with
[00:31:46.160 --> 00:31:53.360]   at Concur Labs was also branching our pipeline. So we would go through all the data verification,
[00:31:53.360 --> 00:31:58.800]   validation, feature engineering. But then once we get to the training or maybe we train a model,
[00:31:58.800 --> 00:32:04.480]   and then we branch and say, why don't we-- we can produce three models. And one model is
[00:32:04.480 --> 00:32:09.840]   converted to JavaScript. The other one is converted to something we can push to our mobile devices.
[00:32:09.840 --> 00:32:14.320]   And then we have the last model, which is then being used in TensorFlow serving applications.
[00:32:14.320 --> 00:32:20.080]   And then we can-- because we run this in those pipelines, we can make sure it runs on the exact
[00:32:20.080 --> 00:32:26.320]   same data sets, on the exact same transformations. And we can even convert those transformations
[00:32:26.320 --> 00:32:31.280]   into the model so we can attach them to the trained models and export them as one artifact.
[00:32:31.280 --> 00:32:38.240]   It's another amazing byproduct that I find from the TFX pipelines that we can then ship one
[00:32:38.240 --> 00:32:44.160]   artifact and make sure that there's no Python script you could potentially lose where you have
[00:32:44.160 --> 00:32:49.840]   your preprocessing steps. That's happened to me where I just used the wrong preprocessing step
[00:32:49.840 --> 00:32:53.760]   for a model. And the performance was-- it was working, but the performance was weird. It didn't
[00:32:53.760 --> 00:32:58.480]   match our validation. And with TFX, you can get around those mistakes.
[00:32:58.480 --> 00:33:04.640]   Great. Great. Thanks for answering all of the folks' questions.
[00:33:06.080 --> 00:33:10.880]   Yeah, and thanks for a really great talk. I'm looking forward to using TensorFlow Extended
[00:33:10.880 --> 00:33:13.280]   the next time I need to orchestrate a big old pipeline.
[00:33:13.280 --> 00:33:21.520]   All right. So thanks a lot, Hannes. Next up, we have our second speaker for the day,
[00:33:21.520 --> 00:33:29.040]   Sarah Hooker. I think she's working a little bit on her AV setup here. So while she's got that,
[00:33:29.040 --> 00:33:35.280]   I'll do a little bit of an introduction. So I initially invited Sarah to come on to this salon
[00:33:35.840 --> 00:33:45.360]   because I saw a paper that she put out a couple of months ago on this variance of gradients for
[00:33:45.360 --> 00:33:50.240]   detecting outliers in data sets. And I thought it was a really cool paper. I'd seen some of her
[00:33:50.240 --> 00:33:59.920]   other stuff. And so I invited her on to talk. And then not too long after I invited her,
[00:33:59.920 --> 00:34:05.840]   she put up another excellent paper that really blew up and caught a lot of attention,
[00:34:05.840 --> 00:34:12.160]   this hardware lottery paper. So she's graciously agreed to talk about both of these things. So
[00:34:12.160 --> 00:34:19.680]   we'll get to hear about both of those from her. And I'm really excited about that. So Sarah,
[00:34:19.680 --> 00:34:23.760]   go ahead and take it away. Hi, Charles. Lovely to be here.
[00:34:24.640 --> 00:34:34.240]   So I've been given the formidable challenge today of presenting two papers on quite not necessarily
[00:34:34.240 --> 00:34:41.760]   a smooth transition in ideas between the two. So I think what I'll ask for is, Charles, if you could
[00:34:41.760 --> 00:34:50.000]   remind me if we get to 5.50 and I still haven't switched over, then I'll switch over to talking
[00:34:50.000 --> 00:34:55.200]   about a very exciting second idea, which is actually with a co-first author called Chirag,
[00:34:55.200 --> 00:35:00.320]   who's lovely, and it's all about estimating example importance and how do we surface
[00:35:00.320 --> 00:35:05.360]   automatically examples for human-in-loop inspection. But to start with, since this
[00:35:05.360 --> 00:35:11.920]   is a special request, I will be talking without lovely slides, but I think that's almost better
[00:35:11.920 --> 00:35:17.360]   for this type of paper. I'll be talking about a project that I recently shared, which is called
[00:35:17.360 --> 00:35:25.440]   the Hardware Lottery. And I think that this project, I don't know if slides would do it
[00:35:25.440 --> 00:35:33.840]   justice because in some ways it's really about this idea of how our choices about hardware and
[00:35:33.840 --> 00:35:38.640]   software have really had implications for the direction of research and the progress that we've
[00:35:38.640 --> 00:35:44.640]   made. What I will invite because of that is a lot of questions. So maybe what I'll start with is
[00:35:46.000 --> 00:35:50.000]   telling a little bit about how I arrived at this idea, because I think it's always fun. You can
[00:35:50.000 --> 00:35:59.120]   always go read the paper. And in fact, maybe I'll just present the background of the paper behind.
[00:35:59.120 --> 00:36:11.920]   But perhaps it's fun to start by talking a little bit about why I came to this idea and why I took
[00:36:11.920 --> 00:36:19.360]   the time to write it down. So a lot of my research has been on the topic of beyond test set accuracy.
[00:36:19.360 --> 00:36:25.680]   So there was a question earlier about should we care? So we should. So a lot of my research has
[00:36:25.680 --> 00:36:33.440]   been how do we train models to not just have high test set accuracy, but to also essentially
[00:36:33.440 --> 00:36:38.800]   have interpretability, fulfill fairness requirements, be compact. And compactness is
[00:36:38.800 --> 00:36:44.080]   interesting because the area of compactness research, there's almost two different motivations
[00:36:44.080 --> 00:36:51.760]   you could care about. So one is this idea that you want to arrive at a theoretically most compact
[00:36:51.760 --> 00:36:56.560]   representation of the network for the task. And a lot of people are interested in compression for
[00:36:56.560 --> 00:37:04.640]   that reason. What is the minimal amount of weights or minimal amount of neurons we can have in order
[00:37:04.640 --> 00:37:10.960]   to still perform and converge well on a task? There's another motivation, which is that if you
[00:37:10.960 --> 00:37:18.880]   have compactness, it really helps in deployment. And so one reason compression is really exciting
[00:37:18.880 --> 00:37:24.720]   is because you can democratize AI in much more resource constrained environments. And I came
[00:37:24.720 --> 00:37:30.160]   into this research area caring about both. I'm really interested in how we can design more
[00:37:31.200 --> 00:37:36.160]   theoretically anchored algorithms where we achieve higher levels of compression.
[00:37:36.160 --> 00:37:41.760]   But I wanted to translate. I grew up in Africa, and I really want these algorithms to work in
[00:37:41.760 --> 00:37:48.240]   the resource constrained environment of low memory, expensive data. You often don't have a
[00:37:48.240 --> 00:37:54.560]   place to charge your phone. And what was interesting is that the hardware lottery really started
[00:37:54.560 --> 00:38:00.640]   because I was trying to figure out how does my theoretical work translate. And that might seem
[00:38:00.640 --> 00:38:07.360]   like a simple question, but it actually became kind of an eight month odyssey and exploration,
[00:38:07.360 --> 00:38:13.200]   because what I found was the more I asked, the more people said, well, we're not quite sure.
[00:38:13.200 --> 00:38:18.720]   So there's some things we do know. So, for example, a lot of compression research is focused
[00:38:18.720 --> 00:38:24.160]   on unstructured sparsity. And what that means is that you're removing weights of a network rather
[00:38:24.160 --> 00:38:29.920]   than neurons, filters, and unstructured sparsity, we know achieves high levels of compression. So
[00:38:29.920 --> 00:38:34.160]   a lot of people theoretically are very excited about it, but it doesn't translate to current
[00:38:34.160 --> 00:38:42.080]   hardware because as far as matrix multipliers are actually very expensive. So we know things like
[00:38:42.080 --> 00:38:46.560]   this, that the hardware is stacked against certain ideas, but there's other things in the software.
[00:38:46.560 --> 00:38:52.320]   So for example, TF Lite was mentioned. TF Lite is wonderful. But in TF Lite, if you want to work in
[00:38:52.320 --> 00:38:58.000]   that type of compressed environment right now, you have to build your world, your entire algorithm
[00:38:58.000 --> 00:39:04.160]   using less than a hundred operations. I mean, it's fascinating. And so that really lays the
[00:39:04.160 --> 00:39:10.480]   groundwork for understanding how the choices we make at a hardware and software level ripple through
[00:39:10.480 --> 00:39:16.000]   and impact the algorithm. And what I realized was the following is that even though I was stumbling
[00:39:16.000 --> 00:39:22.800]   and kind of peppering all these amazing teams at Google with these questions, historically,
[00:39:22.800 --> 00:39:29.760]   these communities have evolved totally in separation. For example, I would challenge
[00:39:29.760 --> 00:39:37.040]   anyone, go read a systems paper tonight. If you're an algorithms researcher, go read a systems paper.
[00:39:37.040 --> 00:39:46.160]   And please let me know if you fully comprehend it. It is a totally different body of knowledge,
[00:39:48.160 --> 00:39:53.280]   manner of publishing, manner of engaging. And that's because for a lot of computer science
[00:39:53.280 --> 00:39:58.640]   history, there's been very little incentive for these three groups, algorithms, software,
[00:39:58.640 --> 00:40:04.720]   and hardware to work together. And that is a lot of what the hardware lottery is about.
[00:40:04.720 --> 00:40:12.960]   The hardware lottery really starts with this grumpy statement, which is that for computer
[00:40:12.960 --> 00:40:19.760]   science history, hardware and software have frequently determined which ideas succeed and
[00:40:19.760 --> 00:40:27.040]   fail. And then a lot of the essay is unpacking that claim and saying, the reason why we say this
[00:40:27.040 --> 00:40:32.160]   is that we know from early computer science history, there've been all these occasions where
[00:40:32.160 --> 00:40:40.720]   the idea could not be executed. There was huge lags. So with the analytical machine close to a
[00:40:40.720 --> 00:40:47.520]   century, but even more recently with deep neural networks, because the idea was proposed, but we
[00:40:47.520 --> 00:40:54.800]   didn't have the hardware and software to operationalize it. The reason, oh, sorry,
[00:40:54.800 --> 00:40:57.600]   go ahead, Charles. You looked like you were going to say something.
[00:40:57.600 --> 00:40:59.280]   Oh, no, I was just thinking.
[00:40:59.280 --> 00:41:04.960]   You were just nodding. Okay. Excellent. Well, I appreciate the nodding as I go through this.
[00:41:06.160 --> 00:41:12.800]   Good feedback, Lou. And I think what's interesting is now we've laid the scene. So this is why I got
[00:41:12.800 --> 00:41:18.880]   into this essay. And a lot of what the first part is, is essentially talking about examples of this
[00:41:18.880 --> 00:41:26.880]   hardware lottery. The reason why I finally felt obliged to put it on archive, and it's quite
[00:41:26.880 --> 00:41:33.760]   funny because I put it in archive and not, this is very different from a typical research paper.
[00:41:33.760 --> 00:41:41.120]   So in many ways, it's been quite fun to see the variety of audience who is connected with it.
[00:41:41.120 --> 00:41:46.640]   But the reason why I felt compelled to do that was the second part of the essay, which is that
[00:41:46.640 --> 00:41:53.840]   really the position of this paper is that while the hardware lottery has existed,
[00:41:53.840 --> 00:42:01.040]   and it's dominated a lot of the rate of computer science progress, I posit that this in fact,
[00:42:01.600 --> 00:42:08.320]   will become a bigger gap between the winners and losers. And the reason why is that our ecosystem
[00:42:08.320 --> 00:42:16.160]   of software and hardware is becoming even more fragmented. So we have kind of, as the last talk,
[00:42:16.160 --> 00:42:21.360]   very well captured, we're starting to get all these different tooling kits at the software level,
[00:42:21.360 --> 00:42:25.680]   but also with the advent of domain specialized hardware. And this is hardware, which
[00:42:25.680 --> 00:42:31.120]   in an interesting way should have solved for the pain points of us developing in isolation.
[00:42:31.120 --> 00:42:36.320]   It's hardware where essentially the focus on is on specializing for specific algorithms.
[00:42:36.320 --> 00:42:41.280]   But the tricky part is, is that because of specializing for certain algorithms,
[00:42:41.280 --> 00:42:46.960]   if another new algorithm comes along, it becomes harder to take that hardware and show that the
[00:42:46.960 --> 00:42:54.480]   new algorithms is a success. And when you have this, when you have essentially more combinations,
[00:42:54.480 --> 00:43:00.560]   but more locked in combinations, and more effort going to certain, it becomes more costly to stray
[00:43:00.560 --> 00:43:05.520]   off the path, the beaten path of research. And right now the beaten path of research
[00:43:05.520 --> 00:43:15.200]   is deep neural networks. So deep neural networks came around in 2012. And essentially, it was an
[00:43:15.200 --> 00:43:20.080]   overnight empirical success. It was one of those rare snapshots in computer science history where
[00:43:20.080 --> 00:43:28.720]   everyone overnight switched. And that was mainly because there was not the hardware prior to 2012,
[00:43:29.600 --> 00:43:35.600]   there'd been iterations throughout the 2000s. But it was really GPUs coming along that allowed
[00:43:35.600 --> 00:43:40.000]   deep neural networks to train to the depth that they needed to show the empirical success.
[00:43:40.000 --> 00:43:46.320]   And for subsequent years, essentially for particularly for computer vision, and then
[00:43:46.320 --> 00:43:54.160]   now across multiple domains, that is what is dominated. But there are clear limitations to
[00:43:54.160 --> 00:43:58.240]   deep neural networks. And there are clear signs that perhaps this is not the only way forward,
[00:43:58.240 --> 00:44:03.200]   particularly for, and there's almost two lenses here. So there are many clear commercial use
[00:44:03.200 --> 00:44:08.400]   cases of deep neural networks. So I think Charles, you must bring a lot of people on your show who
[00:44:08.400 --> 00:44:14.080]   are really excited about how do we translate this breakthrough to commercial use cases. And I feel
[00:44:14.080 --> 00:44:20.000]   like there we're just, it's like we're at the beginning of the road. It's super fun road ahead.
[00:44:20.000 --> 00:44:26.080]   There's many different domains that we haven't yet enabled. I mean, this is part of the motivation
[00:44:26.080 --> 00:44:31.520]   for resource constrained environments, why I'm so interested in that. But there's another side,
[00:44:31.520 --> 00:44:38.880]   which is that in terms of an overarching goal of furthering this history of artificial intelligence
[00:44:38.880 --> 00:44:45.280]   research, the clear limitations of deep neural networks are starting to show. One of which is
[00:44:45.280 --> 00:44:52.160]   that it's becoming increasingly costly to scale. They're very expensive models, in the sense that
[00:44:52.160 --> 00:44:59.120]   we're throwing a lot of parameters at a problem for not much return. A good example of this is
[00:44:59.120 --> 00:45:05.360]   the enormous growth in weights of deep neural networks. So you go from inception V3 to inception
[00:45:05.360 --> 00:45:12.320]   V4, it's almost a doubling of weights, but you only get an extra two percentage points in test
[00:45:12.320 --> 00:45:21.920]   inaccuracy. So both the training cost as well as the decreasing returns are becoming apparent.
[00:45:21.920 --> 00:45:27.600]   But furthermore, there's key issues that deep neural networks that we don't see in other forms
[00:45:27.600 --> 00:45:32.880]   of biological intelligence, such as deep neural networks, evidence, catastrophic forgetting,
[00:45:32.880 --> 00:45:38.240]   which is a very snazzy word to mean that when you train deep neural networks on new data,
[00:45:38.240 --> 00:45:45.440]   it tends to forget what it was taught before, which humans don't have, because we show this
[00:45:45.440 --> 00:45:51.760]   remarkable versatility to pick up skills over the course of our lifetime, while retaining skills
[00:45:51.760 --> 00:45:58.480]   that we may have learned years ago. I think the other thing is that we show far more efficiency
[00:45:58.480 --> 00:46:07.600]   in terms of how we treat examples. So we often don't need to do a full forward and backward pass
[00:46:08.320 --> 00:46:12.560]   in fact, most of what you are seeing right now, what I'm seeing right now, unless you do something
[00:46:12.560 --> 00:46:19.600]   crazy, like put your hand out here, is virtual simulation. Like I've already simulated what I
[00:46:19.600 --> 00:46:25.200]   expect you to do. And if that matches what my prediction is, I don't do a full forward pass
[00:46:25.200 --> 00:46:32.560]   of the inputs. And that's usually efficient. It makes us able to do far more with far less.
[00:46:32.560 --> 00:46:41.520]   Our brain runs on the power of an electric shaver, razor. And in contrast, GPT-3 costs $12 million
[00:46:41.520 --> 00:46:51.600]   dollars to train. So all this to say, the limits of our current approach, the cliff of this hardware,
[00:46:51.600 --> 00:46:57.440]   software algorithm combination are becoming apparent. And that's why this was fun to write
[00:46:57.440 --> 00:47:04.000]   and to think about, because it also, in my own mind, forced me to think about what I would recommend
[00:47:04.000 --> 00:47:10.640]   to solve for this cliff. But I'm going to pause there, because I kind of talked a bit and
[00:47:10.640 --> 00:47:14.000]   hopefully gave a flavor of the work. And maybe I'll return it to you.
[00:47:14.000 --> 00:47:23.360]   Yeah, thanks for that context, both sort of, you know, how you came to like thinking about this
[00:47:23.360 --> 00:47:28.160]   problem and how you were thinking about it as, you know, at a meta level. And in addition, sort of
[00:47:28.160 --> 00:47:33.840]   what your presentation of the contents of the paper is all, yeah, very interesting. One thing that
[00:47:33.840 --> 00:47:43.600]   really stuck out to me, I think, was you referred a couple of times to biological intelligence as
[00:47:43.600 --> 00:47:50.960]   like an alternative, as a source for ideas. And so my PhD is in neuroscience. I was at the Redwood
[00:47:50.960 --> 00:47:56.320]   Center for Theoretical Neuroscience, which is a group that does a lot of sort of looking to biology
[00:47:56.320 --> 00:48:02.560]   for inspiration to design algorithms. And I think, yeah, we often found there's kind of this struggle
[00:48:02.560 --> 00:48:08.160]   that a lot of people sort of feel, you know, we learn to fly not by putting feathers on our wings,
[00:48:08.160 --> 00:48:13.280]   but by building jet engines. That's like a refrain you hear often from folks who are,
[00:48:13.280 --> 00:48:17.520]   especially folks who are working in deep learning and don't think that we should get our inspiration
[00:48:17.520 --> 00:48:24.800]   from biological systems. So I'm curious, you know, what do you think, you mentioned predictive
[00:48:24.800 --> 00:48:28.560]   coding, I guess, what other kinds of things do you think we can learn? Like, is it, you know,
[00:48:28.560 --> 00:48:35.920]   event-based, you know, spike-based models? Is it additional recurrence? Is it, yeah, what is it
[00:48:35.920 --> 00:48:42.480]   that you think biological intelligence can offer us as ways forward? I think it offers us counterfactual
[00:48:43.120 --> 00:48:50.880]   data points, right? So right now, essentially, because this journey, this mission is so recent.
[00:48:50.880 --> 00:48:57.840]   So the Dartmouth Council was in 1946. And since then, that was what really pulled together people
[00:48:57.840 --> 00:49:04.640]   first talking about transferring to machines, skills normally reserved for humans. But we're
[00:49:04.640 --> 00:49:11.440]   still less than a century out from when that occurred. And in many ways, our search space
[00:49:11.440 --> 00:49:18.160]   is very small. We have, and it's primarily dictated what combination of ideas we can explore,
[00:49:18.160 --> 00:49:22.800]   it's primarily dictated by the stack, by the degrees of freedom that we lose as we first
[00:49:22.800 --> 00:49:28.880]   choose our hardware, then we choose our software. So my point with biological examples is not to
[00:49:28.880 --> 00:49:35.520]   suggest this is the way. And in fact, I think I would be a foolish person to make that bet,
[00:49:36.720 --> 00:49:43.680]   particularly documented like this. But it's to say that we at least know there are things
[00:49:43.680 --> 00:49:49.840]   that we biologically do differently, that we are not doing with our current models. And the reason
[00:49:49.840 --> 00:49:55.920]   why that's important is that domain specialized hardware is really, really, really tailored to
[00:49:55.920 --> 00:50:02.720]   our current models. Essentially, the emphasis there is on making our current models commercially
[00:50:02.720 --> 00:50:09.600]   viable with a distant secondary consideration, distant, being future research directions.
[00:50:09.600 --> 00:50:16.640]   And so while I'm not going to take the bet that deep neural networks are not the way forward,
[00:50:16.640 --> 00:50:24.240]   I'm going to suggest that there are clearly multiple ways of mapping the world. For example,
[00:50:24.240 --> 00:50:29.520]   I work in computer vision a lot, thinking about the interpretability of computer vision,
[00:50:29.520 --> 00:50:35.520]   as well as the compression of computer vision models. Computer vision and CNNs in particular
[00:50:35.520 --> 00:50:42.720]   interesting because we often for interpretability expect these models to extract importance in the
[00:50:42.720 --> 00:50:48.400]   same way that we do. However, there's no such constraint that we train these models with to
[00:50:48.400 --> 00:50:55.200]   do that. In many ways, the reason why CNNs demonstrate this remarkable ability, particularly
[00:50:55.200 --> 00:51:01.760]   on medical images, to extract features that we can is that our vision is log scale on purpose.
[00:51:01.760 --> 00:51:07.840]   We need a noticeable change in order to register something happening for our own robustness.
[00:51:07.840 --> 00:51:12.960]   There's no such constraint on deep neural networks for the same thing. In fact, that's why deep
[00:51:12.960 --> 00:51:19.280]   neural networks can extract these pixel wise differences. And so things like that, I often think
[00:51:21.600 --> 00:51:26.560]   it's not so much that we articulate it as this is the way, it's more that we need to increase our
[00:51:26.560 --> 00:51:32.000]   search space and make it cheaper to explore that search space. I see. I do want to ask one more
[00:51:32.000 --> 00:51:39.360]   question before we pass over to the variants of gradient work, which is like what I saw as a big
[00:51:39.360 --> 00:51:45.520]   impediment was basically a certain amount of like conservatism and or on the part of granting
[00:51:45.520 --> 00:51:50.000]   agencies and of large companies that could possibly fund the kind of research necessary
[00:51:50.000 --> 00:51:58.080]   to break us out of this current iteration of the hardware lottery. On the one hand, you've got
[00:51:58.080 --> 00:52:03.120]   people doing really exciting new work constantly with neural networks. And then on the other hand,
[00:52:03.120 --> 00:52:11.360]   you have people who still are in the world of toy models saying, oh, I think the oscillation based
[00:52:11.360 --> 00:52:16.400]   or optical networks or spiking networks are really cool and that's what we should be spending money
[00:52:16.400 --> 00:52:22.320]   on. So do you think that there's a way that these institutions can be convinced at this, at this,
[00:52:22.320 --> 00:52:28.080]   you know, scale necessary to break us out of the, out of this lottery? Or do you think that
[00:52:28.080 --> 00:52:32.560]   you have a sort of pessimistic view that this is just a feature of the way research is done and
[00:52:32.560 --> 00:52:38.960]   we aren't going to break out of it? So it is a feature of how research is done. We're never
[00:52:38.960 --> 00:52:42.720]   going to eliminate the hardware lottery. I think the main point of the essay is that right now it
[00:52:42.720 --> 00:52:48.720]   dominates computer science progress and the degree to which it dominates is problematic. So
[00:52:48.720 --> 00:52:54.560]   in order for it to dominate less, we need to make it quicker to iterate and quicker to iterate in
[00:52:54.560 --> 00:53:02.160]   the hardware space is extremely expensive. You need a lot of, you know, bucks of whatever currency
[00:53:02.160 --> 00:53:08.080]   denomination you want to talk in. And that is the tricky part. So there needs to be more support
[00:53:08.080 --> 00:53:15.520]   from essentially, I would say governments because private sector is always going to lean towards
[00:53:15.520 --> 00:53:21.760]   hardware that's optimized for commercial use cases. And that has its benefits too. You need,
[00:53:21.760 --> 00:53:26.480]   you know, a mix of different people who are designing and thinking about these problems.
[00:53:26.480 --> 00:53:32.400]   But right now the gap in terms of government expenditure in this area is painful. It's,
[00:53:32.400 --> 00:53:37.280]   I mean, it's kind of fascinating. I think even the discrepancy between U.S. expenditure and
[00:53:37.280 --> 00:53:43.520]   Chinese expenditure, China is so it's thrown so much more into this ring than the U.S. has.
[00:53:43.520 --> 00:53:48.640]   And so it's really interesting to look at those dynamics as well. Software engineers have the
[00:53:48.640 --> 00:53:52.960]   highest burden on their shoulders. And at this point, we're just going to end up showing a slide
[00:53:52.960 --> 00:53:57.840]   from the other presentation, but that's okay. Cause I think, I think these topics are interesting.
[00:53:57.840 --> 00:54:02.160]   And you know, when you said it, let's do two, I just, I, the writing was on the wall. I was like,
[00:54:04.240 --> 00:54:10.960]   but no, it's software engineers who have the biggest burden to play and biggest role to play
[00:54:10.960 --> 00:54:19.200]   in many ways, because the tricky thing is, is that a lot of the bottleneck now is in the design of
[00:54:19.200 --> 00:54:25.040]   compilers. And it's also in the design of the software layers. And partly it's making us
[00:54:25.040 --> 00:54:29.440]   understand what is the cost of using different hardware, because right now it's just very costly
[00:54:29.440 --> 00:54:34.720]   to even try out different types of hardware. In my research flow, I basically have a single type
[00:54:34.720 --> 00:54:39.280]   of hardware that I use. If I need to release code and switch hardware, so that I'm using a different
[00:54:39.280 --> 00:54:45.760]   type, it's, it's like multiple days of switching out operations. It's not something I do experiment
[00:54:45.760 --> 00:54:53.360]   to experiment. And that's a great example of a pain point, which is really an engineering pain
[00:54:53.360 --> 00:54:58.000]   point. The secondary thing is that we just need better feedback loops. I mean, error messages
[00:54:58.320 --> 00:55:05.440]   at a hardware level are painful. I mean, as a, as a researcher, I want to care about hardware,
[00:55:05.440 --> 00:55:12.320]   but I don't have the time to, because it really is not a precise enough feedback loop for it to
[00:55:12.320 --> 00:55:16.960]   be actionable. And I think that's another main point here is that despite hardware and software
[00:55:16.960 --> 00:55:24.240]   mattering so much, researchers do not talk about it. And the reason why is that it's quite rational
[00:55:24.240 --> 00:55:29.280]   why we don't talk about it. It's because it's seen as something we can't change. It's seen as
[00:55:29.280 --> 00:55:36.240]   something so, so time consuming to try and influence that we almost abstracted away.
[00:55:36.240 --> 00:55:43.040]   And I think that that's the interesting part is how can we make it cheaper for us to engage in
[00:55:43.040 --> 00:55:48.560]   that conversation? I am going to put up the slide because I do want my lovely co-author, Sharad,
[00:55:49.920 --> 00:55:54.080]   he's my co-first story, he's fantastic. So let me...
[00:55:54.080 --> 00:55:58.400]   I was honest to goodness really excited about this paper. So, you know, if you can stick around for
[00:55:58.400 --> 00:56:00.880]   an extra five minutes, I can spend 10 minutes on it.
[00:56:00.880 --> 00:56:09.440]   Oh, okay. Yeah, I'm happy to. I'll at least put this up while we, because we only have a few
[00:56:09.440 --> 00:56:13.280]   minutes left. So we'll stay on the hardware lottery, and then I'll do a brief summary for the
[00:56:13.280 --> 00:56:20.400]   next, for people who want to stick around. But yeah, the paper with Sharad, who is my lovely
[00:56:20.400 --> 00:56:27.840]   co-first author, and this paper is about estimating example difficulty. And so I'm going to do a
[00:56:27.840 --> 00:56:34.000]   little bit of a hat switcheroo here. And I want to talk about really another property of beyond
[00:56:34.000 --> 00:56:39.840]   tested accuracy, which is how do we audit our models? And how do we surface an understanding
[00:56:39.840 --> 00:56:48.400]   of how the models learn a decision boundary to people who need to navigate and to make decisions
[00:56:48.400 --> 00:56:56.000]   about AI safety. And the big obstacle in terms of ranking examples by difficulty to date has been
[00:56:56.000 --> 00:57:01.440]   the computational cost, it's very expensive. And so the work with Sharad is really proposing a much
[00:57:01.440 --> 00:57:06.640]   cheaper measure to achieve this. In fact, what we leverage is the variance of the gradients during
[00:57:06.640 --> 00:57:11.280]   the training cycle as well. So it fits into your traditional training regime where you save
[00:57:11.280 --> 00:57:16.640]   checkpoints as you train, you can leverage those checkpoints to understand what examples are
[00:57:16.640 --> 00:57:22.800]   challenging and what are easy by looking at which gradients converge the fastest. And it turns out to
[00:57:22.800 --> 00:57:29.040]   be very effective at servicing the most problematic examples for humans to take a look at, either for
[00:57:29.040 --> 00:57:37.360]   further annotation or to audit for accountability and fairness. So that was very jazzy. I think I
[00:57:37.360 --> 00:57:42.240]   got that under two minutes, if that's possible. That was quite a compact summary.
[00:57:42.240 --> 00:57:49.680]   Yeah. Well compressed, you know. So makes me wonder what that perhaps forgot. But
[00:57:49.680 --> 00:57:56.720]   yeah, I mean, if you want to spend like a little bit talking about the,
[00:57:56.720 --> 00:58:01.760]   maybe showing some of the results or the key figures of the paper, just in the, you know,
[00:58:01.760 --> 00:58:05.520]   folks will stick around, I think, for a couple minutes. And we post these on YouTube afterwards,
[00:58:05.520 --> 00:58:10.640]   and folks, I'm sure would be happy to sit. Oh, I see. Okay, so I can go rogue. Okay,
[00:58:10.640 --> 00:58:17.200]   you can do whatever. I'll go rogue. Maybe I'll even squeeze in eight minutes. That's fun.
[00:58:17.760 --> 00:58:27.840]   All right. So if the so, yeah, absolutely. And a lot of what this research is focused on,
[00:58:27.840 --> 00:58:32.320]   and I guess I already spoke to this, but my research is really on this area of going beyond
[00:58:32.320 --> 00:58:38.960]   test accuracy. And that's because we know that, and perhaps I'll just present this.
[00:58:40.480 --> 00:58:50.080]   Lovely. Okay, we're rolling. And a lot of the motivation for this work is that we often have
[00:58:50.080 --> 00:58:56.320]   accuracy without true learning. And so top line metrics often hide critical model behavior. And
[00:58:56.320 --> 00:59:01.760]   that's really the role these interpretability tools in my mind is to provide intuition and
[00:59:01.760 --> 00:59:09.600]   to find meaningful examples for humans to be able to audit a lot of the issues. And a lot of work
[00:59:09.600 --> 00:59:16.880]   to date on interpretability has focused on local explanations. And what is the difference? So local
[00:59:16.880 --> 00:59:23.680]   explanation, you may have seen a saliency map before. They're widely used by practitioners,
[00:59:23.680 --> 00:59:29.440]   where essentially, for a given example, you estimate what is important for the model prediction.
[00:59:29.440 --> 00:59:36.320]   So this is, I guess it's a sandwich. Is it a meat sandwich, perhaps? It's like a pulled meat,
[00:59:37.520 --> 00:59:41.600]   some variety of sandwich, and the model is highlighted these parts of the sandwich as
[00:59:41.600 --> 00:59:47.040]   contributing the most to the model prediction. And the reason why a lot of work on interpretability
[00:59:47.040 --> 00:59:52.160]   has been limited to these single examples, there's actually a few reasons. So one is the high
[00:59:52.160 --> 00:59:56.880]   dimensional input space. So it's just so expensive to try and scale feature importance across the
[00:59:56.880 --> 01:00:03.840]   data set, because even a single image may have a quarter of a million features. And the other is
[01:00:03.840 --> 01:00:10.320]   that, in fact, a lot of the onus has been on providing single end user explanations. So for
[01:00:10.320 --> 01:00:16.320]   example, if I'm in a doctor's office, I'm always going to want the explanation from my x ray. And
[01:00:16.320 --> 01:00:20.080]   you can imagine multiple use cases in which you will always want to try and explain a single
[01:00:20.080 --> 01:00:27.680]   prediction. However, a lot of my earlier research was showing that essentially, meaningful
[01:00:27.680 --> 01:00:32.400]   explanation does not always equate with reliable. And so you can have these saliency maps, which
[01:00:32.400 --> 01:00:37.920]   look very meaningful, but can be easily manipulated. And in fact, we show that we show that
[01:00:37.920 --> 01:00:45.760]   it can be manipulated to show a kitty imposed on top of a seven, which is fun. And we also did
[01:00:45.760 --> 01:00:50.400]   work, which was showing that often these exam, these estimates of feature importance are no
[01:00:50.400 --> 01:00:56.480]   better than a random estimate. And so a lot of my recent research is focused on what next.
[01:00:57.440 --> 01:01:03.920]   And what next is really thought about what happens after a human sees a saliency map,
[01:01:03.920 --> 01:01:12.880]   what actionable decision do they make? And if I were to ask you, Chris, essentially,
[01:01:12.880 --> 01:01:18.400]   what would you, Charles, what would you do when you saw this? You may actually struggle to
[01:01:18.400 --> 01:01:25.920]   make a decision. Is this a good saliency map or is this a bad saliency map? And in fact,
[01:01:25.920 --> 01:01:31.040]   you may want to look at additional saliency maps. And this is normal because a lot of our
[01:01:31.040 --> 01:01:37.920]   understanding of what is good or bad is relative. And this is where it becomes tricky because
[01:01:37.920 --> 01:01:43.200]   can a human inspect all the saliency maps of a dataset, particularly with training datasets now
[01:01:43.200 --> 01:01:50.480]   that are extremely large? And so really this question of how do we surface what is important
[01:01:50.480 --> 01:01:55.120]   for a domain expert to understand and particularly to surface attractable subsets, something that
[01:01:55.120 --> 01:02:00.800]   they can actually within their annotation budget or inspection budget, be able to understand the
[01:02:00.800 --> 01:02:07.440]   behavior of the model. And this is the work with Chirag, who's my lovely collaborator. And really,
[01:02:07.440 --> 01:02:11.280]   the work revolves around how do we understand how feature importance forms of the course of
[01:02:11.280 --> 01:02:16.560]   training? So what you see here is there's very interesting research which suggests that there's
[01:02:16.560 --> 01:02:26.640]   distinct stages of training. So, for example, the chart on your left, I believe, yes, your left,
[01:02:26.640 --> 01:02:35.840]   is critical learning periods and deep neural networks. And so, in fact, this is a chart which
[01:02:35.840 --> 01:02:41.600]   shows that, in fact, if you train a model on corrupted data early on, it can no longer converge
[01:02:41.600 --> 01:02:46.480]   to better data later on, suggesting that some special attributes that early stage of training
[01:02:46.480 --> 01:02:52.800]   that determine how feature importance is formed. And we leverage this fact in this work. And,
[01:02:52.800 --> 01:02:59.040]   in fact, what we say is the hypothesis that we formulate is that for easy examples,
[01:02:59.040 --> 01:03:04.080]   the model will converge quickly on a fairly narrow range of gradients because it's consistently
[01:03:04.080 --> 01:03:09.760]   predicting the same prediction for that image. For hard examples, the model is trying to place
[01:03:09.760 --> 01:03:14.160]   the image at different parts of the decision boundary. And so, in fact, it will keep on
[01:03:14.160 --> 01:03:18.480]   oscillating the gradients and we'll see higher average variance in the gradients.
[01:03:18.480 --> 01:03:23.120]   And so that's what we do. We look at checkpoints of the course of training in the same way that
[01:03:23.120 --> 01:03:28.560]   you would typically save your checkpoints in a normal training regime. And then we compute the
[01:03:28.560 --> 01:03:34.480]   input gradients and we look at the variance of that image consistently over the course of training.
[01:03:35.280 --> 01:03:42.560]   And what we find is that it actually provides a really useful ranking of each class. And how do
[01:03:42.560 --> 01:03:49.440]   we evaluate that? So we qualitatively visualize the lowest VOG and the highest. And you can kind
[01:03:49.440 --> 01:03:54.480]   of see that they have these very distinct semantic clustering traits. So lowest VOG
[01:03:54.480 --> 01:04:01.120]   has these much more, I guess, centered, crisp images. Highest VOG is much more cluttered.
[01:04:02.160 --> 01:04:12.240]   But we also look qualitatively and we see that VOG is much more effective at discriminating
[01:04:12.240 --> 01:04:18.560]   between easy and challenging examples. So this is CFAR 100 and this is the bottom 10.
[01:04:18.560 --> 01:04:26.640]   So the bottom 10 percentile of VOG. And you can see it's much lower test accuracy. And then,
[01:04:26.640 --> 01:04:31.920]   oh, apologies. And then this is the top 10 percentile. And you can see it's much higher
[01:04:31.920 --> 01:04:38.720]   test set accuracy. And so VOG is doing a great job of ranking the entire dataset so that it surfaces
[01:04:38.720 --> 01:04:44.960]   as high score the most difficult examples to evaluate. And the benefits is that it really
[01:04:44.960 --> 01:04:51.200]   fits into Patricia's current workflow. And you can leverage the checkpoint stored across training.
[01:04:51.200 --> 01:04:54.560]   I'm not going to talk about these other two works, which essentially try and get at this
[01:04:56.160 --> 01:05:04.240]   common theme of ranking and surfacing. But a lot of it is how do we leverage differences in models
[01:05:04.240 --> 01:05:08.720]   to surface what is worthy of auditing, which is a theme that I think is very important for
[01:05:08.720 --> 01:05:13.520]   interpretability, particularly as we try and apply it to production systems and figure out how to
[01:05:13.520 --> 01:05:20.720]   equip humans to check before they deploy if there are any potential issues with the model. So I'm
[01:05:20.720 --> 01:05:28.560]   going to stop there and maybe I'll turn it back over to you, Charles. Yeah, I think maybe just a
[01:05:28.560 --> 01:05:34.960]   couple of questions before we go. You know, I at least want to catch the U.S. presidential debate.
[01:05:34.960 --> 01:05:41.520]   Hopefully they'll discuss how to appropriately respond to the hardware lottery and the need for
[01:05:41.520 --> 01:05:47.840]   additional investment in hardware. But I don't have high hopes that that will be discussed.
[01:05:48.560 --> 01:05:54.480]   A question from Swear Shah. Do you know whether this approach of looking at variance and gradients,
[01:05:54.480 --> 01:05:58.400]   would this work for models like semantic segmentation? Or do you think this is
[01:05:58.400 --> 01:06:04.160]   something that might be sort of more specific to classification and computer vision?
[01:06:04.160 --> 01:06:10.480]   You can use it for semantic segmentation. The formulation just has to be changed a little
[01:06:10.480 --> 01:06:16.480]   because you end up predicting an area. But actually, there's an internal team at Google
[01:06:16.480 --> 01:06:25.280]   right now that's testing it for segmentation tasks. So partly, I believe we will be open
[01:06:25.280 --> 01:06:32.000]   sourcing the code within the next two weeks. So I think we could also include a variant for that.
[01:06:32.000 --> 01:06:37.920]   Great. Yeah, it'd be interesting to see. I guess the one other domain in which it can often be
[01:06:37.920 --> 01:06:46.720]   hard to translate from computer vision is the reinforcement learning, deep reinforcement
[01:06:46.720 --> 01:06:49.760]   learning. I feel like every time I look at the papers in that field, they're using different
[01:06:49.760 --> 01:06:53.440]   optimizers and they're thinking about the problems very differently. So you think that looking at
[01:06:53.440 --> 01:06:59.280]   some sort of variance of reward or variance of prediction error or something like that might be
[01:06:59.280 --> 01:07:06.720]   able to translate this to figure out where our agents are most confused? In some ways, the primary
[01:07:06.720 --> 01:07:14.880]   hypothesis of this is that the gradient itself, the variance of it, the degree to which it changes
[01:07:14.880 --> 01:07:20.000]   at different intervals is indicative of uncertainty. And that applies to any model
[01:07:20.000 --> 01:07:26.720]   that's trained in a gradient optimization-based approach, which is kind of all of them at this
[01:07:26.720 --> 01:07:34.960]   point, or at least a lot of state-of-art models in different fields. So I think it's interesting
[01:07:34.960 --> 01:07:40.240]   to think about. There you naturally have just a lot more noise to begin with, because essentially
[01:07:40.240 --> 01:07:45.840]   you also have task switching, but also the intervals at which you do task switching are
[01:07:45.840 --> 01:07:54.240]   often not predetermined. So the one caveat I would say is that we do class normalized
[01:07:54.240 --> 01:08:00.800]   variance of gradients, because essentially we look at class rankings. It's far less clear how
[01:08:00.800 --> 01:08:05.440]   you would do that in a setting where you don't have a distinct class or you don't have a crisp
[01:08:05.440 --> 01:08:10.320]   sense of demarcation. But that's kind of cool to think about, because perhaps you don't need that.
[01:08:10.320 --> 01:08:16.880]   Maybe you can just use the roar variance. So yeah, I see you nodding, Charles. If you want to take
[01:08:16.880 --> 01:08:22.240]   this up as a research direction, let me know. I don't know. I don't have much time for research
[01:08:22.240 --> 01:08:26.320]   these days. I'm mostly doing webinars in which I ask people about their really cool research,
[01:08:26.320 --> 01:08:32.480]   so there's not so much time. Yeah, fair. You can't optimize for it all. Something has to give.
[01:08:32.480 --> 01:08:39.120]   I do want to - I guess I got one more question before I let you go. So one thing I notice when
[01:08:39.120 --> 01:08:43.840]   I look at the distributions of my gradients, these are marginal distributions, so not class specific,
[01:08:43.840 --> 01:08:50.800]   but I see this incredible degree of sparsity. I see almost like a Cauchy or a Laplace distribution,
[01:08:50.800 --> 01:08:56.480]   where it's like almost all zero, and then there's heavy tails. Do you think that the higher moments,
[01:08:56.480 --> 01:09:00.640]   like skew and kurtosis, have anything to say? Or do you think that the variance of gradients
[01:09:00.640 --> 01:09:09.840]   basically captures all that there is to capture in those higher moments? Well, I guess in many
[01:09:09.840 --> 01:09:14.880]   ways what we're capturing with variance is time, right? So we're looking at different snapshots
[01:09:14.880 --> 01:09:19.760]   across training, which is really the information that's being conveyed, because our primary
[01:09:19.760 --> 01:09:24.320]   assumption is that time, after a certain amount of time, the model has already learned the easy
[01:09:24.320 --> 01:09:30.800]   examples, and so the variance will be downweighted by the convergence over time, and for hard,
[01:09:30.800 --> 01:09:35.840]   it will just keep on oscillating. So in that sense, I don't know if a single snapshot is
[01:09:35.840 --> 01:09:42.400]   competitive. That's partly why I think we had such cool clustering, and it was so exciting,
[01:09:42.400 --> 01:09:46.960]   because this is a far cheaper way to get at much more expensive ways that have attempted in the
[01:09:46.960 --> 01:09:52.560]   past to do this by looking at a single snapshot. Like there's a fantastic paper which was about
[01:09:52.560 --> 01:09:58.240]   influence functions, which is about single snapshot, but it's extremely costly. There's
[01:09:58.240 --> 01:10:02.160]   another paper about C-score, which is super interesting, but it requires like 20,000
[01:10:02.160 --> 01:10:10.160]   retraining of your model. So this is in many ways, the exciting part of this is that it's a cheap
[01:10:10.160 --> 01:10:16.720]   proxy to get at the single snapshot, much more complex formulation of the ranking.
[01:10:16.720 --> 01:10:23.920]   Gotcha. Yeah. Yeah. Maybe no need to overcomplicate it by calculating, say,
[01:10:23.920 --> 01:10:28.560]   the entropy of the gradient, when you can probably well approximate the behavior of
[01:10:28.560 --> 01:10:37.840]   that with something like the variance. Okay. Well, I'm going to get out of here and go and
[01:10:37.840 --> 01:10:43.040]   catch that debate, but thank you so much, Sarah, for coming and being willing to talk about your
[01:10:43.040 --> 01:10:49.440]   work and answer everyone's questions. It was a real pleasure. Yes, lovely. Yeah. Nice chatting.
[01:10:49.440 --> 01:10:56.160]   Bye, Charles. Take care. And take care. Oh, Hannes, you stuck around. Glad you got a chance
[01:10:56.160 --> 01:11:01.280]   to see the talk as well. Thanks to everyone who showed up and who asked questions, and thanks to
[01:11:01.280 --> 01:11:07.200]   Hannes as well. All right. Bye, everyone. See you in two weeks.
[01:11:07.200 --> 01:11:15.640]   [BLANK_AUDIO]

