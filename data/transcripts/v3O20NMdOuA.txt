
[00:00:00.000 --> 00:00:03.760]   [MUSIC]
[00:00:03.760 --> 00:00:07.900]   I think people see the output models like NoDolly,
[00:00:07.900 --> 00:00:13.320]   GPT-3, etc., and they're amazed by what AI can do.
[00:00:13.320 --> 00:00:16.840]   And so the conversation doesn't even hinge on like,
[00:00:16.840 --> 00:00:21.480]   we have access to this dataset or we have access to this talent pool.
[00:00:21.480 --> 00:00:26.440]   It's more, AI is magical, what can we do with it?
[00:00:26.440 --> 00:00:30.360]   And again, I think that is somewhat dangerous.
[00:00:30.360 --> 00:00:32.240]   >> You're listening to Gradient Dissent,
[00:00:32.240 --> 00:00:34.760]   a show about machine learning in the real world.
[00:00:34.760 --> 00:00:36.400]   And I'm your host, Lucas Biewald.
[00:00:36.400 --> 00:00:42.560]   Sarah Catanzaro was a practicing data scientist and then went into venture.
[00:00:42.560 --> 00:00:45.360]   She's currently a general partner at Amplify Partners and
[00:00:45.360 --> 00:00:48.800]   one of the leading investors in AI and ML.
[00:00:48.800 --> 00:00:52.800]   Her investments include a whole bunch of companies I admire like RunwayML,
[00:00:52.800 --> 00:00:55.840]   OctoML, Gantry, and others.
[00:00:55.840 --> 00:00:59.720]   It's really interesting to talk to an investor who's also technical.
[00:00:59.720 --> 00:01:03.560]   She has insights both on how the technology is built and
[00:01:03.560 --> 00:01:06.440]   how it's being adopted by the market at large.
[00:01:06.440 --> 00:01:08.840]   This is a really fun conversation and I hope you enjoy it.
[00:01:08.840 --> 00:01:11.280]   Sarah, thanks so much for doing this.
[00:01:11.280 --> 00:01:13.000]   I've been looking forward to this one.
[00:01:13.000 --> 00:01:15.680]   I had a bunch of questions prepped and then I was looking at your Twitter and
[00:01:15.680 --> 00:01:18.240]   I was like, ooh, there's a whole bunch of other stuff that we should-
[00:01:18.240 --> 00:01:22.360]   >> [LAUGH] Yeah, yeah.
[00:01:22.360 --> 00:01:28.160]   I feel like I've been doing a lot of thinking out loud recently,
[00:01:28.160 --> 00:01:35.080]   including in response to a lot of the hype around stable diffusion, LLMs, etc.
[00:01:35.080 --> 00:01:41.560]   I appreciate the fact that both of us were there in the 2013,
[00:01:41.560 --> 00:01:46.560]   2014 phase where every company was claiming to be an AI company.
[00:01:46.560 --> 00:01:50.560]   And it feels like we're kind of heading down that road again,
[00:01:50.560 --> 00:01:52.280]   which scares me a little bit.
[00:01:52.280 --> 00:01:58.200]   >> [LAUGH] >> And I hope at least there are enough
[00:01:58.200 --> 00:02:03.920]   companies, people who remember the lessons of the last AI renaissance.
[00:02:03.920 --> 00:02:05.680]   But we'll see.
[00:02:05.680 --> 00:02:11.600]   >> Well, let's get right into it then because I'll say from my perspective,
[00:02:11.600 --> 00:02:15.920]   I totally remember at least one other AI bubble,
[00:02:15.920 --> 00:02:18.600]   maybe more depending on how you count it.
[00:02:18.600 --> 00:02:24.520]   But I guess from where I sit, it feels like this one might be different in
[00:02:24.520 --> 00:02:29.040]   the sense that I feel like these challenges that were always seemed like super,
[00:02:29.040 --> 00:02:33.040]   super hard, seem like they're really working.
[00:02:33.040 --> 00:02:39.520]   And I feel like I see applications happening unbelievably
[00:02:39.520 --> 00:02:42.400]   fast after the paper comes out, actually,
[00:02:42.400 --> 00:02:46.720]   even maybe before there's time to even publish any paper on the topic.
[00:02:46.720 --> 00:02:51.760]   So I think I might be more bullish about large language models and
[00:02:51.760 --> 00:02:55.080]   stable diffusion than you, which is great because we can actually have
[00:02:55.080 --> 00:02:56.880]   an interesting conversation here.
[00:02:56.880 --> 00:03:02.840]   But I thought it's interesting, you've invested in Runway.
[00:03:02.840 --> 00:03:08.800]   And just the other day, Chris was showing me a natural language input into
[00:03:08.800 --> 00:03:13.400]   Runway where you could basically type what you want and it would sort of set up
[00:03:13.400 --> 00:03:15.920]   the video editing to work that way.
[00:03:15.920 --> 00:03:19.760]   And I thought, oh my gosh, this might be a totally new kind of
[00:03:19.760 --> 00:03:24.640]   interface that lots of software might quickly adopt.
[00:03:24.640 --> 00:03:27.200]   I guess, but it sounds like, looking at your Twitter,
[00:03:27.200 --> 00:03:29.760]   it sounds like you were playing with large language models and
[00:03:29.760 --> 00:03:32.680]   finding it super frustrating and broken.
[00:03:32.680 --> 00:03:33.640]   Tell me about that.
[00:03:33.640 --> 00:03:34.240]   Yeah.
[00:03:34.240 --> 00:03:39.880]   So I think my concern is less about the capabilities of large language
[00:03:39.880 --> 00:03:45.600]   models specifically and more about some of the lessons that we learned during
[00:03:45.600 --> 00:03:53.920]   the last AI renaissance, which I think was roughly 2014 to maybe 2017,
[00:03:53.920 --> 00:03:58.720]   around the time that Elphago came out.
[00:03:58.720 --> 00:04:04.520]   People were really excited about the capabilities of GANs and RLs.
[00:04:04.520 --> 00:04:10.840]   And at the time, I remember companies like Airbnb, Uber, Lyft,
[00:04:10.840 --> 00:04:16.440]   building these big research teams, but not really having a clear agenda
[00:04:16.440 --> 00:04:22.000]   for those research teams or understanding how the objectives of their
[00:04:22.000 --> 00:04:30.800]   research teams might align with the objectives of the broader organization.
[00:04:30.800 --> 00:04:35.880]   And then similarly, you saw all of these startup founders emerge that were
[00:04:35.880 --> 00:04:44.040]   talking about changing healthcare with GANs or changing finance with RL,
[00:04:44.040 --> 00:04:50.160]   but didn't really have insights into the nuances of those industries.
[00:04:50.160 --> 00:04:56.520]   And so my feeling of why ML didn't work the last time around, or rather
[00:04:56.520 --> 00:05:01.240]   why ML adoption didn't occur at the pace that we anticipated, was that it
[00:05:01.240 --> 00:05:09.680]   was not really a technical problem, but rather a product go-to-market problem.
[00:05:09.680 --> 00:05:15.240]   I am hoping that this time around, we've both learned from our mistakes,
[00:05:15.240 --> 00:05:20.120]   but also that in the intervening time period, created enough enabling
[00:05:20.120 --> 00:05:26.920]   technologies such that two things can occur.
[00:05:26.920 --> 00:05:32.000]   One is that companies can fail fast.
[00:05:32.000 --> 00:05:36.480]   Frankly, one of the things that scares me is that back then, I remember
[00:05:36.480 --> 00:05:40.960]   a bunch of companies reaching out and basically saying things like,
[00:05:40.960 --> 00:05:43.120]   "Hey, we've got a bunch of data.
[00:05:43.120 --> 00:05:47.440]   We'd love for you to come in and talk to us about our AI strategy."
[00:05:47.440 --> 00:05:50.960]   And thinking, "I don't care if you have a bunch of data.
[00:05:50.960 --> 00:05:55.960]   Let's talk about a bunch of problems that you have and how ML can
[00:05:55.960 --> 00:05:59.200]   solve those problems."
[00:05:59.200 --> 00:06:05.280]   I've come to believe that you can't fight that urge, that founders will
[00:06:05.280 --> 00:06:08.720]   always be enticed by the promise of AI.
[00:06:08.720 --> 00:06:13.920]   But if they're able to experiment with it quickly, then I think they can
[00:06:13.920 --> 00:06:18.400]   start to learn more about the infrastructure and data and other
[00:06:18.400 --> 00:06:23.640]   investments that they may need to make in order for their AI initiatives
[00:06:23.640 --> 00:06:26.200]   to be successful.
[00:06:26.200 --> 00:06:32.320]   At the same time, I think by creating these higher-level interfaces that
[00:06:32.320 --> 00:06:39.960]   make ML more accessible to potentially the domain expert, it allows people
[00:06:39.960 --> 00:06:45.480]   with a more thorough understanding of business problems to at least
[00:06:45.480 --> 00:06:48.160]   prototype AI solutions.
[00:06:48.160 --> 00:06:52.320]   I'm somewhat skeptical that these very high-level interfaces will allow them
[00:06:52.320 --> 00:06:58.800]   to build production ML at scale, but at least they can see, "Does it work?
[00:06:58.800 --> 00:07:04.720]   And do I need to now hire a data and ML team to realize this initiative
[00:07:04.720 --> 00:07:05.720]   further?"
[00:07:05.720 --> 00:07:09.760]   Do you have companies in mind that you like that are creating these
[00:07:09.760 --> 00:07:16.720]   higher-level interfaces off of ML technology that makes them usable for
[00:07:16.720 --> 00:07:18.400]   real-world applications?
[00:07:19.000 --> 00:07:20.000]   >> Yeah.
[00:07:20.000 --> 00:07:25.320]   So, I mean, I think Renwe is actually a perfect example of the phenomena
[00:07:25.320 --> 00:07:28.280]   that I see playing out.
[00:07:28.280 --> 00:07:31.840]   Some people may not know, but Renwe actually kind of started off more as
[00:07:31.840 --> 00:07:34.160]   a model marketplace.
[00:07:34.160 --> 00:07:38.320]   Their goal had been to make GANs and other types of models accessible
[00:07:38.320 --> 00:07:44.640]   to creative professionals, but they weren't really focused on building
[00:07:44.640 --> 00:07:49.640]   out the video editing tools, at least initially.
[00:07:49.640 --> 00:07:53.000]   So they created these higher-level interfaces such that various creative
[00:07:53.000 --> 00:08:01.280]   professionals, whether it was artists or directors or photographers, could
[00:08:01.280 --> 00:08:05.120]   start to experiment with ML models.
[00:08:05.120 --> 00:08:10.720]   And what they saw was that some of the most popular models were models that
[00:08:10.720 --> 00:08:14.480]   automated routine tasks associated with video editing.
[00:08:14.480 --> 00:08:19.360]   And so based on that user behavior, they decided to double down on video
[00:08:19.360 --> 00:08:20.360]   editing.
[00:08:20.360 --> 00:08:24.200]   And in fact, a lot of the model architectures that they've since created,
[00:08:24.200 --> 00:08:29.880]   including stable diffusion, were really purpose-built to support the
[00:08:29.880 --> 00:08:32.960]   workflows of video editors.
[00:08:32.960 --> 00:08:40.920]   So I like that sort of workflow where you use a prototype or you use these
[00:08:40.920 --> 00:08:46.880]   higher-level interfaces to get insight into what users need, as well as
[00:08:46.880 --> 00:08:50.360]   potentially the limitations of the underlying technology.
[00:08:50.360 --> 00:08:53.200]   And then you iterate from there.
[00:08:53.200 --> 00:08:57.680]   So I totally remember a time, I think of the era you're talking about, 2014
[00:08:57.680 --> 00:09:03.400]   to 2017, when every company was like, "Oh, we have this data.
[00:09:03.400 --> 00:09:08.240]   It must be valuable because we can build a model on top of it."
[00:09:08.240 --> 00:09:11.920]   Do you see some analogy today to that?
[00:09:11.920 --> 00:09:20.960]   What's the common request of a ML team that's misguided or should be thinking
[00:09:20.960 --> 00:09:21.960]   more about problems?
[00:09:21.960 --> 00:09:28.000]   Because I feel like data maybe isn't seeming quite as valuable in a world of
[00:09:28.000 --> 00:09:30.360]   LLMs and big models.
[00:09:30.360 --> 00:09:31.360]   Yeah.
[00:09:31.480 --> 00:09:39.600]   So I think what we're seeing today is arguably more nefarious than what we saw
[00:09:39.600 --> 00:09:44.920]   back then because at least at that point in time, companies had invested in
[00:09:44.920 --> 00:09:46.000]   collecting data.
[00:09:46.000 --> 00:09:50.200]   They had thought about possibly what data to collect.
[00:09:50.200 --> 00:09:54.160]   And so there was some understanding of how to work with data.
[00:09:54.160 --> 00:10:00.760]   I think people see the output models like Dolly, GPT-3, et cetera, and they're
[00:10:00.760 --> 00:10:03.720]   amazed by what AI can do.
[00:10:03.720 --> 00:10:08.560]   And so the conversation doesn't even hinge on like, "We have access to this
[00:10:08.560 --> 00:10:14.120]   dataset," or "We have access to this talent pool," or "We have this type of
[00:10:14.120 --> 00:10:21.200]   workflow that could benefit from these generative capabilities."
[00:10:21.200 --> 00:10:24.440]   It's more, "AI is magical.
[00:10:24.440 --> 00:10:26.560]   What can we do with it?
[00:10:26.560 --> 00:10:28.880]   Come in and talk to us about this."
[00:10:28.880 --> 00:10:33.920]   And again, I think that is somewhat dangerous.
[00:10:33.920 --> 00:10:42.760]   I was at a conference just last week, and there was a presentation on ML
[00:10:42.760 --> 00:10:46.280]   infrastructure at an music company.
[00:10:46.280 --> 00:10:53.200]   And somebody in the audience asked, "Does the AI listen to songs?"
[00:10:53.200 --> 00:10:59.840]   And it's a perfectly reasonable question, but I think it does kind of belies some
[00:10:59.840 --> 00:11:03.560]   of the misunderstanding of AI and how it works.
[00:11:03.560 --> 00:11:05.960]   I guess in what sense?
[00:11:05.960 --> 00:11:11.960]   I think people think about AI as artificial agents.
[00:11:11.960 --> 00:11:17.840]   They think of AI as something that could listen to a song, not just something that
[00:11:17.840 --> 00:11:29.400]   could represent a song and make predictions based upon the content of that song.
[00:11:29.400 --> 00:11:41.760]   So again, I think better understanding what LLMs are and what they can do will be
[00:11:41.760 --> 00:11:47.040]   really necessary to identify when they can be useful.
[00:11:47.040 --> 00:11:53.840]   This might sound like a softball, but I was genuinely interested in this.
[00:11:53.840 --> 00:11:58.120]   I feel like one of the things that you do really well, at least in my conversations
[00:11:58.120 --> 00:12:06.400]   with you, is maintain a pretty deep technical and current knowledge of what's going on in
[00:12:06.400 --> 00:12:11.080]   data stacks, basically, or data infrastructure and ML infrastructure.
[00:12:11.080 --> 00:12:17.080]   But yet, you're not maintaining data infrastructure as far as I know.
[00:12:17.080 --> 00:12:23.400]   I'm kind of curious how you stay on top of a field that seems like it requires such hands-on
[00:12:23.400 --> 00:12:27.640]   engagement to understand it well, or at least I feel like it does for me.
[00:12:27.640 --> 00:12:31.160]   Yeah, just curious what your process is.
[00:12:31.160 --> 00:12:32.160]   Yeah.
[00:12:32.160 --> 00:12:39.120]   So it's interesting because I'd say that in some ways that is one of my biggest concerns.
[00:12:39.120 --> 00:12:42.120]   I've been in venture now for about seven years.
[00:12:42.120 --> 00:12:47.480]   And so I can still say that I spent most of my career in data.
[00:12:47.480 --> 00:12:52.240]   But it won't be long before that is no longer true.
[00:12:52.240 --> 00:13:00.600]   And certainly I have found that my practical technical skills have gotten rustier.
[00:13:00.600 --> 00:13:07.360]   One comment on that is that I do think that losing my Python SQL skills, et cetera, has
[00:13:07.360 --> 00:13:14.080]   actually enabled me to look at some of the tools and platforms that are available to
[00:13:14.080 --> 00:13:17.040]   users today with a fresh set of eyes.
[00:13:17.040 --> 00:13:24.720]   I'm not as entrenched in the same patterns of behavior and workflows as I was when I
[00:13:24.720 --> 00:13:26.240]   was a practitioner.
[00:13:26.240 --> 00:13:30.600]   So it's been helpful to shed some of my biases.
[00:13:30.600 --> 00:13:38.440]   But I think what I've discovered is that you can understand how something works without
[00:13:38.440 --> 00:13:39.760]   using it.
[00:13:39.760 --> 00:13:47.200]   And therefore, there are two things that are kind of critical to building technical understanding
[00:13:47.200 --> 00:13:48.760]   for me.
[00:13:48.760 --> 00:13:55.240]   One is just spending a lot of time with practitioners and hearing about their experiences, how they're
[00:13:55.240 --> 00:14:01.040]   using various tools, how they're thinking about various sets of technologies.
[00:14:01.040 --> 00:14:06.160]   And frankly, just learning from them, it almost feels like a shortcut.
[00:14:06.160 --> 00:14:14.200]   Like hey, instead of trying to figure out what the difference is between automated prompting
[00:14:14.200 --> 00:14:22.440]   and prefix tuning, just going to ask somebody and have a conversation with them.
[00:14:22.440 --> 00:14:27.440]   Which is kind of like coincidental and perhaps even ironic.
[00:14:27.440 --> 00:14:34.360]   And accelerate my learning by just learning from people with expertise in those areas.
[00:14:34.360 --> 00:14:41.040]   So there's a lot that I just learned through conversation with practitioners.
[00:14:41.040 --> 00:14:52.160]   But I think going one level deeper, either reading white papers or reading research papers
[00:14:52.160 --> 00:15:01.120]   that give you kind of a high level overview of an architecture or how something works
[00:15:01.120 --> 00:15:07.760]   without getting into the nitty gritty of the underlying code or math allows me to kind
[00:15:07.760 --> 00:15:14.440]   of reason about these components that are practical level of abstraction.
[00:15:14.440 --> 00:15:16.920]   I can see how things fit together.
[00:15:16.920 --> 00:15:19.320]   I understand how they work.
[00:15:19.320 --> 00:15:23.320]   That doesn't necessarily mean that I'd be able to implement them.
[00:15:23.320 --> 00:15:26.600]   Definitely doesn't mean that I'd be able to iterate on them.
[00:15:26.600 --> 00:15:34.720]   But it's like enough depth to reason about a component and its place in a broader technical
[00:15:34.720 --> 00:15:35.720]   stack.
[00:15:35.720 --> 00:15:37.200]   It's funny, though.
[00:15:37.200 --> 00:15:42.800]   Sometimes I feel like investors, I mean, all investors do that to some extent and I totally
[00:15:42.800 --> 00:15:44.680]   get why.
[00:15:44.680 --> 00:15:51.640]   But I think that I often feel also paranoid about losing my technical skills because I
[00:15:51.640 --> 00:15:58.320]   feel like if all you can do is sort of figure out what box something belongs to, it's really
[00:15:58.320 --> 00:16:00.960]   hard for you to evaluate the things that don't fit into boxes.
[00:16:00.960 --> 00:16:04.680]   And I feel like almost all the interesting advances, actually all the products that we
[00:16:04.680 --> 00:16:08.840]   want to come out with at Weights & Biases, generally is stuff where it doesn't fit neatly
[00:16:08.840 --> 00:16:13.000]   into one of those ML workflow diagrams that people make.
[00:16:13.000 --> 00:16:17.680]   Because if it was one of those boxes, then of course people are doing it because it makes
[00:16:17.680 --> 00:16:18.680]   logical sense.
[00:16:18.680 --> 00:16:23.560]   But it's sort of when that stuff gets reshuffled and it does seem like you're able to maintain
[00:16:23.560 --> 00:16:30.360]   a much greater level of technical depth than the average investor, even in the data space,
[00:16:30.360 --> 00:16:32.200]   which is why I wanted to have you on this podcast.
[00:16:32.200 --> 00:16:34.600]   I hope I'm not offending any of my current investors.
[00:16:34.600 --> 00:16:36.280]   There's a caveat there.
[00:16:36.280 --> 00:16:37.280]   You all are wonderful.
[00:16:37.280 --> 00:16:41.760]   But I really do feel like you somehow maintained a much greater technical depth than most of
[00:16:41.760 --> 00:16:45.240]   your colleagues.
[00:16:45.240 --> 00:16:51.920]   So in many ways, I'm amazed by my colleagues and what they do, because I think there are
[00:16:51.920 --> 00:16:58.640]   many investors that can reason about the growth of companies and reason about sets of boxes
[00:16:58.640 --> 00:17:08.000]   and the relationships between those boxes without understanding what those boxes do.
[00:17:08.000 --> 00:17:16.280]   I don't think I could do that, but I've always also just been the type of person who needs
[00:17:16.280 --> 00:17:19.960]   to go a little bit deeper.
[00:17:19.960 --> 00:17:28.280]   As an example, I started my career in data science, but I'd amplify, I also invest in
[00:17:28.280 --> 00:17:29.280]   databases.
[00:17:29.280 --> 00:17:36.720]   And at some point, writing SQL queries, working with data frames, I just wanted to better
[00:17:36.720 --> 00:17:39.960]   understand what was happening.
[00:17:39.960 --> 00:17:46.520]   When I write a SQL query and data shows up in my SQL workbench, what is happening on
[00:17:46.520 --> 00:17:48.960]   my computer?
[00:17:48.960 --> 00:17:54.720]   I think a lot of people take that stuff for granted, and they can.
[00:17:54.720 --> 00:17:56.540]   That is the beauty of abstractions.
[00:17:56.540 --> 00:17:58.680]   That is the beauty of technology.
[00:17:58.680 --> 00:18:01.360]   We are able to have this video conference.
[00:18:01.360 --> 00:18:07.840]   We are able to connect over the internet without understanding how the internet works.
[00:18:07.840 --> 00:18:14.520]   But my personality is as such that I want to understand how the internet works.
[00:18:14.520 --> 00:18:22.600]   I want to understand why I have service in some places and why I don't have service and
[00:18:22.600 --> 00:18:27.680]   why my data frame is slower than my SQL query.
[00:18:27.680 --> 00:18:33.760]   And I do think that that makes me think about technical systems in different ways.
[00:18:33.760 --> 00:18:35.440]   That's funny.
[00:18:35.440 --> 00:18:41.280]   My co-founder, Sean, is obsessed with, in technical interviews, assessing if someone
[00:18:41.280 --> 00:18:44.840]   understands how it stands how the computer works, in his words, which I think is really
[00:18:44.840 --> 00:18:47.880]   interesting because I feel like I'm actually not.
[00:18:47.880 --> 00:18:49.120]   That's kind of a weakness of mine.
[00:18:49.120 --> 00:18:55.960]   I always wonder about a lot of the details there, but it is sort of an interesting perspective.
[00:18:55.960 --> 00:19:02.480]   I love working with all of my colleagues who have that same drive to understand how everything
[00:19:02.480 --> 00:19:03.480]   works.
[00:19:03.480 --> 00:19:04.480]   Okay.
[00:19:04.480 --> 00:19:08.120]   Here's another question that I was wondering, I was thinking about.
[00:19:08.120 --> 00:19:15.160]   If I were to come to you, and I had a company in the data and ML space, and I had a bunch
[00:19:15.160 --> 00:19:22.880]   of customers that were really who we think of as tech forward, like say Airbnb and Google
[00:19:22.880 --> 00:19:28.600]   and yeah, I don't know, that genre, would that be more impressive or would you be more
[00:19:28.600 --> 00:19:32.760]   thinking I'm likely to succeed than if I came to you at the set of customers who we don't
[00:19:32.760 --> 00:19:35.000]   normally think of as tech forward?
[00:19:35.000 --> 00:19:40.680]   Say like an insurance company, like a large insurance company and a large pharma company,
[00:19:40.680 --> 00:19:46.680]   which would you look at and say, "Oh, that seems like that company is going to succeed."
[00:19:46.680 --> 00:19:52.280]   Because part of me sort of watches technology sort of flow from the more tech forward companies
[00:19:52.280 --> 00:19:53.280]   everywhere.
[00:19:53.280 --> 00:19:56.800]   But another part of me is like, "Wow, these kind of less tech forward companies have a
[00:19:56.800 --> 00:20:00.440]   whole set of different needs and often a different tech stack."
[00:20:00.440 --> 00:20:04.520]   And certainly there's more of them and they have more budget for this stuff.
[00:20:04.520 --> 00:20:07.520]   So which would be the more impressive pitch for you?
[00:20:07.520 --> 00:20:08.520]   Yeah.
[00:20:08.520 --> 00:20:09.520]   Yeah.
[00:20:09.520 --> 00:20:16.520]   It's funny because I think in many ways, the way that VCs make decisions, the way that
[00:20:16.520 --> 00:20:21.800]   we think about deals is actually super similar to some of the patterns that we observe.
[00:20:21.800 --> 00:20:25.080]   Like neural networks.
[00:20:25.080 --> 00:20:28.240]   And that of course means that we have bias.
[00:20:28.240 --> 00:20:34.400]   It also means that we learn from patterns that we've observed.
[00:20:34.400 --> 00:20:40.800]   So I can give you the honest answer and then I can also give you the rational answer.
[00:20:40.800 --> 00:20:47.920]   The honest answer is that I would be more impressed by a company that has engaged with
[00:20:47.920 --> 00:20:52.800]   tech forward customers for the reasons that you described.
[00:20:52.800 --> 00:20:59.360]   In the past, we have generally seen that tech will spread from the Airbnbs and Ubers and
[00:20:59.360 --> 00:21:05.420]   no fangs of the world into the enterprise and not the other way around.
[00:21:05.420 --> 00:21:11.900]   We also have a bias that these more traditional enterprises tend to move slower.
[00:21:11.900 --> 00:21:17.740]   There tends to be a lot of bureaucratic red tape that you need to navigate.
[00:21:17.740 --> 00:21:23.760]   And as such, those markets tend to be less attractive.
[00:21:23.760 --> 00:21:32.080]   So on its face, if you just said you don't have any additional information about the
[00:21:32.080 --> 00:21:37.680]   velocity of sales, about the quality of the tech or team, et cetera, but here are two
[00:21:37.680 --> 00:21:38.680]   equivalent companies.
[00:21:38.680 --> 00:21:39.680]   Or holding them equal, I guess.
[00:21:39.680 --> 00:21:40.680]   Yeah, equivalent.
[00:21:40.680 --> 00:21:41.680]   Yeah.
[00:21:41.680 --> 00:21:42.680]   Yeah.
[00:21:42.680 --> 00:21:43.680]   Yeah.
[00:21:43.680 --> 00:21:52.560]   That said, I think that is one of the biases that can cause us to make poor decisions.
[00:21:52.560 --> 00:21:56.480]   What really matters are some of the things that I just alluded to.
[00:21:56.480 --> 00:22:05.800]   If you're able to sell into insurance companies repeatedly and with high velocity, that is
[00:22:05.800 --> 00:22:12.440]   arguably a better business than a company that spends six to 12 months trying to sell
[00:22:12.440 --> 00:22:15.720]   into tech companies.
[00:22:15.720 --> 00:22:24.080]   So it's less about to whom do you sell and more about is that a big market?
[00:22:24.080 --> 00:22:27.320]   Are you able to sell efficiently?
[00:22:27.320 --> 00:22:30.600]   Are you able to sell scalably?
[00:22:30.600 --> 00:22:38.640]   And I think sometimes we need to be aware of our biases and the impact that marquee
[00:22:38.640 --> 00:22:42.560]   logos can have on our decision making.
[00:22:42.560 --> 00:22:43.560]   But it sounds like you...
[00:22:43.560 --> 00:22:46.400]   Well, I can't tell if you think it's a rational bias or not.
[00:22:46.400 --> 00:22:49.160]   I mean, in some sense, you could call all pattern matching biases.
[00:22:49.160 --> 00:22:58.040]   But do you really think you think it would be rational to be less enamored with tech
[00:22:58.040 --> 00:23:03.560]   forward customers than you actually are?
[00:23:03.560 --> 00:23:14.120]   So I think we need to ask ourselves and probe on under what circumstances might enterprises
[00:23:14.120 --> 00:23:16.640]   move quickly.
[00:23:16.640 --> 00:23:22.320]   A great example of this is a company called Afresh, which was one of the companies that
[00:23:22.320 --> 00:23:30.360]   did use RL to disrupt an industry at that time that so many companies were trying to
[00:23:30.360 --> 00:23:35.120]   do the same thing but didn't have as much insight into what was happening within an
[00:23:35.120 --> 00:23:36.640]   industry.
[00:23:36.640 --> 00:23:43.680]   So they offer tech solutions, including for things like inventory management and forecasting
[00:23:43.680 --> 00:23:46.680]   to companies in the grocery space.
[00:23:46.680 --> 00:23:55.880]   Now, you might think that grocery is going to be a super outdated, slow moving industry
[00:23:55.880 --> 00:24:05.680]   and therefore that selling into grocery chains would be long and tedious and perhaps not
[00:24:05.680 --> 00:24:06.680]   very scalable.
[00:24:06.680 --> 00:24:14.960]   But at the time, a lot of grocery stores were responding to and/or otherwise just terrified
[00:24:14.960 --> 00:24:19.480]   by the acquisition of Whole Foods by Amazon.
[00:24:19.480 --> 00:24:28.440]   This was then proceeded by the pandemic, which certainly put a lot of stress on their online
[00:24:28.440 --> 00:24:35.160]   and kind of like multi-channel delivery and e-commerce capabilities.
[00:24:35.160 --> 00:24:43.280]   And so there were these exogenous shocks, which made what might have been slow moving
[00:24:43.280 --> 00:24:47.500]   market participants move a lot faster.
[00:24:47.500 --> 00:24:53.600]   And I think those are the phenomena that we're sometimes blind to because we just hear like
[00:24:53.600 --> 00:25:03.040]   grocery or healthcare or manufacturing and think slow, rather than thinking like, what
[00:25:03.040 --> 00:25:07.640]   would it take for the participants in that sector to move fast?
[00:25:07.640 --> 00:25:08.640]   That makes sense.
[00:25:08.640 --> 00:25:09.640]   Okay.
[00:25:09.640 --> 00:25:15.400]   Here's another point that you made on Twitter that I was contemplating, and I actually don't
[00:25:15.400 --> 00:25:19.240]   think I have a strong point of view on this, although I really should given the company
[00:25:19.240 --> 00:25:20.240]   that I'm running.
[00:25:20.240 --> 00:25:27.080]   But you mentioned a lot of, I think GCs have been saying that you expect the point solution,
[00:25:27.080 --> 00:25:32.520]   MLOps space to consolidate.
[00:25:32.520 --> 00:25:38.480]   And I guess one thing that's interesting about that is that I think you've invested in some
[00:25:38.480 --> 00:25:39.480]   MLOps tools.
[00:25:39.480 --> 00:25:44.360]   So do you sort of expect them to sort of like expand in scope and like eat the other companies?
[00:25:44.360 --> 00:25:48.120]   Is that something that you need to bet on when you invest in them or would you be sort
[00:25:48.120 --> 00:25:52.000]   of happy to see them get bought by other tools?
[00:25:52.000 --> 00:25:57.600]   Or how do you think about investment then in MLOps tools company with that worldview?
[00:25:57.600 --> 00:25:59.600]   That's my practical question.
[00:25:59.600 --> 00:26:05.200]   And then I guess the other thing that I observe is that it doesn't necessarily seem like developer
[00:26:05.200 --> 00:26:09.000]   tools in general is consolidating.
[00:26:09.000 --> 00:26:13.840]   So I think I might even agree with you, but I wonder how you sort of pattern match that
[00:26:13.840 --> 00:26:17.920]   against developer tools or even maybe like the data stack.
[00:26:17.920 --> 00:26:18.920]   I don't know.
[00:26:18.920 --> 00:26:22.040]   Do you think that the data stack is also consolidating or what's going on there?
[00:26:22.040 --> 00:26:27.160]   Sorry, I just dumped a whole bunch of different questions on you, but I would love to hear
[00:26:27.160 --> 00:26:28.160]   your thoughts.
[00:26:28.160 --> 00:26:31.960]   Those are great questions.
[00:26:31.960 --> 00:26:40.080]   So I do think that like in general, most technical tools and platforms will go through like phases
[00:26:40.080 --> 00:26:48.760]   of consolidation and like decoupling or as people love to say today, like bundling and
[00:26:48.760 --> 00:26:51.840]   unbundling.
[00:26:51.840 --> 00:26:57.120]   I think it's just the nature of like point solutions versus end to end platforms.
[00:26:57.120 --> 00:27:00.280]   Like you have a bunch of point solutions that are difficult to maintain.
[00:27:00.280 --> 00:27:03.880]   They may be challenging to integrate.
[00:27:03.880 --> 00:27:08.680]   You then kind of like bias towards like end to end platforms.
[00:27:08.680 --> 00:27:11.160]   You adopted end to end platform.
[00:27:11.160 --> 00:27:18.120]   It doesn't like address a certain edge case or like a use case that you're experiencing.
[00:27:18.120 --> 00:27:24.580]   So you'd buy a new tool for like that edge case and said, you know, unbundling happens.
[00:27:24.580 --> 00:27:30.040]   And I think like the pendulum will always swing back and forth between like bundling
[00:27:30.040 --> 00:27:37.620]   and unbundling for that reason or like coupling and decoupling for that reason.
[00:27:37.620 --> 00:27:44.760]   To be clear, like as a former buyer, I don't think that like point solutions or end to
[00:27:44.760 --> 00:27:51.800]   end platforms are like the best solutions like for a company.
[00:27:51.800 --> 00:28:02.480]   I think like there's space in the middle where you have a product that can solve a few adjacent
[00:28:02.480 --> 00:28:06.480]   problems and so that's typically like what I look for when I invest.
[00:28:06.480 --> 00:28:12.400]   I want to make sure that like the company in which I'm investing is solving an urgent
[00:28:12.400 --> 00:28:15.680]   and often like point problem.
[00:28:15.680 --> 00:28:19.080]   They're solving an urgent and specific problem.
[00:28:19.080 --> 00:28:25.400]   However, I typically also want to see that like the founder has a hypothesis about like
[00:28:25.400 --> 00:28:32.800]   how they would expand into adjacent problem areas.
[00:28:32.800 --> 00:28:43.560]   So it's not that I think solving point problems is bad, but I do think like given the pendulum
[00:28:43.560 --> 00:28:51.040]   of coupling and decoupling, like having some hypotheses about like the areas that you can
[00:28:51.040 --> 00:28:55.600]   expand into becomes critical.
[00:28:55.600 --> 00:29:04.000]   It's interesting to consider why this may or may not happen in the world of like developer
[00:29:04.000 --> 00:29:05.880]   tools.
[00:29:05.880 --> 00:29:11.200]   I'd argue that like you'd still see consolidation.
[00:29:11.200 --> 00:29:22.280]   However, the consolidation tends to happen across like layers of the stack versus across
[00:29:22.280 --> 00:29:23.280]   the workflow.
[00:29:23.280 --> 00:29:24.280]   Interesting.
[00:29:24.280 --> 00:29:28.840]   Tell me what are you thinking of there?
[00:29:28.840 --> 00:29:40.120]   Things like serverless where you're no longer like reasoning about like resources and config.
[00:29:40.120 --> 00:29:45.960]   That might not be impacting like other parts of your developer workflow.
[00:29:45.960 --> 00:29:54.760]   Like that might not be eating into your like Git-based development workflows or your like
[00:29:54.760 --> 00:29:59.160]   testing processes and things like that.
[00:29:59.160 --> 00:30:09.480]   But it is eating into, you know, how you think about like managing VMs or containers.
[00:30:09.480 --> 00:30:20.800]   It is possibly like eating into how you think about working with like cloud vendors and
[00:30:20.800 --> 00:30:24.240]   deciding upon like underlying hardware and things like that.
[00:30:24.240 --> 00:30:33.960]   So it might be the case that like in software development, like we've seen companies or
[00:30:33.960 --> 00:30:39.200]   we've seen like vendors solve specific problems, but solve those like all the way down the
[00:30:39.200 --> 00:30:40.760]   stack.
[00:30:40.760 --> 00:30:47.840]   I haven't really thought about that as deeply.
[00:30:47.840 --> 00:30:51.240]   But I think, you know, it's a worthwhile question to ask.
[00:30:51.240 --> 00:30:57.160]   I would say that like one of the big differences, though, that I see and that we, of course,
[00:30:57.160 --> 00:31:05.000]   need to be mindful of is that like there are far more developers than there are data practitioners.
[00:31:05.000 --> 00:31:12.200]   And so when you're trying to answer the question, like how does this thing get big, those building
[00:31:12.200 --> 00:31:21.440]   developer tools can arguably like solve a specific problem for a larger number of people
[00:31:21.440 --> 00:31:23.720]   versus data teams.
[00:31:23.720 --> 00:31:29.040]   When you're trying to answer this question of like, how does this get big?
[00:31:29.040 --> 00:31:34.840]   You could potentially get stumped just by like the number of people for whom a tool
[00:31:34.840 --> 00:31:37.760]   is actually applicable.
[00:31:37.760 --> 00:31:41.240]   And is that what gives you the intuition that we're in a moment of bundling, that there's
[00:31:41.240 --> 00:31:45.400]   just all these kind of point solutions that you feel like kind of can't survive on their
[00:31:45.400 --> 00:31:49.080]   own just given the size of the market that they're in?
[00:31:49.080 --> 00:31:50.080]   Yeah.
[00:31:50.080 --> 00:31:53.080]   So, I mean, I think it's a combination of things.
[00:31:53.080 --> 00:32:01.360]   I think on one hand, I see like a lot of the slivers are getting tinier.
[00:32:01.360 --> 00:32:11.760]   You start to see things like model deployment solutions or like computer vision and perhaps,
[00:32:11.760 --> 00:32:20.360]   you know, some subset of like computer vision architectures where you might think to yourself,
[00:32:20.360 --> 00:32:27.600]   like, okay, I understand why the existing tools are maybe not like optimal for that
[00:32:27.600 --> 00:32:31.880]   specific use case, but that's really narrow.
[00:32:31.880 --> 00:32:37.560]   And like to my point about thinking about like these orthogonal problems, like it's
[00:32:37.560 --> 00:32:42.960]   unclear like how you go from that to something meatier.
[00:32:42.960 --> 00:32:44.880]   So that's one phenomena that I observe.
[00:32:44.880 --> 00:32:50.440]   I think the other is just that like practitioners are really, really struggling to stitch things
[00:32:50.440 --> 00:32:51.600]   together.
[00:32:51.600 --> 00:32:56.120]   The way a friend put it to me about like a year ago, he basically said he feels like,
[00:32:56.120 --> 00:33:03.160]   you know, vendors are handing him a steering wheel and an engine and a dashboard and, you
[00:33:03.160 --> 00:33:08.520]   know, a chassis and saying like build a fast, safe car.
[00:33:08.520 --> 00:33:11.920]   And those components might not even fit together.
[00:33:11.920 --> 00:33:16.240]   And there's no instruction manual.
[00:33:16.240 --> 00:33:24.280]   And you know, it's easy to like cast shade on, you know, the startups that are like building
[00:33:24.280 --> 00:33:28.920]   these tools and platforms, but I think one of the things that is more challenging in
[00:33:28.920 --> 00:33:39.480]   the ML space than even like data and analytics is that like a lot of the ML engineering and
[00:33:39.480 --> 00:33:43.120]   ML development workflows are really heterogeneous now.
[00:33:43.120 --> 00:33:48.760]   So if you're a vendor and you're trying to think about like, okay, with whom should I
[00:33:48.760 --> 00:33:49.760]   partner?
[00:33:49.760 --> 00:33:50.760]   With whom should I integrate?
[00:33:50.760 --> 00:33:55.400]   Like, do I spend time on supporting this integration?
[00:33:55.400 --> 00:34:01.800]   It's tougher to make those decisions when like practices and workflows are so fragmented
[00:34:01.800 --> 00:34:03.560]   and heterogeneous.
[00:34:03.560 --> 00:34:11.680]   So I do think that creating like more of a cohesive ecosystem has been difficult, not
[00:34:11.680 --> 00:34:18.880]   because like vendors are dumb, but because there's just a lot going on.
[00:34:18.880 --> 00:34:25.080]   I think the other challenge maybe is that when there's so many like different technologies
[00:34:25.080 --> 00:34:28.680]   that people want to like integrate into what they're doing, because there's so much exciting
[00:34:28.680 --> 00:34:33.640]   research and like things that, you know, come along, you know, based on like different frameworks
[00:34:33.640 --> 00:34:35.440]   and so on.
[00:34:35.440 --> 00:34:39.600]   It's hard to imagine like an end to end system that would actually be able to absorb like
[00:34:39.600 --> 00:34:44.880]   every possible, you know, like model architecture, like immediately as fast as companies like
[00:34:44.880 --> 00:34:46.880]   want to actually use it.
[00:34:46.880 --> 00:34:49.440]   Yeah, yeah, 100%.
[00:34:49.440 --> 00:34:54.000]   I mean, I have been thinking about this in the context of LLMs.
[00:34:54.000 --> 00:35:03.960]   Like we don't yet know how the consumers or like users of pre-trained models are going
[00:35:03.960 --> 00:35:09.360]   to interact with those who create the pre-trained models.
[00:35:09.360 --> 00:35:14.360]   Like will they be doing their own fine tuning?
[00:35:14.360 --> 00:35:17.920]   Will they be doing their own prompt engineering?
[00:35:17.920 --> 00:35:23.600]   Will they just be interacting with the LLM via API?
[00:35:23.600 --> 00:35:28.280]   And without like insight into those interaction models, it's really hard to think about like
[00:35:28.280 --> 00:35:30.320]   building the right set of tools.
[00:35:30.320 --> 00:35:37.200]   It's also unclear to me if like the adoption of LLMs would actually imply that we need
[00:35:37.200 --> 00:35:46.560]   a new set of tools, both for model development and deployment and management and production.
[00:35:46.560 --> 00:35:51.480]   So I have a lot of empathy for people who are building ML tools and platforms because
[00:35:51.480 --> 00:35:56.320]   like it's a constantly moving target.
[00:35:56.320 --> 00:36:04.640]   And yet, you know, there's the expectation that though you're able to support like heterogeneity
[00:36:04.640 --> 00:36:10.840]   in all regards, like all regards, like whether it's the model architecture or the data type
[00:36:10.840 --> 00:36:18.960]   or like the hardware backend or the team structure or the user skill set, like there's so much
[00:36:18.960 --> 00:36:21.920]   that is different from like org to org.
[00:36:21.920 --> 00:36:27.160]   So I think building great tools is really challenging right now.
[00:36:27.160 --> 00:36:29.840]   I guess that's a good segue to a question I was going to ask you.
[00:36:29.840 --> 00:36:36.640]   When you look at LLMs, do you have an intuition on if a new set of tools are needed to make
[00:36:36.640 --> 00:36:37.640]   these functional?
[00:36:37.640 --> 00:36:38.640]   Yeah.
[00:36:38.640 --> 00:36:47.680]   So, I mean, I think like one of the bigger questions that I have is again, like on how
[00:36:47.680 --> 00:36:53.600]   the consumers of LLMs or how the users of the LLMs will actually interact with those
[00:36:53.600 --> 00:37:04.920]   LLMs and more specifically, who will own fine tuning?
[00:37:04.920 --> 00:37:11.160]   I imagine that like there are certain challenges that will need to be addressed both with regards
[00:37:11.160 --> 00:37:17.440]   to like how we collaborate on the development of LLMs, but also how we think about like
[00:37:17.440 --> 00:37:20.520]   the impact of iterations on LLMs.
[00:37:20.520 --> 00:37:29.240]   So for example, you know, if OpenAI wants to retrain one of their models or like otherwise
[00:37:29.240 --> 00:37:35.920]   tweak the architecture, how do they evaluate the impact of that change on all of the people
[00:37:35.920 --> 00:37:47.840]   who are interfacing with the GPT-3 API or with any of their other products?
[00:37:47.840 --> 00:37:53.400]   I think a lot of the tools that were built for model development and deployment today
[00:37:53.400 --> 00:37:58.680]   kind of assumed that the people who were developing models would be the same set of people or
[00:37:58.680 --> 00:38:08.120]   at least like within the same corporate umbrella as those who are deploying and managing models
[00:38:08.120 --> 00:38:10.120]   in production.
[00:38:10.120 --> 00:38:18.800]   And if LLMs drive a shift where in those who are developing models and those who are deploying
[00:38:18.800 --> 00:38:24.120]   and building applications around models are two completely separate parties, then some
[00:38:24.120 --> 00:38:28.680]   of the tools that we have today might be ill suited for that context.
[00:38:28.680 --> 00:38:31.880]   Do you think we're headed towards a world like that where there's like a small number
[00:38:31.880 --> 00:38:37.520]   of companies generating foundational models and then mostly what other companies are doing
[00:38:37.520 --> 00:38:42.920]   is fine tuning them or doing some kind of like prompt engineering to get good results
[00:38:42.920 --> 00:38:45.440]   out of them?
[00:38:45.440 --> 00:38:51.240]   So here we're getting a little bit into the like technical nitty gritty, but like my impression
[00:38:51.240 --> 00:39:00.360]   from tracking like the research community so far has been not all though LLMs are great
[00:39:00.360 --> 00:39:06.160]   or like what we typically think of as like unstructured data, primarily like images,
[00:39:06.160 --> 00:39:09.720]   text, video, et cetera, audio too.
[00:39:09.720 --> 00:39:16.280]   They have not like outperformed like gradient boosting or like more traditional methods
[00:39:16.280 --> 00:39:20.960]   on structured data sets, including like tabular and time series data.
[00:39:20.960 --> 00:39:28.080]   Although there's some work on like time series that I think is pretty compelling.
[00:39:28.080 --> 00:39:34.360]   This is one of those areas where I feel like the research community just completely underestimates
[00:39:34.360 --> 00:39:41.440]   how many businesses operate on structured data.
[00:39:41.440 --> 00:39:49.840]   While it's possible that adoption of LLMs will drive like this new interaction model
[00:39:49.840 --> 00:39:56.320]   or like new market model, wherein some companies built these large foundation models and others
[00:39:56.320 --> 00:40:04.880]   interact with those, I don't see like gradient boosting or like more classical approaches
[00:40:04.880 --> 00:40:13.720]   going anywhere because I don't see structured data going anywhere.
[00:40:13.720 --> 00:40:20.480]   Arguably like structured data powers many of the most critical use cases within organizations,
[00:40:20.480 --> 00:40:27.120]   like ranging from search and recommendation engines to like fraud detection.
[00:40:27.120 --> 00:40:35.320]   And I think it would be a tragedy to kind of like neglect the needs of those who are
[00:40:35.320 --> 00:40:44.280]   using, I don't want to say like simpler approaches, but certainly simpler approaches and more
[00:40:44.280 --> 00:40:53.520]   complex approaches, but using architectures that are not perhaps like attention based
[00:40:53.520 --> 00:40:57.000]   when working with like the specific data sets.
[00:40:57.000 --> 00:40:58.400]   Interesting.
[00:40:58.400 --> 00:41:07.200]   Do you have an opinion on, how to say this, I feel like many investors, especially, but
[00:41:07.200 --> 00:41:13.920]   I think like many smart people looking at the space of ML and data, they think, wow,
[00:41:13.920 --> 00:41:21.400]   this is going to kind of commoditize, like tools are going to make this easier, like
[00:41:21.400 --> 00:41:27.480]   less companies are going to want to do this internally and spend money on expensive resources.
[00:41:27.480 --> 00:41:31.960]   But I guess when I look at what companies actually do, is it seems like they spend more
[00:41:31.960 --> 00:41:38.400]   and more and even like kind of push up the salaries and have this like fight for scarce
[00:41:38.400 --> 00:41:40.320]   specific talent.
[00:41:40.320 --> 00:41:43.560]   So I guess, which way do you sort of predict things are going?
[00:41:43.560 --> 00:41:51.440]   Like, do you think like 10 years down the road, ML salaries like go up or do they go
[00:41:51.440 --> 00:41:52.440]   down?
[00:41:52.440 --> 00:41:54.680]   Maybe as a more concrete way of putting that.
[00:41:54.680 --> 00:41:55.680]   Yeah.
[00:41:55.680 --> 00:41:58.240]   I mean, that's a great question.
[00:41:58.240 --> 00:42:03.480]   I probably expect that like the variance would increase.
[00:42:03.480 --> 00:42:15.560]   So my guess is that like there are certain applications that may be commoditized or at
[00:42:15.560 --> 00:42:25.760]   least that may be commoditized for some subset of the market while others continue to be
[00:42:25.760 --> 00:42:27.880]   pursued in-house.
[00:42:27.880 --> 00:42:32.440]   Search is perhaps like a very interesting example.
[00:42:32.440 --> 00:42:41.280]   For some businesses, they may be like more than happy to rely upon like a vendor to provide
[00:42:41.280 --> 00:42:46.800]   those semantic or like vector-based search capabilities.
[00:42:46.800 --> 00:42:55.480]   While search may have an impact on their bottom line, perhaps it's not like the most critical
[00:42:55.480 --> 00:43:01.480]   or like most impactful thing to their business, but rather like just a capability that they
[00:43:01.480 --> 00:43:04.640]   have.
[00:43:04.640 --> 00:43:15.160]   So this is not to say that like Slack actually like uses a like vendor or should use a vendor.
[00:43:15.160 --> 00:43:21.280]   But like as far as I can tell, like Slack doesn't really like monetize on search.
[00:43:21.280 --> 00:43:27.440]   You'd contrast that, however, with like an e-commerce business or something like Google
[00:43:27.440 --> 00:43:33.120]   where their ability to deliver like the highest quality search results and their ability to
[00:43:33.120 --> 00:43:41.000]   like improve search just marginally could be a huge impact on revenue.
[00:43:41.000 --> 00:43:48.600]   And those companies are probably likely to develop their own models.
[00:43:48.600 --> 00:43:58.200]   So I think we'll see that some companies do their own model development.
[00:43:58.200 --> 00:44:02.240]   Some use cases are not commoditized.
[00:44:02.240 --> 00:44:10.720]   And those companies for those use cases, you see very high ML salaries.
[00:44:10.720 --> 00:44:16.880]   But then, you know, perhaps like for others, like you're really just a software engineer
[00:44:16.880 --> 00:44:24.280]   who knows like a little bit about ML and can interface with some of these models like through
[00:44:24.280 --> 00:44:37.000]   APIs and can reason about like the output of experiments and behavior that you might
[00:44:37.000 --> 00:44:39.960]   see in production.
[00:44:39.960 --> 00:44:44.360]   I guess in that vein, and you sort of alluded to this earlier a little bit, like what do
[00:44:44.360 --> 00:44:53.400]   you think about all these sort of like low code and no code interfaces into exploring
[00:44:53.400 --> 00:44:55.160]   data, building ML models?
[00:44:55.160 --> 00:44:58.880]   I mean, I guess I think you mentioned earlier that you think that's like generally like
[00:44:58.880 --> 00:45:02.440]   a really exciting trend.
[00:45:02.440 --> 00:45:13.080]   So my opinions on this category are pretty nuanced.
[00:45:13.080 --> 00:45:17.040]   So I was thinking about like where to start.
[00:45:17.040 --> 00:45:22.280]   Generally speaking, like I'm very skeptical of like no code, low code solutions.
[00:45:22.280 --> 00:45:29.200]   I find that like many of these tools, no matter like what the sector, what the use case, they
[00:45:29.200 --> 00:45:37.040]   end up like shifting the burden of work, not necessarily like removing that burden or even
[00:45:37.040 --> 00:45:38.520]   lightening that burden.
[00:45:38.520 --> 00:45:42.040]   So a great example is like self-service analytics.
[00:45:42.040 --> 00:45:48.760]   My own belief is that in general, like most self-service analytics tools don't actually
[00:45:48.760 --> 00:45:57.520]   like reduce the burden that like the data team or analytics team bears, but rather like
[00:45:57.520 --> 00:46:04.000]   shift the work of the data team from building analytics products to like debugging, explaining,
[00:46:04.000 --> 00:46:06.080]   or fixing analytics products.
[00:46:06.080 --> 00:46:11.480]   And I think like the same can be true in the ML space.
[00:46:11.480 --> 00:46:18.040]   Why I'm excited about like some of these tools in the ML space is that I actually think that
[00:46:18.040 --> 00:46:24.120]   in ML, failing fast is really critical.
[00:46:24.120 --> 00:46:30.240]   And you know, some of these tools that enable users to like prototype ML-driven solutions
[00:46:30.240 --> 00:46:34.960]   might help them better understand like, is this going to work?
[00:46:34.960 --> 00:46:39.840]   What additional investments do I need?
[00:46:39.840 --> 00:46:47.960]   What do my users expect from the system before they make a decision to invest further?
[00:46:47.960 --> 00:46:53.600]   So it enables that kind of like quick prototyping learning and failing fast.
[00:46:53.600 --> 00:47:00.640]   The other thing that I feel quite strongly about is that we need to explore ways to kind
[00:47:00.640 --> 00:47:06.280]   of like decouple model development and ML-driven app development.
[00:47:06.280 --> 00:47:13.200]   Whenever I talk to companies about like their ML architectures or their ML stack, it becomes
[00:47:13.200 --> 00:47:22.480]   so obvious that like ML is just this one tiny component in a much larger app architecture
[00:47:22.480 --> 00:47:32.680]   where the prediction service might be connecting with other databases or stream processing
[00:47:32.680 --> 00:47:41.120]   systems or other microservices, tools for like authorization and so on and so forth.
[00:47:41.120 --> 00:47:46.560]   And so like I think it's really important to be able to like build applications around
[00:47:46.560 --> 00:47:51.840]   a prediction service while independently like iterating on the model that powers that prediction
[00:47:51.840 --> 00:47:52.920]   service.
[00:47:52.920 --> 00:48:00.160]   And so I am somewhat long on tools that enable engineers to prototype ML-driven systems so
[00:48:00.160 --> 00:48:03.320]   that they can build those application architectures.
[00:48:03.320 --> 00:48:08.560]   And then once they have a better understanding of kind of the full system requirements, including
[00:48:08.560 --> 00:48:14.520]   some of the latency associated with things like moving data around, they can kind of
[00:48:14.520 --> 00:48:22.440]   pass off a fuller spec to a data scientist who will iterate on the model and model architecture
[00:48:22.440 --> 00:48:31.200]   armed with the knowledge that like these are the attributes that we need in order to make
[00:48:31.200 --> 00:48:33.680]   this project successful.
[00:48:33.680 --> 00:48:35.000]   That makes sense.
[00:48:35.000 --> 00:48:36.000]   Okay.
[00:48:36.000 --> 00:48:37.000]   Another question.
[00:48:37.000 --> 00:48:43.840]   Do you, when you invest in a company that is like providing like some kind of like ML
[00:48:43.840 --> 00:48:47.580]   or data service, does it cross your mind?
[00:48:47.580 --> 00:48:50.040]   Like what if AWS does that?
[00:48:50.040 --> 00:48:51.960]   Or like, you know, GCP or Azure?
[00:48:51.960 --> 00:48:54.120]   Is that an important thing to consider, do you think?
[00:48:54.120 --> 00:48:56.320]   Or is that irrelevant?
[00:48:56.320 --> 00:48:58.320]   Yeah, yeah.
[00:48:58.320 --> 00:49:06.960]   I smile because I feel like this question, it comes up somewhere between like one to
[00:49:06.960 --> 00:49:11.440]   five times a week, given the areas that Amplify invests in.
[00:49:11.440 --> 00:49:16.680]   You know, we're primarily focused on data and ML tools and platforms, enterprise infrastructure
[00:49:16.680 --> 00:49:17.960]   and developer tools.
[00:49:17.960 --> 00:49:25.480]   Like we're constantly fielding this question of like, what if, you know, AWS or GCP or
[00:49:25.480 --> 00:49:27.920]   Azure does this?
[00:49:27.920 --> 00:49:33.900]   Won't you know that company won't like that market, et cetera, get crushed?
[00:49:33.900 --> 00:49:39.000]   In the past, you know, what I've told people is that like I have found that startups tend
[00:49:39.000 --> 00:49:42.560]   to be better at building developer experiences.
[00:49:42.560 --> 00:49:46.040]   Like anecdotally, this is just something that we observe.
[00:49:46.040 --> 00:49:51.160]   People complain a lot about like the experience of using AWS tools, the experience of using
[00:49:51.160 --> 00:49:54.520]   things like SageMaker.
[00:49:54.520 --> 00:49:59.800]   And I've thought a little bit more about like why that's the case.
[00:49:59.800 --> 00:50:10.400]   I think generally speaking, like the cloud vendors need to develop for their most spendy
[00:50:10.400 --> 00:50:11.400]   customers.
[00:50:11.400 --> 00:50:15.480]   Like they're their like highest paying customers.
[00:50:15.480 --> 00:50:21.360]   Their highest paying customers tend to be enterprises, shockingly.
[00:50:21.360 --> 00:50:26.200]   And as such, like they're developing for an enterprise user who probably has like fairly
[00:50:26.200 --> 00:50:32.040]   strict privacy security requirements, who may have like a very distinct way of like
[00:50:32.040 --> 00:50:40.080]   organizing their teams, who may be bringing in a persona, like a specific skill set into
[00:50:40.080 --> 00:50:43.120]   like data science or ML roles.
[00:50:43.120 --> 00:50:49.880]   So if I had to present a hypothesis about like why they haven't been able to compete
[00:50:49.880 --> 00:50:57.560]   on developer experiences, I think it's because often they are creating tools and platforms
[00:50:57.560 --> 00:51:04.720]   for a developer who is not as representative of the rest of the market.
[00:51:04.720 --> 00:51:11.760]   But to be honest, like with the passage of time, I've just seen enough examples of companies
[00:51:11.760 --> 00:51:16.600]   that have been able to outcompete the cloud vendors where like I just don't worry about
[00:51:16.600 --> 00:51:18.880]   it that much anymore.
[00:51:18.880 --> 00:51:22.200]   Have you ever seen anyone get crushed?
[00:51:22.200 --> 00:51:24.880]   Has that happened in your career?
[00:51:24.880 --> 00:51:30.720]   No, not, I mean, I'm sure it has.
[00:51:30.720 --> 00:51:37.980]   But it's hard for me to think of an example, whereas like it's easy to think of many, many
[00:51:37.980 --> 00:51:46.120]   examples of companies that were not crushed by the cloud vendors.
[00:51:46.120 --> 00:51:59.080]   You know, if anything, I think like sometimes we see that like startups get, they sell too
[00:51:59.080 --> 00:52:00.800]   soon.
[00:52:00.800 --> 00:52:06.080]   Like the way in which, you know, the cloud vendors outcompete them as putting some juicy
[00:52:06.080 --> 00:52:10.240]   acquisition offer in front of them and then they don't have to compete.
[00:52:10.240 --> 00:52:15.680]   So like that's the only example I could see or think of like off the top of my head of
[00:52:15.680 --> 00:52:20.440]   like, you know, the cloud vendors like crushing potential competitor, like they crush it with
[00:52:20.440 --> 00:52:21.440]   their dollars.
[00:52:21.440 --> 00:52:26.600]   They suffocate companies with their acquisition offers.
[00:52:26.600 --> 00:52:29.480]   R&D through M&A.
[00:52:29.480 --> 00:52:30.480]   Yeah.
[00:52:30.480 --> 00:52:37.720]   So I saw an interview or a conversation that you had with Andrew Ng and I thought you had
[00:52:37.720 --> 00:52:43.200]   an interesting point that, you know, academic benchmarks, you know, they often don't really
[00:52:43.200 --> 00:52:48.400]   reflect industry use cases, but you're kind of pointing out that industry, you know, has
[00:52:48.400 --> 00:52:51.400]   some share of the blame for this.
[00:52:51.400 --> 00:52:53.960]   Could you say more on that topic?
[00:52:53.960 --> 00:52:55.320]   Oh, absolutely.
[00:52:55.320 --> 00:53:01.680]   I mean, I am really grateful to Andrew for actually like drawing my attention to this
[00:53:01.680 --> 00:53:02.680]   issue.
[00:53:02.680 --> 00:53:09.680]   I think like we often think about like the gap between like research and industry, but
[00:53:09.680 --> 00:53:16.120]   we don't as often think about like the gap between like industry and research.
[00:53:16.120 --> 00:53:21.680]   And so, you know, Andrew and I had been talking about this challenge of like structured data
[00:53:21.680 --> 00:53:27.400]   versus unstructured data, and I think I said to him, like, what I see in industry is that
[00:53:27.400 --> 00:53:32.360]   most ML teams are working with popular and time series data.
[00:53:32.360 --> 00:53:37.680]   What I see in the research community is that like most researchers are building new model
[00:53:37.680 --> 00:53:40.140]   architectures for unstructured data.
[00:53:40.140 --> 00:53:47.000]   And so like there's a big mismatch between what like model architectures people in industry
[00:53:47.000 --> 00:53:51.560]   need given the data that is available to them, as well as like given the types of problems
[00:53:51.560 --> 00:53:56.120]   that they're trying to solve and like the research that's becoming available.
[00:53:56.120 --> 00:54:02.920]   Now, he pointed out to me, and this is something that I hadn't really thought about before,
[00:54:02.920 --> 00:54:05.200]   researchers have access to unstructured data.
[00:54:05.200 --> 00:54:07.560]   They have access to things like ImageNet.
[00:54:07.560 --> 00:54:15.280]   They don't have access to like high volumes of data on user sessions or, you know, logs,
[00:54:15.280 --> 00:54:17.160]   metrics and events.
[00:54:17.160 --> 00:54:23.520]   Like the data sets that like tend to be like the lifeblood of most companies.
[00:54:23.520 --> 00:54:32.640]   And it is very difficult to like innovate on, you know, AI techniques or data sets to
[00:54:32.640 --> 00:54:36.720]   which you have zero access.
[00:54:36.720 --> 00:54:42.000]   So I think it's easy to like point at research and be like, ah, there's such a big gap between
[00:54:42.000 --> 00:54:45.520]   like what they're building and what we need.
[00:54:45.520 --> 00:54:51.040]   But I think we also need to be mindful of like what the research community can do, given
[00:54:51.040 --> 00:54:53.800]   the resources that they have available to them.
[00:54:53.800 --> 00:55:01.520]   I've seen like a couple of efforts by a few organizations to like open source their data
[00:55:01.520 --> 00:55:02.520]   sets.
[00:55:02.520 --> 00:55:08.000]   But it's tough because like oftentimes the most valuable data sets are like the most
[00:55:08.000 --> 00:55:14.920]   sensitive ones, like what company wants to share their like click through data that would
[00:55:14.920 --> 00:55:19.440]   hopefully like reveal, you know, the state of their business, some of the experiments
[00:55:19.440 --> 00:55:21.440]   that they're running and so on and so forth.
[00:55:21.440 --> 00:55:23.240]   Well, there's also not a lot of upside.
[00:55:23.240 --> 00:55:28.040]   I mean, I remember the Netflix contest was like such a popular, awesome thing.
[00:55:28.040 --> 00:55:31.720]   Got so many people involved, you know, so much attention to research to Netflix.
[00:55:31.720 --> 00:55:34.200]   Still like a seminal data set, right?
[00:55:34.200 --> 00:55:37.880]   But they didn't do a second one because they felt like, you know, there were user privacy
[00:55:37.880 --> 00:55:41.120]   issues that they couldn't get around to release it.
[00:55:41.120 --> 00:55:46.320]   Or I don't know if you remember when AOL released their, you know, a subset of their query logs.
[00:55:46.320 --> 00:55:48.200]   It was so exciting to actually have that.
[00:55:48.200 --> 00:55:52.000]   You know, I was in research at the time and I was like, this is data set is like gold.
[00:55:52.000 --> 00:55:55.520]   And then like the next day they fired the person that released it and their boss, I
[00:55:55.520 --> 00:55:57.200]   think their boss's boss, right?
[00:55:57.200 --> 00:56:00.080]   Because you know, there was somehow per se identifying information in that.
[00:56:00.080 --> 00:56:06.320]   So it's hard to see like a lot of upside, you know, for corporations, even if they were
[00:56:06.320 --> 00:56:14.000]   sort of neutral on the impact of, on the sort of like, you know, company secrets, IP issue.
[00:56:14.000 --> 00:56:15.000]   Yeah.
[00:56:15.000 --> 00:56:16.000]   Yeah.
[00:56:16.000 --> 00:56:20.120]   Now, one of the things that I have seen that has been very encouraging is like more and
[00:56:20.120 --> 00:56:28.160]   more kind of interview studies or like meta-analyses coming out of the like research community
[00:56:28.160 --> 00:56:35.360]   where it's clear that like the researchers are interested in better understanding the
[00:56:35.360 --> 00:56:40.320]   problems that practitioners face in industry.
[00:56:40.320 --> 00:56:46.320]   One critique that I've had of those studies in the past is that like the authors tend
[00:56:46.320 --> 00:56:53.160]   to interview people to whom they have immediate access, which means that they often interview
[00:56:53.160 --> 00:57:00.400]   practitioners at like some of their like funding organizations, the organizations that are
[00:57:00.400 --> 00:57:08.280]   sponsoring their labs, which means that they tend to bias more towards like larger enterprises
[00:57:08.280 --> 00:57:11.920]   or like the thing companies.
[00:57:11.920 --> 00:57:19.600]   So like they're interviewing people at Facebook, Apple, Tesla on their like data and of all
[00:57:19.600 --> 00:57:27.520]   tools, platforms, practices, and then drawing conclusions about like all of industry.
[00:57:27.520 --> 00:57:32.800]   But I think that, you know, recently I've seen a couple of studies come out where there's
[00:57:32.800 --> 00:57:43.040]   been a more focused effort to get a more like random or at least, you know, more diverse
[00:57:43.040 --> 00:57:51.240]   sample of practitioners from like both smaller startups, more traditional companies, bigger
[00:57:51.240 --> 00:57:56.720]   tech companies, et cetera, to really better understand like both the similarities and
[00:57:56.720 --> 00:58:02.360]   differences between how they approach model developments and deployment.
[00:58:02.360 --> 00:58:04.720]   And I hope that, you know, that continues.
[00:58:04.720 --> 00:58:09.720]   Do you have a study that's top of mind that you could point us to?
[00:58:09.720 --> 00:58:11.200]   Yeah.
[00:58:11.200 --> 00:58:17.240]   So Shreya Shingar, who would actually be a university associate.
[00:58:17.240 --> 00:58:18.240]   Nice.
[00:58:18.240 --> 00:58:19.240]   Yeah.
[00:58:19.240 --> 00:58:20.240]   Yeah.
[00:58:20.240 --> 00:58:25.280]   So, I mean, I was really thrilled because like Shreya actually reached out to us and
[00:58:25.280 --> 00:58:32.720]   said like, hey, can you connect us to people at different types of companies?
[00:58:32.720 --> 00:58:39.920]   Like I've got connections to people at, you know, Instagram, Facebook, Apple, et cetera,
[00:58:39.920 --> 00:58:40.920]   et cetera.
[00:58:40.920 --> 00:58:49.760]   But like I want to talk to people at mid-market companies or like early stage startups and
[00:58:49.760 --> 00:58:55.560]   B2B companies and like better understand some of the nuances of their workflows.
[00:58:55.560 --> 00:58:56.560]   And what was the name of the paper?
[00:58:56.560 --> 00:58:58.360]   I think I just saw it.
[00:58:58.360 --> 00:59:02.120]   Operationalizing Machine Learning in an Interview Study.
[00:59:02.120 --> 00:59:03.120]   Nice.
[00:59:03.120 --> 00:59:04.120]   Thank you.
[00:59:04.120 --> 00:59:05.120]   Yeah, I agree.
[00:59:05.120 --> 00:59:07.120]   That was an excellent, excellent paper.
[00:59:07.120 --> 00:59:08.120]   Yeah.
[00:59:08.120 --> 00:59:09.120]   Yeah.
[00:59:09.120 --> 00:59:13.240]   I mean, the other thing that I had said, I like, you know, sent Shreya a text message
[00:59:13.240 --> 00:59:15.040]   after reading through it.
[00:59:15.040 --> 00:59:19.160]   The other thing that I really appreciated about the interview study was that she didn't
[00:59:19.160 --> 00:59:25.680]   like cherry pick the insights that were most likely to drive like interesting research
[00:59:25.680 --> 00:59:27.320]   questions or solutions.
[00:59:27.320 --> 00:59:34.520]   I think she took like a really genuine and unbiased approach to thinking about like,
[00:59:34.520 --> 00:59:38.360]   what are the problems that people are talking about?
[00:59:38.360 --> 00:59:42.080]   And what are the ways in which they're solving them?
[00:59:42.080 --> 00:59:45.400]   And you know, let's highlight that like there are a bunch of problems that people are just
[00:59:45.400 --> 00:59:53.520]   solving and in practical, albeit like hacky ways, but ways that like they're content with.
[00:59:53.520 --> 00:59:58.040]   So I thought it was a very honest study.
[00:59:58.040 --> 00:59:59.040]   Totally.
[00:59:59.040 --> 01:00:00.440]   I totally agree.
[01:00:00.440 --> 01:00:09.480]   Well, I guess if we are possibly headed towards another bubble in machine learning or machine
[01:00:09.480 --> 01:00:13.320]   intelligence, I guess, as you sometimes call it.
[01:00:13.320 --> 01:00:18.560]   Do you have any advice for a startup founder like me or maybe like an ML practitioner,
[01:00:18.560 --> 01:00:20.800]   which is like most of our audience?
[01:00:20.800 --> 01:00:26.520]   Like I mean, having gone through like another bubble, how would you think about it?
[01:00:26.520 --> 01:00:30.280]   What would you do if you sort of started to see, I think we are already seeing bubble
[01:00:30.280 --> 01:00:31.280]   esque behavior.
[01:00:31.280 --> 01:00:36.280]   What are the lessons?
[01:00:36.280 --> 01:00:44.880]   Yeah, I mean, I think the most critical lesson that like I saw learned like the last time
[01:00:44.880 --> 01:00:54.720]   around was like focus on your users or like focus on the like strategic problems that
[01:00:54.720 --> 01:01:05.560]   you're trying to solve and really, really understand if and why ML is the best tool
[01:01:05.560 --> 01:01:08.520]   to solve that problem.
[01:01:08.520 --> 01:01:13.400]   I think it's critical to think about like machine learning as a very important tool
[01:01:13.400 --> 01:01:17.800]   in our toolkit, but one of several tools.
[01:01:17.800 --> 01:01:22.760]   I was catching up with a friend a couple of weeks ago and she had mentioned to me that
[01:01:22.760 --> 01:01:34.200]   like the way in which she prioritizes ML projects is through regular conversations with their
[01:01:34.200 --> 01:01:41.880]   product leadership and engineering leadership and her representing ML leadership about like
[01:01:41.880 --> 01:01:49.720]   the product roadmap, about like the user behaviors that they're trying to unlock and then thinking
[01:01:49.720 --> 01:01:55.480]   about like whether you know, ML or traditional software development approaches are like a
[01:01:55.480 --> 01:01:58.720]   better tool for achieving those things.
[01:01:58.720 --> 01:02:06.360]   And I think like as long as we continue to think about ML as a tool to solve problems
[01:02:06.360 --> 01:02:11.840]   and as long as we have the tools that enable us to better understand if ML is solving those
[01:02:11.840 --> 01:02:20.360]   problems and how to improve upon its ability to solve those problems, then ML can be a
[01:02:20.360 --> 01:02:28.080]   super powerful tool and one that we learn to wield in more powerful ways too.
[01:02:28.080 --> 01:02:36.320]   But I feel almost like a broken record saying this given the lessons learned in the past.
[01:02:36.320 --> 01:02:43.080]   Like if we treat ML like a silver bullet, if we treat it like a hammer looking for a
[01:02:43.080 --> 01:02:50.760]   nail, like that was the pattern that I think led to failure.
[01:02:50.760 --> 01:02:54.900]   Don't think about like what ML can do for you.
[01:02:54.900 --> 01:03:00.600]   Think about like what you can do for your country and if ML is the right way to do that,
[01:03:00.600 --> 01:03:04.080]   I guess.
[01:03:04.080 --> 01:03:13.400]   I think that's the lesson that we learned and I hope it's the lesson that we will carry
[01:03:13.400 --> 01:03:14.400]   forth.
[01:03:14.400 --> 01:03:16.760]   I love it.
[01:03:16.760 --> 01:03:19.600]   We always end with two open-ended questions.
[01:03:19.600 --> 01:03:25.060]   And the first of the two is, if you had extra time, what's something that you'd like to
[01:03:25.060 --> 01:03:29.960]   spend more time researching or put another way, what's an underrated topic in data or
[01:03:29.960 --> 01:03:30.960]   machine learning?
[01:03:30.960 --> 01:03:36.620]   Oh man, that one is like very easy for me, like programming languages.
[01:03:36.620 --> 01:03:41.980]   I would love to spend more time like learning about like programming languages.
[01:03:41.980 --> 01:03:48.940]   Like I am definitely not convinced that like Python is the right interface for data science
[01:03:48.940 --> 01:03:53.860]   or that SQL is the right interface for analytics work.
[01:03:53.860 --> 01:03:58.940]   But I would really love to learn more about like programming language design so that I
[01:03:58.940 --> 01:04:06.020]   could better diagnose like if and why Python and SQL are the wrong tools and how one might
[01:04:06.020 --> 01:04:11.980]   go about like building a better PL interface for data scientists, ML engineers and analysts.
[01:04:11.980 --> 01:04:12.980]   Okay.
[01:04:12.980 --> 01:04:17.660]   A question that I didn't ask because I thought it was a little weird or maybe nosy is why
[01:04:17.660 --> 01:04:23.420]   you're asking on Twitter if anyone knew any female Rust developers?
[01:04:23.420 --> 01:04:29.420]   Because I will say Rust comes up just a shocking amount on this podcast.
[01:04:29.420 --> 01:04:33.300]   And I was wondering like what's driving the interest in Rust and then if there was some
[01:04:33.300 --> 01:04:38.700]   reason behind looking for a female Rust developer and if you actually found one.
[01:04:38.700 --> 01:04:41.340]   Yeah, yeah.
[01:04:41.340 --> 01:04:43.580]   So full transparency.
[01:04:43.580 --> 01:04:48.700]   And I think like I maybe put some of this on Twitter too.
[01:04:48.700 --> 01:04:53.860]   So like quick background is that, you know, certainly earlier in my career I felt like
[01:04:53.860 --> 01:04:58.780]   oftentimes like I wasn't getting invited to like the same set of events, et cetera, as
[01:04:58.780 --> 01:05:00.420]   some of my male peers.
[01:05:00.420 --> 01:05:05.060]   And therefore like I wasn't getting exposure to like the same set of conversations, maybe
[01:05:05.060 --> 01:05:10.260]   even the same like opportunities to like potentially see deals and things like that.
[01:05:10.260 --> 01:05:17.540]   So like I feel pretty strongly that like we need to have like women in the room when we
[01:05:17.540 --> 01:05:23.740]   host events to ensure that like they're getting exposed to like the same set of opportunities.
[01:05:23.740 --> 01:05:30.180]   That like we're not doing things to like hamper their progress in the industries in which
[01:05:30.180 --> 01:05:33.220]   they operate.
[01:05:33.220 --> 01:05:39.500]   We were hosting a Rust developer dinner and looked at the guest list and like there weren't
[01:05:39.500 --> 01:05:44.700]   that many women and I felt like we could do better.
[01:05:44.700 --> 01:05:49.140]   So thus the origins of my question.
[01:05:49.140 --> 01:05:53.100]   Why Rust?
[01:05:53.100 --> 01:05:59.060]   I wish I spent more time studying programming languages so I could like better understand
[01:05:59.060 --> 01:06:04.620]   like why people are shifting from like C++ to Rust.
[01:06:04.620 --> 01:06:12.460]   Luca Palmieri who I believe is now at AWS actually has like a great blog post on why
[01:06:12.460 --> 01:06:22.740]   Rust might be a more appropriate like backend for Python libraries that often have C++ backends.
[01:06:22.740 --> 01:06:30.940]   Things like Pandas where like we experience it as Python, but in fact like it has a C++
[01:06:30.940 --> 01:06:31.940]   backend.
[01:06:31.940 --> 01:06:36.980]   So I've heard that like Rust is more accessible than C++ and therefore could perhaps like
[01:06:36.980 --> 01:06:44.980]   invite more data practitioners to actually like contribute to some of those projects.
[01:06:44.980 --> 01:06:53.700]   But I don't know enough to really say like why Rust is so magical other than, you know,
[01:06:53.700 --> 01:06:59.460]   a lot of smart people apparently like Linus Torvald to believe it is.
[01:06:59.460 --> 01:07:02.660]   If it's good enough for him, it's good enough for us.
[01:07:02.660 --> 01:07:03.660]   I don't know.
[01:07:03.660 --> 01:07:04.660]   Fair enough.
[01:07:04.660 --> 01:07:05.660]   All right.
[01:07:05.660 --> 01:07:12.540]   Well, my final question for you is when you look at the sort of ML workflow today kind
[01:07:12.540 --> 01:07:17.540]   of going from like, you know, research to deploy it into production, where do you see
[01:07:17.540 --> 01:07:21.820]   the biggest bottlenecks or maybe where do you see the most surprising bottlenecks for,
[01:07:21.820 --> 01:07:23.940]   you know, your portfolio companies?
[01:07:23.940 --> 01:07:25.060]   >> Yeah.
[01:07:25.060 --> 01:07:34.500]   So I generally think that like the -- there are two bottlenecks that I would call attention
[01:07:34.500 --> 01:07:35.500]   to.
[01:07:35.500 --> 01:07:36.500]   Actually, three.
[01:07:36.500 --> 01:07:37.500]   Sorry.
[01:07:37.500 --> 01:07:41.300]   I'm being kind of indecisive here.
[01:07:41.300 --> 01:07:48.580]   One pattern that I've observed with ML is that like we often iterate on ML driven applications
[01:07:48.580 --> 01:07:55.700]   or ML driven features more frequently than we iterate on like more traditional software
[01:07:55.700 --> 01:07:56.700]   features.
[01:07:56.700 --> 01:08:05.900]   So to give an example, like we may iterate on a pricing algorithm like far more frequently
[01:08:05.900 --> 01:08:13.460]   than we would iterate on a like navigation panel or an onboarding flow or something like
[01:08:13.460 --> 01:08:14.460]   that.
[01:08:14.460 --> 01:08:21.660]   You know, earlier I was talking about like understanding how ML can solve like user and
[01:08:21.660 --> 01:08:23.060]   company problems.
[01:08:23.060 --> 01:08:29.220]   I don't really think we have enough insight into the way in which model performance correlates
[01:08:29.220 --> 01:08:37.380]   with behavioral data or like, you know, product engagement to iterate super effectively on
[01:08:37.380 --> 01:08:38.380]   models.
[01:08:38.380 --> 01:08:45.100]   So I think that like that has been a limitation and one that, you know, could have like nefarious
[01:08:45.100 --> 01:08:48.840]   effects in the future.
[01:08:48.840 --> 01:08:53.020]   Another big challenge that I see, and I alluded to this before, is like the challenge of building
[01:08:53.020 --> 01:08:58.780]   software applications around a prediction service or around a model.
[01:08:58.780 --> 01:09:05.340]   I think in the past people might have talked about this as like a model deployment problem.
[01:09:05.340 --> 01:09:11.940]   The problem isn't like, you know, containerizing your model and like implementing a prediction
[01:09:11.940 --> 01:09:13.540]   service in production.
[01:09:13.540 --> 01:09:17.220]   I think like that has gotten significantly easier.
[01:09:17.220 --> 01:09:23.420]   The problem is like connecting to like five different databases each which have like different
[01:09:23.420 --> 01:09:34.580]   sets of like ACID guarantees, latency profiles, also connecting to like a UI service, potentially
[01:09:34.580 --> 01:09:38.860]   like connecting to other application services.
[01:09:38.860 --> 01:09:43.100]   So like the problem is like the software development, like what you've got is a trained model, but
[01:09:43.100 --> 01:09:47.420]   like now you actually have to build a software application.
[01:09:47.420 --> 01:09:55.660]   And I don't think we have great tools to like facilitate that process either for ML engineers
[01:09:55.660 --> 01:09:59.380]   or for software engineers.
[01:09:59.380 --> 01:10:06.260]   And then around the same space, I also think that like the transition from like research
[01:10:06.260 --> 01:10:14.340]   to production can still, and back can still be challenging.
[01:10:14.340 --> 01:10:20.060]   So perhaps like what a company wants to do upon seeing an issue associated with the model
[01:10:20.060 --> 01:10:26.980]   in production is actually see the experiment runs associated with that model so that they
[01:10:26.980 --> 01:10:32.300]   might get more insight into like what is now happening in that production environment.
[01:10:32.300 --> 01:10:38.860]   Like that shouldn't be difficult to do.
[01:10:38.860 --> 01:10:46.220]   But in the past, I think we really developed tools either for model development or for
[01:10:46.220 --> 01:10:47.860]   like ML ops.
[01:10:47.860 --> 01:10:52.940]   And we're starting to see some of the like pain points that arise when like those sets
[01:10:52.940 --> 01:10:57.500]   of tools are not coupled together.
[01:10:57.500 --> 01:10:58.500]   Cool.
[01:10:58.500 --> 01:11:02.060]   Yeah, that all definitely resonates with me.
[01:11:02.060 --> 01:11:08.140]   Lest I sound too cynical, like I am really optimistic about like the future of ML.
[01:11:08.140 --> 01:11:14.420]   I think we just need to do it like in a sane and rational way and be mindful of like what
[01:11:14.420 --> 01:11:21.540]   we're trying to accomplish here instead of just focusing on like flashy press releases
[01:11:21.540 --> 01:11:23.740]   and cool demos.
[01:11:23.740 --> 01:11:29.900]   I was thinking as you were talking about like the hype cycle on like large language models
[01:11:29.900 --> 01:11:34.660]   and stuff, like I was thinking like VCs probably feel the hype cycle like the fastest because
[01:11:34.660 --> 01:11:39.300]   like I'm like, man, we've like basically solved the Turing test and like no one cares.
[01:11:39.300 --> 01:11:41.620]   Like my parents are like, what even is this?
[01:11:41.620 --> 01:11:44.260]   It's like, go on, this is like awesome.
[01:11:44.260 --> 01:11:45.260]   Like look at it.
[01:11:45.260 --> 01:11:49.620]   But I think like, you know, every investor like knows about like stable diffusion, but
[01:11:49.620 --> 01:11:54.980]   I don't think like, I mean, I even come across like chief, you know, chief data officers
[01:11:54.980 --> 01:11:58.340]   at like Fortune 500 companies that are like, what's like stable diffusion?
[01:11:58.340 --> 01:12:02.380]   It's like, come on, you should like, you should know about this.
[01:12:02.380 --> 01:12:03.380]   But anyway.
[01:12:03.380 --> 01:12:04.380]   Yeah, yeah.
[01:12:04.380 --> 01:12:12.140]   I mean, but I think like there's this awareness though of like, this is where the hard work
[01:12:12.140 --> 01:12:13.140]   starts.
[01:12:13.140 --> 01:12:14.140]   Yeah, yeah.
[01:12:14.140 --> 01:12:15.140]   Totally.
[01:12:15.140 --> 01:12:16.140]   Like, great.
[01:12:16.140 --> 01:12:23.500]   So if we're able to like, generate like, beautiful, like artistic renderings based on like textual
[01:12:23.500 --> 01:12:31.900]   prompts, okay, how do we generate like photos that are, you know, equivalent to that which
[01:12:31.900 --> 01:12:35.020]   a professional photographer would produce?
[01:12:35.020 --> 01:12:39.860]   Because like, that's what it's going to take to get like, you know, a Getty images or like
[01:12:39.860 --> 01:12:45.060]   Flickr to adopt something like stable diffusion.
[01:12:45.060 --> 01:12:52.940]   How do we make like automated rotoscoping so good that like, you know, video editor
[01:12:52.940 --> 01:12:57.820]   like doesn't need to correct the mask at all.
[01:12:57.820 --> 01:13:03.060]   Because you know, that's what it's going to take for like, Renwe to compete with like
[01:13:03.060 --> 01:13:05.580]   some of the more traditional video editors.
[01:13:05.580 --> 01:13:11.020]   You know, I saw through Renwe that like, the research is not good enough.
[01:13:11.020 --> 01:13:15.820]   Like they've had to do like a lot of like engineering as well as their own research
[01:13:15.820 --> 01:13:20.340]   in order to like operationalize some of these things.
[01:13:20.340 --> 01:13:28.300]   And so like, I am so optimistic about like the potential of the technologies.
[01:13:28.300 --> 01:13:35.180]   But like, I also am realistic that like reining them in and actually like leveraging these
[01:13:35.180 --> 01:13:45.180]   technologies to like do good in the world or to like build great products is hard.
[01:13:45.180 --> 01:13:46.180]   Short anecdote.
[01:13:46.180 --> 01:13:54.180]   But I've been talking to a founder who is working on like brain computer interfaces
[01:13:54.180 --> 01:14:01.380]   and actually like developed this like technology where like effectively like it's able to read
[01:14:01.380 --> 01:14:02.380]   lines.
[01:14:02.380 --> 01:14:07.620]   Like you had to put on, you know, some like helmet thing.
[01:14:07.620 --> 01:14:13.380]   But like once the helmet was on, it could kind of transcribe thoughts.
[01:14:13.380 --> 01:14:17.300]   And they were able to get it to like work.
[01:14:17.300 --> 01:14:25.580]   Now the founder like subsequently like shifted focus to like the gaming space, doing more
[01:14:25.580 --> 01:14:28.140]   work with like haptic interfaces.
[01:14:28.140 --> 01:14:34.140]   And I was asking him like, why didn't you pursue, you know, the mind reading tech further?
[01:14:34.140 --> 01:14:38.540]   And he said to me, like, we couldn't find any great use cases.
[01:14:38.540 --> 01:14:40.940]   And like, isn't that crazy?
[01:14:40.940 --> 01:14:44.060]   But I think like this is tech.
[01:14:44.060 --> 01:14:50.620]   Like sometimes you can do absolutely remarkable things with technology.
[01:14:50.620 --> 01:14:52.340]   But it doesn't matter.
[01:14:52.340 --> 01:14:58.100]   Like it doesn't matter unless you figure out like how to appeal to people and get them
[01:14:58.100 --> 01:14:59.580]   to use it.
[01:14:59.580 --> 01:15:06.660]   And like how to align that technology with an important set of problems.
[01:15:06.660 --> 01:15:15.580]   So like I think that's that is the thing as VCs we need to like continue to remind ourselves.
[01:15:15.580 --> 01:15:21.660]   Because like I mean, tech is not easy.
[01:15:21.660 --> 01:15:22.660]   Tech is not easy.
[01:15:22.660 --> 01:15:24.940]   But like people are not easy either.
[01:15:24.940 --> 01:15:28.580]   Like both are really hard.
[01:15:28.580 --> 01:15:34.420]   And like unlocking new sets of technologies often means that like we're granted the opportunity
[01:15:34.420 --> 01:15:38.780]   to solve like really hard human problems.
[01:15:38.780 --> 01:15:51.580]   So yeah, I mean, I guess TLDR, you know, if GPP3 starts reading minds, like maybe we should
[01:15:51.580 --> 01:15:55.260]   maybe we'll be able to find some some applications for it.
[01:15:55.260 --> 01:15:56.980]   But we'll see.
[01:15:56.980 --> 01:15:59.180]   Thanks so much, Sarah.
[01:15:59.180 --> 01:16:00.980]   That was super fun.
[01:16:00.980 --> 01:16:02.980]   Yeah, yeah, for sure.
[01:16:02.980 --> 01:16:03.980]   Bye.
[01:16:03.980 --> 01:16:08.940]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:16:08.940 --> 01:16:13.700]   to the show notes in the description where you can find links to all the papers that
[01:16:13.700 --> 01:16:17.940]   are mentioned, supplemental material and a transcription that we worked really hard to
[01:16:17.940 --> 01:16:18.940]   produce.
[01:16:18.940 --> 01:16:19.940]   So check it out.
[01:16:19.940 --> 01:16:22.520]   (upbeat music)

