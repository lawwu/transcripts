
[00:00:00.000 --> 00:00:03.480]   The following is a conversation with Demis Hassabis,
[00:00:03.480 --> 00:00:06.720]   CEO and co-founder of DeepMind,
[00:00:06.720 --> 00:00:08.600]   a company that has published and built
[00:00:08.600 --> 00:00:12.200]   some of the most incredible artificial intelligence systems
[00:00:12.200 --> 00:00:14.120]   in the history of computing,
[00:00:14.120 --> 00:00:18.040]   including AlphaZero that learned all by itself
[00:00:18.040 --> 00:00:21.000]   to play the game of Go better than any human in the world,
[00:00:21.000 --> 00:00:25.760]   and AlphaFold2 that solved protein folding.
[00:00:25.760 --> 00:00:30.320]   Both tasks considered nearly impossible for a very long time.
[00:00:30.320 --> 00:00:33.960]   Demis is widely considered to be one of the most brilliant
[00:00:33.960 --> 00:00:36.560]   and impactful humans in the history
[00:00:36.560 --> 00:00:38.120]   of artificial intelligence
[00:00:38.120 --> 00:00:41.240]   and science and engineering in general.
[00:00:41.240 --> 00:00:44.600]   This was truly an honor and a pleasure for me
[00:00:44.600 --> 00:00:47.360]   to finally sit down with him for this conversation,
[00:00:47.360 --> 00:00:50.560]   and I'm sure we will talk many times again in the future.
[00:00:50.560 --> 00:00:53.280]   This is the Lux Freedman Podcast.
[00:00:53.280 --> 00:00:55.480]   To support it, please check out our sponsors
[00:00:55.480 --> 00:00:56.760]   in the description.
[00:00:56.760 --> 00:01:00.560]   And now, dear friends, here's Demis Hassabis.
[00:01:00.560 --> 00:01:04.000]   Let's start with a bit of a personal question.
[00:01:04.000 --> 00:01:07.800]   Am I an AI program you wrote to interview people
[00:01:07.800 --> 00:01:10.080]   until I get good enough to interview you?
[00:01:10.080 --> 00:01:13.080]   - Well, I'd be impressed if you were.
[00:01:13.080 --> 00:01:14.840]   I'd be impressed with myself if you were.
[00:01:14.840 --> 00:01:16.480]   I don't think we're quite up to that yet,
[00:01:16.480 --> 00:01:18.760]   but maybe you're from the future, Lex.
[00:01:18.760 --> 00:01:20.360]   - If you did, would you tell me?
[00:01:20.360 --> 00:01:23.040]   Is that a good thing to tell a language model
[00:01:23.040 --> 00:01:25.040]   that's tasked with interviewing
[00:01:25.040 --> 00:01:27.400]   that it is in fact AI?
[00:01:27.400 --> 00:01:29.640]   - Maybe we're in a kind of meta-Turing test.
[00:01:29.640 --> 00:01:32.400]   Probably it would be a good idea not to tell you
[00:01:32.400 --> 00:01:33.880]   so it doesn't change your behavior, right?
[00:01:33.880 --> 00:01:34.720]   - This is a kind of--
[00:01:34.720 --> 00:01:37.080]   - Heisenberg uncertainty principle situation.
[00:01:37.080 --> 00:01:39.040]   If I told you, you'd behave differently.
[00:01:39.040 --> 00:01:40.960]   Maybe that's what's happening with us, of course.
[00:01:40.960 --> 00:01:42.760]   - This is a benchmark from the future
[00:01:42.760 --> 00:01:46.520]   where they replay 2022 as a year
[00:01:46.520 --> 00:01:49.400]   before AIs were good enough yet,
[00:01:49.400 --> 00:01:52.080]   and now we want to see, is it gonna pass?
[00:01:52.080 --> 00:01:52.920]   - Exactly.
[00:01:53.960 --> 00:01:56.000]   - If I was such a program,
[00:01:56.000 --> 00:01:57.960]   would you be able to tell, do you think?
[00:01:57.960 --> 00:01:59.960]   So to the Turing test question,
[00:01:59.960 --> 00:02:04.960]   you've talked about the benchmark for solving intelligence.
[00:02:04.960 --> 00:02:07.320]   What would be the impressive thing?
[00:02:07.320 --> 00:02:09.120]   You've talked about winning a Nobel Prize
[00:02:09.120 --> 00:02:11.440]   and AI system winning a Nobel Prize,
[00:02:11.440 --> 00:02:14.400]   but I still return to the Turing test as a compelling test.
[00:02:14.400 --> 00:02:17.280]   The spirit of the Turing test is a compelling test.
[00:02:17.280 --> 00:02:18.520]   - Yeah, the Turing test, of course,
[00:02:18.520 --> 00:02:20.200]   it's been unbelievably influential,
[00:02:20.200 --> 00:02:22.120]   and Turing's one of my all-time heroes,
[00:02:22.120 --> 00:02:25.040]   but I think if you look back at the 1950 papers,
[00:02:25.040 --> 00:02:27.000]   original paper and read the original,
[00:02:27.000 --> 00:02:28.840]   you'll see, I don't think he meant it
[00:02:28.840 --> 00:02:30.880]   to be a rigorous formal test.
[00:02:30.880 --> 00:02:32.880]   I think it was more like a thought experiment,
[00:02:32.880 --> 00:02:34.640]   almost a bit of philosophy he was writing
[00:02:34.640 --> 00:02:36.440]   if you look at the style of the paper.
[00:02:36.440 --> 00:02:38.680]   And you can see he didn't specify it very rigorously.
[00:02:38.680 --> 00:02:41.840]   So for example, he didn't specify the knowledge
[00:02:41.840 --> 00:02:44.040]   that the expert or judge would have,
[00:02:44.040 --> 00:02:48.320]   how much time would they have to investigate this.
[00:02:48.320 --> 00:02:49.480]   So these important parameters,
[00:02:49.480 --> 00:02:53.200]   if you were gonna make it a true sort of formal test.
[00:02:53.200 --> 00:02:56.600]   And by some measures,
[00:02:56.600 --> 00:02:59.040]   people claim the Turing test passed several,
[00:02:59.040 --> 00:03:00.920]   a decade ago, I remember someone claiming that
[00:03:00.920 --> 00:03:05.920]   with a kind of very bog standard normal logic model,
[00:03:05.920 --> 00:03:08.440]   because they pretended it was a kid.
[00:03:08.440 --> 00:03:13.280]   So the judges thought that the machine was a child.
[00:03:13.280 --> 00:03:17.600]   So that would be very different from an expert AI person
[00:03:17.600 --> 00:03:19.880]   interrogating machine and knowing how it was built
[00:03:19.880 --> 00:03:20.720]   and so on.
[00:03:20.720 --> 00:03:24.560]   So I think, we should probably move away from that
[00:03:24.560 --> 00:03:28.800]   as a formal test and move more towards a general test
[00:03:28.800 --> 00:03:32.040]   where we test the AI capabilities on a range of tasks
[00:03:32.040 --> 00:03:35.320]   and see if it reaches human level or above performance
[00:03:35.320 --> 00:03:37.160]   on maybe thousands,
[00:03:37.160 --> 00:03:39.200]   perhaps even millions of tasks eventually
[00:03:39.200 --> 00:03:41.920]   and cover the entire sort of cognitive space.
[00:03:41.920 --> 00:03:44.120]   So I think for its time,
[00:03:44.120 --> 00:03:46.960]   it was an amazing thought experiment and also 1950s,
[00:03:46.960 --> 00:03:49.440]   obviously it was barely the dawn of the computer age.
[00:03:49.440 --> 00:03:51.480]   So of course he only thought about text
[00:03:51.480 --> 00:03:54.600]   and now we have a lot more different inputs.
[00:03:54.600 --> 00:03:57.080]   - So yeah, maybe the better thing to test
[00:03:57.080 --> 00:03:58.360]   is the generalizability.
[00:03:58.360 --> 00:03:59.680]   So across multiple tasks,
[00:03:59.680 --> 00:04:04.560]   but I think it's also possible as systems like God or show
[00:04:04.560 --> 00:04:08.320]   that eventually that might map right back to language.
[00:04:08.320 --> 00:04:10.800]   So you might be able to demonstrate your ability
[00:04:10.800 --> 00:04:14.760]   to generalize across tasks by then communicating
[00:04:14.760 --> 00:04:17.080]   your ability to generalize across tasks,
[00:04:17.080 --> 00:04:19.200]   which is kind of what we do through conversation anyway,
[00:04:19.200 --> 00:04:20.800]   when we jump around.
[00:04:20.800 --> 00:04:23.720]   Ultimately what's in there in that conversation
[00:04:23.720 --> 00:04:27.000]   is not just you moving around knowledge,
[00:04:27.000 --> 00:04:30.360]   it's you moving around like these entirely different
[00:04:30.360 --> 00:04:34.920]   modalities of understanding that ultimately map
[00:04:34.920 --> 00:04:38.920]   to your ability to operate successfully
[00:04:38.920 --> 00:04:42.600]   in all of these domains, which you can think of as tasks.
[00:04:42.600 --> 00:04:44.680]   - Yeah, I think certainly we as humans,
[00:04:44.680 --> 00:04:48.440]   use language as our main generalization communication tool.
[00:04:48.440 --> 00:04:51.280]   So I think we end up thinking in language
[00:04:51.280 --> 00:04:54.320]   and expressing our solutions in language.
[00:04:54.320 --> 00:04:58.000]   So it's gonna be very powerful mode
[00:04:58.000 --> 00:05:03.000]   in which to explain the system, to explain what it's doing.
[00:05:03.000 --> 00:05:07.480]   But I don't think it's the only modality that matters.
[00:05:07.480 --> 00:05:09.960]   So I think there's gonna be a lot of,
[00:05:09.960 --> 00:05:13.400]   there's a lot of different ways to express capabilities
[00:05:13.400 --> 00:05:15.600]   other than just language.
[00:05:15.600 --> 00:05:19.040]   - Yeah, visual, robotics, body language.
[00:05:19.040 --> 00:05:23.800]   Yeah, action is the interactive aspect of all that.
[00:05:23.800 --> 00:05:24.800]   That's all part of it.
[00:05:24.800 --> 00:05:26.800]   - But what's interesting with GATO is that
[00:05:26.800 --> 00:05:30.240]   it's sort of pushing prediction to the maximum
[00:05:30.240 --> 00:05:33.440]   in terms of like, mapping arbitrary sequences
[00:05:33.440 --> 00:05:35.480]   to other sequences and sort of just predicting
[00:05:35.480 --> 00:05:36.440]   what's gonna happen next.
[00:05:36.440 --> 00:05:41.040]   So prediction seems to be fundamental to intelligence.
[00:05:41.040 --> 00:05:44.160]   - And what you're predicting doesn't so much matter.
[00:05:44.160 --> 00:05:46.840]   - Yeah, it seems like you can generalize that quite well.
[00:05:46.840 --> 00:05:49.640]   So obviously language models predict the next word.
[00:05:49.640 --> 00:05:53.880]   GATO predicts potentially any action or any token.
[00:05:53.880 --> 00:05:55.320]   And it's just the beginning really.
[00:05:55.320 --> 00:05:58.080]   It's our most general agent one could call it so far.
[00:05:58.080 --> 00:06:01.240]   But that itself can be scaled up massively more
[00:06:01.240 --> 00:06:02.160]   than we've done so far.
[00:06:02.160 --> 00:06:04.240]   And obviously we're in the middle of doing that.
[00:06:04.240 --> 00:06:06.880]   - But the big part of solving AGI
[00:06:06.880 --> 00:06:11.020]   is creating benchmarks that help us get closer and closer.
[00:06:11.020 --> 00:06:14.880]   Sort of creating benchmarks that test the generalizability.
[00:06:14.880 --> 00:06:17.400]   And it's just still interesting that this fella,
[00:06:17.400 --> 00:06:20.480]   Alan Turing, was one of the first
[00:06:20.480 --> 00:06:22.560]   and probably still one of the only people
[00:06:22.560 --> 00:06:25.000]   that was trying, maybe philosophically,
[00:06:25.000 --> 00:06:26.800]   but was trying to formulate a benchmark
[00:06:26.800 --> 00:06:27.840]   that could be followed.
[00:06:27.840 --> 00:06:30.920]   It is, even though it's fuzzy,
[00:06:30.920 --> 00:06:32.480]   it's still sufficiently rigorous
[00:06:32.480 --> 00:06:33.960]   to where you can run that test.
[00:06:33.960 --> 00:06:36.600]   And I still think something like the Turing test
[00:06:36.600 --> 00:06:39.320]   will at the end of the day be the thing
[00:06:39.320 --> 00:06:42.500]   that truly impresses other humans.
[00:06:42.500 --> 00:06:46.380]   So that you can have a close friend who's in the AI system.
[00:06:46.380 --> 00:06:48.300]   For that friend to be a good friend,
[00:06:48.300 --> 00:06:53.120]   they're going to have to be able to play StarCraft.
[00:06:53.120 --> 00:06:56.580]   And they're gonna have to do all of these tasks.
[00:06:56.580 --> 00:06:59.540]   Get you a beer, so the robotics tasks.
[00:06:59.540 --> 00:07:02.060]   Play games with you.
[00:07:02.060 --> 00:07:04.760]   Use language, humor, all of those kinds of things.
[00:07:04.760 --> 00:07:07.980]   But that ultimately can boil down to language.
[00:07:07.980 --> 00:07:11.180]   It feels like, not in terms of the AI community,
[00:07:11.180 --> 00:07:13.100]   but in terms of the actual impact
[00:07:13.100 --> 00:07:14.780]   of general intelligence on the world,
[00:07:14.780 --> 00:07:16.620]   it feels like language will be the place
[00:07:16.620 --> 00:07:18.460]   where it truly shines.
[00:07:18.460 --> 00:07:20.620]   - I think so, because it's such an important
[00:07:20.620 --> 00:07:22.480]   kind of input/output for us.
[00:07:22.480 --> 00:07:23.320]   I think you're right.
[00:07:23.320 --> 00:07:25.780]   I think the Turing test, the kind of,
[00:07:25.780 --> 00:07:28.860]   the philosophy behind it, which is the idea of,
[00:07:28.860 --> 00:07:33.860]   can a machine mimic the behaviors of a human?
[00:07:34.980 --> 00:07:38.700]   And I would say wider than just language and text.
[00:07:38.700 --> 00:07:41.320]   Then, in terms of actions and everything else,
[00:07:41.320 --> 00:07:42.700]   creativity, all these things,
[00:07:42.700 --> 00:07:45.320]   then if it can sort of match or exceed
[00:07:45.320 --> 00:07:47.340]   human cognitive capabilities,
[00:07:47.340 --> 00:07:49.900]   then I think we have a true intelligence.
[00:07:49.900 --> 00:07:51.660]   So from that perspective, you're right.
[00:07:51.660 --> 00:07:55.980]   I think he did formulate the right kind of setup.
[00:07:55.980 --> 00:07:59.280]   - I just, I think there'll be a kind of humor
[00:07:59.280 --> 00:08:00.920]   in the AI systems of the future,
[00:08:00.920 --> 00:08:02.620]   looking back to this conversation,
[00:08:03.540 --> 00:08:06.020]   and thinking about the Turing test,
[00:08:06.020 --> 00:08:09.380]   and also thinking about, by that time,
[00:08:09.380 --> 00:08:11.900]   they would know which year they were finally able
[00:08:11.900 --> 00:08:15.580]   to sort of cross the threshold of human-level intelligence,
[00:08:15.580 --> 00:08:18.820]   and think how funny it is that we humans
[00:08:18.820 --> 00:08:21.620]   were still confused about this whole problem.
[00:08:21.620 --> 00:08:22.460]   (Lex laughs)
[00:08:22.460 --> 00:08:23.460]   - Absolutely. - That's been long ago
[00:08:23.460 --> 00:08:24.740]   been solved.
[00:08:24.740 --> 00:08:27.260]   Well, hello to the AI agents of the future.
[00:08:27.260 --> 00:08:30.300]   Anyway, so going back to your journey,
[00:08:30.300 --> 00:08:33.780]   when did you fall in love with programming first?
[00:08:33.780 --> 00:08:35.940]   - Well, I was pretty young age, actually.
[00:08:35.940 --> 00:08:40.860]   So I started off, actually, games was my first love,
[00:08:40.860 --> 00:08:43.680]   so starting to play chess when I was around four years old,
[00:08:43.680 --> 00:08:46.160]   and then it was actually with winnings
[00:08:46.160 --> 00:08:48.400]   from a chess competition that I managed
[00:08:48.400 --> 00:08:49.780]   to buy my first chess computer
[00:08:49.780 --> 00:08:50.820]   when I was about eight years old.
[00:08:50.820 --> 00:08:53.180]   It was a ZX Spectrum, which was hugely popular
[00:08:53.180 --> 00:08:54.740]   in the UK at the time.
[00:08:54.740 --> 00:08:56.540]   And it was an amazing machine,
[00:08:56.540 --> 00:08:59.320]   because I think it trained a whole generation
[00:08:59.320 --> 00:09:02.540]   of programmers in the UK, because it was so accessible.
[00:09:02.540 --> 00:09:03.820]   You literally switched it on,
[00:09:03.820 --> 00:09:05.140]   and there was the basic prompt,
[00:09:05.140 --> 00:09:06.660]   and you could just get going.
[00:09:06.660 --> 00:09:10.460]   And my parents didn't really know anything about computers,
[00:09:10.460 --> 00:09:12.580]   but because it was my money from a chess competition,
[00:09:12.580 --> 00:09:14.900]   I could say I wanted to buy it.
[00:09:14.900 --> 00:09:17.940]   And then I just went to bookstores,
[00:09:17.940 --> 00:09:19.860]   got books on programming,
[00:09:19.860 --> 00:09:23.500]   and started typing in the programming code.
[00:09:23.500 --> 00:09:26.460]   And then, of course, once you start doing that,
[00:09:26.460 --> 00:09:29.140]   you start adjusting it, and then making your own games.
[00:09:29.140 --> 00:09:30.840]   And that's when I fell in love with computers
[00:09:30.840 --> 00:09:33.880]   and realised that they were a very magical device.
[00:09:33.880 --> 00:09:36.440]   In a way, I wouldn't have been able
[00:09:36.440 --> 00:09:37.420]   to explain this at the time,
[00:09:37.420 --> 00:09:38.600]   but I felt that they were sort of
[00:09:38.600 --> 00:09:40.920]   almost a magical extension of your mind.
[00:09:40.920 --> 00:09:42.120]   I always had this feeling,
[00:09:42.120 --> 00:09:43.860]   and I've always loved this about computers,
[00:09:43.860 --> 00:09:46.160]   that you can set them off doing something,
[00:09:46.160 --> 00:09:48.500]   some task for you, you can go to sleep,
[00:09:48.500 --> 00:09:50.500]   come back the next day, and it's solved.
[00:09:50.500 --> 00:09:53.080]   That feels magical to me.
[00:09:53.080 --> 00:09:55.280]   So, I mean, all machines do that to some extent.
[00:09:55.280 --> 00:09:57.660]   They all enhance our natural capabilities.
[00:09:57.660 --> 00:10:00.120]   Obviously, cars make us, allow us to move faster
[00:10:00.120 --> 00:10:04.560]   than we can run, but this was a machine to extend the mind.
[00:10:04.560 --> 00:10:08.520]   And then, of course, AI is the ultimate expression
[00:10:08.520 --> 00:10:11.400]   of what a machine may be able to do or learn.
[00:10:11.400 --> 00:10:13.560]   So, very naturally for me,
[00:10:13.560 --> 00:10:16.080]   that thought extended into AI quite quickly.
[00:10:16.080 --> 00:10:18.600]   - Do you remember the programming language
[00:10:18.600 --> 00:10:20.400]   that was first started?
[00:10:20.400 --> 00:10:21.240]   - Yeah.
[00:10:21.240 --> 00:10:22.080]   - Was it special to the machine?
[00:10:22.080 --> 00:10:23.600]   - No, it was just a basic.
[00:10:23.600 --> 00:10:25.920]   I think it was just basic on the ZX Spectrum.
[00:10:25.920 --> 00:10:27.520]   I don't know what specific form it was.
[00:10:27.520 --> 00:10:29.660]   And then later on, I got a Commodore Amiga,
[00:10:29.660 --> 00:10:32.820]   which was a fantastic machine.
[00:10:32.820 --> 00:10:33.820]   - Now you're just showing off.
[00:10:33.820 --> 00:10:36.540]   - So, yeah, well, lots of my friends had Atari STs,
[00:10:36.540 --> 00:10:37.860]   and I managed to get Amigas.
[00:10:37.860 --> 00:10:40.720]   It was a bit more powerful, and that was incredible.
[00:10:40.720 --> 00:10:43.820]   And used to do programming in Assembler,
[00:10:43.820 --> 00:10:48.140]   and also Amos Basic, this specific form of basic.
[00:10:48.140 --> 00:10:49.320]   It was incredible, actually.
[00:10:49.320 --> 00:10:51.060]   So, I learned all my coding skills.
[00:10:51.060 --> 00:10:53.060]   - And when did you fall in love with AI?
[00:10:53.060 --> 00:10:56.900]   So, when did you first start to gain an understanding
[00:10:56.900 --> 00:10:58.880]   that you can not just write programs
[00:10:58.880 --> 00:11:01.640]   that do some mathematical operations for you
[00:11:01.640 --> 00:11:05.400]   while you sleep, but something that's akin
[00:11:05.400 --> 00:11:08.800]   to bringing an entity to life?
[00:11:08.800 --> 00:11:11.840]   Sort of a thing that can figure out something
[00:11:11.840 --> 00:11:15.920]   more complicated than a simple mathematical operation.
[00:11:15.920 --> 00:11:17.600]   - Yeah, so there was a few stages for me,
[00:11:17.600 --> 00:11:18.940]   all while I was very young.
[00:11:18.940 --> 00:11:21.720]   So, first of all, as I was trying to improve
[00:11:21.720 --> 00:11:23.160]   at playing chess, I was captaining
[00:11:23.160 --> 00:11:24.720]   various England junior chess teams.
[00:11:24.860 --> 00:11:27.500]   At the time, when I was about maybe 10, 11 years old,
[00:11:27.500 --> 00:11:29.380]   I was gonna become a professional chess player.
[00:11:29.380 --> 00:11:31.280]   That was my first thought.
[00:11:31.280 --> 00:11:34.700]   - So, that dream was there to try to get
[00:11:34.700 --> 00:11:35.540]   to the highest level of chess.
[00:11:35.540 --> 00:11:39.180]   - Yeah, so I was, when I was about 12 years old,
[00:11:39.180 --> 00:11:41.380]   I got to Master Standard, and I was second highest rated
[00:11:41.380 --> 00:11:42.740]   player in the world to Judith Polgar,
[00:11:42.740 --> 00:11:45.780]   who obviously ended up being an amazing chess player,
[00:11:45.780 --> 00:11:48.600]   and a world women's champion.
[00:11:48.600 --> 00:11:50.780]   And when I was trying to improve at chess,
[00:11:50.780 --> 00:11:52.780]   what you do is, obviously, first of all,
[00:11:52.780 --> 00:11:55.140]   you're trying to improve your own thinking processes.
[00:11:55.140 --> 00:11:58.120]   So, that leads you to thinking about thinking.
[00:11:58.120 --> 00:12:00.420]   How is your brain coming up with these ideas?
[00:12:00.420 --> 00:12:01.940]   Why is it making mistakes?
[00:12:01.940 --> 00:12:04.580]   How can you improve that thought process?
[00:12:04.580 --> 00:12:06.380]   But the second thing is that you,
[00:12:06.380 --> 00:12:09.700]   it was just the beginning, this was like in the early 80s,
[00:12:09.700 --> 00:12:11.280]   mid 80s, of chess computers.
[00:12:11.280 --> 00:12:12.800]   If you remember, they were physical balls,
[00:12:12.800 --> 00:12:14.020]   like the one we have in front of us,
[00:12:14.020 --> 00:12:17.020]   and you press down the squares.
[00:12:17.020 --> 00:12:19.660]   And I think Kasparov had a branded version of it
[00:12:19.660 --> 00:12:21.020]   that I got.
[00:12:21.020 --> 00:12:22.980]   And you were, you know, you used to,
[00:12:22.980 --> 00:12:24.700]   they're not as strong as they are today,
[00:12:24.700 --> 00:12:26.380]   but they were pretty strong,
[00:12:26.380 --> 00:12:29.060]   and you used to practice against them
[00:12:29.060 --> 00:12:31.460]   to try and improve your openings and other things.
[00:12:31.460 --> 00:12:33.500]   And so I remember, I think I probably got my first one,
[00:12:33.500 --> 00:12:34.940]   I was around 11 or 12.
[00:12:34.940 --> 00:12:37.420]   And I remember thinking, this is amazing,
[00:12:37.420 --> 00:12:39.260]   you know, how has someone programmed
[00:12:39.260 --> 00:12:42.940]   this chess board to play chess?
[00:12:42.940 --> 00:12:45.660]   And it was a very formative book I bought,
[00:12:45.660 --> 00:12:49.020]   which was called The Chess Computer Handbook by David Levy.
[00:12:49.020 --> 00:12:50.700]   This thing came out in 1984 or something,
[00:12:50.700 --> 00:12:52.380]   so I must have got it when I was about 11, 12.
[00:12:52.380 --> 00:12:56.140]   And it explained fully how these chess programs were made.
[00:12:56.140 --> 00:12:58.940]   And I remember my first AI program being,
[00:12:58.940 --> 00:13:00.420]   programming my Amiga,
[00:13:00.420 --> 00:13:02.900]   it couldn't, it wasn't powerful enough to play chess,
[00:13:02.900 --> 00:13:04.220]   I couldn't write a whole chess program,
[00:13:04.220 --> 00:13:06.620]   but I wrote a program for it to play Othello,
[00:13:06.620 --> 00:13:09.340]   or Reversi, it's sometimes called, I think, in the US.
[00:13:09.340 --> 00:13:11.780]   And so a slightly simpler game than chess,
[00:13:11.780 --> 00:13:14.380]   but I used all of the principles that chess programs had,
[00:13:14.380 --> 00:13:16.020]   alpha, beta, search, all of that.
[00:13:16.020 --> 00:13:17.420]   And that was my first AI program,
[00:13:17.420 --> 00:13:19.420]   I remember that very well, I was around 12 years old.
[00:13:19.420 --> 00:13:21.660]   So that brought me into AI.
[00:13:21.660 --> 00:13:24.100]   And then the second part was later on,
[00:13:24.100 --> 00:13:25.540]   I was around 16, 17,
[00:13:25.540 --> 00:13:28.820]   and I was writing games professionally, designing games,
[00:13:28.820 --> 00:13:30.620]   writing a game called Theme Park,
[00:13:30.620 --> 00:13:34.020]   which had AI as a core gameplay component
[00:13:34.020 --> 00:13:35.660]   as part of the simulation.
[00:13:35.660 --> 00:13:38.460]   And it sold millions of copies around the world,
[00:13:38.460 --> 00:13:40.980]   and people loved the way that the AI,
[00:13:40.980 --> 00:13:44.460]   even though it was relatively simple by today's AI standards,
[00:13:44.460 --> 00:13:47.700]   was reacting to the way you as the player played it.
[00:13:47.700 --> 00:13:49.220]   So it was called a sandbox game,
[00:13:49.220 --> 00:13:51.340]   so it was one of the first types of games like that,
[00:13:51.340 --> 00:13:52.660]   along with SimCity,
[00:13:52.660 --> 00:13:55.700]   and it meant that every game you played was unique.
[00:13:55.700 --> 00:13:57.100]   - Is there something you could say,
[00:13:57.100 --> 00:13:58.860]   just on a small tangent,
[00:13:58.860 --> 00:14:02.180]   about really impressive AI
[00:14:02.180 --> 00:14:06.540]   from a game design, human enjoyment perspective,
[00:14:06.540 --> 00:14:09.660]   really impressive AI that you've seen in games,
[00:14:09.660 --> 00:14:12.500]   and maybe what does it take to create AI system,
[00:14:12.500 --> 00:14:14.220]   and how hard of a problem is that?
[00:14:14.220 --> 00:14:18.340]   So a million questions, just as a brief tangent.
[00:14:18.340 --> 00:14:22.660]   - Well, look, I think games have been significant
[00:14:22.660 --> 00:14:23.700]   in my life for three reasons.
[00:14:23.700 --> 00:14:26.100]   So first of all, I was playing them
[00:14:26.100 --> 00:14:28.780]   and training myself on games when I was a kid.
[00:14:28.780 --> 00:14:31.460]   Then I went through a phase of designing games
[00:14:31.460 --> 00:14:32.980]   and writing AI for games.
[00:14:32.980 --> 00:14:35.100]   So all the games I professionally wrote
[00:14:35.100 --> 00:14:37.700]   had AI as a core component.
[00:14:37.700 --> 00:14:40.060]   And that was mostly in the '90s,
[00:14:40.060 --> 00:14:42.980]   and the reason I was doing that in games industry
[00:14:42.980 --> 00:14:45.100]   was at the time, the games industry,
[00:14:45.100 --> 00:14:47.180]   I think, was the cutting edge of technology.
[00:14:47.180 --> 00:14:49.820]   So whether it was graphics with people like John Carmack
[00:14:49.820 --> 00:14:53.060]   and Quake and those kind of things, or AI,
[00:14:53.060 --> 00:14:56.140]   I think actually all the action was going on in games.
[00:14:56.140 --> 00:14:58.460]   And we're still reaping the benefits of that,
[00:14:58.460 --> 00:15:01.500]   even with things like GPUs, which I find ironic,
[00:15:01.500 --> 00:15:03.700]   was obviously invented for graphics, computer graphics,
[00:15:03.700 --> 00:15:06.220]   but then turns out to be amazingly useful for AI.
[00:15:06.220 --> 00:15:08.380]   It just turns out everything's a matrix multiplication,
[00:15:08.380 --> 00:15:11.140]   it appears in the whole world.
[00:15:11.140 --> 00:15:15.780]   So I think games at the time had the most cutting edge AI,
[00:15:15.780 --> 00:15:19.780]   and a lot of the games, I was involved in writing,
[00:15:19.780 --> 00:15:21.220]   so there was a game called "Black and White,"
[00:15:21.220 --> 00:15:22.700]   which was one game I was involved with
[00:15:22.700 --> 00:15:24.820]   in the early stages of, which I still think
[00:15:24.820 --> 00:15:29.340]   is the most impressive example of reinforcement learning
[00:15:29.340 --> 00:15:30.500]   in a computer game.
[00:15:30.500 --> 00:15:33.620]   So in that game, you trained a little pet animal.
[00:15:33.620 --> 00:15:35.500]   - It's a brilliant game.
[00:15:35.500 --> 00:15:36.340]   - Yeah, and it sort of learned
[00:15:36.340 --> 00:15:37.620]   from how you were treating it.
[00:15:37.620 --> 00:15:40.660]   So if you treated it badly, then it became mean,
[00:15:40.660 --> 00:15:42.900]   and then it would be mean to your villagers
[00:15:42.900 --> 00:15:45.020]   and your population, the sort of,
[00:15:45.020 --> 00:15:47.220]   the little tribe that you were running.
[00:15:47.220 --> 00:15:49.380]   But if you were kind to it, then it would be kind.
[00:15:49.380 --> 00:15:51.060]   And people were fascinated by how that worked,
[00:15:51.060 --> 00:15:52.300]   and so was I, to be honest,
[00:15:52.300 --> 00:15:54.100]   with the way it kind of developed.
[00:15:54.100 --> 00:15:55.940]   And-- - Especially the mapping
[00:15:55.940 --> 00:15:57.500]   to good and evil. - Yeah.
[00:15:57.500 --> 00:15:59.740]   - It made you realize, made me realize
[00:15:59.740 --> 00:16:02.300]   that you can sort of, in the way,
[00:16:02.300 --> 00:16:07.300]   in the choices you make, can define where you end up,
[00:16:07.300 --> 00:16:12.420]   and that means all of us are capable of the good, evil.
[00:16:12.620 --> 00:16:15.260]   It all matters in the different choices
[00:16:15.260 --> 00:16:18.220]   along the trajectory to those places that you make.
[00:16:18.220 --> 00:16:19.060]   It's fascinating.
[00:16:19.060 --> 00:16:21.380]   I mean, games can do that philosophically to you,
[00:16:21.380 --> 00:16:22.540]   and it's rare, it seems rare.
[00:16:22.540 --> 00:16:24.680]   - Yeah, well, games are, I think, a unique medium,
[00:16:24.680 --> 00:16:26.580]   because you as the player,
[00:16:26.580 --> 00:16:30.080]   you're not just passively consuming the entertainment,
[00:16:30.080 --> 00:16:34.300]   right, you're actually actively involved as an agent.
[00:16:34.300 --> 00:16:36.180]   So I think that's what makes it, in some ways,
[00:16:36.180 --> 00:16:38.420]   can be more visceral than other mediums
[00:16:38.420 --> 00:16:40.020]   like films and books.
[00:16:40.020 --> 00:16:42.660]   So the second, so that was designing AI in games,
[00:16:42.660 --> 00:16:46.460]   and then the third use we've used of AI
[00:16:46.460 --> 00:16:48.460]   is in "Deep Mind," from the beginning,
[00:16:48.460 --> 00:16:50.940]   which is using games as a testing ground
[00:16:50.940 --> 00:16:55.020]   for proving out AI algorithms and developing AI algorithms.
[00:16:55.020 --> 00:16:59.100]   And that was a sort of a core component of our vision
[00:16:59.100 --> 00:17:00.360]   at the start of "Deep Mind,"
[00:17:00.360 --> 00:17:03.220]   was that we would use games very heavily
[00:17:03.220 --> 00:17:06.420]   as our main testing ground, certainly to begin with,
[00:17:06.420 --> 00:17:08.600]   because it's super efficient to use games,
[00:17:08.600 --> 00:17:11.500]   and also, you know, it's very easy to have metrics
[00:17:11.500 --> 00:17:14.100]   to see how well your systems are improving
[00:17:14.100 --> 00:17:15.900]   and what direction your ideas are going in
[00:17:15.900 --> 00:17:18.420]   and whether you're making incremental improvements.
[00:17:18.420 --> 00:17:20.420]   - And because those games are often rooted
[00:17:20.420 --> 00:17:23.420]   in something that humans did for a long time beforehand,
[00:17:23.420 --> 00:17:26.540]   there's already a strong set of rules,
[00:17:26.540 --> 00:17:28.280]   like it's already a damn good benchmark.
[00:17:28.280 --> 00:17:30.220]   - Yes, it's really good for so many reasons,
[00:17:30.220 --> 00:17:32.860]   because you've got clear measures
[00:17:32.860 --> 00:17:35.580]   of how good humans can be at these things.
[00:17:35.580 --> 00:17:36.860]   And in some cases, like "Go,"
[00:17:36.860 --> 00:17:39.760]   we've been playing it for thousands of years,
[00:17:39.760 --> 00:17:43.340]   and often they have scores or at least win conditions.
[00:17:43.340 --> 00:17:45.660]   So it's very easy for reward learning systems
[00:17:45.660 --> 00:17:46.500]   to get a reward.
[00:17:46.500 --> 00:17:49.340]   It's very easy to specify what that reward is.
[00:17:49.340 --> 00:17:53.620]   And also at the end, it's easy to test externally,
[00:17:53.620 --> 00:17:56.120]   you know, how strong is your system,
[00:17:56.120 --> 00:17:58.140]   by of course playing against, you know,
[00:17:58.140 --> 00:18:00.220]   the world's strongest players at those games.
[00:18:00.220 --> 00:18:02.700]   So it's so good for so many reasons,
[00:18:02.700 --> 00:18:03.940]   and it's also very efficient
[00:18:03.940 --> 00:18:06.660]   to run potentially millions of simulations
[00:18:06.660 --> 00:18:08.260]   in parallel on the cloud.
[00:18:08.260 --> 00:18:12.820]   So I think there's a huge reason why we were so successful
[00:18:12.820 --> 00:18:14.740]   back in, you know, starting out 2010,
[00:18:14.740 --> 00:18:16.660]   how come we were able to progress so quickly,
[00:18:16.660 --> 00:18:18.880]   because we've utilized games.
[00:18:18.880 --> 00:18:21.300]   And, you know, at the beginning of "DeepMind,"
[00:18:21.300 --> 00:18:24.580]   we also hired some amazing game engineers
[00:18:24.580 --> 00:18:28.020]   who I knew from my previous lives in the games industry,
[00:18:28.020 --> 00:18:30.940]   and that helped to bootstrap us very quickly.
[00:18:30.940 --> 00:18:33.860]   - And plus it's somehow super compelling,
[00:18:33.860 --> 00:18:38.060]   almost at a philosophical level of man versus machine
[00:18:38.060 --> 00:18:41.220]   over chess board or a Go board.
[00:18:41.220 --> 00:18:43.620]   And especially given that the entire history of AI
[00:18:43.620 --> 00:18:45.980]   is defined by people saying it's gonna be impossible
[00:18:45.980 --> 00:18:50.980]   to make a machine that beats a human being in chess.
[00:18:50.980 --> 00:18:54.580]   And then once that happened, people were certain
[00:18:54.580 --> 00:18:58.020]   when I was coming up in AI that Go is not a game
[00:18:58.020 --> 00:19:01.220]   that can be solved because of the combinatorial complexity.
[00:19:01.220 --> 00:19:04.140]   It's just too, it's, you know,
[00:19:04.140 --> 00:19:06.640]   no matter how much Moore's law you have,
[00:19:06.640 --> 00:19:08.580]   compute is just never going to be able
[00:19:08.580 --> 00:19:10.180]   to crack the game of Go.
[00:19:10.180 --> 00:19:14.900]   And so then there's something compelling about facing,
[00:19:14.900 --> 00:19:18.140]   sort of taking on the impossibility of that task
[00:19:18.140 --> 00:19:23.140]   from the AI researcher perspective, engineer perspective,
[00:19:23.140 --> 00:19:27.020]   and then as a human being just observing this whole thing,
[00:19:27.020 --> 00:19:31.500]   your beliefs about what you thought was impossible
[00:19:31.500 --> 00:19:37.500]   being broken apart, it's humbling to realize
[00:19:37.500 --> 00:19:40.500]   we're not as smart as we thought.
[00:19:40.500 --> 00:19:43.140]   It's humbling to realize that the things we think
[00:19:43.140 --> 00:19:46.980]   are impossible now perhaps will be done in the future.
[00:19:46.980 --> 00:19:50.820]   There's something really powerful about a game,
[00:19:50.820 --> 00:19:52.900]   AI system beating a human being in a game
[00:19:52.900 --> 00:19:55.700]   that drives that message home
[00:19:55.700 --> 00:19:58.020]   for like millions, billions of people,
[00:19:58.020 --> 00:19:59.340]   especially in the case of Go.
[00:19:59.340 --> 00:20:00.520]   - Sure.
[00:20:00.520 --> 00:20:01.940]   Well, look, I think it's a, I mean,
[00:20:01.940 --> 00:20:03.740]   it has been a fascinating journey,
[00:20:03.740 --> 00:20:06.900]   and especially as I think about it from,
[00:20:06.900 --> 00:20:08.780]   I can understand it from both sides,
[00:20:08.780 --> 00:20:13.100]   both as the AI, you know, creators of the AI,
[00:20:13.100 --> 00:20:15.660]   but also as a games player originally.
[00:20:15.660 --> 00:20:17.540]   So, you know, it was a really interesting,
[00:20:17.540 --> 00:20:20.380]   you know, I mean, it was a fantastic,
[00:20:20.380 --> 00:20:22.100]   but also somewhat bittersweet moment,
[00:20:22.100 --> 00:20:25.340]   the AlphaGo match for me, seeing that,
[00:20:25.340 --> 00:20:28.500]   and being obviously heavily, heavily involved in that.
[00:20:28.500 --> 00:20:32.500]   But, you know, as you say, chess has been the,
[00:20:32.500 --> 00:20:34.380]   I mean, Kasparov, I think rightly called it
[00:20:34.380 --> 00:20:37.300]   the Drosophila of intelligence, right?
[00:20:37.300 --> 00:20:39.500]   So it's sort of, I love that phrase,
[00:20:39.500 --> 00:20:42.940]   and I think he's right, because chess has been
[00:20:42.940 --> 00:20:45.380]   hand in hand with AI from the beginning
[00:20:45.380 --> 00:20:47.420]   of the whole field, right?
[00:20:47.420 --> 00:20:49.580]   So I think every AI practitioner,
[00:20:49.580 --> 00:20:52.420]   starting with Turing and Claude Shannon and all those,
[00:20:52.420 --> 00:20:55.380]   the sort of forefathers of the field,
[00:20:55.380 --> 00:20:58.820]   tried their hand at writing a chess program.
[00:20:58.820 --> 00:21:01.140]   I've got original edition of Claude Shannon's
[00:21:01.140 --> 00:21:03.980]   first chess program, I think it was 1949,
[00:21:03.980 --> 00:21:08.620]   the original sort of paper, and they all did that.
[00:21:08.620 --> 00:21:11.060]   And Turing famously wrote a chess program
[00:21:11.060 --> 00:21:12.460]   that all the computers around then
[00:21:12.460 --> 00:21:13.740]   were obviously too slow to run it.
[00:21:13.740 --> 00:21:16.020]   So he had to run, he had to be the computer, right?
[00:21:16.020 --> 00:21:18.900]   So he literally, I think, spent two or three days
[00:21:18.900 --> 00:21:21.340]   running his own program by hand with pencil and paper
[00:21:21.340 --> 00:21:24.940]   and playing a friend of his with his chess program.
[00:21:24.940 --> 00:21:29.940]   So of course, Deep Blue was a huge moment beating Kasparov.
[00:21:29.940 --> 00:21:31.820]   But actually, when that happened,
[00:21:31.820 --> 00:21:34.060]   I remember that very, very vividly, of course,
[00:21:34.060 --> 00:21:36.580]   because it was chess and computers and AI,
[00:21:36.580 --> 00:21:39.180]   all the things I loved, and I was at college at the time.
[00:21:39.180 --> 00:21:40.740]   But I remember coming away from that
[00:21:40.740 --> 00:21:43.020]   being more impressed by Kasparov's mind
[00:21:43.020 --> 00:21:46.420]   than I was by Deep Blue, because here was Kasparov
[00:21:46.420 --> 00:21:48.860]   with his human mind, not only could he play chess
[00:21:48.860 --> 00:21:50.140]   more or less to the same level
[00:21:50.140 --> 00:21:53.140]   as this brute of a calculation machine,
[00:21:53.140 --> 00:21:54.580]   but of course, Kasparov can do
[00:21:54.580 --> 00:21:55.780]   everything else humans can do,
[00:21:55.780 --> 00:21:58.140]   ride a bike, talk many languages, do politics,
[00:21:58.140 --> 00:22:00.860]   all the rest of the amazing things that Kasparov does.
[00:22:00.860 --> 00:22:04.500]   And so with the same brain, and yet Deep Blue,
[00:22:04.500 --> 00:22:07.020]   brilliant as it was at chess,
[00:22:07.020 --> 00:22:09.380]   it'd been hand-coded for chess,
[00:22:09.380 --> 00:22:13.140]   and actually had distilled the knowledge
[00:22:13.140 --> 00:22:16.420]   of chess grandmasters into a cool program,
[00:22:16.420 --> 00:22:17.820]   but it couldn't do anything else.
[00:22:17.820 --> 00:22:20.100]   Like, it couldn't even play a strictly simpler game
[00:22:20.100 --> 00:22:21.300]   like tic-tac-toe.
[00:22:21.300 --> 00:22:25.900]   So something to me was missing from intelligence
[00:22:25.900 --> 00:22:28.500]   from that system that we would regard as intelligence.
[00:22:28.500 --> 00:22:30.900]   And I think it was this idea of generality
[00:22:30.900 --> 00:22:32.180]   and also learning.
[00:22:32.180 --> 00:22:36.140]   So, and that's obviously what we tried to do with AlphaGo.
[00:22:36.140 --> 00:22:38.580]   - Yeah, with AlphaGo and AlphaZero and MuZero
[00:22:38.580 --> 00:22:40.460]   and then Gato and all the things
[00:22:40.460 --> 00:22:43.220]   that we'll get into some parts of,
[00:22:43.220 --> 00:22:45.660]   there's just a fascinating trajectory here.
[00:22:45.660 --> 00:22:48.540]   But let's just stick on chess briefly.
[00:22:48.540 --> 00:22:50.180]   On the human side of chess,
[00:22:50.180 --> 00:22:53.420]   you've proposed that from a game design perspective,
[00:22:53.420 --> 00:22:56.460]   the thing that makes chess compelling as a game
[00:22:56.460 --> 00:22:59.540]   is that there's a creative tension
[00:22:59.540 --> 00:23:02.940]   between a bishop and the knight.
[00:23:02.940 --> 00:23:04.020]   Can you explain this?
[00:23:04.020 --> 00:23:05.580]   First of all, it's really interesting
[00:23:05.580 --> 00:23:08.620]   to think about what makes a game compelling,
[00:23:08.620 --> 00:23:10.980]   makes it stick across centuries.
[00:23:10.980 --> 00:23:13.460]   - Yeah, I was sort of thinking about this,
[00:23:13.460 --> 00:23:15.460]   and actually a lot of even amazing chess players
[00:23:15.460 --> 00:23:16.860]   don't think about it necessarily
[00:23:16.860 --> 00:23:18.300]   from a games designer point of view.
[00:23:18.300 --> 00:23:20.260]   So it's with my game design hat on
[00:23:20.260 --> 00:23:21.260]   that I was thinking about this.
[00:23:21.260 --> 00:23:23.100]   Why is chess so compelling?
[00:23:23.100 --> 00:23:27.580]   And I think a critical reason is the dynamicness
[00:23:27.580 --> 00:23:29.980]   of the different kind of chess positions you can have,
[00:23:29.980 --> 00:23:32.220]   whether they're closed or open and other things
[00:23:32.220 --> 00:23:33.500]   comes from the bishop and the knight.
[00:23:33.500 --> 00:23:36.500]   So if you think about how different
[00:23:36.500 --> 00:23:39.260]   the capabilities of the bishop and knight are
[00:23:39.260 --> 00:23:40.860]   in terms of the way they move,
[00:23:40.860 --> 00:23:43.100]   and then somehow chess has evolved
[00:23:43.100 --> 00:23:44.860]   to balance those two capabilities
[00:23:44.860 --> 00:23:46.100]   more or less equally.
[00:23:46.100 --> 00:23:48.740]   So they're both roughly worth three points each.
[00:23:48.740 --> 00:23:50.580]   - So you think that dynamics is always there,
[00:23:50.580 --> 00:23:51.660]   and then the rest of the rules
[00:23:51.660 --> 00:23:53.780]   are kind of trying to stabilize the game?
[00:23:53.780 --> 00:23:54.620]   - Well, maybe.
[00:23:54.620 --> 00:23:55.460]   I mean, it's sort of, I don't know,
[00:23:55.460 --> 00:23:56.540]   it's chicken and egg situation.
[00:23:56.540 --> 00:23:57.700]   Probably both came together.
[00:23:57.700 --> 00:24:00.540]   But the fact that it's got to this beautiful equilibrium
[00:24:00.540 --> 00:24:02.380]   where you can have the bishop and knight,
[00:24:02.380 --> 00:24:04.420]   they're so different in power,
[00:24:04.420 --> 00:24:06.900]   but so equal in value across the set
[00:24:06.900 --> 00:24:09.500]   of the universe of all positions, right?
[00:24:09.500 --> 00:24:11.580]   Somehow they've been balanced by humanity
[00:24:11.580 --> 00:24:13.500]   over hundreds of years.
[00:24:13.500 --> 00:24:16.900]   I think gives the game the creative tension
[00:24:16.900 --> 00:24:19.020]   that you can swap the bishop and knight,
[00:24:19.020 --> 00:24:20.140]   for a bishop for a knight,
[00:24:20.140 --> 00:24:22.100]   and they're more or less worth the same,
[00:24:22.100 --> 00:24:24.020]   but now you aim for a different type of position.
[00:24:24.020 --> 00:24:26.060]   If you have the knight, you want a closed position.
[00:24:26.060 --> 00:24:28.180]   If you have the bishop, you want an open position.
[00:24:28.180 --> 00:24:29.220]   So I think that creates a lot
[00:24:29.220 --> 00:24:30.940]   of the creative tension in chess.
[00:24:30.940 --> 00:24:34.060]   - So some kind of controlled creative tension.
[00:24:34.060 --> 00:24:35.980]   From an AI perspective,
[00:24:35.980 --> 00:24:38.820]   do you think AI systems could eventually design games
[00:24:38.820 --> 00:24:41.620]   that are optimally compelling to humans?
[00:24:41.620 --> 00:24:42.940]   - Well, that's an interesting question.
[00:24:42.940 --> 00:24:45.980]   Sometimes I get asked about AI and creativity,
[00:24:45.980 --> 00:24:48.860]   and the way I answer that is relevant to that question,
[00:24:48.860 --> 00:24:51.260]   which is that I think there are different levels
[00:24:51.260 --> 00:24:52.900]   of creativity, one could say.
[00:24:52.900 --> 00:24:55.300]   So I think if we define creativity
[00:24:55.300 --> 00:24:57.260]   as coming up with something original, right,
[00:24:57.260 --> 00:24:59.300]   that's useful for a purpose,
[00:24:59.300 --> 00:25:02.220]   then I think the kind of lowest level of creativity
[00:25:02.220 --> 00:25:03.700]   is like an interpolation,
[00:25:03.700 --> 00:25:06.260]   so an averaging of all the examples you see.
[00:25:06.260 --> 00:25:08.300]   So maybe a very basic AI system could say
[00:25:08.300 --> 00:25:09.140]   you could have that.
[00:25:09.140 --> 00:25:11.380]   So you show it millions of pictures of cats,
[00:25:11.380 --> 00:25:13.820]   and then you say, "Give me an average-looking cat," right?
[00:25:13.820 --> 00:25:15.460]   "Generate me an average-looking cat."
[00:25:15.460 --> 00:25:17.180]   I would call that interpolation.
[00:25:17.180 --> 00:25:18.700]   Then there's extrapolation,
[00:25:18.700 --> 00:25:20.420]   which something like AlphaGo showed.
[00:25:20.420 --> 00:25:24.300]   So AlphaGo played millions of games of Go against itself,
[00:25:24.300 --> 00:25:26.580]   and then it came up with brilliant new ideas
[00:25:26.580 --> 00:25:28.180]   like move 37 in game two,
[00:25:28.180 --> 00:25:30.740]   brilliant motif strategies in Go
[00:25:30.740 --> 00:25:32.820]   that no humans had ever thought of,
[00:25:32.820 --> 00:25:34.780]   even though we've played it for thousands of years
[00:25:34.780 --> 00:25:36.580]   and professionally for hundreds of years.
[00:25:36.580 --> 00:25:38.820]   So that, I call that extrapolation.
[00:25:38.820 --> 00:25:41.060]   But then there's still a level above that,
[00:25:41.060 --> 00:25:44.020]   which is, you know, you could call out-of-the-box thinking
[00:25:44.020 --> 00:25:47.540]   or true innovation, which is, could you invent Go, right?
[00:25:47.540 --> 00:25:48.500]   Could you invent chess?
[00:25:48.500 --> 00:25:50.260]   And not just come up with a brilliant chess move
[00:25:50.260 --> 00:25:51.340]   or a brilliant Go move,
[00:25:51.340 --> 00:25:53.700]   but can you actually invent chess
[00:25:53.700 --> 00:25:55.900]   or something as good as chess or Go?
[00:25:55.900 --> 00:25:58.900]   And I think one day, AI could,
[00:25:58.900 --> 00:26:02.260]   but what's missing is how would you even specify that task
[00:26:02.260 --> 00:26:04.500]   to a program right now?
[00:26:04.500 --> 00:26:05.420]   And the way I would do it,
[00:26:05.420 --> 00:26:08.820]   if I was telling a human to do it or a games designer,
[00:26:08.820 --> 00:26:10.260]   a human games designer to do it,
[00:26:10.260 --> 00:26:11.460]   is I would say something like Go,
[00:26:11.460 --> 00:26:14.180]   I would say, come up with a game
[00:26:14.180 --> 00:26:16.140]   that only takes five minutes to learn,
[00:26:16.140 --> 00:26:17.940]   which Go does 'cause it's got simple rules,
[00:26:17.940 --> 00:26:20.300]   but many lifetimes to master, right?
[00:26:20.300 --> 00:26:22.140]   Or impossible to master in one lifetime
[00:26:22.140 --> 00:26:23.980]   'cause it's so deep and so complex.
[00:26:23.980 --> 00:26:27.500]   And then it's aesthetically beautiful.
[00:26:27.500 --> 00:26:30.820]   And also it can be completed in three or four hours
[00:26:30.820 --> 00:26:33.660]   of gameplay time, which is, you know, useful for our,
[00:26:33.660 --> 00:26:35.980]   us, you know, in a human day.
[00:26:35.980 --> 00:26:39.300]   And so you might specify these sort of high-level concepts
[00:26:39.300 --> 00:26:41.380]   like that, and then, you know, with that,
[00:26:41.380 --> 00:26:43.460]   and then maybe a few other things,
[00:26:43.460 --> 00:26:48.260]   one could imagine that Go satisfies those constraints.
[00:26:48.260 --> 00:26:50.300]   But the problem is, is that we're not able
[00:26:50.300 --> 00:26:53.740]   to specify abstract notions like that,
[00:26:53.740 --> 00:26:57.500]   high-level abstract notions like that yet to our AI systems.
[00:26:57.500 --> 00:26:59.500]   And I think there's still something missing there
[00:26:59.500 --> 00:27:02.460]   in terms of high-level concepts or abstractions
[00:27:02.460 --> 00:27:03.700]   that they truly understand
[00:27:03.700 --> 00:27:07.180]   and that are, you know, combinable and compositional.
[00:27:07.180 --> 00:27:10.380]   And so for the moment, I think AI is capable
[00:27:10.380 --> 00:27:12.380]   of doing interpolation and extrapolation,
[00:27:12.380 --> 00:27:14.140]   but not true invention.
[00:27:14.140 --> 00:27:18.500]   - So coming up with rule sets and optimizing
[00:27:18.500 --> 00:27:21.220]   with complicated objectives around those rule sets,
[00:27:21.220 --> 00:27:26.060]   we can't currently do, but you could take a specific rule set
[00:27:26.060 --> 00:27:28.900]   and then run a kind of self-play experiment
[00:27:28.900 --> 00:27:32.580]   to see how long, just observe how an AI system
[00:27:32.580 --> 00:27:36.540]   from scratch learns, how long is that journey of learning?
[00:27:36.540 --> 00:27:39.700]   And maybe if it satisfies some of those other things
[00:27:39.700 --> 00:27:42.300]   you mentioned in terms of quickness to learn and so on,
[00:27:42.300 --> 00:27:44.700]   and you could see a long journey to master
[00:27:44.700 --> 00:27:46.820]   for even an AI system,
[00:27:46.820 --> 00:27:49.860]   then you could say that this is a promising game.
[00:27:49.860 --> 00:27:52.300]   But it would be nice to do almost like alpha codes
[00:27:52.300 --> 00:27:53.940]   or programming rules.
[00:27:53.940 --> 00:27:55.740]   So generating rules that kind of,
[00:27:55.740 --> 00:28:00.420]   that automate even that part of the generation of rules.
[00:28:00.420 --> 00:28:02.980]   - So I have thought about systems actually,
[00:28:02.980 --> 00:28:05.660]   I think it'd be amazing for a games designer
[00:28:05.660 --> 00:28:09.180]   if you could have a system that takes your game,
[00:28:09.180 --> 00:28:11.940]   plays it tens of millions of times, maybe overnight,
[00:28:11.940 --> 00:28:13.820]   and then self-balances the rules better.
[00:28:13.820 --> 00:28:18.060]   So it tweaks the rules and maybe the equations
[00:28:18.060 --> 00:28:22.700]   and the parameters so that the game is more balanced,
[00:28:22.700 --> 00:28:26.260]   the units in the game or some of the rules could be tweaked.
[00:28:26.260 --> 00:28:28.300]   So it's a bit of like giving a base set
[00:28:28.300 --> 00:28:30.780]   and then allowing a Monte Carlo tree search
[00:28:30.780 --> 00:28:33.380]   or something like that to sort of explore it.
[00:28:33.380 --> 00:28:37.100]   And I think that would be super powerful tool actually
[00:28:37.100 --> 00:28:39.740]   for balancing, auto balancing a game,
[00:28:39.740 --> 00:28:42.140]   which usually takes thousands of hours
[00:28:42.140 --> 00:28:44.540]   from hundreds of games, human games testers normally
[00:28:44.540 --> 00:28:47.500]   to balance some game like StarCraft,
[00:28:47.500 --> 00:28:49.700]   which is, you know, Blizzard are amazing
[00:28:49.700 --> 00:28:50.660]   at balancing their games,
[00:28:50.660 --> 00:28:52.580]   but it takes them years and years and years.
[00:28:52.580 --> 00:28:54.140]   So one could imagine at some point
[00:28:54.140 --> 00:28:57.260]   when this stuff becomes efficient enough to,
[00:28:57.260 --> 00:28:59.540]   you know, you might better do that like overnight.
[00:28:59.540 --> 00:29:02.740]   - Do you think a game that is optimal
[00:29:02.740 --> 00:29:04.980]   designed by an AI system
[00:29:04.980 --> 00:29:08.260]   would look very much like a Planet Earth?
[00:29:08.260 --> 00:29:10.740]   - Maybe, maybe.
[00:29:10.740 --> 00:29:13.060]   It's only the sort of game I would love to make is,
[00:29:13.060 --> 00:29:15.980]   and I've tried, you know, in my games career,
[00:29:15.980 --> 00:29:17.460]   the games design career, you know,
[00:29:17.460 --> 00:29:20.260]   my first big game was designing a theme park,
[00:29:20.260 --> 00:29:21.380]   an amusement park.
[00:29:21.380 --> 00:29:23.700]   Then with games like Republic,
[00:29:23.700 --> 00:29:25.660]   I tried to, you know, have games where we designed
[00:29:25.660 --> 00:29:28.460]   whole cities and allowed you to play in.
[00:29:28.460 --> 00:29:30.260]   So, and of course people like Will Wright
[00:29:30.260 --> 00:29:32.620]   have written games like Sim Earth,
[00:29:32.620 --> 00:29:34.300]   trying to simulate the whole of Earth.
[00:29:34.300 --> 00:29:36.140]   Pretty tricky, but I think--
[00:29:36.140 --> 00:29:37.580]   - Sim Earth, I haven't actually played that one.
[00:29:37.580 --> 00:29:40.300]   So what is it, does it incorporate of evolution or?
[00:29:40.300 --> 00:29:42.660]   - Yeah, it has evolution and it sort of,
[00:29:42.660 --> 00:29:45.340]   it tries to, it sort of treats it as an entire biosphere,
[00:29:45.340 --> 00:29:47.260]   but from quite high level.
[00:29:47.260 --> 00:29:48.100]   So--
[00:29:48.100 --> 00:29:50.300]   - It'd be nice to be able to sort of zoom in,
[00:29:50.300 --> 00:29:51.740]   zoom out or zoom in. - Exactly, exactly.
[00:29:51.740 --> 00:29:53.460]   So obviously it couldn't do, that was in the,
[00:29:53.460 --> 00:29:54.940]   I think he wrote that in the 90s.
[00:29:54.940 --> 00:29:57.580]   So it couldn't, you know, it wasn't able to do that.
[00:29:57.580 --> 00:29:59.220]   But that would be obviously
[00:29:59.220 --> 00:30:01.460]   the ultimate sandbox game, of course.
[00:30:01.460 --> 00:30:04.780]   - On that topic, do you think we're living in a simulation?
[00:30:04.780 --> 00:30:06.620]   - Yes, well, so, okay, so I--
[00:30:06.620 --> 00:30:09.300]   - We're gonna jump around from the absurdly philosophical
[00:30:09.300 --> 00:30:10.740]   to the technical. - Sure, sure.
[00:30:10.740 --> 00:30:11.900]   Very, very happy to.
[00:30:11.900 --> 00:30:13.780]   So I think my answer to that question
[00:30:13.780 --> 00:30:17.620]   is a little bit complex because there is simulation theory,
[00:30:17.620 --> 00:30:19.100]   which obviously Nick Bostrom, I think,
[00:30:19.100 --> 00:30:20.600]   famously first proposed.
[00:30:20.600 --> 00:30:24.700]   And I don't quite believe it in that sense.
[00:30:24.700 --> 00:30:29.580]   So in the sense that are we in some sort of computer game
[00:30:29.580 --> 00:30:32.660]   or have our descendants somehow recreated
[00:30:32.660 --> 00:30:35.540]   Earth in the 21st century
[00:30:35.540 --> 00:30:38.420]   and for some kind of experimental reason?
[00:30:38.420 --> 00:30:43.340]   I think that, but I do think that we might be,
[00:30:43.340 --> 00:30:46.940]   that the best way to understand physics and the universe
[00:30:46.940 --> 00:30:49.260]   is from a computational perspective.
[00:30:49.260 --> 00:30:52.380]   So understanding it as an information universe
[00:30:52.380 --> 00:30:55.700]   and actually information being the most fundamental
[00:30:55.700 --> 00:30:59.900]   unit of reality rather than matter or energy.
[00:30:59.900 --> 00:31:02.660]   So a physicist would say, matter or energy,
[00:31:02.660 --> 00:31:04.420]   E equals MC squared, these are the things
[00:31:04.420 --> 00:31:07.340]   that are the fundamentals of the universe.
[00:31:07.340 --> 00:31:10.980]   I'd actually say information, which of course itself
[00:31:10.980 --> 00:31:13.540]   can specify energy or matter, right?
[00:31:13.540 --> 00:31:16.860]   Matter is actually just, we're just out the way our bodies
[00:31:16.860 --> 00:31:18.700]   and all the molecules in our body are arranged
[00:31:18.700 --> 00:31:19.700]   as information.
[00:31:19.700 --> 00:31:23.060]   So I think information may be the most fundamental way
[00:31:23.060 --> 00:31:26.580]   to describe the universe and therefore you could say
[00:31:26.580 --> 00:31:29.820]   we're in some sort of simulation because of that.
[00:31:29.820 --> 00:31:32.780]   But I'm not really a subscriber to the idea
[00:31:32.780 --> 00:31:35.900]   that these are sort of throwaway billions
[00:31:35.900 --> 00:31:36.860]   of simulations around.
[00:31:36.860 --> 00:31:39.380]   I think this is actually very critical
[00:31:39.380 --> 00:31:41.700]   and possibly unique, this simulation.
[00:31:41.700 --> 00:31:42.580]   - This particular one?
[00:31:42.580 --> 00:31:43.420]   - Yes.
[00:31:43.420 --> 00:31:48.420]   - And you just mean treating the universe as a computer
[00:31:48.420 --> 00:31:52.180]   that's processing and modifying information
[00:31:52.180 --> 00:31:54.820]   is a good way to solve the problems of physics,
[00:31:54.820 --> 00:31:59.660]   of chemistry, of biology and perhaps of humanity and so on.
[00:31:59.660 --> 00:32:02.180]   - Yes, I think understanding physics
[00:32:02.180 --> 00:32:05.860]   in terms of information theory might be the best way
[00:32:05.860 --> 00:32:09.340]   to really understand what's going on here.
[00:32:09.340 --> 00:32:13.500]   - From our understanding of a universal Turing machine,
[00:32:13.500 --> 00:32:15.220]   from our understanding of a computer,
[00:32:15.220 --> 00:32:17.380]   do you think there's something outside
[00:32:17.380 --> 00:32:19.400]   of the capabilities of a computer
[00:32:19.400 --> 00:32:20.940]   that is present in our universe?
[00:32:20.940 --> 00:32:23.500]   You have a disagreement with Roger Penrose
[00:32:23.500 --> 00:32:25.900]   about the nature of consciousness.
[00:32:25.900 --> 00:32:27.740]   He thinks that consciousness is more
[00:32:27.740 --> 00:32:28.980]   than just a computation.
[00:32:28.980 --> 00:32:32.660]   Do you think all of it, the whole shebang,
[00:32:32.660 --> 00:32:33.980]   can be a computation?
[00:32:33.980 --> 00:32:35.740]   - Yeah, I've had many fascinating debates
[00:32:35.740 --> 00:32:39.560]   with Sir Roger Penrose and obviously he's famously,
[00:32:39.560 --> 00:32:41.380]   and I read "Emperors of the New Mind"
[00:32:41.380 --> 00:32:45.220]   and his books, his classical books,
[00:32:45.220 --> 00:32:47.660]   and they were pretty influential in the '90s.
[00:32:47.660 --> 00:32:50.820]   And he believes that there's something more,
[00:32:50.820 --> 00:32:52.900]   something quantum that is needed
[00:32:52.900 --> 00:32:55.740]   to explain consciousness in the brain.
[00:32:55.740 --> 00:32:58.220]   I think about what we're doing actually at DeepMind
[00:32:58.220 --> 00:32:59.820]   and what my career is being,
[00:32:59.820 --> 00:33:01.820]   we're almost like Turing's champion.
[00:33:01.820 --> 00:33:03.580]   So we are pushing Turing machines
[00:33:03.580 --> 00:33:05.920]   or classical computation to the limits.
[00:33:05.920 --> 00:33:09.340]   What are the limits of what classical computing can do?
[00:33:09.340 --> 00:33:13.560]   Now, and at the same time, I've also studied neuroscience
[00:33:13.560 --> 00:33:15.460]   to see, and that's why I did my PhD in,
[00:33:15.460 --> 00:33:17.660]   was to see, also to look at,
[00:33:17.660 --> 00:33:19.160]   is there anything quantum in the brain
[00:33:19.160 --> 00:33:21.260]   from a neuroscience or biological perspective?
[00:33:21.260 --> 00:33:24.460]   And so far, I think most neuroscientists
[00:33:24.460 --> 00:33:26.380]   and most mainstream biologists and neuroscientists
[00:33:26.380 --> 00:33:29.420]   would say there's no evidence of any quantum systems
[00:33:29.420 --> 00:33:30.740]   or effects in the brain.
[00:33:30.740 --> 00:33:32.980]   As far as we can see, it can be mostly explained
[00:33:32.980 --> 00:33:35.860]   by classical theories.
[00:33:35.860 --> 00:33:39.300]   So, and then, so there's sort of the search
[00:33:39.300 --> 00:33:40.620]   from the biology side.
[00:33:40.620 --> 00:33:44.300]   And then at the same time, there's the raising of the water,
[00:33:44.300 --> 00:33:47.260]   the bar, from what classical Turing machines can do.
[00:33:48.220 --> 00:33:51.680]   And including our new AI systems.
[00:33:51.680 --> 00:33:54.000]   And as you alluded to earlier,
[00:33:54.000 --> 00:33:57.760]   I think AI, especially in the last decade plus,
[00:33:57.760 --> 00:33:59.880]   has been a continual story now,
[00:33:59.880 --> 00:34:03.920]   surprising events and surprising successes,
[00:34:03.920 --> 00:34:05.800]   knocking over one theory after another
[00:34:05.800 --> 00:34:07.760]   of what was thought to be impossible,
[00:34:07.760 --> 00:34:10.080]   from Go to protein folding and so on.
[00:34:10.080 --> 00:34:14.760]   And so I think I would be very hesitant
[00:34:14.760 --> 00:34:19.520]   to bet against how far the universal Turing machine
[00:34:19.520 --> 00:34:23.400]   and classical computation paradigm can go.
[00:34:23.400 --> 00:34:27.280]   And my betting would be that all of, certainly,
[00:34:27.280 --> 00:34:30.680]   what's going on in our brain can probably be mimicked
[00:34:30.680 --> 00:34:34.720]   or approximated on a classical machine,
[00:34:34.720 --> 00:34:38.400]   not requiring something metaphysical or quantum.
[00:34:38.400 --> 00:34:41.720]   - And we'll get there with some of the work with AlphaFold,
[00:34:41.720 --> 00:34:45.080]   which I think begins the journey of modeling
[00:34:45.080 --> 00:34:48.160]   this beautiful and complex world of biology.
[00:34:48.160 --> 00:34:50.160]   So you think all the magic of the human mind
[00:34:50.160 --> 00:34:53.520]   comes from this, just a few pounds of mush,
[00:34:53.520 --> 00:34:57.480]   of biological computational mush
[00:34:57.480 --> 00:35:00.560]   that's akin to some of the neural networks,
[00:35:00.560 --> 00:35:03.800]   not directly, but in spirit,
[00:35:03.800 --> 00:35:06.200]   that DeepMind has been working with?
[00:35:06.200 --> 00:35:08.360]   - Well, look, I think it's, you say it's a few,
[00:35:08.360 --> 00:35:10.040]   you know, of course, this is, I think,
[00:35:10.040 --> 00:35:12.760]   the biggest miracle of the universe is that
[00:35:12.760 --> 00:35:15.040]   it is just a few pounds of mush in our skulls,
[00:35:15.040 --> 00:35:18.560]   and yet it's also, our brains are the most complex objects
[00:35:18.560 --> 00:35:20.220]   that we know of in the universe.
[00:35:20.220 --> 00:35:22.360]   So there's something profoundly beautiful
[00:35:22.360 --> 00:35:23.920]   and amazing about our brains,
[00:35:23.920 --> 00:35:28.640]   and I think that it's an incredibly,
[00:35:28.640 --> 00:35:30.640]   incredible efficient machine,
[00:35:30.640 --> 00:35:35.560]   and it's a phenomenon, basically.
[00:35:35.560 --> 00:35:37.480]   And I think that building AI,
[00:35:37.480 --> 00:35:38.920]   one of the reasons I wanna build AI,
[00:35:38.920 --> 00:35:40.440]   and I've always wanted to, is,
[00:35:40.440 --> 00:35:43.800]   I think by building an intelligent artifact like AI,
[00:35:43.800 --> 00:35:46.480]   and then comparing it to the human mind,
[00:35:46.480 --> 00:35:49.560]   that will help us unlock the uniqueness
[00:35:49.560 --> 00:35:50.960]   and the true secrets of the mind
[00:35:50.960 --> 00:35:53.480]   that we've always wondered about since the dawn of history,
[00:35:53.480 --> 00:35:57.800]   like consciousness, dreaming, creativity, emotions.
[00:35:57.800 --> 00:36:00.760]   What are all these things, right?
[00:36:00.760 --> 00:36:04.200]   We've wondered about them since the dawn of humanity,
[00:36:04.200 --> 00:36:05.940]   and I think one of the reasons,
[00:36:05.940 --> 00:36:08.280]   and, you know, I love philosophy and philosophy of mind,
[00:36:08.280 --> 00:36:09.920]   is, we found it difficult,
[00:36:09.920 --> 00:36:11.960]   is there haven't been the tools for us to really,
[00:36:11.960 --> 00:36:13.400]   other than introspection,
[00:36:13.400 --> 00:36:15.880]   to, from very clever people in history,
[00:36:15.880 --> 00:36:17.200]   very clever philosophers,
[00:36:17.200 --> 00:36:19.360]   to really investigate this scientifically.
[00:36:19.360 --> 00:36:21.720]   But now, suddenly we have a plethora of tools.
[00:36:21.720 --> 00:36:23.240]   Firstly, we have all of the neuroscience tools,
[00:36:23.240 --> 00:36:25.900]   fMRI machines, single-cell recording, all of this stuff,
[00:36:25.900 --> 00:36:29.000]   but we also have the ability, computers and AI,
[00:36:29.000 --> 00:36:31.640]   to build intelligent systems.
[00:36:31.640 --> 00:36:34.720]   So I think that, you know,
[00:36:34.720 --> 00:36:37.320]   I think it is amazing what the human mind does,
[00:36:37.320 --> 00:36:41.120]   and I'm kind of in awe of it, really,
[00:36:41.120 --> 00:36:44.440]   and I think it's amazing that, without human minds,
[00:36:44.440 --> 00:36:46.780]   we're able to build things like computers,
[00:36:46.780 --> 00:36:48.240]   and actually even, you know,
[00:36:48.240 --> 00:36:49.880]   think and investigate about these questions.
[00:36:49.880 --> 00:36:52.720]   I think that's also a testament to the human mind.
[00:36:52.720 --> 00:36:56.200]   - Yeah, the universe built the human mind
[00:36:56.200 --> 00:36:57.620]   that now is building computers
[00:36:57.620 --> 00:37:00.380]   that help us understand both the universe
[00:37:00.380 --> 00:37:01.480]   and our own human mind.
[00:37:01.480 --> 00:37:02.660]   - That's right, that's exactly it.
[00:37:02.660 --> 00:37:03.920]   I mean, I think that's one, you know,
[00:37:03.920 --> 00:37:05.760]   one could say we are,
[00:37:05.760 --> 00:37:08.160]   maybe we're the mechanism by which the universe
[00:37:08.160 --> 00:37:09.840]   is going to try and understand itself.
[00:37:09.840 --> 00:37:10.680]   - Yeah.
[00:37:10.680 --> 00:37:12.520]   (laughing)
[00:37:12.520 --> 00:37:13.360]   It's beautiful.
[00:37:13.360 --> 00:37:16.960]   So let's go to the basic building blocks of biology
[00:37:16.960 --> 00:37:19.400]   that I think is another angle
[00:37:19.400 --> 00:37:21.440]   at which you can start to understand the human mind,
[00:37:21.440 --> 00:37:23.400]   the human body, which is quite fascinating,
[00:37:23.400 --> 00:37:26.640]   which is, from the basic building blocks,
[00:37:26.640 --> 00:37:28.960]   start to simulate, start to model
[00:37:28.960 --> 00:37:30.480]   how, from those building blocks,
[00:37:30.480 --> 00:37:33.080]   you can construct bigger and bigger, more complex systems,
[00:37:33.080 --> 00:37:35.820]   maybe one day the entirety of the human biology.
[00:37:35.820 --> 00:37:39.680]   So here's another problem that thought
[00:37:39.680 --> 00:37:42.720]   to be impossible to solve, which is protein folding.
[00:37:42.720 --> 00:37:47.720]   And AlphaFold, or specifically AlphaFold2,
[00:37:47.720 --> 00:37:48.840]   did just that.
[00:37:48.840 --> 00:37:50.320]   It solved protein folding.
[00:37:50.320 --> 00:37:53.400]   I think it's one of the biggest breakthroughs,
[00:37:53.400 --> 00:37:55.140]   certainly in the history of structural biology,
[00:37:55.140 --> 00:37:58.200]   but in general, in science.
[00:37:58.200 --> 00:38:02.280]   Maybe from a high level,
[00:38:02.280 --> 00:38:04.840]   what is it and how does it work?
[00:38:04.840 --> 00:38:08.680]   And then we can ask some fascinating questions after.
[00:38:08.680 --> 00:38:09.960]   - Sure.
[00:38:09.960 --> 00:38:12.360]   So maybe to explain it to people
[00:38:12.360 --> 00:38:14.400]   not familiar with protein folding is,
[00:38:14.400 --> 00:38:15.780]   first of all, explain proteins,
[00:38:15.780 --> 00:38:18.840]   which is, proteins are essential to all life.
[00:38:18.840 --> 00:38:21.520]   Every function in your body depends on proteins.
[00:38:21.520 --> 00:38:23.920]   Sometimes they're called the workhorses of biology.
[00:38:23.920 --> 00:38:24.880]   And if you look into them,
[00:38:24.880 --> 00:38:26.640]   and obviously as part of AlphaFold,
[00:38:26.640 --> 00:38:30.200]   I've been researching proteins and structural biology
[00:38:30.200 --> 00:38:31.760]   for the last few years,
[00:38:31.760 --> 00:38:34.760]   they're amazing little bio-nano-machines proteins.
[00:38:34.760 --> 00:38:35.920]   They're incredible if you actually watch
[00:38:35.920 --> 00:38:37.200]   little videos of how they work,
[00:38:37.200 --> 00:38:39.000]   animations of how they work.
[00:38:39.000 --> 00:38:42.600]   And proteins are specified by their genetic sequence,
[00:38:42.600 --> 00:38:44.280]   called their amino acid sequence.
[00:38:44.280 --> 00:38:47.040]   So you can think of it as their genetic makeup.
[00:38:47.040 --> 00:38:50.320]   And then in the body, in nature,
[00:38:50.320 --> 00:38:53.340]   when they fold up into a 3D structure.
[00:38:53.340 --> 00:38:55.280]   So you can think of it as a string of beads,
[00:38:55.280 --> 00:38:57.120]   and then they fold up into a ball.
[00:38:57.120 --> 00:38:58.280]   Now the key thing is,
[00:38:58.280 --> 00:39:01.100]   you want to know what that 3D structure is,
[00:39:01.100 --> 00:39:04.480]   because the 3D structure of a protein
[00:39:04.480 --> 00:39:06.720]   is what helps to determine what does it do,
[00:39:06.720 --> 00:39:08.560]   the function it does in your body.
[00:39:08.560 --> 00:39:12.280]   And also, if you're interested in drugs or disease,
[00:39:12.280 --> 00:39:13.920]   you need to understand that 3D structure.
[00:39:13.920 --> 00:39:15.800]   Because if you want to target something
[00:39:15.800 --> 00:39:17.040]   with a drug compound,
[00:39:17.040 --> 00:39:20.240]   about to block something the protein's doing,
[00:39:20.240 --> 00:39:21.960]   you need to understand where it's gonna bind
[00:39:21.960 --> 00:39:23.400]   on the surface of the protein.
[00:39:23.400 --> 00:39:24.900]   So obviously, in order to do that,
[00:39:24.900 --> 00:39:26.680]   you need to understand the 3D structure.
[00:39:26.680 --> 00:39:28.600]   - So the structure's mapped to the function.
[00:39:28.600 --> 00:39:29.820]   - The structure's mapped to the function.
[00:39:29.820 --> 00:39:31.840]   And the structure is obviously somehow
[00:39:31.840 --> 00:39:34.840]   specified by the amino acid sequence.
[00:39:34.840 --> 00:39:37.420]   And that's, in essence, the protein folding problem is,
[00:39:37.420 --> 00:39:39.620]   can you just from the amino acid sequence,
[00:39:39.620 --> 00:39:42.560]   the one-dimensional string of letters,
[00:39:42.560 --> 00:39:45.600]   can you immediately computationally predict
[00:39:45.600 --> 00:39:47.160]   the 3D structure?
[00:39:47.160 --> 00:39:50.040]   And this has been a grand challenge in biology
[00:39:50.040 --> 00:39:51.540]   for over 50 years.
[00:39:51.540 --> 00:39:53.200]   So I think it was first articulated
[00:39:53.200 --> 00:39:56.840]   by Christian Anfiensen, a Nobel Prize winner in 1972,
[00:39:56.840 --> 00:39:59.280]   as part of his Nobel Prize winning lecture.
[00:39:59.280 --> 00:40:01.900]   And he just speculated this should be possible
[00:40:01.900 --> 00:40:05.000]   to go from the amino acid sequence to the 3D structure.
[00:40:05.000 --> 00:40:06.100]   But he didn't say how.
[00:40:06.100 --> 00:40:09.480]   So it's been described to me as equivalent
[00:40:09.480 --> 00:40:12.320]   to Fermat's last theorem, but for biology.
[00:40:12.320 --> 00:40:15.180]   - You should, as somebody that very well might win
[00:40:15.180 --> 00:40:16.600]   the Nobel Prize in the future,
[00:40:16.600 --> 00:40:18.360]   but outside of that,
[00:40:18.360 --> 00:40:20.020]   you should do more of that kind of thing.
[00:40:20.020 --> 00:40:22.200]   In the margin, just put random things.
[00:40:22.200 --> 00:40:24.480]   That'll take like 200 years to solve.
[00:40:24.480 --> 00:40:26.040]   - Set people off for 200 years.
[00:40:26.040 --> 00:40:27.760]   - It should be possible.
[00:40:27.760 --> 00:40:29.240]   - And just don't give any details.
[00:40:29.240 --> 00:40:30.640]   - Exactly, I think everyone should, exactly.
[00:40:30.640 --> 00:40:33.520]   It should be, I'll have to remember that for future.
[00:40:33.520 --> 00:40:36.280]   So yeah, so he set off with this one throwaway remark,
[00:40:36.280 --> 00:40:40.280]   just like Fermat, he set off this whole 50-year field,
[00:40:40.280 --> 00:40:44.400]   really, of computational biology.
[00:40:44.400 --> 00:40:46.240]   And they got stuck.
[00:40:46.240 --> 00:40:48.520]   They hadn't really got very far with doing this.
[00:40:48.520 --> 00:40:52.500]   And until now, until AlphaFold came along,
[00:40:52.500 --> 00:40:55.500]   this is done experimentally, very painstakingly.
[00:40:55.500 --> 00:40:56.580]   So the rule of thumb is,
[00:40:56.580 --> 00:40:58.720]   and you have to crystallize the protein,
[00:40:58.720 --> 00:40:59.840]   which is really difficult.
[00:40:59.840 --> 00:41:03.080]   Some proteins can't be crystallized like membrane proteins.
[00:41:03.080 --> 00:41:05.960]   And then you have to use very expensive electron microscopes
[00:41:05.960 --> 00:41:08.200]   or X-ray crystallography machines,
[00:41:08.200 --> 00:41:10.680]   really painstaking work to get the 3D structure
[00:41:10.680 --> 00:41:12.420]   and visualize the 3D structure.
[00:41:12.420 --> 00:41:14.880]   So the rule of thumb in experimental biology
[00:41:14.880 --> 00:41:16.860]   is that it takes one PhD student,
[00:41:16.860 --> 00:41:19.400]   their entire PhD, to do one protein.
[00:41:19.400 --> 00:41:23.440]   And with AlphaFold2, we're able to predict
[00:41:23.440 --> 00:41:26.400]   the 3D structure in a matter of seconds.
[00:41:26.400 --> 00:41:30.240]   And so over Christmas, we did the whole human proteome,
[00:41:30.240 --> 00:41:33.280]   or every protein in the human body, all 20,000 proteins.
[00:41:33.280 --> 00:41:34.760]   So the human proteome's like the equivalent
[00:41:34.760 --> 00:41:37.560]   of the human genome, but on protein space.
[00:41:37.560 --> 00:41:40.240]   And sort of revolutionized, really,
[00:41:40.240 --> 00:41:43.300]   what structural biologists can do.
[00:41:43.300 --> 00:41:45.720]   Because now, they don't have to worry
[00:41:45.720 --> 00:41:47.960]   about these painstaking experimentals.
[00:41:47.960 --> 00:41:49.560]   Should they put all of their effort in or not?
[00:41:49.560 --> 00:41:51.080]   They can almost just look up the structure
[00:41:51.080 --> 00:41:53.280]   of their proteins like a Google search.
[00:41:53.280 --> 00:41:56.880]   - And so there's a data set on which it's trained
[00:41:56.880 --> 00:41:58.800]   and how to map this amino acid sequence.
[00:41:58.800 --> 00:42:00.760]   First of all, it's incredible that a protein,
[00:42:00.760 --> 00:42:01.920]   this little chemical computer,
[00:42:01.920 --> 00:42:03.760]   is able to do that computation itself
[00:42:03.760 --> 00:42:07.800]   in some kind of distributed way and do it very quickly.
[00:42:07.800 --> 00:42:08.840]   That's a weird thing.
[00:42:08.840 --> 00:42:11.720]   And they evolved that way 'cause in the beginning,
[00:42:11.720 --> 00:42:13.160]   I mean, that's a great invention,
[00:42:13.160 --> 00:42:14.640]   just the protein itself.
[00:42:14.640 --> 00:42:16.320]   - Yes, I mean-- - And then there's,
[00:42:16.320 --> 00:42:18.900]   I think, probably a history of,
[00:42:18.900 --> 00:42:22.720]   like they evolved to have many of these proteins.
[00:42:22.720 --> 00:42:26.560]   And those proteins figure out how to be computers themselves
[00:42:26.560 --> 00:42:28.520]   in such a way that you can create structures
[00:42:28.520 --> 00:42:30.520]   that can interact in complex ways with each other
[00:42:30.520 --> 00:42:32.620]   in order to form high-level functions.
[00:42:32.620 --> 00:42:35.480]   I mean, it's a weird system that they figured it out.
[00:42:35.480 --> 00:42:36.320]   - Well, for sure.
[00:42:36.320 --> 00:42:38.960]   I mean, maybe we should talk about the origins of life too.
[00:42:38.960 --> 00:42:41.140]   But proteins themselves, I think, are magical
[00:42:41.140 --> 00:42:45.760]   and incredible, as I said, little bio-nano machines.
[00:42:45.760 --> 00:42:50.760]   And actually, Leventhal, who is another scientist,
[00:42:51.000 --> 00:42:55.080]   a contemporary of Amundsen, he coined this Leventhal,
[00:42:55.080 --> 00:42:56.800]   what became known as Leventhal's paradox,
[00:42:56.800 --> 00:42:58.280]   which is exactly what you're saying.
[00:42:58.280 --> 00:43:01.540]   He calculated roughly an average protein,
[00:43:01.540 --> 00:43:04.900]   which is maybe 2,000 amino acids bases long,
[00:43:04.900 --> 00:43:09.960]   can fold in maybe 10 to the power 300
[00:43:09.960 --> 00:43:11.480]   different conformations.
[00:43:11.480 --> 00:43:13.320]   So there's 10 to the power 300 different ways
[00:43:13.320 --> 00:43:14.800]   that protein could fold up.
[00:43:14.800 --> 00:43:19.120]   And yet somehow, in nature, physics solves this
[00:43:19.120 --> 00:43:20.540]   in a matter of milliseconds.
[00:43:20.540 --> 00:43:22.320]   So proteins fold up in your body
[00:43:22.320 --> 00:43:25.600]   in sometimes in fractions of a second.
[00:43:25.600 --> 00:43:29.080]   So physics is somehow solving that search problem.
[00:43:29.080 --> 00:43:31.200]   - And just to be clear, in many of these cases,
[00:43:31.200 --> 00:43:33.040]   maybe you can correct me if I'm wrong,
[00:43:33.040 --> 00:43:37.680]   there's often a unique way for that sequence to form itself.
[00:43:37.680 --> 00:43:41.240]   So among that huge number of possibilities,
[00:43:41.240 --> 00:43:43.560]   it figures out a way how to stably,
[00:43:43.560 --> 00:43:47.780]   in some cases, there might be a misfunction, so on,
[00:43:47.780 --> 00:43:50.040]   which leads to a lot of the disorders and stuff like that.
[00:43:50.040 --> 00:43:52.760]   But most of the time, it's a unique mapping.
[00:43:52.760 --> 00:43:54.840]   And that unique mapping's not obvious.
[00:43:54.840 --> 00:43:55.920]   - No, exactly.
[00:43:55.920 --> 00:43:57.120]   - It's just what the problem is.
[00:43:57.120 --> 00:43:57.960]   - No, exactly.
[00:43:57.960 --> 00:43:59.640]   So there's a unique mapping, usually,
[00:43:59.640 --> 00:44:01.880]   in a healthy, if it's healthy.
[00:44:01.880 --> 00:44:05.440]   And as you say, in disease, so for example, Alzheimer's,
[00:44:05.440 --> 00:44:09.040]   one conjecture is that it's because of misfolded protein,
[00:44:09.040 --> 00:44:12.040]   a protein that folds in the wrong way, amyloid beta protein.
[00:44:12.040 --> 00:44:14.600]   So, and then because it folds in the wrong way,
[00:44:14.600 --> 00:44:17.640]   it gets tangled up, right, in your neurons.
[00:44:17.640 --> 00:44:20.560]   So it's super important to understand
[00:44:20.560 --> 00:44:23.600]   both healthy functioning and also disease,
[00:44:23.600 --> 00:44:26.480]   is to understand what these things are doing
[00:44:26.480 --> 00:44:27.600]   and how they're structuring.
[00:44:27.600 --> 00:44:30.560]   Of course, the next step is sometimes proteins change shape
[00:44:30.560 --> 00:44:32.160]   when they interact with something.
[00:44:32.160 --> 00:44:35.940]   So they're not just static, necessarily, in biology.
[00:44:35.940 --> 00:44:39.800]   - Maybe you can give some interesting,
[00:44:39.800 --> 00:44:41.360]   sort of beautiful things to you
[00:44:41.360 --> 00:44:44.160]   about these early days of alpha fold,
[00:44:44.160 --> 00:44:46.160]   of solving this problem.
[00:44:46.160 --> 00:44:51.160]   Because unlike games, this is real physical systems
[00:44:51.160 --> 00:44:55.640]   that are less amenable to self-play type of mechanisms.
[00:44:55.640 --> 00:44:56.600]   - Sure.
[00:44:56.600 --> 00:44:58.440]   - The size of the data set is smaller
[00:44:58.440 --> 00:44:59.760]   than you might otherwise like,
[00:44:59.760 --> 00:45:01.780]   so you have to be very clever about certain things.
[00:45:01.780 --> 00:45:03.640]   Is there something you could speak to
[00:45:03.640 --> 00:45:06.680]   what was very hard to solve
[00:45:06.680 --> 00:45:09.920]   and what are some beautiful aspects about the solution?
[00:45:09.920 --> 00:45:12.800]   - Yeah, I would say alpha fold is the most complex
[00:45:12.800 --> 00:45:14.600]   and also probably most meaningful system
[00:45:14.600 --> 00:45:15.860]   we've built so far.
[00:45:15.860 --> 00:45:18.400]   So it's been an amazing time actually in the last,
[00:45:18.400 --> 00:45:20.520]   you know, two, three years to see that come through
[00:45:20.520 --> 00:45:23.000]   because as we talked about earlier,
[00:45:23.000 --> 00:45:25.480]   you know, games is what we started on,
[00:45:25.480 --> 00:45:27.900]   building things like AlphaGo and AlphaZero.
[00:45:27.900 --> 00:45:30.400]   But really the ultimate goal was to,
[00:45:30.400 --> 00:45:33.160]   not just to crack games, it was just to build,
[00:45:33.160 --> 00:45:35.320]   use them to bootstrap general learning systems
[00:45:35.320 --> 00:45:37.460]   we could then apply to real world challenges.
[00:45:37.460 --> 00:45:40.640]   Specifically, my passion is scientific challenges
[00:45:40.640 --> 00:45:41.920]   like protein folding.
[00:45:41.920 --> 00:45:43.280]   And then alpha fold, of course,
[00:45:43.280 --> 00:45:45.360]   is our first big proof point of that.
[00:45:45.360 --> 00:45:49.000]   And so, you know, in terms of the data
[00:45:49.000 --> 00:45:50.920]   and the amount of innovations that had to go into it,
[00:45:50.920 --> 00:45:53.060]   we, you know, it was like more than 30
[00:45:53.060 --> 00:45:55.540]   different component algorithms needed to be put together
[00:45:55.540 --> 00:45:57.960]   to crack the protein folding.
[00:45:57.960 --> 00:46:00.800]   I think some of the big innovations were the
[00:46:00.800 --> 00:46:04.220]   kind of building in some hard coded constraints
[00:46:04.220 --> 00:46:07.760]   around physics and evolutionary biology
[00:46:07.760 --> 00:46:10.440]   to constrain sort of things like the bond angles
[00:46:10.440 --> 00:46:14.240]   in the protein and things like that.
[00:46:15.160 --> 00:46:18.040]   But not to impact the learning system.
[00:46:18.040 --> 00:46:21.000]   So still allowing the system to be able to learn
[00:46:21.000 --> 00:46:25.520]   the physics itself from the examples that we had.
[00:46:25.520 --> 00:46:26.620]   And the examples, as you say,
[00:46:26.620 --> 00:46:28.840]   there are only about 150,000 proteins,
[00:46:28.840 --> 00:46:31.240]   even after 40 years of experimental biology,
[00:46:31.240 --> 00:46:33.880]   only around 150,000 proteins have been,
[00:46:33.880 --> 00:46:35.920]   the structures have been found out about.
[00:46:35.920 --> 00:46:37.120]   So that was our training set,
[00:46:37.120 --> 00:46:41.140]   which is much less than normally we would like to use.
[00:46:41.140 --> 00:46:43.840]   But using various tricks, things like self-distillation.
[00:46:43.840 --> 00:46:48.280]   So actually using alpha fold predictions,
[00:46:48.280 --> 00:46:49.480]   some of the best predictions
[00:46:49.480 --> 00:46:51.000]   that it thought was highly confident in,
[00:46:51.000 --> 00:46:53.320]   we put them back into the training set, right?
[00:46:53.320 --> 00:46:55.440]   To make the training set bigger.
[00:46:55.440 --> 00:46:58.400]   That was critical to alpha fold working.
[00:46:58.400 --> 00:47:00.840]   So there was actually a huge number of different
[00:47:00.840 --> 00:47:03.560]   innovations like that that were required
[00:47:03.560 --> 00:47:06.080]   to ultimately crack the problem.
[00:47:06.080 --> 00:47:09.720]   Alpha fold one, what it produced was a distogram.
[00:47:09.720 --> 00:47:13.620]   So a kind of a matrix of the pair wise distances
[00:47:13.620 --> 00:47:17.760]   between all of the molecules in the protein.
[00:47:17.760 --> 00:47:20.440]   And then there had to be a separate optimization process
[00:47:20.440 --> 00:47:23.640]   to create the 3D structure.
[00:47:23.640 --> 00:47:25.120]   And what we did for alpha fold two
[00:47:25.120 --> 00:47:26.900]   is make it truly end to end.
[00:47:26.900 --> 00:47:31.720]   So we went straight from the amino acid sequence of bases
[00:47:31.720 --> 00:47:33.880]   to the 3D structure directly,
[00:47:33.880 --> 00:47:36.080]   without going through this intermediate step.
[00:47:36.080 --> 00:47:39.040]   And in machine learning, what we've always found is that
[00:47:39.040 --> 00:47:42.160]   the more end to end you can make it, the better the system.
[00:47:42.160 --> 00:47:45.640]   And it's probably because we, you know,
[00:47:45.640 --> 00:47:47.480]   in the end, the system's better at learning
[00:47:47.480 --> 00:47:48.520]   what the constraints are
[00:47:48.520 --> 00:47:51.880]   than we are as the human designers of specifying it.
[00:47:51.880 --> 00:47:54.000]   So anytime you can let it flow end to end
[00:47:54.000 --> 00:47:55.360]   and actually just generate what it is
[00:47:55.360 --> 00:47:58.400]   you're really looking for, in this case, the 3D structure,
[00:47:58.400 --> 00:48:00.520]   you're better off than having this intermediate step,
[00:48:00.520 --> 00:48:03.320]   which you then have to handcraft the next step for.
[00:48:03.320 --> 00:48:06.120]   So it's better to let the gradients and the learning
[00:48:06.120 --> 00:48:08.960]   flow all the way through the system from the endpoint,
[00:48:08.960 --> 00:48:10.840]   the end output you want to the inputs.
[00:48:10.840 --> 00:48:11.960]   - So that's a good way to start.
[00:48:11.960 --> 00:48:14.320]   I mean, you problem handcraft a bunch of stuff,
[00:48:14.320 --> 00:48:16.640]   add a bunch of manual constraints
[00:48:16.640 --> 00:48:18.640]   with a small end to end learning piece
[00:48:18.640 --> 00:48:21.560]   or a small learning piece and grow that learning piece
[00:48:21.560 --> 00:48:22.840]   until it consumes the whole thing.
[00:48:22.840 --> 00:48:23.680]   - That's right.
[00:48:23.680 --> 00:48:25.320]   And so you can also see, you know,
[00:48:25.320 --> 00:48:26.960]   this is a bit of a method we've developed
[00:48:26.960 --> 00:48:29.600]   over doing many sort of successful outfits.
[00:48:29.600 --> 00:48:31.920]   We call them Alpha X projects, right?
[00:48:31.920 --> 00:48:33.800]   Is, and the easiest way to see that
[00:48:33.800 --> 00:48:36.720]   is the evolution of AlphaGo to AlphaZero.
[00:48:36.720 --> 00:48:39.640]   So AlphaGo was a learning system,
[00:48:39.640 --> 00:48:42.280]   but it was specifically trained to only play Go, right?
[00:48:42.280 --> 00:48:44.200]   So, and what we wanted to do
[00:48:44.200 --> 00:48:45.360]   in the first version of AlphaGo
[00:48:45.360 --> 00:48:47.520]   is just get to world champion performance
[00:48:47.520 --> 00:48:49.200]   no matter how we did it, right?
[00:48:49.200 --> 00:48:51.360]   And then of course, AlphaGo Zero,
[00:48:51.360 --> 00:48:54.080]   we removed the need to use human games
[00:48:54.080 --> 00:48:55.480]   as a starting point, right?
[00:48:55.480 --> 00:48:57.960]   So it could just play against itself
[00:48:57.960 --> 00:49:00.280]   from random starting point from the beginning.
[00:49:00.280 --> 00:49:03.720]   So that removed the need for human knowledge about Go.
[00:49:03.720 --> 00:49:05.960]   And then finally AlphaZero then generalized it
[00:49:05.960 --> 00:49:08.920]   so that any things we had in there, the system,
[00:49:08.920 --> 00:49:12.240]   including things like symmetry of the Go board were removed.
[00:49:12.240 --> 00:49:14.600]   So that AlphaZero could play from scratch
[00:49:14.600 --> 00:49:15.600]   any two-player game.
[00:49:15.600 --> 00:49:17.480]   And then MuZero, which is the final,
[00:49:17.480 --> 00:49:19.640]   our latest version of that set of things,
[00:49:19.640 --> 00:49:22.120]   was then extending it so that you didn't even have to give it
[00:49:22.120 --> 00:49:23.200]   the rules of the game.
[00:49:23.200 --> 00:49:24.880]   It would learn that for itself.
[00:49:24.880 --> 00:49:26.600]   So it could also deal with computer games
[00:49:26.600 --> 00:49:27.760]   as well as board games.
[00:49:27.760 --> 00:49:31.840]   - So that line of AlphaGo, AlphaGo Zero, AlphaZero, MuZero,
[00:49:31.840 --> 00:49:34.200]   that's the full trajectory of what you can take
[00:49:34.200 --> 00:49:39.200]   from imitation learning to full self-supervised learning.
[00:49:39.200 --> 00:49:41.640]   - Yeah, exactly.
[00:49:41.640 --> 00:49:45.520]   And learning the entire structure of the environment
[00:49:45.520 --> 00:49:47.640]   you put in from scratch, right?
[00:49:47.640 --> 00:49:51.840]   And bootstrapping it through self-play yourself.
[00:49:51.840 --> 00:49:53.720]   But the thing is it would have been impossible, I think,
[00:49:53.720 --> 00:49:57.400]   or very hard for us to build AlphaZero or MuZero first
[00:49:57.400 --> 00:49:58.600]   out of the box.
[00:49:58.600 --> 00:50:01.400]   - Even psychologically, because you have to believe
[00:50:01.400 --> 00:50:03.040]   in yourself for a very long time.
[00:50:03.040 --> 00:50:04.640]   You're constantly dealing with doubt
[00:50:04.640 --> 00:50:06.720]   'cause a lot of people say that it's impossible.
[00:50:06.720 --> 00:50:08.640]   - Exactly, so it was hard enough just to do Go.
[00:50:08.640 --> 00:50:10.760]   As you were saying, everyone thought that was impossible
[00:50:10.760 --> 00:50:15.200]   or at least a decade away from when we did it back in 2015,
[00:50:15.200 --> 00:50:17.320]   24, you know, 2016.
[00:50:17.320 --> 00:50:20.960]   And so, yes, it would have been psychologically
[00:50:20.960 --> 00:50:23.080]   probably very difficult as well as the fact that,
[00:50:23.080 --> 00:50:26.440]   of course, we learn a lot by building AlphaGo first.
[00:50:26.440 --> 00:50:28.520]   Right, so I think this is why I call AI
[00:50:28.520 --> 00:50:29.920]   an engineering science.
[00:50:29.920 --> 00:50:32.280]   It's one of the most fascinating science disciplines,
[00:50:32.280 --> 00:50:34.680]   but it's also an engineering science in the sense that,
[00:50:34.680 --> 00:50:38.200]   unlike natural sciences, the phenomenon you're studying
[00:50:38.200 --> 00:50:39.440]   doesn't exist out in nature.
[00:50:39.440 --> 00:50:40.880]   You have to build it first.
[00:50:40.880 --> 00:50:42.480]   So you have to build the artifact first
[00:50:42.480 --> 00:50:46.480]   and then you can study and pull it apart and how it works.
[00:50:46.480 --> 00:50:50.000]   - This is tough to ask you this question
[00:50:50.000 --> 00:50:51.480]   'cause you probably will say it's everything,
[00:50:51.480 --> 00:50:54.360]   but let's try to think through this
[00:50:54.360 --> 00:50:56.480]   because you're in a very interesting position
[00:50:56.480 --> 00:51:00.320]   where DeepMind is a place of some of the most brilliant ideas
[00:51:00.320 --> 00:51:01.760]   in the history of AI,
[00:51:01.760 --> 00:51:04.580]   but it's also a place of brilliant engineering.
[00:51:04.580 --> 00:51:08.040]   So how much of solving intelligence,
[00:51:08.040 --> 00:51:12.120]   this big goal for DeepMind, how much of it is science?
[00:51:12.120 --> 00:51:13.320]   How much is engineering?
[00:51:13.320 --> 00:51:14.720]   So how much is the algorithms?
[00:51:14.720 --> 00:51:16.160]   How much is the data?
[00:51:16.160 --> 00:51:19.840]   How much is the hardware compute infrastructure?
[00:51:19.840 --> 00:51:22.760]   How much is it the software compute infrastructure?
[00:51:22.760 --> 00:51:24.800]   What else is there?
[00:51:24.800 --> 00:51:27.240]   How much is the human infrastructure?
[00:51:27.240 --> 00:51:30.120]   And like just the humans interacting certain kinds of ways.
[00:51:30.120 --> 00:51:31.720]   It's based of all those ideas.
[00:51:31.720 --> 00:51:34.160]   How much is maybe like philosophy?
[00:51:34.160 --> 00:51:35.080]   What's the key?
[00:51:35.080 --> 00:51:40.680]   If you were to sort of look back,
[00:51:40.680 --> 00:51:43.200]   like if we go forward 200 years and look back,
[00:51:43.200 --> 00:51:46.320]   what was the key thing that solved intelligence?
[00:51:46.320 --> 00:51:47.760]   Is it the ideas or the engineering?
[00:51:47.760 --> 00:51:49.080]   - I think it's a combination.
[00:51:49.080 --> 00:51:49.920]   First of all, of course,
[00:51:49.920 --> 00:51:51.360]   it's a combination of all those things,
[00:51:51.360 --> 00:51:54.760]   but the ratios of them changed over time.
[00:51:54.760 --> 00:51:57.480]   So even in the last 12 years,
[00:51:57.480 --> 00:51:59.420]   so we started DeepMind in 2010,
[00:51:59.420 --> 00:52:01.960]   which is hard to imagine now because 2010,
[00:52:01.960 --> 00:52:03.400]   it's only 12 short years ago,
[00:52:03.400 --> 00:52:05.120]   but nobody was talking about AI.
[00:52:05.120 --> 00:52:07.600]   I don't know if you remember back to your MIT days,
[00:52:07.600 --> 00:52:08.720]   no one was talking about it.
[00:52:08.720 --> 00:52:11.080]   I did a postdoc at MIT back around then,
[00:52:11.080 --> 00:52:12.880]   and it was sort of thought of as a,
[00:52:12.880 --> 00:52:14.200]   well, look, we know AI doesn't work.
[00:52:14.200 --> 00:52:17.040]   We tried this hard in the '90s at places like MIT,
[00:52:17.040 --> 00:52:20.280]   mostly using logic systems and old-fashioned sort of,
[00:52:20.280 --> 00:52:22.600]   good old-fashioned AI, we would call it now.
[00:52:22.600 --> 00:52:25.320]   People like Minsky and Patrick Winston,
[00:52:25.320 --> 00:52:26.720]   and you know all these characters, right?
[00:52:26.720 --> 00:52:28.280]   And used to debate a few of them,
[00:52:28.280 --> 00:52:29.500]   and they used to think I was mad,
[00:52:29.500 --> 00:52:31.020]   thinking about that some new advance
[00:52:31.020 --> 00:52:32.340]   could be done with learning systems.
[00:52:32.340 --> 00:52:34.740]   And I was actually pleased to hear that
[00:52:34.740 --> 00:52:36.940]   because at least you know you're on a unique track
[00:52:36.940 --> 00:52:37.860]   at that point, right?
[00:52:37.860 --> 00:52:41.860]   Even if all of your professors are telling you you're mad.
[00:52:41.860 --> 00:52:43.860]   And of course, in industry,
[00:52:43.860 --> 00:52:44.700]   we couldn't get, you know,
[00:52:44.700 --> 00:52:47.660]   it was difficult to get two cents together,
[00:52:47.660 --> 00:52:48.940]   which is hard to imagine now as well,
[00:52:48.940 --> 00:52:51.540]   given that it's the biggest sort of buzzword in VCs
[00:52:51.540 --> 00:52:54.700]   and fundraising's easy and all these kinds of things today.
[00:52:54.700 --> 00:52:57.700]   So back in 2010, it was very difficult.
[00:52:57.700 --> 00:52:59.340]   And the reason we started then,
[00:52:59.340 --> 00:53:01.080]   and Shane and I used to discuss
[00:53:01.080 --> 00:53:04.900]   what were the sort of founding tenets of DeepMind,
[00:53:04.900 --> 00:53:06.120]   and it was various things.
[00:53:06.120 --> 00:53:08.680]   One was algorithmic advances.
[00:53:08.680 --> 00:53:11.140]   So deep learning, you know, Jeff Hinton and co.
[00:53:11.140 --> 00:53:13.140]   had just sort of invented that in academia,
[00:53:13.140 --> 00:53:15.220]   but no one in industry knew about it.
[00:53:15.220 --> 00:53:16.660]   We love reinforcement learning.
[00:53:16.660 --> 00:53:18.260]   We thought that could be scaled up.
[00:53:18.260 --> 00:53:20.160]   But also understanding about the human brain
[00:53:20.160 --> 00:53:23.920]   had advanced quite a lot in the decade prior,
[00:53:23.920 --> 00:53:25.460]   with fMRI machines and other things.
[00:53:25.460 --> 00:53:27.740]   So we could get some good hints
[00:53:27.740 --> 00:53:29.700]   about architectures and algorithms
[00:53:29.700 --> 00:53:33.420]   and sort of representations maybe that the brain uses.
[00:53:33.420 --> 00:53:36.900]   So at a systems level, not at a implementation level.
[00:53:36.900 --> 00:53:41.060]   And then the other big things were compute and GPUs, right?
[00:53:41.060 --> 00:53:44.180]   So we could see a compute was gonna be really useful
[00:53:44.180 --> 00:53:47.020]   and it got to a place where it become commoditized,
[00:53:47.020 --> 00:53:48.580]   mostly through the games industry,
[00:53:48.580 --> 00:53:50.780]   and that could be taken advantage of.
[00:53:50.780 --> 00:53:52.820]   And then the final thing was also mathematical
[00:53:52.820 --> 00:53:55.000]   and theoretical definitions of intelligence.
[00:53:55.000 --> 00:53:57.620]   So things like AIXI, A-I-X-E,
[00:53:57.620 --> 00:54:00.220]   which Shane worked on with his supervisor, Marcus Hutter,
[00:54:00.220 --> 00:54:03.420]   which is this sort of theoretical proof really
[00:54:03.420 --> 00:54:05.340]   of universal intelligence,
[00:54:05.340 --> 00:54:07.940]   which is actually a reinforcement learning system.
[00:54:07.940 --> 00:54:09.980]   In the limit, I mean, it assumes infinite compute
[00:54:09.980 --> 00:54:11.460]   and infinite memory in the way, you know,
[00:54:11.460 --> 00:54:12.960]   like a Turing machine proves.
[00:54:12.960 --> 00:54:15.900]   But I was also waiting to see something like that too,
[00:54:15.900 --> 00:54:19.500]   to, you know, like Turing machines and computation theory
[00:54:19.500 --> 00:54:21.580]   that people like Turing and Shannon came up with
[00:54:21.580 --> 00:54:23.720]   underpins modern computer science.
[00:54:24.820 --> 00:54:26.420]   You know, I was waiting for a theory like that
[00:54:26.420 --> 00:54:28.880]   to sort of underpin AGI research.
[00:54:28.880 --> 00:54:30.940]   So when I met Shane and saw he was working
[00:54:30.940 --> 00:54:32.180]   on something like that, you know,
[00:54:32.180 --> 00:54:34.540]   that to me was a sort of final piece of the jigsaw.
[00:54:34.540 --> 00:54:36.420]   So in the early days,
[00:54:36.420 --> 00:54:40.460]   I would say that ideas were the most important, you know,
[00:54:40.460 --> 00:54:42.460]   for us it was deep reinforcement learning,
[00:54:42.460 --> 00:54:44.620]   scaling up deep learning.
[00:54:44.620 --> 00:54:46.240]   Of course we've seen transformers.
[00:54:46.240 --> 00:54:48.100]   So huge leaps, I would say, you know,
[00:54:48.100 --> 00:54:51.500]   three or four from, if you think from 2010 till now,
[00:54:51.500 --> 00:54:53.700]   huge evolutions, things like AlphaGo.
[00:54:54.640 --> 00:54:58.460]   And maybe there's a few more still needed.
[00:54:58.460 --> 00:55:01.680]   But as we get closer to AI, AGI,
[00:55:01.680 --> 00:55:05.180]   I think engineering becomes more and more important
[00:55:05.180 --> 00:55:08.360]   and data because scale and of course the recent,
[00:55:08.360 --> 00:55:11.020]   you know, results of GPT-3 and all the big language models
[00:55:11.020 --> 00:55:13.360]   and large models, including our ones,
[00:55:13.360 --> 00:55:16.560]   has shown that scale is, and large models
[00:55:16.560 --> 00:55:18.660]   are clearly gonna be a necessary,
[00:55:18.660 --> 00:55:22.520]   but perhaps not sufficient part of an AGI solution.
[00:55:22.520 --> 00:55:25.040]   - And throughout that, like you said,
[00:55:25.040 --> 00:55:27.220]   and I'd like to give you a big thank you.
[00:55:27.220 --> 00:55:29.300]   You're one of the pioneers in this,
[00:55:29.300 --> 00:55:32.660]   is sticking by ideas like reinforcement learning,
[00:55:32.660 --> 00:55:34.060]   that this can actually work,
[00:55:34.060 --> 00:55:38.980]   given actually limited success in the past.
[00:55:38.980 --> 00:55:41.980]   And also, which we still don't know,
[00:55:41.980 --> 00:55:46.980]   but proudly having the best researchers in the world
[00:55:46.980 --> 00:55:49.900]   and talking about solving intelligence.
[00:55:49.900 --> 00:55:51.460]   So talking about whatever you call it,
[00:55:51.460 --> 00:55:55.240]   AGI or something like this, that speaking of MIT,
[00:55:55.240 --> 00:55:57.840]   that's just something you wouldn't bring up.
[00:55:57.840 --> 00:55:58.680]   - No.
[00:55:58.680 --> 00:56:04.120]   - Maybe you did in like 40, 50 years ago,
[00:56:04.120 --> 00:56:05.440]   but that was,
[00:56:05.440 --> 00:56:09.880]   AI was a place where you do tinkering,
[00:56:09.880 --> 00:56:13.120]   very small scale, not very ambitious projects.
[00:56:13.120 --> 00:56:16.720]   And maybe the biggest ambitious projects
[00:56:16.720 --> 00:56:18.020]   were in the space of robotics
[00:56:18.020 --> 00:56:19.720]   and doing like the DARPA challenge.
[00:56:19.720 --> 00:56:21.780]   But the task of solving intelligence
[00:56:21.780 --> 00:56:24.900]   and believing you can, that's really, really powerful.
[00:56:24.900 --> 00:56:28.020]   So in order for engineering to do its work,
[00:56:28.020 --> 00:56:31.280]   to have great engineers, build great systems,
[00:56:31.280 --> 00:56:32.680]   you have to have that belief,
[00:56:32.680 --> 00:56:34.280]   that threads throughout the whole thing,
[00:56:34.280 --> 00:56:35.400]   that you can actually solve
[00:56:35.400 --> 00:56:37.000]   some of these impossible challenges.
[00:56:37.000 --> 00:56:37.840]   - Yeah, that's right.
[00:56:37.840 --> 00:56:41.080]   And back in 2010, our mission statement,
[00:56:41.080 --> 00:56:42.620]   and still is today,
[00:56:42.620 --> 00:56:45.920]   it was used to be solving step one, solve intelligence,
[00:56:45.920 --> 00:56:47.840]   step two, use it to solve everything else.
[00:56:47.840 --> 00:56:51.160]   So if you can imagine pitching that to VC in 2010,
[00:56:51.160 --> 00:56:52.680]   the kind of looks we got,
[00:56:52.680 --> 00:56:55.880]   we managed to find a few kooky people to back us,
[00:56:55.880 --> 00:56:57.680]   but it was tricky.
[00:56:57.680 --> 00:57:00.180]   And I got to the point where we wouldn't mention it
[00:57:00.180 --> 00:57:01.560]   to any of our professors,
[00:57:01.560 --> 00:57:03.120]   because they would just eye roll
[00:57:03.120 --> 00:57:05.920]   and think we committed career suicide.
[00:57:05.920 --> 00:57:10.040]   So it was, there's a lot of things that we had to do,
[00:57:10.040 --> 00:57:11.520]   but we always believed it.
[00:57:11.520 --> 00:57:13.240]   And one reason, by the way,
[00:57:13.240 --> 00:57:16.160]   one reason I've always believed in reinforcement learning
[00:57:16.160 --> 00:57:19.160]   is that if you look at neuroscience,
[00:57:19.160 --> 00:57:22.720]   that is the way that the primate brain learns.
[00:57:22.720 --> 00:57:24.880]   One of the main mechanisms is the dopamine system
[00:57:24.880 --> 00:57:26.440]   implements some form of TD learning.
[00:57:26.440 --> 00:57:28.680]   It was a very famous result in the late '90s,
[00:57:28.680 --> 00:57:31.200]   where they saw this in monkeys,
[00:57:31.200 --> 00:57:34.520]   and as a proper game prediction error.
[00:57:34.520 --> 00:57:36.800]   So again, in the limit,
[00:57:36.800 --> 00:57:38.800]   this is what I think you can use neuroscience for,
[00:57:38.800 --> 00:57:41.480]   is in any, at mathematics,
[00:57:41.480 --> 00:57:43.160]   when you're doing something as ambitious
[00:57:43.160 --> 00:57:44.560]   as trying to solve intelligence,
[00:57:44.560 --> 00:57:46.480]   and you're, you know, it's blue sky research,
[00:57:46.480 --> 00:57:47.760]   no one knows how to do it.
[00:57:47.760 --> 00:57:50.200]   You need to use any evidence
[00:57:50.200 --> 00:57:52.160]   or any source of information you can
[00:57:52.160 --> 00:57:54.280]   to help guide you in the right direction
[00:57:54.280 --> 00:57:56.680]   or give you confidence you're going in the right direction.
[00:57:56.680 --> 00:57:59.880]   So that was one reason we pushed so hard on that.
[00:57:59.880 --> 00:58:01.880]   And just going back to your earlier question
[00:58:01.880 --> 00:58:03.160]   about organization,
[00:58:03.160 --> 00:58:05.360]   the other big thing that I think we innovated with
[00:58:05.360 --> 00:58:10.320]   at DeepMind to encourage invention and innovation
[00:58:10.320 --> 00:58:12.920]   was the multidisciplinary organization we built,
[00:58:12.920 --> 00:58:14.160]   and we still have today.
[00:58:14.160 --> 00:58:16.680]   So DeepMind originally was a confluence
[00:58:16.680 --> 00:58:19.400]   of the most cutting edge knowledge in neuroscience
[00:58:19.400 --> 00:58:22.840]   with machine learning, engineering, and mathematics, right?
[00:58:22.840 --> 00:58:24.120]   And gaming.
[00:58:24.120 --> 00:58:26.760]   And then since then, we've built that out even further.
[00:58:26.760 --> 00:58:30.280]   So we have philosophers here and, you know, ethicists,
[00:58:30.280 --> 00:58:33.160]   but also other types of scientists, physicists, and so on.
[00:58:33.160 --> 00:58:35.120]   And that's what brings together,
[00:58:35.120 --> 00:58:38.720]   I tried to build a sort of new type of Bell Labs,
[00:58:38.720 --> 00:58:40.360]   but in its golden era, right?
[00:58:40.360 --> 00:58:43.800]   And a new expression of that
[00:58:43.800 --> 00:58:48.440]   to try and foster this incredible sort of innovation machine.
[00:58:48.440 --> 00:58:50.560]   So talking about the humans in the machine,
[00:58:50.560 --> 00:58:53.040]   DeepMind itself is a learning machine
[00:58:53.040 --> 00:58:55.560]   with lots of amazing human minds in it
[00:58:55.560 --> 00:58:58.880]   coming together to try and build these learning systems.
[00:58:58.880 --> 00:59:04.920]   - If we return to the big ambitious dream of AlphaFold
[00:59:04.920 --> 00:59:06.760]   that may be the early steps
[00:59:06.760 --> 00:59:10.740]   on a very long journey in biology,
[00:59:12.560 --> 00:59:14.120]   do you think the same kind of approach
[00:59:14.120 --> 00:59:16.360]   can you use to predict the structure and function
[00:59:16.360 --> 00:59:18.640]   of more complex biological systems?
[00:59:18.640 --> 00:59:21.400]   So multi-protein interaction,
[00:59:21.400 --> 00:59:24.320]   and then, I mean, you can go out from there.
[00:59:24.320 --> 00:59:26.840]   Just simulating bigger and bigger systems
[00:59:26.840 --> 00:59:29.480]   that eventually simulate something like the human brain
[00:59:29.480 --> 00:59:32.480]   or the human body, just the big mush,
[00:59:32.480 --> 00:59:36.360]   the mess of the beautiful, resilient mess of biology.
[00:59:36.360 --> 00:59:39.520]   Do you see that as a long-term vision?
[00:59:39.520 --> 00:59:40.360]   - I do.
[00:59:40.360 --> 00:59:43.440]   I think, you know, if you think about what are the things,
[00:59:43.440 --> 00:59:45.620]   top things I wanted to apply AI to
[00:59:45.620 --> 00:59:47.640]   once we had powerful enough systems,
[00:59:47.640 --> 00:59:52.160]   biology and curing diseases and understanding biology
[00:59:52.160 --> 00:59:54.080]   was right up there, you know, top of my list.
[00:59:54.080 --> 00:59:56.720]   That's one of the reasons I personally pushed that myself
[00:59:56.720 --> 00:59:58.040]   and with AlphaFold.
[00:59:58.040 --> 01:00:01.160]   But I think AlphaFold, amazing as it is,
[01:00:01.160 --> 01:00:02.960]   is just the beginning.
[01:00:02.960 --> 01:00:07.120]   And I hope it's evidence of what could be done
[01:00:07.120 --> 01:00:08.780]   with computational methods.
[01:00:08.780 --> 01:00:12.160]   So, you know, AlphaFold solved this huge problem
[01:00:12.160 --> 01:00:15.200]   of the structure of proteins, but biology is dynamic.
[01:00:15.200 --> 01:00:16.880]   So really what I imagine from here,
[01:00:16.880 --> 01:00:18.620]   and we're working on all these things now,
[01:00:18.620 --> 01:00:23.120]   is protein-protein interaction, protein-ligand binding,
[01:00:23.120 --> 01:00:25.360]   so reacting with molecules.
[01:00:25.360 --> 01:00:27.600]   Then you wanna get built up to pathways,
[01:00:27.600 --> 01:00:29.960]   and then eventually a virtual cell.
[01:00:29.960 --> 01:00:32.640]   That's my dream, maybe in the next 10 years.
[01:00:32.640 --> 01:00:34.520]   And I've been talking actually to a lot of biologists,
[01:00:34.520 --> 01:00:36.760]   friends of mine, Paul Nurse, who runs the Crick Institute,
[01:00:36.760 --> 01:00:39.100]   amazing biologist, Nobel Prize-winning biologist.
[01:00:39.100 --> 01:00:42.100]   We've been discussing for 20 years now virtual cells.
[01:00:42.100 --> 01:00:44.740]   Could you build a virtual simulation of a cell?
[01:00:44.740 --> 01:00:46.260]   And if you could, that would be incredible
[01:00:46.260 --> 01:00:48.100]   for biology and disease discovery,
[01:00:48.100 --> 01:00:49.500]   'cause you could do loads of experiments
[01:00:49.500 --> 01:00:52.420]   on the virtual cell, and then only at the last stage,
[01:00:52.420 --> 01:00:53.920]   validate it in the wet lab.
[01:00:53.920 --> 01:00:56.400]   So you could, you know, in terms of the search space
[01:00:56.400 --> 01:00:58.020]   of discovering new drugs, you know,
[01:00:58.020 --> 01:01:00.620]   it takes 10 years roughly to go from,
[01:01:00.620 --> 01:01:03.380]   to go from, you know, identifying a target
[01:01:03.380 --> 01:01:06.480]   to having a drug candidate.
[01:01:06.480 --> 01:01:08.100]   Maybe that could be shortened to, you know,
[01:01:08.100 --> 01:01:09.780]   by an order of magnitude with,
[01:01:09.780 --> 01:01:13.140]   if you could do most of that work in silico.
[01:01:13.140 --> 01:01:15.780]   So in order to get to a virtual cell,
[01:01:15.780 --> 01:01:18.360]   we have to build up understanding
[01:01:18.360 --> 01:01:20.780]   of different parts of biology and the interactions.
[01:01:20.780 --> 01:01:24.580]   And so, you know, every few years we talk about this,
[01:01:24.580 --> 01:01:25.620]   I talked about this with Paul.
[01:01:25.620 --> 01:01:27.860]   And then finally, last year after AlphaFold,
[01:01:27.860 --> 01:01:30.580]   I said, now's the time, we can finally go for it.
[01:01:30.580 --> 01:01:32.380]   And AlphaFold's the first proof point
[01:01:32.380 --> 01:01:33.820]   that this might be possible.
[01:01:33.820 --> 01:01:35.940]   And he's very excited, and we have some collaborations
[01:01:35.940 --> 01:01:38.460]   with his lab, they're just across the road actually
[01:01:38.460 --> 01:01:40.380]   from us, it's just, you know, wonderful being here
[01:01:40.380 --> 01:01:42.900]   in Kings Cross with the Crick Institute across the road.
[01:01:42.900 --> 01:01:45.980]   And I think the next steps, you know,
[01:01:45.980 --> 01:01:48.660]   I think there's gonna be some amazing advances in biology
[01:01:48.660 --> 01:01:50.980]   built on top of things like AlphaFold.
[01:01:50.980 --> 01:01:53.140]   We're already seeing that with the community doing that
[01:01:53.140 --> 01:01:56.020]   after we've open sourced it and released it.
[01:01:56.020 --> 01:02:00.140]   And, you know, I often say that I think,
[01:02:00.140 --> 01:02:02.300]   if you think of mathematics
[01:02:02.300 --> 01:02:05.020]   is the perfect description language for physics.
[01:02:05.020 --> 01:02:06.820]   I think AI might be, end up being
[01:02:06.820 --> 01:02:09.180]   the perfect description language for biology
[01:02:09.180 --> 01:02:12.940]   because biology is so messy, it's so emergent,
[01:02:12.940 --> 01:02:15.220]   so dynamic and complex.
[01:02:15.220 --> 01:02:17.460]   I think, I find it very hard to believe we'll ever get
[01:02:17.460 --> 01:02:20.180]   to something as elegant as Newton's laws of motions
[01:02:20.180 --> 01:02:21.660]   to describe a cell, right?
[01:02:21.660 --> 01:02:23.500]   It's just too complicated.
[01:02:23.500 --> 01:02:26.060]   So I think AI is the right tool for this.
[01:02:26.060 --> 01:02:29.380]   - You have to start at the basic building blocks
[01:02:29.380 --> 01:02:31.580]   and use AI to run the simulation
[01:02:31.580 --> 01:02:32.780]   for all those building blocks.
[01:02:32.820 --> 01:02:35.940]   So have a very strong way to do prediction
[01:02:35.940 --> 01:02:37.700]   of what given these building blocks,
[01:02:37.700 --> 01:02:40.780]   what kind of biology, how the function
[01:02:40.780 --> 01:02:43.580]   and the evolution of that biological system.
[01:02:43.580 --> 01:02:45.220]   It's almost like a cellular automata.
[01:02:45.220 --> 01:02:46.060]   You have to run it.
[01:02:46.060 --> 01:02:47.820]   You can't analyze it from a high level.
[01:02:47.820 --> 01:02:49.740]   You have to take the basic ingredients,
[01:02:49.740 --> 01:02:51.860]   figure out the rules and let it run.
[01:02:51.860 --> 01:02:53.900]   But in this case, the rules are very difficult
[01:02:53.900 --> 01:02:55.060]   to figure out. - Yes, exactly.
[01:02:55.060 --> 01:02:56.140]   - You have to learn them.
[01:02:56.140 --> 01:02:56.980]   - That's exactly it.
[01:02:56.980 --> 01:02:58.940]   So it's the biology is too complicated
[01:02:58.940 --> 01:03:00.700]   to figure out the rules.
[01:03:00.700 --> 01:03:03.500]   It's too emergent, too dynamic,
[01:03:03.500 --> 01:03:04.980]   say compared to a physics system
[01:03:04.980 --> 01:03:06.940]   like the motion of a planet, right?
[01:03:06.940 --> 01:03:09.100]   And so you have to learn the rules
[01:03:09.100 --> 01:03:11.820]   and that's exactly the type of systems that we're building.
[01:03:11.820 --> 01:03:14.660]   - So you mentioned you've open sourced AlphaFold
[01:03:14.660 --> 01:03:16.500]   and even the data involved.
[01:03:16.500 --> 01:03:19.940]   To me personally, also really happy
[01:03:19.940 --> 01:03:22.580]   and a big thank you for open sourcing with JoCo,
[01:03:22.580 --> 01:03:24.860]   the physics simulation engine
[01:03:24.860 --> 01:03:28.980]   that's often used for robotics research and so on.
[01:03:28.980 --> 01:03:31.060]   So I think that's a pretty gangster move.
[01:03:31.060 --> 01:03:33.260]   So what's the, (laughs)
[01:03:33.260 --> 01:03:35.140]   what's, I mean, this,
[01:03:35.140 --> 01:03:39.020]   very few companies or people do that kind of thing.
[01:03:39.020 --> 01:03:41.140]   What's the philosophy behind that?
[01:03:41.140 --> 01:03:42.860]   - You know, it's a case by case basis
[01:03:42.860 --> 01:03:43.980]   and in both those cases,
[01:03:43.980 --> 01:03:47.300]   we felt that was the maximum benefit to humanity to do that
[01:03:47.300 --> 01:03:49.420]   and the scientific community.
[01:03:49.420 --> 01:03:52.940]   In one case, the robotics physics community with Mojoco.
[01:03:52.940 --> 01:03:53.940]   So-- - We purchased it.
[01:03:53.940 --> 01:03:55.540]   - We purchased it-- - Open source.
[01:03:55.540 --> 01:03:57.580]   - Yes, we purchased it for the express principle
[01:03:57.580 --> 01:03:58.500]   to open source it.
[01:03:58.500 --> 01:04:02.380]   So, you know, I hope people appreciate that.
[01:04:02.380 --> 01:04:04.020]   It's great to hear that you do.
[01:04:04.020 --> 01:04:05.740]   And then the second thing was,
[01:04:05.740 --> 01:04:08.140]   and mostly we did it because the person building it
[01:04:08.140 --> 01:04:11.900]   was not able to cope with supporting it anymore
[01:04:11.900 --> 01:04:13.540]   'cause it got too big for him.
[01:04:13.540 --> 01:04:16.700]   He's an amazing professor who built it in the first place.
[01:04:16.700 --> 01:04:18.180]   So we helped him out with that.
[01:04:18.180 --> 01:04:20.460]   And then with AlphaFold, even bigger, I would say,
[01:04:20.460 --> 01:04:21.900]   and I think in that case,
[01:04:21.900 --> 01:04:25.460]   we decided that there were so many downstream applications
[01:04:25.460 --> 01:04:29.340]   of AlphaFold that we couldn't possibly even imagine
[01:04:29.340 --> 01:04:30.420]   what they all were.
[01:04:30.420 --> 01:04:34.300]   So the best way to accelerate drug discovery
[01:04:34.300 --> 01:04:38.500]   and also fundamental research would be to give
[01:04:38.500 --> 01:04:42.380]   all that data away and the system itself.
[01:04:42.380 --> 01:04:45.220]   You know, it's been so gratifying to see
[01:04:45.220 --> 01:04:46.980]   what people have done that within just one year,
[01:04:46.980 --> 01:04:49.180]   which is a short amount of time in science.
[01:04:49.180 --> 01:04:54.100]   And it's being used by over 500,000 researchers have used it.
[01:04:54.100 --> 01:04:56.500]   We think that's almost every biologist in the world.
[01:04:56.500 --> 01:04:58.820]   I think there's roughly 500,000 biologists in the world,
[01:04:58.820 --> 01:04:59.940]   professional biologists,
[01:04:59.940 --> 01:05:03.260]   have used it to look at their proteins of interest.
[01:05:03.260 --> 01:05:06.500]   We've seen amazing fundamental research done.
[01:05:06.500 --> 01:05:08.980]   So a couple of weeks ago, front cover,
[01:05:08.980 --> 01:05:10.780]   there was a whole special history of science,
[01:05:10.780 --> 01:05:11.980]   including the front cover,
[01:05:11.980 --> 01:05:13.940]   which had the nuclear pore complex on it,
[01:05:13.940 --> 01:05:15.740]   which is one of the biggest proteins in the body.
[01:05:15.740 --> 01:05:18.900]   The nuclear pore complex is a protein that governs
[01:05:18.900 --> 01:05:21.660]   all the nutrients going in and out of your cell nucleus.
[01:05:21.660 --> 01:05:23.540]   So it's there like little hall gateways
[01:05:23.540 --> 01:05:25.740]   that open and close to let things go in
[01:05:25.740 --> 01:05:27.260]   and out of your cell nucleus.
[01:05:27.260 --> 01:05:28.700]   So they're really important,
[01:05:28.700 --> 01:05:29.940]   but they're huge because they're massive
[01:05:29.940 --> 01:05:31.620]   donut ring shaped things.
[01:05:31.620 --> 01:05:33.420]   And they've been looking to try and figure out
[01:05:33.420 --> 01:05:34.940]   that structure for decades.
[01:05:34.940 --> 01:05:37.100]   And they have lots of experimental data,
[01:05:37.100 --> 01:05:39.540]   but it's too low resolution, there's bits missing.
[01:05:39.540 --> 01:05:43.060]   And they were able to, like a giant Lego jigsaw puzzle,
[01:05:43.060 --> 01:05:46.140]   use alpha fold predictions plus experimental data
[01:05:46.140 --> 01:05:49.740]   and combined those two independent sources of information,
[01:05:49.740 --> 01:05:51.220]   actually four different groups around the world
[01:05:51.220 --> 01:05:54.580]   were able to put it together more or less simultaneously
[01:05:54.580 --> 01:05:56.260]   using alpha fold predictions.
[01:05:56.260 --> 01:05:57.700]   So that's been amazing to see.
[01:05:57.700 --> 01:05:59.380]   And pretty much every pharma company,
[01:05:59.380 --> 01:06:01.420]   every drug company executive I've spoken to
[01:06:01.420 --> 01:06:03.740]   has said that their teams are using alpha fold
[01:06:03.740 --> 01:06:08.020]   to accelerate whatever drugs they're trying to discover.
[01:06:08.020 --> 01:06:11.420]   So I think the knock on effect has been enormous
[01:06:11.420 --> 01:06:15.220]   in terms of the impact that alpha fold has made.
[01:06:15.220 --> 01:06:17.820]   - And it's probably bringing in, it's creating biologists,
[01:06:17.820 --> 01:06:20.780]   it's bringing more people into the field,
[01:06:20.780 --> 01:06:21.820]   both on the excitement
[01:06:21.820 --> 01:06:24.580]   and both on the technical skills involved.
[01:06:24.580 --> 01:06:28.780]   And it's almost like a gateway drug to biology.
[01:06:28.780 --> 01:06:29.620]   - Yes, it is.
[01:06:29.620 --> 01:06:32.660]   And to get more computational people involved too, hopefully.
[01:06:32.660 --> 01:06:35.980]   And I think for us, the next stage, as I said,
[01:06:35.980 --> 01:06:37.980]   future we have to have other considerations too.
[01:06:37.980 --> 01:06:39.620]   We're building on top of alpha fold
[01:06:39.620 --> 01:06:41.220]   and these other ideas I discussed with you
[01:06:41.220 --> 01:06:42.780]   about protein-protein interactions
[01:06:42.780 --> 01:06:44.820]   and genomics and other things.
[01:06:44.820 --> 01:06:46.220]   And not everything will be open source.
[01:06:46.220 --> 01:06:48.020]   Some of it we'll do commercially
[01:06:48.020 --> 01:06:49.020]   'cause that will be the best way
[01:06:49.020 --> 01:06:51.740]   to actually get the most resources and impact behind it.
[01:06:51.740 --> 01:06:55.300]   In other ways, some other projects we'll do non-profit style.
[01:06:55.300 --> 01:06:58.540]   And also we have to consider for future things as well,
[01:06:58.540 --> 01:07:01.620]   safety and ethics as well, like synthetic biology,
[01:07:01.620 --> 01:07:05.100]   there is dual use and we have to think about that as well.
[01:07:05.100 --> 01:07:08.620]   With alpha fold, we consulted with 30 different bioethicists
[01:07:08.620 --> 01:07:10.260]   and other people expert in this field
[01:07:10.260 --> 01:07:13.300]   to make sure it was safe before we released it.
[01:07:13.300 --> 01:07:15.300]   So there'll be other considerations in future.
[01:07:15.300 --> 01:07:18.340]   But for right now, I think alpha fold is a kind of a gift
[01:07:18.340 --> 01:07:20.860]   from us to the scientific community.
[01:07:20.860 --> 01:07:24.220]   - So I'm pretty sure that something like alpha fold
[01:07:24.220 --> 01:07:29.140]   would be part of Nobel prizes in the future.
[01:07:29.140 --> 01:07:32.500]   But us humans, of course, are horrible with credit assignment
[01:07:32.500 --> 01:07:34.540]   so we'll of course give it to the humans.
[01:07:34.540 --> 01:07:39.380]   Do you think there will be a day when AI system
[01:07:39.380 --> 01:07:45.140]   can't be denied that it earned that Nobel prize?
[01:07:45.140 --> 01:07:47.460]   Do you think we will see that in 21st century?
[01:07:47.460 --> 01:07:50.220]   - It depends what type of AIs we end up building,
[01:07:50.220 --> 01:07:53.580]   whether they're goal seeking agents
[01:07:53.580 --> 01:07:57.820]   who specifies the goals, who comes up with the hypotheses,
[01:07:57.820 --> 01:08:00.860]   who determines which problems to tackle.
[01:08:00.860 --> 01:08:02.420]   - And tweets about it, announcement of the results.
[01:08:02.420 --> 01:08:05.420]   - Yes, it's announced the results exactly as part of it.
[01:08:05.420 --> 01:08:07.900]   So I think right now, of course,
[01:08:07.900 --> 01:08:12.180]   it's amazing human ingenuity that's behind these systems
[01:08:12.180 --> 01:08:15.100]   and then the system in my opinion is just a tool.
[01:08:15.100 --> 01:08:17.260]   Be a bit like saying with Galileo
[01:08:17.260 --> 01:08:19.180]   and his telescope, the ingenuity,
[01:08:19.180 --> 01:08:21.140]   the credit should go to the telescope.
[01:08:21.140 --> 01:08:23.580]   I mean, it's clearly Galileo building the tool
[01:08:23.580 --> 01:08:25.180]   which he then uses.
[01:08:25.180 --> 01:08:27.340]   So I still see that in the same way today
[01:08:27.340 --> 01:08:29.860]   even though these tools learn for themselves.
[01:08:29.860 --> 01:08:32.940]   I think of things like alpha fold
[01:08:32.940 --> 01:08:35.820]   and the things we're building as the ultimate tools
[01:08:35.820 --> 01:08:38.580]   for science and for acquiring new knowledge
[01:08:38.580 --> 01:08:41.140]   to help us as scientists acquire new knowledge.
[01:08:41.140 --> 01:08:43.220]   I think one day there will come a point
[01:08:43.220 --> 01:08:46.340]   where an AI system may solve
[01:08:46.340 --> 01:08:48.780]   or come up with something like general relativity
[01:08:48.780 --> 01:08:52.020]   of its own bat, not just by averaging everything
[01:08:52.020 --> 01:08:55.220]   on the internet or averaging everything on PubMed.
[01:08:55.220 --> 01:08:56.300]   Although that would be interesting to see
[01:08:56.300 --> 01:08:58.500]   what that would come up with.
[01:08:58.500 --> 01:09:00.380]   So that to me is a bit like our earlier debate
[01:09:00.380 --> 01:09:03.220]   about creativity, inventing Go
[01:09:03.220 --> 01:09:06.220]   rather than just coming up with a good Go move.
[01:09:06.220 --> 01:09:10.380]   And so I think solving, I think to,
[01:09:10.380 --> 01:09:11.780]   if we wanted to give it the credit
[01:09:11.780 --> 01:09:13.500]   of like a Nobel type of thing,
[01:09:13.500 --> 01:09:15.740]   then it would need to invent Go
[01:09:15.740 --> 01:09:19.300]   and sort of invent that new conjecture out of the blue
[01:09:19.300 --> 01:09:22.740]   rather than being specified by the human scientists
[01:09:22.740 --> 01:09:23.580]   or the human creators.
[01:09:23.580 --> 01:09:26.300]   So I think right now it's definitely just a tool.
[01:09:26.300 --> 01:09:27.900]   - Although it is interesting how far you get
[01:09:27.900 --> 01:09:29.940]   by averaging everything on the internet, like you said,
[01:09:29.940 --> 01:09:33.140]   because a lot of people do see science
[01:09:33.140 --> 01:09:35.620]   as you're always standing on the shoulders of giants.
[01:09:35.620 --> 01:09:40.060]   And the question is how much are you really reaching
[01:09:40.060 --> 01:09:42.020]   up above the shoulders of giants?
[01:09:42.020 --> 01:09:46.260]   Maybe it's just a simulating different kinds of results
[01:09:46.260 --> 01:09:49.380]   of the past with ultimately this new perspective
[01:09:49.380 --> 01:09:51.100]   that gives you this breakthrough idea.
[01:09:51.100 --> 01:09:54.380]   But that idea may not be novel
[01:09:54.380 --> 01:09:56.140]   in the way that it can't be already discovered
[01:09:56.140 --> 01:09:56.980]   on the internet.
[01:09:56.980 --> 01:10:00.060]   Maybe the Nobel prizes of the next hundred years
[01:10:00.060 --> 01:10:03.020]   are already all there on the internet to be discovered.
[01:10:03.020 --> 01:10:04.540]   - They could be, they could be.
[01:10:04.540 --> 01:10:08.940]   I mean, I think this is one of the big mysteries I think
[01:10:08.940 --> 01:10:12.900]   is that, first of all, I believe a lot of the big,
[01:10:12.900 --> 01:10:14.380]   new breakthroughs that are gonna come
[01:10:14.380 --> 01:10:16.580]   in the next few decades, and even in the last decade,
[01:10:16.580 --> 01:10:18.260]   are gonna come at the intersection
[01:10:18.260 --> 01:10:20.140]   between different subject areas
[01:10:20.140 --> 01:10:23.420]   where there'll be some new connection that's found
[01:10:23.420 --> 01:10:26.140]   between what seemingly were disparate areas.
[01:10:26.140 --> 01:10:28.780]   And one can even think of DeepMind, as I said earlier,
[01:10:28.780 --> 01:10:31.700]   as a sort of interdisciplinary between neuroscience ideas
[01:10:31.700 --> 01:10:35.020]   and AI engineering ideas originally.
[01:10:35.020 --> 01:10:37.900]   And so I think there's that.
[01:10:37.900 --> 01:10:40.340]   And then one of the things we can't imagine today is,
[01:10:40.340 --> 01:10:41.660]   and one of the reasons I think people,
[01:10:41.660 --> 01:10:44.380]   we were so surprised by how well large models worked,
[01:10:44.380 --> 01:10:47.860]   is that actually it's very hard for our human minds,
[01:10:47.860 --> 01:10:49.380]   our limited human minds to understand
[01:10:49.380 --> 01:10:51.980]   what it would be like to read the whole internet, right?
[01:10:51.980 --> 01:10:53.460]   I think we can do a thought experiment,
[01:10:53.460 --> 01:10:54.620]   and I used to do this, of like,
[01:10:54.620 --> 01:10:57.540]   well, what if I read the whole of Wikipedia?
[01:10:57.540 --> 01:10:58.420]   What would I know?
[01:10:58.420 --> 01:11:00.420]   And I think our minds can just about comprehend
[01:11:00.420 --> 01:11:01.860]   maybe what that would be like,
[01:11:01.860 --> 01:11:04.380]   but the whole internet is beyond comprehension.
[01:11:04.380 --> 01:11:07.380]   So I think we just don't understand what it would be like
[01:11:07.380 --> 01:11:10.300]   to be able to hold all of that in mind, potentially, right?
[01:11:10.300 --> 01:11:12.900]   And then active at once,
[01:11:12.900 --> 01:11:14.500]   and then maybe what are the connections
[01:11:14.500 --> 01:11:15.740]   that are available there?
[01:11:15.740 --> 01:11:17.460]   So I think no doubt there are huge things
[01:11:17.460 --> 01:11:19.220]   to be discovered just like that.
[01:11:19.220 --> 01:11:22.220]   But I do think there is this other type of creativity,
[01:11:22.220 --> 01:11:24.660]   of a true spark of new knowledge,
[01:11:24.660 --> 01:11:26.620]   new idea never thought before about,
[01:11:26.620 --> 01:11:29.260]   can't be averaged from things that are known,
[01:11:29.260 --> 01:11:31.900]   that really, of course, everything come,
[01:11:31.900 --> 01:11:33.580]   nobody creates in a vacuum,
[01:11:33.580 --> 01:11:35.340]   so there must be clues somewhere,
[01:11:35.380 --> 01:11:38.260]   but just a unique way of putting those things together.
[01:11:38.260 --> 01:11:40.460]   I think some of the greatest scientists in history
[01:11:40.460 --> 01:11:42.180]   have displayed that, I would say,
[01:11:42.180 --> 01:11:45.060]   although it's very hard to know, going back to their time,
[01:11:45.060 --> 01:11:48.060]   what was exactly known when they came up with those things.
[01:11:48.060 --> 01:11:51.220]   - Although, you're making me really think,
[01:11:51.220 --> 01:11:53.260]   because just the thought experiment
[01:11:53.260 --> 01:11:57.300]   of deeply knowing 100 Wikipedia pages,
[01:11:57.300 --> 01:11:59.140]   I don't think I can,
[01:11:59.140 --> 01:12:01.740]   I've been really impressed by Wikipedia
[01:12:01.740 --> 01:12:03.380]   for technical topics.
[01:12:03.380 --> 01:12:07.020]   So if you know 100 pages or 1,000 pages,
[01:12:07.020 --> 01:12:10.100]   I don't think you can visually, truly comprehend
[01:12:10.100 --> 01:12:13.180]   what kind of intelligence that is.
[01:12:13.180 --> 01:12:14.700]   That's a pretty powerful intelligence.
[01:12:14.700 --> 01:12:16.100]   If you know how to use that
[01:12:16.100 --> 01:12:18.340]   and integrate that information correctly,
[01:12:18.340 --> 01:12:20.020]   I think you can go really far.
[01:12:20.020 --> 01:12:22.060]   You can probably construct thought experiments
[01:12:22.060 --> 01:12:25.860]   based on that, like simulate different ideas.
[01:12:25.860 --> 01:12:28.860]   So if this is true, let me run this thought experiment,
[01:12:28.860 --> 01:12:30.180]   then maybe this is true.
[01:12:30.180 --> 01:12:31.340]   It's not really invention,
[01:12:31.340 --> 01:12:34.740]   it's just taking literally the knowledge
[01:12:34.740 --> 01:12:36.060]   and using it to construct
[01:12:36.060 --> 01:12:37.900]   a very basic simulation of the world.
[01:12:37.900 --> 01:12:40.100]   I mean, some argue it's romantic in part,
[01:12:40.100 --> 01:12:42.380]   but Einstein would do the same kind of things
[01:12:42.380 --> 01:12:43.700]   with thought experiments, right?
[01:12:43.700 --> 01:12:46.300]   - Yeah, one could imagine doing that systematically
[01:12:46.300 --> 01:12:48.420]   across millions of Wikipedia pages,
[01:12:48.420 --> 01:12:50.340]   plus PubMed, all these things.
[01:12:50.340 --> 01:12:52.980]   I think there are many, many things
[01:12:52.980 --> 01:12:55.260]   to be discovered like that that are hugely useful.
[01:12:55.260 --> 01:12:56.180]   You know, you could imagine,
[01:12:56.180 --> 01:12:57.580]   and I want us to do some of these things
[01:12:57.580 --> 01:12:59.980]   in material science, like room temperature superconductors
[01:12:59.980 --> 01:13:01.500]   is something on my list one day
[01:13:01.500 --> 01:13:04.980]   that I'd like to have an AI system to help build,
[01:13:04.980 --> 01:13:06.620]   better optimized batteries,
[01:13:06.620 --> 01:13:08.940]   all of these sort of mechanical things.
[01:13:08.940 --> 01:13:11.580]   I think a systematic sort of search
[01:13:11.580 --> 01:13:14.340]   could be guided by a model,
[01:13:14.340 --> 01:13:17.100]   could be extremely powerful.
[01:13:17.100 --> 01:13:20.160]   - So speaking of which, you have a paper on nuclear fusion,
[01:13:20.160 --> 01:13:23.100]   magnetic control of tachymeric plasmas
[01:13:23.100 --> 01:13:24.700]   through deep reinforcement learning.
[01:13:24.700 --> 01:13:29.700]   So you're seeking to solve nuclear fusion with deep RL.
[01:13:29.780 --> 01:13:31.820]   So it's doing control of high temperature plasmas.
[01:13:31.820 --> 01:13:33.500]   Can you explain this work?
[01:13:33.500 --> 01:13:36.340]   And can AI eventually solve nuclear fusion?
[01:13:36.340 --> 01:13:39.380]   - (laughs) It's been very fun last year or two,
[01:13:39.380 --> 01:13:41.180]   and very productive because we've been taking off
[01:13:41.180 --> 01:13:43.820]   a lot of my dream projects, if you like,
[01:13:43.820 --> 01:13:45.620]   of things that I've collected over the years
[01:13:45.620 --> 01:13:48.140]   of areas of science that I would like to,
[01:13:48.140 --> 01:13:49.660]   I think could be very transformative
[01:13:49.660 --> 01:13:51.180]   if we helped accelerate,
[01:13:51.180 --> 01:13:53.580]   and are really interesting problems,
[01:13:53.580 --> 01:13:55.700]   scientific challenges in and of themselves.
[01:13:55.700 --> 01:13:57.020]   - So this is energy.
[01:13:57.020 --> 01:13:58.500]   - So energy, yes, exactly.
[01:13:58.500 --> 01:13:59.940]   So energy and climate.
[01:13:59.940 --> 01:14:01.740]   So we talked about disease and biology
[01:14:01.740 --> 01:14:04.500]   as being one of the biggest places I think AI can help with.
[01:14:04.500 --> 01:14:07.100]   I think energy and climate is another one.
[01:14:07.100 --> 01:14:09.220]   So maybe they would be my top two.
[01:14:09.220 --> 01:14:12.500]   And fusion is one area I think AI can help with.
[01:14:12.500 --> 01:14:15.340]   Now, fusion has many challenges,
[01:14:15.340 --> 01:14:17.220]   mostly physics and material science
[01:14:17.220 --> 01:14:18.540]   and engineering challenges as well,
[01:14:18.540 --> 01:14:20.460]   to build these massive fusion reactors
[01:14:20.460 --> 01:14:21.860]   and contain the plasma.
[01:14:21.860 --> 01:14:22.700]   And what we try to do,
[01:14:22.700 --> 01:14:26.220]   and whenever we go into a new field to apply our systems,
[01:14:26.220 --> 01:14:29.180]   is we look for, we talk to domain experts,
[01:14:29.180 --> 01:14:30.620]   we try and find the best people in the world
[01:14:30.620 --> 01:14:31.620]   to collaborate with.
[01:14:31.620 --> 01:14:34.060]   In this case, in fusion,
[01:14:34.060 --> 01:14:36.340]   we collaborated with EPFL in Switzerland,
[01:14:36.340 --> 01:14:38.180]   the Swiss Technical Institute, who are amazing.
[01:14:38.180 --> 01:14:39.580]   They have a test reactor.
[01:14:39.580 --> 01:14:41.300]   They were willing to let us use,
[01:14:41.300 --> 01:14:43.340]   which I double-checked with the team
[01:14:43.340 --> 01:14:46.060]   we were gonna use carefully and safely.
[01:14:46.060 --> 01:14:47.700]   I was impressed they managed to persuade them
[01:14:47.700 --> 01:14:49.060]   to let us use it.
[01:14:49.060 --> 01:14:53.380]   And it's an amazing test reactor they have there.
[01:14:53.380 --> 01:14:56.940]   And they try all sorts of pretty crazy experiments on it.
[01:14:56.940 --> 01:14:59.660]   And what we tend to look at is,
[01:14:59.660 --> 01:15:01.700]   if we go into a new domain like fusion,
[01:15:01.700 --> 01:15:04.140]   what are all the bottleneck problems?
[01:15:04.140 --> 01:15:05.940]   Like thinking from first principles,
[01:15:05.940 --> 01:15:06.980]   what are all the bottleneck problems
[01:15:06.980 --> 01:15:09.260]   that are still stopping fusion working today?
[01:15:09.260 --> 01:15:12.100]   And then we look at, we get a fusion expert to tell us,
[01:15:12.100 --> 01:15:13.740]   and then we look at those bottlenecks,
[01:15:13.740 --> 01:15:14.580]   and we look at the ones,
[01:15:14.580 --> 01:15:18.100]   which ones are amenable to our AI methods today.
[01:15:18.100 --> 01:15:18.940]   - Yes.
[01:15:18.940 --> 01:15:22.220]   - And would be interesting from a research perspective,
[01:15:22.220 --> 01:15:24.420]   from our point of view, from an AI point of view.
[01:15:24.420 --> 01:15:26.740]   And that would address one of their bottlenecks.
[01:15:26.740 --> 01:15:29.700]   And in this case, plasma control was perfect.
[01:15:29.700 --> 01:15:32.500]   So, the plasma, it's a million degrees Celsius,
[01:15:32.500 --> 01:15:34.660]   something like that, hotter than the sun.
[01:15:34.660 --> 01:15:37.660]   And there's obviously no material that can contain it.
[01:15:37.660 --> 01:15:39.460]   So they have to be containing these magnetic,
[01:15:39.460 --> 01:15:42.540]   very powerful superconducting magnetic fields.
[01:15:42.540 --> 01:15:44.860]   But the problem is plasma, it's pretty unstable,
[01:15:44.860 --> 01:15:45.700]   as you imagine.
[01:15:45.700 --> 01:15:49.340]   You're kind of holding a mini sun, mini star in a reactor.
[01:15:49.340 --> 01:15:52.540]   So, you kind of want to predict ahead of time
[01:15:52.540 --> 01:15:54.060]   what the plasma is gonna do,
[01:15:54.060 --> 01:15:58.140]   so you can move the magnetic field within a few milliseconds
[01:15:58.140 --> 01:16:00.940]   to basically contain what it's gonna do next.
[01:16:00.940 --> 01:16:03.140]   So it seems like a perfect problem, if you think of it,
[01:16:03.140 --> 01:16:06.300]   for like a reinforcement learning prediction problem.
[01:16:06.300 --> 01:16:08.140]   So, you got a controller,
[01:16:08.140 --> 01:16:09.740]   you're gonna move the magnetic field.
[01:16:09.740 --> 01:16:11.380]   And until we came along,
[01:16:11.380 --> 01:16:14.460]   they were doing it with traditional operational
[01:16:14.460 --> 01:16:16.740]   research type of controllers,
[01:16:16.740 --> 01:16:18.300]   which are kind of handcrafted.
[01:16:18.300 --> 01:16:20.460]   And the problem is, of course, they can't react in the moment
[01:16:20.460 --> 01:16:21.620]   to something the plasma is doing.
[01:16:21.620 --> 01:16:23.020]   They have to be hard-coded.
[01:16:23.020 --> 01:16:26.020]   And again, knowing that that's normally our go-to solution
[01:16:26.020 --> 01:16:27.940]   is we would like to learn that instead.
[01:16:27.940 --> 01:16:30.300]   And they also had a simulator of these plasma.
[01:16:30.300 --> 01:16:31.460]   So there were lots of criteria
[01:16:31.460 --> 01:16:34.740]   that matched what we like to use.
[01:16:34.740 --> 01:16:38.420]   - So, can AI eventually solve nuclear fusion?
[01:16:38.420 --> 01:16:39.740]   - Well, so we, with this problem,
[01:16:39.740 --> 01:16:42.020]   and we published it in a Nature paper last year,
[01:16:42.020 --> 01:16:43.820]   we held the fusion,
[01:16:43.820 --> 01:16:46.180]   we held the plasma in a specific shapes.
[01:16:46.180 --> 01:16:48.380]   So actually, it's almost like carving the plasma
[01:16:48.380 --> 01:16:51.020]   into different shapes and hold it there
[01:16:51.020 --> 01:16:52.860]   for the record amount of time.
[01:16:52.860 --> 01:16:57.580]   So, that's one of the problems of fusion sort of solved.
[01:16:57.580 --> 01:16:59.820]   - So, have a controller that's able to,
[01:16:59.820 --> 01:17:01.460]   no matter the shape--
[01:17:01.460 --> 01:17:02.300]   - Contain it.
[01:17:02.300 --> 01:17:03.180]   - Contain it. - Yeah, contain it
[01:17:03.180 --> 01:17:04.180]   and hold it in structure.
[01:17:04.180 --> 01:17:05.780]   And there's different shapes that are better
[01:17:05.780 --> 01:17:10.060]   for the energy productions called droplets and so on.
[01:17:10.060 --> 01:17:11.860]   So, that was huge.
[01:17:11.860 --> 01:17:12.740]   And now we're looking,
[01:17:12.740 --> 01:17:14.420]   we're talking to lots of fusion startups
[01:17:14.420 --> 01:17:17.420]   to see what's the next problem we can tackle
[01:17:17.420 --> 01:17:19.380]   in the fusion area.
[01:17:19.380 --> 01:17:23.020]   - So, another fascinating place in a paper titled,
[01:17:23.020 --> 01:17:25.100]   "Pushing the Frontiers of Density Functionals
[01:17:25.100 --> 01:17:27.540]   by Solving the Fractional Electron Problem."
[01:17:27.540 --> 01:17:30.900]   So, you're taking on modeling and simulating
[01:17:30.900 --> 01:17:33.340]   the quantum mechanical behavior of electrons.
[01:17:33.340 --> 01:17:34.180]   - Yes.
[01:17:34.180 --> 01:17:39.260]   - Can you explain this work and can AI model
[01:17:39.260 --> 01:17:41.580]   and simulate arbitrary quantum mechanical systems
[01:17:41.580 --> 01:17:42.420]   in the future?
[01:17:42.420 --> 01:17:44.260]   - Yeah, so this is another problem I've had my eye on
[01:17:44.260 --> 01:17:47.180]   for a decade or more,
[01:17:47.180 --> 01:17:51.220]   which is sort of simulating the properties of electrons.
[01:17:51.220 --> 01:17:54.300]   If you can do that, you can basically describe
[01:17:54.300 --> 01:17:58.060]   how elements and materials and substances work.
[01:17:58.060 --> 01:18:00.020]   So, it's kind of like fundamental
[01:18:00.020 --> 01:18:02.860]   if you wanna advance material science.
[01:18:02.860 --> 01:18:05.260]   And we have Schrodinger's equation
[01:18:05.260 --> 01:18:06.500]   and then we have approximations
[01:18:06.500 --> 01:18:08.420]   to that density functional theory.
[01:18:08.420 --> 01:18:10.580]   These things are famous.
[01:18:10.580 --> 01:18:13.220]   And people try and write approximations
[01:18:13.220 --> 01:18:18.220]   to these functionals and kind of come up with descriptions
[01:18:18.220 --> 01:18:20.620]   of the electron clouds, where they're gonna go,
[01:18:20.620 --> 01:18:22.100]   how they're gonna interact
[01:18:22.100 --> 01:18:24.220]   when you put two elements together.
[01:18:24.220 --> 01:18:26.780]   And what we try to do is learn a simulation,
[01:18:26.780 --> 01:18:30.540]   learn a functional that will describe more chemistry,
[01:18:30.540 --> 01:18:31.780]   types of chemistry.
[01:18:31.780 --> 01:18:35.580]   So, until now, you can run expensive simulations,
[01:18:35.580 --> 01:18:38.780]   but then you can only simulate very small molecules,
[01:18:38.780 --> 01:18:40.180]   very simple molecules.
[01:18:40.180 --> 01:18:43.060]   We would like to simulate large materials.
[01:18:43.060 --> 01:18:45.780]   And so, today there's no way of doing that.
[01:18:45.780 --> 01:18:48.580]   And we're building up towards building functionals
[01:18:48.580 --> 01:18:51.220]   that approximate Schrodinger's equation
[01:18:51.220 --> 01:18:55.580]   and then allow you to describe what the electrons are doing.
[01:18:55.580 --> 01:18:58.420]   And all material sort of science and material properties
[01:18:58.420 --> 01:19:01.340]   are governed by the electrons and how they interact.
[01:19:01.340 --> 01:19:05.820]   - So, have a good summarization of the simulation
[01:19:05.820 --> 01:19:07.100]   through the functional,
[01:19:07.100 --> 01:19:11.340]   but one that is still close
[01:19:11.340 --> 01:19:13.180]   to what the actual simulation would come out with.
[01:19:13.180 --> 01:19:16.660]   So, how difficult is that task?
[01:19:16.660 --> 01:19:17.700]   What's involved in that task?
[01:19:17.700 --> 01:19:20.700]   Is it running those complicated simulations
[01:19:20.700 --> 01:19:23.260]   and learning the task of mapping
[01:19:23.260 --> 01:19:25.180]   from the initial conditions and the parameters
[01:19:25.180 --> 01:19:27.660]   of the simulation, learning what the functional would be?
[01:19:27.660 --> 01:19:29.380]   - Yeah, so it's pretty tricky.
[01:19:29.380 --> 01:19:31.260]   And we've done it with, you know,
[01:19:31.260 --> 01:19:35.100]   the nice thing is we can run a lot of the simulations,
[01:19:35.100 --> 01:19:39.060]   the molecular dynamic simulations on our compute clusters.
[01:19:39.060 --> 01:19:40.780]   And so, that generates a lot of data.
[01:19:40.780 --> 01:19:42.740]   So, in this case, the data is generated.
[01:19:42.740 --> 01:19:44.700]   So, we like those sort of systems.
[01:19:44.700 --> 01:19:48.020]   And that's why we use games, it's simulator generated data.
[01:19:48.020 --> 01:19:51.060]   And we can kind of create as much of it as we want really.
[01:19:51.060 --> 01:19:53.220]   And just let's leave some, you know,
[01:19:53.220 --> 01:19:55.180]   if any computers are free in the cloud,
[01:19:55.180 --> 01:19:57.580]   we just run some of these calculations, right?
[01:19:57.580 --> 01:19:59.260]   Compute cluster calculations.
[01:19:59.260 --> 01:20:00.980]   - I like how the free compute time
[01:20:00.980 --> 01:20:02.140]   is used up on quantum mechanics.
[01:20:02.140 --> 01:20:03.460]   - Yeah, quantum mechanics, exactly.
[01:20:03.460 --> 01:20:06.180]   Simulations and protein simulations and other things.
[01:20:06.180 --> 01:20:09.780]   And so, you know, when you're not searching on YouTube
[01:20:09.780 --> 01:20:11.260]   for video, cat videos,
[01:20:11.260 --> 01:20:13.900]   we're using those computers usefully in quantum chemistry.
[01:20:13.900 --> 01:20:14.740]   It's the idea.
[01:20:14.740 --> 01:20:15.580]   - Finally, it's been used for good.
[01:20:15.580 --> 01:20:16.940]   - And putting them to good use.
[01:20:16.940 --> 01:20:19.700]   And then, yeah, and then all of that computational data
[01:20:19.700 --> 01:20:20.780]   that's generated,
[01:20:20.780 --> 01:20:23.420]   we can then try and learn the functionals from that,
[01:20:23.420 --> 01:20:25.620]   which of course are way more efficient
[01:20:25.620 --> 01:20:27.020]   once we learn the functional
[01:20:27.020 --> 01:20:30.540]   than running those simulations would be.
[01:20:30.540 --> 01:20:34.100]   - Do you think one day AI may allow us to do something like
[01:20:34.100 --> 01:20:36.340]   basically crack open physics?
[01:20:36.340 --> 01:20:39.460]   So, do something like travel faster than the speed of light?
[01:20:39.460 --> 01:20:42.940]   - My ultimate aim has always been with AI is,
[01:20:42.940 --> 01:20:46.260]   the reason I am personally working on AI for my whole life,
[01:20:46.260 --> 01:20:50.300]   it was to build a tool to help us understand the universe.
[01:20:50.300 --> 01:20:53.740]   So I wanted to, and that means physics really,
[01:20:53.740 --> 01:20:54.860]   and the nature of reality.
[01:20:54.860 --> 01:20:57.980]   So I don't think we have systems
[01:20:57.980 --> 01:20:59.340]   that are capable of doing that yet,
[01:20:59.340 --> 01:21:00.940]   but when we get towards AGI,
[01:21:00.940 --> 01:21:02.860]   I think that's one of the first things
[01:21:02.860 --> 01:21:05.260]   I think we should apply AGI to.
[01:21:05.260 --> 01:21:07.100]   I would like to test the limits of physics
[01:21:07.100 --> 01:21:08.540]   and our knowledge of physics.
[01:21:08.540 --> 01:21:10.020]   There's so many things we don't know.
[01:21:10.020 --> 01:21:12.300]   This is one thing I find fascinating about science,
[01:21:12.300 --> 01:21:15.060]   and as a huge proponent of the scientific method
[01:21:15.060 --> 01:21:17.860]   as being one of the greatest ideas humanity's ever had
[01:21:17.860 --> 01:21:20.140]   and allowed us to progress with our knowledge.
[01:21:20.140 --> 01:21:21.980]   But I think as a true scientist,
[01:21:21.980 --> 01:21:25.180]   I think what you find is the more you find out,
[01:21:25.180 --> 01:21:27.020]   the more you realize we don't know.
[01:21:27.020 --> 01:21:29.860]   And I always think that it's surprising
[01:21:29.860 --> 01:21:31.860]   that more people aren't troubled.
[01:21:31.860 --> 01:21:33.980]   Every night I think about all these things
[01:21:33.980 --> 01:21:35.220]   we interact with all the time,
[01:21:35.220 --> 01:21:36.860]   that we have no idea how they work.
[01:21:36.860 --> 01:21:40.900]   Time, consciousness, gravity, life.
[01:21:40.900 --> 01:21:41.740]   We can't, I mean,
[01:21:41.740 --> 01:21:43.860]   these are all the fundamental things of nature.
[01:21:43.860 --> 01:21:44.700]   - I think the way we--
[01:21:44.700 --> 01:21:47.340]   - We don't really know what they are.
[01:21:47.340 --> 01:21:51.500]   - To live life, we pin certain assumptions on them
[01:21:51.500 --> 01:21:54.780]   and kind of treat our assumptions as if they're fact.
[01:21:54.780 --> 01:21:55.620]   - Yeah.
[01:21:55.620 --> 01:21:56.580]   - That allows us to sort of--
[01:21:56.580 --> 01:21:57.580]   - Box them off somehow.
[01:21:57.580 --> 01:21:59.020]   - Yeah, box them off somehow.
[01:21:59.020 --> 01:22:02.340]   But the reality is when you think of time,
[01:22:02.340 --> 01:22:03.540]   you should remind yourself,
[01:22:03.540 --> 01:22:06.780]   you should take it off the shelf
[01:22:06.780 --> 01:22:09.020]   and realize like, no, we have a bunch of assumptions.
[01:22:09.020 --> 01:22:10.100]   There's still a lot of,
[01:22:10.100 --> 01:22:11.540]   there's even now a lot of debate.
[01:22:11.540 --> 01:22:15.540]   There's a lot of uncertainty about exactly what is time.
[01:22:15.540 --> 01:22:17.500]   Is there an error of time?
[01:22:17.500 --> 01:22:19.500]   You know, there's a lot of fundamental questions
[01:22:19.500 --> 01:22:21.180]   that you can't just make assumptions about.
[01:22:21.180 --> 01:22:26.180]   And maybe AI allows you to not put anything on the shelf.
[01:22:26.180 --> 01:22:28.540]   - Yeah.
[01:22:28.540 --> 01:22:30.220]   - Not make any hard assumptions
[01:22:30.220 --> 01:22:32.060]   and really open it up and see what--
[01:22:32.060 --> 01:22:34.660]   - Exactly, I think we should be truly open-minded about that
[01:22:34.660 --> 01:22:39.060]   and exactly that, not be dogmatic to a particular theory.
[01:22:39.060 --> 01:22:42.860]   It'll also allow us to build better tools,
[01:22:42.860 --> 01:22:45.420]   experimental tools eventually,
[01:22:45.420 --> 01:22:47.300]   that can then test certain theories
[01:22:47.300 --> 01:22:49.020]   that may not be testable today.
[01:22:49.020 --> 01:22:51.780]   About things about like what we spoke about
[01:22:51.780 --> 01:22:53.500]   at the beginning about the computational nature
[01:22:53.500 --> 01:22:56.260]   of the universe, how one might, if that was true,
[01:22:56.260 --> 01:22:58.340]   how one might go about testing that, right?
[01:22:58.340 --> 01:23:00.740]   And how much, you know, there are people
[01:23:00.740 --> 01:23:03.180]   who've conjectured people like Scott Aronson
[01:23:03.180 --> 01:23:05.740]   and others about, you know, how much information
[01:23:05.740 --> 01:23:10.140]   can a specific Planck unit of space and time contain, right?
[01:23:10.140 --> 01:23:13.020]   So one might be able to think about testing those ideas
[01:23:13.020 --> 01:23:16.460]   if you had AI helping you build
[01:23:16.460 --> 01:23:20.500]   some new exquisite experimental tools.
[01:23:20.500 --> 01:23:21.980]   This is what I imagine, you know,
[01:23:21.980 --> 01:23:24.140]   many decades from now we'll be able to do.
[01:23:24.140 --> 01:23:26.860]   - And what kind of questions can be answered
[01:23:26.860 --> 01:23:29.740]   to running a simulation of them?
[01:23:29.740 --> 01:23:31.820]   So there's a bunch of physics simulations
[01:23:31.820 --> 01:23:33.660]   you can imagine that could be run
[01:23:33.660 --> 01:23:36.700]   in some kind of efficient way,
[01:23:36.700 --> 01:23:39.540]   much like you're doing in the quantum simulation work.
[01:23:39.540 --> 01:23:43.020]   And perhaps even the origin of life.
[01:23:43.020 --> 01:23:46.020]   So figuring out how going even back
[01:23:46.020 --> 01:23:48.580]   before the work of AlphaFold begins,
[01:23:48.580 --> 01:23:53.380]   of how this whole thing emerges from a rock.
[01:23:53.380 --> 01:23:54.220]   - Yes.
[01:23:54.220 --> 01:23:55.060]   - From a static thing.
[01:23:55.060 --> 01:23:57.860]   What do you, do you think AI will allow us to,
[01:23:57.860 --> 01:23:59.740]   is that something you have your eye on?
[01:23:59.740 --> 01:24:02.340]   It's trying to understand the origin of life.
[01:24:02.340 --> 01:24:04.620]   First of all, yourself, what do you think,
[01:24:04.620 --> 01:24:08.780]   how the heck did life originate on Earth?
[01:24:08.780 --> 01:24:11.140]   - Yeah, well, maybe I'll come to that in a second,
[01:24:11.140 --> 01:24:13.820]   but I think the ultimate use of AI
[01:24:13.820 --> 01:24:18.100]   is to kind of use it to accelerate science to the maximum.
[01:24:18.100 --> 01:24:22.620]   So I think of it a little bit like the tree of all knowledge.
[01:24:22.620 --> 01:24:23.860]   If you imagine that's all the knowledge
[01:24:23.860 --> 01:24:25.820]   there is in the universe to attain.
[01:24:25.820 --> 01:24:29.340]   And we sort of barely scratched the surface of that so far
[01:24:29.340 --> 01:24:31.980]   and even though we've done pretty well
[01:24:31.980 --> 01:24:34.340]   since the enlightenment, right, as humanity.
[01:24:34.340 --> 01:24:36.860]   And I think AI will turbocharge all of that
[01:24:36.860 --> 01:24:38.620]   like we've seen with AlphaFold.
[01:24:38.620 --> 01:24:41.420]   And I want to explore as much of that tree of knowledge
[01:24:41.420 --> 01:24:42.940]   as is possible to do.
[01:24:42.940 --> 01:24:46.420]   And I think that involves AI helping us
[01:24:46.420 --> 01:24:49.660]   with understanding or finding patterns,
[01:24:49.660 --> 01:24:52.220]   but also potentially designing and building new tools,
[01:24:52.220 --> 01:24:53.580]   experimental tools.
[01:24:53.580 --> 01:24:54.840]   So I think that's all,
[01:24:54.840 --> 01:24:58.900]   and also running simulations and learning simulations,
[01:24:58.900 --> 01:25:00.220]   all of that we're already,
[01:25:00.220 --> 01:25:04.980]   we're sort of doing at a baby steps level here.
[01:25:04.980 --> 01:25:08.540]   But I can imagine that in the decades to come
[01:25:08.540 --> 01:25:12.940]   as what's the full flourishing of that line of thinking.
[01:25:12.940 --> 01:25:15.140]   It's gonna be truly incredible, I would say.
[01:25:15.140 --> 01:25:17.340]   - If I visualize this tree of knowledge,
[01:25:17.340 --> 01:25:19.580]   something tells me that that knowledge,
[01:25:19.580 --> 01:25:21.980]   tree of knowledge for humans is much smaller.
[01:25:21.980 --> 01:25:25.060]   In the set of all possible trees of knowledge,
[01:25:25.060 --> 01:25:26.620]   it's actually quite small,
[01:25:26.620 --> 01:25:30.340]   given our cognitive limitations,
[01:25:30.340 --> 01:25:33.680]   limited cognitive capabilities,
[01:25:33.680 --> 01:25:35.740]   that even with the tools we build,
[01:25:35.740 --> 01:25:38.100]   we still won't be able to understand a lot of things.
[01:25:38.100 --> 01:25:41.140]   And that's perhaps what non-human systems
[01:25:41.140 --> 01:25:44.820]   might be able to reach farther, not just as tools,
[01:25:44.820 --> 01:25:47.180]   but in themselves understanding something
[01:25:47.180 --> 01:25:48.500]   that they can bring back.
[01:25:48.500 --> 01:25:50.180]   - Yeah, it could well be.
[01:25:50.180 --> 01:25:51.780]   So, I mean, there's so many things
[01:25:51.780 --> 01:25:55.020]   that are sort of encapsulated in what you just said there.
[01:25:55.020 --> 01:25:58.340]   I think, first of all, there's two different things.
[01:25:58.340 --> 01:26:00.580]   There's like, what do we understand today?
[01:26:00.580 --> 01:26:02.680]   What could the human mind understand?
[01:26:02.680 --> 01:26:03.920]   And what is the totality
[01:26:03.920 --> 01:26:06.420]   of what is there to be understood?
[01:26:06.420 --> 01:26:08.620]   And so there's three concentric,
[01:26:08.620 --> 01:26:10.740]   you can think of them as three larger and larger trees
[01:26:10.740 --> 01:26:12.900]   or exploring more branches of that tree.
[01:26:12.900 --> 01:26:15.940]   And I think with AI, we're gonna explore that whole lot.
[01:26:15.940 --> 01:26:19.140]   Now, the question is, if you think about
[01:26:19.140 --> 01:26:21.860]   what is the totality of what could be understood,
[01:26:21.860 --> 01:26:24.820]   there may be some fundamental physics reasons
[01:26:24.820 --> 01:26:26.340]   why certain things can't be understood,
[01:26:26.340 --> 01:26:29.040]   like what's outside a simulation or outside the universe.
[01:26:29.040 --> 01:26:32.360]   Maybe it's not understandable from within the universe.
[01:26:32.360 --> 01:26:34.900]   So there may be some hard constraints like that.
[01:26:34.900 --> 01:26:36.020]   - Could be smaller constraints,
[01:26:36.020 --> 01:26:40.540]   like we think of space-time as fundamental.
[01:26:40.540 --> 01:26:42.900]   Our human brains are really used to this idea
[01:26:42.900 --> 01:26:44.700]   of a three-dimensional world with time.
[01:26:44.700 --> 01:26:46.100]   - Right. - Maybe--
[01:26:46.100 --> 01:26:47.820]   - But our tools could go beyond that.
[01:26:47.820 --> 01:26:49.780]   They wouldn't have that limitation necessary.
[01:26:49.780 --> 01:26:51.820]   They could think in 11 dimensions, 12 dimensions,
[01:26:51.820 --> 01:26:52.980]   whatever is needed.
[01:26:52.980 --> 01:26:55.700]   But we could still maybe understand that
[01:26:55.700 --> 01:26:56.780]   in several different ways.
[01:26:56.780 --> 01:26:59.100]   The example I always give is,
[01:26:59.100 --> 01:27:01.460]   when I play Garry Kasparov at Speed Chess
[01:27:01.460 --> 01:27:04.160]   or we've talked about chess and these kinds of things,
[01:27:04.160 --> 01:27:07.580]   if you're reasonably good at chess,
[01:27:07.580 --> 01:27:11.260]   you can't come up with the move Garry comes up with
[01:27:11.260 --> 01:27:13.380]   in his move, but he can explain it to you.
[01:27:13.380 --> 01:27:14.220]   - And you can understand.
[01:27:14.220 --> 01:27:16.740]   - And you can understand post hoc the reasoning.
[01:27:16.740 --> 01:27:19.460]   So I think there's an even further level of like,
[01:27:19.460 --> 01:27:21.700]   well, maybe you couldn't have invented that thing,
[01:27:21.700 --> 01:27:24.340]   but going back to using language again,
[01:27:24.340 --> 01:27:27.100]   perhaps you can understand and appreciate that.
[01:27:27.100 --> 01:27:30.220]   Same way that you can appreciate Vivaldi or Mozart
[01:27:30.220 --> 01:27:32.740]   or something without, you can appreciate the beauty of that
[01:27:32.740 --> 01:27:35.860]   without being able to construct it yourself, right?
[01:27:35.860 --> 01:27:37.460]   Invent the music yourself.
[01:27:37.460 --> 01:27:39.340]   So I think we see this in all forms of life.
[01:27:39.340 --> 01:27:42.480]   So it will be that times a million,
[01:27:42.480 --> 01:27:45.860]   but you can imagine also one sign of intelligence
[01:27:45.860 --> 01:27:49.540]   is the ability to explain things clearly and simply, right?
[01:27:49.540 --> 01:27:50.460]   People like Richard Feynman,
[01:27:50.460 --> 01:27:52.420]   another one of my old time heroes used to say that, right?
[01:27:52.420 --> 01:27:55.620]   If you can't, if you can explain it something simply,
[01:27:55.620 --> 01:27:58.640]   then that's the best sign, a complex topic simply,
[01:27:58.640 --> 01:28:00.680]   then that's one of the best signs of you understanding it.
[01:28:00.680 --> 01:28:01.520]   - Yeah.
[01:28:01.520 --> 01:28:04.460]   I can see myself talking trash in the AI system in that way.
[01:28:04.460 --> 01:28:07.780]   It gets frustrated how dumb I am
[01:28:07.780 --> 01:28:09.900]   in trying to explain something to me.
[01:28:09.900 --> 01:28:11.580]   I was like, well, that means you're not intelligent
[01:28:11.580 --> 01:28:12.740]   because if you were intelligent,
[01:28:12.740 --> 01:28:14.420]   you'd be able to explain it simply.
[01:28:14.420 --> 01:28:16.700]   - Yeah, of course, there's also the other option,
[01:28:16.700 --> 01:28:19.580]   of course, we could enhance ourselves and without devices.
[01:28:19.580 --> 01:28:23.140]   We are already sort of symbiotic with our compute devices,
[01:28:23.140 --> 01:28:24.640]   right, with our phones and other things.
[01:28:24.640 --> 01:28:27.140]   And there's stuff like Neuralink and Acceptra
[01:28:27.140 --> 01:28:30.020]   that could advance that further.
[01:28:30.020 --> 01:28:33.900]   So I think there's lots of really amazing possibilities
[01:28:33.900 --> 01:28:35.380]   that I could foresee from here.
[01:28:35.380 --> 01:28:37.020]   - Well, let me ask you some wild questions.
[01:28:37.020 --> 01:28:39.900]   So out there looking for friends,
[01:28:39.900 --> 01:28:43.140]   do you think there's a lot of alien civilizations out there?
[01:28:43.140 --> 01:28:44.980]   - So I guess this also goes back
[01:28:44.980 --> 01:28:46.700]   to your origin of life question too,
[01:28:46.700 --> 01:28:48.300]   because I think that that's key.
[01:28:49.400 --> 01:28:51.460]   My personal opinion looking at all this,
[01:28:51.460 --> 01:28:53.780]   and it's one of my hobbies, physics, I guess.
[01:28:53.780 --> 01:28:56.980]   So it's something I think about a lot
[01:28:56.980 --> 01:29:00.860]   and talk to a lot of experts on and read a lot of books on.
[01:29:00.860 --> 01:29:05.380]   And I think my feeling currently is that we are alone.
[01:29:05.380 --> 01:29:07.260]   I think that's the most likely scenario
[01:29:07.260 --> 01:29:08.860]   given what evidence we have.
[01:29:08.860 --> 01:29:13.860]   So, and the reasoning is I think that we've tried
[01:29:13.860 --> 01:29:16.220]   since things like SETI program,
[01:29:16.220 --> 01:29:19.960]   and I guess since the dawning of the space age,
[01:29:19.960 --> 01:29:22.400]   we've had telescopes, open radio telescopes
[01:29:22.400 --> 01:29:23.400]   and other things.
[01:29:23.400 --> 01:29:27.360]   And if you think about and try to detect signals.
[01:29:27.360 --> 01:29:30.220]   Now, if you think about the evolution of humans on earth,
[01:29:30.220 --> 01:29:34.040]   we could have easily been a million years ahead
[01:29:34.040 --> 01:29:36.560]   of our time now or a million years behind, right?
[01:29:36.560 --> 01:29:39.520]   Easily with just some slightly different quirk
[01:29:39.520 --> 01:29:42.480]   thing happening hundreds of thousands of years ago,
[01:29:42.480 --> 01:29:43.740]   things could have been slightly different.
[01:29:43.740 --> 01:29:46.280]   If the meteor had hit the dinosaurs a million years earlier,
[01:29:46.280 --> 01:29:48.200]   maybe things would have evolved.
[01:29:48.200 --> 01:29:51.000]   We'd be a million years ahead of where we are now.
[01:29:51.000 --> 01:29:54.160]   So what that means is if you imagine where humanity will be
[01:29:54.160 --> 01:29:56.800]   in a few hundred years, let alone a million years,
[01:29:56.800 --> 01:29:59.920]   especially if we hopefully, you know,
[01:29:59.920 --> 01:30:02.280]   solve things like climate change and other things,
[01:30:02.280 --> 01:30:05.720]   and we continue to flourish and we build things like AI
[01:30:05.720 --> 01:30:08.000]   and we do space traveling and all of the stuff
[01:30:08.000 --> 01:30:10.880]   that humans have dreamed of forever, right?
[01:30:10.880 --> 01:30:12.680]   And sci-fi is talked about forever.
[01:30:13.340 --> 01:30:16.820]   We will be spreading across the stars, right?
[01:30:16.820 --> 01:30:19.300]   And Voigt-Neumann famously calculated, you know,
[01:30:19.300 --> 01:30:20.860]   it would only take about a million years
[01:30:20.860 --> 01:30:23.300]   if you send out Voigt-Neumann probes to the nearest,
[01:30:23.300 --> 01:30:26.240]   you know, the nearest other solar systems.
[01:30:26.240 --> 01:30:28.380]   And then they built, all they did was build
[01:30:28.380 --> 01:30:30.500]   two more versions of themselves and set those two out
[01:30:30.500 --> 01:30:32.300]   to the next nearest systems.
[01:30:32.300 --> 01:30:33.500]   You know, within a million years,
[01:30:33.500 --> 01:30:35.100]   I think you would have one of these probes
[01:30:35.100 --> 01:30:36.940]   in every system in the galaxy.
[01:30:36.940 --> 01:30:40.080]   So it's not actually in cosmological time.
[01:30:40.080 --> 01:30:42.120]   That's actually a very short amount of time.
[01:30:42.120 --> 01:30:44.340]   So, and, you know, we've, people like Dyson
[01:30:44.340 --> 01:30:47.320]   have thought about constructing Dyson spheres around stars
[01:30:47.320 --> 01:30:49.840]   to collect all the energy coming out of the star.
[01:30:49.840 --> 01:30:51.820]   You know, that, there would be constructions like that
[01:30:51.820 --> 01:30:54.180]   would be visible across space,
[01:30:54.180 --> 01:30:56.020]   probably even across a galaxy.
[01:30:56.020 --> 01:30:57.940]   So, and then, you know, if you think about
[01:30:57.940 --> 01:31:00.780]   all of our radio, television emissions
[01:31:00.780 --> 01:31:04.220]   that have gone out since the, you know, 30s and 40s,
[01:31:04.220 --> 01:31:06.780]   imagine a million years of that.
[01:31:06.780 --> 01:31:10.020]   And now hundreds of civilizations doing that.
[01:31:10.020 --> 01:31:12.240]   When we opened our ears at the point
[01:31:12.240 --> 01:31:15.920]   we got technologically sophisticated enough in the space age,
[01:31:15.920 --> 01:31:19.160]   we should have heard a cacophony of voices.
[01:31:19.160 --> 01:31:20.920]   We should have joined that cacophony of voices.
[01:31:20.920 --> 01:31:24.520]   And what we did, we opened our ears and we heard nothing.
[01:31:24.520 --> 01:31:27.560]   And many people who argue that there are aliens would say,
[01:31:27.560 --> 01:31:29.960]   well, we haven't really done exhaustive search yet.
[01:31:29.960 --> 01:31:31.920]   And maybe we're looking in the wrong bands
[01:31:31.920 --> 01:31:33.800]   and we've got the wrong devices
[01:31:33.800 --> 01:31:36.120]   and we wouldn't notice what an alien form was like
[01:31:36.120 --> 01:31:38.280]   'cause it'd be so different to what we're used to.
[01:31:38.280 --> 01:31:40.660]   But, you know, I don't really buy that,
[01:31:40.660 --> 01:31:42.620]   that it shouldn't be as difficult as that.
[01:31:42.620 --> 01:31:44.340]   Like, I think we've searched enough.
[01:31:44.340 --> 01:31:45.660]   - It should be everywhere.
[01:31:45.660 --> 01:31:47.300]   - If it was, yeah, it should be everywhere.
[01:31:47.300 --> 01:31:49.220]   We should see Dyson spheres being put up,
[01:31:49.220 --> 01:31:50.620]   suns blinking in and out.
[01:31:50.620 --> 01:31:52.900]   You know, there should be a lot of evidence for those things.
[01:31:52.900 --> 01:31:54.180]   And then there are other people who argue,
[01:31:54.180 --> 01:31:55.980]   well, the sort of safari view of like,
[01:31:55.980 --> 01:31:57.820]   well, we're a primitive species still
[01:31:57.820 --> 01:31:59.400]   'cause we're not space faring yet.
[01:31:59.400 --> 01:32:01.420]   And we're, you know, there's some kind of global,
[01:32:01.420 --> 01:32:03.340]   like universal rule not to interfere,
[01:32:03.340 --> 01:32:04.580]   you know, Star Trek rule.
[01:32:04.580 --> 01:32:07.340]   But like, look, we can't even coordinate humans
[01:32:07.340 --> 01:32:08.660]   to deal with climate change.
[01:32:08.660 --> 01:32:10.060]   And we're one species.
[01:32:10.060 --> 01:32:12.420]   What is the chance that of all of these different
[01:32:12.420 --> 01:32:14.800]   human civilization, you know, alien civilizations,
[01:32:14.800 --> 01:32:16.740]   they would have the same priorities
[01:32:16.740 --> 01:32:20.180]   and agree across, you know, these kinds of matters.
[01:32:20.180 --> 01:32:21.860]   And even if that was true,
[01:32:21.860 --> 01:32:25.020]   and we were in some sort of safari for our own good,
[01:32:25.020 --> 01:32:26.340]   to me, that's not much different
[01:32:26.340 --> 01:32:27.620]   from the simulation hypothesis.
[01:32:27.620 --> 01:32:29.860]   Because what does it mean, the simulation hypothesis?
[01:32:29.860 --> 01:32:31.340]   I think in its most fundamental level,
[01:32:31.340 --> 01:32:34.940]   it means what we're seeing is not quite reality, right?
[01:32:34.940 --> 01:32:37.740]   It's something, there's something more deeper underlying it,
[01:32:37.740 --> 01:32:39.060]   maybe computational.
[01:32:39.060 --> 01:32:42.540]   Now, if we were in a sort of safari park,
[01:32:42.540 --> 01:32:44.420]   and everything we were seeing was a hologram,
[01:32:44.420 --> 01:32:46.460]   and it was projected by the aliens or whatever,
[01:32:46.460 --> 01:32:48.660]   that to me is not much different than thinking
[01:32:48.660 --> 01:32:50.220]   we're inside of another universe.
[01:32:50.220 --> 01:32:53.100]   'Cause we still can't see true reality, right?
[01:32:53.100 --> 01:32:55.060]   - I mean, there's other explanations.
[01:32:55.060 --> 01:32:57.980]   It could be that the way they're communicating
[01:32:57.980 --> 01:32:59.260]   is just fundamentally different.
[01:32:59.260 --> 01:33:01.180]   That we're too dumb to understand
[01:33:01.180 --> 01:33:03.860]   the much better methods of communication they have.
[01:33:03.860 --> 01:33:06.580]   It could be, I mean, it's silly to say,
[01:33:06.580 --> 01:33:09.940]   but our own thoughts could be the methods
[01:33:09.940 --> 01:33:11.180]   by which they're communicating.
[01:33:11.180 --> 01:33:13.220]   Like, the place from which our ideas,
[01:33:13.220 --> 01:33:15.180]   writers talk about this, like the muse.
[01:33:15.180 --> 01:33:16.020]   - Yeah.
[01:33:16.020 --> 01:33:20.900]   - The, I mean, it sounds like very kind of wild,
[01:33:20.900 --> 01:33:22.140]   but it could be thoughts,
[01:33:22.140 --> 01:33:24.580]   it could be some interactions with our mind
[01:33:24.580 --> 01:33:27.820]   that we think are originating from us
[01:33:27.820 --> 01:33:31.420]   is actually something that is coming
[01:33:31.420 --> 01:33:33.020]   from other life forms elsewhere.
[01:33:33.020 --> 01:33:34.860]   Consciousness itself might be that.
[01:33:34.860 --> 01:33:37.340]   - It could be, but I don't see any sensible argument
[01:33:37.340 --> 01:33:41.580]   to the why would all of the alien species behave this way?
[01:33:41.580 --> 01:33:43.180]   Yeah, some of them would be more primitive,
[01:33:43.180 --> 01:33:44.900]   they would be close to our level.
[01:33:44.900 --> 01:33:46.700]   You know, there should be a whole sort of
[01:33:46.700 --> 01:33:48.700]   normal distribution of these things, right?
[01:33:48.700 --> 01:33:50.940]   Some would be aggressive, some would be, you know,
[01:33:50.940 --> 01:33:55.380]   curious, others would be very historical and philosophical.
[01:33:55.380 --> 01:33:57.140]   Because, you know, maybe they're a million years
[01:33:57.140 --> 01:34:00.140]   older than us, but it's not, it shouldn't be like,
[01:34:00.140 --> 01:34:02.980]   I mean, one alien civilization might be like that,
[01:34:02.980 --> 01:34:04.180]   communicating thoughts and others,
[01:34:04.180 --> 01:34:05.780]   but I don't see why, you know,
[01:34:05.780 --> 01:34:07.700]   potentially the hundreds there should be
[01:34:07.700 --> 01:34:10.020]   would be uniform in this way, right?
[01:34:10.020 --> 01:34:13.020]   - It could be a violent dictatorship that the people,
[01:34:13.020 --> 01:34:17.620]   the alien civilizations that become successful,
[01:34:17.620 --> 01:34:22.620]   become, gain the ability to be destructive,
[01:34:22.620 --> 01:34:25.020]   an order of magnitude more destructive.
[01:34:25.020 --> 01:34:28.720]   But of course the sad thought,
[01:34:29.860 --> 01:34:32.660]   well, either humans are very special,
[01:34:32.660 --> 01:34:35.500]   we took a lot of leaps that arrived
[01:34:35.500 --> 01:34:36.940]   at what it means to be human.
[01:34:36.940 --> 01:34:41.140]   There's a question there, which was the hardest,
[01:34:41.140 --> 01:34:42.740]   which was the most special?
[01:34:42.740 --> 01:34:45.220]   But also if others have reached this level,
[01:34:45.220 --> 01:34:47.680]   and maybe many others have reached this level,
[01:34:47.680 --> 01:34:52.680]   the great filter that's prevented them from going farther,
[01:34:52.680 --> 01:34:55.060]   to becoming a multi-planetary species,
[01:34:55.060 --> 01:34:57.740]   or reaching out into the stars.
[01:34:57.740 --> 01:35:00.260]   And those are really important questions for us,
[01:35:00.260 --> 01:35:04.980]   whether there's other alien civilizations out there or not,
[01:35:04.980 --> 01:35:07.260]   this is very useful for us to think about.
[01:35:07.260 --> 01:35:10.500]   If we destroy ourselves, how will we do it?
[01:35:10.500 --> 01:35:12.260]   And how easy is it to do?
[01:35:12.260 --> 01:35:14.420]   - Yeah, well, you know, these are big questions,
[01:35:14.420 --> 01:35:15.620]   and I've thought about these a lot,
[01:35:15.620 --> 01:35:19.860]   but the interesting thing is that if we're alone,
[01:35:19.860 --> 01:35:22.280]   that's somewhat comforting from the great filter perspective,
[01:35:22.280 --> 01:35:25.540]   because it probably means the great filters are past us,
[01:35:25.540 --> 01:35:26.540]   and I'm pretty sure they are.
[01:35:26.540 --> 01:35:29.340]   So going back to your origin of life question,
[01:35:29.340 --> 01:35:30.860]   there are some incredible things
[01:35:30.860 --> 01:35:32.060]   that no one knows how happened.
[01:35:32.060 --> 01:35:35.540]   Like obviously the first life form from chemical soup,
[01:35:35.540 --> 01:35:37.100]   that seems pretty hard.
[01:35:37.100 --> 01:35:39.020]   But I would guess the multicellular,
[01:35:39.020 --> 01:35:42.460]   I wouldn't be that surprised if we saw single cell
[01:35:42.460 --> 01:35:45.740]   sort of life forms elsewhere, bacteria type things.
[01:35:45.740 --> 01:35:48.300]   But multicellular life seems incredibly hard,
[01:35:48.300 --> 01:35:50.500]   that step of capturing mitochondria,
[01:35:50.500 --> 01:35:53.420]   and then sort of using that as part of yourself,
[01:35:53.420 --> 01:35:54.260]   when you've just eaten it.
[01:35:54.260 --> 01:35:57.820]   - Would you say that's the biggest, the most,
[01:35:57.820 --> 01:36:00.900]   like if you had to choose one,
[01:36:00.900 --> 01:36:03.180]   sort of a hitchhiker's guide to the galaxy,
[01:36:03.180 --> 01:36:04.660]   one sentence summary of like,
[01:36:04.660 --> 01:36:07.540]   oh, those clever creatures did this,
[01:36:07.540 --> 01:36:08.540]   that would be the multicellular.
[01:36:08.540 --> 01:36:10.880]   - I think that was probably the one that's the biggest.
[01:36:10.880 --> 01:36:11.860]   I mean, there's a great book called
[01:36:11.860 --> 01:36:15.060]   "The 10 Great Inventions of Evolution" by Nick Lane,
[01:36:15.060 --> 01:36:17.300]   and he speculates on 10 of these,
[01:36:17.300 --> 01:36:20.100]   you know, what could be great filters.
[01:36:20.100 --> 01:36:20.980]   I think that's one.
[01:36:20.980 --> 01:36:23.860]   I think the advent of intelligence,
[01:36:23.860 --> 01:36:25.460]   and conscious intelligence,
[01:36:25.460 --> 01:36:27.500]   and in order to us to be able to do science
[01:36:27.500 --> 01:36:29.860]   and things like that, is huge as well.
[01:36:29.860 --> 01:36:31.900]   I mean, it's only evolved once as far as,
[01:36:31.900 --> 01:36:34.820]   you know, in Earth history.
[01:36:34.820 --> 01:36:37.140]   So that would be a later candidate,
[01:36:37.140 --> 01:36:39.100]   but there's certainly for the early candidates,
[01:36:39.100 --> 01:36:41.420]   I think multicellular life forms is huge.
[01:36:41.420 --> 01:36:43.540]   - By the way, it's interesting to ask you
[01:36:43.540 --> 01:36:45.740]   if you can hypothesize about
[01:36:45.740 --> 01:36:47.960]   what is the origin of intelligence?
[01:36:47.960 --> 01:36:52.960]   Is it that we started cooking meat over fire?
[01:36:53.620 --> 01:36:55.500]   Is it that we somehow figured out
[01:36:55.500 --> 01:36:57.020]   that we could be very powerful
[01:36:57.020 --> 01:36:58.100]   and we start collaborating?
[01:36:58.100 --> 01:37:03.100]   So cooperation between our ancestors
[01:37:03.100 --> 01:37:05.920]   so that we can overthrow the alpha male?
[01:37:05.920 --> 01:37:07.740]   What is it, Richard?
[01:37:07.740 --> 01:37:08.900]   I talked to Richard Ranham,
[01:37:08.900 --> 01:37:10.740]   who thinks we're all just beta males
[01:37:10.740 --> 01:37:12.060]   who figured out how to collaborate
[01:37:12.060 --> 01:37:16.340]   to defeat the dictator, the authoritarian alpha male
[01:37:16.340 --> 01:37:19.140]   that control the tribe.
[01:37:19.140 --> 01:37:20.900]   Is there other explanation?
[01:37:20.900 --> 01:37:23.340]   Was there a 2001 space odyssey?
[01:37:23.340 --> 01:37:24.820]   - A space odyssey type of monolith
[01:37:24.820 --> 01:37:25.900]   that came down to Earth?
[01:37:25.900 --> 01:37:28.100]   - Well, I think all of those things
[01:37:28.100 --> 01:37:29.220]   you suggested are good candidates.
[01:37:29.220 --> 01:37:31.300]   Fire and cooking, right?
[01:37:31.300 --> 01:37:36.140]   So that's clearly important for energy efficiency,
[01:37:36.140 --> 01:37:38.540]   cooking our meat and then being able
[01:37:38.540 --> 01:37:41.000]   to be more efficient about eating it
[01:37:41.000 --> 01:37:42.620]   and consuming the energy.
[01:37:42.620 --> 01:37:44.340]   I think that's huge.
[01:37:44.340 --> 01:37:46.340]   And then utilizing fire and tools.
[01:37:46.340 --> 01:37:49.140]   I think you're right about the tribal cooperation aspects
[01:37:49.140 --> 01:37:51.560]   and probably language is part of that
[01:37:51.560 --> 01:37:52.900]   because probably that's what allowed us
[01:37:52.900 --> 01:37:54.180]   to out-compete Neanderthals
[01:37:54.180 --> 01:37:56.700]   and perhaps less cooperative species.
[01:37:56.700 --> 01:37:59.280]   So that may be the case.
[01:37:59.280 --> 01:38:01.660]   Tool making, spears, axes.
[01:38:01.660 --> 01:38:03.180]   I think that let us, I mean,
[01:38:03.180 --> 01:38:04.420]   I think it's pretty clear now
[01:38:04.420 --> 01:38:05.620]   that humans were responsible
[01:38:05.620 --> 01:38:08.340]   for a lot of the extinctions of megafauna,
[01:38:08.340 --> 01:38:11.360]   especially in the Americas when humans arrived.
[01:38:11.360 --> 01:38:14.940]   So you can imagine once you discover tool usage,
[01:38:14.940 --> 01:38:16.260]   how powerful that would have been
[01:38:16.260 --> 01:38:18.020]   and how scary for animals.
[01:38:18.020 --> 01:38:21.160]   So I think all of those could have been explanations for it.
[01:38:21.160 --> 01:38:22.700]   Now, the interesting thing is
[01:38:22.700 --> 01:38:24.560]   that it's a bit like general intelligence too,
[01:38:24.560 --> 01:38:28.620]   is it's very costly to begin with to have a brain
[01:38:28.620 --> 01:38:30.020]   and especially a general purpose brain
[01:38:30.020 --> 01:38:31.420]   rather than a special purpose one.
[01:38:31.420 --> 01:38:32.820]   'Cause the amount of energy our brains use,
[01:38:32.820 --> 01:38:34.860]   I think it's like 20% of the body's energy.
[01:38:34.860 --> 01:38:36.140]   And it's massive.
[01:38:36.140 --> 01:38:37.140]   And when you're thinking chess,
[01:38:37.140 --> 01:38:39.460]   one of the funny things that we used to say
[01:38:39.460 --> 01:38:42.020]   is it's as much as a racing driver uses
[01:38:42.020 --> 01:38:44.060]   for a whole Formula One race.
[01:38:44.060 --> 01:38:46.380]   Just playing a game of serious high-level chess,
[01:38:46.380 --> 01:38:49.340]   which you wouldn't think, just sitting there.
[01:38:49.340 --> 01:38:52.080]   Because the brain's using so much energy.
[01:38:52.080 --> 01:38:54.800]   So in order for an animal or an organism to justify that,
[01:38:54.800 --> 01:38:57.880]   there has to be a huge payoff.
[01:38:57.880 --> 01:39:00.320]   And the problem with half a brain
[01:39:00.320 --> 01:39:05.320]   or half intelligence, say an IQs of like a monkey brain,
[01:39:05.320 --> 01:39:10.280]   it's not clear you can justify that evolutionary
[01:39:10.280 --> 01:39:12.480]   until you get to the human level brain.
[01:39:12.480 --> 01:39:14.800]   And so, but how do you do that jump?
[01:39:14.800 --> 01:39:15.640]   It's very difficult,
[01:39:15.640 --> 01:39:17.200]   which is why I think it's only been done once
[01:39:17.200 --> 01:39:19.860]   from the sort of specialized brains that you see in animals
[01:39:19.860 --> 01:39:22.540]   to this sort of general purpose,
[01:39:22.540 --> 01:39:25.180]   cheering powerful brains that humans have.
[01:39:25.180 --> 01:39:28.980]   And which allows us to invent the modern world.
[01:39:28.980 --> 01:39:33.660]   And it takes a lot to cross that barrier.
[01:39:33.660 --> 01:39:35.660]   And I think we've seen the same with AI systems,
[01:39:35.660 --> 01:39:38.260]   which is that maybe until very recently,
[01:39:38.260 --> 01:39:40.960]   it's always been easier to craft a specific solution
[01:39:40.960 --> 01:39:42.380]   to a problem like chess
[01:39:42.380 --> 01:39:44.540]   than it has been to build a general learning system
[01:39:44.540 --> 01:39:46.340]   that could potentially do many things.
[01:39:46.340 --> 01:39:49.520]   'Cause initially, that system will be way worse
[01:39:49.520 --> 01:39:52.160]   than less efficient than the specialized system.
[01:39:52.160 --> 01:39:55.880]   - So one of the interesting quirks of the human mind
[01:39:55.880 --> 01:40:00.880]   of this evolved system is that it appears to be conscious.
[01:40:00.880 --> 01:40:02.960]   This thing that we don't quite understand,
[01:40:02.960 --> 01:40:06.600]   but it seems very special,
[01:40:06.600 --> 01:40:08.760]   is ability to have a subjective experience
[01:40:08.760 --> 01:40:12.280]   that it feels like something to eat a cookie,
[01:40:12.280 --> 01:40:14.320]   the deliciousness of it or see a color
[01:40:14.320 --> 01:40:15.560]   and that kind of stuff.
[01:40:15.560 --> 01:40:17.940]   Do you think in order to solve intelligence,
[01:40:17.940 --> 01:40:20.700]   we also need to solve consciousness along the way?
[01:40:20.700 --> 01:40:23.940]   Do you think AGI systems need to have consciousness
[01:40:23.940 --> 01:40:27.980]   in order to be truly intelligent?
[01:40:27.980 --> 01:40:29.640]   - Yeah, we thought about this a lot actually.
[01:40:29.640 --> 01:40:33.420]   And I think that my guess is that consciousness
[01:40:33.420 --> 01:40:35.800]   and intelligence are double dissociable.
[01:40:35.800 --> 01:40:38.360]   So you can have one without the other both ways.
[01:40:38.360 --> 01:40:41.600]   And I think you can see that with consciousness in that,
[01:40:41.600 --> 01:40:44.160]   I think some animals, pets,
[01:40:44.160 --> 01:40:46.280]   if you have a pet dog or something like that,
[01:40:46.280 --> 01:40:47.920]   you can see some of the higher animals
[01:40:47.920 --> 01:40:50.240]   and dolphins, things like that,
[01:40:50.240 --> 01:40:55.240]   have self-awareness and are very sociable, seem to dream.
[01:40:55.240 --> 01:40:59.040]   Those kinds of, a lot of the traits one would regard
[01:40:59.040 --> 01:41:01.660]   as being kind of conscious and self-aware.
[01:41:01.660 --> 01:41:05.120]   But yet they're not that smart, right?
[01:41:05.120 --> 01:41:08.120]   So they're not that intelligent by say IQ standards
[01:41:08.120 --> 01:41:08.960]   or something like that.
[01:41:08.960 --> 01:41:11.120]   - Yeah, it's also possible that our understanding
[01:41:11.120 --> 01:41:14.600]   of intelligence is flawed, like putting an IQ to it.
[01:41:14.600 --> 01:41:15.440]   - Sure.
[01:41:15.440 --> 01:41:17.360]   - Maybe the thing that a dog can do
[01:41:17.360 --> 01:41:20.640]   is actually gone very far along the path of intelligence
[01:41:20.640 --> 01:41:23.240]   and we humans are just able to play chess
[01:41:23.240 --> 01:41:24.840]   and maybe write poems.
[01:41:24.840 --> 01:41:27.040]   - Right, but if we go back to the idea of AGI
[01:41:27.040 --> 01:41:29.480]   and general intelligence, dogs are very specialized, right?
[01:41:29.480 --> 01:41:30.920]   Most animals are pretty specialized.
[01:41:30.920 --> 01:41:32.360]   They can be amazing at what they do,
[01:41:32.360 --> 01:41:35.600]   but they're like kind of elite sports people or something.
[01:41:35.600 --> 01:41:38.040]   Right, so they do one thing extremely well
[01:41:38.040 --> 01:41:40.120]   'cause their entire brain is optimized.
[01:41:40.120 --> 01:41:41.920]   - They have somehow convinced the entirety
[01:41:41.920 --> 01:41:44.560]   of the human population to feed them and service them.
[01:41:44.560 --> 01:41:46.440]   So in some way they're controlling.
[01:41:46.440 --> 01:41:49.360]   - Yes, exactly, well we co-evolved to some crazy degree,
[01:41:49.360 --> 01:41:52.320]   right, including the way the dogs, you know,
[01:41:52.320 --> 01:41:55.200]   even wag their tails and twitch their noses, right?
[01:41:55.200 --> 01:41:57.520]   We find it inexorably cute.
[01:41:57.520 --> 01:42:01.880]   But I think you can also see intelligence on the other side.
[01:42:01.880 --> 01:42:06.200]   So systems like artificial systems that are amazingly smart
[01:42:06.200 --> 01:42:08.720]   at certain things, like maybe playing Go and chess
[01:42:08.720 --> 01:42:11.760]   and other things, but they don't feel at all
[01:42:11.760 --> 01:42:14.480]   in any shape or form conscious in the way that,
[01:42:14.480 --> 01:42:17.280]   you know, you do to me or I do to you.
[01:42:17.280 --> 01:42:21.320]   And I think actually building AI,
[01:42:21.320 --> 01:42:25.440]   these intelligent constructs, is one of the best ways
[01:42:25.440 --> 01:42:28.040]   to explore the mystery of consciousness, to break it down.
[01:42:28.040 --> 01:42:33.040]   Because we're gonna have devices that are pretty smart
[01:42:33.040 --> 01:42:36.240]   at certain things or capable of certain things,
[01:42:36.240 --> 01:42:39.160]   but potentially won't have any semblance
[01:42:39.160 --> 01:42:40.800]   of self-awareness or other things.
[01:42:40.800 --> 01:42:43.880]   And in fact, I would advocate, if there's a choice,
[01:42:43.880 --> 01:42:46.560]   building systems in the first place, AI systems,
[01:42:46.560 --> 01:42:48.640]   that are not conscious to begin with,
[01:42:48.640 --> 01:42:52.440]   are just tools until we understand them better
[01:42:52.440 --> 01:42:53.960]   and the capabilities better.
[01:42:53.960 --> 01:42:58.320]   - So on that topic, just not as the CEO of DeepMind,
[01:42:58.320 --> 01:43:01.480]   just as a human being, let me ask you about this
[01:43:01.480 --> 01:43:05.320]   one particular anecdotal evidence of the Google engineer
[01:43:05.320 --> 01:43:09.880]   who made a comment or believed that there's some aspect
[01:43:09.880 --> 01:43:13.240]   of a language model, the Lambda language model,
[01:43:13.240 --> 01:43:15.960]   that exhibited sentience.
[01:43:15.960 --> 01:43:18.440]   So you said you believe there might be a responsibility
[01:43:18.440 --> 01:43:21.120]   to build systems that are not sentient.
[01:43:21.120 --> 01:43:23.520]   And this experience of a particular engineer,
[01:43:23.520 --> 01:43:25.880]   I think, I'd love to get your general opinion
[01:43:25.880 --> 01:43:28.000]   on this kind of thing, but I think it will happen
[01:43:28.000 --> 01:43:31.480]   more and more and more, which not when engineers,
[01:43:31.480 --> 01:43:33.120]   but when people out there that don't have
[01:43:33.120 --> 01:43:34.760]   an engineer background start interacting
[01:43:34.760 --> 01:43:37.120]   with increasingly intelligent systems,
[01:43:37.120 --> 01:43:40.600]   we anthropomorphize them, they start to have
[01:43:40.600 --> 01:43:45.000]   deep, impactful interactions with us in a way
[01:43:45.000 --> 01:43:47.920]   that we miss them when they're gone.
[01:43:47.920 --> 01:43:51.960]   And we sure as heck feel like they're living entities,
[01:43:51.960 --> 01:43:55.200]   self-aware entities, and maybe even we project sentience
[01:43:55.200 --> 01:43:56.040]   onto them.
[01:43:56.040 --> 01:43:59.960]   So what's your thought about this particular system?
[01:43:59.960 --> 01:44:04.560]   Have you ever met a language model that's sentient?
[01:44:04.560 --> 01:44:06.320]   - No, no, no.
[01:44:06.320 --> 01:44:10.160]   - What do you make of the case of when you kind of feel
[01:44:10.160 --> 01:44:12.920]   that there's some elements of sentience to this system?
[01:44:12.920 --> 01:44:15.040]   - Yeah, so this is an interesting question
[01:44:15.040 --> 01:44:17.760]   and obviously a very fundamental one.
[01:44:17.760 --> 01:44:20.760]   So first thing to say is I think that none of the systems
[01:44:20.760 --> 01:44:23.640]   we have today, I would say, even have one iota
[01:44:23.640 --> 01:44:26.320]   of semblance of consciousness or sentience.
[01:44:26.320 --> 01:44:29.720]   That's my personal feeling, interacting with them every day.
[01:44:29.720 --> 01:44:32.440]   So I think this way premature to be discussing
[01:44:32.440 --> 01:44:34.160]   what that engineer talked about.
[01:44:34.160 --> 01:44:36.480]   I think at the moment it's more of a projection
[01:44:36.480 --> 01:44:39.080]   of the way our own minds work, which is to see
[01:44:39.080 --> 01:44:44.360]   sort of purpose and direction in almost anything that we,
[01:44:44.360 --> 01:44:48.200]   you know, our brains are trained to interpret agency,
[01:44:48.200 --> 01:44:52.280]   basically, in things, even inanimate things sometimes.
[01:44:52.280 --> 01:44:54.880]   And of course, with a language system,
[01:44:54.880 --> 01:44:57.080]   'cause language is so fundamental to intelligence,
[01:44:57.080 --> 01:45:00.440]   it's gonna be easy for us to anthropomorphize that.
[01:45:00.440 --> 01:45:03.840]   I mean, back in the day, even the first, you know,
[01:45:03.840 --> 01:45:05.800]   the dumbest sort of template chatbots ever,
[01:45:05.800 --> 01:45:09.200]   Eliza and the ilk of the original chatbots
[01:45:09.200 --> 01:45:11.160]   back in the '60s fooled some people
[01:45:11.160 --> 01:45:12.600]   under certain circumstances, right?
[01:45:12.600 --> 01:45:14.040]   It pretended to be a psychologist.
[01:45:14.040 --> 01:45:16.080]   So just basically rabbit back to you
[01:45:16.080 --> 01:45:18.240]   the same question you asked it back to you.
[01:45:18.240 --> 01:45:21.280]   And some people believe that.
[01:45:21.280 --> 01:45:23.240]   So I don't think we can, this is why I think
[01:45:23.240 --> 01:45:25.400]   the Turing test is a little bit flawed as a formal test
[01:45:25.400 --> 01:45:29.200]   because it depends on the sophistication of the judge,
[01:45:29.200 --> 01:45:33.240]   whether or not they are qualified to make that distinction.
[01:45:33.240 --> 01:45:36.760]   So I think we should talk to, you know,
[01:45:36.760 --> 01:45:38.280]   the top philosophers about this,
[01:45:38.280 --> 01:45:40.800]   people like Daniel Dennett and David Chalmers
[01:45:40.800 --> 01:45:42.480]   and others who've obviously thought deeply
[01:45:42.480 --> 01:45:43.640]   about consciousness.
[01:45:43.640 --> 01:45:46.000]   Of course, consciousness itself hasn't been well,
[01:45:46.000 --> 01:45:47.720]   there's no agreed definition.
[01:45:47.720 --> 01:45:51.040]   If I was to, you know, speculate about that,
[01:45:51.040 --> 01:45:55.080]   you know, I kind of, the working definition I like is,
[01:45:55.080 --> 01:45:57.200]   it's the way information feels when, you know,
[01:45:57.200 --> 01:45:58.040]   it gets processed.
[01:45:58.040 --> 01:46:00.120]   I think maybe Max Tegmark came up with that.
[01:46:00.120 --> 01:46:01.000]   I like that idea.
[01:46:01.000 --> 01:46:02.240]   I don't know if it helps us get towards
[01:46:02.240 --> 01:46:03.880]   any more operational thing,
[01:46:03.880 --> 01:46:07.760]   but I think it's a nice way of viewing it.
[01:46:07.760 --> 01:46:09.920]   I think we can obviously see from neuroscience
[01:46:09.920 --> 01:46:11.680]   certain prerequisites that are required,
[01:46:11.680 --> 01:46:14.400]   like self-awareness, I think is necessary,
[01:46:14.400 --> 01:46:16.040]   but not sufficient component.
[01:46:16.040 --> 01:46:18.120]   This idea of a self and other,
[01:46:18.120 --> 01:46:22.480]   and set of coherent preferences that are coherent over time.
[01:46:22.480 --> 01:46:24.800]   You know, these things are maybe memory.
[01:46:24.800 --> 01:46:26.200]   These things are probably needed
[01:46:26.200 --> 01:46:29.320]   for a sentient or conscious being.
[01:46:29.320 --> 01:46:31.800]   But the reason, the difficult thing I think for us
[01:46:31.800 --> 01:46:33.400]   when we get, and I think this is a really interesting
[01:46:33.400 --> 01:46:37.280]   philosophical debate, is when we get closer to AGI
[01:46:37.280 --> 01:46:40.680]   and, you know, and much more powerful systems
[01:46:40.680 --> 01:46:42.240]   than we have today,
[01:46:42.240 --> 01:46:44.440]   how are we going to make this judgment?
[01:46:44.440 --> 01:46:47.000]   And one way, which is the Turing test,
[01:46:47.000 --> 01:46:48.640]   is sort of a behavioral judgment.
[01:46:48.640 --> 01:46:52.080]   Is the system exhibiting all the behaviors
[01:46:52.080 --> 01:46:56.880]   that a human sentient or a sentient being would exhibit?
[01:46:56.880 --> 01:46:58.160]   Is it answering the right questions?
[01:46:58.160 --> 01:46:59.160]   Is it saying the right things?
[01:46:59.160 --> 01:47:01.160]   Is it indistinguishable from a human?
[01:47:01.960 --> 01:47:03.360]   And so on.
[01:47:03.360 --> 01:47:05.760]   But I think there's a second thing
[01:47:05.760 --> 01:47:09.040]   that makes us as humans regard each other as sentient.
[01:47:09.040 --> 01:47:10.920]   Right, why do we think this?
[01:47:10.920 --> 01:47:12.720]   And I debated this with Daniel Dennett.
[01:47:12.720 --> 01:47:13.880]   And I think there's a second reason
[01:47:13.880 --> 01:47:15.600]   that's often overlooked,
[01:47:15.600 --> 01:47:18.040]   which is that we're running on the same substrate.
[01:47:18.040 --> 01:47:21.120]   Right, so if we're exhibiting the same behavior,
[01:47:21.120 --> 01:47:22.680]   more or less, as humans,
[01:47:22.680 --> 01:47:24.400]   and we're running on the same, you know,
[01:47:24.400 --> 01:47:26.200]   carbon-based biological substrate,
[01:47:26.200 --> 01:47:29.560]   the squishy, you know, few pounds of flesh in our skulls,
[01:47:29.560 --> 01:47:32.800]   then the most parsimonious, I think, explanation
[01:47:32.800 --> 01:47:35.040]   is that you're feeling the same thing as I'm feeling.
[01:47:35.040 --> 01:47:37.840]   Right, but we will never have that second part,
[01:47:37.840 --> 01:47:40.680]   the substrate equivalence, with a machine.
[01:47:40.680 --> 01:47:43.960]   Right, so we will have to only judge based on the behavior.
[01:47:43.960 --> 01:47:45.920]   And I think the substrate equivalence
[01:47:45.920 --> 01:47:48.200]   is a critical part of why we make assumptions
[01:47:48.200 --> 01:47:49.080]   that we're conscious.
[01:47:49.080 --> 01:47:51.040]   And in fact, even with animals,
[01:47:51.040 --> 01:47:52.680]   high-level animals, why we think they might be,
[01:47:52.680 --> 01:47:54.160]   'cause they're exhibiting some of the behaviors
[01:47:54.160 --> 01:47:55.920]   we would expect from a sentient animal,
[01:47:55.920 --> 01:47:57.600]   and we know they're made of the same things,
[01:47:57.600 --> 01:47:58.680]   biological neurons.
[01:47:58.680 --> 01:48:02.880]   So we're gonna have to come up with explanations
[01:48:02.880 --> 01:48:06.320]   or models of the gap between substrate differences
[01:48:06.320 --> 01:48:07.760]   between machines and humans
[01:48:07.760 --> 01:48:10.840]   to get anywhere beyond the behavioral.
[01:48:10.840 --> 01:48:12.920]   But to me, sort of the practical question
[01:48:12.920 --> 01:48:16.080]   is very interesting and very important.
[01:48:16.080 --> 01:48:18.680]   When you have millions, perhaps billions of people
[01:48:18.680 --> 01:48:20.840]   believing that you have a sentient AI,
[01:48:20.840 --> 01:48:23.080]   believing what that Google engineer believed,
[01:48:23.080 --> 01:48:26.360]   which I just see as an obvious,
[01:48:26.360 --> 01:48:28.760]   very near-term future thing,
[01:48:28.760 --> 01:48:31.200]   certainly on the path to AGI,
[01:48:31.200 --> 01:48:33.160]   how does that change the world?
[01:48:33.160 --> 01:48:35.280]   What's the responsibility of the AI system
[01:48:35.280 --> 01:48:37.120]   to help those millions of people?
[01:48:37.120 --> 01:48:39.800]   And also, what's the ethical thing?
[01:48:39.800 --> 01:48:44.120]   Because you can make a lot of people happy
[01:48:44.120 --> 01:48:48.240]   by creating a meaningful, deep experience
[01:48:48.240 --> 01:48:52.520]   with a system that's faking it before it makes it.
[01:48:52.520 --> 01:48:53.360]   - Yeah.
[01:48:53.360 --> 01:48:56.320]   I don't, are we the right,
[01:48:56.320 --> 01:48:59.960]   who is to say what's the right thing to do?
[01:48:59.960 --> 01:49:01.880]   Should AI always be tools?
[01:49:01.880 --> 01:49:02.720]   Like, why? - Sure.
[01:49:02.720 --> 01:49:06.080]   - Why are we constraining AIs to always be tools
[01:49:06.080 --> 01:49:07.920]   as opposed to friends?
[01:49:07.920 --> 01:49:09.560]   - Yeah, I think, well, I mean,
[01:49:09.560 --> 01:49:14.080]   these are fantastic questions and also critical ones.
[01:49:14.080 --> 01:49:16.480]   And we've been thinking about this
[01:49:16.480 --> 01:49:18.280]   since the start of DeepMind and before that
[01:49:18.280 --> 01:49:19.760]   because we planned for success,
[01:49:19.800 --> 01:49:24.800]   and however remote that looked like back in 2010.
[01:49:24.800 --> 01:49:27.200]   And we've always had sort of these ethical considerations
[01:49:27.200 --> 01:49:28.680]   as fundamental at DeepMind.
[01:49:28.680 --> 01:49:31.960]   And my current thinking on the language models
[01:49:31.960 --> 01:49:34.160]   is, and large models, is they're not ready,
[01:49:34.160 --> 01:49:36.720]   we don't understand them well enough yet.
[01:49:36.720 --> 01:49:40.480]   And in terms of analysis tools and guardrails,
[01:49:40.480 --> 01:49:42.320]   what they can and can't do and so on,
[01:49:42.320 --> 01:49:44.000]   to deploy them at scale.
[01:49:44.000 --> 01:49:47.120]   Because I think there are big, still ethical questions,
[01:49:47.120 --> 01:49:48.920]   like should an AI system always announce
[01:49:48.920 --> 01:49:50.880]   that it is an AI system to begin with?
[01:49:50.880 --> 01:49:51.840]   Probably yes.
[01:49:51.840 --> 01:49:55.800]   What do you do about answering those philosophical questions
[01:49:55.800 --> 01:49:59.080]   about the feelings people may have about AI systems,
[01:49:59.080 --> 01:50:01.000]   perhaps incorrectly attributed?
[01:50:01.000 --> 01:50:03.120]   So I think there's a whole bunch of research
[01:50:03.120 --> 01:50:06.320]   that needs to be done first to responsibly,
[01:50:06.320 --> 01:50:09.400]   before you can responsibly deploy these systems at scale.
[01:50:09.400 --> 01:50:12.320]   That will be at least be my current position.
[01:50:12.320 --> 01:50:15.360]   Over time, I'm very confident we'll have those tools,
[01:50:15.360 --> 01:50:17.280]   like interpretability questions,
[01:50:18.160 --> 01:50:20.920]   and analysis questions.
[01:50:20.920 --> 01:50:23.480]   And then with the ethical quandary,
[01:50:23.480 --> 01:50:28.480]   I think there it's important to look beyond just science.
[01:50:28.480 --> 01:50:31.720]   That's why I think philosophy, social sciences,
[01:50:31.720 --> 01:50:34.720]   even theology, other things like that come into it.
[01:50:34.720 --> 01:50:37.400]   Where arts and humanities,
[01:50:37.400 --> 01:50:39.120]   what does it mean to be human
[01:50:39.120 --> 01:50:40.320]   and the spirit of being human
[01:50:40.320 --> 01:50:43.680]   and to enhance that and the human condition, right?
[01:50:43.680 --> 01:50:45.080]   And allow us to experience things
[01:50:45.080 --> 01:50:46.400]   we could never experience before
[01:50:46.400 --> 01:50:49.080]   and improve the overall human condition
[01:50:49.080 --> 01:50:51.640]   and humanity overall, get radical abundance,
[01:50:51.640 --> 01:50:54.120]   solve many scientific problems, solve disease.
[01:50:54.120 --> 01:50:55.240]   So this is the era I think,
[01:50:55.240 --> 01:50:57.520]   this is the amazing era I think we're heading into
[01:50:57.520 --> 01:50:58.560]   if we do it right.
[01:50:58.560 --> 01:51:00.800]   But we've got to be careful.
[01:51:00.800 --> 01:51:02.680]   We've already seen with things like social media,
[01:51:02.680 --> 01:51:05.920]   how dual use technologies can be misused by,
[01:51:05.920 --> 01:51:10.920]   firstly, by bad actors or naive actors or crazy actors, right?
[01:51:10.920 --> 01:51:15.680]   So there's that set of just the common or garden misuse
[01:51:15.680 --> 01:51:18.000]   of existing dual use technology.
[01:51:18.000 --> 01:51:20.960]   And then of course, there's an additional thing
[01:51:20.960 --> 01:51:21.960]   that has to be overcome with AI
[01:51:21.960 --> 01:51:24.480]   that eventually it may have its own agency.
[01:51:24.480 --> 01:51:28.720]   So it could be good or bad in of itself.
[01:51:28.720 --> 01:51:31.480]   So I think these questions have to be approached
[01:51:31.480 --> 01:51:35.360]   very carefully using the scientific method, I would say,
[01:51:35.360 --> 01:51:38.680]   in terms of hypothesis generation, careful control testing,
[01:51:38.680 --> 01:51:40.720]   not live A/B testing out in the world,
[01:51:40.720 --> 01:51:44.400]   because with powerful technologies like AI,
[01:51:44.400 --> 01:51:47.680]   if something goes wrong, it may cause a lot of harm
[01:51:47.680 --> 01:51:49.160]   before you can fix it.
[01:51:49.160 --> 01:51:52.040]   It's not like an imaging app or game app
[01:51:52.040 --> 01:51:56.200]   where if something goes wrong, it's relatively easy to fix
[01:51:56.200 --> 01:51:58.000]   and the harm is relatively small.
[01:51:58.000 --> 01:52:02.600]   So I think it comes with the usual cliche
[01:52:02.600 --> 01:52:05.280]   of like with a lot of power comes a lot of responsibility.
[01:52:05.280 --> 01:52:07.840]   And I think that's the case here with things like AI
[01:52:07.840 --> 01:52:11.120]   given the enormous opportunity in front of us.
[01:52:11.120 --> 01:52:14.120]   And I think we need a lot of voices
[01:52:14.120 --> 01:52:17.920]   and as many inputs into things like the design of the systems
[01:52:17.920 --> 01:52:19.880]   and the values they should have
[01:52:19.880 --> 01:52:22.360]   and what goals should they be put to.
[01:52:22.360 --> 01:52:24.520]   I think as wide a group of voices as possible
[01:52:24.520 --> 01:52:26.760]   beyond just the technologists is needed
[01:52:26.760 --> 01:52:29.080]   to input into that and to have a say in that,
[01:52:29.080 --> 01:52:31.800]   especially when it comes to deployment of these systems,
[01:52:31.800 --> 01:52:33.440]   which is when the rubber really hits the road,
[01:52:33.440 --> 01:52:35.440]   it really affects the general person in the street
[01:52:35.440 --> 01:52:37.360]   rather than fundamental research.
[01:52:37.360 --> 01:52:40.240]   And that's why I say, I think as a first step,
[01:52:40.240 --> 01:52:42.360]   it would be better if we have the choice
[01:52:42.360 --> 01:52:45.120]   to build these systems as tools to give.
[01:52:45.120 --> 01:52:46.680]   And I'm not saying that it should never,
[01:52:46.680 --> 01:52:47.960]   they should never go beyond tools
[01:52:47.960 --> 01:52:50.360]   'cause of course the potential is there
[01:52:50.360 --> 01:52:52.960]   for it to go way beyond just tools.
[01:52:52.960 --> 01:52:55.800]   But I think that would be a good first step
[01:52:55.800 --> 01:52:58.880]   in order for us to allow us to carefully experiment
[01:52:58.880 --> 01:53:01.000]   and understand what these things can do.
[01:53:01.000 --> 01:53:05.800]   - So the leap between tool to sentient entity being
[01:53:05.800 --> 01:53:07.240]   is one we should take very care of.
[01:53:07.240 --> 01:53:08.280]   - Yes.
[01:53:08.280 --> 01:53:11.120]   - Let me ask a dark personal question.
[01:53:11.120 --> 01:53:13.480]   So you're one of the most brilliant people
[01:53:13.480 --> 01:53:16.800]   in the AI community, also one of the most kind
[01:53:16.800 --> 01:53:20.860]   and if I may say sort of loved people in the community.
[01:53:20.860 --> 01:53:25.860]   That said, creation of a super intelligent AI system
[01:53:25.860 --> 01:53:32.680]   would be one of the most powerful things in the world,
[01:53:32.680 --> 01:53:34.820]   tools or otherwise.
[01:53:34.820 --> 01:53:37.560]   And again, as the old saying goes,
[01:53:37.560 --> 01:53:40.620]   power corrupts and absolute power corrupts absolutely.
[01:53:41.620 --> 01:53:46.620]   You are likely to be one of the people,
[01:53:46.620 --> 01:53:50.280]   I would say probably the most likely person
[01:53:50.280 --> 01:53:53.240]   to be in the control of such a system.
[01:53:53.240 --> 01:53:57.140]   Do you think about the corrupting nature of power
[01:53:57.140 --> 01:53:59.540]   when you talk about these kinds of systems
[01:53:59.540 --> 01:54:04.540]   that as all dictators and people have caused atrocities
[01:54:04.540 --> 01:54:07.780]   in the past always think they're doing good.
[01:54:07.780 --> 01:54:10.940]   But they don't do good because the powers polluted
[01:54:10.940 --> 01:54:13.700]   their mind about what is good and what is evil.
[01:54:13.700 --> 01:54:14.820]   Do you think about this stuff
[01:54:14.820 --> 01:54:16.420]   or are we just focused on language model?
[01:54:16.420 --> 01:54:18.700]   - No, I think about them all the time.
[01:54:18.700 --> 01:54:22.340]   And I think what are the defenses against that?
[01:54:22.340 --> 01:54:24.820]   I think one thing is to remain very grounded
[01:54:24.820 --> 01:54:28.780]   and sort of humble no matter what you do or achieve.
[01:54:28.780 --> 01:54:30.380]   And I try to do that.
[01:54:30.380 --> 01:54:32.180]   My best friends are still my set of friends
[01:54:32.180 --> 01:54:34.620]   from my undergraduate Cambridge days.
[01:54:34.700 --> 01:54:38.100]   My family and friends are very important.
[01:54:38.100 --> 01:54:42.380]   I've always, I think trying to be a multidisciplinary person
[01:54:42.380 --> 01:54:43.780]   it helps to keep you humble
[01:54:43.780 --> 01:54:45.920]   because no matter how good you are at one topic,
[01:54:45.920 --> 01:54:47.620]   someone will be better than you at that.
[01:54:47.620 --> 01:54:50.980]   And always relearning a new topic again from scratch
[01:54:50.980 --> 01:54:53.380]   is a new field is very humbling.
[01:54:53.380 --> 01:54:56.420]   So for me, that's been biology over the last five years.
[01:54:56.420 --> 01:54:59.020]   Huge area topic and it's been,
[01:54:59.020 --> 01:55:00.260]   and I just love doing that,
[01:55:00.260 --> 01:55:01.620]   but it helps to keep you grounded
[01:55:01.620 --> 01:55:03.180]   like and keeps you open-minded.
[01:55:04.340 --> 01:55:06.380]   And then the other important thing
[01:55:06.380 --> 01:55:07.660]   is to have a really group,
[01:55:07.660 --> 01:55:10.820]   amazing set of people around you at your company
[01:55:10.820 --> 01:55:13.660]   or your organization who are also very ethical
[01:55:13.660 --> 01:55:16.840]   and grounded themselves and help to keep you that way.
[01:55:16.840 --> 01:55:18.880]   And then ultimately, just to answer your question,
[01:55:18.880 --> 01:55:22.020]   I hope we're gonna be a big part of birthing AI
[01:55:22.020 --> 01:55:24.460]   and that being the greatest benefit to humanity
[01:55:24.460 --> 01:55:26.820]   of any tool or technology ever
[01:55:26.820 --> 01:55:29.540]   and getting us into a world of radical abundance
[01:55:29.540 --> 01:55:31.140]   and curing diseases
[01:55:32.100 --> 01:55:34.260]   and solving many of the big challenges we have
[01:55:34.260 --> 01:55:36.380]   in front of us and then ultimately,
[01:55:36.380 --> 01:55:38.260]   help the ultimate flourishing of humanity
[01:55:38.260 --> 01:55:41.180]   to travel the stars and find those aliens if they are there.
[01:55:41.180 --> 01:55:43.500]   And if they're not there, find out why they're not there,
[01:55:43.500 --> 01:55:45.580]   what is going on here in the universe.
[01:55:45.580 --> 01:55:47.380]   This is all to come
[01:55:47.380 --> 01:55:49.460]   and that's what I've always dreamed about.
[01:55:49.460 --> 01:55:53.020]   But I think AI is too big an idea.
[01:55:53.020 --> 01:55:54.780]   It's not going to be,
[01:55:54.780 --> 01:55:56.980]   there'll be a certain set of pioneers who get there first.
[01:55:56.980 --> 01:55:58.600]   I hope we're in the vanguard
[01:55:58.600 --> 01:56:00.380]   so we can influence how that goes.
[01:56:00.380 --> 01:56:02.460]   And I think it matters who builds,
[01:56:02.460 --> 01:56:06.500]   which cultures they come from and what values they have,
[01:56:06.500 --> 01:56:07.860]   the builders of AI systems.
[01:56:07.860 --> 01:56:09.300]   'Cause I think even though the AI system
[01:56:09.300 --> 01:56:11.580]   is gonna learn for itself most of its knowledge,
[01:56:11.580 --> 01:56:14.780]   there'll be a residue in the system of the culture
[01:56:14.780 --> 01:56:17.140]   and the values of the creators of that system.
[01:56:17.140 --> 01:56:18.700]   And there's interesting questions
[01:56:18.700 --> 01:56:21.580]   to discuss about that geopolitically,
[01:56:21.580 --> 01:56:23.820]   different cultures as we're in a more fragmented world
[01:56:23.820 --> 01:56:24.900]   than ever unfortunately,
[01:56:24.900 --> 01:56:27.500]   I think in terms of global cooperation,
[01:56:27.500 --> 01:56:29.220]   we see that in things like climate
[01:56:29.220 --> 01:56:31.980]   where we can't seem to get our act together globally
[01:56:31.980 --> 01:56:34.060]   to cooperate on these pressing matters.
[01:56:34.060 --> 01:56:35.580]   I hope that will change over time.
[01:56:35.580 --> 01:56:38.600]   Perhaps if we get to an era of radical abundance,
[01:56:38.600 --> 01:56:40.420]   we don't have to be so competitive anymore.
[01:56:40.420 --> 01:56:42.620]   Maybe we can be more cooperative
[01:56:42.620 --> 01:56:44.300]   if resources aren't so scarce.
[01:56:44.300 --> 01:56:48.220]   - It's true that in terms of power corrupting
[01:56:48.220 --> 01:56:50.020]   and leading to destructive things,
[01:56:50.020 --> 01:56:52.780]   it seems that some of the atrocities of the past
[01:56:52.780 --> 01:56:56.660]   happen when there's a significant constraint on resources.
[01:56:56.660 --> 01:56:57.540]   - I think that's the first thing.
[01:56:57.540 --> 01:56:58.380]   I don't think that's enough.
[01:56:58.380 --> 01:57:02.420]   I think scarcity is one thing that's led to competition,
[01:57:02.420 --> 01:57:03.980]   sort of zero sum game thinking.
[01:57:03.980 --> 01:57:06.080]   I would like us to all be in a positive sum world.
[01:57:06.080 --> 01:57:08.460]   And I think for that, you have to remove scarcity.
[01:57:08.460 --> 01:57:10.780]   I don't think that's enough unfortunately to get world peace
[01:57:10.780 --> 01:57:12.780]   because there's also other corrupting things
[01:57:12.780 --> 01:57:15.460]   like wanting power over people and this kind of stuff,
[01:57:15.460 --> 01:57:18.980]   which is not necessarily satisfied by just abundance,
[01:57:18.980 --> 01:57:20.240]   but I think it will help.
[01:57:20.240 --> 01:57:24.860]   But I think ultimately AI is not gonna be run
[01:57:24.860 --> 01:57:26.740]   by any one person or one organisation.
[01:57:26.740 --> 01:57:28.020]   I think it should belong to the world,
[01:57:28.020 --> 01:57:29.580]   belong to humanity.
[01:57:29.580 --> 01:57:33.100]   And I think there'll be many ways this will happen.
[01:57:33.100 --> 01:57:36.840]   And ultimately, everybody should have a say in that.
[01:57:36.840 --> 01:57:41.300]   - Do you have advice for young people
[01:57:41.300 --> 01:57:43.060]   in high school and college,
[01:57:43.060 --> 01:57:45.820]   maybe if they're interested in AI
[01:57:45.820 --> 01:57:50.700]   or interested in having a big impact on the world,
[01:57:50.700 --> 01:57:53.220]   what they should do to have a career they can be proud of
[01:57:53.220 --> 01:57:55.060]   or to have a life they can be proud of?
[01:57:55.060 --> 01:57:57.460]   - I love giving talks to the next generation.
[01:57:57.460 --> 01:57:59.180]   What I say to them is actually two things.
[01:57:59.180 --> 01:58:02.420]   I think the most important things to learn about
[01:58:02.420 --> 01:58:04.540]   and to find out about when you're young
[01:58:04.540 --> 01:58:07.140]   is what are your true passions is first of all,
[01:58:07.140 --> 01:58:07.960]   as two things.
[01:58:07.960 --> 01:58:09.740]   One is find your true passions.
[01:58:09.740 --> 01:58:11.860]   And I think you can do that by,
[01:58:11.860 --> 01:58:14.660]   the way to do that is to explore as many things as possible
[01:58:14.660 --> 01:58:16.540]   when you're young and you have the time
[01:58:16.540 --> 01:58:19.180]   and you can take those risks.
[01:58:19.180 --> 01:58:21.100]   I would also encourage people to look at the,
[01:58:21.100 --> 01:58:24.620]   finding the connections between things in a unique way.
[01:58:24.620 --> 01:58:27.300]   I think that's a really great way to find a passion.
[01:58:27.300 --> 01:58:30.660]   Second thing I would say, advise is know yourself.
[01:58:30.660 --> 01:58:35.620]   So spend a lot of time understanding how you work best.
[01:58:35.620 --> 01:58:37.740]   Like what are the optimal times to work?
[01:58:37.740 --> 01:58:39.900]   What are the optimal ways that you study?
[01:58:39.900 --> 01:58:42.300]   What are your, how do you deal with pressure?
[01:58:42.300 --> 01:58:44.580]   Sort of test yourself in various scenarios
[01:58:44.580 --> 01:58:47.260]   and try and improve your weaknesses,
[01:58:47.260 --> 01:58:50.740]   but also find out what your unique skills and strengths are
[01:58:50.740 --> 01:58:52.180]   and then hone those.
[01:58:52.180 --> 01:58:54.540]   So then that's what will be your super value
[01:58:54.540 --> 01:58:55.900]   in the world later on.
[01:58:55.900 --> 01:58:57.860]   And if you can then combine those two things
[01:58:57.860 --> 01:59:01.220]   and find passions that you're genuinely excited about,
[01:59:01.220 --> 01:59:05.360]   that intersect with what your unique strong skills are,
[01:59:05.360 --> 01:59:07.860]   then you're onto something incredible.
[01:59:07.860 --> 01:59:10.900]   And I think you can make a huge difference in the world.
[01:59:10.900 --> 01:59:12.740]   - So let me ask about know yourself.
[01:59:12.740 --> 01:59:13.580]   This is fun.
[01:59:13.580 --> 01:59:14.420]   This is fun.
[01:59:14.420 --> 01:59:18.140]   Quick questions about day in the life, the perfect day,
[01:59:18.140 --> 01:59:21.180]   the perfect productive day in the life of Demis' house.
[01:59:21.180 --> 01:59:26.180]   Maybe these days you're, there's a lot involved.
[01:59:26.180 --> 01:59:29.020]   So maybe a slightly younger Demis' house,
[01:59:29.020 --> 01:59:31.420]   where you could focus on a single project maybe.
[01:59:31.420 --> 01:59:34.460]   How early do you wake up?
[01:59:34.460 --> 01:59:35.620]   Are you a night owl?
[01:59:35.620 --> 01:59:36.780]   Do you wake up early in the morning?
[01:59:36.780 --> 01:59:39.180]   What are some interesting habits?
[01:59:39.180 --> 01:59:42.420]   How many dozens of cups of coffees do you drink a day?
[01:59:42.420 --> 01:59:46.340]   What's the computer that you use?
[01:59:46.340 --> 01:59:47.180]   What's the setup?
[01:59:47.180 --> 01:59:48.000]   How many screens?
[01:59:48.000 --> 01:59:49.140]   What kind of keyboard?
[01:59:49.140 --> 01:59:51.420]   Are we talking Emacs Vim?
[01:59:51.420 --> 01:59:53.340]   Are we talking something more modern?
[01:59:53.340 --> 01:59:54.500]   So there's a bunch of those questions.
[01:59:54.500 --> 01:59:56.780]   So maybe day in the life.
[01:59:56.780 --> 01:59:57.620]   - Yes.
[01:59:57.620 --> 01:59:58.940]   - What's the perfect day involved?
[01:59:58.940 --> 02:00:00.860]   - Well, these days it's quite different
[02:00:00.860 --> 02:00:02.660]   from say 10, 20 years ago.
[02:00:02.660 --> 02:00:06.300]   Back 10, 20 years ago, it would have been a whole day
[02:00:06.300 --> 02:00:10.860]   of research, individual research or programming,
[02:00:10.860 --> 02:00:12.540]   doing some experiment, neuroscience,
[02:00:12.540 --> 02:00:14.020]   computer science experiment,
[02:00:14.020 --> 02:00:16.220]   reading lots of research papers.
[02:00:16.620 --> 02:00:18.380]   And then perhaps at nighttime,
[02:00:18.380 --> 02:00:24.700]   reading science fiction books or playing some games.
[02:00:24.700 --> 02:00:28.340]   - But lots of focus, so deep focused work
[02:00:28.340 --> 02:00:31.820]   on whether it's programming or reading research papers.
[02:00:31.820 --> 02:00:32.660]   - Yes, yes.
[02:00:32.660 --> 02:00:35.260]   So that would be lots of deep focus work.
[02:00:35.260 --> 02:00:38.060]   These days, for the last sort of, I guess,
[02:00:38.060 --> 02:00:40.980]   five to 10 years, I've actually got quite a structure
[02:00:40.980 --> 02:00:42.300]   that works very well for me now,
[02:00:42.300 --> 02:00:46.100]   which is that I'm a complete night owl, always have been.
[02:00:46.100 --> 02:00:47.660]   So I optimize for that.
[02:00:47.660 --> 02:00:50.740]   So I basically do a normal day's work,
[02:00:50.740 --> 02:00:52.540]   get into work about 11 o'clock
[02:00:52.540 --> 02:00:56.380]   and sort of do work till about seven in the office.
[02:00:56.380 --> 02:00:58.940]   And I will arrange back-to-back meetings
[02:00:58.940 --> 02:01:00.900]   for the entire time of that.
[02:01:00.900 --> 02:01:03.180]   And with as many, meet as many people as possible.
[02:01:03.180 --> 02:01:06.460]   So that's my collaboration management part of the day.
[02:01:06.460 --> 02:01:10.620]   Then I go home, spend time with the family and friends,
[02:01:10.620 --> 02:01:13.580]   have dinner, relax a little bit.
[02:01:13.580 --> 02:01:15.220]   And then I start a second day of work.
[02:01:15.220 --> 02:01:18.500]   I call it my second day of work around 10 p.m., 11 p.m.
[02:01:18.500 --> 02:01:20.860]   And that's the time till about the small hours
[02:01:20.860 --> 02:01:22.540]   of the morning, four or five in the morning,
[02:01:22.540 --> 02:01:26.500]   where I will do my thinking and reading research,
[02:01:26.500 --> 02:01:28.060]   writing research papers.
[02:01:28.060 --> 02:01:30.980]   Sadly, I don't have time to code anymore,
[02:01:30.980 --> 02:01:34.900]   but it's not efficient to do that these days,
[02:01:34.900 --> 02:01:37.140]   given the amount of time I have.
[02:01:37.140 --> 02:01:40.740]   But that's when I do, maybe do the long kind of stretches
[02:01:40.740 --> 02:01:42.460]   of thinking and planning.
[02:01:42.460 --> 02:01:45.260]   And then probably, using email or other things,
[02:01:45.260 --> 02:01:47.900]   I would fire off a lot of things to my team
[02:01:47.900 --> 02:01:49.380]   to deal with the next morning.
[02:01:49.380 --> 02:01:51.620]   But actually, thinking about this overnight,
[02:01:51.620 --> 02:01:53.220]   we should go for this project
[02:01:53.220 --> 02:01:54.860]   or arrange this meeting the next day.
[02:01:54.860 --> 02:01:56.140]   - When you're thinking through a problem,
[02:01:56.140 --> 02:01:58.140]   are you talking about a sheet of paper?
[02:01:58.140 --> 02:02:01.060]   Is there some structured process?
[02:02:01.060 --> 02:02:04.340]   - I still like pencil and paper best for working out things,
[02:02:04.340 --> 02:02:06.740]   but these days it's just so efficient
[02:02:06.740 --> 02:02:08.740]   to read research papers just on the screen.
[02:02:08.740 --> 02:02:10.220]   I still often print them out, actually.
[02:02:10.220 --> 02:02:12.500]   I still prefer to mark out things.
[02:02:12.500 --> 02:02:14.460]   And I find it goes into the brain quicker,
[02:02:14.460 --> 02:02:15.980]   better and sticks in the brain better
[02:02:15.980 --> 02:02:19.420]   when you're still using physical pen and pencil and paper.
[02:02:19.420 --> 02:02:20.780]   - So you take notes with the--
[02:02:20.780 --> 02:02:22.420]   - I have lots of notes, electronic ones,
[02:02:22.420 --> 02:02:27.420]   and also whole stacks of notebooks that I use at home, yeah.
[02:02:27.420 --> 02:02:29.820]   - On some of these most challenging next steps,
[02:02:29.820 --> 02:02:32.620]   for example, stuff none of us know about
[02:02:32.620 --> 02:02:35.540]   that you're working on, you're thinking,
[02:02:35.540 --> 02:02:37.580]   there's some deep thinking required there, right?
[02:02:37.580 --> 02:02:39.380]   Like what is the right problem?
[02:02:39.380 --> 02:02:41.260]   What is the right approach?
[02:02:41.260 --> 02:02:43.860]   Because you're gonna have to invest a huge amount of time
[02:02:43.860 --> 02:02:44.780]   for the whole team.
[02:02:44.780 --> 02:02:46.700]   They're going to have to pursue this thing.
[02:02:46.700 --> 02:02:48.500]   What's the right way to do it?
[02:02:48.500 --> 02:02:50.020]   Is RL gonna work here or not?
[02:02:50.020 --> 02:02:50.860]   - Yes.
[02:02:50.860 --> 02:02:53.100]   - What's the right thing to try?
[02:02:53.100 --> 02:02:55.020]   What's the right benchmark to use?
[02:02:55.020 --> 02:02:57.260]   Do we need to construct a benchmark from scratch?
[02:02:57.260 --> 02:02:58.140]   All those kinds of things.
[02:02:58.140 --> 02:03:00.180]   - Yes, so I think of all those kind of things
[02:03:00.180 --> 02:03:03.420]   in the night time phase, but also much more,
[02:03:03.420 --> 02:03:07.620]   I find I've always found the quiet hours of the morning
[02:03:07.620 --> 02:03:11.420]   when everyone's asleep, it's super quiet outside.
[02:03:11.420 --> 02:03:13.380]   I love that time, it's the golden hours,
[02:03:13.380 --> 02:03:16.500]   like between like one and three in the morning.
[02:03:16.500 --> 02:03:18.900]   Put some music on, some inspiring music on,
[02:03:18.900 --> 02:03:21.580]   and then think these deep thoughts.
[02:03:21.580 --> 02:03:24.220]   So that's when I would read my philosophy books
[02:03:24.220 --> 02:03:28.820]   and Spinoza's my recent favorite, Kant, all these things.
[02:03:28.820 --> 02:03:33.660]   And I read about a great scientist of history,
[02:03:33.660 --> 02:03:35.660]   how they did things, how they thought things.
[02:03:35.660 --> 02:03:37.220]   So that's when you do all your create,
[02:03:37.220 --> 02:03:39.140]   that's when I do all my creative thinking.
[02:03:39.140 --> 02:03:41.780]   And it's good, I think people recommend
[02:03:41.780 --> 02:03:45.100]   you do your sort of creative thinking in one block.
[02:03:45.100 --> 02:03:47.140]   And the way I organize the day,
[02:03:47.140 --> 02:03:48.540]   that way I don't get interrupted
[02:03:48.540 --> 02:03:51.460]   'cause obviously no one else is up at those times.
[02:03:51.460 --> 02:03:55.900]   So I can go, I can sort of get super deep
[02:03:55.900 --> 02:03:57.580]   and super into flow.
[02:03:57.580 --> 02:03:59.620]   The other nice thing about doing it night time wise
[02:03:59.620 --> 02:04:02.780]   is if I'm really onto something
[02:04:02.780 --> 02:04:04.940]   or I've got really deep into something,
[02:04:04.940 --> 02:04:06.860]   I can choose to extend it
[02:04:06.860 --> 02:04:08.980]   and I'll go into six in the morning, whatever,
[02:04:08.980 --> 02:04:10.780]   and then I'll just pay for it the next day.
[02:04:10.780 --> 02:04:12.980]   So I'll be a bit tired and I won't be my best,
[02:04:12.980 --> 02:04:13.900]   but that's fine.
[02:04:13.900 --> 02:04:16.660]   I can decide, looking at my schedule the next day
[02:04:16.660 --> 02:04:19.380]   that I'm given where I'm at with this particular thought
[02:04:19.380 --> 02:04:22.820]   or creative idea that I'm gonna pay that cost the next day.
[02:04:22.820 --> 02:04:25.380]   So I think that's more flexible
[02:04:25.380 --> 02:04:27.660]   than morning people who do that.
[02:04:27.660 --> 02:04:28.780]   They get up at four in the morning,
[02:04:28.780 --> 02:04:31.020]   they can also do those golden hours then,
[02:04:31.020 --> 02:04:32.660]   but then their start of their scheduled day
[02:04:32.660 --> 02:04:34.780]   starts at breakfast, you know, 8 a.m., whatever,
[02:04:34.780 --> 02:04:36.020]   they have their first meeting.
[02:04:36.020 --> 02:04:36.860]   And then it's hard,
[02:04:36.860 --> 02:04:38.980]   you have to reschedule a day if you're in flow.
[02:04:38.980 --> 02:04:41.900]   - Yeah, that could be a true special thread of thoughts
[02:04:41.900 --> 02:04:45.140]   that you're too passionate about.
[02:04:45.140 --> 02:04:46.740]   This is where some of the greatest ideas
[02:04:46.740 --> 02:04:47.740]   could potentially come
[02:04:47.740 --> 02:04:51.380]   is when you just lose yourself late into the night.
[02:04:51.380 --> 02:04:53.180]   And for the meetings, I mean,
[02:04:53.180 --> 02:04:54.860]   you're loading in really hard problems
[02:04:54.860 --> 02:04:56.500]   in a very short amount of time.
[02:04:56.500 --> 02:04:58.820]   So you have to do some kind of first principles thinking
[02:04:58.820 --> 02:05:00.180]   here, it's like, what's the problem?
[02:05:00.180 --> 02:05:01.340]   What's the state of things?
[02:05:01.340 --> 02:05:03.140]   What's the right next step?
[02:05:03.140 --> 02:05:05.100]   - You have to get really good at context switching,
[02:05:05.100 --> 02:05:07.220]   which is one of the hardest things,
[02:05:07.220 --> 02:05:09.020]   'cause especially as we do so many things,
[02:05:09.020 --> 02:05:10.780]   if you include all the scientific things we do,
[02:05:10.780 --> 02:05:12.580]   scientific fields we're working in,
[02:05:12.580 --> 02:05:15.380]   these are entire complex fields in themselves,
[02:05:15.380 --> 02:05:18.980]   and you have to sort of keep abreast of that.
[02:05:18.980 --> 02:05:20.020]   But I enjoy it.
[02:05:20.020 --> 02:05:23.860]   I've always been a sort of generalist in a way,
[02:05:23.860 --> 02:05:24.780]   and that's actually what happened
[02:05:24.780 --> 02:05:26.420]   with my games career after chess.
[02:05:26.420 --> 02:05:29.260]   One of the reasons I stopped playing chess
[02:05:29.260 --> 02:05:30.340]   was 'cause I got into computers,
[02:05:30.340 --> 02:05:32.100]   but also I started realizing there were many
[02:05:32.100 --> 02:05:33.900]   other great games out there to play too.
[02:05:33.900 --> 02:05:36.940]   So I've always been that way, inclined, multidisciplinary,
[02:05:36.940 --> 02:05:39.140]   and there's too many interesting things in the world
[02:05:39.140 --> 02:05:41.700]   to spend all your time just on one thing.
[02:05:41.700 --> 02:05:43.260]   - So you mentioned Spinoza,
[02:05:43.260 --> 02:05:47.660]   gotta ask the big, ridiculously big question about life.
[02:05:47.660 --> 02:05:50.500]   What do you think is the meaning of this whole thing?
[02:05:50.500 --> 02:05:52.580]   Why are we humans here?
[02:05:52.580 --> 02:05:54.580]   You've already mentioned that perhaps
[02:05:54.580 --> 02:05:56.740]   the universe created us.
[02:05:56.740 --> 02:05:58.940]   Is that why you think we're here?
[02:05:58.940 --> 02:06:00.100]   To understand how the universe--
[02:06:00.100 --> 02:06:02.100]   - Yeah, I think my answer to that would be,
[02:06:02.100 --> 02:06:03.980]   and at least the life I'm living,
[02:06:03.980 --> 02:06:08.100]   is to gain and understand knowledge,
[02:06:08.100 --> 02:06:10.620]   to gain knowledge and understand the universe.
[02:06:10.620 --> 02:06:12.260]   That's what I think,
[02:06:12.260 --> 02:06:13.860]   I can't see any higher purpose than that.
[02:06:13.860 --> 02:06:15.700]   If you think back to the classical Greeks,
[02:06:15.700 --> 02:06:17.300]   the virtue of gaining knowledge,
[02:06:17.300 --> 02:06:20.460]   I think it's one of the few true virtues
[02:06:20.460 --> 02:06:23.580]   is to understand the world around us
[02:06:23.580 --> 02:06:25.460]   and the context and humanity better.
[02:06:25.460 --> 02:06:27.820]   And I think if you do that,
[02:06:27.820 --> 02:06:29.140]   you become more compassionate
[02:06:29.140 --> 02:06:32.060]   and more understanding yourself and more tolerant
[02:06:32.060 --> 02:06:33.580]   and all these, I think all these other things
[02:06:33.580 --> 02:06:34.740]   may flow from that.
[02:06:34.740 --> 02:06:37.660]   And to me, understanding the nature of reality,
[02:06:37.660 --> 02:06:38.740]   that is the biggest question.
[02:06:38.740 --> 02:06:41.380]   What is going on here is sometimes the colloquial way I say,
[02:06:41.380 --> 02:06:43.620]   what is really going on here?
[02:06:43.620 --> 02:06:44.900]   It's so mysterious.
[02:06:44.900 --> 02:06:46.820]   I feel like we're in some huge puzzle.
[02:06:46.820 --> 02:06:49.980]   But the world is also seems to be,
[02:06:49.980 --> 02:06:53.100]   the universe seems to be structured in a way,
[02:06:53.100 --> 02:06:55.860]   why is it structured in a way that science is even possible?
[02:06:55.860 --> 02:06:58.180]   That methods, the scientific method works,
[02:06:58.180 --> 02:06:59.300]   things are repeatable.
[02:06:59.300 --> 02:07:02.580]   It feels like it's almost structured in a way
[02:07:02.580 --> 02:07:05.020]   to be conducive to gaining knowledge.
[02:07:05.020 --> 02:07:08.020]   So I feel like, and why should computers be even possible?
[02:07:08.020 --> 02:07:11.900]   Isn't that amazing that computational electronic devices
[02:07:11.900 --> 02:07:14.100]   can be possible?
[02:07:14.100 --> 02:07:15.300]   And they're made of sand,
[02:07:15.300 --> 02:07:17.460]   our most common element that we have,
[02:07:17.460 --> 02:07:19.940]   silicon on the Earth's crust,
[02:07:19.940 --> 02:07:21.500]   that could be made of diamond or something,
[02:07:21.500 --> 02:07:23.820]   then we would have only had one computer.
[02:07:23.820 --> 02:07:26.540]   So a lot of things are kind of slightly suspicious to me.
[02:07:26.540 --> 02:07:27.740]   - It sure as heck sounds,
[02:07:27.740 --> 02:07:29.700]   this puzzle sure as heck sounds like something
[02:07:29.700 --> 02:07:30.740]   we talked about earlier,
[02:07:30.740 --> 02:07:35.120]   what it takes to design a game that's really fun to play
[02:07:35.120 --> 02:07:36.620]   for prolonged periods of time.
[02:07:36.620 --> 02:07:40.460]   And it does seem like this puzzle, like you mentioned,
[02:07:40.460 --> 02:07:42.320]   the more you learn about it,
[02:07:42.320 --> 02:07:44.900]   the more you realize how little you know.
[02:07:44.900 --> 02:07:46.060]   So it humbles you,
[02:07:46.060 --> 02:07:49.060]   but excites you by the possibility of learning more.
[02:07:49.060 --> 02:07:53.600]   It's one heck of a puzzle we got going on here.
[02:07:53.600 --> 02:07:56.460]   So like I mentioned, of all the people in the world,
[02:07:56.460 --> 02:08:01.460]   you're very likely to be the one who creates the AGI system
[02:08:01.460 --> 02:08:06.360]   that achieves human level intelligence and goes beyond it.
[02:08:06.360 --> 02:08:07.660]   So if you got a chance,
[02:08:07.660 --> 02:08:09.460]   and very well you could be the person
[02:08:09.460 --> 02:08:11.100]   that goes into the room with the system
[02:08:11.100 --> 02:08:13.180]   and have a conversation,
[02:08:13.180 --> 02:08:15.300]   maybe you only get to ask one question.
[02:08:15.300 --> 02:08:18.160]   If you do, what question would you ask her?
[02:08:18.160 --> 02:08:21.500]   - I would probably ask,
[02:08:21.500 --> 02:08:23.300]   what is the true nature of reality?
[02:08:23.640 --> 02:08:24.560]   I think that's the question.
[02:08:24.560 --> 02:08:26.000]   I don't know if I'd understand the answer
[02:08:26.000 --> 02:08:28.520]   'cause maybe it would be 42 or something like that.
[02:08:28.520 --> 02:08:30.980]   But that's the question I would ask.
[02:08:30.980 --> 02:08:34.800]   - And then there'll be a deep sigh from the systems,
[02:08:34.800 --> 02:08:37.440]   like, all right, how do I explain to this human?
[02:08:37.440 --> 02:08:41.840]   All right, let me, I don't have time to explain.
[02:08:41.840 --> 02:08:43.720]   Maybe I'll draw you a picture.
[02:08:43.720 --> 02:08:46.480]   It is, I mean, how do you even begin
[02:08:46.480 --> 02:08:49.840]   to answer that question?
[02:08:49.840 --> 02:08:52.760]   - Well, I think it would--
[02:08:52.760 --> 02:08:55.720]   - What would you think the answer could possibly look like?
[02:08:55.720 --> 02:08:58.440]   - I think it could start looking like
[02:08:58.440 --> 02:09:02.100]   more fundamental explanations of physics
[02:09:02.100 --> 02:09:03.960]   would be the beginning.
[02:09:03.960 --> 02:09:05.780]   More careful specification of that,
[02:09:05.780 --> 02:09:07.740]   taking you, walking us through by the hand
[02:09:07.740 --> 02:09:10.660]   as to what one would do to maybe prove those things out.
[02:09:10.660 --> 02:09:13.740]   - Maybe giving you glimpses of what things
[02:09:13.740 --> 02:09:15.680]   you totally missed in the physics of today.
[02:09:15.680 --> 02:09:16.760]   - Exactly, exactly.
[02:09:16.760 --> 02:09:19.300]   - Just here's glimpses of, no,
[02:09:19.500 --> 02:09:23.660]   there's a much more elaborate world
[02:09:23.660 --> 02:09:25.500]   or a much simpler world or something.
[02:09:25.500 --> 02:09:30.260]   - A much deeper, maybe simpler explanation of things,
[02:09:30.260 --> 02:09:31.900]   right, than the standard model of physics,
[02:09:31.900 --> 02:09:34.860]   which we know doesn't work, but we still keep adding to.
[02:09:34.860 --> 02:09:37.940]   So, and that's how I think the beginning
[02:09:37.940 --> 02:09:38.940]   of an explanation would look.
[02:09:38.940 --> 02:09:41.260]   And it would start encompassing many of the mysteries
[02:09:41.260 --> 02:09:43.380]   that we have wondered about for thousands of years,
[02:09:43.380 --> 02:09:47.140]   like consciousness, dreaming, life,
[02:09:47.140 --> 02:09:48.820]   and gravity, all of these things.
[02:09:48.820 --> 02:09:51.100]   - Yeah, giving us glimpses of explanations
[02:09:51.100 --> 02:09:52.620]   for those things, yeah.
[02:09:52.620 --> 02:09:57.180]   Well, Demis, you're one of the special human beings
[02:09:57.180 --> 02:09:59.060]   in this giant puzzle of ours,
[02:09:59.060 --> 02:10:01.020]   and it's a huge honor that you would take a pause
[02:10:01.020 --> 02:10:03.220]   from the bigger puzzle to solve this small puzzle
[02:10:03.220 --> 02:10:04.740]   of a conversation with me today.
[02:10:04.740 --> 02:10:06.260]   It's truly an honor and a pleasure.
[02:10:06.260 --> 02:10:07.100]   Thank you so much. - Thank you for having me.
[02:10:07.100 --> 02:10:07.940]   I really enjoyed it.
[02:10:07.940 --> 02:10:09.100]   Thanks, Lex.
[02:10:09.100 --> 02:10:10.580]   - Thanks for listening to this conversation
[02:10:10.580 --> 02:10:11.980]   with Demis Hassabis.
[02:10:11.980 --> 02:10:13.180]   To support this podcast,
[02:10:13.180 --> 02:10:15.820]   please check out our sponsors in the description.
[02:10:15.820 --> 02:10:17.860]   And now, let me leave you with some words
[02:10:17.860 --> 02:10:20.340]   from Edgar Dijkstra.
[02:10:20.340 --> 02:10:23.460]   "Computer science is no more about computers
[02:10:23.460 --> 02:10:26.340]   than astronomy is about telescopes."
[02:10:26.340 --> 02:10:29.980]   Thank you for listening, and hope to see you next time.
[02:10:29.980 --> 02:10:32.580]   (upbeat music)
[02:10:32.580 --> 02:10:35.180]   (upbeat music)
[02:10:35.180 --> 02:10:45.180]   [BLANK_AUDIO]

