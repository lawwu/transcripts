
[00:00:00.000 --> 00:00:03.500]   Let me quickly make sure we live.
[00:00:03.500 --> 00:00:05.000]   I always mess up sometimes.
[00:00:05.000 --> 00:00:20.000]   Awesome.
[00:00:20.000 --> 00:00:22.500]   I think, no, not yet.
[00:00:22.500 --> 00:00:25.000]   Now I can hear myself, which means we're good.
[00:00:25.000 --> 00:00:26.500]   Welcome everyone.
[00:00:26.500 --> 00:00:27.500]   Thanks for joining us.
[00:00:27.500 --> 00:00:29.500]   I know you're not here to listen from me,
[00:00:29.500 --> 00:00:32.500]   so I'll shut up real quickly after introducing Muvvi.
[00:00:32.500 --> 00:00:36.000]   I hope, I don't think anyone needs an introduction to Muvvi.
[00:00:36.000 --> 00:00:42.000]   She leads the, she was just telling me she works on the entire Keras ecosystem
[00:00:42.000 --> 00:00:44.000]   in Transformers repository.
[00:00:44.000 --> 00:00:46.000]   I'm sure everyone knows what Transformers is.
[00:00:46.000 --> 00:00:49.000]   If you don't, it's the Hugging Face Transformers we're talking about.
[00:00:49.000 --> 00:00:53.500]   She also has this intersection of a developer advocate role at Hugging Face.
[00:00:53.500 --> 00:00:57.500]   She does a lot of community stuff there and is also a Google Dev expert.
[00:00:57.500 --> 00:01:03.000]   Basically, if you do anything remotely related to NLP, Hugging Face, or Keras,
[00:01:03.000 --> 00:01:04.500]   you would know Muvvi.
[00:01:04.500 --> 00:01:10.000]   She'll be telling us about how to get started in Transformers using Keras.
[00:01:10.000 --> 00:01:15.000]   I had promised everyone we'll try to get great speakers to give you great workshops,
[00:01:15.000 --> 00:01:17.000]   so I couldn't have topped it.
[00:01:17.000 --> 00:01:19.000]   With that, I'll hand it over to you.
[00:01:19.000 --> 00:01:21.000]   Thanks for joining us, Muvvi.
[00:01:21.000 --> 00:01:24.000]   Thank you for the kind words.
[00:01:24.000 --> 00:01:26.000]   It was great.
[00:01:26.000 --> 00:01:28.500]   I've been waiting a long time for this workshop.
[00:01:28.500 --> 00:01:32.500]   I know I have delayed it a lot because I was going back and forth,
[00:01:32.500 --> 00:01:35.500]   but I'm excited to learn from you, Muvvi.
[00:01:35.500 --> 00:01:37.500]   Thank you so much for this kind introduction.
[00:01:37.500 --> 00:01:45.500]   Today we are going to see how to use Transformers with TensorFlow Keras.
[00:01:45.500 --> 00:01:49.500]   About me, I'm a developer advocate at Hugging Face.
[00:01:49.500 --> 00:01:53.500]   I'm basically maintaining the Keras ecosystem at Hugging Face
[00:01:53.500 --> 00:01:59.500]   as well as we have a separate thing for the TensorFlow Transformers.
[00:01:59.500 --> 00:02:03.000]   I'm also doing community work.
[00:02:03.000 --> 00:02:09.000]   I'm currently an NLP researcher on morphologically rich languages.
[00:02:09.000 --> 00:02:15.500]   Previously, I was working as a machine learning engineer doing natural language processing.
[00:02:15.500 --> 00:02:21.500]   I think you have already been through transfer learning in the Keras book.
[00:02:21.500 --> 00:02:24.500]   Let me explain this quickly.
[00:02:24.500 --> 00:02:30.500]   Basically, in the current paradigm, we have something called pre-trained models.
[00:02:30.500 --> 00:02:33.500]   Pre-trained models are like...
[00:02:33.500 --> 00:02:40.500]   Imagine a company has basically trained a very big model with petabytes of data.
[00:02:40.500 --> 00:02:43.500]   It can be BERT, GPT-2.
[00:02:43.500 --> 00:02:46.500]   The most famous ones are those.
[00:02:46.500 --> 00:02:50.500]   There are also stuff for the image as well.
[00:02:50.500 --> 00:02:56.500]   Instead of starting from scratch, like a traditional machine learning application,
[00:02:56.500 --> 00:02:59.500]   basically gets trained from scratch,
[00:02:59.500 --> 00:03:04.500]   you build, for instance, a sequential or a functional model.
[00:03:04.500 --> 00:03:08.500]   Then you give your data and you train it.
[00:03:08.500 --> 00:03:13.500]   Instead of doing this, what we do is we get a pre-trained model.
[00:03:13.500 --> 00:03:18.500]   We put a classifier layer on top of it, depending on our problem.
[00:03:18.500 --> 00:03:22.500]   And we train it again on our own data.
[00:03:22.500 --> 00:03:25.500]   This is called fine-tuning. We tweak the weights.
[00:03:25.500 --> 00:03:32.500]   And we transfer the information from that big model to our own application, basically.
[00:03:32.500 --> 00:03:37.500]   This is something that has, I think, started with BERT.
[00:03:37.500 --> 00:03:46.500]   BERT is this model that is used for mostly the understanding-related tasks.
[00:03:46.500 --> 00:03:51.500]   The findings of understanding, I say, for instance, sentiment analysis,
[00:03:51.500 --> 00:03:55.500]   assessing the sentiment of a sentence, or question answering,
[00:03:55.500 --> 00:03:59.500]   extracting an answer from a question,
[00:03:59.500 --> 00:04:05.500]   and the context information that we look for the answer from.
[00:04:05.500 --> 00:04:08.500]   Basically, this is how it is done.
[00:04:08.500 --> 00:04:12.500]   We take this big model. We put a classifier layer on top.
[00:04:12.500 --> 00:04:15.500]   We train it on our own data, and voila.
[00:04:15.500 --> 00:04:18.500]   This whole process is called fine-tuning.
[00:04:18.500 --> 00:04:21.500]   I will go through how fine-tuning works.
[00:04:21.500 --> 00:04:26.500]   We will have an example application today.
[00:04:26.500 --> 00:04:32.500]   We will basically train a sentiment analysis model using Hugging Face Transformers,
[00:04:32.500 --> 00:04:36.500]   TensorFlow Keras. We will check our experiments with weights and biases.
[00:04:36.500 --> 00:04:40.500]   And finally, build a demo with Gradio.
[00:04:40.500 --> 00:04:45.500]   What's your favorite model? What's your favorite transformer?
[00:04:45.500 --> 00:04:47.500]   What's my favorite ensemble?
[00:04:47.500 --> 00:04:49.500]   No, transformer. What's your favorite transformer model?
[00:04:49.500 --> 00:04:51.500]   Transformer. Okay, definitely BERT.
[00:04:51.500 --> 00:04:54.500]   Because I feel like... Yeah, BERT.
[00:04:54.500 --> 00:04:58.500]   Because BERT is more predictable compared to generative models.
[00:04:58.500 --> 00:05:02.500]   I've always worked on understanding tasks mostly,
[00:05:02.500 --> 00:05:04.500]   information retrieval and stuff.
[00:05:04.500 --> 00:05:10.500]   And I really like the idea of the fact that you have this...
[00:05:10.500 --> 00:05:15.500]   I was showing the diagram, but yeah, let me go through it.
[00:05:15.500 --> 00:05:22.500]   This idea of masking a word and predicting it,
[00:05:22.500 --> 00:05:28.500]   and just getting the whole language modeling distribution is so smart.
[00:05:28.500 --> 00:05:32.500]   And also, in BERT, one sentence...
[00:05:32.500 --> 00:05:36.500]   We have another thing called next sentence prediction,
[00:05:36.500 --> 00:05:42.500]   which we can do tasks that involve sentence pairs.
[00:05:42.500 --> 00:05:49.500]   For instance, NLI, I will show today something like NLI.
[00:05:49.500 --> 00:05:55.500]   You take two sentences and you say, "Hey, there is a contradiction between them."
[00:05:55.500 --> 00:05:59.500]   So this next sentence prediction thing enables us to do fine-tuning.
[00:05:59.500 --> 00:06:05.500]   I feel like these two ideas that BERT is based on is incredibly smart.
[00:06:05.500 --> 00:06:10.500]   And you can just now take a BERT model and train it easily.
[00:06:10.500 --> 00:06:12.500]   It's crazy.
[00:06:12.500 --> 00:06:15.500]   I think Thomas said at some point it's also very cheesy, right?
[00:06:15.500 --> 00:06:18.500]   Because it was trained on some Shakespearean taste,
[00:06:18.500 --> 00:06:23.500]   so it chucks out poems if you try to poke at it sometimes.
[00:06:23.500 --> 00:06:25.500]   Sorry to derail the conversation.
[00:06:25.500 --> 00:06:27.500]   No, it's okay.
[00:06:27.500 --> 00:06:29.500]   But I just love BERT.
[00:06:29.500 --> 00:06:35.500]   I love it more than any other model like GPT and maybe T5.
[00:06:35.500 --> 00:06:39.500]   I also like T5, but BERT is my absolute favorite.
[00:06:39.500 --> 00:06:42.500]   I'm such a geek sometimes. It's just weird.
[00:06:42.500 --> 00:06:47.500]   I just sound very off sometimes.
[00:06:47.500 --> 00:06:52.500]   No, this is awesome. Please continue. I'll shut up.
[00:06:52.500 --> 00:06:59.500]   So basically what we do in Hugging Face is that we have these big models
[00:06:59.500 --> 00:07:02.500]   and also we have models contributed by community.
[00:07:02.500 --> 00:07:06.500]   Community goes to Hugging Face Hub, takes the BERT model,
[00:07:06.500 --> 00:07:11.500]   trains it on their own data, and then they also push it there
[00:07:11.500 --> 00:07:13.500]   so that other people can use it.
[00:07:13.500 --> 00:07:17.500]   It's just like a marketplace of machine learning models
[00:07:17.500 --> 00:07:19.500]   that you can actually use.
[00:07:19.500 --> 00:07:24.500]   Most of the time you don't even need to train a model from scratch.
[00:07:24.500 --> 00:07:28.500]   So this is how the model section of Hugging Face Hub looks like.
[00:07:28.500 --> 00:07:33.500]   We have other sections for datasets and stuff as well.
[00:07:33.500 --> 00:07:38.500]   And we provide the tooling for the -- also, by the way, these are open source.
[00:07:38.500 --> 00:07:45.500]   We provide the tooling of the fine-tuning process with our two libraries,
[00:07:45.500 --> 00:07:49.500]   open source libraries called Transformers and Datasets.
[00:07:49.500 --> 00:07:54.500]   We also have Datasets, and these are backed by your favorite frameworks,
[00:07:54.500 --> 00:07:58.500]   TensorFlow, PyTorch, JAX, everything.
[00:07:58.500 --> 00:08:05.500]   So you can just go into the Hub, take your model, and just fine-tune
[00:08:05.500 --> 00:08:11.500]   a model using a dataset from Datasets, or you can use Transformers
[00:08:11.500 --> 00:08:15.500]   in the process which we will do today.
[00:08:15.500 --> 00:08:18.500]   And let me go through quickly.
[00:08:18.500 --> 00:08:20.500]   Do you have a question, Sanyam?
[00:08:20.500 --> 00:08:23.500]   >> I was just going to add, it also integrates -- Transformers
[00:08:23.500 --> 00:08:26.500]   really integrates well with Weights and Biases also, so we have a single
[00:08:26.500 --> 00:08:30.500]   line of integration for those who want to tag their experiments.
[00:08:30.500 --> 00:08:31.500]   >> Yeah.
[00:08:31.500 --> 00:08:33.500]   Which we will see today.
[00:08:33.500 --> 00:08:34.500]   >> Yeah.
[00:08:34.500 --> 00:08:35.500]   We'll be seeing that.
[00:08:35.500 --> 00:08:39.500]   I'm too excited, so I'll keep jumping in here.
[00:08:39.500 --> 00:08:41.500]   Please continue.
[00:08:41.500 --> 00:08:46.500]   >> And it's like most of the time, like you don't -- as I have told you,
[00:08:46.500 --> 00:08:52.500]   you don't even need to do this for the -- you don't even need to, like,
[00:08:52.500 --> 00:08:54.500]   train from scratch.
[00:08:54.500 --> 00:08:59.500]   If you think a model is good for, you know, like you went to Hugging Face
[00:08:59.500 --> 00:09:06.500]   Hub and saw the model is there, and you have read the model card and
[00:09:06.500 --> 00:09:10.500]   seen that it might be useful for your use case, you can just, you know,
[00:09:10.500 --> 00:09:14.500]   call the pipeline on that model and just, you know, use the model right
[00:09:14.500 --> 00:09:16.500]   away.
[00:09:16.500 --> 00:09:19.500]   And we don't do only NLP anymore.
[00:09:19.500 --> 00:09:24.500]   We have audio models, we have image models, computer vision models, and
[00:09:24.500 --> 00:09:28.500]   you can use -- there's a pipeline for those as well.
[00:09:28.500 --> 00:09:31.500]   So let's see them quickly in an action.
[00:09:31.500 --> 00:09:36.500]   Like there are a few of the use cases that I would like to mention for
[00:09:36.500 --> 00:09:39.500]   NLP, like maybe something you could use in the industry.
[00:09:39.500 --> 00:09:46.500]   So one is this, for instance, you have questions your customers are
[00:09:46.500 --> 00:09:48.500]   always asking, okay?
[00:09:48.500 --> 00:09:52.500]   And you have this very big document that contains answers to your
[00:09:52.500 --> 00:09:54.500]   questions.
[00:09:54.500 --> 00:09:58.500]   You can just embed, you know, like a question answering model in your
[00:09:58.500 --> 00:10:03.500]   own -- on your website, or like, you know, you usually have it serving
[00:10:03.500 --> 00:10:05.500]   somewhere else and, you know, send a request.
[00:10:05.500 --> 00:10:11.500]   But anyway, you can basically have this question answering model and
[00:10:11.500 --> 00:10:15.500]   have your own document, and, you know, questions are coming from your
[00:10:15.500 --> 00:10:20.500]   customers, and you can search for that question's answer in those
[00:10:20.500 --> 00:10:25.500]   documents and just bring it back, bring the answer back to the customer.
[00:10:25.500 --> 00:10:29.500]   This is something you can do with BERT fine-tuned on question answering.
[00:10:29.500 --> 00:10:34.500]   Another thing is, like, for instance, you have your invoices, okay?
[00:10:34.500 --> 00:10:39.500]   And these invoices, you can get names, addresses, organization names,
[00:10:39.500 --> 00:10:41.500]   et cetera.
[00:10:41.500 --> 00:10:46.500]   You can automate information retrieval from those invoices using token
[00:10:46.500 --> 00:10:48.500]   classification models.
[00:10:48.500 --> 00:10:51.500]   These are called -- this is actually called named entity recognition
[00:10:51.500 --> 00:10:54.500]   because you are trying to get the named entities.
[00:10:54.500 --> 00:10:59.500]   But the whole abstraction over the task is called token classification
[00:10:59.500 --> 00:11:03.500]   because you are trying to predict what a word corresponds to.
[00:11:03.500 --> 00:11:07.500]   Is it a name or is it -- is it not a named entity?
[00:11:07.500 --> 00:11:10.500]   You know, things like that.
[00:11:10.500 --> 00:11:14.500]   And, you know, you can have some fun side projects by, you know, like
[00:11:14.500 --> 00:11:18.500]   you can create a conversation legend that speaks like your favorite movie
[00:11:18.500 --> 00:11:20.500]   character.
[00:11:20.500 --> 00:11:23.500]   I actually tried it on Darth Vader, but realized that, you know, in
[00:11:23.500 --> 00:11:26.500]   the movie, there's not much dialogues, actually.
[00:11:26.500 --> 00:11:33.500]   Like, you can take the script, the conversation turns in a movie, and
[00:11:33.500 --> 00:11:37.500]   then fine-tuned dialogue based on it, and it will speak like your
[00:11:37.500 --> 00:11:40.500]   favorite movie character.
[00:11:40.500 --> 00:11:42.500]   >> Who's even watched that movie?
[00:11:42.500 --> 00:11:44.500]   It's like so many of them are there.
[00:11:44.500 --> 00:11:46.500]   I haven't even watched the series.
[00:11:46.500 --> 00:11:49.500]   >> Yeah.
[00:11:49.500 --> 00:11:52.500]   It's just a fun side project.
[00:11:52.500 --> 00:11:54.500]   Nothing serious.
[00:11:54.500 --> 00:11:57.500]   It might come up as biased as well, so don't put anything like that in
[00:11:57.500 --> 00:11:59.500]   production.
[00:11:59.500 --> 00:12:01.500]   Use on your own.
[00:12:01.500 --> 00:12:05.500]   You know, use on your own risk.
[00:12:05.500 --> 00:12:09.500]   And, like, for instance, just an example of a simple pipeline is like
[00:12:09.500 --> 00:12:15.500]   you have a question and the context from what it seems it's like
[00:12:15.500 --> 00:12:17.500]   Dungeons and Dragons game.
[00:12:17.500 --> 00:12:21.500]   You know, adventure is approached by mysterious stranger, blah, blah,
[00:12:21.500 --> 00:12:24.500]   and you have the question, and it extracts the answer from that
[00:12:24.500 --> 00:12:28.500]   question.
[00:12:28.500 --> 00:12:32.500]   And this is done by a couple of lines, just a few lines.
[00:12:32.500 --> 00:12:37.500]   You have question and your context, and you call the QA model pipeline.
[00:12:37.500 --> 00:12:42.500]   And if you don't give any model from the Hugging Face Hub, in Hugging
[00:12:42.500 --> 00:12:44.500]   Face Hub, models have identifiers.
[00:12:44.500 --> 00:12:48.500]   If you don't give any model, it will be initialized with the bird for
[00:12:48.500 --> 00:12:50.500]   question answering.
[00:12:50.500 --> 00:12:53.500]   And you can just infer using, you know, giving your question and the
[00:12:53.500 --> 00:12:57.500]   context and just get the answer.
[00:12:57.500 --> 00:13:05.500]   And this also works with TensorFlow Keras, so that's what I was going to
[00:13:05.500 --> 00:13:07.500]   elaborate on.
[00:13:07.500 --> 00:13:11.500]   So basically this is a very simple, like, code snippet.
[00:13:11.500 --> 00:13:13.500]   You might not understand anything.
[00:13:13.500 --> 00:13:16.500]   Might sound like an alien language, but we will go through it during the
[00:13:16.500 --> 00:13:18.500]   workshop itself.
[00:13:18.500 --> 00:13:23.500]   So just to say that you can train a language model from scratch using
[00:13:23.500 --> 00:13:27.500]   TensorFlow Keras, and it is just like native Keras.
[00:13:27.500 --> 00:13:32.500]   Like, you know, you have -- you define your learning rate, weight, decay,
[00:13:32.500 --> 00:13:38.500]   optimizer, and compile your model and just call model.fit.
[00:13:38.500 --> 00:13:44.500]   There is a couple of preprocessing steps before that, but, you know, this
[00:13:44.500 --> 00:13:51.500]   is roughly close to the, you know, your average workflow in Keras.
[00:13:51.500 --> 00:13:56.500]   And you can also find, you know, a downstream task, which is what we are
[00:13:56.500 --> 00:14:01.500]   going to do today, and it is just like native Keras again.
[00:14:01.500 --> 00:14:06.500]   And also, I will not go through it today, but you can also serve.
[00:14:06.500 --> 00:14:11.500]   Like, you can take a model from Hugging Face Hub and just serve using
[00:14:11.500 --> 00:14:17.500]   TensorFlow serving if you want to use them in the TensorFlow extended, like,
[00:14:17.500 --> 00:14:23.500]   you know, TF Lite, TFJS.
[00:14:23.500 --> 00:14:26.500]   You can basically use anything.
[00:14:26.500 --> 00:14:30.500]   Like, they should be relatively small, though, for the Lite models at
[00:14:30.500 --> 00:14:32.500]   least.
[00:14:32.500 --> 00:14:37.500]   And you can even do, like, because Hugging Face Hub provides the, you
[00:14:37.500 --> 00:14:41.500]   know, encapsulation behind the conversion from PyTorch to TensorFlow
[00:14:41.500 --> 00:14:44.500]   and TensorFlow to PyTorch.
[00:14:44.500 --> 00:14:49.500]   You can even take a PyTorch model and save as, you know, TensorFlow Keras
[00:14:49.500 --> 00:14:55.500]   saved model format and just serve on TensorFlow serving, which is -- I think
[00:14:55.500 --> 00:15:00.500]   it is very cool, because otherwise you would have to take a PyTorch model,
[00:15:00.500 --> 00:15:05.500]   convert to ONNX and from ONNX to TensorFlow serving, which is a
[00:15:05.500 --> 00:15:09.500]   cumbersome thing, but only two lines you just convert from PyTorch to
[00:15:09.500 --> 00:15:11.500]   TensorFlow.
[00:15:11.500 --> 00:15:13.500]   Do you have any questions?
[00:15:13.500 --> 00:15:16.500]   >> I was going to say we will try to get Merv to teach us about
[00:15:16.500 --> 00:15:18.500]   productionizing.
[00:15:18.500 --> 00:15:20.500]   If she likes me as a host, she will come back and teach us.
[00:15:20.500 --> 00:15:22.500]   If not, we won't see her again.
[00:15:22.500 --> 00:15:25.500]   >> I mean, of course, I would love to do that.
[00:15:25.500 --> 00:15:30.500]   We can do another workshop, and I could show you how to do this with
[00:15:30.500 --> 00:15:33.500]   Docker and TensorFlow serving, if it's okay.
[00:15:33.500 --> 00:15:35.500]   >> Totally.
[00:15:35.500 --> 00:15:37.500]   >> That would be great.
[00:15:37.500 --> 00:15:40.500]   But I'm not, like, the best, just as a disclaimer.
[00:15:40.500 --> 00:15:44.500]   There is, like, so many people in Hugging Face doing amazing stuff over
[00:15:44.500 --> 00:15:47.500]   optimizing stuff in production.
[00:15:47.500 --> 00:15:53.500]   So I'm not the best, but I am, like, I know a couple of tricks and stuff.
[00:15:53.500 --> 00:15:58.500]   And also, like, the most kerosene thing is that, you know, like, if
[00:15:58.500 --> 00:16:02.500]   you want to host your model on the Hugging Face hub so that other
[00:16:02.500 --> 00:16:06.500]   people can use it or your teammates can directly pull and use with a
[00:16:06.500 --> 00:16:12.500]   couple of lines of code, you can just push your model using push to
[00:16:12.500 --> 00:16:17.500]   hub callback, which is just another TensorFlow callback that we have
[00:16:17.500 --> 00:16:19.500]   written.
[00:16:19.500 --> 00:16:24.500]   It just syncs for every epoch or every step.
[00:16:24.500 --> 00:16:29.500]   It just syncs the model with the model on the hub, like the repository
[00:16:29.500 --> 00:16:33.500]   on the hub, which I will show you today.
[00:16:33.500 --> 00:16:39.500]   And also, if you define the TensorBoard callback in this callback, or,
[00:16:39.500 --> 00:16:45.500]   like, if you want to later push using Git, you just need to, like, pass
[00:16:45.500 --> 00:16:50.500]   it to the TensorBoard log directory, and we will host them on the hub
[00:16:50.500 --> 00:16:53.500]   for you, which is cool.
[00:16:53.500 --> 00:16:58.500]   And lastly, if you, like, this -- all of these apply for the
[00:16:58.500 --> 00:17:02.500]   transformers models that are based on TensorFlow.
[00:17:02.500 --> 00:17:06.500]   But we also host, you know, Keras models, like regular Keras models
[00:17:06.500 --> 00:17:09.500]   that are not transformers.
[00:17:09.500 --> 00:17:14.500]   So if you want to do that, we have written this function that
[00:17:14.500 --> 00:17:18.500]   basically takes your model and pushes it to the -- pushes your model
[00:17:18.500 --> 00:17:20.500]   to the hub.
[00:17:20.500 --> 00:17:23.500]   You can also, you know, push your TensorBoard traces, and we will
[00:17:23.500 --> 00:17:25.500]   host them for you.
[00:17:25.500 --> 00:17:31.500]   And, yeah, if you'd like to learn more, I'm going to show the
[00:17:31.500 --> 00:17:33.500]   notebook now.
[00:17:33.500 --> 00:17:37.500]   But we also have, like, a course that goes through all of the theory,
[00:17:37.500 --> 00:17:39.500]   the tricks.
[00:17:39.500 --> 00:17:42.500]   There are notebooks, like, everything that you need.
[00:17:42.500 --> 00:17:45.500]   You can also reach out to me from this e-mail.
[00:17:45.500 --> 00:17:50.500]   This is it for the presentation part, and we can get to the
[00:17:50.500 --> 00:17:54.500]   notebooks if there is no questions.
[00:17:54.500 --> 00:17:58.500]   >> There's just Star Wars references, which I absolutely don't
[00:17:58.500 --> 00:18:00.500]   understand.
[00:18:00.500 --> 00:18:02.500]   Someone says --
[00:18:02.500 --> 00:18:05.500]   >> Yeah, I have definitely picked the wrong character to build
[00:18:05.500 --> 00:18:07.500]   biology PT on.
[00:18:07.500 --> 00:18:13.500]   >> There is, like, especially in the four, five, six, there is so
[00:18:13.500 --> 00:18:15.500]   little number of dialogues.
[00:18:15.500 --> 00:18:18.500]   It's all, you know, other stuff.
[00:18:18.500 --> 00:18:25.500]   For the first three, you have just very little number of
[00:18:25.500 --> 00:18:27.500]   dialogues.
[00:18:27.500 --> 00:18:31.500]   >> I also wanted to mention we're also hosting a study group
[00:18:31.500 --> 00:18:33.500]   around the awesome Hugging Face course.
[00:18:33.500 --> 00:18:37.500]   If anyone wants to check that out.
[00:18:37.500 --> 00:18:39.500]   >> I didn't know that.
[00:18:39.500 --> 00:18:41.500]   Seriously, you are going to do Hugging Face course?
[00:18:41.500 --> 00:18:44.500]   >> We've been hosting it every Sunday for a while now.
[00:18:44.500 --> 00:18:46.500]   >> I should just come over.
[00:18:46.500 --> 00:18:48.500]   I will just drop down.
[00:18:48.500 --> 00:18:50.500]   This is awesome.
[00:18:50.500 --> 00:18:52.500]   >> Please do.
[00:18:52.500 --> 00:18:54.500]   Maybe we'll do a third workshop there.
[00:18:54.500 --> 00:18:56.500]   >> Yeah, sure.
[00:18:56.500 --> 00:18:58.500]   Let's do it.
[00:18:58.500 --> 00:19:00.500]   I'm going to definitely reach out to you, and we will plan
[00:19:00.500 --> 00:19:02.500]   this.
[00:19:02.500 --> 00:19:06.500]   >> I will upset Julian, and he will say I'm pulling you in for
[00:19:06.500 --> 00:19:08.500]   a lot of work.
[00:19:08.500 --> 00:19:10.500]   >> No, it's fine.
[00:19:10.500 --> 00:19:12.500]   He would be more than happy.
[00:19:12.500 --> 00:19:14.500]   Otherwise I would be bugging him.
[00:19:14.500 --> 00:19:18.500]   I wonder if he's listening, by the way.
[00:19:18.500 --> 00:19:23.500]   >> People are saying that sounds fun.
[00:19:23.500 --> 00:19:28.500]   >> Sometimes Julian drops by to, you know, talks and stuff, and
[00:19:28.500 --> 00:19:32.500]   it makes me incredibly happy.
[00:19:32.500 --> 00:19:34.500]   >> Yeah, he's awesome.
[00:19:34.500 --> 00:19:36.500]   Could you please zoom in a bit?
[00:19:36.500 --> 00:19:38.500]   >> Yeah, sure.
[00:19:38.500 --> 00:19:42.500]   I'm going to zoom in, but let me like -- I'm going to first go
[00:19:42.500 --> 00:19:45.500]   through the, you know, nature of understanding tasks and
[00:19:45.500 --> 00:19:47.500]   everything in here.
[00:19:47.500 --> 00:19:49.500]   If it's okay.
[00:19:49.500 --> 00:19:54.500]   And then we can get to the other code parts.
[00:19:54.500 --> 00:20:01.500]   So, basically, all of the NLP and also computer vision goes
[00:20:01.500 --> 00:20:05.500]   with the, you know, like improves by the benchmarks, and
[00:20:05.500 --> 00:20:08.500]   these benchmarks are data sets, so basically whenever someone
[00:20:08.500 --> 00:20:13.500]   trains a model, they just use these benchmarks to test if
[00:20:13.500 --> 00:20:16.500]   their model is state-of-the-art or not.
[00:20:16.500 --> 00:20:18.500]   There are a couple of benchmarks.
[00:20:18.500 --> 00:20:23.500]   One is the new benchmark, and this is for the -- for the
[00:20:23.500 --> 00:20:29.500]   understanding tasks, and we also use this to fine-tune our
[00:20:29.500 --> 00:20:33.500]   models on understanding tasks, like, you know, BERT models
[00:20:33.500 --> 00:20:36.500]   mostly on understanding tasks, and it's a family of tasks,
[00:20:36.500 --> 00:20:38.500]   actually.
[00:20:38.500 --> 00:20:42.500]   There is not only one task, but it's a family of tasks that
[00:20:42.500 --> 00:20:47.500]   contains -- all of them are about classifying a sentence.
[00:20:47.500 --> 00:20:51.500]   It just -- you know, the only thing that changes is the
[00:20:51.500 --> 00:20:53.500]   purpose of that task, basically.
[00:20:53.500 --> 00:20:57.500]   So, we have, for instance, corpus of linguistic
[00:20:57.500 --> 00:21:03.500]   acceptability, which is like assessing if a sentence is
[00:21:03.500 --> 00:21:08.500]   true -- grammatically true or not grammatically appropriate,
[00:21:08.500 --> 00:21:13.500]   acceptable or not, and these are all hosted in TensorFlow
[00:21:13.500 --> 00:21:17.500]   data sets and also hugging face data sets, but TensorFlow
[00:21:18.500 --> 00:21:23.500]   has a lot of big -- how can I say, like, explanations for
[00:21:23.500 --> 00:21:26.500]   everything, and I'm going to show you.
[00:21:26.500 --> 00:21:30.500]   So, for instance, the default config for this is cola, which
[00:21:30.500 --> 00:21:33.500]   is like acceptability.
[00:21:33.500 --> 00:21:35.500]   Let me show you quick examples.
[00:21:35.500 --> 00:21:40.500]   For instance, sentence, "I broke the twig of the branch" is an
[00:21:40.500 --> 00:21:44.500]   acceptable sentence, so it is labelled as one.
[00:21:44.500 --> 00:21:49.500]   And, you know, one corresponds to acceptable in this case.
[00:21:49.500 --> 00:21:54.500]   And then an acceptable one, "The inspector analyzed the
[00:21:54.500 --> 00:21:58.500]   soundness in the building" is unacceptable, for instance.
[00:21:58.500 --> 00:22:02.500]   A couple of other examples that I think are cool is, for
[00:22:02.500 --> 00:22:07.500]   instance, we have Quora question pairs data sets, and I have
[00:22:07.500 --> 00:22:11.500]   previously used it in my previous jobs to augment the, you
[00:22:11.500 --> 00:22:15.500]   know, frequently asked questions in my training data.
[00:22:15.500 --> 00:22:20.500]   Like, I had to deal with frequently asked questions, and
[00:22:20.500 --> 00:22:25.500]   I basically used this model -- used this data set to find
[00:22:25.500 --> 00:22:30.500]   Unity 5, and then I have basically paraphrased questions
[00:22:30.500 --> 00:22:35.500]   that had the same meaning but were different so that my
[00:22:35.500 --> 00:22:38.500]   training data was a little bit more diverse.
[00:22:38.500 --> 00:22:44.500]   So the examples are, for instance, we have "How can I
[00:22:44.500 --> 00:22:49.500]   become rich in short time?" and "How can I become rich?"
[00:22:49.500 --> 00:22:52.500]   are duplicates, so it is one.
[00:22:52.500 --> 00:22:55.500]   And there are other ones that are not duplicates.
[00:22:55.500 --> 00:22:59.500]   And we have, for instance, QNLI, which is another cool
[00:22:59.500 --> 00:23:00.500]   thing.
[00:23:00.500 --> 00:23:03.500]   You can use this to -- I'm going to explain the task
[00:23:03.500 --> 00:23:06.500]   first and then tell you the use case for this.
[00:23:06.500 --> 00:23:12.500]   So we have questions and sentences.
[00:23:12.500 --> 00:23:20.500]   And this model, this data set, has basically questions and
[00:23:20.500 --> 00:23:23.500]   the sentences that might contain the answers to those
[00:23:23.500 --> 00:23:25.500]   questions or not.
[00:23:25.500 --> 00:23:29.500]   And if it contains the answer to the question, it will
[00:23:29.500 --> 00:23:30.500]   return zero.
[00:23:30.500 --> 00:23:35.500]   And if it's not, it will return one, which is a bit
[00:23:35.500 --> 00:23:37.500]   confusing in my opinion.
[00:23:37.500 --> 00:23:41.500]   But, yeah, you can use this to extract information from
[00:23:41.500 --> 00:23:45.500]   papers, like iterate over every piece of text in a
[00:23:45.500 --> 00:23:49.500]   document, and then just, you know, get the potential text
[00:23:49.500 --> 00:23:52.500]   that might contain answer to your question.
[00:23:52.500 --> 00:23:55.500]   Which is, I think, works better than question-answering
[00:23:55.500 --> 00:23:59.500]   models because you also have the context, and thanks to
[00:23:59.500 --> 00:24:03.500]   this context, you can see if the question is actually
[00:24:03.500 --> 00:24:05.500]   answered or not.
[00:24:05.500 --> 00:24:09.500]   And today we are going to go through the Stanford
[00:24:09.500 --> 00:24:13.500]   Sentiment Treebank 1, which is a sentiment analysis
[00:24:13.500 --> 00:24:14.500]   data set.
[00:24:14.500 --> 00:24:21.500]   And it has sentences and two labels, positive or
[00:24:21.500 --> 00:24:27.500]   negative, and according to their sentiments, basically.
[00:24:27.500 --> 00:24:31.500]   There are different variants of this task, by the way.
[00:24:31.500 --> 00:24:35.500]   So, for example, if you are doing an aspect-based
[00:24:35.500 --> 00:24:39.500]   sentiment analysis, it is like, let's say, I really
[00:24:39.500 --> 00:24:43.500]   like this product, but the price is too much, so I
[00:24:43.500 --> 00:24:47.500]   wouldn't recommend that, but the quality is very good.
[00:24:47.500 --> 00:24:51.500]   So you cannot really put this in positive or negative.
[00:24:51.500 --> 00:24:55.500]   So it has different aspects, like, for instance, it has
[00:24:55.500 --> 00:24:58.500]   a price aspect, it has a quality aspect.
[00:24:58.500 --> 00:25:02.500]   So you can test it across different aspects because
[00:25:02.500 --> 00:25:06.500]   nobody says, oh, my God, this is an incredible product.
[00:25:06.500 --> 00:25:08.500]   I definitely recommend that.
[00:25:08.500 --> 00:25:10.500]   It's rarely the case.
[00:25:10.500 --> 00:25:13.500]   People usually look at one product in different
[00:25:13.500 --> 00:25:14.500]   aspects.
[00:25:14.500 --> 00:25:18.500]   And there are, like, emotion data sets that contain,
[00:25:18.500 --> 00:25:22.500]   like, for instance, you have movie reviews, and there
[00:25:22.500 --> 00:25:26.500]   is, like, you know, anger, I don't know, happiness,
[00:25:26.500 --> 00:25:27.500]   et cetera.
[00:25:27.500 --> 00:25:31.500]   And how we solve it is that we have this birth model,
[00:25:31.500 --> 00:25:35.500]   and let's say you have five emotions in one data set.
[00:25:35.500 --> 00:25:39.500]   You basically put a classifier layer of five output
[00:25:39.500 --> 00:25:43.500]   units, and then you just do the fine-tuning, which
[00:25:43.500 --> 00:25:47.500]   performs better than, you know, a model that is
[00:25:47.500 --> 00:25:49.500]   trained from scratch.
[00:25:49.500 --> 00:25:55.500]   So you basically do this, like, we have all of these
[00:25:55.500 --> 00:26:01.500]   data sets for understanding tasks, and we use them to
[00:26:01.500 --> 00:26:05.500]   fine-tune a birth and accomplish our task.
[00:26:05.500 --> 00:26:09.500]   In Hugging Face Hub, by the way, I'm going to -- let
[00:26:09.500 --> 00:26:12.500]   me do it like this.
[00:26:12.500 --> 00:26:17.500]   You can basically get this with, you know, text
[00:26:17.500 --> 00:26:20.500]   classification, because it's a text classification
[00:26:20.500 --> 00:26:21.500]   task.
[00:26:21.500 --> 00:26:25.500]   You will find all of the glue tasks under one category.
[00:26:25.500 --> 00:26:29.500]   We have multiple sentiment analysis tasks, and we also
[00:26:29.500 --> 00:26:33.500]   have them across different languages, because people
[00:26:33.500 --> 00:26:37.500]   also have these glue tasks across different languages
[00:26:37.500 --> 00:26:38.500]   as well.
[00:26:38.500 --> 00:26:42.500]   For instance, we have one for Turkish.
[00:26:42.500 --> 00:26:45.500]   There is many for them.
[00:26:45.500 --> 00:26:48.500]   Anyway, let's go back.
[00:26:48.500 --> 00:26:52.500]   So today what we are going to do is we are going to
[00:26:52.500 --> 00:26:56.500]   take a birth, and then we will fine-tune it on our
[00:26:56.500 --> 00:27:00.500]   glue, one of the glue data sets, which is Stanford
[00:27:00.500 --> 00:27:04.500]   Sentiment Treebank, and then we will build a demo and
[00:27:04.500 --> 00:27:08.500]   we will track our experiments using weights and biases.
[00:27:08.500 --> 00:27:12.500]   For this, there are necessary packages you have to
[00:27:12.500 --> 00:27:15.500]   have, except for the Hugging Face Hub.
[00:27:15.500 --> 00:27:19.500]   We are having this Hugging Face Hub because we want to
[00:27:19.500 --> 00:27:23.500]   push our model and get it hosted in the Hugging Face
[00:27:23.500 --> 00:27:27.500]   Hub, and if you don't want that, you can just use the
[00:27:27.500 --> 00:27:29.500]   three of these.
[00:27:29.500 --> 00:27:33.500]   And so we have to -- for me to track my experiments, I
[00:27:33.500 --> 00:27:37.500]   have to look into weights and biases, so I have already
[00:27:37.500 --> 00:27:38.500]   did this.
[00:27:38.500 --> 00:27:41.500]   I don't want to run this notebook from scratch, because
[00:27:41.500 --> 00:27:44.500]   I will have to train a model, so I will just go and
[00:27:44.500 --> 00:27:46.500]   look at the code itself.
[00:27:46.500 --> 00:27:50.500]   There is an explanation on the family of the glue
[00:27:50.500 --> 00:27:54.500]   tasks here, and we will have a key for this task
[00:27:54.500 --> 00:27:55.500]   sentiment analysis.
[00:27:55.500 --> 00:27:57.500]   It's called SST2.
[00:27:57.500 --> 00:28:01.500]   The name comes from the data set itself, so in here we
[00:28:01.500 --> 00:28:03.500]   have SST2, as you remember.
[00:28:03.500 --> 00:28:04.500]   Any questions?
[00:28:04.500 --> 00:28:08.500]   >> I was just going to mention to anyone who signed
[00:28:08.500 --> 00:28:12.500]   up, we emailed this Colab to you, so please don't worry
[00:28:12.500 --> 00:28:14.500]   about not seeing the link.
[00:28:14.500 --> 00:28:16.500]   >> Cool.
[00:28:16.500 --> 00:28:18.500]   >> Please continue.
[00:28:18.500 --> 00:28:20.500]   >> Okay.
[00:28:20.500 --> 00:28:24.500]   And we are going to get this model from Hugging Face
[00:28:24.500 --> 00:28:27.500]   Hub, which is distilled birth based.
[00:28:27.500 --> 00:28:32.500]   So basically distilled birth is a birth variant that is
[00:28:32.500 --> 00:28:36.500]   smaller but has an equivalent performance with birth
[00:28:36.500 --> 00:28:37.500]   based.
[00:28:37.500 --> 00:28:42.500]   I think it's cool, because not everyone has the luxury
[00:28:42.500 --> 00:28:47.500]   to train a big birth model, so instead we can just take
[00:28:47.500 --> 00:28:50.500]   a distilled birth and just train it.
[00:28:50.500 --> 00:28:55.500]   So if you want to use a model from Hugging Face, you
[00:28:55.500 --> 00:29:00.500]   have to get this identifier and just pass it to the
[00:29:00.500 --> 00:29:01.500]   transformers.
[00:29:01.500 --> 00:29:04.500]   And we define our batch size here.
[00:29:04.500 --> 00:29:10.500]   And we are going to load our data set from Hugging Face
[00:29:10.500 --> 00:29:12.500]   data sets.
[00:29:12.500 --> 00:29:16.500]   It is backed by TensorFlow data sets, by the way.
[00:29:16.500 --> 00:29:19.500]   So let's see our data set.
[00:29:19.500 --> 00:29:21.500]   It is under Glue.
[00:29:21.500 --> 00:29:25.500]   And this Glue has multiple tasks, and each task has
[00:29:25.500 --> 00:29:26.500]   splits.
[00:29:26.500 --> 00:29:31.500]   So when we want to load SST2, we pass firstly the task
[00:29:31.500 --> 00:29:35.500]   itself, and Glue itself, and then the task.
[00:29:35.500 --> 00:29:39.500]   So we load our data set like this.
[00:29:39.500 --> 00:29:44.500]   And then so each model, I don't know if you have gone
[00:29:44.500 --> 00:29:49.500]   through this section in the book, but we have for each
[00:29:49.500 --> 00:29:54.500]   model, we have a tokenizer that gets the -- sorry for
[00:29:54.500 --> 00:29:56.500]   the corgis, by the way.
[00:29:56.500 --> 00:30:00.500]   They have roses because it was Valentine's Day and
[00:30:00.500 --> 00:30:05.500]   they left it open for the single people to suffer even
[00:30:05.500 --> 00:30:06.500]   further.
[00:30:06.500 --> 00:30:11.500]   So for each -- in each model, each model has its own
[00:30:11.500 --> 00:30:14.500]   tokenizer, and these tokenizers have, you know, like
[00:30:14.500 --> 00:30:18.500]   they split our text and then maps it to some numbers
[00:30:18.500 --> 00:30:20.500]   that model can understand.
[00:30:20.500 --> 00:30:24.500]   So we load our tokenizer by passing the model itself,
[00:30:24.500 --> 00:30:28.500]   which is distilled word based on case, to auto
[00:30:28.500 --> 00:30:30.500]   tokenizer.
[00:30:30.500 --> 00:30:34.500]   So what this does is it goes to the model and finds the
[00:30:34.500 --> 00:30:36.500]   tokenizer and brings it to us.
[00:30:36.500 --> 00:30:40.500]   And for instance, it's -- when we pass, like we can
[00:30:40.500 --> 00:30:45.500]   pass one text or multiple texts to a tokenizer, and it's
[00:30:45.500 --> 00:30:50.500]   going to map these to the -- map them to the numbers.
[00:30:50.500 --> 00:30:54.500]   And for instance, let's see an example from the data
[00:30:54.500 --> 00:30:55.500]   set.
[00:30:55.500 --> 00:31:01.500]   We take the first example from the training and see
[00:31:01.500 --> 00:31:05.500]   the sentence hide new secretions from the parental
[00:31:05.500 --> 00:31:06.500]   units.
[00:31:06.500 --> 00:31:08.500]   This is an example.
[00:31:08.500 --> 00:31:12.500]   And to tokenize our data set, tokenize our training data,
[00:31:12.500 --> 00:31:16.500]   we have to, like -- we have written this preprocessing
[00:31:16.500 --> 00:31:21.500]   function that we can pass the data set in, and then it
[00:31:21.500 --> 00:31:24.500]   is going to tokenize our whole data set.
[00:31:24.500 --> 00:31:29.500]   And we will -- why didn't we directly do this, the
[00:31:29.500 --> 00:31:31.500]   question is.
[00:31:31.500 --> 00:31:35.500]   So there is something called map method of data sets,
[00:31:35.500 --> 00:31:37.500]   hugging face data sets.
[00:31:37.500 --> 00:31:41.500]   So whenever you use a data set from hugging face, you
[00:31:41.500 --> 00:31:45.500]   can call map, which is going to apply this preprocessing
[00:31:45.500 --> 00:31:48.500]   function to your data set and batch.
[00:31:48.500 --> 00:31:52.500]   And, you know, there is a couple of other stuff you can
[00:31:52.500 --> 00:31:55.500]   do to increase the performance of this.
[00:31:55.500 --> 00:31:59.500]   >> I can -- I'm really sorry about it.
[00:31:59.500 --> 00:32:06.500]   Let me try to stop the corgis.
[00:32:06.500 --> 00:32:09.500]   >> I think I meant it as a joke.
[00:32:09.500 --> 00:32:10.500]   I think.
[00:32:10.500 --> 00:32:11.500]   >> A joke.
[00:32:11.500 --> 00:32:13.500]   I am sorry.
[00:32:13.500 --> 00:32:16.500]   >> No, I think this person meant it as a joke.
[00:32:16.500 --> 00:32:18.500]   Please make it stop.
[00:32:18.500 --> 00:32:22.500]   >> I honestly don't know where to shut the corgis down.
[00:32:22.500 --> 00:32:24.500]   >> They can't be stopped.
[00:32:24.500 --> 00:32:26.500]   >> They can't be stopped now.
[00:32:26.500 --> 00:32:31.500]   They have invaded my code.
[00:32:31.500 --> 00:32:33.500]   >> Please continue.
[00:32:33.500 --> 00:32:40.500]   >> So basically we get the -- you know, we basically pass
[00:32:40.500 --> 00:32:45.500]   our -- we basically pass our preprocessing function, like
[00:32:45.500 --> 00:32:51.500]   we apply it to the whole data set here by calling data sets
[00:32:51.500 --> 00:32:53.500]   dot map.
[00:32:53.500 --> 00:32:58.500]   And lastly, we have something called data collater, which is
[00:32:58.500 --> 00:33:03.500]   going to group each batch of samples together and apply some
[00:33:03.500 --> 00:33:05.500]   preprocessing on this.
[00:33:05.500 --> 00:33:09.500]   Like, for instance, for this one, we need to pad the
[00:33:09.500 --> 00:33:11.500]   examples to batch them.
[00:33:11.500 --> 00:33:14.500]   So we call data collater with padding.
[00:33:14.500 --> 00:33:18.500]   So there are different data collaters for different tasks.
[00:33:18.500 --> 00:33:22.500]   For the text classification one, we have this one.
[00:33:22.500 --> 00:33:28.500]   And we pass our tokenizer and don't forget to set this to
[00:33:28.500 --> 00:33:30.500]   return to TensorFlow.
[00:33:30.500 --> 00:33:36.500]   And lastly, so basically there is something called tf data
[00:33:36.500 --> 00:33:42.500]   dot data set, which is like the data set object that Keras
[00:33:42.500 --> 00:33:44.500]   accepts natively.
[00:33:44.500 --> 00:33:49.500]   And if we want to -- if you want to use this, like if you
[00:33:49.500 --> 00:33:54.500]   want to use this, you can directly call to TensorFlow data
[00:33:54.500 --> 00:33:59.500]   set from hugging face, which is going to convert your data to
[00:33:59.500 --> 00:34:04.500]   tf data data set, which is more convenient to train the model.
[00:34:04.500 --> 00:34:09.500]   And for this, we call -- like we call this with the tokenizer
[00:34:09.500 --> 00:34:12.500]   which are listed here.
[00:34:12.500 --> 00:34:14.500]   I think I passed it.
[00:34:14.500 --> 00:34:16.500]   Sorry.
[00:34:16.500 --> 00:34:20.500]   But they are like input IDs and attention mask.
[00:34:20.500 --> 00:34:27.500]   And we set the label column and we set shuffle through the
[00:34:27.500 --> 00:34:32.500]   batch size and we pass our data collater inside and this is
[00:34:32.500 --> 00:34:37.500]   going to return tf data data set, which we can directly pass
[00:34:37.500 --> 00:34:39.500]   to TensorFlow.
[00:34:39.500 --> 00:34:44.500]   And lastly, just like any other Keras model, we have -- we
[00:34:44.500 --> 00:34:49.500]   will define our loss, which is sparse categorical Keras
[00:34:49.500 --> 00:34:51.500]   entropy for the classification.
[00:34:51.500 --> 00:34:54.500]   For all classification, you can use this.
[00:34:54.500 --> 00:34:58.500]   And just -- it's just the same with the, you know, text
[00:34:58.500 --> 00:35:00.500]   classification problems.
[00:35:00.500 --> 00:35:03.500]   And we pass the number of labels.
[00:35:03.500 --> 00:35:07.500]   And we will call our model.
[00:35:07.500 --> 00:35:09.500]   That's why it's important and I will explain what is being
[00:35:09.500 --> 00:35:11.500]   done.
[00:35:11.500 --> 00:35:13.500]   So bear with me.
[00:35:13.500 --> 00:35:17.500]   So when I call tf automodel for sequence classification, so
[00:35:17.500 --> 00:35:23.500]   think of this as like model structure for, you know, model
[00:35:23.500 --> 00:35:28.500]   class for TensorFlow transformer models in hugging face.
[00:35:28.500 --> 00:35:33.500]   What this basically does is it takes the model weights and it
[00:35:33.500 --> 00:35:38.500]   puts a classifier layer on top with the number of labels so
[00:35:38.500 --> 00:35:43.500]   that, you know, let's say you have five labels in your data
[00:35:43.500 --> 00:35:45.500]   set.
[00:35:45.500 --> 00:35:49.500]   It's going to put five output units for the classification
[00:35:49.500 --> 00:35:51.500]   task.
[00:35:51.500 --> 00:35:54.500]   So basically I'm taking the birth model and then I'm putting
[00:35:54.500 --> 00:35:57.500]   a classifier layer on top and then I just fine tune.
[00:35:57.500 --> 00:36:00.500]   This is what is being done over here, actually.
[00:36:00.500 --> 00:36:04.500]   And the same class exists for PyTorch models.
[00:36:04.500 --> 00:36:09.500]   And if one good thing I have mentioned previously, but again,
[00:36:09.500 --> 00:36:13.500]   if two -- if for a specific task, let's say sequence
[00:36:13.500 --> 00:36:17.500]   classification, if I have the TensorFlow architecture and if I
[00:36:17.500 --> 00:36:21.500]   have PyTorch architecture, you know, these classes, then I can
[00:36:21.500 --> 00:36:23.500]   convert one weight to another.
[00:36:23.500 --> 00:36:29.500]   So let's say you want to use stuff from TensorFlow ecosystem.
[00:36:29.500 --> 00:36:36.500]   You can take PyTorch models and call this -- for instance, call
[00:36:36.500 --> 00:36:41.500]   this and say from PT equals true.
[00:36:41.500 --> 00:36:45.500]   And you can just, you know, take that PyTorch model and
[00:36:45.500 --> 00:36:49.500]   convert its weights to TensorFlow and use anything you want in
[00:36:49.500 --> 00:36:54.500]   TensorFlow, which is incredibly nice because then you don't have
[00:36:54.500 --> 00:36:58.500]   to deal with -- it's just one line of code and you don't have
[00:36:58.500 --> 00:37:03.500]   to do it with O and NX or trying to convert the weights yourself.
[00:37:03.500 --> 00:37:09.500]   And lastly, we call Adam optimizer from TensorFlow Keras
[00:37:09.500 --> 00:37:13.500]   and compile our model with the optimizer and loss.
[00:37:13.500 --> 00:37:17.500]   I'm going to get to these, but let me quickly go through the
[00:37:17.500 --> 00:37:23.500]   model.fit and I will just call model.fit somewhere over here.
[00:37:23.500 --> 00:37:28.500]   So this is the main Transformers work flow where it is done.
[00:37:28.500 --> 00:37:33.500]   But I will just add things to improve my work flow,
[00:37:33.500 --> 00:37:39.500]   reproducibility, and, you know, improve the collaboration.
[00:37:39.500 --> 00:37:43.500]   So to do this, I'm going to go through that.
[00:37:43.500 --> 00:37:47.500]   >> Okay.
[00:37:47.500 --> 00:37:49.500]   >> I was just going to mention the callback.
[00:37:49.500 --> 00:37:51.500]   >> No, I will go through all of the callbacks.
[00:37:51.500 --> 00:37:52.500]   Don't worry.
[00:37:52.500 --> 00:37:54.500]   We will spend a good amount of time.
[00:37:54.500 --> 00:37:55.500]   >> I trust you.
[00:37:55.500 --> 00:37:58.500]   Okay.
[00:37:58.500 --> 00:38:03.500]   >> So because I want my model to be hosted on Hugging Face, I
[00:38:03.500 --> 00:38:08.500]   can just, you know, like because we use large file storage
[00:38:08.500 --> 00:38:14.500]   system, I just get the necessary script and the installation
[00:38:14.500 --> 00:38:17.500]   for the Git LFS in these two lines.
[00:38:17.500 --> 00:38:21.500]   And then I just call Git LFS install to initialize Git LFS
[00:38:21.500 --> 00:38:26.500]   in the directory I'm working on so that Git LFS will track
[00:38:26.500 --> 00:38:30.500]   large files in the directory I'm working in.
[00:38:30.500 --> 00:38:37.500]   And I'm going to call Hugging Face CLI login and it redirects
[00:38:37.500 --> 00:38:40.500]   you to your token page.
[00:38:40.500 --> 00:38:45.500]   And I have my tokens over here with different access levels.
[00:38:45.500 --> 00:38:49.500]   And for, you know, pushing my model, I have to call, you
[00:38:49.500 --> 00:38:51.500]   know, write one.
[00:38:51.500 --> 00:38:56.500]   And I just log in here so that I can say, hey, this is my
[00:38:56.500 --> 00:38:57.500]   model.
[00:38:57.500 --> 00:39:00.500]   Can you just push it to the other end?
[00:39:00.500 --> 00:39:06.500]   And I am going to initialize bases, you know, just pass my
[00:39:06.500 --> 00:39:09.500]   user name and the project name.
[00:39:09.500 --> 00:39:12.500]   I have already trained my model, so I'm going to show you
[00:39:12.500 --> 00:39:14.500]   the logs.
[00:39:14.500 --> 00:39:19.500]   And I am going to -- I just pass the config and I call
[00:39:19.500 --> 00:39:26.500]   wait_sambiosis_keras callback to track my experiments.
[00:39:26.500 --> 00:39:31.500]   And, yeah, so another, like, cool thing I have previously
[00:39:31.500 --> 00:39:34.500]   mentioned is push to hub callback which can push your
[00:39:34.500 --> 00:39:37.500]   model to the Hugging Face hub.
[00:39:37.500 --> 00:39:40.500]   And I will tell you how to do this.
[00:39:40.500 --> 00:39:44.500]   And you can also do -- you can also call TensorBoard which
[00:39:44.500 --> 00:39:48.500]   can just, you know, send the TensorBoard logs and host it
[00:39:48.500 --> 00:39:50.500]   on the repo story.
[00:39:50.500 --> 00:39:54.500]   And for this, I initialize it with the output directory of
[00:39:54.500 --> 00:39:56.500]   the model.
[00:39:56.500 --> 00:40:01.500]   And the model ID I want my model to be hosted in, which is
[00:40:01.500 --> 00:40:03.500]   basically this.
[00:40:03.500 --> 00:40:06.500]   And it creates one from scratch.
[00:40:06.500 --> 00:40:10.500]   And if you already have a hosted repository for your model
[00:40:10.500 --> 00:40:15.500]   on the hub, it's going -- you can also pass the repo URL,
[00:40:15.500 --> 00:40:18.500]   which is another, like, argument here, but I didn't do
[00:40:18.500 --> 00:40:20.500]   that.
[00:40:20.500 --> 00:40:24.500]   And I have to pass my tokenizer so that auto tokenizer can
[00:40:24.500 --> 00:40:27.500]   find my tokenizer if someone else wants to use it.
[00:40:27.500 --> 00:40:32.500]   And I am going to define -- so another cool thing, we have
[00:40:32.500 --> 00:40:37.500]   this callback called Keras metric callback, which you
[00:40:37.500 --> 00:40:41.500]   pass metric computation function inside and your
[00:40:41.500 --> 00:40:43.500]   evaluation data set.
[00:40:43.500 --> 00:40:48.500]   And it's going to calculate metric at every epoch and tell
[00:40:48.500 --> 00:40:50.500]   you what that metric is.
[00:40:50.500 --> 00:40:54.500]   Because in most of the NLP problems, in this one, we have
[00:40:54.500 --> 00:40:56.500]   accuracy.
[00:40:56.500 --> 00:41:01.500]   But in other NLP problems, we have scores like rouge or
[00:41:01.500 --> 00:41:03.500]   something.
[00:41:03.500 --> 00:41:07.500]   And you have to calculate it differently and you have to
[00:41:07.500 --> 00:41:10.500]   intervene into the training loop with the tokenizer and
[00:41:10.500 --> 00:41:12.500]   everything.
[00:41:12.500 --> 00:41:15.500]   That's why we have written this and this is like, you
[00:41:15.500 --> 00:41:19.500]   know, we have written this with math, the TensorFlow
[00:41:19.500 --> 00:41:24.500]   maintainer, and this was one of my first PRs in the
[00:41:24.500 --> 00:41:28.500]   Transformer, so I was quite proud of this.
[00:41:28.500 --> 00:41:32.500]   And this is the metric function and the evaluation
[00:41:32.500 --> 00:41:34.500]   data set.
[00:41:34.500 --> 00:41:38.500]   And for this one, I just used accuracy, which is like a
[00:41:38.500 --> 00:41:40.500]   very simple metric.
[00:41:40.500 --> 00:41:43.500]   Because it's a classification task.
[00:41:43.500 --> 00:41:47.500]   Also, this metric that's compute is coming from the
[00:41:47.500 --> 00:41:50.500]   hugging face data sets library.
[00:41:50.500 --> 00:41:54.500]   It just computes the metrics given the predictions and
[00:41:54.500 --> 00:41:56.500]   the labels.
[00:41:56.500 --> 00:42:00.500]   So I'm going to pass so many callbacks into this one,
[00:42:00.500 --> 00:42:04.500]   metric callback, weights and biases callback, and push the
[00:42:04.500 --> 00:42:06.500]   hub callback.
[00:42:06.500 --> 00:42:12.500]   And I have -- when I call model.fit, basically I give
[00:42:12.500 --> 00:42:16.500]   my training data set, validation data set, epochs and
[00:42:16.500 --> 00:42:18.500]   the callbacks.
[00:42:18.500 --> 00:42:20.500]   And this is going to train.
[00:42:20.500 --> 00:42:25.500]   And as you can see, at the end of every epoch, I have the
[00:42:25.500 --> 00:42:27.500]   accuracy here.
[00:42:27.500 --> 00:42:31.500]   >> This might be a good time to show the dashboard, no?
[00:42:31.500 --> 00:42:33.500]   If you can click on the --
[00:42:33.500 --> 00:42:35.500]   >> Sure.
[00:42:35.500 --> 00:42:38.500]   So I have already -- I'm already hosting it.
[00:42:38.500 --> 00:42:40.500]   Like I have already pushed it.
[00:42:40.500 --> 00:42:45.500]   So I have this -- I don't know how to talk about this
[00:42:45.500 --> 00:42:47.500]   curve.
[00:42:47.500 --> 00:42:49.500]   >> I can talk about it.
[00:42:49.500 --> 00:42:52.500]   So the first thing I like to do is I like to judge people
[00:42:52.500 --> 00:42:54.500]   on the graphic card.
[00:42:54.500 --> 00:42:57.500]   So I look at their system settings and then I see if
[00:42:57.500 --> 00:43:00.500]   they're using Colab or just a CPU that's also tracked.
[00:43:00.500 --> 00:43:04.500]   So when move used a callback, that means that -- there you
[00:43:04.500 --> 00:43:06.500]   go.
[00:43:06.500 --> 00:43:08.500]   You can see it's -- you can see the graphic card.
[00:43:08.500 --> 00:43:10.500]   >> Would you like me to go up?
[00:43:10.500 --> 00:43:13.500]   >> No, I was just looking at the graphic card and trying to
[00:43:13.500 --> 00:43:15.500]   figure out which one it was.
[00:43:15.500 --> 00:43:18.500]   As I was going to mention, we integrate with all of the
[00:43:18.500 --> 00:43:20.500]   frameworks.
[00:43:20.500 --> 00:43:22.500]   So when sent in the callback, it automatically captures a
[00:43:22.500 --> 00:43:24.500]   callback.
[00:43:24.500 --> 00:43:27.500]   So you can see the system usage, which is like my lazy
[00:43:27.500 --> 00:43:29.500]   way of benchmarking the code.
[00:43:29.500 --> 00:43:31.500]   People say you should benchmark it by writing it.
[00:43:31.500 --> 00:43:34.500]   I just log everything to it and then I can see maybe the
[00:43:34.500 --> 00:43:36.500]   GPUs isn't at 100%.
[00:43:36.500 --> 00:43:38.500]   >> I'm just saying.
[00:43:38.500 --> 00:43:40.500]   >> Sorry?
[00:43:40.500 --> 00:43:42.500]   >> I don't judge you for this.
[00:43:42.500 --> 00:43:44.500]   >> I mean, it works, right?
[00:43:44.500 --> 00:43:46.500]   It's the same thing.
[00:43:46.500 --> 00:43:48.500]   No one will know.
[00:43:48.500 --> 00:43:50.500]   Now they do.
[00:43:50.500 --> 00:43:53.500]   >> Apart from that, you can also see all of these runs.
[00:43:53.500 --> 00:43:55.500]   So I believe, Merv, you run two runs.
[00:43:55.500 --> 00:43:57.500]   >> I have multiple runs, yeah.
[00:43:57.500 --> 00:43:59.500]   I have trained this previously.
[00:43:59.500 --> 00:44:01.500]   >> Yeah.
[00:44:01.500 --> 00:44:03.500]   So you can see the different plots up above if you scroll
[00:44:03.500 --> 00:44:05.500]   up.
[00:44:05.500 --> 00:44:07.500]   >> Yeah.
[00:44:07.500 --> 00:44:09.500]   >> You can see -- I think it's interrupted the second run,
[00:44:09.500 --> 00:44:11.500]   so that's why we just see one plot.
[00:44:11.500 --> 00:44:14.500]   Ideally, if you run -- usually people run 300, 400 runs, so
[00:44:14.500 --> 00:44:16.500]   you can visualize all of them together.
[00:44:16.500 --> 00:44:19.500]   That makes life easier because then you can go ahead and
[00:44:19.500 --> 00:44:21.500]   select just one that you really want to look at, stuff like
[00:44:21.500 --> 00:44:23.500]   that.
[00:44:23.500 --> 00:44:27.500]   On the left, if you -- on the left, if you click on the
[00:44:27.500 --> 00:44:29.500]   broom.
[00:44:29.500 --> 00:44:31.500]   >> Blue.
[00:44:31.500 --> 00:44:33.500]   >> The broom.
[00:44:33.500 --> 00:44:35.500]   Sorry.
[00:44:35.500 --> 00:44:37.500]   >> Yeah.
[00:44:37.500 --> 00:44:39.500]   >> So that's a hyper parameter sweep.
[00:44:39.500 --> 00:44:41.500]   And when you click create sweep, we can also suggest some
[00:44:41.500 --> 00:44:46.500]   parameters and if you scroll down.
[00:44:46.500 --> 00:44:49.500]   >> I think, yeah.
[00:44:49.500 --> 00:44:51.500]   >> If you click on initialize sweep.
[00:44:51.500 --> 00:44:54.500]   So we do some Bayesian stuff and automatically suggest some
[00:44:54.500 --> 00:44:56.500]   hyper parameters, you can change that stuff.
[00:44:56.500 --> 00:44:58.500]   But you can just copy this one line.
[00:44:58.500 --> 00:45:01.500]   And now if you have like multiple machines, you can just
[00:45:01.500 --> 00:45:03.500]   drop this line everywhere.
[00:45:03.500 --> 00:45:07.500]   And weights and biases will take care as a central agent will
[00:45:07.500 --> 00:45:10.500]   take care of your multiple agents across which you can run
[00:45:10.500 --> 00:45:12.500]   hyper parameter sweeps as well.
[00:45:12.500 --> 00:45:18.500]   So your OB1 will have optimized dialogues through that.
[00:45:18.500 --> 00:45:20.500]   >> This is really cool, actually.
[00:45:20.500 --> 00:45:22.500]   >> Yeah.
[00:45:22.500 --> 00:45:25.500]   If you run a hyper parameter sweep, right, the plots that
[00:45:25.500 --> 00:45:27.500]   come out of it, I really like staring at them.
[00:45:27.500 --> 00:45:30.500]   So that's another thing that I think a lot of people in the
[00:45:30.500 --> 00:45:32.500]   team do.
[00:45:32.500 --> 00:45:34.500]   >> That's the most geekiest thing I have ever heard from
[00:45:34.500 --> 00:45:36.500]   someone.
[00:45:36.500 --> 00:45:40.500]   >> I was tempted to tweet the day before yesterday the models I
[00:45:40.500 --> 00:45:49.500]   get and like a log, weights and biases log image.
[00:45:49.500 --> 00:45:51.500]   Yeah, that joke didn't say it.
[00:45:51.500 --> 00:45:56.500]   Anyways, please continue.
[00:45:56.500 --> 00:46:02.500]   >> So because I have called the push to hub callback, my model
[00:46:02.500 --> 00:46:06.500]   is also on the hosted on the hugging face hub, which means I
[00:46:06.500 --> 00:46:12.500]   can just call it with the pipeline or inference API.
[00:46:12.500 --> 00:46:16.500]   Pipeline basically downloads the model and then, you know,
[00:46:16.500 --> 00:46:21.500]   the model puts tokenizer at the beginning of the model so that
[00:46:21.500 --> 00:46:25.500]   you know you put an input text and then it goes through the
[00:46:25.500 --> 00:46:28.500]   tokenizer and it goes through the model and you get the
[00:46:28.500 --> 00:46:32.500]   prediction, which is really cool because it saves you from a
[00:46:32.500 --> 00:46:34.500]   lot of hassle.
[00:46:34.500 --> 00:46:37.500]   It's like a box that does everything for you.
[00:46:37.500 --> 00:46:41.500]   And in the pipeline, there is also something called, you
[00:46:41.500 --> 00:46:45.500]   know, we have like our model configurations and in model
[00:46:45.500 --> 00:46:50.500]   configurations, you have label to the ID and ID to the label,
[00:46:50.500 --> 00:46:54.500]   for instance, like in here.
[00:46:54.500 --> 00:46:58.500]   You know, like positive maps to one and negative maps to zero.
[00:46:58.500 --> 00:47:02.500]   We need to keep this because your model usually doesn't say,
[00:47:02.500 --> 00:47:04.500]   this is positive.
[00:47:04.500 --> 00:47:10.500]   It will say, I think like with 80% probability, this is zero
[00:47:10.500 --> 00:47:16.500]   and you will just take the arg max over all of the results and
[00:47:16.500 --> 00:47:21.500]   get the, you know, mapping from that result to the label itself,
[00:47:21.500 --> 00:47:23.500]   which is negative.
[00:47:23.500 --> 00:47:31.500]   This is already done by the pipeline itself.
[00:47:31.500 --> 00:47:34.500]   And as I have told you, my model is hosted here, which is
[00:47:34.500 --> 00:47:36.500]   really cool.
[00:47:36.500 --> 00:47:41.500]   And there is a hosted inference API, which is like, you know,
[00:47:41.500 --> 00:47:43.500]   I love this.
[00:47:43.500 --> 00:47:47.500]   I love weights and biases, let's say.
[00:47:47.500 --> 00:47:49.500]   My model is poorly trained.
[00:47:49.500 --> 00:47:53.500]   I trained it for only four epochs, so it is actually
[00:47:53.500 --> 00:47:56.500]   really -- it's not going to perform well.
[00:47:56.500 --> 00:48:01.500]   But let's see what it does.
[00:48:01.500 --> 00:48:07.500]   I can just test this model right away here.
[00:48:07.500 --> 00:48:10.500]   And for the -- there is usually a cached example and for the
[00:48:10.500 --> 00:48:14.500]   examples that are not cached, it's going to load the model
[00:48:14.500 --> 00:48:19.500]   again, so we will wait for it, but let's go through this.
[00:48:19.500 --> 00:48:22.500]   >> We have a legend in the YouTube chat.
[00:48:22.500 --> 00:48:24.500]   Abhishek has joined us.
[00:48:24.500 --> 00:48:27.500]   I just wanted to shout out his incredible YouTube channel.
[00:48:27.500 --> 00:48:30.500]   He's a forex grand master and I don't know how many things he
[00:48:30.500 --> 00:48:32.500]   does.
[00:48:32.500 --> 00:48:34.500]   >> He's just an amazing person that does everything in one
[00:48:34.500 --> 00:48:36.500]   place.
[00:48:36.500 --> 00:48:38.500]   He has this big community.
[00:48:38.500 --> 00:48:40.500]   He has this YouTube channel.
[00:48:40.500 --> 00:48:42.500]   He does awesome tutorials.
[00:48:42.500 --> 00:48:44.500]   I just -- he's so underappreciated.
[00:48:44.500 --> 00:48:46.500]   He's also doing --
[00:48:46.500 --> 00:48:49.500]   >> Abhishek, I gave you a shout out.
[00:48:49.500 --> 00:48:54.500]   Please send me a 3080TN.
[00:48:54.500 --> 00:48:56.500]   >> Yeah, he's also doing competitions and stuff.
[00:48:56.500 --> 00:48:58.500]   It's just crazy.
[00:48:58.500 --> 00:49:06.500]   Anyway, so basically I can call pipeline on this, you know, I
[00:49:06.500 --> 00:49:09.500]   call the text classification pipeline, which is the
[00:49:09.500 --> 00:49:13.500]   abstraction over all of those glue models because you do not
[00:49:13.500 --> 00:49:16.500]   need this for every single split.
[00:49:16.500 --> 00:49:19.500]   You can just say this is a text classification problem.
[00:49:19.500 --> 00:49:22.500]   And you call your model like this.
[00:49:22.500 --> 00:49:25.500]   You give your model as an argument.
[00:49:25.500 --> 00:49:28.500]   And it's going to download your model and build a pipeline over
[00:49:28.500 --> 00:49:34.500]   it and you can just give your input and this is going to say
[00:49:34.500 --> 00:49:36.500]   it's positive.
[00:49:36.500 --> 00:49:39.500]   It's actually positive, but I don't have the necessary labels
[00:49:39.500 --> 00:49:41.500]   for this.
[00:49:41.500 --> 00:49:44.500]   This just maps the positive and you can just know that and the
[00:49:44.500 --> 00:49:46.500]   score of this.
[00:49:46.500 --> 00:49:50.500]   Which is really convenient to test your models.
[00:49:50.500 --> 00:49:58.500]   And we will see how to build a UI over this.
[00:49:58.500 --> 00:50:05.500]   So I basically call install Gradio, which is a really good
[00:50:05.500 --> 00:50:09.500]   library to build your UIs.
[00:50:09.500 --> 00:50:12.500]   And I import Gradio as GR.
[00:50:12.500 --> 00:50:17.500]   And basically thanks to the integration, you can just call
[00:50:17.500 --> 00:50:22.500]   GR interface.load and you can give your hugging face model by
[00:50:22.500 --> 00:50:25.500]   prefixing your model as hugging face.
[00:50:25.500 --> 00:50:30.500]   Otherwise you have to define every single thing, Gradio
[00:50:30.500 --> 00:50:32.500]   interface.
[00:50:32.500 --> 00:50:38.500]   You know, give your model here, give input, I don't know, input
[00:50:38.500 --> 00:50:44.500]   type, which is like, you know, text box, output type, which is
[00:50:44.500 --> 00:50:49.500]   like another, I don't know, label, et cetera, et cetera.
[00:50:49.500 --> 00:50:54.500]   But because this knows that this is a text classification model,
[00:50:54.500 --> 00:50:59.500]   it will just say, hey, if this is a text classification model, I
[00:50:59.500 --> 00:51:04.500]   can just put the text -- put a text box and the labels like
[00:51:04.500 --> 00:51:08.500]   this, and just, you know, like create it for you.
[00:51:08.500 --> 00:51:12.500]   And, you know, not let you do everything.
[00:51:12.500 --> 00:51:16.500]   But like otherwise if you have a regular Keras model, you can
[00:51:16.500 --> 00:51:18.500]   just do it like this.
[00:51:18.500 --> 00:51:22.500]   And I can define examples to, you know, hint the user, you
[00:51:22.500 --> 00:51:26.500]   know, saying you can pass this -- you can use it like that.
[00:51:26.500 --> 00:51:32.500]   And, you know, a couple of other stuff, like I have the title of
[00:51:32.500 --> 00:51:35.500]   my demo, and which is here.
[00:51:35.500 --> 00:51:37.500]   I'm sorry for the team.
[00:51:37.500 --> 00:51:42.500]   I just love this team a lot, and I should have changed that.
[00:51:42.500 --> 00:51:46.500]   And I have the description of the demo.
[00:51:46.500 --> 00:51:51.500]   And I just call launch, and it will launch it for me.
[00:51:51.500 --> 00:51:56.500]   So basically you have a UI, and you can even host it on the
[00:51:56.500 --> 00:52:00.500]   hugging faces for everyone else to play, which you have
[00:52:00.500 --> 00:52:06.500]   probably seen with the anime gun and other cool stuff.
[00:52:06.500 --> 00:52:11.500]   But, yeah, it's just so easy to build like a demo for like
[00:52:11.500 --> 00:52:13.500]   three lines of code.
[00:52:13.500 --> 00:52:17.500]   Because when I was a machine learning engineer, oh, my God,
[00:52:17.500 --> 00:52:21.500]   like for only internal demos, I would spend a day or so,
[00:52:21.500 --> 00:52:25.500]   because I wasn't a backhand developer, and I didn't know
[00:52:25.500 --> 00:52:29.500]   like how to build a UI, you know, like use everything, like
[00:52:29.500 --> 00:52:33.500]   I don't know, serve it with, you know, like requests and,
[00:52:33.500 --> 00:52:35.500]   you know, stuff like that.
[00:52:35.500 --> 00:52:40.500]   It was just so hard, you know, to serve it on ng rock.
[00:52:40.500 --> 00:52:43.500]   You know, like I just don't like it, okay?
[00:52:43.500 --> 00:52:46.500]   And I feel like no one has to deal with this.
[00:52:46.500 --> 00:52:49.500]   You just need to focus on the things that are important for
[00:52:49.500 --> 00:52:52.500]   you as a machine learning engineer, which is like
[00:52:52.500 --> 00:52:56.500]   optimizing your model and, you know, making it better and not
[00:52:56.500 --> 00:52:59.500]   building a UI over it.
[00:52:59.500 --> 00:53:04.500]   So, like, in the -- I didn't build a UI for this, but I have
[00:53:04.500 --> 00:53:06.500]   already to use one.
[00:53:06.500 --> 00:53:09.500]   Like I have built one for question answering, for
[00:53:09.500 --> 00:53:11.500]   instance.
[00:53:11.500 --> 00:53:14.500]   It's just served here, and your team members can see it.
[00:53:14.500 --> 00:53:18.500]   And you can also only your team members can see it if you set
[00:53:18.500 --> 00:53:20.500]   it to private.
[00:53:20.500 --> 00:53:24.500]   Like, for instance, I have this text, and I have a question,
[00:53:24.500 --> 00:53:26.500]   and I am returned an answer.
[00:53:26.500 --> 00:53:32.500]   And this is, again, done with just a few lines of code here.
[00:53:32.500 --> 00:53:37.500]   You know, graduate interface, blah, blah.
[00:53:37.500 --> 00:53:40.500]   So, also, like, just a quick shout out.
[00:53:40.500 --> 00:53:45.500]   We have a Keras working group at the Hugging Face, if you'd
[00:53:45.500 --> 00:53:47.500]   like to join us.
[00:53:47.500 --> 00:53:51.500]   What we're doing is basically we are getting all of the
[00:53:51.500 --> 00:53:56.500]   examples from Keras.io, which you love, and we serialize those
[00:53:56.500 --> 00:54:01.500]   models and build demos over those models and just host it
[00:54:01.500 --> 00:54:06.500]   here so that, you know, anyone who is looking at a cool Keras
[00:54:06.500 --> 00:54:10.500]   example can, you know, find the model here, for instance,
[00:54:10.500 --> 00:54:14.500]   Swin Transformers, and here there is a space.
[00:54:14.500 --> 00:54:17.500]   But, yeah, let's go to Swin Transformers first.
[00:54:17.500 --> 00:54:21.500]   You know, you can just pull the model and use it yourself
[00:54:21.500 --> 00:54:25.500]   instead of, you know, going to Keras.io and getting the
[00:54:25.500 --> 00:54:27.500]   notebook and then training it.
[00:54:27.500 --> 00:54:33.500]   And also we have spaces for this, and currently we are doing
[00:54:33.500 --> 00:54:41.500]   a sprint, which is like for the win, you know, like Keras
[00:54:41.500 --> 00:54:44.500]   reproduces Keras for the win.
[00:54:44.500 --> 00:54:48.500]   So, yeah, we try to reproduce those Keras examples and, you
[00:54:48.500 --> 00:54:51.500]   know, letting the community test them if you'd like to join.
[00:54:51.500 --> 00:54:54.500]   It's a cool thing to do.
[00:54:54.500 --> 00:54:57.500]   And this concludes the workshop.
[00:54:57.500 --> 00:55:00.500]   And you can have questions.
[00:55:00.500 --> 00:55:01.500]   >> This was awesome.
[00:55:01.500 --> 00:55:02.500]   Thanks.
[00:55:02.500 --> 00:55:05.500]   Thanks for the rundown and thanks for walking us through
[00:55:05.500 --> 00:55:06.500]   every single detail.
[00:55:06.500 --> 00:55:10.500]   I'm sure everyone loved this.
[00:55:10.500 --> 00:55:13.500]   I'm sure this wasn't for me, but Leon said this.
[00:55:13.500 --> 00:55:14.500]   >> Thank you, Abhishek.
[00:55:14.500 --> 00:55:16.500]   Thanks, Leon.
[00:55:16.500 --> 00:55:19.500]   There's a question by Amit.
[00:55:19.500 --> 00:55:23.500]   Let me copy/paste it here.
[00:55:23.500 --> 00:55:29.500]   And I'll highlight it in one second.
[00:55:29.500 --> 00:55:32.500]   So they're asking, I want to modify one of the -- this is
[00:55:32.500 --> 00:55:35.500]   underrated to the workshop, one of the machine translation
[00:55:35.500 --> 00:55:38.500]   models, not fine tuning or adding tails.
[00:55:38.500 --> 00:55:42.500]   How can they do that and train the modified model with
[00:55:42.500 --> 00:55:44.500]   modifications from scratch?
[00:55:44.500 --> 00:55:51.500]   >> I think if you already have the model class, I will also
[00:55:51.500 --> 00:55:52.500]   share it here.
[00:55:52.500 --> 00:55:56.500]   Let me share this.
[00:55:56.500 --> 00:56:00.500]   If you already have the, you know, model class itself and
[00:56:00.500 --> 00:56:05.500]   you just want minor changes on it, I think you can do that.
[00:56:05.500 --> 00:56:11.500]   And, you know, just, you know, take all the model architecture
[00:56:11.500 --> 00:56:15.500]   and just, you know, modify it and use it on your local
[00:56:15.500 --> 00:56:19.500]   instead of, you know, calling it every time from transformers.
[00:56:19.500 --> 00:56:21.500]   But this is just a wild guess.
[00:56:21.500 --> 00:56:23.500]   I would probably do it like that.
[00:56:23.500 --> 00:56:27.500]   You can also, like, there is -- we also have a forum you can
[00:56:27.500 --> 00:56:29.500]   ask questions from.
[00:56:29.500 --> 00:56:33.500]   Let me send you the link.
[00:56:33.500 --> 00:56:39.500]   You can get better insights over there.
[00:56:39.500 --> 00:56:43.500]   >> Thanks for that.
[00:56:43.500 --> 00:56:44.500]   Awesome.
[00:56:44.500 --> 00:56:51.500]   I'm just checking if there are any other questions.
[00:56:51.500 --> 00:56:53.500]   Someone says really nice presentation.
[00:56:53.500 --> 00:56:55.500]   I enjoyed it as well.
[00:56:55.500 --> 00:56:59.500]   So thanks.
[00:56:59.500 --> 00:57:04.500]   >> Could you share a link to fine tune GPT-J2 or Neo?
[00:57:04.500 --> 00:57:07.500]   Any idea when Neo is releasing on the hub?
[00:57:07.500 --> 00:57:11.500]   >> Let me check for a sec.
[00:57:11.500 --> 00:57:16.500]   So I will share a generic link for all of the Hugging Face
[00:57:16.500 --> 00:57:18.500]   examples.
[00:57:18.500 --> 00:57:23.500]   But these GPT-J and Neo as far as I know are very big models.
[00:57:23.500 --> 00:57:26.500]   So I don't know if you can actually fine tune.
[00:57:26.500 --> 00:57:31.500]   I mean, you can fine tune or pre-train GPT-J2.
[00:57:31.500 --> 00:57:36.500]   But let me check the examples and also maybe notebooks because
[00:57:36.500 --> 00:57:42.500]   they are more for educational purposes.
[00:57:42.500 --> 00:57:50.500]   Transformers, notebooks.
[00:57:50.500 --> 00:57:53.500]   So all of our examples are here.
[00:57:53.500 --> 00:58:07.500]   Let me copy and paste that link.
[00:58:07.500 --> 00:58:09.500]   I don't see any other questions.
[00:58:09.500 --> 00:58:12.500]   I will quickly mention the Hugging Face study group I mentioned.
[00:58:12.500 --> 00:58:15.500]   Hopefully I will ask more to join in the future.
[00:58:15.500 --> 00:58:18.500]   >> I definitely will join you.
[00:58:18.500 --> 00:58:21.500]   Let me go there.
[00:58:21.500 --> 00:58:24.500]   So this is the link for anyone that's interested.
[00:58:24.500 --> 00:58:26.500]   I will wait for any more questions.
[00:58:26.500 --> 00:58:29.500]   If not, we can wrap up in 30 seconds.
[00:58:29.500 --> 00:58:31.500]   >> Yeah, sure.
[00:58:31.500 --> 00:58:35.500]   You can also ask me your questions from my Twitter.
[00:58:35.500 --> 00:58:37.500]   >> I will quickly --
[00:58:37.500 --> 00:58:40.500]   >> I'm going to give my email.
[00:58:40.500 --> 00:58:44.500]   But if you ask it to the, you know, discord or forum, then
[00:58:44.500 --> 00:58:47.500]   people will just answer.
[00:58:47.500 --> 00:58:52.500]   And which is like we have a very engaging community that people
[00:58:52.500 --> 00:58:54.500]   help each other a lot.
[00:58:54.500 --> 00:58:58.500]   So you can -- it's so weird to look at it.
[00:58:58.500 --> 00:59:01.500]   >> Please follow Merv at her first name.
[00:59:01.500 --> 00:59:04.500]   I will drop that in the chat.
[00:59:04.500 --> 00:59:06.500]   She is quite active on Twitter.
[00:59:06.500 --> 00:59:09.500]   As far as I understand, you reply to every single question
[00:59:09.500 --> 00:59:10.500]   there.
[00:59:10.500 --> 00:59:11.500]   So I'm sure --
[00:59:11.500 --> 00:59:13.500]   >> Yeah, I try to.
[00:59:13.500 --> 00:59:15.500]   >> People can connect with you there.
[00:59:15.500 --> 00:59:17.500]   >> Great, by the way.
[00:59:17.500 --> 00:59:19.500]   Thank you so much for hosting me.
[00:59:19.500 --> 00:59:21.500]   We should do this more often.
[00:59:21.500 --> 00:59:23.500]   >> No, thanks.
[00:59:23.500 --> 00:59:25.500]   Thanks to you for your time.
[00:59:25.500 --> 00:59:27.500]   I would love to collaborate.
[00:59:27.500 --> 00:59:29.500]   I highlighted Merv's Twitter handle.
[00:59:29.500 --> 00:59:31.500]   I would recommend you follow her.
[00:59:31.500 --> 00:59:33.500]   She also shares a lot about tricks around.
[00:59:33.500 --> 00:59:37.500]   I think transformer models in general.
[00:59:37.500 --> 00:59:39.500]   NLP, everything.
[00:59:39.500 --> 00:59:42.500]   So basically follow her on there.
[00:59:42.500 --> 00:59:45.500]   Keep up with the hugging face stuff.
[00:59:45.500 --> 00:59:47.500]   Also join our study group.
[00:59:47.500 --> 00:59:49.500]   And with that, I'll wrap up.
[00:59:49.500 --> 00:59:51.500]   Thanks, everyone, for joining.
[00:59:51.500 --> 00:59:53.500]   And thanks to you, Merv, for the awesome presentation.
[00:59:53.500 --> 00:59:55.500]   >> Thank you.
[00:59:55.500 --> 00:59:57.500]   See you.

