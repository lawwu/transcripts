
[00:00:00.000 --> 00:00:08.300]   I think that simulator has this huge scaling promise.
[00:00:08.300 --> 00:00:14.340]   You take any scenario you saw and you release the agents and you release yourself and you
[00:00:14.340 --> 00:00:18.260]   can try all kinds of stuff and they can try all kinds of stuff.
[00:00:18.260 --> 00:00:22.620]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:22.620 --> 00:00:24.860]   and I'm your host, Lukas Biewald.
[00:00:24.860 --> 00:00:30.380]   Today I'm talking with an old friend, Drago Angelov, who is currently a distinguished
[00:00:30.380 --> 00:00:33.780]   scientist and head of research at Waymo.
[00:00:33.780 --> 00:00:38.500]   He's been working on image models for at least the past 20 years and was another student
[00:00:38.500 --> 00:00:41.340]   of Daphne Koller, who is also on this podcast.
[00:00:41.340 --> 00:00:43.100]   This is a super fun conversation.
[00:00:43.100 --> 00:00:46.040]   I hope you enjoy it as much as I did.
[00:00:46.040 --> 00:00:50.140]   My first question is something I hadn't realized, which is that you're one of the authors of
[00:00:50.140 --> 00:00:51.940]   the original Inception architecture.
[00:00:51.940 --> 00:00:52.940]   That's right.
[00:00:52.940 --> 00:00:55.980]   I should know, but I somehow missed that.
[00:00:55.980 --> 00:01:01.300]   But can you tell me how that came about and what you were thinking about at the time?
[00:01:01.300 --> 00:01:04.740]   So actually the story goes back even before.
[00:01:04.740 --> 00:01:11.620]   At Google, I worked on Street View for a bit, which was related to autonomous driving.
[00:01:11.620 --> 00:01:15.780]   And one of the areas where actually computer vision used to work pretty well, there were
[00:01:15.780 --> 00:01:20.500]   two phase detection, license place blurring that worked pretty well at the time.
[00:01:20.500 --> 00:01:27.220]   And the other thing that worked really well is 3D reconstruction from cameras, from LiDAR.
[00:01:27.220 --> 00:01:30.500]   So that's what I used to work on, bundle adjustment and so on.
[00:01:30.500 --> 00:01:34.020]   And were those deep learning models at the time?
[00:01:34.020 --> 00:01:35.820]   None of them were deep learning.
[00:01:35.820 --> 00:01:41.900]   Actually we had one person in 2008 or 2009, he came from Microsoft.
[00:01:41.900 --> 00:01:44.460]   I think his name was Ahmed or Abdul.
[00:01:44.460 --> 00:01:52.000]   And he used deep nets to essentially detect and blur license plates.
[00:01:52.000 --> 00:01:58.040]   And everyone was very unhappy that he used deep nets because it was his own code base
[00:01:58.040 --> 00:02:00.260]   and no one else was doing anything like it.
[00:02:00.260 --> 00:02:05.340]   And of course you could modernize and upgrade it by doing support vector machines.
[00:02:05.340 --> 00:02:10.580]   So eventually people tried to modernize and upgrade with support vector machines, the
[00:02:10.580 --> 00:02:13.380]   neural net things, and they didn't quite succeed.
[00:02:13.380 --> 00:02:16.820]   And I think they regressed a bit, but everyone used technology they understood.
[00:02:16.820 --> 00:02:22.700]   So I think I didn't work exactly on that problem, but I think it was at the time, that's how
[00:02:22.700 --> 00:02:25.440]   we used to do it.
[00:02:25.440 --> 00:02:29.940]   So that was in 2009, maybe in 10.
[00:02:29.940 --> 00:02:35.340]   And after working in this field, I decided that maybe I should do something more adventurous
[00:02:35.340 --> 00:02:42.700]   in my career and join a team in Los Angeles that essentially was called Google Goggles.
[00:02:42.700 --> 00:02:45.180]   It was not the glass.
[00:02:45.180 --> 00:02:50.060]   It was a little app that did computer vision and we used to use it for experimental computer
[00:02:50.060 --> 00:02:51.780]   vision tasks.
[00:02:51.780 --> 00:02:58.280]   And so there we started experimenting with different applications, well of learning and
[00:02:58.280 --> 00:02:59.720]   deep learning to computer vision.
[00:02:59.720 --> 00:03:03.500]   How can we solve, how do we recognize these objects in these pictures?
[00:03:03.500 --> 00:03:08.500]   And at the time, there was a time when, so I was a tech lead manager of a small team.
[00:03:08.500 --> 00:03:14.620]   There were four of us and half of us did, well, graphical models, deformable parts models.
[00:03:14.620 --> 00:03:20.020]   And you may be familiar when I was a student of Daphne Koller, we did a lot of those.
[00:03:20.020 --> 00:03:24.740]   And then half of us, the other half, we were experimenting with deep learning and that
[00:03:24.740 --> 00:03:27.660]   was Christian Zegedian and Dmitry Erhan.
[00:03:27.660 --> 00:03:31.880]   And so in those early days, the deep learning models at Google were based, were something
[00:03:31.880 --> 00:03:37.780]   that was called QuokkNet, which is a non-convolutional big neural network that Quokk is a student
[00:03:37.780 --> 00:03:39.680]   of Andrei brought.
[00:03:39.680 --> 00:03:44.260]   And for a while we were chancing it with deformable parts models.
[00:03:44.260 --> 00:03:46.860]   And I was working with an intern who later became a Googler.
[00:03:46.860 --> 00:03:52.380]   We had the best deformable parts type detector, even collaborated with Professor Deva Ramanan.
[00:03:52.380 --> 00:03:55.220]   He was also in the LA area at the time.
[00:03:55.220 --> 00:03:58.300]   And so we had a collaboration, built something nice.
[00:03:58.300 --> 00:04:00.700]   And so that's where Christian Zegedian came in.
[00:04:00.700 --> 00:04:03.460]   He was actually on my team.
[00:04:03.460 --> 00:04:08.620]   And for a while, the deformable parts were beating the deep nets.
[00:04:08.620 --> 00:04:14.860]   But then eventually AlexNet came in and then all of a sudden no custom solution could beat
[00:04:14.860 --> 00:04:16.660]   the deep nets.
[00:04:16.660 --> 00:04:19.100]   And so I switched to this, but we were early on this already.
[00:04:19.100 --> 00:04:21.980]   We had people that had been doing this for a while.
[00:04:21.980 --> 00:04:23.460]   And so two interesting things happened.
[00:04:23.460 --> 00:04:29.900]   We started optimizing the architectures because partly actually in Google, that was the easiest
[00:04:29.900 --> 00:04:33.380]   thing to do because the system for training them, it was called disbelief, was pretty
[00:04:33.380 --> 00:04:36.020]   unwieldy and so it couldn't be too smart.
[00:04:36.020 --> 00:04:40.020]   So the easiest thing to do is to just tweak the architecture.
[00:04:40.020 --> 00:04:44.500]   And so we're tweaking it and Christian one day comes to me and says, "Hey Drago, I have
[00:04:44.500 --> 00:04:45.780]   this idea.
[00:04:45.780 --> 00:04:47.900]   It's a Hebbian inspired idea.
[00:04:47.900 --> 00:04:50.460]   I'm going to train this new architecture."
[00:04:50.460 --> 00:04:52.460]   I was like, "Oh, Christian, very nice."
[00:04:52.460 --> 00:04:53.820]   I mean, we could be playing too.
[00:04:53.820 --> 00:04:57.740]   I had some versions of architecture that was one or 2% better or something.
[00:04:57.740 --> 00:04:59.860]   Christian says, "Well, what part do you change?"
[00:04:59.860 --> 00:05:02.140]   I was like, "I'm going to change everything."
[00:05:02.140 --> 00:05:04.300]   I was like, "Oh, that's a great engineering approach.
[00:05:04.300 --> 00:05:08.180]   Aren't you worried that who knows what will happen?"
[00:05:08.180 --> 00:05:09.300]   "No, I have a good intuition.
[00:05:09.300 --> 00:05:10.300]   I'm already training some.
[00:05:10.300 --> 00:05:11.300]   It's doing great."
[00:05:11.300 --> 00:05:14.580]   A bit later, he's like, "Look at this thing.
[00:05:14.580 --> 00:05:18.700]   It's like it beats anything we've ever seen."
[00:05:18.700 --> 00:05:23.180]   And that's when we started, well, decided to also do the ImageNet challenge.
[00:05:23.180 --> 00:05:26.060]   We had this and some detector work as well.
[00:05:26.060 --> 00:05:27.860]   SSD came out of it.
[00:05:27.860 --> 00:05:31.860]   That's also a very strong contribution by Christian.
[00:05:31.860 --> 00:05:36.740]   But he was bald and he decided to just try more ambitious things.
[00:05:36.740 --> 00:05:40.100]   In these early competitions, people still try to do a lot of smart things in the old
[00:05:40.100 --> 00:05:41.100]   style.
[00:05:41.100 --> 00:05:45.980]   They tried to embed known algorithms in the networks instead of making the networks better.
[00:05:45.980 --> 00:05:49.620]   And we, for good or bad, were in the environment where it was the easiest thing we could do
[00:05:49.620 --> 00:05:50.780]   is make the networks better.
[00:05:50.780 --> 00:05:51.780]   And so that's what we did.
[00:05:51.780 --> 00:05:54.480]   And I think that really helped early on.
[00:05:54.480 --> 00:05:59.380]   What was the intuition that he had?
[00:05:59.380 --> 00:06:02.900]   What was the tweak that really made a difference?
[00:06:02.900 --> 00:06:09.200]   I think there is, if you remember the inception architecture, it had this in each module,
[00:06:09.200 --> 00:06:10.740]   there were several paths.
[00:06:10.740 --> 00:06:13.440]   And so one path was do one by one convolution.
[00:06:13.440 --> 00:06:16.540]   So mostly just adding depth processing.
[00:06:16.540 --> 00:06:20.340]   Then it was three by three and five by five convolutions.
[00:06:20.340 --> 00:06:25.340]   And those were adding, expanding the receptive fields.
[00:06:25.340 --> 00:06:28.580]   And then you had the separate channel for each, which kept the model still tractable.
[00:06:28.580 --> 00:06:32.240]   So it's not like number of inputs, then number of outputs.
[00:06:32.240 --> 00:06:37.040]   So you had some, maybe it's something like block diagonal, not quite structure because
[00:06:37.040 --> 00:06:38.200]   I had three channels.
[00:06:38.200 --> 00:06:41.580]   And then you again condensed the information from those.
[00:06:41.580 --> 00:06:44.260]   That was the idea of this is the idea for a block.
[00:06:44.260 --> 00:06:50.020]   And so it's a nice compact way to still add a lot of rich structure and depth.
[00:06:50.020 --> 00:06:52.060]   And so that was, I think, very powerful.
[00:06:52.060 --> 00:06:55.380]   I think if you ask Christian, he'll give you a whole other story why he came up with this
[00:06:55.380 --> 00:06:56.380]   model.
[00:06:56.380 --> 00:06:57.940]   I'm not sure I'm the best person to channel it.
[00:06:57.940 --> 00:07:00.740]   You should invite him.
[00:07:00.740 --> 00:07:03.540]   He did a lot of these early visionary things.
[00:07:03.540 --> 00:07:07.340]   But we all worked on it together and that's how I was part of it.
[00:07:07.340 --> 00:07:10.300]   And the other thing we, actually again, Christian was very involved.
[00:07:10.300 --> 00:07:13.560]   We discovered this was 2013 or so.
[00:07:13.560 --> 00:07:19.380]   So deep nets were used a lot for classification, but not many had used them for detection.
[00:07:19.380 --> 00:07:26.140]   And so Hartmut Neven, who was our director, came and said, "Hey, Christian, I have this
[00:07:26.140 --> 00:07:27.140]   idea."
[00:07:27.140 --> 00:07:28.620]   I think some of it come from Christian.
[00:07:28.620 --> 00:07:31.260]   He's like, "Let's make a better detector.
[00:07:31.260 --> 00:07:36.460]   We just, you know, black propagate the signal through the network and see which parts of
[00:07:36.460 --> 00:07:39.980]   the image caused it to fire that it's a cat.
[00:07:39.980 --> 00:07:43.900]   And then if you do this, you will find where the cat was because the network will highlight
[00:07:43.900 --> 00:07:44.900]   you the cat."
[00:07:44.900 --> 00:07:48.060]   And so we're like, "Oh, that's a cool way to do object detector.
[00:07:48.060 --> 00:07:50.420]   We don't need to, we can mostly use a classifier.
[00:07:50.420 --> 00:07:51.420]   Let's try it."
[00:07:51.420 --> 00:07:57.780]   And so we tried a few versions, but Christian tried it and it's like, "It doesn't work."
[00:07:57.780 --> 00:07:59.020]   And it's like, "Why doesn't it work?"
[00:07:59.020 --> 00:08:00.260]   Well, the image doesn't change.
[00:08:00.260 --> 00:08:01.580]   Now it says to me, it's not a cat.
[00:08:01.580 --> 00:08:04.140]   It's whatever, giraffe, I mean, you name it.
[00:08:04.140 --> 00:08:05.140]   Right.
[00:08:05.140 --> 00:08:06.420]   And we're like, "That's strange."
[00:08:06.420 --> 00:08:07.420]   Right?
[00:08:07.420 --> 00:08:08.420]   And like he debugged for a long time.
[00:08:08.420 --> 00:08:12.940]   I mean, it's also like at the time the system was complicated, the written was not easy
[00:08:12.940 --> 00:08:13.940]   to debug.
[00:08:13.940 --> 00:08:17.100]   Maybe two months he debugged, including trying on MNIST.
[00:08:17.100 --> 00:08:19.140]   You could do the same.
[00:08:19.140 --> 00:08:22.060]   And then eventually we realized, okay, something's happening here.
[00:08:22.060 --> 00:08:24.200]   There's these adversarial examples.
[00:08:24.200 --> 00:08:28.280]   You can just flip the label without much visible, any changes in the image.
[00:08:28.280 --> 00:08:34.420]   But we set off to discover a detector and then ultimately he ended up with the paper.
[00:08:34.420 --> 00:08:38.660]   They bunched several discoveries in that paper, but by far the primary one was the adversarial
[00:08:38.660 --> 00:08:39.660]   examples.
[00:08:39.660 --> 00:08:44.020]   That must have been an exciting moment.
[00:08:44.020 --> 00:08:49.180]   Did it feel like these image tasks were getting better much faster at that time, or did it
[00:08:49.180 --> 00:08:50.740]   feel like a gradual change?
[00:08:50.740 --> 00:08:54.420]   I mean, it was a very exciting time, right?
[00:08:54.420 --> 00:08:59.260]   When a new set of whole new field offers in front of you, let's try to do computer vision
[00:08:59.260 --> 00:09:02.540]   with deep nets and most of it hasn't been done.
[00:09:02.540 --> 00:09:04.740]   And most people are not doing it either.
[00:09:04.740 --> 00:09:05.740]   Right?
[00:09:05.740 --> 00:09:08.380]   I mean, there were a lot of developments at that time.
[00:09:08.380 --> 00:09:10.580]   Like every few months, something pretty major happened.
[00:09:10.580 --> 00:09:15.060]   I mean, strangely enough, this continues.
[00:09:15.060 --> 00:09:19.060]   If you caught a bunch of people in 2015, 16 and say, okay, what's left in computer vision
[00:09:19.060 --> 00:09:21.260]   into the computer vision, how much should we do?
[00:09:21.260 --> 00:09:23.380]   Yeah, we're like, we're pretty good already.
[00:09:23.380 --> 00:09:25.220]   I mean, that's why I went to self-driving.
[00:09:25.220 --> 00:09:30.100]   I was like, okay, 2D computer vision on images is pretty good now, like in 2015.
[00:09:30.100 --> 00:09:31.100]   Let's do cars.
[00:09:31.100 --> 00:09:33.420]   That's a whole other game.
[00:09:33.420 --> 00:09:37.700]   But now early on, there were a lot of big developments like batch normalization came
[00:09:37.700 --> 00:09:41.780]   out, Sergey Ioffe and again, Christian Zagidi were involved.
[00:09:41.780 --> 00:09:42.980]   That's down the line.
[00:09:42.980 --> 00:09:46.740]   I mean, in Google brain, people did a lot of really cool things.
[00:09:46.740 --> 00:09:50.980]   So it was just like, one after the other, there was a group of people.
[00:09:50.980 --> 00:09:56.260]   That was also a time when a lot of academics came to Google to do deep learning.
[00:09:56.260 --> 00:09:57.260]   Right?
[00:09:57.260 --> 00:09:59.340]   Later, a lot of them went back to academia.
[00:09:59.340 --> 00:10:01.100]   They realized they can still do it there for a while.
[00:10:01.100 --> 00:10:03.060]   It's like, we just need to do it in the big companies.
[00:10:03.060 --> 00:10:05.420]   There was a bit of this, at least that was my exposure to it.
[00:10:05.420 --> 00:10:08.380]   Maybe people have different interpretations.
[00:10:08.380 --> 00:10:09.780]   It kind of went back and forth.
[00:10:09.780 --> 00:10:15.100]   Now with what people call foundation models and the big transformer language models, people
[00:10:15.100 --> 00:10:17.500]   say, maybe we should be in the industry again.
[00:10:17.500 --> 00:10:24.940]   But there was a time when people could go back to academia and not feel too deprived.
[00:10:24.940 --> 00:10:28.420]   I think there have been four versions of Inception.
[00:10:28.420 --> 00:10:33.180]   Are people still working on improving these architectures or does it feel like we've kind
[00:10:33.180 --> 00:10:36.260]   of squeezed out all the improvements from that?
[00:10:36.260 --> 00:10:39.380]   I mean, it's moved on.
[00:10:39.380 --> 00:10:44.380]   There is actually a guy in the Waymo research team called Ming-Sheng Tan, who worked with
[00:10:44.380 --> 00:10:47.340]   Kwok Le, the famous Kwok-Net I described.
[00:10:47.340 --> 00:10:48.860]   It was not convolutional.
[00:10:48.860 --> 00:10:55.260]   Of course, not, "Hey, Kwok," I don't mean anything bad.
[00:10:55.260 --> 00:10:58.300]   Kwok's doing great work.
[00:10:58.300 --> 00:11:03.980]   I think Christian just moved away from trying to improve the architecture.
[00:11:03.980 --> 00:11:11.340]   So there were Inception and then I think Francois Chollet, who also was briefly on the team
[00:11:11.340 --> 00:11:15.420]   I led at Google, that was still 15, who did Keras.
[00:11:15.420 --> 00:11:20.300]   He had, I think, Exception, or Nexception, another name.
[00:11:20.300 --> 00:11:22.300]   Yeah, that's in the Keras library.
[00:11:22.300 --> 00:11:23.820]   Yeah, for sure.
[00:11:23.820 --> 00:11:25.260]   So he developed that.
[00:11:25.260 --> 00:11:32.700]   I think afterwards, now people move to the large transformer models, right?
[00:11:32.700 --> 00:11:38.700]   So XCIT and SWIN transformers, I think, and Google.
[00:11:38.700 --> 00:11:43.260]   So Ming-Sheng Tan's own work, there's a model called KoteNet.
[00:11:43.260 --> 00:11:49.260]   So in our times, we top one on ImageNet, we would get maybe 70% accuracy.
[00:11:49.260 --> 00:11:53.460]   And of course, top five, it's a lot better.
[00:11:53.460 --> 00:11:55.060]   People used to score at top five.
[00:11:55.060 --> 00:12:01.860]   Now people can get, I think, 90% top one with a lot of pre-training on large dataset and
[00:12:01.860 --> 00:12:02.860]   other things.
[00:12:02.860 --> 00:12:07.820]   But this KoteNet is a hybrid KoteNet transformer, and it's dramatically bigger than the models
[00:12:07.820 --> 00:12:08.820]   we used to train.
[00:12:08.820 --> 00:12:12.220]   And it's pre-trained on a lot more stuff, potentially.
[00:12:12.220 --> 00:12:16.260]   But people have pushed what's possible in ImageNet a lot further.
[00:12:16.260 --> 00:12:20.180]   Now, I'm not sure how much yet further you can push it, given the inherent limitation
[00:12:20.180 --> 00:12:21.180]   of the dataset.
[00:12:21.180 --> 00:12:22.180]   Right.
[00:12:22.180 --> 00:12:30.260]   But I mean, people are very good at ImageNet with different technology than what we used
[00:12:30.260 --> 00:12:31.380]   to do.
[00:12:31.380 --> 00:12:34.780]   And what's the inter-annotator agreement on ImageNet?
[00:12:34.780 --> 00:12:36.540]   How will the humans do ImageNet?
[00:12:36.540 --> 00:12:37.540]   I have no idea.
[00:12:37.540 --> 00:12:46.860]   I mean, in the old days, Andrej Karpathy, he did a test where he tried to label the
[00:12:46.860 --> 00:12:50.900]   test set after training himself.
[00:12:50.900 --> 00:12:53.420]   And I think the models were competitive with him.
[00:12:53.420 --> 00:12:56.660]   So by now, I think they blow humans out of the water.
[00:12:56.660 --> 00:13:00.540]   Speaking of Andrej Karpathy, actually, it's a small world, but in 2012, when we were doing
[00:13:00.540 --> 00:13:06.580]   deformable models and deep nets, both, actually, I was going there with the story, and then
[00:13:06.580 --> 00:13:08.420]   we went to other directions.
[00:13:08.420 --> 00:13:13.040]   I had the chance to pick either Andrej as an intern to do deep learning, or a guy called
[00:13:13.040 --> 00:13:17.420]   Xinxin Zhu from Devaramanan's lab to do deformable models.
[00:13:17.420 --> 00:13:22.680]   And I picked Xinxin Zhu.
[00:13:22.680 --> 00:13:24.760]   So I never get to work with Andrej.
[00:13:24.760 --> 00:13:25.760]   Maybe to my peril.
[00:13:25.760 --> 00:13:32.620]   Well, I remember I interviewed with you to be your little research assistant as a master's
[00:13:32.620 --> 00:13:37.720]   student and you chose Jimmy Pang, who is a very talented guy.
[00:13:37.720 --> 00:13:38.720]   He's a very talented guy.
[00:13:38.720 --> 00:13:39.720]   I'm sorry.
[00:13:39.720 --> 00:13:40.720]   Don't hold this against me.
[00:13:40.720 --> 00:13:42.800]   No, it's a good choice.
[00:13:42.800 --> 00:13:48.120]   I can't hold that one against you.
[00:13:48.120 --> 00:13:49.120]   Hopefully it worked out for all of us.
[00:13:49.120 --> 00:13:51.960]   It worked out for everyone, yeah.
[00:13:51.960 --> 00:13:58.240]   So I'm really curious about, you've been in autonomous vehicles for quite a while.
[00:13:58.240 --> 00:14:02.840]   And I guess from the outside, it sort of feels like autonomous vehicles are kind of steadily
[00:14:02.840 --> 00:14:03.840]   improving.
[00:14:03.840 --> 00:14:09.160]   It sort of feels inevitable to me, I guess, but so hard to tell.
[00:14:09.160 --> 00:14:14.480]   And really be able to just purchase an autonomous vehicle and ride it.
[00:14:14.480 --> 00:14:20.040]   I'm kind of curious, I'm really curious about what your thoughts are, where things go, but
[00:14:20.040 --> 00:14:25.680]   have there been major breakthroughs in autonomous vehicles in the last 10 years that you've
[00:14:25.680 --> 00:14:27.520]   been working on them?
[00:14:27.520 --> 00:14:31.840]   Or has it been really an iterative process?
[00:14:31.840 --> 00:14:37.760]   So it's an interesting domain because I wasn't at Waymo early on, but people that were at
[00:14:37.760 --> 00:14:42.200]   Waymo were very proud of the demos they can do even 10 years ago, 12 years ago.
[00:14:42.200 --> 00:14:43.200]   Right?
[00:14:43.200 --> 00:14:48.800]   When I was 13 years old, I mean, we've worked on the problem a long time.
[00:14:48.800 --> 00:14:55.000]   As everyone also understands, it's the very rare cases that, the interactive rare cases
[00:14:55.000 --> 00:15:00.360]   that you need to be robust and all the possible failures that you need to be robust that makes
[00:15:00.360 --> 00:15:01.360]   it so hard.
[00:15:01.360 --> 00:15:04.120]   A lot of these improvements are not so easily perceptible.
[00:15:04.120 --> 00:15:11.520]   So in the early times when you sit on the vehicle, it feels pretty good, but you need
[00:15:11.520 --> 00:15:17.280]   sometimes dramatic improvements under the hood to make sure that it's really pretty
[00:15:17.280 --> 00:15:19.280]   good and comparable to humans.
[00:15:19.280 --> 00:15:23.640]   I mean, humans ultimately are pretty good at driving, all things considered, especially
[00:15:23.640 --> 00:15:25.560]   when they pay attention, right?
[00:15:25.560 --> 00:15:30.800]   Which is, yeah, which of course, that's one big advantage of autonomous vehicles, they
[00:15:30.800 --> 00:15:31.960]   always pay attention.
[00:15:31.960 --> 00:15:37.760]   I would say that over the last 10 years, and I'm happy to be part of the process, that's
[00:15:37.760 --> 00:15:43.700]   why, like just in computer vision, but here even more, obviously, I think the entire technology
[00:15:43.700 --> 00:15:45.280]   is being rethought, ground up.
[00:15:45.280 --> 00:15:51.760]   I think machine learning takes constantly more prominent roles and the types of machine
[00:15:51.760 --> 00:15:56.240]   learning and the models we do continue improving at a fast pace.
[00:15:56.240 --> 00:15:58.920]   And so there is a lot of capabilities.
[00:15:58.920 --> 00:16:05.240]   And I think you can see that for a while, maybe there were no notable launches, right?
[00:16:05.240 --> 00:16:07.440]   Even though you would hear about the space.
[00:16:07.440 --> 00:16:09.360]   Now people start launching things, right?
[00:16:09.360 --> 00:16:19.680]   I mean, Waymo launched the first fully driverless service in Phoenix in 2020, right?
[00:16:19.680 --> 00:16:26.200]   And in public, I think we've driven over half a million miles in autonomous mode, right?
[00:16:26.200 --> 00:16:32.600]   In San Francisco, we started driverless operations.
[00:16:32.600 --> 00:16:33.800]   That's another big milestone.
[00:16:33.800 --> 00:16:41.600]   I mean, we're building a stack that can handle car, truck, highway domain cities, but it's
[00:16:41.600 --> 00:16:45.200]   one driver still, but these are deployments we're having.
[00:16:45.200 --> 00:16:47.520]   We announced we will launch downtown Phoenix.
[00:16:47.520 --> 00:16:53.280]   I was on the car actually in San Francisco in a driverless operation maybe 10 days ago.
[00:16:53.280 --> 00:16:54.280]   It's awesome, right?
[00:16:54.280 --> 00:16:58.840]   And I think when you start seeing milestone like this, right, they're meaningful.
[00:16:58.840 --> 00:17:04.760]   Now the truly meaningful milestones is when you release it at large scale, scope and scale,
[00:17:04.760 --> 00:17:05.760]   right?
[00:17:05.760 --> 00:17:10.440]   It's just, you want to do it in a thoughtful manner and make sure that you're confident
[00:17:10.440 --> 00:17:15.200]   when you put these things out there that they interact well with people and are safe for
[00:17:15.200 --> 00:17:17.920]   everybody.
[00:17:17.920 --> 00:17:25.040]   When you say that the autonomous vehicles have been really redesigned and machine learning
[00:17:25.040 --> 00:17:29.440]   takes a more prominent role, could you give me a flavor of what the trends are?
[00:17:29.440 --> 00:17:36.560]   Are things moving to a more end-to-end system where the inputs are a camera and the output
[00:17:36.560 --> 00:17:38.160]   is which way to turn the steering wheel?
[00:17:38.160 --> 00:17:43.400]   Are things becoming more componentized where each piece is responsible for something?
[00:17:43.400 --> 00:17:47.640]   What are the big trends over the last 10 years?
[00:17:47.640 --> 00:17:53.360]   The main trend I would say, and I've tried as a leader of the research team at Waymo,
[00:17:53.360 --> 00:17:58.560]   which does pretty much primarily ML, almost exclusively, right?
[00:17:58.560 --> 00:18:05.640]   But we started maybe applying to perception and then prediction and understanding behavior
[00:18:05.640 --> 00:18:09.160]   and then planning and then in the simulation, right?
[00:18:09.160 --> 00:18:13.880]   I think it's permeated every aspect of the system, on board, off board.
[00:18:13.880 --> 00:18:17.080]   There's machine learning in all these components.
[00:18:17.080 --> 00:18:23.440]   I think, I mean, not major models, meaning they're not just small features, they're core
[00:18:23.440 --> 00:18:25.280]   parts of the system.
[00:18:25.280 --> 00:18:30.920]   And I think on the macro level, that's a change that's definitely happened at Waymo.
[00:18:30.920 --> 00:18:38.720]   I think when people started early on in 2009, there was the famous Sebastian Trundbog, it's
[00:18:38.720 --> 00:18:41.440]   like probabilistic robotics, right?
[00:18:41.440 --> 00:18:45.360]   There you have the LiDAR, you can create all these segments out of the LiDAR, then you
[00:18:45.360 --> 00:18:47.800]   can reason about the segments, you can build...
[00:18:47.800 --> 00:18:52.680]   Like initially people without the deep learning models to build a very modular stack with
[00:18:52.680 --> 00:18:57.400]   very many modules, each does a little something, you put them all together, it's a significant
[00:18:57.400 --> 00:18:59.360]   engineering challenge.
[00:18:59.360 --> 00:19:03.660]   And the trend has been larger and larger neural nets, right?
[00:19:03.660 --> 00:19:07.720]   Doing more and more, potentially going from neural nets in narrow scope to neural nets
[00:19:07.720 --> 00:19:09.520]   in wider scopes.
[00:19:09.520 --> 00:19:15.040]   Maybe narrow nets from one task to neural nets to do multiple tasks.
[00:19:15.040 --> 00:19:21.880]   So the trend is for the modules with the help of machine learning models to grow larger
[00:19:21.880 --> 00:19:24.680]   and fewer.
[00:19:24.680 --> 00:19:31.160]   Now there is an interesting question, and this is an area of exciting research and not
[00:19:31.160 --> 00:19:38.880]   only, I think in the industry, some companies espouse a fully end-to-end learned approach.
[00:19:38.880 --> 00:19:44.720]   And there is no clarity yet if fully end-to-end learned system is actually better.
[00:19:44.720 --> 00:19:52.400]   There is in life, I think when you build these things, there is often trade-offs between
[00:19:52.400 --> 00:19:53.920]   different extremes, right?
[00:19:53.920 --> 00:19:59.520]   So each of these things has its pluses and its minuses, and you want somehow to take
[00:19:59.520 --> 00:20:04.480]   advantage of the pluses, but not to be stung too much by the minuses.
[00:20:04.480 --> 00:20:09.160]   And so we were maybe too much on the end of too modular system, too many small pieces
[00:20:09.160 --> 00:20:11.100]   written by engineers.
[00:20:11.100 --> 00:20:16.320]   Whether the answer is several or large modules or a single end-to-end thing, I think is an
[00:20:16.320 --> 00:20:17.320]   open question.
[00:20:17.320 --> 00:20:22.840]   I think this is an area that we're still, I mean, as a research team, we're exploring
[00:20:22.840 --> 00:20:27.000]   the repercussions of these things, but I mean, the industry is exploring because people have
[00:20:27.000 --> 00:20:30.340]   different vision for some of these things, right?
[00:20:30.340 --> 00:20:33.600]   But I don't think, I would not say there's some serious trade-offs to doing everything
[00:20:33.600 --> 00:20:35.560]   end-to-end, not in academia, by the way.
[00:20:35.560 --> 00:20:41.920]   If you take an academic dataset and you train more end-to-end in the small scope, you will
[00:20:41.920 --> 00:20:46.960]   probably do better, but that does not mean you built a better system in the production
[00:20:46.960 --> 00:20:47.960]   setting.
[00:20:47.960 --> 00:20:48.960]   Why is that?
[00:20:48.960 --> 00:20:51.760]   What are the pitfalls?
[00:20:51.760 --> 00:20:57.240]   I think ultimately, academic setting, you look for a lot more like average metrics and
[00:20:57.240 --> 00:20:59.160]   things, right?
[00:20:59.160 --> 00:21:01.800]   And the dataset is small.
[00:21:01.800 --> 00:21:06.940]   So clearly if you build something that incorporates everything and co-trains, you will probably
[00:21:06.940 --> 00:21:09.760]   do better, especially if you optimize on it.
[00:21:09.760 --> 00:21:14.300]   In the production setting, you're looking to be robust to the very rare cases.
[00:21:14.300 --> 00:21:20.360]   You look at speed of iteration and ability for people to fix your model if there's issues,
[00:21:20.360 --> 00:21:21.360]   right?
[00:21:21.360 --> 00:21:26.880]   You look at the stability, like understanding there is issues, being able to dig in.
[00:21:26.880 --> 00:21:29.800]   I mean, simulation requirements too, right?
[00:21:29.800 --> 00:21:34.720]   So if you have fully end-to-end model, now you need to, your simulation has to be end-to-end
[00:21:34.720 --> 00:21:38.000]   and you need to simulate all the sensors as needed and so on.
[00:21:38.000 --> 00:21:43.160]   That's maybe a lot more expensive than some intermediate representation that may be simpler
[00:21:43.160 --> 00:21:44.160]   to simulate.
[00:21:44.160 --> 00:21:50.760]   Maybe it does not pass all the information the model may want to pass, but at the same
[00:21:50.760 --> 00:21:52.680]   time you'll get other benefits.
[00:21:52.680 --> 00:21:54.000]   Maybe you can train closed loop, right?
[00:21:54.000 --> 00:21:57.040]   A lot faster that now also can help you.
[00:21:57.040 --> 00:22:00.800]   And so there's very interesting trade-offs in this space.
[00:22:00.800 --> 00:22:07.040]   I think one thing that I've really noticed from my vantage point of selling tooling to
[00:22:07.040 --> 00:22:14.560]   autonomous vehicles is how many customers there are for labeled data and then weights
[00:22:14.560 --> 00:22:16.440]   and biases stuff.
[00:22:16.440 --> 00:22:21.720]   Do you think it's a kind of a wasted effort that so many different smart teams are tackling
[00:22:21.720 --> 00:22:23.720]   the same problem?
[00:22:23.720 --> 00:22:28.240]   Do you feel like there's a diversity of approaches that's interesting?
[00:22:28.240 --> 00:22:32.840]   Does Waymo have a specific point of view that's different than Zooks or other places that
[00:22:32.840 --> 00:22:34.000]   you know of?
[00:22:34.000 --> 00:22:38.440]   I think if you start looking at the stack, first of all, you don't really know.
[00:22:38.440 --> 00:22:42.040]   I don't know exactly what the stacks of the other companies, right?
[00:22:42.040 --> 00:22:43.720]   They're proprietary.
[00:22:43.720 --> 00:22:49.440]   I think there is an interesting search space where you're saying, "I'm going to design
[00:22:49.440 --> 00:22:51.000]   this system.
[00:22:51.000 --> 00:22:52.880]   It will have these APIs.
[00:22:52.880 --> 00:22:55.200]   These are the intermediate outputs.
[00:22:55.200 --> 00:22:56.560]   This is how I build my tooling.
[00:22:56.560 --> 00:22:59.200]   This is how I understand how the system is doing.
[00:22:59.200 --> 00:23:01.360]   This is how I iterate on each of them."
[00:23:01.360 --> 00:23:07.440]   That's why these representations are beneficial, say, for onboard perception, onboard performance,
[00:23:07.440 --> 00:23:10.280]   or for example, in simulation, right?
[00:23:10.280 --> 00:23:12.920]   You take all this into consideration, it's a very wide search space.
[00:23:12.920 --> 00:23:19.840]   I think every company ends up with somewhat different APIs, design choices, trade-offs,
[00:23:19.840 --> 00:23:23.520]   and how modular versus not trade-offs, and how much machine learning they put versus
[00:23:23.520 --> 00:23:24.960]   not.
[00:23:24.960 --> 00:23:27.520]   This is very understudied because ultimately it's hard.
[00:23:27.520 --> 00:23:34.320]   I think in research, it's a lot easier to study every one problem in isolation, and
[00:23:34.320 --> 00:23:36.840]   you can say, "Okay, let's do 3D flow prediction."
[00:23:36.840 --> 00:23:41.000]   We have some state of the art 3D flow prediction, or monocular depth, others do.
[00:23:41.000 --> 00:23:45.360]   I think when you start combining them, there is actually a lot of variability possible,
[00:23:45.360 --> 00:23:48.880]   and people's stacks end up quite different in there.
[00:23:48.880 --> 00:23:54.320]   Even if on a high level, you can say they're somewhat in a similar way.
[00:23:54.320 --> 00:23:55.800]   Interesting.
[00:23:55.800 --> 00:24:07.420]   I guess, do you feel like there's still deep problems to solve between now and everyone
[00:24:07.420 --> 00:24:10.840]   riding in autonomous vehicles?
[00:24:10.840 --> 00:24:13.120]   I mean, scalability is always a problem, right?
[00:24:13.120 --> 00:24:18.360]   Safely, cheaply scaling to say, I always think, what system do I need to build to scale to
[00:24:18.360 --> 00:24:21.000]   a dozen cities cheaply?
[00:24:21.000 --> 00:24:26.120]   But wait, I want to understand that because with a normal piece of software, you wouldn't
[00:24:26.120 --> 00:24:30.800]   only deploy it in San Francisco and Phoenix.
[00:24:30.800 --> 00:24:36.160]   If it's safe in one city, it'd be safe in every location.
[00:24:36.160 --> 00:24:41.760]   What makes it hard to, I don't know, go to LA and have the same thing deployed, or go
[00:24:41.760 --> 00:24:44.720]   to Boston and deploy it?
[00:24:44.720 --> 00:24:47.560]   Why does it have to be one city at a time versus...
[00:24:47.560 --> 00:24:50.600]   Usually software goes everywhere at once.
[00:24:50.600 --> 00:24:58.160]   I think we're at the point where we're building software that should deploy most places and
[00:24:58.160 --> 00:25:00.520]   be pretty good at it.
[00:25:00.520 --> 00:25:06.400]   Maybe historically with the probabilistic robotics stuff, that wasn't quite the case.
[00:25:06.400 --> 00:25:10.240]   Still, you need to validate that and be sure.
[00:25:10.240 --> 00:25:16.680]   I think there is a lot of local particularities in every location that you should make sure
[00:25:16.680 --> 00:25:17.680]   you can handle.
[00:25:17.680 --> 00:25:21.960]   Like there's some strange roundabout in this place, and there is some eight way intersection
[00:25:21.960 --> 00:25:26.960]   in this other place, and maybe here in Pittsburgh, they do the Pittsburgh left that you need
[00:25:26.960 --> 00:25:28.520]   to understand.
[00:25:28.520 --> 00:25:33.600]   So someone needs to go still, collect data from these places, potentially tune the models
[00:25:33.600 --> 00:25:39.240]   from these places, potentially then do the safety analysis to convince yourself you actually
[00:25:39.240 --> 00:25:40.240]   should deploy.
[00:25:40.240 --> 00:25:41.240]   And that is work.
[00:25:41.240 --> 00:25:46.040]   That's why you don't just build it once and, "Okay, let's just drive and see what happens."
[00:25:46.040 --> 00:25:52.960]   You can do that, but if you actually remove the driver, I'm not sure how responsible that
[00:25:52.960 --> 00:25:53.960]   it is.
[00:25:53.960 --> 00:25:54.960]   All right.
[00:25:54.960 --> 00:26:01.080]   But I mean, Google has actually mapped every city on the planet, it feels like.
[00:26:01.080 --> 00:26:04.280]   Shouldn't it be possible to send some cars and collect data?
[00:26:04.280 --> 00:26:07.680]   I mean, we are sending cars and collecting data.
[00:26:07.680 --> 00:26:09.420]   So we're growing our scope.
[00:26:09.420 --> 00:26:15.560]   So now we have Chandler in Phoenix, now San Francisco we announced.
[00:26:15.560 --> 00:26:21.440]   Now downtown Phoenix we announced that we will deploy starting this year.
[00:26:21.440 --> 00:26:28.720]   We're collecting data if there's been public postings in New York, say in the winter and
[00:26:28.720 --> 00:26:29.720]   in Los Angeles.
[00:26:29.720 --> 00:26:34.560]   And of course we now have trucks, collecting highway data for trucks and behavior around
[00:26:34.560 --> 00:26:37.280]   trucks, which is important, I think.
[00:26:37.280 --> 00:26:41.960]   There's somewhat different behaviors around trucks and different issues like seeing around
[00:26:41.960 --> 00:26:45.520]   the trailer, for example, that you don't have with the car and you have somewhat different
[00:26:45.520 --> 00:26:47.020]   sense of configuration.
[00:26:47.020 --> 00:26:48.800]   So we're broadening.
[00:26:48.800 --> 00:26:53.760]   I think the car is, and I agree with you, if you look at every city as one deployment
[00:26:53.760 --> 00:26:57.920]   and one piece of software and you just develop it and launch it in that city, I mean, that
[00:26:57.920 --> 00:26:58.920]   doesn't scale.
[00:26:58.920 --> 00:27:03.140]   So the way we think of it, we're building one driver if possible.
[00:27:03.140 --> 00:27:07.760]   This driver is able to handle all these environments as possible, including potentially as much
[00:27:07.760 --> 00:27:13.040]   as possible cars and trucks, even though there will be some small differences, but the core
[00:27:13.040 --> 00:27:16.160]   of all pieces is similar, right?
[00:27:16.160 --> 00:27:18.400]   In the nature of what they want to do.
[00:27:18.400 --> 00:27:25.160]   And then you iterate and when you're comfortable that you have enough data about safety and
[00:27:25.160 --> 00:27:30.680]   passing the bar, you launch.
[00:27:30.680 --> 00:27:39.780]   Do you have an opinion on LIDAR versus vision only approaches?
[00:27:39.780 --> 00:27:43.240]   By the way, maybe one more topic on the previous question.
[00:27:43.240 --> 00:27:46.620]   I think if you ask what are the big open questions, right?
[00:27:46.620 --> 00:27:52.820]   I think one of the interesting topics is, and this is a scaling factor for you, you
[00:27:52.820 --> 00:27:58.180]   want more machine learning in the planner if possible, and you want a realistic simulation
[00:27:58.180 --> 00:28:07.820]   environment where you can just replay full system scenarios and without too much human
[00:28:07.820 --> 00:28:11.400]   involvement determine whether you're improving or not improving.
[00:28:11.400 --> 00:28:17.220]   So for us, the big challenge is it's a very complex endeavor because essentially it's
[00:28:17.220 --> 00:28:22.060]   not like someone gave you the perfect simulator for autonomous vehicles.
[00:28:22.060 --> 00:28:25.820]   So you need to build one and ideally you build one from the sensor data and the data you
[00:28:25.820 --> 00:28:29.820]   collected so that's real to sim.
[00:28:29.820 --> 00:28:33.180]   And now, well, by what metrics do you build the simulator?
[00:28:33.180 --> 00:28:37.700]   You need to establish metrics for the simulator that constitute acceptable simulation.
[00:28:37.700 --> 00:28:40.420]   And for us, simulation, a lot of it is about behavior of agents.
[00:28:40.420 --> 00:28:46.620]   It's not just how something looks, even though we like the network too.
[00:28:46.620 --> 00:28:50.460]   We've done NERF and 3D reconstruction and all kinds of things.
[00:28:50.460 --> 00:28:54.460]   But ultimately the behavior system of the main things you need to solve.
[00:28:54.460 --> 00:28:57.220]   So you need realistic behavior in the simulator.
[00:28:57.220 --> 00:29:00.060]   Then when you have that, then you have the other metrics, which says, what does it mean
[00:29:00.060 --> 00:29:02.300]   to drive well in the simulator in the world?
[00:29:02.300 --> 00:29:03.300]   So you need both.
[00:29:03.300 --> 00:29:05.500]   You need to build both things.
[00:29:05.500 --> 00:29:11.380]   And the further you go, the easier it is to improve these pieces because the less you
[00:29:11.380 --> 00:29:13.060]   need humans in the loop.
[00:29:13.060 --> 00:29:14.340]   We can still improve them.
[00:29:14.340 --> 00:29:17.700]   You don't need perfect realistic simulator to improve your driving.
[00:29:17.700 --> 00:29:22.300]   It's just it requires more human judgment.
[00:29:22.300 --> 00:29:29.340]   But there is a process in these areas, a lot is possible still.
[00:29:29.340 --> 00:29:33.060]   And we hopefully will show more interesting work this year.
[00:29:33.060 --> 00:29:38.140]   We sent a couple of papers in the space that people may find interesting.
[00:29:38.140 --> 00:29:39.140]   Cool.
[00:29:39.140 --> 00:29:43.340]   And that might even be applicable to things outside of autonomous vehicles.
[00:29:43.340 --> 00:29:49.100]   It seems like sim to real type stuff is necessary for any kind of robotics application.
[00:29:49.100 --> 00:29:51.820]   Yeah, real to sim in some sense.
[00:29:51.820 --> 00:29:57.940]   I mean, the specific instantiation is maybe a little different, but I would say that that's
[00:29:57.940 --> 00:30:02.420]   one of the nice properties of AVs is that it is a complete robotics problem.
[00:30:02.420 --> 00:30:08.260]   Maybe it's of a specific kind, but a lot of the things you need to solve for other robotics
[00:30:08.260 --> 00:30:13.420]   problems are at least in some shape covered.
[00:30:13.420 --> 00:30:18.460]   And so there will be hopefully a lot of positive spillover from our domain to others.
[00:30:18.460 --> 00:30:22.660]   Okay, well, a couple of practical questions that everyone on the team wanted me to ask
[00:30:22.660 --> 00:30:24.460]   you if you could talk about it.
[00:30:24.460 --> 00:30:33.260]   I guess, do you have an opinion on LIDAR and more complicated sensors versus vision only
[00:30:33.260 --> 00:30:34.260]   approaches?
[00:30:34.260 --> 00:30:39.500]   Do you think LIDAR will always be needed to make things safe?
[00:30:39.500 --> 00:30:49.540]   I think ultimately it's a question of, I mean, I don't know if LIDAR will always be needed,
[00:30:49.540 --> 00:30:51.500]   but I think it's really great.
[00:30:51.500 --> 00:30:55.020]   I know it's not very expensive, right?
[00:30:55.020 --> 00:30:59.180]   And I think it even makes your computer vision much better and it makes your simulation much
[00:30:59.180 --> 00:31:02.380]   better, which then immediately also results in better driving.
[00:31:02.380 --> 00:31:07.060]   I mean, it's a fantastic sensor that you can just have for now.
[00:31:07.060 --> 00:31:09.580]   So why not have it?
[00:31:09.580 --> 00:31:13.300]   I think there is this convergence happening in some sense, like LIDAR is becoming more
[00:31:13.300 --> 00:31:14.300]   like cameras.
[00:31:14.300 --> 00:31:17.860]   It's higher and higher res, maybe even can do the passive lighting.
[00:31:17.860 --> 00:31:22.100]   So then it is a camera also while being a LIDAR and it's cheaper and cheaper with the
[00:31:22.100 --> 00:31:24.300]   current technologies.
[00:31:24.300 --> 00:31:30.100]   On the other hand, obviously our 3D perception cameras, even compared to two years ago, is
[00:31:30.100 --> 00:31:33.260]   dramatically better.
[00:31:33.260 --> 00:31:35.220]   I really like having the LIDAR to me.
[00:31:35.220 --> 00:31:38.220]   It's a safety feature.
[00:31:38.220 --> 00:31:41.600]   It's a lot safer having being in the car with LIDAR than not.
[00:31:41.600 --> 00:31:44.980]   Maybe it's theoretically possible to just do it with cameras and maybe it will play
[00:31:44.980 --> 00:31:47.260]   out, but do you want to risk it and why?
[00:31:47.260 --> 00:31:52.300]   I mean, it's easy to remove LIDAR, no one's stopping you, right?
[00:31:52.300 --> 00:31:55.440]   It's not like we don't have state of the art camera approaches.
[00:31:55.440 --> 00:31:57.180]   It's easy to remove maps too, right?
[00:31:57.180 --> 00:31:58.780]   Yeah, that was my next question.
[00:31:58.780 --> 00:31:59.780]   Yeah.
[00:31:59.780 --> 00:32:01.860]   I mean, how critical do you think the mapping is?
[00:32:01.860 --> 00:32:04.680]   Because that doesn't seem scalable necessarily, right?
[00:32:04.680 --> 00:32:05.680]   It's pretty scalable.
[00:32:05.680 --> 00:32:12.500]   I mean, you can do mapping with machine learning in some sense if you design it properly.
[00:32:12.500 --> 00:32:14.700]   Generally maps are prior.
[00:32:14.700 --> 00:32:20.420]   They tell you about an environment, especially environment you drove a lot in, what to expect,
[00:32:20.420 --> 00:32:23.580]   what is behind this occlusion, right?
[00:32:23.580 --> 00:32:27.900]   What did you, when you looked at this intersection, what does this thing really tell you to do
[00:32:27.900 --> 00:32:28.900]   versus not?
[00:32:28.900 --> 00:32:31.240]   Or what to expect around that corner?
[00:32:31.240 --> 00:32:34.060]   If you can have some of this information, why not use it, right?
[00:32:34.060 --> 00:32:36.340]   I mean, it's safer.
[00:32:36.340 --> 00:32:41.360]   Now should you trust the map as is and require it is correct, right?
[00:32:41.360 --> 00:32:42.360]   That's not scalable.
[00:32:42.360 --> 00:32:46.580]   If you say the map is given to me, I need to maintain it true, otherwise I can't drive.
[00:32:46.580 --> 00:32:52.740]   I mean, you cannot deploy autonomous driving at scale then, right?
[00:32:52.740 --> 00:32:53.740]   You don't have a business.
[00:32:53.740 --> 00:32:58.220]   I mean, people do construction, put cones, they change the traffic lights, they repaint
[00:32:58.220 --> 00:33:02.060]   things on the highways where the trucks drive, they reroute lanes.
[00:33:02.060 --> 00:33:03.980]   I mean, you need to deal with this.
[00:33:03.980 --> 00:33:08.060]   Otherwise, I mean, you don't have a business ultimately in the end, right?
[00:33:08.060 --> 00:33:12.140]   So you can't trust maps blindly, but then why not have a prior?
[00:33:12.140 --> 00:33:16.860]   I mean, we drive a city and even to do the safety case or just to collect data to understand
[00:33:16.860 --> 00:33:19.260]   what people do, why not have a map prior?
[00:33:19.260 --> 00:33:20.260]   Right.
[00:33:20.260 --> 00:33:26.020]   I mean, do you think it helps enough that there sort of will be one winner in autonomous
[00:33:26.020 --> 00:33:31.380]   vehicles that everyone uses, then it gets better because it collects the map data?
[00:33:31.380 --> 00:33:33.540]   Is it that much of an advantage?
[00:33:33.540 --> 00:33:38.740]   Which the map data, I think generally there is scaling benefits in autonomous driving.
[00:33:38.740 --> 00:33:44.540]   And I think a lot of the scaling benefits, they accrue when you use large machine learning
[00:33:44.540 --> 00:33:45.540]   models, right?
[00:33:45.540 --> 00:33:49.420]   I mean, you see the extreme case with GPT-3 and the big language model.
[00:33:49.420 --> 00:33:54.820]   Like in our days, when we started with Daphne Koller, we learned that there is a bias variance
[00:33:54.820 --> 00:33:59.900]   trade off and you want the outcome razor, you want to penalize models that have high
[00:33:59.900 --> 00:34:03.060]   expressivity and you will get the best generalization, right?
[00:34:03.060 --> 00:34:07.820]   So the simplest model that explains your data is great, right?
[00:34:07.820 --> 00:34:10.140]   Probably better than some fancy overfitting model.
[00:34:10.140 --> 00:34:11.500]   Now all of this is on its head.
[00:34:11.500 --> 00:34:15.180]   You say, I want to train the huge model that's much, much bigger than anything on tons of
[00:34:15.180 --> 00:34:18.920]   data that may be the same as mine or different, right?
[00:34:18.920 --> 00:34:23.500]   And that model will generalize better for me, right?
[00:34:23.500 --> 00:34:25.500]   Now in AVs, what does this mean?
[00:34:25.500 --> 00:34:32.260]   Well, we have all this data, Waymo has more data than vast majority of companies of different
[00:34:32.260 --> 00:34:33.260]   platforms.
[00:34:33.260 --> 00:34:37.700]   I mean, we're 13 years driven, 20 million miles in autonomous mode, right?
[00:34:37.700 --> 00:34:40.700]   We have whatever, 20 billion miles in simulation.
[00:34:40.700 --> 00:34:43.380]   Simulation is also data.
[00:34:43.380 --> 00:34:45.660]   Now we have cars and trucks.
[00:34:45.660 --> 00:34:48.940]   All of these things, if you take the large machine learning point of view, makes the
[00:34:48.940 --> 00:34:51.380]   models better because you have more data.
[00:34:51.380 --> 00:34:52.380]   It's more diverse data.
[00:34:52.380 --> 00:34:55.860]   It captures, well, we tried to say everything that you could see.
[00:34:55.860 --> 00:34:59.660]   If you do your job well, these models will actually generalize better.
[00:34:59.660 --> 00:35:03.300]   It helps you having cars to do well on trucks.
[00:35:03.300 --> 00:35:05.140]   And I have all this great car data, right?
[00:35:05.140 --> 00:35:08.980]   Like you added to the models for the truck and it helps a lot.
[00:35:08.980 --> 00:35:13.100]   And car data is a lot cheaper to collect than truck too, and maybe a lot more diverse.
[00:35:13.100 --> 00:35:18.460]   I mean, often on the highways, being a truck, you drive fairly conservatively, right?
[00:35:18.460 --> 00:35:24.100]   And fairly few things happen on the highway, but it's a multiplier for you, the multi-platform
[00:35:24.100 --> 00:35:26.660]   setting, right?
[00:35:26.660 --> 00:35:29.500]   So our domain is friendly to this, I think.
[00:35:29.500 --> 00:35:30.500]   Right, right.
[00:35:30.500 --> 00:35:36.540]   I mean, it seems, I guess, could you talk a little bit about why Waymo is investing
[00:35:36.540 --> 00:35:37.540]   in trucks?
[00:35:37.540 --> 00:35:43.420]   It seems to me like a different enough domain, like more different than a different city
[00:35:43.420 --> 00:35:47.460]   that I could imagine my first thought would be, well, you'd probably get the cities working
[00:35:47.460 --> 00:35:51.580]   first of the car and then switch to a truck, but it must not be.
[00:35:51.580 --> 00:35:53.860]   Could you talk about that?
[00:35:53.860 --> 00:35:57.660]   So I think there's some difference between the two, but ultimately most of the pieces
[00:35:57.660 --> 00:36:00.500]   are similar enough that you can share.
[00:36:00.500 --> 00:36:03.580]   You can share roughly the same modular design.
[00:36:03.580 --> 00:36:06.460]   You can share roughly similar types of models.
[00:36:06.460 --> 00:36:10.220]   You can share roughly the same types of simulation environments.
[00:36:10.220 --> 00:36:15.140]   You can benefit by cross-pollinating the data between the two domains.
[00:36:15.140 --> 00:36:18.020]   For example, to understand how others behave, right?
[00:36:18.020 --> 00:36:21.220]   Like I mean, you can just collect data of how people behave with cars.
[00:36:21.220 --> 00:36:23.980]   It will generalize to trucks, right?
[00:36:23.980 --> 00:36:29.020]   There's some unique problems with trucks that do have to be solved.
[00:36:29.020 --> 00:36:35.740]   One of them is, well, you need to see a lot further for a truck, partly because a fully
[00:36:35.740 --> 00:36:38.660]   loaded truck takes a while to stop.
[00:36:38.660 --> 00:36:44.540]   Also if you want to change lanes for a truck, sometimes you need to create gaps, right?
[00:36:44.540 --> 00:36:49.860]   And it takes longer to create gaps and do it without cutting people off unnecessarily
[00:36:49.860 --> 00:36:51.200]   than for a car.
[00:36:51.200 --> 00:36:56.020]   And so you need to anticipate a lot sooner that now, you know, maybe, and you need to
[00:36:56.020 --> 00:37:00.000]   see around the trailer or be smarter how you infer what's behind your trailer.
[00:37:00.000 --> 00:37:04.020]   So there are a few of these problems and you know, that's why I have a bit different sense
[00:37:04.020 --> 00:37:05.280]   of configuration.
[00:37:05.280 --> 00:37:09.820]   But if you look at the core pieces, a lot of the other logic, like which models you
[00:37:09.820 --> 00:37:13.960]   would put together, what to put in each model is very similar.
[00:37:13.960 --> 00:37:15.640]   All the infrastructure is similar.
[00:37:15.640 --> 00:37:19.940]   Now trucking is a very big use case, right?
[00:37:19.940 --> 00:37:21.460]   It's a big market.
[00:37:21.460 --> 00:37:25.300]   And so it makes sense from that standpoint.
[00:37:25.300 --> 00:37:31.020]   There is enough cross-pollination and commonality, more so than differences, I would say.
[00:37:31.020 --> 00:37:32.020]   Okay.
[00:37:32.020 --> 00:37:37.820]   Another question I wanted to ask, maybe you get this all the time, but such a common adversarial
[00:37:37.820 --> 00:37:44.140]   example is like, you know, slightly modifying street signs to make a system think it's a
[00:37:44.140 --> 00:37:45.340]   different sign.
[00:37:45.340 --> 00:37:49.420]   Is that like a toy thing that doesn't really come up and doesn't really kind of cross your
[00:37:49.420 --> 00:37:51.580]   mind as a major problem?
[00:37:51.580 --> 00:37:55.460]   Or is that like something that you actually really worry about trying to create autonomous
[00:37:55.460 --> 00:37:56.460]   vehicles?
[00:37:56.460 --> 00:38:01.700]   So in our case, we have three different sensors, right?
[00:38:01.700 --> 00:38:05.660]   And I don't think you can filter different sensors nearly as easily independently.
[00:38:05.660 --> 00:38:08.740]   Well, furthermore, we have redundancy between the sensors, right?
[00:38:08.740 --> 00:38:14.180]   When you want, part of the beauty of having active sensors is, I mean, one of them can
[00:38:14.180 --> 00:38:19.780]   fail and they can still fairly independently detect things for you.
[00:38:19.780 --> 00:38:25.100]   And so from that standpoint, a hybrid stack with multiple different sensors is more robust.
[00:38:25.100 --> 00:38:26.100]   That's one.
[00:38:26.100 --> 00:38:32.160]   Second, I think generally this adversarial problems fall in the bucket of robustness
[00:38:32.160 --> 00:38:35.540]   and in some sense, unsupervised domain adaptation.
[00:38:35.540 --> 00:38:39.700]   You want to generalize to similar situations.
[00:38:39.700 --> 00:38:43.060]   And in research, we have studied these topics.
[00:38:43.060 --> 00:38:51.780]   We have methods currently that, you know, we've investigated that help against either
[00:38:51.780 --> 00:38:54.180]   transferring from one domain to another.
[00:38:54.180 --> 00:38:57.220]   There is a paper called SPG that we put up.
[00:38:57.220 --> 00:39:03.460]   That's an interesting take on essentially adding more structure to your prediction task,
[00:39:03.460 --> 00:39:05.940]   to make it more robust to new conditions.
[00:39:05.940 --> 00:39:10.180]   Like you're trying to say in sunny weather, then you want to work in rainy weather.
[00:39:10.180 --> 00:39:14.220]   Turns out that instead of just regressing 3D boxes, if you first have an intermediate
[00:39:14.220 --> 00:39:18.300]   task that regularizes, predicts your point clouds and fills it in.
[00:39:18.300 --> 00:39:22.860]   And then from that, from this canonicalized, regularized point cloud, now you predict your
[00:39:22.860 --> 00:39:23.860]   box.
[00:39:23.860 --> 00:39:27.300]   Turns out you get a lot of robustness.
[00:39:27.300 --> 00:39:30.100]   And we did it with unsupervised domain adaptation in mind.
[00:39:30.100 --> 00:39:35.020]   By the way, in the Waymo open data set, we made a release some data for people to study
[00:39:35.020 --> 00:39:36.020]   this.
[00:39:36.020 --> 00:39:40.220]   We can talk maybe more about this later about the open data set.
[00:39:40.220 --> 00:39:44.900]   So we did it with this in mind and then we realized, oh, this method is actually number
[00:39:44.900 --> 00:39:47.020]   one on the KITTI leaderboard.
[00:39:47.020 --> 00:39:50.220]   KITTI is one of the, for hard detection cases.
[00:39:50.220 --> 00:39:52.500]   In its time, that was maybe a year ago.
[00:39:52.500 --> 00:39:55.860]   And that's because when you do well, now KITTI is a small data set.
[00:39:55.860 --> 00:39:57.780]   There's rare examples.
[00:39:57.780 --> 00:40:02.340]   When you add robustness to domain adaptation and you do it well, it just happens to do
[00:40:02.340 --> 00:40:05.380]   well on these examples, on more of these hard examples.
[00:40:05.380 --> 00:40:11.300]   And so these are techniques that we are exploring and we have at this point significant experience
[00:40:11.300 --> 00:40:12.300]   with.
[00:40:12.300 --> 00:40:17.900]   Both the adversarial techniques, there's actually a large space of them.
[00:40:17.900 --> 00:40:24.220]   There is a challenge, like many techniques make you more robust to adversarial cases,
[00:40:24.220 --> 00:40:28.140]   but really hurt your performance in nominal cases.
[00:40:28.140 --> 00:40:35.500]   And the challenge is to find robustness methods to train your model such that you don't regress
[00:40:35.500 --> 00:40:36.500]   the common cases.
[00:40:36.500 --> 00:40:41.180]   If anything, you get better and you get more robust to the adversarial attacks.
[00:40:41.180 --> 00:40:42.180]   And there are such methods.
[00:40:42.180 --> 00:40:48.700]   Well, that's a good segue into something I want to ask you about, which is the open data
[00:40:48.700 --> 00:40:49.700]   sets that you've been releasing.
[00:40:49.700 --> 00:40:53.420]   I mean, could you maybe first of all talk about what they are?
[00:40:53.420 --> 00:40:58.380]   And I'd love to hear the motivation and what's been surprising and the reaction from the
[00:40:58.380 --> 00:41:03.420]   community after putting them out.
[00:41:03.420 --> 00:41:10.060]   I would say that when I joined Waymo in 2018 and we started the research team, which is
[00:41:10.060 --> 00:41:17.340]   applied research team internally, most of our work actually is primarily focused at
[00:41:17.340 --> 00:41:21.980]   improving Waymo systems with machine learning and with Widow Publish too, a good amount,
[00:41:21.980 --> 00:41:24.460]   but not all our work you see, right?
[00:41:24.460 --> 00:41:26.740]   We're not just made for academia.
[00:41:26.740 --> 00:41:30.780]   So we wanted to engage better the community.
[00:41:30.780 --> 00:41:35.460]   And then the question is, well, how do I collaborate with you?
[00:41:35.460 --> 00:41:40.180]   Or how do we encourage you to work on certain problems?
[00:41:40.180 --> 00:41:44.020]   And at the time, especially when we started planning the data set, there was the kitty
[00:41:44.020 --> 00:41:52.380]   data set, which by modern measures, it was done, I think in 2010, 2012 was tiny.
[00:41:52.380 --> 00:41:58.060]   And so then we thought, okay, well, the best way to encourage people to solve problems
[00:41:58.060 --> 00:42:06.820]   relevant to our setup, which is a lot more data and the problems we're interested in,
[00:42:06.820 --> 00:42:08.300]   let's start releasing data, right?
[00:42:08.300 --> 00:42:11.180]   That people can just push the state of the art with, right?
[00:42:11.180 --> 00:42:13.220]   That's what the community does not have.
[00:42:13.220 --> 00:42:18.540]   And so we released what I believe is still one of the largest and richest data sets.
[00:42:18.540 --> 00:42:21.980]   And we're actively making it better and better and better.
[00:42:21.980 --> 00:42:26.100]   And if you checked it out two years ago, even come back and see the kind of things we have
[00:42:26.100 --> 00:42:28.700]   now and we will continue releasing interesting data.
[00:42:28.700 --> 00:42:33.020]   So we have 3D bounding boxes over time, 2D bounding boxes over time.
[00:42:33.020 --> 00:42:35.540]   Now we have 3D semantic segmentation.
[00:42:35.540 --> 00:42:41.420]   We have 2D and 3D post key points for people in busy scenes type of data that has very
[00:42:41.420 --> 00:42:46.420]   little other data sets you can see in the wild of such data.
[00:42:46.420 --> 00:42:48.740]   We have a bunch of interesting challenges.
[00:42:48.740 --> 00:42:54.020]   So one of the interesting things is, so we released the perception data set and we picked
[00:42:54.020 --> 00:42:58.940]   2000 run sequences, which in its time was quite a lot, right?
[00:42:58.940 --> 00:43:04.540]   So 2000, 20 second sequences compared to anything else, it's a humongous amount of data.
[00:43:04.540 --> 00:43:07.660]   And then we started trying to do behavior prediction task with it.
[00:43:07.660 --> 00:43:12.620]   And if you do this, you realize that for behavior prediction, you need an order of magnitude
[00:43:12.620 --> 00:43:13.620]   yet more data.
[00:43:13.620 --> 00:43:14.620]   Why?
[00:43:14.620 --> 00:43:19.660]   Because say a scene of 20 seconds, each has 200 objects and you may be at 10 Hertz in
[00:43:19.660 --> 00:43:21.140]   our data set, right?
[00:43:21.140 --> 00:43:26.040]   That's tens of thousands of instances that you observed over this 20 seconds.
[00:43:26.040 --> 00:43:29.500]   And maybe you will see one interesting interaction or not, right?
[00:43:29.500 --> 00:43:31.680]   In the whole sequence.
[00:43:31.680 --> 00:43:36.820]   And so from this standpoint, then like, okay, well, what is a reasonable size of data for
[00:43:36.820 --> 00:43:41.340]   behavior understanding and understanding interesting interactions?
[00:43:41.340 --> 00:43:47.700]   And it came up, okay, well, if we had 2000 perception sequences, you want 100,000 behavior
[00:43:47.700 --> 00:43:49.620]   sequences.
[00:43:49.620 --> 00:43:54.380]   And then of course, then the question is, okay, well, if you release the sensor data
[00:43:54.380 --> 00:43:57.220]   for all of these, how are people even going to download it?
[00:43:57.220 --> 00:44:00.460]   And so then we did some very interesting things.
[00:44:00.460 --> 00:44:06.900]   We released vectorized data of the environment produced by our sensors by actually novel
[00:44:06.900 --> 00:44:11.300]   systems we have, a system called auto labeling, which I think is pretty key for the autonomous
[00:44:11.300 --> 00:44:17.660]   driving space, which in hindsight, after you observe the whole scene, you can try to as
[00:44:17.660 --> 00:44:21.680]   perfectly as possible to create everything that happened, right?
[00:44:21.680 --> 00:44:22.940]   And we have novel work on this.
[00:44:22.940 --> 00:44:26.100]   It's published maybe a year ago or two years ago.
[00:44:26.100 --> 00:44:31.860]   And with this work, we actually made our data set and it's still probably state of the art
[00:44:31.860 --> 00:44:33.700]   of what you can do with these models.
[00:44:33.700 --> 00:44:35.900]   It's very clean data of a kind that was never done.
[00:44:35.900 --> 00:44:40.100]   And so you can study aspects you could not before.
[00:44:40.100 --> 00:44:43.860]   And have people engaged with it in ways that were unexpected?
[00:44:43.860 --> 00:44:46.900]   Has it been useful to you?
[00:44:46.900 --> 00:44:53.340]   People come up with very powerful models, which is part of the appeal.
[00:44:53.340 --> 00:44:58.820]   And you have people from industry, from academia, even kids from high school in some cases,
[00:44:58.820 --> 00:45:05.780]   like one of our challenges, which is really impressive to see just the broad, it's worldwide
[00:45:05.780 --> 00:45:07.220]   reach.
[00:45:07.220 --> 00:45:11.540]   What's interesting is we release it with some problems in mind and we help try to suggest
[00:45:11.540 --> 00:45:12.540]   problems.
[00:45:12.540 --> 00:45:17.960]   And the way we try to suggest problems is, we've been running challenges for three years
[00:45:17.960 --> 00:45:19.860]   straight with prizes.
[00:45:19.860 --> 00:45:21.820]   And so we say, here's a problem.
[00:45:21.820 --> 00:45:25.460]   Here's a metric we believe is suitable for this problem.
[00:45:25.460 --> 00:45:26.460]   Right?
[00:45:26.460 --> 00:45:28.900]   Like, please submit.
[00:45:28.900 --> 00:45:29.900]   Here's the leaderboard.
[00:45:29.900 --> 00:45:30.900]   Here's you can submit.
[00:45:30.900 --> 00:45:36.260]   If you do well, you can win and come to our workshop.
[00:45:36.260 --> 00:45:40.660]   This year we also have a workshop at CVPR, one of the two premier computer vision conferences
[00:45:40.660 --> 00:45:42.520]   you get to present.
[00:45:42.520 --> 00:45:43.980]   And people participate.
[00:45:43.980 --> 00:45:48.340]   And every year we expand the set of challenges that we have.
[00:45:48.340 --> 00:45:52.900]   So this year we have three completely new challenges.
[00:45:52.900 --> 00:45:55.900]   Some are really unique that have not been run.
[00:45:55.900 --> 00:46:02.140]   So occupancy prediction, future occupancy prediction, both for occluded and unoccluded
[00:46:02.140 --> 00:46:03.700]   agents with flow.
[00:46:03.700 --> 00:46:06.540]   That has, there's a few such challenges.
[00:46:06.540 --> 00:46:12.540]   We have one on, actually from five cameras over time, can you reconstruct the 3D boxes
[00:46:12.540 --> 00:46:15.300]   accurately?
[00:46:15.300 --> 00:46:20.140]   Like there is variants of this for a single camera, but for multiple cameras over time
[00:46:20.140 --> 00:46:25.100]   with rolling shutter, which is a real setup on the car, we worked out some very interesting
[00:46:25.100 --> 00:46:28.360]   metrics and setup that has not been done before.
[00:46:28.360 --> 00:46:29.360]   That is very core.
[00:46:29.360 --> 00:46:35.100]   Like I think that, I mean, a lot of people do appreciate, I think the releases, and I
[00:46:35.100 --> 00:46:38.620]   think the more we release, the more different research people can do, because now they can
[00:46:38.620 --> 00:46:44.140]   study how all of these enrich each other and how the perception and motion data set, they
[00:46:44.140 --> 00:46:46.260]   have certain compatibility.
[00:46:46.260 --> 00:46:52.020]   You can reason how to combine some of these and it gives you a lot of opportunity.
[00:46:52.020 --> 00:46:57.020]   But the last point is people started solving problems we haven't thought of with this data
[00:46:57.020 --> 00:47:00.380]   or do different research, including ourselves.
[00:47:00.380 --> 00:47:05.420]   So for example, you can use our data to say, train NERF models.
[00:47:05.420 --> 00:47:09.700]   I mean, we have all this rich data from all over the place.
[00:47:09.700 --> 00:47:10.700]   You could do that.
[00:47:10.700 --> 00:47:14.140]   You can train 3D reconstruction models, right?
[00:47:14.140 --> 00:47:15.540]   You can do shape completion models.
[00:47:15.540 --> 00:47:20.620]   I mean, there is a lot of things you can do when you have such rich data with these 3D,
[00:47:20.620 --> 00:47:22.580]   when we release two sensors, camera and lighter.
[00:47:22.580 --> 00:47:27.180]   If you have camera and lighter in interesting environments, you can do it.
[00:47:27.180 --> 00:47:28.180]   Cool.
[00:47:28.180 --> 00:47:35.180]   Is it a challenge to convince the business that it's a useful thing to release this stuff?
[00:47:35.180 --> 00:47:39.460]   Is there objections that like there's IP that might kind of leak out or even like privacy
[00:47:39.460 --> 00:47:41.620]   issues possible?
[00:47:41.620 --> 00:47:43.340]   There were objections.
[00:47:43.340 --> 00:47:49.500]   I think ultimately people, I'm thankful and WAME was a great place to have a research
[00:47:49.500 --> 00:47:50.500]   team.
[00:47:50.500 --> 00:47:55.300]   I think it's a great collaborative environment with people that really appreciate the value
[00:47:55.300 --> 00:47:58.180]   it can bring, especially in an open-ended field.
[00:47:58.180 --> 00:48:01.220]   I think you can really balance the concerns, right?
[00:48:01.220 --> 00:48:07.820]   I don't think with us releasing the open data set, it will give such a huge leg to the competition
[00:48:07.820 --> 00:48:14.300]   because we released some data for people to study problems in this space, right?
[00:48:14.300 --> 00:48:18.100]   I think ultimately it's really helpful to everyone, but it's not defining.
[00:48:18.100 --> 00:48:22.580]   I think there's a lot more positives for everybody than worries for WAME.
[00:48:22.580 --> 00:48:26.340]   By releasing it, we hopefully struck a good balance.
[00:48:26.340 --> 00:48:28.540]   And it has been a lot of work.
[00:48:28.540 --> 00:48:35.380]   I mean, ultimately we want to release data and at the quality that befits the WAME brand.
[00:48:35.380 --> 00:48:41.040]   That means that we need to take, say, blurring all the faces license plates well.
[00:48:41.040 --> 00:48:46.780]   We need to make sure that the annotations are very high quality, which they are.
[00:48:46.780 --> 00:48:53.060]   We really paid a lot of attention and we ran models to keep mining for potential errors
[00:48:53.060 --> 00:48:55.900]   in our 2D and 3D annotations.
[00:48:55.900 --> 00:48:58.080]   I think they are very high quality.
[00:48:58.080 --> 00:49:00.580]   And so hopefully people can benefit from that.
[00:49:00.580 --> 00:49:07.340]   Well, look, we always end with two open-ended questions that I'd love to try before you
[00:49:07.340 --> 00:49:08.340]   go.
[00:49:08.340 --> 00:49:13.340]   So what do you think is kind of an understudied part of machine learning or something that
[00:49:13.340 --> 00:49:17.500]   you would want to look into if you had more time?
[00:49:17.500 --> 00:49:26.900]   Well, I would say that I'm perfectly happy doing the problems we have because ultimately
[00:49:26.900 --> 00:49:31.220]   they cover most machine learning problems are represented in our domain.
[00:49:31.220 --> 00:49:32.900]   I would say a few.
[00:49:32.900 --> 00:49:38.900]   So one of the fascinating areas that we're looking at is, and AV is really stressed,
[00:49:38.900 --> 00:49:40.660]   is you want robust systems.
[00:49:40.660 --> 00:49:42.700]   And we touched on this.
[00:49:42.700 --> 00:49:44.140]   So what does that mean?
[00:49:44.140 --> 00:49:46.340]   And this means many things and it depends which system.
[00:49:46.340 --> 00:49:51.580]   So one of them is you want to build inductive bias and structure in the, if you think of
[00:49:51.580 --> 00:49:56.860]   the whole thing as one big architecture, you want to build the right structure so it generalizes.
[00:49:56.860 --> 00:50:03.380]   This means pick the right API, pick the right designs and representations.
[00:50:03.380 --> 00:50:08.660]   There is a certain flow in our models, which I think now became a lot more popular in the
[00:50:08.660 --> 00:50:10.500]   whole ML community.
[00:50:10.500 --> 00:50:16.380]   You go from perspective view with tens of millions of points, scans, you name it.
[00:50:16.380 --> 00:50:26.740]   And then you create a Euclidean space, maybe in top down view with ultimately, how would
[00:50:26.740 --> 00:50:32.900]   I say, with objects, with relations, with polylines or structure.
[00:50:32.900 --> 00:50:36.100]   And in that one, models generalize a lot better.
[00:50:36.100 --> 00:50:37.420]   So you want to do more of this.
[00:50:37.420 --> 00:50:38.420]   That's one.
[00:50:38.420 --> 00:50:42.940]   The other one, which we touched on very briefly, but a big part of it is when you train the
[00:50:42.940 --> 00:50:50.940]   systems and make them robust, you need to be able to detect the rare examples.
[00:50:50.940 --> 00:50:54.220]   So why do you want to detect?
[00:50:54.220 --> 00:50:58.580]   If you detect the rare examples, you can of course bias your training set and metrics
[00:50:58.580 --> 00:51:01.180]   to make sure you do well on them.
[00:51:01.180 --> 00:51:08.420]   When you drive, if you know, you don't know, it's already a huge help because machine learning
[00:51:08.420 --> 00:51:12.620]   models, you can think of them, they're very performant when you trust them.
[00:51:12.620 --> 00:51:17.100]   And if you don't trust them, you can fall to something a lot more cautious and safe.
[00:51:17.100 --> 00:51:19.780]   You just need to know when.
[00:51:19.780 --> 00:51:25.500]   And so there's a lot of techniques you can study to do this.
[00:51:25.500 --> 00:51:29.980]   We can talk about finding rare examples if we get to it, but we have a whole bunch of
[00:51:29.980 --> 00:51:30.980]   research on this.
[00:51:30.980 --> 00:51:32.660]   We can maybe after with me.
[00:51:32.660 --> 00:51:36.220]   There's another one that I find fascinating and we touched on.
[00:51:36.220 --> 00:51:39.060]   This is domain gap between simulation and real world.
[00:51:39.060 --> 00:51:46.700]   How and what should be the simulation such that you can train the best possible autonomous
[00:51:46.700 --> 00:51:47.700]   vehicle stack?
[00:51:47.700 --> 00:51:50.620]   How do I build it from the data I collected?
[00:51:50.620 --> 00:51:54.780]   What are the metrics for the simulator that it should optimize as realism?
[00:51:54.780 --> 00:51:57.940]   And then how do you put planning agents in it?
[00:51:57.940 --> 00:51:59.460]   I think that is a fascinating-
[00:51:59.460 --> 00:52:01.740]   Can you give me some examples of results in that?
[00:52:01.740 --> 00:52:05.180]   I'm not familiar with that work.
[00:52:05.180 --> 00:52:07.340]   So there are several things you can do.
[00:52:07.340 --> 00:52:09.140]   There are several aspects of realism.
[00:52:09.140 --> 00:52:16.780]   So you can think of it when you put your vehicle in the simulator, you want to produce inputs
[00:52:16.780 --> 00:52:23.260]   to the vehicle that are similar or highly similar to what you see in the real world.
[00:52:23.260 --> 00:52:26.620]   Then the outcomes in the simulator are pertinent.
[00:52:26.620 --> 00:52:31.980]   What are the inputs to your vehicle?
[00:52:31.980 --> 00:52:37.540]   It's sensor data and it's the behavior of other agents in the simulator.
[00:52:37.540 --> 00:52:39.620]   So these are the two main axes.
[00:52:39.620 --> 00:52:42.100]   Some kind of sensor data or perception realism.
[00:52:42.100 --> 00:52:45.540]   Maybe you do some intermediate representation that's a lot cheaper than simulating every
[00:52:45.540 --> 00:52:48.180]   pixel but you need something.
[00:52:48.180 --> 00:52:52.300]   And now you need agents to behave realistically, meaning they react to you.
[00:52:52.300 --> 00:52:53.300]   Agents need to react to you.
[00:52:53.300 --> 00:52:58.620]   Like you do something different, the simulator needs to cause an effect.
[00:52:58.620 --> 00:52:59.620]   There's a reaction.
[00:52:59.620 --> 00:53:01.500]   It needs to be reasonable.
[00:53:01.500 --> 00:53:03.620]   Now how reasonable does it need to be?
[00:53:03.620 --> 00:53:04.620]   Well it varies.
[00:53:04.620 --> 00:53:09.820]   Even so as you know, like there is a strong work on domain randomization in other domains.
[00:53:09.820 --> 00:53:15.580]   If you want to train a more robust model, they can even try somewhat unreasonable things.
[00:53:15.580 --> 00:53:19.500]   As long as there's enough of them, you can build a more robust model.
[00:53:19.500 --> 00:53:25.300]   In our domain, you also want simulator to ideally be a good measure of risk.
[00:53:25.300 --> 00:53:26.780]   And that's a higher requirement.
[00:53:26.780 --> 00:53:32.660]   You need taller level for what realistic means because it needs to be somehow correlated
[00:53:32.660 --> 00:53:34.940]   to the real rates.
[00:53:34.940 --> 00:53:36.940]   But how would you even know that though?
[00:53:36.940 --> 00:53:44.620]   If it's reacting to what the agent does, how do you quantify how good your simulation is?
[00:53:44.620 --> 00:53:48.140]   The agent might do something that you never saw in the real world.
[00:53:48.140 --> 00:53:52.740]   How could you even know if the simulation is realistic?
[00:53:52.740 --> 00:53:58.900]   So there's two measures in which you can measure realism of agents that we think and we've
[00:53:58.900 --> 00:54:01.060]   presented in past talks.
[00:54:01.060 --> 00:54:04.180]   One of them is during test of sorts, you look at the scene.
[00:54:04.180 --> 00:54:06.500]   It's like, could this agent have done this?
[00:54:06.500 --> 00:54:10.180]   Is it likely or completely impossible?
[00:54:10.180 --> 00:54:11.180]   That's one.
[00:54:11.180 --> 00:54:14.140]   So that's a proof of existence that it's realistic.
[00:54:14.140 --> 00:54:19.940]   Then you have distributional realism, which is, let's say, how often someone will cut
[00:54:19.940 --> 00:54:25.020]   off in front of you, or what is the breaking profile, or how long does someone take to
[00:54:25.020 --> 00:54:27.700]   pay attention to you?
[00:54:27.700 --> 00:54:33.260]   That is the type of useful distributional realism that you can enforce.
[00:54:33.260 --> 00:54:38.500]   And this makes sure that agents behave at least on the distributional level, similar
[00:54:38.500 --> 00:54:39.500]   to what you observe.
[00:54:39.500 --> 00:54:41.500]   Now, we observe a ton of behavior.
[00:54:41.500 --> 00:54:46.780]   So we have enough data to know roughly what the distribution of these things is.
[00:54:46.780 --> 00:54:51.060]   One of the challenges is, agents acting in a continuous space, it's somehow practically
[00:54:51.060 --> 00:54:54.300]   an infinite distribution.
[00:54:54.300 --> 00:54:59.860]   But you can take slices of it that are meaningful and enforce that those are matching.
[00:54:59.860 --> 00:55:03.140]   So there's certain design there that you need to build in.
[00:55:03.140 --> 00:55:08.220]   I would imagine there's parts of the distribution that you might care about, but it would be
[00:55:08.220 --> 00:55:12.300]   dangerous to even do in the real world, but you might really care.
[00:55:12.300 --> 00:55:17.860]   What happens if you slam on the brakes or make a hard turn?
[00:55:17.860 --> 00:55:22.900]   You can play any future you want, in theory, if you build it right.
[00:55:22.900 --> 00:55:29.140]   I think that simulator has this huge scaling promise.
[00:55:29.140 --> 00:55:35.140]   You take any scenario you saw, and you release the agents, and you release yourself, and
[00:55:35.140 --> 00:55:38.980]   you can try all kinds of stuff, and they can try all kinds of stuff.
[00:55:38.980 --> 00:55:40.700]   And you can learn from that.
[00:55:40.700 --> 00:55:47.780]   So it multiplies your data via, if you have good models for the agents, now you have 100x
[00:55:47.780 --> 00:55:50.100]   multiplier on everything.
[00:55:50.100 --> 00:55:51.100]   And that's fascinating.
[00:55:51.100 --> 00:55:55.820]   And maybe if you can score roughly how likely each future is, then you have even a likelihood
[00:55:55.820 --> 00:55:56.820]   estimate.
[00:55:56.820 --> 00:56:01.660]   You can sample adversarially and bias yourself towards the interesting cases.
[00:56:01.660 --> 00:56:05.420]   Try someone to try to cut in front of you when you're riding.
[00:56:05.420 --> 00:56:08.300]   Most of the times they don't, and maybe 1% they will.
[00:56:08.300 --> 00:56:10.100]   Then you see what happens if they try.
[00:56:10.100 --> 00:56:15.820]   There's different ways to build it, but you have opportunity if you do it right to really
[00:56:15.820 --> 00:56:19.620]   dramatically increase, say, the cases of collisions you can replay.
[00:56:19.620 --> 00:56:25.560]   Because we don't see that many collisions, thank God, even when we drive a lot.
[00:56:25.560 --> 00:56:30.420]   But I can make you a lot in the simulator, and some will be more realistic than others,
[00:56:30.420 --> 00:56:35.140]   and it makes you a nice area to study these things safely, of course.
[00:56:35.140 --> 00:56:39.500]   It's the best if that's where you study them.
[00:56:39.500 --> 00:56:47.660]   Do you have more thoughts on finding unusual examples?
[00:56:47.660 --> 00:56:49.460]   Active learning has been around for a long time.
[00:56:49.460 --> 00:56:54.060]   It's something that I think most companies use when they want to actually deploy something
[00:56:54.060 --> 00:56:56.420]   to production.
[00:56:56.420 --> 00:57:01.660]   Are you talking about active learning or something more complicated here?
[00:57:01.660 --> 00:57:03.900]   I will say a few things.
[00:57:03.900 --> 00:57:07.940]   Some of them are papers of ours, observations.
[00:57:07.940 --> 00:57:12.980]   Basically, our domain is ripe for finding the rare examples.
[00:57:12.980 --> 00:57:15.220]   It's one of the main tasks you need to do.
[00:57:15.220 --> 00:57:22.060]   Most of the time you drive, it should be boring, and you need to find, and we collect a ton
[00:57:22.060 --> 00:57:23.620]   of data, which is great.
[00:57:23.620 --> 00:57:29.100]   Almost the setting is you have some proxy for infinite unlabeled data, and you have
[00:57:29.100 --> 00:57:30.660]   some labeling budget, of course.
[00:57:30.660 --> 00:57:35.700]   You can label yourself some data as you run a labeling company.
[00:57:35.700 --> 00:57:43.620]   Now how do you benefit the most from this data you just collect?
[00:57:43.620 --> 00:57:46.340]   Most of the examples are somewhere if you can find them.
[00:57:46.340 --> 00:57:51.100]   That's the first observation.
[00:57:51.100 --> 00:57:52.100]   That's one.
[00:57:52.100 --> 00:57:56.900]   Now, if you were to find them, you can data augment a lot out of them.
[00:57:56.900 --> 00:57:57.900]   That's a good way to go.
[00:57:57.900 --> 00:58:02.100]   We have papers on how to perturb them in different ways.
[00:58:02.100 --> 00:58:03.100]   You can do this for cameras.
[00:58:03.100 --> 00:58:04.100]   You can do it for ladder.
[00:58:04.100 --> 00:58:10.420]   You can even machine learn how to best perturb them to get best results with that work.
[00:58:10.420 --> 00:58:19.140]   I think I'll get to you in a second about ways to find rare examples.
[00:58:19.140 --> 00:58:22.460]   There is a long tail learning literature, typically.
[00:58:22.460 --> 00:58:26.620]   A lot of the long tail literature was done, I mean, driven in academia by data sets such
[00:58:26.620 --> 00:58:32.020]   as ImageNet or some like, I don't know, is it the Bergs data set?
[00:58:32.020 --> 00:58:33.020]   There is one.
[00:58:33.020 --> 00:58:34.020]   We used to do it.
[00:58:34.020 --> 00:58:36.940]   Small data set.
[00:58:36.940 --> 00:58:42.460]   We used to do at Google when I worked for Google goggles, breeds of dogs and types of
[00:58:42.460 --> 00:58:46.820]   birds and types of food and all kinds of things.
[00:58:46.820 --> 00:58:52.140]   Typically the literature on long tail is driven by these rich semantic data sets that you
[00:58:52.140 --> 00:58:59.540]   have some very rare thing, like, I don't know, a rare breed of bird or a rare breed of plant.
[00:58:59.540 --> 00:59:03.420]   Then you need to detect it with five examples.
[00:59:03.420 --> 00:59:06.700]   But that is a world in which everything was named.
[00:59:06.700 --> 00:59:11.180]   And so just this name was rare and you just had five examples.
[00:59:11.180 --> 00:59:14.060]   So let's maybe learn to do the most with them.
[00:59:14.060 --> 00:59:16.020]   That's one way.
[00:59:16.020 --> 00:59:17.860]   Now in our world, it's a little different.
[00:59:17.860 --> 00:59:23.740]   So in autonomous driving, you don't want to name every type of plant or even every type
[00:59:23.740 --> 00:59:24.740]   of dog.
[00:59:24.740 --> 00:59:26.420]   So you have fairly broad category.
[00:59:26.420 --> 00:59:29.340]   Take the category vehicle, I mean, to an extreme.
[00:59:29.340 --> 00:59:34.940]   There's all kinds of vehicles in the category vehicle and 80% of it will be like boring
[00:59:34.940 --> 00:59:37.600]   sedans say.
[00:59:37.600 --> 00:59:42.140]   And then down the line, you can have all kinds of strange configurations of things people
[00:59:42.140 --> 00:59:43.140]   do, right?
[00:59:43.140 --> 00:59:46.700]   Cement mixer with a trailer or something or trams on the road.
[00:59:46.700 --> 00:59:48.100]   I mean, you can have.
[00:59:48.100 --> 00:59:52.140]   So now in this big bucket, like what is a rare example?
[00:59:52.140 --> 00:59:54.860]   You don't want to name it.
[00:59:54.860 --> 00:59:56.680]   And rare is not the same as hard.
[00:59:56.680 --> 00:59:59.180]   That's also this property.
[00:59:59.180 --> 01:00:01.140]   So I'll give you an intuition.
[01:00:01.140 --> 01:00:06.340]   And I think the people sometimes say, oh, we're going to train an ensemble.
[01:00:06.340 --> 01:00:09.300]   And where the ensemble disagrees, we'll just label, right?
[01:00:09.300 --> 01:00:10.620]   That's very standard.
[01:00:10.620 --> 01:00:11.740]   Now what's the problem there?
[01:00:11.740 --> 01:00:14.980]   Well, the ensemble finds hard examples.
[01:00:14.980 --> 01:00:17.100]   Models disagree.
[01:00:17.100 --> 01:00:18.860]   Not easy to tell what it is.
[01:00:18.860 --> 01:00:23.620]   That doesn't mean it's actually, first of all, rare, second beneficial to label.
[01:00:23.620 --> 01:00:27.580]   And you can see this intuition, maybe the easiest in lighter perception.
[01:00:27.580 --> 01:00:29.500]   So you do lighter perception.
[01:00:29.500 --> 01:00:34.980]   If you do ensemble mining, we've studied this, actually a great guy in research called Max
[01:00:34.980 --> 01:00:36.300]   studied this.
[01:00:36.300 --> 01:00:40.380]   You'll get cars in parking lots far away.
[01:00:40.380 --> 01:00:41.780]   They have five lighter points.
[01:00:41.780 --> 01:00:46.740]   The models clearly disagree how the bounding box should be.
[01:00:46.740 --> 01:00:49.540]   But that's not a very useful example.
[01:00:49.540 --> 01:00:53.020]   It's not like if you mine more examples with five lighter points, you'll get much better
[01:00:53.020 --> 01:00:55.700]   on them, right?
[01:00:55.700 --> 01:00:59.620]   And so you need some mechanism to tell rare from hard.
[01:00:59.620 --> 01:01:00.620]   So what do you do?
[01:01:00.620 --> 01:01:01.620]   What's the intuition?
[01:01:01.620 --> 01:01:08.260]   Well, you can build, for example, a model that estimates given features of the examples,
[01:01:08.260 --> 01:01:12.260]   a distribution and check which actually rare things versus one we've seen a lot, right?
[01:01:12.260 --> 01:01:16.940]   And we have paper on this still unpublished, so I will not say more, but it hopefully will
[01:01:16.940 --> 01:01:19.100]   be soon.
[01:01:19.100 --> 01:01:24.180]   Obviously that's one way to think about it, but I just thought it's an interesting distinction.
[01:01:24.180 --> 01:01:28.940]   There's another work we did, which is called GradTail that is published.
[01:01:28.940 --> 01:01:30.940]   And that's a bit of the same idea.
[01:01:30.940 --> 01:01:40.500]   It's like, let's define long tail as kind of uncertainty related to the model or the
[01:01:40.500 --> 01:01:41.700]   task you're doing.
[01:01:41.700 --> 01:01:47.740]   It's not so much that some class is long tail somewhere, right?
[01:01:47.740 --> 01:01:52.580]   So it's epistemic uncertainty related to the model you're training.
[01:01:52.580 --> 01:01:55.340]   And what does that mean really?
[01:01:55.340 --> 01:02:00.700]   And again, this comes to say, actually Zhao Chen, who is the author of that, primary author
[01:02:00.700 --> 01:02:02.340]   of that paper has this reasoning.
[01:02:02.340 --> 01:02:08.220]   Well, some rare kind of apple is really important and relevant when you try to tell the types
[01:02:08.220 --> 01:02:09.220]   of apples.
[01:02:09.220 --> 01:02:16.140]   But if you try to tell apples versus oranges, that's maybe not even the relevant case, right?
[01:02:16.140 --> 01:02:22.020]   So we have a definition of long tail, which is if an example, when it train it, has a
[01:02:22.020 --> 01:02:29.260]   gradient that is orthogonal or different from the mean gradient for the class.
[01:02:29.260 --> 01:02:33.180]   Sorry, how do you define a gradient for label?
[01:02:33.180 --> 01:02:40.900]   I mean, you can back prop the gradient for an example, there's some layer and you can
[01:02:40.900 --> 01:02:47.620]   check on average what examples from that class as a gradient around this time.
[01:02:47.620 --> 01:02:54.380]   And if you have different gradient, and actually, so it can be either orthogonal or negative.
[01:02:54.380 --> 01:03:01.700]   So you can argue which one is which, but if you have different enough gradient, it's ultimately
[01:03:01.700 --> 01:03:03.500]   long tail example.
[01:03:03.500 --> 01:03:06.460]   And sometimes you find different examples that semantics does not give you.
[01:03:06.460 --> 01:03:11.620]   For example, these types of examples have maybe a class fridge, but the fridge is open.
[01:03:11.620 --> 01:03:15.020]   It has some strange point of view.
[01:03:15.020 --> 01:03:16.460]   That's a rare example.
[01:03:16.460 --> 01:03:18.020]   The class is common, right?
[01:03:18.020 --> 01:03:20.300]   You can predict depth or regress something.
[01:03:20.300 --> 01:03:25.220]   The rare example can be the depths of points far away that are close to something occluded.
[01:03:25.220 --> 01:03:28.620]   Like you can't even name these things, but that doesn't mean that they don't exist.
[01:03:28.620 --> 01:03:31.380]   The long tail in semantics are different concepts.
[01:03:31.380 --> 01:03:37.300]   And so we've explored this a bit because it's relevant to our domain.
[01:03:37.300 --> 01:03:42.860]   Is the intuition that these change your model the most?
[01:03:42.860 --> 01:03:46.660]   I mean, the rare examples, they will improve the model the most because you can actually
[01:03:46.660 --> 01:03:49.660]   learn it if you feed it.
[01:03:49.660 --> 01:03:53.660]   As opposed to the hard one, I mean, you can feed it, but model will waste capacity trying
[01:03:53.660 --> 01:03:56.940]   to solve something that's hard to solve in the first place.
[01:03:56.940 --> 01:03:58.340]   But when mining it, it matters.
[01:03:58.340 --> 01:04:02.860]   Now, of course, there is a whole method point, which is what is an example you should mine?
[01:04:02.860 --> 01:04:09.380]   If you have a full on integrated eval of the whole system, you can try to introspect in
[01:04:09.380 --> 01:04:10.380]   the system.
[01:04:10.380 --> 01:04:16.460]   And at that point, the example you should mine is the one that caused you troubles downstream.
[01:04:16.460 --> 01:04:18.260]   So that's the optimal world in which you mine.
[01:04:18.260 --> 01:04:24.100]   The problem is that now it's complicated because it couples all your system and evaluation.
[01:04:24.100 --> 01:04:27.660]   So if your evaluation is perfect, you should do it.
[01:04:27.660 --> 01:04:33.300]   If not, then some of these simpler, more modular approaches give you a lot of the benefit with
[01:04:33.300 --> 01:04:34.300]   a lot simpler set.
[01:04:34.300 --> 01:04:35.300]   Cool.
[01:04:35.300 --> 01:04:36.300]   All right.
[01:04:36.300 --> 01:04:41.820]   Well, I want to make sure I get my last question in, which is basically what's the hardest
[01:04:41.820 --> 01:04:44.300]   part of making this stuff work in production?
[01:04:44.300 --> 01:04:51.540]   When you think about from just soup to nuts, making autonomous vehicles work, what's the
[01:04:51.540 --> 01:04:55.140]   step that's most unexpectedly challenging?
[01:04:55.140 --> 01:04:56.140]   So which question is this?
[01:04:56.140 --> 01:04:57.580]   Is this from research to production?
[01:04:57.580 --> 01:04:58.940]   What's hard or what is hard about it?
[01:04:58.940 --> 01:05:00.140]   Research to production, exactly.
[01:05:00.140 --> 01:05:04.420]   So you have something working in research or something like working in the open dataset
[01:05:04.420 --> 01:05:08.300]   challenge or a thing that works in a Kaggle competition, and now you need to make the
[01:05:08.300 --> 01:05:09.820]   car go.
[01:05:09.820 --> 01:05:12.620]   Where does this break down the most?
[01:05:12.620 --> 01:05:13.620]   I'll tell you.
[01:05:13.620 --> 01:05:21.780]   Actually, my first experience with this is I was at Street View and that was maybe 2007.
[01:05:21.780 --> 01:05:28.020]   I learned how to say automatically calibrate the camera on the car to the IMU and the GPS
[01:05:28.020 --> 01:05:29.020]   system.
[01:05:29.020 --> 01:05:30.020]   Every once in a while they were miscalibrated.
[01:05:30.020 --> 01:05:33.020]   The panoramas were all crooked in strange ways.
[01:05:33.020 --> 01:05:35.060]   You don't want to do it manually because there's so much data.
[01:05:35.060 --> 01:05:41.420]   So I came up with a system that did it and I had great results maybe in two months, say.
[01:05:41.420 --> 01:05:44.020]   And then it's like, all right, let's ship it.
[01:05:44.020 --> 01:05:49.980]   And then you start, I run it now on a lot more results and I see all kinds of issues.
[01:05:49.980 --> 01:05:53.340]   And someone put the bag over this thing.
[01:05:53.340 --> 01:05:58.780]   In other cases, the car got stopped for too long so you can't do a structure for motion.
[01:05:58.780 --> 01:06:00.020]   You just find a bunch of them.
[01:06:00.020 --> 01:06:03.580]   And that was maybe three more months.
[01:06:03.580 --> 01:06:06.020]   And then you run it, it's like, I'm there.
[01:06:06.020 --> 01:06:07.020]   And then you run it again.
[01:06:07.020 --> 01:06:11.840]   And of course, in a large enough dataset, everything that can go wrong does go wrong.
[01:06:11.840 --> 01:06:15.900]   And then you find a whole set of yet more rare cases that you need to worry about.
[01:06:15.900 --> 01:06:18.160]   So three more months on those.
[01:06:18.160 --> 01:06:20.220]   And I think that to me taught me, oh, I see.
[01:06:20.220 --> 01:06:25.820]   So from something that works good enough on the demo case, which is typically a paper,
[01:06:25.820 --> 01:06:29.780]   to something actually working, there is still a big chasm.
[01:06:29.780 --> 01:06:33.320]   And I think a lot of it comes from additional requirements.
[01:06:33.320 --> 01:06:35.880]   So for a paper, you have academic metrics.
[01:06:35.880 --> 01:06:36.940]   They're usually permissive.
[01:06:36.940 --> 01:06:42.340]   They're usually fairly average type metrics over something.
[01:06:42.340 --> 01:06:45.940]   And you have, the only constraint is, okay, there's this one or two metrics you picked
[01:06:45.940 --> 01:06:48.540]   and let's show the main ones and let's show it works well.
[01:06:48.540 --> 01:06:53.260]   And then you go to the production folks and they say, oh, but we want this model to produce
[01:06:53.260 --> 01:06:54.900]   three more things.
[01:06:54.900 --> 01:06:58.420]   And there's this rare case that it doesn't work on that it should work on.
[01:06:58.420 --> 01:07:03.020]   And furthermore, it needs to run like three times faster.
[01:07:03.020 --> 01:07:05.860]   And it needs to, you need to build it into this system with these constraints.
[01:07:05.860 --> 01:07:07.620]   And you're like, oh, great.
[01:07:07.620 --> 01:07:11.100]   Now my work may be tripled or quintupled.
[01:07:11.100 --> 01:07:13.340]   And then all the downstream models have to work with it too.
[01:07:13.340 --> 01:07:14.340]   It can't break them now.
[01:07:14.340 --> 01:07:15.340]   It maybe produces new signals.
[01:07:15.340 --> 01:07:17.820]   Now I need to work to fix those.
[01:07:17.820 --> 01:07:24.060]   So that's the usual, I think, story of how to get a research model in production is,
[01:07:24.060 --> 01:07:28.180]   I mean, you need to persist and go, there's a step, stage two or three.
[01:07:28.180 --> 01:07:35.140]   Now the issue there is, like, even if one or two people are enough to get the demo result,
[01:07:35.140 --> 01:07:37.540]   they may not be enough to push the thing to the production.
[01:07:37.540 --> 01:07:41.540]   So now I need to ideally, and we're lucky that we have a lot of great collaborations
[01:07:41.540 --> 01:07:46.300]   with the production team, so we just do it together.
[01:07:46.300 --> 01:07:53.620]   And then it's several people, and you need a lot more people ultimately to do this.
[01:07:53.620 --> 01:07:57.500]   And that's why also in research, we are applied research team.
[01:07:57.500 --> 01:08:02.660]   We are not there to try every possible thing and learn something.
[01:08:02.660 --> 01:08:09.020]   We're trying to ideally guess the right things to try, show that they work, and then spend
[01:08:09.020 --> 01:08:15.980]   sizable effort if needed to build infrastructure integration, I mean, often frameworks even,
[01:08:15.980 --> 01:08:20.900]   jointly with the teams such that many people now can accomplish it successfully.
[01:08:20.900 --> 01:08:23.820]   So that's why the team is not too small either.
[01:08:23.820 --> 01:08:28.060]   When you actually have applied aspiration and shipping aspiration, you usually need
[01:08:28.060 --> 01:08:29.060]   larger teams.
[01:08:29.060 --> 01:08:35.020]   So now you need fewer larger efforts because that's what's conducive to actually landing
[01:08:35.020 --> 01:08:36.020]   the things beyond papers.
[01:08:36.020 --> 01:08:37.020]   Right, right.
[01:08:37.020 --> 01:08:38.020]   Awesome.
[01:08:38.020 --> 01:08:39.020]   Well, thanks so much Drago.
[01:08:39.020 --> 01:08:41.140]   This was really fun.
[01:08:41.140 --> 01:08:42.460]   Great talking to you.
[01:08:42.460 --> 01:08:43.460]   Yeah.
[01:08:43.460 --> 01:08:44.980]   Likewise, thanks for having me.
[01:08:44.980 --> 01:08:49.380]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:08:49.380 --> 01:08:54.100]   to the show notes in the description where you can find links to all the papers that
[01:08:54.100 --> 01:08:58.340]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:08:58.340 --> 01:08:59.340]   produce.
[01:08:59.340 --> 01:09:00.340]   So check it out.

