
[00:00:00.000 --> 00:00:21.080]   Hi, everyone. I'm Jeff Shoumei, and I want to share with you an interesting generative AI
[00:00:21.080 --> 00:00:29.360]   project that I recently did. Not too long ago, I made a game with 100% AI-generated content.
[00:00:30.000 --> 00:00:36.720]   It's a simple game where you're wandering around lost in the forest, and you go from scene to scene
[00:00:36.720 --> 00:00:43.720]   having encounters that impact your vigor and your courage. And the idea is that you want to
[00:00:43.720 --> 00:00:52.140]   find your home before you run out of courage. There's 16 scenes in a 4x4 grid, and so if you
[00:00:52.140 --> 00:00:57.760]   play a few times, you will have seen them all. Now, my favorite part of making this game
[00:00:58.720 --> 00:01:05.120]   was generating each scene and just seeing what AI would come up with. And I thought,
[00:01:05.120 --> 00:01:12.000]   wouldn't it be cool to share that experience with the player? What if every time they went to a new
[00:01:12.000 --> 00:01:20.080]   scene, it was generated fresh for them, and every game would be unique and different this way? It would
[00:01:20.080 --> 00:01:28.400]   be a game of infinite exploration. That sounded so cool that I wanted to try to do it. Now, the first
[00:01:28.400 --> 00:01:34.720]   thing that I would need to do is to generate each scene and have a consistent way of doing that.
[00:01:34.720 --> 00:01:42.640]   My scene definitions are JSON objects that describe what the scene is when you first find it, as well
[00:01:42.640 --> 00:01:46.480]   as when you come back to it later, and how that impacts your stats.
[00:01:47.360 --> 00:01:55.520]   So I started out by using OpenAI's completion endpoint and doing some prompt engineering.
[00:01:56.880 --> 00:02:07.600]   This is the prompt that I used. This is a very detailed prompt. It's rather long, but it worked really
[00:02:07.600 --> 00:02:15.280]   well. Most of the time, I would get scenes that had the right JSON format and the content was good.
[00:02:15.280 --> 00:02:24.000]   It was fitting. It was varied. It was interesting. So I was happy with this. But I wanted to make it even
[00:02:24.000 --> 00:02:36.640]   more reliable. And I decided to fine-tune a model. I used OpenAI's fine-tuning endpoint, and they recommend
[00:02:36.640 --> 00:02:47.680]   50 to 100 examples. I generated 50 examples, just like these, and used them to fine-tune.
[00:02:47.680 --> 00:02:58.560]   Now, the key is I shortened the prompt. I simplified it. I took out any of the JSON and just generally
[00:02:58.560 --> 00:03:06.160]   described what I wanted, hoping that that information would be embedded in the training data.
[00:03:06.160 --> 00:03:14.960]   And I tried this out. I wasn't sure if it would work. And I tried it. It only cost about a dollar
[00:03:14.960 --> 00:03:21.920]   or two. That includes generating all the examples and doing the fine-tuning. And when I tried it,
[00:03:21.920 --> 00:03:30.320]   I was very happy to find that it worked perfectly. Even though I didn't mention the JSON at all,
[00:03:30.320 --> 00:03:38.320]   it came out perfect because of what was in the examples. And that meant I had less tokens in the
[00:03:38.320 --> 00:03:46.080]   prompt, which is faster and cheaper and just easier to work with. So I was really pleased with how this
[00:03:46.080 --> 00:03:57.680]   worked. The next step was to make the images. Now, I used a tool called Leonardo. Leonardo not only lets you
[00:03:57.680 --> 00:04:05.600]   generate images. They also let you create your own image models. And this is great for a game because
[00:04:05.600 --> 00:04:12.480]   it means that you can have stylistically consistent images, which is exactly what I needed.
[00:04:12.480 --> 00:04:20.240]   So I spent a while using all the different parameters that Leonardo offers and working with the prompt to
[00:04:20.240 --> 00:04:27.520]   try and find an image that looked right and that I liked. It turned out that using the description
[00:04:27.520 --> 00:04:34.880]   directly from the scene as the prompt made nice pictures, which I was surprised about since it had
[00:04:34.880 --> 00:04:47.680]   like second person and said things other than what was in there, but it worked out great. Now, the tricky part with fine-tuning an image model is that you need consistent images that have
[00:04:47.680 --> 00:04:55.600]   like the parts that should be the same are the same in all of your training data, but the parts that you want
[00:04:55.600 --> 00:05:01.600]   to vary need to be varied. Otherwise, it will overfit and all of your images will look the same. But if you
[00:05:01.600 --> 00:05:09.040]   don't have that consistency between them, then it won't really know what you want and you won't get that
[00:05:09.040 --> 00:05:16.960]   good stylistic consistency. This was really tricky, especially in my case. I needed the perspective and the
[00:05:16.960 --> 00:05:27.760]   scale to be consistent from scene to scene. Obviously, I needed them all to be set in the forest and I wanted to have this overall tone and texture that looked the same.
[00:05:29.600 --> 00:05:35.440]   Some of my scenes have people in them, some have animals, some have buildings, some have nothing,
[00:05:35.440 --> 00:05:41.680]   and so it was hard to get that variety. I ended up having to train a couple of models with different
[00:05:41.680 --> 00:05:52.880]   parameters, different sets of images, but I eventually found one that worked out. And to test it out, I generated a lot of images. I mean,
[00:05:52.880 --> 00:06:16.880]   a whole bunch. And you can see they all have similar features like the zigzag path down the middle. Obviously, the trees and look and everything looks the same. And yet, there's plenty of variety. Each one is unique and different, but still feels cohesive, which I am very pleased about.
[00:06:16.880 --> 00:06:22.720]   So now I had everything I needed to put it together and make the game.
[00:06:22.720 --> 00:06:35.280]   I made a simple asset server that had an AI pipeline, starting by requesting a new scene from OpenAI's endpoint using my custom model.
[00:06:35.280 --> 00:06:41.280]   Once I get that, I validate the JSON to make sure that it's got all the keys it needs.
[00:06:41.280 --> 00:06:53.680]   If it's good, I take the description and I send that to Leonardo. Leonardo makes an image from my custom model, gives it back to me. I put it all together and send it off.
[00:06:53.680 --> 00:06:57.280]   Now, did this work? Well, let me show you.
[00:06:58.320 --> 00:07:02.000]   Here is an example scene that was created.
[00:07:02.000 --> 00:07:05.840]   And I'm very happy with it.
[00:07:05.840 --> 00:07:16.160]   I made a simple preview server so that I could scroll through a bunch of these scenes that I generated to make sure they worked.
[00:07:16.160 --> 00:07:27.680]   And it looked good. So I made some changes to the game to request images each time the player went to a new scene.
[00:07:27.680 --> 00:07:30.640]   Now there was a problem here.
[00:07:30.640 --> 00:07:40.080]   It takes 10, 20, sometimes 30 seconds to do this, and that wouldn't be good for the play experience.
[00:07:40.080 --> 00:07:43.200]   So what I did is I added some caching.
[00:07:43.200 --> 00:07:54.560]   I pre-fill a bunch of these scenes, and then as scenes are taken out of it, I fill it back up again once it gets below a certain threshold.
[00:07:54.560 --> 00:07:58.320]   And that way, there's always a scene that's ready to go.
[00:07:58.320 --> 00:08:05.120]   With that, the game was ready, and I'm going to share it with you right now.
[00:08:05.120 --> 00:08:12.320]   Now keep in mind, everything that we see has never been seen before and will never be seen again.
[00:08:15.760 --> 00:08:17.280]   So this is the game.
[00:08:17.280 --> 00:08:23.600]   You always start out at this lamppost, and you have to wander around and find your way home.
[00:08:23.600 --> 00:08:29.280]   Your stats are in the bottom left corner. As your vigor goes down, your speed goes down as well.
[00:08:29.280 --> 00:08:33.280]   And as the courage goes down, the viewport will get smaller and smaller.
[00:08:33.280 --> 00:08:36.800]   Let's look around and explore. We're going to move down.
[00:08:39.440 --> 00:08:43.040]   Here's the first generated scene. This looks really cool.
[00:08:43.040 --> 00:08:50.160]   This is like you encounter a soft blue pulsating light coming from the organic formation scattered
[00:08:50.160 --> 00:08:55.680]   around the glade. Your fear and tiredness lift, and you feel rejuvenated, and the vigor goes up,
[00:08:55.680 --> 00:09:01.120]   but I'm already at full. So that's really cool. Let's head off in this direction now.
[00:09:01.120 --> 00:09:06.960]   I won't read all of these, but this looks like a cool campfire scene, which is really neat.
[00:09:07.520 --> 00:09:09.120]   And I'm going to head down.
[00:09:09.120 --> 00:09:17.920]   And what have we got here? There's a large dark cave over here at the end of the path somewhere,
[00:09:17.920 --> 00:09:22.320]   and it's daunting, so my courage is going down. Let's head this way instead.
[00:09:22.320 --> 00:09:30.320]   And now we've gotten into some fog, foggy trees, and...
[00:09:32.560 --> 00:09:39.440]   hard to see. Let's go back. This is like a really windy road that we're going through.
[00:09:39.440 --> 00:09:49.840]   Let's head down. Oh, I'm back where I started. Well, this is the game, and it would continue on and
[00:09:49.840 --> 00:09:53.680]   on and on until you find your way home, and then you can just play again, and it would be different
[00:09:53.680 --> 00:10:02.640]   every time. That's great. I just have a few closing thoughts. One thing is that these images are low
[00:10:02.640 --> 00:10:12.720]   resolution. They're 512 pixels, and I could make them a higher resolution by adding an AI upscaler to my
[00:10:12.720 --> 00:10:22.560]   pipeline. It would add more time, so it's a trade-off. Also, I could get more creative with adding something
[00:10:22.560 --> 00:10:30.400]   to the prompt to make a scene. For example, I could let the user select a theme, or maybe even get the
[00:10:30.400 --> 00:10:37.840]   time of day or the current weather at the location of where the user is set, and then the scenes could
[00:10:37.840 --> 00:10:47.360]   be generated to match where they are for a very immersive experience. And of course, I can use this
[00:10:47.360 --> 00:10:55.920]   same process on other projects. That's all. I hope that you found this interesting and enjoyed watching it
[00:10:55.920 --> 00:11:07.040]   as much as I enjoyed putting it all together. Thank you so much.

