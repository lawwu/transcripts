
[00:00:00.000 --> 00:00:04.500]   Hi out there can't really tell who you are how many of you are out there, but it sounds like a lot
[00:00:04.500 --> 00:00:08.200]   Raise your hand if you've heard of no code
[00:00:08.200 --> 00:00:16.120]   Okay, squinting I see I see some hands. Okay, it's definitely an improvement since 2012 when we started the company
[00:00:16.120 --> 00:00:21.440]   But before I go into to no code and how it's going to allow AI to be more broadly
[00:00:21.440 --> 00:00:26.320]   Experienced by by by the enterprise and and small business
[00:00:27.120 --> 00:00:31.780]   Knowledge workforce. I want to start with a small shout out to weights and biases
[00:00:31.780 --> 00:00:35.060]   I actually started my career in 2009 as an intern at
[00:00:35.060 --> 00:00:39.280]   Lucas and and Chris van Pelt's previous company crowdflowers
[00:00:39.280 --> 00:00:44.000]   So very full circle that you know, we're all back here now at this moment
[00:00:44.000 --> 00:00:50.000]   And I think it's it's been interesting for me to watch AI really take off over the the recent past
[00:00:50.000 --> 00:00:56.640]   I started my my my college experience being very interested in AI and at the time neural networks
[00:00:56.640 --> 00:01:03.720]   I told around a lot in MATLAB and was trying to find some interesting applications of in that case neural nets and
[00:01:03.720 --> 00:01:11.800]   Ultimately, I decided my time would be better spent at that time learning UX because the the constant for me at the time was okay
[00:01:11.800 --> 00:01:15.360]   You can build all of this advanced inference. You can do interesting stuff with that
[00:01:15.360 --> 00:01:18.520]   You need high quality data. You can get some really interesting outputs
[00:01:18.520 --> 00:01:23.760]   But to wrap it all up into an experience that people are going to be able to use and get value from
[00:01:24.080 --> 00:01:26.940]   We're still gonna need the principles of UX, right?
[00:01:26.940 --> 00:01:31.660]   Whether that's with a natural language input or through GUIs, you know
[00:01:31.660 --> 00:01:39.160]   understanding UX and really how humans can interface with a computer is gonna be the the universal language the constant of how
[00:01:39.160 --> 00:01:45.280]   Software experiences are realized more broadly. And so that's how we started our table in 2012
[00:01:45.280 --> 00:01:52.320]   We were really, you know going out with this mission of democratizing the building of useful apps and to do so with a no-code
[00:01:52.680 --> 00:01:55.960]   Paradigm which at the time was not very well understood. I mean arguably
[00:01:55.960 --> 00:01:59.160]   you know really not not a category and
[00:01:59.160 --> 00:02:04.040]   Today, you know, I think what we've done is taken all of the principles of building apps
[00:02:04.040 --> 00:02:09.680]   So you have a data layer you have logic you have interface and we've distilled them into a form
[00:02:09.680 --> 00:02:14.960]   Where you can just drag and drop it sort of like what you do with a Squarespace or a Wix for websites
[00:02:14.960 --> 00:02:18.840]   Airtable is to two apps now. How does this all tie in to AI?
[00:02:18.840 --> 00:02:20.720]   We'll get to that
[00:02:20.720 --> 00:02:25.580]   I think the most exciting thing to me looking at AI and especially Gen AI today
[00:02:25.580 --> 00:02:32.200]   Versus where the state of the art was in 2009 when I graduated is that now out of the box you have these incredible
[00:02:32.200 --> 00:02:36.340]   Foundation models and specifically for me LLMs are the really exciting thing
[00:02:36.340 --> 00:02:40.880]   That are capable of being applied to a really broad range of knowledge work
[00:02:40.880 --> 00:02:44.800]   So no longer do you have to go and have a really large and high quality
[00:02:44.800 --> 00:02:49.840]   Dataset that you can then train your own model on and then you know only after a lot of effort
[00:02:49.840 --> 00:02:52.080]   Can you do some useful inference?
[00:02:52.080 --> 00:02:59.040]   Applications but instead now as we've all experienced from chat GPT and you know, you can do funny and silly things with it
[00:02:59.040 --> 00:03:01.560]   But you can also do quite advanced things, right?
[00:03:01.560 --> 00:03:05.480]   and I think that's the part that is most piqued my interest as a
[00:03:05.480 --> 00:03:10.120]   technologist and somebody who loves thinking about how do you take really powerful technology and
[00:03:10.120 --> 00:03:18.080]   Productize it into a way that anyone can use that more people can get value from it's basically what we did with the foundations of
[00:03:18.080 --> 00:03:21.520]   Airtable taking the concept of database powered applications and
[00:03:21.520 --> 00:03:26.240]   Democratizing it into this no code GUI experience and it's what we're now setting out to do
[00:03:26.240 --> 00:03:29.920]   With AI and that the power of gen AI and especially LLMs
[00:03:29.920 --> 00:03:37.700]   Now this talk is not just about Airtable what I want to leave you with are four principles of how to build great UX
[00:03:37.700 --> 00:03:40.840]   Around LLMs that make them more immediately
[00:03:40.840 --> 00:03:47.280]   Valuable to real use cases and real people especially within a workplace setting
[00:03:48.120 --> 00:03:52.800]   One final thought on on the the current, you know point in time that we're in
[00:03:52.800 --> 00:03:59.880]   I think what's really blown me away is that as you've you've watched these models progress over the even of the recent past
[00:03:59.880 --> 00:04:04.000]   The breakthroughs in terms of what they're able to do, you know, it's not just about
[00:04:04.000 --> 00:04:08.760]   You know being able to stylistically edit content or summarize etc
[00:04:08.760 --> 00:04:12.680]   But I think a lot of people over gravitate on these more narrow and I would say
[00:04:12.680 --> 00:04:16.400]   You know less reasoning or creativity or knowledge heavy
[00:04:17.080 --> 00:04:22.080]   prompts, but in fact the the you know, the reality is that if you go in and you really push
[00:04:22.080 --> 00:04:25.960]   These models to their limits they're capable of much more
[00:04:25.960 --> 00:04:30.280]   I mean dare I say it even strategy if you think of yourself as a
[00:04:30.280 --> 00:04:35.960]   you know product manager or somebody working on it on a new feature to actually go in and be able to get a
[00:04:35.960 --> 00:04:43.800]   LLM to give you a very useful starting point is really magical and I think that we will only unlock more of these use cases
[00:04:43.800 --> 00:04:46.320]   throughout the world throughout the you know, the
[00:04:46.720 --> 00:04:54.120]   The outer reaches of every company by giving the tools to end users who are closest to their own work closest to the data
[00:04:54.120 --> 00:05:00.720]   That they can use to apply into these LLMs to be able to really explore the edges of what's possible
[00:05:00.720 --> 00:05:08.160]   some of the screenshots here from the you know, some of the recent news pieces that we've probably all seen, you know
[00:05:08.160 --> 00:05:14.880]   foundation models be able to you know, pass the bar being able to now, you know pass job interviews and
[00:05:15.400 --> 00:05:17.400]   and of course, I think we've all seen the
[00:05:17.400 --> 00:05:22.960]   Microsoft research paper that talks about the the sparks of really incredible
[00:05:22.960 --> 00:05:26.580]   And and and thought-provoking, you know
[00:05:26.580 --> 00:05:33.280]   Signs of general intelligence that we've seen from you know, especially the really large LLMs like gbd4
[00:05:33.280 --> 00:05:41.640]   So, of course, you know as I said the the the real bottleneck to adoption as we believe it at our table and and you know
[00:05:41.640 --> 00:05:46.800]   Within the no-code community is not actually at this point just the capabilities of the models themselves
[00:05:46.800 --> 00:05:53.040]   and of course, there's going to be a lot of value in the innovation of you know, coming up with better foundation models models with
[00:05:53.040 --> 00:05:58.800]   Multimodal capabilities fine-tuning models, you know, it's really exciting to see the explosion of
[00:05:58.800 --> 00:06:01.440]   Progress there at that layer
[00:06:01.440 --> 00:06:05.800]   But my personal conviction is that even if you froze in this moment in time
[00:06:05.880 --> 00:06:11.360]   The current capabilities of the LLMs that are out there or that you can train yourself or fine-tune yourself
[00:06:11.360 --> 00:06:15.960]   There's a lot more applications for these LLMs and what they can do
[00:06:15.960 --> 00:06:23.220]   Especially within an enterprise context that are just screaming to be explored and deployed but lack of vehicle to do so
[00:06:23.220 --> 00:06:25.080]   and
[00:06:25.080 --> 00:06:27.080]   That's where we think no code comes in
[00:06:27.080 --> 00:06:32.400]   So to just give a quick example of what a no-code experience looks like this is air table
[00:06:32.960 --> 00:06:37.120]   This is how you would build a very simple inventory management app. You have a data layer
[00:06:37.120 --> 00:06:41.320]   You've got this interface layer and you can publish it all very very easy
[00:06:41.320 --> 00:06:46.800]   No code required at the same time if you want to extend it with more capabilities, you can go in and do so
[00:06:46.800 --> 00:06:50.080]   And
[00:06:50.080 --> 00:06:52.560]   if we think about the the broad range of
[00:06:52.560 --> 00:06:55.640]   You know applications that you might want to build with LLMs
[00:06:55.640 --> 00:07:02.240]   I think there's some that are going to be solved by functional point solutions. So, you know for sales related workflows
[00:07:02.280 --> 00:07:05.120]   there's tools out there like Salesforce or gong or
[00:07:05.120 --> 00:07:10.320]   Outreach to IO that are already and are surely going to build more
[00:07:10.320 --> 00:07:13.600]   specific LLM powered solutions such as
[00:07:13.600 --> 00:07:21.960]   Automatically generating emails for outbound reps, you know, so we can all get even more customized and incessant outbound messages in our inbox
[00:07:21.960 --> 00:07:27.600]   but I think that if you if you really tally up the broad range of work that the
[00:07:27.760 --> 00:07:33.000]   All of the knowledge workers out there in the world are doing only a very narrow subset of that falls into
[00:07:33.000 --> 00:07:37.440]   What can be applied within the context of an existing point solution?
[00:07:37.440 --> 00:07:39.840]   Of course, you know the the you know
[00:07:39.840 --> 00:07:41.440]   There's a next category of you know
[00:07:41.440 --> 00:07:46.580]   As many of you here are already working on building custom apps on top of LLMs, right?
[00:07:46.580 --> 00:07:52.240]   And of course, there's gonna be a lot of great innovation in terms of the entire stack to make it easier to build
[00:07:52.240 --> 00:07:58.520]   You know fine-tuned models to go and build the the apps on top of it for observability on that for chaining, etc
[00:07:58.520 --> 00:08:06.080]   and I think that's all really really good overall for the world and for innovation and yet I think that the broader landscape of
[00:08:06.080 --> 00:08:11.260]   What LLMs could be applied to can only be solved by a no-code platform
[00:08:11.260 --> 00:08:13.120]   I don't think it's going to be solved by
[00:08:13.120 --> 00:08:20.480]   Just chat based experiences that aren't embedded into real data into real workflows and have a different UX than just you know
[00:08:20.480 --> 00:08:25.840]   chat style interaction and so if you think about all of the different applications that could be built if
[00:08:25.840 --> 00:08:31.460]   You know the the app developers in this room were all replicated by a million times
[00:08:31.460 --> 00:08:36.520]   How many more apps would we build even for the small and even seemingly trivial use cases?
[00:08:36.520 --> 00:08:40.840]   Within companies and even some outside of companies within you know, the consumer world. So
[00:08:40.840 --> 00:08:42.080]   Really?
[00:08:42.080 --> 00:08:44.080]   there there's just this broad landscape of
[00:08:44.400 --> 00:08:51.840]   applicability for LLMs that have not yet been exploited because of the the difficulty the massive friction and the cost of
[00:08:51.840 --> 00:08:53.920]   Actually going and building those apps
[00:08:53.920 --> 00:08:55.760]   now
[00:08:55.760 --> 00:08:57.680]   This isn't just about air table
[00:08:57.680 --> 00:09:04.680]   This is really about the the design concepts that make it easier and faster to build those apps and and actually
[00:09:04.680 --> 00:09:13.240]   Enable real value to be both validated and then deployed into the real enterprise applications or enterprise use cases
[00:09:13.920 --> 00:09:18.840]   That are waiting to to to see AI transformation. The first is
[00:09:18.840 --> 00:09:21.560]   You know, I think this is a pretty straightforward one
[00:09:21.560 --> 00:09:27.720]   You know embedding into first-party data and workflows is a really crucial way of unlocking value out of these LLMs
[00:09:27.720 --> 00:09:31.040]   so, you know again going back to 2009 and kind of the
[00:09:31.040 --> 00:09:35.200]   You know pre image net pre AI Renaissance era
[00:09:35.200 --> 00:09:43.040]   you know, I thought of AI and and really specifically neural nets as kind of this this way that you could generate almost like a
[00:09:43.200 --> 00:09:46.240]   Very advanced form of polynomial regression, right you could do
[00:09:46.240 --> 00:09:49.080]   Inference and and you know, it's very useful
[00:09:49.080 --> 00:09:52.840]   But in a very narrowly applicable sense you have to have all this data
[00:09:52.840 --> 00:09:58.720]   You have to train it and then you you get these outputs that you apply in very narrow context
[00:09:58.720 --> 00:10:02.780]   Like, you know the Netflix data science prize generate better recommendations for movies
[00:10:02.780 --> 00:10:07.620]   But I think for me what's been really really mind-boggling about the current
[00:10:07.960 --> 00:10:15.040]   Class of gen AI models is that out of the box without any additional fine-tuning even and of course you can get even more gains
[00:10:15.040 --> 00:10:18.960]   if you do fine-tune you can apply it to a really broad range of work and
[00:10:18.960 --> 00:10:27.000]   The key to unlocking a lot of that value is to embed that model into the place of that work into small-scale data
[00:10:27.000 --> 00:10:32.280]   So you don't have to go and actually train it yourself. You're not even fine-tuning it, but you're just embedding it
[00:10:32.280 --> 00:10:39.260]   Think about like a you know a spreadsheet use case where every field in or every cell inside the spreadsheet can actually reference
[00:10:39.260 --> 00:10:44.880]   You know a large language model and take his inputs any data from the rest of that table and get out
[00:10:44.880 --> 00:10:49.440]   whether it's a small data transformation or something much more advanced and strategic like
[00:10:49.440 --> 00:10:56.600]   generate me a product requirements doc for a new feature based on some of these inputs which could include user feedback or
[00:10:56.600 --> 00:11:00.120]   You know requirements or information about this this feature
[00:11:00.480 --> 00:11:07.640]   And so really that embedding into local and small-scale data actually unlocks a ton of value from these LLMs. I
[00:11:07.640 --> 00:11:14.320]   Think the second is also really crucial and something that no code does very well
[00:11:14.320 --> 00:11:21.520]   which is if you make the output very visible and interactive all of a sudden you get past a lot of the challenges of
[00:11:21.520 --> 00:11:28.920]   hallucinations of you know accuracy problems, right because there's a ton of use cases where the LLM is going to generate output
[00:11:29.120 --> 00:11:33.000]   That may not be right a hundred percent of the time, but even when it's wrong
[00:11:33.000 --> 00:11:40.920]   It's usefully wrong meaning, you know a starting point for in that product requirements document or a marketing campaign concept
[00:11:40.920 --> 00:11:46.640]   You know the starting point doesn't have to be right and in fact sometimes having even a bad starting point
[00:11:46.640 --> 00:11:52.640]   I mean we actually internally at air table sometimes talk about let's generate with humans a bad starting point
[00:11:52.640 --> 00:11:57.600]   So that we normalize the act of taking a first draft and revising it reacting to it
[00:11:57.600 --> 00:12:05.120]   And so if AI is your thought partner that can actually go and generate even a bad first draft of something really strategic and useful
[00:12:05.120 --> 00:12:08.920]   That you can react to there's so many more applications that are unlocked
[00:12:08.920 --> 00:12:14.240]   Then when you're trying to apply it only to those domains where it has to be right and it has to be right
[00:12:14.240 --> 00:12:21.960]   silently without human intervention and without a very usable editable way to take that output and do something with it the
[00:12:24.760 --> 00:12:26.760]   Third is of course
[00:12:26.760 --> 00:12:30.400]   Chainability and composability and of course, you know as developers
[00:12:30.400 --> 00:12:35.520]   I think we've probably all use link chain seen it, you know and understand this concept as a developer
[00:12:35.520 --> 00:12:39.280]   But I think the the even more powerful in my view
[00:12:39.280 --> 00:12:44.440]   Opportunity is to introduce chain ability not just at the development time of an application
[00:12:44.440 --> 00:12:50.960]   but in fact at the runtime for the end users to experiment with and so one of the core tenets of air table is that
[00:12:50.960 --> 00:12:58.520]   We see ourselves as this Lego kit every piece of that Lego kit our fields our formulas the way that interface layouts work
[00:12:58.520 --> 00:13:00.040]   Are all composable
[00:13:00.040 --> 00:13:02.640]   so when you create a field output in air table
[00:13:02.640 --> 00:13:09.380]   You can then feed that output into any other field or to reference it in a different table or to compose it onto an interface
[00:13:09.380 --> 00:13:17.040]   Layout however you want and what that means is that in air table our AI field implementation allows you to go and take the output
[00:13:17.040 --> 00:13:19.360]   Of you know an AI generated set
[00:13:19.360 --> 00:13:22.080]   so let's say you use an LLM to create a
[00:13:22.080 --> 00:13:25.840]   product requirements doc for a feature you can then take that document and
[00:13:25.840 --> 00:13:30.800]   Feed it into an automation where when a button is clicked it gets sent out by email
[00:13:30.800 --> 00:13:35.240]   Or it could be that you actually feed it into yet another step. That's powered by an LLM
[00:13:35.240 --> 00:13:40.580]   So take the first output step review it with a human and then give it to another LLM call
[00:13:40.580 --> 00:13:42.400]   That's able to do something else useful
[00:13:42.400 --> 00:13:46.840]   Like for instance generate the marketing brief or the press release for this feature
[00:13:46.840 --> 00:13:50.680]   Both of which were created by AI at least as a starting point
[00:13:50.680 --> 00:14:02.080]   And the fourth is really ride the wave of model progression and what this means is that you know
[00:14:02.080 --> 00:14:06.560]   As we've all seen and I think especially excitingly over the the recent, you know past six months
[00:14:06.560 --> 00:14:08.560]   I think there's just been this explosion of
[00:14:09.280 --> 00:14:14.080]   Surprising innovation, especially in the open source world and you know pre-trained models, you know
[00:14:14.080 --> 00:14:18.720]   Where you get the weights out of the box and it's becoming easier and easier to fine-tune
[00:14:18.720 --> 00:14:22.840]   And I think all this means is that models are getting better
[00:14:22.840 --> 00:14:31.040]   you know, they're getting cheaper and the range of models that you can use for different applications or different domains is getting wider and
[00:14:31.040 --> 00:14:36.840]   Our view at air table is you know, rather than try to go and compete by creating our own model
[00:14:36.840 --> 00:14:40.760]   We want to build a product experience that rides on top of all of the models
[00:14:40.760 --> 00:14:45.000]   So, you know interchangeability is an important tenet of how we build our product
[00:14:45.000 --> 00:14:50.560]   You know as you go and implement AI as a core concept into an air table app through no code
[00:14:50.560 --> 00:14:54.360]   We want to make it really easy to benefit from performance gain
[00:14:54.360 --> 00:14:57.720]   So when opening I releases GPT for you know
[00:14:57.720 --> 00:15:03.080]   You can instantly switch from your 3.5 implementation to 4 with just a click of the button, right?
[00:15:03.080 --> 00:15:09.040]   Or if you want to switch from you know, GPT for to anthropics clod model with 100k tokens
[00:15:09.040 --> 00:15:09.840]   You can do that
[00:15:09.840 --> 00:15:16.080]   And so really the the goal is to make our platform and I think this is a good design pattern in general
[00:15:16.080 --> 00:15:19.840]   Interoperable with different models underneath and really for us
[00:15:19.840 --> 00:15:26.200]   it's about focusing on that embed into data embed into workflows make the the output really usable and
[00:15:26.520 --> 00:15:32.160]   Finally allow useful chaining and composability to happen so that all of the above
[00:15:32.160 --> 00:15:37.080]   Compounds on top of the value of the models that are progressing at a really really fast rate
[00:15:37.080 --> 00:15:42.760]   And and something that you know really should benefit us rather than feel competitive to us
[00:15:42.760 --> 00:15:51.600]   So the four design patterns hope they're useful if anyone has questions, please feel free to reach out how we at air table comm and
[00:15:52.320 --> 00:15:58.120]   Really excited to see what all of you build whether it's with no code code or something else that we haven't thought of yet
[00:15:58.120 --> 00:16:01.420]   But thank you very much for your time and good to see you all
[00:16:01.420 --> 00:16:03.420]   [Applause]
[00:16:04.020 --> 00:16:06.020]   [Music]
[00:16:06.620 --> 00:16:08.620]   [Music]
[00:16:08.620 --> 00:16:17.100]   [BLANK_AUDIO]

