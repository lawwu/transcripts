
[00:00:00.000 --> 00:00:03.800]   to check on different platforms if we're live or not. I have a
[00:00:03.800 --> 00:00:09.960]   paranoia of huge mistakes I make every time. I'll just wait to
[00:00:09.960 --> 00:00:19.280]   double check on that. I can hear an echo which means he's live.
[00:00:19.280 --> 00:00:22.440]   Awesome. Hey, everybody. Thank you for joining us. I'm super
[00:00:22.440 --> 00:00:25.600]   excited to be hosting Alex Watson today. Alex, thank you so
[00:00:25.600 --> 00:00:26.960]   much again for joining us.
[00:00:27.320 --> 00:00:31.640]   Thanks, Sam. Appreciate it. So I'll quickly introduce Alex
[00:00:31.640 --> 00:00:34.480]   quickly introduce framework stock. Then I'll start by
[00:00:34.480 --> 00:00:37.080]   asking Alex about his background. And then Alex has
[00:00:37.080 --> 00:00:40.280]   some exciting demos that I'll let him share with us. So I'll
[00:00:40.280 --> 00:00:44.400]   keep the questions from my end up with shorter today. Alex is
[00:00:44.400 --> 00:00:48.080]   the co founder and chief product officer at Gretel AI. It's a
[00:00:48.080 --> 00:00:51.960]   data privacy company that's building tools that allows us to
[00:00:51.960 --> 00:00:55.120]   share data safely and collaborate with sensitive data.
[00:00:55.120 --> 00:00:58.080]   It's something that I'm learning recently. I just learned about
[00:00:58.080 --> 00:01:02.200]   GDPR. And I know it's, it's a huge sensitive topic. So I'm
[00:01:02.200 --> 00:01:05.880]   excited to learn more about this. Before this Alex founded
[00:01:05.880 --> 00:01:11.760]   harvest.ai he was working in the world of NLP. And before that,
[00:01:11.760 --> 00:01:14.840]   he was at AWS, he has a background in computer science.
[00:01:14.840 --> 00:01:19.440]   So, Alex, I want to start by asking you, at which point in
[00:01:19.440 --> 00:01:23.320]   your career did you get interested in data privacy and
[00:01:23.320 --> 00:01:24.920]   machine learning, broadly speaking?
[00:01:24.920 --> 00:01:29.880]   That's a that's a good question. So I got early exposure to
[00:01:29.880 --> 00:01:33.440]   machine learning, actually, out of college, I graduated right
[00:01:33.440 --> 00:01:37.680]   after September 11 2001. And actually got recruited to join
[00:01:37.680 --> 00:01:40.280]   the NSA. So I was there for about seven years and got to
[00:01:40.280 --> 00:01:42.920]   work on some really cool kind of like big data and machine
[00:01:42.920 --> 00:01:46.840]   learning problems. So that's really kind of where my passion
[00:01:46.840 --> 00:01:52.320]   around machine learning started. And, you know, kind of maybe on
[00:01:52.320 --> 00:01:58.360]   a quick background to the journey to Gradle today. As you
[00:01:58.360 --> 00:02:00.800]   mentioned earlier, I co founded a company called harvest, or
[00:02:00.800 --> 00:02:03.680]   helping companies that were starting to use SaaS
[00:02:03.680 --> 00:02:07.720]   applications like Google Suite or Office 365, like to identify
[00:02:07.720 --> 00:02:10.360]   and protect important information in the cloud. So we
[00:02:10.360 --> 00:02:14.720]   used NLP to do that. AWS was another area that we built
[00:02:14.720 --> 00:02:18.280]   protection for we ended up getting acquired by AWS in 2016.
[00:02:18.280 --> 00:02:22.000]   And had a really kind of unique opportunity to come in and
[00:02:22.000 --> 00:02:25.680]   launch AWS his first security services. So Macy was our
[00:02:25.680 --> 00:02:28.280]   product at harvest, and we launched it as a service at AWS
[00:02:28.280 --> 00:02:31.720]   that customers use to identify and protect information in the
[00:02:31.720 --> 00:02:36.600]   cloud. And I think a couple things that I saw or I learned
[00:02:36.600 --> 00:02:39.720]   there really kind of like brought me to to Gretel, you
[00:02:39.720 --> 00:02:43.000]   know, one was inside the walls of AWS, it's just like
[00:02:43.000 --> 00:02:45.760]   incredible access, we had the data through data warehouse. And
[00:02:45.760 --> 00:02:48.440]   you know, that's no accident. And so, you know, is it
[00:02:48.440 --> 00:02:51.920]   something that I think, creates a real advantage, and it was
[00:02:51.920 --> 00:02:56.240]   powered by like a 500 person compliance team. Like, well,
[00:02:56.240 --> 00:02:59.000]   not every company can match that. But when you have that,
[00:02:59.000 --> 00:03:01.920]   you know, if you want to validate an idea, like, bring a
[00:03:01.920 --> 00:03:03.760]   data warehouse, like, it's really nice to have that at
[00:03:03.760 --> 00:03:07.600]   your fingertips. Second piece that I noticed, too, was like,
[00:03:07.600 --> 00:03:10.680]   how hard even for our like, most sophisticated best
[00:03:10.680 --> 00:03:13.800]   customers, how hard it was for them to enable access to that
[00:03:13.800 --> 00:03:16.680]   data. So you could have an incredible data warehouse with
[00:03:16.680 --> 00:03:20.880]   the best like raw customer data in the world. But point 01% of
[00:03:20.880 --> 00:03:22.920]   people in your business can access that data. And it's for
[00:03:22.920 --> 00:03:26.160]   very good reasons. It's privacy protection, and it's real kind
[00:03:26.160 --> 00:03:30.000]   of raw customer information. So really, for me, the theory at
[00:03:30.000 --> 00:03:33.200]   Gretel, and what got me so excited about synthetic data,
[00:03:33.200 --> 00:03:35.440]   and the work we're gonna talk about today is saying, like,
[00:03:35.440 --> 00:03:41.120]   can we enable fast, safe access to data by dive into what
[00:03:41.120 --> 00:03:43.800]   synthetic data is in a minute, but applying the best and kind
[00:03:43.800 --> 00:03:45.840]   of privacy enhancing technologies to enable that
[00:03:45.840 --> 00:03:46.360]   access.
[00:03:46.360 --> 00:03:49.760]   Sanyam Bhutani: Did you have access to unlimited compute
[00:03:49.760 --> 00:03:51.920]   power? I've spent my life savings on this box, which I
[00:03:51.920 --> 00:03:55.240]   show off very proudly. Did you get like unlimited access to?
[00:03:55.240 --> 00:03:55.800]   I got a little
[00:03:55.800 --> 00:03:59.160]   Jason Tucker: spoiled during my time there for sure. There was
[00:03:59.160 --> 00:04:03.880]   a funny story when we were first launching Macy. A lot of the
[00:04:03.880 --> 00:04:08.160]   services at AWS revolve around as you would imagine, like
[00:04:08.160 --> 00:04:10.480]   reinvent and New York summit, like the two biggest
[00:04:10.480 --> 00:04:14.440]   conferences we have, and we wanted to get Macy out the door.
[00:04:14.440 --> 00:04:18.920]   And we had to kind of rebuild a lot of the infrastructure we
[00:04:18.920 --> 00:04:25.000]   built during the harvest days to work at AWS scale. And the one
[00:04:25.000 --> 00:04:27.400]   part that we couldn't get done in within the timeline we were
[00:04:27.400 --> 00:04:30.120]   looking at was the user interface. So everything else
[00:04:30.120 --> 00:04:33.520]   was multi tenant and built to AWS specs and per scale and NLP
[00:04:33.520 --> 00:04:37.600]   and things like that. And the user interface was lagging about
[00:04:37.600 --> 00:04:41.480]   45 days behind launch date. So we had a real question, what are
[00:04:41.480 --> 00:04:44.120]   we going to do? And then we realized that, you know, kind of
[00:04:44.120 --> 00:04:46.200]   elasticity of the cloud. And actually, when you looked at
[00:04:46.200 --> 00:04:51.760]   cost, it wasn't that bad. We launched with one single
[00:04:51.760 --> 00:04:54.760]   instance for every single customer that would sign into
[00:04:54.760 --> 00:04:59.240]   use Macy for the first 45 days. And what that equated to since
[00:04:59.240 --> 00:05:02.600]   you know, you don't do anything in a single region in AWS was
[00:05:02.600 --> 00:05:04.960]   not just one box, but it's three boxes because you had to cover
[00:05:04.960 --> 00:05:07.880]   each availability zone in each region we launched in. So we
[00:05:07.880 --> 00:05:11.960]   launched with 18,000 boxes running our user interface for
[00:05:11.960 --> 00:05:15.680]   the first 45 days of Macy, which was quite the experience.
[00:05:16.680 --> 00:05:19.040]   Sanyam Bhutani: That sounds amazing. I can only imagine the
[00:05:19.040 --> 00:05:23.400]   dashboard experience behind that. At what point did you
[00:05:23.400 --> 00:05:29.080]   decide to co founding a company because as I understand, Amazon
[00:05:29.080 --> 00:05:31.840]   is quite a big company. That's a completely different experience
[00:05:31.840 --> 00:05:35.600]   from being a co founder, having your own company having your own
[00:05:35.600 --> 00:05:38.920]   product, which you're much more involved in, I would assume that
[00:05:38.920 --> 00:05:42.280]   you have much more control over much more say over what was the
[00:05:42.280 --> 00:05:45.280]   convincing factor there? What led you to the strong decision?
[00:05:46.200 --> 00:05:50.600]   How's the combination of factors I got a chance to start a
[00:05:50.600 --> 00:05:53.400]   business which is Gretel with, you know, my three co founders
[00:05:53.400 --> 00:05:56.480]   who are incredibly close friends that we had talked about doing
[00:05:56.480 --> 00:06:01.200]   this for 10 years. And we saw privacy as being such a problem
[00:06:01.200 --> 00:06:06.120]   to tackle. And something that I think, in some cases, like a
[00:06:06.120 --> 00:06:08.560]   startup can be really well positioned to tackle a problem
[00:06:08.560 --> 00:06:12.120]   because, you know, you're not locked into one ecosystem or
[00:06:12.120 --> 00:06:14.080]   anything like that. You can kind of help customers that are
[00:06:14.080 --> 00:06:16.880]   building multi cloud, for example. So that kind of
[00:06:16.880 --> 00:06:20.320]   combination of availability, which is the hardest part when
[00:06:20.320 --> 00:06:22.320]   you're starting a company is like, you know, those people you
[00:06:22.320 --> 00:06:25.400]   want to start the business with becoming available, was really
[00:06:25.400 --> 00:06:30.600]   kind of what pushed me over the edge. And, you know, it's been
[00:06:30.600 --> 00:06:34.720]   two years now, and we've done three fundraisers. And this has
[00:06:34.720 --> 00:06:37.000]   been, you know, the one of the better work experiences of my
[00:06:37.000 --> 00:06:37.320]   life.
[00:06:37.320 --> 00:06:40.200]   Sanyam Bhutani: So can you please for our audience who
[00:06:40.200 --> 00:06:43.360]   might not know what's what is greater day and what is the
[00:06:43.360 --> 00:06:45.240]   project? What problem are you trying to solve?
[00:06:45.240 --> 00:06:48.800]   Yeah, I have some slides here. Let me let me show them.
[00:06:48.800 --> 00:06:49.800]   Please.
[00:06:49.800 --> 00:07:01.680]   Assume I can see this.
[00:07:01.680 --> 00:07:03.520]   Okay, go ahead and play.
[00:07:03.520 --> 00:07:12.800]   I'll start with the definition of synthetic data first, and
[00:07:12.800 --> 00:07:17.200]   then we'll go back to the use cases around it. So at Gretl,
[00:07:17.200 --> 00:07:20.960]   our focus is a set of simple API's that can be used by
[00:07:20.960 --> 00:07:25.880]   developers and data scientists to solve, essentially the data
[00:07:25.880 --> 00:07:28.720]   access problem. So how do we enable the face fastest and
[00:07:28.720 --> 00:07:32.680]   safest data access possible through applying the best and
[00:07:32.680 --> 00:07:37.360]   privacy enhancing technologies, an area that we have really
[00:07:37.360 --> 00:07:40.080]   focused on quite a bit, we have open source package, which we'll
[00:07:40.080 --> 00:07:44.080]   use here in a few minutes, and a service built around it is
[00:07:44.080 --> 00:07:48.320]   synthetic data. And to kind of boil down synthetic data really
[00:07:48.320 --> 00:07:51.200]   like this definition that Nvidia used when they just described
[00:07:51.200 --> 00:07:57.920]   it. It is the idea that it is a artificially generated version
[00:07:57.920 --> 00:08:01.560]   of real world data. And it can be generated by computer
[00:08:01.560 --> 00:08:05.760]   simulation can be generated by a machine learning model, or
[00:08:05.760 --> 00:08:08.520]   anything in between the idea of synthetic data is nothing new.
[00:08:08.560 --> 00:08:11.720]   This has been the idea has been around since the 60s or the 70s
[00:08:11.720 --> 00:08:14.720]   earliest days of compute when people didn't have access to
[00:08:14.720 --> 00:08:17.240]   enough data, and started creating their own. That was
[00:08:17.240 --> 00:08:19.960]   really the beginning of synthetic data. What we've seen
[00:08:19.960 --> 00:08:24.320]   in the past five years, with advances in, for example, like
[00:08:24.320 --> 00:08:28.200]   GANs and language models, we're all pretty familiar with the
[00:08:28.200 --> 00:08:33.000]   work at OpenAI, for example, with GPT is the advance of the
[00:08:33.000 --> 00:08:36.880]   neural networks ability to learn the insights and distributions
[00:08:36.880 --> 00:08:41.200]   inside data and recreate them. So just like OpenAI creates
[00:08:41.200 --> 00:08:44.640]   really advanced natural language, text and processing
[00:08:44.640 --> 00:08:48.040]   and things like that, we have focused on how do we create
[00:08:48.040 --> 00:08:51.800]   realistic data sets. And those data sets can be tabular, they
[00:08:51.800 --> 00:08:54.680]   can be time series, they can be images or really anything in
[00:08:54.680 --> 00:09:03.200]   between. So going back and saying like, what are the use
[00:09:03.200 --> 00:09:05.760]   cases that we see most often with our customers, we see
[00:09:05.760 --> 00:09:09.320]   several. And here I wanted to highlight some of the top things
[00:09:09.320 --> 00:09:11.160]   that we've seen, you know, in the two years that we've
[00:09:11.160 --> 00:09:14.520]   launched Gretel, it's free, similar to Weights and Biases
[00:09:14.520 --> 00:09:16.600]   for anyone to sign up and start working with it, all you need
[00:09:16.600 --> 00:09:20.160]   is a GitHub or a Gmail address. And you know, in those two
[00:09:20.160 --> 00:09:23.840]   years, we've had several 1000 developers on our platform and,
[00:09:23.840 --> 00:09:26.360]   you know, in the 10s to hundreds of companies we're working
[00:09:26.360 --> 00:09:29.080]   with. And we've seen what started out with this really
[00:09:29.080 --> 00:09:31.920]   open experiment with synthetic data boiled down into several
[00:09:31.920 --> 00:09:36.520]   different use cases. You know, starting at the bottom, you
[00:09:36.520 --> 00:09:39.400]   know, for maybe the most important data is so often
[00:09:39.400 --> 00:09:43.600]   inside a business or across teams, there just isn't a way to
[00:09:43.600 --> 00:09:46.880]   make data shareable, right? Like you have real customer data,
[00:09:46.880 --> 00:09:50.680]   you don't necessarily know the other teams create silos. So the
[00:09:50.680 --> 00:09:53.440]   question is, you know, by creating an artificial version
[00:09:53.440 --> 00:09:56.880]   of that data set with privacy guarantees built in, can we
[00:09:56.880 --> 00:09:59.120]   enable faster information sharing and more safe
[00:09:59.120 --> 00:10:03.320]   information sharing? Going up a step from there, and I think
[00:10:03.320 --> 00:10:05.440]   something that we're all familiar with in the machine
[00:10:05.440 --> 00:10:08.440]   learning space, is that we never have enough data or we never
[00:10:08.440 --> 00:10:11.160]   have enough of the right data. And that really touches on these
[00:10:11.160 --> 00:10:16.480]   next two topics. I may only have a few examples, we see this
[00:10:16.480 --> 00:10:19.760]   happening more and more, we'll get this in future slides of a
[00:10:19.760 --> 00:10:22.800]   data, and I want to create more examples like it. So I can train
[00:10:22.800 --> 00:10:25.560]   machine learning algorithm that will generalize better. That's a
[00:10:25.560 --> 00:10:29.080]   really big use case for creating more samples from example, like
[00:10:29.080 --> 00:10:33.080]   a text utterance or a rare event that might happen inside of your
[00:10:33.080 --> 00:10:36.120]   machine learning data sets. And finally, this is a really
[00:10:36.120 --> 00:10:38.800]   exciting use case that actually came up from different customer
[00:10:38.800 --> 00:10:42.720]   conversations. As soon as some of our customers realized in the
[00:10:42.720 --> 00:10:45.560]   early days of Gradle that they could create more data, they
[00:10:45.560 --> 00:10:48.000]   wanted to control the type of data they're creating. And some
[00:10:48.000 --> 00:10:50.840]   of the really neat use cases we've had in the healthcare
[00:10:50.840 --> 00:10:53.920]   space or in the fraud detection space, or really anywhere
[00:10:53.920 --> 00:10:58.360]   between, is saying, "Gavin, you know, some type of bias in my
[00:10:58.360 --> 00:11:01.920]   data, maybe it's gender, maybe it's ethnicity, maybe it's
[00:11:01.920 --> 00:11:07.240]   location, right? Can I influence the creation of this model to
[00:11:07.240 --> 00:11:10.560]   create a more fair or like less biased data?"
[00:11:10.560 --> 00:11:20.120]   So I promise this is the only Gartner slide that I will show
[00:11:20.120 --> 00:11:22.680]   during the conversation today, but I think it hits on an
[00:11:22.680 --> 00:11:25.760]   interesting topic, right? So at first, what you see here in this
[00:11:25.760 --> 00:11:27.720]   slide looks pretty controversial, right? This idea
[00:11:27.720 --> 00:11:31.600]   that by 2030, the amount of data, synthetic data used to
[00:11:31.600 --> 00:11:33.840]   train machine learning models will actually supersede the
[00:11:33.840 --> 00:11:37.440]   amount of real data. That's actually being driven by a
[00:11:37.440 --> 00:11:39.640]   couple trends. And we're seeing this with our customers pretty
[00:11:39.640 --> 00:11:43.800]   often. You know, one, I think that's pretty easy to
[00:11:43.800 --> 00:11:50.200]   understand and to see is that if you look at the technology, the
[00:11:50.200 --> 00:11:54.040]   IoT devices in our lives, those IoT devices are advancing
[00:11:54.080 --> 00:11:56.560]   rapidly. And I'll use, for example, like a voice
[00:11:56.560 --> 00:12:01.520]   assistant, like an Alexa or a Google Assistant. Whereas two
[00:12:01.520 --> 00:12:05.080]   years ago, processing for Alexa, for example, would have been
[00:12:05.080 --> 00:12:07.640]   done in the cloud. So it would open up an audio stream and send
[00:12:07.640 --> 00:12:10.360]   your audio stream to the cloud, that would infer your text and
[00:12:10.360 --> 00:12:13.840]   your meaning and respond back to you. That's now becoming
[00:12:13.840 --> 00:12:17.600]   possible on chips. So it's happening on device. It gives
[00:12:17.600 --> 00:12:20.480]   you a faster response time and a better experience as a customer.
[00:12:20.480 --> 00:12:23.360]   It's also better for privacy, and that that real data is not
[00:12:23.360 --> 00:12:26.720]   being sent to the cloud. The challenge that creates is that
[00:12:26.720 --> 00:12:30.520]   those massive models trained on tons of public data, or, you
[00:12:30.520 --> 00:12:32.760]   know, in the case of voice assistant, the data from from
[00:12:32.760 --> 00:12:36.920]   customers isn't there. So when you happen to, for example, like
[00:12:36.920 --> 00:12:39.480]   an utterance, like, you know, Alexa, open the garage door, for
[00:12:39.480 --> 00:12:43.640]   example, that is causing difficulty for your model, the
[00:12:43.640 --> 00:12:46.880]   question is, how do you get more data, generating, you know,
[00:12:46.880 --> 00:12:49.960]   having hiring people to, you know, say these utterances
[00:12:49.960 --> 00:12:52.800]   themselves can be very expensive. So the question is,
[00:12:52.800 --> 00:12:55.840]   can we automate this using natural language processing and
[00:12:55.840 --> 00:12:58.840]   speech synthesis and things like that. So you know, those are
[00:12:58.840 --> 00:13:01.040]   some of the trends that we see driving the use of synthetic
[00:13:01.040 --> 00:13:09.680]   data. So for today, what I wanted to do is touch on a
[00:13:09.680 --> 00:13:14.320]   couple of the core API's that we have at Gretl. It's not all
[00:13:14.320 --> 00:13:17.640]   synthetic data. And then from next go into a couple use cases
[00:13:17.640 --> 00:13:21.560]   and we'll actually run a use case that we had for a top
[00:13:21.560 --> 00:13:25.160]   financial organization in the past couple weeks, we were given
[00:13:25.160 --> 00:13:27.720]   permission to talk about where we did both the parameters we
[00:13:27.720 --> 00:13:31.040]   with weights and biases and then had a really positive result. So
[00:13:31.040 --> 00:13:35.600]   kind of a fun kind of working example to go through. With
[00:13:35.600 --> 00:13:38.920]   Gretl, we view ourselves as a toolkit that can be used by data
[00:13:38.920 --> 00:13:42.080]   scientists or developers. So the goal is to make API's that are
[00:13:42.080 --> 00:13:45.640]   cleanly documented, it's easy to use as possible, and help you
[00:13:45.640 --> 00:13:49.600]   do really hard things. We've talked a bit about synthetics,
[00:13:50.280 --> 00:13:53.880]   training some sort of, you know, language model or GAN, depending
[00:13:53.880 --> 00:13:58.480]   on your use case, to, to on a sensitive data set and telling
[00:13:58.480 --> 00:14:00.920]   it to recreate a similar data set where we can impose some
[00:14:00.920 --> 00:14:04.520]   level of privacy guarantees. We'll go through some of the
[00:14:04.520 --> 00:14:07.880]   specific types of privacy guarantees that you can get
[00:14:07.880 --> 00:14:10.600]   anywhere from things that we have built called privacy
[00:14:10.600 --> 00:14:13.040]   filters, which are meant to improve your resistance to
[00:14:13.040 --> 00:14:15.680]   different types of attacks on data that can include data
[00:14:15.680 --> 00:14:19.920]   linkage attacks, joint ability attacks, things like that
[00:14:19.920 --> 00:14:23.760]   membership inference, to differential privacy, which
[00:14:23.760 --> 00:14:26.840]   actually gives you formal levels of protection. So like when I
[00:14:26.840 --> 00:14:30.920]   say formal, like mathematically proven levels of privacy on a
[00:14:30.920 --> 00:14:34.200]   data set, but that comes at a real cost, it comes at a hit to
[00:14:34.200 --> 00:14:36.960]   the utility of the accuracy of your data. That's something
[00:14:36.960 --> 00:14:40.840]   we'll talk about today. The two other API's, you know, fit in
[00:14:40.840 --> 00:14:44.640]   with synthetics very well. In the middle here, you have
[00:14:44.640 --> 00:14:48.440]   transforms. And that is saying, we don't always want or need to
[00:14:48.480 --> 00:14:50.440]   train a machine learning model on our data set. Sometimes we
[00:14:50.440 --> 00:14:52.520]   just need to change things, we need to encrypt a customer
[00:14:52.520 --> 00:14:58.320]   identifier, we need to identify and replace sensitive location
[00:14:58.320 --> 00:15:02.160]   data or things like that inside of a stream. So often we see
[00:15:02.160 --> 00:15:05.920]   these API's being used together, classify and label applies in
[00:15:05.920 --> 00:15:11.080]   ER to any data stream that goes through Gretel, it labels that
[00:15:11.080 --> 00:15:13.680]   data and that enables policies that we can create with the
[00:15:13.680 --> 00:15:18.280]   transform API, for example, to automatically redact, encrypt,
[00:15:18.280 --> 00:15:22.600]   hash or whatever you want to transform your data. Synthetics
[00:15:22.600 --> 00:15:24.760]   is the final layer where we train the machine learning model
[00:15:24.760 --> 00:15:27.560]   on it. And the kind of power you get out of synthetics is the
[00:15:27.560 --> 00:15:30.480]   ability to control the type of data you create, and to create
[00:15:30.480 --> 00:15:43.480]   unlimited amounts of that data. For a workshop today, we had a
[00:15:43.480 --> 00:15:47.480]   really kind of unique customer challenge recently where we were
[00:15:47.480 --> 00:15:51.880]   in a bake off from a top five global financial firm that was
[00:15:51.880 --> 00:15:55.120]   trying to evaluate the use of synthetic data. And what was
[00:15:55.120 --> 00:15:58.200]   their use case? Like, where are they coming at this from? They
[00:15:58.200 --> 00:16:01.440]   have policies that enable data sharing between teams or between
[00:16:01.440 --> 00:16:05.360]   organizations at the bank. But those policies are based on
[00:16:05.360 --> 00:16:08.760]   manual redaction. And the feedback that we got from that
[00:16:08.760 --> 00:16:12.440]   customer was that the kind of manual de-identification using
[00:16:12.440 --> 00:16:16.240]   k-anonymity and redaction, basically just, you know, kind
[00:16:16.240 --> 00:16:18.360]   of blanking out any kind of sensitive data that exists in
[00:16:18.360 --> 00:16:21.680]   the data set was so strong that it made the data basically
[00:16:21.680 --> 00:16:26.000]   unusable. So teams, while they were able to follow a process to
[00:16:26.000 --> 00:16:29.360]   enable sharing between teams, don't do it because the data
[00:16:29.360 --> 00:16:33.120]   becomes basically so obscured that it's not usable. So the
[00:16:33.120 --> 00:16:35.800]   question was, is it possible to train a synthetic model to
[00:16:35.800 --> 00:16:38.480]   create another data set that looks just like a set of data
[00:16:38.480 --> 00:16:43.320]   sets that they gave us? Maintains the same level of
[00:16:43.320 --> 00:16:47.360]   utility for machine learning task, but has increased privacy.
[00:16:47.360 --> 00:16:51.440]   So we were given a series of data sets, some, you know,
[00:16:51.440 --> 00:16:54.400]   really small, some were time series data sets, which means
[00:16:54.400 --> 00:16:56.760]   they have a time component, which creates another kind of
[00:16:56.760 --> 00:17:00.480]   dimension to look at. And some were really large. And the one
[00:17:00.480 --> 00:17:03.760]   that I wanted to go through today, that, you know, we'll go
[00:17:03.760 --> 00:17:06.160]   through a couple live examples. And you can follow along with,
[00:17:06.880 --> 00:17:11.080]   if you wish, is the smaller data set that also had a time series
[00:17:11.080 --> 00:17:13.280]   component, which made it very difficult because we had to
[00:17:13.280 --> 00:17:17.560]   learn both how to recreate realistic data from from top to
[00:17:17.560 --> 00:17:20.560]   bottom, but we had to maintain time correlation in between each
[00:17:20.560 --> 00:17:26.120]   different element. So for that, we'll go ahead and talk about
[00:17:26.120 --> 00:17:31.240]   how we did this. The goal was to create a process that would
[00:17:31.240 --> 00:17:34.440]   enable information sharing for any team across that giant, you
[00:17:34.440 --> 00:17:37.920]   know, regulatory controlled financial organization. And what
[00:17:37.920 --> 00:17:41.080]   we started with was a couple different steps here, you go
[00:17:41.080 --> 00:17:44.560]   with real world data on the left, we use our transform APIs
[00:17:44.560 --> 00:17:47.680]   and our classified APIs to identify PII, for example, that
[00:17:47.680 --> 00:17:51.880]   exists inside that data to de identify it. The next step is to
[00:17:51.880 --> 00:17:55.160]   train our synthetic model, and then generate use that to
[00:17:55.160 --> 00:17:58.320]   generate synthetic data. And what the customer wanted to do
[00:17:58.320 --> 00:18:01.840]   was evaluate the privacy and the accuracy of that data. And
[00:18:01.840 --> 00:18:04.760]   that's what we're going to dive into the next steps. But saying
[00:18:04.760 --> 00:18:08.200]   like, what is the best privacy and accuracy possible balance
[00:18:08.200 --> 00:18:11.480]   that we can find is what we're going to do in these next few
[00:18:11.480 --> 00:18:16.720]   steps. So I've got slides here, but I think with this, this
[00:18:16.720 --> 00:18:20.520]   audience, it might be best just to go over to an example. First
[00:18:20.520 --> 00:18:24.000]   thing that we did when we had this data set and was to log
[00:18:24.000 --> 00:18:27.800]   right into our console and was to try running, generating
[00:18:27.800 --> 00:18:30.200]   synthetic model just using the default parameters. So let's go
[00:18:30.200 --> 00:18:34.680]   ahead and do that ourselves here. We'll go to console dot
[00:18:34.680 --> 00:18:40.520]   gretl dot cloud. I'm already signed in, all you need is a
[00:18:40.520 --> 00:18:45.600]   GitHub or a Gmail address to sign in, it's free to use. So
[00:18:45.600 --> 00:18:47.320]   we're going to go ahead and sign in here. And we're going to go
[00:18:47.320 --> 00:18:50.520]   ahead and create a synthetic model. So this will help us see
[00:18:50.520 --> 00:18:52.280]   the different parameters and things like that, that we're
[00:18:52.280 --> 00:18:54.640]   going to end up optimizing in a parameter sweep in the next
[00:18:54.640 --> 00:19:01.040]   step. So I can choose a file here, go to desktop, let's see,
[00:19:01.040 --> 00:19:04.080]   pull back this credit time series file.
[00:19:04.080 --> 00:19:10.280]   Open it.
[00:19:10.280 --> 00:19:15.640]   So here we see, we've selected our data set, we're going to
[00:19:15.640 --> 00:19:18.960]   create a new project, name it credit time series data sets,
[00:19:19.080 --> 00:19:25.440]   click Continue. This is going to upload the file to the gretl
[00:19:25.440 --> 00:19:28.720]   service and allow us to choose in the console, what type of API
[00:19:28.720 --> 00:19:31.280]   we want to run. And we have options of using SDK or
[00:19:31.280 --> 00:19:33.720]   command line interface, consoles are really great place to get
[00:19:33.720 --> 00:19:35.360]   started and just kind of get your head wrapped around the
[00:19:35.360 --> 00:19:38.480]   data set. We're going to create synthetic data from it. So we'll
[00:19:38.480 --> 00:19:40.800]   go ahead and follow that work stream here. We can see
[00:19:40.800 --> 00:19:44.720]   discovered as we saw earlier 5000 records, only six fields of
[00:19:44.720 --> 00:19:47.680]   data and mostly numeric data. And we're going to go ahead and
[00:19:47.680 --> 00:19:50.560]   just go with our default recommended configuration. So
[00:19:50.560 --> 00:19:53.400]   here we can see one of the big things that we try to do with
[00:19:53.400 --> 00:19:55.960]   synthetic data is abstract some of the complexity for a
[00:19:55.960 --> 00:19:58.680]   different data set. So we see a bunch of parameters that machine
[00:19:58.680 --> 00:20:01.680]   learning folks here are probably very familiar with. We'll
[00:20:01.680 --> 00:20:05.120]   recommend a policy or allow you to select different policies
[00:20:05.120 --> 00:20:08.160]   here. We'll go with the default and just click Begin training.
[00:20:08.160 --> 00:20:10.640]   And let's see what happens. So here we pulled back a sample.
[00:20:10.640 --> 00:20:13.880]   Sorry, could you zoom in a bit? I think it might be the text
[00:20:13.880 --> 00:20:16.080]   might be a bit small for the audience. Let me see if I can
[00:20:16.080 --> 00:20:18.440]   make it bigger. That's perfect. Thank you.
[00:20:18.440 --> 00:20:22.800]   Here we can see it's going to give us a preview, like a
[00:20:22.800 --> 00:20:25.920]   pandas dataframe style preview of the the data set. And we can
[00:20:25.920 --> 00:20:29.080]   see, you know, we have a derived field, which is the net amount,
[00:20:29.080 --> 00:20:32.440]   but really a typical credit time series data set where we have a
[00:20:32.440 --> 00:20:36.480]   date, we have an account balance, we have a district ID.
[00:20:36.480 --> 00:20:40.560]   So this is for individual district bank that's reporting
[00:20:40.560 --> 00:20:44.000]   its information, credit and debit amounts. And our goal is
[00:20:44.000 --> 00:20:47.680]   to recreate a time series data set that maintains the same kind
[00:20:47.680 --> 00:20:51.160]   of time series for each district. So here using the
[00:20:51.160 --> 00:20:53.000]   default parameters, it's going to go ahead and load these
[00:20:53.000 --> 00:20:55.880]   things up, you can see those default parameters. This is what
[00:20:55.880 --> 00:21:01.160]   we're going to optimize in the next few steps. It started up a
[00:21:01.160 --> 00:21:03.280]   worker. So essentially, what it's doing the cloud here is
[00:21:03.280 --> 00:21:06.960]   spinning up a GPU instance inside a container, and it's
[00:21:06.960 --> 00:21:09.040]   beginning training. And this is going to take a while. So I can
[00:21:09.040 --> 00:21:11.840]   go ahead and kind of find a project and skip to the results.
[00:21:12.160 --> 00:21:14.760]   But at the end, what we'll do is generate a sample data set. And
[00:21:14.760 --> 00:21:17.080]   then it will give us a report telling us about the quality of
[00:21:17.080 --> 00:21:20.120]   that data. Essentially, how good of a job did the model do
[00:21:20.120 --> 00:21:25.360]   learning using the default parameters? projects here, I go
[00:21:25.360 --> 00:21:29.320]   ahead and search for this example. So this is a completed
[00:21:29.320 --> 00:21:34.160]   model that we ran. So it's called Hanson's with gorilla.
[00:21:34.160 --> 00:21:36.640]   Go ahead and take a look at it. See how it turned out.
[00:21:36.640 --> 00:21:40.560]   Sorry, I'm I'm a bit confused. Are we generating more data when
[00:21:40.560 --> 00:21:43.400]   you say you're training a model here is that model to generate
[00:21:43.400 --> 00:21:46.520]   more synthetic data based on the pattern inside of the one you
[00:21:46.520 --> 00:21:47.280]   just uploaded?
[00:21:47.280 --> 00:21:51.720]   That is the end goal. That is the task. This step is training
[00:21:51.720 --> 00:21:54.560]   the model. So we can use that model to generate as much data
[00:21:54.560 --> 00:21:58.240]   as you want to. So when we just kicked off training a minute ago
[00:21:58.240 --> 00:22:01.800]   it behind the scenes is training a sequential language model on
[00:22:01.800 --> 00:22:04.760]   this data set that we just looked at. It will then generate
[00:22:04.760 --> 00:22:07.840]   a sample data set which we can look at and it will use that as
[00:22:07.840 --> 00:22:10.200]   you can see here is a sample data set of generated to
[00:22:10.200 --> 00:22:12.520]   evaluate the quality of the model. So essentially, what
[00:22:12.520 --> 00:22:15.680]   we're going to do is throw the kitchen sink at it from a data
[00:22:15.680 --> 00:22:18.640]   science perspective and say like, looking at correlations
[00:22:18.640 --> 00:22:22.360]   and distributions of our big work, you know, synthetic data
[00:22:22.360 --> 00:22:25.200]   versus the real world data, how good of a job did we do?
[00:22:25.200 --> 00:22:26.880]   That makes sense.
[00:22:26.880 --> 00:22:30.760]   So here we can see our quality score wasn't wasn't that great.
[00:22:30.760 --> 00:22:33.280]   It trained in about 10 minutes. And let's go ahead and take a
[00:22:33.280 --> 00:22:36.640]   look and see why. So we click on the report. What I like to do
[00:22:36.640 --> 00:22:42.120]   is go to the detailed report. So I'll pull this open. And I
[00:22:42.120 --> 00:22:45.800]   say HTML representation here and see that by default, we have
[00:22:45.800 --> 00:22:49.160]   privacy filters on so it recreated 5000 rows, but none of
[00:22:49.160 --> 00:22:51.640]   those rows matched the data it was trained on, which is a good
[00:22:51.640 --> 00:22:55.360]   thing. Get a quick overview of the privacy of our data. Where
[00:22:55.360 --> 00:22:57.720]   I really like to go when it comes to look at these things is
[00:22:57.720 --> 00:23:01.120]   looking at correlations that existed in the training data set
[00:23:01.120 --> 00:23:03.040]   where you can see some real correlations here between
[00:23:03.040 --> 00:23:06.440]   account balance and debit amount, for example. Here we can
[00:23:06.440 --> 00:23:09.560]   see using default parameters on this, like, actually very
[00:23:09.560 --> 00:23:12.560]   challenging data set, how well were those correlations
[00:23:12.560 --> 00:23:16.080]   maintained. And we can see that the model here, unfortunately,
[00:23:16.080 --> 00:23:18.040]   and this is pretty rare. And this is why I picked this data
[00:23:18.040 --> 00:23:21.920]   set actually had a hard time with this data set to begin
[00:23:21.920 --> 00:23:24.480]   with. So here you see the difference. And you can see for
[00:23:24.480 --> 00:23:26.880]   some of these things like that relationship between account
[00:23:26.880 --> 00:23:30.440]   balance and the amount of date, debit amount, for example, was
[00:23:30.440 --> 00:23:35.240]   not recreated really well by the model. Also a PCA view, another
[00:23:35.240 --> 00:23:38.400]   kind of favorite in the data scientists toolkit here, here we
[00:23:38.400 --> 00:23:40.880]   look at that roughly the distribution of elements and we
[00:23:40.880 --> 00:23:43.840]   can get a good feel for whether the model overfit or not. Here
[00:23:43.840 --> 00:23:46.200]   we see a pretty good distribution. So it distributed
[00:23:46.200 --> 00:23:50.640]   the elements well. We can look at individual field correlations
[00:23:50.640 --> 00:23:52.680]   and things like that. And you can see the account balance
[00:23:52.680 --> 00:23:56.120]   looks pretty good. The district ID it seemed to the synthetic
[00:23:56.120 --> 00:23:58.640]   model seemed to have picked a couple districts that it liked
[00:23:58.640 --> 00:24:00.840]   better than some of the others, which may have caused some of
[00:24:00.840 --> 00:24:04.360]   our problems. But overall, the rest of these distributions when
[00:24:04.360 --> 00:24:07.920]   you compare real world versus synthetic data look pretty good.
[00:24:07.920 --> 00:24:14.240]   So a 60 out of 100 for us is an indication that the model had
[00:24:14.240 --> 00:24:17.200]   some room left for optimization. And our challenge here was to
[00:24:17.200 --> 00:24:20.600]   create the best data set that we possibly could. So the next
[00:24:20.600 --> 00:24:22.920]   thing we've done is, you know, taking a look at those default
[00:24:22.920 --> 00:24:25.640]   parameters, can we run a parameter sweep? Can we train a
[00:24:25.640 --> 00:24:29.840]   couple models on this and see if we can improve the accuracy. So
[00:24:29.840 --> 00:24:32.640]   here I'm going to share a notebook. If anyone wants to
[00:24:32.640 --> 00:24:34.960]   follow along, there's a simple way to get to this notebook. If
[00:24:34.960 --> 00:24:38.880]   you go to docs, I've read all about AI. Let's go ahead down to
[00:24:38.880 --> 00:24:42.520]   synthetics, SDK notebooks. Down at the bottom, you'll see this
[00:24:42.520 --> 00:24:46.040]   parameter sweep using weights and biases. Click open in CoLab
[00:24:46.040 --> 00:24:50.840]   and that's where we are right now. I'm going to make this font
[00:24:50.840 --> 00:24:51.760]   a little bit bigger.
[00:24:52.160 --> 00:24:53.280]   I was just going to request.
[00:24:53.280 --> 00:24:58.000]   And we'll go through you know, what looks like a fairly
[00:24:58.000 --> 00:24:59.880]   complex notebook, but actually the end of the day is pretty
[00:24:59.880 --> 00:25:01.120]   simple. So
[00:25:01.120 --> 00:25:04.960]   I wanted to interrupt you and ask just out of curiosity, how
[00:25:04.960 --> 00:25:08.640]   are you evaluating that score? And also, do you assume that
[00:25:08.640 --> 00:25:11.440]   someone would have done ED on the data set? So like if I
[00:25:11.440 --> 00:25:15.600]   upload a garbage data set, what happens there? What's what's
[00:25:15.600 --> 00:25:17.520]   the synthetic data that comes out of it?
[00:25:19.000 --> 00:25:22.120]   To use the like the age old term, you know, garbage in
[00:25:22.120 --> 00:25:22.640]   garbage out.
[00:25:22.640 --> 00:25:26.680]   The good thing about our synthetic model and the
[00:25:26.680 --> 00:25:29.360]   approach is that it's very flexible for different types of
[00:25:29.360 --> 00:25:33.520]   data. So we don't have to define a column as being dates, for
[00:25:33.520 --> 00:25:37.280]   example, or a text field as being text. Under the hood, as I
[00:25:37.280 --> 00:25:40.400]   mentioned earlier, it's, it's, it's a language model. So it
[00:25:40.400 --> 00:25:42.880]   essentially tries to recreate the language that it was
[00:25:42.880 --> 00:25:46.560]   trained on, which makes it very flexible. So you can view it as
[00:25:46.560 --> 00:25:50.720]   like a, an open AI GPT, but applied to in this case, tabular
[00:25:50.720 --> 00:25:51.080]   data.
[00:25:51.080 --> 00:25:54.880]   That makes sense. Thanks for that clarification.
[00:25:54.880 --> 00:25:58.400]   So running through here, I've made a few modifications to the
[00:25:58.400 --> 00:26:02.960]   the default weights and biases parameter sweep. So here we can
[00:26:02.960 --> 00:26:04.560]   see we're going to install weights and biases, we're
[00:26:04.560 --> 00:26:06.560]   installed the Gradle client, we're gonna log in,
[00:26:06.560 --> 00:26:11.000]   log in next to Gradle, so you can give an API key or we can
[00:26:11.000 --> 00:26:13.480]   drop it as an environment variable, whatever works best.
[00:26:14.480 --> 00:26:17.480]   So we're going to adjust some of those hyper parameters that you
[00:26:17.480 --> 00:26:20.480]   saw in the default configuration, run a series of
[00:26:20.480 --> 00:26:23.280]   tests and see how good of a result we can get here with the
[00:26:23.280 --> 00:26:23.800]   data set.
[00:26:23.800 --> 00:26:28.000]   You one of the questions you were asking earlier was like,
[00:26:28.000 --> 00:26:30.680]   what is the how are we optimizing? So we'll see this
[00:26:30.680 --> 00:26:33.640]   in just a second here. So we're going to do a Bayesian sweep,
[00:26:33.640 --> 00:26:37.560]   which means we're going to ask the underlying model with
[00:26:37.560 --> 00:26:40.920]   weights and biases to continuously iterate and improve
[00:26:40.920 --> 00:26:43.240]   on the parameters that it's picking based on the results
[00:26:43.240 --> 00:26:43.800]   that it's seen.
[00:26:44.800 --> 00:26:47.800]   The metric we're going to use here isn't our loss or accuracy
[00:26:47.800 --> 00:26:50.280]   like you normally would do. Here we're using we're introducing a
[00:26:50.280 --> 00:26:54.640]   metric called SQS. SQS is the synthetic quality score that we
[00:26:54.640 --> 00:26:58.800]   just saw the 60 out of 100 that we're going to try to optimize,
[00:26:58.800 --> 00:27:04.040]   which is meant to be a single number to kind of highlight, you
[00:27:04.040 --> 00:27:06.760]   know, the quality of the distribution, the quality of the
[00:27:06.760 --> 00:27:08.440]   correlations that were learned by our model.
[00:27:12.800 --> 00:27:15.120]   Here, we're going to go ahead and build our initial parameter
[00:27:15.120 --> 00:27:18.160]   sweep. So very, you know, here, we're picking a series of
[00:27:18.160 --> 00:27:20.720]   hyper parameters that are very sensitive to the size and the
[00:27:20.720 --> 00:27:23.640]   type of data, the number of epochs in the learning rate, for
[00:27:23.640 --> 00:27:28.120]   example, vocabulary size under the hood, Gretel is using
[00:27:28.120 --> 00:27:31.920]   sentence piece, or word piece based tokenization. So you can
[00:27:31.920 --> 00:27:35.200]   choose a vocab size of zero says I want you just to break it
[00:27:35.200 --> 00:27:37.640]   down and predict the next character. So single character
[00:27:37.640 --> 00:27:42.000]   based prediction, or we can say you can use up to 20,000, build
[00:27:42.000 --> 00:27:44.920]   a vocabulary yourself using unsupervised learning of up to
[00:27:44.920 --> 00:27:48.360]   20,000 different words. So we'll see how that affects the model
[00:27:48.360 --> 00:27:53.040]   performance, the level of RNN complexity. So for this model,
[00:27:53.040 --> 00:27:57.320]   we're using a LSTM. So how many LSTM units do we want to use?
[00:27:57.320 --> 00:28:02.560]   1024, 2048, or higher units are really good for free text. If
[00:28:02.560 --> 00:28:06.160]   you have not much complexity, you can get much faster answer
[00:28:06.520 --> 00:28:13.040]   using 64 to 56. So we'll create this sweep. And the next thing
[00:28:13.040 --> 00:28:16.280]   we need to do after building the sweep is to build a sense first
[00:28:16.280 --> 00:28:20.000]   load our data set, same data set we looked at a minute ago. So
[00:28:20.000 --> 00:28:26.320]   we can see it have six different columns and 5500 rows. We'll
[00:28:26.320 --> 00:28:30.400]   create a project here inside weights and biases. And then
[00:28:30.400 --> 00:28:32.920]   here's really where the meat of our project is. So here, we're
[00:28:32.920 --> 00:28:35.280]   going to define a train function. And that train
[00:28:35.280 --> 00:28:39.360]   function is going to pull back the configuration that weights
[00:28:39.360 --> 00:28:42.720]   and biases is creating. It's going to update the Gretel
[00:28:42.720 --> 00:28:46.040]   configuration. So here we pull back our default configuration
[00:28:46.040 --> 00:28:50.120]   yaml. We're just doing that with a Python call. We're modifying
[00:28:50.120 --> 00:28:52.920]   that configuration based on whatever for this individual
[00:28:52.920 --> 00:28:58.320]   test that weights and biases wants to use. We've got some
[00:28:58.320 --> 00:29:01.240]   code in here, since we don't have a native integration for
[00:29:01.240 --> 00:29:04.280]   weights and biases yet to extract the the accuracy and the
[00:29:04.280 --> 00:29:08.560]   loss in the epoch and timestamp. And at the end of each run, we
[00:29:08.560 --> 00:29:11.400]   upload this metric, this synthetic quality score metric
[00:29:11.400 --> 00:29:14.560]   that we extract from the from the report. So this is sent
[00:29:14.560 --> 00:29:17.600]   back and this is what we're trying to optimize on. And for
[00:29:17.600 --> 00:29:20.520]   the next step, I told you to run on a step of 20 different counts.
[00:29:20.520 --> 00:29:23.040]   So we're gonna run 20 experiments and see what types of
[00:29:23.040 --> 00:29:25.240]   parameters worked and how good we can get that synthetic
[00:29:25.240 --> 00:29:30.760]   quality score. Go ahead and click here. Let's open up our
[00:29:30.760 --> 00:29:37.000]   results and see how this these experiments went. So we can see
[00:29:37.000 --> 00:29:40.160]   the results of 20 different experiments. My favorite place
[00:29:40.160 --> 00:29:43.120]   to start here is going all the way to the end. I have very
[00:29:43.120 --> 00:29:45.680]   little patience for these things. We see a definite
[00:29:45.680 --> 00:29:48.600]   pattern of different hyper parameters resulting in much
[00:29:48.600 --> 00:29:52.160]   higher quality SQS scores. So here, you know, we see us
[00:29:52.160 --> 00:29:55.600]   getting into the 70 out of 100 range, which is pretty good. The
[00:29:55.600 --> 00:29:59.120]   final score that we got with a few additional tweaks was up at
[00:29:59.120 --> 00:30:02.280]   89. So I'm previewing kind of what's next. But here we see
[00:30:02.280 --> 00:30:05.720]   batch size not mattering that much. The number of epochs it
[00:30:05.720 --> 00:30:09.640]   looks like, you know, 50 ish epochs works very well. And that
[00:30:09.640 --> 00:30:12.080]   kind of really the kind of surprising thing here or the
[00:30:12.080 --> 00:30:15.040]   thing that that we have seen is we're working with small data
[00:30:15.040 --> 00:30:19.000]   sets versus large data set is actually using less RNN units
[00:30:19.000 --> 00:30:22.240]   for the LSTM and a smaller vocabulary size. And what that
[00:30:22.240 --> 00:30:24.760]   means is by doing character based tokenization instead of
[00:30:24.760 --> 00:30:27.160]   setting vocabulary, it actually gives the model a lot more
[00:30:27.160 --> 00:30:30.360]   opportunities to learn on a really limited data set. So here
[00:30:30.360 --> 00:30:32.800]   we can see some examples where we told it to do character based
[00:30:32.800 --> 00:30:36.000]   tokenization and a relatively complex network and end up with
[00:30:36.000 --> 00:30:39.720]   really good results. So for the next step, we're going to take
[00:30:39.720 --> 00:30:44.240]   the optimal parameters from the sweep and apply it to creating a
[00:30:44.240 --> 00:30:47.080]   model. And then we'll look at actually how it performs on a
[00:30:47.080 --> 00:30:51.040]   real world task. So SQS is beneficial in the sense that it
[00:30:51.040 --> 00:30:54.640]   gives us a very general view of how our data set performs. It
[00:30:54.640 --> 00:30:57.280]   doesn't save a particular task, ML inference, time series,
[00:30:57.280 --> 00:31:00.520]   things like that, how well it's going to perform. So the next
[00:31:00.520 --> 00:31:03.400]   step here that we're going to go to is talking about how well it
[00:31:03.400 --> 00:31:07.240]   worked for the real world task that the bank that we're working
[00:31:07.240 --> 00:31:08.080]   with wanted to apply it to.
[00:31:08.080 --> 00:31:21.640]   So skipping on a little bit more, every type of data set has
[00:31:21.640 --> 00:31:24.480]   a different type of task that you want to apply to it and time
[00:31:24.480 --> 00:31:27.440]   series is particularly tricky in the sense of how are you going
[00:31:27.440 --> 00:31:30.640]   to evaluate this time series model? There aren't a lot of
[00:31:30.640 --> 00:31:33.200]   really established metrics for evaluating the quality of a
[00:31:33.200 --> 00:31:37.720]   synthetic time series. The bank that we're working with, the
[00:31:37.720 --> 00:31:40.280]   data science team, they're a really excellent team and they
[00:31:40.280 --> 00:31:43.880]   came up with an idea. What they wanted to do was to fit in a
[00:31:43.880 --> 00:31:47.920]   REMA model on that time series and essentially measure the
[00:31:48.320 --> 00:31:56.120]   root mean squared error for the accuracy or loss that we have of
[00:31:56.120 --> 00:32:00.240]   our model for real world data versus synthetic data. So here
[00:32:00.240 --> 00:32:02.520]   what we did is instead of using the synthetic quality score, we
[00:32:02.520 --> 00:32:07.920]   ran the same configuration, same 20 sweeps and essentially
[00:32:07.920 --> 00:32:10.960]   applied the results of those sweeps, so generated data set
[00:32:10.960 --> 00:32:13.960]   and ran on this REMA model, which I'll show you guys the
[00:32:13.960 --> 00:32:17.840]   code for in a minute, against both the synthetic and the real
[00:32:17.840 --> 00:32:20.320]   world data. And what we look for is we wanted to minimize the
[00:32:20.320 --> 00:32:25.320]   level of RMSE. And what was really interesting here is that
[00:32:25.320 --> 00:32:28.360]   actually if you look at this config, here's config 19 and the
[00:32:28.360 --> 00:32:32.080]   19th sweep, our real world loss, at least for some of these
[00:32:32.080 --> 00:32:35.680]   parameters, was more than it was for synthetic data. It actually
[00:32:35.680 --> 00:32:38.640]   shows that the model learned something about this data set
[00:32:38.640 --> 00:32:40.800]   that actually gave it better than real world performance,
[00:32:40.800 --> 00:32:46.040]   which is interesting. And what we decided to do is select that
[00:32:46.040 --> 00:32:55.480]   configuration and use that to generate a final data set. Let's
[00:32:55.480 --> 00:32:58.520]   go ahead now and we'll jump over to this final notebook. And
[00:32:58.520 --> 00:33:00.880]   we'll look at this final data set configuration that we ran
[00:33:00.880 --> 00:33:03.120]   and the synthetic quality score with it. So I've got another
[00:33:03.120 --> 00:33:05.440]   notebook here, you can get to the same notebook here by going
[00:33:05.440 --> 00:33:11.040]   SDK notebooks and opening up the time series POC. And I'm going
[00:33:11.040 --> 00:33:16.280]   to zip in a little bit to the point where we can see the
[00:33:16.280 --> 00:33:21.480]   actual model training. Thanks for the patience here. So here
[00:33:21.480 --> 00:33:24.800]   we can see our optimized parameters based on a really
[00:33:24.800 --> 00:33:27.800]   pretty detailed and complex sweep that we did across the
[00:33:27.800 --> 00:33:32.160]   parameters. By using that we trained a model we generated
[00:33:32.160 --> 00:33:35.960]   data. We can look at the synthetic quality score report
[00:33:35.960 --> 00:33:38.400]   here and we can see the improvements that were made from
[00:33:38.400 --> 00:33:41.520]   this optimization. So here we're up to an 89 out of 100. After
[00:33:41.520 --> 00:33:44.760]   this really detailed sweep, we can see once again, no training
[00:33:44.760 --> 00:33:48.040]   lines were duplicated in the data, get some ideas here about
[00:33:48.040 --> 00:33:50.280]   the different levels of privacy protections that were enabled.
[00:33:50.280 --> 00:33:53.600]   And then here we see this is the really good news is that we see
[00:33:53.600 --> 00:33:56.040]   that those correlations that were in the original input set
[00:33:56.040 --> 00:33:59.120]   were learned and recreated very well by the synthetic data set.
[00:33:59.120 --> 00:34:01.840]   So this is a this is an example where a little tuning goes a
[00:34:01.840 --> 00:34:06.880]   long way. So once again, you know, good initial indications
[00:34:06.880 --> 00:34:09.600]   that this is going to this is going to be a good model for us.
[00:34:09.600 --> 00:34:13.040]   So we're happy with this and that equality report. Here we
[00:34:13.040 --> 00:34:15.920]   can see field distributions match very well between the
[00:34:15.920 --> 00:34:17.960]   synthetic and real world data, but not exactly. You don't want
[00:34:17.960 --> 00:34:21.560]   it to match exactly or you've got a problem. So good
[00:34:21.560 --> 00:34:23.960]   indications here. And then what we'll look at next is
[00:34:23.960 --> 00:34:27.640]   implementation that we had. First, we took a make this a
[00:34:27.640 --> 00:34:32.080]   little smaller. So we picked an arbitrary district, you know,
[00:34:32.080 --> 00:34:35.200]   13 lucky number, let's try it. And we looked at the time series
[00:34:35.200 --> 00:34:37.640]   for the synthetic versus the original data. So here we see
[00:34:37.640 --> 00:34:40.760]   kind of a nice distribution, we see a trend, and we see our
[00:34:40.760 --> 00:34:43.720]   synthetic data following that trend very well, which is, you
[00:34:43.720 --> 00:34:46.320]   know, it's one example out of 100. We're looking at district
[00:34:46.320 --> 00:34:51.720]   13 here, but it's it's a and these different variables. So
[00:34:51.720 --> 00:34:53.560]   we're looking at the net amount of the credit amount and things
[00:34:53.560 --> 00:34:56.200]   like that. But we see it following that time series very
[00:34:56.200 --> 00:34:58.200]   well, which means that the learn the model appeared to have
[00:34:58.200 --> 00:35:04.560]   learned that and been able to recreate it. For a Remo, we
[00:35:04.560 --> 00:35:06.840]   didn't know exactly how the bank was going to implement this.
[00:35:06.840 --> 00:35:10.880]   So we ended up using SK learn, we use the Sari max model for
[00:35:10.880 --> 00:35:15.840]   our own implementation for the evaluating the model accuracy.
[00:35:15.840 --> 00:35:18.560]   But here's the code, you can kind of follow through this. But
[00:35:18.560 --> 00:35:21.000]   essentially, what we do is we fit the model, we measure the
[00:35:21.000 --> 00:35:25.000]   root mean squared error against our best configuration. And then
[00:35:25.000 --> 00:35:28.600]   here we can see the results. So for synthetic data, you know,
[00:35:28.600 --> 00:35:31.360]   he would see one across one of these variables, actually that
[00:35:31.360 --> 00:35:33.960]   the the net amount actually had better than real world
[00:35:33.960 --> 00:35:37.440]   performance. And on some of these other variables that we're
[00:35:37.440 --> 00:35:39.720]   looking at, we had a slight degradation in performance
[00:35:39.720 --> 00:35:43.440]   moving to synthetic from real world. But the benefit that we
[00:35:43.440 --> 00:35:46.560]   get there is privacy. And that's a second part to talk about.
[00:35:46.560 --> 00:35:50.560]   This is a really difficult data set because it's so small. Often
[00:35:50.560 --> 00:35:54.200]   we see data sets in the you know, 50,000 100,000 million
[00:35:54.200 --> 00:35:58.080]   size that we're synthetic data is just as good or right at
[00:35:58.080 --> 00:36:00.360]   sometimes even better than the real world performance.
[00:36:00.360 --> 00:36:09.120]   So back, taking a look at privacy, we have the really the
[00:36:09.120 --> 00:36:12.640]   easy kind of view of privacy here, where we can see none of
[00:36:12.640 --> 00:36:14.720]   the data was recreated. And that ended up being the metric that
[00:36:14.720 --> 00:36:17.840]   the bank was using to evaluate privacy was saying, you know,
[00:36:17.840 --> 00:36:22.200]   what combinations of fields recreated between the synthetic
[00:36:22.200 --> 00:36:27.880]   and the real world data. As part of the initial run that you'll
[00:36:27.880 --> 00:36:31.280]   see in that notebook, we do a transform, which introduces a
[00:36:31.280 --> 00:36:35.680]   number shift into dates and into numbers. So we knew from the
[00:36:35.680 --> 00:36:38.000]   very beginning that none of the data or no combination data
[00:36:38.000 --> 00:36:40.240]   would be recreated, because we did essentially a
[00:36:40.240 --> 00:36:44.120]   transformation. But it's still good to see that even between
[00:36:44.120 --> 00:36:47.560]   the training data here, nothing was was recreated. Where things
[00:36:47.560 --> 00:36:50.320]   really start to get fancy is finding that balance between
[00:36:50.320 --> 00:36:53.240]   accuracy, like how well is this gonna work for my ML use case,
[00:36:53.240 --> 00:36:55.800]   and privacy, right? Am I comfortable sharing this with my
[00:36:55.800 --> 00:36:58.280]   team? Am I comfortable sharing to other teams? Am I comfortable
[00:36:58.280 --> 00:37:00.920]   like publishing this? How do we make it really simple to do
[00:37:00.920 --> 00:37:04.040]   that? We have a set of tools. And here I'm going to talk and
[00:37:04.040 --> 00:37:07.040]   walk through one of the initial steps here, which is called
[00:37:07.040 --> 00:37:10.400]   privacy filtering. This is on by default. And this was something
[00:37:10.400 --> 00:37:13.320]   that was built by our team. And we have some some blogs talking
[00:37:13.320 --> 00:37:16.640]   about how it works. But our applied science team took a look
[00:37:16.640 --> 00:37:21.040]   at different attacks that happen on privacy of data sets. And
[00:37:21.040 --> 00:37:24.280]   happy to dive into some examples here for real world examples of
[00:37:24.280 --> 00:37:26.840]   this. But, you know, given a data set that you believe has
[00:37:26.840 --> 00:37:30.400]   been de identified, how might someone attack it or exploit it
[00:37:30.400 --> 00:37:32.920]   to determine, you know, membership inference, for
[00:37:32.920 --> 00:37:36.280]   example, like was someone I know, or something I know,
[00:37:36.280 --> 00:37:39.120]   present inside of this training data set, which can have some
[00:37:39.120 --> 00:37:42.920]   serious privacy implications. And a second type of attack that
[00:37:42.920 --> 00:37:45.520]   we're really concerned about is joint ability or data linkage
[00:37:45.520 --> 00:37:48.600]   attacks. And in that case, you have a data set that you believe
[00:37:48.600 --> 00:37:52.400]   has been de identified. But it can be combined with some other
[00:37:52.400 --> 00:37:55.080]   public data set that you know, you either know about or you
[00:37:55.080 --> 00:37:58.520]   don't know about, that would uncover the identities of the
[00:37:58.520 --> 00:38:02.040]   users. And what we found looking at those different types of
[00:38:02.040 --> 00:38:07.240]   attacks is the most really susceptible records inside of
[00:38:07.240 --> 00:38:10.680]   this entire data set are the outliers. So whether you know a
[00:38:10.680 --> 00:38:12.880]   record is too similar to the training data, which creates
[00:38:12.880 --> 00:38:15.880]   risk, or you've got something that's just fundamentally
[00:38:15.880 --> 00:38:18.880]   different, right, you have a user location that's in Seattle
[00:38:18.880 --> 00:38:21.880]   when the rest of your locations are across Western United
[00:38:21.880 --> 00:38:26.200]   States, or things like that, really create privacy risks. So
[00:38:26.200 --> 00:38:29.240]   we have a series of filters that we're kind of visualizing right
[00:38:29.240 --> 00:38:32.680]   here between the synthetic and the real world data. And here
[00:38:32.680 --> 00:38:36.280]   you can see, for these outliers scores, like how few records
[00:38:36.280 --> 00:38:38.840]   actually need to be removed to give you really actually pretty
[00:38:38.840 --> 00:38:42.800]   strong protections for privacy. So by default, we set for
[00:38:42.800 --> 00:38:45.760]   outliers, we have this kind of medium or higher level. So you
[00:38:45.760 --> 00:38:48.640]   can see a very small percentage of overall records actually
[00:38:48.640 --> 00:38:50.960]   being filtered out, but that giving you really strong
[00:38:50.960 --> 00:38:53.760]   privacy guarantees. If you were sharing this across your
[00:38:53.760 --> 00:38:56.560]   business, for example, you don't trust me or know the other teams
[00:38:56.560 --> 00:38:59.080]   you're sharing with you want stronger permissions. Here, you
[00:38:59.080 --> 00:39:01.200]   can look at something called the privacy protection level and set
[00:39:01.200 --> 00:39:04.800]   it to high. So you see more records being filtered. And then
[00:39:04.800 --> 00:39:07.440]   the final step and that's something I would encourage, you
[00:39:07.440 --> 00:39:11.120]   know, anyone that's interested, try using differential privacy,
[00:39:11.120 --> 00:39:13.560]   I would recommend trying it on a different data set, all you have
[00:39:13.560 --> 00:39:16.480]   to do is change the hyper parameters from, you know, DP,
[00:39:16.480 --> 00:39:19.400]   which is differential privacy false to true, and experiment
[00:39:19.400 --> 00:39:22.360]   there. But what we found is like actually using a surprisingly
[00:39:22.360 --> 00:39:25.960]   small amount of noise and clipping model gives you really
[00:39:25.960 --> 00:39:31.040]   strong real world protections. So you know, your ability, while
[00:39:31.040 --> 00:39:34.200]   you're nowhere close to a delta or an epsilon that would give
[00:39:34.200 --> 00:39:37.640]   you mathematical formal guarantees, what we found is
[00:39:37.640 --> 00:39:41.080]   that for practical guarantees, like was a credit card member
[00:39:41.080 --> 00:39:43.800]   number memorized, my data set was a name memorized or things
[00:39:43.800 --> 00:39:47.720]   like that, a small amount of noise and clipping used by our
[00:39:47.760 --> 00:39:52.440]   implementation of it's called DPSGD actually gives you really
[00:39:52.440 --> 00:39:58.080]   strong privacy benefits. So in conclusion here, really the
[00:39:58.080 --> 00:40:02.040]   combination here of both pretty encouraging results when we
[00:40:02.040 --> 00:40:04.680]   looked at, you know, individual pieces of the data or the
[00:40:04.680 --> 00:40:08.960]   overall root mean squared error, were really good. And combining
[00:40:08.960 --> 00:40:12.480]   that with this, like kind of view of privacy, and essentially
[00:40:12.480 --> 00:40:16.200]   having some some knobs that you can tune to adjust your privacy
[00:40:16.200 --> 00:40:20.080]   and accuracy balance. Met the needs for you know, this
[00:40:20.080 --> 00:40:22.600]   particular customer, it was a really neat challenge for us,
[00:40:22.600 --> 00:40:26.440]   because it was such a small data set. And we had to infer or
[00:40:26.440 --> 00:40:28.920]   learn so many different patterns in such a small set of data.
[00:40:28.920 --> 00:40:35.840]   Well, Sandy, I want to make sure we leave time, I'm happy to go
[00:40:35.840 --> 00:40:39.480]   over some other use cases as well. But that's most of what I
[00:40:39.480 --> 00:40:40.680]   had planned for today.
[00:40:40.680 --> 00:40:44.320]   I don't see any community questions. Let me quickly
[00:40:44.320 --> 00:40:48.240]   refresh that just to make sure. But I was really curious about
[00:40:48.240 --> 00:40:51.080]   how this works effectively on other data sets. Can you talk
[00:40:51.080 --> 00:40:55.320]   about that outside of time series fit? We can we use the
[00:40:55.320 --> 00:40:56.240]   greater test? Okay.
[00:40:56.240 --> 00:41:00.680]   Yeah, we see Gretel being used. And that's really kind of our
[00:41:00.680 --> 00:41:03.480]   goal is to build the most general, you know, platform for
[00:41:03.480 --> 00:41:08.600]   synthetic data possible. So we have worked in anywhere from
[00:41:08.600 --> 00:41:10.920]   working with the gaming community, we've got a couple
[00:41:10.920 --> 00:41:14.840]   customers in the gaming space, that are training models to
[00:41:14.840 --> 00:41:20.720]   detect cheating inside of their, their infrastructure, right? So
[00:41:20.720 --> 00:41:24.440]   it's a big problem. Yeah, huge, as we as we know, right. And
[00:41:24.440 --> 00:41:26.360]   then, you know, when you come down to at the end of the day,
[00:41:26.360 --> 00:41:29.400]   like, you have some known kind of bot activity you can train
[00:41:29.400 --> 00:41:33.360]   on, but you also need, you know, human activity in there as well.
[00:41:33.360 --> 00:41:36.280]   And that player data is very sensitive, right, including like
[00:41:36.280 --> 00:41:39.160]   AR and VR space, and times like games are sending back
[00:41:39.160 --> 00:41:43.800]   incredibly sensitive information about like user health or your
[00:41:43.800 --> 00:41:45.640]   heartbeats per minute coming back from Apple Watch. The
[00:41:45.640 --> 00:41:47.640]   question is like, how do I use that data in a way that's
[00:41:47.640 --> 00:41:51.920]   privacy preserving? That's been a cool example. We've got, and I
[00:41:51.920 --> 00:41:54.640]   think the biggest probably torture test that we've had for
[00:41:54.640 --> 00:41:57.480]   synthetic data, and probably one of the most complex data sets I
[00:41:57.480 --> 00:42:01.040]   could ever imagine was, we had an opportunity to, to partner
[00:42:01.040 --> 00:42:04.760]   and work with a company called Illumina, which is in the
[00:42:04.760 --> 00:42:09.200]   genomic space, and working initially with mice genomes. But
[00:42:09.200 --> 00:42:13.600]   essentially, we started out with this idea of given medical
[00:42:13.600 --> 00:42:17.920]   research, and you know, all of the cures that are being powered
[00:42:17.920 --> 00:42:23.360]   by like a data driven medicine right now. The genomic data is
[00:42:23.360 --> 00:42:26.280]   the most identifying and hard to share of any data that you can
[00:42:26.280 --> 00:42:28.880]   imagine. When you look at the complexity of this, even a mouse
[00:42:28.880 --> 00:42:36.040]   genome is 97,000 columns of data that your system has to learn and
[00:42:36.040 --> 00:42:38.280]   be able to recreate deep correlations inside that data
[00:42:38.280 --> 00:42:42.560]   set. So that was, has been a really fun research area for us.
[00:42:42.560 --> 00:42:45.680]   We recently published some code to GitHub that you can use to,
[00:42:45.680 --> 00:42:49.360]   you know, take a mouse data set and create an entirely
[00:42:49.360 --> 00:42:54.400]   artificial Jurassic Park style genome using our synthetic data
[00:42:54.440 --> 00:42:58.880]   model, which really kind of shows the scalability of it. And
[00:42:58.880 --> 00:43:01.200]   I think we're also we're fascinated by use cases we're
[00:43:01.200 --> 00:43:05.000]   hearing from customers around images and audio and video and
[00:43:05.000 --> 00:43:08.200]   these different, you know, even metaverse type use cases that
[00:43:08.200 --> 00:43:11.320]   are coming up. So that's really, it's been exciting challenge to
[00:43:11.320 --> 00:43:11.720]   work on.
[00:43:11.720 --> 00:43:15.600]   Sanyam Bhutani: You mentioned this is, we could think of it
[00:43:15.600 --> 00:43:19.600]   like a language model that basically anonymizes the data
[00:43:19.600 --> 00:43:24.960]   and synthesizes it. So I'm, if you could talk about it, isn't
[00:43:24.960 --> 00:43:28.200]   this computationally really expensive to run? Because I'm
[00:43:28.200 --> 00:43:31.880]   asking it to be like a really big language model that captures
[00:43:31.880 --> 00:43:32.800]   a lot of stuff?
[00:43:32.800 --> 00:43:36.360]   Yeah, great question. So one of the great things about language
[00:43:36.360 --> 00:43:39.640]   models is their ability to generalize and to learn, you
[00:43:39.640 --> 00:43:42.240]   know, very quickly. The flip side, as you called out is
[00:43:42.240 --> 00:43:46.920]   computational cost. So you know, as an example, you saw with the
[00:43:46.920 --> 00:43:49.760]   credit time series data set, it took about 10 minutes using
[00:43:49.760 --> 00:43:53.600]   NVIDIA T4 GPU to train the model and generate the initial data
[00:43:53.600 --> 00:43:57.640]   set. So there is a real compute window, especially when you're
[00:43:57.640 --> 00:44:00.520]   kind of starting from scratch, and you're not fine tuning a
[00:44:00.520 --> 00:44:07.320]   pre trained model. I think, as we've seen, you know, language
[00:44:07.320 --> 00:44:11.320]   models, they generalize incredibly well. So it's a great
[00:44:11.320 --> 00:44:15.840]   place to start. When you look across our team right now, the
[00:44:16.440 --> 00:44:20.160]   kind of like active research areas that we have are largely
[00:44:20.160 --> 00:44:26.000]   around. Let me turn off sharing here. Like, how do we apply
[00:44:26.000 --> 00:44:28.840]   GANs, which can be very efficient once they're trained to
[00:44:28.840 --> 00:44:32.440]   do, you know, create data and do inference. So we just did a post
[00:44:32.440 --> 00:44:36.920]   on using image data with GANs. So pretty promising stuff and
[00:44:36.920 --> 00:44:38.040]   really excited to see where it goes.
[00:44:38.040 --> 00:44:44.160]   And is it the same model that works for image data as well?
[00:44:44.560 --> 00:44:48.200]   Because, again, if you're okay with talking about that, because
[00:44:48.200 --> 00:44:51.440]   I'm really curious, I haven't seen any literature. Yeah, this
[00:44:51.440 --> 00:44:55.040]   I haven't seen any literature around taking a language model,
[00:44:55.040 --> 00:44:58.160]   although I could foresee like it could probably predict a lot of
[00:44:58.160 --> 00:45:03.880]   check out open AI's image GPT implementation. And really, they
[00:45:03.880 --> 00:45:06.640]   showed that it is possible. But the net effect of that was
[00:45:06.640 --> 00:45:08.880]   learning that it's computationally there's it's no
[00:45:08.880 --> 00:45:15.040]   match for for like using a GAN, for example, as far as image
[00:45:15.040 --> 00:45:17.840]   generation, but it was fascinating that that's possible.
[00:45:17.840 --> 00:45:22.680]   For our image work, we used something called contrastive
[00:45:22.680 --> 00:45:26.360]   unpaired translation. So it's a recent paper that came out by
[00:45:26.360 --> 00:45:33.280]   the same creators of like the pics to pics and cycle GAN
[00:45:33.280 --> 00:45:38.640]   implementation. So like, really cool stuff. Essentially, what
[00:45:38.640 --> 00:45:42.600]   we did is we took a, like a data set and trained an image
[00:45:42.600 --> 00:45:45.560]   translation problem. And what was kind of neat is we train,
[00:45:45.560 --> 00:45:50.720]   you know, data set A was a picture of maps of downtown, I
[00:45:50.720 --> 00:45:53.320]   guess we used in this case, downtown Santa Monica, downtown
[00:45:53.320 --> 00:45:57.160]   San Francisco. And what we tried to translate that to was another
[00:45:57.160 --> 00:46:02.200]   set of maps that had annotated locations for where public ebike
[00:46:02.200 --> 00:46:05.120]   scooter feeds are. And essentially, what we're doing is
[00:46:05.120 --> 00:46:07.680]   we're training a model based on a limited amount of data to
[00:46:07.680 --> 00:46:11.920]   predict, in this case, where humans or where like scooters
[00:46:11.920 --> 00:46:15.680]   might be. And then we want to do is say, is it possible to, does
[00:46:15.680 --> 00:46:18.800]   this generalize? Can we take a model that's trained on Austin,
[00:46:18.800 --> 00:46:21.400]   Texas and San Francisco and use it to predict where realistic
[00:46:21.400 --> 00:46:25.560]   scooter locations might be in Tokyo, or Osaka and places like
[00:46:25.560 --> 00:46:29.680]   that, just by adding that context of maps, and have really
[00:46:29.680 --> 00:46:34.720]   positive results, actually. So I'm happy to link to the article
[00:46:34.720 --> 00:46:36.880]   we put on towards data science and the code for that, but
[00:46:36.880 --> 00:46:41.480]   pretty excited. And also really enthused by the, you know,
[00:46:41.480 --> 00:46:44.320]   there's a lot of training that needs to go into getting began
[00:46:44.320 --> 00:46:47.480]   to work correctly. But after we trained it, the inference and
[00:46:47.480 --> 00:46:51.320]   ability to create a synthetic data set is very quick. And that
[00:46:51.320 --> 00:46:53.680]   matters when customers are using things at scale on their own all
[00:46:53.680 --> 00:46:55.400]   the time. So very good question.
[00:46:55.400 --> 00:47:00.160]   Sanyam Bhutani: And I'm again, maybe I need to check out the
[00:47:00.160 --> 00:47:05.200]   people. So how do you in that case evaluate the synthetic data
[00:47:05.200 --> 00:47:08.640]   for let's say, if you're taking the similar language model for
[00:47:08.640 --> 00:47:12.120]   generating images, or like you said, genomic data, how do you
[00:47:12.120 --> 00:47:19.080]   then maybe set to, I don't know, error metric or how do you test
[00:47:19.080 --> 00:47:19.360]   that?
[00:47:19.360 --> 00:47:22.920]   Jason Antic: It's it depends on the data set, we have found the
[00:47:22.920 --> 00:47:25.720]   synthetic quality score metric that we built, and we published
[00:47:25.720 --> 00:47:30.520]   how it works is, is a good start. And really, what it's
[00:47:30.520 --> 00:47:34.080]   looking at is the distribution of data created by the model
[00:47:34.080 --> 00:47:36.720]   versus and correlations versus the real world data that was
[00:47:36.720 --> 00:47:41.760]   trained on. That works as a start as a start for evaluating
[00:47:41.760 --> 00:47:44.440]   the quality of the genomic data we did with mice, we actually
[00:47:44.440 --> 00:47:46.880]   ran something called the GWAS, which is a genome wide
[00:47:46.880 --> 00:47:50.200]   association study. And we recreated the results of a paper
[00:47:50.200 --> 00:47:54.840]   based on these mice, which was, which is really cool. Images is
[00:47:54.840 --> 00:47:57.320]   a whole new thing. There's some really cool work being done for
[00:47:57.320 --> 00:48:03.280]   distance metrics on images. And so, you know, you can, you know,
[00:48:03.280 --> 00:48:08.680]   start with a standard image metric distance metric, what we
[00:48:08.680 --> 00:48:12.760]   ended up doing for location data was a little different. Since we
[00:48:12.760 --> 00:48:16.280]   had a few cities where we actually had public e-bike
[00:48:16.280 --> 00:48:19.480]   scooter feeds that weren't in the training set, what we were
[00:48:19.480 --> 00:48:22.080]   able to do is predict where scooter locations were in the
[00:48:22.080 --> 00:48:25.680]   city and then try to measure the difference between the centroids
[00:48:25.680 --> 00:48:30.400]   for the scooter locations. But that is an evolving metric,
[00:48:30.440 --> 00:48:33.120]   something we've got to make really easy for people to work
[00:48:33.120 --> 00:48:36.520]   with. And, you know, we're looking for feedback from the
[00:48:36.520 --> 00:48:38.320]   community on ways to best do that.
[00:48:38.320 --> 00:48:42.320]   Sanyam Bhutani: Thanks. Thanks for answering that. I'll check
[00:48:42.320 --> 00:48:47.200]   out the blogs for sure. Those are the questions I had. We
[00:48:47.200 --> 00:48:50.800]   still have 10 minutes if you want to share anything from your
[00:48:50.800 --> 00:48:52.400]   end. Maybe another demo.
[00:48:52.400 --> 00:48:54.000]   Yeah, maybe I'll touch on.
[00:48:54.000 --> 00:48:58.880]   One of my favorite examples is actually one of the first, you
[00:48:58.880 --> 00:49:02.680]   know, times a customer asked if it would be possible to change
[00:49:02.680 --> 00:49:06.240]   the shape of a data set, essentially adjust for biases
[00:49:06.240 --> 00:49:10.880]   that existed in it. Many of us that have worked in the, that's
[00:49:10.880 --> 00:49:13.040]   ever done anything on Kaggle, we worked with the data sets from
[00:49:13.040 --> 00:49:17.040]   UCI. Right. So whether it's adult census income data set,
[00:49:17.040 --> 00:49:19.880]   there's another one they have, which is the heart disease
[00:49:19.880 --> 00:49:23.200]   prediction data set, very popular data set on Kaggle. And
[00:49:23.200 --> 00:49:26.440]   let me share my screen again. What they noticed, what they
[00:49:26.440 --> 00:49:31.440]   were curious to work on was that while this data set is really
[00:49:31.440 --> 00:49:34.080]   cool, and it allows kind of unique data set for predicting
[00:49:34.080 --> 00:49:37.720]   heart disease, when you look at distribution of elements, it's
[00:49:37.720 --> 00:49:42.080]   two to one ratio of males to females in that data set. So any
[00:49:42.080 --> 00:49:44.480]   you know, algorithm that you run on there, and you're able to
[00:49:44.480 --> 00:49:48.320]   really see this when you look at the Kaggle notebooks, is going
[00:49:48.320 --> 00:49:50.520]   to get great at detecting male heart disease and probably be
[00:49:50.520 --> 00:49:53.360]   pretty poor at detecting female heart disease. So the question
[00:49:53.360 --> 00:50:01.680]   was, can we use our synthetic model? Can we use a synthetic
[00:50:01.680 --> 00:50:08.000]   model to essentially predict? Let me catch up here. One
[00:50:08.000 --> 00:50:14.280]   second. Can we use a synthetic model essentially to make up for
[00:50:14.280 --> 00:50:17.120]   the representation bias difference that we see here. So
[00:50:17.120 --> 00:50:19.560]   here's a really simple dimension that we're looking at. We're
[00:50:19.560 --> 00:50:24.520]   only looking at the sex attribute here. In later
[00:50:24.520 --> 00:50:27.040]   examples, we'll, you know, balance out age, we'll balance
[00:50:27.040 --> 00:50:29.800]   out income in different data sets. But here, first example
[00:50:29.800 --> 00:50:35.200]   was, can we just boost the, adjust the female gender bias
[00:50:35.200 --> 00:50:39.440]   and balance out the minority class there. So here, what we
[00:50:39.440 --> 00:50:41.600]   need to do is generate additional female records,
[00:50:41.600 --> 00:50:46.280]   trained pretty fast, it's a pretty small data set. And then
[00:50:46.280 --> 00:50:49.200]   once once we've done that, like I've created more female records
[00:50:49.200 --> 00:50:51.440]   using the model, essentially conditional generation using a
[00:50:51.440 --> 00:50:53.720]   model saying, I want you to generate more records that
[00:50:53.720 --> 00:50:57.120]   matches attributes. How well does it perform? And does it
[00:50:57.120 --> 00:50:59.120]   just get better at female heart disease detection? Or does it
[00:50:59.120 --> 00:51:02.400]   get better overall? This is a really encouraging example.
[00:51:02.400 --> 00:51:05.000]   We've dived into this more. And this is a pretty kind of hot
[00:51:05.000 --> 00:51:08.760]   research area for a lot of our customers. So here, what we've
[00:51:08.760 --> 00:51:11.640]   got as a comparison using one of those top notebooks on Kaggle,
[00:51:11.640 --> 00:51:16.720]   where we took both our real world data, and then we took an
[00:51:16.760 --> 00:51:19.920]   augmented data set, which is a real world data plus these
[00:51:19.920 --> 00:51:22.840]   additional female 111 female records that we generated to
[00:51:22.840 --> 00:51:26.720]   balance out the data set, we ran a series of different
[00:51:26.720 --> 00:51:30.120]   classification tasks on them. So you know, predict whether this
[00:51:30.120 --> 00:51:35.280]   patient has heart disease. And here we're looking at the F1
[00:51:35.280 --> 00:51:38.320]   score that we have between each of these different algorithms.
[00:51:38.320 --> 00:51:43.120]   So here on the left, you see the real world data. And on the
[00:51:43.120 --> 00:51:48.200]   right, you see the the synthetic and it was really encouraging
[00:51:48.200 --> 00:51:51.320]   results here, where you know, across five different algorithms
[00:51:51.320 --> 00:51:54.480]   or six different algorithms, five of them actually improved
[00:51:54.480 --> 00:51:57.400]   using synthetic data. So you could view this as an oversampling
[00:51:57.400 --> 00:52:00.080]   technique, you know, it's not brand new, it's just a really
[00:52:00.080 --> 00:52:03.280]   intelligent way of oversampling. And the only area that got hit,
[00:52:03.280 --> 00:52:05.800]   and this is kind of interesting to you was in the naive Bayes
[00:52:05.800 --> 00:52:08.880]   classifier, you know, as we know, that assumes no kind of
[00:52:08.880 --> 00:52:12.560]   dependence between the different features. So that may have had
[00:52:12.560 --> 00:52:15.040]   something to do with why the real world data outperforms
[00:52:15.040 --> 00:52:18.200]   synthetic in that use case. But here's like a pretty encouraging
[00:52:18.200 --> 00:52:23.560]   example that this idea of oversampling, or adjusting for
[00:52:23.560 --> 00:52:26.960]   class imbalances that might be, you know, ethically concerned
[00:52:26.960 --> 00:52:30.200]   about, for your data set can give you pretty encouraging
[00:52:30.200 --> 00:52:30.640]   results.
[00:52:30.640 --> 00:52:36.000]   Sanyam Bhutani: This was really interesting to see. I'm curious
[00:52:36.000 --> 00:52:38.800]   to apply this to human image data, for example, because I
[00:52:38.800 --> 00:52:41.720]   know some, especially since we're talking about Kaggle, I
[00:52:41.720 --> 00:52:45.080]   know some competitions have this like insane imbalance.
[00:52:45.080 --> 00:52:51.120]   Yeah. So you're talking about, like extreme class imbalance,
[00:52:51.120 --> 00:52:54.280]   like you might see with cybersecurity attacks, or rare
[00:52:54.280 --> 00:52:55.520]   diseases and things like that.
[00:52:55.520 --> 00:53:00.200]   That plus, I was also thinking, is it possible to apply to image
[00:53:00.200 --> 00:53:04.000]   data? Maybe where you know, you have like 10 examples for one
[00:53:04.000 --> 00:53:07.680]   class, compared to 1000 to 10,000 examples for others?
[00:53:08.680 --> 00:53:14.480]   That's a really interesting question. Really interesting
[00:53:14.480 --> 00:53:17.680]   question. We have to think I haven't spent too much time
[00:53:17.680 --> 00:53:22.000]   thinking about that. But that totally makes sense. Medical
[00:53:22.000 --> 00:53:25.760]   space, right? If you've got a few pictures of skin cancer,
[00:53:25.760 --> 00:53:28.760]   right, and you want to create more examples like that to help
[00:53:28.760 --> 00:53:31.040]   your algorithm generalize, it seems like that's a real use
[00:53:31.040 --> 00:53:31.360]   case.
[00:53:31.360 --> 00:53:34.800]   Sanyam Bhutani: Because in Kaggle, that's, that's something
[00:53:34.800 --> 00:53:37.240]   that could push you to the top of the leaderboard.
[00:53:38.200 --> 00:53:38.600]   Uh huh.
[00:53:38.600 --> 00:53:41.120]   That's that's where this thought is coming from.
[00:53:41.120 --> 00:53:46.640]   I love it. That's, that's really interesting. I'd love to like,
[00:53:46.640 --> 00:53:48.600]   if you've got any thoughts on it, love to follow up sometime.
[00:53:48.600 --> 00:53:57.760]   Awesome. I'm quickly checking here if there are any. I don't
[00:53:57.760 --> 00:54:00.800]   see any questions. So I believe we can wrap up any things we
[00:54:00.800 --> 00:54:03.840]   missed that you want to touch upon or any things that you want
[00:54:03.840 --> 00:54:05.600]   to add before we wrap up?
[00:54:05.920 --> 00:54:07.960]   Not at all. Thank you for inviting me on today. It was a
[00:54:07.960 --> 00:54:09.120]   pleasure. Pleasure talking.
[00:54:09.120 --> 00:54:13.080]   Thank you so much. I'll quickly mention, you should check out
[00:54:13.080 --> 00:54:18.160]   Gretel on Twitter that is Gretel_AI. You can find, I
[00:54:18.160 --> 00:54:21.680]   believe their website on there and also their documentation. I
[00:54:21.680 --> 00:54:24.680]   created a shorter URL for today's demo where I just
[00:54:24.680 --> 00:54:28.360]   basically have the collab linked off so you can find it on this
[00:54:28.360 --> 00:54:33.200]   link. And if you want, you can connect with Alex on Twitter.
[00:54:33.560 --> 00:54:37.480]   His handle is @alexwatson405. Any other platforms you want to
[00:54:37.480 --> 00:54:38.320]   mention Alex?
[00:54:38.320 --> 00:54:41.720]   That's fantastic. If you wanted to jump in and start working with
[00:54:41.720 --> 00:54:44.000]   Gretel, you can just go to Gretel.ai and click sign up or
[00:54:44.000 --> 00:54:46.320]   sign in and get started immediately.
[00:54:46.320 --> 00:54:50.800]   Awesome. Thanks again for your time, Alex. And thanks again for
[00:54:50.800 --> 00:54:54.160]   this fun demo. I really enjoyed it. And I'm curious to try it
[00:54:54.160 --> 00:54:57.800]   for the image examples I mentioned. I might just give it
[00:54:57.800 --> 00:54:58.880]   a spin for Kaggle tonight.
[00:54:58.880 --> 00:55:01.600]   Thanks, Sanyam. Bye.
[00:55:01.600 --> 00:55:11.200]   [BLANK_AUDIO]

