
[00:00:00.000 --> 00:00:03.000]   The following is a conversation with Russ Tedrick,
[00:00:03.000 --> 00:00:05.560]   a roboticist and professor at MIT
[00:00:05.560 --> 00:00:07.880]   and vice president of robotics research
[00:00:07.880 --> 00:00:11.240]   at Toyota Research Institute or TRI.
[00:00:11.240 --> 00:00:15.160]   He works on control of robots in interesting,
[00:00:15.160 --> 00:00:18.000]   complicated, underactuated, stochastic,
[00:00:18.000 --> 00:00:19.960]   difficult to model situations.
[00:00:19.960 --> 00:00:22.640]   He's a great teacher and a great person.
[00:00:22.640 --> 00:00:25.040]   One of my favorites at MIT.
[00:00:25.040 --> 00:00:28.280]   We'll get into a lot of topics in this conversation
[00:00:28.280 --> 00:00:32.760]   from his time leading MIT's DARPA Robotics Challenge Team
[00:00:32.760 --> 00:00:35.400]   to the awesome fact that he often runs
[00:00:35.400 --> 00:00:40.400]   close to a marathon a day to and from work barefoot.
[00:00:40.400 --> 00:00:42.800]   For a world-class roboticist interested
[00:00:42.800 --> 00:00:44.520]   in elegant, efficient control
[00:00:44.520 --> 00:00:49.300]   of underactuated dynamical systems like the human body,
[00:00:49.300 --> 00:00:51.440]   this fact makes Russ one of the most
[00:00:51.440 --> 00:00:53.200]   fascinating people I know.
[00:00:53.200 --> 00:00:55.800]   Quick summary of the ads.
[00:00:55.800 --> 00:00:58.440]   Three sponsors, Magic Spoon Cereal,
[00:00:58.440 --> 00:01:00.800]   BetterHelp and ExpressVPN.
[00:01:00.800 --> 00:01:02.640]   Please consider supporting this podcast
[00:01:02.640 --> 00:01:05.720]   by going to magicspoon.com/lex
[00:01:05.720 --> 00:01:08.000]   and using code LEX at checkout,
[00:01:08.000 --> 00:01:10.520]   going to betterhelp.com/lex
[00:01:10.520 --> 00:01:14.680]   and signing up at expressvpn.com/lexpod.
[00:01:14.680 --> 00:01:16.520]   Click the links in the description,
[00:01:16.520 --> 00:01:18.840]   buy the stuff, get the discount.
[00:01:18.840 --> 00:01:21.840]   It really is the best way to support this podcast.
[00:01:21.840 --> 00:01:24.040]   If you enjoy this thing, subscribe on YouTube,
[00:01:24.040 --> 00:01:26.240]   review it with five stars on Apple Podcast,
[00:01:26.240 --> 00:01:28.280]   support it on Patreon or connect with me
[00:01:28.280 --> 00:01:31.280]   on Twitter @lexfriedman.
[00:01:31.280 --> 00:01:33.640]   As usual, I'll do a few minutes of ads now
[00:01:33.640 --> 00:01:34.880]   and never any ads in the middle
[00:01:34.880 --> 00:01:37.880]   that can break the flow of the conversation.
[00:01:37.880 --> 00:01:40.880]   This episode is supported by Magic Spoon,
[00:01:40.880 --> 00:01:43.440]   low-carb, keto-friendly cereal.
[00:01:43.440 --> 00:01:45.800]   I've been on a mix of keto or carnivore diet
[00:01:45.800 --> 00:01:47.320]   for a very long time now.
[00:01:47.320 --> 00:01:50.520]   That means eating very little carbs.
[00:01:50.520 --> 00:01:52.200]   I used to love cereal.
[00:01:52.200 --> 00:01:54.960]   Obviously, most have crazy amounts of sugar,
[00:01:54.960 --> 00:01:58.000]   which is terrible for you, so I quit years ago.
[00:01:58.000 --> 00:02:00.440]   But Magic Spoon is a totally new thing.
[00:02:00.440 --> 00:02:03.040]   Zero sugar, 11 grams of protein,
[00:02:03.040 --> 00:02:05.720]   and only three net grams of carbs.
[00:02:05.720 --> 00:02:07.240]   It tastes delicious.
[00:02:07.240 --> 00:02:09.640]   It has a bunch of flavors, they're all good,
[00:02:09.640 --> 00:02:12.560]   but if you know what's good for you, you'll go with cocoa,
[00:02:12.560 --> 00:02:15.840]   my favorite flavor and the flavor of champions.
[00:02:15.840 --> 00:02:19.480]   Click the magicspoon.com/lex link in the description,
[00:02:19.480 --> 00:02:22.160]   use code LEX at checkout to get the discount
[00:02:22.160 --> 00:02:24.400]   and to let them know I sent you.
[00:02:24.400 --> 00:02:26.680]   So buy all of their cereal.
[00:02:26.680 --> 00:02:28.640]   It's delicious and good for you.
[00:02:28.640 --> 00:02:29.640]   You won't regret it.
[00:02:29.640 --> 00:02:33.160]   The show is also sponsored by BetterHelp,
[00:02:33.160 --> 00:02:36.040]   spelled H-E-L-P, help.
[00:02:36.040 --> 00:02:39.440]   Check it out at betterhelp.com/lex.
[00:02:39.440 --> 00:02:40.600]   They figure out what you need
[00:02:40.600 --> 00:02:43.240]   and match you with a licensed professional therapist
[00:02:43.240 --> 00:02:44.960]   in under 48 hours.
[00:02:44.960 --> 00:02:47.640]   It's not a crisis line, it's not self-help,
[00:02:47.640 --> 00:02:51.040]   it is professional counseling done securely online.
[00:02:51.040 --> 00:02:53.720]   As you may know, I'm a bit from the David Goggins line
[00:02:53.720 --> 00:02:57.080]   of creatures and so have some demons to contend with,
[00:02:57.080 --> 00:03:01.560]   usually on long runs or all-nighters full of self-doubt.
[00:03:01.560 --> 00:03:04.360]   I think suffering is essential for creation,
[00:03:04.360 --> 00:03:06.040]   but you can suffer beautifully
[00:03:06.040 --> 00:03:08.200]   in a way that doesn't destroy you.
[00:03:08.200 --> 00:03:11.540]   For most people, I think a good therapist can help in this,
[00:03:11.540 --> 00:03:13.400]   so it's at least worth a try.
[00:03:13.400 --> 00:03:15.640]   Check out the reviews, they're all good.
[00:03:15.640 --> 00:03:19.220]   It's easy, private, affordable, available worldwide.
[00:03:19.220 --> 00:03:21.640]   You can communicate by text anytime
[00:03:21.640 --> 00:03:25.080]   and schedule weekly audio and video sessions.
[00:03:25.080 --> 00:03:28.500]   Check it out at betterhelp.com/lex.
[00:03:28.500 --> 00:03:31.840]   This show is also sponsored by ExpressVPN.
[00:03:31.840 --> 00:03:35.700]   Get it at expressvpn.com/lexpod to get a discount
[00:03:35.700 --> 00:03:37.680]   and to support this podcast.
[00:03:37.680 --> 00:03:39.680]   Have you ever watched "The Office"?
[00:03:39.680 --> 00:03:41.920]   If you have, you probably know it's based
[00:03:41.920 --> 00:03:45.120]   on a UK series also called "The Office."
[00:03:45.120 --> 00:03:48.080]   Not to stir up trouble, but I personally think
[00:03:48.080 --> 00:03:50.320]   the British version is actually more brilliant
[00:03:50.320 --> 00:03:53.120]   than the American one, but both are amazing.
[00:03:53.120 --> 00:03:56.140]   Anyway, there are actually nine other countries
[00:03:56.140 --> 00:03:58.400]   with their own version of "The Office."
[00:03:58.400 --> 00:04:01.180]   You can get access to them with no geo-restriction
[00:04:01.180 --> 00:04:03.600]   when you use ExpressVPN.
[00:04:03.600 --> 00:04:05.560]   It lets you control where you want sites
[00:04:05.560 --> 00:04:07.340]   to think you're located.
[00:04:07.340 --> 00:04:10.380]   You can choose from nearly 100 different countries,
[00:04:10.380 --> 00:04:12.120]   giving you access to content
[00:04:12.120 --> 00:04:14.040]   that isn't available in your region.
[00:04:14.040 --> 00:04:19.040]   So again, get it on any device at expressvpn.com/lexpod
[00:04:19.040 --> 00:04:22.060]   to get an extra three months free
[00:04:22.060 --> 00:04:25.000]   and to support this podcast.
[00:04:25.000 --> 00:04:28.620]   And now here's my conversation with Russ Tedrick.
[00:04:28.620 --> 00:04:33.440]   What is the most beautiful motion of a animal or robot
[00:04:33.440 --> 00:04:34.540]   that you've ever seen?
[00:04:34.540 --> 00:04:38.280]   - I think the most beautiful motion of a robot
[00:04:38.280 --> 00:04:41.120]   has to be the passive dynamic walkers.
[00:04:41.120 --> 00:04:43.320]   I think there's just something fundamentally beautiful.
[00:04:43.320 --> 00:04:45.360]   The ones in particular that Steve Collins built
[00:04:45.360 --> 00:04:50.360]   with Andy Ruina at Cornell, a 3D walking machine.
[00:04:50.360 --> 00:04:53.720]   So it was not confined to a boom or a plane
[00:04:53.720 --> 00:04:57.460]   that you put it on top of a small ramp,
[00:04:57.460 --> 00:04:59.100]   give it a little push.
[00:04:59.100 --> 00:05:00.500]   It's powered only by gravity,
[00:05:00.500 --> 00:05:04.320]   no controllers, no batteries whatsoever.
[00:05:04.320 --> 00:05:06.160]   It just falls down the ramp.
[00:05:06.160 --> 00:05:09.520]   And at the time it looked more natural, more graceful,
[00:05:09.520 --> 00:05:13.460]   more human-like than any robot we'd seen to date.
[00:05:13.460 --> 00:05:15.240]   Powered only by gravity.
[00:05:15.240 --> 00:05:16.200]   - How does it work?
[00:05:16.200 --> 00:05:18.480]   - Well, okay, the simplest model,
[00:05:18.480 --> 00:05:19.480]   it's kind of like a slinky.
[00:05:19.480 --> 00:05:21.520]   It's like an elaborate slinky.
[00:05:21.520 --> 00:05:23.820]   One of the simplest models we use to think about it
[00:05:23.820 --> 00:05:25.340]   is actually a rimless wheel.
[00:05:25.340 --> 00:05:30.080]   So imagine taking a bicycle wheel, but take the rim off.
[00:05:30.080 --> 00:05:32.640]   So it's now just got a bunch of spokes.
[00:05:32.640 --> 00:05:33.720]   If you give that a push,
[00:05:33.720 --> 00:05:35.840]   it still wants to roll down the ramp.
[00:05:35.840 --> 00:05:38.180]   But every time its foot, its spoke comes around
[00:05:38.180 --> 00:05:40.660]   and hits the ground, it loses a little energy.
[00:05:40.660 --> 00:05:43.320]   Every time it takes a step forward,
[00:05:43.320 --> 00:05:44.580]   it gains a little energy.
[00:05:44.580 --> 00:05:48.240]   Those things can come into perfect balance.
[00:05:48.240 --> 00:05:51.280]   And actually they want to, it's a stable phenomenon.
[00:05:51.280 --> 00:05:53.760]   If it's going too slow, it'll speed up.
[00:05:53.760 --> 00:05:55.920]   If it's going too fast, it'll slow down.
[00:05:55.920 --> 00:05:58.200]   And it comes into a stable periodic motion.
[00:05:58.200 --> 00:06:02.160]   Now you can take that rimless wheel,
[00:06:02.160 --> 00:06:05.080]   which doesn't look very much like a human walking,
[00:06:05.080 --> 00:06:08.120]   take all the extra spokes away, put a hinge in the middle.
[00:06:08.120 --> 00:06:09.740]   Now it's two legs.
[00:06:09.740 --> 00:06:12.020]   That's called our compass gate walker.
[00:06:12.020 --> 00:06:13.840]   That can still, you give it a little push,
[00:06:13.840 --> 00:06:15.560]   starts falling down a ramp.
[00:06:15.560 --> 00:06:17.280]   Looks a little bit more like walking.
[00:06:17.280 --> 00:06:18.400]   At least it's a biped.
[00:06:18.400 --> 00:06:22.360]   But what Steve and Andy and Ted McGeer
[00:06:22.360 --> 00:06:23.520]   started the whole exercise,
[00:06:23.520 --> 00:06:25.240]   but what Steve and Andy did was they took it
[00:06:25.240 --> 00:06:27.480]   to this beautiful conclusion
[00:06:27.480 --> 00:06:32.460]   where they built something that had knees, arms, a torso,
[00:06:32.460 --> 00:06:36.320]   the arms swung naturally, give it a little push.
[00:06:36.320 --> 00:06:38.720]   And that looked like a stroll through the park.
[00:06:38.720 --> 00:06:40.240]   - How do you design something like that?
[00:06:40.240 --> 00:06:42.360]   I mean, is that art or science?
[00:06:42.360 --> 00:06:43.800]   - It's on the boundary.
[00:06:43.800 --> 00:06:47.640]   I think there's a science to getting close to the solution.
[00:06:47.640 --> 00:06:50.160]   I think there's certainly art in the way that they,
[00:06:50.160 --> 00:06:52.000]   they made a beautiful robot.
[00:06:52.000 --> 00:06:56.640]   But then the finesse, because this was,
[00:06:56.640 --> 00:06:57.600]   they were working with a system
[00:06:57.600 --> 00:07:01.060]   that wasn't perfectly modeled, wasn't perfectly controlled.
[00:07:01.060 --> 00:07:02.800]   There's all these little tricks
[00:07:02.800 --> 00:07:05.480]   that you have to tune the suction cups at the knees,
[00:07:05.480 --> 00:07:07.960]   for instance, so that they stick,
[00:07:07.960 --> 00:07:09.640]   but then they release at just the right time.
[00:07:09.640 --> 00:07:12.360]   Or there's all these little tricks of the trade,
[00:07:12.360 --> 00:07:14.440]   which really are art, but it was a point.
[00:07:14.440 --> 00:07:16.200]   I mean, it made the point.
[00:07:16.200 --> 00:07:18.800]   We were at that time, the walking robot,
[00:07:18.800 --> 00:07:21.840]   the best walking robot in the world was Honda's ASIMO.
[00:07:21.840 --> 00:07:24.120]   Absolutely marvel of modern engineering.
[00:07:24.120 --> 00:07:25.240]   - This is 90s?
[00:07:25.240 --> 00:07:27.440]   - This was in 97 when they first released,
[00:07:27.440 --> 00:07:29.920]   it sort of announced P2 and then it went through,
[00:07:29.920 --> 00:07:32.360]   it was ASIMO by then in 2004.
[00:07:34.880 --> 00:07:37.840]   - It looks like this very cautious walking,
[00:07:37.840 --> 00:07:41.280]   like you're walking on hot coals or something like that.
[00:07:41.280 --> 00:07:43.760]   - I think it gets a bad rap.
[00:07:43.760 --> 00:07:45.340]   ASIMO is a beautiful machine.
[00:07:45.340 --> 00:07:47.000]   It does walk with its knees bent.
[00:07:47.000 --> 00:07:49.760]   Our Atlas walking had its knees bent,
[00:07:49.760 --> 00:07:52.360]   but actually ASIMO was pretty fantastic,
[00:07:52.360 --> 00:07:54.320]   but it wasn't energy efficient.
[00:07:54.320 --> 00:07:56.660]   Neither was Atlas when we worked on Atlas.
[00:07:56.660 --> 00:08:00.520]   None of our robots that have been that complicated
[00:08:00.520 --> 00:08:02.480]   have been very energy efficient.
[00:08:04.060 --> 00:08:09.060]   But there's a thing that happens when you do control,
[00:08:09.060 --> 00:08:12.480]   when you try to control a system of that complexity.
[00:08:12.480 --> 00:08:16.480]   You try to use your motors to basically counteract gravity.
[00:08:16.480 --> 00:08:20.680]   Take whatever the world's doing to you and push back,
[00:08:20.680 --> 00:08:23.520]   erase the dynamics of the world
[00:08:23.520 --> 00:08:25.040]   and impose the dynamics you want
[00:08:25.040 --> 00:08:28.220]   because you can make them simple and analyzable,
[00:08:28.220 --> 00:08:29.700]   mathematically simple.
[00:08:30.780 --> 00:08:34.420]   And this was a very sort of beautiful example
[00:08:34.420 --> 00:08:36.380]   that you don't have to do that.
[00:08:36.380 --> 00:08:39.080]   You can just let go, let physics do most of the work.
[00:08:39.080 --> 00:08:42.180]   And you just have to give it a little bit of energy.
[00:08:42.180 --> 00:08:43.540]   This one only walked down a ramp.
[00:08:43.540 --> 00:08:45.340]   It would never walk on the flat.
[00:08:45.340 --> 00:08:46.160]   To walk on the flat,
[00:08:46.160 --> 00:08:48.460]   you have to give a little energy at some point.
[00:08:48.460 --> 00:08:51.580]   But maybe instead of trying to take the forces
[00:08:51.580 --> 00:08:55.200]   imparted to you by the world and replacing them,
[00:08:55.200 --> 00:08:58.220]   what we should be doing is letting the world push us around
[00:08:58.220 --> 00:08:59.340]   and we go with the flow.
[00:08:59.340 --> 00:09:01.260]   Very zen, very zen robot.
[00:09:01.260 --> 00:09:03.420]   - Yeah, but okay, so that sounds very zen.
[00:09:03.420 --> 00:09:08.220]   But I can also imagine how many
[00:09:08.220 --> 00:09:11.340]   failed versions they had to go through.
[00:09:11.340 --> 00:09:14.020]   I would say it's probably,
[00:09:14.020 --> 00:09:15.320]   would you say it's in the thousands
[00:09:15.320 --> 00:09:17.940]   that they've had to have the system fall down
[00:09:17.940 --> 00:09:19.860]   before they figured out how to get--
[00:09:19.860 --> 00:09:22.580]   - I don't know if it's thousands, but it's a lot.
[00:09:22.580 --> 00:09:25.020]   It takes some patience, there's no question.
[00:09:25.020 --> 00:09:28.340]   - So in that sense, control might help a little bit.
[00:09:28.340 --> 00:09:32.140]   - Oh, I think everybody, even at the time,
[00:09:32.140 --> 00:09:35.060]   said that the answer is to do that with control.
[00:09:35.060 --> 00:09:36.380]   But it was just pointing out
[00:09:36.380 --> 00:09:39.180]   that maybe the way we're doing control right now
[00:09:39.180 --> 00:09:41.100]   isn't the way we should.
[00:09:41.100 --> 00:09:43.860]   - Got it, so what about on the animal side?
[00:09:43.860 --> 00:09:46.220]   The ones that figured out how to move efficiently?
[00:09:46.220 --> 00:09:49.460]   Is there anything you find inspiring or beautiful
[00:09:49.460 --> 00:09:50.300]   in the movement of any particular animal?
[00:09:50.300 --> 00:09:52.020]   - I do have a favorite example.
[00:09:52.020 --> 00:09:52.860]   - Okay.
[00:09:52.860 --> 00:09:54.380]   (laughing)
[00:09:54.380 --> 00:09:57.180]   - So it sort of goes with the passive walking idea.
[00:09:57.180 --> 00:10:01.420]   So is there, how energy efficient are animals?
[00:10:01.420 --> 00:10:03.820]   Okay, there's a great series of experiments
[00:10:03.820 --> 00:10:07.500]   by George Lauder at Harvard and Mike Tranefilo at MIT.
[00:10:07.500 --> 00:10:11.820]   They were studying fish swimming in a water tunnel, okay?
[00:10:11.820 --> 00:10:15.260]   And one of these, the type of fish they were studying
[00:10:15.260 --> 00:10:17.260]   were these rainbow trout,
[00:10:17.260 --> 00:10:20.380]   because there was a phenomenon well understood
[00:10:20.380 --> 00:10:21.220]   that rainbow trout,
[00:10:21.220 --> 00:10:23.520]   when they're swimming upstream at mating season,
[00:10:23.520 --> 00:10:25.100]   they kind of hang out behind the rocks.
[00:10:25.100 --> 00:10:26.060]   And it looks like, I mean,
[00:10:26.060 --> 00:10:28.100]   that's tiring work swimming upstream.
[00:10:28.100 --> 00:10:29.180]   They're hanging out behind the rocks.
[00:10:29.180 --> 00:10:31.980]   Maybe there's something energetically interesting there.
[00:10:31.980 --> 00:10:33.400]   So they tried to recreate that.
[00:10:33.400 --> 00:10:36.420]   They put in this water tunnel, a rock basically,
[00:10:36.420 --> 00:10:40.580]   a cylinder that had the same sort of vortex street,
[00:10:40.580 --> 00:10:42.460]   the eddies coming off the back of the rock
[00:10:42.460 --> 00:10:44.260]   that you would see in a stream.
[00:10:44.260 --> 00:10:46.100]   And they put a real fish behind this
[00:10:46.100 --> 00:10:48.020]   and watched how it swims.
[00:10:48.020 --> 00:10:51.980]   And the amazing thing is that if you watch from above
[00:10:51.980 --> 00:10:53.800]   what the fish swims when it's not behind a rock,
[00:10:53.800 --> 00:10:56.100]   it has a particular gate.
[00:10:56.100 --> 00:10:58.760]   You can identify the fish the same way you look at a human
[00:10:58.760 --> 00:10:59.820]   looking at walking down the street.
[00:10:59.820 --> 00:11:02.440]   You sort of have a sense of how a human walks.
[00:11:02.440 --> 00:11:04.200]   The fish has a characteristic gate.
[00:11:04.200 --> 00:11:07.960]   You put that fish behind the rock, its gate changes.
[00:11:07.960 --> 00:11:12.700]   And what they saw was that it was actually resonating
[00:11:12.700 --> 00:11:15.120]   and kind of surfing between the vortices.
[00:11:15.120 --> 00:11:20.120]   Now, here was the experiment that really was the clincher,
[00:11:20.120 --> 00:11:20.960]   because there was still,
[00:11:20.960 --> 00:11:23.960]   it wasn't clear how much of that was mechanics of the fish,
[00:11:23.960 --> 00:11:26.900]   how much of that is control, the brain.
[00:11:26.900 --> 00:11:28.440]   So the clincher experiment,
[00:11:28.440 --> 00:11:29.760]   and maybe one of my favorites to date,
[00:11:29.760 --> 00:11:31.980]   although there are many good experiments.
[00:11:31.980 --> 00:11:37.020]   They took, this was now a dead fish.
[00:11:37.020 --> 00:11:40.160]   They took a dead fish.
[00:11:40.160 --> 00:11:41.600]   They put a string that went,
[00:11:41.600 --> 00:11:44.120]   that tied the mouse of the fish to the rock.
[00:11:44.120 --> 00:11:47.120]   So it couldn't go back and get caught in the grates.
[00:11:47.120 --> 00:11:49.160]   And then they asked, what would that dead fish do
[00:11:49.160 --> 00:11:51.160]   when it was hanging out behind the rock?
[00:11:51.160 --> 00:11:52.000]   And so what you'd expect,
[00:11:52.000 --> 00:11:53.760]   it sort of flopped around like a dead fish
[00:11:53.760 --> 00:11:56.120]   in the vortex wake,
[00:11:56.120 --> 00:11:57.760]   until something sort of amazing happens.
[00:11:57.760 --> 00:12:00.960]   And this video is worth putting in.
[00:12:00.960 --> 00:12:04.000]   - What happens?
[00:12:04.000 --> 00:12:06.560]   - The dead fish basically starts swimming upstream.
[00:12:06.560 --> 00:12:12.120]   It's completely dead, no brain, no motors, no control,
[00:12:12.120 --> 00:12:14.560]   but it's somehow the mechanics of the fish
[00:12:14.560 --> 00:12:16.320]   resonate with the vortex street,
[00:12:16.320 --> 00:12:18.240]   and it starts swimming upstream.
[00:12:18.240 --> 00:12:20.480]   It's one of the best examples ever.
[00:12:20.480 --> 00:12:23.720]   - Who do you give credit for that to?
[00:12:23.720 --> 00:12:27.960]   Is that just evolution constantly just figuring out
[00:12:27.960 --> 00:12:30.880]   by killing a lot of generations of animals,
[00:12:30.880 --> 00:12:32.700]   like the most efficient motion?
[00:12:32.700 --> 00:12:37.440]   Or maybe the physics of our world completely,
[00:12:37.440 --> 00:12:40.900]   is evolution applied not only to animals,
[00:12:40.900 --> 00:12:45.200]   but just the entirety of it somehow drives to efficiency?
[00:12:45.200 --> 00:12:46.980]   Like nature likes efficiency?
[00:12:48.120 --> 00:12:50.000]   I don't know if that question even makes any sense.
[00:12:50.000 --> 00:12:51.600]   - I understand the question.
[00:12:51.600 --> 00:12:54.480]   I mean, do they co-evolve?
[00:12:54.480 --> 00:12:56.400]   - Yeah, somehow co, yeah.
[00:12:56.400 --> 00:12:59.020]   I don't know if an environment can evolve, but.
[00:12:59.020 --> 00:13:02.320]   - I mean, there are experiments that people do,
[00:13:02.320 --> 00:13:05.400]   careful experiments that show that animals
[00:13:05.400 --> 00:13:08.680]   can adapt to unusual situations and recover efficiency.
[00:13:08.680 --> 00:13:11.120]   So there seems like, at least in one direction,
[00:13:11.120 --> 00:13:12.720]   I think there is reason to believe
[00:13:12.720 --> 00:13:14.540]   that the animal's motor system,
[00:13:14.540 --> 00:13:17.160]   and probably its mechanics,
[00:13:18.080 --> 00:13:20.040]   adapt in order to be more efficient.
[00:13:20.040 --> 00:13:23.080]   But efficiency isn't the only goal, of course.
[00:13:23.080 --> 00:13:26.160]   Sometimes it's too easy to think about only efficiency.
[00:13:26.160 --> 00:13:28.840]   But we have to do a lot of other things first,
[00:13:28.840 --> 00:13:32.820]   not get eaten, and then all other things being equal,
[00:13:32.820 --> 00:13:34.080]   try to save energy.
[00:13:34.080 --> 00:13:36.080]   - By the way, let's draw a distinction
[00:13:36.080 --> 00:13:38.120]   between control and mechanics.
[00:13:38.120 --> 00:13:40.780]   Like how would you define each?
[00:13:40.780 --> 00:13:43.900]   - Yeah, I mean, I think part of the point is that
[00:13:43.900 --> 00:13:47.840]   we shouldn't draw a line as clearly as we tend to.
[00:13:47.840 --> 00:13:51.420]   But on a robot, we have motors,
[00:13:51.420 --> 00:13:54.800]   and we have the links of the robot, let's say.
[00:13:54.800 --> 00:13:56.240]   If the motors are turned off,
[00:13:56.240 --> 00:13:58.180]   the robot has some passive dynamics.
[00:13:58.180 --> 00:14:01.320]   Gravity does the work.
[00:14:01.320 --> 00:14:03.680]   You can put springs, I would call that mechanics.
[00:14:03.680 --> 00:14:04.920]   If we have springs and dampers,
[00:14:04.920 --> 00:14:07.600]   which our muscles are springs and dampers and tendons.
[00:14:07.600 --> 00:14:10.400]   But then you have something that's doing active work,
[00:14:10.400 --> 00:14:13.200]   putting energy in, which are your motors on the robot.
[00:14:13.200 --> 00:14:16.560]   The controller's job is to send commands to the motor
[00:14:16.560 --> 00:14:18.600]   that add new energy into the system.
[00:14:18.600 --> 00:14:22.440]   So the mechanics and control interplay
[00:14:22.440 --> 00:14:24.760]   somewhere the divide is around,
[00:14:24.760 --> 00:14:27.520]   did you decide to send some commands to your motor,
[00:14:27.520 --> 00:14:28.920]   or did you just leave the motors off,
[00:14:28.920 --> 00:14:30.520]   let them do their work?
[00:14:30.520 --> 00:14:33.880]   - So would you say is most of nature
[00:14:33.880 --> 00:14:39.800]   on the dynamic side or the control side?
[00:14:39.800 --> 00:14:42.240]   So like if you look at biological systems,
[00:14:42.240 --> 00:14:45.320]   we're living in a pandemic now,
[00:14:45.320 --> 00:14:46.680]   do you think a virus is a,
[00:14:46.680 --> 00:14:50.040]   do you think it's a dynamic system,
[00:14:50.040 --> 00:14:54.060]   or is there a lot of control, intelligence?
[00:14:54.060 --> 00:14:56.160]   - I think it's both, but I think we maybe
[00:14:56.160 --> 00:14:58.720]   have underestimated how important the dynamics are.
[00:14:58.720 --> 00:15:04.280]   I mean even our bodies, the mechanics of our bodies,
[00:15:04.280 --> 00:15:06.240]   certainly with exercise they evolve.
[00:15:06.240 --> 00:15:11.000]   So I actually, I lost a finger in early 2000s,
[00:15:11.000 --> 00:15:14.400]   and it's my fifth metacarpal.
[00:15:14.400 --> 00:15:16.560]   And it turns out you use that a lot,
[00:15:16.560 --> 00:15:19.280]   in ways you don't expect when you're opening jars,
[00:15:19.280 --> 00:15:20.580]   even when I'm just walking around,
[00:15:20.580 --> 00:15:22.480]   if I bump it on something,
[00:15:22.480 --> 00:15:26.720]   there's a bone there that was used to taking contact.
[00:15:26.720 --> 00:15:28.800]   My fourth metacarpal wasn't used to taking contact,
[00:15:28.800 --> 00:15:31.060]   it used to hurt, still does a little bit.
[00:15:31.060 --> 00:15:33.900]   But actually my bone has remodeled, right?
[00:15:33.900 --> 00:15:39.560]   Over a couple of years, the geometry,
[00:15:39.560 --> 00:15:42.080]   the mechanics of that bone changed
[00:15:42.080 --> 00:15:44.300]   to address the new circumstances.
[00:15:44.300 --> 00:15:46.800]   So the idea that somehow it's only our brain
[00:15:46.800 --> 00:15:48.920]   that's adapting or evolving is not right.
[00:15:48.920 --> 00:15:52.520]   - Maybe sticking on evolution for a bit,
[00:15:52.520 --> 00:15:56.680]   'cause it's tended to create some interesting things.
[00:15:56.680 --> 00:16:01.680]   Bipedal walking, why the heck did evolution give us,
[00:16:01.680 --> 00:16:06.520]   I think we're, are we the only mammals that walk on two feet?
[00:16:06.520 --> 00:16:10.520]   - No, I mean there's a bunch of animals that do it, a bit.
[00:16:10.520 --> 00:16:13.560]   I think we are the most successful bipeds.
[00:16:13.560 --> 00:16:18.560]   - I think I read somewhere that the reason
[00:16:18.560 --> 00:16:24.120]   the evolution made us walk on two feet
[00:16:24.120 --> 00:16:26.780]   is because there's an advantage to being able
[00:16:26.780 --> 00:16:29.600]   to carry food back to the tribe or something like that.
[00:16:29.600 --> 00:16:33.380]   So you can carry, it's kind of this communal,
[00:16:33.380 --> 00:16:36.520]   cooperative thing, so to carry stuff back
[00:16:36.520 --> 00:16:41.520]   to a place of shelter and so on to share with others.
[00:16:41.640 --> 00:16:46.040]   - Do you understand at all the value of walking on two feet
[00:16:46.040 --> 00:16:49.400]   from both a robotics and a human perspective?
[00:16:49.400 --> 00:16:51.680]   - Yeah, there are some great books written
[00:16:51.680 --> 00:16:56.080]   about evolution of, walking evolution of the human body.
[00:16:56.080 --> 00:17:00.600]   I think it's easy though to make bad evolutionary arguments.
[00:17:00.600 --> 00:17:02.200]   - Sure.
[00:17:02.200 --> 00:17:05.320]   Most of them are probably bad, but what else can we do?
[00:17:05.320 --> 00:17:11.120]   - I mean I think a lot of what dominated our evolution
[00:17:11.120 --> 00:17:15.080]   probably was not the things that worked well
[00:17:15.080 --> 00:17:20.080]   sort of in the steady state, when things are good.
[00:17:20.080 --> 00:17:25.040]   But for instance, people talk about what we should eat now
[00:17:25.040 --> 00:17:28.320]   because our ancestors were meat eaters or whatever.
[00:17:28.320 --> 00:17:30.240]   - Oh yeah, I love that, yeah.
[00:17:30.240 --> 00:17:35.240]   - But probably the reason that one pre-Homo sapien species
[00:17:35.240 --> 00:17:40.700]   versus another survived was not because of
[00:17:40.700 --> 00:17:45.320]   whether they ate well when there was lots of food,
[00:17:45.320 --> 00:17:49.600]   but when the ice age came, probably one of them
[00:17:49.600 --> 00:17:50.960]   happened to be in the wrong place,
[00:17:50.960 --> 00:17:54.200]   one of them happened to forage a food that was okay
[00:17:54.200 --> 00:17:57.720]   even when the glaciers came or something like that.
[00:17:57.720 --> 00:18:00.560]   - There's a million variables that contributed
[00:18:00.560 --> 00:18:04.080]   and we can't, and are actually, the amount of information
[00:18:04.080 --> 00:18:06.680]   we're working with in telling these stories,
[00:18:06.680 --> 00:18:10.220]   these evolutionary stories is very little.
[00:18:10.220 --> 00:18:13.040]   So yeah, just like you said, it seems like,
[00:18:13.040 --> 00:18:15.680]   if you study history, it seems like history turns
[00:18:15.680 --> 00:18:20.680]   on these little events that otherwise would seem meaningless,
[00:18:20.680 --> 00:18:26.480]   but in the grant, in retrospect, were turning points.
[00:18:26.480 --> 00:18:28.360]   - Absolutely.
[00:18:28.360 --> 00:18:30.920]   - And that's probably how, like somebody got hit
[00:18:30.920 --> 00:18:33.600]   in the head with a rock because somebody slept
[00:18:33.600 --> 00:18:37.020]   with the wrong person back in the cave days
[00:18:37.020 --> 00:18:39.240]   and somebody got angry and that turned,
[00:18:40.200 --> 00:18:43.280]   warring tribes combined with the environment,
[00:18:43.280 --> 00:18:46.480]   all those millions of things and the meat eating,
[00:18:46.480 --> 00:18:49.360]   which I get a lot of criticism because I don't know,
[00:18:49.360 --> 00:18:51.480]   I don't know what your dietary processes are like,
[00:18:51.480 --> 00:18:55.040]   but these days I've been eating only meat,
[00:18:55.040 --> 00:18:59.080]   which is, there's a large community of people who say,
[00:18:59.080 --> 00:19:01.080]   yeah, probably make evolutionary arguments
[00:19:01.080 --> 00:19:02.720]   and say you're doing a great job.
[00:19:02.720 --> 00:19:05.760]   There's probably an even larger community of people,
[00:19:05.760 --> 00:19:08.520]   including my mom, who says it's a deeply unhealthy,
[00:19:08.520 --> 00:19:10.760]   it's wrong, but I just feel good doing it.
[00:19:10.760 --> 00:19:12.980]   But you're right, these evolutionary arguments
[00:19:12.980 --> 00:19:15.440]   can be flawed, but is there anything interesting
[00:19:15.440 --> 00:19:17.320]   to pull out for--
[00:19:17.320 --> 00:19:19.360]   - There's a great book, by the way,
[00:19:19.360 --> 00:19:21.280]   well, a series of books by Nicholas Taleb
[00:19:21.280 --> 00:19:23.840]   about fooled by randomness and black swan.
[00:19:23.840 --> 00:19:26.840]   Highly recommend them, but yeah,
[00:19:26.840 --> 00:19:30.320]   they make the point nicely that probably it was
[00:19:30.320 --> 00:19:35.320]   a few random events that, yes, maybe it was someone
[00:19:35.320 --> 00:19:37.660]   getting hit by a rock, as you say.
[00:19:38.660 --> 00:19:42.540]   - That said, do you think, I don't know how to ask
[00:19:42.540 --> 00:19:44.060]   this question or how to talk about this,
[00:19:44.060 --> 00:19:45.660]   but there's something elegant and beautiful
[00:19:45.660 --> 00:19:47.820]   about moving on two feet.
[00:19:47.820 --> 00:19:50.300]   Obviously biased, because I'm human,
[00:19:50.300 --> 00:19:53.300]   but from a robotics perspective, too,
[00:19:53.300 --> 00:19:55.420]   you work with robots on two feet.
[00:19:55.420 --> 00:20:00.100]   Is it all useful to build robots that are on two feet
[00:20:00.100 --> 00:20:01.100]   as opposed to four?
[00:20:01.100 --> 00:20:02.340]   Is there something useful about it?
[00:20:02.340 --> 00:20:05.500]   - I think the most, I mean, the reason I spent a long time
[00:20:05.500 --> 00:20:08.980]   working on bipedal walking was because it was hard,
[00:20:08.980 --> 00:20:12.460]   and it challenged control theory in ways
[00:20:12.460 --> 00:20:13.960]   that I thought were important.
[00:20:13.960 --> 00:20:18.540]   I wouldn't have ever tried to convince you
[00:20:18.540 --> 00:20:22.420]   that you should start a company around bipeds
[00:20:22.420 --> 00:20:24.260]   or something like this.
[00:20:24.260 --> 00:20:26.100]   There are people that make pretty compelling arguments.
[00:20:26.100 --> 00:20:28.940]   I think the most compelling one is that the world
[00:20:28.940 --> 00:20:32.300]   is built for the human form, and if you want a robot
[00:20:32.300 --> 00:20:36.900]   to work in the world we have today, then having a human form
[00:20:36.900 --> 00:20:38.260]   is a pretty good way to go.
[00:20:38.260 --> 00:20:42.620]   There are places that a biped can go that would be hard
[00:20:42.620 --> 00:20:47.620]   for other form factors to go, even natural places.
[00:20:47.620 --> 00:20:51.940]   But at some point in the long run, we'll be building
[00:20:51.940 --> 00:20:54.260]   our environments for our robots, probably,
[00:20:54.260 --> 00:20:56.540]   and so maybe that argument falls aside.
[00:20:56.540 --> 00:20:58.820]   - So you famously run barefoot.
[00:20:58.820 --> 00:21:02.180]   Do you still run barefoot?
[00:21:02.180 --> 00:21:03.100]   - I still run barefoot.
[00:21:03.100 --> 00:21:04.780]   - That's so awesome.
[00:21:04.780 --> 00:21:06.340]   - Much to my wife's chagrin.
[00:21:06.340 --> 00:21:09.340]   - Do you wanna make an evolutionary argument
[00:21:09.340 --> 00:21:11.820]   for why running barefoot is advantageous?
[00:21:11.820 --> 00:21:17.540]   What have you learned about human and robot movement
[00:21:17.540 --> 00:21:19.840]   in general from running barefoot?
[00:21:19.840 --> 00:21:23.660]   Human or robot and/or?
[00:21:23.660 --> 00:21:25.640]   - Well, you know, it happened the other way, right?
[00:21:25.640 --> 00:21:30.640]   So I was studying walking robots, and there's a great
[00:21:30.740 --> 00:21:34.300]   conference called the Dynamic Walking Conference,
[00:21:34.300 --> 00:21:36.980]   where it brings together both the biomechanics community
[00:21:36.980 --> 00:21:39.900]   and the walking robots community.
[00:21:39.900 --> 00:21:43.060]   And so I had been going to this for years and hearing
[00:21:43.060 --> 00:21:45.060]   talks by people who study barefoot running
[00:21:45.060 --> 00:21:46.740]   and other mechanics of running.
[00:21:46.740 --> 00:21:50.280]   So I did eventually read Born to Run.
[00:21:50.280 --> 00:21:51.860]   Most people read Born to Run and then--
[00:21:51.860 --> 00:21:52.700]   - First then.
[00:21:52.700 --> 00:21:54.060]   - Right?
[00:21:54.060 --> 00:21:55.820]   The other thing I had going for me is actually that
[00:21:55.820 --> 00:22:00.700]   I wasn't a runner before, and I learned to run
[00:22:00.700 --> 00:22:02.860]   after I had learned about barefoot running,
[00:22:02.860 --> 00:22:05.420]   or I mean, started running longer distances.
[00:22:05.420 --> 00:22:07.300]   So I didn't have to unlearn.
[00:22:07.300 --> 00:22:11.020]   And I'm definitely, I'm a big fan of it for me,
[00:22:11.020 --> 00:22:13.980]   but I'm not gonna, I tend to not try to convince
[00:22:13.980 --> 00:22:14.800]   other people.
[00:22:14.800 --> 00:22:17.180]   There's people who run beautifully with shoes on,
[00:22:17.180 --> 00:22:18.260]   and that's good.
[00:22:18.260 --> 00:22:21.860]   But here's why it makes sense for me.
[00:22:24.060 --> 00:22:26.380]   It's all about the long-term game, right?
[00:22:26.380 --> 00:22:29.460]   So I think it's just too easy to run 10 miles,
[00:22:29.460 --> 00:22:31.580]   feel pretty good, and then you get home at night
[00:22:31.580 --> 00:22:33.820]   and you realize, my knees hurt.
[00:22:33.820 --> 00:22:35.500]   I did something wrong, right?
[00:22:35.500 --> 00:22:41.580]   If you take your shoes off, then if you hit hard
[00:22:41.580 --> 00:22:45.720]   with your foot at all, then it hurts.
[00:22:45.720 --> 00:22:49.300]   You don't like run 10 miles and then realize
[00:22:49.300 --> 00:22:50.800]   you've done some damage.
[00:22:50.800 --> 00:22:53.340]   You have immediate feedback telling you that you've done
[00:22:53.340 --> 00:22:55.420]   something that's maybe suboptimal.
[00:22:55.420 --> 00:22:56.540]   And you change your gait.
[00:22:56.540 --> 00:22:57.720]   I mean, it's even subconscious.
[00:22:57.720 --> 00:23:00.620]   If I, right now, having run many miles barefoot,
[00:23:00.620 --> 00:23:03.460]   if I put a shoe on, my gait changes in a way
[00:23:03.460 --> 00:23:04.860]   that I think is not as good.
[00:23:04.860 --> 00:23:09.540]   So it makes me land softer.
[00:23:09.540 --> 00:23:13.780]   And I think my goals for running are to do it
[00:23:13.780 --> 00:23:18.780]   for as long as I can into old age, not to win any races.
[00:23:18.780 --> 00:23:22.760]   And so for me, this is a way to protect myself.
[00:23:23.440 --> 00:23:27.760]   - Yeah, I think, first of all, I've tried running barefoot
[00:23:27.760 --> 00:23:30.520]   many years ago, probably the other way,
[00:23:30.520 --> 00:23:33.960]   just reading Born to Run.
[00:23:33.960 --> 00:23:38.760]   But just to understand, because I felt like I couldn't
[00:23:38.760 --> 00:23:40.880]   put in the miles that I wanted to.
[00:23:40.880 --> 00:23:44.720]   And it feels like running, for me, and I think
[00:23:44.720 --> 00:23:47.600]   for a lot of people, was one of those activities
[00:23:47.600 --> 00:23:51.160]   that we do often and we never really try to learn
[00:23:51.160 --> 00:23:52.140]   to do correctly.
[00:23:52.140 --> 00:23:57.440]   Like, it's funny, there's so many activities we do every day
[00:23:57.440 --> 00:24:00.320]   like brushing our teeth, right?
[00:24:00.320 --> 00:24:03.400]   I think a lot of us, at least me, probably have never
[00:24:03.400 --> 00:24:07.080]   deeply studied how to properly brush my teeth, right?
[00:24:07.080 --> 00:24:09.000]   Or wash, as now with the pandemic,
[00:24:09.000 --> 00:24:10.680]   or how to properly wash our hands.
[00:24:10.680 --> 00:24:13.840]   We do it every day, but we haven't really studied,
[00:24:13.840 --> 00:24:15.240]   like, am I doing this correctly?
[00:24:15.240 --> 00:24:17.120]   But running felt like one of those things
[00:24:17.120 --> 00:24:20.240]   that it was absurd not to study how to do correctly
[00:24:20.240 --> 00:24:23.320]   'cause it's the source of so much pain and suffering.
[00:24:23.320 --> 00:24:25.720]   Like, I hate running, but I do it.
[00:24:25.720 --> 00:24:28.960]   I do it because I hate it, but I feel good afterwards.
[00:24:28.960 --> 00:24:30.600]   But I think it feels like you need to learn
[00:24:30.600 --> 00:24:33.160]   how to do it properly, so that's where barefoot running
[00:24:33.160 --> 00:24:35.760]   came in, and then I quickly realized that my gait
[00:24:35.760 --> 00:24:38.040]   was completely wrong.
[00:24:38.040 --> 00:24:43.040]   I was taking huge, like, steps and landing hard on the heel,
[00:24:43.040 --> 00:24:45.840]   all those elements.
[00:24:45.840 --> 00:24:47.600]   And so, yeah, from that, I actually learned
[00:24:47.600 --> 00:24:49.200]   to take really small steps.
[00:24:49.200 --> 00:24:52.320]   Look, I already forgot the number,
[00:24:52.320 --> 00:24:55.640]   but I feel like it was 180 a minute or something like that.
[00:24:55.640 --> 00:25:00.160]   And I remember I actually just took songs
[00:25:00.160 --> 00:25:03.440]   that are 180 beats per minute,
[00:25:03.440 --> 00:25:05.560]   and then, like, tried to run at that beat,
[00:25:05.560 --> 00:25:07.720]   just to teach myself.
[00:25:07.720 --> 00:25:11.160]   It took a long time, and I feel like after a while,
[00:25:11.160 --> 00:25:14.360]   you learn to run, but you adjust it properly
[00:25:14.360 --> 00:25:16.000]   without going all the way to barefoot.
[00:25:16.000 --> 00:25:19.440]   I feel like barefoot is the legit way to do it.
[00:25:19.440 --> 00:25:21.680]   I mean, I think a lot of people
[00:25:21.680 --> 00:25:23.400]   would be really curious about it.
[00:25:23.400 --> 00:25:25.560]   Can you, if they're interested in trying,
[00:25:25.560 --> 00:25:27.840]   what would you, how would you recommend
[00:25:27.840 --> 00:25:30.760]   they start or try or explore?
[00:25:30.760 --> 00:25:32.600]   - Slowly. (laughs)
[00:25:32.600 --> 00:25:33.720]   That's the biggest thing people do,
[00:25:33.720 --> 00:25:35.920]   is they are excellent runners,
[00:25:35.920 --> 00:25:38.360]   and they're used to running long distances or running fast,
[00:25:38.360 --> 00:25:41.520]   and they take their shoes off, and they hurt themselves
[00:25:41.520 --> 00:25:42.840]   instantly trying to do something
[00:25:42.840 --> 00:25:44.280]   that they were used to doing.
[00:25:44.280 --> 00:25:46.000]   I think I lucked out in the sense
[00:25:46.000 --> 00:25:50.200]   that I couldn't run very far when I first started trying.
[00:25:50.200 --> 00:25:51.840]   And I run with minimal shoes, too.
[00:25:51.840 --> 00:25:54.360]   I mean, I will bring along a pair of,
[00:25:54.360 --> 00:25:56.320]   actually, like, Aqua socks or something like this
[00:25:56.320 --> 00:25:58.360]   I can just slip on, or running sandals.
[00:25:58.360 --> 00:26:00.400]   I've tried all of them.
[00:26:00.400 --> 00:26:02.640]   - What's the difference between a minimal shoe
[00:26:02.640 --> 00:26:03.800]   and nothing at all?
[00:26:03.800 --> 00:26:07.060]   What's, like, feeling-wise, what does it feel like?
[00:26:07.060 --> 00:26:10.040]   - There is a, I mean, I notice my gait changing, right?
[00:26:10.040 --> 00:26:15.040]   So, I mean, your foot has as many muscles and sensors
[00:26:15.040 --> 00:26:17.640]   as your hand does, right?
[00:26:17.640 --> 00:26:20.000]   - Sensors, ooh, okay.
[00:26:20.000 --> 00:26:23.240]   - And we do amazing things with our hands.
[00:26:23.240 --> 00:26:26.040]   And we stick our foot in a big, solid shoe, right?
[00:26:26.040 --> 00:26:29.680]   So there's, I think, you know, when you're barefoot,
[00:26:29.680 --> 00:26:33.280]   you're just giving yourself more proprioception.
[00:26:33.280 --> 00:26:35.760]   And that's why you're more aware of some of the gait flaws
[00:26:35.760 --> 00:26:37.120]   and stuff like this.
[00:26:37.120 --> 00:26:39.400]   Now, you have less protection, too.
[00:26:39.400 --> 00:26:40.760]   So--
[00:26:40.760 --> 00:26:42.440]   - Rocks and stuff.
[00:26:42.440 --> 00:26:45.200]   - I mean, yeah, so I think people who are afraid
[00:26:45.200 --> 00:26:47.200]   of barefoot running, they're worried about getting cuts
[00:26:47.200 --> 00:26:48.700]   or getting, stepping on rocks.
[00:26:48.700 --> 00:26:51.600]   First of all, even if that was a concern,
[00:26:51.600 --> 00:26:54.280]   I think those are all, like, very short-term.
[00:26:54.280 --> 00:26:55.440]   You know, if I get a scratch or something,
[00:26:55.440 --> 00:26:56.560]   it'll heal in a week.
[00:26:56.560 --> 00:26:58.260]   If I blow out my knees, I'm done running forever.
[00:26:58.260 --> 00:27:01.760]   So I will trade the short-term for the long-term, anytime.
[00:27:01.760 --> 00:27:04.800]   But even then, you know, and this, again,
[00:27:04.800 --> 00:27:07.800]   to my wife's chagrin, your feet get tough, right?
[00:27:07.800 --> 00:27:08.640]   And--
[00:27:08.640 --> 00:27:09.480]   - Your calves, okay.
[00:27:09.480 --> 00:27:11.560]   - Yeah, I can run over almost anything now.
[00:27:11.560 --> 00:27:13.760]   (laughing)
[00:27:13.760 --> 00:27:17.260]   - I mean, what, maybe, can you talk about,
[00:27:17.260 --> 00:27:21.920]   is there hint, like, is there tips or tricks
[00:27:21.920 --> 00:27:24.800]   that you have, suggestions about,
[00:27:24.800 --> 00:27:26.600]   like, if I wanted to try it?
[00:27:26.600 --> 00:27:29.560]   - You know, there is a good book, actually.
[00:27:29.560 --> 00:27:32.700]   There's probably more good books since I read them.
[00:27:32.700 --> 00:27:35.660]   But Ken Bob, Barefoot Ken Bob Saxton.
[00:27:37.360 --> 00:27:38.840]   He's an interesting guy.
[00:27:38.840 --> 00:27:42.640]   But I think his book captures the right way
[00:27:42.640 --> 00:27:45.040]   to describe running, barefoot running to somebody,
[00:27:45.040 --> 00:27:46.640]   better than any other I've seen.
[00:27:46.640 --> 00:27:52.560]   - So, you run pretty good distances, and you bike,
[00:27:52.560 --> 00:27:57.560]   and is there, you know, if we talk about bucket list items,
[00:27:57.560 --> 00:28:00.240]   is there something crazy on your bucket list,
[00:28:00.240 --> 00:28:02.820]   athletically, that you hope to do one day?
[00:28:02.820 --> 00:28:07.200]   - I mean, my commute is already a little crazy.
[00:28:07.200 --> 00:28:09.040]   - What are we talking about here?
[00:28:09.040 --> 00:28:11.480]   What distance are we talking about?
[00:28:11.480 --> 00:28:14.720]   - Well, I live about 12 miles from MIT,
[00:28:14.720 --> 00:28:16.660]   but you can find lots of different ways to get there.
[00:28:16.660 --> 00:28:20.560]   So, I mean, I've run there for many years, I've biked there.
[00:28:20.560 --> 00:28:21.520]   - Long ways?
[00:28:21.520 --> 00:28:23.920]   - Yeah, but normally I would try to run in
[00:28:23.920 --> 00:28:26.000]   and then bike home, bike in, run home.
[00:28:26.000 --> 00:28:28.200]   - But you have run there and back before?
[00:28:28.200 --> 00:28:29.020]   - Sure.
[00:28:29.020 --> 00:28:29.860]   - Barefoot?
[00:28:29.860 --> 00:28:32.320]   - Yeah, or with minimal shoes or whatever that--
[00:28:32.320 --> 00:28:34.400]   - 12 times two?
[00:28:34.400 --> 00:28:35.240]   - Yeah.
[00:28:35.240 --> 00:28:36.060]   - Okay.
[00:28:36.060 --> 00:28:38.400]   - It became kind of a game of how can I get to work.
[00:28:38.400 --> 00:28:41.000]   I've rollerbladed, I've done all kinds of weird stuff,
[00:28:41.000 --> 00:28:42.680]   but my favorite one these days,
[00:28:42.680 --> 00:28:45.040]   I've been taking the Charles River to work.
[00:28:45.040 --> 00:28:50.040]   So, I can put in a little rowboat, not so far from my house,
[00:28:50.040 --> 00:28:53.320]   but the Charles River takes a long way to get to MIT.
[00:28:53.320 --> 00:28:56.400]   So, I can spend a long time getting there.
[00:28:56.400 --> 00:28:58.740]   And it's, you know, it's not about, I don't know,
[00:28:58.740 --> 00:29:02.560]   it's just about, I've had people ask me,
[00:29:02.560 --> 00:29:04.440]   how can you justify taking that time?
[00:29:05.800 --> 00:29:10.120]   But for me, it's just a magical time to think,
[00:29:10.120 --> 00:29:13.760]   to compress, decompress, you know,
[00:29:13.760 --> 00:29:16.220]   especially I'll wake up, do a lot of work in the morning,
[00:29:16.220 --> 00:29:19.180]   and then I kind of have to just let that settle
[00:29:19.180 --> 00:29:20.700]   before I'm ready for all my meetings.
[00:29:20.700 --> 00:29:22.940]   And then on the way home, it's a great time to load it,
[00:29:22.940 --> 00:29:24.600]   sort of let that settle.
[00:29:24.600 --> 00:29:29.600]   - You lead a large group of people.
[00:29:29.600 --> 00:29:33.980]   I mean, is there days where you're like,
[00:29:33.980 --> 00:29:36.620]   oh shit, I gotta get to work in an hour.
[00:29:36.620 --> 00:29:43.660]   Like, I mean, is there a tension there?
[00:29:43.660 --> 00:29:47.940]   And like, if we look at the grand scheme of things,
[00:29:47.940 --> 00:29:49.500]   just like you said, long-term,
[00:29:49.500 --> 00:29:51.700]   that meeting probably doesn't matter.
[00:29:51.700 --> 00:29:54.660]   Like, you can always say, I'll run
[00:29:54.660 --> 00:29:57.100]   and let the meeting happen how it happens.
[00:29:57.100 --> 00:30:01.340]   Like, how do you, that zen,
[00:30:01.340 --> 00:30:03.580]   what do you do with that tension
[00:30:03.580 --> 00:30:05.580]   between the real world saying urgently,
[00:30:05.580 --> 00:30:08.200]   you need to be there, this is important,
[00:30:08.200 --> 00:30:10.020]   everything is melting down,
[00:30:10.020 --> 00:30:11.780]   how are we gonna fix this robot?
[00:30:11.780 --> 00:30:14.620]   There's this critical meeting,
[00:30:14.620 --> 00:30:17.980]   and then there's this zen beauty of just running,
[00:30:17.980 --> 00:30:20.380]   the simplicity of it, you along with nature.
[00:30:20.380 --> 00:30:22.660]   What do you do with that?
[00:30:22.660 --> 00:30:25.500]   - I would say I'm not a fast runner, particularly.
[00:30:25.500 --> 00:30:26.940]   Probably my fastest splits ever
[00:30:26.940 --> 00:30:29.180]   was when I had to get to daycare on time
[00:30:29.180 --> 00:30:30.660]   because they were gonna charge me, you know,
[00:30:30.660 --> 00:30:32.780]   some dollar per minute that I was late.
[00:30:33.540 --> 00:30:35.920]   I've run some fast splits to daycare,
[00:30:35.920 --> 00:30:39.840]   but those times are past now.
[00:30:39.840 --> 00:30:44.920]   I think work, you can find a work-life balance in that way.
[00:30:44.920 --> 00:30:46.180]   I think you just have to.
[00:30:46.180 --> 00:30:48.660]   I think I am better at work
[00:30:48.660 --> 00:30:52.220]   because I take time to think on the way in.
[00:30:52.220 --> 00:30:54.400]   So I plan my day around it,
[00:30:54.400 --> 00:31:00.340]   and I rarely feel that those are really at odds.
[00:31:00.940 --> 00:31:03.420]   - So what, the bucket list item.
[00:31:03.420 --> 00:31:07.260]   If we're talking 12 times two,
[00:31:07.260 --> 00:31:08.860]   or approaching a marathon,
[00:31:08.860 --> 00:31:15.100]   what, have you run an ultra marathon before?
[00:31:15.100 --> 00:31:16.800]   Do you do races?
[00:31:16.800 --> 00:31:17.640]   Is there, what's--
[00:31:17.640 --> 00:31:18.780]   - Not to win.
[00:31:18.780 --> 00:31:21.660]   - Not to-- (laughs)
[00:31:21.660 --> 00:31:23.740]   - I'm not gonna take a dinghy across the Atlantic
[00:31:23.740 --> 00:31:24.820]   or something if that's what you want,
[00:31:24.820 --> 00:31:27.940]   but if someone does and wants to write a book,
[00:31:27.940 --> 00:31:28.780]   I would totally read it
[00:31:28.780 --> 00:31:31.140]   'cause I'm a sucker for that kind of thing.
[00:31:31.140 --> 00:31:33.580]   No, I do have some fun things that I will try.
[00:31:33.580 --> 00:31:35.300]   I like to, when I travel,
[00:31:35.300 --> 00:31:37.020]   I almost always bike to the Logan Airport
[00:31:37.020 --> 00:31:38.740]   and fold up a little folding bike
[00:31:38.740 --> 00:31:41.020]   and then take it with me and bike to wherever I'm going.
[00:31:41.020 --> 00:31:42.380]   And it's taken me,
[00:31:42.380 --> 00:31:44.580]   or I'll take a stand-up paddleboard these days
[00:31:44.580 --> 00:31:45.500]   on the airplane,
[00:31:45.500 --> 00:31:46.580]   and then I'll try to paddle around
[00:31:46.580 --> 00:31:47.860]   where I'm going or whatever.
[00:31:47.860 --> 00:31:49.900]   And I've done some crazy things.
[00:31:49.900 --> 00:31:52.040]   - But not for the,
[00:31:52.040 --> 00:31:55.660]   I now talk, I don't know if you know
[00:31:55.660 --> 00:31:57.500]   who David Goggins is by any chance.
[00:31:57.500 --> 00:31:58.460]   - Not well, but yeah.
[00:31:58.460 --> 00:32:00.140]   - But I talk to him now every day.
[00:32:00.140 --> 00:32:05.140]   So he's the person who made me do this stupid challenge.
[00:32:05.140 --> 00:32:10.160]   So he's insane, and he does things for the purpose,
[00:32:10.160 --> 00:32:11.340]   in the best kind of way.
[00:32:11.340 --> 00:32:16.340]   He does things like for the explicit purpose of suffering.
[00:32:16.340 --> 00:32:18.420]   Like he picks the thing that,
[00:32:18.420 --> 00:32:21.500]   like whatever he thinks he can do, he does more.
[00:32:21.500 --> 00:32:26.140]   So is that, do you have that thing in you?
[00:32:26.140 --> 00:32:27.300]   Or are you--
[00:32:27.300 --> 00:32:29.100]   - I think it's become the opposite.
[00:32:29.100 --> 00:32:30.660]   (Lex laughing)
[00:32:30.660 --> 00:32:32.260]   - So you're like that dynamical system,
[00:32:32.260 --> 00:32:34.380]   that the walker, the efficient--
[00:32:34.380 --> 00:32:37.840]   - Yeah, it's leave no pain, right?
[00:32:37.840 --> 00:32:41.540]   You should end feeling better than you started.
[00:32:41.540 --> 00:32:44.540]   But it's mostly, I think,
[00:32:44.540 --> 00:32:47.700]   and COVID has tested this 'cause I've lost my commute.
[00:32:47.700 --> 00:32:51.940]   I think I'm perfectly happy walking around town
[00:32:51.940 --> 00:32:55.620]   with my wife and kids if they could get them to go.
[00:32:55.620 --> 00:32:57.780]   And it's more about just getting outside
[00:32:57.780 --> 00:32:59.980]   and getting away from the keyboard for some time
[00:32:59.980 --> 00:33:01.380]   just to let things compress.
[00:33:01.380 --> 00:33:04.100]   - Let's go into robotics a little bit.
[00:33:04.100 --> 00:33:06.740]   What to you is the most beautiful idea in robotics?
[00:33:06.740 --> 00:33:09.740]   Whether we're talking about control,
[00:33:09.740 --> 00:33:12.740]   or whether we're talking about optimization
[00:33:12.740 --> 00:33:14.200]   and the math side of things,
[00:33:14.200 --> 00:33:16.180]   or the engineering side of things,
[00:33:16.180 --> 00:33:18.180]   or the philosophical side of things.
[00:33:18.180 --> 00:33:23.520]   - I think I've been lucky to experience something
[00:33:23.520 --> 00:33:27.700]   that not so many roboticists have experienced,
[00:33:27.700 --> 00:33:30.220]   which is to hang out
[00:33:30.220 --> 00:33:33.540]   with some really amazing control theorists.
[00:33:33.540 --> 00:33:39.420]   And the clarity of thought
[00:33:39.420 --> 00:33:44.660]   that some of the more mathematical control theory can bring
[00:33:44.660 --> 00:33:47.500]   to even very complex, messy-looking problems
[00:33:47.500 --> 00:33:53.140]   is really, it really had a big impact on me
[00:33:53.140 --> 00:33:57.900]   and I had a day even just a couple of weeks ago
[00:33:57.900 --> 00:34:01.020]   where I had spent the day on a Zoom robotics conference
[00:34:01.020 --> 00:34:04.060]   having great conversations with lots of people.
[00:34:04.060 --> 00:34:06.780]   Felt really good about the ideas
[00:34:06.780 --> 00:34:09.500]   that were flowing and the like.
[00:34:09.500 --> 00:34:12.940]   And then I had a late afternoon meeting
[00:34:12.940 --> 00:34:15.540]   with one of my favorite control theorists
[00:34:15.540 --> 00:34:20.540]   and we went from these abstract discussions
[00:34:20.780 --> 00:34:25.740]   about maybes and what ifs and what a great idea,
[00:34:25.740 --> 00:34:29.340]   to these super precise statements
[00:34:29.340 --> 00:34:33.860]   about systems that aren't that much more simple
[00:34:33.860 --> 00:34:37.640]   or abstract than the ones I care about deeply.
[00:34:37.640 --> 00:34:40.380]   And the contrast of that is,
[00:34:40.380 --> 00:34:44.020]   I don't know, it really gets me.
[00:34:44.020 --> 00:34:47.820]   I think people underestimate
[00:34:49.880 --> 00:34:53.520]   maybe the power of clear thinking.
[00:34:53.520 --> 00:34:58.260]   And so for instance, deep learning is amazing.
[00:34:58.260 --> 00:35:02.440]   I use it heavily in our work.
[00:35:02.440 --> 00:35:04.800]   I think it's changed the world unquestionable.
[00:35:04.800 --> 00:35:09.000]   It makes it easy to get things to work
[00:35:09.000 --> 00:35:10.600]   without thinking as critically about it.
[00:35:10.600 --> 00:35:13.420]   So I think one of the challenges as an educator
[00:35:13.420 --> 00:35:17.120]   is to think about how do we make sure people get a taste
[00:35:17.120 --> 00:35:22.120]   of the more rigorous thinking that I think goes along
[00:35:22.120 --> 00:35:24.760]   with some different approaches.
[00:35:24.760 --> 00:35:26.200]   - Yeah, so that's really interesting.
[00:35:26.200 --> 00:35:29.040]   So understanding the fundamentals,
[00:35:29.040 --> 00:35:33.880]   the first principles of the problem,
[00:35:33.880 --> 00:35:35.920]   or in this case, it's mechanics,
[00:35:35.920 --> 00:35:40.460]   like how a thing moves, how a thing behaves,
[00:35:40.460 --> 00:35:42.420]   like all the forces involved,
[00:35:42.420 --> 00:35:44.780]   like really getting a deep understanding of that.
[00:35:44.780 --> 00:35:47.280]   I mean, from physics, the first principle thing
[00:35:47.280 --> 00:35:51.060]   come from physics, and here it's literally physics.
[00:35:51.060 --> 00:35:53.880]   Yeah, and this applies, in deep learning,
[00:35:53.880 --> 00:35:56.920]   this applies to not just, I mean,
[00:35:56.920 --> 00:35:59.200]   it applies so cleanly in robotics,
[00:35:59.200 --> 00:36:03.520]   but it also applies to just in any data set.
[00:36:03.520 --> 00:36:07.040]   I find this true, I mean, driving as well.
[00:36:07.040 --> 00:36:10.800]   There's a lot of folks that work on autonomous vehicles
[00:36:11.720 --> 00:36:16.720]   that don't study driving, like deeply.
[00:36:16.720 --> 00:36:23.080]   I might be coming a little bit from the psychology side,
[00:36:23.080 --> 00:36:28.080]   but I remember I spent a ridiculous number of hours
[00:36:28.080 --> 00:36:31.880]   at lunch at this like lawn chair,
[00:36:31.880 --> 00:36:35.680]   and I would sit somewhere in MIT's campus,
[00:36:35.680 --> 00:36:37.280]   there's a few interesting intersections,
[00:36:37.280 --> 00:36:39.380]   and we just watch people cross.
[00:36:39.380 --> 00:36:43.240]   So we were studying pedestrian behavior,
[00:36:43.240 --> 00:36:46.600]   and I felt like, I had to record a lot of video to try,
[00:36:46.600 --> 00:36:47.800]   and then there's the computer vision
[00:36:47.800 --> 00:36:50.840]   extracts their movement, how they move their head, and so on.
[00:36:50.840 --> 00:36:55.320]   But like every time, I felt like I didn't understand enough.
[00:36:55.320 --> 00:36:58.600]   I just, I felt like I wasn't understanding
[00:36:58.600 --> 00:37:01.620]   what, how are people signaling to each other?
[00:37:01.620 --> 00:37:03.560]   What are they thinking?
[00:37:03.560 --> 00:37:07.780]   How cognizant are they of their fear of death?
[00:37:07.780 --> 00:37:11.880]   Like, what's the underlying game theory here?
[00:37:11.880 --> 00:37:14.120]   What are the incentives?
[00:37:14.120 --> 00:37:17.800]   And then I finally found a live stream of an intersection
[00:37:17.800 --> 00:37:20.280]   that's like high def that I just, I would watch,
[00:37:20.280 --> 00:37:21.720]   so I wouldn't have to sit out there.
[00:37:21.720 --> 00:37:23.600]   But that's interesting, so like, I feel--
[00:37:23.600 --> 00:37:25.120]   - That's tough, that's a tough example,
[00:37:25.120 --> 00:37:27.040]   because I mean, the learning--
[00:37:27.040 --> 00:37:28.760]   - Humans are involved.
[00:37:28.760 --> 00:37:33.420]   - Not just because human, but I think the learning mantra
[00:37:33.420 --> 00:37:35.480]   is that basically the statistics of the data
[00:37:35.480 --> 00:37:37.920]   will tell me things I need to know, right?
[00:37:37.920 --> 00:37:42.840]   And for the example you gave of all the nuances
[00:37:42.840 --> 00:37:46.840]   of eye contact or hand gestures or whatever
[00:37:46.840 --> 00:37:48.880]   that are happening for these subtle interactions
[00:37:48.880 --> 00:37:51.120]   between pedestrians and traffic, right?
[00:37:51.120 --> 00:37:54.560]   Maybe the data will tell that story.
[00:37:54.560 --> 00:37:59.560]   Maybe even one level more meta than what you're saying.
[00:37:59.560 --> 00:38:03.800]   For a particular problem, I think it might be the case
[00:38:03.800 --> 00:38:06.000]   that data should tell us the story.
[00:38:06.000 --> 00:38:09.400]   But I think there's a rigorous thinking
[00:38:09.400 --> 00:38:13.120]   that is just an essential skill for a mathematician
[00:38:13.120 --> 00:38:18.120]   or an engineer that I just don't wanna lose it.
[00:38:18.120 --> 00:38:22.440]   There are certainly super rigorous control,
[00:38:22.440 --> 00:38:24.920]   or sorry, machine learning people.
[00:38:24.920 --> 00:38:27.980]   I just think deep learning makes it so easy
[00:38:27.980 --> 00:38:30.960]   to do some things that our next generation,
[00:38:31.560 --> 00:38:36.560]   are not immediately rewarded for going through
[00:38:36.560 --> 00:38:38.520]   some of the more rigorous approaches.
[00:38:38.520 --> 00:38:40.760]   And then I wonder where that takes us.
[00:38:40.760 --> 00:38:42.240]   Well, I'm actually optimistic about it.
[00:38:42.240 --> 00:38:45.840]   I just want to do my part to try to steer
[00:38:45.840 --> 00:38:48.000]   that rigorous thinking.
[00:38:48.000 --> 00:38:51.040]   - So there's like two questions I wanna ask.
[00:38:51.040 --> 00:38:56.040]   Do you have sort of a good example of rigorous thinking
[00:38:56.040 --> 00:39:00.840]   where it's easy to get lazy and not do the rigorous thinking
[00:39:00.840 --> 00:39:02.480]   and the other question I have is like,
[00:39:02.480 --> 00:39:07.480]   do you have advice of how to practice rigorous thinking
[00:39:07.480 --> 00:39:13.680]   in all the computer science disciplines
[00:39:13.680 --> 00:39:14.720]   that we've mentioned?
[00:39:14.720 --> 00:39:21.360]   - Yeah, I mean, there are times where problems
[00:39:21.360 --> 00:39:24.800]   that can be solved with well-known mature methods
[00:39:24.800 --> 00:39:30.280]   could also be solved with a deep learning approach
[00:39:30.280 --> 00:39:35.280]   and there's an argument that you must use learning
[00:39:35.280 --> 00:39:38.320]   even for the parts we already think we know
[00:39:38.320 --> 00:39:39.760]   because if the human has touched it,
[00:39:39.760 --> 00:39:42.420]   then you've biased the system
[00:39:42.420 --> 00:39:44.280]   and you've suddenly put a bottleneck in there
[00:39:44.280 --> 00:39:46.280]   that is your own mental model.
[00:39:46.280 --> 00:39:48.480]   But something like inverting a matrix,
[00:39:48.480 --> 00:39:50.720]   you know, I think we know how to do that pretty well,
[00:39:50.720 --> 00:39:51.960]   even if it's a pretty big matrix
[00:39:51.960 --> 00:39:53.120]   and we understand that pretty well
[00:39:53.120 --> 00:39:55.040]   and you could train a deep network to do it,
[00:39:55.040 --> 00:39:57.320]   but you shouldn't probably.
[00:39:57.840 --> 00:40:02.200]   - So in that sense, rigorous thinking is understanding
[00:40:02.200 --> 00:40:07.200]   the scope and the limitations of the methods that we have,
[00:40:07.200 --> 00:40:10.120]   like how to use the tools of mathematics properly.
[00:40:10.120 --> 00:40:15.080]   - Yeah, I think, you know, taking a class on analysis
[00:40:15.080 --> 00:40:17.360]   is all I'm sort of arguing.
[00:40:17.360 --> 00:40:20.060]   Take a chance to stop and force yourself
[00:40:20.060 --> 00:40:23.520]   to think rigorously about even, you know,
[00:40:23.520 --> 00:40:24.880]   the rational numbers or something,
[00:40:24.880 --> 00:40:27.760]   you know, it doesn't have to be the end all problem,
[00:40:27.760 --> 00:40:31.080]   but that exercise of clear thinking,
[00:40:31.080 --> 00:40:33.420]   I think goes a long way
[00:40:33.420 --> 00:40:35.280]   and I just want to make sure we keep preaching it.
[00:40:35.280 --> 00:40:36.360]   - We don't lose it. - Yeah.
[00:40:36.360 --> 00:40:39.560]   - But do you think when you're doing like rigorous thinking
[00:40:39.560 --> 00:40:43.240]   or like maybe trying to write down equations
[00:40:43.240 --> 00:40:47.960]   or sort of explicitly, like formally describe a system,
[00:40:47.960 --> 00:40:51.560]   do you think we naturally simplify things too much?
[00:40:51.560 --> 00:40:53.980]   Is that a danger you run into?
[00:40:53.980 --> 00:40:56.200]   Like in order to be able to understand something
[00:40:56.200 --> 00:40:58.180]   about the system mathematically,
[00:40:58.180 --> 00:41:01.680]   we make it too much of a toy example.
[00:41:01.680 --> 00:41:04.480]   - But I think that's the good stuff, right?
[00:41:04.480 --> 00:41:07.040]   - That's how you understand the fundamentals?
[00:41:07.040 --> 00:41:07.880]   - I think so.
[00:41:07.880 --> 00:41:10.400]   I think maybe even that's a key to intelligence
[00:41:10.400 --> 00:41:12.480]   or something, but I mean, okay,
[00:41:12.480 --> 00:41:15.120]   what if Newton and Galileo had deep learning
[00:41:15.120 --> 00:41:18.360]   and they had done a bunch of experiments
[00:41:18.360 --> 00:41:20.280]   and they told the world,
[00:41:20.280 --> 00:41:22.400]   "Here's your weights of your neural network.
[00:41:22.400 --> 00:41:23.520]   "We've solved the problem."
[00:41:23.520 --> 00:41:24.360]   - Yeah.
[00:41:24.360 --> 00:41:25.340]   - Where would we be today?
[00:41:25.340 --> 00:41:28.400]   I don't think we'd be as far as we are.
[00:41:28.400 --> 00:41:29.240]   There's something to be said
[00:41:29.240 --> 00:41:32.520]   about having the simplest explanation for a phenomenon.
[00:41:32.520 --> 00:41:37.180]   So I don't doubt that we can train neural networks
[00:41:37.180 --> 00:41:39.500]   to predict even physical,
[00:41:39.500 --> 00:41:44.660]   F equals MA type equations,
[00:41:44.660 --> 00:41:51.320]   but I maybe, I want another Newton to come along
[00:41:51.320 --> 00:41:52.920]   'cause I think there's more to do
[00:41:52.920 --> 00:41:56.020]   in terms of coming up with the simple models
[00:41:56.020 --> 00:41:59.860]   for more complicated tasks.
[00:41:59.860 --> 00:42:01.180]   - Yeah.
[00:42:01.180 --> 00:42:04.240]   Let's not offend the AI systems from 50 years
[00:42:04.240 --> 00:42:06.340]   from now that are listening to this
[00:42:06.340 --> 00:42:08.260]   that are probably better at,
[00:42:08.260 --> 00:42:10.180]   might be better at coming up
[00:42:10.180 --> 00:42:13.100]   with F equals MA equations themselves.
[00:42:13.100 --> 00:42:15.420]   - Oh, sorry, I actually think learning
[00:42:15.420 --> 00:42:18.220]   is probably a route to achieving this,
[00:42:18.220 --> 00:42:21.180]   but the representation matters, right?
[00:42:21.180 --> 00:42:24.700]   And I think having a function
[00:42:24.700 --> 00:42:27.260]   that takes my inputs to outputs
[00:42:27.260 --> 00:42:30.760]   that is arbitrarily complex may not be the end goal.
[00:42:30.760 --> 00:42:34.140]   I think there's still the most simple
[00:42:34.140 --> 00:42:36.400]   or parsimonious explanation for the data.
[00:42:36.400 --> 00:42:39.020]   Simple doesn't mean low dimensional.
[00:42:39.020 --> 00:42:41.020]   That's one thing I think that we've,
[00:42:41.020 --> 00:42:41.980]   a lesson that we've learned.
[00:42:41.980 --> 00:42:46.080]   So a standard way to do model reduction
[00:42:46.080 --> 00:42:47.860]   or system identification and controls
[00:42:47.860 --> 00:42:50.100]   is to, the typical formulation is that you try
[00:42:50.100 --> 00:42:53.420]   to find the minimal state dimension realization
[00:42:53.420 --> 00:42:56.140]   of a system that hits some error bounds
[00:42:56.140 --> 00:42:57.780]   or something like that.
[00:42:57.780 --> 00:43:00.340]   And that's maybe not, I think we're learning
[00:43:00.340 --> 00:43:02.900]   that that was, that state dimension
[00:43:02.900 --> 00:43:04.540]   is not the right metric.
[00:43:04.540 --> 00:43:06.820]   - Of complexity.
[00:43:06.820 --> 00:43:07.640]   - Of complexity.
[00:43:07.640 --> 00:43:09.460]   But for me, I think a lot about contact,
[00:43:09.460 --> 00:43:10.840]   the mechanics of contact.
[00:43:10.840 --> 00:43:13.660]   The robot hand is picking up an object or something.
[00:43:13.660 --> 00:43:16.660]   And when I write down the equations of motion for that,
[00:43:16.660 --> 00:43:19.100]   they look incredibly complex,
[00:43:19.100 --> 00:43:23.420]   not because, actually not so much
[00:43:23.420 --> 00:43:26.660]   because of the dynamics of the hand when it's moving,
[00:43:26.660 --> 00:43:28.500]   but it's just the interactions
[00:43:28.500 --> 00:43:30.860]   and when they turn on and off, right?
[00:43:30.860 --> 00:43:33.300]   So having a high dimensional, you know,
[00:43:33.300 --> 00:43:36.420]   but simple description of what's happening out here is fine.
[00:43:36.420 --> 00:43:38.500]   But if, when I actually start touching,
[00:43:38.500 --> 00:43:41.860]   if I write down a different dynamical system
[00:43:41.860 --> 00:43:45.420]   for every polygon on my robot hand
[00:43:45.420 --> 00:43:47.300]   and every polygon on the object,
[00:43:47.300 --> 00:43:49.000]   whether it's in contact or not,
[00:43:49.000 --> 00:43:51.700]   with all the combinatorics that explodes there,
[00:43:51.700 --> 00:43:54.460]   then that's too complex.
[00:43:54.460 --> 00:43:55.780]   So I need to somehow summarize that
[00:43:55.780 --> 00:44:00.780]   with a more intuitive physics way of thinking.
[00:44:00.780 --> 00:44:03.500]   And yeah, I'm very optimistic
[00:44:03.500 --> 00:44:05.700]   that machine learning will get us there.
[00:44:05.700 --> 00:44:08.220]   - First of all, I mean, I'll probably do it
[00:44:08.220 --> 00:44:10.820]   in the introduction, but you're one
[00:44:10.820 --> 00:44:12.900]   of the great robotics people at MIT.
[00:44:12.900 --> 00:44:14.300]   You're a professor at MIT.
[00:44:14.300 --> 00:44:16.480]   You've teach a lot of amazing courses.
[00:44:16.480 --> 00:44:21.480]   You run a large group and you have a important history
[00:44:21.480 --> 00:44:23.960]   for MIT, I think, as being a part
[00:44:23.960 --> 00:44:26.340]   of the DARPA Robotics Challenge.
[00:44:26.340 --> 00:44:30.000]   Can you maybe first say what is the DARPA Robotics Challenge
[00:44:30.000 --> 00:44:35.000]   and then tell your story around it, your journey with it?
[00:44:35.000 --> 00:44:37.220]   - Yeah, sure.
[00:44:37.220 --> 00:44:41.060]   So the DARPA Robotics Challenge,
[00:44:41.060 --> 00:44:44.720]   it came on the tails of the DARPA Grand Challenge
[00:44:44.720 --> 00:44:45.940]   and DARPA Urban Challenge,
[00:44:45.940 --> 00:44:48.420]   which were the challenges that brought us,
[00:44:48.420 --> 00:44:52.740]   put a spotlight on self-driving cars.
[00:44:52.740 --> 00:45:00.400]   Gil Pratt was at DARPA and pitched a new challenge
[00:45:00.400 --> 00:45:03.540]   that involved disaster response.
[00:45:03.540 --> 00:45:07.160]   It didn't explicitly require humanoids,
[00:45:07.160 --> 00:45:09.220]   although humanoids came into the picture.
[00:45:09.220 --> 00:45:14.740]   This happened shortly after the Fukushima disaster in Japan.
[00:45:14.740 --> 00:45:17.660]   And our challenge was motivated roughly by that
[00:45:17.660 --> 00:45:21.060]   because that was a case where if we had had robots
[00:45:21.060 --> 00:45:22.700]   that were ready to be sent in,
[00:45:22.700 --> 00:45:26.580]   there's a chance that we could have averted disaster.
[00:45:26.580 --> 00:45:30.620]   And certainly after the, in the disaster response,
[00:45:30.620 --> 00:45:32.380]   there were times where we would have loved
[00:45:32.380 --> 00:45:33.540]   to have sent robots in.
[00:45:33.540 --> 00:45:39.220]   So in practice, what we ended up with was a grand challenge,
[00:45:39.220 --> 00:45:41.180]   a DARPA Robotics Challenge,
[00:45:42.140 --> 00:45:47.140]   where Boston Dynamics was to make humanoid robots.
[00:45:47.140 --> 00:45:52.500]   People like me and the amazing team at MIT
[00:45:52.500 --> 00:45:56.740]   were competing first in a simulation challenge
[00:45:56.740 --> 00:45:59.420]   to try to be one of the ones that wins the right
[00:45:59.420 --> 00:46:03.300]   to work on one of the Boston Dynamics humanoids
[00:46:03.300 --> 00:46:06.580]   in order to compete in the final challenge,
[00:46:06.580 --> 00:46:08.540]   which was a physical challenge.
[00:46:08.540 --> 00:46:10.340]   - And at that point, it was already,
[00:46:10.340 --> 00:46:13.740]   so it was decided as humanoid robots early on.
[00:46:13.740 --> 00:46:15.100]   - There were two tracks.
[00:46:15.100 --> 00:46:16.860]   You could enter as a hardware team
[00:46:16.860 --> 00:46:18.460]   where you brought your own robot,
[00:46:18.460 --> 00:46:21.340]   or you could enter through the virtual robotics challenge
[00:46:21.340 --> 00:46:24.260]   as a software team that would try to win the right
[00:46:24.260 --> 00:46:25.940]   to use one of the Boston Dynamics robots.
[00:46:25.940 --> 00:46:27.940]   - Which are called Atlas. - Atlas.
[00:46:27.940 --> 00:46:30.860]   - Humanoid robots. - Yeah, it was a 400 pound
[00:46:30.860 --> 00:46:34.440]   Marvel, but a pretty big, scary looking robot.
[00:46:34.440 --> 00:46:38.260]   - Expensive too. - Expensive, yeah.
[00:46:38.260 --> 00:46:42.340]   - Okay, so, I mean, how did you feel
[00:46:42.340 --> 00:46:44.780]   the prospect of this kind of challenge?
[00:46:44.780 --> 00:46:48.820]   I mean, it seems, you know, autonomous vehicles,
[00:46:48.820 --> 00:46:51.060]   yeah, I guess that sounds hard,
[00:46:51.060 --> 00:46:53.980]   but not really from a robotics perspective.
[00:46:53.980 --> 00:46:56.020]   It's like, didn't they do it in the '80s
[00:46:56.020 --> 00:46:57.780]   is the kind of feeling I would have
[00:46:57.780 --> 00:47:00.820]   like when you first look at the problem.
[00:47:00.820 --> 00:47:04.900]   It's on wheels, but like humanoid robots,
[00:47:04.900 --> 00:47:07.080]   that sounds really hard.
[00:47:08.140 --> 00:47:12.860]   So what, psychologically speaking,
[00:47:12.860 --> 00:47:15.780]   what were you feeling, excited, scared?
[00:47:15.780 --> 00:47:18.020]   Why the heck did you get yourself involved
[00:47:18.020 --> 00:47:19.660]   in this kind of messy challenge?
[00:47:19.660 --> 00:47:21.980]   - We didn't really know for sure
[00:47:21.980 --> 00:47:23.420]   what we were signing up for,
[00:47:23.420 --> 00:47:26.840]   in the sense that you could have had something that,
[00:47:26.840 --> 00:47:30.780]   as it was described in the call for participation,
[00:47:30.780 --> 00:47:32.700]   that could have put a huge emphasis
[00:47:32.700 --> 00:47:35.700]   on the dynamics of walking and not falling down
[00:47:35.700 --> 00:47:37.380]   and walking over rough terrain,
[00:47:37.380 --> 00:47:38.580]   or the same description,
[00:47:38.580 --> 00:47:40.780]   'cause the robot had to go into this disaster area
[00:47:40.780 --> 00:47:44.580]   and turn valves and pick up a drill
[00:47:44.580 --> 00:47:45.780]   and cut a hole through a wall.
[00:47:45.780 --> 00:47:48.420]   It had to do some interesting things.
[00:47:48.420 --> 00:47:51.860]   The challenge could have really highlighted perception
[00:47:51.860 --> 00:47:54.860]   and autonomous planning,
[00:47:54.860 --> 00:47:57.860]   or it ended up that, you know,
[00:47:57.860 --> 00:48:01.080]   locomoting over a complex terrain
[00:48:01.080 --> 00:48:04.020]   played a pretty big role in the competition.
[00:48:04.020 --> 00:48:05.520]   So--
[00:48:05.520 --> 00:48:08.360]   - And the degree of autonomy wasn't clear.
[00:48:08.360 --> 00:48:09.960]   - The degree of autonomy was always
[00:48:09.960 --> 00:48:11.920]   a central part of the discussion.
[00:48:11.920 --> 00:48:15.560]   So what wasn't clear was how we would be able,
[00:48:15.560 --> 00:48:17.520]   how far we'd be able to get with it.
[00:48:17.520 --> 00:48:21.640]   So the idea was always that you want semi-autonomy,
[00:48:21.640 --> 00:48:24.280]   that you want the robot to have enough compute
[00:48:24.280 --> 00:48:27.640]   that you can have a degraded network link to a human.
[00:48:27.640 --> 00:48:30.640]   And so the same way we had degraded networks
[00:48:30.640 --> 00:48:33.160]   at many natural disasters,
[00:48:33.160 --> 00:48:34.960]   you'd send your robot in,
[00:48:34.960 --> 00:48:37.520]   you'd be able to get a few bits back and forth,
[00:48:37.520 --> 00:48:38.920]   but you don't get to have enough,
[00:48:38.920 --> 00:48:42.080]   potentially, to fully operate the robot,
[00:48:42.080 --> 00:48:43.320]   every joint of the robot.
[00:48:43.320 --> 00:48:46.160]   So, and then the question was,
[00:48:46.160 --> 00:48:48.880]   and the gamesmanship of the organizers
[00:48:48.880 --> 00:48:50.680]   was to figure out what we're capable of,
[00:48:50.680 --> 00:48:52.580]   push us as far as we could,
[00:48:52.580 --> 00:48:55.300]   so that it would differentiate the teams
[00:48:55.300 --> 00:48:57.520]   that put more autonomy on the robot
[00:48:57.520 --> 00:48:59.320]   and had a few clicks and just said,
[00:48:59.320 --> 00:49:00.920]   "Go there, do this, go there, do this,"
[00:49:00.920 --> 00:49:03.400]   versus someone who's picking every footstep
[00:49:03.400 --> 00:49:04.560]   or something like that.
[00:49:05.280 --> 00:49:10.280]   - So what were some memories,
[00:49:10.280 --> 00:49:13.600]   painful, triumphant from the experience?
[00:49:13.600 --> 00:49:15.040]   Like what was that journey?
[00:49:15.040 --> 00:49:17.660]   Maybe if you can dig in a little deeper,
[00:49:17.660 --> 00:49:21.120]   maybe even on the technical side, on the team side,
[00:49:21.120 --> 00:49:23.000]   that whole process of,
[00:49:23.000 --> 00:49:28.200]   from the early idea stages to actually competing.
[00:49:28.200 --> 00:49:30.600]   - I mean, this was a defining experience for me.
[00:49:30.600 --> 00:49:33.900]   It came at the right time for me in my career.
[00:49:33.900 --> 00:49:37.440]   I had gotten tenure before I was due a sabbatical,
[00:49:37.440 --> 00:49:39.800]   and most people do something relaxing
[00:49:39.800 --> 00:49:41.880]   and restorative for a sabbatical.
[00:49:41.880 --> 00:49:44.480]   - So you got tenure before this?
[00:49:44.480 --> 00:49:46.160]   - Yeah, yeah, yeah.
[00:49:46.160 --> 00:49:48.040]   It was a good time for me.
[00:49:48.040 --> 00:49:50.920]   We had a bunch of algorithms that we were very happy with.
[00:49:50.920 --> 00:49:52.520]   We wanted to see how far we could push them,
[00:49:52.520 --> 00:49:54.880]   and this was a chance to really test our mettle,
[00:49:54.880 --> 00:49:56.840]   to do more proper software engineering.
[00:49:56.840 --> 00:50:01.520]   The team, we all just worked our butts off.
[00:50:02.920 --> 00:50:04.860]   We're in that lab almost all the time.
[00:50:04.860 --> 00:50:09.580]   Okay, so I mean, there were some, of course,
[00:50:09.580 --> 00:50:12.060]   high highs and low lows throughout that,
[00:50:12.060 --> 00:50:14.980]   anytime you're not sleeping and devoting your life
[00:50:14.980 --> 00:50:16.360]   to a 400-pound humanoid.
[00:50:16.360 --> 00:50:20.680]   I remember actually one funny moment
[00:50:20.680 --> 00:50:21.900]   where we're all super tired,
[00:50:21.900 --> 00:50:24.720]   and so Atlas had to walk across cinder blocks.
[00:50:24.720 --> 00:50:26.500]   That was one of the obstacles.
[00:50:26.500 --> 00:50:28.420]   And I remember Atlas was powered down,
[00:50:28.420 --> 00:50:31.260]   hanging limp on its harness,
[00:50:31.260 --> 00:50:33.980]   and the humans were there picking up
[00:50:33.980 --> 00:50:35.180]   and laying the brick down
[00:50:35.180 --> 00:50:36.460]   so that the robot could walk over it.
[00:50:36.460 --> 00:50:38.460]   And I thought, "What is wrong with this?"
[00:50:38.460 --> 00:50:41.560]   We've got a robot just watching us
[00:50:41.560 --> 00:50:44.160]   do all the manual labor so that it can take its little
[00:50:44.160 --> 00:50:46.740]   stroll across the terrain.
[00:50:46.740 --> 00:50:52.140]   I mean, even the virtual robotics challenge
[00:50:52.140 --> 00:50:54.620]   was super nerve-wracking and dramatic.
[00:50:54.620 --> 00:50:59.620]   I remember, so we were using Gazebo as a simulator.
[00:50:59.620 --> 00:51:02.300]   We were using Gazebo as a simulator on the cloud,
[00:51:02.300 --> 00:51:03.940]   and there was all these interesting challenges.
[00:51:03.940 --> 00:51:08.580]   I think the investment that OSRFC,
[00:51:08.580 --> 00:51:10.020]   whatever they were called at that time,
[00:51:10.020 --> 00:51:12.220]   Brian Gerke's team at Open Source Robotics,
[00:51:12.220 --> 00:51:16.020]   they were pushing on the capabilities of Gazebo
[00:51:16.020 --> 00:51:20.380]   in order to scale it to the complexity of these challenges.
[00:51:20.380 --> 00:51:23.900]   So up to the virtual competition.
[00:51:23.900 --> 00:51:26.180]   So the virtual competition was,
[00:51:26.180 --> 00:51:28.460]   you will sign on at a certain time,
[00:51:28.460 --> 00:51:29.820]   and we'll have a network connection
[00:51:29.820 --> 00:51:32.060]   to another machine on the cloud
[00:51:32.060 --> 00:51:34.860]   that is running the simulator of your robot.
[00:51:34.860 --> 00:51:38.140]   And your controller will run on this computer,
[00:51:38.140 --> 00:51:40.880]   and the physics will run on the other,
[00:51:40.880 --> 00:51:43.020]   and you have to connect.
[00:51:43.020 --> 00:51:48.020]   Now, the physics, they wanted it to run at real-time rates,
[00:51:48.020 --> 00:51:50.700]   because there was an element of human interaction.
[00:51:50.700 --> 00:51:53.260]   And humans, if you do want to tele-op,
[00:51:53.260 --> 00:51:56.100]   it works way better if it's at frame rate.
[00:51:56.100 --> 00:51:57.140]   - Oh, cool.
[00:51:57.140 --> 00:51:58.740]   - But it was very hard to simulate
[00:51:58.740 --> 00:52:03.260]   these complex scenes at real-time rate.
[00:52:03.260 --> 00:52:06.540]   So right up to days before the competition,
[00:52:06.540 --> 00:52:11.060]   the simulator wasn't quite at real-time rate.
[00:52:11.060 --> 00:52:12.100]   And that was great for me,
[00:52:12.100 --> 00:52:13.780]   because my controller was solving
[00:52:13.780 --> 00:52:16.340]   a pretty big optimization problem,
[00:52:16.340 --> 00:52:17.800]   and it wasn't quite at real-time rate.
[00:52:17.800 --> 00:52:18.900]   So I was fine.
[00:52:18.900 --> 00:52:20.520]   I was keeping up with the simulator.
[00:52:20.520 --> 00:52:22.900]   We were both running at about 0.7.
[00:52:22.900 --> 00:52:24.980]   And I remember getting this email,
[00:52:24.980 --> 00:52:28.460]   and by the way, the perception folks on our team hated,
[00:52:28.460 --> 00:52:31.460]   that they knew that if my controller was too slow,
[00:52:31.460 --> 00:52:32.500]   the robot was gonna fall down.
[00:52:32.500 --> 00:52:34.920]   And no matter how good their perception system was,
[00:52:34.920 --> 00:52:36.940]   if I can't make my controller fast.
[00:52:36.940 --> 00:52:37.940]   Anyways, we get this email
[00:52:37.940 --> 00:52:40.300]   like three days before the virtual competition.
[00:52:40.300 --> 00:52:41.500]   You know, it's for all the marbles.
[00:52:41.500 --> 00:52:44.940]   We're gonna either get a humanoid robot or we're not.
[00:52:44.940 --> 00:52:46.380]   And we get an email saying, "Good news.
[00:52:46.380 --> 00:52:48.660]   "We made the robot, the simulator faster."
[00:52:48.660 --> 00:52:49.500]   It's now 1.0.
[00:52:49.500 --> 00:52:54.780]   And I was just like, "Oh man, what are we gonna do here?"
[00:52:54.780 --> 00:52:58.380]   So that came in late at night for me.
[00:52:58.380 --> 00:53:00.500]   - A few days ahead.
[00:53:00.500 --> 00:53:01.420]   - A few days ahead.
[00:53:01.420 --> 00:53:03.980]   I went over, it happened at Frank Permenter,
[00:53:03.980 --> 00:53:06.820]   who's a very, very sharp,
[00:53:06.820 --> 00:53:10.220]   he was a student at the time working on optimization.
[00:53:10.220 --> 00:53:12.160]   He was still in lab.
[00:53:12.160 --> 00:53:15.100]   Frank, we need to make
[00:53:15.100 --> 00:53:17.340]   this quadratic programming solver faster.
[00:53:17.340 --> 00:53:19.840]   Not like a little faster, it's actually, you know.
[00:53:19.840 --> 00:53:24.180]   And we wrote a new solver for that QP,
[00:53:24.980 --> 00:53:26.620]   together, that night.
[00:53:26.620 --> 00:53:29.420]   - And you saw her. - It was terrifying.
[00:53:29.420 --> 00:53:31.940]   - So there's a really hard optimization problem
[00:53:31.940 --> 00:53:33.500]   that you're constantly solving.
[00:53:33.500 --> 00:53:36.860]   You didn't make the optimization problem simpler?
[00:53:36.860 --> 00:53:38.540]   You wrote a new solver?
[00:53:38.540 --> 00:53:42.860]   - So, I mean, your observation is almost spot on.
[00:53:42.860 --> 00:53:44.540]   What we did was what everybody,
[00:53:44.540 --> 00:53:45.820]   I mean, people know how to do this,
[00:53:45.820 --> 00:53:49.300]   but we had not yet done this idea of warm starting.
[00:53:49.300 --> 00:53:51.380]   So we are solving a big optimization problem
[00:53:51.380 --> 00:53:52.700]   at every time step.
[00:53:52.700 --> 00:53:54.300]   But if you're running fast enough,
[00:53:54.300 --> 00:53:55.700]   the optimization problem you're solving
[00:53:55.700 --> 00:53:57.940]   on the last time step is pretty similar
[00:53:57.940 --> 00:54:00.100]   to the optimization you're gonna solve with the next.
[00:54:00.100 --> 00:54:02.300]   We, of course, had told our commercial solver
[00:54:02.300 --> 00:54:03.620]   to use warm starting,
[00:54:03.620 --> 00:54:07.220]   but even the interface to that commercial solver
[00:54:07.220 --> 00:54:09.860]   was causing us these delays.
[00:54:09.860 --> 00:54:12.780]   So what we did was we basically wrote,
[00:54:12.780 --> 00:54:15.420]   we called it fast QP at the time.
[00:54:15.420 --> 00:54:18.500]   We wrote a very lightweight, very fast layer,
[00:54:18.500 --> 00:54:22.160]   which would basically check if nearby solutions
[00:54:22.160 --> 00:54:23.860]   to the quadratic program,
[00:54:23.860 --> 00:54:28.000]   which were very easily checked, could stabilize the robot.
[00:54:28.000 --> 00:54:30.740]   And if they couldn't, we would fall back to the solver.
[00:54:30.740 --> 00:54:33.140]   - You couldn't really test this well, right?
[00:54:33.140 --> 00:54:34.300]   Or like?
[00:54:34.300 --> 00:54:37.380]   - So we always knew that if we fell back,
[00:54:37.380 --> 00:54:40.420]   if we, it got to the point where if for some reason
[00:54:40.420 --> 00:54:42.820]   things slowed down and we fell back to the original solver,
[00:54:42.820 --> 00:54:45.060]   the robot would actually literally fall down.
[00:54:45.060 --> 00:54:51.060]   So it was a harrowing sort of ledge we were sort of on.
[00:54:51.180 --> 00:54:54.880]   But I mean, actually, like the 400 pound humanoid
[00:54:54.880 --> 00:54:55.840]   could come crashing to the ground
[00:54:55.840 --> 00:54:58.020]   if your solver's not fast enough.
[00:54:58.020 --> 00:55:01.160]   But we had lots of good experiences.
[00:55:01.160 --> 00:55:05.600]   - Can I ask you a weird question I get
[00:55:05.600 --> 00:55:09.460]   about the idea of hard work?
[00:55:09.460 --> 00:55:14.320]   So actually people, like students of yours
[00:55:14.320 --> 00:55:15.960]   that I've interacted with,
[00:55:15.960 --> 00:55:19.400]   and just, and robotics people in general,
[00:55:19.400 --> 00:55:24.400]   but they have moments, at moments have worked harder
[00:55:24.400 --> 00:55:29.280]   than most people I know in terms of,
[00:55:29.280 --> 00:55:30.580]   if you look at different disciplines
[00:55:30.580 --> 00:55:32.360]   of how hard people work.
[00:55:32.360 --> 00:55:34.580]   But they're also like the happiest.
[00:55:34.580 --> 00:55:36.980]   Like, just like, I don't know.
[00:55:36.980 --> 00:55:39.200]   It's the same thing with like running,
[00:55:39.200 --> 00:55:41.380]   people that push themselves to like the limit,
[00:55:41.380 --> 00:55:45.420]   they also seem to be like the most like full of life somehow.
[00:55:45.420 --> 00:55:48.680]   And I get often criticized, like,
[00:55:48.680 --> 00:55:50.360]   "You're not getting enough sleep.
[00:55:50.360 --> 00:55:51.960]   "What are you doing to your body?"
[00:55:51.960 --> 00:55:54.680]   Blah, blah, blah, like this kind of stuff.
[00:55:54.680 --> 00:55:57.960]   And I usually just kind of respond like,
[00:55:57.960 --> 00:56:00.840]   "I'm doing what I love, I'm passionate about it.
[00:56:00.840 --> 00:56:04.840]   "I love it, I feel like it's invigorating."
[00:56:04.840 --> 00:56:07.640]   I actually think, I don't think the lack of sleep
[00:56:07.640 --> 00:56:08.880]   is what hurts you.
[00:56:08.880 --> 00:56:10.960]   I think what hurts you is stress
[00:56:10.960 --> 00:56:13.280]   and lack of doing things that you're passionate about.
[00:56:13.280 --> 00:56:16.280]   But in this world, yeah, I mean, can you comment about
[00:56:16.960 --> 00:56:21.280]   why the heck robotics people are
[00:56:21.280 --> 00:56:23.800]   (laughs)
[00:56:23.800 --> 00:56:26.200]   willing to push themselves to that degree?
[00:56:26.200 --> 00:56:27.680]   Is there value in that?
[00:56:27.680 --> 00:56:29.380]   And why are they so happy?
[00:56:29.380 --> 00:56:31.920]   - I think you got it right.
[00:56:31.920 --> 00:56:36.440]   I mean, I think the causality is not that we work hard.
[00:56:36.440 --> 00:56:38.520]   And I think other disciplines work very hard too.
[00:56:38.520 --> 00:56:40.300]   But I don't think it's that we work hard
[00:56:40.300 --> 00:56:43.160]   and therefore we are happy.
[00:56:43.160 --> 00:56:44.720]   I think we found something
[00:56:44.720 --> 00:56:46.600]   that we're truly passionate about.
[00:56:46.600 --> 00:56:50.000]   It makes us very happy.
[00:56:50.000 --> 00:56:52.280]   And then we get a little involved with it
[00:56:52.280 --> 00:56:54.600]   and spend a lot of time on it.
[00:56:54.600 --> 00:56:56.000]   What a luxury to have something
[00:56:56.000 --> 00:56:58.260]   that you wanna spend all your time on, right?
[00:56:58.260 --> 00:57:00.800]   - We could talk about this for many hours,
[00:57:00.800 --> 00:57:03.880]   but maybe if we could pick,
[00:57:03.880 --> 00:57:05.480]   is there something on the technical side
[00:57:05.480 --> 00:57:08.260]   on the approach that you took that's interesting
[00:57:08.260 --> 00:57:10.240]   that turned out to be a terrible failure
[00:57:10.240 --> 00:57:13.800]   or a success that you carry into your work today
[00:57:13.800 --> 00:57:17.280]   about all the different ideas that were involved
[00:57:17.280 --> 00:57:22.280]   in making, whether in the simulation or in the real world,
[00:57:22.280 --> 00:57:25.520]   making the semi-autonomous system work?
[00:57:25.520 --> 00:57:30.880]   - I mean, it really did teach me something fundamental
[00:57:30.880 --> 00:57:33.560]   about what it's gonna take to get robustness
[00:57:33.560 --> 00:57:35.320]   out of a system of this complexity.
[00:57:35.320 --> 00:57:37.740]   I would say the DARPA challenge
[00:57:37.740 --> 00:57:41.040]   really was foundational in my thinking.
[00:57:41.040 --> 00:57:43.720]   I think the autonomous driving community thinks about this.
[00:57:43.720 --> 00:57:46.640]   I think lots of people thinking about safety critical
[00:57:46.640 --> 00:57:48.920]   systems that might have machine learning in the loop
[00:57:48.920 --> 00:57:50.360]   are thinking about these questions.
[00:57:50.360 --> 00:57:54.240]   For me, the DARPA challenge was the moment where I realized,
[00:57:54.240 --> 00:57:58.920]   we've spent every waking minute running this robot.
[00:57:58.920 --> 00:58:01.440]   And again, for the physical competition,
[00:58:01.440 --> 00:58:02.540]   days before the competition,
[00:58:02.540 --> 00:58:04.440]   we saw the robot fall down in a way
[00:58:04.440 --> 00:58:05.980]   it had never fallen down before.
[00:58:05.980 --> 00:58:09.260]   I thought, how could we have found that?
[00:58:09.260 --> 00:58:13.600]   We only have one robot, it's running almost all the time.
[00:58:13.600 --> 00:58:15.560]   We just didn't have enough hours in the day
[00:58:15.560 --> 00:58:17.120]   to test that robot.
[00:58:17.120 --> 00:58:19.400]   Something has to change, right?
[00:58:19.400 --> 00:58:22.760]   And I think that, I mean, I would say that the team that won
[00:58:22.760 --> 00:58:28.040]   from KAIST was the team that had two robots
[00:58:28.040 --> 00:58:30.560]   and was able to do not only incredible engineering,
[00:58:30.560 --> 00:58:33.260]   just absolutely top-rate engineering,
[00:58:33.260 --> 00:58:36.120]   but also they were able to test at a rate
[00:58:36.120 --> 00:58:39.620]   and discipline that we didn't keep up with.
[00:58:39.620 --> 00:58:41.120]   - What does testing look like?
[00:58:41.120 --> 00:58:42.280]   What are we talking about here?
[00:58:42.280 --> 00:58:45.040]   Like what's a loop of tests?
[00:58:45.040 --> 00:58:48.760]   Like from start to finish, what is a loop of testing?
[00:58:48.760 --> 00:58:51.280]   - Yeah, I mean, I think there's a whole philosophy
[00:58:51.280 --> 00:58:52.120]   to testing.
[00:58:52.120 --> 00:58:54.400]   There's the unit tests, and you can do that on a hardware.
[00:58:54.400 --> 00:58:56.340]   You can do that in a small piece of code.
[00:58:56.340 --> 00:58:58.240]   You write one function, you should write a test
[00:58:58.240 --> 00:59:00.600]   that checks that function's input and outputs.
[00:59:00.600 --> 00:59:02.400]   You should also write an integration test
[00:59:02.400 --> 00:59:05.280]   at the other extreme of running the whole system together,
[00:59:05.280 --> 00:59:09.080]   where they try to turn on all of the different functions
[00:59:09.080 --> 00:59:11.560]   that you think are correct.
[00:59:11.560 --> 00:59:13.360]   It's much harder to write the specifications
[00:59:13.360 --> 00:59:14.480]   for a system-level test,
[00:59:14.480 --> 00:59:17.320]   especially if that system is as complicated
[00:59:17.320 --> 00:59:21.040]   as a humanoid robot, but the philosophy is sort of the same.
[00:59:21.040 --> 00:59:24.160]   On the real robot, it's no different,
[00:59:24.160 --> 00:59:26.040]   but on a real robot,
[00:59:26.040 --> 00:59:28.640]   it's impossible to run the same experiment twice.
[00:59:28.640 --> 00:59:32.480]   So if you see a failure,
[00:59:32.480 --> 00:59:34.400]   you hope you caught something in the logs
[00:59:34.400 --> 00:59:35.640]   that tell you what happened,
[00:59:35.640 --> 00:59:37.320]   but you'd probably never be able to run exactly
[00:59:37.320 --> 00:59:38.440]   that experiment again.
[00:59:39.400 --> 00:59:44.400]   And right now, I think our philosophy is just,
[00:59:44.400 --> 00:59:47.880]   basically, Monte Carlo estimation,
[00:59:47.880 --> 00:59:50.880]   is just run as many experiments as we can,
[00:59:50.880 --> 00:59:53.080]   maybe try to set up the environment
[00:59:53.080 --> 00:59:56.600]   to make the things we are worried about
[00:59:56.600 --> 00:59:59.880]   happen as often as possible,
[00:59:59.880 --> 01:00:02.280]   but really we're relying on somewhat random search
[01:00:02.280 --> 01:00:03.180]   in order to test.
[01:00:03.180 --> 01:00:05.480]   Maybe that's all we'll ever be able to,
[01:00:05.480 --> 01:00:08.160]   but I think, you know, 'cause there's an argument
[01:00:08.160 --> 01:00:10.520]   that the things that'll get you
[01:00:10.520 --> 01:00:14.040]   are the things that are really nuanced in the world,
[01:00:14.040 --> 01:00:15.700]   and it'd be very hard to, for instance,
[01:00:15.700 --> 01:00:16.880]   put back in a simulation.
[01:00:16.880 --> 01:00:19.860]   - Yeah, I guess the edge cases.
[01:00:19.860 --> 01:00:21.800]   What was the hardest thing?
[01:00:21.800 --> 01:00:24.680]   Like, so you said walking over rough terrain,
[01:00:24.680 --> 01:00:27.120]   like just taking footsteps.
[01:00:27.120 --> 01:00:31.360]   I mean, people, it's so dramatic and painful
[01:00:31.360 --> 01:00:33.520]   in a certain kind of way to watch these videos
[01:00:33.520 --> 01:00:37.600]   from the DRC of robots falling.
[01:00:37.600 --> 01:00:38.440]   - Yep.
[01:00:38.440 --> 01:00:39.440]   - It's just so heartbreaking.
[01:00:39.440 --> 01:00:40.280]   I don't know.
[01:00:40.280 --> 01:00:42.400]   Maybe it's because, for me at least,
[01:00:42.400 --> 01:00:45.140]   we anthropomorphize the robot.
[01:00:45.140 --> 01:00:48.400]   Of course, it's also funny for some reason.
[01:00:48.400 --> 01:00:50.260]   Like, humans falling is funny.
[01:00:50.260 --> 01:00:53.400]   It's some dark reason.
[01:00:53.400 --> 01:00:55.300]   I'm not sure why it is so,
[01:00:55.300 --> 01:00:57.880]   but it's also like tragic and painful.
[01:00:57.880 --> 01:01:00.160]   And so speaking of which,
[01:01:00.160 --> 01:01:05.000]   I mean, what made the robots fall and fail in your view?
[01:01:05.000 --> 01:01:07.160]   - So I can tell you exactly what happened.
[01:01:07.160 --> 01:01:08.360]   I contributed one of those.
[01:01:08.360 --> 01:01:10.960]   Our team contributed one of those spectacular falls.
[01:01:10.960 --> 01:01:15.560]   Every one of those falls has a complicated story.
[01:01:15.560 --> 01:01:16.920]   I mean, at one time,
[01:01:16.920 --> 01:01:19.160]   the power effectively went out on the robot
[01:01:19.160 --> 01:01:21.700]   because it had been sitting at the door
[01:01:21.700 --> 01:01:24.400]   waiting for a green light to be able to proceed
[01:01:24.400 --> 01:01:26.280]   and its batteries, you know,
[01:01:26.280 --> 01:01:28.060]   and therefore it just fell backwards
[01:01:28.060 --> 01:01:29.280]   and smashed its head to the ground.
[01:01:29.280 --> 01:01:30.120]   And it was hilarious,
[01:01:30.120 --> 01:01:32.760]   but it wasn't because of bad software, right?
[01:01:32.760 --> 01:01:37.120]   But for ours, so the hardest part of the challenge,
[01:01:37.120 --> 01:01:38.680]   the hardest task in my view
[01:01:38.680 --> 01:01:40.440]   was getting out of the Polaris.
[01:01:40.440 --> 01:01:43.760]   It was actually relatively easy to drive the Polaris.
[01:01:43.760 --> 01:01:44.600]   - Can you tell the story?
[01:01:44.600 --> 01:01:45.440]   Sorry to interrupt. - No, of course.
[01:01:45.440 --> 01:01:46.920]   - The story of the car.
[01:01:46.920 --> 01:01:51.200]   People should watch this video.
[01:01:51.200 --> 01:01:53.880]   I mean, the thing you've come up with is just brilliant,
[01:01:53.880 --> 01:01:55.120]   but anyway, sorry.
[01:01:55.120 --> 01:01:56.920]   - Yeah, we kind of joke,
[01:01:56.920 --> 01:01:59.040]   we call it the big robot little car problem
[01:01:59.040 --> 01:02:02.400]   because somehow the race organizers
[01:02:02.400 --> 01:02:05.360]   decided to give us a 400 pound humanoid
[01:02:05.360 --> 01:02:07.480]   and that they also provided the vehicle,
[01:02:07.480 --> 01:02:08.680]   which was a little Polaris.
[01:02:08.680 --> 01:02:11.800]   And the robot didn't really fit in the car.
[01:02:11.800 --> 01:02:14.560]   So you couldn't drive the car with your feet
[01:02:14.560 --> 01:02:15.720]   under the steering column.
[01:02:15.720 --> 01:02:19.560]   We actually had to straddle the main column
[01:02:19.560 --> 01:02:23.600]   of the, and have basically one foot in the passenger seat,
[01:02:23.600 --> 01:02:25.280]   one foot in the driver's seat
[01:02:25.280 --> 01:02:27.620]   and then drive with our left hand.
[01:02:27.620 --> 01:02:31.320]   But the hard part was we had to then park the car,
[01:02:31.320 --> 01:02:33.080]   get out of the car.
[01:02:33.080 --> 01:02:34.320]   It didn't have a door, that was okay,
[01:02:34.320 --> 01:02:38.760]   but it's just getting up from crouch, from sitting
[01:02:38.760 --> 01:02:41.920]   when you're in this very constrained environment.
[01:02:41.920 --> 01:02:44.360]   - First of all, I remember after watching those videos,
[01:02:44.360 --> 01:02:47.080]   I was much more cognizant of how hard is it,
[01:02:47.080 --> 01:02:49.640]   it is for me to get in and out of the car
[01:02:49.640 --> 01:02:51.280]   and out of the car especially.
[01:02:51.280 --> 01:02:54.280]   Like it's actually a really difficult control problem.
[01:02:54.280 --> 01:02:55.120]   - Yeah.
[01:02:55.120 --> 01:02:58.360]   - And I'm very cognizant of it when I'm like injured
[01:02:58.360 --> 01:03:00.120]   for whatever reason. - No, it's really hard.
[01:03:00.120 --> 01:03:01.440]   - Yeah.
[01:03:01.440 --> 01:03:03.560]   So how did you approach this problem?
[01:03:03.560 --> 01:03:08.160]   - So we had, you think of NASA's operations
[01:03:08.160 --> 01:03:10.680]   and they have these checklists, pre-launch checklists
[01:03:10.680 --> 01:03:12.380]   and they're like, we weren't far off from that.
[01:03:12.380 --> 01:03:14.720]   We had this big checklist and on the first day
[01:03:14.720 --> 01:03:17.520]   of the competition, we were running down our checklist.
[01:03:17.520 --> 01:03:19.120]   And one of the things we had to do,
[01:03:19.120 --> 01:03:21.320]   we had to turn off the controller,
[01:03:21.320 --> 01:03:23.320]   the piece of software that was running
[01:03:23.320 --> 01:03:25.560]   that would drive the left foot of the robot
[01:03:25.560 --> 01:03:28.120]   in order to accelerate on the gas.
[01:03:28.120 --> 01:03:30.840]   And then we turned on our balancing controller.
[01:03:30.840 --> 01:03:34.280]   And the nerves, jitters of the first day of the competition,
[01:03:34.280 --> 01:03:35.680]   someone forgot to check that box
[01:03:35.680 --> 01:03:37.560]   and turn that controller off.
[01:03:37.560 --> 01:03:42.280]   So we used a lot of motion planning to figure out
[01:03:42.280 --> 01:03:45.320]   a sort of configuration of the robot
[01:03:45.320 --> 01:03:47.200]   that we could get up and over.
[01:03:47.200 --> 01:03:50.300]   We relied heavily on our balancing controller.
[01:03:50.300 --> 01:03:53.760]   And basically there were, when the robot was in one
[01:03:53.760 --> 01:03:57.560]   of its most precarious sort of configurations,
[01:03:57.560 --> 01:04:00.920]   trying to sneak its big leg out of the side,
[01:04:00.920 --> 01:04:05.040]   the other controller that thought it was still driving
[01:04:05.040 --> 01:04:06.880]   told its left foot to go like this.
[01:04:06.880 --> 01:04:11.880]   And that wasn't good, but it turned disastrous for us
[01:04:11.880 --> 01:04:17.000]   because what happened was a little bit of push here.
[01:04:17.000 --> 01:04:21.100]   Actually, we have videos of us running into the robot
[01:04:21.100 --> 01:04:24.720]   with a 10 foot pole and it kind of will recover.
[01:04:24.720 --> 01:04:27.840]   But this is a case where there's no space to recover.
[01:04:27.840 --> 01:04:30.200]   So a lot of our secondary balancing mechanisms
[01:04:30.200 --> 01:04:32.200]   about like take a step to recover,
[01:04:32.200 --> 01:04:33.800]   they were all disabled because we were in the car
[01:04:33.800 --> 01:04:35.360]   and there's no place to step.
[01:04:35.360 --> 01:04:38.400]   So we were relying on our just lowest level reflexes.
[01:04:38.400 --> 01:04:42.200]   And even then, I think just hitting the foot on the seat,
[01:04:42.200 --> 01:04:45.000]   on the floor, we probably could have recovered from it.
[01:04:45.000 --> 01:04:46.440]   But the thing that was bad that happened
[01:04:46.440 --> 01:04:49.480]   is when we did that and we jostled a little bit,
[01:04:49.480 --> 01:04:53.760]   the tailbone of our robot was only a little off the seat,
[01:04:53.760 --> 01:04:54.600]   it hit the seat.
[01:04:54.600 --> 01:04:58.280]   And the other foot came off the ground just a little bit.
[01:04:58.280 --> 01:05:02.320]   And nothing in our plans had ever told us what to do
[01:05:02.320 --> 01:05:05.160]   if your butt's on the seat and your feet are in the air.
[01:05:05.160 --> 01:05:06.080]   - Feet in the air.
[01:05:06.080 --> 01:05:10.000]   - And then the thing is, once you get off the script,
[01:05:10.000 --> 01:05:12.800]   things can go very wrong because even our state estimation,
[01:05:12.800 --> 01:05:15.240]   our system that was trying to collect all the data
[01:05:15.240 --> 01:05:16.800]   from the sensors and understand
[01:05:16.800 --> 01:05:18.520]   what's happening with the robot,
[01:05:18.520 --> 01:05:20.120]   it didn't know about this situation.
[01:05:20.120 --> 01:05:22.840]   So it was predicting things that were just wrong.
[01:05:22.840 --> 01:05:26.560]   And then we did a violent shake and fell off
[01:05:26.560 --> 01:05:29.200]   in our face first out of the robot.
[01:05:29.200 --> 01:05:32.560]   - But like into the destination.
[01:05:32.560 --> 01:05:35.480]   - That's true, we fell in and we got our point for egress.
[01:05:35.480 --> 01:05:39.320]   - But so is there any hope for, that's interesting,
[01:05:39.320 --> 01:05:43.320]   is there any hope for Atlas to be able to do something
[01:05:43.320 --> 01:05:46.360]   when it's just on its butt and feet in the air?
[01:05:46.360 --> 01:05:47.200]   - Absolutely.
[01:05:47.200 --> 01:05:50.960]   No, so that is one of the big challenges.
[01:05:50.960 --> 01:05:53.880]   And I think it's still true, you know,
[01:05:53.880 --> 01:05:58.880]   Boston Dynamics and Andy Mow and there's this incredible
[01:05:58.880 --> 01:06:02.040]   work on legged robots happening around the world.
[01:06:02.040 --> 01:06:07.680]   Most of them still are very good at the case
[01:06:07.680 --> 01:06:10.160]   where you're making contact with the world at your feet.
[01:06:10.160 --> 01:06:12.240]   And they have typically point feet relatively,
[01:06:12.240 --> 01:06:14.560]   their balls on their feet, for instance.
[01:06:14.560 --> 01:06:17.920]   If those robots get in a situation where the elbow
[01:06:17.920 --> 01:06:19.960]   hits the wall or something like this,
[01:06:19.960 --> 01:06:21.320]   that's a pretty different situation.
[01:06:21.320 --> 01:06:24.200]   Now they have layers of mechanisms that will make,
[01:06:24.200 --> 01:06:28.680]   I think the more mature solutions have ways in which
[01:06:28.680 --> 01:06:31.320]   the controller won't do stupid things.
[01:06:31.320 --> 01:06:34.840]   But a human for instance, is able to leverage
[01:06:34.840 --> 01:06:36.840]   incidental contact in order to accomplish a goal.
[01:06:36.840 --> 01:06:38.920]   In fact, I might, if you push me, I might actually
[01:06:38.920 --> 01:06:42.320]   put my hand out and make a new brand new contact.
[01:06:42.320 --> 01:06:45.040]   The feet of the robot are doing this on quadrupeds,
[01:06:45.040 --> 01:06:49.200]   but we mostly in robotics are afraid of contact
[01:06:49.200 --> 01:06:52.100]   on the rest of our body, which is crazy.
[01:06:52.100 --> 01:06:56.120]   There's this whole field of motion planning,
[01:06:56.120 --> 01:06:58.120]   collision free motion planning.
[01:06:58.120 --> 01:07:00.760]   And we write very complex algorithms so that the robot
[01:07:00.760 --> 01:07:04.120]   can dance around and make sure it doesn't touch the world.
[01:07:04.120 --> 01:07:07.800]   - So people are just afraid of contact
[01:07:07.800 --> 01:07:09.920]   'cause contact is seen as a difficult.
[01:07:09.920 --> 01:07:12.280]   - It's still a difficult control problem
[01:07:12.280 --> 01:07:13.420]   and sensing problem.
[01:07:13.420 --> 01:07:16.560]   - Now you're a serious person.
[01:07:17.080 --> 01:07:19.880]   (laughs)
[01:07:19.880 --> 01:07:21.200]   - I'm a little bit of an idiot
[01:07:21.200 --> 01:07:24.160]   and I'm going to ask you some dumb questions.
[01:07:24.160 --> 01:07:27.160]   So I do martial arts.
[01:07:27.160 --> 01:07:30.400]   So like jujitsu, I wrestled my whole life.
[01:07:30.400 --> 01:07:33.280]   So let me ask the question,
[01:07:33.280 --> 01:07:36.920]   whenever people learn that I do any kind of AI
[01:07:36.920 --> 01:07:39.760]   or like I mentioned robots and things like that,
[01:07:39.760 --> 01:07:41.920]   they say when are we gonna have robots
[01:07:41.920 --> 01:07:46.840]   that can win in a wrestling match
[01:07:46.840 --> 01:07:49.040]   or in a fight against a human?
[01:07:49.040 --> 01:07:52.200]   So we just mentioned sitting on your butt,
[01:07:52.200 --> 01:07:53.960]   if you're in the air, that's a common position,
[01:07:53.960 --> 01:07:55.440]   jujitsu when you're on the ground,
[01:07:55.440 --> 01:07:57.640]   you're a down opponent.
[01:07:57.640 --> 01:08:03.840]   Like how difficult do you think is the problem
[01:08:03.840 --> 01:08:06.920]   and when will we have a robot that can defeat a human
[01:08:06.920 --> 01:08:08.640]   in a wrestling match?
[01:08:08.640 --> 01:08:09.960]   And we're talking about a lot,
[01:08:10.920 --> 01:08:12.480]   I don't know if you're familiar with wrestling,
[01:08:12.480 --> 01:08:14.040]   but essentially--
[01:08:14.040 --> 01:08:16.240]   - Not very.
[01:08:16.240 --> 01:08:19.680]   - It's basically the art of contact.
[01:08:19.680 --> 01:08:24.680]   It's like, 'cause you're picking contact points
[01:08:24.680 --> 01:08:27.040]   and then using like leverage,
[01:08:27.040 --> 01:08:31.440]   like to off balance, to trick people.
[01:08:31.440 --> 01:08:35.660]   It's like you make them feel like you're doing one thing
[01:08:35.660 --> 01:08:38.880]   and then they change their balance
[01:08:38.880 --> 01:08:41.640]   and then you switch what you're doing
[01:08:41.640 --> 01:08:44.080]   and then results in a throw or whatever.
[01:08:44.080 --> 01:08:47.880]   So it's basically the art of multiple contacts.
[01:08:47.880 --> 01:08:50.840]   - Awesome, that's a nice description of it.
[01:08:50.840 --> 01:08:53.040]   So there's also an opponent in there, right?
[01:08:53.040 --> 01:08:54.160]   So if--
[01:08:54.160 --> 01:08:55.040]   - Very dynamic.
[01:08:55.040 --> 01:08:58.520]   - Right, if you are wrestling a human
[01:08:58.520 --> 01:09:02.880]   and are in a game theoretic situation with a human,
[01:09:02.880 --> 01:09:04.200]   that's still hard.
[01:09:06.600 --> 01:09:09.160]   But just to speak to the quickly reasoning
[01:09:09.160 --> 01:09:11.360]   about contact part of it, for instance.
[01:09:11.360 --> 01:09:13.400]   - Yeah, maybe even throwing the game theory out of it,
[01:09:13.400 --> 01:09:17.700]   almost like a, yeah, almost like a non-dynamic opponent.
[01:09:17.700 --> 01:09:18.640]   - Right.
[01:09:18.640 --> 01:09:20.080]   There's reasons to be optimistic,
[01:09:20.080 --> 01:09:22.660]   but I think our best understanding of those problems
[01:09:22.660 --> 01:09:23.940]   are still pretty hard.
[01:09:23.940 --> 01:09:29.800]   I have been increasingly focused on manipulation,
[01:09:29.800 --> 01:09:31.720]   partly where that's a case where the contact
[01:09:31.720 --> 01:09:33.160]   has to be much more rich.
[01:09:35.800 --> 01:09:38.240]   And there are some really impressive examples
[01:09:38.240 --> 01:09:41.800]   of deep learning policies, controllers,
[01:09:41.800 --> 01:09:46.800]   that can appear to do good things through contact.
[01:09:46.800 --> 01:09:52.480]   We've even got new examples of deep learning models
[01:09:52.480 --> 01:09:54.360]   of predicting what's gonna happen to objects
[01:09:54.360 --> 01:09:56.200]   as they go through contact.
[01:09:56.200 --> 01:09:59.760]   But I think the challenge you just offered there
[01:09:59.760 --> 01:10:01.480]   still eludes us, right?
[01:10:01.480 --> 01:10:03.600]   The ability to make a decision
[01:10:03.600 --> 01:10:05.280]   based on those models quickly.
[01:10:05.280 --> 01:10:10.080]   I have to think though, it's hard for humans too,
[01:10:10.080 --> 01:10:11.320]   when you get that complicated.
[01:10:11.320 --> 01:10:16.080]   I think probably you had maybe a slow motion version
[01:10:16.080 --> 01:10:17.920]   of where you learn the basic skills,
[01:10:17.920 --> 01:10:20.660]   and you've probably gotten better at it,
[01:10:20.660 --> 01:10:24.620]   and there's much more subtlety.
[01:10:24.620 --> 01:10:26.820]   But it might still be hard to actually,
[01:10:26.820 --> 01:10:32.120]   really on the fly, take a model of your humanoid
[01:10:32.120 --> 01:10:35.240]   and figure out how to plan the optimal sequence.
[01:10:35.240 --> 01:10:36.680]   That might be a problem we never solve.
[01:10:36.680 --> 01:10:40.360]   - Well, I mean, one of the most amazing things to me
[01:10:40.360 --> 01:10:43.720]   about the, we could talk about martial arts,
[01:10:43.720 --> 01:10:45.360]   we could also talk about dancing,
[01:10:45.360 --> 01:10:47.680]   doesn't really matter, too human.
[01:10:47.680 --> 01:10:51.200]   I think it's the most interesting study of contact.
[01:10:51.200 --> 01:10:53.200]   It's not even the dynamic element of it.
[01:10:53.200 --> 01:10:58.760]   Like when you get good at it, it's so effortless.
[01:10:58.760 --> 01:11:00.880]   Like I can just, I'm very cognizant
[01:11:00.880 --> 01:11:03.400]   of the entirety of the learning process
[01:11:03.400 --> 01:11:07.640]   being essentially like learning how to move my body
[01:11:07.640 --> 01:11:12.640]   in a way that I could throw very large weights around
[01:11:12.640 --> 01:11:14.700]   effortlessly.
[01:11:14.700 --> 01:11:18.500]   And I can feel the learning.
[01:11:18.500 --> 01:11:21.560]   Like I'm a huge believer in drilling of techniques.
[01:11:21.560 --> 01:11:23.560]   And you can just like feel your,
[01:11:23.560 --> 01:11:26.760]   you're not feeling, you're feeling, sorry,
[01:11:26.760 --> 01:11:29.760]   you're learning it intellectually a little bit,
[01:11:29.760 --> 01:11:32.800]   but a lot of it is the body learning it somehow,
[01:11:32.800 --> 01:11:33.880]   like instinctually.
[01:11:33.880 --> 01:11:37.160]   And whatever that learning is, that's really,
[01:11:37.160 --> 01:11:40.760]   I'm not even sure if that's equivalent
[01:11:40.760 --> 01:11:44.720]   to like a deep learning, learning a controller.
[01:11:44.720 --> 01:11:46.780]   I think it's something more,
[01:11:46.780 --> 01:11:49.680]   it feels like there's a lot of distributed learning
[01:11:49.680 --> 01:11:50.520]   going on.
[01:11:50.520 --> 01:11:54.480]   - Yeah, I think there's hierarchy and composition
[01:11:54.480 --> 01:11:57.960]   probably in the systems
[01:11:57.960 --> 01:11:59.900]   that we don't capture very well yet.
[01:11:59.900 --> 01:12:02.440]   You have layers of control systems,
[01:12:02.440 --> 01:12:03.960]   you have reflexes at the bottom layer,
[01:12:03.960 --> 01:12:08.960]   and you have a system that's capable of planning a vacation
[01:12:08.960 --> 01:12:11.320]   to some distant country,
[01:12:11.320 --> 01:12:12.560]   which is probably,
[01:12:12.560 --> 01:12:14.840]   you probably don't have a controller or a policy
[01:12:14.840 --> 01:12:18.740]   for every possible destination you'll ever pick, right?
[01:12:18.740 --> 01:12:23.480]   But there's something magical in the in-between.
[01:12:23.480 --> 01:12:26.400]   And how do you go from these low level feedback loops
[01:12:26.400 --> 01:12:30.040]   to something that feels like a pretty complex
[01:12:30.040 --> 01:12:31.080]   set of outcomes?
[01:12:31.080 --> 01:12:34.760]   My guess is, I think there's evidence
[01:12:34.760 --> 01:12:37.640]   that you can plan at some of these levels, right?
[01:12:37.640 --> 01:12:41.760]   So Josh Tenenbaum just showed it in his talk the other day.
[01:12:41.760 --> 01:12:43.360]   He's got a game he likes to talk about,
[01:12:43.360 --> 01:12:46.720]   I think he calls it the pick three game or something,
[01:12:46.720 --> 01:12:50.760]   where he puts a bunch of clutter down in front of a person,
[01:12:50.760 --> 01:12:52.400]   and he says, "Okay, pick three objects."
[01:12:52.400 --> 01:12:55.760]   And it might be a telephone or a shoe
[01:12:55.760 --> 01:12:58.980]   or a Kleenex box or whatever.
[01:12:58.980 --> 01:13:01.880]   And apparently you pick three items and then you pick,
[01:13:01.880 --> 01:13:04.120]   he says, "Okay, pick the first one up with your right hand,
[01:13:04.120 --> 01:13:06.400]   the second one up with your left hand.
[01:13:06.400 --> 01:13:08.040]   Now using those objects,
[01:13:08.040 --> 01:13:10.160]   now as tools, pick up the third object."
[01:13:10.160 --> 01:13:16.120]   Right, so that's down at the level of physics and mechanics
[01:13:16.120 --> 01:13:20.520]   and contact mechanics that I think we do learning,
[01:13:20.520 --> 01:13:21.920]   or we do have policies for,
[01:13:21.920 --> 01:13:24.760]   we do control for, almost feedback.
[01:13:24.760 --> 01:13:26.320]   But somehow we're able to still,
[01:13:26.320 --> 01:13:28.440]   I mean, I've never picked up a telephone
[01:13:28.440 --> 01:13:30.280]   with a shoe and a water bottle before,
[01:13:30.280 --> 01:13:32.480]   and somehow, and it takes me a little longer
[01:13:32.480 --> 01:13:34.520]   to do that the first time,
[01:13:34.520 --> 01:13:37.300]   but most of the time we can sort of figure that out.
[01:13:37.300 --> 01:13:41.240]   So, yeah, I think the amazing thing
[01:13:41.240 --> 01:13:44.120]   is this ability to be flexible with our models,
[01:13:44.120 --> 01:13:46.160]   plan when we need to,
[01:13:46.160 --> 01:13:49.320]   use our well-oiled controllers when we don't,
[01:13:49.320 --> 01:13:51.820]   when we're in familiar territory.
[01:13:51.820 --> 01:13:55.560]   Having models, I think the other thing you just said
[01:13:55.560 --> 01:13:57.120]   was something about,
[01:13:57.120 --> 01:13:58.800]   I think your awareness of what's happening
[01:13:58.800 --> 01:14:02.360]   is even changing as you improve your expertise, right?
[01:14:02.360 --> 01:14:04.960]   So maybe you have a very approximate model
[01:14:04.960 --> 01:14:06.240]   of the mechanics to begin with,
[01:14:06.240 --> 01:14:09.320]   and as you gain expertise,
[01:14:09.320 --> 01:14:11.920]   you get a more refined version of that model.
[01:14:11.920 --> 01:14:16.920]   You're aware of muscles or balanced components
[01:14:17.080 --> 01:14:19.680]   that you just weren't even aware of before.
[01:14:19.680 --> 01:14:21.760]   So how do you scaffold that?
[01:14:21.760 --> 01:14:24.200]   - Yeah, plus the fear of injury,
[01:14:24.200 --> 01:14:28.800]   the ambition of goals, of excelling,
[01:14:28.800 --> 01:14:32.040]   and fear of mortality.
[01:14:32.040 --> 01:14:34.620]   Let's see what else is in there as motivations.
[01:14:34.620 --> 01:14:38.040]   Overinflated ego in the beginning,
[01:14:38.040 --> 01:14:42.880]   and then a crash of confidence in the middle.
[01:14:42.880 --> 01:14:46.720]   All of those seem to be essential for the learning process.
[01:14:46.720 --> 01:14:48.160]   And if all that's good,
[01:14:48.160 --> 01:14:50.520]   then you're probably optimizing energy efficiency.
[01:14:50.520 --> 01:14:53.120]   - Yeah, right, so we have to get that right.
[01:14:53.120 --> 01:14:58.120]   So there was this idea that you would have robots play soccer
[01:14:58.120 --> 01:15:03.840]   better than human players by 2050.
[01:15:03.840 --> 01:15:05.720]   That was the goal.
[01:15:05.720 --> 01:15:10.200]   Basically, was the goal to beat world champion team,
[01:15:10.200 --> 01:15:11.400]   to become a World Cup,
[01:15:11.400 --> 01:15:13.400]   beat like a World Cup level team?
[01:15:13.400 --> 01:15:15.920]   So are we gonna see that first,
[01:15:15.920 --> 01:15:19.600]   or a robot, if you're familiar,
[01:15:19.600 --> 01:15:23.480]   there's an organization called UFC for mixed martial arts.
[01:15:23.480 --> 01:15:27.160]   Are we gonna see a World Cup championship soccer team
[01:15:27.160 --> 01:15:28.480]   that have robots,
[01:15:28.480 --> 01:15:33.480]   or a UFC champion mixed martial artist that's a robot?
[01:15:33.480 --> 01:15:37.200]   - I mean, it's very hard to say one thing is harder,
[01:15:37.200 --> 01:15:38.640]   some problem's harder than the other.
[01:15:38.640 --> 01:15:43.640]   What probably matters is who started the organization.
[01:15:45.040 --> 01:15:47.160]   I mean, I think RoboCup has a pretty serious following,
[01:15:47.160 --> 01:15:50.880]   and there is a history now of people playing that game,
[01:15:50.880 --> 01:15:51.840]   learning about that game,
[01:15:51.840 --> 01:15:53.640]   building robots to play that game,
[01:15:53.640 --> 01:15:55.880]   building increasingly more human robots.
[01:15:55.880 --> 01:15:57.040]   It's got momentum.
[01:15:57.040 --> 01:16:00.960]   So if you want to have mixed martial arts compete,
[01:16:00.960 --> 01:16:04.000]   you better start your organization now, right?
[01:16:04.000 --> 01:16:07.720]   I think almost independent of which problem
[01:16:07.720 --> 01:16:08.680]   is technically harder,
[01:16:08.680 --> 01:16:11.400]   'cause they're both hard and they're both different.
[01:16:11.400 --> 01:16:12.240]   - That's a good point.
[01:16:12.240 --> 01:16:14.680]   I mean, those videos are just hilarious.
[01:16:14.680 --> 01:16:18.840]   Like, especially the humanoid robots trying to play soccer.
[01:16:18.840 --> 01:16:23.400]   I mean, they're kind of terrible right now.
[01:16:23.400 --> 01:16:26.000]   - I mean, I guess there is RoboSumo wrestling.
[01:16:26.000 --> 01:16:27.880]   There's like the RoboOne competitions
[01:16:27.880 --> 01:16:31.160]   where they do have these robots that go on a table
[01:16:31.160 --> 01:16:32.080]   and basically fight.
[01:16:32.080 --> 01:16:33.720]   So maybe I'm wrong.
[01:16:33.720 --> 01:16:37.160]   - First of all, do you have a year in mind for RoboCup,
[01:16:37.160 --> 01:16:39.080]   just from a robotics perspective?
[01:16:39.080 --> 01:16:42.160]   Seems like a super exciting possibility.
[01:16:42.160 --> 01:16:46.360]   - That, like in the physical space,
[01:16:46.360 --> 01:16:47.640]   this is what's interesting.
[01:16:47.640 --> 01:16:50.600]   I think the world is captivated.
[01:16:50.600 --> 01:16:52.800]   I think it's really exciting.
[01:16:52.800 --> 01:16:56.440]   It inspires just a huge number of people
[01:16:56.440 --> 01:17:00.840]   when a machine beats a human
[01:17:00.840 --> 01:17:03.520]   at a game that humans are really damn good at.
[01:17:03.520 --> 01:17:05.800]   So you're talking about chess and Go,
[01:17:05.800 --> 01:17:09.880]   but that's in the world of digital.
[01:17:09.880 --> 01:17:13.360]   I don't think machines have beat humans
[01:17:13.360 --> 01:17:16.080]   at a game in the physical space yet,
[01:17:16.080 --> 01:17:17.760]   but that would be just--
[01:17:17.760 --> 01:17:20.400]   - You have to make the rules very carefully, right?
[01:17:20.400 --> 01:17:23.040]   I mean, if Atlas kicked me in the shins, I'm down,
[01:17:23.040 --> 01:17:25.520]   and game over.
[01:17:25.520 --> 01:17:30.520]   So it's very subtle on what's fair.
[01:17:30.520 --> 01:17:33.320]   - I think the fighting one is a weird one, yeah,
[01:17:33.320 --> 01:17:35.240]   'cause you're talking about a machine
[01:17:35.240 --> 01:17:36.560]   that's much stronger than you.
[01:17:36.560 --> 01:17:39.200]   But yeah, in terms of soccer, basketball,
[01:17:39.200 --> 01:17:40.440]   all those kinds. - Even soccer, right?
[01:17:40.440 --> 01:17:43.480]   I mean, as soon as there's contact or whatever,
[01:17:43.480 --> 01:17:46.520]   and there are some things that the robot will do better.
[01:17:46.520 --> 01:17:51.520]   I think if you really set yourself up to try to see
[01:17:51.520 --> 01:17:53.120]   could robots win the game of soccer
[01:17:53.120 --> 01:17:54.560]   as the rules were written,
[01:17:54.560 --> 01:17:56.920]   the right thing for the robot to do
[01:17:56.920 --> 01:17:59.640]   is to play very differently than a human would play.
[01:17:59.640 --> 01:18:04.000]   You're not gonna get the perfect soccer player robot.
[01:18:04.000 --> 01:18:07.880]   You're gonna get something that exploits the rules,
[01:18:07.880 --> 01:18:10.720]   exploits its super actuators,
[01:18:10.720 --> 01:18:14.440]   its super low bandwidth feedback loops or whatever,
[01:18:14.440 --> 01:18:15.720]   and it's gonna play the game differently
[01:18:15.720 --> 01:18:16.960]   than you want it to play.
[01:18:16.960 --> 01:18:21.360]   And I bet there's ways, I bet there's loopholes, right?
[01:18:21.360 --> 01:18:24.120]   We saw that in the DARPA challenge,
[01:18:24.120 --> 01:18:28.200]   that it's very hard to write a set of rules
[01:18:28.200 --> 01:18:32.840]   that someone can't find a way to exploit.
[01:18:32.840 --> 01:18:35.600]   - Let me ask another ridiculous question.
[01:18:35.600 --> 01:18:38.640]   I think this might be the last ridiculous question, but--
[01:18:38.640 --> 01:18:39.480]   - I doubt it.
[01:18:39.480 --> 01:18:41.080]   (laughing)
[01:18:41.080 --> 01:18:44.560]   - I aspire to ask as many ridiculous questions
[01:18:44.560 --> 01:18:48.040]   of a brilliant MIT professor.
[01:18:48.040 --> 01:18:52.440]   Okay, I don't know if you've seen The Black Mirror.
[01:18:52.440 --> 01:18:56.720]   - It's funny, I never watched the episode.
[01:18:56.720 --> 01:18:58.840]   I know when it happened, though,
[01:18:58.840 --> 01:19:03.080]   because I gave a talk to some MIT faculty one day,
[01:19:03.080 --> 01:19:05.720]   on an unassuming Monday or whatever,
[01:19:05.720 --> 01:19:08.480]   I was telling them about the state of robotics.
[01:19:08.480 --> 01:19:10.760]   And I showed some video from Boston Dynamics
[01:19:10.760 --> 01:19:13.960]   of the quadruped spot at the time.
[01:19:13.960 --> 01:19:15.920]   It was their early version of spot.
[01:19:15.920 --> 01:19:19.280]   And there was a look of horror that went across the room.
[01:19:19.280 --> 01:19:23.160]   And I said, "I've shown videos like this a lot of times.
[01:19:23.160 --> 01:19:24.000]   "What happened?"
[01:19:24.000 --> 01:19:26.800]   And it turns out that this video had,
[01:19:26.800 --> 01:19:29.800]   this Black Mirror episode had changed the way people watched
[01:19:31.920 --> 01:19:33.120]   the videos I was putting out.
[01:19:33.120 --> 01:19:34.720]   - The way they see these kinds of robots.
[01:19:34.720 --> 01:19:37.760]   So I talked to so many people who are just terrified
[01:19:37.760 --> 01:19:40.960]   because of that episode probably of these kinds of robots.
[01:19:40.960 --> 01:19:44.520]   I almost want to say that they almost enjoy being terrified.
[01:19:44.520 --> 01:19:47.100]   I don't even know what it is about human psychology
[01:19:47.100 --> 01:19:49.240]   that kind of imagine doomsday,
[01:19:49.240 --> 01:19:52.760]   the destruction of the universe or our society,
[01:19:52.760 --> 01:19:57.320]   and kind of enjoy being afraid.
[01:19:57.320 --> 01:19:58.360]   I don't want to simplify it,
[01:19:58.360 --> 01:20:01.000]   but it feels like they talk about it so often,
[01:20:01.000 --> 01:20:06.000]   it almost, there does seem to be an addictive quality to it.
[01:20:06.000 --> 01:20:09.440]   I talked to a guy, a guy named Joe Rogan,
[01:20:09.440 --> 01:20:11.520]   who's kind of the flag bearer
[01:20:11.520 --> 01:20:13.620]   for being terrified of these robots.
[01:20:13.620 --> 01:20:17.280]   Do you have a, two questions.
[01:20:17.280 --> 01:20:18.560]   One, do you have an understanding
[01:20:18.560 --> 01:20:21.640]   of why people are afraid of robots?
[01:20:21.640 --> 01:20:24.880]   And the second question is, in Black Mirror,
[01:20:24.880 --> 01:20:26.320]   just to tell you the episode,
[01:20:26.320 --> 01:20:28.100]   I don't even remember it that much anymore,
[01:20:28.100 --> 01:20:31.020]   but these robots, I think they can shoot
[01:20:31.020 --> 01:20:32.740]   like a pellet or something.
[01:20:32.740 --> 01:20:36.460]   They basically have, it's basically a spot with a gun.
[01:20:36.460 --> 01:20:41.460]   And how far are we away from having robots
[01:20:41.460 --> 01:20:43.780]   that go rogue like that?
[01:20:43.780 --> 01:20:48.380]   You know, basically spot that goes rogue for some reason
[01:20:48.380 --> 01:20:49.940]   and somehow finds a gun.
[01:20:49.940 --> 01:20:56.260]   - Right, so, I mean, I'm not a psychologist.
[01:20:56.380 --> 01:21:00.740]   I think, I don't know exactly why people react
[01:21:00.740 --> 01:21:01.640]   the way they do.
[01:21:01.640 --> 01:21:06.980]   I think we have to be careful
[01:21:06.980 --> 01:21:09.860]   about the way robots influence our society and the like.
[01:21:09.860 --> 01:21:11.700]   I think that's something, that's a responsibility
[01:21:11.700 --> 01:21:14.000]   that roboticists need to embrace.
[01:21:14.000 --> 01:21:17.380]   I don't think robots are gonna come after me
[01:21:17.380 --> 01:21:20.340]   with a kitchen knife or a pellet gun right away.
[01:21:20.340 --> 01:21:23.180]   And I mean, if they were programmed in such a way,
[01:21:23.180 --> 01:21:25.340]   but I used to joke with Atlas
[01:21:25.340 --> 01:21:28.700]   that all I had to do was run for five minutes
[01:21:28.700 --> 01:21:30.180]   and its battery would run out.
[01:21:30.180 --> 01:21:32.460]   But actually they've got a very big battery
[01:21:32.460 --> 01:21:33.300]   in there by the end.
[01:21:33.300 --> 01:21:34.460]   So it was over an hour.
[01:21:34.460 --> 01:21:39.380]   I think the fear is a bit cultural though.
[01:21:39.380 --> 01:21:42.740]   'Cause I mean, you notice that,
[01:21:42.740 --> 01:21:45.920]   like I think in my age in the US,
[01:21:45.920 --> 01:21:48.220]   we grew up watching Terminator, right?
[01:21:48.220 --> 01:21:50.460]   If I had grown up at the same time in Japan,
[01:21:50.460 --> 01:21:52.720]   I probably would have been watching Astro Boy.
[01:21:52.720 --> 01:21:55.840]   And there's a very different reaction to robots
[01:21:55.840 --> 01:21:57.440]   in different countries, right?
[01:21:57.440 --> 01:22:00.000]   So I don't know if it's a human innate fear
[01:22:00.000 --> 01:22:02.600]   of metal marvels,
[01:22:02.600 --> 01:22:06.400]   or if it's something that we've done to ourselves
[01:22:06.400 --> 01:22:07.420]   with our sci-fi.
[01:22:07.420 --> 01:22:12.560]   - Yeah, the stories we tell ourselves through movies,
[01:22:12.560 --> 01:22:16.760]   through just, through popular media.
[01:22:16.760 --> 01:22:19.560]   But if I were to tell,
[01:22:19.560 --> 01:22:21.520]   if you were my therapist and I said,
[01:22:21.520 --> 01:22:26.280]   "I'm really terrified that we're going to have these robots
[01:22:26.280 --> 01:22:29.380]   very soon that will hurt us."
[01:22:29.380 --> 01:22:35.560]   Like, how do you approach making me feel better?
[01:22:35.560 --> 01:22:39.600]   Like, why shouldn't people be afraid?
[01:22:39.600 --> 01:22:44.480]   There's a, I think there's a video that went viral recently.
[01:22:44.480 --> 01:22:46.720]   Everything was spot in Boston,
[01:22:46.720 --> 01:22:48.360]   then it goes viral in general.
[01:22:48.360 --> 01:22:50.040]   But usually it's like really cool stuff.
[01:22:50.040 --> 01:22:51.400]   Like they're doing flips and stuff,
[01:22:51.400 --> 01:22:52.720]   or like sad stuff,
[01:22:52.720 --> 01:22:57.280]   Atlas being hit with a broomstick or something like that.
[01:22:57.280 --> 01:23:00.800]   But there's a video where I think
[01:23:00.800 --> 01:23:03.560]   one of the new productions bought robots,
[01:23:03.560 --> 01:23:04.600]   which are awesome.
[01:23:04.600 --> 01:23:08.520]   It was like patrolling somewhere in some country.
[01:23:08.520 --> 01:23:11.880]   And people immediately were saying,
[01:23:11.880 --> 01:23:16.320]   this is the dystopian future, the surveillance state.
[01:23:16.320 --> 01:23:19.440]   For some reason, you can just have a camera.
[01:23:19.440 --> 01:23:23.400]   Something about spot, being able to walk on four feet
[01:23:23.400 --> 01:23:25.920]   with like really terrified people.
[01:23:25.920 --> 01:23:30.920]   So what do you say to those people?
[01:23:30.920 --> 01:23:33.800]   I think there is a legitimate fear there
[01:23:33.800 --> 01:23:36.140]   because so much of our future is uncertain.
[01:23:36.140 --> 01:23:40.080]   But at the same time, technically speaking,
[01:23:40.080 --> 01:23:41.880]   it seems like we're not there yet.
[01:23:41.880 --> 01:23:42.840]   So what do you say?
[01:23:42.840 --> 01:23:48.560]   - I mean, I think technology is complicated.
[01:23:48.560 --> 01:23:49.920]   It can be used in many ways.
[01:23:49.920 --> 01:23:53.160]   I think there are purely software attacks
[01:23:53.160 --> 01:23:59.000]   somebody could use to do great damage.
[01:23:59.000 --> 01:24:00.480]   Maybe they have already.
[01:24:00.480 --> 01:24:06.480]   I think wheeled robots could be used in bad ways too.
[01:24:06.480 --> 01:24:10.520]   - Drones. - Drones, right?
[01:24:10.520 --> 01:24:16.360]   I don't think that, let's see.
[01:24:16.360 --> 01:24:19.880]   I don't want to be building technology
[01:24:19.880 --> 01:24:21.840]   just because I'm compelled to build technology
[01:24:21.840 --> 01:24:23.520]   and I don't think about it.
[01:24:23.520 --> 01:24:27.720]   But I would consider myself a technological optimist,
[01:24:27.720 --> 01:24:32.200]   I guess, in the sense that I think we should continue
[01:24:32.200 --> 01:24:37.200]   to create and evolve and our world will change.
[01:24:37.200 --> 01:24:40.760]   And if we will introduce new challenges,
[01:24:40.760 --> 01:24:42.880]   we'll screw something up maybe.
[01:24:42.880 --> 01:24:46.200]   But I think also we'll invent ourselves
[01:24:46.200 --> 01:24:49.360]   out of those challenges and life will go on.
[01:24:49.360 --> 01:24:51.880]   - So it's interesting 'cause you didn't mention
[01:24:51.880 --> 01:24:54.520]   this is technically too hard.
[01:24:54.520 --> 01:24:55.880]   - I don't think robots are,
[01:24:55.880 --> 01:24:59.120]   I think people attribute a robot that looks like an animal
[01:24:59.120 --> 01:25:02.120]   as maybe having a level of self-awareness
[01:25:02.120 --> 01:25:05.200]   or consciousness or something that they don't have yet.
[01:25:05.200 --> 01:25:11.720]   I think our ability to anthropomorphize those robots
[01:25:11.720 --> 01:25:14.920]   is probably, we're assuming that they have
[01:25:14.920 --> 01:25:17.960]   a level of intelligence that they don't yet have
[01:25:17.960 --> 01:25:20.060]   and that might be part of the fear.
[01:25:20.060 --> 01:25:22.280]   So in that sense, it's too hard.
[01:25:22.280 --> 01:25:25.760]   But there are many scary things in the world, right?
[01:25:25.760 --> 01:25:29.880]   So I think we're right to ask those questions,
[01:25:29.880 --> 01:25:33.600]   we're right to think about the implications of our work.
[01:25:33.600 --> 01:25:39.720]   - Right, in the short term as we're working on it, for sure.
[01:25:39.720 --> 01:25:43.880]   Is there something long-term that scares you
[01:25:43.880 --> 01:25:47.680]   about our future with AI and robots?
[01:25:47.680 --> 01:25:52.400]   A lot of folks from Elon Musk to Sam Harris
[01:25:52.400 --> 01:25:56.860]   to a lot of folks talk about the existential threats
[01:25:56.860 --> 01:25:58.880]   about artificial intelligence.
[01:25:58.880 --> 01:26:03.680]   Oftentimes robots kind of inspire that the most
[01:26:03.680 --> 01:26:05.840]   because of the anthropomorphism.
[01:26:05.840 --> 01:26:07.400]   Do you have any fears?
[01:26:07.400 --> 01:26:09.020]   - It's an important question.
[01:26:09.900 --> 01:26:14.900]   I actually, I think I like Rod Brooks answer
[01:26:14.900 --> 01:26:16.620]   maybe the best on this.
[01:26:16.620 --> 01:26:18.900]   I think, and it's not the only answer he's given
[01:26:18.900 --> 01:26:21.300]   over the years, but maybe one of my favorites is,
[01:26:21.300 --> 01:26:25.420]   he says, it's not gonna be,
[01:26:25.420 --> 01:26:27.820]   he's got a book, "Flesh and Machines," I believe.
[01:26:27.820 --> 01:26:31.900]   It's not gonna be the robots versus the people.
[01:26:31.900 --> 01:26:34.260]   We're all gonna be robot people.
[01:26:34.260 --> 01:26:37.980]   Because we already have smartphones,
[01:26:37.980 --> 01:26:40.700]   some of us have serious technology
[01:26:40.700 --> 01:26:41.940]   implanted in our bodies already,
[01:26:41.940 --> 01:26:44.940]   whether we have a hearing aid or a pacemaker
[01:26:44.940 --> 01:26:46.360]   or anything like this.
[01:26:46.360 --> 01:26:50.880]   People with amputations might have prosthetics.
[01:26:50.880 --> 01:26:57.300]   That's a trend I think that is likely to continue.
[01:26:57.300 --> 01:27:01.380]   I mean, this is now wild speculation.
[01:27:01.380 --> 01:27:05.460]   But I mean, when do we get to cognitive implants
[01:27:05.460 --> 01:27:06.580]   and the like?
[01:27:06.580 --> 01:27:09.460]   Yeah, with neural link, brain-computer interfaces.
[01:27:09.460 --> 01:27:10.300]   That's interesting.
[01:27:10.300 --> 01:27:12.580]   So there's a dance between humans and robots.
[01:27:12.580 --> 01:27:13.920]   It's going to be,
[01:27:13.920 --> 01:27:18.900]   it's going to be impossible to be scared
[01:27:18.900 --> 01:27:23.380]   of the other out there, the robot,
[01:27:23.380 --> 01:27:26.020]   because the robot will be part of us, essentially.
[01:27:26.020 --> 01:27:29.860]   It'd be so intricately sort of part of our society.
[01:27:29.860 --> 01:27:33.040]   - Yeah, and it might not even be implanted part of us,
[01:27:33.040 --> 01:27:37.220]   but just it's so much a part of our society.
[01:27:37.220 --> 01:27:39.380]   - So in that sense, the smartphone is already the robot
[01:27:39.380 --> 01:27:40.820]   we should be afraid of, yeah.
[01:27:40.820 --> 01:27:44.720]   I mean, yeah, and all the usual fears arise
[01:27:44.720 --> 01:27:50.440]   of the misinformation, the manipulation,
[01:27:50.440 --> 01:27:53.520]   all those kinds of things that,
[01:27:53.520 --> 01:27:57.880]   the problems are all the same.
[01:27:57.880 --> 01:28:00.700]   They're human problems, essentially, it feels like.
[01:28:00.700 --> 01:28:03.420]   - Yeah, I mean, I think the way we interact
[01:28:03.420 --> 01:28:06.740]   with each other online is changing the value we put on
[01:28:06.740 --> 01:28:10.460]   personal interaction, and that's a crazy big change
[01:28:10.460 --> 01:28:13.080]   that's going to happen and has already been ripping
[01:28:13.080 --> 01:28:14.180]   through our society, right?
[01:28:14.180 --> 01:28:18.100]   And that has implications that are massive.
[01:28:18.100 --> 01:28:19.300]   I don't know if they should be scared of it
[01:28:19.300 --> 01:28:23.700]   or go with the flow, but I don't see
[01:28:23.700 --> 01:28:26.540]   some battle lines between humans and robots
[01:28:26.540 --> 01:28:29.620]   being the first thing to worry about.
[01:28:29.620 --> 01:28:33.380]   - I mean, I do want to just, as a kind of comment,
[01:28:33.380 --> 01:28:35.500]   maybe you can comment about your just feelings
[01:28:35.500 --> 01:28:37.740]   about Boston Dynamics in general,
[01:28:37.740 --> 01:28:40.340]   but I love science, I love engineering.
[01:28:40.340 --> 01:28:42.580]   I think there's so many beautiful ideas in it.
[01:28:42.580 --> 01:28:45.340]   And when I look at Boston Dynamics
[01:28:45.340 --> 01:28:50.340]   or legged robots in general, I think they inspire people
[01:28:50.340 --> 01:28:56.500]   curiosity and feelings in general, excitement
[01:28:56.500 --> 01:28:58.980]   about engineering more than almost anything else
[01:28:58.980 --> 01:29:00.660]   in popular culture.
[01:29:00.660 --> 01:29:04.820]   And I think that's such an exciting responsibility
[01:29:04.820 --> 01:29:06.860]   and possibility for robotics.
[01:29:06.860 --> 01:29:10.500]   And Boston Dynamics is riding that wave pretty damn well.
[01:29:10.500 --> 01:29:14.020]   Like they found it, they've discovered that hunger
[01:29:14.020 --> 01:29:17.580]   and curiosity in the people, and they're doing magic with it.
[01:29:17.580 --> 01:29:19.860]   I don't care if they, I mean, I guess it's their company,
[01:29:19.860 --> 01:29:21.380]   they have to make money, right?
[01:29:21.380 --> 01:29:24.340]   But they're already doing incredible work
[01:29:24.340 --> 01:29:26.980]   in inspiring the world about technology.
[01:29:26.980 --> 01:29:30.740]   I mean, do you have thoughts about Boston Dynamics
[01:29:30.740 --> 01:29:34.660]   and maybe others, your own work in robotics
[01:29:34.660 --> 01:29:36.620]   and inspiring the world in that way?
[01:29:36.620 --> 01:29:39.020]   - I completely agree.
[01:29:39.020 --> 01:29:42.660]   I think Boston Dynamics is absolutely awesome.
[01:29:42.660 --> 01:29:46.140]   I think I show my kids those videos,
[01:29:46.140 --> 01:29:48.620]   and the best thing that happens is sometimes
[01:29:48.620 --> 01:29:50.740]   they've already seen them, right?
[01:29:50.740 --> 01:29:55.380]   I think, I just think it's a pinnacle of success
[01:29:55.380 --> 01:29:58.780]   in robotics that is just one of the best things
[01:29:58.780 --> 01:29:59.620]   that's happened.
[01:29:59.620 --> 01:30:01.660]   I absolutely, completely agree.
[01:30:01.660 --> 01:30:05.180]   - One of the heartbreaking things to me
[01:30:05.180 --> 01:30:09.580]   is how many robotics companies fail.
[01:30:09.580 --> 01:30:13.060]   How hard it is to make money with the robotics company.
[01:30:13.060 --> 01:30:18.060]   Like iRobot like went through hell just to arrive at Arumba
[01:30:18.060 --> 01:30:19.820]   to figure out one product.
[01:30:19.820 --> 01:30:23.860]   And then there's so many home robotics companies
[01:30:23.860 --> 01:30:28.860]   like Jibo and Anki, the cutest toy.
[01:30:28.860 --> 01:30:35.260]   There's a great robot I thought went down.
[01:30:35.260 --> 01:30:36.300]   I'm forgetting a bunch of them,
[01:30:36.300 --> 01:30:37.980]   but a bunch of robotics companies fail.
[01:30:37.980 --> 01:30:40.580]   Rod's company, Rethink Robotics.
[01:30:40.580 --> 01:30:47.220]   Like, do you have anything hopeful to say
[01:30:47.220 --> 01:30:50.300]   about the possibility of making money with robots?
[01:30:50.300 --> 01:30:53.940]   - Oh, I think you can't just look at the failures.
[01:30:53.940 --> 01:30:55.940]   You can, I mean, Boston Dynamics is a success.
[01:30:55.940 --> 01:30:57.580]   There's lots of companies that are still
[01:30:57.580 --> 01:31:01.140]   doing amazingly good work in robotics.
[01:31:01.140 --> 01:31:05.380]   I mean, this is the capitalist ecology or something, right?
[01:31:05.380 --> 01:31:07.700]   I think you have many companies, you have many startups,
[01:31:07.700 --> 01:31:11.380]   and they push each other forward, and many of them fail,
[01:31:11.380 --> 01:31:12.500]   and some of them get through,
[01:31:12.500 --> 01:31:15.580]   and that's sort of the natural--
[01:31:15.580 --> 01:31:16.420]   - Way of things.
[01:31:16.420 --> 01:31:17.260]   - Way of those things.
[01:31:17.260 --> 01:31:20.460]   I don't know that, is robotics really that much worse?
[01:31:20.460 --> 01:31:22.300]   I feel the pain that you feel too.
[01:31:22.300 --> 01:31:26.460]   Every time I read one of these, sometimes it's friends,
[01:31:26.460 --> 01:31:31.460]   and I definitely wish it went better, went differently.
[01:31:31.460 --> 01:31:38.340]   But I think it's healthy and good to have bursts of ideas,
[01:31:38.340 --> 01:31:40.660]   bursts of activities, ideas.
[01:31:40.660 --> 01:31:43.500]   If they are really aggressive, they should fail sometimes.
[01:31:43.500 --> 01:31:46.940]   Certainly that's the research mantra, right?
[01:31:46.940 --> 01:31:50.780]   If you're succeeding at every problem you attempt,
[01:31:50.780 --> 01:31:53.420]   then you're not choosing aggressively enough.
[01:31:53.420 --> 01:31:56.020]   - Is it exciting to you, the new Spot?
[01:31:56.020 --> 01:31:57.660]   - Oh, it's so good.
[01:31:57.660 --> 01:32:00.220]   - When are you getting him as a pet, or it?
[01:32:00.220 --> 01:32:03.300]   - Yeah, I mean, I have to dig up 75K right now.
[01:32:03.300 --> 01:32:04.140]   (laughing)
[01:32:04.140 --> 01:32:05.740]   - I mean, it's so cool that there's a price tag.
[01:32:05.740 --> 01:32:08.660]   You can go and then actually buy it.
[01:32:08.660 --> 01:32:11.540]   - I have a Skydio R1, love it.
[01:32:11.540 --> 01:32:16.540]   So, no, I would absolutely be a customer.
[01:32:17.540 --> 01:32:20.100]   - I wonder what your kids would think about it.
[01:32:20.100 --> 01:32:24.420]   - I actually, Zach from Boston Dynamics
[01:32:24.420 --> 01:32:27.180]   would let my kid drive in one of their demos one time,
[01:32:27.180 --> 01:32:31.140]   and that was just so good, so good.
[01:32:31.140 --> 01:32:34.220]   So, I'll forever be grateful for that.
[01:32:34.220 --> 01:32:35.540]   - And there's something magical
[01:32:35.540 --> 01:32:38.940]   about the anthropomorphization of that arm.
[01:32:38.940 --> 01:32:42.620]   It adds another level of human connection.
[01:32:42.620 --> 01:32:47.500]   I'm not sure we understand from a control aspect
[01:32:47.500 --> 01:32:49.540]   the value of anthropomorphization.
[01:32:49.540 --> 01:32:54.020]   I think that's an understudied
[01:32:54.020 --> 01:32:57.060]   and underunderstood engineering problem.
[01:32:57.060 --> 01:33:00.180]   There's been a, psychologists have been studying it.
[01:33:00.180 --> 01:33:02.900]   I think it's part, like, manipulating our mind
[01:33:02.900 --> 01:33:06.780]   to believe things is a valuable engineering.
[01:33:06.780 --> 01:33:08.860]   Like, this is another degree of freedom
[01:33:08.860 --> 01:33:09.860]   that can be controlled.
[01:33:09.860 --> 01:33:11.420]   - I like that, yeah, I think that's right.
[01:33:11.420 --> 01:33:15.980]   I think there's something that humans seem to do,
[01:33:15.980 --> 01:33:19.020]   or maybe my dangerous introspection is,
[01:33:19.020 --> 01:33:23.860]   I think we are able to make very simple models
[01:33:23.860 --> 01:33:27.820]   that assume a lot about the world very quickly,
[01:33:27.820 --> 01:33:31.260]   and then it takes us a lot more time, like your wrestling.
[01:33:31.260 --> 01:33:32.660]   You probably thought you knew
[01:33:32.660 --> 01:33:33.740]   what you were doing with wrestling,
[01:33:33.740 --> 01:33:36.940]   and you were fairly functional as a complete wrestler,
[01:33:36.940 --> 01:33:39.380]   and then you slowly got more expertise.
[01:33:39.380 --> 01:33:44.380]   So maybe it's natural that our first level of defense
[01:33:44.380 --> 01:33:48.140]   against seeing a new robot is to think of it
[01:33:48.140 --> 01:33:52.500]   in our existing models of how humans and animals behave.
[01:33:52.500 --> 01:33:55.140]   And it's just, as you spend more time with it,
[01:33:55.140 --> 01:33:57.100]   then you'll develop more sophisticated models
[01:33:57.100 --> 01:33:59.540]   that will appreciate the differences.
[01:33:59.540 --> 01:34:01.700]   - Exactly.
[01:34:01.700 --> 01:34:04.460]   Can you say what does it take to control a robot?
[01:34:04.460 --> 01:34:08.620]   Like, what is the control problem of a robot?
[01:34:08.620 --> 01:34:11.020]   And in general, what is a robot in your view?
[01:34:11.020 --> 01:34:13.940]   Like, how do you think of this system?
[01:34:13.940 --> 01:34:16.060]   - What is a robot?
[01:34:16.060 --> 01:34:17.620]   - What is a robot?
[01:34:17.620 --> 01:34:18.700]   - I think robotics-- - I told you
[01:34:18.700 --> 01:34:20.060]   ridiculous questions.
[01:34:20.060 --> 01:34:21.540]   - No, no, it's good.
[01:34:21.540 --> 01:34:23.020]   I mean, there's standard definitions
[01:34:23.020 --> 01:34:27.500]   of combining computation with some ability
[01:34:27.500 --> 01:34:29.100]   to do mechanical work.
[01:34:29.100 --> 01:34:31.020]   I think that gets us pretty close.
[01:34:31.020 --> 01:34:34.220]   But I think robotics has this problem
[01:34:34.220 --> 01:34:37.220]   that once things really work,
[01:34:37.220 --> 01:34:38.940]   we don't call them robots anymore.
[01:34:38.940 --> 01:34:42.940]   Like, my dishwasher at home is pretty sophisticated,
[01:34:42.940 --> 01:34:45.620]   beautiful mechanisms.
[01:34:45.620 --> 01:34:46.940]   There's actually a pretty good computer,
[01:34:46.940 --> 01:34:49.580]   probably a couple of chips in there doing amazing things.
[01:34:49.580 --> 01:34:51.620]   We don't think of that as a robot anymore,
[01:34:51.620 --> 01:34:53.300]   which isn't fair, 'cause then,
[01:34:53.300 --> 01:34:56.220]   roughly it means that robotics always has to
[01:34:56.220 --> 01:34:58.340]   solve the next problem
[01:34:58.340 --> 01:35:00.580]   and doesn't get to celebrate its past successes.
[01:35:00.580 --> 01:35:04.740]   I mean, even factory room floor robots
[01:35:04.740 --> 01:35:06.860]   are super successful.
[01:35:06.860 --> 01:35:08.260]   They're amazing.
[01:35:08.260 --> 01:35:09.500]   But that's not the ones,
[01:35:09.500 --> 01:35:10.860]   I mean, people think of them as robots,
[01:35:10.860 --> 01:35:13.300]   but they don't, if you ask what are the successes
[01:35:13.300 --> 01:35:16.140]   of robotics, somehow it doesn't come
[01:35:16.140 --> 01:35:17.860]   to your mind immediately.
[01:35:17.860 --> 01:35:20.540]   - So the definition of robot is a system
[01:35:20.540 --> 01:35:23.500]   with some level of automation that fails frequently.
[01:35:23.500 --> 01:35:25.940]   - Something like, it's the computation
[01:35:25.940 --> 01:35:29.940]   plus mechanical work and an unsolved problem.
[01:35:29.940 --> 01:35:30.780]   (laughing)
[01:35:30.780 --> 01:35:32.260]   - Unsolved problem, yeah.
[01:35:32.260 --> 01:35:35.380]   So from a perspective of control
[01:35:35.380 --> 01:35:39.780]   and mechanics, dynamics, what is a robot?
[01:35:39.780 --> 01:35:42.340]   - So there are many different types of robots.
[01:35:42.340 --> 01:35:47.340]   The control that you need for a Jibo robot,
[01:35:47.340 --> 01:35:50.540]   some robot that's sitting on your countertop
[01:35:50.540 --> 01:35:52.380]   and interacting with you,
[01:35:52.380 --> 01:35:54.660]   but not touching you, for instance,
[01:35:54.660 --> 01:35:56.820]   is very different than what you need for an autonomous car
[01:35:56.820 --> 01:35:59.420]   or an autonomous drone.
[01:35:59.420 --> 01:36:00.980]   It's very different than what you need for a robot
[01:36:00.980 --> 01:36:03.900]   that's gonna walk or pick things up with its hands.
[01:36:04.700 --> 01:36:09.100]   My passion has always been for the places
[01:36:09.100 --> 01:36:10.500]   where you're interacting more,
[01:36:10.500 --> 01:36:13.660]   you're doing more dynamic interactions with the world.
[01:36:13.660 --> 01:36:17.700]   So walking, now manipulation.
[01:36:17.700 --> 01:36:21.660]   And the control problems there are beautiful.
[01:36:21.660 --> 01:36:25.900]   I think contact is one thing that differentiates them
[01:36:25.900 --> 01:36:29.180]   from many of the control problems we've solved classically.
[01:36:29.180 --> 01:36:32.740]   Right, like modern control grew up stabilizing fighter jets
[01:36:32.740 --> 01:36:34.020]   that were passively unstable
[01:36:34.020 --> 01:36:36.380]   and there's like amazing success stories
[01:36:36.380 --> 01:36:38.060]   from control all over the place.
[01:36:38.060 --> 01:36:41.300]   Power grid, I mean, there's all kinds of,
[01:36:41.300 --> 01:36:44.620]   it's everywhere that we don't even realize,
[01:36:44.620 --> 01:36:47.500]   just like AI is now.
[01:36:47.500 --> 01:36:50.660]   - So you mentioned contact, like what's contact?
[01:36:50.660 --> 01:36:54.940]   - So an airplane is an extremely complex system
[01:36:54.940 --> 01:36:57.340]   or a spacecraft landing or whatever,
[01:36:57.340 --> 01:36:59.300]   but at least it has the luxury
[01:36:59.300 --> 01:37:03.620]   of things change relatively continuously.
[01:37:03.620 --> 01:37:04.900]   That's an oversimplification.
[01:37:04.900 --> 01:37:07.020]   But if I make a small change
[01:37:07.020 --> 01:37:10.100]   in the command I send to my actuator,
[01:37:10.100 --> 01:37:12.660]   then the path that the robot will take
[01:37:12.660 --> 01:37:15.820]   tends to change only by a small amount.
[01:37:15.820 --> 01:37:18.860]   - And there's a feedback mechanism here.
[01:37:18.860 --> 01:37:19.700]   That's what we're talking about.
[01:37:19.700 --> 01:37:20.940]   - And there's a feedback mechanism.
[01:37:20.940 --> 01:37:23.780]   And thinking about this as locally,
[01:37:23.780 --> 01:37:25.780]   like a linear system, for instance,
[01:37:25.780 --> 01:37:29.180]   I can use more linear algebra tools
[01:37:29.180 --> 01:37:31.300]   to study systems like that,
[01:37:31.300 --> 01:37:33.020]   generalizations of linear algebra
[01:37:33.020 --> 01:37:35.500]   to these smooth systems.
[01:37:35.500 --> 01:37:37.300]   What is contact?
[01:37:37.300 --> 01:37:41.500]   A robot has something very discontinuous
[01:37:41.500 --> 01:37:43.580]   that happens when it makes or breaks,
[01:37:43.580 --> 01:37:45.380]   when it starts touching the world.
[01:37:45.380 --> 01:37:48.060]   And even the way it touches or the order of contacts
[01:37:48.060 --> 01:37:53.060]   can change the outcome in potentially unpredictable ways.
[01:37:53.060 --> 01:37:55.860]   Not unpredictable, but complex ways.
[01:37:55.860 --> 01:37:58.860]   I do think there's a little bit of a,
[01:38:01.420 --> 01:38:04.540]   a lot of people will say that contact is hard in robotics,
[01:38:04.540 --> 01:38:05.580]   even to simulate.
[01:38:05.580 --> 01:38:08.660]   And I think there's a little bit of a,
[01:38:08.660 --> 01:38:09.580]   there's truth to that,
[01:38:09.580 --> 01:38:11.980]   but maybe a misunderstanding around that.
[01:38:11.980 --> 01:38:18.500]   So what is limiting is that when we think about our robots
[01:38:18.500 --> 01:38:21.340]   and we write our simulators,
[01:38:21.340 --> 01:38:24.420]   we often make an assumption that objects are rigid.
[01:38:24.420 --> 01:38:28.100]   And when it comes down,
[01:38:28.100 --> 01:38:30.180]   you know, that their mass moves all,
[01:38:30.180 --> 01:38:32.140]   you know, it stays in a constant position
[01:38:32.140 --> 01:38:33.740]   relative to each other itself.
[01:38:33.740 --> 01:38:39.260]   And that leads to some paradoxes
[01:38:39.260 --> 01:38:40.500]   when you go to try to talk about
[01:38:40.500 --> 01:38:43.140]   rigid body mechanics and contact.
[01:38:43.140 --> 01:38:45.460]   And so for instance,
[01:38:45.460 --> 01:38:49.500]   if I have a three-legged stool with just a,
[01:38:49.500 --> 01:38:51.900]   imagine it comes to a point at the legs.
[01:38:51.900 --> 01:38:54.420]   So it's only touching the world at a point.
[01:38:54.420 --> 01:38:56.940]   If I draw my physics,
[01:38:56.940 --> 01:39:00.340]   my high school physics diagram of this system,
[01:39:00.340 --> 01:39:02.340]   then there's a couple of things that I'm given
[01:39:02.340 --> 01:39:03.860]   by elementary physics.
[01:39:03.860 --> 01:39:06.380]   I know if the system, if the table is at rest,
[01:39:06.380 --> 01:39:08.540]   if it's not moving, it's zero velocities.
[01:39:08.540 --> 01:39:11.180]   That means that the normal force,
[01:39:11.180 --> 01:39:13.340]   all the forces are in balance.
[01:39:13.340 --> 01:39:16.460]   So the force of gravity is being countered
[01:39:16.460 --> 01:39:20.140]   by the forces that the ground is pushing on my table legs.
[01:39:20.140 --> 01:39:23.980]   I also know since it's not rotating
[01:39:23.980 --> 01:39:25.900]   that the moments have to balance.
[01:39:25.900 --> 01:39:29.620]   And since it can, it's a three-dimensional table,
[01:39:29.620 --> 01:39:31.220]   it could fall in any direction.
[01:39:31.220 --> 01:39:33.140]   It actually tells me uniquely
[01:39:33.140 --> 01:39:35.460]   what those three normal forces have to be.
[01:39:35.460 --> 01:39:40.780]   If I have four legs on my table, four-legged table,
[01:39:40.780 --> 01:39:43.380]   and they were perfectly machined
[01:39:43.380 --> 01:39:45.420]   to be exactly the right, same height,
[01:39:45.420 --> 01:39:48.140]   and they're set down and the table's not moving,
[01:39:48.140 --> 01:39:52.060]   then the basic conservation laws don't tell me
[01:39:52.060 --> 01:39:54.140]   there are many solutions for the forces
[01:39:54.140 --> 01:39:56.700]   that the ground could be putting on my legs
[01:39:56.700 --> 01:39:59.100]   that would still result in the table not moving.
[01:39:59.100 --> 01:40:02.340]   Now, the reason, that seems fine.
[01:40:02.340 --> 01:40:03.980]   I could just pick one.
[01:40:03.980 --> 01:40:06.880]   But it gets funny now because if you think about friction,
[01:40:06.880 --> 01:40:12.100]   what we think about with friction is our standard model says
[01:40:12.100 --> 01:40:15.940]   the amount of force that the table will push back,
[01:40:15.940 --> 01:40:18.100]   if I were to now try to push my table sideways,
[01:40:18.100 --> 01:40:19.460]   I guess I have a table here,
[01:40:19.460 --> 01:40:23.060]   is proportional to the normal force.
[01:40:24.060 --> 01:40:27.220]   So if I'm barely touching and I push, I'll slide,
[01:40:27.220 --> 01:40:30.500]   but if I'm pushing more and I push, I'll slide less.
[01:40:30.500 --> 01:40:33.780]   It's called Coulomb friction, is our standard model.
[01:40:33.780 --> 01:40:35.580]   Now, if you don't know what the normal force is
[01:40:35.580 --> 01:40:38.880]   on the four legs and you push the table,
[01:40:38.880 --> 01:40:42.440]   then you don't know what the friction forces are gonna be.
[01:40:42.440 --> 01:40:45.620]   And so you can't actually tell,
[01:40:45.620 --> 01:40:48.020]   the laws just aren't explicit yet
[01:40:48.020 --> 01:40:49.740]   about which way the table's gonna go.
[01:40:49.740 --> 01:40:51.420]   It could veer off to the left,
[01:40:51.420 --> 01:40:52.740]   it could veer off to the right,
[01:40:52.740 --> 01:40:53.780]   it could go straight.
[01:40:53.780 --> 01:40:58.500]   So the rigid body assumption of contact
[01:40:58.500 --> 01:40:59.900]   leaves us with some paradoxes,
[01:40:59.900 --> 01:41:02.900]   which are annoying for writing simulators
[01:41:02.900 --> 01:41:04.300]   and for writing controllers.
[01:41:04.300 --> 01:41:09.500]   We still do that sometimes because soft contact
[01:41:09.500 --> 01:41:13.220]   is potentially harder numerically or whatever,
[01:41:13.220 --> 01:41:14.720]   and the best simulators do both
[01:41:14.720 --> 01:41:17.060]   or do some combination of the two.
[01:41:17.060 --> 01:41:19.380]   But anyways, because of these kinds of paradoxes,
[01:41:19.380 --> 01:41:21.920]   there's all kinds of paradoxes in contact.
[01:41:22.680 --> 01:41:25.400]   Mostly due to these rigid body assumptions.
[01:41:25.400 --> 01:41:29.760]   It becomes very hard to write the same kind of control laws
[01:41:29.760 --> 01:41:31.520]   that we've been able to be successful with
[01:41:31.520 --> 01:41:33.900]   for like fighter jets.
[01:41:33.900 --> 01:41:36.400]   We haven't been as successful writing those controllers
[01:41:36.400 --> 01:41:37.920]   for manipulation.
[01:41:37.920 --> 01:41:39.680]   - And so you don't know what's going to happen
[01:41:39.680 --> 01:41:41.960]   at the point of contact, at the moment of contact.
[01:41:41.960 --> 01:41:43.400]   - There are situations absolutely
[01:41:43.400 --> 01:41:46.240]   where our laws don't tell us.
[01:41:46.240 --> 01:41:47.920]   So the standard approach, that's okay.
[01:41:47.920 --> 01:41:51.640]   I mean, instead of having a differential equation,
[01:41:51.640 --> 01:41:54.140]   you end up with a differential inclusion, it's called.
[01:41:54.140 --> 01:41:56.560]   It's a set valued equation.
[01:41:56.560 --> 01:41:58.800]   It says that I'm in this configuration,
[01:41:58.800 --> 01:42:00.520]   I have these forces applied on me,
[01:42:00.520 --> 01:42:03.960]   and there's a set of things that could happen.
[01:42:03.960 --> 01:42:07.400]   And you can-- - And those aren't continuous,
[01:42:07.400 --> 01:42:12.020]   I mean, what, so when you say non-smooth,
[01:42:12.020 --> 01:42:16.200]   they're not only not smooth, but this is discontinuous?
[01:42:16.200 --> 01:42:18.480]   - The non-smooth comes in when I make
[01:42:18.480 --> 01:42:20.400]   or break a new contact first,
[01:42:20.400 --> 01:42:22.880]   or when I transition from stick to slip.
[01:42:22.880 --> 01:42:25.280]   So you typically have static friction,
[01:42:25.280 --> 01:42:26.440]   and then you'll start sliding,
[01:42:26.440 --> 01:42:28.440]   and that'll be a discontinuous change
[01:42:28.440 --> 01:42:31.360]   in velocity, for instance,
[01:42:31.360 --> 01:42:33.360]   especially if you come to rest or--
[01:42:33.360 --> 01:42:34.480]   - That's so fascinating.
[01:42:34.480 --> 01:42:37.720]   Okay, so what do you do?
[01:42:37.720 --> 01:42:39.760]   Sorry, I interrupted you. - That's fine.
[01:42:39.760 --> 01:42:44.160]   - What's the hope under so much uncertainty
[01:42:44.160 --> 01:42:45.440]   about what's going to happen?
[01:42:45.440 --> 01:42:46.280]   What are you supposed to do?
[01:42:46.280 --> 01:42:48.520]   - I mean, control has an answer for this.
[01:42:48.520 --> 01:42:50.240]   Robust control is one approach,
[01:42:50.240 --> 01:42:52.640]   but roughly, you can write controllers
[01:42:52.640 --> 01:42:55.920]   which try to still perform the right task
[01:42:55.920 --> 01:42:58.120]   despite all the things that could possibly happen.
[01:42:58.120 --> 01:43:00.000]   The world might want the table to go this way and this way,
[01:43:00.000 --> 01:43:01.560]   but if I write a controller
[01:43:01.560 --> 01:43:04.320]   that pushes a little bit more and pushes a little bit,
[01:43:04.320 --> 01:43:08.000]   I can certainly make the table go in the direction I want.
[01:43:08.000 --> 01:43:10.000]   It just puts a little bit more of a burden
[01:43:10.000 --> 01:43:12.160]   on the control system, right?
[01:43:12.160 --> 01:43:15.440]   And these discontinuities do change the control system
[01:43:15.440 --> 01:43:19.840]   because the way we write it down right now,
[01:43:19.840 --> 01:43:24.320]   every different control configuration,
[01:43:24.320 --> 01:43:26.200]   including sticking or sliding
[01:43:26.200 --> 01:43:29.200]   or parts of my body that are in contact or not,
[01:43:29.200 --> 01:43:30.840]   looks like a different system.
[01:43:30.840 --> 01:43:31.880]   And I think of them,
[01:43:31.880 --> 01:43:34.680]   I reason about them separately or differently,
[01:43:34.680 --> 01:43:38.000]   and the combinatorics of that blow up, right?
[01:43:38.000 --> 01:43:41.440]   So I just don't have enough time to compute
[01:43:41.440 --> 01:43:45.040]   all the possible contact configurations of my humanoid.
[01:43:45.040 --> 01:43:49.040]   Interestingly, I mean, I'm a humanoid.
[01:43:49.040 --> 01:43:51.600]   I have lots of degrees of freedom, lots of joints.
[01:43:51.600 --> 01:43:54.960]   I've only been around for a handful of years.
[01:43:54.960 --> 01:43:55.800]   It's getting up there,
[01:43:55.800 --> 01:43:59.200]   but I haven't had time in my life
[01:43:59.200 --> 01:44:02.060]   to visit all of the states in my system,
[01:44:02.060 --> 01:44:05.240]   certainly all the contact configurations.
[01:44:05.240 --> 01:44:08.320]   So if step one is to consider
[01:44:08.320 --> 01:44:11.360]   every possible contact configuration that I'll ever be in,
[01:44:12.160 --> 01:44:16.040]   that's probably not a problem I need to solve, right?
[01:44:16.040 --> 01:44:18.360]   - Just as a small tangent,
[01:44:18.360 --> 01:44:20.360]   what's a contact configuration?
[01:44:20.360 --> 01:44:26.240]   Just so we can enumerate, what are we talking about?
[01:44:26.240 --> 01:44:27.560]   How many are there?
[01:44:27.560 --> 01:44:29.960]   - The simplest example maybe would be,
[01:44:29.960 --> 01:44:32.680]   imagine a robot with a flat foot.
[01:44:32.680 --> 01:44:35.400]   And we think about the phases of gait
[01:44:35.400 --> 01:44:39.960]   where the heel strikes and then the front toe strikes,
[01:44:39.960 --> 01:44:42.420]   and then you can heel up, toe off.
[01:44:42.420 --> 01:44:46.680]   Those are each different contact configurations.
[01:44:46.680 --> 01:44:48.280]   I only had two different contacts,
[01:44:48.280 --> 01:44:51.380]   but I ended up with four different contact configurations.
[01:44:51.380 --> 01:44:56.380]   Now, of course, my robot might actually have bumps on it
[01:44:56.380 --> 01:45:00.600]   or other things, so it could be much more subtle than that.
[01:45:00.600 --> 01:45:03.120]   But it's just even with one sort of box
[01:45:03.120 --> 01:45:06.200]   interacting with the ground already in the plane
[01:45:06.200 --> 01:45:07.040]   has that many, right?
[01:45:07.040 --> 01:45:09.380]   And if I was just even a 3D foot,
[01:45:09.380 --> 01:45:11.220]   then it probably my left toe might touch
[01:45:11.220 --> 01:45:14.320]   just before my right toe and things get subtle.
[01:45:14.320 --> 01:45:16.440]   Now, if I'm a dexterous hand
[01:45:16.440 --> 01:45:20.700]   and I go to talk about just grabbing a water bottle,
[01:45:20.700 --> 01:45:26.680]   if I have to enumerate every possible order
[01:45:26.680 --> 01:45:30.960]   that my hand came into contact with the bottle,
[01:45:30.960 --> 01:45:32.920]   then I'm dead in the water.
[01:45:32.920 --> 01:45:35.360]   Any approach that we were able to get away with that
[01:45:35.360 --> 01:45:38.440]   in walking because we mostly touched the ground
[01:45:38.440 --> 01:45:40.920]   more than a small number of points, for instance,
[01:45:40.920 --> 01:45:43.880]   and we haven't been able to get dexterous hands that way.
[01:45:43.880 --> 01:45:50.120]   - So you've mentioned that people think that contact
[01:45:50.120 --> 01:45:55.760]   is really hard and that that's the reason
[01:45:55.760 --> 01:46:00.600]   that robotic manipulation problem is really hard.
[01:46:00.600 --> 01:46:04.960]   Is there any flaws in that thinking?
[01:46:06.600 --> 01:46:10.560]   - So I think simulating contact is one aspect.
[01:46:10.560 --> 01:46:12.880]   I know people often say that we don't,
[01:46:12.880 --> 01:46:16.360]   that one of the reasons that we have a limit in robotics
[01:46:16.360 --> 01:46:19.080]   is because we do not simulate contact accurately
[01:46:19.080 --> 01:46:20.880]   in our simulators.
[01:46:20.880 --> 01:46:22.120]   And I think that is,
[01:46:22.120 --> 01:46:26.120]   the extent to which that's true is partly
[01:46:26.120 --> 01:46:27.920]   because our simulators,
[01:46:27.920 --> 01:46:29.960]   we haven't got mature enough simulators.
[01:46:29.960 --> 01:46:34.160]   There are some things that are still hard, difficult,
[01:46:34.160 --> 01:46:35.260]   that we should change.
[01:46:35.980 --> 01:46:40.980]   But we actually, we know what the governing equations are.
[01:46:40.980 --> 01:46:44.700]   They have some foibles, like this indeterminacy,
[01:46:44.700 --> 01:46:47.200]   but we should be able to simulate them accurately.
[01:46:47.200 --> 01:46:51.440]   We have incredible open source community in robotics,
[01:46:51.440 --> 01:46:54.340]   but it actually just takes a professional engineering team
[01:46:54.340 --> 01:46:57.740]   a lot of work to write a very good simulator like that.
[01:46:57.740 --> 01:47:02.160]   - My word is, I believe you've written, Drake.
[01:47:02.160 --> 01:47:04.500]   - There's a team of people.
[01:47:04.500 --> 01:47:07.300]   I certainly spent a lot of hours on it myself.
[01:47:07.300 --> 01:47:08.620]   - Well, what is Drake?
[01:47:08.620 --> 01:47:13.620]   What does it take to create a simulation environment
[01:47:13.620 --> 01:47:18.580]   for the kind of difficult control problems
[01:47:18.580 --> 01:47:19.580]   we're talking about?
[01:47:19.580 --> 01:47:24.640]   - Right, so Drake is the simulator that I've been working on.
[01:47:24.640 --> 01:47:26.780]   There are other good simulators out there.
[01:47:26.780 --> 01:47:29.700]   I don't like to think of Drake as just a simulator,
[01:47:29.700 --> 01:47:31.780]   'cause we write our controllers in Drake,
[01:47:31.780 --> 01:47:34.340]   we write our perception systems a little bit in Drake,
[01:47:34.340 --> 01:47:37.060]   but we write all of our low-level control
[01:47:37.060 --> 01:47:40.820]   and even planning and optimization.
[01:47:40.820 --> 01:47:42.460]   - So it has optimization capabilities as well?
[01:47:42.460 --> 01:47:43.620]   - Absolutely, yeah.
[01:47:43.620 --> 01:47:46.000]   I mean, Drake is three things, roughly.
[01:47:46.000 --> 01:47:48.220]   It's an optimization library, which is,
[01:47:48.220 --> 01:47:52.340]   sits on, it provides a layer of abstraction
[01:47:52.340 --> 01:47:55.900]   in C++ and Python for commercial solvers.
[01:47:55.900 --> 01:48:00.740]   You can write linear programs, quadratic programs,
[01:48:00.740 --> 01:48:03.340]   semi-definite programs, sums of squares programs,
[01:48:03.340 --> 01:48:05.660]   the ones we've used, mixed integer programs,
[01:48:05.660 --> 01:48:07.940]   and it will do the work to curate those
[01:48:07.940 --> 01:48:09.780]   and send them to whatever the right solver is,
[01:48:09.780 --> 01:48:12.500]   for instance, and it provides a level of abstraction.
[01:48:12.500 --> 01:48:18.340]   The second thing is a system modeling language,
[01:48:18.340 --> 01:48:20.900]   a bit like LabVIEW or Simulink,
[01:48:20.900 --> 01:48:24.820]   where you can make block diagrams out of complex systems,
[01:48:24.820 --> 01:48:26.660]   or it's like ROS in that sense,
[01:48:26.660 --> 01:48:29.020]   where you might have lots of ROS nodes
[01:48:29.020 --> 01:48:31.980]   that are each doing some part of your system,
[01:48:31.980 --> 01:48:33.820]   but to contrast it with ROS,
[01:48:33.820 --> 01:48:37.860]   we try to write, if you write a Drake system,
[01:48:37.860 --> 01:48:41.300]   then you have to, it asks you to describe
[01:48:41.300 --> 01:48:43.020]   a little bit more about the system.
[01:48:43.020 --> 01:48:46.260]   If you have any state, for instance, in the system,
[01:48:46.260 --> 01:48:47.700]   any variables that are gonna persist,
[01:48:47.700 --> 01:48:49.140]   you have to declare them.
[01:48:49.140 --> 01:48:51.660]   Parameters can be declared and the like,
[01:48:51.660 --> 01:48:54.180]   but the advantage of doing that is that you can,
[01:48:54.180 --> 01:48:57.480]   if you like, run things all on one process,
[01:48:57.480 --> 01:49:00.220]   but you can also do control design against it.
[01:49:00.220 --> 01:49:03.100]   You can do, I mean, simple things like rewinding
[01:49:03.100 --> 01:49:07.780]   and playing back your simulations, for instance.
[01:49:07.780 --> 01:49:09.580]   You know, these things, you get some rewards
[01:49:09.580 --> 01:49:11.380]   for spending a little bit more upfront cost
[01:49:11.380 --> 01:49:12.700]   in describing each system.
[01:49:12.700 --> 01:49:16.900]   And I was inspired to do that
[01:49:16.900 --> 01:49:20.340]   because I think the complexity of Atlas, for instance,
[01:49:20.340 --> 01:49:22.620]   is just so great.
[01:49:22.620 --> 01:49:24.140]   And I think, although, I mean,
[01:49:24.140 --> 01:49:26.940]   ROS has been incredible, absolute,
[01:49:26.940 --> 01:49:30.700]   huge fan of what it's done for the robotics community,
[01:49:30.700 --> 01:49:35.480]   but the ability to rapidly put different pieces together
[01:49:35.480 --> 01:49:37.960]   and have a functioning thing is very good.
[01:49:37.960 --> 01:49:42.860]   But I do think that it's hard to think clearly
[01:49:42.860 --> 01:49:44.980]   about a bag of disparate parts,
[01:49:44.980 --> 01:49:48.180]   Mr. Potato Head kind of software stack.
[01:49:48.180 --> 01:49:53.060]   And if you can, you know, ask a little bit more
[01:49:53.060 --> 01:49:54.180]   out of each of those parts,
[01:49:54.180 --> 01:49:56.100]   then you can understand the way they work better.
[01:49:56.100 --> 01:49:59.260]   You can try to verify them and the like,
[01:49:59.260 --> 01:50:02.660]   or you can do learning against them.
[01:50:02.660 --> 01:50:04.460]   And then one of those systems, the last thing,
[01:50:04.460 --> 01:50:06.460]   I said the first two things that Drake is,
[01:50:06.460 --> 01:50:09.620]   but the last thing is that there is a set
[01:50:09.620 --> 01:50:12.520]   of multi-body equations, rigid body equations,
[01:50:12.520 --> 01:50:16.740]   that is trying to provide a system that simulates physics.
[01:50:16.740 --> 01:50:20.020]   And that, we also have renderers and other things,
[01:50:20.020 --> 01:50:23.260]   but I think the physics component of Drake is special
[01:50:23.260 --> 01:50:27.700]   in the sense that we have done excessive amount
[01:50:27.700 --> 01:50:29.820]   of engineering to make sure
[01:50:29.820 --> 01:50:31.540]   that we've written the equations correctly.
[01:50:31.540 --> 01:50:34.100]   Every possible tumbling satellite or spinning top
[01:50:34.100 --> 01:50:36.100]   or anything that we could possibly write as a test
[01:50:36.100 --> 01:50:36.940]   is tested.
[01:50:36.940 --> 01:50:40.620]   We are making some, you know, I think,
[01:50:40.620 --> 01:50:44.220]   fundamental improvements on the way you simulate contact.
[01:50:44.220 --> 01:50:47.580]   - Just what does it take to simulate contact?
[01:50:47.580 --> 01:50:49.080]   I mean, it just seems,
[01:50:49.080 --> 01:50:52.380]   I mean, there's something just beautiful
[01:50:52.380 --> 01:50:55.220]   with the way you were explaining contact
[01:50:55.220 --> 01:50:58.340]   and you were tapping your fingers on the table
[01:50:58.340 --> 01:50:59.500]   while you're doing it.
[01:50:59.500 --> 01:51:00.740]   Just--
[01:51:00.740 --> 01:51:01.580]   - Easily, right?
[01:51:01.580 --> 01:51:04.820]   - Easily, just not even,
[01:51:04.820 --> 01:51:06.660]   it was helping you think, I guess.
[01:51:06.660 --> 01:51:14.020]   You have this awesome demo of loading
[01:51:14.020 --> 01:51:15.680]   or unloading a dishwasher.
[01:51:15.680 --> 01:51:18.820]   Just picking up a plate,
[01:51:21.460 --> 01:51:24.020]   grasping it like for the first time.
[01:51:24.020 --> 01:51:28.180]   That just seems like so difficult.
[01:51:28.180 --> 01:51:32.400]   What, how do you simulate any of that?
[01:51:32.400 --> 01:51:36.540]   - So it was really interesting that what happened was that
[01:51:36.540 --> 01:51:39.220]   we started getting more professional
[01:51:39.220 --> 01:51:40.500]   about our software development
[01:51:40.500 --> 01:51:42.300]   during the DARPA Robotics Challenge.
[01:51:42.300 --> 01:51:46.060]   I learned the value of software engineering
[01:51:46.060 --> 01:51:48.640]   and how to bridle complexity.
[01:51:48.640 --> 01:51:52.780]   I guess that's what I want to somehow fight against
[01:51:52.780 --> 01:51:54.760]   and bring some of the clear thinking of controls
[01:51:54.760 --> 01:51:58.200]   into these complex systems we're building for robots.
[01:51:58.200 --> 01:52:02.940]   Shortly after the DARPA Robotics Challenge,
[01:52:02.940 --> 01:52:04.600]   Toyota opened a research institute,
[01:52:04.600 --> 01:52:07.260]   TRI, Toyota Research Institute.
[01:52:07.260 --> 01:52:10.880]   They put one of their, there's three locations.
[01:52:10.880 --> 01:52:13.040]   One of them is just down the street from MIT
[01:52:13.040 --> 01:52:16.240]   and I helped ramp that up
[01:52:17.520 --> 01:52:20.860]   as a part of my, the end of my sabbatical, I guess.
[01:52:20.860 --> 01:52:28.480]   So TRI has given me, the TRI Robotics effort
[01:52:28.480 --> 01:52:32.640]   has made this investment in simulation in Drake.
[01:52:32.640 --> 01:52:34.480]   And Michael Sherman leads a team there
[01:52:34.480 --> 01:52:37.800]   of just absolutely top-notch dynamics experts
[01:52:37.800 --> 01:52:40.120]   that are trying to write those simulators
[01:52:40.120 --> 01:52:41.960]   that can pick up the dishes.
[01:52:41.960 --> 01:52:44.780]   And there's also a team working on manipulation there
[01:52:44.780 --> 01:52:48.980]   that is taking problems like loading the dishwasher
[01:52:48.980 --> 01:52:53.180]   and we're using that to study these really hard corner cases
[01:52:53.180 --> 01:52:55.280]   kind of problems in manipulation.
[01:52:55.280 --> 01:52:59.720]   So for me, this, you know, simulating the dishes,
[01:52:59.720 --> 01:53:01.580]   we could actually write a controller.
[01:53:01.580 --> 01:53:05.040]   If we just cared about picking up dishes in the sink once,
[01:53:05.040 --> 01:53:05.880]   we could write a controller
[01:53:05.880 --> 01:53:07.720]   without any simulation whatsoever
[01:53:07.720 --> 01:53:10.040]   and we could call it done.
[01:53:10.040 --> 01:53:12.140]   But we want to understand like,
[01:53:12.140 --> 01:53:17.040]   what is the path you take to actually get to a robot
[01:53:17.040 --> 01:53:22.040]   that could perform that for any dish in anybody's kitchen
[01:53:22.040 --> 01:53:23.280]   with enough confidence
[01:53:23.280 --> 01:53:26.520]   that it could be a commercial product, right?
[01:53:26.520 --> 01:53:29.360]   And it has deep learning perception in the loop.
[01:53:29.360 --> 01:53:31.060]   It has complex dynamics in the loop.
[01:53:31.060 --> 01:53:33.260]   It has controller, it has a planner.
[01:53:33.260 --> 01:53:36.340]   And how do you take all of that complexity
[01:53:36.340 --> 01:53:39.020]   and put it through this engineering discipline
[01:53:39.020 --> 01:53:42.420]   and verification and validation process
[01:53:42.420 --> 01:53:46.440]   to actually get enough confidence to deploy?
[01:53:46.440 --> 01:53:49.840]   I mean, the DARPA challenge made me realize
[01:53:49.840 --> 01:53:52.000]   that that's not something you throw over the fence
[01:53:52.000 --> 01:53:54.060]   and hope that somebody will harden it for you.
[01:53:54.060 --> 01:53:57.380]   That there are really fundamental challenges
[01:53:57.380 --> 01:53:59.820]   in closing that last gap.
[01:53:59.820 --> 01:54:02.320]   - They're doing the validation and the testing.
[01:54:02.320 --> 01:54:05.980]   - I think it might even change the way we have to think
[01:54:05.980 --> 01:54:08.040]   about the way we write systems.
[01:54:08.980 --> 01:54:13.980]   What happens if you have the robot running lots of tests
[01:54:13.980 --> 01:54:19.040]   and it screws up, it breaks a dish, right?
[01:54:19.040 --> 01:54:19.960]   How do you capture that?
[01:54:19.960 --> 01:54:23.600]   I said, you can't run the same simulation
[01:54:23.600 --> 01:54:27.020]   or the same experiment twice on a real robot.
[01:54:27.020 --> 01:54:31.520]   Do we have to be able to bring that one-off failure
[01:54:31.520 --> 01:54:34.200]   back into simulation in order to change our controllers,
[01:54:34.200 --> 01:54:37.240]   study it, make sure it won't happen again?
[01:54:37.240 --> 01:54:40.600]   Do we, is it enough to just try to add that
[01:54:40.600 --> 01:54:43.800]   to our distribution and understand that on average,
[01:54:43.800 --> 01:54:45.920]   we're gonna cover that situation again?
[01:54:45.920 --> 01:54:49.960]   There's like really subtle questions at the corner cases
[01:54:49.960 --> 01:54:53.520]   that I think we don't yet have satisfying answers for.
[01:54:53.520 --> 01:54:55.120]   - How do you find the corner cases?
[01:54:55.120 --> 01:54:57.160]   That's one kind of, is there,
[01:54:57.160 --> 01:55:01.280]   do you think that's possible to create a systematized way
[01:55:01.280 --> 01:55:04.720]   of discovering corner cases efficiently?
[01:55:04.720 --> 01:55:05.560]   - Yes.
[01:55:05.560 --> 01:55:07.600]   - And whatever the problem is?
[01:55:07.600 --> 01:55:10.760]   - Yes, I mean, I think we have to get better at that.
[01:55:10.760 --> 01:55:14.920]   I mean, control theory has, for decades,
[01:55:14.920 --> 01:55:16.900]   talked about active experiment design.
[01:55:16.900 --> 01:55:18.680]   - What's that?
[01:55:18.680 --> 01:55:22.080]   - So people call it curiosity these days.
[01:55:22.080 --> 01:55:24.100]   It's roughly this idea of trying to,
[01:55:24.100 --> 01:55:25.520]   exploration or exploitation,
[01:55:25.520 --> 01:55:28.120]   but in the active experiment design is even,
[01:55:28.120 --> 01:55:29.640]   is more specific.
[01:55:29.640 --> 01:55:34.120]   You could try to understand the uncertainty in your system,
[01:55:34.120 --> 01:55:36.480]   design the experiment that will provide
[01:55:36.480 --> 01:55:40.120]   the maximum information to reduce that uncertainty.
[01:55:40.120 --> 01:55:42.360]   If there's a parameter you wanna learn about,
[01:55:42.360 --> 01:55:45.440]   what is the optimal trajectory I could execute
[01:55:45.440 --> 01:55:47.640]   to learn about that parameter, for instance?
[01:55:47.640 --> 01:55:51.720]   Scaling that up to something that has a deep network
[01:55:51.720 --> 01:55:55.660]   in the loop and a planning in the loop is tough.
[01:55:55.660 --> 01:55:57.000]   We've done some work on,
[01:55:57.000 --> 01:56:00.280]   you know, with Matt O'Kelley and Amansina,
[01:56:00.280 --> 01:56:03.600]   we've worked on some falsification algorithms
[01:56:03.600 --> 01:56:05.600]   that are trying to do rare event simulation
[01:56:05.600 --> 01:56:08.120]   that try to just hammer on your simulator.
[01:56:08.120 --> 01:56:10.000]   And if your simulator is good enough,
[01:56:10.000 --> 01:56:13.420]   you can spend a lot of time,
[01:56:13.420 --> 01:56:15.840]   you can write good algorithms
[01:56:15.840 --> 01:56:19.920]   that try to spend most of their time in the corner cases.
[01:56:19.920 --> 01:56:24.920]   So you basically imagine you're building an autonomous car
[01:56:24.920 --> 01:56:27.320]   and you wanna put it in, I don't know,
[01:56:27.320 --> 01:56:29.360]   downtown New Delhi all the time, right?
[01:56:29.360 --> 01:56:30.740]   And accelerated testing.
[01:56:30.740 --> 01:56:33.340]   If you can write sampling strategies,
[01:56:33.340 --> 01:56:36.260]   which figure out where your controller's performing badly
[01:56:36.260 --> 01:56:38.160]   in simulation and start generating
[01:56:38.160 --> 01:56:39.720]   lots of examples around that.
[01:56:39.720 --> 01:56:44.060]   You know, it's just the space of possible places
[01:56:44.060 --> 01:56:48.040]   where that can be, where things can go wrong is very big.
[01:56:48.040 --> 01:56:49.800]   So it's hard to write those algorithms.
[01:56:49.800 --> 01:56:51.720]   - Yeah, rare event simulation
[01:56:51.720 --> 01:56:53.720]   is just like a really compelling notion.
[01:56:53.720 --> 01:56:55.800]   If it's possible.
[01:56:55.800 --> 01:56:58.800]   - We joked and we call it the black swan generator.
[01:56:58.800 --> 01:57:00.080]   - It's a black swan.
[01:57:00.080 --> 01:57:01.680]   - 'Cause you don't just want the rare events,
[01:57:01.680 --> 01:57:04.000]   you want the ones that are highly impactful.
[01:57:04.000 --> 01:57:05.660]   - I mean, that's the most,
[01:57:05.660 --> 01:57:08.780]   those are the most sort of profound questions
[01:57:08.780 --> 01:57:10.120]   we ask of our world.
[01:57:10.120 --> 01:57:15.120]   Like, what's the worst that can happen?
[01:57:15.120 --> 01:57:18.080]   But what we're really asking
[01:57:18.080 --> 01:57:20.800]   isn't some kind of like computer science,
[01:57:20.800 --> 01:57:22.580]   worst case analysis.
[01:57:22.580 --> 01:57:25.600]   We're asking like, what are the millions of ways
[01:57:25.600 --> 01:57:27.360]   this can go wrong?
[01:57:27.360 --> 01:57:29.500]   And that's like our curiosity.
[01:57:31.200 --> 01:57:34.880]   We humans, I think are pretty bad at,
[01:57:34.880 --> 01:57:36.960]   we just like run into it.
[01:57:36.960 --> 01:57:38.520]   And I think there's a distributed sense
[01:57:38.520 --> 01:57:41.720]   because there's now like 7.5 billion of us.
[01:57:41.720 --> 01:57:42.840]   And so there's a lot of them,
[01:57:42.840 --> 01:57:45.000]   and then a lot of them write blog posts
[01:57:45.000 --> 01:57:46.480]   about the stupid thing they've done.
[01:57:46.480 --> 01:57:48.840]   So we learn in a distributed way.
[01:57:48.840 --> 01:57:50.800]   There's some--
[01:57:50.800 --> 01:57:52.720]   - I think that's gonna be important for robots too.
[01:57:52.720 --> 01:57:53.560]   - Yeah.
[01:57:53.560 --> 01:57:55.880]   - I mean, that's another massive theme
[01:57:55.880 --> 01:57:58.760]   at Toyota Research for robotics
[01:57:58.760 --> 01:58:00.560]   is this fleet learning concept.
[01:58:00.560 --> 01:58:04.760]   Is the idea that I as a human,
[01:58:04.760 --> 01:58:07.680]   I don't have enough time to visit all of my states.
[01:58:07.680 --> 01:58:08.520]   Right?
[01:58:08.520 --> 01:58:10.160]   There's just a, it's very hard for one robot
[01:58:10.160 --> 01:58:11.600]   to experience all the things.
[01:58:11.600 --> 01:58:14.680]   But that's not actually the problem we have to solve.
[01:58:14.680 --> 01:58:15.520]   Right?
[01:58:15.520 --> 01:58:17.720]   We're gonna have fleets of robots
[01:58:17.720 --> 01:58:20.680]   that can have very similar appendages.
[01:58:20.680 --> 01:58:24.160]   And at some point, maybe collectively,
[01:58:24.160 --> 01:58:29.160]   they have enough data that their computational processes
[01:58:29.320 --> 01:58:30.680]   should be set up differently than ours.
[01:58:30.680 --> 01:58:31.880]   Right?
[01:58:31.880 --> 01:58:34.200]   - It's this vision of just,
[01:58:34.200 --> 01:58:38.880]   I mean, all these dishwasher unloading robots.
[01:58:38.880 --> 01:58:42.600]   I mean, that robot dropping a plate
[01:58:42.600 --> 01:58:46.880]   and a human looking at the robot, probably pissed off.
[01:58:46.880 --> 01:58:47.840]   - Yeah.
[01:58:47.840 --> 01:58:51.200]   - But that's a special moment to record.
[01:58:51.200 --> 01:58:54.520]   I think one thing in terms of fleet learning,
[01:58:54.520 --> 01:58:57.720]   and I've seen that because I've talked to a lot of folks,
[01:58:57.720 --> 01:59:01.200]   just like Tesla users or Tesla drivers,
[01:59:01.200 --> 01:59:03.600]   they're another company that's using
[01:59:03.600 --> 01:59:05.240]   this kind of fleet learning idea.
[01:59:05.240 --> 01:59:08.160]   One hopeful thing I have about humans
[01:59:08.160 --> 01:59:13.160]   is they really enjoy when a system improves, learns.
[01:59:13.160 --> 01:59:14.640]   So they enjoy fleet learning.
[01:59:14.640 --> 01:59:17.200]   And the reason it's hopeful for me
[01:59:17.200 --> 01:59:20.240]   is they're willing to put up with something
[01:59:20.240 --> 01:59:22.600]   that's kind of dumb right now.
[01:59:22.600 --> 01:59:25.480]   And they're like, if it's improving,
[01:59:25.480 --> 01:59:29.280]   they almost enjoy being part of the teaching.
[01:59:29.280 --> 01:59:30.880]   It almost like if you have kids,
[01:59:30.880 --> 01:59:32.200]   like you're teaching them something.
[01:59:32.200 --> 01:59:33.480]   - Right.
[01:59:33.480 --> 01:59:35.080]   - I think that's a beautiful thing
[01:59:35.080 --> 01:59:36.200]   'cause that gives me hope
[01:59:36.200 --> 01:59:38.640]   that we can put dumb robots out there.
[01:59:38.640 --> 01:59:43.280]   I mean, the problem on the Tesla side with cars,
[01:59:43.280 --> 01:59:44.440]   cars can kill you.
[01:59:44.440 --> 01:59:47.680]   That makes the problem so much harder.
[01:59:47.680 --> 01:59:50.520]   Dishwasher unloading is a little safe.
[01:59:50.520 --> 01:59:54.160]   That's why home robotics is really exciting.
[01:59:54.160 --> 01:59:55.680]   And just to clarify, I mean,
[01:59:55.680 --> 01:59:57.560]   for people who might not know,
[01:59:57.560 --> 02:00:00.080]   I mean, TRI, Toyota Research Institute,
[02:00:00.080 --> 02:00:03.960]   so they're pretty well known
[02:00:03.960 --> 02:00:06.120]   for like autonomous vehicle research,
[02:00:06.120 --> 02:00:10.240]   but they're also interested in home robotics.
[02:00:10.240 --> 02:00:12.760]   - Yep, there's a big group working on,
[02:00:12.760 --> 02:00:14.320]   multiple groups working on home robotics.
[02:00:14.320 --> 02:00:17.440]   It's a major part of the portfolio.
[02:00:17.440 --> 02:00:19.080]   There's also a couple other projects
[02:00:19.080 --> 02:00:21.280]   and advanced materials discovery,
[02:00:21.280 --> 02:00:24.400]   using AI and machine learning to discover new materials
[02:00:24.400 --> 02:00:27.920]   for car batteries and the like, for instance.
[02:00:27.920 --> 02:00:31.520]   Yeah, and that's been actually incredibly successful team.
[02:00:31.520 --> 02:00:32.800]   There's new projects starting up too.
[02:00:32.800 --> 02:00:33.640]   So--
[02:00:33.640 --> 02:00:38.640]   - Do you see a future of where like robots are in our home
[02:00:38.640 --> 02:00:43.960]   and like robots that have like actuators
[02:00:43.960 --> 02:00:46.600]   that look like arms in our home
[02:00:46.600 --> 02:00:49.320]   or like more like humanoid type robots?
[02:00:49.320 --> 02:00:51.760]   Or is this, are we gonna do the same thing
[02:00:51.760 --> 02:00:53.840]   that you just mentioned that,
[02:00:53.840 --> 02:00:55.960]   the dishwasher is no longer a robot.
[02:00:55.960 --> 02:00:58.680]   We're going to just not even see them as robots.
[02:00:58.680 --> 02:01:02.480]   But I mean, what's your vision of the home of the future?
[02:01:02.480 --> 02:01:06.180]   10, 20 years from now, 50 years if you get crazy.
[02:01:06.180 --> 02:01:10.680]   - Yeah, I think we already have Roombas cruising around.
[02:01:10.680 --> 02:01:13.680]   We have, you know, Alexa's or Google Homes
[02:01:13.680 --> 02:01:16.200]   on our kitchen counter.
[02:01:16.200 --> 02:01:18.040]   It's only a matter of time till they spring arms
[02:01:18.040 --> 02:01:20.760]   and start doing something useful like that.
[02:01:20.760 --> 02:01:23.840]   So I do think it's coming.
[02:01:23.840 --> 02:01:27.680]   I think lots of people have lots of motivations
[02:01:27.680 --> 02:01:29.400]   for doing it.
[02:01:29.400 --> 02:01:31.520]   It's been super interesting actually learning
[02:01:31.520 --> 02:01:33.880]   about Toyota's vision for it,
[02:01:33.880 --> 02:01:36.380]   which is about helping people age in place.
[02:01:36.380 --> 02:01:41.600]   'Cause I think that's not necessarily the first entry,
[02:01:41.600 --> 02:01:44.360]   the most lucrative entry point,
[02:01:44.360 --> 02:01:45.920]   but it's the problem maybe that
[02:01:47.680 --> 02:01:50.040]   we really need to solve no matter what.
[02:01:50.040 --> 02:01:53.920]   And so I think there's a real opportunity.
[02:01:53.920 --> 02:01:55.760]   It's a delicate problem.
[02:01:55.760 --> 02:01:59.340]   How do you work with people, help people,
[02:01:59.340 --> 02:02:02.340]   keep them active, engaged, you know,
[02:02:02.340 --> 02:02:05.120]   but improve the quality of life
[02:02:05.120 --> 02:02:08.360]   and help them age in place, for instance.
[02:02:08.360 --> 02:02:12.480]   - It's interesting 'cause older folks are also,
[02:02:12.480 --> 02:02:14.480]   I mean, there's a contrast there because
[02:02:15.880 --> 02:02:18.120]   they're not always the folks
[02:02:18.120 --> 02:02:20.920]   who are the most comfortable with technology, for example.
[02:02:20.920 --> 02:02:24.600]   So there's a division that's interesting there
[02:02:24.600 --> 02:02:27.120]   that you can do so much good with a robot
[02:02:27.120 --> 02:02:32.060]   for older folks,
[02:02:32.060 --> 02:02:36.400]   but there's a gap to fill of understanding.
[02:02:36.400 --> 02:02:38.400]   I mean, it's actually kind of beautiful.
[02:02:38.400 --> 02:02:41.160]   Robot is learning about the human
[02:02:41.160 --> 02:02:44.840]   and the human is kind of learning about this new robot thing.
[02:02:44.840 --> 02:02:47.240]   And it's also with,
[02:02:47.240 --> 02:02:51.440]   at least with, like when I talk to my parents about robots,
[02:02:51.440 --> 02:02:54.520]   there's a little bit of a blank slate there too.
[02:02:54.520 --> 02:02:59.360]   Like you can, I mean, they don't know anything about robotics
[02:02:59.360 --> 02:03:02.640]   so it's completely like wide open.
[02:03:02.640 --> 02:03:03.880]   They don't have, they haven't,
[02:03:03.880 --> 02:03:05.720]   my parents haven't seen Black Mirror.
[02:03:05.720 --> 02:03:09.480]   So like it's a blank slate.
[02:03:09.480 --> 02:03:11.960]   Here's a cool thing, like what can it do for me?
[02:03:11.960 --> 02:03:12.800]   - Yeah.
[02:03:12.800 --> 02:03:14.360]   - So it's an exciting space.
[02:03:14.360 --> 02:03:16.360]   - I think it's a really important space.
[02:03:16.360 --> 02:03:20.000]   I do feel like, you know, a few years ago,
[02:03:20.000 --> 02:03:22.720]   drones were successful enough in academia.
[02:03:22.720 --> 02:03:26.000]   They kind of broke out and started in industry
[02:03:26.000 --> 02:03:29.120]   and autonomous cars have been happening.
[02:03:29.120 --> 02:03:32.880]   It does feel like manipulation in logistics, of course,
[02:03:32.880 --> 02:03:35.680]   first, but in the home shortly after,
[02:03:35.680 --> 02:03:37.160]   it seems like one of the next big things
[02:03:37.160 --> 02:03:40.040]   that's gonna really pop.
[02:03:40.040 --> 02:03:42.080]   - So I don't think we talked about it,
[02:03:42.080 --> 02:03:44.560]   but what's soft robotics?
[02:03:44.560 --> 02:03:48.560]   So we talked about like rigid bodies.
[02:03:48.560 --> 02:03:52.120]   If we can just linger on this whole touch thing.
[02:03:52.120 --> 02:03:54.680]   - Yeah, so what's soft robotics?
[02:03:54.680 --> 02:03:59.680]   So I told you that I really dislike the fact
[02:03:59.680 --> 02:04:03.180]   that robots are afraid of touching the world
[02:04:03.180 --> 02:04:04.880]   all over their body.
[02:04:04.880 --> 02:04:06.920]   So there's a couple reasons for that.
[02:04:06.920 --> 02:04:08.760]   If you look carefully at all the places
[02:04:08.760 --> 02:04:11.240]   that robots actually do touch the world,
[02:04:11.240 --> 02:04:12.560]   they're almost always soft.
[02:04:12.560 --> 02:04:14.680]   They have some sort of pad on their fingers
[02:04:14.680 --> 02:04:16.900]   or a rubber sole on their foot.
[02:04:16.900 --> 02:04:19.280]   But if you look up and down the arm,
[02:04:19.280 --> 02:04:21.680]   we're just pure aluminum or something.
[02:04:21.680 --> 02:04:26.640]   So that makes it hard actually.
[02:04:26.640 --> 02:04:30.440]   In fact, hitting the table with your rigid arm
[02:04:30.440 --> 02:04:34.560]   or nearly rigid arm has some of the problems
[02:04:34.560 --> 02:04:37.220]   that we talked about in terms of simulation.
[02:04:37.220 --> 02:04:39.920]   I think it fundamentally changes the mechanics of contact
[02:04:39.920 --> 02:04:41.640]   when you're soft, right?
[02:04:41.640 --> 02:04:44.980]   You turn point contacts into patch contacts,
[02:04:44.980 --> 02:04:46.980]   which can have torsional friction.
[02:04:46.980 --> 02:04:49.240]   You can have distributed load.
[02:04:49.240 --> 02:04:52.420]   If I wanna pick up an egg, right?
[02:04:52.420 --> 02:04:54.240]   If I pick it up with two points,
[02:04:54.240 --> 02:04:56.200]   then in order to put enough force
[02:04:56.200 --> 02:04:57.280]   to sustain the weight of the egg,
[02:04:57.280 --> 02:04:59.920]   I might have to put a lot of force to break the egg.
[02:04:59.920 --> 02:05:04.420]   If I envelop it with contact all around,
[02:05:04.420 --> 02:05:07.520]   then I can distribute my force across the shell of the egg
[02:05:07.520 --> 02:05:09.760]   and have a better chance of not breaking it.
[02:05:09.760 --> 02:05:12.440]   So soft robotics is for me a lot about
[02:05:12.440 --> 02:05:15.520]   changing the mechanics of contact.
[02:05:15.520 --> 02:05:17.480]   - Does it make the problem a lot harder?
[02:05:17.480 --> 02:05:21.360]   (laughing)
[02:05:21.360 --> 02:05:22.400]   - Quite the opposite.
[02:05:22.400 --> 02:05:26.760]   It changes the computational problem.
[02:05:26.760 --> 02:05:28.560]   I think because of the,
[02:05:28.560 --> 02:05:31.880]   I think our world and our mathematics
[02:05:31.880 --> 02:05:34.520]   has biased us towards rigid.
[02:05:34.520 --> 02:05:37.520]   But it really should make things better in some ways, right?
[02:05:37.520 --> 02:05:43.080]   It's a, I think the future is unwritten there.
[02:05:43.080 --> 02:05:45.480]   But the other thing--
[02:05:45.480 --> 02:05:46.840]   - I think ultimately, sorry to interrupt,
[02:05:46.840 --> 02:05:49.560]   but I think ultimately it will make things simpler
[02:05:49.560 --> 02:05:51.560]   if we embrace the softness of the world.
[02:05:51.560 --> 02:05:55.720]   - It makes things smoother, right?
[02:05:55.720 --> 02:06:00.720]   So the result of small actions is less discontinuous,
[02:06:02.320 --> 02:06:05.080]   but it also means potentially less, you know,
[02:06:05.080 --> 02:06:08.080]   instantaneously bad, for instance.
[02:06:08.080 --> 02:06:11.520]   I won't necessarily contact something and send it flying off.
[02:06:11.520 --> 02:06:14.240]   The other aspect of it
[02:06:14.240 --> 02:06:16.000]   that just happens to dovetail really well
[02:06:16.000 --> 02:06:18.400]   is that soft robotics tends to be a place
[02:06:18.400 --> 02:06:20.240]   where we can embed a lot of sensors too.
[02:06:20.240 --> 02:06:24.720]   So if you change your hardware and make it more soft,
[02:06:24.720 --> 02:06:26.680]   then you can potentially have a tactile sensor,
[02:06:26.680 --> 02:06:29.020]   which is measuring the deformation.
[02:06:30.680 --> 02:06:34.880]   So there's a team at TRI that's working on soft hands,
[02:06:34.880 --> 02:06:38.160]   and you get so much more information.
[02:06:38.160 --> 02:06:41.720]   If you can put a camera behind the skin roughly
[02:06:41.720 --> 02:06:45.760]   and get fantastic tactile information,
[02:06:45.760 --> 02:06:49.120]   which is, it's super important.
[02:06:49.120 --> 02:06:50.080]   Like in manipulation,
[02:06:50.080 --> 02:06:52.840]   one of the things that really is frustrating
[02:06:52.840 --> 02:06:55.080]   is if you work super hard on your head-mounted,
[02:06:55.080 --> 02:06:57.720]   on your perception system for your head-mounted cameras,
[02:06:57.720 --> 02:06:59.480]   and then you've identified an object,
[02:06:59.480 --> 02:07:00.360]   you reach down to touch it,
[02:07:00.360 --> 02:07:01.920]   and the last thing that happens,
[02:07:01.920 --> 02:07:04.000]   right before the most important time,
[02:07:04.000 --> 02:07:05.760]   you stick your hand and you're occluding
[02:07:05.760 --> 02:07:07.400]   your head-mounted sensors, right?
[02:07:07.400 --> 02:07:10.200]   So in all the part that really matters,
[02:07:10.200 --> 02:07:13.560]   all of your off-board sensors are occluded.
[02:07:13.560 --> 02:07:15.920]   And really, if you don't have tactile information,
[02:07:15.920 --> 02:07:19.300]   then you're blind in an important way.
[02:07:19.300 --> 02:07:23.160]   So it happens that soft robotics and tactile sensing
[02:07:23.160 --> 02:07:25.080]   tend to go hand in hand.
[02:07:25.080 --> 02:07:26.820]   - I think we've kind of talked about it,
[02:07:26.820 --> 02:07:31.060]   but you taught a course on under-actuated robotics.
[02:07:31.060 --> 02:07:32.780]   I believe that was the name of it, actually.
[02:07:32.780 --> 02:07:33.620]   - That's right.
[02:07:33.620 --> 02:07:37.340]   - Can you talk about it in that context?
[02:07:37.340 --> 02:07:40.380]   What is under-actuated robotics?
[02:07:40.380 --> 02:07:43.740]   - Right, so under-actuated robotics is my graduate course.
[02:07:43.740 --> 02:07:46.640]   It's online mostly now,
[02:07:46.640 --> 02:07:47.480]   in the sense that the lectures--
[02:07:47.480 --> 02:07:49.060]   - Several versions of it, I think.
[02:07:49.060 --> 02:07:49.900]   - Right, the YouTube--
[02:07:49.900 --> 02:07:52.060]   - It's really great, I recommend it highly.
[02:07:52.060 --> 02:07:56.180]   - Look on YouTube for the 2020 versions until March,
[02:07:56.180 --> 02:07:59.180]   and then you have to go back to 2019, thanks to COVID.
[02:07:59.180 --> 02:08:03.800]   No, I've poured my heart into that class.
[02:08:03.800 --> 02:08:06.900]   And lecture one is basically explaining
[02:08:06.900 --> 02:08:08.220]   what the word under-actuated means.
[02:08:08.220 --> 02:08:10.140]   So people are very kind to show up,
[02:08:10.140 --> 02:08:12.500]   and then maybe have to learn
[02:08:12.500 --> 02:08:13.720]   what the title of the course means
[02:08:13.720 --> 02:08:15.680]   over the course of the first lecture.
[02:08:15.680 --> 02:08:17.780]   - That first lecture's really good.
[02:08:17.780 --> 02:08:19.020]   You should watch it.
[02:08:19.020 --> 02:08:20.140]   - Thanks.
[02:08:20.140 --> 02:08:21.780]   It's a strange name,
[02:08:21.780 --> 02:08:26.160]   but I thought it captured the essence
[02:08:26.160 --> 02:08:28.220]   of what control was good at doing,
[02:08:28.220 --> 02:08:30.260]   and what control was bad at doing.
[02:08:30.260 --> 02:08:32.220]   So what do I mean by under-actuated?
[02:08:32.220 --> 02:08:34.980]   So a mechanical system
[02:08:34.980 --> 02:08:39.780]   has many degrees of freedom, for instance.
[02:08:39.780 --> 02:08:42.260]   I think of a joint as a degree of freedom.
[02:08:42.260 --> 02:08:46.500]   And it has some number of actuators, motors.
[02:08:46.500 --> 02:08:49.540]   So if you have a robot that's bolted to the table
[02:08:49.540 --> 02:08:54.420]   that has five degrees of freedom, and five motors,
[02:08:54.420 --> 02:08:56.260]   then you have a fully actuated robot.
[02:08:56.260 --> 02:09:00.860]   If you take away one of those motors,
[02:09:00.860 --> 02:09:03.500]   then you have an under-actuated robot.
[02:09:03.500 --> 02:09:05.260]   Now why on earth?
[02:09:05.260 --> 02:09:07.780]   I have a good friend who likes to tease me.
[02:09:07.780 --> 02:09:09.780]   He said, "Russ, if you had more research funding,
[02:09:09.780 --> 02:09:12.100]   "would you work on fully actuated robots?"
[02:09:12.100 --> 02:09:14.180]   - Yeah. (laughs)
[02:09:14.180 --> 02:09:15.500]   - The answer's no.
[02:09:15.500 --> 02:09:17.740]   The world gives us under-actuated robots,
[02:09:17.740 --> 02:09:18.740]   whether we like it or not.
[02:09:18.740 --> 02:09:20.100]   I'm a human.
[02:09:20.100 --> 02:09:21.740]   I'm an under-actuated robot,
[02:09:21.740 --> 02:09:23.740]   even though I have more muscles
[02:09:23.740 --> 02:09:25.420]   than my big degrees of freedom,
[02:09:25.420 --> 02:09:27.980]   because I have, in some places,
[02:09:27.980 --> 02:09:30.180]   multiple muscles attached to the same joint.
[02:09:30.180 --> 02:09:34.140]   But still, there's a really important degree of freedom
[02:09:34.140 --> 02:09:35.620]   that I have, which is the location
[02:09:35.620 --> 02:09:38.780]   of my center of mass in space, for instance.
[02:09:38.780 --> 02:09:42.740]   All right, I can jump into the air,
[02:09:42.740 --> 02:09:45.460]   and there's no motor that connects my center of mass
[02:09:45.460 --> 02:09:47.460]   to the ground in that case.
[02:09:47.460 --> 02:09:49.700]   So I have to think about the implications
[02:09:49.700 --> 02:09:51.980]   of not having control over everything.
[02:09:51.980 --> 02:09:56.820]   The passive dynamic walkers are the extreme view of that,
[02:09:56.820 --> 02:09:58.140]   where you've taken away all the motors,
[02:09:58.140 --> 02:10:00.260]   and you have to let physics do the work.
[02:10:00.260 --> 02:10:02.500]   But it shows up in all of the walking robots,
[02:10:02.500 --> 02:10:04.820]   where you have to use some of the actuators
[02:10:04.820 --> 02:10:07.260]   to push and pull even the degrees of freedom
[02:10:07.260 --> 02:10:09.180]   that you don't have an actuator on.
[02:10:09.180 --> 02:10:13.420]   - That's referring to walking if you're falling forward.
[02:10:13.420 --> 02:10:16.540]   Is there a way to walk that's fully actuated?
[02:10:16.540 --> 02:10:18.620]   - So it's a subtle point.
[02:10:18.620 --> 02:10:22.340]   When you're in contact, and you have your feet
[02:10:22.340 --> 02:10:26.780]   on the ground, there are still limits to what you can do.
[02:10:26.780 --> 02:10:29.380]   Unless I have suction cups on my feet,
[02:10:29.380 --> 02:10:31.380]   I cannot accelerate my center of mass
[02:10:31.380 --> 02:10:34.020]   towards the ground faster than gravity,
[02:10:34.020 --> 02:10:36.180]   'cause I can't get a force pushing me down.
[02:10:36.180 --> 02:10:39.660]   But I can still do most of the things that I want to.
[02:10:39.660 --> 02:10:42.260]   So you can get away with basically thinking
[02:10:42.260 --> 02:10:43.700]   of the system as fully actuated,
[02:10:43.700 --> 02:10:46.540]   unless you suddenly needed to accelerate down super fast.
[02:10:46.540 --> 02:10:49.260]   But as soon as I take a step,
[02:10:49.260 --> 02:10:52.940]   I get into the more nuanced territory.
[02:10:52.940 --> 02:10:55.740]   And to get to really dynamic robots,
[02:10:55.740 --> 02:10:59.220]   or airplanes, or other things,
[02:10:59.220 --> 02:11:02.620]   I think you have to embrace the under-actuated dynamics.
[02:11:02.620 --> 02:11:04.460]   Manipulation, people think,
[02:11:04.460 --> 02:11:06.940]   is manipulation under-actuated?
[02:11:06.940 --> 02:11:10.580]   Even if my arm is fully actuated, I have a motor,
[02:11:10.580 --> 02:11:13.500]   if my goal is to control the position
[02:11:13.500 --> 02:11:16.020]   and orientation of this cup,
[02:11:16.020 --> 02:11:19.220]   then I don't have an actuator for that directly.
[02:11:19.220 --> 02:11:21.060]   So I have to use my actuators over here
[02:11:21.060 --> 02:11:22.260]   to control this thing.
[02:11:22.260 --> 02:11:24.340]   Now it gets even worse,
[02:11:24.340 --> 02:11:26.900]   like what if I have to button my shirt?
[02:11:26.900 --> 02:11:31.340]   What are the degrees of freedom of my shirt?
[02:11:31.340 --> 02:11:34.500]   I suddenly, that's a hard question to think about.
[02:11:34.500 --> 02:11:36.020]   It kind of makes me queasy
[02:11:36.020 --> 02:11:38.860]   as thinking about my state-space control ideas.
[02:11:39.860 --> 02:11:41.780]   But actually those are the problems
[02:11:41.780 --> 02:11:44.500]   that make me so excited about manipulation right now,
[02:11:44.500 --> 02:11:46.980]   is that it breaks some of the,
[02:11:46.980 --> 02:11:49.980]   it breaks a lot of the foundational control stuff
[02:11:49.980 --> 02:11:51.500]   that I've been thinking about.
[02:11:51.500 --> 02:11:54.940]   - What are some interesting insights you could say
[02:11:54.940 --> 02:11:57.980]   about trying to solve an under-actuated,
[02:11:57.980 --> 02:12:02.300]   a control in an under-actuated system?
[02:12:02.300 --> 02:12:04.740]   - So I think the philosophy there
[02:12:04.740 --> 02:12:07.140]   is let physics do more of the work.
[02:12:08.380 --> 02:12:12.140]   The technical approach has been optimization.
[02:12:12.140 --> 02:12:15.020]   So you typically formulate your decision-making for control
[02:12:15.020 --> 02:12:17.060]   as an optimization problem,
[02:12:17.060 --> 02:12:19.340]   and you use the language of optimal control
[02:12:19.340 --> 02:12:22.700]   and sometimes often numerical optimal control
[02:12:22.700 --> 02:12:24.660]   in order to make those decisions
[02:12:24.660 --> 02:12:29.020]   and balance these complicated equations of,
[02:12:29.020 --> 02:12:30.820]   and in order to control.
[02:12:30.820 --> 02:12:33.060]   You don't have to use optimal control
[02:12:33.060 --> 02:12:34.820]   to do under-actuated systems,
[02:12:34.820 --> 02:12:36.260]   but that has been the technical approach
[02:12:36.300 --> 02:12:39.060]   that has borne the most fruit in our,
[02:12:39.060 --> 02:12:40.860]   at least in our line of work.
[02:12:40.860 --> 02:12:44.020]   - And there's some, so in under-actuated systems,
[02:12:44.020 --> 02:12:46.780]   when you say let physics do some of the work,
[02:12:46.780 --> 02:12:50.300]   so there's a kind of feedback loop
[02:12:50.300 --> 02:12:54.500]   that observes the state that the physics brought you to.
[02:12:54.500 --> 02:12:57.740]   So like you've, there's a perception there,
[02:12:57.740 --> 02:13:00.260]   there's a feedback somehow.
[02:13:00.260 --> 02:13:05.380]   Do you ever loop in like complicated perception systems
[02:13:05.380 --> 02:13:06.860]   into this whole picture?
[02:13:06.860 --> 02:13:09.580]   - Right, right around the time of the DARPA challenge,
[02:13:09.580 --> 02:13:11.300]   we had a complicated perception system
[02:13:11.300 --> 02:13:12.660]   in the DARPA challenge.
[02:13:12.660 --> 02:13:15.540]   We also started to embrace perception
[02:13:15.540 --> 02:13:17.300]   for our flying vehicles at the time.
[02:13:17.300 --> 02:13:20.060]   We had a really good project
[02:13:20.060 --> 02:13:21.780]   on trying to make airplanes fly
[02:13:21.780 --> 02:13:23.340]   at high speeds through forests.
[02:13:23.340 --> 02:13:27.420]   Sertac Karaman was on that project,
[02:13:27.420 --> 02:13:30.700]   and we had, it was a really fun team to work on.
[02:13:30.700 --> 02:13:34.180]   He's carried it farther, much farther forward since then.
[02:13:34.180 --> 02:13:35.940]   - And that's using cameras for perception?
[02:13:35.940 --> 02:13:37.540]   - So that was using cameras.
[02:13:37.540 --> 02:13:41.740]   That was, at the time, we felt like LIDAR was too heavy
[02:13:41.740 --> 02:13:46.740]   and too power heavy to be carried on a light UAV,
[02:13:46.740 --> 02:13:49.180]   and we were using cameras.
[02:13:49.180 --> 02:13:50.340]   And that was a big part of it,
[02:13:50.340 --> 02:13:53.060]   was just how do you do even stereo matching
[02:13:53.060 --> 02:13:56.140]   at a fast enough rate with a small camera,
[02:13:56.140 --> 02:13:57.580]   a small onboard compute.
[02:13:57.580 --> 02:14:00.660]   Since then, we have now,
[02:14:00.660 --> 02:14:04.100]   so the deep learning revolution unquestionably changed
[02:14:04.100 --> 02:14:08.980]   what we can do with perception for robotics and control.
[02:14:08.980 --> 02:14:10.940]   So in manipulation, we can address,
[02:14:10.940 --> 02:14:14.620]   we can use perception in, I think, a much deeper way.
[02:14:14.620 --> 02:14:17.300]   And we get into not only,
[02:14:17.300 --> 02:14:19.780]   I think the first use of it naturally
[02:14:19.780 --> 02:14:22.900]   would be to ask your deep learning system
[02:14:22.900 --> 02:14:25.940]   to look at the cameras and produce the state,
[02:14:25.940 --> 02:14:28.860]   which is like the pose of my thing, for instance.
[02:14:28.860 --> 02:14:30.420]   But I think we've quickly found out
[02:14:30.420 --> 02:14:33.580]   that that's not always the right thing to do.
[02:14:34.420 --> 02:14:35.620]   - Why is that?
[02:14:35.620 --> 02:14:38.380]   - Because what's the state of my shirt?
[02:14:38.380 --> 02:14:39.220]   Imagine--
[02:14:39.220 --> 02:14:40.860]   - Is it very noisy, you mean?
[02:14:40.860 --> 02:14:46.140]   - If the first step of me trying to button my shirt
[02:14:46.140 --> 02:14:48.580]   is estimate the full state of my shirt,
[02:14:48.580 --> 02:14:50.260]   including what's happening in the back,
[02:14:50.260 --> 02:14:51.780]   you know, whatever, whatever,
[02:14:51.780 --> 02:14:55.780]   that's just not the right specification.
[02:14:55.780 --> 02:14:57.500]   There are aspects of the state
[02:14:57.500 --> 02:15:00.260]   that are very important to the task.
[02:15:00.260 --> 02:15:03.220]   There are many that are unobservable
[02:15:03.220 --> 02:15:05.860]   and not important to the task.
[02:15:05.860 --> 02:15:06.940]   So you really need,
[02:15:06.940 --> 02:15:10.300]   it begs new questions about state representation.
[02:15:10.300 --> 02:15:13.140]   Another example that we've been playing with in lab
[02:15:13.140 --> 02:15:17.660]   has been just the idea of chopping onions, okay?
[02:15:17.660 --> 02:15:19.540]   Or carrots, turns out to be better.
[02:15:19.540 --> 02:15:22.480]   So the onions stink up the lab.
[02:15:22.480 --> 02:15:26.180]   And they're hard to see in a camera.
[02:15:26.180 --> 02:15:27.020]   But--
[02:15:27.020 --> 02:15:27.860]   (Lex laughing)
[02:15:27.860 --> 02:15:28.700]   - The details matter, yeah.
[02:15:28.700 --> 02:15:30.140]   - Details matter, you know?
[02:15:30.140 --> 02:15:30.980]   So--
[02:15:32.380 --> 02:15:33.220]   - Chop and carrot.
[02:15:33.220 --> 02:15:35.180]   - If I'm moving around a particular object, right?
[02:15:35.180 --> 02:15:36.020]   Then I think about,
[02:15:36.020 --> 02:15:37.940]   oh, it's got a position or an orientation in space,
[02:15:37.940 --> 02:15:39.720]   that's the description I want.
[02:15:39.720 --> 02:15:42.660]   Now, when I'm chopping an onion, okay,
[02:15:42.660 --> 02:15:44.220]   the first chop comes down,
[02:15:44.220 --> 02:15:46.740]   I have now 100 pieces of onion.
[02:15:46.740 --> 02:15:50.260]   Does my control system really need to understand
[02:15:50.260 --> 02:15:51.780]   the position and orientation
[02:15:51.780 --> 02:15:53.940]   and even the shape of the 100 pieces of onion
[02:15:53.940 --> 02:15:56.040]   in order to make a decision?
[02:15:56.040 --> 02:15:56.880]   Probably not, you know?
[02:15:56.880 --> 02:15:58.820]   And if I keep going, I'm just getting,
[02:15:58.820 --> 02:16:01.780]   more and more is my state space getting bigger as I cut?
[02:16:02.500 --> 02:16:06.020]   It's not right.
[02:16:06.020 --> 02:16:08.100]   So somehow there's a-- - What do you do?
[02:16:08.100 --> 02:16:13.100]   - I think there's a richer idea of state.
[02:16:13.100 --> 02:16:15.740]   It's not the state that is given to us
[02:16:15.740 --> 02:16:17.180]   by Lagrangian mechanics.
[02:16:17.180 --> 02:16:21.340]   There is a proper Lagrangian state of the system,
[02:16:21.340 --> 02:16:22.900]   but the relevant state for this
[02:16:22.900 --> 02:16:26.460]   is some latent state,
[02:16:26.460 --> 02:16:28.540]   is what we call it in machine learning.
[02:16:28.540 --> 02:16:32.180]   But there's some different state representation.
[02:16:32.180 --> 02:16:34.980]   - Some compressed representation?
[02:16:34.980 --> 02:16:37.220]   - And that's what I worry about saying compressed
[02:16:37.220 --> 02:16:41.420]   because I don't mind that it's low dimensional or not,
[02:16:41.420 --> 02:16:46.220]   but it has to be something that's easier to think about.
[02:16:46.220 --> 02:16:47.340]   - By us humans.
[02:16:47.340 --> 02:16:49.300]   - Or my algorithms.
[02:16:49.300 --> 02:16:53.900]   - Or the algorithms being like control, optimal control.
[02:16:53.900 --> 02:16:56.540]   - So for instance, if the contact mechanics
[02:16:56.540 --> 02:16:58.500]   of all of those onion pieces
[02:16:58.500 --> 02:17:00.580]   and all the permutations of possible touches
[02:17:00.580 --> 02:17:01.940]   between those onion pieces,
[02:17:01.940 --> 02:17:05.100]   you can give me a high dimensional state representation,
[02:17:05.100 --> 02:17:06.820]   I'm okay if it's linear.
[02:17:06.820 --> 02:17:08.700]   But if I have to think about all the possible
[02:17:08.700 --> 02:17:10.780]   shattering combinatorics of that,
[02:17:10.780 --> 02:17:13.900]   then my robot's gonna sit there thinking
[02:17:13.900 --> 02:17:17.380]   and the soup's gonna get cold or something.
[02:17:17.380 --> 02:17:20.140]   - So since you taught the course,
[02:17:20.140 --> 02:17:22.780]   it kinda entered my mind,
[02:17:22.780 --> 02:17:26.020]   the idea of underactuated as really compelling
[02:17:26.020 --> 02:17:29.500]   to see the world in this kind of way.
[02:17:29.500 --> 02:17:32.420]   Do you ever, if we talk about onions
[02:17:32.420 --> 02:17:35.460]   or you talk about the world with people in it in general,
[02:17:35.460 --> 02:17:39.980]   do you see the world as basically an underactuated system?
[02:17:39.980 --> 02:17:42.180]   Do you often look at the world in this way?
[02:17:42.180 --> 02:17:44.740]   Or is this overreach?
[02:17:44.740 --> 02:17:49.100]   - Underactuated is a way of life, man.
[02:17:49.100 --> 02:17:51.420]   - Exactly, I guess that's what I'm asking.
[02:17:51.420 --> 02:17:54.900]   - I do think it's everywhere.
[02:17:54.900 --> 02:17:57.260]   I think in some places,
[02:17:57.260 --> 02:18:01.500]   we already have natural tools to deal with it.
[02:18:01.500 --> 02:18:02.380]   It rears its head.
[02:18:02.380 --> 02:18:04.700]   I mean, in linear systems, it's not a problem.
[02:18:04.700 --> 02:18:07.260]   An underactuated linear system
[02:18:07.260 --> 02:18:08.940]   is really not sufficiently distinct
[02:18:08.940 --> 02:18:10.780]   from a fully actuated linear system.
[02:18:10.780 --> 02:18:15.540]   It's a subtle point about when that becomes a bottleneck
[02:18:15.540 --> 02:18:17.140]   in what we know how to do with control.
[02:18:17.140 --> 02:18:18.780]   It happens to be a bottleneck,
[02:18:18.780 --> 02:18:22.420]   although we've gotten incredibly good solutions now.
[02:18:22.420 --> 02:18:23.740]   But for a long time,
[02:18:23.740 --> 02:18:27.060]   I felt that that was the key bottleneck in Legged Robots.
[02:18:27.060 --> 02:18:29.580]   And roughly now, the underactuated course is,
[02:18:29.580 --> 02:18:33.780]   me trying to tell people everything I can
[02:18:33.780 --> 02:18:36.460]   about how to make Atlas do a backflip.
[02:18:36.460 --> 02:18:39.860]   I have a second course now
[02:18:39.860 --> 02:18:41.220]   that I teach in the other semesters,
[02:18:41.220 --> 02:18:43.540]   which is on manipulation.
[02:18:43.540 --> 02:18:45.820]   And that's where we get into now more of the,
[02:18:45.820 --> 02:18:47.100]   that's a newer class.
[02:18:47.100 --> 02:18:51.540]   I'm hoping to put it online this fall completely.
[02:18:51.540 --> 02:18:53.660]   And that's gonna have much more aspects
[02:18:53.660 --> 02:18:55.420]   about these perception problems
[02:18:55.420 --> 02:18:57.180]   and the state representation questions,
[02:18:57.180 --> 02:18:59.220]   and then how do you do control.
[02:18:59.220 --> 02:19:03.980]   And the thing that's a little bit sad is that,
[02:19:03.980 --> 02:19:07.420]   for me at least, is there's a lot of manipulation tasks
[02:19:07.420 --> 02:19:09.220]   that people wanna do and should wanna do.
[02:19:09.220 --> 02:19:12.700]   They could start a company with it and be very successful
[02:19:12.700 --> 02:19:15.540]   that don't actually require you to think that much
[02:19:15.540 --> 02:19:18.020]   about dynamics at all, even,
[02:19:18.020 --> 02:19:19.980]   but certainly underactuated dynamics.
[02:19:19.980 --> 02:19:23.060]   Once I have, if I reach out and grab something,
[02:19:23.060 --> 02:19:25.660]   if I can sort of assume it's rigidly attached to my hand,
[02:19:25.660 --> 02:19:26.860]   then I can do a lot of interesting,
[02:19:26.860 --> 02:19:28.740]   meaningful things with it
[02:19:28.740 --> 02:19:30.900]   without really ever thinking about the dynamics
[02:19:30.900 --> 02:19:31.940]   of that object.
[02:19:31.940 --> 02:19:34.940]   So we've built systems that kind of
[02:19:34.940 --> 02:19:38.060]   reduce the need for that,
[02:19:38.060 --> 02:19:40.900]   enveloping grasps and the like.
[02:19:40.900 --> 02:19:44.300]   But I think the really good problems in manipulation,
[02:19:44.300 --> 02:19:46.460]   so manipulation, by the way,
[02:19:46.460 --> 02:19:49.740]   is more than just pick and place.
[02:19:49.740 --> 02:19:53.020]   That's like, a lot of people think of that, just grasping.
[02:19:53.020 --> 02:19:55.460]   I don't mean that, I mean buttoning my shirt.
[02:19:55.460 --> 02:19:57.700]   I mean tying shoelaces.
[02:19:57.700 --> 02:19:58.860]   - Tying shoelaces. - How do you program
[02:19:58.860 --> 02:20:00.300]   a robot to tie shoelaces?
[02:20:00.300 --> 02:20:04.020]   And not just one shoe, but every shoe, right?
[02:20:04.020 --> 02:20:06.820]   That's a really good problem.
[02:20:06.820 --> 02:20:08.260]   It's tempting to write down
[02:20:08.260 --> 02:20:11.900]   like the infinite dimensional state of the laces.
[02:20:11.900 --> 02:20:16.140]   That's probably not needed to write a good controller.
[02:20:16.140 --> 02:20:19.460]   I know we could hand design a controller that would do it,
[02:20:19.460 --> 02:20:20.300]   but I don't want that.
[02:20:20.300 --> 02:20:22.860]   I wanna understand the principles
[02:20:22.860 --> 02:20:24.540]   that would allow me to solve another problem
[02:20:24.540 --> 02:20:26.460]   that's kind of like that.
[02:20:26.460 --> 02:20:30.900]   But I think if we can stay pure in our approach,
[02:20:30.900 --> 02:20:34.940]   then the challenge of tying anybody's shoes
[02:20:34.940 --> 02:20:36.260]   is a great challenge.
[02:20:36.260 --> 02:20:37.180]   - That's a great challenge.
[02:20:37.180 --> 02:20:40.900]   I mean, and the soft touch comes into play there.
[02:20:40.900 --> 02:20:42.180]   That's really interesting.
[02:20:42.180 --> 02:20:46.220]   Let me ask another ridiculous question on this topic.
[02:20:47.500 --> 02:20:49.780]   How important is touch?
[02:20:49.780 --> 02:20:52.300]   We haven't talked much about humans,
[02:20:52.300 --> 02:20:54.780]   but I have this argument with my dad
[02:20:54.780 --> 02:20:59.620]   where like I think you can fall in love with a robot
[02:20:59.620 --> 02:21:02.580]   based on language alone.
[02:21:02.580 --> 02:21:05.360]   And he believes that touch is essential.
[02:21:05.360 --> 02:21:07.780]   Touch and smell, he says.
[02:21:07.780 --> 02:21:14.860]   So in terms of robots, you know, connecting with humans,
[02:21:16.540 --> 02:21:18.660]   and we can go philosophical
[02:21:18.660 --> 02:21:21.820]   in terms of like a deep, meaningful connection, like love,
[02:21:21.820 --> 02:21:25.540]   but even just like collaborating in an interesting way,
[02:21:25.540 --> 02:21:26.900]   how important is touch?
[02:21:26.900 --> 02:21:32.820]   From an engineering perspective and a philosophical one.
[02:21:32.820 --> 02:21:34.460]   - I think it's super important.
[02:21:34.460 --> 02:21:37.060]   Even just in a practical sense,
[02:21:37.060 --> 02:21:39.260]   if we forget about the emotional part of it,
[02:21:39.260 --> 02:21:43.340]   but for robots to interact safely
[02:21:43.340 --> 02:21:46.420]   while they're doing meaningful mechanical work
[02:21:46.420 --> 02:21:52.260]   in the close contact with or vicinity of people
[02:21:52.260 --> 02:21:55.260]   that need help, I think we have to have them,
[02:21:55.260 --> 02:21:57.540]   we have to build them differently.
[02:21:57.540 --> 02:21:59.900]   They have to be afraid, not afraid of touching the world.
[02:21:59.900 --> 02:22:02.860]   So I think Baymax is just awesome.
[02:22:02.860 --> 02:22:06.300]   That's just like the movie of "Big Hero 6"
[02:22:06.300 --> 02:22:08.740]   and the concept of Baymax, that's just awesome.
[02:22:08.740 --> 02:22:13.060]   I think we should, and we have some folks at Toyota
[02:22:13.060 --> 02:22:14.460]   that are trying to, Toyota Research,
[02:22:14.460 --> 02:22:16.900]   that are trying to build Baymax roughly.
[02:22:16.900 --> 02:22:21.900]   And I think it's just a fantastically good project.
[02:22:21.900 --> 02:22:25.660]   I think it will change the way people physically interact.
[02:22:25.660 --> 02:22:28.020]   The same way, I mean, you gave a couple examples earlier,
[02:22:28.020 --> 02:22:31.980]   but if the robot that was walking around my home
[02:22:31.980 --> 02:22:34.020]   looked more like a teddy bear
[02:22:34.020 --> 02:22:36.020]   and a little less like the Terminator,
[02:22:36.020 --> 02:22:38.940]   that could change completely the way people perceive it
[02:22:38.940 --> 02:22:39.860]   and interact with it.
[02:22:39.860 --> 02:22:43.300]   And maybe they'll even wanna teach it, like you said.
[02:22:43.300 --> 02:22:47.700]   You could not quite gamify it,
[02:22:47.700 --> 02:22:50.100]   but somehow instead of people judging it
[02:22:50.100 --> 02:22:54.380]   and looking at it as if it's not doing as well as a human,
[02:22:54.380 --> 02:22:57.100]   they're gonna try to help out the cute teddy bear.
[02:22:57.100 --> 02:22:57.940]   Who knows?
[02:22:57.940 --> 02:23:01.260]   But I think we're building robots wrong
[02:23:01.260 --> 02:23:06.260]   and being more soft and more contact is important.
[02:23:07.820 --> 02:23:09.900]   - Yeah, like all the magical moments
[02:23:09.900 --> 02:23:12.380]   I can remember with robots.
[02:23:12.380 --> 02:23:16.000]   Well, first of all, just visiting your lab and seeing Atlas,
[02:23:16.000 --> 02:23:18.300]   but also Spot Mini.
[02:23:18.300 --> 02:23:21.700]   When I first saw Spot Mini in person
[02:23:21.700 --> 02:23:26.300]   and hung out with him, her, it,
[02:23:26.300 --> 02:23:28.380]   I don't have trouble engendering robots.
[02:23:28.380 --> 02:23:31.500]   I feel robotics people really always say it.
[02:23:31.500 --> 02:23:34.480]   I kinda like the idea that it's a her or him.
[02:23:35.820 --> 02:23:38.820]   It's a magical moment, but there's no touching.
[02:23:38.820 --> 02:23:41.660]   I guess the question I have, have you ever been,
[02:23:41.660 --> 02:23:44.980]   like, have you had a human robot experience
[02:23:44.980 --> 02:23:47.980]   where a robot touched you?
[02:23:47.980 --> 02:23:53.020]   And it was like, wait, was there a moment
[02:23:53.020 --> 02:23:56.100]   that you've forgotten that a robot is a robot?
[02:23:56.100 --> 02:24:00.860]   And the anthropomorphization stepped in
[02:24:00.860 --> 02:24:03.520]   and for a second you forgot that it's not human?
[02:24:04.940 --> 02:24:07.860]   - I mean, I think when you're in on the details,
[02:24:07.860 --> 02:24:12.400]   then we of course anthropomorphized our work with Atlas,
[02:24:12.400 --> 02:24:17.140]   but in verbal communication and the like,
[02:24:17.140 --> 02:24:20.460]   I think we were pretty aware of it as a machine
[02:24:20.460 --> 02:24:21.860]   that needed to be respected.
[02:24:21.860 --> 02:24:26.300]   I actually, I worry more about the smaller robots
[02:24:26.300 --> 02:24:29.580]   that could still move quickly if programmed wrong
[02:24:29.580 --> 02:24:32.260]   and we have to be careful actually about safety
[02:24:32.260 --> 02:24:33.780]   and the like right now.
[02:24:33.780 --> 02:24:36.420]   And that, if we build our robots correctly,
[02:24:36.420 --> 02:24:40.340]   I think then a lot of those concerns could go away.
[02:24:40.340 --> 02:24:41.300]   And we're seeing that trend.
[02:24:41.300 --> 02:24:44.140]   We're seeing the lower cost, lighter weight arms now
[02:24:44.140 --> 02:24:46.760]   that could be fundamentally safe.
[02:24:46.760 --> 02:24:52.180]   I mean, I do think touch is so fundamental.
[02:24:52.180 --> 02:24:54.340]   Ted Adelson is great.
[02:24:54.340 --> 02:24:57.580]   He's a perceptual scientist at MIT
[02:24:57.580 --> 02:25:01.420]   and he studied vision most of his life.
[02:25:01.420 --> 02:25:04.380]   And he said, "When I had kids,
[02:25:04.380 --> 02:25:06.040]   "I expected to be fascinated
[02:25:06.040 --> 02:25:07.940]   "by their perceptual development."
[02:25:07.940 --> 02:25:12.580]   But what really, what he noticed was,
[02:25:12.580 --> 02:25:14.140]   felt more impressive, more dominant
[02:25:14.140 --> 02:25:16.220]   was the way that they would touch everything
[02:25:16.220 --> 02:25:17.780]   and lick everything and pick things up,
[02:25:17.780 --> 02:25:19.420]   stick it on their tongue and whatever.
[02:25:19.420 --> 02:25:22.500]   And he said, watching his daughter
[02:25:22.500 --> 02:25:26.620]   convinced him that actually he needed to study
[02:25:26.620 --> 02:25:28.580]   tactile sensing more.
[02:25:28.580 --> 02:25:33.540]   So there's something very important.
[02:25:33.540 --> 02:25:35.780]   I think it's a little bit also of the passive
[02:25:35.780 --> 02:25:38.660]   versus active part of the world, right?
[02:25:38.660 --> 02:25:41.460]   You can passively perceive the world,
[02:25:41.460 --> 02:25:45.060]   but it's fundamentally different
[02:25:45.060 --> 02:25:47.140]   if you can do an experiment, right?
[02:25:47.140 --> 02:25:48.820]   And if you can change the world.
[02:25:48.820 --> 02:25:51.660]   And you can learn a lot more than a passive observer.
[02:25:51.660 --> 02:25:56.940]   So you can, in dialogue, that was your initial example,
[02:25:56.940 --> 02:26:00.060]   you could have an active experiment exchange.
[02:26:00.060 --> 02:26:03.020]   But I think if you're just a camera watching YouTube,
[02:26:03.020 --> 02:26:05.860]   I think that's a very different problem
[02:26:05.860 --> 02:26:10.720]   than if you're a robot that can apply force and touch.
[02:26:10.720 --> 02:26:15.500]   I think it's important.
[02:26:15.500 --> 02:26:17.980]   - Yeah, I think it's just an exciting area of research.
[02:26:17.980 --> 02:26:19.220]   I think you're probably right
[02:26:19.220 --> 02:26:21.460]   that this hasn't been under-researched.
[02:26:21.460 --> 02:26:25.740]   To me, as a person who's captivated
[02:26:25.740 --> 02:26:27.780]   by the idea of human robot interaction,
[02:26:27.780 --> 02:26:32.780]   it feels like such a rich opportunity to explore touch.
[02:26:32.780 --> 02:26:35.860]   Not even from a safety perspective,
[02:26:35.860 --> 02:26:38.060]   but like you said, the emotional too.
[02:26:38.060 --> 02:26:39.700]   I mean, safety comes first,
[02:26:39.700 --> 02:26:46.260]   but the next step is like a real human connection.
[02:26:46.260 --> 02:26:51.420]   Even in the industrial setting,
[02:26:51.420 --> 02:26:55.580]   it just feels like it's nice for the robot.
[02:26:55.580 --> 02:26:58.220]   - I don't know, you might disagree with this,
[02:26:58.220 --> 02:27:04.420]   'cause I think it's important to see robots as tools often,
[02:27:04.420 --> 02:27:06.140]   but I don't know.
[02:27:06.140 --> 02:27:08.620]   I think they're just always going to be more effective
[02:27:08.620 --> 02:27:10.220]   once you humanize them.
[02:27:10.220 --> 02:27:14.420]   It's convenient now to think of them as tools
[02:27:14.420 --> 02:27:16.220]   'cause we wanna focus on the safety,
[02:27:16.220 --> 02:27:21.220]   but I think ultimately to create a good experience
[02:27:21.220 --> 02:27:24.940]   for the worker, for the person,
[02:27:24.940 --> 02:27:28.020]   there has to be a human element.
[02:27:28.020 --> 02:27:29.260]   I don't know, for me.
[02:27:29.260 --> 02:27:33.140]   It feels like an industrial robotic arm
[02:27:33.140 --> 02:27:34.900]   would be better if it has a human element.
[02:27:34.900 --> 02:27:37.060]   I think like we think robotics had that idea
[02:27:37.060 --> 02:27:40.300]   with Baxter and having eyes and so on.
[02:27:40.300 --> 02:27:43.100]   I don't know, I'm a big believer in that.
[02:27:43.100 --> 02:27:48.100]   - It's not my area, but I am also a big believer.
[02:27:48.100 --> 02:27:51.640]   - Do you have an emotional connection to Atlas?
[02:27:51.640 --> 02:27:53.820]   (laughing)
[02:27:53.820 --> 02:27:55.020]   Like, do you miss him?
[02:27:55.020 --> 02:27:56.020]   - I mean, yes.
[02:27:56.020 --> 02:28:03.460]   I don't know if I more so than if I had
[02:28:03.460 --> 02:28:06.300]   a different science project that I'd worked on super hard.
[02:28:06.300 --> 02:28:11.300]   But yeah, I mean, the robot,
[02:28:11.300 --> 02:28:14.820]   we basically had to do heart surgery on the robot
[02:28:14.820 --> 02:28:17.500]   in the final competition 'cause we melted the core.
[02:28:17.500 --> 02:28:23.180]   Yeah, there was something about watching that robot
[02:28:23.180 --> 02:28:24.940]   hanging there, we know we had to compete with it
[02:28:24.940 --> 02:28:28.060]   in an hour and it was getting its guts ripped out.
[02:28:28.060 --> 02:28:30.540]   - Those are all historic moments.
[02:28:30.540 --> 02:28:32.980]   I think if you look back like 100 years from now,
[02:28:32.980 --> 02:28:38.560]   yeah, I think those are important moments in robotics.
[02:28:38.560 --> 02:28:40.020]   I mean, these are the early days.
[02:28:40.020 --> 02:28:41.220]   You look at like the early days
[02:28:41.220 --> 02:28:42.780]   of a lot of scientific disciplines,
[02:28:42.780 --> 02:28:45.380]   they look ridiculous, it's full of failure.
[02:28:45.380 --> 02:28:48.340]   But it feels like robotics will be important
[02:28:48.340 --> 02:28:52.460]   in the coming 100 years.
[02:28:52.460 --> 02:28:54.060]   And these are the early days.
[02:28:54.060 --> 02:29:00.140]   So I think a lot of people look at a brilliant person
[02:29:00.140 --> 02:29:02.260]   such as yourself and are curious
[02:29:02.260 --> 02:29:05.180]   about the intellectual journey they've took.
[02:29:05.180 --> 02:29:09.600]   Is there maybe three books, technical, fiction,
[02:29:09.600 --> 02:29:13.860]   philosophical that had a big impact on your life
[02:29:13.860 --> 02:29:16.760]   that you would recommend perhaps others reading?
[02:29:16.760 --> 02:29:21.740]   - Yeah, so I actually didn't read that much as a kid,
[02:29:21.740 --> 02:29:24.380]   but I read fairly voraciously now.
[02:29:24.380 --> 02:29:28.860]   There are some recent books that if you're interested
[02:29:28.860 --> 02:29:33.860]   in this kind of topic, like "AI Superpowers" by Kai-Fu Lee
[02:29:33.860 --> 02:29:36.940]   is just a fantastic read, you must read that.
[02:29:36.940 --> 02:29:44.060]   Yuval Harari, I think that can open your mind.
[02:29:44.060 --> 02:29:48.060]   - "Sapiens." - "Sapiens" is the first one.
[02:29:48.060 --> 02:29:49.880]   "Homo Deus" is the second, yeah.
[02:29:51.020 --> 02:29:53.500]   We mentioned "The Black Swan" by Taleb.
[02:29:53.500 --> 02:29:56.080]   I think that's a good sort of mind opener.
[02:29:56.080 --> 02:30:02.220]   I actually, so there's maybe
[02:30:02.220 --> 02:30:06.220]   a more controversial recommendation I could give.
[02:30:06.220 --> 02:30:08.740]   - Great, we love controversial.
[02:30:08.740 --> 02:30:11.580]   - In some sense, it's so classical it might surprise you.
[02:30:11.580 --> 02:30:15.980]   But I actually recently read Mortimer Adler's
[02:30:15.980 --> 02:30:17.600]   "How to Read a Book."
[02:30:17.600 --> 02:30:19.060]   Not so long, it was a while ago.
[02:30:19.060 --> 02:30:22.340]   But some people hate that book.
[02:30:22.340 --> 02:30:24.860]   I loved it.
[02:30:24.860 --> 02:30:29.020]   I think we're in this time right now where,
[02:30:29.020 --> 02:30:33.820]   boy, we're just inundated with research papers
[02:30:33.820 --> 02:30:38.620]   that you could read on archive with limited peer review
[02:30:38.620 --> 02:30:41.020]   and just this wealth of information.
[02:30:41.020 --> 02:30:47.140]   I don't know, I think the passion
[02:30:47.140 --> 02:30:50.060]   of what you can get out of a book,
[02:30:50.060 --> 02:30:53.880]   a really good book or a really good paper if you find it,
[02:30:53.880 --> 02:30:56.080]   the attitude, the realization that you're only gonna find
[02:30:56.080 --> 02:30:58.520]   a few that really are worth all your time.
[02:30:58.520 --> 02:31:02.540]   But then once you find them, you should just dig in
[02:31:02.540 --> 02:31:06.380]   and understand it very deeply and it's worth
[02:31:06.380 --> 02:31:11.300]   marking it up and having the hard copy,
[02:31:11.300 --> 02:31:14.780]   writing in the side notes, side margins.
[02:31:16.020 --> 02:31:17.260]   I think that was really,
[02:31:17.260 --> 02:31:21.420]   I read it at the right time where I was just feeling
[02:31:21.420 --> 02:31:26.260]   just overwhelmed with really low quality stuff, I guess.
[02:31:26.260 --> 02:31:32.420]   And similarly, I'm giving more than three now.
[02:31:32.420 --> 02:31:35.300]   I'm sorry if I've exceeded my quota.
[02:31:35.300 --> 02:31:38.180]   - But on that topic just real quick is,
[02:31:38.180 --> 02:31:41.700]   so basically finding a few companions
[02:31:41.700 --> 02:31:43.740]   to keep for the rest of your life
[02:31:43.740 --> 02:31:45.800]   in terms of papers and books and so on.
[02:31:45.800 --> 02:31:49.420]   And those are the ones, like not doing,
[02:31:49.420 --> 02:31:52.880]   what is it, FOMO, fear of missing out,
[02:31:52.880 --> 02:31:54.820]   constantly trying to update yourself,
[02:31:54.820 --> 02:31:57.540]   but really deeply making a life journey
[02:31:57.540 --> 02:32:01.300]   of studying a particular paper essentially, set of papers.
[02:32:01.300 --> 02:32:06.060]   - Yeah, I think when you really find something,
[02:32:06.060 --> 02:32:07.740]   which a book that resonates with you
[02:32:07.740 --> 02:32:10.400]   might not be the same book that resonates with me,
[02:32:10.400 --> 02:32:13.140]   but when you really find one that resonates with you,
[02:32:13.140 --> 02:32:15.180]   I think the dialogue that happens,
[02:32:15.180 --> 02:32:18.260]   and that's what I loved that Adler was saying,
[02:32:18.260 --> 02:32:20.560]   I think Socrates and Plato say,
[02:32:20.560 --> 02:32:25.700]   the written word is never gonna capture
[02:32:25.700 --> 02:32:28.000]   the beauty of dialogue, right?
[02:32:28.000 --> 02:32:29.200]   But Adler says, no, no,
[02:32:29.200 --> 02:32:35.340]   a really good book is a dialogue between you and the author,
[02:32:35.340 --> 02:32:37.300]   and it crosses time and space.
[02:32:37.300 --> 02:32:40.660]   I don't know, I think it's a very romantic,
[02:32:40.660 --> 02:32:42.720]   there's a bunch of specific advice,
[02:32:42.720 --> 02:32:44.360]   which you can just gloss over,
[02:32:44.360 --> 02:32:47.220]   but the romantic view of how to read
[02:32:47.220 --> 02:32:51.020]   and really appreciate it is so good.
[02:32:51.020 --> 02:32:53.860]   And similarly, teaching,
[02:32:53.860 --> 02:32:58.860]   I thought a lot about teaching,
[02:32:58.860 --> 02:33:03.260]   so Isaac Asimov, great science fiction writer,
[02:33:03.260 --> 02:33:05.300]   has also actually spent a lot of his career
[02:33:05.300 --> 02:33:07.220]   writing nonfiction, right?
[02:33:07.220 --> 02:33:08.720]   His memoir is fantastic.
[02:33:08.720 --> 02:33:12.700]   He was passionate about explaining things, right?
[02:33:12.700 --> 02:33:14.600]   He wrote all kinds of books on all kinds of topics
[02:33:14.600 --> 02:33:17.700]   in science, he was known as the great explainer.
[02:33:17.700 --> 02:33:22.340]   And I do really resonate with his style
[02:33:22.340 --> 02:33:27.100]   and just his way of talking about,
[02:33:27.100 --> 02:33:30.520]   by communicating and explaining to something
[02:33:30.520 --> 02:33:32.500]   is really the way that you learn something.
[02:33:32.500 --> 02:33:36.220]   I think about problems very differently
[02:33:36.220 --> 02:33:39.180]   because of the way I've been given the opportunity
[02:33:39.180 --> 02:33:43.540]   to teach them at MIT, and have questions asked,
[02:33:43.540 --> 02:33:47.660]   the fear of the lecture, the experience of the lecture
[02:33:47.660 --> 02:33:50.200]   and the questions I get and the interactions
[02:33:50.200 --> 02:33:53.140]   just forces me to be rock solid on these ideas
[02:33:53.140 --> 02:33:55.040]   in a way that if I didn't have that,
[02:33:55.040 --> 02:33:58.260]   I don't know I would be in a different intellectual space.
[02:33:58.260 --> 02:34:00.420]   - Also, video, does that scare you
[02:34:00.420 --> 02:34:02.140]   that your lectures are online,
[02:34:02.140 --> 02:34:04.660]   and people like me in sweatpants can sit,
[02:34:04.660 --> 02:34:07.940]   sipping coffee and watch you give lectures?
[02:34:07.940 --> 02:34:09.140]   - I think it's great.
[02:34:09.140 --> 02:34:12.820]   I do think that something's changed right now,
[02:34:12.820 --> 02:34:16.900]   which is, right now we're giving lectures over Zoom,
[02:34:16.900 --> 02:34:19.300]   I mean, giving seminars over Zoom and everything.
[02:34:19.300 --> 02:34:24.380]   I'm trying to figure out, I think it's a new medium.
[02:34:24.380 --> 02:34:25.540]   - Do you think it's-- - I'm trying to figure out
[02:34:25.540 --> 02:34:28.020]   how to exploit it. - How to exploit it.
[02:34:28.020 --> 02:34:33.020]   Yeah, I've been quite cynical
[02:34:34.500 --> 02:34:39.500]   about the human to human connection over that medium,
[02:34:39.500 --> 02:34:43.580]   but I think that's because it hasn't been explored fully,
[02:34:43.580 --> 02:34:45.820]   and teaching is a different thing.
[02:34:45.820 --> 02:34:49.140]   - Every lecture is a, I'm sorry, every seminar even,
[02:34:49.140 --> 02:34:51.020]   I think every talk I give,
[02:34:51.020 --> 02:34:55.020]   is an opportunity to give that differently.
[02:34:55.020 --> 02:34:57.980]   I can deliver content directly into your browser.
[02:34:57.980 --> 02:35:00.060]   You have a WebGL engine right there.
[02:35:00.060 --> 02:35:04.940]   I can throw 3D content into your browser
[02:35:04.940 --> 02:35:06.940]   while you're listening to me, right?
[02:35:06.940 --> 02:35:08.700]   And I can assume that you have a,
[02:35:08.700 --> 02:35:11.780]   at least a powerful enough laptop or something
[02:35:11.780 --> 02:35:13.740]   to watch Zoom while I'm doing that,
[02:35:13.740 --> 02:35:15.100]   while I'm giving a lecture.
[02:35:15.100 --> 02:35:18.100]   That's a new communication tool
[02:35:18.100 --> 02:35:19.980]   that I didn't have last year, right?
[02:35:19.980 --> 02:35:24.220]   And I think robotics can potentially benefit a lot
[02:35:24.220 --> 02:35:25.340]   from teaching that way.
[02:35:25.340 --> 02:35:28.220]   We'll see, it's gonna be an experiment this fall.
[02:35:28.220 --> 02:35:29.420]   - It's interesting. - I'm thinking a lot
[02:35:29.420 --> 02:35:30.420]   about it.
[02:35:30.420 --> 02:35:35.420]   - Yeah, and also the length of lectures,
[02:35:35.420 --> 02:35:38.900]   or the length of, there's something,
[02:35:38.900 --> 02:35:42.980]   so I guarantee you, 80% of people
[02:35:42.980 --> 02:35:44.980]   who started listening to our conversation
[02:35:44.980 --> 02:35:48.020]   are still listening to now, which is crazy to me.
[02:35:48.020 --> 02:35:52.740]   There's a patience and an interest in long-form content,
[02:35:52.740 --> 02:35:55.300]   but at the same time, there's a magic
[02:35:55.300 --> 02:35:58.020]   to forcing yourself to condense,
[02:35:58.020 --> 02:36:01.300]   an idea to a short as possible,
[02:36:01.300 --> 02:36:04.740]   shortest possible like clip.
[02:36:04.740 --> 02:36:06.260]   It can be a part of a longer thing,
[02:36:06.260 --> 02:36:09.700]   but like just a really beautifully condensed an idea.
[02:36:09.700 --> 02:36:11.980]   There's a lot of opportunity there
[02:36:11.980 --> 02:36:14.660]   that's easier to do in remote with,
[02:36:14.660 --> 02:36:19.100]   I don't know, with editing too.
[02:36:19.100 --> 02:36:20.860]   Editing is an interesting thing.
[02:36:20.860 --> 02:36:25.100]   Well, most professors don't get,
[02:36:25.100 --> 02:36:25.940]   when they give a lecture,
[02:36:25.940 --> 02:36:28.300]   they don't get to go back and edit out parts,
[02:36:28.300 --> 02:36:31.660]   like crisp it up a little bit.
[02:36:31.660 --> 02:36:34.260]   That's also, it can do magic.
[02:36:34.260 --> 02:36:37.700]   Like if you remove like five to 10 minutes
[02:36:37.700 --> 02:36:41.220]   from an hour lecture, it can actually,
[02:36:41.220 --> 02:36:43.300]   it can make something special of a lecture.
[02:36:43.300 --> 02:36:47.940]   I've seen that in myself and in others too,
[02:36:47.940 --> 02:36:50.660]   'cause I edit other people's lectures to extract clips.
[02:36:50.660 --> 02:36:52.820]   It's like there's certain tangents that are like,
[02:36:52.820 --> 02:36:54.500]   that lose, they're not interesting.
[02:36:54.500 --> 02:36:57.260]   They're mumbling, they're just not,
[02:36:57.260 --> 02:36:59.900]   they're not clarifying, they're not helpful at all.
[02:36:59.900 --> 02:37:02.940]   And once you remove them, it's just, I don't know.
[02:37:02.940 --> 02:37:04.660]   Editing can be magic.
[02:37:04.660 --> 02:37:05.980]   - It can take a lot of time.
[02:37:05.980 --> 02:37:09.060]   - Yeah, it takes, it depends like what is teaching,
[02:37:09.060 --> 02:37:09.900]   you have to ask.
[02:37:09.900 --> 02:37:10.740]   - Yeah.
[02:37:10.740 --> 02:37:14.580]   - Yeah, 'cause I find the editing process
[02:37:14.580 --> 02:37:19.580]   is also beneficial as for teaching,
[02:37:19.580 --> 02:37:21.700]   but also for your own learning.
[02:37:21.700 --> 02:37:23.780]   I don't know if, have you watched yourself?
[02:37:23.780 --> 02:37:24.620]   - Yeah, sure.
[02:37:24.620 --> 02:37:26.220]   - Have you watched those videos?
[02:37:26.220 --> 02:37:28.380]   - I mean, not all of them.
[02:37:28.380 --> 02:37:33.380]   - It could be painful to see how to improve.
[02:37:33.380 --> 02:37:37.260]   - So do you find that, I know you segment your podcast.
[02:37:37.260 --> 02:37:39.700]   Do you think that helps people
[02:37:39.700 --> 02:37:42.220]   with the attention span aspect of it?
[02:37:42.220 --> 02:37:43.060]   Or is it--
[02:37:43.060 --> 02:37:44.260]   - Segment like sections like--
[02:37:44.260 --> 02:37:46.380]   - Yeah, we're talking about this topic, whatever.
[02:37:46.380 --> 02:37:48.300]   - Nope, nope, that just helps me.
[02:37:48.300 --> 02:37:49.460]   It's actually bad.
[02:37:49.460 --> 02:37:52.940]   So, and you've been incredible.
[02:37:53.860 --> 02:37:56.460]   So I'm learning, like I'm afraid of conversation.
[02:37:56.460 --> 02:37:59.180]   This is even today, I'm terrified of talking to you.
[02:37:59.180 --> 02:38:03.540]   I mean, it's something I'm trying to remove from myself.
[02:38:03.540 --> 02:38:07.460]   A guy, I mean, I learned from a lot of people,
[02:38:07.460 --> 02:38:10.780]   but really there's been a few people
[02:38:10.780 --> 02:38:14.140]   who's been inspirational to me in terms of conversation.
[02:38:14.140 --> 02:38:16.340]   Whatever people think of him, Joe Rogan
[02:38:16.340 --> 02:38:17.540]   has been inspirational to me
[02:38:17.540 --> 02:38:20.540]   because comedians have been too.
[02:38:20.540 --> 02:38:23.340]   Being able to just have fun and enjoy themselves
[02:38:23.340 --> 02:38:25.620]   and lose themselves in conversation,
[02:38:25.620 --> 02:38:28.860]   that requires you to be a great storyteller,
[02:38:28.860 --> 02:38:31.500]   to be able to pull a lot of different pieces
[02:38:31.500 --> 02:38:32.860]   of information together,
[02:38:32.860 --> 02:38:36.540]   but mostly just to enjoy yourself in conversations.
[02:38:36.540 --> 02:38:38.100]   And I'm trying to learn that.
[02:38:38.100 --> 02:38:41.700]   These notes are, you see me looking down,
[02:38:41.700 --> 02:38:43.060]   that's like a safety blanket
[02:38:43.060 --> 02:38:45.300]   that I'm trying to let go of more and more.
[02:38:45.300 --> 02:38:46.300]   - Cool.
[02:38:46.300 --> 02:38:49.500]   - So that's, people love just regular conversation.
[02:38:49.500 --> 02:38:52.740]   That's what they, the structure is like, whatever.
[02:38:52.740 --> 02:38:57.700]   I would say maybe like 10 to,
[02:38:57.700 --> 02:38:58.740]   so there's a bunch of,
[02:38:58.740 --> 02:39:03.860]   there's probably a couple thousand PhD students
[02:39:03.860 --> 02:39:07.020]   listening to this right now, right?
[02:39:07.020 --> 02:39:09.580]   And they might know what we're talking about,
[02:39:09.580 --> 02:39:14.580]   but there is somebody, I guarantee you right now in Russia,
[02:39:14.580 --> 02:39:18.020]   some kid who's just like, who's just smoked some weed,
[02:39:18.020 --> 02:39:20.580]   sitting back and just enjoying
[02:39:20.580 --> 02:39:22.540]   the hell out of this conversation.
[02:39:22.540 --> 02:39:23.820]   Not really understanding,
[02:39:23.820 --> 02:39:25.940]   he kind of watched some Boston Dynamics videos.
[02:39:25.940 --> 02:39:27.300]   He's just enjoying it.
[02:39:27.300 --> 02:39:30.260]   And I salute you, sir.
[02:39:30.260 --> 02:39:33.820]   No, but just like, there's so much variety of people
[02:39:33.820 --> 02:39:36.180]   that just have curiosity about engineering,
[02:39:36.180 --> 02:39:38.980]   about sciences, about mathematics.
[02:39:38.980 --> 02:39:41.380]   And also like I should,
[02:39:41.380 --> 02:39:45.980]   I mean, enjoying it is one thing,
[02:39:45.980 --> 02:39:50.140]   but I also often notice it inspires people to,
[02:39:50.140 --> 02:39:51.860]   there's a lot of people who are like
[02:39:51.860 --> 02:39:54.780]   in their undergraduate studies trying to figure out what,
[02:39:54.780 --> 02:39:57.100]   trying to figure out what to pursue.
[02:39:57.100 --> 02:39:59.940]   And these conversations can really spark
[02:39:59.940 --> 02:40:02.620]   the direction of their life.
[02:40:02.620 --> 02:40:04.460]   And in terms of robotics, I hope it does,
[02:40:04.460 --> 02:40:07.380]   'cause I'm excited about the possibilities
[02:40:07.380 --> 02:40:08.500]   of what robotics brings.
[02:40:08.500 --> 02:40:13.380]   On that topic, do you have advice?
[02:40:13.380 --> 02:40:17.700]   Like what advice would you give to a young person about life?
[02:40:17.700 --> 02:40:20.140]   - A young person about life
[02:40:20.140 --> 02:40:22.940]   or a young person about life in robotics?
[02:40:22.940 --> 02:40:25.100]   - It could be in robotics.
[02:40:25.100 --> 02:40:26.580]   It could be in life in general.
[02:40:26.580 --> 02:40:28.380]   It could be career.
[02:40:28.380 --> 02:40:31.260]   It could be relationship advice.
[02:40:31.260 --> 02:40:32.820]   It could be running advice.
[02:40:32.820 --> 02:40:36.540]   Just like, that's one of the things I see,
[02:40:36.540 --> 02:40:38.580]   like we talked to like 20 year olds.
[02:40:38.580 --> 02:40:42.420]   They're like, how do I do this thing?
[02:40:42.420 --> 02:40:43.940]   What do I do?
[02:40:43.940 --> 02:40:48.020]   If they come up to you, what would you tell them?
[02:40:48.020 --> 02:40:52.540]   - I think it's an interesting time to be a kid these days.
[02:40:52.540 --> 02:40:57.220]   Everything points to this being
[02:40:57.220 --> 02:40:59.340]   sort of a winner take all economy and the like.
[02:40:59.340 --> 02:41:04.340]   I think the people that will really excel, in my opinion,
[02:41:04.340 --> 02:41:08.400]   are gonna be the ones that can think deeply about problems.
[02:41:11.240 --> 02:41:13.980]   You have to be able to ask questions agilely
[02:41:13.980 --> 02:41:15.860]   and use the internet for everything it's good for
[02:41:15.860 --> 02:41:16.700]   and stuff like this.
[02:41:16.700 --> 02:41:19.460]   And I think a lot of people will develop those skills.
[02:41:19.460 --> 02:41:24.460]   I think the leaders, thought leaders,
[02:41:24.460 --> 02:41:26.820]   robotics leaders, whatever,
[02:41:26.820 --> 02:41:29.060]   are gonna be the ones that can do more
[02:41:29.060 --> 02:41:31.300]   and they can think very deeply and critically.
[02:41:31.300 --> 02:41:34.980]   And that's a harder thing to learn.
[02:41:34.980 --> 02:41:38.100]   I think one path to learning that is through mathematics,
[02:41:38.100 --> 02:41:39.100]   through engineering.
[02:41:39.520 --> 02:41:44.160]   I would encourage people to start math early.
[02:41:44.160 --> 02:41:46.880]   I mean, I didn't really start.
[02:41:46.880 --> 02:41:50.460]   I mean, I was always in the better math classes
[02:41:50.460 --> 02:41:51.300]   that I could take,
[02:41:51.300 --> 02:41:54.720]   but I wasn't pursuing super advanced mathematics
[02:41:54.720 --> 02:41:56.680]   or anything like that until I got to MIT.
[02:41:56.680 --> 02:41:59.000]   I think MIT lit me up
[02:41:59.000 --> 02:42:04.000]   and really started the life that I'm living now.
[02:42:05.600 --> 02:42:10.600]   But yeah, I really want kids to dig deep,
[02:42:10.600 --> 02:42:12.460]   really understand things, building things too.
[02:42:12.460 --> 02:42:15.180]   I mean, pull things apart, put them back together.
[02:42:15.180 --> 02:42:17.160]   Like that's just such a good way
[02:42:17.160 --> 02:42:19.980]   to really understand things
[02:42:19.980 --> 02:42:23.780]   and expect it to be a long journey, right?
[02:42:23.780 --> 02:42:27.260]   You don't have to know everything.
[02:42:27.260 --> 02:42:29.500]   You're never gonna know everything.
[02:42:29.500 --> 02:42:31.480]   - So think deeply and stick with it.
[02:42:31.480 --> 02:42:33.940]   - Enjoy the ride,
[02:42:33.940 --> 02:42:38.060]   but just make sure you're not, yeah.
[02:42:38.060 --> 02:42:40.580]   Just make sure you're stopping
[02:42:40.580 --> 02:42:43.220]   to think about why things work.
[02:42:43.220 --> 02:42:44.060]   - It's true.
[02:42:44.060 --> 02:42:45.420]   It's easy to lose yourself
[02:42:45.420 --> 02:42:49.120]   in the distractions of the world.
[02:42:49.120 --> 02:42:52.740]   - We're overwhelmed with content right now,
[02:42:52.740 --> 02:42:56.260]   but you have to stop and pick some of it
[02:42:56.260 --> 02:42:57.880]   and really understand.
[02:42:57.880 --> 02:43:00.380]   - Yeah, on the book point,
[02:43:00.380 --> 02:43:04.980]   I've read "Animal Farm" by George Orwell
[02:43:04.980 --> 02:43:06.180]   a ridiculous number of times.
[02:43:06.180 --> 02:43:07.900]   So for me, like that book,
[02:43:07.900 --> 02:43:09.780]   I don't know if it's a good book in general,
[02:43:09.780 --> 02:43:12.200]   but for me, it connects deeply somehow.
[02:43:12.200 --> 02:43:15.080]   It somehow connects.
[02:43:15.080 --> 02:43:18.300]   So I was born in the Soviet Union.
[02:43:18.300 --> 02:43:20.500]   So it connects to me to the entirety of the history
[02:43:20.500 --> 02:43:23.220]   of the Soviet Union and to World War II
[02:43:23.220 --> 02:43:28.060]   and to the love and hatred and suffering that went on there.
[02:43:28.060 --> 02:43:33.060]   And the corrupting nature of power and greed
[02:43:33.060 --> 02:43:35.300]   and just somehow,
[02:43:35.300 --> 02:43:38.140]   that book has taught me more about life
[02:43:38.140 --> 02:43:39.420]   than like anything else.
[02:43:39.420 --> 02:43:41.060]   Even though it's just like a silly,
[02:43:41.060 --> 02:43:46.060]   like childlike book about pigs.
[02:43:46.060 --> 02:43:48.980]   I don't know why, it just connects and inspires.
[02:43:48.980 --> 02:43:50.700]   And the same, there's a few,
[02:43:50.700 --> 02:43:53.820]   yeah, there's a few technical books too
[02:43:53.820 --> 02:43:57.580]   and algorithms that just, yeah, you return to often.
[02:43:58.500 --> 02:43:59.740]   I'm with you.
[02:43:59.740 --> 02:44:02.900]   Yeah, there's, I don't know.
[02:44:02.900 --> 02:44:05.420]   And I've been losing that because of the internet.
[02:44:05.420 --> 02:44:07.260]   I've been like going on,
[02:44:07.260 --> 02:44:11.300]   I've been going on archive and blog posts and GitHub
[02:44:11.300 --> 02:44:16.220]   and the new thing and you lose your ability
[02:44:16.220 --> 02:44:18.140]   to really master an idea.
[02:44:18.140 --> 02:44:18.980]   - Right.
[02:44:18.980 --> 02:44:19.820]   - Wow.
[02:44:19.820 --> 02:44:21.140]   - Exactly right.
[02:44:21.140 --> 02:44:23.560]   - What's a fond memory from childhood?
[02:44:25.540 --> 02:44:27.460]   Baby, Russ, Tedrick.
[02:44:27.460 --> 02:44:31.180]   - Well, I guess I just said that,
[02:44:31.180 --> 02:44:36.820]   at least my current life began when I got to MIT.
[02:44:36.820 --> 02:44:38.940]   If I have to go farther than that.
[02:44:38.940 --> 02:44:41.340]   - Yeah, was there a life before MIT?
[02:44:41.340 --> 02:44:43.140]   - Oh, absolutely.
[02:44:43.140 --> 02:44:47.860]   But let me actually tell you what happened
[02:44:47.860 --> 02:44:48.900]   when I first got to MIT.
[02:44:48.900 --> 02:44:52.260]   'Cause that, I think might be relevant here.
[02:44:52.260 --> 02:44:57.260]   But I had taken a computer engineering degree at Michigan.
[02:44:57.260 --> 02:45:00.420]   I enjoyed it immensely, learned a bunch of stuff.
[02:45:00.420 --> 02:45:03.560]   I liked computers, I liked programming.
[02:45:03.560 --> 02:45:07.340]   But when I did get to MIT and started working
[02:45:07.340 --> 02:45:10.300]   with Sebastian Sung, theoretical physicist,
[02:45:10.300 --> 02:45:11.880]   computational neuroscientist,
[02:45:11.880 --> 02:45:17.220]   the culture here was just different.
[02:45:17.220 --> 02:45:20.260]   It demanded more of me, certainly mathematically
[02:45:20.260 --> 02:45:21.860]   and in the critical thinking.
[02:45:22.660 --> 02:45:27.660]   And I remember the day that I borrowed one of the books
[02:45:27.660 --> 02:45:29.780]   from my advisor's office and walked down
[02:45:29.780 --> 02:45:32.140]   to the Charles River and was like,
[02:45:32.140 --> 02:45:33.500]   I'm getting my butt kicked.
[02:45:33.500 --> 02:45:38.140]   And I think that's gonna happen to everybody
[02:45:38.140 --> 02:45:40.220]   who's doing this kind of stuff.
[02:45:40.220 --> 02:45:45.220]   I think, I expected you to ask me the meaning of life.
[02:45:45.220 --> 02:45:50.980]   I think that the, somehow I think that's gotta be part of it.
[02:45:51.580 --> 02:45:52.780]   This--
[02:45:52.780 --> 02:45:53.900]   - Doing hard things?
[02:45:53.900 --> 02:45:55.940]   - Yeah.
[02:45:55.940 --> 02:45:58.180]   - Did you consider quitting at any point?
[02:45:58.180 --> 02:45:59.740]   Did you consider this isn't for me?
[02:45:59.740 --> 02:46:01.700]   - No, never that.
[02:46:01.700 --> 02:46:06.700]   I mean, I was working hard, but I was loving it.
[02:46:06.700 --> 02:46:09.480]   I think there's this magical thing where you,
[02:46:09.480 --> 02:46:13.300]   I'm lucky to surround myself with people that basically,
[02:46:13.300 --> 02:46:17.860]   almost every day I'll see something,
[02:46:17.860 --> 02:46:20.300]   I'll be told something or something that I realize,
[02:46:20.300 --> 02:46:21.980]   wow, I don't understand that.
[02:46:21.980 --> 02:46:24.140]   And if I could just understand that,
[02:46:24.140 --> 02:46:26.000]   there's something else to learn,
[02:46:26.000 --> 02:46:28.100]   that if I could just learn that thing,
[02:46:28.100 --> 02:46:30.300]   I would connect another piece of the puzzle.
[02:46:30.300 --> 02:46:35.300]   And I think that is just such an important aspect
[02:46:35.300 --> 02:46:40.220]   and being willing to understand what you can and can't do
[02:46:40.220 --> 02:46:43.540]   and loving the journey of going
[02:46:43.540 --> 02:46:44.780]   and learning those other things.
[02:46:44.780 --> 02:46:46.220]   I think that's the best part.
[02:46:47.300 --> 02:46:51.460]   - I don't think there's a better way to end it, Russ.
[02:46:51.460 --> 02:46:55.540]   You've been an inspiration to me since I showed up at MIT.
[02:46:55.540 --> 02:46:57.660]   Your work has been an inspiration to the world.
[02:46:57.660 --> 02:46:59.700]   This conversation was amazing.
[02:46:59.700 --> 02:47:01.660]   I can't wait to see what you do next
[02:47:01.660 --> 02:47:03.740]   with robotics, home robots.
[02:47:03.740 --> 02:47:05.700]   I hope to see your work in my home one day.
[02:47:05.700 --> 02:47:07.460]   So thanks so much for talking today.
[02:47:07.460 --> 02:47:08.300]   It's been awesome.
[02:47:08.300 --> 02:47:09.460]   - Cheers.
[02:47:09.460 --> 02:47:11.020]   - Thanks for listening to this conversation
[02:47:11.020 --> 02:47:12.180]   with Russ Tedrick.
[02:47:12.180 --> 02:47:14.140]   And thank you to our sponsors,
[02:47:14.140 --> 02:47:18.180]   Magic Spoon Cereal, BetterHelp and ExpressVPN.
[02:47:18.180 --> 02:47:20.140]   Please consider supporting this podcast
[02:47:20.140 --> 02:47:23.360]   by going to magicspoon.com/lex
[02:47:23.360 --> 02:47:25.460]   and using code LEX at checkout.
[02:47:25.460 --> 02:47:27.780]   Go into betterhelp.com/lex
[02:47:27.780 --> 02:47:32.780]   and signing up at expressvpn.com/lexpod.
[02:47:32.780 --> 02:47:36.220]   Click the links, buy the stuff, get the discount.
[02:47:36.220 --> 02:47:39.380]   It really is the best way to support this podcast.
[02:47:39.380 --> 02:47:41.520]   If you enjoy this thing, subscribe on YouTube,
[02:47:41.520 --> 02:47:43.680]   review it with Five Stars and Apple Podcast,
[02:47:43.680 --> 02:47:46.540]   support it on Patreon or connect with me on Twitter
[02:47:46.540 --> 02:47:50.620]   at Lex Friedman spelled somehow without the E,
[02:47:50.620 --> 02:47:53.460]   just F-R-I-D-M-A-N.
[02:47:53.460 --> 02:47:55.100]   And now let me leave you with some words
[02:47:55.100 --> 02:47:56.620]   from Neil deGrasse Tyson,
[02:47:56.620 --> 02:47:58.500]   talking about robots in space
[02:47:58.500 --> 02:48:00.660]   and the emphasis we humans put
[02:48:00.660 --> 02:48:03.540]   on human-based space exploration.
[02:48:03.540 --> 02:48:05.580]   "Robots are important.
[02:48:05.580 --> 02:48:07.860]   "If I don my pure scientist hat,
[02:48:07.860 --> 02:48:09.900]   "I would say just send robots.
[02:48:09.900 --> 02:48:12.240]   "I'll stay down here and get the data.
[02:48:12.240 --> 02:48:14.980]   "But nobody's ever given a parade for a robot.
[02:48:14.980 --> 02:48:17.840]   "Nobody's ever named a high school after a robot.
[02:48:17.840 --> 02:48:20.060]   "So when I don my public educator hat,
[02:48:20.060 --> 02:48:22.680]   "I have to recognize the elements of exploration
[02:48:22.680 --> 02:48:24.060]   "that excite people.
[02:48:24.060 --> 02:48:26.880]   "It's not only the discoveries and the beautiful photos
[02:48:26.880 --> 02:48:28.900]   "that come down from the heavens.
[02:48:28.900 --> 02:48:33.080]   "It's the vicarious participation in discovery itself."
[02:48:33.080 --> 02:48:37.240]   Thank you for listening and hope to see you next time.
[02:48:37.240 --> 02:48:39.820]   (upbeat music)
[02:48:39.820 --> 02:48:42.400]   (upbeat music)
[02:48:42.400 --> 02:48:52.400]   [BLANK_AUDIO]

