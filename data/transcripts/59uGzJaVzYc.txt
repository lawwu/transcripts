
[00:00:00.000 --> 00:00:08.880]   free to unmute yourself and ask any questions. So I'll give it a minute to get started on YouTube.
[00:00:08.880 --> 00:00:18.720]   It always takes a minute. Awesome. I can hear myself which means real life. And expect people
[00:00:18.720 --> 00:00:28.560]   to be joining us on YouTube as well. Make sure they're able to watch. Amazing. Hey, everybody,
[00:00:28.560 --> 00:00:33.760]   welcome back to our paper reading group. This is the first one of the year. It's
[00:00:33.760 --> 00:00:40.320]   titled revisiting deep learning models for tabular data. It's great to see everyone in the
[00:00:40.320 --> 00:00:46.320]   zoom call. So to people watching on YouTube, if you'd like to join us on there's a signup link and
[00:00:46.320 --> 00:00:52.640]   you can join the zoom call discussed directly on there. Or you're welcome to asking questions in
[00:00:52.640 --> 00:00:57.200]   the YouTube chat or on our forums as well. Howsoever you want. We'd love to, I love to
[00:00:57.200 --> 00:01:05.280]   keep the session interactive. And I'm happy to take your questions from anywhere. We in the
[00:01:05.280 --> 00:01:11.360]   previous session, we had looked at papers from NeurIPS. This was a paper that really caught my
[00:01:11.360 --> 00:01:19.360]   eye. And I had summarized five different papers in our previous session. So for today, we'll be
[00:01:19.360 --> 00:01:24.160]   taking a deeper dive into this one. It's titled revisiting deep learning models for tabular data.
[00:01:24.880 --> 00:01:35.600]   And the title gives it away, but tabular data has been dominated by XGBoost, CADBoost, RandomForest,
[00:01:35.600 --> 00:01:40.400]   all of these techniques. So today in this paper, using this paper, we'll be taking a look at
[00:01:40.400 --> 00:01:47.600]   can we effectively apply deep learning models to tabular data sets? And does it make sense to apply
[00:01:47.600 --> 00:01:51.920]   them? If you've worked in the industry, you would know there are different stakeholders who
[00:01:52.800 --> 00:01:59.040]   are interested in the outcomes. Can you convince them that you'll need enough firepower, you need
[00:01:59.040 --> 00:02:07.360]   enough resources to train these, they give enough accuracy over the other models? So we'll be taking
[00:02:07.360 --> 00:02:16.320]   a look at all of these questions. I want to point everyone to this link. You're welcome to
[00:02:16.320 --> 00:02:20.720]   ask questions anywhere. But if you want, you can also ask them on our forums where
[00:02:21.520 --> 00:02:28.320]   this will remain a bit structured since we use discourse and it can thread the conversation.
[00:02:28.320 --> 00:02:32.320]   So it's a bit easier to follow, but I'm keeping an eye out everywhere. So please
[00:02:32.320 --> 00:02:37.200]   join anywhere you like. Let me post this link on YouTube real quick.
[00:02:37.200 --> 00:02:47.360]   I'm sorry, I know I see many familiar names every time I know they might be a bit annoyed by this
[00:02:47.360 --> 00:02:52.240]   process that I repeat every time. But I want to make sure everyone who's new is quite welcomed
[00:02:52.240 --> 00:02:57.440]   and they're happy to ask questions. I want to make sure that it's a good experience for them as well.
[00:02:57.440 --> 00:03:05.440]   So I'll come back to this question towards the later half. But this is how you ask questions.
[00:03:05.440 --> 00:03:09.520]   You can sign up on our forums, reply to this thread and just leave a comment here.
[00:03:09.520 --> 00:03:14.320]   To the people in the Zoom call, you're welcome to unmute yourself and interrupt me at any time
[00:03:14.320 --> 00:03:20.240]   at all. So without further ado, let's dive into today's agenda. This is a 60 minute session,
[00:03:20.240 --> 00:03:28.160]   we might go over a bit. It's quite a meaty topic to dive into. The agenda is, we'll be looking at
[00:03:28.160 --> 00:03:33.280]   the RTDL paper. So I'll be highlighting different sections of the paper. Actually, let me show this
[00:03:33.280 --> 00:03:38.320]   to you guys. I've seen a few annotated papers on the internet. And I think it's an interesting
[00:03:38.320 --> 00:03:46.080]   exercise. So on our GitHub, I'll soon be uploading these. But I'll try to do this. On the video call,
[00:03:46.080 --> 00:03:51.840]   I have annotated the paper and I'll publish it in a few days on our GitHub. But we'll be reading
[00:03:51.840 --> 00:03:59.520]   through the entire paper. I'll just point out the important sections to everyone. Followed by that,
[00:03:59.520 --> 00:04:07.440]   I'll take a look at the code implementation. And we'll also go away with a few homework
[00:04:07.440 --> 00:04:11.200]   suggestions, which you're welcome to follow or not many people do they come back with a good
[00:04:11.200 --> 00:04:18.080]   sense of learning. Even if you don't, that's totally fine. The method of selecting these
[00:04:18.080 --> 00:04:24.720]   papers, one thing I skipped was it's heavily opinionated and it is for you all. So I welcome
[00:04:24.720 --> 00:04:31.200]   any suggestions for our future sessions. I'm looking at this paper today. But if there's one
[00:04:31.200 --> 00:04:36.720]   paper that you really want me to look at, I am 100% open to your feedback, just send it my way
[00:04:36.720 --> 00:04:42.960]   and we'll make it happen for the next session. The resources I'm using, I'm not smart enough to
[00:04:42.960 --> 00:04:48.960]   re-implement this paper by myself. So I am stealing from their GitHub implementation. It is really
[00:04:48.960 --> 00:04:56.240]   well implemented. If you want to find it, just look up the four characters RTDL on Google. It
[00:04:56.240 --> 00:05:02.080]   should take you to the Yandex Research, which is an incredible research lab as I understand,
[00:05:02.080 --> 00:05:08.160]   implementation of the paper. Here you can find a link to the paper and also their website,
[00:05:08.160 --> 00:05:14.560]   which is actually the documentation. This exists as an open source package. You can install it and
[00:05:14.560 --> 00:05:19.760]   start playing around with it as well. Anyone who's attended our other sessions knows I really
[00:05:19.760 --> 00:05:24.720]   encourage you to play with code a lot. I really encourage everyone to spend more time with code
[00:05:24.720 --> 00:05:28.880]   and it's really nice to see that this packet comes with an open source implementation. So
[00:05:28.880 --> 00:05:33.600]   those are the resources that I wanted to point everyone to. As a reminder,
[00:05:33.600 --> 00:05:37.920]   either unmute yourself on Zoom or ask any questions on our forums.
[00:05:37.920 --> 00:05:50.640]   So I'll start with sharing the context of this paper. The challenge with tabular data and why
[00:05:50.640 --> 00:05:57.920]   it can't be used really well with deep learning is deep learning involves creating a stack
[00:05:58.800 --> 00:06:01.760]   of layers that understand different representations.
[00:06:01.760 --> 00:06:08.480]   And usually this data is a vector of heterogeneous features that represent
[00:06:08.480 --> 00:06:13.040]   images, that represent audio, so on and so forth.
[00:06:13.040 --> 00:06:19.360]   This so far hasn't been effectively applied to tabular data because
[00:06:19.360 --> 00:06:25.600]   XCBoost, decision trees, random forest really outperform this.
[00:06:27.600 --> 00:06:33.600]   So is there any state of the art model for tabular data? That is one question to ask
[00:06:33.600 --> 00:06:40.320]   yourself. Whenever you look at images, you can simply go to papers with code, right?
[00:06:40.320 --> 00:06:47.280]   And if you want to browse the state of the art, you can find all of these
[00:06:47.280 --> 00:06:53.200]   papers on different benchmarks, right? So there are 263 benchmarks.
[00:06:54.400 --> 00:06:59.840]   The challenge with tabular data sets, there's no established image net.
[00:06:59.840 --> 00:07:06.240]   There is a lack of benchmarks. So even though we've had some interesting deep learning
[00:07:06.240 --> 00:07:11.840]   architectures in this particular paper, they also benchmark against them. They compare,
[00:07:11.840 --> 00:07:17.040]   they suggested architectures against these as well. We don't have an image net where we can
[00:07:17.040 --> 00:07:24.080]   compare these. So how can you claim that your model works better? And this is a problem that's
[00:07:24.080 --> 00:07:34.160]   not addressed in this paper. They actually use, I think it's 11 data sets across which all of
[00:07:34.160 --> 00:07:38.560]   these models are compared. So they don't pick one single data set. They actually use 11 of these
[00:07:38.560 --> 00:07:41.600]   and they benchmark different models across all of them.
[00:07:41.600 --> 00:07:49.200]   Another interesting challenge where deep learning has been somewhat applied to tabular data sets
[00:07:49.200 --> 00:07:58.240]   would be heterogeneous data in the form of tabular data sets. Let me show you an example.
[00:07:58.240 --> 00:08:05.680]   So this is a competition from three years ago. And in this, you were supposed to predict
[00:08:05.680 --> 00:08:13.200]   which pet can be adopted faster. And you had a few things to help you. So you had,
[00:08:13.200 --> 00:08:17.280]   I'm sorry, I'm looking at the test data set. You had images of the pets.
[00:08:18.960 --> 00:08:25.280]   You had different details about their breed, so on and so forth. So this real quickly becomes
[00:08:25.280 --> 00:08:32.880]   a multi-modal problem, right? So you have tabular data set, but inside of that,
[00:08:32.880 --> 00:08:37.840]   you also have images. One of these columns, as I remember, was the description about the pet.
[00:08:37.840 --> 00:08:44.000]   So that's also an NLP problem. In this way, you could potentially consider applying
[00:08:45.040 --> 00:08:50.880]   deep learning just to the images, just to the text, and apply random forest to every other
[00:08:50.880 --> 00:08:57.120]   column. That is a totally viable solution. That hasn't been explored in the paper,
[00:08:57.120 --> 00:09:01.520]   but historically that is also one of the ways in which this has been done.
[00:09:01.520 --> 00:09:12.080]   So all of these things make tabular data really interesting. There could be continuous variables
[00:09:12.080 --> 00:09:18.800]   inside of it. There could be categorical variables. You sometimes require feature engineering,
[00:09:18.800 --> 00:09:24.640]   especially for random forest to work well. If you do more feature engineering, it's prone to
[00:09:24.640 --> 00:09:32.240]   performing better. And as I mentioned earlier, there's no established image net, right? There's
[00:09:32.240 --> 00:09:37.680]   no established deep learning architecture for the same reason. There's no single data set that you
[00:09:37.680 --> 00:09:42.960]   can go to and tell which data set works better. So for that reason, in this paper, the authors
[00:09:42.960 --> 00:09:50.240]   use a bunch of data sets across which they compare all architectures. So here's the overview of the
[00:09:50.240 --> 00:09:58.000]   paper. They suggest two architectures. Their argument is ResNet has worked so well in images.
[00:09:58.000 --> 00:10:07.200]   So let's try and implement that for tabular data sets. And let's also take a look at attention.
[00:10:07.200 --> 00:10:14.400]   So let's adapt transformers to data sets. And then they compare these against the traditional
[00:10:14.400 --> 00:10:21.360]   techniques. Good and good traditional, XGBoost is from 2014. It's a bit recent. Relatively,
[00:10:21.360 --> 00:10:26.640]   it's at least more recent than AlexNet. So they compare against these techniques and tell us
[00:10:26.640 --> 00:10:33.280]   which model works better. I think we'll look at these in the code. So I'll probably switch
[00:10:33.280 --> 00:10:37.120]   screens to sharing the paper. Any questions so far by anyone?
[00:10:37.120 --> 00:10:50.320]   Awesome. No questions. To the people in the Zoom call, this is meant to be interactive. Please
[00:10:50.320 --> 00:11:00.800]   interrupt any time as you want. I'm always a bit slow when switching sharing screens. So please
[00:11:00.800 --> 00:11:11.040]   pardon my lack of speed here. Awesome. I have my annotated paper in my hand. And I'll use that
[00:11:11.040 --> 00:11:17.120]   as a guide to point things out as I read this paper with you all. So in this paper,
[00:11:17.120 --> 00:11:25.760]   the most important thing in the introduction is the field still lacks effective baselines,
[00:11:25.760 --> 00:11:31.920]   as I mentioned. And there are no easy to use models that provide performance across
[00:11:31.920 --> 00:11:35.600]   different platforms. I'm sorry, is this visible to everyone? Should I zoom in?
[00:11:35.600 --> 00:11:45.600]   Would someone please confirm? It's okay. It's okay. Awesome. Thanks. Thank you.
[00:11:45.600 --> 00:11:54.480]   So as I mentioned earlier, they first propose a ResNet-like architecture. Let me
[00:11:55.440 --> 00:12:04.080]   select a marker or highlighter. I'm more of a pen and paper person. But this is how I need to do it.
[00:12:04.080 --> 00:12:10.240]   So they propose two models. The first one is a ResNet-like architecture, which is a good baseline.
[00:12:10.240 --> 00:12:15.360]   And the second one is a simple adaptation of the transformer architecture.
[00:12:15.360 --> 00:12:23.600]   They also compare these against gradient boosted trees and conclude that there is no universally
[00:12:23.600 --> 00:12:29.760]   superior solution. So spoiler alert, we'll be trying a lot of things. And the conclusion will
[00:12:29.760 --> 00:12:38.400]   be no, deep learning doesn't win everything. It's not the best always. So in the introduction,
[00:12:38.400 --> 00:12:42.880]   the authors talk about different techniques and how they're applied to deep learning,
[00:12:42.880 --> 00:12:47.280]   where all deep learning shines. You can read this. I'll skip over the details here.
[00:12:47.280 --> 00:13:02.160]   So to summarize the first paragraph, I've gone through it and this is a 60 minute call. Hence,
[00:13:02.160 --> 00:13:07.840]   I'm rushing through topics. I expect you all to read this parcels and I'm giving you the detail
[00:13:07.840 --> 00:13:14.240]   points. So they start with the hypothesis that well studied deep learning architectures may be
[00:13:14.240 --> 00:13:22.080]   under explored. Deep learning architectures that work really well with images, we haven't,
[00:13:22.080 --> 00:13:27.440]   at least in academic world, no one has mentioned if they've been applied well to tabular datasets.
[00:13:27.440 --> 00:13:34.480]   So the first architecture that they started with is a ResNet-like architecture. You know,
[00:13:34.480 --> 00:13:39.280]   our ResNet-50 model, the classic good old model that works really well with units,
[00:13:39.920 --> 00:13:45.280]   works really well with image classification for a good baseline. They'll implement that for tabular
[00:13:45.280 --> 00:13:50.880]   data. And we look at what datasets. So far, they haven't mentioned what datasets they'll
[00:13:50.880 --> 00:13:55.840]   be implementing this on. We'll take a look afterwards. And the second one is a feature
[00:13:55.840 --> 00:14:01.760]   token transformer. How do I know the full form? I've read the paper ahead, they mentioned it later
[00:14:01.760 --> 00:14:11.120]   on. So for now it's just F3 transformer. They thoroughly evaluate these for tabular deep
[00:14:11.120 --> 00:14:19.840]   learning on a lot of tasks. And the demonstration is a simple ResNet-like architecture. So not
[00:14:19.840 --> 00:14:27.920]   really simple. It's a bit complicated than your standard MLP model. But the standard ResNet,
[00:14:27.920 --> 00:14:32.880]   the simple ResNet without anything funky is an effective baseline.
[00:14:32.880 --> 00:14:42.160]   And the F3 transformer becomes a new powerful solution for the field. So in some of the areas
[00:14:42.160 --> 00:14:46.880]   we'll see it takes longer to train. Yes, as you would expect with a transformer model.
[00:14:46.880 --> 00:14:55.440]   But it performs definitely a bit better than ResNets. And then we learn this is a wasted effort.
[00:14:55.440 --> 00:15:00.240]   Not really, we do learn a few good things. But there is no superior solution amongst
[00:15:00.240 --> 00:15:05.920]   GBDT, gradient boosted decision trees, or deep learning models.
[00:15:05.920 --> 00:15:12.480]   So now I'll skip further. They explore about the related work, they talk about shallow state
[00:15:12.480 --> 00:15:18.800]   of the art, which are GBDT, XGBoost, LightGBM, all of those things.
[00:15:25.200 --> 00:15:28.640]   And then they talk about differential trees, attention based models.
[00:15:28.640 --> 00:15:33.840]   In the future, PRGI might potentially cover these papers as well for tabular data if there's
[00:15:33.840 --> 00:15:38.320]   interest. But in this, they compare these approaches or just list them out.
[00:15:38.320 --> 00:15:44.720]   So furthermore, they define three architectures that they'll be using.
[00:15:44.720 --> 00:15:50.240]   The first one is an MLP architecture. The second one is a ResNet.
[00:15:51.600 --> 00:15:58.160]   And the third one is an FD transformer. So let's take a look inside of a Colab notebook of how
[00:15:58.160 --> 00:16:02.000]   these look like in code. And then we'll look at the theoretical definition. I think that'll work
[00:16:02.000 --> 00:16:08.960]   a bit better. Open-ended approach, but for me, this works well. So I'll enforce this on everyone.
[00:16:08.960 --> 00:16:16.720]   Sorry if someone disagrees. I'm always slow with switching sharing screens. So please pardon my
[00:16:16.720 --> 00:16:22.560]   speed, as always. Any questions so far? People in the Zoom call?
[00:16:22.560 --> 00:16:30.480]   Yeah, quick question or thought. So when we're talking about images and ResNet and everything,
[00:16:30.480 --> 00:16:39.040]   we have convolutional nets. And then there is the thought that it works well with localized
[00:16:41.200 --> 00:16:50.160]   data and its translation, invariant, et cetera. Is there anything about deep learning where they
[00:16:50.160 --> 00:16:58.720]   say, OK, this particular aspect of deep learning is what would make deep learning work well with
[00:16:58.720 --> 00:17:06.160]   tabular data? That is discussed in this paper. So there doesn't exist any effective way of
[00:17:07.040 --> 00:17:13.760]   demonstrating that, because every paper picks up different data sets. So even though I would
[00:17:13.760 --> 00:17:22.480]   suggest you to check out Node, N-O-D-E, and TabNet, T-A-B-N-E-T. These are two deep learning
[00:17:22.480 --> 00:17:28.480]   models, but they use different data sets. So it's not apples to apples comparison.
[00:17:28.480 --> 00:17:31.040]   OK, thanks.
[00:17:31.040 --> 00:17:33.280]   Great question.
[00:17:33.920 --> 00:17:40.800]   So as expected, I will quickly reconnect.
[00:17:40.800 --> 00:17:54.560]   I am quite new to Colab. I have a server that I use locally, but I don't like the pain of
[00:17:54.560 --> 00:17:59.920]   installing everything. So I've installed the RTDL package, which is the title of the paper and
[00:17:59.920 --> 00:18:05.440]   the title of the package. And inside of that, you can see there is a tokenizer. There is a feature
[00:18:05.440 --> 00:18:11.520]   tokenizer, the transformer that we want to take a look at, an MLP implementation.
[00:18:11.520 --> 00:18:17.520]   I don't think anyone in this call would know what that is, but we'll still take a look.
[00:18:17.520 --> 00:18:27.840]   And a good ResNet model applied to tabular data. So we're looking at the implementation in code.
[00:18:27.840 --> 00:18:31.520]   I'll print out the blog so that we can have a better understanding and then go back to the
[00:18:31.520 --> 00:18:40.640]   paper. So I'm assuming the input is a three by three matrix. And it would look like so.
[00:18:40.640 --> 00:18:47.840]   So from here, I'm importing the MLP model and using the defaults for making a baseline.
[00:18:47.840 --> 00:18:51.120]   Inside of the repository, authors show you how you can make a baseline.
[00:18:51.120 --> 00:18:55.520]   For the sake of my laziness, I just copied the defaults from there,
[00:18:56.400 --> 00:19:06.240]   which require me to define an input layer, the number of layers-- sorry, the number of neurons
[00:19:06.240 --> 00:19:12.880]   in the layers, dropout, and output. So I'm assuming it's a single output, possibly a regression problem.
[00:19:12.880 --> 00:19:20.960]   So our good old MLP model would be a sequential block consisting of a linear layer followed by
[00:19:20.960 --> 00:19:25.680]   an activation followed by a dropout. I'm assuming everyone knows what these are.
[00:19:25.680 --> 00:19:31.280]   If you don't, please interrupt. We're using a ReLU here. I think later on,
[00:19:31.280 --> 00:19:34.880]   they used GeLU as well and some form of normalizations.
[00:19:34.880 --> 00:19:43.680]   I am seeing a question where someone-- let me read that out. ResNets use CNNs, which is based
[00:19:43.680 --> 00:19:50.480]   upon a receptive field hypothesis. So how does tabular data confirm this hypothesis? We'll look
[00:19:50.480 --> 00:19:52.560]   at that, and I'll come back to the question. Thanks, Shimon.
[00:19:52.560 --> 00:20:07.280]   Next, we call the ResNet model. And this would consist of a first layer having 128 neurons,
[00:20:07.280 --> 00:20:12.080]   hidden one with 256. We'll define separate dropouts for first and second.
[00:20:12.080 --> 00:20:19.760]   Zero isn't good for second. Let's say it's 0.1. And we would create two Res blocks.
[00:20:19.760 --> 00:20:23.200]   So if you've read the ResNet papers where we have these skip connections,
[00:20:23.200 --> 00:20:27.600]   we'll create two blocks. And I'm, again, assuming it's a single output
[00:20:27.600 --> 00:20:30.560]   problem, possibly a regression problem.
[00:20:30.560 --> 00:20:39.680]   So essentially, we've created two blocks. I'm curious what happens if we make this 0.
[00:20:39.680 --> 00:20:48.240]   It shouldn't work, right? OK, it just becomes a linear model. So if you don't have any Res blocks,
[00:20:49.600 --> 00:20:53.680]   they've taken care of it in the code. It just becomes a linear model, curiously enough.
[00:20:53.680 --> 00:20:55.760]   Sorry about that.
[00:20:55.760 --> 00:21:04.560]   Kishore, could you please unmute and clarify your question?
[00:21:04.560 --> 00:21:16.000]   Or I'll continue, please, interrupt any time. So we have put in two blocks.
[00:21:16.560 --> 00:21:21.520]   And inside of the block, we apply a patch norm, a linear layer, a ReLU,
[00:21:21.520 --> 00:21:27.760]   dropout to every single layer. And between those, we also apply a linear layer.
[00:21:27.760 --> 00:21:37.920]   I'm trying to find where to define the skip connection here.
[00:21:45.360 --> 00:21:48.560]   Maybe I should switch to the paper to quickly showcase this point.
[00:21:48.560 --> 00:21:51.760]   Sorry about that. I'm confusing myself a little.
[00:21:51.760 --> 00:22:01.440]   So this is the definition of ResBlock in pseudocode. A ResNet would have
[00:22:01.440 --> 00:22:08.960]   ResNet blocks implemented inside of each other. And a ResNet block would have
[00:22:10.080 --> 00:22:14.240]   input added with the following. So this becomes
[00:22:14.240 --> 00:22:30.160]   one block. And finally, your prediction is the activation followed by a linear layer applied to
[00:22:30.160 --> 00:22:37.040]   this output. Switching back to the notebook, I was confusing myself if they add this or not. They do.
[00:22:38.000 --> 00:22:44.720]   Someone is asking, can I go over the sequential architecture? Sure. Let me
[00:22:44.720 --> 00:22:53.840]   tell you how you can do that. So I'm assuming you're curious about this block.
[00:22:53.840 --> 00:23:03.360]   Let's hover over to torch.py torch. And let's see what does sequential mean.
[00:23:05.200 --> 00:23:09.760]   Torch.nn.sequential. That looks like the first problem. A sequential container,
[00:23:09.760 --> 00:23:13.760]   modules would be added to it in order they are passed to the constructor.
[00:23:13.760 --> 00:23:20.000]   What's the difference between sequential and a module list? A module list is exactly what it
[00:23:20.000 --> 00:23:26.320]   sounds. On the other hand, layers in sequential are connected in a cascading way. So these layers
[00:23:26.320 --> 00:23:32.160]   are passed in a linear order. So your input goes through these in a linear order.
[00:23:33.440 --> 00:23:38.560]   If this doesn't answer the question, please feel free to follow or check out the docs if you'd like.
[00:23:38.560 --> 00:23:47.920]   So here is our ResNet block and we would add our input after every single block.
[00:23:47.920 --> 00:23:56.800]   Let's take a look at what does the transformer look like. So I am declaring the number of
[00:23:56.800 --> 00:24:02.560]   features as the shape of the input. There are no categorical variables for this vanilla example.
[00:24:03.520 --> 00:24:08.640]   And inside of the paper, they tell you this makes the model work faster. If you assume the
[00:24:08.640 --> 00:24:13.920]   last layer is the penultimate, it makes it work faster. That's a tiny bit of detail.
[00:24:13.920 --> 00:24:28.400]   So I have defined the transformer like so. There are different blocks inside of it, of course.
[00:24:29.200 --> 00:24:36.240]   This one basically calls attention and we define the query key and value pairs like so.
[00:24:36.240 --> 00:24:41.440]   We of course implement dropout. Anyone who's new to transformers,
[00:24:41.440 --> 00:24:46.960]   if I remember correctly, you always want to apply some dropout transformers like dropout.
[00:24:46.960 --> 00:24:56.320]   Followed by which you pass it. So this is one single block inside of the transformer model.
[00:24:57.920 --> 00:25:05.280]   You pass it through a linear layer followed by an activation re-JLU. So look this one up. I was
[00:25:05.280 --> 00:25:10.240]   new to this when I was reading the paper. Followed by dropout and another linear layer.
[00:25:10.240 --> 00:25:18.720]   Then we apply a dropout to the attention model, the residual model, and pass it further to a
[00:25:18.720 --> 00:25:25.840]   normalization model layer, which would be layer norm. I won't go into what layer norm is. Please
[00:25:25.840 --> 00:25:32.800]   feel free to look that up. And then we repeat this for how many so ever blocks we want to create.
[00:25:32.800 --> 00:25:42.480]   Going back to the paper, I'll pause for any questions here for a 30 second break.
[00:25:42.480 --> 00:25:47.840]   Any questions so far?
[00:25:53.920 --> 00:25:57.440]   Either I'm doing an awesome job or a terrible job since I don't hear any questions.
[00:25:57.440 --> 00:26:03.120]   In the resident model, can you find how they add a skip connection in the code?
[00:26:03.120 --> 00:26:07.760]   Yeah, let me see if I can do that live. So
[00:26:07.760 --> 00:26:14.480]   let's head over to, oh, I'm not sharing my screen.
[00:26:18.240 --> 00:26:26.560]   I have the repository open here. I think it would be inside of lib. Nope. RTTL.
[00:26:28.720 --> 00:26:41.040]   Modules.
[00:26:41.040 --> 00:26:51.680]   Someone already shared the link. Sorry, I was just following up on the chat. Thank you.
[00:26:53.520 --> 00:27:01.520]   Where do they add? Oh, there we go. I think it was among inside of the forward layer,
[00:27:01.520 --> 00:27:08.880]   they define it like so. Okay, thank you. Thanks for the question.
[00:27:08.880 --> 00:27:15.600]   So I was any other questions, guys?
[00:27:22.400 --> 00:27:27.760]   I'm using chai to keep myself hydrated, which is subjectively a good idea, I would say.
[00:27:27.760 --> 00:27:35.760]   Coming back to the paper, I've told you what the MLP implementation looks like.
[00:27:35.760 --> 00:27:41.280]   And the resonate one thing to be aware of here, switching to my highlighter.
[00:27:41.280 --> 00:27:49.120]   The main building block is simplified compared to the original architecture.
[00:27:50.080 --> 00:27:54.640]   And there is a clear path from the input to output because we are simply adding.
[00:27:54.640 --> 00:27:57.200]   And thanks for that question, which clarifies this.
[00:27:57.200 --> 00:28:03.520]   So this is the simplified model of our resonate implementation.
[00:28:03.520 --> 00:28:10.720]   Now, what does the FT transformer do? I'm looking at written notes here from the paper.
[00:28:10.720 --> 00:28:16.880]   I'm a pen and paper guy. But we take the features, the both categorical
[00:28:18.720 --> 00:28:25.920]   and continuous features. And we pass them through a tokenizer on which we apply
[00:28:25.920 --> 00:28:34.720]   transformer layers. So here we have a feature tokenizer, which creates all of the CLS tokens
[00:28:34.720 --> 00:28:41.200]   based on which we can call the predictions. That's a simplified explanation. Like I said,
[00:28:41.200 --> 00:28:45.440]   this is completely open source, I encourage everyone to check the implementation out if
[00:28:45.440 --> 00:28:53.040]   you're curious. And another thing to point out. So they theoretically define what does
[00:28:53.040 --> 00:29:06.800]   the feature tokenizer do here? Sorry, I was catching up on the chart and someone pointed
[00:29:06.800 --> 00:29:13.680]   to the same skip connection. Thanks, Yvonne. Cool. Coming back to this.
[00:29:13.680 --> 00:29:22.720]   So the embedding that we are creating here, basically, we create an embedding for all of
[00:29:22.720 --> 00:29:31.680]   the features, both categorical and continuous. It is defined like so. So we add a bias by an
[00:29:32.960 --> 00:29:37.280]   sorry, we add a bias to a function that's applied to our input.
[00:29:37.280 --> 00:29:47.760]   And the function is essentially a matrix multiplication of weights with the input.
[00:29:47.760 --> 00:29:51.200]   I think they mentioned this somewhere in the follow up paragraphs.
[00:29:51.200 --> 00:29:59.280]   Fj number is implemented as element wise multiplication with the vector
[00:30:00.000 --> 00:30:06.640]   weights of numbers. And if you're applying this to categorical, it's the lookup table
[00:30:06.640 --> 00:30:14.320]   for categorical features. And then after applying this, like so for both categorical and continuous,
[00:30:14.320 --> 00:30:20.080]   so we need to account for categorical cases and numerical cases, both are taken care of.
[00:30:20.080 --> 00:30:28.160]   Then we stack all of these. After which we simply apply a transformer layer, nothing
[00:30:28.160 --> 00:30:36.960]   interesting here, I think apply multi head self attention with eight attention heads.
[00:30:36.960 --> 00:30:44.080]   I think it's mentioned in the appendix, but this would have eight attention heads if I'm not too
[00:30:44.080 --> 00:30:49.280]   wrong. Finally, to get the prediction, it's passed through linearly. So we looked at this in code.
[00:30:49.280 --> 00:30:55.600]   Now we've looked at the pseudocode. Here are the three architectures, MLP,
[00:30:56.480 --> 00:31:02.240]   standard vanilla, multi-layer perceptron. The ResNet-like architecture, which we debugged,
[00:31:02.240 --> 00:31:10.560]   has a skip connection. We solve that mystery. And of FT, feature tokenizer plus transformer.
[00:31:10.560 --> 00:31:19.600]   And now we'll compare how do these three models perform across different tasks. But for that,
[00:31:19.600 --> 00:31:23.840]   we need to define different tasks. And we also need to look at the competitors. So
[00:31:23.840 --> 00:31:31.120]   they look at 1, 2, 3, 4, 5, 6, 7, 8 papers or 8 modules. I'm sure this would be familiar to
[00:31:31.120 --> 00:31:37.360]   everyone else. These are other recent papers. Some of them have an MLP-like architecture,
[00:31:37.360 --> 00:31:42.160]   some more have decision trees, but the authors compare against all of them.
[00:31:42.160 --> 00:31:46.000]   So now moving on to the experiments.
[00:31:49.600 --> 00:31:54.880]   They also compare in the appendix, not in the main paper, how much training time does it take?
[00:31:54.880 --> 00:32:03.520]   Sorry, an interesting bit. And they also compare
[00:32:03.520 --> 00:32:11.520]   the information on hardware. So how many GPUs did it really take for the same?
[00:32:15.680 --> 00:32:24.160]   They do not apply pre-training. They do not consider additional loss functions.
[00:32:24.160 --> 00:32:28.880]   They do not consider data augmentation. Although I'm curious if anyone knows what
[00:32:28.880 --> 00:32:33.280]   data augmentation can be applied to tabular data. Right off the top of my head, I don't know.
[00:32:33.280 --> 00:32:38.800]   They do not apply distillation, learning rate, warmup, decay, or other things.
[00:32:38.800 --> 00:32:44.400]   So a way to potentially improve these comparisons would be, again, this is a homework to people in
[00:32:44.400 --> 00:32:48.800]   the call. If you want, try applying these things and see if you can improve the accuracy.
[00:32:48.800 --> 00:32:55.920]   It would at the worst at least improve the training time. Accuracy maybe or maybe not.
[00:32:55.920 --> 00:33:04.960]   The goal by the authors is to evaluate the impact of inductive biases by the different
[00:33:04.960 --> 00:33:11.760]   model architectures. So now moving on to how do they evaluate it. They use 11 public datasets.
[00:33:13.600 --> 00:33:19.920]   And it is good to see that they've clarified this, but they are quite careful of not messing
[00:33:19.920 --> 00:33:28.080]   up the comparisons. So there is exactly one train validation test split. And all of the algorithms,
[00:33:28.080 --> 00:33:34.720]   all of the models get a fair treatment. So they all are evaluated on the same splits.
[00:33:34.720 --> 00:33:40.640]   Test accuracy across all of the models would be universal. What are the datasets? You can look
[00:33:40.640 --> 00:33:47.680]   at these in details. The first one is California housing, real estate. There's another one on
[00:33:47.680 --> 00:33:56.160]   adult income estimation. There's a bunch around physical particles.
[00:33:56.160 --> 00:34:04.640]   Details around more physics datasets. I have looked at these, but there's no interesting bit
[00:34:04.640 --> 00:34:13.120]   to mention here. The key thing is some are regression problem. Some are also classification
[00:34:13.120 --> 00:34:19.040]   problem. So as you can see, they have a good number of objects across all of these. Let me
[00:34:19.040 --> 00:34:27.040]   zoom in a bit here. They have a good number of features, the highest being 2000 for one of the
[00:34:27.040 --> 00:34:33.520]   datasets. What is the EP dataset? It's simulated physics experiments. That makes sense. That should
[00:34:33.520 --> 00:34:39.440]   have a large number. The smallest number of features is just eight, which is CA. That would
[00:34:39.440 --> 00:34:48.640]   be the California housing dataset. It's interesting to see that simulated physics experiments have more
[00:34:48.640 --> 00:34:57.040]   parameters than datasets on housing, curiously enough. Okay. So these are all of the datasets
[00:34:57.040 --> 00:35:05.760]   across which our models are evaluated. How do they pre-process them? We use,
[00:35:05.760 --> 00:35:12.000]   we, implying the authors, used quantile transformation from scikit-learn
[00:35:12.000 --> 00:35:18.640]   and they applied a standardization, which means subtracting the mean and scaling to two of the
[00:35:18.640 --> 00:35:25.840]   datasets. This one has images. So in images, at least it's common practice if you've ever
[00:35:25.840 --> 00:35:33.840]   done that. For tuning the model, so hyper parameter tuning, they carefully tune each
[00:35:33.840 --> 00:35:43.440]   model's hyper parameters. And I think I wanted to point another thing out. It's good to see that
[00:35:43.440 --> 00:35:49.440]   the authors have clarified that test set is never used for tuning. They are quite careful of simply
[00:35:49.440 --> 00:35:56.240]   keeping it to the validation and training dataset. So we use 11 datasets. We apply two simple
[00:35:56.240 --> 00:36:04.000]   pre-processing techniques. We tune them on the validation and train dataset. They never tune on
[00:36:04.000 --> 00:36:10.480]   test and they never apply any interesting techniques that are applied to deep learning.
[00:36:10.480 --> 00:36:15.520]   So no pre-training, no additional functions, no augmentation, no warmup, no decay,
[00:36:15.520 --> 00:36:23.360]   and other tricks, so to speak. This is the setup for comparing all of these. And we are already
[00:36:23.360 --> 00:36:28.480]   being set up for a disappointment because we know the conclusion was ambiguous. But it's good to
[00:36:28.480 --> 00:36:39.680]   cover all of these. I don't think there's anything interesting to mention from the further one.
[00:36:39.680 --> 00:36:44.800]   Evaluation is quite straightforward. They run 15 experiments with different random speeds.
[00:36:45.280 --> 00:36:53.600]   Random seeds. Sorry, my throat is starting to dry up a bit. And they do ensemble. So they obtain
[00:36:53.600 --> 00:37:00.480]   three ensembles by splitting the 15 models into three disjoint groups and averaging predictions.
[00:37:00.480 --> 00:37:10.080]   So the ensemble is a simple average here. I would say model from group one,
[00:37:11.280 --> 00:37:14.240]   model from group two, plus model from group three,
[00:37:14.240 --> 00:37:20.240]   divided by three. And this would be the ensemble predictions.
[00:37:20.240 --> 00:37:25.120]   That's how they get the predictions.
[00:37:25.120 --> 00:37:35.680]   And I don't think there's anything interesting to mention. One cool thing that I noted was for
[00:37:35.680 --> 00:37:42.000]   CADBoost, they don't do anything. They just use the inbuilt support. So CADBoost has
[00:37:42.000 --> 00:37:46.000]   upper hand because there's literally no tuning being done here.
[00:37:46.000 --> 00:37:56.640]   Now in the evaluation, they do note that ResNet performs really well and so does FTT across all
[00:37:56.640 --> 00:38:04.880]   of these. It's of course not universal. As you can see, FTT is bold across a lot of these options.
[00:38:05.440 --> 00:38:12.720]   And ResNet, if you look closely, the values for ResNet are just a bit smaller or just a little
[00:38:12.720 --> 00:38:20.880]   far away. So these two models, at least in that sense, work really well. And the conclusion here
[00:38:20.880 --> 00:38:31.120]   is MLP is still a good sanity check. ResNet is an effective baseline. And it consistently
[00:38:31.680 --> 00:38:40.960]   outperforms the competitors. FTT performs best on most tasks and becomes a new powerful solution
[00:38:40.960 --> 00:38:48.480]   for the field. They use OptTuner to tune the model and they do suggest that.
[00:38:48.480 --> 00:38:56.080]   Furthermore, they I think compare this with Node and they share how do they do that. No
[00:38:56.080 --> 00:39:01.280]   interesting things here, so I'll continue further. And then they compare deep learning
[00:39:01.280 --> 00:39:15.920]   with gradient boosted decision trees. They accomplish that by comparing ensembles
[00:39:15.920 --> 00:39:22.720]   instead of single models. Since boosted trees is essentially an ensembling technique and we expect
[00:39:23.520 --> 00:39:27.760]   deep learning architecture should in theory, you can read this paper,
[00:39:27.760 --> 00:39:35.280]   but deep learning architecture should perform better with ensembling. So later they learn.
[00:39:35.280 --> 00:39:43.200]   First, when you start with the out of box performance.
[00:39:52.000 --> 00:39:56.480]   Sorry, I'm getting confused myself. I just want to make sure which performs better here.
[00:39:56.480 --> 00:40:03.840]   Right. Sorry about this. So for the default configuration, FT transformer, the attention
[00:40:03.840 --> 00:40:13.040]   based approach is better than GBDT. And when you start tuning this, so the thing that all
[00:40:13.040 --> 00:40:17.360]   data scientists or machine learning practitioners would at least do to some extent,
[00:40:17.360 --> 00:40:24.400]   the easiest part of the job, once hyper parameters are properly tuned, the boosted approach start
[00:40:24.400 --> 00:40:31.120]   dominating on some datasets, which is quite interesting to know, right? Because the nice
[00:40:31.120 --> 00:40:37.200]   thing about and the reason why they still dominate the industry is because their resource,
[00:40:37.200 --> 00:40:42.880]   I'm trying to think of the opposite word of intensive, but their
[00:40:43.840 --> 00:40:51.120]   resource light. Sorry, I'm confusing myself, but GBDT don't require as much resources,
[00:40:51.120 --> 00:40:56.480]   as much training time or as much efforts to train as compared to deep learning models,
[00:40:56.480 --> 00:41:02.000]   even when you're tuning them. So I would assume without even looking at the training time,
[00:41:02.000 --> 00:41:08.640]   if you were to tune an XGBoost model, that would happen much faster than you would be required to
[00:41:08.640 --> 00:41:17.120]   tune a transformer model. So in some cases, it starts dominating it to add to our confusion,
[00:41:17.120 --> 00:41:25.360]   which means that maybe these are a better way. The main takeaway would be then,
[00:41:25.360 --> 00:41:30.560]   there is no universal solution among deep learning model and boosted trees.
[00:41:30.560 --> 00:41:37.120]   Deep learning research efforts aimed at surpassing them should focus on datasets where
[00:41:37.520 --> 00:41:46.400]   boosted trees outperforms state of the art solution and not include just the DL friendly
[00:41:46.400 --> 00:41:51.840]   problems. So the thing here is, and I'm sure whenever anyone has done a product demo or
[00:41:51.840 --> 00:41:57.040]   whatsoever, you always want to use something that favors you, right? You would probably
[00:41:57.040 --> 00:42:03.040]   want to avoid showcasing the downside. So in many papers, the part of the problem that I was
[00:42:03.040 --> 00:42:10.000]   speaking about earlier, you are prone to using a dataset where deep learning might perform better
[00:42:10.000 --> 00:42:16.480]   compared to boosted trees. You heard evaluate many models, you would probably use weights and biases
[00:42:16.480 --> 00:42:23.600]   to compare how well do your experiments do. And then for those problems, you would just use the
[00:42:23.600 --> 00:42:28.560]   ones where deep learning works better and you wouldn't compare them on the ones where boosted
[00:42:28.560 --> 00:42:34.160]   trees might be outperforming. So the author suggests two people working in the field to use
[00:42:34.160 --> 00:42:44.080]   datasets where also GBDT boosted trees outperform deep learning models.
[00:42:44.080 --> 00:42:54.480]   So an intriguing property that they mention of F3 transformer would be
[00:42:56.480 --> 00:43:07.120]   that it does perform on par with ResNet on a lot of problems.
[00:43:07.120 --> 00:43:17.840]   And we could state that it's a more universal model for tabular datasets.
[00:43:17.840 --> 00:43:25.040]   But then comes the question, when is F3 transformer better than ResNet?
[00:43:26.000 --> 00:43:33.360]   So to do that, they do theoretical test against synthetic tasks. Sorry, I'm again,
[00:43:33.360 --> 00:43:37.600]   my throat is drying up. So I'll take a sip of water. I'll take a quick break. Sorry about that.
[00:43:37.600 --> 00:43:42.880]   Meanwhile, any questions from anyone?
[00:43:42.880 --> 00:44:11.920]   I'm just catching up on the YouTube chart.
[00:44:11.920 --> 00:44:21.600]   Can you explain the attention module if it's not too much? It's a bit too much.
[00:44:21.600 --> 00:44:28.000]   A quick explanation. Let me point you to a few resources. Let me see if I can
[00:44:28.000 --> 00:44:34.080]   quickly find a good thread.
[00:44:40.560 --> 00:44:46.240]   Sorry, I'm just quickly scrolling on Twitter to find an awesome thread that I remember.
[00:44:46.240 --> 00:45:04.400]   Give me one minute to find that thread.
[00:45:09.680 --> 00:45:17.760]   Where is it? So Tanishq Abraham, who's a bit of a genius, he completed his PhD at the age of 17 or
[00:45:17.760 --> 00:45:25.280]   18, I believe, which is quite crazy to know he did an incredible thread showcasing different
[00:45:25.280 --> 00:45:32.160]   resources around attention. Found it. Sorry about that. It took me a minute because I do
[00:45:32.160 --> 00:45:41.040]   spend a lot of time on Twitter. I would highly recommend to Khalid, please consider checking
[00:45:41.040 --> 00:45:48.480]   this thread out. This has a lot of incredible resources that you can find on attention and
[00:45:48.480 --> 00:45:52.400]   transformer. So I would encourage you to go through these in order. As I mentioned, these
[00:45:52.400 --> 00:45:58.160]   are by a bit of a child genius. So he knows what he's doing because he's the only person or one of
[00:45:58.160 --> 00:46:06.880]   the only person who I know has completed his PhD at age 17. Next question is by Amul Patil, disjoint
[00:46:06.880 --> 00:46:15.680]   group of ensembles of 15 models. Anything that how can I simplify this? That's a question.
[00:46:20.800 --> 00:46:28.560]   So the point is they use different sets of 15 models. I'm almost restating the thing like we
[00:46:28.560 --> 00:46:39.200]   do in school. They average the predictions from 15 models that are different in the sense of setup.
[00:46:39.200 --> 00:46:45.680]   So they might have different layers, different number of parameters, so on and so forth. I'm
[00:46:45.680 --> 00:46:53.840]   not sure if that helps. Please follow up if it doesn't. Okay, I'm going back to the paper.
[00:46:53.840 --> 00:47:05.760]   So how do they evaluate when can Ft transformer perform better than resonate? They set up a
[00:47:05.760 --> 00:47:14.000]   theoretical bunch of synthetic benchmarks. And then they note vendors resonate start performing
[00:47:14.000 --> 00:47:20.240]   better vendors Ft transformer perform better and windows cat boost perform better. I'll encourage
[00:47:20.240 --> 00:47:25.600]   you to check this out. But this again, doesn't leave you with anything conclusive. So I wasn't
[00:47:25.600 --> 00:47:32.000]   quite clear on this part of the implementation. Can I post a link? Thanks, David for posting
[00:47:32.000 --> 00:47:40.400]   the link. I forgot to post it in the zoom chat. Continuing further. I don't think anything
[00:47:40.400 --> 00:47:48.160]   important is to be highlighted here. I jump forward to the conclusion, which would be
[00:47:48.160 --> 00:47:57.840]   we investigated different models in deep learning. And we improve the SOTA baseline and tabular data
[00:47:57.840 --> 00:48:04.160]   first, by demonstrating that a simple quote unquote resonate like architecture is an effective
[00:48:04.160 --> 00:48:12.720]   baseline. Second, we have we implying the authors propose a transformer model, a simple adaptation
[00:48:12.720 --> 00:48:20.640]   of transformer architecture that outperforms other solutions on most of the tasks. And at the end,
[00:48:20.640 --> 00:48:26.320]   they conclude that there's no single good model, but it's good to know that you could at least
[00:48:26.320 --> 00:48:31.360]   play around with resonates play around with the transformer implementation and
[00:48:32.640 --> 00:48:40.640]   maybe beat the accuracy of these. So depending on your business use case, at the end of the day,
[00:48:40.640 --> 00:48:45.280]   why do we want to understand these architectures? Why do we want to read papers, we want to take
[00:48:45.280 --> 00:48:51.840]   this knowledge and apply it somewhere, right? So what did we really learn today? You should probably
[00:48:51.840 --> 00:48:59.920]   look at other architectures from world of NLP. I can't think of any maybe try an LSTM although I
[00:48:59.920 --> 00:49:06.320]   can't imagine how that would be applied effectively to tabular data sets. Maybe look at other CNN
[00:49:06.320 --> 00:49:13.040]   architectures and see if you can simplify them and apply them to tabular data. This is a research
[00:49:13.040 --> 00:49:20.000]   problem. So you might be the one of the only people working on it. You could try that. If you
[00:49:20.000 --> 00:49:25.840]   want and if you're working on tabular problems, see where resonate performs better. And if it's
[00:49:25.840 --> 00:49:31.200]   similar, so the data set where resonate performs better see if it's similar to a data set that you
[00:49:31.200 --> 00:49:40.160]   are working on in your day job. And maybe that really improves your accuracy. Maybe you get a
[00:49:40.160 --> 00:49:46.160]   nice pay bump because of that. Or you get a good experiment on a Kaggle competition where you go
[00:49:46.160 --> 00:49:52.960]   above on the leaderboard against your friends. So those are a few suggested ideas here.
[00:49:55.680 --> 00:50:01.200]   Going further in the paper to point out one interesting fact that I noticed in the appendix.
[00:50:01.200 --> 00:50:07.680]   Appendix appendices are a bit interesting because they tell you these minute details. So the
[00:50:07.680 --> 00:50:12.080]   paper tells you about the research idea, the appendices or the supplementary material tells
[00:50:12.080 --> 00:50:19.040]   you about the details of implementation. So good to see that they evaluate the models on one or more
[00:50:19.600 --> 00:50:27.280]   one or more v100 which is not as crazy as 1000 that is good to see. They talk about the data
[00:50:27.280 --> 00:50:35.040]   set pre processing is quite simple here. The thing I wanted to point out was they compare the training
[00:50:35.040 --> 00:50:42.960]   times and they compare vendors resonate train. How long does resonate take to train and how long
[00:50:42.960 --> 00:50:49.280]   does this take? As expected, almost universally the transformer model would take longer. I'm sure
[00:50:49.280 --> 00:50:56.000]   everyone would have anticipated that except for one data set where it's almost on par. And if I
[00:50:56.000 --> 00:51:01.360]   would go back, that would probably boil down to the reason because this had I think a smaller
[00:51:01.360 --> 00:51:08.880]   number of examples. So transformer does perform better, but at the risk of at least taking
[00:51:08.880 --> 00:51:17.360]   2.5 times more time, I think that's a good average, almost 2.53 times as longer to train as
[00:51:17.360 --> 00:51:27.520]   resonate. So that gives you some context if it's worth using the same flipping pages further to
[00:51:27.520 --> 00:51:35.040]   see if there's any other thing I want to highlight. I don't think so. So I'll stop sharing my screen
[00:51:35.040 --> 00:51:50.880]   and jump back to my presentation. So again, to conclude, we first of all understood the problem,
[00:51:50.880 --> 00:51:58.160]   how broken the problem of deep learning for tabular data sets is. We established a benchmark
[00:51:58.160 --> 00:52:07.440]   by looking at 11 different data sets. And from there, we implemented two architectures,
[00:52:07.440 --> 00:52:17.040]   a resonant like architecture applied to tabular data, and then a FT tokenizer. So sorry, FT plus
[00:52:17.040 --> 00:52:22.400]   transform architecture featured tokenizer plus transform architecture. We looked at their
[00:52:22.400 --> 00:52:27.920]   implementations. Someone from the audience corrected me and we were able to find that
[00:52:27.920 --> 00:52:36.160]   there is indeed a skip connection for resonant. And we later looked and when do these perform
[00:52:36.160 --> 00:52:41.200]   better? How long do they take to train? We also looked at the implementation for them and how
[00:52:41.200 --> 00:52:49.840]   can we call them and apply them to our code. Here are a bunch of suggested homeworks. If this area
[00:52:49.840 --> 00:52:55.360]   of research is of interest to you or if this paper was of interest to you, if you work with tabular
[00:52:55.360 --> 00:53:01.040]   data, try these data sets, apply them to a competition that Kaggle hosts every month.
[00:53:01.040 --> 00:53:06.800]   It's called the TPS Kaggle competition. It's meant for beginners. I'm sorry, I assume everyone in
[00:53:06.800 --> 00:53:12.480]   this call is not a beginner, but the competition is quite friendly. So it should take you an evening
[00:53:12.480 --> 00:53:21.200]   to apply. Consider improving the accuracy with some pre-processing. Try these fancy tricks from
[00:53:21.200 --> 00:53:27.680]   deep learning or forming up, applying a scheduler, etc. And then don't compare. Also consider
[00:53:27.680 --> 00:53:34.480]   ensembling with booster trees. That's one thing that the paper misses, which is what if you not
[00:53:34.480 --> 00:53:43.360]   just ensemble deep learning models, right? What if you ensemble them with GBDT? That's a mouthful
[00:53:43.360 --> 00:53:52.000]   and this time I didn't slip, but ensemble them. So again, to point everyone to the resources,
[00:53:52.000 --> 00:53:57.680]   I will post these links, but if you Google RTDL, that should take you to the open source
[00:53:57.680 --> 00:54:06.240]   implementation, which will point you to the website, point to the paper, and also a collab
[00:54:06.240 --> 00:54:12.640]   demo link somewhere in here that you can play around with. I will spend the last five minutes
[00:54:12.640 --> 00:54:23.440]   for any discussions or questions. So please feel free to ask them now. This is a great suggestion
[00:54:23.440 --> 00:54:32.840]   by Bharat. Any questions by anyone? >>
[00:54:32.840 --> 00:54:42.560]   Yeah, I'm still thinking about what did they do in the deep learning models that
[00:54:42.560 --> 00:54:50.720]   would have benefited tabular data versus... I mean, I might have missed something, but a lot
[00:54:50.720 --> 00:54:57.840]   of it was out of the box, kind of tune a little bit versus how trees were developed from...
[00:54:57.840 --> 00:55:05.360]   I mean, how random forests were developed from trees, I should say. Things like that, that
[00:55:05.360 --> 00:55:15.840]   would help trained with deep learning nets. Do you see what I'm saying?
[00:55:15.840 --> 00:55:21.600]   >> I do understand. So what techniques do they apply that help the DL models?
[00:55:21.600 --> 00:55:29.040]   As far as I know, and as far as I've dug in, it's just, they're just throwing the simple
[00:55:29.040 --> 00:55:36.080]   model and inputting all of the data sets straightforward to the model. So maybe I need
[00:55:36.080 --> 00:55:41.280]   to dig deeper. I'll probably tweet about this if I find any details or post on our forum. So
[00:55:41.280 --> 00:55:46.960]   thanks for the great question. I'll have to dig deeper. >> Yeah. I mean, I'm just commenting. It
[00:55:46.960 --> 00:55:51.920]   just seems like there would be something that you would need to do. >> I'm quite suspicious.
[00:55:51.920 --> 00:56:01.840]   >> In general, I mean, I'm not saying about your paper, but because you have transformations
[00:56:01.840 --> 00:56:10.080]   with images that help augmentation, you have masking with the transformers. So
[00:56:10.800 --> 00:56:16.480]   it kind of helps you explore the data which is needed for these high capacity models.
[00:56:16.480 --> 00:56:25.440]   So that would be one thing that would be needed. And what is it that they're doing, or what is it
[00:56:25.440 --> 00:56:33.560]   that one could do in that area? So yeah, so just thinking along those lines. >>
[00:56:35.840 --> 00:56:49.920]   I know I'll point you to this one. Let me share my screen. I mentioned the TPS competition. Let
[00:56:49.920 --> 00:56:58.880]   me see if I can find them. There we go. I would encourage you to check out the winning solutions
[00:56:58.880 --> 00:57:04.880]   from the previous one. These happen every month. And people do apply. I think in the first one,
[00:57:04.880 --> 00:57:10.160]   someone blew everyone's mind by applying a VAE, a variable autoencoder, to the problem.
[00:57:10.160 --> 00:57:15.360]   And this problem is just tabular datasets. Mostly they're synthetically generated or
[00:57:15.360 --> 00:57:20.800]   something like that. This one is slightly different, but it's just tabular datasets.
[00:57:20.800 --> 00:57:25.120]   And the winning solutions from here are quite interesting. Let me share that link with you.
[00:57:25.120 --> 00:57:32.880]   So that would be one source where you might find some details.
[00:57:33.440 --> 00:57:40.720]   >> Great. Thank you. And thank you for the presentation. A lot to know and a lot to think
[00:57:40.720 --> 00:57:45.200]   about. So appreciate that. >> Awesome. Thanks. Thanks for joining.
[00:57:45.200 --> 00:57:52.560]   Any other questions by anyone else? We still have two minutes. I could take one or two more questions.
[00:57:52.560 --> 00:58:00.080]   Are the DL models considered less explainable? Great question, Hassan.
[00:58:01.680 --> 00:58:09.440]   They do not look at interpretability in this benchmark. I would say, yes, they're less
[00:58:09.440 --> 00:58:14.800]   interpretable than boosted trees, of course, but that becomes an independent discussion.
[00:58:14.800 --> 00:58:19.440]   I should probably dig up papers along those lines. Maybe I should do them in a future one. So
[00:58:19.440 --> 00:58:28.640]   interpretability. But yes, the deep learning models are less explainable or less interpretable
[00:58:28.640 --> 00:58:34.240]   usually than simpler tree-based models.
[00:58:34.240 --> 00:58:48.160]   One final call for any questions. If not, we will wrap up. Let me check the YouTube chat.
[00:58:48.160 --> 00:58:57.600]   I see a question by Pawan. It's hard to visualize tokenization in numerical and
[00:58:57.600 --> 00:59:04.720]   categorical features. If there are any other resources for transformer models?
[00:59:04.720 --> 00:59:16.000]   I'm not sure right off the top of my head. The resources I shared were for understanding
[00:59:16.000 --> 00:59:24.240]   attention and transformer. Let me try digging this up. If I do, I'll share it on Twitter,
[00:59:24.240 --> 00:59:31.520]   but I'll go to my homework for it. Awesome. We're on the clock mark. So I'll thank everyone
[00:59:31.520 --> 00:59:37.840]   for joining. I'll see you all for the session at the next month. I think it will be Feb 13 when we
[00:59:37.840 --> 00:59:43.680]   meet again for a PRG. We are also hosting a Keras reading group. Yesterday, we had Francois
[00:59:43.680 --> 00:59:49.520]   Chollet who kindly agreed to join us for an AMA and we are going through his book,
[00:59:49.520 --> 00:59:53.760]   Deep Learning with Python. If you're interested in joining that, that's happening every Saturday.
[00:59:53.760 --> 01:00:01.600]   Starting next Sunday, we will have a Hugging Face x Fast AI meetup where we'll go through the
[01:00:01.600 --> 01:00:06.240]   Hugging Face course. So if that is of interest to you, as you can see, we have a lot of things going
[01:00:06.240 --> 01:00:12.880]   on. I invite you to join that as well. We keep sharing these links on both LinkedIn and Twitter.
[01:00:12.880 --> 01:00:18.240]   I'm assuming that's where everyone found the session. So you can find those links there. I
[01:00:18.240 --> 01:00:22.880]   look forward to seeing you next month. I look forward to seeing you in either of these sessions.
[01:00:22.880 --> 01:00:28.160]   Thanks for joining everyone. And I'll see you next month in the PRG group.
[01:00:28.160 --> 01:00:41.280]   Thank you for the session. Thanks for your time.
[01:00:42.640 --> 01:00:48.880]   You're welcome.
[01:00:48.880 --> 01:00:53.880]   Thanks for watching.

