
[00:00:00.000 --> 00:00:08.320]   So, first of all, I wanted to thank our sponsors and especially our biggest dollar sponsors,
[00:00:08.320 --> 00:00:12.280]   Kotu and Felicis, who are also investors in Weights and Biases.
[00:00:12.280 --> 00:00:17.280]   And I just wanted to say, honestly, they've been great investors, and specifically, they
[00:00:17.280 --> 00:00:20.880]   pushed us to do this conference, which we were terrified of doing.
[00:00:20.880 --> 00:00:22.800]   We tried to get out of it, but they were totally right.
[00:00:22.800 --> 00:00:24.080]   I'm so glad that we did this.
[00:00:24.080 --> 00:00:25.520]   So thank you, Kotu and Felicis.
[00:00:25.520 --> 00:00:26.520]   It was a great idea.
[00:00:26.520 --> 00:00:32.760]   And thanks to all of our other sponsors.
[00:00:32.760 --> 00:00:36.480]   You all signed up, having no idea how many people were going to show up.
[00:00:36.480 --> 00:00:37.840]   We had no idea either.
[00:00:37.840 --> 00:00:41.920]   You should definitely check out their booths in the back.
[00:00:41.920 --> 00:00:44.320]   I also want to give a shout out to our amazing speakers.
[00:00:44.320 --> 00:00:46.280]   We have awesome talks lined up today.
[00:00:46.280 --> 00:00:50.400]   Every single one of them are talks that I'm personally excited about seeing.
[00:00:50.400 --> 00:00:54.080]   And finally, I wanted to give a shout out to all of our users.
[00:00:54.080 --> 00:00:55.560]   This conference and this talk is really for you.
[00:00:55.560 --> 00:00:56.560]   I'm curious.
[00:00:56.560 --> 00:00:57.560]   Can you just do this for me?
[00:00:57.560 --> 00:00:59.600]   How many people are users of Weights and Biases?
[00:00:59.600 --> 00:01:00.600]   Tell me.
[00:01:00.600 --> 00:01:01.600]   Awesome.
[00:01:01.600 --> 00:01:02.600]   Thank you.
[00:01:02.600 --> 00:01:03.600]   All right.
[00:01:03.600 --> 00:01:04.600]   So I'm Lucas.
[00:01:04.600 --> 00:01:09.960]   I'm the CEO and one of the cofounders of Weights and Biases.
[00:01:09.960 --> 00:01:14.280]   And at Weights and Biases, we've been on a mission to build the best tools for machine
[00:01:14.280 --> 00:01:18.120]   practitioners, machine learning practitioners.
[00:01:18.120 --> 00:01:22.660]   And today, I want to talk about, one, the progress that we've made over the past five
[00:01:22.660 --> 00:01:29.920]   years against our mission, and two, our perspective on large language models or foundation models
[00:01:29.920 --> 00:01:36.840]   and our vision on how our product evolves to help make LLMs work in the real world.
[00:01:36.840 --> 00:01:40.880]   So Weights and Biases' mission has been remarkably similar since we started.
[00:01:40.880 --> 00:01:46.400]   And I think that's because Sean and Chris and I have always cared so much about the
[00:01:46.400 --> 00:01:51.960]   power of machine learning and the chance to help companies really make machine learning
[00:01:51.960 --> 00:01:54.060]   work in the real world.
[00:01:54.060 --> 00:01:58.460]   And we started Weights and Biases in 2018, but we had actually been doing machine learning
[00:01:58.460 --> 00:02:02.100]   and building machine learning tools since 2003.
[00:02:02.100 --> 00:02:05.220]   And so when we started Weights and Biases, we did have a sense of the problem that we
[00:02:05.220 --> 00:02:07.480]   were solving.
[00:02:07.480 --> 00:02:09.640]   And we articulated this way from the very beginning, right?
[00:02:09.640 --> 00:02:14.220]   There's software 1.0, which I always feel a little bit saying, but Andrej Karpathy said
[00:02:14.220 --> 00:02:17.120]   it, so if it's good enough for him, it's good enough for me.
[00:02:17.120 --> 00:02:21.760]   Software 1.0, we call the way -- we mean the way of developing software that's been around
[00:02:21.760 --> 00:02:22.760]   for 70 years.
[00:02:22.760 --> 00:02:27.800]   And in the past 20 years, we've seen just an explosion of developer tools.
[00:02:27.800 --> 00:02:33.200]   Software 2.0 is the machine learning software development that all of us in this room are
[00:02:33.200 --> 00:02:34.640]   doing.
[00:02:34.640 --> 00:02:40.640]   And it has some similarities, some similar problems, but it uses surprisingly few of
[00:02:40.640 --> 00:02:44.520]   the developer tools written for software 1.0.
[00:02:44.520 --> 00:02:47.480]   And so when we started Weights and Biases, we had two core theses, and they actually
[00:02:47.480 --> 00:02:50.200]   were both controversial at the time.
[00:02:50.200 --> 00:02:55.400]   Our first thesis was just that software 2.0 needed a different set of tools.
[00:02:55.400 --> 00:02:56.400]   And this really was controversial.
[00:02:56.400 --> 00:03:00.680]   This is the first thing that our friends asked us, that our investors asked us, "Do you really
[00:03:00.680 --> 00:03:03.240]   need a different set of tools for machine learning?
[00:03:03.240 --> 00:03:04.580]   The workflow looks similar.
[00:03:04.580 --> 00:03:07.940]   Why would ML need its own CI/CD system?"
[00:03:07.940 --> 00:03:12.080]   And our response was just that the workflow is known, the pain points are known, they've
[00:03:12.080 --> 00:03:18.640]   been known for years, and yet nobody is using the existing tools for ML.
[00:03:18.640 --> 00:03:25.000]   Tools are so crucial to ML development, but nobody puts their models in GitHub.
[00:03:25.000 --> 00:03:29.800]   And fundamentally, I think this is because machine learning development, as you know,
[00:03:29.800 --> 00:03:31.900]   is a lot different than software development.
[00:03:31.900 --> 00:03:33.960]   Software development feels like engineering.
[00:03:33.960 --> 00:03:34.960]   It's so fun.
[00:03:34.960 --> 00:03:36.000]   It's so satisfying, right?
[00:03:36.000 --> 00:03:40.880]   Every step you make generally takes you closer to a goal, whereas machine learning is more
[00:03:40.880 --> 00:03:42.880]   of an exploration.
[00:03:42.880 --> 00:03:47.840]   And that's a really deep difference that makes your pain different and makes the tools that
[00:03:47.840 --> 00:03:50.760]   you want different.
[00:03:50.760 --> 00:03:57.240]   And our second core thesis related was that we wanted to build tools for the ML practitioners
[00:03:57.240 --> 00:04:00.880]   specifically, the people building and using the ML models.
[00:04:00.880 --> 00:04:07.040]   And this was in real contrast to our competitors at the time who were uniformly building for
[00:04:07.040 --> 00:04:09.840]   executives or building for organizations doing ML.
[00:04:09.840 --> 00:04:13.240]   And so they would talk about democratizing AI all the time, and that's something we care
[00:04:13.240 --> 00:04:14.240]   about too.
[00:04:14.240 --> 00:04:19.200]   But democratizing AI to our competitors was basically like, "Wow, these ML experts are
[00:04:19.200 --> 00:04:20.200]   really expensive.
[00:04:20.200 --> 00:04:25.920]   Maybe we could automate them out of a job and just make ML without any ML practitioners."
[00:04:25.920 --> 00:04:32.600]   And so as our competition built an end-to-end platform, which totally makes sense, every
[00:04:32.600 --> 00:04:35.880]   executive that I talk to has wanted to buy one tool.
[00:04:35.880 --> 00:04:38.080]   They always say, "Look, I don't want to piece together things.
[00:04:38.080 --> 00:04:40.760]   I want to buy one end-to-end platform."
[00:04:40.760 --> 00:04:44.280]   We asked the practitioners what they wanted, and they wanted specific solutions to real
[00:04:44.280 --> 00:04:46.000]   problems that they had.
[00:04:46.000 --> 00:04:50.280]   And so we started with a single pain point, and we picked that pain point to be experiment
[00:04:50.280 --> 00:04:51.280]   tracking.
[00:04:51.280 --> 00:04:55.640]   And I think we picked it intuitively because it was the thing that ML practitioners felt
[00:04:55.640 --> 00:05:02.520]   the most, and it was the thing that executives and VCs understood the least.
[00:05:02.520 --> 00:05:05.600]   And so now everyone loves...
[00:05:05.600 --> 00:05:06.840]   They call it PLG, right?
[00:05:06.840 --> 00:05:10.040]   Like selling to individual contributors and then rolling it up into a deal.
[00:05:10.040 --> 00:05:12.400]   And everyone loves machine learning tools.
[00:05:12.400 --> 00:05:17.480]   But I swear to you, five years ago, investors told me, "ML practitioners have no budget,
[00:05:17.480 --> 00:05:20.400]   and there's probably only 1,000 practitioners there.
[00:05:20.400 --> 00:05:26.120]   Now we have a few thousand people in a room watching this talk just in San Francisco."
[00:05:26.120 --> 00:05:29.880]   And just to pull one think piece from the time, I don't know if any of you remember
[00:05:29.880 --> 00:05:34.360]   this, "Your deep learning tools for enterprises startup will fail," is literally a popular
[00:05:34.360 --> 00:05:36.360]   Medium article that came out.
[00:05:36.360 --> 00:05:37.800]   And it wasn't a crazy article.
[00:05:37.800 --> 00:05:38.800]   It actually made sense.
[00:05:38.800 --> 00:05:42.880]   But there's a subtle point and a true point that the author is making here, which is,
[00:05:42.880 --> 00:05:46.080]   "Look, deep learning applications are super disparate.
[00:05:46.080 --> 00:05:47.440]   Every company does it differently.
[00:05:47.440 --> 00:05:50.600]   Every data set needs its own way to evaluate it.
[00:05:50.600 --> 00:05:53.960]   Every application needs a different type of analysis.
[00:05:53.960 --> 00:05:57.560]   Every use case needs a different visualization to work with it."
[00:05:57.560 --> 00:05:58.800]   And you know what?
[00:05:58.800 --> 00:06:02.860]   Our first year post-launch, it felt like Clemens was right.
[00:06:02.860 --> 00:06:05.280]   This is our actual graph of active users post-launch.
[00:06:05.280 --> 00:06:09.640]   And you can see that our weekly count one year in, this is really for the early-stage
[00:06:09.640 --> 00:06:14.560]   entrepreneurs in the audience, our weekly user count was 23.
[00:06:14.560 --> 00:06:17.280]   But I was honestly looking at this like, "Man, this graph is going up.
[00:06:17.280 --> 00:06:18.280]   I like this."
[00:06:18.280 --> 00:06:22.960]   But no one else felt that way.
[00:06:22.960 --> 00:06:25.280]   And we didn't really know what to do.
[00:06:25.280 --> 00:06:30.020]   We kind of just continued to do what was intuitive to us.
[00:06:30.020 --> 00:06:32.640]   And Carrie, who's here today, worked deeply.
[00:06:32.640 --> 00:06:34.600]   We all worked deeply with our heaviest users.
[00:06:34.600 --> 00:06:36.840]   We emphasized the happiest users.
[00:06:36.840 --> 00:06:40.480]   And this is a photo I love of Carrie on-site with Peter at OpenAI.
[00:06:40.480 --> 00:06:44.160]   And I think the reason I love it was because we noticed here that OpenAI actually had a
[00:06:44.160 --> 00:06:47.120]   Weights & Biases dashboard up on their wall.
[00:06:47.120 --> 00:06:48.680]   And that gave us some hope.
[00:06:48.680 --> 00:06:52.240]   We thought, "Well, maybe there's only 23 users we can count, but some people are walking
[00:06:52.240 --> 00:06:53.920]   by that and looking at the dashboard.
[00:06:53.920 --> 00:06:57.000]   So that's kind of like a user."
[00:06:57.000 --> 00:07:02.200]   We also tracked feedback from the very beginning and listened carefully.
[00:07:02.200 --> 00:07:07.640]   And then we saw that people really liked our product, but our friends kept saying, "You
[00:07:07.640 --> 00:07:08.640]   know, I don't know.
[00:07:08.640 --> 00:07:09.640]   I haven't tried it yet.
[00:07:09.640 --> 00:07:10.640]   I haven't gotten around to it."
[00:07:10.640 --> 00:07:14.160]   And we kept taking away excuses for not using our product.
[00:07:14.160 --> 00:07:18.920]   And so we really feel proud that we've gotten down to a 60-second integration to get something
[00:07:18.920 --> 00:07:20.520]   useful out of Weights & Biases.
[00:07:20.520 --> 00:07:24.120]   And actually, in order to do that, we had to integrate with everything out there, everything
[00:07:24.120 --> 00:07:26.680]   that ML practitioners are already using.
[00:07:26.680 --> 00:07:32.600]   We actually have a lot of our integration partners here today.
[00:07:32.600 --> 00:07:37.640]   And as we continued to work on this, we started to hear from our customers, "Hey, you know,
[00:07:37.640 --> 00:07:40.400]   we think you spent too much time on experiment tracking.
[00:07:40.400 --> 00:07:42.000]   We like the experiment tracking.
[00:07:42.000 --> 00:07:45.640]   Can you take the same approach and solve some of my other pain points?"
[00:07:45.640 --> 00:07:49.320]   And so the first thing we did was we built sweeps to help optimize models.
[00:07:49.320 --> 00:07:52.480]   We built artifacts to help version data and pipelines.
[00:07:52.480 --> 00:07:54.360]   We built tables to prep and visualize data.
[00:07:54.360 --> 00:07:58.000]   This is like a five-year product roadmap I'm summarizing in five seconds here.
[00:07:58.000 --> 00:07:59.000]   We built models.
[00:07:59.000 --> 00:08:01.640]   We launched that a few months ago as a model registry.
[00:08:01.640 --> 00:08:07.920]   And we built automations, just shipped that very recently, to do CI/CD.
[00:08:07.920 --> 00:08:11.400]   But I think the important thing that I wanted to emphasize is in that process of building
[00:08:11.400 --> 00:08:16.560]   these specific solutions, we started to realize that there were underlying pieces that were
[00:08:16.560 --> 00:08:20.360]   actually bigger than the individual solutions that were necessary to make this work across
[00:08:20.360 --> 00:08:21.720]   all of our customers.
[00:08:21.720 --> 00:08:25.240]   And the first thing is we had to make it work inside of large companies.
[00:08:25.240 --> 00:08:27.840]   So we had to support managed cloud.
[00:08:27.840 --> 00:08:31.280]   We had to support a way that folks didn't have to send all their data to Weights and
[00:08:31.280 --> 00:08:34.360]   Biases to get the value out of the analysis.
[00:08:34.360 --> 00:08:40.360]   And we realized that actually 70, 80% of our customer base doing machine learning is enterprise.
[00:08:40.360 --> 00:08:45.720]   And I think even as ML becomes more mainstream, security and privacy become more and more
[00:08:45.720 --> 00:08:47.560]   critical, not less.
[00:08:47.560 --> 00:08:54.000]   The second thing is, and this was a real surprise to us, when you're doing a machine learning
[00:08:54.000 --> 00:08:59.880]   experimental workflow, the work product is actually the things that you're learning.
[00:08:59.880 --> 00:09:03.000]   And so you want to share those learnings with the other parts of your team.
[00:09:03.000 --> 00:09:04.920]   That's actually the core IP that you generate.
[00:09:04.920 --> 00:09:11.880]   And so we built reports to support collaboration everywhere.
[00:09:11.880 --> 00:09:16.240]   We also learned that when you're doing an experimental workflow, you often want to go
[00:09:16.240 --> 00:09:18.240]   back and understand what a colleague did.
[00:09:18.240 --> 00:09:24.320]   And that colleague is often your previous self six weeks ago or six months ago.
[00:09:24.320 --> 00:09:30.760]   And so making reproducibility even possible is critical and often overlooked.
[00:09:30.760 --> 00:09:36.120]   And then finally, Clemens was totally right, that every application needs a different analysis
[00:09:36.120 --> 00:09:37.840]   and a different visualization.
[00:09:37.840 --> 00:09:43.440]   And so we worked super hard to figure out how to let our users, let our customers build
[00:09:43.440 --> 00:09:49.160]   their own visualizations that they needed to get the analysis they needed to do.
[00:09:49.160 --> 00:09:55.160]   And so now I feel super proud that we really are a full MLOps platform.
[00:09:55.160 --> 00:09:59.660]   But in that process of building, I feel like we've kept our core tenets of simplicity and
[00:09:59.660 --> 00:10:03.960]   interoperability and focus on developers.
[00:10:03.960 --> 00:10:06.640]   And I feel a little shy showing this slide, but I feel so proud of it.
[00:10:06.640 --> 00:10:09.640]   At our first user conference, I just want to show you the customer appreciation that
[00:10:09.640 --> 00:10:12.880]   I just, I really appreciate the positive things that people have said.
[00:10:12.880 --> 00:10:15.040]   And it's kind of exactly what we hope they said.
[00:10:15.040 --> 00:10:19.400]   Like at Toyota Research, weights and biases use the official record of every ML model.
[00:10:19.400 --> 00:10:23.240]   And Adrian's talking later, so you can ask him if he still believes that after he said
[00:10:23.240 --> 00:10:25.360]   it.
[00:10:25.360 --> 00:10:30.520]   And that graph actually got a lot better.
[00:10:30.520 --> 00:10:34.800]   So the qualitative feedback was good for a long time, but then actually it turned into
[00:10:34.800 --> 00:10:36.880]   massive user growth that you always dream of.
[00:10:36.880 --> 00:10:43.880]   That's a catch-22.
[00:10:43.880 --> 00:10:49.560]   But honestly, the thing that I feel most proud of is the diversity and breadth of the customers
[00:10:49.560 --> 00:10:51.060]   using weights and biases.
[00:10:51.060 --> 00:10:56.440]   People ask me, what's the best part, the most fun part of running weights and biases?
[00:10:56.440 --> 00:11:00.200]   And it's so clear to me that it's having front row seats to this explosion of ML applications.
[00:11:00.200 --> 00:11:05.720]   Like I remember a lot of us coming out of college, the interesting ML applications out
[00:11:05.720 --> 00:11:08.840]   there is you could rank ads or you could try to make money on Wall Street.
[00:11:08.840 --> 00:11:13.840]   And now we see ML teams making awesome games, making organic farming easier, making better
[00:11:13.840 --> 00:11:16.880]   medicine, making customer support less annoying.
[00:11:16.880 --> 00:11:20.360]   Like we just live vicariously through all the amazing things that our customers are
[00:11:20.360 --> 00:11:22.320]   doing.
[00:11:22.320 --> 00:11:26.200]   And honestly, investors still actually ask me about the TAM of the market, but I just
[00:11:26.200 --> 00:11:30.280]   don't worry about it because I don't think there's any company out there of significant
[00:11:30.280 --> 00:11:33.640]   size that at this point is not doing ML in some shape or form.
[00:11:33.640 --> 00:11:37.520]   And we know this because we see it across every industry, people are using weights and
[00:11:37.520 --> 00:11:38.520]   biases.
[00:11:38.520 --> 00:11:42.760]   So we feel so proud of that work.
[00:11:42.760 --> 00:11:46.800]   But then what came along next is probably the reason that so many of you are here, which
[00:11:46.800 --> 00:11:52.720]   is this huge shock to our industry, which is the explosion of LLMs.
[00:11:52.720 --> 00:11:55.160]   And I'm sure that's what many of you are here to talk about.
[00:11:55.160 --> 00:12:00.440]   And that's what we're going to talk about for the rest of this presentation.
[00:12:00.440 --> 00:12:04.680]   I think the best way I've heard of putting what's happened in LLMs is something Sequoia
[00:12:04.680 --> 00:12:07.800]   said, which is it is a Cambrian explosion.
[00:12:07.800 --> 00:12:10.020]   I think that's a great metaphor.
[00:12:10.020 --> 00:12:14.440]   And I think back to when everyone started building Facebook apps and there was so much
[00:12:14.440 --> 00:12:18.100]   excitement or when iOS came out and everyone was building those apps.
[00:12:18.100 --> 00:12:23.280]   And this feels like that, but it feels like a million times bigger than that.
[00:12:23.280 --> 00:12:30.400]   And the amazing thing is that this technology really works in a tangible way.
[00:12:30.400 --> 00:12:34.500]   Suddenly natural language interfaces to products are not annoying.
[00:12:34.500 --> 00:12:38.080]   Suddenly chat bots are something I want to engage with.
[00:12:38.080 --> 00:12:41.840]   And honestly, I can fill these slides with these incredible images in the background
[00:12:41.840 --> 00:12:46.420]   basically just for fun at the last minute.
[00:12:46.420 --> 00:12:49.920]   And the crazy thing also to realize is this tech has only been out for a couple months.
[00:12:49.920 --> 00:12:55.000]   So there's obviously so much disruption that's about to happen everywhere.
[00:12:55.000 --> 00:13:00.160]   And it's such a cliche as a tech CEO to talk about disruption innovation.
[00:13:00.160 --> 00:13:02.760]   I don't know how to emphasize this enough.
[00:13:02.760 --> 00:13:05.000]   I think some of us might miss it.
[00:13:05.000 --> 00:13:10.160]   This really is a special moment in tech that I have never seen in my career.
[00:13:10.160 --> 00:13:13.200]   And I really think this is a moment that our grandkids will be asking us about.
[00:13:13.200 --> 00:13:15.520]   What was it like when LLMs first came out?
[00:13:15.520 --> 00:13:22.520]   And so what's happened with that is an explosion of LLMOps developer tools.
[00:13:22.520 --> 00:13:26.960]   It's interesting to think about ML, which was around for decades before I think the
[00:13:26.960 --> 00:13:30.280]   real first production ML application.
[00:13:30.280 --> 00:13:34.280]   And then ML applications were around for at least a decade before anyone coined the term
[00:13:34.280 --> 00:13:35.600]   ML Ops.
[00:13:35.600 --> 00:13:40.400]   And then ML Ops was actually around for quite a while before any investor made an infographic
[00:13:40.400 --> 00:13:42.880]   of ML Ops companies.
[00:13:42.880 --> 00:13:50.600]   But LLMOps has exploded on a very accelerated timescale.
[00:13:50.600 --> 00:13:54.360]   This is not like raising for ML Ops five, six years ago.
[00:13:54.360 --> 00:13:59.120]   There's actually been an explosion of LLMOps developer tools infographics.
[00:13:59.120 --> 00:14:01.640]   There's so many tools that everyone can put out an infographic.
[00:14:01.640 --> 00:14:06.920]   And it might make you think, wow, maybe we're at peak hype, especially when you consider
[00:14:06.920 --> 00:14:12.100]   the fact that almost none of these LLM applications are in production yet.
[00:14:12.100 --> 00:14:15.680]   So none of these LLM companies are making money yet.
[00:14:15.680 --> 00:14:20.860]   And yet there's enough developer tools to fill a market map just serving these non-money
[00:14:20.860 --> 00:14:22.520]   making entities.
[00:14:22.520 --> 00:14:24.520]   How could this be?
[00:14:24.520 --> 00:14:28.640]   And I think my explanation is no one read my think piece, your LLM tools, remember
[00:14:28.640 --> 00:14:35.160]   I started to fail.
[00:14:35.160 --> 00:14:40.560]   But honestly, even though I understand why people think that there's hype and there is
[00:14:40.560 --> 00:14:45.320]   hype, I actually think that we might be even underestimating what's going on.
[00:14:45.320 --> 00:14:50.960]   In fact, a question that we keep asking ourselves is, does it matter to build the best tools
[00:14:50.960 --> 00:14:57.120]   for machine learning practitioners if GPT-4 can outperform models on a ton of different
[00:14:57.120 --> 00:14:58.120]   use cases?
[00:14:58.120 --> 00:15:03.360]   Is anyone going to be building models in the next year or two?
[00:15:03.360 --> 00:15:07.000]   And when we think about machine learning practitioners, we go back to our original definition, which
[00:15:07.000 --> 00:15:09.160]   is in our onboarding documentation.
[00:15:09.160 --> 00:15:13.700]   And I think it's notable that we didn't define machine learning practitioner as someone who
[00:15:13.700 --> 00:15:18.140]   builds neural networks or builds boosted trees.
[00:15:18.140 --> 00:15:22.160]   We think about someone who's actually trying to make machine learning do something.
[00:15:22.160 --> 00:15:27.520]   So our quick assessment is that LLMs unlock the ability of almost anyone, at least any
[00:15:27.520 --> 00:15:31.560]   software developer, to be a machine learning practitioner.
[00:15:31.560 --> 00:15:34.260]   And in fact, we see this in our own internal data.
[00:15:34.260 --> 00:15:38.700]   So when we first started the company, literally everyone was an ML researcher or a machine
[00:15:38.700 --> 00:15:40.160]   learning engineer.
[00:15:40.160 --> 00:15:44.480]   And today when we take a sample of our users and we look at their titles on LinkedIn, we
[00:15:44.480 --> 00:15:49.000]   see that over half of our users look more like software engineers or founders or product
[00:15:49.000 --> 00:15:52.240]   managers than traditional ML engineers or researchers.
[00:15:52.240 --> 00:15:55.880]   And we expect this trend to accelerate.
[00:15:55.880 --> 00:16:00.880]   People have been talking a lot about the democratization of AI, but I think this is the form that you're
[00:16:00.880 --> 00:16:01.880]   seeing it.
[00:16:01.880 --> 00:16:03.520]   And I think it's finally happening.
[00:16:03.520 --> 00:16:06.840]   And we plan to embrace it.
[00:16:06.840 --> 00:16:11.320]   We have a slightly more subtle perspective on what happens to ML practitioners, which
[00:16:11.320 --> 00:16:14.340]   is that we think it splits into three personas.
[00:16:14.340 --> 00:16:18.040]   And today you might actually be all three of these personas, but I think that they're
[00:16:18.040 --> 00:16:20.240]   going to start to diverge.
[00:16:20.240 --> 00:16:24.800]   So first of all, we have LLM creators, which are building the foundation models that other
[00:16:24.800 --> 00:16:26.280]   people use.
[00:16:26.280 --> 00:16:32.520]   LLM fine tuners that are taking the models and training them on their own data.
[00:16:32.520 --> 00:16:38.200]   And then finally, LLM users, maybe this is what some people call prompt engineers, that
[00:16:38.200 --> 00:16:42.760]   take the LLMs and make them do something useful without retraining them.
[00:16:42.760 --> 00:16:47.120]   So how do we think about these three personas and what are we doing for these three personas?
[00:16:47.120 --> 00:16:52.800]   First of all, LLM creators, the people making the LLMs, we love working with you.
[00:16:52.800 --> 00:16:57.940]   We've worked super hard to make Weights and Biases scalable for large distributed models.
[00:16:57.940 --> 00:17:03.460]   And we're really proud that we work with almost all of the organizations that have built important
[00:17:03.460 --> 00:17:04.460]   foundation models.
[00:17:04.460 --> 00:17:10.500]   This is NVIDIA, this is Anthropic, Mosaic, Stability, Midjourney, Hugging Face, Meta,
[00:17:10.500 --> 00:17:11.500]   OpenAI.
[00:17:11.500 --> 00:17:15.580]   I can't think of their household names now, but maybe in the future.
[00:17:15.580 --> 00:17:19.220]   And I think that innovation here is super important and innovation comes from lots of
[00:17:19.220 --> 00:17:20.700]   different people trying it.
[00:17:20.700 --> 00:17:26.580]   So we are excited to as best we can help democratize the building of LLMs.
[00:17:26.580 --> 00:17:33.440]   Unfortunately, we can't make GPUs cheaper, but we can help share insights.
[00:17:33.440 --> 00:17:35.300]   And Boris is actually here today.
[00:17:35.300 --> 00:17:36.980]   Shout out to Boris if you're in the audience.
[00:17:36.980 --> 00:17:38.220]   I saw him earlier.
[00:17:38.220 --> 00:17:43.940]   Boris was the first person to really do this, where he built his own version of Dolly, Dolly
[00:17:43.940 --> 00:17:47.100]   Mini, completely by himself.
[00:17:47.100 --> 00:17:48.900]   And he tracked everything out in the open.
[00:17:48.900 --> 00:17:52.760]   So you can actually go see exactly what he was doing.
[00:17:52.760 --> 00:17:57.260]   And then I think this inspired Eleuther to train all of their models in public.
[00:17:57.260 --> 00:18:02.260]   So every single experiment that Eleuther does, and by the way, they make GPT-J, GPT-NeoX,
[00:18:02.260 --> 00:18:05.100]   if you're fine tuning, probably one of the models you're using.
[00:18:05.100 --> 00:18:10.060]   You can see every detail of every run they did, whether it was successful or unsuccessful.
[00:18:10.060 --> 00:18:16.560]   And to me, this is science happening in public, and we'd love, love to support this.
[00:18:16.560 --> 00:18:20.300]   And so for people, we hear a lot of this, if you're just thinking about training an
[00:18:20.300 --> 00:18:21.580]   LLM from scratch.
[00:18:21.580 --> 00:18:26.440]   We combined all the best practices that we heard from all our customers doing it and
[00:18:26.440 --> 00:18:30.200]   put it into a white paper that we've made publicly available.
[00:18:30.200 --> 00:18:34.740]   And so we will absolutely help you train your own LLM, because I think it's good for the
[00:18:34.740 --> 00:18:38.700]   world for people to try it, and it's good for our business when people try it.
[00:18:38.700 --> 00:18:42.580]   But honestly, you probably shouldn't be training your own LLM.
[00:18:42.580 --> 00:18:47.420]   You probably should at least be fine tuning one of the many existing fantastic LLMs.
[00:18:47.420 --> 00:18:50.600]   Which leads me to the LLM fine tuners.
[00:18:50.600 --> 00:18:52.940]   And this is, I think, the bulk of our users today.
[00:18:52.940 --> 00:18:57.580]   And I just want to say, if you're fine tuning an LLM, we have products for you.
[00:18:57.580 --> 00:18:58.940]   We have awesome products for you.
[00:18:58.940 --> 00:19:02.740]   First of all, we have an integration with Hugging Face, which is actually a zero line
[00:19:02.740 --> 00:19:03.740]   integration.
[00:19:03.740 --> 00:19:08.220]   So when you switch to port weights and biases, you will suddenly get visualizations of all
[00:19:08.220 --> 00:19:13.060]   the hardware you're using, all the loss curves, how well your model is doing as it trains
[00:19:13.060 --> 00:19:14.460]   over time.
[00:19:14.460 --> 00:19:18.260]   Inevitably, you're going to want to tune hyperparameters.
[00:19:18.260 --> 00:19:21.080]   And most people do this totally ad hoc.
[00:19:21.080 --> 00:19:22.300]   This wastes a ton of money.
[00:19:22.300 --> 00:19:23.660]   It wastes a ton of time.
[00:19:23.660 --> 00:19:28.300]   So we've made sweeps so easy, you have no excuse for not doing a principled hyperparameter
[00:19:28.300 --> 00:19:30.300]   search when you're fine tuning LLMs.
[00:19:30.300 --> 00:19:31.540]   You're going to want to track data.
[00:19:31.540 --> 00:19:33.140]   So you can use artifacts for that.
[00:19:33.140 --> 00:19:36.580]   The data tracking gets more complicated when you're chaining together models, when you're
[00:19:36.580 --> 00:19:38.540]   fine tuning over and over.
[00:19:38.540 --> 00:19:43.180]   You can have an immutable record of how every single model was trained in your system.
[00:19:43.180 --> 00:19:44.660]   You're going to do data exploration.
[00:19:44.660 --> 00:19:46.300]   There's no doubt about it.
[00:19:46.300 --> 00:19:50.820]   Weights and biases tables is like an interactive data frame backed by a database.
[00:19:50.820 --> 00:19:52.260]   And it's not just text and numbers.
[00:19:52.260 --> 00:19:56.960]   You can see images, graphs, molecules, even custom visualizations.
[00:19:56.960 --> 00:19:58.980]   And finally, Kerry showed you the model registry.
[00:19:58.980 --> 00:20:02.540]   But this works great for fine tuning, especially well for fine tuning.
[00:20:02.540 --> 00:20:07.300]   You can run a standard set of tests before anything goes into production.
[00:20:07.300 --> 00:20:11.620]   But I think the most important constituent, the fastest growing constituent, and probably
[00:20:11.620 --> 00:20:17.260]   long term, the biggest constituent here is the LLM users.
[00:20:17.260 --> 00:20:20.860]   And we are really, really excited to support the LLM users.
[00:20:20.860 --> 00:20:25.460]   But it presents us with a significant new challenge, a different challenge that we haven't
[00:20:25.460 --> 00:20:26.700]   seen at Weights and Biases.
[00:20:26.700 --> 00:20:29.800]   Because we knew deeply the ML workflow.
[00:20:29.800 --> 00:20:34.500]   But I honestly think no one really knows the LLM workflow.
[00:20:34.500 --> 00:20:37.660]   I think we're figuring that out together right now.
[00:20:37.660 --> 00:20:40.420]   So there are some pain points that we know about.
[00:20:40.420 --> 00:20:42.160]   Prompt engineering is a pain point.
[00:20:42.160 --> 00:20:43.680]   Model selection is a pain point.
[00:20:43.680 --> 00:20:47.060]   And we will build for those pain points.
[00:20:47.060 --> 00:20:50.420]   But I think the workflow is changing right now.
[00:20:50.420 --> 00:20:52.020]   And so what do we do?
[00:20:52.020 --> 00:20:56.180]   First of all, we've put out a course available to all of you.
[00:20:56.180 --> 00:21:00.160]   Best practices, latest best practices for using LLMs.
[00:21:00.160 --> 00:21:05.060]   And you can access it in our other ML Ops courses at this link here, wb.me/courses.
[00:21:05.060 --> 00:21:07.040]   This has really served us in the past.
[00:21:07.040 --> 00:21:08.640]   Because we don't just learn by teaching.
[00:21:08.640 --> 00:21:11.220]   We learn from watching what our students do.
[00:21:11.220 --> 00:21:15.700]   We also very quickly shipped a product called Prompts.
[00:21:15.700 --> 00:21:20.020]   So this helps LLM users track and debug the prompts that they're making and putting in
[00:21:20.020 --> 00:21:25.260]   front of LLMs, especially in the context of multiple steps chained together.
[00:21:25.260 --> 00:21:27.660]   We made a great integration with LangChain.
[00:21:27.660 --> 00:21:30.300]   And the LangChain founder Harrison is speaking later.
[00:21:30.300 --> 00:21:31.740]   And a great integration with OpenAI.
[00:21:31.740 --> 00:21:34.440]   So it's super easy to use.
[00:21:34.440 --> 00:21:38.820]   But I would not claim that this solves every problem you have.
[00:21:38.820 --> 00:21:46.020]   And so we're thinking about how do we build a cohesive end-to-end platform for LLM users.
[00:21:46.020 --> 00:21:50.040]   And in thinking about this, we go back to kind of the core needs of the people doing
[00:21:50.040 --> 00:21:51.040]   these things.
[00:21:51.040 --> 00:21:53.940]   And again, we look at this where we see writing code versus building models.
[00:21:53.940 --> 00:21:59.660]   This experimental workflow versus this very specific building style of workflow.
[00:21:59.660 --> 00:22:02.140]   And how LLM users fits into that.
[00:22:02.140 --> 00:22:08.940]   And I would argue that building models and using models has a very different kind of
[00:22:08.940 --> 00:22:09.940]   experiment.
[00:22:09.940 --> 00:22:13.620]   But they're both very, very experimental iterative workflows.
[00:22:13.620 --> 00:22:18.740]   And when experiments are the important thing, the speed that you can get feedback is always
[00:22:18.740 --> 00:22:19.840]   a limiting factor.
[00:22:19.840 --> 00:22:24.780]   So tightening up that cycle where you get feedback faster is the most important thing
[00:22:24.780 --> 00:22:26.240]   for productivity.
[00:22:26.240 --> 00:22:31.940]   And the second thing is that the learning that you do along the way really is your product.
[00:22:31.940 --> 00:22:33.780]   It really is your IP.
[00:22:33.780 --> 00:22:38.660]   And so making it easy to save and share that IP is crucial to being successful when you're
[00:22:38.660 --> 00:22:42.180]   building LLMs or when you're using LLMs.
[00:22:42.180 --> 00:22:48.380]   And so because the core of the workflow is the same with building LLMs and using LLMs,
[00:22:48.380 --> 00:22:53.420]   the core of our infrastructure I actually think works really well today for using LLMs.
[00:22:53.420 --> 00:22:57.460]   So experiment tracking.
[00:22:57.460 --> 00:23:03.420]   So both use cases need reproducibility, flexibility, and collaboration.
[00:23:03.420 --> 00:23:06.140]   Enterprise keeps us secure at every step.
[00:23:06.140 --> 00:23:10.260]   Reports is still great for collaboration when you're using LLMs.
[00:23:10.260 --> 00:23:12.660]   Launch is great for reproducibility.
[00:23:12.660 --> 00:23:16.780]   We launched this recently and we've already built an LLM evaluation use case on top of
[00:23:16.780 --> 00:23:18.500]   it that you can really use.
[00:23:18.500 --> 00:23:24.460]   And then finally, Weave is the part of our architecture that lets us build powerful visualizations.
[00:23:24.460 --> 00:23:26.180]   And it's super flexible.
[00:23:26.180 --> 00:23:28.060]   And so opening up to the world has been really hard.
[00:23:28.060 --> 00:23:31.880]   And we've been working on this for actually several years.
[00:23:31.880 --> 00:23:36.420]   So we use Weave internally to build our own tools quickly, like prompts that I showed
[00:23:36.420 --> 00:23:37.420]   you.
[00:23:37.420 --> 00:23:40.380]   And we're going to continue to use it to build our own tools.
[00:23:40.380 --> 00:23:45.380]   But we know that we can't build tools that every ML team needs to use.
[00:23:45.380 --> 00:23:50.900]   And so we want to make this available to more people so that you can build the own things,
[00:23:50.900 --> 00:23:57.060]   the own analyses that you need to be successful in whatever kind of application you have.
[00:23:57.060 --> 00:24:03.580]   And it's a hard thing to show, but we want to show it to you through amazing applications
[00:24:03.580 --> 00:24:04.940]   that we've built on top of it.
[00:24:04.940 --> 00:24:08.740]   And so if all you take away is just these exciting applications that you can use, that's
[00:24:08.740 --> 00:24:09.840]   great.
[00:24:09.840 --> 00:24:13.700]   But we also want you to know that you can make your own applications.
[00:24:13.700 --> 00:24:19.020]   And to give you an extended introduction to Weave, here is my co-founder, Sean, to do
[00:24:19.020 --> 00:24:20.020]   it's his brainchild.
[00:24:20.020 --> 00:24:21.020]   Thank you.
[00:24:21.020 --> 00:24:21.020]   [ Applause ]
[00:24:21.340 --> 00:24:23.340]   [ Music ]
[00:24:23.660 --> 00:24:25.660]   [ Music ]
[00:24:25.660 --> 00:24:35.060]   [silence]

