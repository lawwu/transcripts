
[00:00:00.000 --> 00:00:06.680]   So can you describe, like, let's take for example, like a speech recognition system.
[00:00:06.680 --> 00:00:10.000]   Can you describe the differences of how you would think about training and deploying if
[00:00:10.000 --> 00:00:16.560]   it was going to like the cloud or a big desktop server versus a Raspberry Pi versus an Arduino?
[00:00:16.560 --> 00:00:17.680]   Yeah.
[00:00:17.680 --> 00:00:26.020]   And the theme again is size and how much space you actually have on these systems.
[00:00:26.020 --> 00:00:33.560]   So you'll be thinking always about how can I make this model as small as possible?
[00:00:33.560 --> 00:00:39.320]   You know, you're looking at making the model probably in the tens of kilobytes for doing,
[00:00:39.320 --> 00:00:44.280]   you know, we have this example of doing speech recognition, and I think it uses like a 20
[00:00:44.280 --> 00:00:45.600]   kilobyte model.
[00:00:45.600 --> 00:00:53.520]   So you're going to be sacrificing accuracy and a whole bunch of other stuff in order
[00:00:53.520 --> 00:00:59.880]   to get something that will actually fit on this really low energy device.
[00:00:59.880 --> 00:01:04.040]   But hopefully it's still accurate enough that it's useful.
[00:01:04.040 --> 00:01:05.040]   Right.
[00:01:05.040 --> 00:01:06.040]   So how do you do that?
[00:01:06.040 --> 00:01:09.320]   Like, how do you how do you reduce the size without compromising accuracy?
[00:01:09.320 --> 00:01:11.640]   Can you describe like some of the techniques?
[00:01:11.640 --> 00:01:12.640]   Yeah.
[00:01:12.640 --> 00:01:21.000]   So I actually just blogged about one trick that I've seen used, but I realized I hadn't
[00:01:21.000 --> 00:01:26.040]   seen in the literature very much, which is where, you know, the classic going back to
[00:01:26.040 --> 00:01:33.360]   AlexNet approach after you do a convolution in like an image recognition network, you
[00:01:33.360 --> 00:01:36.000]   often have like a pooling stage.
[00:01:36.000 --> 00:01:41.800]   So that pooling stage, you know, would either do average pooling or max pooling.
[00:01:41.800 --> 00:01:50.000]   And what that's doing is it's taking the output of the convolution, which is often the same
[00:01:50.000 --> 00:01:53.840]   size as the input, but with a lot more channels.
[00:01:53.840 --> 00:01:59.920]   And then it's taking blocks of like two by two values and it's saying, hey, I'm going
[00:01:59.920 --> 00:02:03.520]   to only take the maximum of that two by two block.
[00:02:03.520 --> 00:02:10.480]   So take four values and output one value or do the same, but do averaging.
[00:02:10.480 --> 00:02:21.200]   And that helps with accuracy, but because you're outputting these very large outputs
[00:02:21.200 --> 00:02:25.440]   from the convolution, that means that you have to have a lot of RAM because you have
[00:02:25.440 --> 00:02:29.440]   to hold the input for the convolution.
[00:02:29.440 --> 00:02:33.560]   And you also have to hold the output, which is the same size as the input, but typically
[00:02:33.560 --> 00:02:35.720]   has more channels.
[00:02:35.720 --> 00:02:39.440]   So the memory size is even larger.
[00:02:39.440 --> 00:02:45.960]   So instead of doing that, a common technique that I've seen in the industry is to use a
[00:02:45.960 --> 00:02:50.640]   stride of two on the convolution.
[00:02:50.640 --> 00:02:54.680]   So instead of having the sliding window just slide over one pixel every time as you're
[00:02:54.680 --> 00:03:01.520]   doing the convolutions, you actually sort of have it two pixels horizontally and vertically.
[00:03:01.520 --> 00:03:14.160]   And that has the effect of outputting the same result or the same size, same number
[00:03:14.160 --> 00:03:19.760]   of elements you would get if you did a convolution plus a sort of a two by two pooling.
[00:03:19.760 --> 00:03:24.960]   But it means that you actually do less compute and you don't have to have nearly as much
[00:03:24.960 --> 00:03:29.520]   kind of active memory kicking around.
[00:03:29.520 --> 00:03:30.520]   Interesting.
[00:03:30.520 --> 00:03:38.520]   I had thought the size of the model, it was just the size of the model's parameters, but
[00:03:38.520 --> 00:03:41.840]   it sounds like you also, I mean, obviously you need some active memory, but it's hard
[00:03:41.840 --> 00:03:45.120]   to imagine that even could be on the order of magnitude of the size of the model.
[00:03:45.120 --> 00:03:50.160]   Like the pixels of the image and then the kind of intermediate results, I guess, can
[00:03:50.160 --> 00:03:52.080]   be bigger than the model.
[00:03:52.080 --> 00:03:53.080]   Yeah.
[00:03:53.080 --> 00:03:58.080]   I mean, well, that's kind of the nice thing about convolution is you get to reuse the
[00:03:58.080 --> 00:04:05.520]   weights in a way that you really don't with fully connected layers.
[00:04:05.520 --> 00:04:12.880]   So you can actually end up with convolution models, the activation memory, taking up a
[00:04:12.880 --> 00:04:16.120]   substantial amount of space.
[00:04:16.120 --> 00:04:19.640]   And I guess I'm also getting into the weeds a bit here because the obvious answer to your
[00:04:19.640 --> 00:04:24.760]   question is also quantization, like taking these floating point models and just turning
[00:04:24.760 --> 00:04:32.520]   them into eight bit because that immediately slashes all of your memory sizes by 75%.
[00:04:32.520 --> 00:04:37.200]   And what about, I mean, I've seen people get onto four bits or even one bit.
[00:04:37.200 --> 00:04:39.160]   Do you have thoughts on that?
[00:04:39.160 --> 00:04:40.160]   Yeah.
[00:04:40.160 --> 00:04:41.160]   Yeah.
[00:04:41.160 --> 00:04:44.160]   That's been some really, really interesting work.
[00:04:44.160 --> 00:04:50.200]   A colleague of mine actually, again, I'll send on a link to the paper, but looked at,
[00:04:50.200 --> 00:04:57.680]   I think it's something about the Pareto optimal, like bit depth for ResNet is like four bits
[00:04:57.680 --> 00:05:00.440]   or something like that.
[00:05:00.440 --> 00:05:05.800]   And there's been some really, really good research about going down to sort of four
[00:05:05.800 --> 00:05:12.840]   bits or two bits, or even going down to sort of binary networks with one bit.
[00:05:12.840 --> 00:05:22.600]   And the biggest challenge from our side is that CPUs aren't generally optimized for anything
[00:05:22.600 --> 00:05:26.160]   other than like eight bit arithmetic.
[00:05:26.160 --> 00:05:36.200]   So going down to these little bit depths, requires some advances in the hardware that
[00:05:36.200 --> 00:05:38.120]   we're actually using.
[00:05:38.120 --> 00:05:42.640]   If you're enjoying gradient descent, you might actually be interested in the main thing we
[00:05:42.640 --> 00:05:48.360]   do here at Weights & Biases, which is making tools to help with machine learning.
[00:05:48.360 --> 00:05:52.080]   If you're building models and you want to track the models that you build, or you want
[00:05:52.080 --> 00:05:55.960]   to track the datasets that go into the models you build, or you want to track the models
[00:05:55.960 --> 00:05:59.720]   that you deploy into production, you can do all that with Weights & Biases.
[00:05:59.720 --> 00:06:03.240]   And best of all, it's free for personal and academic use.
[00:06:03.240 --> 00:06:06.320]   Go check us out at WMB.com and let me know how it goes.

