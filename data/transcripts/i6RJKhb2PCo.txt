
[00:00:00.000 --> 00:00:07.360]   Language models have a pronounced tendency to memorize long passages of particular texts in their training data.
[00:00:07.360 --> 00:00:16.280]   So one of the OpenAI APIs, for example, can regurgitate most of a chapter of a Harry Potter book from a paragraph of a prompt.
[00:00:16.280 --> 00:00:18.800]   But there are a lot of contexts in which that's non-desirable.
[00:00:18.800 --> 00:00:28.800]   So if you're able to understand how memorization arises in the course of training and what influences which data points end up getting memorized,
[00:00:28.800 --> 00:00:31.800]   you can use that to design language models, better language models.
[00:00:31.800 --> 00:00:36.040]   In the context of memorization, there's often... it's not that we don't want models to memorize things.
[00:00:36.040 --> 00:00:42.320]   Certainly we want to memorize what 2+2 is, but there are particular kinds of data, personal and private information, for example,
[00:00:42.320 --> 00:00:49.320]   or information that encodes maybe even true information, but information that we don't want the model to be repeating frequently.
[00:00:49.320 --> 00:00:57.200]   Being able to understand which data points get memorized and why can hopefully enable us to design models in a way that is going to cause them
[00:00:57.200 --> 00:01:01.300]   to only memorize that or we don't care yet to memorize.

