
[00:00:00.000 --> 00:00:13.080]   to hit an echo. I don't hear the echo yet. It's always this
[00:00:13.080 --> 00:00:17.000]   paranoia of mine where I want to make sure we're live because
[00:00:17.000 --> 00:00:24.960]   I've messed up in the past. Let me double check. Awesome. I can
[00:00:24.960 --> 00:00:28.600]   I can hear the echo and we can get started. Before we start,
[00:00:28.600 --> 00:00:30.680]   do you watch Marvel movies?
[00:00:30.680 --> 00:00:35.000]   Not really. I don't I don't watch movies a lot.
[00:00:35.000 --> 00:00:39.360]   But there's this like, there's this superhero Captain Marvel.
[00:00:39.360 --> 00:00:42.440]   She's from like another universe. She's super awesome.
[00:00:42.440 --> 00:00:45.720]   And I wanted to bring that context because whenever I'm
[00:00:45.720 --> 00:00:48.600]   introducing you, it feels like I'm introducing Captain Marvel.
[00:00:48.600 --> 00:00:57.360]   Sorry, I was just gonna say that's what it feels like to
[00:00:57.360 --> 00:01:00.200]   introduce your background because you accomplish so many
[00:01:00.200 --> 00:01:01.040]   awesome things.
[00:01:01.040 --> 00:01:05.440]   Oh, okay. I'm sure you can. I trust you that you pick you're
[00:01:05.440 --> 00:01:09.120]   picking a good, I don't know, reference or analogy, even though
[00:01:09.120 --> 00:01:10.520]   I haven't watched the movies.
[00:01:10.520 --> 00:01:15.280]   But yeah, for the audience. Thanks. Thanks for joining us.
[00:01:15.280 --> 00:01:18.720]   I quickly want to introduce Inez and like I said, she's, if I
[00:01:18.720 --> 00:01:22.000]   may, Captain Marvel of the coding universe. She started
[00:01:22.000 --> 00:01:27.120]   coding at the age of 11 by designing websites. And through
[00:01:27.120 --> 00:01:30.280]   a career path, she eventually ended up luckily for all of us
[00:01:30.280 --> 00:01:35.280]   in open source world creating projects for NLP and AI. We
[00:01:35.280 --> 00:01:38.240]   probably know her through a spacey contributions and also as
[00:01:38.240 --> 00:01:41.880]   a founder of explosion. Many people don't know this, but
[00:01:41.880 --> 00:01:45.160]   spacey is backed by explosion. That's the company behind it.
[00:01:45.160 --> 00:01:48.960]   We'll be asking Inez all questions around that. And to
[00:01:48.960 --> 00:01:51.280]   the people joining us on YouTube, this is the forums
[00:01:51.280 --> 00:01:54.360]   where I'm looking. And if you want to ask any question, the
[00:01:54.600 --> 00:01:58.400]   pinned comment should take you here. And all you have to do to
[00:01:58.400 --> 00:02:05.600]   ask any question is just hit reply. And just leave a comment
[00:02:05.600 --> 00:02:08.960]   here and I'll be I'll be going through them in sequential
[00:02:08.960 --> 00:02:14.440]   order. Awesome. So I guess I'll just go in a random order.
[00:02:14.440 --> 00:02:17.280]   These might not be the most organized, but I'll try to keep
[00:02:17.280 --> 00:02:23.560]   them organized. I recently saw that the Guardian shared the
[00:02:23.560 --> 00:02:28.160]   use prodigious PC for finding quotes in their article. Are
[00:02:28.160 --> 00:02:32.600]   there any particular examples of the products you build that
[00:02:32.600 --> 00:02:36.120]   you're particularly proud of and some that you're not the most
[00:02:36.120 --> 00:02:36.720]   proud of?
[00:02:36.720 --> 00:02:40.760]   Oh, that's an interesting question, especially the not so
[00:02:40.760 --> 00:02:43.600]   proud of. I mean, they have been really cool and interesting use
[00:02:43.600 --> 00:02:45.440]   cases. And it's actually it's funny, because this is a
[00:02:45.440 --> 00:02:48.680]   question I am asked a lot, and I'm always struggling to really
[00:02:48.680 --> 00:02:51.880]   find people always want like, I don't know something like super
[00:02:51.880 --> 00:02:55.160]   magical AI or something that sounds like incredibly fancy.
[00:02:55.160 --> 00:02:58.040]   And the truth is that like, a lot of the use cases that have
[00:02:58.040 --> 00:03:02.360]   been kind of exciting have also been sort of, they don't sound
[00:03:02.360 --> 00:03:04.640]   very exciting. They're often quite boring, like there have
[00:03:04.640 --> 00:03:08.000]   been interesting uses in fact checking in like various also
[00:03:08.000 --> 00:03:12.000]   journalistic areas. There have been very interesting projects
[00:03:12.000 --> 00:03:15.400]   done in like the healthcare domain, but often actually where
[00:03:15.400 --> 00:03:18.480]   you know, the technology makes a big has a big, big impact in
[00:03:18.480 --> 00:03:22.160]   businesses. It can often be like automating something relatively
[00:03:22.160 --> 00:03:27.240]   boring or making internal processes more reliable. And
[00:03:27.240 --> 00:03:31.600]   that's not everything is as like fancy as cool, the Guardian uses
[00:03:31.600 --> 00:03:33.960]   spacey and prodigy. But I also want to make always want to make
[00:03:33.960 --> 00:03:37.160]   sure that like, all these other projects also get, you know,
[00:03:37.160 --> 00:03:42.480]   some recognition or it's not just yet, it's not just all that
[00:03:42.480 --> 00:03:46.480]   stuff that sounds sounds maybe more sounds super exciting. And
[00:03:46.840 --> 00:03:51.840]   I'm sure that about the things we're less proud of. The thing
[00:03:51.840 --> 00:03:54.480]   is, I think it's, if you can extend this a bit more to the
[00:03:54.480 --> 00:03:59.040]   general like ecosystem, they are obviously use cases of NLP that
[00:03:59.040 --> 00:04:02.360]   are very problematic. And of course, you know, we don't
[00:04:02.360 --> 00:04:05.560]   always have an influence on what people use our software for. But
[00:04:05.560 --> 00:04:09.720]   in general, we do, for example, for commercial software, we do
[00:04:09.720 --> 00:04:14.080]   not sell to organisations that are primarily involved in
[00:04:14.400 --> 00:04:17.560]   military and intelligence. And that because that is something,
[00:04:17.560 --> 00:04:20.000]   especially in the government area that's not regulated in
[00:04:20.000 --> 00:04:23.000]   private companies, it is in government use cases, it's not.
[00:04:23.000 --> 00:04:26.240]   And so that is something I'm very wary of. And where I think,
[00:04:26.240 --> 00:04:30.480]   you know, we should keep an eye on how technology is used to
[00:04:30.480 --> 00:04:35.400]   create a framework around using the technology responsibly and
[00:04:35.400 --> 00:04:39.040]   not for things that have a potentially very bad outcome.
[00:04:39.040 --> 00:04:42.160]   Sanyam Bhutani: It's also such a controversial topic, right?
[00:04:42.160 --> 00:04:44.720]   Like someone might say you're patriotic, and you want to
[00:04:44.720 --> 00:04:48.360]   support your own country's military, but then you'd like
[00:04:48.360 --> 00:04:51.320]   technically going against humanity. I don't want to get
[00:04:51.320 --> 00:04:55.120]   into like a super controversial topic. But it's also, it also
[00:04:55.120 --> 00:04:56.920]   gets really spicy for some people.
[00:04:56.920 --> 00:04:58.360]   Dr. Kiran Mazumdar-Shaw: Sure, I think, but I think for
[00:04:58.360 --> 00:05:02.640]   government, the main problem is that there is no regulation. A
[00:05:02.640 --> 00:05:04.920]   lot of the projects are secret, we do not know what people are
[00:05:04.920 --> 00:05:09.280]   working on. And there is much more of a mechanism for private
[00:05:09.280 --> 00:05:13.280]   companies and for like the, you know, the open market. And these
[00:05:13.280 --> 00:05:15.600]   use cases are regulated and government is not and I think
[00:05:15.600 --> 00:05:17.920]   that is even aside from what people are actually working on,
[00:05:17.920 --> 00:05:19.680]   I think that's one of the biggest problems.
[00:05:19.680 --> 00:05:25.120]   Sanyam Bhutani: But again, this is another hot take of mine. I
[00:05:25.120 --> 00:05:29.080]   also assume many people tend to overmarket things, maybe machine
[00:05:29.080 --> 00:05:31.600]   learning isn't that heavily used, maybe not in the most
[00:05:31.600 --> 00:05:33.640]   advanced manners in most of the places.
[00:05:33.640 --> 00:05:37.520]   Dr. Kiran Mazumdar-Shaw: Well, sure, I don't know. I mean, I
[00:05:37.520 --> 00:05:39.800]   think everyone wants to do something with machine learning.
[00:05:39.800 --> 00:05:42.200]   But the you know, the truth is, okay, they are they are very
[00:05:42.200 --> 00:05:44.240]   great use cases where you can really make a difference. And
[00:05:44.240 --> 00:05:47.440]   there are other use cases, you know, where it can't, but I do
[00:05:47.440 --> 00:05:50.760]   think, especially when it comes to, you know, information
[00:05:50.760 --> 00:05:53.800]   extraction, natural language understanding, that's like,
[00:05:53.800 --> 00:05:56.120]   that's an area where actually the technology is already
[00:05:56.120 --> 00:05:59.720]   delivering a lot of value in companies, as opposed to maybe
[00:05:59.720 --> 00:06:03.000]   other use cases that are a lot more like experimental, or it's
[00:06:03.000 --> 00:06:05.480]   like, you know, some company has some innovation lab where they're
[00:06:05.480 --> 00:06:08.960]   trying out something new that might or might not lead
[00:06:08.960 --> 00:06:11.480]   anywhere. But, you know, there is definitely there are a lot of
[00:06:11.480 --> 00:06:15.080]   use cases where a lot of value already comes from machine
[00:06:15.080 --> 00:06:15.440]   learning.
[00:06:15.440 --> 00:06:19.320]   Sanyam Bhutani: Awesome. Sorry for going on that tangent. But
[00:06:19.320 --> 00:06:23.200]   to bring back the original question, like extending it a
[00:06:23.200 --> 00:06:27.080]   bit further, is there any one single project that you're the
[00:06:27.080 --> 00:06:30.200]   most proud of that Spacey has helped accomplish? I'm sure
[00:06:30.200 --> 00:06:31.840]   there would be a lot maybe you could pick a few.
[00:06:32.680 --> 00:06:37.200]   Oh, I mean, I think more generally, I would say, I
[00:06:37.200 --> 00:06:42.120]   definitely, I'm definitely very proud of Spacey having helped a
[00:06:42.120 --> 00:06:45.880]   lot of people get started with NLP and actually go from, you
[00:06:45.880 --> 00:06:49.640]   know, doing, you know, just starting out exploring the
[00:06:49.640 --> 00:06:52.200]   technology to building like really powerful, but really
[00:06:52.200 --> 00:06:56.000]   advanced systems. And that's also something, you know, we've
[00:06:56.000 --> 00:06:58.480]   been trying to do with a, with a software, they've also been
[00:06:58.480 --> 00:07:02.200]   really cool projects that are basically built on top of Spacey
[00:07:02.200 --> 00:07:05.320]   and provide these very task specific pipelines that I think
[00:07:05.320 --> 00:07:09.040]   are really cool. For example, there's Spacey by Allen AI,
[00:07:09.040 --> 00:07:12.680]   who've developed a pipeline with lots of different components.
[00:07:12.680 --> 00:07:15.320]   For the biomedical domain, there's a project called
[00:07:15.320 --> 00:07:18.800]   Blackstone, which did the same for legal text. And in a lot of
[00:07:18.800 --> 00:07:21.680]   these projects, a lot of work went into designing these
[00:07:21.680 --> 00:07:26.800]   individual NLP components very specific for the task that
[00:07:26.800 --> 00:07:30.680]   they're solving. And so that stuff, I think is very cool. And
[00:07:30.680 --> 00:07:32.680]   especially because these projects are also open source,
[00:07:32.680 --> 00:07:36.800]   so you can go and download them, use them, maybe adapt them from
[00:07:36.800 --> 00:07:38.680]   you for your own projects.
[00:07:38.680 --> 00:07:44.440]   Awesome. As I said earlier, this will be quite half hazard since
[00:07:44.440 --> 00:07:47.120]   we have questions on all different directions. I'm trying
[00:07:47.120 --> 00:07:51.400]   to organize them. It'll still be a bit random. But for the people
[00:07:51.400 --> 00:07:53.560]   joining in, please keep the questions coming. This is for
[00:07:53.560 --> 00:07:56.240]   you. This is not an interview that I'm getting to, so please
[00:07:56.240 --> 00:08:01.000]   keep the questions coming. Going back to your early days, you
[00:08:01.000 --> 00:08:03.880]   have a very pragmatic view of how you approach projects.
[00:08:03.880 --> 00:08:06.600]   You've talked about this in the Europython conference as well. I
[00:08:06.600 --> 00:08:08.320]   absolutely enjoyed that keynote.
[00:08:08.320 --> 00:08:09.120]   Thanks.
[00:08:09.120 --> 00:08:14.400]   So when you start with any project, possibly in the open
[00:08:14.400 --> 00:08:16.960]   source domain, or let's say when you get started, when you got
[00:08:16.960 --> 00:08:19.960]   started on Spacey, how did you improve from there as a coder
[00:08:19.960 --> 00:08:22.320]   yourself? How did you make that conscious effort?
[00:08:22.320 --> 00:08:25.920]   Yeah, I mean, for me, it's like, I'm not a person, I don't
[00:08:25.920 --> 00:08:29.000]   learn very well from like courses or books. So I really
[00:08:29.000 --> 00:08:31.760]   need to be actually doing things. And so for me, it's
[00:08:31.760 --> 00:08:34.680]   always helped to really have a project or something specific
[00:08:34.680 --> 00:08:37.600]   I'm trying to solve. And then I'm like, you know, when I get
[00:08:37.600 --> 00:08:41.320]   really excited about doing something, I get like, really
[00:08:41.320 --> 00:08:43.880]   into it. And I spend a lot of time doing it. And for me, that
[00:08:43.880 --> 00:08:47.880]   was kind of the best way to learn things and to get into,
[00:08:47.880 --> 00:08:50.400]   you know, NLP more generally, or when I started working on
[00:08:50.400 --> 00:08:55.160]   Spacey. Like, I don't know, I'm not, I don't typically sit down
[00:08:55.200 --> 00:08:59.000]   and just learn something like, okay, I need a project. And
[00:08:59.000 --> 00:09:01.600]   then that's also the best way for me to learn new things and
[00:09:01.600 --> 00:09:03.280]   try out new technologies even now.
[00:09:03.280 --> 00:09:07.960]   So Saurav extends this question further, like, how do you find
[00:09:07.960 --> 00:09:11.800]   the motivation for that without any external factor? So if
[00:09:11.800 --> 00:09:13.960]   you're working on a project, how do you motivate yourself?
[00:09:13.960 --> 00:09:17.680]   I mean, I do think it does come, you know, it does have to come
[00:09:17.680 --> 00:09:21.680]   from, I don't know, the inside. And, you know, I probably
[00:09:21.680 --> 00:09:25.000]   couldn't be doing that job if I was like, just not interested
[00:09:25.000 --> 00:09:28.040]   in computers or not interested in programming, I think then I
[00:09:28.040 --> 00:09:31.600]   would be, you know, in the wrong profession. So I do think I'm
[00:09:31.600 --> 00:09:34.320]   lucky that okay, this is something I enjoy. And I do tend
[00:09:34.320 --> 00:09:37.560]   to get quite excited about things if I find something that
[00:09:37.560 --> 00:09:41.960]   I want to do. And yeah, even I feel like even working remotely
[00:09:41.960 --> 00:09:45.480]   has really worked well for me. I never thought, you know, I would
[00:09:45.480 --> 00:09:48.000]   find it relatively easy to just, you know, be at home and
[00:09:48.000 --> 00:09:52.520]   motivate myself to do stuff. But yeah, I think it's always been a
[00:09:52.720 --> 00:09:56.960]   hobby of mine. So I kind of found a way to turn the things I
[00:09:56.960 --> 00:09:59.600]   like into a job. And that's how I stay motivated.
[00:09:59.600 --> 00:10:04.400]   Sanyam Bhutani: Was was what's the what was the origin story
[00:10:04.400 --> 00:10:07.560]   around Explosion? Was this through the consulting business
[00:10:07.560 --> 00:10:10.360]   that you started? Like you said, you follow your passions and
[00:10:10.360 --> 00:10:13.640]   someone is also curious about how did the name Explosion come
[00:10:13.640 --> 00:10:13.880]   about?
[00:10:13.880 --> 00:10:18.360]   Unknown: Okay, so, but so basically, the hope is the quick
[00:10:18.360 --> 00:10:23.520]   story is I, I was, I did have a background in linguistic, I have
[00:10:23.520 --> 00:10:27.960]   a background in development. But I am, you know, I initially I
[00:10:27.960 --> 00:10:31.360]   didn't go like, you know, the traditional computer science
[00:10:31.360 --> 00:10:35.520]   road, I ended up meeting my co founder, Matt, who was actually
[00:10:35.520 --> 00:10:37.880]   writing spacey at the time, and we realized that, oh, there's
[00:10:37.880 --> 00:10:41.000]   kind of a lot of stuff we have in common. And, you know, I had
[00:10:41.000 --> 00:10:43.320]   a background in this, and I was interested in it. And so we
[00:10:43.320 --> 00:10:47.520]   started working together on spacey. And then quite some time
[00:10:47.520 --> 00:10:50.040]   later, we were like, Oh, we should, you know, really start a
[00:10:50.040 --> 00:10:52.280]   company around this. And the plan of spacey has always been
[00:10:52.280 --> 00:10:55.240]   to turn this into a commercial business. So it wasn't like the
[00:10:55.240 --> 00:10:58.120]   case that it was an open source project that ended up being
[00:10:58.120 --> 00:11:01.000]   popular. And then someone was like, Oh, how can I make money
[00:11:01.000 --> 00:11:03.480]   from this? Like the idea was, hey, there's a lot of demand.
[00:11:03.480 --> 00:11:07.160]   People were interested even in my co founder, Matt's research
[00:11:07.160 --> 00:11:09.400]   cult when he was in academia. And he was like, now they should
[00:11:09.400 --> 00:11:12.400]   really for industry use cases, you need something better, you
[00:11:12.400 --> 00:11:14.400]   need something more performant, you need something that's really
[00:11:14.400 --> 00:11:18.080]   designed for real world use cases. And that's kind of how
[00:11:18.080 --> 00:11:22.520]   spacey was born. And so we started explosion shortly after
[00:11:22.520 --> 00:11:28.000]   that. And the idea was, well, we needed, you know, to bootstrap
[00:11:28.000 --> 00:11:33.280]   and fund the company. Back then, you know, a lot of the typical
[00:11:33.280 --> 00:11:35.520]   investment routes, it wasn't very attractive to us, it didn't
[00:11:35.520 --> 00:11:38.080]   seem you know, it wasn't very compatible with the sort of
[00:11:38.080 --> 00:11:41.120]   thing we were doing. We it was always very important to us that
[00:11:41.120 --> 00:11:43.960]   you know, it's the project, the open source project, and what
[00:11:43.960 --> 00:11:47.880]   we're doing was sustainable from day one, and we wanted to have a
[00:11:47.880 --> 00:11:49.440]   profitable company. And so
[00:11:49.440 --> 00:11:51.600]   Sanyam Bhutani: your KPI was profit, as you mentioned.
[00:11:51.600 --> 00:11:53.040]   Yeah, exactly.
[00:11:53.040 --> 00:11:54.720]   Dr. Rima Bhanu: And I think I think that's still true. I mean,
[00:11:54.720 --> 00:11:57.280]   that's still how we operate. And I think you know, money does not
[00:11:57.280 --> 00:11:59.840]   lie and profit doesn't lie. And you know, you can you can lie to
[00:11:59.840 --> 00:12:02.640]   yourself for a long time. But if you're actually making money,
[00:12:02.640 --> 00:12:05.920]   which I think a company should, then you know, that's the most
[00:12:05.920 --> 00:12:09.240]   straightforward way of looking at it. But yeah, so we did what
[00:12:09.240 --> 00:12:12.640]   we call, we race the client round. So we basically saw, hey,
[00:12:12.640 --> 00:12:16.200]   there was a lot of interest on companies, and we could just do
[00:12:16.200 --> 00:12:19.480]   some projects for them and also get to know the use cases better
[00:12:19.480 --> 00:12:23.360]   and see what people are working on, what people really need. And
[00:12:23.360 --> 00:12:25.440]   that's what we did with explosion for the first six
[00:12:25.440 --> 00:12:29.000]   months. And then we released our first commercial product
[00:12:29.000 --> 00:12:32.920]   prodigy. And we're funded off that. And actually, to yet to
[00:12:32.920 --> 00:12:35.440]   answer the explosion question, I think we were just like
[00:12:35.440 --> 00:12:39.480]   brainstorming words, and you know, terms that were sort of
[00:12:39.480 --> 00:12:42.760]   related, we wanted something that basically fit into the
[00:12:42.760 --> 00:12:48.360]   sort of space terminology that we already had for spacey and
[00:12:48.360 --> 00:12:53.480]   some of our other other tools, and the domain was still
[00:12:53.480 --> 00:12:58.000]   available. We thought AI, which was obviously a big factor. And
[00:12:58.000 --> 00:12:59.800]   so we settled on explosion.
[00:12:59.800 --> 00:13:05.920]   Sanyam Bhutani: Awesome. Yeah, I think it's also I, some founders
[00:13:05.920 --> 00:13:08.560]   say that they also searching for a name that's available. And
[00:13:08.560 --> 00:13:11.640]   these times a lot of names are usually taken maybe, were you
[00:13:11.640 --> 00:13:14.600]   also thinking like, you wanted to create an explosion in the AI
[00:13:14.600 --> 00:13:14.920]   world?
[00:13:14.920 --> 00:13:20.680]   Or even at the time, there was this term like the AI explosion.
[00:13:20.680 --> 00:13:23.560]   And I don't know, it had like, you know, it does have positive
[00:13:23.560 --> 00:13:26.560]   and negative connotations. But it's, you know, I think I do
[00:13:26.560 --> 00:13:30.720]   think it's a very dynamic term. And in general, actually, if you
[00:13:30.720 --> 00:13:33.800]   look at like the how we're calling our projects and tools
[00:13:33.800 --> 00:13:38.000]   and things, we always like to choose words that have like,
[00:13:38.000 --> 00:13:40.560]   you know, some sort of meaning, but everything's kind of, you
[00:13:40.560 --> 00:13:46.400]   know, mostly a word that exists, or some play on a word. Yeah.
[00:13:46.400 --> 00:13:50.280]   Sanyam Bhutani: Makes sense. Continuing on the previous
[00:13:50.280 --> 00:13:54.120]   question, how did you decide these lines of product? This is
[00:13:54.120 --> 00:13:57.280]   a question by Ayush and explosion went from spacey to
[00:13:57.280 --> 00:14:02.360]   prodigy, then think, what was the decision making factor
[00:14:02.360 --> 00:14:03.200]   behind this?
[00:14:03.200 --> 00:14:05.840]   Yeah, so it was actually, I would actually say, I think was
[00:14:05.840 --> 00:14:09.000]   kind of around before prodigy, because it was always kind of
[00:14:09.000 --> 00:14:11.760]   the machine learning library, powering spacey, we just made it
[00:14:11.760 --> 00:14:14.680]   a separate library, but it's kind of part of that ecosystem.
[00:14:14.680 --> 00:14:17.600]   But basically, for prodigy, even when we were doing the few
[00:14:17.600 --> 00:14:20.360]   client projects at the beginning of the company, we saw that
[00:14:20.360 --> 00:14:22.720]   training data was like the number one thing that was
[00:14:22.720 --> 00:14:26.160]   holding people back. A lot of people would just outsource their
[00:14:26.160 --> 00:14:30.200]   data collection to Mechanical Turk and then wonder why the
[00:14:30.200 --> 00:14:33.480]   data they got back was like not so usable and low quality. And
[00:14:33.480 --> 00:14:36.720]   people really didn't spend enough time engaging with their
[00:14:36.720 --> 00:14:41.000]   data and breaking down their problem. And we also, we knew we
[00:14:41.000 --> 00:14:44.440]   wanted an annotation tool, we wanted to create data for the
[00:14:44.440 --> 00:14:48.080]   things we were working on. So yeah, an annotation tool was very
[00:14:48.080 --> 00:14:51.080]   high on our list. And so we focused on that we built the
[00:14:51.080 --> 00:14:53.760]   type of tool that we would have wanted to use that was
[00:14:53.760 --> 00:14:59.240]   programmable. You know, and that also kind of reimagined some of
[00:14:59.240 --> 00:15:01.880]   the general annotation workflows to make them efficient. So data
[00:15:01.880 --> 00:15:05.800]   scientists could start by creating some data themselves
[00:15:05.800 --> 00:15:09.120]   before scaling things up. And that's, that's how prodigy was
[00:15:09.120 --> 00:15:12.520]   born. So we really saw, yeah, annotation was like incredibly
[00:15:12.520 --> 00:15:14.960]   important for what we were doing. It was a very natural
[00:15:14.960 --> 00:15:17.840]   follow on from spacey. Because say, if you're also as a
[00:15:17.840 --> 00:15:20.120]   commercial product, if you're a power user of spacey, if you're
[00:15:20.120 --> 00:15:23.480]   power user of machine learning, more generally, you almost
[00:15:23.480 --> 00:15:26.640]   always want to train your own models. And to train your own
[00:15:26.640 --> 00:15:29.680]   models, you need data, and then you need some way to create
[00:15:29.680 --> 00:15:32.640]   data. So prodigy is kind of, yeah, it was kind of a natural
[00:15:32.640 --> 00:15:33.400]   first product.
[00:15:33.400 --> 00:15:36.760]   Sanyam Bhutani: Well, this was also right around the time when
[00:15:36.760 --> 00:15:38.960]   Mechanical Turk was taking off, right?
[00:15:38.960 --> 00:15:41.760]   Dr. Anna Waters: Yeah, I mean, I think Mechanical Turk's been
[00:15:41.760 --> 00:15:44.880]   around for I think, much longer. And also, it's still used in
[00:15:44.880 --> 00:15:48.600]   like academia, where people have other, you know, budget
[00:15:48.600 --> 00:15:51.200]   constraints, or like other ways of creating the data. But we I
[00:15:51.200 --> 00:15:53.720]   do believe that like, this kind of idea of outsourcing
[00:15:53.720 --> 00:15:56.440]   annotation is something we will look back on the way we look
[00:15:56.440 --> 00:16:00.440]   back at on outsourcing programming, like you can't,
[00:16:00.440 --> 00:16:04.600]   data creation is part of the development process, data needs
[00:16:04.600 --> 00:16:08.200]   to be developed, just like your code needs to be developed. And
[00:16:08.200 --> 00:16:12.800]   people I do think also people often underestimate the work
[00:16:12.800 --> 00:16:15.240]   that goes into designing your annotation schemes and design
[00:16:15.240 --> 00:16:17.880]   designing what your model should predict, which is also something
[00:16:17.880 --> 00:16:19.960]   that's quite different from academia, if you're researching,
[00:16:19.960 --> 00:16:23.120]   you usually start off with a predefined data set. And you're
[00:16:23.120 --> 00:16:26.320]   also trying to build data sets that are quite hard, so that you
[00:16:26.320 --> 00:16:30.080]   can compare your algorithms on them. In industry use cases, you
[00:16:30.080 --> 00:16:32.120]   don't have that problem, you can actually make your problem
[00:16:32.120 --> 00:16:35.040]   easier. And that's usually the way forward, you think of
[00:16:35.040 --> 00:16:37.280]   something you want to solve. And then you think about how, how do
[00:16:37.280 --> 00:16:39.600]   I break this down into machine learning components? And how do
[00:16:39.600 --> 00:16:41.960]   I break this down into components that predict
[00:16:41.960 --> 00:16:44.400]   something that's as straightforward as possible, and
[00:16:44.400 --> 00:16:47.800]   something that I can annotate without too much ambiguity? And
[00:16:47.800 --> 00:16:50.840]   how do I, and even if there is ambiguity, how do I deal with
[00:16:50.840 --> 00:16:53.960]   it, it needs to be consistent. Otherwise, there's no way a
[00:16:53.960 --> 00:16:58.240]   computer can learn this. And these are all decisions that,
[00:16:58.240 --> 00:17:04.120]   yeah, you have to make when you're building a model. And
[00:17:04.120 --> 00:17:09.560]   that's also what, you know, what Prodigy tries to emphasize. And
[00:17:09.560 --> 00:17:13.720]   yeah, so I think, even you know, when you when you're reading
[00:17:13.720 --> 00:17:15.920]   about people's annotation processes, or this Guardian
[00:17:15.920 --> 00:17:18.840]   use case that you mentioned earlier, that was also that was
[00:17:18.840 --> 00:17:21.320]   a cool article. And they really spent a lot of time explaining
[00:17:21.320 --> 00:17:23.680]   how yeah, they had journalists and they had a task that
[00:17:23.680 --> 00:17:27.520]   sounded pretty straightforward, like annotate quotes in text.
[00:17:27.520 --> 00:17:29.920]   Most people would think, Oh, yeah, I know what a quote is
[00:17:29.920 --> 00:17:32.840]   that easy to annotate. But once you actually start working on
[00:17:32.840 --> 00:17:35.240]   it, you find all these edge cases, because language is
[00:17:35.240 --> 00:17:38.960]   complex language is not ambiguous. That's the whole
[00:17:38.960 --> 00:17:41.120]   point. And that's the whole difficulty. And so then you
[00:17:41.120 --> 00:17:43.840]   start annotating this. So you're having all these discussions
[00:17:43.840 --> 00:17:45.840]   from a different annotation project that I think my co
[00:17:45.840 --> 00:17:48.560]   founder was involved in, they ended up spending hours
[00:17:48.560 --> 00:17:52.680]   discussing whether the forehead is part of the face. And you
[00:17:52.680 --> 00:17:55.440]   know, you can discuss that in a group of people. And maybe, you
[00:17:55.440 --> 00:17:58.360]   know, some people agree, some people disagree. And this, you
[00:17:58.360 --> 00:18:00.320]   know, you need to come up with an answer. And that's why it's
[00:18:00.320 --> 00:18:04.960]   so important to really spend some time with your data. And
[00:18:04.960 --> 00:18:07.600]   the other nice thing is nowadays, back in the day,
[00:18:07.600 --> 00:18:10.400]   people still have this like big, big data idea where you needed
[00:18:10.400 --> 00:18:14.040]   millions of examples to even get anything done. And now we have
[00:18:14.040 --> 00:18:17.800]   transfer learning, we don't actually need massive data sets,
[00:18:17.800 --> 00:18:21.360]   we need good data sets. And you can actually train pretty good
[00:18:21.360 --> 00:18:24.680]   models with like a few 100 annotations. So it's, that's not
[00:18:24.680 --> 00:18:27.040]   the problem anymore, you don't need to like scale it up to
[00:18:27.040 --> 00:18:30.000]   infinity. But you need good workflows, and you need
[00:18:30.000 --> 00:18:32.640]   iterative workflows. And that's more important than just the
[00:18:32.640 --> 00:18:36.800]   sheer volume of clicks that you and that's also why these
[00:18:36.800 --> 00:18:40.080]   outsourced click worker solutions are becoming less
[00:18:40.080 --> 00:18:43.320]   interesting for or less relevant for industry use cases, because
[00:18:43.320 --> 00:18:47.680]   it's not about the cheap click work, it's about the quality of
[00:18:48.040 --> 00:18:51.640]   and how well it fits into your machine learning workflow,
[00:18:51.640 --> 00:18:52.200]   basically.
[00:18:52.200 --> 00:18:57.120]   Yeah, and the reason I brought up the mechanical point, because
[00:18:57.120 --> 00:19:00.280]   from what I was trying to understand, historically, at the
[00:19:00.280 --> 00:19:02.320]   time, there weren't many options. And I was trying to
[00:19:02.320 --> 00:19:04.920]   understand if that was one of the reasons you see that you saw
[00:19:04.920 --> 00:19:08.880]   this clear, missing block in the community. And maybe that was
[00:19:08.880 --> 00:19:11.040]   one of the core reasons for creating prodigy.
[00:19:11.040 --> 00:19:14.600]   Yeah, I think definitely. Or we definitely saw that outsourcing
[00:19:14.600 --> 00:19:17.920]   annotation and outsourcing data creation data is the core
[00:19:17.920 --> 00:19:21.680]   of your model, that outsourcing that was not a really viable way
[00:19:21.680 --> 00:19:27.520]   for industry going forward. Just like outsourcing programming is,
[00:19:27.520 --> 00:19:31.480]   I think most companies now understand is not the right way
[00:19:31.480 --> 00:19:34.000]   forward. Most companies hire development teams, and it's very
[00:19:34.000 --> 00:19:37.680]   important to have your a lot of your development in house. And I
[00:19:37.680 --> 00:19:39.840]   think the same goes for data. And there are also other
[00:19:39.840 --> 00:19:41.960]   concerns, a lot of companies, people working on in the
[00:19:41.960 --> 00:19:44.720]   healthcare domain that you don't want to send your data to other
[00:19:44.720 --> 00:19:48.040]   people's service, you want to work on your own infrastructure,
[00:19:48.040 --> 00:19:51.120]   you want to have, you know, a tool that doesn't, that isn't
[00:19:51.120 --> 00:19:56.640]   like invasive, and, you know, collects all your data, or, you
[00:19:56.640 --> 00:19:59.440]   know, sends all your stuff to other people. So that's another
[00:19:59.440 --> 00:20:02.080]   consideration where, you know, it's important for us to have
[00:20:02.080 --> 00:20:06.280]   tooling that people can run on their own infrastructure. Yeah.
[00:20:06.280 --> 00:20:11.000]   And earlier for the people who joined us, I was just sharing
[00:20:11.000 --> 00:20:14.680]   the live demo on the website. And I was just showing them how
[00:20:14.680 --> 00:20:19.000]   innovative it is to actually go through this. And it, in
[00:20:19.000 --> 00:20:22.280]   hindsight, I guess it was also one of the reasons that since
[00:20:22.280 --> 00:20:25.160]   you bootstrapped the business, it was so easy for you to create
[00:20:25.160 --> 00:20:28.800]   this model where you sell a one time license. Otherwise, it's
[00:20:28.800 --> 00:20:33.040]   so hard to actually see this across many industries where you
[00:20:33.040 --> 00:20:35.760]   just sell off the license and the developers can work on it
[00:20:35.760 --> 00:20:39.360]   without using a centralized server or which is common across
[00:20:39.360 --> 00:20:40.280]   many services.
[00:20:40.280 --> 00:20:42.600]   Yeah, yeah. And I think it's actually it is something that's,
[00:20:42.640 --> 00:20:44.920]   you know, speaks to developers, like it's kind of this old
[00:20:44.920 --> 00:20:46.680]   school model where like back in the day, you could buy
[00:20:46.680 --> 00:20:50.840]   Photoshop, and then you'd have it. And, and I think even less
[00:20:50.840 --> 00:20:54.000]   about the licensing, it's more about the kind of open source
[00:20:54.000 --> 00:20:57.800]   feeling of the of a tool or like the programmability, because I
[00:20:57.800 --> 00:21:00.360]   think again, also, when talking about open source, a lot of
[00:21:00.360 --> 00:21:03.000]   people have this misconception that companies like open source
[00:21:03.000 --> 00:21:05.600]   because it's free. I don't think that's necessarily the case
[00:21:05.600 --> 00:21:08.000]   companies and people like open source, because it's
[00:21:08.000 --> 00:21:11.000]   programmable, you can install it, it's very easily accessible,
[00:21:11.000 --> 00:21:15.440]   you pip install a package, you clone something from GitHub, and
[00:21:15.440 --> 00:21:18.960]   you want to extend something and you can write some code to make
[00:21:18.960 --> 00:21:22.440]   it do what you want. And I think that is one of the main appeals
[00:21:22.440 --> 00:21:27.160]   of open source. And that's also what we are trying to
[00:21:27.160 --> 00:21:30.320]   accomplish in other tools, even if they're not free. I don't
[00:21:30.320 --> 00:21:32.720]   think that's like, that's the main motivating factor. It's
[00:21:32.720 --> 00:21:35.360]   that they're programmable, they're, you know,
[00:21:35.360 --> 00:21:38.200]   straightforward, they integrate into your development workflow.
[00:21:38.480 --> 00:21:41.640]   And that's, I think what people want. And that's also why we
[00:21:41.640 --> 00:21:43.440]   built initially built product in this way.
[00:21:43.440 --> 00:21:47.960]   I mean, I wasn't a part of those decisions. But without getting
[00:21:47.960 --> 00:21:50.840]   too much product team, that's also one philosophy, I believe
[00:21:50.840 --> 00:21:55.040]   weights and biases closely follows for the product. One
[00:21:55.040 --> 00:21:57.480]   thing I'm really curious, and I've also asked you earlier, but
[00:21:57.480 --> 00:21:59.920]   I'll ask again, since the audience is also curious, as I
[00:21:59.920 --> 00:22:02.560]   was sharing the prodigy screen, and this is one thing we've seen
[00:22:02.560 --> 00:22:06.680]   across your slide decks, your videos, where does that strong
[00:22:06.680 --> 00:22:10.840]   sense of aesthetics come for you? You're you're even your
[00:22:10.840 --> 00:22:12.760]   software is like artwork. So
[00:22:12.760 --> 00:22:19.680]   I think it's just something I've always enjoyed. And you know,
[00:22:19.680 --> 00:22:21.760]   yeah, as you mentioned earlier, I got into programming for
[00:22:21.760 --> 00:22:24.960]   making websites. And, you know, this is it's just something I've
[00:22:24.960 --> 00:22:27.480]   always liked. And I do think it was also something when we
[00:22:27.480 --> 00:22:30.000]   started out, there was a bit missing. And where I felt like,
[00:22:30.000 --> 00:22:34.160]   Oh, cool, there's actually a lot to innovate on the UI user
[00:22:34.160 --> 00:22:39.760]   experience more generally, and, you know, the polish and, you
[00:22:39.760 --> 00:22:42.160]   know, usability of stuff. And it's always something that
[00:22:42.160 --> 00:22:45.040]   interested me. And I was like, Oh, cool, there's actually a
[00:22:45.040 --> 00:22:49.520]   lot of stuff to do. And, you know, the web is very powerful,
[00:22:49.520 --> 00:22:52.040]   the browser is very powerful. There are a lot of things, you
[00:22:52.040 --> 00:22:54.320]   know, we can do to make it easier for people to use
[00:22:54.320 --> 00:22:58.560]   software. And yeah, I think that's definitely Yeah, it's
[00:22:58.560 --> 00:23:02.240]   definitely been one of the main things we've been focusing on.
[00:23:02.240 --> 00:23:04.320]   And also, actually, one of the things I'm definitely I'm
[00:23:04.320 --> 00:23:08.240]   definitely proud of, that we've been able to come up with kind
[00:23:08.240 --> 00:23:10.800]   of a good level of usability that people like, and it
[00:23:10.800 --> 00:23:11.920]   actually makes a difference.
[00:23:11.920 --> 00:23:17.800]   Sanyam Bhutani: It does, like, does extra effort go into that?
[00:23:17.800 --> 00:23:21.960]   Because we see many products that are, I'm just speaking as a
[00:23:21.960 --> 00:23:24.720]   user, we see many products that are great, but don't look so
[00:23:24.720 --> 00:23:27.600]   good. So is that like a super conscious effort, where you like
[00:23:27.600 --> 00:23:31.160]   to spend extra time, extra efforts to make it better
[00:23:31.160 --> 00:23:31.680]   looking?
[00:23:31.680 --> 00:23:35.480]   I mean, sure, like, it's, you know, it's also I think it's
[00:23:35.480 --> 00:23:38.120]   kind of it starts earlier that it's kind of core of the
[00:23:38.120 --> 00:23:40.480]   development process. Like, you know, we don't build things by
[00:23:40.480 --> 00:23:43.120]   like, okay, building like a really hacky version, and then
[00:23:43.120 --> 00:23:45.360]   rewriting it all to make it nicer. Like, I do think it's
[00:23:45.360 --> 00:23:48.080]   just part of the development workflow that we have a certain
[00:23:48.080 --> 00:23:51.680]   standard of how we want to want things to look and work. And
[00:23:51.680 --> 00:23:54.240]   also, we always expect that we don't we're not just
[00:23:54.240 --> 00:23:58.480]   implementing what people want, we do always need to innovate or
[00:23:58.480 --> 00:24:02.360]   think about like, okay, what, what, what do people actually
[00:24:02.360 --> 00:24:06.000]   need? And how can we really how can we make this better? Because
[00:24:06.000 --> 00:24:09.480]   often, even as a user, I might be like, oh, this, this tool
[00:24:09.480 --> 00:24:12.920]   could be better, or I it doesn't quite do what I want, or I want
[00:24:12.920 --> 00:24:17.200]   a tool that does x, but then, you know, I, then maybe some
[00:24:17.200 --> 00:24:19.600]   tool comes along that does something completely differently.
[00:24:19.600 --> 00:24:21.680]   And I'm like, Oh, this is great. And it really, you know, really
[00:24:21.680 --> 00:24:24.120]   takes this extra effort. And you can't just basically, you can't
[00:24:24.120 --> 00:24:26.840]   just go around and ask users, what interface do you want? And
[00:24:26.840 --> 00:24:28.680]   then a user tells you what interface they want, and you
[00:24:28.680 --> 00:24:31.520]   build it, I don't think that's how you build good products, you
[00:24:31.520 --> 00:24:33.600]   need to, it takes a bit more work, you need to actually
[00:24:33.600 --> 00:24:37.360]   understand what the user really wants, and what the user needs.
[00:24:37.360 --> 00:24:42.440]   And, you know, what the best type best abstractions that are
[00:24:42.440 --> 00:24:47.400]   actually useful for a very broad audience of users. And that's,
[00:24:47.400 --> 00:24:50.600]   that's, that definitely takes take some extra effort. But, you
[00:24:50.600 --> 00:24:52.720]   know, I view that as like, just part of what we do.
[00:24:52.720 --> 00:24:56.000]   Sanyam Bhutani: That again, boils back to your pragmatic
[00:24:56.000 --> 00:25:00.080]   mindset. And you've also shared this, again, in the keynote, I
[00:25:00.080 --> 00:25:02.760]   was at a few times, that's I remember it so well, you shared
[00:25:02.760 --> 00:25:07.240]   where each each team member is like, either complimentary to
[00:25:07.240 --> 00:25:09.880]   each other, or they like a generalist where they all have
[00:25:09.880 --> 00:25:13.040]   the similar mindset and complimentary skill sets. So I
[00:25:13.040 --> 00:25:15.320]   think that also goes to attribution to that.
[00:25:15.320 --> 00:25:18.280]   Dr. Anna Budge: Yeah, I think also, that's something we can do
[00:25:18.280 --> 00:25:21.880]   as a relatively, you know, with a relatively small team, you
[00:25:21.880 --> 00:25:24.520]   know, you can really take advantage of people's, you know,
[00:25:24.520 --> 00:25:27.680]   individual skill sets. And also, you know, maybe skill sets
[00:25:27.680 --> 00:25:30.520]   that don't fit into like the vague, classic job description
[00:25:30.520 --> 00:25:33.520]   of a machine learning engineer, because that's also not me
[00:25:33.520 --> 00:25:36.880]   either. Like, I'm, I have a lot of different skills that, you
[00:25:36.880 --> 00:25:39.680]   know, maybe won't fit any standard job description, but
[00:25:39.680 --> 00:25:43.520]   like, I can make it work. And I think so as a company, we can
[00:25:43.520 --> 00:25:48.600]   also make that work and use people skills in that way. Yeah.
[00:25:48.600 --> 00:25:51.680]   Sanyam Bhutani: I also loved how you responded someone from the
[00:25:51.680 --> 00:25:55.160]   audience without like, making them look bad. They asked, what
[00:25:55.160 --> 00:25:58.280]   are your thoughts about hiring a junior developer? And you said
[00:25:58.280 --> 00:26:00.240]   there's there's no such definition, because everyone
[00:26:00.240 --> 00:26:02.920]   comes with different experiences from life and you have to value
[00:26:02.920 --> 00:26:05.360]   those with your company's values.
[00:26:05.360 --> 00:26:07.520]   Dr. Anna Budge: No, that's that's true. And I think even
[00:26:07.520 --> 00:26:09.520]   now, yes, we've, you know, we hire people at different
[00:26:09.520 --> 00:26:12.360]   experience levels. But I also I do think, you know, yes, you
[00:26:12.360 --> 00:26:14.840]   can, you can hire people with less experience, but then also
[00:26:14.840 --> 00:26:17.960]   as a company, you should be committed to teaching them
[00:26:17.960 --> 00:26:20.720]   things. Like, you know, you can't just hire like cheap
[00:26:20.720 --> 00:26:23.320]   programmers and exploit them because they have no experience
[00:26:23.320 --> 00:26:29.160]   and are cheap. Like that's also not great. But yeah, no, it's
[00:26:29.160 --> 00:26:29.360]   true.
[00:26:29.360 --> 00:26:33.360]   Sanyam Bhutani: Okay. Now, this is a shift from topics. This is
[00:26:33.360 --> 00:26:37.320]   more on the leadership side. So I've organized the questions on
[00:26:37.320 --> 00:26:42.000]   to there. The first question is by Thomas, I believe, what
[00:26:42.000 --> 00:26:44.640]   personal sacrifices have you made throughout your career?
[00:26:44.640 --> 00:26:46.480]   Maybe as a leader as well?
[00:26:47.280 --> 00:26:48.920]   Dr. Anna Budge: Yeah, I mean, this is an interesting
[00:26:48.920 --> 00:26:54.840]   question. I mean, the thing is, like, I want to, like, prefix
[00:26:54.840 --> 00:26:58.840]   this by saying that, okay, I'm also I am very privileged in the
[00:26:58.840 --> 00:27:01.440]   position that I'm in, like, I'm not going to sit here and talk
[00:27:01.440 --> 00:27:04.760]   about how like, oh, look, my life is so hard, because I work
[00:27:04.760 --> 00:27:08.120]   all the time. And I do X, because I'm also incredibly
[00:27:08.120 --> 00:27:10.760]   privileged that I can, you know, run the company this way. And
[00:27:10.760 --> 00:27:13.600]   we're doing something that makes an impact that happened to or we
[00:27:13.600 --> 00:27:16.520]   all probably most of us being here happen to work in an
[00:27:16.520 --> 00:27:21.640]   industry where our skills are very valuable. And that is, you
[00:27:21.640 --> 00:27:24.200]   know, that's a privilege. But like, sure, it's, there are
[00:27:24.200 --> 00:27:26.920]   pros and cons of being self employed and running your own
[00:27:26.920 --> 00:27:30.520]   company. One of them is one of the cons is clearly that, well,
[00:27:30.520 --> 00:27:35.160]   you know, you're kind of always working, you're always sort of,
[00:27:35.160 --> 00:27:37.560]   you know, engaged in things in a good way, because it's something
[00:27:37.560 --> 00:27:40.760]   I enjoy. But it's also, you know, it does take over your
[00:27:40.760 --> 00:27:44.400]   life to some extent. And I bet, which, but on the other hand, I
[00:27:44.400 --> 00:27:47.040]   benefit from this a lot, because it's my business, and it's my
[00:27:47.040 --> 00:27:51.120]   company. But it can be tough that also things become very
[00:27:51.120 --> 00:27:54.040]   personal, like everything, everything is very personal. I
[00:27:54.040 --> 00:27:56.720]   can't like, you know, turn off my computer and not give a shit
[00:27:56.720 --> 00:28:00.680]   about the company and then show up to work again. You know, the
[00:28:00.680 --> 00:28:04.000]   next Monday, like that is, that is a difference from, you know,
[00:28:04.000 --> 00:28:11.080]   a lot of other models. But But yeah, I don't I, you know, I
[00:28:11.080 --> 00:28:15.240]   wouldn't I wouldn't say I've made like, a lot of sacrifices
[00:28:15.240 --> 00:28:18.200]   in that sense, also, because I do enjoy my work, or even when,
[00:28:18.200 --> 00:28:20.720]   you know, maybe my friends will be like, ah, you're sitting on
[00:28:20.720 --> 00:28:23.280]   your computer all day, and you sit on your computer all weekend.
[00:28:23.280 --> 00:28:26.520]   And I'm like, yeah, I call this recreational programming. It's
[00:28:26.520 --> 00:28:30.400]   like what I do for fun. It's like, I don't know, I was
[00:28:30.400 --> 00:28:33.160]   talking, I was talking to my personal trainer once, and he
[00:28:33.160 --> 00:28:35.800]   was all like, yeah, I can't understand how you just sit in
[00:28:35.800 --> 00:28:38.280]   front of your computer all day. But then we realized, he's like,
[00:28:38.280 --> 00:28:40.440]   Oh, like, what do you do on a weekend? He's like, well, I go
[00:28:40.440 --> 00:28:44.840]   for a run. I, you know, go for a hike, I go swimming. And I'm
[00:28:44.840 --> 00:28:47.480]   like, yes, it's like, it's kind of, it's a similar thing. It's
[00:28:47.480 --> 00:28:53.480]   just what I enjoy. And it's also my job. I arguably, it's less
[00:28:53.480 --> 00:28:56.880]   healthy than like, I don't know, going for a run and swimming and
[00:28:56.880 --> 00:28:59.080]   spending all your weekend being active. But still,
[00:28:59.080 --> 00:29:02.840]   Sanyam Bhutani: it's more fulfilling, though, for I'm
[00:29:02.840 --> 00:29:05.920]   guessing for you. And I feel the same to some extent, it's more
[00:29:05.920 --> 00:29:11.400]   fulfilling in a way. Yeah, I'll definitely use the term
[00:29:11.400 --> 00:29:14.760]   recreational programming with my co-workers whenever they try to
[00:29:14.760 --> 00:29:20.760]   complain next time. Moving on to the list, what's what's the
[00:29:20.760 --> 00:29:23.920]   leadership lesson that you've learned that's unique to being a
[00:29:23.920 --> 00:29:24.720]   female leader?
[00:29:24.720 --> 00:29:29.640]   I mean, like, also, I will prefix this with like, I don't
[00:29:29.640 --> 00:29:31.920]   know who asked this question. And this is not a personal
[00:29:31.920 --> 00:29:35.440]   comment towards that person. But I do think one of the main
[00:29:35.440 --> 00:29:38.040]   lessons is that I'm being asked about what it's like to be a
[00:29:38.040 --> 00:29:41.880]   female founder when this shouldn't happen. Or like, it's,
[00:29:41.880 --> 00:29:44.200]   you know, it's, you know, it shouldn't be a question that
[00:29:44.200 --> 00:29:46.760]   people people ask, I know, it's still something people ask. I
[00:29:46.760 --> 00:29:49.520]   know, it's still something people focus on. I know, it's,
[00:29:49.520 --> 00:29:52.000]   you know, there are lots of contexts where this comes up.
[00:29:52.000 --> 00:29:55.560]   But yeah, I think the main lesson is like, oh, just because
[00:29:55.560 --> 00:29:58.600]   I'm female, suddenly, everything's about me being a
[00:29:58.600 --> 00:30:01.200]   female founder instead of just a founder.
[00:30:01.200 --> 00:30:11.160]   So sorry about that. I think this should have clarified.
[00:30:11.160 --> 00:30:14.760]   No, I mean, no, no. So I mean, also, like, just to be fair,
[00:30:14.760 --> 00:30:17.680]   like, no, you know, no, I totally understand. You know,
[00:30:17.680 --> 00:30:19.880]   you those questions have been great. They have been like,
[00:30:19.880 --> 00:30:23.320]   thoughtful. I'm not saying you know, this wasn't like a diss.
[00:30:27.240 --> 00:30:28.840]   I agree with you.
[00:30:28.840 --> 00:30:34.040]   There was another question about COVID. I mean, actually, the
[00:30:34.040 --> 00:30:38.040]   COVID answer is quite simple, because actually, we did not.
[00:30:38.040 --> 00:30:42.360]   This also leads into like the remote work stuff. We actually
[00:30:42.360 --> 00:30:44.840]   didn't have to do that very much, which again, is also we
[00:30:44.840 --> 00:30:47.160]   were in a very privileged and lucky situation. But we've
[00:30:47.160 --> 00:30:51.000]   always been remote. We've never really had an office, like at
[00:30:51.000 --> 00:30:54.800]   least a fixed office. We have people all over the world. So
[00:30:55.600 --> 00:31:00.200]   it was kind of, yeah, as you know, bad COVID was in a lot of
[00:31:00.200 --> 00:31:02.680]   other ways. And also on, you know, personal level for a lot
[00:31:02.680 --> 00:31:07.040]   of people, in terms of work, it was pretty straightforward. Or I
[00:31:07.040 --> 00:31:09.120]   felt like, oh, yeah, I've been, you know, I've been training my
[00:31:09.120 --> 00:31:11.680]   whole life for this, just being at home in front of my computer.
[00:31:11.680 --> 00:31:13.840]   That's what I've been doing before. And that's what I'm
[00:31:13.840 --> 00:31:16.080]   doing now. Yeah.
[00:31:16.080 --> 00:31:21.320]   Ardil, if I remember correctly, the team has grown a lot since
[00:31:21.320 --> 00:31:25.120]   the last time I got to interview you. So was remote
[00:31:25.120 --> 00:31:29.400]   first always a mindset for explosion? And what do you
[00:31:29.400 --> 00:31:33.200]   usually look at or look for when you're trying to grow the team
[00:31:33.200 --> 00:31:34.360]   or hiring members?
[00:31:34.360 --> 00:31:37.920]   Yeah, so definitely, I think being a remote team was
[00:31:37.920 --> 00:31:40.640]   definitely, you know, quite conscious thing. It's also kind
[00:31:40.640 --> 00:31:42.920]   of how how things develop, you know, you'll be meet a lot of
[00:31:42.920 --> 00:31:46.200]   good people, we kind of don't want to, you know, we don't want
[00:31:46.200 --> 00:31:49.280]   to be so invasive to people's lives by like telling them where
[00:31:49.280 --> 00:31:51.440]   they should where they need to live for their job. I think it's
[00:31:51.440 --> 00:31:54.000]   actually great if people can be wherever they want to be and be
[00:31:54.000 --> 00:31:57.880]   from, you know, be in whichever country they happen to live in
[00:31:57.880 --> 00:32:01.640]   and do a job if they're good. You know, I don't want to limit
[00:32:01.640 --> 00:32:06.240]   the choice of people to people who want to move to our city or
[00:32:06.240 --> 00:32:09.440]   one of the few cities that we've picked. So that was definitely,
[00:32:09.440 --> 00:32:14.160]   yeah, a factor we wanted to hire good people. We've actually, you
[00:32:14.160 --> 00:32:17.240]   know, we've had a list, we have kind of a list of people that
[00:32:17.240 --> 00:32:19.440]   we've worked with that we're also in this very fortunate
[00:32:19.440 --> 00:32:22.480]   position every day as a community. You know, we didn't
[00:32:22.480 --> 00:32:26.280]   have to put out a lot of hiring calls, we were, we were just
[00:32:26.280 --> 00:32:29.120]   able when we're in a position to hire more people, we were able
[00:32:29.120 --> 00:32:32.200]   to just reach out to people we knew and be like, hey, do you
[00:32:32.200 --> 00:32:35.200]   want to work with us? And a lot of people were like, Oh, cool,
[00:32:35.200 --> 00:32:38.400]   this is awesome. And we had a lot of people, we were very
[00:32:38.400 --> 00:32:41.040]   confident in hiring, because we'd seen their work. They've
[00:32:41.040 --> 00:32:44.600]   been active members of the community, they built stuff that
[00:32:44.600 --> 00:32:48.960]   was public, and we were like, oh, cool. And so that it did
[00:32:48.960 --> 00:32:52.680]   feel this this round of hiring felt very, very natural to us.
[00:32:52.680 --> 00:32:57.160]   Sanyam Bhutani: Any, any tips for someone who's trying to
[00:32:57.160 --> 00:33:00.280]   follow a similar path we try to, where they're trying to create a
[00:33:00.280 --> 00:33:03.640]   portfolio that gets your attention, for example, in the
[00:33:03.640 --> 00:33:05.200]   future, what should they do?
[00:33:05.200 --> 00:33:07.360]   Rebecca Henderson: I mean, that's, that's also an
[00:33:07.360 --> 00:33:10.280]   interesting question. Because, I mean, it's a bit tricky,
[00:33:10.280 --> 00:33:12.400]   because on the one hand, you know, I don't want to go out
[00:33:12.400 --> 00:33:14.720]   here and tell people, oh, you should just spend all of this
[00:33:14.720 --> 00:33:17.360]   time working for free, open source and publishing your
[00:33:17.360 --> 00:33:23.200]   things in order to get noticed. Because, again, this is also,
[00:33:23.200 --> 00:33:26.560]   you know, a privilege to have that you have all this time to
[00:33:26.560 --> 00:33:30.960]   do unpaid work to get, you know, to get a profile online. But I
[00:33:30.960 --> 00:33:33.120]   do think there is definitely, you know, there are a lot of
[00:33:33.120 --> 00:33:37.040]   opportunities out there to build things built, you know,
[00:33:37.040 --> 00:33:42.320]   extensions, plugins, you know, little open source projects, and
[00:33:42.320 --> 00:33:46.720]   that's certainly something we would look at. And, you know,
[00:33:46.760 --> 00:33:50.400]   kind of how people write code, what you know, it doesn't even
[00:33:50.400 --> 00:33:54.640]   have to be like, you know, fancy projects and fancy papers. But
[00:33:54.640 --> 00:33:58.440]   just, you know, that, you know, some some level of creativity,
[00:33:58.440 --> 00:34:02.520]   some level of problem solving. And I do think yeah, open
[00:34:02.520 --> 00:34:05.840]   source, or just the ability to publish things open source does
[00:34:05.840 --> 00:34:09.320]   make it a lot easier to have something to show. And for
[00:34:09.320 --> 00:34:13.040]   example, we haven't, we don't typically do coding interviews,
[00:34:13.040 --> 00:34:16.280]   because yeah, we think, you know, I don't know if we could,
[00:34:16.280 --> 00:34:19.360]   you know, we've never really done whiteboard interviews
[00:34:19.360 --> 00:34:21.680]   ourselves, I don't think it's very useful, especially, you
[00:34:21.680 --> 00:34:24.080]   know, talking about having people with these complementary
[00:34:24.080 --> 00:34:26.680]   skills, having people with maybe slightly different skills,
[00:34:26.680 --> 00:34:29.760]   coding, there's nothing we would learn from a whiteboard
[00:34:29.760 --> 00:34:32.960]   interview, about that person, it's something you can do, if
[00:34:32.960 --> 00:34:35.680]   you Google where you need a huge pool of people who are very
[00:34:35.680 --> 00:34:38.320]   similar, and they, you know, you want to meet a certain standard.
[00:34:38.320 --> 00:34:40.960]   But if you're a smaller company, it doesn't actually tell you very
[00:34:40.960 --> 00:34:44.720]   much. So what we usually do is we work, we do a project with
[00:34:44.720 --> 00:34:47.160]   someone, we kind of work together, we see what it's like
[00:34:47.160 --> 00:34:50.560]   working together. And then it's a much better, you know, point
[00:34:50.560 --> 00:34:53.720]   to decide that, hey, let's, let's keep working together.
[00:34:53.720 --> 00:34:56.840]   Sanyam Bhutani: That sounds like a much more thoughtful approach
[00:34:56.840 --> 00:35:00.400]   as well. Google, I was reading, or any company at that scale,
[00:35:00.400 --> 00:35:03.520]   for that matter, like needs to hire at least 1000s of engineers
[00:35:03.520 --> 00:35:06.080]   just to keep the headcount. And for that, they have this
[00:35:06.080 --> 00:35:10.680]   standard approach. And if I may, for like a smaller company,
[00:35:10.680 --> 00:35:13.040]   like exclusion, I'm sorry, I'm speaking on your behalf. It's a
[00:35:13.040 --> 00:35:15.880]   smaller team, we need more ownership. So it's, it's a
[00:35:15.880 --> 00:35:17.560]   totally different mindset, if I may.
[00:35:17.560 --> 00:35:20.000]   Yeah, yeah, I think, yeah, of course. And I think also,
[00:35:20.000 --> 00:35:22.800]   they're even even, you know, you can go a lot bigger than that.
[00:35:22.800 --> 00:35:24.800]   And I still think I don't know what if the, you know, not
[00:35:24.800 --> 00:35:28.360]   everyone is Google. And I think that's, yeah, a lot of startups
[00:35:28.360 --> 00:35:31.920]   started copying whatever, like really large tech companies did,
[00:35:31.920 --> 00:35:34.040]   because it's like, Oh, if they're doing it must be right.
[00:35:34.040 --> 00:35:37.040]   I don't think that always applies. And it again, it ties
[00:35:37.040 --> 00:35:39.960]   back into this idea of, well, we need to, you need to reason
[00:35:39.960 --> 00:35:42.520]   about what you're doing. I think that might have also been one of
[00:35:42.520 --> 00:35:45.200]   one of the takeaways from that talk that you're referencing,
[00:35:45.200 --> 00:35:48.200]   we've always, we've also always done things differently. We've
[00:35:48.200 --> 00:35:51.560]   bootstrapped the company, we've always done things slightly
[00:35:51.560 --> 00:35:53.880]   differently from the startup playbook. And it means that
[00:35:53.880 --> 00:35:55.840]   well, if you're doing things differently, at every step of
[00:35:55.840 --> 00:35:58.240]   the way, you have to reason about things, there's no
[00:35:58.240 --> 00:36:02.000]   playbook, you can follow, to be safe, you everything has to make
[00:36:02.000 --> 00:36:07.080]   sense. And so, you know, we actually, yeah, we're not, at
[00:36:07.080 --> 00:36:09.040]   every step, we're thinking about, well, what's the most
[00:36:09.040 --> 00:36:12.880]   reasonable way to solve this problem? Or what's the most
[00:36:12.880 --> 00:36:15.400]   reasonable way to hire people? What's the most reasonable way
[00:36:15.400 --> 00:36:18.720]   to do that? And then we have to do that. And of course, we have
[00:36:18.720 --> 00:36:22.200]   to be right, because if we're wrong, that really sucks. So
[00:36:22.200 --> 00:36:24.600]   and that's, that's kind of that's the that's the magic of
[00:36:24.600 --> 00:36:26.880]   running a business. It's like you have to make lots of
[00:36:26.880 --> 00:36:30.760]   decisions every day. And you mostly have to be right. In the
[00:36:30.760 --> 00:36:31.680]   decisions you make.
[00:36:31.680 --> 00:36:36.160]   Sanyam Bhutani: As the CEO, do you spend a lot of time just
[00:36:36.160 --> 00:36:39.720]   thinking about these ideas? Or what does they what does your
[00:36:39.720 --> 00:36:42.600]   day I know you still code a lot. You said I was going to get a
[00:36:42.600 --> 00:36:44.960]   profile and it says you still contribute so much code to
[00:36:44.960 --> 00:36:49.640]   spacey. But as as the CEO, when you wear that hat, what does
[00:36:49.640 --> 00:36:50.480]   your day look like?
[00:36:50.480 --> 00:36:53.160]   Yeah, I mean, it's interesting you mentioned that because it's
[00:36:53.160 --> 00:36:56.400]   definitely like, there has been this more transition phase
[00:36:56.400 --> 00:37:00.840]   where, okay, I do have to get used to like, just not not doing
[00:37:00.840 --> 00:37:03.240]   everything myself, basically, because that used to be in the
[00:37:03.240 --> 00:37:05.600]   very beginning. That's how we started. And I do think that was
[00:37:05.600 --> 00:37:08.120]   very healthy, because it means, you know, we've always been very
[00:37:08.120 --> 00:37:10.800]   close to the work we're doing. And I do think it's quite very
[00:37:10.800 --> 00:37:14.120]   healthy if people, even people running the company have a very
[00:37:14.120 --> 00:37:17.720]   close idea of what the company does and how it's done. Because
[00:37:17.720 --> 00:37:20.680]   I think a lot of cases where things don't work out, the
[00:37:20.680 --> 00:37:24.520]   problem is that you have people, the engineers being very
[00:37:24.520 --> 00:37:28.880]   disconnected from the people making the decisions. But yeah,
[00:37:28.880 --> 00:37:31.800]   I definitely, I think at the moment with our team, that's
[00:37:31.800 --> 00:37:34.040]   been growing, we have a few, we have more people now we have
[00:37:34.040 --> 00:37:37.680]   more projects in the pipeline. So a lot of my work also
[00:37:37.680 --> 00:37:41.160]   consists of, you know, reviewing things people have done, you
[00:37:41.160 --> 00:37:44.200]   know, coordinating things and, you know, making sure that all
[00:37:44.200 --> 00:37:48.360]   the projects move forward, you know, giving giving my opinion
[00:37:48.360 --> 00:37:51.800]   on things. And then there's still some, you know, I'm still
[00:37:51.800 --> 00:37:54.160]   involved in some of the support, I'm still involved in, you know,
[00:37:54.160 --> 00:37:57.880]   talking to users, still involved in some software development.
[00:37:57.880 --> 00:38:01.600]   And, you know, I do like writing code. But yeah.
[00:38:02.640 --> 00:38:05.920]   Sanyam Bhutani: What's if I may, what's one of the like, tricky
[00:38:05.920 --> 00:38:10.360]   parts of your job as as leading the company or as the CEO?
[00:38:10.360 --> 00:38:11.400]   What's one tricky part?
[00:38:11.400 --> 00:38:13.680]   Rebecca Henderson: Tricky part? I mean, I don't know, it depends
[00:38:13.680 --> 00:38:18.160]   on what you mean by tricky, like, it's challenging. Yeah. I
[00:38:18.160 --> 00:38:22.240]   mean, I do think, you know, the aspect of, okay, I'm not, you
[00:38:22.240 --> 00:38:24.960]   know, I can't just do everything myself anymore. There is a point
[00:38:24.960 --> 00:38:27.680]   where that you get to where you know, okay, I, you know, I have
[00:38:27.680 --> 00:38:31.000]   it's other people are doing some a lot of the work and I kind of
[00:38:31.000 --> 00:38:34.600]   have to, you know, supervise that and make sure it's going
[00:38:34.600 --> 00:38:38.880]   ahead. But I can't just do everything myself anymore. And
[00:38:38.880 --> 00:38:42.160]   also, you know, taking a step back and you know, knowing I
[00:38:42.160 --> 00:38:43.880]   couldn't, you know, we have a great team, I can trust
[00:38:43.880 --> 00:38:46.800]   everyone, everyone's doing like an amazing job. And I can just
[00:38:46.800 --> 00:38:49.320]   like, oh, hand up hand something off that I used to do, and
[00:38:49.320 --> 00:38:51.640]   someone else will do it. And we'll probably maybe even do it
[00:38:51.640 --> 00:38:54.480]   better than I would have done or has like great ideas and move
[00:38:54.480 --> 00:38:56.640]   this forward. But it is especially if you've started a
[00:38:56.640 --> 00:39:00.240]   company as well. And it's been, you know, based on products that,
[00:39:00.440 --> 00:39:02.520]   you know, you've built or that you were very involved in to
[00:39:02.520 --> 00:39:06.960]   kind of, you know, take a step back and also, you know, that
[00:39:06.960 --> 00:39:11.840]   like other people take over. That was definitely Yeah, that
[00:39:11.840 --> 00:39:13.480]   was that was kind of a learning experience.
[00:39:13.480 --> 00:39:16.320]   Sanyam Bhutani: I can imagine delegation being harder there,
[00:39:16.320 --> 00:39:20.280]   because you just spend so much time as if this was like one of
[00:39:20.280 --> 00:39:23.680]   your main focuses for so long. And now you need to just to be
[00:39:23.680 --> 00:39:24.920]   able to do more things.
[00:39:24.920 --> 00:39:28.280]   Yeah. And also as a person, I'm also like, you know, I like to,
[00:39:28.280 --> 00:39:31.440]   you know, get hands on and like do things I'm not, you know, I'm
[00:39:31.440 --> 00:39:33.480]   not very comfortable just sitting back and letting other
[00:39:33.480 --> 00:39:37.080]   people do the job. I don't know, maybe that's also me being like
[00:39:37.080 --> 00:39:39.920]   German, but it's, you know, they like, I don't know, there's this
[00:39:39.920 --> 00:39:43.000]   sort of mindset where like, or I don't know, it's less, I felt
[00:39:43.000 --> 00:39:45.560]   bad, like thinking about, oh, maybe I should hire someone to
[00:39:45.560 --> 00:39:48.360]   help clean my apartment. I'm like, No, I'm like, I can clean
[00:39:48.360 --> 00:39:50.920]   my own apartment. Like, you know, I'm not, you know, I'm
[00:39:50.920 --> 00:39:53.920]   perfectly capable of doing that. It feels like wrong, not to do
[00:39:53.920 --> 00:39:56.160]   it. But then to be like, well, look, it's, it is more
[00:39:56.160 --> 00:39:59.720]   practical to just have someone else do some of the parts. So I
[00:39:59.720 --> 00:40:03.600]   can focus on other things I'm doing. But I do think it's just
[00:40:03.600 --> 00:40:06.600]   this this mindset, it's less about like, oh, I'm like, I
[00:40:06.600 --> 00:40:08.520]   don't think I'm particularly controlling. It's a lot. It's
[00:40:08.520 --> 00:40:12.280]   more about like, oh, I want to get involved and help. I don't
[00:40:12.280 --> 00:40:16.200]   just want to tell people what to do. And then sometimes you also
[00:40:16.200 --> 00:40:20.480]   have to, you know, you have to let go and have to let other
[00:40:20.480 --> 00:40:21.320]   people do the work.
[00:40:22.760 --> 00:40:27.320]   Sanyam Bhutani: I can understand how it's challenging for you. If
[00:40:27.320 --> 00:40:32.560]   if I may, you've shown us this different business model,
[00:40:32.560 --> 00:40:36.840]   especially with Explosion where you've sold 5% equity of your
[00:40:36.840 --> 00:40:40.880]   company and raised money through that. Does that change anything
[00:40:40.880 --> 00:40:44.160]   for you on the roadmaps? Or are you still continue? Will you
[00:40:44.160 --> 00:40:46.640]   still continue doing whatever you've been doing so far?
[00:40:46.640 --> 00:40:47.880]   I mean,
[00:40:47.880 --> 00:40:49.840]   Dr. Jennifer Noxen: actually, it was all like based on, you
[00:40:49.840 --> 00:40:52.120]   know, the plans we already had, but we were at the point, you
[00:40:52.120 --> 00:40:55.360]   know, our next product is Prodigy Teams, which will be a
[00:40:55.360 --> 00:41:00.360]   more SaaS like platform around Prodigy, but with the same data
[00:41:00.360 --> 00:41:03.760]   privacy, the same scriptability. And because of all of that, it's
[00:41:03.760 --> 00:41:06.800]   a very complex piece of software. It's a complex thing.
[00:41:06.800 --> 00:41:09.680]   And, you know, we've wanted to get this ready for quite a while.
[00:41:09.680 --> 00:41:13.680]   So that we also realized that like, we want to get some more
[00:41:13.680 --> 00:41:16.280]   people involved. We want to hire some more people. And we also
[00:41:16.280 --> 00:41:20.520]   want to focus on hiring a lot of experienced and good people
[00:41:20.560 --> 00:41:23.000]   because I don't think it's, you know, it doesn't make sense to
[00:41:23.000 --> 00:41:26.240]   just try to, you know, be very stingy. They're like, you know,
[00:41:26.240 --> 00:41:28.880]   we really need need a great team in place to make this work. And
[00:41:28.880 --> 00:41:33.680]   so that was, that was kind of the motivation. And because we
[00:41:33.680 --> 00:41:36.800]   were also the business, we were a profitable company, we were in
[00:41:36.800 --> 00:41:40.080]   a position of having, you know, a successful business. So we
[00:41:40.080 --> 00:41:43.960]   were able to say, okay, we don't need to make some grandiose
[00:41:43.960 --> 00:41:48.240]   promises about like, changing the world with our AI, or
[00:41:48.240 --> 00:41:50.240]   something like that, even though, you know, sure, you can
[00:41:50.240 --> 00:41:52.520]   have a big impact, but like, you know, we didn't, we don't need
[00:41:52.520 --> 00:41:55.200]   to come up with some grandiose dream to sell to investors, we
[00:41:55.200 --> 00:41:59.600]   can just say, here's like an actual business. And we were
[00:41:59.600 --> 00:42:03.760]   able to sell some shares in that business in return for money
[00:42:03.760 --> 00:42:06.560]   that we can use to hire some more people and to move our
[00:42:06.560 --> 00:42:09.560]   plans forward. And yeah,
[00:42:09.560 --> 00:42:14.080]   Sanyam Bhutani: Does this sound very organic as well to me, at
[00:42:14.080 --> 00:42:14.440]   least?
[00:42:14.440 --> 00:42:17.120]   Sarah: Yeah. And I mean, it's also I think they are some, you
[00:42:17.120 --> 00:42:19.320]   know, there's, it always depends on the on the business. There
[00:42:19.320 --> 00:42:20.920]   are a lot of, you know, there are a lot of businesses where
[00:42:20.920 --> 00:42:24.480]   yes, you know, you need you need more capital, you need to, you
[00:42:24.480 --> 00:42:27.480]   know, you, you can't just so easily bootstrap in certain
[00:42:27.480 --> 00:42:29.840]   ways, like it's all it all really depends. But like, in our
[00:42:29.840 --> 00:42:32.560]   case, for the work we're doing, we definitely see a lot of the
[00:42:32.560 --> 00:42:35.240]   classic venture capital approaches as not very
[00:42:35.240 --> 00:42:38.880]   compatible with, you know, the way we're doing things, and also
[00:42:38.880 --> 00:42:42.520]   the kind of stability that open source requires. We have told a
[00:42:42.520 --> 00:42:45.360]   lot of people that they can rely on our software, they can rely
[00:42:45.600 --> 00:42:50.240]   on spacey, they should be using our tools. And I do feel like we
[00:42:50.240 --> 00:42:53.080]   have a certain responsibility to our users. And yes, there might
[00:42:53.080 --> 00:42:56.040]   be people, there might be companies who then turn around
[00:42:56.040 --> 00:42:58.880]   and, you know, act as if, you know, it doesn't matter. We're
[00:42:58.880 --> 00:43:01.120]   now I don't know, going for something else. But I don't
[00:43:01.120 --> 00:43:06.920]   think that's a fair or that's a decent to do as a company. It's
[00:43:06.920 --> 00:43:09.520]   certainly not a company, you know, that I would want to run.
[00:43:09.520 --> 00:43:13.520]   So we think this level of stability and making sure that
[00:43:13.520 --> 00:43:17.640]   our open source work is self sufficient and stable is very
[00:43:17.640 --> 00:43:22.040]   important for the community and for our users. And that's why we
[00:43:22.040 --> 00:43:26.200]   chose an approach like that. And I think it's still, you know, it
[00:43:26.200 --> 00:43:28.240]   works. And yes, it worked well, because we did things slightly
[00:43:28.240 --> 00:43:32.000]   differently before. We're not, you know, we can't follow any
[00:43:32.000 --> 00:43:34.440]   playbook, we're just kind of doing things that way. But
[00:43:34.440 --> 00:43:38.520]   overall, I'm very satisfied. I'm also been, you know, very happy
[00:43:38.520 --> 00:43:43.080]   that it's, you know, even in, you know, the investment
[00:43:43.080 --> 00:43:45.520]   ecosystem, things, things change, it's been very
[00:43:45.520 --> 00:43:50.320]   refreshing to see that they are investors who do understand open
[00:43:50.320 --> 00:43:53.640]   source and do understand, you know, how developer tools and
[00:43:53.640 --> 00:43:57.760]   open source are kind of different from maybe like other
[00:43:57.760 --> 00:44:01.560]   other, you know, pieces of software that startups were
[00:44:01.560 --> 00:44:03.560]   involved in previously, and how it does require a different
[00:44:03.560 --> 00:44:07.840]   mindset and require running things a different way, or, you
[00:44:07.840 --> 00:44:10.640]   know, that our approach makes sense and is reasonable. And so
[00:44:10.640 --> 00:44:13.040]   yeah, that was also a factor in it.
[00:44:13.040 --> 00:44:17.000]   Sanyam Bhutani: Makes sense. Let me reshare my screen and
[00:44:17.000 --> 00:44:22.640]   continue. Someone is asking, when will you release a course
[00:44:22.640 --> 00:44:24.280]   for v3 of spacey?
[00:44:24.280 --> 00:44:27.520]   Oh, yeah, actually, this is something we are currently
[00:44:27.520 --> 00:44:31.720]   working on. If you want to, like you can lurk on on the pull
[00:44:31.720 --> 00:44:35.640]   requests, because there is a there is a PR up with a deploy
[00:44:35.640 --> 00:44:38.480]   preview. So it's pretty much ready, we have it ready for
[00:44:38.480 --> 00:44:40.840]   free languages. So we're probably going to have that out
[00:44:40.840 --> 00:44:44.080]   this week. So it was a very timely question. But But the
[00:44:44.080 --> 00:44:47.120]   truth is also that it's really just one chapter that's changed
[00:44:47.120 --> 00:44:50.840]   because, you know, spacey, spacey has changed a lot over
[00:44:50.840 --> 00:44:53.680]   the years, obviously, and v3 brought a lot of new features,
[00:44:53.680 --> 00:44:56.240]   but the user facing API and a lot of the things you do
[00:44:56.240 --> 00:45:00.320]   actually hasn't hasn't changed so much. So most of the course
[00:45:00.320 --> 00:45:03.240]   was still like perfectly compatible with the new version
[00:45:03.240 --> 00:45:06.720]   and only some parts of like, you know, training models had to
[00:45:06.720 --> 00:45:09.360]   change. So that was also something that's quite
[00:45:09.360 --> 00:45:11.960]   satisfying that like, oh, it's not, you know, we do release new
[00:45:11.960 --> 00:45:14.560]   versions, and we release a lot of changes and release a lot of
[00:45:14.560 --> 00:45:18.920]   features. But yeah, we have it's also been the software has been
[00:45:18.920 --> 00:45:21.280]   very stable. And it's not like, oh, we're breaking everyone's
[00:45:21.280 --> 00:45:24.280]   workflows and introducing like a completely different library
[00:45:24.280 --> 00:45:27.680]   branded as a new version. So yeah.
[00:45:27.680 --> 00:45:32.520]   Over time, have have you seen any principles evolve for this
[00:45:32.560 --> 00:45:35.240]   API design that you mentioned, like you've always been so
[00:45:35.240 --> 00:45:39.040]   conscious not to break the user facing API? I assume it was
[00:45:39.040 --> 00:45:41.520]   quite practical in the beginning. But now since it's a
[00:45:41.520 --> 00:45:44.320]   bigger code base, are there any principles that you try to
[00:45:44.320 --> 00:45:47.440]   follow while trying to maintain or update the internal
[00:45:47.440 --> 00:45:48.560]   framework?
[00:45:48.560 --> 00:45:52.520]   So definitely stability is a big point. Because again, like I
[00:45:52.520 --> 00:45:54.680]   said, like we have been since the beginning, we've been
[00:45:54.680 --> 00:45:57.720]   telling people, hey, spacey is a library for industrial strength,
[00:45:57.720 --> 00:46:00.000]   natural language processing, and it's something you should rely
[00:46:00.000 --> 00:46:03.240]   on in production. And yes, you can, you know, but I feel like
[00:46:03.240 --> 00:46:06.160]   if you say that you also have to take the responsibility which
[00:46:06.160 --> 00:46:09.520]   comes with it, which is a you have to, you know, try and not
[00:46:09.520 --> 00:46:12.520]   to break people's shit, because that's really annoying. If you
[00:46:12.520 --> 00:46:15.040]   know, they're using your stuff in production. It also means
[00:46:15.040 --> 00:46:17.880]   that well, you can't just have it both ways and rely on the
[00:46:17.880 --> 00:46:20.840]   community to fix your bugs. Like we were always very conscious
[00:46:20.840 --> 00:46:23.080]   that if there is a problem, and we're telling people to use our
[00:46:23.080 --> 00:46:26.080]   software, well, if there's a problem, we need to fix it. And
[00:46:26.080 --> 00:46:28.400]   it's kind of on us to fix it. And we can't just turn around and
[00:46:28.400 --> 00:46:32.200]   be like, oh, it's an open source project. So, you know, it's
[00:46:32.200 --> 00:46:34.400]   always great to have get people involved. But it's like, oh,
[00:46:34.400 --> 00:46:37.880]   you know, you can't tell people, hey, use my software and get
[00:46:37.880 --> 00:46:39.840]   lots of users and then turn around and blame it on the
[00:46:39.840 --> 00:46:42.680]   community if your software doesn't work. So that was
[00:46:42.680 --> 00:46:45.160]   definitely a principle in terms of just okay, how we organize
[00:46:45.160 --> 00:46:48.880]   the engineering and otherwise, another principle we have for
[00:46:48.880 --> 00:46:52.520]   writing code or writing developer tools more generally
[00:46:52.520 --> 00:46:57.000]   is what I refer to as let them write code. So the idea is, you
[00:46:57.000 --> 00:47:00.880]   know, you can you want to have tools that are easy to use, but
[00:47:00.880 --> 00:47:03.960]   also very powerful. And that's what at the core of spacey,
[00:47:03.960 --> 00:47:07.560]   that's what at the core of Prodigy. And the idea behind it
[00:47:07.560 --> 00:47:11.400]   is, well, instead of adding more and more abstractions to make
[00:47:11.400 --> 00:47:15.920]   things easier, you want to be writing code and writing APIs
[00:47:15.920 --> 00:47:20.120]   that people can program with. So, you know, instead of, you
[00:47:20.120 --> 00:47:23.440]   know, having to think of everything a user might want,
[00:47:23.440 --> 00:47:26.280]   you can provide users with an API that they can just extend
[00:47:26.480 --> 00:47:29.760]   and keep working. No developer wants to like be blocked by
[00:47:29.760 --> 00:47:33.160]   some feature requests. If you know, there's an API that lets
[00:47:33.160 --> 00:47:36.440]   you add your own database that lets you add your own function,
[00:47:36.440 --> 00:47:39.280]   you want to do that you don't want to just have to submit a
[00:47:39.280 --> 00:47:42.240]   pull request or like submit a feature request and wait for
[00:47:42.240 --> 00:47:45.440]   someone to implement that that's like, not great for developer
[00:47:45.440 --> 00:47:48.560]   tools. So this kind of script ability and building APIs that
[00:47:48.560 --> 00:47:54.200]   extensible has been very important. And I guess also the
[00:47:54.200 --> 00:47:58.040]   other ideas, well, you, some people might tell you that,
[00:47:58.040 --> 00:48:00.440]   like, oh, you shouldn't be reinventing the wheel. I do.
[00:48:00.440 --> 00:48:03.000]   I'm not so opposed to that. I think you can reinvent the
[00:48:03.000 --> 00:48:06.040]   wheel, but you shouldn't be reinventing the road. And I
[00:48:06.040 --> 00:48:08.440]   think that's, that's a common like pitfall. That's a common
[00:48:08.440 --> 00:48:13.000]   problem you can fall into. And I think and what this means is,
[00:48:13.000 --> 00:48:17.120]   there is an ecosystem, there is a way of doing things, they are
[00:48:17.120 --> 00:48:20.000]   people's workflows. And for example, in Python, they are
[00:48:20.000 --> 00:48:23.800]   shaped by the Python ecosystem. And you can, you know, build a
[00:48:23.800 --> 00:48:26.480]   great library that does some things very differently and
[00:48:26.480 --> 00:48:30.000]   comes up with new ideas for old problems. But you shouldn't want
[00:48:30.000 --> 00:48:34.200]   to own people's entire way of working, like you want to be
[00:48:34.200 --> 00:48:36.320]   building something that integrates with what people are
[00:48:36.320 --> 00:48:38.720]   already doing. And for example, Python is one way like you want
[00:48:38.720 --> 00:48:42.680]   to, you want to follow the way people write Python code, you,
[00:48:42.680 --> 00:48:46.880]   you know, you can provide people with like helpful command line
[00:48:46.880 --> 00:48:49.440]   tools and helpful, helpful things, but you don't want to
[00:48:49.440 --> 00:48:52.840]   want people to suddenly have to use your platform or your
[00:48:53.240 --> 00:48:58.080]   operating system or your, you know, I don't know your, your
[00:48:58.080 --> 00:49:01.720]   layer on top of everything. You want people to program like they
[00:49:01.720 --> 00:49:05.480]   normally do, with some help of your library, basically. So
[00:49:05.480 --> 00:49:08.720]   that's this idea of, you know, not, you know, we never want to
[00:49:08.720 --> 00:49:12.280]   reinvent the road, but maybe we'll reinvent the wheel, maybe
[00:49:12.280 --> 00:49:16.360]   we won't. But, yeah, and I think another advantage of that is that
[00:49:16.360 --> 00:49:19.560]   for people learning a new technology, it means that there
[00:49:19.560 --> 00:49:22.280]   are a lot of other resources, you know, you don't just have to
[00:49:22.280 --> 00:49:25.240]   you learn a framework, like I'm usually quite, I know people
[00:49:25.240 --> 00:49:27.840]   want to learn spaCy and we have a course for that, that teaches
[00:49:27.840 --> 00:49:30.400]   some of the basic principles and how the library works. But I'm
[00:49:30.400 --> 00:49:34.520]   very, I do think you shouldn't have to just learn a framework.
[00:49:34.520 --> 00:49:38.040]   Like if you learn, if you know Python, you can, and know
[00:49:38.040 --> 00:49:41.000]   machine learning, you can learn a bit about, you know, spaCy's
[00:49:41.000 --> 00:49:43.840]   methods, and then you can use spaCy like, and you can, you
[00:49:43.840 --> 00:49:46.920]   know, you can iterate over an object, you can use what you
[00:49:46.920 --> 00:49:50.320]   already know in programming to do something. And I think that's,
[00:49:50.400 --> 00:49:53.240]   that's also something we follow in our other tools. So in
[00:49:53.240 --> 00:49:58.200]   Prodigy, workflow is a Python function. The data is queued up
[00:49:58.200 --> 00:50:00.960]   by a generator, all of these are things maybe someone doesn't
[00:50:00.960 --> 00:50:02.880]   know, but like you can learn about, you don't have to rely on
[00:50:02.880 --> 00:50:07.760]   us to provide the documentation for it, you can learn, you can
[00:50:07.760 --> 00:50:10.360]   find tons of resources. And if you know these concepts, you can
[00:50:10.360 --> 00:50:13.800]   just use things and you don't just have to learn all our APIs.
[00:50:13.800 --> 00:50:18.040]   Sanyam Bhutani: On the topic of reinventing the wheel, you said
[00:50:18.040 --> 00:50:20.840]   you're not opposed to that. Have you ever found yourself
[00:50:20.840 --> 00:50:25.320]   rewriting parts of the codebase or maybe the codebase? Or just
[00:50:25.320 --> 00:50:25.920]   refactoring?
[00:50:25.920 --> 00:50:30.080]   I'll be sure we I would say we constantly, you know, refactor
[00:50:30.080 --> 00:50:32.680]   things like when we did spaCy 3, there was a lot of refactoring
[00:50:32.680 --> 00:50:36.240]   involved. But I also think, you know, it is important to set up
[00:50:36.240 --> 00:50:38.840]   a codebase well from the beginning, like actually, a fun
[00:50:38.840 --> 00:50:42.160]   fact about spaCy is that a lot of the data structures, like the
[00:50:42.160 --> 00:50:45.600]   doc, token, span, these are all things they've been around since
[00:50:45.600 --> 00:50:48.680]   the beginning. It's one of the first things that Matt wrote
[00:50:48.680 --> 00:50:51.520]   when he started spaCy. And a lot of them actually haven't changed
[00:50:51.520 --> 00:50:54.600]   very much at all. So a lot of that, and that took a lot of
[00:50:54.600 --> 00:50:56.400]   time. And maybe, you know, someone would have back then
[00:50:56.400 --> 00:50:58.880]   would have said, like, Oh, why do you spend all this time
[00:50:58.880 --> 00:51:03.000]   writing these really efficient, memory managed data structures?
[00:51:03.000 --> 00:51:05.880]   And why do you really care so much about this, just use like a
[00:51:05.880 --> 00:51:10.160]   dictionary and be done with it. But it's really paid off to, you
[00:51:10.160 --> 00:51:13.040]   know, do things well from the beginning. Because if you don't,
[00:51:13.040 --> 00:51:15.880]   then yeah, you really have to refactor everything. And that
[00:51:15.880 --> 00:51:19.560]   really sucks. And, you know, ideally, you don't want to be
[00:51:19.560 --> 00:51:24.160]   doing that. So but of course, we do refactor a lot. We, you know,
[00:51:24.160 --> 00:51:28.120]   we, you know, we write a lot of code, we change things, we
[00:51:28.120 --> 00:51:30.520]   improve things, I think that's a normal part of software
[00:51:30.520 --> 00:51:31.040]   development.
[00:51:31.040 --> 00:51:35.480]   Makes sense. I would I could possibly keep talking about
[00:51:35.480 --> 00:51:38.720]   that. But since we're coming up on time, I'll go back to the
[00:51:38.720 --> 00:51:43.640]   community questions. We've discussed this. But one thing we
[00:51:43.640 --> 00:51:47.240]   haven't from here for this question is, how do the
[00:51:47.240 --> 00:51:52.320]   brainstorming sessions happen for you? Ever? Or like, even
[00:51:52.320 --> 00:51:54.960]   especially I'm curious about this part as a remote team, how
[00:51:54.960 --> 00:51:59.000]   do you make these sessions happen? Over zoom over async
[00:51:59.000 --> 00:52:00.000]   docs? I'm also
[00:52:00.000 --> 00:52:02.480]   Yeah, I mean, this is this is a great question. Because
[00:52:02.480 --> 00:52:05.480]   definitely like, yeah, also, yeah, thanks. I'm glad like,
[00:52:05.480 --> 00:52:08.240]   you know, yeah, like the conflict system. This was this
[00:52:08.240 --> 00:52:10.160]   is definitely also something we're quite proud of. And this
[00:52:10.160 --> 00:52:14.280]   did take quite some, you know, iterating and brainstorming.
[00:52:14.280 --> 00:52:17.480]   And actually, for spacey three, we did that was just before
[00:52:17.480 --> 00:52:20.520]   COVID, we did end up doing a small like team meetup in
[00:52:20.520 --> 00:52:23.760]   Berlin, where we had the core developers all meeting up. And
[00:52:23.760 --> 00:52:26.360]   we did do a lot of brainstorming, kind of in real
[00:52:26.360 --> 00:52:30.920]   life and going back and forth on things. We do also use we do
[00:52:30.920 --> 00:52:34.200]   use a lot of planning documents, especially now with a bigger
[00:52:34.200 --> 00:52:38.040]   remote team. We basically, you know, we just write down the
[00:52:38.040 --> 00:52:40.360]   thought process. And we also think that's very important.
[00:52:40.360 --> 00:52:43.840]   Like, you know, if you work on software, you should be able to,
[00:52:43.840 --> 00:52:47.240]   you know, think, write down your thoughts and write down a
[00:52:47.240 --> 00:52:50.400]   proposal and that sort of, you know, way and communicate these
[00:52:50.400 --> 00:52:53.440]   ideas. And that's also something we do. And like, yeah,
[00:52:53.440 --> 00:52:55.840]   definitely for spacey three, there was a lot of back and
[00:52:55.840 --> 00:52:59.120]   forth, we did have some calls, we did have, you know, just a
[00:52:59.120 --> 00:53:03.720]   lot of, you know, code review, and, you know, trying things
[00:53:03.720 --> 00:53:06.200]   out and an ad that doesn't work, that doesn't work, because
[00:53:06.200 --> 00:53:09.760]   really, the idea, the idea with the config was, well, we have
[00:53:09.760 --> 00:53:12.160]   like, machine learning is incredibly complex. And there
[00:53:12.160 --> 00:53:15.000]   are a lot of the hyper parameters, the settings, every
[00:53:15.000 --> 00:53:17.560]   part of the whole process of training a model, for example,
[00:53:17.560 --> 00:53:20.760]   has like, settings. And previously, you know, if you
[00:53:20.760 --> 00:53:24.000]   just write different functions, and you pass in some settings,
[00:53:24.000 --> 00:53:26.800]   you end up with this problem of having all these hidden defaults
[00:53:26.800 --> 00:53:29.280]   you're not setting one parameter there, and then your whole like
[00:53:29.280 --> 00:53:32.920]   thing falls apart, like hidden, hidden defaults in code were
[00:53:32.920 --> 00:53:35.600]   like, you know, a big issue. And so instead of saying, Oh, we're
[00:53:35.600 --> 00:53:38.840]   not exposing any of the abstractions, we decided, let's
[00:53:38.840 --> 00:53:42.840]   just expose them all, like, you obviously don't have to write
[00:53:42.840 --> 00:53:44.920]   down all your config parameters, and can be automatically
[00:53:44.920 --> 00:53:48.240]   generated. But we want to make sure that every setting is
[00:53:48.240 --> 00:53:52.160]   recorded. And settings can also include code, which is, you
[00:53:52.160 --> 00:53:54.880]   know, another problem you otherwise have, you're like,
[00:53:54.880 --> 00:53:57.160]   just programming in your config, and you end up with too many
[00:53:57.160 --> 00:54:00.880]   settings. And so what we did instead was, we let you write
[00:54:00.880 --> 00:54:03.840]   refer to functions in code. And then, you know, you can really
[00:54:03.840 --> 00:54:06.680]   build up the tree of different objects, you can customize every
[00:54:06.680 --> 00:54:10.640]   part of it if you want, but you can also not. And so that did
[00:54:10.640 --> 00:54:14.240]   take some time. And yeah, I think it's definitely talking
[00:54:14.240 --> 00:54:18.680]   planning documents, you know, really thinking, you know, it
[00:54:18.680 --> 00:54:22.280]   does require also very close contact with the user and what
[00:54:22.280 --> 00:54:26.080]   people are trying to solve and different, you know, use cases,
[00:54:26.080 --> 00:54:29.240]   which we're also hoping to do more going forward.
[00:54:29.240 --> 00:54:31.720]   Sanyam Bhutani: Yeah, I was curious about that, but I'm
[00:54:31.720 --> 00:54:34.640]   guessing a lot would come through user feedback or through
[00:54:34.640 --> 00:54:37.760]   open PRs or issues where people try to tell you something that
[00:54:37.760 --> 00:54:40.000]   maybe in hindsight, you wouldn't have thought of and that leads
[00:54:40.000 --> 00:54:41.480]   to the factor or addition.
[00:54:41.480 --> 00:54:45.080]   Rebecca Hendry: But I think also just getting like really seeing
[00:54:45.080 --> 00:54:48.240]   what people are building, like, you know, even just the end goal
[00:54:48.240 --> 00:54:50.360]   and the types of applications people are building, because the
[00:54:50.360 --> 00:54:53.120]   problem you do have an open source is that you only really
[00:54:53.120 --> 00:54:56.520]   hear from people who have a problem. And that's not and who
[00:54:56.520 --> 00:54:59.000]   want to communicate that problem. And that's not always
[00:54:59.600 --> 00:55:03.320]   so helpful, because it's, you know, it's only one small slice
[00:55:03.320 --> 00:55:07.160]   of the usage segment. So we also, we do want to do going
[00:55:07.160 --> 00:55:09.280]   forward, we don't want to do a few more, we don't want to do
[00:55:09.280 --> 00:55:14.400]   more projects, where we help users and companies build
[00:55:14.400 --> 00:55:17.360]   spacy pipelines and break down their problems into machine
[00:55:17.360 --> 00:55:19.960]   learning components, because we want to make sure that also we're
[00:55:19.960 --> 00:55:23.240]   working on software that people use. And we want to make sure
[00:55:23.240 --> 00:55:26.080]   that we also stay up to date with what people are trying to
[00:55:26.080 --> 00:55:29.320]   do. And we know all the use cases, because we want to, you
[00:55:29.320 --> 00:55:32.320]   know, be able to confidently build new components that solve
[00:55:32.320 --> 00:55:35.520]   new problems that people are having. And, you know, what
[00:55:35.520 --> 00:55:38.400]   types of things do you want to predict over text? What makes
[00:55:38.400 --> 00:55:40.360]   most sense? How do you want to combine them? There's so many
[00:55:40.360 --> 00:55:43.240]   questions. And I think that you can only really do that if you
[00:55:43.240 --> 00:55:45.880]   get to see real world problems.
[00:55:45.880 --> 00:55:49.720]   Sanyam Bhutani: Makes sense. There are a few comments about
[00:55:49.720 --> 00:55:52.760]   how everyone's enjoying your energy, which I found to be
[00:55:52.760 --> 00:55:56.880]   truly incredible throughout all of your talks. But I could keep
[00:55:56.880 --> 00:55:59.040]   asking questions to be respectful of your time. I'll
[00:55:59.040 --> 00:56:03.680]   just take two last questions from the community. One of which
[00:56:03.680 --> 00:56:07.040]   is, do you have any thoughts on the role of linguists in the
[00:56:07.040 --> 00:56:07.800]   field of AI?
[00:56:07.800 --> 00:56:12.440]   Dr. Rima Bhanu: The role of linguists? So I do think I mean,
[00:56:12.440 --> 00:56:16.800]   AI is very broad, but I do think for NLP, there's definitely a,
[00:56:16.800 --> 00:56:20.800]   you know, a, you know, a need for, you know, people who know
[00:56:20.800 --> 00:56:23.360]   about linguistics. And I think linguistics is incredibly
[00:56:23.360 --> 00:56:26.560]   important just to even understand some of the basics
[00:56:26.560 --> 00:56:30.000]   about how language works, and which then, you know, kind of,
[00:56:30.000 --> 00:56:35.960]   you know, influences what how we are building systems to analyze
[00:56:35.960 --> 00:56:38.080]   language. And I think linguistics helps a lot there.
[00:56:38.080 --> 00:56:41.520]   And I, it's not maybe the number one thing you should be focusing
[00:56:41.520 --> 00:56:44.520]   on, or if you're coming into the field from like, a more general
[00:56:44.520 --> 00:56:46.280]   programming background, or you've done machine learning,
[00:56:46.280 --> 00:56:48.400]   you're like, how much linguistics do I really need?
[00:56:48.400 --> 00:56:50.840]   It's maybe not the number one thing to focus on, but it's an
[00:56:50.840 --> 00:56:54.320]   important thing to focus on. And, you know, there is this,
[00:56:54.320 --> 00:56:57.960]   often, when I talk to people, it seems like linguistics is this
[00:56:57.960 --> 00:57:00.120]   sort of, if people don't have a linguistics background, it's
[00:57:00.120 --> 00:57:04.160]   kind of this secret source of like, that people feel like
[00:57:04.160 --> 00:57:06.000]   they're missing, like you often come in, and you're trying to
[00:57:06.000 --> 00:57:07.720]   solve a problem, you're like, there's something I'm missing,
[00:57:07.720 --> 00:57:09.560]   there's something I'm not getting here, I feel like I'm,
[00:57:09.560 --> 00:57:13.880]   I don't know, there is this structure in the language that I
[00:57:13.880 --> 00:57:18.280]   want to take advantage of, but that I can't quite get behind.
[00:57:18.280 --> 00:57:20.800]   And I do think linguistics helps with that. It's also something
[00:57:20.800 --> 00:57:23.280]   we emphasize in the team, like a lot of the people on our team
[00:57:23.280 --> 00:57:26.240]   have a linguistics background, I think it's very, it's very
[00:57:26.240 --> 00:57:30.960]   important in a team. And yeah, I also hope that the you know,
[00:57:30.960 --> 00:57:33.880]   the field should really, you know, recognize this more. And,
[00:57:33.880 --> 00:57:37.040]   you know, this is important.
[00:57:37.040 --> 00:57:41.920]   Sanyam Bhutani: Makes sense. I'll request the audience to
[00:57:41.920 --> 00:57:44.640]   send in one last question, which I'll pick in the meantime, I'll
[00:57:44.640 --> 00:57:48.480]   just point out to where people can find you on the internet. I
[00:57:48.480 --> 00:57:51.600]   think one of the top resources would be our website, Inez.io
[00:57:51.600 --> 00:57:56.320]   and they can find all of the links. Inez is also on Twitter
[00:57:56.320 --> 00:58:00.200]   @_inezmontani, her first name concatenated with the second
[00:58:00.200 --> 00:58:04.840]   name. There's a GitHub which should point you to all of the
[00:58:04.840 --> 00:58:08.000]   projects that she's contributing to. And you could also go to
[00:58:08.000 --> 00:58:11.560]   Explosion instead. You could also watch Explosion's
[00:58:11.560 --> 00:58:14.400]   tutorials, which I've found I've always found to have the same
[00:58:14.400 --> 00:58:19.200]   level of aesthetics across all of them. And I'm personally
[00:58:19.200 --> 00:58:22.880]   really looking forward to the next course. There's also the
[00:58:22.880 --> 00:58:27.000]   website where you can check out Prodigy and use it if you like.
[00:58:27.000 --> 00:58:28.560]   Did I miss anything Inez?
[00:58:28.560 --> 00:58:31.760]   Inez Montani: Oh, no, I think those are the most important
[00:58:31.760 --> 00:58:36.040]   points. Yeah, there's Explosion, there's Spacey, we're on like
[00:58:36.040 --> 00:58:40.240]   social media. But I think, yeah, if you know, if you kind of
[00:58:40.240 --> 00:58:42.760]   start from one place, and then you can usually find all the
[00:58:42.760 --> 00:58:44.360]   other things we're doing if you're interested.
[00:58:44.360 --> 00:58:47.920]   Sanyam Bhutani: I see one user is typing the questions. I'll
[00:58:47.920 --> 00:58:48.520]   give them one.
[00:58:48.520 --> 00:58:50.840]   Inez Montani: Okay, yeah, sure. No, I mean, I have time. I don't
[00:58:50.840 --> 00:58:54.760]   want to, you know, just, yeah, use up all of your time. But if
[00:58:54.760 --> 00:58:56.320]   you have more questions, I'm here.
[00:58:56.320 --> 00:59:00.520]   Sanyam Bhutani: Awesome. In that case, I'll probably ask two
[00:59:00.520 --> 00:59:05.840]   more. There's one question around. I don't, I think it's
[00:59:05.840 --> 00:59:09.240]   not clear, but they're asking about models that Spacey is
[00:59:09.240 --> 00:59:13.320]   using now. Maybe they're asking about Prodigy. Or I'm not sure.
[00:59:13.320 --> 00:59:16.600]   Inez Montani: Okay, yeah, if you can pull it up, maybe I can, I
[00:59:16.600 --> 00:59:17.040]   can read it.
[00:59:17.040 --> 00:59:21.640]   Sanyam Bhutani: Sure. It's still in the YouTube chat. I'll have
[00:59:21.640 --> 00:59:22.480]   to zoom in a bit.
[00:59:22.480 --> 00:59:29.480]   Inez Montani: Prodigy, very nice. Data models that Spacey
[00:59:29.480 --> 00:59:34.400]   uses. Oh, you can. So I think the easiest explanation is we
[00:59:34.400 --> 00:59:39.960]   have on the Spacey page, we have an API section that tells us,
[00:59:39.960 --> 00:59:42.240]   gives you an overview of all the model architectures, if that's
[00:59:42.240 --> 00:59:44.640]   what you're talking about. If you mean like, oh, what's what
[00:59:44.640 --> 00:59:48.080]   kind of implementations? You know, the different components
[00:59:48.080 --> 00:59:51.440]   using what's the what's the default name entity recognizer?
[00:59:51.440 --> 00:59:55.080]   That's all something we have documented in the architectures.
[00:59:55.080 --> 00:59:57.840]   And they're all different functions. And you can see if
[00:59:57.840 --> 01:00:00.560]   you're interested, depending on the level of detail, you can see
[01:00:00.560 --> 01:00:03.800]   the types of models, how they implemented that you can see all
[01:00:03.800 --> 01:00:06.520]   the hyper parameters, if you really want to dig deeper, you
[01:00:06.520 --> 01:00:09.280]   can see the defaults that they use. So I think that's the best
[01:00:09.280 --> 01:00:12.440]   resource because it's also the implementations they vary by
[01:00:12.440 --> 01:00:17.680]   component. We provide some pre trained pipelines, you can train
[01:00:17.680 --> 01:00:21.840]   your own pipelines with different settings. Yeah. I
[01:00:21.840 --> 01:00:23.720]   think yeah, API docs are the best place to start.
[01:00:23.720 --> 01:00:27.400]   Sanyam Bhutani: Awesome. Thanks. Do you have any thoughts on
[01:00:27.400 --> 01:00:30.720]   what's one underrated thing in NLP? I'm stealing this question
[01:00:30.720 --> 01:00:34.240]   from a CEO who always asked this in his podcast, gradient
[01:00:34.240 --> 01:00:36.880]   descent, which you've also appeared on but
[01:00:36.880 --> 01:00:40.120]   Rebecca Hendrickson: was was not a question. Did I answer this
[01:00:40.120 --> 01:00:43.320]   question before? Because if so, I'm like, I don't I don't
[01:00:43.320 --> 01:00:45.840]   remember answering this question before. So I'd be curious what I
[01:00:45.840 --> 01:00:48.800]   said back then if if Lucas actually asked that question on
[01:00:48.800 --> 01:00:54.200]   the podcast. Yeah, I think I did like well to to to kind of stay
[01:00:54.200 --> 01:00:58.120]   on brand, I do think data is like, you know, still incredibly
[01:00:58.120 --> 01:01:01.880]   underrated, like iterating on your data, you know, focusing on
[01:01:01.880 --> 01:01:05.400]   your data, spending time with your data, thinking about, you
[01:01:05.400 --> 01:01:09.200]   know, how to break break down, breaking down problems and
[01:01:09.200 --> 01:01:13.320]   focusing on all the stuff around. You know, how you know,
[01:01:13.320 --> 01:01:16.360]   how you even solving your problem, before you get into the
[01:01:16.360 --> 01:01:20.800]   details of picking which model to train or, or, you know, what
[01:01:20.800 --> 01:01:23.160]   embeddings to initialize your model with. That's like, that's
[01:01:23.160 --> 01:01:25.120]   pretty interesting. There's a lot of work happening there. But
[01:01:25.120 --> 01:01:28.360]   often, you know, this is just like the little the cherry on
[01:01:28.360 --> 01:01:31.360]   top that like makes your, you know, your project actually go
[01:01:31.360 --> 01:01:34.240]   like there's so much more work that goes into the whole process
[01:01:34.240 --> 01:01:37.840]   from, you know, thinking about what do I what do I even want to
[01:01:37.840 --> 01:01:40.680]   predict at the end? What should my pipeline even do? Which
[01:01:40.680 --> 01:01:43.320]   doesn't even necessarily have to map to your business problem?
[01:01:43.320 --> 01:01:47.000]   It could be quite different. How should my application work? What
[01:01:47.000 --> 01:01:49.440]   do I care about? Do I care about accuracy? Do I care about
[01:01:49.440 --> 01:01:52.360]   latency? Do I care about something completely different?
[01:01:52.640 --> 01:01:55.640]   And then which components do I train? Which components can I
[01:01:55.640 --> 01:01:59.160]   train more reliably? Does it make sense to train something
[01:01:59.160 --> 01:02:02.400]   where I predict end to end? What are the advantages or
[01:02:02.400 --> 01:02:06.040]   disadvantages of that? Maybe I should focus on predicting
[01:02:06.040 --> 01:02:10.080]   linguists, very linguistic annotations, all the way through
[01:02:10.080 --> 01:02:13.040]   to how do I annotate it? Oh, shit, I can't annotate it that
[01:02:13.040 --> 01:02:16.800]   way. Nobody can ever agree on what labels apply. It's, let's
[01:02:16.800 --> 01:02:19.760]   do it differently. Oh, there's so much work goes into that. And
[01:02:19.760 --> 01:02:22.840]   then on top of that, you have all the work of Okay, how do I
[01:02:22.840 --> 01:02:25.840]   make my workflow actually reproducible? That's something
[01:02:25.840 --> 01:02:28.240]   where I don't know, also weights and biases comes in where like,
[01:02:28.240 --> 01:02:31.520]   okay, how do I have a workflow that actually scales up where
[01:02:31.520 --> 01:02:36.240]   I'm not just constantly hacking around on things or like, you
[01:02:36.240 --> 01:02:39.120]   know, when I've changed a bunch of stuff, and then I have no
[01:02:39.120 --> 01:02:41.640]   idea what I changed. And then I send it over to my colleague,
[01:02:41.640 --> 01:02:43.920]   and my colleague can't run anything. And it just nothing
[01:02:43.920 --> 01:02:48.400]   works. Anyway, there's so much more to machine learning than
[01:02:48.400 --> 01:02:53.400]   just adding out training a model or thinking about implementations.
[01:02:53.400 --> 01:02:57.120]   And yeah, I do think a lot of this should get more attention.
[01:02:57.120 --> 01:03:01.360]   I totally agree with you in the way to remain respectful of your
[01:03:01.360 --> 01:03:04.040]   time. This would be the last question I'll take and I was
[01:03:04.040 --> 01:03:08.760]   waiting for this one. I think you've already spoken about this
[01:03:08.760 --> 01:03:09.040]   but
[01:03:09.040 --> 01:03:12.840]   they're asking a lot of different different implementations.
[01:03:15.000 --> 01:03:19.120]   Low resource languages. Yeah, I think more generally, if you're
[01:03:19.120 --> 01:03:24.560]   talking about language, you know, language support, I think
[01:03:24.560 --> 01:03:27.560]   the biggest bottleneck is definitely the data and being,
[01:03:27.560 --> 01:03:31.400]   you know, having data sets that you can train models from that
[01:03:31.400 --> 01:03:35.280]   are, you know, that generalize well, and, you know, yeah, low
[01:03:35.280 --> 01:03:38.280]   resource languages have the disadvantage that they have
[01:03:38.280 --> 01:03:41.680]   fewer resources, which is why we're talking about them that
[01:03:41.680 --> 01:03:45.120]   way. And so I do think there's definitely, you know, work that
[01:03:45.120 --> 01:03:48.360]   can be done in like, data sets or in creating, you know,
[01:03:48.360 --> 01:03:51.320]   creating some annotations coming up running some experiments.
[01:03:51.320 --> 01:03:54.320]   That's definitely also a way that, you know, you can help us
[01:03:54.320 --> 01:03:57.480]   but if you find like, and, you know, a data set, something that
[01:03:57.480 --> 01:04:00.200]   could work well, and you've tried it out, and you found the
[01:04:00.200 --> 01:04:03.320]   configuration that works and, you know, something that produces
[01:04:03.320 --> 01:04:05.960]   good results, we'd always be interested in hearing about that.
[01:04:08.080 --> 01:04:13.520]   And then what's the PhD research? So I'm not really for
[01:04:13.520 --> 01:04:18.240]   the question before that I'm not so so skills for NLP, for NLP
[01:04:18.240 --> 01:04:22.800]   engineers in level PhD. It's difficult. So I don't I can't
[01:04:22.800 --> 01:04:25.840]   speak on, you know, I don't know if the question is whether you
[01:04:25.840 --> 01:04:28.800]   want to get into a PhD or whether you have a PhD and are
[01:04:28.800 --> 01:04:32.480]   looking to get more skills. I don't know. I'm not I'm not an
[01:04:32.520 --> 01:04:39.000]   expert on research PhDs. About the internship question, we, we
[01:04:39.000 --> 01:04:42.160]   have done some internships in the past, but we also we, that's
[01:04:42.160 --> 01:04:43.880]   actually something I mentioned earlier, we're very conscious of
[01:04:43.880 --> 01:04:46.640]   the fact that like we want, you know, if we work with someone
[01:04:46.640 --> 01:04:48.960]   who wants to learn things, we want to be able to provide them
[01:04:48.960 --> 01:04:52.800]   an environment where, you know, they can do that. And so, you
[01:04:52.800 --> 01:04:56.760]   know, we don't want to take on too many people at the same time
[01:04:56.760 --> 01:04:59.200]   and not be able to provide them the experience because the idea
[01:04:59.200 --> 01:05:01.840]   of an internship is not that you are cheap labor, the idea of an
[01:05:01.840 --> 01:05:05.320]   internship is that you learn things. And so similarly, it's
[01:05:05.320 --> 01:05:08.840]   like why we do have people working with us who have a
[01:05:08.840 --> 01:05:11.920]   research background. We also we don't have any explicit
[01:05:11.920 --> 01:05:14.800]   positions where you'd be just doing research and publishing
[01:05:14.800 --> 01:05:18.080]   papers. That's, you know, yes, we can obviously something can
[01:05:18.080 --> 01:05:20.320]   lead to a paper, something can lead to interesting research,
[01:05:20.320 --> 01:05:23.800]   but we do want to be very focused on, you know, everything
[01:05:23.800 --> 01:05:26.960]   we do is focused around building, making practical
[01:05:26.960 --> 01:05:29.800]   things for people to use. It's focused on industry use cases.
[01:05:30.680 --> 01:05:34.760]   So, yeah, that's kind of, that's what we do. But if you know, so
[01:05:34.760 --> 01:05:37.640]   it's really, you know, we all people, we have some background
[01:05:37.640 --> 01:05:40.160]   in the technology, and we want to think about how can we take
[01:05:40.160 --> 01:05:43.120]   what's happening in research? How can we take what people are
[01:05:43.120 --> 01:05:46.040]   exploring? What makes sense? What generalizes? What works?
[01:05:46.040 --> 01:05:49.720]   What could also work well for industry use cases? Or how can
[01:05:49.720 --> 01:05:54.320]   we reframe these things and try them out and rebuild them so
[01:05:54.320 --> 01:05:57.640]   that they do solve problems for a lot of people? That's kind of
[01:05:57.640 --> 01:05:58.440]   the core of what we do.
[01:05:59.760 --> 01:06:03.680]   Awesome. With that, I'll quickly remind the audience, they can
[01:06:03.680 --> 01:06:07.360]   find your website in NS.io. They can connect with you on Twitter
[01:06:07.360 --> 01:06:12.720]   @_nsmontani. There's the Explosion GitHub, Explosion
[01:06:12.720 --> 01:06:17.480]   YouTube, and Podigi website. Inez, thank you so much again,
[01:06:17.480 --> 01:06:19.800]   for your time and for answering all of these questions. It's
[01:06:19.800 --> 01:06:22.920]   always such an honor to learn from you. And thank you for all
[01:06:22.920 --> 01:06:23.520]   of your coming.
[01:06:23.520 --> 01:06:25.840]   No, thanks. Thanks for having me. Thanks for all the thanks for
[01:06:25.840 --> 01:06:29.120]   all the great questions. This was a lot of fun. And yeah, if
[01:06:29.120 --> 01:06:30.960]   you have more questions, let me know.
[01:06:30.960 --> 01:06:37.920]   Awesome. Thanks. I'll end the live stream.
[01:06:37.920 --> 01:06:47.920]   [BLANK_AUDIO]

