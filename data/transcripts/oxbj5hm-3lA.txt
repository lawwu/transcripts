
[00:00:00.000 --> 00:00:03.920]   So maybe my hot take here, I don't know how hot it is,
[00:00:03.920 --> 00:00:08.920]   is that like most intelligence is pattern matching.
[00:00:08.920 --> 00:00:12.280]   And you can do a lot of really good pattern matching
[00:00:12.280 --> 00:00:15.420]   if you have a hierarchy of associated memories.
[00:00:15.420 --> 00:00:20.000]   So you start with your very basic associations
[00:00:20.000 --> 00:00:22.720]   between just like objects in the real world,
[00:00:22.720 --> 00:00:26.040]   but you can then chain those
[00:00:26.040 --> 00:00:28.040]   and have more abstract associations,
[00:00:28.040 --> 00:00:30.260]   such as like a wedding ring symbolizes
[00:00:30.260 --> 00:00:33.400]   like so many other associations that are downstream.
[00:00:33.400 --> 00:00:38.400]   And so, and you can even generalize the attention operation
[00:00:38.400 --> 00:00:42.480]   and this associated memory as the MLP layer as well.
[00:00:42.480 --> 00:00:43.960]   And it's in a long-term setting
[00:00:43.960 --> 00:00:47.580]   where you don't have like tokens in your current context.
[00:00:47.580 --> 00:00:50.040]   But I think this is an argument
[00:00:50.040 --> 00:00:54.120]   that like association is all you need.
[00:00:54.120 --> 00:00:56.940]   Okay, so let me ask a stupid question.
[00:00:56.940 --> 00:00:58.840]   So you like reach Sherlock Holmes, right?
[00:00:58.840 --> 00:01:01.320]   And like the guy's incredibly sample efficient.
[00:01:01.320 --> 00:01:03.240]   He'll like see a few observations
[00:01:03.240 --> 00:01:07.920]   and he'll like basically figure out who committed the crime
[00:01:07.920 --> 00:01:09.840]   'cause there's a series of deductive steps
[00:01:09.840 --> 00:01:11.280]   that leads from somebody's tattoo
[00:01:11.280 --> 00:01:14.680]   and what's on the wall to the implications of that.
[00:01:14.680 --> 00:01:18.000]   How does that fit into this picture?
[00:01:18.000 --> 00:01:20.920]   'Cause like crucially what makes them smart
[00:01:20.920 --> 00:01:23.400]   is that there's not like an association,
[00:01:23.400 --> 00:01:25.840]   but there's a sort of deductive connection
[00:01:25.840 --> 00:01:28.000]   between different pieces of information.
[00:01:28.000 --> 00:01:29.860]   Would you just explain it
[00:01:29.860 --> 00:01:32.500]   as that's just like higher level association?
[00:01:32.500 --> 00:01:33.340]   Like, yeah.
[00:01:33.340 --> 00:01:34.260]   - I think so, yeah.
[00:01:34.260 --> 00:01:36.380]   So I think learning these higher level associations
[00:01:36.380 --> 00:01:38.440]   to be able to then map patterns to each other
[00:01:38.440 --> 00:01:40.340]   as kind of like a meta learning.
[00:01:40.340 --> 00:01:41.180]   I think in this case,
[00:01:41.180 --> 00:01:43.300]   he would also just have a really long context length
[00:01:43.300 --> 00:01:46.140]   or a really long working memory, right?
[00:01:46.140 --> 00:01:47.900]   Where he can like have all of these bits
[00:01:47.900 --> 00:01:50.440]   and continuously query them
[00:01:50.440 --> 00:01:52.900]   as he's coming up with whatever theory.
[00:01:52.900 --> 00:01:55.660]   So that the theory is moving through the residual stream
[00:01:55.660 --> 00:02:00.660]   and then his attention heads are querying his context,
[00:02:00.660 --> 00:02:06.180]   but then how he's projecting his query and keys in the space
[00:02:06.180 --> 00:02:11.060]   and how his MLPs are then retrieving like longer term facts
[00:02:11.060 --> 00:02:13.240]   or modifying that information
[00:02:13.240 --> 00:02:15.500]   is allowing him to then in later layers
[00:02:15.500 --> 00:02:17.820]   do even more sophisticated queries
[00:02:17.820 --> 00:02:20.540]   and slowly be able to reason through
[00:02:20.540 --> 00:02:22.340]   and come to a meaningful conclusion.
[00:02:22.340 --> 00:02:24.020]   - That feels right to me in terms of like
[00:02:24.020 --> 00:02:24.980]   looking back in the past,
[00:02:24.980 --> 00:02:25.940]   you're selectively reading
[00:02:25.940 --> 00:02:27.940]   in certain piece of information, comparing them.
[00:02:27.940 --> 00:02:29.300]   Maybe that informs your next step
[00:02:29.300 --> 00:02:31.700]   of like what piece of information you now need to pull in.
[00:02:31.700 --> 00:02:32.860]   And then you build this representation,
[00:02:32.860 --> 00:02:35.100]   which I like progressively looks closer and closer
[00:02:35.100 --> 00:02:37.220]   and closer to like the suspect in your case.
[00:02:37.220 --> 00:02:38.580]   - Yeah, yeah.
[00:02:38.580 --> 00:02:40.500]   - That doesn't feel at all outlandish.
[00:02:40.500 --> 00:02:43.020]   It does bring to mind like a very funny eval to do
[00:02:43.020 --> 00:02:44.380]   would be like a Sherlock Holmes eval.
[00:02:44.380 --> 00:02:45.940]   Let's say you put the entire book into context
[00:02:45.940 --> 00:02:46.980]   and then you have like a sentence,
[00:02:46.980 --> 00:02:49.540]   which is like the suspect is like X,
[00:02:49.540 --> 00:02:51.300]   then you have like a logic probability distribution
[00:02:51.300 --> 00:02:53.220]   over like the different characters in the book.
[00:02:53.220 --> 00:02:55.540]   And then like, as you put more like--
[00:02:55.540 --> 00:02:57.420]   - That would be super cool.
[00:02:57.420 --> 00:03:01.700]   - I wonder if you'd get anything at all, but it'd be cool.
[00:03:01.700 --> 00:03:03.420]   - Sherlock Holmes is probably already in the training data.
[00:03:03.420 --> 00:03:04.260]   - Yeah.
[00:03:04.260 --> 00:03:05.100]   - You gotta get like a mystery novel
[00:03:05.100 --> 00:03:06.300]   that was written in the--
[00:03:06.300 --> 00:03:07.740]   - You can get an hour long to write it.
[00:03:07.740 --> 00:03:10.300]   - Or we can like, we can purposely exclude it, right?
[00:03:10.300 --> 00:03:11.420]   - Oh, you can, how do you--
[00:03:11.420 --> 00:03:13.220]   - Well, you need to scrape any discussion of it
[00:03:13.220 --> 00:03:14.700]   from Reddit or any other thing, right?
[00:03:14.700 --> 00:03:15.860]   - Right, it's hard.
[00:03:15.860 --> 00:03:17.060]   But that's like one of the challenges
[00:03:17.060 --> 00:03:18.900]   that goes into things like long context evals
[00:03:18.900 --> 00:03:20.580]   is to get a good one.
[00:03:20.580 --> 00:03:23.460]   You need to know that it's not in your training data.
[00:03:23.460 --> 00:03:25.060]   You just like put in the effort to exclude it.
[00:03:25.060 --> 00:03:28.340]   - If it's all associations all the way down,
[00:03:28.340 --> 00:03:29.660]   does that mean we should be less worried
[00:03:29.660 --> 00:03:30.940]   about super intelligence?
[00:03:30.940 --> 00:03:32.940]   'Cause there's not this sense in which it's like
[00:03:32.940 --> 00:03:34.900]   Sherlock Holmes plus plus.
[00:03:34.900 --> 00:03:37.900]   It'll still need to just like find these associations,
[00:03:37.900 --> 00:03:40.180]   like humans find associations and like,
[00:03:40.180 --> 00:03:41.020]   you know what I mean?
[00:03:41.020 --> 00:03:43.220]   It's not just like, it sees a frame of the world
[00:03:43.220 --> 00:03:45.940]   and it's like figured out all the laws of physics.
[00:03:45.940 --> 00:03:49.820]   - So for me, 'cause this is a very legitimate response,
[00:03:49.820 --> 00:03:50.660]   right?
[00:03:50.660 --> 00:03:52.820]   It's like, well, artificial general intelligence aren't,
[00:03:52.820 --> 00:03:54.780]   if you say humans are generally intelligent,
[00:03:54.780 --> 00:03:57.580]   then they're no more capable or competent.
[00:03:57.580 --> 00:03:59.580]   I'm just worried that you have that level
[00:03:59.580 --> 00:04:01.980]   of general intelligence in Silicon,
[00:04:01.980 --> 00:04:04.340]   where you can then immediately clone
[00:04:04.340 --> 00:04:06.100]   hundreds of thousands of agents
[00:04:06.100 --> 00:04:07.460]   and they don't need to sleep
[00:04:07.460 --> 00:04:09.820]   and they can have super long context windows
[00:04:09.820 --> 00:04:11.460]   and then they can start recursively improving
[00:04:11.460 --> 00:04:13.540]   and then things get really scary.
[00:04:13.540 --> 00:04:15.340]   So I think to answer your original question,
[00:04:15.340 --> 00:04:16.180]   yes, you're right.
[00:04:16.180 --> 00:04:17.900]   They would still need to learn associations, but--
[00:04:17.980 --> 00:04:20.380]   - But the recursive self-improvement
[00:04:20.380 --> 00:04:22.860]   would still have to be them,
[00:04:22.860 --> 00:04:24.260]   like if intelligence is fundamentally
[00:04:24.260 --> 00:04:25.380]   about these associations,
[00:04:25.380 --> 00:04:26.700]   like the improvement is just them
[00:04:26.700 --> 00:04:27.860]   getting better at association.
[00:04:27.860 --> 00:04:30.260]   There's not like another thing that's happening.
[00:04:30.260 --> 00:04:33.060]   And so then it seems like you might disagree
[00:04:33.060 --> 00:04:34.380]   with the intuition that,
[00:04:34.380 --> 00:04:35.820]   well, they can't be that much more powerful
[00:04:35.820 --> 00:04:36.660]   if they're just doing associations.
[00:04:36.660 --> 00:04:37.860]   - Well, I think then you can get
[00:04:37.860 --> 00:04:40.700]   into really interesting cases of meta-learning.
[00:04:40.700 --> 00:04:42.980]   Like when you play a new video game
[00:04:42.980 --> 00:04:45.460]   or like study a new textbook,
[00:04:45.460 --> 00:04:47.780]   you're bringing a whole bunch of skills to the table
[00:04:47.780 --> 00:04:49.980]   to form those associations much more quickly.
[00:04:49.980 --> 00:04:51.940]   And like, because everything in some way
[00:04:51.940 --> 00:04:53.420]   ties back to the physical worlds,
[00:04:53.420 --> 00:04:55.700]   I think there are like general features
[00:04:55.700 --> 00:04:56.540]   that you can pick up
[00:04:56.540 --> 00:04:58.620]   and then apply in novel circumstances.

