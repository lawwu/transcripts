
[00:00:00.000 --> 00:00:03.360]   [MUSIC PLAYING]
[00:00:03.360 --> 00:00:09.760]   So on the WMB platform outline, you
[00:00:09.760 --> 00:00:14.240]   can see here we've got modular tools from having data sets
[00:00:14.240 --> 00:00:17.520]   that you're preprocessing and preparing through experiment
[00:00:17.520 --> 00:00:21.520]   tracking for developing new models, all the way to plans
[00:00:21.520 --> 00:00:25.560]   to build tools for managing models that are in production
[00:00:25.560 --> 00:00:27.520]   and governance on that end.
[00:00:27.520 --> 00:00:30.640]   And so what this more complete picture gives us
[00:00:30.640 --> 00:00:34.200]   is a set of modular tools that can solve pain
[00:00:34.200 --> 00:00:36.640]   points across the ML lifecycle.
[00:00:36.640 --> 00:00:38.880]   And so within this vision, today I'm
[00:00:38.880 --> 00:00:42.280]   going to highlight the model management component.
[00:00:42.280 --> 00:00:44.200]   And this is our key focus right now
[00:00:44.200 --> 00:00:47.280]   for improving the way people can manage the models that they're
[00:00:47.280 --> 00:00:50.400]   already logging to us, make it easier to coordinate
[00:00:50.400 --> 00:00:54.720]   and collaborate centrally, and ultimately have an automation
[00:00:54.720 --> 00:00:56.960]   cycle where you can automatically retrain a model
[00:00:56.960 --> 00:00:59.440]   when you have new data, or you can automatically
[00:00:59.440 --> 00:01:03.840]   evaluate and test a model once you have a new good candidate
[00:01:03.840 --> 00:01:06.080]   that you might want to replace the current model that's
[00:01:06.080 --> 00:01:07.560]   in production with.
[00:01:07.560 --> 00:01:11.000]   So what are the key pain points that we're hearing?
[00:01:11.000 --> 00:01:12.780]   Well, when we're talking to customers,
[00:01:12.780 --> 00:01:15.320]   a key theme of collaboration reproducibility
[00:01:15.320 --> 00:01:16.800]   consistently comes up.
[00:01:16.800 --> 00:01:20.160]   So issues like siloed projects, it's hard to discover.
[00:01:20.160 --> 00:01:22.280]   What are the existing relevant models
[00:01:22.280 --> 00:01:25.040]   that people have already trained my team?
[00:01:25.040 --> 00:01:27.960]   And how can I pull them down and use them?
[00:01:27.960 --> 00:01:31.120]   And if I can find those models, it's often not clear
[00:01:31.120 --> 00:01:34.640]   how to reproduce and reuse those previous pieces of work
[00:01:34.640 --> 00:01:37.520]   without something like a model card.
[00:01:37.520 --> 00:01:40.320]   We also have issues around governance and lineage
[00:01:40.320 --> 00:01:41.680]   frequently come up.
[00:01:41.680 --> 00:01:45.880]   So problems like having ad hoc handoffs between teams,
[00:01:45.880 --> 00:01:48.560]   where there's no standard process where, say,
[00:01:48.560 --> 00:01:51.400]   a researcher hands off a model to an ML engineer
[00:01:51.400 --> 00:01:53.520]   to then productionize.
[00:01:53.520 --> 00:01:55.960]   Sometimes that's even like sending a Slack message
[00:01:55.960 --> 00:01:59.720]   to say something is in the S3 bucket and go look at it.
[00:01:59.720 --> 00:02:03.280]   Now, that process also potentially
[00:02:03.280 --> 00:02:05.000]   causes unclear lineage.
[00:02:05.000 --> 00:02:07.560]   If there's this ad hoc handoff, then maybe it's
[00:02:07.560 --> 00:02:10.500]   not clear where a model really came from.
[00:02:10.500 --> 00:02:13.800]   What's the exact history, the code, the data set versions,
[00:02:13.800 --> 00:02:15.480]   everything that happened upstream
[00:02:15.480 --> 00:02:18.080]   to produce a given set of model weights?
[00:02:18.080 --> 00:02:21.720]   So governance and lineage was a key theme that we kept hearing.
[00:02:21.720 --> 00:02:25.960]   And over here on the right, I've got this funny screenshot
[00:02:25.960 --> 00:02:30.840]   that I love from an ODSC article, where it's basically
[00:02:30.840 --> 00:02:35.760]   a handoff point that's a finder file system with an iPy
[00:02:35.760 --> 00:02:38.720]   notebook training a model.
[00:02:38.720 --> 00:02:41.320]   We've got a scraper notebook and then
[00:02:41.320 --> 00:02:44.880]   a bunch of different experiments and models loosely in here.
[00:02:44.880 --> 00:02:47.760]   And I've seen before folks basically say, hey,
[00:02:47.760 --> 00:02:48.880]   I got a new job.
[00:02:48.880 --> 00:02:51.560]   Here's my desktop with a bunch of notebooks
[00:02:51.560 --> 00:02:52.520]   and all of my research.
[00:02:52.520 --> 00:02:53.400]   Good luck.
[00:02:53.400 --> 00:02:55.480]   Here's all the stuff that I've been working on.
[00:02:55.480 --> 00:02:57.880]   And without that standardized process
[00:02:57.880 --> 00:02:59.840]   for tracking where something came from
[00:02:59.840 --> 00:03:01.600]   and then organized way of communicating
[00:03:01.600 --> 00:03:03.920]   then how to use it, it ends up becoming
[00:03:03.920 --> 00:03:07.480]   kind of a soup of different models.
[00:03:07.480 --> 00:03:10.280]   So the last theme that we're really hearing here
[00:03:10.280 --> 00:03:12.520]   is automation and CI/CD.
[00:03:12.520 --> 00:03:15.200]   So right now, there's often a lot of manual testing.
[00:03:15.200 --> 00:03:17.320]   Even once you have a model in production
[00:03:17.320 --> 00:03:19.880]   and are iterating on it, there's still--
[00:03:19.880 --> 00:03:22.600]   a lot of team members are manually running ad hoc scripts
[00:03:22.600 --> 00:03:24.520]   to test new candidate models, hopefully
[00:03:24.520 --> 00:03:26.080]   on the latest evaluation data set.
[00:03:26.080 --> 00:03:29.640]   But sometimes it's not clear if the model in production
[00:03:29.640 --> 00:03:31.600]   was evaluated on the same test set
[00:03:31.600 --> 00:03:34.960]   as the model of the new candidate that you're testing.
[00:03:34.960 --> 00:03:39.280]   So issues there around kind of a manual process of testing.
[00:03:39.280 --> 00:03:41.240]   And then ad hoc retraining.
[00:03:41.240 --> 00:03:44.400]   So it's often slow to retrain a model on the latest data,
[00:03:44.400 --> 00:03:46.360]   especially if there isn't this automated system
[00:03:46.360 --> 00:03:49.120]   to test and deploy.
[00:03:49.120 --> 00:03:50.480]   So what are our solutions?
[00:03:50.480 --> 00:03:52.240]   How do we make this process better?
[00:03:52.240 --> 00:03:56.400]   Well, in working on WMD models, we're
[00:03:56.400 --> 00:03:59.040]   focusing on collaboration reproducibility first.
[00:03:59.040 --> 00:04:01.320]   So having a central model repository,
[00:04:01.320 --> 00:04:04.720]   like this little snippet here that you see on the right.
[00:04:04.720 --> 00:04:07.440]   And that's the live demo that I'll get into in a moment.
[00:04:07.440 --> 00:04:09.040]   So having all of the things that you're
[00:04:09.040 --> 00:04:11.160]   logging in one central place.
[00:04:11.160 --> 00:04:15.000]   Also communicating essentially with model cards and metadata.
[00:04:15.000 --> 00:04:18.480]   So having that context of the exact learning rate
[00:04:18.480 --> 00:04:20.080]   that you use when you train the model,
[00:04:20.080 --> 00:04:22.760]   or the exact git commit and the diff patch.
[00:04:22.760 --> 00:04:25.440]   Any of the details that you need to reproduce
[00:04:25.440 --> 00:04:28.080]   where that model came from, try to capture that automatically
[00:04:28.080 --> 00:04:31.720]   and make it easy to access from the central page.
[00:04:31.720 --> 00:04:33.960]   For governance and lineage, we're
[00:04:33.960 --> 00:04:37.160]   making it easier to have the standardized tracked lifecycle.
[00:04:37.160 --> 00:04:39.960]   So adding a couple of lines to your different scripts
[00:04:39.960 --> 00:04:41.480]   in your pipeline means that you get
[00:04:41.480 --> 00:04:45.400]   to track exactly when a model moved between different stages.
[00:04:45.400 --> 00:04:47.880]   And we also capture when you move
[00:04:47.880 --> 00:04:50.600]   a model between staging and production
[00:04:50.600 --> 00:04:53.080]   and who signed off on that.
[00:04:53.080 --> 00:04:56.280]   We're also capturing this complete traceable history.
[00:04:56.280 --> 00:05:00.240]   So if you think about a tweak to a preprocessing step
[00:05:00.240 --> 00:05:04.840]   could actually ultimately really affect the accuracy
[00:05:04.840 --> 00:05:09.240]   of the model downstream, then capturing the changes upstream
[00:05:09.240 --> 00:05:11.640]   to that preprocessing step is also
[00:05:11.640 --> 00:05:15.920]   really critical to making this entire process traceable.
[00:05:15.920 --> 00:05:20.000]   And so with WMB and those couple lines of code tracking upstream,
[00:05:20.000 --> 00:05:21.880]   you can capture that complete history.
[00:05:21.880 --> 00:05:24.220]   And I'll show you what that history looks like in the UI
[00:05:24.220 --> 00:05:25.400]   here in a moment.
[00:05:25.400 --> 00:05:29.520]   And then third, that automation and CI/CD piece,
[00:05:29.520 --> 00:05:32.720]   we're allowing you to have this comprehensive model evaluation
[00:05:32.720 --> 00:05:34.600]   and it happened in a central place.
[00:05:34.600 --> 00:05:35.920]   So you can automatically evaluate
[00:05:35.920 --> 00:05:38.360]   models that are good candidates for the registry.
[00:05:38.360 --> 00:05:41.240]   And my colleague Igor will touch on more of this automation
[00:05:41.240 --> 00:05:45.680]   for eval and retraining in his section right after this.
[00:05:45.680 --> 00:05:48.560]   So now I'll get into a live demo and talk specifically
[00:05:48.560 --> 00:05:50.720]   about the collaboration reproducibility,
[00:05:50.720 --> 00:05:52.920]   having that single pane of glass for all of your model
[00:05:52.920 --> 00:05:55.400]   development, and then governance and lineage,
[00:05:55.400 --> 00:05:58.360]   so how you can keep track of when people move things
[00:05:58.360 --> 00:06:01.280]   between different stages.
[00:06:01.280 --> 00:06:04.880]   So here in my next tab, this is what the model registry
[00:06:04.880 --> 00:06:05.720]   looks like.
[00:06:05.720 --> 00:06:08.760]   What you're seeing here on the right-hand side
[00:06:08.760 --> 00:06:11.720]   in this sidebar are different registered models.
[00:06:11.720 --> 00:06:15.000]   So these are different tasks, like detecting a stoplight
[00:06:15.000 --> 00:06:17.280]   or a model for mapping.
[00:06:17.280 --> 00:06:19.960]   In this case, we're looking at nature classification.
[00:06:19.960 --> 00:06:23.280]   And here we can see each different version of the model
[00:06:23.280 --> 00:06:25.280]   that's been added in here.
[00:06:25.280 --> 00:06:27.400]   So how this works is different researchers
[00:06:27.400 --> 00:06:28.840]   are iterating on this problem.
[00:06:28.840 --> 00:06:31.320]   They've got maybe dozens and dozens of model checkpoints
[00:06:31.320 --> 00:06:32.640]   in different projects.
[00:06:32.640 --> 00:06:34.120]   But when they have a good candidate,
[00:06:34.120 --> 00:06:35.520]   they can link it in here.
[00:06:35.520 --> 00:06:37.480]   And it'll show up here in this table.
[00:06:37.480 --> 00:06:40.360]   So a new candidate is added to the registered model.
[00:06:40.360 --> 00:06:42.680]   And once it's linked, I can then dive
[00:06:42.680 --> 00:06:47.160]   into any of these individual models and see more details.
[00:06:47.160 --> 00:06:48.520]   Now here, my colleague has actually
[00:06:48.520 --> 00:06:51.600]   filled in this really clear model card.
[00:06:51.600 --> 00:06:53.960]   So I can see what the expected inputs and outputs
[00:06:53.960 --> 00:06:56.320]   are for this given model.
[00:06:56.320 --> 00:06:59.840]   And I can also get back and see exactly what the run was
[00:06:59.840 --> 00:07:01.320]   that produced this model.
[00:07:01.320 --> 00:07:04.480]   So if I want to see things like, what were the training curves?
[00:07:04.480 --> 00:07:09.200]   Or what were the layers of the model that you were training?
[00:07:09.200 --> 00:07:10.480]   I can see exactly that.
[00:07:10.480 --> 00:07:12.760]   I can also get back to the exact version of the code.
[00:07:12.760 --> 00:07:14.320]   I can see when it was trained.
[00:07:14.320 --> 00:07:17.240]   I can see the Python version, the configs,
[00:07:17.240 --> 00:07:20.040]   so any hyperparameters.
[00:07:20.040 --> 00:07:22.120]   All of that is accessible just with one
[00:07:22.120 --> 00:07:24.640]   click from the model registry.
[00:07:24.640 --> 00:07:27.240]   Now I can also see a summary of the metadata
[00:07:27.240 --> 00:07:29.400]   here right on the metadata tab.
[00:07:29.400 --> 00:07:32.480]   So I can look at those hyperparameters.
[00:07:32.480 --> 00:07:34.640]   And if I sent this to someone else,
[00:07:34.640 --> 00:07:37.360]   they could pull up this snippet of code
[00:07:37.360 --> 00:07:39.880]   and pull down my model and start to use it
[00:07:39.880 --> 00:07:43.480]   with a simple chunk for usage.
[00:07:43.480 --> 00:07:47.560]   Now for files, you can save anything into a model.
[00:07:47.560 --> 00:07:51.280]   We aren't going to restrict what you're
[00:07:51.280 --> 00:07:55.120]   allowed to save into this versioned folder of data.
[00:07:55.120 --> 00:07:57.760]   So you could imagine having your H5 file in here.
[00:07:57.760 --> 00:08:01.080]   But you could also imagine having additional files that
[00:08:01.080 --> 00:08:02.920]   are maybe helper scripts that someone who's
[00:08:02.920 --> 00:08:05.120]   using the model might need.
[00:08:05.120 --> 00:08:10.280]   So we try to be flexible and agnostic in this regard
[00:08:10.280 --> 00:08:12.640]   so you can decide how you're managing the model
[00:08:12.640 --> 00:08:14.240]   that you've saved here.
[00:08:14.240 --> 00:08:16.640]   And then lineage, this is the exciting part
[00:08:16.640 --> 00:08:19.320]   that I was really looking forward to getting to.
[00:08:19.320 --> 00:08:22.200]   So we're looking right now at a model artifact
[00:08:22.200 --> 00:08:24.240]   that was saved to WMB and then linked
[00:08:24.240 --> 00:08:26.080]   into this registered model.
[00:08:26.080 --> 00:08:27.520]   Now where did it come from?
[00:08:27.520 --> 00:08:29.800]   I can step one step back in the DAC
[00:08:29.800 --> 00:08:32.720]   and see, OK, that's the run that we already opened.
[00:08:32.720 --> 00:08:34.800]   If I open that again, you'll recognize
[00:08:34.800 --> 00:08:37.440]   this is that same run that we were just looking
[00:08:37.440 --> 00:08:40.160]   at for Keras model training.
[00:08:40.160 --> 00:08:42.720]   Now what did that run use?
[00:08:42.720 --> 00:08:43.840]   I can open that up.
[00:08:43.840 --> 00:08:48.520]   And that's the training data that was pulled in to train on.
[00:08:48.520 --> 00:08:51.480]   And I can see exactly the version of data
[00:08:51.480 --> 00:08:53.560]   that was used to train that model.
[00:08:53.560 --> 00:08:57.960]   In this case, I can even get in and see the exact data itself.
[00:08:57.960 --> 00:09:01.840]   And if you zoom out, you can see all of the other steps that
[00:09:01.840 --> 00:09:04.600]   happened upstream of that ultimately the model
[00:09:04.600 --> 00:09:05.680]   that we're looking at.
[00:09:05.680 --> 00:09:08.080]   So I can get back and see the full history of where
[00:09:08.080 --> 00:09:10.640]   this thing came from.
[00:09:10.640 --> 00:09:16.160]   Now inside this specific registered model,
[00:09:16.160 --> 00:09:18.280]   we also have different stages.
[00:09:18.280 --> 00:09:20.160]   So we have production.
[00:09:20.160 --> 00:09:21.200]   We have latest.
[00:09:21.200 --> 00:09:23.880]   And you saw that here in the versions table.
[00:09:23.880 --> 00:09:28.080]   So how do I identify who moved this model into production?
[00:09:28.080 --> 00:09:30.560]   Well, here in the action history,
[00:09:30.560 --> 00:09:32.720]   you can see it was me 15 minutes ago.
[00:09:32.720 --> 00:09:34.840]   I added the alias production.
[00:09:34.840 --> 00:09:38.600]   So I moved this version of the model into this state.
[00:09:38.600 --> 00:09:42.160]   And now if I wanted to add a new model in here,
[00:09:42.160 --> 00:09:45.080]   I could also track that on this page.
[00:09:45.080 --> 00:09:46.000]   So let's try that now.
[00:09:46.000 --> 00:09:50.160]   Let's imagine we have a project that we're working on,
[00:09:50.160 --> 00:09:53.480]   like, for example, this model registry end-to-end demo
[00:09:53.480 --> 00:09:54.560]   project.
[00:09:54.560 --> 00:09:56.840]   And here you'll recognize this project page
[00:09:56.840 --> 00:10:00.120]   if you're tracking experiments with WMB.
[00:10:00.120 --> 00:10:02.000]   So here on the left side, we've got
[00:10:02.000 --> 00:10:05.040]   runs that are tracking different training jobs,
[00:10:05.040 --> 00:10:07.480]   different instances of training a model.
[00:10:07.480 --> 00:10:10.240]   And we've got two different runs, green and purple.
[00:10:10.240 --> 00:10:12.720]   And it looks like over here on the accuracy chart,
[00:10:12.720 --> 00:10:16.680]   green at the end of training is performing just slightly more
[00:10:16.680 --> 00:10:18.480]   poorly than purple.
[00:10:18.480 --> 00:10:24.040]   So I'm going to click into this run here and see, OK,
[00:10:24.040 --> 00:10:24.880]   it looks great.
[00:10:24.880 --> 00:10:29.600]   I can look at any of the files, the model architecture.
[00:10:29.600 --> 00:10:33.200]   And I can pull up the artifacts and see the models
[00:10:33.200 --> 00:10:34.760]   that it produced.
[00:10:34.760 --> 00:10:37.480]   And I know that that last step is the one
[00:10:37.480 --> 00:10:40.800]   that I want to use to then link into the registry.
[00:10:40.800 --> 00:10:43.880]   So I'll go back to my All Versions table.
[00:10:43.880 --> 00:10:47.960]   We can see that, OK, latest was version 5 just now.
[00:10:47.960 --> 00:10:53.440]   And here I can then pick this latest artifact
[00:10:53.440 --> 00:10:55.800]   and pull it in and link it.
[00:10:55.800 --> 00:10:57.160]   And apologies for my internet.
[00:10:57.160 --> 00:10:59.400]   It looks like that's taking a moment.
[00:10:59.400 --> 00:11:02.400]   So ultimately, what the Model Registry is providing us here
[00:11:02.400 --> 00:11:05.800]   is a way to capture across the different projects
[00:11:05.800 --> 00:11:09.280]   that you're working on the central list of the best,
[00:11:09.280 --> 00:11:11.640]   most useful versions of models.
[00:11:11.640 --> 00:11:15.640]   And what that will give us is an organized view
[00:11:15.640 --> 00:11:18.360]   that anyone else, even if they haven't been involved
[00:11:18.360 --> 00:11:20.680]   in the process of model training and development,
[00:11:20.680 --> 00:11:23.760]   they'll be able to come in and identify,
[00:11:23.760 --> 00:11:25.480]   here's the model that's in production.
[00:11:25.480 --> 00:11:26.880]   Here's the new candidate.
[00:11:26.880 --> 00:11:30.360]   And this is what I need to know about it in that overview,
[00:11:30.360 --> 00:11:32.000]   in that model card.
[00:11:32.000 --> 00:11:35.920]   And so what that's giving us is that communication,
[00:11:35.920 --> 00:11:38.240]   collaboration in the central hub,
[00:11:38.240 --> 00:11:42.600]   and then that ability to trace exactly what's happening,
[00:11:42.600 --> 00:11:46.120]   both from the metadata and the lineage that we touched on
[00:11:46.120 --> 00:11:49.080]   and that action history of every step of things
[00:11:49.080 --> 00:11:50.320]   that have changed.
[00:11:50.320 --> 00:11:53.240]   And so ultimately, what this tool is about
[00:11:53.240 --> 00:11:58.080]   is giving you that central place to discuss models as a team
[00:11:58.080 --> 00:12:01.040]   and to hand off models between different stages
[00:12:01.040 --> 00:12:02.840]   of your pipeline.
[00:12:02.840 --> 00:12:06.680]   So now we've talked about those core collaboration
[00:12:06.680 --> 00:12:08.360]   and governance pieces.
[00:12:08.360 --> 00:12:10.960]   Next up is automation and CI/CD.
[00:12:10.960 --> 00:12:13.120]   And this is a really exciting next step.
[00:12:13.120 --> 00:12:15.760]   How do we take those actions that you saw in the model
[00:12:15.760 --> 00:12:19.800]   registry and move models through that lifecycle?
[00:12:19.800 --> 00:12:23.320]   So that's, for example, I have a new model
[00:12:23.320 --> 00:12:24.600]   that is in production.
[00:12:24.600 --> 00:12:28.040]   And now we've got fresh data coming in from the field.
[00:12:28.040 --> 00:12:30.560]   How do I retrain that model on the latest data
[00:12:30.560 --> 00:12:33.400]   and trigger that automatically when data is available?
[00:12:33.400 --> 00:12:36.520]   Or automatically testing the latest candidate model.
[00:12:36.520 --> 00:12:39.160]   So say a new model is linked in there to the registry.
[00:12:39.160 --> 00:12:43.000]   How do you automatically kick off a test suite?
[00:12:43.000 --> 00:12:46.480]   Or now that model's passed all these tests, it's looking good.
[00:12:46.480 --> 00:12:49.040]   How do we deploy that model to production?
[00:12:49.040 --> 00:12:51.360]   So I'll hand it off to Igor now to talk more
[00:12:51.360 --> 00:12:53.400]   about automation and CI/CD.
[00:12:53.400 --> 00:12:54.960]   Thanks for the introduction.
[00:12:54.960 --> 00:12:57.800]   So I'm happy to talk about our launch product line.
[00:12:57.800 --> 00:12:59.840]   Weights and Biases has always had the vision
[00:12:59.840 --> 00:13:02.600]   to integrate well with your existing infrastructure.
[00:13:02.600 --> 00:13:04.040]   And launch actually gets its name
[00:13:04.040 --> 00:13:08.800]   for being able to launch jobs into that infrastructure.
[00:13:08.800 --> 00:13:11.200]   In terms of the goals of the product line,
[00:13:11.200 --> 00:13:14.360]   Weights and Biases cares deeply about reproducibility.
[00:13:14.360 --> 00:13:15.760]   And so we want to make sure we're
[00:13:15.760 --> 00:13:17.800]   compatible with containerized workflows
[00:13:17.800 --> 00:13:21.520]   and work well with tools such as Docker and Kubernetes.
[00:13:21.520 --> 00:13:24.160]   We want to connect the ML practitioner to compute.
[00:13:24.160 --> 00:13:26.480]   So envision you have a hyperparameter tuning job
[00:13:26.480 --> 00:13:27.480]   that you want to run.
[00:13:27.480 --> 00:13:29.440]   You can then simply send that to a cluster,
[00:13:29.440 --> 00:13:33.000]   parallelize the work, and have the job complete much faster.
[00:13:33.000 --> 00:13:34.880]   And we want to make sure we abstract the way
[00:13:34.880 --> 00:13:37.680]   the complexity of actually using that infrastructure
[00:13:37.680 --> 00:13:40.920]   and making sure that it's used in getting an ROI
[00:13:40.920 --> 00:13:44.400]   by having one central UI within Weights and Biases
[00:13:44.400 --> 00:13:46.080]   that you can launch jobs from.
[00:13:46.080 --> 00:13:49.120]   It's a UI that you're already familiar with your other tasks.
[00:13:49.120 --> 00:13:51.600]   And this notion of building a bridge between the ML
[00:13:51.600 --> 00:13:53.400]   practitioner and your organization
[00:13:53.400 --> 00:13:56.040]   to other personas such as the ML Ops engineer
[00:13:56.040 --> 00:13:58.160]   and working with your IT team.
[00:13:58.160 --> 00:14:00.360]   We know that the practitioner really
[00:14:00.360 --> 00:14:03.480]   wants to focus on building models, running experiments.
[00:14:03.480 --> 00:14:06.720]   They don't necessarily want to deal with configuration files,
[00:14:06.720 --> 00:14:09.600]   helm charts, and all the specifics around Kubernetes.
[00:14:09.600 --> 00:14:15.680]   So having a seamless way to connect to the infrastructure.
[00:14:15.680 --> 00:14:18.680]   So this is a visual representation of the workflow.
[00:14:18.680 --> 00:14:23.000]   So an ML practitioner may have a piece of local code
[00:14:23.000 --> 00:14:26.120]   that they're iterating on and allowing them to then easily
[00:14:26.120 --> 00:14:29.760]   connect to a cluster, get access to more resources,
[00:14:29.760 --> 00:14:33.560]   and really power up their workflow.
[00:14:33.560 --> 00:14:36.000]   So let's actually dive into a demo
[00:14:36.000 --> 00:14:38.840]   and being able to create and launch a job through the Weights
[00:14:38.840 --> 00:14:40.280]   and Biases UI.
[00:14:40.280 --> 00:14:41.680]   Weights and Biases is flexible.
[00:14:41.680 --> 00:14:44.880]   So there's actually a variety of ways that we can create a job.
[00:14:44.880 --> 00:14:47.080]   You can run some samples.
[00:14:47.080 --> 00:14:49.200]   Code have one line that corresponds
[00:14:49.200 --> 00:14:50.560]   to Weights and Biases.
[00:14:50.560 --> 00:14:52.840]   You can launch a job via a Docker image.
[00:14:52.840 --> 00:14:56.720]   Weights and Biases accepts a Git URL.
[00:14:56.720 --> 00:15:00.000]   Or you can launch a job directly within the user interface.
[00:15:00.000 --> 00:15:02.720]   And I'll show you how to do that as well.
[00:15:02.720 --> 00:15:06.920]   So if we go to Weights and Biases UI,
[00:15:06.920 --> 00:15:10.680]   you'll see a new tab here that appears that's called Launch.
[00:15:10.680 --> 00:15:13.040]   And this is a job queue that now anybody
[00:15:13.040 --> 00:15:16.880]   within your organizations can send jobs to to execute.
[00:15:16.880 --> 00:15:19.240]   Now, there's a one-time setup that's
[00:15:19.240 --> 00:15:21.080]   required to connect Weights and Biases
[00:15:21.080 --> 00:15:23.200]   Launch to your infrastructure.
[00:15:23.200 --> 00:15:26.520]   In this case, my ML ops team has already made that connection.
[00:15:26.520 --> 00:15:28.480]   And this queue can then be subsequently
[00:15:28.480 --> 00:15:33.240]   reused by anybody within the organization.
[00:15:33.240 --> 00:15:36.320]   Now, let me show you how to create a job
[00:15:36.320 --> 00:15:37.680]   via both code and the UI.
[00:15:37.680 --> 00:15:41.080]   So let's start with code.
[00:15:41.080 --> 00:15:44.920]   So this is a simple training script that I have.
[00:15:44.920 --> 00:15:47.520]   This training a model on the Fashion Amnesty dataset
[00:15:47.520 --> 00:15:50.320]   detecting the type of clothing.
[00:15:50.320 --> 00:15:53.880]   I'm using Weights and Biases to log some of the loss metrics
[00:15:53.880 --> 00:15:55.800]   as well as some of the sample training images
[00:15:55.800 --> 00:15:58.200]   from the training dataset.
[00:15:58.200 --> 00:16:02.680]   Now, with this one line of code, Weights and Biases
[00:16:02.680 --> 00:16:05.120]   will automatically capture all the information that's
[00:16:05.120 --> 00:16:07.160]   needed to reproduce this job.
[00:16:07.160 --> 00:16:10.640]   And I'll show you that within the UI.
[00:16:10.640 --> 00:16:13.480]   I'm also passing in some configuration parameters
[00:16:13.480 --> 00:16:14.800]   into the model.
[00:16:14.800 --> 00:16:16.640]   With Weights and Biases Launch, we'll
[00:16:16.640 --> 00:16:18.520]   actually expose these via the UI.
[00:16:18.520 --> 00:16:21.760]   And these will be knobs that you can tune and be able to launch
[00:16:21.760 --> 00:16:24.280]   new experiments from.
[00:16:24.280 --> 00:16:26.120]   So let's run this piece of code.
[00:16:27.120 --> 00:16:33.120]   And if I copy and paste this, you
[00:16:33.120 --> 00:16:38.120]   can see that the job is now executing.
[00:16:38.120 --> 00:16:41.120]   All the results are now being streamed in real time
[00:16:41.120 --> 00:16:42.120]   into the UI.
[00:16:42.120 --> 00:16:46.640]   Now, notice Weights and Biases, again,
[00:16:46.640 --> 00:16:49.240]   automatically going to capture that the hardware the job is
[00:16:49.240 --> 00:16:50.440]   running on.
[00:16:50.440 --> 00:16:52.280]   They get information.
[00:16:52.280 --> 00:16:54.120]   We have a lot of information.
[00:16:54.120 --> 00:16:57.400]   Get information, we capture the code
[00:16:57.400 --> 00:17:02.120]   that the job is running on.
[00:17:02.120 --> 00:17:09.200]   If we capture all of the requirements and dependencies,
[00:17:09.200 --> 00:17:12.800]   if you launch this job with a Docker image,
[00:17:12.800 --> 00:17:15.400]   we can capture the Docker information as well.
[00:17:15.400 --> 00:17:19.800]   Again, the goal is full reproducibility.
[00:17:19.800 --> 00:17:23.200]   So let's actually now add this job to the queue
[00:17:23.200 --> 00:17:24.920]   and execute it.
[00:17:24.920 --> 00:17:27.600]   So if we go to Weights and Biases Launch
[00:17:27.600 --> 00:17:30.920]   and we add a job to the queue, again,
[00:17:30.920 --> 00:17:33.480]   we accept jobs from a variety of these sources.
[00:17:33.480 --> 00:17:38.320]   But let's clone the existing run that I have just completed.
[00:17:38.320 --> 00:17:40.800]   Here, what you see is a parameterized version
[00:17:40.800 --> 00:17:41.640]   of that job.
[00:17:41.640 --> 00:17:44.040]   And remember these knobs that I mentioned
[00:17:44.040 --> 00:17:45.360]   via the training script?
[00:17:45.360 --> 00:17:48.080]   I can now overwrite any of these via the UI.
[00:17:48.080 --> 00:17:49.920]   So if I wanted to change my learning rate
[00:17:49.920 --> 00:17:53.440]   or change the amount of epochs that the model is trained upon,
[00:17:53.440 --> 00:17:56.960]   I can do that directly from the UI.
[00:17:56.960 --> 00:18:00.880]   If I want to then send this job to my cluster
[00:18:00.880 --> 00:18:02.520]   or the infrastructure of my choice,
[00:18:02.520 --> 00:18:05.880]   I can then now select that as a resource of where
[00:18:05.880 --> 00:18:08.240]   the job is going to be run.
[00:18:08.240 --> 00:18:14.440]   So if I push this run, the job has now been enqueued.
[00:18:14.440 --> 00:18:17.760]   And I have an agent set up to automatically listen
[00:18:17.760 --> 00:18:19.080]   for this run.
[00:18:19.080 --> 00:18:23.880]   And it will pick up upon and start rebuilding the run
[00:18:23.880 --> 00:18:26.480]   and start executing.
[00:18:26.480 --> 00:18:28.560]   And notice, everything I'm doing,
[00:18:28.560 --> 00:18:31.400]   I'm just-- I have a sample piece of training code.
[00:18:31.400 --> 00:18:33.360]   I'm just clicking buttons via the UI.
[00:18:33.360 --> 00:18:36.080]   I don't necessarily need to deal with YAML files.
[00:18:36.080 --> 00:18:37.680]   And it allows me as a practitioner
[00:18:37.680 --> 00:18:41.000]   to much more effectively collaborate with my MLOps team
[00:18:41.000 --> 00:18:42.880]   and with my IT team.
[00:18:42.880 --> 00:18:43.920]   OK, boom.
[00:18:43.920 --> 00:18:46.760]   You see that the job has already started.
[00:18:46.760 --> 00:18:48.960]   And then the new job is now running.
[00:18:48.960 --> 00:18:52.360]   And the results will now be streamed in real time
[00:18:52.360 --> 00:18:53.560]   via the UI as well.
[00:18:53.560 --> 00:18:57.560]   Perfect.
[00:18:57.560 --> 00:19:02.440]   So this job is now running.
[00:19:02.440 --> 00:19:08.240]   So zooming out, again, as Phil and Kerry mentioned,
[00:19:08.240 --> 00:19:10.240]   the vision of Weights and Biases has always
[00:19:10.240 --> 00:19:12.040]   been an end-to-end platform.
[00:19:12.040 --> 00:19:15.520]   And Launch is a platform layer and a generic job execution
[00:19:15.520 --> 00:19:18.360]   layer that allow us to expand to other parts
[00:19:18.360 --> 00:19:21.000]   of the pipeline.
[00:19:21.000 --> 00:19:23.480]   In a subsequent release, we have actually
[00:19:23.480 --> 00:19:25.320]   just released a functionality that's
[00:19:25.320 --> 00:19:28.600]   known as triggers, where you can trigger these jobs contingent
[00:19:28.600 --> 00:19:29.880]   upon an event.
[00:19:29.880 --> 00:19:33.840]   So envision you log on your data set within Weights and Biases.
[00:19:33.840 --> 00:19:36.200]   You can then automatically trigger a model training
[00:19:36.200 --> 00:19:38.840]   job to run, a model evaluation job to run.
[00:19:38.840 --> 00:19:40.920]   You can trigger a suite of tests that you
[00:19:40.920 --> 00:19:42.760]   can test against all of the metrics
[00:19:42.760 --> 00:19:45.760]   that Weights and Biases captures.
[00:19:45.760 --> 00:19:49.440]   And so Launch is a product line.
[00:19:49.440 --> 00:19:50.480]   This is a sneak peek.
[00:19:50.480 --> 00:19:51.960]   It's a private preview product.
[00:19:51.960 --> 00:19:53.800]   But it's undergoing rapid evolution.
[00:19:53.800 --> 00:19:55.960]   So stay tuned for the subsequent releases.
[00:19:55.960 --> 00:19:59.320]   [MUSIC PLAYING]
[00:19:59.320 --> 00:20:02.360]   [INAUDIBLE]
[00:20:02.360 --> 00:20:05.720]   [MUSIC PLAYING]
[00:20:05.720 --> 00:20:09.080]   [MUSIC PLAYING]
[00:20:10.040 --> 00:20:13.400]   [MUSIC PLAYING]
[00:20:13.400 --> 00:20:15.980]   (upbeat music)

