
[00:00:00.000 --> 00:00:05.000]   paranoia that like, chases me down every night before I sleep
[00:00:05.000 --> 00:00:08.040]   where I like, remember one stupid mistake that I did.
[00:00:08.040 --> 00:00:13.120]   You realize that you only got like the last sentence of the
[00:00:13.120 --> 00:00:14.200]   recording or something.
[00:00:14.200 --> 00:00:14.520]   Yeah.
[00:00:14.520 --> 00:00:22.880]   Awesome. I'm just refreshing everywhere to make sure we live.
[00:00:29.560 --> 00:00:32.080]   Amazing. I can hear a double echo, which means we're live.
[00:00:32.080 --> 00:00:37.040]   Terrifying experience, you know, honestly, like, you want to hear
[00:00:37.040 --> 00:00:40.440]   the echo. There's this 30 second delay and like, I'm freaking out
[00:00:40.440 --> 00:00:41.600]   because what's what's going on?
[00:00:41.600 --> 00:00:45.720]   I just got the LinkedIn notification. So don't worry.
[00:00:45.720 --> 00:00:49.560]   That's good to know. Awesome. I'll start by introducing Mark
[00:00:49.560 --> 00:00:53.120]   and the series. And then we'll start by interviewing Mark about
[00:00:53.120 --> 00:00:56.400]   his journey. So welcome back everyone to who's who I was
[00:00:56.400 --> 00:00:58.800]   joining us live and thank you for anyone who's watching the
[00:00:58.800 --> 00:01:04.280]   recording. I'm super excited to be talking to Mark today. Mark
[00:01:04.280 --> 00:01:09.280]   solo silver medal, solo silver medal on the 33rd position in
[00:01:09.280 --> 00:01:12.400]   the pet finder competition and we'll be learning about his
[00:01:12.400 --> 00:01:18.120]   solution and his journey like always. This is the 8th Chai Time
[00:01:18.120 --> 00:01:21.440]   Kaggle walkthrough. So if you're new here, we've been doing this
[00:01:21.440 --> 00:01:25.760]   for a while and my goal is to always try to understand the
[00:01:25.760 --> 00:01:29.480]   brains of awesome Kagglers and understand how did they get to
[00:01:29.480 --> 00:01:32.240]   the top of the leaderboard? How did they get to where they are
[00:01:32.240 --> 00:01:35.560]   in the journey? So if you want, you can also check out the other
[00:01:35.560 --> 00:01:39.600]   videos in the playlist. Although I really encourage you to Kaggle
[00:01:39.600 --> 00:01:44.280]   more than just watching the videos instead. So why am I
[00:01:44.280 --> 00:01:47.040]   doing this? What's the reason behind this? This is part of
[00:01:47.040 --> 00:01:50.920]   Chai Time Data Science 2.0. Chai Time Data Science was one of
[00:01:50.920 --> 00:01:53.800]   the top streamed Kaggle podcast where I used to interview my
[00:01:53.800 --> 00:01:57.280]   Kaggle heroes. And this is a step further in that where I
[00:01:57.280 --> 00:02:00.600]   also try to understand the solutions. We also try to
[00:02:00.600 --> 00:02:04.040]   understand machine learning frameworks and deep learning
[00:02:04.040 --> 00:02:10.520]   papers. So it's a more holistic approach in my opinion. And I'm
[00:02:10.520 --> 00:02:14.080]   interviewing Mark today. Mark has just achieved the title of
[00:02:14.080 --> 00:02:18.680]   Kaggle competition master. He's a senior data scientist at 84.51
[00:02:18.680 --> 00:02:20.280]   degrees. Am I saying that correctly?
[00:02:20.760 --> 00:02:23.840]   It's 84.51. But I could never blame anyone for getting that
[00:02:23.840 --> 00:02:24.120]   wrong.
[00:02:24.120 --> 00:02:29.400]   And you can find him on Twitter at his first name concatenated
[00:02:29.400 --> 00:02:31.440]   with the second name, I would highly encourage everyone to
[00:02:31.440 --> 00:02:34.000]   check his Twitter out. He posts very insightful stuff there.
[00:02:34.000 --> 00:02:37.240]   Mark, thanks again for joining us today. And I'm super excited
[00:02:37.240 --> 00:02:38.480]   to be talking to you.
[00:02:38.480 --> 00:02:43.440]   Yeah, yeah, it's an honour to be here. I love the show so much.
[00:02:43.440 --> 00:02:45.440]   And I'm so excited to get into it.
[00:02:45.440 --> 00:02:50.360]   So I want to start by asking questions around your background
[00:02:50.400 --> 00:02:54.760]   and I saw that you this is not financial advice for anyone, but
[00:02:54.760 --> 00:02:58.400]   are you buying the dip? This is in the third week of January for
[00:02:58.400 --> 00:02:59.560]   those who are watching.
[00:02:59.560 --> 00:03:03.640]   Dollar cost average. That's what I'll say. Just keep buying.
[00:03:03.640 --> 00:03:04.640]   Yeah.
[00:03:04.640 --> 00:03:08.760]   Mark has a background in the blockchain world as well. That's
[00:03:08.760 --> 00:03:14.120]   why that question came up. But I'm, can you please share how
[00:03:14.120 --> 00:03:17.360]   did you get started in the world of data science? Because I saw
[00:03:17.360 --> 00:03:20.600]   that you come from a mechanical engineering background and then
[00:03:20.600 --> 00:03:23.680]   you made the transition was the transition hard for you? Because
[00:03:23.680 --> 00:03:27.040]   I really believe being from a computer science background
[00:03:27.040 --> 00:03:30.720]   hasn't helped me at all. I might be really stupid. But how did
[00:03:30.720 --> 00:03:33.600]   you make the transition? How did you go about learning all the
[00:03:33.600 --> 00:03:34.080]   things?
[00:03:34.080 --> 00:03:38.000]   Yeah, yeah. So like you said, I started off in mechanical
[00:03:38.000 --> 00:03:43.120]   engineering, went to University of Michigan. And while I was
[00:03:43.120 --> 00:03:47.720]   there, they they have you take this freshman year computer
[00:03:47.720 --> 00:03:51.400]   science class where you basically go through, you know,
[00:03:51.400 --> 00:03:56.880]   c++ and MATLAB. And, you know, I appreciated it, I appreciated
[00:03:56.880 --> 00:04:01.640]   how powerful it was, but I didn't really like it. You know,
[00:04:01.640 --> 00:04:03.680]   I was doing all this programming, I felt like for the
[00:04:03.680 --> 00:04:05.760]   sake of programming, and I didn't really feel like I was
[00:04:05.760 --> 00:04:08.520]   accomplishing anything. So kind of threw it to the side, I
[00:04:08.520 --> 00:04:13.520]   wasn't really that interested. And fast forward to a couple
[00:04:13.520 --> 00:04:18.560]   years from that I was doing a co op in crash testing with
[00:04:18.560 --> 00:04:23.640]   Toyota. So I was basically entirely on the mechanical
[00:04:23.640 --> 00:04:27.000]   engineering side of things, but I was running these huge
[00:04:27.000 --> 00:04:33.720]   simulations to see how our cars would do in in a crash test. And
[00:04:33.720 --> 00:04:38.320]   they had a Python script that would take those simulations and
[00:04:38.840 --> 00:04:42.120]   batch them up and send them off to a server. And they had this
[00:04:42.120 --> 00:04:46.080]   weird, esoteric error, I think it was something it ended up
[00:04:46.080 --> 00:04:49.280]   something to do with file paths or something, but they didn't
[00:04:49.280 --> 00:04:52.520]   have any time to look into it. And so they asked me if I could
[00:04:52.520 --> 00:04:55.280]   do it. And I said, Well, I have, you know, a semester of
[00:04:55.280 --> 00:04:58.280]   programming experience years ago, so maybe no, but no
[00:04:58.280 --> 00:05:03.480]   promises. So I looked into it, and I ended up figuring it out.
[00:05:03.480 --> 00:05:07.840]   Fortunately, it was it was easy enough that that my, you know,
[00:05:07.880 --> 00:05:12.280]   lack of experience could still figure it out. But I kind of
[00:05:12.280 --> 00:05:15.960]   got hooked because I realized that, you know, I loved the sort
[00:05:15.960 --> 00:05:19.560]   of the problem solving method I had to go through to figure it
[00:05:19.560 --> 00:05:23.640]   out. And so in my free time, I you know, I started taking
[00:05:23.640 --> 00:05:27.480]   Python courses, whatever I could find code Academy, just whatever
[00:05:27.480 --> 00:05:32.160]   I could find on the internet. And I liked it so much. I saw
[00:05:32.160 --> 00:05:36.520]   that this class the next semester, that that was about
[00:05:36.520 --> 00:05:40.720]   self driving cars. And University of Michigan is, is
[00:05:40.720 --> 00:05:44.480]   very close to the big three auto manufacturers in in the US. And
[00:05:44.480 --> 00:05:48.320]   so they had, you know, there, it was a very inspired course, it
[00:05:48.320 --> 00:05:53.800]   was sort of a progenitor for courses like that. And little
[00:05:53.800 --> 00:05:56.680]   did I know, it was a graduate level course that I somehow got
[00:05:56.680 --> 00:06:00.280]   into, because I was mechanical engineering student, I guess
[00:06:00.280 --> 00:06:03.640]   they were letting undergrads in. And so I got, I got smoked by
[00:06:03.640 --> 00:06:06.760]   this class, like, I'm not gonna, I'm not gonna sugarcoat it.
[00:06:06.760 --> 00:06:09.600]   Fortunately, I took it pass fail. So I didn't have a grade
[00:06:09.600 --> 00:06:13.160]   letter associated with it. But, you know, it went through
[00:06:13.160 --> 00:06:17.200]   everything from nonlinear optimization to convolutional
[00:06:17.200 --> 00:06:21.800]   neural networks. And I really was inspired after it to learn
[00:06:21.800 --> 00:06:25.040]   more about it. So I just went down a rabbit hole. I mean, I
[00:06:25.040 --> 00:06:29.040]   could not get enough of it. So you asked if it was difficult. I
[00:06:29.040 --> 00:06:33.120]   don't know, because I spent so, so much time working on it that
[00:06:33.480 --> 00:06:37.080]   I was going to figure it out one way or another. It took me some
[00:06:37.080 --> 00:06:41.640]   time, but I had spent so much time by the end of it that I was
[00:06:41.640 --> 00:06:44.280]   more interested in data science than I was mechanical
[00:06:44.280 --> 00:06:47.040]   engineering. Even though I love mechanical engineering, I spent
[00:06:47.040 --> 00:06:51.320]   so much time building robots and whatnot. So initially, when I
[00:06:51.320 --> 00:06:56.440]   was looking for jobs, I was looking at, you know, smart
[00:06:56.440 --> 00:07:00.920]   manufacturing companies and anyone that would, that would
[00:07:00.920 --> 00:07:04.440]   want someone from a mechanical engineering background, but not
[00:07:04.440 --> 00:07:08.720]   a, not necessarily like a pure data science background, but
[00:07:08.720 --> 00:07:15.200]   but would allow me to flex that. And none of it really jumped out
[00:07:15.200 --> 00:07:17.720]   to me. And it didn't really seem like most of the positions were
[00:07:17.720 --> 00:07:20.080]   hiring directly out of college, I wanted people with more
[00:07:20.080 --> 00:07:27.000]   experience. And so I came across 8451. For American viewers, it's
[00:07:27.640 --> 00:07:31.400]   a wholly owned subsidiary of Kroger, we do basically all the
[00:07:31.400 --> 00:07:35.360]   data science work for for Kroger, which is one of the
[00:07:35.360 --> 00:07:41.120]   largest grocers in the US, and headquartered in Cincinnati. And
[00:07:41.120 --> 00:07:44.040]   that's where I'm from. And so it was appealing, you know, it was
[00:07:44.040 --> 00:07:46.880]   a familiar area, familiar company. And, you know, I
[00:07:46.880 --> 00:07:50.560]   searched around and saw that 8451 had this phenomenal
[00:07:50.560 --> 00:07:53.480]   development program that where it was basically like, we care
[00:07:53.480 --> 00:07:56.960]   more, of course, we want people from some sort of a data science
[00:07:56.960 --> 00:07:59.640]   background, but we care more about people with the mind. And
[00:07:59.640 --> 00:08:03.440]   it's like, we just want the people that have the mind for
[00:08:03.440 --> 00:08:06.040]   this and are good communicators and whatnot, and we will teach
[00:08:06.040 --> 00:08:12.640]   you. And I said, sign me up. So here I am. Yeah, that's me.
[00:08:12.640 --> 00:08:14.760]   Sanyam Bhutani: That's, that's awesome to hear. Where did the
[00:08:14.760 --> 00:08:18.760]   passion for keeping our programming come from? Because
[00:08:18.760 --> 00:08:22.520]   my interactions with my peers from like mechanical, mechanical
[00:08:22.520 --> 00:08:26.120]   engineering background is they usually condense and programmers,
[00:08:26.120 --> 00:08:28.840]   they think like it's the stupidest job ever. Where did
[00:08:28.840 --> 00:08:30.760]   your interest for that world come from?
[00:08:30.760 --> 00:08:34.480]   Matt Walter: It's hard to say. I mean, I think I think it just
[00:08:34.480 --> 00:08:38.520]   hit me perfectly at the right time in my life. You know, my
[00:08:38.520 --> 00:08:42.400]   brother is in cybersecurity, and he was much more of a programmer
[00:08:42.400 --> 00:08:46.800]   throughout his life. And so you could just say I just had the
[00:08:46.800 --> 00:08:49.560]   brain for it and didn't realize it or something, or maybe it was
[00:08:49.560 --> 00:08:52.280]   something that was always going to reach out and hit me in the
[00:08:52.280 --> 00:08:56.400]   face at some point. But, you know, I think that what I like
[00:08:56.400 --> 00:09:00.080]   is building systems. That was one of the things that enjoyed,
[00:09:00.080 --> 00:09:03.400]   or that I enjoyed the most about mechanical engineering was,
[00:09:03.400 --> 00:09:09.560]   you're building a robot, and you have to break that robot into a
[00:09:09.560 --> 00:09:12.320]   million components. I mean, not literally a million, but a ton
[00:09:12.320 --> 00:09:15.440]   of components. I mean, you're, you're in a CAD software, and
[00:09:15.440 --> 00:09:19.480]   you're designing, you know, like some pillow block that goes on
[00:09:19.480 --> 00:09:22.280]   the undercarriage that you won't even see. And then you have to
[00:09:22.280 --> 00:09:26.080]   take all of those, put them into a big drawing to see if it
[00:09:26.080 --> 00:09:29.320]   works. And then you have to go hand manufacture every one of
[00:09:29.320 --> 00:09:33.320]   those. And programming is just a giant analogy to that. You know,
[00:09:33.320 --> 00:09:37.080]   we'll get into the code for this solution later. But I program
[00:09:37.080 --> 00:09:41.560]   like that, too. I, I love to break things out modularly, and
[00:09:41.560 --> 00:09:45.360]   then build them into this one big solution. And I think that
[00:09:45.360 --> 00:09:48.560]   one of the exciting things that really drew me into data science
[00:09:48.560 --> 00:09:51.320]   specifically that I didn't get in mechanical engineering is
[00:09:51.320 --> 00:09:55.240]   that, or I guess I kind of got it, but I love the sort of
[00:09:55.240 --> 00:09:59.200]   inexactness of everything. It's like, the reason we're training
[00:09:59.200 --> 00:10:02.200]   a model is because we can't figure out how to write code to
[00:10:02.200 --> 00:10:06.360]   do it. So you know, you're, you're taking all this complex
[00:10:06.360 --> 00:10:10.920]   system and putting it together to build this thing that solves
[00:10:10.920 --> 00:10:15.560]   an incredibly complicated problem. And that just really
[00:10:15.560 --> 00:10:16.200]   speaks to me.
[00:10:17.360 --> 00:10:19.720]   Sanyam Bhutani: That's, that's awesome to hear. You will also,
[00:10:19.720 --> 00:10:23.560]   I think, trying your, so this is me trying to look at your
[00:10:23.560 --> 00:10:27.240]   LinkedIn profile and figure out the timelines. But from what I
[00:10:27.240 --> 00:10:30.560]   understood that you're also trying to build a own business,
[00:10:30.560 --> 00:10:33.320]   your own business as well, I believe you were a CTO of a
[00:10:33.320 --> 00:10:37.040]   startup in the Ethereum world. So tell us more about that versus
[00:10:37.040 --> 00:10:39.960]   also passion for building products and stuff like that.
[00:10:39.960 --> 00:10:45.480]   Yeah, so that that was kind of a college thing that never really
[00:10:45.480 --> 00:10:50.000]   came to fruition as much. But it was one of my original forays.
[00:10:50.000 --> 00:10:53.880]   After I figured out, you know, how to do predictive modeling, I
[00:10:53.880 --> 00:10:58.240]   was interested in any way that I could apply that. And so I saw
[00:10:58.240 --> 00:11:02.640]   that, you know, the crypto world was in absolute chaos and
[00:11:02.640 --> 00:11:08.080]   markets going up and down. And I figured, you know, I know, I
[00:11:08.080 --> 00:11:11.160]   know, I know that nothing really has changed. But so I started,
[00:11:11.160 --> 00:11:14.360]   you know, it was anywhere I could apply stuff. And so a
[00:11:14.360 --> 00:11:17.120]   couple buddies of mine got together and we were like, what
[00:11:17.120 --> 00:11:20.720]   can we build? I don't know everything. So it was kind of, I
[00:11:20.720 --> 00:11:24.760]   would call that sort of a young and dumb sort of experience. But
[00:11:24.760 --> 00:11:29.280]   I learned so much about data science from that, you know, out
[00:11:29.280 --> 00:11:33.080]   you could take blockchain out and replace it with anything. It
[00:11:33.080 --> 00:11:37.120]   was just my first experience really getting to apply data
[00:11:37.120 --> 00:11:41.320]   science and programming and databases and all of these
[00:11:41.320 --> 00:11:46.240]   different things into into initial products. Did you also
[00:11:46.240 --> 00:11:49.040]   gain some like software engineering experience there?
[00:11:49.040 --> 00:11:51.680]   Because that's one part that data scientists just skip and
[00:11:51.680 --> 00:11:55.080]   jump ahead most of the time. Yeah, yeah. And again, coming
[00:11:55.080 --> 00:11:58.200]   from a from a from an engineering perspective, that's
[00:11:58.200 --> 00:12:01.840]   that's like, when I I took two classes in college, and both of
[00:12:01.840 --> 00:12:04.800]   them were basically CS classes, they were software engineering
[00:12:04.800 --> 00:12:08.720]   classes. And so that's the perspective I come I come to,
[00:12:08.960 --> 00:12:12.760]   especially Python from, you know, I appreciate Python for
[00:12:12.760 --> 00:12:18.360]   what it is, because it's not the C based languages. But I, I
[00:12:18.360 --> 00:12:22.680]   still program a lot of Python, like it's one of those
[00:12:22.680 --> 00:12:27.280]   languages, because, you know, I think I just appreciate the, the
[00:12:27.280 --> 00:12:31.480]   rigor. And one of the issues that you have in data science
[00:12:31.480 --> 00:12:34.360]   is it's really hard to experiment quickly with new
[00:12:34.360 --> 00:12:37.600]   things. And I tweet about this all the time that the best
[00:12:37.600 --> 00:12:42.200]   models come from rapid iteration. And the best way that
[00:12:42.200 --> 00:12:49.280]   I found to rapidly iterate is to, to build very modular
[00:12:49.280 --> 00:12:53.960]   project structures, where you can quickly swap in and swap out
[00:12:53.960 --> 00:12:58.160]   new things. And I know that a lot of Kagglers will build
[00:12:58.160 --> 00:13:03.480]   everything in this sort of intertwined Jupyter notebook,
[00:13:03.480 --> 00:13:06.040]   not there's anything wrong with notebooks, I love notebooks.
[00:13:06.040 --> 00:13:10.360]   But I, what I found is that it's so important for me to separate
[00:13:10.360 --> 00:13:13.280]   everything out into its own little modular bit so that I can
[00:13:13.280 --> 00:13:16.600]   quickly swap in and out new parts and see what works and
[00:13:16.600 --> 00:13:17.160]   what doesn't.
[00:13:17.160 --> 00:13:21.320]   Sanyam Bhutani: Yep, that that's one thing I've learned from
[00:13:21.320 --> 00:13:25.760]   Kagglers, they're super fast at prototyping. And I, from what
[00:13:25.760 --> 00:13:29.120]   I've been hearing, it's really useful in the industry, I just
[00:13:29.120 --> 00:13:32.040]   create content. So I have no idea what I'm talking about. How
[00:13:32.040 --> 00:13:35.840]   did you get? How did you get passionate about Kaggle? And how
[00:13:35.840 --> 00:13:37.440]   did your Kaggle journey begin?
[00:13:37.440 --> 00:13:42.840]   Yeah, I'm a, an absolute nut for competition. I played sports my
[00:13:42.840 --> 00:13:46.840]   whole life. I played club sports in college. I was a I was a
[00:13:46.840 --> 00:13:51.040]   baseball pitcher, and outfielder. And so I love
[00:13:51.040 --> 00:13:55.280]   competing however I can. And, but really, like, I shouldn't
[00:13:55.280 --> 00:13:57.960]   even say that I got into it because of the competition, I
[00:13:57.960 --> 00:14:02.840]   more so got into it because I saw it, you know, I can't
[00:14:02.840 --> 00:14:05.280]   remember what my first competition was, if it was the I
[00:14:05.280 --> 00:14:09.200]   think it might have been the NFL rushing yards prediction
[00:14:09.200 --> 00:14:13.400]   competition from a couple of years ago. And I got into it
[00:14:13.400 --> 00:14:16.320]   because it was like, hey, here's an opportunity for me to use
[00:14:16.320 --> 00:14:21.560]   data science on a problem that interests me. And, you know, I
[00:14:21.560 --> 00:14:25.600]   started reading, I had read some past solutions to past problems.
[00:14:25.600 --> 00:14:29.160]   And I was like, this is, this is an incredible learning resource.
[00:14:29.160 --> 00:14:32.560]   This is where I want to be. I mean, it seemed like I didn't
[00:14:32.560 --> 00:14:35.480]   really realize how true this was, at the time, but it seemed
[00:14:35.480 --> 00:14:39.520]   like this is the place where things happen. You know, and I
[00:14:39.520 --> 00:14:44.000]   want to, I want to be a part of it. Yeah, what you'll find is
[00:14:44.000 --> 00:14:49.160]   that that competition isn't on my page, because it was a reruns
[00:14:49.160 --> 00:14:53.560]   competition. And my, my, I ran out of time in the private
[00:14:53.560 --> 00:14:57.120]   leaderboard submission, which is disappointing, because the cross
[00:14:57.120 --> 00:15:00.320]   validation was really accurate, and I was in silver medal range.
[00:15:00.520 --> 00:15:04.440]   So if I had gotten a silver medal solo in my first Kaggle
[00:15:04.440 --> 00:15:07.240]   competition, that would have just been like, that would have
[00:15:07.240 --> 00:15:10.760]   been great. But no, I had a humbling experience.
[00:15:10.760 --> 00:15:14.200]   Sanyam Bhutani: I can imagine your frustration. I got a huge
[00:15:14.200 --> 00:15:17.040]   shakeup in my first competition. And I don't want to talk about
[00:15:17.040 --> 00:15:17.840]   that. So I'll shut up.
[00:15:17.840 --> 00:15:22.000]   Unknown: I genuinely couldn't even go to Kaggle.com for at
[00:15:22.000 --> 00:15:25.640]   least a month afterwards. I was so embarrassed. So embarrassed.
[00:15:26.720 --> 00:15:32.240]   So from from there, how did you go about learning and solo gold
[00:15:32.240 --> 00:15:36.080]   bagging a medal on the MLB player digital engagement
[00:15:36.080 --> 00:15:39.880]   forecasting competition and also meddling across solo meddling
[00:15:39.880 --> 00:15:42.760]   across the recent pet finder competition? How did you learn?
[00:15:42.760 --> 00:15:46.680]   How did you go about improving your skills? One, like one thing
[00:15:46.680 --> 00:15:50.080]   I always remember is that frustration that I had, which
[00:15:50.080 --> 00:15:52.880]   still hasn't gone away, which is, you see yourself on the
[00:15:52.880 --> 00:15:56.680]   leaderboard, you think of the greatest idea, I let me rephrase
[00:15:56.680 --> 00:15:58.960]   I would think of the greatest idea I would submit and I will
[00:15:58.960 --> 00:16:02.360]   be like in the bottom half of the leaderboard. So yeah, how
[00:16:02.360 --> 00:16:05.280]   did you improve? If at all you face that feeling?
[00:16:05.280 --> 00:16:10.280]   Yeah, I mean, it was really, it was very overwhelming at first.
[00:16:10.280 --> 00:16:15.400]   I mean, I cannot even express how you know, people often talk
[00:16:15.400 --> 00:16:18.040]   about Dunning Kruger, where it's like, you think you're a genius
[00:16:18.040 --> 00:16:21.200]   at first, and then you fall off. For me, there was no I'm a
[00:16:21.200 --> 00:16:26.120]   genius. It was I was at the bottom for the whole time. And
[00:16:26.120 --> 00:16:29.920]   it was a lot of me just bumping around in the dark and reading,
[00:16:29.920 --> 00:16:33.400]   reading Kaggle solutions, reading blog posts, reading
[00:16:33.400 --> 00:16:38.160]   research papers, figuring out, I mean, like reading a ton of old
[00:16:38.160 --> 00:16:42.960]   Kaggle solutions where I would go to a competition page, and
[00:16:42.960 --> 00:16:47.680]   look at take a quick glance at the data. And just see, okay,
[00:16:47.680 --> 00:16:51.480]   how do I think I would solve this problem? Yeah, yeah, there
[00:16:51.480 --> 00:16:55.600]   you go. That's Yeah, there you go. You took the words right out
[00:16:55.600 --> 00:17:00.560]   of my mouth. But I, it was just so much practice, and just
[00:17:00.560 --> 00:17:03.320]   trying and failing and trying and failing and trying and
[00:17:03.320 --> 00:17:06.920]   failing. And then I would say that the real breakthroughs for
[00:17:06.920 --> 00:17:11.480]   me came. I know you're a big fan of the fast AI course. And I, I
[00:17:11.480 --> 00:17:15.040]   watched the probably two or three months after the lectures
[00:17:15.040 --> 00:17:20.480]   were posted on YouTube, I watched the 2019 version of the,
[00:17:20.480 --> 00:17:24.600]   of the fast AI course, and it all clicked. I mean, it all
[00:17:24.600 --> 00:17:28.360]   clicked, like Jeremy, Jeremy Howard teaches in a way that
[00:17:28.360 --> 00:17:34.040]   just resonated with me. And, you know, I don't use fast AI to
[00:17:34.040 --> 00:17:39.240]   this day, but the way that he taught that course was so
[00:17:39.240 --> 00:17:43.000]   revolutionary for me, because not only I mean, I would
[00:17:43.000 --> 00:17:45.440]   recommend it to anyone, it's probably, in my opinion, the
[00:17:45.440 --> 00:17:49.000]   best deep learning course, that holistic deep learning course to
[00:17:49.000 --> 00:17:54.080]   go from kind of zero to hero out there, but it was particularly
[00:17:54.080 --> 00:17:57.440]   for me, because he really teaches it in sort of a systems
[00:17:57.440 --> 00:18:01.640]   way of thinking, and an intuitive way of thinking. You
[00:18:01.640 --> 00:18:05.240]   know, he doesn't teach you the atom optimizer in terms of like,
[00:18:05.240 --> 00:18:09.000]   okay, here's the code, figure it out. It's like, okay, well, we
[00:18:09.000 --> 00:18:11.960]   started with SGD with momentum, and then we figured out we can
[00:18:11.960 --> 00:18:15.360]   improve that with RMS prop, and then Adam, and here's the
[00:18:15.360 --> 00:18:18.600]   intuition. Here's why they came up with it. Not, you know, not
[00:18:18.600 --> 00:18:20.520]   some super technical definition.
[00:18:20.520 --> 00:18:24.320]   Sanyam Bhutani: Yeah, as you said, I'm a huge fan as well.
[00:18:24.320 --> 00:18:30.560]   Yeah. Yeah. So from there, how do you today decide which
[00:18:30.560 --> 00:18:33.760]   competitions to take part in transitioning to about the pet
[00:18:33.760 --> 00:18:37.200]   finder competition? How did you decide that this is the
[00:18:37.200 --> 00:18:40.120]   competition I want to spend a few weeks on, even though I want
[00:18:40.120 --> 00:18:42.840]   to point out to the audience, you just spend a really small
[00:18:42.840 --> 00:18:45.320]   amount of your time, which is like two to three weeks compared
[00:18:45.320 --> 00:18:48.200]   to the, I think, three or four months for which this
[00:18:48.200 --> 00:18:49.360]   competition done for?
[00:18:49.360 --> 00:18:55.120]   Yeah, so there were combinations of factors for this one. But
[00:18:55.120 --> 00:18:59.440]   generally speaking, I think a lot of Kagglers think the same
[00:18:59.440 --> 00:19:02.440]   way, but it was it's just finding competitions where I
[00:19:02.440 --> 00:19:07.280]   feel like I can learn something new. Generally, I try not to
[00:19:07.280 --> 00:19:10.560]   throw myself into something that I'm going to have
[00:19:10.560 --> 00:19:14.960]   absolutely no base of knowledge on at all, which really isn't
[00:19:14.960 --> 00:19:17.840]   really anything anymore. I think that was more so a year or two
[00:19:17.840 --> 00:19:23.080]   ago, where I there were entire fields of, say, deep learning
[00:19:23.080 --> 00:19:27.960]   that I had never even forayed into. But it's really finding
[00:19:27.960 --> 00:19:31.200]   something specific about a competition that I really want
[00:19:31.200 --> 00:19:34.200]   to learn. And then I just jump in, like, I don't let myself
[00:19:34.240 --> 00:19:39.480]   back out because the human mind as soon as we hit something
[00:19:39.480 --> 00:19:42.920]   rough, you know, we're much more likely to just shy away. And so
[00:19:42.920 --> 00:19:48.160]   I try to just dive in headfirst. And for this one, the
[00:19:48.160 --> 00:19:53.160]   combination of factors was I was extremely busy in my life when I
[00:19:53.160 --> 00:19:55.560]   started, but I really, really wanted to do a Kaggle
[00:19:55.560 --> 00:19:58.960]   competition. And so this one was something that I knew I could
[00:19:58.960 --> 00:20:03.240]   focus on one specific thing. It was a very straightforward
[00:20:03.240 --> 00:20:05.560]   competition, it was just here's some images, and you're
[00:20:05.560 --> 00:20:10.280]   predicting a regression target. Very straightforward. But, you
[00:20:10.280 --> 00:20:13.840]   know, I could tell immediately it was going to be, you know,
[00:20:13.840 --> 00:20:17.920]   how good is your cross validation? And how good is your
[00:20:17.920 --> 00:20:21.520]   experiment tracking? Because with the exception of a couple
[00:20:21.520 --> 00:20:26.680]   people, everyone was ensembling these huge ensembles. And so I
[00:20:26.680 --> 00:20:30.560]   could just see it from a mile away. And so I, you know, I
[00:20:30.560 --> 00:20:34.280]   started using weights and biases during the gravitational wave
[00:20:34.280 --> 00:20:38.160]   challenge, which is the one I did prior to this one. And I
[00:20:38.160 --> 00:20:43.040]   realized how just how powerful it was to have a really good
[00:20:43.040 --> 00:20:46.000]   experiment tracker rather than I was using TensorBoard before,
[00:20:46.000 --> 00:20:51.240]   which is fine. But it requires a lot more manual effort to get
[00:20:51.240 --> 00:20:55.600]   kind of what you get out of weights and biases. And, you
[00:20:55.600 --> 00:20:58.520]   know, credit to Google for putting that out there, because
[00:20:58.560 --> 00:21:02.400]   there was basically nothing like that before, before TensorBoard.
[00:21:02.400 --> 00:21:05.240]   But then I realized what a quantum leap it was just how
[00:21:05.240 --> 00:21:09.440]   much more you could get out of it. And so I realized, this is
[00:21:09.440 --> 00:21:12.880]   going to be where I where I learned that. And I still suck
[00:21:12.880 --> 00:21:15.320]   at it. I'm sure that if you went and looked at my weights and
[00:21:15.320 --> 00:21:19.400]   biases dashboard, you would have to shield your eyes because it's
[00:21:19.400 --> 00:21:21.960]   just not well organized. And there's probably more I could
[00:21:21.960 --> 00:21:25.360]   put on the, you know, the dashboards and whatnot. But I
[00:21:25.360 --> 00:21:31.560]   learned so much about how you should use a large, how you can
[00:21:31.560 --> 00:21:36.040]   essentially use, you know, big data on like meta big data
[00:21:36.040 --> 00:21:38.360]   almost, you know, you can collect all this data about your
[00:21:38.360 --> 00:21:41.600]   training processes and, and know what to do differently in the
[00:21:41.600 --> 00:21:44.440]   future quickly figure out if things are working or if they
[00:21:44.440 --> 00:21:49.320]   don't. So, yeah, I mean, that was specific to this one. But,
[00:21:49.320 --> 00:21:51.800]   you know, again, it was it was just about finding something
[00:21:51.800 --> 00:21:52.960]   that I found interesting.
[00:21:54.000 --> 00:21:56.160]   Sanyam Bhutani: No, thanks. Thanks for the shout out. And I
[00:21:56.160 --> 00:21:58.920]   am pretty sure no one from the team watches my video so I can
[00:21:58.920 --> 00:22:05.200]   say this. I'm myself learning the framework a lot. And yeah,
[00:22:05.200 --> 00:22:08.520]   there's like many interesting ways, not just one obvious way
[00:22:08.520 --> 00:22:11.920]   especially to track stuff. And every time I speak to a
[00:22:11.920 --> 00:22:14.000]   colleague, I'm like blown away. Oh, I should have known that
[00:22:14.000 --> 00:22:16.960]   feature. I can't believe I've been here for a few weeks. And I
[00:22:16.960 --> 00:22:17.840]   don't know of that.
[00:22:17.840 --> 00:22:21.960]   Unknown: Oh my gosh. Well, I mean, there's so much I mean, it
[00:22:21.960 --> 00:22:27.080]   blows me away. I found a from one of the tutorials this like
[00:22:27.080 --> 00:22:32.120]   image logging callback for for PyTorch lightning, which is what
[00:22:32.120 --> 00:22:38.280]   I mostly use. And it blew me away. I mean, I could just a
[00:22:38.280 --> 00:22:42.320]   simple little callback that, you know, takes care of everything,
[00:22:42.320 --> 00:22:47.240]   right? It was just done. You know, I had a histogram plotting
[00:22:47.240 --> 00:22:49.760]   for my prediction distribution. I didn't even have to do that on
[00:22:49.760 --> 00:22:52.520]   my own. I mean, I did it, it was set it and forget it, you know,
[00:22:52.520 --> 00:22:55.920]   I did it one time, and I never had to do it again. And I think
[00:22:55.920 --> 00:23:00.560]   we all have these issues with being lazy down the line where
[00:23:00.560 --> 00:23:03.920]   you know that you need to set like I need to write an error
[00:23:03.920 --> 00:23:07.600]   analysis script at the start of the competition or else it's not
[00:23:07.600 --> 00:23:10.400]   happening later. I'm just gonna get lazy and be like, I think
[00:23:10.400 --> 00:23:14.720]   it's doing all right. I don't really know. And it just it
[00:23:14.720 --> 00:23:16.960]   really reinforces those best practices.
[00:23:18.000 --> 00:23:20.280]   Sanyam Bhutani: That's great to do. Awesome. So coming back to
[00:23:20.280 --> 00:23:23.200]   the competition for the next few minutes, what I'll try to do is
[00:23:23.200 --> 00:23:27.360]   painfully try to summarize the overview of the competition and
[00:23:27.360 --> 00:23:30.800]   mark please correct me wherever you think I'm wrong. So for the
[00:23:30.800 --> 00:23:34.280]   audience, I'll try to explain what the problem is. And the
[00:23:34.280 --> 00:23:36.840]   problem is quite simple. You have to look at cats and dogs
[00:23:36.840 --> 00:23:41.720]   images. And from there, you need to predict how popular I love
[00:23:41.720 --> 00:23:45.400]   that they can be and you need to I believe, predict their
[00:23:45.400 --> 00:23:48.200]   popularity score. So that's the regression problem that I
[00:23:48.200 --> 00:23:53.720]   believe Mark was talking about earlier. From for doing that, I
[00:23:53.720 --> 00:23:56.520]   think I'm not too sure because I might have forgotten I
[00:23:56.520 --> 00:24:00.480]   participated in the older competition. These were meta
[00:24:00.480 --> 00:24:03.440]   features that we were trying to predict, I think by our models,
[00:24:03.440 --> 00:24:06.800]   but this time you've got a lot of metadata around is the
[00:24:06.800 --> 00:24:11.920]   subject in focus are both eyes facing front or near front with
[00:24:11.920 --> 00:24:16.000]   at least one eye properly clear. These are like things if you sit
[00:24:16.000 --> 00:24:19.720]   back, I believe and think about this is what makes pets cuter,
[00:24:19.720 --> 00:24:23.800]   right and pet finder, I believe their mission is to help fast
[00:24:23.800 --> 00:24:26.560]   track pet adoption. So they really want to figure out how
[00:24:26.560 --> 00:24:31.720]   can they improve that and what makes pets popular. So if their
[00:24:31.720 --> 00:24:34.360]   face is in the center of the photo, if they are nearer to the
[00:24:34.360 --> 00:24:38.200]   camera, if they're this focus inside of the image, you had all
[00:24:38.200 --> 00:24:46.080]   of this metadata along with all of these images in the I need to
[00:24:46.080 --> 00:24:53.400]   refresh that. I'm not sure Have you been around Kaggle during
[00:24:53.400 --> 00:24:55.080]   those days when it just used to crash?
[00:24:55.080 --> 00:25:00.440]   Oh, yeah, I think I got I think I got in just at the tail end of
[00:25:00.440 --> 00:25:02.920]   that. But yes, I know exactly what you're talking about.
[00:25:02.920 --> 00:25:06.800]   So it's, it's definitely much better now. But as you can see,
[00:25:06.800 --> 00:25:09.840]   these are a lot of images, this probably I'm guessing wouldn't
[00:25:09.840 --> 00:25:14.280]   help fast track their adoption. So we need to help them
[00:25:14.280 --> 00:25:18.320]   understand through using our models, which pets can be
[00:25:18.320 --> 00:25:21.400]   adopted faster. Have I summarized this correctly?
[00:25:21.400 --> 00:25:25.840]   Yes, the only the only thing I would add, and we'll get into it
[00:25:25.840 --> 00:25:28.760]   later is just how noisy the target variable is because it's
[00:25:28.760 --> 00:25:34.040]   a it's a derived metric and it's not. It's tough. So as as cute
[00:25:34.040 --> 00:25:37.760]   as the name popularity is it really, it grew some gray hairs
[00:25:37.760 --> 00:25:38.400]   on my head.
[00:25:38.400 --> 00:25:44.160]   Okay, so from there, I'd love to understand how did you approach
[00:25:44.160 --> 00:25:47.080]   modeling? I remember my colleague I stuck with had put
[00:25:47.080 --> 00:25:50.440]   out this incredible kernel on efficient need we do there was
[00:25:50.440 --> 00:25:55.680]   another Kaggle grandmaster. I'm trying to find that among the
[00:25:55.680 --> 00:25:58.920]   million times that I have open who had put out a starter
[00:25:58.920 --> 00:26:02.960]   kernel on efficient and be night a b0 which had this nice
[00:26:02.960 --> 00:26:07.440]   dashboard and they were trying to showcase the grad cam of hey,
[00:26:07.440 --> 00:26:10.720]   efficient networks well and this is what it's actually detecting
[00:26:10.720 --> 00:26:13.960]   inside of the images. So how did when you saw the problem? How
[00:26:13.960 --> 00:26:16.520]   did you approach it? What was your baseline and what were your
[00:26:16.520 --> 00:26:17.360]   first thoughts?
[00:26:17.360 --> 00:26:21.360]   Yeah, I mean, I think that the the simplest baseline here
[00:26:21.360 --> 00:26:25.080]   because it's an RMSE competition was just what's the standard
[00:26:25.080 --> 00:26:28.560]   deviation of the target. I there was a lot of conversation at the
[00:26:28.560 --> 00:26:31.960]   beginning, like very beginning of the competition, which was
[00:26:32.000 --> 00:26:36.320]   like, are our models even learning anything? And you
[00:26:36.320 --> 00:26:39.560]   couldn't really tell because it was a pretty high RMSE. I mean,
[00:26:39.560 --> 00:26:44.000]   I think anyone who looks at a competition where the target is
[00:26:44.000 --> 00:26:49.440]   zero to 100 and your RMSE is like 17 sits there and wonders,
[00:26:49.440 --> 00:26:52.280]   Am I really learning anything, but you know, my benchmark for
[00:26:52.280 --> 00:26:55.360]   it was just what's the standard deviation of the target and it
[00:26:55.360 --> 00:26:59.960]   ended up being 20 or so. So you can use that to say, a totally
[00:26:59.960 --> 00:27:05.720]   naive model would have an RMSE of about 20. So I started there.
[00:27:05.720 --> 00:27:10.040]   And, you know, normally, I would do a little bit more complex
[00:27:10.040 --> 00:27:13.920]   benchmarking. But I had seen early on that the metadata
[00:27:13.920 --> 00:27:18.160]   didn't really seem to provide a ton of value. I think that Chris
[00:27:18.160 --> 00:27:21.680]   DeYacht had a good solution where he also used the metadata,
[00:27:21.680 --> 00:27:26.480]   but most people I don't think did. And so you know, I, I at
[00:27:26.480 --> 00:27:31.840]   least searched out to see if if anyone could just use the
[00:27:31.840 --> 00:27:35.520]   metadata to predict the target. And what I saw was the solutions
[00:27:35.520 --> 00:27:41.680]   that only used the metadata were about at that 20 RMSE. So that
[00:27:41.680 --> 00:27:46.400]   was pretty, you know, pretty dead to rights confirmation that
[00:27:46.400 --> 00:27:51.120]   the metadata was just not very useful. So I kind of proceeded
[00:27:51.120 --> 00:27:55.040]   at least at first without it. And I just I just did what I
[00:27:55.040 --> 00:27:58.200]   would do with any other competition. And I set up a
[00:27:58.200 --> 00:28:04.560]   super simple model with a validation set that seemed to
[00:28:04.560 --> 00:28:09.640]   make sense to me. And just, you know, through pytorch image
[00:28:09.640 --> 00:28:12.920]   models, the wonderful Tim repo and, and just got it like a
[00:28:12.920 --> 00:28:19.120]   ResNet 34 baseline or something like that. And the cross
[00:28:19.120 --> 00:28:22.200]   validation was super important. I spent I spent a lot of time on
[00:28:22.200 --> 00:28:26.040]   that for this competition. As I found out very quickly, if you
[00:28:26.040 --> 00:28:28.320]   went through a bunch of the notebooks, you would find some
[00:28:28.320 --> 00:28:33.560]   sketchy validation to say the least. So I would say that my
[00:28:33.560 --> 00:28:38.000]   my start was just get the stupidest baseline I could out
[00:28:38.000 --> 00:28:41.840]   with the simplest data augmentation and then iterate on
[00:28:41.840 --> 00:28:46.360]   on my cross validation. And the way that my timeline worked for
[00:28:46.360 --> 00:28:48.800]   this competition was I spent probably about a week at the
[00:28:48.800 --> 00:28:53.200]   beginning of the competition actually working on it. And then
[00:28:53.200 --> 00:28:57.800]   at the for the last two weeks of the competition, I actually took
[00:28:57.800 --> 00:29:01.680]   it seriously and modeled. So there was a almost three month
[00:29:01.680 --> 00:29:05.280]   gap in the middle where I did nothing. So the first week was
[00:29:05.280 --> 00:29:08.600]   just me figuring out what made sense to me as a validation
[00:29:08.600 --> 00:29:11.440]   strategy, because the public leaderboard really wasn't
[00:29:11.440 --> 00:29:12.280]   helping with that.
[00:29:12.280 --> 00:29:16.440]   Sanyam Bhutani: Where did that intuition come from? I'm really
[00:29:16.440 --> 00:29:19.080]   curious, because even like you mentioned, whenever you go to
[00:29:19.080 --> 00:29:22.120]   the discussions, there's this now it's solutions, but during
[00:29:22.120 --> 00:29:24.640]   the heat of the competition, people are sharing so much
[00:29:24.640 --> 00:29:27.520]   stuff, first of all, because it's incentivized, they would
[00:29:27.520 --> 00:29:30.720]   appreciate the upvotes, they would get medals, there's also
[00:29:30.720 --> 00:29:34.400]   so many kernels. And one thing I've learned the hard way is not
[00:29:34.400 --> 00:29:38.120]   all of them are the best kernel. So how did you sift through
[00:29:38.120 --> 00:29:42.040]   these? How did you understand what's, what's probably what's
[00:29:42.040 --> 00:29:44.320]   the one thing you can take from here? And what should you do
[00:29:44.320 --> 00:29:44.840]   yourself?
[00:29:45.800 --> 00:29:50.240]   Yeah, so I tried to, I mean, you read you read Kaggle discussions
[00:29:50.240 --> 00:29:54.040]   enough, and it's almost a meme, you know, CB greater than sign
[00:29:54.040 --> 00:29:58.960]   leaderboard or whatever, you know, people, all the most
[00:29:58.960 --> 00:30:02.920]   successful Kagglers repeat that. And so if everyone who's having
[00:30:02.920 --> 00:30:06.640]   success keeps saying something, it's probably worth at least
[00:30:06.640 --> 00:30:11.240]   looking into. And now, I will say, I haven't really
[00:30:11.240 --> 00:30:15.440]   participated in a lot of competitions seriously that had
[00:30:15.440 --> 00:30:18.680]   massive shakeup. So I haven't really, I don't know if I had
[00:30:18.680 --> 00:30:23.400]   fully taken it to heart, but I at least knew that I, I could I
[00:30:23.400 --> 00:30:26.560]   had to appreciate it. I mean, I think the only one on my
[00:30:26.560 --> 00:30:30.880]   competitions page that really had shakeup was the m five
[00:30:30.880 --> 00:30:34.040]   competition. And I only worked on that one for maybe a couple
[00:30:34.040 --> 00:30:39.960]   of weeks before I lost interest. So, you know, I, I just
[00:30:40.000 --> 00:30:43.120]   understood its importance, especially because I was looking
[00:30:43.120 --> 00:30:46.120]   at a lot of kernels and just thinking to myself, this doesn't
[00:30:46.120 --> 00:30:53.240]   look right. There was a very popular kernel that was, they
[00:30:53.240 --> 00:30:56.560]   didn't even really, I don't even think they realized that they
[00:30:56.560 --> 00:31:00.720]   weren't doing cross validation correctly. They were basically,
[00:31:00.720 --> 00:31:03.280]   I don't want to disparage anyone, because I think it was
[00:31:03.280 --> 00:31:06.560]   one of their first kernels posted ever, which is like, if you can,
[00:31:06.560 --> 00:31:09.680]   if you can even get yourself to posting a kernel, like props to
[00:31:09.680 --> 00:31:15.000]   you, because I never do. But I, they, I think they were
[00:31:15.000 --> 00:31:18.880]   basically taking a 20% random sample over 10 folds. And so
[00:31:18.880 --> 00:31:21.880]   they were doing basically bootstrap cross validation, but
[00:31:21.880 --> 00:31:24.920]   they weren't doing it in a way that you could rely on the
[00:31:24.920 --> 00:31:27.600]   cross validation score, but it was doing really well on the
[00:31:27.600 --> 00:31:32.240]   public leaderboard. And so what I said to myself is, yeah, I
[00:31:32.240 --> 00:31:35.080]   mean, that that ain't right. You know, there is something wrong
[00:31:35.080 --> 00:31:39.640]   here. So I think I realized pretty early on, based on all of
[00:31:39.640 --> 00:31:44.480]   that, all of those sorts of piecing all of that together,
[00:31:44.480 --> 00:31:49.280]   that cross validation was going to be super important here. And
[00:31:49.280 --> 00:31:52.920]   it did end up being I think I was 400 something place on the on
[00:31:52.920 --> 00:31:56.400]   the public leaderboard, and then I ended up in 30th. So it looks
[00:31:56.400 --> 00:31:58.080]   like my starting intuition was correct.
[00:31:58.080 --> 00:32:01.640]   Sanyam Bhutani: Were you surprised by that at all? Or were
[00:32:01.640 --> 00:32:04.760]   you expecting that? Because that is a huge jump.
[00:32:06.480 --> 00:32:10.400]   I think right after I saw it, I shouted and my girlfriend went,
[00:32:10.400 --> 00:32:16.640]   What? Are you okay? Yes, I'm very okay, actually. So yeah, I
[00:32:16.640 --> 00:32:20.400]   was, I was shocked. I mean, I think I have maybe a little bit
[00:32:20.400 --> 00:32:23.440]   of an ego going into it. I think I knew that what I was doing was
[00:32:23.440 --> 00:32:26.840]   pretty solid. I was looking at my cross validation scores and
[00:32:26.840 --> 00:32:30.760]   thinking, this looks pretty good. And when I look at public
[00:32:30.760 --> 00:32:34.880]   leaderboard, or public kernels that are actually running good
[00:32:34.880 --> 00:32:38.600]   cross validation, I was beating I was beating their scores. And
[00:32:38.600 --> 00:32:42.360]   so I knew, I knew there was at least something good, but I
[00:32:42.360 --> 00:32:46.840]   thought, maybe I'll get a bronze. I don't know. You know,
[00:32:46.840 --> 00:32:49.200]   when you when you put that little time into a competition,
[00:32:49.200 --> 00:32:52.240]   you don't really have high expectations. So I was thinking,
[00:32:52.240 --> 00:32:54.560]   you know, maybe I'll maybe I'll snag bronze or something.
[00:32:54.560 --> 00:32:57.920]   Sanyam Bhutani: But congratulations again on the
[00:32:57.920 --> 00:33:00.800]   solo sale. Where I know that post you up to the Masters
[00:33:00.800 --> 00:33:02.960]   ranking. So that's, that's really awesome to see.
[00:33:02.960 --> 00:33:03.960]   Appreciate it.
[00:33:04.840 --> 00:33:07.320]   For the next bit, I would request you to share your
[00:33:07.320 --> 00:33:11.400]   solution overview. And I believe I'll hand over the screen share
[00:33:11.400 --> 00:33:12.440]   bit to you for that.
[00:33:12.440 --> 00:33:16.640]   Yeah, yeah. So before I get into it, I'll just kind of talk over
[00:33:16.640 --> 00:33:21.760]   what I did. So I, for most of the competition, I was doing
[00:33:21.760 --> 00:33:26.320]   fivefold cross validation. And basically, the way I set up my,
[00:33:26.320 --> 00:33:30.720]   my validation was something I saw actually fairly early on
[00:33:30.720 --> 00:33:35.680]   from a legend in the Kaggle community, Phalanx, who is an
[00:33:35.680 --> 00:33:43.400]   absolute, absolutely incredible at vision competitions. But what
[00:33:43.400 --> 00:33:46.400]   you could you what you could notice about the target was that
[00:33:46.400 --> 00:33:50.520]   despite the fact that it came as a floating point number, it was
[00:33:50.520 --> 00:33:56.880]   just integers from one to 100. So you could just do stratified
[00:33:56.880 --> 00:34:02.040]   k fold on those integers. So instead of doing the common sort
[00:34:02.040 --> 00:34:06.480]   of, I always credit Abhishek Thakur with, with this sort of
[00:34:06.480 --> 00:34:09.440]   methodology of binning regression targets, and then
[00:34:09.440 --> 00:34:12.520]   stratifying on those, I'm sure that somebody else came up with
[00:34:12.520 --> 00:34:15.240]   it before him, but he's the first one who I saw posting
[00:34:15.240 --> 00:34:18.440]   about it. Instead of doing something like that, which a lot
[00:34:18.440 --> 00:34:22.280]   of people were, I figured why, why bin an integer target when
[00:34:22.280 --> 00:34:26.720]   you could just immediately stratify off of it. So I found
[00:34:26.720 --> 00:34:30.400]   that it made sense to me. Again, there was no public leaderboard
[00:34:30.400 --> 00:34:33.720]   feedback that you could get from this. In fact, it seemed like
[00:34:33.720 --> 00:34:36.720]   sometimes the better my cross validation got the worst my
[00:34:36.720 --> 00:34:41.000]   public leaderboard score got. So it was frustrating, but it felt
[00:34:41.000 --> 00:34:45.520]   like it made sense to me. So I just I went forward with it. The
[00:34:45.520 --> 00:34:52.280]   second part of the solution is that transformers and any model
[00:34:52.280 --> 00:34:56.680]   that did not rely on batch norm generally, did really well. So I
[00:34:56.680 --> 00:35:01.360]   noticed that not only did SWIN, my SWIN transformer model do
[00:35:01.360 --> 00:35:07.240]   well, and I tried convit later on, and there's one more I'm
[00:35:07.240 --> 00:35:13.280]   forgetting. But NFNet did really well. And the TIM library has
[00:35:13.280 --> 00:35:17.800]   this wonderful ECA line of NFNet models that are essentially a
[00:35:17.800 --> 00:35:21.160]   more efficient implementation of the original NFNet models,
[00:35:21.160 --> 00:35:26.000]   because those NFNet models are our beasts. So I found that all
[00:35:26.000 --> 00:35:28.800]   those worked well. And I'm, you know, looking back on it, I'm
[00:35:28.800 --> 00:35:32.000]   really disappointed I didn't use that intuition more effectively,
[00:35:32.000 --> 00:35:34.560]   because I know Chris DeYacht found that if you just freeze
[00:35:34.560 --> 00:35:37.920]   the batch norm, then you could use any model. In fact, he just
[00:35:37.920 --> 00:35:40.680]   used an efficient net. So I wish I had figured that out. But
[00:35:40.680 --> 00:35:45.000]   nonetheless, I, I found a few different types of transformers
[00:35:45.000 --> 00:35:48.520]   that worked well for me. And go ahead.
[00:35:48.520 --> 00:35:50.600]   Sanyam Bhutani: Sorry, I was just going to interrupt and ask
[00:35:50.600 --> 00:35:53.640]   how are you going about discovering these because even
[00:35:53.640 --> 00:35:56.920]   if you go to like, let's say the hugging face or TIM repo, TIM
[00:35:56.920 --> 00:35:59.920]   has like 500 models at this point, how are you selecting
[00:35:59.920 --> 00:36:00.360]   these?
[00:36:00.360 --> 00:36:04.200]   Chris Swenor: I'd love to say I did something more scientific,
[00:36:04.200 --> 00:36:07.120]   like, you know, literature analysis and whatnot. But
[00:36:07.120 --> 00:36:10.960]   really, what I did for a lot of it was SWIN, the SWIN transformer
[00:36:10.960 --> 00:36:16.080]   had been posted pretty early on. So that was pretty easy. But I
[00:36:16.080 --> 00:36:21.360]   just kind of went through his, the models folder on PyTorch
[00:36:21.360 --> 00:36:24.960]   image models and said, Oh, that looks interesting. And then I
[00:36:24.960 --> 00:36:28.400]   clicked on it and said, Is it a transformer or does it not use
[00:36:28.400 --> 00:36:32.120]   batch norm? If that was true, I gave it a shot. It was pretty
[00:36:32.120 --> 00:36:37.400]   simple, honestly. Yeah, normally, normally, I do more
[00:36:37.400 --> 00:36:41.920]   research, but you know, that was that. So I kind of, I kind of
[00:36:41.920 --> 00:36:44.320]   had a moment where I looked at the clock and said, I better
[00:36:44.320 --> 00:36:47.800]   figure out something, something to differentiate my solution,
[00:36:47.800 --> 00:36:51.480]   because I didn't really think that was going to do it. So the
[00:36:51.480 --> 00:36:56.240]   two things that I did were first I upped from my five folds to 10
[00:36:56.240 --> 00:37:00.280]   folds, because that just kind of helps final leaderboard scores.
[00:37:00.280 --> 00:37:04.360]   And then the other thing was, I went back to the previous pet
[00:37:04.360 --> 00:37:07.360]   finder competition and grabbed all of the train and test
[00:37:07.360 --> 00:37:11.160]   images. And I pseudo labeled them. That was another
[00:37:11.160 --> 00:37:14.240]   opportunity that I felt like I had for learning I had never
[00:37:15.040 --> 00:37:18.800]   gotten pseudo labeling to work for me. But I found that when I
[00:37:18.800 --> 00:37:22.560]   included all of those images, I tried I tried to do all kinds of
[00:37:22.560 --> 00:37:25.920]   things with filtering out bad images or whatever, but none of
[00:37:25.920 --> 00:37:29.560]   it helped. But what I found was that including those images
[00:37:29.560 --> 00:37:33.960]   really helped my cross validation. Like significantly,
[00:37:33.960 --> 00:37:36.600]   it was about you know, a point one or point two improvement for
[00:37:36.600 --> 00:37:39.160]   all my models, which for this competition was very, very
[00:37:39.160 --> 00:37:46.720]   significant. So what I did was I, I basically reckoned in my
[00:37:46.720 --> 00:37:49.880]   head, how much time do I have left in the competition? How
[00:37:49.880 --> 00:37:53.800]   many models do I want to train? And I saw how long pseudo
[00:37:53.800 --> 00:37:58.520]   labeling was going to take. And so I set up a giant run script
[00:37:58.520 --> 00:38:03.680]   to get all of the pseudo labeled models to run and they leave
[00:38:03.680 --> 00:38:08.400]   enough time for testing and whatnot. And I had, I was the
[00:38:08.400 --> 00:38:12.000]   original end date was on a Thursday and Kaggle had some
[00:38:12.000 --> 00:38:14.520]   issues that caused people to accidentally get banned or
[00:38:14.520 --> 00:38:18.320]   whatever. So they extended the deadline by a day. And thank
[00:38:18.320 --> 00:38:21.760]   goodness because I would have barely beaten the initial
[00:38:21.760 --> 00:38:22.480]   deadline.
[00:38:22.480 --> 00:38:30.200]   No, no, I didn't. Fortunately, I didn't. But I was so glad that
[00:38:30.200 --> 00:38:35.320]   people did. I cannot even tell you, Sanyam. But I so I finished
[00:38:35.320 --> 00:38:41.080]   up all the models just in the nick of time. And I, I have this
[00:38:41.080 --> 00:38:45.520]   linear ensembling script that I use for all of my competitions.
[00:38:45.520 --> 00:38:50.760]   threw them all in there got optimal model weights, and
[00:38:50.760 --> 00:38:57.080]   submitted it with absolutely no expectations. So yeah, so I can
[00:38:57.080 --> 00:39:00.640]   go into, into my solution a little bit more in detail. Let
[00:39:00.640 --> 00:39:02.200]   me pull up the GitHub.
[00:39:03.200 --> 00:39:13.560]   I was all almost tempted to start a thread where I could
[00:39:13.560 --> 00:39:17.560]   like farm a gold medal of the competition deadline extension
[00:39:17.560 --> 00:39:18.080]   memes.
[00:39:18.080 --> 00:39:23.960]   Have anything like that is meme gold. I mean, I think that some
[00:39:23.960 --> 00:39:27.360]   people on Kaggle should be competing for memes rather than
[00:39:27.360 --> 00:39:31.520]   anything else because they're, they're so good at it. Alright,
[00:39:31.520 --> 00:39:37.840]   can you see the GitHub window? I can use. Awesome. So here's my,
[00:39:37.840 --> 00:39:41.000]   my repository that it looks like I just threw everything into a
[00:39:41.000 --> 00:39:44.400]   bunch of high files, but I have a few directories that I get
[00:39:44.400 --> 00:39:46.960]   ignored because it's where all the data and checkpoints and
[00:39:46.960 --> 00:39:53.800]   stuff are. But the I would say it all kind of starts in the
[00:39:53.800 --> 00:39:56.880]   the train script, I can sort of jump into other scripts from
[00:39:56.880 --> 00:40:02.520]   here. But this is when I said that I got that, that that big
[00:40:02.520 --> 00:40:07.120]   run script together. It was all facilitated by the fact that I
[00:40:07.120 --> 00:40:12.600]   have this one master pipe pie file that has all my different
[00:40:12.600 --> 00:40:17.120]   command line arguments that I could just orchestrate a giant
[00:40:17.120 --> 00:40:21.360]   set of runs together and have it run. And that's one of the
[00:40:21.360 --> 00:40:25.000]   things I like more about the this sort of setup than running
[00:40:25.040 --> 00:40:30.080]   out of notebooks. There are solutions to batch notebooks.
[00:40:30.080 --> 00:40:33.640]   But I just find that, you know, eventually I'd like to get to a
[00:40:33.640 --> 00:40:37.240]   point where I define all of the knobs that I'm going to be
[00:40:37.240 --> 00:40:43.880]   tweaking. And then I zoom in a touch. Yeah. That's perfect.
[00:40:43.880 --> 00:40:48.480]   Awesome. So I kind of define all the knobs that I'm going to be
[00:40:48.480 --> 00:40:51.760]   tweaking and then I make them command line arguments. You can
[00:40:51.760 --> 00:40:58.400]   see I have a bunch up here. And so what I did to make this sort
[00:40:58.400 --> 00:41:02.080]   of tractable to keep all the experiments in my head was, as
[00:41:02.080 --> 00:41:07.120]   we scroll down here, I define my weights and biases logger here.
[00:41:07.120 --> 00:41:11.520]   And you can see that I basically just named my models, what's
[00:41:11.520 --> 00:41:16.520]   the, what's the class of model? What seed am I running it on?
[00:41:17.280 --> 00:41:22.080]   What's the sort of experiment name? And then I delineated,
[00:41:22.080 --> 00:41:25.080]   hey, these are my tenfold models. And so that's what I
[00:41:25.080 --> 00:41:29.240]   mean when I say that experiment tracking was was absolutely,
[00:41:29.240 --> 00:41:35.400]   completely necessary for this. So nothing really exciting from
[00:41:35.400 --> 00:41:39.480]   here on out. I just run the trainer, PyTorch Lightning
[00:41:39.480 --> 00:41:42.640]   trainer, shout out to PyTorch Lightning, my favorite deep
[00:41:42.640 --> 00:41:47.320]   learning library. And, and then log some final metrics of
[00:41:47.320 --> 00:41:53.920]   weights and biases. So I can get into my model definition, and
[00:41:53.920 --> 00:41:56.840]   then kind of talk about how I set that up, because that's
[00:41:56.840 --> 00:42:03.600]   another important part of this. So what I did here was, I set up
[00:42:03.600 --> 00:42:08.240]   a model that was just built around all of those knobs that
[00:42:08.240 --> 00:42:10.960]   I'm tweaking, you can see all the arguments here are just
[00:42:11.160 --> 00:42:17.320]   hyperparameter tuner tuning bits. So everything from model
[00:42:17.320 --> 00:42:24.840]   dropout to stochastic, we call it stochastic depth, mix up and
[00:42:24.840 --> 00:42:29.520]   mix up parameters and cut mix and, and then whether or not,
[00:42:29.520 --> 00:42:32.840]   there were a lot of people that treated the competition like a
[00:42:32.840 --> 00:42:36.280]   classification problem, since it was like bounded regression.
[00:42:36.280 --> 00:42:40.400]   And that's what I mean here. And that is something I did along
[00:42:40.400 --> 00:42:45.640]   with just about everybody else. And then I set up a class, this
[00:42:45.640 --> 00:42:50.480]   backbone class, not really that interesting, because Tim create
[00:42:50.480 --> 00:42:56.320]   model the the PyTorch image models library basically takes
[00:42:56.320 --> 00:42:58.800]   care of this for you. But I didn't really understand that at
[00:42:58.800 --> 00:43:01.800]   the beginning of competition. So I just set up a backbone that
[00:43:01.800 --> 00:43:06.080]   could take any model name and form it into a CNN vision
[00:43:06.080 --> 00:43:11.360]   transformer backbone, pooling, and a simple linear head. And
[00:43:11.360 --> 00:43:14.360]   that was really all there was from a modeling perspective,
[00:43:14.360 --> 00:43:19.360]   every one of my models followed that very dead simple as basic
[00:43:19.360 --> 00:43:24.600]   as it gets structure. And then from here on out, it's just
[00:43:24.600 --> 00:43:28.680]   defining whether or not I'm doing pooling based on whether
[00:43:28.680 --> 00:43:31.480]   or not it's a transformer model, I'm sure there's a better way of
[00:43:31.480 --> 00:43:35.960]   doing it. And then just, here's my optimizer, here's my training
[00:43:36.000 --> 00:43:42.400]   step. Here's my validation step. And then here's how I here's
[00:43:42.400 --> 00:43:46.240]   how I did some, some more experiment logging. I mentioned
[00:43:46.240 --> 00:43:50.920]   before that I that I plotted my prediction distribution using
[00:43:50.920 --> 00:43:53.840]   weights and biases. And that's just me doing that right there.
[00:43:53.840 --> 00:44:01.040]   Directly copied code from a weights and biases collab. Very
[00:44:01.040 --> 00:44:04.200]   proud of that. And then I have some other things down here that
[00:44:04.240 --> 00:44:08.440]   I didn't end up using, but I subclassed my initial model and
[00:44:08.440 --> 00:44:13.640]   made a model that just returned the the step before the head in
[00:44:13.640 --> 00:44:17.520]   case I wanted to do some support support vector machine boosting.
[00:44:17.520 --> 00:44:20.480]   I'm disappointed I didn't find a way to make that work better
[00:44:20.480 --> 00:44:23.280]   because the winning a lot of the winning solutions did that,
[00:44:23.280 --> 00:44:30.200]   including the number one solution. And thanks again to
[00:44:30.200 --> 00:44:32.920]   Christie, I think I mentioned him three times now because he
[00:44:32.920 --> 00:44:34.440]   pointed that out early on in the competition.
[00:44:34.440 --> 00:44:35.400]   Sanyam Bhutani: He's an absolute legend.
[00:44:35.400 --> 00:44:38.160]   Chris Swenor: It's incredible. I mean, I've learned so much from
[00:44:38.160 --> 00:44:42.680]   him. It's I should probably pay him 40 grand or something for a
[00:44:42.680 --> 00:44:49.280]   year's worth of master's level education. And then I have a
[00:44:49.280 --> 00:44:54.360]   sort of not very interesting validation script that I use to
[00:44:54.360 --> 00:44:59.880]   actually generate out of fold predictions. And then I throw
[00:44:59.880 --> 00:45:04.160]   all of that into this out of fold option optimization script.
[00:45:04.160 --> 00:45:08.840]   The guy I cannot remember his name specifically, I think it
[00:45:08.840 --> 00:45:14.480]   was I think it's your own job, who, who was working on I
[00:45:14.480 --> 00:45:17.800]   cannot remember, it might have been the mechanisms of action
[00:45:17.800 --> 00:45:21.520]   competition. And he put together this sort of out of fold
[00:45:21.520 --> 00:45:27.760]   optimization framework using sci pi. And I have used that in any
[00:45:27.760 --> 00:45:31.600]   competition, I can use it in from that point on. And so I
[00:45:31.600 --> 00:45:36.120]   just throw everything, all of my out of folds into this one
[00:45:36.120 --> 00:45:44.120]   dictionary, and then I just use the sci pi minimize to, to
[00:45:44.120 --> 00:45:49.280]   figure out what the optimal weights for an ensemble are. And
[00:45:49.280 --> 00:45:55.200]   you absolutely can do it for RMSE using there was I think
[00:45:55.200 --> 00:45:59.000]   it's called big chaos, the the Netflix solution, they, they did
[00:45:59.000 --> 00:46:02.920]   all the linear algebra to figure out the sort of optimal equation
[00:46:02.920 --> 00:46:06.000]   you can use to solve it directly. I already had this
[00:46:06.000 --> 00:46:09.160]   setup. So I didn't really bother figuring that out. Even though,
[00:46:09.160 --> 00:46:11.920]   in retrospect, I probably should have because it's probably
[00:46:11.920 --> 00:46:16.600]   better. But so I use this out of fold optimization to figure out
[00:46:16.600 --> 00:46:22.280]   how to how to blend all my models together. And yeah, I
[00:46:22.280 --> 00:46:26.120]   mean, that's really it, I guess I can show the the run script
[00:46:26.120 --> 00:46:28.560]   what I mean, I have a lot of it commented out because I was just
[00:46:28.560 --> 00:46:32.160]   rerunning some that failed. But this is how I would run it, I
[00:46:32.160 --> 00:46:37.920]   would just say Python train ECA and F net with this name, with
[00:46:37.920 --> 00:46:41.320]   this learning rate, this weight decay, this batch size, as you
[00:46:41.320 --> 00:46:45.040]   can see, I have a puny little GPU, I just have an RTX 2070. So
[00:46:45.040 --> 00:46:50.880]   I could just get three in in in my GPU at once with 512 by 512
[00:46:50.920 --> 00:46:56.320]   image size. And I trained on three different seeds. And I
[00:46:56.320 --> 00:47:05.080]   did that with NF net l2, SWIN base 384 convit small, and then
[00:47:05.080 --> 00:47:09.920]   SWIN large 224, which isn't in here right now. And then Oh,
[00:47:09.920 --> 00:47:13.560]   well, then I guess I should say, I have just a modified version
[00:47:13.560 --> 00:47:18.480]   of the train script to do pseudo labeling on. This is the last
[00:47:18.480 --> 00:47:21.680]   thing I'll I'll point out where basically all I do differently
[00:47:21.680 --> 00:47:26.800]   is first I load. So if I'm training on folds one through
[00:47:26.800 --> 00:47:31.920]   nine, or yeah, one through 10, one through nine, I load in the
[00:47:31.920 --> 00:47:38.200]   model associated with with that training data. So I load in the
[00:47:38.200 --> 00:47:42.480]   the the model that was trained on the competition data folds
[00:47:42.480 --> 00:47:46.680]   one through 10. And then we're sorry, one through nine. And
[00:47:46.680 --> 00:47:50.040]   then I make predictions on the pseudo label data. So the
[00:47:50.040 --> 00:47:53.160]   competent the data from the previous competition, and then
[00:47:53.160 --> 00:47:57.080]   I just concatenate that in with my training data and then train
[00:47:57.080 --> 00:48:00.880]   a new model from scratch on all of that data. So that was how I
[00:48:00.880 --> 00:48:06.680]   did pseudo labeling. But yeah, that's, that's the whole thing.
[00:48:06.680 --> 00:48:10.440]   Sanyam Bhutani: It's such a beautiful solution, because I
[00:48:10.440 --> 00:48:14.000]   know many characters, not to crap on anyone, but many people
[00:48:14.000 --> 00:48:17.400]   wouldn't even put in the effort to structure it so nicely. And
[00:48:17.400 --> 00:48:20.760]   it's so nice and modular, and really easy to understand. So I
[00:48:20.760 --> 00:48:24.120]   think that's one of the really awesome things in your solution.
[00:48:24.120 --> 00:48:28.840]   Yeah, I have had I've been bit enough by solutions in the past
[00:48:28.840 --> 00:48:32.920]   where I'm scrolling through like 1000 lines of code just to get
[00:48:32.920 --> 00:48:36.640]   to the part where I train my model. And it drove me so up the
[00:48:36.640 --> 00:48:39.800]   wall that I just couldn't do it anymore. I had to I had to do it
[00:48:39.800 --> 00:48:42.440]   like this. I can't do it any other way anymore.
[00:48:43.480 --> 00:48:46.720]   I'd love to also understand how we're using weights and biases
[00:48:46.720 --> 00:48:49.960]   here. And how are you tracking your experiments? If at all? How
[00:48:49.960 --> 00:48:51.240]   was that useful, if at all?
[00:48:51.240 --> 00:48:56.680]   Sure, I'll, I'll open up my my dashboard, but I'll caveat it
[00:48:56.680 --> 00:49:01.200]   with gore warning or something. This is R rated. It's going to
[00:49:01.200 --> 00:49:04.720]   be a little disgusting. I'll try to hide because I'm using it for
[00:49:04.720 --> 00:49:08.960]   a new competition right now. So okay, I think we're, I think
[00:49:08.960 --> 00:49:14.360]   we're on that competition. Let me zoom this over.
[00:49:14.360 --> 00:49:16.160]   You're not sharing your screen.
[00:49:16.160 --> 00:49:18.320]   I know. I know. Yeah, I'm just making sure I don't share
[00:49:18.320 --> 00:49:25.760]   anything sensitive yet. So share, share screen. There we
[00:49:25.760 --> 00:49:29.240]   go. Alright, can you see my weights and biases screen?
[00:49:29.240 --> 00:49:32.280]   Yeah, I can see an infinite which means I don't get any
[00:49:32.280 --> 00:49:33.520]   secret information.
[00:49:36.000 --> 00:49:41.080]   Awesome. So I so basically, you can see the charts over here. I
[00:49:41.080 --> 00:49:45.800]   was just charting. There wasn't a ton to a ton of metrics
[00:49:45.800 --> 00:49:51.200]   specifically to log as far as plot plot worthy ones. So I had
[00:49:51.200 --> 00:49:56.280]   my RMSE loss, which was the actual competition metric. But
[00:49:56.280 --> 00:50:00.080]   as I mentioned before, a lot of people were were treating this
[00:50:00.080 --> 00:50:03.800]   as a classification problem. So I guess I can I can dive into
[00:50:03.800 --> 00:50:06.920]   that a tiny bit more to make this make more sense. But when
[00:50:06.920 --> 00:50:09.480]   you have a bounded regression problem, so in other words,
[00:50:09.480 --> 00:50:13.320]   it's never lower than zero, and it's never greater than 100. One
[00:50:13.320 --> 00:50:15.880]   thing that I've learned that was brought up in the fast AI
[00:50:15.880 --> 00:50:18.400]   course, and it was brought up a lot at the beginning of this
[00:50:18.400 --> 00:50:25.040]   competition is a nice thing to do is to actually run a sigmoid
[00:50:25.040 --> 00:50:28.720]   on your output, so that you can have a sigmoid that basically
[00:50:28.720 --> 00:50:34.840]   bounds it between zero and 100 or zero and one. And so you do
[00:50:34.840 --> 00:50:39.320]   that, and then you can train it on RMSE. But I found it worked a
[00:50:39.320 --> 00:50:43.680]   little bit better to, to optimize on binary cross entropy
[00:50:43.680 --> 00:50:46.960]   and just treat it like a classification problem. So
[00:50:46.960 --> 00:50:49.200]   that's where I was logging this over here. I didn't really pay
[00:50:49.200 --> 00:50:52.360]   too much attention to this, because RMSE would show
[00:50:52.360 --> 00:50:58.640]   basically the exact same sort of shape. So the I'd say that the
[00:50:58.640 --> 00:51:02.360]   best part about this, the two best parts is if I, if I click
[00:51:02.360 --> 00:51:09.680]   into one of these, you can see that I, I have the prediction
[00:51:09.680 --> 00:51:13.480]   distribution over time, plotted out. And so this was really nice
[00:51:13.480 --> 00:51:17.600]   just to sort of see, you know, are there are there are there
[00:51:17.600 --> 00:51:22.840]   areas of the target distribution that my model just isn't really
[00:51:22.840 --> 00:51:27.760]   picking up on? And, and as you can see, again, it was a really
[00:51:27.760 --> 00:51:31.440]   noisy metric. So you can see the prediction distribution shifting
[00:51:31.440 --> 00:51:34.840]   around over time. But, but generally, this looks like the
[00:51:34.840 --> 00:51:39.320]   target distribution, by no means is that saying that you have a
[00:51:39.320 --> 00:51:42.720]   perfect solution. In fact, it'd be very surprising if your
[00:51:42.720 --> 00:51:45.040]   prediction distribution didn't look like the target
[00:51:45.040 --> 00:51:48.200]   distribution, but it's at least a good sanity check to make
[00:51:48.200 --> 00:51:52.240]   sure, okay, I'm actually learning something here and my
[00:51:52.240 --> 00:51:57.120]   targets aren't corrupted or whatever. So and then another
[00:51:57.120 --> 00:52:00.720]   great thing about this is because I am running it from the
[00:52:00.720 --> 00:52:05.120]   command line, both, I can also see like, you know, how it's
[00:52:05.120 --> 00:52:08.960]   going in real time if I'm on my phone or whatever. But I also
[00:52:08.960 --> 00:52:12.520]   could go to the overview and see, oh, you know, I trained
[00:52:12.520 --> 00:52:16.960]   this, you know, forever ago. And, and I forget what
[00:52:16.960 --> 00:52:22.840]   parameters I use. So I could just go to my CLI. My, my, my
[00:52:22.840 --> 00:52:26.160]   command. See, yeah, I could just say, oh, I'm using pseudo
[00:52:26.160 --> 00:52:31.840]   train, which means I'm pseudo labeling this. Here's the model
[00:52:31.840 --> 00:52:35.120]   class here was the experiment type. And here were all my
[00:52:35.120 --> 00:52:39.800]   hyper parameters. So that I mean, it's just invaluable. And,
[00:52:39.800 --> 00:52:43.000]   and I was using collab to train a lot of the models. So I could
[00:52:43.000 --> 00:52:46.440]   see Oh, yeah, because I'm using a p 100. That means I was in I
[00:52:46.440 --> 00:52:52.160]   was in collab. So I again, like it, it was so invaluable for
[00:52:52.160 --> 00:52:54.520]   this, I cannot recommend people try it enough.
[00:52:54.520 --> 00:53:00.160]   Awesome. Thanks. Thanks for the. Yeah, I'm sure that you
[00:53:00.160 --> 00:53:00.840]   appreciate it.
[00:53:00.840 --> 00:53:04.440]   Of course, of course. But there's there's one more thing. I
[00:53:04.440 --> 00:53:06.880]   hope they appreciate it, because it's a great piece of software.
[00:53:06.880 --> 00:53:11.040]   But I was naming all of my models, things that were easily
[00:53:11.040 --> 00:53:16.880]   searchable. So I would name, you know, all of the relevant
[00:53:16.880 --> 00:53:21.800]   experiments, like random resize ragog. If I search that you'll
[00:53:21.800 --> 00:53:26.640]   see, like all of the models that went into my final solution,
[00:53:26.640 --> 00:53:30.400]   because they were all based on the same, the same like
[00:53:30.400 --> 00:53:33.880]   augmentation and experiment setup. So it was really, really
[00:53:33.880 --> 00:53:36.640]   easy to filter by the things I want. Now I know that there's a
[00:53:36.640 --> 00:53:39.880]   grouping functionality. I'm going to be honest, I didn't
[00:53:39.880 --> 00:53:42.800]   have time to figure it out. So that when I say there's gore in
[00:53:42.800 --> 00:53:44.000]   here, that's what I mean.
[00:53:48.720 --> 00:53:52.400]   The audience, I again want to remind everyone that Mark bagged
[00:53:52.400 --> 00:53:55.520]   the metal in two, three weeks worth of efforts. And usually
[00:53:55.520 --> 00:53:59.560]   you see people submitting at least 50 plus models to the
[00:53:59.560 --> 00:54:02.880]   Kaggle leaderboard. I just saw 17 next to his name. So that
[00:54:02.880 --> 00:54:06.960]   really speaks to how much how much he was able to accomplish
[00:54:06.960 --> 00:54:07.880]   in such a short time.
[00:54:07.880 --> 00:54:10.880]   Hey, get it right on every submission. That's what I say.
[00:54:10.880 --> 00:54:15.920]   Awesome. So I believe that concludes the solution
[00:54:15.920 --> 00:54:19.560]   overview. Any things you want to add here or any things before
[00:54:19.560 --> 00:54:21.280]   we move on to the community? Ask me anything.
[00:54:21.280 --> 00:54:26.240]   Trust your cross validation. If I if I had chased the public
[00:54:26.240 --> 00:54:30.400]   leaderboard, I'd be in like 700th place right now. Again,
[00:54:30.400 --> 00:54:33.040]   like I said, I was in 400 something place at the end of
[00:54:33.040 --> 00:54:35.480]   the competition, and I knew there was going to be shake up.
[00:54:35.480 --> 00:54:38.680]   I could not have imagined it was going to be shake up like that,
[00:54:38.680 --> 00:54:39.000]   though.
[00:54:39.000 --> 00:54:44.360]   Awesome. That's that's one comment that I always keep
[00:54:44.360 --> 00:54:49.440]   hearing from Kagglers. That's reassuring. Someone's asking how
[00:54:49.440 --> 00:54:52.720]   many hours on an average do you usually spend on Kaggle?
[00:54:52.720 --> 00:54:57.680]   Oh, there is no average week on Kaggle. It varies so heavily by
[00:54:57.680 --> 00:55:01.320]   how much stuff I have in my free time. I do I do a lot of stuff.
[00:55:01.320 --> 00:55:05.600]   I commit myself to a lot of things. And so if I have a if I
[00:55:05.600 --> 00:55:09.960]   have a free week, and you know, everyone else in my life is
[00:55:09.960 --> 00:55:15.280]   busy, you know, I'll spend most days after work, like, just
[00:55:15.280 --> 00:55:20.440]   about every hour I can get. But a lot of weeks, it's like I can
[00:55:20.440 --> 00:55:26.640]   work on it one or two days. And then the rest of the days, I set
[00:55:26.640 --> 00:55:29.440]   up my script so that I can run experiments in the background.
[00:55:29.440 --> 00:55:32.560]   And so I'm not actively sitting there. I'm just letting
[00:55:32.560 --> 00:55:35.720]   experiments run and seeing that I come back the next day and see
[00:55:35.720 --> 00:55:36.720]   what actually worked.
[00:55:38.320 --> 00:55:42.800]   That makes sense. Yeah. The next one is what is your key
[00:55:42.800 --> 00:55:45.400]   learning after doing so many competitions?
[00:55:45.400 --> 00:55:50.080]   I haven't won any competition. So let's I'll correct the
[00:55:50.080 --> 00:55:54.720]   comments are there. But um, you know, it's a lot of it is just
[00:55:54.720 --> 00:56:01.040]   like, you know, you can read a research paper. And it'll tell
[00:56:01.040 --> 00:56:03.560]   you that it's state of the art. It's the best model out there.
[00:56:03.560 --> 00:56:06.640]   But there is no better pressure cooker than a Kaggle
[00:56:06.640 --> 00:56:08.960]   competition. I'm serious. I mean, for any supervised
[00:56:08.960 --> 00:56:12.960]   learning task, it's, does it work on Kaggle? I mean, you
[00:56:12.960 --> 00:56:16.240]   know, put up or shut up kind of thing. And, you know, I respect
[00:56:16.240 --> 00:56:19.360]   all the researchers and the work that they put in. But a lot of
[00:56:19.360 --> 00:56:23.600]   it is just you got to try stuff for yourself to see if it works.
[00:56:23.600 --> 00:56:28.400]   And, you know, you can't sit there and read discussions of
[00:56:28.400 --> 00:56:30.680]   what other people are trying. Because if it's out in the
[00:56:30.680 --> 00:56:34.840]   public, it's no longer going to differentiate your solution. So
[00:56:35.920 --> 00:56:40.240]   I mean, with some exceptions, but, you know, you got to just
[00:56:40.240 --> 00:56:43.440]   try new things. And trying new things is the best way to learn.
[00:56:43.440 --> 00:56:46.760]   I mean, you know, I just listened to a couple of your
[00:56:46.760 --> 00:56:52.400]   other podcasts last week. And I think it was data source that
[00:56:52.400 --> 00:56:54.840]   said from someone else that, you know, if you didn't code, you
[00:56:54.840 --> 00:56:59.080]   didn't learn. And I mean, absolutely. You know, you learn
[00:56:59.080 --> 00:57:03.200]   so much. You learn so much by just sitting down and forcing
[00:57:03.200 --> 00:57:06.720]   yourself to do it. You know, it's it's super easy to go into
[00:57:06.720 --> 00:57:10.720]   a public kernel from a guy like Chris deod or someone and just
[00:57:10.720 --> 00:57:14.000]   copy it. All right, that works. You know, I trust them. They
[00:57:14.000 --> 00:57:16.440]   work at yo Nvidia or wherever they must know what they're
[00:57:16.440 --> 00:57:19.800]   doing. But you will never know what you're doing unless you sit
[00:57:19.800 --> 00:57:21.520]   there and just do it yourself.
[00:57:21.520 --> 00:57:25.680]   Sanyam Bhutani: I saw a tweet by CPMP who's one of the most
[00:57:25.680 --> 00:57:29.480]   respected Kagglers who said I only joined competition where he
[00:57:29.560 --> 00:57:33.720]   he only joins competition where he knows he can learn more and
[00:57:33.720 --> 00:57:36.840]   not the ones where he can do well. And then Jeeba who's a
[00:57:36.840 --> 00:57:40.320]   Kaggle legend, former number one absolute legend in the Kaggle
[00:57:40.320 --> 00:57:41.840]   world replied, I do the same.
[00:57:41.840 --> 00:57:47.440]   Get out of here. No, nobody wants to talk to you. All right,
[00:57:47.440 --> 00:57:49.960]   you guys are too good. You're making the rest of us look bad.
[00:57:49.960 --> 00:57:53.440]   They're like, yeah, I want to enter to learn and then they
[00:57:53.440 --> 00:57:56.440]   just win. It's like, Oh, great. I want another competition.
[00:57:57.680 --> 00:58:03.920]   I was trying to find the tweet. And then somewhere Jeeba
[00:58:03.920 --> 00:58:05.240]   replied, same here.
[00:58:05.240 --> 00:58:11.680]   Yeah, and I mean, jokes aside, I mean, I said it, they said it
[00:58:11.680 --> 00:58:15.720]   any any successful Kaggler I've seen has the same mentality. And
[00:58:15.720 --> 00:58:19.280]   eventually, the things that you don't know start to get reduced
[00:58:19.280 --> 00:58:22.360]   into smaller and smaller parts of the problem. And so the
[00:58:22.360 --> 00:58:27.040]   reason why they're able to say that and still win is because
[00:58:27.200 --> 00:58:30.680]   it's not like they start programming with absolutely no
[00:58:30.680 --> 00:58:33.000]   base of knowledge on the problem. You know, there's a
[00:58:33.000 --> 00:58:36.760]   specific part that they want to learn. And so that's why they
[00:58:36.760 --> 00:58:39.600]   can still do so well. So they're, they're incredible for
[00:58:39.600 --> 00:58:40.160]   that reason.
[00:58:40.160 --> 00:58:44.560]   Sanyam Bhutani: 100% This is a question from me. How did
[00:58:44.560 --> 00:58:47.240]   Kaggle help you in your professional journey? Did it
[00:58:47.240 --> 00:58:50.600]   help you at work? And did you see any learnings bleeding over
[00:58:50.600 --> 00:58:55.040]   to the real world? I think that's a BS argument that
[00:58:55.040 --> 00:58:57.560]   everyone makes that Kaggle doesn't help you in the real
[00:58:57.560 --> 00:58:59.960]   world. I know it does. I'm curious, how did it help you?
[00:58:59.960 --> 00:59:03.640]   Matt Moccero: Model validation is probably one of the biggest
[00:59:03.640 --> 00:59:08.040]   parts. It's so hard. And, you know, I'm in a master's program
[00:59:08.040 --> 00:59:12.240]   right now, just to sort of shore out a lot of the sort of like
[00:59:12.240 --> 00:59:15.880]   Bayesian statistics, things that I missed entering the field from
[00:59:15.880 --> 00:59:20.160]   the outside. And what I've realized is it's a great, it's a
[00:59:20.160 --> 00:59:23.160]   great program. It's the Georgia Tech online Masters of
[00:59:23.160 --> 00:59:27.000]   Analytics, it's like $10,000 for the entire program. I mean, you
[00:59:27.000 --> 00:59:31.800]   cannot get a better deal than that. But that program and every
[00:59:31.800 --> 00:59:35.520]   other program, they just do not teach model validation very
[00:59:35.520 --> 00:59:39.040]   well. And I'm fortunate to work alongside some incredibly
[00:59:39.040 --> 00:59:41.440]   talented data scientists. I mean, I work with people every
[00:59:41.440 --> 00:59:45.360]   day that blow my mind. And I think that one of the things
[00:59:45.360 --> 00:59:49.440]   that I can bring, that differentiates me is model
[00:59:49.440 --> 00:59:53.240]   validation, because it's just so hard to find good resources.
[00:59:53.240 --> 00:59:57.040]   And you will never sit down in an academic course and be able
[00:59:57.040 --> 01:00:00.680]   to learn top tier model validation, because it always
[01:00:00.680 --> 01:00:04.800]   comes from trying a ton of different competitions and
[01:00:04.800 --> 01:00:08.320]   seeing all these, the niche differences with every
[01:00:08.320 --> 01:00:11.880]   competition and what you have to do to get around those problems.
[01:00:11.880 --> 01:00:15.840]   And you start to see it enough that things start repeating, but
[01:00:15.840 --> 01:00:19.320]   it's not after you've seen a lot of problems before that
[01:00:19.320 --> 01:00:24.960]   starts happening. That and just staying on the cutting edge with
[01:00:24.960 --> 01:00:28.280]   deep learning. If you aren't on the cutting edge, you will not
[01:00:28.280 --> 01:00:32.880]   be able to compete in a lot on a lot of Kaggle competitions. It's
[01:00:32.880 --> 01:00:35.960]   becoming less and less frequent in Kaggle that you're doing a
[01:00:35.960 --> 01:00:38.800]   competition that isn't with deep learning. You know, there's some
[01:00:38.800 --> 01:00:42.200]   market prediction competitions out there that I'm not in that
[01:00:42.200 --> 01:00:46.560]   probably are more like light GBM, XGBoost focused, but I'm
[01:00:46.560 --> 01:00:49.320]   sure that there will be deep learning in there. And if you,
[01:00:49.320 --> 01:00:52.360]   if you always want to have a competition that you can compete
[01:00:52.360 --> 01:00:55.200]   in on Kaggle, you have to know deep learning, you have to know
[01:00:55.200 --> 01:00:59.480]   NLP, you have to know computer vision. And so being on that
[01:00:59.480 --> 01:01:05.000]   cutting edge has given me sort of newer ideas where if you're
[01:01:05.000 --> 01:01:10.360]   not immersed in your free time, you'll be a little bit behind
[01:01:10.360 --> 01:01:10.680]   on.
[01:01:10.680 --> 01:01:14.560]   Sanyam Bhutani: I want to go back to a previous point, not
[01:01:14.560 --> 01:01:18.120]   that I actively compete, maybe I'm just scared, but I get time
[01:01:18.120 --> 01:01:20.760]   to sometimes read the forums. And I remember seeing Swin
[01:01:20.760 --> 01:01:24.880]   transformer on there way before the current research papers
[01:01:24.880 --> 01:01:29.480]   started citing it as really powerful. So if you want to
[01:01:29.480 --> 01:01:33.720]   really apply those models, every paper has an incentive really to
[01:01:33.720 --> 01:01:37.120]   you know, you won't say, Hey, I created this, this thing, it's
[01:01:37.120 --> 01:01:40.640]   not good, you know, and says that. So people always claim
[01:01:40.640 --> 01:01:44.840]   that this is the best, but that's how you really keep up to
[01:01:44.840 --> 01:01:47.760]   date with cutting edge also, in my opinion, by taking that model
[01:01:47.760 --> 01:01:49.960]   and testing it against others on a leaderboard.
[01:01:49.960 --> 01:01:53.680]   Chris Swenord: Yeah, I actually saw in the gravitational wave
[01:01:53.680 --> 01:01:57.480]   challenge, not too long ago, I saw a researcher who invented
[01:01:57.480 --> 01:02:01.760]   some, oh, gosh, I can't even do it justice. But it was some,
[01:02:01.760 --> 01:02:07.040]   some like type of model that was a spin off of a transformer. I
[01:02:07.040 --> 01:02:10.440]   know it was time it was at a time where a lot of sort of
[01:02:10.440 --> 01:02:14.040]   Fourier type attention mechanisms and mechanisms were
[01:02:14.040 --> 01:02:20.160]   being introduced. And he had his own take on it. And he applied
[01:02:20.160 --> 01:02:23.120]   it to the competition and got a super high leaderboard score.
[01:02:23.120 --> 01:02:28.080]   And he had a YouTube, YouTube video that was sort of
[01:02:28.080 --> 01:02:31.640]   describing his research that was up before the competition. And I
[01:02:31.640 --> 01:02:35.600]   saw the view count just skyrocket from the competition.
[01:02:35.600 --> 01:02:39.840]   Because, you know, he put it out there publicly, it worked, he
[01:02:39.840 --> 01:02:42.400]   did, he had a very high leaderboard position for the
[01:02:42.400 --> 01:02:48.360]   time. And posted publicly, hey, this is the model I'm using. And
[01:02:48.360 --> 01:02:51.000]   and yeah, I mean, there's no better way to prove to people
[01:02:51.000 --> 01:02:53.240]   that your research works and go throw it in a Kaggle
[01:02:53.240 --> 01:02:53.920]   competition.
[01:02:53.920 --> 01:02:59.560]   Sanyam Bhutani: Totally. Yeah. Again, these are in random order,
[01:02:59.560 --> 01:03:02.360]   I'm trying to structure them best, but still, they'll be a
[01:03:02.360 --> 01:03:06.360]   bit random. How do you if at all handle pre processing of large
[01:03:06.360 --> 01:03:07.960]   data sets and insects?
[01:03:08.880 --> 01:03:14.240]   More RAM? No, I'm kidding. You can you can learn a lot of
[01:03:14.240 --> 01:03:17.960]   tricks that are, you know, kind of good programming practices.
[01:03:17.960 --> 01:03:24.160]   You'll see a lot of things like just a lot of like settings like
[01:03:24.160 --> 01:03:28.520]   unnecessary joins or using pandas where you don't need to,
[01:03:28.520 --> 01:03:33.600]   it's hard to get into specifics, but I, you know, not filtering
[01:03:33.600 --> 01:03:37.240]   properly, not filtering often enough, not understanding that
[01:03:37.240 --> 01:03:40.080]   Python is a garbage collected language. That's a big one for
[01:03:40.080 --> 01:03:43.920]   me, I run into it all the time. And what I mean by that is, you
[01:03:43.920 --> 01:03:49.280]   know, you'll often have to delete a large data frame, and
[01:03:49.280 --> 01:03:54.560]   then actually run GC dot collect from the Python GC module to
[01:03:54.560 --> 01:03:59.160]   really reclaim that that memory. And it's a lot of things. This
[01:03:59.160 --> 01:04:01.040]   is one of the things that you can learn really well from
[01:04:01.040 --> 01:04:05.080]   public kernels, because the Kaggle notebooks are very
[01:04:05.080 --> 01:04:07.920]   resource constrained, they're only 16 gigabytes, I think I
[01:04:07.920 --> 01:04:12.400]   have 64 gigs on my home machine, which all my friends who aren't
[01:04:12.400 --> 01:04:15.960]   in data science, like, why do you? Why could you possibly do
[01:04:15.960 --> 01:04:20.240]   that? And I'm like, if I could get 128, I would I mean, like,
[01:04:20.240 --> 01:04:24.200]   if I could get 530 90s, I would but I'm stuck over here with my
[01:04:24.200 --> 01:04:29.200]   puny little 2070. I'm getting sidetracked. But I learned a lot
[01:04:29.200 --> 01:04:34.680]   of that in the MLB competition. Because the cool thing about the
[01:04:34.680 --> 01:04:38.720]   MLB competition was that they just threw all the data in the
[01:04:38.720 --> 01:04:42.200]   world at you, they threw everything from what day the
[01:04:42.200 --> 01:04:46.320]   games were played on what stadium, who what the rosters
[01:04:46.320 --> 01:04:49.760]   were, and then gave you pitch level data. So I mean, every
[01:04:49.760 --> 01:04:53.120]   pitch had a record in a data frame. And I was fortunate
[01:04:53.120 --> 01:04:55.520]   enough to have played with that data before because I'm such a
[01:04:55.520 --> 01:05:01.040]   big baseball fan, as I mentioned before. And so you really
[01:05:01.040 --> 01:05:03.920]   appreciated in that competition where it's like, I need to
[01:05:03.920 --> 01:05:07.040]   figure out what works before I start throwing everything at it
[01:05:07.040 --> 01:05:11.040]   because otherwise, I'm going to run out of RAM constantly. So it
[01:05:11.040 --> 01:05:14.720]   was a lot of like, how do I feature engineer faster? How do
[01:05:14.720 --> 01:05:19.880]   I cleverly filter? How do I you know, how do I set up
[01:05:19.880 --> 01:05:23.680]   experiments that that don't include that where I can try and
[01:05:23.680 --> 01:05:26.920]   do things quickly, but I'm not including too much data at once.
[01:05:26.920 --> 01:05:31.800]   And so it's kind of where that the programming skills come into
[01:05:31.800 --> 01:05:34.880]   play as a data scientist. And I would say that you you neglect
[01:05:34.880 --> 01:05:36.120]   those at your own peril.
[01:05:36.120 --> 01:05:39.960]   Sanyam Bhutani: Yeah, like you said, it's it's this constant
[01:05:39.960 --> 01:05:43.320]   things of errors that you keep learning from I remember, this
[01:05:43.320 --> 01:05:45.640]   is like, again, one of the stupid learnings from my
[01:05:45.640 --> 01:05:48.320]   process. I'm sure everyone knows this, but I was in the
[01:05:48.320 --> 01:05:51.640]   Microsoft malware competition where I was just going with the
[01:05:51.640 --> 01:05:55.480]   pandas defaults. And just even trying to fit a boosted tree on
[01:05:55.480 --> 01:06:00.200]   it would make my like 128 gig RAM overflow. And then I
[01:06:00.200 --> 01:06:03.080]   realized, oh, no, you don't go with the defaults, you send it
[01:06:03.080 --> 01:06:06.440]   to a low to low D type if you can, and you should for such a
[01:06:06.440 --> 01:06:07.040]   data set.
[01:06:07.040 --> 01:06:10.400]   Yeah, yeah. And there's a wonderful function that has
[01:06:10.400 --> 01:06:13.960]   floated around Kaggle for as long as possible reduce mem usage
[01:06:13.960 --> 01:06:17.480]   that goes through all of your pandas columns and turns them
[01:06:17.480 --> 01:06:21.320]   into the lowest, the lowest memory setting that that column
[01:06:21.320 --> 01:06:24.160]   can handle. And it's little tricks like that, that you pick
[01:06:24.160 --> 01:06:24.680]   up, you know?
[01:06:25.880 --> 01:06:33.480]   Absolutely. Is there a strange trick in your current solution
[01:06:33.480 --> 01:06:35.440]   that worked surprisingly well?
[01:06:35.440 --> 01:06:44.480]   Oh, I would say it was just strange how consistently batch
[01:06:44.480 --> 01:06:50.120]   norm models didn't work. And I would say if I had had the
[01:06:50.120 --> 01:06:53.760]   intuition to just freeze batch norm, I would have said that
[01:06:53.760 --> 01:06:58.080]   would have been it. But I would say the strange trick that
[01:06:58.080 --> 01:07:01.960]   worked. The strange trick that actually worked was just how
[01:07:01.960 --> 01:07:05.440]   well pseudo labeling worked. Because like I said, that target
[01:07:05.440 --> 01:07:10.200]   variable was so noisy. I did not expect it to work. I expected
[01:07:10.200 --> 01:07:13.840]   it to just add more noise to a solution and make my cross
[01:07:13.840 --> 01:07:17.480]   validation worse. Because when I was pseudo labeling, the
[01:07:17.480 --> 01:07:21.080]   majority of my training data was actually from the previous
[01:07:21.080 --> 01:07:25.480]   competition. I think it was maybe maybe 20% of the total
[01:07:25.480 --> 01:07:29.080]   data set that was actually from the current competition. So
[01:07:29.080 --> 01:07:34.240]   actually had a label on it. But there was a good blog post, the
[01:07:34.240 --> 01:07:37.720]   blog post or Kaggle notebook that I found again, from Chris
[01:07:37.720 --> 01:07:43.240]   de Yacht, where he where he wrote about why pseudo labeling
[01:07:43.240 --> 01:07:47.720]   works. And I won't get into it. But it basically is is super
[01:07:47.720 --> 01:07:51.600]   interesting to see that it's less about the accuracy of the
[01:07:51.600 --> 01:07:56.640]   target, and more so about giving your model a better perspective
[01:07:56.640 --> 01:08:02.040]   on the landscape of potential training data. So it can slice
[01:08:02.040 --> 01:08:03.480]   it more effectively.
[01:08:03.480 --> 01:08:09.080]   Sanyam Bhutani: Got it. I don't see any other questions. So I'll
[01:08:09.080 --> 01:08:12.320]   end up with the question which I always end up with what's one
[01:08:12.320 --> 01:08:15.320]   best advice you would tell to someone who's just starting the
[01:08:15.320 --> 01:08:15.840]   journey?
[01:08:16.600 --> 01:08:20.800]   Um, I'll turn it into two because I can't decide. But
[01:08:20.800 --> 01:08:23.720]   number one is take the cross validation, or the sorry, the
[01:08:23.720 --> 01:08:30.040]   fast AI course, fast AI, if you can just take one course, it's
[01:08:30.040 --> 01:08:34.080]   the best. I mean, it's great. I don't know about the newest
[01:08:34.080 --> 01:08:37.200]   versions of it, because I haven't perused those. But it's
[01:08:37.200 --> 01:08:41.880]   truly it's not just deep learning zero to hero, it's
[01:08:41.960 --> 01:08:45.880]   generally programming and machine learning zero to hero.
[01:08:45.880 --> 01:08:50.600]   I mean, he's basically starts with very little assumption of
[01:08:50.600 --> 01:08:55.080]   Python knowledge. And you go from implementing, you go from
[01:08:55.080 --> 01:08:59.880]   that to implementing a ton of PyTorch modules. Like if you
[01:08:59.880 --> 01:09:04.160]   don't know how nn.lstm works under the hood or the LSTM cell,
[01:09:04.160 --> 01:09:09.880]   it's because you didn't take the fast AI course. So you know, he
[01:09:09.880 --> 01:09:14.920]   goes through everything from that to his wild take on Python
[01:09:14.920 --> 01:09:18.080]   and how you should be using it. And it teaches you so much about
[01:09:18.080 --> 01:09:21.440]   the language, too. So now you have to be a good programmer.
[01:09:21.440 --> 01:09:25.000]   And the second bit that I kind of slide in at the end after I
[01:09:25.000 --> 01:09:27.840]   say that is model validation, learn how to validate your
[01:09:27.840 --> 01:09:29.280]   models or they're useless.
[01:09:29.280 --> 01:09:34.480]   Awesome. You saved me the same in the effort of saying faster
[01:09:34.480 --> 01:09:36.480]   is the best deep learning course. So thanks for saying
[01:09:36.480 --> 01:09:36.760]   that.
[01:09:37.360 --> 01:09:40.120]   And learn weights and biases. I mean, come on, how else are you
[01:09:40.120 --> 01:09:41.920]   gonna know what experiments work?
[01:09:41.920 --> 01:09:45.280]   Thanks. Thanks for the shout out. Of course, of course, of
[01:09:45.280 --> 01:09:45.520]   course.
[01:09:45.520 --> 01:09:48.920]   Any anything we missed in the interview that you want to cover
[01:09:48.920 --> 01:09:50.560]   before we conclude the live stream?
[01:09:50.560 --> 01:09:55.160]   I don't really think so. I think I just like to, you know, thank
[01:09:55.160 --> 01:10:00.560]   8451 one more time for for really, you know, taking
[01:10:00.560 --> 01:10:03.520]   somebody in who didn't really have much of a data science
[01:10:03.520 --> 01:10:07.120]   background at first and making me helping mold me into the, you
[01:10:07.120 --> 01:10:10.760]   know, the person I am now because Kaggle can teach you all
[01:10:10.760 --> 01:10:13.520]   the great things about modeling, but there's so much about like
[01:10:13.520 --> 01:10:17.720]   data engineering that you just don't get on Kaggle. You know, I
[01:10:17.720 --> 01:10:20.640]   would always resent that when someone says that Kaggle isn't
[01:10:20.640 --> 01:10:23.080]   applicable to the real world, because I would say that you
[01:10:23.080 --> 01:10:25.920]   don't understand how important model validation is, if you're
[01:10:25.920 --> 01:10:31.600]   saying that. But there's so much with like, how you use databases
[01:10:31.600 --> 01:10:35.320]   and how you parse, you know, billions of records in a data
[01:10:35.320 --> 01:10:38.240]   set, and things like that, that you just don't get as well in
[01:10:38.240 --> 01:10:38.760]   Kaggle.
[01:10:38.760 --> 01:10:44.800]   Awesome. Thanks. Thanks for that. I'll quickly mention that
[01:10:44.800 --> 01:10:49.040]   Mark is quite active on Twitter, you can you should follow him
[01:10:49.040 --> 01:10:52.520]   there. I won't say you can. He posts really insightful threads
[01:10:52.520 --> 01:10:56.960]   that I would highly encourage everyone to check out. Any other
[01:10:56.960 --> 01:10:59.360]   platforms that you want to highlight? I know you don't post
[01:10:59.360 --> 01:11:01.880]   as much on Kaggle. I hope you share your insights there as
[01:11:01.880 --> 01:11:04.560]   well. That might change hopefully, if I can get you to
[01:11:04.560 --> 01:11:04.960]   do that.
[01:11:05.600 --> 01:11:08.800]   Yeah, um, you know, one of the one of the issues with Twitter
[01:11:08.800 --> 01:11:13.120]   is that, you know, you have to fight with how much nuance to
[01:11:13.120 --> 01:11:17.360]   include in your posts when you only have 280, 280 characters
[01:11:17.360 --> 01:11:20.320]   per tweet. And so that's why I like posting threads a lot
[01:11:20.320 --> 01:11:23.240]   because I can write more, but inevitably, I leave, I leave a
[01:11:23.240 --> 01:11:27.520]   lot of details out. So I'm going to introduce like a newsletter
[01:11:27.520 --> 01:11:31.040]   or a blog, it'll probably be one of those things to sort of maybe
[01:11:31.040 --> 01:11:34.680]   weekly or monthly wrap up a lot of the things that I tweeted
[01:11:34.680 --> 01:11:39.360]   about things I saw and just kind of put a bow on a lot of them. I
[01:11:39.360 --> 01:11:41.920]   don't have anything on that now, but I'll promote it on my on my
[01:11:41.920 --> 01:11:45.680]   Twitter page once I have it. So so keep an eye out for that.
[01:11:45.680 --> 01:11:50.240]   Amazing. With that, I'd love to wrap up the live stream. Mark,
[01:11:50.240 --> 01:11:53.000]   thank you so much again for such an insightful interview and
[01:11:53.000 --> 01:11:56.720]   sharing your solution, sharing your insights and tips that I
[01:11:56.720 --> 01:11:59.800]   think I always learn so much, but I hope the audience did
[01:11:59.800 --> 01:12:03.760]   learn a lot as well. And yeah, with that, I'd also love to
[01:12:03.760 --> 01:12:07.200]   thank everyone who is watching the recording or tuned in. So
[01:12:07.200 --> 01:12:07.920]   thanks, everyone.
[01:12:07.920 --> 01:12:10.520]   Yeah, thank you so much, Sanyam. It was a pleasure.
[01:12:10.520 --> 01:12:13.480]   Thanks, everyone. Bye bye.
[01:12:13.480 --> 01:12:16.060]   (upbeat music)

