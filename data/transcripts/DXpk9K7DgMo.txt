
[00:00:00.000 --> 00:00:03.140]   Today we're gonna talk about using open source models
[00:00:03.140 --> 00:00:05.480]   in Hugging Face and LangChain.
[00:00:05.480 --> 00:00:07.400]   We're going to be focusing specifically
[00:00:07.400 --> 00:00:10.360]   on the MPT7B model,
[00:00:10.360 --> 00:00:12.600]   which I'm sure some of you have heard
[00:00:12.600 --> 00:00:16.600]   as one of these fine-tuned versions of this model
[00:00:16.600 --> 00:00:21.600]   actually has a context window of 65,000 tokens,
[00:00:21.600 --> 00:00:25.000]   which is pretty huge.
[00:00:25.000 --> 00:00:28.200]   At the moment of recording this video,
[00:00:28.200 --> 00:00:32.280]   GPT-4, the one that's generally available to people,
[00:00:32.280 --> 00:00:35.800]   has a context window of 8,000 tokens,
[00:00:35.800 --> 00:00:40.200]   and they have a version that goes up to 32,000,
[00:00:40.200 --> 00:00:42.000]   but I'm actually not aware of anyone
[00:00:42.000 --> 00:00:44.280]   that has access to that at the moment.
[00:00:44.280 --> 00:00:49.280]   So basically we're limited with GPT-4 to 8,000 tokens.
[00:00:49.280 --> 00:00:53.880]   Now, MPT7B, like I said,
[00:00:53.880 --> 00:00:55.920]   we can have that super huge model,
[00:00:55.920 --> 00:00:57.720]   but there are also a lot of other models
[00:00:57.720 --> 00:00:59.120]   that are available as well.
[00:00:59.120 --> 00:01:02.200]   So let me just go ahead and show you those very quickly.
[00:01:02.200 --> 00:01:04.480]   So just head over to Hugging Face,
[00:01:04.480 --> 00:01:06.920]   which is where we're gonna pull these models from.
[00:01:06.920 --> 00:01:08.960]   And you can see actually straight away,
[00:01:08.960 --> 00:01:10.840]   we have these four models.
[00:01:10.840 --> 00:01:15.840]   So the MPT7B is the core, that's the pre-trained model,
[00:01:15.840 --> 00:01:17.840]   that's the foundation model.
[00:01:17.840 --> 00:01:19.640]   Then we have StoryWriter, Chat, and Instruct.
[00:01:19.640 --> 00:01:21.280]   These are all fine-tuned models.
[00:01:21.280 --> 00:01:24.000]   So StoryWriter is the one you've probably heard about,
[00:01:24.000 --> 00:01:29.000]   which has a max context window of 65,000 tokens,
[00:01:29.000 --> 00:01:30.880]   which is pretty huge.
[00:01:30.880 --> 00:01:33.640]   And in reality, it actually goes up to higher.
[00:01:33.640 --> 00:01:36.920]   So I believe they say, ah, here, right?
[00:01:36.920 --> 00:01:41.280]   So we demonstrate generations as long as 84,000 tokens,
[00:01:41.280 --> 00:01:44.400]   which is, I would say, pretty impressive.
[00:01:44.400 --> 00:01:47.240]   And then if we, actually, we can come over to here,
[00:01:47.240 --> 00:01:50.200]   scroll down, and we can see the other models as well.
[00:01:50.200 --> 00:01:52.640]   So we have this Chat model, the Instruct model,
[00:01:52.640 --> 00:01:54.160]   and obviously the Foundation model.
[00:01:54.160 --> 00:01:55.720]   We're gonna be using the Instruct model
[00:01:55.720 --> 00:02:00.200]   because, I mean, most of the use cases I see
[00:02:00.200 --> 00:02:04.600]   kind of rely on us providing instructions to these models.
[00:02:04.600 --> 00:02:07.960]   And therefore, I think most people out there
[00:02:07.960 --> 00:02:10.400]   actually are going to want to use this model, okay?
[00:02:10.400 --> 00:02:12.480]   Because, yeah, we can give it instructions,
[00:02:12.480 --> 00:02:15.280]   it's gonna be able to follow them better than the others.
[00:02:15.280 --> 00:02:18.840]   So, yeah, we're gonna see how we can use this both.
[00:02:18.840 --> 00:02:20.120]   So initially in HuggingFace,
[00:02:20.120 --> 00:02:22.000]   we're gonna see how we can load that into HuggingFace,
[00:02:22.000 --> 00:02:24.400]   and then we're gonna see how we can take that
[00:02:24.400 --> 00:02:25.960]   and actually load it into LimeChain,
[00:02:25.960 --> 00:02:28.000]   which obviously has a few more features
[00:02:28.000 --> 00:02:29.520]   on the agent side of things.
[00:02:29.520 --> 00:02:31.600]   Okay, so the first thing we're gonna want to do
[00:02:31.600 --> 00:02:35.240]   is actually do a few pip installs.
[00:02:35.240 --> 00:02:37.640]   So we have Transformers, Accelerate.
[00:02:37.640 --> 00:02:41.280]   So Accelerate, we need that in order to basically optimize
[00:02:41.280 --> 00:02:44.320]   how we're running this on our GPU.
[00:02:44.320 --> 00:02:46.040]   We will want to run this on a GPU,
[00:02:46.040 --> 00:02:48.240]   otherwise you're going to be waiting
[00:02:48.240 --> 00:02:50.000]   an impossibly long time.
[00:02:50.000 --> 00:02:53.600]   So, yeah, if you don't have access to a GPU,
[00:02:53.600 --> 00:02:57.360]   I would recommend you figure that out.
[00:02:57.360 --> 00:02:59.280]   So right now I'm running this on Colab,
[00:02:59.280 --> 00:03:01.160]   and actually there'll be a link to this notebook
[00:03:01.160 --> 00:03:03.000]   as well on the top of the video.
[00:03:03.000 --> 00:03:05.960]   So from Colab, you can run on GPU, okay?
[00:03:05.960 --> 00:03:09.920]   So you just go to Runtime, Change Runtime Type.
[00:03:09.920 --> 00:03:13.560]   You initially maybe on None, so you click GPU.
[00:03:13.560 --> 00:03:17.120]   GPU Type, so I'm using T4, which is the smallest one on here
[00:03:17.120 --> 00:03:19.080]   and the standard version of T4
[00:03:19.080 --> 00:03:22.080]   you can get on the free version of Colab.
[00:03:22.080 --> 00:03:25.720]   But for me, that wasn't actually big enough
[00:03:25.720 --> 00:03:30.720]   to run the MPT7B model, unfortunately.
[00:03:30.720 --> 00:03:35.760]   So I'm currently on Colab Pro now, thanks to this model.
[00:03:35.760 --> 00:03:41.280]   And with that, I can switch up to the high-RAM version.
[00:03:41.280 --> 00:03:43.440]   Now, obviously you have to pay for that,
[00:03:43.440 --> 00:03:46.640]   but you don't have to pay that much, okay?
[00:03:46.640 --> 00:03:49.800]   It's not a significant cost.
[00:03:49.800 --> 00:03:52.640]   But of course, I know this will be limiting for some people,
[00:03:52.640 --> 00:03:55.440]   but this is the best and cheapest option
[00:03:55.440 --> 00:03:57.240]   I can find right now.
[00:03:57.240 --> 00:03:59.920]   Okay, so back on the installers, we have INOPS.
[00:03:59.920 --> 00:04:02.880]   So this is, again, it's used by the MPT model.
[00:04:02.880 --> 00:04:04.760]   Naturally, we're gonna be using LangChain.
[00:04:04.760 --> 00:04:07.160]   And do I use Wikipedia here?
[00:04:07.160 --> 00:04:09.440]   I actually don't think I use this anymore.
[00:04:09.440 --> 00:04:12.120]   And Xform is just for an optimization
[00:04:12.120 --> 00:04:15.360]   in our transform functions.
[00:04:15.360 --> 00:04:17.520]   Okay, once we have all those installed,
[00:04:17.520 --> 00:04:18.360]   we come down here,
[00:04:18.360 --> 00:04:21.160]   and this is where we initialize the model.
[00:04:21.160 --> 00:04:22.760]   Okay, so like I said, we're gonna be using
[00:04:22.760 --> 00:04:25.200]   the instruct model, okay?
[00:04:25.200 --> 00:04:28.480]   One thing, so if you do want to use StoryWriter
[00:04:28.480 --> 00:04:31.360]   and you want to use that huge context window,
[00:04:31.360 --> 00:04:35.200]   you would go StoryWriter, okay?
[00:04:35.200 --> 00:04:37.400]   And then here, you would write,
[00:04:37.400 --> 00:04:42.160]   I don't know, what is it, 65,000, which is kind of nuts.
[00:04:42.160 --> 00:04:43.400]   But in order to run that,
[00:04:43.400 --> 00:04:46.280]   you're gonna definitely need more than a T4 GPU.
[00:04:46.280 --> 00:04:49.040]   Basically, the higher the max sequence length is,
[00:04:49.040 --> 00:04:52.360]   the bigger your GPU memory is going to need to be.
[00:04:52.360 --> 00:04:55.280]   So yeah, you need something big to run that.
[00:04:55.280 --> 00:04:57.800]   But we're just gonna stick with this, instruct.
[00:04:57.800 --> 00:05:02.640]   This 2048 is the typical or the standard sequence length
[00:05:02.640 --> 00:05:04.200]   for these other models.
[00:05:04.200 --> 00:05:07.360]   So instruct, or the base model, the foundation model,
[00:05:07.360 --> 00:05:08.640]   instruct and chat.
[00:05:08.640 --> 00:05:10.800]   And this is also something important.
[00:05:10.800 --> 00:05:13.120]   So the trust remote code, we have to have that
[00:05:13.120 --> 00:05:16.720]   because essentially the MPT models
[00:05:16.720 --> 00:05:19.920]   are not fully supported by HuggingFace yet.
[00:05:19.920 --> 00:05:22.720]   So we have to rely on this remote code
[00:05:22.720 --> 00:05:27.480]   that is basically stored in the model directory for this
[00:05:27.480 --> 00:05:30.840]   to set up all the endpoints and everything for the model.
[00:05:30.840 --> 00:05:35.200]   Okay, then we switch the model to evaluation mode.
[00:05:35.200 --> 00:05:39.560]   So that just switches a few options within the model
[00:05:39.560 --> 00:05:41.280]   that says, okay, we're not training,
[00:05:41.280 --> 00:05:43.240]   we're now performing inference.
[00:05:43.240 --> 00:05:45.000]   Okay, we're now doing predictions.
[00:05:45.000 --> 00:05:48.400]   And then we want to move our model to device.
[00:05:48.400 --> 00:05:50.880]   So the device, we decided here.
[00:05:50.880 --> 00:05:54.120]   Okay, so CUDA, and then we have CUDA current device.
[00:05:54.120 --> 00:05:55.640]   If we scroll down to the end here,
[00:05:55.640 --> 00:05:58.080]   we should see what that moved it to.
[00:05:58.080 --> 00:06:01.160]   Yeah, so model loaded to CUDA at zero.
[00:06:01.160 --> 00:06:02.560]   Now, just one thing,
[00:06:02.560 --> 00:06:05.160]   this takes a little bit of time to run, okay?
[00:06:05.160 --> 00:06:06.840]   Like here, it just took a minute.
[00:06:06.840 --> 00:06:08.920]   I think that's because most of the model
[00:06:08.920 --> 00:06:10.640]   was probably already downloaded for me.
[00:06:10.640 --> 00:06:12.640]   If you're downloading and initializing this,
[00:06:12.640 --> 00:06:17.120]   expect to wait like five, 10 minutes, at least on Colab.
[00:06:17.120 --> 00:06:18.920]   But once that has been downloaded,
[00:06:18.920 --> 00:06:23.920]   you should be good to use it to basically initialize it.
[00:06:23.920 --> 00:06:26.280]   And it will just take like a minute or so
[00:06:26.280 --> 00:06:28.440]   because you only need to download it once.
[00:06:28.440 --> 00:06:31.280]   Okay, and then we initialize our tokenizer.
[00:06:31.280 --> 00:06:34.080]   So the tokenizer is actually using this
[00:06:34.080 --> 00:06:38.160]   Luther AI's GPT Neox 20B.
[00:06:38.160 --> 00:06:40.040]   This is just, this is a tokenizer.
[00:06:40.040 --> 00:06:41.960]   So when I say tokenizer,
[00:06:41.960 --> 00:06:44.600]   it's basically the thing that will translate
[00:06:44.600 --> 00:06:47.760]   from human readable plain text
[00:06:47.760 --> 00:06:51.080]   to transformer or large language model,
[00:06:51.080 --> 00:06:53.200]   readable token IDs, right?
[00:06:53.200 --> 00:06:55.920]   So it's gonna convert like the word V
[00:06:55.920 --> 00:07:00.320]   into the token ID 41, for example, right?
[00:07:00.320 --> 00:07:03.720]   And then they get fed into the large language model.
[00:07:03.720 --> 00:07:07.840]   Now the MPT7B model was trained using this tokenizer here.
[00:07:07.840 --> 00:07:10.000]   Right, so we have to use that tokenizer.
[00:07:10.000 --> 00:07:11.280]   Then what we need to do
[00:07:11.280 --> 00:07:13.600]   is define a stopping criteria of the model.
[00:07:13.600 --> 00:07:15.800]   So I should, I don't know if I mentioned this,
[00:07:15.800 --> 00:07:17.240]   but right now what we're doing
[00:07:17.240 --> 00:07:20.920]   is actually initializing the honey face pipeline.
[00:07:20.920 --> 00:07:23.920]   So within that pipeline, we have the large language model,
[00:07:23.920 --> 00:07:26.440]   the tokenizer, both of those we've just created
[00:07:26.440 --> 00:07:28.840]   and also stopping criteria object, right?
[00:07:28.840 --> 00:07:30.960]   Stopping criteria object,
[00:07:30.960 --> 00:07:35.440]   let me come down to where we create it, is this here.
[00:07:35.440 --> 00:07:40.440]   Okay, so basically MPT7B has been trained
[00:07:40.440 --> 00:07:43.440]   to add this particular bit of text
[00:07:43.440 --> 00:07:45.280]   at the end of its generations,
[00:07:45.280 --> 00:07:48.600]   when it's like, okay, I'm finished, right?
[00:07:48.600 --> 00:07:51.640]   But there's nothing within that model
[00:07:51.640 --> 00:07:54.440]   that will stop it from actually generating text
[00:07:54.440 --> 00:07:55.840]   at that point, right?
[00:07:55.840 --> 00:07:58.320]   It will just, it will generate this, right?
[00:07:58.320 --> 00:08:03.320]   And then it will actually just continue generating text.
[00:08:03.320 --> 00:08:05.240]   And the text that it generates after this
[00:08:05.240 --> 00:08:07.200]   is generally just going to be gibberish
[00:08:07.200 --> 00:08:09.960]   because it's been trained to generate this
[00:08:09.960 --> 00:08:12.800]   at the end of a meaningful answer, right?
[00:08:12.800 --> 00:08:14.040]   After generating this,
[00:08:14.040 --> 00:08:17.760]   it's able to just begin generating anything, okay?
[00:08:17.760 --> 00:08:20.960]   It's not going to be useful stuff.
[00:08:20.960 --> 00:08:24.560]   So what we need to do is define this
[00:08:24.560 --> 00:08:26.560]   as a stopping criteria for the model.
[00:08:26.560 --> 00:08:28.080]   We need to go in there and say,
[00:08:28.080 --> 00:08:30.640]   okay, when the model says end of text,
[00:08:30.640 --> 00:08:33.280]   when it gives this token to us, we stop, right?
[00:08:33.280 --> 00:08:35.280]   We need to specify that.
[00:08:35.280 --> 00:08:39.960]   And we do that using this stopping criteria list object.
[00:08:39.960 --> 00:08:44.360]   Okay, so that requires a stopping criteria object,
[00:08:44.360 --> 00:08:45.680]   which we've defined here.
[00:08:45.680 --> 00:08:48.000]   So, I mean, you can see this.
[00:08:48.000 --> 00:08:51.440]   So these parameters are just the default parameters needed
[00:08:51.440 --> 00:08:53.840]   by this stopping criteria object.
[00:08:53.840 --> 00:08:56.800]   And basically what it's going to do is say,
[00:08:56.800 --> 00:08:58.600]   okay, for SUP ID.
[00:08:58.600 --> 00:09:01.200]   So we have these SUP token IDs.
[00:09:01.200 --> 00:09:03.240]   Maybe I can just show you these.
[00:09:03.240 --> 00:09:04.440]   Maybe that's easier.
[00:09:04.440 --> 00:09:09.160]   So SUP token IDs,
[00:09:09.160 --> 00:09:12.560]   and it's just going to be a few integers, right?
[00:09:12.560 --> 00:09:15.440]   Those integers, actually it's one integer,
[00:09:15.440 --> 00:09:17.880]   which represents this, right?
[00:09:17.880 --> 00:09:19.480]   So I said before the tokenized,
[00:09:19.480 --> 00:09:22.240]   it translates from plain text to the token IDs.
[00:09:22.240 --> 00:09:23.080]   That's what this is.
[00:09:23.080 --> 00:09:24.800]   This is the plain text version.
[00:09:24.800 --> 00:09:27.080]   This is the token ID version, right?
[00:09:27.080 --> 00:09:30.560]   And it's going to say, okay, for the SUP ID here,
[00:09:30.560 --> 00:09:32.800]   so actually just for zero,
[00:09:32.800 --> 00:09:34.960]   if the input IDs,
[00:09:34.960 --> 00:09:38.400]   so the last input ID is equal to that,
[00:09:38.400 --> 00:09:41.480]   we're going to say, okay, it's time to stop, right?
[00:09:41.480 --> 00:09:44.480]   Otherwise it's not time to stop, you can keep going.
[00:09:44.480 --> 00:09:46.440]   And that's it, okay?
[00:09:46.440 --> 00:09:49.240]   So that gives us our stopping criteria object.
[00:09:49.240 --> 00:09:52.920]   And then we just pass that into our pipeline.
[00:09:52.920 --> 00:09:57.120]   So the pipeline is basically the tokenization,
[00:09:57.120 --> 00:10:00.520]   the model and the generation from that model,
[00:10:00.520 --> 00:10:02.680]   and then also this stopping criteria,
[00:10:02.680 --> 00:10:04.960]   all packaged into a nice little function.
[00:10:04.960 --> 00:10:07.120]   So within that pipeline,
[00:10:07.120 --> 00:10:09.280]   we pass in obviously our model, our tokenizer,
[00:10:09.280 --> 00:10:10.640]   and the stopping criteria,
[00:10:10.640 --> 00:10:13.360]   but there's also a few things we need as well.
[00:10:13.360 --> 00:10:15.360]   So return full text.
[00:10:15.360 --> 00:10:16.720]   So if we have this false,
[00:10:16.720 --> 00:10:19.920]   it's just going to return the generated part
[00:10:19.920 --> 00:10:23.200]   or generate portion of some text.
[00:10:23.200 --> 00:10:24.400]   And that's fine, you can do that.
[00:10:24.400 --> 00:10:26.440]   There's actually no problem with that.
[00:10:26.440 --> 00:10:28.680]   But if you want to use this in light chain,
[00:10:28.680 --> 00:10:31.600]   we need to return the generated text
[00:10:31.600 --> 00:10:33.840]   and also the input text.
[00:10:33.840 --> 00:10:35.480]   We need to return full text
[00:10:35.480 --> 00:10:37.320]   because we're going to be using line chain later.
[00:10:37.320 --> 00:10:40.280]   That's why we set return full text equal to true.
[00:10:40.280 --> 00:10:42.400]   If you were just wanting to use this and hung and face,
[00:10:42.400 --> 00:10:45.360]   you don't need to, you don't need to have this as true.
[00:10:45.360 --> 00:10:49.000]   Then our task here is text generation.
[00:10:49.000 --> 00:10:51.840]   Okay, so this just says, okay, we want to generate text.
[00:10:51.840 --> 00:10:53.560]   The device here is important.
[00:10:53.560 --> 00:10:55.920]   We obviously want to use our CUDA enabled GPU.
[00:10:55.920 --> 00:10:56.880]   So we set that.
[00:10:56.880 --> 00:10:59.080]   And then we have a few other model
[00:10:59.080 --> 00:11:01.160]   specific parameters down here.
[00:11:01.160 --> 00:11:02.600]   Or we could call them generation
[00:11:02.600 --> 00:11:04.520]   specific parameters as well.
[00:11:04.520 --> 00:11:08.160]   So the temperature is like the randomness of your output.
[00:11:08.160 --> 00:11:09.200]   Zero is the minimum.
[00:11:09.200 --> 00:11:11.640]   It's basically zero randomness
[00:11:11.640 --> 00:11:14.080]   and one is maximum randomness.
[00:11:14.080 --> 00:11:18.280]   Okay, so imagine it's kind of like how random
[00:11:18.280 --> 00:11:22.560]   the predicted tokens or the next words are going to be.
[00:11:22.560 --> 00:11:23.600]   Then we have top P.
[00:11:23.600 --> 00:11:27.560]   So top P is basically we're going to select
[00:11:27.560 --> 00:11:30.360]   from the top tokens on each prediction
[00:11:30.360 --> 00:11:33.240]   from whose probability adds up to 15%.
[00:11:33.240 --> 00:11:36.440]   And I would recommend if you want to read about this,
[00:11:36.440 --> 00:11:39.360]   I'd recommend looking at this page from Cohere.
[00:11:39.360 --> 00:11:43.200]   So there'll be a link at the top of the video right now.
[00:11:43.200 --> 00:11:45.000]   They explain this really nicely.
[00:11:45.000 --> 00:11:47.640]   So yeah, you can kind of see
[00:11:47.640 --> 00:11:50.760]   they use 0.15 here as well, right?
[00:11:50.760 --> 00:11:54.000]   So consider only top tokens whose likelihoods
[00:11:54.000 --> 00:11:56.520]   add up to that 15% and then ignore the others.
[00:11:56.520 --> 00:11:58.680]   So with each step, right?
[00:11:58.680 --> 00:12:02.480]   Each generation step, you're predicting the next token
[00:12:02.480 --> 00:12:03.520]   or the next word.
[00:12:03.520 --> 00:12:05.040]   You can think of it like that.
[00:12:05.040 --> 00:12:09.160]   And by setting top P equals 0.15,
[00:12:09.160 --> 00:12:13.680]   we're just going to consider the possible next words
[00:12:13.680 --> 00:12:15.680]   'cause we're predicting for all of the words
[00:12:15.680 --> 00:12:17.520]   in that tokenizer.
[00:12:17.520 --> 00:12:21.560]   We're going to consider the top words
[00:12:21.560 --> 00:12:26.560]   whose together their likelihood adds up to 15%, right?
[00:12:27.240 --> 00:12:29.040]   The total, okay?
[00:12:29.040 --> 00:12:32.240]   So you can see that there, they visualize it very nicely.
[00:12:32.240 --> 00:12:34.880]   I don't think my explanation
[00:12:34.880 --> 00:12:37.520]   can compare to this visualization.
[00:12:37.520 --> 00:12:39.440]   Okay, and then we have top K.
[00:12:39.440 --> 00:12:42.160]   This is another value, kind of similar thing, right?
[00:12:42.160 --> 00:12:45.640]   So top K, if we come up to here,
[00:12:45.640 --> 00:12:47.720]   and this is easy to explain,
[00:12:47.720 --> 00:12:51.160]   we're picking from the top K tokens, right?
[00:12:51.160 --> 00:12:55.400]   So in this case, if you had top K equal to one,
[00:12:55.400 --> 00:12:57.680]   it would only select United
[00:12:57.680 --> 00:13:00.600]   or it could only decide on selecting United.
[00:13:00.600 --> 00:13:01.960]   If you had top K equal to two,
[00:13:01.960 --> 00:13:04.240]   you could do United or Netherlands.
[00:13:04.240 --> 00:13:05.200]   Top K equal to three,
[00:13:05.200 --> 00:13:08.240]   you could choose any of these top three, right?
[00:13:08.240 --> 00:13:12.200]   That is what the top K is actually doing.
[00:13:12.200 --> 00:13:15.080]   And actually you can visualize that here as well.
[00:13:15.080 --> 00:13:17.920]   Okay, and okay, what I've done here is
[00:13:17.920 --> 00:13:19.320]   set top K equal to zero.
[00:13:19.320 --> 00:13:22.120]   That's because I don't want to consider top K
[00:13:22.120 --> 00:13:25.800]   because I'm already defining the limits
[00:13:25.800 --> 00:13:30.200]   on the number of tokens to decide from using top P, okay?
[00:13:30.200 --> 00:13:32.760]   So I don't activate the top K there.
[00:13:32.760 --> 00:13:35.480]   Then we have the max, not max,
[00:13:35.480 --> 00:13:38.400]   max number of tokens to generate in the output.
[00:13:38.400 --> 00:13:40.200]   So with each generation,
[00:13:40.200 --> 00:13:41.920]   I'm saying I don't want you to generate
[00:13:41.920 --> 00:13:43.480]   any more than 64 tokens.
[00:13:43.480 --> 00:13:45.280]   You can increase that, right?
[00:13:45.280 --> 00:13:46.880]   So the max context window,
[00:13:46.880 --> 00:13:50.240]   so that's inputs and outputs for this model,
[00:13:50.240 --> 00:13:52.200]   we've already set it to,
[00:13:52.200 --> 00:13:55.480]   it's a max sequence time from earlier, 2048.
[00:13:55.480 --> 00:13:59.080]   So you can go much higher than 64 that I've set here.
[00:13:59.080 --> 00:14:02.280]   And then also we have this repetition penalty.
[00:14:02.280 --> 00:14:04.800]   That's super important because otherwise
[00:14:04.800 --> 00:14:07.480]   this is going to start repeating things
[00:14:07.480 --> 00:14:08.680]   over and over again.
[00:14:08.680 --> 00:14:10.960]   So the default value for that actually is one
[00:14:10.960 --> 00:14:13.600]   in that we can see more repetition.
[00:14:13.600 --> 00:14:14.920]   We switch that to 1.1
[00:14:14.920 --> 00:14:18.360]   and we're generally not going to see that anymore.
[00:14:18.360 --> 00:14:20.960]   Okay, so let's run this.
[00:14:20.960 --> 00:14:23.400]   So we say, explain to me the difference
[00:14:23.400 --> 00:14:26.080]   between nuclear fission and fusion.
[00:14:26.080 --> 00:14:28.680]   So this is from an example somewhere.
[00:14:28.680 --> 00:14:30.680]   I think it was Hugging Face,
[00:14:30.680 --> 00:14:33.000]   but I don't actually remember
[00:14:33.000 --> 00:14:34.600]   where I got that from exactly.
[00:14:34.600 --> 00:14:39.000]   Anyone does know, feel free to mention that in the comments.
[00:14:39.000 --> 00:14:42.200]   So we have the input, okay?
[00:14:42.200 --> 00:14:46.280]   So we said return full text.
[00:14:46.280 --> 00:14:47.680]   So we have the input here.
[00:14:47.680 --> 00:14:48.760]   And then we also have the output.
[00:14:48.760 --> 00:14:51.440]   So nuclear fission is a process that splits heavy atoms
[00:14:51.440 --> 00:14:53.960]   into smaller, lighter ones, so on and so on.
[00:14:53.960 --> 00:14:56.720]   Nuclear fusion occurs when two light
[00:14:56.720 --> 00:14:58.840]   atomic nuclei are combined.
[00:14:58.840 --> 00:15:01.680]   As far as I know, that is correct.
[00:15:01.680 --> 00:15:03.440]   So that looks pretty good.
[00:15:03.440 --> 00:15:05.120]   And then I've also added a note here
[00:15:05.120 --> 00:15:10.120]   on if you'd like to use the Triton optimized implementations.
[00:15:10.120 --> 00:15:14.040]   So Triton in this scenario, as far as I understand,
[00:15:14.040 --> 00:15:17.480]   is the way that the attention is implemented.
[00:15:17.480 --> 00:15:19.600]   It can be implemented either in PyTorch,
[00:15:19.600 --> 00:15:21.800]   which is what we're using by default.
[00:15:21.800 --> 00:15:26.800]   It can be implemented with flash attention or using Triton.
[00:15:26.800 --> 00:15:30.560]   And if you use Triton, it's gonna use more memory,
[00:15:30.560 --> 00:15:31.640]   but it will be faster
[00:15:31.640 --> 00:15:33.560]   when you're actually performing inference.
[00:15:33.560 --> 00:15:34.960]   So you can do that.
[00:15:34.960 --> 00:15:36.040]   The reason I haven't used it here
[00:15:36.040 --> 00:15:40.200]   is because the install takes just an insanely long time.
[00:15:40.200 --> 00:15:43.520]   So I just gave up with that.
[00:15:43.520 --> 00:15:45.400]   But as far as I know,
[00:15:45.400 --> 00:15:47.600]   this sort of setup here should work.
[00:15:47.600 --> 00:15:49.960]   So you pip install Triton and you go through
[00:15:49.960 --> 00:15:52.320]   and then this should work, okay.
[00:15:52.320 --> 00:15:55.160]   Just be wary of that added memory usage.
[00:15:55.160 --> 00:15:56.800]   So yeah, we've seen, okay,
[00:15:56.800 --> 00:15:59.600]   this is how we're gonna use this in the HuggingFace.
[00:15:59.600 --> 00:16:01.440]   So generating text.
[00:16:01.440 --> 00:16:04.240]   Now let's move on to the LineChain side of things.
[00:16:04.240 --> 00:16:06.680]   So how do we implement this in LineChain?
[00:16:06.680 --> 00:16:07.960]   Okay, so we're gonna use this
[00:16:07.960 --> 00:16:10.160]   with the simplest chain possible.
[00:16:10.160 --> 00:16:11.840]   So the LLM chain.
[00:16:11.840 --> 00:16:13.880]   For the LLM, we're going to initialize it
[00:16:13.880 --> 00:16:15.440]   via the HuggingFace pipeline,
[00:16:15.440 --> 00:16:18.400]   which is basically local HuggingFace model.
[00:16:18.400 --> 00:16:20.400]   And for that, we need our pipeline,
[00:16:20.400 --> 00:16:23.760]   which we have conveniently already initialized up here.
[00:16:23.760 --> 00:16:26.840]   So we just pass that into there.
[00:16:26.840 --> 00:16:29.560]   We have our prompt template.
[00:16:29.560 --> 00:16:31.600]   Okay, nothing, right?
[00:16:31.600 --> 00:16:33.560]   It's just the instruction here.
[00:16:33.560 --> 00:16:36.560]   So basically we have some inputs and that's it.
[00:16:36.560 --> 00:16:37.720]   I'm just defining that
[00:16:37.720 --> 00:16:40.200]   so that we can define this LLM chain.
[00:16:40.200 --> 00:16:42.200]   Okay, we initialize that.
[00:16:42.200 --> 00:16:43.280]   And then we come down to here
[00:16:43.280 --> 00:16:46.040]   and we can use the LLM chain to predict.
[00:16:46.040 --> 00:16:47.640]   And for the prediction,
[00:16:47.640 --> 00:16:50.160]   we just pass in those instructions again.
[00:16:50.160 --> 00:16:51.800]   Okay, so same question as before.
[00:16:51.800 --> 00:16:55.360]   So in this case, we should get pretty much the same answer.
[00:16:55.360 --> 00:16:56.880]   So we can run that.
[00:16:56.880 --> 00:16:58.680]   Okay, and the output we get there is this.
[00:16:58.680 --> 00:17:00.600]   So as far as I can tell,
[00:17:00.600 --> 00:17:04.600]   it's pretty much the same as what we got last time.
[00:17:04.600 --> 00:17:08.120]   Okay, so looks good.
[00:17:08.120 --> 00:17:10.720]   And with that, we've now implemented MPT7B
[00:17:10.720 --> 00:17:15.120]   in both HuggingFace and also LineChain as well.
[00:17:15.120 --> 00:17:18.280]   So naturally, if you just want to generate texts,
[00:17:18.280 --> 00:17:19.440]   you can use HuggingFace.
[00:17:19.440 --> 00:17:21.160]   But obviously, if you want to have access
[00:17:21.160 --> 00:17:24.200]   to all of the features that LineChain offers,
[00:17:24.200 --> 00:17:26.840]   all the chains, agents, all this sort of stuff,
[00:17:26.840 --> 00:17:30.120]   then you obviously just take on this actual set
[00:17:30.120 --> 00:17:33.840]   and you have your originally HuggingFace pipeline
[00:17:33.840 --> 00:17:36.480]   now integrated with LineChain,
[00:17:36.480 --> 00:17:39.320]   which I think is pretty cool and super easy to do.
[00:17:39.320 --> 00:17:40.800]   It's not that difficult.
[00:17:40.800 --> 00:17:43.800]   So with that, that's the end of this video.
[00:17:43.800 --> 00:17:46.520]   We've explored how we can actually begin
[00:17:46.520 --> 00:17:49.520]   using open source models in LineChain,
[00:17:49.520 --> 00:17:52.600]   which I think opens up a lot of opportunities for us.
[00:17:52.600 --> 00:17:56.720]   You know, fine tuning models, just using smaller models.
[00:17:56.720 --> 00:18:00.160]   Maybe you don't always need like a big GPT-4
[00:18:00.160 --> 00:18:03.000]   for all of our use cases.
[00:18:03.000 --> 00:18:05.560]   So I think this is the sort of thing
[00:18:05.560 --> 00:18:07.760]   where we'll see a lot more going forwards,
[00:18:07.760 --> 00:18:10.840]   a lot more open source, smaller model is being used.
[00:18:10.840 --> 00:18:14.240]   Of course, I still think OpenAI is gonna be used plenty,
[00:18:14.240 --> 00:18:16.400]   because honestly, in terms of performance,
[00:18:16.400 --> 00:18:18.000]   there are no open source models
[00:18:18.000 --> 00:18:22.440]   that are genuinely comparable to GPT-3.5
[00:18:22.440 --> 00:18:24.280]   or GPT-4 at the moment.
[00:18:24.280 --> 00:18:26.920]   You know, maybe going forwards, there will be eventually,
[00:18:26.920 --> 00:18:29.560]   but right now, we're not quite there.
[00:18:29.560 --> 00:18:31.680]   So yeah, that's it for this video.
[00:18:31.680 --> 00:18:34.240]   I hope all this has been interesting and useful.
[00:18:34.240 --> 00:18:35.960]   Thank you very much for watching
[00:18:35.960 --> 00:18:37.800]   and I will see you again in the next one.
[00:18:37.800 --> 00:18:38.640]   Bye.
[00:18:38.640 --> 00:18:41.240]   (gentle music)
[00:18:42.120 --> 00:18:44.720]   (gentle music)
[00:18:44.720 --> 00:18:47.320]   (gentle music)
[00:18:47.320 --> 00:18:49.920]   (gentle music)
[00:18:49.920 --> 00:18:52.500]   (gentle music)
[00:18:52.500 --> 00:18:54.560]   you

