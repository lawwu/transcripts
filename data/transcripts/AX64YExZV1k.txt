
[00:00:00.000 --> 00:00:07.000]   [Music]
[00:00:07.000 --> 00:00:14.000]   All right, let's get started. So today we're here to discuss the EfficientNet V2 paper.
[00:00:14.000 --> 00:00:17.000]   I'm not feeling the best today, so bear with me.
[00:00:17.000 --> 00:00:23.000]   And I also thought we'll do things a little bit differently this time. So rather than
[00:00:23.000 --> 00:00:28.000]   rather than having a pre-annotated version for the paper and then just going through that part of the paper,
[00:00:28.000 --> 00:00:34.000]   I'll actually show you how I go about reading a new paper and then we'll go through the abstract first,
[00:00:34.000 --> 00:00:37.000]   and we'll go through the introduction, read the conclusion first.
[00:00:37.000 --> 00:00:42.000]   And pretty much I just want to show you the process of how I read this efficient,
[00:00:42.000 --> 00:00:45.000]   how I first read this EfficientNet V2 paper.
[00:00:45.000 --> 00:00:53.000]   And then we'll go deeply into we'll go deeply into everything that's that's being mentioned in the paper.
[00:00:53.000 --> 00:01:00.000]   So my setup is a little bit different today, but should still be should still be good for everybody.
[00:01:00.000 --> 00:01:05.000]   So can you guys see my screen as well? Yes, we can see it.
[00:01:05.000 --> 00:01:15.000]   OK, awesome. Cool. So then as part of this, the first thing I'm going to assume as we go and we start looking into the EfficientNet V2 paper,
[00:01:15.000 --> 00:01:22.000]   I'm going to assume that we all know about what EfficientNets are.
[00:01:22.000 --> 00:01:26.000]   So that's EfficientNet V1. I'm going to assume some of that.
[00:01:26.000 --> 00:01:35.000]   I'm going to assume you have some background in CV so you know about what a ResNet is or what a bottleneck block is.
[00:01:35.000 --> 00:01:42.000]   But in saying so, I'll be giving some brief, brief introductions to each of these, including bottleneck blocks.
[00:01:42.000 --> 00:01:47.000]   And as we go through, I'll be providing context for everything.
[00:01:47.000 --> 00:01:53.000]   So with that being said, let's get started. So in terms of when we look into the EfficientNet V2 paper,
[00:01:53.000 --> 00:01:59.000]   there's when I first read this paper, there's actually not a lot of things that are very new, very innovative.
[00:01:59.000 --> 00:02:08.000]   So when I say it's not very new and innovative, what the authors have done is that they took the existing excellent ideas.
[00:02:08.000 --> 00:02:22.000]   So they took excellent existing ideas like progressive resizing.
[00:02:22.000 --> 00:02:32.000]   And the next one is, again, a fused NB-Conf. So we look at what these are.
[00:02:32.000 --> 00:02:37.000]   But these are the two main things, progressive resizing and then fused NB-Conf.
[00:02:37.000 --> 00:02:46.000]   So these are both from before. I think progressive resizing was first introduced in 2018 by Jeremy Howard and FastAI team.
[00:02:46.000 --> 00:02:51.000]   And then fused NB-Conf has been there since 2019 as part of the Edge TPU.
[00:02:51.000 --> 00:02:57.000]   So EfficientNet V2 is basically an improvement on EfficientNet V1.
[00:02:57.000 --> 00:03:02.000]   So they pretty much thought there's some things wrong with EfficientNet V1.
[00:03:02.000 --> 00:03:08.000]   They looked at the disadvantages, they looked at where things are slow or what's missing with EfficientNet V1.
[00:03:08.000 --> 00:03:17.000]   And they said, OK, let's release V2. So with that being said, let's get started.
[00:03:17.000 --> 00:03:21.000]   So with the so then again, as I said, things are a little bit different today.
[00:03:21.000 --> 00:03:28.000]   We'll read this paper as I go. It's not going to be I'll not just look at the some part of the paper, but the whole paper.
[00:03:28.000 --> 00:03:37.000]   And then if we have time, we'll also have if this time and if there's interest, then we can also look into the source code of EfficientNet V2.
[00:03:37.000 --> 00:03:42.000]   That's something, again, that's a bit different in this paper reading group than in the past paper reading groups.
[00:03:42.000 --> 00:03:50.000]   So the first thing then as I go, the first thing I read is the abstract and I go very quickly into reading the abstract.
[00:03:50.000 --> 00:03:59.000]   So the main thing is this paper introduces EfficientNet V2 and then it's faster training speed, better parameter efficiency, all good so far.
[00:03:59.000 --> 00:04:06.000]   To develop these models, they did some things like neural architecture search that has also been part of EfficientNets.
[00:04:06.000 --> 00:04:15.000]   So EfficientNet V1 had neural architecture search. And then the next thing that they said is we use new basically operations such as FusedMVCon,
[00:04:15.000 --> 00:04:23.000]   which we'll look into in a bit more detail. And then finally, they developed this EfficientNet V2, which is faster than state of the art models.
[00:04:23.000 --> 00:04:28.000]   So they don't say which state of the art model. And it's 6.8x smaller.
[00:04:28.000 --> 00:04:35.000]   But the question is 6.8x smaller than what or what is the state of the art model that they're referring to right now?
[00:04:35.000 --> 00:04:42.000]   But that's OK. I mean, as long as they say that, we'll believe that that's true.
[00:04:42.000 --> 00:04:49.000]   Then the next thing they say is we speed up the training by introducing progressively increasing the image size during training.
[00:04:49.000 --> 00:04:54.000]   So that's again, this idea, as I mentioned, is this progressive resizing.
[00:04:54.000 --> 00:05:00.000]   We'll have a look at this in a bit more detail exactly as what progressive resizing is.
[00:05:00.000 --> 00:05:07.000]   And then the last thing they say is with progressive resizing, EfficientNet V2 significantly outperforms previous models.
[00:05:07.000 --> 00:05:14.000]   And once they train it on the ImageNet 21K dataset, it gets 87.3% top one.
[00:05:14.000 --> 00:05:19.000]   So then a few things that have been mentioned here that we should look at. The first one is FusedMVCon.
[00:05:19.000 --> 00:05:27.000]   The second one is this idea of progressively increasing the image size during training.
[00:05:27.000 --> 00:05:33.000]   And then what's this ImageNet 21K? Oh, sorry. Also, one more thing.
[00:05:33.000 --> 00:05:44.000]   If you haven't read what the EfficientNet paper is, so that's the first version of that's EfficientNet V1.
[00:05:44.000 --> 00:05:53.000]   So if you go to my blog, I have this blog on EfficientNets that would explain everything about EfficientNets,
[00:05:53.000 --> 00:06:01.000]   about what neural architecture search is, what's this idea of compound scaling, and then how does the EfficientNet architecture look like?
[00:06:01.000 --> 00:06:09.000]   So in EfficientNet V1, they use something that's called an MBConf. So we'll have a brief look into what MBConf is.
[00:06:09.000 --> 00:06:15.000]   But for more details, if you're interested, I'm sharing this in Zoom chat.
[00:06:15.000 --> 00:06:21.000]   Please have a look and also go about reading this EfficientNet blog.
[00:06:21.000 --> 00:06:26.000]   Another thing I want to say as part of the questions, something because we're live on YouTube as well.
[00:06:26.000 --> 00:06:35.000]   So something I do is we have 1db.me/EfficientNetV2 as the URL.
[00:06:35.000 --> 00:06:42.000]   So if I go to this URL, it will take me to this report and I only monitor the comments on this report.
[00:06:42.000 --> 00:06:48.000]   This is something that we've been doing as part of every paper reading group.
[00:06:48.000 --> 00:06:54.000]   So let me post this in Zoom as well.
[00:06:54.000 --> 00:07:00.000]   So you should have the two URLs. So as I go and as I go through the EfficientNet V2 paper,
[00:07:00.000 --> 00:07:06.000]   if you have any questions, then please put a comment on this report. I'll come to this report every 10-15 minutes.
[00:07:06.000 --> 00:07:12.000]   I'll have a look at the questions and then answer those questions. But if you're asking questions in the chat, I'm really sorry,
[00:07:12.000 --> 00:07:18.000]   but I am not monitoring the chat, Zoom chat or YouTube chat for that matter.
[00:07:18.000 --> 00:07:26.000]   OK, so then back to the EfficientNet V2 paper. So the first thing is, what is this progressive resizing idea?
[00:07:26.000 --> 00:07:35.000]   So what I saw in Fast.ai, what Jeremy was doing, which was really interesting,
[00:07:35.000 --> 00:07:45.000]   is that when you have a model, the first thing you do is you input two to four by two to four images
[00:07:45.000 --> 00:07:52.000]   and then you train it by some loss, if it's a classification, let's call it cross entropy loss.
[00:07:52.000 --> 00:07:57.000]   Then you train this model. So this model is now trained for these two to four by two to four images.
[00:07:57.000 --> 00:08:04.000]   But because in a CNN, in a convolutional network, you could just pretty much take this exact same model
[00:08:04.000 --> 00:08:12.000]   and then instead pass 256 by 256, keeping the same weights. So the weights remain the same.
[00:08:12.000 --> 00:08:19.000]   You could then pass 256 by 256 image size and then you could fine tune this model on this bigger image size.
[00:08:19.000 --> 00:08:26.000]   And then you could do a third iteration where you could pass in 512 by 512 image size.
[00:08:26.000 --> 00:08:31.000]   So as you see, as we're training the model, the image sizes are going bigger and bigger.
[00:08:31.000 --> 00:08:37.000]   So we started with a really small image size. What are the benefits of having a really small image size?
[00:08:37.000 --> 00:08:44.000]   The benefits are it's faster, less computation intensive.
[00:08:44.000 --> 00:08:51.000]   So these are the two main ones. And then it's also easier for the model to train because it requires less computation.
[00:08:51.000 --> 00:08:54.000]   It's also easier for the model to train with a smaller image size.
[00:08:54.000 --> 00:09:01.000]   But when we train on a two to four by two to four image size, then the model, if it's ImageNet.
[00:09:01.000 --> 00:09:10.000]   So ImageNet has how many? ImageNet has about, I think, 1.4 million images and about a thousand classes.
[00:09:10.000 --> 00:09:20.000]   So what this model, when it trains on two to four by two to four image size, what happens is then this model has already some idea about these thousand classes.
[00:09:20.000 --> 00:09:26.000]   Like if it's pets, then it already knows how dogs look like. If it's it already knows how cats look like.
[00:09:26.000 --> 00:09:32.000]   And then when we increase the image size, then it's actually trying to then the model accuracy is going to improve.
[00:09:32.000 --> 00:09:41.000]   And this is, again, an idea of compound scaling. So what when we want to have bigger models, there's like three ways of increasing the model capacity.
[00:09:41.000 --> 00:09:46.000]   So the first thing, the first way is you increase the image size.
[00:09:46.000 --> 00:09:51.000]   The second thing is you increase the depth, which means you make the model deeper.
[00:09:51.000 --> 00:10:00.000]   So if it has 10 layers, it makes it it makes it 20. I mean, instead of having 10 layers, you have 20 layers, which is increasing the depth.
[00:10:00.000 --> 00:10:05.000]   And then the third thing is increasing the width. That's just increasing the number of channels.
[00:10:05.000 --> 00:10:10.000]   So, again, these are just the basics that I assume you would know.
[00:10:10.000 --> 00:10:17.000]   But if you don't, then have a look at this. Have a read of the EfficientNet blog that I mentioned.
[00:10:17.000 --> 00:10:22.000]   And in there, you'll find everything that's been explained. What does it mean by scaling the depth?
[00:10:22.000 --> 00:10:26.000]   What does it mean by scaling the width? What does it mean by scaling the resolution?
[00:10:26.000 --> 00:10:36.000]   And the EfficientNet paper, what they said is instead of increasing one thing at a time, instead of just increasing the depth, instead of just increasing the width or the resolution,
[00:10:36.000 --> 00:10:43.000]   why don't you increase everything at the same time? And that's that's what compound scaling refers to.
[00:10:43.000 --> 00:10:53.000]   So that's just the abstract. That's how I like to read an abstract. I just have a look at the abstract and I kind of join the dots with previous papers.
[00:10:53.000 --> 00:11:00.000]   So the next thing is, again, a fused NB-Conv. I won't tell you what an I know what an NB-Conv is from EfficientNet v1.
[00:11:00.000 --> 00:11:09.000]   And as you know what an NB-Conv is from an EfficientNet v1, but we will look into it together as we go.
[00:11:09.000 --> 00:11:13.000]   And then we'll look at what this fused NB-Conv is and how is that different from NB-Conv.
[00:11:13.000 --> 00:11:20.000]   So let's just read the abstract first. That's the very first thing. And then we pay more attention to the introduction.
[00:11:20.000 --> 00:11:26.000]   So in introduction, what they say is pretty much that training efficiency is important.
[00:11:26.000 --> 00:11:34.000]   So, for example, you have models like GPT-3. So GPT-3 is a very large, very large, I think about a billion parameters, if not more.
[00:11:34.000 --> 00:11:40.000]   But it can do things very well. But the problem is it requires thousands of GPUs.
[00:11:40.000 --> 00:11:44.000]   And a standard person, especially me, I don't have access to those thousands of GPUs.
[00:11:44.000 --> 00:11:49.000]   So if I want to retrain GPT-3, it's actually going to be a problem.
[00:11:49.000 --> 00:11:58.000]   I won't be able to do it with the amount of GPUs I have access to or the amount of GPUs a normal person, unless your Google or Facebook would have access to.
[00:11:58.000 --> 00:12:08.000]   So to increase training efficiency, we've already had a look at in one of our past paper reading groups, we had a look at the NF-ResNets.
[00:12:08.000 --> 00:12:15.000]   All that does is that removes batch normalization. So that's just the basic introduction that training efficiency is really important.
[00:12:15.000 --> 00:12:26.000]   And what I mean by training efficiency or what the paper means by training efficiency is just this idea that your model should be fast.
[00:12:26.000 --> 00:12:38.000]   And what does it mean by the model should be fast? It basically means that if you have images to process, if you want to make predictions on a set of images, then this model should be really efficient.
[00:12:38.000 --> 00:12:43.000]   Like it should not be too big, something like a GPT-3 that requires thousands of GPUs.
[00:12:43.000 --> 00:12:56.000]   Or it should not be too slow. It should not take like 10 seconds per image because then you won't be able to use this model that you have trained that takes 10 seconds in a real world scenario.
[00:12:56.000 --> 00:13:07.000]   So that's what it means by training efficiency. And EfficientNet have been, EfficientNet v1 was one of the best papers when it came to training efficiency and also model accuracy.
[00:13:07.000 --> 00:13:17.000]   So EfficientNet v1 used this idea of compound scaling and what that did was they were able to have this really high accuracy while being efficient.
[00:13:17.000 --> 00:13:22.000]   But there was some drawbacks. So we'll have a look at the drawbacks.
[00:13:22.000 --> 00:13:26.000]   So again, they say in this paper, we again use neural architecture search.
[00:13:26.000 --> 00:13:32.000]   If you don't know what neural architecture search is, I will show you just a little bit on what that means.
[00:13:32.000 --> 00:13:42.000]   But again, if you go back to that EfficientNet v1 blog, that will have more background on what neural architecture search is and how that looks like.
[00:13:42.000 --> 00:13:48.000]   OK, so now, as I said, EfficientNet v2 looks at the drawbacks of EfficientNets.
[00:13:48.000 --> 00:13:58.000]   So that's here. So our study shows in EfficientNets, first thing, training with large image sizes is slow, which is true.
[00:13:58.000 --> 00:14:07.000]   Like if you have a really big image size, 1024 by 1024 or 512 by 512, then your GPU.
[00:14:07.000 --> 00:14:15.000]   So if you have a GPU, this is my GPU. If it has 16 gigabytes of.
[00:14:15.000 --> 00:14:20.000]   Sorry, if it has megabytes, I've just confused myself. It is gigabytes.
[00:14:20.000 --> 00:14:29.000]   If it has 16 gigabytes of. Sam, is that is that a correct memory is 16 megabytes or gigabytes has to be gigabytes, right?
[00:14:29.000 --> 00:14:36.000]   A standard GPU, even 100. Yes. Yeah. I'm just confusing myself.
[00:14:36.000 --> 00:14:44.000]   Thanks for that. So anyway, if it has that much memory and you have an image size, that's like 1024 by 1024.
[00:14:44.000 --> 00:14:49.000]   So that's my first image size. And if it's two to four by two to four. So say that's my two options.
[00:14:49.000 --> 00:14:56.000]   My first option is having this smaller image size. And my second option is having this bigger image size.
[00:14:56.000 --> 00:15:03.000]   If I have this smaller image size, then I'll be able to fit maybe five and two patches.
[00:15:03.000 --> 00:15:06.000]   So basically my bat size. Sorry, my bat size would be a five and two.
[00:15:06.000 --> 00:15:12.000]   I would be able to fit five and two images of this smaller image size in this GPU.
[00:15:12.000 --> 00:15:18.000]   But if I have a bigger image size, then I'll only be able to fit maybe, say, 32 or 64.
[00:15:18.000 --> 00:15:26.000]   So what happens is when your image size goes bigger, the bat size goes smaller.
[00:15:26.000 --> 00:15:34.000]   So if you have thousand images, then let's say if you have then if you have 500 images per batch,
[00:15:34.000 --> 00:15:41.000]   then you actually have to do a lot less iterations and it's much faster to do training on smaller image sizes.
[00:15:41.000 --> 00:15:45.000]   But if you have a bigger image size, then it's actually much slower. So that's what they mean.
[00:15:45.000 --> 00:15:54.000]   Training with very large image sizes is slow because there's only so many images that can fit in a batch on a single GPU.
[00:15:54.000 --> 00:15:59.000]   The next thing that they mention is this thing about depth wise convolutions are really slow in early layers.
[00:15:59.000 --> 00:16:06.000]   So right now, again, if you've read the Efficient V1 paper, you'd know where depth wise convolutions are used.
[00:16:06.000 --> 00:16:17.000]   But that's basically I'll give you context. So the Efficient paper, the architecture looks something like this.
[00:16:17.000 --> 00:16:26.000]   So this is Efficient V1 paper. That's how. This is how the architecture looks like.
[00:16:26.000 --> 00:16:33.000]   What's this nbconv? nbconv layer looks like this.
[00:16:33.000 --> 00:16:39.000]   That's nbconv. Specifically, nbconv6 looks like this.
[00:16:39.000 --> 00:16:44.000]   So what you have first, you have a conv. It's like an inverted residual.
[00:16:44.000 --> 00:16:48.000]   So what you have is typically you have a conv one by one.
[00:16:48.000 --> 00:16:52.000]   It's just like a one by one convolution to increase the number of channels.
[00:16:52.000 --> 00:16:59.000]   So if your input image has or your input feature map has F channels, then what this conv one by one will do,
[00:16:59.000 --> 00:17:06.000]   it will increase the number of channels from F to 6F. So that's just increasing the number of channels.
[00:17:06.000 --> 00:17:12.000]   And then you apply a depth wise convolution. So this is where a depth wise convolution gets applied.
[00:17:12.000 --> 00:17:17.000]   And I'm just showing you what an nbconv looks like. So this is where the depth wise convolution gets applied.
[00:17:17.000 --> 00:17:24.000]   So you again have an output that's the same as edge by, well, same height, same width, but again, 6F channels.
[00:17:24.000 --> 00:17:31.000]   And then you can have another convolution layer that will reduce these number of channels from 6F back to F.
[00:17:31.000 --> 00:17:38.000]   So that's your nbconv that was used in EfficientNet. Where is this idea coming from?
[00:17:38.000 --> 00:17:45.000]   This idea is very similar to a bottleneck block. So in ResNet, you have this thing called a bottleneck block.
[00:17:45.000 --> 00:17:51.000]   What's the idea? Let's say your input has 256 dimensions.
[00:17:51.000 --> 00:17:57.000]   So your input looks something like, say, 64 by 64 by 256 channels.
[00:17:57.000 --> 00:18:05.000]   That's your input feature map. If you try and do a convolution, a regular convolution on this many channels,
[00:18:05.000 --> 00:18:13.000]   it's going to be a lot more computation. Right. So then what they do is this is again, this is just a bottleneck.
[00:18:13.000 --> 00:18:17.000]   So in ResNet, you have a basic block and you have a bottleneck block.
[00:18:17.000 --> 00:18:22.000]   This is from the ResNet paper. You have a basic block and you have a bottleneck block.
[00:18:22.000 --> 00:18:30.000]   So what the bottleneck block does is it will reduce the dimensions from 256 to 64 by just using a one by one conv.
[00:18:30.000 --> 00:18:36.000]   So then using a one by one conv, what you have is your number of channels have reduced to 64.
[00:18:36.000 --> 00:18:41.000]   And then instead of applying the convolution to these 256 channels, you apply the convolution to 64 channels.
[00:18:41.000 --> 00:18:49.000]   So that's way that way what happens is your computations are less.
[00:18:49.000 --> 00:18:56.000]   It still covers the same. I mean, it's just a bit it's just an easy way of first reducing the number of channels,
[00:18:56.000 --> 00:19:00.000]   then applying a convolution and then again increasing back the number of channels.
[00:19:00.000 --> 00:19:08.000]   This way, it's much easier to do it this way. And then nbconv is just an inverted way of doing it.
[00:19:08.000 --> 00:19:18.000]   It's like an inverted residual block. It was first, I think it was first introduced in a MobileNet V2 paper.
[00:19:18.000 --> 00:19:23.000]   So I'm just giving you context. This is just we are not discussing the efficient V2 paper right now.
[00:19:23.000 --> 00:19:27.000]   This is just giving you context on where these terms come from.
[00:19:27.000 --> 00:19:34.000]   What is nbconv? When was it first sort of released? nbconv was first part of MobileNet V2.
[00:19:34.000 --> 00:19:39.000]   From what I've read in the research papers. And then they use something called a depthwise convolution.
[00:19:39.000 --> 00:19:46.000]   So see how this is different from bottleneck block. A bottleneck block was reducing the number of channels from 256 to 64,
[00:19:46.000 --> 00:19:54.000]   basically going from F to F by 4. What this inverted, that's why this is called an inverted residual,
[00:19:54.000 --> 00:19:57.000]   inverted residual block is because it's increasing the number of channels.
[00:19:57.000 --> 00:20:05.000]   It's going from F to 6F. But instead of using a regular convolution, they're using a depthwise convolution.
[00:20:05.000 --> 00:20:12.000]   So they're using depthwise conv. I'm giving you context again.
[00:20:12.000 --> 00:20:19.000]   If we go into the MobileNet V2 paper, this is where it's first introduced.
[00:20:19.000 --> 00:20:23.000]   So bottleneck with expansion layer. So if you want to, if you have, if you have,
[00:20:23.000 --> 00:20:28.000]   if you really want to dig deep into all of these things, go back and have a look at these.
[00:20:28.000 --> 00:20:33.000]   This is where the inverted residuals was first sort of introduced.
[00:20:33.000 --> 00:20:39.000]   And they're using again, one by one, followed by depthwise and then a one by one conv.
[00:20:39.000 --> 00:20:47.000]   And so then what is, so what is this depthwise convolution?
[00:20:47.000 --> 00:20:58.000]   OK, so let's say this is my width. This is my height.
[00:20:58.000 --> 00:21:02.000]   Let's just call this height, let's call this width, it doesn't matter. And let's say I have three channels.
[00:21:02.000 --> 00:21:11.000]   So my input is H by W by 3. In a regular, I think if you know what convolutions are,
[00:21:11.000 --> 00:21:17.000]   then what you would know is if this is my input image. So an input image is just,
[00:21:17.000 --> 00:21:24.000]   let's say I have a three channel image, blue, red, RGB, basically.
[00:21:24.000 --> 00:21:33.000]   So let's just say these are my three. So my input is H by W by 3.
[00:21:33.000 --> 00:21:40.000]   Three channels being these RGB. So this is G, this is R, this is, sorry, this is B.
[00:21:40.000 --> 00:21:45.000]   This is R and this is G. So let's say this is my input, right?
[00:21:45.000 --> 00:21:52.000]   When you do a convolution operation, what happens is your convolution kernel looks something like this.
[00:21:52.000 --> 00:21:57.000]   This is H dash, W dash, and then again, this is three channels.
[00:21:57.000 --> 00:22:02.000]   So this is H. It shouldn't be H dash, it would be K.
[00:22:02.000 --> 00:22:10.000]   So if my convolution, if you're in PyTorch and you say something like NN dot conv 2D,
[00:22:10.000 --> 00:22:16.000]   and you say three by three, that just means that your kernel size is three by three.
[00:22:16.000 --> 00:22:19.000]   So let's say this kernel size is equal to three by three.
[00:22:19.000 --> 00:22:24.000]   And then it's also a three channel because you want to apply that to this three channel RGB input.
[00:22:24.000 --> 00:22:31.000]   Then what happens is this K by K will get applied to this section of this image.
[00:22:31.000 --> 00:22:35.000]   I'm just showing you what a regular convolution looks like.
[00:22:35.000 --> 00:22:43.000]   And then each of these elements, they do a convolution operation on all three channels together.
[00:22:43.000 --> 00:22:50.000]   So you get an output. And then you have N such kernels.
[00:22:50.000 --> 00:23:02.000]   So let's say you have N such kernels, then you get an output of one by one by N just for this part of,
[00:23:02.000 --> 00:23:07.000]   because it will be applied to each convolution. And then that's what gets added here.
[00:23:07.000 --> 00:23:15.000]   So this output is just this much. That's just a regular convolution operation.
[00:23:15.000 --> 00:23:21.000]   How is depth wise convolution different and what is the idea behind a depth wise convolution?
[00:23:21.000 --> 00:23:29.000]   What they do is instead of in a regular convolution, when you do the convolution operation on all three channels together,
[00:23:29.000 --> 00:23:36.000]   what you do is you separate the channels. So this channel goes here, this channel goes here, this channel goes here.
[00:23:36.000 --> 00:23:44.000]   Then instead of having a filter, which looked like K by K by three, you have a filter that looks like K by K by one.
[00:23:44.000 --> 00:23:49.000]   So each channel has its own. It's basically a broken up.
[00:23:49.000 --> 00:23:54.000]   It's basically like you're breaking up the regular convolution. Instead of doing all three channels at once,
[00:23:54.000 --> 00:24:00.000]   you're doing one by one channel. So when you have this kernel of channel size one,
[00:24:00.000 --> 00:24:05.000]   you do the convolution operation with this one channel. So you get some output.
[00:24:05.000 --> 00:24:10.000]   Then you do this second kernel with the second channel and then third kernel by this third channel.
[00:24:10.000 --> 00:24:18.000]   But each kernel has one channel instead of like in a regular convolution, you could have three channels or 64 channels or like multiple channels.
[00:24:18.000 --> 00:24:24.000]   But in depth wise convolution, this is how that looks like. And then so you get some convoluted outputs, right?
[00:24:24.000 --> 00:24:28.000]   This is your output feature maps. You join them together.
[00:24:28.000 --> 00:24:36.000]   So you have so if this was H, well, W dash by H dash after doing this convolution operation,
[00:24:36.000 --> 00:24:41.000]   then this would look something like W dash H dash by three. It's kind of the same output.
[00:24:41.000 --> 00:24:47.000]   Like you kind of got the it's kind of doing the same thing, but just the operation is broken up.
[00:24:47.000 --> 00:24:53.000]   And then you have a one by one convolution that will mix the channels with each other.
[00:24:53.000 --> 00:25:03.000]   So that's just what depth wise convolution looks like. And this is how this depth wise convolution operation is different from a regular convolution.
[00:25:03.000 --> 00:25:13.000]   The benefit of doing this instead of doing like a regular convolution, the benefit of doing it this way is that the computation,
[00:25:13.000 --> 00:25:22.000]   if you look at the number of multiply addition operations, it's less as you compare it to a regular convolution.
[00:25:22.000 --> 00:25:30.000]   So this way of doing this, having a depth wise convolution is is in terms of the MLOps, in terms of the multiply addition operations,
[00:25:30.000 --> 00:25:40.000]   it has less operations to be done. So in your head, you would think, OK, then that means it's it has to be faster on a GPU because it has less operations.
[00:25:40.000 --> 00:25:49.000]   But what they actually found, what the efficient at V2 paper found is that using this depth wise convolution is actually slow.
[00:25:49.000 --> 00:25:55.000]   So that's contradictory, right? That's very contradictory. The depth wise convolution is meant to be faster, but then it's slow.
[00:25:55.000 --> 00:26:07.000]   Why is that? I'm trying to find the edge TPU paper.
[00:26:07.000 --> 00:26:14.000]   One second. OK, the answer is here.
[00:26:14.000 --> 00:26:26.000]   I won't go deep into it, but this is just this is this paper. This is the accelerator of a neural network design using AutoML, which is the edge TPU paper.
[00:26:26.000 --> 00:26:52.000]   They basically said that the depth wise convolution. So if you have a look at this. Sorry, I don't have this open.
[00:26:52.000 --> 00:27:04.000]   OK, there it is. So then what they the reason why this this this way of like on your left, this is nbconv.
[00:27:04.000 --> 00:27:11.000]   So if you remember, nbconv was just this, you have a convolution one by one, then a depth wise, then a convolution. Right.
[00:27:11.000 --> 00:27:19.000]   So this is your nbconv. You have a one by one convolution, then a three by three depth wise, then a one by one convolution. Right.
[00:27:19.000 --> 00:27:31.000]   So what they noticed is that in modern GPUs or like the GPUs that we use in our day to day lives, what they noticed is that the utilization of this depth wise convolution is really low.
[00:27:31.000 --> 00:27:48.000]   So even though the number of multiply addition operations in a depth wise convolution compared to regular convolution are less, even then, but the utilization on the GPU is much less compared to a to a regular convolution.
[00:27:48.000 --> 00:27:59.000]   So what that means is when you're actually using this on a standard GPU, the depth wise convolution is actually going to be slower than rather than if you were just using a normal convolution.
[00:27:59.000 --> 00:28:07.000]   So this in turn turned out to be efficient. This turned out to be a bottleneck in the efficient and we want paper.
[00:28:07.000 --> 00:28:17.000]   So what they found is that we should not use depth wise. They pretty much found this that using depth wise convolution is actually slower than what they thought would be faster.
[00:28:17.000 --> 00:28:26.000]   So that's just a bit of background. When they say depth wise convolution, depth wise convolutions are slow in early layers.
[00:28:26.000 --> 00:28:33.000]   The next thing that they say is equally scaling up every stage is suboptimal. So again, this is compound scaling. This is what they did in compound scaling.
[00:28:33.000 --> 00:28:51.000]   So they found three ways of scaling up depth, width, and then resolution. And what they did was previously we would just like architectures would just either increase the depth or architectures would just increase the width or architectures would just increase the resolution.
[00:28:51.000 --> 00:29:06.000]   So what they just said is, let's increase that all by 1.1x. So the new architecture is 1.1 times deep, 1.1 times wider, and the image size is 1.1 times higher. But it's using like the same parameter to sort of scale all three together.
[00:29:06.000 --> 00:29:20.000]   And this sort of line, what they figured out is that equally scaling up every stage is not the best way. There should be a different way that you should probably increase width by two, depth by one, or resolution by 0.5.
[00:29:20.000 --> 00:29:25.000]   That's just giving you context.
[00:29:25.000 --> 00:29:31.000]   So I'll take up questions right now. If there's any questions on... This is fairly important as we go forward.
[00:29:31.000 --> 00:29:48.000]   The main things you should know until now that we've read is that even though depth-wise convolutions have lower multiplier addition operations than a regular convolution, in standard GPUs, this is slower.
[00:29:48.000 --> 00:29:54.000]   Then you should also have right now, if it's okay, that you should have some idea about what compound scaling is.
[00:29:54.000 --> 00:30:06.000]   So if there's any questions, I'll take up questions on what I've explained so far. We haven't looked into the EfficientV2 paper yet. I've just given you context on what EfficientV1 looked like.
[00:30:06.000 --> 00:30:09.000]   So let me have a look if there's any questions so far.
[00:30:09.000 --> 00:30:28.000]   >> Hey, I had a question. I'll just speak it out loud. So in your practice, how do you benchmark if something like depth-wise convolution is actually working smoothly? Because I assume, at least if I were to use something that's just checking how long it takes to run, it's a lot of functions, right? So how do you benchmark that?
[00:30:28.000 --> 00:30:47.000]   >> Oh, I don't do anything. Ross Whitman has done everything for us. So if you don't know, I use PyTorch in my day-to-day life. So in PyTorch, there's this wonderful library by Ross called PyTorch Image Models. It's also called TIM. So that's the library there.
[00:30:47.000 --> 00:31:02.000]   This has a benchmark script. So if you search benchmark, so there's this benchmark.py script. All you can do is then you can pass in your model architecture with depth-wise convolution or you could pass in the EfficientV2.
[00:31:02.000 --> 00:31:19.000]   So this will tell you how long the latency is or how many images get passed. Then you can actually compare the different speeds. So yeah, so this is definitely a good way of having a look at it. I think there's also some results.csv in here.
[00:31:19.000 --> 00:31:35.000]   If I go in Results, there's all these different results that there are. So if I have a look at Results ImageNet, you can see what the top error is for what image size, what the parameter count is, and how these different architectures compare to each other.
[00:31:35.000 --> 00:31:49.000]   So I think there's also one with latency. I'm not sure. I believe there would be one. But this is definitely the way I go if I want to compare speed and if I want to check the latency of nbconf.
[00:31:49.000 --> 00:32:06.000]   So I'm going to go in Results ImageNet and I'm going to check the latency of nbconf.
[00:32:06.000 --> 00:32:13.000]   And I'm going to check the latency of nbconf.
[00:32:13.000 --> 00:32:25.000]   In fact, the EfficientV2 paper, when they were checking the latency, they also used this exact same repository that I've pointed you to. So this is from the EfficientV2 paper that we're discussing today.
[00:32:25.000 --> 00:32:48.000]   So EfficientV2 paper optimized for training. They perform well for inference. And since latency often depends on the hardware, so that's exactly what I said, when we're using depth-wise convolution, even though the number of multiply addition operations are less, it actually depends on the hardware that you're using, what CUDA, if you have the latest version of CUDA, what container you're using.
[00:32:48.000 --> 00:33:01.000]   So it also depends on a lot of practical things like which GPU you're running things on. So what they did is we use the same PyTorch image models code base, ROS fitment, and run all models on the same machine using a bat size of 16.
[00:33:01.000 --> 00:33:20.000]   And they're using the same repository to benchmark how fast EfficientV2 is compared to EfficientV1. And then they see, okay, compared to the recent ResNest or other papers, EfficientV2 achieves 0.6 bit accuracy and also 2.8x faster inference speed.
[00:33:20.000 --> 00:33:27.000]   So that's the way that I go about it and that's the way that researchers in EfficientV2 have gone about it as well.
[00:33:27.000 --> 00:33:29.000]   Awesome.
[00:33:29.000 --> 00:33:38.000]   Does anybody else have any other questions?
[00:33:38.000 --> 00:33:44.000]   Ops just stands for operations. When I say ML Ops, I just mean multiply. Yeah, ML operations.
[00:33:44.000 --> 00:34:00.000]   So that's what I know all of this sounds complex right now but I've actually made it a really good effort in the EfficientV1 paper to give context for each of these things and why nbconv is fast or slow or those sort of things.
[00:34:00.000 --> 00:34:17.000]   So far, the key takeaways are nbconv, which I've showed you, is a 1x1 conv followed by a 3x3 depth-wise followed by a 1x1 conv.
[00:34:17.000 --> 00:34:19.000]   So that's the nbconv block.
[00:34:19.000 --> 00:34:25.000]   This is slower than a basically a fused nbconv.
[00:34:25.000 --> 00:34:30.000]   I haven't told you what fused nbconv is, so I'll do that next.
[00:34:30.000 --> 00:34:43.000]   But what a fused nbconv does is that instead of using this 3x3 depth-wise and a 1x1 convolution, it just says, OK, let me just use a regular 3x3 convolution followed by a 1x1 convolution.
[00:34:43.000 --> 00:34:45.000]   So that's what we look at next.
[00:34:45.000 --> 00:34:57.000]   That's just the idea because depth-wise convolution is not optimized for-- sorry, could I please ask everybody to mute themselves?
[00:34:57.000 --> 00:35:00.000]   Thank you.
[00:35:00.000 --> 00:35:05.000]   Because depth-wise convolution is basically slower on modern GPUs.
[00:35:05.000 --> 00:35:09.000]   That's why maybe the faster way to go about this is to use fused nbconv.
[00:35:09.000 --> 00:35:22.000]   The next thing that we have looked at so far is that this way of scaling depth with end resolution by just using a single parameter and then scaling everything by a same number is not the best way to go about it.
[00:35:22.000 --> 00:35:24.000]   So that's the second drawback.
[00:35:24.000 --> 00:35:29.000]   And the third drawback is training with very large image sizes is slow.
[00:35:29.000 --> 00:35:35.000]   Because the higher or the larger image size that you go, the more memory it takes in GPU.
[00:35:35.000 --> 00:35:37.000]   And that's why the batch size then becomes smaller.
[00:35:37.000 --> 00:35:41.000]   And as your batch size becomes smaller, your training takes more time.
[00:35:41.000 --> 00:35:44.000]   So that's the main things that they found.
[00:35:44.000 --> 00:35:53.000]   OK, these are the three main things that we need to fix in EfficientV2 if you want to make it better than EfficientV1.
[00:35:53.000 --> 00:35:55.000]   The next thing they also -- this is on top.
[00:35:55.000 --> 00:36:07.000]   So not only did they fix these three things that we'll see how they fixed it, the next thing that they also did is this idea of progressively increasing image size during training, which I've already told you looks something like this.
[00:36:07.000 --> 00:36:15.000]   That you have 2 to 4 by 2 to 4 at first and you train your model, then you go to 256 by 256, and then finally you fine tune your model on 5 and 2 by 5.
[00:36:15.000 --> 00:36:22.000]   So you start with a very small image size first, but you increase it after some certain amount of epochs.
[00:36:22.000 --> 00:36:32.000]   And that's this idea of progressive increasing image size, which has already been introduced by Jeremy Howard in 2018.
[00:36:32.000 --> 00:36:34.000]   So that's the main things.
[00:36:34.000 --> 00:36:38.000]   Now let me tell you what FusedNBConv is.
[00:36:38.000 --> 00:36:43.000]   I've told you what NBConv is, but what is this FusedNBConv?
[00:36:43.000 --> 00:36:46.000]   FusedNBConv is not something that's new.
[00:36:46.000 --> 00:36:56.000]   FusedNBConv is something that was introduced in 2019.
[00:36:56.000 --> 00:37:03.000]   And again, all they did was when in NBConv -- this is the paper called Edge TPU.
[00:37:03.000 --> 00:37:09.000]   So this paper name is Accelerator-Aware Neural Network Design Using AutoML.
[00:37:09.000 --> 00:37:13.000]   And this is a really good paper that introduced Edge TPU.
[00:37:13.000 --> 00:37:28.000]   And this paper was actually looking at EfficientNet paper and they said, "Okay, let me try and use this on the modern TPUs and see, even though depth-wise has less number of operations, does it actually mean that it's faster on a TPU?
[00:37:28.000 --> 00:37:30.000]   Does it actually mean that it's faster on a TPU?"
[00:37:30.000 --> 00:37:34.000]   So they try and optimize this EfficientNet paper on TPUs.
[00:37:34.000 --> 00:37:48.000]   And they introduced this idea that instead of having these two things, where a one-by-one convolution takes the channel size from 128 to 512, and then the depth-wise convolution occurs on this that does from 512 to 512,
[00:37:48.000 --> 00:38:04.000]   they replaced this with a single operation, which is this three-by-three convolution that's very much optimized and has a high utilization of 37.7% compared to a 1.07% on the modern accelerators that we use, TPUs or GPUs.
[00:38:04.000 --> 00:38:11.000]   So all that they did was replace this with this three-by-three convolution, and then everything else remains the same.
[00:38:11.000 --> 00:38:18.000]   So this architecture on the right is called, or this idea is called FusedNB-Conv.
[00:38:18.000 --> 00:38:25.000]   This is also called, I mean, in the EfficientNet V2 paper, they call it FusedNB-Conv in PyTorch Image Models library that I showed you.
[00:38:25.000 --> 00:38:29.000]   I think it's called Edge TPU Residual NB-Conv, something like that.
[00:38:29.000 --> 00:38:37.000]   Let me confirm.
[00:38:37.000 --> 00:38:50.000]   So something I want to show you is, again, when I mentioned on Twitter today that we're discussing the EfficientNet V2 paper, Ross Whitman, he's the author of PyTorch Image Models.
[00:38:50.000 --> 00:38:53.000]   Again, this is just everything that I've told you so far.
[00:38:53.000 --> 00:38:55.000]   He said the same thing.
[00:38:55.000 --> 00:39:00.000]   The FusedNB-Conv has been around since 2019, which is this exact paper.
[00:39:00.000 --> 00:39:04.000]   This Edge TPU paper has been around since 2019.
[00:39:04.000 --> 00:39:17.000]   And on your left is the NB-Conv that uses depth-wise convolution, and then they said it's not utilizing the GPUs or TPUs very well, so just replace it with three-by-three convolution.
[00:39:17.000 --> 00:39:25.000]   And it's just called Edge Residual in TIM.
[00:39:25.000 --> 00:39:28.000]   Basically, in the same library, it's just called Edge Residual.
[00:39:28.000 --> 00:39:37.000]   So anyway, if you want to try all of these things in code, definitely PyTorch Image Models is one of the easiest ways to go, and that's how I kind of go about it.
[00:39:37.000 --> 00:39:45.000]   But the way this paper reading group is different is that I'm trying to -- as we go, I'm also trying to give context on all of these things.
[00:39:45.000 --> 00:39:55.000]   So it might be a little more complex, but if you're not able to get -- like, if you're not able to get what I'm trying to explain right now, that's also completely okay.
[00:39:55.000 --> 00:39:58.000]   Go back. This video will be available on YouTube.
[00:39:58.000 --> 00:40:18.000]   Sit on your own time, and then all these references that I'm providing, read the EfficientNet blog, read this, have a look at this PyTorch Image Models, and then have a look at what progressive resizing -- so this idea of having two to four images at first and then increasing the image size that's introduced by Jeremy Howard.
[00:40:18.000 --> 00:40:22.000]   Like, go to this research paper, click on that, and then have a read on what that is.
[00:40:22.000 --> 00:40:29.000]   Once you have this sort of context that I'm giving you, it will really help you, and then you'll really be able to understand EfficientNet a lot better.
[00:40:29.000 --> 00:40:37.000]   But now, going forward, so now, so far, just summarizing what we've already looked at so far.
[00:40:37.000 --> 00:40:41.000]   We've already looked at what nbconv is.
[00:40:41.000 --> 00:40:43.000]   This uses DepthWise.
[00:40:43.000 --> 00:40:49.000]   We know that it's not the fastest way of utilizing the GPUs or modern accelerators like GPUs.
[00:40:49.000 --> 00:40:55.000]   So replacement for nbconv can be fused nbconv.
[00:40:55.000 --> 00:41:04.000]   It's just called -- it's funny when you think of context, it's just called fused nbconv because I like to think of it that they fuse these two operations as one.
[00:41:04.000 --> 00:41:07.000]   So that's why it's called fused nbconv.
[00:41:07.000 --> 00:41:11.000]   So fused nbconv just uses a regular three by three convolution.
[00:41:11.000 --> 00:41:16.000]   The next thing we've looked at is compound scaling.
[00:41:16.000 --> 00:41:21.000]   And the authors have mentioned that compound scaling is not the best way to go.
[00:41:21.000 --> 00:41:25.000]   There might be another way or a better way to go.
[00:41:25.000 --> 00:41:31.000]   And the last thing that we've looked at so far is progressive resizing.
[00:41:31.000 --> 00:41:39.000]   So progressive resizing is just this idea first introduced in 2018 by Jeremy that just goes from two to four.
[00:41:39.000 --> 00:41:42.000]   First you train your model on two to four image size.
[00:41:42.000 --> 00:41:44.000]   Then you train it on 256.
[00:41:44.000 --> 00:41:46.000]   And then you train it on 512.
[00:41:46.000 --> 00:41:54.000]   So we've just looked at these four things so far.
[00:41:54.000 --> 00:41:55.000]   Okay.
[00:41:55.000 --> 00:41:58.000]   So then the main contributions of this paper.
[00:41:58.000 --> 00:42:00.000]   I'm not going to read this paper first.
[00:42:00.000 --> 00:42:03.000]   I just like to read the abstract first and then go into the introduction.
[00:42:03.000 --> 00:42:08.000]   So the main contributions are we introduced this paper called efficient and we do.
[00:42:08.000 --> 00:42:10.000]   It uses something called neural architecture search.
[00:42:10.000 --> 00:42:13.000]   I will show you some context of what that means.
[00:42:13.000 --> 00:42:16.000]   They introduced an improved method of progressive learning.
[00:42:16.000 --> 00:42:18.000]   I mean they call it improved method.
[00:42:18.000 --> 00:42:20.000]   I kind of disagree with that.
[00:42:20.000 --> 00:42:22.000]   I think Jeremy already did all these experiments.
[00:42:22.000 --> 00:42:24.000]   He just didn't publish a paper.
[00:42:24.000 --> 00:42:29.000]   But this is all, again, things that have been there in the past.
[00:42:29.000 --> 00:42:39.000]   And then finally using these tidbits that have been there like the fuse and be convent 2019, the progressive resizing in 2018 and bringing them together and taking away the depth-wise convolution.
[00:42:39.000 --> 00:42:51.000]   This pretty much gets us to efficient and we do, which gives us this 11x faster training speed and 6.8x better parameter efficiency.
[00:42:51.000 --> 00:42:55.000]   So that's pretty much all there is.
[00:42:55.000 --> 00:42:57.000]   I mean there's not a lot in the efficient and we do paper.
[00:42:57.000 --> 00:43:00.000]   If you look at it, I mean it's just okay.
[00:43:00.000 --> 00:43:07.000]   If I'm a researcher, all I'm doing is I'm looking at okay what's new or what's what are these things that I can do to improve my network.
[00:43:07.000 --> 00:43:14.000]   Okay, in 2018 there was this idea of progressive resizing let's try and implement that in efficient that we want. Cool, that works.
[00:43:14.000 --> 00:43:23.000]   What else can I do, I know that the depth-wise convolution doesn't work so the fuse and be convent was released in 2019 let's find use that and let's see if that works.
[00:43:23.000 --> 00:43:45.000]   So when you bring all of these things together, that really gives you the efficient and we do architecture that it is today and it gets like 85.7%. I'm not making this number up it's mentioned in this paper somewhere that it gets 85.7% accuracy on image net so that's a really good high, high accuracy for for convolutional network.
[00:43:45.000 --> 00:43:52.000]   So something else I want to touch on is this there's a lot of reference for neural architecture search.
[00:43:52.000 --> 00:43:59.000]   So why is that I mean why is this neural architecture search being used what's the context of neural architecture search.
[00:43:59.000 --> 00:44:07.000]   So, the context has been given here in this figure two, table three of the efficient at V2 paper.
[00:44:07.000 --> 00:44:21.000]   So, from so far we know, in the con is not the way to go and become this slow it takes it utilizes the GPU, like 1% compared to 33% of regular con.
[00:44:21.000 --> 00:44:28.000]   And what we want to do is we want to replace this in the con with a with a fuse and become here and try and experiment that.
[00:44:28.000 --> 00:44:36.000]   And the efficient net.
[00:44:36.000 --> 00:44:40.000]   So this is what the efficient net architecture looks like. Right.
[00:44:40.000 --> 00:44:46.000]   So I'm just going to try and copy paste this.
[00:44:46.000 --> 00:45:02.000]   Would it let me, I'm not sure. Okay, it doesn't let me. So anyway, so this is what the efficient net. This is the efficient net, we want to see how it's using nb conven all of these stages it's using nb con here and become here, pretty much in all of the stages, and we already
[00:45:02.000 --> 00:45:09.000]   have a fused nb con with fuse and become right.
[00:45:09.000 --> 00:45:23.000]   But, but how do you replace like a do you replace all of them with fuse and become like this is another question like how do you go about replacing this nb con with fuse and become you replace all of them do you replace just the first one and everything else remains
[00:45:23.000 --> 00:45:35.000]   or do you replace just the last three with fuse and become I'm just going to call it FMC and do you replace just the last three or do you just replace the first fourth and seventh like what's the combination like how do you go about finding the right combination
[00:45:35.000 --> 00:45:45.000]   because if you go about doing it manually it's going to take ages because you have all these different combinations that you can try to replace nb con with fuse and become.
[00:45:45.000 --> 00:46:14.000]   So, so something that the authors did is they tried and replace the tried to replace this and become with the fuse and become in stages.
[00:46:14.000 --> 00:46:23.000]   I'm just pasting it here so you can kind of see what's going on.
[00:46:23.000 --> 00:46:26.000]   It should let me move it.
[00:46:26.000 --> 00:46:29.000]   As you can see I'm very slow when it comes to these things.
[00:46:29.000 --> 00:46:38.000]   But anyway, this is what's happening. So this was the on your right is this, this is the efficient net. We want.
[00:46:38.000 --> 00:46:55.000]   And then what the authors are doing is that they first try and replace in stage one, two, three, so they say okay let's replace this in stages one, two, three, and then what we can see is that if everything remains in because my accuracy is 82.8%.
[00:46:55.000 --> 00:47:09.000]   If I replace it just in the first three stages my accuracy goes up. And even though my flops go up the number of images, so flops are just multiply addition operations, basically.
[00:47:09.000 --> 00:47:26.000]   So, as I said, the nb con was not the.
[00:47:26.000 --> 00:47:42.000]   So, let's say I go 155 images per second on a B 100 GPU. But the minute you replace some of these, you replace these ones with a fuse and be con a, your accuracy goes up, and you can have more images on your B 100.
[00:47:42.000 --> 00:47:49.000]   So, so that's why we want to replace nb con with fuse and because fuse and become this faster.
[00:47:49.000 --> 00:48:02.000]   And then, okay, they say okay that works I mean that's a good result right. But what if I replaced from stage one to five. In that case my accuracy remains the same images go slightly up like from 216 to 223.
[00:48:02.000 --> 00:48:14.000]   And the last one is okay let's replace everything with a fuse and because, in that case the accuracy goes down, and the number of images also goes like 206 it's not the highest.
[00:48:14.000 --> 00:48:24.000]   So, a lot of thought okay there's no easy way for us to find what a good combination is of using fuse and be called and what's a good combination of using nb con.
[00:48:24.000 --> 00:48:28.000]   So what this said is let's use neural architecture search.
[00:48:28.000 --> 00:48:40.000]   How does neural architecture search look like
[00:48:40.000 --> 00:48:43.000]   going back to the blog.
[00:48:43.000 --> 00:48:53.000]   So, what does neural architecture search looks like in general I'm just giving you general context on what neural architecture search look like you have your models you have something called a trainer.
[00:48:53.000 --> 00:49:06.000]   So your controller is what controls your model architecture in a way, it's it's sampling models from a search space again, you don't need to know everything about what neural architecture search looks like but just some context would be helpful.
[00:49:06.000 --> 00:49:12.000]   I'm the controller so let me try and explain this.
[00:49:12.000 --> 00:49:25.000]   So, this is from the M NAS net paper. So there's all these different papers that come together in writing a single paper and to actually.
[00:49:25.000 --> 00:49:38.000]   So this is from M NAS net paper. So, M NAS net is the paper that used that you sort of this neural architecture search idea for mobile phones.
[00:49:38.000 --> 00:49:50.000]   So, we should and we want simply reuse this same architecture to then have neural architecture search to find right now the question we're trying to solve is what's a good combination of nb con been fused and be called.
[00:49:50.000 --> 00:50:00.000]   So you have this controller so this, let's say you have all these different models you have m one, m two, m three, m seven all these different.
[00:50:00.000 --> 00:50:07.000]   I call it search space I'm just trying to simplify this neural architecture search for everybody. This is just a very simple version of neural architecture search.
[00:50:07.000 --> 00:50:19.000]   So let's say you have all these different models that basically your search space looks something like this, where in m one you have just a fuse and become with the first layer in and do you have in the first two three in m three you have first five or first
[00:50:19.000 --> 00:50:22.000]   to seven, or whatever.
[00:50:22.000 --> 00:50:32.000]   And then there's all these different operations that you can have, you have like a three by three convolution like basically you have a lot of things that you can pick and choose from.
[00:50:32.000 --> 00:50:44.000]   So, your subset looks like this there's like all of these different things that all of these different combinations all of these different components that you can pick and choose your trainer will train the model.
[00:50:44.000 --> 00:50:51.000]   And then you will check the accuracy and latency which becomes your objective reward. And then based on this reward.
[00:50:51.000 --> 00:50:59.000]   The controller will say, oh, the accuracy is not up to the mark it's not as good as I wanted to be so let's try a different combination.
[00:50:59.000 --> 00:51:13.000]   It would say okay even though the accuracy is high the latency is not that good for me. So let's try a different combination. And then by repeating this process over and over and over again, you come to architecture that you think that then the controller
[00:51:13.000 --> 00:51:29.000]   will say, okay, this is good enough. In this case, my reward function is the highest my accuracy is good and my latency is good. So that's this idea of neural architecture so it's like you're no longer designing your network architecture, the deep
[00:51:29.000 --> 00:51:36.000]   learning using reinforcement learning this neural architecture search idea is doing it for you.
[00:51:36.000 --> 00:51:43.000]   And then by doing this, doing this process.
[00:51:43.000 --> 00:51:47.000]   It gives us the efficient that we do architecture.
[00:51:47.000 --> 00:51:57.000]   So this is what efficient and we do architecture looks like. So that's it like I could just stop right here and I could tell you, that's it that's all that there is to know about efficient that we do.
[00:51:57.000 --> 00:52:09.000]   So let's take more things so you can see how now it's using still a combination of fuse and be convinced and be con, but you can see that it's using fuse and be con in the earlier list.
[00:52:09.000 --> 00:52:13.000]   So let me go back to the drawbacks.
[00:52:13.000 --> 00:52:24.000]   So one of the drawbacks that we had a look at was this, the depth wise convolutions are really slow in the early list. That was convolutions are used in nb cons.
[00:52:24.000 --> 00:52:35.000]   So to take that away. The new architecture uses fuse then becomes in the earlier list so that way you're not even using depth wise convolution in the earlier list so that's been fixed.
[00:52:35.000 --> 00:52:49.000]   The next thing was compound scaling we'll have a look at that very soon. But then this is what the efficient at we do architecture looks like and this is the process and how this this architecture was designed.
[00:52:49.000 --> 00:52:52.000]   So, taking questions now.
[00:52:52.000 --> 00:53:00.000]   So there's any questions.
[00:53:00.000 --> 00:53:11.000]   Hey, I'm on in the few stem in me calm in the diagram there was something called se is that the squeeze and excitation blogger.
[00:53:11.000 --> 00:53:16.000]   Absolutely. That is, that is squeezing excitation.
[00:53:16.000 --> 00:53:28.000]   And for everybody else who doesn't know squeeze and excitation, I somehow keep referring to my blocks because I've just written all about them. There is a squeeze and excitation blog that I've also written in the past.
[00:53:28.000 --> 00:53:30.000]   I'll just share this with everybody.
[00:53:30.000 --> 00:53:35.000]   So in this squeeze and excitation will just show you what what squeeze and excitation means.
[00:53:35.000 --> 00:53:45.000]   But to answer the question yes se is is basically just adding squeeze and excitation. Where did you see that sorry. Where was the se I forgotten where se was.
[00:53:45.000 --> 00:53:56.000]   Hello.
[00:53:56.000 --> 00:54:04.000]   It was in the, it was in the MB con that that diagram that you were showing right.
[00:54:04.000 --> 00:54:28.000]   Yeah, MB con.
[00:54:28.000 --> 00:54:35.000]   So that's what basically fuse and become looks like. But let's just add squeeze and excitation to it.
[00:54:35.000 --> 00:54:45.000]   So it just adds a bit more computation. Basically, squeeze and excitation is just a way of like adding attention to your network so then it pays attention to the different channels differently.
[00:54:45.000 --> 00:54:47.000]   But for context again.
[00:54:47.000 --> 00:54:49.000]   We're not discussing that today.
[00:54:49.000 --> 00:54:52.000]   But yes, thank you. That's a really good question.
[00:54:52.000 --> 00:55:03.000]   Are there any other questions so far on how the efficient and we do paper has been designed or what's going on with efficient and we do.
[00:55:03.000 --> 00:55:08.000]   Interesting to see the GPU cores are slightly slower than GPUs.
[00:55:08.000 --> 00:55:10.000]   Oh, that's not a.
[00:55:10.000 --> 00:55:15.000]   I don't think that's a way. How did.
[00:55:15.000 --> 00:55:21.000]   Who's commented that same and how is that.
[00:55:21.000 --> 00:55:30.000]   I was just looking at table three and I think this is like an inappropriate comparison because our GPU has like more than one course, but this was in table three.
[00:55:30.000 --> 00:55:31.000]   Oh, this one.
[00:55:31.000 --> 00:55:32.000]   Yes.
[00:55:32.000 --> 00:55:38.000]   So how do you know that because our TPUs are pretty much having more images per second for core compared to.
[00:55:38.000 --> 00:55:45.000]   Oh, sorry I was reading it and was I thought I thought it was seconds.
[00:55:45.000 --> 00:55:50.000]   That's okay. I think that's fine. Yeah, that's why I was like okay.
[00:55:50.000 --> 00:56:01.000]   Sorry is the TPU really slow because that shouldn't be because now with the TPU VMs out I mean that's just all the craze that there is today. TPUs are generally meant to be faster so I think that's a great comment.
[00:56:01.000 --> 00:56:12.000]   Anyway, um, so that's that's like to summarize, and there's more in the paper like they do more ablation studies and they, they dig deeper into the efficient and we do paper.
[00:56:12.000 --> 00:56:18.000]   But to summarize I think I'll summarize everything that we've we've sort of learned so far.
[00:56:18.000 --> 00:56:20.000]   It looks like this.
[00:56:20.000 --> 00:56:26.000]   Efficient at we want started with
[00:56:26.000 --> 00:56:30.000]   nbconf layers.
[00:56:30.000 --> 00:56:41.000]   Really fast, so it was fast and accurate efficient at we want have been in the top most performing networks and this is not to take away.
[00:56:41.000 --> 00:56:52.000]   It's this is not to say that efficient we want was a bad architecture efficient and we want if not was one of the best and highly performing architectures, using a convolution neural network.
[00:56:52.000 --> 00:57:10.000]   And in fact, all the latest architectures that you see like with or kite or basically all these transform architectures all these new architectures that are coming, even NF nets all these latest papers that are coming they compare with efficient nets, we want.
[00:57:10.000 --> 00:57:21.000]   So this is a really good benchmark or this was efficient and we want was kind of the king of the jungle for quite a quite some time and it was used a lot.
[00:57:21.000 --> 00:57:34.000]   But the authors thought okay there's some some things that are wrong nbconf is slow in the early days. So what can we do, let's try and replace this with fused nbconf.
[00:57:34.000 --> 00:57:50.000]   And then, this gives us rise to the efficient net we to paper that uses fused nbconf in the earlier layers and nbconf in the later layers.
[00:57:50.000 --> 00:58:04.000]   So that's how this architecture was born and the process of finding this efficient that we do architecture was through neural architecture search. So that's just how this design architecture looks like.
[00:58:04.000 --> 00:58:12.000]   So that's just the design of this architecture. What we're still missing is the training.
[00:58:12.000 --> 00:58:23.000]   So, how did the authors go about okay they designed a new architecture that's perfect I mean it's it's it's faster, we all know it's like it has it can process more images and all that.
[00:58:23.000 --> 00:58:30.000]   But how do you then go about training this network.
[00:58:30.000 --> 00:58:41.000]   How do you go about training and how do you go about scaling this network. So, note that this efficient and we do architecture that we designed.
[00:58:41.000 --> 00:58:58.000]   The authors designed over here is a small architecture. But if you want to, like, if you want to have a higher accuracy, you want to be able to scale this network in depth with our resolution so then there's two things that are still remaining for us to be answered,
[00:58:58.000 --> 00:59:07.000]   is the training and the scaling scaling just means making this model architecture bigger.
[00:59:07.000 --> 00:59:19.000]   And those are answered here. So then, they just say we scale up with few additional optimizations.
[00:59:19.000 --> 00:59:29.000]   We restrict the maximum image size to 480. So what they've said is, when we're scaling the image resolution, we will not go above 480 as an image size.
[00:59:29.000 --> 00:59:38.000]   That's because one of the drawbacks that they found with the efficient at me one paper was that training on very large image sizes is slow, we've seen that in part of the products.
[00:59:38.000 --> 00:59:52.000]   So they said as part of that drawback. Let's do this, let's not go above 480 as very large images, often lead to extensive memory and training speed overhead.
[00:59:52.000 --> 01:00:08.000]   So, that's just the first, like this is again scaling it still compound scaling like it's still scaling the depth with and resolution, but in terms of the resolution they're not going above 480 so that's one of the key differences in efficient that we do.
[01:00:08.000 --> 01:00:13.000]   The next thing is progressive learning, as I've mentioned, so progressive learning.
[01:00:13.000 --> 01:00:19.000]   So this is again part of the training.
[01:00:19.000 --> 01:00:23.000]   So, the answer for this is over here.
[01:00:23.000 --> 01:00:38.000]   Progressive learning with adaptive regularization. So what they say with progressive learning is, this is my model, let's call it efficient that we do let's just call it for simplicity.
[01:00:38.000 --> 01:00:54.000]   Let's train this on two to four by two to four for say, if the total epochs are hundred like I want to train this model for hundred epochs. Let's just train this for 25 epochs on this image size.
[01:00:54.000 --> 01:01:04.000]   Then let's just go to 256 by 256 for another 25 epochs, let's just go to three.
[01:01:04.000 --> 01:01:13.000]   I don't know 348 just making these numbers up but just to show you like how that looks like. 348 by 348. And then you finally go 480 by 480.
[01:01:13.000 --> 01:01:24.000]   So that's not how these images would scale like they have a rule on how these image sizes would go up. So, this image size would go up based on a rule, that rule is mentioned in algorithm one.
[01:01:24.000 --> 01:01:29.000]   I'm just trying to simplify that process on what progressive learning looks like.
[01:01:29.000 --> 01:01:40.000]   So they say okay for the next 25 epochs, train on 256 by 256, then 348 by 245, and then lastly on 480 by 480 because that's the highest you can go.
[01:01:40.000 --> 01:01:50.000]   What they say like how like what they say their innovation is, you have in terms of regularization, you have three things.
[01:01:50.000 --> 01:02:04.000]   Crop out, random augment, so it's called random augment. It's just a data augmentation and you have mix up. All these three things kind of add regularization.
[01:02:04.000 --> 01:02:12.000]   If you don't know what any of these are again to give you context.
[01:02:12.000 --> 01:02:24.000]   To give you context, if you go in Tim Docs, sorry that's not the right place. If you go in Tim Docs, if you go in augmentation in brand augment, I'll just copy paste this as well.
[01:02:24.000 --> 01:02:33.000]   This will give you an introduction on what random augment looks like and how you can experiment with random augment using the Titus image models library.
[01:02:33.000 --> 01:02:43.000]   So basically what they said, the key idea, drop out, you just search dropout, you'll find lots of articles and again same for mix up. There's plenty of good articles on mix up.
[01:02:43.000 --> 01:02:58.000]   So what this, what the idea was, if the regularization here is R naught, so this is my regularization, as the image size goes up, what that means is if the image size is bigger, that means there's more computation.
[01:02:58.000 --> 01:03:13.000]   And if there's more computation, that means that my model can sort of over fit, like it has more capacity. Then what you do is you increase the regularization. So it goes R naught plus some number, like let's just call it R1.
[01:03:13.000 --> 01:03:30.000]   This regularization will then go R1. And then you again increase regularization to R2 and then you again increase regularization to R3. There's a very simple rule. It's just basically you have initial plus final minus initial by number of stages.
[01:03:30.000 --> 01:03:41.000]   And then it goes up from initial plus final minus initial by number of stages. That's just increasing the image size and that's just scaling the regularization based on this rule.
[01:03:41.000 --> 01:03:53.000]   So they use this algorithm one to train the efficient NP2 paper. And this idea of increasing regularization as you're increasing the image size, they termed this as progressive learning.
[01:03:53.000 --> 01:03:59.000]   And these are the three regularizations, as I've already mentioned.
[01:03:59.000 --> 01:04:17.000]   And that's it. That's really it. In terms of then this progressive training settings, they just say, OK, if you have a look and if you now go back and read this paper, Efficient at V2, you can go back and have a look at what this algorithm one looks like.
[01:04:17.000 --> 01:04:33.000]   And then from that algorithm one, this table six will tell you, OK, Efficient at V2 S looks like this. It goes from 128 to 300 image size from random augment to 5 to 15, dropout from 0.1 to 0.3, L goes from 128 to 380 and all that stuff.
[01:04:33.000 --> 01:04:39.000]   Like this will all give you the configuration. So now you know what these different architectures look like.
[01:04:39.000 --> 01:04:50.000]   If you want to experiment with Efficient at V2 in code, again, PyTorch, if you want to do it in PyTorch, PyTorch image models is one of the ways to go about it.
[01:04:50.000 --> 01:05:10.000]   You can just say in PyTorch image models, you can just say something like you can just say import Tim and then Tim.createModel, just pass in Efficient at V2 here. And that's it. That will create your model.
[01:05:10.000 --> 01:05:29.000]   That's just how Tim works. It's that easy in PyTorch image models. In TensorFlow, I think the original source code is in TensorFlow. So let me try and find that.
[01:05:29.000 --> 01:05:46.000]   But that's pretty much it in terms of understanding this paper. So let's have a look at-- there it is. So in terms of TensorFlow in AutoML, Efficient at V2, this is where you want to go. So let me copy paste that.
[01:05:46.000 --> 01:05:51.000]   Cool. That's it. That's Efficient at V2 paper. Does anybody have a question?
[01:05:51.000 --> 01:05:53.000]   Yeah, I have a question.
[01:05:53.000 --> 01:05:54.000]   Yeah.
[01:05:54.000 --> 01:06:10.000]   So I'm new to neural architecture search. So the thing is that this Efficient at must have been-- the reinforcement learning must have been trained on a specific data set, correct?
[01:06:10.000 --> 01:06:11.000]   Keep going.
[01:06:11.000 --> 01:06:25.000]   So will that be specific to a given data set, or does this neural architecture search algorithm supply across data? So to be more specific, so let's say NASNet V1, for example.
[01:06:25.000 --> 01:06:35.000]   So is that used widely as a backbone for most of the algorithms like object detection in general? I mean, in practice, how is it applicable?
[01:06:35.000 --> 01:06:47.000]   So neural architecture search is a general concept that can be applied to lots of different data or different model architectures. In terms of backbones, there's a lot of different backbones that are available.
[01:06:47.000 --> 01:07:01.000]   So MNASNet was just a mobile architecture. You're free to use an MNASNet backbone or you're free to use an Efficient backbone or all these different backbones that are available.
[01:07:01.000 --> 01:07:06.000]   But to answer the question in terms of reinforcement learning, that's a very general concept.
[01:07:06.000 --> 01:07:17.000]   So I mean, should we run this architecture search on the different data sets? Will that be more efficient or just using this given architecture on any kind of--
[01:07:17.000 --> 01:07:27.000]   Because you're trying to optimize on ImageNet, I mean, that's just the easiest way to go because you want to optimize for a certain amount of images.
[01:07:27.000 --> 01:07:38.000]   If your end goal is to train ImageNet, then you use neural architecture search using ImageNet. If your end goal is to use a medical, then you can try with medical data sets.
[01:07:38.000 --> 01:07:53.000]   Personally, I haven't dabbled with neural architecture search much to know that if you want to-- I mean, I think what you're asking is, if you want to train on ImageNet, then would it make sense to use neural architecture search on ImageNet, medical images, all these different data sets that are there?
[01:07:53.000 --> 01:08:06.000]   I would say personally from what I've read, that's not common. People, when they're training on ImageNet, they use ImageNet images.
[01:08:06.000 --> 01:08:08.000]   Does that help?
[01:08:08.000 --> 01:08:10.000]   Yeah.
[01:08:10.000 --> 01:08:23.000]   I want to just add a comment there. So back in the day when Kaggle didn't have a restriction on GPUs, and I assume this is the reason they added that restriction, I would just try to run something along these lines.
[01:08:23.000 --> 01:08:39.000]   But it's quite expensive in terms of GPUs. So normally you wouldn't even want to think about running a NAS search at all, unless you have just like thousands of free credits and you just want to go through them.
[01:08:39.000 --> 01:08:42.000]   Yeah.
[01:08:42.000 --> 01:08:58.000]   Any hypothesis on why Fused nbconv for all layers reduces accuracy compared to just using it for the first three layers and sticking with nbconv6? I don't know. That's a really good question. I also was curious about that, why would nbconv do that?
[01:08:58.000 --> 01:09:05.000]   But that's an open question. I don't have an answer for that.
[01:09:05.000 --> 01:09:11.000]   With respect to depth-- sorry.
[01:09:11.000 --> 01:09:20.000]   With respect to depth-wise convolutions having less operation performing poorly, the only reliable measure of computer performance is time, less operation does not.
[01:09:20.000 --> 01:09:34.000]   Yes, that's a good thing. I mean, less multiplication addition operation does not necessarily mean that it will take less time and be faster on a GPU. So that's a really good comment.
[01:09:34.000 --> 01:09:44.000]   It makes sense on progressive scaling, but how do you dynamically scale the depth when you change the network architecture? So I think that's just, again, going back to compound scaling.
[01:09:44.000 --> 01:09:54.000]   So I think that's, again, very similar to what efficient at v1 does. You just add, like increase the number of blocks. So if you have like five blocks, you just make it 10 blocks.
[01:09:54.000 --> 01:10:06.000]   But overall, like the structure of those blocks or a fused nbconv remains fused nbconv. Instead of just one, you have three in bigger networks.
[01:10:06.000 --> 01:10:15.000]   That's just the way of scaling. And that's, again, very similar. So in efficient at v2, I think they scaled it exactly the same way as they scaled efficient at v1.
[01:10:15.000 --> 01:10:27.000]   I need to go back and confirm, but that's what I think I read, is that they just scale it the same way as they scale efficient at, because they know that that scaling sort of works by efficient at v1, then why change it for v2?
[01:10:27.000 --> 01:10:41.000]   It does change the network structure as in, of course, it's more deeper. It has more layers, but that's the idea. That's why you want to change the network structure to make it deeper, like so it can have more computation.
[01:10:41.000 --> 01:10:46.000]   Cool. Hope that helps. Taking last question if there is any.
[01:10:46.000 --> 01:10:51.000]   So I'm still unclear on the algorithm one table.
[01:10:51.000 --> 01:10:55.000]   So, algorithm one.
[01:10:55.000 --> 01:10:57.000]   This one.
[01:10:57.000 --> 01:10:58.000]   Yes.
[01:10:58.000 --> 01:11:04.000]   Oh, yes. Okay. Okay. Do you want me to try and explain that very quickly. Yes, yes please.
[01:11:04.000 --> 01:11:16.000]   Okay, it's very easy. So if you have, let's say you have your minimum image size is 100. So you want to start with a minimum image size of 100. And let's say you want to go to a max image size of 200.
[01:11:16.000 --> 01:11:19.000]   Right. And let's say you're training in four stages.
[01:11:19.000 --> 01:11:32.000]   So four stages. So the first thing you go is you start with 100. Then the next thing you go is you go 100 plus 200 minus 100 by four, which is 100, which is 125.
[01:11:32.000 --> 01:11:52.000]   Then you go 100 plus twice 200 minus 100 by four, which is 150. And then you go 175. Like that's how this scaling works. Like that's just, that's exactly what this algorithm is saying. You go initial plus final minus initial by m minus one. Sorry.
[01:11:52.000 --> 01:11:54.000]   This should be three.
[01:11:54.000 --> 01:11:56.000]   My bad.
[01:11:56.000 --> 01:12:06.000]   This should be three, this becomes 133, this should be three, this becomes 166 and then this becomes 199.9 or something like that. But that's just it.
[01:12:06.000 --> 01:12:11.000]   Does that help now? Yes, yes. And then how do you then calculate the regularization you need to apply?
[01:12:11.000 --> 01:12:26.000]   Same. So see how in this table they have like a minimum and maximum dropout rate. So if you have, let's say, they said the 0.1 to 0.3, then you go 0.1 plus 0.3 minus 0.1 by three.
[01:12:26.000 --> 01:12:41.000]   Then 0.1 plus three times 0.3 minus 0.1 by three. So this gets canceled and then you get whatever. I mean, this is just how you go. So in the end you're going to 0.3.
[01:12:41.000 --> 01:12:48.000]   Okay, gotcha. So you apply all three regularizations all the time. It is just how much you apply varies.
[01:12:48.000 --> 01:13:00.000]   No, it's in the stages, right? So because if you have 100 epochs, you're training it in stages. So the first 25 epochs, you train with the image size of I naught and a regularization of R naught.
[01:13:00.000 --> 01:13:13.000]   So this could be 100 and this could be 0.1. In the second stage of 25 epochs, you go image size of 133 and then the regularization becomes whatever the answer is of 0.1 plus 0.2 by three, basically.
[01:13:13.000 --> 01:13:22.000]   So whatever that answer is, then that becomes a regularization. Then in the next 25 epochs, you go image size 166 and then whatever the answer of regularization is.
[01:13:22.000 --> 01:13:28.000]   And then in the last 25 epochs, you go 200 and then 0.3 as your regularization. Does that help?
[01:13:28.000 --> 01:13:41.000]   Yes, yes. It's very interesting that they do it in step like this, which doesn't depend upon your loss curves or anything. It actually just goes based on fixed epochs.
[01:13:41.000 --> 01:13:54.000]   Yeah, yeah, totally. But yeah, again, this idea of progressive resizing is not new. I've seen this being implemented in the fast AI library before.
[01:13:54.000 --> 01:14:03.000]   They just sort of do it in one step instead of training this first and then reusing the weights. It's just one step.
[01:14:03.000 --> 01:14:14.000]   Anyway, so that's it for today. If there's any other questions, please feel free to post them at this URL, 1dbe.me/efficientretwe2.
[01:14:14.000 --> 01:14:17.000]   And cool. I'll see you guys next week.
[01:14:17.000 --> 01:14:38.000]   [MUSIC]

