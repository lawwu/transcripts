
[00:00:00.000 --> 00:00:03.120]   The following is a conversation with Peter Abbeel.
[00:00:03.120 --> 00:00:07.840]   He's a professor at UC Berkeley and the director of the Berkeley Robotics Learning Lab.
[00:00:07.840 --> 00:00:15.360]   He's one of the top researchers in the world working on how we make robots understand and interact with the world around them,
[00:00:15.360 --> 00:00:19.720]   especially using imitation and deep reinforcement learning.
[00:00:19.720 --> 00:00:26.400]   This conversation is part of the MIT course on artificial general intelligence and the Artificial Intelligence Podcast.
[00:00:26.400 --> 00:00:31.680]   If you enjoy it, please subscribe on YouTube, iTunes, or your podcast provider of choice,
[00:00:31.680 --> 00:00:36.920]   or simply connect with me on Twitter @LexFriedman, spelled F-R-I-D.
[00:00:36.920 --> 00:00:41.400]   And now, here's my conversation with Peter Abbeel.
[00:00:41.400 --> 00:00:46.200]   You've mentioned that if there was one person you could meet, it would be Roger Federer.
[00:00:46.200 --> 00:00:54.320]   So let me ask, when do you think we'll have a robot that fully autonomously can beat Roger Federer at tennis,
[00:00:54.320 --> 00:00:57.520]   a Roger Federer-level player at tennis?
[00:00:57.520 --> 00:01:02.560]   Well, first, if you can make it happen for me to meet Roger, let me know.
[00:01:02.560 --> 00:01:08.920]   In terms of getting a robot to beat him at tennis, it's kind of an interesting question,
[00:01:08.920 --> 00:01:16.760]   because for a lot of the challenges we think about in AI, the software is really the missing piece.
[00:01:16.760 --> 00:01:22.640]   But for something like this, the hardware is nowhere near either.
[00:01:22.640 --> 00:01:28.520]   To really have a robot that can physically run around, the Boston Dynamics robots are starting to get there,
[00:01:28.520 --> 00:01:36.800]   but still not really human-level ability to run around and then swing a racket.
[00:01:36.800 --> 00:01:38.320]   So you think that's a hardware problem?
[00:01:38.320 --> 00:01:41.560]   I don't think it's a hardware problem only. I think it's a hardware and a software problem.
[00:01:41.560 --> 00:01:45.640]   I think it's both. And I think they'll have independent progress.
[00:01:45.640 --> 00:01:51.560]   So I'd say the hardware, maybe in 10, 15 years.
[00:01:51.560 --> 00:01:55.000]   On clay, not grass. I mean, grass is probably harder.
[00:01:55.000 --> 00:01:58.840]   Well, the clay, I'm not sure what's harder, grass or clay.
[00:01:58.840 --> 00:02:05.880]   The clay involves sliding, which might be harder to master, actually.
[00:02:05.880 --> 00:02:09.880]   But you're not limited to bipedal. I'm sure there's no...
[00:02:09.880 --> 00:02:13.080]   Well, if we can build a machine, it's a whole different question, of course.
[00:02:13.080 --> 00:02:17.560]   If you can say, "Okay, this robot can be on wheels, it can move around on wheels,
[00:02:17.560 --> 00:02:22.680]   and can be designed differently," then I think that can be done sooner,
[00:02:22.680 --> 00:02:26.120]   probably than a full humanoid type of setup.
[00:02:26.120 --> 00:02:31.160]   What do you think of swing a racket? So you've worked at basic manipulation.
[00:02:31.160 --> 00:02:34.120]   How hard do you think is the task of swinging a racket,
[00:02:34.120 --> 00:02:39.240]   with being able to hit a nice backhand or a forehand?
[00:02:39.240 --> 00:02:43.960]   Let's say we just set up stationary, a nice robot arm, let's say,
[00:02:43.960 --> 00:02:48.360]   you know, a standard industrial arm, and it can watch the ball come
[00:02:48.360 --> 00:02:51.400]   and then swing the racket. It's a good question.
[00:02:51.400 --> 00:02:56.040]   I'm not sure it would be super hard to do.
[00:02:56.040 --> 00:02:58.040]   I mean, I'm sure it would require a lot...
[00:02:58.040 --> 00:03:01.400]   If we do it with reinforced milling, it would require a lot of trial and error.
[00:03:01.400 --> 00:03:03.240]   It's not going to swing it right the first time around.
[00:03:03.240 --> 00:03:09.320]   But yeah, I don't see why I couldn't swing it the right way.
[00:03:09.320 --> 00:03:11.960]   I think it's learnable. I think if you set up a ball machine,
[00:03:11.960 --> 00:03:17.640]   let's say, on one side, and then a robot with a tennis racket on the other side,
[00:03:17.640 --> 00:03:23.000]   I think it's learnable. And maybe a little bit of pre-training and simulation.
[00:03:23.000 --> 00:03:27.160]   Yeah, I think that's feasible. I think the swing the racket is feasible.
[00:03:27.160 --> 00:03:31.320]   It'd be very interesting to see how much precision it can get.
[00:03:31.320 --> 00:03:36.760]   Because, I mean, that's where, I mean, some of the human players can hit it
[00:03:36.760 --> 00:03:39.160]   on the lines, which is very high precision.
[00:03:39.160 --> 00:03:41.960]   With spin. The spin is an interesting...
[00:03:41.960 --> 00:03:45.560]   Whether RL can learn to put a spin on the ball.
[00:03:45.560 --> 00:03:48.120]   Well, you got me interested. Maybe someday we'll set this up.
[00:03:48.120 --> 00:03:48.620]   Someday, sure.
[00:03:48.620 --> 00:03:51.080]   You got me intrigued.
[00:03:51.080 --> 00:03:54.120]   Your answer is basically, okay, for this problem, it sounds fascinating.
[00:03:54.120 --> 00:03:57.960]   But for the general problem of a tennis player, we might be a little bit farther away.
[00:03:57.960 --> 00:04:02.200]   What's the most impressive thing you've seen a robot do in the physical world?
[00:04:04.120 --> 00:04:10.920]   So physically, for me, it's the Boston Dynamics videos.
[00:04:10.920 --> 00:04:14.200]   Always just bring home and just super impressed.
[00:04:14.200 --> 00:04:19.400]   Recently, the robot running up the stairs, doing the parkour type thing.
[00:04:19.400 --> 00:04:22.200]   I mean, yes, we don't know what's underneath.
[00:04:22.200 --> 00:04:23.880]   They don't really write a lot of detail.
[00:04:23.880 --> 00:04:28.360]   But even if it's hard-coded underneath, which it might or might not be,
[00:04:28.360 --> 00:04:31.880]   just the physical abilities of doing that parkour, that's a very impressive.
[00:04:32.600 --> 00:04:36.680]   So have you met Spot Mini or any of those robots in person?
[00:04:36.680 --> 00:04:42.840]   Met Spot Mini last year in April at the Mars event that Jeff Bezos organizes.
[00:04:42.840 --> 00:04:47.640]   They brought it out there and it was nicely following around Jeff.
[00:04:47.640 --> 00:04:52.120]   When Jeff left the room, they had it follow him along, which was pretty impressive.
[00:04:52.120 --> 00:04:57.960]   So I think there's some confidence to know that there's no learning going on in those robots.
[00:04:57.960 --> 00:04:58.920]   The psychology of it.
[00:04:58.920 --> 00:05:03.400]   So while knowing that, while knowing there's not, if there's any learning going on, it's very limited.
[00:05:03.400 --> 00:05:08.680]   I met Spot Mini earlier this year and knowing everything that's going on,
[00:05:08.680 --> 00:05:12.360]   having one-on-one interaction, so I get to spend some time alone.
[00:05:12.360 --> 00:05:18.200]   And there's immediately a deep connection on the psychological level.
[00:05:18.200 --> 00:05:22.280]   Even though you know the fundamentals, how it works, there's something magical.
[00:05:23.240 --> 00:05:29.000]   So do you think about the psychology of interacting with robots in the physical world?
[00:05:29.000 --> 00:05:35.960]   Even you just showed me the PR2, the robot, and there was a little bit something like a face,
[00:05:35.960 --> 00:05:38.360]   had a little bit something like a face.
[00:05:38.360 --> 00:05:40.520]   There's something that immediately draws you to it.
[00:05:40.520 --> 00:05:45.000]   Do you think about that aspect of the robotics problem?
[00:05:45.000 --> 00:05:48.200]   Well, it's very hard with Brad here.
[00:05:48.200 --> 00:05:52.040]   We'll give him a name, Berkeley Robot, for the elimination of tedious tasks.
[00:05:52.040 --> 00:05:56.440]   It's very hard to not think of the robot as a person.
[00:05:56.440 --> 00:05:59.400]   And it seems like everybody calls them a he for whatever reason,
[00:05:59.400 --> 00:06:01.880]   but that also makes it more a person than if it was a it.
[00:06:01.880 --> 00:06:07.160]   And it seems pretty natural to think of it that way.
[00:06:07.160 --> 00:06:08.520]   This past weekend really struck me.
[00:06:08.520 --> 00:06:15.160]   I've seen Pepper many times on videos, but then I was at an event organized by,
[00:06:15.160 --> 00:06:21.880]   this was by Fidelity, and they had scripted Pepper to help moderate some sessions.
[00:06:22.600 --> 00:06:26.360]   And they had scripted Pepper to have the personality of a child a little bit.
[00:06:26.360 --> 00:06:31.720]   And it was very hard to not think of it as its own person in some sense,
[00:06:31.720 --> 00:06:34.360]   because it was just kind of jumping, it would just jump into conversation,
[00:06:34.360 --> 00:06:35.720]   making it very interactive.
[00:06:35.720 --> 00:06:39.240]   Moderate would be saying, Pepper would just jump in, "Hold on, how about me?
[00:06:39.240 --> 00:06:41.240]   Can I participate in this too?"
[00:06:41.240 --> 00:06:43.560]   And just like, "Okay, this is like a person."
[00:06:43.560 --> 00:06:45.400]   And that was 100% scripted.
[00:06:45.400 --> 00:06:49.640]   And even then it was hard not to have that sense of somehow there is something there.
[00:06:50.520 --> 00:06:54.280]   So as we have robots interact in this physical world,
[00:06:54.280 --> 00:06:57.080]   is that a signal that could be used in reinforcement learning?
[00:06:57.080 --> 00:07:00.120]   You've worked a little bit in this direction,
[00:07:00.120 --> 00:07:02.920]   but do you think that psychology can be somehow pulled in?
[00:07:02.920 --> 00:07:08.920]   Yes, that's a question I would say a lot of people ask.
[00:07:08.920 --> 00:07:12.840]   And I think part of why they ask it is they're thinking about
[00:07:12.840 --> 00:07:16.520]   how unique are we really still as people?
[00:07:16.520 --> 00:07:19.640]   Like after they see some results, they see a computer play Go,
[00:07:19.640 --> 00:07:23.640]   they see a computer do this, that, they're like, "Okay, but can it really have emotion?
[00:07:23.640 --> 00:07:26.680]   Can it really interact with us in that way?"
[00:07:26.680 --> 00:07:29.960]   And then once you're around robots, you already start feeling it.
[00:07:29.960 --> 00:07:34.280]   And I think that kind of maybe methodologically, the way that I think of it is,
[00:07:34.280 --> 00:07:39.000]   if you run something like reinforcement learning, it's about optimizing some objective.
[00:07:39.000 --> 00:07:46.360]   And there's no reason that the objective couldn't be tied into
[00:07:47.480 --> 00:07:50.600]   how much does a person like interacting with this system?
[00:07:50.600 --> 00:07:54.280]   And why could not the reinforcement learning system optimize for
[00:07:54.280 --> 00:07:56.040]   the robot being fun to be around?
[00:07:56.040 --> 00:08:00.440]   And why wouldn't it then naturally become more and more attractive and more and more,
[00:08:00.440 --> 00:08:04.520]   maybe like a person or like a pet, I don't know what it would exactly be,
[00:08:04.520 --> 00:08:08.120]   but more and more have those features and acquire them automatically.
[00:08:08.120 --> 00:08:13.160]   - As long as you can formalize an objective of what it means to like something,
[00:08:13.160 --> 00:08:16.600]   how you exhibit, what's the ground truth?
[00:08:17.240 --> 00:08:19.320]   How do you get the reward from human?
[00:08:19.320 --> 00:08:22.280]   Because you have to somehow collect that information within you, human.
[00:08:22.280 --> 00:08:26.840]   But you're saying if you can formulate as an objective, it can be learned.
[00:08:26.840 --> 00:08:29.320]   - There's no reason it couldn't emerge through learning.
[00:08:29.320 --> 00:08:31.400]   And maybe one way to formulate as an objective,
[00:08:31.400 --> 00:08:33.720]   you wouldn't have to necessarily score it explicitly.
[00:08:33.720 --> 00:08:37.720]   So standard rewards are numbers, and numbers are hard to come by.
[00:08:37.720 --> 00:08:41.160]   This is a 1.5 or 1.7 on some scale.
[00:08:41.160 --> 00:08:42.840]   It's very hard to do for a person.
[00:08:42.840 --> 00:08:44.680]   But much easier is for a person to say,
[00:08:45.320 --> 00:08:49.000]   "Okay, what you did the last five minutes was much nicer
[00:08:49.000 --> 00:08:51.080]   than we did the previous five minutes."
[00:08:51.080 --> 00:08:53.000]   And that now gives a comparison.
[00:08:53.000 --> 00:08:55.160]   And in fact, there have been some results in that.
[00:08:55.160 --> 00:09:00.040]   For example, Paul Christiano and collaborators at OpenAI had the hopper,
[00:09:00.040 --> 00:09:03.720]   Mojoco hopper, a one-legged robot, learn to do backflips.
[00:09:03.720 --> 00:09:06.840]   Purely from feedback, I like this better than that.
[00:09:06.840 --> 00:09:08.600]   That's kind of equally good.
[00:09:08.600 --> 00:09:10.840]   And after a bunch of interactions,
[00:09:10.840 --> 00:09:14.200]   it figured out what it was the person was asking for, namely a backflip.
[00:09:14.200 --> 00:09:15.240]   And so I think the same thing--
[00:09:15.240 --> 00:09:18.520]   - Oh, it wasn't trying to do a backflip.
[00:09:18.520 --> 00:09:20.680]   It was just getting a score from the comparison score
[00:09:20.680 --> 00:09:21.880]   from the person based on--
[00:09:21.880 --> 00:09:25.240]   - The person having in mind, in their own mind,
[00:09:25.240 --> 00:09:27.320]   I wanted to do a backflip,
[00:09:27.320 --> 00:09:30.680]   but the robot didn't know what it was supposed to be doing.
[00:09:30.680 --> 00:09:32.680]   It just knew that sometimes the person said,
[00:09:32.680 --> 00:09:34.440]   "This is better, this is worse."
[00:09:34.440 --> 00:09:35.880]   And then the robot figured out
[00:09:35.880 --> 00:09:38.600]   what the person was actually after was a backflip.
[00:09:38.600 --> 00:09:40.840]   And I'd imagine the same would be true for things
[00:09:40.840 --> 00:09:43.000]   like more interactive robots
[00:09:43.000 --> 00:09:45.000]   that the robot would figure out over time,
[00:09:45.000 --> 00:09:47.960]   "Oh, this kind of thing apparently is appreciated more
[00:09:47.960 --> 00:09:49.160]   than this other kind of thing."
[00:09:49.160 --> 00:09:53.880]   - So when I first picked up Sutton's,
[00:09:53.880 --> 00:09:55.880]   Richard Sutton's reinforcement learning book,
[00:09:55.880 --> 00:09:59.880]   before sort of this deep learning,
[00:09:59.880 --> 00:10:03.160]   before the re-emergence of neural networks
[00:10:03.160 --> 00:10:04.920]   as a powerful mechanism for machine learning,
[00:10:04.920 --> 00:10:07.560]   RL seemed to me like magic.
[00:10:07.560 --> 00:10:10.200]   It was beautiful.
[00:10:10.200 --> 00:10:13.480]   So that seemed like what intelligence is,
[00:10:13.480 --> 00:10:14.840]   RL reinforcement learning.
[00:10:14.840 --> 00:10:20.200]   So how do you think we can possibly learn anything
[00:10:20.200 --> 00:10:22.920]   about the world when the reward for the actions
[00:10:22.920 --> 00:10:25.240]   is delayed, is so sparse?
[00:10:25.240 --> 00:10:29.240]   Why do you think RL works?
[00:10:29.240 --> 00:10:32.120]   Why do you think you can learn anything
[00:10:32.120 --> 00:10:34.440]   under such sparse rewards,
[00:10:34.440 --> 00:10:36.760]   whether it's regular reinforcement learning
[00:10:36.760 --> 00:10:38.360]   or deep reinforcement learning?
[00:10:38.360 --> 00:10:39.400]   What's your intuition?
[00:10:40.360 --> 00:10:43.240]   - The counterpart of that is why is RL,
[00:10:43.240 --> 00:10:47.000]   why does it need so many samples,
[00:10:47.000 --> 00:10:48.520]   so many experiences to learn from?
[00:10:48.520 --> 00:10:50.520]   Because really what's happening is
[00:10:50.520 --> 00:10:52.040]   when you have a sparse reward,
[00:10:52.040 --> 00:10:55.000]   you do something maybe for like, I don't know,
[00:10:55.000 --> 00:10:57.160]   you take a hundred actions and then you get a reward.
[00:10:57.160 --> 00:10:59.480]   And maybe you get like a score of three.
[00:10:59.480 --> 00:11:02.760]   And I'm like, okay, three, not sure what that means.
[00:11:02.760 --> 00:11:04.280]   You go again and now you get two.
[00:11:04.280 --> 00:11:06.920]   And now you know that that sequence of a hundred actions
[00:11:06.920 --> 00:11:08.120]   that you did the second time around
[00:11:08.120 --> 00:11:10.440]   somehow was worse than the sequence of a hundred actions
[00:11:10.440 --> 00:11:11.720]   you did the first time around.
[00:11:11.720 --> 00:11:13.560]   But that's tough to now know
[00:11:13.560 --> 00:11:15.000]   which one of those were better or worse.
[00:11:15.000 --> 00:11:16.760]   Some might've been good and bad in either one.
[00:11:16.760 --> 00:11:19.640]   And so that's why you need so many experiences.
[00:11:19.640 --> 00:11:21.160]   But once you have enough experiences,
[00:11:21.160 --> 00:11:23.240]   effectively RL is teasing that apart.
[00:11:23.240 --> 00:11:24.360]   It's starting to say, okay,
[00:11:24.360 --> 00:11:27.640]   what is consistently there when you get a higher reward?
[00:11:27.640 --> 00:11:29.800]   And what's consistently there when you get a lower reward?
[00:11:29.800 --> 00:11:31.880]   And then kind of the magic of,
[00:11:31.880 --> 00:11:34.040]   and sometimes the policy gradient update is to say,
[00:11:34.040 --> 00:11:36.840]   now let's update the neural network
[00:11:36.840 --> 00:11:39.000]   to make the actions that were kind of present
[00:11:39.000 --> 00:11:41.320]   when things are good, more likely,
[00:11:41.320 --> 00:11:42.920]   and make the actions that are present
[00:11:42.920 --> 00:11:44.680]   when things are not as good, less likely.
[00:11:44.680 --> 00:11:46.920]   - So that is the counterpoint,
[00:11:46.920 --> 00:11:49.480]   but it seems like you would need to run it
[00:11:49.480 --> 00:11:50.840]   a lot more than you do.
[00:11:50.840 --> 00:11:52.600]   Even though right now people could say
[00:11:52.600 --> 00:11:54.280]   that RL is very inefficient,
[00:11:54.280 --> 00:11:56.120]   but it seems to be way more efficient
[00:11:56.120 --> 00:11:58.280]   than one would imagine on paper.
[00:11:58.280 --> 00:12:01.800]   That the simple updates to the policy,
[00:12:01.800 --> 00:12:05.160]   the policy gradient that somehow you can learn exactly.
[00:12:05.160 --> 00:12:07.560]   You just said, what are the common actions
[00:12:07.560 --> 00:12:09.640]   that seem to produce some good results?
[00:12:09.640 --> 00:12:11.720]   That that somehow can learn anything.
[00:12:11.720 --> 00:12:14.440]   It seems counterintuitive at least.
[00:12:14.440 --> 00:12:16.760]   Is there some intuition behind it?
[00:12:16.760 --> 00:12:21.720]   - Yeah, so I think there's a few ways to think about this.
[00:12:21.720 --> 00:12:25.640]   The way I tend to think about it mostly originally,
[00:12:25.640 --> 00:12:28.920]   when we started working on deep reinforcement learning
[00:12:28.920 --> 00:12:32.680]   here at Berkeley, which was maybe 2011, 12, 13,
[00:12:32.680 --> 00:12:36.040]   around that time, John Shulman was a PhD student,
[00:12:36.040 --> 00:12:38.120]   initially kind of driving it forward here.
[00:12:38.120 --> 00:12:43.960]   And the way we thought about it at the time was,
[00:12:43.960 --> 00:12:46.920]   if you think about rectified linear units
[00:12:46.920 --> 00:12:49.000]   or kind of rectifier type neural networks,
[00:12:49.000 --> 00:12:50.840]   what do you get?
[00:12:50.840 --> 00:12:52.840]   You get something that's piecewise
[00:12:52.840 --> 00:12:54.200]   linear feedback control.
[00:12:54.200 --> 00:12:56.920]   And if you look at the literature,
[00:12:56.920 --> 00:12:59.160]   linear feedback control is extremely successful.
[00:12:59.160 --> 00:13:02.040]   It can solve many, many problems surprisingly well.
[00:13:02.600 --> 00:13:05.560]   I remember, for example, when we did helicopter flight,
[00:13:05.560 --> 00:13:07.240]   if you're in a stationary flight regime,
[00:13:07.240 --> 00:13:10.360]   not a non-stationary, but a stationary flight regime
[00:13:10.360 --> 00:13:12.360]   like hover, you can use linear feedback control
[00:13:12.360 --> 00:13:15.400]   to stabilize a helicopter, a very complex dynamical system,
[00:13:15.400 --> 00:13:18.280]   but the controller is relatively simple.
[00:13:18.280 --> 00:13:19.880]   And so I think that's a big part of it is that
[00:13:19.880 --> 00:13:22.200]   if you do feedback control,
[00:13:22.200 --> 00:13:24.120]   even though the system you control can be very,
[00:13:24.120 --> 00:13:28.600]   very complex, often relatively simple control architectures
[00:13:28.600 --> 00:13:29.720]   can already do a lot,
[00:13:30.360 --> 00:13:32.440]   but then also just linear is not good enough.
[00:13:32.440 --> 00:13:35.000]   And so one way you can think of these neural networks
[00:13:35.000 --> 00:13:36.920]   is that in some sense, they tile the space,
[00:13:36.920 --> 00:13:39.320]   which people were already trying to do more by hand
[00:13:39.320 --> 00:13:40.920]   or with finite state machines.
[00:13:40.920 --> 00:13:42.360]   Say this linear controller here,
[00:13:42.360 --> 00:13:43.640]   this linear controller here.
[00:13:43.640 --> 00:13:45.480]   Neural network learns to tile the space
[00:13:45.480 --> 00:13:46.440]   and say linear controller here,
[00:13:46.440 --> 00:13:48.120]   another linear controller here,
[00:13:48.120 --> 00:13:49.400]   but it's more subtle than that.
[00:13:49.400 --> 00:13:51.880]   And so it's benefiting from this linear control aspect,
[00:13:51.880 --> 00:13:53.480]   it's benefiting from the tiling,
[00:13:53.480 --> 00:13:56.680]   but it's somehow tiling it one dimension at a time.
[00:13:56.680 --> 00:13:59.320]   Because if let's say you have a two layer network,
[00:13:59.320 --> 00:14:00.520]   if in that hidden layer,
[00:14:00.520 --> 00:14:04.840]   you make a transition from active to inactive
[00:14:04.840 --> 00:14:05.720]   or the other way around,
[00:14:05.720 --> 00:14:08.280]   that is essentially one axis,
[00:14:08.280 --> 00:14:09.400]   but not axis aligned,
[00:14:09.400 --> 00:14:12.200]   but one direction that you change.
[00:14:12.200 --> 00:14:15.160]   And so you have this kind of very gradual tiling of the space
[00:14:15.160 --> 00:14:16.680]   where you have a lot of sharing
[00:14:16.680 --> 00:14:19.400]   between the linear controllers that tile the space.
[00:14:19.400 --> 00:14:22.120]   And that was always my intuition as to why to expect
[00:14:22.120 --> 00:14:24.680]   that this might work pretty well.
[00:14:24.680 --> 00:14:25.960]   It's essentially leveraging the fact
[00:14:25.960 --> 00:14:27.720]   that linear feedback control is so good,
[00:14:28.360 --> 00:14:29.720]   but of course not enough.
[00:14:29.720 --> 00:14:31.640]   And this is a gradual tiling of the space
[00:14:31.640 --> 00:14:33.400]   with linear feedback controls
[00:14:33.400 --> 00:14:36.040]   that share a lot of expertise across them.
[00:14:36.040 --> 00:14:38.840]   - So that's really nice intuition.
[00:14:38.840 --> 00:14:40.840]   But do you think that scales
[00:14:40.840 --> 00:14:42.440]   to the more and more general problems
[00:14:42.440 --> 00:14:47.000]   of when you start going up the number of dimensions
[00:14:47.000 --> 00:14:51.480]   when you start going down
[00:14:51.480 --> 00:14:55.240]   in terms of how often you get a clean reward signal?
[00:14:55.240 --> 00:14:57.240]   Does that intuition carry forward
[00:14:57.240 --> 00:14:59.560]   to those crazier, weirder worlds
[00:14:59.560 --> 00:15:01.160]   that we think of as the real world?
[00:15:01.160 --> 00:15:07.880]   - So I think where things get really tricky
[00:15:07.880 --> 00:15:08.680]   in the real world
[00:15:08.680 --> 00:15:10.840]   compared to the things we've looked at so far
[00:15:10.840 --> 00:15:13.080]   with great success in reinforcement learning
[00:15:13.080 --> 00:15:17.160]   is the time scales,
[00:15:17.160 --> 00:15:18.920]   which takes us to an extreme.
[00:15:18.920 --> 00:15:21.640]   So when you think about the real world,
[00:15:21.640 --> 00:15:23.240]   I mean, I don't know,
[00:15:23.240 --> 00:15:26.760]   maybe some student decided to do a PhD here, right?
[00:15:26.760 --> 00:15:28.520]   Okay, that's a decision.
[00:15:28.520 --> 00:15:29.960]   That's a very high level decision.
[00:15:29.960 --> 00:15:32.440]   But if you think about their lives,
[00:15:32.440 --> 00:15:33.960]   I mean, any person's life,
[00:15:33.960 --> 00:15:37.240]   it's a sequence of muscle fiber contractions
[00:15:37.240 --> 00:15:38.200]   and relaxations,
[00:15:38.200 --> 00:15:40.120]   and that's how you interact with the world.
[00:15:40.120 --> 00:15:42.520]   And that's a very high frequency control thing,
[00:15:42.520 --> 00:15:44.440]   but it's ultimately what you do
[00:15:44.440 --> 00:15:45.560]   and how you affect the world.
[00:15:45.560 --> 00:15:48.120]   Until I guess we have brain readings,
[00:15:48.120 --> 00:15:49.640]   and you can maybe do it slightly differently,
[00:15:49.640 --> 00:15:51.960]   but typically that's how you affect the world.
[00:15:51.960 --> 00:15:56.200]   And the decision of doing a PhD is like so abstract
[00:15:56.200 --> 00:15:59.160]   relative to what you're actually doing in the world.
[00:15:59.160 --> 00:16:01.000]   And I think that's where credit assignment
[00:16:01.000 --> 00:16:04.680]   becomes just completely beyond
[00:16:04.680 --> 00:16:06.600]   what any current RL algorithm can do.
[00:16:06.600 --> 00:16:08.840]   And we need hierarchical reasoning
[00:16:08.840 --> 00:16:11.640]   at a level that is just not available at all yet.
[00:16:11.640 --> 00:16:14.760]   - Where do you think we can pick up hierarchical reasoning?
[00:16:14.760 --> 00:16:15.880]   By which mechanisms?
[00:16:15.880 --> 00:16:18.520]   - Yeah, so maybe let me highlight
[00:16:18.520 --> 00:16:20.600]   what I think the limitations are
[00:16:20.600 --> 00:16:24.920]   of what already was done 20, 30 years ago.
[00:16:25.880 --> 00:16:27.560]   In fact, you'll find reasoning systems
[00:16:27.560 --> 00:16:30.760]   that reason over relatively long horizons,
[00:16:30.760 --> 00:16:32.680]   but the problem is that they were not grounded
[00:16:32.680 --> 00:16:33.480]   in the real world.
[00:16:33.480 --> 00:16:37.000]   So people would have to hand design
[00:16:37.000 --> 00:16:40.520]   some kind of logical,
[00:16:40.520 --> 00:16:43.800]   dynamical descriptions of the world,
[00:16:43.800 --> 00:16:46.200]   and that didn't tie into perception.
[00:16:46.200 --> 00:16:49.080]   And so it didn't tie into real objects and so forth.
[00:16:49.080 --> 00:16:51.000]   And so that was a big gap.
[00:16:51.000 --> 00:16:52.200]   Now with deep learning,
[00:16:52.200 --> 00:16:54.040]   we start having the ability to
[00:16:55.400 --> 00:16:59.480]   really see with sensors, process that,
[00:16:59.480 --> 00:17:01.320]   and understand what's in the world.
[00:17:01.320 --> 00:17:02.680]   And so it's a good time to try
[00:17:02.680 --> 00:17:03.880]   to bring these things together.
[00:17:03.880 --> 00:17:06.280]   I see a few ways of getting there.
[00:17:06.280 --> 00:17:08.040]   One way to get there would be to say,
[00:17:08.040 --> 00:17:09.960]   deep learning can get bolted on somehow
[00:17:09.960 --> 00:17:12.200]   to some of these more traditional approaches.
[00:17:12.200 --> 00:17:13.960]   Now, bolted on would probably mean
[00:17:13.960 --> 00:17:16.200]   you need to do some kind of end-to-end training,
[00:17:16.200 --> 00:17:18.440]   where you say, "My deep learning processing
[00:17:18.440 --> 00:17:20.680]   somehow leads to a representation
[00:17:20.680 --> 00:17:23.720]   that in turn uses some kind of
[00:17:23.720 --> 00:17:27.320]   traditional underlying dynamical systems
[00:17:27.320 --> 00:17:29.720]   that can be used for planning."
[00:17:29.720 --> 00:17:31.320]   And that's, for example, the direction
[00:17:31.320 --> 00:17:33.320]   Aviv Tamar and Thanar Kuretach here
[00:17:33.320 --> 00:17:35.000]   have been pushing with causal infoGAN
[00:17:35.000 --> 00:17:36.200]   and of course other people too.
[00:17:36.200 --> 00:17:38.120]   That's one way.
[00:17:38.120 --> 00:17:41.000]   Can we somehow force it into the form factor
[00:17:41.000 --> 00:17:43.560]   that is amenable to reasoning?
[00:17:43.560 --> 00:17:46.440]   Another direction we've been thinking about
[00:17:46.440 --> 00:17:47.640]   for a long time
[00:17:47.640 --> 00:17:50.120]   and didn't make any progress on
[00:17:50.120 --> 00:17:52.600]   was more information-theoretic approaches.
[00:17:53.560 --> 00:17:55.080]   So the idea there was that
[00:17:55.080 --> 00:17:57.960]   what it means to take high-level action
[00:17:57.960 --> 00:18:02.440]   is to choose a latent variable now
[00:18:02.440 --> 00:18:03.560]   that tells you a lot about
[00:18:03.560 --> 00:18:05.240]   what's going to be the case in the future.
[00:18:05.240 --> 00:18:06.280]   Because that's what it means
[00:18:06.280 --> 00:18:08.600]   to take a high-level action.
[00:18:08.600 --> 00:18:12.920]   I say, "Okay, I decide I'm going to navigate
[00:18:12.920 --> 00:18:15.400]   to the gas station because I need to get gas for my car.
[00:18:15.400 --> 00:18:17.720]   Well, that'll now take five minutes to get there."
[00:18:17.720 --> 00:18:19.160]   But the fact that I get there,
[00:18:19.160 --> 00:18:20.200]   I could already tell that
[00:18:20.200 --> 00:18:23.160]   from the high-level action I took much earlier.
[00:18:23.720 --> 00:18:27.560]   That, we had a very hard time getting success with.
[00:18:27.560 --> 00:18:30.520]   Not saying it's a dead end necessarily,
[00:18:30.520 --> 00:18:32.280]   but we had a lot of trouble getting that to work.
[00:18:32.280 --> 00:18:34.520]   And then we started revisiting the notion
[00:18:34.520 --> 00:18:36.120]   of what are we really trying to achieve?
[00:18:36.120 --> 00:18:38.920]   What we're trying to achieve
[00:18:38.920 --> 00:18:40.840]   is not necessarily hierarchy per se,
[00:18:40.840 --> 00:18:42.840]   but you can think about what does hierarchy give us?
[00:18:42.840 --> 00:18:45.720]   What we hope it would give us
[00:18:45.720 --> 00:18:46.840]   is better credit assignment.
[00:18:46.840 --> 00:18:51.720]   What is better credit assignment is giving us?
[00:18:51.720 --> 00:18:53.960]   It gives us faster learning.
[00:18:53.960 --> 00:18:59.640]   And so, faster learning is ultimately maybe what we're after.
[00:18:59.640 --> 00:19:01.640]   And so, that's what we ended up with,
[00:19:01.640 --> 00:19:04.760]   the RL squared paper on learning to reinforcement learn,
[00:19:04.760 --> 00:19:07.480]   which at a time Rocky Duan led.
[00:19:07.480 --> 00:19:10.920]   And that's exactly the meta-learning approach
[00:19:10.920 --> 00:19:13.400]   where you say, "Okay, we don't know how to design hierarchy.
[00:19:13.400 --> 00:19:15.560]   We know what we want to get from it.
[00:19:15.560 --> 00:19:17.160]   Let's just enter and optimize
[00:19:17.160 --> 00:19:18.760]   for what we want to get from it
[00:19:18.760 --> 00:19:19.960]   and see if it might emerge."
[00:19:19.960 --> 00:19:21.000]   And we saw things emerge.
[00:19:21.000 --> 00:19:24.760]   The maze navigation had consistent motion down hallways,
[00:19:24.760 --> 00:19:26.920]   which is what you want.
[00:19:26.920 --> 00:19:28.120]   A hierarchical control should say,
[00:19:28.120 --> 00:19:29.560]   "I want to go down this hallway."
[00:19:29.560 --> 00:19:31.480]   And then when there is an option to take a turn,
[00:19:31.480 --> 00:19:33.640]   I can decide whether to take a turn or not and repeat.
[00:19:33.640 --> 00:19:35.240]   Even had the notion of,
[00:19:35.240 --> 00:19:37.080]   "Where have you been before or not?"
[00:19:37.080 --> 00:19:39.000]   Do not revisit places you've been before.
[00:19:39.000 --> 00:19:41.880]   It still didn't scale yet
[00:19:41.880 --> 00:19:44.760]   to the real world kind of scenarios
[00:19:44.760 --> 00:19:45.800]   I think you had in mind,
[00:19:45.800 --> 00:19:47.000]   but it was some sign of life
[00:19:47.000 --> 00:19:50.040]   that maybe you can meta-learn these hierarchical concepts.
[00:19:51.000 --> 00:19:56.040]   I mean, it seems like through these meta-learning concepts,
[00:19:56.040 --> 00:19:59.640]   we get at what I think is one of the hardest
[00:19:59.640 --> 00:20:02.200]   and most important problems of AI,
[00:20:02.200 --> 00:20:03.320]   which is transfer learning.
[00:20:03.320 --> 00:20:05.080]   So it's generalization.
[00:20:05.080 --> 00:20:08.280]   How far along this journey
[00:20:08.280 --> 00:20:10.360]   towards building general systems are we?
[00:20:10.360 --> 00:20:12.760]   Being able to do transfer learning well.
[00:20:12.760 --> 00:20:16.680]   So there's some signs that you can generalize a little bit,
[00:20:16.680 --> 00:20:19.400]   but do you think we're on the right path
[00:20:19.400 --> 00:20:23.560]   or totally different breakthroughs are needed
[00:20:23.560 --> 00:20:26.680]   to be able to transfer knowledge
[00:20:26.680 --> 00:20:28.680]   between different learned models?
[00:20:28.680 --> 00:20:33.000]   - Yeah, I'm pretty torn on this.
[00:20:33.000 --> 00:20:35.160]   I think there are some very impressive--
[00:20:35.160 --> 00:20:35.960]   - The present of the day.
[00:20:35.960 --> 00:20:40.120]   - Well, there's just some very impressive results already.
[00:20:40.120 --> 00:20:40.280]   Right?
[00:20:40.280 --> 00:20:43.000]   I mean, I would say when,
[00:20:43.000 --> 00:20:47.080]   even with the initial kind of big breakthrough in 2012
[00:20:47.080 --> 00:20:48.520]   with AlexNet, right?
[00:20:48.520 --> 00:20:50.840]   The initial thing is, okay, great.
[00:20:50.840 --> 00:20:54.680]   This does better on ImageNet, hence image recognition.
[00:20:54.680 --> 00:20:57.560]   But then immediately thereafter,
[00:20:57.560 --> 00:20:59.560]   there was of course the notion that,
[00:20:59.560 --> 00:21:03.080]   wow, what was learned on ImageNet,
[00:21:03.080 --> 00:21:04.840]   and you now want to solve a new task,
[00:21:04.840 --> 00:21:07.400]   you can fine tune AlexNet for new tasks.
[00:21:07.400 --> 00:21:11.880]   And that was often found to be the even bigger deal
[00:21:11.880 --> 00:21:14.120]   that you learn something that was reusable,
[00:21:14.120 --> 00:21:15.880]   which was not often the case before.
[00:21:15.880 --> 00:21:16.680]   Usually machine learning,
[00:21:16.680 --> 00:21:18.920]   you learn something for one scenario, and that was it.
[00:21:18.920 --> 00:21:20.120]   - Yeah, and that's really exciting.
[00:21:20.120 --> 00:21:22.200]   I mean, that's a huge application.
[00:21:22.200 --> 00:21:23.480]   That's probably the biggest success
[00:21:23.480 --> 00:21:25.160]   of transfer learning to date,
[00:21:25.160 --> 00:21:27.320]   in terms of scope and impact.
[00:21:27.320 --> 00:21:29.000]   - That was a huge breakthrough.
[00:21:29.000 --> 00:21:33.960]   And then recently, I feel like similar kind of,
[00:21:33.960 --> 00:21:35.960]   by scaling things up,
[00:21:35.960 --> 00:21:37.880]   it seems like this has been expanded upon.
[00:21:37.880 --> 00:21:39.800]   Like people training even bigger networks,
[00:21:39.800 --> 00:21:41.320]   they might transfer even better.
[00:21:41.320 --> 00:21:43.000]   If you looked at, for example,
[00:21:43.000 --> 00:21:45.240]   some of the OpenAI results on language models
[00:21:45.240 --> 00:21:48.120]   and some of the recent Google results on language models,
[00:21:48.120 --> 00:21:52.200]   they are learned for just prediction,
[00:21:52.200 --> 00:21:56.760]   and then they get reused for other tasks.
[00:21:56.760 --> 00:21:58.440]   And so I think there is something there
[00:21:58.440 --> 00:22:00.360]   where somehow if you train a big enough model
[00:22:00.360 --> 00:22:01.960]   on enough things,
[00:22:01.960 --> 00:22:04.920]   it seems to transfer some deep mind results
[00:22:04.920 --> 00:22:05.880]   that I thought were very impressive,
[00:22:05.880 --> 00:22:07.080]   the Unreal results,
[00:22:07.080 --> 00:22:11.000]   where it was learned to navigate mazes
[00:22:11.000 --> 00:22:13.640]   in ways where it wasn't just doing reinforcement learning,
[00:22:13.640 --> 00:22:14.920]   but it had other objectives,
[00:22:14.920 --> 00:22:16.680]   was optimizing for.
[00:22:16.680 --> 00:22:19.080]   So I think there's a lot of interesting results already.
[00:22:19.080 --> 00:22:24.040]   I think maybe where it's hard to wrap my head around
[00:22:24.040 --> 00:22:26.200]   is to which extent,
[00:22:26.200 --> 00:22:28.440]   or when do we call something generalization,
[00:22:28.440 --> 00:22:31.240]   or the levels of generalization
[00:22:31.240 --> 00:22:33.560]   involved in these different tasks.
[00:22:33.560 --> 00:22:34.060]   - Yeah.
[00:22:34.060 --> 00:22:38.360]   You draw this, by the way, just to frame things.
[00:22:38.360 --> 00:22:40.600]   I've heard you say somewhere,
[00:22:40.600 --> 00:22:42.600]   it's the difference between learning to master
[00:22:42.600 --> 00:22:44.360]   versus learning to generalize.
[00:22:44.360 --> 00:22:47.720]   It's a nice line to think about,
[00:22:47.720 --> 00:22:50.120]   and I guess you're saying that's a gray area
[00:22:50.120 --> 00:22:53.560]   of what learning to master and learning to generalize,
[00:22:53.560 --> 00:22:54.600]   where one starts and one ends.
[00:22:54.600 --> 00:22:55.880]   - I think I might've heard this.
[00:22:55.880 --> 00:22:57.720]   I might've heard it somewhere else,
[00:22:57.720 --> 00:23:00.360]   and I think it might've been one of your interviews,
[00:23:00.360 --> 00:23:02.040]   maybe the one with Yoshua Benjamin,
[00:23:02.040 --> 00:23:03.480]   I'm not 100% sure,
[00:23:03.480 --> 00:23:05.080]   but I liked the example,
[00:23:05.080 --> 00:23:08.280]   and I'm not sure who it was,
[00:23:08.280 --> 00:23:10.520]   but the example was essentially,
[00:23:10.520 --> 00:23:13.160]   if you use current deep learning techniques,
[00:23:13.160 --> 00:23:14.840]   what we're doing to predict,
[00:23:14.840 --> 00:23:20.440]   let's say, the relative motion of our planets,
[00:23:20.440 --> 00:23:21.560]   it would do pretty well,
[00:23:21.560 --> 00:23:26.280]   but then now if a massive new mass
[00:23:26.280 --> 00:23:27.640]   enters our solar system,
[00:23:27.640 --> 00:23:31.960]   it would probably not predict what would happen, right?
[00:23:31.960 --> 00:23:33.400]   And that's a different kind of generalization.
[00:23:33.400 --> 00:23:36.520]   That's a generalization that relies on the ultimate,
[00:23:36.520 --> 00:23:38.440]   simplest, simplest explanation
[00:23:38.440 --> 00:23:40.040]   that we have available today
[00:23:40.040 --> 00:23:41.400]   to explain the motion of planets,
[00:23:41.400 --> 00:23:43.560]   whereas just pattern recognition could predict
[00:23:43.560 --> 00:23:47.160]   our current solar system motion pretty well, no problem.
[00:23:47.160 --> 00:23:48.680]   And so I think that's an example
[00:23:48.680 --> 00:23:52.200]   of a kind of generalization that is a little different
[00:23:52.200 --> 00:23:53.960]   from what we've achieved so far.
[00:23:53.960 --> 00:23:59.560]   And it's not clear if just regularizing more
[00:23:59.560 --> 00:24:01.320]   and forcing it to come up with a simpler,
[00:24:01.320 --> 00:24:02.440]   simpler, simpler explanation,
[00:24:02.440 --> 00:24:03.640]   say, look, this is not simple,
[00:24:03.640 --> 00:24:05.400]   but that's what physics researchers do, right?
[00:24:05.400 --> 00:24:08.040]   They say, can I make this even simpler?
[00:24:08.040 --> 00:24:09.240]   How simple can I get this?
[00:24:09.240 --> 00:24:10.280]   What's the simplest equation
[00:24:10.280 --> 00:24:12.200]   that can explain everything, right?
[00:24:12.200 --> 00:24:15.320]   The master equation for the entire dynamics of the universe.
[00:24:15.320 --> 00:24:17.480]   We haven't really pushed that direction as hard
[00:24:17.480 --> 00:24:19.160]   in deep learning, I would say.
[00:24:19.160 --> 00:24:21.880]   Not sure if it should be pushed,
[00:24:21.880 --> 00:24:24.360]   but it seems a kind of generalization you get from that
[00:24:24.360 --> 00:24:27.240]   that you don't get in our current methods so far.
[00:24:27.240 --> 00:24:29.960]   - So I just talked to Vladimir Vapnik, for example,
[00:24:29.960 --> 00:24:34.040]   who's a statistician, statistical learning,
[00:24:34.040 --> 00:24:35.720]   and he kind of dreams of creating,
[00:24:36.920 --> 00:24:40.920]   the E equals MC squared for learning, right?
[00:24:40.920 --> 00:24:42.280]   The general theory of learning.
[00:24:42.280 --> 00:24:47.640]   Do you think that's a fruitless pursuit in near term,
[00:24:47.640 --> 00:24:50.600]   within the next several decades?
[00:24:50.600 --> 00:24:53.480]   - I think that's a really interesting pursuit
[00:24:53.480 --> 00:24:55.560]   and in the following sense,
[00:24:55.560 --> 00:24:57.960]   in that there is a lot of evidence
[00:24:57.960 --> 00:25:02.680]   that the brain is pretty modular.
[00:25:02.680 --> 00:25:05.400]   And so I wouldn't maybe think of it as the theory,
[00:25:05.400 --> 00:25:06.920]   maybe the underlying theory,
[00:25:06.920 --> 00:25:09.000]   but more kind of the principle
[00:25:09.000 --> 00:25:12.200]   where there have been findings
[00:25:12.200 --> 00:25:15.000]   where people who are blind
[00:25:15.000 --> 00:25:16.440]   will use the part of the brain
[00:25:16.440 --> 00:25:20.200]   usually used for vision for other functions.
[00:25:20.200 --> 00:25:24.520]   And even after some kind of,
[00:25:24.520 --> 00:25:26.200]   if people get rewired in some way,
[00:25:26.200 --> 00:25:28.520]   they might be able to reuse parts of their brain
[00:25:28.520 --> 00:25:29.320]   for other functions.
[00:25:29.320 --> 00:25:34.200]   And so what that suggests is some kind of modularity
[00:25:35.000 --> 00:25:39.080]   and I think it is a pretty natural thing to strive for,
[00:25:39.080 --> 00:25:41.560]   to see, can we find that modularity?
[00:25:41.560 --> 00:25:43.000]   Can we find this thing?
[00:25:43.000 --> 00:25:44.840]   Of course, it's not every part of the brain
[00:25:44.840 --> 00:25:45.800]   is not exactly the same.
[00:25:45.800 --> 00:25:48.360]   Not everything can be rewired arbitrarily.
[00:25:48.360 --> 00:25:50.040]   But if you think of things like the neocortex,
[00:25:50.040 --> 00:25:51.560]   which is a pretty big part of the brain,
[00:25:51.560 --> 00:25:55.720]   that seems fairly modular from what the findings so far.
[00:25:55.720 --> 00:25:59.080]   Can you design something equally modular?
[00:25:59.080 --> 00:26:00.280]   And if you can just grow it,
[00:26:00.280 --> 00:26:02.280]   it becomes more capable probably.
[00:26:02.280 --> 00:26:04.760]   I think that would be the kind of interesting
[00:26:04.760 --> 00:26:06.280]   underlying principle to shoot for
[00:26:06.280 --> 00:26:08.360]   that is not unrealistic.
[00:26:08.360 --> 00:26:12.600]   - Do you think you prefer math
[00:26:12.600 --> 00:26:15.080]   or empirical trial and error
[00:26:15.080 --> 00:26:16.680]   for the discovery of the essence
[00:26:16.680 --> 00:26:18.840]   of what it means to do something intelligent?
[00:26:18.840 --> 00:26:21.960]   So reinforcement learning embodies both groups, right?
[00:26:21.960 --> 00:26:26.280]   To prove that something converges, prove the bounds.
[00:26:26.280 --> 00:26:27.720]   And then at the same time,
[00:26:27.720 --> 00:26:29.160]   a lot of those successes are,
[00:26:29.160 --> 00:26:31.400]   well, let's try this and see if it works.
[00:26:31.400 --> 00:26:33.240]   So which do you gravitate towards?
[00:26:33.240 --> 00:26:35.720]   How do you think of those two parts of your brain?
[00:26:35.720 --> 00:26:42.360]   - So maybe I would prefer
[00:26:42.360 --> 00:26:45.480]   we could make the progress with mathematics.
[00:26:45.480 --> 00:26:46.920]   And the reason maybe I would prefer that
[00:26:46.920 --> 00:26:49.000]   is because often if you have something
[00:26:49.000 --> 00:26:51.880]   you can mathematically formalize,
[00:26:51.880 --> 00:26:55.560]   you can leapfrog a lot of experimentation.
[00:26:55.560 --> 00:26:58.120]   And experimentation takes a long time to get through.
[00:26:58.120 --> 00:27:00.280]   And a lot of trial and error,
[00:27:01.160 --> 00:27:03.160]   kind of reinforcement learning your research process,
[00:27:03.160 --> 00:27:05.480]   but you need to do a lot of trial and error
[00:27:05.480 --> 00:27:06.520]   before you get to a success.
[00:27:06.520 --> 00:27:08.360]   So if you can leapfrog that, to my mind,
[00:27:08.360 --> 00:27:09.720]   that's what the math is about.
[00:27:09.720 --> 00:27:13.080]   And hopefully once you do a bunch of experiments,
[00:27:13.080 --> 00:27:14.360]   you start seeing a pattern.
[00:27:14.360 --> 00:27:17.480]   You can do some derivations that leapfrog some experiments.
[00:27:17.480 --> 00:27:18.840]   But I agree with you.
[00:27:18.840 --> 00:27:20.680]   I mean, in practice, a lot of the progress
[00:27:20.680 --> 00:27:23.080]   has been such that we have not been able to find
[00:27:23.080 --> 00:27:25.000]   the math that allows it to leapfrog ahead.
[00:27:25.000 --> 00:27:27.960]   And we are kind of making gradual progress
[00:27:27.960 --> 00:27:29.320]   one step at a time.
[00:27:29.320 --> 00:27:31.080]   A new experiment here, a new experiment there
[00:27:31.080 --> 00:27:34.280]   that gives us new insights and gradually building up,
[00:27:34.280 --> 00:27:36.440]   but not getting to something yet where we're just,
[00:27:36.440 --> 00:27:38.920]   "Okay, here's an equation that now explains how,"
[00:27:38.920 --> 00:27:40.440]   you know, that would be,
[00:27:40.440 --> 00:27:42.440]   have been two years of experimentation to get there,
[00:27:42.440 --> 00:27:44.760]   but this tells us what the result's going to be.
[00:27:44.760 --> 00:27:47.080]   Unfortunately, not so much yet.
[00:27:47.080 --> 00:27:49.320]   - Not so much yet, but your hope is there.
[00:27:49.320 --> 00:27:53.560]   In trying to teach robots or systems
[00:27:53.560 --> 00:27:57.960]   to do everyday tasks or even in simulation,
[00:27:58.840 --> 00:28:01.400]   what do you think you're more excited about?
[00:28:01.400 --> 00:28:04.680]   Imitation learning or self-play?
[00:28:04.680 --> 00:28:07.400]   So letting robots learn from humans
[00:28:07.400 --> 00:28:11.240]   or letting robots plan their own,
[00:28:11.240 --> 00:28:13.640]   try to figure out in their own way
[00:28:13.640 --> 00:28:18.120]   and eventually play, eventually interact with humans
[00:28:18.120 --> 00:28:19.960]   or solve whatever problem is.
[00:28:19.960 --> 00:28:21.720]   What's the more exciting to you?
[00:28:21.720 --> 00:28:24.280]   What's more promising, you think, as a research direction?
[00:28:24.280 --> 00:28:31.400]   - So when we look at self-play,
[00:28:31.400 --> 00:28:34.200]   what's so beautiful about it is,
[00:28:34.200 --> 00:28:36.280]   goes back to kind of the challenges
[00:28:36.280 --> 00:28:37.160]   in reinforcement learning.
[00:28:37.160 --> 00:28:38.360]   So the challenge of reinforcement learning
[00:28:38.360 --> 00:28:39.160]   is getting signal.
[00:28:39.160 --> 00:28:41.880]   And if you don't never succeed,
[00:28:41.880 --> 00:28:43.160]   you don't get any signal.
[00:28:43.160 --> 00:28:46.600]   In self-play, you're on both sides.
[00:28:46.600 --> 00:28:47.800]   So one of you succeeds.
[00:28:47.800 --> 00:28:49.800]   And the beauty is also one of you fails.
[00:28:49.800 --> 00:28:51.000]   And so you see the contrast.
[00:28:51.000 --> 00:28:52.680]   You see the one version of me
[00:28:52.680 --> 00:28:53.880]   that did better than the other version.
[00:28:53.880 --> 00:28:55.880]   And so every time you play yourself,
[00:28:55.880 --> 00:28:57.080]   you get signal.
[00:28:57.080 --> 00:28:59.880]   And so whenever you can turn something into self-play,
[00:28:59.880 --> 00:29:01.960]   you're in a beautiful situation
[00:29:01.960 --> 00:29:04.680]   where you can naturally learn much more quickly
[00:29:04.680 --> 00:29:07.800]   than in most other reinforced learning environments.
[00:29:07.800 --> 00:29:11.720]   So I think if somehow we can turn more
[00:29:11.720 --> 00:29:13.640]   reinforcement learning problems
[00:29:13.640 --> 00:29:15.400]   into self-play formulations,
[00:29:15.400 --> 00:29:17.080]   that would go really, really far.
[00:29:17.080 --> 00:29:20.520]   So far, self-play has been largely around games
[00:29:20.520 --> 00:29:22.600]   where there is natural opponents.
[00:29:22.600 --> 00:29:24.520]   But if we could do self-play for other things,
[00:29:24.520 --> 00:29:25.400]   and let's say, I don't know,
[00:29:25.400 --> 00:29:26.760]   a robot learns to build a house.
[00:29:26.760 --> 00:29:28.280]   I mean, that's a pretty advanced thing
[00:29:28.280 --> 00:29:29.400]   to try to do for a robot,
[00:29:29.400 --> 00:29:31.720]   but maybe it tries to build a hut or something.
[00:29:31.720 --> 00:29:34.040]   If that can be done through self-play,
[00:29:34.040 --> 00:29:35.240]   it would learn a lot more quickly
[00:29:35.240 --> 00:29:36.360]   if somebody can figure that out.
[00:29:36.360 --> 00:29:37.880]   And I think that would be something
[00:29:37.880 --> 00:29:39.320]   where it goes closer
[00:29:39.320 --> 00:29:41.400]   to kind of the mathematical leapfrogging
[00:29:41.400 --> 00:29:43.720]   where somebody figures out a formalism to say,
[00:29:43.720 --> 00:29:47.000]   "Okay, any RL problem by playing this and this idea,
[00:29:47.000 --> 00:29:48.520]   "you can turn it into a self-play problem
[00:29:48.520 --> 00:29:50.440]   "where you get signal a lot more easily."
[00:29:52.440 --> 00:29:54.200]   Reality is, many problems
[00:29:54.200 --> 00:29:55.880]   we don't know how to turn into self-play.
[00:29:55.880 --> 00:29:58.760]   And so either we need to provide detailed reward,
[00:29:58.760 --> 00:30:00.840]   that doesn't just reward for achieving a goal,
[00:30:00.840 --> 00:30:02.680]   but rewards for making progress,
[00:30:02.680 --> 00:30:04.440]   and that becomes time-consuming.
[00:30:04.440 --> 00:30:05.720]   And once you're starting to do that,
[00:30:05.720 --> 00:30:07.000]   let's say you want a robot to do something,
[00:30:07.000 --> 00:30:09.000]   you need to give all this detailed reward,
[00:30:09.000 --> 00:30:10.600]   well, why not just give a demonstration?
[00:30:10.600 --> 00:30:11.160]   - Right.
[00:30:11.160 --> 00:30:13.000]   - Because why not just show the robot?
[00:30:13.000 --> 00:30:16.360]   And now the question is, how do you show the robot?
[00:30:16.360 --> 00:30:18.440]   One way to show is to tally operate the robot,
[00:30:18.440 --> 00:30:20.280]   and then the robot really experiences things.
[00:30:20.840 --> 00:30:22.840]   And that's nice because that's really high signal
[00:30:22.840 --> 00:30:24.840]   to noise ratio data, and we've done a lot of that.
[00:30:24.840 --> 00:30:26.760]   And you teach your robot skills,
[00:30:26.760 --> 00:30:29.640]   in just 10 minutes, you can teach a robot a new basic skill,
[00:30:29.640 --> 00:30:32.120]   like, "Okay, pick up the bottle, place it somewhere else."
[00:30:32.120 --> 00:30:34.120]   That's a skill, no matter where the bottle starts,
[00:30:34.120 --> 00:30:36.040]   maybe it always goes onto a target or something.
[00:30:36.040 --> 00:30:39.080]   That's fairly easy to teach your robot with tally-up.
[00:30:39.080 --> 00:30:42.120]   Now, what's even more interesting,
[00:30:42.120 --> 00:30:43.160]   if you can now teach your robot
[00:30:43.160 --> 00:30:44.920]   through third-person learning,
[00:30:44.920 --> 00:30:46.840]   where the robot watches you do something,
[00:30:46.840 --> 00:30:49.960]   and doesn't experience it, but just watches it and says,
[00:30:49.960 --> 00:30:51.400]   "Okay, well, if you're showing me that,
[00:30:51.400 --> 00:30:53.640]   that means I should be doing this,
[00:30:53.640 --> 00:30:55.160]   and I'm not gonna be using your hand,
[00:30:55.160 --> 00:30:56.840]   because I don't get to control your hand,
[00:30:56.840 --> 00:30:59.320]   but I'm gonna use my hand, I do that mapping."
[00:30:59.320 --> 00:31:01.960]   And so that's where I think one of the big breakthroughs
[00:31:01.960 --> 00:31:03.160]   has happened this year.
[00:31:03.160 --> 00:31:05.160]   This was led by Chelsea Finn here.
[00:31:05.160 --> 00:31:08.040]   It's almost like learning a machine translation
[00:31:08.040 --> 00:31:11.080]   for demonstrations, where you have a human demonstration,
[00:31:11.080 --> 00:31:13.080]   and the robot learns to translate it into
[00:31:13.080 --> 00:31:15.640]   what it means for the robot to do it.
[00:31:15.640 --> 00:31:17.400]   And that was a meta-learning formulation,
[00:31:17.400 --> 00:31:19.800]   learn from one to get the other.
[00:31:19.800 --> 00:31:22.840]   And that, I think, opens up a lot of opportunities
[00:31:22.840 --> 00:31:24.280]   to learn a lot more quickly.
[00:31:24.280 --> 00:31:26.360]   - So my focus is on autonomous vehicles.
[00:31:26.360 --> 00:31:28.920]   Do you think this approach of third-person watching,
[00:31:28.920 --> 00:31:31.800]   the autonomous driving is amenable
[00:31:31.800 --> 00:31:33.000]   to this kind of approach?
[00:31:33.000 --> 00:31:37.960]   - So for autonomous driving, I would say it's,
[00:31:37.960 --> 00:31:41.400]   third-person is slightly easier.
[00:31:41.400 --> 00:31:43.240]   And the reason I'm gonna say it's slightly easier
[00:31:43.240 --> 00:31:45.480]   to do with third-person is because
[00:31:46.520 --> 00:31:48.760]   the car dynamics are very well understood.
[00:31:48.760 --> 00:31:51.720]   So the-- - Easier than
[00:31:51.720 --> 00:31:53.800]   first-person, you mean?
[00:31:53.800 --> 00:31:55.560]   Or easier than-- - I think it's,
[00:31:55.560 --> 00:31:57.400]   so I think the distinction between third-person
[00:31:57.400 --> 00:32:00.120]   and first-person is not a very important distinction
[00:32:00.120 --> 00:32:01.720]   for autonomous driving.
[00:32:01.720 --> 00:32:06.040]   They're very similar, because the distinction is really about
[00:32:06.040 --> 00:32:07.720]   who turns the steering wheel,
[00:32:07.720 --> 00:32:11.720]   or maybe, let me put it differently.
[00:32:11.720 --> 00:32:14.680]   How to get from a point where you are now
[00:32:14.680 --> 00:32:17.320]   to a point, let's say, a couple meters in front of you.
[00:32:17.320 --> 00:32:19.080]   And that's a problem that's very well understood.
[00:32:19.080 --> 00:32:20.120]   And that's the only distinction
[00:32:20.120 --> 00:32:21.640]   between third and first-person there.
[00:32:21.640 --> 00:32:23.080]   Whereas with the robot manipulation,
[00:32:23.080 --> 00:32:25.240]   interaction forces are very complex,
[00:32:25.240 --> 00:32:26.680]   and it's still a very different thing.
[00:32:26.680 --> 00:32:31.320]   For autonomous driving, I think there is still the question,
[00:32:31.320 --> 00:32:33.880]   imitation versus RL.
[00:32:33.880 --> 00:32:36.600]   So imitation gives you a lot more signal.
[00:32:36.600 --> 00:32:38.760]   I think where imitation is lacking
[00:32:38.760 --> 00:32:41.560]   and needs some extra machinery is,
[00:32:42.280 --> 00:32:45.320]   it doesn't, in its normal format,
[00:32:45.320 --> 00:32:47.560]   doesn't think about goals or objectives.
[00:32:47.560 --> 00:32:50.920]   And of course, there are versions of imitation learning,
[00:32:50.920 --> 00:32:52.760]   inverse reinforcement learning type imitation learning,
[00:32:52.760 --> 00:32:54.440]   which also thinks about goals.
[00:32:54.440 --> 00:32:56.840]   I think then we're getting much closer.
[00:32:56.840 --> 00:32:58.520]   But I think it's very hard to think of a
[00:32:58.520 --> 00:33:03.880]   fully reactive car generalizing well.
[00:33:03.880 --> 00:33:05.800]   If it really doesn't have a notion of objectives
[00:33:05.800 --> 00:33:08.520]   to generalize well to the kind of generality
[00:33:08.520 --> 00:33:09.400]   that you would want.
[00:33:09.400 --> 00:33:11.960]   You'd want more than just that reactivity
[00:33:11.960 --> 00:33:15.160]   that you get from just behavioral cloning/supervised learning.
[00:33:15.160 --> 00:33:18.280]   - So a lot of the work,
[00:33:18.280 --> 00:33:21.880]   whether it's self-play or even imitation learning,
[00:33:21.880 --> 00:33:24.040]   would benefit significantly from simulation,
[00:33:24.040 --> 00:33:26.360]   from effective simulation.
[00:33:26.360 --> 00:33:28.120]   And you're doing a lot of stuff in the physical world
[00:33:28.120 --> 00:33:29.480]   and in simulation.
[00:33:29.480 --> 00:33:32.440]   Do you have hope for greater and greater
[00:33:32.440 --> 00:33:38.200]   power of simulation being boundless eventually
[00:33:38.200 --> 00:33:40.520]   to where most of what we need to operate
[00:33:40.520 --> 00:33:43.560]   in the physical world could be simulated
[00:33:43.560 --> 00:33:46.280]   to a degree that's directly transferable
[00:33:46.280 --> 00:33:47.400]   to the physical world?
[00:33:47.400 --> 00:33:49.080]   Or are we still very far away from that?
[00:33:49.080 --> 00:33:57.560]   - So I think we could even rephrase that question
[00:33:57.560 --> 00:33:58.280]   in some sense.
[00:33:58.280 --> 00:33:58.920]   - Please.
[00:33:58.920 --> 00:34:03.560]   - And so the power of simulation, right?
[00:34:03.560 --> 00:34:06.440]   As simulators get better and better,
[00:34:06.440 --> 00:34:08.840]   of course, becomes stronger
[00:34:08.840 --> 00:34:10.360]   and we can learn more in simulation.
[00:34:11.080 --> 00:34:12.280]   But there's also another version,
[00:34:12.280 --> 00:34:13.560]   which is where you say the simulator
[00:34:13.560 --> 00:34:15.240]   doesn't even have to be that precise.
[00:34:15.240 --> 00:34:17.800]   As long as it's somewhat representative
[00:34:17.800 --> 00:34:20.360]   and instead of trying to get one simulator
[00:34:20.360 --> 00:34:23.000]   that is sufficiently precise to learn in
[00:34:23.000 --> 00:34:25.160]   and transfer really well to the real world,
[00:34:25.160 --> 00:34:26.600]   I'm going to build many simulators.
[00:34:26.600 --> 00:34:28.040]   - Ensemble of simulators.
[00:34:28.040 --> 00:34:29.240]   - Ensemble of simulators.
[00:34:29.240 --> 00:34:31.960]   Not any single one of them
[00:34:31.960 --> 00:34:34.520]   is sufficiently representative of the real world
[00:34:34.520 --> 00:34:37.800]   such that it would work if you train in there.
[00:34:37.800 --> 00:34:39.400]   But if you train in all of them,
[00:34:39.720 --> 00:34:43.400]   then there is something that's good in all of them.
[00:34:43.400 --> 00:34:47.480]   The real world will just be another one of them
[00:34:47.480 --> 00:34:49.480]   that's not identical to any one of them,
[00:34:49.480 --> 00:34:50.600]   but just another one of them.
[00:34:50.600 --> 00:34:52.920]   - Another sample from the distribution of simulators.
[00:34:52.920 --> 00:34:53.240]   - Exactly.
[00:34:53.240 --> 00:34:54.680]   - We do live in a simulation,
[00:34:54.680 --> 00:34:57.400]   so this is just one other one.
[00:34:57.400 --> 00:34:59.160]   - I'm not sure about that, but yeah.
[00:34:59.160 --> 00:35:03.320]   It's definitely a very advanced simulator if it is.
[00:35:03.320 --> 00:35:04.760]   - Yeah, it's a pretty good one.
[00:35:04.760 --> 00:35:07.320]   I've talked to Russell.
[00:35:07.320 --> 00:35:09.080]   It's something you think about a little bit too.
[00:35:09.320 --> 00:35:11.880]   Of course, you're really trying to build these systems,
[00:35:11.880 --> 00:35:13.640]   but do you think about the future of AI?
[00:35:13.640 --> 00:35:15.560]   A lot of people have concern about safety.
[00:35:15.560 --> 00:35:18.120]   How do you think about AI safety
[00:35:18.120 --> 00:35:20.920]   as you build robots that are operating in the physical world?
[00:35:20.920 --> 00:35:24.920]   How do you approach this problem
[00:35:24.920 --> 00:35:27.400]   in an engineering kind of way, in a systematic way?
[00:35:27.400 --> 00:35:32.200]   - So when a robot is doing things,
[00:35:32.200 --> 00:35:36.120]   you kind of have a few notions of safety to worry about.
[00:35:36.120 --> 00:35:39.240]   One is that the robot is physically strong
[00:35:39.240 --> 00:35:41.480]   and of course could do a lot of damage.
[00:35:41.480 --> 00:35:45.240]   Same for cars, which we can think of as robots too in some way.
[00:35:45.240 --> 00:35:48.200]   And this could be completely unintentional.
[00:35:48.200 --> 00:35:51.640]   So it could be not the kind of long-term AI safety concerns
[00:35:51.640 --> 00:35:54.200]   that, okay, AI is smarter than us and now what do we do?
[00:35:54.200 --> 00:35:55.720]   But it could be just very practical.
[00:35:55.720 --> 00:35:57.720]   Okay, this robot, if it makes a mistake,
[00:35:57.720 --> 00:36:00.520]   what are the results going to be?
[00:36:00.520 --> 00:36:02.200]   Of course, simulation comes in a lot there
[00:36:02.200 --> 00:36:04.040]   to test in simulation.
[00:36:04.280 --> 00:36:05.560]   It's a difficult question.
[00:36:05.560 --> 00:36:07.400]   And I'm always wondering, like I always wonder,
[00:36:07.400 --> 00:36:09.960]   let's say you look at, let's go back to driving
[00:36:09.960 --> 00:36:12.200]   'cause a lot of people know driving well, of course.
[00:36:12.200 --> 00:36:16.600]   What do we do to test somebody for driving, right?
[00:36:16.600 --> 00:36:18.440]   To get a driver's license?
[00:36:18.440 --> 00:36:19.400]   What do they really do?
[00:36:19.400 --> 00:36:24.840]   I mean, you fill out some tests and then you drive.
[00:36:24.840 --> 00:36:27.640]   And I mean, in suburban California,
[00:36:27.640 --> 00:36:31.080]   that driving test is just you drive around the block,
[00:36:31.080 --> 00:36:34.600]   pull over, you do a stop sign successfully
[00:36:34.600 --> 00:36:37.560]   and then you pull over again and you're pretty much done.
[00:36:37.560 --> 00:36:42.680]   And you're like, okay, if a self-driving car did that,
[00:36:42.680 --> 00:36:45.080]   would you trust it that it can drive?
[00:36:45.080 --> 00:36:47.240]   And I'd be like, no, that's not enough for me to trust it.
[00:36:47.240 --> 00:36:49.800]   But somehow for humans, we've figured out
[00:36:49.800 --> 00:36:53.160]   that somebody being able to do that is representative
[00:36:53.160 --> 00:36:56.280]   of them being able to do a lot of other things.
[00:36:56.280 --> 00:36:58.360]   And so I think somehow for humans,
[00:36:58.360 --> 00:37:02.040]   we've figured out representative tests of what it means
[00:37:02.040 --> 00:37:04.120]   if you can do this, what you can really do.
[00:37:04.120 --> 00:37:05.720]   Of course, testing humans,
[00:37:05.720 --> 00:37:07.400]   humans don't wanna be tested at all times.
[00:37:07.400 --> 00:37:08.520]   Self-driving cars or robots
[00:37:08.520 --> 00:37:10.200]   could be tested more often probably.
[00:37:10.200 --> 00:37:11.640]   You can have replicants that get tested
[00:37:11.640 --> 00:37:13.080]   and they're known to be identical
[00:37:13.080 --> 00:37:15.400]   'cause they use the same neural net and so forth.
[00:37:15.400 --> 00:37:19.480]   But still, I feel like we don't have this kind of unit tests
[00:37:19.480 --> 00:37:22.680]   or proper tests for robots.
[00:37:22.680 --> 00:37:23.800]   And I think there's something very interesting
[00:37:23.800 --> 00:37:25.080]   to be thought about there,
[00:37:25.080 --> 00:37:26.680]   especially as you update things.
[00:37:26.680 --> 00:37:28.120]   Your software improves,
[00:37:28.120 --> 00:37:31.000]   you have a better self-driving car suite, you update it.
[00:37:31.000 --> 00:37:34.600]   How do you know it's indeed more capable on everything
[00:37:34.600 --> 00:37:35.960]   than what you had before
[00:37:35.960 --> 00:37:40.120]   that you didn't have any bad things creep into it?
[00:37:40.120 --> 00:37:42.120]   So I think that's a very interesting direction of research
[00:37:42.120 --> 00:37:44.920]   that there is no real solution yet,
[00:37:44.920 --> 00:37:46.520]   except that somehow for humans we do.
[00:37:46.520 --> 00:37:49.400]   'Cause we say, okay, you have a driving test, you passed,
[00:37:49.400 --> 00:37:50.520]   you can go on the road now.
[00:37:50.520 --> 00:37:53.400]   And humans have accidents every like million
[00:37:53.400 --> 00:37:55.640]   or 10 million miles, something pretty phenomenal.
[00:37:55.640 --> 00:38:01.560]   Compared to that short test that is being done.
[00:38:01.560 --> 00:38:05.240]   - So let me ask, you've mentioned that Andrew Ang,
[00:38:05.240 --> 00:38:07.560]   by example, showed you the value of kindness.
[00:38:07.560 --> 00:38:14.440]   Do you think the space of policies,
[00:38:14.440 --> 00:38:16.680]   good policies for humans and for AI
[00:38:16.680 --> 00:38:20.120]   is populated by policies that,
[00:38:20.120 --> 00:38:25.480]   with kindness or ones that are the opposite?
[00:38:25.720 --> 00:38:28.040]   Exploitation, even evil.
[00:38:28.040 --> 00:38:30.120]   So if you just look at the sea of policies
[00:38:30.120 --> 00:38:32.440]   we operate under as human beings,
[00:38:32.440 --> 00:38:35.080]   or if AI system had to operate in this real world,
[00:38:35.080 --> 00:38:37.880]   do you think it's really easy to find policies
[00:38:37.880 --> 00:38:39.400]   that are full of kindness,
[00:38:39.400 --> 00:38:41.160]   like would naturally fall into them?
[00:38:41.160 --> 00:38:44.440]   Or is it like a very hard optimization problem?
[00:38:44.440 --> 00:38:50.680]   - I mean, there is kind of two optimizations happening
[00:38:50.680 --> 00:38:52.120]   for humans, right?
[00:38:52.120 --> 00:38:53.080]   So for humans, there's kind of
[00:38:53.080 --> 00:38:54.600]   the very long-term optimization,
[00:38:54.600 --> 00:38:56.680]   which evolution has done for us.
[00:38:56.680 --> 00:39:00.520]   And we're kind of predisposed to like certain things.
[00:39:00.520 --> 00:39:02.600]   And that's in some sense what makes our learning easier
[00:39:02.600 --> 00:39:04.600]   because I mean, we know things like pain
[00:39:04.600 --> 00:39:08.200]   and hunger and thirst.
[00:39:08.200 --> 00:39:09.960]   And the fact that we know about those
[00:39:09.960 --> 00:39:11.640]   is not something that we were taught.
[00:39:11.640 --> 00:39:12.520]   That's kind of innate.
[00:39:12.520 --> 00:39:13.800]   When we're hungry, we're unhappy.
[00:39:13.800 --> 00:39:15.160]   When we're thirsty, we're unhappy.
[00:39:15.160 --> 00:39:18.280]   When we have pain, we're unhappy.
[00:39:18.280 --> 00:39:21.560]   And ultimately evolution built that into us
[00:39:21.560 --> 00:39:22.520]   to think about those things.
[00:39:22.520 --> 00:39:24.520]   And so I think there is a notion that
[00:39:24.520 --> 00:39:27.320]   it seems somehow humans evolved in general
[00:39:27.320 --> 00:39:32.120]   to prefer to get along in some ways,
[00:39:32.120 --> 00:39:36.280]   but at the same time, also to be very territorial
[00:39:36.280 --> 00:39:39.720]   and kind of centric to their own tribe.
[00:39:39.720 --> 00:39:43.480]   It seems like that's the kind of space
[00:39:43.480 --> 00:39:44.600]   we converged onto.
[00:39:44.600 --> 00:39:46.520]   I mean, I'm not an expert in anthropology,
[00:39:46.520 --> 00:39:47.960]   but it seems like we're very kind of
[00:39:47.960 --> 00:39:50.200]   good within our own tribe,
[00:39:50.200 --> 00:39:51.720]   but need to be taught.
[00:39:52.440 --> 00:39:54.360]   To be nice to other tribes.
[00:39:54.360 --> 00:39:56.200]   - Well, if you look at Steven Pinker,
[00:39:56.200 --> 00:39:57.800]   he highlights this pretty nicely
[00:39:57.800 --> 00:40:02.200]   in "Better Angels of Our Nature"
[00:40:02.200 --> 00:40:03.480]   where he talks about violence
[00:40:03.480 --> 00:40:05.480]   decreasing over time consistently.
[00:40:05.480 --> 00:40:08.200]   So whatever tension, whatever teams we pick,
[00:40:08.200 --> 00:40:11.000]   it seems that the long arc of history
[00:40:11.000 --> 00:40:13.720]   goes towards us getting along more and more.
[00:40:13.720 --> 00:40:14.760]   - I hope so.
[00:40:14.760 --> 00:40:16.200]   (laughing)
[00:40:16.200 --> 00:40:17.880]   - So do you think that,
[00:40:17.880 --> 00:40:22.200]   do you think it's possible to teach RL
[00:40:22.200 --> 00:40:26.040]   based robots this kind of kindness,
[00:40:26.040 --> 00:40:28.280]   this kind of ability to interact with humans,
[00:40:28.280 --> 00:40:29.560]   this kind of policy,
[00:40:29.560 --> 00:40:32.120]   even to, let me ask a fun one.
[00:40:32.120 --> 00:40:35.000]   Do you think it's possible to teach RL based robot
[00:40:35.000 --> 00:40:36.200]   to love a human being
[00:40:36.200 --> 00:40:39.960]   and to inspire that human to love the robot back?
[00:40:39.960 --> 00:40:43.720]   So to like a RL based algorithm
[00:40:43.720 --> 00:40:45.880]   that leads to a happy marriage.
[00:40:45.880 --> 00:40:48.760]   - That's an interesting question.
[00:40:48.760 --> 00:40:52.520]   Maybe I'll answer it with another question, right?
[00:40:52.520 --> 00:40:54.120]   (laughing)
[00:40:54.120 --> 00:40:56.520]   'Cause I mean, but I'll come back to it.
[00:40:56.520 --> 00:40:58.040]   So another question you can have is,
[00:40:58.040 --> 00:41:02.840]   okay, I mean, how close does some people's happiness get
[00:41:02.840 --> 00:41:07.480]   from interacting with just a really nice dog?
[00:41:07.480 --> 00:41:09.720]   Like, I mean, dogs, you come home,
[00:41:09.720 --> 00:41:10.520]   that's what dogs do.
[00:41:10.520 --> 00:41:11.960]   They greet you, they're excited.
[00:41:11.960 --> 00:41:14.520]   It makes you happy when you come home to your dog.
[00:41:14.520 --> 00:41:16.280]   You're just like, okay, this is exciting.
[00:41:16.280 --> 00:41:18.120]   They're always happy when I'm here.
[00:41:18.120 --> 00:41:19.640]   I mean, if they don't greet you,
[00:41:19.640 --> 00:41:20.520]   'cause maybe whatever,
[00:41:20.520 --> 00:41:22.920]   your partner took him on a trip or something,
[00:41:22.920 --> 00:41:25.960]   you might not be nearly as happy when you get home, right?
[00:41:25.960 --> 00:41:27.560]   And so the kind of,
[00:41:27.560 --> 00:41:31.000]   it seems like the level of reasoning a dog has
[00:41:31.000 --> 00:41:32.040]   is pretty sophisticated,
[00:41:32.040 --> 00:41:35.480]   but then it's still not yet at the level of human reasoning.
[00:41:35.480 --> 00:41:37.640]   And so it seems like we don't even need to achieve
[00:41:37.640 --> 00:41:40.360]   human level reasoning to get like very strong affection
[00:41:40.360 --> 00:41:41.560]   with humans.
[00:41:41.560 --> 00:41:44.280]   And so my thinking is why not, right?
[00:41:44.280 --> 00:41:45.480]   Why couldn't with an AI,
[00:41:45.480 --> 00:41:48.920]   couldn't we achieve the kind of level of affection
[00:41:48.920 --> 00:41:51.960]   that humans feel among each other
[00:41:51.960 --> 00:41:55.800]   or with friendly animals and so forth?
[00:41:55.800 --> 00:41:59.560]   It's a question, is it a good thing for us or not?
[00:41:59.560 --> 00:42:01.240]   That's another thing, right?
[00:42:01.240 --> 00:42:02.040]   Because I mean,
[00:42:02.040 --> 00:42:05.720]   but I don't see why not.
[00:42:05.720 --> 00:42:07.000]   - Why not, yeah.
[00:42:07.000 --> 00:42:08.920]   So Elon Musk says love is the answer.
[00:42:08.920 --> 00:42:12.520]   Maybe he should say love is the objective function
[00:42:12.520 --> 00:42:14.360]   and then RL is the answer, right?
[00:42:14.360 --> 00:42:15.640]   (both laughing)
[00:42:15.640 --> 00:42:16.200]   - Well, maybe.
[00:42:16.200 --> 00:42:17.480]   (both laughing)
[00:42:17.480 --> 00:42:18.760]   - Oh, Peter, thank you so much.
[00:42:18.760 --> 00:42:20.120]   I don't wanna take up more of your time.
[00:42:20.120 --> 00:42:21.400]   Thank you so much for talking today.
[00:42:21.400 --> 00:42:23.320]   - Well, thanks for coming by.
[00:42:23.320 --> 00:42:24.200]   Great to have you visit.
[00:42:24.680 --> 00:42:27.260]   (upbeat music)
[00:42:27.260 --> 00:42:29.840]   (upbeat music)
[00:42:29.840 --> 00:42:32.420]   (upbeat music)
[00:42:32.420 --> 00:42:35.000]   (upbeat music)
[00:42:35.000 --> 00:42:37.580]   (upbeat music)
[00:42:37.580 --> 00:42:40.160]   (upbeat music)
[00:42:40.160 --> 00:42:50.160]   [BLANK_AUDIO]

