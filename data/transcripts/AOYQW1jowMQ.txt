
[00:00:00.000 --> 00:00:03.500]   - If you don't risk it, you don't get this get,
[00:00:03.500 --> 00:00:04.420]   is what I've heard.
[00:00:04.420 --> 00:00:05.260]   And so I think it's worth it.
[00:00:05.260 --> 00:00:06.820]   - Exactly, exactly, go for it, go for it.
[00:00:06.820 --> 00:00:09.420]   We have to try things.
[00:00:09.420 --> 00:00:10.400]   - Great, all right.
[00:00:10.400 --> 00:00:14.060]   Well, while that gets going and gets started,
[00:00:14.060 --> 00:00:17.760]   let me tell you a little bit about our speaker for today.
[00:00:17.760 --> 00:00:21.440]   So I'm really excited to have on Javier Idiomi,
[00:00:21.440 --> 00:00:24.340]   who is a, what you might call
[00:00:24.340 --> 00:00:26.760]   a multidisciplinary multidisciplinist,
[00:00:26.760 --> 00:00:29.200]   who's done just a wide variety of work
[00:00:29.200 --> 00:00:30.840]   in a wide variety of fields,
[00:00:30.840 --> 00:00:34.900]   both scientific, artistic, political, lots of places.
[00:00:34.900 --> 00:00:36.880]   And one of those things that he worked on
[00:00:36.880 --> 00:00:40.140]   is something that's really near and dear to my heart,
[00:00:40.140 --> 00:00:44.500]   which is the landscapes of the losses of neural networks.
[00:00:44.500 --> 00:00:47.080]   That is the shape of the loss function
[00:00:47.080 --> 00:00:51.060]   that we normally only get to see in the form of gradients
[00:00:51.060 --> 00:00:53.680]   or a few tiny glimpses,
[00:00:53.680 --> 00:00:57.560]   this really cool way of getting to see it in greater detail.
[00:00:57.560 --> 00:01:00.120]   And so there's, it's an amazing project.
[00:01:00.120 --> 00:01:02.620]   It's a ton of fun to both to play around with
[00:01:02.620 --> 00:01:04.600]   and to learn a lot about neural networks
[00:01:04.600 --> 00:01:06.760]   and he'll be talking about it today.
[00:01:06.760 --> 00:01:10.340]   So without any further ado,
[00:01:10.340 --> 00:01:13.100]   let me hand it over to Javier.
[00:01:13.100 --> 00:01:17.320]   - Thank you very much, Charles, for introduction.
[00:01:17.320 --> 00:01:18.840]   And thank you to Weights and Biases
[00:01:18.840 --> 00:01:20.240]   for inviting me to give this talk.
[00:01:20.240 --> 00:01:21.360]   It's really exciting.
[00:01:21.360 --> 00:01:23.640]   And also very excited that you've done your PhD
[00:01:23.640 --> 00:01:25.480]   also in a similar area.
[00:01:25.480 --> 00:01:26.880]   So this is super exciting.
[00:01:26.880 --> 00:01:28.280]   All right, so I'm gonna speak
[00:01:28.280 --> 00:01:30.000]   about the Lost Landscape Project.
[00:01:30.000 --> 00:01:33.040]   So what I'm gonna do is over a few minutes,
[00:01:33.040 --> 00:01:36.000]   I'm gonna speak a bit about the context of it all.
[00:01:36.000 --> 00:01:39.800]   Then I'm gonna take some minutes to speak about the dynamics
[00:01:39.800 --> 00:01:41.840]   of these landscapes in movement.
[00:01:41.840 --> 00:01:43.600]   That is really fascinating.
[00:01:43.600 --> 00:01:46.640]   And then I'm going to show a 10 minute video
[00:01:46.640 --> 00:01:48.880]   with many examples of the project.
[00:01:48.880 --> 00:01:52.640]   And I'm gonna speak over the video describing what we see.
[00:01:52.640 --> 00:01:54.960]   And I will end up talking a little bit
[00:01:54.960 --> 00:01:57.600]   about the Lost Landscape Explorer app
[00:01:57.600 --> 00:01:59.960]   that I just released a few days ago.
[00:01:59.960 --> 00:02:00.960]   So let's go for it.
[00:02:00.960 --> 00:02:04.840]   So as Charles said, the background, my background,
[00:02:04.840 --> 00:02:08.280]   the background of this project is multidisciplinary.
[00:02:08.280 --> 00:02:10.000]   And like my background,
[00:02:10.000 --> 00:02:12.880]   there is a combination of engineering, technical fields,
[00:02:12.880 --> 00:02:14.760]   many creative fields.
[00:02:14.760 --> 00:02:17.320]   And so what are we doing in the Lost Landscape Project?
[00:02:17.320 --> 00:02:20.280]   So we care, of course, very much, right,
[00:02:20.280 --> 00:02:22.920]   about the performance of our neural networks.
[00:02:22.920 --> 00:02:26.080]   And we measure that performance with the loss functions.
[00:02:26.080 --> 00:02:30.360]   And these loss functions are comparing what we are obtaining
[00:02:30.360 --> 00:02:33.120]   and the target that we want to obtain.
[00:02:33.120 --> 00:02:34.920]   And this way we get the loss value
[00:02:34.920 --> 00:02:36.280]   and we use back propagation
[00:02:36.280 --> 00:02:38.200]   and we change the weights of the network.
[00:02:38.200 --> 00:02:41.880]   And of course, the loss function depends on these weights
[00:02:41.880 --> 00:02:44.000]   and we may have millions or billions
[00:02:44.000 --> 00:02:46.360]   and very soon trillions of weights.
[00:02:46.360 --> 00:02:48.640]   So these are very high dimensional
[00:02:48.640 --> 00:02:50.840]   and nonlinear functions.
[00:02:50.840 --> 00:02:52.280]   And of course, we can use different types
[00:02:52.280 --> 00:02:53.120]   of loss functions,
[00:02:53.120 --> 00:02:55.280]   depending on the objective of what we're doing,
[00:02:55.280 --> 00:02:57.200]   if we're doing a regression, classification,
[00:02:57.200 --> 00:02:58.320]   or working with GANs.
[00:02:58.320 --> 00:03:00.280]   But the objective is always the same.
[00:03:00.280 --> 00:03:04.720]   We are trying to push these loss value as small as possible,
[00:03:04.720 --> 00:03:05.880]   of course, in different ways,
[00:03:05.880 --> 00:03:07.160]   depending on the constraints
[00:03:07.160 --> 00:03:09.400]   and the needs of whatever we're doing.
[00:03:09.400 --> 00:03:13.000]   And so it would be really interesting for us to understand
[00:03:13.000 --> 00:03:15.000]   what is going on in all of this complexity
[00:03:15.000 --> 00:03:17.600]   on all of these complex weight spaces
[00:03:17.600 --> 00:03:19.680]   and to tackle this challenge,
[00:03:19.680 --> 00:03:24.440]   numerical analysis and visualization complement each other
[00:03:24.440 --> 00:03:27.240]   and they are partners that work together.
[00:03:27.240 --> 00:03:28.320]   And working together,
[00:03:28.320 --> 00:03:31.400]   they can help us understand through this,
[00:03:31.400 --> 00:03:36.400]   through understanding these complex high dimensional spaces,
[00:03:36.400 --> 00:03:39.520]   more about how generalization works,
[00:03:39.520 --> 00:03:41.840]   ways to improve our architectures
[00:03:41.840 --> 00:03:45.040]   and our optimization algorithms, et cetera, et cetera.
[00:03:45.040 --> 00:03:47.000]   And we all know the importance
[00:03:47.000 --> 00:03:49.120]   and the usefulness of visualization
[00:03:49.120 --> 00:03:52.280]   because visualization complements numerical analysis,
[00:03:52.280 --> 00:03:54.960]   but it deals with information in a different way.
[00:03:54.960 --> 00:03:57.120]   And it gives us a very unique
[00:03:57.120 --> 00:03:59.320]   and different perspective on things.
[00:03:59.320 --> 00:04:00.600]   That's why, for example,
[00:04:00.600 --> 00:04:04.360]   Einstein used to mainly use visualization
[00:04:04.360 --> 00:04:06.560]   when he was chasing new insights.
[00:04:06.560 --> 00:04:07.400]   For example,
[00:04:07.400 --> 00:04:10.600]   imagining that he was a photon traveling through space,
[00:04:10.600 --> 00:04:12.880]   one of the many visualizations he used
[00:04:12.880 --> 00:04:15.200]   to get to his theory of relativity.
[00:04:15.200 --> 00:04:16.040]   And on the right,
[00:04:16.040 --> 00:04:18.840]   we can see the first picture of a black hole
[00:04:18.840 --> 00:04:20.680]   that was released recently.
[00:04:20.680 --> 00:04:25.080]   And that may eventually one day earn a Nobel prize.
[00:04:25.080 --> 00:04:27.200]   And these visualizations,
[00:04:27.200 --> 00:04:29.280]   what Einstein used to imagine
[00:04:29.280 --> 00:04:30.720]   and the picture of the black hole
[00:04:30.720 --> 00:04:32.480]   and the representations that we will see
[00:04:32.480 --> 00:04:33.480]   of the lost landscapes,
[00:04:33.480 --> 00:04:35.920]   they are big simplifications
[00:04:35.920 --> 00:04:39.440]   of that underlying complexity and reality.
[00:04:39.440 --> 00:04:41.320]   And yet they are so important.
[00:04:41.320 --> 00:04:42.280]   Why is this?
[00:04:42.280 --> 00:04:44.600]   Because when we are dealing with things
[00:04:44.600 --> 00:04:48.000]   and with complexities that are so unreachable,
[00:04:48.000 --> 00:04:50.520]   so far away from our reality,
[00:04:50.520 --> 00:04:51.800]   like the black hole,
[00:04:51.800 --> 00:04:54.040]   like what Einstein was dealing with,
[00:04:54.040 --> 00:04:55.920]   or like these lost landscape,
[00:04:55.920 --> 00:04:58.800]   like these very high dimensional spaces
[00:04:58.800 --> 00:05:01.280]   with billions of dimensions,
[00:05:01.280 --> 00:05:05.760]   even these very simple cross sections,
[00:05:05.760 --> 00:05:07.240]   simplifications,
[00:05:09.480 --> 00:05:13.160]   can be the key that take us to very unique insights.
[00:05:13.160 --> 00:05:17.360]   But the important question that we always have to do,
[00:05:17.360 --> 00:05:19.920]   either with the imagination of Einstein
[00:05:19.920 --> 00:05:21.360]   or the picture of the black hole
[00:05:21.360 --> 00:05:22.520]   of these representations
[00:05:22.520 --> 00:05:24.160]   that we will see of the lost landscapes
[00:05:24.160 --> 00:05:29.160]   is are they capturing at least some important connection
[00:05:29.160 --> 00:05:32.160]   that provides useful and actionable data?
[00:05:32.160 --> 00:05:33.000]   And we will see that
[00:05:33.000 --> 00:05:35.320]   with these lost landscape representations,
[00:05:35.320 --> 00:05:36.920]   the answer is yes.
[00:05:36.920 --> 00:05:39.200]   And we will explain why very soon.
[00:05:40.200 --> 00:05:42.640]   But so our challenge is that
[00:05:42.640 --> 00:05:46.800]   we're a little speck of dust in this big chart
[00:05:46.800 --> 00:05:49.120]   of the amount of dimensions
[00:05:49.120 --> 00:05:52.520]   that we can deal with in these visualizations.
[00:05:52.520 --> 00:05:55.120]   And of course, we can only visualize three dimensions.
[00:05:55.120 --> 00:05:58.680]   So we have to somehow simplify this complexity
[00:05:58.680 --> 00:06:02.320]   and we use dimensionality reduction techniques to do that.
[00:06:02.320 --> 00:06:03.240]   And in the last years,
[00:06:03.240 --> 00:06:04.520]   researchers have been using
[00:06:04.520 --> 00:06:07.120]   these dimensionality reduction techniques
[00:06:07.120 --> 00:06:10.440]   to create 1D plots, then 2D plots, then 3D plots.
[00:06:10.440 --> 00:06:13.080]   And for example, one of my favorite papers
[00:06:13.080 --> 00:06:15.760]   is this one by Tom Goldstein's team
[00:06:15.760 --> 00:06:18.120]   that is an absolute reference in this area,
[00:06:18.120 --> 00:06:20.800]   wonderful paper, and I refer to it very often.
[00:06:20.800 --> 00:06:23.560]   And so the lost landscape project
[00:06:23.560 --> 00:06:25.640]   that I'm speaking about today,
[00:06:25.640 --> 00:06:26.960]   what it's trying to do
[00:06:26.960 --> 00:06:29.480]   is to bring greater and greater detail
[00:06:29.480 --> 00:06:31.040]   to these visualizations
[00:06:31.040 --> 00:06:34.360]   and also to study them in movement,
[00:06:34.360 --> 00:06:35.840]   the dynamics in movement
[00:06:35.840 --> 00:06:37.400]   through the steps and the epochs,
[00:06:37.400 --> 00:06:41.760]   and also to bring to them the creative aspect
[00:06:41.760 --> 00:06:44.760]   that is not just about making these representations
[00:06:44.760 --> 00:06:47.280]   look nice, because a lot of people see them
[00:06:47.280 --> 00:06:49.440]   and they say, "Oh, they look nice."
[00:06:49.440 --> 00:06:51.120]   But it's more than that, right?
[00:06:51.120 --> 00:06:52.560]   For example, in this example,
[00:06:52.560 --> 00:06:54.640]   we can see a picture of a galaxy on the left,
[00:06:54.640 --> 00:06:56.200]   and this is the raw picture.
[00:06:56.200 --> 00:06:59.200]   And on the right, we see what people typically see,
[00:06:59.200 --> 00:07:01.480]   that is the post-produced version.
[00:07:01.480 --> 00:07:04.720]   And of course, the one on the right is much more beautiful,
[00:07:04.720 --> 00:07:06.720]   but it's more than that, right?
[00:07:06.720 --> 00:07:09.320]   It is easier to understand,
[00:07:09.320 --> 00:07:11.080]   it is easier to work with,
[00:07:11.080 --> 00:07:13.520]   and if we may arrive to a new insight
[00:07:13.520 --> 00:07:15.400]   with one of these two,
[00:07:15.400 --> 00:07:17.800]   it's probably going to be with the one on the right.
[00:07:17.800 --> 00:07:21.360]   So the creative aspect of the project
[00:07:21.360 --> 00:07:24.200]   is more than just about making it look nice,
[00:07:24.200 --> 00:07:26.480]   it's also about those other things.
[00:07:26.480 --> 00:07:29.160]   So we have all of this complexity,
[00:07:29.160 --> 00:07:31.160]   and in the last years, also researchers,
[00:07:31.160 --> 00:07:34.160]   as we take this dimensionality,
[00:07:34.160 --> 00:07:37.600]   we reduce the representations of these weight spaces,
[00:07:37.600 --> 00:07:39.520]   and we combine them with the loss values,
[00:07:39.520 --> 00:07:41.160]   and we build these representations,
[00:07:41.160 --> 00:07:43.040]   researchers have found connections
[00:07:43.040 --> 00:07:45.360]   between the geometry of the surfaces,
[00:07:45.360 --> 00:07:48.120]   also, I mean, using both numerical analysis
[00:07:48.120 --> 00:07:49.200]   and visualizations,
[00:07:49.200 --> 00:07:50.880]   and different properties of the networks.
[00:07:50.880 --> 00:07:53.040]   We all know that when we use ResNets, right?
[00:07:53.040 --> 00:07:56.840]   If we use skip connections, we get a smoother surface.
[00:07:56.840 --> 00:08:00.280]   And if we, for example, use SGD with small batch sizes,
[00:08:00.280 --> 00:08:03.160]   we tend to convert to flatter minima,
[00:08:03.160 --> 00:08:04.960]   and this tends to correlate,
[00:08:04.960 --> 00:08:07.920]   most researchers agree with better generalization,
[00:08:07.920 --> 00:08:09.560]   which is quite intuitive,
[00:08:09.560 --> 00:08:11.560]   because if we are in a minima
[00:08:11.560 --> 00:08:13.360]   in a certain point in weight space,
[00:08:13.360 --> 00:08:15.320]   and we move a little bit,
[00:08:15.320 --> 00:08:18.600]   and the minima is flatter, as we move,
[00:08:18.600 --> 00:08:21.080]   the loss value is not going to change too much,
[00:08:21.080 --> 00:08:25.000]   so we kind of have a wider margin of safety,
[00:08:25.000 --> 00:08:26.840]   which means that the boundaries,
[00:08:26.840 --> 00:08:29.560]   that for example, in classification, separate our data,
[00:08:29.560 --> 00:08:31.320]   they are gonna be more resilient.
[00:08:31.320 --> 00:08:34.120]   So, we find this type of connections,
[00:08:34.120 --> 00:08:36.000]   that's why we're interested in creating
[00:08:36.000 --> 00:08:37.760]   these types of visualizations,
[00:08:37.760 --> 00:08:40.480]   and so we get to a point in weight space,
[00:08:40.480 --> 00:08:41.880]   and we don't wanna be blind,
[00:08:41.880 --> 00:08:44.520]   we want to understand what happens
[00:08:44.520 --> 00:08:46.480]   if we move a little bit,
[00:08:46.480 --> 00:08:48.320]   what happens around us?
[00:08:48.320 --> 00:08:49.920]   Is the surface rough?
[00:08:49.920 --> 00:08:50.880]   Is it a smoother?
[00:08:50.880 --> 00:08:52.320]   Is it full of non-convexities?
[00:08:52.320 --> 00:08:53.160]   What's going on?
[00:08:53.160 --> 00:08:55.320]   And we use different mathematical operations, right?
[00:08:55.320 --> 00:08:57.360]   To navigate these weight spaces
[00:08:57.360 --> 00:08:58.640]   in the high dimensionality,
[00:08:58.680 --> 00:09:00.840]   we can do an interpolation
[00:09:00.840 --> 00:09:03.160]   from a point in weight space in a random direction,
[00:09:03.160 --> 00:09:05.040]   or we can do linear interpolation
[00:09:05.040 --> 00:09:06.920]   between two points in weight space,
[00:09:06.920 --> 00:09:09.200]   or we can pick a couple of random directions
[00:09:09.200 --> 00:09:10.720]   that are orthogonal to each other,
[00:09:10.720 --> 00:09:11.640]   and they make a plane,
[00:09:11.640 --> 00:09:14.320]   and we can slice the high dimensionality with it,
[00:09:14.320 --> 00:09:15.840]   and project it onto the plane,
[00:09:15.840 --> 00:09:18.760]   and calculate a lot of loss values on a grid on the plane,
[00:09:18.760 --> 00:09:21.360]   and build this 3D surface,
[00:09:21.360 --> 00:09:23.800]   or we can pick a couple of minimizers,
[00:09:23.800 --> 00:09:25.160]   points in weight space,
[00:09:25.160 --> 00:09:27.800]   and look for paths that connect
[00:09:27.800 --> 00:09:29.600]   these two minima,
[00:09:29.600 --> 00:09:31.360]   maintaining a low loss value,
[00:09:31.360 --> 00:09:32.560]   and this is a wonderful paper,
[00:09:32.560 --> 00:09:35.560]   and I will be talking about this in the examples.
[00:09:35.560 --> 00:09:38.160]   So we are exploring all of these cross sections
[00:09:38.160 --> 00:09:39.560]   of this hidden beauty,
[00:09:39.560 --> 00:09:42.400]   these hidden mysteries in the high dimensionality,
[00:09:42.400 --> 00:09:44.560]   and we can do this dimensionality reduction
[00:09:44.560 --> 00:09:45.560]   in many different ways.
[00:09:45.560 --> 00:09:49.000]   For example, we can use PCA directions,
[00:09:49.000 --> 00:09:50.360]   and these are the directions
[00:09:50.360 --> 00:09:53.040]   that are the most optimized in the system,
[00:09:53.040 --> 00:09:54.200]   down the gradient,
[00:09:54.200 --> 00:09:55.040]   and if we do this,
[00:09:55.040 --> 00:09:56.680]   we will be able to capture, for example,
[00:09:56.680 --> 00:09:59.400]   pretty well the variation in the trajectories,
[00:09:59.400 --> 00:10:01.720]   for example, the trajectories of SGD,
[00:10:01.720 --> 00:10:04.120]   but we will not capture so well
[00:10:04.120 --> 00:10:07.800]   all the richness around in the rest of the landscape,
[00:10:07.800 --> 00:10:09.480]   with the distribution of non-convexities
[00:10:09.480 --> 00:10:11.560]   and convexities, if they exist.
[00:10:11.560 --> 00:10:14.440]   Conversely, if we use random directions, for example,
[00:10:14.440 --> 00:10:16.720]   we will better be able to capture
[00:10:16.720 --> 00:10:18.160]   the richness of the landscape,
[00:10:18.160 --> 00:10:21.000]   the distributions of non-convexities and convexities,
[00:10:21.000 --> 00:10:23.040]   but we will have more trouble to capture,
[00:10:23.040 --> 00:10:25.200]   for example, the variation of the trajectories.
[00:10:25.200 --> 00:10:26.200]   Why is this?
[00:10:26.200 --> 00:10:28.520]   Because it has been shown that the trajectory,
[00:10:28.520 --> 00:10:30.480]   for example, of SGD, right,
[00:10:30.480 --> 00:10:34.720]   it exists on a very low dimensional subspace,
[00:10:34.720 --> 00:10:36.840]   and if we pick a random direction,
[00:10:36.840 --> 00:10:40.000]   because of the counterintuitive properties
[00:10:40.000 --> 00:10:41.640]   of high dimensional space,
[00:10:41.640 --> 00:10:44.600]   it will probably be orthogonal to the trajectory,
[00:10:44.600 --> 00:10:46.080]   and we will not be able to capture
[00:10:46.080 --> 00:10:47.600]   well the variation in the trajectory.
[00:10:47.600 --> 00:10:50.360]   So in summary, different strategies,
[00:10:50.360 --> 00:10:53.400]   different directions serve for different purposes,
[00:10:53.400 --> 00:10:55.840]   and we can do this dimensionality reductions
[00:10:55.840 --> 00:10:57.640]   in many different ways.
[00:10:57.640 --> 00:11:00.320]   Because of the same counterintuitive properties
[00:11:00.320 --> 00:11:02.000]   of these high dimensional spaces,
[00:11:02.000 --> 00:11:04.160]   when we pick a couple of random directions,
[00:11:04.160 --> 00:11:05.920]   it's practically guaranteed
[00:11:05.920 --> 00:11:07.760]   that they are going to be orthogonal to each other,
[00:11:07.760 --> 00:11:10.880]   because as we know, the higher the dimensionality,
[00:11:10.880 --> 00:11:13.360]   the higher the proportion of the space
[00:11:13.360 --> 00:11:16.120]   that is being occupied by the orthogonal vectors,
[00:11:16.120 --> 00:11:18.040]   and we can use the cosine similarity
[00:11:18.040 --> 00:11:21.400]   and basic proofs to demonstrate this.
[00:11:21.400 --> 00:11:24.120]   And finally, finally, the last thing
[00:11:24.120 --> 00:11:26.400]   that we have to consider very important
[00:11:26.400 --> 00:11:29.400]   is that we must normalize these directions,
[00:11:29.400 --> 00:11:31.520]   and it's very intuitive to understand why.
[00:11:31.520 --> 00:11:34.640]   If we have, for example, a couple of minimizers
[00:11:34.640 --> 00:11:36.760]   with different magnitudes in the weights,
[00:11:36.760 --> 00:11:38.840]   and we apply the same perturbation
[00:11:38.840 --> 00:11:41.400]   of the same magnitude to them,
[00:11:41.400 --> 00:11:44.480]   and we build visualizations and we compare them,
[00:11:44.480 --> 00:11:47.480]   we're going to see differences that are being caused
[00:11:47.480 --> 00:11:50.720]   because of the lack of the right proportions
[00:11:50.720 --> 00:11:53.480]   between the perturbation and the magnitudes of these weights.
[00:11:53.480 --> 00:11:56.600]   And this can produce different types of issues,
[00:11:56.600 --> 00:11:58.280]   like we see in this example
[00:11:58.280 --> 00:12:01.400]   from Tom Goldstein's team paper.
[00:12:01.400 --> 00:12:03.440]   We can have, for example, a small batch size
[00:12:03.440 --> 00:12:05.080]   and large batch size minimizers,
[00:12:05.080 --> 00:12:07.160]   and we can apply weighted decay,
[00:12:07.160 --> 00:12:09.840]   and of course, the updates are going to happen more often
[00:12:09.840 --> 00:12:11.360]   in the case of the small batch,
[00:12:11.360 --> 00:12:13.520]   so those weights are going to get smaller.
[00:12:13.520 --> 00:12:15.200]   And then we build the visualizations,
[00:12:15.200 --> 00:12:16.280]   and we see that, for example,
[00:12:16.280 --> 00:12:19.400]   a minima that looked flatter now looks sharper,
[00:12:19.400 --> 00:12:21.360]   but the generalization is still better.
[00:12:21.360 --> 00:12:23.760]   So again, we're seeing the impact
[00:12:23.760 --> 00:12:27.120]   of the lack of the right proportions
[00:12:27.120 --> 00:12:29.240]   of the perturbation compared to the weights.
[00:12:29.240 --> 00:12:32.120]   So we solve this very easily normalizing the directions.
[00:12:32.120 --> 00:12:34.560]   We can normalize by layer, by filter.
[00:12:34.560 --> 00:12:37.120]   By filter works very well if we have a conf net.
[00:12:37.120 --> 00:12:40.000]   If it's not a conf net, we can apply similar thing
[00:12:40.000 --> 00:12:42.720]   with the equivalent structure, and that's it.
[00:12:42.720 --> 00:12:45.280]   We create these dimensionality reductions,
[00:12:45.280 --> 00:12:49.280]   we produce these 3D representations,
[00:12:49.280 --> 00:12:51.880]   and then we ask ourselves,
[00:12:51.880 --> 00:12:55.560]   so are we capturing in these visualizations
[00:12:55.560 --> 00:12:57.120]   useful and actionable data?
[00:12:57.120 --> 00:12:59.640]   Do we have there a connection
[00:12:59.640 --> 00:13:02.320]   that even if we're so far away
[00:13:02.320 --> 00:13:05.160]   is useful enough to work with it?
[00:13:05.160 --> 00:13:07.960]   And yes, the researchers have demonstrated
[00:13:07.960 --> 00:13:10.880]   that the main curvatures of this dimensionality
[00:13:10.880 --> 00:13:14.000]   reduced representations are a weighted average
[00:13:14.000 --> 00:13:17.320]   of the curvatures in the high dimensional space.
[00:13:17.320 --> 00:13:20.720]   And to demonstrate this, they use numerical analysis,
[00:13:20.720 --> 00:13:23.320]   they use the Hessian, they use the eigenvalues,
[00:13:23.320 --> 00:13:25.000]   the extreme eigenvalues,
[00:13:25.000 --> 00:13:27.040]   the ratio of the extreme eigenvalues.
[00:13:27.040 --> 00:13:29.240]   They can, for example, build heat maps
[00:13:29.240 --> 00:13:32.240]   and study the distribution of non-convexities
[00:13:32.240 --> 00:13:34.600]   and convexities in the high dimensional space
[00:13:34.600 --> 00:13:37.640]   and compare them and see that they match.
[00:13:37.640 --> 00:13:40.560]   And that, for example, if we see non-convexities
[00:13:40.560 --> 00:13:44.480]   in this dimensionality reduced representations,
[00:13:44.480 --> 00:13:46.320]   it means there will be non-convexities
[00:13:46.320 --> 00:13:48.720]   in the high dimensional space.
[00:13:48.720 --> 00:13:50.840]   And if we have a positive curvature,
[00:13:50.840 --> 00:13:51.840]   if we have convexities,
[00:13:51.840 --> 00:13:55.200]   it doesn't mean that in the high dimensional space,
[00:13:55.200 --> 00:13:56.120]   everything is convex,
[00:13:56.120 --> 00:13:59.120]   but it means that the positive curvature is dominant.
[00:13:59.120 --> 00:14:01.160]   So in this way, we demonstrate
[00:14:01.160 --> 00:14:04.200]   that there is a solid enough connection
[00:14:04.200 --> 00:14:07.680]   that allows us to work with these visualizations
[00:14:07.680 --> 00:14:11.160]   and to try to chase new insights through them.
[00:14:11.160 --> 00:14:14.840]   And then finally, combining different networks
[00:14:14.840 --> 00:14:16.360]   and different parameters,
[00:14:16.360 --> 00:14:18.800]   these representations get built.
[00:14:18.800 --> 00:14:20.240]   Now we get to the dynamics
[00:14:20.240 --> 00:14:23.080]   that I haven't spoken so much about in other talks.
[00:14:23.080 --> 00:14:27.600]   As we do the same thing, not just in static terms,
[00:14:27.600 --> 00:14:30.760]   but also through different steps and epochs,
[00:14:30.760 --> 00:14:33.080]   and we build these representations in movement,
[00:14:33.080 --> 00:14:36.000]   we begin to see this counter-intuitive effects.
[00:14:36.000 --> 00:14:38.480]   We see shapes that appear out of nowhere.
[00:14:38.480 --> 00:14:40.280]   We see morphology shifts.
[00:14:40.280 --> 00:14:41.640]   We see tunneling.
[00:14:41.640 --> 00:14:43.120]   Why are these things happening?
[00:14:43.120 --> 00:14:45.800]   And we're gonna see these things in the examples
[00:14:45.800 --> 00:14:47.760]   that I'm going to show later.
[00:14:47.760 --> 00:14:51.080]   And for that, we have to accept that in comparison
[00:14:51.080 --> 00:14:53.640]   with this big challenge that we have ahead of us
[00:14:53.640 --> 00:14:56.520]   in these very, very high dimensional spaces,
[00:14:56.520 --> 00:15:00.880]   we live in our own flat land reality.
[00:15:00.880 --> 00:15:04.640]   And there is a wonderful talk given by mathematician,
[00:15:04.640 --> 00:15:06.920]   Matt Parker at the Royal Institution
[00:15:06.920 --> 00:15:09.000]   called Four Dimensional Maths,
[00:15:09.000 --> 00:15:12.600]   where he provides these beautiful examples in video
[00:15:12.600 --> 00:15:14.760]   where we see, for example, a 3D cube
[00:15:14.760 --> 00:15:16.720]   that is moving through a 2D world.
[00:15:16.720 --> 00:15:18.720]   And from the perspective of the 2D world,
[00:15:18.720 --> 00:15:22.480]   what we see is this square that appears out of nowhere
[00:15:22.480 --> 00:15:24.520]   and eventually disappears.
[00:15:24.520 --> 00:15:26.040]   Of course, from the perspective of the...
[00:15:26.040 --> 00:15:28.840]   And here we see the same thing corner first,
[00:15:28.840 --> 00:15:30.080]   and then it gets crazier.
[00:15:30.080 --> 00:15:32.040]   We see a triangle that appears.
[00:15:32.040 --> 00:15:35.840]   It becomes a different shape and eventually disappears.
[00:15:35.840 --> 00:15:37.960]   From the perspective of the 3D world,
[00:15:37.960 --> 00:15:39.480]   everything is continuous,
[00:15:39.480 --> 00:15:43.240]   but from the perspective of the 2D world,
[00:15:43.240 --> 00:15:44.800]   we see this counterintuitive effect.
[00:15:44.800 --> 00:15:46.400]   This is falling edge first,
[00:15:46.400 --> 00:15:49.400]   and we see this rectangle that appears out of nowhere.
[00:15:49.400 --> 00:15:51.240]   It expands, then it contracts,
[00:15:51.240 --> 00:15:53.160]   and then it disappears again.
[00:15:53.160 --> 00:15:56.840]   And now, now we see the same thing.
[00:15:56.840 --> 00:15:59.560]   And on the right, we can see what would be the perspective
[00:15:59.560 --> 00:16:01.880]   from the 2D world, all right?
[00:16:01.880 --> 00:16:04.080]   And now we're going to see the simulation
[00:16:04.080 --> 00:16:08.400]   of a 4D object falling through a 3D world,
[00:16:08.400 --> 00:16:12.760]   and the shapes get even more complex and even crazier.
[00:16:12.760 --> 00:16:16.440]   So in the representations of the landscapes in movement,
[00:16:16.440 --> 00:16:19.120]   we're going to see a similar dynamic.
[00:16:19.120 --> 00:16:22.720]   As we move through these steps and the epochs,
[00:16:22.720 --> 00:16:26.800]   every move triggers a dimensionality reduction
[00:16:26.800 --> 00:16:30.400]   transformation, and this is analog to filtering
[00:16:30.400 --> 00:16:32.520]   that high-dimensional space
[00:16:32.520 --> 00:16:35.520]   through our low-dimensional reality.
[00:16:35.520 --> 00:16:38.480]   The high-dimensional continuity remains,
[00:16:38.480 --> 00:16:41.640]   but those counterintuitive changes are caused
[00:16:41.640 --> 00:16:45.000]   by the act of filtering it through our low-dimensionality
[00:16:45.000 --> 00:16:46.600]   as we progress through that space.
[00:16:46.600 --> 00:16:49.280]   And this is what produces these counterintuitive effects
[00:16:49.280 --> 00:16:52.480]   that we're going to see in these examples.
[00:16:52.480 --> 00:16:53.520]   There is also the noise,
[00:16:53.520 --> 00:16:55.720]   because there is morphology noise in the landscapes
[00:16:55.720 --> 00:16:57.040]   that, of course, for example,
[00:16:57.040 --> 00:16:59.000]   like the skip connections in the ResNets
[00:16:59.000 --> 00:17:01.680]   that produce this moving effect.
[00:17:01.680 --> 00:17:04.120]   They come from the architecture, the hyperparameters,
[00:17:04.120 --> 00:17:05.320]   and there is the dynamic noise.
[00:17:05.320 --> 00:17:07.720]   And an example is if we use, for example,
[00:17:07.720 --> 00:17:10.560]   small batch sizes and we're capturing steps,
[00:17:10.560 --> 00:17:12.000]   and of course, in each batch,
[00:17:12.000 --> 00:17:14.480]   we're using a different part of the dataset.
[00:17:14.480 --> 00:17:16.680]   So this is going to produce a variation
[00:17:16.680 --> 00:17:19.280]   in the dynamics of the landscape, for example.
[00:17:19.280 --> 00:17:22.320]   All right, and we also see in the examples
[00:17:22.320 --> 00:17:27.320]   that Matt provides how, as we go higher in dimensionality,
[00:17:27.320 --> 00:17:29.720]   these counterintuitive effects,
[00:17:29.720 --> 00:17:32.520]   they get more and more complex as well.
[00:17:32.520 --> 00:17:35.080]   And in our case, as we go from this billion
[00:17:35.080 --> 00:17:36.600]   and soon trillion of dimensions
[00:17:36.600 --> 00:17:39.040]   to our two dimensions plus the last three,
[00:17:39.040 --> 00:17:41.880]   we're going to see a different counterintuitive things
[00:17:41.880 --> 00:17:43.880]   happening that we're going to see in the examples.
[00:17:43.880 --> 00:17:45.080]   One of them is tunneling.
[00:17:45.080 --> 00:17:47.480]   We may see at the beginning of a training process,
[00:17:47.480 --> 00:17:49.400]   a landscape that is quite flat.
[00:17:49.400 --> 00:17:51.720]   And then as the training progresses,
[00:17:51.720 --> 00:17:56.120]   we see how the convexity begins to tunnel down the landscape.
[00:17:56.120 --> 00:17:57.800]   And this, of course, doesn't mean
[00:17:57.800 --> 00:17:59.400]   that in the high dimensionality,
[00:17:59.400 --> 00:18:01.600]   there is like a hole being created.
[00:18:01.600 --> 00:18:02.440]   No, of course not.
[00:18:02.440 --> 00:18:04.040]   As we know in the high dimensionality,
[00:18:04.040 --> 00:18:05.960]   we're just moving in these millions and billions
[00:18:05.960 --> 00:18:07.080]   of different directions.
[00:18:07.080 --> 00:18:10.240]   But when we filter that through our flatland reality,
[00:18:10.240 --> 00:18:13.160]   we see this counterintuitive tunneling in this case.
[00:18:13.160 --> 00:18:14.800]   Now, what is really interesting about this
[00:18:14.800 --> 00:18:17.600]   is that we've been worried for a long time
[00:18:17.600 --> 00:18:21.080]   about the local minima and the subtle points, et cetera.
[00:18:21.080 --> 00:18:24.960]   But now we are beginning studying this type of dynamics
[00:18:24.960 --> 00:18:27.480]   with numerical analysis and with these visualizations,
[00:18:27.480 --> 00:18:30.280]   we're starting to go into mode connectivity,
[00:18:30.280 --> 00:18:32.640]   finding that is much easier than we thought
[00:18:32.640 --> 00:18:34.480]   to connect the different minima.
[00:18:34.480 --> 00:18:36.680]   And we're going into all of this concept
[00:18:36.680 --> 00:18:39.320]   of the blessing of dimensionality that, for example,
[00:18:39.320 --> 00:18:43.360]   Babak Hasibi explains really well in this wonderful talk,
[00:18:43.360 --> 00:18:46.120]   in which he tells us that when we initialize well
[00:18:46.120 --> 00:18:49.680]   our networks, the problem of finding a good minima
[00:18:49.680 --> 00:18:52.960]   goes from finding a needle in a haystack
[00:18:52.960 --> 00:18:56.320]   to exploring a haystack full of needles,
[00:18:56.320 --> 00:19:00.240]   because it becomes almost a local problem.
[00:19:00.240 --> 00:19:04.520]   And by studying all of these dynamics as well,
[00:19:04.520 --> 00:19:07.040]   we're getting closer to understand more and more
[00:19:07.040 --> 00:19:11.000]   how the high dimensionality has a lot of blessings
[00:19:11.000 --> 00:19:14.080]   that are being rebuilt little by little.
[00:19:14.080 --> 00:19:16.600]   So we're exploring all of these cross sections
[00:19:16.600 --> 00:19:18.120]   of this hidden beauty.
[00:19:18.120 --> 00:19:21.200]   And in whatever dimensionality we exist,
[00:19:21.200 --> 00:19:23.160]   we are encompassing the lower dimensions,
[00:19:23.160 --> 00:19:25.240]   we are exploring the cross sections
[00:19:25.240 --> 00:19:26.360]   of the higher dimensions.
[00:19:26.360 --> 00:19:28.080]   That's why in our 3D reality,
[00:19:28.080 --> 00:19:30.120]   we can see these slices of time.
[00:19:30.120 --> 00:19:32.080]   If we could live in higher dimensions,
[00:19:32.080 --> 00:19:35.800]   above time, maybe we would see time as all at once, right?
[00:19:35.800 --> 00:19:37.800]   But we are exploring all of these cross sections
[00:19:37.800 --> 00:19:40.320]   and always doing these dimensionality reductions
[00:19:40.320 --> 00:19:41.880]   that we can do in many different ways
[00:19:41.880 --> 00:19:43.120]   with different strategies,
[00:19:43.120 --> 00:19:45.120]   and even adding extra factors
[00:19:45.120 --> 00:19:47.040]   to communicate extra information.
[00:19:47.040 --> 00:19:49.920]   Extra informations, but they are all quite similar
[00:19:49.920 --> 00:19:51.320]   to each other.
[00:19:51.320 --> 00:19:54.760]   And finally, in the creation of these representations,
[00:19:54.760 --> 00:19:57.600]   and we are now going to see this 10 minute video.
[00:19:57.600 --> 00:20:00.840]   So as I explained, this is a multidisciplinary project
[00:20:00.840 --> 00:20:04.120]   in which I am combining a lot of different things, right?
[00:20:04.120 --> 00:20:07.000]   That go from the engineering, the programming,
[00:20:07.000 --> 00:20:09.520]   the optimization, the modeling,
[00:20:09.520 --> 00:20:12.560]   all the, what is the 3D surfaces and the audio visual,
[00:20:12.560 --> 00:20:14.920]   et cetera, et cetera, all of the different parts.
[00:20:14.920 --> 00:20:18.200]   And now we're gonna see this 10 minute video with examples.
[00:20:18.200 --> 00:20:20.840]   So the first one is the learning rate stress test
[00:20:20.840 --> 00:20:23.040]   in which I am basically using
[00:20:23.040 --> 00:20:26.080]   different learning rate schedules
[00:20:26.080 --> 00:20:31.080]   to basically study different resiliency ranges
[00:20:31.080 --> 00:20:34.680]   in which I modify the learning rates in very radical ways.
[00:20:34.680 --> 00:20:37.960]   And I study how the surfaces respond.
[00:20:37.960 --> 00:20:41.560]   And we can see how as you reach different parts
[00:20:41.560 --> 00:20:43.520]   of the high dimensionality,
[00:20:43.520 --> 00:20:44.800]   where the process collapses,
[00:20:44.800 --> 00:20:49.040]   this basically transforms into a similar collapse
[00:20:49.040 --> 00:20:51.560]   in the dimensionality reduced representation.
[00:20:51.560 --> 00:20:55.360]   We can also see the process of tunneling in these examples,
[00:20:55.360 --> 00:20:58.320]   how the convexity begins to tunnel down
[00:20:58.320 --> 00:20:59.400]   the landscape as well.
[00:20:59.400 --> 00:21:04.000]   So we're also using here another learning rate schedule
[00:21:04.000 --> 00:21:06.360]   and analyzing these resiliency ranges
[00:21:06.360 --> 00:21:09.800]   to see what are the ranges in which we can still take
[00:21:09.800 --> 00:21:13.240]   the learning, the training process to a good solution
[00:21:13.240 --> 00:21:16.680]   while still doing radical changes in the learning rate.
[00:21:16.680 --> 00:21:19.280]   This is now mode connectivity, a wonderful paper,
[00:21:19.280 --> 00:21:22.800]   a collaboration with my friends of NYU and MIT,
[00:21:22.800 --> 00:21:24.440]   Timur Garipov, Pavel Izmailov,
[00:21:24.440 --> 00:21:26.720]   and we have Dimitri Podopodkin, Dimitri Vetrov
[00:21:26.720 --> 00:21:28.440]   and Andrew Gordon-Wilson.
[00:21:28.440 --> 00:21:31.520]   And what these researchers are doing in mode connectivity,
[00:21:31.520 --> 00:21:35.800]   is they get to, for example, three solutions,
[00:21:35.800 --> 00:21:39.760]   three different minima with SGD,
[00:21:39.760 --> 00:21:42.320]   and three points make a plane,
[00:21:42.320 --> 00:21:44.600]   and they take two of these points
[00:21:44.600 --> 00:21:46.080]   and they rotate this plane
[00:21:46.080 --> 00:21:47.920]   through all the high dimensionality
[00:21:47.920 --> 00:21:52.520]   until they find a path that links these two minima
[00:21:52.520 --> 00:21:55.480]   through maybe a Bessier curve or a polygonal path
[00:21:55.480 --> 00:21:58.600]   while maintaining a very low loss value.
[00:21:58.600 --> 00:22:01.720]   And what is really fascinating in the visualization
[00:22:01.720 --> 00:22:04.680]   is that we can see how the barrier that separates
[00:22:04.680 --> 00:22:06.920]   with a high loss value, the different minima,
[00:22:06.920 --> 00:22:09.440]   or it's like melts and disappears gradually
[00:22:09.440 --> 00:22:12.400]   as the training progresses, as the algorithm progresses
[00:22:12.400 --> 00:22:14.080]   until the minima are linked.
[00:22:14.080 --> 00:22:16.760]   And again, this doesn't mean that in the high dimensionality
[00:22:16.760 --> 00:22:19.920]   the barrier between the minima is disappearing,
[00:22:19.920 --> 00:22:21.880]   it means that as we move in these millions
[00:22:21.880 --> 00:22:24.680]   and billions of directions in the high dimensionality,
[00:22:24.680 --> 00:22:28.800]   as we dimensionality to reduce this to our flatland reality,
[00:22:28.800 --> 00:22:30.920]   the counter-intuitive effect that we see
[00:22:30.920 --> 00:22:34.160]   is this barrier melting between the minima.
[00:22:34.160 --> 00:22:37.600]   And we see here, finally, the two minima connected,
[00:22:37.600 --> 00:22:39.320]   and the straight connection
[00:22:39.320 --> 00:22:40.960]   still has a very high loss value,
[00:22:40.960 --> 00:22:42.920]   but the Bessier curve connection
[00:22:42.920 --> 00:22:47.000]   has this very low loss value of connection.
[00:22:47.000 --> 00:22:48.880]   Here we see a static representation
[00:22:48.880 --> 00:22:51.160]   in very high resolution where we see the contrast
[00:22:51.160 --> 00:22:54.640]   between the rough surface in the high loss value areas
[00:22:54.640 --> 00:22:57.600]   and the smoother part that connects the minima.
[00:22:57.600 --> 00:23:02.160]   This is a cenital view of the connection between the minima.
[00:23:02.160 --> 00:23:06.200]   This is also a capture of that dramatic contrast
[00:23:06.200 --> 00:23:10.280]   in the rough surface in the high loss value areas
[00:23:10.280 --> 00:23:13.520]   that are higher than the area that connects the minima.
[00:23:13.520 --> 00:23:16.960]   And this very, very dramatic contrast between both these,
[00:23:16.960 --> 00:23:18.280]   I call the cathedral,
[00:23:18.400 --> 00:23:20.800]   another dramatic representation
[00:23:20.800 --> 00:23:23.120]   of this contrast between both areas.
[00:23:23.120 --> 00:23:26.160]   I see that because of the connection,
[00:23:26.160 --> 00:23:28.720]   the playing of the video is struggling a little bit.
[00:23:28.720 --> 00:23:32.640]   Okay, so the studies in which I change different parameters
[00:23:32.640 --> 00:23:36.080]   and then I capture specific parts of the landscape
[00:23:36.080 --> 00:23:38.880]   to study them as I change different parameters,
[00:23:38.880 --> 00:23:40.800]   drop out, et cetera, et cetera.
[00:23:40.800 --> 00:23:45.400]   And I study various specific parts of the landscape
[00:23:45.400 --> 00:23:46.440]   changing different parameters.
[00:23:46.440 --> 00:23:48.880]   This is an example, for example, with ResNets
[00:23:48.880 --> 00:23:53.040]   as we study the morphology and the dynamics, for example,
[00:23:53.040 --> 00:23:56.160]   with non-skip connections, with skip connections
[00:23:56.160 --> 00:24:00.240]   and analyzing all the changes in morphology and dynamics.
[00:24:00.240 --> 00:24:02.960]   This is the Lost Landscape Library Project.
[00:24:02.960 --> 00:24:07.520]   This is an ongoing project different from the Explorer app.
[00:24:07.520 --> 00:24:11.080]   This is an ongoing project to be able to understand
[00:24:11.080 --> 00:24:15.760]   the impact on the surface of many different gradual changes
[00:24:15.760 --> 00:24:17.360]   in the parameters of the networks.
[00:24:17.360 --> 00:24:19.120]   This is a long-term project.
[00:24:19.120 --> 00:24:22.480]   I don't know when it will be finished.
[00:24:22.480 --> 00:24:24.720]   It is different to the Explorer app
[00:24:24.720 --> 00:24:26.800]   and this is ongoing at the moment.
[00:24:26.800 --> 00:24:29.960]   And yes, this is the Lost Landscape Library Project.
[00:24:29.960 --> 00:24:32.400]   And now we move to the collaborations
[00:24:32.400 --> 00:24:35.560]   with the Landscape Research Deep Learning Group
[00:24:35.560 --> 00:24:37.200]   founded by Deganta Misra
[00:24:37.200 --> 00:24:39.040]   that has now joined Weights & Biases.
[00:24:39.040 --> 00:24:40.400]   So this is very exciting.
[00:24:40.400 --> 00:24:42.680]   And we do, for example, here a collaboration
[00:24:42.680 --> 00:24:47.000]   to study the lost landscapes of activation functions
[00:24:47.000 --> 00:24:49.680]   like MIS, SWISS, and RELU.
[00:24:49.680 --> 00:24:53.280]   Basically, in this study, right,
[00:24:53.280 --> 00:24:57.720]   we are analyzing the 200th epoch of the training process
[00:24:57.720 --> 00:24:59.960]   of a ResNet-20 network.
[00:24:59.960 --> 00:25:02.720]   And we are analyzing how well-conditioned
[00:25:02.720 --> 00:25:06.440]   are the surfaces, what is the quality of their minima,
[00:25:06.440 --> 00:25:08.760]   the flatness of their minima, et cetera, et cetera.
[00:25:08.760 --> 00:25:10.880]   And we are seeing that, for example,
[00:25:10.880 --> 00:25:13.360]   MIS has this very well-conditioned surface
[00:25:13.360 --> 00:25:15.960]   in comparison, for example, with RELU
[00:25:15.960 --> 00:25:18.160]   and the quality of the minima,
[00:25:18.160 --> 00:25:22.400]   the flatness of the minima is really nice, et cetera, et cetera.
[00:25:22.400 --> 00:25:24.400]   And we do these types of collaborations
[00:25:24.400 --> 00:25:26.960]   that we're expanding in different directions.
[00:25:26.960 --> 00:25:27.800]   All right.
[00:25:27.800 --> 00:25:32.320]   Lottery Garden, this is regarding the paper
[00:25:32.320 --> 00:25:33.840]   by Jonathan Frankel and Michael Carvin,
[00:25:33.840 --> 00:25:35.240]   the Lottery Ticket Hypothesis.
[00:25:35.240 --> 00:25:38.000]   And I am pruning gradually the network,
[00:25:38.000 --> 00:25:40.080]   pruning the smallest weights first.
[00:25:40.080 --> 00:25:42.880]   And as I prune the networks throughout six epochs,
[00:25:42.880 --> 00:25:44.520]   I represent the lost landscapes.
[00:25:44.520 --> 00:25:45.480]   And it's really fascinating,
[00:25:45.480 --> 00:25:48.400]   but because we can see that up to 40, 50%,
[00:25:48.400 --> 00:25:51.240]   the performance can even exceed the performance
[00:25:51.240 --> 00:25:52.600]   of the fully trained network.
[00:25:52.600 --> 00:25:54.560]   And the, digamos, the landscapes
[00:25:54.560 --> 00:25:56.560]   are really, really well-conditioned.
[00:25:56.560 --> 00:25:59.160]   And the only when we get to 80 or 90%,
[00:25:59.160 --> 00:26:00.960]   the process begins to collapse.
[00:26:00.960 --> 00:26:02.840]   This is when they break through these planes,
[00:26:02.840 --> 00:26:04.680]   they are going beyond the performance
[00:26:04.680 --> 00:26:06.880]   of the fully trained network.
[00:26:06.880 --> 00:26:09.080]   And we can see, therefore, that this,
[00:26:09.080 --> 00:26:10.800]   indeed, there is a subset of the weights
[00:26:10.800 --> 00:26:14.120]   that are responsible for the most of the performance
[00:26:14.120 --> 00:26:15.680]   of the fully trained network.
[00:26:15.680 --> 00:26:18.840]   And we can see that the landscape is really well-behaved
[00:26:18.840 --> 00:26:21.680]   until we reach the 80s or the 90%.
[00:26:21.680 --> 00:26:25.160]   And here we can see 20% how we exceed the performance
[00:26:25.160 --> 00:26:27.080]   of the fully trained network sometimes
[00:26:27.080 --> 00:26:28.920]   throughout the six epochs, right?
[00:26:28.920 --> 00:26:32.000]   And as we approach 60, 70%,
[00:26:32.000 --> 00:26:34.360]   we see that this tunneling process
[00:26:34.360 --> 00:26:37.880]   begins to be unable to reach as low
[00:26:37.880 --> 00:26:40.040]   until eventually we get to a part
[00:26:40.040 --> 00:26:41.920]   in the high dimensional space
[00:26:41.920 --> 00:26:45.360]   that collapses completely the transformation.
[00:26:45.360 --> 00:26:48.480]   Edge horizon and downfall, as I do these experiments,
[00:26:48.480 --> 00:26:51.120]   I give names to different parts that I'm interested in.
[00:26:51.120 --> 00:26:52.760]   So I call the edge horizon,
[00:26:52.760 --> 00:26:55.600]   the transition to the main convexity of the landscape
[00:26:55.600 --> 00:26:59.600]   and the downfall, the transition from the edge horizon
[00:26:59.600 --> 00:27:01.040]   to the bottom of the minima.
[00:27:01.040 --> 00:27:03.920]   And of course we have the minima, et cetera, et cetera.
[00:27:03.920 --> 00:27:06.400]   And when I do these studies, this is interesting paper
[00:27:06.400 --> 00:27:08.600]   because it tells us, for example, that we used to think
[00:27:08.600 --> 00:27:12.040]   that batch norm was correlated with a smoother surface,
[00:27:12.040 --> 00:27:14.560]   but then they say, oh, but when we do a different combination
[00:27:14.560 --> 00:27:16.040]   of, for example, of depth, et cetera,
[00:27:16.040 --> 00:27:17.040]   we see something different.
[00:27:17.040 --> 00:27:18.640]   And this is the same thing that I have found
[00:27:18.640 --> 00:27:20.200]   in many of my experiments.
[00:27:20.200 --> 00:27:21.840]   It takes a lot of time
[00:27:21.840 --> 00:27:24.480]   because sometimes you get some correlation,
[00:27:24.480 --> 00:27:26.920]   but then with a different combination of parameters,
[00:27:26.920 --> 00:27:28.840]   it goes to a different direction, et cetera.
[00:27:28.840 --> 00:27:32.440]   Okay, this is a very high resolution representation
[00:27:32.440 --> 00:27:36.360]   of an edge horizon, the approach to an edge horizon.
[00:27:36.360 --> 00:27:39.600]   And using also the creative aspect of the project
[00:27:39.600 --> 00:27:44.000]   to make it easier to understand the morphology.
[00:27:44.000 --> 00:27:49.000]   This is a small, sorry, a slow process of tunneling.
[00:27:49.000 --> 00:27:54.720]   Also, that ball represents the minimizer.
[00:27:54.720 --> 00:27:58.080]   This is a transition from the top of an edge horizon
[00:27:58.080 --> 00:28:00.360]   to the bottom below the minima
[00:28:00.360 --> 00:28:02.760]   to study the minima from below.
[00:28:02.760 --> 00:28:05.760]   It's a pity that because of the connection,
[00:28:05.760 --> 00:28:07.760]   I think it's because I'm also recording it,
[00:28:07.760 --> 00:28:09.160]   it's struggling the video,
[00:28:09.160 --> 00:28:12.160]   but we can mount this afterwards.
[00:28:12.160 --> 00:28:16.240]   Okay, this is also a flyby on top of an edge horizon.
[00:28:16.240 --> 00:28:17.080]   And this is one of my favorites.
[00:28:17.080 --> 00:28:19.640]   This is one of the first ones I created.
[00:28:19.640 --> 00:28:21.360]   This is also the beginning
[00:28:21.360 --> 00:28:23.240]   of one of these tunneling processes.
[00:28:23.240 --> 00:28:24.160]   It's one of my favorites
[00:28:24.160 --> 00:28:27.240]   because it's also one of the first ones I produced.
[00:28:27.240 --> 00:28:29.920]   All right, and now we are moving,
[00:28:29.920 --> 00:28:32.160]   okay, this is another static representation
[00:28:32.160 --> 00:28:33.000]   of an edge horizon.
[00:28:33.040 --> 00:28:36.120]   This is a high resolution capture of a downfall.
[00:28:36.120 --> 00:28:38.200]   And we have other representations
[00:28:38.200 --> 00:28:41.080]   of edge horizons as well in here.
[00:28:41.080 --> 00:28:42.120]   All right, where are we going now?
[00:28:42.120 --> 00:28:42.960]   Dropout.
[00:28:42.960 --> 00:28:44.240]   The dropout, as we begin to add
[00:28:44.240 --> 00:28:46.360]   during the training process, of course,
[00:28:46.360 --> 00:28:47.680]   dropout to the networks,
[00:28:47.680 --> 00:28:49.560]   this homogeneous layer of noise
[00:28:49.560 --> 00:28:51.280]   begins to take over the landscape.
[00:28:51.280 --> 00:28:52.720]   And of course, this is a layer of noise
[00:28:52.720 --> 00:28:56.160]   that is disruptive enough to prevent too much overfitting
[00:28:56.160 --> 00:28:58.680]   and too much memorization of the paths
[00:28:58.680 --> 00:28:59.840]   throughout weight space,
[00:28:59.840 --> 00:29:02.560]   but not disruptive enough to prevent the network
[00:29:02.560 --> 00:29:04.200]   from reaching a good solution,
[00:29:04.200 --> 00:29:05.240]   unless, of course,
[00:29:05.240 --> 00:29:07.520]   unless we go too crazy with the dropout.
[00:29:07.520 --> 00:29:10.040]   And then of course we will, you know,
[00:29:10.040 --> 00:29:13.680]   make impossible the movements in weight space.
[00:29:13.680 --> 00:29:16.560]   So yeah, and we see here the representation in movement
[00:29:16.560 --> 00:29:18.360]   as we crank up the dropout
[00:29:18.360 --> 00:29:21.280]   and this layer of noise begins
[00:29:21.280 --> 00:29:23.160]   to take over the landscape.
[00:29:23.160 --> 00:29:24.240]   Fascinating.
[00:29:24.240 --> 00:29:25.840]   So that is the dropout.
[00:29:25.840 --> 00:29:28.240]   And now we're gonna move to Bayesian deep learning,
[00:29:28.240 --> 00:29:29.320]   dealing with uncertainty
[00:29:29.320 --> 00:29:31.440]   so that we don't do overconfident predictions.
[00:29:31.440 --> 00:29:33.840]   And this is the group of researchers
[00:29:33.840 --> 00:29:35.880]   with Wesley Maddox, Timur Garipov,
[00:29:35.880 --> 00:29:37.160]   Pablo Izmailov, Dmitry Vetrov,
[00:29:37.160 --> 00:29:38.440]   and Andrew Gordon-Wilson.
[00:29:38.440 --> 00:29:41.520]   And of course they want to build a probability distribution
[00:29:41.520 --> 00:29:42.680]   of the weights of the network
[00:29:42.680 --> 00:29:44.280]   that they call the posterior, right?
[00:29:44.280 --> 00:29:45.400]   But this is intractable,
[00:29:45.400 --> 00:29:48.840]   so they approximate it through the trajectory of SGD.
[00:29:48.840 --> 00:29:51.480]   So what they do is they get to a good solution in SGD
[00:29:51.480 --> 00:29:53.960]   and then they put a high learning rate
[00:29:53.960 --> 00:29:55.720]   and they explore all around
[00:29:55.720 --> 00:29:57.520]   and they get to different solutions
[00:29:57.520 --> 00:29:58.960]   that explain the training data,
[00:29:58.960 --> 00:30:00.960]   but do different predictions on the test data.
[00:30:00.960 --> 00:30:03.120]   And they build a Gaussian distribution with those
[00:30:03.120 --> 00:30:05.120]   and they demonstrate that that Gaussian distribution
[00:30:05.120 --> 00:30:07.560]   explains the geometry of the loss surface well.
[00:30:07.560 --> 00:30:08.640]   And in this collaboration,
[00:30:08.640 --> 00:30:11.600]   we are representing the actual proper loss landscape
[00:30:11.600 --> 00:30:13.240]   built with the real data
[00:30:13.240 --> 00:30:15.760]   and the exact position of the solutions
[00:30:15.760 --> 00:30:18.640]   and also of the Gaussian distribution as well.
[00:30:18.640 --> 00:30:23.360]   Yeah, I know this was presented in New Ripps 2019.
[00:30:23.360 --> 00:30:26.720]   The previous paper was of New Ripps 2018.
[00:30:26.720 --> 00:30:28.000]   Okay.
[00:30:28.000 --> 00:30:32.520]   And now we go to, I think we're going to go to the GANs.
[00:30:32.520 --> 00:30:35.080]   Yes, the wild networks.
[00:30:35.080 --> 00:30:37.320]   So we all know what is the problem with the GANs,
[00:30:37.320 --> 00:30:40.880]   that is that the loss function does not correlate well
[00:30:40.880 --> 00:30:43.960]   with the performance, for example, of the generator
[00:30:43.960 --> 00:30:48.520]   or the quality of the images that it produces.
[00:30:48.520 --> 00:30:50.360]   But if we use a different type of loss function,
[00:30:50.360 --> 00:30:51.920]   like for example, the Wasserstein,
[00:30:51.920 --> 00:30:53.680]   a different type of GAN, the Wasserstein GAN
[00:30:53.680 --> 00:30:55.480]   with a different type of loss function
[00:30:55.480 --> 00:30:57.160]   that uses the Wasserstein distance,
[00:30:57.160 --> 00:30:59.600]   the shortest distance to move a probability mass
[00:30:59.600 --> 00:31:01.080]   from a distribution to another,
[00:31:01.080 --> 00:31:04.320]   then just that type of loss function correlates much better
[00:31:04.320 --> 00:31:07.080]   with the performance of the generator.
[00:31:07.080 --> 00:31:10.120]   And then we can study better the dynamics
[00:31:10.120 --> 00:31:13.200]   of the loss surface of a generator.
[00:31:13.200 --> 00:31:17.440]   And we can see this more unpredictable dynamics.
[00:31:17.440 --> 00:31:19.360]   We can really see that these networks
[00:31:19.360 --> 00:31:21.880]   are so much harder to control,
[00:31:21.880 --> 00:31:25.760]   so much harder to tame as we study the dynamics,
[00:31:25.760 --> 00:31:29.000]   for example, of the surfaces of the generator.
[00:31:29.000 --> 00:31:31.360]   And this is a project with a neural concept,
[00:31:31.360 --> 00:31:34.000]   Swiss company about geometric deep learning.
[00:31:34.000 --> 00:31:35.080]   This is not lost landscape,
[00:31:35.080 --> 00:31:36.520]   it's just to show you that the project
[00:31:36.520 --> 00:31:39.080]   is also branching in different directions.
[00:31:39.080 --> 00:31:41.480]   And this is basically predicting
[00:31:41.480 --> 00:31:44.200]   the aerodynamic properties of a drone,
[00:31:44.200 --> 00:31:48.000]   the pressure exerted by the air on the surface of the drone.
[00:31:48.000 --> 00:31:52.120]   And we can also visualize the features being learned
[00:31:52.120 --> 00:31:55.480]   by the filters, which in a normal ConvNet,
[00:31:55.480 --> 00:31:57.840]   we would visualize with a 2D image,
[00:31:57.840 --> 00:32:00.760]   but in a geometric 3D ConvNet,
[00:32:00.760 --> 00:32:02.920]   we can map on top of the surface.
[00:32:02.920 --> 00:32:04.560]   And finally, to begin this,
[00:32:04.560 --> 00:32:06.400]   sorry, to end this presentation,
[00:32:06.400 --> 00:32:09.160]   this is the Lost Landscape Explorer app
[00:32:09.160 --> 00:32:11.080]   that I just launched a few days ago.
[00:32:11.080 --> 00:32:14.120]   And this is an app where you can explore
[00:32:14.120 --> 00:32:17.200]   different lost landscapes created with this dimensionality
[00:32:17.200 --> 00:32:19.720]   reduced techniques and with real data,
[00:32:19.720 --> 00:32:22.760]   and you can navigate them in 3D.
[00:32:22.760 --> 00:32:24.120]   And you have different features,
[00:32:24.120 --> 00:32:26.320]   like for example, this auto descent mode,
[00:32:26.320 --> 00:32:28.000]   you can click anywhere on the landscape
[00:32:28.000 --> 00:32:31.560]   to begin a sort of descent through the gradients
[00:32:31.560 --> 00:32:33.680]   or the sub gradients as you like.
[00:32:33.680 --> 00:32:36.680]   And of course, this is not doing a gradient descent
[00:32:36.680 --> 00:32:38.400]   in the high dimensional space, all right?
[00:32:38.400 --> 00:32:39.480]   This is a gradient descent
[00:32:39.480 --> 00:32:42.320]   through the dimensionality reduced representation,
[00:32:42.320 --> 00:32:45.040]   but it can be useful for educational purposes
[00:32:45.040 --> 00:32:48.520]   and also to reflect on different research aspects.
[00:32:48.520 --> 00:32:50.760]   For example, it's very fun that you can do
[00:32:50.760 --> 00:32:53.640]   one of these descents and you can see that it gets trapped,
[00:32:53.640 --> 00:32:56.160]   in some part of the landscape in a local minima.
[00:32:56.160 --> 00:32:59.200]   And then you have a descent rate control
[00:32:59.200 --> 00:33:01.240]   that you can manipulate in real time.
[00:33:01.240 --> 00:33:04.200]   And you can crank it up and help it escape
[00:33:04.200 --> 00:33:06.800]   that local minima and continue moving down,
[00:33:06.800 --> 00:33:09.120]   or you can get to a good minima.
[00:33:09.120 --> 00:33:13.680]   And then you can, you know, the gradient gets almost zero,
[00:33:13.680 --> 00:33:15.880]   but then you crank up the learning rate
[00:33:15.880 --> 00:33:17.240]   at the bottom of the minima,
[00:33:17.240 --> 00:33:20.400]   and then you explore the minima and get to other solutions,
[00:33:20.400 --> 00:33:23.560]   which is what the Bayesian friends are doing, for example.
[00:33:23.560 --> 00:33:26.120]   So, you know, you can reflect on different research aspects
[00:33:26.120 --> 00:33:27.840]   and use it for educational purposes.
[00:33:27.840 --> 00:33:31.600]   And I will be adding more features over time to this tool.
[00:33:31.600 --> 00:33:35.600]   And you can also capture different views of the landscapes,
[00:33:35.600 --> 00:33:38.240]   you know, in PNGs, et cetera, et cetera,
[00:33:38.240 --> 00:33:40.120]   there are other features.
[00:33:40.120 --> 00:33:43.840]   And this can be accessed for free, completely open,
[00:33:43.840 --> 00:33:45.640]   and you don't have to install anything.
[00:33:45.640 --> 00:33:47.840]   This, I programmed this in JavaScript.
[00:33:47.840 --> 00:33:51.520]   This is using a React, this is using WebGL technology,
[00:33:51.520 --> 00:33:53.240]   and it can be installed.
[00:33:53.240 --> 00:33:54.680]   This is a progressive web app,
[00:33:54.680 --> 00:33:55.880]   so it can be installed anywhere.
[00:33:55.880 --> 00:33:58.680]   It doesn't require logging or register or anything.
[00:33:58.680 --> 00:34:00.840]   And I will be adding more features to it over time,
[00:34:00.840 --> 00:34:05.840]   and this can be accessed at lostlandscape.com/explorer.
[00:34:05.840 --> 00:34:08.600]   And that's it, that concludes the presentation.
[00:34:08.600 --> 00:34:11.000]   And if you want to explore more of my work,
[00:34:11.000 --> 00:34:15.040]   you can go to ideami.com, and that's it.
[00:34:15.040 --> 00:34:15.960]   Thank you very much.
[00:34:15.960 --> 00:34:20.120]   And now we can open time for questions and dialogue.
[00:34:20.120 --> 00:34:20.960]   Let's go for it.
[00:34:21.600 --> 00:34:25.600]   - Great, yeah, thanks, Javier, for the, like,
[00:34:25.600 --> 00:34:28.760]   yeah, such a wide array of experiments that you've done
[00:34:28.760 --> 00:34:32.680]   using these tools to understand lost landscapes better.
[00:34:32.680 --> 00:34:35.960]   It's really just, yeah, the breadth is really impressive.
[00:34:35.960 --> 00:34:37.720]   - Right.
[00:34:37.720 --> 00:34:39.880]   - So I wanted to like dive in a little bit
[00:34:39.880 --> 00:34:42.400]   and hear maybe a little bit more about some of those,
[00:34:42.400 --> 00:34:43.400]   like some of those experiments
[00:34:43.400 --> 00:34:45.600]   and what you feel like you learned from them.
[00:34:45.600 --> 00:34:46.440]   - Yes.
[00:34:46.440 --> 00:34:48.640]   - So one, I think,
[00:34:48.640 --> 00:34:53.640]   so one example, one thing you talked about was,
[00:34:53.640 --> 00:34:55.960]   you talked about using LR schedulers
[00:34:55.960 --> 00:34:59.680]   and like, and so changing the learning rate
[00:34:59.680 --> 00:35:03.400]   as you were going and how that affected the, like,
[00:35:03.400 --> 00:35:05.560]   the sorts of places on the lost landscape
[00:35:05.560 --> 00:35:07.200]   that the optimization ended up.
[00:35:07.200 --> 00:35:12.000]   So like my view of LR schedulers is that, you know,
[00:35:12.000 --> 00:35:15.000]   they're kind of there to like let you optimize for a while
[00:35:15.000 --> 00:35:17.840]   and then kick you out of where you've been optimized to
[00:35:17.840 --> 00:35:20.480]   and let you find another, you know, another place to go.
[00:35:20.480 --> 00:35:22.760]   Is that something that you see in the lost landscapes
[00:35:22.760 --> 00:35:26.080]   or is there a different intuition about these LR schedulers
[00:35:26.080 --> 00:35:28.640]   that you gain from using them with this tool?
[00:35:28.640 --> 00:35:30.440]   - Yeah, I mean, that point that you talk about,
[00:35:30.440 --> 00:35:31.760]   I absolutely agree.
[00:35:31.760 --> 00:35:36.480]   I think, you know, what I mainly see is that,
[00:35:36.480 --> 00:35:38.120]   it's again, the resiliency,
[00:35:38.120 --> 00:35:41.080]   this unreasonable effectiveness of deep learner, right?
[00:35:41.080 --> 00:35:45.160]   I mean, the resiliency that the training process has
[00:35:45.160 --> 00:35:46.680]   when you try, you know,
[00:35:46.680 --> 00:35:49.000]   when you try different resiliency ranges,
[00:35:49.000 --> 00:35:51.400]   you can see that the training process
[00:35:51.400 --> 00:35:53.840]   in these deep learning neural networks
[00:35:53.840 --> 00:35:55.160]   is really resilient.
[00:35:55.160 --> 00:35:59.840]   It can really recover many times from movements
[00:35:59.840 --> 00:36:04.000]   produced by these, you know, learning rates schedules
[00:36:04.000 --> 00:36:06.680]   that may take you to some, you know,
[00:36:06.680 --> 00:36:09.200]   they may interfere a bit with the training process,
[00:36:09.200 --> 00:36:11.600]   but eventually you can return to a path
[00:36:11.600 --> 00:36:14.200]   that takes you to a good minima and a good solution.
[00:36:14.200 --> 00:36:16.640]   So this is the main thing that for me connects again
[00:36:16.640 --> 00:36:18.800]   with the blessings of dimensionality,
[00:36:18.800 --> 00:36:23.080]   that there seems to be a lot of resilience in the process,
[00:36:23.080 --> 00:36:26.280]   that even when you try to push, you know,
[00:36:26.280 --> 00:36:29.600]   the learning rate in crazy ways, of course, yes,
[00:36:29.600 --> 00:36:32.200]   you can, I mean, you can break the process easily
[00:36:32.200 --> 00:36:34.520]   if you do that, but there is, you know,
[00:36:34.520 --> 00:36:38.600]   there is kind of a margin of error that in my opinion
[00:36:38.600 --> 00:36:43.440]   is larger than it would be, you know, reasonable to assume,
[00:36:43.440 --> 00:36:45.040]   you know, and this is a little bit what I find.
[00:36:45.040 --> 00:36:46.200]   Of course, if you wanna break it down,
[00:36:46.200 --> 00:36:47.440]   you can break it down very easily,
[00:36:47.440 --> 00:36:50.200]   but there is still basically my intuition
[00:36:50.200 --> 00:36:51.880]   that I get from this exploration
[00:36:51.880 --> 00:36:54.680]   is that the margin of error again that we have
[00:36:54.680 --> 00:36:57.800]   is really wider than one would expect, yeah.
[00:36:57.800 --> 00:37:00.400]   - That's an interesting perspective.
[00:37:00.400 --> 00:37:02.480]   Yeah, it seems it's, so I've,
[00:37:02.480 --> 00:37:04.080]   some of the work that I've done involved
[00:37:04.080 --> 00:37:06.240]   second order optimizers applied to neural networks.
[00:37:06.240 --> 00:37:08.440]   So building Hessians, trying to invert them
[00:37:08.440 --> 00:37:09.520]   and use that information.
[00:37:09.520 --> 00:37:12.120]   And I found like in that instance,
[00:37:12.120 --> 00:37:14.760]   those optimizers tend not to be very robust
[00:37:14.760 --> 00:37:19.520]   either to like, you know, noise changes in the loss surface
[00:37:19.520 --> 00:37:21.440]   and also bugs.
[00:37:21.440 --> 00:37:24.280]   So if you implement gradient descent in a buggy way,
[00:37:24.280 --> 00:37:26.400]   it actually tends to still work decently well.
[00:37:26.400 --> 00:37:28.680]   If you implement one of these second order optimizers
[00:37:28.680 --> 00:37:30.720]   in a buggy way, it tends to do,
[00:37:30.720 --> 00:37:33.880]   it tends to be wet hot garbage instead.
[00:37:33.880 --> 00:37:36.840]   - Well, I mean, that's one of the things
[00:37:36.840 --> 00:37:40.360]   that a lot of people say about gradient descent, right?
[00:37:40.360 --> 00:37:44.240]   That is very slow, but it's kind of the thing that,
[00:37:44.240 --> 00:37:47.400]   you know, it's really a trustworthy in that sense, right?
[00:37:47.400 --> 00:37:51.080]   - Yeah, definitely.
[00:37:51.080 --> 00:37:53.320]   And, you know, I mean, it's,
[00:37:53.320 --> 00:37:55.920]   you can get faster things than gradient descent
[00:37:55.920 --> 00:37:58.880]   in a lot of like specific example problems,
[00:37:58.880 --> 00:38:02.440]   but it's really hard to beat on neural networks.
[00:38:02.440 --> 00:38:04.160]   - Exactly, exactly.
[00:38:04.160 --> 00:38:09.000]   - Yeah, I guess another question, sort of like,
[00:38:09.000 --> 00:38:12.040]   so we normally see these like one dimensional slices
[00:38:12.040 --> 00:38:13.680]   through the loss surface that are just,
[00:38:13.680 --> 00:38:16.760]   we calculate the loss on each batch as we're going
[00:38:16.760 --> 00:38:19.520]   and we get a single value as we go through these,
[00:38:19.520 --> 00:38:24.880]   yeah, as we like load batches,
[00:38:24.880 --> 00:38:26.800]   calculate loss, update our gradients.
[00:38:26.800 --> 00:38:29.160]   And you pretty much always see something
[00:38:29.160 --> 00:38:30.960]   that just looks like this nice,
[00:38:30.960 --> 00:38:35.360]   like this nice curve, the exact curve you would expect
[00:38:35.400 --> 00:38:39.360]   from doing say gradient descent on a, like a perfect bowl,
[00:38:39.360 --> 00:38:41.960]   just like a quadratic loss surface.
[00:38:41.960 --> 00:38:45.120]   So what do you think is going on there?
[00:38:45.120 --> 00:38:47.440]   Like why, you know,
[00:38:47.440 --> 00:38:49.520]   'cause it seems like there's a lot of richness
[00:38:49.520 --> 00:38:52.600]   in the landscapes that you see
[00:38:52.600 --> 00:38:54.240]   in the loss landscapes project,
[00:38:54.240 --> 00:38:56.880]   but then you don't see that
[00:38:56.880 --> 00:39:00.680]   in the one dimensional slice that we get during training.
[00:39:00.680 --> 00:39:04.560]   - Well, I mean, it really depends.
[00:39:04.560 --> 00:39:06.880]   I mean, as for example,
[00:39:06.880 --> 00:39:09.840]   Tom Goldstein's team's papers have shown
[00:39:09.840 --> 00:39:13.360]   all these richness of non-convexities and convexities
[00:39:13.360 --> 00:39:15.880]   depend a lot on how you design the network.
[00:39:15.880 --> 00:39:19.480]   I mean, if you have an architecture that is, of course,
[00:39:19.480 --> 00:39:21.640]   you know, very optimized with the skip connections,
[00:39:21.640 --> 00:39:24.040]   like in a ResNet, you get this perfect bowl,
[00:39:24.040 --> 00:39:25.720]   like massively smooth,
[00:39:25.720 --> 00:39:28.720]   but as you begin to play with different parameters
[00:39:28.720 --> 00:39:31.440]   and you get to more challenging configurations,
[00:39:31.440 --> 00:39:34.560]   then you get, you know, way more richness in the landscape.
[00:39:34.560 --> 00:39:37.200]   Now, way more richness of distributions
[00:39:37.200 --> 00:39:39.200]   of non-convexities and convexities.
[00:39:39.200 --> 00:39:42.120]   And for example, when we did, you know,
[00:39:42.120 --> 00:39:47.120]   the collaboration with, in the mode connectivity paper,
[00:39:47.120 --> 00:39:49.960]   at the beginning, this is also another interesting thing.
[00:39:49.960 --> 00:39:50.920]   At the beginning,
[00:39:50.920 --> 00:39:56.760]   we did the dimensionality reduction to show the two minima
[00:39:56.760 --> 00:40:01.600]   and we were looking on a certain range around the two minima
[00:40:01.600 --> 00:40:03.800]   and we could just see,
[00:40:03.800 --> 00:40:07.320]   let's say, what is the two, you know,
[00:40:07.320 --> 00:40:09.440]   bowls that go down to the two minima
[00:40:09.440 --> 00:40:12.200]   and, you know, pretty smooth and pretty well behaved.
[00:40:12.200 --> 00:40:15.360]   And then we said, let's look further away.
[00:40:15.360 --> 00:40:19.160]   So we began to expand the exploration
[00:40:19.160 --> 00:40:22.960]   and we began to do the calculations on a larger range.
[00:40:22.960 --> 00:40:25.400]   And as we started to move on a larger range,
[00:40:25.400 --> 00:40:28.560]   we begin to find this area that is around
[00:40:28.560 --> 00:40:30.440]   that has a higher loss value
[00:40:30.440 --> 00:40:32.800]   and is really, really, really rough
[00:40:32.800 --> 00:40:36.200]   and full of all of these places where you can get trapped.
[00:40:36.200 --> 00:40:38.720]   So many times it's also, Charles,
[00:40:38.720 --> 00:40:41.320]   about the range that you're exploring, okay?
[00:40:41.320 --> 00:40:42.560]   I mean, it has to do, again,
[00:40:42.560 --> 00:40:45.120]   it has to do with the type of slides and directions
[00:40:45.120 --> 00:40:46.960]   and also with the range that you're exploring.
[00:40:46.960 --> 00:40:50.760]   As I said during the talk, if you use PCA directions
[00:40:50.760 --> 00:40:53.200]   and a lot of people are, you know,
[00:40:53.200 --> 00:40:55.640]   creating these representations with PCA directions,
[00:40:55.640 --> 00:40:58.920]   then you are typically going to see just the bowl
[00:40:58.920 --> 00:41:02.000]   and very well behaved because you are just slicing
[00:41:02.000 --> 00:41:05.280]   through the most optimized directions down the gradient.
[00:41:05.280 --> 00:41:07.120]   And you're just gonna see that, you know,
[00:41:07.120 --> 00:41:09.840]   very, very bowl, very optimized bowl of the landscape.
[00:41:09.840 --> 00:41:11.440]   And that, you know, a lot of people
[00:41:11.440 --> 00:41:12.760]   are doing representations like that
[00:41:12.760 --> 00:41:13.760]   and that's what they see.
[00:41:13.760 --> 00:41:15.080]   So that's one part of it.
[00:41:15.080 --> 00:41:17.560]   But the other part of it is range, okay?
[00:41:17.560 --> 00:41:20.320]   It depends then when you build a representation,
[00:41:20.320 --> 00:41:23.920]   how much range in magnitude are you calculating
[00:41:23.920 --> 00:41:26.480]   to visualize around the starting point?
[00:41:26.480 --> 00:41:28.560]   And as you move further away,
[00:41:28.560 --> 00:41:32.040]   as we did in the mode connectivity paper collaboration,
[00:41:32.040 --> 00:41:34.640]   you begin to see rougher areas
[00:41:34.640 --> 00:41:39.240]   that many times surround the central area of convexity.
[00:41:39.240 --> 00:41:40.080]   Yeah.
[00:41:40.080 --> 00:41:40.920]   - Mm-hmm.
[00:41:40.920 --> 00:41:43.040]   And actually, technical clarifying points.
[00:41:43.040 --> 00:41:46.040]   When you make these lost landscape visualizations,
[00:41:46.040 --> 00:41:48.920]   how many data examples are you using?
[00:41:48.920 --> 00:41:52.680]   Like, is it more like on the order of a single batch
[00:41:52.680 --> 00:41:55.160]   or is it more like on the order of most of the dataset
[00:41:55.160 --> 00:41:56.480]   or all of the dataset?
[00:41:56.480 --> 00:41:58.200]   - Yeah, that is a great question.
[00:41:58.200 --> 00:41:59.040]   That is a great question.
[00:41:59.040 --> 00:41:59.880]   So it depends.
[00:41:59.880 --> 00:42:00.720]   It depends.
[00:42:00.720 --> 00:42:03.320]   Sometimes it's all of the data, sometimes it's a subset.
[00:42:03.320 --> 00:42:06.280]   So, you know, researchers have demonstrated
[00:42:06.280 --> 00:42:11.280]   that if you use a decent percentage of the whole data,
[00:42:11.280 --> 00:42:13.400]   the results are gonna be very similar.
[00:42:13.400 --> 00:42:15.440]   So sometimes it's the whole data
[00:42:15.440 --> 00:42:17.000]   and sometimes it's a subset,
[00:42:17.000 --> 00:42:19.800]   but it cannot be, of course, a very small part of the data.
[00:42:19.800 --> 00:42:21.800]   It has to be a decent subset.
[00:42:21.800 --> 00:42:22.640]   - I see.
[00:42:22.640 --> 00:42:25.840]   So the roughness you're seeing isn't roughness
[00:42:25.840 --> 00:42:28.200]   you would expect to vary batch to batch, right?
[00:42:28.200 --> 00:42:30.640]   That is like, you're looking at something
[00:42:30.640 --> 00:42:32.920]   that's pretty close to what the average value
[00:42:32.920 --> 00:42:35.240]   would be across batches.
[00:42:35.240 --> 00:42:39.920]   - I mean, I don't think it's so much related to that.
[00:42:39.920 --> 00:42:42.120]   Again, I mean, the roughness that I'm seeing,
[00:42:42.120 --> 00:42:43.320]   I mean, the roughness that I'm seeing,
[00:42:43.320 --> 00:42:45.560]   like we see again in these, you know,
[00:42:45.560 --> 00:42:49.040]   areas that surround the two minima in that example,
[00:42:49.040 --> 00:42:53.640]   for example, it's just a property of, you know,
[00:42:53.640 --> 00:42:56.760]   the other areas of the weight space
[00:42:56.760 --> 00:42:59.400]   that take you to higher loss values.
[00:42:59.400 --> 00:43:00.720]   So this is the roughness.
[00:43:00.720 --> 00:43:04.960]   For example, the variation in the dynamics
[00:43:04.960 --> 00:43:08.600]   that produce variations in the surface of the landscape,
[00:43:08.600 --> 00:43:11.400]   those, for example, are correlated with the batch sizes.
[00:43:11.400 --> 00:43:13.080]   If you are doing these calculations,
[00:43:13.080 --> 00:43:15.200]   because wait a minute, just to clarify,
[00:43:15.200 --> 00:43:17.200]   we have to differentiate two things here.
[00:43:17.200 --> 00:43:21.760]   One thing is as we capture the data,
[00:43:21.760 --> 00:43:25.040]   what is the batch size we're capturing through the dynamics?
[00:43:25.040 --> 00:43:27.120]   And another thing is as we're calculating
[00:43:27.120 --> 00:43:28.240]   the landscape itself,
[00:43:28.240 --> 00:43:30.960]   how much percentage of the data we're using, okay?
[00:43:30.960 --> 00:43:32.880]   Because for example, in the dynamics,
[00:43:32.880 --> 00:43:35.040]   if we use a very small batch size,
[00:43:35.040 --> 00:43:37.400]   then we're gonna be using a different proportion
[00:43:37.400 --> 00:43:39.240]   of the data in different steps.
[00:43:39.240 --> 00:43:42.520]   And this is gonna produce also more dramatic variations
[00:43:42.520 --> 00:43:43.600]   in the dynamics, okay?
[00:43:43.600 --> 00:43:45.920]   So both of these are different aspects, yeah.
[00:43:45.920 --> 00:43:48.040]   - Right, right.
[00:43:48.040 --> 00:43:49.920]   I guess my mental model a little bit
[00:43:49.920 --> 00:43:51.840]   when I was looking at some of those rough landscapes
[00:43:51.840 --> 00:43:55.080]   was that that was, oh, that's what I would see on a single,
[00:43:55.080 --> 00:43:56.720]   on, you know, one batch at a time.
[00:43:56.720 --> 00:43:59.280]   Maybe I would see like this very rough thing.
[00:43:59.280 --> 00:44:00.440]   Then on another batch,
[00:44:00.440 --> 00:44:02.720]   I would see something that was also rough,
[00:44:02.720 --> 00:44:04.360]   but maybe had a pattern
[00:44:04.360 --> 00:44:06.640]   that would mostly like interfere with that one.
[00:44:06.640 --> 00:44:08.640]   So they would be high where the other one was low
[00:44:08.640 --> 00:44:11.520]   and just sort of be kind of orthogonal to each other.
[00:44:11.520 --> 00:44:13.800]   But it sounds like that roughness, as you said,
[00:44:13.800 --> 00:44:16.920]   is an inherent property of the loss surface that's-
[00:44:16.920 --> 00:44:19.760]   - Yeah, because, I mean, when you're talking,
[00:44:19.760 --> 00:44:21.320]   when you're talking, Charles, now,
[00:44:21.320 --> 00:44:23.640]   about the difference between one batch and another batch,
[00:44:23.640 --> 00:44:25.440]   are you talking about the dynamics?
[00:44:25.440 --> 00:44:28.600]   I mean, the sequence to the steps, right?
[00:44:28.600 --> 00:44:29.920]   You are not talking, I mean,
[00:44:29.920 --> 00:44:32.360]   you are not talking about the static representation
[00:44:32.360 --> 00:44:34.000]   or which one are you talking about?
[00:44:34.000 --> 00:44:36.480]   - I'm thinking about the static representations here,
[00:44:36.480 --> 00:44:38.960]   not that one batch size might see something different
[00:44:38.960 --> 00:44:42.120]   than another in terms of, yeah, where it goes.
[00:44:42.120 --> 00:44:43.240]   - Well, of course, of course.
[00:44:43.240 --> 00:44:46.240]   I mean, if you build the same representation,
[00:44:46.240 --> 00:44:48.680]   yeah, if you build the same representation
[00:44:48.680 --> 00:44:52.000]   and you used a different proportion of the data
[00:44:52.000 --> 00:44:54.560]   on different repetitions,
[00:44:54.560 --> 00:44:56.160]   yes, you would see differences.
[00:44:56.160 --> 00:44:59.040]   But on the very same representation,
[00:44:59.040 --> 00:45:01.640]   if you are consistent with the data that you are using,
[00:45:01.640 --> 00:45:03.960]   then no, then basically what you are seeing
[00:45:03.960 --> 00:45:06.960]   is the actual property of the difference,
[00:45:07.280 --> 00:45:10.480]   that is the actual geometry that is being produced
[00:45:10.480 --> 00:45:13.080]   because of the difference in the loss values.
[00:45:13.080 --> 00:45:14.840]   - But then with the dropout section
[00:45:14.840 --> 00:45:17.840]   where you saw that really rough loft surface,
[00:45:17.840 --> 00:45:20.520]   that was with a fixed dropout mask, right?
[00:45:20.520 --> 00:45:23.320]   That was like a fixed set of nodes have been set to zero
[00:45:23.320 --> 00:45:25.960]   through all the calculations.
[00:45:25.960 --> 00:45:27.200]   - Which one specifically?
[00:45:27.200 --> 00:45:30.360]   Was that one of the static ones
[00:45:30.360 --> 00:45:31.960]   or was that the one in movement?
[00:45:31.960 --> 00:45:34.480]   - Yeah, one of the static.
[00:45:34.480 --> 00:45:36.640]   So just a static one from this section
[00:45:36.640 --> 00:45:38.120]   where you showed that the dropout,
[00:45:38.120 --> 00:45:41.480]   the loss surface for dropout had all those like punctate,
[00:45:41.480 --> 00:45:45.360]   like sort of peaks and tiny little divots.
[00:45:45.360 --> 00:45:46.240]   - Yeah, yeah, yeah, exactly.
[00:45:46.240 --> 00:45:47.240]   That one, that one, I mean,
[00:45:47.240 --> 00:45:50.960]   each of those ones has a specific fixed dropout value.
[00:45:50.960 --> 00:45:52.440]   Exactly.
[00:45:52.440 --> 00:45:53.280]   - I see.
[00:45:53.280 --> 00:45:55.640]   So that one, so then when you showed them in motion,
[00:45:55.640 --> 00:45:58.160]   then the dropout masks were changing over time.
[00:45:58.160 --> 00:46:00.760]   - Yes, because if you look at that video
[00:46:00.760 --> 00:46:02.360]   and we could, I mean, we could take a look
[00:46:02.360 --> 00:46:04.320]   or you can take, or we can take a look later.
[00:46:04.320 --> 00:46:05.840]   But basically in that video,
[00:46:05.840 --> 00:46:08.880]   I am gradually cranking up the dropout value.
[00:46:08.880 --> 00:46:11.440]   Okay, so that is a training process
[00:46:11.440 --> 00:46:14.560]   in which I begin with a very, very small dropout value
[00:46:14.560 --> 00:46:16.800]   and I gradually increase it.
[00:46:16.800 --> 00:46:18.120]   Okay. - I see.
[00:46:18.120 --> 00:46:19.560]   - Yeah, and as I increase it,
[00:46:19.560 --> 00:46:22.000]   that layer of noise increases as well.
[00:46:22.000 --> 00:46:24.400]   - I see.
[00:46:24.400 --> 00:46:27.240]   So that roughness, the roughness that came from the dropout,
[00:46:27.240 --> 00:46:29.920]   like that would, you know, that's gonna vary definitely
[00:46:29.920 --> 00:46:33.600]   and sort of be averaged out as you go across lots of masks.
[00:46:33.600 --> 00:46:37.560]   But the roughness from that you showed in the, you know,
[00:46:37.560 --> 00:46:39.760]   in all the rest of them without dropout,
[00:46:39.760 --> 00:46:42.320]   that roughness is something that's, you know,
[00:46:42.320 --> 00:46:45.160]   that's gonna stick around from batch to batch.
[00:46:45.160 --> 00:46:46.000]   - Yes, yes.
[00:46:46.000 --> 00:46:48.320]   If you are consistent with the data that you use,
[00:46:48.320 --> 00:46:49.400]   yes, absolutely.
[00:46:49.400 --> 00:46:52.520]   Yeah, if you used a proportion of the data
[00:46:52.520 --> 00:46:55.080]   and you change that proportion between the representations,
[00:46:55.080 --> 00:46:57.400]   then it would be different, but you have to be consistent.
[00:46:57.400 --> 00:46:58.240]   We have to be consistent.
[00:46:58.240 --> 00:47:00.800]   So if we're always consistent, you know,
[00:47:00.800 --> 00:47:02.160]   on every presentation,
[00:47:02.200 --> 00:47:04.840]   then that's what you get everywhere in the landscape
[00:47:04.840 --> 00:47:06.040]   is consistent.
[00:47:06.040 --> 00:47:06.880]   Yeah.
[00:47:06.880 --> 00:47:08.800]   Yeah.
[00:47:08.800 --> 00:47:11.560]   - Interesting. - Definitely, definitely.
[00:47:11.560 --> 00:47:15.720]   - So I guess the part of the loss,
[00:47:15.720 --> 00:47:17.160]   so the feature of lost landscapes
[00:47:17.160 --> 00:47:20.560]   that I ended up studying in my dissertation
[00:47:20.560 --> 00:47:24.240]   was this very particular thing that we noticed,
[00:47:24.240 --> 00:47:27.240]   which was that very often we would find,
[00:47:27.240 --> 00:47:31.520]   we would find regions where the gradient
[00:47:31.520 --> 00:47:33.200]   was directly in the,
[00:47:33.200 --> 00:47:36.760]   it was like lied directly in the null space of the Hessian.
[00:47:36.760 --> 00:47:38.400]   So in the direction of the gradient,
[00:47:38.400 --> 00:47:40.120]   just looking in that direction,
[00:47:40.120 --> 00:47:43.640]   what that effectively meant was that the lost landscape was,
[00:47:43.640 --> 00:47:45.240]   I had zero curvature.
[00:47:45.240 --> 00:47:49.000]   - Okay, zero curvature?
[00:47:49.000 --> 00:47:52.000]   - Zero curvature. - So the most critical point.
[00:47:52.000 --> 00:47:56.240]   - So non-zero gradient,
[00:47:56.240 --> 00:48:00.480]   but zero curvature is what these points were.
[00:48:00.480 --> 00:48:01.320]   - Okay.
[00:48:01.320 --> 00:48:02.760]   - Zero curvature in this direction.
[00:48:02.760 --> 00:48:06.160]   So, you know, you could point along a principle,
[00:48:06.160 --> 00:48:08.800]   you know, principle eigenvalue direction that's positive,
[00:48:08.800 --> 00:48:10.560]   you'd point along one that's negative.
[00:48:10.560 --> 00:48:11.680]   And in one case, you know,
[00:48:11.680 --> 00:48:14.320]   the loss surface would curve up in that direction.
[00:48:14.320 --> 00:48:17.440]   In the other case, it would curve down in that direction.
[00:48:17.440 --> 00:48:20.120]   And in these directions, there's no curvature at all.
[00:48:20.120 --> 00:48:23.200]   So you just get something that looks like a,
[00:48:23.200 --> 00:48:27.080]   you know, like a plane in that direction.
[00:48:27.080 --> 00:48:29.680]   So I guess I'm kind of curious,
[00:48:29.680 --> 00:48:31.880]   and it's also a well-known fact
[00:48:31.880 --> 00:48:35.880]   about rectified linear networks that in principle,
[00:48:35.880 --> 00:48:37.920]   if you don't change the parameters too much,
[00:48:37.920 --> 00:48:40.920]   they should behave like a polynomial.
[00:48:40.920 --> 00:48:44.920]   So there's these sort of like,
[00:48:44.920 --> 00:48:47.960]   there's these ideas that there should be some simple shapes.
[00:48:47.960 --> 00:48:51.240]   And I'm curious if you've ever seen sort of any regions
[00:48:51.240 --> 00:48:53.560]   in these lost landscape visualizations
[00:48:53.560 --> 00:48:56.320]   that look like a region that's, you know,
[00:48:56.320 --> 00:48:58.520]   like a perfect quadratic bowl,
[00:48:58.520 --> 00:49:01.400]   or that looks like a completely flat plane.
[00:49:01.400 --> 00:49:03.480]   - That's interesting.
[00:49:03.480 --> 00:49:05.240]   That's interesting.
[00:49:05.240 --> 00:49:10.040]   A completely flat plane, I don't think so.
[00:49:10.040 --> 00:49:11.560]   Not a completely flat plane.
[00:49:11.560 --> 00:49:15.040]   I mean, the perfect bowl, the perfect bowl,
[00:49:15.040 --> 00:49:16.400]   definitely, these many, many times,
[00:49:16.400 --> 00:49:17.760]   but not the perfect plane.
[00:49:17.760 --> 00:49:19.600]   Not, sorry, not like, you know,
[00:49:19.600 --> 00:49:21.240]   like a proper plane like that.
[00:49:21.240 --> 00:49:22.160]   No, not like that.
[00:49:22.160 --> 00:49:23.480]   No, definitely not.
[00:49:23.480 --> 00:49:26.200]   - Yeah, my suspicion is that like fundamentally
[00:49:26.200 --> 00:49:27.640]   this geometric aspect here
[00:49:27.640 --> 00:49:29.720]   these like sort of flat regions is,
[00:49:29.720 --> 00:49:31.600]   it's a problem analytically.
[00:49:31.600 --> 00:49:34.800]   It's a problem like numerically and analytically,
[00:49:34.800 --> 00:49:36.960]   but they are these very small regions.
[00:49:36.960 --> 00:49:40.880]   And so it's not something that would show up in like,
[00:49:40.880 --> 00:49:43.160]   it's something that can attract an optimizer
[00:49:43.160 --> 00:49:44.200]   that's badly designed,
[00:49:44.200 --> 00:49:47.080]   but it's not something that you would see
[00:49:47.080 --> 00:49:48.680]   in the lost landscape if you weren't looking.
[00:49:48.680 --> 00:49:49.720]   - No, I mean, you know,
[00:49:49.720 --> 00:49:51.400]   the flatter things that I have seen
[00:49:51.400 --> 00:49:54.920]   is when you go really, really crazy with experiments
[00:49:54.920 --> 00:49:57.600]   and you reach a part of the high dimensional space
[00:49:57.600 --> 00:49:59.720]   that collapses completely the process,
[00:49:59.720 --> 00:50:01.720]   then you can get a dimensionality reduction
[00:50:01.720 --> 00:50:04.760]   that literally is a plane.
[00:50:04.760 --> 00:50:05.960]   I mean, the whole thing,
[00:50:05.960 --> 00:50:07.720]   because the whole thing collapses completely, you know,
[00:50:07.720 --> 00:50:11.920]   but not in an isolated little part, yeah.
[00:50:11.920 --> 00:50:15.320]   Interesting, that's interesting.
[00:50:15.320 --> 00:50:16.160]   That's interesting.
[00:50:16.160 --> 00:50:17.760]   Yeah, I mean, if you say that it could be like
[00:50:17.760 --> 00:50:20.160]   a numerical issue, that's interesting.
[00:50:20.160 --> 00:50:23.520]   - Yeah, it's effectively, it's like a very special point,
[00:50:23.520 --> 00:50:26.200]   just like a minimizer is a very special point, right?
[00:50:26.200 --> 00:50:28.400]   And so like finding one of these points
[00:50:28.400 --> 00:50:32.560]   where there's this sort of brief little region of flatness
[00:50:32.560 --> 00:50:33.560]   is only something you'd find
[00:50:33.560 --> 00:50:35.720]   if you really were looking for it.
[00:50:35.720 --> 00:50:36.560]   - Yeah, that's interesting.
[00:50:36.560 --> 00:50:38.720]   Maybe they are out there, who knows?
[00:50:38.720 --> 00:50:44.000]   Maybe we'll have to search for them.
[00:50:44.000 --> 00:50:45.040]   - Let's see.
[00:50:45.040 --> 00:50:46.440]   We talked about a couple of the different,
[00:50:46.440 --> 00:50:49.840]   so you mentioned, yeah, the utility of skip connections.
[00:50:49.840 --> 00:50:55.720]   Oh, so you briefly touched on some stuff
[00:50:55.720 --> 00:50:57.280]   about batch norm from,
[00:50:57.280 --> 00:50:58.960]   you mentioned the paper from Mike Mahoney's group
[00:50:58.960 --> 00:51:02.240]   about like their sort of heretical view
[00:51:02.240 --> 00:51:03.320]   of what batch norm does.
[00:51:03.320 --> 00:51:05.280]   Could you tell us a little bit more about
[00:51:05.280 --> 00:51:08.720]   like how you see batch norm influencing these,
[00:51:08.720 --> 00:51:12.200]   the like landscapes and the visualizations that you've seen
[00:51:12.200 --> 00:51:14.120]   in neural network training in general?
[00:51:14.120 --> 00:51:18.120]   - Well, you know, I mean, it's very correlated
[00:51:18.120 --> 00:51:20.600]   to that paper in the sense that I have done
[00:51:20.600 --> 00:51:22.520]   quite a few experiments with batch norm
[00:51:22.520 --> 00:51:26.440]   and I have got a bit of a contradictory effects
[00:51:26.440 --> 00:51:29.960]   because again, I have seen the impact of batch norm
[00:51:29.960 --> 00:51:32.640]   apparently is smoothing surfaces,
[00:51:32.640 --> 00:51:35.040]   but also the opposite as well.
[00:51:35.040 --> 00:51:37.600]   So I don't have a clear view at the moment of it.
[00:51:37.600 --> 00:51:41.360]   You know, there was a point in which I was quite convinced
[00:51:41.360 --> 00:51:43.520]   that I have a clear correlation,
[00:51:43.520 --> 00:51:46.360]   but eventually I got to a point similar
[00:51:46.360 --> 00:51:49.040]   to what this paper is also talking about.
[00:51:49.040 --> 00:51:51.520]   And this is the problem as well,
[00:51:51.520 --> 00:51:53.440]   working with lost landscapes that as you know,
[00:51:53.440 --> 00:51:56.280]   it takes a lot of time and a lot of computation
[00:51:56.280 --> 00:51:59.400]   and there are a lot more comparisons still to be done.
[00:51:59.400 --> 00:52:01.240]   So yeah.
[00:52:01.240 --> 00:52:05.240]   - So when you say smoothness there, just a quick question.
[00:52:05.240 --> 00:52:07.640]   So my view of what batch norm does
[00:52:07.640 --> 00:52:10.640]   is that it improves the condition number of the surface.
[00:52:10.640 --> 00:52:12.480]   So like that notion of smoothness
[00:52:12.480 --> 00:52:14.640]   in terms of the relationship between the like small,
[00:52:14.640 --> 00:52:16.720]   negative, small positive eigenvalues
[00:52:16.720 --> 00:52:18.040]   and large positive eigenvalues,
[00:52:18.040 --> 00:52:20.680]   pulling those really high positive eigenvalues down.
[00:52:20.680 --> 00:52:23.480]   Is that what is meant by smoothness?
[00:52:23.480 --> 00:52:24.320]   - Yes.
[00:52:24.320 --> 00:52:25.160]   - Here?
[00:52:25.160 --> 00:52:26.280]   - Yes, the conditioning, this is right.
[00:52:26.280 --> 00:52:28.040]   I mean, the conditioning of the surface,
[00:52:28.040 --> 00:52:32.840]   you know, the degree of variation that you could get.
[00:52:32.840 --> 00:52:36.160]   Yes, it's a little bit related to that, absolutely.
[00:52:36.160 --> 00:52:37.880]   - Yeah, 'cause there's sort of like two,
[00:52:37.880 --> 00:52:39.120]   there's like the notion of roughness
[00:52:39.120 --> 00:52:40.160]   that we were talking about before,
[00:52:40.160 --> 00:52:43.560]   where you have a landscape that's very foamy, right?
[00:52:43.560 --> 00:52:45.840]   But that landscape could still have
[00:52:45.840 --> 00:52:49.040]   like pretty well-behaved maximum eigenvalues,
[00:52:49.960 --> 00:52:53.040]   like an egg crate is relatively rough,
[00:52:53.040 --> 00:52:56.480]   but you never have like an eigenvalue
[00:52:56.480 --> 00:52:59.040]   that's like 10,000 times larger than the rest of them.
[00:52:59.040 --> 00:53:00.360]   - That's correct.
[00:53:00.360 --> 00:53:03.880]   - Yeah, so there's slightly different notions of smoothness.
[00:53:03.880 --> 00:53:05.480]   And I was just sort of curious
[00:53:05.480 --> 00:53:08.600]   which one was more important for the-
[00:53:08.600 --> 00:53:10.480]   - I mean, you are correct, you are correct.
[00:53:10.480 --> 00:53:13.200]   And we also have to consider one thing
[00:53:13.200 --> 00:53:15.320]   that in some of these representations
[00:53:15.320 --> 00:53:17.760]   captured in very, very high resolution,
[00:53:17.760 --> 00:53:22.080]   we also see some variations that we may not perceive
[00:53:22.080 --> 00:53:26.080]   when we capture these representations with smaller detail.
[00:53:26.080 --> 00:53:30.320]   And some of these variations are also, you know,
[00:53:30.320 --> 00:53:32.840]   they are not things that necessarily
[00:53:32.840 --> 00:53:36.400]   are gonna interfere so much with the process.
[00:53:36.400 --> 00:53:40.880]   So as you say, the surface could be well-conditioned
[00:53:40.880 --> 00:53:42.720]   and still have certain variations.
[00:53:46.360 --> 00:53:50.080]   All right, so it's looking like about six o'clock
[00:53:50.080 --> 00:53:54.680]   and I know you are up quite late in European time
[00:53:54.680 --> 00:53:56.760]   to give this talk.
[00:53:56.760 --> 00:53:59.000]   So thank you so much for coming
[00:53:59.000 --> 00:54:01.080]   and sharing your work with us.
[00:54:01.080 --> 00:54:02.440]   - It is my big pleasure, Charles.
[00:54:02.440 --> 00:54:04.080]   Thank you very much, it was a pleasure.
[00:54:04.080 --> 00:54:05.200]   Thank you.
[00:54:05.200 --> 00:54:06.640]   - All right, I will catch you around
[00:54:06.640 --> 00:54:08.880]   and also the folks in the audience
[00:54:08.880 --> 00:54:10.240]   and catching the stream later.
[00:54:10.240 --> 00:54:12.840]   I will see you at our next Weights of Biases Salon
[00:54:12.840 --> 00:54:16.800]   on March 30th to talk about our reproducibility challenge.
[00:54:16.800 --> 00:54:18.960]   So see you then, bye.
[00:54:18.960 --> 00:54:24.260]   Thanks for watching.

