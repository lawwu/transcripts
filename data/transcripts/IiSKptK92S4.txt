
[00:00:00.000 --> 00:00:03.560]   - Let's talk about superintelligence,
[00:00:03.560 --> 00:00:05.680]   at least for a little bit.
[00:00:05.680 --> 00:00:07.280]   And let's start at the basics.
[00:00:07.280 --> 00:00:09.340]   What to you is intelligence?
[00:00:09.340 --> 00:00:14.760]   - Yeah, not to get too stuck with the definitional question.
[00:00:14.760 --> 00:00:17.400]   I mean, the common sense to understand,
[00:00:17.400 --> 00:00:19.800]   like the ability to solve complex problems,
[00:00:19.800 --> 00:00:23.040]   to learn from experience, to plan, to reason,
[00:00:23.040 --> 00:00:27.320]   some combination of things like that.
[00:00:27.320 --> 00:00:29.840]   - Is consciousness mixed up into that or no?
[00:00:29.840 --> 00:00:31.760]   Is consciousness mixed up into that or is it-
[00:00:31.760 --> 00:00:34.880]   - Well, I think it could be fairly intelligent,
[00:00:34.880 --> 00:00:38.000]   at least without being conscious probably.
[00:00:38.000 --> 00:00:42.160]   - So then what is superintelligence?
[00:00:42.160 --> 00:00:44.920]   - Yeah, that would be like something that was much more-
[00:00:44.920 --> 00:00:46.320]   - Of that.
[00:00:46.320 --> 00:00:49.000]   - Had much more general cognitive capacity
[00:00:49.000 --> 00:00:50.400]   than we humans have.
[00:00:50.400 --> 00:00:53.920]   So if we talk about general superintelligence,
[00:00:53.920 --> 00:00:57.840]   it would be much faster learner be able to reason
[00:00:57.840 --> 00:01:00.320]   much better, make plans that are more effective
[00:01:00.320 --> 00:01:03.080]   at achieving its goals, say in a wide range
[00:01:03.080 --> 00:01:05.720]   of complex, challenging environments.
[00:01:05.720 --> 00:01:08.880]   - In terms of, as we turn our eye to the idea
[00:01:08.880 --> 00:01:12.740]   of sort of existential threats from superintelligence,
[00:01:12.740 --> 00:01:16.240]   do you think superintelligence has to exist
[00:01:16.240 --> 00:01:19.520]   in the physical world or can it be digital only?
[00:01:19.520 --> 00:01:23.960]   Sort of, we think of our general intelligence as us humans,
[00:01:23.960 --> 00:01:27.360]   as an intelligence that's associated with the body
[00:01:27.360 --> 00:01:28.880]   that's able to interact with the world,
[00:01:28.880 --> 00:01:32.760]   that's able to affect the world directly with physically.
[00:01:32.760 --> 00:01:34.960]   - I mean, digital only is perfectly fine, I think.
[00:01:34.960 --> 00:01:37.720]   I mean, it's physical in the sense that obviously
[00:01:37.720 --> 00:01:40.840]   the computers and the memories are physical.
[00:01:40.840 --> 00:01:43.640]   - But it's capability to affect the world sort of-
[00:01:43.640 --> 00:01:46.480]   - Could be very strong even if it has a limited
[00:01:46.480 --> 00:01:51.360]   set of actuators, if it can type text on the screen
[00:01:51.360 --> 00:01:54.480]   or something like that, that would be, I think, ample.
[00:01:54.480 --> 00:01:59.480]   - So in terms of the concerns of existential threat of AI,
[00:01:59.480 --> 00:02:03.560]   how can an AI system that's in the digital world
[00:02:03.560 --> 00:02:07.000]   have existential risk sort of,
[00:02:07.000 --> 00:02:10.520]   and what are the attack vectors for a digital system?
[00:02:10.520 --> 00:02:12.920]   - Well, I mean, I guess maybe to take one step back,
[00:02:12.920 --> 00:02:16.600]   so I should emphasize that I also think
[00:02:16.600 --> 00:02:18.880]   there's this huge positive potential
[00:02:18.880 --> 00:02:22.040]   from machine intelligence, including superintelligence.
[00:02:22.040 --> 00:02:26.920]   And I wanna stress that because some of my writing
[00:02:26.920 --> 00:02:29.440]   has focused on what can go wrong.
[00:02:29.440 --> 00:02:31.800]   And when I wrote the book "Superintelligence,"
[00:02:31.800 --> 00:02:36.800]   at that point, I felt that there was a kind of neglect
[00:02:36.800 --> 00:02:40.440]   of what would happen if AI succeeds,
[00:02:40.440 --> 00:02:42.280]   and in particular, a need to get
[00:02:42.280 --> 00:02:45.000]   a more granular understanding of where the pitfalls are
[00:02:45.000 --> 00:02:46.200]   so we can avoid them.
[00:02:46.200 --> 00:02:51.680]   I think that since the book came out in 2014,
[00:02:51.680 --> 00:02:54.640]   there has been a much wider recognition of that,
[00:02:54.640 --> 00:02:56.560]   and a number of research groups
[00:02:56.560 --> 00:02:58.800]   are now actually working on developing,
[00:02:58.800 --> 00:03:01.400]   say, AI alignment techniques and so on and so forth.
[00:03:01.400 --> 00:03:05.440]   So I'd like, yeah, I think now it's important
[00:03:05.440 --> 00:03:10.040]   to make sure we bring back onto the table
[00:03:10.040 --> 00:03:11.000]   the upside as well.
[00:03:11.000 --> 00:03:14.560]   - And there's a little bit of a neglect now on the upside,
[00:03:14.560 --> 00:03:17.560]   which is, I mean, if you look at, I was talking to a friend,
[00:03:17.560 --> 00:03:20.440]   if you look at the amount of information that is available,
[00:03:20.440 --> 00:03:22.480]   or people talking, or people being excited
[00:03:22.480 --> 00:03:25.720]   about the positive possibilities of general intelligence,
[00:03:25.720 --> 00:03:29.160]   that's not, it's far outnumbered
[00:03:29.160 --> 00:03:31.520]   by the negative possibilities
[00:03:31.520 --> 00:03:34.000]   in terms of our public discourse.
[00:03:34.000 --> 00:03:35.640]   - Possibly, yeah.
[00:03:35.640 --> 00:03:37.680]   It's hard to measure it, but--
[00:03:37.680 --> 00:03:39.720]   - What are, can you linger on that for a little bit?
[00:03:39.720 --> 00:03:44.440]   What are some, to you, possible big positive impacts
[00:03:44.440 --> 00:03:46.920]   of general intelligence, superintelligence?
[00:03:46.920 --> 00:03:48.560]   - Well, I mean, superintelligence,
[00:03:48.560 --> 00:03:51.560]   because I tend to also wanna distinguish
[00:03:51.560 --> 00:03:54.680]   these two different contexts of thinking about AI
[00:03:54.680 --> 00:03:57.960]   and AI impacts, the kind of near-term and long-term,
[00:03:57.960 --> 00:04:01.800]   if you want, both of which I think are legitimate things
[00:04:01.800 --> 00:04:06.800]   to think about, and people should discuss both of them,
[00:04:06.800 --> 00:04:10.640]   but they are different, and they often get mixed up,
[00:04:10.640 --> 00:04:13.760]   and then I get, you get confusion.
[00:04:13.760 --> 00:04:15.160]   Like, I think you get simultaneously,
[00:04:15.160 --> 00:04:17.200]   like maybe an overhyping of the near-term
[00:04:17.200 --> 00:04:18.880]   and an underhyping of the long-term,
[00:04:18.880 --> 00:04:20.920]   and so I think as long as we keep them apart,
[00:04:20.920 --> 00:04:23.840]   we can have, like, two good conversations,
[00:04:23.840 --> 00:04:26.160]   but, or we can mix them together
[00:04:26.160 --> 00:04:27.280]   and have one bad conversation.
[00:04:27.280 --> 00:04:30.360]   - Can you clarify just the two things we're talking about,
[00:04:30.360 --> 00:04:32.200]   the near-term and the long-term?
[00:04:32.200 --> 00:04:33.040]   What are the distinctions?
[00:04:33.040 --> 00:04:36.680]   - Well, it's a blurry distinction,
[00:04:36.680 --> 00:04:38.880]   but say the things I wrote about in this book,
[00:04:38.880 --> 00:04:41.840]   superintelligence, long-term,
[00:04:41.840 --> 00:04:45.680]   things people are worrying about today
[00:04:45.680 --> 00:04:48.680]   with, I don't know, algorithmic discrimination,
[00:04:48.680 --> 00:04:53.120]   or even things, self-driving cars and drones and stuff,
[00:04:53.120 --> 00:04:54.120]   more near-term.
[00:04:54.120 --> 00:04:58.880]   And then, of course, you could imagine some medium-term
[00:04:58.880 --> 00:05:01.920]   where they kind of overlap and one evolves into the other.
[00:05:01.920 --> 00:05:05.400]   But at any rate, I think both, yeah,
[00:05:05.400 --> 00:05:08.440]   the issues look kind of somewhat different
[00:05:08.440 --> 00:05:10.240]   depending on which of these contexts.
[00:05:10.240 --> 00:05:12.640]   - So I think it would be nice
[00:05:12.640 --> 00:05:14.440]   if we can talk about the long-term,
[00:05:15.360 --> 00:05:20.360]   and think about a positive impact
[00:05:20.360 --> 00:05:24.280]   or a better world because of the existence
[00:05:24.280 --> 00:05:26.520]   of the long-term superintelligence.
[00:05:26.520 --> 00:05:28.000]   Do you have views of such a world?
[00:05:28.000 --> 00:05:30.960]   - Yeah, I mean, I guess it's a little hard to articulate
[00:05:30.960 --> 00:05:33.320]   because it seems obvious that the world
[00:05:33.320 --> 00:05:36.560]   has a lot of problems as it currently stands.
[00:05:36.560 --> 00:05:41.120]   And it's hard to think of any one of those
[00:05:41.120 --> 00:05:43.720]   which it wouldn't be useful to have,
[00:05:43.720 --> 00:05:48.720]   like, a friendly aligned superintelligence working on.
[00:05:48.720 --> 00:05:53.720]   - So from health to the economic system
[00:05:53.720 --> 00:05:56.960]   to be able to sort of improve the investment
[00:05:56.960 --> 00:05:59.120]   and trade and foreign policy decisions,
[00:05:59.120 --> 00:06:00.880]   all that kind of stuff.
[00:06:00.880 --> 00:06:03.120]   - All that kind of stuff and a lot more.
[00:06:03.120 --> 00:06:06.760]   - I mean, what's the killer app?
[00:06:06.760 --> 00:06:08.320]   - Well, I don't think there is one.
[00:06:08.320 --> 00:06:12.960]   I think AI, especially artificial general intelligence,
[00:06:12.960 --> 00:06:16.440]   is really the ultimate general purpose technology.
[00:06:16.440 --> 00:06:18.360]   So it's not that there is this one problem,
[00:06:18.360 --> 00:06:20.760]   this one area where it will have a big impact,
[00:06:20.760 --> 00:06:23.520]   but if and when it succeeds,
[00:06:23.520 --> 00:06:26.320]   it will really apply across the board
[00:06:26.320 --> 00:06:29.960]   in all fields where human creativity and intelligence
[00:06:29.960 --> 00:06:31.160]   and problem solving is useful,
[00:06:31.160 --> 00:06:33.680]   which is pretty much all fields, right?
[00:06:33.680 --> 00:06:36.680]   The thing that it would do
[00:06:36.680 --> 00:06:39.480]   is give us a lot more control over nature.
[00:06:39.480 --> 00:06:41.680]   It wouldn't automatically solve the problems
[00:06:41.680 --> 00:06:43.960]   that arise from conflict between humans,
[00:06:43.960 --> 00:06:46.840]   fundamentally political problems.
[00:06:46.840 --> 00:06:48.280]   Some subset of those might go away
[00:06:48.280 --> 00:06:50.800]   if we just had more resources and cooler tech,
[00:06:50.800 --> 00:06:54.800]   but some subset would require coordination
[00:06:54.800 --> 00:06:58.800]   that is not automatically achieved
[00:06:58.800 --> 00:07:01.600]   just by having more technological capability.
[00:07:01.600 --> 00:07:03.440]   But anything that's not of that sort,
[00:07:03.440 --> 00:07:05.680]   I think you just get like an enormous boost
[00:07:05.680 --> 00:07:09.600]   with this kind of cognitive technology
[00:07:09.600 --> 00:07:11.480]   once it goes all the way.
[00:07:11.480 --> 00:07:13.920]   Now, again, that doesn't mean I'm like thinking,
[00:07:13.920 --> 00:07:18.920]   oh, people don't recognize what's possible
[00:07:18.920 --> 00:07:20.680]   with current technology
[00:07:20.680 --> 00:07:22.760]   and like sometimes things get overhyped,
[00:07:22.760 --> 00:07:25.600]   but I mean, those are perfectly consistent views to hold,
[00:07:25.600 --> 00:07:28.480]   the ultimate potential being enormous.
[00:07:28.480 --> 00:07:30.400]   And then it's a very different question
[00:07:30.400 --> 00:07:31.880]   of how far are we from that
[00:07:31.880 --> 00:07:34.040]   or what can we do with near-term technology?
[00:07:34.040 --> 00:07:35.120]   - Yeah, so what's your intuition
[00:07:35.120 --> 00:07:37.840]   about the idea of intelligence explosion?
[00:07:37.840 --> 00:07:38.880]   So there's this,
[00:07:40.680 --> 00:07:42.800]   you know, when you start to think about that leap
[00:07:42.800 --> 00:07:44.880]   from the near-term to the long-term,
[00:07:44.880 --> 00:07:46.880]   the natural inclination,
[00:07:46.880 --> 00:07:49.720]   like for me, sort of building machine learning systems today
[00:07:49.720 --> 00:07:51.760]   it seems like it's a lot of work
[00:07:51.760 --> 00:07:53.640]   to get to general intelligence,
[00:07:53.640 --> 00:07:55.880]   but there's some intuition of exponential growth,
[00:07:55.880 --> 00:07:59.480]   of exponential improvement, of intelligence explosion.
[00:07:59.480 --> 00:08:04.200]   Can you maybe try to elucidate,
[00:08:04.200 --> 00:08:07.680]   try to talk about what's your intuition
[00:08:07.680 --> 00:08:11.560]   about the possibility of a intelligence explosion,
[00:08:11.560 --> 00:08:13.920]   that it won't be this gradual, slow process,
[00:08:13.920 --> 00:08:15.960]   there might be a phase shift?
[00:08:15.960 --> 00:08:19.680]   - Yeah, I think it's,
[00:08:19.680 --> 00:08:22.080]   we don't know how explosive it will be.
[00:08:22.080 --> 00:08:24.200]   I think for what it's worth,
[00:08:24.200 --> 00:08:27.960]   seems fairly likely to me that at some point
[00:08:27.960 --> 00:08:29.960]   there will be some intelligence explosion,
[00:08:29.960 --> 00:08:32.000]   like some period of time
[00:08:32.000 --> 00:08:34.560]   where progress in AI becomes extremely rapid.
[00:08:35.600 --> 00:08:39.080]   Roughly in the area where you might say
[00:08:39.080 --> 00:08:42.280]   it's kind of human-ish equivalent
[00:08:42.280 --> 00:08:46.040]   in core cognitive faculties,
[00:08:46.040 --> 00:08:48.600]   that the concept of human equivalent,
[00:08:48.600 --> 00:08:51.680]   like it starts to break down when you look too closely at it
[00:08:51.680 --> 00:08:54.400]   and just how explosive does something have to be
[00:08:54.400 --> 00:08:57.640]   for it to be called an intelligence explosion?
[00:08:57.640 --> 00:08:59.600]   Like, does it have to be like overnight literally,
[00:08:59.600 --> 00:09:01.040]   or a few years?
[00:09:01.040 --> 00:09:04.680]   But overall, I guess,
[00:09:04.680 --> 00:09:08.160]   if you plotted the opinions of different people
[00:09:08.160 --> 00:09:10.720]   in the world, I guess that would be somewhat more
[00:09:10.720 --> 00:09:14.080]   probability towards the intelligence explosion scenario
[00:09:14.080 --> 00:09:18.200]   than probably the average AI researcher, I guess.
[00:09:18.200 --> 00:09:21.400]   - So, and then the other part of the intelligence explosion,
[00:09:21.400 --> 00:09:24.600]   or just, forget explosion, just progress,
[00:09:24.600 --> 00:09:27.040]   is once you achieve that gray area
[00:09:27.040 --> 00:09:29.040]   of human-level intelligence,
[00:09:29.040 --> 00:09:31.760]   is it obvious to you that we should be able
[00:09:31.760 --> 00:09:35.760]   to proceed beyond it to get to super intelligence?
[00:09:35.760 --> 00:09:39.760]   - Yeah, that seems, I mean, as much as any of these things
[00:09:39.760 --> 00:09:43.680]   can be obvious, given we've never had one,
[00:09:43.680 --> 00:09:44.760]   people have different views,
[00:09:44.760 --> 00:09:46.040]   smart people have different views,
[00:09:46.040 --> 00:09:49.280]   it's like there's some degree of uncertainty
[00:09:49.280 --> 00:09:52.200]   that always remains for any big, futuristic,
[00:09:52.200 --> 00:09:54.800]   philosophical, grand question
[00:09:54.800 --> 00:09:56.640]   that just we realize humans are fallible,
[00:09:56.640 --> 00:09:58.200]   especially about these things.
[00:09:58.200 --> 00:10:01.680]   But it does seem, as far as I'm judging things,
[00:10:01.680 --> 00:10:03.680]   based on my own impressions,
[00:10:03.680 --> 00:10:08.160]   that it seems very unlikely that that would be a ceiling
[00:10:08.160 --> 00:10:12.720]   at or near human cognitive capacity.
[00:10:12.720 --> 00:10:15.600]   - But, and that's such a, I don't know,
[00:10:15.600 --> 00:10:17.520]   that's such a special moment.
[00:10:17.520 --> 00:10:20.120]   It's both terrifying and exciting
[00:10:20.120 --> 00:10:23.640]   to create a system that's beyond our intelligence.
[00:10:23.640 --> 00:10:27.120]   So, maybe you can step back and say,
[00:10:27.120 --> 00:10:30.360]   like, how does that possibility make you feel?
[00:10:30.360 --> 00:10:33.240]   That we can create something,
[00:10:33.240 --> 00:10:37.000]   it feels like there's a line beyond which it steps,
[00:10:37.000 --> 00:10:39.760]   it'll be able to outsmart you,
[00:10:39.760 --> 00:10:44.240]   and therefore it feels like a step where we lose control.
[00:10:44.240 --> 00:10:48.200]   - Well, I don't think the latter follows,
[00:10:48.200 --> 00:10:50.600]   that is, you could imagine,
[00:10:50.600 --> 00:10:52.920]   and in fact, this is what a number of people
[00:10:52.920 --> 00:10:56.040]   are working towards, making sure that we could ultimately
[00:10:56.880 --> 00:11:00.640]   project higher levels of problem-solving ability
[00:11:00.640 --> 00:11:03.520]   while still making sure that they are aligned,
[00:11:03.520 --> 00:11:06.240]   like they are in the service of human values.
[00:11:06.240 --> 00:11:11.280]   I mean, so, losing control, I think,
[00:11:11.280 --> 00:11:15.040]   is not a given that that would happen.
[00:11:15.040 --> 00:11:16.760]   Now, you asked how it makes you feel.
[00:11:16.760 --> 00:11:19.400]   I mean, to some extent, I've lived with this for so long,
[00:11:19.400 --> 00:11:22.560]   since as long as I can remember,
[00:11:22.560 --> 00:11:25.160]   being an adult or even a teenager,
[00:11:25.160 --> 00:11:27.000]   it seemed to me obvious that at some point,
[00:11:27.000 --> 00:11:28.440]   AI will succeed.
[00:11:28.440 --> 00:11:33.440]   - And so, I actually misspoke, I didn't mean control.
[00:11:33.440 --> 00:11:36.680]   I meant, because the control problem is an interesting thing
[00:11:36.680 --> 00:11:40.320]   and I think the hope is, at least we should be able
[00:11:40.320 --> 00:11:44.040]   to maintain control over systems that are smarter than us,
[00:11:44.040 --> 00:11:48.380]   but we do lose our specialness.
[00:11:48.380 --> 00:11:53.240]   It's sort of, we'll lose our place
[00:11:53.240 --> 00:11:57.000]   as the smartest, coolest thing on Earth.
[00:11:57.000 --> 00:12:00.600]   And there's an ego involved with that,
[00:12:00.600 --> 00:12:04.480]   that humans aren't very good at dealing with.
[00:12:04.480 --> 00:12:08.480]   I mean, I value my intelligence as a human being.
[00:12:08.480 --> 00:12:11.040]   It seems like a big transformative step
[00:12:11.040 --> 00:12:13.320]   to realize there's something out there
[00:12:13.320 --> 00:12:14.600]   that's more intelligent.
[00:12:14.600 --> 00:12:17.800]   I mean, you don't see that as such a fundamental--
[00:12:17.800 --> 00:12:21.920]   - Yeah, I think, yes, a lot, I think it would be small.
[00:12:21.920 --> 00:12:25.120]   I mean, I think there are already a lot of things out there
[00:12:25.120 --> 00:12:27.320]   that are, I mean, certainly if you think the universe
[00:12:27.320 --> 00:12:29.240]   is big, there's gonna be other civilizations
[00:12:29.240 --> 00:12:31.680]   that already have super intelligences
[00:12:31.680 --> 00:12:35.480]   or that just naturally have brains the size of beach balls
[00:12:35.480 --> 00:12:39.300]   and are like completely leaving us in the dust.
[00:12:39.300 --> 00:12:42.280]   And we haven't come face to face with that.
[00:12:42.280 --> 00:12:43.480]   - We haven't come face to face,
[00:12:43.480 --> 00:12:45.600]   but I mean, that's an open question,
[00:12:45.600 --> 00:12:50.520]   what would happen in a kind of post-human world,
[00:12:50.520 --> 00:12:55.520]   like how much day to day would these super intelligences
[00:12:55.520 --> 00:12:58.360]   be involved in the lives of ordinary?
[00:12:58.360 --> 00:13:01.360]   I mean, you could imagine some scenario
[00:13:01.360 --> 00:13:03.020]   where it would be more like a background thing
[00:13:03.020 --> 00:13:05.120]   that would help protect against some things,
[00:13:05.120 --> 00:13:08.480]   but you wouldn't, like there wouldn't be this intrusive
[00:13:08.480 --> 00:13:10.800]   kind of like making you feel bad
[00:13:10.800 --> 00:13:13.360]   by like making clever jokes on your expense.
[00:13:13.360 --> 00:13:14.680]   Like there's like all sorts of things
[00:13:14.680 --> 00:13:16.760]   that maybe in the human context
[00:13:16.760 --> 00:13:19.400]   would feel awkward about that.
[00:13:19.400 --> 00:13:21.360]   You don't wanna be the dumbest kid in your class,
[00:13:21.360 --> 00:13:22.200]   everybody picks it.
[00:13:22.200 --> 00:13:23.800]   Like a lot of those things,
[00:13:23.800 --> 00:13:26.760]   maybe you need to abstract away from,
[00:13:26.760 --> 00:13:28.220]   if you're thinking about this context
[00:13:28.220 --> 00:13:30.720]   where we have infrastructure that is in some sense
[00:13:30.720 --> 00:13:34.940]   beyond any or all humans.
[00:13:34.940 --> 00:13:38.200]   I mean, it's a little bit like say the scientific community
[00:13:38.200 --> 00:13:41.040]   as a whole, if you think of that as a mind,
[00:13:41.040 --> 00:13:42.040]   it's a little bit of metaphor,
[00:13:42.040 --> 00:13:46.600]   but I mean, obviously it's gotta be like way more capacious
[00:13:46.600 --> 00:13:48.200]   than any individual.
[00:13:48.200 --> 00:13:51.280]   So in some sense, there is this mind like thing
[00:13:51.280 --> 00:13:56.080]   already out there that's just vastly more intelligent
[00:13:56.080 --> 00:13:58.280]   than a new individual is.
[00:13:58.280 --> 00:14:01.400]   And we think, okay, that's,
[00:14:01.400 --> 00:14:04.000]   you just accept that as a fact.
[00:14:04.000 --> 00:14:06.200]   - That's the basic fabric of our existence
[00:14:06.200 --> 00:14:07.640]   is there's a super intelligent.
[00:14:07.640 --> 00:14:09.320]   - Yeah, you get used to a lot of.
[00:14:09.320 --> 00:14:12.600]   - I mean, there's already Google and Twitter and Facebook,
[00:14:12.600 --> 00:14:17.600]   these recommender systems that are the basic fabric
[00:14:17.640 --> 00:14:21.600]   of our, I could see them becoming,
[00:14:21.600 --> 00:14:23.720]   I mean, do you think of the collective intelligence
[00:14:23.720 --> 00:14:25.920]   of these systems as already perhaps
[00:14:25.920 --> 00:14:27.720]   reaching super intelligence level?
[00:14:27.720 --> 00:14:30.600]   - Well, I mean, so here it comes to this,
[00:14:30.600 --> 00:14:33.120]   the concept of intelligence and the scale
[00:14:33.120 --> 00:14:35.960]   and what human level means.
[00:14:35.960 --> 00:14:41.560]   The kind of vagueness and indeterminacy of those concepts
[00:14:41.560 --> 00:14:46.560]   starts to dominate how you would answer that question.
[00:14:47.600 --> 00:14:50.480]   So like say the Google search engine
[00:14:50.480 --> 00:14:53.440]   has a very high capacity of a certain kind,
[00:14:53.440 --> 00:14:56.760]   like remembering and retrieving information,
[00:14:56.760 --> 00:15:02.680]   particularly like text or images
[00:15:02.680 --> 00:15:07.680]   that you have a kind of string, a word string key,
[00:15:07.680 --> 00:15:09.120]   obviously superhuman at that,
[00:15:09.120 --> 00:15:14.120]   but a vast set of other things it can't even do at all,
[00:15:14.120 --> 00:15:16.200]   not just not do well.
[00:15:17.200 --> 00:15:19.600]   So you have these current AI systems
[00:15:19.600 --> 00:15:22.880]   that are superhuman in some limited domain
[00:15:22.880 --> 00:15:27.880]   and then like radically subhuman in all other domains.
[00:15:27.880 --> 00:15:31.040]   Same with a chess, like are just a simple computer
[00:15:31.040 --> 00:15:33.000]   that can multiply really large numbers, right?
[00:15:33.000 --> 00:15:36.040]   So it's gonna have this like one spike of super intelligence
[00:15:36.040 --> 00:15:38.880]   and then a kind of a zero level of capability
[00:15:38.880 --> 00:15:40.920]   across all other cognitive fields.
[00:15:40.920 --> 00:15:44.160]   - Yeah, I don't necessarily think the generalness,
[00:15:44.160 --> 00:15:45.440]   I mean, I'm not so attached to it,
[00:15:45.440 --> 00:15:49.160]   but I could sort of, it's a gray area and it's a feeling,
[00:15:49.160 --> 00:15:53.000]   but to me sort of alpha zero
[00:15:53.000 --> 00:15:56.320]   is somehow much more intelligent,
[00:15:56.320 --> 00:15:59.160]   much, much more intelligent than Deep Blue.
[00:15:59.160 --> 00:16:01.680]   And to say which domain,
[00:16:01.680 --> 00:16:03.680]   well, you could say, well, these are both just board game,
[00:16:03.680 --> 00:16:05.440]   they're both just able to play board games,
[00:16:05.440 --> 00:16:07.860]   who cares if they're gonna do better or not?
[00:16:07.860 --> 00:16:10.440]   But there's something about the learning, the self play--
[00:16:10.440 --> 00:16:13.400]   - The learning, yeah. - That makes it,
[00:16:13.400 --> 00:16:16.320]   crosses over into that land of intelligence
[00:16:16.320 --> 00:16:18.440]   that doesn't necessarily need to be general.
[00:16:18.440 --> 00:16:22.120]   In the same way, Google is much closer to Deep Blue currently
[00:16:22.120 --> 00:16:23.800]   in terms of its search engine--
[00:16:23.800 --> 00:16:24.920]   - Yeah. - Than it is to
[00:16:24.920 --> 00:16:26.600]   sort of the alpha zero.
[00:16:26.600 --> 00:16:28.360]   And the moment it becomes,
[00:16:28.360 --> 00:16:30.040]   and the moment these recommender systems
[00:16:30.040 --> 00:16:33.080]   really become more like alpha zero,
[00:16:33.080 --> 00:16:36.640]   but being able to learn a lot without the constraints
[00:16:36.640 --> 00:16:40.320]   of being heavily constrained by human interaction,
[00:16:40.320 --> 00:16:43.400]   that seems like a special moment in time.
[00:16:43.400 --> 00:16:46.400]   - I mean, certainly learning ability
[00:16:46.400 --> 00:16:51.200]   seems to be an important facet of general intelligence.
[00:16:51.200 --> 00:16:52.480]   - Right. - That you can take
[00:16:52.480 --> 00:16:55.840]   some new domain that you haven't seen before,
[00:16:55.840 --> 00:16:57.880]   and you weren't specifically pre-programmed for,
[00:16:57.880 --> 00:17:00.040]   and then figure out what's going on there,
[00:17:00.040 --> 00:17:02.160]   and eventually become really good at it.
[00:17:02.160 --> 00:17:05.000]   So that's something alpha zero
[00:17:05.000 --> 00:17:07.640]   has much more of than Deep Blue had.
[00:17:08.720 --> 00:17:12.640]   And in fact, I mean, systems like alpha zero can learn,
[00:17:12.640 --> 00:17:14.560]   not just Go, but other,
[00:17:14.560 --> 00:17:17.960]   in fact, probably beat Deep Blue in chess and so forth.
[00:17:17.960 --> 00:17:18.800]   Right? - Yeah,
[00:17:18.800 --> 00:17:19.640]   not just Deep Blue. - So you do see this--
[00:17:19.640 --> 00:17:20.760]   - Destroy Deep Blue. - This general,
[00:17:20.760 --> 00:17:22.400]   and so it matches the intuition.
[00:17:22.400 --> 00:17:23.920]   We feel it's more intelligent,
[00:17:23.920 --> 00:17:25.280]   and it also has more of this
[00:17:25.280 --> 00:17:27.200]   general purpose learning ability.
[00:17:27.200 --> 00:17:29.600]   And if we get systems
[00:17:29.600 --> 00:17:31.560]   that have even more general purpose learning ability,
[00:17:31.560 --> 00:17:33.520]   it might also trigger an even stronger intuition
[00:17:33.520 --> 00:17:36.760]   that they are actually starting to get smart.
[00:17:36.760 --> 00:17:38.320]   - So if you were to pick a future,
[00:17:38.320 --> 00:17:42.520]   what do you think a utopia looks like with AGI systems?
[00:17:42.520 --> 00:17:48.160]   Is it the neural link, brain-computer interface world,
[00:17:48.160 --> 00:17:50.400]   where we're kind of really closely interlinked
[00:17:50.400 --> 00:17:52.400]   with AI systems?
[00:17:52.400 --> 00:17:56.840]   Is it possibly where AGI systems replace us completely
[00:17:56.840 --> 00:18:01.840]   while maintaining the values and the consciousness?
[00:18:01.840 --> 00:18:04.720]   Is it something like it's a completely invisible fabric,
[00:18:04.720 --> 00:18:07.080]   like you mentioned, a society where it's just AIDS
[00:18:07.080 --> 00:18:09.240]   and a lot of stuff that we do,
[00:18:09.240 --> 00:18:10.840]   like curing diseases and so on?
[00:18:10.840 --> 00:18:13.120]   What is utopia if you get to pick?
[00:18:13.120 --> 00:18:14.680]   - Yeah, I mean, it's a good question,
[00:18:14.680 --> 00:18:17.800]   and a deep and difficult one.
[00:18:17.800 --> 00:18:19.080]   I'm quite interested in it.
[00:18:19.080 --> 00:18:22.280]   I don't have all the answers yet,
[00:18:22.280 --> 00:18:23.800]   but, or might never have,
[00:18:23.800 --> 00:18:27.280]   but I think there are some different observations
[00:18:27.280 --> 00:18:28.120]   one can make.
[00:18:28.120 --> 00:18:32.400]   One is if this scenario actually did come to pass,
[00:18:32.400 --> 00:18:37.400]   it would open up this vast space of possible modes of being.
[00:18:37.400 --> 00:18:42.520]   On one hand, material and resource constraints
[00:18:42.520 --> 00:18:44.920]   would just be expanded dramatically.
[00:18:44.920 --> 00:18:49.480]   So there would be a lot of, a big pie, let's say, right?
[00:18:49.480 --> 00:18:53.680]   Also, it would enable us to do things,
[00:18:53.680 --> 00:18:57.560]   including to ourselves,
[00:18:57.560 --> 00:19:00.480]   or like that it would just open up
[00:19:00.480 --> 00:19:02.080]   this much larger design space
[00:19:02.080 --> 00:19:06.120]   and options space than we have ever had access to
[00:19:06.120 --> 00:19:07.120]   in human history.
[00:19:07.120 --> 00:19:10.000]   So I think two things follow from that.
[00:19:10.000 --> 00:19:13.120]   One is that we probably would need to make
[00:19:13.120 --> 00:19:18.120]   a fairly fundamental rethink of what ultimately we value,
[00:19:18.120 --> 00:19:20.640]   like think things through more from first principles.
[00:19:20.640 --> 00:19:22.480]   The context would be so different from the familiar
[00:19:22.480 --> 00:19:25.000]   that we could have just take what we've always been doing
[00:19:25.000 --> 00:19:28.280]   and then like, oh, well, we have this cleaning robot
[00:19:28.280 --> 00:19:31.760]   that cleans the dishes in the sink.
[00:19:31.760 --> 00:19:33.000]   And a few other small things.
[00:19:33.000 --> 00:19:34.640]   And like, I think we would have to go back
[00:19:34.640 --> 00:19:35.760]   to first principles.
[00:19:35.760 --> 00:19:37.760]   - So even from the individual level,
[00:19:37.760 --> 00:19:40.400]   go back to the first principles of what is the meaning
[00:19:40.400 --> 00:19:42.880]   of life, what is happiness, what is fulfillment?
[00:19:42.880 --> 00:19:44.160]   - Yeah.
[00:19:44.160 --> 00:19:48.560]   And then also connected to this large space of resources
[00:19:48.560 --> 00:19:51.960]   is that it would be possible.
[00:19:51.960 --> 00:19:56.960]   And I think something we should aim for is to do well
[00:19:59.000 --> 00:20:02.720]   by the lights of more than one value system.
[00:20:02.720 --> 00:20:08.880]   That is, we wouldn't have to choose only one value criterion
[00:20:08.880 --> 00:20:17.520]   and say, we're gonna do something that scores really high
[00:20:17.520 --> 00:20:21.920]   on the metric of say hedonism.
[00:20:21.920 --> 00:20:26.480]   And then it's like a zero by other criteria,
[00:20:26.480 --> 00:20:29.120]   like kind of wire headed brain Cinebat.
[00:20:29.120 --> 00:20:31.680]   And it's like a lot of pleasure, that's good.
[00:20:31.680 --> 00:20:33.720]   But then like no beauty, no achievement.
[00:20:33.720 --> 00:20:36.320]   Or pick it up.
[00:20:36.320 --> 00:20:39.800]   I think to some significant, not unlimited sense,
[00:20:39.800 --> 00:20:44.080]   but a significant sense, it would be possible to do very well
[00:20:44.080 --> 00:20:44.960]   by many criteria.
[00:20:44.960 --> 00:20:49.960]   Like maybe you could get like 98% of the best
[00:20:49.960 --> 00:20:52.760]   according to several criteria at the same time,
[00:20:52.760 --> 00:20:57.760]   given this great expansion of the option space.
[00:20:57.760 --> 00:20:59.560]   And so-
[00:20:59.560 --> 00:21:02.960]   - So have competing value systems, competing criteria
[00:21:02.960 --> 00:21:07.960]   as a sort of forever, just like our Democrat
[00:21:07.960 --> 00:21:10.400]   versus Republican, there seems to be this always
[00:21:10.400 --> 00:21:14.320]   multiple parties that are useful for our progress in society,
[00:21:14.320 --> 00:21:16.920]   even though it might seem dysfunctional inside the moment,
[00:21:16.920 --> 00:21:20.000]   but having the multiple value systems
[00:21:20.000 --> 00:21:25.000]   seems to be beneficial for, I guess, a balance of power.
[00:21:25.000 --> 00:21:27.960]   - So that's, yeah, not exactly what I have in mind
[00:21:27.960 --> 00:21:29.800]   that it's, well, although it can be,
[00:21:29.800 --> 00:21:31.360]   maybe in an indirect way it is.
[00:21:31.360 --> 00:21:36.760]   But that if you had the chance to do something
[00:21:36.760 --> 00:21:41.560]   that scored well on several different metrics,
[00:21:41.560 --> 00:21:43.480]   our first instinct should be to do that
[00:21:43.480 --> 00:21:46.840]   rather than immediately leap to the thing,
[00:21:46.840 --> 00:21:48.360]   which ones of these value systems
[00:21:48.360 --> 00:21:49.640]   are we gonna screw over?
[00:21:49.640 --> 00:21:50.960]   Like I think our first instinct,
[00:21:50.960 --> 00:21:53.240]   let's first try to do very well by all of them.
[00:21:53.240 --> 00:21:55.880]   Then it might be that you can't get 100% of all,
[00:21:55.880 --> 00:21:58.720]   and you would have to then like have the hard conversation
[00:21:58.720 --> 00:22:00.680]   about which one will only get 97%.
[00:22:00.680 --> 00:22:02.080]   - There you go, there's my cynicism
[00:22:02.080 --> 00:22:04.920]   that all of existence is always a trade-off.
[00:22:04.920 --> 00:22:07.560]   But you say, maybe it's not such a bad trade-off.
[00:22:07.560 --> 00:22:08.800]   Let's first at least try-
[00:22:08.800 --> 00:22:11.040]   - Well, this would be a distinctive context
[00:22:11.040 --> 00:22:16.040]   in which at least some of the constraints would be removed.
[00:22:16.040 --> 00:22:17.560]   - I'll leave you there.
[00:22:17.560 --> 00:22:19.160]   - So there's probably still be trade-offs in the end.
[00:22:19.160 --> 00:22:20.680]   It's just that we should first make sure
[00:22:20.680 --> 00:22:24.640]   we at least take advantage of this abundance.
[00:22:24.640 --> 00:22:27.320]   So in terms of thinking about this,
[00:22:27.320 --> 00:22:29.440]   like, yeah, one should think,
[00:22:29.440 --> 00:22:34.440]   I think in this kind of frame of mind of generosity
[00:22:34.440 --> 00:22:39.000]   and inclusiveness to different value systems
[00:22:39.000 --> 00:22:42.280]   and see how far one can get there first.
[00:22:42.280 --> 00:22:45.280]   And I think one could do something
[00:22:45.280 --> 00:22:46.480]   that would be very good
[00:22:46.480 --> 00:22:49.760]   according to many different criteria.
[00:22:49.760 --> 00:22:51.840]   (laughs)
[00:22:51.840 --> 00:22:54.440]   (upbeat music)
[00:22:54.440 --> 00:22:57.040]   (upbeat music)
[00:22:57.040 --> 00:22:59.640]   (upbeat music)
[00:22:59.640 --> 00:23:02.240]   (upbeat music)
[00:23:02.240 --> 00:23:04.840]   (upbeat music)
[00:23:04.840 --> 00:23:07.440]   (upbeat music)
[00:23:07.440 --> 00:23:17.440]   [BLANK_AUDIO]

