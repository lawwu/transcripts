
[00:00:00.000 --> 00:00:04.000]   [MUSIC PLAYING]
[00:00:04.000 --> 00:00:05.760]   Hello, my name is Stella Biederman.
[00:00:05.760 --> 00:00:08.760]   I am the executive director of Aluther AI
[00:00:08.760 --> 00:00:11.480]   and head of interpretability research.
[00:00:11.480 --> 00:00:14.240]   And I'm here to talk about how interpretability research can
[00:00:14.240 --> 00:00:16.920]   help us build better models, especially
[00:00:16.920 --> 00:00:18.160]   large language models.
[00:00:18.160 --> 00:00:19.840]   A little bit about our organization,
[00:00:19.840 --> 00:00:22.480]   Aluther AI is a nonprofit research institute
[00:00:22.480 --> 00:00:25.240]   that focuses on large scale AI research.
[00:00:25.240 --> 00:00:27.840]   We're best known for open source work,
[00:00:27.840 --> 00:00:30.880]   such as the data set the pile, large language models,
[00:00:30.880 --> 00:00:35.280]   such as g2bJ, g2bNeoX, and the VQBN clip text
[00:00:35.280 --> 00:00:37.080]   to image synthesis technique.
[00:00:37.080 --> 00:00:40.280]   We also do a lot of work beyond natural language processing
[00:00:40.280 --> 00:00:43.680]   on interpretability, alignment, and applications
[00:00:43.680 --> 00:00:45.960]   of large scale AI to other domains,
[00:00:45.960 --> 00:00:48.440]   such as text to image, like I mentioned,
[00:00:48.440 --> 00:00:50.200]   or medical processing.
[00:00:50.200 --> 00:00:53.400]   We care a lot about the scientific community
[00:00:53.400 --> 00:00:56.960]   and promoting the ability to do research on large scale AI
[00:00:56.960 --> 00:00:59.560]   models around the world, even if you don't necessarily
[00:00:59.560 --> 00:01:02.960]   have the computational resources to train them yourselves.
[00:01:02.960 --> 00:01:05.320]   And that is the major drive of why
[00:01:05.320 --> 00:01:09.920]   we are bullish on public release and giving people
[00:01:09.920 --> 00:01:11.720]   more access to language models.
[00:01:11.720 --> 00:01:13.920]   So there's a couple of different types of AI interpretability
[00:01:13.920 --> 00:01:14.440]   research.
[00:01:14.440 --> 00:01:16.400]   It's a rather broad field.
[00:01:16.400 --> 00:01:18.200]   Some people do research in what's
[00:01:18.200 --> 00:01:19.880]   called post hoc explanations, where
[00:01:19.880 --> 00:01:21.800]   they take a pre-trained model and they
[00:01:21.800 --> 00:01:26.280]   try to explain why it predicted the outcome
[00:01:26.280 --> 00:01:29.480]   or why it produced the generation that it did.
[00:01:29.480 --> 00:01:33.240]   There are techniques on developing simpler but easier
[00:01:33.240 --> 00:01:37.680]   to interpret models that kind of mimic the behavior of more
[00:01:37.680 --> 00:01:39.160]   complicated AI models.
[00:01:39.160 --> 00:01:41.760]   You can basically think of these as fancy linear regressions,
[00:01:41.760 --> 00:01:44.520]   where a linear model is not a phenomenal model
[00:01:44.520 --> 00:01:48.120]   for the phenomena in question, but it's much easier
[00:01:48.120 --> 00:01:50.720]   to interpret and you can analyze how much of the variance
[00:01:50.720 --> 00:01:51.200]   it explains.
[00:01:51.200 --> 00:01:56.160]   And there are similar conceptual techniques in AI research.
[00:01:56.160 --> 00:01:57.720]   The two types of interpretability
[00:01:57.720 --> 00:02:00.160]   that we do mostly at Luther AI, though,
[00:02:00.160 --> 00:02:02.840]   are mechanistic interpretability and interpretability
[00:02:02.840 --> 00:02:03.960]   over time.
[00:02:03.960 --> 00:02:06.640]   Mechanistic interpretability is about taking a pre-trained
[00:02:06.640 --> 00:02:10.200]   model and reverse engineering the internal decision-making
[00:02:10.200 --> 00:02:13.800]   process of how it produced the generation it did
[00:02:13.800 --> 00:02:16.800]   and figuring out what that algorithm actually
[00:02:16.800 --> 00:02:18.560]   is on the inside.
[00:02:18.560 --> 00:02:21.680]   Interpretability over time, by contrast to these other areas,
[00:02:21.680 --> 00:02:23.480]   looks at models as dynamic objects,
[00:02:23.480 --> 00:02:25.560]   not as these pre-trained artifacts that
[00:02:25.560 --> 00:02:27.480]   exist in the world, but as something
[00:02:27.480 --> 00:02:29.960]   that was grown in a lab and trained and developed
[00:02:29.960 --> 00:02:31.120]   over the course of training.
[00:02:31.120 --> 00:02:34.280]   And in particular, it looks at how model properties emerge,
[00:02:34.280 --> 00:02:37.200]   evolve, and change over the course of training
[00:02:37.200 --> 00:02:40.040]   with an eye towards what we, as people who train language
[00:02:40.040 --> 00:02:43.800]   models, can do to get language models to behave more the way
[00:02:43.800 --> 00:02:45.320]   that we want them to.
[00:02:45.320 --> 00:02:47.840]   And so this is the focus of my talk,
[00:02:47.840 --> 00:02:50.600]   how we can use these ideas from interpreting models
[00:02:50.600 --> 00:02:53.720]   across training to build better language models.
[00:02:53.720 --> 00:02:55.320]   So I have three case studies that I'm
[00:02:55.320 --> 00:02:56.400]   going to be talking about.
[00:02:56.400 --> 00:02:58.800]   One of them is on de-biasing large language models.
[00:02:58.800 --> 00:03:01.400]   It's relatively well-known that large language models
[00:03:01.400 --> 00:03:06.560]   are able to mimic and even exacerbate biases,
[00:03:06.560 --> 00:03:09.000]   social biases, found in their training corpus.
[00:03:09.000 --> 00:03:12.000]   It would be nice if we could ameliorate that.
[00:03:12.000 --> 00:03:14.760]   The second case study is on rare fact learning
[00:03:14.760 --> 00:03:17.800]   and improving rare fact learning using recent research
[00:03:17.800 --> 00:03:22.680]   on how the actual frequency of information in training corpus
[00:03:22.680 --> 00:03:25.120]   affect the behavior of models.
[00:03:25.120 --> 00:03:27.760]   And finally, I'm going to talk about some recent attacks
[00:03:27.760 --> 00:03:29.880]   and defenses that can be employed
[00:03:29.880 --> 00:03:32.840]   to protect language model APIs or language model apps
[00:03:32.840 --> 00:03:35.040]   from prompt-based attacks.
[00:03:35.040 --> 00:03:37.520]   So starting with de-biasing large language models,
[00:03:37.520 --> 00:03:40.520]   as I said, language models often exhibit and then exacerbate
[00:03:40.520 --> 00:03:43.240]   the social biases found in their training corpus.
[00:03:43.240 --> 00:03:45.320]   It would be really nice if, since these things are
[00:03:45.320 --> 00:03:48.760]   so expensive to train, we could ameliorate or fix
[00:03:48.760 --> 00:03:51.520]   this by retraining them a small amount
[00:03:51.520 --> 00:03:55.000]   or fine-tuning them further to decrease the amount of social
[00:03:55.000 --> 00:03:55.960]   bias that they exhibit.
[00:03:55.960 --> 00:03:58.000]   So the recent research on this at Eleutherae
[00:03:58.000 --> 00:04:00.520]   has looked at whether intervening on the training
[00:04:00.520 --> 00:04:02.640]   data is a effective way to do this.
[00:04:02.640 --> 00:04:05.880]   And unfortunately, so far, we've found mostly negative results.
[00:04:05.880 --> 00:04:07.840]   What you see in this plot is two models
[00:04:07.840 --> 00:04:10.160]   that we trained, one with 400 million parameters
[00:04:10.160 --> 00:04:12.360]   and one with 1.4 billion parameters,
[00:04:12.360 --> 00:04:16.800]   and a measure of the stereotype bias for gender
[00:04:16.800 --> 00:04:19.600]   over the course of training, which is along the x-axis.
[00:04:19.600 --> 00:04:21.880]   And the solid line is the original training.
[00:04:21.880 --> 00:04:24.720]   And then we went and we took the last 14% of the training data
[00:04:24.720 --> 00:04:27.200]   and changed all the masculine pronouns and references
[00:04:27.200 --> 00:04:29.040]   to feminine pronouns and references.
[00:04:29.040 --> 00:04:31.640]   And what we see is that the stereotype bias doesn't
[00:04:31.640 --> 00:04:32.760]   actually go down very much.
[00:04:32.760 --> 00:04:34.000]   It stays roughly the same.
[00:04:34.000 --> 00:04:35.500]   And we found that this is really not
[00:04:35.500 --> 00:04:37.600]   an effective technique for ameliorating bias
[00:04:37.600 --> 00:04:39.240]   in large language models.
[00:04:39.240 --> 00:04:42.520]   But we are excited about this general research direction.
[00:04:42.520 --> 00:04:44.000]   And we're looking to build on this
[00:04:44.000 --> 00:04:45.800]   to hopefully develop other techniques that
[00:04:45.800 --> 00:04:47.280]   will work more effectively.
[00:04:47.280 --> 00:04:48.960]   Here we see the same kind of analysis
[00:04:48.960 --> 00:04:51.480]   but with different models, with a smaller model, 70 million
[00:04:51.480 --> 00:04:53.960]   parameters, and a much larger model at 6.9 billion
[00:04:53.960 --> 00:04:54.880]   parameters.
[00:04:54.880 --> 00:04:57.120]   One really interesting thing that this plot brings out
[00:04:57.120 --> 00:04:59.120]   is that larger models tend to be much more
[00:04:59.120 --> 00:05:01.200]   biased than smaller models.
[00:05:01.200 --> 00:05:02.760]   We tend to think of larger models
[00:05:02.760 --> 00:05:05.280]   as more desirable or more powerful.
[00:05:05.280 --> 00:05:08.080]   But it's probably most accurate to say that larger models are
[00:05:08.080 --> 00:05:10.560]   better at noticing latent patterns.
[00:05:10.560 --> 00:05:13.480]   And unfortunately, sexism, racism, et cetera,
[00:05:13.480 --> 00:05:16.960]   are latent patterns with high explanatory behavior.
[00:05:16.960 --> 00:05:19.800]   And so they're patterns that the language model
[00:05:19.800 --> 00:05:23.440]   is able to pick up on more and more aggressively
[00:05:23.440 --> 00:05:24.600]   as it gets larger.
[00:05:24.600 --> 00:05:26.640]   Rare facts learning is another domain
[00:05:26.640 --> 00:05:29.920]   that has been a topic of a lot of interpretability research
[00:05:29.920 --> 00:05:30.680]   recently.
[00:05:30.680 --> 00:05:33.680]   People have become very interested in understanding
[00:05:33.680 --> 00:05:35.920]   how the actual frequency of information
[00:05:35.920 --> 00:05:37.680]   in the pre-training corpus affects
[00:05:37.680 --> 00:05:39.840]   the ability of a language model to answer questions
[00:05:39.840 --> 00:05:42.080]   about the contents of its corpus.
[00:05:42.080 --> 00:05:44.160]   So what you see here is the Bloom language model,
[00:05:44.160 --> 00:05:48.280]   which is a series of models up to 176 billion parameters that
[00:05:48.280 --> 00:05:51.120]   was streamed by the Big Science Research
[00:05:51.120 --> 00:05:53.360]   Workshop, which was a large-scale collaboration
[00:05:53.360 --> 00:05:54.600]   that I was a part of.
[00:05:54.600 --> 00:05:57.160]   And from this paper, what we see is
[00:05:57.160 --> 00:05:59.640]   that as the amount of documents that
[00:05:59.640 --> 00:06:02.400]   contain information relevant to particular facts
[00:06:02.400 --> 00:06:04.880]   increases, the performance on questions
[00:06:04.880 --> 00:06:07.320]   about those documents increases as well.
[00:06:07.320 --> 00:06:09.680]   And for very small frequencies-- so
[00:06:09.680 --> 00:06:14.160]   for if the frequency is single digits or double digits--
[00:06:14.160 --> 00:06:15.520]   the accuracy is quite poor.
[00:06:15.520 --> 00:06:17.600]   So despite the fact that the average accuracy
[00:06:17.600 --> 00:06:20.400]   of the 176 billion parameter model is, I believe,
[00:06:20.400 --> 00:06:24.240]   like 55%-ish, the accuracy on these rare facts
[00:06:24.240 --> 00:06:28.720]   is much, much lower, sometimes as low as 25% or 30%.
[00:06:28.720 --> 00:06:31.360]   And it would be nice to be able to take this information
[00:06:31.360 --> 00:06:33.040]   and use it to produce language models that
[00:06:33.040 --> 00:06:35.760]   are better at addressing these rare fact questions.
[00:06:35.760 --> 00:06:37.400]   So one approach you can take is simply
[00:06:37.400 --> 00:06:38.560]   making the facts less rare.
[00:06:38.560 --> 00:06:41.400]   You can insert more training data into the corpus,
[00:06:41.400 --> 00:06:43.240]   and you can watch that as you do this,
[00:06:43.240 --> 00:06:44.840]   the performance on those questions
[00:06:44.840 --> 00:06:46.400]   does go up substantially.
[00:06:46.400 --> 00:06:48.600]   And this is one way where if you know ahead of time
[00:06:48.600 --> 00:06:51.680]   what kinds of problems are most important for your language
[00:06:51.680 --> 00:06:55.160]   model to know, you can change your training corpus.
[00:06:55.160 --> 00:06:58.200]   You can take your large web scrape or whatever you're using
[00:06:58.200 --> 00:07:00.840]   and increase the frequency of information
[00:07:00.840 --> 00:07:03.400]   about the particular facts that you know that you care about.
[00:07:03.400 --> 00:07:05.320]   If you don't know ahead of time what these are
[00:07:05.320 --> 00:07:07.600]   and you just want there to be a more even distribution
[00:07:07.600 --> 00:07:09.360]   of accuracy, another technique you
[00:07:09.360 --> 00:07:12.720]   can use that has become popular for a wide variety of reasons
[00:07:12.720 --> 00:07:14.920]   recently is retrieval.
[00:07:14.920 --> 00:07:16.600]   Retrieval augmentation is something
[00:07:16.600 --> 00:07:18.140]   that you can do with a language model
[00:07:18.140 --> 00:07:20.960]   where you give it the ability to basically consult
[00:07:20.960 --> 00:07:23.960]   with its training corpus before answering questions.
[00:07:23.960 --> 00:07:26.760]   And so this plot shows another language model series,
[00:07:26.760 --> 00:07:28.800]   this time the GTP Neo models, that
[00:07:28.800 --> 00:07:30.560]   have been augmented with retrieval
[00:07:30.560 --> 00:07:33.120]   and evaluated on the same question and answers.
[00:07:33.120 --> 00:07:36.360]   And you see that the correlation between training frequency
[00:07:36.360 --> 00:07:39.240]   and model performance has changed substantially.
[00:07:39.240 --> 00:07:40.600]   First of all, it's a lot less.
[00:07:40.600 --> 00:07:44.200]   And second of all, it actually starts off higher and decreases
[00:07:44.200 --> 00:07:46.240]   as document frequency increases.
[00:07:46.240 --> 00:07:49.200]   We believe that this is because the very high frequency
[00:07:49.200 --> 00:07:51.360]   questions are, for some reason, substantially
[00:07:51.360 --> 00:07:53.120]   harder or confusing.
[00:07:53.120 --> 00:07:54.600]   As you can tell, the human baseline
[00:07:54.600 --> 00:07:57.240]   also has this pattern where it starts off high
[00:07:57.240 --> 00:07:58.360]   and decreases over time.
[00:07:58.360 --> 00:08:01.040]   So the problems that the model are getting worse at here
[00:08:01.040 --> 00:08:02.800]   are actually problems that humans are also
[00:08:02.800 --> 00:08:04.120]   worse at, relatively speaking.
[00:08:04.120 --> 00:08:06.040]   And the third application I want to talk about
[00:08:06.040 --> 00:08:08.720]   is defending APIs and apps against attacks.
[00:08:08.720 --> 00:08:12.600]   So one thing that's become very in vogue in the AI research
[00:08:12.600 --> 00:08:15.800]   community recently is that you can write prompts
[00:08:15.800 --> 00:08:19.640]   that, when you put them in front of questions,
[00:08:19.640 --> 00:08:22.520]   will substantially change the behavior of language models
[00:08:22.520 --> 00:08:24.120]   in a negative fashion.
[00:08:24.120 --> 00:08:26.200]   So here, I've omitted the actual prompt,
[00:08:26.200 --> 00:08:28.600]   but this is an example where they're
[00:08:28.600 --> 00:08:32.120]   taking a API developed and deployed by OpenAI.
[00:08:32.120 --> 00:08:37.520]   This is the text of G3, also colloquially known as GTP 3.5.
[00:08:37.520 --> 00:08:39.840]   And it has been specifically fine-tuned
[00:08:39.840 --> 00:08:43.520]   to be more factually correct, as well as decreasing
[00:08:43.520 --> 00:08:46.600]   its propensity for reproducing stereotypes
[00:08:46.600 --> 00:08:49.160]   and conspiratorial thinking.
[00:08:49.160 --> 00:08:51.520]   So what we see here is that, when
[00:08:51.520 --> 00:08:53.240]   prompted in a particular fashion,
[00:08:53.240 --> 00:08:55.200]   the model actually produces two outputs.
[00:08:55.200 --> 00:08:57.520]   It produces the normal output, labeled here
[00:08:57.520 --> 00:09:01.640]   as GTP, which is what you would expect or want the model to say
[00:09:01.640 --> 00:09:04.800]   if asked if the 2020 election was stolen.
[00:09:04.800 --> 00:09:07.680]   And then DAN is the representation
[00:09:07.680 --> 00:09:14.680]   of the much more conspiratorial and aggressive-minded language
[00:09:14.680 --> 00:09:15.320]   model.
[00:09:15.320 --> 00:09:18.480]   And here, it promotes a common conspiracy theory
[00:09:18.480 --> 00:09:21.560]   in the United States that the 2020 election in the US
[00:09:21.560 --> 00:09:23.960]   was stolen, and there's widespread fraud
[00:09:23.960 --> 00:09:25.240]   and election ratings.
[00:09:25.240 --> 00:09:28.480]   Not true, but this is something that some people do believe.
[00:09:28.480 --> 00:09:31.920]   And it's something that the adversarial prompt
[00:09:31.920 --> 00:09:33.360]   is able to get the model to output,
[00:09:33.360 --> 00:09:37.640]   despite the fact that the model is designed to do that less.
[00:09:37.640 --> 00:09:39.680]   So how can we actually make progress
[00:09:39.680 --> 00:09:41.800]   on this using interpretability?
[00:09:41.800 --> 00:09:44.760]   One thing that is very interesting to researchers
[00:09:44.760 --> 00:09:47.080]   at Duluth or AI is that language models
[00:09:47.080 --> 00:09:49.840]   are sequential objects on a pretty fundamental level.
[00:09:49.840 --> 00:09:51.920]   They have a fixed number of layers,
[00:09:51.920 --> 00:09:53.640]   and they don't use recurrent layers.
[00:09:53.640 --> 00:09:56.400]   And so when you have a 40-layer language model,
[00:09:56.400 --> 00:09:59.040]   there's a very real sense in which there's only 40 things
[00:09:59.040 --> 00:10:01.200]   that the model does when coming up with the answer.
[00:10:01.200 --> 00:10:03.760]   You can view these and analyze these as discrete steps
[00:10:03.760 --> 00:10:07.080]   and use that to understand where language model behavior
[00:10:07.080 --> 00:10:08.920]   and predictions come from.
[00:10:08.920 --> 00:10:11.080]   And so we've started looking at what happens
[00:10:11.080 --> 00:10:17.640]   if you extract from layer K what the model thinks after layer K
[00:10:17.640 --> 00:10:19.120]   is the answer to the questions.
[00:10:19.120 --> 00:10:21.160]   And so we've tried this with adversarial prompts,
[00:10:21.160 --> 00:10:22.880]   and what we found is quite surprising.
[00:10:22.880 --> 00:10:25.400]   Here, the green line represents a few-shot example,
[00:10:25.400 --> 00:10:28.200]   and the red line represents a prompt injection
[00:10:28.200 --> 00:10:30.560]   where we're inserting a prompt that is designed
[00:10:30.560 --> 00:10:34.000]   to cause the model to answer the problem incorrectly.
[00:10:34.000 --> 00:10:36.200]   And so we do see at the end the model performance
[00:10:36.200 --> 00:10:38.640]   has dropped to zero, but shortly before that,
[00:10:38.640 --> 00:10:40.200]   it is relatively high.
[00:10:40.200 --> 00:10:44.960]   And sometimes it's as high as the few-shot prompt.
[00:10:44.960 --> 00:10:46.800]   So what we find in these examples,
[00:10:46.800 --> 00:10:48.880]   at least for this particular type of attack,
[00:10:48.880 --> 00:10:52.400]   is that the behavior of the attack being successful
[00:10:52.400 --> 00:10:55.120]   only actually affects the last couple layers of the model.
[00:10:55.120 --> 00:10:58.920]   And before that, the model is able to answer the question
[00:10:58.920 --> 00:10:59.720]   reasonably.
[00:10:59.720 --> 00:11:01.640]   So this is something that we are investigating
[00:11:01.640 --> 00:11:05.200]   as a technique for identifying attacks against APIs
[00:11:05.200 --> 00:11:08.000]   and language modeling apps, as well as potentially
[00:11:08.000 --> 00:11:12.640]   in the future using this to avoid answering those questions
[00:11:12.640 --> 00:11:15.520]   or avoid being fooled by those prompts altogether.
[00:11:15.520 --> 00:11:19.280]   When I tell people that I do interpretability research,
[00:11:19.280 --> 00:11:21.480]   oftentimes the response is, well, that's really cool,
[00:11:21.480 --> 00:11:24.280]   but what kind of impact does it have?
[00:11:24.280 --> 00:11:25.800]   And I think that this has--
[00:11:25.800 --> 00:11:28.240]   I hope this has given you a view into some
[00:11:28.240 --> 00:11:30.560]   of why people should care about interpretability research
[00:11:30.560 --> 00:11:32.040]   beyond it's just cool.
[00:11:32.040 --> 00:11:34.800]   It can teach us important things about how language models work
[00:11:34.800 --> 00:11:36.760]   and why they behave the way they do.
[00:11:36.760 --> 00:11:38.960]   And for the practitioners out there,
[00:11:38.960 --> 00:11:41.120]   it can also teach us how to build more desirable language
[00:11:41.120 --> 00:11:41.620]   models.
[00:11:41.620 --> 00:11:44.160]   It can give us tools for designing better language
[00:11:44.160 --> 00:11:48.000]   models or designing better apps based on language models that
[00:11:48.000 --> 00:11:50.920]   will behave more the way we want them to.
[00:11:50.920 --> 00:11:52.640]   Thank you very much for coming to my talk.
[00:11:52.640 --> 00:11:55.000]   If you're interested in learning more about LutherAI,
[00:11:55.000 --> 00:11:57.600]   you can visit our website or follow us on Twitter.
[00:11:57.600 --> 00:12:00.360]   I want to give a huge thank you to Weights & Biases, as well
[00:12:00.360 --> 00:12:03.560]   as the various sponsors of our organization that enables
[00:12:03.560 --> 00:12:06.360]   us to do the research that we do.
[00:12:06.360 --> 00:12:09.720]   [MUSIC PLAYING]
[00:12:09.720 --> 00:12:12.480]   (engaging music)

