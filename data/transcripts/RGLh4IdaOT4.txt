
[00:00:00.000 --> 00:00:06.640]   Hello, good morning. I'm Carrie. I'm the founding PM of WMB and I'm really excited to get to share
[00:00:06.640 --> 00:00:14.480]   this awesome webinar on Model CI with you. I'm joined today by Hough, who's one of our fantastic
[00:00:14.480 --> 00:00:20.960]   sales machine learning engineers. And Hough, I really appreciate the work you've done to
[00:00:20.960 --> 00:00:28.400]   build a demo for Model CI CD today. I'm so excited to tell the story together. And I'll start out
[00:00:28.400 --> 00:00:33.760]   with a short overview of the product and the tools that we offer. And then I'll pass it over to Hough
[00:00:33.760 --> 00:00:41.280]   to get into the live demo here today. So thank you so much, Hough, for joining. Now I'll pull
[00:00:41.280 --> 00:00:49.760]   up some slides and give a high level overview of the product and our platform. So first off,
[00:00:49.760 --> 00:00:56.960]   zooming out, Weights and Biases is a whole suite of tools for machine learning. And so that includes
[00:00:56.960 --> 00:01:02.720]   tools around models, prompts, and then our high level platform to allow you to do things like
[00:01:02.720 --> 00:01:08.080]   track your models and datasets. So I'll start with models briefly. And this is something you're
[00:01:08.080 --> 00:01:14.880]   likely more familiar with. This is our product that we started building in 2018. So first off,
[00:01:14.880 --> 00:01:20.960]   you're tracking experiments. So as you train a model, tracking the progress and seeing if it's
[00:01:20.960 --> 00:01:25.760]   better than the previous model you trained. Then you're versioning and preparing your data. So
[00:01:25.760 --> 00:01:31.680]   you're looking at things like, is there a class imbalance in my dataset? Or is this model
[00:01:31.680 --> 00:01:39.040]   struggling on a certain class or a certain type of data? Then you're collaborating. So you're
[00:01:39.040 --> 00:01:45.600]   identifying high level insights in WMB and then sharing those with your team. And finally,
[00:01:45.600 --> 00:01:50.160]   you're having a system of records, a central place where you can see all of the different
[00:01:51.040 --> 00:01:57.440]   models that you've trained in one page. And you can identify what the best model is,
[00:01:57.440 --> 00:01:59.920]   and then make sure that everyone's on the same page about that.
[00:01:59.920 --> 00:02:07.200]   This also pairs well with other tools. So a key value at WMB is being able to integrate
[00:02:07.200 --> 00:02:11.280]   with other systems or other tools that you're already using. So that could be,
[00:02:11.280 --> 00:02:17.280]   say you're using PyTorch Lightning or LangChain or another tool in the ecosystem. Our goal is to
[00:02:17.280 --> 00:02:23.840]   make it as seamless as possible to easily pull an integration off the shelf and track the results.
[00:02:23.840 --> 00:02:29.040]   We also integrate with whatever infrastructure you're using. So say you have a machine under
[00:02:29.040 --> 00:02:35.440]   your desk, say you have a huge cluster in the cloud in AWS or GCP or Azure or somewhere else,
[00:02:35.440 --> 00:02:39.840]   we'll integrate easily with all of those destinations, make it easy to track your
[00:02:39.840 --> 00:02:45.760]   results. So let's talk about prompts. And this is the focus of today's discussion. This is around
[00:02:45.760 --> 00:02:53.200]   LLMs and how to track the progress you're making there. So with prompts, we have kind of two themes.
[00:02:53.200 --> 00:03:00.640]   The first theme is fine tuning. So how can you efficiently fine tune a huge model and make it
[00:03:00.640 --> 00:03:06.320]   fit your specific business use case? Now, here we offer integrations with Hugging Face and PyTorch.
[00:03:06.320 --> 00:03:11.760]   So making it really easy to track the results of fine tuning. And then we log back metrics
[00:03:11.760 --> 00:03:17.360]   and samples from training. And so Hof will touch on this in the demo today. We also have prompt
[00:03:17.360 --> 00:03:23.600]   engineering tools. So if you're iterating on prompts, if you're tweaking the system prompt
[00:03:23.600 --> 00:03:30.240]   to try to make it generate better results for your users, we can actually track the progress of that.
[00:03:30.240 --> 00:03:35.200]   And we do it really easily with something like a LangChain integration, where we can even show you
[00:03:35.200 --> 00:03:41.920]   the stages of each step that the model takes when it takes the input and then gives you back the
[00:03:41.920 --> 00:03:48.480]   output. So both of these sides of both fine tuning and prompt engineering are captured here in WB
[00:03:48.480 --> 00:03:55.600]   prompts. So I'll touch on traces first. So we have this integration with OpenAI evals that allows
[00:03:55.600 --> 00:04:03.840]   you to see this central dashboard of the money that you're spending on using their API, the
[00:04:03.840 --> 00:04:09.440]   accuracy, and you can visualize these results as you iterate. And so this is an example, a screenshot
[00:04:09.440 --> 00:04:15.200]   from a report that was generated directly from WB. And Hof will actually touch on exactly how you can
[00:04:15.200 --> 00:04:21.760]   generate a report automatically. So each time you run a new evaluation, you could get a standardized
[00:04:21.760 --> 00:04:28.880]   view like this one. We also have tools for understanding and debugging chains. So looking at
[00:04:28.880 --> 00:04:34.160]   each of the inputs and the output that the model produced and whether or not it was a success or
[00:04:34.160 --> 00:04:40.880]   failure. So being able to visualize the results of evaluation for a different prompt. And then
[00:04:40.880 --> 00:04:46.880]   I'm diving in here to this little trace timeline. And what that allows you to do is actually drill
[00:04:46.880 --> 00:04:56.000]   down and see each step of the process. So what happened when, say, the OpenAI API was called,
[00:04:56.000 --> 00:05:01.600]   what the inputs and outputs were, or if it was using a tool like a calculator. So we capture
[00:05:01.600 --> 00:05:06.640]   all of that, which means it's a lot easier to debug and see where things are going wrong and say
[00:05:06.640 --> 00:05:13.280]   your use of link chain. Now, we also have this monitoring tool. And there are a lot of problems
[00:05:13.280 --> 00:05:18.720]   with monitoring LMs in production. This is a hard, frustrating thing. And it's really hard to
[00:05:18.720 --> 00:05:24.480]   systematically evaluate what poor performance looks like. So to know if one model is doing worse than
[00:05:24.480 --> 00:05:30.320]   another model once you deploy it. It's also really expensive. Deploying these models requires a lot
[00:05:30.320 --> 00:05:36.240]   of infrastructure and also new development patterns. There's also constantly evolving
[00:05:36.240 --> 00:05:40.880]   foundations. So you can't just set up a model and leave it and be sure that it will continue
[00:05:40.880 --> 00:05:47.040]   achieving the performance it got when it was deployed. It's also hard to capture and combine
[00:05:47.040 --> 00:05:53.040]   inference data. So how do we address all of these challenges? Well, with production monitoring in
[00:05:53.040 --> 00:06:00.960]   Weights and Biases, you can use this central dashboard to capture the results of how the
[00:06:00.960 --> 00:06:08.640]   model is performing and see the centralized dashboard with exploration in the context of
[00:06:08.640 --> 00:06:12.560]   the larger development that you've been doing. So you don't need to jump from tool to tool to
[00:06:12.560 --> 00:06:18.080]   understand how the model is performing. We also make it easy to monitor the cost of using that
[00:06:18.080 --> 00:06:24.800]   large language model. So how much are you paying OpenAI or Anthropic for using their API to
[00:06:24.800 --> 00:06:31.840]   serve predictions? It's also integrated directly with the rest of our experiment tracking tools
[00:06:31.840 --> 00:06:38.240]   that I just touched on. So it's a lot easier to go from one stage of your development life cycle
[00:06:38.240 --> 00:06:44.720]   to another. And for production data, this is where we can capture something that looks a little bit
[00:06:44.720 --> 00:06:50.800]   like Datadog, where we have an observability page for a model that's in production. We give you this
[00:06:50.800 --> 00:06:59.360]   centralized dashboard that is scalable for LLM applications and unifies those two chunks,
[00:06:59.360 --> 00:07:05.120]   the experiments piece and the production piece. So as you're iterating and developing, and then
[00:07:05.120 --> 00:07:13.280]   also once the model's in production. And finally, this is a platform that has defaults that we'll
[00:07:13.280 --> 00:07:18.000]   set up that are driven by the use cases we're seeing with customers in the field, but it's also
[00:07:18.000 --> 00:07:22.560]   very configurable. And so the goal here is for you to be able to customize exactly the types of
[00:07:22.560 --> 00:07:27.840]   charts and alerting that you'd want to see in production. So next I'll touch on the model
[00:07:27.840 --> 00:07:34.080]   registry. So how are we capturing models in this system? So we have these challenges where each of
[00:07:34.080 --> 00:07:40.320]   the different members of the team need something slightly different. And so I'll touch on first,
[00:07:40.320 --> 00:07:47.280]   the MLOps persona. They need a source of truth centrally for the latest best models. And right
[00:07:47.280 --> 00:07:52.480]   now, if a practitioner is logging a new model, training a new model, it's really easy for that
[00:07:52.480 --> 00:07:56.800]   to get lost. Maybe it goes to an S3 bucket somewhere. Maybe they have like a Google sheet
[00:07:56.800 --> 00:08:01.520]   with links to different models, but it's not totally clear where the very best model is.
[00:08:01.520 --> 00:08:08.080]   So MLOps persona needs that central source of truth. The ML exec needs to see all the model
[00:08:08.080 --> 00:08:13.760]   tasks. So what are the different projects people are working on and how are they performing? Do we
[00:08:13.760 --> 00:08:19.680]   have a model for a chatbot that's in production right now? And how is it doing? Is it serving
[00:08:19.680 --> 00:08:25.680]   predictions well? How is it being iterated on by the team? On the compliance side, this is really
[00:08:25.680 --> 00:08:32.640]   critical for some of our customers who are in a space that's highly governed by a lot of compliance
[00:08:32.640 --> 00:08:38.640]   restrictions like banking or anything in the financial sector or healthcare sector. Being
[00:08:38.640 --> 00:08:44.880]   able to enforce data governance policies is really critical to be able to meaningfully use machine
[00:08:44.880 --> 00:08:50.480]   learning in production. So having a tool that helps enforce that will be valuable for both
[00:08:50.480 --> 00:08:56.960]   compliance and also the execs. On the PM side, being able to see documentation for the models.
[00:08:56.960 --> 00:09:03.040]   So understanding kind of the model card of exactly what went into the model, what the caveats are,
[00:09:03.040 --> 00:09:07.280]   what the restrictions are, what the expected input and output shape is. And finally,
[00:09:07.280 --> 00:09:13.120]   the practitioner is constantly iterating, building new models and needs to publish those very best
[00:09:13.120 --> 00:09:19.280]   models into a central place. And so how do we address this in the model workflow? So at a high
[00:09:19.280 --> 00:09:25.840]   level, and you'll recognize this pattern in your own context, you start off with experimentation.
[00:09:25.840 --> 00:09:30.960]   And we have this now handoff process to promote the model into the registry. Then we have the
[00:09:30.960 --> 00:09:36.560]   staging step. So that takes that new promoted model and tests it and make sure that that can be
[00:09:36.560 --> 00:09:43.600]   consumed by say a team that's doing deeper evaluation or testing for different critical
[00:09:43.600 --> 00:09:51.520]   cases. Now automation, that can allow the process of taking a model and deploying it to be really
[00:09:51.520 --> 00:09:58.080]   standardized and clean across each member of the team so that there's a centralized action for
[00:09:58.080 --> 00:10:02.080]   deploying to staging to interact with the model, as well as deploying to production.
[00:10:02.080 --> 00:10:09.360]   And we'll also really touch on controls here. So in the context of automation and then deploying
[00:10:09.360 --> 00:10:14.320]   to production endpoints, we really care about who's allowed to do that. And so Hough will touch
[00:10:14.320 --> 00:10:19.600]   on how we can control the access there to make sure only the right people can deploy a model
[00:10:19.600 --> 00:10:25.760]   to a production service. So now what does the model registry look like? So how am I capturing
[00:10:25.760 --> 00:10:33.840]   this overwork flow in a central place? First, we have the central model repository. So all of your
[00:10:33.840 --> 00:10:39.840]   tasks that you're doing at work are centralized in a single page. And you can dive in to see a
[00:10:39.840 --> 00:10:45.840]   model card for each of those tasks. So you can say, OK, give me all the details that I need to
[00:10:45.840 --> 00:10:51.760]   know about this chatbot that we have in production. Then we also have lineage. So say somebody's
[00:10:51.760 --> 00:11:02.240]   updated that we're able to capture the history of what's changed. So in this context, we're able to
[00:11:02.240 --> 00:11:08.720]   see, OK, there are a few different members of the team who've updated the model. And we can trace
[00:11:08.720 --> 00:11:15.360]   back to see where that model came from. So in the context of LLMs, this could be who has fine-tuned
[00:11:15.360 --> 00:11:21.840]   the model on the latest data. When did it change? What was the data set that was used to fine-tune
[00:11:21.840 --> 00:11:27.680]   that model? And then when was it put into production? And finally, for webhooks, we're
[00:11:27.680 --> 00:11:35.680]   able to make this step seamless and automated. So we know that we're able to take the task of
[00:11:35.680 --> 00:11:43.840]   retraining or deploying to staging or production and connect to an external system and make that
[00:11:45.120 --> 00:11:52.320]   action of deployment or retraining automatic. We can also do that with launch jobs. And the
[00:11:52.320 --> 00:11:57.200]   cool thing about launch jobs is it makes it easy to connect to the infrastructure that you're using,
[00:11:57.200 --> 00:12:03.280]   whether that's a cloud cluster or a machine in the office, and run workloads on that. So Hough will
[00:12:03.280 --> 00:12:08.640]   talk about how you can connect to those systems to make it really easy to run automations,
[00:12:08.640 --> 00:12:13.440]   to automate parts of the system. So what are customers saying about this process?
[00:12:14.640 --> 00:12:20.400]   Before using this pipeline of being able to capture models centrally,
[00:12:20.400 --> 00:12:28.800]   it really was tough for folks to manage their models centrally. And so M. Copa highlights here,
[00:12:28.800 --> 00:12:34.160]   "Before using the Model Registry, I managed all of our forecast models in my head," which honestly
[00:12:34.160 --> 00:12:39.920]   is not that uncommon. It's easy to lose track of what the best models are. Another customer we're
[00:12:39.920 --> 00:12:44.240]   working with talks about how they're using the Model Registry as a source of truth and key
[00:12:44.240 --> 00:12:50.400]   component of their backend. A large telecom provider uses model cards and evaluation metrics
[00:12:50.400 --> 00:12:56.320]   in WMB to have everyone from the product management team to the engineering team on the same page.
[00:12:56.320 --> 00:13:02.480]   So getting into the model CI/CD, this is the workflow that we're about to present.
[00:13:02.480 --> 00:13:07.040]   So first, the team members involved, the practitioner, the ML Ops engineer, the team
[00:13:07.040 --> 00:13:12.960]   leader. In the context of LLMs, this also includes people like prompt engineers who are iterating on
[00:13:12.960 --> 00:13:20.000]   the prompts that are being used in production. And so when I say ML practitioner, that includes
[00:13:20.000 --> 00:13:25.840]   that group as well. And for the model lifecycle, you've got fresh data coming in from production,
[00:13:25.840 --> 00:13:31.520]   you have a model to fine tune on that fresh data, and you train, evaluate, document the results,
[00:13:31.520 --> 00:13:36.560]   and then deploy that model to an inference server. And critically, this is a loop, right? So this
[00:13:36.560 --> 00:13:41.760]   goes from being deployed to production to getting fresh data, users are interacting with your model,
[00:13:41.760 --> 00:13:46.400]   and then you have that data to then fine tune or iterate and retrain the model.
[00:13:46.400 --> 00:13:53.920]   And so in terms of tooling, we have tools for each stage of this lifecycle. And critically,
[00:13:53.920 --> 00:14:00.240]   I think the value that Haf is going to walk through in his demo is how each of these components
[00:14:00.240 --> 00:14:05.520]   connect, right? So you're not just training a model, and then you have to use another system
[00:14:05.520 --> 00:14:10.560]   to evaluate or deploy. We make it really easy to step between each stage of this lifecycle,
[00:14:10.560 --> 00:14:17.040]   as well as connect to external tools to do this. So I'm really excited to see your demo today,
[00:14:17.040 --> 00:14:23.040]   Haf. I want to pass it over to you. >> Awesome. Thank you very much,
[00:14:23.040 --> 00:14:28.960]   Kari. Excited to be presenting to everyone today. As Kari mentioned, I'm a success machine
[00:14:28.960 --> 00:14:33.680]   learning engineer at Weights and Biases. But for the purposes of this demo, I'm going to be an ML
[00:14:33.680 --> 00:14:40.160]   engineer at a fictional company called ReviewCo. And so we're presenting on production models here.
[00:14:40.160 --> 00:14:45.120]   So we're actually going to start with a sort of production endpoint, and then go through the
[00:14:45.120 --> 00:14:51.120]   process of looking at sort of newer models that have been developed inside of WNB, using the model
[00:14:51.120 --> 00:14:56.800]   registry to then trigger sort of an updated version of a better performing model, hopefully,
[00:14:56.800 --> 00:15:02.160]   as Kari was discussing. So you can see here, so we started the demo in Postman, a very sort of
[00:15:02.160 --> 00:15:08.720]   production friendly way to look at endpoints. And you can see ReviewCo has this sort of auto
[00:15:08.720 --> 00:15:13.920]   suggest endpoint that we've built out where we're serving a model such that if a user starts one of
[00:15:13.920 --> 00:15:19.280]   our fictional reviews, the chocolate tour was excellent. If they're not finishing that, we can
[00:15:19.280 --> 00:15:24.400]   sort of suggest some reasonable ways to complete that. So sort of the current version of the model,
[00:15:24.400 --> 00:15:27.520]   rather negatively inflected to saying, I'm not sure if it was the best, but it was,
[00:15:27.520 --> 00:15:33.440]   and is sort of continuing on there. So this is sort of our production model. And what we're
[00:15:33.440 --> 00:15:38.960]   going to do now is switch to weights and biases, where our team has been hard at work coming up
[00:15:38.960 --> 00:15:45.120]   with some staging candidates that we might want to consider swapping into that production endpoint.
[00:15:45.120 --> 00:15:50.240]   So we can see here, as Kari was mentioning, model registry, in this case, ReviewCo, just has sort of
[00:15:50.240 --> 00:15:56.080]   one registered model, small scrappy startup getting off the ground. And we can see sort of,
[00:15:56.080 --> 00:16:00.560]   this is really a home base for sort of all of the work that's been going on for this,
[00:16:00.560 --> 00:16:04.400]   you know, review auto complete auto suggest model that we've been working on. You can see we have
[00:16:04.400 --> 00:16:09.520]   some automations, which we'll get to in a second. We also have Slack notifications configured here.
[00:16:09.520 --> 00:16:16.080]   So anytime, you know, a new candidate version of a model is coming out, that sort of goes to a Slack
[00:16:16.080 --> 00:16:21.840]   channel that the team can review. And there's also a description about, you know, the various pieces
[00:16:21.840 --> 00:16:26.320]   of tech that's going on. So currently, I think there was a question in the chat about whether
[00:16:26.320 --> 00:16:32.400]   we're talking about sort of LLM ops, MLM ops, or sort of a mix of those. But in this particular
[00:16:32.400 --> 00:16:40.320]   case, you know, we're taking a sort of off the shelf, open source, LLM OPT, which is open source
[00:16:40.320 --> 00:16:46.320]   by by meta, I think, and then using some of those parameter efficient fine tuning methods, Laura,
[00:16:46.320 --> 00:16:51.760]   specifically, in this case, to create some some fine tuned versions of that, so that we can have
[00:16:51.760 --> 00:16:56.160]   something that's going to, you know, suggest things similar to the kinds of reviews that our
[00:16:56.160 --> 00:17:05.200]   users at review co have done. And then, you know, I think serving models, serving speed for LLMs is
[00:17:05.200 --> 00:17:09.120]   also something that's, you know, particularly top of mind for a lot of customers. And so you can see
[00:17:09.120 --> 00:17:15.520]   here, we have sort of a staging model, and then also this staging C two model version that we've
[00:17:15.520 --> 00:17:22.640]   tagged, where we sort of used this this library C translate to, to compress and quantize that model
[00:17:22.640 --> 00:17:26.480]   and sort of store that version. So I think, you know, a lot of our customers are starting to think
[00:17:26.480 --> 00:17:30.480]   about, you know, how we can have automations and jobs such that if you have, you know, a staging
[00:17:30.480 --> 00:17:36.000]   candidate, let's, you know, do file format conversions, optimizations and save that
[00:17:36.000 --> 00:17:42.080]   version in the model registry as well. So we'll, we'll come back here in a second to adjust these
[00:17:42.080 --> 00:17:47.360]   aliases. But first, I want to talk about the automations linked here, and then go into sort
[00:17:47.360 --> 00:17:53.280]   of where all of this sort of run data was captured inside of WNB. So first off, we can see there's
[00:17:53.280 --> 00:17:58.320]   sort of two automations related to this. The first is what we're sort of going to finish this demo
[00:17:58.320 --> 00:18:04.400]   with, there's an automation for deploying the production model. And so our review co team
[00:18:04.400 --> 00:18:09.040]   has a deployment input stood up, we're seeing, you know, some customers have sort of serverless
[00:18:09.040 --> 00:18:13.200]   technologies, some customers using GitHub actions, or a variety of flavors of ways to
[00:18:13.200 --> 00:18:18.400]   to do a deployment loop like this. But in this case, you know, there's that endpoint, and we have
[00:18:18.400 --> 00:18:25.440]   webhook support, and weights and biases such that anytime someone comes in and adds an alias,
[00:18:25.440 --> 00:18:32.640]   specifically production, that's going to trigger this webhook to deploy the production version of
[00:18:32.640 --> 00:18:38.880]   that model. So we'll sort of see that that happened live in the in the demo today. There's also a
[00:18:38.880 --> 00:18:47.040]   second automation, using a launch job, which has been running as the team has been producing sort
[00:18:47.040 --> 00:18:52.800]   of new candidate models here. And so what this does is listen for the staging alias. And anytime
[00:18:52.800 --> 00:18:59.120]   that happens, this spins up a job to pull that model down, take sort of a larger validation
[00:18:59.120 --> 00:19:04.240]   sort of holdout set that isn't used for training, generate some inferences with that and put that
[00:19:04.240 --> 00:19:09.280]   into one of those reports that that Kerry was showing earlier, which we'll see in the demo.
[00:19:09.280 --> 00:19:14.560]   And the team can sort of use that to determine, you know, should is that we should we do a go
[00:19:14.560 --> 00:19:20.400]   no go decision on putting this model into production. So those are the the two automations
[00:19:20.400 --> 00:19:25.360]   we're going to see come to life here in a second. But, you know, first off, what's sort of happening
[00:19:25.360 --> 00:19:30.160]   in this this WB project, you can see here, there's, you know, 90 runs, our team's been hard at work
[00:19:30.160 --> 00:19:37.520]   on a lot of training jobs, using, you know, Tesla T4 GPUs, we can see, you know, some, you know,
[00:19:37.520 --> 00:19:42.000]   reasonable evaluation loss scores compared to where where they might have been before. And then
[00:19:42.000 --> 00:19:47.200]   because this is using our hugging face integration, you can see here, it's one of our sort of most,
[00:19:47.200 --> 00:19:53.680]   most informative integration. So a lot of configs about all of these training runs are being saved
[00:19:53.680 --> 00:19:59.360]   to WNB. But sort of especially with with LLMs, you know, one of the questions we often get is,
[00:19:59.360 --> 00:20:04.560]   you know, how can we make it easier for data scientists and ML engineers to access those GPUs.
[00:20:04.560 --> 00:20:10.000]   And so for this project, for the most part, whether we were running hyper parameter sweeps,
[00:20:10.000 --> 00:20:17.120]   or sort of one off training runs, we were using launch jobs. And so over here for, you know,
[00:20:17.120 --> 00:20:23.920]   this project, you can see there's sort of two jobs. The first is that sort of model automation,
[00:20:23.920 --> 00:20:28.640]   evaluation job, we were talking about that that's going to run in an automated fashion,
[00:20:28.640 --> 00:20:34.880]   we'll talk about that in a second. There's also this, you know, fine tuning job that we've created
[00:20:34.880 --> 00:20:40.400]   here. And you can click into here. And you can see sort of all the different runs that were
[00:20:40.400 --> 00:20:49.200]   triggered for this. You can also for a launch job, you can also go through and find the specific
[00:20:49.200 --> 00:20:54.960]   code. If you have sort of code saving enabled for a job. So we can see here where we're, you know,
[00:20:54.960 --> 00:21:01.040]   importing some various pytorch and hugging face libraries, you know, setting our WB entities to
[00:21:01.040 --> 00:21:05.360]   review code getting the project right. And then, you know, for our hugging face integrations,
[00:21:05.360 --> 00:21:11.840]   it's sort of a couple lines to set some environment variables to really capture a whole
[00:21:11.840 --> 00:21:17.520]   lot of useful information about these models. So setting one be log model checkpoint, that's going
[00:21:17.520 --> 00:21:21.680]   to make it so that all of these jobs are saving checkpoints. So if anything, we need to be
[00:21:21.680 --> 00:21:26.720]   preempted and resumed later, which is especially a common practice for for LM workflows, that's
[00:21:26.720 --> 00:21:30.560]   going to be taken for us automatically. And it's also going to make it really easy to take the
[00:21:30.560 --> 00:21:36.480]   final version of this model and bring it into the model registry. And then also one be watch is
[00:21:36.480 --> 00:21:43.200]   going to log the the gradients of the models for us as well. But you know, from from a data
[00:21:43.200 --> 00:21:47.920]   scientist, once that's been sort of built up, the really nice thing here is that we can now
[00:21:47.920 --> 00:21:54.160]   launch that directly to GPUs from the UI. So let's say, you know, you want to adjust the learning
[00:21:54.160 --> 00:21:59.520]   rate, you know, up here, you want to use the production version of the data set, which we'll
[00:21:59.520 --> 00:22:04.400]   talk about data set versioning at the end, maybe you want to do, you know, a smaller number of
[00:22:04.400 --> 00:22:11.440]   training ethics. From here in the UI, we sort of have the ability to launch straight to that GPU
[00:22:11.440 --> 00:22:18.880]   instance, the WNB is connected to. And so that's sort of one way that the review co team created
[00:22:18.880 --> 00:22:25.440]   these jobs. The second way was sort of using our hyper parameter sweeps functionality, which
[00:22:25.440 --> 00:22:30.480]   not going to go into that into the webinar today, but happy to have follow on conversations with
[00:22:30.480 --> 00:22:35.840]   folks who are interested in that. So, you know, to the question of how did all of this this data get
[00:22:35.840 --> 00:22:41.120]   here? You know, answer number one is sort of launching to GPUs with this particular job.
[00:22:41.120 --> 00:22:47.200]   Answer number two is that we also sort of running some some evaluation jobs. And so these,
[00:22:47.200 --> 00:22:54.800]   you can see various different runs anytime staging candidate was added to the model registry,
[00:22:54.800 --> 00:23:01.520]   we ran this job to evaluate it and save a report. So we can click in here and see sort of what this
[00:23:01.520 --> 00:23:07.040]   model evaluation job is doing. So, you know, one thing it's doing is saving some high level metrics.
[00:23:07.040 --> 00:23:13.680]   So it's, you know, latency is very important. So ran some metrics to see sort of in seconds,
[00:23:13.680 --> 00:23:18.400]   sort of response times, you can see here, the quantized compressed version that we mentioned
[00:23:18.400 --> 00:23:23.520]   using C translate to only take sort of point four seconds for three reviews, the sort of
[00:23:23.520 --> 00:23:29.040]   unoptimized versions are taking almost 10x longer. So that's something we might want to, you know,
[00:23:29.040 --> 00:23:34.640]   take into consideration in terms of which model we want to put into production. Also sort of
[00:23:34.640 --> 00:23:39.520]   saving this table of sort of, you know, those holdouts at prompts, as well as responses for
[00:23:39.520 --> 00:23:46.320]   the various models. But, you know, to make it really easy for the team, what we actually did,
[00:23:46.320 --> 00:23:51.520]   and so we actually have slack notifications for the team such that as soon as a good candidate
[00:23:51.520 --> 00:23:57.040]   model comes, comes up, we're going to take that send a slack notification folks and go in and
[00:23:57.040 --> 00:24:05.040]   view the report. I'm not yet brave enough to demo slack live on a webinar. So we're using WNB's
[00:24:05.040 --> 00:24:12.640]   ability to capture logs. And so, you know, we can see here, this job finishes and we have this URL
[00:24:12.640 --> 00:24:18.880]   for a report. And what's sort of really sleek and something we're starting to see a few customers do
[00:24:18.880 --> 00:24:23.360]   is we can actually use this report, assuming we're logged in with the right permissions
[00:24:23.360 --> 00:24:28.720]   to actually trigger that webhook to update the model that's that's live in production right now.
[00:24:28.720 --> 00:24:34.480]   And so, again, this is this is going to be a WNB report, fairly similar in structure to what,
[00:24:34.480 --> 00:24:39.520]   you know, a lot of folks on this call might have been creating, you know, by clicking around in
[00:24:39.520 --> 00:24:44.480]   the UI, but this was sort of all scripted using our reports API. And so you can see sort of various
[00:24:44.480 --> 00:24:50.480]   sections here. So just sort of a level setting overview at review co like to make things fun.
[00:24:50.480 --> 00:24:56.640]   So aiming for five stars for the model, fun little Easter egg, you can embed Spotify and YouTube in
[00:24:56.640 --> 00:25:03.200]   reports. Best wishes for the staging model. You can also include sort of, you know, key metrics
[00:25:03.200 --> 00:25:07.280]   about sort of all of your training runs here. So, you know, what's the minimum evaluation loss,
[00:25:07.280 --> 00:25:12.320]   training loss, sort of longest training runtime if you're really concerned about, you know, spending
[00:25:12.320 --> 00:25:18.960]   lots of lots of GPU budget. Sort of good standard WNB chart so we can keep, you know, keep a track
[00:25:18.960 --> 00:25:24.000]   of, you know, which model, which one run was performing the best. Maybe should we maybe
[00:25:24.000 --> 00:25:28.960]   continue training for longer? Are we starting to asymptote here? But particularly for, you know,
[00:25:28.960 --> 00:25:35.200]   the candidate staging model, we can sort of see that table here, sort of make those comparisons.
[00:25:35.200 --> 00:25:42.240]   And also, you know, look at some sort of sample predictions here. So having worked in NLP for a
[00:25:42.240 --> 00:25:48.560]   while, this is perhaps one of my favorite features is that long tech span sort of automatically pop
[00:25:48.560 --> 00:25:54.240]   out. And so you can sort of copy and sort of explore them in sort of a very easy, intuitive
[00:25:54.240 --> 00:25:59.520]   way versus working with, you know, spreadsheets, you know, notebooks, things like that.
[00:25:59.520 --> 00:26:05.440]   But we can see here, you know, this is sort of some of the sample prompts. You know, wish I
[00:26:05.440 --> 00:26:09.840]   could have gotten a haircut after spending five. And you can see sort of the current production
[00:26:09.840 --> 00:26:15.520]   model is currently going off and talking about years in the industry. And it's getting into this
[00:26:15.520 --> 00:26:21.760]   sort of loop about being in the same boat, which is maybe not sort of the highest quality output.
[00:26:21.760 --> 00:26:27.520]   Meanwhile, the model that we fine tune is at least, you know, realizing that haircuts are
[00:26:27.520 --> 00:26:32.960]   related to salons. Not particularly sure why the model thinks it wasn't allowed to have a haircut,
[00:26:32.960 --> 00:26:37.680]   but, you know, for, you know, auto suggesting a few of the next words, you know, much more sort
[00:26:37.680 --> 00:26:43.760]   of on target and the same thing for the compressed version of the model as well. So by sort of taking
[00:26:43.760 --> 00:26:49.360]   into account, you know, this, you know, additional context, we might want to go ahead and let's say,
[00:26:49.360 --> 00:26:55.360]   you know, the compressed version, you know, performing 10x faster and seemingly still
[00:26:55.360 --> 00:26:59.760]   having, you know, sort of pretty reasonable results. We might want to use that webhook
[00:26:59.760 --> 00:27:04.960]   to move that into production right now, right here. And so this is, you know, a view of the
[00:27:04.960 --> 00:27:10.720]   model registry inside of this report. And so what we can do is click straight into, you know, the
[00:27:10.720 --> 00:27:17.920]   staging candidate that was compressed, and we can add a production alias to that. And so one thing
[00:27:17.920 --> 00:27:22.720]   you're going to note is that production is actually sort of in a different color. And so, you know,
[00:27:22.720 --> 00:27:29.360]   for aliases that you're going to tie to webhooks and other automations, you're not going to want
[00:27:29.360 --> 00:27:34.800]   probably every member of the team to have the ability to move a model into production. So this
[00:27:34.800 --> 00:27:40.560]   is protected. And since I'm an admin for the model registry, I was able to add that alias,
[00:27:40.560 --> 00:27:47.520]   which is sort of triggering that webhook to fire. And so to sort of prove that we can now go back
[00:27:47.520 --> 00:27:52.400]   to sort of our original example. And, you know, the chocolate tour was excellent. Sort of the
[00:27:52.400 --> 00:27:56.160]   old version of the model was saying, I'm not sure if it was the best, but it was dot dot dot.
[00:27:56.160 --> 00:28:02.640]   And now we can see, you know, having just changed an alias from the model registry,
[00:28:02.640 --> 00:28:07.840]   because we had that webhook up to our sort of endpoint that our team was using. We've now sort
[00:28:07.840 --> 00:28:12.080]   of swapped the live production model at this endpoint to say, you know, but I was a little
[00:28:12.080 --> 00:28:16.720]   disappointed that I didn't get to see, like, probably slightly better quality, a little bit
[00:28:16.720 --> 00:28:22.480]   less negative, you know, maybe some some better, better progress there. So I think from, you know,
[00:28:22.480 --> 00:28:27.520]   looking at those, those example outputs, probably some more time, you know, fine tuning models is
[00:28:27.520 --> 00:28:37.280]   going to really be helpful for review code there. So taking a taking a step back. So what we sort of
[00:28:37.280 --> 00:28:43.840]   went through there was starting with the production model, going inside W and B, seeing what work the
[00:28:43.840 --> 00:28:49.120]   team had already done in promoting a candidate model to production, and sort of, you know,
[00:28:49.120 --> 00:28:54.240]   getting a good loop there. But for, for folks that are, you know, thinking about, you know,
[00:28:54.240 --> 00:29:00.240]   model ci sort of new data coming in, is sort of another key aspect that we're seeing folks use.
[00:29:00.240 --> 00:29:05.360]   And so all the things we talked about in terms of model versioning, they're sort of similar analogs
[00:29:05.360 --> 00:29:10.720]   to that for data sets to sort of close that ci loops, you can see here, you know, for the artifacts
[00:29:10.720 --> 00:29:16.080]   for this project, I have sort of a variety of versions of models, there's sort of this down
[00:29:16.080 --> 00:29:20.720]   sampled version that has sort of less data for sort of fast, quick training runs. And there's
[00:29:20.720 --> 00:29:25.360]   sort of the the current production version of the data set, which you can see sort of split
[00:29:25.360 --> 00:29:33.120]   into train test valid files that was used to train that model. The really interesting thing that you
[00:29:33.120 --> 00:29:40.160]   can do is you can also set automations for that. And so we have an automation set here, such that
[00:29:40.160 --> 00:29:45.600]   anytime, you know, a new version of that reviews data set is created, that's going to launch that
[00:29:45.600 --> 00:29:51.920]   training job as well. So you're not going to even necessarily need the team to come in, click launch
[00:29:51.920 --> 00:29:56.960]   on that, the fine tuning job, like we were talking about, that can just be triggered automatically
[00:29:56.960 --> 00:30:01.760]   by just sort of adding a new data set here. And to show what that looks like, you know,
[00:30:01.760 --> 00:30:11.280]   popping here to the terminal, we can sort of run a Python, you know, script to ingest new data from
[00:30:11.280 --> 00:30:16.320]   sort of a data CSV file, sort of a very common workflow where we're seeing from customers where
[00:30:16.320 --> 00:30:22.080]   every sort of week or two, you know, you get sort of a new set of data. So this is going to, you
[00:30:22.080 --> 00:30:30.800]   know, just add a new version of that artifact. And so popping into to WB, then we can sort of refresh
[00:30:30.800 --> 00:30:35.920]   and see a couple things here. So the first is that we now sort of have this sort of staging latest
[00:30:35.920 --> 00:30:40.960]   version of that data set. And sort of the second thing you can note is that, you know, why is
[00:30:40.960 --> 00:30:47.680]   deluge 117 just happened, and is now sort of training on that staging data set. And so that
[00:30:47.680 --> 00:30:56.080]   happened, again, automatically, all we really did was add one, one new data set version. And now that
[00:30:56.080 --> 00:31:02.880]   is triggering a launch job on our GPUs just from sort of a batch, batch data ingest. So again,
[00:31:02.880 --> 00:31:08.000]   sort of a lot of exciting pieces that we're seeing, you know, folks pull together with the model
[00:31:08.000 --> 00:31:13.520]   registry so that you can have, you know, automations to, you know, coming back here to,
[00:31:13.520 --> 00:31:19.600]   to take, you know, production models, send them to a live endpoint. Or, you know, anytime, let's
[00:31:19.600 --> 00:31:24.480]   say that job that we just kicked off, if it has a really good evaluation loss, we might, you know,
[00:31:24.480 --> 00:31:29.680]   tag that as a new staging candidate, and then have sort of a separate job to sort of evaluate that.
[00:31:29.680 --> 00:31:34.320]   So again, you know, the sort of real key power here, I think we're seeing from a lot of users is,
[00:31:34.320 --> 00:31:39.680]   you know, from the WMB UI, you now have the ability to do sort of so much sort of production
[00:31:39.680 --> 00:31:46.240]   work, as well as sort of eval staging work without having to jump to a different, different interface.
[00:31:46.240 --> 00:31:53.280]   So hopefully, you know, fingers crossed, with a few more iterations, a few more batches of
[00:31:53.280 --> 00:31:57.680]   fresh data, you know, review codes model can get maybe a little bit better. And that'll take them
[00:31:57.680 --> 00:32:03.600]   to be, you know, the next next gen AI unicorn out there. With that back to back to you, Carrie.
[00:32:03.600 --> 00:32:11.920]   Awesome. Thanks so much, Hop. That was that was a wonderful demo. And, and it's just it feels
[00:32:11.920 --> 00:32:17.760]   really good having seen kind of the process of development, you know, over the years at
[00:32:17.760 --> 00:32:22.400]   Weights and Biases, to see that we've gotten to this point where this entire lifecycle is possible.
[00:32:23.120 --> 00:32:30.240]   So really appreciate walking through that with you. Andrea has just let me know that there's
[00:32:30.240 --> 00:32:37.840]   a good question in the chat from George, who's asking about the differences between LLM ops
[00:32:37.840 --> 00:32:43.280]   versus MO ops. And so I'll take a swing at this. And then, Hop, I'd love your opinion, too.
[00:32:43.280 --> 00:32:49.840]   So what are issues beyond fine tuning LLMs, like prompt engineering? How do we address these?
[00:32:50.560 --> 00:32:56.160]   So my perspective is, there's, there's a shared pattern that's fundamental between both of them,
[00:32:56.160 --> 00:33:01.280]   right, where you have something that you change, you want to test it and make sure that it's better
[00:33:01.280 --> 00:33:05.920]   than the previous one, and then deploy it and monitor how that's performing, and then go back
[00:33:05.920 --> 00:33:11.520]   and iterate. So in the world of models that Hop just described, that's going to be fine tuning
[00:33:11.520 --> 00:33:16.960]   that model, deploying it in the world of prompts, which I touched on briefly in my slides,
[00:33:16.960 --> 00:33:23.280]   that's iterating on the prompt, seeing results in something like the samples table, debugging things
[00:33:23.280 --> 00:33:29.760]   like the link chain, you know, issues where maybe it's calling the wrong tool, or it gets the wrong
[00:33:29.760 --> 00:33:35.760]   answer from a tool. Or, you know, I actually have had a lot of timeouts when I'm, when I'm using
[00:33:35.760 --> 00:33:43.200]   link chain. So many of my bugs are just from like, rate limiting. And, and then how do you move from
[00:33:43.200 --> 00:33:49.440]   there to, to having that prompt then used in production, monitor that, and be able to iterate
[00:33:49.440 --> 00:33:58.320]   on it. And so in my mind, the the LLM ops piece of this, from the prompt side, is really about using
[00:33:58.320 --> 00:34:05.760]   those two new components that we've recently come out with. So on the traces side, that's
[00:34:05.760 --> 00:34:14.160]   visualizing each input and output. And, and then on the, and I'll actually I'll pull up an example,
[00:34:14.160 --> 00:34:19.360]   because I think it's a little bit easier to understand this by looking at a live example.
[00:34:19.360 --> 00:34:28.160]   So one moment. So here on this page, I'm looking at a report, visualizing the results of using
[00:34:29.120 --> 00:34:36.480]   link chain to interact with, with a model that's trying to take an input of a math question,
[00:34:36.480 --> 00:34:42.800]   and produce an output of the correct answer. And you can see there's actually a lot of false,
[00:34:42.800 --> 00:34:48.560]   there's a lot of failures here, a lot of inputs that didn't get an output at all. And there's
[00:34:48.560 --> 00:34:53.680]   also some ones that worked. So in this example, so I'll click in here, I can dig into the trace
[00:34:53.680 --> 00:34:58.160]   timeline view below. And so it's this interaction, George, when you're asking, you know, what's the
[00:34:58.160 --> 00:35:04.080]   difference, where being able to see sample executions of what happened at each step,
[00:35:04.080 --> 00:35:10.880]   and, and dig into, you know, behind the scenes, how, for example, the chain is being executed,
[00:35:10.880 --> 00:35:16.960]   this is the portion that in my mind, is slightly different from this core workflow that that Hoff
[00:35:16.960 --> 00:35:23.120]   just outlined. But I think fundamentally, the higher level pattern of I've changed something,
[00:35:23.120 --> 00:35:29.760]   I want to test it and then deploy it is still, you know, matches, both in the traditional ML
[00:35:29.760 --> 00:35:36.800]   of retraining model, and in the prompt iteration sense. And so I think there's, there's this key
[00:35:36.800 --> 00:35:42.400]   new tool for visualizing the results of, you know, editing the prompt and seeing, seeing how it's
[00:35:42.400 --> 00:35:47.920]   performing. And Hoff, I'm curious, from your perspective, what's your thoughts on the difference
[00:35:47.920 --> 00:35:56.000]   between LM ops and ml ops with WP? Yeah, thanks, Gary. Yeah, so I definitely plus one to everything
[00:35:56.000 --> 00:36:00.240]   you're saying there, I think, you know, from from talking with customers, it almost feels like
[00:36:00.240 --> 00:36:07.440]   LM ops often is sort of a superset of ml ops, you know, to your point, that sort of general workflow
[00:36:07.440 --> 00:36:12.000]   that we sort of walked through for review co of, hey, we have a fresh new set of data, we want to
[00:36:12.000 --> 00:36:16.560]   fine tune a model, check its performance and sort of bring that into production. That's something
[00:36:16.560 --> 00:36:23.120]   you could imagine holding true for, you know, computer vision workflows, for sort of really
[00:36:23.120 --> 00:36:29.120]   nuanced biological models, sort of whatever type of model would exist there, that there's definitely
[00:36:29.120 --> 00:36:34.400]   a little bit of nuance that just can get a little bit trickier with LM space. So for example,
[00:36:34.400 --> 00:36:39.360]   definitely glossed over it for review code. But when you use, you know, parameter efficient,
[00:36:39.360 --> 00:36:46.480]   fine tuning, that's just saving the slices of sort of new new parameters. And so there's a little bit
[00:36:46.480 --> 00:36:50.400]   of extra work you need to think through about where which do you want to persist? How do you
[00:36:50.400 --> 00:36:54.800]   then get sort of the full model available for production. So I think, you know, there's generally
[00:36:54.800 --> 00:36:59.520]   just some extra complications to LLM that folks are working through as that tooling is evolving
[00:36:59.520 --> 00:37:05.680]   so quickly. So there's, you know, I think it's the same basic workflows, but just a little more
[00:37:05.680 --> 00:37:11.920]   complicated because of how how l the LLM are. And then to your other point, Gary, there's,
[00:37:11.920 --> 00:37:16.320]   you know, some stuff that's just going to be brand new, that's not really traditionally in ml ops. So
[00:37:16.800 --> 00:37:21.760]   you know, how are you iterating on your, you know, retrieval augmented generation? What are you
[00:37:21.760 --> 00:37:27.520]   thinking about few shot learning and managing those sort of data sources, you know, having to access,
[00:37:27.520 --> 00:37:31.120]   you know, a data store like that is not something that folks typically think through when sort of
[00:37:31.120 --> 00:37:37.120]   training models, or going to production. So you know, a lot of the trace tools that we built out
[00:37:37.120 --> 00:37:43.840]   are really, you know, around supporting that use case and definitely new, similarly to, you know,
[00:37:44.400 --> 00:37:49.840]   iterating on sort of various prompt versions, and things like that. And then, you know, a couple of
[00:37:49.840 --> 00:37:54.880]   folks, you know, especially, you know, some customers that are doing workflows like like
[00:37:54.880 --> 00:38:00.640]   review co, where you're, you know, fine tuning and open source model, sometimes they will actually
[00:38:00.640 --> 00:38:06.160]   incorporate sort of paid API's as part of their email process. So I know we have connections to
[00:38:06.160 --> 00:38:12.320]   open AI evals, other folks are just using a really high powerful model to provide some extra feedback
[00:38:12.320 --> 00:38:17.360]   of, you know, which of these three candidate models is producing the best answer. And so I
[00:38:17.360 --> 00:38:21.920]   think, you know, again, LLM ops just feels like a really additive super set of there's so many
[00:38:21.920 --> 00:38:27.920]   extra workflows, you can now do and also your base workflows. Again, the process is going to be the
[00:38:27.920 --> 00:38:34.160]   same as CV and other spaces. But given how fast the tooling is changing, sort of, you know, very,
[00:38:34.160 --> 00:38:41.600]   very tricky to get it right. Yeah, yeah, absolutely. Thank you, Huff. And Andrea is highlighting
[00:38:41.600 --> 00:38:46.960]   another question from the chat from Ganesh. Could you give an example on multimodal data,
[00:38:46.960 --> 00:38:54.800]   like tabular plus text data. So I did pull one up that I'm familiar with, from from our work here.
[00:38:54.800 --> 00:39:00.880]   And this is showing what it looks like inside weights and biases to interact with, with, you
[00:39:00.880 --> 00:39:07.760]   know, image models, and then text to image synthesis. And so going from like a text prompt to
[00:39:07.760 --> 00:39:11.440]   the results, and maybe there are different types of results from different types of models.
[00:39:11.440 --> 00:39:18.800]   And, and so the nice thing here is, if you have a few different models that are tested on the
[00:39:18.800 --> 00:39:24.320]   same text input, you can actually compare them in this, this same central view. And here in this
[00:39:24.320 --> 00:39:29.280]   page that I'm displaying, this is actually, you know, blog post that we posted publicly,
[00:39:29.280 --> 00:39:35.840]   and I'll share the link as well. But the, the actual, you know, page that I'm loading is a
[00:39:35.840 --> 00:39:41.040]   report in WMB. And the nice thing about that is you could literally create this internally for
[00:39:41.040 --> 00:39:44.880]   your team. So you could say, like, here's, you know, some text input that I tried that generated
[00:39:44.880 --> 00:39:51.360]   these images, or here's some images that we input, and they generated, like the model generated these
[00:39:51.360 --> 00:39:58.320]   captions. And here's some very good examples. It's really good at describing landscape scenes,
[00:39:58.320 --> 00:40:04.080]   perhaps, but it actually has a really hard time with scenes with a lot of people. And so maybe
[00:40:04.080 --> 00:40:09.360]   we can analyze what's going on there, we can talk about the findings and debug. And you can describe
[00:40:09.360 --> 00:40:15.840]   that all in a report like this. So I could come in here and update and edit both the visualizations,
[00:40:15.840 --> 00:40:22.880]   so these text and image tables, as well as the descriptive narrative around them. So explaining
[00:40:22.880 --> 00:40:26.880]   to my team, you know, what's happening in my research, and then what the right next steps are.
[00:40:26.880 --> 00:40:31.600]   So this is, this is one example that immediately came to mind when you were asking about
[00:40:31.600 --> 00:40:38.640]   multimodal data. Hof, do you have a perspective? Are there any, any sort of custom things that's
[00:40:38.640 --> 00:40:44.080]   good to know about in Weights and Biases for multimodal data? Yeah, so I think keeping your
[00:40:44.080 --> 00:40:49.520]   sort of screen up right there, I think sort of that table, which, you know, for the demo earlier,
[00:40:49.520 --> 00:40:55.600]   was sort of all text, but the fact that you can have sort of text next to sort of different layers,
[00:40:55.600 --> 00:41:00.880]   you know, different segmentation inputs, sort of different text samples, sort of a really valuable
[00:41:00.880 --> 00:41:06.560]   way that a lot of, you know, customers that I've been working with, are able to make sense of
[00:41:06.560 --> 00:41:11.360]   multimodal data, because I think a key challenge is often, you know, there's some tooling built
[00:41:11.360 --> 00:41:17.760]   off built out for images or audio or video, and then there's sort of nothing, you know, equivalent,
[00:41:17.760 --> 00:41:21.440]   or maybe something entirely separate for sort of the textual piece. And so there's no sort of
[00:41:21.440 --> 00:41:27.040]   unified place to bring that together. And so given, you know, the ability to bring that into
[00:41:27.040 --> 00:41:32.240]   tables, sort of render those images live, and then, you know, there's the ability to sort of filter,
[00:41:32.240 --> 00:41:37.120]   group by, and have all of those things update. So if you want to, you know, filter off of,
[00:41:37.120 --> 00:41:43.360]   you know, a keyword in the text, and then, you know, group by something else, and then look at
[00:41:43.360 --> 00:41:49.520]   an array of images and sort of hop through those quickly, you can do that by logging a table to W&B
[00:41:49.520 --> 00:41:53.840]   and then looking at it in a workspace or bringing it into reports like this that we shared.
[00:41:54.560 --> 00:41:59.920]   And so I think having, you know, one central place to look at both modes of data together,
[00:41:59.920 --> 00:42:05.680]   when you're figuring out where the model is performing well, or not as well, is sort of a
[00:42:05.680 --> 00:42:10.240]   key benefit from tables that we've seen. Yeah, absolutely makes sense. Yeah, thank you.
[00:42:10.240 --> 00:42:19.760]   So then, I know Andrea surfaced Hai's question around RLHF and shared a couple links there.
[00:42:20.960 --> 00:42:26.400]   Are there any other questions in the chat that we could cover here? I feel like this has been
[00:42:26.400 --> 00:42:34.080]   a real whirlwind of touching on basically every component in the W&B lifecycle. So, you know,
[00:42:34.080 --> 00:42:39.760]   I recognize this is a lot of content and, yeah, really, really appreciate getting to share this
[00:42:39.760 --> 00:42:46.960]   with folks. As a next step for, you know, being able to make this, you know, useful for your team,
[00:42:48.000 --> 00:42:54.880]   I think, in my mind, maybe the easiest place to start, and I'll pull up that slide of
[00:42:54.880 --> 00:43:02.480]   a diagram of the model lifecycle. You know, if you're thinking, okay, watch this webinar,
[00:43:02.480 --> 00:43:09.520]   what do I do next? In my mind, you really start with this piece of either iterating on a prompt
[00:43:09.520 --> 00:43:18.320]   or fine tuning and then move on to this larger lifecycle that Haf was outlining of evaluating
[00:43:18.320 --> 00:43:24.240]   the performance and then documenting and deploying the model. And so, when you start with that,
[00:43:24.240 --> 00:43:29.840]   you know, tweaking the prompts and iterating, we do have a couple of different tools to make that
[00:43:29.840 --> 00:43:37.520]   easy. And so, if you're looking for a good place to start, my suggestion is, you know, in the docs,
[00:43:37.520 --> 00:43:43.440]   we've got this quick start for just quickly being able to track models' performance. We have the
[00:43:43.440 --> 00:43:49.040]   model registry, so that's organizing your models centrally. And then we also touched on prompts
[00:43:49.040 --> 00:43:54.880]   here. So, if you're working with LLMs, this is the key suite of tools for tracing and visualizing
[00:43:54.880 --> 00:43:59.840]   those results. And so, if you're looking for the next place to get started, what I'd love to leave
[00:43:59.840 --> 00:44:05.360]   you with is, you know, these tools are open to try out, and we really want to hear from you and
[00:44:05.360 --> 00:44:10.720]   help you get set up. So, if you have any questions about applying this to your workflow, definitely
[00:44:10.720 --> 00:44:16.960]   don't hesitate to reach out. And yeah, I really appreciate just getting to share, you know, each
[00:44:16.960 --> 00:44:24.400]   of these components with you. Oh, and I see a couple more questions have just come in. So,
[00:44:24.400 --> 00:44:34.800]   I'll touch on these before we end. So, let's see. I see one about the WMB roadmap. So, in terms of
[00:44:34.800 --> 00:44:42.000]   the, you know, core vision for WMB, we started with experiment tracking. We expanded to this
[00:44:42.000 --> 00:44:48.960]   workflow of making it easier to iterate on datasets, on models, on prompts. And a critical
[00:44:48.960 --> 00:44:57.520]   theme for us moving forward is both the enterprise. So, addressing things like broadening our
[00:44:57.520 --> 00:45:04.640]   integrations with other external tools, making it easier to control the workflow so it's even
[00:45:04.640 --> 00:45:09.920]   more clear who is taking what step along this life cycle. And then we're also really focused
[00:45:09.920 --> 00:45:15.520]   around LLM tooling. So, things like the production monitoring that I showed you that private preview
[00:45:15.520 --> 00:45:24.960]   of. In terms of, you know, where our roadmap is going, we see that there's this shift from ML
[00:45:24.960 --> 00:45:30.320]   practitioners being someone who has a PhD in machine learning to people who are working on
[00:45:30.320 --> 00:45:35.840]   and building models and iterating on prompts being a much larger set of folks with different
[00:45:35.840 --> 00:45:40.240]   backgrounds. So, that includes, you know, folks who are doing prompt engineering who maybe are
[00:45:40.240 --> 00:45:44.800]   coming from a software engineering background and are really interested in getting a model to be
[00:45:44.800 --> 00:45:50.880]   useful in production by tweaking the prompts as well as fine tuning in these other pieces we
[00:45:50.880 --> 00:45:58.720]   touched on. So, that's why it seems, you know, so opportune for us at Weights and Biases to be able
[00:45:59.360 --> 00:46:05.200]   to go from this point of experiment tracking to covering the larger life cycle of how you get a
[00:46:05.200 --> 00:46:11.040]   good model to then put into production and be useful for your team. So, key focuses on production
[00:46:11.040 --> 00:46:16.880]   monitoring and LLMs and then enterprise controls and making it really easy to productionize your
[00:46:16.880 --> 00:46:23.200]   model. So, then I see Samit has a question. Is there a report for retraining models for the
[00:46:23.200 --> 00:46:30.960]   use case Matt discussed of having new data every two weeks? So, Haf, I'll pass that to you. Is
[00:46:30.960 --> 00:46:39.120]   there any guide around automatic retraining on new data? So, we might be creating one coming out of
[00:46:39.120 --> 00:46:44.080]   this workshop. So, I think there's nothing available yet to share, but I think as a follow-up,
[00:46:44.080 --> 00:46:49.520]   we can sort of like, I mean, again, it's using the building blocks that we sort of talked about
[00:46:49.520 --> 00:46:55.600]   before. So, automation. So, anytime there's a change to that sort of source dataset and then
[00:46:55.600 --> 00:47:00.400]   sort of having that launch job that's going to sort of retrain, I think there's sort of a best
[00:47:00.400 --> 00:47:08.800]   practice there that we've started to see where you can actually in your WMB config have a parameter
[00:47:08.800 --> 00:47:13.120]   for the alias for the dataset version that you want to do, which is sort of letting me in the
[00:47:13.120 --> 00:47:17.840]   example, you know, swap from sort of the down sampled version to a production version and then
[00:47:17.840 --> 00:47:23.520]   use a staging version whenever we're doing the retraining. So, mostly sort of using standard
[00:47:23.520 --> 00:47:27.360]   pieces there are docs for, but given that there's interest, might sort of write up something
[00:47:27.360 --> 00:47:35.760]   focused specifically on that. Perfect. And totally agree, Haf. I think that this workflow is a
[00:47:35.760 --> 00:47:41.600]   perfect addition after this webinar to create this. So, Andrea, I'd love your help following
[00:47:41.600 --> 00:47:48.640]   up with Simon after this event to share what we create. I see a very specific question from
[00:47:48.640 --> 00:47:54.000]   Carlos. I could take care of it. Go for it. So, question was in sort of around whether
[00:47:54.000 --> 00:47:58.960]   artifact caching was smart. And so, whether if only, you know, a few labels had changed,
[00:47:58.960 --> 00:48:05.760]   you know, is there like a new artifact or sort of how does that work? So, specifically for the
[00:48:05.760 --> 00:48:11.600]   demo that we had there, those sort of train test and valid splits actually contain sort of
[00:48:11.600 --> 00:48:17.680]   individual parquet files. And so, the way that I was sort of doing batch ingest, which is similar
[00:48:17.680 --> 00:48:22.800]   to what I've seen a few customers do as well, is using one of our capabilities called incremental
[00:48:22.800 --> 00:48:29.520]   artifacts. And so, all that's doing is sort of incrementally adding in sort of new, in this case,
[00:48:29.520 --> 00:48:33.280]   I was doing sort of timestamped, you know, for here's, you know, the batch of data at this
[00:48:33.280 --> 00:48:39.840]   particular time that's being added incrementally to the train data. And then, whenever the sort of
[00:48:39.840 --> 00:48:46.480]   training job kicks off, that just sort of aggregates all of that data at once. So, especially for folks
[00:48:46.480 --> 00:48:50.880]   that, you know, have sort of a stream sort of using the ability and artifacts to, you know,
[00:48:50.880 --> 00:48:56.160]   have folders separating things and then just incrementally adding files is a pattern that we've
[00:48:56.160 --> 00:49:07.680]   seen before there. >> Perfect. Thank you, Hof. And I see one more question from Gordon who wants to
[00:49:07.680 --> 00:49:14.480]   know, do we have an example of how a user can provide feedback to the results of an LLM? And,
[00:49:14.480 --> 00:49:20.640]   Gordon, this is a fantastic question. Back to the roadmap conversation, this is actually exactly
[00:49:20.640 --> 00:49:26.000]   what, you know, what we've been discussing here is, you know, how to make it really easy to take,
[00:49:26.000 --> 00:49:33.360]   you know, the columns that you've logged and then add a custom column. And so, we don't have a
[00:49:33.360 --> 00:49:37.600]   report for this, but I think this is another good report that we could create as a quick follow-up
[00:49:37.600 --> 00:49:44.720]   here. And the pattern for doing this is using just the standard API that we have already, WMB.log.
[00:49:44.720 --> 00:49:50.400]   And so, you can customize, you know, what columns are visible in that table I was showing you.
[00:49:50.400 --> 00:49:55.360]   And that makes it possible for you to have your own evaluation criteria, as well as the defaults
[00:49:55.360 --> 00:50:00.400]   that I was showing. So, Andrea, I'm taking a note to follow up with Gordon as well,
[00:50:00.400 --> 00:50:06.960]   as well as Samit to talk about those two additional reports from this discussion.
[00:50:06.960 --> 00:50:15.040]   Awesome. Looks like that's all the questions we have here today. I really appreciate getting to
[00:50:15.040 --> 00:50:20.640]   share these workflows with you, and I really hope that you'll give it a try for tracking your own
[00:50:20.640 --> 00:50:25.280]   process here. Please don't hesitate to reach out. We're really excited to help people use
[00:50:25.280 --> 00:50:29.840]   this process in your own teams. And Haf, thanks for an awesome demo.
[00:50:29.840 --> 00:50:36.720]   Cool. Have a good one, everyone. Cheers, all. Cheers.

