
[00:00:00.000 --> 00:00:06.000]   We have these two camps of users, the researchers and the developers.
[00:00:06.000 --> 00:00:08.960]   And developers keep telling us, "Hey, I just want one button.
[00:00:08.960 --> 00:00:11.000]   I just want the best model to come out."
[00:00:11.000 --> 00:00:16.680]   And then a lot of the researchers want to fiddle more with the parameters.
[00:00:16.680 --> 00:00:20.640]   I think we can probably satisfy both for a long time.
[00:00:20.640 --> 00:00:25.000]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:25.000 --> 00:00:27.160]   and I'm your host, Lukas Biewald.
[00:00:27.160 --> 00:00:32.960]   Today I'm talking with Peter Wellender, longtime friend and currently VP of Product and Partnerships
[00:00:32.960 --> 00:00:37.040]   at OpenAI, running GPT-3 and other things.
[00:00:37.040 --> 00:00:41.040]   And before that, research lead at OpenAI, where he was one of Weights & Biases' very
[00:00:41.040 --> 00:00:43.120]   first customers.
[00:00:43.120 --> 00:00:46.720]   And before that, head of machine learning at Dropbox.
[00:00:46.720 --> 00:00:51.300]   And I'm also talking with Boris Deyma, machine learning engineer at Weights & Biases.
[00:00:51.300 --> 00:00:57.380]   And we're going to talk about GPT-3 and the recently announced integration that GPT-3
[00:00:57.380 --> 00:00:59.040]   did with Weights & Biases.
[00:00:59.040 --> 00:01:01.620]   So this should be a lot of fun.
[00:01:01.620 --> 00:01:07.020]   So Peter, the last time we talked, I think you were working on research at OpenAI, and
[00:01:07.020 --> 00:01:09.380]   that's most of the time that I've known you.
[00:01:09.380 --> 00:01:14.500]   But now we find that you're VP of Product and Partnerships at OpenAI.
[00:01:14.500 --> 00:01:18.860]   I'm kind of curious what that means and what you're doing day to day.
[00:01:18.860 --> 00:01:20.540]   Yeah, sure.
[00:01:20.540 --> 00:01:28.100]   What I do today is quite different from when I did research, for sure.
[00:01:28.100 --> 00:01:33.860]   For me, doing research has always been about solving the hardest problems that are out
[00:01:33.860 --> 00:01:39.540]   there in order to actually have some sort of impact on the world.
[00:01:39.540 --> 00:01:44.660]   So I'm personally much more driven by the end goals of research rather than the research
[00:01:44.660 --> 00:01:45.660]   itself.
[00:01:45.660 --> 00:01:51.300]   I don't want to do research, go down and explore things research-wise, but it's always been
[00:01:51.300 --> 00:01:53.700]   with some goal at the end of it.
[00:01:53.700 --> 00:02:00.580]   And one exciting thing that has happened with GPT-3, a lot of the things that I did when
[00:02:00.580 --> 00:02:05.300]   I started at OpenAI was I did things on the robotics side.
[00:02:05.300 --> 00:02:11.300]   And with robotics, there's still some gap from the stuff you can do in the lab and what
[00:02:11.300 --> 00:02:12.700]   you can do in the real world.
[00:02:12.700 --> 00:02:22.260]   And with GPT-3, when we got our first results in GPT-3, it was clear that we had something
[00:02:22.260 --> 00:02:27.300]   that we could start applying to real-world problems rather than just do cool demos.
[00:02:27.300 --> 00:02:30.940]   When I worked in robotics, what we got at the end was a really cool demo of a robotic
[00:02:30.940 --> 00:02:31.940]   hand solving Rubik's Cube.
[00:02:31.940 --> 00:02:35.420]   But it's not like you could start deploying this in everybody's home.
[00:02:35.420 --> 00:02:40.460]   And even if it worked robustly enough to do that, I don't know how useful it would be
[00:02:40.460 --> 00:02:44.500]   to solve Rubik's Cubes, a very expensive way of doing that.
[00:02:44.500 --> 00:02:49.620]   But with GPT-3, we had a language model that you can now apply to solve all kinds of different
[00:02:49.620 --> 00:02:56.460]   problems, everything from translation to summarization to things like classification and question
[00:02:56.460 --> 00:02:57.460]   and answering.
[00:02:57.460 --> 00:02:58.740]   It was a very flexible model.
[00:02:58.740 --> 00:03:05.860]   So what we set out to do was to start just seeing if this was good enough of a model
[00:03:05.860 --> 00:03:08.220]   to actually solve real-world problems.
[00:03:08.220 --> 00:03:13.820]   And for me, that's just a really fun area to focus on.
[00:03:13.820 --> 00:03:19.500]   It's like when you have this really powerful new technology that has the potential of just
[00:03:19.500 --> 00:03:26.860]   changing a lot of things in the way they work, it's all about finding the right problems
[00:03:26.860 --> 00:03:30.220]   to go after and then seeing how you build.
[00:03:30.220 --> 00:03:35.180]   You take the tools you have in your toolbox to solve those problems.
[00:03:35.180 --> 00:03:40.180]   The difference is that what I did as a researcher was very much coming up with the right benchmarks
[00:03:40.180 --> 00:03:45.500]   and the right ways to measure progress, where there was a goal that was really far out and
[00:03:45.500 --> 00:03:50.340]   you needed to come up with these toy ways of evaluating progress.
[00:03:50.340 --> 00:03:57.140]   And now it's like customers telling us, "Hey, I'm trying to apply GPT-3 to this use case
[00:03:57.140 --> 00:04:02.020]   and it doesn't work," or "It's too slow," or something like that.
[00:04:02.020 --> 00:04:03.340]   Those problems are much more concrete.
[00:04:03.340 --> 00:04:10.500]   So my day-to-day, right now it's much more around building a team that can solve these
[00:04:10.500 --> 00:04:15.940]   real-world problems with the technology that we have developed at OpenAI.
[00:04:15.940 --> 00:04:25.300]   When you look at GPT-3 versus the other approaches for large language models out there, that
[00:04:25.300 --> 00:04:31.740]   kind of seems to be a trend, are there key differences that you notice in how it works?
[00:04:31.740 --> 00:04:33.740]   Or are there key elements that take difference somehow?
[00:04:33.740 --> 00:04:35.100]   Yeah, that's a good question.
[00:04:35.100 --> 00:04:42.900]   I think that what I really like about GPT-3 and the main way in my mind that it is different
[00:04:42.900 --> 00:04:46.140]   is that it's just extremely simple.
[00:04:46.140 --> 00:04:52.460]   All that GPT-3 does is, so GPT-3 is a large language model, big neural network, and it's
[00:04:52.460 --> 00:04:56.500]   using this kind of transformer architecture that Google introduced a couple of years ago
[00:04:56.500 --> 00:05:01.060]   that has been really popular and is basically powering all different language models these
[00:05:01.060 --> 00:05:07.300]   days and it's starting to make its way into other areas like computer vision as well.
[00:05:07.300 --> 00:05:11.820]   But the way GPT-3 is set up is very simple.
[00:05:11.820 --> 00:05:17.940]   It has some context, which basically means it can look at a history of texts.
[00:05:17.940 --> 00:05:21.900]   So maybe if you're reading a book, you can look at the page of text or the paragraph
[00:05:21.900 --> 00:05:26.300]   of text, and then it's trying to predict the next word.
[00:05:26.300 --> 00:05:28.300]   And that's the way that GPT-3 is trained.
[00:05:28.300 --> 00:05:33.020]   It's just trained on lots of texts from lots of different sources, mostly from the internet.
[00:05:33.020 --> 00:05:36.540]   And it's just trained to kind of over and over again, based on some words it's seen,
[00:05:36.540 --> 00:05:37.540]   predict the next word.
[00:05:37.540 --> 00:05:42.980]   You can start with only a few words, but when we train these models today, we train them
[00:05:42.980 --> 00:05:46.180]   on the order of a thousand or a few thousand words.
[00:05:46.180 --> 00:05:51.380]   They can look back at those thousand words and then try to predict the next word.
[00:05:51.380 --> 00:05:57.740]   So the setup is super, super simple and you just train it on these huge data sets of texts
[00:05:57.740 --> 00:06:02.980]   in order to keep on predicting the next word and get really, really good at that.
[00:06:02.980 --> 00:06:08.740]   And I think the surprising thing with GPT-3 was that if you do that, then you make the
[00:06:08.740 --> 00:06:10.380]   model really, really large.
[00:06:10.380 --> 00:06:14.260]   So it has a huge capacity of learning.
[00:06:14.260 --> 00:06:20.860]   Then it gets really good at a bunch of tasks for which you previously needed specialized
[00:06:20.860 --> 00:06:21.860]   models.
[00:06:21.860 --> 00:06:26.140]   Like if you want to do translation, you would need a specialized kind of translation neural
[00:06:26.140 --> 00:06:31.380]   network, or if you want to do summarization, similarly, you will set up your neural network
[00:06:31.380 --> 00:06:34.620]   in a particular way and then train it on only summarization tasks.
[00:06:34.620 --> 00:06:39.260]   And what we found with GPT-3 is that you actually get very close to state of the art performance
[00:06:39.260 --> 00:06:43.780]   on a number of these benchmarks that measure things like summarization, translation, question
[00:06:43.780 --> 00:06:48.560]   answering and so on, with a model that has just been trained on the internet to not do
[00:06:48.560 --> 00:06:54.900]   any of those tasks specifically, but by just being able to kind of reproduce text in a
[00:06:54.900 --> 00:06:57.500]   similar way that it has read it.
[00:06:57.500 --> 00:07:01.900]   So practically though, how do you apply it to say a translation task?
[00:07:01.900 --> 00:07:05.300]   How do you take predicting the next word and make it do a translation?
[00:07:05.300 --> 00:07:06.780]   Yeah, that's a great question.
[00:07:06.780 --> 00:07:11.980]   So in a lot of those other large language models, there are certain steps where you
[00:07:11.980 --> 00:07:15.380]   would take a piece of text and you would encode it.
[00:07:15.380 --> 00:07:19.700]   So you would create some representation in your neural network, and then you would have
[00:07:19.700 --> 00:07:24.100]   sort of a decoder that would take that and then kind of write some sentence.
[00:07:24.100 --> 00:07:28.420]   So if you did translation, for example, you would encode that into some sort of representation,
[00:07:28.420 --> 00:07:31.180]   and then you would have a separate piece of neural network that took that representation
[00:07:31.180 --> 00:07:32.940]   and tried to output what you wanted.
[00:07:32.940 --> 00:07:40.380]   So the input might be like a sentence in German and output might be a sentence in English.
[00:07:40.380 --> 00:07:42.300]   And it's been trained specifically for that.
[00:07:42.300 --> 00:07:45.860]   And for GPT-3, to your question then, what do you do with GPT-3?
[00:07:45.860 --> 00:07:51.020]   The simplest way you would do it is that you would provide a few examples of what translations
[00:07:51.020 --> 00:07:52.740]   might look like in just pure text.
[00:07:52.740 --> 00:07:59.100]   You would write German colon and some sentence in German and then English colon, some sentence
[00:07:59.100 --> 00:08:00.100]   in English.
[00:08:00.100 --> 00:08:05.460]   You could provide only a single one, then the setup is called one shot.
[00:08:05.460 --> 00:08:10.620]   You could provide a few examples of basically German colon, English colon examples, and
[00:08:10.620 --> 00:08:14.420]   then you would put in the new sentence that you would want to translate.
[00:08:14.420 --> 00:08:16.500]   That's called few shot training, where you have a few examples.
[00:08:16.500 --> 00:08:21.820]   And the model would just, by looking at the pattern of what it's now seeing in its context,
[00:08:21.820 --> 00:08:25.820]   then you can predict, we can produce a translation.
[00:08:25.820 --> 00:08:27.580]   So it's a very simple setup.
[00:08:27.580 --> 00:08:32.860]   Basically, the way I think about telling GPT-3 what to do is a little bit like how you would
[00:08:32.860 --> 00:08:34.460]   actually tell a human to do the same thing.
[00:08:34.460 --> 00:08:37.980]   If you're writing an email, if I'm writing an email to you saying, "Hey, Lukas, I want
[00:08:37.980 --> 00:08:43.140]   you to translate some sentences," what I would do is, "Hey, I would just ask you, please
[00:08:43.140 --> 00:08:44.420]   translate these sentences."
[00:08:44.420 --> 00:08:48.300]   And I would maybe provide a few examples to give you a sense of the tone.
[00:08:48.300 --> 00:08:52.100]   Like, do I want a more formal translation, more casual translation?
[00:08:52.100 --> 00:08:53.780]   You would pick up on the pattern.
[00:08:53.780 --> 00:08:56.140]   You would give it then a sentence in German, if you...
[00:08:56.140 --> 00:08:59.500]   I don't know if you know German, but you would be able to translate it to English.
[00:08:59.500 --> 00:09:03.900]   And it turns out now with our latest models, you don't actually even have to provide those
[00:09:03.900 --> 00:09:04.900]   examples.
[00:09:04.900 --> 00:09:07.820]   You can often just ask the models, just as you would ask a human.
[00:09:07.820 --> 00:09:12.860]   Like, "Hey, translate this sentence to me," or, "Summarize this piece of text."
[00:09:12.860 --> 00:09:14.860]   We just found that that's how people wanted to use the models.
[00:09:14.860 --> 00:09:16.660]   We kind of made them work like that.
[00:09:16.660 --> 00:09:18.740]   But that's how simple it is.
[00:09:18.740 --> 00:09:23.300]   You just kind of tell it what you want to do, and it will do its best attempt at just
[00:09:23.300 --> 00:09:24.300]   doing it.
[00:09:24.300 --> 00:09:29.300]   So did you make a concerted effort to train the model on multiple languages?
[00:09:29.300 --> 00:09:30.900]   Or was it mostly English?
[00:09:30.900 --> 00:09:34.380]   Where did the corpus come from?
[00:09:34.380 --> 00:09:35.380]   We actually did the opposite.
[00:09:35.380 --> 00:09:39.980]   Initially, when we trained GPT-3, we made a concerted effort not to train it on other
[00:09:39.980 --> 00:09:44.900]   languages than English, because it turns out that even though these models are huge, there's
[00:09:44.900 --> 00:09:47.740]   a trade-off in your dataset mix.
[00:09:47.740 --> 00:09:52.420]   If you train it on English, but then lots of other languages, it would just not end
[00:09:52.420 --> 00:09:54.500]   up being as good at English tasks.
[00:09:54.500 --> 00:10:00.860]   And ultimately, when we train this, we want to see just generally how good can it be at
[00:10:00.860 --> 00:10:02.220]   more general capabilities.
[00:10:02.220 --> 00:10:04.740]   We didn't care as much about translation.
[00:10:04.740 --> 00:10:10.380]   So whenever we put in extra languages, that would just be at a cost of being good at performing
[00:10:10.380 --> 00:10:13.660]   other tasks in English, like question answering and summarization and so on.
[00:10:13.660 --> 00:10:19.340]   But it turned out, even by explicitly trying to filter out most other languages, probably
[00:10:19.340 --> 00:10:23.500]   a few small percentage points of the data turned out to be in other languages.
[00:10:23.500 --> 00:10:28.620]   And even with that, the model is just incredibly good at translation.
[00:10:28.620 --> 00:10:32.740]   It's close to state of the art in a lot of translation tasks.
[00:10:32.740 --> 00:10:37.220]   I'm a native Swedish speaker, but I've lost my ability to write things in Swedish these
[00:10:37.220 --> 00:10:39.460]   days because I never do it.
[00:10:39.460 --> 00:10:45.180]   So what I do these days is I write it in English and I ask GPT-3 to translate it to me.
[00:10:45.180 --> 00:10:46.540]   And that's usually my starting point.
[00:10:46.540 --> 00:10:47.540]   I won't get it perfect.
[00:10:47.540 --> 00:10:51.540]   I need to fiddle with a few things, but it's surprisingly good.
[00:10:51.540 --> 00:10:55.820]   And the amount of Swedish training data in the model was really, really small.
[00:10:55.820 --> 00:11:00.100]   I think we've been constantly updating our models and making them better and better.
[00:11:00.100 --> 00:11:04.980]   So now we are introducing more and more language data as we figured out how to make these trade-offs
[00:11:04.980 --> 00:11:06.540]   in more optimized ways.
[00:11:06.540 --> 00:11:09.340]   But yeah, originally we actually wanted the opposite.
[00:11:09.340 --> 00:11:11.580]   We just wanted to be really good at English.
[00:11:11.580 --> 00:11:15.980]   And is it predicting words or is it predicting one character at a time?
[00:11:15.980 --> 00:11:18.860]   How does that work?
[00:11:18.860 --> 00:11:20.420]   Yeah, neither of those.
[00:11:20.420 --> 00:11:24.660]   It's actually predicting something called tokens, which is part of words is maybe the
[00:11:24.660 --> 00:11:26.940]   way to think about it.
[00:11:26.940 --> 00:11:33.100]   For the most common English words, they are captured by a single token.
[00:11:33.100 --> 00:11:35.380]   And a token is basically what it is.
[00:11:35.380 --> 00:11:41.020]   I think in our current setup, we have about 50,000 of these tokens that we map them onto
[00:11:41.020 --> 00:11:46.220]   kind of sequences of characters so that it ends up being like a common word like "hi"
[00:11:46.220 --> 00:11:49.000]   or "the" ends up being one token.
[00:11:49.000 --> 00:11:53.540]   But then if you have a more uncommon word like "encyclopedia" or something, you're probably
[00:11:53.540 --> 00:11:55.540]   going to break it up into two or three tokens.
[00:11:55.540 --> 00:12:00.740]   So it's like word pieces that just makes it easier and more efficient for these language
[00:12:00.740 --> 00:12:02.420]   models to consume text.
[00:12:02.420 --> 00:12:06.180]   In principle, you can actually do it at the character level as well.
[00:12:06.180 --> 00:12:10.580]   It just gets very inefficient, but that's where the field is probably moving.
[00:12:10.580 --> 00:12:14.340]   It's going to just do it at the character level.
[00:12:14.340 --> 00:12:18.060]   But I would think that might make foreign languages really hard.
[00:12:18.060 --> 00:12:24.580]   For example, would Asian languages be impossible then if they have far more tokens?
[00:12:24.580 --> 00:12:28.620]   I guess maybe you could argue they've sort of done the tokenization for you by having
[00:12:28.620 --> 00:12:34.060]   a larger number of characters that encode a bigger chunk of meaning.
[00:12:34.060 --> 00:12:38.940]   Yeah, it is definitely the case that the way you train your tokenizer would have an impact
[00:12:38.940 --> 00:12:42.380]   on the performance of different languages.
[00:12:42.380 --> 00:12:45.220]   Usually those two things are trained in two different steps.
[00:12:45.220 --> 00:12:49.700]   You would train your tokenizer on some corpus of data, and then you would separately train
[00:12:49.700 --> 00:12:54.980]   your models with that tokenizer on some other datasets.
[00:12:54.980 --> 00:12:58.860]   In order to get your models really good at different languages, you need to train the
[00:12:58.860 --> 00:13:01.500]   tokenizer as well over multiple languages.
[00:13:01.500 --> 00:13:10.740]   It's definitely more expensive to use other languages because a German word just ends
[00:13:10.740 --> 00:13:15.460]   up being more tokens because we've trained on much less of it, while English is very
[00:13:15.460 --> 00:13:19.620]   efficient where a lot of words are a single token.
[00:13:19.620 --> 00:13:23.180]   So it makes it both a little bit worse at other languages and more expensive.
[00:13:23.180 --> 00:13:24.180]   I see.
[00:13:24.180 --> 00:13:25.980]   Could I translate something into Japanese?
[00:13:25.980 --> 00:13:28.300]   Would that even be possible for GPT-3?
[00:13:28.300 --> 00:13:29.740]   Oh yeah.
[00:13:29.740 --> 00:13:36.820]   One comment I remember was a Japanese user of ours, they really like to use GPT-3 to
[00:13:36.820 --> 00:13:42.100]   translate technical documentation between English and Japanese because they found that
[00:13:42.100 --> 00:13:47.300]   GPT-3 was much better at this translation of technical documentation than Google Translate.
[00:13:47.300 --> 00:13:51.820]   This was a year back, so it's possible that Google Translate is better now, but probably
[00:13:51.820 --> 00:13:56.060]   just a chance thing based on the datasets that we had.
[00:13:56.060 --> 00:14:02.580]   The really cool thing actually with the translation capabilities of GPT-3 is that we haven't really
[00:14:02.580 --> 00:14:08.020]   trained the model on explicit pairs of input and output, kind of translated pieces of text,
[00:14:08.020 --> 00:14:11.420]   like what you usually call in the aligned pieces of text.
[00:14:11.420 --> 00:14:14.180]   It's just like it's seen a lot of Japanese.
[00:14:14.180 --> 00:14:16.180]   It's seen a lot of, well, not super much.
[00:14:16.180 --> 00:14:23.580]   It's seen a bunch of Japanese, but a whole ton of English and somehow through learning
[00:14:23.580 --> 00:14:29.740]   how to predict the next word, there's been enough of little pieces of text, blog posts
[00:14:29.740 --> 00:14:34.740]   or whatever, where the author is switching between Japanese and English and maybe doing
[00:14:34.740 --> 00:14:39.940]   some translation on some sentences where he's found the mapping and then somehow has a representation
[00:14:39.940 --> 00:14:44.260]   that's good enough then to generalize to arbitrary translation tasks.
[00:14:44.260 --> 00:14:48.260]   For me, that's just kind of magical, that it's just by reading lots of English text,
[00:14:48.260 --> 00:14:53.900]   lots of Japanese text, and then maybe accidentally finding a few aligned pairs in all of the
[00:14:53.900 --> 00:14:56.820]   data, it's able to do that kind of translation.
[00:14:56.820 --> 00:14:57.820]   That's pretty crazy to me.
[00:14:57.820 --> 00:15:00.660]   That is really amazing.
[00:15:00.660 --> 00:15:06.460]   Is this performance kind of tangibly different than earlier versions of GPT?
[00:15:06.460 --> 00:15:13.420]   Was there something that happened in GPT-3 where OpenAI thought, "Okay, we can use this
[00:15:13.420 --> 00:15:15.900]   for real world commercial applications."
[00:15:15.900 --> 00:15:19.780]   Was it sort of like a performance level that it needed to get above?
[00:15:19.780 --> 00:15:21.460]   Yeah, definitely.
[00:15:21.460 --> 00:15:28.380]   I think the big difference between GPT-2 and GPT-3 was really, it was trained on more data
[00:15:28.380 --> 00:15:31.100]   and it was a bigger model by two orders of magnitude.
[00:15:31.100 --> 00:15:37.020]   I think the original GPT-2 was about 1.5 billion parameters and GPT-3, the biggest model was
[00:15:37.020 --> 00:15:38.020]   175 billion parameters.
[00:15:38.020 --> 00:15:41.020]   It went up by two orders of magnitude.
[00:15:41.020 --> 00:15:45.140]   Since it was a much bigger model, it also needed more data.
[00:15:45.140 --> 00:15:50.220]   The surprising thing is that that's sort of what it took to kind of go from feeling fairly
[00:15:50.220 --> 00:15:52.820]   kind of dumb to interact with.
[00:15:52.820 --> 00:15:58.540]   GPT-2 was kind of cool, but it also felt kind of incredibly stupid most of the time.
[00:15:58.540 --> 00:16:03.300]   I think with GPT-3, it went to being sometimes just surprisingly good.
[00:16:03.300 --> 00:16:09.780]   Don't get me wrong, GPT-3 does allow silly mistakes still, but it does the right thing
[00:16:09.780 --> 00:16:14.540]   probably like 30 to 50% of the time on some tasks and sometimes even better than that.
[00:16:14.540 --> 00:16:19.020]   It's sort of like suddenly before you would need to kind of sample and try out tasks and
[00:16:19.020 --> 00:16:22.380]   maybe once every kind of 20 or something, you would see something, "Oh, this looks pretty
[00:16:22.380 --> 00:16:23.380]   good."
[00:16:23.380 --> 00:16:28.220]   With GPT-3, it kind of started happening every third time or every half time, second time
[00:16:28.220 --> 00:16:29.220]   or every fifth time.
[00:16:29.220 --> 00:16:31.940]   Then you're like, "Oh my God, this is actually..."
[00:16:31.940 --> 00:16:37.120]   For things like summarizing text, for example, one example we have is summarizing a piece
[00:16:37.120 --> 00:16:39.540]   of text in the style of a second grader.
[00:16:39.540 --> 00:16:44.900]   It's just incredible how the model is able to kind of simplify words, get the gist of
[00:16:44.900 --> 00:16:46.260]   a piece of text and so on.
[00:16:46.260 --> 00:16:50.140]   Again, it's not perfect, but it's just really good.
[00:16:50.140 --> 00:16:53.980]   Obviously, there's a lot of academic benchmarks.
[00:16:53.980 --> 00:16:58.540]   You can run these models and you can see it just getting much better on all of those academic
[00:16:58.540 --> 00:17:01.940]   benchmarks, but it was a whole different field to it.
[00:17:01.940 --> 00:17:07.940]   When you wanted to prototype something, the difference is that now it's just easy to get
[00:17:07.940 --> 00:17:10.980]   something that works pretty well.
[00:17:10.980 --> 00:17:14.860]   That's sort of why we decided like, "Hey, now it seems useful."
[00:17:14.860 --> 00:17:20.660]   GPT-2 didn't seem really useful to the same extent, but GPT-3 for all these tasks, we
[00:17:20.660 --> 00:17:24.420]   felt like, "Okay, it's close enough to kind of state of the art if you have a specialized
[00:17:24.420 --> 00:17:25.420]   model or whatever.
[00:17:25.420 --> 00:17:30.860]   A clever, clever programmer should be able to apply it to whatever task they have."
[00:17:30.860 --> 00:17:34.340]   That was what we set out to validate with the API.
[00:17:34.340 --> 00:17:39.220]   What are some of the use cases that you feel really proud of where it really works?
[00:17:39.220 --> 00:17:43.620]   Are there any that you could point us to where we could go interact with it in a commercial
[00:17:43.620 --> 00:17:44.620]   setting somewhere?
[00:17:44.620 --> 00:17:45.620]   Yeah, sure.
[00:17:45.620 --> 00:17:53.740]   I think some of the areas where we were most surprised were copywriting and question answering
[00:17:53.740 --> 00:17:56.740]   and generally creative writing.
[00:17:56.740 --> 00:18:01.800]   For copywriting, what happened there was that there was a number of companies that started
[00:18:01.800 --> 00:18:04.340]   building on top of our platform.
[00:18:04.340 --> 00:18:08.940]   Some of these companies are like, I think, Copysmith was one of the first ones, CopyAI.
[00:18:08.940 --> 00:18:13.880]   There's also Jarvis, I think recently changed their name to a different name, and a number
[00:18:13.880 --> 00:18:15.500]   of other of these companies.
[00:18:15.500 --> 00:18:20.540]   What they did was really clever because they realized that, as I said, when you're using
[00:18:20.540 --> 00:18:23.740]   GPT-3 to kind of do some task, it's not perfect.
[00:18:23.740 --> 00:18:27.020]   Every now and then you would get something that doesn't really make sense.
[00:18:27.020 --> 00:18:32.600]   But if you're doing copywriting tasks, like you want to write, say, some engaging product
[00:18:32.600 --> 00:18:38.600]   description based on some attributes of a product, like a shoe, maybe the type of sole,
[00:18:38.600 --> 00:18:43.340]   the color, some other attributes of the shoe, and you want to write something really engaging
[00:18:43.340 --> 00:18:49.400]   about that, then the problem that you as a human face is that you get into some kind
[00:18:49.400 --> 00:18:53.600]   of writer's block, like where do I even start?
[00:18:53.600 --> 00:18:58.580]   What these companies started doing is they took GPT-3 and they used it to kind of generate
[00:18:58.580 --> 00:19:03.660]   a few kind of starting points or a few variations of how you could write product descriptions.
[00:19:03.660 --> 00:19:11.060]   And then what you find is more often than not, if you generate five of those examples,
[00:19:11.060 --> 00:19:15.140]   one of them would look really good, and you can kind of use that as your starting point.
[00:19:15.140 --> 00:19:20.080]   Maybe you just take it as it is, or you make some small tweaks to it.
[00:19:20.080 --> 00:19:23.660]   It's a way to really almost aid in human creativity.
[00:19:23.660 --> 00:19:24.980]   And I think that's just so cool.
[00:19:24.980 --> 00:19:30.140]   It was at writers who would tell us, "Hey, I've been trying to write this book for half
[00:19:30.140 --> 00:19:31.140]   a year now.
[00:19:31.140 --> 00:19:33.700]   I just keep on getting stuck in writer's block."
[00:19:33.700 --> 00:19:37.580]   And then you started using your playground for GPT-3, and now it took me two weeks to
[00:19:37.580 --> 00:19:40.100]   turn out the whole book.
[00:19:40.100 --> 00:19:43.260]   When you get stuck, it can create an interesting storyline.
[00:19:43.260 --> 00:19:46.460]   And as a creative writer, you start exploring that.
[00:19:46.460 --> 00:19:47.780]   "Yeah, that's okay.
[00:19:47.780 --> 00:19:51.940]   I wouldn't have thought of this character going down in that direction, but let's explore
[00:19:51.940 --> 00:19:52.940]   that."
[00:19:52.940 --> 00:19:55.660]   And then it becomes a much more fun, engaging process.
[00:19:55.660 --> 00:19:59.580]   So it's almost like as a human, now you have a brainstorming partner that you can apply
[00:19:59.580 --> 00:20:00.580]   to all these different tasks.
[00:20:00.580 --> 00:20:04.700]   And I think what I found was really cool is to kind of see a number of companies kind
[00:20:04.700 --> 00:20:11.300]   of really leveraging that and creating kind of new experience that you couldn't do before.
[00:20:11.300 --> 00:20:13.500]   So I think that one is really exciting.
[00:20:13.500 --> 00:20:15.180]   I think question answering is also really cool.
[00:20:15.180 --> 00:20:18.580]   But this one was quite unexpected.
[00:20:18.580 --> 00:20:22.460]   I don't think we would have predicted that one being such a big use case.
[00:20:22.460 --> 00:20:30.580]   It seems like one of the advantages of GPT-3 is that it works right out of the box.
[00:20:30.580 --> 00:20:35.780]   But I could also imagine for some teams, there might be a concern about what do you do if
[00:20:35.780 --> 00:20:38.240]   something goes wrong.
[00:20:38.240 --> 00:20:43.980]   I guess I'm curious, do you typically work with ML teams inside of companies, or is it
[00:20:43.980 --> 00:20:48.620]   more like engineers that view the benefit here is that they don't have to figure out
[00:20:48.620 --> 00:20:52.740]   how machine learning works to kind of get the benefit of natural language processing?
[00:20:52.740 --> 00:20:57.940]   Or do you tend to integrate this with ML teams into a kind of bigger ML workflow?
[00:20:57.940 --> 00:21:00.400]   Yeah, that's a good question.
[00:21:00.400 --> 00:21:02.700]   It's a bit of a mix, I would say.
[00:21:02.700 --> 00:21:11.660]   We've had multiple machine learning teams who already had their own models that they
[00:21:11.660 --> 00:21:16.780]   would have downloaded the models online and so on, and they would have adapted them for
[00:21:16.780 --> 00:21:17.780]   their tasks.
[00:21:17.780 --> 00:21:22.420]   And then they find our API and start doing the same thing using our API.
[00:21:22.420 --> 00:21:26.420]   And it just turns out that you can get much better performance from our models.
[00:21:26.420 --> 00:21:31.660]   Just because there doesn't exist, there isn't an open source version of the biggest models
[00:21:31.660 --> 00:21:34.100]   that we have, the best models.
[00:21:34.100 --> 00:21:37.820]   And so for a lot of tasks, that's kind of what works the best.
[00:21:37.820 --> 00:21:44.780]   I think probably the majority of our customers are more in the other camp of just really
[00:21:44.780 --> 00:21:45.780]   smart developers.
[00:21:45.780 --> 00:21:50.420]   And when I say developers, it's a pretty broad group.
[00:21:50.420 --> 00:21:54.980]   We see everything from programmers, engineers, to designers, PMs.
[00:21:54.980 --> 00:22:00.260]   A number of people have told us that the OpenAI API was sort of what got them into programming
[00:22:00.260 --> 00:22:03.780]   because they got really good results from just in our playground where you can interact
[00:22:03.780 --> 00:22:04.780]   with our models.
[00:22:04.780 --> 00:22:07.860]   And they've got ideas and they started to learn how to code and they start connecting
[00:22:07.860 --> 00:22:11.580]   with no code tools like Bubble.io and stuff like that.
[00:22:11.580 --> 00:22:13.420]   It's kind of really lowered that barrier.
[00:22:13.420 --> 00:22:17.940]   You don't have to learn, become a machine learning expert to get really good results
[00:22:17.940 --> 00:22:19.260]   out of these models.
[00:22:19.260 --> 00:22:25.340]   You just kind of have to be kind of good at iterating and figure out how to kind of write
[00:22:25.340 --> 00:22:26.700]   the instructions to the model.
[00:22:26.700 --> 00:22:30.500]   It's a little bit like, everybody becomes a manager.
[00:22:30.500 --> 00:22:36.540]   You have to give really good instruction to your employee if you want them to do the task
[00:22:36.540 --> 00:22:37.900]   as you want it to be done.
[00:22:37.900 --> 00:22:38.900]   And it's very similar with these models.
[00:22:38.900 --> 00:22:43.940]   If you under-specify your tasks, you're going to get very high variance in the outputs.
[00:22:43.940 --> 00:22:47.980]   But if you get really good at specifying and providing a few examples, then you get really
[00:22:47.980 --> 00:22:48.980]   good results.
[00:22:48.980 --> 00:22:50.640]   And that's not a machine learning skill.
[00:22:50.640 --> 00:22:56.800]   That's almost more of a task specification management skill.
[00:22:56.800 --> 00:23:01.700]   And so I feel like a lot of people can kind of pick that up really quickly.
[00:23:01.700 --> 00:23:04.060]   I think that I've been really excited about that.
[00:23:04.060 --> 00:23:09.860]   Just seeing so many people get access to these models that just seem like you have to have
[00:23:09.860 --> 00:23:13.780]   a PhD in machine learning to work with before.
[00:23:13.780 --> 00:23:19.740]   I feel like I've heard of people talk about a new role called prompt engineer that might
[00:23:19.740 --> 00:23:24.380]   be related to this, so figuring out how to prompt GPT-3 to get it to do what you want
[00:23:24.380 --> 00:23:25.380]   it to do.
[00:23:25.380 --> 00:23:32.560]   So this one is interesting because early on when we had the first version of the API,
[00:23:32.560 --> 00:23:41.120]   we had a really smart guy who is a world-renowned author, but also a programmer, Andrew Main.
[00:23:41.120 --> 00:23:48.480]   He was one of the early users of the API, and he got the internal name of the prompt
[00:23:48.480 --> 00:23:50.480]   whisperer, or GPT-3 whisperer.
[00:23:50.480 --> 00:23:55.760]   He really knew how to craft the prompts to get the best results.
[00:23:55.760 --> 00:24:01.680]   And since it's been trained on the internet, you need to put your mind in how would a text
[00:24:01.680 --> 00:24:02.680]   on the internet start.
[00:24:02.680 --> 00:24:07.880]   So if you wanted a really good recipe, you had to start writing in the tone of a recipe
[00:24:07.880 --> 00:24:10.880]   book or a food blog post or something like that.
[00:24:10.880 --> 00:24:15.280]   It's not like you could just ask the model to do what you wanted it to do.
[00:24:15.280 --> 00:24:18.760]   So I think initially there was a big piece to that.
[00:24:18.760 --> 00:24:24.760]   You really had to be good at understanding the intricacies of GPT-3 and design really
[00:24:24.760 --> 00:24:25.760]   good prompts.
[00:24:25.760 --> 00:24:31.480]   Over the past one and a half years since we launched, we saw people struggling with this
[00:24:31.480 --> 00:24:32.480]   a lot.
[00:24:32.480 --> 00:24:38.200]   So we developed a new set of models that we call the instruct GPT, which actually just
[00:24:38.200 --> 00:24:42.440]   like last week became the default in our API.
[00:24:42.440 --> 00:24:46.840]   And the reason we're calling instruct GPT is because you just provide instructions.
[00:24:46.840 --> 00:24:51.120]   So I would say prompt design is a little bit less of a thing now.
[00:24:51.120 --> 00:24:54.200]   You could just tell the model what you wanted to do and provide a few examples.
[00:24:54.200 --> 00:24:59.040]   There's still a little thing about the formatting might impact how you provide your examples
[00:24:59.040 --> 00:25:00.040]   and so on.
[00:25:00.040 --> 00:25:04.600]   GPT-3 is super robust to that, but sometimes it does matter a little bit.
[00:25:04.600 --> 00:25:07.040]   Some tweaking matters.
[00:25:07.040 --> 00:25:11.640]   But I would say it's less of a thing now than it was a year ago.
[00:25:11.640 --> 00:25:15.680]   And my hope is that it becomes less and less of a thing and it becomes much more almost
[00:25:15.680 --> 00:25:18.120]   interactive.
[00:25:18.120 --> 00:25:21.680]   And you've also launched the ability to fine tune the models.
[00:25:21.680 --> 00:25:25.360]   What's the thinking there and where is that useful?
[00:25:25.360 --> 00:25:32.120]   The surprising thing with GPT-3 was that you got really good results, zero shot, where
[00:25:32.120 --> 00:25:35.000]   you only provided an example.
[00:25:35.000 --> 00:25:39.920]   No example, just instructions of like, "Hey, translate this sentence from German to English."
[00:25:39.920 --> 00:25:46.040]   Or you provided a few shot examples where you provide a few pairs of German and English.
[00:25:46.040 --> 00:25:51.280]   And with just a few shot examples, you could get just surprising good results.
[00:25:51.280 --> 00:25:56.720]   But what that meant in practice is that the accuracies are very task dependent, but for
[00:25:56.720 --> 00:26:01.800]   some tasks, maybe 30% of the time, you got an output that was acceptable to put in a
[00:26:01.800 --> 00:26:02.800]   product.
[00:26:02.800 --> 00:26:07.880]   And then for other tasks that were more simple, you'll get it maybe 70% of the time.
[00:26:07.880 --> 00:26:12.840]   And so when it's not good every time, you have to be very clever in the way you expose
[00:26:12.840 --> 00:26:13.840]   it in your product.
[00:26:13.840 --> 00:26:18.520]   And that's why, for example, it worked well for a lot of those copywriting companies,
[00:26:18.520 --> 00:26:22.640]   because you could just provide a few examples and you knew that at least one of them would
[00:26:22.640 --> 00:26:23.640]   be good.
[00:26:23.640 --> 00:26:26.280]   And that's all the user needs.
[00:26:26.280 --> 00:26:30.840]   But with fine tuning, what you can do is basically you can customize your model.
[00:26:30.840 --> 00:26:35.200]   So you can provide more examples of the inputs and outputs you want to do.
[00:26:35.200 --> 00:26:41.240]   If you want to do translation or if you want to summarize articles, you can provide a few
[00:26:41.240 --> 00:26:45.600]   hundred examples of articles that have been human written summaries.
[00:26:45.600 --> 00:26:50.200]   And you can actually update GPT-3 to do much better at that task.
[00:26:50.200 --> 00:26:52.160]   You couldn't put all those examples in your prompt.
[00:26:52.160 --> 00:26:54.480]   The prompt has limited space.
[00:26:54.480 --> 00:27:00.400]   But with fine tuning, you're working these examples into the connections of these neural
[00:27:00.400 --> 00:27:02.620]   network, into the weights of the neural network.
[00:27:02.620 --> 00:27:07.080]   And so in some way, you have an infinite prompt.
[00:27:07.080 --> 00:27:08.520]   You can provide as many examples as you want.
[00:27:08.520 --> 00:27:12.600]   Obviously, the more examples, the longer it would take to fine tune and the more costly
[00:27:12.600 --> 00:27:13.600]   it would be.
[00:27:13.600 --> 00:27:18.860]   But fine tuning is basically that concept of taking a bunch of input and output examples
[00:27:18.860 --> 00:27:23.320]   and working them into the model and getting a new version of the model out that's really
[00:27:23.320 --> 00:27:26.560]   good at that task for which you provided examples.
[00:27:26.560 --> 00:27:32.120]   It turns out with only a few hundred examples or around 100 examples, you can get significant
[00:27:32.120 --> 00:27:33.560]   boosts in accuracy.
[00:27:33.560 --> 00:27:39.120]   So we had a number of customers that have used it, like KeeperTax, they're doing these,
[00:27:39.120 --> 00:27:44.080]   they're analyzing transactions to find these tax write-offs and stuff like that.
[00:27:44.080 --> 00:27:48.000]   And so what they're doing is they're extracting the relevant pieces of text, they're classifying
[00:27:48.000 --> 00:27:49.000]   and so on.
[00:27:49.000 --> 00:27:52.040]   And so they fine tune models and got much, much better results with fine tune models,
[00:27:52.040 --> 00:27:53.040]   for example.
[00:27:53.040 --> 00:27:55.600]   And we've seen that over and over again with our customers.
[00:27:55.600 --> 00:27:58.960]   They can get really good results that can often be good enough for a prototype, but
[00:27:58.960 --> 00:28:03.080]   then in order to get it to high enough accuracy to put it in production, which is usually
[00:28:03.080 --> 00:28:09.720]   more than 90% or 95 or 99%, fine tuning on some datasets that they have or they've put
[00:28:09.720 --> 00:28:11.840]   together gets them all the way.
[00:28:11.840 --> 00:28:18.160]   So that enabled many more applications than you could do before.
[00:28:18.160 --> 00:28:21.240]   So we just made it very simple to do this fine tuning.
[00:28:21.240 --> 00:28:22.240]   Cool.
[00:28:22.240 --> 00:28:26.680]   And I have to ask you about the Weights & Biases integration.
[00:28:26.680 --> 00:28:27.920]   We're so excited about it.
[00:28:27.920 --> 00:28:32.400]   I don't know if people listening would know that you used Weights & Biases from the very
[00:28:32.400 --> 00:28:37.640]   early days and provided a ton of incredibly useful feedback that's in the product, but
[00:28:37.640 --> 00:28:44.000]   I was curious how you thought about how that integration might be useful for users of GPT-3.
[00:28:44.000 --> 00:28:48.840]   So I think this is the background of my usage of Weights & Biases.
[00:28:48.840 --> 00:28:56.600]   I was one of the first users and it just improved my research workflow so much that I'm a big
[00:28:56.600 --> 00:28:58.600]   Weights & Biases spokesperson now.
[00:28:58.600 --> 00:29:04.840]   Basically what it does is that it allows you to track your experiments in a really nice
[00:29:04.840 --> 00:29:05.840]   way.
[00:29:05.840 --> 00:29:09.200]   As you're training your models, you can get all the stats.
[00:29:09.200 --> 00:29:13.400]   Anybody who's trained machine learning models knows that you have to look at a bunch of
[00:29:13.400 --> 00:29:18.040]   curves as you're doing your training to make sure that the models are learning in the way
[00:29:18.040 --> 00:29:19.040]   that you want.
[00:29:19.040 --> 00:29:25.240]   And a lot of the work you do as a machine learning engineer is to do that iteration
[00:29:25.240 --> 00:29:27.720]   on your models and seeing if you can improve your results.
[00:29:27.720 --> 00:29:31.480]   And a lot of that is looking at those learning graphs and so on.
[00:29:31.480 --> 00:29:34.720]   And it's really good because Weights & Biases provides you with this kind of history of
[00:29:34.720 --> 00:29:38.880]   the experiments you run, that you compare experiments and that you can track your progress
[00:29:38.880 --> 00:29:41.280]   and share it with your team and so on.
[00:29:41.280 --> 00:29:46.880]   And what we did is basically make an integration so that as you fine tune your models, your
[00:29:46.880 --> 00:29:52.280]   GPT models via our API, all your experiments, all your training runs show up in the Weights
[00:29:52.280 --> 00:29:54.560]   & Biases interface.
[00:29:54.560 --> 00:29:58.520]   So you get that same convenience, but now for things that are training in our clusters
[00:29:58.520 --> 00:29:59.520]   and so on.
[00:29:59.520 --> 00:30:04.560]   So you can kind of see as our fine tuning process is happening, as the model is updating
[00:30:04.560 --> 00:30:08.520]   its weights based on each new iteration or going through the data set, you can kind of
[00:30:08.520 --> 00:30:11.800]   see your metrics and so on improve.
[00:30:11.800 --> 00:30:16.680]   And we provide a number of different parameters so it lets you iterate and try out different
[00:30:16.680 --> 00:30:18.800]   parameters and so on and see your progress.
[00:30:18.800 --> 00:30:23.960]   So yeah, it's just much more delightful to train your models that way, to kind of have
[00:30:23.960 --> 00:30:28.880]   that place where you can go and look at your results in an ongoing way.
[00:30:28.880 --> 00:30:31.800]   So that's what's super exciting integration for us.
[00:30:31.800 --> 00:30:36.080]   It lets you kind of keep track of all your fine tunes in a much better way than we have
[00:30:36.080 --> 00:30:37.520]   like a command line interface.
[00:30:37.520 --> 00:30:41.440]   It's not at all as pretty as the Weights & Biases way of tracking things.
[00:30:41.440 --> 00:30:46.240]   Boris, you actually said you did the integration and you said it was one line.
[00:30:46.240 --> 00:30:47.240]   Is that right?
[00:30:47.240 --> 00:30:51.400]   I mean, my question for you is more how you thought about how it might be used, but I'm
[00:30:51.400 --> 00:30:54.000]   curious, was it really a one line integration?
[00:30:54.000 --> 00:30:59.240]   I mean, there's a few more in the code, but the way for the user is just to type a line,
[00:30:59.240 --> 00:31:05.280]   to type like OpenAI 1DB sync and you can automatically sync all these runs to a dashboard.
[00:31:05.280 --> 00:31:10.840]   The idea was that there's a lot of people who use the API that are not ML engineers.
[00:31:10.840 --> 00:31:15.880]   So you don't want them to have to learn, okay, what am I supposed to log or how do I take
[00:31:15.880 --> 00:31:16.880]   care of a data set?
[00:31:16.880 --> 00:31:20.760]   And the OpenAI API was like so convenient when you want to train a model.
[00:31:20.760 --> 00:31:25.520]   You just pass a file that is your data set and it cleans up the data set and then you
[00:31:25.520 --> 00:31:27.920]   pass a new command and it fine tunes everything.
[00:31:27.920 --> 00:31:31.360]   So it was a bit the idea of keeping the same simplicity.
[00:31:31.360 --> 00:31:35.760]   So you will just type that one command and then all the magic happens behind the scene
[00:31:35.760 --> 00:31:40.520]   and you have all your visuals and you can compare your models and see like, is it worth
[00:31:40.520 --> 00:31:42.080]   giving more training samples?
[00:31:42.080 --> 00:31:44.600]   How much did my model improve from that?
[00:31:44.600 --> 00:31:47.360]   What is the effect of tweaking that little parameter here?
[00:31:47.360 --> 00:31:50.120]   And what data set did I have when I trained that model?
[00:31:50.120 --> 00:31:54.760]   So it's trying to make it as easy as possible for users to benefit from all the features
[00:31:54.760 --> 00:31:58.280]   when they don't necessarily know what's on the basis initially.
[00:31:58.280 --> 00:32:02.640]   And I guess for both of you, what are the parameters that you can actually tweak?
[00:32:02.640 --> 00:32:07.280]   Because the way you've described it, it sounds to me like there might not be any parameters.
[00:32:07.280 --> 00:32:09.680]   How do parameters get involved here?
[00:32:09.680 --> 00:32:14.600]   So before I answer that question, one thing that really stands out to me, by the way,
[00:32:14.600 --> 00:32:18.720]   why I really like this integration generally was that there is this concept of just making
[00:32:18.720 --> 00:32:21.560]   these kind of advanced things very simple.
[00:32:21.560 --> 00:32:27.680]   And I think I still remember when Lucas, you, Sean, and Chris kind of did the first Ways
[00:32:27.680 --> 00:32:33.760]   and Biases demo and it was basically just like import 1B and it was like to kind of
[00:32:33.760 --> 00:32:35.840]   just start logging the experiment.
[00:32:35.840 --> 00:32:40.320]   I think that philosophy of just making it super simple to get going is something we
[00:32:40.320 --> 00:32:45.120]   have tried to also do in our API, where it's like you import open AI and then a single
[00:32:45.120 --> 00:32:50.240]   API call, a single layer of Python or JavaScript kind of gets you to use GPT-3 and start creating
[00:32:50.240 --> 00:32:52.120]   kind of completions and stuff.
[00:32:52.120 --> 00:32:56.880]   I really like that kind of simplicity and that's what we try to do with this integration.
[00:32:56.880 --> 00:33:03.600]   But to your question about the kind of parameters, we've tried to make this quite simple in our
[00:33:03.600 --> 00:33:06.040]   API.
[00:33:06.040 --> 00:33:08.760]   We try to kind of make the defaults very, very good.
[00:33:08.760 --> 00:33:12.520]   And generally you can get really good results with fine tuning without fiddling much with
[00:33:12.520 --> 00:33:16.920]   the parameters at all, but some kind of makes more of a difference.
[00:33:16.920 --> 00:33:21.280]   Like you can set, for example, the learning rate, that's how much you're updating the
[00:33:21.280 --> 00:33:24.320]   weights with each learning step.
[00:33:24.320 --> 00:33:27.920]   You can set things like how many passes you want to go through the data.
[00:33:27.920 --> 00:33:32.000]   It turns out if you go through the data too many times, then you're going to overfit on
[00:33:32.000 --> 00:33:33.000]   your data set.
[00:33:33.000 --> 00:33:37.480]   So these GPT-3 models being really big, you often only need like on the order of like
[00:33:37.480 --> 00:33:41.120]   two to five iterations through your data to get really good results.
[00:33:41.120 --> 00:33:44.240]   And if you go further than that, you sometimes overfit.
[00:33:44.240 --> 00:33:49.720]   And there are more advanced parameters as well, but I cannot be playing a bit with the
[00:33:49.720 --> 00:33:52.760]   number of epochs you want to train it for and their learning rate.
[00:33:52.760 --> 00:33:54.960]   That gets you 90% of the way there.
[00:33:54.960 --> 00:34:00.200]   And if you start fiddling with other parameters, it's not going to give you that much more.
[00:34:00.200 --> 00:34:06.320]   Was part of the thinking of leaving the parameters in to just give the person tweaking it the
[00:34:06.320 --> 00:34:10.040]   joy of messing with parameters?
[00:34:10.040 --> 00:34:15.640]   I think honestly, I would love it if it was completely automatic.
[00:34:15.640 --> 00:34:20.800]   That said, we do have a number of more research-oriented customers who really do like the fiddling.
[00:34:20.800 --> 00:34:23.280]   So I think it would be hard for us to remove it.
[00:34:23.280 --> 00:34:28.440]   But as I said, we have these two camps of users, the researchers and the developers.
[00:34:28.440 --> 00:34:31.400]   And developers keep telling us like, "Hey, I just want one button.
[00:34:31.400 --> 00:34:33.840]   I just want the best model to come out."
[00:34:33.840 --> 00:34:38.320]   And then a lot of the researchers want to fiddle more with the parameters.
[00:34:38.320 --> 00:34:42.880]   And I think we can probably satisfy both for a long time.
[00:34:42.880 --> 00:34:48.000]   Boris, I don't know which category you put yourself in because you make some amazing,
[00:34:48.000 --> 00:34:49.000]   beautiful demos.
[00:34:49.000 --> 00:34:51.960]   And I know that you love to tweak parameters.
[00:34:51.960 --> 00:34:56.520]   I'm curious your experience playing with the GPT-3 model.
[00:34:56.520 --> 00:35:00.560]   I definitely like having the good default because initially you don't really know what
[00:35:00.560 --> 00:35:01.560]   you should change.
[00:35:01.560 --> 00:35:06.440]   And let's say you would choose the wrong parameter and nothing works, it wouldn't be a nice experience.
[00:35:06.440 --> 00:35:11.160]   So I like that if you don't choose anything, it's already going to be pretty good.
[00:35:11.160 --> 00:35:14.920]   Then I really like to tweak the parameters to see, okay, what would be the effect and
[00:35:14.920 --> 00:35:16.320]   try to play with intuition.
[00:35:16.320 --> 00:35:20.480]   And in addition to the parameters that Peter mentioned, there's two that interest me a
[00:35:20.480 --> 00:35:22.080]   lot too.
[00:35:22.080 --> 00:35:24.440]   You can decide which model you fine tune.
[00:35:24.440 --> 00:35:26.360]   So there's a model of different size.
[00:35:26.360 --> 00:35:32.280]   And if you use a larger model, maybe your API is going to be a bit slower, but your
[00:35:32.280 --> 00:35:34.840]   accuracy will be better.
[00:35:34.840 --> 00:35:37.400]   And maybe sometimes you don't need it, maybe sometimes you need it.
[00:35:37.400 --> 00:35:40.600]   So I like to see the effect of which model I use.
[00:35:40.600 --> 00:35:45.040]   And I like to also see the effect of how many training samples can I give.
[00:35:45.040 --> 00:35:51.160]   Like if I give only 20 samples versus giving 100 or 200, because then it gives you an idea
[00:35:51.160 --> 00:35:56.240]   on how much my model is going to be better as I develop a larger dataset.
[00:35:56.240 --> 00:36:01.160]   So those are kind of parameters I like to play with and see what other predictions based
[00:36:01.160 --> 00:36:02.160]   on this.
[00:36:02.160 --> 00:36:05.800]   Yeah, that last one, I think it's actually super important.
[00:36:05.800 --> 00:36:09.720]   I think it's one of the most common advice we give people over and over again.
[00:36:09.720 --> 00:36:13.560]   It's like, start with a small set of examples, then double it and see how much of a different
[00:36:13.560 --> 00:36:14.560]   improvement you get.
[00:36:14.560 --> 00:36:19.040]   Usually, if you double your amount of training data, then you're going to see some linear
[00:36:19.040 --> 00:36:21.440]   improvement in your error rate.
[00:36:21.440 --> 00:36:26.120]   So if you have 10% error rate or something, then you double your training data, you're
[00:36:26.120 --> 00:36:28.320]   going to get down to maybe 8% error rate.
[00:36:28.320 --> 00:36:31.160]   And then you double it again, you get down to 6% error rate.
[00:36:31.160 --> 00:36:35.880]   And so if you can start seeing that trend, then you can suddenly get a sense of how much
[00:36:35.880 --> 00:36:41.080]   would it actually cost me in terms of labeling more data and so on to get the result that
[00:36:41.080 --> 00:36:42.520]   I want and so on.
[00:36:42.520 --> 00:36:46.080]   So it's a very powerful thing to do.
[00:36:46.080 --> 00:36:49.280]   Are the results of training these models reproducible?
[00:36:49.280 --> 00:36:54.320]   How much variability is there each time you fine tune it?
[00:36:54.320 --> 00:36:59.280]   Would you get the same model if you fine tuned on the same data two different times?
[00:36:59.280 --> 00:37:04.640]   I think so in principle, you can set it up to be quite reproducible.
[00:37:04.640 --> 00:37:09.800]   If you basically train it on the same data, basically what do you want to do when you
[00:37:09.800 --> 00:37:15.240]   train is on each iteration, training iteration, you have a batch of data, a number of examples.
[00:37:15.240 --> 00:37:20.480]   You can actually set the batch size, how many samples per update you want.
[00:37:20.480 --> 00:37:23.160]   And I think it defaults to 32 or something like that.
[00:37:23.160 --> 00:37:25.160]   And when you do that, you also want to shuffle the data.
[00:37:25.160 --> 00:37:27.560]   So you want to take a random sample of your training data.
[00:37:27.560 --> 00:37:32.440]   As long as you keep those randomizations consistent between your training runs, you're essentially
[00:37:32.440 --> 00:37:35.000]   going to get the same model at the end of it.
[00:37:35.000 --> 00:37:37.040]   It's going to be fairly reproducible.
[00:37:37.040 --> 00:37:44.240]   The only caveat is that in practice, and this is true even for inference, we have a parameter
[00:37:44.240 --> 00:37:48.920]   called temperature where you can set the variability in the output, higher temperature, the more
[00:37:48.920 --> 00:37:49.920]   variability.
[00:37:49.920 --> 00:37:55.320]   And even if you put it at zero, there's no real guarantee that you're going to get completely
[00:37:55.320 --> 00:38:01.480]   deterministic output because there's enough noise and little weirdness with floating point
[00:38:01.480 --> 00:38:07.880]   arithmetic and so on in these GPUs with these really big models that it's very hard to guarantee
[00:38:07.880 --> 00:38:10.280]   complete determinism.
[00:38:10.280 --> 00:38:13.720]   So we get people asking about that a lot.
[00:38:13.720 --> 00:38:17.800]   And the answer is always like, well, unfortunately we can't provide that, but you can get something
[00:38:17.800 --> 00:38:21.600]   that's fairly ghosted, but you should just make your experiment robust enough that you
[00:38:21.600 --> 00:38:25.080]   don't really care too much about the determinism.
[00:38:25.080 --> 00:38:32.080]   I would think operationally having everyone have their own fine-tuned model would be much
[00:38:32.080 --> 00:38:38.360]   more of an infrastructure challenge than everybody using the API that hits the same model.
[00:38:38.360 --> 00:38:42.400]   Has that been a big undertaking to allow that to happen?
[00:38:42.400 --> 00:38:48.280]   Do you have to swap in and out the different models as people start to use them?
[00:38:48.280 --> 00:38:50.880]   Yeah, no, for sure.
[00:38:50.880 --> 00:38:56.760]   When we started out, the way we did fine-tuning was basically, in some way you almost rented
[00:38:56.760 --> 00:38:59.520]   a set of GPUs where the models ran on.
[00:38:59.520 --> 00:39:04.880]   And even for some of the absolutely earliest fine-tuning customers, we essentially charged
[00:39:04.880 --> 00:39:09.240]   them by GPU hour to some extent, per hour how much they were using the models.
[00:39:09.240 --> 00:39:13.440]   And even from the very beginning, I think within six months after launching the API,
[00:39:13.440 --> 00:39:16.520]   we had a few select customers that had fine-tuned models and stuff like that.
[00:39:16.520 --> 00:39:18.400]   And that's the way it worked.
[00:39:18.400 --> 00:39:23.520]   The problem with that is if you're trying something new, GPUs hours are expensive.
[00:39:23.520 --> 00:39:29.800]   So you don't want to really pay to reserve a GPU for even a fraction of an hour.
[00:39:29.800 --> 00:39:32.840]   It just adds up really, really quickly.
[00:39:32.840 --> 00:39:38.040]   So we just set a goal of saying, well, as soon as you have fine-tuned your model, you
[00:39:38.040 --> 00:39:40.820]   should immediately be able to just use that model.
[00:39:40.820 --> 00:39:45.680]   And you should just have to pay for basically the tokens that go into it at the inference
[00:39:45.680 --> 00:39:48.320]   time, whatever you put in your prompt.
[00:39:48.320 --> 00:39:53.840]   And so that was definitely a huge engineering challenge, make that experience really great.
[00:39:53.840 --> 00:39:58.240]   You just kick off your fine-tune, when it's done, get the fine-tuned model name out.
[00:39:58.240 --> 00:40:01.760]   And now you can use that model in the API to just get a result immediately.
[00:40:01.760 --> 00:40:03.560]   And you're not going to be charged by hour or whatever.
[00:40:03.560 --> 00:40:06.120]   You're just going to be charged the same way you're going to be charged by the API.
[00:40:06.120 --> 00:40:08.640]   So that was really tricky.
[00:40:08.640 --> 00:40:15.520]   We have an amazing engineering team at OpenAI has really figured out a lot of tricks around
[00:40:15.520 --> 00:40:19.880]   balancing where these models end up and caching them in the right way and so on to create
[00:40:19.880 --> 00:40:21.600]   a great experience around that.
[00:40:21.600 --> 00:40:27.920]   I'm curious if you fine-tune the entire model or you fine-tune just part of it to make it
[00:40:27.920 --> 00:40:28.920]   more efficient.
[00:40:28.920 --> 00:40:33.840]   Yeah, you can imagine, there's just lots of tricks that we're using to make this happen,
[00:40:33.840 --> 00:40:39.280]   but we're constantly trying to figure out new ways of doing it where there are challenges
[00:40:39.280 --> 00:40:44.800]   with if you want to fine-tune a whole 75 billion parameter model, it can get really expensive
[00:40:44.800 --> 00:40:45.800]   and hard and so on.
[00:40:45.800 --> 00:40:49.840]   And there are tricks you can do to make it much faster.
[00:40:49.840 --> 00:40:57.760]   Do you feel like the thing between you and everyone using GPT-3 for natural language
[00:40:57.760 --> 00:41:02.640]   tests is more quality and performance of the model itself?
[00:41:02.640 --> 00:41:04.600]   Or is it something else?
[00:41:04.600 --> 00:41:10.760]   Is it something about integration or monitoring of production or something like that?
[00:41:10.760 --> 00:41:21.920]   I think definitely the key things we focused on when we built the API was what matters
[00:41:21.920 --> 00:41:25.200]   the most is really the capability of the models.
[00:41:25.200 --> 00:41:28.200]   And then number two is you need to have fast inference.
[00:41:28.200 --> 00:41:33.440]   Before we created our API, for large language models, nobody cared about inference.
[00:41:33.440 --> 00:41:36.720]   Everybody cared just how quickly can you train them because that's what mattered, so you
[00:41:36.720 --> 00:41:39.400]   can get your benchmarks result at the end of the day.
[00:41:39.400 --> 00:41:45.160]   So we did just a ton of engineering to make inference super, super fast.
[00:41:45.160 --> 00:41:50.200]   I can remember over the course of the first few months of us getting the first prototype
[00:41:50.200 --> 00:41:55.120]   of the API, a customer starting to use it, we increased the inference speed like 200
[00:41:55.120 --> 00:41:56.120]   fold or something like that.
[00:41:56.120 --> 00:41:59.920]   It was lots of effort that was done to make that super fast.
[00:41:59.920 --> 00:42:04.240]   And then the third thing is things around safety-oriented things.
[00:42:04.240 --> 00:42:09.320]   One of the reasons we invested in these instructive GPT models is that we saw that sometimes you
[00:42:09.320 --> 00:42:14.040]   can get surprising outputs of models that you don't expect.
[00:42:14.040 --> 00:42:19.240]   For example, you might write a very innocent sentence and it might turn very dark for some
[00:42:19.240 --> 00:42:24.400]   reason or you might get some more biased outputs in different ways.
[00:42:24.400 --> 00:42:32.000]   With our instructs-oriented models, by default, they behave in a much more expected way, but
[00:42:32.000 --> 00:42:34.800]   you can also specify the behavior in a much better way.
[00:42:34.800 --> 00:42:40.600]   So I think it turns out when safety and capability comes hand in hand, it just becomes a better
[00:42:40.600 --> 00:42:43.720]   product when you can control it better.
[00:42:43.720 --> 00:42:47.640]   Those are definitely the things we have focused on.
[00:42:47.640 --> 00:42:52.160]   I think we're doing much better on than alternatives that are out there.
[00:42:52.160 --> 00:42:58.160]   But there's also the third thing that we have put a lot of focus on is just making it really
[00:42:58.160 --> 00:42:59.760]   simple to use.
[00:42:59.760 --> 00:43:03.640]   The fact that you don't have to load up models that you can just call a fine-tuned model
[00:43:03.640 --> 00:43:09.000]   that is just a single line of Python to call the API.
[00:43:09.000 --> 00:43:11.840]   That's also been really central to us.
[00:43:11.840 --> 00:43:14.200]   We want this to be easy to use by everyone.
[00:43:14.200 --> 00:43:15.200]   Awesome.
[00:43:15.200 --> 00:43:16.200]   Well, thank you very much.
[00:43:16.200 --> 00:43:21.360]   It's really nice to talk to you and congratulations on making such a successful product.
[00:43:21.360 --> 00:43:22.360]   Thank you.
[00:43:22.360 --> 00:43:26.760]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:43:26.760 --> 00:43:31.480]   to the show notes in the description where you can find links to all the papers that
[00:43:31.480 --> 00:43:35.640]   are mentioned, supplemental material, and a transcription that we worked really hard
[00:43:35.640 --> 00:43:36.640]   to produce.
[00:43:36.640 --> 00:43:37.080]   So check it out.
[00:43:37.080 --> 00:43:39.040]   (upbeat music)

