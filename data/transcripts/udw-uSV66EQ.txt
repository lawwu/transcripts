
[00:00:00.000 --> 00:00:01.080]   It looks really fun.
[00:00:01.080 --> 00:00:02.720]   So I thought I'd dive into that one.
[00:00:02.720 --> 00:00:04.240]   You don't rest.
[00:00:04.240 --> 00:00:05.160]   Yeah.
[00:00:05.160 --> 00:00:07.360]   You're on to the next one already.
[00:00:07.360 --> 00:00:07.920]   Yeah.
[00:00:07.920 --> 00:00:14.560]   It was like, I was like, because the common one got, I think it took about eight
[00:00:14.560 --> 00:00:16.360]   days for the leaderboard to finalize.
[00:00:16.360 --> 00:00:19.000]   And I was like, please lead to finalize the leader.
[00:00:19.000 --> 00:00:21.320]   But you know, when you're in that little in between zone, it's like,
[00:00:21.320 --> 00:00:22.920]   oh, they're not finalizing the leaderboard.
[00:00:22.920 --> 00:00:26.360]   So I thought, okay, I'll settle the nerves a bit by starting a new competition.
[00:00:26.360 --> 00:00:29.760]   Is that how we settle on those?
[00:00:30.760 --> 00:00:33.120]   That's a sign of a true addiction, isn't it?
[00:00:33.120 --> 00:00:34.240]   Yeah.
[00:00:34.240 --> 00:00:39.000]   I remember when our team got the, uh, we were eight, what is the lead on the
[00:00:39.000 --> 00:00:42.280]   pet finder and the leaderboard was saying that it's going to finalize.
[00:00:42.280 --> 00:00:45.640]   I was just like sure that we're going to drop down like this.
[00:00:45.640 --> 00:00:46.720]   This can't happen.
[00:00:46.720 --> 00:00:49.760]   There's going to be something that happens right until it's finalized.
[00:00:49.760 --> 00:00:54.840]   It, obviously it's very rare that something happens, but yeah, it's just this.
[00:00:54.840 --> 00:00:56.320]   And it was like eight days.
[00:00:56.320 --> 00:00:59.400]   It's like, okay, I just need to start a new competition and just not think
[00:00:59.400 --> 00:01:00.840]   about this anymore.
[00:01:00.840 --> 00:01:04.560]   No, but I was like really happy to see your name.
[00:01:04.560 --> 00:01:07.640]   I saw the dragon, sorry, the dinosaur there.
[00:01:07.640 --> 00:01:09.480]   And I was like, is this Anjum?
[00:01:09.480 --> 00:01:10.040]   Oh, no way.
[00:01:10.040 --> 00:01:10.760]   That's awesome.
[00:01:10.760 --> 00:01:11.160]   Yeah.
[00:01:11.160 --> 00:01:12.560]   Yeah.
[00:01:12.560 --> 00:01:13.440]   I was, yeah.
[00:01:13.440 --> 00:01:18.080]   Cause I w I wasn't initially planning to do it solo.
[00:01:18.080 --> 00:01:23.840]   Cause, uh, but because I was on holidays for like the last part of the competition,
[00:01:23.840 --> 00:01:27.200]   I didn't really want to team up with someone and then go away because that's
[00:01:27.200 --> 00:01:28.640]   not really fair on the teammates.
[00:01:29.200 --> 00:01:32.000]   So I thought, okay, I'll see if I can do it solo.
[00:01:32.000 --> 00:01:33.000]   And it seems to work out.
[00:01:33.000 --> 00:01:33.280]   Okay.
[00:01:33.280 --> 00:01:35.280]   And you got the gold medal.
[00:01:35.280 --> 00:01:37.640]   The hardest thing on Kaggle.
[00:01:37.640 --> 00:01:38.360]   Yeah.
[00:01:38.360 --> 00:01:40.040]   So I was pretty happy with that.
[00:01:40.040 --> 00:01:42.360]   No, that was incredible.
[00:01:42.360 --> 00:01:45.200]   We have a few people in already guys.
[00:01:45.200 --> 00:01:46.600]   We just starting with Anjum.
[00:01:46.600 --> 00:01:47.480]   We get to start.
[00:01:47.480 --> 00:01:49.320]   So feel free to say hello.
[00:01:49.320 --> 00:01:51.160]   You should like, I'm just catching up with Anjum.
[00:01:51.160 --> 00:01:52.800]   We've been friends for a long time on Kaggle.
[00:01:52.800 --> 00:01:57.880]   What hardware are you rocking right now?
[00:01:57.880 --> 00:01:59.080]   Did you get the 3090?
[00:01:59.280 --> 00:01:59.720]   Oh yeah.
[00:01:59.720 --> 00:02:04.160]   So I managed to get some, uh, two, I managed to get two 3090s in the end.
[00:02:04.160 --> 00:02:07.840]   So in the UK there was yeah.
[00:02:07.840 --> 00:02:09.280]   Shortage everywhere.
[00:02:09.280 --> 00:02:15.600]   And I was checking the, there's one main importer for 3090s and
[00:02:15.600 --> 00:02:16.840]   they were selling them brand new.
[00:02:16.840 --> 00:02:19.120]   And the prices were just going up and up and up.
[00:02:19.120 --> 00:02:24.240]   And then eventually I said, well, I'll just get one off eBay from one of the scalpers.
[00:02:24.240 --> 00:02:28.720]   I didn't really want to give a scalper money, but it was still cheaper than
[00:02:28.720 --> 00:02:33.400]   what, what it was brand new from the other website, the prices were
[00:02:33.400 --> 00:02:34.560]   just going in one direction.
[00:02:34.560 --> 00:02:37.160]   So I managed to get two founders edition cards.
[00:02:37.160 --> 00:02:39.040]   Um, but they work nicely.
[00:02:39.040 --> 00:02:41.040]   Like what case are you using?
[00:02:41.040 --> 00:02:41.640]   I'm just curious.
[00:02:41.640 --> 00:02:45.360]   I've got a, I think I've got like a, I don't know what it is.
[00:02:45.360 --> 00:02:48.400]   It's a Logitech D750.
[00:02:48.400 --> 00:02:49.320]   It's quite an old one.
[00:02:49.320 --> 00:02:52.240]   A lot of, a lot of bits on my computer.
[00:02:52.240 --> 00:02:56.440]   It's a bit of a Frankenstein computer from previous computers.
[00:02:57.120 --> 00:02:59.080]   Uh, that's how it is.
[00:02:59.080 --> 00:02:59.400]   Right.
[00:02:59.400 --> 00:02:59.960]   Yeah.
[00:02:59.960 --> 00:03:00.160]   Yeah.
[00:03:00.160 --> 00:03:00.560]   It's good.
[00:03:00.560 --> 00:03:02.480]   It's good to carry over bits and pieces.
[00:03:02.480 --> 00:03:07.400]   But in fact, my, the first computer I bought for Kaggle was
[00:03:07.400 --> 00:03:09.760]   actually like a secondhand computer.
[00:03:09.760 --> 00:03:12.360]   It was a old HP workstation.
[00:03:12.360 --> 00:03:13.280]   Okay.
[00:03:13.280 --> 00:03:15.960]   And they super cool, the HP workstations.
[00:03:15.960 --> 00:03:18.880]   And, but, and then I moved to the Middle East.
[00:03:18.880 --> 00:03:22.000]   I was living in Kuwait for a while and the power supply died.
[00:03:22.000 --> 00:03:26.920]   And in HP workstations, everything is custom inside and there was no chance
[00:03:26.920 --> 00:03:28.720]   of me, you can't buy those parts.
[00:03:28.720 --> 00:03:29.120]   Yeah.
[00:03:29.120 --> 00:03:33.800]   And I took, I took it to a random guy in the market and he said, Oh, maybe I can
[00:03:33.800 --> 00:03:37.400]   fix it, maybe I can't and couldn't fix the power supply, so I had to just build
[00:03:37.400 --> 00:03:38.800]   a new computer out there and then.
[00:03:38.800 --> 00:03:45.440]   So I mean, that kind of worked out for the best, you know, secretly I wanted to build
[00:03:45.440 --> 00:03:48.560]   one, so that was cool.
[00:03:48.560 --> 00:03:51.080]   Yeah.
[00:03:51.080 --> 00:03:51.320]   Yeah.
[00:03:51.320 --> 00:03:55.080]   So it keeps, I'm in this little room where my computer lives and it gets very hot.
[00:03:55.080 --> 00:03:56.520]   So, you know, normally you have to keep this.
[00:03:56.520 --> 00:03:57.240]   Tell me about it.
[00:03:57.240 --> 00:04:02.560]   My wife was like, Oh, you're burning so much electricity.
[00:04:02.560 --> 00:04:05.680]   And now that now I've won a little bit of money, she's like, okay, okay.
[00:04:05.680 --> 00:04:06.640]   I understand.
[00:04:06.640 --> 00:04:09.120]   Now you don't have to fight that much.
[00:04:09.120 --> 00:04:12.560]   Awesome.
[00:04:12.560 --> 00:04:18.760]   Again, as a reminder to people joining in, I'm just catching up with Anjum, but feel
[00:04:18.760 --> 00:04:21.000]   free to say, Hey, we'll just get started in a few minutes.
[00:04:24.360 --> 00:04:28.120]   I have like, I have to leave the side lid open of my PC.
[00:04:28.120 --> 00:04:29.520]   I'm sure you've seen the case, right?
[00:04:29.520 --> 00:04:32.840]   Like it's just, it's a mess, honestly.
[00:04:32.840 --> 00:04:34.600]   This is the other one.
[00:04:34.600 --> 00:04:39.680]   The main one that has the two 3090s and A6000 is over outside of the frame.
[00:04:39.680 --> 00:04:40.040]   Yeah.
[00:04:40.040 --> 00:04:40.320]   Yeah.
[00:04:40.320 --> 00:04:41.520]   Yeah.
[00:04:41.520 --> 00:04:46.240]   My, my news, I've got some extra fans, Jerry rigged inside mine.
[00:04:46.240 --> 00:04:48.960]   So the points towards the GPU and stuff.
[00:04:48.960 --> 00:04:50.120]   So that kind of helps.
[00:04:50.200 --> 00:04:55.840]   And also found that, um, obviously there's, there's, there's variation on,
[00:04:55.840 --> 00:04:59.720]   cause on the 3090, the back, the memories on the back, right.
[00:04:59.720 --> 00:05:00.400]   Yeah.
[00:05:00.400 --> 00:05:02.240]   And there's just thermal pads on that.
[00:05:02.240 --> 00:05:07.680]   So, um, there's a little bit of variation on how good they are, like cooling down.
[00:05:07.680 --> 00:05:11.400]   So I found that just swapping the order of the GPUs, there's one GPU that
[00:05:11.400 --> 00:05:12.600]   keeps cooler than the other one.
[00:05:12.600 --> 00:05:14.320]   I found that helps a lot.
[00:05:14.320 --> 00:05:15.760]   Interesting.
[00:05:15.760 --> 00:05:16.960]   Yeah.
[00:05:16.960 --> 00:05:18.400]   I just have the MSI one.
[00:05:18.400 --> 00:05:19.200]   So it's different.
[00:05:19.200 --> 00:05:19.560]   Mm hmm.
[00:05:19.560 --> 00:05:21.600]   That's cool.
[00:05:21.600 --> 00:05:22.000]   Yeah.
[00:05:22.000 --> 00:05:23.680]   All right.
[00:05:23.680 --> 00:05:25.120]   Let's I'll just get started.
[00:05:25.120 --> 00:05:26.400]   I'll take a person, get started.
[00:05:26.400 --> 00:05:30.080]   So just one moment.
[00:05:30.080 --> 00:05:33.760]   So hi everyone.
[00:05:33.760 --> 00:05:35.840]   Thanks for joining the first carrot talk.
[00:05:35.840 --> 00:05:39.040]   I quickly want to point out that we'll be taking questions to the forums.
[00:05:39.040 --> 00:05:43.000]   So, uh, this is discourse by the way, if you're not familiar, you can like.
[00:05:43.000 --> 00:05:44.040]   Who's here.
[00:05:44.040 --> 00:05:47.040]   I can't like my own post, but make sure you say hello in this thread.
[00:05:47.920 --> 00:05:51.600]   And if you'd like to ask questions to Anjum, uh, you can post them here.
[00:05:51.600 --> 00:06:01.080]   I asked him what is his favorite side and hopefully should go through.
[00:06:01.080 --> 00:06:02.800]   So please ask your questions.
[00:06:02.800 --> 00:06:06.080]   There, since we're also live streaming on YouTube and within the zoom call,
[00:06:06.080 --> 00:06:07.840]   it's difficult to monitor everything.
[00:06:07.840 --> 00:06:09.280]   So I'll be taking questions from there.
[00:06:09.280 --> 00:06:12.320]   All right.
[00:06:12.320 --> 00:06:15.160]   With that, I think we're all set to get started.
[00:06:15.160 --> 00:06:24.720]   Welcome to this is I think the first, uh, Kaggle talk at Bateson biases, also
[00:06:24.720 --> 00:06:29.440]   the first child time live and the child time walkthrough I'm like incredibly
[00:06:29.440 --> 00:06:30.960]   excited to be hosting Anjum.
[00:06:30.960 --> 00:06:34.680]   I've known Anjum for a few years through the Kaggle noobs community.
[00:06:34.680 --> 00:06:36.040]   It's an incredible community.
[00:06:36.040 --> 00:06:39.920]   And I just saw his, uh, I can, I was just telling him on the leaderboard.
[00:06:39.920 --> 00:06:42.320]   And I was so happy to see him on the fourth position on
[00:06:42.320 --> 00:06:43.600]   the common lead competition.
[00:06:43.600 --> 00:06:46.600]   So I don't think I was at Bateson biases at the time.
[00:06:46.600 --> 00:06:52.240]   I just instantly reached out and, uh, he kindly agreed to also later do this talk.
[00:06:52.240 --> 00:06:53.240]   So Anjum, thanks.
[00:06:53.240 --> 00:06:54.160]   Thanks for joining us.
[00:06:54.160 --> 00:06:56.800]   I'm really looking forward to diving into a solution today.
[00:06:56.800 --> 00:06:58.080]   No, thank you for having me.
[00:06:58.080 --> 00:07:00.840]   It's a real honor and I'm super excited about this.
[00:07:00.840 --> 00:07:01.880]   This is going to be really fun.
[00:07:01.880 --> 00:07:03.440]   Likewise.
[00:07:03.440 --> 00:07:07.880]   I have gone through a solution, so I know a little bit more, but, uh, I'm sure
[00:07:07.880 --> 00:07:09.080]   everyone will get to learn it.
[00:07:09.080 --> 00:07:12.040]   Real quick.
[00:07:12.040 --> 00:07:13.440]   I'll point out the resources.
[00:07:13.440 --> 00:07:15.960]   These are there on the thread as well.
[00:07:15.960 --> 00:07:18.440]   You can find Anjum's writeup on Kaggle.
[00:07:18.440 --> 00:07:20.880]   That is just a high level description description.
[00:07:20.880 --> 00:07:24.640]   He's also published his GitHub repository, which you can find.
[00:07:24.640 --> 00:07:28.200]   If you're new to Kaggle, I'll probably point this out, but Kaggle has this new
[00:07:28.200 --> 00:07:32.640]   mechanism where you can train models offline for a few competitions, and then
[00:07:32.640 --> 00:07:34.920]   you're forced to do inference on Kaggle.
[00:07:34.920 --> 00:07:39.760]   So you're like constrained in a way of how crazy ensembles can you build.
[00:07:39.800 --> 00:07:44.320]   Uh, I'm still built, I think around 30 or 38 models, if I remember correctly.
[00:07:44.320 --> 00:07:47.160]   So there's, there's still some leeway there.
[00:07:47.160 --> 00:07:52.400]   Uh, I'll be using a dashboard that one of our ambassadors built, uh, because
[00:07:52.400 --> 00:07:54.200]   I'm lazy and I didn't want to do EDA.
[00:07:54.200 --> 00:07:55.640]   So you can check that out as well.
[00:07:55.640 --> 00:08:01.600]   And, uh, this is essentially CTDS 2.0, uh, like I pointed out on Twitter.
[00:08:01.600 --> 00:08:07.320]   So I'll quickly go through what is CTDS 2.0 and here's the agenda for today.
[00:08:07.360 --> 00:08:12.480]   Uh, after that, you know, I just, I like the reason for me doing these
[00:08:12.480 --> 00:08:15.920]   interviews was like, I know Anjun's solution.
[00:08:15.920 --> 00:08:19.480]   I know he built these models, but I'm also really curious.
[00:08:19.480 --> 00:08:22.240]   Like, how did he become this awesome Kaggle competition master?
[00:08:22.240 --> 00:08:27.280]   Like, how did he get to the top of, I think in the top hundred in the leaderboard?
[00:08:27.280 --> 00:08:30.440]   Like, I want to know, how did he study these things?
[00:08:30.440 --> 00:08:33.080]   So that's why I asked these questions and we'll be starting there.
[00:08:34.000 --> 00:08:38.560]   Followed by understanding the competition, and then we'll dive into the dataset and
[00:08:38.560 --> 00:08:43.080]   Anjun will present his solution and you all are free to ask any questions that you want.
[00:08:43.080 --> 00:08:45.320]   Keep the questions coming.
[00:08:45.320 --> 00:08:48.240]   I'll keep looking at them here, here and there.
[00:08:48.240 --> 00:08:53.280]   So this, like I said, is, uh, in a way, I think data science 2.0,
[00:08:53.280 --> 00:08:54.760]   because I ran out of ideas.
[00:08:54.760 --> 00:08:56.320]   So that's why I called it this.
[00:08:56.320 --> 00:08:59.920]   Uh, what I'm trying to do is live interviews here.
[00:08:59.960 --> 00:09:05.160]   Uh, these are also community ask me any things and what we really want to achieve
[00:09:05.160 --> 00:09:11.280]   as a community is, uh, understanding solutions that are up there on the top of the leaderboard.
[00:09:11.280 --> 00:09:17.960]   I truly believe that Kaggle competitions are a great way of learning and winners, as you'll
[00:09:17.960 --> 00:09:22.600]   find from Anjun have these like interesting techniques that are so buried in the forums
[00:09:22.600 --> 00:09:26.080]   and you can only understand if you've taken part in the competition.
[00:09:27.400 --> 00:09:32.000]   So we're trying to uncover those, uh, through the series, at least that's what I hope, but
[00:09:32.000 --> 00:09:35.680]   I'll, I look forward to hearing from you if you find that is the case or not.
[00:09:35.680 --> 00:09:40.000]   So I'll start by introducing Anjun.
[00:09:40.000 --> 00:09:42.520]   Uh, he's Anjun Sayed on Twitter.
[00:09:42.520 --> 00:09:44.040]   Uh, give him a follow there.
[00:09:44.040 --> 00:09:49.000]   He's competition's master currently in the top hundred of the worldwide rankings.
[00:09:49.000 --> 00:09:53.560]   That's out of 166,000 data scientists.
[00:09:53.800 --> 00:09:58.440]   He's a senior data scientist at BP and he's a physicist by background.
[00:09:58.440 --> 00:10:04.520]   So I want to start by the traditional, uh, data science question and answer.
[00:10:04.520 --> 00:10:08.600]   Uh, first of all, thanks again, Anjun for saying yes to this request as well.
[00:10:08.600 --> 00:10:12.040]   Uh, really looking forward to knowing more about your journey.
[00:10:12.040 --> 00:10:15.680]   I know you started as a physicist, so maybe you can tell us about your journey
[00:10:15.680 --> 00:10:17.320]   and how did you find data science?
[00:10:17.320 --> 00:10:17.840]   Yeah.
[00:10:17.840 --> 00:10:23.360]   So I've been, yeah, probably been active on Kaggle for the last three
[00:10:23.360 --> 00:10:25.560]   years, but I joined Kaggle a long time ago.
[00:10:25.560 --> 00:10:27.920]   So I joined Kaggle probably six years ago.
[00:10:27.920 --> 00:10:31.760]   Uh, but yeah, I've been coding for ages.
[00:10:31.760 --> 00:10:36.480]   So I think I learned how to code probably when I was like 12 or 13.
[00:10:36.480 --> 00:10:38.720]   So my parents got me this Lego set.
[00:10:38.720 --> 00:10:40.760]   So it was called like Lego Mindstorms.
[00:10:40.760 --> 00:10:45.720]   And there was this, um, programmable, programmable brick and you could attach
[00:10:45.720 --> 00:10:47.280]   little sensors and motors to it.
[00:10:47.280 --> 00:10:51.680]   I tried to build this photocopier with this light, it's a light sensor on one
[00:10:51.680 --> 00:10:56.000]   side and then like a little pen on the other side and it'll, it'll scan a piece
[00:10:56.000 --> 00:11:01.160]   of paper and then I found that the, the little scripting, the coding thing that
[00:11:01.160 --> 00:11:03.040]   Lego supplied to this stuff is too slow.
[00:11:03.040 --> 00:11:08.320]   So all my images are coming out blurry and I found these guys at MIT had hacked
[00:11:08.320 --> 00:11:10.520]   the firmware for this brick so you could run C on it.
[00:11:10.520 --> 00:11:16.880]   So I built, I rewrote all my programming C to get this photocopier to work out a Lego.
[00:11:16.880 --> 00:11:19.920]   So that's when I started learning how to code when I was a kid.
[00:11:20.640 --> 00:11:21.640]   So I started with C.
[00:11:21.640 --> 00:11:22.960]   You learned C when you were 13?
[00:11:22.960 --> 00:11:23.560]   Yeah.
[00:11:23.560 --> 00:11:30.080]   That was the first language I learned, which sounds crazy now looking back.
[00:11:30.080 --> 00:11:35.520]   Um, and then I went to, yeah, studied physics at university, uh,
[00:11:35.520 --> 00:11:37.560]   specialized in particle physics.
[00:11:37.560 --> 00:11:45.320]   So when, um, we started doing sort of detailed simulations of the Higgs decay.
[00:11:45.320 --> 00:11:48.200]   So using Monte Carlo simulation, that was all done in C as well.
[00:11:48.200 --> 00:11:51.640]   Cause when you're doing simulations, you're going to do them really fast.
[00:11:51.640 --> 00:11:56.280]   So we were trying to characterize what the Higgs decay would look like before
[00:11:56.280 --> 00:11:58.320]   the large Hadron collider started up.
[00:11:58.320 --> 00:12:00.200]   So we know, knew what they were looking for.
[00:12:00.200 --> 00:12:04.120]   Kind of knew what, where the Higgs would sit, but wanted to see what that looked like.
[00:12:04.120 --> 00:12:08.720]   Um, yeah, so we did, did a bit of that at university.
[00:12:08.720 --> 00:12:12.800]   So, and then I started work at BP as a petrophysicist.
[00:12:12.800 --> 00:12:15.640]   So petrophysicist is a strange sounding job.
[00:12:16.400 --> 00:12:21.120]   But, um, so it's when in the oil and gas industry, we drill these
[00:12:21.120 --> 00:12:23.320]   wells where the oil and gas comes up.
[00:12:23.320 --> 00:12:28.040]   And when you drill the well, you will send down a little tool on a
[00:12:28.040 --> 00:12:29.760]   piece of wire called a logging tool.
[00:12:29.760 --> 00:12:35.080]   And it'll measure things like electromagnetic, acoustic, um,
[00:12:35.080 --> 00:12:37.080]   nuclear properties of the rock.
[00:12:37.080 --> 00:12:42.920]   And it was my job to take that data and then turn it into something useful.
[00:12:42.920 --> 00:12:46.880]   So like the porosity of the rock, is there oil or gas or water there?
[00:12:46.880 --> 00:12:49.040]   How well does that rock flow?
[00:12:49.040 --> 00:12:53.560]   Um, so it's kind of, I was a scientist working with data, but it wasn't
[00:12:53.560 --> 00:12:55.240]   called a data scientist at that point.
[00:12:55.240 --> 00:13:01.040]   Um, so, um, as, as many scientists in, in data science these days.
[00:13:01.040 --> 00:13:08.800]   And, um, and at that time, so Python was almost, Python was popular then, but,
[00:13:08.800 --> 00:13:11.640]   um, it was almost seen like a little bit of a toy language.
[00:13:11.640 --> 00:13:17.160]   So, you know, in, in Microsoft Excel, you can write little macros in visual basic.
[00:13:17.160 --> 00:13:20.680]   A lot of software at that time had Python built into it.
[00:13:20.680 --> 00:13:22.480]   So you can do little custom scripts.
[00:13:22.480 --> 00:13:26.880]   So if I wanted to build a, um, convert density into porosity, I'd write a
[00:13:26.880 --> 00:13:29.160]   little one-liner in Python to do that.
[00:13:29.160 --> 00:13:33.160]   So I've been using Python that way in like our little soft in specialist
[00:13:33.160 --> 00:13:36.840]   software like that for 12 years or something, it's quite common, you know,
[00:13:36.840 --> 00:13:40.760]   even like in music production software, you have a little Python kernel.
[00:13:41.080 --> 00:13:43.880]   Now Python is probably one of the most popular languages in the world.
[00:13:43.880 --> 00:13:45.680]   So it's a bit different now.
[00:13:45.680 --> 00:13:50.600]   Um, and yeah, so through, through that, I learned a bit of Python, got more into
[00:13:50.600 --> 00:13:53.520]   the sort of statistics and machine learning world, and then I just got,
[00:13:53.520 --> 00:13:55.040]   I joined Kaggle six years ago.
[00:13:55.040 --> 00:13:59.360]   And cause, um, at that time, all the blogs and things were saying, Oh, if you
[00:13:59.360 --> 00:14:01.000]   want to learn data science, go to Kaggle.
[00:14:01.000 --> 00:14:03.800]   And Kaggle was a really different place back then.
[00:14:03.800 --> 00:14:06.320]   It was, you know, TensorFlow had just been released.
[00:14:06.320 --> 00:14:07.560]   There was no PyTorch.
[00:14:08.040 --> 00:14:13.960]   Everything was scikit-learn, XGBoost, like GBM, or the really like heavy
[00:14:13.960 --> 00:14:16.480]   grandmasters, they were coding their own random stuff.
[00:14:16.480 --> 00:14:17.200]   It's crazy.
[00:14:17.200 --> 00:14:20.640]   So I still feel the same, like how do you understand?
[00:14:20.640 --> 00:14:25.560]   It's a, it's a very different landscape, but it's still the same feeling.
[00:14:25.560 --> 00:14:29.440]   And, and yeah, there's a lot more different variability.
[00:14:29.440 --> 00:14:34.120]   So in back then the notebooks, you could run R and Julia in the notebooks as well.
[00:14:34.120 --> 00:14:35.880]   So it's quite a lot of diversity in that.
[00:14:36.400 --> 00:14:40.360]   So I entered my first competition, which is, you know, every Christmas Kaggle
[00:14:40.360 --> 00:14:45.160]   has, um, sort of a Santa themed competition is usually an optimization thing.
[00:14:45.160 --> 00:14:47.560]   So now, okay, I can code a little bit.
[00:14:47.560 --> 00:14:49.000]   Uh, I'll give that a go.
[00:14:49.000 --> 00:14:51.000]   And it was so hard.
[00:14:51.000 --> 00:14:56.920]   I came, I think I came in the bottom 5%, 10%, and I got terrified.
[00:14:56.920 --> 00:14:58.880]   Like my level was so low.
[00:14:58.880 --> 00:15:02.000]   I couldn't even understand any of the solutions.
[00:15:02.000 --> 00:15:05.720]   I got terrified of Kaggle and it was probably my biggest regret.
[00:15:05.760 --> 00:15:08.560]   I left Kaggle for about, yeah, that's the one.
[00:15:08.560 --> 00:15:09.100]   Yeah.
[00:15:09.100 --> 00:15:12.240]   Santa's so, so say six years ago.
[00:15:12.240 --> 00:15:13.680]   So that was my first competition.
[00:15:13.680 --> 00:15:15.680]   I got so scared of Kaggle.
[00:15:15.680 --> 00:15:20.960]   I felt like this place was so daunting and I left and it's my biggest regret.
[00:15:20.960 --> 00:15:24.680]   Anyone who's starting out with Kaggle, don't give up, just stick with it
[00:15:24.680 --> 00:15:27.000]   because it's the best place to learn.
[00:15:27.000 --> 00:15:31.080]   And I really regret leaving Kaggle for a few years.
[00:15:31.080 --> 00:15:33.280]   So I went off, did my own thing.
[00:15:33.760 --> 00:15:37.280]   I got buried in a hole of deep reinforcement learning and
[00:15:37.280 --> 00:15:38.720]   all sorts of other projects.
[00:15:38.720 --> 00:15:42.840]   I've, you know, I did a few, I think I tried to do some course courses.
[00:15:42.840 --> 00:15:45.400]   I think I started Andrew Ng's course probably about four
[00:15:45.400 --> 00:15:46.800]   times and never finished it.
[00:15:46.800 --> 00:15:52.680]   Um, and then probably about three years ago, there was a competition, the
[00:15:52.680 --> 00:15:55.360]   TGS salt identification challenge.
[00:15:55.360 --> 00:16:00.200]   Um, so that, that was interesting for me because it had a bit of geophysics.
[00:16:00.200 --> 00:16:05.600]   So I was familiar with the data through work and I just watched, um, all of the
[00:16:05.600 --> 00:16:11.440]   fast AI languages, uh, fast AI lectures on convolution neural nets.
[00:16:11.440 --> 00:16:13.400]   So, okay, let's give this a go.
[00:16:13.400 --> 00:16:19.040]   So I think I finished just outside the bronze, bronze medal territory, but I
[00:16:19.040 --> 00:16:20.680]   was fully committed to this competition.
[00:16:20.680 --> 00:16:21.840]   Like there were three months.
[00:16:21.840 --> 00:16:24.240]   I just really focused on this competition.
[00:16:24.240 --> 00:16:26.400]   So this was your addiction point?
[00:16:26.400 --> 00:16:27.960]   If I may, this is the addiction point.
[00:16:27.960 --> 00:16:28.960]   This is the turning point.
[00:16:28.960 --> 00:16:31.800]   And, um, at this point I was using TensorFlow.
[00:16:31.800 --> 00:16:34.240]   So I built a U-net from scratch.
[00:16:34.240 --> 00:16:35.920]   I built a ResNet from scratch.
[00:16:35.920 --> 00:16:41.200]   I was reading papers with, um, I built with spatial and squeeze,
[00:16:41.200 --> 00:16:43.560]   channel squeeze excitation block.
[00:16:43.560 --> 00:16:45.760]   So those are the bits that are inefficient net these days.
[00:16:45.760 --> 00:16:47.840]   Built all of that from scratch.
[00:16:47.840 --> 00:16:51.320]   Um, just finished outside the top 10% and that's when the addiction started.
[00:16:51.320 --> 00:16:56.640]   And I was looking, when the competition finished, I looked back and I said, wow,
[00:16:56.680 --> 00:17:01.440]   I've learned more in those three months than I did in the previous three years.
[00:17:01.440 --> 00:17:03.400]   I thought, oh, well, this is amazing.
[00:17:03.400 --> 00:17:04.360]   I'm going to stick to this.
[00:17:04.360 --> 00:17:06.520]   And then that's when the addiction started.
[00:17:06.520 --> 00:17:09.240]   So been, been competing ever since then.
[00:17:09.240 --> 00:17:13.080]   Yeah, it's, it's been an incredible journey for you.
[00:17:13.080 --> 00:17:17.360]   I was looking at your profile and since then you've made it to the top hundred.
[00:17:17.360 --> 00:17:19.520]   I, again, like it's, it's so incredible.
[00:17:19.520 --> 00:17:23.320]   I think like people really underestimated, like these are the best of the best
[00:17:23.320 --> 00:17:26.040]   competing on the leaderboard and you made it to the top hundred.
[00:17:26.040 --> 00:17:29.600]   You have two gold medals, eight silver medals.
[00:17:29.600 --> 00:17:34.200]   Uh, one thing I'm curious about is you've competed according to the, uh,
[00:17:34.200 --> 00:17:37.280]   stats that show up against your profile, 90% solo.
[00:17:37.280 --> 00:17:39.880]   So like, is there any reason like you compete solo?
[00:17:39.880 --> 00:17:43.920]   So yeah, I've competed twice.
[00:17:43.920 --> 00:17:47.600]   In fact, it's something that I want to try to get into more.
[00:17:47.600 --> 00:17:50.400]   I want to, is my new year's resolution this year to team up more.
[00:17:50.400 --> 00:17:54.920]   But, um, the two times I've teamed up, it's been really enjoyable.
[00:17:55.320 --> 00:17:57.840]   I've learned so much from it, but it's very intense.
[00:17:57.840 --> 00:18:00.400]   You know, if you don't want to be that person in the team, who's
[00:18:00.400 --> 00:18:01.760]   not pulling your weight, right?
[00:18:01.760 --> 00:18:07.680]   So if I like, for example, in this, the common lit competition, I was actually
[00:18:07.680 --> 00:18:10.840]   away for the last few weeks of the competition, so I didn't want to team
[00:18:10.840 --> 00:18:13.720]   up with someone and disappear for the last week of the competition.
[00:18:13.720 --> 00:18:19.960]   So, um, I like, I kind of liked the freedom of being solo, but it is, I do
[00:18:19.960 --> 00:18:24.360]   want to team up more going forward because, you know, you learn so much
[00:18:24.360 --> 00:18:28.800]   from other people and that additional intensity when you're in a team is,
[00:18:28.800 --> 00:18:35.000]   is actually really good, but it's, yeah, it's unintentional that I am a solo person.
[00:18:35.000 --> 00:18:36.240]   Yeah.
[00:18:36.240 --> 00:18:37.920]   My, my experience has been the same.
[00:18:37.920 --> 00:18:41.920]   Like I've, my like achievements come through just like incredible teammates
[00:18:41.920 --> 00:18:46.520]   and unlike what I expected, I don't need to be at a level people are like very
[00:18:46.520 --> 00:18:50.480]   open about, you know, explaining their code as well, teaching concepts.
[00:18:50.480 --> 00:18:54.480]   Like I didn't know if I remember label encoding also at that time.
[00:18:54.480 --> 00:18:58.160]   And someone was like, just my teammate just sat me down and explained it very nicely.
[00:18:58.160 --> 00:18:58.520]   Yeah.
[00:18:58.520 --> 00:18:58.880]   Yeah.
[00:18:58.880 --> 00:18:59.080]   Yeah.
[00:18:59.080 --> 00:19:01.800]   It's a super valuable experience teaming up.
[00:19:01.800 --> 00:19:02.600]   Uh, yeah.
[00:19:02.600 --> 00:19:05.760]   And it's something I'm going to try to do more often going forward.
[00:19:05.760 --> 00:19:07.840]   Yeah.
[00:19:07.840 --> 00:19:11.440]   I look forward to more gold medals and then we can invite you for more talks.
[00:19:11.440 --> 00:19:13.960]   Absolutely.
[00:19:13.960 --> 00:19:19.960]   So how's, uh, and this will allow me to transition back to the competition.
[00:19:19.960 --> 00:19:23.280]   How do you usually approach, uh, competitions once you sign up?
[00:19:23.280 --> 00:19:29.840]   So I normally pick competitions wherever there's a sort of gap in my knowledge.
[00:19:29.840 --> 00:19:35.640]   So, uh, you know, if I want to learn about NLP, I'll join an L NLP competition.
[00:19:35.640 --> 00:19:40.520]   Um, so I try to pick competitions that have a gap in my knowledge.
[00:19:40.520 --> 00:19:43.840]   Um, and also quite, quite like the science-y based ones.
[00:19:43.840 --> 00:19:47.960]   So I really enjoyed the one with predicting molecular properties.
[00:19:47.960 --> 00:19:48.760]   That was pretty cool.
[00:19:48.760 --> 00:19:49.960]   So it's quite physics-y.
[00:19:49.960 --> 00:19:53.760]   Um, I'm really enjoying the gravitational waves one at the moment.
[00:19:53.760 --> 00:19:55.800]   So I like, I like those sorts of things.
[00:19:55.800 --> 00:20:01.520]   Um, and the other thing is whenever you're, if you're working on a problem
[00:20:01.520 --> 00:20:06.040]   and you, you always get a little bit of an edge if you research the problem.
[00:20:06.040 --> 00:20:09.520]   And I really like researching the science-y stuff because, you know, you
[00:20:09.520 --> 00:20:13.640]   learn a little bit about the world around you then, so that's kind of, kind of like,
[00:20:13.640 --> 00:20:17.600]   I always go for this more science-y based, um, competitions.
[00:20:18.480 --> 00:20:26.240]   Um, but yeah, but what's my sort of a general approach is, uh, yeah, have a look around.
[00:20:26.240 --> 00:20:32.680]   Um, I'd usually open up someone else's EDA notebook, cause I'm terrible at EDA.
[00:20:32.680 --> 00:20:34.600]   It takes me 20 minutes to make a plot.
[00:20:34.600 --> 00:20:38.000]   I take five hours.
[00:20:38.000 --> 00:20:43.360]   So you're like, I'm pretty sure I've Googled 10 times this week, how to make, how to
[00:20:43.360 --> 00:20:44.800]   change a figure size in my plot.
[00:20:44.800 --> 00:20:49.360]   I'm happy to know I'm not the only one.
[00:20:49.360 --> 00:20:55.080]   So I normally fire up someone else's notebook and if the data looks really
[00:20:55.080 --> 00:21:02.720]   interesting and the competition has like lots of scope for crazy ideas, um, then I'll jump in.
[00:21:02.720 --> 00:21:10.320]   Um, so then my normal approach is download the data and then I'll try to do my own
[00:21:10.320 --> 00:21:16.360]   data exploration, um, mostly not really getting into the nitty gritty of the data,
[00:21:16.360 --> 00:21:21.280]   but the first question I've tried to answer is how, how can I build a cross
[00:21:21.280 --> 00:21:24.240]   validation scheme that's robust for this dataset?
[00:21:24.240 --> 00:21:29.360]   So for example, I, you know, if it was for say medical data, you might want to
[00:21:29.360 --> 00:21:31.520]   group by patient ID and stuff like that.
[00:21:31.520 --> 00:21:37.080]   So I would try to really quickly explore the data, see how I can get a robust
[00:21:37.080 --> 00:21:41.160]   cross validation scheme, because if you don't have a good cross validation
[00:21:41.160 --> 00:21:45.440]   scheme, you're blind, um, ideally you don't want to be relying on a
[00:21:45.440 --> 00:21:46.880]   public leaderboard for anything.
[00:21:46.880 --> 00:21:51.160]   So I kind of treat the public leaderboard, if you're doing five fold cross
[00:21:51.160 --> 00:21:55.880]   validation, treat that as a six fold, not as your private leaderboard.
[00:21:55.880 --> 00:22:03.520]   Um, so the, I try to do a little bit of exploration, not, not too deep, try to
[00:22:03.520 --> 00:22:08.680]   finalize my validation scheme and then try to get a baseline submission on the
[00:22:08.680 --> 00:22:12.200]   leaderboard within 48 to 72 hours.
[00:22:12.200 --> 00:22:15.680]   The reason I tried to do it too quickly is because I might get distracted by
[00:22:15.680 --> 00:22:19.200]   something else and go somewhere, you know, when you, when you fire up a new dataset,
[00:22:19.200 --> 00:22:21.080]   you get excited and you want to do it quickly.
[00:22:21.080 --> 00:22:24.960]   If I don't do it in that timeframe, I'll probably lose interest and do something
[00:22:24.960 --> 00:22:25.240]   else.
[00:22:25.240 --> 00:22:31.680]   So I tried to get on the leaderboard, get a baseline model on, um, with, and I tried
[00:22:31.680 --> 00:22:35.400]   to do that without looking at any of the existing public notebooks.
[00:22:35.400 --> 00:22:42.240]   So, cause, um, yeah, it's that way you've done everything originally and you're not
[00:22:42.240 --> 00:22:44.240]   biased by any other people's thoughts.
[00:22:44.240 --> 00:22:47.320]   And even if it's, you know, right at the bottom of the leaderboard, you've already
[00:22:47.320 --> 00:22:50.400]   got your pipeline in shape and you can start tweaking ideas.
[00:22:50.400 --> 00:22:57.160]   And then normally once I've done that, I usually have, I have probably four rules
[00:22:57.160 --> 00:22:59.800]   I try to stick to when I'm doing a competition.
[00:22:59.800 --> 00:23:02.520]   The first three rules are pretty, pretty standard.
[00:23:02.520 --> 00:23:06.720]   It was the fourth one I probably picked up probably a year and a half ago.
[00:23:06.720 --> 00:23:09.520]   And that really helped me boost my leaderboard position.
[00:23:09.520 --> 00:23:16.200]   So the first one, um, and I've all, since I started competing in Kaggle, I've never
[00:23:16.200 --> 00:23:17.880]   forked a public notebook.
[00:23:17.880 --> 00:23:19.000]   Never.
[00:23:19.000 --> 00:23:23.520]   Um, so if I like a notebook, I'd always upvote it, always upvote the notebook.
[00:23:23.520 --> 00:23:24.440]   It helps the authors.
[00:23:25.400 --> 00:23:28.960]   And then I, if there's something I like in there, I'll always
[00:23:28.960 --> 00:23:30.600]   reimplement it from scratch.
[00:23:30.600 --> 00:23:35.920]   Um, and that way you really understand what's going on in the code.
[00:23:35.920 --> 00:23:38.000]   And if there's any bugs, you'll catch it.
[00:23:38.000 --> 00:23:42.720]   You know, um, Abhishek, um, his, his thing is if you didn't code, you didn't learn.
[00:23:42.720 --> 00:23:48.480]   Cause if you just hit the fork button, you, you don't know, you, you, you might
[00:23:48.480 --> 00:23:51.640]   scan through the code, but you may not, might not comprehend what it is.
[00:23:52.560 --> 00:23:56.680]   It's like this weird, uh, you nod to the cell and you press shift enter.
[00:23:56.680 --> 00:24:00.400]   But when you try to replicate, like, then you realize, ah, I'm not that good.
[00:24:00.400 --> 00:24:02.840]   I don't feel as smart as I was like doing earlier.
[00:24:02.840 --> 00:24:05.280]   At least for me, that's what that's like.
[00:24:05.280 --> 00:24:09.640]   Just cause just going through the motions and re implementing something from scratch
[00:24:09.640 --> 00:24:13.120]   helps really cement it, cement an idea and a concept in your head.
[00:24:13.120 --> 00:24:16.960]   And the other thing with public notebooks is that people publish
[00:24:16.960 --> 00:24:18.760]   notebooks for different reasons.
[00:24:18.880 --> 00:24:22.640]   So, um, last week I was trying to learn something new in, in the
[00:24:22.640 --> 00:24:24.880]   gravitational waves, um, competition.
[00:24:24.880 --> 00:24:29.400]   So I published a notebook on wavelet transforms just because I wanted to
[00:24:29.400 --> 00:24:32.680]   better understand it and I wanted to get feedback from other people.
[00:24:32.680 --> 00:24:36.760]   Um, just because I published it, I did, I wasn't saying you need, you need
[00:24:36.760 --> 00:24:38.320]   to use this on your pipeline.
[00:24:38.320 --> 00:24:41.520]   It was mostly to get feedback and probably definitely bugs in there.
[00:24:41.520 --> 00:24:45.320]   It's probably definitely better ways of implementing that thing.
[00:24:45.560 --> 00:24:49.920]   But, um, you know, so you just, just be aware that people publish
[00:24:49.920 --> 00:24:51.160]   notebooks for different reasons.
[00:24:51.160 --> 00:24:54.080]   A lot of the time is because they want to learn as well.
[00:24:54.080 --> 00:24:54.840]   Yeah.
[00:24:54.840 --> 00:24:58.720]   Um, so that's, that's when I started Kaggle, that's the number one rule I
[00:24:58.720 --> 00:25:02.280]   tried to stick to, which I kind of, kind of do.
[00:25:02.280 --> 00:25:07.480]   Um, the other thing was never to use ideas from the discussion
[00:25:07.480 --> 00:25:08.640]   boards without testing them.
[00:25:08.640 --> 00:25:13.240]   Because, um, there's a lot of really good ideas on the discussion boards.
[00:25:13.320 --> 00:25:16.600]   Um, and you know, if someone says, Hey, we, I did this and I boosted my
[00:25:16.600 --> 00:25:22.600]   leaderboard position by this, um, They, they might be right and they might be wrong.
[00:25:22.600 --> 00:25:26.600]   You know, um, the, the Kaggle boards have really, it's probably the
[00:25:26.600 --> 00:25:28.160]   friendliest place on the internet.
[00:25:28.160 --> 00:25:28.960]   I love it.
[00:25:28.960 --> 00:25:30.320]   It's a really good community.
[00:25:30.320 --> 00:25:35.280]   And there's, I don't think people share malicious knowledge on that intentionally,
[00:25:35.280 --> 00:25:40.680]   but if there's an idea shared on there and someone gets a leaderboard boost, it might
[00:25:40.680 --> 00:25:44.080]   not translate in a leaderboard boost for you, and there's a whole number of reasons
[00:25:44.080 --> 00:25:48.080]   for that, maybe they, they using a different validation scheme, maybe they
[00:25:48.080 --> 00:25:52.160]   only tested it on a single fold, or maybe they have a leakage in the cross validation.
[00:25:52.160 --> 00:25:57.960]   So it's always don't, don't take an idea from the discussion board and
[00:25:57.960 --> 00:26:00.600]   then just go with it blindly, always test it.
[00:26:00.600 --> 00:26:09.400]   Um, and then the third thing I tried to stick to when I, um, in, when I'm doing
[00:26:09.400 --> 00:26:13.760]   my sort of experiments is it sounds really simple, um, and it's really hard
[00:26:13.760 --> 00:26:17.400]   for me because I'm a very impatient person, but just, just to change one thing
[00:26:17.400 --> 00:26:25.920]   at a time, um, so, you know, in science, in science, if you're trying to figure
[00:26:25.920 --> 00:26:31.000]   out if changing something actually made a difference, you can only tell, tell, if
[00:26:31.000 --> 00:26:32.360]   you only change one thing at a time.
[00:26:32.360 --> 00:26:36.520]   And one thing you see quite commonly on, on the discussion boards is, uh, you
[00:26:36.520 --> 00:26:42.080]   know, let's say you're in an image competition and someone says, Oh, I got
[00:26:42.080 --> 00:26:47.560]   this much of a leaderboard boost by going from efficient net B0 to B7, but
[00:26:47.560 --> 00:26:52.200]   obviously you're going to have to reduce your batch size to fit a B7 model on your
[00:26:52.200 --> 00:26:52.720]   GPU.
[00:26:52.720 --> 00:26:54.120]   So you've changed two things there.
[00:26:54.120 --> 00:26:56.960]   So how do you know if your batch size has made the difference or
[00:26:56.960 --> 00:26:58.400]   the model has made a difference?
[00:26:58.400 --> 00:27:02.400]   So you just got to be very careful around changing one thing at a time.
[00:27:02.400 --> 00:27:05.800]   It's very difficult, especially with deep learning, because things take a long
[00:27:05.800 --> 00:27:06.720]   time to do.
[00:27:06.720 --> 00:27:10.360]   And if you're impatient like me, it's very hard to, Oh, maybe I'll change
[00:27:10.360 --> 00:27:11.560]   this and this at the same time.
[00:27:11.560 --> 00:27:16.120]   Um, it's just going to be patient and change one thing at a time.
[00:27:16.120 --> 00:27:21.840]   And then the last thing that I tried to, I've really tried to focus on in the
[00:27:21.840 --> 00:27:24.800]   last few years, and I think it's, I think it's been working okay for me.
[00:27:24.800 --> 00:27:32.600]   And I think maybe, um, CPMC said it, um, one day, but he said that the best
[00:27:32.600 --> 00:27:37.440]   Kagglers, um, don't know, always have the best idea that people at the top of
[00:27:37.440 --> 00:27:38.960]   leaderboard don't always have the best ideas.
[00:27:38.960 --> 00:27:43.000]   They just have the ability to test them most amount of ideas in the shortest
[00:27:43.000 --> 00:27:43.520]   amount of time.
[00:27:43.520 --> 00:27:46.280]   And I really tried to focus on that.
[00:27:46.280 --> 00:27:52.400]   So, um, you know, if you read a lot of the previous solutions, unless there's a
[00:27:52.400 --> 00:27:56.360]   problem with the data set, like a leakage, or there's some sort of difference
[00:27:56.360 --> 00:28:01.280]   between the train train and, and the test they've set a lot of the top solutions.
[00:28:01.320 --> 00:28:04.440]   Aren't there's not one magic, right?
[00:28:04.440 --> 00:28:07.680]   There's not one thing that makes it a top solution.
[00:28:07.680 --> 00:28:10.640]   I mean, unless there's a leak and those are common anymore.
[00:28:10.640 --> 00:28:12.000]   Yeah.
[00:28:12.000 --> 00:28:18.400]   So I would say 90, 90% of the competitions, the best solutions are just a really good
[00:28:18.400 --> 00:28:20.640]   culmination of lots of small ideas, right.
[00:28:20.640 --> 00:28:22.040]   That work well together.
[00:28:22.040 --> 00:28:28.640]   So if you're able to test lots of ideas quickly, that's going to really help you.
[00:28:29.160 --> 00:28:35.560]   So one thing I've tried to do is just, um, work with a small model.
[00:28:35.560 --> 00:28:39.120]   You know, if you have a really big data set, maybe work with half of the data set
[00:28:39.120 --> 00:28:41.440]   or 10% of the data set.
[00:28:41.440 --> 00:28:45.600]   And, um, you know, if you see in the discussion board, someone's got a great
[00:28:45.600 --> 00:28:50.640]   score with an efficient net B7, stay with B0 for as long as possible.
[00:28:50.640 --> 00:28:51.000]   Right.
[00:28:51.000 --> 00:28:57.000]   And it allows you to get your idea iteration down rather than days, get
[00:28:57.000 --> 00:28:58.560]   it down to hours and minutes.
[00:28:58.960 --> 00:29:03.760]   And you can just test so many more ideas and it, that kind of philosophy
[00:29:03.760 --> 00:29:06.000]   works well in real life, right?
[00:29:06.000 --> 00:29:10.160]   You know, like a formula one team, they, they don't build full scale
[00:29:10.160 --> 00:29:11.840]   cars to put in a wind tunnel.
[00:29:11.840 --> 00:29:16.400]   They, they, they have model cars or, um, there's a guy called Gordon Murray.
[00:29:16.400 --> 00:29:20.440]   He used to build formula one cars and he's building a new car called the T50.
[00:29:20.440 --> 00:29:22.920]   And it's got this brand new V12 engine in it.
[00:29:22.920 --> 00:29:25.640]   He went to the company called Cosworth to build it.
[00:29:26.360 --> 00:29:29.600]   And they, they didn't bother developing a V12 engine because that would be
[00:29:29.600 --> 00:29:31.440]   too big and expensive to make every time.
[00:29:31.440 --> 00:29:35.440]   So they built a three cylinder engine, did all the development on that, and
[00:29:35.440 --> 00:29:38.560]   then scaled it up because it's one quarter of the size and one quarter of the cost.
[00:29:38.560 --> 00:29:42.960]   So I tried to approach my experimentation that way.
[00:29:42.960 --> 00:29:45.760]   So stay small for as long as possible.
[00:29:45.760 --> 00:29:49.600]   Try to iterate your ideas in minutes or hours rather than days.
[00:29:49.600 --> 00:29:52.560]   And then once you get towards the end of the competition, then
[00:29:52.560 --> 00:29:54.400]   train, start training your bigger models.
[00:29:54.480 --> 00:29:57.080]   And it seems to be working all right for me.
[00:29:57.080 --> 00:29:59.880]   So far, that's, that's probably when I started to see the
[00:29:59.880 --> 00:30:01.760]   biggest change in my performance.
[00:30:01.760 --> 00:30:06.280]   I want to highlight like how incredibly difficult that is because like I've
[00:30:06.280 --> 00:30:09.760]   recently started competing again and I found my old posts, which was from my
[00:30:09.760 --> 00:30:14.200]   first competition and that's how I recall this, but like my intuition at that time.
[00:30:14.200 --> 00:30:17.680]   And I'm sure this might be for a few people is to just like throw the biggest
[00:30:17.680 --> 00:30:22.040]   model, because if I throw a ResNet at an image classification problem, it should
[00:30:22.040 --> 00:30:25.440]   work, right? Maybe not ResNet, maybe a FishNet B5, like it should work.
[00:30:25.440 --> 00:30:26.600]   That's the best model.
[00:30:26.600 --> 00:30:28.560]   Like I should just throw the complete data.
[00:30:28.560 --> 00:30:29.320]   No, no, no, no.
[00:30:29.320 --> 00:30:31.000]   You'll, you'll find out the hard way.
[00:30:31.000 --> 00:30:32.480]   Three days later.
[00:30:32.480 --> 00:30:33.560]   Exactly.
[00:30:33.560 --> 00:30:34.000]   Exactly.
[00:30:34.000 --> 00:30:36.960]   And it's always tempting because you always see that one guy in the discussion
[00:30:36.960 --> 00:30:41.520]   board saying, I've got this score with a B7 or whatever, and you're like, Oh, and
[00:30:41.520 --> 00:30:43.440]   it feels good to get up to the leaderboard, right?
[00:30:43.440 --> 00:30:48.240]   If it feels really good when you train a big model and you get up, but it feels
[00:30:48.240 --> 00:30:52.880]   even better if you can match that score with a tiny model, with a bunch of really
[00:30:52.880 --> 00:30:53.600]   good ideas.
[00:30:53.600 --> 00:30:56.480]   And then when you train the big model, it's even better.
[00:30:56.480 --> 00:30:59.000]   Um, and it's also good.
[00:30:59.000 --> 00:31:03.120]   Usually in the competition, there's a thread that says best single model.
[00:31:03.120 --> 00:31:07.200]   And if you can put a post up there saying, Oh, I've got this score with a
[00:31:07.200 --> 00:31:09.760]   ResNet 34 or something like that.
[00:31:09.760 --> 00:31:13.960]   It really helps if you want to team up and stuff like that, because, um, people
[00:31:13.960 --> 00:31:15.400]   see that and it's good.
[00:31:15.400 --> 00:31:18.560]   And it's just, it's just easier to iterate ideas.
[00:31:18.560 --> 00:31:20.160]   You can test more stuff quicker.
[00:31:20.160 --> 00:31:25.280]   Uh, and yeah, the, my mantra is test, test most of my ideas in the shortest
[00:31:25.280 --> 00:31:25.920]   amount of time.
[00:31:25.920 --> 00:31:30.640]   Um, and that I tried to write my code to help that.
[00:31:30.640 --> 00:31:35.560]   So I can just line up experiments one after another and do it quickly.
[00:31:35.560 --> 00:31:37.080]   So we can get into that later on.
[00:31:37.080 --> 00:31:37.320]   Yeah.
[00:31:37.320 --> 00:31:38.280]   Yeah.
[00:31:38.280 --> 00:31:40.920]   We'll definitely be diving into, into the code base.
[00:31:40.920 --> 00:31:45.440]   And you must agree to do walkthrough for us so that like no one is upset with that.
[00:31:45.440 --> 00:31:50.200]   But, uh, I'll, I'll, what I'll do is I'll start by describing the problem and
[00:31:50.200 --> 00:31:52.520]   Anjum, you can help me out if I miss out on any points.
[00:31:52.520 --> 00:31:57.720]   So what I like to do is I usually just, uh, head over to the page, try to read
[00:31:57.720 --> 00:32:01.840]   around a bit, try to understand sometimes like my biggest mistake has not, has been
[00:32:01.840 --> 00:32:03.400]   like not to read this thoroughly.
[00:32:03.400 --> 00:32:07.560]   And then 15 days into the competition, I'd come back and realize I should
[00:32:07.560 --> 00:32:08.680]   have just read this.
[00:32:08.680 --> 00:32:09.640]   Yeah.
[00:32:09.640 --> 00:32:13.800]   There's obviously quite a lot of little subtle things in here that you can just,
[00:32:13.800 --> 00:32:15.120]   if you scan through, you might miss.
[00:32:15.120 --> 00:32:16.640]   And then you read it again.
[00:32:16.640 --> 00:32:19.480]   It's like, oh, that's why the data looks like this or something like that.
[00:32:19.480 --> 00:32:20.760]   Yeah.
[00:32:20.760 --> 00:32:24.880]   So I've already read through this, but the problem statement here is, uh, usually
[00:32:24.880 --> 00:32:29.040]   people use, uh, tools like, I don't know how to pronounce this, so I won't.
[00:32:29.040 --> 00:32:31.880]   And, uh, I haven't studied literature in a long time.
[00:32:31.880 --> 00:32:36.240]   So I had to look up what syllables are, but, uh, for different grades, they have
[00:32:36.240 --> 00:32:42.640]   this score of understanding how, uh, difficult or easy a passages and the
[00:32:42.640 --> 00:32:44.360]   argument by the organizers.
[00:32:44.360 --> 00:32:51.200]   So organizers here is common lit, which is an organization that
[00:32:51.200 --> 00:32:53.720]   makes, uh, students better readers.
[00:32:53.720 --> 00:32:57.280]   So they're trying to help them, uh, through this competition and other
[00:32:57.280 --> 00:33:02.800]   softwares, so their argument is this isn't the best way and they want, uh,
[00:33:02.800 --> 00:33:06.280]   they wanted the competitors to use, uh, machine learning to help them.
[00:33:06.280 --> 00:33:11.720]   After this, I usually hover over to evaluation because sometimes the
[00:33:11.720 --> 00:33:14.560]   metrics are weird and then I have to like find a kernel where it's
[00:33:14.560 --> 00:33:16.440]   implemented so that I can copy it.
[00:33:16.440 --> 00:33:21.760]   But luckily here was the RMSE, uh, which is pretty straightforward.
[00:33:21.760 --> 00:33:27.440]   The reason it's RMSE is, uh, for this problem, uh, remember, I just said about
[00:33:27.440 --> 00:33:32.880]   this code, uh, you need to target the targets are these scores and you
[00:33:32.880 --> 00:33:35.000]   need to predict this number.
[00:33:35.000 --> 00:33:37.000]   So that's why it's a regression problem.
[00:33:37.000 --> 00:33:40.160]   And for regression problems, you have root mean squared error.
[00:33:40.160 --> 00:33:43.520]   I'm not going to describe that, but please feel free to ask if you're not clear on that.
[00:33:43.520 --> 00:33:49.080]   Uh, and this is quite unusual for an NLP problem, because normally with NLP, you
[00:33:49.080 --> 00:33:53.560]   might be classifying something or trying to extract a sequence in this case.
[00:33:53.560 --> 00:33:58.760]   It's a regression problem, which is I've not seen an NLP before, but, um, it's quite
[00:33:58.760 --> 00:33:59.240]   unusual.
[00:33:59.240 --> 00:34:06.440]   So yeah, it, it really, and the other thing with RMSE, you can, if for a loss
[00:34:06.440 --> 00:34:09.120]   function, you can directly optimize that as well.
[00:34:09.120 --> 00:34:12.160]   So this is in a lot of competitions.
[00:34:12.160 --> 00:34:17.400]   If the metric, the metric is not directly optimized, like AUC or something like
[00:34:17.400 --> 00:34:19.920]   that, you have to use a proxy loss function.
[00:34:20.560 --> 00:34:24.400]   But for RMSE, you can directly optimize that, which is just handy.
[00:34:24.400 --> 00:34:25.720]   It makes things easier for us.
[00:34:25.720 --> 00:34:31.240]   Well, AUC, I remember you can just like make 0.5 predictions and then you get a nice
[00:34:31.240 --> 00:34:34.320]   score and you get deceived as if you're doing a nice prediction, but you really
[00:34:34.320 --> 00:34:34.560]   know it.
[00:34:34.560 --> 00:34:38.040]   So you have to come up with tricks there.
[00:34:38.040 --> 00:34:43.880]   Um, but yeah, like I was looking at the data and it's this incredibly small data
[00:34:43.880 --> 00:34:46.480]   set of, I think about 2,500 rows.
[00:34:46.600 --> 00:34:49.680]   Uh, switch over to column.
[00:34:49.680 --> 00:34:55.440]   So the columns here, uh, and I've already gone ahead and written about these, please
[00:34:55.440 --> 00:35:00.760]   excuse this writing, but these are the columns you have the ID, which is the ID of
[00:35:00.760 --> 00:35:01.280]   the text.
[00:35:01.280 --> 00:35:03.200]   The URL is there for source.
[00:35:03.200 --> 00:35:08.440]   Uh, I'm assuming people might have crawled websites, uh, Kagglers do like fancy stuff.
[00:35:08.440 --> 00:35:10.120]   So maybe that's why they left in there.
[00:35:10.880 --> 00:35:19.240]   Licenses talk about, uh, for example, is it by CC, CC by four, which means that can
[00:35:19.240 --> 00:35:23.760]   you just use this and like build upon it as long as you give credit licenses are like
[00:35:23.760 --> 00:35:26.280]   quite a nitty gritty.
[00:35:26.280 --> 00:35:29.920]   I don't want to get in them, but like these usually give you the freedom to work with
[00:35:29.920 --> 00:35:30.280]   data.
[00:35:30.280 --> 00:35:35.360]   Then there's the excerpt, which is the main NLP need.
[00:35:35.360 --> 00:35:39.320]   Uh, and there's, there's the like words of the paragraphs that you need to
[00:35:39.320 --> 00:35:42.560]   understand your model needs to understand and predict the target.
[00:35:42.560 --> 00:35:44.720]   And then there are the errors.
[00:35:44.720 --> 00:35:49.200]   Now these, again, the one thing I learned after a while was there were different
[00:35:49.200 --> 00:35:51.520]   reviewers which had given these errors.
[00:35:51.520 --> 00:35:55.480]   So that's why like there's a spread of errors out in here.
[00:35:55.480 --> 00:35:59.000]   Awesome.
[00:35:59.000 --> 00:35:59.800]   Did I miss anything?
[00:35:59.800 --> 00:36:01.520]   No, no, that's it.
[00:36:01.520 --> 00:36:01.880]   Yeah.
[00:36:01.880 --> 00:36:03.360]   Yeah.
[00:36:03.360 --> 00:36:08.640]   So I guess if you expand this, you can kind of see the distribution of the target
[00:36:08.640 --> 00:36:09.040]   variable.
[00:36:09.040 --> 00:36:09.520]   There you go.
[00:36:09.520 --> 00:36:14.880]   So it looks like it's almost like a Gaussian distribution, but not quite.
[00:36:14.880 --> 00:36:19.800]   And you get, you get scores from say minus three to plus two.
[00:36:19.800 --> 00:36:24.000]   Um, and relating to how difficult the passage is to reading.
[00:36:24.000 --> 00:36:30.920]   And I think, um, yeah, so the scores were, I think they were derived to using
[00:36:30.920 --> 00:36:32.840]   something called a Bradley Terry model.
[00:36:33.840 --> 00:36:40.480]   And, um, there were, I think they asked lots of teachers to, um, with pairs of
[00:36:40.480 --> 00:36:43.880]   combinations of these experts and try to ask them which one is harder.
[00:36:43.880 --> 00:36:50.360]   And then they used, um, those sort of put all of those ratings in a matrix.
[00:36:50.360 --> 00:36:53.280]   And then there's a, there's some sort of model that converts those
[00:36:53.280 --> 00:36:55.120]   ratios to a score like this.
[00:36:55.120 --> 00:37:01.120]   And because not all of the teachers saw all of the experts, they were able to
[00:37:01.120 --> 00:37:03.480]   calculate a standard error on that prediction.
[00:37:03.960 --> 00:37:05.600]   Which I thought was an interesting feature.
[00:37:05.600 --> 00:37:08.680]   I, and what is one of the reasons I jumped into the competition, because I
[00:37:08.680 --> 00:37:13.560]   work with uncertain data all the time with work and how you treat errors is
[00:37:13.560 --> 00:37:18.720]   that to be quite careful, but in the end, the standard error was, didn't come in
[00:37:18.720 --> 00:37:22.280]   that useful, but we can talk about that later, but it was an interesting feature,
[00:37:22.280 --> 00:37:24.280]   interesting thing to add to the dataset.
[00:37:24.280 --> 00:37:25.320]   Yeah.
[00:37:25.320 --> 00:37:26.760]   Sounds interesting for sure.
[00:37:26.760 --> 00:37:29.520]   And yeah, it does look like a Gaussian distribution.
[00:37:29.520 --> 00:37:31.360]   Uh, again, we'll not go into that depth.
[00:37:31.360 --> 00:37:33.000]   If you're not clear why those are important.
[00:37:33.000 --> 00:37:33.680]   Feel free to ask.
[00:37:33.680 --> 00:37:37.000]   We'd be happy to go into that, but this is a slightly advanced discussion.
[00:37:37.000 --> 00:37:38.600]   So I'm just skipping over those details.
[00:37:38.600 --> 00:37:41.480]   Please don't feel like you're not welcome to ask questions as a reminder.
[00:37:41.480 --> 00:37:46.800]   The thing I like to do, uh, here afterwards is I just usually hop over to
[00:37:46.800 --> 00:37:53.200]   code, I sort by best score, some hyper parameter tuning and submit, but that's
[00:37:53.200 --> 00:37:56.280]   not, that's not why I wanted to point these out.
[00:37:56.280 --> 00:38:01.440]   We have a few Kaggle grandmaster who's also the, who are also our ambassadors
[00:38:01.440 --> 00:38:04.120]   who've created these nice dashboards for us.
[00:38:04.120 --> 00:38:07.600]   So I'm just going to go through those to again, highlight a few details
[00:38:07.600 --> 00:38:10.840]   about the data since they've done the ED and I didn't want to butcher their work.
[00:38:10.840 --> 00:38:14.520]   So this is by Andrada Altiano.
[00:38:14.520 --> 00:38:17.080]   Uh, yeah, I love Andrada's notebooks.
[00:38:17.080 --> 00:38:18.080]   They're always so good.
[00:38:18.080 --> 00:38:23.160]   Her notebooks are just like this incredible artist, the ones where you get to like,
[00:38:23.160 --> 00:38:25.560]   it's just a different definition.
[00:38:25.560 --> 00:38:28.360]   Yeah, exactly.
[00:38:30.000 --> 00:38:31.600]   I think this is more of a baseline.
[00:38:31.600 --> 00:38:33.800]   So I, I'm, I'm confusing myself.
[00:38:33.800 --> 00:38:36.120]   I wanted to show Ruchi's dashboard.
[00:38:36.120 --> 00:38:42.200]   Ruchi is also, she's a 2x Kaggle grandmaster and she did this ED inside of
[00:38:42.200 --> 00:38:45.080]   her kernel, which is, you can find this up.
[00:38:45.080 --> 00:38:50.160]   Uh, I request Andrea to share this as well, but the nice thing we can do is
[00:38:50.160 --> 00:38:53.600]   since she's created a dashboard, I don't have to go through a notebook.
[00:38:53.600 --> 00:38:56.760]   I just get all of the details right here and we can glance over those.
[00:38:56.760 --> 00:38:58.360]   Yeah, this is really cool.
[00:38:58.360 --> 00:39:02.920]   So I, I, I didn't know that you could do this with weights and biases, because
[00:39:02.920 --> 00:39:07.640]   this is kind of like a Power BI or Tableau sort of dashboard view.
[00:39:07.640 --> 00:39:10.880]   I've only used it for sort of model logging, but this is super cool.
[00:39:10.880 --> 00:39:15.200]   Like this is when I saw Ruchi's notebook, it was like, wow, I
[00:39:15.200 --> 00:39:16.240]   didn't know you could do this.
[00:39:16.240 --> 00:39:21.240]   Honestly, I learned after joining as well, but don't take my colleagues.
[00:39:21.240 --> 00:39:26.400]   So let's, let's go through these quickly.
[00:39:26.440 --> 00:39:29.840]   Uh, would you want to like walk us through these since you're familiar with the data?
[00:39:29.840 --> 00:39:31.480]   Yeah.
[00:39:31.480 --> 00:39:37.480]   So, um, there's, yeah, Ruchi's done a great, great work of like exploring it,
[00:39:37.480 --> 00:39:43.360]   exploring the data, trying to see how, um, you know, there's like the average
[00:39:43.360 --> 00:39:45.760]   word length distribution and things like that.
[00:39:45.760 --> 00:39:52.800]   These are pretty cool, cool summations of, um, um, how, what the data set looks like.
[00:39:52.960 --> 00:39:55.840]   So one of the more interesting ones I saw, if you scroll down
[00:39:55.840 --> 00:39:57.640]   to the Flesch reading ease one.
[00:39:57.640 --> 00:40:01.680]   So yeah, there, there, there was quite a bit of scroll up a little bit.
[00:40:01.680 --> 00:40:02.560]   Sorry.
[00:40:02.560 --> 00:40:03.200]   Yeah.
[00:40:03.200 --> 00:40:04.160]   Is that gray chart?
[00:40:04.160 --> 00:40:06.040]   I'm using an external mouse for that.
[00:40:06.040 --> 00:40:06.640]   Oh, okay.
[00:40:06.640 --> 00:40:13.720]   Um, yeah, so there was a, there was a lot of, um, she, she related the Flesch
[00:40:13.720 --> 00:40:17.680]   reading ease, which is, she, I think she calculated using a package called
[00:40:17.680 --> 00:40:19.920]   text stat, which I used in the end as well.
[00:40:20.800 --> 00:40:26.280]   And it is also able to come up with a distribution, which kind of
[00:40:26.280 --> 00:40:28.080]   looks like the target distribution.
[00:40:28.080 --> 00:40:30.840]   And she did a really cool correlation matrix.
[00:40:30.840 --> 00:40:37.440]   I think it was back in the original notebook of how different, um, ways of
[00:40:37.440 --> 00:40:41.840]   calculating readability score using the more classical techniques correlates
[00:40:41.840 --> 00:40:44.760]   with the, um, target variable.
[00:40:44.760 --> 00:40:46.960]   So if you go back to a notebook, I think.
[00:40:50.120 --> 00:40:57.440]   I'm checking, uh, like zoom comes in the way right at the top of browser windows.
[00:40:57.440 --> 00:40:58.960]   I'm just trying to fight that.
[00:40:58.960 --> 00:41:00.600]   Let me move back to the side.
[00:41:00.600 --> 00:41:02.000]   Now we could.
[00:41:02.000 --> 00:41:04.640]   Yeah.
[00:41:04.640 --> 00:41:07.000]   So this was really cool.
[00:41:07.000 --> 00:41:12.360]   If you look up reading, I'm just trying to find the words.
[00:41:12.360 --> 00:41:15.400]   Yeah.
[00:41:15.400 --> 00:41:18.680]   If you click on the right, it says readability tests, the test tube.
[00:41:18.680 --> 00:41:19.200]   Yeah.
[00:41:19.400 --> 00:41:19.720]   Gotcha.
[00:41:19.720 --> 00:41:24.440]   I'll just have to scroll.
[00:41:24.440 --> 00:41:26.000]   Sometimes.
[00:41:26.000 --> 00:41:32.440]   They created, like, uh, if you remember on zoom, it was like a little
[00:41:32.440 --> 00:41:34.120]   worse, a few years ago now.
[00:41:34.120 --> 00:41:34.480]   Yeah.
[00:41:34.480 --> 00:41:34.680]   Yeah.
[00:41:34.680 --> 00:41:36.200]   It is so much better now.
[00:41:36.200 --> 00:41:36.560]   Yeah.
[00:41:36.560 --> 00:41:38.960]   Yeah.
[00:41:38.960 --> 00:41:39.640]   Look at these charts.
[00:41:39.640 --> 00:41:40.320]   They're amazing.
[00:41:40.320 --> 00:41:41.360]   Yeah.
[00:41:41.360 --> 00:41:43.280]   Take me days to make this.
[00:41:46.200 --> 00:41:49.280]   So there's different text properties and things like that.
[00:41:49.280 --> 00:41:51.320]   Uh, we keep going down.
[00:41:51.320 --> 00:41:55.800]   Keep going.
[00:41:55.800 --> 00:41:56.240]   Keep going.
[00:41:56.240 --> 00:41:59.240]   This is the one, this is this one.
[00:41:59.240 --> 00:42:03.280]   This was a really interesting chart for me when I, when I found reaches notebook.
[00:42:03.280 --> 00:42:09.160]   So the bottom bottom line is the target variable and how it correlates with
[00:42:09.160 --> 00:42:13.960]   different sort of classical, um, reading readability tests.
[00:42:13.960 --> 00:42:17.000]   So there's the flesh readability, the one we were talking about before
[00:42:17.000 --> 00:42:21.080]   as a correlation of 0.52, which is actually not bad, right?
[00:42:21.080 --> 00:42:26.560]   So if you were planning to add additional features to your models,
[00:42:26.560 --> 00:42:28.000]   that could be a way to go.
[00:42:28.000 --> 00:42:28.800]   And I did try that.
[00:42:28.800 --> 00:42:33.360]   Um, and yeah, there's all these other, um, readability features.
[00:42:33.360 --> 00:42:37.240]   They might be negatively correlated, but yeah, really interesting stuff.
[00:42:37.240 --> 00:42:40.920]   And I think the other thing I saw, which is interesting, if you scroll, scroll
[00:42:40.920 --> 00:42:45.320]   up to the, I think it's one of the second plots, so right at the top,
[00:42:45.320 --> 00:42:46.760]   sorry, I'm making you jump around.
[00:42:46.760 --> 00:42:53.680]   The thing about zoom is like, there's a lag between the scrolling.
[00:42:53.680 --> 00:42:57.680]   So maybe the right one and then, uh, yeah, I'm past it in a second.
[00:42:57.680 --> 00:42:59.080]   Is this the one missing?
[00:42:59.080 --> 00:43:02.560]   Well, no, the next one, I think maybe, or the one with the
[00:43:02.560 --> 00:43:04.480]   standard error with the hexagons.
[00:43:04.480 --> 00:43:07.600]   I think it's there in the dashboard as well.
[00:43:07.600 --> 00:43:08.420]   Yeah.
[00:43:10.600 --> 00:43:11.100]   This one?
[00:43:11.100 --> 00:43:12.320]   No, no, no.
[00:43:12.320 --> 00:43:17.800]   Well, it's kind of related to that, but it was a scatter plot, um, in, in the
[00:43:17.800 --> 00:43:22.200]   notebook with, um, the target variable versus the standard, this one.
[00:43:22.200 --> 00:43:22.700]   Yeah.
[00:43:22.700 --> 00:43:23.960]   This one's really cool.
[00:43:23.960 --> 00:43:28.200]   So we've got the target variable going along the bottom and then
[00:43:28.200 --> 00:43:29.840]   standard error along the top.
[00:43:29.840 --> 00:43:37.040]   So normally it, um, so what this chart is showing, there's a relationship
[00:43:37.040 --> 00:43:38.840]   between the target variable and the error.
[00:43:39.600 --> 00:43:43.400]   Uh, so it's statisticians invented a really hard word to say for
[00:43:43.400 --> 00:43:45.480]   that is called heteroscedasticity.
[00:43:45.480 --> 00:43:46.760]   I don't know if I said that right.
[00:43:46.760 --> 00:43:51.840]   So it's when the error of something is related to its value.
[00:43:51.840 --> 00:43:56.280]   Um, so normally when you're doing say a linear regression, you assume that
[00:43:56.280 --> 00:43:58.000]   the error is a constant all along.
[00:43:58.000 --> 00:44:02.840]   But I, I spent a long time thinking about this, about this dataset.
[00:44:02.840 --> 00:44:07.880]   And I thought, well, actually the really hard to read samples and the really
[00:44:07.880 --> 00:44:11.960]   easy to read samples, there's a fewer number of samples for that.
[00:44:11.960 --> 00:44:16.600]   So if they calculating a standard error on that, the error will be wider just
[00:44:16.600 --> 00:44:20.600]   because they've got fewer, really hard to read or really easy to read samples.
[00:44:20.600 --> 00:44:23.880]   So that's why the standard error will be larger.
[00:44:23.880 --> 00:44:30.400]   So I think that this sort of this smiley face trend is actually related to the
[00:44:30.400 --> 00:44:33.480]   number of samples you have at the extreme ends of the target rather than
[00:44:33.480 --> 00:44:39.400]   an actual feature of the data, which is why I think in the end, the standard
[00:44:39.400 --> 00:44:43.840]   error wasn't too important for this competition, but it's an interesting
[00:44:43.840 --> 00:44:46.560]   thing to see when you're exploring the data.
[00:44:46.560 --> 00:44:51.160]   Yeah, I guess you could only understand that through this plot.
[00:44:51.160 --> 00:44:52.760]   Otherwise it wouldn't have been that clear.
[00:44:52.760 --> 00:44:59.320]   Um, it's quickly grants with the common word.
[00:44:59.320 --> 00:45:04.360]   So she's also plotted, uh, with bits and biases, you can just log the graphs and
[00:45:04.360 --> 00:45:08.680]   luckily then I don't have to scroll and I can just look at these plots right here.
[00:45:08.680 --> 00:45:12.760]   But it's also interesting to see that apparently, uh, school kids are being
[00:45:12.760 --> 00:45:18.960]   told a lot about carbon dioxide, probably for good measure and, uh, different words.
[00:45:18.960 --> 00:45:22.560]   I don't think these like mean a lot, probably these appear a lot in the search.
[00:45:22.560 --> 00:45:23.520]   Yeah.
[00:45:23.520 --> 00:45:25.720]   I had a quick scan through.
[00:45:25.800 --> 00:45:31.760]   Um, so there's that licenses column and a lot of the articles were from Wikipedia.
[00:45:31.760 --> 00:45:38.320]   And things, um, was it the licenses or the other, the source column or something?
[00:45:38.320 --> 00:45:43.240]   Um, and yeah, there's a lot of articles from Wikipedia and more science-y based
[00:45:43.240 --> 00:45:47.040]   stuff, so things like voltage or something like that.
[00:45:47.040 --> 00:45:50.640]   So it might be quite a lot of science-y based words in here.
[00:45:50.640 --> 00:45:52.560]   So I thought that was interesting as well.
[00:45:53.240 --> 00:45:57.280]   Yeah, I don't know why they would make high schoolers read Wikipedia pages.
[00:45:57.280 --> 00:46:00.680]   I saw a lot of Wikipedia links there as well.
[00:46:00.680 --> 00:46:01.400]   Yeah.
[00:46:01.400 --> 00:46:04.400]   But yeah, this is really cool.
[00:46:04.400 --> 00:46:07.720]   This is really, really interesting use for weights and biases.
[00:46:07.720 --> 00:46:13.440]   Um, so yeah, we did quite a lot of, um, more classical NMP stuff.
[00:46:13.440 --> 00:46:16.200]   So looking at unigrams and bigrams and stuff like that.
[00:46:16.200 --> 00:46:19.640]   And this, this is really cool approach.
[00:46:19.640 --> 00:46:23.080]   So if you, when you make your first baseline model, it's always good to
[00:46:23.080 --> 00:46:29.560]   take the simplest approach, you know, if, uh, if you start with a new data set and
[00:46:29.560 --> 00:46:32.800]   then you're doing a regression task, start with the linear regression rather than
[00:46:32.800 --> 00:46:34.760]   going straight into a deep neural network.
[00:46:34.760 --> 00:46:35.360]   Yeah.
[00:46:35.360 --> 00:46:37.080]   I was going to say GPT-203.
[00:46:37.080 --> 00:46:37.640]   Yeah.
[00:46:37.640 --> 00:46:45.760]   So doing this sort of, um, classical sort of NLP techniques, um, it's really cool to see.
[00:46:47.320 --> 00:46:47.440]   Yeah.
[00:46:47.440 --> 00:46:51.480]   One, one more thing, like why this is useful is, uh, we can also see the
[00:46:51.480 --> 00:46:56.400]   average for glen distribution, and this is also like creating a word sequence
[00:46:56.400 --> 00:46:59.760]   for different, uh, transformer models also.
[00:46:59.760 --> 00:47:01.760]   So that's what it is.
[00:47:01.760 --> 00:47:04.920]   All right.
[00:47:04.920 --> 00:47:09.600]   Um, I have a few more dashboards open, but I think we've covered those.
[00:47:09.600 --> 00:47:13.080]   So the next thing I want to hop over is, uh, engine solution.
[00:47:13.080 --> 00:47:15.560]   Uh, is there anything else you want to discuss from the
[00:47:15.960 --> 00:47:17.600]   Uh, no, I don't think so.
[00:47:17.600 --> 00:47:20.120]   I think that, I think we covered most, most of those things.
[00:47:20.120 --> 00:47:20.400]   Yeah.
[00:47:20.400 --> 00:47:21.760]   Awesome.
[00:47:21.760 --> 00:47:23.560]   Let me quickly close these tabs.
[00:47:23.560 --> 00:47:24.360]   One moment.
[00:47:24.360 --> 00:47:31.440]   So I'll quickly glance over these and then I'll request Anjum to go into the details.
[00:47:31.440 --> 00:47:35.680]   But, uh, one thing I learned was he didn't do a separate CV setup.
[00:47:35.680 --> 00:47:38.520]   Apparently the local CV and the leaderboard were quite
[00:47:38.520 --> 00:47:40.280]   correlated for this competition.
[00:47:40.280 --> 00:47:44.120]   Uh, this is quite important to understand because when you're competing on Kaggle,
[00:47:44.120 --> 00:47:47.960]   you have your local accuracy, which your models would spit out.
[00:47:47.960 --> 00:47:53.800]   And you need to keep looking at those because, uh, those are the ground
[00:47:53.800 --> 00:47:55.480]   to for you at least at that moment.
[00:47:55.480 --> 00:47:58.160]   And on Kaggle, you just get five submissions.
[00:47:58.160 --> 00:48:03.560]   So throughout these few months of competing, even Anjum has made 76
[00:48:03.560 --> 00:48:08.160]   submissions, and if you can like have a good ground score, that becomes your group.
[00:48:08.160 --> 00:48:11.640]   Now you don't need to submit to Kaggle every time you can just trust that.
[00:48:12.240 --> 00:48:16.600]   So that's why on Kaggle you also see this phrase called trust your CV, which is
[00:48:16.600 --> 00:48:19.120]   basically just trust your local CV once you set it up.
[00:48:19.120 --> 00:48:21.400]   I think there's even a team called trust your CV, right?
[00:48:21.400 --> 00:48:22.600]   Yeah.
[00:48:22.600 --> 00:48:25.160]   I think Chris Deo means trust your CV.
[00:48:25.160 --> 00:48:31.800]   So he used Abhishek's method, which we can discuss in a bit.
[00:48:31.800 --> 00:48:36.520]   And after that, he added an attention block to the last sequence
[00:48:36.520 --> 00:48:37.840]   state of the transformer.
[00:48:37.840 --> 00:48:42.120]   I've done a very poor visualization of it, but I'm guessing this,
[00:48:42.160 --> 00:48:43.200]   this is what it was like.
[00:48:43.200 --> 00:48:46.680]   You would just add an attention block to the output.
[00:48:46.680 --> 00:48:50.840]   And then you also had these text features, which we can discuss in a second.
[00:48:50.840 --> 00:48:52.520]   You would just concatenate these.
[00:48:52.520 --> 00:48:53.240]   Yeah.
[00:48:53.240 --> 00:48:57.600]   After which you would just run this through a regression head.
[00:48:57.600 --> 00:49:00.800]   So guys, if you remember, this is a regression problem because we're
[00:49:00.800 --> 00:49:04.600]   trying to predict this target from here.
[00:49:04.600 --> 00:49:08.000]   So we get a number and then we can calculate the root mean squared error.
[00:49:08.000 --> 00:49:10.160]   Is this correct?
[00:49:10.160 --> 00:49:11.680]   Or is it just a correct representation?
[00:49:11.680 --> 00:49:12.760]   That's spot on.
[00:49:12.760 --> 00:49:13.200]   Spot on.
[00:49:13.200 --> 00:49:15.240]   Awesome.
[00:49:15.240 --> 00:49:20.360]   And these features I was just talking about, and I'm also personally
[00:49:20.360 --> 00:49:24.040]   curious, like you mentioned, these weren't quite helpful as you expected.
[00:49:24.040 --> 00:49:27.120]   You use these features, maybe you could elaborate what these are.
[00:49:27.120 --> 00:49:27.720]   Yeah.
[00:49:27.720 --> 00:49:30.280]   And turned out these weren't impactful.
[00:49:30.280 --> 00:49:32.280]   So I'm also curious, how did you discover that?
[00:49:32.280 --> 00:49:33.000]   Yeah.
[00:49:33.000 --> 00:49:35.280]   Okay.
[00:49:35.280 --> 00:49:37.720]   Uh, where should we start?
[00:49:37.720 --> 00:49:41.640]   Should we get into, should we get into some code?
[00:49:41.920 --> 00:49:43.560]   Or where do you want to go next?
[00:49:43.560 --> 00:49:46.000]   Do you want to show us?
[00:49:46.000 --> 00:49:48.000]   So, yeah, let's do that.
[00:49:48.000 --> 00:49:50.560]   Um, let me see.
[00:49:50.560 --> 00:50:00.920]   Uh, I've forgotten how to show you.
[00:50:00.920 --> 00:50:04.360]   There are all of these like details that we can read through the solution, but if
[00:50:04.360 --> 00:50:07.920]   you're not computed, you probably wouldn't understand them so well.
[00:50:07.920 --> 00:50:11.240]   So that's why I was really excited about this walkthrough as well.
[00:50:11.560 --> 00:50:12.560]   For the session.
[00:50:12.560 --> 00:50:16.080]   So can you see this, this screen at the moment?
[00:50:16.080 --> 00:50:16.960]   Yes.
[00:50:16.960 --> 00:50:18.440]   I see the title.
[00:50:18.440 --> 00:50:18.920]   Cool.
[00:50:18.920 --> 00:50:28.040]   So, um, one thing, if I start with a small dataset, um, like this one, so I normally
[00:50:28.040 --> 00:50:34.760]   do my baseline model and yeah, so I used, um, Abhishek spinning and stratification
[00:50:34.760 --> 00:50:38.120]   code, so let's go to data, right.
[00:50:38.120 --> 00:50:40.320]   So we'll start with the data.
[00:50:40.920 --> 00:50:46.320]   So one thing I do, if I copy someone else's code, I'll always attribute it at the top
[00:50:46.320 --> 00:50:48.920]   here, just so that I know where I got it from.
[00:50:48.920 --> 00:50:54.520]   And then, um, if anyone else copies it, then at least then they've got some attribution.
[00:50:54.520 --> 00:51:00.560]   Um, the only thing I changed with, um, Abhishek's thing is I, I added this bit,
[00:51:00.560 --> 00:51:02.360]   this bit, so the random state.
[00:51:02.360 --> 00:51:09.600]   So, um, so the way, what, what Abhishek has done here is because this is a continuous
[00:51:09.640 --> 00:51:15.720]   target, um, but we want to create our five folds to look as equal as possible.
[00:51:15.720 --> 00:51:20.760]   So they look, when you, when you take a random sample and have five random groups,
[00:51:20.760 --> 00:51:24.560]   you want those two groups, those five groups to look as similar as possible.
[00:51:24.560 --> 00:51:31.720]   Um, but so he binned it using this, this function called pandas cut, um, using a
[00:51:31.720 --> 00:51:36.960]   number of bins and then split it up into five, uh, five folds.
[00:51:37.480 --> 00:51:42.160]   So one thing I generally do if you, if you start with a very small data set is
[00:51:42.160 --> 00:51:44.440]   change this random state.
[00:51:44.440 --> 00:51:50.800]   And when you do the stratified K fold and then run another experiment, and if you see
[00:51:50.800 --> 00:51:55.480]   these score changes by quite a lot, that means your score can vary.
[00:51:55.480 --> 00:51:59.080]   Uh, the score varies just by pure randomness, right?
[00:51:59.080 --> 00:52:02.200]   And I found that I did some experiments.
[00:52:02.200 --> 00:52:06.600]   I changed this random state and I saw, I saw my cross-validation score
[00:52:06.600 --> 00:52:07.480]   jump around quite a lot.
[00:52:07.480 --> 00:52:10.320]   It was very, very sensitive to the random seed.
[00:52:10.320 --> 00:52:12.840]   I'm not, and don't get confused.
[00:52:12.840 --> 00:52:14.600]   The random seed is not the hyper parameter.
[00:52:14.600 --> 00:52:17.120]   It's not supposed to be a number you tune.
[00:52:17.120 --> 00:52:23.120]   It's just, uh, it's, it just shows you, um, how noisy the data is.
[00:52:23.120 --> 00:52:29.560]   So when I did that, that's when I knew that if for my cross-validation scheme,
[00:52:29.560 --> 00:52:35.080]   I'd have to run my five folds using lots of different random seeds and, um, track
[00:52:35.080 --> 00:52:40.960]   that, so that actually ended up being, um, quite a lot of experiments.
[00:52:40.960 --> 00:52:45.440]   So I was going through my weights and biases dashboard, which is this.
[00:52:45.440 --> 00:52:47.880]   So my, my, my dashboard is not as pretty as Ruchi's.
[00:52:47.880 --> 00:52:49.840]   It's very, very minimalistic.
[00:52:49.840 --> 00:52:55.240]   Um, but it's, I usually set, set things up like this.
[00:52:55.240 --> 00:52:59.160]   So you can see for each experiment, I had 25 runs.
[00:52:59.160 --> 00:53:04.600]   So that is five, five seeds and then five folds.
[00:53:04.600 --> 00:53:07.720]   So I usually set up my models like this.
[00:53:07.720 --> 00:53:12.280]   So each run has a timestamp and then you've got five folds in there.
[00:53:12.280 --> 00:53:15.360]   And then I've grouped it by something called a slug.
[00:53:15.360 --> 00:53:18.200]   So this is, this is a really cool Python package called cool name.
[00:53:18.200 --> 00:53:20.560]   And it just gives you a random name.
[00:53:20.560 --> 00:53:23.080]   Some of these names are excellent.
[00:53:23.080 --> 00:53:25.840]   So like spectacular hyena, right?
[00:53:25.840 --> 00:53:28.800]   Refine clever demonic crowd.
[00:53:28.800 --> 00:53:34.440]   So there's a package called cool name.
[00:53:34.440 --> 00:53:41.920]   So, um, so, so the way I would, if I go back here, so the way I would kick off
[00:53:41.920 --> 00:53:48.520]   my script is I would have some entry points, train and infer, and I keep
[00:53:48.520 --> 00:53:51.480]   my code in this, um, source folder.
[00:53:51.480 --> 00:53:56.240]   And then I kick off my train, um, scripts using this shell script.
[00:53:56.240 --> 00:54:00.880]   So I have five different random seeds and you can see, I guess I was
[00:54:00.880 --> 00:54:02.120]   playing around with other seeds.
[00:54:02.560 --> 00:54:04.880]   This is the cool name thing I was talking about.
[00:54:04.880 --> 00:54:09.880]   So it'll give it, it'll generate a nice, a funny name for the group of names.
[00:54:09.880 --> 00:54:13.640]   And then it'll run five folds over these five seeds.
[00:54:13.640 --> 00:54:17.640]   And then you'd end up with 25 runs like this.
[00:54:17.640 --> 00:54:19.440]   So this was pretty cool.
[00:54:19.440 --> 00:54:25.040]   And you know, if I go back here, like go have a look at the project.
[00:54:25.040 --> 00:54:29.240]   So your baseline, uh, I'm just looking at the last curve was around.
[00:54:30.640 --> 00:54:31.800]   I think point three.
[00:54:31.800 --> 00:54:33.160]   Yeah.
[00:54:33.160 --> 00:54:33.440]   Yeah.
[00:54:33.440 --> 00:54:34.280]   Somewhere around that.
[00:54:34.280 --> 00:54:36.800]   So you can see that the loss is very noisy.
[00:54:36.800 --> 00:54:42.800]   The train loss is pretty smooth, but the, um, the validation loss, because
[00:54:42.800 --> 00:54:45.080]   it's quite a small data set, it jumps around quite a bit.
[00:54:45.080 --> 00:54:48.280]   I'll, I'll go, I'll go into that in a second.
[00:54:48.280 --> 00:54:53.240]   Cause, um, yeah, I'll, I'll show you how I found essentially.
[00:54:53.240 --> 00:54:56.560]   You want to try to find the best model in all of this, right?
[00:54:56.560 --> 00:54:57.880]   Yeah.
[00:54:58.400 --> 00:55:01.760]   Um, so it was quite cool, uh, using, oh, here we go.
[00:55:01.760 --> 00:55:01.960]   Yeah.
[00:55:01.960 --> 00:55:07.680]   So I've got it for this, for this, um, competition I did 2300 runs.
[00:55:07.680 --> 00:55:09.080]   That's incredible.
[00:55:09.080 --> 00:55:09.760]   Yeah.
[00:55:09.760 --> 00:55:15.240]   So it just speaks to like, what, how many experiments does it take to get to exactly?
[00:55:15.240 --> 00:55:20.480]   So it comes back to what I was saying earlier, the mantra of, um, test as many
[00:55:20.480 --> 00:55:22.160]   ideas in the shortest amount of time.
[00:55:23.240 --> 00:55:31.680]   Um, so if I go back to, so, so I, I usually have a shell script like this and I can
[00:55:31.680 --> 00:55:33.960]   write another shell script just to line up experiments.
[00:55:33.960 --> 00:55:37.120]   So I just keep running train, train, train, train, train with
[00:55:37.120 --> 00:55:38.720]   different set of hyper parameters.
[00:55:38.720 --> 00:55:41.440]   So I'll have a file here, like hyper parameters.yaml.
[00:55:41.440 --> 00:55:44.880]   And I'll have hyper parameters for all my different models.
[00:55:44.880 --> 00:55:50.160]   And I could just type them in, line up on my experiments and then go meet my
[00:55:50.160 --> 00:55:54.800]   friends and then come back and then my machine's been working at a hundred
[00:55:54.800 --> 00:55:57.040]   percent, but running lots of ideas.
[00:55:57.040 --> 00:55:59.200]   And the room is boiling at boiling temperature.
[00:55:59.200 --> 00:56:06.720]   So the initial, I would say the initial first few hundred runs I did with Roberta
[00:56:06.720 --> 00:56:07.240]   base.
[00:56:07.240 --> 00:56:10.720]   Um, so this was on my machine.
[00:56:10.720 --> 00:56:13.520]   It probably did, took about six minutes per fold.
[00:56:13.520 --> 00:56:19.040]   So I tried, so I tried to, again, coming back to the mantra of test as many ideas
[00:56:19.040 --> 00:56:20.000]   as short as possible.
[00:56:20.000 --> 00:56:23.760]   I want, I managed, I wanted to get my experiment time very short so I can test
[00:56:23.760 --> 00:56:24.560]   lots of ideas.
[00:56:24.560 --> 00:56:30.080]   And because I had to, uh, test across lots of different random seeds, I had to do
[00:56:30.080 --> 00:56:31.400]   25 runs of that.
[00:56:31.400 --> 00:56:32.920]   So 25 times six minutes.
[00:56:32.920 --> 00:56:37.000]   Um, and also to track that as well.
[00:56:37.000 --> 00:56:38.640]   So I used a Google sheet.
[00:56:38.640 --> 00:56:41.280]   So this is what my Google sheet looks like.
[00:56:41.280 --> 00:56:47.200]   So there was actually, if you go into my repository, there's a little Python file
[00:56:47.200 --> 00:56:48.640]   here called Agscores.
[00:56:48.680 --> 00:56:52.520]   And this writes my cross validation scores to this Google sheet.
[00:56:52.520 --> 00:56:57.080]   Um, so you can see that this is the group.
[00:56:57.080 --> 00:57:01.000]   These are the five runs that I did along the five seeds.
[00:57:01.000 --> 00:57:02.720]   And then these are the five folds.
[00:57:02.720 --> 00:57:07.800]   And then I would take the average of these scores in a sort of a pivot table like
[00:57:07.800 --> 00:57:08.280]   this.
[00:57:08.280 --> 00:57:12.520]   And then that, that would be my final cross validation score.
[00:57:15.120 --> 00:57:19.400]   So that's how, that's how I knew if I changed something in the model, if it was
[00:57:19.400 --> 00:57:21.240]   making, making something better or worse.
[00:57:21.240 --> 00:57:25.880]   If I changed any parameters in my attention block, if I was adding any more
[00:57:25.880 --> 00:57:27.560]   features, any things like that.
[00:57:27.560 --> 00:57:33.880]   So I was using these 25 runs and essentially averaging those, um, to
[00:57:33.880 --> 00:57:36.160]   generate my final cross validation score.
[00:57:36.160 --> 00:57:41.080]   I'm also curious, why did you pick Roberta as like the baseline model?
[00:57:41.080 --> 00:57:42.360]   Like what was the intuition there?
[00:57:42.360 --> 00:57:44.000]   So, uh, yeah, good question.
[00:57:44.000 --> 00:57:47.600]   So Roberta is generally a really good, um, all arounder.
[00:57:47.600 --> 00:57:48.800]   It runs fast.
[00:57:48.800 --> 00:57:50.200]   So Roberta base runs fast.
[00:57:50.200 --> 00:57:55.040]   So that, that ticks my box for having something I can iterate ideas fast and it
[00:57:55.040 --> 00:57:56.280]   just works really well.
[00:57:56.280 --> 00:58:05.720]   And so the other thing I did as well, if we go to Hugging Face, um, models, Roberta.
[00:58:05.720 --> 00:58:07.480]   So Roberta works well.
[00:58:07.480 --> 00:58:11.920]   If you go into the paper, there's lots of reasons why it's such a great model.
[00:58:12.560 --> 00:58:16.320]   But one of the reasons is that it's trained on a huge amount of data.
[00:58:16.320 --> 00:58:20.240]   And if you have a look at what it was trained on, I love these little
[00:58:20.240 --> 00:58:22.160]   model, model cards on Hugging Face.
[00:58:22.160 --> 00:58:23.000]   They're really useful.
[00:58:23.000 --> 00:58:28.200]   It was trained on Wikipedia, which is cool because we have lots of
[00:58:28.200 --> 00:58:34.080]   Wikipedia in our data set and something called book corpus, which is interesting
[00:58:34.080 --> 00:58:39.560]   because, um, I think in the common data set, you can see there's some, um, story
[00:58:39.560 --> 00:58:44.120]   book, um, style, um, excerpts from stories.
[00:58:44.120 --> 00:58:47.240]   I think there was something called African story book or something like that.
[00:58:47.240 --> 00:58:49.840]   And then there's another story data set.
[00:58:49.840 --> 00:58:54.720]   So I started with Roberta base because it's super fast to run worked pretty well.
[00:58:54.720 --> 00:58:58.520]   A lot of the public notebooks were getting pretty good scores with that.
[00:58:58.520 --> 00:59:02.480]   Um, and then from that, I sort of branched out.
[00:59:02.480 --> 00:59:06.600]   So if you go back here, Hugging Face has a really handy way of filtering.
[00:59:06.600 --> 00:59:08.080]   Models here.
[00:59:08.080 --> 00:59:12.600]   So if I wanted models that were trained on Wikipedia, I could click on this button
[00:59:12.600 --> 00:59:14.840]   and there's all these other models that are trained in Wikipedia.
[00:59:14.840 --> 00:59:16.880]   And then there's interesting models.
[00:59:16.880 --> 00:59:21.920]   We like, you know, funnel transformer, um, bird Albert and stuff like that.
[00:59:21.920 --> 00:59:23.680]   Did you try any of these as well?
[00:59:23.680 --> 00:59:24.280]   Yeah.
[00:59:24.280 --> 00:59:24.960]   Tried them all.
[00:59:24.960 --> 00:59:25.680]   Tried them all.
[00:59:25.680 --> 00:59:25.880]   Yeah.
[00:59:25.880 --> 00:59:33.160]   So, um, so one of the really cool things is about Hugging Face.
[00:59:33.640 --> 00:59:38.600]   And this, I like this a lot is they have this, these classes called auto models
[00:59:38.600 --> 00:59:41.440]   and they call it the auto classes.
[00:59:41.440 --> 00:59:42.720]   Let's go back to Hugging Face.
[00:59:42.720 --> 00:59:49.560]   Um, so in transformers, okay, they have them.
[00:59:49.560 --> 00:59:50.760]   Auto classes.
[00:59:50.760 --> 00:59:52.920]   So they have all of these things.
[00:59:52.920 --> 00:59:56.360]   So auto config or to tokenize a auto model.
[00:59:56.360 --> 01:00:02.440]   And essentially you could use this class and you, you just have to pass it.
[01:00:02.640 --> 01:00:05.320]   The name of the model you want to do, and you don't have to change any of your code.
[01:00:05.320 --> 01:00:06.520]   You can just switch your model.
[01:00:06.520 --> 01:00:08.160]   So that's what I did.
[01:00:08.160 --> 01:00:08.800]   So if I go back,
[01:00:08.800 --> 01:00:10.720]   You can just add it as a parameter and then
[01:00:10.720 --> 01:00:11.280]   Exactly.
[01:00:11.280 --> 01:00:12.840]   speak that in your bash script.
[01:00:12.840 --> 01:00:13.440]   Yeah.
[01:00:13.440 --> 01:00:14.960]   So that's what I did.
[01:00:14.960 --> 01:00:18.400]   So in this hyper parameter file, I just had the model name, which is
[01:00:18.400 --> 01:00:20.760]   exact, essentially this name here.
[01:00:20.760 --> 01:00:25.320]   Um, it will only be different models.
[01:00:25.760 --> 01:00:33.760]   And then in my code, if I go to, um, models, you can see that it's
[01:00:33.760 --> 01:00:35.200]   just using these auto classes.
[01:00:35.200 --> 01:00:39.640]   So I load the configuration using that model name.
[01:00:39.640 --> 01:00:45.760]   Um, the tokenizer, which is in the datasets file, it loads that from
[01:00:45.760 --> 01:00:48.280]   an auto tokenizer and then auto model as well.
[01:00:48.280 --> 01:00:50.280]   And that's just using the model name.
[01:00:50.680 --> 01:00:56.400]   So the, again, coming back to the same idea of trying, testing as many ideas
[01:00:56.400 --> 01:00:57.640]   in the shortest amount of time.
[01:00:57.640 --> 01:01:03.240]   If you can parameterize your code so that you can just change one string and
[01:01:03.240 --> 01:01:07.320]   completely change your model architecture, you can iterate ideas so much faster.
[01:01:07.320 --> 01:01:12.040]   Um, so that's essentially, go ahead.
[01:01:12.040 --> 01:01:15.920]   How much extra effort does it, uh, like, do you need, do you put in to
[01:01:15.920 --> 01:01:17.040]   like set up the repository?
[01:01:17.040 --> 01:01:18.360]   Like how much time did it take?
[01:01:18.400 --> 01:01:22.520]   I'm guessing it's less because like you've been in data science for a long time.
[01:01:22.520 --> 01:01:24.760]   So like, is this how you always start your experiments?
[01:01:24.760 --> 01:01:29.920]   Yeah, I guess, I guess like it's, it's a process of evolved over time.
[01:01:29.920 --> 01:01:36.360]   Um, so now, um, I think Abhishek did a really good video a while ago
[01:01:36.360 --> 01:01:38.280]   about how he sets up his folders.
[01:01:38.280 --> 01:01:42.000]   And I think I started with that and I sort of, um, swapped and
[01:01:42.000 --> 01:01:43.840]   changed stuff for how I work as well.
[01:01:44.440 --> 01:01:50.520]   Um, and then, yeah, I sort of converged on this sort of, um, layout.
[01:01:50.520 --> 01:01:55.240]   And then the other cool thing is I learned the other day is if you go to
[01:01:55.240 --> 01:02:00.920]   GitHub, you can set up, um, something called a template repository.
[01:02:00.920 --> 01:02:03.760]   So is this going to work?
[01:02:03.760 --> 01:02:06.000]   So I've made a repository called Kaggle template.
[01:02:06.000 --> 01:02:09.840]   You guys can use it if you want or whatever, and you can hit this button.
[01:02:10.560 --> 01:02:15.520]   Um, you go into the settings, say users set this as a template repository.
[01:02:15.520 --> 01:02:19.640]   And so you use this template and then you just set a new repository name and
[01:02:19.640 --> 01:02:23.320]   then it'll create a new repository using that template, which saves
[01:02:23.320 --> 01:02:24.400]   you a lot of time as well.
[01:02:24.400 --> 01:02:27.280]   Uh, yeah.
[01:02:27.280 --> 01:02:30.120]   So I, I've started using that, um, quite a lot.
[01:02:30.120 --> 01:02:33.280]   It is another, there's another tool called cookie cutter as well.
[01:02:33.280 --> 01:02:36.400]   Um, which makes template repositories for you.
[01:02:36.400 --> 01:02:37.360]   It's, it's quite heavy.
[01:02:37.360 --> 01:02:39.760]   It's quite like, it's quite customizable.
[01:02:39.800 --> 01:02:43.960]   But for what I need, these sort of GitHub templates work well.
[01:02:43.960 --> 01:02:48.480]   So I kind of, um, yeah, so I have usually have two entry
[01:02:48.480 --> 01:02:50.920]   points, uh, train and infer.
[01:02:50.920 --> 01:02:55.000]   Um, I'd have a shell script, which calls train.
[01:02:55.000 --> 01:02:58.600]   Um, so that, and then I can write another shell script, which will
[01:02:58.600 --> 01:03:00.440]   just do lots of experiments.
[01:03:00.440 --> 01:03:03.200]   All I do is change the config, the name of the configuration.
[01:03:03.200 --> 01:03:06.520]   And then most of my code lives in this source file.
[01:03:06.520 --> 01:03:08.760]   Awesome.
[01:03:09.240 --> 01:03:11.000]   So that's the models.
[01:03:11.000 --> 01:03:11.440]   Yeah.
[01:03:11.440 --> 01:03:12.160]   Let's go through that.
[01:03:12.160 --> 01:03:13.680]   Let's go into that.
[01:03:13.680 --> 01:03:18.040]   So this is, um, um, the model.
[01:03:18.040 --> 01:03:22.440]   So I do point out that like, I was really curious, like, it's quite
[01:03:22.440 --> 01:03:25.480]   confusing to understand how do you add a regression head and it's just
[01:03:25.480 --> 01:03:27.120]   like adding a simple linear layer.
[01:03:27.120 --> 01:03:29.080]   If I remember correctly, that's how you got it.
[01:03:29.080 --> 01:03:30.160]   Yeah.
[01:03:30.160 --> 01:03:31.800]   It's okay.
[01:03:31.800 --> 01:03:36.000]   It's quite confusing terminology because, um, I don't, lots of
[01:03:36.000 --> 01:03:38.240]   people ask what, what's a head.
[01:03:38.840 --> 01:03:43.040]   Um, so in my, in my mind, what a head is basically the bit, the
[01:03:43.040 --> 01:03:46.680]   last, very final part of the model is the bit that makes prediction.
[01:03:46.680 --> 01:03:52.840]   And you use a pre-trained encoder or pre-trained com net, whatever
[01:03:52.840 --> 01:03:54.440]   that does all the heavy lifting.
[01:03:54.440 --> 01:03:58.760]   And it generates a feature at the end, just a tensor of some shape.
[01:03:58.760 --> 01:04:02.880]   And then you pass that to your head, which turns that tensor
[01:04:02.880 --> 01:04:05.440]   of some shape into a prediction.
[01:04:05.960 --> 01:04:09.640]   And in this case, there's a prediction of shape one, because
[01:04:09.640 --> 01:04:11.120]   you're just predicting one value.
[01:04:11.120 --> 01:04:14.600]   In other cases, that prediction could be a segmentation
[01:04:14.600 --> 01:04:15.840]   mask or something like that.
[01:04:15.840 --> 01:04:20.680]   Um, so that's, that's what people mean when they talk about a head.
[01:04:20.680 --> 01:04:24.440]   So it's basically going from a feature that's extracted from the big
[01:04:24.440 --> 01:04:26.440]   transformer or com net or something.
[01:04:26.440 --> 01:04:30.600]   And you turn that feature into the actual thing you want to predict.
[01:04:30.600 --> 01:04:33.600]   Um, so this is my model.
[01:04:34.120 --> 01:04:40.000]   So I use, um, PyTorch lightning, um, which is, which I, I really enjoy using,
[01:04:40.000 --> 01:04:45.280]   but if you're, I would say if you're learning how to use PyTorch, or if you're
[01:04:45.280 --> 01:04:50.680]   new to deep learning, maybe don't use PyTorch lightning because, um, it's good
[01:04:50.680 --> 01:04:54.440]   to write your own training loops and validation loops and stuff like that.
[01:04:54.440 --> 01:04:56.280]   Because you really learn how things work.
[01:04:56.280 --> 01:04:58.960]   If there's a bug, then you know where to look for it.
[01:04:58.960 --> 01:05:01.240]   You learn how to calculate your own metrics and stuff.
[01:05:01.760 --> 01:05:04.680]   But once you've done that 20 or 30 times, it's only so many times you
[01:05:04.680 --> 01:05:05.640]   can write your own training.
[01:05:05.640 --> 01:05:07.640]   So I use PyTorch.
[01:05:07.640 --> 01:05:09.240]   Shameless plug as well.
[01:05:09.240 --> 01:05:12.840]   Uh, sorry to interrupt you again, but we'll be reading this book, which
[01:05:12.840 --> 01:05:14.840]   is our deep learning with PyTorch.
[01:05:14.840 --> 01:05:20.040]   This is by the cool devs and one of the author will be here next week for a webinar.
[01:05:20.040 --> 01:05:24.360]   So if you're, if you're new to PyTorch, uh, I would recommend signing up for that.
[01:05:24.360 --> 01:05:25.840]   Yeah, absolutely.
[01:05:25.840 --> 01:05:26.040]   Yeah.
[01:05:26.040 --> 01:05:30.080]   But yeah, PyTorch lightning is really cool.
[01:05:30.360 --> 01:05:35.320]   Um, but yeah, if you're new to PyTorch, I totally recommend just writing your
[01:05:35.320 --> 01:05:39.400]   own loops and stuff, training loops, validation loops, metric calculation,
[01:05:39.400 --> 01:05:40.080]   things like that.
[01:05:40.080 --> 01:05:45.200]   So the PyTorch lightning is it's, uh, it works.
[01:05:45.200 --> 01:05:50.360]   Um, you have a lightning module, which is actually just a normal torch module,
[01:05:50.360 --> 01:05:52.160]   which you'll probably cover later.
[01:05:52.160 --> 01:05:55.440]   And then you, you define all your stuff here.
[01:05:55.440 --> 01:05:59.000]   So here I defined, um, my model.
[01:05:59.000 --> 01:06:00.080]   So my transformer.
[01:06:01.080 --> 01:06:07.480]   And I also defined my head, uh, and then the regression, um, head here.
[01:06:07.480 --> 01:06:14.920]   So at this, at this stage, it kind of looks like a normal, um, PyTorch module.
[01:06:14.920 --> 01:06:22.000]   So you have, uh, an, um, an init function and you have a forward
[01:06:22.000 --> 01:06:24.200]   function, which is normal PyTorch.
[01:06:24.200 --> 01:06:26.280]   Yeah.
[01:06:26.560 --> 01:06:31.800]   And then the only bit different bits with PyTorch lightning is you put, um,
[01:06:31.800 --> 01:06:33.680]   what happens every training step.
[01:06:33.680 --> 01:06:37.680]   So with PyTorch lightning, you have a separate class called a trainer,
[01:06:37.680 --> 01:06:39.480]   which deals with all the loops.
[01:06:39.480 --> 01:06:46.200]   Um, and then that trainer calls these additional functions, um, to do stuff.
[01:06:46.200 --> 01:06:50.680]   Uh, and, you know, because a torch module is just a normal Python class.
[01:06:50.680 --> 01:06:52.640]   You can put anything in there and that's what they've done.
[01:06:53.240 --> 01:06:56.760]   Um, and you can make your own deep learning framework if you wanted to.
[01:06:56.760 --> 01:06:59.800]   Um, you can just put anything you want in these classes.
[01:06:59.800 --> 01:07:08.360]   Um, so my model is a transformer, uh, which I defined using this
[01:07:08.360 --> 01:07:10.440]   input argument called model name.
[01:07:10.440 --> 01:07:15.880]   And then it, it outputs, um, a state.
[01:07:15.880 --> 01:07:17.960]   And then that state goes into the head.
[01:07:17.960 --> 01:07:21.200]   And then that head calculates the target variable.
[01:07:21.920 --> 01:07:24.960]   Right at line 92, we can see the attention head as well.
[01:07:24.960 --> 01:07:27.480]   You would just talk about the attention that you've added.
[01:07:27.480 --> 01:07:32.040]   And at line 90, I believe you add the regressor head, which
[01:07:32.040 --> 01:07:33.200]   we were just talking about.
[01:07:33.200 --> 01:07:33.560]   Yeah.
[01:07:33.560 --> 01:07:33.920]   Yeah.
[01:07:33.920 --> 01:07:34.120]   Yeah.
[01:07:34.120 --> 01:07:34.400]   Yeah.
[01:07:34.400 --> 01:07:39.000]   So it's probably easier to show you what's going on in say maybe a notebook.
[01:07:39.000 --> 01:07:44.440]   So let's say if, if you've got a model, so I've got a reverse of base here.
[01:07:44.440 --> 01:07:48.960]   Um, I've got a tokenizer here.
[01:07:49.000 --> 01:07:52.720]   So auto tokenizer, which I've called from this model name parameter.
[01:07:52.720 --> 01:07:55.720]   Um, I called use the config as well.
[01:07:55.720 --> 01:07:59.040]   So the, these conflict files are useful for hugging face because they have
[01:07:59.040 --> 01:08:01.040]   some internal parameters of the model.
[01:08:01.040 --> 01:08:04.480]   So sometimes it's useful to know how many, what's the
[01:08:04.480 --> 01:08:06.560]   output size of the transformer.
[01:08:06.560 --> 01:08:09.280]   So then you can, um, you know what to do there.
[01:08:09.280 --> 01:08:15.640]   So for example, um, I want to know the hidden size.
[01:08:15.640 --> 01:08:18.000]   So it's 768.
[01:08:18.000 --> 01:08:24.960]   So if I know that that is 768, then I know that my head has to accept a tensor by 768.
[01:08:24.960 --> 01:08:29.720]   Um, so these auto model classes are super cool.
[01:08:29.720 --> 01:08:33.360]   You define the model like this, and then you just find an input.
[01:08:33.360 --> 01:08:35.280]   So I've got a little string here.
[01:08:35.280 --> 01:08:36.720]   Sorry, my dog is very naughty.
[01:08:36.720 --> 01:08:42.520]   And then, and then you get the outputs.
[01:08:42.520 --> 01:08:46.800]   So you could either, you can have this flag, the outputs hidden states as well.
[01:08:46.800 --> 01:08:49.920]   But so you get an output, which is this old dict.
[01:08:49.920 --> 01:08:56.480]   Um, so you get something called a last hidden state, a pooler output.
[01:08:56.480 --> 01:08:59.240]   And if you, there's these optional hidden states.
[01:08:59.240 --> 01:09:04.440]   Um, so that's the output you get from the transformer.
[01:09:04.440 --> 01:09:06.440]   So the idea of the head is,
[01:09:06.440 --> 01:09:09.080]   could you please repeat what's pooler output?
[01:09:09.080 --> 01:09:10.040]   I wasn't clear on that.
[01:09:10.040 --> 01:09:14.640]   So, so you've got, uh, I'll explain in a sec.
[01:09:14.640 --> 01:09:14.960]   Yeah.
[01:09:15.040 --> 01:09:18.240]   So, um, you've got different hidden states.
[01:09:18.240 --> 01:09:21.040]   So, um, let me show you.
[01:09:21.040 --> 01:09:22.480]   So you've got hidden states.
[01:09:22.480 --> 01:09:24.960]   So this is from each layer of the encoder.
[01:09:24.960 --> 01:09:39.520]   So this is actually a tuple of, uh, hidden states.
[01:09:39.520 --> 01:09:44.080]   So you've got 13 hidden states here from different layers of the transformer.
[01:09:44.400 --> 01:09:50.120]   Um, the last hidden state is a tensor, which is this shape.
[01:09:50.120 --> 01:09:51.800]   So this is batch size.
[01:09:51.800 --> 01:09:56.000]   This is the length of the sequence, which could be two, five, six or whatever.
[01:09:56.000 --> 01:10:00.880]   This, this number changes depending on the length of the sequence, but you can, you
[01:10:00.880 --> 01:10:04.760]   can fix that and then pad it to, if you've got short sequences, you can pad it to a
[01:10:04.760 --> 01:10:07.480]   certain size and then you've got the pooler output.
[01:10:07.480 --> 01:10:13.960]   So to, to send this to a linear layer, you need a one dimensional tensor.
[01:10:14.960 --> 01:10:17.200]   Um, which is what this is, right?
[01:10:17.200 --> 01:10:24.000]   So you go from a tensor like this, um, and you need to reduce one of the dimensions.
[01:10:24.000 --> 01:10:27.120]   So you need to get rid of this dimension here.
[01:10:27.120 --> 01:10:32.080]   So what the pooler output does is it takes the mean along this dimension.
[01:10:32.080 --> 01:10:39.560]   So you've got a tensor of 768 by nine and you just take the mean of those, um, nine
[01:10:39.560 --> 01:10:45.960]   rows, and then you end up with a, um, you end up with a tensor that looks like this
[01:10:45.960 --> 01:10:47.800]   and you can pass that to a linear layer then.
[01:10:47.800 --> 01:10:52.160]   So you go from 768 down to one, which is your target output target.
[01:10:52.160 --> 01:10:57.040]   Um, so the pool output is cool.
[01:10:57.040 --> 01:11:00.480]   And if you go into the head and face code, they use mean pooling.
[01:11:00.480 --> 01:11:06.600]   Um, and you know, mean pooling is cool, but if you think about it, a mean pooling
[01:11:06.600 --> 01:11:10.840]   is kind of just, it's a weighted average, but the weights are the number of features.
[01:11:10.840 --> 01:11:11.080]   Right?
[01:11:11.080 --> 01:11:17.320]   So what if you could just do a weighted combination, um, that wasn't mean, you
[01:11:17.320 --> 01:11:18.960]   know, you could choose what those weights are.
[01:11:18.960 --> 01:11:21.600]   So this is where the attention block comes in.
[01:11:21.600 --> 01:11:23.840]   So that's essentially what we're doing here.
[01:11:23.840 --> 01:11:28.520]   We're calculating what those weights are and then summing them up, which is the
[01:11:28.520 --> 01:11:31.360]   same as a mean pooling, but those, those weights are different.
[01:11:31.720 --> 01:11:37.600]   So instead of using this pooled output, I'm taking this last hidden state here.
[01:11:37.600 --> 01:11:44.000]   And then reducing this dimension by using this sort of weighted sum.
[01:11:44.000 --> 01:11:48.240]   And then I get, if I put that through the attention block using the last hidden
[01:11:48.240 --> 01:11:52.960]   state, I end up with a one dimensional tensor, and then I can pass that to a
[01:11:52.960 --> 01:11:57.400]   linear layer and output my target that I'm trying to predict.
[01:11:58.600 --> 01:12:01.960]   I want to plug my colleague Aman Arora's session.
[01:12:01.960 --> 01:12:05.560]   He just explained attention very nicely today as well.
[01:12:05.560 --> 01:12:08.880]   But if you've gone through those, you can understand it from what I
[01:12:08.880 --> 01:12:12.520]   understand, please correct me if I'm wrong, but adding this block at a high
[01:12:12.520 --> 01:12:18.640]   level would like just tell the model which words to pay attention to basically.
[01:12:18.640 --> 01:12:19.400]   Yeah.
[01:12:19.400 --> 01:12:20.080]   Yeah.
[01:12:20.080 --> 01:12:23.080]   That's a, that's a, that's a really easy way to explain it.
[01:12:23.080 --> 01:12:23.360]   Yeah.
[01:12:23.360 --> 01:12:27.800]   So those weights that, uh, you know, if you're, if you're just taking mean
[01:12:27.800 --> 01:12:32.440]   pooling, those weights are equal for everything you want with the attention
[01:12:32.440 --> 01:12:37.600]   block, those weights will be bigger or smaller depending on what's interesting
[01:12:37.600 --> 01:12:43.480]   to the model, um, and it's a, it's a more learnable way of getting the model to
[01:12:43.480 --> 01:12:49.560]   learn, okay, given a bit of text, tell me what's the most interesting bit for the
[01:12:49.560 --> 01:12:55.240]   readability score rather than you implying, um, almost an inductive bias by
[01:12:55.240 --> 01:12:56.800]   just taking the mean of everything.
[01:12:56.800 --> 01:12:57.760]   Hmm.
[01:12:58.200 --> 01:13:01.680]   Um, so you get, it's another way of getting the model to learn about what's
[01:13:01.680 --> 01:13:04.120]   interesting to help it predict more accurately.
[01:13:04.120 --> 01:13:05.640]   Thanks.
[01:13:05.640 --> 01:13:06.600]   Thanks for clarifying that.
[01:13:06.600 --> 01:13:07.400]   That's cool.
[01:13:07.400 --> 01:13:13.720]   Um, so after I've put that through the attention block, I get a tensor of
[01:13:13.720 --> 01:13:19.280]   shape 768, which I can then throw into, uh, just a linear head.
[01:13:19.280 --> 01:13:26.160]   So if you, if you tell it that N hidden is 768, um, it'll be a linear head with
[01:13:26.760 --> 01:13:30.800]   an output of one, I've also got two here because I was playing around with the
[01:13:30.800 --> 01:13:35.960]   different loss function, um, called KL loss, um, where I was actually trying to
[01:13:35.960 --> 01:13:38.560]   predict the target and the standard error.
[01:13:38.560 --> 01:13:44.160]   So that other standard error, um, feature we were talking about at the start, um,
[01:13:44.160 --> 01:13:47.720]   or, you know, you can just predict a size of one.
[01:13:47.720 --> 01:13:55.200]   So if you go down to here, so here we've got the model, um, you do the forward
[01:13:55.200 --> 01:13:58.440]   pass, Oh, I forgot about the additional features.
[01:13:58.440 --> 01:13:58.800]   Sorry.
[01:13:58.800 --> 01:14:04.040]   So this is the output from the attention head.
[01:14:04.040 --> 01:14:07.400]   So this is the shape in the example with Roberta base.
[01:14:07.400 --> 01:14:12.440]   So this will be a 768 long one dimensional tensor.
[01:14:12.440 --> 01:14:15.160]   And then I've got some other features.
[01:14:15.160 --> 01:14:17.720]   Which I just concatenate onto that.
[01:14:17.720 --> 01:14:20.400]   So what are those other features?
[01:14:20.400 --> 01:14:27.600]   So if we go back to my datasets, uh, here I've got the, this is the normal
[01:14:27.600 --> 01:14:31.440]   PI torch dataset, I generate some additional features.
[01:14:31.440 --> 01:14:36.240]   So this is using tech stat, which, which we had in Ruchi's, um, notebook.
[01:14:36.240 --> 01:14:42.040]   And I've calculated the flesh reading ease and smog, smog in texts.
[01:14:42.040 --> 01:14:44.840]   And also, um, normalize those as well.
[01:14:45.840 --> 01:14:50.520]   Um, according to some numbers, just, just from that I calculated from the entire
[01:14:50.520 --> 01:14:56.920]   dataset, um, so those features, uh, keep, keep them back.
[01:14:56.920 --> 01:14:59.000]   So we just been calculating those numbers.
[01:14:59.000 --> 01:14:59.360]   Yes.
[01:14:59.360 --> 01:15:00.800]   I've got those two numbers.
[01:15:00.800 --> 01:15:01.760]   Exactly.
[01:15:01.760 --> 01:15:07.320]   So I, I got those two numbers, concatenate that to the output for the attention
[01:15:07.320 --> 01:15:11.520]   head, and then throw that into the linear layer, which is the rest.
[01:15:11.520 --> 01:15:13.080]   Awesome.
[01:15:13.080 --> 01:15:14.280]   That makes sense.
[01:15:14.280 --> 01:15:15.160]   That that's how you get.
[01:15:16.160 --> 01:15:16.640]   Yeah.
[01:15:16.640 --> 01:15:17.360]   Yeah.
[01:15:17.360 --> 01:15:21.280]   So that's how we get from go from the transformer output.
[01:15:21.280 --> 01:15:23.880]   You put that output through the attention head.
[01:15:23.880 --> 01:15:25.640]   So you get one dimensional tensor.
[01:15:25.640 --> 01:15:30.560]   You concatenate two more features onto that, and then put that through the
[01:15:30.560 --> 01:15:33.320]   linear layer to get your predicted target.
[01:15:33.320 --> 01:15:39.200]   And then you push that through the loss function and then that's what
[01:15:39.200 --> 01:15:40.360]   you use to train the model.
[01:15:42.240 --> 01:15:46.920]   I'm curious, were you like also monitoring, uh, how much is the GPU being used?
[01:15:46.920 --> 01:15:51.280]   Because like I have 30 nineties as well, uh, but Kaggle has P 100.
[01:15:51.280 --> 01:15:54.480]   So for inference, were you like accounting for batch sizes or
[01:15:54.480 --> 01:15:55.760]   like, did you think about that?
[01:15:55.760 --> 01:15:56.520]   Yeah.
[01:15:56.520 --> 01:16:02.520]   So some, so in my final, uh, let's see.
[01:16:02.520 --> 01:16:10.320]   So my final ensemble had these 38 models and some of these were pretty big.
[01:16:11.080 --> 01:16:14.760]   So some were doing inference on the Kaggle GPUs.
[01:16:14.760 --> 01:16:16.640]   I had to use smaller batch sizes for those.
[01:16:16.640 --> 01:16:19.040]   And it's quite hard because Kaggle doesn't give you much
[01:16:19.040 --> 01:16:20.480]   feedback when there's an error.
[01:16:20.480 --> 01:16:27.240]   So, so I know, so I know it's a real struggle.
[01:16:27.240 --> 01:16:31.200]   You, you, you know, your submission notebook throwing an error and you have no idea why.
[01:16:31.200 --> 01:16:37.080]   So you have to backtrack and see what you, um, see what, see what changed.
[01:16:37.080 --> 01:16:38.480]   So I had to order these models.
[01:16:38.480 --> 01:16:40.040]   Some of these were pretty big models.
[01:16:40.040 --> 01:16:43.880]   So I think I would know if I added this model and I was doing an error is like,
[01:16:43.880 --> 01:16:45.520]   well, maybe I'm running out of memory.
[01:16:45.520 --> 01:16:47.400]   So I'd have to use a smaller batch size for that.
[01:16:47.400 --> 01:16:53.560]   Um, so this, this competition was a bit strange because, um,
[01:16:53.560 --> 01:16:55.520]   the, the test set was very small.
[01:16:55.520 --> 01:17:01.720]   So for, for say one of these models, you could run, you could produce all
[01:17:01.720 --> 01:17:03.640]   your predictions in about 10 minutes.
[01:17:03.640 --> 01:17:07.800]   But this competition, they gave you three hours of GPU time.
[01:17:07.800 --> 01:17:09.800]   And I was like, okay, we've got three hours.
[01:17:10.240 --> 01:17:13.760]   I may as well throw as many models at it as I can in those three hours.
[01:17:13.760 --> 01:17:19.960]   So that's how I got into this, uh, this 38 model ensemble, which
[01:17:19.960 --> 01:17:26.160]   sounds a bit crazy when you say, you know, if you were working in, in a, in, in my
[01:17:26.160 --> 01:17:30.680]   day job, if I showed my boss a 38 model ensemble to predict something, he'd throw
[01:17:30.680 --> 01:17:35.560]   me out, but this, this is the world of competitions, right?
[01:17:36.400 --> 01:17:42.840]   Um, so, and actually the GPU utilization during the submission was actually quite
[01:17:42.840 --> 01:17:47.240]   low, most of that time is actually loading models because these models are huge.
[01:17:47.240 --> 01:17:48.640]   These are like gigabytes, right?
[01:17:48.640 --> 01:17:52.080]   I think all of these models all together was something like 170,
[01:17:52.080 --> 01:17:54.080]   180 gigabytes of model weights.
[01:17:54.080 --> 01:17:59.440]   Um, so most of that, you mentioned the trick, uh, as well in the discussion,
[01:17:59.440 --> 01:18:00.560]   maybe you could highlight that.
[01:18:00.560 --> 01:18:01.720]   How did you fit these in?
[01:18:01.720 --> 01:18:03.800]   Because Kaggle has a 20 GB limit.
[01:18:03.800 --> 01:18:04.480]   Yeah.
[01:18:04.480 --> 01:18:04.760]   Yeah.
[01:18:05.880 --> 01:18:06.240]   Actually.
[01:18:06.240 --> 01:18:10.760]   So I think I've still got, still got it here.
[01:18:10.760 --> 01:18:13.520]   So I'll show you, I'll show you what I did.
[01:18:13.520 --> 01:18:20.720]   So, so Kaggle has, um, yeah, a private quota of a hundred gigabytes.
[01:18:20.720 --> 01:18:33.600]   So, um, so that, that, that was, but, um, a hundred gigabytes of models only used
[01:18:33.600 --> 01:18:37.160]   probably an hour and a half of submission time, so I needed to get another
[01:18:37.160 --> 01:18:38.760]   hundred gigabytes in there somehow.
[01:18:38.760 --> 01:18:46.640]   So there's a trick, there's a trick that, um, someone, I think I've forgotten
[01:18:46.640 --> 01:18:50.040]   their name, I'll, I'll have to go find it, but they, they shared on the, on the
[01:18:50.040 --> 01:18:58.160]   discussion board, there's a trick that if you, if you make a notebook, uh, if you
[01:18:58.160 --> 01:19:04.080]   make a notebook and then you save something in the Kaggle.output folder of
[01:19:04.080 --> 01:19:08.880]   your notebook, um, those things just stay there and it doesn't count
[01:19:08.880 --> 01:19:10.440]   towards your storage quota.
[01:19:10.440 --> 01:19:17.760]   So, um, so I, what I was doing is I was uploading my models to a Google cloud
[01:19:17.760 --> 01:19:22.800]   bucket, and then I had this little notebook where I would pull stuff from my
[01:19:22.800 --> 01:19:27.600]   Google cloud bucket and then save them in the, um, Kaggle.working folder.
[01:19:27.600 --> 01:19:29.760]   And then I would save that notebook.
[01:19:29.760 --> 01:19:34.400]   And I've got, if I go back here, I've got so many of these notebooks just
[01:19:34.400 --> 01:19:40.080]   full of, you see all of these, um, timestamps, that that was what I was
[01:19:40.080 --> 01:19:40.720]   showing before.
[01:19:40.720 --> 01:19:44.440]   So all of these, all of these models have different model weights and you can
[01:19:44.440 --> 01:19:48.920]   attach that to your notebook as a dataset and it doesn't count to your quota.
[01:19:48.920 --> 01:19:52.600]   So that's how I managed to get up to the three hour runtime limit.
[01:19:52.920 --> 01:19:57.280]   Um, just by doing this, maybe, maybe Kaggle will catch on to this trick one day.
[01:19:57.280 --> 01:20:00.280]   I was just going to say, if we get a limit, we know who caused it.
[01:20:00.280 --> 01:20:03.720]   I probably should delete some of these now.
[01:20:03.720 --> 01:20:07.760]   But yeah, that's, I love it.
[01:20:07.760 --> 01:20:08.480]   That's incredible.
[01:20:08.480 --> 01:20:12.440]   I'm sure many people would just be like scratching their head if they found this
[01:20:12.440 --> 01:20:12.920]   out now.
[01:20:12.920 --> 01:20:18.440]   So yeah, it was a real concern for me because, um, at that time, if you were in
[01:20:18.440 --> 01:20:22.480]   a team, you had an advantage because you could use hundred plus hundred and you'd
[01:20:22.480 --> 01:20:23.520]   have more storage.
[01:20:23.520 --> 01:20:29.280]   So, but luckily I found this trick that someone kindly shared on the, on the
[01:20:29.280 --> 01:20:31.800]   discussion board and it worked out well for me.
[01:20:31.800 --> 01:20:35.680]   Um, but yeah, it was, it was, it was interesting.
[01:20:35.680 --> 01:20:41.400]   So, so let me see.
[01:20:41.400 --> 01:20:45.640]   I think we've understood so far, how did you do the feature engineering, those two
[01:20:45.640 --> 01:20:50.920]   features that we added the models, uh, what's left, I believe was the ensemble
[01:20:50.920 --> 01:20:51.240]   part.
[01:20:51.240 --> 01:20:52.200]   Ensemble part.
[01:20:52.200 --> 01:20:52.640]   Yeah.
[01:20:52.640 --> 01:20:59.120]   So because, uh, usually when you ensemble, so this is what I did.
[01:20:59.120 --> 01:21:07.960]   So for every, every model that, um, I had, I would generate a file called oops.
[01:21:07.960 --> 01:21:11.680]   So oops stands for out of fold predictions.
[01:21:12.640 --> 01:21:17.520]   So it's really important that when you're running your model in inference mode.
[01:21:17.520 --> 01:21:22.400]   So if you train on four folds and then you predict on the fifth fold, you save
[01:21:22.400 --> 01:21:28.680]   those out of fold predictions and you can, if we, maybe I'll go somewhere else.
[01:21:28.680 --> 01:21:31.960]   It'd be easier to show here.
[01:21:31.960 --> 01:21:34.280]   Here you go.
[01:21:34.280 --> 01:21:35.640]   Here's some more data sets.
[01:21:35.640 --> 01:21:39.560]   So I would save these predictions.
[01:21:39.560 --> 01:21:41.280]   So I concatenated that.
[01:21:41.320 --> 01:21:43.560]   So this is the actual train CSV data.
[01:21:43.560 --> 01:21:47.240]   And then I've added these two columns called fold and prediction.
[01:21:47.240 --> 01:21:51.600]   So these are my out of fold predictions and these are really useful.
[01:21:51.600 --> 01:21:57.800]   So if you're ensembling models, you would use these predictions to train some sort
[01:21:57.800 --> 01:22:01.400]   of a other model to predict your final answer.
[01:22:01.400 --> 01:22:09.440]   Um, so I used all of these out of four predictions, but the tricky bit was, so
[01:22:09.440 --> 01:22:12.760]   normally when you do this, you're supposed to use the same validation scheme.
[01:22:12.760 --> 01:22:18.480]   So you keep your folds the same, but the thing I was worried about because I was
[01:22:18.480 --> 01:22:21.800]   changing my random seed, all of these folds are slightly different.
[01:22:21.800 --> 01:22:23.320]   So I was really scared about leakage.
[01:22:23.320 --> 01:22:30.400]   Um, so the only, the cross validation scheme that I lent towards was something
[01:22:30.400 --> 01:22:32.520]   called leave one out cross validation.
[01:22:32.520 --> 01:22:37.880]   So because there's a small data set, only 2,800 samples, you could do leave one
[01:22:37.880 --> 01:22:42.840]   out cross validation to try to fit build, um, on ensemble model.
[01:22:42.840 --> 01:22:46.880]   So I used the other thing I really like using for
[01:22:46.880 --> 01:22:48.840]   ensembling is, um, linear models.
[01:22:48.840 --> 01:22:51.040]   You just use a linear model.
[01:22:51.040 --> 01:22:53.000]   It's really hard to overfit.
[01:22:53.000 --> 01:22:57.280]   Um, you know, if you use one thing, lots of people like using something like
[01:22:57.280 --> 01:22:59.840]   XG boost and something, something like that for the ensemble.
[01:22:59.840 --> 01:23:05.560]   Um, I, I, I've never got it to work, but I'm not very good at it on something.
[01:23:05.560 --> 01:23:08.640]   So that was one of the reasons I got into this competition because I wanted
[01:23:08.640 --> 01:23:10.000]   to learn how to ensemble as well.
[01:23:10.000 --> 01:23:16.200]   Um, so I used this, this, um, scikit-learn class, so called Ridge CV.
[01:23:16.200 --> 01:23:21.840]   And it, it, by default, it does this efficient leave one out cross validation.
[01:23:21.840 --> 01:23:25.600]   And I don't know what, if you look in the code, it's like magic is really deep
[01:23:25.600 --> 01:23:29.520]   matrix maps, but it does leave one out cross validation super fast.
[01:23:29.520 --> 01:23:31.720]   Um, so that's what I did.
[01:23:31.720 --> 01:23:37.040]   I use, uh, I had all of my models, so I took all of these models
[01:23:37.040 --> 01:23:38.640]   that I trained in this spreadsheet.
[01:23:38.640 --> 01:23:46.320]   And then I ran, um, the Ridge CV on there and then Ridge CV gives you a little,
[01:23:46.320 --> 01:23:51.840]   um, best score, which is actually mean squared error, which is exactly what we
[01:23:51.840 --> 01:23:53.240]   were, we wanted to predict.
[01:23:53.240 --> 01:23:58.920]   Um, and then what I did is let's say I had 300 models, whatever.
[01:23:59.160 --> 01:24:06.520]   Um, and then I would try, run it again with every combination of 299 models and
[01:24:06.520 --> 01:24:11.160]   see which more, if I removed one model, which one would improve my score the best.
[01:24:11.160 --> 01:24:19.240]   So I would iteratively keep removing models to, uh, whichever model was the worst.
[01:24:19.240 --> 01:24:28.400]   So, um, keep pruning models until I got down to, uh, and then, um, I got down,
[01:24:28.480 --> 01:24:30.000]   let me see if I can get the notebook.
[01:24:30.000 --> 01:24:34.000]   So let's see.
[01:24:34.000 --> 01:24:41.840]   Um, it might be easier to show you notebooks stacking.
[01:24:41.840 --> 01:24:48.400]   So here's all my models up here.
[01:24:48.400 --> 01:24:50.920]   All of these checkpoints.
[01:24:50.920 --> 01:24:53.240]   I'm just trying to absorb the checkpoints.
[01:24:53.240 --> 01:24:53.760]   Yeah.
[01:24:53.760 --> 01:24:55.080]   There's so, there's so many.
[01:24:55.080 --> 01:24:56.800]   Is this big enough for you?
[01:24:56.800 --> 01:24:57.680]   Should I make it bigger?
[01:24:58.680 --> 01:24:59.440]   This is fine.
[01:24:59.440 --> 01:25:00.040]   Yeah.
[01:25:00.040 --> 01:25:01.320]   Let me get a little bit bigger.
[01:25:01.320 --> 01:25:05.280]   Um, so what I would do.
[01:25:05.280 --> 01:25:08.680]   Here you go.
[01:25:08.680 --> 01:25:13.360]   So I'd start off with all of the models and calculate a CV score.
[01:25:13.360 --> 01:25:19.680]   And then I keep, keep removing models, whichever one improved my CV score the
[01:25:19.680 --> 01:25:23.800]   most, and you can see, you can see like, um, the RMSE and this column keeps
[01:25:23.800 --> 01:25:25.680]   dropping, dropping, dropping.
[01:25:25.920 --> 01:25:28.320]   And then this is the size of all of the models together.
[01:25:28.320 --> 01:25:34.280]   And obviously I can't submit a model with one terabyte on Cado, not in three hours
[01:25:34.280 --> 01:25:34.800]   runtime.
[01:25:34.800 --> 01:25:41.720]   So I would keep going down until I found that I could probably run around about
[01:25:41.720 --> 01:25:47.160]   170 or 180 gigabytes worth of model weights in the three hour runtime.
[01:25:47.160 --> 01:25:51.480]   So that was, this was the ensemble that I would use.
[01:25:51.480 --> 01:25:57.080]   So there'll be about 38 models left in the bucket and I would use that.
[01:25:57.080 --> 01:26:00.800]   And so that was my, um, final ensemble.
[01:26:00.800 --> 01:26:04.760]   So I found that that worked better than sort of starting with your best model
[01:26:04.760 --> 01:26:06.080]   and keep adding good ones.
[01:26:06.080 --> 01:26:14.240]   Um, because I've, I've thought that, um, you know, you want to get an ensemble
[01:26:14.240 --> 01:26:17.840]   of models that work well together, not, not lots of strong models.
[01:26:18.400 --> 01:26:24.240]   So there was a, there was something I found digging around, uh, where is it?
[01:26:24.240 --> 01:26:31.040]   There is this, this really interesting, um, approach.
[01:26:31.040 --> 01:26:34.960]   So this is from the Netflix prize, uh, in 2009.
[01:26:34.960 --> 01:26:38.360]   So there's a Michael Jara, he's still active on Cado, I think.
[01:26:38.360 --> 01:26:43.120]   And they've got a really interesting ensemble approach where they just keep
[01:26:43.320 --> 01:26:51.120]   adding models, but not to make better individual models, make a good overall
[01:26:51.120 --> 01:26:51.800]   ensemble.
[01:26:51.800 --> 01:26:55.720]   Um, so usually when you're on something Kaggle, you take your
[01:26:55.720 --> 01:26:57.400]   best models and average them.
[01:26:57.400 --> 01:27:01.880]   This, this approach is slightly different because, you know, if you look at the
[01:27:01.880 --> 01:27:05.960]   actual cross validation scores with some of these models, you know, this model
[01:27:05.960 --> 01:27:08.880]   here, this Albert large is terrible.
[01:27:08.880 --> 01:27:12.440]   You know, if I submitted that on the leaderboard, it would be terrible.
[01:27:12.480 --> 01:27:17.360]   But when combined with the other models, there was some sort of self-correction
[01:27:17.360 --> 01:27:17.800]   going on.
[01:27:17.800 --> 01:27:22.280]   So there was some, some strength that this was bringing that would correct the
[01:27:22.280 --> 01:27:24.960]   errors of the other models, which was interesting for me.
[01:27:24.960 --> 01:27:28.520]   That was the first time I've done that ensemble in that way.
[01:27:28.520 --> 01:27:32.520]   Isn't this also the base of a lot of boosted algorithms?
[01:27:32.520 --> 01:27:35.480]   Isn't that how they also combine models?
[01:27:35.480 --> 01:27:36.680]   I might be wrong on this.
[01:27:36.680 --> 01:27:37.520]   Yeah.
[01:27:37.520 --> 01:27:37.840]   Yeah.
[01:27:37.840 --> 01:27:42.120]   So, uh, yeah, there's, it's, it's really interesting.
[01:27:42.120 --> 01:27:46.720]   I definitely recommend having a look at this, um, this big chaos solution
[01:27:46.720 --> 01:27:47.880]   to the Netflix grand prize.
[01:27:47.880 --> 01:27:52.160]   This is in 2009, but it's really interesting read.
[01:27:52.160 --> 01:27:58.160]   Um, so actually they, I actually did use, um, the ensemble model.
[01:27:58.160 --> 01:28:05.360]   So, um, I found a implementation of that where you actually use your cross
[01:28:05.360 --> 01:28:08.440]   validation scores to estimate the weights as well.
[01:28:09.160 --> 01:28:12.440]   And that one had even better private leaderboard score, but I wasn't so
[01:28:12.440 --> 01:28:15.720]   confident in the cross validation score, which is why I didn't select
[01:28:15.720 --> 01:28:16.960]   that as my final submission.
[01:28:16.960 --> 01:28:22.760]   Um, I was more confident in the CV score from, from this, which is what,
[01:28:22.760 --> 01:28:25.200]   what, what, what my final submission was in the end.
[01:28:25.200 --> 01:28:27.640]   Gotcha.
[01:28:27.640 --> 01:28:32.040]   So that, that is the complete picture of the money.
[01:28:32.040 --> 01:28:33.040]   Yeah.
[01:28:33.040 --> 01:28:35.440]   Money ball, if I may.
[01:28:35.440 --> 01:28:36.280]   Yeah.
[01:28:36.680 --> 01:28:41.480]   And I think normally in, you know, in, in other competitions, you know, say
[01:28:41.480 --> 01:28:46.840]   like some of the image competitions, it takes too long to train 38 models.
[01:28:46.840 --> 01:28:51.600]   So you don't have that many to ensemble, but in this, this competition, I was
[01:28:51.600 --> 01:28:55.640]   able to train that many models because the data set was small, which is, uh,
[01:28:55.640 --> 01:28:59.760]   probably one of the reasons I helped me get a solo gold, if it was, if it was
[01:28:59.760 --> 01:29:04.480]   a bigger data set, um, and if the models took longer to train, then I would, you
[01:29:04.480 --> 01:29:09.280]   would probably struggle getting, um, into the solo it there's got, in fact, most,
[01:29:09.280 --> 01:29:13.080]   a lot of the top positions were solo as well, which is unusual these days for Kaggle.
[01:29:13.080 --> 01:29:14.280]   So, yeah.
[01:29:14.280 --> 01:29:16.160]   That's awesome.
[01:29:16.160 --> 01:29:20.320]   I really wanted to dive into this because like, uh, I don't think at least in
[01:29:20.320 --> 01:29:24.160]   chai time data science, I wasn't able to understand the integrity details.
[01:29:24.160 --> 01:29:28.640]   Like we just found out one trick and also, uh, at least as a community, we
[01:29:28.640 --> 01:29:33.040]   were able to understand how did you pick that ensemble because ensemble just
[01:29:33.040 --> 01:29:36.600]   sounds like you're just averaging all of these CSV files, but no, there's more to
[01:29:36.600 --> 01:29:36.840]   that.
[01:29:36.840 --> 01:29:39.320]   There's like a method to the madness in a way.
[01:29:39.320 --> 01:29:41.200]   And we got to learn all of that.
[01:29:41.200 --> 01:29:42.000]   Yeah.
[01:29:42.000 --> 01:29:45.160]   It's a, it has to be guided by cross-validation.
[01:29:45.160 --> 01:29:49.920]   It's so easy to find, um, have any sort of leakage that goes on.
[01:29:49.920 --> 01:29:53.840]   Otherwise, um, you know, you don't, without cross-validation you're blind.
[01:29:53.840 --> 01:29:56.640]   Absolutely.
[01:29:56.640 --> 01:29:57.400]   All right.
[01:29:57.400 --> 01:30:00.000]   I think, uh, we can move on to the community.
[01:30:00.000 --> 01:30:02.000]   Amy, I'll just take a peek.
[01:30:02.000 --> 01:30:03.320]   I don't see any questions.
[01:30:03.320 --> 01:30:07.600]   Um, not in the zoom chat answer.
[01:30:07.600 --> 01:30:11.360]   Uh, please feel free to ask any questions I'd left over the last 15 minutes.
[01:30:11.360 --> 01:30:16.600]   Uh, we might go over the little, but just for community asking questions.
[01:30:16.600 --> 01:30:20.720]   Uh, awesome.
[01:30:20.720 --> 01:30:23.120]   I could show you some other stuff.
[01:30:23.120 --> 01:30:26.240]   I, um, about, um, weights and biases.
[01:30:26.240 --> 01:30:30.520]   There was one thing that I've been using weights and biases for since March now.
[01:30:30.520 --> 01:30:34.720]   So there's tons of stuff I don't know about it, but one, one thing that
[01:30:34.720 --> 01:30:37.280]   worked really well for me is this thing.
[01:30:37.280 --> 01:30:41.440]   I don't know if you've seen this before, but, um, when you do a run on
[01:30:41.440 --> 01:30:45.120]   weights and biases, it, it stores the git state.
[01:30:45.120 --> 01:30:49.480]   So when you're doing like loads of experiments and you change something,
[01:30:49.480 --> 01:30:52.960]   even if it's something that you've not committed, it stores that.
[01:30:52.960 --> 01:30:57.560]   So if you click on this, which I, this blows my mind, but it's actually,
[01:30:57.800 --> 01:31:02.200]   uh, a branch of what your code looked like at the time, even if you never
[01:31:02.200 --> 01:31:04.800]   committed it, so if you've changed something, it's like, Oh, that worked.
[01:31:04.800 --> 01:31:07.280]   I've forgotten what it was because I forgot to write it down.
[01:31:07.280 --> 01:31:09.080]   That was pretty amazing.
[01:31:09.080 --> 01:31:10.560]   So that's really cool.
[01:31:10.560 --> 01:31:12.920]   I like, I really enjoy that about weights and biases.
[01:31:12.920 --> 01:31:17.120]   Um, I'm just like finding out after joining, but like there's so
[01:31:17.120 --> 01:31:20.320]   many details to all of the things.
[01:31:20.320 --> 01:31:20.640]   Yeah.
[01:31:20.640 --> 01:31:21.520]   Yeah.
[01:31:21.520 --> 01:31:24.120]   There's like, I'm still, I'm still learning all of this stuff.
[01:31:24.160 --> 01:31:29.840]   Um, yeah, I need to spend more time really getting into like lots of these
[01:31:29.840 --> 01:31:33.240]   sort of features, uh, you know, it's, it's great.
[01:31:33.240 --> 01:31:34.040]   I really enjoy it.
[01:31:34.040 --> 01:31:39.800]   The other thing is like, um, I used to use TensorBoard a lot and you know, when
[01:31:39.800 --> 01:31:43.800]   you like, you get addicted to it and you're watching your loss curves down,
[01:31:43.800 --> 01:31:47.480]   going down and then you're like, Oh God, it's one o'clock at night.
[01:31:47.480 --> 01:31:48.280]   I've got work tomorrow.
[01:31:48.280 --> 01:31:49.440]   What am I doing with my life?
[01:31:49.440 --> 01:31:50.680]   I'm just watching lost curves.
[01:31:52.840 --> 01:31:55.720]   Oh, but like weights and biases is great because you know, if you're on a train or
[01:31:55.720 --> 01:31:57.680]   something going to work, you can just check on your phone.
[01:31:57.680 --> 01:31:59.160]   It's like, yeah, my model's still going.
[01:31:59.160 --> 01:32:02.000]   I really enjoy that aspect of it.
[01:32:02.000 --> 01:32:04.200]   That's lovely to hear.
[01:32:04.200 --> 01:32:06.000]   Thanks for the, thanks for the shout out.
[01:32:06.000 --> 01:32:07.920]   Um, awesome.
[01:32:07.920 --> 01:32:11.120]   So I don't see any question, but I'll, I'll squeeze in mine seamlessly
[01:32:11.120 --> 01:32:13.600]   because I'm hosting the event.
[01:32:13.600 --> 01:32:14.760]   I'm just curious.
[01:32:14.760 --> 01:32:18.920]   This is a random series of questions that I always end up asking towards the end.
[01:32:18.960 --> 01:32:24.480]   Uh, what's your favorite competition of, uh, one that you've completed in and
[01:32:24.480 --> 01:32:28.000]   one that you just like reading or studying about on Kaggle?
[01:32:28.000 --> 01:32:30.800]   Uh, yeah, I really like, I stopped sharing.
[01:32:30.800 --> 01:32:34.480]   I really liked the, uh, predicting molecular properties one.
[01:32:34.480 --> 01:32:35.480]   That was really cool.
[01:32:35.480 --> 01:32:39.960]   Um, you know, because a lot of the final submissions, final solutions,
[01:32:39.960 --> 01:32:41.000]   they're all very different.
[01:32:41.000 --> 01:32:46.920]   Um, and also that sort of really studying something and getting
[01:32:46.920 --> 01:32:51.040]   creative with how you generate features and stuff like that, rather than just
[01:32:51.040 --> 01:32:54.200]   brute forcing it by throwing it deep, deep neural nets and stuff.
[01:32:54.200 --> 01:32:56.640]   Um, I really enjoyed that competition.
[01:32:56.640 --> 01:32:57.320]   Yeah.
[01:32:57.320 --> 01:32:59.360]   And I learned a lot from that about, you know, things like
[01:32:59.360 --> 01:33:00.560]   chemistry and stuff like that.
[01:33:00.560 --> 01:33:02.360]   Awesome.
[01:33:02.360 --> 01:33:08.400]   Uh, do you have a favorite algorithm for any set of problem that you always start with?
[01:33:08.400 --> 01:33:10.720]   Uh, favorite algorithm?
[01:33:10.720 --> 01:33:11.960]   Not really.
[01:33:11.960 --> 01:33:15.720]   Um, I like whatever one works the best, really.
[01:33:15.920 --> 01:33:20.160]   I, I I'm, I'm a massive advocate for just the using linear models.
[01:33:20.160 --> 01:33:25.640]   Whenever, whenever I start a project, I'd always start with either logistic
[01:33:25.640 --> 01:33:31.000]   regression or linear regression, mostly because it helps you understand, you know,
[01:33:31.000 --> 01:33:36.120]   first of all, you got a baseline model and they really explainable as well.
[01:33:36.120 --> 01:33:36.800]   And it's really hard.
[01:33:36.800 --> 01:33:40.760]   It's really easy to show that to someone and they'll understand it straight away.
[01:33:40.760 --> 01:33:43.040]   So I'm a big fan of linear models.
[01:33:45.400 --> 01:33:48.120]   That's, that's like really hard to implement in practice also.
[01:33:48.120 --> 01:33:52.280]   Like you're always so tempted to, I would just be tempted to try Roberta.
[01:33:52.280 --> 01:33:56.160]   Now that you've mentioned it both, whenever I think of NLP, Roberta,
[01:33:56.160 --> 01:33:57.240]   that's what I would want.
[01:33:57.240 --> 01:34:00.520]   Okay.
[01:34:00.520 --> 01:34:01.720]   Uh, this is a hard one.
[01:34:01.720 --> 01:34:03.240]   Some people find it hard.
[01:34:03.240 --> 01:34:05.040]   Uh, do you have a favorite game?
[01:34:05.040 --> 01:34:06.200]   Uh, you can pick two.
[01:34:06.200 --> 01:34:09.440]   So one of all time and one from the current set that you're playing.
[01:34:09.440 --> 01:34:13.600]   It's been a while since I actually played computer games, but I used to,
[01:34:14.000 --> 01:34:16.320]   I used to like all the sort of racing games and stuff.
[01:34:16.320 --> 01:34:22.160]   Um, and I really used to enjoy, I really enjoyed like the final fantasy games.
[01:34:22.160 --> 01:34:26.560]   So final fantasy seven been meaning to get the remake of it, but that never got
[01:34:26.560 --> 01:34:29.720]   around to it and never had time to get around to it, but it looks amazing.
[01:34:29.720 --> 01:34:30.200]   The remake.
[01:34:30.200 --> 01:34:33.240]   Diva mentioned the same one.
[01:34:33.240 --> 01:34:36.640]   Awesome.
[01:34:36.640 --> 01:34:37.760]   Last question.
[01:34:37.760 --> 01:34:40.880]   Uh, any special trick that you could share with us?
[01:34:41.280 --> 01:34:44.480]   Any secret to Kaggle that you've picked up over the years?
[01:34:44.480 --> 01:34:47.160]   You can say, no, you can keep the secret.
[01:34:47.160 --> 01:34:50.360]   I can't give you all the secrets.
[01:34:50.360 --> 01:34:50.840]   Can I?
[01:34:50.840 --> 01:34:57.720]   No, I think, I think the thing that's worked best for me is just the, just the
[01:34:57.720 --> 01:35:02.480]   mantra of test as many ideas in the shortest amount of time that, um, if we,
[01:35:02.480 --> 01:35:07.400]   if you can do that, if you write your code so that you're able to do that, you'll
[01:35:07.400 --> 01:35:12.560]   start to see your leaderboard, um, position really improve is, is, is really
[01:35:12.560 --> 01:35:15.920]   powerful that approach of approaching machine learning.
[01:35:15.920 --> 01:35:21.080]   That's again, I want to point out that's incredibly hard to implement in.
[01:35:21.080 --> 01:35:27.920]   Before we end, I would quickly point out Andrew's profile on Kaggle.
[01:35:27.920 --> 01:35:28.720]   It's data source.
[01:35:28.720 --> 01:35:31.600]   You can find him there and you can also find the reason why it's called.
[01:35:31.600 --> 01:35:36.720]   So if you go over to this kernel, you can see the plot and that's how his profile
[01:35:36.720 --> 01:35:37.600]   picture came about.
[01:35:37.600 --> 01:35:42.760]   He's also on LinkedIn and he's on Twitter as well.
[01:35:42.760 --> 01:35:46.160]   Simply add anything else that I missed.
[01:35:46.160 --> 01:35:47.560]   No, no, that's it.
[01:35:47.560 --> 01:35:48.920]   This has been super fun.
[01:35:48.920 --> 01:35:50.440]   Um, I really enjoyed it.
[01:35:50.440 --> 01:35:52.040]   Thank you so much for having me on.
[01:35:52.040 --> 01:35:53.560]   Likewise.
[01:35:53.560 --> 01:35:54.320]   Thanks so much.
[01:35:54.320 --> 01:35:55.720]   I see one question.
[01:35:55.720 --> 01:35:58.440]   So let's try to squeeze that in a real quick.
[01:35:58.440 --> 01:36:01.800]   How do you approach ED and data visualization in NLP?
[01:36:01.800 --> 01:36:03.440]   Uh, how do you break down the task?
[01:36:03.440 --> 01:36:05.440]   Oh, how do I do it?
[01:36:05.480 --> 01:36:07.120]   I look at someone else's notebook.
[01:36:07.120 --> 01:36:11.080]   No, yeah.
[01:36:11.080 --> 01:36:14.520]   EDA with NLP is really tough.
[01:36:14.520 --> 01:36:18.920]   Uh, you know, it's not, it's, it's not like you can plot one feature against the
[01:36:18.920 --> 01:36:19.120]   other.
[01:36:19.120 --> 01:36:22.080]   You have to generate those features and then plot them against each other.
[01:36:22.080 --> 01:36:27.000]   Um, so yeah, I, I'd recommend having a look at some, some of the other notebooks
[01:36:27.000 --> 01:36:32.200]   in this competition because people, people much better than me at EDA have done some
[01:36:32.200 --> 01:36:33.800]   really beautiful plots.
[01:36:33.800 --> 01:36:38.920]   But I think the, the common theme is you're going to have to take the text and
[01:36:38.920 --> 01:36:43.560]   then generate features from them and then try to do your analysis on those features,
[01:36:43.560 --> 01:36:45.760]   which is kind of a two step process.
[01:36:45.760 --> 01:36:49.680]   It's not like you start with a CSV file full of numbers, which you can start
[01:36:49.680 --> 01:36:50.120]   plotting.
[01:36:50.120 --> 01:36:53.120]   So there's that additional step, but it's tricky.
[01:36:53.120 --> 01:36:57.920]   I understand really understanding NLP data and exploring it is tough.
[01:36:57.920 --> 01:37:00.880]   And sometimes the best way is just to sit there and read it.
[01:37:03.320 --> 01:37:07.200]   I'm looking at the chai competition and I really wish like I had someone, I can
[01:37:07.200 --> 01:37:11.240]   speak Hindi, but I can't speak the other language Tamil and I'm just trying to
[01:37:11.240 --> 01:37:12.360]   find a teammate who can.
[01:37:12.360 --> 01:37:15.000]   So yeah, I echo with you there.
[01:37:15.000 --> 01:37:17.480]   Yeah.
[01:37:17.480 --> 01:37:19.200]   That one looks like a tough competition.
[01:37:19.200 --> 01:37:22.360]   I mean, maybe I'll jump into it, but yeah, it sounds interesting.
[01:37:22.360 --> 01:37:24.360]   We all will see on the leaderboard.
[01:37:24.360 --> 01:37:25.800]   I'll probably be at the bottom.
[01:37:25.800 --> 01:37:29.760]   You'll be on the top, but I look forward to seeing you there.
[01:37:30.560 --> 01:37:34.280]   I don't want to go much over the time since we've already gone overboard, but
[01:37:34.280 --> 01:37:36.240]   Anjum, I just wanted to say thank you so much.
[01:37:36.240 --> 01:37:39.040]   And it was really an honor learning about your solution.
[01:37:39.040 --> 01:37:40.600]   And thanks for sharing your journey with us.
[01:37:40.600 --> 01:37:41.480]   And I thank you.
[01:37:41.480 --> 01:37:42.200]   It's been an honor.
[01:37:42.200 --> 01:37:42.720]   Thanks.
[01:37:42.720 --> 01:37:43.600]   Thanks very much.
[01:37:43.600 --> 01:37:46.660]   [LAUGHS]

