
[00:00:00.000 --> 00:00:03.600]   And you were telling me that it can produce like this results, right?
[00:00:03.600 --> 00:00:07.880]   Without, without even, there's no humans in the low data set.
[00:00:07.880 --> 00:00:14.520]   It learned to learn to do that just by looking at pictures of tableware or like
[00:00:14.520 --> 00:00:18.680]   gyms or swimming pools or bowling places.
[00:00:18.680 --> 00:00:19.600]   That's really impressive.
[00:00:19.600 --> 00:00:25.280]   Hey, what is going on everybody?
[00:00:25.280 --> 00:00:26.240]   It's Ivan here.
[00:00:26.240 --> 00:00:29.880]   And in this video, we'll be taking a look at a really, really cool way to do low
[00:00:29.880 --> 00:00:31.240]   light enhancement of images.
[00:00:31.240 --> 00:00:35.760]   AKA we'll take really, really dark images where like you can barely make out any
[00:00:35.760 --> 00:00:40.960]   detail and then like enhance them using deep learning in a way that it'll look as
[00:00:40.960 --> 00:00:43.000]   if they were taking in the daylight.
[00:00:43.000 --> 00:00:47.480]   And they'll go from like being like really, really dark to being like pretty
[00:00:47.480 --> 00:00:50.640]   okay, where we can like actually make out details and looking good.
[00:00:50.640 --> 00:00:54.120]   And that's not the only cool thing about this video though.
[00:00:54.120 --> 00:00:58.040]   The, the other cool thing about this is, is how we're going to be doing it.
[00:00:58.080 --> 00:01:00.320]   And we'll use a really interesting model.
[00:01:00.320 --> 00:01:02.920]   That's a really interesting approach.
[00:01:02.920 --> 00:01:06.960]   I should say that's allowing us to do it without needing labels.
[00:01:06.960 --> 00:01:10.440]   So we're going to train the model, but we're only going to train it using the
[00:01:10.440 --> 00:01:14.960]   dark images and it's going to learn how to like enhance them, but without like
[00:01:14.960 --> 00:01:15.600]   ground truth.
[00:01:15.600 --> 00:01:18.120]   And it's going to be like unsupervised learning in this way.
[00:01:18.120 --> 00:01:22.200]   And the actual model would be only 350 kilobytes.
[00:01:22.200 --> 00:01:26.440]   And I know that sounds crazy, but we'll kind of dive into how it all works.
[00:01:26.440 --> 00:01:28.960]   And it's going to be really interesting one.
[00:01:28.960 --> 00:01:31.680]   And along the way, I'll be showing you how to train such a model.
[00:01:31.680 --> 00:01:35.440]   And we'll learn about how to use weights and biases, which is like a really
[00:01:35.440 --> 00:01:40.040]   powerful ML apps platform for keeping track of your ML experiments and Keras,
[00:01:40.040 --> 00:01:43.760]   which is like one of the most popular deep learning frameworks out there and
[00:01:43.760 --> 00:01:48.080]   kind of how to use them together to like optimize your training, machine
[00:01:48.080 --> 00:01:49.440]   learning training workflow.
[00:01:49.440 --> 00:01:52.120]   So that's also going to be like a pretty interesting part about the video.
[00:01:52.120 --> 00:01:54.480]   And yeah, I'll show you kind of how to train, how to use it.
[00:01:54.480 --> 00:01:58.360]   And we'll also have a pretty interesting conversation with a person who built
[00:01:58.360 --> 00:02:00.800]   this implementation, which we'll be looking at.
[00:02:00.800 --> 00:02:02.040]   So stick around for that.
[00:02:02.040 --> 00:02:05.280]   So if you're excited about this video, smash that like button.
[00:02:05.280 --> 00:02:07.840]   And if you have any questions, you may leave a comment.
[00:02:07.840 --> 00:02:11.080]   And if you want to see more videos like this about cool machine learning
[00:02:11.080 --> 00:02:14.800]   topics, you can subscribe to our channel also.
[00:02:14.800 --> 00:02:16.120]   And I hope you enjoyed this video.
[00:02:16.120 --> 00:02:20.440]   So the approach that we're going to be using is called zero reference, deep
[00:02:20.440 --> 00:02:24.400]   curve estimation, and right now we're looking at a Google call up notebook
[00:02:24.400 --> 00:02:26.000]   where the approach is implemented.
[00:02:26.000 --> 00:02:29.680]   So I'll just go and pretty much start writing the cells of code here.
[00:02:29.680 --> 00:02:34.520]   So we'll install a one DB, which is the weights and biases, Python client.
[00:02:34.520 --> 00:02:39.040]   Um, and then we'll import some modules and next step, we're going to be using
[00:02:39.040 --> 00:02:44.240]   WNB artifacts, which is like a tool within weights and biases for a model
[00:02:44.240 --> 00:02:45.320]   and data set versioning.
[00:02:45.320 --> 00:02:49.480]   And so like I've stored the low data set, the low light data set that they've
[00:02:49.480 --> 00:02:51.600]   called it that, um, kind of in the cloud.
[00:02:51.600 --> 00:02:54.840]   And we're going to use it to, we're going to use WNB artifacts to pretty much
[00:02:54.840 --> 00:02:58.800]   put that data set into the, uh, like virtual machine environment of this
[00:02:58.800 --> 00:02:59.960]   Google call of notebook.
[00:02:59.960 --> 00:03:02.080]   And I'll just like run the self code to do it.
[00:03:02.080 --> 00:03:09.640]   So after running this line, we can see that there's an artifacts folder
[00:03:09.640 --> 00:03:13.200]   appearing, which has our data set and we've, uh, pretty much unzipped it.
[00:03:13.200 --> 00:03:14.600]   And so it's ready to be used.
[00:03:14.600 --> 00:03:18.240]   Uh, here are some of the images from it, but that's kind of like not a really
[00:03:18.240 --> 00:03:21.160]   great way to explore the data by like, just like clicking around it.
[00:03:21.160 --> 00:03:24.640]   So next up in their code, I'm going to use WNB tables, which is a tool for,
[00:03:24.640 --> 00:03:28.320]   um, model and data set versioning to essentially help us visualize the data
[00:03:28.320 --> 00:03:30.680]   set and kind of explore it and to know what we're dealing with.
[00:03:30.680 --> 00:03:34.440]   Um, so here I'm pretty much just like defining a table with like a low light
[00:03:34.440 --> 00:03:38.920]   and the highlight images, and we're going to like log in this way, the first
[00:03:38.920 --> 00:03:41.320]   hundred training images from the low data set, and we'll be able to like
[00:03:41.320 --> 00:03:44.200]   interactively explore it, uh, in just a few moments.
[00:03:44.200 --> 00:03:47.280]   And so at this point, we also prompt you to log into your weights and biases
[00:03:47.280 --> 00:03:49.960]   account, uh, by like pasting an API key.
[00:03:50.160 --> 00:03:52.680]   And if you don't have like a weights and biases account, you can quickly
[00:03:52.680 --> 00:03:55.200]   create one and then like proceed normally with the video.
[00:03:55.200 --> 00:03:59.520]   And so now after the data has finished syncing, um, we can click on this link
[00:03:59.520 --> 00:04:03.520]   and we'll open the run page within weights and biases, uh, which now has a
[00:04:03.520 --> 00:04:06.680]   WNB table, um, with our data set.
[00:04:06.680 --> 00:04:10.960]   So now we can see like side by side, the, uh, low light and the like highlight
[00:04:10.960 --> 00:04:14.320]   images, um, we can maybe make it a bit larger like that.
[00:04:14.320 --> 00:04:15.240]   Yeah, that looks good.
[00:04:15.240 --> 00:04:19.000]   It's like, uh, here's kind of how the data set looks like at least the first,
[00:04:19.040 --> 00:04:23.200]   like the whole data says about like 400 images and it's like the first hundred
[00:04:23.200 --> 00:04:28.000]   from there, like here's a low light image and here's a, uh, uh, highlight image
[00:04:28.000 --> 00:04:28.600]   right here.
[00:04:28.600 --> 00:04:33.080]   So we can kind of maybe just scroll around, going to explore, um, how it's
[00:04:33.080 --> 00:04:35.800]   looking, check out some more images.
[00:04:35.800 --> 00:04:39.880]   So here's some, as you can see, it's kind of like, yeah, it's, it's like not a big
[00:04:39.880 --> 00:04:40.440]   data set.
[00:04:40.440 --> 00:04:44.360]   Uh, I think somebody pretty much just like took a bunch of pictures, uh, around
[00:04:44.360 --> 00:04:46.080]   their house and made it.
[00:04:46.080 --> 00:04:50.160]   So here are some low light basketball balls right here.
[00:04:50.160 --> 00:04:54.720]   Um, solid, solid data set material, uh, obviously.
[00:04:54.720 --> 00:04:58.080]   But the thing about the approach that we're going to be using in this video is
[00:04:58.080 --> 00:05:01.480]   that while we're looking at this data set, it has both the like low light and the
[00:05:01.480 --> 00:05:02.840]   like full light images.
[00:05:02.840 --> 00:05:08.200]   Um, we're actually only really interested in the low light ones, like, uh, all of
[00:05:08.200 --> 00:05:11.040]   the training and all of the low light enhancement will be done by fitting the
[00:05:11.040 --> 00:05:13.280]   model, only the low light images.
[00:05:13.600 --> 00:05:17.720]   And it's been, uh, it's been like a really, it's like a really strange way to
[00:05:17.720 --> 00:05:18.720]   approach this problem.
[00:05:18.720 --> 00:05:21.960]   Like we're going to like, you know, like we're going to learn to do all that
[00:05:21.960 --> 00:05:26.200]   enhancement, but like the model is only going to see like the dark images.
[00:05:26.200 --> 00:05:28.840]   Like how would, how would that work even really?
[00:05:28.840 --> 00:05:34.920]   Um, and, and I don't know if I have that many good ways to explain it honestly.
[00:05:34.920 --> 00:05:42.240]   Um, well actually like the person who built this implementation, Sharmik
[00:05:42.240 --> 00:05:45.400]   Rakshit is, uh, he's kind of a good buddy of mine.
[00:05:45.400 --> 00:05:48.600]   So like maybe I could like hit him up and we could talk.
[00:05:48.600 --> 00:05:49.640]   I don't know.
[00:05:49.640 --> 00:05:53.440]   Let's just, let's just see if he's like free and he's like down, down to do it.
[00:05:53.440 --> 00:05:53.840]   You know?
[00:05:53.840 --> 00:05:56.040]   Um, where's my Zoom?
[00:05:56.040 --> 00:05:56.240]   Okay.
[00:05:56.240 --> 00:05:58.000]   Let me just like start a Zoom meeting.
[00:05:58.000 --> 00:05:59.440]   Let's see how it goes.
[00:05:59.440 --> 00:06:00.360]   Um.
[00:06:00.360 --> 00:06:06.960]   Oh, hey Sharmik.
[00:06:06.960 --> 00:06:10.320]   Uh, nice to see you in this, uh, Zoom meeting with me.
[00:06:10.400 --> 00:06:15.320]   Just want to hang out in the video and chat about some low light enhancement stuff.
[00:06:15.320 --> 00:06:17.760]   Uh, hi Ivan.
[00:06:17.760 --> 00:06:19.200]   Uh, pleasure is mine.
[00:06:19.200 --> 00:06:22.880]   Uh, I'm, I'm really excited to be on this video with you.
[00:06:22.880 --> 00:06:27.800]   So I was like hoping for a long time, like we will be, we will get to do this.
[00:06:27.800 --> 00:06:29.520]   So yeah, I'm really excited.
[00:06:29.520 --> 00:06:30.840]   Awesome, man.
[00:06:30.840 --> 00:06:31.720]   Yeah, let's do it.
[00:06:31.720 --> 00:06:33.160]   All right, man.
[00:06:33.160 --> 00:06:37.200]   So can you kind of give us like a bit of a spiel of like some special things about
[00:06:37.200 --> 00:06:42.880]   like zero DC, kind of like, uh, for which tasks may somebody like find it a compelling
[00:06:42.880 --> 00:06:46.320]   model to use, you know, like what are, what are, what are some cool things about it?
[00:06:46.320 --> 00:06:48.400]   Uh, yeah.
[00:06:48.400 --> 00:06:53.120]   So the best thing about zero DC is, uh, the way the authors have formulated the
[00:06:53.120 --> 00:06:54.600]   problem of low light enhancement.
[00:06:54.600 --> 00:07:00.120]   So, uh, zero DC doesn't need any kind of feared or unpaired supervision, uh,
[00:07:00.120 --> 00:07:02.320]   needed for training, uh, the model.
[00:07:02.320 --> 00:07:05.600]   So, uh, this, this, this is very interesting.
[00:07:05.600 --> 00:07:10.160]   And the implication is basically you can just take a bunch of low light images,
[00:07:10.160 --> 00:07:12.160]   uh, that may be lying in the internet.
[00:07:12.160 --> 00:07:12.840]   You can scrape it.
[00:07:12.840 --> 00:07:17.000]   Uh, you can even take it from maybe from your favorite horror movies.
[00:07:17.000 --> 00:07:19.240]   You can take it from your favorite Zack Snyder movie.
[00:07:19.240 --> 00:07:22.840]   You can take it from your favorite Batman movie and you can just throw it at the
[00:07:22.840 --> 00:07:25.640]   model and paint it and you should get good results.
[00:07:25.640 --> 00:07:30.240]   Uh, the thing is that you don't have to go out and, you know, manually
[00:07:30.240 --> 00:07:31.520]   create ground truth levels.
[00:07:31.520 --> 00:07:35.240]   Uh, what I mean is that the corresponding enhanced images, you don't
[00:07:35.240 --> 00:07:37.840]   need to show it to zero DC, zero DC.
[00:07:37.840 --> 00:07:43.280]   And learn, uh, how to enhance the low light images by just taking a look at,
[00:07:43.280 --> 00:07:45.120]   uh, the low light images themselves.
[00:07:45.120 --> 00:07:46.960]   So it's completely unsupervised.
[00:07:46.960 --> 00:07:52.600]   And, uh, another thing about zero DC, which I really, uh, find very appealing
[00:07:52.600 --> 00:07:55.760]   is it's really lightweight and it's really fast.
[00:07:55.760 --> 00:08:00.360]   So the model, if you save the implementation of this model, you'll
[00:08:00.360 --> 00:08:03.960]   just get, I think around three 50 kilobytes, uh, which is really small.
[00:08:04.000 --> 00:08:09.920]   And, uh, and in 2022 for a deep learning model that does something with vision,
[00:08:09.920 --> 00:08:13.560]   like that blew me away when I first learned that that's like crazy.
[00:08:13.560 --> 00:08:14.760]   Yeah.
[00:08:14.760 --> 00:08:18.920]   I, and that is something, uh, how I personally, uh, found zero DC.
[00:08:18.920 --> 00:08:19.560]   So, yeah.
[00:08:19.560 --> 00:08:22.840]   Uh, uh, the implications are amazing.
[00:08:22.840 --> 00:08:27.120]   So basically since it's so lightweight, you can pair it up with other
[00:08:27.120 --> 00:08:28.280]   computer vision applications.
[00:08:28.280 --> 00:08:30.920]   Like, uh, you can perform object detection in the dark.
[00:08:31.080 --> 00:08:34.080]   You can, you can perform a semantic segmentation in the dark.
[00:08:34.080 --> 00:08:38.240]   So since it doesn't slow down the pipeline a lot, you can pair it up with
[00:08:38.240 --> 00:08:39.800]   other computer vision applications.
[00:08:39.800 --> 00:08:45.520]   And like the most, the coolest application I can think of right now is you can, uh,
[00:08:45.520 --> 00:08:48.680]   create a real time migration system on your phone.
[00:08:48.680 --> 00:08:54.320]   Uh, since this model is very lightweight, it's perfectly capable on running on a
[00:08:54.320 --> 00:08:55.840]   mobile device and edge device.
[00:08:55.840 --> 00:08:58.640]   So you can just build a real time migration system out of it.
[00:08:58.640 --> 00:08:59.520]   Very easy.
[00:09:00.000 --> 00:09:03.760]   Man, like the fact that, you know, it's 350 kilobytes and it doesn't need like
[00:09:03.760 --> 00:09:04.920]   labeled images at all.
[00:09:04.920 --> 00:09:08.840]   Like the fact that you can like really literally scrape just one kind of like
[00:09:08.840 --> 00:09:12.160]   dark images and train something on it that's runs fast.
[00:09:12.160 --> 00:09:14.360]   And again, 350 kilobytes.
[00:09:14.360 --> 00:09:16.600]   Like what sorcery is that?
[00:09:16.600 --> 00:09:19.760]   Um, that's, that's like, sounds incredible.
[00:09:19.760 --> 00:09:21.000]   So like, let's, let's talk more about it.
[00:09:21.000 --> 00:09:24.120]   Like, let's like, yeah, let's like dive into some, some, some stuff with it.
[00:09:24.120 --> 00:09:27.360]   So, all right, show me if we're looking at the WNB table that I've locked from
[00:09:27.360 --> 00:09:31.920]   running the notebook with like essentially testing images from the low data set.
[00:09:31.920 --> 00:09:36.880]   Uh, here we can like check out like the, the, like the original image, uh, and
[00:09:36.880 --> 00:09:41.840]   the image that was gotten with like an auto contrast algorithm and like the
[00:09:41.840 --> 00:09:46.880]   results with like zero, zero to see produces like, like this ones and like
[00:09:46.880 --> 00:09:48.400]   all that is, it's not a large data set, right?
[00:09:48.400 --> 00:09:49.680]   Like it's pretty, pretty small.
[00:09:49.680 --> 00:09:51.160]   Yeah.
[00:09:51.160 --> 00:09:52.880]   Uh, it's, it's a pretty small data set.
[00:09:52.920 --> 00:09:57.800]   And as you can see, like even on the small data set, the images that, uh, you, uh,
[00:09:57.800 --> 00:10:01.720]   they are there in the test set, uh, are completely exclusive from the ones in
[00:10:01.720 --> 00:10:03.080]   the training and validation set.
[00:10:03.080 --> 00:10:06.520]   So it's kind of performing quite well.
[00:10:06.520 --> 00:10:10.440]   Uh, I might even add it's a day and night difference.
[00:10:10.440 --> 00:10:11.400]   Yeah.
[00:10:11.400 --> 00:10:11.600]   Yeah.
[00:10:11.600 --> 00:10:11.680]   Yeah.
[00:10:11.680 --> 00:10:11.880]   Yeah.
[00:10:11.880 --> 00:10:16.400]   Cause like with some of the, Oh, nice.
[00:10:16.400 --> 00:10:16.760]   Okay.
[00:10:16.760 --> 00:10:19.720]   That took me, that took me a minute to catch that one.
[00:10:19.720 --> 00:10:20.240]   That was great.
[00:10:20.480 --> 00:10:20.680]   Yeah.
[00:10:20.680 --> 00:10:25.280]   Again, that's like surprisingly well for like about 400 training images, right?
[00:10:25.280 --> 00:10:28.680]   And some of them are like duplicates with different level of lighting.
[00:10:28.680 --> 00:10:33.040]   So like, uh, you know, like the, for instance, like let's take this image
[00:10:33.040 --> 00:10:34.560]   and like, let's check out like this image.
[00:10:34.560 --> 00:10:37.720]   This are, you know, yeah.
[00:10:37.720 --> 00:10:40.160]   As you were saying, night and day difference, a hundred percent.
[00:10:40.160 --> 00:10:44.160]   Um, I also went and I tried to run it on some of the, like, as we were
[00:10:44.160 --> 00:10:46.280]   talking, like kind of movie movies, images.
[00:10:46.600 --> 00:10:51.960]   So I ran it on some like Game of Thrones images and it's doing like, really like,
[00:10:51.960 --> 00:10:54.880]   this is like amazing, like this looks so cool.
[00:10:54.880 --> 00:10:56.240]   It's like, check this out.
[00:10:56.240 --> 00:10:57.280]   Like, right.
[00:10:57.280 --> 00:11:02.320]   Uh, then on this image, um, then also when I tried it, like on some other
[00:11:02.320 --> 00:11:06.360]   images and on some images, you know, this is also like a Game of Thrones image.
[00:11:06.360 --> 00:11:06.640]   Right.
[00:11:06.640 --> 00:11:09.920]   But, and this one, it's kind of doing like a weird thing with color.
[00:11:09.920 --> 00:11:13.320]   I tried it on the Batman one too, right here.
[00:11:13.320 --> 00:11:16.080]   And here it's kind of, seems to be like overexposing.
[00:11:16.080 --> 00:11:19.400]   So do you have any thoughts on why in some images does it do like a really
[00:11:19.400 --> 00:11:22.840]   amazing job and like, in some images, like here with Batman or like here with
[00:11:22.840 --> 00:11:26.560]   like Game of Thrones, like something's just off, like the color scheme is off
[00:11:26.560 --> 00:11:28.640]   or like it's way too overexposed.
[00:11:28.640 --> 00:11:30.040]   Like what do you think is going on here?
[00:11:30.040 --> 00:11:31.400]   Yeah.
[00:11:31.400 --> 00:11:35.360]   So one of the things I would like to point out in this case, uh, the Batman,
[00:11:35.360 --> 00:11:38.680]   uh, frame and, uh, one from Game of Thrones, which you are showing to be
[00:11:38.680 --> 00:11:41.240]   overexposed are actually pretty well lit scenes.
[00:11:41.240 --> 00:11:43.720]   So these are not low light scenes.
[00:11:43.760 --> 00:11:46.640]   These may be dark, dark frames, but these are not low light.
[00:11:46.640 --> 00:11:50.080]   Uh, as you can see the Batman frame, especially it's pretty well lit.
[00:11:50.080 --> 00:11:55.000]   Uh, so like, uh, it's, it's, uh, compared to the initial Jon Snow, uh,
[00:11:55.000 --> 00:11:58.640]   frame that you showed, uh, it's, it's, uh, actually pretty well lit.
[00:11:58.640 --> 00:12:02.680]   So, uh, yeah, yeah, yeah.
[00:12:02.680 --> 00:12:06.040]   So this is actually completely, uh, low light frame.
[00:12:06.040 --> 00:12:09.880]   So, so the enhancement works pretty well in this case, I would say.
[00:12:10.200 --> 00:12:15.360]   Uh, but yes, uh, the TLDR is that if you train a zero DC on a way larger data
[00:12:15.360 --> 00:12:19.640]   set, uh, of a higher quality that compared to a lot of data set, a lot of
[00:12:19.640 --> 00:12:21.240]   data set is technically a toy data set.
[00:12:21.240 --> 00:12:25.440]   It's good for prototyping, but yeah, if you, if you train, uh, the model on a
[00:12:25.440 --> 00:12:29.760]   way larger data set, you can achieve way better results, uh, compared to this one.
[00:12:29.760 --> 00:12:33.600]   And I think it's going to reduce the problem of overexposure quite a bit.
[00:12:33.600 --> 00:12:35.880]   Yeah, that's, that's really interesting.
[00:12:35.880 --> 00:12:38.720]   And like, you know, this frame, it kind of looks dark, but again, when we
[00:12:38.720 --> 00:12:41.880]   compare it to like the actual type of images that it's all while training,
[00:12:41.880 --> 00:12:42.960]   like, like this one's right.
[00:12:42.960 --> 00:12:47.480]   Like I can see how, like, say the level of light, like in this scene and the
[00:12:47.480 --> 00:12:51.360]   level of light, like in this scene are like a lot more similar than like, say
[00:12:51.360 --> 00:12:54.720]   in this scene where, you know, even when I was selecting, it kind of looked dark
[00:12:54.720 --> 00:12:58.400]   to me, but like, yeah, I guess like this is actually like a pretty well lit scene.
[00:12:58.400 --> 00:12:59.680]   And so it was kind of this one.
[00:12:59.680 --> 00:13:00.000]   Yeah.
[00:13:00.000 --> 00:13:02.840]   So, so that's the thing about this, the low light model, right?
[00:13:02.840 --> 00:13:08.320]   Like, you know, you almost as a human would not even like, I think you
[00:13:08.480 --> 00:13:11.360]   wouldn't even want to like select this image because you're like, what, what's
[00:13:11.360 --> 00:13:13.560]   in there really like, is there really any detail?
[00:13:13.560 --> 00:13:17.520]   So maybe that's why I went for like this frames because it's out like,
[00:13:17.520 --> 00:13:20.760]   well, they're kind of dark, but yeah, that's really interesting.
[00:13:20.760 --> 00:13:22.440]   And you were telling me before that like...
[00:13:22.440 --> 00:13:26.000]   And that's the thing, dark doesn't equate to low light.
[00:13:26.000 --> 00:13:28.520]   So low light is the absence of light.
[00:13:28.520 --> 00:13:34.040]   And in these kinds of movies and TV shows, especially the cinematographers
[00:13:34.080 --> 00:13:38.880]   are really playing with the level of lighting to create a certain kind of,
[00:13:38.880 --> 00:13:40.440]   you know, interesting frame.
[00:13:40.440 --> 00:13:44.760]   So I wouldn't call that, I wouldn't call these type of images as corrupted images
[00:13:44.760 --> 00:13:45.120]   at all.
[00:13:45.120 --> 00:13:45.960]   Yeah.
[00:13:45.960 --> 00:13:46.200]   Yeah.
[00:13:46.200 --> 00:13:46.480]   Yeah.
[00:13:46.480 --> 00:13:50.560]   And you were telling me that like, it can produce like this results, right?
[00:13:50.560 --> 00:13:54.880]   Without, without even, like, there's no humans in the low data set.
[00:13:54.880 --> 00:14:03.320]   Like it learned to learn to do that just by looking at pictures of like tableware
[00:14:03.320 --> 00:14:08.000]   or like gyms or swimming pools or bowling places.
[00:14:08.000 --> 00:14:09.640]   That's really impressive.
[00:14:09.640 --> 00:14:09.920]   Yeah.
[00:14:09.920 --> 00:14:11.360]   Yeah.
[00:14:11.360 --> 00:14:11.560]   Yeah.
[00:14:11.560 --> 00:14:13.560]   I would say, I would say it's pretty impressive.
[00:14:13.560 --> 00:14:17.520]   And it's, it's, it's thanks to the clever way, the problem of low light
[00:14:17.520 --> 00:14:19.960]   enhancement has been formulated in zero DC, I would say.
[00:14:19.960 --> 00:14:20.680]   Yeah.
[00:14:20.680 --> 00:14:25.040]   So we actually want to do like a little challenge for our amazing community.
[00:14:25.040 --> 00:14:29.280]   And so as we're talking like, the GLDR here is that like, if you give it more
[00:14:29.280 --> 00:14:34.520]   data, cause like right now it's got kind of the, well, the data set is like, it's
[00:14:34.520 --> 00:14:35.640]   more of like a toy data set.
[00:14:35.640 --> 00:14:40.000]   And if we can, if, if, if you want to work on this problem and you want to fit it
[00:14:40.000 --> 00:14:44.720]   like some images from like, we'll, we'll kind of leave links in the resources,
[00:14:44.720 --> 00:14:48.480]   like in the description and like in the call up notebook, kind of like other
[00:14:48.480 --> 00:14:49.920]   data set that you can use for training.
[00:14:49.920 --> 00:14:53.880]   And so if you want to get some cool results with it, like tweet about it,
[00:14:53.880 --> 00:14:57.360]   tag @WaytonBias and us on Twitter and we'll see it.
[00:14:57.360 --> 00:15:00.600]   And like, I don't know, that would be really, really cool because again, the
[00:15:00.600 --> 00:15:02.560]   model is amazing on a toy data set.
[00:15:02.560 --> 00:15:06.400]   Like imagine what it can do on like a larger data set, especially given as
[00:15:06.400 --> 00:15:09.920]   you, Shormik talked about that, like, we don't even need like labeled images.
[00:15:09.920 --> 00:15:12.320]   We just need like more low light images.
[00:15:12.320 --> 00:15:12.680]   Right.
[00:15:12.680 --> 00:15:14.320]   Just so cool.
[00:15:14.320 --> 00:15:21.440]   I would in fact, encourage our community members to not even just take existing
[00:15:21.440 --> 00:15:25.400]   data sets from our research repositories like Kaggle.
[00:15:25.720 --> 00:15:27.880]   You can in fact contribute your own data sets.
[00:15:27.880 --> 00:15:32.200]   Why not take, you know, data sets from your favorite horror movies, your
[00:15:32.200 --> 00:15:35.960]   favorite Zack Snyder movies, that, that, that could be really cool.
[00:15:35.960 --> 00:15:39.880]   That would be in fact, interesting to see how Zero DC performs on this
[00:15:39.880 --> 00:15:41.720]   kind of different scenarios.
[00:15:41.720 --> 00:15:42.200]   Yeah.
[00:15:42.200 --> 00:15:45.760]   And like, if you find, if we see that, that's kind of like, that's also, I
[00:15:45.760 --> 00:15:46.680]   think it's a really good point.
[00:15:46.680 --> 00:15:51.840]   Like if we actually want to, you know, say like this scene is like, do you
[00:15:51.840 --> 00:15:55.680]   think it would generalize well if like, you know, like say we're theorizing
[00:15:55.680 --> 00:15:58.920]   here, like it does poorly because the image that's trained on are like
[00:15:58.920 --> 00:15:59.800]   actual low light.
[00:15:59.800 --> 00:16:00.120]   Right.
[00:16:00.120 --> 00:16:03.600]   But if somebody collects images that are like similar, maybe they're like
[00:16:03.600 --> 00:16:07.720]   from horror movies where they're low light, but they're not like no light,
[00:16:07.720 --> 00:16:11.280]   you know, it would probably start doing a better job with like this images.
[00:16:11.280 --> 00:16:11.520]   Right.
[00:16:11.520 --> 00:16:12.360]   Yeah.
[00:16:12.360 --> 00:16:12.520]   Yeah.
[00:16:12.520 --> 00:16:12.760]   Yeah.
[00:16:12.760 --> 00:16:17.440]   It would, it would basically stop treating this kind of cases as edge cases
[00:16:17.440 --> 00:16:20.760]   or outliers and basically we'll learn to generalize on these cases.
[00:16:21.520 --> 00:16:21.840]   Yeah.
[00:16:21.840 --> 00:16:22.560]   That's amazing.
[00:16:22.560 --> 00:16:25.000]   And like, we'll talk in a moment about how it works, but yeah.
[00:16:25.000 --> 00:16:27.920]   So like do your community, like if you want to experiment with this dataset
[00:16:27.920 --> 00:16:32.400]   and you want to share your results, post it on Twitter, tag me, tag Shormik,
[00:16:32.400 --> 00:16:33.360]   tag Weights and Biases.
[00:16:33.360 --> 00:16:33.960]   We'll see it.
[00:16:33.960 --> 00:16:37.280]   That'll be so interesting because I think it's such a, such a cool
[00:16:37.280 --> 00:16:41.040]   implementation here and such a cool like idea of how to make it, how to make it
[00:16:41.040 --> 00:16:41.400]   work.
[00:16:41.400 --> 00:16:45.040]   So, all right, Shormik, so maybe you could like talk a little bit about how
[00:16:45.040 --> 00:16:48.720]   like zero DC works and about some of the like magic that's happening behind
[00:16:48.720 --> 00:16:49.320]   the scenes.
[00:16:51.000 --> 00:16:51.800]   Yeah, sure.
[00:16:51.800 --> 00:16:58.120]   So the real magic of zero DC is actually the way the authors of zero DC actually
[00:16:58.120 --> 00:17:00.800]   formulate the problem of low light image enhancement.
[00:17:00.800 --> 00:17:05.520]   So basically it doesn't basically just predict an enhanced image.
[00:17:05.520 --> 00:17:10.200]   So there's a deep neural network, which basically, you know, takes a low
[00:17:10.200 --> 00:17:17.960]   light image and basically predicts a image specific tonal curve for that
[00:17:17.960 --> 00:17:19.000]   particular image.
[00:17:20.360 --> 00:17:25.320]   By the way, do you want me to show you a Google presentation?
[00:17:25.320 --> 00:17:26.240]   I have it ready.
[00:17:26.240 --> 00:17:26.760]   Like it.
[00:17:26.760 --> 00:17:29.880]   Oh, it's so convenient that it's just, it's just there.
[00:17:29.880 --> 00:17:30.240]   Yeah.
[00:17:30.240 --> 00:17:31.040]   Please do it.
[00:17:31.040 --> 00:17:31.280]   Yeah.
[00:17:31.280 --> 00:17:36.000]   Cause like it's, and then like, if you have it laying around, yeah, let's check
[00:17:36.000 --> 00:17:36.280]   it out.
[00:17:36.280 --> 00:17:40.040]   Yeah, I kind of have it laying around.
[00:17:40.040 --> 00:17:45.960]   So basically this is, this is just the deep neural network that is the heart of
[00:17:45.960 --> 00:17:46.440]   zero DC.
[00:17:46.800 --> 00:17:50.640]   So this is called DC net deep estimation network.
[00:17:50.640 --> 00:17:55.280]   So it's like, you can see it's a very simple unit, like fully convolutional
[00:17:55.280 --> 00:18:01.960]   model, which basically predicts 24 curve parameter maps in three channels.
[00:18:01.960 --> 00:18:06.880]   So it's eight curve parameter maps in total for a particular image.
[00:18:06.880 --> 00:18:11.920]   And basically what it does is given a particular low light image, you pass it
[00:18:11.920 --> 00:18:19.120]   through the DC net, you get this eight curve parameter maps and you kind of use
[00:18:19.120 --> 00:18:22.200]   the above formula that is shown there.
[00:18:22.200 --> 00:18:29.560]   You basically use our formula to use the image specific curve parameter to enhance
[00:18:29.560 --> 00:18:31.080]   the image in eight iterations.
[00:18:31.080 --> 00:18:36.240]   So there are eight curve parameter maps and you combine each of them iteratively
[00:18:37.400 --> 00:18:42.560]   and you get a better and better image that improves the quality step by step.
[00:18:42.560 --> 00:18:47.440]   So here's the thing in the current implementation, you can see only the final
[00:18:47.440 --> 00:18:47.720]   image.
[00:18:47.720 --> 00:18:55.040]   You can technically see the enhanced images that are intermediate also.
[00:18:55.040 --> 00:19:00.040]   So let's say for the frames that we saw were kind of overexposed in the final
[00:19:00.040 --> 00:19:04.120]   image, there could be an intermediate state where they weren't overexposed and
[00:19:04.640 --> 00:19:09.480]   could have struck the final, very sweet balance that we were looking for.
[00:19:09.480 --> 00:19:13.960]   But yes, so our goal is actually to get the final image.
[00:19:13.960 --> 00:19:14.560]   Correct.
[00:19:14.560 --> 00:19:15.080]   Exactly.
[00:19:15.080 --> 00:19:15.360]   Correct.
[00:19:15.360 --> 00:19:17.440]   So yeah, that's not an excuse I would say.
[00:19:17.440 --> 00:19:21.920]   But yeah, this is overall how Zero DC frames the problem of low light.
[00:19:21.920 --> 00:19:30.640]   But the thing is, since it's a completely unsupervised framework, how does it, you
[00:19:30.640 --> 00:19:33.640]   know, optimize, how does it frame the optimization for them?
[00:19:33.640 --> 00:19:34.680]   That's even more interesting.
[00:19:34.680 --> 00:19:38.640]   So in a supervised framework, you basically have a ground truth to compare
[00:19:38.640 --> 00:19:38.920]   against.
[00:19:38.920 --> 00:19:41.400]   Zero DC doesn't have a ground truth to compare against.
[00:19:41.400 --> 00:19:46.040]   So it basically takes the root of what the authors of the paper like to call
[00:19:46.040 --> 00:19:47.360]   non-reference loss functions.
[00:19:47.360 --> 00:19:53.160]   So the non-reference loss functions do not basically try to, you know, compare
[00:19:53.160 --> 00:19:59.240]   a referential metric against a ground truth.
[00:19:59.280 --> 00:20:05.560]   Instead, the non-reference loss functions basically try to optimize a certain
[00:20:05.560 --> 00:20:11.280]   aspect of the image enhancement procedure, which we would want to be present in
[00:20:11.280 --> 00:20:13.360]   the final enhanced image.
[00:20:13.360 --> 00:20:19.000]   So as you can see, there's a color constancy loss, which basically makes sure
[00:20:19.000 --> 00:20:24.760]   that the final enhanced image doesn't show a huge amount of deviation in color.
[00:20:25.680 --> 00:20:31.200]   There's the exposure control loss, which again, basically controls that the final
[00:20:31.200 --> 00:20:34.560]   enhanced image doesn't deviate too much in terms of exposure.
[00:20:34.560 --> 00:20:40.400]   The illumination smoothness loss basically, instead of working on the image, it
[00:20:40.400 --> 00:20:45.640]   actually works on the curve parameter maps and it basically preserves the
[00:20:45.640 --> 00:20:48.920]   monotonicity of the relations of neighboring pixels.
[00:20:48.920 --> 00:20:50.520]   And all these losses, right?
[00:20:50.520 --> 00:20:56.760]   They're helping adjust those eight color curves, right?
[00:20:56.760 --> 00:21:04.800]   Like that's, you know, the color curves, then there's some computation with the
[00:21:04.800 --> 00:21:07.200]   loss function that helps optimize the...
[00:21:07.200 --> 00:21:08.000]   Yes, yes.
[00:21:08.000 --> 00:21:12.360]   So the final loss function, which you optimize is a weighted sum of all these
[00:21:12.360 --> 00:21:13.280]   four loss functions.
[00:21:13.280 --> 00:21:18.320]   So technically speaking, by optimizing the final weighted sum of all these loss
[00:21:18.320 --> 00:21:23.080]   functions, you are optimizing each and every non-reference loss functions
[00:21:23.080 --> 00:21:23.920]   individually as well.
[00:21:23.920 --> 00:21:25.200]   Nice.
[00:21:25.200 --> 00:21:32.240]   So it's kind of like, you know, we get the color curves and essentially, yeah,
[00:21:32.240 --> 00:21:36.600]   like what you're explaining is that like this loss functions are trying to like
[00:21:36.600 --> 00:21:41.800]   pre-preserve like the things that were in the low light image that should still be
[00:21:41.800 --> 00:21:43.960]   in the well light image.
[00:21:43.960 --> 00:21:44.280]   Yeah.
[00:21:44.280 --> 00:21:45.440]   Like color consistency.
[00:21:45.600 --> 00:21:49.840]   And like the color constancy, even the spatial constancy is very important.
[00:21:49.840 --> 00:21:55.080]   Like the gradient of colors and the patterns should be completely preserved.
[00:21:55.080 --> 00:21:55.280]   Right.
[00:21:55.280 --> 00:22:00.600]   You are just adjusting the lighting of the frame to make it better.
[00:22:00.600 --> 00:22:03.760]   But that doesn't mean like you are changing the image completely.
[00:22:03.760 --> 00:22:04.520]   Yeah.
[00:22:04.520 --> 00:22:09.200]   So after we get like the color curves, there's some sort of operation that we
[00:22:09.200 --> 00:22:13.680]   apply, like this color curves that the network has predicted, we do something
[00:22:13.680 --> 00:22:16.800]   with them with relations to the low light image and we get...
[00:22:16.800 --> 00:22:17.280]   Yeah.
[00:22:17.280 --> 00:22:19.200]   Yeah.
[00:22:19.200 --> 00:22:28.160]   So, so like we use this aforementioned formula to basically apply a curve
[00:22:28.160 --> 00:22:33.640]   parameter map on top of the low light image to get the corresponding enhanced
[00:22:33.640 --> 00:22:37.520]   image for that particular curve parameter map using this particular formula.
[00:22:37.520 --> 00:22:42.240]   Oh, that's what you've been saying that like this eight iteration that are
[00:22:42.240 --> 00:22:45.480]   happening, like the eight different kind of variations of the low light
[00:22:45.480 --> 00:22:46.800]   images before the final one.
[00:22:46.800 --> 00:22:46.880]   Yeah.
[00:22:46.880 --> 00:22:51.640]   And this, this is, and since there are eight curve parameter maps per image,
[00:22:51.640 --> 00:22:55.400]   so this, there are basically, you get basically eight enhanced images.
[00:22:55.400 --> 00:22:56.200]   Yeah.
[00:22:56.200 --> 00:22:58.080]   Well, Sharmik, thank you for the explanation.
[00:22:58.080 --> 00:23:01.120]   And like, it's a, it's nice to see that you always have like a solid Google
[00:23:01.120 --> 00:23:04.840]   appraisal whenever I'm gonna hit you up and ask you to explain something.
[00:23:04.840 --> 00:23:09.040]   I also know that you, you know, you are a big contributor to the Keras
[00:23:09.040 --> 00:23:10.640]   community and you really enjoy the framework.
[00:23:10.800 --> 00:23:13.800]   So you can maybe talk about a couple of things that you like, like about it,
[00:23:13.800 --> 00:23:16.680]   especially in how was your experience working on this project and things,
[00:23:16.680 --> 00:23:17.360]   things like that.
[00:23:17.360 --> 00:23:19.400]   Yeah.
[00:23:19.400 --> 00:23:26.000]   So one of the reasons I actually implemented Zero DC with Keras is to show
[00:23:26.000 --> 00:23:32.240]   off how flexible and convenient it is to use for not just for deep learning
[00:23:32.240 --> 00:23:34.920]   research, but also deep learning in the production.
[00:23:35.360 --> 00:23:42.880]   So, so, so the best thing about Keras is that the code is really readable.
[00:23:42.880 --> 00:23:44.720]   The code is consistent.
[00:23:44.720 --> 00:23:47.080]   It's simple.
[00:23:47.080 --> 00:23:53.360]   Let's just say, and you know, like if you put it all together, it translates
[00:23:53.360 --> 00:23:58.000]   to only one phrase in my brain, which is it's deep learning for humans.
[00:23:58.000 --> 00:24:04.120]   So, so, so yeah, that's, that's what I would say the philosophy is behind Keras.
[00:24:04.200 --> 00:24:09.400]   And you know one of the things I love about TensorFlow and Keras in general
[00:24:09.400 --> 00:24:11.240]   is the community is amazing.
[00:24:11.240 --> 00:24:14.840]   There are always amazing people who are there in the community who
[00:24:14.840 --> 00:24:15.880]   are ready to help you.
[00:24:15.880 --> 00:24:21.160]   And I've learned a lot from you know, interacting with community members
[00:24:21.160 --> 00:24:22.520]   and even collaborating with them.
[00:24:22.520 --> 00:24:28.480]   So yeah, overall you know, Keras is, I would say it's a really awesome thing.
[00:24:28.480 --> 00:24:29.200]   Yeah.
[00:24:29.200 --> 00:24:33.040]   And man, you, you are one of the people who makes that community really, really
[00:24:33.040 --> 00:24:34.600]   amazing, so shout out to you.
[00:24:34.600 --> 00:24:35.440]   Thank you.
[00:24:35.440 --> 00:24:43.040]   And also here's the thing, like, like, you know, like TensorFlow works really
[00:24:43.040 --> 00:24:44.600]   well with weights and biases, I have to say.
[00:24:44.600 --> 00:24:49.840]   It's just one callback which works right out of the box and you can start logging
[00:24:49.840 --> 00:24:56.120]   with, you know, weights and biases and you can even sync your TensorBoard instances
[00:24:56.120 --> 00:25:02.320]   with weights and biases using the one DB Keras callback, which is really amazing.
[00:25:02.320 --> 00:25:02.720]   I would say.
[00:25:03.120 --> 00:25:04.280]   Yeah, that's really convenient.
[00:25:04.280 --> 00:25:08.160]   Like that, that this functionality is like there and like, I was also kind of
[00:25:08.160 --> 00:25:11.840]   like, I remember when I, like, I think the first time I implemented like WMB, it
[00:25:11.840 --> 00:25:15.320]   was like also in a Keras network and I basically hooked up with like, yeah, we
[00:25:15.320 --> 00:25:17.040]   paste this callback here and it does the thing.
[00:25:17.040 --> 00:25:21.520]   And then, and then, yeah, like that was like, again, my first experience with
[00:25:21.520 --> 00:25:25.880]   WMB was with Keras and like, so I can very much relate to that for sure.
[00:25:25.880 --> 00:25:28.240]   Yeah.
[00:25:28.240 --> 00:25:28.560]   Yeah.
[00:25:28.560 --> 00:25:31.360]   So, so that's the thing, right?
[00:25:31.640 --> 00:25:37.320]   Like two very, you know, when you put two great things, you make something beautiful.
[00:25:37.320 --> 00:25:42.560]   So you put Keras and you put weights and biases and you make amazing dashboards
[00:25:42.560 --> 00:25:47.320]   and reports and that's, that's, that's something to behold, I would say.
[00:25:47.320 --> 00:25:49.400]   Oh man, beautiful words, honestly.
[00:25:49.400 --> 00:25:52.880]   That's, that's like way more poetic than I could ever ask for, but I love it.
[00:25:52.880 --> 00:25:54.360]   Thank you so much, Shormik.
[00:25:54.360 --> 00:25:55.920]   I'll talk to you soon.
[00:25:55.920 --> 00:25:59.280]   Like whenever I need some, some, some explaining, I'll just call you in the
[00:25:59.280 --> 00:26:02.480]   middle of the night, like today and you'll do it again.
[00:26:02.480 --> 00:26:04.320]   Feel free to call me anytime, Ivan.
[00:26:04.320 --> 00:26:05.440]   The pleasure is mine.
[00:26:05.440 --> 00:26:06.440]   Thank you so much.
[00:26:06.440 --> 00:26:08.080]   Thank you, Ivan.
[00:26:08.080 --> 00:26:08.640]   Bye bye.
[00:26:08.640 --> 00:26:09.600]   Bye bye.
[00:26:09.600 --> 00:26:15.120]   Man, that was such a cool conversation, which was totally unscripted and
[00:26:15.120 --> 00:26:17.160]   totally live, by the way.
[00:26:17.160 --> 00:26:21.560]   But yeah, big shout out to Shormik for doing this and being in the video.
[00:26:21.560 --> 00:26:26.280]   And like, I'm sure you all, and me included, we all have a lot more context
[00:26:26.280 --> 00:26:30.960]   to dive into the next part of the video, which is going to be training and how,
[00:26:30.960 --> 00:26:34.320]   how we actually can like, train the model and perform inference.
[00:26:34.320 --> 00:26:38.280]   So next up, let's go and start writing the cells that define
[00:26:38.280 --> 00:26:39.840]   the last functions in the model.
[00:26:39.840 --> 00:26:48.240]   So next up, we'll initialize a new which-nobiosis run, which will be for
[00:26:48.240 --> 00:26:50.560]   training in the same project.
[00:26:50.560 --> 00:26:52.920]   And we've also like specified this project name.
[00:26:53.200 --> 00:26:57.280]   When we were logging the WNB table to explore the data sets.
[00:26:57.280 --> 00:26:59.680]   So it's like this low light zero DC project.
[00:26:59.680 --> 00:27:02.840]   And now we'll also like initialize a new run in this project and runs are, by
[00:27:02.840 --> 00:27:06.320]   the way, these things in the left, which can be instances of us, like training
[00:27:06.320 --> 00:27:11.280]   models or like logging data sets or performing like inference in the test
[00:27:11.280 --> 00:27:13.320]   data and things like that.
[00:27:13.320 --> 00:27:17.160]   Like when there, when runs have training metrics, we can see the
[00:27:17.160 --> 00:27:18.480]   training metrics displayed here.
[00:27:18.480 --> 00:27:23.080]   And this is like a which-nobiosis project dashboard in general, where the idea is
[00:27:23.080 --> 00:27:25.520]   that like we're lagging all these different runs and some of them are like
[00:27:25.520 --> 00:27:28.760]   for training and then we can like go and we can like interactively, you know,
[00:27:28.760 --> 00:27:32.920]   like explore like how the training went for like different configurations of our
[00:27:32.920 --> 00:27:33.720]   models, you know?
[00:27:33.720 --> 00:27:36.840]   And now we'll start like another such new training run.
[00:27:36.840 --> 00:27:41.480]   So I'll go and I'll just run the cell and it'll start the new run
[00:27:41.480 --> 00:27:42.520]   that we'll use for training.
[00:27:42.520 --> 00:27:47.040]   Then here's, I've kind of mentioned, like, as we're going to be like logging
[00:27:47.040 --> 00:27:50.240]   different model variations, like some of them are going to do better.
[00:27:50.240 --> 00:27:51.440]   Some of them are going to do worse.
[00:27:51.440 --> 00:27:54.560]   And we kind of want to like keep track of the different hyperparameters that
[00:27:54.560 --> 00:27:59.400]   those models are using, because like say, like one model did such a great job.
[00:27:59.400 --> 00:28:02.400]   We want to then be able to like go and see like, Hey, oh, it used like this
[00:28:02.400 --> 00:28:05.760]   hyperparameter, like say batch size or number of epochs or like learning rate.
[00:28:05.760 --> 00:28:06.640]   And that helped it.
[00:28:06.640 --> 00:28:12.200]   So here we'll use WNB config, kind of like syntax.
[00:28:12.200 --> 00:28:15.360]   And so we'll say that our learning rate, like in this case, I'm just like using
[00:28:15.360 --> 00:28:18.960]   learning rates as an example of the thing that we can like vary that'll
[00:28:18.960 --> 00:28:20.120]   like affect the training.
[00:28:20.560 --> 00:28:20.760]   Yeah.
[00:28:20.760 --> 00:28:24.080]   So the last training round that I've done, which was like this little one,
[00:28:24.080 --> 00:28:29.960]   which was pretty short, the, the orange one it had the learning rate of, so it
[00:28:29.960 --> 00:28:32.000]   was like one to the power of minus three.
[00:28:32.000 --> 00:28:35.080]   So let's say in this case, we'll go and we'll make it like one to the power
[00:28:35.080 --> 00:28:36.360]   of minus four, for example.
[00:28:36.360 --> 00:28:39.880]   And as we can see, like we're defining this config variable and we're also
[00:28:39.880 --> 00:28:41.320]   like passing into the model.
[00:28:41.320 --> 00:28:45.240]   And that's kind of like how, like which device is going to know which
[00:28:45.240 --> 00:28:46.440]   hyperparameter it is.
[00:28:46.440 --> 00:28:48.400]   And that's kind of like how the model is going to know that we're using
[00:28:48.400 --> 00:28:49.880]   this hyperparameter also.
[00:28:50.200 --> 00:28:53.320]   And then it goes pretty much one little snippet of code, which lets
[00:28:53.320 --> 00:28:56.680]   Keras to start sending the training metrics to weights and biases.
[00:28:56.680 --> 00:29:01.440]   Which is like us adding this little callback right here.
[00:29:01.440 --> 00:29:06.120]   So this one newbie callback, the way we're getting it is that we're in that
[00:29:06.120 --> 00:29:08.360]   we're pretty much importing it from WNB.keras.
[00:29:08.360 --> 00:29:13.320]   We're importing the WNB callback and then we're just going and we're
[00:29:13.320 --> 00:29:15.760]   passing WNB callback here.
[00:29:15.760 --> 00:29:18.560]   And so the cool thing here, if you have like, say several callbacks, it's
[00:29:18.560 --> 00:29:23.120]   going to be like your callback, but then like just the comma weights and
[00:29:23.120 --> 00:29:26.360]   biases callback, and you can like make the best of both worlds in that way,
[00:29:26.360 --> 00:29:27.040]   for example.
[00:29:27.040 --> 00:29:29.400]   But yeah, that's pretty much the only thing we need to do to like integrate
[00:29:29.400 --> 00:29:31.080]   Keras and weights and biases together.
[00:29:31.080 --> 00:29:34.320]   Which is like, yeah, and this like zero model that fit in the callbacks
[00:29:34.320 --> 00:29:35.720]   past the weights and biases callback.
[00:29:35.720 --> 00:29:37.960]   So I'll set the number of rebugs to be 50.
[00:29:37.960 --> 00:29:41.800]   And now let's pretty much jump into training by running this self code.
[00:29:41.800 --> 00:29:45.400]   Then so I think note here is like how quickly it's training.
[00:29:45.800 --> 00:29:49.600]   Because again, like it's a really small model that trains like really, really
[00:29:49.600 --> 00:29:54.040]   quickly and it's training like on the free like Google call up pretty much.
[00:29:54.040 --> 00:29:58.000]   And it's like taking, you know, like seven to eight seconds per epoch.
[00:29:58.000 --> 00:30:02.320]   And as our model is training, we can now go to the run page, which we've
[00:30:02.320 --> 00:30:06.320]   like initialized here, it gives us like a link and like actually start
[00:30:06.320 --> 00:30:08.000]   seeing the metrics coming in.
[00:30:08.000 --> 00:30:13.040]   You can see like the smoothness loss going down and like other metrics like
[00:30:13.040 --> 00:30:14.720]   that, we can actually, there's a new run.
[00:30:15.000 --> 00:30:18.720]   It, the automatically generated name for it is like Lila shape.
[00:30:18.720 --> 00:30:21.120]   Let's see, let's make it like a better color.
[00:30:21.120 --> 00:30:22.680]   So maybe it like sticks out a bit more.
[00:30:22.680 --> 00:30:26.000]   Cause I've done quite a few runs that like, as you can see are the same
[00:30:26.000 --> 00:30:28.680]   learning rate and unsurprisingly enough, they're kind of like
[00:30:28.680 --> 00:30:30.920]   training in really similar ways.
[00:30:30.920 --> 00:30:33.720]   But it also seems like there are like so many runs in general.
[00:30:33.720 --> 00:30:35.440]   So maybe we can go on like filter.
[00:30:35.440 --> 00:30:37.840]   Let's filter runs by job type.
[00:30:37.840 --> 00:30:41.200]   Let's say we'll leave all the training runs right here, which pretty much
[00:30:41.200 --> 00:30:43.680]   excludes all the ones who are just like lock tables or something.
[00:30:43.680 --> 00:30:50.040]   And let's, let's say we'll well, we'll visualize none and I will visualize
[00:30:50.040 --> 00:30:52.120]   like say only this current one.
[00:30:52.120 --> 00:30:56.880]   And then maybe a couple other ones that we can use to compare it against.
[00:30:56.880 --> 00:30:59.840]   Let's say like this one.
[00:30:59.840 --> 00:31:00.960]   Okay.
[00:31:00.960 --> 00:31:03.280]   So now it gives me like a lot of a better ideas, like how
[00:31:03.280 --> 00:31:04.480]   they typically training went.
[00:31:04.480 --> 00:31:08.040]   So here are some that I try to think with like really high learning rate
[00:31:08.040 --> 00:31:11.840]   that kind of converged faster, but this is like the red one is the one we're
[00:31:12.120 --> 00:31:15.680]   training right now in the Google call up notebook and it's kind of pulling
[00:31:15.680 --> 00:31:17.200]   the metrics life as it goes.
[00:31:17.200 --> 00:31:20.520]   So like, yeah, that's kind of pretty interesting way to like keep track of
[00:31:20.520 --> 00:31:25.600]   how your model is training, but like, you know, not be just staring at like
[00:31:25.600 --> 00:31:29.000]   this metrics here, but to have something where you're like, you know, can
[00:31:29.000 --> 00:31:32.920]   compare it against like other training sessions and see life kind of how the
[00:31:32.920 --> 00:31:35.080]   metrics are unfolding and stuff like that.
[00:31:35.080 --> 00:31:39.920]   And so as we can see, our model has now finished training and you know, we
[00:31:39.920 --> 00:31:42.440]   can go in the dashboard and like, yeah, we can see that like, yeah, we know
[00:31:42.440 --> 00:31:46.960]   we did like say 50 bucks, not a hundred bucks, like in this case, like the
[00:31:46.960 --> 00:31:51.080]   validation was that it reaches like still so, so, so similar that it's like,
[00:31:51.080 --> 00:31:54.800]   yeah, it's, it's probably, you know, it's, it's probably all right for a model.
[00:31:54.800 --> 00:31:58.840]   And then if you want to like, say, compare all the like learning rates
[00:31:58.840 --> 00:32:02.840]   for the different models, we can go see into the runs table and be like, okay,
[00:32:02.840 --> 00:32:04.760]   like let's find a learning rate, have a parameter.
[00:32:04.760 --> 00:32:07.720]   I didn't like it for all of the training runs that I did before, but like for
[00:32:07.720 --> 00:32:12.320]   the ones that I did, we can see it like, Hey, for example, like this orange run
[00:32:12.320 --> 00:32:16.400]   right here, which like converged well, quite fast that I would say had like a
[00:32:16.400 --> 00:32:20.040]   lower learning rate or like, actually I think it's kind of like a higher
[00:32:20.040 --> 00:32:20.920]   learning rate actually.
[00:32:20.920 --> 00:32:21.160]   Yeah.
[00:32:21.160 --> 00:32:21.480]   Right.
[00:32:21.480 --> 00:32:26.840]   And like the red ones say like, Hey, it had the higher learning rate and it took
[00:32:26.840 --> 00:32:29.400]   it a little longer to converge, but it's like, I don't know, maybe you did that
[00:32:29.400 --> 00:32:33.120]   more accurately and then we can also find all the different like hyper parameters
[00:32:33.120 --> 00:32:37.440]   in the run page itself, but you know, as great as it is to be looking at metrics
[00:32:37.680 --> 00:32:41.160]   when it comes to any sort of like generative or in our case, enhancing
[00:32:41.160 --> 00:32:45.640]   models, like you always want to see the finished product to be able to make a
[00:32:45.640 --> 00:32:50.040]   judgment as to whether it's like performing well or it needs some work on it.
[00:32:50.040 --> 00:32:51.600]   And that's what we're going to do.
[00:32:51.600 --> 00:32:56.480]   Now we're going to perform inference on 15 images from the test data set from,
[00:32:56.480 --> 00:33:00.720]   from like the law that is it that we've, uh, that we've used for training.
[00:33:00.720 --> 00:33:05.920]   And so we'll define some functions for inference and then we'll run the cell to,
[00:33:06.200 --> 00:33:09.640]   uh, to actually perform it and we're going to log it as a WNB table again.
[00:33:09.640 --> 00:33:14.760]   And then, so we'll explore like the original low light image versus the like
[00:33:14.760 --> 00:33:18.440]   algorithmically enhanced by an auto contrasting algorithm image.
[00:33:18.440 --> 00:33:22.560]   And then like our like deep learning, uh, zero to see enhanced image.
[00:33:22.560 --> 00:33:27.240]   And so it's finished, um, sinking, sinking into WNB table and doing the inference.
[00:33:27.240 --> 00:33:31.960]   Now let's go and click on the run page and open our latest run, which will
[00:33:31.960 --> 00:33:33.320]   have the table if we can look at.
[00:33:35.240 --> 00:33:38.280]   So maybe let's make the images larger again.
[00:33:38.280 --> 00:33:38.960]   Yep.
[00:33:38.960 --> 00:33:41.080]   It's now we can just like, that's what we've started with.
[00:33:41.080 --> 00:33:43.240]   That's the original low light image.
[00:33:43.240 --> 00:33:46.280]   And that's how the algorithm, uh, can enhance it.
[00:33:46.280 --> 00:33:48.680]   And that's how like our deep learning approach is.
[00:33:48.680 --> 00:33:53.080]   We've explored it from, uh, you know, the, the awesome zero to see paper
[00:33:53.080 --> 00:33:58.360]   implemented like that in Python and Keras, uh, can, can do the enhancement job.
[00:33:58.360 --> 00:34:02.800]   So as you can see, like, it's quite a drastic difference, you know, like here,
[00:34:02.800 --> 00:34:06.640]   it seems like you can almost like, you can probably read the handwriting here.
[00:34:06.640 --> 00:34:11.400]   You know, if I spoke some Chinese, if I'm not mistaken, I could probably read it,
[00:34:11.400 --> 00:34:13.400]   but, but I kind of not yet.
[00:34:13.400 --> 00:34:18.960]   Um, and then, uh, yeah, here are the countries, the algorithm is doing some
[00:34:18.960 --> 00:34:21.880]   weird stuff with colors, but CRDC can handle it actually.
[00:34:21.880 --> 00:34:25.120]   Um, and then it's kind of me and show me talk.
[00:34:25.120 --> 00:34:29.040]   Like, it's also, you know, like the, um, the fact that like it was trained
[00:34:29.040 --> 00:34:32.680]   in similar type of images, it's, I think it's also helping it to do quite
[00:34:32.680 --> 00:34:35.240]   a good job and then it can also generalize like some of the movie images
[00:34:35.240 --> 00:34:38.880]   that we've passed in, if we like train it more on like the actual movie images,
[00:34:38.880 --> 00:34:41.120]   it can like do even better on them pretty much.
[00:34:41.120 --> 00:34:43.720]   That's the, it's actually kind of like the last thing that I wanted to show
[00:34:43.720 --> 00:34:47.080]   you is that like, Hey, if we save the model, uh, it's going to be like, yeah,
[00:34:47.080 --> 00:34:54.000]   like 340, 45 kilobytes, you know, like, so like it's doing that on 350
[00:34:54.000 --> 00:34:56.800]   kilobytes, which is pretty, pretty impressive.
[00:34:56.800 --> 00:35:02.000]   Oh, and you can see like this image, like here, the, and this test
[00:35:02.000 --> 00:35:04.680]   images, right, like you needless to say, like the model did not see.
[00:35:04.680 --> 00:35:07.240]   I have the same, we just write like while training.
[00:35:07.240 --> 00:35:10.520]   So it's, it's doing all that by just looking at all of the other training
[00:35:10.520 --> 00:35:13.720]   images, but here you can see like the auto contrasting algorithm, like
[00:35:13.720 --> 00:35:18.040]   completely kind of choking, I would say, and like really not doing a great job
[00:35:18.040 --> 00:35:22.440]   at all, but like zero disease is actually like able to handle it in this case.
[00:35:22.440 --> 00:35:25.120]   Uh, which is a, which is a pretty, pretty cool.
[00:35:25.120 --> 00:35:27.400]   So that's pretty much it for this video.
[00:35:27.400 --> 00:35:31.200]   And you're going to find the Google call up notebook that I've been using in the
[00:35:31.200 --> 00:35:35.280]   video description and as we ensure McTalks, like feel free to accept their
[00:35:35.280 --> 00:35:39.160]   challenge and, uh, do some fun stuff with like zero DC, maybe put together
[00:35:39.160 --> 00:35:43.720]   like your own data set and like, you can tweet it as, uh, your results and
[00:35:43.720 --> 00:35:46.720]   we'll be able to see it and it'll be really cool for us to check it out.
[00:35:46.720 --> 00:35:49.040]   And, uh, yeah, in general, like thanks.
[00:35:49.040 --> 00:35:52.560]   Thanks so much for watching this video and I really hope that you enjoyed it
[00:35:52.560 --> 00:35:56.320]   and found it useful and so subscribe to our channel to see more, uh,
[00:35:56.320 --> 00:35:58.040]   tutorials, interviews, and talks.
[00:35:58.040 --> 00:35:59.960]   And yeah, just thank you so much for watching.
[00:36:00.200 --> 00:36:02.800]   Hope it was fun, hope it was useful.
[00:36:02.800 --> 00:36:03.800]   Bye-bye.

