
[00:00:00.000 --> 00:00:04.560]   I wish the thing we use the ASI for is to cure the diseases and do all the good in the world.
[00:00:04.560 --> 00:00:12.320]   But it is my prediction that in the endgame, what will be at stake will not just be cool products,
[00:00:12.320 --> 00:00:17.360]   but whether liberal democracy survives, whether the CCP survives, what the world order for the
[00:00:17.360 --> 00:00:22.720]   next century will be. And when that is at stake, forces will be activated that are way beyond what
[00:00:22.720 --> 00:00:30.800]   we're talking about now. And in this crazy race at the end, the national security implications
[00:00:30.800 --> 00:00:35.920]   will be the most important. Sort of like World War II, it's like, yeah, nuclear energy had its
[00:00:35.920 --> 00:00:41.200]   day. But in the initial period when this technology was first discovered, you had to
[00:00:41.200 --> 00:00:46.160]   stabilize the situation, you had to get nukes, you had to do it right. And then the civilian
[00:00:46.160 --> 00:00:49.440]   applications had their day. I think people talk about AGI, and they're always just talking about
[00:00:49.440 --> 00:00:53.440]   the private AI labs. And I think I just really want to challenge that assumption. It just seems
[00:00:53.440 --> 00:00:58.240]   pretty likely to me, as we've talked about, for reasons we've talked about, that the national
[00:00:58.240 --> 00:01:03.520]   security state is going to get involved. And I think there's a lot of ways this could look like.
[00:01:03.520 --> 00:01:07.360]   Is it like nationalization? Is it a public-private partnership? Is it a kind of defense
[00:01:07.360 --> 00:01:10.960]   contractor-like relationship? Is it a sort of government project that suits up all the people?
[00:01:10.960 --> 00:01:18.800]   And so there's a spectrum there. But I think people are just vastly underrating the chances
[00:01:18.800 --> 00:01:22.800]   of this more or less looking like a government project. When we have literal superintelligence
[00:01:22.800 --> 00:01:26.880]   on our cluster, and you have a billion superintelligence scientists, and they can
[00:01:26.880 --> 00:01:30.480]   hack everything, they can Stuxnet the Chinese data centers, they're starting to build the
[00:01:30.480 --> 00:01:34.880]   robo-armies, you really think they'll be a private company? And the government would be like, "Oh my
[00:01:34.880 --> 00:01:39.600]   god, what is going on?" Even backing up, why do we want to win against China? We want to win
[00:01:39.600 --> 00:01:46.560]   against China because we don't want a top-down authoritarian system to win. Now, if the way to
[00:01:46.560 --> 00:01:52.160]   beat that is that the most important technology that humanity will have has to be controlled by
[00:01:52.160 --> 00:01:59.520]   a top-down government, what was the point? So let's run our cards with privatization. That's
[00:01:59.520 --> 00:02:03.200]   the way we get to the classic liberal market-based system we want for the ESIs.
[00:02:03.200 --> 00:02:08.000]   Yeah. All right. So a lot to talk about here. Maybe I'll start a bit about actually looking
[00:02:08.000 --> 00:02:11.280]   at what the private world would look like. And I think this is part of where the "there's no
[00:02:11.280 --> 00:02:15.360]   alternative" comes from. And then let's look at what the government project looks like,
[00:02:15.360 --> 00:02:19.840]   what checks and balances look like, and so on. All right. Private world. First of all,
[00:02:19.840 --> 00:02:24.320]   a lot of people right now talk about open source. And I think there's this misconception that AGI
[00:02:24.320 --> 00:02:28.480]   development is going to be like, "Oh, it's going to be some beautiful decentralized thing and some
[00:02:28.480 --> 00:02:32.720]   giddy community of coders who gets to collaborate on it." That's not how it's going to look like.
[00:02:32.720 --> 00:02:36.480]   A hundred-billion-dollar, trillion-dollar cluster, it's not going to be that many people
[00:02:36.480 --> 00:02:40.080]   that have it. The algorithms, it's like right now, open source is kind of good because people
[00:02:40.080 --> 00:02:43.280]   just use the stuff that was published. And so basically, the algorithms were published.
[00:02:43.280 --> 00:02:47.360]   Or as Mistral, they just kind of leave DeepMind and take all the secrets with them and they just
[00:02:47.360 --> 00:02:53.600]   kind of replicate it. But that's not going to continue to be in the case. And so, the open
[00:02:53.600 --> 00:02:58.960]   source alternative... Also, people say stuff like, "1026 flops, it'll be in my phone." No, it won't.
[00:02:58.960 --> 00:03:03.040]   It's like Moore's Law is really slow. AI chips are getting better. But the hundred-billion-dollar
[00:03:03.040 --> 00:03:08.080]   computer will not cost a thousand dollars within your lifetime or whatever, aside from AI.
[00:03:08.080 --> 00:03:14.640]   So anyway, it's going to be two or three big players on the private world.
[00:03:14.640 --> 00:03:23.360]   And so look, a few things. So first of all, you talk about the enormous power that
[00:03:23.360 --> 00:03:26.400]   superintelligence will have and that the government will have.
[00:03:26.400 --> 00:03:32.560]   I think it's pretty plausible that the alternative world is that one AI company has that power.
[00:03:32.560 --> 00:03:35.760]   And specifically, if we're talking about lead, it's like, "What? No, no, no. OpenAI has a six-month
[00:03:35.760 --> 00:03:42.560]   lead." So then you're talking about basically the most powerful weapon ever, and you're kind of
[00:03:42.560 --> 00:03:46.720]   making this radical bet on a private company CEO as the benevolent dictator.
[00:03:46.720 --> 00:03:50.320]   No, no. Not necessarily. Any other thing that's privatized, we don't count on that
[00:03:50.320 --> 00:03:56.400]   being benevolent. Look, think of, for example, somebody who manufactures industrial fertilizer.
[00:03:56.400 --> 00:04:02.480]   The person with this factory, if they went back to an ancient civilization,
[00:04:02.480 --> 00:04:05.520]   they could blow up Rome. They could probably blow up Washington, D.C.
[00:04:05.520 --> 00:04:10.560]   And I think people underrate that there are actually a lot of private actors who have the
[00:04:10.560 --> 00:04:15.760]   ability to... There's a lot of people who control the water supply or whatever, and we can count on
[00:04:15.760 --> 00:04:20.800]   cooperation and market-based incentives to basically keep a balance of power. I gather
[00:04:20.800 --> 00:04:24.160]   things are proceeding really fast, but we have a lot of historical evidence that this is the
[00:04:24.160 --> 00:04:28.000]   thing that works best. I mean, what do we do with nukes, right? The way we keep the nukes in check
[00:04:28.000 --> 00:04:32.400]   is not like a sort of beefed-up Second Amendment, where each state has their own little nuclear
[00:04:32.400 --> 00:04:38.080]   arsenal, and Dario and Sam have their own little nuclear arsenal. No, no. It's institutions. It's
[00:04:38.080 --> 00:04:47.360]   constitutions. It's laws. It's courts. I'm not sure that this sort of balance of power analogy
[00:04:47.360 --> 00:04:52.800]   holds. In fact, the government having the biggest guns was sort of an enormous civilizational
[00:04:52.800 --> 00:04:57.520]   achievement, like Landfrieden in the Holy Roman Empire. If somebody from the town over committed
[00:04:57.520 --> 00:05:03.520]   a crime on you, you didn't start a big battle between the two towns. No, you take it to a court
[00:05:03.520 --> 00:05:08.880]   of the Holy Roman Empire, and they would decide. It's a big achievement. Now, the thing about the
[00:05:08.880 --> 00:05:12.880]   industrial fertilizer, I think the key difference is speed and offense-defense balance issues,
[00:05:12.880 --> 00:05:20.720]   right? It's like 20th century, and 10 years, and a few years. That is an incredibly scary period,
[00:05:20.720 --> 00:05:26.080]   and it is incredibly scary because you're going through just this enormous array of destructive
[00:05:26.080 --> 00:05:31.440]   technology and this enormous amount of basically military advance. You would have gone from
[00:05:31.440 --> 00:05:40.960]   bayonets and horses to tank armies and fighter jets in a couple of years, and then to nukes and
[00:05:40.960 --> 00:05:49.200]   ICBMs just in a matter of years. It is that speed that creates-- I think, basically, the way I think
[00:05:49.200 --> 00:05:53.200]   about it is there's going to be this initial just incredibly volatile, incredibly dangerous period,
[00:05:53.200 --> 00:05:56.800]   and somehow, we have to make it through that. That's going to be incredibly challenging.
[00:05:56.800 --> 00:06:02.800]   That's where you need the government project. If you can make it through that, then you go to,
[00:06:02.800 --> 00:06:07.920]   now, the situation has been stabilized. We don't face this imminent national security threat.
[00:06:07.920 --> 00:06:12.800]   It's like, yes, there were WMDs that came along the way, but either we've managed to have a stable
[00:06:12.800 --> 00:06:16.960]   offense-defense balance. I think bioweapons initially are a huge issue. An attacker can
[00:06:16.960 --> 00:06:21.200]   just create 1,000 different synthetic viruses and spread them, and it's going to be really hard for
[00:06:21.200 --> 00:06:25.840]   you to make a defense against each. But maybe at some point, you figure out the universal defense
[00:06:25.840 --> 00:06:29.440]   against every possible virus, and then you're in a stable situation again on the offense-defense
[00:06:29.440 --> 00:06:33.840]   balance. Or you do the thing you do with planes, where there's certain capabilities that the
[00:06:33.840 --> 00:06:37.760]   private sector isn't allowed to have, and you've figured out what's going on, restrict those,
[00:06:37.760 --> 00:06:45.120]   and then you can let the civilian uses run free. I'm skeptical of this because there's--
[00:06:45.120 --> 00:06:53.120]   Sorry. The other important thing is, I talked about this, maybe it's one company with all this
[00:06:53.120 --> 00:06:57.360]   power. I think it is unprecedented because it's like the industrial fertilizer guy cannot overthrow
[00:06:57.360 --> 00:07:01.520]   the US government. I think it's quite plausible that the AI company with superintelligence can
[00:07:01.520 --> 00:07:03.840]   overthrow the US government. But there'll be multiple AI companies, right? And I buy that
[00:07:03.840 --> 00:07:07.520]   one of them could be ahead. So it's not obvious that it'll be multiple. I think it's, again,
[00:07:07.520 --> 00:07:10.080]   if there's a six-month lead, maybe there's two or three. I agree.
[00:07:10.080 --> 00:07:13.840]   But if there's two or three, then what you have is just a crazy race between these two or three
[00:07:13.840 --> 00:07:18.160]   companies. It's like, whatever, Demis and Sam, they're just like, "I don't want to let the other
[00:07:18.160 --> 00:07:22.480]   one win." And they're both developing their nuclear arsenals and the robot. It's just like,
[00:07:22.480 --> 00:07:25.840]   also, come on, the government is not going to let these people-- are they going to let-- is
[00:07:25.840 --> 00:07:31.280]   Dario going to be the one developing the super hacking Stuxnet and deploying against the Chinese
[00:07:31.280 --> 00:07:36.000]   data center? The other issue, though, is it won't just-- if it's two or three, it won't just be two
[00:07:36.000 --> 00:07:40.000]   or three. There'll be two or three, and it'll be China and Russia and North Korea. Because in the
[00:07:40.000 --> 00:07:43.680]   private lab world, there is no way they'll have security that is good enough.
[00:07:43.680 --> 00:07:49.520]   Okay, I agree that I'm worried if, let's say, Amr Dario have a one-year lead on ASI in that
[00:07:49.520 --> 00:07:54.400]   world, then I'm concerned about this being privatized. But in that exact same world,
[00:07:54.400 --> 00:07:57.600]   I'm very concerned about Donald Trump having the capability. And potentially,
[00:07:57.600 --> 00:08:01.040]   if we're living in a world where the takeoff is slower than you anticipate,
[00:08:01.040 --> 00:08:06.080]   in that world, I'm like, very much, I want the private companies. So in no part of this matrix,
[00:08:06.080 --> 00:08:09.840]   this is obviously true that the government-led project is better than the private project.
[00:08:09.840 --> 00:08:11.840]   Let's talk about the government project a little bit and checks and balances.
[00:08:11.840 --> 00:08:15.760]   In some sense, I think my argument is a sort of Birkin argument, which is like,
[00:08:15.760 --> 00:08:20.960]   American checks and balances have held for over 200 years and through crazy technological
[00:08:20.960 --> 00:08:24.560]   revolutions. The US military could kill every civilian in the United States.
[00:08:24.560 --> 00:08:28.000]   But you're going to make that argument, the private-public balance of power has held for
[00:08:28.000 --> 00:08:30.640]   hundreds of years, and then like-- Corporate-- but yeah, why has it held?
[00:08:30.640 --> 00:08:34.720]   Because the government has the biggest guns. And has never before has a single CEO or a
[00:08:34.720 --> 00:08:39.280]   random nonprofit board had the ability to launch nukes. The United States National Security State
[00:08:39.280 --> 00:08:42.960]   is going to be intimately involved with this. And this will-- the labs, whether-- and I think,
[00:08:42.960 --> 00:08:45.920]   again, the government-- a lot of what I think is the government project looks like it is basically
[00:08:45.920 --> 00:08:50.400]   a joint venture between the cloud providers, between some of the labs and the government.
[00:08:50.400 --> 00:08:55.280]   And so I think there is no world in which the government isn't intimately involved in this
[00:08:55.280 --> 00:08:59.360]   crazy period. At the very least, basically, the intelligence agencies need to be running security
[00:08:59.360 --> 00:09:02.000]   for these labs. So they're already kind of like-- they're controlling everything. They're
[00:09:02.000 --> 00:09:05.840]   controlling access to everything. Then they're going to be like-- probably, again, if we're in
[00:09:05.840 --> 00:09:10.240]   this really volatile international situation, a lot of the initial applications, it'll suck.
[00:09:10.240 --> 00:09:14.880]   It's not what I want to use ASI for. We'll be trying to somehow stabilize this crazy situation.
[00:09:14.880 --> 00:09:20.480]   Somehow, we need to prevent proliferation of some crazy new WMDs and the undermining of
[00:09:20.480 --> 00:09:28.720]   mutually assured destruction to North Korea and Russia and China. And so I think-- I basically
[00:09:28.720 --> 00:09:33.920]   think your world-- I think there's much more spectrum than you're acknowledging here. And I
[00:09:33.920 --> 00:09:37.680]   think, basically, the world in which it's private labs is extremely heavy government involvement.
[00:09:37.680 --> 00:09:41.920]   And really, what we're debating is what form of government project. But it is going to look much
[00:09:41.920 --> 00:09:46.560]   more like the national security state than anything it does look like, like a startup,
[00:09:46.560 --> 00:09:50.560]   as it is right now. Look, I think something like that makes sense. I would be-- if it's
[00:09:50.560 --> 00:09:55.440]   like the Manhattan Project, then I'm very worried, where it's like this is part of the US military,
[00:09:56.800 --> 00:10:00.800]   where if it's more like, "Listen, you've got to talk to Jake Sullivan before you run the
[00:10:00.800 --> 00:10:04.480]   next training run." It's like Lockheed Martin, Skunk Word's part of the US military. It's like,
[00:10:04.480 --> 00:10:08.640]   they call the shots. Yeah, I don't think that's great. I think that's bad. I think it would be
[00:10:08.640 --> 00:10:12.880]   bad if that happened with ASI. What is the alternative? Okay, so it's closer to my end
[00:10:12.880 --> 00:10:17.040]   of the spectrum where, yeah, you do have to talk to Jake Sullivan before you can launch the next
[00:10:17.040 --> 00:10:21.760]   training cluster. Yeah. But there's many companies who are still going for it. Yeah. And the
[00:10:21.760 --> 00:10:26.800]   government will be intimately involved in the security. But the AI is still broadly deployed,
[00:10:26.800 --> 00:10:31.120]   and alignment works in the sense that you can make sure that it's not-- the system level
[00:10:31.120 --> 00:10:34.240]   prompt is like, you can't help people make bioweapons or something. Sure, sure, sure. I mean,
[00:10:34.240 --> 00:10:37.120]   I expect the AIs to be broadly deployed. I mean, first of all-- Even if it's a government project?
[00:10:37.120 --> 00:10:39.760]   Yeah, I mean, look, I think, first of all, like, I think the Meadows of the world, you know,
[00:10:39.760 --> 00:10:43.280]   open sourcing their AIs, you know, that are two years behind or whatever, yeah, super valuable
[00:10:43.280 --> 00:10:47.120]   role. They're going to like, you know, and so there's going to be some question of like, either
[00:10:47.120 --> 00:10:51.280]   the offense-defense balance is fine. And so like, even if they open source two-year-old AIs, it's
[00:10:51.280 --> 00:10:54.480]   fine. Or it's like, there's some restrictions on the most extreme dual-use capabilities. Like,
[00:10:54.480 --> 00:10:58.320]   you know, you don't let private companies sell kind of crazy weapons. And that's great, and that
[00:10:58.320 --> 00:11:01.680]   will help with the diffusion. I mean, I think the sort of alignment angle during the intelligence
[00:11:01.680 --> 00:11:04.960]   explosion, it's going to, you know, it's not a process of like, years of bureaucracy, and then
[00:11:04.960 --> 00:11:09.280]   you can kind of write some standards. I think it looks much more like basically a war. And like,
[00:11:09.280 --> 00:11:12.880]   you have a fog of war. It's like, look, it's like, is it safe to do the next oom? You know,
[00:11:12.880 --> 00:11:16.320]   and it's like, ah, you know, like, you know, we're like three ooms into the intelligence explosion.
[00:11:16.320 --> 00:11:22.000]   We don't really understand what's going on anymore. You know, the, you know, like a bunch of
[00:11:22.000 --> 00:11:25.760]   our like, generalization scaling curves are like, kind of looking not great. You know, some of our
[00:11:25.760 --> 00:11:28.880]   like, automated AI researchers that are doing alignment are saying it's fine, but we don't
[00:11:28.880 --> 00:11:33.440]   quite trust them. In this test, you know, the like, the AIs started doing naughty things, and
[00:11:33.440 --> 00:11:37.600]   ah, but then we like, hammered it out, and then it was fine. And like, ah, should we, should we go
[00:11:37.600 --> 00:11:41.120]   ahead? Should we take, you know, another six months? Also, by the way, you know, like, China
[00:11:41.120 --> 00:11:44.160]   just stole the weights, or we, you know, they're about to like, deploy the robo army. Like, what
[00:11:44.160 --> 00:11:51.280]   do we do? I think it's this, I think it is this crazy situation, um, and um, you know, basically,
[00:11:51.280 --> 00:11:56.000]   you, you are relying much more on kind of like, a sane chain of command than you are on sort of
[00:11:56.000 --> 00:11:59.680]   some like, you know, deliberative regulatory scheme. I wish you had, you were able to do the
[00:11:59.680 --> 00:12:03.200]   deliberative regulatory scheme. And this is the thing about the private companies too. I don't
[00:12:03.200 --> 00:12:10.000]   think, you know, they all claim they're going to do safety, but I think it's really rough when
[00:12:10.000 --> 00:12:14.480]   you're in the commercial race and they're startups, you know, and startups, startups are startups.
[00:12:14.480 --> 00:12:16.720]   You know, I think they're not fit to handle WMDs.
[00:12:16.880 --> 00:12:17.460]   [END]
[00:12:17.460 --> 00:12:27.460]   [BLANK_AUDIO]

