
[00:00:00.000 --> 00:00:04.280]   So I'm Lee Redden, co-founder and CTO of Blue River Technology.
[00:00:04.280 --> 00:00:08.520]   We're a company that builds big agriculture equipment.
[00:00:08.520 --> 00:00:13.320]   We started out building this bot, which we
[00:00:13.320 --> 00:00:15.280]   won't be able to see right now.
[00:00:15.280 --> 00:00:18.880]   But it went through--
[00:00:18.880 --> 00:00:22.320]   I can't even go and do request access.
[00:00:22.320 --> 00:00:26.080]   But it went through, and it had cameras.
[00:00:26.080 --> 00:00:28.200]   And it's pulled behind a tractor.
[00:00:28.200 --> 00:00:31.480]   It is in several forms 20 feet wide,
[00:00:31.480 --> 00:00:35.480]   but is now 60 feet wide and goes six miles an hour.
[00:00:35.480 --> 00:00:39.260]   And it has cameras that detect every individual plant.
[00:00:39.260 --> 00:00:41.920]   And then in less than 80 milliseconds,
[00:00:41.920 --> 00:00:45.280]   it processes the image, determines what's a weed
[00:00:45.280 --> 00:00:47.160]   and what's a crop.
[00:00:47.160 --> 00:00:49.040]   It then has a bank of actuators that
[00:00:49.040 --> 00:00:54.320]   go through and actuate and spray and kill just the weeds.
[00:00:54.320 --> 00:00:57.280]   Then it has a second camera system beyond then
[00:00:57.280 --> 00:01:00.160]   that takes images of how it acted.
[00:01:00.160 --> 00:01:04.840]   And continuously, every 50 feet, it collects several images
[00:01:04.840 --> 00:01:06.320]   and analyzes how it acted.
[00:01:06.320 --> 00:01:09.880]   And it adjusts for its own internal parameters,
[00:01:09.880 --> 00:01:14.960]   such as the spacing of the nozzles.
[00:01:14.960 --> 00:01:17.880]   And this makes sure that it maintains centimeter level
[00:01:17.880 --> 00:01:22.000]   accuracy as it goes six to 10 miles an hour
[00:01:22.000 --> 00:01:27.120]   and being 60 feet wide with hundreds of actuators,
[00:01:27.120 --> 00:01:34.520]   several dozen cameras on board, and maintains that accuracy
[00:01:34.520 --> 00:01:35.800]   as it goes through the fields.
[00:01:35.800 --> 00:01:36.920]   We built a whole fleet.
[00:01:36.920 --> 00:01:39.620]   And it operated for some amount of time
[00:01:39.620 --> 00:01:43.720]   on around 10% of the lettuce in the United States.
[00:01:43.720 --> 00:01:46.640]   And now we're transferring that over.
[00:01:46.640 --> 00:01:50.160]   And one of the big things that happened to us
[00:01:50.160 --> 00:01:52.600]   was we went through--
[00:01:52.600 --> 00:01:57.320]   the company was started in 2011.
[00:01:57.320 --> 00:02:04.640]   And in 2011, there was only the top way to do machine learning.
[00:02:04.640 --> 00:02:08.600]   And we started in lettuce thinning.
[00:02:08.600 --> 00:02:14.000]   And in about 2015 or so, we switched over
[00:02:14.000 --> 00:02:16.640]   to do deep learning.
[00:02:16.640 --> 00:02:19.440]   And that enabled us to do weeding.
[00:02:19.440 --> 00:02:21.760]   And one of the big differences here
[00:02:21.760 --> 00:02:26.360]   was that we knew in 2011 that the accuracy of the computer
[00:02:26.360 --> 00:02:31.520]   vision models wasn't good enough to do weeding applications.
[00:02:31.520 --> 00:02:33.320]   So we'd gone through, and we had benchmarked,
[00:02:33.320 --> 00:02:34.760]   and we had figured.
[00:02:34.760 --> 00:02:38.080]   And the only reason to talk about any of the things
[00:02:38.080 --> 00:02:40.500]   that we're going to talk about, the only reason
[00:02:40.500 --> 00:02:42.800]   to do deep learning, the only reason to do any of these
[00:02:42.800 --> 00:02:45.200]   things, is because you want your model
[00:02:45.200 --> 00:02:47.680]   to get to be more accurate.
[00:02:47.680 --> 00:02:52.520]   And so there's some threshold where, if you're an entrepreneur,
[00:02:52.520 --> 00:02:55.720]   you're going to be deciding, can I start my business?
[00:02:55.720 --> 00:02:58.800]   Is this good enough to actually launch a product off of?
[00:02:58.800 --> 00:03:00.120]   Or is it not?
[00:03:00.120 --> 00:03:03.960]   And for us, in 2011, when we came out,
[00:03:03.960 --> 00:03:07.320]   we found that the classical computer vision models
[00:03:07.320 --> 00:03:10.240]   were good enough to do one application, lettuce thinning.
[00:03:10.240 --> 00:03:12.640]   But were not good enough to do lettuce weeding.
[00:03:12.640 --> 00:03:14.120]   And so as time went on--
[00:03:14.120 --> 00:03:16.960]   [INAUDIBLE]
[00:03:16.960 --> 00:03:24.640]   Yeah, so lettuce thinning is one where all of the same plant
[00:03:24.640 --> 00:03:26.000]   are in the field.
[00:03:26.000 --> 00:03:29.280]   And you have to differentiate in between individual plants that
[00:03:29.280 --> 00:03:31.080]   are all the same species.
[00:03:31.080 --> 00:03:34.000]   And weeding, you start to have a jumbled mess
[00:03:34.000 --> 00:03:37.440]   of many different types of plants throughout the field.
[00:03:37.440 --> 00:03:43.400]   And so in one case, you have a very regular pattern of plants.
[00:03:43.400 --> 00:03:45.480]   And you can depend on this, and you can depend on it
[00:03:45.480 --> 00:03:47.880]   being a certain size.
[00:03:47.880 --> 00:03:50.880]   In weeding applications, you start
[00:03:50.880 --> 00:03:55.560]   to get plants that vary in size much more drastically.
[00:03:55.560 --> 00:03:59.040]   They also vary in how they look much more drastically.
[00:03:59.040 --> 00:04:04.200]   And so as we came through and we were taking a look at this,
[00:04:04.200 --> 00:04:08.600]   we found that we really needed the more controlled area
[00:04:08.600 --> 00:04:12.800]   of thinning to actually start and launch the business into.
[00:04:12.800 --> 00:04:17.200]   And so I'd propose that one of the big shifts that
[00:04:17.200 --> 00:04:20.720]   has happened is this.
[00:04:20.720 --> 00:04:25.480]   But I think that you can now go out
[00:04:25.480 --> 00:04:28.440]   and you can take new training data.
[00:04:28.440 --> 00:04:32.040]   So say we had a model--
[00:04:32.040 --> 00:04:36.600]   we were wanting a model to go and work in Texas, for example.
[00:04:36.600 --> 00:04:41.540]   And we hadn't been in hail storm cotton before.
[00:04:41.540 --> 00:04:44.400]   And so we could go and we could collect data.
[00:04:44.400 --> 00:04:48.400]   We could have it labeled that had hail-damaged cotton.
[00:04:48.400 --> 00:04:50.960]   We could then put that into our training data,
[00:04:50.960 --> 00:04:54.120]   train the model on it, not do a bunch of model tweaks,
[00:04:54.120 --> 00:04:56.640]   not do new hyperparameter searches,
[00:04:56.640 --> 00:04:58.840]   not do any new engineering work there.
[00:04:58.840 --> 00:05:00.840]   And then on the other end, reliably
[00:05:00.840 --> 00:05:03.080]   expect a model to come out that would
[00:05:03.080 --> 00:05:05.520]   work on hail-damaged cotton.
[00:05:05.520 --> 00:05:11.820]   And so big question is, how true is that flow?
[00:05:11.820 --> 00:05:14.540]   It's not 0% true and it's not 100% true.
[00:05:14.540 --> 00:05:16.300]   It's somewhere in there.
[00:05:16.300 --> 00:05:19.460]   Just like all of these models, they're not 0% accurate.
[00:05:19.460 --> 00:05:20.620]   They're not 100% accurate.
[00:05:20.620 --> 00:05:24.460]   They're somewhere in there in how accurate they are.
[00:05:24.460 --> 00:05:30.260]   And so for me, a lot of this is about keening in
[00:05:30.260 --> 00:05:34.100]   on exactly what percent accurate you're at.
[00:05:34.100 --> 00:05:36.500]   You're taking step by step.
[00:05:36.500 --> 00:05:40.140]   You're doing step by step experiments over many months
[00:05:40.140 --> 00:05:46.220]   to figure out how to increase that accuracy,
[00:05:46.220 --> 00:05:50.580]   and especially if you're looking at the entrepreneurship track
[00:05:50.580 --> 00:05:53.940]   where you need a model to get to be above a certain percent
[00:05:53.940 --> 00:05:56.100]   to make the whole business valid.
[00:05:56.100 --> 00:05:57.860]   Otherwise, if it's below a certain percent,
[00:05:57.860 --> 00:05:59.200]   the whole business is not valid.
[00:06:03.000 --> 00:06:07.240]   And so I'm just going to compare the journey a little bit
[00:06:07.240 --> 00:06:09.800]   to one that--
[00:06:09.800 --> 00:06:14.480]   who has seen the documentary?
[00:06:14.480 --> 00:06:18.160]   So I don't want to ruin anything, but he lives.
[00:06:18.160 --> 00:06:21.200]   And he soloed El Cap.
[00:06:21.200 --> 00:06:24.360]   And so if you're--
[00:06:24.360 --> 00:06:24.840]   [INAUDIBLE]
[00:06:25.480 --> 00:06:25.980]   [INAUDIBLE]
[00:06:25.980 --> 00:06:40.120]   So before deep learning, we were probably around 80% accurate
[00:06:40.120 --> 00:06:42.880]   on weeding on that transfer over.
[00:06:42.880 --> 00:06:46.640]   We're in the 90-some percent on thinning.
[00:06:46.640 --> 00:06:50.640]   There was a lot less penalty for not
[00:06:50.640 --> 00:06:53.440]   being accurate in the previous application, which
[00:06:53.440 --> 00:06:56.040]   really enabled it.
[00:06:56.040 --> 00:07:00.360]   Post deep learning, we strive in the 90%, 95%.
[00:07:00.360 --> 00:07:05.120]   And then you can make up some of ambiguous or bad decisions
[00:07:05.120 --> 00:07:08.640]   based on your action characteristics.
[00:07:08.640 --> 00:07:14.640]   That's a good question.
[00:07:14.640 --> 00:07:18.760]   And so how do you solo El Cap?
[00:07:18.760 --> 00:07:20.260]   Well, if you want to start soloing,
[00:07:20.260 --> 00:07:22.040]   you've got to climb small mountains.
[00:07:22.040 --> 00:07:23.840]   You've got to use gear.
[00:07:23.840 --> 00:07:26.280]   You should study the mountain in lots of detail
[00:07:26.280 --> 00:07:28.280]   from the ground.
[00:07:28.280 --> 00:07:30.540]   You should study the mountains and the routes
[00:07:30.540 --> 00:07:34.440]   when you're actually on the mountain, actually climbing.
[00:07:34.440 --> 00:07:36.120]   You should climb with a friend with gear.
[00:07:36.120 --> 00:07:37.760]   You should do this a lot.
[00:07:37.760 --> 00:07:41.600]   Then if you've done all those things and you feel very ready,
[00:07:41.600 --> 00:07:44.360]   you should consider free soloing El Cap.
[00:07:47.200 --> 00:07:52.400]   And so how do you train neural networks?
[00:07:52.400 --> 00:07:54.080]   You should train with small models.
[00:07:54.080 --> 00:07:57.640]   You should use visualizations.
[00:07:57.640 --> 00:07:59.760]   You should come back and you should study the data
[00:07:59.760 --> 00:08:01.880]   before you start training.
[00:08:01.880 --> 00:08:04.120]   You should study it in detail.
[00:08:04.120 --> 00:08:07.600]   You should study the data while you're training.
[00:08:07.600 --> 00:08:09.480]   You should train with lots of visualizations
[00:08:09.480 --> 00:08:12.280]   in the debugging.
[00:08:12.280 --> 00:08:15.560]   And when you're really sure, you can tune and squeeze,
[00:08:15.560 --> 00:08:19.200]   and you can do all these kind of final steps with the model that
[00:08:19.200 --> 00:08:20.760]   aren't like--
[00:08:20.760 --> 00:08:23.200]   you're pretty sure that you're going to get a slight
[00:08:23.200 --> 00:08:25.920]   performance increase at the very end,
[00:08:25.920 --> 00:08:27.200]   but they're engineering steps.
[00:08:27.200 --> 00:08:30.860]   They're time consuming, and you can squeeze that last bit
[00:08:30.860 --> 00:08:32.240]   of performance out.
[00:08:32.240 --> 00:08:34.400]   But you should definitely go through the first steps
[00:08:34.400 --> 00:08:35.920]   to begin with.
[00:08:35.920 --> 00:08:39.720]   And so if you're going to solo, you
[00:08:39.720 --> 00:08:41.920]   should climb smaller mountains.
[00:08:41.920 --> 00:08:45.540]   If you look at climbers, they get really used to their gear.
[00:08:45.540 --> 00:08:46.880]   They know how their shoe works.
[00:08:46.880 --> 00:08:52.440]   They know how these things that clip in to the wall works.
[00:08:52.440 --> 00:08:54.480]   If you're going to start to train neural networks,
[00:08:54.480 --> 00:08:57.360]   you should really understand your gear.
[00:08:57.360 --> 00:08:58.720]   This is backprop.
[00:08:58.720 --> 00:09:02.920]   That's like one of your fundamental components.
[00:09:02.920 --> 00:09:06.560]   You should run backprop with just a single iteration
[00:09:06.560 --> 00:09:10.440]   and make sure that your loss does what you think it should.
[00:09:10.440 --> 00:09:13.120]   So you form a hypothesis beforehand.
[00:09:13.120 --> 00:09:14.200]   You run the experiment.
[00:09:14.200 --> 00:09:17.460]   You can calculate out what you think the loss should
[00:09:17.460 --> 00:09:21.640]   be when you just do initialization.
[00:09:21.640 --> 00:09:24.200]   You should get a simplified baseline,
[00:09:24.200 --> 00:09:27.420]   and you should make sure that you can really overfit.
[00:09:27.420 --> 00:09:32.400]   And so in some of the images that we've done,
[00:09:32.400 --> 00:09:33.720]   we'll have labels.
[00:09:33.720 --> 00:09:37.320]   And so all your data, you'll have labels somewhere in there.
[00:09:37.320 --> 00:09:41.480]   And what you could do to make a very simplified baseline is
[00:09:41.480 --> 00:09:46.320]   you could take your output, and you
[00:09:46.320 --> 00:09:48.560]   can mix your label in there.
[00:09:48.560 --> 00:09:51.280]   And you can do that in ever harder and harder ways.
[00:09:51.280 --> 00:09:52.880]   If you're doing images, you can just
[00:09:52.880 --> 00:09:55.840]   put a little bit of a trace over the image,
[00:09:55.840 --> 00:09:58.600]   or you could put a big trace over the image
[00:09:58.600 --> 00:09:59.960]   and see if it still trains.
[00:09:59.960 --> 00:10:02.320]   This makes a very simplified baseline
[00:10:02.320 --> 00:10:05.680]   that you can come out.
[00:10:05.680 --> 00:10:14.720]   And so in this, be very keyed in on a hypothesis
[00:10:14.720 --> 00:10:16.600]   that you can pull out that you can then
[00:10:16.600 --> 00:10:18.800]   test with your neural network.
[00:10:18.800 --> 00:10:27.760]   If you're going to solo LCAP, you
[00:10:27.760 --> 00:10:30.720]   should really study the routes from the ground.
[00:10:30.720 --> 00:10:32.480]   You should look at what others have done.
[00:10:35.100 --> 00:10:38.200]   And if you're going to be an entrepreneur
[00:10:38.200 --> 00:10:40.120]   and you're relying on this model,
[00:10:40.120 --> 00:10:42.660]   a big question before you start is
[00:10:42.660 --> 00:10:44.840]   if you can make it to the top.
[00:10:44.840 --> 00:10:47.560]   Is this actually a number that you should then
[00:10:47.560 --> 00:10:53.460]   go through the steps three, four, five to get to?
[00:10:53.460 --> 00:10:57.600]   So there's some other questions you can ask about that.
[00:10:57.600 --> 00:11:01.400]   When you're training neural networks,
[00:11:01.400 --> 00:11:04.520]   you should really just study your data
[00:11:04.520 --> 00:11:08.280]   for an extended period of time before you pick up
[00:11:08.280 --> 00:11:10.520]   the rest of the tools.
[00:11:10.520 --> 00:11:15.480]   Oftentimes, I'll take the data and I'll
[00:11:15.480 --> 00:11:20.120]   flip through images for hours.
[00:11:20.120 --> 00:11:22.780]   I'll go in and I'll label a batch myself.
[00:11:22.780 --> 00:11:24.800]   I'll go see and make sure.
[00:11:24.800 --> 00:11:28.440]   And what I'm often keying in on is something
[00:11:28.440 --> 00:11:29.640]   that my mind is doing.
[00:11:29.640 --> 00:11:33.440]   Like, am I able to pick up how I would detect things
[00:11:33.440 --> 00:11:34.840]   in these images?
[00:11:34.840 --> 00:11:38.040]   How much does it matter of the bigger receptive fields
[00:11:38.040 --> 00:11:40.120]   or the smaller blocks?
[00:11:40.120 --> 00:11:45.280]   How much is it color cues that I'm picking up on?
[00:11:45.280 --> 00:11:47.680]   Are there biases or shifts?
[00:11:47.680 --> 00:11:50.640]   Are there two classes of data that I'm really
[00:11:50.640 --> 00:11:52.440]   trying to distinguish?
[00:11:52.440 --> 00:11:54.480]   What comes out on the tails?
[00:11:54.480 --> 00:11:56.120]   And I'll do all of this before picking up
[00:11:56.120 --> 00:11:59.600]   any of the training tools so that I can really, really
[00:11:59.600 --> 00:12:03.240]   understand the data that I'm going
[00:12:03.240 --> 00:12:06.480]   to be working with beforehand.
[00:12:06.480 --> 00:12:12.480]   Then you move to the mountain.
[00:12:12.480 --> 00:12:13.320]   Start to climb.
[00:12:13.320 --> 00:12:14.720]   You start to move around.
[00:12:14.720 --> 00:12:16.760]   Start to see how accurate the routes are
[00:12:16.760 --> 00:12:18.640]   compared to what you thought.
[00:12:18.640 --> 00:12:20.760]   How does your body work on those routes?
[00:12:20.760 --> 00:12:23.200]   You're visualizing every detail and you're
[00:12:23.200 --> 00:12:25.960]   making sure that your mental model is
[00:12:25.960 --> 00:12:28.820]   correct with how you climb.
[00:12:28.820 --> 00:12:33.620]   And so next step for training neural networks
[00:12:33.620 --> 00:12:38.120]   is to go through and have this simple baseline.
[00:12:38.120 --> 00:12:38.640]   Come back.
[00:12:38.640 --> 00:12:41.120]   Make sure that it's working for you.
[00:12:41.120 --> 00:12:46.120]   Go in and initialize as well as possible.
[00:12:46.120 --> 00:12:49.480]   And so we often see these loss curves that just go like--
[00:12:49.480 --> 00:12:50.560]   they start really high.
[00:12:50.560 --> 00:12:52.680]   They start with a terrible loss and then they just
[00:12:52.680 --> 00:12:53.920]   drop right away.
[00:12:56.780 --> 00:12:58.880]   And what you can do is you could go in
[00:12:58.880 --> 00:13:01.080]   if you know that your output is going
[00:13:01.080 --> 00:13:03.480]   to have a certain bias to it.
[00:13:03.480 --> 00:13:04.800]   You can just pre-compute that.
[00:13:04.800 --> 00:13:07.520]   You can have that be your initial output.
[00:13:07.520 --> 00:13:12.400]   You don't have to go in and have it learn all of those weights.
[00:13:12.400 --> 00:13:15.800]   You can start from a much better initialization there.
[00:13:15.800 --> 00:13:17.360]   You can test on your training data.
[00:13:17.360 --> 00:13:21.160]   This is going to be the best case
[00:13:21.160 --> 00:13:23.200]   that you can expect for your model to ever get.
[00:13:26.240 --> 00:13:29.260]   And when you're going through and you're benchmarking,
[00:13:29.260 --> 00:13:31.820]   can I get a very high performance model,
[00:13:31.820 --> 00:13:35.140]   being able to go through and test on your training data
[00:13:35.140 --> 00:13:39.680]   will give you your highest level accuracy.
[00:13:39.680 --> 00:13:42.460]   Increase the precision of your model.
[00:13:42.460 --> 00:13:46.340]   Use extended precision and all of this
[00:13:46.340 --> 00:13:51.860]   on much more test examples than you would regularly use.
[00:13:54.860 --> 00:13:57.620]   Use Adam as your optimizer also.
[00:13:57.620 --> 00:14:05.140]   So if you're going to climb, don't be a hero.
[00:14:05.140 --> 00:14:07.420]   You're learning with gear here.
[00:14:07.420 --> 00:14:10.100]   You could fall off the mountain.
[00:14:10.100 --> 00:14:11.300]   Wear a helmet.
[00:14:11.300 --> 00:14:12.460]   Use a belay device.
[00:14:12.460 --> 00:14:15.140]   Keep all of this stuff intact.
[00:14:15.140 --> 00:14:16.640]   Think very carefully if you actually
[00:14:16.640 --> 00:14:19.060]   want to go and free solo this mountain.
[00:14:19.060 --> 00:14:22.020]   [SIDE CONVERSATION]
[00:14:22.020 --> 00:14:41.860]   If you're going to train neural networks here,
[00:14:41.860 --> 00:14:47.380]   there's a couple of big steps that you'd want to keep.
[00:14:47.380 --> 00:14:51.460]   You want to make sure that you're
[00:14:51.460 --> 00:14:53.740]   caching the models correctly.
[00:14:53.740 --> 00:14:58.740]   And so you look at--
[00:14:58.740 --> 00:15:01.220]   oftentimes, there's a guaranteed way--
[00:15:01.220 --> 00:15:03.340]   I found there's a lot of fanciness
[00:15:03.340 --> 00:15:05.820]   that people put in on the math side
[00:15:05.820 --> 00:15:09.060]   to how to get better models, how to understand bias and variance
[00:15:09.060 --> 00:15:14.340]   and things like this, how to use GANs for data augmentation
[00:15:14.340 --> 00:15:16.580]   and stuff like that.
[00:15:16.580 --> 00:15:20.900]   What I found is if you really want to get better models,
[00:15:20.900 --> 00:15:24.660]   get more training data, figure out the augmentations that
[00:15:24.660 --> 00:15:27.660]   work, don't take those and don't put those
[00:15:27.660 --> 00:15:28.780]   in at the very beginning.
[00:15:28.780 --> 00:15:31.780]   Keep those till the end.
[00:15:31.780 --> 00:15:35.360]   You have many tricks out there that you can use.
[00:15:35.360 --> 00:15:37.900]   You don't have to start with all of them in there.
[00:15:37.900 --> 00:15:39.900]   Some of them are just very nice engineering ones
[00:15:39.900 --> 00:15:44.100]   that you can toss in at a much later stage.
[00:15:44.100 --> 00:15:47.820]   Image augmentation, for one example,
[00:15:47.820 --> 00:15:51.140]   is one where you go through.
[00:15:51.140 --> 00:15:55.940]   And it's like simulation, but it's just much better.
[00:15:55.940 --> 00:15:58.940]   You start with the world as it is.
[00:15:58.940 --> 00:16:01.380]   You start with one of these images as it is.
[00:16:01.380 --> 00:16:04.860]   And then you augment in several different ways.
[00:16:04.860 --> 00:16:07.700]   And if you start to look through your data
[00:16:07.700 --> 00:16:13.860]   and you start with the step of just analyzing it in depth,
[00:16:13.860 --> 00:16:16.020]   you really start to understand how you can go through
[00:16:16.020 --> 00:16:18.460]   and you can augment this data in ways
[00:16:18.460 --> 00:16:20.500]   that is actually meaningful.
[00:16:20.500 --> 00:16:23.740]   And then you'll come to the question of you're
[00:16:23.740 --> 00:16:27.300]   going to put in some parameter for how much augmentation
[00:16:27.300 --> 00:16:28.380]   to have.
[00:16:28.380 --> 00:16:38.180]   And so you take an image and you're adding in white noise
[00:16:38.180 --> 00:16:39.540]   to it.
[00:16:39.540 --> 00:16:43.900]   So pixels, for example, on a camera
[00:16:43.900 --> 00:16:47.060]   are all susceptible to a certain amount of noise.
[00:16:47.060 --> 00:16:50.340]   And so you have a thought, OK, I'm going to go
[00:16:50.340 --> 00:16:53.340]   and I'm going to augment all these pixels with noise.
[00:16:53.340 --> 00:16:58.260]   So now you have to choose what sort of variance you want to add
[00:16:58.260 --> 00:17:02.300]   and you want to add a bias to it and what sort of results
[00:17:02.300 --> 00:17:04.580]   are going to give you the best ones.
[00:17:04.580 --> 00:17:07.500]   And at that point, you come back.
[00:17:07.500 --> 00:17:10.860]   And this isn't till--
[00:17:10.860 --> 00:17:15.700]   we're one step away from free-soloing El Capier.
[00:17:15.700 --> 00:17:18.260]   This is very far along in the process.
[00:17:18.260 --> 00:17:20.520]   This is when you're starting to get to that squeeze
[00:17:20.520 --> 00:17:23.380]   and tune portion.
[00:17:23.380 --> 00:17:26.940]   But you'll come back and you know that your gear works.
[00:17:26.940 --> 00:17:30.140]   You know backprop works exactly how you want.
[00:17:30.140 --> 00:17:33.060]   You know that you've got the initialization,
[00:17:33.060 --> 00:17:34.740]   so you don't get this crazy hockey stick
[00:17:34.740 --> 00:17:36.100]   loss at the very beginning.
[00:17:36.100 --> 00:17:37.540]   It starts kind of--
[00:17:37.540 --> 00:17:41.460]   and you've done as good of a job as you can to start it out low.
[00:17:41.460 --> 00:17:44.940]   You know that you're not having any sort of other mismatch
[00:17:44.940 --> 00:17:46.780]   coming in.
[00:17:46.780 --> 00:17:50.740]   But then you'll come and you'll take a step like augmentation.
[00:17:50.740 --> 00:17:53.220]   And you'll be making this decision about where
[00:17:53.220 --> 00:17:55.940]   your parameter, how much variance you want on it.
[00:17:55.940 --> 00:17:58.860]   And so you'll go through and you'll
[00:17:58.860 --> 00:18:02.100]   do this step by step of you'll run an experiment.
[00:18:02.100 --> 00:18:03.820]   You'll run it across.
[00:18:03.820 --> 00:18:06.180]   You'll have a hypothesis at the beginning.
[00:18:06.180 --> 00:18:09.620]   You'll say, yeah, if I tune this too high,
[00:18:09.620 --> 00:18:12.060]   I'm going to get images that are totally blown out
[00:18:12.060 --> 00:18:13.740]   in both directions.
[00:18:13.740 --> 00:18:16.540]   And that's going to obviously be bad.
[00:18:16.540 --> 00:18:21.060]   If I have this number be very small, like zero,
[00:18:21.060 --> 00:18:22.740]   well, that's where I'm at now.
[00:18:22.740 --> 00:18:25.060]   So there's some number in between these two
[00:18:25.060 --> 00:18:27.780]   that I need to go through and choose.
[00:18:27.780 --> 00:18:29.660]   And then you'll run that experiment.
[00:18:29.660 --> 00:18:33.860]   You'll run through different parameters of it.
[00:18:33.860 --> 00:18:36.300]   You'll make a hypothesis beforehand.
[00:18:36.300 --> 00:18:39.220]   And you'll make sure that it tunes just
[00:18:39.220 --> 00:18:41.780]   how you think it would.
[00:18:41.780 --> 00:18:46.620]   There should be some sort of graph that it gets better.
[00:18:46.620 --> 00:18:47.780]   It kind of peaks out.
[00:18:47.780 --> 00:18:50.340]   And then it gets worse.
[00:18:50.340 --> 00:18:53.820]   And if you see that it does something different,
[00:18:53.820 --> 00:18:56.380]   that's a problem.
[00:18:56.380 --> 00:18:59.180]   And when you're really trying to get to that peak performance--
[00:18:59.180 --> 00:19:02.500]   because the only reason that we're running neural networks
[00:19:02.500 --> 00:19:05.940]   is because we want an ever-increasing performance.
[00:19:05.940 --> 00:19:12.780]   And so in this case, you will find this little small failure
[00:19:12.780 --> 00:19:13.660]   there.
[00:19:13.660 --> 00:19:16.340]   And you have to come back and hopefully you
[00:19:16.340 --> 00:19:23.620]   haven't gone through and ran and tossed in 10 things at once.
[00:19:23.620 --> 00:19:26.220]   That's one of the very worst things that you can do,
[00:19:26.220 --> 00:19:29.340]   is go through and toss 10 things in at the same time
[00:19:29.340 --> 00:19:32.060]   and then expect them all to kind of magically sit nice
[00:19:32.060 --> 00:19:34.140]   at the end.
[00:19:34.140 --> 00:19:37.740]   And it's one where when we go through--
[00:19:37.740 --> 00:19:39.820]   when I go through and I build these systems,
[00:19:39.820 --> 00:19:42.460]   you're doing it over a long time.
[00:19:42.460 --> 00:19:47.980]   You're building a system to go out and have max performance.
[00:19:47.980 --> 00:19:52.980]   And that's a big part of also when you take it and ship it
[00:19:52.980 --> 00:19:56.940]   into the world, is that you'll always come back
[00:19:56.940 --> 00:19:59.580]   and you'll be retuning these parameters kind
[00:19:59.580 --> 00:20:01.620]   of for the life of the system.
[00:20:01.620 --> 00:20:04.340]   And if you've gone through and you've understood
[00:20:04.340 --> 00:20:08.580]   each individual component, then you'll
[00:20:08.580 --> 00:20:10.300]   be in just a much better place.
[00:20:10.300 --> 00:20:18.060]   So when you come and you're ready to free solo LCAP,
[00:20:18.060 --> 00:20:19.500]   don't be a hero.
[00:20:19.500 --> 00:20:22.380]   Only move to here if you want to have
[00:20:22.380 --> 00:20:23.260]   state of the art.
[00:20:23.260 --> 00:20:28.620]   When you're really sure and you're
[00:20:28.620 --> 00:20:33.620]   ready to kind of tune and squeeze at the very end,
[00:20:33.620 --> 00:20:34.700]   don't use grid search.
[00:20:34.700 --> 00:20:36.020]   Use random search.
[00:20:36.020 --> 00:20:38.540]   Ensemble methods are very good.
[00:20:38.540 --> 00:20:42.140]   It's always going to take more compute there.
[00:20:42.140 --> 00:20:44.780]   Start to get into extreme amounts of data.
[00:20:44.780 --> 00:20:49.980]   And I think one of the best bets people can make
[00:20:49.980 --> 00:20:53.180]   is on compute.
[00:20:53.180 --> 00:20:56.660]   Anybody who bets against compute almost always loses.
[00:20:56.660 --> 00:21:04.740]   And so come back to kind of a question at the beginning.
[00:21:04.740 --> 00:21:08.300]   If you go in and you start to question
[00:21:08.300 --> 00:21:11.300]   each one of these aspects of how true
[00:21:11.300 --> 00:21:18.540]   is it in this world where if we take hail hit cotton,
[00:21:18.540 --> 00:21:21.460]   and we now include this as part of our training set,
[00:21:21.460 --> 00:21:24.740]   does our model actually generalize to it?
[00:21:24.740 --> 00:21:27.220]   And it's true like some of--
[00:21:27.220 --> 00:21:31.700]   it's not true 100%, and it's not true 0.
[00:21:31.700 --> 00:21:33.740]   What we found is that with deep learning
[00:21:33.740 --> 00:21:36.340]   and these sorts of tools, it tends to be
[00:21:36.340 --> 00:21:38.900]   much more true than false.
[00:21:38.900 --> 00:21:42.340]   It tends towards not having to require all these new
[00:21:42.340 --> 00:21:44.820]   parameters to be re-optimized.
[00:21:44.820 --> 00:21:46.900]   You don't have to maybe go in and re-optimize
[00:21:46.900 --> 00:21:52.820]   your 10 different steps of data augmentation for it.
[00:21:52.820 --> 00:21:57.900]   But it doesn't always turn out to be 100% true.
[00:21:57.900 --> 00:22:02.300]   In robotic systems, we found that--
[00:22:02.300 --> 00:22:07.420]   and this was really not obvious when starting out.
[00:22:07.420 --> 00:22:13.580]   And so you could take each one of these as a single days test.
[00:22:13.580 --> 00:22:15.980]   So we went to the field four times.
[00:22:15.980 --> 00:22:20.820]   We collected data, four different data sets, 0, 1, 2,
[00:22:20.820 --> 00:22:21.780]   3.
[00:22:21.780 --> 00:22:27.820]   And we were looking for how we should do our train test split.
[00:22:27.820 --> 00:22:34.580]   So naturally, take your favorite percentages, 80/20, 80/10/10,
[00:22:34.580 --> 00:22:39.500]   70/15/15, whatever it is, and split up
[00:22:39.500 --> 00:22:42.260]   training data from test data.
[00:22:42.260 --> 00:22:46.180]   And then go through, train naturally on your training data
[00:22:46.180 --> 00:22:50.120]   and see how well you do on your test data.
[00:22:50.120 --> 00:22:54.660]   And this was a major problem.
[00:22:54.660 --> 00:23:00.060]   We would get really good results on this.
[00:23:00.060 --> 00:23:02.060]   And then we would go to the field,
[00:23:02.060 --> 00:23:04.820]   and we would get terrible results.
[00:23:04.820 --> 00:23:08.340]   And then we would come back, and we would
[00:23:08.340 --> 00:23:11.460]   have collected another one.
[00:23:11.460 --> 00:23:13.420]   And we would be working on it.
[00:23:13.420 --> 00:23:15.340]   And we would label it, of course.
[00:23:15.340 --> 00:23:17.180]   And then we would put it in here.
[00:23:17.180 --> 00:23:20.820]   And then we would do good again.
[00:23:20.820 --> 00:23:23.180]   And can anybody see why this is a problem?
[00:23:23.180 --> 00:23:25.620]   You haven't seen the picture, so you
[00:23:25.620 --> 00:23:28.580]   know you're training on one hour of the next day.
[00:23:28.580 --> 00:23:29.080]   Yes.
[00:23:29.080 --> 00:23:30.540]   You don't know the next day.
[00:23:30.540 --> 00:23:31.780]   Yeah.
[00:23:31.780 --> 00:23:37.820]   So this was a major problem because, inadvertently, we
[00:23:37.820 --> 00:23:39.620]   were cheating.
[00:23:39.620 --> 00:23:45.300]   This is not fully training on your test set,
[00:23:45.300 --> 00:23:47.860]   but this isn't keeping them separate.
[00:23:47.860 --> 00:23:51.500]   And so you have some of that next day's data
[00:23:51.500 --> 00:23:53.900]   in with your training data.
[00:23:53.900 --> 00:23:58.900]   And so for robotic systems, we moved to one
[00:23:58.900 --> 00:24:02.100]   where we'll do single holdouts.
[00:24:02.100 --> 00:24:06.980]   We'll take it an entire day, an entire field, an entire batch,
[00:24:06.980 --> 00:24:09.380]   and we'll hold it out completely.
[00:24:09.380 --> 00:24:12.860]   We'll then round-robin this in between.
[00:24:12.860 --> 00:24:17.620]   And that will give us many different data points.
[00:24:17.620 --> 00:24:20.100]   And so instead of just having single instances
[00:24:20.100 --> 00:24:21.740]   of how we're doing--
[00:24:21.740 --> 00:24:25.620]   this is a super old graph--
[00:24:25.620 --> 00:24:28.780]   but we would take all of these different data points,
[00:24:28.780 --> 00:24:30.500]   and we'd plot them.
[00:24:30.500 --> 00:24:35.420]   And what would happen is, if you're down here,
[00:24:35.420 --> 00:24:38.780]   you get a phone call, and the machine is broken.
[00:24:38.780 --> 00:24:41.900]   And this graph, in particular, was back
[00:24:41.900 --> 00:24:44.140]   in the days of pre-deep learning.
[00:24:44.140 --> 00:24:45.820]   And so then I would go into the office,
[00:24:45.820 --> 00:24:48.980]   and I would have to feature engineer all the numbers
[00:24:48.980 --> 00:24:52.820]   to be up and to the right again.
[00:24:52.820 --> 00:24:56.500]   And then you'd move from there.
[00:24:56.500 --> 00:24:58.940]   But in robotic systems, if you start
[00:24:58.940 --> 00:25:03.700]   to do this round-robin style, you can come through,
[00:25:03.700 --> 00:25:08.220]   and you can make, and you can plot each individual point.
[00:25:08.220 --> 00:25:11.860]   Like, your big goal is, take the worst point and move it up.
[00:25:11.860 --> 00:25:13.360]   Take the worst point and move it up.
[00:25:13.360 --> 00:25:15.500]   It's not about moving the average.
[00:25:15.500 --> 00:25:17.820]   It's not about moving the center.
[00:25:17.820 --> 00:25:20.980]   The ones down here are your bad days.
[00:25:20.980 --> 00:25:24.660]   And so you start to get some different metrics,
[00:25:24.660 --> 00:25:27.420]   we found, when working with the robotic systems.
[00:25:27.420 --> 00:25:32.380]   One of the things we were working on
[00:25:32.380 --> 00:25:35.940]   was, how do you make a model?
[00:25:35.940 --> 00:25:38.420]   And how do you actually find these parameters,
[00:25:38.420 --> 00:25:41.260]   this entire setup, the network architecture,
[00:25:41.260 --> 00:25:47.500]   to go through and be as general as possible for a crop
[00:25:47.500 --> 00:25:50.380]   that we weren't able to collect data on?
[00:25:50.380 --> 00:25:57.820]   And this was kind of posed to us because we
[00:25:57.820 --> 00:26:00.860]   wanted to do cotton in America.
[00:26:00.860 --> 00:26:05.100]   And we had cotton data from Australia.
[00:26:05.100 --> 00:26:10.700]   We didn't actually have that yet, but we wanted--
[00:26:10.700 --> 00:26:13.140]   cotton data in Australia is available.
[00:26:13.140 --> 00:26:16.940]   Six months off cotton data in America,
[00:26:16.940 --> 00:26:18.780]   northern southern hemisphere.
[00:26:18.780 --> 00:26:23.900]   And so we went out, and we challenged ourself
[00:26:23.900 --> 00:26:28.300]   to be able to make a model that would generalize
[00:26:28.300 --> 00:26:30.460]   to every individual crop.
[00:26:30.460 --> 00:26:32.900]   And so if you could go through--
[00:26:32.900 --> 00:26:36.260]   and the thought was, if you can tune all of these parameters,
[00:26:36.260 --> 00:26:38.300]   and you can get everything set up
[00:26:38.300 --> 00:26:42.100]   to generalize in between corn and chickpeas,
[00:26:42.100 --> 00:26:44.700]   and in between peppers and lettuce,
[00:26:44.700 --> 00:26:47.060]   then the very same thing will work
[00:26:47.060 --> 00:26:51.340]   if you wanted to generalize in between cotton in Australia
[00:26:51.340 --> 00:26:54.700]   and cotton in the United States.
[00:26:54.700 --> 00:26:59.140]   And that way, we would have had all of these building blocks
[00:26:59.140 --> 00:27:02.300]   set up, kind of as good as we could.
[00:27:02.300 --> 00:27:07.220]   But then when we started to get cotton data from the United
[00:27:07.220 --> 00:27:10.060]   States, so our plan was--
[00:27:10.060 --> 00:27:12.300]   crops are seasonal.
[00:27:12.300 --> 00:27:16.420]   We'll go down, and we'll collect cotton data in the United
[00:27:16.420 --> 00:27:18.020]   States.
[00:27:18.020 --> 00:27:20.540]   And we'll take it.
[00:27:20.540 --> 00:27:23.380]   We'll spin a model over the next couple of days.
[00:27:23.380 --> 00:27:27.700]   We'll deploy it to the field, and we'll see how it works.
[00:27:27.700 --> 00:27:30.120]   And then we'll take the data, we'll deploy it to the field,
[00:27:30.120 --> 00:27:32.500]   and we'll see how it works.
[00:27:32.500 --> 00:27:38.500]   But it was with a data set that we had no prior knowledge of.
[00:27:38.500 --> 00:27:41.940]   We had no data from cotton in Texas at any point.
[00:27:41.940 --> 00:27:47.580]   And so we worked really hard to build all of these blocks,
[00:27:47.580 --> 00:27:51.740]   to kind of use this example of having
[00:27:51.740 --> 00:27:56.260]   a model and the hyperparameters and the augmentation
[00:27:56.260 --> 00:27:58.740]   and everything in place to be able to generalize
[00:27:58.740 --> 00:28:01.300]   in between each one of these classes
[00:28:01.300 --> 00:28:05.260]   so that when we came to a new class, we could go through,
[00:28:05.260 --> 00:28:07.580]   and we could use all those mechanisms.
[00:28:07.580 --> 00:28:09.500]   It's kind of a big pre-training step for us.
[00:28:09.500 --> 00:28:19.980]   Here's some examples of some of the images that come in.
[00:28:19.980 --> 00:28:21.260]   Here's some examples.
[00:28:21.260 --> 00:28:25.220]   I just pulled these from our Slack channel quite a while ago,
[00:28:25.220 --> 00:28:29.980]   but of several challenges that having this bot deployed
[00:28:29.980 --> 00:28:31.980]   in the world gives us.
[00:28:31.980 --> 00:28:35.580]   Gives us tons of weeds coming in,
[00:28:35.580 --> 00:28:39.980]   very hard to detect plants, oversaturation in many cases,
[00:28:39.980 --> 00:28:43.340]   who knows even what to do with the top left.
[00:28:43.340 --> 00:28:48.060]   When building and deploying the product, you have to come.
[00:28:48.060 --> 00:28:49.520]   And when you dig into your data set,
[00:28:49.520 --> 00:28:51.180]   you may find stuff like that.
[00:28:51.180 --> 00:28:53.380]   It's like, does that even fit inside my class?
[00:28:53.380 --> 00:28:55.180]   Should I go through and remove it?
[00:28:55.180 --> 00:28:57.580]   Is this some other pre-step that I do?
[00:28:57.580 --> 00:29:04.460]   One of the things we've found is that as we scale out
[00:29:04.460 --> 00:29:08.900]   these systems, you'll start with--
[00:29:08.900 --> 00:29:11.360]   the vision system that I've talked about the most
[00:29:11.360 --> 00:29:16.700]   is this one where it takes an image of--
[00:29:16.700 --> 00:29:18.140]   it does this.
[00:29:18.140 --> 00:29:19.940]   It takes an image, and it goes through,
[00:29:19.940 --> 00:29:21.980]   and it detects cotton, and it detects weeds,
[00:29:21.980 --> 00:29:23.760]   or it detects lettuce and detects weeds,
[00:29:23.760 --> 00:29:26.540]   or something like that.
[00:29:26.540 --> 00:29:30.900]   But at the same point, that's just actually
[00:29:30.900 --> 00:29:34.780]   a pretty small number of the vision systems on board.
[00:29:34.780 --> 00:29:36.940]   To really build out these systems,
[00:29:36.940 --> 00:29:42.540]   found is every time you want to exponentiate
[00:29:42.540 --> 00:29:44.540]   the number of machines that you're doing,
[00:29:44.540 --> 00:29:47.100]   you have to build twice as many systems.
[00:29:47.100 --> 00:29:51.500]   And so as we've scaled out, you go through
[00:29:51.500 --> 00:29:53.880]   and you build systems that may detect things like this,
[00:29:53.880 --> 00:29:55.880]   and it may detect smudges on the lenses,
[00:29:55.880 --> 00:29:58.280]   and it may detect all of these things
[00:29:58.280 --> 00:30:01.320]   so that you can easily take that data
[00:30:01.320 --> 00:30:03.480]   and then move it into the side bucket.
[00:30:03.480 --> 00:30:05.480]   It's no longer stuff that your machine learning
[00:30:05.480 --> 00:30:07.280]   algorithm has to fight with.
[00:30:07.280 --> 00:30:10.120]   It's now stuff that somebody with a rag
[00:30:10.120 --> 00:30:12.140]   can go through and wipe off a lens
[00:30:12.140 --> 00:30:13.640]   or something of that nature.
[00:30:16.880 --> 00:30:22.380]   We often set up teams, and we set them up
[00:30:22.380 --> 00:30:24.660]   in three different groups.
[00:30:24.660 --> 00:30:26.000]   One of the big reasons to do this
[00:30:26.000 --> 00:30:34.960]   is that we found that these problems are kind of so hard
[00:30:34.960 --> 00:30:37.080]   that you really have to have a full stack
[00:30:37.080 --> 00:30:39.400]   person all the way through.
[00:30:39.400 --> 00:30:42.900]   But where you can have nice boundaries in between people
[00:30:42.900 --> 00:30:45.300]   and nice roles and responsibilities,
[00:30:45.300 --> 00:30:47.540]   you should take advantage of that.
[00:30:47.540 --> 00:30:49.920]   And so we have a data labeling group,
[00:30:49.920 --> 00:30:53.700]   and they take requirements from the machine learning
[00:30:53.700 --> 00:30:59.140]   experts and the product managers for what data to put in place.
[00:30:59.140 --> 00:31:04.420]   The machine learning experts are responsible for the magic
[00:31:04.420 --> 00:31:07.300]   to happen to create the algorithms.
[00:31:07.300 --> 00:31:09.100]   The product managers actually get
[00:31:09.100 --> 00:31:15.740]   to decide what data they want to work with as part
[00:31:15.740 --> 00:31:17.940]   of their validation tests.
[00:31:17.940 --> 00:31:21.500]   And so in this split, you don't necessarily
[00:31:21.500 --> 00:31:24.700]   have the machine learning engineers say that.
[00:31:24.700 --> 00:31:27.220]   It's the product managers that are going through--
[00:31:27.220 --> 00:31:29.820]   done a lot of training and working with individual product
[00:31:29.820 --> 00:31:33.000]   managers to have them be able to talk to machine learning
[00:31:33.000 --> 00:31:36.460]   experts where they can give out precision and recall,
[00:31:36.460 --> 00:31:40.300]   and they can talk in the language and open clature
[00:31:40.300 --> 00:31:44.980]   and actually specify products in a useful way.
[00:31:44.980 --> 00:31:54.540]   And so I wanted to quote one of the sources from Andre's blog.
[00:31:54.540 --> 00:31:56.820]   And thank you, and open it up for questions.
[00:31:59.980 --> 00:32:05.100]   Can you describe in more detail bias initialization
[00:32:05.100 --> 00:32:06.980]   and what techniques you use there?
[00:32:06.980 --> 00:32:07.540]   Yeah.
[00:32:07.540 --> 00:32:11.700]   So in initialization, you can go through
[00:32:11.700 --> 00:32:17.660]   and you can look at the last layer output.
[00:32:17.660 --> 00:32:22.740]   And so if you knew that one of your classes
[00:32:22.740 --> 00:32:27.660]   was weighted like, say, it was 5 to 1, you can go through
[00:32:27.660 --> 00:32:31.300]   and you can initialize one of those classes in a 5 to 1
[00:32:31.300 --> 00:32:32.660]   weighting.
[00:32:32.660 --> 00:32:35.620]   There, you can also go through and--
[00:32:35.620 --> 00:32:45.220]   yeah, that would be the big first one.
[00:32:45.220 --> 00:32:49.820]   You can also look at losses with just random initialization.
[00:32:52.500 --> 00:32:58.260]   So pre-training almost never hurts on a network.
[00:32:58.260 --> 00:32:59.900]   If you can go in--
[00:32:59.900 --> 00:33:02.620]   this isn't like the unsupervised training world.
[00:33:02.620 --> 00:33:07.260]   This is supervised pre-training of a network.
[00:33:07.260 --> 00:33:10.060]   And supervised pre-training is just a great way
[00:33:10.060 --> 00:33:12.620]   to go in and initialize your networks.
[00:33:12.620 --> 00:33:15.380]   And so when you start to get things like batch norm
[00:33:15.380 --> 00:33:20.060]   and dropout and mainly batch norm in place,
[00:33:20.060 --> 00:33:22.740]   having this good initialization at the beginning
[00:33:22.740 --> 00:33:25.620]   helps some of those parameters.
[00:33:25.620 --> 00:33:28.300]   I thought there was something other than-- so it's just,
[00:33:28.300 --> 00:33:30.940]   you train a model, you start with that for the next one,
[00:33:30.940 --> 00:33:33.620]   you train when you transfer.
[00:33:33.620 --> 00:33:34.100]   Yeah.
[00:33:34.100 --> 00:33:34.600]   OK.
[00:33:34.600 --> 00:33:35.100]   That's it.
[00:33:35.100 --> 00:33:42.220]   How did you get to the--
[00:33:42.220 --> 00:33:44.740]   how do you get to convincing people
[00:33:44.740 --> 00:33:48.580]   who use the product of the magic and the--
[00:33:48.580 --> 00:33:52.620]   and deal with the mistakes that the model might make?
[00:33:52.620 --> 00:33:54.700]   How do you convince people with--
[00:33:54.700 --> 00:33:57.820]   Yeah, if you walk up to someone and it is this--
[00:33:57.820 --> 00:34:00.140]   if you haven't heard of some machine learning system
[00:34:00.140 --> 00:34:02.940]   like this, this feels like magic.
[00:34:02.940 --> 00:34:06.500]   How do you get people to buy into that?
[00:34:06.500 --> 00:34:08.700]   And then it's also going to be not 100%
[00:34:08.700 --> 00:34:10.300]   so it's going to make mistakes.
[00:34:10.300 --> 00:34:10.800]   Yes.
[00:34:10.800 --> 00:34:13.540]   How do you deal with that aspect of the product?
[00:34:13.540 --> 00:34:14.180]   Yeah.
[00:34:14.180 --> 00:34:16.820]   I think you might have, depending
[00:34:16.820 --> 00:34:17.980]   on who your audience is--
[00:34:17.980 --> 00:34:19.360]   so there's a group of people that
[00:34:19.360 --> 00:34:21.580]   have an algorithm in place already
[00:34:21.580 --> 00:34:26.140]   and they're just looking for one that has a better performance.
[00:34:26.140 --> 00:34:29.660]   And so that group is one where it's kind of an obvious,
[00:34:29.660 --> 00:34:36.500]   you know, hey, yeah, you like 93%, how about 95%, right?
[00:34:36.500 --> 00:34:41.380]   And so you can get an easy sell into that,
[00:34:41.380 --> 00:34:45.020]   depending on the data pipeline set up around it.
[00:34:45.020 --> 00:34:47.260]   When you go in and you start to talk about a totally
[00:34:47.260 --> 00:34:48.740]   new application, right?
[00:34:48.740 --> 00:34:51.740]   Like we've seen numerous--
[00:34:51.740 --> 00:34:53.460]   especially in the perception world--
[00:34:53.460 --> 00:34:59.620]   numerous opportunities open up that just did not exist before.
[00:34:59.620 --> 00:35:01.740]   Because you see models, you know,
[00:35:01.740 --> 00:35:05.580]   and like you look at ImageNet in 2011
[00:35:05.580 --> 00:35:07.660]   and it's like 30-some percent error, right?
[00:35:07.660 --> 00:35:08.980]   That's not a--
[00:35:08.980 --> 00:35:11.940]   like I was looking at that and I'm like, OK, duh.
[00:35:11.940 --> 00:35:16.500]   You know, that's not good enough for many things.
[00:35:16.500 --> 00:35:20.980]   And so then you may be convincing--
[00:35:20.980 --> 00:35:23.420]   you know, first you have to convince yourself, like,
[00:35:23.420 --> 00:35:25.380]   is this model actually going to move up
[00:35:25.380 --> 00:35:29.540]   to the percent where it does make sense to then launch it
[00:35:29.540 --> 00:35:31.380]   in this application?
[00:35:31.380 --> 00:35:33.500]   And then you get into some of the specific product
[00:35:33.500 --> 00:35:34.700]   side of it.
[00:35:34.700 --> 00:35:38.860]   You know, what-- you know, sometimes pretty costly
[00:35:38.860 --> 00:35:40.580]   to bring some of this stuff up.
[00:35:40.580 --> 00:35:42.620]   There's a lot of infrastructure involved.
[00:35:42.620 --> 00:35:46.140]   Is it going to be worth the investment for--
[00:35:46.140 --> 00:35:48.340]   on the people side to bring this stuff up?
[00:35:48.340 --> 00:35:56.580]   So how did you go through the process
[00:35:56.580 --> 00:35:59.220]   of retrofitting your organization
[00:35:59.220 --> 00:36:02.660]   or engineering pipeline from machine learning
[00:36:02.660 --> 00:36:03.980]   to deep learning?
[00:36:03.980 --> 00:36:06.580]   Where did all those future engineers go?
[00:36:06.580 --> 00:36:07.080]   Right?
[00:36:07.080 --> 00:36:08.580]   You know?
[00:36:08.580 --> 00:36:09.660]   Yeah.
[00:36:09.660 --> 00:36:11.820]   Yeah, so we kept all of the future engineers.
[00:36:11.820 --> 00:36:20.460]   They all became deep learning engineers, including myself.
[00:36:20.460 --> 00:36:25.820]   I mean, this is like the--
[00:36:25.820 --> 00:36:27.540]   so there were some wild days.
[00:36:27.540 --> 00:36:30.060]   So if you look at--
[00:36:30.060 --> 00:36:35.620]   like, you had the AlexNet paper, and you had this VGGNet
[00:36:35.620 --> 00:36:41.340]   for a while, and kind of things pre-ResNet.
[00:36:41.340 --> 00:36:45.380]   It wasn't-- I would not say that these systems acted in ways
[00:36:45.380 --> 00:36:47.540]   that you would expect them to act.
[00:36:47.540 --> 00:36:49.960]   Like, you would go through, and you would do stuff with it,
[00:36:49.960 --> 00:36:54.740]   and it would act in totally unintuitive ways.
[00:36:54.740 --> 00:36:57.180]   You know, you would go in, and you would add a layer,
[00:36:57.180 --> 00:36:58.680]   and it would get better performance.
[00:36:58.680 --> 00:37:00.680]   And then you would add a layer, and it would get worse.
[00:37:00.680 --> 00:37:03.260]   And you would add another layer, and it would get worse.
[00:37:03.260 --> 00:37:05.420]   And you would add another-- and you'd be like,
[00:37:05.420 --> 00:37:07.580]   what's going on here?
[00:37:07.580 --> 00:37:09.260]   And so people at that time--
[00:37:09.500 --> 00:37:11.860]   and deep learning, it was--
[00:37:11.860 --> 00:37:15.940]   the nomenclature was more than seven layers.
[00:37:15.940 --> 00:37:19.900]   Now it's kind of synonymous with neural networks.
[00:37:19.900 --> 00:37:21.380]   But at that time, it was like, yeah,
[00:37:21.380 --> 00:37:24.780]   if you got more than seven layers to train,
[00:37:24.780 --> 00:37:26.400]   you were doing deep learning.
[00:37:26.400 --> 00:37:30.160]   And it was because it wasn't figured out, necessarily,
[00:37:30.160 --> 00:37:34.540]   how to get more than seven layers to train.
[00:37:34.540 --> 00:37:36.900]   And so some of the VGG worked, right?
[00:37:36.900 --> 00:37:40.780]   It was like, you'd go through, and you'd train a network.
[00:37:40.780 --> 00:37:43.020]   You'd take all of those weights, and you'd transfer it
[00:37:43.020 --> 00:37:44.820]   to a slightly deeper network, and you would train it
[00:37:44.820 --> 00:37:45.460]   a little bit more.
[00:37:45.460 --> 00:37:46.940]   You'd do the exact same thing again,
[00:37:46.940 --> 00:37:48.580]   and you'd train it a little bit more.
[00:37:48.580 --> 00:37:50.420]   And that was your entire pipeline,
[00:37:50.420 --> 00:37:52.980]   because it did weird things of when you'd add a layer,
[00:37:52.980 --> 00:37:53.820]   it would get worse.
[00:37:53.820 --> 00:37:57.180]   And you'd add a layer, it would get worse.
[00:37:57.180 --> 00:38:00.900]   And so this comes back, like, some
[00:38:00.900 --> 00:38:05.100]   to kind of figuring out the specific details.
[00:38:05.100 --> 00:38:07.940]   I forgot what the actual question was here.
[00:38:07.940 --> 00:38:10.900]   So how did you retrofit your engineering pipeline
[00:38:10.900 --> 00:38:13.300]   from machine learning to deep learning?
[00:38:13.300 --> 00:38:14.660]   Yeah.
[00:38:14.660 --> 00:38:19.420]   I mean, we did it in a big leap.
[00:38:19.420 --> 00:38:23.940]   So we decided to make an entirely different product
[00:38:23.940 --> 00:38:27.820]   than we were making before, because I went through,
[00:38:27.820 --> 00:38:31.540]   and I looked at the numbers, and the numbers were better.
[00:38:31.540 --> 00:38:35.020]   And the numbers actually justified a weeding product
[00:38:35.020 --> 00:38:36.060]   in the market.
[00:38:36.060 --> 00:38:38.380]   And I thought that we could hit higher numbers.
[00:38:38.380 --> 00:38:41.780]   And so we went from lettuce thinning
[00:38:41.780 --> 00:38:46.060]   that was running on quad-core desktop i7s
[00:38:46.060 --> 00:38:53.260]   to starting to pull and get pre-orders on NVIDIA Jetsons
[00:38:53.260 --> 00:38:54.100]   that came out.
[00:38:54.100 --> 00:38:59.340]   And we bought racks of GPUs and started doing training.
[00:38:59.340 --> 00:39:03.940]   This was when CAFE was kind of the only thing around.
[00:39:03.940 --> 00:39:11.300]   And we were just slowly building up our knowledge in this area.
[00:39:11.300 --> 00:39:17.940]   We started out by taking some of the lettuce data
[00:39:17.940 --> 00:39:20.700]   and just seeing if we could do very basic things.
[00:39:20.700 --> 00:39:22.920]   And so in images, you'll get to a point
[00:39:22.920 --> 00:39:26.540]   where you'll want to go through, and you'll
[00:39:26.540 --> 00:39:28.460]   want to put boxes around, or you'll
[00:39:28.460 --> 00:39:31.060]   want to do semantic segmentation.
[00:39:31.060 --> 00:39:32.520]   These are challenging tasks.
[00:39:32.520 --> 00:39:36.220]   These are on the far end of the scale.
[00:39:36.220 --> 00:39:39.660]   What we started with was we started with classification.
[00:39:39.660 --> 00:39:43.740]   So can we pull out just a patch that we know is perfectly lettuce
[00:39:43.740 --> 00:39:46.220]   because we knew where they were at,
[00:39:46.220 --> 00:39:48.120]   and a patch that is something else?
[00:39:48.120 --> 00:39:51.940]   And can we label these as far as classification goes?
[00:39:51.940 --> 00:39:55.260]   We started to build that confidence in--
[00:39:55.260 --> 00:39:58.780]   the thought was, if you can't do classification,
[00:39:58.780 --> 00:40:00.860]   you can't do detection.
[00:40:00.860 --> 00:40:03.320]   If you can't get classification rates to be high,
[00:40:03.320 --> 00:40:05.420]   you can't get detection rates to be high.
[00:40:05.420 --> 00:40:10.560]   And so we were going through and benchmarking time by time,
[00:40:10.560 --> 00:40:12.220]   how accurate can you get it?
[00:40:12.220 --> 00:40:16.000]   Can you get it to the product stage?
[00:40:16.000 --> 00:40:16.500]   [INAUDIBLE]
[00:40:16.500 --> 00:40:17.000]   Just one more.
[00:40:17.000 --> 00:40:18.460]   We've got one or two more questions.
[00:40:18.460 --> 00:40:19.440]   [INAUDIBLE]
[00:40:19.440 --> 00:40:22.760]   Back to his question, for future engineers in the machine
[00:40:22.760 --> 00:40:26.600]   learning era, can they be better in understanding machine learning
[00:40:26.600 --> 00:40:28.760]   models because they have a domain knowledge?
[00:40:28.760 --> 00:40:30.820]   So whether the model is correct or not,
[00:40:30.820 --> 00:40:33.080]   whether it's true or not, whether it's
[00:40:33.080 --> 00:40:35.920]   using the right kind of features to make predictions
[00:40:35.920 --> 00:40:38.920]   and different things that we can't do it on.
[00:40:38.920 --> 00:40:41.640]   So what I was trying to say here was
[00:40:41.640 --> 00:40:44.800]   that the future engineers in the machine learning era,
[00:40:44.800 --> 00:40:48.920]   can they be used to interpret the deep learning models
[00:40:48.920 --> 00:40:51.560]   because they have a domain knowledge because they were
[00:40:51.560 --> 00:40:54.040]   doing future engineering?
[00:40:54.040 --> 00:40:54.540]   So--
[00:40:54.540 --> 00:40:56.200]   Yeah.
[00:40:56.200 --> 00:41:00.080]   Yeah, I mean, just us as a team, we went through.
[00:41:00.080 --> 00:41:03.560]   And this was also at a time where
[00:41:03.560 --> 00:41:06.640]   it was essentially impossible to hire deep learning
[00:41:06.640 --> 00:41:12.840]   engineers on a very small startup salary.
[00:41:12.840 --> 00:41:16.600]   And so we decided to do lots of internal training.
[00:41:16.600 --> 00:41:20.240]   So the people that were part of the computer vision team
[00:41:20.240 --> 00:41:23.600]   before that were doing future engineering,
[00:41:23.600 --> 00:41:28.400]   we put on a track where they would take classes.
[00:41:28.400 --> 00:41:32.560]   They had 30% to 50% of their days
[00:41:32.560 --> 00:41:35.680]   set aside to learn, to experiment
[00:41:35.680 --> 00:41:39.120]   with these different models.
[00:41:39.120 --> 00:41:41.960]   And we built a little community.
[00:41:41.960 --> 00:41:44.840]   And it wasn't like there was lots of us there, right?
[00:41:44.840 --> 00:41:50.160]   There was 30 people in the whole company at the same time.
[00:41:50.160 --> 00:41:54.600]   So we're talking like three people, four people.
[00:41:54.600 --> 00:41:57.640]   Went through and did this.
[00:41:57.640 --> 00:42:00.320]   But yeah, that was to go through and to have
[00:42:00.320 --> 00:42:02.240]   our own community inside of the company
[00:42:02.240 --> 00:42:05.600]   that we could pass off papers, and we could learn,
[00:42:05.600 --> 00:42:06.880]   and we could implement stuff.
[00:42:06.880 --> 00:42:14.880]   It was a big switch to switch the hardware
[00:42:14.880 --> 00:42:17.520]   that we were using to switch the training methods that we
[00:42:17.520 --> 00:42:19.200]   were using.
[00:42:19.200 --> 00:42:23.760]   It became so much simpler from a systems architecture
[00:42:23.760 --> 00:42:25.440]   point of view.
[00:42:25.440 --> 00:42:28.720]   One of the things that's very nice about a ConvNet
[00:42:28.720 --> 00:42:33.200]   all the way through that's not obvious in a robotic system
[00:42:33.200 --> 00:42:35.440]   is it takes the same amount of time
[00:42:35.440 --> 00:42:38.880]   to do inference every single time.
[00:42:38.880 --> 00:42:41.160]   All these classical computer vision models
[00:42:41.160 --> 00:42:44.000]   that would go through, and it would find individual features,
[00:42:44.000 --> 00:42:45.280]   and it would pull those out.
[00:42:45.280 --> 00:42:47.480]   And then you would have a list of key points.
[00:42:47.480 --> 00:42:48.880]   And then you'd pull out features,
[00:42:48.880 --> 00:42:50.600]   and you'd classify those.
[00:42:50.600 --> 00:42:53.040]   Well, your list could be this long,
[00:42:53.040 --> 00:42:55.280]   or it could be this long, at which point
[00:42:55.280 --> 00:42:58.440]   it's going to take this much time versus this much time.
[00:42:58.440 --> 00:43:00.680]   And in this robotic system, you're
[00:43:00.680 --> 00:43:05.880]   running in this world where you need it to fit in a time bucket.
[00:43:05.880 --> 00:43:09.960]   And so even though the compute requirements for deep learning
[00:43:09.960 --> 00:43:12.800]   were much higher, they were much more predictable.
[00:43:12.800 --> 00:43:16.880]   And we didn't have to go and deal with this systems lag that
[00:43:16.880 --> 00:43:19.360]   came in and sat on top of--
[00:43:19.360 --> 00:43:23.440]   that we'd like--
[00:43:23.440 --> 00:43:25.560]   the computer vision engineers would pass off
[00:43:25.560 --> 00:43:29.120]   a model that would sometimes take 20 milliseconds,
[00:43:29.120 --> 00:43:30.920]   but every now and then take five seconds.
[00:43:30.920 --> 00:43:36.880]   Maybe last question.
[00:43:36.880 --> 00:43:40.080]   How do you simulate the real life issues
[00:43:40.080 --> 00:43:42.680]   like you mentioned, like smudges in the camera,
[00:43:42.680 --> 00:43:44.760]   or maybe a rainy day?
[00:43:44.760 --> 00:43:50.400]   Or how do you anticipate for all that and train your models?
[00:43:50.400 --> 00:43:55.440]   Yeah, we would go through, and we'd actually collect data.
[00:43:55.440 --> 00:43:58.040]   Many of these smudges on the camera
[00:43:58.040 --> 00:44:01.120]   are not deep learning based models.
[00:44:01.120 --> 00:44:03.680]   They're ones where you'd look at a couple images in a row,
[00:44:03.680 --> 00:44:07.680]   and you'd hand tune a feature out.
[00:44:07.680 --> 00:44:12.160]   And so we'd go through, and you'd
[00:44:12.160 --> 00:44:14.400]   pick out these side objects.
[00:44:14.400 --> 00:44:16.400]   You'd pick out the ones, and you'd say,
[00:44:16.400 --> 00:44:18.320]   here's one with a bunch of hay in it.
[00:44:18.320 --> 00:44:20.120]   Let's not count that.
[00:44:20.120 --> 00:44:23.360]   Here's one with under saturation or over saturation.
[00:44:23.360 --> 00:44:25.320]   We're dealing with cameras outdoors,
[00:44:25.320 --> 00:44:27.400]   so that's a big problem.
[00:44:27.400 --> 00:44:31.120]   And so we'd be able to go through and segment
[00:44:31.120 --> 00:44:34.920]   those individual ones and have individual team members work
[00:44:34.920 --> 00:44:37.840]   on those problems.
[00:44:37.840 --> 00:44:41.800]   One of the funny ones, actually, with--
[00:44:41.800 --> 00:44:45.760]   we were using, in the machine learning days,
[00:44:45.760 --> 00:44:48.400]   with feature engineering, we went through,
[00:44:48.400 --> 00:44:52.640]   and we collected a lot of fields worth of data.
[00:44:52.640 --> 00:44:57.280]   And this is one about a non-intuitive thing.
[00:44:57.280 --> 00:45:00.800]   So we would go, and we would take, and we had 10 fields.
[00:45:00.800 --> 00:45:01.800]   We had 15 fields.
[00:45:01.800 --> 00:45:03.560]   We had 20 fields.
[00:45:03.560 --> 00:45:05.120]   So on and so on.
[00:45:05.120 --> 00:45:08.520]   And we found that the model would get better.
[00:45:08.520 --> 00:45:10.040]   This was a support vector machine,
[00:45:10.040 --> 00:45:13.760]   so we tuned the kernel like crazy
[00:45:13.760 --> 00:45:16.640]   to get that thing to work.
[00:45:16.640 --> 00:45:21.680]   And it worked better up until 19 fields, at which point
[00:45:21.680 --> 00:45:23.000]   you'd add the 20th in.
[00:45:23.000 --> 00:45:24.120]   It wouldn't get any better.
[00:45:24.120 --> 00:45:25.960]   21st, no better.
[00:45:25.960 --> 00:45:29.440]   All the way through 100, you'd pretty much be flatlined.
[00:45:29.440 --> 00:45:32.780]   And it was at that point, that model, the support vector
[00:45:32.780 --> 00:45:37.320]   machine, didn't have enough capacity.
[00:45:37.320 --> 00:45:42.760]   It had 150 different values that it was learning.
[00:45:42.760 --> 00:45:44.880]   And it was one of--
[00:45:44.880 --> 00:45:47.320]   and we were sitting there for about six months,
[00:45:47.320 --> 00:45:50.520]   which felt like forever, with this model and this extra data
[00:45:50.520 --> 00:45:52.480]   that came out.
[00:45:52.480 --> 00:45:55.320]   And knowing that we were collecting more data,
[00:45:55.320 --> 00:45:58.440]   and we weren't fulfilling this virtuous cycle of having
[00:45:58.440 --> 00:46:00.580]   more data, having more labeled data,
[00:46:00.580 --> 00:46:02.960]   and having a better model.
[00:46:02.960 --> 00:46:06.240]   It's something like in the deep learning,
[00:46:06.240 --> 00:46:08.560]   nobody's ever going to run into that.
[00:46:08.560 --> 00:46:11.240]   Add another layer.
[00:46:11.240 --> 00:46:13.960]   And so, yeah.
[00:46:13.960 --> 00:46:14.460]   Great.
[00:46:14.460 --> 00:46:15.160]   Thank you, Lee.
[00:46:15.160 --> 00:46:18.520]   [APPLAUSE]

