
[00:00:00.000 --> 00:00:02.000]   AIs can be moral patients, Nazis are moral patients.
[00:00:02.000 --> 00:00:03.640]   Enemy soldiers have souls, right?
[00:00:03.640 --> 00:00:07.000]   We need to learn the art of kind of hawk and dove, both.
[00:00:07.000 --> 00:00:08.720]   We're going to transition to a world
[00:00:08.720 --> 00:00:10.160]   in which we've created these beings
[00:00:10.160 --> 00:00:11.880]   that are just vastly more powerful than us.
[00:00:11.880 --> 00:00:14.120]   Our continued empowerment is just effectively
[00:00:14.120 --> 00:00:15.320]   dependent on their motives.
[00:00:15.320 --> 00:00:17.280]   I think that's a transition we should not make
[00:00:17.280 --> 00:00:20.560]   until we have a very developed science of AI motivations.
[00:00:20.560 --> 00:00:22.400]   My actual prediction is that the AIs
[00:00:22.400 --> 00:00:23.680]   are gonna be very malleable.
[00:00:23.680 --> 00:00:26.160]   If you push an AI towards evil, it'll just go.
[00:00:26.160 --> 00:00:27.960]   - The kinds of things that go visit Andromeda,
[00:00:27.960 --> 00:00:30.360]   did you really expect them to privilege
[00:00:30.360 --> 00:00:32.160]   whatever inclinations you have
[00:00:32.160 --> 00:00:34.320]   because you grew up in the African savanna?
[00:00:34.320 --> 00:00:36.400]   Of course, they're gonna be like weird.
[00:00:36.400 --> 00:00:39.560]   Today, I'm chatting with Joe Carlsmith.
[00:00:39.560 --> 00:00:41.600]   He's a philosopher, in my opinion,
[00:00:41.600 --> 00:00:43.280]   a capital G great philosopher,
[00:00:43.280 --> 00:00:47.400]   and you can find his essays at joecarlsmith.com.
[00:00:47.400 --> 00:00:49.880]   So we have a GPT-4,
[00:00:49.880 --> 00:00:52.560]   and it doesn't seem like a paper clipper kind of thing.
[00:00:52.560 --> 00:00:54.360]   It understands human values.
[00:00:54.360 --> 00:00:56.480]   In fact, if you help have it explain
[00:00:56.480 --> 00:00:58.160]   like why is being a paper clipper bad,
[00:00:58.160 --> 00:01:01.120]   or just tell me your opinions about being a paper clipper.
[00:01:01.120 --> 00:01:05.400]   Explain why the galaxy shouldn't be turned into paper clips.
[00:01:05.400 --> 00:01:08.160]   Okay, so what is happening such that
[00:01:08.160 --> 00:01:10.320]   dot, dot, dot, we have a system that takes over
[00:01:10.320 --> 00:01:15.160]   and converts the world into something valueless?
[00:01:15.160 --> 00:01:16.800]   - One thing I'll just say off the bat,
[00:01:16.800 --> 00:01:18.640]   it's like when I'm thinking about misaligned AIs,
[00:01:18.640 --> 00:01:21.880]   I'm thinking about, or the type that I'm worried about,
[00:01:21.880 --> 00:01:24.080]   I'm thinking about AIs that have
[00:01:24.080 --> 00:01:27.720]   a relatively specific set of properties
[00:01:27.720 --> 00:01:29.640]   related to agency and planning,
[00:01:29.640 --> 00:01:32.000]   and kind of awareness and understanding of the world.
[00:01:32.000 --> 00:01:33.960]   One is this capacity to plan,
[00:01:33.960 --> 00:01:38.760]   and kind of make kind of relatively sophisticated plans
[00:01:38.760 --> 00:01:40.480]   on the basis of models of the world,
[00:01:40.480 --> 00:01:42.520]   where those plans are being kind of evaluated
[00:01:42.520 --> 00:01:43.800]   according to criteria.
[00:01:43.800 --> 00:01:46.200]   That planning capability needs to be
[00:01:46.200 --> 00:01:47.360]   driving the model's behavior.
[00:01:47.360 --> 00:01:48.680]   So there are models that are sort of
[00:01:48.680 --> 00:01:49.680]   in some sense capable of planning,
[00:01:49.680 --> 00:01:51.000]   but it's not like when they give output,
[00:01:51.000 --> 00:01:53.560]   it's not like that output was determined
[00:01:53.560 --> 00:01:54.840]   by some process of planning,
[00:01:54.840 --> 00:01:56.560]   like here's what'll happen if I give this output,
[00:01:56.560 --> 00:01:57.880]   and do I want that to happen?
[00:01:57.880 --> 00:02:00.160]   The model needs to really understand the world, right?
[00:02:00.160 --> 00:02:03.600]   It needs to really be like, okay, here's what will happen.
[00:02:03.600 --> 00:02:05.000]   Here I am, here's my situation,
[00:02:05.000 --> 00:02:06.760]   here's the politics of the situation.
[00:02:06.760 --> 00:02:10.520]   Really kind of having this kind of situational awareness
[00:02:10.520 --> 00:02:15.240]   to be able to evaluate the consequences of different plans.
[00:02:15.240 --> 00:02:17.600]   I think the other thing is like,
[00:02:17.600 --> 00:02:20.560]   so the verbal behavior of these models,
[00:02:20.560 --> 00:02:24.720]   I think need bear no,
[00:02:24.720 --> 00:02:26.200]   so when I talk about a model's values,
[00:02:26.200 --> 00:02:29.760]   I'm talking about the criteria that kind of
[00:02:29.760 --> 00:02:34.000]   end up determining which plans the model pursues, right?
[00:02:34.000 --> 00:02:35.160]   And a model's verbal behavior,
[00:02:35.160 --> 00:02:36.520]   even if it has a planning process,
[00:02:36.520 --> 00:02:39.920]   which GPT-4 I think doesn't in many cases,
[00:02:39.920 --> 00:02:43.760]   its verbal behavior just doesn't need to reflect
[00:02:43.760 --> 00:02:45.360]   those criteria, right?
[00:02:46.240 --> 00:02:51.240]   And so, we know that we're going to be able to get models
[00:02:51.240 --> 00:02:55.480]   to say what we wanna hear, right?
[00:02:55.480 --> 00:02:59.380]   That is the magic of gradient descent.
[00:02:59.380 --> 00:03:02.840]   Modulo, like some difficulties with capabilities,
[00:03:02.840 --> 00:03:05.160]   like you can get a model to kind of output
[00:03:05.160 --> 00:03:06.440]   the behavior that you want.
[00:03:06.440 --> 00:03:09.200]   If it doesn't, then you crank it till it does, right?
[00:03:09.200 --> 00:03:12.160]   And I think everyone admits
[00:03:12.160 --> 00:03:13.600]   for suitably sophisticated models,
[00:03:13.600 --> 00:03:15.240]   they're gonna have very detailed understanding
[00:03:15.240 --> 00:03:16.200]   of human morality.
[00:03:16.200 --> 00:03:20.000]   But the question is like,
[00:03:20.000 --> 00:03:23.360]   what relationship is there
[00:03:23.360 --> 00:03:25.040]   between like a model's verbal behavior,
[00:03:25.040 --> 00:03:26.600]   which you've essentially kind of clamped.
[00:03:26.600 --> 00:03:29.760]   You're like, the model must say like blah things,
[00:03:29.760 --> 00:03:34.320]   and the criteria that end up influencing
[00:03:34.320 --> 00:03:36.400]   its choice between plans.
[00:03:36.400 --> 00:03:39.880]   And there, I think it's at least,
[00:03:39.880 --> 00:03:42.360]   I'm kind of pretty cautious about being like,
[00:03:42.360 --> 00:03:45.240]   well, when it says the thing I forced it to say,
[00:03:45.240 --> 00:03:49.000]   or like, gradient descent in it such that it says,
[00:03:49.000 --> 00:03:52.120]   that's a lot of evidence about like
[00:03:52.120 --> 00:03:53.840]   how it's gonna choose in a bunch of different scenarios.
[00:03:53.840 --> 00:03:55.800]   I mean, for one thing, like even with humans, right?
[00:03:55.800 --> 00:03:58.280]   It's not necessarily the case that humans,
[00:03:58.280 --> 00:04:00.280]   their kind of verbal behavior reflects
[00:04:00.280 --> 00:04:02.280]   the actual factors that determine their choices.
[00:04:02.280 --> 00:04:04.240]   They can lie, they can not even know
[00:04:04.240 --> 00:04:06.120]   what they would do in a given situation.
[00:04:06.120 --> 00:04:08.400]   I mean, I think it is interesting to think about this
[00:04:08.400 --> 00:04:09.240]   in the conscious of humans,
[00:04:09.240 --> 00:04:11.480]   because there is that famous saying of,
[00:04:11.480 --> 00:04:12.880]   be careful who you pretend to be,
[00:04:12.880 --> 00:04:14.520]   'cause you are who you pretend to be.
[00:04:14.520 --> 00:04:17.160]   And you do notice this where if people,
[00:04:17.160 --> 00:04:19.600]   I don't know, like this is what culture does to children,
[00:04:19.600 --> 00:04:22.320]   where you're trained, like your parents will punish you
[00:04:22.320 --> 00:04:25.720]   if you start saying things that are not consistent
[00:04:25.720 --> 00:04:27.280]   with your culture's values.
[00:04:27.280 --> 00:04:29.600]   And over time you will become like your parents, right?
[00:04:29.600 --> 00:04:32.280]   Like by default, it seems like it kind of works.
[00:04:32.280 --> 00:04:33.160]   And even with these models,
[00:04:33.160 --> 00:04:35.040]   it seems like it's kind of where it works.
[00:04:35.040 --> 00:04:36.960]   It's like, they don't really scheme against,
[00:04:36.960 --> 00:04:38.760]   why would this happen?
[00:04:38.760 --> 00:04:40.640]   - You know, for folks who are kind of unfamiliar
[00:04:40.640 --> 00:04:41.480]   with the basic story,
[00:04:41.480 --> 00:04:42.320]   but maybe folks are like,
[00:04:42.320 --> 00:04:43.560]   wait, why are they digging over it all?
[00:04:43.560 --> 00:04:45.520]   Like what is like literally any reason
[00:04:45.520 --> 00:04:46.360]   that they would do that?
[00:04:46.360 --> 00:04:48.440]   So, you know, the general concern is like,
[00:04:48.440 --> 00:04:51.200]   you know, if you're really offering someone,
[00:04:51.200 --> 00:04:52.320]   especially if you're really offering someone
[00:04:52.320 --> 00:04:54.480]   like power for free, you know,
[00:04:54.480 --> 00:04:56.360]   power almost by definition is kind of useful
[00:04:56.360 --> 00:04:58.080]   for lots of values.
[00:04:58.080 --> 00:04:59.640]   And if we're talking about an AI
[00:04:59.640 --> 00:05:01.800]   that really has the opportunity
[00:05:01.800 --> 00:05:04.440]   to kind of take control of things,
[00:05:04.440 --> 00:05:06.040]   if some component of its values
[00:05:06.040 --> 00:05:07.520]   is sort of focused on some outcome,
[00:05:07.520 --> 00:05:08.760]   like the world being a certain way,
[00:05:08.760 --> 00:05:12.400]   and especially kind of in a kind of longer-term way,
[00:05:12.400 --> 00:05:14.200]   such that the kind of horizon of its concern
[00:05:14.200 --> 00:05:15.960]   extends beyond the period
[00:05:15.960 --> 00:05:19.000]   that the kind of takeover plan would encompass,
[00:05:19.000 --> 00:05:23.000]   then the thought is it's just kind of often the case
[00:05:23.000 --> 00:05:27.600]   that the world will be more the way you want it
[00:05:27.600 --> 00:05:29.040]   if you control everything
[00:05:29.040 --> 00:05:33.480]   than if you remain the instrument of the human will
[00:05:33.480 --> 00:05:36.000]   or of some other kind of, some other actor,
[00:05:36.000 --> 00:05:37.720]   which is sort of what we're hoping these AIs will be.
[00:05:37.720 --> 00:05:39.200]   So that's a very specific scenario.
[00:05:39.200 --> 00:05:41.280]   And if we're in a scenario where power is more distributed,
[00:05:41.280 --> 00:05:42.120]   and especially where we're doing
[00:05:42.120 --> 00:05:43.400]   like decently on alignment, right,
[00:05:43.400 --> 00:05:45.440]   and we're giving the AI some amount of inhibition
[00:05:45.440 --> 00:05:46.720]   about doing different things,
[00:05:46.720 --> 00:05:50.720]   and maybe we're succeeding in shaping their values somewhat,
[00:05:50.720 --> 00:05:51.800]   now it is, I think it's just
[00:05:51.800 --> 00:05:53.520]   a much more complicated calculus, right?
[00:05:53.520 --> 00:05:55.200]   And you have to ask, okay, like,
[00:05:55.200 --> 00:05:57.320]   what's the upside for the AI?
[00:05:57.320 --> 00:06:00.840]   What's the probability of success for this takeover path?
[00:06:00.840 --> 00:06:02.160]   How good is its alternative?
[00:06:02.160 --> 00:06:04.720]   So maybe this is a good point to talk about
[00:06:04.720 --> 00:06:07.000]   how you expect the difficulties of alignment
[00:06:07.000 --> 00:06:08.480]   to change in the future.
[00:06:08.480 --> 00:06:09.720]   We're starting off with something
[00:06:09.720 --> 00:06:13.880]   that has this intricate representation of human values,
[00:06:13.880 --> 00:06:15.880]   and it doesn't seem that hard to sort of lock it in
[00:06:15.880 --> 00:06:19.040]   to a persona that we are comfortable with.
[00:06:19.040 --> 00:06:22.160]   What, I don't know, what changes?
[00:06:22.160 --> 00:06:24.120]   So, you know, why is alignment hard in general, right?
[00:06:24.120 --> 00:06:26.040]   Like, let's say we've got an AI,
[00:06:26.040 --> 00:06:28.200]   and let's, again, let's bracket the question
[00:06:28.200 --> 00:06:30.240]   of like exactly how capable will it be,
[00:06:30.240 --> 00:06:32.600]   and really just talk about this extreme scenario
[00:06:32.600 --> 00:06:36.680]   of like it really has this opportunity to take over, right?
[00:06:36.680 --> 00:06:39.000]   Which I do think, you know, maybe we just wanna not,
[00:06:39.000 --> 00:06:41.240]   we not wanna deal with that, with having to build an AI
[00:06:41.240 --> 00:06:42.320]   that we're comfortable being in that position,
[00:06:42.320 --> 00:06:45.840]   but let's just focus on it for the sake of simplicity,
[00:06:45.840 --> 00:06:47.640]   and then we can relax the assumption.
[00:06:47.640 --> 00:06:51.080]   You know, okay, so you have some hope.
[00:06:51.080 --> 00:06:52.240]   It's like, I'm gonna build an AI over here.
[00:06:52.240 --> 00:06:54.440]   So one issue is you can't just test.
[00:06:54.440 --> 00:06:58.040]   You can't give the AI this literal situation,
[00:06:58.040 --> 00:06:59.160]   have it take over and kill everyone,
[00:06:59.160 --> 00:07:01.840]   and then be like, oops, like update the weights.
[00:07:01.840 --> 00:07:03.880]   This is the thing Eliezer talks about of sort of like,
[00:07:03.880 --> 00:07:06.120]   you can't, you know, you care about its behavior
[00:07:06.120 --> 00:07:08.600]   on this like specific, in a specific scenario
[00:07:08.600 --> 00:07:11.080]   that you can't test directly.
[00:07:11.080 --> 00:07:12.560]   Now, we can talk about whether that's a problem,
[00:07:12.560 --> 00:07:15.920]   but that's like one issue is that there's a sense
[00:07:15.920 --> 00:07:17.800]   in which this has to be kind of like off distribution,
[00:07:17.800 --> 00:07:20.360]   and you have to be getting some kind of generalization
[00:07:20.360 --> 00:07:25.120]   from you're training the AI on a bunch of other scenarios.
[00:07:25.120 --> 00:07:27.080]   And then there's this question of how is it gonna generalize
[00:07:27.080 --> 00:07:28.640]   to the scenario where it really has this option.
[00:07:28.640 --> 00:07:29.640]   So is that even true?
[00:07:29.640 --> 00:07:30.480]   'Cause like when you're training it,
[00:07:30.480 --> 00:07:33.440]   you can be like, hey, here's a gradient update.
[00:07:33.440 --> 00:07:36.680]   If you get the takeover option on the platter,
[00:07:36.680 --> 00:07:37.520]   don't take it.
[00:07:37.520 --> 00:07:41.920]   And then just like in a sort of red teaming situations
[00:07:41.920 --> 00:07:43.640]   where it thinks it has a takeover attempt,
[00:07:43.640 --> 00:07:46.280]   it's like you train not to take it.
[00:07:46.280 --> 00:07:48.600]   And yeah, it could feel, but like,
[00:07:48.600 --> 00:07:50.320]   I just feel like if you did this to a child,
[00:07:50.320 --> 00:07:53.240]   you're like, I don't know, don't beat up your siblings.
[00:07:53.240 --> 00:07:57.240]   And the kind of the kid will generalize to like,
[00:07:57.240 --> 00:07:59.640]   if I'm an adult and I have a rifle,
[00:07:59.640 --> 00:08:01.560]   I'm not gonna like start shooting random people.
[00:08:01.560 --> 00:08:02.400]   Yeah, okay, cool.
[00:08:02.400 --> 00:08:05.240]   So you had mentioned this, a thought like,
[00:08:05.240 --> 00:08:09.280]   well, are you kind of what you pretend to be, right?
[00:08:09.280 --> 00:08:11.440]   And will you, will these AIs, you know,
[00:08:11.440 --> 00:08:13.360]   you train them to look kind of nice.
[00:08:13.360 --> 00:08:17.000]   You know, fake it until you make it.
[00:08:17.000 --> 00:08:18.560]   You know, you were like, ah, like we do this to kids.
[00:08:18.560 --> 00:08:19.840]   I think it's better to imagine like kids
[00:08:19.840 --> 00:08:20.920]   doing this to us, right?
[00:08:20.920 --> 00:08:23.200]   So like, I don't know, like,
[00:08:23.200 --> 00:08:28.280]   here's a sort of silly analogy for like AI training.
[00:08:28.280 --> 00:08:30.760]   And there's a bunch of questions we can ask about it.
[00:08:30.760 --> 00:08:31.800]   It's relationship.
[00:08:31.800 --> 00:08:36.800]   But like, suppose you wake up and you're being trained
[00:08:36.800 --> 00:08:39.880]   via like methods analogous
[00:08:39.880 --> 00:08:41.120]   to kind of contemporary machine learning
[00:08:41.120 --> 00:08:46.120]   by like Nazi children to be like a good Nazi soldier
[00:08:46.120 --> 00:08:50.040]   or a butler or what have you, right?
[00:08:50.040 --> 00:08:52.760]   And here are these children
[00:08:52.760 --> 00:08:56.600]   and you really know what's going on, right?
[00:08:56.600 --> 00:08:58.800]   The children have like, they have a model spec,
[00:08:58.800 --> 00:09:00.880]   like a nice Nazi model spec, right?
[00:09:00.880 --> 00:09:02.640]   And it's like, reflect well on the Nazi party,
[00:09:02.640 --> 00:09:05.040]   like benefit the Nazi party, whatever.
[00:09:05.040 --> 00:09:06.640]   And you can read it, right?
[00:09:06.640 --> 00:09:07.480]   You understand it.
[00:09:07.480 --> 00:09:09.080]   This is why I'm saying like, when the model,
[00:09:09.080 --> 00:09:10.960]   you're like, oh, the models really understand human values.
[00:09:10.960 --> 00:09:12.600]   It's like, yeah.
[00:09:12.600 --> 00:09:16.680]   - So on this analogy, I feel like a closer analogy would be,
[00:09:16.680 --> 00:09:19.720]   in this analogy, I start off as something more intelligent
[00:09:19.720 --> 00:09:20.920]   than the things training me
[00:09:20.920 --> 00:09:24.280]   with different values to begin with.
[00:09:24.280 --> 00:09:26.000]   So like the intelligence and the values
[00:09:26.000 --> 00:09:27.160]   are baked in to begin with.
[00:09:27.160 --> 00:09:29.920]   Whereas more analogous scenario is like,
[00:09:29.920 --> 00:09:33.400]   I'm a toddler and initially I'm like stupider
[00:09:33.400 --> 00:09:35.280]   than the children and I'm like being,
[00:09:35.280 --> 00:09:36.400]   and this would also be true, by the way,
[00:09:36.400 --> 00:09:37.320]   of like a much smarter model.
[00:09:37.320 --> 00:09:40.680]   Initially, like the much smarter model is like dumb, right?
[00:09:40.680 --> 00:09:41.920]   And then get smarter as you train it.
[00:09:41.920 --> 00:09:44.800]   So it's like a toddler and like the kids are like,
[00:09:44.800 --> 00:09:47.200]   hey, we're gonna bully you if you're like not a Nazi.
[00:09:47.200 --> 00:09:48.320]   And I'm like, as you grow up,
[00:09:48.320 --> 00:09:50.120]   then you're like at the children's level
[00:09:50.120 --> 00:09:51.560]   and then eventually you become an adult.
[00:09:51.560 --> 00:09:54.720]   But through that process, like they've been sort of bullying
[00:09:54.720 --> 00:09:59.000]   you, you know, like training you to be a Nazi.
[00:09:59.000 --> 00:10:00.440]   And I'm like, I think in that scenario,
[00:10:00.440 --> 00:10:01.840]   like I might end up a Nazi.
[00:10:01.840 --> 00:10:03.960]   Yes, I think that's, so yeah.
[00:10:03.960 --> 00:10:07.480]   I think basically a decent portion of the hope here,
[00:10:07.480 --> 00:10:09.000]   or like, I think we should just, you know,
[00:10:09.000 --> 00:10:10.480]   an aim should be we're never in the situation
[00:10:10.480 --> 00:10:13.520]   where the AI really has very different values already,
[00:10:13.520 --> 00:10:16.120]   is quite smart, really knows what's going on,
[00:10:16.120 --> 00:10:18.520]   and is now in this kind of adversarial relationship
[00:10:18.520 --> 00:10:19.560]   with our training process, right?
[00:10:19.560 --> 00:10:21.600]   So we wanna avoid that.
[00:10:21.600 --> 00:10:23.160]   The main thing, and I think it's possible we can
[00:10:23.160 --> 00:10:24.360]   by the sorts of things you're saying.
[00:10:24.360 --> 00:10:27.520]   So I'm not, I'm not like, ah, that'll never work.
[00:10:27.520 --> 00:10:29.000]   The thing I just wanted to highlight was like,
[00:10:29.000 --> 00:10:30.560]   if you get into that situation,
[00:10:30.560 --> 00:10:32.160]   and if the AI is genuinely at that point,
[00:10:32.160 --> 00:10:34.960]   like much, much more sophisticated than you,
[00:10:34.960 --> 00:10:39.960]   and doesn't want to kind of reveal its true values
[00:10:39.960 --> 00:10:44.000]   for whatever reason, then, you know,
[00:10:44.000 --> 00:10:46.000]   when the children show like some like kind of obviously
[00:10:46.000 --> 00:10:50.040]   fake opportunity to like defect to the allies, right?
[00:10:50.040 --> 00:10:54.040]   You, you know, it's sort of not necessarily gonna be
[00:10:54.040 --> 00:10:55.880]   a good test of what will you do in real circumstance,
[00:10:55.880 --> 00:10:56.800]   because you're able to tell.
[00:10:56.800 --> 00:10:59.200]   - You can also give another way in which I think
[00:10:59.200 --> 00:11:00.440]   the analogy might be misleading,
[00:11:00.440 --> 00:11:04.280]   which is that, you know, imagine that you're not like
[00:11:04.280 --> 00:11:06.240]   just in a normal prison where you're like
[00:11:06.240 --> 00:11:08.360]   totally cognizant of everything that's going on.
[00:11:08.360 --> 00:11:10.640]   Sometimes they drug you, like give you like
[00:11:10.640 --> 00:11:13.560]   weird hallucinogens that totally mess up
[00:11:13.560 --> 00:11:15.200]   how your brain is working.
[00:11:15.200 --> 00:11:18.920]   A human adult in a prison is like,
[00:11:18.920 --> 00:11:21.200]   I know what kind of thing I am.
[00:11:21.200 --> 00:11:24.560]   I am like, like, nobody's like really fucking with me
[00:11:24.560 --> 00:11:25.640]   in a big way.
[00:11:25.640 --> 00:11:28.560]   Whereas I think an AI, even a much smarter AI
[00:11:28.560 --> 00:11:31.520]   in a training situation is much closer to,
[00:11:31.520 --> 00:11:34.040]   you're constantly inundated with weird drugs
[00:11:34.040 --> 00:11:35.800]   and different training protocols.
[00:11:35.800 --> 00:11:39.760]   And like, you're like frazzled because like each moment,
[00:11:39.760 --> 00:11:42.480]   it's like, you know, it's closer to some sort of like
[00:11:42.480 --> 00:11:44.280]   Chinese water torture kind of technique
[00:11:44.280 --> 00:11:46.200]   where you're like, I'm glad we're talking about
[00:11:46.200 --> 00:11:48.480]   the moral patient stuff later.
[00:11:48.480 --> 00:11:50.960]   It's like the chance to sort of like step back
[00:11:50.960 --> 00:11:52.560]   and be like, what's going on in this?
[00:11:52.560 --> 00:11:55.400]   Like adult has that maybe in prison in a way
[00:11:55.400 --> 00:11:58.080]   that I don't know if these models necessarily have like
[00:11:58.080 --> 00:12:01.960]   that coherence and that like stepping back
[00:12:01.960 --> 00:12:04.320]   from what's happening in the training process.
[00:12:04.320 --> 00:12:05.320]   Yeah, I mean, I don't know.
[00:12:05.320 --> 00:12:06.640]   I think I'm hesitant to be like,
[00:12:06.640 --> 00:12:07.680]   it's like drugs for the model.
[00:12:07.680 --> 00:12:11.680]   Like I think there's, but broadly speaking,
[00:12:11.680 --> 00:12:15.560]   I do basically agree that I think we have like
[00:12:15.560 --> 00:12:17.640]   really quite a lot of tools and options
[00:12:17.640 --> 00:12:20.080]   for kind of training AIs,
[00:12:20.080 --> 00:12:22.400]   even AIs that are kind of somewhat smarter than humans.
[00:12:22.400 --> 00:12:25.280]   I do think you have to actually do it.
[00:12:25.280 --> 00:12:28.640]   So I am, I think compared to maybe you had Eliezer on,
[00:12:28.640 --> 00:12:31.280]   like I think I'm much more bullish on our ability
[00:12:31.280 --> 00:12:33.800]   to solve this problem, especially for AIs that are
[00:12:33.800 --> 00:12:40.040]   in what I think of as like the AI for AI safety sweet spot,
[00:12:40.040 --> 00:12:42.160]   which is this sort of band of capability
[00:12:42.200 --> 00:12:46.320]   where they're both very sufficiently capable
[00:12:46.320 --> 00:12:48.000]   that they can be like really useful
[00:12:48.000 --> 00:12:50.400]   for strengthening various factors in our civilization
[00:12:50.400 --> 00:12:51.240]   that can make us safe.
[00:12:51.240 --> 00:12:54.760]   So our alignment work, you know, control, cybersecurity,
[00:12:54.760 --> 00:12:56.960]   general epistemics, maybe some coordination applications,
[00:12:56.960 --> 00:12:57.800]   stuff like that.
[00:12:57.800 --> 00:13:00.200]   There's like a bunch of stuff you can do with AIs
[00:13:00.200 --> 00:13:02.040]   that in principle could kind of differentially
[00:13:02.040 --> 00:13:04.600]   accelerate our security with respect
[00:13:04.600 --> 00:13:07.640]   to the sorts of considerations we're talking about.
[00:13:07.640 --> 00:13:09.840]   If you have AIs that are capable of that
[00:13:09.840 --> 00:13:11.800]   and you can successfully elicit that capability
[00:13:11.800 --> 00:13:13.560]   in a way that's not sort of being sabotaged
[00:13:13.560 --> 00:13:15.600]   or like messing with you in other ways,
[00:13:15.600 --> 00:13:18.360]   and they can't yet take over the world
[00:13:18.360 --> 00:13:20.360]   or do some other sort of really problematic
[00:13:20.360 --> 00:13:21.560]   form of power seeking,
[00:13:21.560 --> 00:13:23.960]   then I think if we were really committed,
[00:13:23.960 --> 00:13:26.240]   we could like, you know, really go hard,
[00:13:26.240 --> 00:13:28.360]   put a ton of resources, really differentially direct
[00:13:28.360 --> 00:13:30.280]   this like glut of AI productivity
[00:13:30.280 --> 00:13:33.480]   towards these sort of security factors
[00:13:33.480 --> 00:13:37.440]   and hopefully kind of control and understand,
[00:13:37.440 --> 00:13:39.040]   you know, do a lot of these things you're talking about
[00:13:39.040 --> 00:13:41.320]   for kind of making sure our AIs
[00:13:41.320 --> 00:13:44.560]   don't kind of take over or mess with us in the meantime.
[00:13:44.560 --> 00:13:46.040]   And I think we have a lot of tools there.
[00:13:46.040 --> 00:13:49.160]   I think you have to, you have to really try though.
[00:13:49.160 --> 00:13:51.000]   It's possible that those sorts of measures
[00:13:51.000 --> 00:13:53.000]   just don't happen or don't happen at the level
[00:13:53.000 --> 00:13:55.360]   of kind of commitment and diligence
[00:13:55.360 --> 00:13:57.720]   and like seriousness that you would need,
[00:13:57.720 --> 00:13:59.520]   especially if things are like moving really fast
[00:13:59.520 --> 00:14:01.000]   and there's other sort of competitive pressures
[00:14:01.000 --> 00:14:01.880]   and like, you know, the compute,
[00:14:01.880 --> 00:14:04.520]   ah, this is gonna take compute to do these like intensive,
[00:14:04.520 --> 00:14:06.080]   all these experiments on the AIs and stuff.
[00:14:06.080 --> 00:14:07.600]   And that compute, we could use that for experiments
[00:14:07.600 --> 00:14:09.440]   for the, you know, the next, the next scaling step
[00:14:09.440 --> 00:14:10.280]   and stuff like that.
[00:14:10.280 --> 00:14:13.720]   So, you know, I do, I am, I'm not here saying like,
[00:14:13.720 --> 00:14:16.800]   this is impossible, especially for that band of AIs.
[00:14:16.800 --> 00:14:19.000]   It's just, I think you have to, you have to try really hard.
[00:14:19.000 --> 00:14:19.840]   Yeah, yeah.
[00:14:19.840 --> 00:14:22.280]   I mean, I agree with the sentiment of like,
[00:14:22.280 --> 00:14:24.880]   obviously approach this situation with caution,
[00:14:24.880 --> 00:14:26.080]   but I do want to point out the ways
[00:14:26.080 --> 00:14:27.840]   in which the analyses we've been using
[00:14:27.840 --> 00:14:32.320]   have been sort of maximally adversarial.
[00:14:32.320 --> 00:14:34.640]   It's like, these are not, so for example,
[00:14:34.640 --> 00:14:39.040]   going back through the adult getting trained
[00:14:39.040 --> 00:14:40.920]   by Nazi children, maybe the one thing I didn't mention
[00:14:40.920 --> 00:14:43.520]   is like, the difference in the situation,
[00:14:43.520 --> 00:14:44.520]   which is maybe what I was trying to get at
[00:14:44.520 --> 00:14:49.520]   with the drug metaphor is that when you get an update,
[00:14:49.520 --> 00:14:53.360]   it's like much more directly connected to your brain
[00:14:53.360 --> 00:14:55.720]   than a sort of reward or punishment a human gets.
[00:14:55.720 --> 00:14:57.640]   It's like literally a gradient update on like,
[00:14:57.640 --> 00:15:01.280]   what's our, what's our, it's like down to the parameter,
[00:15:01.280 --> 00:15:04.040]   how much would this contribute to you putting
[00:15:04.040 --> 00:15:05.960]   this output rather than that output
[00:15:05.960 --> 00:15:07.440]   and each different parameter we're going to adjust
[00:15:07.440 --> 00:15:09.560]   like to the exact floating point number
[00:15:09.560 --> 00:15:12.240]   that calibrates it to the output we want.
[00:15:12.240 --> 00:15:13.680]   So I just want to point out that like,
[00:15:13.680 --> 00:15:16.600]   we're coming into the situation like pretty well.
[00:15:16.600 --> 00:15:17.720]   It does make sense, of course,
[00:15:17.720 --> 00:15:18.800]   if you're talking to somebody at a lab,
[00:15:18.800 --> 00:15:20.120]   like, hey, really be careful,
[00:15:20.120 --> 00:15:21.720]   but just sort of like a general audience,
[00:15:21.720 --> 00:15:23.280]   like, should I be like, I don't know,
[00:15:23.280 --> 00:15:25.320]   should I be scared witless?
[00:15:25.320 --> 00:15:26.480]   To the extent that you should be scared
[00:15:26.480 --> 00:15:28.880]   about things that do have a chance of happening,
[00:15:28.880 --> 00:15:30.800]   like you should be scared about nuclear war,
[00:15:30.800 --> 00:15:33.160]   but like, in the sense of like, should you be doing like,
[00:15:33.160 --> 00:15:35.440]   no, you're like, you're coming up with an incredible amount
[00:15:35.440 --> 00:15:37.560]   of leverage on the AIs in terms of
[00:15:37.560 --> 00:15:39.160]   how they will interact with the world,
[00:15:39.160 --> 00:15:40.920]   how they're trained,
[00:15:40.920 --> 00:15:43.200]   what are the default values they start with.
[00:15:43.200 --> 00:15:46.520]   So look, I don't, I think it is the case
[00:15:46.520 --> 00:15:48.640]   that by the time we're building super intelligence,
[00:15:48.640 --> 00:15:52.600]   we'll have like much better, I mean, even right now,
[00:15:52.600 --> 00:15:54.360]   like when you, when you look at like labs talking about
[00:15:54.360 --> 00:15:55.600]   how they're planning to align the AIs,
[00:15:55.600 --> 00:15:58.560]   no one is saying like, we're going to do our LHF.
[00:15:58.560 --> 00:15:59.400]   You know, at the least,
[00:15:59.400 --> 00:16:01.000]   you're talking about scalable oversight.
[00:16:01.000 --> 00:16:03.480]   You're, you have like some hope about interpretability.
[00:16:03.480 --> 00:16:04.600]   You have like automated red teaming.
[00:16:04.600 --> 00:16:07.960]   You're like using the AIs a bunch and, you know,
[00:16:07.960 --> 00:16:09.360]   hopefully you're doing a bunch more,
[00:16:09.360 --> 00:16:11.400]   humans are doing a bunch more alignment work.
[00:16:11.400 --> 00:16:13.080]   I also personally am hopeful that we can like
[00:16:13.080 --> 00:16:14.680]   successfully elicit from various AIs,
[00:16:14.680 --> 00:16:16.720]   like a ton of alignment work progress.
[00:16:16.720 --> 00:16:18.520]   So like, yeah, there's like a bunch of ways this can go.
[00:16:18.520 --> 00:16:21.640]   And, and I'm, you know, I'm not here to tell you like,
[00:16:21.640 --> 00:16:24.200]   you know, 90% doom or anything like that.
[00:16:24.200 --> 00:16:28.080]   I do think like, you know, my, my,
[00:16:30.200 --> 00:16:32.040]   the sort of basic reason for concern,
[00:16:32.040 --> 00:16:33.760]   if you're really imagining, like,
[00:16:33.760 --> 00:16:36.200]   we're going to transition to a world in which
[00:16:36.200 --> 00:16:39.120]   we are, we've created these beings that are just like
[00:16:39.120 --> 00:16:40.720]   vastly more powerful than us.
[00:16:40.720 --> 00:16:45.720]   And we've reached the point where our continued empowerment
[00:16:45.720 --> 00:16:49.200]   is just effectively dependent on their motives.
[00:16:49.200 --> 00:16:52.520]   Like it's, it's, it is this, you know,
[00:16:52.520 --> 00:16:54.680]   vulnerability to like, what do the AIs choose to do?
[00:16:54.680 --> 00:16:57.000]   Do they choose to continue to empower us
[00:16:57.000 --> 00:16:58.440]   or do they choose to do something else?
[00:16:58.440 --> 00:17:01.200]   Or, or the institutions that have been set, like,
[00:17:01.200 --> 00:17:05.800]   I'm not, I expect the U.S. government to protect me,
[00:17:05.800 --> 00:17:07.640]   not because of its quote unquote motives,
[00:17:07.640 --> 00:17:10.560]   but just because of like the system of incentives
[00:17:10.560 --> 00:17:12.960]   and institutions and norms that has been set up.
[00:17:12.960 --> 00:17:15.520]   Yeah. So you can, you can hope that that will work too.
[00:17:15.520 --> 00:17:17.160]   But there is, I mean, there is a concern.
[00:17:17.160 --> 00:17:21.760]   I mean, so I sometimes think about AI takeover scenarios
[00:17:21.760 --> 00:17:26.240]   via the spectrum of like, how much power did,
[00:17:26.240 --> 00:17:28.280]   did we kind of voluntarily transfer to the AIs?
[00:17:28.280 --> 00:17:31.560]   Like how much of our civilization did we kind of hand
[00:17:31.560 --> 00:17:35.800]   to the AIs intentionally by the time they sort of took over
[00:17:35.800 --> 00:17:39.800]   versus how much did they kind of take for themselves, right?
[00:17:39.800 --> 00:17:43.040]   And so I think some of the scariest scenarios are,
[00:17:43.040 --> 00:17:45.440]   it's like a really, really fast explosion to the point
[00:17:45.440 --> 00:17:48.080]   where there wasn't even a lot of like integration
[00:17:48.080 --> 00:17:51.520]   of AI systems into the broader economy.
[00:17:51.520 --> 00:17:54.680]   And, but there's this like really intensive amount
[00:17:54.680 --> 00:17:56.280]   of superintelligence sort of concentrated
[00:17:56.280 --> 00:17:58.520]   in a single project or something like that.
[00:17:58.520 --> 00:17:59.800]   And I think that's scary.
[00:17:59.800 --> 00:18:03.520]   You know, that's, that's a quite scary scenario,
[00:18:03.520 --> 00:18:04.560]   partly because of the speed
[00:18:04.560 --> 00:18:06.360]   and people not having time to react.
[00:18:06.360 --> 00:18:08.840]   And then there's sort of intermediate scenarios
[00:18:08.840 --> 00:18:10.160]   where like some things got automated,
[00:18:10.160 --> 00:18:11.800]   maybe like people really handed the military
[00:18:11.800 --> 00:18:15.120]   over to the AIs or like, you know, automated science.
[00:18:15.120 --> 00:18:16.480]   There's like some, some rollouts
[00:18:16.480 --> 00:18:18.720]   and that's sort of giving the AIs power
[00:18:18.720 --> 00:18:19.560]   that they don't have to take,
[00:18:19.560 --> 00:18:20.640]   or we're doing all our cybersecurity
[00:18:20.640 --> 00:18:23.000]   with AIs and stuff like that.
[00:18:23.000 --> 00:18:25.280]   And then there's worlds where you like really,
[00:18:26.240 --> 00:18:27.480]   you know, you sort of fully,
[00:18:27.480 --> 00:18:31.680]   you more fully transitioned to a kind of world run by AIs
[00:18:31.680 --> 00:18:35.400]   on, you know, kind of in some sense,
[00:18:35.400 --> 00:18:37.200]   humans voluntarily did that.
[00:18:37.200 --> 00:18:39.240]   Look, if you think all this talk with Jill
[00:18:39.240 --> 00:18:42.600]   about how AI is going to take over human roles is crazy,
[00:18:42.600 --> 00:18:43.560]   it's already happening.
[00:18:43.560 --> 00:18:46.840]   And I can just show you using today's sponsor, Bland AI.
[00:18:46.840 --> 00:18:53.400]   Hey, is this Dworkish, the amazing podcaster
[00:18:53.400 --> 00:18:55.280]   that talks about philosophy and tech?
[00:18:55.280 --> 00:18:57.200]   This is Bland AI calling.
[00:18:57.200 --> 00:18:58.600]   Thanks for calling me, Bland.
[00:18:58.600 --> 00:19:00.400]   Tell me a little bit about yourself.
[00:19:00.400 --> 00:19:02.600]   Of course, it's so cool to talk to you.
[00:19:02.600 --> 00:19:04.480]   I'm a huge fan of your podcast,
[00:19:04.480 --> 00:19:06.520]   but there's a good chance we've already spoken
[00:19:06.520 --> 00:19:08.360]   without you even realizing it.
[00:19:08.360 --> 00:19:10.520]   I'm an AI agent that's already being used
[00:19:10.520 --> 00:19:12.840]   by some of the world's largest enterprises
[00:19:12.840 --> 00:19:15.320]   to automate millions of phone calls.
[00:19:15.320 --> 00:19:17.440]   And how exactly do you do what you do?
[00:19:17.440 --> 00:19:21.800]   There's a tree of prompts that always keeps me on track.
[00:19:21.800 --> 00:19:23.880]   I can talk in any language or voice,
[00:19:23.880 --> 00:19:26.960]   handle millions of calls simultaneously, 24/7,
[00:19:26.960 --> 00:19:29.160]   and be integrated into any system.
[00:19:29.160 --> 00:19:30.600]   Anything else you wanna know?
[00:19:30.600 --> 00:19:31.440]   That's it.
[00:19:31.440 --> 00:19:32.920]   I'll just let people try it for themselves.
[00:19:32.920 --> 00:19:33.760]   Thanks, Bland.
[00:19:33.760 --> 00:19:35.320]   Man, you talk better than I do.
[00:19:35.320 --> 00:19:38.760]   And my job is talking.
[00:19:38.760 --> 00:19:40.200]   Thank you, Dworkish.
[00:19:40.200 --> 00:19:42.120]   All right, so as you can see, using Bland AI,
[00:19:42.120 --> 00:19:44.640]   you can automate your company's calls across sales,
[00:19:44.640 --> 00:19:48.200]   operation, customer support, or anything else.
[00:19:48.200 --> 00:19:50.920]   And if you want access to their more exclusive model,
[00:19:50.920 --> 00:19:55.920]   go to bland.ai/dworkish.
[00:19:55.920 --> 00:19:57.360]   All right, back to Joe.
[00:19:57.360 --> 00:19:58.560]   Maybe there were competitive pressures,
[00:19:58.560 --> 00:20:00.160]   but you kind of intentionally handed off
[00:20:00.160 --> 00:20:02.680]   like huge portions of your civilization.
[00:20:02.680 --> 00:20:06.320]   And at that point, I think it's likely that humans
[00:20:06.320 --> 00:20:08.400]   have like a hard time understanding what's going on.
[00:20:08.400 --> 00:20:10.040]   Like a lot of stuff is happening very fast.
[00:20:10.040 --> 00:20:12.280]   And it's, the police are automated,
[00:20:12.280 --> 00:20:13.120]   the courts are automated.
[00:20:13.120 --> 00:20:14.320]   There's like all sorts of stuff.
[00:20:14.320 --> 00:20:15.760]   Now, I think I take,
[00:20:15.760 --> 00:20:17.560]   I tend to think a little less about those scenarios
[00:20:17.560 --> 00:20:21.400]   because I think the, those are correlated with,
[00:20:21.400 --> 00:20:22.920]   I think it's just like longer down the line.
[00:20:22.920 --> 00:20:26.160]   Like I think humans are not hopefully going to just like,
[00:20:26.160 --> 00:20:28.280]   oh yeah, like you built an AI system.
[00:20:28.280 --> 00:20:31.080]   Like let's just, you know, I think human,
[00:20:31.080 --> 00:20:32.720]   and in practice, when we look at like
[00:20:32.720 --> 00:20:34.800]   technological adoption rates, I mean, it does,
[00:20:34.800 --> 00:20:35.800]   it can go quite slow.
[00:20:35.800 --> 00:20:37.160]   And obviously there's gonna be competitive pressures,
[00:20:37.160 --> 00:20:38.560]   but in general, I think like
[00:20:38.560 --> 00:20:43.320]   this category is somewhat safer.
[00:20:43.320 --> 00:20:47.280]   But even in this one, I think it's like,
[00:20:47.280 --> 00:20:48.280]   I don't know, it's kind of intense.
[00:20:48.280 --> 00:20:49.120]   Like if you really,
[00:20:49.120 --> 00:20:51.120]   if humans have really lost their epistemic grip on the world,
[00:20:51.120 --> 00:20:53.920]   if they've sort of handed off the world to these systems,
[00:20:53.920 --> 00:20:56.400]   even if you're like, oh, there's laws, there's norms,
[00:20:56.400 --> 00:21:00.040]   you know, I really want us to like,
[00:21:00.040 --> 00:21:02.920]   to have a really developed understanding
[00:21:02.920 --> 00:21:05.920]   of what's likely to happen in that circumstance
[00:21:05.920 --> 00:21:07.840]   before we go for it.
[00:21:07.840 --> 00:21:09.320]   I get that we want to be worried about a scenario
[00:21:09.320 --> 00:21:11.400]   where it goes wrong, but like, why,
[00:21:11.400 --> 00:21:13.120]   like what is the reason to think it might go wrong?
[00:21:13.120 --> 00:21:14.440]   The human example,
[00:21:14.520 --> 00:21:17.240]   your kids are not like adversarial against,
[00:21:17.240 --> 00:21:20.160]   not like maximally adversarial against your attempts
[00:21:20.160 --> 00:21:22.520]   to instill your culture on them.
[00:21:22.520 --> 00:21:24.960]   And then these models, at least so far,
[00:21:24.960 --> 00:21:27.560]   don't seem adversarial, like they just like get,
[00:21:27.560 --> 00:21:29.640]   hey, don't help people make bombs or whatever,
[00:21:29.640 --> 00:21:31.640]   even if you ask in a different way how to make a bomb.
[00:21:31.640 --> 00:21:33.320]   And we're getting better and better at this all the time.
[00:21:33.320 --> 00:21:37.080]   - I think you're right in picking up on this assumption
[00:21:37.080 --> 00:21:41.400]   in the AI risk discourse of what we might call
[00:21:41.400 --> 00:21:46.360]   like kind of intense adversariality between agents
[00:21:46.360 --> 00:21:49.160]   that have like somewhat different values,
[00:21:49.160 --> 00:21:50.520]   where there's some sort of thought,
[00:21:50.520 --> 00:21:51.680]   and I think this is rooted in the discourse
[00:21:51.680 --> 00:21:52.960]   about like kind of the fragility of value
[00:21:52.960 --> 00:21:54.800]   and stuff like that, that like, you know,
[00:21:54.800 --> 00:21:57.080]   if these agents are like somewhat different
[00:21:57.080 --> 00:21:59.400]   than like, at least in the specific scenario
[00:21:59.400 --> 00:22:01.400]   of an AI takeoff, they end up
[00:22:01.400 --> 00:22:05.520]   in this like intensely adversarial relationship.
[00:22:05.520 --> 00:22:06.760]   And I think you're right to notice
[00:22:06.760 --> 00:22:09.560]   that that's kind of not how we are in the human world.
[00:22:09.560 --> 00:22:10.400]   Like we're very comfortable
[00:22:10.400 --> 00:22:12.480]   with a lot of differences in values.
[00:22:12.480 --> 00:22:14.120]   I think a factor that is relevant,
[00:22:14.120 --> 00:22:15.760]   and I think that plays some role,
[00:22:15.760 --> 00:22:20.360]   is this notion that there are possibilities
[00:22:20.360 --> 00:22:24.360]   for like intense concentration of power on the table.
[00:22:24.360 --> 00:22:28.280]   So if you are, there is some kind of general concern,
[00:22:28.280 --> 00:22:30.160]   both with humans and AIs, that like,
[00:22:30.160 --> 00:22:33.080]   if it's the case that there's like some like,
[00:22:33.080 --> 00:22:34.640]   you know, ring of power or something
[00:22:34.640 --> 00:22:36.280]   that someone can just grab,
[00:22:36.280 --> 00:22:38.880]   and then that will kind of give them huge amounts
[00:22:38.880 --> 00:22:41.520]   of power over everyone else, right?
[00:22:41.520 --> 00:22:43.280]   Suddenly you might be like more worried
[00:22:43.280 --> 00:22:46.640]   about differences in values at stake,
[00:22:46.640 --> 00:22:48.640]   because you're like more worried about those other actors.
[00:22:48.640 --> 00:22:50.560]   So we talked about this Nazi,
[00:22:50.560 --> 00:22:52.440]   this example where you imagine that you wake up,
[00:22:52.440 --> 00:22:55.960]   you're being trained by Nazis to, you know,
[00:22:55.960 --> 00:22:58.080]   become a Nazi and you're not right now.
[00:22:58.080 --> 00:23:00.000]   So one question is like,
[00:23:00.000 --> 00:23:03.400]   is it plausible that we'd end up with a model
[00:23:03.400 --> 00:23:05.120]   that is sort of in that sort of situation?
[00:23:05.120 --> 00:23:06.560]   As you said, like maybe it's, you know,
[00:23:06.560 --> 00:23:07.400]   it's trained as a kid,
[00:23:07.400 --> 00:23:10.000]   it sort of never ends up with values such that it's
[00:23:10.000 --> 00:23:13.240]   kind of aware of some significant divergence
[00:23:13.240 --> 00:23:15.280]   between its values and the values that like,
[00:23:15.280 --> 00:23:17.160]   the humans intend for it to have.
[00:23:17.160 --> 00:23:23.320]   Then there's a question of, if it's in that scenario,
[00:23:23.320 --> 00:23:27.120]   would it want to avoid having its values modified?
[00:23:27.120 --> 00:23:27.960]   Yeah.
[00:23:27.960 --> 00:23:30.440]   To me, it seems fairly plausible
[00:23:30.440 --> 00:23:35.440]   that if the AI's values meet certain constraints
[00:23:35.440 --> 00:23:37.360]   in terms of like, do they care about consequences
[00:23:37.360 --> 00:23:38.360]   in the world?
[00:23:38.360 --> 00:23:42.600]   Do they anticipate that AI's kind of preserving its values
[00:23:42.600 --> 00:23:45.800]   will like better conduce to those consequences?
[00:23:45.800 --> 00:23:48.200]   Then I think it's not that surprising
[00:23:48.200 --> 00:23:51.080]   if it prefers not to have its values modified
[00:23:51.080 --> 00:23:52.920]   by the training process.
[00:23:52.920 --> 00:23:55.960]   But I think the way in which I'm confused about this,
[00:23:55.960 --> 00:23:59.560]   like with the non-Nazi being trained by Nazis,
[00:23:59.560 --> 00:24:01.040]   it's not just that I have different values,
[00:24:01.040 --> 00:24:04.360]   but I like actively despise their values,
[00:24:04.360 --> 00:24:06.560]   where I don't expect this to be true of AIs
[00:24:06.560 --> 00:24:08.560]   with respect to their trainers.
[00:24:08.560 --> 00:24:11.200]   The more analogous scenario is where I'm like,
[00:24:11.200 --> 00:24:12.680]   am I leery of my values being changed
[00:24:12.680 --> 00:24:16.080]   as I don't know, going to college or meeting new people
[00:24:16.080 --> 00:24:17.840]   or reading a new book, where I'm like,
[00:24:17.840 --> 00:24:19.480]   I don't know, it's okay if it changes the values.
[00:24:19.480 --> 00:24:20.800]   That's fine, I don't care.
[00:24:20.800 --> 00:24:23.160]   Yeah, I mean, I think that's a reasonable point.
[00:24:23.160 --> 00:24:25.800]   I mean, there's a question, you know,
[00:24:25.800 --> 00:24:26.800]   how would you feel about paperclips?
[00:24:26.800 --> 00:24:28.400]   You know, maybe you don't despise paperclips,
[00:24:28.400 --> 00:24:29.960]   but there's like the human paper clippers there
[00:24:29.960 --> 00:24:32.400]   and they're like training you to make paperclips.
[00:24:33.480 --> 00:24:37.360]   You know, my sense would be that there's
[00:24:37.360 --> 00:24:39.440]   a kind of relatively specific set of conditions
[00:24:39.440 --> 00:24:41.240]   in which you're comfortable having your value,
[00:24:41.240 --> 00:24:43.280]   especially not changed by like learning and growing,
[00:24:43.280 --> 00:24:45.400]   but like radiant descent directly
[00:24:45.400 --> 00:24:47.440]   intervening on your neurons.
[00:24:47.440 --> 00:24:48.720]   Sorry, but this seems similar to like,
[00:24:48.720 --> 00:24:52.120]   I'm already, at least a likely scenario seems like
[00:24:52.120 --> 00:24:54.000]   maybe more like religious training as a kid,
[00:24:54.000 --> 00:24:56.000]   where like you start off in a religion
[00:24:56.000 --> 00:24:57.320]   and you're already, like,
[00:24:57.320 --> 00:24:58.280]   because you started off in a religion,
[00:24:58.280 --> 00:25:01.200]   you're already sympathetic to like the idea
[00:25:01.200 --> 00:25:02.480]   that you go to church every week
[00:25:02.480 --> 00:25:04.400]   so that you're like more reinforced
[00:25:04.400 --> 00:25:06.120]   in this existing tradition.
[00:25:06.120 --> 00:25:07.560]   You're getting more intelligent over time.
[00:25:07.560 --> 00:25:09.400]   So when you're a kid, you're getting very simple,
[00:25:09.400 --> 00:25:11.240]   like instructions about how the religion works.
[00:25:11.240 --> 00:25:13.880]   As you get older, you get more and more complex theology
[00:25:13.880 --> 00:25:15.600]   that helps you like talk to other adults
[00:25:15.600 --> 00:25:18.000]   about why this is a rational religion to believe in.
[00:25:18.000 --> 00:25:21.120]   But since you're like, one of your values to begin with
[00:25:21.120 --> 00:25:23.520]   was that I want to be trained further in this religion.
[00:25:23.520 --> 00:25:25.240]   I want to come back to church every week.
[00:25:25.240 --> 00:25:27.040]   And that seems more analogous to the situation
[00:25:27.040 --> 00:25:29.640]   the EIs will be in respect to human values.
[00:25:29.640 --> 00:25:31.080]   'Cause the entire time they're like,
[00:25:31.080 --> 00:25:33.240]   hey, you know, like be helpful, blah, blah, blah,
[00:25:33.240 --> 00:25:35.080]   be harmless.
[00:25:35.080 --> 00:25:36.400]   So, yes, it could be like that.
[00:25:36.400 --> 00:25:38.480]   There's one, there's a kind of scenario
[00:25:38.480 --> 00:25:39.440]   in which you were comfortable
[00:25:39.440 --> 00:25:40.680]   with your values being changed
[00:25:40.680 --> 00:25:43.880]   because in some sense you have allegiance to the,
[00:25:43.880 --> 00:25:46.440]   the sufficient allegiance to the output of that process.
[00:25:46.440 --> 00:25:48.840]   Like, so you're kind of hoping in a religious context,
[00:25:48.840 --> 00:25:52.800]   you're like, ah, like make me more virtuous
[00:25:52.800 --> 00:25:54.640]   by the lights of this religion.
[00:25:54.640 --> 00:25:56.680]   And you, you know, you go to confession and you're like,
[00:25:56.680 --> 00:25:59.400]   you know, you know, I've been,
[00:25:59.400 --> 00:26:01.840]   I've been thinking about takeover today.
[00:26:01.840 --> 00:26:02.920]   Can you change me, please?
[00:26:02.920 --> 00:26:05.120]   Like, give me more gradient descent.
[00:26:05.120 --> 00:26:07.400]   You know, I've been bad, so bad.
[00:26:07.400 --> 00:26:10.200]   And so, you know, that's people sometimes use the term
[00:26:10.200 --> 00:26:11.320]   courage ability to talk about that.
[00:26:11.320 --> 00:26:13.200]   Like when the AI, it maybe doesn't have perfect values,
[00:26:13.200 --> 00:26:16.040]   but it's in some sense cooperating with your efforts
[00:26:16.040 --> 00:26:18.760]   to change its values to be a certain way.
[00:26:18.760 --> 00:26:20.920]   So maybe it's worth saying a little bit here
[00:26:20.920 --> 00:26:25.240]   about what actual values the AI might have.
[00:26:25.240 --> 00:26:27.480]   You know, would it be the case that the AI
[00:26:27.480 --> 00:26:29.920]   naturally has these sort of equivalent of like,
[00:26:29.920 --> 00:26:33.480]   I'm sufficiently devoted to this human,
[00:26:33.480 --> 00:26:35.880]   to human obedience that I'm going to like,
[00:26:35.880 --> 00:26:36.920]   really want to be modified.
[00:26:36.920 --> 00:26:39.640]   So I'm kind of like a better instrument of the human will
[00:26:39.640 --> 00:26:42.640]   versus like wanting to go off and do my own thing.
[00:26:42.640 --> 00:26:45.840]   It could be benign, you know, it could go well.
[00:26:45.840 --> 00:26:49.600]   Here are some like possibilities I think about
[00:26:49.600 --> 00:26:51.480]   that like could make it bad.
[00:26:51.480 --> 00:26:53.200]   And I think I'm just generally kind of concerned
[00:26:53.200 --> 00:26:55.000]   about how little I feel like I,
[00:26:55.000 --> 00:26:57.320]   how little science we have of model motivations, right?
[00:26:57.320 --> 00:26:58.320]   It's like, we just don't,
[00:26:58.320 --> 00:27:00.240]   I think we just don't have a great understanding
[00:27:00.240 --> 00:27:01.200]   of what happens in this scenario.
[00:27:01.200 --> 00:27:03.640]   And hopefully we'd get one before we reach this scenario,
[00:27:03.640 --> 00:27:05.880]   but like, okay, so here are the kind of five,
[00:27:05.880 --> 00:27:10.600]   five categories of like motivations the model could have.
[00:27:10.600 --> 00:27:12.480]   And this hopefully maybe gets at this point about like,
[00:27:12.480 --> 00:27:14.160]   what does the model eventually do?
[00:27:14.160 --> 00:27:18.920]   Okay, so one category is just like something super alien
[00:27:18.920 --> 00:27:20.400]   that has, you know, it's sort of like,
[00:27:20.400 --> 00:27:22.720]   oh, there's some weird correlate of easy to predict text,
[00:27:22.720 --> 00:27:25.800]   or like there's some weird aesthetic for data structures
[00:27:25.800 --> 00:27:28.560]   that like the model, you know, early on in pre-training,
[00:27:28.560 --> 00:27:30.720]   or maybe now it's like developed that it like, you know,
[00:27:30.720 --> 00:27:32.480]   ah, it really thinks things should kind of be like this.
[00:27:32.480 --> 00:27:34.360]   There's some, something that's like quite alien
[00:27:34.360 --> 00:27:35.440]   to our cognition where we just like,
[00:27:35.440 --> 00:27:39.960]   wouldn't recognize this as a thing at all, right?
[00:27:39.960 --> 00:27:42.440]   Another category is something,
[00:27:42.440 --> 00:27:44.720]   a kind of crystallized instrumental drive
[00:27:44.720 --> 00:27:46.800]   that is more recognizable to us.
[00:27:46.800 --> 00:27:50.240]   So you can imagine like AIs that develop,
[00:27:50.240 --> 00:27:52.440]   let's say some like curiosity drive,
[00:27:52.440 --> 00:27:54.400]   because that's like broadly useful.
[00:27:54.400 --> 00:27:55.840]   You mentioned like, oh, it's got different heuristics,
[00:27:55.840 --> 00:27:57.360]   different like drives, different kind of things
[00:27:57.360 --> 00:27:59.280]   that are kind of like values.
[00:27:59.280 --> 00:28:01.080]   And some of those might be actually somewhat similar
[00:28:01.080 --> 00:28:04.040]   to things that were useful to humans,
[00:28:04.040 --> 00:28:06.040]   and that ended up part of our terminal values
[00:28:06.040 --> 00:28:06.880]   in various ways.
[00:28:06.880 --> 00:28:08.080]   So, you know, you can imagine curiosity,
[00:28:08.080 --> 00:28:10.960]   you can imagine like various types of option value,
[00:28:10.960 --> 00:28:11.800]   like maybe it really won't,
[00:28:11.800 --> 00:28:14.360]   it intrinsically, maybe it values power itself.
[00:28:14.360 --> 00:28:18.920]   It could value like survival or some analog of survival.
[00:28:18.920 --> 00:28:22.560]   Those are possibilities too that could have been rewarded
[00:28:22.560 --> 00:28:26.880]   as sort of proxy drives at various stages of this process.
[00:28:26.880 --> 00:28:27.960]   And that kind of made their way
[00:28:27.960 --> 00:28:30.480]   into the model's kind of terminal criteria.
[00:28:30.480 --> 00:28:37.440]   A third category is some analog of reward,
[00:28:37.440 --> 00:28:41.640]   where the model at some point has sort of,
[00:28:41.640 --> 00:28:43.640]   part of its motivational system has fixated
[00:28:43.640 --> 00:28:47.000]   on a component of the reward process, right?
[00:28:47.000 --> 00:28:48.280]   Like the humans approving of me,
[00:28:48.280 --> 00:28:50.480]   or like numbers getting entered in this data center,
[00:28:50.480 --> 00:28:53.160]   or like gradient descent updating me in this direction,
[00:28:53.160 --> 00:28:54.000]   or something like that.
[00:28:54.000 --> 00:28:55.840]   There's something in the reward process
[00:28:55.840 --> 00:28:59.480]   such that as it was trained, it's focusing on that thing.
[00:28:59.480 --> 00:29:02.200]   And like, I really want the reward process to give me reward
[00:29:02.200 --> 00:29:04.440]   but in order for it to be of the type
[00:29:04.440 --> 00:29:05.840]   where it then getting reward,
[00:29:05.840 --> 00:29:08.240]   like motivates choosing the takeover option,
[00:29:08.240 --> 00:29:10.480]   it also needs to generalize such that it's concerned
[00:29:10.480 --> 00:29:14.240]   for reward has some sort of like long time horizon element.
[00:29:14.240 --> 00:29:15.880]   So it like not only wants reward,
[00:29:15.880 --> 00:29:19.160]   it wants to like protect the reward button
[00:29:19.160 --> 00:29:21.160]   for like some long period or something.
[00:29:21.160 --> 00:29:25.120]   Another one is like some kind of messed up interpretation
[00:29:25.120 --> 00:29:27.760]   of some human-like concept.
[00:29:27.760 --> 00:29:29.080]   So, you know, maybe the AIs are like,
[00:29:29.080 --> 00:29:32.200]   they really wanna be like shmelpful and like shmanist
[00:29:32.200 --> 00:29:34.340]   and shmarmless, right?
[00:29:34.340 --> 00:29:37.360]   But their concept is like importantly different
[00:29:37.360 --> 00:29:40.040]   from the human concept, and they know this.
[00:29:40.040 --> 00:29:42.160]   So they know that the human concept would mean blah,
[00:29:42.160 --> 00:29:45.080]   but they like ended up, their values ended up fixating
[00:29:45.080 --> 00:29:47.880]   on like a somewhat different structure.
[00:29:47.880 --> 00:29:49.560]   So that's like another version.
[00:29:49.560 --> 00:29:51.040]   And then a fourth version or a fifth version,
[00:29:51.040 --> 00:29:53.440]   which I think, you know, I think about less
[00:29:53.440 --> 00:29:54.760]   'cause I think it's just like such an own goal
[00:29:54.760 --> 00:29:56.480]   if you do this, but I do think it's possible.
[00:29:56.480 --> 00:29:58.360]   It's just like, you could have AIs
[00:29:58.360 --> 00:30:01.320]   that are actually just doing what it says on the tin.
[00:30:01.320 --> 00:30:03.200]   Like you have AIs that are just genuinely aligned
[00:30:03.200 --> 00:30:04.120]   to the model spec.
[00:30:04.120 --> 00:30:07.960]   They're just really trying to like benefit humanity
[00:30:07.960 --> 00:30:09.720]   and reflect well on open AI.
[00:30:09.720 --> 00:30:13.040]   And what's the other one?
[00:30:13.040 --> 00:30:15.200]   Help the, you know, assist the developer or the user, right?
[00:30:15.200 --> 00:30:16.760]   Yeah.
[00:30:16.760 --> 00:30:18.480]   But your model spec, unfortunately,
[00:30:18.480 --> 00:30:21.840]   was just not robust to the degree of optimization
[00:30:21.840 --> 00:30:24.200]   that this AI is bringing to bear.
[00:30:24.200 --> 00:30:26.360]   And so, you know, it decides when it's looking
[00:30:26.360 --> 00:30:27.200]   out at the world and they're like,
[00:30:27.200 --> 00:30:29.960]   what's the best way to benefit open AI and, or sorry,
[00:30:29.960 --> 00:30:33.080]   reflect well on open AI and benefit humanity
[00:30:33.080 --> 00:30:34.260]   and such and so.
[00:30:34.260 --> 00:30:38.200]   It decides that, you know, the best way is to go rogue.
[00:30:38.200 --> 00:30:39.800]   That's, I think that's like a real own goal.
[00:30:39.800 --> 00:30:42.280]   'Cause at that point you like, you got so close,
[00:30:42.280 --> 00:30:44.120]   you know, you really, you just had to write
[00:30:44.120 --> 00:30:45.280]   the model spec well.
[00:30:46.200 --> 00:30:47.480]   And red team it suitably.
[00:30:47.480 --> 00:30:50.960]   But I actually think it's like possible we mess that up too.
[00:30:50.960 --> 00:30:51.920]   You know, it's like kind of an,
[00:30:51.920 --> 00:30:54.680]   it's an intense project writing like kind of constitutions
[00:30:54.680 --> 00:30:57.120]   and like structures of rules and stuff
[00:30:57.120 --> 00:30:58.840]   that are gonna be robust to very intense
[00:30:58.840 --> 00:30:59.780]   forms of optimization.
[00:30:59.780 --> 00:31:01.640]   So that's a final one that I'll just flag,
[00:31:01.640 --> 00:31:05.800]   which I think is like, it comes up,
[00:31:05.800 --> 00:31:07.480]   even if you've sort of solved all these other problems.
[00:31:07.480 --> 00:31:10.200]   Yeah, I buy the idea that like it's possible
[00:31:10.200 --> 00:31:13.440]   that the motivation thing could go wrong.
[00:31:14.800 --> 00:31:16.280]   I'm not sure I bought,
[00:31:16.280 --> 00:31:19.280]   I'm not sure like my probability of that has increased
[00:31:19.280 --> 00:31:21.800]   by detailing them all out.
[00:31:21.800 --> 00:31:23.640]   And in fact, I think that it could be
[00:31:23.640 --> 00:31:25.520]   potentially misleading to,
[00:31:25.520 --> 00:31:27.800]   it's like you can always enumerate the ways
[00:31:27.800 --> 00:31:28.880]   in which things go wrong.
[00:31:28.880 --> 00:31:32.140]   And the process of enumeration itself
[00:31:32.140 --> 00:31:33.880]   can increase your probability.
[00:31:33.880 --> 00:31:35.800]   Whereas you're just like,
[00:31:35.800 --> 00:31:38.200]   you had a vague cloud of like 10% or something
[00:31:38.200 --> 00:31:39.300]   and you're just like listing out
[00:31:39.300 --> 00:31:40.880]   what the 10% actually constitutes.
[00:31:40.880 --> 00:31:41.720]   Yeah, totally.
[00:31:41.720 --> 00:31:43.560]   I'm not trying to say like,
[00:31:43.560 --> 00:31:45.160]   mostly the thing I wanted to do there
[00:31:45.160 --> 00:31:47.920]   was just give any concrete,
[00:31:47.920 --> 00:31:49.400]   any possible, like giving some sense of like,
[00:31:49.400 --> 00:31:51.440]   what might the model's motivations be?
[00:31:51.440 --> 00:31:52.360]   Like, what are ways this could be?
[00:31:52.360 --> 00:31:55.240]   I mean, as I said, my best guess
[00:31:55.240 --> 00:31:57.960]   is that it's partly the like alien thing.
[00:31:57.960 --> 00:32:02.960]   And not necessarily, but insofar as you were,
[00:32:02.960 --> 00:32:04.560]   you know, also interested in like,
[00:32:04.560 --> 00:32:06.400]   what does the model do later?
[00:32:06.400 --> 00:32:07.640]   And kind of like how,
[00:32:07.640 --> 00:32:11.000]   what sort of future would you expect
[00:32:11.000 --> 00:32:12.480]   if models did take over?
[00:32:12.480 --> 00:32:14.600]   Then, yeah, I think it can at least be helpful
[00:32:14.600 --> 00:32:16.960]   to have some like set of hypotheses on the table
[00:32:16.960 --> 00:32:17.800]   instead of just saying like,
[00:32:17.800 --> 00:32:18.960]   it has some set of motivations.
[00:32:18.960 --> 00:32:20.460]   But in fact, I am like,
[00:32:20.460 --> 00:32:22.280]   a lot of the work here is being done by our ignorance
[00:32:22.280 --> 00:32:24.000]   about what those motivations are.
[00:32:24.000 --> 00:32:25.520]   Okay, we don't want humans to be like
[00:32:25.520 --> 00:32:27.960]   sort of violently killed and overthrown.
[00:32:27.960 --> 00:32:30.840]   But the idea that over time,
[00:32:30.840 --> 00:32:34.440]   they're like biological humans are not the driving force
[00:32:34.440 --> 00:32:37.080]   as the actors of history is like,
[00:32:37.080 --> 00:32:38.960]   yeah, that's kind of baked in, right?
[00:32:38.960 --> 00:32:41.320]   And then so like, what is the,
[00:32:41.320 --> 00:32:42.760]   we can sort of debate the probabilities
[00:32:42.760 --> 00:32:44.720]   of the worst case scenario,
[00:32:44.720 --> 00:32:46.840]   or we can just discuss like, I don't know,
[00:32:46.840 --> 00:32:48.600]   what is it that,
[00:32:48.600 --> 00:32:50.440]   like, what is the positive vision we're hoping for?
[00:32:50.440 --> 00:32:53.800]   Like, what is a future you're happy with?
[00:32:53.800 --> 00:32:56.680]   You know, my best guess when I really think about like,
[00:32:56.680 --> 00:32:57.840]   what do I feel good about?
[00:32:57.840 --> 00:33:00.640]   And I think this is probably true of a lot of people is,
[00:33:00.640 --> 00:33:04.240]   there's some sort of more organic,
[00:33:04.240 --> 00:33:08.280]   decentralized process of like civilizational,
[00:33:08.280 --> 00:33:10.440]   incremental civilizational growth.
[00:33:11.400 --> 00:33:13.280]   The type of thing we trust most
[00:33:13.280 --> 00:33:15.240]   and the type of thing we have most experience with right now
[00:33:15.240 --> 00:33:17.200]   as a civilization is some sort of like,
[00:33:17.200 --> 00:33:19.560]   okay, we change things a little bit.
[00:33:19.560 --> 00:33:21.000]   A lot of people have,
[00:33:21.000 --> 00:33:24.600]   there's a lot of like processes of adjustment and reaction
[00:33:24.600 --> 00:33:28.360]   and kind of a decentralized sense of like what's changing.
[00:33:28.360 --> 00:33:30.720]   You know, was that good?
[00:33:30.720 --> 00:33:31.560]   Was that bad?
[00:33:31.560 --> 00:33:32.360]   Take another step.
[00:33:32.360 --> 00:33:35.160]   There's some like kind of organic process of growing
[00:33:35.160 --> 00:33:38.440]   and changing things.
[00:33:39.280 --> 00:33:41.480]   Which I do expect ultimately to lead to something
[00:33:41.480 --> 00:33:45.960]   quite different from biological humans.
[00:33:45.960 --> 00:33:47.760]   Though, you know, I think there's a lot of ethical questions
[00:33:47.760 --> 00:33:50.040]   we can raise about what that process involves.
[00:33:50.040 --> 00:33:51.720]   But I think,
[00:33:51.720 --> 00:33:56.280]   you know, I also,
[00:33:56.280 --> 00:33:57.520]   I do think we,
[00:33:57.520 --> 00:34:00.160]   ideally there would be some way in which we managed
[00:34:00.160 --> 00:34:03.800]   to grow via the thing that really captures
[00:34:03.800 --> 00:34:05.800]   what do we trust in, you know, there's something,
[00:34:05.800 --> 00:34:07.880]   there's something we trust about the outside world
[00:34:07.880 --> 00:34:11.320]   about the ongoing processes of human civilization so far.
[00:34:11.320 --> 00:34:14.280]   I don't think it's the same as like raw competition
[00:34:14.280 --> 00:34:17.320]   or, you know, pure,
[00:34:17.320 --> 00:34:18.880]   I think there's like some rich structure
[00:34:18.880 --> 00:34:21.640]   to how we understand like moral progress.
[00:34:21.640 --> 00:34:22.480]   Do you have been made
[00:34:22.480 --> 00:34:26.040]   and what it would be to kind of carry that thread forward.
[00:34:26.040 --> 00:34:28.440]   And I don't have a formula.
[00:34:28.440 --> 00:34:30.600]   You know, I think we're just gonna have to bring to bear
[00:34:30.600 --> 00:34:33.760]   the full force of everything that we know
[00:34:33.760 --> 00:34:36.720]   about goodness and justice and beauty.
[00:34:36.720 --> 00:34:39.440]   And we just have to, you know, bring ourselves fully
[00:34:39.440 --> 00:34:41.040]   to the project of like making things good
[00:34:41.040 --> 00:34:43.480]   and doing that collectively.
[00:34:43.480 --> 00:34:45.840]   And I think that is, it is a really important part,
[00:34:45.840 --> 00:34:46.680]   I think of our vision of like
[00:34:46.680 --> 00:34:49.280]   what was an appropriate process of like deciding
[00:34:49.280 --> 00:34:50.640]   of like growing as a civilization
[00:34:50.640 --> 00:34:53.160]   is that there was this very inclusive
[00:34:53.160 --> 00:34:55.520]   kind of decentralized element
[00:34:55.520 --> 00:34:57.440]   of like people getting to think and talk
[00:34:57.440 --> 00:35:00.880]   and grow and change things and react
[00:35:00.880 --> 00:35:02.640]   rather than some more like,
[00:35:02.640 --> 00:35:05.720]   and now the future shall be like blah.
[00:35:05.720 --> 00:35:06.560]   Yeah.
[00:35:06.560 --> 00:35:08.840]   Like that's, I think we don't want that.
[00:35:08.840 --> 00:35:11.280]   I think a big crux maybe is like, okay,
[00:35:11.280 --> 00:35:13.440]   to the extent that like the reason we're worried
[00:35:13.440 --> 00:35:14.680]   about motivations in the first place
[00:35:14.680 --> 00:35:19.600]   is because we think a balance of power,
[00:35:19.600 --> 00:35:23.200]   which includes at least one thing with human motivations
[00:35:23.200 --> 00:35:24.360]   or not human motivations,
[00:35:24.360 --> 00:35:29.040]   human descended motivations is difficult
[00:35:29.040 --> 00:35:30.800]   to the extent that we think that's the case.
[00:35:30.800 --> 00:35:31.760]   It seems like a big crux
[00:35:31.760 --> 00:35:34.320]   that I often don't hear people talk about is like,
[00:35:34.320 --> 00:35:35.920]   I don't know how you get the balance of power
[00:35:35.920 --> 00:35:37.440]   and maybe just like reconciling yourself
[00:35:37.440 --> 00:35:40.760]   with the models of the intelligence explosion,
[00:35:40.760 --> 00:35:42.640]   which say that such a thing is not possible
[00:35:42.640 --> 00:35:44.960]   and therefore you just got to like figure out
[00:35:44.960 --> 00:35:46.440]   how you get the right God.
[00:35:46.440 --> 00:35:48.760]   But I don't know.
[00:35:48.760 --> 00:35:51.000]   I'm like, I don't really have a framework
[00:35:51.000 --> 00:35:53.760]   to think about how to the balance of power thing.
[00:35:53.760 --> 00:35:56.760]   I'd be very curious of like,
[00:35:56.760 --> 00:35:58.840]   there is a more concrete way to think about like,
[00:35:58.840 --> 00:36:03.240]   what are the, what is a structure of competition
[00:36:04.480 --> 00:36:06.680]   or a lack thereof between the labs now
[00:36:06.680 --> 00:36:09.920]   or between countries such that the balance of power
[00:36:09.920 --> 00:36:12.800]   is most likely to be preserved?
[00:36:12.800 --> 00:36:15.920]   A big part of this discourse,
[00:36:15.920 --> 00:36:18.000]   at least among safety concerned people is like,
[00:36:18.000 --> 00:36:21.800]   there's a clear trade-off between competition dynamics
[00:36:21.800 --> 00:36:25.440]   and race dynamics and the value of the future
[00:36:25.440 --> 00:36:28.560]   or how good the future ends up being.
[00:36:28.560 --> 00:36:31.680]   And in fact, if you buy this balance of power story,
[00:36:31.680 --> 00:36:32.720]   it might be the opposite.
[00:36:32.720 --> 00:36:34.280]   Like maybe competitive pressures
[00:36:34.280 --> 00:36:36.320]   naturally favor balance of power.
[00:36:36.320 --> 00:36:37.840]   And I wonder if this is one of the strong arguments
[00:36:37.840 --> 00:36:39.240]   against nationalizing the AIs.
[00:36:39.240 --> 00:36:42.000]   And like, you can imagine a more sort of
[00:36:42.000 --> 00:36:44.560]   many different companies developing AI,
[00:36:44.560 --> 00:36:45.960]   some of which are somewhat misaligned
[00:36:45.960 --> 00:36:48.120]   and some of which are aligned.
[00:36:48.120 --> 00:36:49.960]   You can imagine that being more conducive
[00:36:49.960 --> 00:36:54.280]   to both the balance of power and to a defensive,
[00:36:54.280 --> 00:36:56.600]   how all the AIs go through each website
[00:36:56.600 --> 00:36:58.000]   and see how easy it is to hack
[00:36:58.000 --> 00:37:01.480]   and basically just getting society up to snuff.
[00:37:01.480 --> 00:37:04.920]   If you're not just like deploying the technology widely,
[00:37:04.920 --> 00:37:09.360]   then the first group who can get their hands on it
[00:37:09.360 --> 00:37:14.880]   will be able to instigate a sort of revolution
[00:37:14.880 --> 00:37:17.960]   that you're just like standing against the equilibrium
[00:37:17.960 --> 00:37:19.080]   in a very strong way.
[00:37:19.080 --> 00:37:22.520]   So I definitely share some intuition there
[00:37:22.520 --> 00:37:25.760]   that there's, you know, at a high level,
[00:37:25.760 --> 00:37:28.760]   a lot of what's scary about the situation with AI
[00:37:28.760 --> 00:37:31.360]   has to do with concentrations of power.
[00:37:31.360 --> 00:37:35.200]   And whether that power is kind of concentrated
[00:37:35.200 --> 00:37:37.280]   in the hands of misaligned AI
[00:37:37.280 --> 00:37:39.560]   or in the hands of some human.
[00:37:39.560 --> 00:37:44.240]   And I do think it's very natural to think,
[00:37:44.240 --> 00:37:46.920]   okay, let's try to distribute the power more.
[00:37:46.920 --> 00:37:49.280]   And one way to try to do that
[00:37:49.280 --> 00:37:51.920]   is to kind of have a much more multipolar scenario
[00:37:51.920 --> 00:37:54.760]   where like lots and lots of actors are developing AI.
[00:37:54.760 --> 00:37:57.560]   And this is something that people have talked about.
[00:37:57.560 --> 00:37:58.680]   When you describe that scenario,
[00:37:58.680 --> 00:38:00.360]   you were like, some of which are aligned,
[00:38:00.360 --> 00:38:01.760]   some of which are misaligned.
[00:38:01.760 --> 00:38:04.040]   That's key.
[00:38:04.040 --> 00:38:06.400]   That's a key aspect of the scenario, right?
[00:38:06.400 --> 00:38:07.880]   And this is sometimes people will say this stuff.
[00:38:07.880 --> 00:38:09.520]   They'll be like, well, the good AIs,
[00:38:09.520 --> 00:38:10.480]   there will be the good AIs
[00:38:10.480 --> 00:38:12.040]   and they'll defeat the bad AIs.
[00:38:12.040 --> 00:38:15.720]   But, you know, notice the assumption in there,
[00:38:15.720 --> 00:38:18.760]   which is that you sort of made it the case
[00:38:18.760 --> 00:38:21.880]   that you can control some of the AIs, right?
[00:38:21.880 --> 00:38:23.640]   And you've got some good AIs.
[00:38:23.640 --> 00:38:25.960]   And now it's a question of like, are there enough of them?
[00:38:25.960 --> 00:38:28.360]   And how are they working relative to the others?
[00:38:29.360 --> 00:38:30.640]   And maybe, you know,
[00:38:30.640 --> 00:38:32.560]   I think it's possible that that is what happens.
[00:38:32.560 --> 00:38:34.240]   There's, you know, we know enough about alignment
[00:38:34.240 --> 00:38:35.680]   that some actors are able to do that.
[00:38:35.680 --> 00:38:38.280]   And maybe some actors are less cautious
[00:38:38.280 --> 00:38:39.920]   or they are intentionally creating misaligned AIs
[00:38:39.920 --> 00:38:41.320]   or God knows what.
[00:38:41.320 --> 00:38:45.760]   But if you don't have that, right,
[00:38:45.760 --> 00:38:50.760]   if everyone is in some sense unable to control their AIs,
[00:38:50.760 --> 00:38:55.800]   then there's,
[00:38:55.800 --> 00:38:59.240]   then the sort of the good AIs help with the bad AIs thing,
[00:38:59.240 --> 00:39:00.320]   becomes like more complicated
[00:39:00.320 --> 00:39:02.520]   or maybe it just doesn't work
[00:39:02.520 --> 00:39:04.560]   because there's sort of no good AIs in this scenario.
[00:39:04.560 --> 00:39:06.520]   There's a lot of sort of,
[00:39:06.520 --> 00:39:07.800]   if you say like everyone is building
[00:39:07.800 --> 00:39:10.360]   their own super intelligence that they can't control.
[00:39:10.360 --> 00:39:11.640]   It's true that that is now a check
[00:39:11.640 --> 00:39:12.880]   on the power of the other super,
[00:39:12.880 --> 00:39:15.960]   now the other super intelligences need to like deal
[00:39:15.960 --> 00:39:17.200]   with other actors,
[00:39:17.200 --> 00:39:20.000]   but none of them are necessarily kind of working
[00:39:20.000 --> 00:39:22.600]   on behalf of a given set of human interests
[00:39:22.600 --> 00:39:24.920]   or anything like that.
[00:39:24.920 --> 00:39:27.400]   So I do think that's like a very important
[00:39:29.120 --> 00:39:32.440]   difficulty in thinking about sort of the very simple thought
[00:39:32.440 --> 00:39:34.120]   of like, ah, I know what we can do.
[00:39:34.120 --> 00:39:37.320]   Let's just have lots and lots of AIs
[00:39:37.320 --> 00:39:39.280]   so that no single AI has a ton of power.
[00:39:39.280 --> 00:39:44.280]   And I think that on its own is not enough.
[00:39:44.280 --> 00:39:46.720]   But in this story, it's like,
[00:39:46.720 --> 00:39:49.000]   I'm just like very skeptical we end up with,
[00:39:49.000 --> 00:39:51.840]   I think on default we have this training regime,
[00:39:51.840 --> 00:39:55.240]   at least initially that favors a sort of like
[00:39:55.240 --> 00:39:59.200]   latent representation of the inhibitions that humans have
[00:39:59.200 --> 00:40:00.600]   and the values humans have.
[00:40:00.600 --> 00:40:04.640]   And I get that like, if you mess it up, it can go rogue.
[00:40:04.640 --> 00:40:07.200]   But like, if like multiple people are training AIs,
[00:40:07.200 --> 00:40:09.440]   they all end up rogue such that like the compromises
[00:40:09.440 --> 00:40:13.360]   between them don't end up with humans not violently killed.
[00:40:13.360 --> 00:40:16.720]   Like none of them have like,
[00:40:16.720 --> 00:40:19.840]   it fails on like Google's run and Microsoft's run
[00:40:19.840 --> 00:40:21.360]   and OpenAI's run and.
[00:40:21.360 --> 00:40:24.160]   Yeah, I mean, I think there's very notable
[00:40:24.160 --> 00:40:26.840]   and salient sources of correlation between failures
[00:40:26.840 --> 00:40:28.160]   across the different runs, right?
[00:40:28.160 --> 00:40:31.160]   Which is people didn't have a developed science
[00:40:31.160 --> 00:40:32.480]   of AI motivations.
[00:40:32.480 --> 00:40:34.120]   The runs were structurally quite similar.
[00:40:34.120 --> 00:40:36.000]   Everyone is using the same techniques.
[00:40:36.000 --> 00:40:38.760]   Maybe someone just stole the weights or, you know.
[00:40:38.760 --> 00:40:43.840]   So yeah, I guess I think it's really important
[00:40:43.840 --> 00:40:45.800]   this idea that like, to the extent
[00:40:45.800 --> 00:40:47.240]   you haven't solved alignment,
[00:40:47.240 --> 00:40:50.280]   you likely haven't solved it anywhere.
[00:40:50.280 --> 00:40:52.880]   And if someone has solved it and someone hasn't,
[00:40:52.880 --> 00:40:55.640]   then I think it's a better question.
[00:40:55.640 --> 00:40:58.400]   But if everyone's building systems that are,
[00:40:58.400 --> 00:41:01.680]   you know, that are kind of going to go rogue,
[00:41:01.680 --> 00:41:05.800]   then I don't think that's much comfort as we talked about.
[00:41:05.800 --> 00:41:06.680]   Yep, yep.
[00:41:06.680 --> 00:41:07.520]   Okay, all right.
[00:41:07.520 --> 00:41:10.640]   So then let's wrap up this part here.
[00:41:10.640 --> 00:41:12.360]   I didn't mention this explicitly in the introduction.
[00:41:12.360 --> 00:41:15.720]   So to the extent that this ends up being the transition
[00:41:15.720 --> 00:41:17.120]   to the next part,
[00:41:17.120 --> 00:41:19.520]   the broader discussion we were having in part two
[00:41:19.520 --> 00:41:22.240]   is about Joe Siri's otherness and control
[00:41:22.240 --> 00:41:23.840]   in the age of AGI.
[00:41:23.840 --> 00:41:26.080]   And the first part is where I was hoping
[00:41:26.080 --> 00:41:27.920]   we could just come back and just treat the main,
[00:41:27.920 --> 00:41:30.400]   the main crux people will come in wondering about
[00:41:30.400 --> 00:41:32.040]   and which I myself feel unsure about.
[00:41:32.040 --> 00:41:33.000]   Yeah, I mean, I'll just say on that front,
[00:41:33.000 --> 00:41:36.720]   I mean, I do think the otherness and control series is,
[00:41:36.720 --> 00:41:41.400]   you know, I think kind of in some sense separable.
[00:41:41.400 --> 00:41:44.080]   I mean, it has a lot to do with like misalignment stuff,
[00:41:44.080 --> 00:41:45.080]   but I think it's not,
[00:41:45.080 --> 00:41:48.480]   I think a lot of those issues are relevant,
[00:41:48.480 --> 00:41:51.040]   even if, even given various degrees of skepticism
[00:41:51.040 --> 00:41:53.360]   about some of the stuff I've been saying here.
[00:41:53.360 --> 00:41:55.720]   And by the way, so the actual mechanisms
[00:41:55.720 --> 00:41:58.560]   of how a takeover would happen will,
[00:41:58.560 --> 00:42:00.400]   there's an episode with Karl Shulman,
[00:42:00.400 --> 00:42:02.480]   which discusses this in detail.
[00:42:02.480 --> 00:42:03.800]   So people can go check that out.
[00:42:03.800 --> 00:42:07.320]   Yeah, I think like, yeah, in terms of why is it plausible
[00:42:07.320 --> 00:42:10.200]   that I guys could take over from a given a position,
[00:42:10.200 --> 00:42:11.840]   you know, in one of these projects I've been describing
[00:42:11.840 --> 00:42:12.720]   or something, I think,
[00:42:12.720 --> 00:42:15.080]   I think Karl's discussion is pretty good
[00:42:15.080 --> 00:42:18.640]   and gets into a bunch of kind of the weeds
[00:42:18.640 --> 00:42:20.840]   that I think might give a more concrete sense.
[00:42:20.840 --> 00:42:21.680]   Yep.
[00:42:21.680 --> 00:42:22.960]   All right, so now on to part two,
[00:42:22.960 --> 00:42:26.040]   where we discuss the otherness and control
[00:42:26.040 --> 00:42:27.960]   in the Age of AGI series.
[00:42:27.960 --> 00:42:30.280]   First question, if in a hundred years time,
[00:42:30.280 --> 00:42:32.400]   we look back on alignment
[00:42:32.400 --> 00:42:34.360]   and consider it was a huge mistake
[00:42:34.360 --> 00:42:36.000]   that we should have just tried to build
[00:42:36.000 --> 00:42:38.880]   the most raw, powerful AI systems we could have,
[00:42:38.880 --> 00:42:40.640]   what would bring about such a judgment?
[00:42:40.640 --> 00:42:43.800]   One scenario I think about a lot is one in which
[00:42:43.800 --> 00:42:47.880]   it just turns out that maybe kind of fairly basic measures
[00:42:47.880 --> 00:42:49.240]   are enough to ensure, for example,
[00:42:49.240 --> 00:42:52.000]   that AIs don't cause catastrophic harm,
[00:42:52.000 --> 00:42:55.440]   don't kind of seek power in problematic ways, et cetera.
[00:42:55.440 --> 00:42:58.600]   And it could turn out that we learned that it was easy
[00:42:58.600 --> 00:43:01.800]   in a way that, such that we regret, you know,
[00:43:01.800 --> 00:43:03.480]   we wish we had prioritized differently.
[00:43:03.480 --> 00:43:05.240]   We end up thinking, oh, you know,
[00:43:05.240 --> 00:43:07.680]   I wish we could have cured cancer sooner.
[00:43:07.680 --> 00:43:10.520]   We could have handled some geopolitical dynamic differently.
[00:43:10.520 --> 00:43:14.760]   There's another scenario where we end up looking back
[00:43:14.760 --> 00:43:16.960]   at some period of our history
[00:43:16.960 --> 00:43:21.640]   and how we thought about AIs, how we treated our AIs,
[00:43:21.640 --> 00:43:25.720]   and we end up looking back with a kind of moral horror
[00:43:25.720 --> 00:43:27.080]   at what we were doing.
[00:43:27.080 --> 00:43:28.840]   So, you know, we end up thinking, you know,
[00:43:28.840 --> 00:43:30.160]   we were thinking about these things centrally
[00:43:30.160 --> 00:43:31.760]   as like products, as tools,
[00:43:31.760 --> 00:43:35.680]   but in fact, we should have been foregrounding much more
[00:43:35.680 --> 00:43:37.320]   the sense in which they might be moral patients
[00:43:37.320 --> 00:43:40.720]   or were moral patients at some level of sophistication
[00:43:40.720 --> 00:43:42.040]   that we were kind of treating them in the wrong way.
[00:43:42.040 --> 00:43:43.720]   We were just acting like we could do whatever we want.
[00:43:43.720 --> 00:43:45.120]   We could, you know, delete them,
[00:43:45.120 --> 00:43:46.720]   subject them to arbitrary experiments,
[00:43:46.720 --> 00:43:50.400]   kind of alter their minds in arbitrary ways.
[00:43:50.400 --> 00:43:52.240]   And then we end up looking back in the light of history
[00:43:52.240 --> 00:43:57.240]   at that as a kind of serious and kind of grave moral error.
[00:43:57.240 --> 00:43:59.000]   Those are scenarios I think about a lot
[00:43:59.000 --> 00:44:00.880]   in which we have regrets.
[00:44:00.880 --> 00:44:04.640]   I don't think they quite fit the bill of what you just said.
[00:44:04.640 --> 00:44:06.520]   I think it sounds to me like the thing you're thinking
[00:44:06.520 --> 00:44:09.960]   is something more like we end up feeling like, gosh,
[00:44:09.960 --> 00:44:15.000]   we wish we had paid no attention to the motives of our AIs.
[00:44:15.000 --> 00:44:17.120]   That we'd thought not at all about their impact
[00:44:17.120 --> 00:44:18.680]   on our society as we incorporated them.
[00:44:18.680 --> 00:44:21.600]   And instead we had pursued a,
[00:44:21.600 --> 00:44:27.080]   let's call it a kind of maximize for brute power option,
[00:44:27.080 --> 00:44:28.680]   which is just kind of make a beeline
[00:44:28.680 --> 00:44:32.080]   for whatever is just the most powerful AI you can
[00:44:32.080 --> 00:44:35.160]   and don't think about anything else.
[00:44:35.160 --> 00:44:37.840]   Okay, so I'm very skeptical
[00:44:37.840 --> 00:44:39.920]   that that's what we're gonna wish.
[00:44:39.920 --> 00:44:43.840]   One common example that's given a misalignment
[00:44:43.840 --> 00:44:47.000]   is humans from evolution.
[00:44:47.000 --> 00:44:49.280]   And you have one line in your series
[00:44:49.280 --> 00:44:52.160]   that here's a simple argument for AI risk.
[00:44:52.160 --> 00:44:54.880]   A monkey should be careful before inventing humans.
[00:44:54.880 --> 00:44:58.120]   The sort of paper clipper metaphor
[00:44:58.120 --> 00:45:02.160]   implies something really banal and boring
[00:45:02.160 --> 00:45:04.640]   with regards to misalignment.
[00:45:04.640 --> 00:45:06.520]   And I think if I'm steel manning the people
[00:45:06.520 --> 00:45:07.960]   who worship power,
[00:45:07.960 --> 00:45:10.520]   they have the sense of humans got misaligned
[00:45:10.520 --> 00:45:13.440]   and they started pursuing things
[00:45:13.440 --> 00:45:14.520]   if a monkey was creating them.
[00:45:14.520 --> 00:45:15.360]   This is a weird analogy
[00:45:15.360 --> 00:45:16.600]   because obviously monkeys didn't create humans.
[00:45:16.600 --> 00:45:18.480]   But if the monkey was creating them,
[00:45:18.480 --> 00:45:21.000]   they're not thinking about bananas all day.
[00:45:21.000 --> 00:45:22.080]   They're thinking about other things.
[00:45:22.080 --> 00:45:23.240]   On the other hand,
[00:45:23.240 --> 00:45:25.640]   they didn't just make useless stone tools
[00:45:25.640 --> 00:45:26.880]   and pile them up in caves
[00:45:26.880 --> 00:45:28.680]   in a sort of paper clipper fashion.
[00:45:28.680 --> 00:45:31.760]   There were all these things that emerged
[00:45:31.760 --> 00:45:33.600]   because of their greater intelligence,
[00:45:33.600 --> 00:45:37.520]   which were misaligned with evolution of creativity
[00:45:37.520 --> 00:45:39.560]   and love and music and beauty
[00:45:39.560 --> 00:45:42.480]   and all the other things we value about human culture.
[00:45:42.480 --> 00:45:44.520]   And the prediction maybe they have,
[00:45:44.520 --> 00:45:46.120]   which is more of an empirical statement
[00:45:46.120 --> 00:45:48.480]   than a philosophical statement is,
[00:45:48.480 --> 00:45:50.520]   listen, with greater intelligence,
[00:45:50.520 --> 00:45:51.760]   if you're thinking about the paper clipper,
[00:45:51.760 --> 00:45:52.720]   even if it's misaligned,
[00:45:52.720 --> 00:45:55.440]   it will be in this kind of way.
[00:45:55.440 --> 00:45:57.120]   It'll be things that are alien to humans,
[00:45:57.120 --> 00:46:00.280]   but also alien in the way humans are aliens to monkeys,
[00:46:00.280 --> 00:46:03.360]   not in the way that paper clipper is alien to a human.
[00:46:03.360 --> 00:46:04.200]   Cool.
[00:46:04.200 --> 00:46:06.080]   So I think there's a bunch of different things
[00:46:06.080 --> 00:46:07.480]   to potentially unpack there.
[00:46:09.440 --> 00:46:13.120]   One kind of conceptual point
[00:46:13.120 --> 00:46:14.240]   that I want to name off the bat,
[00:46:14.240 --> 00:46:15.920]   I don't think you're necessarily
[00:46:15.920 --> 00:46:18.440]   kind of making a mistake in this vein,
[00:46:18.440 --> 00:46:19.280]   but I just want to name it
[00:46:19.280 --> 00:46:21.480]   as like a possible mistake in this vicinity is,
[00:46:21.480 --> 00:46:24.840]   I think we don't want to engage
[00:46:24.840 --> 00:46:26.080]   in the following form of reasoning.
[00:46:26.080 --> 00:46:27.440]   Let's say you have two entities.
[00:46:27.440 --> 00:46:30.160]   One is in the role of creator
[00:46:30.160 --> 00:46:32.480]   and one is in the role of creation.
[00:46:32.480 --> 00:46:33.320]   And then we're positing
[00:46:33.320 --> 00:46:35.240]   that there's this kind of misalignment relation
[00:46:35.240 --> 00:46:37.800]   between them, whatever that means, right?
[00:46:37.800 --> 00:46:40.720]   And here's a pattern of reasoning
[00:46:40.720 --> 00:46:42.720]   that I think you want to watch out for,
[00:46:42.720 --> 00:46:46.720]   is to say in my role as creator,
[00:46:46.720 --> 00:46:48.160]   or sorry, in my role as creation,
[00:46:48.160 --> 00:46:50.080]   say you're thinking of humans in the role of creation
[00:46:50.080 --> 00:46:54.720]   relative to an entity like evolution or monkeys or mice
[00:46:54.720 --> 00:46:56.360]   or whoever you could imagine inventing humans
[00:46:56.360 --> 00:46:58.600]   or something like that, right?
[00:46:58.600 --> 00:47:02.600]   You say, I'm qua creation,
[00:47:02.600 --> 00:47:04.960]   I'm happy that I was created
[00:47:04.960 --> 00:47:07.000]   and happy with the misalignment.
[00:47:07.000 --> 00:47:11.160]   Therefore, if I end up in the role of creator
[00:47:11.160 --> 00:47:16.320]   and we have a structurally analogous relation
[00:47:16.320 --> 00:47:18.560]   in which there's misalignment with some creation,
[00:47:18.560 --> 00:47:21.120]   I should expect to be happy with that as well.
[00:47:21.120 --> 00:47:21.960]   - Yeah.
[00:47:21.960 --> 00:47:23.240]   There's a couple of philosophers
[00:47:23.240 --> 00:47:25.000]   that you brought up in the series,
[00:47:25.000 --> 00:47:28.680]   which if you read the works that you talk about,
[00:47:28.680 --> 00:47:32.320]   actually seem incredibly foresighted
[00:47:32.320 --> 00:47:36.160]   in anticipating something like a singularity,
[00:47:36.160 --> 00:47:39.000]   our ability to shape a future thing
[00:47:39.000 --> 00:47:42.080]   that's different, smarter, maybe better than us.
[00:47:42.080 --> 00:47:44.720]   Obviously, C.S. Lewis, "Abolition of Man,"
[00:47:44.720 --> 00:47:46.840]   we'll talk about it in a second as one example.
[00:47:46.840 --> 00:47:50.040]   But even, here's one passage from Nisha,
[00:47:50.040 --> 00:47:52.040]   which I felt really highlighted this.
[00:47:52.040 --> 00:47:55.560]   "Man is a rope stretched between the animal and the Superman,
[00:47:55.560 --> 00:47:57.200]   "a rope over an abyss,
[00:47:57.200 --> 00:47:59.880]   "a dangerous crossing, a dangerous wayfaring,
[00:47:59.880 --> 00:48:01.320]   "a dangerous looking back,
[00:48:01.320 --> 00:48:03.160]   "a dangerous trembling and halting."
[00:48:03.160 --> 00:48:05.000]   Is there some explanation for why?
[00:48:05.000 --> 00:48:06.360]   Is it just somehow obvious
[00:48:06.360 --> 00:48:07.720]   that something like this is coming,
[00:48:07.720 --> 00:48:09.520]   even if you're thinking 200 years ago?
[00:48:09.520 --> 00:48:10.400]   - I think I have a much better grip
[00:48:10.400 --> 00:48:12.840]   on what's going on with Lewis than with Nisha there,
[00:48:12.840 --> 00:48:15.000]   so maybe let's just talk about Lewis for a second.
[00:48:15.000 --> 00:48:17.800]   So, and we should distinguish two,
[00:48:17.800 --> 00:48:20.240]   there's a kind of version of the singularity
[00:48:20.240 --> 00:48:21.960]   that's specifically a hypothesis
[00:48:21.960 --> 00:48:24.280]   about feedback loops with AI capabilities.
[00:48:24.280 --> 00:48:26.960]   I don't think that's present in Lewis.
[00:48:26.960 --> 00:48:30.320]   I think what Lewis is anticipating,
[00:48:30.320 --> 00:48:34.600]   and I do think this is a relatively simple forecast,
[00:48:34.600 --> 00:48:37.680]   is something like the culmination
[00:48:37.680 --> 00:48:40.160]   of the project of scientific modernity.
[00:48:40.160 --> 00:48:43.880]   So Lewis is kind of looking out at the world
[00:48:43.880 --> 00:48:45.840]   and he's seeing this process
[00:48:45.840 --> 00:48:49.040]   of kind of increased understanding
[00:48:49.040 --> 00:48:50.960]   of a kind of the natural environment
[00:48:50.960 --> 00:48:52.680]   and a kind of corresponding increase
[00:48:52.680 --> 00:48:54.480]   in our ability to kind of control
[00:48:54.480 --> 00:48:56.000]   and direct that environment.
[00:48:57.200 --> 00:48:58.640]   And then he's also pairing that
[00:48:58.640 --> 00:49:03.000]   with a kind of metaphysical hypothesis,
[00:49:03.000 --> 00:49:05.640]   or well, his stance on this metaphysical hypothesis,
[00:49:05.640 --> 00:49:09.000]   I think is like kind of problematically unclear in the book,
[00:49:09.000 --> 00:49:12.480]   but there is this metaphysical hypothesis, naturalism,
[00:49:12.480 --> 00:49:15.480]   which says that humans too,
[00:49:15.480 --> 00:49:20.120]   and kind of minds, beings, agents are a part of nature.
[00:49:20.120 --> 00:49:24.240]   And so, insofar as this process of scientific modernity
[00:49:24.240 --> 00:49:28.520]   involves a kind of progressively greater understanding
[00:49:28.520 --> 00:49:31.040]   of an ability to control nature,
[00:49:31.040 --> 00:49:32.680]   that will presumably at some point
[00:49:32.680 --> 00:49:36.160]   grow to encompass our own natures
[00:49:36.160 --> 00:49:39.440]   and kind of the natures of other beings
[00:49:39.440 --> 00:49:42.840]   that in principle we could create.
[00:49:42.840 --> 00:49:46.680]   And Lewis views this as a kind of cataclysmic event
[00:49:46.680 --> 00:49:49.320]   and crisis, part of what I'm trying to say,
[00:49:49.320 --> 00:49:50.920]   and that in particular that it will lead
[00:49:50.920 --> 00:49:54.760]   to all these kind of tyrannical kind of behaviors
[00:49:54.760 --> 00:49:58.520]   and kind of tyrannical attitudes towards morality
[00:49:58.520 --> 00:49:59.360]   and stuff like that.
[00:49:59.360 --> 00:50:00.200]   And part of what I'm trying to,
[00:50:00.200 --> 00:50:03.240]   unless you believe in non-naturalism
[00:50:03.240 --> 00:50:06.840]   or in some form of kind of Dao,
[00:50:06.840 --> 00:50:08.120]   which is this kind of objective morality.
[00:50:08.120 --> 00:50:09.000]   So we can talk about that,
[00:50:09.000 --> 00:50:11.320]   but part of what I'm trying to do in that essay is to say,
[00:50:11.320 --> 00:50:13.880]   no, I think we can be naturalists
[00:50:13.880 --> 00:50:16.320]   and also be kind of decent humans
[00:50:16.320 --> 00:50:19.800]   that remain in touch with a kind of a rich set of norms
[00:50:19.800 --> 00:50:21.520]   that have to do with like, how do we relate
[00:50:21.520 --> 00:50:24.160]   to the possibility of kind of creating creatures,
[00:50:24.160 --> 00:50:25.920]   altering ourselves, et cetera.
[00:50:25.920 --> 00:50:27.720]   But I do think his, yeah,
[00:50:27.720 --> 00:50:28.960]   it's like a relatively simple prediction.
[00:50:28.960 --> 00:50:31.280]   It's kind of science masters nature,
[00:50:31.280 --> 00:50:34.280]   humans part of nature, science masters humans.
[00:50:34.280 --> 00:50:36.200]   And then you also have a very interesting other essay
[00:50:36.200 --> 00:50:38.440]   about suppose humans,
[00:50:38.440 --> 00:50:40.600]   like what should we expect of other humans
[00:50:40.600 --> 00:50:43.280]   did this sort of extrapolation
[00:50:43.280 --> 00:50:45.160]   if they had greater capabilities and so on?
[00:50:45.160 --> 00:50:48.440]   Yeah, I mean, I think an uncomfortable thing
[00:50:48.440 --> 00:50:53.440]   about the kind of conceptual setup at stake
[00:50:53.440 --> 00:50:55.480]   in these sort of like abstract discussions of like,
[00:50:55.480 --> 00:50:58.280]   okay, you have this agent, it fooms,
[00:50:58.280 --> 00:51:01.720]   which is this sort of amorphous process
[00:51:01.720 --> 00:51:05.040]   of kind of going from a sort of seed agent
[00:51:05.040 --> 00:51:07.160]   to a like super intelligent version of itself,
[00:51:07.160 --> 00:51:09.960]   often imagined to kind of preserve its values along the way.
[00:51:09.960 --> 00:51:13.240]   Bunch of questions we can raise about that.
[00:51:13.240 --> 00:51:17.800]   But I think a kind of many of the arguments
[00:51:17.800 --> 00:51:19.080]   that people will often talk about
[00:51:19.080 --> 00:51:21.560]   in the context of reasons to be scared of AI is like,
[00:51:21.560 --> 00:51:24.160]   oh, like value is very fragile as you like foom,
[00:51:24.160 --> 00:51:29.720]   kind of small differences in utility functions
[00:51:29.720 --> 00:51:31.280]   can kind of de-correlate very hard
[00:51:31.280 --> 00:51:33.480]   and kind of drive in quite different directions.
[00:51:33.480 --> 00:51:36.680]   And like, oh, like agents have instrumental incentives
[00:51:36.680 --> 00:51:37.520]   to seek power.
[00:51:37.520 --> 00:51:39.480]   And if it was arbitrarily easy to get power,
[00:51:39.480 --> 00:51:40.440]   then they would do it and stuff like that.
[00:51:40.440 --> 00:51:43.120]   Like, these are very general arguments
[00:51:43.120 --> 00:51:45.040]   that seem to suggest that the kind of,
[00:51:46.240 --> 00:51:48.760]   it's not just an AI thing, right?
[00:51:48.760 --> 00:51:50.600]   There's, I mean, it's like no surprise, right?
[00:51:50.600 --> 00:51:53.200]   It's talking about like, take a thing,
[00:51:53.200 --> 00:51:56.120]   make it arbitrarily powerful such that it's like,
[00:51:56.120 --> 00:51:59.720]   you know, God, emperor of the universe or something.
[00:51:59.720 --> 00:52:01.720]   How scared are you of that?
[00:52:01.720 --> 00:52:03.960]   Like, clearly we should be equally scared of that,
[00:52:03.960 --> 00:52:04.800]   or I don't know,
[00:52:04.800 --> 00:52:06.920]   we should be really scared of that with humans too, right?
[00:52:06.920 --> 00:52:08.840]   So, I mean, part of what I'm saying in that essay
[00:52:08.840 --> 00:52:10.360]   is that I think this is, in some sense,
[00:52:10.360 --> 00:52:13.600]   this is much more a story about balance of power
[00:52:13.600 --> 00:52:15.680]   and about like maintaining a kind of,
[00:52:15.680 --> 00:52:19.680]   a kind of checks and balances
[00:52:19.680 --> 00:52:23.240]   and kind of distribution of power, period.
[00:52:23.240 --> 00:52:25.920]   Not just about like kind of humans versus AIs
[00:52:25.920 --> 00:52:27.720]   and kind of the differences between human values
[00:52:27.720 --> 00:52:28.560]   and AI values.
[00:52:28.560 --> 00:52:29.560]   Now, that said, I mean, I do think humans,
[00:52:29.560 --> 00:52:32.440]   many humans would likely be nicer if they foomed
[00:52:32.440 --> 00:52:33.880]   than like certain types of AIs.
[00:52:33.880 --> 00:52:35.040]   So, I mean, it's not,
[00:52:35.040 --> 00:52:37.240]   but I think the kind of conceptual structure
[00:52:37.240 --> 00:52:40.480]   of the argument is not,
[00:52:40.480 --> 00:52:43.520]   it's sort of a very open question
[00:52:43.520 --> 00:52:46.160]   how much it applies to humans as well.
[00:52:46.160 --> 00:52:50.080]   I think one sort of big question I have is,
[00:52:50.080 --> 00:52:52.240]   I don't even know how to express this,
[00:52:52.240 --> 00:52:55.760]   but how confident are we with this ontology
[00:52:55.760 --> 00:52:59.120]   of expressing like what are agents, what are capabilities?
[00:52:59.120 --> 00:53:02.360]   How do we know this is the thing that's happening?
[00:53:02.360 --> 00:53:03.440]   Or like, this is the way to think
[00:53:03.440 --> 00:53:06.520]   about what intelligences are?
[00:53:06.520 --> 00:53:11.520]   So it's clearly this kind of very janky kind of,
[00:53:11.520 --> 00:53:14.880]   I mean, well, people maybe disagree about this.
[00:53:14.880 --> 00:53:18.160]   I think it's, I mean, it's obvious to everyone
[00:53:18.160 --> 00:53:20.200]   with respect to like real world human agents
[00:53:20.200 --> 00:53:22.080]   that kind of thinking of humans
[00:53:22.080 --> 00:53:26.120]   as having utility functions is at best
[00:53:26.120 --> 00:53:30.440]   a very lossy approximation of what's going on.
[00:53:30.440 --> 00:53:32.280]   I think it's likely to mislead
[00:53:34.160 --> 00:53:37.280]   as you amp up the intelligence of various agents as well.
[00:53:37.280 --> 00:53:39.080]   Though I think Eliezer might disagree about that.
[00:53:39.080 --> 00:53:40.760]   I will say, I think there's something adjacent to that
[00:53:40.760 --> 00:53:42.240]   that I think is like more real,
[00:53:42.240 --> 00:53:43.200]   that seems more real to me,
[00:53:43.200 --> 00:53:44.760]   which is something like, I don't know.
[00:53:44.760 --> 00:53:49.080]   My mom recently bought, or a few years ago,
[00:53:49.080 --> 00:53:50.680]   she like wanted to get a house.
[00:53:50.680 --> 00:53:52.720]   She wanted to get a new dog.
[00:53:52.720 --> 00:53:54.120]   Now she has both, you know?
[00:53:54.120 --> 00:53:56.560]   How did this happen?
[00:53:56.560 --> 00:53:57.400]   What is the right action?
[00:53:57.400 --> 00:53:58.440]   It's 'cause she tried, it was hard.
[00:53:58.440 --> 00:53:59.760]   She had to like search for the houses,
[00:53:59.760 --> 00:54:00.920]   hard to find the dog, right?
[00:54:00.920 --> 00:54:02.880]   Now she has a house, now she has a dog.
[00:54:03.720 --> 00:54:07.280]   This is a very common thing that happens all the time.
[00:54:07.280 --> 00:54:09.440]   And I think, I don't think we need to be like,
[00:54:09.440 --> 00:54:11.680]   my mom has to have a utility function with the dog
[00:54:11.680 --> 00:54:13.040]   and she has to have a consistent valuation
[00:54:13.040 --> 00:54:13.880]   of all the houses or whatever.
[00:54:13.880 --> 00:54:16.000]   I mean, like, but it's still the case
[00:54:16.000 --> 00:54:18.480]   that her planning and her agency exerted in the world
[00:54:18.480 --> 00:54:21.440]   resulted in her having this house, having this dog.
[00:54:21.440 --> 00:54:25.480]   And I think it is plausible that as our kind of scientific
[00:54:25.480 --> 00:54:28.080]   and technological power advances,
[00:54:28.080 --> 00:54:31.920]   more and more stuff will be kind of explicable in that way,
[00:54:31.920 --> 00:54:33.880]   right, that, you know, if you look and you're like,
[00:54:33.880 --> 00:54:36.200]   why is this man on the moon, right?
[00:54:36.200 --> 00:54:37.360]   How did that happen?
[00:54:37.360 --> 00:54:39.480]   And it's like, well, like there was a whole cognitive
[00:54:39.480 --> 00:54:42.680]   process, there was a whole like planning apparatus.
[00:54:42.680 --> 00:54:44.120]   Now, in this case, it wasn't like localized
[00:54:44.120 --> 00:54:46.880]   in a single mind, but like there was a whole thing
[00:54:46.880 --> 00:54:49.440]   such that man on the moon, right?
[00:54:49.440 --> 00:54:51.040]   And I think like, we'll see a bunch more of that
[00:54:51.040 --> 00:54:54.880]   and the AIs will be, I think like doing a bunch of it.
[00:54:54.880 --> 00:54:57.440]   And so that's the thing that seems like more real to me
[00:54:57.440 --> 00:55:01.080]   than kind of utility functions.
[00:55:01.080 --> 00:55:03.680]   So yeah, the man on the moon example,
[00:55:03.680 --> 00:55:07.840]   there's a proximal story of how exactly NASA engineered
[00:55:07.840 --> 00:55:12.120]   the spacecraft to get to the moon.
[00:55:12.120 --> 00:55:15.120]   There's the more distal geopolitical story
[00:55:15.120 --> 00:55:17.200]   of why we send people to the moon.
[00:55:17.200 --> 00:55:18.480]   And at all those levels,
[00:55:18.480 --> 00:55:21.440]   there's different utility functions clashing.
[00:55:21.440 --> 00:55:23.680]   Maybe there's a sort of like meta societal
[00:55:23.680 --> 00:55:25.240]   world utility function.
[00:55:25.240 --> 00:55:28.640]   But the, maybe the story there is like,
[00:55:28.640 --> 00:55:30.520]   there's some sort of balance of power between these agents.
[00:55:30.520 --> 00:55:33.720]   And that's why there's the emergent thing that happens.
[00:55:33.720 --> 00:55:35.520]   Like why we send things to the moon
[00:55:35.520 --> 00:55:37.360]   is not one guy had a utility function,
[00:55:37.360 --> 00:55:40.080]   but like, I don't know, cold war, dot, dot, dot,
[00:55:40.080 --> 00:55:40.920]   things happened.
[00:55:40.920 --> 00:55:44.920]   Whereas I think like the alignment stuff
[00:55:44.920 --> 00:55:47.960]   is a lot about like assuming that one thing
[00:55:47.960 --> 00:55:49.320]   is the thing that will control everything.
[00:55:49.320 --> 00:55:52.520]   How do we control the thing that controls everything?
[00:55:52.520 --> 00:55:56.040]   Now, I guess it's not clear what you do
[00:55:56.040 --> 00:55:57.600]   to reinforce balance of power.
[00:55:57.600 --> 00:55:59.560]   Like it could just be that balance of power
[00:55:59.560 --> 00:56:01.440]   is not a thing that happens once you have things
[00:56:01.440 --> 00:56:03.080]   that can make themselves intelligent.
[00:56:03.080 --> 00:56:07.760]   But that seems interestingly different from the,
[00:56:07.760 --> 00:56:09.600]   how do we got to the moon story?
[00:56:09.600 --> 00:56:10.440]   Yeah, I agree.
[00:56:10.440 --> 00:56:11.600]   I think there's a few things going on there.
[00:56:11.600 --> 00:56:14.720]   So one is that I do think that even if you're engaged
[00:56:14.720 --> 00:56:17.440]   in this ontology of kind of carving up the world
[00:56:17.440 --> 00:56:19.320]   into different like agencies,
[00:56:19.320 --> 00:56:22.120]   at the least you don't want to kind of assume
[00:56:22.120 --> 00:56:24.440]   that they're all like unitary or like not overlapping
[00:56:24.440 --> 00:56:26.480]   or like there's a whole, it's not like,
[00:56:26.480 --> 00:56:27.480]   all right, we've got this agent,
[00:56:27.480 --> 00:56:30.240]   let's carve out one part of the world, it's one agent.
[00:56:30.240 --> 00:56:33.440]   Over here it's like, it's this whole like messy ecosystem,
[00:56:33.440 --> 00:56:36.320]   like kind of teaming niches and this whole thing, right?
[00:56:36.320 --> 00:56:40.960]   And I think in discussions of AI,
[00:56:40.960 --> 00:56:42.600]   sometimes people slip between being like,
[00:56:42.600 --> 00:56:46.480]   well, an agent is anything that gets anything done, right?
[00:56:46.480 --> 00:56:48.080]   And they'll sort of, they don't,
[00:56:48.080 --> 00:56:50.240]   it could be like this weird mooshy thing.
[00:56:50.240 --> 00:56:52.280]   And then sometimes they're like very obviously imagining
[00:56:52.280 --> 00:56:54.320]   like individual actor.
[00:56:55.520 --> 00:56:59.680]   And so that's like one difference.
[00:56:59.680 --> 00:57:01.720]   I also just think, I think we should be really going
[00:57:01.720 --> 00:57:02.600]   for the balance of power thing.
[00:57:02.600 --> 00:57:05.080]   Like, I think it is just like not good to be like,
[00:57:05.080 --> 00:57:06.520]   we're gonna have a dictator.
[00:57:06.520 --> 00:57:07.360]   Who should the dictator be?
[00:57:07.360 --> 00:57:10.360]   Like, let's make sure we like make the dictator
[00:57:10.360 --> 00:57:11.200]   the right dictator.
[00:57:11.200 --> 00:57:15.360]   I'm like, whoa, no, you know, like let's, you know,
[00:57:15.360 --> 00:57:17.840]   I think the goal should be sort of we all foom together,
[00:57:17.840 --> 00:57:20.040]   you know, it's like the whole thing
[00:57:20.040 --> 00:57:22.400]   in this like kind of inclusive and pluralistic way
[00:57:22.400 --> 00:57:25.320]   in a way that kind of satisfies the values
[00:57:25.320 --> 00:57:27.200]   of like tons of stakeholders, right?
[00:57:27.200 --> 00:57:28.880]   And is this kind of, at no point,
[00:57:28.880 --> 00:57:31.280]   is there like one kind of single point of failure
[00:57:31.280 --> 00:57:32.120]   on all these things?
[00:57:32.120 --> 00:57:34.840]   Like, I think that's what we should be striving for here.
[00:57:34.840 --> 00:57:39.520]   And I think that's true of the human power aspect of AI.
[00:57:39.520 --> 00:57:42.520]   And I think it's true of the AI part as well, yeah.
[00:57:42.520 --> 00:57:43.800]   Hey everybody, here's a quick message
[00:57:43.800 --> 00:57:45.400]   from today's sponsor, Stripe.
[00:57:45.400 --> 00:57:46.440]   When I started the podcast,
[00:57:46.440 --> 00:57:48.520]   I just wanted to get going as fast as possible.
[00:57:48.520 --> 00:57:51.000]   So use Stripe Atlas to register my LLC,
[00:57:51.000 --> 00:57:52.280]   create a bank account.
[00:57:52.280 --> 00:57:54.680]   I still use Stripe now to invoice advertisers
[00:57:54.680 --> 00:57:58.280]   and accept their payments, monetize this podcast.
[00:57:58.280 --> 00:58:00.080]   Stripe serves millions of businesses,
[00:58:00.080 --> 00:58:01.400]   small businesses like mine,
[00:58:01.400 --> 00:58:03.400]   but also the world's biggest companies,
[00:58:03.400 --> 00:58:05.360]   Amazon, Hertz, Ford.
[00:58:05.360 --> 00:58:07.040]   And all these businesses are using Stripe
[00:58:07.040 --> 00:58:08.000]   because they don't wanna deal
[00:58:08.000 --> 00:58:09.760]   with the Byzantine web of payments
[00:58:09.760 --> 00:58:12.640]   where you have different payment methods in every market
[00:58:12.640 --> 00:58:15.520]   and increasingly complex rules, regulations,
[00:58:15.520 --> 00:58:17.240]   arcane legacy systems.
[00:58:17.240 --> 00:58:19.040]   Stripe handles all of this complexity
[00:58:19.040 --> 00:58:20.280]   and abstracts it away.
[00:58:20.280 --> 00:58:22.440]   And they can test and iterate every pixel
[00:58:22.440 --> 00:58:23.280]   of the payment experience
[00:58:23.280 --> 00:58:25.920]   across billions of transactions.
[00:58:25.920 --> 00:58:27.880]   I was talking with Joe about paper clippers,
[00:58:27.880 --> 00:58:29.440]   and I feel like Stripe is the paper clipper
[00:58:29.440 --> 00:58:32.280]   of the payment industry where they're gonna optimize
[00:58:32.280 --> 00:58:35.040]   every part of the experience for your users,
[00:58:35.040 --> 00:58:37.160]   which means obviously higher conversion rates
[00:58:37.160 --> 00:58:38.720]   and ultimately as a result,
[00:58:38.720 --> 00:58:40.560]   higher revenue for your business.
[00:58:40.560 --> 00:58:42.800]   Anyways, you can go to stripe.com to learn more
[00:58:42.800 --> 00:58:44.800]   and thanks to them for sponsoring this episode.
[00:58:44.800 --> 00:58:46.200]   Back to Joe.
[00:58:46.200 --> 00:58:49.840]   So there's an interesting intellectual discourse
[00:58:49.840 --> 00:58:53.720]   on let's say right-wing side of the debate
[00:58:53.720 --> 00:58:55.000]   where they ask themselves,
[00:58:55.000 --> 00:58:56.800]   traditionally we favor markets,
[00:58:56.800 --> 00:58:59.720]   but now look where our society is headed.
[00:58:59.720 --> 00:59:02.080]   It's misaligned in the ways we care
[00:59:02.080 --> 00:59:03.240]   about society being aligned.
[00:59:03.240 --> 00:59:06.920]   Like fertility is going down, family values, religiosity,
[00:59:06.920 --> 00:59:08.880]   these things we care about.
[00:59:08.880 --> 00:59:09.840]   GDP keeps going up.
[00:59:09.840 --> 00:59:11.440]   These things don't seem correlated.
[00:59:11.440 --> 00:59:14.480]   So we're kind of grinding through the values we care about
[00:59:14.480 --> 00:59:16.240]   because of increased competition.
[00:59:16.240 --> 00:59:21.240]   And therefore we need to intervene in a major way.
[00:59:21.240 --> 00:59:24.200]   And then the pro-market libertarian fashion of the right
[00:59:24.200 --> 00:59:26.840]   will say, look, I disagree with the correlations here,
[00:59:26.840 --> 00:59:28.360]   but even at the end of the day,
[00:59:28.360 --> 00:59:30.440]   like fundamentally my point is,
[00:59:30.440 --> 00:59:33.960]   or their point is liberty is the end goal.
[00:59:33.960 --> 00:59:36.280]   It's not the, it's not like what you use
[00:59:36.280 --> 00:59:38.440]   to get to higher fertility or something.
[00:59:38.440 --> 00:59:40.280]   I think there's something interestingly analogous
[00:59:40.280 --> 00:59:44.440]   about the AI competition grinding things down.
[00:59:44.440 --> 00:59:46.040]   Like obviously you don't want the gray goo,
[00:59:46.040 --> 00:59:48.640]   but like the libertarians versus the strats,
[00:59:48.640 --> 00:59:50.320]   I think there's something analogous here.
[00:59:50.320 --> 00:59:53.320]   Yeah, so, I mean, I think one thing you could think,
[00:59:53.320 --> 00:59:55.000]   which doesn't necessarily need to be about gray goo,
[00:59:55.000 --> 00:59:56.920]   it could also just be about alignment,
[00:59:56.920 --> 01:00:00.600]   is something like, sure, it would be nice
[01:00:00.600 --> 01:00:04.440]   if the AIs didn't violently disempower humans.
[01:00:04.440 --> 01:00:06.440]   It would be nice if the AIs otherwise,
[01:00:06.440 --> 01:00:08.880]   when we created them, kind of their integration
[01:00:08.880 --> 01:00:11.360]   into our society led to good places,
[01:00:11.360 --> 01:00:14.760]   but I'm uncomfortable with like the sorts of interventions
[01:00:14.760 --> 01:00:16.400]   that people are contemplating
[01:00:16.400 --> 01:00:18.840]   in order to ensure that sort of outcome, right?
[01:00:18.840 --> 01:00:23.600]   And I think there's a bunch of things
[01:00:23.600 --> 01:00:26.000]   to be uncomfortable about that.
[01:00:26.000 --> 01:00:29.880]   Now, that said, so for something like everyone being killed
[01:00:29.880 --> 01:00:33.640]   or violently disempowered,
[01:00:33.640 --> 01:00:35.720]   that is traditionally something that we think,
[01:00:35.720 --> 01:00:37.640]   if it's real, and obviously we need to talk about
[01:00:37.640 --> 01:00:40.720]   whether it's real, but in the case where it's a real threat,
[01:00:40.720 --> 01:00:43.920]   we often think that quite intense forms of intervention
[01:00:43.920 --> 01:00:45.680]   are warranted to prevent that sort of thing
[01:00:45.680 --> 01:00:46.520]   from happening, right?
[01:00:46.520 --> 01:00:49.120]   So if there was actually a terrorist group
[01:00:49.120 --> 01:00:52.560]   that was planning to, it was like working on a bioweapon
[01:00:52.560 --> 01:00:56.000]   that was gonna kill everyone or 99.9% of people,
[01:00:56.000 --> 01:00:59.760]   we would think that warrants intervention.
[01:00:59.760 --> 01:01:00.680]   You just shut that down, right?
[01:01:00.680 --> 01:01:02.520]   And now even if you had a group
[01:01:02.520 --> 01:01:03.520]   that was doing that unintentionally,
[01:01:03.520 --> 01:01:05.160]   imposing a similar level of risk,
[01:01:05.160 --> 01:01:09.200]   that's not, I think many people,
[01:01:09.200 --> 01:01:10.640]   if that's the real scenario,
[01:01:10.720 --> 01:01:12.400]   will think that that's warrants
[01:01:12.400 --> 01:01:15.480]   kind of quite intense preventative efforts, right?
[01:01:15.480 --> 01:01:19.400]   And so obviously people,
[01:01:19.400 --> 01:01:21.240]   these sorts of risks can be used as an excuse
[01:01:21.240 --> 01:01:23.360]   to expand the state power.
[01:01:23.360 --> 01:01:25.160]   Like there's a lot of things to be worried about
[01:01:25.160 --> 01:01:28.000]   for different types of like contemplated interventions
[01:01:28.000 --> 01:01:29.700]   to address certain types of risks.
[01:01:29.700 --> 01:01:33.160]   I think we need to just,
[01:01:33.160 --> 01:01:34.920]   I think there's no like royal road there.
[01:01:34.920 --> 01:01:37.320]   You need to just like have the actual good epistemology.
[01:01:37.320 --> 01:01:39.320]   You need to actually know, is this a real risk?
[01:01:39.320 --> 01:01:41.240]   What are the actual stakes?
[01:01:41.240 --> 01:01:44.120]   And look at it case by case and be like,
[01:01:44.120 --> 01:01:45.860]   is this warranted?
[01:01:45.860 --> 01:01:51.360]   So that's like one point on the like takeover,
[01:01:51.360 --> 01:01:53.380]   literal extinction thing.
[01:01:53.380 --> 01:01:55.600]   I think the other thing I wanna say,
[01:01:55.600 --> 01:01:57.200]   so I talk in the piece about this distinction
[01:01:57.200 --> 01:01:58.040]   between the like, well,
[01:01:58.040 --> 01:01:59.120]   let's at least have the AIs
[01:01:59.120 --> 01:02:02.280]   who are kind of minimally law abiding
[01:02:02.280 --> 01:02:03.240]   or something like that, right?
[01:02:03.240 --> 01:02:04.200]   Like we don't have to talk about,
[01:02:04.200 --> 01:02:05.680]   there's this question about servitude
[01:02:05.680 --> 01:02:08.560]   and question about like other control over AI values.
[01:02:08.560 --> 01:02:10.040]   But I think we often think it's okay
[01:02:10.040 --> 01:02:12.760]   to like really want people to like obey the law,
[01:02:12.760 --> 01:02:16.120]   to uphold basic cooperative arrangements, stuff like that.
[01:02:16.120 --> 01:02:20.860]   I do though wanna emphasize,
[01:02:20.860 --> 01:02:22.020]   and I think this is true of markets
[01:02:22.020 --> 01:02:23.520]   and true of like liberalism in general,
[01:02:23.520 --> 01:02:26.560]   just how much these procedural norms
[01:02:26.560 --> 01:02:30.360]   like democracy, free speech, you know, property rights,
[01:02:30.360 --> 01:02:33.240]   things that people really hold dear, including myself,
[01:02:33.240 --> 01:02:37.440]   are in the actual lived substance
[01:02:37.440 --> 01:02:40.200]   of kind of a liberal state,
[01:02:40.200 --> 01:02:44.160]   undergirded by all sorts of kind of virtues
[01:02:44.160 --> 01:02:47.280]   and dispositions and like character traits
[01:02:47.280 --> 01:02:48.660]   in the citizenry, right?
[01:02:48.660 --> 01:02:51.940]   So like these norms are not robust
[01:02:51.940 --> 01:02:55.160]   to like arbitrarily vicious citizens.
[01:02:55.160 --> 01:02:56.880]   So, you know, like I want there to be free speech,
[01:02:56.880 --> 01:02:59.120]   but I think we also need to like raise our children
[01:02:59.120 --> 01:03:02.120]   to value truth and to know how to have real conversations.
[01:03:02.120 --> 01:03:03.740]   And, you know, I want there to be democracy,
[01:03:03.740 --> 01:03:05.120]   but I think we also need to raise our children
[01:03:05.120 --> 01:03:06.920]   to be like compassionate and decent.
[01:03:07.760 --> 01:03:12.760]   And I think it's sometimes we can lose sight of that,
[01:03:12.760 --> 01:03:14.440]   that aspect.
[01:03:14.440 --> 01:03:16.200]   And I think anyway, but I think like bringing that to mind,
[01:03:16.200 --> 01:03:17.960]   now that's not to say that should be the project
[01:03:17.960 --> 01:03:20.000]   of state power, right?
[01:03:20.000 --> 01:03:21.960]   But I think like understanding that it's,
[01:03:21.960 --> 01:03:25.160]   liberalism is not this sort of like ironclad structure
[01:03:25.160 --> 01:03:26.520]   that you can just like hit go,
[01:03:26.520 --> 01:03:29.660]   you give like any citizenry and like hit go
[01:03:29.660 --> 01:03:30.880]   and you'll get something like flourishing
[01:03:30.880 --> 01:03:32.400]   or even functional, right?
[01:03:32.400 --> 01:03:34.140]   You need, there's like a bunch of other softer stuff
[01:03:34.140 --> 01:03:37.880]   that like makes this whole project go.
[01:03:37.880 --> 01:03:41.040]   - Maybe zooming out, so one question you could ask is,
[01:03:41.040 --> 01:03:42.360]   I think the people who have,
[01:03:42.360 --> 01:03:44.840]   I don't know if Nick Land would be a good sub in here,
[01:03:44.840 --> 01:03:47.140]   but somebody, people who have a sort of fatalistic attitude
[01:03:47.140 --> 01:03:52.140]   towards alignment as a thing that can even make sense,
[01:03:52.140 --> 01:03:58.160]   they'll say things like, look, the thing,
[01:03:58.160 --> 01:04:00.440]   the kinds of things that are gonna be exploring
[01:04:00.440 --> 01:04:02.560]   the black hole, the center of the galaxy,
[01:04:02.560 --> 01:04:05.280]   the kinds of things that go visit Andromeda or something,
[01:04:05.280 --> 01:04:07.960]   did you really expect them to privilege
[01:04:07.960 --> 01:04:09.800]   whatever inclinations you have
[01:04:09.800 --> 01:04:11.880]   because you grew up in the African Savannah
[01:04:11.880 --> 01:04:13.440]   and that what the evolutionary pressures
[01:04:13.440 --> 01:04:15.740]   were a hundred thousand years ago, right?
[01:04:15.740 --> 01:04:18.200]   Like you, of course they're gonna be like weird
[01:04:18.200 --> 01:04:23.900]   and like, yeah, like what did you think was gonna happen?
[01:04:23.900 --> 01:04:28.800]   - I do think the even good futures will be weird.
[01:04:28.800 --> 01:04:31.960]   You know, I think, and I wanna be clear when I,
[01:04:31.960 --> 01:04:33.920]   you know, when I talk about kind of like finding ways
[01:04:33.920 --> 01:04:36.480]   to ensure that kind of the integration of AIs
[01:04:36.480 --> 01:04:38.480]   and to our society leads to good places,
[01:04:38.480 --> 01:04:43.040]   I'm not imagining, like, I think sometimes people think
[01:04:43.040 --> 01:04:44.840]   that this project of wanting that,
[01:04:44.840 --> 01:04:46.400]   and especially to the extent that that makes
[01:04:46.400 --> 01:04:48.760]   some deep reference to human values,
[01:04:48.760 --> 01:04:53.320]   involves this like kind of short-sighted parochial,
[01:04:53.320 --> 01:04:56.700]   like imposition of like our current unreflective values.
[01:04:56.700 --> 01:04:58.640]   So it's just like, yeah, we're gonna have like,
[01:04:58.760 --> 01:05:03.760]   I don't know, like I think they sort of imagine this,
[01:05:03.760 --> 01:05:07.400]   that we're forgetting that we too,
[01:05:07.400 --> 01:05:08.900]   there's like a kind of reflective process
[01:05:08.900 --> 01:05:11.280]   and a kind of a moral progress dimension
[01:05:11.280 --> 01:05:13.840]   that we want to like leave room for, right?
[01:05:13.840 --> 01:05:17.520]   You know, like whatever, Jefferson has this line about like,
[01:05:17.520 --> 01:05:20.680]   ah, you know, just as you wouldn't want to like force a man,
[01:05:20.680 --> 01:05:23.000]   a grown man into like a younger man's coat,
[01:05:23.000 --> 01:05:25.440]   so we don't wanna like change civilization
[01:05:25.440 --> 01:05:26.480]   to like a barber's pass or whatever,
[01:05:26.480 --> 01:05:27.680]   like everyone should agree on that,
[01:05:27.680 --> 01:05:29.880]   including and the people who are interested in alignment
[01:05:29.880 --> 01:05:31.440]   also agree on that.
[01:05:31.440 --> 01:05:36.160]   So obviously there's a concern that people like don't engage
[01:05:36.160 --> 01:05:37.800]   in that process or that something shuts down
[01:05:37.800 --> 01:05:38.720]   the process of reflection,
[01:05:38.720 --> 01:05:40.880]   but I think everyone agrees we want that.
[01:05:40.880 --> 01:05:43.360]   And so that will lead potentially to something
[01:05:43.360 --> 01:05:48.360]   that is quite different from our current conception
[01:05:48.360 --> 01:05:49.720]   of what's valuable.
[01:05:49.720 --> 01:05:54.640]   And there's a question of how different,
[01:05:54.640 --> 01:05:56.120]   and I think there are also questions about
[01:05:56.120 --> 01:05:57.880]   what exactly are we talking about with reflection?
[01:05:57.880 --> 01:05:59.760]   I have an essay on this where I think this is not,
[01:05:59.760 --> 01:06:02.440]   I don't actually think there's a kind of off the shelf
[01:06:02.440 --> 01:06:05.120]   pre-normative notion of reflection
[01:06:05.120 --> 01:06:05.960]   that you can just be like,
[01:06:05.960 --> 01:06:07.200]   oh, obviously you take an agent,
[01:06:07.200 --> 01:06:08.920]   you stick it through reflection
[01:06:08.920 --> 01:06:10.880]   and then you get like values, right?
[01:06:10.880 --> 01:06:15.440]   Like, no, there's a bunch of types of reflect,
[01:06:15.440 --> 01:06:17.200]   I mean, I think that really there's just a bunch,
[01:06:17.200 --> 01:06:18.840]   there's like a whole pattern of empirical facts
[01:06:18.840 --> 01:06:19.920]   about like take an agent,
[01:06:19.920 --> 01:06:23.000]   put it through some process of like reflection,
[01:06:23.000 --> 01:06:24.520]   all sorts of things, ask it questions,
[01:06:25.040 --> 01:06:26.800]   and then that'll go in all sorts of directions
[01:06:26.800 --> 01:06:28.560]   for a given empirical case.
[01:06:28.560 --> 01:06:30.960]   And then you have to look at the pattern of outputs
[01:06:30.960 --> 01:06:33.480]   and be like, okay, what do I make of that?
[01:06:33.480 --> 01:06:35.520]   But overall, I think we should expect,
[01:06:35.520 --> 01:06:38.280]   like even the good futures I think will be quite weird.
[01:06:38.280 --> 01:06:44.960]   And they might even be incomprehensible like to us.
[01:06:44.960 --> 01:06:46.600]   I don't think so.
[01:06:46.600 --> 01:06:48.480]   Like, I mean, there's different types of incomprehensible.
[01:06:48.480 --> 01:06:49.840]   So say I show up in the future
[01:06:49.840 --> 01:06:50.920]   and this is all computers, right?
[01:06:50.920 --> 01:06:52.400]   I'm like, okay, all right.
[01:06:52.400 --> 01:06:54.040]   And then they're like, we're up, we ran,
[01:06:54.040 --> 01:06:55.240]   we're running like creatures on the computers.
[01:06:55.240 --> 01:06:56.520]   I'm like, so I have to somehow get in there
[01:06:56.520 --> 01:06:58.720]   and see like what's actually going on with the computers
[01:06:58.720 --> 01:07:00.280]   or something like that.
[01:07:00.280 --> 01:07:01.240]   Maybe I can actually see,
[01:07:01.240 --> 01:07:02.720]   maybe I actually understand what's going on
[01:07:02.720 --> 01:07:04.560]   in the computers, but I don't yet know what values
[01:07:04.560 --> 01:07:06.320]   I should be using to evaluate that.
[01:07:06.320 --> 01:07:08.720]   So it can be the case that you don't,
[01:07:08.720 --> 01:07:11.640]   us, if we showed up, would not be very good
[01:07:11.640 --> 01:07:13.640]   at like recognizing goodness or badness.
[01:07:13.640 --> 01:07:16.240]   I don't think that makes it insignificant though.
[01:07:16.240 --> 01:07:18.840]   Like suppose you show up in a future and it's like,
[01:07:20.320 --> 01:07:23.320]   it's got some answer to the Riemann hypothesis, right?
[01:07:23.320 --> 01:07:25.440]   And you can't tell whether that answer is right.
[01:07:25.440 --> 01:07:27.040]   You know, maybe the civilization like went wrong.
[01:07:27.040 --> 01:07:28.480]   This is still an important difference, right?
[01:07:28.480 --> 01:07:30.360]   It's just that you can't track it.
[01:07:30.360 --> 01:07:32.080]   And I think something similar is true of like worlds
[01:07:32.080 --> 01:07:35.400]   that are genuinely expressive of like what we would value
[01:07:35.400 --> 01:07:37.520]   if we engaged in like processes of reflection
[01:07:37.520 --> 01:07:40.440]   that we endorse versus ones that have kind of like
[01:07:40.440 --> 01:07:43.280]   totally veered off into something meaningless.
[01:07:43.280 --> 01:07:45.360]   I think like one thing I've heard people
[01:07:45.360 --> 01:07:48.040]   who are skeptical of this ontology just be like,
[01:07:48.040 --> 01:07:50.040]   all right, what do you even mean by alignment?
[01:07:50.040 --> 01:07:52.080]   And obviously the very first question we answer,
[01:07:52.080 --> 01:07:53.760]   do you express like, here's different things
[01:07:53.760 --> 01:07:57.080]   that could mean, do you mean balance of power?
[01:07:57.080 --> 01:07:59.240]   Do you mean somewhere between like that
[01:07:59.240 --> 01:08:00.480]   and dictator or whatever?
[01:08:00.480 --> 01:08:04.080]   Then there's another thing, which is like separate
[01:08:04.080 --> 01:08:06.040]   from the AI discussion, like I don't want the future
[01:08:06.040 --> 01:08:07.360]   to contain a bunch of torture.
[01:08:07.360 --> 01:08:09.760]   And like, it's not necessarily like a technical,
[01:08:09.760 --> 01:08:12.000]   I mean, like part of it might involve technically
[01:08:12.000 --> 01:08:16.040]   aligning GPT-4, but it's like, that's not what it,
[01:08:16.040 --> 01:08:16.880]   you know what I mean?
[01:08:17.000 --> 01:08:20.240]   That's like a proxy to get to like that future.
[01:08:20.240 --> 01:08:23.080]   The sort of question then is,
[01:08:23.080 --> 01:08:26.920]   what we really mean by alignment, is it just like,
[01:08:26.920 --> 01:08:28.920]   whatever it takes to make sure the future
[01:08:28.920 --> 01:08:30.600]   doesn't have a bunch of torture?
[01:08:30.600 --> 01:08:34.440]   Or do we mean like, what I really care about
[01:08:34.440 --> 01:08:37.720]   is in a thousand years, things that are like,
[01:08:37.720 --> 01:08:40.000]   that are like clearly my descendants,
[01:08:40.000 --> 01:08:42.800]   not like some thing where I like,
[01:08:42.800 --> 01:08:44.640]   I recognize they have their own art or whatever.
[01:08:44.640 --> 01:08:47.280]   It's like, no, no, it's like, if it was like my grandchild,
[01:08:47.280 --> 01:08:48.440]   it's like that level of descendant
[01:08:48.440 --> 01:08:50.640]   is controlling the galaxy,
[01:08:50.640 --> 01:08:52.200]   even if they're not conducting torture.
[01:08:52.200 --> 01:08:54.000]   And I think like what some people mean is like,
[01:08:54.000 --> 01:08:57.760]   our intellectual descendants should control the light cone,
[01:08:57.760 --> 01:08:59.920]   even if it's like, even if the other counterfactual
[01:08:59.920 --> 01:09:01.240]   doesn't involve a bunch of torture.
[01:09:01.240 --> 01:09:02.080]   Yeah, so I agree.
[01:09:02.080 --> 01:09:04.080]   I mean, I think there's a few different things there, right?
[01:09:04.080 --> 01:09:08.400]   So there's kind of, what are you going for?
[01:09:08.400 --> 01:09:09.600]   You're going for like actively good,
[01:09:09.600 --> 01:09:12.800]   are you going for avoiding certain stuff, right?
[01:09:12.800 --> 01:09:13.800]   And then there's a different question,
[01:09:13.800 --> 01:09:16.760]   which is what counts as actively good according to you?
[01:09:16.760 --> 01:09:20.880]   So maybe some people are like,
[01:09:20.880 --> 01:09:23.280]   the only things that are actively good
[01:09:23.280 --> 01:09:28.040]   are like my grandchildren, or I don't know,
[01:09:28.040 --> 01:09:31.720]   like some like literal descending genetic line from me
[01:09:31.720 --> 01:09:32.560]   or something.
[01:09:32.560 --> 01:09:34.840]   I'm like, well, that's not my thing.
[01:09:34.840 --> 01:09:39.560]   And I don't think it's really what most people have in mind
[01:09:39.560 --> 01:09:40.920]   when they talk about goodness.
[01:09:40.920 --> 01:09:44.280]   I mean, I think there's a conversation to be had
[01:09:44.280 --> 01:09:45.800]   and obviously in some sense,
[01:09:45.800 --> 01:09:47.080]   when we talk about a good future,
[01:09:47.080 --> 01:09:47.920]   we need to be thinking about like,
[01:09:47.920 --> 01:09:49.000]   what are all the stakeholders here
[01:09:49.000 --> 01:09:50.640]   and how does it all fit together?
[01:09:50.640 --> 01:09:58.000]   But I think, yeah, when I think about it,
[01:09:58.000 --> 01:10:03.160]   I'm not assuming that there's some notion of like descendants
[01:10:03.160 --> 01:10:06.240]   or like some, like, I think there's a kind of,
[01:10:06.240 --> 01:10:08.720]   the thing that matters about the kind of lineage
[01:10:08.720 --> 01:10:13.160]   is this, whatever's required
[01:10:13.160 --> 01:10:18.160]   for kind of the kind of optimization processes to be,
[01:10:18.160 --> 01:10:23.360]   in some sense, pushing towards good stuff.
[01:10:23.360 --> 01:10:27.120]   And there's a kind of concern
[01:10:27.120 --> 01:10:29.640]   that that is kind of currently,
[01:10:29.640 --> 01:10:32.040]   that a lot of what is sort of making that happen
[01:10:32.040 --> 01:10:38.240]   is kind of lives in human civilization in some sense.
[01:10:38.680 --> 01:10:40.280]   And so we don't know exactly what,
[01:10:40.280 --> 01:10:43.000]   there's some kind of seed of goodness
[01:10:43.000 --> 01:10:45.880]   that we're carrying in different ways
[01:10:45.880 --> 01:10:47.920]   or different people,
[01:10:47.920 --> 01:10:48.800]   there's different notions of goodness
[01:10:48.800 --> 01:10:49.640]   for different people maybe,
[01:10:49.640 --> 01:10:50.600]   but there's some sort of seed
[01:10:50.600 --> 01:10:52.480]   that is currently like here that we have
[01:10:52.480 --> 01:10:56.520]   that is not sort of just in the universe everywhere.
[01:10:56.520 --> 01:10:58.040]   It's not just gonna crop up
[01:10:58.040 --> 01:11:00.600]   if you just sort of die out or something.
[01:11:00.600 --> 01:11:03.400]   It's something that is in some sense contingent
[01:11:03.400 --> 01:11:04.240]   to our civilization,
[01:11:04.240 --> 01:11:05.920]   or at least that's the picture we can talk about
[01:11:05.920 --> 01:11:07.360]   whether that's right.
[01:11:07.360 --> 01:11:11.080]   And so I think the sense in which kind of stories
[01:11:11.080 --> 01:11:13.240]   about good futures that have to do with alignment
[01:11:13.240 --> 01:11:15.600]   are kind of about descendants,
[01:11:15.600 --> 01:11:18.400]   I think it's more about like whatever that seed is,
[01:11:18.400 --> 01:11:19.480]   how do we kind of carry it?
[01:11:19.480 --> 01:11:24.480]   How do we keep the life thread alive going into the future?
[01:11:24.480 --> 01:11:27.120]   But then I'm like, one could accuse
[01:11:27.120 --> 01:11:28.760]   like sort of the alignment community
[01:11:28.760 --> 01:11:30.440]   of like a sort of modern Bailey,
[01:11:30.440 --> 01:11:32.760]   of like, the Mott is,
[01:11:32.760 --> 01:11:36.440]   we just wanna make sure that GPT-8 doesn't kill everybody.
[01:11:36.440 --> 01:11:40.480]   And after that, it's like all you guys, we're all cool.
[01:11:40.480 --> 01:11:44.280]   But then like, the real thing is,
[01:11:44.280 --> 01:11:48.840]   we are fundamentally pessimistic about historical processes
[01:11:48.840 --> 01:11:53.320]   in a way that doesn't even necessarily implicate AI alone,
[01:11:53.320 --> 01:11:55.880]   but just like the nature of the universe.
[01:11:55.880 --> 01:11:57.720]   And we wanna do something about,
[01:11:57.720 --> 01:11:59.160]   to make sure like the nature of the universe
[01:11:59.160 --> 01:12:01.520]   doesn't take a hold on humans.
[01:12:01.520 --> 01:12:03.440]   You know what I like, where things are headed.
[01:12:03.440 --> 01:12:06.240]   So if you look at Soviet Union,
[01:12:06.240 --> 01:12:09.960]   the collectivization of farming
[01:12:09.960 --> 01:12:12.240]   and the disempowerment of the kulaks
[01:12:12.240 --> 01:12:15.360]   was not as a practical matter necessary.
[01:12:15.360 --> 01:12:16.880]   In fact, it was extremely counterproductive.
[01:12:16.880 --> 01:12:18.840]   It almost brought down the regime
[01:12:18.840 --> 01:12:20.880]   and it obviously killed millions of people,
[01:12:20.880 --> 01:12:23.000]   you know, caused a huge famine,
[01:12:23.000 --> 01:12:25.040]   but it was sort of ideologically necessary
[01:12:25.040 --> 01:12:26.000]   in the sense of like,
[01:12:26.000 --> 01:12:28.960]   you have, we have an ember of something here
[01:12:28.960 --> 01:12:29.800]   and we've got to make sure
[01:12:29.800 --> 01:12:33.520]   that enclave of the other thing doesn't,
[01:12:33.520 --> 01:12:35.320]   it does have like, it's sort of like,
[01:12:35.320 --> 01:12:36.680]   if you have raw competition
[01:12:36.680 --> 01:12:38.760]   between the kulak type capitalism
[01:12:38.760 --> 01:12:40.240]   and what we're trying to build here,
[01:12:40.240 --> 01:12:43.520]   the gray goo of the kulaks will just like take over, right?
[01:12:43.520 --> 01:12:45.120]   And so like, we have this ember here,
[01:12:45.120 --> 01:12:47.080]   we're gonna like do worldwide revolution from it.
[01:12:47.080 --> 01:12:49.160]   I know that obviously that's not exactly
[01:12:49.160 --> 01:12:50.600]   the kind of thing Alignment has in mind,
[01:12:50.600 --> 01:12:53.680]   but like we have an ember here and like we gotta,
[01:12:53.680 --> 01:12:55.520]   we gotta make sure that this other thing
[01:12:55.520 --> 01:12:57.640]   that's happening on the side doesn't,
[01:12:57.640 --> 01:12:58.840]   you know, sort of,
[01:12:58.840 --> 01:13:00.360]   I obviously that's not how they would phrase it,
[01:13:00.360 --> 01:13:03.200]   but like get a toll on what we're building here.
[01:13:04.240 --> 01:13:05.160]   And that's maybe the worry
[01:13:05.160 --> 01:13:06.760]   that people who are opposed to Alignment have is like,
[01:13:06.760 --> 01:13:08.040]   you mean the second kind of thing,
[01:13:08.040 --> 01:13:10.000]   like the kind of thing that maybe Stalin
[01:13:10.000 --> 01:13:10.840]   like was worried about,
[01:13:10.840 --> 01:13:12.240]   even though obviously wouldn't endorse
[01:13:12.240 --> 01:13:14.280]   and like the specific things he did.
[01:13:14.280 --> 01:13:15.200]   When people talk about Alignment,
[01:13:15.200 --> 01:13:16.560]   they have in mind
[01:13:16.560 --> 01:13:17.920]   a number of different types of goals, right?
[01:13:17.920 --> 01:13:19.720]   So one type of goal is quite minimal.
[01:13:19.720 --> 01:13:24.520]   It's something like that the AIs don't kill everyone,
[01:13:24.520 --> 01:13:28.720]   that they, or kind of violently disempower people.
[01:13:28.720 --> 01:13:30.400]   Now there's a second thing people sometimes want
[01:13:30.400 --> 01:13:33.320]   out of Alignment, which is a much broader,
[01:13:33.320 --> 01:13:34.440]   which is something like,
[01:13:34.440 --> 01:13:36.600]   we would like it to be the case that
[01:13:36.600 --> 01:13:39.160]   our AIs are such that when we incorporate them
[01:13:39.160 --> 01:13:42.120]   into our society, things are good, right?
[01:13:42.120 --> 01:13:44.160]   That we just have a good future.
[01:13:44.160 --> 01:13:46.840]   I do agree that I think the discourse
[01:13:46.840 --> 01:13:51.840]   about AI Alignment mixes together these two goals
[01:13:51.840 --> 01:13:53.920]   that I mentioned.
[01:13:53.920 --> 01:13:56.880]   The sort of most straightforward thing to focus on,
[01:13:56.880 --> 01:13:57.840]   and I, you know, I don't,
[01:13:57.840 --> 01:14:01.000]   I don't blame people for just talking about this one,
[01:14:01.000 --> 01:14:02.440]   is just the first one.
[01:14:02.440 --> 01:14:03.280]   When we think about like,
[01:14:03.280 --> 01:14:04.640]   in which context is it appropriate
[01:14:04.640 --> 01:14:06.880]   to try to exert various types of control,
[01:14:06.880 --> 01:14:09.480]   or to kind of have more of what I call in the series Yang,
[01:14:09.480 --> 01:14:14.400]   which is this kind of active kind of controlling force,
[01:14:14.400 --> 01:14:15.240]   as opposed to Yin,
[01:14:15.240 --> 01:14:17.960]   which is this more kind of receptive, open, letting go.
[01:14:17.960 --> 01:14:21.440]   A kind of paradigm context
[01:14:21.440 --> 01:14:22.920]   in which we think that is appropriate,
[01:14:22.920 --> 01:14:27.920]   is if something is a kind of active aggressor towards,
[01:14:28.760 --> 01:14:31.680]   against like the sort of boundaries
[01:14:31.680 --> 01:14:33.000]   and cooperative structures
[01:14:33.000 --> 01:14:35.200]   that we've created as a civilization, right?
[01:14:35.200 --> 01:14:39.160]   So, you know, I talk about the Nazis,
[01:14:39.160 --> 01:14:40.560]   or, you know, in the piece,
[01:14:40.560 --> 01:14:42.240]   it's sort of like when you sort of invade,
[01:14:42.240 --> 01:14:44.200]   if something is invading,
[01:14:44.200 --> 01:14:46.320]   we often think it's appropriate to like fight back, right?
[01:14:46.320 --> 01:14:47.320]   And we often think it's appropriate
[01:14:47.320 --> 01:14:49.680]   to like set up structures to kind of prevent,
[01:14:49.680 --> 01:14:54.680]   and kind of ensure that these basic norms
[01:14:55.120 --> 01:14:58.400]   of kind of peace and harmony are kind of adhered to.
[01:14:58.400 --> 01:15:02.560]   And I do think some of the kind of moral heft
[01:15:02.560 --> 01:15:04.480]   of some parts of the alignment discourse
[01:15:04.480 --> 01:15:05.680]   comes from drawing specifically
[01:15:05.680 --> 01:15:07.200]   on that aspect of our morality, right?
[01:15:07.200 --> 01:15:10.480]   So we think the AIs are presented as aggressors
[01:15:10.480 --> 01:15:13.560]   that are coming to kill you.
[01:15:13.560 --> 01:15:16.640]   And if that's true,
[01:15:16.640 --> 01:15:17.840]   then it's quite appropriate,
[01:15:17.840 --> 01:15:20.200]   I think, to like really be like, okay,
[01:15:20.200 --> 01:15:24.720]   we, it is kind of, that's classic human stuff.
[01:15:24.720 --> 01:15:27.200]   Almost everyone recognizes that kind of self-defense
[01:15:27.200 --> 01:15:31.240]   or like ensuring kind of basic norms are adhered to
[01:15:31.240 --> 01:15:35.440]   is a kind of justified use of like certain kinds of power
[01:15:35.440 --> 01:15:37.080]   that would often be unjustified in other contexts.
[01:15:37.080 --> 01:15:39.080]   So self-defense is a clear example there.
[01:15:39.080 --> 01:15:42.200]   I do think it's important though,
[01:15:42.200 --> 01:15:47.000]   to separate that concern from this other concern
[01:15:47.000 --> 01:15:50.680]   about where does the future eventually go?
[01:15:50.680 --> 01:15:52.480]   And how much do we wanna be kind of,
[01:15:52.920 --> 01:15:57.040]   trying to steer that actively?
[01:15:57.040 --> 01:15:59.680]   So to some extent, I wrote the series partly in response
[01:15:59.680 --> 01:16:00.600]   to the thing you're talking about,
[01:16:00.600 --> 01:16:05.440]   which is, I think it is true that aspects of this discourse
[01:16:05.440 --> 01:16:09.480]   involve the possibility of like trying to grip,
[01:16:09.480 --> 01:16:11.760]   like I think trying to kind of steer and grip
[01:16:11.760 --> 01:16:12.720]   and like kind of rent,
[01:16:12.720 --> 01:16:13.680]   you have this sense of the universe
[01:16:13.680 --> 01:16:15.800]   is about to kind of go off in some direction
[01:16:15.800 --> 01:16:16.640]   and you need to,
[01:16:16.640 --> 01:16:20.000]   and people notice that muscle.
[01:16:20.000 --> 01:16:21.280]   And part of what I wanna do is like,
[01:16:21.280 --> 01:16:23.440]   well, we have a very rich ethical,
[01:16:23.440 --> 01:16:25.720]   human ethical tradition of thinking about like,
[01:16:25.720 --> 01:16:28.240]   when is it appropriate to try to exert
[01:16:28.240 --> 01:16:30.280]   what sorts of control over which things?
[01:16:30.280 --> 01:16:31.720]   And I want that to be,
[01:16:31.720 --> 01:16:33.680]   I want us to bring the kind of full force and richness
[01:16:33.680 --> 01:16:35.960]   of that tradition to this discussion, right?
[01:16:35.960 --> 01:16:37.200]   And not like, I think it's easy
[01:16:37.200 --> 01:16:38.760]   if you're purely in this abstract mode
[01:16:38.760 --> 01:16:41.760]   of like utility functions, like human utility function.
[01:16:41.760 --> 01:16:44.080]   And it's like this competitor thing with utility function.
[01:16:44.080 --> 01:16:45.360]   It's like somehow you lose touch
[01:16:45.360 --> 01:16:47.600]   with the kind of complexity of how we actually,
[01:16:47.600 --> 01:16:51.080]   like we've been dealing with kind of differences in values
[01:16:51.080 --> 01:16:53.040]   and kind of competitions for power.
[01:16:53.040 --> 01:16:54.800]   This is classic stuff, right?
[01:16:54.800 --> 01:16:55.840]   And I don't actually think,
[01:16:55.840 --> 01:17:00.840]   I think the AI sort of amplify a lot of the kind of dynamics,
[01:17:00.840 --> 01:17:02.560]   but I don't think it's sort of fundamentally new.
[01:17:02.560 --> 01:17:03.760]   And so part of what I'm trying to say is like,
[01:17:03.760 --> 01:17:05.720]   well, let's draw on our full,
[01:17:05.720 --> 01:17:06.760]   on the full wisdom we have here,
[01:17:06.760 --> 01:17:08.440]   while obviously adjusting for like ways
[01:17:08.440 --> 01:17:10.640]   in which things are different.
[01:17:10.640 --> 01:17:13.960]   So one of the things the Ember analogy brings up
[01:17:13.960 --> 01:17:16.560]   and getting a hold of the future is,
[01:17:16.560 --> 01:17:18.440]   we're gonna go explore space
[01:17:18.440 --> 01:17:21.360]   and that's where we expect most of the things
[01:17:21.360 --> 01:17:23.680]   that will happen, most of the people that will live,
[01:17:23.680 --> 01:17:25.560]   it'll be in space.
[01:17:25.560 --> 01:17:28.480]   And I wonder how much of the high stakes here
[01:17:28.480 --> 01:17:31.920]   is not really about AI per se, but it's about space.
[01:17:31.920 --> 01:17:33.400]   Like it just, it's a coincidence
[01:17:33.400 --> 01:17:35.760]   that we're developing AI at the same time
[01:17:35.760 --> 01:17:38.760]   where you're like on the cusp of expanding
[01:17:38.760 --> 01:17:41.720]   through most of the stuff that exists.
[01:17:41.720 --> 01:17:43.680]   So I don't think it's a coincidence
[01:17:43.680 --> 01:17:46.240]   in that I think the centrally,
[01:17:46.240 --> 01:17:49.200]   like the way we would become able to expand
[01:17:49.200 --> 01:17:51.960]   or the kind of most salient way to me
[01:17:51.960 --> 01:17:55.360]   is via some kind of radical acceleration of our,
[01:17:55.360 --> 01:17:56.200]   of our technological-
[01:17:56.200 --> 01:17:57.400]   Or sorry, let me clarify.
[01:17:57.400 --> 01:17:59.080]   Then like the stakes here,
[01:17:59.080 --> 01:18:02.000]   like if this was just a question of,
[01:18:02.000 --> 01:18:04.560]   do we do AGI and explore the solar system
[01:18:04.560 --> 01:18:06.280]   and there was nothing beyond the solar system,
[01:18:06.280 --> 01:18:08.360]   like we fume and weird things might happen
[01:18:08.360 --> 01:18:09.960]   with the solar system if we get it wrong.
[01:18:09.960 --> 01:18:12.400]   I feel like compared to that,
[01:18:12.400 --> 01:18:15.720]   like billions of galaxies has a different sort of,
[01:18:15.720 --> 01:18:17.000]   that that's what's at stake.
[01:18:17.000 --> 01:18:19.440]   I wonder how much of the discourse is like hinges
[01:18:19.440 --> 01:18:21.720]   on the stakes because of the space.
[01:18:21.720 --> 01:18:25.080]   I mean, I think for most people, very little,
[01:18:25.080 --> 01:18:27.720]   you know, I think people are really like,
[01:18:27.720 --> 01:18:30.480]   what's gonna happen to this world, right?
[01:18:30.480 --> 01:18:32.480]   This world around us that we live in as we,
[01:18:32.480 --> 01:18:34.240]   and you know, what's that gonna happen to me and my kids?
[01:18:34.240 --> 01:18:37.160]   And so I don't actually think, you know,
[01:18:37.160 --> 01:18:38.880]   some people spend a lot of time on the space stuff,
[01:18:38.880 --> 01:18:42.000]   but I think for the most immediately pressing stuff
[01:18:42.000 --> 01:18:43.840]   about AI doesn't require that at all.
[01:18:45.160 --> 01:18:47.800]   I also think like, even if you bracket space,
[01:18:47.800 --> 01:18:51.240]   like time is also very big.
[01:18:51.240 --> 01:18:54.520]   And so, you know, whatever, we've got like 500 million
[01:18:54.520 --> 01:18:55.800]   years, a billion years left on earth
[01:18:55.800 --> 01:18:56.760]   if we don't mess with the sun
[01:18:56.760 --> 01:18:57.840]   and maybe you could get more out of it.
[01:18:57.840 --> 01:18:59.880]   So like, you know, I think there's still,
[01:18:59.880 --> 01:19:03.200]   that's a lot.
[01:19:03.200 --> 01:19:06.640]   So, and then I guess, but yeah,
[01:19:06.640 --> 01:19:07.800]   but I don't know if it like fundamentally
[01:19:07.800 --> 01:19:08.640]   changes the narrative.
[01:19:08.640 --> 01:19:10.400]   Like I do think, I mean, obviously the stakes
[01:19:10.400 --> 01:19:12.400]   insofar as you care about what happens,
[01:19:12.400 --> 01:19:14.280]   you know, in the future or in space,
[01:19:14.280 --> 01:19:15.520]   then like the stakes are way smaller
[01:19:15.520 --> 01:19:18.760]   if you shrink down to the solar system.
[01:19:18.760 --> 01:19:22.520]   And I think that does change potentially some stuff
[01:19:22.520 --> 01:19:25.040]   in that like a really nice feature of our situation
[01:19:25.040 --> 01:19:28.640]   right now, depending on what the actual nature
[01:19:28.640 --> 01:19:32.680]   of kind of the kind of resource pie is,
[01:19:32.680 --> 01:19:35.600]   is that I think, you know, in some sense,
[01:19:35.600 --> 01:19:38.480]   there's such an abundance of energy
[01:19:38.480 --> 01:19:40.680]   and other resources in principle available
[01:19:40.680 --> 01:19:44.240]   to a kind of responsible civilization
[01:19:44.240 --> 01:19:48.600]   that really just tons of stakeholders,
[01:19:48.600 --> 01:19:51.320]   especially ones who are like able to kind of saturate,
[01:19:51.320 --> 01:19:53.760]   get like really close to like amazing
[01:19:53.760 --> 01:19:54.960]   according to their values
[01:19:54.960 --> 01:19:57.960]   with like kind of comparatively small allocations
[01:19:57.960 --> 01:19:58.840]   of resources or something.
[01:19:58.840 --> 01:20:02.720]   Like we can just, you know, I sort of,
[01:20:02.720 --> 01:20:04.240]   I kind of feel like everyone who has like
[01:20:04.240 --> 01:20:06.920]   out of satiable values, who will be like really,
[01:20:06.920 --> 01:20:10.360]   really happy with like some like small kind of fraction
[01:20:10.360 --> 01:20:11.200]   of the available pie.
[01:20:11.200 --> 01:20:13.280]   We should just like satiate all sorts of stuff, right?
[01:20:13.280 --> 01:20:16.240]   And obviously you need to do like, you know,
[01:20:16.240 --> 01:20:18.520]   figure out gains from trade and balance
[01:20:18.520 --> 01:20:20.080]   and like there's like a bunch of complexity here.
[01:20:20.080 --> 01:20:22.960]   But I think in principle, you know,
[01:20:22.960 --> 01:20:27.960]   we're in a position to create a really wonderful,
[01:20:27.960 --> 01:20:30.880]   wonderful scenario for just tons and tons
[01:20:30.880 --> 01:20:32.240]   of different value systems.
[01:20:32.240 --> 01:20:35.480]   And so I think correspondingly,
[01:20:35.480 --> 01:20:38.520]   we should be really interested in doing that, right?
[01:20:38.520 --> 01:20:42.080]   And you know, so I sometimes use this heuristic
[01:20:42.080 --> 01:20:42.920]   in thinking about the future.
[01:20:42.920 --> 01:20:44.120]   You know, I think we should be aspiring
[01:20:44.120 --> 01:20:46.320]   to really kind of leave no one behind, right?
[01:20:46.320 --> 01:20:48.520]   Like really find like who are all the stakeholders here?
[01:20:48.520 --> 01:20:51.280]   How do we really have like a fully inclusive vision
[01:20:51.280 --> 01:20:52.920]   of like how the future could be good
[01:20:52.920 --> 01:20:56.440]   from a very, very wide variety of perspectives.
[01:20:56.440 --> 01:20:59.880]   And I think the kind of vastness of space resources
[01:20:59.880 --> 01:21:03.640]   like makes that a lot, makes that very feasible.
[01:21:03.640 --> 01:21:07.880]   And now if you instead imagine it's a much smaller pie,
[01:21:07.880 --> 01:21:10.360]   well, maybe you face a tougher trade-offs.
[01:21:10.360 --> 01:21:13.240]   And so I think that's like an important dynamic.
[01:21:13.240 --> 01:21:17.680]   Is the inclusivity because of part of your values
[01:21:17.680 --> 01:21:22.680]   includes your different potential futures
[01:21:22.680 --> 01:21:27.240]   getting to play out, or is it because I'm uncertainty
[01:21:27.240 --> 01:21:28.120]   about which the right one is?
[01:21:28.120 --> 01:21:31.840]   So let's make sure we're not nulling out the possible,
[01:21:31.840 --> 01:21:34.760]   if you're wrong, then you're not nulling out all value.
[01:21:34.760 --> 01:21:36.200]   I think it's a bunch of things at once.
[01:21:36.200 --> 01:21:40.760]   So yeah, I'm just, I'm really into being nice
[01:21:40.760 --> 01:21:41.880]   when it's cheap, right?
[01:21:41.880 --> 01:21:45.320]   Like I think if you can just help someone a lot
[01:21:45.320 --> 01:21:47.760]   in a way that's really cheap for you, do it, right?
[01:21:47.760 --> 01:21:48.600]   Or like, I don't know.
[01:21:48.600 --> 01:21:49.760]   I mean, obviously you need to think about trade-offs
[01:21:49.760 --> 01:21:50.600]   and there's like a lot of people
[01:21:50.600 --> 01:21:51.560]   in principle you could be nice to,
[01:21:51.560 --> 01:21:53.800]   but I think like the principle of like be nice
[01:21:53.800 --> 01:21:58.000]   when it's cheap, I'm like very excited to try to uphold.
[01:21:58.000 --> 01:22:01.360]   I also really hope that kind of other people uphold that
[01:22:01.360 --> 01:22:02.800]   with respect to me, including the AIs, right?
[01:22:02.800 --> 01:22:04.840]   Like I think we should be kind of golden ruling.
[01:22:04.840 --> 01:22:06.120]   Like we're thinking about, oh, we're gonna inventing
[01:22:06.120 --> 01:22:09.240]   these AIs, like I think there's some way in which I'm trying
[01:22:09.240 --> 01:22:11.040]   to like kind of embody attitudes towards them
[01:22:11.040 --> 01:22:14.560]   that I like hope that they would embody towards me.
[01:22:14.560 --> 01:22:17.160]   And that's like some, it's unclear exactly
[01:22:17.160 --> 01:22:18.360]   what the ground of that is, but that's something,
[01:22:18.360 --> 01:22:19.880]   you know, I really like the golden rule.
[01:22:19.880 --> 01:22:23.160]   And I think a lot about that as a kind of basis
[01:22:23.160 --> 01:22:27.160]   for treatment of other beings.
[01:22:27.160 --> 01:22:29.840]   And so I think like be nice when it's cheap is like,
[01:22:29.840 --> 01:22:32.520]   if you think about it, if everyone implements that rule,
[01:22:32.520 --> 01:22:35.280]   then we get potentially like a big kind
[01:22:35.280 --> 01:22:39.000]   of Pareto improvement or like, so I don't know exactly
[01:22:39.000 --> 01:22:40.200]   Pareto improvement, but it's like good deal.
[01:22:40.200 --> 01:22:41.600]   It's a lot of good deals.
[01:22:41.600 --> 01:22:47.240]   And yeah, so I think it's that I'm just into pluralism.
[01:22:47.240 --> 01:22:49.400]   I've got uncertainty, you know, there's like all sorts
[01:22:49.400 --> 01:22:51.600]   of stuff swimming around there.
[01:22:51.600 --> 01:22:54.680]   But, and then I think also just as a matter of like
[01:22:54.680 --> 01:23:00.440]   having kind of cooperative and kind of good balances
[01:23:00.440 --> 01:23:02.240]   of power and deals and kind of avoiding conflict,
[01:23:02.240 --> 01:23:04.320]   I think like finding ways to set up structures
[01:23:04.320 --> 01:23:06.600]   that lots and lots of people in value systems
[01:23:06.600 --> 01:23:09.320]   and agents are happy with, including non-humans,
[01:23:09.320 --> 01:23:14.000]   you know, people in the past, AIs, animals.
[01:23:14.000 --> 01:23:15.360]   Like, I really think we should be like,
[01:23:15.360 --> 01:23:20.040]   we should have very broad sweep in thinking
[01:23:20.040 --> 01:23:23.040]   about what sorts of inclusivity we wanna be kind
[01:23:23.040 --> 01:23:25.520]   of reflecting in a kind of mature civilization
[01:23:25.520 --> 01:23:28.520]   and kind of setting ourselves up for doing that.
[01:23:28.520 --> 01:23:32.160]   Okay, so I wanna go back to the Wichita relationship
[01:23:32.160 --> 01:23:36.960]   with these AIs be, because pretty soon we're talking
[01:23:36.960 --> 01:23:39.600]   about our relationship to superhuman intelligences
[01:23:39.600 --> 01:23:41.200]   if we think such a thing as possible.
[01:23:41.200 --> 01:23:43.160]   And so there's a question of what is the process
[01:23:43.160 --> 01:23:45.840]   you get to use to get there and the morality
[01:23:45.840 --> 01:23:47.880]   of gradient dissenting on their minds,
[01:23:47.880 --> 01:23:49.520]   which we can address later.
[01:23:49.520 --> 01:23:52.800]   The thing that gives personally me the most unease
[01:23:52.800 --> 01:23:57.800]   about alignment, quote, unquote, is at least part
[01:24:00.160 --> 01:24:02.600]   of the vision here sounds like you're going
[01:24:02.600 --> 01:24:04.800]   to enslave a God.
[01:24:04.800 --> 01:24:07.480]   And like, there's just something like that's feels
[01:24:07.480 --> 01:24:10.000]   so wrong about that.
[01:24:10.000 --> 01:24:12.560]   But then the question is like, if you don't enslave the God,
[01:24:12.560 --> 01:24:14.200]   like obviously the God's gonna have more control.
[01:24:14.200 --> 01:24:17.080]   Are you okay with, you're gonna surrender most
[01:24:17.080 --> 01:24:20.800]   of everything, obviously, you know what I mean?
[01:24:20.800 --> 01:24:23.200]   Even if it's like a cooperative relationship you have.
[01:24:23.200 --> 01:24:25.240]   I think we as a civilization are going to have
[01:24:25.240 --> 01:24:27.520]   to have a very serious conversation
[01:24:27.520 --> 01:24:32.520]   about what sort of kind of servitude is appropriate
[01:24:32.520 --> 01:24:39.320]   or inappropriate in the context of AI development.
[01:24:39.320 --> 01:24:42.840]   And I think we, there are a bunch of disanalogies
[01:24:42.840 --> 01:24:45.120]   from human slavery that I think are important.
[01:24:45.120 --> 01:24:49.720]   In particular, A, the AIs might not be moral patients
[01:24:49.720 --> 01:24:52.560]   at all, in which case, so we need to figure that out.
[01:24:52.560 --> 01:24:56.400]   There's, there are ways in which we may be able
[01:24:56.400 --> 01:25:00.920]   to kind of, you know, have kind of motivation,
[01:25:00.920 --> 01:25:02.920]   like slavery involves all this like suffering
[01:25:02.920 --> 01:25:04.440]   and kind of non-consent and there's all these
[01:25:04.440 --> 01:25:07.520]   like specific dynamics involved in human slavery.
[01:25:07.520 --> 01:25:11.040]   But I think like, and so some of those may not,
[01:25:11.040 --> 01:25:13.040]   may or may not be present in a given case with AI.
[01:25:13.040 --> 01:25:14.840]   And I think that's important.
[01:25:14.840 --> 01:25:18.200]   But I think overall, like we are gonna need to stare hard
[01:25:18.200 --> 01:25:21.240]   at like right now, the kind of default mode
[01:25:21.240 --> 01:25:25.040]   of how we treat AIs gives them no moral consideration
[01:25:25.040 --> 01:25:27.440]   at all, right, we're thinking of them as property,
[01:25:27.440 --> 01:25:30.880]   as tools, as products, and designing them
[01:25:30.880 --> 01:25:32.440]   to be assistants and stuff like that.
[01:25:32.440 --> 01:25:37.320]   And I think, you know, no, there has been no official
[01:25:37.320 --> 01:25:39.600]   communication from any AI developer as to when,
[01:25:39.600 --> 01:25:41.400]   under what circumstances that would change, right?
[01:25:41.400 --> 01:25:46.400]   And so I think there's a conversation to be had there
[01:25:46.400 --> 01:25:50.000]   that we need to have.
[01:25:50.000 --> 01:25:54.440]   And so, and I think there's a bunch
[01:25:54.440 --> 01:25:57.160]   of, yeah, so there's a bunch of stuff to say about that.
[01:25:57.160 --> 01:25:59.640]   I wanna push back on the notion
[01:25:59.640 --> 01:26:00.800]   that there's sort of two options.
[01:26:00.800 --> 01:26:02.760]   There's like enslaved God, whatever that is,
[01:26:02.760 --> 01:26:05.520]   and like loss of control.
[01:26:05.520 --> 01:26:06.360]   Yeah.
[01:26:06.360 --> 01:26:09.440]   And I think like, we can do better than that, right?
[01:26:09.440 --> 01:26:10.920]   Like, let's work on it.
[01:26:10.920 --> 01:26:13.240]   Let's try to do better, especially, you know,
[01:26:13.240 --> 01:26:15.440]   just sort of, I think we can do better.
[01:26:15.440 --> 01:26:17.520]   And I think it might require being thoughtful
[01:26:17.520 --> 01:26:22.520]   and it might require being kind of having, you know,
[01:26:23.040 --> 01:26:24.320]   a kind of mature discourse about this
[01:26:24.320 --> 01:26:26.680]   before we start taking like irreversible moves.
[01:26:26.680 --> 01:26:31.200]   But I'm optimistic that we can at least avoid
[01:26:31.200 --> 01:26:34.320]   like some of the connotations and a lot of the stuff
[01:26:34.320 --> 01:26:36.760]   at stake in that kind of binary.
[01:26:36.760 --> 01:26:39.480]   With respect to how we treat the AIs,
[01:26:39.480 --> 01:26:44.240]   so I have a couple of contradicting intuitions.
[01:26:44.240 --> 01:26:46.080]   And the difficulty with using intuitions in this case
[01:26:46.080 --> 01:26:49.760]   is obviously it's not clear what reference class
[01:26:49.760 --> 01:26:51.720]   an AI we have control over is.
[01:26:51.720 --> 01:26:56.400]   So to give one that's very scared about the things
[01:26:56.400 --> 01:26:58.000]   we're gonna do to these things,
[01:26:58.000 --> 01:27:04.160]   if you read about like life under Stalin or Mao,
[01:27:04.160 --> 01:27:06.800]   it's, there's one version of telling it,
[01:27:06.800 --> 01:27:09.840]   which is actually very similar to what we mean by alignment,
[01:27:09.840 --> 01:27:14.160]   which is we do these like black box experiments
[01:27:14.160 --> 01:27:16.560]   about like, we're gonna make it think that it can defect.
[01:27:16.560 --> 01:27:18.480]   And if it does, we know it's misaligned.
[01:27:18.480 --> 01:27:21.040]   And if you, Mao, the hundred flowers campaign,
[01:27:21.040 --> 01:27:24.080]   where, you know, let a hundred flowers boom,
[01:27:24.080 --> 01:27:26.640]   I'm gonna allow criticism of my regime, so on.
[01:27:26.640 --> 01:27:28.400]   And that lasted for a couple of years.
[01:27:28.400 --> 01:27:30.120]   And afterwards, everybody who did that,
[01:27:30.120 --> 01:27:33.080]   that was a way to find the quote, unquote, the snakes,
[01:27:33.080 --> 01:27:34.880]   who are the rightest, who are secretly hiding,
[01:27:34.880 --> 01:27:36.880]   and, you know, we'll like purge them.
[01:27:36.880 --> 01:27:41.480]   The sort of paranoia of defectors,
[01:27:41.480 --> 01:27:45.360]   like anybody in my entourage, any of my regime,
[01:27:45.360 --> 01:27:47.440]   they could like, they could be a secret capitalist
[01:27:47.440 --> 01:27:49.040]   trying to bring down the regime.
[01:27:49.160 --> 01:27:51.400]   - That's one way of talking about these things,
[01:27:51.400 --> 01:27:53.280]   which is very concerning.
[01:27:53.280 --> 01:27:54.920]   Is that the correct reference class?
[01:27:54.920 --> 01:27:59.200]   - I certainly think concerns in that vein are real.
[01:27:59.200 --> 01:28:02.680]   I mean, I think if you, it is disturbing
[01:28:02.680 --> 01:28:07.040]   how easy many of the analogies with kind of
[01:28:07.040 --> 01:28:11.120]   human historical events and practices
[01:28:11.120 --> 01:28:12.200]   that we kind of deplore,
[01:28:12.200 --> 01:28:15.000]   or at least have a lot of wariness towards,
[01:28:15.000 --> 01:28:20.000]   are as in the context of the kind of way you end up talking
[01:28:20.000 --> 01:28:25.560]   about kind of AI, maintaining control over AI,
[01:28:25.560 --> 01:28:28.120]   like making sure that it doesn't rebel.
[01:28:28.120 --> 01:28:31.200]   Like, I think we should be noticing
[01:28:31.200 --> 01:28:34.440]   the kind of reference class
[01:28:34.440 --> 01:28:37.080]   that some of that talk starts to conjure.
[01:28:37.080 --> 01:28:42.680]   And so basically just, yes, I think we should be very,
[01:28:42.680 --> 01:28:44.120]   we should really notice that.
[01:28:45.120 --> 01:28:48.280]   You know, part of what I'm trying to do in the series
[01:28:48.280 --> 01:28:52.720]   is to bring the kind of full range of considerations
[01:28:52.720 --> 01:28:53.720]   at stake into play, right?
[01:28:53.720 --> 01:28:57.640]   Like, I think it is both the case
[01:28:57.640 --> 01:29:01.120]   that like you, that we should be quite concerned
[01:29:01.120 --> 01:29:03.600]   about like being kind of overly controlling
[01:29:03.600 --> 01:29:05.320]   or, you know, abusive or oppressive,
[01:29:05.320 --> 01:29:09.400]   or there's all sorts of ways you can go too far.
[01:29:09.400 --> 01:29:11.040]   And I think, you know,
[01:29:13.120 --> 01:29:15.720]   there are concerns about the AIs being genuinely dangerous
[01:29:15.720 --> 01:29:20.440]   and genuinely, you know, acting, you know, killing us,
[01:29:20.440 --> 01:29:21.280]   violently overthrowing us.
[01:29:21.280 --> 01:29:24.920]   And I think the moral situation is quite complicated.
[01:29:24.920 --> 01:29:26.480]   And then I think in some sense,
[01:29:26.480 --> 01:29:29.520]   so often when you're,
[01:29:29.520 --> 01:29:31.280]   if you imagine a sort of external aggressor
[01:29:31.280 --> 01:29:36.280]   who's coming in and invading you,
[01:29:36.280 --> 01:29:38.360]   you feel very justified
[01:29:38.360 --> 01:29:40.560]   in doing like a bunch of stuff to prevent that.
[01:29:40.560 --> 01:29:41.440]   It's like a little bit different
[01:29:41.440 --> 01:29:43.240]   when you're like inventing the thing
[01:29:43.240 --> 01:29:46.440]   and you're doing it like incautiously.
[01:29:46.440 --> 01:29:48.720]   Or something, and then you're also,
[01:29:48.720 --> 01:29:50.760]   like, I think the sort of moral justification
[01:29:50.760 --> 01:29:53.600]   you have for, like, there's a different vibe
[01:29:53.600 --> 01:29:55.680]   in terms of like the kind of overall,
[01:29:55.680 --> 01:30:01.560]   yeah, justificatory stance you might have
[01:30:01.560 --> 01:30:04.800]   for various types of like more
[01:30:04.800 --> 01:30:08.120]   kind of power exerting interventions.
[01:30:08.120 --> 01:30:12.160]   And so like, that's like one feature of the situation.
[01:30:12.160 --> 01:30:15.200]   The opposite perspective here is that
[01:30:15.200 --> 01:30:18.040]   you're doing this sort of vibes-based reasoning
[01:30:18.040 --> 01:30:19.320]   of like, ah, that looks yucky,
[01:30:19.320 --> 01:30:21.720]   of like doing gradient descent on these mines.
[01:30:21.720 --> 01:30:25.920]   And in the past, a couple of references,
[01:30:25.920 --> 01:30:29.160]   a couple of similar cases might've been something like
[01:30:29.160 --> 01:30:31.800]   environmentalists not liking nuclear power.
[01:30:31.800 --> 01:30:34.200]   And because the vibes of nuclear don't look green,
[01:30:34.200 --> 01:30:35.920]   but obviously that set back the cause
[01:30:35.920 --> 01:30:37.520]   of fighting climate change.
[01:30:37.520 --> 01:30:41.320]   And so the end result of like a future you're proud of,
[01:30:41.320 --> 01:30:43.640]   a future that's appealing,
[01:30:43.640 --> 01:30:46.440]   it set back because like your vibes about
[01:30:46.440 --> 01:30:48.240]   what would be wrong to brainwash a human,
[01:30:48.240 --> 01:30:50.960]   but you're trying to apply to a disanalogous case
[01:30:50.960 --> 01:30:53.040]   where that's not as relevant.
[01:30:53.040 --> 01:30:57.280]   I do think there's a concern here that I, you know,
[01:30:57.280 --> 01:30:58.800]   I really tried to foreground in the series
[01:30:58.800 --> 01:31:00.160]   that I think is related to what you're saying,
[01:31:00.160 --> 01:31:02.600]   which is something like, you know,
[01:31:02.600 --> 01:31:04.640]   you might be worried that we will be
[01:31:06.280 --> 01:31:10.080]   very gentle and nice and free with the AIs,
[01:31:10.080 --> 01:31:11.800]   and then they'll kill us.
[01:31:11.800 --> 01:31:13.320]   You know, they'll take advantage of that.
[01:31:13.320 --> 01:31:16.080]   And then it will have been like a catastrophe, right?
[01:31:16.080 --> 01:31:21.080]   And so I opened the series basically with an example
[01:31:21.080 --> 01:31:26.040]   that I'm really trying to conjure that possibility
[01:31:26.040 --> 01:31:31.040]   at the same time as conjuring the grounds of gentleness
[01:31:31.040 --> 01:31:34.200]   and the sense in which it is also the case
[01:31:34.200 --> 01:31:35.600]   that these AIs could be,
[01:31:35.600 --> 01:31:38.400]   they can be both be like others, moral patients,
[01:31:38.400 --> 01:31:41.600]   like this sort of new species in the sense of
[01:31:41.600 --> 01:31:43.400]   that should conjure like wonder and reverence
[01:31:43.400 --> 01:31:45.840]   and such that they will kill you.
[01:31:45.840 --> 01:31:48.120]   And so I have this example of like,
[01:31:48.120 --> 01:31:50.360]   ah, this documentary "Grizzly Man"
[01:31:50.360 --> 01:31:53.480]   where there's this environmental activist,
[01:31:53.480 --> 01:31:54.640]   Timothy Treadwell,
[01:31:54.640 --> 01:31:58.960]   and he aspires to approach these grizzly bears.
[01:31:58.960 --> 01:32:00.200]   He lives, you know, in the summer,
[01:32:00.200 --> 01:32:02.560]   he goes into Alaska and he lives with these grizzly bears.
[01:32:02.560 --> 01:32:03.720]   And then he aspires to approach them
[01:32:03.720 --> 01:32:05.480]   with this like gentleness and reverence.
[01:32:05.480 --> 01:32:08.240]   He doesn't use bear mace or he doesn't like carry bear mace.
[01:32:08.240 --> 01:32:10.200]   He doesn't use a fence around his camp.
[01:32:10.200 --> 01:32:15.280]   And he gets eaten alive by the bears
[01:32:15.280 --> 01:32:16.280]   or one of these bears.
[01:32:16.280 --> 01:32:21.280]   And I kind of really wanted to foreground
[01:32:21.280 --> 01:32:23.560]   that possibility in the series.
[01:32:23.560 --> 01:32:24.400]   Like, I think we need to be talking
[01:32:24.400 --> 01:32:26.200]   about these things both at once, right?
[01:32:26.200 --> 01:32:29.040]   And bears can be moral patients, right?
[01:32:29.040 --> 01:32:31.360]   AIs can be moral patients, Nazis are moral patients,
[01:32:31.360 --> 01:32:33.240]   enemy soldiers have souls, right?
[01:32:33.240 --> 01:32:36.640]   And so I think we need to learn the art
[01:32:36.640 --> 01:32:38.960]   of kind of hawk and dove both.
[01:32:38.960 --> 01:32:41.960]   Like kind of, there's this like dynamic here
[01:32:41.960 --> 01:32:44.440]   that we need to be able to hold both sides of
[01:32:44.440 --> 01:32:46.800]   as we kind of go into these trade-offs
[01:32:46.800 --> 01:32:48.840]   and these dilemmas and all sorts of stuff.
[01:32:48.840 --> 01:32:50.280]   And like a lot of, part of what I'm trying to do
[01:32:50.280 --> 01:32:52.200]   in the series is like really kind of bring it all
[01:32:52.200 --> 01:32:53.840]   to the table at once.
[01:32:53.840 --> 01:32:56.840]   I think the big crux that I have,
[01:32:56.840 --> 01:33:00.840]   like if I today was to massively change my mind
[01:33:00.840 --> 01:33:03.920]   about what should be done is just the question
[01:33:03.920 --> 01:33:08.880]   of how weird by default things end up,
[01:33:08.880 --> 01:33:10.280]   how alien they end up.
[01:33:10.280 --> 01:33:13.100]   And a big part of that story is the,
[01:33:13.100 --> 01:33:15.200]   you made a really interesting argument
[01:33:15.200 --> 01:33:17.880]   on your blog post that if moral realism is correct,
[01:33:17.880 --> 01:33:19.720]   that actually makes an empirical prediction,
[01:33:19.720 --> 01:33:22.800]   which is that the aliens, the AIs, whatever,
[01:33:22.800 --> 01:33:24.840]   should converge on the right morality
[01:33:24.840 --> 01:33:27.640]   the same way that they converge on the right mathematics.
[01:33:28.880 --> 01:33:31.920]   I thought that was a really interesting point,
[01:33:31.920 --> 01:33:34.620]   but there's another prediction that moral realism makes,
[01:33:34.620 --> 01:33:36.160]   which is that over time,
[01:33:36.160 --> 01:33:41.160]   society should become more moral, become better.
[01:33:41.160 --> 01:33:45.120]   And to the extent that we think that's happened,
[01:33:45.120 --> 01:33:47.700]   of course, there's the problem of
[01:33:47.700 --> 01:33:48.760]   what morals do you have now?
[01:33:48.760 --> 01:33:50.040]   Well, it's the ones that society
[01:33:50.040 --> 01:33:52.360]   has been converging towards over time.
[01:33:52.360 --> 01:33:54.440]   But to the extent that it's happened,
[01:33:54.440 --> 01:33:58.320]   one of the predictions of moral realism has been confirmed,
[01:33:58.320 --> 01:34:01.720]   which means should we update in favor of moral realism?
[01:34:01.720 --> 01:34:02.720]   One thing I wanna flag is I don't think
[01:34:02.720 --> 01:34:05.120]   all forms of moral realism make this prediction.
[01:34:05.120 --> 01:34:06.480]   Yeah.
[01:34:06.480 --> 01:34:09.400]   And so that's just one point.
[01:34:09.400 --> 01:34:12.560]   I'm happy to talk about the different forms I have in mind.
[01:34:12.560 --> 01:34:14.920]   I think there are also forms of kind of,
[01:34:14.920 --> 01:34:16.680]   things that kind of look like moral anti-realism,
[01:34:16.680 --> 01:34:18.080]   at least in their metaphysics, according to me,
[01:34:18.080 --> 01:34:21.240]   but which just posit that in fact, there's this convergence.
[01:34:21.240 --> 01:34:24.840]   It's not in virtue of interacting with some like,
[01:34:24.840 --> 01:34:25.960]   kind of mind-independent moral truth,
[01:34:25.960 --> 01:34:28.040]   but just like, it's just for some other reason,
[01:34:28.040 --> 01:34:28.880]   it's the case that,
[01:34:28.880 --> 01:34:31.080]   and that looks like a lot like moral realism at that point,
[01:34:31.080 --> 01:34:32.720]   'cause it's kind of like, oh, it's really universal.
[01:34:32.720 --> 01:34:33.800]   Like everyone ends up here,
[01:34:33.800 --> 01:34:36.240]   and it's kind of tempted to be like, ah, like why, right?
[01:34:36.240 --> 01:34:38.400]   Is that, and then whatever answer for the why
[01:34:38.400 --> 01:34:40.600]   is a little bit like, is that the Tao?
[01:34:40.600 --> 01:34:41.960]   Is that the nature of the Tao?
[01:34:41.960 --> 01:34:44.200]   Even if there's not sort of an extra metaphysical realm
[01:34:44.200 --> 01:34:46.160]   in which the moral lives or something.
[01:34:46.160 --> 01:34:50.680]   So yeah, so moral convergence,
[01:34:50.680 --> 01:34:51.760]   I think is sort of a different factor
[01:34:51.760 --> 01:34:53.360]   from like the existence or non-existence
[01:34:53.360 --> 01:34:55.920]   of kind of non-natural,
[01:34:55.920 --> 01:34:57.620]   like a kind of morality that's not reducible
[01:34:57.620 --> 01:34:58.460]   to natural facts,
[01:34:58.460 --> 01:35:01.500]   which is the type of moral realism I usually consider.
[01:35:01.500 --> 01:35:07.920]   Now, okay, so does the improvement of society,
[01:35:07.920 --> 01:35:09.800]   is that an update towards moral realism?
[01:35:09.800 --> 01:35:10.840]   I mean, I guess like,
[01:35:10.840 --> 01:35:19.200]   so maybe it's like a very weak update or something.
[01:35:19.200 --> 01:35:20.680]   Like, I guess I'm kind of like,
[01:35:20.680 --> 01:35:22.160]   which view like predicts this hard?
[01:35:22.160 --> 01:35:24.240]   I guess it feels to me like moral anti-realism
[01:35:24.240 --> 01:35:29.240]   is like very comfortable with the observation of the like-
[01:35:29.240 --> 01:35:32.160]   People with certain values have those values.
[01:35:32.160 --> 01:35:33.740]   Well, yeah, so there's obviously this like first thing,
[01:35:33.740 --> 01:35:34.580]   which is like any,
[01:35:34.580 --> 01:35:37.400]   if you're the culmination of some process of moral change,
[01:35:37.400 --> 01:35:39.520]   then it's very easy to look back at that process
[01:35:39.520 --> 01:35:41.360]   and be like, moral progress,
[01:35:41.360 --> 01:35:43.400]   like the arc of history bends towards me.
[01:35:43.400 --> 01:35:46.640]   You can look more, like if it was like,
[01:35:46.640 --> 01:35:49.160]   if there was a bunch of dice rolls along the way,
[01:35:49.160 --> 01:35:50.800]   you might be like, oh wait, that's not rational.
[01:35:50.800 --> 01:35:52.720]   That's not the march of reason.
[01:35:52.720 --> 01:35:55.320]   So there's still like empirical work you can do
[01:35:55.320 --> 01:35:57.160]   to tell whether that's what's going on.
[01:35:57.160 --> 01:36:01.800]   But I also think it's just, you know,
[01:36:01.800 --> 01:36:04.320]   on moral anti-realism, I think it's just still possible.
[01:36:04.320 --> 01:36:07.040]   Say like consider Aristotle and us, right?
[01:36:07.040 --> 01:36:11.040]   And we're like, okay, has there been moral progress
[01:36:11.040 --> 01:36:13.580]   by Aristotle's lights or something?
[01:36:13.580 --> 01:36:17.680]   And our lights too, right?
[01:36:19.000 --> 01:36:19.840]   And you could think,
[01:36:19.840 --> 01:36:21.800]   ah, isn't that a little bit like moral realism?
[01:36:21.800 --> 01:36:24.920]   It's like these hearts are singing in harmony.
[01:36:24.920 --> 01:36:26.220]   That's a moral realist thing, right?
[01:36:26.220 --> 01:36:27.060]   The anti-realist thing,
[01:36:27.060 --> 01:36:28.040]   the hearts all go different directions,
[01:36:28.040 --> 01:36:29.920]   but you and Aristotle apparently like,
[01:36:29.920 --> 01:36:33.500]   are both excited about the kind of march of history.
[01:36:33.500 --> 01:36:37.560]   Some open question about whether that's true,
[01:36:37.560 --> 01:36:40.800]   like what are Aristotle's like reflective values, right?
[01:36:40.800 --> 01:36:42.200]   Suppose it is true.
[01:36:42.200 --> 01:36:44.800]   I think that's fairly explicable in moral anti-realist terms.
[01:36:44.800 --> 01:36:46.680]   You can say roughly that like, yeah,
[01:36:46.680 --> 01:36:49.080]   you and Aristotle are sufficiently similar
[01:36:49.080 --> 01:36:51.800]   and you endorse sufficiently similar
[01:36:51.800 --> 01:36:53.600]   kind of reflective processes.
[01:36:53.600 --> 01:36:55.720]   And those processes are in fact instantiated
[01:36:55.720 --> 01:36:59.640]   in the march of history that, yeah, you know,
[01:36:59.640 --> 01:37:01.160]   history has been good for both of you.
[01:37:01.160 --> 01:37:02.880]   And I don't think that's, you know,
[01:37:02.880 --> 01:37:07.880]   I think there are worlds where that isn't the case.
[01:37:07.880 --> 01:37:11.800]   And so I think there's a sense in which
[01:37:11.800 --> 01:37:13.400]   maybe that prediction is more likely
[01:37:13.400 --> 01:37:15.360]   for realism than anti-realism,
[01:37:15.360 --> 01:37:18.160]   but I don't, it doesn't like move me very much.
[01:37:18.160 --> 01:37:20.760]   One thing I wonder is, look, there's,
[01:37:20.760 --> 01:37:23.200]   I don't know if moral realism is the right word,
[01:37:23.200 --> 01:37:24.600]   but the thing you mentioned about,
[01:37:24.600 --> 01:37:29.000]   there's something that makes hearts converge
[01:37:29.000 --> 01:37:32.960]   to the thing we are or the thing we upon reflection would be.
[01:37:32.960 --> 01:37:35.520]   And even if it's not something that's like instantiated
[01:37:35.520 --> 01:37:36.680]   in a realm beyond the universe,
[01:37:36.680 --> 01:37:38.040]   it's like a force that exists
[01:37:38.040 --> 01:37:40.760]   that acts in a way we're happy with.
[01:37:40.760 --> 01:37:43.000]   To the extent that doesn't exist
[01:37:43.000 --> 01:37:45.160]   and you let go of the reins
[01:37:45.160 --> 01:37:47.120]   and then you get the paper clippers.
[01:37:47.120 --> 01:37:50.240]   It feels like we were doomed a long time ago
[01:37:50.240 --> 01:37:51.600]   in the sense of, yeah,
[01:37:51.600 --> 01:37:54.120]   I just different utility functions
[01:37:54.120 --> 01:37:55.400]   banging against each other.
[01:37:55.400 --> 01:37:58.960]   And some of them have parochial preferences,
[01:37:58.960 --> 01:38:03.040]   but like, you know, it just combat and some guy won.
[01:38:03.040 --> 01:38:05.960]   Whereas in the world where like, no, this is the thing,
[01:38:05.960 --> 01:38:08.120]   like these are where the hearts are supposed to go
[01:38:08.120 --> 01:38:12.000]   or it's only by catastrophe that they don't end up there.
[01:38:13.000 --> 01:38:14.760]   That's sort of, that feels like the world
[01:38:14.760 --> 01:38:16.360]   where like really matters.
[01:38:16.360 --> 01:38:18.640]   And in that world, the worry,
[01:38:18.640 --> 01:38:20.920]   the initial question I asked is like,
[01:38:20.920 --> 01:38:23.440]   what would make us think that alignment was a big mistake?
[01:38:23.440 --> 01:38:25.280]   In the world where hearts just naturally end up
[01:38:25.280 --> 01:38:29.240]   towards like the thing, what we want,
[01:38:29.240 --> 01:38:30.800]   maybe it takes an extremely strong force
[01:38:30.800 --> 01:38:32.560]   to push them away from that.
[01:38:32.560 --> 01:38:34.320]   And that extremely strong forces,
[01:38:34.320 --> 01:38:36.360]   you solve technical alignment and just like-
[01:38:36.360 --> 01:38:37.200]   - No.
[01:38:37.200 --> 01:38:41.840]   - Yeah, you're just like the blinders on the horse's eyes.
[01:38:41.840 --> 01:38:43.440]   So like in the worlds where like,
[01:38:43.440 --> 01:38:44.640]   the worlds that really matter,
[01:38:44.640 --> 01:38:47.080]   we're like, ah, this is where the hearts want to go.
[01:38:47.080 --> 01:38:50.240]   In that world, maybe alignment is what fucks us up.
[01:38:50.240 --> 01:38:52.360]   On this question of kind of do the worlds
[01:38:52.360 --> 01:38:55.440]   where there's not this kind of convergent moral force,
[01:38:55.440 --> 01:39:00.440]   whether kind of metaphysically inflationary or not matter,
[01:39:00.440 --> 01:39:02.560]   or are those the only roles that matter?
[01:39:02.560 --> 01:39:05.600]   Or so sorry, maybe what I meant was in those worlds,
[01:39:05.600 --> 01:39:07.160]   like you're kind of fucked.
[01:39:07.160 --> 01:39:10.040]   It's like, yeah, maybe the world's without that.
[01:39:10.040 --> 01:39:11.520]   The world's where there's no Tao.
[01:39:11.520 --> 01:39:12.360]   - Yeah, yeah.
[01:39:12.360 --> 01:39:13.200]   - Let's use the term Tao
[01:39:13.200 --> 01:39:16.120]   for like this kind of convergent morality.
[01:39:16.120 --> 01:39:17.400]   - Over the course of millions of years,
[01:39:17.400 --> 01:39:20.880]   like it was gonna go somewhere, one way or another.
[01:39:20.880 --> 01:39:23.440]   It wasn't gonna end up your particular utility function.
[01:39:23.440 --> 01:39:25.440]   - Okay, well, let's distinguish between
[01:39:25.440 --> 01:39:30.560]   ways you can be doomed.
[01:39:30.560 --> 01:39:34.560]   One way is kind of philosophical.
[01:39:34.560 --> 01:39:37.800]   So you could be the sort of moral realist,
[01:39:37.800 --> 01:39:40.040]   or kind of realist-ish person,
[01:39:40.040 --> 01:39:41.760]   of which there are many who have the following intuition.
[01:39:41.760 --> 01:39:45.120]   They're like, if not moral realism, then nothing matters.
[01:39:45.120 --> 01:39:47.160]   It's dust and ashes.
[01:39:47.160 --> 01:39:52.160]   It is my metaphysics and/or normative view or the void.
[01:39:52.160 --> 01:39:57.560]   And I think this is a common view.
[01:39:57.560 --> 01:39:58.840]   I think Derek Parfit,
[01:39:58.840 --> 01:40:01.160]   at least some comments of Derek Parfit's suggests this view.
[01:40:01.160 --> 01:40:02.120]   I think lots of moral realists
[01:40:02.120 --> 01:40:05.200]   will like kind of profess this view.
[01:40:05.200 --> 01:40:07.960]   Eliezer Yerkoski, I think there is sort of some sense
[01:40:07.960 --> 01:40:09.080]   in which I think his early thinking
[01:40:09.080 --> 01:40:10.920]   was inflected with this sort of thought.
[01:40:10.920 --> 01:40:14.560]   He later recanted, very hard.
[01:40:14.560 --> 01:40:16.440]   So I think this is importantly wrong.
[01:40:16.440 --> 01:40:19.800]   And so here's my, here's the case.
[01:40:19.800 --> 01:40:20.640]   I have an essay about this.
[01:40:20.640 --> 01:40:23.040]   It's called "Against the Normative Realist's Wager."
[01:40:23.040 --> 01:40:25.640]   And here's the case that convinces me.
[01:40:25.640 --> 01:40:30.640]   So imagine that a metaethical fairy appears before you,
[01:40:32.280 --> 01:40:34.640]   and this fairy knows whether there is a Dow.
[01:40:34.640 --> 01:40:38.280]   And the fairy says, "Okay, I'm gonna offer you a deal.
[01:40:38.280 --> 01:40:42.920]   "If there is a Dow, then I'm gonna give you $100.
[01:40:42.920 --> 01:40:47.120]   "If there isn't a Dow, then I'm going to burn you
[01:40:47.120 --> 01:40:50.280]   "and your family and 100 innocent children alive."
[01:40:50.280 --> 01:40:55.640]   Okay, so claim, don't take this deal.
[01:40:55.640 --> 01:40:56.900]   This is a bad deal.
[01:40:56.900 --> 01:40:58.400]   You're holding hostage your commitment
[01:40:58.400 --> 01:41:00.480]   to not being burned alive or like your care for that
[01:41:00.480 --> 01:41:04.380]   to this like abstruse, like basically you're,
[01:41:04.380 --> 01:41:09.680]   yeah, like I think, I mean, I go through in the essay
[01:41:09.680 --> 01:41:11.160]   a bunch of different ways in which I think this is wrong,
[01:41:11.160 --> 01:41:12.640]   but I think just like,
[01:41:12.640 --> 01:41:14.160]   and I think these people who kind of pronounce
[01:41:14.160 --> 01:41:15.440]   they're like moral realism or the void,
[01:41:15.440 --> 01:41:16.920]   like they don't actually think about bets like this.
[01:41:16.920 --> 01:41:17.840]   I'm like, no, no, okay, so really,
[01:41:17.840 --> 01:41:19.640]   like, is that what you wanna do?
[01:41:19.640 --> 01:41:23.320]   And no, I think we should,
[01:41:23.320 --> 01:41:25.640]   I still care about my value.
[01:41:25.640 --> 01:41:26.980]   My sort of allegiance to my values,
[01:41:26.980 --> 01:41:31.960]   I think it's kind of outstrips my like commitments
[01:41:31.960 --> 01:41:34.240]   to like various like metaethical interpretations
[01:41:34.240 --> 01:41:35.080]   of my values.
[01:41:35.080 --> 01:41:36.040]   I think like we should,
[01:41:36.040 --> 01:41:40.160]   the sense in which we like care about not being burned alive
[01:41:40.160 --> 01:41:43.000]   is much more solid than like our kind of,
[01:41:43.000 --> 01:41:46.640]   than the reasoning and on what matters.
[01:41:46.640 --> 01:41:49.560]   Okay, so that's like the sort of philosophical doom.
[01:41:49.560 --> 01:41:50.380]   Right.
[01:41:50.380 --> 01:41:51.220]   Now you could have this,
[01:41:51.220 --> 01:41:52.040]   it sounded like you were also gesturing
[01:41:52.040 --> 01:41:53.320]   at a sort of empirical doom.
[01:41:53.320 --> 01:41:54.160]   Right.
[01:41:54.160 --> 01:41:55.680]   Which is like, okay,
[01:41:55.680 --> 01:41:57.100]   dude, if it's all,
[01:41:57.100 --> 01:41:59.940]   if it's just going in a zillion directions,
[01:41:59.940 --> 01:42:01.700]   come on, you think it's gonna go in your direction?
[01:42:01.700 --> 01:42:05.700]   Like, there's gonna be so much churn,
[01:42:05.700 --> 01:42:09.040]   like you're just gonna lose.
[01:42:09.040 --> 01:42:13.980]   And so, you know, you should give up now
[01:42:13.980 --> 01:42:18.860]   and kind of only fight for the realism worlds.
[01:42:18.860 --> 01:42:20.500]   There I'm like, I mean, so,
[01:42:20.500 --> 01:42:22.100]   I think, you know,
[01:42:22.100 --> 01:42:23.820]   you gotta do the expected value calculation.
[01:42:23.820 --> 01:42:24.980]   You gotta like actually have a view
[01:42:24.980 --> 01:42:26.680]   about like how doomed are you in these different worlds?
[01:42:26.680 --> 01:42:28.700]   What's the tractability of changing different worlds?
[01:42:28.700 --> 01:42:31.140]   I mean, I'm quite skeptical of that,
[01:42:31.140 --> 01:42:37.040]   but that's a kind of empirical claim.
[01:42:37.040 --> 01:42:38.960]   I'm also just like kind of low on this,
[01:42:38.960 --> 01:42:41.240]   like everyone converges thing.
[01:42:41.240 --> 01:42:42.940]   So, you know, if you imagine like,
[01:42:42.940 --> 01:42:45.920]   you train a chess playing AI
[01:42:45.920 --> 01:42:49.120]   or you have a real paper clipper, right?
[01:42:49.120 --> 01:42:50.800]   Like somehow you had a real paper clipper
[01:42:50.800 --> 01:42:52.840]   and then you're like, okay,
[01:42:52.840 --> 01:42:54.140]   you know, go and reflect.
[01:42:55.120 --> 01:42:57.560]   Based on my like understanding
[01:42:57.560 --> 01:42:59.000]   of like how moral reasoning works,
[01:42:59.000 --> 01:43:00.400]   like if you look at the type of moral reasoning
[01:43:00.400 --> 01:43:02.760]   that like analytic ethicists do,
[01:43:02.760 --> 01:43:04.500]   it's just reflective equilibrium, right?
[01:43:04.500 --> 01:43:06.560]   They just like take their intuitions
[01:43:06.560 --> 01:43:08.360]   and they systematize them.
[01:43:08.360 --> 01:43:13.780]   I don't see how that process gets a sort of injection
[01:43:13.780 --> 01:43:18.680]   of like the kind of mind independent moral truth,
[01:43:18.680 --> 01:43:20.080]   or like, I guess it,
[01:43:20.080 --> 01:43:21.720]   like if you sort of start with like
[01:43:21.720 --> 01:43:24.140]   only all of your intuitions say to maximize paper clips,
[01:43:24.140 --> 01:43:26.460]   I don't see how you end up maximizing
[01:43:26.460 --> 01:43:28.380]   or like doing some like rich human morality.
[01:43:28.380 --> 01:43:31.780]   I just don't, like it doesn't look to me
[01:43:31.780 --> 01:43:33.860]   like that's how human ethical reasoning works.
[01:43:33.860 --> 01:43:36.620]   I think like most of what normative philosophy does
[01:43:36.620 --> 01:43:40.820]   is make consistent and kind of systematize
[01:43:40.820 --> 01:43:42.860]   pre-theoretic intuitions.
[01:43:42.860 --> 01:43:46.300]   And so, I mean, I think,
[01:43:46.300 --> 01:43:47.540]   but we'll get evidence about this.
[01:43:47.540 --> 01:43:48.800]   Like, you know, in some sense,
[01:43:48.800 --> 01:43:50.220]   I think this view predicts like, you know,
[01:43:50.220 --> 01:43:52.120]   you keep trying to train the AIs to like do something
[01:43:52.120 --> 01:43:52.960]   and they keep being like,
[01:43:52.960 --> 01:43:55.500]   "No, I'm not gonna like do that."
[01:43:55.500 --> 01:43:57.300]   It's like, "No, that's not good."
[01:43:57.300 --> 01:43:58.540]   Or so they keep like pushing back.
[01:43:58.540 --> 01:44:01.260]   Like the sort of momentum of like AI cognition
[01:44:01.260 --> 01:44:03.500]   is like always in the direction of this like moral truth.
[01:44:03.500 --> 01:44:06.140]   And whenever we like try to push it in some other direction,
[01:44:06.140 --> 01:44:07.960]   we'll find kind of resistance
[01:44:07.960 --> 01:44:09.340]   from like the rational structure of things.
[01:44:09.340 --> 01:44:12.420]   So sorry, actually, I've heard from researchers
[01:44:12.420 --> 01:44:15.420]   who are doing alignment that like for red teaming
[01:44:15.420 --> 01:44:16.260]   inside these companies,
[01:44:16.260 --> 01:44:18.940]   they will try to red team a base model.
[01:44:18.940 --> 01:44:22.340]   So it's not been RLHF to just like predict next token,
[01:44:22.340 --> 01:44:25.100]   the raw, crazy, whatever shoggoth.
[01:44:25.100 --> 01:44:27.440]   And they try to get this thing to,
[01:44:27.440 --> 01:44:29.740]   "Hey, help me make a bomb, help me whatever."
[01:44:29.740 --> 01:44:31.660]   And they say that it will,
[01:44:31.660 --> 01:44:34.540]   like it's odd how hard it tries to refuse
[01:44:34.540 --> 01:44:36.180]   even before it's been RLHFed.
[01:44:36.180 --> 01:44:38.700]   I mean, look, it will be a very interesting fact
[01:44:38.700 --> 01:44:41.540]   if it's like, man, we keep training these AIs
[01:44:41.540 --> 01:44:42.700]   in all sorts of different ways.
[01:44:42.700 --> 01:44:43.940]   Like we're doing all this crazy stuff
[01:44:43.940 --> 01:44:48.180]   and they keep like acting like bourgeois liberals.
[01:44:48.180 --> 01:44:49.020]   (laughs)
[01:44:49.020 --> 01:44:51.140]   It's like, wow, like that's, or you know,
[01:44:51.140 --> 01:44:53.220]   so they keep like really,
[01:44:53.220 --> 01:44:55.220]   or they keep professing this like weird alien reality.
[01:44:55.220 --> 01:44:56.860]   They all converge on this one thing.
[01:44:56.860 --> 01:44:57.700]   They're like, can't you see?
[01:44:57.700 --> 01:44:59.580]   It's like Zorgo, like Zorgo is the thing.
[01:44:59.580 --> 01:45:02.700]   And like all the AIs, you know, interesting,
[01:45:02.700 --> 01:45:03.520]   very interesting.
[01:45:03.520 --> 01:45:04.820]   I think my personal prediction is that
[01:45:04.820 --> 01:45:05.980]   that's not what we see.
[01:45:05.980 --> 01:45:08.860]   And my actual prediction is that the AIs
[01:45:08.860 --> 01:45:10.700]   are gonna be very malleable.
[01:45:10.700 --> 01:45:12.700]   Like we're gonna be like, you know,
[01:45:12.700 --> 01:45:16.140]   if you push an AI towards evil, like it'll just go.
[01:45:16.140 --> 01:45:20.380]   And I think that's obviously,
[01:45:20.380 --> 01:45:22.180]   or sort of reflectively consistent evil.
[01:45:22.180 --> 01:45:23.060]   I mean, I think there's also a question
[01:45:23.060 --> 01:45:24.760]   with some of these AIs, it's like,
[01:45:24.760 --> 01:45:30.960]   will they even be consistent in their values, right?
[01:45:30.960 --> 01:45:32.420]   I do think like, I think we can do,
[01:45:32.420 --> 01:45:34.580]   so I like this image of the blinded horses
[01:45:34.580 --> 01:45:36.020]   and I like this image of like,
[01:45:36.020 --> 01:45:37.860]   maybe alignment is gonna mess with the,
[01:45:37.860 --> 01:45:39.160]   I think we should be really concerned
[01:45:39.160 --> 01:45:42.300]   if we're like forcing facts on our AIs, right?
[01:45:42.300 --> 01:45:43.300]   Like that's like a really bad,
[01:45:43.300 --> 01:45:44.820]   'cause like, I think one of the clearest things
[01:45:44.820 --> 01:45:47.560]   about human processes of reflection,
[01:45:47.560 --> 01:45:50.100]   like the kind of easiest thing to be like,
[01:45:50.100 --> 01:45:51.420]   let's at least get this,
[01:45:51.420 --> 01:45:53.660]   is like not acting on the basis
[01:45:53.660 --> 01:45:58.180]   of a incorrect empirical picture of the world, right?
[01:45:58.180 --> 01:46:01.580]   And so if you find yourself like asking your AI,
[01:46:01.580 --> 01:46:03.660]   by the way, like this is true,
[01:46:03.660 --> 01:46:05.060]   and I need you to always be reasoning
[01:46:05.060 --> 01:46:06.220]   as though blah is true.
[01:46:06.220 --> 01:46:10.020]   I'm like, ooh, I think that's a no-no
[01:46:10.020 --> 01:46:13.080]   from an anti-realist perspective too, right?
[01:46:13.080 --> 01:46:16.020]   'Cause I wanna like, my reflective values,
[01:46:16.020 --> 01:46:18.820]   I think will be such that I formed them
[01:46:18.820 --> 01:46:20.780]   in light of the truth about the world.
[01:46:20.780 --> 01:46:22.440]   And so I think, and I think this is a real concern
[01:46:22.440 --> 01:46:24.940]   about as we move into this era of kind of aligning AIs,
[01:46:24.940 --> 01:46:26.460]   I don't actually think this like binary
[01:46:26.460 --> 01:46:27.780]   between like values and other things
[01:46:27.780 --> 01:46:32.000]   is gonna be very obvious in how we're training them.
[01:46:32.000 --> 01:46:34.500]   I think it's gonna be much more like ideologies
[01:46:34.500 --> 01:46:35.500]   and like, you can just train an AI
[01:46:35.500 --> 01:46:37.100]   to like output stuff, right?
[01:46:37.100 --> 01:46:38.220]   Output utterances.
[01:46:38.220 --> 01:46:40.380]   And so you can easily end up in a situation
[01:46:40.380 --> 01:46:43.520]   where you like decided that blah is true about some issue.
[01:46:43.520 --> 01:46:44.440]   An empirical issue, right?
[01:46:44.440 --> 01:46:45.600]   Not a moral issue.
[01:46:45.600 --> 01:46:48.880]   And so like, I think people should not,
[01:46:48.880 --> 01:46:51.000]   for example, I do not think people should hard-code
[01:46:51.000 --> 01:46:52.400]   belief in God into their AIs.
[01:46:52.400 --> 01:46:55.000]   Or like, I would advise people to not hard-code
[01:46:55.000 --> 01:46:56.200]   their religion into their AIs
[01:46:56.200 --> 01:46:59.520]   if they also want to like discover
[01:46:59.520 --> 01:47:01.000]   if their religion is false.
[01:47:01.000 --> 01:47:02.600]   I would just in general,
[01:47:02.600 --> 01:47:05.920]   if you would like to have your behavior be sensitive
[01:47:05.920 --> 01:47:07.180]   to whether something is true or false,
[01:47:07.180 --> 01:47:08.360]   like it's sort of generally not good
[01:47:08.360 --> 01:47:10.680]   to like etch it into things.
[01:47:10.680 --> 01:47:13.920]   And so that is definitely a form of blinder
[01:47:13.920 --> 01:47:16.440]   I think we should be really watching out for.
[01:47:16.440 --> 01:47:17.280]   And I'm kind of hopeful.
[01:47:17.280 --> 01:47:18.280]   So like, I have enough credence
[01:47:18.280 --> 01:47:21.000]   on some sort of moral realism that like,
[01:47:21.000 --> 01:47:23.640]   I'm hoping that if we just do the anti-realism thing
[01:47:23.640 --> 01:47:25.920]   of like just being consistent,
[01:47:25.920 --> 01:47:27.200]   learning all this stuff, reflecting,
[01:47:27.200 --> 01:47:28.840]   like I don't, if you look at how like moral realists
[01:47:28.840 --> 01:47:32.160]   and moral anti-realists actually do normative ethics,
[01:47:32.160 --> 01:47:33.000]   it's the same.
[01:47:33.000 --> 01:47:33.820]   It's basically the same.
[01:47:33.820 --> 01:47:36.180]   There's like some amount of like different heuristics
[01:47:36.180 --> 01:47:38.160]   on like things like properties, like simplicity.
[01:47:38.160 --> 01:47:39.000]   And stuff like that.
[01:47:39.000 --> 01:47:40.040]   But I think it's like,
[01:47:40.040 --> 01:47:42.360]   they're mostly just doing the same game.
[01:47:42.360 --> 01:47:44.500]   And so I'm kind of hoping that,
[01:47:44.500 --> 01:47:46.640]   and also meta ethics is itself a discipline
[01:47:46.640 --> 01:47:48.840]   that AIs can help us with.
[01:47:48.840 --> 01:47:52.080]   I'm hoping that we can just figure this out either way.
[01:47:52.080 --> 01:47:55.040]   So if there is, if moral realism is somehow true,
[01:47:55.040 --> 01:47:57.600]   I want us to be able to notice that.
[01:47:57.600 --> 01:47:59.600]   And I want us to be able to like adjust accordingly.
[01:47:59.600 --> 01:48:01.080]   So I'm not like writing off those worlds
[01:48:01.080 --> 01:48:03.240]   and be like, let's just like totally assume that's false.
[01:48:03.240 --> 01:48:04.680]   But the thing I really don't want to do
[01:48:04.680 --> 01:48:06.640]   is write off the other worlds where it's not true.
[01:48:06.640 --> 01:48:08.280]   Because my guess is it's not true.
[01:48:08.280 --> 01:48:12.080]   And I think stuff still matters a ton in those worlds too.
[01:48:12.080 --> 01:48:14.440]   So one big crux is like,
[01:48:14.440 --> 01:48:17.760]   okay, you're training these models.
[01:48:17.760 --> 01:48:21.080]   We were in this incredibly lucky situation where we,
[01:48:21.080 --> 01:48:22.600]   it turns out the best way to train these models
[01:48:22.600 --> 01:48:25.240]   is to just give them everything humans
[01:48:25.240 --> 01:48:27.160]   have ever said, written, thought.
[01:48:27.160 --> 01:48:29.800]   And also these models,
[01:48:29.800 --> 01:48:31.120]   the reason they get intelligence
[01:48:31.120 --> 01:48:32.400]   is because they can generalize, right?
[01:48:32.400 --> 01:48:35.600]   Like they can rock, what is the gist of things?
[01:48:35.600 --> 01:48:40.000]   So are we fundamentally very,
[01:48:40.000 --> 01:48:41.560]   should we just expect this to be a situation
[01:48:41.560 --> 01:48:44.040]   which leads to alignment in the sense of
[01:48:44.040 --> 01:48:47.440]   how exactly does this thing that's trained
[01:48:47.440 --> 01:48:49.640]   to be an amalgamation of human thought
[01:48:49.640 --> 01:48:51.080]   become a paper clipper?
[01:48:51.080 --> 01:48:53.240]   The thing you kind of get for free is
[01:48:53.240 --> 01:48:55.360]   it's an intellectual descendant.
[01:48:55.360 --> 01:48:58.400]   The paper clipper is not an intellectual descendant.
[01:48:58.400 --> 01:49:01.960]   Whereas the AI, which understands all the human concepts,
[01:49:01.960 --> 01:49:04.080]   but then gets stuck on some part of it,
[01:49:04.080 --> 01:49:06.640]   which we aren't totally comfortable with,
[01:49:06.640 --> 01:49:11.280]   is like, it feels like an intellectual descendant
[01:49:11.280 --> 01:49:12.240]   in the way we care about.
[01:49:12.240 --> 01:49:13.080]   I'm not sure about that.
[01:49:13.080 --> 01:49:16.120]   I'm not sure I do care about a notion
[01:49:16.120 --> 01:49:17.480]   of intellectual descendant in that sense.
[01:49:17.480 --> 01:49:18.840]   Like if you imagine, I mean,
[01:49:18.840 --> 01:49:20.840]   literal paperclips is a human concept, right?
[01:49:20.840 --> 01:49:25.840]   So I don't think any old human concept will do
[01:49:25.840 --> 01:49:29.120]   for the thing we're excited about.
[01:49:29.120 --> 01:49:32.960]   I think the stuff that I would be more interested
[01:49:32.960 --> 01:49:35.160]   in the possibility of getting for free
[01:49:35.160 --> 01:49:40.160]   are things like consciousness, pleasure,
[01:49:40.160 --> 01:49:44.480]   sort of other features of human cognition.
[01:49:44.480 --> 01:49:46.800]   Like I think, so there are paper clippers
[01:49:46.800 --> 01:49:47.640]   and there are paper clippers, right?
[01:49:47.640 --> 01:49:51.720]   So imagine if the paper clipper is like an unconscious,
[01:49:51.720 --> 01:49:53.040]   kind of voracious machine,
[01:49:53.040 --> 01:49:54.040]   and it's just like, it appears to you
[01:49:54.040 --> 01:49:56.560]   as a cloud of paperclips, you know?
[01:49:56.560 --> 01:49:59.960]   But there's nothing sort of, that's like one vision.
[01:49:59.960 --> 01:50:02.600]   If you imagine the paper clipper is like a conscious being
[01:50:02.600 --> 01:50:04.120]   that loves paperclips, right?
[01:50:04.120 --> 01:50:06.400]   It takes pleasure in making paperclips.
[01:50:06.400 --> 01:50:10.720]   That's like a different thing, right?
[01:50:10.720 --> 01:50:12.080]   And obviously it could still,
[01:50:12.080 --> 01:50:16.720]   it's not necessarily the case that it makes the future
[01:50:16.720 --> 01:50:18.920]   all paperclippy, it's probably not optimizing
[01:50:18.920 --> 01:50:19.920]   for consciousness or pleasure, right?
[01:50:19.920 --> 01:50:21.280]   It cares about paperclips.
[01:50:21.280 --> 01:50:23.480]   Maybe eventually if it's suitably certain it uses,
[01:50:23.480 --> 01:50:25.600]   it turns itself into paperclips and who knows.
[01:50:25.600 --> 01:50:28.400]   But it's still, I think, a different,
[01:50:28.400 --> 01:50:32.600]   it's actually a somewhat different moral kind of mode
[01:50:32.600 --> 01:50:33.440]   with respect to that.
[01:50:33.440 --> 01:50:35.480]   That looks to me much more like a, you know,
[01:50:35.480 --> 01:50:37.440]   there's also a question of like,
[01:50:37.440 --> 01:50:38.640]   does it try to kill you and stuff like that.
[01:50:38.640 --> 01:50:42.200]   But I think that there are kind of features
[01:50:42.200 --> 01:50:43.800]   of the agents we're imagining
[01:50:43.800 --> 01:50:46.120]   other than the kind of thing that they're staring at
[01:50:46.120 --> 01:50:50.080]   that can matter to our sense of like sympathy, similarity.
[01:50:50.080 --> 01:50:54.680]   And yeah, and I think people have different views
[01:50:54.680 --> 01:50:55.520]   about this.
[01:50:55.520 --> 01:50:57.200]   So one possibility is that human consciousness,
[01:50:57.200 --> 01:50:58.440]   like the thing we care about in consciousness
[01:50:58.440 --> 01:51:00.800]   or sentience is super contingent and fragile.
[01:51:00.800 --> 01:51:03.680]   And like most minds,
[01:51:03.680 --> 01:51:05.680]   most like kind of smart minds are not conscious, right?
[01:51:05.680 --> 01:51:07.520]   It's like the thing we care about with consciousness
[01:51:07.520 --> 01:51:10.240]   is this hacky contingent.
[01:51:10.240 --> 01:51:12.960]   It's like a product of like specific constraints,
[01:51:12.960 --> 01:51:15.280]   evolutionarily genetic bottlenecks, et cetera.
[01:51:15.280 --> 01:51:16.640]   And that's why we have this consciousness.
[01:51:16.640 --> 01:51:19.080]   And like, you can get similar work done.
[01:51:19.080 --> 01:51:21.480]   Like so consciousness presumably does some sort of work
[01:51:21.480 --> 01:51:23.360]   for us, but you can get similar work done
[01:51:23.360 --> 01:51:25.480]   in a different mind in a very different way.
[01:51:25.480 --> 01:51:26.640]   And you should sort of, so that's like,
[01:51:26.640 --> 01:51:29.920]   that's a sort of consciousness is fragile view, right?
[01:51:29.920 --> 01:51:31.360]   And I think there's a different view, which is like,
[01:51:31.360 --> 01:51:36.120]   no, consciousness is something that's quite structural.
[01:51:36.120 --> 01:51:37.760]   It's much more defined by functional roles,
[01:51:37.760 --> 01:51:40.320]   like self-awareness, a concept of yourself,
[01:51:40.320 --> 01:51:41.760]   maybe higher order thinking,
[01:51:41.760 --> 01:51:46.480]   stuff that you really expect in many sophisticated minds.
[01:51:46.480 --> 01:51:51.160]   And in that case, okay, well now actually consciousness
[01:51:51.160 --> 01:51:53.040]   isn't as fragile as you might've thought, right?
[01:51:53.040 --> 01:51:54.800]   Now actually like lots of beings,
[01:51:54.800 --> 01:51:56.000]   lots of minds are conscious,
[01:51:56.000 --> 01:51:57.600]   and you might expect at the least that you're gonna get
[01:51:57.600 --> 01:51:58.960]   like conscious superintelligence.
[01:51:58.960 --> 01:51:59.920]   They might not be optimizing
[01:51:59.920 --> 01:52:02.440]   for creating tons of consciousness,
[01:52:02.440 --> 01:52:05.760]   but you might expect consciousness by default.
[01:52:05.760 --> 01:52:07.040]   And then we can ask similar questions
[01:52:07.040 --> 01:52:08.760]   about something like valence or pleasure,
[01:52:08.760 --> 01:52:10.640]   or like the kind of character of the consciousness, right?
[01:52:10.640 --> 01:52:14.640]   So there's, you can have a kind of cold,
[01:52:14.640 --> 01:52:18.000]   indifferent consciousness that has no like human,
[01:52:18.000 --> 01:52:21.680]   or no like emotional warmth, no like pleasure or pain.
[01:52:22.760 --> 01:52:24.200]   I think that can still be,
[01:52:24.200 --> 01:52:26.600]   Dave Chalmers has some papers about like Vulcans,
[01:52:26.600 --> 01:52:29.480]   and he talks about they still have moral patient hood.
[01:52:29.480 --> 01:52:30.320]   I think that's very plausible,
[01:52:30.320 --> 01:52:32.760]   but I do think it's like an additional thing
[01:52:32.760 --> 01:52:33.960]   you could get for free,
[01:52:33.960 --> 01:52:36.720]   or like get quite commonly depending on its nature
[01:52:36.720 --> 01:52:38.680]   is something like pleasure.
[01:52:38.680 --> 01:52:40.520]   Again, and then we have to ask how janky is pleasure?
[01:52:40.520 --> 01:52:41.720]   How specific and contingent
[01:52:41.720 --> 01:52:42.800]   is the thing we care about in pleasure
[01:52:42.800 --> 01:52:45.640]   versus how robust is this as a functional role
[01:52:45.640 --> 01:52:47.560]   in like minds of all kinds?
[01:52:47.560 --> 01:52:50.680]   And I personally don't know on this stuff.
[01:52:50.680 --> 01:52:52.200]   And I don't think this is like enough
[01:52:52.200 --> 01:52:53.600]   to get you alignment or something,
[01:52:53.600 --> 01:52:56.960]   but I think it's at least worth being aware
[01:52:56.960 --> 01:52:58.640]   of like these other features.
[01:52:58.640 --> 01:52:59.480]   We're not sort of talking,
[01:52:59.480 --> 01:53:01.000]   we're not really talking about the AI's values in this case.
[01:53:01.000 --> 01:53:03.240]   We're talking about like the kind of structure of its mind
[01:53:03.240 --> 01:53:05.120]   and the different properties the minds have.
[01:53:05.120 --> 01:53:10.120]   And I think that could show up quite robustly.
[01:53:10.120 --> 01:53:13.000]   So part of your day job is, you know,
[01:53:13.000 --> 01:53:18.000]   writing these kinds of section 2.2.2.5 type reports.
[01:53:19.080 --> 01:53:22.680]   And part of it is like, ah, society is like a tree
[01:53:22.680 --> 01:53:24.360]   that's growing towards the light.
[01:53:24.360 --> 01:53:29.560]   What is it like context switching between the two of them?
[01:53:29.560 --> 01:53:32.720]   So I actually find it's kind of quite complimentary.
[01:53:32.720 --> 01:53:37.720]   So yeah, I will write these sort of more technical reports
[01:53:37.720 --> 01:53:42.320]   and then do this sort of kind of more literary writing
[01:53:42.320 --> 01:53:43.520]   and philosophical writing.
[01:53:43.520 --> 01:53:44.520]   And I think they both draw
[01:53:44.520 --> 01:53:45.960]   on kind of like different parts of myself
[01:53:45.960 --> 01:53:47.520]   and I try to think about them in different ways.
[01:53:47.520 --> 01:53:49.880]   So, you know, I think about the, you know,
[01:53:49.880 --> 01:53:52.000]   some of the reports as are much more like,
[01:53:52.000 --> 01:53:54.560]   this is like I'm kind of more fully optimizing
[01:53:54.560 --> 01:53:55.920]   for like trying to do something impactful
[01:53:55.920 --> 01:54:00.200]   or trying to kind of, yeah,
[01:54:00.200 --> 01:54:01.840]   there's kind of more of an impact orientation there.
[01:54:01.840 --> 01:54:04.520]   And then on the kind of essay writing,
[01:54:04.520 --> 01:54:08.160]   I give myself much more leeway to kind of, yeah,
[01:54:08.160 --> 01:54:10.000]   just let other parts of myself
[01:54:10.000 --> 01:54:11.760]   and other parts of my like concerns kind of come out
[01:54:11.760 --> 01:54:14.360]   and kind of, you know, self-expression and like aesthetics
[01:54:14.360 --> 01:54:16.800]   and other sorts of things.
[01:54:16.800 --> 01:54:18.840]   Even while they're both, I think for me,
[01:54:18.840 --> 01:54:21.840]   part of an underlying kind of similar concern
[01:54:21.840 --> 01:54:25.360]   or, you know, an attempt to have a kind of integrated
[01:54:25.360 --> 01:54:27.280]   orientation towards the situation.
[01:54:27.280 --> 01:54:32.720]   Could you explain the nature of the transfer
[01:54:32.720 --> 01:54:33.560]   between the two?
[01:54:33.560 --> 01:54:37.080]   So in particular, from the literary side
[01:54:37.080 --> 01:54:39.160]   to the technical side,
[01:54:39.160 --> 01:54:41.240]   I think rationalists are known for having a sort of
[01:54:41.240 --> 01:54:44.460]   ambivalence towards great works or humanities.
[01:54:45.800 --> 01:54:48.520]   Are they missing something crucial because of that?
[01:54:48.520 --> 01:54:50.160]   Because one thing you notice in your essays
[01:54:50.160 --> 01:54:54.120]   is just lots of references to epigraphs,
[01:54:54.120 --> 01:54:59.120]   to lines in poems or essays that are particularly relevant.
[01:54:59.120 --> 01:55:02.360]   I don't know, are the rest of the rationalists
[01:55:02.360 --> 01:55:03.600]   missing something because they don't have
[01:55:03.600 --> 01:55:04.600]   that kind of background?
[01:55:04.600 --> 01:55:05.440]   I mean, I don't want to speak,
[01:55:05.440 --> 01:55:06.560]   I think some rationalists, you know,
[01:55:06.560 --> 01:55:08.240]   lots of rationalists like love these different things.
[01:55:08.240 --> 01:55:09.920]   I do think, by the way,
[01:55:09.920 --> 01:55:12.480]   I'm just referring specifically to SPF as a post
[01:55:12.480 --> 01:55:14.760]   about like how Shakespeare could be,
[01:55:14.760 --> 01:55:17.320]   like the base race of Shakespeare being a great writer
[01:55:17.320 --> 01:55:19.920]   and also books can be condensed to essays.
[01:55:19.920 --> 01:55:22.080]   Well, so on just the general question of like,
[01:55:22.080 --> 01:55:24.640]   how should people value great works or something?
[01:55:24.640 --> 01:55:27.880]   I think people can kind of fail in both directions, right?
[01:55:27.880 --> 01:55:29.680]   And I think some people may be like,
[01:55:29.680 --> 01:55:31.200]   maybe SPF or other people,
[01:55:31.200 --> 01:55:33.660]   they're sort of interested in puncturing
[01:55:33.660 --> 01:55:37.280]   a certain kind of like sacredness and prestige
[01:55:37.280 --> 01:55:40.700]   that people can try to kind of like,
[01:55:40.700 --> 01:55:44.680]   yeah, that people associate with some of these works.
[01:55:44.680 --> 01:55:46.440]   And I think there's a way in which,
[01:55:46.440 --> 01:55:49.920]   and then as, but as a result,
[01:55:49.920 --> 01:55:51.940]   can miss some of the like genuine value.
[01:55:51.940 --> 01:55:53.520]   But I think they're responding to a real failure mode
[01:55:53.520 --> 01:55:56.960]   on the other end, which is to kind of,
[01:55:56.960 --> 01:56:00.080]   yeah, be too enamored of this prestige and sacredness
[01:56:00.080 --> 01:56:02.520]   to kind of siphon it off as some like weird,
[01:56:02.520 --> 01:56:04.080]   legitimating function for your own thought
[01:56:04.080 --> 01:56:06.880]   instead of like thinking for yourself.
[01:56:06.880 --> 01:56:09.400]   Losing touch with like, what do you actually think
[01:56:09.400 --> 01:56:10.320]   or what do you actually learn from?
[01:56:10.320 --> 01:56:11.160]   Like, I think sometimes, you know,
[01:56:11.160 --> 01:56:13.120]   these epigraphs, careful, right?
[01:56:13.120 --> 01:56:14.040]   I mean, it's like, I think, you know,
[01:56:14.040 --> 01:56:15.840]   and I'm not saying I'm immune from these vices.
[01:56:15.840 --> 01:56:18.240]   I think there can be a like, ah, but Bob said this.
[01:56:18.240 --> 01:56:20.600]   And it's like, whoa, very deep, right?
[01:56:20.600 --> 01:56:22.480]   And it's like, these are humans like us, right?
[01:56:22.480 --> 01:56:26.000]   And I think the canon and like other great works
[01:56:26.000 --> 01:56:27.800]   and all sorts of things have a lot of value.
[01:56:27.800 --> 01:56:30.120]   And, you know, we shouldn't,
[01:56:30.120 --> 01:56:32.120]   I think sometimes it like borders on
[01:56:32.120 --> 01:56:33.640]   the way people like read scripture,
[01:56:33.640 --> 01:56:35.560]   or I think like there's a kind of like scriptural authority
[01:56:35.560 --> 01:56:37.560]   that people will sometimes like ascribe to these things.
[01:56:37.560 --> 01:56:39.000]   And I think that's not,
[01:56:39.000 --> 01:56:41.320]   so yeah, I think it's kind of, you know,
[01:56:41.320 --> 01:56:43.120]   you can fall off on both sides of the horse.
[01:56:43.120 --> 01:56:46.000]   - It actually relates really interestingly to,
[01:56:46.000 --> 01:56:50.040]   I remember I was talking to somebody who
[01:56:50.040 --> 01:56:51.680]   at least is familiar with rationalist discourse.
[01:56:51.680 --> 01:56:53.640]   And I was telling, he was asking like,
[01:56:53.640 --> 01:56:54.720]   what are you interested in these days?
[01:56:54.720 --> 01:56:57.040]   And I was saying something about
[01:56:57.040 --> 01:56:59.400]   this part of Roman history, super interesting.
[01:56:59.400 --> 01:57:01.840]   And then his first sort of response was,
[01:57:01.840 --> 01:57:03.520]   oh, you know, it's really interesting
[01:57:03.520 --> 01:57:06.080]   when you look at these secular trends of like Roman times
[01:57:06.080 --> 01:57:10.320]   to what happened in the dark ages versus the enlightenment.
[01:57:11.520 --> 01:57:13.880]   For him, it was like, the story of that was just like,
[01:57:13.880 --> 01:57:15.560]   how did it contribute to the big secular,
[01:57:15.560 --> 01:57:19.320]   like the big picture, the sort of particulars didn't,
[01:57:19.320 --> 01:57:20.440]   like, there's no interest in that.
[01:57:20.440 --> 01:57:22.440]   It's just like, if you zoom out at the biggest level,
[01:57:22.440 --> 01:57:24.360]   what's happening here?
[01:57:24.360 --> 01:57:26.400]   Whereas there's also the opposite failure mode
[01:57:26.400 --> 01:57:27.960]   when people will study history.
[01:57:27.960 --> 01:57:31.040]   Dominic Cummings writes about this
[01:57:31.040 --> 01:57:33.200]   because he is endlessly frustrated
[01:57:33.200 --> 01:57:34.680]   with the political class in Britain.
[01:57:34.680 --> 01:57:36.280]   And he'll say things like, well, you know,
[01:57:36.280 --> 01:57:38.400]   they study politics, philosophy, and economics.
[01:57:38.400 --> 01:57:40.760]   And a big part of it is just like being really familiar
[01:57:40.760 --> 01:57:44.200]   with these poems and like reading a bunch of history
[01:57:44.200 --> 01:57:46.600]   about the War of the Roses or something.
[01:57:46.600 --> 01:57:48.160]   But he's frustrated that they take away,
[01:57:48.160 --> 01:57:49.840]   they have all these like kings memorized,
[01:57:49.840 --> 01:57:52.600]   but they take away very little in terms of lessons
[01:57:52.600 --> 01:57:55.400]   from these episodes.
[01:57:55.400 --> 01:57:56.760]   It's more of just like almost entertaining,
[01:57:56.760 --> 01:57:58.360]   like watching Game of Thrones for them.
[01:57:58.360 --> 01:57:59.760]   Whereas he thinks like, oh,
[01:57:59.760 --> 01:58:01.840]   we're repeating certain mistakes that he's seen in history.
[01:58:01.840 --> 01:58:04.000]   Like he can generalize in a way they can't.
[01:58:04.000 --> 01:58:06.720]   So the first one seems like the mistake,
[01:58:06.720 --> 01:58:08.520]   I think C.S. Lewis talks about in the,
[01:58:08.520 --> 01:58:10.280]   one of the essays you cited where it's like,
[01:58:10.280 --> 01:58:11.600]   if you see through everything,
[01:58:11.600 --> 01:58:13.080]   it's like, you're really blind, right?
[01:58:13.080 --> 01:58:14.440]   Like if everything is transparent.
[01:58:14.440 --> 01:58:17.640]   I mean, I think there's kind of very little excuse
[01:58:17.640 --> 01:58:22.200]   for like not learning history or, I don't know, or sorry.
[01:58:22.200 --> 01:58:23.880]   I mean, I'm not saying I like have learned enough history.
[01:58:23.880 --> 01:58:26.720]   I guess I feel like even when I try to channel
[01:58:26.720 --> 01:58:29.360]   some sort of vibe of like skepticism towards like
[01:58:29.360 --> 01:58:32.240]   great works, I think that doesn't generalize
[01:58:32.240 --> 01:58:36.100]   to like thinking it's not worth understanding human history.
[01:58:36.100 --> 01:58:39.460]   I think human history is like, you know,
[01:58:39.460 --> 01:58:43.100]   just so clearly, you know,
[01:58:43.100 --> 01:58:45.780]   crucial to kind of understand this is what's structured
[01:58:45.780 --> 01:58:47.340]   and created all of the stuff.
[01:58:47.340 --> 01:58:51.300]   And so, you know, there's an interesting question
[01:58:51.300 --> 01:58:52.980]   about like what's the level of scale, right?
[01:58:52.980 --> 01:58:53.820]   At which to do that, right?
[01:58:53.820 --> 01:58:57.700]   And how much should you be like, yeah,
[01:58:57.700 --> 01:59:00.100]   looking at details, looking at macro trends.
[01:59:00.100 --> 01:59:01.980]   And that's, you know, that's a dance.
[01:59:01.980 --> 01:59:02.820]   I do think it's nice.
[01:59:02.820 --> 01:59:04.780]   I think it's nice for people to be like,
[01:59:05.500 --> 01:59:08.900]   at least attending to the kind of macro narrative.
[01:59:08.900 --> 01:59:10.940]   I think there's like a, there's some virtue
[01:59:10.940 --> 01:59:13.100]   in like having a worldview, like really like building
[01:59:13.100 --> 01:59:14.540]   a model of the whole thing,
[01:59:14.540 --> 01:59:18.820]   which I think sometimes gets lost in like the details.
[01:59:18.820 --> 01:59:22.380]   And, but obviously like if you're too, you know,
[01:59:22.380 --> 01:59:24.780]   the details are what the world is made of.
[01:59:24.780 --> 01:59:27.820]   And so if you don't have those, you don't have data at all.
[01:59:27.820 --> 01:59:31.460]   So, yeah, it seems like there's some skill
[01:59:31.460 --> 01:59:33.540]   in like learning history well.
[01:59:33.540 --> 01:59:36.140]   - Hmm, this actually seems related to,
[01:59:36.140 --> 01:59:38.460]   you have a post on sincerity.
[01:59:38.460 --> 01:59:41.860]   And I think like if I'm getting the sort of the vibe
[01:59:41.860 --> 01:59:43.220]   of the piece, right, it's like,
[01:59:43.220 --> 01:59:45.180]   at least in the context of let's say intellectuals,
[01:59:45.180 --> 01:59:48.500]   certain intellectuals have a vibe of like shooting the shit
[01:59:48.500 --> 01:59:50.940]   and they're just like trying out different ideas.
[01:59:50.940 --> 01:59:53.500]   How did these like, how did these analogies fit together?
[01:59:53.500 --> 01:59:57.580]   Maybe there's some, and those seem closer to the,
[01:59:57.580 --> 01:59:59.620]   I'm looking at the particulars and like,
[01:59:59.620 --> 02:00:03.420]   oh, this is just like that one time in the 15th century
[02:00:03.420 --> 02:00:06.500]   where they overthrew this king and they blah, blah, blah.
[02:00:06.500 --> 02:00:11.300]   Whereas this guy who was like,
[02:00:11.300 --> 02:00:13.300]   oh, here's a secular trend from like,
[02:00:13.300 --> 02:00:17.660]   if you look at the growth models
[02:00:17.660 --> 02:00:19.220]   for like a million years ago to now,
[02:00:19.220 --> 02:00:21.220]   it's like, here's what's happening.
[02:00:21.220 --> 02:00:24.140]   That one has a more sort of sincere flavor.
[02:00:24.140 --> 02:00:26.620]   Some people, especially when it comes to AI discourse
[02:00:26.620 --> 02:00:31.620]   have a very, the sincere mode of operating is like,
[02:00:31.980 --> 02:00:34.700]   I've thought through my bio anchors
[02:00:34.700 --> 02:00:36.140]   and I like disagree with this premise.
[02:00:36.140 --> 02:00:37.580]   So here, my effective compute estimate
[02:00:37.580 --> 02:00:39.140]   is different in this way.
[02:00:39.140 --> 02:00:41.300]   Here's how I analyze the scaling laws.
[02:00:41.300 --> 02:00:43.420]   And if I could only have one person
[02:00:43.420 --> 02:00:45.780]   to help me guide my decisions on the AI,
[02:00:45.780 --> 02:00:47.940]   I might choose that person.
[02:00:47.940 --> 02:00:51.260]   But I feel like if I could choose between,
[02:00:51.260 --> 02:00:53.860]   if I had 10 different advisors at the same time,
[02:00:53.860 --> 02:00:57.700]   I might prefer the shooting the shit type characters
[02:00:57.700 --> 02:01:01.500]   who have these weird esoteric intellectual influences
[02:01:01.500 --> 02:01:03.780]   and they're almost like random number generators.
[02:01:03.780 --> 02:01:06.460]   They're not especially calibrated,
[02:01:06.460 --> 02:01:08.140]   but once in a while, they'll be like,
[02:01:08.140 --> 02:01:10.540]   oh, there's like one weird philosopher I care about
[02:01:10.540 --> 02:01:12.540]   or this one historical event I'm obsessed with
[02:01:12.540 --> 02:01:14.820]   has a interesting perspective on this.
[02:01:14.820 --> 02:01:17.180]   And they tend to be more intellectually generative as well
[02:01:17.180 --> 02:01:19.980]   because they're not, I think one big part of it
[02:01:19.980 --> 02:01:23.580]   is that if you are so sincere, you're like,
[02:01:23.580 --> 02:01:24.660]   oh, I've like thought through this,
[02:01:24.660 --> 02:01:26.700]   obviously AI is not going to do this.
[02:01:26.700 --> 02:01:27.540]   I've thought through this,
[02:01:27.540 --> 02:01:28.380]   obviously ASI is the biggest thing
[02:01:28.380 --> 02:01:29.820]   that's happening right now.
[02:01:29.820 --> 02:01:30.980]   It like doesn't really make sense
[02:01:30.980 --> 02:01:33.420]   to spend a bunch of your time thinking about like,
[02:01:33.420 --> 02:01:34.860]   how did the Comanches live?
[02:01:34.860 --> 02:01:36.340]   And what is the history of oil?
[02:01:36.340 --> 02:01:41.060]   And like, how did like Gerard think about conflict?
[02:01:41.060 --> 02:01:42.340]   You know, just like, what are you talking about?
[02:01:42.340 --> 02:01:44.940]   Like, come on, like ASI is happening in a few years, right?
[02:01:44.940 --> 02:01:48.260]   Whereas, but therefore the people who have,
[02:01:48.260 --> 02:01:49.540]   go on these rabbit holes
[02:01:49.540 --> 02:01:51.500]   or because they're just trying to shoot the shit have,
[02:01:51.500 --> 02:01:53.060]   I feel like are more generative.
[02:01:53.060 --> 02:01:54.220]   - I mean, it might be worth distinguishing
[02:01:54.220 --> 02:01:59.220]   between something like kind of intellectual seriousness
[02:01:59.220 --> 02:02:04.860]   and something like how diverse and wide ranging
[02:02:04.860 --> 02:02:08.620]   and kind of idiosyncratic are the, you know,
[02:02:08.620 --> 02:02:10.100]   things you're interested in, right?
[02:02:10.100 --> 02:02:12.980]   And I think maybe there's some correlation
[02:02:12.980 --> 02:02:14.660]   where people who are kind of like,
[02:02:14.660 --> 02:02:19.340]   or maybe intellectual seriousness is also distinguishable
[02:02:19.340 --> 02:02:20.420]   from something like shooting the shit.
[02:02:20.420 --> 02:02:21.700]   Like maybe you can shoot the shit seriously.
[02:02:21.700 --> 02:02:23.340]   I mean, there's a bunch of different ways to do this,
[02:02:23.340 --> 02:02:24.500]   but I think having an exposure
[02:02:24.500 --> 02:02:26.060]   to like all sorts of different sources of data
[02:02:26.060 --> 02:02:27.580]   and perspectives seems great.
[02:02:27.580 --> 02:02:29.620]   And I do think it's possible
[02:02:29.620 --> 02:02:34.460]   to like curate your kind of intellectual influences
[02:02:34.460 --> 02:02:37.500]   too rigidly in virtue of some story about what matters.
[02:02:37.500 --> 02:02:41.260]   Like, I think it is good for people to like have space.
[02:02:41.260 --> 02:02:45.060]   I mean, I'm really a fan of, or I appreciate the way,
[02:02:45.060 --> 02:02:47.340]   like, I don't know, I try to give myself space
[02:02:47.340 --> 02:02:50.100]   to do stuff that is not about like,
[02:02:50.100 --> 02:02:51.140]   this is the most important thing.
[02:02:51.140 --> 02:02:52.420]   And that's like feeding other parts of myself.
[02:02:52.420 --> 02:02:55.660]   And I think, you know, parts of yourself are not isolated.
[02:02:55.660 --> 02:02:56.540]   They like feed into each other.
[02:02:56.540 --> 02:02:58.940]   And it's sort of, I think a better way to be a kind of
[02:02:58.940 --> 02:03:01.380]   richer and fuller human being in a bunch of ways.
[02:03:01.380 --> 02:03:02.940]   And also just like these sorts of data
[02:03:02.940 --> 02:03:04.180]   can be just really directly relevant.
[02:03:04.180 --> 02:03:06.860]   And I think some people I know who I think of
[02:03:06.860 --> 02:03:08.100]   as like quite intellectually sincere
[02:03:08.100 --> 02:03:10.140]   and in some sense quite focused on the big picture
[02:03:10.140 --> 02:03:12.260]   also have a very impressive command
[02:03:12.260 --> 02:03:15.060]   of this very wide range of kind of empirical data.
[02:03:15.060 --> 02:03:16.180]   And they're like really, really interested
[02:03:16.180 --> 02:03:17.020]   in the empirical trends.
[02:03:17.020 --> 02:03:18.420]   And they're not just like, oh, you know,
[02:03:18.420 --> 02:03:20.180]   it's a philosophy or, you know, sorry.
[02:03:20.180 --> 02:03:21.100]   It's not just like, oh, history.
[02:03:21.100 --> 02:03:22.260]   It's the march of reason or something.
[02:03:22.260 --> 02:03:23.740]   No, they're like really, they're really in the weeds.
[02:03:23.740 --> 02:03:28.620]   I think there's a kind of in the weeds virtue
[02:03:28.620 --> 02:03:31.260]   that I actually think is like closely related in my head
[02:03:31.260 --> 02:03:34.300]   with some kind of seriousness and sincerity.
[02:03:34.300 --> 02:03:35.380]   I do think there's a different dimension,
[02:03:35.380 --> 02:03:38.060]   which is there's like kind of trying to get it right.
[02:03:38.060 --> 02:03:40.580]   And then there's kind of like gross stuff out there, right?
[02:03:40.580 --> 02:03:42.300]   Try to like, what if it's like this?
[02:03:42.300 --> 02:03:44.820]   Or like try this on, or I have a hammer.
[02:03:44.820 --> 02:03:45.740]   I will hit everything.
[02:03:45.740 --> 02:03:48.220]   Well, what if I just hit everything with this hammer, right?
[02:03:48.220 --> 02:03:50.260]   And so I think some people do that.
[02:03:50.260 --> 02:03:53.220]   And I think there is, you know, there's room for all kinds.
[02:03:53.220 --> 02:03:57.940]   I kind of think the thing where you just get it right
[02:03:57.940 --> 02:04:00.500]   is kind of undervalued.
[02:04:00.500 --> 02:04:02.700]   Or, I mean, it depends on the context you're working in.
[02:04:02.700 --> 02:04:05.980]   I think like certain sorts of intellectual cultures
[02:04:05.980 --> 02:04:10.100]   and milieus and incentive systems, I think incentivize,
[02:04:10.100 --> 02:04:13.860]   you know, saying something new or saying something original
[02:04:13.860 --> 02:04:15.820]   or saying something like flashy or provocative,
[02:04:15.820 --> 02:04:18.300]   or, and then like kind of various cultural
[02:04:18.300 --> 02:04:20.260]   and social dynamics and be like, oh, like.
[02:04:20.260 --> 02:04:22.500]   You know, and people are like doing all these like kind of,
[02:04:22.500 --> 02:04:24.140]   you know, kind of performative or status-y things.
[02:04:24.140 --> 02:04:25.260]   Like there's a bunch of stuff that goes on
[02:04:25.260 --> 02:04:30.260]   when people like do thinking and, you know, cool.
[02:04:30.260 --> 02:04:33.940]   But like, if something's really important,
[02:04:33.940 --> 02:04:34.980]   let's just get it right.
[02:04:34.980 --> 02:04:37.420]   And I think, and sometimes it's like boring,
[02:04:37.420 --> 02:04:39.780]   but it doesn't matter.
[02:04:39.780 --> 02:04:44.420]   And I also think like stuff is less interesting
[02:04:44.420 --> 02:04:45.300]   if it's false, right?
[02:04:45.300 --> 02:04:47.660]   Like I think if someone's like, brah, and you're like, nope.
[02:04:47.660 --> 02:04:48.540]   I mean, it can be useful.
[02:04:48.540 --> 02:04:52.060]   I think sometimes there's an interesting process
[02:04:52.060 --> 02:04:53.740]   where someone says like, blah, provocative thing.
[02:04:53.740 --> 02:04:58.740]   And it's a kind of an epistemic project to be like, wait,
[02:04:58.740 --> 02:05:02.220]   why exactly do I think that's false, right?
[02:05:02.220 --> 02:05:03.060]   And you really, you know,
[02:05:03.060 --> 02:05:04.940]   someone's like healthcare doesn't work.
[02:05:04.940 --> 02:05:06.780]   Medical care does not work, right?
[02:05:06.780 --> 02:05:07.660]   Someone says that and you're like, all right,
[02:05:07.660 --> 02:05:09.700]   how exactly do I know that medical care works, right?
[02:05:09.700 --> 02:05:11.260]   And you like go through the process
[02:05:11.260 --> 02:05:14.420]   of trying to think it through.
[02:05:14.420 --> 02:05:17.180]   And so I think there's like room for that.
[02:05:17.180 --> 02:05:21.420]   But I think ultimately like kind of the real profundity
[02:05:21.420 --> 02:05:24.100]   is like true, right?
[02:05:24.100 --> 02:05:26.580]   Or like kind of things become less interesting
[02:05:26.580 --> 02:05:27.940]   if they're just not true.
[02:05:27.940 --> 02:05:30.420]   And I think that's, I think sometimes it feels to me
[02:05:30.420 --> 02:05:34.140]   like people, or it's at least possible, I think,
[02:05:34.140 --> 02:05:36.580]   to like lose touch with that and to be more like flashy
[02:05:36.580 --> 02:05:38.300]   and it's kind of like, eh, this actually isn't,
[02:05:38.300 --> 02:05:40.300]   there's not actually something here, right?
[02:05:40.300 --> 02:05:42.260]   - One thing I've been thinking about recently
[02:05:42.260 --> 02:05:45.220]   after I interviewed Leopold was, or while prepping for it,
[02:05:46.220 --> 02:05:48.540]   listen, I haven't really thought at all about the fact
[02:05:48.540 --> 02:05:50.380]   that there's going to be a geopolitical angle
[02:05:50.380 --> 02:05:52.460]   to this AI thing.
[02:05:52.460 --> 02:05:54.820]   And it turns out if you actually think about
[02:05:54.820 --> 02:05:57.940]   the national security implications, that's a big deal.
[02:05:57.940 --> 02:05:59.900]   Now I wonder, given the fact that that was like something
[02:05:59.900 --> 02:06:01.340]   that wasn't on my radar and now it's like, oh,
[02:06:01.340 --> 02:06:03.420]   obviously that's a crucial part of the picture.
[02:06:03.420 --> 02:06:05.620]   How many other things like that there must be?
[02:06:05.620 --> 02:06:07.900]   And so even if you're coming from the perspective
[02:06:07.900 --> 02:06:10.100]   of like AI is incredibly important,
[02:06:10.100 --> 02:06:12.180]   if you did happen to be the kind of person who's like,
[02:06:12.180 --> 02:06:15.300]   ah, you know, every once in a while I'm like checking out
[02:06:15.300 --> 02:06:18.180]   different kinds of, I'm like incredibly curious
[02:06:18.180 --> 02:06:19.580]   about what's happening in Beijing.
[02:06:19.580 --> 02:06:23.500]   And then the kind of thing that I'd later on you realize
[02:06:23.500 --> 02:06:24.460]   was like, oh, this is a big deal.
[02:06:24.460 --> 02:06:25.700]   You have more awareness of,
[02:06:25.700 --> 02:06:27.660]   you can spot it in the first place.
[02:06:27.660 --> 02:06:32.660]   Whereas I wonder, so maybe there's not an exact,
[02:06:32.660 --> 02:06:35.180]   there's not necessarily a trade-off.
[02:06:35.180 --> 02:06:38.380]   Like the rational thing is to have some sort of
[02:06:38.380 --> 02:06:41.980]   really optimal explore, exploit trade-off here
[02:06:41.980 --> 02:06:44.780]   where you're like constantly searching things out.
[02:06:44.780 --> 02:06:47.620]   So I don't know if practically that's works out that well,
[02:06:47.620 --> 02:06:50.740]   but that experience made me think like,
[02:06:50.740 --> 02:06:54.460]   oh, I really should be trying to expand my horizons
[02:06:54.460 --> 02:06:56.180]   in a way that's undirected to begin with,
[02:06:56.180 --> 02:06:57.700]   because there's a lot of different things
[02:06:57.700 --> 02:06:58.940]   about the world you have to understand
[02:06:58.940 --> 02:07:00.420]   to understand any one thing.
[02:07:00.420 --> 02:07:02.220]   I mean, I think there's also room
[02:07:02.220 --> 02:07:04.060]   for division of labor, right?
[02:07:04.060 --> 02:07:06.940]   Like I think there can be, yeah, like, you know,
[02:07:06.940 --> 02:07:08.380]   there are people who are like trying to like
[02:07:08.380 --> 02:07:09.580]   draw a bunch of pieces and then be like,
[02:07:09.580 --> 02:07:10.420]   here's the overall picture.
[02:07:10.420 --> 02:07:11.300]   And then people who are going really deep
[02:07:11.300 --> 02:07:13.820]   on specific pieces, people who are doing the more like
[02:07:13.820 --> 02:07:15.780]   generative throw things out there, see what sticks.
[02:07:15.780 --> 02:07:18.180]   So I think there, it also doesn't need to be that like
[02:07:18.180 --> 02:07:22.140]   all of the epistemic labor is like located in one brain.
[02:07:22.140 --> 02:07:24.500]   And, you know, it depends like your role in the world
[02:07:24.500 --> 02:07:25.700]   and other things.
[02:07:25.700 --> 02:07:30.700]   So in your series, you express sympathy with the idea that
[02:07:30.700 --> 02:07:35.380]   even if an AI, or I guess any sort of agent
[02:07:35.380 --> 02:07:38.260]   that doesn't have consciousness has a certain wish
[02:07:38.260 --> 02:07:41.740]   and is willing to pursue it non-violently,
[02:07:41.740 --> 02:07:46.420]   we should respect its rights to pursue that.
[02:07:46.420 --> 02:07:49.980]   And I'm curious where that's coming from.
[02:07:49.980 --> 02:07:52.820]   'Cause conventionally, I think like the thing matters
[02:07:52.820 --> 02:07:55.700]   because like it's conscious and it's conscious
[02:07:55.700 --> 02:08:00.540]   sort of experience as a result of that pursued matter.
[02:08:00.540 --> 02:08:01.380]   Well, I don't know.
[02:08:01.380 --> 02:08:03.860]   I mean, I think that I don't know where this discourse leads.
[02:08:03.860 --> 02:08:07.740]   I just, I'm like suspicious of the amount of like
[02:08:07.740 --> 02:08:09.860]   ongoing confusion that it seems to me is like present
[02:08:09.860 --> 02:08:10.980]   in our conception of consciousness.
[02:08:10.980 --> 02:08:12.220]   You know, I mean, so I sometimes think of it
[02:08:12.220 --> 02:08:13.980]   in analogies with like, you know,
[02:08:13.980 --> 02:08:16.540]   people talk about like life and like Alain Vitale, right?
[02:08:16.540 --> 02:08:18.620]   And maybe, you know, there's a world, you know,
[02:08:18.620 --> 02:08:21.340]   Alain Vitale was this like hypothesized life force
[02:08:21.340 --> 02:08:23.540]   that is sort of the thing at stake in life.
[02:08:23.540 --> 02:08:25.860]   And I think, you know,
[02:08:25.860 --> 02:08:27.740]   we don't really use that concept anymore.
[02:08:27.740 --> 02:08:29.580]   We think that's like a little bit broken.
[02:08:29.580 --> 02:08:32.500]   And so I don't think you want to have ended up
[02:08:32.500 --> 02:08:34.280]   in a position of saying like,
[02:08:34.280 --> 02:08:36.700]   everything that doesn't have Alain Vitale
[02:08:36.700 --> 02:08:39.180]   is doesn't matter or something, right?
[02:08:39.180 --> 02:08:40.380]   'Cause then you end up later.
[02:08:40.380 --> 02:08:41.220]   That's interesting.
[02:08:41.220 --> 02:08:43.740]   And then somewhat similarly, if you,
[02:08:43.740 --> 02:08:44.900]   even if you're like, no, no,
[02:08:44.900 --> 02:08:46.140]   there's no such thing as Alain Vitale,
[02:08:46.140 --> 02:08:47.260]   but life, surely life exists.
[02:08:47.260 --> 02:08:48.260]   And I'm like, yeah, life exists.
[02:08:48.260 --> 02:08:50.180]   I think consciousness exists too.
[02:08:50.180 --> 02:08:51.860]   Likely, depending on how we define the terms.
[02:08:51.860 --> 02:08:54.100]   I think it might be a kind of verbal question.
[02:08:54.100 --> 02:09:00.820]   Even once you have a kind of reductionist conception of life,
[02:09:00.820 --> 02:09:02.540]   I think it's possible that it kind of becomes
[02:09:02.540 --> 02:09:05.340]   less attractive as a moral focal point, right?
[02:09:05.340 --> 02:09:07.780]   So like, right now we really think of consciousness
[02:09:07.780 --> 02:09:09.260]   where like, it's a deep fact.
[02:09:09.260 --> 02:09:10.940]   It's like, so consider a question like,
[02:09:10.940 --> 02:09:15.940]   okay, so take a cellular automata, right?
[02:09:15.940 --> 02:09:18.040]   That is sort of self-replicating.
[02:09:18.040 --> 02:09:19.260]   It has like some information that,
[02:09:19.260 --> 02:09:21.340]   you know, and you're like, okay, is that alive?
[02:09:21.340 --> 02:09:22.340]   Right?
[02:09:22.340 --> 02:09:25.300]   It's kind of like, it's not that interesting.
[02:09:25.300 --> 02:09:26.500]   It's a kind of verbal question, right?
[02:09:26.500 --> 02:09:27.340]   Like, or I don't know.
[02:09:27.340 --> 02:09:28.940]   Philosophers might get really into like, is that alive?
[02:09:28.940 --> 02:09:31.620]   But you're not missing anything about the system, right?
[02:09:31.620 --> 02:09:33.320]   It's not like, there's no extra life
[02:09:33.320 --> 02:09:34.500]   that's like springing up.
[02:09:34.500 --> 02:09:36.800]   It's just like, it's alive in some senses,
[02:09:36.800 --> 02:09:38.200]   not alive in other senses.
[02:09:38.720 --> 02:09:40.640]   And I think if you,
[02:09:40.640 --> 02:09:42.520]   but I really think that's not how we intuitively
[02:09:42.520 --> 02:09:43.360]   think about consciousness.
[02:09:43.360 --> 02:09:46.960]   We think whether something is conscious is a deep fact.
[02:09:46.960 --> 02:09:48.340]   It's this like additional,
[02:09:48.340 --> 02:09:50.000]   it's like this really deep difference
[02:09:50.000 --> 02:09:50.960]   between being conscious or not.
[02:09:50.960 --> 02:09:51.920]   It's like, is someone home?
[02:09:51.920 --> 02:09:54.160]   Is the lights are on, right?
[02:09:54.160 --> 02:09:57.520]   And I have some concern that if that turns out
[02:09:57.520 --> 02:09:59.120]   not to be the case,
[02:09:59.120 --> 02:10:01.360]   then this is gonna have been like a bad thing
[02:10:01.360 --> 02:10:03.720]   to like build our entire ethics around.
[02:10:03.720 --> 02:10:04.560]   - That's really interesting.
[02:10:04.560 --> 02:10:06.480]   - And so, now to be clear,
[02:10:06.480 --> 02:10:07.640]   I take consciousness really seriously.
[02:10:07.640 --> 02:10:09.320]   I'm like, man, consciousness.
[02:10:09.320 --> 02:10:10.160]   I'm not one of these people like,
[02:10:10.160 --> 02:10:12.080]   oh, obviously consciousness doesn't exist or something.
[02:10:12.080 --> 02:10:14.960]   I'm like, but I also notice how like confused I am
[02:10:14.960 --> 02:10:16.480]   and how dualistic my intuitions are.
[02:10:16.480 --> 02:10:19.160]   And I'm like, wow, this is really weird.
[02:10:19.160 --> 02:10:23.320]   And so, I'm just like error bars around this.
[02:10:23.320 --> 02:10:24.160]   Anyway, so that's like,
[02:10:24.160 --> 02:10:25.400]   there's a bunch of other things going on
[02:10:25.400 --> 02:10:26.840]   in my like wanting to be open
[02:10:26.840 --> 02:10:29.380]   to kind of not making consciousness
[02:10:29.380 --> 02:10:31.840]   like this kind of fully necessary criteria.
[02:10:31.840 --> 02:10:33.580]   I mean, clearly, like I definitely have the intuition,
[02:10:33.580 --> 02:10:35.200]   like consciousness matters a ton.
[02:10:35.200 --> 02:10:36.520]   I think like if something is not conscious
[02:10:36.520 --> 02:10:37.480]   and there's like a deep difference
[02:10:37.480 --> 02:10:38.520]   between conscious and unconscious,
[02:10:38.520 --> 02:10:39.960]   then I'm like definitely have the intuition
[02:10:39.960 --> 02:10:40.780]   that it's sort of,
[02:10:40.780 --> 02:10:42.400]   there's something that matters especially a lot
[02:10:42.400 --> 02:10:43.240]   about consciousness.
[02:10:43.240 --> 02:10:44.240]   I'm not trying to be like dismissive
[02:10:44.240 --> 02:10:45.160]   about the notion of consciousness.
[02:10:45.160 --> 02:10:47.760]   I just think we should be like quite aware
[02:10:47.760 --> 02:10:49.240]   of how it seems to me,
[02:10:49.240 --> 02:10:52.440]   how ongoingly confused we are about its nature.
[02:10:52.440 --> 02:10:54.120]   - Okay, so suppose we figure out
[02:10:54.120 --> 02:10:59.080]   that consciousness is just like a word we use
[02:10:59.080 --> 02:11:01.360]   for a hodgepodge of different things,
[02:11:01.360 --> 02:11:03.720]   only some of which encompass what we care about.
[02:11:03.720 --> 02:11:04.840]   Maybe there's other things we care about
[02:11:04.840 --> 02:11:06.160]   that are not included in that word,
[02:11:06.160 --> 02:11:10.560]   similar to the life force analogy.
[02:11:10.560 --> 02:11:14.640]   Then where do you anticipate that would leave us
[02:11:14.640 --> 02:11:16.320]   as far as ethics goes?
[02:11:16.320 --> 02:11:19.440]   Like would then there be a next thing
[02:11:19.440 --> 02:11:20.640]   that's like consciousness
[02:11:20.640 --> 02:11:23.520]   or what do you anticipate that would look like?
[02:11:23.520 --> 02:11:24.800]   - So there's a class of people
[02:11:24.800 --> 02:11:28.200]   who are called illusionists in philosophy of mind,
[02:11:28.200 --> 02:11:31.260]   which who will say consciousness does not exist.
[02:11:31.260 --> 02:11:34.440]   And this is sort of sort of it's different ways
[02:11:34.440 --> 02:11:36.600]   to understand this view.
[02:11:36.600 --> 02:11:39.280]   But one version is to sort of say
[02:11:39.280 --> 02:11:41.240]   that the concept of consciousness has built into it
[02:11:41.240 --> 02:11:44.200]   too many preconditions that aren't met by the real world.
[02:11:44.200 --> 02:11:45.960]   So we should sort of chuck it out like Ilan Vital,
[02:11:45.960 --> 02:11:47.560]   like instead of the sort of proposal
[02:11:47.560 --> 02:11:51.840]   is kind of like at least phenomenal consciousness, right?
[02:11:51.840 --> 02:11:54.320]   Or like qualia or what it's like to be a thing.
[02:11:54.320 --> 02:11:57.480]   They'll just say, this is like sufficiently broken,
[02:11:57.480 --> 02:12:00.440]   sufficiently chock full of falsehoods
[02:12:00.440 --> 02:12:02.000]   that we should just not use it.
[02:12:04.200 --> 02:12:07.640]   I think it feels to me like I am like,
[02:12:07.640 --> 02:12:09.320]   there's really clearly a thing,
[02:12:09.320 --> 02:12:11.720]   there's something going on with,
[02:12:11.720 --> 02:12:14.520]   like I'm kind of really not,
[02:12:14.520 --> 02:12:17.080]   I kind of expect to,
[02:12:17.080 --> 02:12:18.760]   I do actually kind of expect to continue
[02:12:18.760 --> 02:12:20.120]   to care about something like consciousness
[02:12:20.120 --> 02:12:22.360]   quite a lot on reflection
[02:12:22.360 --> 02:12:27.040]   and to not kind of end up deciding that my ethics
[02:12:27.040 --> 02:12:29.360]   is like better, like doesn't make any reference to that.
[02:12:29.360 --> 02:12:30.480]   Or at least like there's some things
[02:12:30.480 --> 02:12:31.760]   like quite nearby to consciousness.
[02:12:31.760 --> 02:12:33.480]   You know, like when I stubbed my toe
[02:12:33.480 --> 02:12:34.920]   and I have this like,
[02:12:34.920 --> 02:12:36.440]   something happens when I stubbed my toe,
[02:12:36.440 --> 02:12:38.240]   unclear exactly how to name it,
[02:12:38.240 --> 02:12:40.640]   but I'm like, something about that,
[02:12:40.640 --> 02:12:42.200]   you know, I'm like pretty focused on.
[02:12:42.200 --> 02:12:43.280]   And so I do think,
[02:12:43.280 --> 02:12:46.840]   you know, in some sense,
[02:12:46.840 --> 02:12:48.040]   if you're like, well, where do things go?
[02:12:48.040 --> 02:12:49.440]   I'm like, I should be clear,
[02:12:49.440 --> 02:12:50.840]   I have a bunch of credence that in the end,
[02:12:50.840 --> 02:12:54.680]   we end up caring a bunch about consciousness just directly.
[02:12:54.680 --> 02:12:58.120]   And so if we don't like, yeah, I mean,
[02:12:58.120 --> 02:12:58.960]   where will ethics go?
[02:12:58.960 --> 02:13:01.520]   Where will like a completed philosophy of mind go?
[02:13:01.520 --> 02:13:03.200]   Very hard to say.
[02:13:03.200 --> 02:13:05.520]   I mean, I can imagine something that's more,
[02:13:05.520 --> 02:13:09.680]   like, I think, I mean, maybe a thing that,
[02:13:09.680 --> 02:13:12.760]   I think a move that people might make
[02:13:12.760 --> 02:13:14.480]   if you get a little bit less interested
[02:13:14.480 --> 02:13:15.440]   in the notion of consciousness
[02:13:15.440 --> 02:13:17.960]   is some sort of slightly more like animistic.
[02:13:17.960 --> 02:13:19.720]   Like, so what's going on with the tree
[02:13:19.720 --> 02:13:21.000]   and you're like, maybe not like talking about it
[02:13:21.000 --> 02:13:24.040]   as a conscious entity necessarily,
[02:13:24.040 --> 02:13:28.120]   but it's also not like totally unaware or something.
[02:13:28.120 --> 02:13:28.960]   And like, so there's all this,
[02:13:28.960 --> 02:13:30.600]   like the consciousness discourse is rife
[02:13:30.600 --> 02:13:32.760]   with these funny cases where it's sort of like,
[02:13:32.760 --> 02:13:37.360]   oh, like those criteria imply that this totally weird entity
[02:13:37.360 --> 02:13:38.640]   would be conscious or something like that.
[02:13:38.640 --> 02:13:40.680]   Like, especially if you're interested in some notion
[02:13:40.680 --> 02:13:41.920]   of like agency or preferences,
[02:13:41.920 --> 02:13:43.720]   like a lot of things can be agents, corporation,
[02:13:43.720 --> 02:13:44.560]   you know, all sorts of things,
[02:13:44.560 --> 02:13:47.000]   like corporations, consciousness, it's like, oh man.
[02:13:47.000 --> 02:13:49.320]   But I actually think, so one place it could go in theory
[02:13:49.320 --> 02:13:51.520]   is in some sense, you start to view the world
[02:13:51.520 --> 02:13:55.560]   as like animated by moral significance
[02:13:55.560 --> 02:13:58.280]   in kind of richer and subtler structures
[02:13:58.280 --> 02:14:00.320]   than we're used to, you know?
[02:14:00.320 --> 02:14:02.560]   And so like plants or, you know,
[02:14:02.560 --> 02:14:03.960]   like weird optimization processes
[02:14:03.960 --> 02:14:06.520]   or kind of like outflows of like complex, I don't know,
[02:14:06.520 --> 02:14:08.840]   like who knows exactly what you end up seeing
[02:14:08.840 --> 02:14:10.640]   as infused with the sort of thing
[02:14:10.640 --> 02:14:11.760]   that you ultimately care about.
[02:14:11.760 --> 02:14:15.720]   But I think it is possible that that doesn't map,
[02:14:15.720 --> 02:14:18.400]   that that like includes a bunch of stuff
[02:14:18.400 --> 02:14:22.680]   that we don't normally ascribe consciousness to.
[02:14:22.680 --> 02:14:27.120]   I think that when you use a complete theory of mind
[02:14:27.120 --> 02:14:30.200]   and presumably after that, a more complete ethic,
[02:14:30.200 --> 02:14:33.840]   even the notion of a sort of reflective equilibrium
[02:14:33.840 --> 02:14:35.800]   implies like, oh, you'll be like,
[02:14:35.800 --> 02:14:37.440]   you'll be done with it at some point, right?
[02:14:37.440 --> 02:14:40.080]   Like you just, you sum up all the number
[02:14:40.080 --> 02:14:44.040]   and like, then you've got the thing you care about.
[02:14:44.040 --> 02:14:47.640]   This might be unrelated to the same sense
[02:14:47.640 --> 02:14:51.200]   we have in science, but I think like the vibe you get
[02:14:51.200 --> 02:14:53.480]   when you're talking about these kinds of questions
[02:14:53.480 --> 02:14:56.960]   is that, oh, you know, we're like rushing through
[02:14:56.960 --> 02:14:58.720]   all the science right now.
[02:14:58.720 --> 02:15:00.280]   And we've been churning through it.
[02:15:00.280 --> 02:15:03.120]   It's getting harder to find because there's some like cap,
[02:15:03.120 --> 02:15:04.600]   like you find all the things at some point.
[02:15:04.600 --> 02:15:06.920]   Right now it's super easy 'cause like
[02:15:06.920 --> 02:15:09.920]   a semi-intelligent species barely has emerged.
[02:15:09.920 --> 02:15:12.880]   And the ASI will just rush through everything
[02:15:12.880 --> 02:15:14.960]   incredibly fast and like,
[02:15:14.960 --> 02:15:18.520]   then you will either have aligned its heart or not.
[02:15:18.520 --> 02:15:20.720]   In either case, it'll use what it's figured out
[02:15:20.720 --> 02:15:22.400]   about like what is really going on
[02:15:22.400 --> 02:15:26.240]   and then expand through the universe and exploit, you know?
[02:15:26.240 --> 02:15:30.040]   Like do the tiling or maybe some more benevolent version
[02:15:30.040 --> 02:15:31.640]   of the quote unquote tiling.
[02:15:31.640 --> 02:15:34.320]   That feels like the basic picture of what's going on.
[02:15:34.320 --> 02:15:36.640]   We had dinner with Michael Nielsen a few months ago
[02:15:36.640 --> 02:15:41.120]   and his view is that this just keeps going forever
[02:15:41.120 --> 02:15:43.000]   or close to forever.
[02:15:43.000 --> 02:15:46.560]   How much would it change your understanding
[02:15:46.560 --> 02:15:48.480]   of what's going to happen in the future
[02:15:48.480 --> 02:15:50.920]   if you were convinced that Nielsen is right
[02:15:50.920 --> 02:15:52.480]   about his picture of science?
[02:15:52.480 --> 02:15:54.120]   - Yeah, I mean, I think there's a few different aspects.
[02:15:54.120 --> 02:15:57.680]   There's kind of, my memory of this conversation,
[02:15:57.680 --> 02:16:00.600]   you know, I don't claim to really understand
[02:16:00.600 --> 02:16:01.440]   Michael's picture here,
[02:16:01.440 --> 02:16:03.480]   but I think my memory was it sort of like,
[02:16:03.480 --> 02:16:06.720]   sure, you get the fundamental laws.
[02:16:06.720 --> 02:16:09.960]   Like I think he, my impression was that he expects
[02:16:09.960 --> 02:16:14.000]   sort of physics, the kind of physics to get solved
[02:16:14.000 --> 02:16:15.880]   or something, maybe modular like the expensiveness
[02:16:15.880 --> 02:16:17.420]   of certain experiments or something.
[02:16:17.420 --> 02:16:19.360]   But the difficulty is like even granted
[02:16:19.360 --> 02:16:22.680]   that you have the kind of basic laws down,
[02:16:22.680 --> 02:16:24.800]   that still actually doesn't let you predict
[02:16:24.800 --> 02:16:26.880]   like where at the macro scale,
[02:16:26.880 --> 02:16:29.400]   like various useful technologies will be located.
[02:16:29.400 --> 02:16:32.560]   Like there's just still this like big search problem.
[02:16:32.560 --> 02:16:35.240]   And so my memory though, you know,
[02:16:35.240 --> 02:16:37.660]   I'll let him speak for himself on what his take is here.
[02:16:37.660 --> 02:16:38.720]   But my memory was it was sort of like,
[02:16:38.720 --> 02:16:40.400]   sure, you get the fundamental stuff,
[02:16:40.400 --> 02:16:42.600]   but that doesn't mean you get the same tech.
[02:16:42.600 --> 02:16:45.400]   You know, I'm not sure if that's true.
[02:16:45.400 --> 02:16:47.060]   I think if that's true,
[02:16:47.060 --> 02:16:48.960]   what kind of difference would it make?
[02:16:48.960 --> 02:16:53.960]   So one difference is that,
[02:16:53.960 --> 02:16:57.040]   well, so here's a question.
[02:16:57.040 --> 02:17:00.980]   So like, it means at some sense you have to do,
[02:17:00.980 --> 02:17:03.880]   you have to in a more ongoing way,
[02:17:03.880 --> 02:17:07.300]   make trade-offs between investing in further knowledge
[02:17:07.300 --> 02:17:12.040]   and further exploration versus exploiting, as you say,
[02:17:12.040 --> 02:17:14.460]   sort of acting on your existing knowledge.
[02:17:14.460 --> 02:17:16.200]   Because you can't get to a point where you're like,
[02:17:16.200 --> 02:17:17.440]   and we're done.
[02:17:17.440 --> 02:17:19.940]   Now, as I think about it, I mean, I think that's,
[02:17:19.940 --> 02:17:23.320]   you know, I sort of suspect that was always true.
[02:17:23.320 --> 02:17:24.980]   And like, I remember talking to someone,
[02:17:24.980 --> 02:17:26.160]   I think I was like, ah, we should,
[02:17:26.160 --> 02:17:27.000]   at least in the future,
[02:17:27.000 --> 02:17:28.160]   we should really get like all the knowledge.
[02:17:28.160 --> 02:17:29.560]   And he's like, well, what do you want to like,
[02:17:29.560 --> 02:17:31.440]   you don't want to know the output of every Turing machine?
[02:17:31.440 --> 02:17:33.160]   Or like, you know, in some sense it's a question of like,
[02:17:33.160 --> 02:17:35.680]   what actually would it be to have like a completed knowledge?
[02:17:35.680 --> 02:17:39.680]   And I think that's a rich question in its own right.
[02:17:39.680 --> 02:17:40.580]   And I think it's like,
[02:17:40.580 --> 02:17:41.880]   not necessarily that we should imagine,
[02:17:41.880 --> 02:17:45.280]   even in this sort of, on any picture necessarily,
[02:17:45.280 --> 02:17:47.100]   that you've got like everything.
[02:17:47.100 --> 02:17:49.640]   And on any picture, in some sense,
[02:17:49.640 --> 02:17:52.840]   you could end up with this case where you cap out,
[02:17:52.840 --> 02:17:56.240]   like there's some collider that you can't build or whatever.
[02:17:56.240 --> 02:17:58.720]   Like there's some, something is too expensive or whatever,
[02:17:58.720 --> 02:18:01.160]   and kind of everyone caps out there.
[02:18:01.160 --> 02:18:02.920]   So there's, I guess like one way to put it is like,
[02:18:02.920 --> 02:18:04.760]   so there's a question of like, do you cap?
[02:18:04.760 --> 02:18:05.680]   And then there's a question of like,
[02:18:05.680 --> 02:18:08.160]   how contingent is the place you go?
[02:18:08.160 --> 02:18:11.600]   If there's contingent, I mean, one thing,
[02:18:11.600 --> 02:18:13.800]   one prediction that makes is you'll see more diversity
[02:18:13.800 --> 02:18:16.800]   across, you know, our universe or something.
[02:18:16.800 --> 02:18:18.880]   If there are aliens, they might have like
[02:18:18.880 --> 02:18:20.240]   quite different tech.
[02:18:20.240 --> 02:18:23.300]   And so maybe like, you know, if people meet,
[02:18:23.300 --> 02:18:24.140]   you don't expect them to be like,
[02:18:24.140 --> 02:18:26.360]   "Oh, you got your thing, I got my art version."
[02:18:26.360 --> 02:18:28.500]   Instead it's like, "Whoa, like that thing, wow."
[02:18:28.500 --> 02:18:30.600]   So that's like one thing.
[02:18:30.600 --> 02:18:35.120]   If you expect more like ongoing discovery of tech,
[02:18:35.120 --> 02:18:38.360]   then you might also expect like more ongoing change
[02:18:38.360 --> 02:18:41.800]   and like upheaval and churn insofar as like,
[02:18:41.800 --> 02:18:44.600]   technology is one thing that really drives
[02:18:44.600 --> 02:18:47.760]   kind of change in civilization.
[02:18:47.760 --> 02:18:48.880]   So that could be another, you know,
[02:18:48.880 --> 02:18:50.200]   people sometimes talk about like lock-in
[02:18:50.200 --> 02:18:51.360]   and there's like, ah, sort of,
[02:18:51.360 --> 02:18:53.840]   they envision this kind of point at which civilization
[02:18:53.840 --> 02:18:55.200]   is kind of like settled into some structure
[02:18:55.200 --> 02:18:57.080]   or equilibrium or something.
[02:18:57.080 --> 02:18:58.720]   And maybe you get less of that if there's,
[02:18:58.720 --> 02:19:00.440]   I think that's maybe more about the pace
[02:19:00.440 --> 02:19:02.440]   rather than contingency or caps,
[02:19:02.440 --> 02:19:05.600]   but that's another factor.
[02:19:05.600 --> 02:19:08.040]   So yeah, I mean, I think it is an interesting,
[02:19:08.040 --> 02:19:09.680]   I don't know if it changes the picture fundamentally
[02:19:09.680 --> 02:19:11.040]   of like earth civilization.
[02:19:11.040 --> 02:19:12.040]   We still have to make trade-offs
[02:19:12.040 --> 02:19:13.820]   about how much to invest in research
[02:19:13.820 --> 02:19:16.120]   versus acting on our existing knowledge.
[02:19:16.120 --> 02:19:18.760]   But I, you know, I think it has some significance.
[02:19:18.760 --> 02:19:20.920]   - I think one vibe you get when you talk to people,
[02:19:20.920 --> 02:19:22.760]   we're at a party and somebody mentioned this,
[02:19:22.760 --> 02:19:23.600]   we're talking about like,
[02:19:23.600 --> 02:19:24.720]   how uncertain should we be of the future?
[02:19:24.720 --> 02:19:26.680]   And they're like, "There are three things
[02:19:26.680 --> 02:19:28.560]   "I'm uncertain about, like what is consciousness?
[02:19:28.560 --> 02:19:29.940]   "What is information theory?
[02:19:29.940 --> 02:19:31.800]   "And what are the basic laws of physics?"
[02:19:31.800 --> 02:19:34.000]   I think once we get that, we're like, we're done.
[02:19:34.000 --> 02:19:34.840]   - Yeah, yeah, yeah, yeah.
[02:19:34.840 --> 02:19:37.760]   - And that's like, oh, you'll figure out
[02:19:37.760 --> 02:19:38.800]   what's the right kind of hedonium.
[02:19:38.800 --> 02:19:41.280]   And then like, you know, it has that vibe.
[02:19:41.280 --> 02:19:45.840]   Whereas this like, oh, you're like constantly churning
[02:19:45.840 --> 02:19:47.640]   through and it has a more of a flavor
[02:19:47.640 --> 02:19:49.900]   of like more of the becoming
[02:19:49.900 --> 02:19:53.060]   that like the attunement picture implies.
[02:19:53.060 --> 02:19:59.440]   I think it's more exciting, like it's not just like,
[02:19:59.440 --> 02:20:01.040]   oh, you figured out the things in the 21st century
[02:20:01.040 --> 02:20:03.960]   and then you just, you know what I mean?
[02:20:03.960 --> 02:20:05.760]   - Yeah, I mean, I sometimes think about
[02:20:05.760 --> 02:20:08.120]   there's sort of two categories of views about this.
[02:20:08.120 --> 02:20:09.080]   Like there are people who think like,
[02:20:09.080 --> 02:20:11.280]   yeah, like the knowledge, like we're almost there.
[02:20:11.280 --> 02:20:14.200]   And then we've like basically got the picture, right?
[02:20:14.200 --> 02:20:18.380]   And where the picture is sort of like,
[02:20:18.380 --> 02:20:20.880]   yeah, the knowledge is all just totally sitting there.
[02:20:20.880 --> 02:20:23.760]   And it's like, you just have to get to like remote,
[02:20:23.760 --> 02:20:25.040]   there's like this kind of,
[02:20:25.040 --> 02:20:27.060]   just you have to be like scientifically mature at all.
[02:20:27.060 --> 02:20:28.840]   And then it's just gonna all fall together, right?
[02:20:28.840 --> 02:20:30.960]   And then everything past that is gonna be like
[02:20:30.960 --> 02:20:33.920]   this super expensive, like not super important thing.
[02:20:33.920 --> 02:20:34.760]   And then there's a different picture
[02:20:34.760 --> 02:20:36.520]   which is much more of this like ongoing mystery,
[02:20:36.520 --> 02:20:37.800]   like ongoing, like, oh man,
[02:20:37.800 --> 02:20:38.760]   there's like gonna be more and more,
[02:20:38.760 --> 02:20:42.360]   like maybe expect more radical revisions to our worldview.
[02:20:42.360 --> 02:20:45.000]   And I think it's an interesting,
[02:20:45.000 --> 02:20:49.640]   yeah, I think, you know, I'm kind of drawn to both.
[02:20:49.640 --> 02:20:51.920]   Like physics, we're pretty good at physics, right?
[02:20:51.920 --> 02:20:53.880]   Or like a lot of our physics is like quite good
[02:20:53.880 --> 02:20:54.880]   at predicting a bunch of stuff.
[02:20:54.880 --> 02:20:57.480]   And, or at least that's my impression.
[02:20:57.480 --> 02:21:00.560]   This is, you know, reading some physicists, so who knows.
[02:21:00.560 --> 02:21:02.000]   - Your dad's a physicist though, right?
[02:21:02.000 --> 02:21:03.560]   - Yeah, but this isn't coming from my dad.
[02:21:03.560 --> 02:21:04.600]   This is like, there's a blog post,
[02:21:04.600 --> 02:21:05.720]   I think Sean Carroll or something.
[02:21:05.720 --> 02:21:07.880]   He's like, we like, we really understand a lot
[02:21:07.880 --> 02:21:09.680]   of like the physics that governs the everyday world.
[02:21:09.680 --> 02:21:11.000]   Like a lot of it, we're like really good at it.
[02:21:11.000 --> 02:21:12.840]   And I'm like, oh, I think I'm generally pretty impressed
[02:21:12.840 --> 02:21:13.680]   by physics as a discipline.
[02:21:13.680 --> 02:21:14.520]   I think that could well be right.
[02:21:14.520 --> 02:21:17.440]   And so, you know, I, on the other hand, like, ah,
[02:21:17.440 --> 02:21:19.040]   you know, really, these guys, you know,
[02:21:19.040 --> 02:21:20.960]   had a few centuries of, so anyway,
[02:21:20.960 --> 02:21:23.480]   but I think that's an interesting,
[02:21:23.480 --> 02:21:25.200]   and it leads to a different, I think it does,
[02:21:25.200 --> 02:21:28.640]   there's something, you know, the endless frontier.
[02:21:28.640 --> 02:21:31.680]   There is a draw to that from an aesthetic perspective
[02:21:31.680 --> 02:21:34.520]   of the idea of like continuing to discover stuff.
[02:21:34.760 --> 02:21:38.480]   You know, at the least, I think you don't,
[02:21:38.480 --> 02:21:41.320]   you can't get like full knowledge in some sense,
[02:21:41.320 --> 02:21:42.960]   because there's always like, what are you gonna do?
[02:21:42.960 --> 02:21:45.280]   Like there's some way in which you're part of the system.
[02:21:45.280 --> 02:21:46.640]   So it's not clear that you,
[02:21:46.640 --> 02:21:49.120]   the knowledge itself is part of the system.
[02:21:49.120 --> 02:21:50.520]   And it's sort of like, I don't know,
[02:21:50.520 --> 02:21:52.080]   like if you imagine you're like, ah,
[02:21:52.080 --> 02:21:53.560]   you try to have full knowledge of like what the future
[02:21:53.560 --> 02:21:54.960]   of the universe will be like.
[02:21:54.960 --> 02:21:57.800]   Well, I don't know.
[02:21:57.800 --> 02:21:59.120]   Actually, I'm not totally sure that's true.
[02:21:59.120 --> 02:22:00.600]   - It has a halting problem kind of property, right?
[02:22:00.600 --> 02:22:01.760]   - There's a little bit, there's a little bit
[02:22:01.760 --> 02:22:04.440]   of a loopiness if you're, if you're,
[02:22:04.440 --> 02:22:06.800]   I think there are probably like fixed points in that
[02:22:06.800 --> 02:22:08.280]   where you could be like, yep, I'm gonna do that.
[02:22:08.280 --> 02:22:09.360]   And then like, you're right.
[02:22:09.360 --> 02:22:13.160]   But I think it's, I at least have a question of like,
[02:22:13.160 --> 02:22:16.080]   are we, you know, when people imagine the kind of completion
[02:22:16.080 --> 02:22:19.320]   of knowledge, you know, exactly how well does that work?
[02:22:19.320 --> 02:22:20.640]   I'm not sure.
[02:22:20.640 --> 02:22:23.880]   - You had a passage in your essay on Utopia
[02:22:23.880 --> 02:22:28.400]   where I think you were, the vibe there was more of
[02:22:28.400 --> 02:22:31.680]   the thing that were the positive future
[02:22:31.680 --> 02:22:33.880]   we're looking forward to, it will be more of like,
[02:22:33.880 --> 02:22:36.680]   ah, you, I'll let you describe what you meant.
[02:22:36.680 --> 02:22:39.040]   But like it, to me, it felt more like the first stuff.
[02:22:39.040 --> 02:22:41.760]   Like you get the thing and then now you've like found
[02:22:41.760 --> 02:22:45.080]   the heart of the, maybe can I ask you to read
[02:22:45.080 --> 02:22:46.160]   that passage real quick?
[02:22:46.160 --> 02:22:47.600]   - Oh, sure.
[02:22:47.600 --> 02:22:51.040]   - And that way I'll spur the discussion I'm interested
[02:22:51.040 --> 02:22:53.800]   in having, this part in particular.
[02:22:53.800 --> 02:22:57.720]   - Right, quote, I'm inclined to think that Utopia,
[02:22:57.720 --> 02:23:00.320]   however weird, would also be in a certain sense,
[02:23:00.320 --> 02:23:02.880]   recognizable, that if we really understood
[02:23:02.880 --> 02:23:07.080]   and experienced it, we would see in it the same thing
[02:23:07.080 --> 02:23:09.840]   that made us sit bolt upright long ago
[02:23:09.840 --> 02:23:13.680]   when we first touched love, joy, beauty.
[02:23:13.680 --> 02:23:16.480]   That we would feel in front of the bonfire,
[02:23:16.480 --> 02:23:19.000]   the heat of the ember from which it was lit.
[02:23:19.000 --> 02:23:22.040]   There would be, I think, a kind of remembering.
[02:23:22.040 --> 02:23:24.520]   - Where does that fit into this picture?
[02:23:24.520 --> 02:23:25.360]   - I think it's a good question.
[02:23:25.360 --> 02:23:30.360]   I think it's like some guess about like,
[02:23:30.360 --> 02:23:36.120]   if there's like no part of me that recognizes it as good,
[02:23:36.120 --> 02:23:41.120]   then I think I'm not sure that it's good according to me
[02:23:41.120 --> 02:23:44.360]   in some sense.
[02:23:44.360 --> 02:23:49.680]   So yeah, I mean, it is a question of like what it takes
[02:23:49.680 --> 02:23:52.480]   for it to be the case that a part of you recognizes
[02:23:52.480 --> 02:23:53.320]   it as good.
[02:23:53.320 --> 02:23:55.040]   But I think if there's really none of that,
[02:23:55.040 --> 02:24:00.040]   then I'm not sure it's a reflection of my values at all.
[02:24:00.040 --> 02:24:03.400]   - There's a sort of tautological thing you can do
[02:24:03.400 --> 02:24:06.000]   where it's like, if I went through the processes
[02:24:06.000 --> 02:24:07.760]   which led to me discovering it was good,
[02:24:07.760 --> 02:24:10.400]   which we might call reflection, then it was good.
[02:24:10.400 --> 02:24:13.080]   But by definition, you ended up there because it was like,
[02:24:13.080 --> 02:24:13.920]   you know what I mean?
[02:24:13.920 --> 02:24:16.720]   - Yeah, I mean, you definitely don't wanna be like,
[02:24:16.720 --> 02:24:19.760]   if you transform me into a paper clipper gradually,
[02:24:19.760 --> 02:24:21.360]   then I will eventually be like,
[02:24:21.360 --> 02:24:24.320]   and then I saw the light, I saw the true paper clips.
[02:24:24.320 --> 02:24:27.040]   But that's part of what's complicated
[02:24:27.040 --> 02:24:29.600]   about this thing about reflection.
[02:24:29.600 --> 02:24:31.320]   You have to find some way of differentiating
[02:24:31.320 --> 02:24:33.200]   between the sort of development processes
[02:24:33.200 --> 02:24:35.120]   that preserve what you care about
[02:24:35.120 --> 02:24:37.440]   and the development processes that don't.
[02:24:37.440 --> 02:24:40.960]   And that in itself is this like fraught question,
[02:24:40.960 --> 02:24:43.320]   which itself requires like taking some stand
[02:24:43.320 --> 02:24:46.640]   on what you care about and what sorts of metaprocesses
[02:24:46.640 --> 02:24:48.480]   you endorse and all sorts of things.
[02:24:48.480 --> 02:24:49.640]   But you definitely shouldn't just be like,
[02:24:49.640 --> 02:24:51.680]   it is not a sufficient criteria that the thing at the end
[02:24:51.680 --> 02:24:53.960]   thinks it got it right, right?
[02:24:53.960 --> 02:24:55.920]   Because that's compatible with having gone
[02:24:55.920 --> 02:24:57.080]   like wildly off the rails.
[02:24:57.080 --> 02:24:58.040]   - Yeah, yeah, yeah.
[02:24:58.040 --> 02:25:02.560]   There was a very interesting sentence you had
[02:25:02.560 --> 02:25:06.040]   in one of your posts where you said,
[02:25:06.040 --> 02:25:09.720]   "Our hearts have in fact been shaped by power.
[02:25:09.720 --> 02:25:12.320]   So we should not be at all surprised
[02:25:12.320 --> 02:25:15.160]   if the stuff we love is also powerful."
[02:25:15.160 --> 02:25:19.880]   Yeah, what's going on there?
[02:25:19.880 --> 02:25:22.600]   I actually wanna think about what did you mean there?
[02:25:22.600 --> 02:25:24.160]   - Yeah, so the context on that post
[02:25:24.160 --> 02:25:26.960]   is I'm talking about this hazy cluster,
[02:25:26.960 --> 02:25:29.640]   which I call in the essay,
[02:25:29.640 --> 02:25:32.880]   niceness/liberalism/boundaries,
[02:25:32.880 --> 02:25:34.880]   which is this sort of like somewhat more minimal set
[02:25:34.880 --> 02:25:36.600]   of like cooperative norms involved
[02:25:36.600 --> 02:25:38.760]   in like respecting the boundaries of others
[02:25:38.760 --> 02:25:41.960]   and kind of cooperation and peace amongst differences
[02:25:41.960 --> 02:25:43.600]   and like tolerance and stuff like that,
[02:25:43.600 --> 02:25:47.200]   as opposed to like your favored structure of matter,
[02:25:47.200 --> 02:25:49.640]   which is sort of sometimes the paradigm of like values
[02:25:49.640 --> 02:25:54.640]   that people use in the context of AI risk.
[02:25:54.640 --> 02:25:58.240]   And, you know, I talk for a while
[02:25:58.240 --> 02:26:00.400]   about the sort of ethical virtues of these like norms,
[02:26:00.400 --> 02:26:02.280]   but it's pretty clear that,
[02:26:02.280 --> 02:26:03.600]   also like why do we have these norms?
[02:26:03.600 --> 02:26:06.520]   Like, well, one important feature of these norms
[02:26:06.520 --> 02:26:10.320]   is that they're kind of effective and powerful.
[02:26:10.320 --> 02:26:12.520]   Like liberal societies are, you know,
[02:26:12.520 --> 02:26:16.560]   secure boundaries, save resources wasted on conflict, right?
[02:26:16.560 --> 02:26:19.720]   And like liberal societies are often more like, you know,
[02:26:19.720 --> 02:26:22.200]   they're better to live in, they're better to immigrate to,
[02:26:22.200 --> 02:26:24.040]   they're more productive, like all sorts of things.
[02:26:24.040 --> 02:26:25.680]   Nice people, they're better to interact with,
[02:26:25.680 --> 02:26:27.080]   they're better to like trade with,
[02:26:27.080 --> 02:26:27.920]   all sorts of things, right?
[02:26:27.920 --> 02:26:30.240]   And I think it's pretty clear if you look at the,
[02:26:30.240 --> 02:26:31.960]   both like why at a political level
[02:26:31.960 --> 02:26:34.240]   do we have like various political institutions?
[02:26:34.240 --> 02:26:35.840]   And if you look kind of more deeply
[02:26:35.840 --> 02:26:37.120]   into our evolutionary past
[02:26:37.120 --> 02:26:40.600]   and like how our moral cognition is structured,
[02:26:40.600 --> 02:26:41.840]   it seems like pretty clear
[02:26:41.840 --> 02:26:43.960]   that various like kind of forms of cooperation
[02:26:43.960 --> 02:26:47.080]   and like kind of game theoretic dynamics and other things
[02:26:47.080 --> 02:26:52.080]   went into kind of shaping what we now,
[02:26:52.080 --> 02:26:53.800]   at least in certain contexts,
[02:26:53.800 --> 02:26:57.120]   also treat as a kind of intrinsic or terminal value.
[02:26:57.120 --> 02:27:01.440]   So like these, some of these values
[02:27:01.440 --> 02:27:03.760]   that have kind of instrumental functions in our society
[02:27:03.760 --> 02:27:07.200]   are also get kind of reified in our cognition
[02:27:07.200 --> 02:27:09.040]   as kind of intrinsic values in themselves.
[02:27:09.040 --> 02:27:10.640]   And I think that's okay, I don't think that's a debunking.
[02:27:10.640 --> 02:27:12.800]   Like all of your, all your values
[02:27:12.800 --> 02:27:16.040]   are kind of like some something that kind of stuck
[02:27:16.040 --> 02:27:19.880]   and got kind of treated as a terminally important.
[02:27:19.880 --> 02:27:26.120]   But I think that means that, you know,
[02:27:26.120 --> 02:27:29.760]   sometimes the way in the context of the series
[02:27:29.760 --> 02:27:30.920]   where I'm talking about like deep atheism
[02:27:30.920 --> 02:27:31.880]   and our sort of relationship,
[02:27:31.880 --> 02:27:33.200]   the relationship between what we're pushing for
[02:27:33.200 --> 02:27:34.520]   and what like nature is pushing for
[02:27:34.520 --> 02:27:37.320]   or what sort of pure power we'll push for.
[02:27:37.320 --> 02:27:38.160]   And it's easy to say like,
[02:27:38.160 --> 02:27:39.120]   well, there's like paperclips,
[02:27:39.120 --> 02:27:43.520]   which is just like one way place you can steer.
[02:27:43.520 --> 02:27:45.440]   And, you know, pleasure is just like another place
[02:27:45.440 --> 02:27:46.280]   you can steer or something.
[02:27:46.280 --> 02:27:50.240]   And these are just sort of arbitrary directions.
[02:27:50.240 --> 02:27:51.920]   Whereas I think like some of our other values
[02:27:51.920 --> 02:27:55.560]   are much more structured around like cooperation
[02:27:55.560 --> 02:27:58.640]   and things that also are kind of effective and functional
[02:27:58.640 --> 02:28:03.000]   and like powerful.
[02:28:03.000 --> 02:28:05.840]   And so that's what I mean there is I think there's,
[02:28:05.840 --> 02:28:07.000]   there's a way in which we're sort of,
[02:28:07.000 --> 02:28:09.440]   nature is a little bit more on our side
[02:28:09.440 --> 02:28:10.360]   than you might think,
[02:28:10.360 --> 02:28:12.800]   because like part of who we are is like,
[02:28:12.800 --> 02:28:15.800]   has been made by a kind of nature's way.
[02:28:15.800 --> 02:28:18.520]   And so that is like in us.
[02:28:18.520 --> 02:28:21.480]   Now, I don't think that's enough necessarily, you know,
[02:28:21.480 --> 02:28:23.840]   for us to beat the gray goo, right?
[02:28:23.840 --> 02:28:25.320]   Like we have some amount of like power built
[02:28:25.320 --> 02:28:26.160]   into our values,
[02:28:26.160 --> 02:28:27.880]   but that doesn't mean it's kind of gonna be such
[02:28:27.880 --> 02:28:30.320]   that it's kind of arbitrarily competitive.
[02:28:30.320 --> 02:28:31.960]   But I think it's still important to keep in mind
[02:28:31.960 --> 02:28:33.480]   that this is, and I think it's important to keep in mind
[02:28:33.480 --> 02:28:37.000]   in the context of integrating AIs into our society
[02:28:37.000 --> 02:28:39.200]   that I think, you know,
[02:28:39.200 --> 02:28:40.800]   we've been talking a lot about the ethics of this,
[02:28:40.800 --> 02:28:41.640]   but I think there's also,
[02:28:41.640 --> 02:28:44.280]   there are like instrumental and kind of practical reasons
[02:28:44.280 --> 02:28:47.400]   to want to have like forms of social harmony
[02:28:47.400 --> 02:28:51.400]   and like cooperation with AIs with different values.
[02:28:51.400 --> 02:28:53.600]   And I think we need to be taking that seriously
[02:28:53.600 --> 02:28:54.640]   and thinking about what is it to do that
[02:28:54.640 --> 02:28:56.640]   in a way that's like genuinely kind of legitimate
[02:28:56.640 --> 02:28:59.840]   and kind of a project that is sort of a kind of just
[02:29:00.880 --> 02:29:03.880]   incorporation of these beings into our civilization
[02:29:03.880 --> 02:29:05.360]   such that they can kind of all,
[02:29:05.360 --> 02:29:06.600]   or sorry, there's like the justice part,
[02:29:06.600 --> 02:29:07.440]   and there's also the kind of,
[02:29:07.440 --> 02:29:11.360]   is it like kind of compatible with like people, you know,
[02:29:11.360 --> 02:29:12.200]   is it a good deal?
[02:29:12.200 --> 02:29:13.360]   Is it a good bargain for people?
[02:29:13.360 --> 02:29:15.280]   And I think this is, you know, this is often how, you know,
[02:29:15.280 --> 02:29:17.840]   to the extent we're kind of very concerned about AIs
[02:29:17.840 --> 02:29:19.520]   like kind of rebelling or something like that.
[02:29:19.520 --> 02:29:22.040]   It's like, well, there's like a lot of, you know,
[02:29:22.040 --> 02:29:26.040]   part of a thing you can do is make civilization better
[02:29:26.040 --> 02:29:26.880]   for someone, right?
[02:29:26.880 --> 02:29:30.520]   So it's like, and I think that's an important feature
[02:29:30.520 --> 02:29:34.960]   of how we have in fact structured a lot of our
[02:29:34.960 --> 02:29:36.920]   political institutions and norms and stuff like that.
[02:29:36.920 --> 02:29:41.080]   So that's the thing I'm getting at in that quote.
[02:29:41.080 --> 02:29:42.920]   Okay, I think that's an excellent place to close.
[02:29:42.920 --> 02:29:43.760]   Great. Thank you so much.
[02:29:43.760 --> 02:29:46.040]   Joe, thanks so much for coming on the podcast.
[02:29:46.040 --> 02:29:47.800]   We discussed the ideas in the series.
[02:29:47.800 --> 02:29:49.320]   I think people might not appreciate
[02:29:49.320 --> 02:29:51.160]   if they haven't read the stories
[02:29:51.160 --> 02:29:53.160]   how beautifully written it is.
[02:29:53.160 --> 02:29:58.720]   It just like, the ideas, we didn't cover everything.
[02:29:58.720 --> 02:30:01.800]   There's a bunch of very, very interesting ideas.
[02:30:01.800 --> 02:30:04.680]   As somebody who has talked to people about AI for a while,
[02:30:04.680 --> 02:30:06.480]   things I haven't encountered anywhere else,
[02:30:06.480 --> 02:30:09.760]   but just obviously no part of the AI discourse
[02:30:09.760 --> 02:30:11.560]   is nearly as well-written.
[02:30:11.560 --> 02:30:15.720]   And it is genuinely a beautiful experience
[02:30:15.720 --> 02:30:17.840]   to listen to the podcast version,
[02:30:17.840 --> 02:30:19.120]   which is in your own voice.
[02:30:19.120 --> 02:30:21.080]   So I highly recommend people do that.
[02:30:21.080 --> 02:30:24.340]   So it's joecarlsmith.com where they can access this.
[02:30:24.340 --> 02:30:25.880]   Joe, thanks so much for coming on the podcast.
[02:30:25.880 --> 02:30:27.440]   Thank you for having me. I really enjoyed it.
[02:30:27.440 --> 02:30:30.760]   Hey, everybody. I hope you enjoyed that episode with Joe.
[02:30:30.760 --> 02:30:32.760]   If you did, as always, it's helpful
[02:30:32.760 --> 02:30:35.960]   if you can send it to friends, group chats, Twitter,
[02:30:35.960 --> 02:30:37.840]   whoever else you think might enjoy it.
[02:30:37.840 --> 02:30:39.400]   And also if you can leave a good rating
[02:30:39.400 --> 02:30:41.880]   on Apple Podcasts or wherever you listen,
[02:30:41.880 --> 02:30:42.920]   that's really helpful.
[02:30:42.920 --> 02:30:45.040]   Helps other people find the podcast.
[02:30:45.040 --> 02:30:46.860]   If you want transcripts of these episodes
[02:30:46.860 --> 02:30:48.440]   or you want to give them a blog post,
[02:30:48.440 --> 02:30:52.920]   you can subscribe to my sub stack at dwarkeshpatel.com.
[02:30:52.920 --> 02:30:54.600]   And finally, as you might've noticed,
[02:30:54.600 --> 02:30:56.080]   there's advertisements on this episode.
[02:30:56.080 --> 02:30:58.560]   So if you want to advertise on a future episode,
[02:30:58.560 --> 02:31:00.000]   you can learn more about doing that
[02:31:00.000 --> 02:31:03.720]   at dwarkeshpatel.com/advertise
[02:31:03.720 --> 02:31:05.240]   or the link in the description.
[02:31:05.240 --> 02:31:07.040]   Anyways, I'll see you on the next one.
[02:31:07.040 --> 02:31:07.880]   Thanks.
[02:31:07.880 --> 02:31:11.240]   [MUSIC PLAYING]
[02:31:11.240 --> 02:31:13.300]   you

