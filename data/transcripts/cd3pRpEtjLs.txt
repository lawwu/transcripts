
[00:00:00.000 --> 00:00:05.400]   [BLANK_AUDIO]
[00:00:05.400 --> 00:00:11.480]   Today I'm delighted to introduce as our final guest speaker, Bean Kim.
[00:00:11.480 --> 00:00:15.760]   Bean Kim is a staff research scientist at Google Brain.
[00:00:15.760 --> 00:00:19.480]   If you're really into Googleology, those funny words at the beginning,
[00:00:19.480 --> 00:00:22.480]   like staff, sort of says how senior you are.
[00:00:22.480 --> 00:00:25.280]   And that means that Bean's a good research scientist.
[00:00:25.280 --> 00:00:30.960]   [LAUGH] So I discovered at lunch today that Bean started out
[00:00:30.960 --> 00:00:36.520]   studying mechanical engineering at Seoul National University.
[00:00:36.520 --> 00:00:40.360]   But she moved on to, I don't know, it's better things or not.
[00:00:40.360 --> 00:00:46.000]   But she moved on to computer science and did her PhD at MIT.
[00:00:46.000 --> 00:00:49.120]   And there she started working on the interpretability and
[00:00:49.120 --> 00:00:52.480]   explainability of machine learning models.
[00:00:52.480 --> 00:00:56.680]   I think she'll be talking about some different parts of her work.
[00:00:56.680 --> 00:01:02.000]   But a theme that she's had in some of her recent work that I find especially
[00:01:02.000 --> 00:01:07.720]   appealing as an NLP person is the idea that we should be using
[00:01:07.720 --> 00:01:11.280]   higher level human interpretable languages for
[00:01:11.280 --> 00:01:15.200]   communication between people and machines.
[00:01:15.200 --> 00:01:20.160]   So welcome, Bean, looking forward to your talk, and go for it.
[00:01:20.160 --> 00:01:21.160]   >> Thank you.
[00:01:21.160 --> 00:01:25.240]   >> [APPLAUSE] >> Thank you.
[00:01:25.240 --> 00:01:26.760]   Thanks for having me.
[00:01:26.760 --> 00:01:28.720]   It's an honor to be here.
[00:01:28.720 --> 00:01:31.880]   It's the rainiest Stanford I've ever seen.
[00:01:31.880 --> 00:01:33.760]   Last night, I got here last night.
[00:01:33.760 --> 00:01:36.960]   But then I live in Seattle, so this is pretty common.
[00:01:36.960 --> 00:01:39.080]   So I still was able to see the blue sky today.
[00:01:39.080 --> 00:01:42.080]   I was like, this works, I really like it here.
[00:01:42.080 --> 00:01:45.120]   So today I'm going to share some of my dreams,
[00:01:45.120 --> 00:01:47.800]   chasing my dreams to communicate with machines.
[00:01:49.040 --> 00:01:53.240]   So if you're in this class, you probably agree, you don't have to,
[00:01:53.240 --> 00:01:57.480]   that large language models and generator models are pretty cool.
[00:01:57.480 --> 00:01:59.040]   They're impressive.
[00:01:59.040 --> 00:02:01.920]   But you may also agree that they're a little bit frightening.
[00:02:01.920 --> 00:02:07.120]   Not just because they're impressive, they're doing a really good job, but
[00:02:07.120 --> 00:02:12.040]   also we're not quite sure where we're going with this technology.
[00:02:12.040 --> 00:02:17.160]   In 10 years out, will we look back and say, that technology was net positive?
[00:02:17.160 --> 00:02:21.280]   Or we will say, that was catastrophic, we didn't know that that would happen.
[00:02:21.280 --> 00:02:28.600]   Ultimately, what I would like to do, or maybe hopefully what we all want to do,
[00:02:28.600 --> 00:02:32.560]   is to have this technology benefit us, humans.
[00:02:32.560 --> 00:02:36.440]   I know in 10 years time, or maybe, well, 20 years or earlier,
[00:02:36.440 --> 00:02:41.320]   he's gonna ask me, he's gonna be like, mom, did you work on this AI stuff?
[00:02:41.320 --> 00:02:42.680]   I watched some of your talks.
[00:02:43.680 --> 00:02:48.680]   And did you know that how this will profoundly change our lives?
[00:02:48.680 --> 00:02:50.560]   And what did you do about that?
[00:02:50.560 --> 00:02:53.480]   And I have to answer that question, and
[00:02:53.480 --> 00:02:56.000]   I really hope that I have some good things to say to him.
[00:02:56.000 --> 00:03:05.120]   So my initial thought, and still so, or current thought,
[00:03:05.120 --> 00:03:09.080]   is that if we want our ultimate goal to be benefit humanity,
[00:03:09.080 --> 00:03:11.480]   why not directly optimize for it, why wait?
[00:03:11.760 --> 00:03:15.480]   So how can we benefit?
[00:03:15.480 --> 00:03:18.240]   There's lots of different ways we can benefit.
[00:03:18.240 --> 00:03:22.640]   But one way we can benefit is to treat this like a colleague.
[00:03:22.640 --> 00:03:26.280]   You know, a colleague who are really good at something.
[00:03:26.280 --> 00:03:28.240]   This colleague is not perfect, but
[00:03:28.240 --> 00:03:31.520]   it's good at something enough that you want to learn something from them.
[00:03:31.520 --> 00:03:37.560]   One difference is though, in this case, is that this colleague is kind of weird.
[00:03:37.560 --> 00:03:40.720]   This colleague might have very different values,
[00:03:40.720 --> 00:03:44.480]   it might have very different experiences in the world.
[00:03:44.480 --> 00:03:48.080]   It may not care about surviving as much as we do.
[00:03:48.080 --> 00:03:52.400]   Maybe mortality isn't really a thing for this colleague.
[00:03:52.400 --> 00:03:55.760]   So you have to navigate that in our conversation.
[00:03:55.760 --> 00:03:59.120]   So what do you do when you first meet somebody?
[00:03:59.120 --> 00:04:01.040]   There's someone so different, what do you do?
[00:04:01.040 --> 00:04:07.320]   You try to have a conversation to figure out how do you do what you do?
[00:04:07.320 --> 00:04:11.280]   How are you solving decades old protein folding problem?
[00:04:11.280 --> 00:04:17.520]   How are you beating the world gold champion so easily, what it seems?
[00:04:17.520 --> 00:04:21.800]   Are you using the same language, the science language that we use,
[00:04:21.800 --> 00:04:23.480]   atoms, molecules?
[00:04:23.480 --> 00:04:27.320]   Or do you think about the world in a very different way?
[00:04:27.320 --> 00:04:29.800]   And more importantly, how can we work together?
[00:04:29.800 --> 00:04:37.000]   I have one alien that I really want to talk to, and it's AlphaGo.
[00:04:37.000 --> 00:04:41.040]   So AlphaGo beat world gold champion Isidore in 2016.
[00:04:41.040 --> 00:04:43.360]   Isidore is from South Korea, I'm from South Korea.
[00:04:43.360 --> 00:04:44.800]   I watched every single match.
[00:04:44.800 --> 00:04:48.440]   It was such a big deal in South Korea and worldwide, I hope.
[00:04:48.440 --> 00:04:53.960]   And in one of the matches, AlphaGo played this move called move 37.
[00:04:53.960 --> 00:04:57.680]   How many people watched AlphaGo matches?
[00:04:57.680 --> 00:05:00.920]   And how many people remember move 37?
[00:05:00.920 --> 00:05:02.600]   Yeah, a few people, right?
[00:05:02.600 --> 00:05:05.360]   And I remember the nine-dan commentator who's
[00:05:05.360 --> 00:05:10.000]   been talking a lot throughout the matches suddenly got really quiet.
[00:05:10.000 --> 00:05:14.640]   And he said, hmm, that's a very strange move.
[00:05:14.640 --> 00:05:17.560]   And I knew then that something really interesting
[00:05:17.560 --> 00:05:20.600]   has just happened in front of my eyes, that this
[00:05:20.600 --> 00:05:21.800]   is going to change something.
[00:05:21.800 --> 00:05:25.320]   This AlphaGo has made something that we're going to remember forever.
[00:05:25.320 --> 00:05:28.840]   And sure enough, this move turned around the game for AlphaGo
[00:05:28.840 --> 00:05:33.160]   and leading AlphaGo to win one of the matches.
[00:05:33.160 --> 00:05:36.440]   So goal players today continue to analyze this move
[00:05:36.440 --> 00:05:38.720]   and still discuss when people talk about,
[00:05:38.720 --> 00:05:42.120]   this is not the move a human would fan them.
[00:05:42.120 --> 00:05:45.680]   So the question is, how did AlphaGo know this is a good move?
[00:05:45.680 --> 00:05:54.720]   My dream is to learn something new by communicating with machines
[00:05:54.720 --> 00:05:58.040]   and having a conversation, and such that humanity
[00:05:58.040 --> 00:06:01.280]   will gain some new angle to our important problems
[00:06:01.280 --> 00:06:04.960]   like medicine and science and many others.
[00:06:04.960 --> 00:06:08.440]   And this is not just about discovering new things.
[00:06:08.440 --> 00:06:11.280]   If you think about reward hacking, you
[00:06:11.280 --> 00:06:14.800]   have to have a meaningful conversation with somebody
[00:06:14.800 --> 00:06:18.200]   to truly figure out what their true goal is.
[00:06:18.200 --> 00:06:23.240]   So in a way, solving this problem is a superset of solving AI safety, too.
[00:06:23.240 --> 00:06:29.080]   So how do we have this conversation?
[00:06:29.080 --> 00:06:33.720]   Conversation assumes that we share some common vocabulary between that
[00:06:33.720 --> 00:06:36.880]   exchange to exchange meaning and ultimately the knowledge.
[00:06:36.880 --> 00:06:40.880]   And naturally, a representation plays a key role in this conversation.
[00:06:40.880 --> 00:06:43.400]   On the left-- and we can visualize this on the left--
[00:06:43.400 --> 00:06:47.440]   we say, this is a representational space of what humans know.
[00:06:47.440 --> 00:06:50.120]   On the right, what machines know.
[00:06:50.120 --> 00:06:54.000]   Here in left circle, there will be something like, this dog is fluffy.
[00:06:54.000 --> 00:06:58.360]   And you know what that means, because we all share somewhat similar vocabulary.
[00:06:59.360 --> 00:07:02.760]   But on the right, we have something like move 37,
[00:07:02.760 --> 00:07:06.200]   where humans yet to have a representation for.
[00:07:06.200 --> 00:07:12.160]   So how do we have this conversation?
[00:07:12.160 --> 00:07:14.800]   Our representational space needs overlap.
[00:07:14.800 --> 00:07:19.160]   And the more overlap we have, the better conversation we're going to have.
[00:07:19.160 --> 00:07:21.520]   Humans are all good at learning new things.
[00:07:21.520 --> 00:07:24.320]   Like here, everyone is learning something new.
[00:07:24.320 --> 00:07:29.920]   So we can expand what we know by learning new concepts and vocabularies.
[00:07:29.920 --> 00:07:32.800]   And doing so, I believe, will help us to build
[00:07:32.800 --> 00:07:36.160]   machines that can better align with our values and our goals.
[00:07:36.160 --> 00:07:40.880]   So this is the talk that I gave.
[00:07:40.880 --> 00:07:43.680]   If you're curious about some of the work we're doing towards this direction,
[00:07:43.680 --> 00:07:44.600]   I highly recommend it.
[00:07:44.600 --> 00:07:45.640]   It's a YouTube video.
[00:07:45.640 --> 00:07:47.280]   I clear keynote, half an hour.
[00:07:47.280 --> 00:07:48.560]   You can fast.
[00:07:48.560 --> 00:07:50.280]   Do a fast feed.
[00:07:50.280 --> 00:07:53.880]   But today, I'm going to talk more about my hopes and dreams.
[00:07:53.880 --> 00:07:59.160]   And hopefully, at the end of the day, your hopes and dreams too.
[00:07:59.160 --> 00:08:03.600]   So first of all, I'm just going to set the expectation.
[00:08:03.600 --> 00:08:09.160]   So at the end of this talk, we still don't know how the move 37 is made.
[00:08:09.160 --> 00:08:09.840]   Sorry.
[00:08:09.840 --> 00:08:12.360]   That's going to take a while.
[00:08:12.360 --> 00:08:16.160]   In fact, the first part of this talk is going
[00:08:16.160 --> 00:08:21.640]   to be about how we move backwards in this progress,
[00:08:21.640 --> 00:08:24.480]   in terms of making this progress in our journey.
[00:08:24.480 --> 00:08:27.960]   And still a very, very small portion of our entire journey
[00:08:27.960 --> 00:08:31.680]   towards understanding move 37.
[00:08:31.680 --> 00:08:35.160]   And of course, this journey wouldn't be like a singular path.
[00:08:35.160 --> 00:08:38.480]   There will be lots of different branches coming in.
[00:08:38.480 --> 00:08:42.560]   Core ideas like transformer helped many domains across.
[00:08:42.560 --> 00:08:43.920]   There will be similar here.
[00:08:43.920 --> 00:08:46.560]   So I'm going to talk in the part two some
[00:08:46.560 --> 00:08:50.560]   of our work on understanding emerging behaviors in reinforcement learning.
[00:08:51.560 --> 00:08:54.280]   And all the techniques that I'm going to talk about
[00:08:54.280 --> 00:08:56.920]   is going to be in principle applicable to NLP.
[00:08:56.920 --> 00:09:05.120]   So coming back to our hopes and dreams, move 37.
[00:09:05.120 --> 00:09:09.440]   So let's first think about how we might realize this dream.
[00:09:09.440 --> 00:09:12.400]   And taking a step back, we have to ask, do we
[00:09:12.400 --> 00:09:17.480]   have tools to first estimate what even machines know?
[00:09:17.480 --> 00:09:20.720]   There has been many development in machine learning last decade
[00:09:20.720 --> 00:09:26.800]   now to develop tools to understand and estimate this purple circle.
[00:09:26.800 --> 00:09:28.880]   So is that accurate?
[00:09:28.880 --> 00:09:31.200]   Unfortunately, many recent research show
[00:09:31.200 --> 00:09:35.880]   that there is a huge gap between what machines actually know
[00:09:35.880 --> 00:09:40.200]   and what we think the machines know.
[00:09:40.200 --> 00:09:43.400]   And identifying and bridging this gap is important
[00:09:43.400 --> 00:09:46.640]   because these tools will form basis for understanding
[00:09:46.640 --> 00:09:47.360]   that move 37.
[00:09:47.360 --> 00:09:51.800]   So what are these tools?
[00:09:51.800 --> 00:09:55.280]   How many people familiar with saliency maps?
[00:09:55.280 --> 00:09:56.480]   A lot, but you don't have to.
[00:09:56.480 --> 00:09:57.680]   I'll explain what it is.
[00:09:57.680 --> 00:10:02.840]   So saliency map is one of the popular interpretability methods.
[00:10:02.840 --> 00:10:06.200]   For simplicity, let's say an image net, you have an image like this.
[00:10:06.200 --> 00:10:07.240]   You have a bird.
[00:10:07.240 --> 00:10:10.920]   The explanation is going to take a form of the same image,
[00:10:10.920 --> 00:10:15.320]   but where each pixel is associated with a number that
[00:10:15.320 --> 00:10:20.760]   is supposed to imply some importance of that pixel
[00:10:20.760 --> 00:10:23.520]   for prediction of this image.
[00:10:23.520 --> 00:10:26.160]   And one definition of that importance
[00:10:26.160 --> 00:10:29.000]   is that that number indicates how the function
[00:10:29.000 --> 00:10:31.240]   look like around this pixel.
[00:10:31.240 --> 00:10:35.640]   So for example, if I have a pixel ixj, maybe around xj,
[00:10:35.640 --> 00:10:38.400]   the function moves up like the yellow curve,
[00:10:38.400 --> 00:10:43.880]   or function is flat, or function is going down like the green curve.
[00:10:43.880 --> 00:10:47.960]   And so if it's flat like a blue curve or red curve,
[00:10:47.960 --> 00:10:51.040]   maybe that feature is irrelevant to predicting bird.
[00:10:51.040 --> 00:10:52.080]   Maybe it's going up.
[00:10:52.080 --> 00:10:55.200]   Then it's maybe more important because the value of x increases
[00:10:55.200 --> 00:10:56.480]   and the function value goes up.
[00:10:56.480 --> 00:10:58.280]   Function value here like a prediction value.
[00:10:58.280 --> 00:11:06.560]   So let's think about what are the few ways why this gap might exist.
[00:11:06.560 --> 00:11:07.440]   There are a few ways.
[00:11:07.440 --> 00:11:09.440]   Not exhaustive, they overlap a little bit,
[00:11:09.440 --> 00:11:11.320]   but helpful for us to think about.
[00:11:11.320 --> 00:11:13.120]   Maybe assumptions are wrong.
[00:11:13.120 --> 00:11:16.360]   So this alien, again, these machines that we train,
[00:11:16.360 --> 00:11:18.520]   works in a completely different, perhaps completely
[00:11:18.520 --> 00:11:21.640]   different representational space, very different experiences
[00:11:21.640 --> 00:11:23.120]   about the world.
[00:11:23.120 --> 00:11:26.840]   So assuming that it sees the world that we do just like we do,
[00:11:26.840 --> 00:11:30.120]   like having the gestalt phenomenon, there's few dots.
[00:11:30.120 --> 00:11:32.680]   Humans have tendency to connect them.
[00:11:32.680 --> 00:11:34.280]   Maybe machines have that too.
[00:11:34.280 --> 00:11:35.480]   Maybe not.
[00:11:35.480 --> 00:11:39.560]   So maybe our assumptions about these machines are wrong.
[00:11:39.560 --> 00:11:42.040]   Maybe our expectations are mismatched.
[00:11:42.040 --> 00:11:45.760]   We thought it was doing x, but it was actually doing y.
[00:11:45.760 --> 00:11:49.000]   Or maybe it's beyond us.
[00:11:49.000 --> 00:11:51.080]   Maybe it's showing something superhuman
[00:11:51.080 --> 00:11:52.920]   that humans just can't understand.
[00:11:52.920 --> 00:11:59.440]   I'm going to dig deeper into some of these, our work.
[00:11:59.440 --> 00:12:01.480]   This is more recent work.
[00:12:01.480 --> 00:12:04.600]   So again, coming back to the earlier story about salience
[00:12:04.600 --> 00:12:09.000]   map, we're going to play with some of these methods.
[00:12:09.000 --> 00:12:14.560]   Now, in 2018, we stumbled upon this phenomenon
[00:12:14.560 --> 00:12:17.400]   that was quite shocking, which was that we were actually
[00:12:17.400 --> 00:12:19.600]   trying to write some different people, again, paper,
[00:12:19.600 --> 00:12:20.920]   of course, the end here.
[00:12:20.920 --> 00:12:23.000]   But we were testing something, and we
[00:12:23.000 --> 00:12:26.400]   realized that trained network and untrained network
[00:12:26.400 --> 00:12:29.440]   has the same, very similar saliency map.
[00:12:29.440 --> 00:12:31.920]   In other words, random prediction
[00:12:31.920 --> 00:12:33.720]   and meaningful prediction were giving me
[00:12:33.720 --> 00:12:36.160]   the same explanation.
[00:12:36.160 --> 00:12:37.440]   So that was puzzling.
[00:12:37.440 --> 00:12:40.800]   We thought we had a bug, but it turned out we didn't.
[00:12:40.800 --> 00:12:45.920]   It actually is indistinguishable qualitatively and quantitatively.
[00:12:45.920 --> 00:12:47.840]   So that was shocking.
[00:12:47.840 --> 00:12:51.560]   But then we wondered, maybe it's a one-off case.
[00:12:51.560 --> 00:12:56.680]   Maybe it still works somehow in practice.
[00:12:56.680 --> 00:12:59.040]   So we tested that in a follow-up paper.
[00:12:59.040 --> 00:13:02.760]   OK, what if the model had an error, one of these errors?
[00:13:02.760 --> 00:13:04.440]   Maybe it has a labeling error.
[00:13:04.440 --> 00:13:06.600]   Maybe it has a spurious correlation.
[00:13:06.600 --> 00:13:09.880]   Maybe it had out-of-distribution at test time.
[00:13:09.880 --> 00:13:12.440]   If we intentionally insert these bugs,
[00:13:12.440 --> 00:13:14.800]   can explanation tell us that there's
[00:13:14.800 --> 00:13:17.360]   something wrong with the model?
[00:13:17.360 --> 00:13:21.720]   It turns out that that's also not quite true.
[00:13:21.720 --> 00:13:24.160]   You might think that, oh, maybe spurious correlation.
[00:13:24.160 --> 00:13:25.960]   Another follow-up work also showed
[00:13:25.960 --> 00:13:28.960]   that this is also not the case.
[00:13:28.960 --> 00:13:31.600]   So we were disappointed.
[00:13:31.600 --> 00:13:33.920]   But then still, we say, you know,
[00:13:33.920 --> 00:13:38.240]   maybe there's no theoretical proof of this.
[00:13:38.240 --> 00:13:40.920]   Maybe this is, again, a lab-setting test.
[00:13:40.920 --> 00:13:44.000]   We had grad students to test this system.
[00:13:44.000 --> 00:13:48.400]   Maybe there's still some hope.
[00:13:48.400 --> 00:13:50.800]   So this is more recent work where we theoretically
[00:13:50.800 --> 00:13:54.760]   prove that some of these methods, very popular methods,
[00:13:54.760 --> 00:13:56.400]   cannot do better than random.
[00:13:56.400 --> 00:14:00.800]   So I'm going to talk a little bit about that.
[00:14:00.800 --> 00:14:02.040]   I'm missing one person.
[00:14:02.040 --> 00:14:03.640]   I'm missing Peng Wei in the author list.
[00:14:03.640 --> 00:14:07.480]   I just realized this is also work with Peng Wei.
[00:14:07.480 --> 00:14:10.240]   So let's first talk about our expectation.
[00:14:10.240 --> 00:14:13.400]   What is our expectation about this tool?
[00:14:13.400 --> 00:14:18.160]   Now, the original paper that developed this method, IG
[00:14:18.160 --> 00:14:20.920]   and Schaub, talks about how IG can
[00:14:20.920 --> 00:14:25.560]   be used for accounting the contributions of each feature.
[00:14:25.560 --> 00:14:27.960]   So what that means is that when the tool assigns
[00:14:27.960 --> 00:14:30.920]   zero attribution to a pixel, we're going to say, OK,
[00:14:30.920 --> 00:14:33.840]   well, pixel is unused by the function.
[00:14:33.840 --> 00:14:36.560]   And that means that f will be insensitive
[00:14:36.560 --> 00:14:40.480]   if I perturb this x.
[00:14:40.480 --> 00:14:43.840]   And in fact, this is how it's been used in practice.
[00:14:43.840 --> 00:14:45.840]   This is a paper published in Nature.
[00:14:45.840 --> 00:14:49.720]   They used Schaub to figure out the eligibility
[00:14:49.720 --> 00:14:53.720]   criteria in a medical trial.
[00:14:53.720 --> 00:14:55.760]   What we show in this work is that none
[00:14:55.760 --> 00:15:01.240]   of these inferences that seemed pretty natural were true.
[00:15:01.240 --> 00:15:03.920]   And in fact, just because popular attribution methods
[00:15:03.920 --> 00:15:07.240]   tell you anything about attribution is x,
[00:15:07.240 --> 00:15:10.520]   you cannot conclude anything about the actual model
[00:15:10.520 --> 00:15:12.640]   behavior.
[00:15:12.640 --> 00:15:15.640]   So how does that work?
[00:15:15.640 --> 00:15:17.960]   How many people here do theory proof?
[00:15:17.960 --> 00:15:21.280]   A few.
[00:15:21.280 --> 00:15:21.960]   Great.
[00:15:21.960 --> 00:15:22.560]   I'll tell you.
[00:15:22.560 --> 00:15:25.600]   I learned about theory proving from this project as well.
[00:15:25.600 --> 00:15:30.080]   So I'll tell you the way that we pursued this particular work
[00:15:30.080 --> 00:15:32.000]   is that first think about this problem.
[00:15:32.000 --> 00:15:35.080]   And we're going to formulate into some other problem
[00:15:35.080 --> 00:15:37.040]   that we know how to solve.
[00:15:37.040 --> 00:15:41.120]   So in this case, we formulate this as hypothesis testing.
[00:15:41.120 --> 00:15:44.280]   Because once you formulate into hypothesis testing, yes or no,
[00:15:44.280 --> 00:15:45.900]   there are lots of tools and statistics
[00:15:45.900 --> 00:15:48.320]   you can use to prove this.
[00:15:48.320 --> 00:15:49.880]   So what is hypothesis?
[00:15:49.880 --> 00:15:52.640]   The hypothesis is that I'm a user.
[00:15:52.640 --> 00:15:55.840]   I got an attribution value from one of these tools.
[00:15:55.840 --> 00:15:59.920]   And I have a mental model of, ah, this feature is important
[00:15:59.920 --> 00:16:01.840]   or maybe not important.
[00:16:01.840 --> 00:16:06.040]   Then the hypothesis is that whether that's true or not.
[00:16:06.040 --> 00:16:09.600]   And what we showed is that given whatever hypothesis you
[00:16:09.600 --> 00:16:14.360]   may have, you cannot do better than random guessing,
[00:16:14.360 --> 00:16:18.080]   invalidating or invalidating this hypothesis testing.
[00:16:18.080 --> 00:16:20.680]   And that means, yes, sometimes it's right.
[00:16:20.680 --> 00:16:22.640]   But you don't do hypothesis testing
[00:16:22.640 --> 00:16:24.760]   if you cannot validate yes or no.
[00:16:24.760 --> 00:16:25.480]   You just don't.
[00:16:25.480 --> 00:16:26.980]   Because what's the point of doing it
[00:16:26.980 --> 00:16:29.920]   if you just don't know if it's as good as random guessing?
[00:16:29.920 --> 00:16:36.640]   And the result is that, yes, for this graph,
[00:16:36.640 --> 00:16:38.720]   it's just a visualization of our result.
[00:16:38.720 --> 00:16:41.280]   If you plot true negative and true positive,
[00:16:41.280 --> 00:16:43.200]   and line is random guessing, because this
[00:16:43.200 --> 00:16:45.160]   is the worst method, that's the best method.
[00:16:45.160 --> 00:16:47.440]   All the equal distance is this line.
[00:16:47.440 --> 00:16:50.600]   Methods that we know, SHOP and IG,
[00:16:50.600 --> 00:16:55.280]   all falls under this line of random guessing.
[00:16:55.280 --> 00:16:57.080]   That's bad news.
[00:16:57.080 --> 00:17:02.600]   But maybe this still works in practice for some reason.
[00:17:02.600 --> 00:17:03.960]   Maybe there were some assumptions
[00:17:03.960 --> 00:17:07.520]   that we had that didn't quite meet in the practice.
[00:17:07.520 --> 00:17:11.560]   So does this phenomenon hold in practice?
[00:17:11.560 --> 00:17:13.600]   The answer is yes.
[00:17:13.600 --> 00:17:16.200]   We now have more image graphs and more bigger models.
[00:17:16.200 --> 00:17:19.720]   But here we test two concrete end tasks
[00:17:19.720 --> 00:17:21.960]   that people care about in interpretability,
[00:17:21.960 --> 00:17:25.040]   or use these methods to do, recourse or spurious
[00:17:25.040 --> 00:17:26.120]   correlation.
[00:17:26.120 --> 00:17:28.080]   So recourse, for those who are not familiar,
[00:17:28.080 --> 00:17:29.880]   is you're getting a loan.
[00:17:29.880 --> 00:17:32.520]   And you wonder whether, if I'm older,
[00:17:32.520 --> 00:17:34.960]   I would have a high chance of getting a loan.
[00:17:34.960 --> 00:17:39.520]   So I tweak this one feature and see if my value goes up or down.
[00:17:39.520 --> 00:17:41.880]   Very reasonable task if people do it all the time.
[00:17:41.880 --> 00:17:45.840]   Pretty significant implication socially.
[00:17:45.840 --> 00:17:50.520]   So for two of these concrete end tasks, both of them
[00:17:50.520 --> 00:17:53.080]   boil down to this hypothesis testing framework
[00:17:53.080 --> 00:17:54.760]   that I talked about.
[00:17:54.760 --> 00:17:57.440]   They're all around the random guessing line,
[00:17:57.440 --> 00:18:01.240]   or worse than random guessing.
[00:18:01.240 --> 00:18:02.960]   So you might say, oh, no.
[00:18:02.960 --> 00:18:04.000]   This is not good.
[00:18:04.000 --> 00:18:05.600]   A lot of people are using these tools.
[00:18:05.600 --> 00:18:06.920]   What do we do?
[00:18:06.920 --> 00:18:10.640]   We have a very simple idea about this.
[00:18:10.640 --> 00:18:15.600]   So people like developing complex tools.
[00:18:15.600 --> 00:18:18.280]   And I really hope you're not one of those people.
[00:18:18.280 --> 00:18:22.640]   Because a lot of times, simple methods work.
[00:18:22.640 --> 00:18:23.840]   Who comes razor?
[00:18:23.840 --> 00:18:25.560]   But also, simple methods are elegant.
[00:18:25.560 --> 00:18:29.040]   There is a reason, perhaps a lot of times, why they work.
[00:18:29.040 --> 00:18:30.400]   They're simple.
[00:18:30.400 --> 00:18:31.760]   You can understand them.
[00:18:31.760 --> 00:18:32.880]   They make sense.
[00:18:32.880 --> 00:18:35.480]   So let's try that idea here.
[00:18:35.480 --> 00:18:38.680]   So again, your goal is to estimate a function shape.
[00:18:38.680 --> 00:18:39.680]   What do you do?
[00:18:39.680 --> 00:18:41.840]   Well, the simplest thing you do is
[00:18:41.840 --> 00:18:43.920]   you have a point of interest.
[00:18:43.920 --> 00:18:47.040]   You sample around that point and evaluate the function
[00:18:47.040 --> 00:18:48.320]   around that point.
[00:18:48.320 --> 00:18:50.760]   If it goes up, maybe function's going up.
[00:18:50.760 --> 00:18:54.480]   If it goes down, maybe function's coming down.
[00:18:54.480 --> 00:18:58.480]   So that's the simplest way you can brute force it.
[00:18:58.480 --> 00:19:01.720]   But then the question is, how many samples do we need?
[00:19:01.720 --> 00:19:05.240]   So here, this is the equation that you're
[00:19:05.240 --> 00:19:07.800]   lifting this line upwards that way
[00:19:07.800 --> 00:19:10.680]   by adding that additional term.
[00:19:10.680 --> 00:19:12.760]   It's proportional to number of samples.
[00:19:12.760 --> 00:19:14.640]   The more samples you have, the better estimation you have.
[00:19:14.640 --> 00:19:15.680]   It makes sense.
[00:19:15.680 --> 00:19:18.720]   And differences in output, how much resolution do you care?
[00:19:18.720 --> 00:19:22.800]   Do you care 0.1 to 0.2?
[00:19:22.800 --> 00:19:26.440]   Or do you only care 0 slope to slope 1?
[00:19:26.440 --> 00:19:29.520]   That's resolution that you care about and number
[00:19:29.520 --> 00:19:31.160]   of features, of course.
[00:19:31.160 --> 00:19:35.560]   So if you worry about making some conclusion based
[00:19:35.560 --> 00:19:38.600]   on function shape, sample.
[00:19:38.600 --> 00:19:39.080]   Easy.
[00:19:42.280 --> 00:19:45.640]   So can we infer the model behavior
[00:19:45.640 --> 00:19:47.640]   using these popular methods?
[00:19:47.640 --> 00:19:49.600]   Answer is no.
[00:19:49.600 --> 00:19:53.480]   And this holds both theory and practice.
[00:19:53.480 --> 00:19:55.480]   We're currently working on even bigger models
[00:19:55.480 --> 00:19:59.800]   to show just again and again empirical evidence that, yes,
[00:19:59.800 --> 00:20:01.440]   it just really doesn't work.
[00:20:01.440 --> 00:20:03.920]   Please think twice and three times
[00:20:03.920 --> 00:20:06.120]   before using these methods.
[00:20:06.120 --> 00:20:09.280]   And also, model-dependent sample complexity.
[00:20:09.280 --> 00:20:11.200]   If your function is kind of crazy,
[00:20:11.200 --> 00:20:13.320]   of course, you're going to need more samples.
[00:20:13.320 --> 00:20:14.400]   So what is the definition?
[00:20:14.400 --> 00:20:17.840]   How do we characterize these functions?
[00:20:17.840 --> 00:20:20.480]   And finally, we haven't quite given up yet.
[00:20:20.480 --> 00:20:22.440]   Because these methods have a pretty good root
[00:20:22.440 --> 00:20:25.840]   in economics and shop-levels and all that.
[00:20:25.840 --> 00:20:30.080]   So maybe there are a lot narrower condition
[00:20:30.080 --> 00:20:31.880]   where these methods work.
[00:20:31.880 --> 00:20:35.040]   And we believe such condition does exist.
[00:20:35.040 --> 00:20:37.120]   We just have to figure out when.
[00:20:37.120 --> 00:20:39.960]   Once we figure out what that condition is,
[00:20:39.960 --> 00:20:42.840]   then in a given function, I can test it and say,
[00:20:42.840 --> 00:20:44.400]   yes, I can use shop here.
[00:20:44.400 --> 00:20:45.840]   Yes, I can use IG here.
[00:20:45.840 --> 00:20:47.520]   Or no, I can't.
[00:20:47.520 --> 00:20:51.800]   That would be still very useful for ongoing work.
[00:20:51.800 --> 00:20:55.040]   Before I go to the next one, any questions?
[00:20:55.040 --> 00:20:56.040]   Yes?
[00:20:56.040 --> 00:20:59.400]   To the findings you have about the JPEG models,
[00:20:59.400 --> 00:21:01.680]   does it only apply to computer vision models?
[00:21:01.680 --> 00:21:04.320]   Or does it apply to NLP-only models?
[00:21:04.320 --> 00:21:06.160]   Any model that has a function.
[00:21:06.160 --> 00:21:09.080]   [LAUGHTER]
[00:21:09.080 --> 00:21:10.240]   Yeah, very simple.
[00:21:10.240 --> 00:21:10.920]   Simple, actually.
[00:21:10.920 --> 00:21:16.920]   Simplish proof that can show simply any function, this holds.
[00:21:16.920 --> 00:21:17.960]   Any other questions?
[00:21:17.960 --> 00:21:21.760]   Wonderful.
[00:21:21.760 --> 00:21:22.280]   Yeah, Chris?
[00:21:22.280 --> 00:21:24.560]   This may disrelate to your last bullet.
[00:21:24.560 --> 00:21:28.800]   But it sort of seems like for the last couple of years,
[00:21:28.800 --> 00:21:32.440]   there have been at least dozens, maybe hundreds of people
[00:21:32.440 --> 00:21:35.320]   writing papers using Shepley values.
[00:21:35.320 --> 00:21:42.520]   I mean, is your guess that most of that work is invalid?
[00:21:42.520 --> 00:21:47.600]   Or that a lot of it might be OK because whatever conditions
[00:21:47.600 --> 00:21:51.320]   that it's all right might often be being there?
[00:21:51.320 --> 00:21:53.680]   So two answers to that question.
[00:21:53.680 --> 00:21:57.760]   My hypothesis testing results shows that it's random.
[00:21:57.760 --> 00:22:04.280]   So maybe in the optimistic case, 50% of those papers,
[00:22:04.280 --> 00:22:06.480]   you hit it.
[00:22:06.480 --> 00:22:09.840]   And on the other side, on the second note,
[00:22:09.840 --> 00:22:12.280]   even if maybe Shep wasn't perfect,
[00:22:12.280 --> 00:22:14.000]   maybe it was kind of wrong.
[00:22:14.000 --> 00:22:17.440]   But even if it helped human at the end task, whatever
[00:22:17.440 --> 00:22:19.720]   that might be, help doctors to be more efficient,
[00:22:19.720 --> 00:22:22.920]   identifying bugs and whatnot, and if they did the validation
[00:22:22.920 --> 00:22:26.640]   correctly with the right control testing setup,
[00:22:26.640 --> 00:22:28.120]   then I think it's good.
[00:22:28.120 --> 00:22:29.840]   You figured out somehow how to make
[00:22:29.840 --> 00:22:33.320]   this noisy tools together work with human in the loop, maybe.
[00:22:33.320 --> 00:22:34.600]   And that's also good.
[00:22:34.600 --> 00:22:37.240]   And I personally really like Shep's paper.
[00:22:37.240 --> 00:22:39.240]   And I'm a good friend with Scott.
[00:22:39.240 --> 00:22:41.160]   And I love all his work.
[00:22:41.160 --> 00:22:43.120]   It's just that I think we need to narrow down
[00:22:43.120 --> 00:22:45.520]   our expectations so that our expectations are
[00:22:45.520 --> 00:22:46.320]   better aligned.
[00:22:46.320 --> 00:22:50.320]   All right.
[00:22:50.320 --> 00:22:52.240]   I'm going to talk about another work that's
[00:22:52.240 --> 00:22:54.160]   kind of similar flavor.
[00:22:54.160 --> 00:22:56.800]   Now it's an NLP.
[00:22:56.800 --> 00:23:00.560]   So this is one of those papers, just like the many other papers
[00:23:00.560 --> 00:23:02.680]   that we ended up writing.
[00:23:02.680 --> 00:23:04.480]   One of those serendipity papers.
[00:23:04.480 --> 00:23:07.560]   So initially, Peter came up as an intern.
[00:23:07.560 --> 00:23:10.360]   And we thought, we're going to locate ethical knowledge
[00:23:10.360 --> 00:23:12.360]   in this large language models.
[00:23:12.360 --> 00:23:14.760]   And then maybe we're going to edit them to make them
[00:23:14.760 --> 00:23:15.800]   a little more ethical.
[00:23:15.800 --> 00:23:17.000]   So that was the goal.
[00:23:17.000 --> 00:23:19.520]   And then we thought, oh, the Rome paper from David Bauer.
[00:23:19.520 --> 00:23:21.560]   And I also love David's work.
[00:23:21.560 --> 00:23:22.840]   And let's use that.
[00:23:22.840 --> 00:23:24.960]   That's the start of this work.
[00:23:24.960 --> 00:23:28.560]   But then we start digging into and implementing the Rome.
[00:23:28.560 --> 00:23:30.960]   And things didn't quite line up.
[00:23:30.960 --> 00:23:34.320]   So we do sanity check, experiment after sanity check.
[00:23:34.320 --> 00:23:36.840]   And we ended up writing a completely different paper,
[00:23:36.840 --> 00:23:39.960]   which I'm about to talk to you about.
[00:23:39.960 --> 00:23:45.200]   So this paper, the Rome, for those who are not familiar,
[00:23:45.200 --> 00:23:47.600]   which I'm going into a little more detail in a bit,
[00:23:47.600 --> 00:23:49.200]   is about editing a model.
[00:23:49.200 --> 00:23:52.600]   So you first locate a knowledge in a model.
[00:23:52.600 --> 00:23:54.640]   Like the space needle is in Seattle.
[00:23:54.640 --> 00:23:56.440]   That's a fact, your knowledge.
[00:23:56.440 --> 00:23:57.560]   You locate them.
[00:23:57.560 --> 00:23:58.960]   You edit them.
[00:23:58.960 --> 00:24:02.040]   Because you can locate them, you can mess with it
[00:24:02.040 --> 00:24:03.280]   to edit that fact.
[00:24:03.280 --> 00:24:05.000]   That's the whole promise of it.
[00:24:05.000 --> 00:24:08.040]   In fact, that's a lot of times how localization or editing
[00:24:08.040 --> 00:24:11.480]   methods were motivated in the literature.
[00:24:11.480 --> 00:24:14.720]   But what we show is that this assumption is actually not
[00:24:14.720 --> 00:24:16.480]   true.
[00:24:16.480 --> 00:24:18.680]   And to be quite honest with you, I still
[00:24:18.680 --> 00:24:22.800]   don't quite get why this is not related.
[00:24:22.800 --> 00:24:24.600]   And I'll talk more about this, because this
[00:24:24.600 --> 00:24:26.480]   is a big question to us.
[00:24:26.480 --> 00:24:29.520]   This is pretty active work.
[00:24:29.520 --> 00:24:33.720]   So substantial fraction of factual knowledge
[00:24:33.720 --> 00:24:37.560]   is stored outside of layers that are identified
[00:24:37.560 --> 00:24:39.480]   as having the knowledge.
[00:24:39.480 --> 00:24:44.600]   And you will see this a little more detail in a bit.
[00:24:44.600 --> 00:24:48.880]   In fact, the correlation between where the location, where
[00:24:48.880 --> 00:24:52.120]   the facts are located, and how well you will edit
[00:24:52.120 --> 00:24:55.920]   if you edit that location is completely correlated,
[00:24:55.920 --> 00:24:57.280]   uncorrelated.
[00:24:57.280 --> 00:25:00.800]   So they have nothing to do with each other.
[00:25:00.800 --> 00:25:04.360]   So we thought, well, maybe it's the problem
[00:25:04.360 --> 00:25:06.160]   with the definition of editing.
[00:25:06.160 --> 00:25:08.960]   What we mean by editing can mean a lot of different things.
[00:25:08.960 --> 00:25:13.040]   So let's think about different ways to edit a thing.
[00:25:13.040 --> 00:25:16.280]   So we tried a bunch of things with little success.
[00:25:16.280 --> 00:25:19.480]   We couldn't find an editing definition that actually
[00:25:19.480 --> 00:25:22.240]   relates really well with localization methods,
[00:25:22.240 --> 00:25:23.880]   like in particular with ROM.
[00:25:24.560 --> 00:25:29.840]   So let's talk a little bit about ROM, how ROM works,
[00:25:29.840 --> 00:25:30.760]   super briefly.
[00:25:30.760 --> 00:25:32.800]   There's a lot of details missed out on the slide,
[00:25:32.800 --> 00:25:34.920]   but roughly you will get the idea.
[00:25:34.920 --> 00:25:38.480]   So ROM is Mangeto, 2022.
[00:25:38.480 --> 00:25:41.920]   They have what's called causal tracing algorithm.
[00:25:41.920 --> 00:25:43.640]   And the way it works is that you're
[00:25:43.640 --> 00:25:46.440]   going to run a model on this particular data
[00:25:46.440 --> 00:25:51.600]   set, counterfact data set, that has this tuple, subject,
[00:25:51.600 --> 00:25:52.800]   relation, and object.
[00:25:52.800 --> 00:25:56.280]   The space needle is located in Seattle.
[00:25:56.280 --> 00:25:59.720]   And so you're going to have a clean run of the space needle
[00:25:59.720 --> 00:26:01.600]   is in Seattle one time.
[00:26:01.600 --> 00:26:04.200]   You stole every single module, every single value,
[00:26:04.200 --> 00:26:05.960]   activations.
[00:26:05.960 --> 00:26:09.680]   And then in the second run, which they call corrupted run,
[00:26:09.680 --> 00:26:13.200]   you're going to add noise in the space needle is--
[00:26:13.200 --> 00:26:15.280]   or the space.
[00:26:15.280 --> 00:26:20.080]   Then you're going to intervene at every single one
[00:26:20.080 --> 00:26:26.320]   of those modules by copying this module to the corrupted run.
[00:26:26.320 --> 00:26:31.480]   So as if that particular model was never interrupted,
[00:26:31.480 --> 00:26:34.360]   noise was never added to that module.
[00:26:34.360 --> 00:26:37.160]   So it's a typical intervention case
[00:26:37.160 --> 00:26:40.720]   where you pretend everything else being equal.
[00:26:40.720 --> 00:26:43.320]   If I change just this one module,
[00:26:43.320 --> 00:26:46.400]   what is the probability of having the right answer?
[00:26:46.400 --> 00:26:48.480]   So in this case, probability of the right answer,
[00:26:48.480 --> 00:26:52.080]   Seattle, given that I know is the model
[00:26:52.080 --> 00:26:55.000]   and I intervened on it.
[00:26:55.000 --> 00:26:57.440]   So at the end of the day, you'll find a graph
[00:26:57.440 --> 00:27:02.560]   like that where each layer and each token has a score.
[00:27:02.560 --> 00:27:07.160]   How likely it is if I intervene on that token in that layer?
[00:27:07.160 --> 00:27:10.960]   How likely is it that I will recover the right answer?
[00:27:10.960 --> 00:27:13.280]   Because if I recover right answer, that's the module.
[00:27:13.280 --> 00:27:16.240]   That's the module that stored the knowledge.
[00:27:16.240 --> 00:27:17.960]   Really reasonable algorithm.
[00:27:17.960 --> 00:27:20.320]   I couldn't find technical flaw in this algorithm.
[00:27:20.320 --> 00:27:21.640]   I quite like it, actually.
[00:27:21.640 --> 00:27:28.320]   But when we start looking at this using the same model
[00:27:28.320 --> 00:27:32.160]   that they used, GPT-J, we realize
[00:27:32.160 --> 00:27:34.840]   that a lot of these facts--
[00:27:34.840 --> 00:27:38.080]   so Roam uses just layer 6 to edit,
[00:27:38.080 --> 00:27:42.200]   because that was supposedly the best layer across this data set
[00:27:42.200 --> 00:27:42.720]   to edit.
[00:27:42.720 --> 00:27:45.360]   Most of the factual knowledge is stored in layer 6.
[00:27:45.360 --> 00:27:49.120]   And they showed editing success and whatnot.
[00:27:49.120 --> 00:27:52.560]   But we realized the truth looks like the graph on the right.
[00:27:52.560 --> 00:27:55.320]   So the red line is the layer 6.
[00:27:55.320 --> 00:27:57.480]   Their extension paper called MEME
[00:27:57.480 --> 00:28:01.360]   edits multiple layers at the blue region.
[00:28:01.360 --> 00:28:04.840]   The black bars are histogram of where the knowledge was
[00:28:04.840 --> 00:28:08.600]   actually peaked if you test every single layer.
[00:28:08.600 --> 00:28:12.000]   And as you can see, not a lot of facts fall into that region.
[00:28:12.000 --> 00:28:14.920]   So in fact, every single fact has different region
[00:28:14.920 --> 00:28:16.160]   that where it peaked.
[00:28:16.160 --> 00:28:20.400]   So layer 6, for a lot of facts, weren't the best layer.
[00:28:20.400 --> 00:28:22.120]   But the editing really worked.
[00:28:22.120 --> 00:28:23.200]   It really works.
[00:28:23.200 --> 00:28:26.080]   And we were able to duplicate the results.
[00:28:26.080 --> 00:28:30.680]   So we thought, what do we do to find this ethical knowledge?
[00:28:30.680 --> 00:28:33.080]   How do we find the best layer to edit?
[00:28:33.080 --> 00:28:34.720]   So that's where we started.
[00:28:34.720 --> 00:28:36.360]   But then we thought, you know what?
[00:28:36.360 --> 00:28:37.640]   Take a step back.
[00:28:37.640 --> 00:28:40.520]   We're going to actually do a sanity check first
[00:28:40.520 --> 00:28:42.520]   to make sure that tracing effect--
[00:28:43.200 --> 00:28:46.640]   the tracing effect is the localization--
[00:28:46.640 --> 00:28:49.560]   implies better editing results.
[00:28:49.560 --> 00:28:53.840]   And that's when everything started to falling apart.
[00:28:53.840 --> 00:28:56.080]   So let's define some metrics first.
[00:28:56.080 --> 00:28:59.360]   The edit success-- this is the rewrite score,
[00:28:59.360 --> 00:29:01.600]   same score as Rome paper used.
[00:29:01.600 --> 00:29:02.960]   That's what we used.
[00:29:02.960 --> 00:29:06.160]   And the tracing effect-- this is localization--
[00:29:06.160 --> 00:29:10.040]   is probably-- you can read the slide.
[00:29:10.040 --> 00:29:13.680]   So when we plotted the relation between tracing effect
[00:29:13.680 --> 00:29:18.200]   and rewrite score, the editing method,
[00:29:18.200 --> 00:29:22.080]   red line implies the perfect correlation.
[00:29:22.080 --> 00:29:23.800]   And that was our assumption, that there
[00:29:23.800 --> 00:29:25.480]   will be perfectly correlated, which
[00:29:25.480 --> 00:29:28.480]   is why we do localization to begin with.
[00:29:28.480 --> 00:29:30.840]   The actual line was yellow.
[00:29:30.840 --> 00:29:32.040]   It's close to zero.
[00:29:32.040 --> 00:29:36.440]   It's actually negative in this particular data set.
[00:29:36.440 --> 00:29:37.680]   That is not even uncorrelated.
[00:29:37.680 --> 00:29:39.920]   It's anti-correlated.
[00:29:39.920 --> 00:29:41.000]   And we didn't stop there.
[00:29:41.000 --> 00:29:42.640]   We were so puzzled.
[00:29:42.640 --> 00:29:44.720]   We're going to do this for every single layer.
[00:29:44.720 --> 00:29:47.120]   And we're going to find R-squared value.
[00:29:47.120 --> 00:29:50.320]   So how much of the choice of layer
[00:29:50.320 --> 00:29:53.000]   versus the localization, the tracing effect,
[00:29:53.000 --> 00:29:57.080]   explains the variance of successful edit?
[00:29:57.080 --> 00:29:58.960]   If you're not familiar with R-squared,
[00:29:58.960 --> 00:29:59.960]   R-squared is like a--
[00:29:59.960 --> 00:30:03.240]   think about it as an importance of a factor.
[00:30:03.240 --> 00:30:06.960]   And it turns out that layer takes 94%.
[00:30:06.960 --> 00:30:10.680]   Tracing effect is 0.16.
[00:30:10.680 --> 00:30:11.960]   And so we were really puzzled.
[00:30:11.960 --> 00:30:13.160]   We were scratching our head.
[00:30:13.160 --> 00:30:15.640]   Why is this true?
[00:30:15.640 --> 00:30:17.520]   But it was true across layer.
[00:30:17.520 --> 00:30:19.200]   We tried all sorts of different things.
[00:30:19.200 --> 00:30:20.800]   We tried different model.
[00:30:20.800 --> 00:30:22.080]   We tried different data set.
[00:30:22.080 --> 00:30:24.600]   It was all roughly the case.
[00:30:24.600 --> 00:30:28.120]   So at this point, we contacted David.
[00:30:28.120 --> 00:30:29.520]   And we started talking about it.
[00:30:29.520 --> 00:30:30.920]   And we resolved them.
[00:30:30.920 --> 00:30:34.600]   They acknowledged that this is a phenomenon that exists.
[00:30:34.600 --> 00:30:35.960]   Yeah, John?
[00:30:35.960 --> 00:30:38.760]   So apart from the layer, the other way
[00:30:38.760 --> 00:30:41.280]   which localization can happen is are you
[00:30:41.280 --> 00:30:42.600]   looking at the correct token?
[00:30:42.600 --> 00:30:44.840]   Is that the other corresponding--
[00:30:44.840 --> 00:30:45.600]   Yeah.
[00:30:45.600 --> 00:30:48.880]   Yeah, in this graph, the token is in--
[00:30:48.880 --> 00:30:52.720]   So the added benefit of the rest of the localization
[00:30:52.720 --> 00:30:54.920]   could only help you look at which is the correct subred
[00:30:54.920 --> 00:30:55.720]   token, is that it?
[00:30:55.720 --> 00:30:56.560]   Yeah, yeah.
[00:30:56.560 --> 00:30:58.200]   So looking at any of the subred tokens
[00:30:58.200 --> 00:30:59.880]   is sort of fine is what I should think of?
[00:30:59.880 --> 00:31:00.440]   Yeah, yeah.
[00:31:00.440 --> 00:31:03.080]   Just layer is the most biggest thing.
[00:31:03.080 --> 00:31:04.680]   That's the only thing you should care.
[00:31:04.680 --> 00:31:05.680]   You care about editing?
[00:31:05.680 --> 00:31:06.480]   Layers.
[00:31:06.480 --> 00:31:08.480]   In fact, don't worry about localization at all.
[00:31:08.480 --> 00:31:13.280]   It's extra wasted carbon climate effect.
[00:31:13.280 --> 00:31:16.240]   So that was our conclusion.
[00:31:16.240 --> 00:31:20.840]   But then we thought, maybe the particular definition of edit
[00:31:20.840 --> 00:31:23.880]   that they used in the room was maybe different.
[00:31:23.880 --> 00:31:26.920]   Maybe there exists a definition of editing
[00:31:26.920 --> 00:31:30.480]   that correlates a lot better with localization.
[00:31:30.480 --> 00:31:31.320]   Because there must be.
[00:31:31.320 --> 00:31:32.360]   I'm still puzzled.
[00:31:32.360 --> 00:31:34.560]   Why is this not correlated?
[00:31:34.560 --> 00:31:38.800]   So we tried a bunch of different definitions of edits.
[00:31:38.800 --> 00:31:41.040]   You might inject an error.
[00:31:41.040 --> 00:31:46.280]   You might reverse the tracing.
[00:31:46.280 --> 00:31:47.720]   You might want to erase a fag.
[00:31:47.720 --> 00:31:49.120]   You might want to amplify the fag.
[00:31:49.120 --> 00:31:49.840]   All these things.
[00:31:49.840 --> 00:31:52.480]   Like maybe one of these will work.
[00:31:52.480 --> 00:31:53.960]   It didn't.
[00:31:53.960 --> 00:31:56.000]   So the graph that you're seeing down here
[00:31:56.000 --> 00:31:59.400]   is our square value for four different methods.
[00:31:59.400 --> 00:32:01.360]   And this wasn't just the case for ROM and MEM.
[00:32:01.360 --> 00:32:04.480]   It was also the case for fine tuning methods.
[00:32:04.480 --> 00:32:06.720]   That you want to look at the difference
[00:32:06.720 --> 00:32:10.280]   between blue and orange bar represents
[00:32:10.280 --> 00:32:12.560]   how much the tracing effect influenced
[00:32:12.560 --> 00:32:14.400]   our square value of the tracing effect.
[00:32:14.400 --> 00:32:16.160]   As you can see, it's ignorable.
[00:32:16.160 --> 00:32:17.920]   They're all the same.
[00:32:17.920 --> 00:32:20.760]   You might feel that effect forcing, the last one,
[00:32:20.760 --> 00:32:22.240]   has a little bit of hope.
[00:32:22.240 --> 00:32:27.000]   But still, compared to the impact of layer, choice of layer,
[00:32:27.000 --> 00:32:28.680]   it's ignorable.
[00:32:28.680 --> 00:32:32.560]   So at this point, we said, OK, well, we
[00:32:32.560 --> 00:32:35.920]   can't locate the ethical knowledge at this project.
[00:32:35.920 --> 00:32:37.800]   We're going to have to switch the direction.
[00:32:37.800 --> 00:32:41.520]   And we ended up doing a lot more in-depth analysis on this.
[00:32:41.520 --> 00:32:48.080]   So in summary, does localization help editing?
[00:32:48.080 --> 00:32:49.520]   No.
[00:32:49.520 --> 00:32:51.600]   The relationship is actually zero.
[00:32:51.600 --> 00:32:55.120]   For this particular editing method, from what I know,
[00:32:55.120 --> 00:32:56.880]   it's pretty state of the art.
[00:32:56.880 --> 00:33:00.360]   And the counterfact data, it's not true.
[00:33:00.360 --> 00:33:01.800]   Are there any other editing method
[00:33:01.800 --> 00:33:02.840]   that correlate better?
[00:33:02.840 --> 00:33:03.440]   No.
[00:33:03.440 --> 00:33:05.680]   But if somebody can answer this question for me,
[00:33:05.680 --> 00:33:07.360]   that will be very satisfying.
[00:33:07.360 --> 00:33:10.480]   I feel like there should still be something there
[00:33:10.480 --> 00:33:12.440]   that we're missing.
[00:33:12.440 --> 00:33:14.920]   But causal tracing, I think what it does
[00:33:14.920 --> 00:33:18.960]   is it reveals the factual information when
[00:33:18.960 --> 00:33:21.920]   the transformer is passing forward.
[00:33:21.920 --> 00:33:24.880]   I think it represents where is the fact when
[00:33:24.880 --> 00:33:26.360]   you're doing that.
[00:33:26.360 --> 00:33:28.480]   But what we found here is that it has nothing
[00:33:28.480 --> 00:33:30.680]   to do with editing success.
[00:33:30.680 --> 00:33:32.000]   Those two things are different.
[00:33:32.000 --> 00:33:35.120]   And we have to resolve that somehow.
[00:33:35.120 --> 00:33:37.880]   But a lot of insights that they found in their paper
[00:33:37.880 --> 00:33:40.520]   is still useful, like the early to mid-range NLP
[00:33:40.520 --> 00:33:42.360]   representation, loss token.
[00:33:42.360 --> 00:33:46.160]   They represent the factual, something we didn't know before.
[00:33:46.160 --> 00:33:50.440]   But it is important not to validate localization methods
[00:33:50.440 --> 00:33:52.960]   using the editing method, now we know,
[00:33:52.960 --> 00:33:56.880]   and maybe not to motivate editing methods using
[00:33:56.880 --> 00:33:58.600]   via localization.
[00:33:58.600 --> 00:34:01.880]   Those are the two things now we know that we shouldn't do,
[00:34:01.880 --> 00:34:05.200]   because we couldn't find a relationship.
[00:34:05.200 --> 00:34:07.920]   Any questions on this one before I move on to the next one?
[00:34:07.920 --> 00:34:17.480]   I'm not shocked by this.
[00:34:17.480 --> 00:34:18.680]   I am shocked by this.
[00:34:18.680 --> 00:34:21.600]   I'm still so puzzled.
[00:34:21.600 --> 00:34:22.800]   There should be something.
[00:34:22.800 --> 00:34:23.400]   I don't know.
[00:34:26.840 --> 00:34:28.840]   All right.
[00:34:28.840 --> 00:34:32.200]   So in summary of this first part,
[00:34:32.200 --> 00:34:35.280]   we talked about why the gap might exist,
[00:34:35.280 --> 00:34:38.800]   what machines know versus what we think machines know.
[00:34:38.800 --> 00:34:40.040]   There are three hypotheses.
[00:34:40.040 --> 00:34:41.080]   There are three ideas.
[00:34:41.080 --> 00:34:41.920]   Assumptions are wrong.
[00:34:41.920 --> 00:34:43.520]   Maybe our expectations are wrong.
[00:34:43.520 --> 00:34:45.280]   Maybe it's beyond us.
[00:34:45.280 --> 00:34:48.480]   There's a good quote that says, "Good artists steal.
[00:34:48.480 --> 00:34:50.520]   I think good researchers doubt."
[00:34:50.520 --> 00:34:54.160]   We have to be really suspicious of everything that we do.
[00:34:54.160 --> 00:34:55.560]   And that's maybe the biggest lesson
[00:34:55.560 --> 00:34:57.880]   that I've learned over many years,
[00:34:57.880 --> 00:35:02.360]   that once you like your results so much, that's a bad sign.
[00:35:02.360 --> 00:35:05.720]   Come back, go home, have a beer, go to sleep.
[00:35:05.720 --> 00:35:09.240]   And next day, you come back and put your paper on your desk
[00:35:09.240 --> 00:35:12.480]   and think, OK, now I'm going to review this paper.
[00:35:12.480 --> 00:35:13.680]   How do I criticize this?
[00:35:13.680 --> 00:35:16.320]   What do I not like about this paper?
[00:35:16.320 --> 00:35:17.520]   That's one way to look at it.
[00:35:17.520 --> 00:35:19.800]   Criticize your own research, and that will
[00:35:19.800 --> 00:35:22.160]   improve your thinking a lot.
[00:35:23.160 --> 00:35:26.560]   So let's bring our attention back to our hopes and dreams.
[00:35:26.560 --> 00:35:28.720]   It keeps coming back.
[00:35:28.720 --> 00:35:33.560]   So here, I came to realize maybe instead of just building
[00:35:33.560 --> 00:35:37.800]   tools to understand, perhaps we need to do some groundwork.
[00:35:37.800 --> 00:35:38.920]   What do I mean?
[00:35:38.920 --> 00:35:41.680]   Well, this alien that we've been dealing with,
[00:35:41.680 --> 00:35:45.960]   trying to generate explanations, seems to be a different kind.
[00:35:45.960 --> 00:35:48.920]   So maybe we should study them as if they're
[00:35:48.920 --> 00:35:50.560]   like newbies to the field.
[00:35:50.560 --> 00:35:54.640]   Study them as if they're like new species in the wild.
[00:35:54.640 --> 00:35:56.160]   So what do you do when you observe
[00:35:56.160 --> 00:35:57.800]   a new species in the wild?
[00:35:57.800 --> 00:35:59.120]   You have a couple of ways.
[00:35:59.120 --> 00:36:02.120]   But one of the ways is to do observational study.
[00:36:02.120 --> 00:36:05.120]   So you saw some species in the wild far away.
[00:36:05.120 --> 00:36:07.280]   First, you just kind of watch them.
[00:36:07.280 --> 00:36:09.560]   You watch them and see what are they like,
[00:36:09.560 --> 00:36:14.440]   what are their habitat, what are their values and whatnot.
[00:36:14.440 --> 00:36:18.000]   And second way, you can actually intervene and do a control
[00:36:18.000 --> 00:36:18.800]   study.
[00:36:18.800 --> 00:36:22.920]   So we did something like this with reinforcement learning
[00:36:22.920 --> 00:36:25.560]   setup.
[00:36:25.560 --> 00:36:29.080]   I'm going to talk about these two papers, first paper.
[00:36:29.080 --> 00:36:31.360]   Emergent behaviors in multi-agent systems
[00:36:31.360 --> 00:36:32.680]   has been so cool.
[00:36:32.680 --> 00:36:36.360]   Who saw this hide and seek video by OpenAI?
[00:36:36.360 --> 00:36:37.120]   Yeah, it's so cool.
[00:36:37.120 --> 00:36:39.240]   If you haven't seen it, just Google it and watch it.
[00:36:39.240 --> 00:36:40.440]   It's so fascinating.
[00:36:40.440 --> 00:36:42.920]   I'm only covering the tip of an iceberg in this.
[00:36:42.920 --> 00:36:47.400]   But at the end of this hide and seek episode, at some point,
[00:36:47.400 --> 00:36:52.080]   the agents discover a bug in this physical system
[00:36:52.080 --> 00:36:55.680]   and start anti-gravity flying in the air
[00:36:55.680 --> 00:36:57.960]   and shooting hiders everywhere.
[00:36:57.960 --> 00:36:59.360]   It's a super interesting video.
[00:36:59.360 --> 00:37:01.320]   You must watch.
[00:37:01.320 --> 00:37:02.360]   So lots of that.
[00:37:02.360 --> 00:37:05.840]   And also humanoid football and capture the flag from deep mind.
[00:37:05.840 --> 00:37:08.560]   Lots of interesting behaviors emerging that we observed.
[00:37:08.560 --> 00:37:12.520]   Here's my favorite one.
[00:37:12.520 --> 00:37:15.680]   But these labels-- so here, these
[00:37:15.680 --> 00:37:19.040]   are labels that are provided by OpenAI, running and chasing,
[00:37:19.040 --> 00:37:21.760]   fort building, and ramp use.
[00:37:21.760 --> 00:37:25.360]   And these ones were that a human or humans
[00:37:25.360 --> 00:37:29.040]   went painstakingly, one by one, watch all these videos
[00:37:29.040 --> 00:37:31.600]   and label them manually.
[00:37:31.600 --> 00:37:34.800]   So our question is, is there a better way
[00:37:34.800 --> 00:37:37.440]   to discover these emergent behaviors?
[00:37:37.440 --> 00:37:39.800]   Perhaps some nice visualization can
[00:37:39.800 --> 00:37:44.960]   help us explore this complex domain a little better.
[00:37:44.960 --> 00:37:47.800]   So that's our goal.
[00:37:47.800 --> 00:37:50.240]   So in this work, we're going to, again,
[00:37:50.240 --> 00:37:52.800]   treat the agents like an observational study,
[00:37:52.800 --> 00:37:53.800]   like a new species.
[00:37:53.800 --> 00:37:55.800]   And we're going to do observational study.
[00:37:55.800 --> 00:37:57.600]   And what that means is that we only
[00:37:57.600 --> 00:37:59.800]   get to observe state and action pair.
[00:37:59.800 --> 00:38:02.600]   So where they are, what are they doing,
[00:38:02.600 --> 00:38:04.080]   what are they doing?
[00:38:04.080 --> 00:38:07.200]   And we're going to discover agent behavior
[00:38:07.200 --> 00:38:10.320]   by basically clustering the data.
[00:38:10.320 --> 00:38:12.600]   That's all we're going to do.
[00:38:12.600 --> 00:38:13.840]   And how do we do it?
[00:38:13.840 --> 00:38:15.600]   Pretty simple.
[00:38:15.600 --> 00:38:18.120]   Generative model-- have you covered the Bayesian
[00:38:18.120 --> 00:38:19.800]   generative model, graphical model?
[00:38:19.800 --> 00:38:20.360]   No, gotcha.
[00:38:20.360 --> 00:38:21.960]   OK.
[00:38:21.960 --> 00:38:22.640]   So think about--
[00:38:22.640 --> 00:38:23.640]   [INAUDIBLE]
[00:38:23.640 --> 00:38:24.840]   Hi.
[00:38:24.840 --> 00:38:27.200]   That also what you teach?
[00:38:27.200 --> 00:38:29.400]   Yeah, so this is a graphical model.
[00:38:29.400 --> 00:38:34.280]   Think about this as a fake or hypothetical data generation
[00:38:34.280 --> 00:38:35.160]   process.
[00:38:35.160 --> 00:38:36.360]   So how does this work?
[00:38:36.360 --> 00:38:37.800]   Like, I'm generating the data.
[00:38:37.800 --> 00:38:39.240]   I created this system.
[00:38:39.240 --> 00:38:43.240]   I'm going to first generate a joint latent embedding space
[00:38:43.240 --> 00:38:45.360]   that represents numbers, that represents
[00:38:45.360 --> 00:38:47.440]   all the behaviors in the system.
[00:38:47.440 --> 00:38:48.920]   And then for each agent, I'm going
[00:38:48.920 --> 00:38:51.480]   to generate another embedding.
[00:38:51.480 --> 00:38:55.360]   And each embedding, when it's conditioned with state,
[00:38:55.360 --> 00:38:57.160]   it's going to generate policy.
[00:38:57.160 --> 00:38:58.960]   It's going to decide what it's going to do,
[00:38:58.960 --> 00:39:02.400]   what action is given the state and the embedding pair.
[00:39:02.400 --> 00:39:05.240]   And then what that whole thing generates
[00:39:05.240 --> 00:39:08.280]   is what you see, the state and action pair.
[00:39:08.280 --> 00:39:09.560]   So how does this work?
[00:39:09.560 --> 00:39:11.880]   And then given this, you build a model.
[00:39:11.880 --> 00:39:15.200]   And you do inference to learn all these parameters.
[00:39:15.200 --> 00:39:16.880]   Kind of same business as neural network,
[00:39:16.880 --> 00:39:20.080]   but it's just have a little more structure.
[00:39:20.080 --> 00:39:21.800]   So this is completely made up, right?
[00:39:21.800 --> 00:39:26.160]   This is like my idea of how these new species might work.
[00:39:26.160 --> 00:39:27.240]   And our goal is to--
[00:39:27.240 --> 00:39:30.920]   we're going to try this and see if anything useful comes up.
[00:39:30.920 --> 00:39:33.320]   And the way you do this is-- one of the ways you do this
[00:39:33.320 --> 00:39:35.720]   is you optimize for a variation of lower bound.
[00:39:35.720 --> 00:39:36.880]   You don't need to know that.
[00:39:36.880 --> 00:39:38.920]   It's very cool, actually.
[00:39:38.920 --> 00:39:42.440]   If one gets into this exponential family business,
[00:39:42.440 --> 00:39:44.160]   very cool.
[00:39:44.160 --> 00:39:46.320]   CS228.
[00:39:46.320 --> 00:39:47.400]   OK.
[00:39:47.400 --> 00:39:49.600]   So here's one of the results that we had.
[00:39:49.600 --> 00:39:52.160]   It's a domain called MuJoCo.
[00:39:52.160 --> 00:39:54.960]   Here, we're going to pretend that we have two agents, one
[00:39:54.960 --> 00:39:57.800]   controlling back leg and one controlling the front leg.
[00:39:57.800 --> 00:40:00.760]   And on the right, we're showing that joint embedding space
[00:40:00.760 --> 00:40:03.040]   z omega and z alpha.
[00:40:03.040 --> 00:40:05.720]   While video is running, I'm going
[00:40:05.720 --> 00:40:08.640]   to try to put the video back.
[00:40:08.640 --> 00:40:14.080]   So now I'm going to select-- this is a visualization
[00:40:14.080 --> 00:40:16.400]   that we built online.
[00:40:16.400 --> 00:40:17.960]   You can go check it out.
[00:40:17.960 --> 00:40:21.440]   You can select a little space in agent 1 space.
[00:40:21.440 --> 00:40:25.120]   And you see it maps to pretty tight space in agent 0.
[00:40:25.120 --> 00:40:27.320]   And it shows pretty decent running ability.
[00:40:27.320 --> 00:40:28.760]   That's cool.
[00:40:28.760 --> 00:40:32.480]   And now I'm going to select somewhere else in agent 1
[00:40:32.480 --> 00:40:35.560]   that maps to kind of dispersed area in agent 0.
[00:40:35.560 --> 00:40:38.800]   It looks like it's not doing as well.
[00:40:38.800 --> 00:40:42.480]   And this is just an insight that we gain for this data only.
[00:40:42.480 --> 00:40:45.800]   But I was quickly able to identify, ah,
[00:40:45.800 --> 00:40:49.840]   this tight mapping business kind of represents
[00:40:49.840 --> 00:40:52.680]   the good running behavior and bad running behaviors.
[00:40:52.680 --> 00:40:55.200]   That's something that you can do pretty efficiently.
[00:40:55.200 --> 00:40:58.160]   And now I'm going to show you something more interesting.
[00:40:58.160 --> 00:41:00.960]   So of course, we have to do this because we have the data.
[00:41:00.960 --> 00:41:01.440]   It's here.
[00:41:01.440 --> 00:41:02.880]   It's so cool.
[00:41:02.880 --> 00:41:07.200]   So we apply this framework in the OpenAI's hide and seek.
[00:41:07.200 --> 00:41:08.640]   This has four agents.
[00:41:08.640 --> 00:41:10.120]   It looks like a simple game, but it
[00:41:10.120 --> 00:41:13.640]   has pretty complex structure, 100 dimensional observations,
[00:41:13.640 --> 00:41:15.520]   five dimensional action space.
[00:41:15.520 --> 00:41:18.200]   So in this work, remember that we
[00:41:18.200 --> 00:41:21.240]   pretend that we don't know the labels given by OpenAI.
[00:41:21.240 --> 00:41:24.360]   We just shuffle them in the mix.
[00:41:24.360 --> 00:41:26.480]   But we can color them, our results,
[00:41:26.480 --> 00:41:28.280]   with respect to their labels.
[00:41:28.280 --> 00:41:32.760]   So again, this is the result of z omega and z alpha.
[00:41:32.760 --> 00:41:33.960]   The individual agents.
[00:41:33.960 --> 00:41:36.340]   But the coloring is something that we didn't know before.
[00:41:36.340 --> 00:41:39.360]   We just did it after the fact.
[00:41:39.360 --> 00:41:41.120]   You can see in the z omega, there's
[00:41:41.120 --> 00:41:46.120]   nice kind of pattern that we can roughly separate what
[00:41:46.120 --> 00:41:48.840]   makes sense to humans and what makes sense to us.
[00:41:48.840 --> 00:41:53.560]   But remember, the green and gray, kind of everywhere,
[00:41:53.560 --> 00:41:54.400]   they're mixed.
[00:41:54.400 --> 00:41:58.440]   So in this particular run of OpenAI's hide and seek,
[00:41:58.440 --> 00:42:00.400]   it seemed that those two representations
[00:42:00.400 --> 00:42:03.360]   were kind of entangled.
[00:42:03.360 --> 00:42:05.400]   The running and chasing, the blue dots,
[00:42:05.400 --> 00:42:08.280]   it seems to be pretty separate and distinguishable
[00:42:08.280 --> 00:42:10.040]   from all the other colors.
[00:42:10.040 --> 00:42:11.920]   And that kind of makes sense, because that's
[00:42:11.920 --> 00:42:13.680]   basis of playing this game.
[00:42:13.680 --> 00:42:15.440]   So if you don't have that representation,
[00:42:15.440 --> 00:42:17.760]   you have a big trouble.
[00:42:17.760 --> 00:42:22.960]   But in case of orange, which is fort building,
[00:42:22.960 --> 00:42:26.220]   it's a lot more distinguishable in hiders.
[00:42:26.220 --> 00:42:28.160]   And that makes sense, because hiders are
[00:42:28.160 --> 00:42:30.400]   the ones building the fort.
[00:42:30.400 --> 00:42:31.720]   And seekers don't build the fort,
[00:42:31.720 --> 00:42:34.400]   so oranges are a little more entangled than seekers.
[00:42:34.400 --> 00:42:38.600]   Perhaps if seekers had built more separate fort building
[00:42:38.600 --> 00:42:40.720]   representation, maybe they would have won this game.
[00:42:40.720 --> 00:42:47.920]   So this work, can we learn something interesting,
[00:42:47.920 --> 00:42:51.960]   emerging behaviors by just simply observing the system?
[00:42:51.960 --> 00:42:54.080]   The answer seems to be yes, at least for the domains
[00:42:54.080 --> 00:42:55.200]   that we tested.
[00:42:55.200 --> 00:42:57.960]   A lot more complex domains should be tested.
[00:42:57.960 --> 00:43:01.280]   But these are the ones we had.
[00:43:01.280 --> 00:43:03.400]   But remember that these methods don't give you
[00:43:03.400 --> 00:43:04.720]   names of these clusters.
[00:43:04.720 --> 00:43:07.840]   So you would have to go and investigate and click
[00:43:07.840 --> 00:43:10.720]   through and explore.
[00:43:10.720 --> 00:43:14.720]   And if the cluster represents super human concept,
[00:43:14.720 --> 00:43:16.080]   this is not going to help you.
[00:43:16.080 --> 00:43:17.800]   And I'll talk a little more about a work
[00:43:17.800 --> 00:43:19.640]   that we do try to help them.
[00:43:19.640 --> 00:43:20.680]   But this is not for you.
[00:43:20.680 --> 00:43:23.440]   This is not going to help you there.
[00:43:23.440 --> 00:43:27.120]   And also, if you have access to the model and the reward
[00:43:27.120 --> 00:43:29.160]   signal, you should use it.
[00:43:29.160 --> 00:43:31.160]   Why dump it?
[00:43:31.160 --> 00:43:33.240]   So next work, we do use it.
[00:43:33.240 --> 00:43:37.000]   I'm going to talk about this work with Nico and Natasha
[00:43:37.000 --> 00:43:39.560]   and Shai again.
[00:43:39.560 --> 00:43:42.200]   So here, this time, we're going to intervene.
[00:43:42.200 --> 00:43:43.780]   We're going to be a little intrusive,
[00:43:43.780 --> 00:43:46.440]   but hopefully we'll learn a little more.
[00:43:46.440 --> 00:43:49.440]   So problem is that we're going to build a new multi-agent
[00:43:49.440 --> 00:43:52.080]   system, going to build it from scratch, such that we
[00:43:52.080 --> 00:43:53.560]   can do control testing.
[00:43:53.560 --> 00:43:54.960]   But at the same time, we shouldn't
[00:43:54.960 --> 00:43:56.520]   sacrifice the performance.
[00:43:56.520 --> 00:43:59.200]   So we're going to try to match the performance
[00:43:59.200 --> 00:44:00.760]   of the overall system.
[00:44:00.760 --> 00:44:03.360]   We do succeed.
[00:44:03.360 --> 00:44:05.840]   I had this paper collaboration with folks
[00:44:05.840 --> 00:44:08.520]   at Stanford, actually, here in 2020,
[00:44:08.520 --> 00:44:11.000]   where we proposed this pretty simple idea, which
[00:44:11.000 --> 00:44:13.240]   is you have a neural network.
[00:44:13.240 --> 00:44:17.080]   Why don't we embed concepts in the middle of the bottleneck,
[00:44:17.080 --> 00:44:19.240]   where one neuron represents trees,
[00:44:19.240 --> 00:44:21.560]   the other represents stripes, and just
[00:44:21.560 --> 00:44:23.720]   train the model end-to-end?
[00:44:23.720 --> 00:44:25.440]   And why are we doing this?
[00:44:25.440 --> 00:44:27.680]   Well, because then at inference time,
[00:44:27.680 --> 00:44:29.880]   you can actually intervene.
[00:44:29.880 --> 00:44:32.160]   You can pretend, you know, predicting zebra,
[00:44:32.160 --> 00:44:33.840]   I don't think trees should matter.
[00:44:33.840 --> 00:44:35.480]   So I'm going to zero out this neuron
[00:44:35.480 --> 00:44:37.480]   and feed forward and see what happens.
[00:44:37.480 --> 00:44:39.640]   So it's particularly useful in the medical setting,
[00:44:39.640 --> 00:44:41.840]   where there are some features that doctors don't want.
[00:44:41.840 --> 00:44:44.800]   We can cancel out and test.
[00:44:44.800 --> 00:44:48.720]   So this is the work to extend this to RL setting.
[00:44:48.720 --> 00:44:53.160]   It's actually not as simple extension as we thought.
[00:44:53.160 --> 00:44:54.920]   It came out to be pretty complex.
[00:44:54.920 --> 00:44:57.240]   But essentially, we're doing that.
[00:44:57.240 --> 00:44:59.880]   And we're building each of the concept bottleneck
[00:44:59.880 --> 00:45:02.120]   for each agent.
[00:45:02.120 --> 00:45:04.000]   And at the end of the day, what you optimize
[00:45:04.000 --> 00:45:06.360]   is what you usually do, typical PPO.
[00:45:06.360 --> 00:45:09.680]   Just think about this as make the auto system work,
[00:45:09.680 --> 00:45:13.360]   plus minimizing the difference between the true concept
[00:45:13.360 --> 00:45:14.840]   and estimated concept.
[00:45:14.840 --> 00:45:17.640]   That's all you do.
[00:45:17.640 --> 00:45:18.600]   Why are we doing this?
[00:45:18.600 --> 00:45:19.600]   You can intervene.
[00:45:19.600 --> 00:45:22.360]   You can pretend now agent two, pretend
[00:45:22.360 --> 00:45:24.480]   that you can't see agent one.
[00:45:24.480 --> 00:45:25.880]   What happens now?
[00:45:25.880 --> 00:45:29.320]   That's what we're doing here.
[00:45:29.320 --> 00:45:31.440]   We're going to do this in two domains.
[00:45:31.440 --> 00:45:37.560]   First domain, how many people saw this cooking game before?
[00:45:37.560 --> 00:45:41.320]   Yeah, it's a pretty commonly used cooking domain
[00:45:41.320 --> 00:45:43.880]   in reinforcement learning, very simple.
[00:45:43.880 --> 00:45:46.280]   We have two agents, yellow and blue.
[00:45:46.280 --> 00:45:48.080]   And they're going to make soup.
[00:45:48.080 --> 00:45:49.520]   They can bring three tomatoes.
[00:45:49.520 --> 00:45:50.800]   They get a word.
[00:45:50.800 --> 00:45:53.440]   They wait for the tomato and bring the dishes,
[00:45:53.440 --> 00:45:54.960]   a dish to the cooking pot.
[00:45:54.960 --> 00:45:56.240]   They get a reward finally.
[00:45:56.240 --> 00:45:59.040]   Their goal is to deliver as many soups as possible,
[00:45:59.040 --> 00:46:01.360]   given some time.
[00:46:01.360 --> 00:46:04.520]   And here, concepts that we use are agent position,
[00:46:04.520 --> 00:46:08.160]   orientation, agent has tomato, has dish, et cetera, et cetera.
[00:46:08.160 --> 00:46:11.280]   Something that's immediately available to you already.
[00:46:11.280 --> 00:46:13.600]   And you can, of course, tweak the environment
[00:46:13.600 --> 00:46:14.920]   to make it more fun.
[00:46:14.920 --> 00:46:18.160]   So you can make it that they have to collaborate.
[00:46:18.160 --> 00:46:19.680]   You can build a wall between them
[00:46:19.680 --> 00:46:21.440]   so that they have to work together in order
[00:46:21.440 --> 00:46:23.360]   to serve any tomato soup.
[00:46:23.360 --> 00:46:25.240]   Or you can make them freely available.
[00:46:25.240 --> 00:46:27.680]   You can work independently or together,
[00:46:27.680 --> 00:46:28.720]   whatever your choice.
[00:46:28.720 --> 00:46:34.160]   First, just kind of sanity check was
[00:46:34.160 --> 00:46:40.080]   that you can detect this emerging behavior of coordination
[00:46:40.080 --> 00:46:41.320]   versus non-coordination.
[00:46:41.320 --> 00:46:43.960]   So when the impassable environment,
[00:46:43.960 --> 00:46:45.840]   when we made up that environment,
[00:46:45.840 --> 00:46:48.960]   and suppose the RL system that we trained worked,
[00:46:48.960 --> 00:46:51.200]   they were able to deliver some soups,
[00:46:51.200 --> 00:46:53.120]   then you see that when we intervene--
[00:46:53.120 --> 00:46:54.440]   this graph, let me explain.
[00:46:54.440 --> 00:46:59.480]   This is a reward of an agent one when there's no intervention.
[00:46:59.480 --> 00:47:01.840]   So this is perfectly good world.
[00:47:01.840 --> 00:47:04.080]   And when there was an intervention.
[00:47:04.080 --> 00:47:07.480]   This is average value of intervening on all concepts.
[00:47:07.480 --> 00:47:10.800]   But I'm also going to show you each concept soon.
[00:47:10.800 --> 00:47:12.880]   If you compare left and right, you
[00:47:12.880 --> 00:47:16.120]   can tell that in the right, when we intervene,
[00:47:16.120 --> 00:47:19.800]   reward deteriorated quite a lot for both of them.
[00:47:19.800 --> 00:47:22.880]   And that's one way to see, ah, they are coordinating.
[00:47:22.880 --> 00:47:26.560]   Because somehow intervening at this concept
[00:47:26.560 --> 00:47:30.480]   impacted a lot of their performance.
[00:47:30.480 --> 00:47:33.240]   But this is what was really interesting to me,
[00:47:33.240 --> 00:47:34.280]   and I'm curious.
[00:47:34.280 --> 00:47:35.600]   Anyone can guess.
[00:47:35.600 --> 00:47:40.080]   So this is the same graph as the one you saw before,
[00:47:40.080 --> 00:47:44.000]   but except I'm plotting for intervention for each concept.
[00:47:44.000 --> 00:47:47.120]   So I'm intervening team position, team orientation,
[00:47:47.120 --> 00:47:49.960]   team has tomato, et cetera, et cetera.
[00:47:49.960 --> 00:47:52.640]   It turns out that they are using--
[00:47:52.640 --> 00:47:56.160]   or rather, when we intervene on team orientation,
[00:47:56.160 --> 00:47:58.760]   the degradation of performance was the biggest,
[00:47:58.760 --> 00:48:00.840]   to the extent that we believe that orientation had
[00:48:00.840 --> 00:48:04.000]   to do with sub-coordination.
[00:48:04.000 --> 00:48:06.440]   Anyone can guess why this might be?
[00:48:06.440 --> 00:48:13.960]   It's not the position.
[00:48:13.960 --> 00:48:14.880]   It's orientation.
[00:48:14.880 --> 00:48:15.880]   Yes?
[00:48:15.880 --> 00:48:17.880]   Just a clarification question on the orientation.
[00:48:17.880 --> 00:48:20.360]   Is that like the direction that the team is projecting?
[00:48:20.360 --> 00:48:21.360]   Yes.
[00:48:21.360 --> 00:48:25.800]   So it seems like orientation would let you predict
[00:48:25.800 --> 00:48:26.800]   where the moving heads?
[00:48:26.800 --> 00:48:27.920]   Yes, yes, that's right.
[00:48:27.920 --> 00:48:28.400]   Yes.
[00:48:28.400 --> 00:48:31.560]   Where were you when I was pulling my hair over this
[00:48:31.560 --> 00:48:32.920]   question?
[00:48:32.920 --> 00:48:34.040]   Yes, that's exactly right.
[00:48:34.040 --> 00:48:36.120]   And initially, I was really puzzled.
[00:48:36.120 --> 00:48:37.160]   Like, why not position?
[00:48:37.160 --> 00:48:38.840]   Because I expected to be positioned.
[00:48:38.840 --> 00:48:40.280]   But exactly, that's exactly right.
[00:48:40.280 --> 00:48:42.400]   So the orientation of the team is
[00:48:42.600 --> 00:48:44.000]   But exactly, that's exactly right.
[00:48:44.000 --> 00:48:47.280]   So the orientation is the first signal
[00:48:47.280 --> 00:48:51.760]   that an agent can get about the next move of the other agent.
[00:48:51.760 --> 00:48:54.200]   Because they're facing the pot, they're going to the pot.
[00:48:54.200 --> 00:48:55.600]   They're facing the tomato, they're
[00:48:55.600 --> 00:48:57.240]   going to get the tomato.
[00:48:57.240 --> 00:49:00.160]   Really interesting intuition.
[00:49:00.160 --> 00:49:05.600]   Obvious to some, but I needed this graph to work that out.
[00:49:05.600 --> 00:49:09.240]   And of course, you can use this to identify lazy agents.
[00:49:09.240 --> 00:49:14.160]   If you look at the rightmost yellow agent, our friend,
[00:49:14.160 --> 00:49:17.200]   just chilling in the background.
[00:49:17.200 --> 00:49:18.000]   And he's lazy.
[00:49:18.000 --> 00:49:19.560]   And if you train an RL agent, there's
[00:49:19.560 --> 00:49:21.560]   always some agents just hanging out.
[00:49:21.560 --> 00:49:23.560]   They just not do anything.
[00:49:23.560 --> 00:49:27.160]   And you can easily identify this by using this graph.
[00:49:27.160 --> 00:49:31.120]   If I intervene, it just doesn't impact any of their rewards.
[00:49:31.120 --> 00:49:32.080]   That one's me.
[00:49:32.080 --> 00:49:36.640]   So the second domain, we're going
[00:49:36.640 --> 00:49:39.080]   to look at a little more complex domain.
[00:49:39.080 --> 00:49:42.920]   So this is studying inter-agent social dynamics.
[00:49:42.920 --> 00:49:45.800]   So in this domain, there is a little bit of tension.
[00:49:45.800 --> 00:49:47.320]   This is called a cleanup.
[00:49:47.320 --> 00:49:49.160]   We have four agents.
[00:49:49.160 --> 00:49:51.560]   They only get rewards if they eat apples.
[00:49:51.560 --> 00:49:54.960]   Just yellow things, or green things are apples.
[00:49:54.960 --> 00:49:57.000]   But if you don't clean the river,
[00:49:57.000 --> 00:49:58.640]   then apple stops you all.
[00:49:58.640 --> 00:50:01.360]   So somebody has to clean the river.
[00:50:01.360 --> 00:50:04.560]   And you can see, if you have four people trying
[00:50:04.560 --> 00:50:07.760]   to collect apples, you can just stay someone else's--
[00:50:07.760 --> 00:50:09.960]   wait until someone else to clean the river
[00:50:09.960 --> 00:50:11.120]   and then collect the apples.
[00:50:11.120 --> 00:50:12.920]   And in fact, that's sometimes what happens.
[00:50:12.920 --> 00:50:20.440]   And concepts here, again, are pretty common things--
[00:50:20.440 --> 00:50:24.480]   position, orientation, and pollution, positions, et
[00:50:24.480 --> 00:50:26.560]   cetera.
[00:50:26.560 --> 00:50:30.920]   So when we first plotted the same graph
[00:50:30.920 --> 00:50:36.280]   as the previous domain, it tells a story.
[00:50:36.280 --> 00:50:41.920]   So the story here is that when I intervene on agent 1,
[00:50:41.920 --> 00:50:45.680]   it seems to influence agent 2 quite a lot,
[00:50:45.680 --> 00:50:49.920]   if you look at these three different graphs,
[00:50:49.920 --> 00:50:53.720]   how reward was impacted when I intervene on agent 1.
[00:50:53.720 --> 00:50:55.400]   It's agent 3 and 4 are fine, but it
[00:50:55.400 --> 00:50:57.320]   seems that only agent 2 is influenced.
[00:50:57.320 --> 00:51:00.720]   Same with idle time, same with the inter-agent distance.
[00:51:00.720 --> 00:51:03.360]   So we were like, oh, maybe that's true.
[00:51:03.360 --> 00:51:04.600]   But we keep wondering.
[00:51:04.600 --> 00:51:06.840]   There's a lot going on in this domain.
[00:51:06.840 --> 00:51:09.840]   How do we know this is the case?
[00:51:09.840 --> 00:51:13.280]   So we decided to take another step.
[00:51:13.280 --> 00:51:18.480]   So we're going to do a little more work here, but not a lot.
[00:51:18.480 --> 00:51:21.040]   We're going to build a graph to discover
[00:51:21.040 --> 00:51:22.960]   inter-agent relationships.
[00:51:22.960 --> 00:51:25.480]   This is simplest, dumbest way to build a graph.
[00:51:25.480 --> 00:51:27.320]   But again, I like simple things.
[00:51:27.320 --> 00:51:28.520]   So how do you build a graph?
[00:51:28.520 --> 00:51:30.640]   Well, suppose that you're building
[00:51:30.640 --> 00:51:31.680]   a graph between movies.
[00:51:31.680 --> 00:51:34.880]   This is not what we do, but just to describe
[00:51:34.880 --> 00:51:36.280]   what we're trying to do.
[00:51:36.280 --> 00:51:37.800]   We have each row.
[00:51:37.800 --> 00:51:39.480]   We're going to build a matrix.
[00:51:39.480 --> 00:51:42.640]   Each row is a movie, and each column
[00:51:42.640 --> 00:51:45.720]   consists of features of these movies, so length,
[00:51:45.720 --> 00:51:48.120]   genre of the movie, and so on.
[00:51:48.120 --> 00:51:52.040]   And the simplest way to build a graph is to do a regression.
[00:51:52.040 --> 00:51:56.440]   So exclude i-th row, and then we're
[00:51:56.440 --> 00:51:58.600]   going to regress over everyone else.
[00:51:58.600 --> 00:52:02.680]   And that gives me beta, which is a kind of coefficient
[00:52:02.680 --> 00:52:04.080]   for each of these.
[00:52:04.080 --> 00:52:08.920]   And that beta represents the strength of the edges.
[00:52:08.920 --> 00:52:11.040]   So this movie is more related to this movie and not
[00:52:11.040 --> 00:52:11.840]   the other movie.
[00:52:11.840 --> 00:52:13.040]   And ta-da, you have a graph.
[00:52:13.040 --> 00:52:14.160]   It's the dumbest way.
[00:52:14.160 --> 00:52:15.400]   There's a lot of caveats to it.
[00:52:15.400 --> 00:52:16.960]   You shouldn't do this a lot of times,
[00:52:16.960 --> 00:52:20.520]   but this is the simplest way to do it.
[00:52:20.520 --> 00:52:22.120]   So we did the same thing here.
[00:52:22.120 --> 00:52:27.760]   Instead of movie, we're going to use intervention on concept
[00:52:27.760 --> 00:52:31.400]   C on agent N as our node.
[00:52:31.400 --> 00:52:34.160]   And to build this matrix, we're going
[00:52:34.160 --> 00:52:36.440]   to use intervention outcome, which
[00:52:36.440 --> 00:52:39.760]   wouldn't have been available without our framework
[00:52:39.760 --> 00:52:45.240]   for reward, resource collected, and many other things.
[00:52:45.240 --> 00:52:47.440]   And when you build this graph, at the end of the day,
[00:52:47.440 --> 00:52:50.000]   you get betas that represent relationship
[00:52:50.000 --> 00:52:54.240]   between these interventions.
[00:52:54.240 --> 00:52:57.120]   So I had a graph of that matrix.
[00:52:57.120 --> 00:53:00.240]   Apparently, I removed before I came over.
[00:53:00.240 --> 00:53:03.320]   But imagine there was a matrix that
[00:53:03.320 --> 00:53:07.800]   is nicely highlighted between agent 1 and 4 and that only,
[00:53:07.800 --> 00:53:11.160]   contradicting the original hypothesis that we had.
[00:53:11.160 --> 00:53:13.320]   And this is the video of it.
[00:53:13.320 --> 00:53:15.640]   So when we stared at that matrix,
[00:53:15.640 --> 00:53:20.160]   it turns out that there is no high edge, strong edges
[00:53:20.160 --> 00:53:22.160]   between agent 1 and 2.
[00:53:22.160 --> 00:53:23.560]   So we were like, that's weird.
[00:53:23.560 --> 00:53:26.360]   But there is strong edges between agent 1 and 4.
[00:53:26.360 --> 00:53:30.280]   So we dig deeper into it, watched a lot of sessions
[00:53:30.280 --> 00:53:32.000]   to validate what's happening.
[00:53:32.000 --> 00:53:35.680]   And it turns out that the story was a lot more complicated.
[00:53:35.680 --> 00:53:39.200]   The 1's orientation was important for 4.
[00:53:39.200 --> 00:53:43.320]   But when that fails, agent 1 and 2 gets cornered in.
[00:53:43.320 --> 00:53:44.800]   And you can see that in the graph.
[00:53:44.800 --> 00:53:50.800]   Agent 4 get agent 1 and 2, blue and yellow agent,
[00:53:50.800 --> 00:53:51.960]   gets in the corner together.
[00:53:51.960 --> 00:53:53.480]   They get stuck.
[00:53:53.480 --> 00:53:55.840]   And this is simply just accidental
[00:53:55.840 --> 00:53:58.720]   because of the way that we built this environment.
[00:53:58.720 --> 00:54:00.880]   It just happened.
[00:54:00.880 --> 00:54:04.640]   But the raw statistics wouldn't have told us this story,
[00:54:04.640 --> 00:54:06.140]   that this was completely accidental.
[00:54:06.140 --> 00:54:08.680]   In fact, there was no correlation, no coordination
[00:54:08.680 --> 00:54:10.200]   between agent 1 and 2.
[00:54:10.200 --> 00:54:14.600]   But only after the graph, we realized this was the case.
[00:54:14.600 --> 00:54:16.640]   Now, this might be a one-off case.
[00:54:16.640 --> 00:54:17.440]   But you know what?
[00:54:17.440 --> 00:54:21.280]   A lot of emerging behaviors that we want to detect, a lot of them
[00:54:21.280 --> 00:54:22.640]   will be one-off case.
[00:54:22.640 --> 00:54:25.000]   And we really want to get to the truth of that
[00:54:25.000 --> 00:54:27.960]   rather than having some surface-level statistics.
[00:54:27.960 --> 00:54:34.920]   So can we build multi-agent system
[00:54:34.920 --> 00:54:37.240]   that enables intervention and performs as well?
[00:54:37.240 --> 00:54:38.160]   The answer is yes.
[00:54:38.160 --> 00:54:41.400]   There's a graph that shows the red line and blue line roughly
[00:54:41.400 --> 00:54:41.880]   aligned.
[00:54:41.880 --> 00:54:42.840]   That's good news.
[00:54:42.840 --> 00:54:45.480]   We are performing as well.
[00:54:45.480 --> 00:54:47.080]   But remember these concepts.
[00:54:47.080 --> 00:54:48.360]   You need to label them.
[00:54:48.360 --> 00:54:50.560]   Or you should have some way of getting those concepts,
[00:54:50.560 --> 00:54:52.240]   positions, and orientation.
[00:54:52.240 --> 00:54:53.880]   That might be something that we would
[00:54:53.880 --> 00:54:56.600]   love to extend in the future.
[00:54:56.600 --> 00:54:58.320]   Before I go on, any questions?
[00:54:58.320 --> 00:55:05.360]   You shy?
[00:55:05.360 --> 00:55:06.360]   You shy?
[00:55:06.360 --> 00:55:07.840]   [LAUGHS]
[00:55:07.840 --> 00:55:11.800]   Cool.
[00:55:11.800 --> 00:55:13.520]   All right.
[00:55:13.520 --> 00:55:16.880]   So I did tell you that we're not going to know,
[00:55:16.880 --> 00:55:18.720]   does the solution to move 37.
[00:55:18.720 --> 00:55:19.720]   I still don't.
[00:55:19.720 --> 00:55:20.960]   I still don't.
[00:55:20.960 --> 00:55:23.760]   But I'll tell you a little bit of work
[00:55:23.760 --> 00:55:27.440]   that I'm currently doing I'm really excited about.
[00:55:27.440 --> 00:55:29.840]   That we started thinking, you know what?
[00:55:29.840 --> 00:55:33.200]   Will this understanding move 37 happen before
[00:55:33.200 --> 00:55:34.560]   within my lifetime?
[00:55:34.560 --> 00:55:35.800]   And I was like, oh, maybe not.
[00:55:35.800 --> 00:55:37.680]   But I kind of want it to happen.
[00:55:37.680 --> 00:55:40.400]   So we start-- this is all about research, right?
[00:55:40.400 --> 00:55:42.360]   You started carving out a space where
[00:55:42.360 --> 00:55:44.320]   things are a little bit solvable.
[00:55:44.320 --> 00:55:46.320]   And you try to attack that problem.
[00:55:46.320 --> 00:55:48.720]   So this is our attempt to do exactly that,
[00:55:48.720 --> 00:55:51.760]   to get a little closer to our ultimate goal,
[00:55:51.760 --> 00:55:56.160]   my ultimate goal, of understanding that move 37.
[00:55:56.160 --> 00:55:59.600]   So before that, how many people here know AlphaZero from TMI?
[00:55:59.600 --> 00:56:00.800]   Yes.
[00:56:00.800 --> 00:56:06.240]   AlphaZero is a self-trained chess playing machine
[00:56:06.240 --> 00:56:07.360]   that beats--
[00:56:07.360 --> 00:56:10.320]   that has higher ELO rating than any other humans
[00:56:10.320 --> 00:56:13.000]   and beats Stockfish, which is arguably no existing
[00:56:13.000 --> 00:56:15.040]   human can beat Stockfish.
[00:56:15.040 --> 00:56:19.640]   So in a previous paper, we try to discover human chess
[00:56:19.640 --> 00:56:22.120]   concepts in this network.
[00:56:22.120 --> 00:56:26.320]   So when does concept like material imbalance
[00:56:26.320 --> 00:56:29.320]   appear in its network, which layer,
[00:56:29.320 --> 00:56:32.720]   and when in the training time, and which we call
[00:56:32.720 --> 00:56:35.140]   what, when, and where plots.
[00:56:35.140 --> 00:56:37.320]   And we also compare the evolution
[00:56:37.320 --> 00:56:40.240]   of opening moves between humans and AlphaZero.
[00:56:40.240 --> 00:56:42.120]   These are the first couple moves that you
[00:56:42.120 --> 00:56:43.800]   make when you play chess.
[00:56:43.800 --> 00:56:46.640]   And as you can see, there's a pretty huge difference.
[00:56:46.640 --> 00:56:49.920]   Left is human, right is AlphaZero.
[00:56:49.920 --> 00:56:54.480]   It turns out that AlphaZero can master, or supposedly master,
[00:56:54.480 --> 00:56:57.600]   a lot of variety of different types of openings.
[00:56:57.600 --> 00:56:59.320]   Openings can be very aggressive.
[00:56:59.320 --> 00:57:01.200]   Openings can be very boring.
[00:57:01.200 --> 00:57:03.640]   Could be very long range, targeting
[00:57:03.640 --> 00:57:06.440]   for long range strategy or short range.
[00:57:06.440 --> 00:57:07.400]   Very different.
[00:57:07.400 --> 00:57:10.960]   So that begs the question, what does AlphaZero know
[00:57:10.960 --> 00:57:12.400]   that humans don't know?
[00:57:12.400 --> 00:57:16.400]   Don't you want to learn what that might be?
[00:57:16.400 --> 00:57:18.000]   So that's what we're doing right now.
[00:57:18.000 --> 00:57:22.080]   We're actually almost-- we're about to evaluate.
[00:57:22.080 --> 00:57:27.120]   So the goal of this work is teach the world chess champion
[00:57:27.120 --> 00:57:31.080]   a new chess, superhuman chess strategy.
[00:57:31.080 --> 00:57:34.080]   And we just got yes from Magnus Carlsen, who
[00:57:34.080 --> 00:57:36.520]   is the world chess champion.
[00:57:36.520 --> 00:57:38.040]   He just lost the match, I know.
[00:57:38.040 --> 00:57:41.440]   But he's still champion in my mind.
[00:57:41.440 --> 00:57:44.400]   He's still champion in two categories, actually.
[00:57:44.400 --> 00:57:45.840]   So the way that we are doing this
[00:57:45.840 --> 00:57:49.320]   is we're going to discover new chess strategy
[00:57:49.320 --> 00:57:54.280]   by explicitly forgetting existing chess strategy, which
[00:57:54.280 --> 00:57:56.520]   we have a lot of data for.
[00:57:56.520 --> 00:57:59.160]   And then we're going to learn a graph, this time
[00:57:59.160 --> 00:58:03.720]   a little more complicated graph, by using
[00:58:03.720 --> 00:58:07.400]   the existing relationships between existing concepts
[00:58:07.400 --> 00:58:09.960]   so that we can get a little bit more idea of what
[00:58:09.960 --> 00:58:12.160]   the new concept might look like.
[00:58:12.160 --> 00:58:15.440]   And Magnus Carlsen-- so my favorite part about this work--
[00:58:15.440 --> 00:58:17.160]   I talk about carving out.
[00:58:17.160 --> 00:58:19.880]   My favorite part about this work is that the evaluation
[00:58:19.880 --> 00:58:21.520]   is going to be pretty clear.
[00:58:21.520 --> 00:58:23.640]   So it's not just like Magnus coming in and say,
[00:58:23.640 --> 00:58:25.960]   oh, your work is kind of nice, and say nice things
[00:58:25.960 --> 00:58:26.720]   about our work.
[00:58:26.720 --> 00:58:29.880]   No, Magnus actually has to solve some puzzles.
[00:58:29.880 --> 00:58:33.400]   And we will be able to evaluate him, whether he did it or not.
[00:58:33.400 --> 00:58:35.320]   So it's like a kind of success and fail.
[00:58:35.320 --> 00:58:36.400]   But I'm extremely excited.
[00:58:36.400 --> 00:58:40.280]   This kind of work I can only do because of Lisa,
[00:58:40.280 --> 00:58:45.400]   who is a champion herself, but also a PhD student at Oxford.
[00:58:45.400 --> 00:58:47.560]   And she played against Magnus in the past,
[00:58:47.560 --> 00:58:50.000]   and many other chess players in the world.
[00:58:50.000 --> 00:58:53.600]   And she's going to be the ultimate pre-superhuman
[00:58:53.600 --> 00:58:56.520]   filtering to filter out these concepts that
[00:58:56.520 --> 00:58:59.400]   will eventually get to Magnus.
[00:58:59.400 --> 00:59:00.760]   So I'm super excited about this.
[00:59:00.760 --> 00:59:02.520]   I have no results, but it's coming up.
[00:59:02.520 --> 00:59:03.240]   I'm excited.
[00:59:03.240 --> 00:59:03.740]   Yes?
[00:59:03.740 --> 00:59:17.660]   [INAUDIBLE]
[00:59:17.660 --> 00:59:19.140]   Puzzles are actually pretty simple.
[00:59:19.140 --> 00:59:22.300]   So the way that we generate concepts
[00:59:22.300 --> 00:59:25.460]   are within the embedding space of alpha 0.
[00:59:25.460 --> 00:59:29.660]   And given that, because alpha 0 has really weird architecture,
[00:59:29.660 --> 00:59:31.660]   so every single latent layer in alpha 0
[00:59:31.660 --> 00:59:33.900]   has the exact same position as a chessboard.
[00:59:33.900 --> 00:59:35.900]   That's just the way that they decide to do it.
[00:59:35.900 --> 00:59:37.420]   So because of that, we can actually
[00:59:37.420 --> 00:59:40.780]   identify or generate the board positions that
[00:59:40.780 --> 00:59:42.900]   corresponds to that concept.
[00:59:42.900 --> 00:59:46.540]   And because we have MCTS, we can predict
[00:59:46.540 --> 00:59:49.900]   what move it's going to make given that board position.
[00:59:49.900 --> 00:59:51.600]   Because at inference time, it's actually
[00:59:51.600 --> 00:59:53.980]   deterministic, a whole alpha 0 thing.
[00:59:53.980 --> 00:59:56.540]   So we have a lot of board positions.
[00:59:56.540 --> 00:59:58.380]   And that's all you need for puzzles.
[00:59:58.380 --> 01:00:01.580]   You give a board position and then ask Magnus to make a move.
[01:00:01.580 --> 01:00:04.660]   We explain the concept and then give Magnus more board
[01:00:04.660 --> 01:00:08.420]   positions and see if we can apply that concept that he just
[01:00:08.420 --> 01:00:09.420]   learned.
[01:00:09.420 --> 01:00:21.740]   [INAUDIBLE]
[01:00:21.740 --> 01:00:27.340]   Yeah, so if I were to ask Stockfish to solve those puzzles,
[01:00:27.340 --> 01:00:28.860]   that would be a different question.
[01:00:28.860 --> 01:00:31.780]   Because we are interested in whether we can teach human,
[01:00:31.780 --> 01:00:32.820]   not Stockfish.
[01:00:32.820 --> 01:00:34.200]   Stockfish might be able to do it.
[01:00:34.200 --> 01:00:36.980]   That's actually an interesting thing that we could do,
[01:00:36.980 --> 01:00:37.900]   now I think about it.
[01:00:37.900 --> 01:00:41.420]   But our goal is to just teach one superhuman.
[01:00:41.420 --> 01:00:45.580]   Like if I have, for example, 10,000 superhuman concepts,
[01:00:45.580 --> 01:00:49.180]   and only three of them are digestible by Magnus,
[01:00:49.180 --> 01:00:50.220]   that's a win.
[01:00:50.220 --> 01:00:52.700]   That would be a big win for this type of research.
[01:00:56.380 --> 01:00:56.880]   Questions?
[01:00:56.880 --> 01:01:04.540]   Yeah, so wrap up.
[01:01:04.540 --> 01:01:06.820]   Small steps towards our hopes and dreams.
[01:01:06.820 --> 01:01:09.820]   We talked about the gap between what machines know
[01:01:09.820 --> 01:01:12.180]   versus what we think machines know.
[01:01:12.180 --> 01:01:14.980]   Three ideas why that might be true.
[01:01:14.980 --> 01:01:16.780]   The three different maybe angles we
[01:01:16.780 --> 01:01:19.060]   can try to attack and answer those questions
[01:01:19.060 --> 01:01:21.340]   and bridge that gap.
[01:01:21.340 --> 01:01:25.140]   We talked about studying aliens, these machines,
[01:01:25.140 --> 01:01:27.380]   in observation study or control study.
[01:01:27.380 --> 01:01:30.460]   There are many other ways to study a species.
[01:01:30.460 --> 01:01:33.100]   And I'm not an expert, but anthropology and other humanities
[01:01:33.100 --> 01:01:36.580]   studies would know a lot better, more about this.
[01:01:36.580 --> 01:01:41.740]   And maybe, just maybe, we can try to understand MOVE 37
[01:01:41.740 --> 01:01:44.300]   at some point, hopefully within my lifetime,
[01:01:44.300 --> 01:01:48.580]   through this chess project that I'm very excited about.
[01:01:48.580 --> 01:01:49.080]   Thank you.
[01:01:49.560 --> 01:01:52.520]   [APPLAUSE]
[01:01:52.520 --> 01:01:58.480]   Thank you very much.
[01:01:58.480 --> 01:01:58.980]   Questions?
[01:01:58.980 --> 01:02:04.400]   You talked about interpretability research
[01:02:04.400 --> 01:02:07.440]   across NLP, vision, and RL.
[01:02:07.440 --> 01:02:09.640]   Do you think there's much hope for taking
[01:02:09.640 --> 01:02:12.000]   certain interpretability techniques from one modality
[01:02:12.000 --> 01:02:13.320]   into other modalities?
[01:02:13.320 --> 01:02:15.800]   And if so, what's the pathway?
[01:02:15.800 --> 01:02:16.300]   Hmm.
[01:02:17.300 --> 01:02:19.580]   So it depends on your goal.
[01:02:19.580 --> 01:02:22.420]   I think-- like, think about fairness research,
[01:02:22.420 --> 01:02:26.220]   which builds on strong mathematical foundation.
[01:02:26.220 --> 01:02:29.860]   And that's applicable for any questions around fairness,
[01:02:29.860 --> 01:02:31.340]   or hopefully applicable.
[01:02:31.340 --> 01:02:33.260]   But then, once you--
[01:02:33.260 --> 01:02:38.100]   if your goal is to actually solve a fairness issue at hand
[01:02:38.100 --> 01:02:40.020]   for somebody, the real person in the world,
[01:02:40.020 --> 01:02:41.660]   that's a completely different question.
[01:02:41.660 --> 01:02:44.060]   You would have to customize it for a particular person.
[01:02:44.060 --> 01:02:47.420]   You would have to customize it for a particular application.
[01:02:47.420 --> 01:02:48.420]   So there are two venues.
[01:02:48.420 --> 01:02:50.820]   And I think similar is true interpretability,
[01:02:50.820 --> 01:02:52.740]   like the theory work that I talked about.
[01:02:52.740 --> 01:02:57.300]   SHAP and IG are used across domains, like vision, texts.
[01:02:57.300 --> 01:03:00.700]   So that theory paper would be applicable across the domain.
[01:03:00.700 --> 01:03:02.700]   Things like RL and the way that we
[01:03:02.700 --> 01:03:05.120]   build that generative model, you would
[01:03:05.120 --> 01:03:07.340]   need to test a little bit more to make sure
[01:03:07.340 --> 01:03:09.620]   that this works in NLP.
[01:03:09.620 --> 01:03:12.700]   I don't even know how to think about agents in NLP yet.
[01:03:12.700 --> 01:03:14.380]   So we will need a little bit of tweaking.
[01:03:14.380 --> 01:03:16.340]   But both directions are fruitful.
[01:03:16.340 --> 01:03:22.660]   I want to have a question.
[01:03:22.660 --> 01:03:28.220]   So I saw some recent work in which some amateur Go players
[01:03:28.220 --> 01:03:30.820]   found a very tricky strategy to trick up.
[01:03:30.820 --> 01:03:32.580]   I think it was AlphaGo.
[01:03:32.580 --> 01:03:35.380]   And that seemed like a concept that humans know
[01:03:35.380 --> 01:03:37.940]   that machines don't in that Venn diagram.
[01:03:37.940 --> 01:03:40.060]   I just want to know your thoughts about that.
[01:03:40.060 --> 01:03:42.300]   Yeah, actually, it's funny you mentioned that.
[01:03:42.300 --> 01:03:46.700]   Lisa can beat AlphaZero pretty easily.
[01:03:46.700 --> 01:03:48.220]   And it's a similar idea.
[01:03:48.220 --> 01:03:52.100]   Because you kind of know what are the most unseen
[01:03:52.100 --> 01:03:53.660]   autodistribution moves are.
[01:03:53.660 --> 01:03:56.580]   And she can break AlphaZero pretty easily.
[01:03:56.580 --> 01:03:59.780]   And Lisa guessed that if Isador had known something more
[01:03:59.780 --> 01:04:03.620]   about AI, then maybe he would have tried to confuse AlphaGo.
[01:04:03.620 --> 01:04:06.420]   But the truth is, it's a high stake game.
[01:04:06.420 --> 01:04:10.180]   Like Isador is the famous star worldwide.
[01:04:10.180 --> 01:04:12.660]   So he wouldn't want to make a move that
[01:04:12.660 --> 01:04:15.220]   would be seen as a complete mistake,
[01:04:15.220 --> 01:04:17.380]   like the one that Magnus made a couple of days
[01:04:17.380 --> 01:04:19.460]   ago that got on the newsfeed everywhere,
[01:04:19.460 --> 01:04:21.620]   that he made this century-wide mistake.
[01:04:21.620 --> 01:04:23.500]   And that probably hurts.
[01:04:23.500 --> 01:04:28.220]   Any other questions?
[01:04:28.220 --> 01:04:28.700]   [INAUDIBLE]
[01:04:28.700 --> 01:04:53.180]   [INAUDIBLE]
[01:04:53.180 --> 01:04:56.500]   These work that I've presented are pretty new.
[01:04:56.500 --> 01:04:59.980]   But there has been a bit of discussion in the robotics,
[01:04:59.980 --> 01:05:02.140]   applying potentially just to robotics.
[01:05:02.140 --> 01:05:03.980]   And of course, I can't talk about details.
[01:05:03.980 --> 01:05:10.220]   But things that reinforcement learning in the wild people
[01:05:10.220 --> 01:05:13.380]   worry about are some of the surprises.
[01:05:13.380 --> 01:05:16.660]   If you have a test for it, like if you have a unit test for it,
[01:05:16.660 --> 01:05:18.100]   you're never going to fail.
[01:05:18.100 --> 01:05:20.820]   Because you're going to test before you deploy.
[01:05:20.820 --> 01:05:24.220]   I think the biggest risk for any of these deployment systems
[01:05:24.220 --> 01:05:27.700]   is the surprises that you didn't expect.
[01:05:27.700 --> 01:05:30.740]   So my work around the visualization and others
[01:05:30.740 --> 01:05:33.540]   aim to help you with that.
[01:05:33.540 --> 01:05:36.780]   So we may not know names of these surprises.
[01:05:36.780 --> 01:05:39.420]   But here's a tool that helps you better discover
[01:05:39.420 --> 01:05:41.620]   those surprises before someone else does
[01:05:41.620 --> 01:05:42.860]   or someone else gets harmed.
[01:05:42.860 --> 01:05:51.060]   Thanks so much for the talk.
[01:05:51.060 --> 01:05:52.980]   This is kind of an open-ended question.
[01:05:52.980 --> 01:05:54.500]   But I was wondering, we're talking
[01:05:54.500 --> 01:05:59.260]   about a lot of ways in which we try to visualize or understand
[01:05:59.260 --> 01:06:01.060]   what's going on in the representation inside
[01:06:01.060 --> 01:06:02.300]   the machine.
[01:06:02.300 --> 01:06:04.820]   But I was wondering whether we could turn it around
[01:06:04.820 --> 01:06:08.260]   and try to teach machines to tell us what--
[01:06:08.260 --> 01:06:11.900]   using our language, what they're doing in their representations.
[01:06:11.900 --> 01:06:13.620]   Like, if we build representations of ours
[01:06:13.620 --> 01:06:16.620]   and then get the machine to do the translation for us
[01:06:16.620 --> 01:06:19.140]   instead of us going into the machine to see it.
[01:06:19.140 --> 01:06:20.300]   Yeah, great question.
[01:06:20.300 --> 01:06:21.940]   So it's a really interesting question.
[01:06:21.940 --> 01:06:26.900]   Because that's something that I kind of tried in my work,
[01:06:26.900 --> 01:06:29.860]   previous work, called Testing with Concept Activation
[01:06:29.860 --> 01:06:30.420]   Vectors.
[01:06:30.420 --> 01:06:34.260]   So that was to map human language into a machine's space
[01:06:34.260 --> 01:06:36.020]   so that they can only speak our language.
[01:06:36.020 --> 01:06:37.860]   Because I understand my language and just
[01:06:37.860 --> 01:06:39.740]   talk to me in my language.
[01:06:39.740 --> 01:06:42.620]   The challenge is that, how would you do that for something
[01:06:42.620 --> 01:06:44.340]   like alpha 0?
[01:06:44.340 --> 01:06:48.340]   Like, we don't have a vocabulary for it, like move 37.
[01:06:48.340 --> 01:06:51.940]   Then there is going to be a lot of missing valuable knowledge
[01:06:51.940 --> 01:06:55.140]   that we might not get from the machine.
[01:06:55.140 --> 01:06:57.940]   So I think the approach has to be both ways.
[01:06:57.940 --> 01:06:59.460]   We should leverage as much as we can.
[01:06:59.460 --> 01:07:02.740]   But acknowledging that, even that mapping,
[01:07:02.740 --> 01:07:05.380]   that trying to map our language to machines,
[01:07:05.380 --> 01:07:07.700]   is not going to be perfect.
[01:07:07.700 --> 01:07:11.940]   Because it's a kind of proxy for what we think a penguin is.
[01:07:11.940 --> 01:07:13.820]   There's a psychology research that says,
[01:07:13.820 --> 01:07:16.420]   everyone thinks very differently about what a penguin is.
[01:07:16.420 --> 01:07:19.300]   Like, if I take a picture of a penguin,
[01:07:19.300 --> 01:07:22.420]   everyone is thinking different penguin right now.
[01:07:22.420 --> 01:07:25.500]   Australia has the cutest penguin, the fairy penguin.
[01:07:25.500 --> 01:07:26.620]   I'm thinking that, right?
[01:07:26.620 --> 01:07:28.740]   But I don't know how many people are thinking that.
[01:07:28.740 --> 01:07:31.100]   So given that, we are so different,
[01:07:31.100 --> 01:07:32.820]   machine's going to think something else.
[01:07:32.820 --> 01:07:34.260]   So how do you bridge that gap?
[01:07:34.260 --> 01:07:37.860]   Extend that to 100 concepts and composing those concepts,
[01:07:37.860 --> 01:07:40.180]   it's going to go out of wild very soon.
[01:07:40.180 --> 01:07:41.740]   So there's pros and cons.
[01:07:41.740 --> 01:07:43.220]   I'm into both of them.
[01:07:43.220 --> 01:07:48.220]   I think some applications, exclusively just using
[01:07:48.220 --> 01:07:50.300]   human concepts are still very helpful.
[01:07:50.300 --> 01:07:53.180]   It gets you halfway.
[01:07:53.180 --> 01:07:56.460]   But my ambition is that we shouldn't stop there.
[01:07:56.460 --> 01:08:00.900]   We should benefit from them by having them teach us new things
[01:08:00.900 --> 01:08:02.060]   that we didn't know before.
[01:08:02.060 --> 01:08:05.580]   Yeah?
[01:08:05.580 --> 01:08:07.740]   So the second thing you talked about with Jerome
[01:08:07.740 --> 01:08:11.460]   was that where knowledge is located in the embedding space
[01:08:11.460 --> 01:08:13.860]   isn't super correlated with what you'd like to edit
[01:08:13.860 --> 01:08:15.180]   to change that knowledge.
[01:08:15.180 --> 01:08:17.940]   Do you think that has any implications for the later
[01:08:17.940 --> 01:08:19.940]   stuff you talked about, like the cost thing?
[01:08:19.940 --> 01:08:23.780]   But I don't know, like trying to locate,
[01:08:23.780 --> 01:08:25.900]   like to just get strategies in the embedding space
[01:08:25.900 --> 01:08:27.420]   might not be as helpful?
[01:08:27.420 --> 01:08:30.100]   Oh, what are the alternatives?
[01:08:30.100 --> 01:08:31.740]   I guess I don't know the alternatives,
[01:08:31.740 --> 01:08:35.540]   just because I feel like the Roam thing as well is not--
[01:08:35.540 --> 01:08:36.260]   That's possible.
[01:08:36.260 --> 01:08:40.060]   So it's like some transformed space of our embedding space
[01:08:40.060 --> 01:08:42.860]   in alpha 0, maybe it's a function applied
[01:08:42.860 --> 01:08:44.340]   to that embedding space.
[01:08:44.340 --> 01:08:49.180]   So thinking about that as a raw vector is a dead end.
[01:08:49.180 --> 01:08:50.180]   Could be.
[01:08:50.180 --> 01:08:52.420]   We'll see how this chess project goes.
[01:08:52.420 --> 01:08:56.020]   In a couple of months, I might rethink my strategy.
[01:08:56.020 --> 01:08:57.820]   But interesting thought.
[01:08:57.820 --> 01:08:58.660]   Yeah?
[01:08:58.660 --> 01:09:00.460]   So I'm a psychology major, and I do
[01:09:00.460 --> 01:09:03.100]   realize that a lot of this stuff that we're trying to do here,
[01:09:03.100 --> 01:09:04.500]   like reasoning parts of the game,
[01:09:04.500 --> 01:09:07.860]   is how we figure out how our brains work.
[01:09:07.860 --> 01:09:10.180]   So do you think that this--
[01:09:10.180 --> 01:09:13.460]   would there be stuff that moves that's
[01:09:13.460 --> 01:09:15.180]   applicable to neural networks?
[01:09:15.180 --> 01:09:16.820]   And on the contrary, do you think
[01:09:16.820 --> 01:09:18.260]   there must be this interpretability
[01:09:18.260 --> 01:09:21.220]   of a study of neural network to help us understand stuff
[01:09:21.220 --> 01:09:22.500]   about our own brain?
[01:09:22.500 --> 01:09:23.260]   Yeah.
[01:09:23.260 --> 01:09:25.420]   Talk to Geoff Hinton.
[01:09:25.420 --> 01:09:26.580]   He would really like this.
[01:09:26.580 --> 01:09:28.260]   So I believe-- I mean, you probably
[01:09:28.260 --> 01:09:29.300]   know about this history.
[01:09:29.300 --> 01:09:31.580]   I think that's how it all started, right?
[01:09:31.580 --> 01:09:37.140]   The whole neural network is to understand human brain.
[01:09:37.140 --> 01:09:39.540]   So that's the answer to your question.
[01:09:39.540 --> 01:09:41.780]   Interesting, however, in my view,
[01:09:41.780 --> 01:09:46.580]   there is some biases that we have in neuroscience
[01:09:46.580 --> 01:09:48.420]   because of the limitations of tools,
[01:09:48.420 --> 01:09:50.700]   like physical tools and availability of humans
[01:09:50.700 --> 01:09:52.100]   that you can poke in.
[01:09:52.100 --> 01:09:54.660]   I think that influences interpretability research.
[01:09:54.660 --> 01:09:56.540]   And I'll give you an example of what I mean.
[01:09:56.540 --> 01:10:00.580]   So in cat, the horizontal line and vertical line neuron
[01:10:00.580 --> 01:10:03.460]   in cat brain, so they put the prop in and figure out
[01:10:03.460 --> 01:10:05.540]   this one neuron detects vertical lines.
[01:10:05.540 --> 01:10:07.180]   And you can validate it.
[01:10:07.180 --> 01:10:08.900]   It's really cool if you look at the video.
[01:10:08.900 --> 01:10:10.260]   The video is still online.
[01:10:10.260 --> 01:10:11.220]   Yeah, what is it?
[01:10:11.220 --> 01:10:12.420]   [INAUDIBLE]
[01:10:12.420 --> 01:10:14.020]   Yes, yes, yes.
[01:10:14.020 --> 01:10:15.780]   So why did they do that?
[01:10:15.780 --> 01:10:20.100]   Well, because you had one cat, poor, poor cat.
[01:10:20.100 --> 01:10:25.340]   And you had-- we can only probe a few neurons at a time, right?
[01:10:25.340 --> 01:10:28.700]   So that implied a lot of-- a few interpretability research
[01:10:28.700 --> 01:10:29.900]   actually looked at--
[01:10:29.900 --> 01:10:32.900]   are very focused on neuron-wise representation.
[01:10:32.900 --> 01:10:35.260]   This one neuron must be very special.
[01:10:35.260 --> 01:10:36.940]   I actually think that's not true.
[01:10:36.940 --> 01:10:40.100]   That was limited by our ability, like physical ability
[01:10:40.100 --> 01:10:41.140]   to plop organisms.
[01:10:41.140 --> 01:10:43.140]   But in your network, you don't have to do that.
[01:10:43.140 --> 01:10:44.860]   You can apply functions to embeddings.
[01:10:44.860 --> 01:10:47.020]   You can change the whole embedding to something else,
[01:10:47.020 --> 01:10:48.060]   overwrite.
[01:10:48.060 --> 01:10:54.100]   So that kind of is actually a obstacle in our thinking
[01:10:54.100 --> 01:10:54.900]   rather than helping.
[01:10:54.900 --> 01:11:03.460]   OK, maybe we should call it there.
[01:11:03.460 --> 01:11:08.740]   So for Thursday, we're not having a lecture on Thursday.
[01:11:08.740 --> 01:11:11.020]   There'll be TAs and me here.
[01:11:11.020 --> 01:11:14.860]   So if you have any last-minute panics on your project
[01:11:14.860 --> 01:11:18.700]   or think we might have some great insight to help you,
[01:11:18.700 --> 01:11:19.860]   we probably won't actually.
[01:11:19.860 --> 01:11:22.220]   It'll be all right.
[01:11:22.220 --> 01:11:25.940]   Do come along, and you can chat to us as we find projects
[01:11:25.940 --> 01:11:27.860]   and we can give you help.
[01:11:27.860 --> 01:11:29.740]   That means that Dean actually got
[01:11:29.740 --> 01:11:33.460]   to give the final lecture of CS224 in today.
[01:11:33.460 --> 01:11:35.500]   So a round of applause for him.
[01:11:35.500 --> 01:11:38.060]   [APPLAUSE]
[01:11:38.060 --> 01:11:48.060]   [BLANK_AUDIO]

