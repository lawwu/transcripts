
[00:00:00.000 --> 00:00:02.720]   In this video, we're going to introduce two new things.
[00:00:02.720 --> 00:00:06.160]   So one is a technique, a very awesome general technique
[00:00:06.160 --> 00:00:07.920]   called data augmentation.
[00:00:07.920 --> 00:00:10.240]   And we're also going to introduce a new data
[00:00:10.240 --> 00:00:13.000]   set called CIFAR, which is kind of like the next data set
[00:00:13.000 --> 00:00:14.600]   that everyone uses after MNIST.
[00:00:14.600 --> 00:00:17.400]   It's slightly bigger images, and it's color images,
[00:00:17.400 --> 00:00:19.800]   and it's images of kind of everyday objects.
[00:00:19.800 --> 00:00:21.720]   But these images are still small,
[00:00:21.720 --> 00:00:24.760]   and it's still a relatively easy classification task.
[00:00:24.760 --> 00:00:27.440]   In fact, these days, the best classifiers
[00:00:27.440 --> 00:00:30.160]   get up to 99% accuracy, which is really amazing.
[00:00:30.160 --> 00:00:33.680]   It's definitely better accuracy than I could get personally.
[00:00:33.680 --> 00:00:36.160]   Data augmentation generally is the practice
[00:00:36.160 --> 00:00:38.880]   of taking your data and modifying it.
[00:00:38.880 --> 00:00:41.040]   So this has a lot of different advantages.
[00:00:41.040 --> 00:00:44.080]   One advantage is that it helps the model generalize
[00:00:44.080 --> 00:00:45.080]   the data better.
[00:00:45.080 --> 00:00:47.800]   So in cases where you don't have enough training data,
[00:00:47.800 --> 00:00:50.160]   you can sort of get more training data
[00:00:50.160 --> 00:00:52.440]   by taking things you have and changing it a little bit.
[00:00:52.440 --> 00:00:53.920]   So one of the things you do might
[00:00:53.920 --> 00:00:58.200]   be rotate your images maybe like five degrees randomly.
[00:00:58.200 --> 00:01:02.880]   And you know that a frog rotated five degrees is still a frog.
[00:01:02.880 --> 00:01:07.760]   Or a frog flipped across a horizontal axis--
[00:01:07.760 --> 00:01:10.400]   I mean, a vertical axis is probably still a frog, right?
[00:01:10.400 --> 00:01:12.440]   So we do all these transformations
[00:01:12.440 --> 00:01:16.760]   before we send the data through to get classified.
[00:01:16.760 --> 00:01:19.000]   We probably wouldn't do this on our testing data,
[00:01:19.000 --> 00:01:22.000]   but on the training data, it's totally fair game.
[00:01:22.000 --> 00:01:24.720]   And the particular transformations you do
[00:01:24.720 --> 00:01:27.120]   are really based upon your knowledge of your data set.
[00:01:27.120 --> 00:01:30.040]   How much can you really distort an image
[00:01:30.040 --> 00:01:32.160]   before it becomes potentially something different?
[00:01:32.160 --> 00:01:36.040]   It's something to think about and apply domain knowledge to.
[00:01:36.040 --> 00:01:39.200]   Also, different types of images might
[00:01:39.200 --> 00:01:41.200]   have different appropriate transformations, right?
[00:01:41.200 --> 00:01:45.600]   So a face turned upside down might be really different.
[00:01:45.600 --> 00:01:47.280]   It might be really maybe not a kind
[00:01:47.280 --> 00:01:48.640]   of face you want to classify.
[00:01:48.640 --> 00:01:51.560]   But a satellite image rotated 180 degrees
[00:01:51.560 --> 00:01:54.000]   is probably still a satellite image of the same thing
[00:01:54.000 --> 00:01:55.080]   that you might really see.
[00:01:55.080 --> 00:01:58.640]   So aerial photography, you can do kind of more rotations
[00:01:58.640 --> 00:02:00.320]   and more vertical flipping.
[00:02:00.320 --> 00:02:02.880]   Whereas images from an upright camera,
[00:02:02.880 --> 00:02:05.240]   you probably wouldn't want to do too much rotation
[00:02:05.240 --> 00:02:06.760]   around the vertical axis.
[00:02:06.760 --> 00:02:10.120]   Because in the real 3D world, the vertical axis
[00:02:10.120 --> 00:02:12.720]   is something that we care about.
[00:02:12.720 --> 00:02:16.120]   So as usual, we start off with a whole bunch of imports,
[00:02:16.120 --> 00:02:19.240]   especially the Keras library that we use extensively here.
[00:02:19.240 --> 00:02:21.440]   And if you have any questions about what
[00:02:21.440 --> 00:02:23.880]   I'm doing up until the data generation,
[00:02:23.880 --> 00:02:27.040]   I really recommend going back and reviewing previous videos
[00:02:27.040 --> 00:02:29.220]   on convolutional neural networks and maybe even
[00:02:29.220 --> 00:02:32.920]   the video on perceptrons and multilayer perceptrons.
[00:02:32.920 --> 00:02:36.680]   So first, we import Keras and some other useful tools.
[00:02:36.680 --> 00:02:39.000]   So then we set some configuration parameters
[00:02:39.000 --> 00:02:41.000]   and we download the CIFAR library.
[00:02:41.000 --> 00:02:42.600]   There's actually two versions of CIFAR.
[00:02:42.600 --> 00:02:44.920]   There's CIFAR 10, which has 10 classes,
[00:02:44.920 --> 00:02:46.680]   and CIFAR 100, which is a little harder
[00:02:46.680 --> 00:02:48.640]   because it has 100 classes.
[00:02:48.640 --> 00:02:51.040]   In this case, I've actually spelled out the class names
[00:02:51.040 --> 00:02:51.540]   for you.
[00:02:51.540 --> 00:02:56.840]   It's airplane, automobile, bird, cat, deer, dog, frog, horse,
[00:02:56.840 --> 00:02:58.760]   ship, and truck.
[00:02:58.760 --> 00:03:02.340]   Very similar to MNIST, we set those variables
[00:03:02.340 --> 00:03:04.800]   in X-train, Y-train, X-test, and Y-test.
[00:03:04.800 --> 00:03:08.560]   So X-train is the actual pixels of each image
[00:03:08.560 --> 00:03:10.360]   that we're dealing with in our training set,
[00:03:10.360 --> 00:03:13.760]   and Y-train is the labels, and so on.
[00:03:13.760 --> 00:03:16.000]   As usual, I think it's smart to take a look at a few
[00:03:16.000 --> 00:03:18.640]   of the images and get a feel for the data
[00:03:18.640 --> 00:03:20.040]   that we're working with before we
[00:03:20.040 --> 00:03:21.360]   start to try to classify it.
[00:03:21.360 --> 00:03:24.840]   So here I've plotted X-train 1, which is probably
[00:03:24.840 --> 00:03:27.600]   the second image in our X-train set.
[00:03:27.600 --> 00:03:29.520]   And you can see that it's probably a truck,
[00:03:29.520 --> 00:03:30.980]   but even it's kind of hard to tell.
[00:03:30.980 --> 00:03:34.040]   It's at such low resolution that this task is fairly hard,
[00:03:34.040 --> 00:03:36.000]   even for a human.
[00:03:36.000 --> 00:03:40.280]   We can look at, say, X-train 10 and see that I think that's
[00:03:40.280 --> 00:03:42.560]   a deer, but it's quite dim.
[00:03:42.560 --> 00:03:46.440]   Some of these images actually are tricky.
[00:03:46.440 --> 00:03:50.680]   Next, we one-hot encode our output labels,
[00:03:50.680 --> 00:03:52.840]   and we normalize our images.
[00:03:52.840 --> 00:03:55.360]   So instead of-- they come by default
[00:03:55.360 --> 00:03:58.240]   with their RGB values between 0 and 255,
[00:03:58.240 --> 00:04:01.560]   and we want to change that to be between 0 and 1.
[00:04:01.560 --> 00:04:02.920]   So we do that.
[00:04:02.920 --> 00:04:07.320]   And then we set up a kind of standard small convolutional
[00:04:07.320 --> 00:04:08.880]   network in Keras.
[00:04:08.880 --> 00:04:10.400]   So it's a sequential model, and we
[00:04:10.400 --> 00:04:14.040]   use a 2D convolution with a 3 by 3 convolution.
[00:04:14.040 --> 00:04:16.480]   And we use the padding to kind of keep the size the same.
[00:04:16.480 --> 00:04:19.640]   So we pad out a little bit to make sure the convolution
[00:04:19.640 --> 00:04:21.240]   doesn't change the dimension.
[00:04:21.240 --> 00:04:23.760]   And then we use the ReLU activation function.
[00:04:23.760 --> 00:04:26.720]   We do the standard max pooling, flatten things out,
[00:04:26.720 --> 00:04:28.160]   and then have two dense layers.
[00:04:28.160 --> 00:04:30.560]   And we've inserted two dropout layers
[00:04:30.560 --> 00:04:32.000]   to help with overfitting.
[00:04:32.000 --> 00:04:34.040]   And this is exactly networks that we
[00:04:34.040 --> 00:04:37.480]   looked at in the earlier convolution section
[00:04:37.480 --> 00:04:38.660]   that we did on MNIST.
[00:04:38.660 --> 00:04:41.000]   The only difference here, actually, is that our input
[00:04:41.000 --> 00:04:43.360]   data is three-dimensional instead of two-dimensional,
[00:04:43.360 --> 00:04:46.240]   because our input images are actually colored images.
[00:04:46.240 --> 00:04:48.120]   So color here is the third dimension,
[00:04:48.120 --> 00:04:50.320]   whereas with MNIST, they're black and white images.
[00:04:50.320 --> 00:04:53.660]   So it's a two-dimensional input data.
[00:04:53.660 --> 00:04:57.520]   Next, we compile the model with the usual categorical cross
[00:04:57.520 --> 00:05:00.520]   entropy, because we're doing a multi-class classification
[00:05:00.520 --> 00:05:02.760]   in the Atom Optimizer.
[00:05:02.760 --> 00:05:05.000]   And then before we start getting crazy with the data
[00:05:05.000 --> 00:05:09.140]   augmentation, let's just fit this model on the basic data.
[00:05:09.140 --> 00:05:11.020]   And we can run this.
[00:05:11.020 --> 00:05:14.060]   And you'll see that it gets reasonably good accuracy.
[00:05:14.060 --> 00:05:20.100]   You can see that it gets reasonably good accuracy.
[00:05:20.100 --> 00:05:21.980]   The validation accuracy here is actually
[00:05:21.980 --> 00:05:24.140]   higher generally than the accuracy,
[00:05:24.140 --> 00:05:25.580]   because we have dropout.
[00:05:25.580 --> 00:05:27.260]   But it looks like the validation accuracy
[00:05:27.260 --> 00:05:31.540]   tops out at around 62%, 63%, which is reasonably respectable.
[00:05:31.540 --> 00:05:34.020]   But we only have 50,000 training examples,
[00:05:34.020 --> 00:05:36.860]   so it does seem like this could benefit potentially
[00:05:36.860 --> 00:05:39.580]   from augmenting the data.
[00:05:39.580 --> 00:05:41.140]   So let's do that.
[00:05:41.140 --> 00:05:42.860]   And Keras actually makes it quite easy
[00:05:42.860 --> 00:05:45.740]   to do data augmentation.
[00:05:45.740 --> 00:05:48.980]   And we do this by setting up an image data generator.
[00:05:48.980 --> 00:05:52.780]   Now, there's two reasons to use data generators in general.
[00:05:52.780 --> 00:05:54.900]   So what this does is instead of setting a variable
[00:05:54.900 --> 00:05:57.900]   with all the data, it has a function
[00:05:57.900 --> 00:06:01.820]   that will return batches of training data examples.
[00:06:01.820 --> 00:06:03.580]   So one reason to use it, and the reason
[00:06:03.580 --> 00:06:05.740]   we're using it here, is because it
[00:06:05.740 --> 00:06:09.100]   lets us do modifications to the data as we pass it through.
[00:06:09.100 --> 00:06:10.780]   So this data generator is actually
[00:06:10.780 --> 00:06:12.460]   going to take the images, and it's
[00:06:12.460 --> 00:06:15.540]   going to stretch them and squeeze them and rotate them.
[00:06:15.540 --> 00:06:18.220]   And all these things, it can do because it's doing a generator.
[00:06:18.220 --> 00:06:20.660]   Each time, it'll be a little bit different.
[00:06:20.660 --> 00:06:22.740]   The other thing that a generator can do
[00:06:22.740 --> 00:06:24.400]   is actually just prevent you from having
[00:06:24.400 --> 00:06:25.940]   to put all your images in memory.
[00:06:25.940 --> 00:06:28.180]   And I'll tell you, when you work on real-world data sets,
[00:06:28.180 --> 00:06:29.260]   that becomes a big issue.
[00:06:29.260 --> 00:06:31.340]   So in most production systems, you'll
[00:06:31.340 --> 00:06:34.700]   probably be using a data generator versus just passing
[00:06:34.700 --> 00:06:39.980]   in gigantic variables into your fit function.
[00:06:39.980 --> 00:06:41.600]   So a generator is good to learn anyway,
[00:06:41.600 --> 00:06:43.580]   but great for this example.
[00:06:43.580 --> 00:06:45.860]   And here, I've set width shift range
[00:06:45.860 --> 00:06:47.740]   to 0.1, which means that it's going
[00:06:47.740 --> 00:06:50.300]   to shift the width of the image by up to 10%.
[00:06:50.300 --> 00:06:53.620]   So it's going to expand and squeeze the image by up
[00:06:53.620 --> 00:06:54.300]   to 10%.
[00:06:54.300 --> 00:06:56.580]   But it's cool to actually look at the characterization
[00:06:56.580 --> 00:06:58.000]   and see if there's any other things
[00:06:58.000 --> 00:06:59.400]   that we might want to try.
[00:06:59.400 --> 00:07:03.040]   Like, for example, we might want to do a little bit of rotation
[00:07:03.040 --> 00:07:03.720]   on the image.
[00:07:03.720 --> 00:07:08.040]   Or another thing that we could do is horizontal flip.
[00:07:08.040 --> 00:07:09.960]   So that means that half the time, we actually
[00:07:09.960 --> 00:07:12.720]   classify the mirror image of the image.
[00:07:12.720 --> 00:07:14.320]   So why don't we also add that?
[00:07:14.320 --> 00:07:21.360]   And now we call data.fit on X-train.
[00:07:21.360 --> 00:07:24.400]   And actually, what this does is it attaches the data generator
[00:07:24.400 --> 00:07:26.240]   to the X-train variable.
[00:07:26.240 --> 00:07:28.640]   So at each step, it's going to pull
[00:07:28.640 --> 00:07:31.600]   a batch size worth of images from X-train,
[00:07:31.600 --> 00:07:34.040]   but it's going to modify those images with the functions
[00:07:34.040 --> 00:07:34.600]   that we added.
[00:07:34.600 --> 00:07:37.000]   So once you've set up the data gen,
[00:07:37.000 --> 00:07:38.880]   then we call fit generator.
[00:07:38.880 --> 00:07:40.920]   This is a lot like model.fit, but instead
[00:07:40.920 --> 00:07:44.920]   of passing in complete arrays, you actually
[00:07:44.920 --> 00:07:48.680]   pass in a function that returns these arrays in batches.
[00:07:48.680 --> 00:07:53.080]   So our data gen here is passing through X-train and Y-train
[00:07:53.080 --> 00:07:57.120]   in chunks of batch size.
[00:07:57.120 --> 00:08:02.600]   So we actually need to set batch size equals config.batchSize
[00:08:02.600 --> 00:08:03.680]   in here.
[00:08:03.680 --> 00:08:07.680]   And then I like to set the steps per epoch,
[00:08:07.680 --> 00:08:10.840]   because otherwise, you won't actually really have epochs.
[00:08:10.840 --> 00:08:12.880]   Because an epoch is typically when you've
[00:08:12.880 --> 00:08:14.240]   stepped through all the data.
[00:08:14.240 --> 00:08:15.800]   But when you have a generator, you're
[00:08:15.800 --> 00:08:17.840]   essentially generating infinite amounts of data.
[00:08:17.840 --> 00:08:19.080]   So there aren't really epochs.
[00:08:19.080 --> 00:08:22.780]   But it's nice to have natural breakpoints where
[00:08:22.780 --> 00:08:25.040]   you check the validation data.
[00:08:25.040 --> 00:08:27.160]   And so I set steps for epoch typically
[00:08:27.160 --> 00:08:28.960]   equal to be the size of the data--
[00:08:28.960 --> 00:08:31.800]   and that's X-train.shape, 0--
[00:08:31.800 --> 00:08:34.480]   divided by config.batchSize.
[00:08:34.480 --> 00:08:35.920]   So this means that we're basically
[00:08:35.920 --> 00:08:38.080]   going to have the same number of steps for epoch
[00:08:38.080 --> 00:08:41.880]   as we would have had if we had just called model.fit.
[00:08:41.880 --> 00:08:44.600]   And so we can set epochs and validation data and callbacks
[00:08:44.600 --> 00:08:47.800]   in exactly the same way that we would have otherwise.
[00:08:47.800 --> 00:08:50.400]   And so it's interesting to think before this runs what this is
[00:08:50.400 --> 00:08:53.620]   going to do to accuracy and validation accuracy.
[00:08:53.620 --> 00:08:56.820]   Typically, what we'll see is it'll make the accuracy lower,
[00:08:56.820 --> 00:09:00.100]   because we've actually made the classification task harder.
[00:09:00.100 --> 00:09:04.200]   We've sort of added some random noise into that task.
[00:09:04.200 --> 00:09:06.940]   But hopefully, in order to complete that task,
[00:09:06.940 --> 00:09:09.300]   it's going to actually make the model generalize better.
[00:09:09.300 --> 00:09:12.580]   And maybe, if we're lucky, it'll make the validation accuracy
[00:09:12.580 --> 00:09:14.460]   higher than it would have otherwise.
[00:09:14.460 --> 00:09:20.260]   If you compare the validation accuracy of our CNN
[00:09:20.260 --> 00:09:25.060]   with augmentation, it's actually better than our vanilla CNN,
[00:09:25.060 --> 00:09:27.500]   and it's steadily improving over time.
[00:09:27.500 --> 00:09:29.380]   So we can add all kinds of things.
[00:09:29.380 --> 00:09:31.440]   And this is a rich vein of things
[00:09:31.440 --> 00:09:32.820]   to try to make your model better.
[00:09:32.820 --> 00:09:36.780]   But we can go back in and add all kinds of transformations
[00:09:36.780 --> 00:09:39.420]   to keep improving the generalization of our model
[00:09:39.420 --> 00:09:40.740]   on this data set.
[00:09:40.740 --> 00:09:44.180]   So just backing up, in general, this kind of augmentation
[00:09:44.180 --> 00:09:46.420]   is different in every domain.
[00:09:46.420 --> 00:09:48.380]   In text, one thing people do is they'll actually
[00:09:48.380 --> 00:09:50.780]   translate their text into a different language
[00:09:50.780 --> 00:09:52.900]   using Google Translate, and then translate it back
[00:09:52.900 --> 00:09:55.180]   into the original language, which
[00:09:55.180 --> 00:09:56.660]   kind of adds a little bit of noise,
[00:09:56.660 --> 00:09:59.300]   but often keeps the same semantics.
[00:09:59.300 --> 00:10:02.300]   And in audio, people add background sound effects
[00:10:02.300 --> 00:10:04.940]   or distort the audio just a little bit
[00:10:04.940 --> 00:10:07.860]   to get variations on the audio.
[00:10:07.860 --> 00:10:09.980]   So this is a really general technique.
[00:10:09.980 --> 00:10:11.900]   It works really effectively.
[00:10:11.900 --> 00:10:14.220]   And I hope you enjoyed this video.
[00:10:14.820 --> 00:10:17.880]   [AUDIO OUT]
[00:10:17.880 --> 00:10:20.940]   [NO AUDIO]
[00:10:20.940 --> 00:10:30.940]   [BLANK_AUDIO]

