
[00:00:00.000 --> 00:00:03.400]   [MUSIC PLAYING]
[00:00:03.400 --> 00:00:04.200]   Hello, everyone.
[00:00:04.200 --> 00:00:06.760]   My name is Anish Shah, a machinery engineer on the growth
[00:00:06.760 --> 00:00:08.240]   team at Weights & Biases.
[00:00:08.240 --> 00:00:11.160]   Weights & Biases boost ML team productivity
[00:00:11.160 --> 00:00:14.680]   by streamlining the ML workflow with minimal code,
[00:00:14.680 --> 00:00:18.040]   instantly debug, compare, and reproduce models,
[00:00:18.040 --> 00:00:21.280]   including architecture, type parameters, Git commits,
[00:00:21.280 --> 00:00:24.840]   model weights, GPU usage, data sets, predictions, and more,
[00:00:24.840 --> 00:00:27.160]   while promoting collaboration and serving
[00:00:27.160 --> 00:00:28.360]   as your ML system of error.
[00:00:28.360 --> 00:00:30.640]   Snowpark Container Services, currently available
[00:00:30.640 --> 00:00:32.400]   in private preview, is an expansion
[00:00:32.400 --> 00:00:34.120]   of Snowflake's processing engine that
[00:00:34.120 --> 00:00:35.920]   provides developers with the flexibility
[00:00:35.920 --> 00:00:38.600]   to deploy container images in Snowflake-managed
[00:00:38.600 --> 00:00:39.560]   infrastructure.
[00:00:39.560 --> 00:00:42.020]   I'm excited to announce that Weights & Biases is available
[00:00:42.020 --> 00:00:43.720]   on Snowpark Container Services, allowing
[00:00:43.720 --> 00:00:45.960]   to easily host and deploy your own instance of Weights
[00:00:45.960 --> 00:00:46.460]   & Biases.
[00:00:46.460 --> 00:00:49.920]   [MUSIC PLAYING]
[00:00:49.920 --> 00:01:05.880]   [MUSIC PLAYING]
[00:01:05.880 --> 00:01:07.400]   The Weights & Biases client is also
[00:01:07.400 --> 00:01:09.440]   available on Snowflake and Aconda channels.
[00:01:09.440 --> 00:01:11.360]   This will enable machine learning practitioners
[00:01:11.360 --> 00:01:12.800]   and adjacent members of your team
[00:01:12.800 --> 00:01:16.040]   to immediately begin managing their entire ML lifecycle
[00:01:16.040 --> 00:01:19.200]   in one centralized and privately hosted location.
[00:01:19.200 --> 00:01:21.240]   To highlight this exciting collaboration,
[00:01:21.240 --> 00:01:23.440]   we want to showcase an example comparing the latest
[00:01:23.440 --> 00:01:25.280]   and greatest from a large language model
[00:01:25.280 --> 00:01:28.600]   space for the creation of a conversational chatbot, which
[00:01:28.600 --> 00:01:31.360]   will allow users to ask natural language questions,
[00:01:31.360 --> 00:01:33.760]   such as list the total sales per country,
[00:01:33.760 --> 00:01:36.760]   or which country's customers spent the most
[00:01:36.760 --> 00:01:38.680]   of their database, without needing
[00:01:38.680 --> 00:01:40.720]   to write SQL themselves.
[00:01:40.720 --> 00:01:42.600]   Behind the scenes, the large language model
[00:01:42.600 --> 00:01:45.200]   is able to understand this human-friendly question
[00:01:45.200 --> 00:01:48.600]   and automatically generate this SQL that is automatically
[00:01:48.600 --> 00:01:50.240]   executed in Snowflake.
[00:01:50.240 --> 00:01:53.080]   To develop this chatbot, we will utilize the popular framework
[00:01:53.080 --> 00:01:55.320]   LangChain to allow us to flexibly communicate
[00:01:55.320 --> 00:01:57.320]   with the most popular LM providers,
[00:01:57.320 --> 00:02:00.840]   such as Claude from Anthropic, Kaman from Cohere,
[00:02:00.840 --> 00:02:03.280]   and of course, GPT from OpenAI.
[00:02:03.280 --> 00:02:05.600]   LangChain will allow us to develop an autonomous agent
[00:02:05.600 --> 00:02:07.600]   from these LLMs, which will have the ability
[00:02:07.600 --> 00:02:09.800]   to reason over natural language questions
[00:02:09.800 --> 00:02:12.440]   and make decisions about how to generate value of SQL
[00:02:12.440 --> 00:02:15.480]   before deciding to execute it within our environment.
[00:02:15.480 --> 00:02:16.880]   To capture this chain of thought,
[00:02:16.880 --> 00:02:19.280]   we utilize our minimal LangChain's weights and biases
[00:02:19.280 --> 00:02:22.280]   integration, which allows us to visualize and inspect
[00:02:22.280 --> 00:02:25.720]   the execution flow of our LLMs, analyze the inputs
[00:02:25.720 --> 00:02:28.840]   and outputs of our LLMs, view the intermediate results,
[00:02:28.840 --> 00:02:30.920]   and securely store and manage our prompts
[00:02:30.920 --> 00:02:33.840]   and LLM agent configurations.
[00:02:33.840 --> 00:02:35.560]   Now jumping to our code, which is
[00:02:35.560 --> 00:02:38.360]   living in the Snowpark container,
[00:02:38.360 --> 00:02:41.920]   we will install our relevant packages
[00:02:41.920 --> 00:02:43.720]   and define the environment variables
[00:02:43.720 --> 00:02:47.320]   necessary to communicate with these LM providers.
[00:02:47.320 --> 00:02:51.040]   Once ready, all we need to do is load our LangChain modules
[00:02:51.040 --> 00:02:53.080]   for creating our autonomous agent.
[00:02:53.080 --> 00:02:55.760]   In this case, we use create SQL agent
[00:02:55.760 --> 00:02:57.240]   to create an agent that communicates
[00:02:57.240 --> 00:03:00.880]   with a SQL database, as you can see here.
[00:03:00.880 --> 00:03:02.720]   And to make this SQL agent work, we
[00:03:02.720 --> 00:03:04.320]   ensure that it has a SQL database
[00:03:04.320 --> 00:03:05.920]   toolkit that can ensure that it has
[00:03:05.920 --> 00:03:08.880]   valid and proper SQL generated so it can properly
[00:03:08.880 --> 00:03:11.840]   communicate with our database.
[00:03:11.840 --> 00:03:13.800]   Agents utilize a large language model
[00:03:13.800 --> 00:03:16.280]   for determining and sequencing actions,
[00:03:16.280 --> 00:03:18.880]   which can be tool usage and output observation,
[00:03:18.880 --> 00:03:20.520]   or even user response.
[00:03:20.520 --> 00:03:23.720]   Tools, simply Python functions with specific roles,
[00:03:23.720 --> 00:03:27.400]   can include Google Search, Database Lookup, Python REPL,
[00:03:27.400 --> 00:03:28.480]   and others.
[00:03:28.480 --> 00:03:31.200]   The ones that are available for our agent are listed here.
[00:03:31.200 --> 00:03:34.800]   Query SQL database tool, Info SQL database tool,
[00:03:34.800 --> 00:03:39.240]   List SQL database tool, and Query Checker tool.
[00:03:39.240 --> 00:03:40.840]   The agent in this case operates as
[00:03:40.840 --> 00:03:44.080]   per the instructions in the prompts defined in the 00shot
[00:03:44.080 --> 00:03:45.120]   agent.
[00:03:45.120 --> 00:03:46.800]   It is tasked with answering questions
[00:03:46.800 --> 00:03:49.360]   using available tools, which for our SQL agent
[00:03:49.360 --> 00:03:50.720]   were listed below.
[00:03:50.720 --> 00:03:52.520]   In this case, instructions are told
[00:03:52.520 --> 00:03:55.840]   to answer the following questions as best as they can,
[00:03:55.840 --> 00:03:59.440]   given access to the following tools.
[00:03:59.440 --> 00:04:01.160]   With our agent tool modules loaded,
[00:04:01.160 --> 00:04:04.120]   we import our native Weights and Biases LangChain integration.
[00:04:04.120 --> 00:04:07.120]   With only a few lines of code, all the complex interactions
[00:04:07.120 --> 00:04:09.240]   that are presented in the execution of our agent
[00:04:09.240 --> 00:04:11.600]   will be captured automatically in our Weights and Biases
[00:04:11.600 --> 00:04:13.800]   dashboard for analysis.
[00:04:13.800 --> 00:04:15.280]   For each of the LLM providers, we
[00:04:15.280 --> 00:04:16.800]   define which specific model we want
[00:04:16.800 --> 00:04:21.480]   to serve as the brain behind our agent, listed here.
[00:04:21.480 --> 00:04:23.280]   We finally load the database we want our agent
[00:04:23.280 --> 00:04:24.560]   to communicate over.
[00:04:24.560 --> 00:04:25.880]   We're using the Chinook database,
[00:04:25.880 --> 00:04:28.680]   which is a sample digital music store with various relationships
[00:04:28.680 --> 00:04:32.000]   and tables, perfect for testing the ability of SQL generated
[00:04:32.000 --> 00:04:33.560]   and executed by our agent.
[00:04:33.560 --> 00:04:35.680]   Lastly, all we need to do before running our agent
[00:04:35.680 --> 00:04:39.680]   is define questions in natural English language,
[00:04:39.680 --> 00:04:47.800]   load our agent, and then pass our questions to our agent.
[00:04:47.800 --> 00:04:50.400]   Our Weights and Biases tracer callback
[00:04:50.400 --> 00:04:52.400]   will automatically capture all the interactions
[00:04:52.400 --> 00:04:55.360]   that you see printed out in the colorful text in Standard
[00:04:55.360 --> 00:04:58.000]   Out below as I scroll.
[00:04:58.000 --> 00:05:01.000]   Personally, I find this text in Standard Out difficult to analyze.
[00:05:01.000 --> 00:05:03.120]   If we navigate to our Weights and Biases dashboard,
[00:05:03.120 --> 00:05:05.240]   we're presented with the collection of LLM experiments
[00:05:05.240 --> 00:05:06.320]   and our runs bar.
[00:05:06.320 --> 00:05:09.960]   We also have our trace view in the panels workspace.
[00:05:09.960 --> 00:05:12.080]   The table at the top gives you high level information
[00:05:12.080 --> 00:05:14.320]   about what you've logged, such as whether the chain was
[00:05:14.320 --> 00:05:17.240]   successful, the inputs and outputs of the chain,
[00:05:17.240 --> 00:05:19.120]   what the make and post of the chain are,
[00:05:19.120 --> 00:05:21.920]   any errors that occurred in the chain, if any.
[00:05:21.920 --> 00:05:24.520]   And when we click the different rows of the table,
[00:05:24.520 --> 00:05:27.440]   we can see the trace timeline you update with more details.
[00:05:27.440 --> 00:05:30.080]   On the bottom half, we see the entire execution trace
[00:05:30.080 --> 00:05:33.960]   of the chain, including the components such as chain, LLMs,
[00:05:33.960 --> 00:05:36.160]   and any tools used by our agent.
[00:05:36.160 --> 00:05:38.560]   In the event of a chain failure, the component that failed
[00:05:38.560 --> 00:05:39.800]   will be highlighted in red.
[00:05:39.800 --> 00:05:42.480]   You can check the inputs to that component debug what went wrong.
[00:05:42.480 --> 00:05:45.600]   In the case, we can see that this action was not valid,
[00:05:45.600 --> 00:05:49.160]   as this is not the proper input to use our tool for our agent.
[00:05:49.160 --> 00:05:51.240]   From our deep analysis over these traces,
[00:05:51.240 --> 00:05:53.640]   we see that GBT-4 is currently the superior option
[00:05:53.640 --> 00:05:55.640]   for SQL generation agent.
[00:05:55.640 --> 00:05:58.040]   As we deploy this agent into our production environments,
[00:05:58.040 --> 00:05:59.960]   we can ensure our data is streamed from our LLM
[00:05:59.960 --> 00:06:01.520]   to our Weights and Biases instance,
[00:06:01.520 --> 00:06:05.160]   posing on a snow park with ease, making debugging a breeze.
[00:06:05.160 --> 00:06:08.520]   [MUSIC PLAYING]
[00:06:08.520 --> 00:06:11.100]   (upbeat music)

