
[00:00:00.000 --> 00:00:00.960]   Hello, everyone.
[00:00:00.960 --> 00:00:04.520]   Welcome to my technical talk today about sequence models.
[00:00:04.520 --> 00:00:06.640]   Here's a preview of what's to come.
[00:00:06.640 --> 00:00:11.240]   First, we'll talk about how a neural network works,
[00:00:11.240 --> 00:00:13.000]   so also known as an RNN.
[00:00:13.000 --> 00:00:15.000]   Second, we'll zoom in--
[00:00:15.000 --> 00:00:17.460]   wow, saying zoom while on Zoom feels a little bit awkward.
[00:00:17.460 --> 00:00:23.000]   But we'll zoom in on the units of RNNs, LSTMs, and GRUs.
[00:00:23.000 --> 00:00:25.240]   Last, we'll look at more advanced RNNs,
[00:00:25.240 --> 00:00:29.920]   like the bidirectional RNN and deep RNNs.
[00:00:29.920 --> 00:00:33.520]   So I'm feeling funny, so let's start with a joke explaining
[00:00:33.520 --> 00:00:37.080]   why sequence models are useful.
[00:00:37.080 --> 00:00:39.920]   A human asks, what do we want?
[00:00:39.920 --> 00:00:43.100]   And a computer answers, natural language processing.
[00:00:43.100 --> 00:00:44.800]   I don't know about you, but I don't think
[00:00:44.800 --> 00:00:47.060]   natural language processing is in the top three things
[00:00:47.060 --> 00:00:47.920]   I want right now.
[00:00:47.920 --> 00:00:52.480]   Maybe not even my top 10, but anyhow, let's go with it.
[00:00:52.480 --> 00:00:55.420]   The human next asks, when do we want it?
[00:00:55.420 --> 00:00:56.840]   This is obviously a trick question,
[00:00:56.840 --> 00:01:00.480]   because humans, even with our relatively long attention
[00:01:00.480 --> 00:01:03.480]   spans, want everything immediately.
[00:01:03.480 --> 00:01:06.920]   By this point in the conversation,
[00:01:06.920 --> 00:01:08.600]   the computer has already forgotten
[00:01:08.600 --> 00:01:11.200]   what we were talking about and asks, when do we want what?
[00:01:11.200 --> 00:01:14.080]   Again, so that's the end of the joke.
[00:01:14.080 --> 00:01:17.900]   I wish Zoom had a laugh track or something to play here.
[00:01:17.900 --> 00:01:20.840]   The takeaway here is that a traditional neural network
[00:01:20.840 --> 00:01:24.640]   model can't follow dialogue or sequences of text well,
[00:01:24.640 --> 00:01:26.800]   which is why sequence models were invented.
[00:01:26.800 --> 00:01:35.720]   The basic sequence model is a recurrent neural network,
[00:01:35.720 --> 00:01:38.080]   so we'll start with that one.
[00:01:38.080 --> 00:01:42.280]   The GRU, LSTM, BiRNN, and DeepRNN,
[00:01:42.280 --> 00:01:44.000]   all of which we will be learning about next,
[00:01:44.000 --> 00:01:47.240]   are extensions of this basic RNN architecture.
[00:01:47.240 --> 00:01:51.280]   Let's start with an example.
[00:01:51.280 --> 00:01:54.480]   Imagine using an RNN for a named entity recognition
[00:01:54.480 --> 00:01:58.000]   problem, like recognize whether a word in a sentence
[00:01:58.000 --> 00:02:00.760]   is a person's name.
[00:02:00.760 --> 00:02:03.920]   In this example, the first word in the sentence or data set
[00:02:03.920 --> 00:02:05.440]   would be the input to an RNN.
[00:02:05.440 --> 00:02:13.200]   This first word is then passed to a hidden layer.
[00:02:13.200 --> 00:02:15.240]   We also pass in an initial vector
[00:02:15.240 --> 00:02:17.400]   that's all zero or randomly initialized
[00:02:17.400 --> 00:02:20.440]   to the same hidden layer.
[00:02:20.440 --> 00:02:23.200]   Inside the hidden layer, math happens and outputs
[00:02:23.200 --> 00:02:26.400]   an activation to pass on to the next hidden layer.
[00:02:26.400 --> 00:02:28.280]   We'll zoom into the math later when
[00:02:28.280 --> 00:02:31.760]   we learn about the recurrent neural unit.
[00:02:31.760 --> 00:02:36.680]   Next, multiple parameters are calculated,
[00:02:36.680 --> 00:02:38.280]   then more math happens.
[00:02:38.280 --> 00:02:41.840]   And the first word input vector of zeros and parameters
[00:02:41.840 --> 00:02:44.440]   are used to output a prediction.
[00:02:44.440 --> 00:02:46.500]   In the example of named entity recognition,
[00:02:46.500 --> 00:02:48.960]   the output would be zero or one.
[00:02:48.960 --> 00:02:52.060]   Zero if the word was not, and one
[00:02:52.060 --> 00:02:53.560]   if the word was a person's name.
[00:02:53.560 --> 00:02:59.140]   The activation parameters, which includes information
[00:02:59.140 --> 00:03:01.080]   about the first word, are then passed
[00:03:01.080 --> 00:03:02.680]   into the next hidden layer and are
[00:03:02.680 --> 00:03:04.640]   used for predicting whether the second word is
[00:03:04.640 --> 00:03:07.000]   a person's name.
[00:03:07.000 --> 00:03:09.120]   We repeat this pattern for the third word.
[00:03:09.120 --> 00:03:11.840]   And as you can see, a sequence is starting to form.
[00:03:11.840 --> 00:03:17.560]   We repeat this pattern for the number of words in the sentence.
[00:03:17.560 --> 00:03:20.740]   Alternatively, you can also forcefully stop the sequence
[00:03:20.740 --> 00:03:25.680]   model by adding a maximum number of time steps.
[00:03:25.680 --> 00:03:29.040]   T can be the number of words or the maximum number
[00:03:29.040 --> 00:03:31.240]   of time steps allowed.
[00:03:31.240 --> 00:03:35.240]   This is the basic architecture of a neural network.
[00:03:35.240 --> 00:03:37.600]   Now we will see how backpropagation
[00:03:37.600 --> 00:03:41.400]   and forward propagation works.
[00:03:41.400 --> 00:03:45.160]   For every prediction, a loss function calculates the loss.
[00:03:45.160 --> 00:03:47.600]   The purpose of this is to optimize the parameter
[00:03:47.600 --> 00:03:50.940]   values in the sequence.
[00:03:50.940 --> 00:03:52.920]   These losses are propagated forward
[00:03:52.920 --> 00:03:57.000]   to inform future output predictions.
[00:03:57.000 --> 00:04:00.000]   The losses are not only propagated forwards,
[00:04:00.000 --> 00:04:02.760]   but also propagated backwards, like this.
[00:04:02.760 --> 00:04:08.520]   Now this here is a unit, which is also known
[00:04:08.520 --> 00:04:12.440]   as a neuron in the hidden layer.
[00:04:12.440 --> 00:04:14.680]   All right, you made it to the first Star Wars GIF.
[00:04:14.680 --> 00:04:16.140]   I hope you guys are Star Wars fans,
[00:04:16.140 --> 00:04:19.600]   because there's more Star Wars GIFs coming up.
[00:04:19.600 --> 00:04:21.640]   Next, we'll look at what the math
[00:04:21.640 --> 00:04:24.360]   looks like inside one of these recurrent neural units.
[00:04:24.360 --> 00:04:31.680]   Here's what the architecture of a unit or neuron
[00:04:31.680 --> 00:04:36.320]   inside the hidden layer of an RNN looks like.
[00:04:36.320 --> 00:04:39.320]   The activation value from the previous time step
[00:04:39.320 --> 00:04:41.880]   and the word vector at the current time step
[00:04:41.880 --> 00:04:45.040]   are inputs to the activation function, which is then
[00:04:45.040 --> 00:04:49.600]   mapped to 0 or 1 output using softmax before being output
[00:04:49.600 --> 00:04:50.920]   as the prediction.
[00:04:50.920 --> 00:04:58.620]   Here's the math showing how the activation at t
[00:04:58.620 --> 00:05:03.640]   is calculated before it's passed on to the next hidden layer.
[00:05:03.640 --> 00:05:06.720]   G is the tanh activation function,
[00:05:06.720 --> 00:05:11.960]   and w is the parameter matrix of the previous activation value
[00:05:11.960 --> 00:05:15.480]   and the current input value.
[00:05:15.480 --> 00:05:18.680]   Lastly, a bias b is added to the output.
[00:05:18.680 --> 00:05:24.120]   I know that was a lot of technical details
[00:05:24.120 --> 00:05:25.920]   in the last few slides.
[00:05:25.920 --> 00:05:28.080]   We're about halfway done.
[00:05:28.080 --> 00:05:29.840]   The next half will still be technical,
[00:05:29.840 --> 00:05:31.760]   but we'll be talking about higher level
[00:05:31.760 --> 00:05:33.680]   differences in the RNN architecture
[00:05:33.680 --> 00:05:34.640]   for different use cases.
[00:05:41.240 --> 00:05:43.280]   Let's start with an example here.
[00:05:43.280 --> 00:05:47.040]   Imagine using an RNN for a sentence generation problem,
[00:05:47.040 --> 00:05:49.640]   maybe like the classic write like Shakespeare one,
[00:05:49.640 --> 00:05:51.320]   if you're familiar with that.
[00:05:51.320 --> 00:05:53.620]   We need to remember the subject of the sentence
[00:05:53.620 --> 00:05:58.360]   to decide whether to generate a plural or singular verb next.
[00:05:58.360 --> 00:06:03.840]   GRUs, or GRUs, are better at this than RNNs.
[00:06:03.840 --> 00:06:07.760]   GRUs are used in place of RNN units in RNNs.
[00:06:07.760 --> 00:06:10.920]   This makes GRUs not only better at understanding longer range
[00:06:10.920 --> 00:06:14.720]   dependencies, but also solves RNNs vanishing gradient
[00:06:14.720 --> 00:06:18.800]   problem, which I'll talk more about at the end of this video
[00:06:18.800 --> 00:06:21.600]   or at the end of this webinar.
[00:06:21.600 --> 00:06:24.360]   The GRU is made up of the memory cell, which
[00:06:24.360 --> 00:06:28.000]   memorizes relevant information, and two gates.
[00:06:28.000 --> 00:06:30.800]   First, the update gate decides what information should
[00:06:30.800 --> 00:06:32.640]   be memorized or forgotten.
[00:06:32.640 --> 00:06:34.640]   And second, the reset gate decides
[00:06:34.640 --> 00:06:37.080]   how much of the past information to forget.
[00:06:38.080 --> 00:06:40.080]   That was the way a GRU works.
[00:06:40.080 --> 00:06:44.360]   The same idea of gates from GRUs are also
[00:06:44.360 --> 00:06:47.120]   used in LSTMs, which we'll be learning about next.
[00:06:47.120 --> 00:06:54.320]   LSTMs, like GRUs, learn long term dependencies
[00:06:54.320 --> 00:06:56.000]   in a sequence.
[00:06:56.000 --> 00:07:01.400]   LSTMs have three gates, so that's one more than a GRU.
[00:07:01.400 --> 00:07:04.080]   The LSTM is made up of a memory cell that
[00:07:04.080 --> 00:07:07.440]   memorizes relevant information and three gates.
[00:07:07.440 --> 00:07:10.080]   First, the forget gate decides what information should
[00:07:10.080 --> 00:07:12.040]   be memorized or forgotten.
[00:07:12.040 --> 00:07:15.120]   Second, the input gate updates the memory cell
[00:07:15.120 --> 00:07:17.200]   if the information is relevant.
[00:07:17.200 --> 00:07:20.560]   And third, the outbook gate decides what the next hidden
[00:07:20.560 --> 00:07:23.200]   state should be.
[00:07:23.200 --> 00:07:25.680]   Using an LSTM, you can develop a neural network
[00:07:25.680 --> 00:07:30.000]   that understands more complex sequences of text.
[00:07:30.000 --> 00:07:33.520]   It's hard to predict what the next hidden state should be.
[00:07:33.520 --> 00:07:37.560]   It's hard to predict whether GRU or LSTM will perform better,
[00:07:37.560 --> 00:07:39.120]   so it's often best to try both.
[00:07:39.120 --> 00:07:45.640]   Coming up next are bidirectional RNNs,
[00:07:45.640 --> 00:07:47.960]   which are bidirectional, like this lightsaber here.
[00:07:47.960 --> 00:07:54.920]   Bidirectional RNNs let you use information
[00:07:54.920 --> 00:07:58.280]   from the beginning and the end of a sequence.
[00:07:58.280 --> 00:08:02.080]   Let's think back to the example of named entity recognition.
[00:08:02.080 --> 00:08:06.200]   Sometimes you need context from not only before, but also
[00:08:06.200 --> 00:08:10.840]   after the word to decide whether the word is a person's name.
[00:08:10.840 --> 00:08:14.640]   The one directional RNN only has forward recurrent layers
[00:08:14.640 --> 00:08:17.240]   and reads the sentence from left to right.
[00:08:17.240 --> 00:08:21.640]   A bidirectional RNN has forward and backward recurrent layers,
[00:08:21.640 --> 00:08:25.920]   allowing it to read from left to right and right to left.
[00:08:25.920 --> 00:08:29.240]   As a result, this model uses the past, present,
[00:08:29.240 --> 00:08:32.880]   and future information when making a prediction.
[00:08:32.880 --> 00:08:36.280]   By the way, the A hidden layers here that you see
[00:08:36.280 --> 00:08:41.120]   can include traditional RNN, GRU, and LSTM units.
[00:08:41.120 --> 00:08:48.600]   That was a biRNN thinking forwards and backwards.
[00:08:48.600 --> 00:08:53.600]   Now let's talk about how to take all RNNs, LSTMs, and GRUs
[00:08:53.600 --> 00:08:55.440]   and construct deep versions of them.
[00:08:56.440 --> 00:08:59.840]   The RNN, GRU, and LSTM you've learned about so far
[00:08:59.840 --> 00:09:03.040]   already work well as is, but sometimes
[00:09:03.040 --> 00:09:06.120]   stacking multiple layers of RNNs together
[00:09:06.120 --> 00:09:10.800]   to build deeper versions of these models perform better.
[00:09:10.800 --> 00:09:12.880]   Here's the standard RNN that you've seen so far.
[00:09:12.880 --> 00:09:19.640]   Then we can just stack more layers on top.
[00:09:19.640 --> 00:09:23.000]   Now this is a new network with three hidden layers.
[00:09:23.000 --> 00:09:25.160]   By the way, these hidden layers don't
[00:09:25.160 --> 00:09:27.120]   need to use the simple RNN units we saw
[00:09:27.120 --> 00:09:28.680]   at the beginning of today's talk.
[00:09:28.680 --> 00:09:31.840]   They can also use GRU and LSTM units.
[00:09:31.840 --> 00:09:34.920]   And if you were wondering, it's possible to build
[00:09:34.920 --> 00:09:37.440]   a deep version of the bidirectional RNN too.
[00:09:37.440 --> 00:09:46.240]   Now that you know how RNNs, GRUs, and LSTMs work,
[00:09:46.240 --> 00:09:49.600]   let's look at the advantages and disadvantages of these options.
[00:09:49.720 --> 00:09:53.160]   The traditional RNN, also known as a vanilla RNN,
[00:09:53.160 --> 00:09:55.120]   is a good model to start with because it
[00:09:55.120 --> 00:09:56.720]   gives you a good baseline to compare
[00:09:56.720 --> 00:09:59.840]   the other models with it.
[00:09:59.840 --> 00:10:04.600]   However, RNNs often face vanishing gradients problems.
[00:10:04.600 --> 00:10:07.560]   This is when the gradient diminishes dramatically
[00:10:07.560 --> 00:10:09.480]   as it's propagated backwards.
[00:10:09.480 --> 00:10:11.600]   The error might be so small that it
[00:10:11.600 --> 00:10:14.320]   might have little effect by the time it reaches the layers
[00:10:14.320 --> 00:10:16.000]   close to the base.
[00:10:16.000 --> 00:10:17.960]   The error might be so small that it
[00:10:17.960 --> 00:10:20.720]   might have little effect by the time it reaches the layers
[00:10:20.720 --> 00:10:22.880]   close to the input of the model, which
[00:10:22.880 --> 00:10:28.320]   is why it's aptly named the vanishing gradients problem.
[00:10:28.320 --> 00:10:31.520]   The GRU fixes this problem because its gates
[00:10:31.520 --> 00:10:34.080]   control the flow of information inside the network
[00:10:34.080 --> 00:10:37.440]   more effectively.
[00:10:37.440 --> 00:10:41.840]   But there's a trade-off between the speed and power
[00:10:41.840 --> 00:10:45.880]   of the network for GRUs and LSTMs.
[00:10:45.880 --> 00:10:47.800]   While the GRU has longer sequences
[00:10:47.800 --> 00:10:51.960]   because of its additional gate, it's still slower than the GRU.
[00:10:51.960 --> 00:10:55.640]   Because biRNNs use information from the past, present,
[00:10:55.640 --> 00:10:58.520]   and future, which is a good thing because it gives the model
[00:10:58.520 --> 00:11:02.960]   more context, you need access to the whole sequence of data
[00:11:02.960 --> 00:11:04.880]   before you can make predictions anywhere.
[00:11:04.880 --> 00:11:06.680]   So it's a double-edged sword.
[00:11:06.680 --> 00:11:08.440]   This can be inconvenient, for example,
[00:11:08.440 --> 00:11:10.760]   when you're building a speech recognition system,
[00:11:10.760 --> 00:11:14.120]   since you will have to wait for the person to stop talking
[00:11:14.120 --> 00:11:16.480]   to you before you can make a prediction.
[00:11:16.480 --> 00:11:18.600]   It's still a good option for most natural language
[00:11:18.600 --> 00:11:20.760]   processing applications, where you have access
[00:11:20.760 --> 00:11:22.040]   to the whole sentence at once.
[00:11:22.040 --> 00:11:27.160]   DeepRNN's hierarchy of hidden layers
[00:11:27.160 --> 00:11:30.160]   enables more complex understanding of the data,
[00:11:30.160 --> 00:11:32.800]   but it's also more computationally expensive
[00:11:32.800 --> 00:11:33.760]   than other options.
[00:11:33.760 --> 00:11:39.680]   So that's all the pros and cons, folks.
[00:11:39.680 --> 00:11:40.400]   Congratulations.
[00:11:40.400 --> 00:11:45.520]   DeepRNN, Recurrent Neural Unit, GRU, LSTM, biRNN,
[00:11:45.520 --> 00:11:48.280]   and DeepRNN to your toolbox to use
[00:11:48.280 --> 00:11:50.600]   when creating sequence models.
[00:11:50.600 --> 00:11:52.360]   I'm looking forward to seeing what you build
[00:11:52.360 --> 00:11:55.600]   using these new tools.
[00:11:55.600 --> 00:11:57.040]   Thank you all for listening.
[00:11:57.040 --> 00:11:58.680]   My name is Pooja Poojarajan.
[00:11:58.680 --> 00:12:00.880]   I'm a deep learning engineer at Node and the USA
[00:12:00.880 --> 00:12:02.680]   Ambassador for Women in AI.
[00:12:02.680 --> 00:12:04.560]   Follow me on Twitter and check out my website,
[00:12:04.560 --> 00:12:06.120]   and feel free to follow up and say hi.
[00:12:06.120 --> 00:12:10.760]   [END PLAYBACK]
[00:12:10.760 --> 00:12:11.600]   Thanks, Pooja.
[00:12:11.600 --> 00:12:12.640]   That was great.
[00:12:12.640 --> 00:12:17.200]   I'm going to drop your Twitter in the chat,
[00:12:17.200 --> 00:12:19.360]   just in case people want to follow you.
[00:12:19.360 --> 00:12:21.800]   OK, I'll stop sharing then.
[00:12:21.800 --> 00:12:22.440]   Cool.
[00:12:22.440 --> 00:12:23.640]   We have some questions.
[00:12:23.640 --> 00:12:26.240]   So someone asked, how do you think about how many layers
[00:12:26.240 --> 00:12:26.720]   to build?
[00:12:26.720 --> 00:12:31.240]   Can you repeat the question?
[00:12:31.240 --> 00:12:34.360]   How do you think about how many layers to build?
[00:12:34.360 --> 00:12:37.880]   So how many layers in your network?
[00:12:37.880 --> 00:12:39.680]   Yeah, that's a good question.
[00:12:39.680 --> 00:12:42.600]   It's honestly trial and error.
[00:12:42.600 --> 00:12:44.960]   I start with the simplest just because I
[00:12:44.960 --> 00:12:46.840]   want the fastest output, right?
[00:12:46.840 --> 00:12:49.920]   If you start with something that's unnecessarily
[00:12:49.920 --> 00:12:53.320]   like 100 layers deep, it's just taking you longer to iterate.
[00:12:53.320 --> 00:12:56.200]   So I usually start with one, two,
[00:12:56.200 --> 00:12:59.200]   and kind of go up from there.
[00:12:59.200 --> 00:13:03.400]   Just a sidebar, though, I also look
[00:13:03.400 --> 00:13:05.480]   to see whether someone else has kind of created
[00:13:05.480 --> 00:13:08.040]   something similar online.
[00:13:08.040 --> 00:13:10.720]   Because you can learn a lot from other people's experiments,
[00:13:10.720 --> 00:13:13.640]   like depending on what type of model you're trying to build.
[00:13:13.640 --> 00:13:17.720]   Because at that point, you don't want to try rerunning--
[00:13:17.720 --> 00:13:19.520]   or you don't always want to rerun something
[00:13:19.520 --> 00:13:20.680]   that someone's already done.
[00:13:20.680 --> 00:13:25.320]   So if you know that you'll need at least 10 layers
[00:13:25.320 --> 00:13:28.200]   for your particular problem, then you
[00:13:28.200 --> 00:13:31.440]   can start there based on previous research.
[00:13:31.440 --> 00:13:33.280]   That's it.
[00:13:33.280 --> 00:13:34.520]   That's a great suggestion.
[00:13:34.520 --> 00:13:38.480]   So Ashi asks, can you share some links
[00:13:38.480 --> 00:13:41.880]   where I can begin to learn about NLP?
[00:13:41.880 --> 00:13:43.880]   So blogs or anything else?
[00:13:43.880 --> 00:13:46.640]   Also, I'm going to drop this--
[00:13:46.640 --> 00:13:48.680]   this blog post is by Andrej Karpathy.
[00:13:48.680 --> 00:13:52.200]   And he talks about how to start from a really small model
[00:13:52.200 --> 00:13:55.000]   and build it out, slowly add layers and stuff.
[00:13:55.000 --> 00:13:57.240]   I highly recommend reading this if you want to--
[00:13:57.240 --> 00:13:59.840]   Definitely check it out, guys.
[00:13:59.840 --> 00:14:01.520]   But sorry, the question was, what
[00:14:01.520 --> 00:14:05.040]   are your favorite resources to learn about NLP?
[00:14:05.040 --> 00:14:07.120]   To learn about NLP.
[00:14:07.120 --> 00:14:09.240]   OK.
[00:14:09.240 --> 00:14:12.760]   Ooh, a good one is this blog called ruder.io.
[00:14:12.760 --> 00:14:16.360]   So R-U-D-E-R dot I-O. It's not my blog.
[00:14:16.360 --> 00:14:17.720]   I wish it was.
[00:14:17.720 --> 00:14:20.200]   But it's Sebastian Ruder's blog.
[00:14:20.200 --> 00:14:22.440]   He's a NLP scientist.
[00:14:22.440 --> 00:14:26.040]   And he has some really good writing on natural language
[00:14:26.040 --> 00:14:27.080]   processing.
[00:14:27.080 --> 00:14:31.000]   But just to get started, I recommend checking out--
[00:14:31.000 --> 00:14:37.360]   there's an NLP video, I think, by--
[00:14:37.360 --> 00:14:40.840]   there's definitely one by Andrew Ning on sequence models.
[00:14:40.840 --> 00:14:42.240]   I really recommend that one.
[00:14:42.240 --> 00:14:44.120]   It should be on Coursera.
[00:14:44.120 --> 00:14:45.640]   OK.
[00:14:45.640 --> 00:14:51.080]   Charles, if you can drop that link for us, that would be great.
[00:14:51.080 --> 00:14:51.800]   Thanks, Charles.
[00:14:51.800 --> 00:14:56.840]   So someone asked, when would you actually
[00:14:56.840 --> 00:14:59.640]   implement a model from scratch?
[00:14:59.640 --> 00:15:01.960]   Would you ever do that?
[00:15:01.960 --> 00:15:04.560]   Yeah.
[00:15:04.560 --> 00:15:06.560]   Again, it depends on your use case.
[00:15:06.560 --> 00:15:12.120]   My philosophy is usually you can use something that already
[00:15:12.120 --> 00:15:17.000]   exists to get you maybe like 80%, 90% of the way.
[00:15:17.000 --> 00:15:19.640]   It's definitely a lot cheaper time-wise
[00:15:19.640 --> 00:15:22.040]   because you can get it going much faster.
[00:15:22.040 --> 00:15:28.160]   But let's say you need to add some customization to it.
[00:15:28.160 --> 00:15:30.480]   Let's say you're working in a really novel space
[00:15:30.480 --> 00:15:33.160]   where it's a really specific problem
[00:15:33.160 --> 00:15:35.200]   that a generalized model won't work for.
[00:15:35.200 --> 00:15:37.440]   Or another example more recently that I faced
[00:15:37.440 --> 00:15:41.480]   was I was trying to add model interpretability capabilities.
[00:15:41.480 --> 00:15:46.320]   So that's like integrated gradients.
[00:15:46.320 --> 00:15:48.640]   Check out PyTorch Captum if you haven't heard of it.
[00:15:48.640 --> 00:15:52.320]   But basically, it makes it difficult to make
[00:15:52.320 --> 00:15:54.640]   your models interpretable if you're using someone else's
[00:15:54.640 --> 00:15:55.560]   architecture.
[00:15:55.560 --> 00:15:57.640]   And you can't really add in the extra math
[00:15:57.640 --> 00:16:02.120]   or the extra explainability features.
[00:16:02.120 --> 00:16:04.760]   So actually, this is a good time to tell you guys.
[00:16:04.760 --> 00:16:06.760]   Pooja is actually going to do a whole series.
[00:16:06.760 --> 00:16:08.040]   So this is her first talk.
[00:16:08.040 --> 00:16:11.160]   But she's going to be doing two more talks at the next two
[00:16:11.160 --> 00:16:14.080]   salons about machine learning explainability.
[00:16:14.080 --> 00:16:15.580]   So if you're interested in that, you
[00:16:15.580 --> 00:16:17.960]   should definitely come to the next ones.
[00:16:17.960 --> 00:16:21.280]   I'm trying to see if there's more questions.
[00:16:21.280 --> 00:16:23.040]   What loss functions are effective
[00:16:23.040 --> 00:16:27.200]   for these kinds of networks?
[00:16:27.200 --> 00:16:30.320]   The loss function also kind of depends on your use case.
[00:16:30.320 --> 00:16:31.960]   It's a bit tough to say.
[00:16:31.960 --> 00:16:35.200]   A classic kind of starter one that if you're not familiar
[00:16:35.200 --> 00:16:37.960]   like what to use at all, I recommend just going with--
[00:16:37.960 --> 00:16:42.840]   I don't know.
[00:16:42.840 --> 00:16:44.640]   I guess it really depends on your use case.
[00:16:44.760 --> 00:16:49.240]   Cross-entropy is a good one for some things.
[00:16:49.240 --> 00:16:54.160]   It really depends on what you're trying to penalize
[00:16:54.160 --> 00:16:58.000]   and how much you're trying to penalize it.
[00:16:58.000 --> 00:17:00.320]   So that's a really broad question.
[00:17:00.320 --> 00:17:01.440]   I'm not sure--
[00:17:01.440 --> 00:17:02.840]   Lavanya, do you know any resources
[00:17:02.840 --> 00:17:05.040]   off the top of your head for just
[00:17:05.040 --> 00:17:07.480]   comparing and contrasting loss functions?
[00:17:07.480 --> 00:17:08.360]   Yes.
[00:17:08.360 --> 00:17:09.360]   I will drop it in.
[00:17:09.360 --> 00:17:10.920]   I'd have to look through my notes.
[00:17:10.920 --> 00:17:12.640]   But this is really--
[00:17:12.640 --> 00:17:15.800]   this guy, he does the deepest dives
[00:17:15.800 --> 00:17:17.760]   on these basic parts of neural networks
[00:17:17.760 --> 00:17:20.200]   that most people don't pay attention to.
[00:17:20.200 --> 00:17:22.480]   But I'll drop that in.
[00:17:22.480 --> 00:17:24.960]   I think those are all the questions.
[00:17:24.960 --> 00:17:26.840]   We'll see you again next week, I guess.
[00:17:26.840 --> 00:17:28.560]   I'm super excited.
[00:17:28.560 --> 00:17:31.120]   Yeah, thanks for having me here, guys.

