
[00:00:00.000 --> 00:00:02.940]   to help build eight reliable agents.
[00:00:02.940 --> 00:00:04.240]   Let's open up.
[00:00:04.240 --> 00:00:12.480]   Hello, everyone.
[00:00:12.480 --> 00:00:16.160]   My name is Tan, and it's very hard to follow up
[00:00:16.160 --> 00:00:17.500]   like who don't research her, right?
[00:00:17.500 --> 00:00:18.980]   From his work here, like figuring out
[00:00:18.980 --> 00:00:21.900]   how to do the best problems in the world.
[00:00:21.900 --> 00:00:24.180]   But I'm excited to talk about UEVAC here.
[00:00:24.180 --> 00:00:27.360]   My name is Tan, and as I said, I have been at UEVAC
[00:00:27.360 --> 00:00:29.800]   for about four years, we are building the AI private sector
[00:00:29.800 --> 00:00:31.980]   for all of our customers.
[00:00:31.980 --> 00:00:35.260]   And the idea is that people are
[00:00:35.260 --> 00:00:37.920]   continuously bad at making financial decisions.
[00:00:37.920 --> 00:00:39.820]   Like, which subscription to capital
[00:00:39.820 --> 00:00:42.200]   is a digital decision for many people to itself.
[00:00:42.200 --> 00:00:45.560]   So imagine that how you think about loan investment,
[00:00:45.560 --> 00:00:49.060]   how to-- especially in a situation like new bank,
[00:00:49.060 --> 00:00:52.920]   where we are the third largest bank in Brazil,
[00:00:52.920 --> 00:00:55.980]   the fastest growing bank in Mexico and Colombia.
[00:00:55.980 --> 00:00:59.280]   We have given first credit card access to 21 million people
[00:00:59.280 --> 00:01:01.740]   in Brazil in last five years alone.
[00:01:01.740 --> 00:01:04.500]   And just to be sure, these numbers are actually outdated
[00:01:04.500 --> 00:01:06.500]   because we just revealed our numbers yesterday
[00:01:06.500 --> 00:01:10.640]   in our earnings call, and these numbers have gone up since then.
[00:01:10.640 --> 00:01:13.800]   But the interesting part is that right in the very beginning,
[00:01:13.800 --> 00:01:19.140]   that we ran into the world of ChatDBT, we got into it.
[00:01:19.140 --> 00:01:21.480]   And we have been working very closely with Langshade, Langshade,
[00:01:21.480 --> 00:01:23.360]   Langshade, and all the teams here.
[00:01:23.360 --> 00:01:25.980]   And I'm excited to talk about a few things we have built,
[00:01:25.980 --> 00:01:32.840]   and how we have built it, and also, like, how we are going to evaluate them effectively.
[00:01:32.840 --> 00:01:36.460]   So just to be clear, I'm going to talk about two different applications here.
[00:01:36.460 --> 00:01:37.660]   First one is the Chatbot.
[00:01:37.660 --> 00:01:39.120]   No surprise.
[00:01:39.120 --> 00:01:41.820]   We have 120 million, almost, users, right?
[00:01:41.820 --> 00:01:46.380]   And we get almost 8.5 million contacts every month.
[00:01:46.380 --> 00:01:50.180]   And out of that, Chat is our main channel.
[00:01:50.180 --> 00:01:54.880]   And in that channel, right now, 16% of them are first dead with elements.
[00:01:54.880 --> 00:01:59.120]   The results are improving, and we are building agents for different situations,
[00:01:59.120 --> 00:02:00.840]   and then we'll talk about them.
[00:02:00.840 --> 00:02:04.460]   And the next one is actually more interesting, and we'll talk about these two applications,
[00:02:04.460 --> 00:02:11.520]   because in the world of finance, I just heard from Hervey that the complexity of the legal matters are important.
[00:02:11.520 --> 00:02:14.960]   For finance, every single dollar, every single thing matters.
[00:02:14.960 --> 00:02:21.460]   And that is important because it creates trust, it makes sure the users are happy with our service, and so forth.
[00:02:21.460 --> 00:02:24.080]   So just for a second, let's look at this application.
[00:02:24.080 --> 00:02:33.820]   So in the application, you can see that we have built an agentic system that can do money transfer over voice, image, and chat
[00:02:33.820 --> 00:02:37.500]   at very low inaccuracy, that I'll show you the numbers really soon.
[00:02:37.500 --> 00:02:41.780]   Here you can see that the user is connecting their account with WhatsApp.
[00:02:41.780 --> 00:02:46.580]   We are asking them multiple times about their password, etc., to make sure they have the right contacts.
[00:02:46.580 --> 00:02:50.980]   And once they do all of them, they can give a very simple instruction in a bit
[00:02:50.980 --> 00:02:55.660]   that, "Hey, make a transfer to, let's say, 400 TIs."
[00:02:55.660 --> 00:03:02.620]   We confirm this is the one that users want, and then, once it confirms, it makes the transfer.
[00:03:02.620 --> 00:03:06.660]   All you need is to take around 70 seconds to make this transfer to 9 different screens.
[00:03:06.660 --> 00:03:08.740]   It's taking less than 30 seconds now.
[00:03:08.740 --> 00:03:16.220]   And you can see the C# is more than 90%, less than 0.5% in accuracy, so on and so forth.
[00:03:16.220 --> 00:03:18.180]   And you're doing that on scale.
[00:03:18.180 --> 00:03:22.860]   And I will talk about where evaluations matter and what kind of things matter.
[00:03:22.860 --> 00:03:29.820]   So just to be on the same page, the experience of building a chatbot and building an application like this is very different.
[00:03:29.820 --> 00:03:37.500]   Because you need to iterate, but at the same time, you need to think about how to not build one observation.
[00:03:37.500 --> 00:03:48.180]   Because if you're building it for one application of this kind, imagine a finance world, you're doing hundreds of these operations to make these changes, or to make money movements, or make micro decisions.
[00:03:48.180 --> 00:03:53.180]   Then you're building hundreds of separate systems and agents, which is just not scalable.
[00:03:53.180 --> 00:03:57.860]   So, thinking of the fact, what is NuBank LLM ecosystem?
[00:03:57.860 --> 00:04:03.860]   NuBank LLM ecosystem has four different layers.
[00:04:03.860 --> 00:04:07.860]   The first one is core engine, testing and evals, tools, and development experience.
[00:04:07.860 --> 00:04:09.860]   I don't have time, unfortunately, to go over in front of them.
[00:04:09.860 --> 00:04:19.860]   But you can see that in three of them right now we are working very closely with LangSleep, and testing and evals is something we'll talk about, LLM as a job, and also online forward evaluation.
[00:04:19.860 --> 00:04:28.860]   And also in the developed experience side, we are using LangGraph and LangSleep from the very beginning from LangSleep, not LangGraph, we are using all of them.
[00:04:28.860 --> 00:04:32.860]   Now, how do we use it, and why it matters?
[00:04:32.860 --> 00:04:33.860]   Let's see.
[00:04:33.860 --> 00:04:34.860]   Okay.
[00:04:34.860 --> 00:04:51.860]   So, the first thing that happens is that without LangGraph, we cannot do more faster iterations, and cannot make it very standard than what a canonical approach we can take to build agentic systems or any kind of black systems.
[00:04:51.860 --> 00:04:56.860]   So, the learning there is that complex LM flows can be hard to analyze.
[00:04:56.860 --> 00:05:05.860]   centralized LL logs and repositories and graphical interface helps people to make faster decisions, because we don't want only our developers to make decisions.
[00:05:05.860 --> 00:05:19.860]   We want our business users to also contribute to it, and the way to do it is by democratizing data and giving them access to how a business analyst, our product managers, our product operations, our total operations,
[00:05:19.860 --> 00:05:29.860]   that how they can make faster decisions in terms of how they can make faster decisions in terms of prompt, in terms of adding inputs, in terms of adding parameters of different kinds, right?
[00:05:29.860 --> 00:05:34.860]   And last but not the least, we can decrease the cognitive effort to represent flows.
[00:05:34.860 --> 00:05:41.860]   and this is something that Treyya was mentioning about that the human interaction is difficult for machine-term discovery.
[00:05:41.860 --> 00:05:44.860]   To graph basically makes that process easier for us.
[00:05:44.860 --> 00:05:49.860]   Now, to be true to my presentation, I will go to the evaluation part.
[00:05:49.860 --> 00:05:55.860]   And on the evaluation side, I will first talk about a few different challenges overall we have.
[00:05:55.860 --> 00:06:00.860]   The first thing is that, as we heard from Kirby that they are not only in Antarctica.
[00:06:00.860 --> 00:06:03.860]   We are only in three countries, so it's a much smaller problem set.
[00:06:03.860 --> 00:06:14.860]   But still you can imagine that when you're dealing with like Portuguese, Spanish, the languages, the dialects, the kind of way people talk, etc., that changes across the country.
[00:06:14.860 --> 00:06:21.860]   And we have 58% of Brazilian population that are customers now, so we have to understand what users are talking about, etc., very extensively.
[00:06:21.860 --> 00:06:24.860]   The second thing is that Nubank's brand presence is huge.
[00:06:24.860 --> 00:06:30.860]   We are more popular than some of the well-known brands like McDonald's or Nike even in Brazil.
[00:06:30.860 --> 00:06:37.860]   So we cannot do anything, especially when it comes to jailbreak or garters, it's very important for us to keep a very high cost.
[00:06:37.860 --> 00:06:40.860]   And last but not the least, we have to pay curate in our messaging.
[00:06:40.860 --> 00:06:43.860]   Because at the end of the day, we are dealing with people's funding.
[00:06:43.860 --> 00:06:51.860]   And money is something that people care about, about advocacy, and losing trust over money transfer is very easy.
[00:06:53.860 --> 00:07:00.860]   So taking a step back again, actually moving a little forward, on the customer service side and the money transfer is based,
[00:07:00.860 --> 00:07:06.860]   we have very different needs from a business side and from a technical side that are kind of valuations we need.
[00:07:06.860 --> 00:07:13.860]   So in the case of customer service, in addition to accuracy, what matters a lot is how are we approaching a customer.
[00:07:13.860 --> 00:07:19.860]   If a customer is calling us, "Hey, where is my card?" or "Hey, I see this chart that I don't recognize."
[00:07:19.860 --> 00:07:25.860]   If we give a very robotic experience, we lose the customer's trust and equity and it matters.
[00:07:25.860 --> 00:07:28.860]   It's very easy for human to have this connection.
[00:07:28.860 --> 00:07:31.860]   It's very hard for machine to have this connection and we all know that.
[00:07:31.860 --> 00:07:33.860]   Also, very high flattening doesn't work.
[00:07:33.860 --> 00:07:40.860]   I think a lot of you have seen that what happened with the activity 4.1 model, last week, and we called and then they launched.
[00:07:40.860 --> 00:07:47.860]   So in addition, in order to do this to judge well, we need to think about, do we understand customers in terms well?
[00:07:47.860 --> 00:07:53.860]   Do we understand that how are we retrieving content and context from different sources that we have internally?
[00:07:53.860 --> 00:07:56.860]   What is the deep link accuracy that we have?
[00:07:56.860 --> 00:08:06.860]   Because imagine our app is 3,000 pages, and in 3,000 pages we have hundreds of deep links and basically landing a user to the way up of the node
[00:08:06.860 --> 00:08:13.860]   and then asking the server through different clicks and go to the page where they get self-service is very tedious and not very effective.
[00:08:13.860 --> 00:08:17.860]   And last but not the least, we need to make sure that we are not hallucinating.
[00:08:17.860 --> 00:08:24.860]   While there's a money transfer, tone and sentiment is okay, but we need to be accurate.
[00:08:24.860 --> 00:08:29.860]   But the accuracy is not only about the transfer money, but also who we are transferring.
[00:08:29.860 --> 00:08:32.860]   Which source we are using for transfer?
[00:08:32.860 --> 00:08:35.860]   Does the person have enough money in their account?
[00:08:35.860 --> 00:08:36.860]   Are there a broad suspect?
[00:08:36.860 --> 00:08:38.860]   Do they have a petting collection?
[00:08:38.860 --> 00:08:42.860]   All of these things are very intricately connected because our customers are using not only one product,
[00:08:42.860 --> 00:08:50.860]   but also the product from lending, to financing, to investment, to banking, to credit card, etc.
[00:08:50.860 --> 00:08:53.860]   And also, often times, they have dependent accounts.
[00:08:53.860 --> 00:08:54.860]   They have multiple cards.
[00:08:54.860 --> 00:08:56.860]   So if you look at all of them together.
[00:08:56.860 --> 00:09:01.860]   So what's important there is that can you identify the named agent recognition properly?
[00:09:01.860 --> 00:09:06.860]   Because you can say, hey, send $100 to my brother.
[00:09:06.860 --> 00:09:11.860]   Now, say you only have one brother, you have saved your brother, and you have sent money before, it's easy.
[00:09:11.860 --> 00:09:15.860]   But imagine the situation where you have not done that with your multiple brothers.
[00:09:15.860 --> 00:09:21.860]   And it's like, my favorite brother, my next favorite brother, didn't come to identify with which brother I'm talking about
[00:09:21.860 --> 00:09:22.860]   to send the money, right?
[00:09:22.860 --> 00:09:25.860]   Because then they don't want to send money to my next favorite brother.
[00:09:25.860 --> 00:09:31.860]   The next one is about making sure the correct interpretation of the user input.
[00:09:31.860 --> 00:09:36.860]   Because if the user is saying that, hey, I want to send $100, but do it tomorrow.
[00:09:36.860 --> 00:09:39.860]   That's a different instruction than doing it right now.
[00:09:39.860 --> 00:09:42.860]   Because if I do it right now, maybe you will learn that it over time.
[00:09:42.860 --> 00:09:46.860]   The last but not the least also identifies that what is the correct action.
[00:09:46.860 --> 00:09:50.860]   Because the user might be saying that, I don't want to send it.
[00:09:50.860 --> 00:09:52.860]   The last tape you saw, I want to cancel it.
[00:09:52.860 --> 00:09:54.860]   So we didn't understand that as well.
[00:09:54.860 --> 00:09:55.860]   So all of these things matter.
[00:09:55.860 --> 00:09:58.860]   And evaluation for all of these things matter.
[00:09:58.860 --> 00:10:02.860]   Without them, we cannot launch a problem.
[00:10:02.860 --> 00:10:08.860]   So in absence of Evalor, in absence of a tool like Landscape, what happens,
[00:10:08.860 --> 00:10:11.860]   is that we have a linear path of development.
[00:10:11.860 --> 00:10:12.860]   We are running every tests.
[00:10:12.860 --> 00:10:14.860]   Because we make all decisions with any tests.
[00:10:14.860 --> 00:10:20.860]   I'm not sure if I could cover it before, but we have 1,800 services that we do deployment every 2 minutes.
[00:10:20.860 --> 00:10:23.860]   So we do every decision by any tests.
[00:10:23.860 --> 00:10:26.860]   And that's with the linear path.
[00:10:26.860 --> 00:10:34.860]   But if we have a system that can very well connect the traces and give observabilities,
[00:10:34.860 --> 00:10:39.860]   give login and then alert on top of it, so on and so forth.
[00:10:39.860 --> 00:10:45.860]   Then we have a full cycle of observability to filtering, to define data sets, to run experiments and so on.
[00:10:45.860 --> 00:10:50.860]   And this is the final we have in other situations.
[00:10:50.860 --> 00:10:55.860]   And we are building with Landscape for our generative AI applications.
[00:10:55.860 --> 00:11:03.860]   I think these two things I've heard a few times in this last couple of talks about offline evaluation and online evaluation.
[00:11:03.860 --> 00:11:06.860]   So I will not go in very deep details of it.
[00:11:06.860 --> 00:11:14.860]   But as you can see, in the case of offline evaluation, this is after the experiment results,
[00:11:14.860 --> 00:11:20.860]   we take them to the other apps and we have individual evaluation and we have pairwise evaluation.
[00:11:20.860 --> 00:11:31.860]   For both of them, I will mostly, I will talk about it later, but we primarily use human labellers in that process.
[00:11:31.860 --> 00:11:35.860]   And then we currently also using algorithms and other custom heuristics.
[00:11:35.860 --> 00:11:41.860]   Based on all of that, we run statistical tests and the linear variant is something that we launch.
[00:11:41.860 --> 00:11:46.860]   Things get more interesting, actually not at this stage, but at the online stage.
[00:11:46.860 --> 00:11:54.860]   Because in online evaluation, you can run things in your sandboxes, in your own, like, more controlled environments.
[00:11:54.860 --> 00:12:00.860]   And in that situation, you have a more continuous view of improvement and development.
[00:12:00.860 --> 00:12:14.860]   If we only do the online evaluation, then our decision-making speed, especially for developers and analysts, is much slower.
[00:12:14.860 --> 00:12:22.860]   But if we can do good online evaluation, and tracing, and logging, and alerting, etc., then our development speed goes up significantly.
[00:12:22.860 --> 00:12:25.860]   So we are doing both of them.
[00:12:25.860 --> 00:12:31.860]   Now, last but not least, I will talk about LLM as a judge, which is something we have talked about a few times.
[00:12:31.860 --> 00:12:34.860]   And the question basically goes back quite many years.
[00:12:34.860 --> 00:12:37.860]   Imagine the situation I was describing about the money transfer.
[00:12:37.860 --> 00:12:43.860]   In that situation, you need to understand who we are sending, what the requests, how much money, from where, all of that.
[00:12:43.860 --> 00:12:52.860]   And doing all of that, sending, let's say, we are currently doing, say, a few hundred thousand or a few million such transactions every day.
[00:12:52.860 --> 00:13:00.860]   That amount of data, and that amount of labeling, even if we do sampling, it's not enough to maintain the quality of the product.
[00:13:00.860 --> 00:13:05.860]   And that's why we need to do more labeling, and doing it only when it's not scalable.
[00:13:05.860 --> 00:13:10.860]   Because training, how people understand, the mistakes people make, and so forth.
[00:13:10.860 --> 00:13:20.860]   So our part was that, let's build LLM as a judge, and try to keep the quality of the judge at the same level of human.
[00:13:20.860 --> 00:13:26.860]   And so we started with the first test, with a simple prompt, we do the 4-0 mini model, because it's cheap.
[00:13:26.860 --> 00:13:29.860]   We didn't do any time tuning in, and see how it works.
[00:13:29.860 --> 00:13:38.860]   And we got that in, like, humans were making 80% accurate decisions, and 20% mistakes, and something like that.
[00:13:38.860 --> 00:13:44.860]   Everyone's exact doesn't show that, but you can imagine that way, a little bit as an accuracy metric.
[00:13:44.860 --> 00:13:56.860]   We are at 51%, and in the case 2, we moved to a fine-tuned model, and we increased the 8-1 score from 51 to 59.
[00:13:56.860 --> 00:14:04.860]   Next test, we changed the prompt, and we got B2, and we got a big jump of 11% point of 17.
[00:14:04.860 --> 00:14:10.860]   So we made another declaration of 4-0, from 4-0 mini to 4-0, it's a better, better, and bigger model.
[00:14:10.860 --> 00:14:24.860]   Changed the prompt again in test 5, changed the pine-tuning again in test 6, and this is where we landed, where we are at x-1 score of 79% compared to 80% of humans, which is quite comparable.
[00:14:24.860 --> 00:14:36.860]   Now, you might ask, next time, why did you move from test 5 to test 6, the prompt was 80, and 79%, because in 79%, we are identifying the inaccurate information better,
[00:14:36.860 --> 00:14:41.860]   and a more, like, more, like, more, like, more inaccuracy we are catching there, that's why we are in here.
[00:14:41.860 --> 00:14:51.860]   And just to no surprise, but actually quite surprising, this whole development took us maybe around 2 weeks, with a couple of developers, to go through these 6 iterations,
[00:14:51.860 --> 00:14:57.860]   and we can only do it because we have the online tracing and system in place, otherwise it wouldn't be possible.
[00:14:57.860 --> 00:15:09.860]   So, taking up, like, wrapping it up all, there is no magic in building any agent, any element, it's hard work, evals are hard work,
[00:15:09.860 --> 00:15:17.860]   and if you don't evaluate, you don't know what you are building, and if you don't know what you are building, then you cannot ship it to the world.
[00:15:17.860 --> 00:15:24.860]   So, do more evals, spend more time, understand what your users are saying, think about not only hallucinations and anxiety, etc.,
[00:15:24.860 --> 00:15:31.860]   but also, like, nuanced situations of, like, empathy, tone of voice, those things matter.
[00:15:31.860 --> 00:15:39.860]   We are in a very exciting time, and thank you all for listening to me, and thank you all for listening to me.
[00:15:39.860 --> 00:15:52.860]   We are now going to a 20 minute break before our next session.
[00:15:52.860 --> 00:15:59.860]   I want to mention to you about Boba, a bar located in a very group-top, an espresso-carsel, in an area for the residents.
[00:15:59.860 --> 00:16:02.680]   Thank you for the rise and pollinators. On through these.

