
[00:00:00.000 --> 00:00:02.000]   -
[00:00:02.000 --> 00:00:18.480]   - Bitter Layout, or the alternative name for this talk,
[00:00:18.480 --> 00:00:20.620]   how I learned to love the model picker.
[00:00:20.620 --> 00:00:21.960]   And hopefully you will too.
[00:00:21.960 --> 00:00:24.320]   So the idea for this talk started
[00:00:24.320 --> 00:00:26.940]   when I was perusing all the AI-first apps
[00:00:26.940 --> 00:00:28.720]   I use all the time and just realizing
[00:00:28.720 --> 00:00:31.180]   how similar they're all starting to look.
[00:00:31.180 --> 00:00:33.840]   Very consistent layout between them all.
[00:00:33.840 --> 00:00:36.720]   And it's not just the chatbots and the Anster engines,
[00:00:36.720 --> 00:00:39.860]   it's also the creative tools, like coding assistants,
[00:00:39.860 --> 00:00:41.980]   like V0 and even Canva.
[00:00:41.980 --> 00:00:45.040]   They're all starting to use this very similar layout.
[00:00:45.040 --> 00:00:48.220]   They've got an input field, they've got this turn-by-turn UX,
[00:00:48.220 --> 00:00:49.780]   they've got this dropdown with just
[00:00:49.780 --> 00:00:52.160]   way too many models to pick from.
[00:00:52.160 --> 00:00:54.500]   And it all feels like they're kind of retrofitting stuff
[00:00:54.500 --> 00:00:56.820]   into this chatbot UX.
[00:00:56.820 --> 00:01:00.280]   But don't worry this talk is not about if chat is the future or not.
[00:01:00.280 --> 00:01:03.120]   I think we've all heard that enough times, at least I have.
[00:01:03.120 --> 00:01:06.660]   And Swix did a good job of humbling all designers with this tweet,
[00:01:06.660 --> 00:01:09.960]   where he basically called all the design thought leaders out
[00:01:09.960 --> 00:01:11.280]   who were saying chat is dead.
[00:01:11.280 --> 00:01:13.280]   And then, or sorry, chat's not the future.
[00:01:13.280 --> 00:01:15.240]   And then they show off their cool demo.
[00:01:15.240 --> 00:01:17.580]   And then shortly after we'll just go back to using ChatGPT.
[00:01:17.580 --> 00:01:19.580]   So, fair point.
[00:01:19.580 --> 00:01:25.120]   I think that right now we're in this state of a bit of a dualistic future of AI UX.
[00:01:25.120 --> 00:01:29.420]   So, I've called this first section Schrodinger's Chat because we all,
[00:01:29.420 --> 00:01:33.380]   you know, all designers know how many usability issues that the chatbots have,
[00:01:33.380 --> 00:01:35.880]   but yet we all still use them every day.
[00:01:35.880 --> 00:01:41.020]   So, it's kind of like obviously they are the future, but at the same time obviously they shouldn't be.
[00:01:41.020 --> 00:01:46.220]   So, I won't go into my thoughts on if they should be, but I'll do some, some anthropology here,
[00:01:46.220 --> 00:01:50.820]   just getting everybody up to speed in case you're not familiar with the great chatbot debate.
[00:01:50.820 --> 00:01:54.140]   I'm not sure if people realize how long we've been debating this,
[00:01:54.140 --> 00:01:58.600]   but I can track it all the way back to 2022 with this post from Linus Lee,
[00:01:58.600 --> 00:02:01.100]   which, this is a great post by the way, still holds up today,
[00:02:01.100 --> 00:02:07.700]   but he essentially says he doesn't think that exposing the raw text completion is really the right paradigm long term.
[00:02:07.700 --> 00:02:12.680]   And so, you know, note the date, May 2022, because a couple months after that,
[00:02:12.680 --> 00:02:16.040]   we have ChatGPT essentially saying, hold my beer.
[00:02:16.040 --> 00:02:20.140]   And, you know, if that's not the right UI paradigm, it certainly didn't bother them.
[00:02:20.140 --> 00:02:25.580]   And I think a lot of other designers have kind of taken note of the escape velocity of that.
[00:02:25.580 --> 00:02:31.400]   But still, the next year, May 2023, we saw some other great posts from people like Emilia Wadenberger,
[00:02:31.400 --> 00:02:37.980]   and then the next month, Maggie Appleton, who are making great arguments about why Chat's not the future.
[00:02:37.980 --> 00:02:42.580]   These, I think, hold up pretty well, but at the same time, yeah, obviously,
[00:02:42.580 --> 00:02:45.680]   you have other designers who are arguing how intuitive it is.
[00:02:45.680 --> 00:02:50.840]   And then as we progress and ChatGPT hits escape velocity,
[00:02:50.840 --> 00:02:56.020]   we're kind of seeing everybody just start to meme the defense of chat, which is like, just look at the chart, you know?
[00:02:56.020 --> 00:02:58.940]   It's like, obviously, it's working, right?
[00:02:58.940 --> 00:03:00.740]   And I think there's something interesting about that.
[00:03:00.740 --> 00:03:05.620]   If you can kind of come to the debate with a meme, it means there's something a bit intuitive about your argument.
[00:03:05.620 --> 00:03:09.120]   So, fair enough.
[00:03:09.120 --> 00:03:16.280]   But then still, even in March of this year, we've had people like Julian Lear writing very good reasons of why Chat should not be the future.
[00:03:16.280 --> 00:03:22.280]   And he's, like, showing clock speed here relative to the different interface paradigms, so it's very convincing.
[00:03:22.280 --> 00:03:28.280]   He pretty much says it's a bottleneck, but at the same time, you know, we'll all probably still go back to using Chat after this.
[00:03:28.280 --> 00:03:38.740]   And so I'll segue into the next section here, which is called models and modes, and this is on this idea of the model picker,
[00:03:38.740 --> 00:03:43.440]   which is this other UI paradigm that's been developing alongside ChatBot's popularity.
[00:03:43.440 --> 00:03:49.440]   It's that, you know, I'm sure everybody knows what it is, but to be clear, it's this dropdown where you just have to select from a million different models.
[00:03:49.440 --> 00:03:57.680]   And so I put this, I made this section in memory of Larry Tesler, if you're not familiar with him, he's kind of a big deal, invented, like, copy and paste and stuff.
[00:03:57.680 --> 00:04:02.040]   But another thing he was famous for was apparently saying, don't mode me in.
[00:04:02.040 --> 00:04:08.740]   I don't actually believe he would say this, but, I mean, the quote is, like, attributed to him, but, you know, he hated modes.
[00:04:08.740 --> 00:04:16.100]   And if you're not familiar with modes, this is a setting and a UI where once you flip it, all of a sudden your inputs are mapping to drastically different outputs.
[00:04:16.100 --> 00:04:21.440]   And so Larry Tesler hated this, he thought it was unintuitive and he wanted everything to be modeless.
[00:04:21.440 --> 00:04:30.100]   And I don't know how many, or, sorry, to give an example of a mode, just to be clear, caps lock, this is a mode, right, you hit caps lock and now your keyboard performs differently.
[00:04:30.100 --> 00:04:43.100]   And then a more recent mode, excuse me, I'm not sure how many people would agree with me on this, but I think that the model picker, sorry, this is my voice at, like, the worst time,
[00:04:43.100 --> 00:04:46.100]   I think that the model picker is a bit of a mode selector as well.
[00:04:46.100 --> 00:04:55.100]   You know, it's, obviously we're dealing with stochastic output and general value applications, so everything, it's kind of not a distinct change in setting,
[00:04:55.100 --> 00:04:58.100]   but once you flip a different model, you have a whole step change of output.
[00:04:58.100 --> 00:05:03.100]   So, to me, that's kind of like a mode selector, and here's a quick video to illustrate this point.
[00:05:03.100 --> 00:05:12.100]   This is a bit old, it's the, an older version of ChatGPT, but you can see I'm trying to use certain modes and the model is not supporting it.
[00:05:12.100 --> 00:05:17.100]   So I have to go through this menu and find which model allows me to use this mode.
[00:05:17.100 --> 00:05:25.100]   And so the argument here is like we're kind of putting modes on top of modes, right, you now have to match models to modes and it's not super intuitive.
[00:05:25.100 --> 00:05:30.100]   They've actually done a great job of redesigning this lately, so this is certainly not like throwing shade at OpenAI.
[00:05:30.100 --> 00:05:36.100]   I think they have a great design team, but I'm just kind of illustrate a moment in time when this was super frustrating to me.
[00:05:36.100 --> 00:05:42.100]   And, like, you kind of just want to talk to the model and be like, here's my use case, like, what mode and what model should I use?
[00:05:42.100 --> 00:05:46.100]   But I don't know, maybe, maybe the model will pick itself and so that won't work.
[00:05:46.100 --> 00:05:51.100]   So this is really illustrating the point of the, the flexibility usability trade-off.
[00:05:51.100 --> 00:05:57.100]   This is a design principle where pretty much you're constantly trying to decide, like, how well do you understand user needs?
[00:05:57.100 --> 00:06:03.100]   And if you can pretty much pinpoint them down, then you can create a very, very usable optimized UX for them.
[00:06:03.100 --> 00:06:12.100]   And, but as you try to make a more flexible system, the usability tends to, to decrease because you're just increasing the amount of edge cases and the complexity.
[00:06:12.100 --> 00:06:13.100]   And the requirements.
[00:06:13.100 --> 00:06:18.100]   And I think that this is a trade-off that doesn't get talked enough about in this is chat the future debate.
[00:06:18.100 --> 00:06:27.100]   You know, we generally talk about in terms of absolutes, but it's really less of a yes and a no, or, and more of like a timeframe and like what trade-offs are we talking about?
[00:06:27.100 --> 00:06:37.100]   And so I'd like to segue into this next section, which is going to push the idea of like when we design interfaces, we really need to consider the zeitgeist that we're working in.
[00:06:37.100 --> 00:06:39.100]   So what are the trade-offs of the time?
[00:06:39.100 --> 00:06:41.100]   You know, what constraints we're working with?
[00:06:41.100 --> 00:06:44.100]   What timeframe could an interface be good for?
[00:06:44.100 --> 00:06:50.100]   The subtitle of this one is called the context of all in which we live and all that came before us.
[00:06:50.100 --> 00:06:53.100]   Easter egg for anybody who remembers that.
[00:06:53.100 --> 00:06:59.100]   And so to get into this section, I'd like to lean into a theory from the innovative solution.
[00:06:59.100 --> 00:07:02.100]   This is the follow up to the innovators dilemma.
[00:07:02.100 --> 00:07:11.100]   And in this book, they try to give you some guidelines on theory of how to approach building products with this idea of product architecture.
[00:07:11.100 --> 00:07:19.100]   So the architecture is generally this idea that you have a system and you're trying to figure out how the components in the system are interacting with each other or interfacing with them.
[00:07:19.100 --> 00:07:25.100]   And so when you start to understand how they interact with each other, you can see different attributes.
[00:07:25.100 --> 00:07:28.100]   So they map them out to these two distinct sides of a spectrum.
[00:07:28.100 --> 00:07:31.100]   You've got integrated architectures and then you've got modular architectures.
[00:07:31.100 --> 00:07:36.100]   And, you know, generally integrated, this is more common in early stage disruption.
[00:07:36.100 --> 00:07:38.100]   And you have proprietary technologies.
[00:07:38.100 --> 00:07:40.100]   They're very optimized, interdependent.
[00:07:40.100 --> 00:07:42.100]   It allows vertical scaling.
[00:07:42.100 --> 00:07:44.100]   And then to contrast that, you have modular.
[00:07:44.100 --> 00:07:47.100]   And this is generally when technologies start to commoditize.
[00:07:47.100 --> 00:07:51.100]   And they can be more interdependent and you can allow horizontal scaling.
[00:07:51.100 --> 00:07:57.100]   But the key point in their theory is that you're never really on one side of the spectrum or the other.
[00:07:57.100 --> 00:07:59.100]   You're instead kind of bouncing between the two.
[00:07:59.100 --> 00:08:06.100]   And the industry as well is having different parts of the tech stack commoditize and decommoditize all at once.
[00:08:06.100 --> 00:08:14.100]   So as a designer or anybody else building, you pretty much have to figure out where are the strategic points to be integrated versus modular.
[00:08:14.100 --> 00:08:17.100]   And so their theory is like IBM is an example.
[00:08:17.100 --> 00:08:24.100]   When they started out making mainframe computers very integrated, then they shifted to personal computers and started to make it more modular.
[00:08:24.100 --> 00:08:31.100]   And then, of course, ended up the whole computer itself ended up commoditizing at some point and they got out of that business.
[00:08:31.100 --> 00:08:40.100]   So thinking through today what parts of the AI industry are commoditizing and decommoditizing can help us think about how to design interfaces.
[00:08:40.100 --> 00:08:49.100]   And so, of course, the main question probably everybody would ask when you start to pose this prompt is, are the models themselves commoditizing?
[00:08:49.100 --> 00:08:52.100]   Because this is kind of like the big topic that everybody debates.
[00:08:52.100 --> 00:08:56.100]   And it brings us to the bitter lesson from Rich Sutton.
[00:08:56.100 --> 00:09:03.100]   If you haven't read this, I definitely recommend doing so if it's not wise to build an AI today without knowing this lesson.
[00:09:03.100 --> 00:09:10.100]   But I'll take the TLDR for this talk is that we shouldn't assume that computation is constant as long as we're seeing scaling laws in effect.
[00:09:10.100 --> 00:09:19.100]   Like as long as the next model is still important, which you can see it still is like every time a new model comes out, everybody's like drop everything and let's check out the new model.
[00:09:19.100 --> 00:09:25.100]   As long as the scaling laws are still in effect, then we can assume that the models themselves are not commoditized.
[00:09:25.100 --> 00:09:31.100]   And so, the bitter lesson actually leads us to what I'll call the bitter design lesson, why not?
[00:09:31.100 --> 00:09:41.100]   Which is this idea that if the basis of competition is inference performance, then the UI itself must be primarily focused on conforming to the next model.
[00:09:41.100 --> 00:09:47.100]   So, said plainly, if the model's not commoditized, it's actually the interface that's the commodity now.
[00:09:47.100 --> 00:09:59.100]   And until that changes when models overshoot user needs and you don't need a rocket scientist doing whatever use case you're doing in ChatGPT, then we can start to explore different integrations within the interface.
[00:09:59.100 --> 00:10:06.100]   But until then, the primary job of every interface really has to be figuring out how to conform to the next model's capabilities.
[00:10:06.100 --> 00:10:14.100]   So, it's kind of a bitter design lesson because then you get layouts like this, which, you know, the bitter layout.
[00:10:14.100 --> 00:10:22.100]   Pretty uninspiring, not super usable, just not very cool at all, but the one thing this does really well is it can absorb the next model's capabilities.
[00:10:22.100 --> 00:10:31.100]   So, as soon as that next model comes out, jam it into the bitter layout, and then, you know, update your model picker, and your app is more intelligent.
[00:10:31.100 --> 00:10:40.100]   So, you know, I hate this design, but, like, as a designer, like, you can't really hate on this ROI, which is you just add one line item, and now your app is NX more intelligent.
[00:10:40.100 --> 00:10:43.100]   So, kind of hard to debate this as a design decision.
[00:10:43.100 --> 00:10:46.100]   So, that's the bitter design lesson.
[00:10:46.100 --> 00:10:52.100]   And the takeaway from this, I think, is, you know, I'm not ready to eat my words on saying Chat is not the future yet.
[00:10:52.100 --> 00:11:01.100]   But I think that it's quite clear that this one attribute of Chat, which is that it can really conform to the next model very easily, is one of the key features we need to keep in mind.
[00:11:01.100 --> 00:11:12.100]   And so, the future of AI/UX until we see models stop commoditizing, or excuse me, until models do commoditize, will be that the future of AI/UX must conform to the next model.
[00:11:12.100 --> 00:11:16.100]   So, that's the bitter design lesson, but how do we go from bitter to sweet?
[00:11:16.100 --> 00:11:18.100]   Right, what comes next?
[00:11:18.100 --> 00:11:24.100]   And, as most things in life, Brett Victor has already given a really good talk on this, so you should just watch this talk.
[00:11:24.100 --> 00:11:25.100]   It's much better than this one.
[00:11:25.100 --> 00:11:33.100]   But, it's called The Future of Programming, and he explains all of the kind of mistakes we've made over the past decades with thinking about programming.
[00:11:33.100 --> 00:11:41.100]   And, specifically uses this, or for this context, he uses this example of how people found it very hard to go from binary to soap, right?
[00:11:41.100 --> 00:11:43.100]   The binary programmers could not understand.
[00:11:43.100 --> 00:11:48.100]   You could give up control to the machine and use these abstraction layers efficiently.
[00:11:48.100 --> 00:11:50.100]   And, they like to hand code everything.
[00:11:50.100 --> 00:11:55.100]   And, of course, like, making this mind shift was really important, as we see in retrospect.
[00:11:55.100 --> 00:12:02.100]   And so, his lesson was, you should stop thinking in terms of procedures and start trying to think of programming in terms of goals and constraints.
[00:12:02.100 --> 00:12:07.100]   And, pretty much guiding programs with these higher layers of abstraction.
[00:12:07.100 --> 00:12:11.100]   And, so, designers, I think, are actually pretty well suited to do this.
[00:12:11.100 --> 00:12:17.100]   You know, it'll be a mindset shift to jump from thinking procedurally to goals and constraints.
[00:12:17.100 --> 00:12:22.100]   You know, we like to be very detailed in how we design user flows and consider all the edge cases and all that.
[00:12:22.100 --> 00:12:23.100]   And, it's very important.
[00:12:23.100 --> 00:12:31.100]   But, we are likely going to have to start to move up a layer of abstraction as we're seeing apps become more and more stochastic and dynamic.
[00:12:31.100 --> 00:12:32.100]   And, just more probabilistic.
[00:12:32.100 --> 00:12:36.100]   So, we can't really envision every possible path now.
[00:12:36.100 --> 00:12:43.100]   So, we have to think a bit more in terms of what constraints can we set and what goals can we set to get people along the happy path.
[00:12:43.100 --> 00:12:53.100]   And, so, you know, design systems, these are already things we use to guide developers and our designers to goals and set constraints for them.
[00:12:53.100 --> 00:12:56.100]   So, what happens when we start using this for generative UI, right?
[00:12:56.100 --> 00:13:02.100]   If we start to collaborate with a model, do we have a design system that keeps them within the right constraints?
[00:13:02.100 --> 00:13:05.100]   Quality assurance, is that like reinforcement learning?
[00:13:05.100 --> 00:13:11.100]   I'm not really sure, but like maybe it is when you go to like critique a design that a model has created.
[00:13:11.100 --> 00:13:14.100]   Will this be like a reinforcement learning loop?
[00:13:14.100 --> 00:13:20.100]   User stories, these are, you know, we're very good as designers as thinking about how to envision what the user is trying to do via user stories.
[00:13:20.100 --> 00:13:28.100]   And, these are kind of like system prompts in a way, like can the model become also a partner in helping set the goal for a user?
[00:13:28.100 --> 00:13:32.100]   And, we can translate some of these user stories into the system prompts themselves.
[00:13:32.100 --> 00:13:39.100]   So, this is pretty speculative, but I think this is a nice prompt for the future of UI design in the AI age.
[00:13:39.100 --> 00:13:48.100]   And, I'd like to end on this quote from Dario Amadai, who says he feels that generative AI systems are more grown than they are built.
[00:13:48.100 --> 00:14:00.100]   And, I like this as a prompt for inspiration for helping us do this shift mindsets and start to think about how the future of UX might be more one where it's less like a process of construction
[00:14:00.100 --> 00:14:03.100]   and perhaps more like a process of gardening.
[00:14:03.100 --> 00:14:14.100]   So, if we can embrace these types of lessons and start to think about design in a new way at a higher level, perhaps then we can move beyond the bitter layout.
[00:14:14.100 --> 00:14:15.100]   Thanks a lot.
[00:14:15.100 --> 00:14:16.100]   Thanks a lot.
[00:14:16.100 --> 00:14:16.100]   Thanks a lot.
[00:14:16.100 --> 00:14:16.100]   Thanks a lot.
[00:14:16.100 --> 00:14:17.100]   Thanks a lot.
[00:14:17.100 --> 00:14:17.100]   Thanks a lot.
[00:14:17.100 --> 00:14:18.100]   Thanks a lot.
[00:14:18.100 --> 00:14:19.100]   Thanks a lot.
[00:14:19.100 --> 00:14:19.100]   Thanks a lot.
[00:14:19.100 --> 00:14:21.160]   you
[00:14:21.160 --> 00:14:23.280]   We'll see you next time.

