
[00:00:00.000 --> 00:00:00.840]   That's good.
[00:00:00.840 --> 00:00:01.440]   All right, cool.
[00:00:01.440 --> 00:00:04.880]   So yeah, so I was asked to give this presentation
[00:00:04.880 --> 00:00:07.080]   on the foundations of deep learning, which
[00:00:07.080 --> 00:00:11.120]   is mostly going over basic feedforward neural networks
[00:00:11.120 --> 00:00:13.640]   and motivating a little bit deep learning
[00:00:13.640 --> 00:00:15.960]   and some of the more recent developments
[00:00:15.960 --> 00:00:17.480]   and some of the topics that you'll
[00:00:17.480 --> 00:00:20.120]   see across the next two days.
[00:00:20.120 --> 00:00:26.080]   So as Andrew mentioned, I have just an hour.
[00:00:26.080 --> 00:00:27.520]   So I'm going to go fairly quickly
[00:00:27.520 --> 00:00:29.160]   on a lot of these things, which I think
[00:00:29.160 --> 00:00:31.600]   would mostly be fine if you're familiar enough
[00:00:31.600 --> 00:00:35.320]   with some machine learning and a little bit about neural nets.
[00:00:35.320 --> 00:00:36.800]   But if you'd like to go into some
[00:00:36.800 --> 00:00:38.280]   of the more specific details, you
[00:00:38.280 --> 00:00:41.320]   can go check out my online lectures on YouTube.
[00:00:41.320 --> 00:00:44.400]   It's now taught by a much younger version of myself.
[00:00:44.400 --> 00:00:47.800]   And so just search for Hugo Larochelle.
[00:00:47.800 --> 00:00:50.640]   And I am not the guy doing a bunch of skateboarding.
[00:00:50.640 --> 00:00:52.920]   I'm the geek teaching about neural nets.
[00:00:52.920 --> 00:00:56.560]   So go check those out if you want more details.
[00:00:56.560 --> 00:00:58.880]   But so what I'll cover today is--
[00:00:58.880 --> 00:01:02.280]   I'll start with just describing and laying out
[00:01:02.280 --> 00:01:04.920]   the notation on feedforward neural networks, that
[00:01:04.920 --> 00:01:07.280]   is, models that take an input vector x--
[00:01:07.280 --> 00:01:09.320]   that might be an image or some text--
[00:01:09.320 --> 00:01:11.360]   and produces an output f of x.
[00:01:11.360 --> 00:01:13.120]   So I'll just describe forward propagation
[00:01:13.120 --> 00:01:16.040]   and the different types of units and the type of functions
[00:01:16.040 --> 00:01:17.680]   we can represent with those.
[00:01:17.680 --> 00:01:20.720]   And then I'll talk about how we actually train neural nets,
[00:01:20.720 --> 00:01:22.600]   describing things like loss functions,
[00:01:22.600 --> 00:01:26.160]   backpropagation that allows us to get a gradient for training
[00:01:26.160 --> 00:01:27.700]   with stochastic gradient descent,
[00:01:27.700 --> 00:01:29.840]   and mention a few tricks of the trade,
[00:01:29.840 --> 00:01:32.720]   so some of the things we do in practice to successfully train
[00:01:32.720 --> 00:01:33.680]   neural nets.
[00:01:33.680 --> 00:01:37.680]   And then I'll end by talking about some developments that
[00:01:37.680 --> 00:01:41.240]   are specifically useful in the context of deep learning, that
[00:01:41.240 --> 00:01:45.220]   is, neural networks with several hidden layers that came out
[00:01:45.220 --> 00:01:46.920]   at the very--
[00:01:46.920 --> 00:01:49.880]   after the beginning of deep learning, say, in 2006.
[00:01:49.880 --> 00:01:52.200]   That is, things like dropout, batch normalization,
[00:01:52.200 --> 00:01:55.800]   and if I have some time, unsupervised pre-training.
[00:01:55.800 --> 00:01:57.560]   So let's get started.
[00:01:57.560 --> 00:02:00.400]   And just talk about, assuming we have some neural network,
[00:02:00.400 --> 00:02:01.680]   how do they actually function?
[00:02:01.680 --> 00:02:04.280]   How do they make predictions?
[00:02:04.280 --> 00:02:06.800]   So let me lay down the notation.
[00:02:06.800 --> 00:02:10.240]   So a multilayer feedforward neural network
[00:02:10.240 --> 00:02:14.200]   is a model that takes as input some vector x, which
[00:02:14.200 --> 00:02:16.400]   I'm representing here with a different node
[00:02:16.400 --> 00:02:19.480]   for each of the dimensions in my input vector.
[00:02:19.480 --> 00:02:23.640]   So each dimension is essentially a unit in that neural network.
[00:02:23.640 --> 00:02:27.080]   And then it eventually produces, at its output layer,
[00:02:27.080 --> 00:02:28.680]   an output.
[00:02:28.680 --> 00:02:31.160]   And we'll focus on classification mostly.
[00:02:31.160 --> 00:02:33.280]   So you'd have multiple units here.
[00:02:33.280 --> 00:02:35.160]   And each unit would correspond to one
[00:02:35.160 --> 00:02:37.280]   of the potential classes in which we would
[00:02:37.280 --> 00:02:38.680]   want to classify our input.
[00:02:38.680 --> 00:02:42.960]   So if we're identifying digits in handwritten character
[00:02:42.960 --> 00:02:46.120]   images, and say we're focusing on digits,
[00:02:46.120 --> 00:02:47.120]   you'd have 10 digits.
[00:02:47.120 --> 00:02:50.120]   So you would have a sort of 0 from 0 to 9.
[00:02:50.120 --> 00:02:52.420]   So you'd have 10 output units.
[00:02:52.420 --> 00:02:54.820]   And to produce an output, the neural net
[00:02:54.820 --> 00:02:58.360]   will go through a series of hidden layers.
[00:02:58.360 --> 00:03:01.000]   And those will be essentially the components
[00:03:01.000 --> 00:03:02.380]   that introduce non-linearity that
[00:03:02.380 --> 00:03:06.340]   allows us to capture and perform very sophisticated types
[00:03:06.340 --> 00:03:08.600]   of classification functions.
[00:03:08.600 --> 00:03:11.500]   So if we have L hidden layers, the way
[00:03:11.500 --> 00:03:16.040]   we compute all the layers in our neural net is as follows.
[00:03:16.040 --> 00:03:17.940]   We first start by computing what I'm
[00:03:17.940 --> 00:03:20.040]   going to call a pre-activation.
[00:03:20.040 --> 00:03:22.200]   I'm going to know that A. And I'm
[00:03:22.200 --> 00:03:23.920]   going to index the layers by k.
[00:03:23.920 --> 00:03:28.080]   So A k is just the pre-activation at layer k.
[00:03:28.080 --> 00:03:32.320]   And that is only simply going to be a linear transformation
[00:03:32.320 --> 00:03:34.080]   of the previous layer.
[00:03:34.080 --> 00:03:38.320]   So I'm going to note h k as the activation on the layer.
[00:03:38.320 --> 00:03:41.200]   And by default, I'll assume that layer 0
[00:03:41.200 --> 00:03:43.080]   is going to be the input.
[00:03:43.080 --> 00:03:46.960]   And so using that notation, the pre-activation at layer k
[00:03:46.960 --> 00:03:49.720]   is going to correspond to taking the activation
[00:03:49.720 --> 00:03:52.200]   at the previous layer, k minus 1,
[00:03:52.200 --> 00:03:54.680]   multiplying it by a matrix, Wk.
[00:03:54.680 --> 00:03:57.640]   Those are the parameters of the layer.
[00:03:57.640 --> 00:04:00.520]   Those essentially corresponds to the connections
[00:04:00.520 --> 00:04:03.080]   between the units between adjacent layers.
[00:04:03.080 --> 00:04:05.120]   And I'm going to add a bias vector.
[00:04:05.120 --> 00:04:07.360]   That's another parameter in my layer.
[00:04:07.360 --> 00:04:09.640]   So that gives me the pre-activation.
[00:04:09.640 --> 00:04:12.280]   And then next, I'm going to get a hidden layer activation
[00:04:12.280 --> 00:04:14.680]   by applying an activation function.
[00:04:14.680 --> 00:04:17.760]   This will introduce some non-linearity in the model.
[00:04:17.760 --> 00:04:19.320]   So I'm going to call that function g.
[00:04:19.320 --> 00:04:22.000]   And we'll go over a few choices.
[00:04:22.000 --> 00:04:26.160]   So we have four common choices for the activation function.
[00:04:26.160 --> 00:04:29.120]   And so I do this from layer 1 to layer L.
[00:04:29.120 --> 00:04:31.320]   And when it comes to the output layer,
[00:04:31.320 --> 00:04:33.480]   I'll also compute a pre-activation
[00:04:33.480 --> 00:04:36.000]   by performing a linear transformation.
[00:04:36.000 --> 00:04:38.600]   But then I'll usually apply a different activation function
[00:04:38.600 --> 00:04:41.960]   depending on the problem I'm trying to solve.
[00:04:41.960 --> 00:04:47.000]   So having said that, let's go to some of the choices
[00:04:47.000 --> 00:04:48.260]   for the activation function.
[00:04:48.260 --> 00:04:50.800]   So some of the activation functions you'll see.
[00:04:50.800 --> 00:04:53.680]   One common one is this sigmoid activation function.
[00:04:53.680 --> 00:04:54.880]   It's this function here.
[00:04:54.880 --> 00:04:58.680]   It's just 1 divided by 1 plus the exponential
[00:04:58.680 --> 00:05:01.360]   of minus the pre-activation.
[00:05:01.360 --> 00:05:04.720]   The shape of this function, you can focus on that, is this here.
[00:05:04.720 --> 00:05:06.160]   It takes the pre-activation, which
[00:05:06.160 --> 00:05:08.520]   can vary from minus infinite to plus infinite.
[00:05:08.520 --> 00:05:11.680]   And it squashes this between 0 and 1.
[00:05:11.680 --> 00:05:15.720]   So it's bounded by below and above, below by 0,
[00:05:15.720 --> 00:05:17.480]   and above by 1.
[00:05:17.480 --> 00:05:19.600]   So it's a function that saturates
[00:05:19.600 --> 00:05:24.440]   if you have very large magnitude positive or negative
[00:05:24.440 --> 00:05:26.760]   pre-activations.
[00:05:26.760 --> 00:05:29.880]   Another common choice is the hyperbolic tangent or tanh
[00:05:29.880 --> 00:05:31.680]   activation function.
[00:05:31.680 --> 00:05:32.600]   This picture here.
[00:05:32.600 --> 00:05:33.720]   So it squashes everything.
[00:05:33.720 --> 00:05:35.840]   But instead of being between 0 and 1,
[00:05:35.840 --> 00:05:38.720]   it's between minus 1 and 1.
[00:05:38.720 --> 00:05:42.600]   And one that's become quite popular in neural nets
[00:05:42.600 --> 00:05:46.120]   is what's known as the rectified linear activation function.
[00:05:46.120 --> 00:05:50.400]   Or in papers, you will see the ReLU unit
[00:05:50.400 --> 00:05:54.880]   that refers to the use of this activation function.
[00:05:54.880 --> 00:05:56.560]   So this one is different from the others
[00:05:56.560 --> 00:06:00.340]   in that it's not bounded above, but it is bounded below.
[00:06:00.340 --> 00:06:05.640]   And it will output 0's exactly if the pre-activation
[00:06:05.640 --> 00:06:08.040]   is negative.
[00:06:08.040 --> 00:06:10.120]   So those are the choices of activation functions
[00:06:10.120 --> 00:06:12.060]   for the hidden layers.
[00:06:12.060 --> 00:06:13.960]   And for the output layer, if we're performing
[00:06:13.960 --> 00:06:16.760]   classification, as I said, in our output layer,
[00:06:16.760 --> 00:06:18.640]   we will have as many units as there
[00:06:18.640 --> 00:06:21.440]   are classes in which an input could belong.
[00:06:21.440 --> 00:06:24.720]   And what we'd like is potentially--
[00:06:24.720 --> 00:06:27.600]   and what we often do is interpret each unit's
[00:06:27.600 --> 00:06:30.320]   activation as the probability, according
[00:06:30.320 --> 00:06:33.600]   to the neural network, that the input belongs
[00:06:33.600 --> 00:06:36.480]   to the corresponding class, that its label y
[00:06:36.480 --> 00:06:39.600]   is the corresponding class C. So C
[00:06:39.600 --> 00:06:43.120]   would be like the index of that unit in the output layer.
[00:06:43.120 --> 00:06:44.520]   So we need an activation function
[00:06:44.520 --> 00:06:46.840]   that produces probabilities, produces
[00:06:46.840 --> 00:06:50.080]   a multinomial distribution over all the different classes.
[00:06:50.080 --> 00:06:52.120]   And the activation function we use for that
[00:06:52.120 --> 00:06:55.280]   is known as the softmax activation function.
[00:06:55.280 --> 00:06:57.400]   It is simply as follows.
[00:06:57.400 --> 00:07:00.440]   You take your pre-activations, and you exponentiate them.
[00:07:00.440 --> 00:07:02.800]   So that's going to give us positive numbers.
[00:07:02.800 --> 00:07:06.440]   And then we divide each of the exponentiated pre-activations
[00:07:06.440 --> 00:07:11.200]   by the sum of all the exponentiated pre-activations.
[00:07:11.200 --> 00:07:13.040]   So because I'm normalizing this way,
[00:07:13.040 --> 00:07:16.640]   it means that all my values in my output layer
[00:07:16.640 --> 00:07:17.920]   are going to sum to 1.
[00:07:17.920 --> 00:07:20.080]   And they're positive because I took the exponential.
[00:07:20.080 --> 00:07:23.080]   So I can interpret that as a multinomial distribution
[00:07:23.080 --> 00:07:26.760]   over the choice of all the C different classes.
[00:07:26.760 --> 00:07:29.040]   So that's what I'll use as the activation function
[00:07:29.040 --> 00:07:32.160]   at the output layer.
[00:07:32.160 --> 00:07:35.100]   And now, beyond the math in terms of conceptually
[00:07:35.100 --> 00:07:38.480]   and also in the way we're going to program neural networks,
[00:07:38.480 --> 00:07:41.020]   often what we'll do is that all these different operations,
[00:07:41.020 --> 00:07:43.520]   the linear transformations, the different types of activation
[00:07:43.520 --> 00:07:47.440]   functions, we'll essentially implement all of them
[00:07:47.440 --> 00:07:52.200]   as an object, an object that take arguments.
[00:07:52.200 --> 00:07:53.660]   And the arguments would essentially
[00:07:53.660 --> 00:07:55.400]   be what other things are being combined
[00:07:55.400 --> 00:07:57.600]   to produce the next value.
[00:07:57.600 --> 00:07:59.520]   So for instance, we would have an object
[00:07:59.520 --> 00:08:02.760]   that might correspond to the computation of pre-activation,
[00:08:02.760 --> 00:08:04.920]   which would take as argument what
[00:08:04.920 --> 00:08:08.280]   is the weight matrix and the bias vector for that layer
[00:08:08.280 --> 00:08:10.920]   and take some layer to transform.
[00:08:10.920 --> 00:08:13.560]   And this object would compute its value
[00:08:13.560 --> 00:08:15.840]   by applying the linear activation,
[00:08:15.840 --> 00:08:17.120]   the linear transformation.
[00:08:17.120 --> 00:08:18.620]   And then we might have objects that
[00:08:18.620 --> 00:08:21.880]   correspond to specific activation functions,
[00:08:21.880 --> 00:08:25.360]   so like a sigmoid object or a tanh object or a ReLU object.
[00:08:25.360 --> 00:08:27.200]   And we just combine these objects together,
[00:08:27.200 --> 00:08:30.960]   chain them into what ends up being a graph, which I refer
[00:08:30.960 --> 00:08:34.680]   to as a flow graph, that represents the computation done
[00:08:34.680 --> 00:08:37.400]   when you do a forward pass in your neural network
[00:08:37.400 --> 00:08:39.520]   up until you reach the output layer.
[00:08:39.520 --> 00:08:41.520]   So I mention it now because you'll
[00:08:41.520 --> 00:08:45.840]   see the different softwares that we presented over the weekend
[00:08:45.840 --> 00:08:50.000]   will essentially exploit some of that representation
[00:08:50.000 --> 00:08:51.600]   of the computation in neural nets.
[00:08:51.600 --> 00:08:53.880]   It will also be handy for computing gradients, which
[00:08:53.880 --> 00:08:57.720]   I'll talk about in a few minutes.
[00:08:57.720 --> 00:09:02.120]   And so that's how we perform predictions in neural networks.
[00:09:02.120 --> 00:09:03.560]   So we get an input.
[00:09:03.560 --> 00:09:05.120]   We eventually reach an output layer
[00:09:05.120 --> 00:09:06.960]   that gives us a distribution over classes
[00:09:06.960 --> 00:09:08.680]   if we're performing classification.
[00:09:08.680 --> 00:09:10.360]   If I want to actually classify, I
[00:09:10.360 --> 00:09:13.400]   would just assign the class corresponding
[00:09:13.400 --> 00:09:16.440]   to the unit that has the highest activation, that
[00:09:16.440 --> 00:09:19.160]   would correspond to classifying to the class that
[00:09:19.160 --> 00:09:21.720]   has the highest probability according to the neural net.
[00:09:21.720 --> 00:09:26.200]   But then you might ask the question, OK,
[00:09:26.200 --> 00:09:29.240]   what kind of problems can we solve with neural networks?
[00:09:29.240 --> 00:09:31.440]   Or more technically, what kind of functions
[00:09:31.440 --> 00:09:34.240]   can we represent mapping from some input x
[00:09:34.240 --> 00:09:36.520]   into some arbitrary output?
[00:09:36.520 --> 00:09:39.480]   And so if you go look at my videos,
[00:09:39.480 --> 00:09:41.920]   I try to give more intuition as to why
[00:09:41.920 --> 00:09:43.160]   we have this result here.
[00:09:43.160 --> 00:09:45.960]   But essentially, if we have a single hidden layer
[00:09:45.960 --> 00:09:48.680]   neural network, it's been shown that with a linear output,
[00:09:48.680 --> 00:09:51.040]   we can approximate any continuous function
[00:09:51.040 --> 00:09:54.560]   arbitrarily well as long as we have enough hidden units.
[00:09:54.560 --> 00:09:57.360]   So that is, there's a value for these biases and these weights
[00:09:57.360 --> 00:09:59.240]   such that any continuous function,
[00:09:59.240 --> 00:10:01.720]   I can actually represent it as well as I want.
[00:10:01.720 --> 00:10:04.760]   I just need to add enough hidden units.
[00:10:04.760 --> 00:10:07.560]   So this result applies if you use activation functions,
[00:10:07.560 --> 00:10:11.360]   non-linear activation functions like sigmoid and tanh.
[00:10:11.360 --> 00:10:14.000]   So as I said in my video, if you want a bit more intuition
[00:10:14.000 --> 00:10:18.040]   as to why that would be, you can go check that out.
[00:10:18.040 --> 00:10:19.920]   But that's a really nice result.
[00:10:19.920 --> 00:10:23.880]   It means that by focusing on this family of machine learning
[00:10:23.880 --> 00:10:27.720]   models that are neural networks, I can pretty much potentially
[00:10:27.720 --> 00:10:30.640]   represent any kind of classification function.
[00:10:30.640 --> 00:10:32.560]   However, this result does not tell us
[00:10:32.560 --> 00:10:35.680]   how do we actually find the weights and the bias values
[00:10:35.680 --> 00:10:38.040]   such that I can represent a given function.
[00:10:38.040 --> 00:10:39.560]   It doesn't essentially tell us how
[00:10:39.560 --> 00:10:41.640]   do we train a neural network.
[00:10:41.640 --> 00:10:44.800]   And so that's what we'll discuss next.
[00:10:44.800 --> 00:10:45.960]   So let's talk about that.
[00:10:45.960 --> 00:10:48.280]   How do we actually, from a data set,
[00:10:48.280 --> 00:10:51.560]   train a neural network to perform good classification
[00:10:51.560 --> 00:10:54.280]   for that problem?
[00:10:54.280 --> 00:10:58.100]   So what we'll typically do is use a framework that's
[00:10:58.100 --> 00:10:59.900]   very generic in machine learning,
[00:10:59.900 --> 00:11:03.060]   known as empirical risk minimization or structural risk
[00:11:03.060 --> 00:11:05.660]   minimization if you're using regularization.
[00:11:05.660 --> 00:11:08.940]   So this framework essentially transforms
[00:11:08.940 --> 00:11:12.860]   a problem of learning as a problem of optimizing.
[00:11:12.860 --> 00:11:16.380]   So what we'll do is that we'll first choose a loss function
[00:11:16.380 --> 00:11:19.540]   that I'm noting as L. And the loss function,
[00:11:19.540 --> 00:11:22.100]   it compares the output of my model,
[00:11:22.100 --> 00:11:23.980]   so the output layer of my neural network,
[00:11:23.980 --> 00:11:25.660]   with the actual target.
[00:11:25.660 --> 00:11:28.540]   So I'm indexing with an exponent here with t
[00:11:28.540 --> 00:11:32.660]   to essentially as the index over all my different examples
[00:11:32.660 --> 00:11:34.860]   in my training set.
[00:11:34.860 --> 00:11:36.940]   And so my loss function will tell me,
[00:11:36.940 --> 00:11:42.660]   is this output good or bad given that the label is actually y?
[00:11:42.660 --> 00:11:47.760]   And what I'll do, I'll also define a regularizer.
[00:11:47.760 --> 00:11:49.620]   So theta here is--
[00:11:49.620 --> 00:11:52.860]   you can think of it as just a concatenation of all my biases
[00:11:52.860 --> 00:11:54.460]   and all of my weights in my neural net.
[00:11:54.460 --> 00:11:58.200]   So those are all the parameters of my neural network.
[00:11:58.200 --> 00:12:00.020]   And the regularizer will essentially
[00:12:00.020 --> 00:12:03.220]   penalize certain values of these weights.
[00:12:03.220 --> 00:12:05.980]   So as I'll talk more specifically later on,
[00:12:05.980 --> 00:12:08.500]   for instance, you might want to have your weights not
[00:12:08.500 --> 00:12:09.940]   be too far from 0.
[00:12:09.940 --> 00:12:14.200]   That's a frequent intuition that we implement with regularizer.
[00:12:14.200 --> 00:12:16.660]   And so the optimization problem that we'll
[00:12:16.660 --> 00:12:19.740]   try to solve when learning is to minimize
[00:12:19.740 --> 00:12:23.900]   the average loss of my neural network over my training
[00:12:23.900 --> 00:12:26.100]   examples, so summing over all training examples.
[00:12:26.100 --> 00:12:28.860]   I have capital T examples.
[00:12:28.860 --> 00:12:33.340]   Plus some weight here that's known as the weight decay,
[00:12:33.340 --> 00:12:37.100]   some hyperparameter lambda, times my regularizer.
[00:12:37.100 --> 00:12:39.280]   So in other words, I'm going to try to have
[00:12:39.280 --> 00:12:43.140]   my loss on my training set the smallest possible over all
[00:12:43.140 --> 00:12:45.260]   the training example and also try
[00:12:45.260 --> 00:12:48.660]   to satisfy my regularizer as much as possible.
[00:12:48.660 --> 00:12:51.420]   And so now we have this optimization problem.
[00:12:51.420 --> 00:12:53.140]   And learning will just correspond
[00:12:53.140 --> 00:12:55.380]   to trying to solve this problem.
[00:12:55.380 --> 00:13:00.940]   So finding this arg min here for over my weights and my biases.
[00:13:00.940 --> 00:13:02.460]   And if I want to do this, I can just
[00:13:02.460 --> 00:13:05.100]   invoke some optimization procedure
[00:13:05.100 --> 00:13:08.860]   from the optimization community.
[00:13:08.860 --> 00:13:10.340]   And the one algorithm that you'll
[00:13:10.340 --> 00:13:14.420]   see constantly in deep learning is stochastic gradient descent.
[00:13:14.420 --> 00:13:16.220]   This is the optimization algorithm
[00:13:16.220 --> 00:13:19.780]   that we'll often use for training neural networks.
[00:13:19.780 --> 00:13:23.620]   So SGD, stochastic gradient descent, functions as follows.
[00:13:23.620 --> 00:13:26.580]   You first initialize all of your parameters.
[00:13:26.580 --> 00:13:29.740]   That is finding initial values for all my weight matrices
[00:13:29.740 --> 00:13:32.060]   and all of my biases.
[00:13:32.060 --> 00:13:34.140]   And then for a certain number of epochs--
[00:13:34.140 --> 00:13:37.740]   so an epoch will be a full pass over all my examples.
[00:13:37.740 --> 00:13:40.100]   That's what I'll call an epoch.
[00:13:40.100 --> 00:13:44.620]   So for a certain number of full iterations over my training
[00:13:44.620 --> 00:13:47.900]   set, I'll draw each training example.
[00:13:47.900 --> 00:13:51.620]   So I pair x, input x, target y.
[00:13:51.620 --> 00:13:55.900]   And then I'll compute what is the gradient of my loss
[00:13:55.900 --> 00:13:58.580]   with respect to my parameters.
[00:13:58.580 --> 00:14:01.040]   All of my parameters, all my weights, and all my biases.
[00:14:01.040 --> 00:14:03.180]   This is what this notation here--
[00:14:03.180 --> 00:14:06.740]   so nabla for the gradient of the loss function.
[00:14:06.740 --> 00:14:10.300]   And here I'm indexing with respect to which parameter.
[00:14:10.300 --> 00:14:12.060]   I want the gradient.
[00:14:12.060 --> 00:14:14.700]   So I'm going to compute what is the gradient of my loss
[00:14:14.700 --> 00:14:17.240]   function with respect to my parameters.
[00:14:17.240 --> 00:14:20.220]   And plus lambda times the gradient of my regularizer
[00:14:20.220 --> 00:14:21.160]   as well.
[00:14:21.160 --> 00:14:23.500]   And then I'm going to get a direction in which I should
[00:14:23.500 --> 00:14:25.220]   move my parameters.
[00:14:25.220 --> 00:14:28.620]   Since the gradient tells me how to increase the loss,
[00:14:28.620 --> 00:14:31.060]   I want to go in the opposite direction and decrease it.
[00:14:31.060 --> 00:14:32.820]   So my direction will be the opposite.
[00:14:32.820 --> 00:14:35.540]   So that's why I have a minus here.
[00:14:35.540 --> 00:14:38.140]   And so this delta is going to be the direction in which I'll
[00:14:38.140 --> 00:14:40.780]   move my parameters by taking a step.
[00:14:40.780 --> 00:14:43.940]   And the step is just a step size alpha,
[00:14:43.940 --> 00:14:46.500]   which is often referred to as a learning rate,
[00:14:46.500 --> 00:14:49.300]   times my direction, which I just add
[00:14:49.300 --> 00:14:52.300]   to my current values of my parameters, my biases
[00:14:52.300 --> 00:14:53.360]   and my weights.
[00:14:53.360 --> 00:14:56.120]   And that's going to give me my new value for all
[00:14:56.120 --> 00:14:57.180]   of my parameters.
[00:14:57.180 --> 00:15:01.260]   And I iterate like that, going over all pairs x, y's,
[00:15:01.260 --> 00:15:03.420]   computing my gradient, taking a step
[00:15:03.420 --> 00:15:05.280]   side in the opposite direction, and then
[00:15:05.280 --> 00:15:07.620]   doing that several times.
[00:15:07.620 --> 00:15:10.620]   So that's how stochastic gradient descent works.
[00:15:10.620 --> 00:15:12.540]   And that's essentially the learning procedure.
[00:15:12.540 --> 00:15:16.100]   It's represented by this procedure.
[00:15:16.100 --> 00:15:17.900]   So in this algorithm, there are a few things
[00:15:17.900 --> 00:15:20.860]   we need to specify to be able to implement it and execute it.
[00:15:20.860 --> 00:15:23.620]   We need a loss function, a choice for the loss function.
[00:15:23.620 --> 00:15:26.860]   We need a procedure that's efficient for computing
[00:15:26.860 --> 00:15:30.620]   the gradient of the loss with respect to my parameters.
[00:15:30.620 --> 00:15:33.300]   We need to choose a regularizer if we want one.
[00:15:33.300 --> 00:15:35.940]   And we need a way of initializing my parameters.
[00:15:35.940 --> 00:15:37.960]   So next, what I'll do is I'll go through each
[00:15:37.960 --> 00:15:39.780]   of these four different things we
[00:15:39.780 --> 00:15:41.700]   need to choose before actually being
[00:15:41.700 --> 00:15:45.660]   able to execute stochastic gradient descent.
[00:15:45.660 --> 00:15:48.060]   So first, the loss function.
[00:15:48.060 --> 00:15:50.980]   So as I said, we will interpret the output layer
[00:15:50.980 --> 00:15:53.900]   as assigning probabilities to each potential class in which
[00:15:53.900 --> 00:15:57.460]   I can classify my input x.
[00:15:57.460 --> 00:15:59.780]   Well, in this case, something that would be natural
[00:15:59.780 --> 00:16:02.120]   is to try to maximize the probability
[00:16:02.120 --> 00:16:05.500]   of the correct class, the actual class in which my example
[00:16:05.500 --> 00:16:06.340]   x t belongs to.
[00:16:06.340 --> 00:16:09.340]   I'd like to increase the value of the probability assigned
[00:16:09.340 --> 00:16:09.900]   by--
[00:16:09.900 --> 00:16:12.820]   computed by my neural network.
[00:16:12.820 --> 00:16:16.380]   And so because we set up the problem in which we
[00:16:16.380 --> 00:16:18.700]   have a loss that we minimize, instead
[00:16:18.700 --> 00:16:20.980]   of maximizing the probability, what we'll actually do
[00:16:20.980 --> 00:16:25.380]   is minimize the negative and the actual log probability,
[00:16:25.380 --> 00:16:28.300]   so the log likelihood of assigning x
[00:16:28.300 --> 00:16:30.340]   to the correct class y.
[00:16:30.340 --> 00:16:32.100]   So this is represented here.
[00:16:32.100 --> 00:16:35.420]   So given my output layer and the true label y,
[00:16:35.420 --> 00:16:40.820]   my loss will be minus the log of the probability of y
[00:16:40.820 --> 00:16:42.140]   according to my neural net.
[00:16:42.140 --> 00:16:44.980]   And that would be, well, take my output layer
[00:16:44.980 --> 00:16:48.500]   and look at the unit, so index the unit corresponding
[00:16:48.500 --> 00:16:50.020]   to the correct class.
[00:16:50.020 --> 00:16:53.500]   So that's why I'm indexing by y here.
[00:16:53.500 --> 00:16:55.740]   We take the log because numerically it
[00:16:55.740 --> 00:16:56.940]   turns out to be more stable.
[00:16:56.940 --> 00:16:59.180]   We get nicer-looking gradients.
[00:16:59.180 --> 00:17:01.340]   And sometimes in certain softwares,
[00:17:01.340 --> 00:17:03.460]   you'll see instead of talking about the negative log
[00:17:03.460 --> 00:17:05.020]   likelihood or log probability, you'll
[00:17:05.020 --> 00:17:07.980]   see it referred as the cross-entropy.
[00:17:07.980 --> 00:17:11.820]   And that's because you can think of this
[00:17:11.820 --> 00:17:15.620]   as performing a sum over all possible classes.
[00:17:15.620 --> 00:17:17.540]   And then for each class, checking, well,
[00:17:17.540 --> 00:17:20.820]   is this potential class the target class?
[00:17:20.820 --> 00:17:24.660]   So I have an indicator function that is 1 if y is equal to c,
[00:17:24.660 --> 00:17:28.100]   so if my iterator class c is actually
[00:17:28.100 --> 00:17:29.700]   equal to the real class.
[00:17:29.700 --> 00:17:33.140]   I'm going to multiply that by the log of the probability
[00:17:33.140 --> 00:17:35.660]   actually assigned to that class c.
[00:17:35.660 --> 00:17:39.380]   And this function here, so this expression here,
[00:17:39.380 --> 00:17:42.900]   is like a cross-entropy between the empirical distribution,
[00:17:42.900 --> 00:17:46.100]   which assigns zero probability to all the other classes,
[00:17:46.100 --> 00:17:48.500]   but a probability of 1 to the correct class,
[00:17:48.500 --> 00:17:50.900]   and the actual distribution over classes
[00:17:50.900 --> 00:17:54.660]   that my neural net is computing, which is f of x.
[00:17:54.660 --> 00:17:56.060]   That's just a technical detail.
[00:17:56.060 --> 00:17:57.540]   You can just think about this.
[00:17:57.540 --> 00:17:59.700]   Here, I only mention it because in certain libraries,
[00:17:59.700 --> 00:18:03.740]   it's actually mentioned as the cross-entropy loss.
[00:18:03.740 --> 00:18:06.060]   So that's for the loss.
[00:18:06.060 --> 00:18:08.260]   Then we need also a procedure for computing
[00:18:08.260 --> 00:18:10.460]   what is the gradient of my loss with respect
[00:18:10.460 --> 00:18:12.620]   to all of my parameters in my neural net,
[00:18:12.620 --> 00:18:15.820]   so the biases and the weights.
[00:18:15.820 --> 00:18:17.300]   You can go look at my videos if you
[00:18:17.300 --> 00:18:20.220]   want the actual derivation of all the details for all
[00:18:20.220 --> 00:18:21.780]   of these different expressions.
[00:18:21.780 --> 00:18:23.740]   I don't have time for that, so all I'll do--
[00:18:23.740 --> 00:18:26.300]   and presumably, a lot of you actually
[00:18:26.300 --> 00:18:28.620]   have seen these derivations.
[00:18:28.620 --> 00:18:30.700]   If you haven't, just go check out the videos.
[00:18:30.700 --> 00:18:33.980]   In any case, I'm going to go through what the algorithm is.
[00:18:33.980 --> 00:18:35.900]   I'm going to highlight some of the key points
[00:18:35.900 --> 00:18:39.220]   that will come up later in understanding how actually
[00:18:39.220 --> 00:18:41.380]   backpropagation functions.
[00:18:41.380 --> 00:18:44.820]   So the basic idea is that we'll compute gradients
[00:18:44.820 --> 00:18:46.420]   by exploiting the chain rule.
[00:18:46.420 --> 00:18:50.220]   And we'll go from the top layer all the way to the bottom,
[00:18:50.220 --> 00:18:53.820]   computing gradients for layers that are closer and closer
[00:18:53.820 --> 00:18:56.540]   to the input as we go, and exploiting the chain rule
[00:18:56.540 --> 00:18:59.820]   to exploit or reuse previous computations we've
[00:18:59.820 --> 00:19:03.460]   made at upper layers to compute the gradients at the layers
[00:19:03.460 --> 00:19:04.980]   below.
[00:19:04.980 --> 00:19:08.140]   So we usually start by computing what is the gradient
[00:19:08.140 --> 00:19:09.300]   at the output layer.
[00:19:09.300 --> 00:19:12.460]   So what's the gradient of my loss with respect
[00:19:12.460 --> 00:19:13.820]   to my output layer?
[00:19:13.820 --> 00:19:15.380]   And actually, it's more convenient
[00:19:15.380 --> 00:19:18.340]   to compute the loss with respect to the pre-activation.
[00:19:18.340 --> 00:19:21.140]   It's actually a very simple expression.
[00:19:21.140 --> 00:19:24.180]   So that's why I have the gradient of this vector,
[00:19:24.180 --> 00:19:25.140]   a l plus 1.
[00:19:25.140 --> 00:19:28.980]   That's the pre-activation at the very last layer of the loss
[00:19:28.980 --> 00:19:32.460]   function, which is minus the log f of x, y.
[00:19:32.460 --> 00:19:35.060]   And it turns out this gradient is super simple.
[00:19:35.060 --> 00:19:37.100]   It's minus E of y.
[00:19:37.100 --> 00:19:40.180]   So that's the one-hot vector for class y.
[00:19:40.180 --> 00:19:43.820]   So what this means is E of y is just a vector filled
[00:19:43.820 --> 00:19:47.860]   with a bunch of 0's and then the 1 at the correct class.
[00:19:47.860 --> 00:19:51.100]   So if y was the fourth class, then in this case,
[00:19:51.100 --> 00:19:53.260]   it would be this vector, where I have a 1 at the fourth
[00:19:53.260 --> 00:19:54.740]   dimension.
[00:19:54.740 --> 00:19:56.580]   So E of y is just a vector.
[00:19:56.580 --> 00:19:58.700]   We call it the one-hot vector full of 0's.
[00:19:58.700 --> 00:20:01.620]   And the single 1 at the position corresponding
[00:20:01.620 --> 00:20:03.540]   to the correct class.
[00:20:03.540 --> 00:20:05.140]   So what this part of the gradient
[00:20:05.140 --> 00:20:07.620]   is essentially saying is that I'm going to increase--
[00:20:07.620 --> 00:20:09.460]   I want to increase the probability
[00:20:09.460 --> 00:20:10.580]   of the correct class.
[00:20:10.580 --> 00:20:12.520]   I want to increase the pre-activation, which
[00:20:12.520 --> 00:20:15.460]   will increase the probability of the correct class.
[00:20:15.460 --> 00:20:18.660]   And I'm going to subtract what is the current probabilities
[00:20:18.660 --> 00:20:21.860]   assigned by my neural net to all of the classes.
[00:20:21.860 --> 00:20:23.580]   So f of x, that's my output layer.
[00:20:23.580 --> 00:20:26.420]   And that's the current beliefs of the neural net
[00:20:26.420 --> 00:20:30.100]   as to in which class, what's the probability of assigning
[00:20:30.100 --> 00:20:31.740]   the input to each class.
[00:20:31.740 --> 00:20:33.740]   So what this is doing is essentially
[00:20:33.740 --> 00:20:36.260]   trying to decrease the probability of everything
[00:20:36.260 --> 00:20:39.900]   and specifically decrease it as much as the neural net currently
[00:20:39.900 --> 00:20:42.900]   believes that the input belongs to it.
[00:20:42.900 --> 00:20:45.420]   And so if you think about the subtraction of these two
[00:20:45.420 --> 00:20:48.260]   things, well, for the class that's the correct class,
[00:20:48.260 --> 00:20:51.140]   I'm going to have 1 minus some number between 0 and 1,
[00:20:51.140 --> 00:20:52.420]   because it's a probability.
[00:20:52.420 --> 00:20:53.700]   So that's going to be positive.
[00:20:53.700 --> 00:20:55.380]   So I'm going to increase the probability
[00:20:55.380 --> 00:20:56.500]   of the correct class.
[00:20:56.500 --> 00:20:58.000]   And for everything else, it's going
[00:20:58.000 --> 00:20:59.820]   to be 0 minus a positive number.
[00:20:59.820 --> 00:21:01.180]   So it's going to be negative.
[00:21:01.180 --> 00:21:03.180]   So I'm actually going to decrease the probability
[00:21:03.180 --> 00:21:04.180]   of everything else.
[00:21:04.180 --> 00:21:05.620]   So intuitively, it makes sense.
[00:21:05.620 --> 00:21:08.740]   This gradient has the right behavior.
[00:21:08.740 --> 00:21:11.580]   And I'm going to take that pre-activation gradient.
[00:21:11.580 --> 00:21:15.340]   I'm going to propagate it from the top to the bottom
[00:21:15.340 --> 00:21:19.740]   and essentially iterating from the last layer, which
[00:21:19.740 --> 00:21:22.380]   is the output layer, L plus 1, all the way down
[00:21:22.380 --> 00:21:23.980]   to the first layer.
[00:21:23.980 --> 00:21:26.780]   And as I'm going down, I'm going to compute the gradients
[00:21:26.780 --> 00:21:28.500]   with respect to my parameters and then
[00:21:28.500 --> 00:21:31.420]   compute what's the gradient for the pre-activation
[00:21:31.420 --> 00:21:34.580]   at the layer below and then iterate like that.
[00:21:34.580 --> 00:21:38.180]   So at each iteration of that loop,
[00:21:38.180 --> 00:21:42.900]   I take what is the current gradient of the loss function
[00:21:42.900 --> 00:21:44.420]   with respect to the pre-activation
[00:21:44.420 --> 00:21:46.220]   at the current layer.
[00:21:46.220 --> 00:21:49.380]   And I can compute the gradient of the loss function
[00:21:49.380 --> 00:21:51.500]   with respect to my weight matrix.
[00:21:51.500 --> 00:21:54.460]   So not doing the derivation here,
[00:21:54.460 --> 00:21:58.180]   it's actually simply this vector.
[00:21:58.180 --> 00:22:00.580]   So in my notation, I assume that all the vectors
[00:22:00.580 --> 00:22:02.220]   are column vectors.
[00:22:02.220 --> 00:22:05.180]   So this pre-activation gradient vector,
[00:22:05.180 --> 00:22:09.020]   and I multiply it by the transpose of the activations,
[00:22:09.020 --> 00:22:14.540]   so the value of the layer right below, the layer k minus 1.
[00:22:14.540 --> 00:22:16.100]   So because I take the transpose, that's
[00:22:16.100 --> 00:22:17.380]   a multiplication like this.
[00:22:17.380 --> 00:22:19.180]   And you can see if I do the outer product,
[00:22:19.180 --> 00:22:20.800]   essentially, between these two vectors,
[00:22:20.800 --> 00:22:24.260]   I'm going to get a matrix of the same size as my weight matrix.
[00:22:24.260 --> 00:22:25.940]   So it all checks out.
[00:22:25.940 --> 00:22:27.500]   That makes sense.
[00:22:27.500 --> 00:22:29.680]   Turns out that the gradient of the loss with respect
[00:22:29.680 --> 00:22:31.940]   to the bias is exactly the gradient
[00:22:31.940 --> 00:22:34.700]   of the loss with respect to the pre-activation.
[00:22:34.700 --> 00:22:36.220]   So that's very simple.
[00:22:36.220 --> 00:22:38.700]   So that gives me now my gradients for my parameters.
[00:22:38.700 --> 00:22:40.420]   Now I need to compute, OK, what is
[00:22:40.420 --> 00:22:42.660]   going to be the gradient of the pre-activations
[00:22:42.660 --> 00:22:44.700]   at the layer below?
[00:22:44.700 --> 00:22:48.980]   Well, first, I'm going to get the gradient of the loss
[00:22:48.980 --> 00:22:54.220]   function with respect to the activation at the layer below.
[00:22:54.220 --> 00:22:57.980]   Well, that's just taking my pre-activation gradient vector
[00:22:57.980 --> 00:22:59.940]   and multiplying it by--
[00:22:59.940 --> 00:23:01.780]   for some reason, it doesn't show here--
[00:23:01.780 --> 00:23:04.660]   and multiply it by the transpose of my weight matrix.
[00:23:04.660 --> 00:23:07.580]   Super simple operation, just a linear transformation
[00:23:07.580 --> 00:23:10.580]   of my gradients at layer k, linear and transformed
[00:23:10.580 --> 00:23:13.780]   to get my gradients of the activation at the layer k
[00:23:13.780 --> 00:23:15.180]   minus 1.
[00:23:15.180 --> 00:23:17.900]   And then to get the gradients of the pre-activation,
[00:23:17.900 --> 00:23:21.020]   so before the activation function,
[00:23:21.020 --> 00:23:22.940]   I'm going to take this gradient here,
[00:23:22.940 --> 00:23:25.540]   which is the gradient of the activation function
[00:23:25.540 --> 00:23:27.220]   at the layer k minus 1.
[00:23:27.220 --> 00:23:29.900]   And then I apply the gradient corresponding
[00:23:29.900 --> 00:23:33.700]   to the partial derivative of my nonlinear activation function.
[00:23:33.700 --> 00:23:37.060]   So this here, this refers to an element-wise product.
[00:23:37.060 --> 00:23:39.860]   So I'm taking these two vectors, this vector here
[00:23:39.860 --> 00:23:40.740]   and this vector here.
[00:23:40.740 --> 00:23:43.740]   I'm going to do an element-wise product between the two.
[00:23:43.740 --> 00:23:47.020]   And this vector here is just the partial derivative
[00:23:47.020 --> 00:23:49.620]   of the activation function for each unit
[00:23:49.620 --> 00:23:52.700]   individually that I've put together into a vector.
[00:23:52.700 --> 00:23:55.180]   This is what this corresponds to.
[00:23:55.180 --> 00:23:57.060]   Now, the key things to notice is first
[00:23:57.060 --> 00:23:59.580]   that this pass, computing all the gradients
[00:23:59.580 --> 00:24:02.820]   and doing all these iterations, is actually fairly cheap.
[00:24:02.820 --> 00:24:05.540]   Complexity is essentially the same as the one
[00:24:05.540 --> 00:24:07.700]   that's doing a forward pass.
[00:24:07.700 --> 00:24:11.180]   So all I'm doing are linear transformations
[00:24:11.180 --> 00:24:14.220]   multiplying by matrices, in this case, the transpose of my weight
[00:24:14.220 --> 00:24:15.060]   matrix.
[00:24:15.060 --> 00:24:17.620]   And then I'm also doing this nonlinear operation
[00:24:17.620 --> 00:24:20.120]   where I'm multiplying by the gradient of the activation
[00:24:20.120 --> 00:24:21.020]   function.
[00:24:21.020 --> 00:24:22.860]   So that's the first thing to notice.
[00:24:22.860 --> 00:24:24.220]   And the second thing to notice is
[00:24:24.220 --> 00:24:27.420]   that here I'm doing this element-wise product.
[00:24:27.420 --> 00:24:30.060]   So if any of these terms here for a unit
[00:24:30.060 --> 00:24:33.900]   is very close to 0, then the pre-activation gradient
[00:24:33.900 --> 00:24:36.460]   is going to be 0 for the next layer.
[00:24:36.460 --> 00:24:39.740]   And I highlight this point because essentially whenever--
[00:24:39.740 --> 00:24:41.500]   that's something to think about a lot when
[00:24:41.500 --> 00:24:42.860]   you're training neural nets.
[00:24:42.860 --> 00:24:46.060]   Whenever this gradient here, these partial derivatives,
[00:24:46.060 --> 00:24:48.660]   come close to 0, then it means the gradient will not
[00:24:48.660 --> 00:24:50.420]   propagate well to the next layer, which
[00:24:50.420 --> 00:24:53.120]   means that you're not going to get a good gradient to update
[00:24:53.120 --> 00:24:54.980]   your parameters.
[00:24:54.980 --> 00:24:56.380]   Now, when does that happen?
[00:24:56.380 --> 00:24:59.380]   When will you see these terms here being close to 0?
[00:24:59.380 --> 00:25:01.580]   Well, that's going to be when the partial derivatives
[00:25:01.580 --> 00:25:03.700]   of these nonlinear activation functions
[00:25:03.700 --> 00:25:05.780]   are close to 0 or 0.
[00:25:05.780 --> 00:25:08.160]   So we can look at the partial derivatives, say,
[00:25:08.160 --> 00:25:10.140]   of the sigmoid function.
[00:25:10.140 --> 00:25:12.460]   It turns out it's super easy to compute.
[00:25:12.460 --> 00:25:15.340]   It's just the sigmoid itself times 1
[00:25:15.340 --> 00:25:18.100]   minus the sigmoid itself.
[00:25:18.100 --> 00:25:20.160]   So that means that whenever the activation
[00:25:20.160 --> 00:25:23.500]   of the unit for a sigmoid unit is close to 1 or close to 0,
[00:25:23.500 --> 00:25:27.540]   I essentially get a partial derivative that's close to 0.
[00:25:27.540 --> 00:25:28.780]   You can kind of see it here.
[00:25:28.780 --> 00:25:30.380]   The slope here is essentially flat,
[00:25:30.380 --> 00:25:31.700]   and the slope here is flat.
[00:25:31.700 --> 00:25:35.260]   That's the value of the partial derivative.
[00:25:35.260 --> 00:25:37.940]   So in other words, if my pre-activations
[00:25:37.940 --> 00:25:39.860]   are very negative or very positive,
[00:25:39.860 --> 00:25:42.940]   or if my unit is very saturated, then gradients
[00:25:42.940 --> 00:25:46.500]   will have a hard time propagating to the next layer.
[00:25:46.500 --> 00:25:49.020]   That's the key insight here.
[00:25:49.020 --> 00:25:51.820]   Same thing for the tanh function.
[00:25:51.820 --> 00:25:53.500]   So it turns out the partial derivative
[00:25:53.500 --> 00:25:54.940]   is also easy to compute.
[00:25:54.940 --> 00:25:57.580]   You just take the tanh value, square it,
[00:25:57.580 --> 00:25:59.980]   and you're going to subtract it to 1.
[00:25:59.980 --> 00:26:04.420]   And indeed, if it's close to minus 1 or close to 1,
[00:26:04.420 --> 00:26:07.180]   you can see that the slope is flat.
[00:26:07.180 --> 00:26:09.420]   So again, if the unit is saturating,
[00:26:09.420 --> 00:26:11.900]   gradients will have a hard time propagating
[00:26:11.900 --> 00:26:14.140]   to the next layers.
[00:26:14.140 --> 00:26:18.140]   And for the ReLU, the rectified linear activation function,
[00:26:18.140 --> 00:26:21.180]   the gradient is even simpler.
[00:26:21.180 --> 00:26:23.580]   You just check whether the pre-activation is greater than
[00:26:23.580 --> 00:26:24.080]   0.
[00:26:24.080 --> 00:26:26.300]   If it is, the partial derivative is 1.
[00:26:26.300 --> 00:26:27.780]   If it's not, it's 0.
[00:26:27.780 --> 00:26:30.020]   So actually, you're going to multiply by 1 or 0.
[00:26:30.020 --> 00:26:31.760]   You essentially get a binary mask
[00:26:31.760 --> 00:26:35.220]   when you're performing the propagation through the ReLU.
[00:26:35.220 --> 00:26:36.180]   And you can see it.
[00:26:36.180 --> 00:26:38.100]   The slope here is flat, and otherwise, you
[00:26:38.100 --> 00:26:40.020]   have a linear function.
[00:26:40.020 --> 00:26:43.940]   So actually, here, the shrinking of the gradient towards 0
[00:26:43.940 --> 00:26:44.860]   is even harder.
[00:26:44.860 --> 00:26:47.780]   It's exactly multiplying by 0 if you have
[00:26:47.780 --> 00:26:52.140]   a unit that's saturating below.
[00:26:52.140 --> 00:26:56.820]   And beyond all the math, in terms of actually using those
[00:26:56.820 --> 00:26:58.800]   in practice, during the weekend, you'll
[00:26:58.800 --> 00:27:01.780]   see three different libraries that essentially allows you
[00:27:01.780 --> 00:27:03.220]   to compute these gradients for you.
[00:27:03.220 --> 00:27:06.000]   You actually usually don't write down backprop.
[00:27:06.000 --> 00:27:08.020]   You just use all of these modules
[00:27:08.020 --> 00:27:09.180]   that you've implemented.
[00:27:09.180 --> 00:27:13.380]   And it turns out there's a way of automatically differentiating
[00:27:13.380 --> 00:27:16.220]   your loss function and getting gradients for free
[00:27:16.220 --> 00:27:19.460]   in terms of effort, in terms of programming effort,
[00:27:19.460 --> 00:27:21.500]   with respect to your parameters.
[00:27:21.500 --> 00:27:23.940]   So conceptually, the way you do this--
[00:27:23.940 --> 00:27:26.460]   and you'll see essentially three different libraries doing it
[00:27:26.460 --> 00:27:28.580]   in slightly different ways.
[00:27:28.580 --> 00:27:31.340]   What you do is you augment your flow graph
[00:27:31.340 --> 00:27:34.420]   by adding, at the very end, the computation of your loss
[00:27:34.420 --> 00:27:35.600]   function.
[00:27:35.600 --> 00:27:38.020]   And then each of these boxes, which are conceptually
[00:27:38.020 --> 00:27:40.540]   objects that are taking arguments and computing
[00:27:40.540 --> 00:27:45.420]   a value, you're going to augment them to also have a method
[00:27:45.420 --> 00:27:47.600]   that's a backprop or a bprop method.
[00:27:47.600 --> 00:27:50.660]   You'll often see, actually, this expression being used, bprop.
[00:27:50.660 --> 00:27:52.980]   And what this method should do is
[00:27:52.980 --> 00:27:54.500]   that it should take as input, what
[00:27:54.500 --> 00:27:57.300]   is the gradient of the loss with respect to myself?
[00:27:57.300 --> 00:28:00.340]   And then it should propagate to its arguments,
[00:28:00.340 --> 00:28:02.780]   so the things that its parents in the flow graph,
[00:28:02.780 --> 00:28:04.780]   the things it takes to compute its own value,
[00:28:04.780 --> 00:28:07.100]   it's going to propagate them using the chain rule, what
[00:28:07.100 --> 00:28:10.780]   is their gradients with respect to the loss?
[00:28:10.780 --> 00:28:14.220]   So what this means is that you would start the process
[00:28:14.220 --> 00:28:16.860]   by initializing, well, the gradient of the loss
[00:28:16.860 --> 00:28:19.360]   with respect to itself is 1.
[00:28:19.360 --> 00:28:22.260]   And then you pass the bprop method here 1.
[00:28:22.260 --> 00:28:26.420]   And then it's going to propagate to its argument, what
[00:28:26.420 --> 00:28:28.240]   is, by using the chain rule, what
[00:28:28.240 --> 00:28:32.040]   is the gradient of the loss with respect to f of x?
[00:28:32.040 --> 00:28:34.800]   And then you're going to call bprop on this object here.
[00:28:34.800 --> 00:28:36.220]   And it's going to compute, well, I
[00:28:36.220 --> 00:28:38.460]   have the gradient of the loss with respect to myself,
[00:28:38.460 --> 00:28:39.300]   f of x.
[00:28:39.300 --> 00:28:42.700]   From this, I can compute what's the gradient of my argument,
[00:28:42.700 --> 00:28:45.140]   which is the pre-activation at layer 2,
[00:28:45.140 --> 00:28:46.580]   with respect to the loss.
[00:28:46.580 --> 00:28:48.380]   So I'm going to reuse the computation I just
[00:28:48.380 --> 00:28:50.860]   got and update it using my--
[00:28:50.860 --> 00:28:53.020]   what is essentially the Jacobian.
[00:28:53.020 --> 00:28:55.140]   And then I'm going to take the pre-activation here,
[00:28:55.140 --> 00:28:57.340]   which now knows what is the gradient of the loss with
[00:28:57.340 --> 00:28:59.180]   respect to itself, the pre-activation.
[00:28:59.180 --> 00:29:02.020]   It's going to propagate to the weights and the biases
[00:29:02.020 --> 00:29:04.200]   and the layer below, updating them with--
[00:29:04.200 --> 00:29:06.280]   informing them of what is the gradient of the loss
[00:29:06.280 --> 00:29:07.420]   with respect to themselves.
[00:29:07.420 --> 00:29:09.080]   And you continue like this, essentially
[00:29:09.080 --> 00:29:12.880]   going through the flow graph, but in the opposite direction.
[00:29:12.880 --> 00:29:15.400]   So the library torch, the basic library torch,
[00:29:15.400 --> 00:29:18.440]   essentially functions like this quite explicitly.
[00:29:18.440 --> 00:29:21.000]   You construct-- you chain these elements together.
[00:29:21.000 --> 00:29:23.000]   And then when you're performing backpropagation,
[00:29:23.000 --> 00:29:24.340]   you're going in the reverse order
[00:29:24.340 --> 00:29:25.880]   of these chained elements.
[00:29:25.880 --> 00:29:29.280]   And then you have libraries like Torchautograd and Theano
[00:29:29.280 --> 00:29:31.240]   and TensorFlow, which you'll learn about,
[00:29:31.240 --> 00:29:34.340]   which are doing things slightly more sophisticated there.
[00:29:34.340 --> 00:29:38.540]   And you'll learn about that later on.
[00:29:38.540 --> 00:29:40.940]   OK, so that's a discussion of how you actually
[00:29:40.940 --> 00:29:44.620]   compute gradients of the loss with respect to the parameters.
[00:29:44.620 --> 00:29:47.540]   So that's another component we need in stochastic gradient
[00:29:47.540 --> 00:29:48.640]   descent.
[00:29:48.640 --> 00:29:50.100]   We can choose a regularizer.
[00:29:50.100 --> 00:29:53.500]   One that's often used is the L2 regularization.
[00:29:53.500 --> 00:29:57.100]   So that's just the sum of the squared of all the weights.
[00:29:57.100 --> 00:30:01.280]   And the gradient of that is just twice times the weight.
[00:30:01.280 --> 00:30:03.400]   So it's a super simple gradient to compute.
[00:30:03.400 --> 00:30:06.840]   We usually don't regularize the biases.
[00:30:06.840 --> 00:30:10.960]   There's no particularly important reason for that.
[00:30:10.960 --> 00:30:15.200]   There are much fewer biases, so it seems less important.
[00:30:15.200 --> 00:30:17.020]   And often, this L2 regularization
[00:30:17.020 --> 00:30:18.760]   is often referred to as weight decay.
[00:30:18.760 --> 00:30:20.320]   So if you hear about weight decay,
[00:30:20.320 --> 00:30:24.400]   that often refers to L2 regularization.
[00:30:24.400 --> 00:30:28.660]   And then finally, and this is also a very important point,
[00:30:28.660 --> 00:30:31.080]   you have to initialize the parameters before you actually
[00:30:31.080 --> 00:30:32.120]   start doing backprop.
[00:30:32.120 --> 00:30:34.160]   And there are a few tricky cases you
[00:30:34.160 --> 00:30:37.640]   need to make sure that you don't fall into.
[00:30:37.640 --> 00:30:40.600]   So the biases, often we initialize them to 0.
[00:30:40.600 --> 00:30:42.880]   There are certain exceptions, but for the most part,
[00:30:42.880 --> 00:30:44.720]   we initialize them to 0.
[00:30:44.720 --> 00:30:47.600]   But for the weights, there are a few things we can't do.
[00:30:47.600 --> 00:30:50.200]   So we can't initialize the weights to 0,
[00:30:50.200 --> 00:30:53.960]   and especially if you have tanh activations.
[00:30:53.960 --> 00:30:56.160]   The reason-- and I won't explain it here,
[00:30:56.160 --> 00:30:59.240]   but it's not a bad exercise to try to figure out why--
[00:30:59.240 --> 00:31:02.100]   is that essentially, when you do your first pass,
[00:31:02.100 --> 00:31:04.520]   you're going to get gradients for all your parameters that
[00:31:04.520 --> 00:31:05.920]   are going to be 0.
[00:31:05.920 --> 00:31:08.960]   So you're going to be stuck at this 0 initialization.
[00:31:08.960 --> 00:31:11.280]   So we can't do that.
[00:31:11.280 --> 00:31:13.160]   We also can't initialize all the weights
[00:31:13.160 --> 00:31:16.440]   to exactly the same value.
[00:31:16.440 --> 00:31:18.720]   Again, you think about it a little bit.
[00:31:18.720 --> 00:31:20.600]   What's going to happen is essentially
[00:31:20.600 --> 00:31:24.720]   that all the weights coming into a unit within the layer
[00:31:24.720 --> 00:31:27.640]   are going to have exactly the same gradients, which
[00:31:27.640 --> 00:31:30.000]   means they're going to be updated exactly the same way,
[00:31:30.000 --> 00:31:32.280]   which means they're going to stay constant the same--
[00:31:32.280 --> 00:31:34.360]   not constant, but they're going to stay the same--
[00:31:34.360 --> 00:31:35.080]   the whole time.
[00:31:35.080 --> 00:31:38.320]   So it's as if you have multiple copies of the same unit.
[00:31:38.320 --> 00:31:40.920]   So you essentially have to break that initial symmetry
[00:31:40.920 --> 00:31:43.080]   that you would create if you initialized everything
[00:31:43.080 --> 00:31:44.520]   to the same value.
[00:31:44.520 --> 00:31:46.260]   So what we end up doing most of the time
[00:31:46.260 --> 00:31:50.480]   is initialize the weights to some randomly generated value.
[00:31:50.480 --> 00:31:52.080]   Often, we generate them--
[00:31:52.080 --> 00:31:54.120]   there are a few other recipes, but one of them
[00:31:54.120 --> 00:31:56.480]   is to initialize them from some uniform distribution
[00:31:56.480 --> 00:31:59.360]   between lower and upper bound.
[00:31:59.360 --> 00:32:01.440]   This is a recipe here that is often
[00:32:01.440 --> 00:32:05.440]   used that has some theoretical grounding that was derived
[00:32:05.440 --> 00:32:06.880]   specifically for the tanh.
[00:32:06.880 --> 00:32:09.880]   There's this paper here by Xavier Guerroux and Yoshua
[00:32:09.880 --> 00:32:13.120]   Bengio you can check out for some intuition as to how
[00:32:13.120 --> 00:32:14.460]   you should initialize the weights.
[00:32:14.460 --> 00:32:17.100]   But essentially, they should be initially random,
[00:32:17.100 --> 00:32:19.320]   and they should be initially close to 0.
[00:32:19.320 --> 00:32:23.160]   Random to break symmetry, and close to 0
[00:32:23.160 --> 00:32:27.040]   so that initially the units are not already saturated.
[00:32:27.040 --> 00:32:28.680]   Because if the units are saturated,
[00:32:28.680 --> 00:32:29.800]   then there are no gradients that are
[00:32:29.800 --> 00:32:31.280]   going to pass through the units.
[00:32:31.280 --> 00:32:33.680]   You're essentially going to get gradients very close to 0
[00:32:33.680 --> 00:32:35.120]   at the lower layers.
[00:32:35.120 --> 00:32:37.360]   So that's the main intuition, is to have weights
[00:32:37.360 --> 00:32:40.200]   that are small and random.
[00:32:40.200 --> 00:32:45.360]   So those are all the pieces we need
[00:32:45.360 --> 00:32:47.200]   for running stochastic gradient descent.
[00:32:47.200 --> 00:32:48.860]   So that allows us to take a training set
[00:32:48.860 --> 00:32:50.720]   and run a certain number of epochs,
[00:32:50.720 --> 00:32:53.920]   and have the neural net learn from that training set.
[00:32:53.920 --> 00:32:57.120]   Now, there are other quantities in our neural network
[00:32:57.120 --> 00:32:59.240]   that we haven't specified how to choose them.
[00:32:59.240 --> 00:33:02.560]   So those are the hyperparameters.
[00:33:02.560 --> 00:33:05.240]   So usually, we're going to have a separate validation set.
[00:33:05.240 --> 00:33:06.880]   Most people here are familiar with machine learning,
[00:33:06.880 --> 00:33:08.440]   so that's a typical procedure.
[00:33:08.440 --> 00:33:10.320]   And then we need to select things like, OK,
[00:33:10.320 --> 00:33:11.560]   how many layers do I want?
[00:33:11.560 --> 00:33:14.200]   How many units per layer do I want?
[00:33:14.200 --> 00:33:16.120]   What's the step size, the learning rate
[00:33:16.120 --> 00:33:17.920]   of my stochastic gradient descent procedure,
[00:33:17.920 --> 00:33:19.540]   that alpha number?
[00:33:19.540 --> 00:33:22.300]   What is the weight decay that I'm going to use?
[00:33:22.300 --> 00:33:24.340]   So a standard thing in machine learning
[00:33:24.340 --> 00:33:27.380]   is to perform a grid search.
[00:33:27.380 --> 00:33:29.340]   That is, if I have two hyperparameters,
[00:33:29.340 --> 00:33:31.320]   I list out a bunch of values I want to try.
[00:33:31.320 --> 00:33:32.780]   So for the number of hidden units,
[00:33:32.780 --> 00:33:36.940]   maybe I want to try 100, 1,000, and 2,000, say.
[00:33:36.940 --> 00:33:38.540]   And then for the learning rate, maybe I
[00:33:38.540 --> 00:33:42.420]   want to try 0.01 and 0.001.
[00:33:42.420 --> 00:33:44.580]   So a grid search would just try all combinations
[00:33:44.580 --> 00:33:46.900]   of these three values for the hidden units
[00:33:46.900 --> 00:33:49.820]   and these two values for the learning rates.
[00:33:49.820 --> 00:33:53.340]   So that means that the more hyperparameters there are,
[00:33:53.340 --> 00:33:57.420]   the number of configurations you have to try out blows up
[00:33:57.420 --> 00:33:59.620]   and grows exponentially.
[00:33:59.620 --> 00:34:03.180]   So another procedure that is now more and more common,
[00:34:03.180 --> 00:34:07.580]   which is more practical, is to perform a form of random search.
[00:34:07.580 --> 00:34:10.020]   In this case, what you do is for each parameter,
[00:34:10.020 --> 00:34:13.460]   you actually determine a distribution of likely values
[00:34:13.460 --> 00:34:14.220]   you'd like to try.
[00:34:14.220 --> 00:34:16.100]   So it could be--
[00:34:16.100 --> 00:34:17.560]   so for the number of hidden units,
[00:34:17.560 --> 00:34:19.500]   maybe I do a uniform distribution
[00:34:19.500 --> 00:34:22.700]   over all integers from 100 to 1,000, say,
[00:34:22.700 --> 00:34:25.220]   or maybe a log uniform distribution.
[00:34:25.220 --> 00:34:26.920]   And for the learning rate, maybe, again,
[00:34:26.920 --> 00:34:32.840]   the log uniform distribution, but from 0.001 to 0.01, say.
[00:34:32.840 --> 00:34:35.260]   And then to get an experiment, so
[00:34:35.260 --> 00:34:37.060]   to get values for my hyperparameters
[00:34:37.060 --> 00:34:39.940]   to do an experiment with and get a performance on my validation
[00:34:39.940 --> 00:34:43.140]   set, I just independently sample from these distributions
[00:34:43.140 --> 00:34:46.460]   for each hyperparameter to get a full configuration
[00:34:46.460 --> 00:34:47.820]   for my experiment.
[00:34:47.820 --> 00:34:50.720]   And then because I have this way of getting one experiment,
[00:34:50.720 --> 00:34:53.740]   I do it independently for all of my jobs, all of my experiments
[00:34:53.740 --> 00:34:54.620]   that I will do.
[00:34:54.620 --> 00:34:58.020]   So in this case, if I know I have enough compute power
[00:34:58.020 --> 00:35:02.120]   to do 50 experiments, I just sample 50 independent samples
[00:35:02.120 --> 00:35:04.100]   from these distributions for hyperparameters,
[00:35:04.100 --> 00:35:07.860]   perform these 50 experiments, and I just take the best one.
[00:35:07.860 --> 00:35:09.880]   What's nice about it is that there are no--
[00:35:09.880 --> 00:35:12.620]   unlike grid search, there are never any holes in the grid.
[00:35:12.620 --> 00:35:15.260]   That is, you just specify how many experiments you do.
[00:35:15.260 --> 00:35:18.460]   If one of your jobs died, well, you just have one less.
[00:35:18.460 --> 00:35:21.900]   But there's no hole in your experiment.
[00:35:21.900 --> 00:35:24.700]   And also, one reason why it's particularly useful,
[00:35:24.700 --> 00:35:29.380]   this approach, is that if you have a specific value in grid
[00:35:29.380 --> 00:35:32.100]   search for one of the hyperparameters that just makes
[00:35:32.100 --> 00:35:35.100]   the experiment not work at all-- so learning rates
[00:35:35.100 --> 00:35:36.300]   are a lot like this.
[00:35:36.300 --> 00:35:38.620]   If you have a learning rate that's too high,
[00:35:38.620 --> 00:35:42.180]   it's quite possible that convergence of the optimization
[00:35:42.180 --> 00:35:43.540]   will not converge.
[00:35:43.540 --> 00:35:45.180]   Well, if you're using a grid search,
[00:35:45.180 --> 00:35:47.060]   it means that for all the experiments that
[00:35:47.060 --> 00:35:48.980]   use that specific value of the learning rate,
[00:35:48.980 --> 00:35:50.460]   they're all going to be garbage.
[00:35:50.460 --> 00:35:52.340]   They're all not going to be useful.
[00:35:52.340 --> 00:35:56.220]   And you don't really get this sort of big waste of computation
[00:35:56.220 --> 00:35:58.540]   if you do a random search, because most likely,
[00:35:58.540 --> 00:36:00.060]   all the values of your hyperparameters
[00:36:00.060 --> 00:36:01.660]   are going to be unique, because they're
[00:36:01.660 --> 00:36:06.140]   samples, say, from a uniform distribution over some range.
[00:36:06.140 --> 00:36:08.620]   So that actually works quite well,
[00:36:08.620 --> 00:36:10.580]   and it's quite recommended.
[00:36:10.580 --> 00:36:12.420]   And there are more advanced methods,
[00:36:12.420 --> 00:36:15.220]   like methods based on machine learning, Bayesian
[00:36:15.220 --> 00:36:18.460]   optimization, or sometimes known as sequential model-based
[00:36:18.460 --> 00:36:21.300]   optimization, that I won't talk about,
[00:36:21.300 --> 00:36:25.740]   but that works a bit better than random search.
[00:36:25.740 --> 00:36:27.780]   And that's another alternative if you
[00:36:27.780 --> 00:36:29.940]   think you have an issue finding good hyperparameters,
[00:36:29.940 --> 00:36:34.380]   is to investigate some of these more advanced methods.
[00:36:34.380 --> 00:36:37.180]   Now, you do this for most of your hyperparameters,
[00:36:37.180 --> 00:36:40.060]   but for the number of epochs, the number of times
[00:36:40.060 --> 00:36:44.700]   you go through all of your examples in your training set,
[00:36:44.700 --> 00:36:49.020]   what we usually do is not grid search or random search,
[00:36:49.020 --> 00:36:51.580]   but we use a thing known as early stopping.
[00:36:51.580 --> 00:36:53.460]   The idea here is that if I've trained
[00:36:53.460 --> 00:36:56.700]   a neural net for 10 epochs, while training a neural net
[00:36:56.700 --> 00:36:59.340]   with all the other hyperparameters kept constant,
[00:36:59.340 --> 00:37:01.300]   but one more epoch is easy.
[00:37:01.300 --> 00:37:02.820]   I just do one more epoch.
[00:37:02.820 --> 00:37:06.340]   So I shouldn't start over and then do, say,
[00:37:06.340 --> 00:37:08.580]   11 epochs from scratch.
[00:37:08.580 --> 00:37:10.300]   And so what we would do is we would just
[00:37:10.300 --> 00:37:12.820]   track what is the performance on the validation set
[00:37:12.820 --> 00:37:14.740]   as I do more and more epochs.
[00:37:14.740 --> 00:37:17.100]   And what we will typically see is the training error
[00:37:17.100 --> 00:37:20.980]   will go down, but the validation set performance will go down
[00:37:20.980 --> 00:37:22.900]   and eventually go up.
[00:37:22.900 --> 00:37:25.080]   The intuition here is that the gap
[00:37:25.080 --> 00:37:27.420]   between the performance on the training set
[00:37:27.420 --> 00:37:29.120]   and the performance on the validation set
[00:37:29.120 --> 00:37:31.260]   will tend to increase.
[00:37:31.260 --> 00:37:34.460]   And since the training curve cannot go below, usually,
[00:37:34.460 --> 00:37:38.280]   some bound, then eventually the validation set performance
[00:37:38.280 --> 00:37:39.820]   has to go up.
[00:37:39.820 --> 00:37:41.320]   Sometimes it won't necessarily go up,
[00:37:41.320 --> 00:37:42.700]   but it sort of stays stable.
[00:37:42.700 --> 00:37:44.080]   So with early stopping, what we do
[00:37:44.080 --> 00:37:46.260]   is that if we reach a point where the validation set
[00:37:46.260 --> 00:37:49.020]   performance hasn't improved from some certain number
[00:37:49.020 --> 00:37:52.340]   of iterations, which we refer to as the look ahead,
[00:37:52.340 --> 00:37:53.340]   we just stop.
[00:37:53.340 --> 00:37:54.660]   We go back to the neural net that
[00:37:54.660 --> 00:37:56.960]   had the best performance overall in the validation set,
[00:37:56.960 --> 00:37:58.700]   and that's my neural network.
[00:37:58.700 --> 00:38:01.460]   So I have now a very cheap way of actually getting
[00:38:01.460 --> 00:38:03.960]   the number of iterations or the number of epochs
[00:38:03.960 --> 00:38:07.220]   over my training set.
[00:38:07.220 --> 00:38:09.500]   A few more tricks of the trade.
[00:38:09.500 --> 00:38:13.060]   So it's always useful to normalize your data.
[00:38:13.060 --> 00:38:16.780]   It will often have the effect of speeding up training.
[00:38:16.780 --> 00:38:19.340]   If you have real value data for binary data,
[00:38:19.340 --> 00:38:21.940]   that's usually keep it as it is.
[00:38:21.940 --> 00:38:24.940]   So what I mean by that is just subtract for each dimension
[00:38:24.940 --> 00:38:27.700]   what is the average in the training set of that dimension,
[00:38:27.700 --> 00:38:29.660]   and then dividing by the standard deviation
[00:38:29.660 --> 00:38:33.500]   of each dimension again in my input space.
[00:38:33.500 --> 00:38:35.980]   So this can speed up training.
[00:38:35.980 --> 00:38:40.020]   We often use a decay on the learning rate.
[00:38:40.020 --> 00:38:41.660]   There are a few methods for doing this.
[00:38:41.660 --> 00:38:45.300]   One that's very simple is to start with a large learning
[00:38:45.300 --> 00:38:47.460]   rate and then track the performance on the validation
[00:38:47.460 --> 00:38:48.140]   set.
[00:38:48.140 --> 00:38:50.980]   And once on the validation set it stops improving,
[00:38:50.980 --> 00:38:53.060]   you decrease your learning rate by some ratio.
[00:38:53.060 --> 00:38:54.780]   Maybe you divide it by 2.
[00:38:54.780 --> 00:38:56.980]   And then you continue training for some time.
[00:38:56.980 --> 00:39:00.620]   Hopefully, the validation set performance starts improving.
[00:39:00.620 --> 00:39:04.020]   And then at some point, it stops improving, and then you stop.
[00:39:04.020 --> 00:39:05.460]   Or you divide again by 2.
[00:39:05.460 --> 00:39:08.180]   So that sort of gives you an adaptive--
[00:39:08.180 --> 00:39:10.260]   using the validation set, an adaptive way
[00:39:10.260 --> 00:39:11.700]   of changing your learning rate.
[00:39:11.700 --> 00:39:14.900]   And that can, again, work better than having a very small
[00:39:14.900 --> 00:39:16.860]   learning rate than waiting for a longer time.
[00:39:16.860 --> 00:39:18.660]   So making very fast progress initially,
[00:39:18.660 --> 00:39:20.500]   and then slower progress towards the end.
[00:39:20.500 --> 00:39:26.260]   Also, I've described so far the approach
[00:39:26.260 --> 00:39:30.740]   for training neural nets that is based on a single example
[00:39:30.740 --> 00:39:31.300]   at a time.
[00:39:31.300 --> 00:39:33.800]   But in practice, we actually use what's called mini-batches.
[00:39:33.800 --> 00:39:36.100]   That is, we compute the loss function
[00:39:36.100 --> 00:39:40.580]   on a small subset of examples, say, 64, 128.
[00:39:40.580 --> 00:39:43.580]   And then we take the average of the loss of all these examples
[00:39:43.580 --> 00:39:45.020]   in that mini-batch.
[00:39:45.020 --> 00:39:47.260]   And that's actually-- we compute the gradient
[00:39:47.260 --> 00:39:49.880]   of this average loss on that mini-batch.
[00:39:49.880 --> 00:39:53.100]   The reason why we do this is that it turns out
[00:39:53.100 --> 00:39:56.780]   that you can very efficiently implement the forward pass
[00:39:56.780 --> 00:40:01.260]   over all of these 64, 128 examples in my mini-batch
[00:40:01.260 --> 00:40:05.300]   in one pass by, instead of doing vector matrix multiplications
[00:40:05.300 --> 00:40:07.280]   when we compute the pre-activations,
[00:40:07.280 --> 00:40:09.760]   doing matrix-matrix multiplications, which
[00:40:09.760 --> 00:40:13.880]   are faster than doing multiple matrix-vector multiplications.
[00:40:13.880 --> 00:40:15.800]   So in your code, often, there will
[00:40:15.800 --> 00:40:17.480]   be this other hyperparameter, which
[00:40:17.480 --> 00:40:21.080]   is mostly optimized for speed in terms of how quickly training
[00:40:21.080 --> 00:40:25.240]   will proceed of the number of examples in your mini-batch.
[00:40:25.240 --> 00:40:27.200]   Other things to improve optimization
[00:40:27.200 --> 00:40:29.560]   might be using a thing like momentum.
[00:40:29.560 --> 00:40:33.320]   That is, instead of using, as the descent direction,
[00:40:33.320 --> 00:40:35.220]   the gradient of the loss function,
[00:40:35.220 --> 00:40:39.040]   I'm actually going to track a descent direction, which
[00:40:39.040 --> 00:40:41.160]   I'm going to compute as the current gradient
[00:40:41.160 --> 00:40:44.080]   for my current example or mini-batch,
[00:40:44.080 --> 00:40:47.040]   plus some fraction of the previous update,
[00:40:47.040 --> 00:40:50.200]   the previous direction of update.
[00:40:50.200 --> 00:40:52.640]   And beta now is a hyperparameter you have to optimize.
[00:40:52.640 --> 00:40:56.840]   So what this does is, if all the update directions agree
[00:40:56.840 --> 00:41:00.720]   across multiple updates, then it will start picking up momentum
[00:41:00.720 --> 00:41:05.840]   and actually make bigger steps in those directions.
[00:41:05.840 --> 00:41:08.760]   And then there are multiple, even more advanced methods
[00:41:08.760 --> 00:41:12.560]   for adding adaptive types of learning rates.
[00:41:12.560 --> 00:41:14.060]   I mentioned them here very quickly,
[00:41:14.060 --> 00:41:15.600]   because you might see them in papers.
[00:41:15.600 --> 00:41:17.480]   There's a method known as AdaGrad,
[00:41:17.480 --> 00:41:19.440]   where the learning rate is actually
[00:41:19.440 --> 00:41:23.260]   scaled for each dimension, so for each weight
[00:41:23.260 --> 00:41:24.440]   and each biases.
[00:41:24.440 --> 00:41:28.800]   It's going to be scaled by what is the square root
[00:41:28.800 --> 00:41:31.920]   of the cumulative sum of the squared gradients.
[00:41:31.920 --> 00:41:35.360]   So what I track is I take my gradient vector at each step.
[00:41:35.360 --> 00:41:39.120]   I do an element-wise square of all the dimensions
[00:41:39.120 --> 00:41:41.160]   of my gradients, my gradient vector.
[00:41:41.160 --> 00:41:43.160]   And then I accumulate that in some variable
[00:41:43.160 --> 00:41:44.920]   that I'm noting as gamma here.
[00:41:44.920 --> 00:41:47.840]   And then for my descent direction, I take the gradient,
[00:41:47.840 --> 00:41:50.080]   and I do an element-wise division
[00:41:50.080 --> 00:41:52.920]   by the square root of this cumulative sum
[00:41:52.920 --> 00:41:54.720]   of squared gradients.
[00:41:54.720 --> 00:41:57.440]   There's also RMSProp, which is essentially like AdaGrad,
[00:41:57.440 --> 00:41:59.640]   but instead of doing a cumulative sum,
[00:41:59.640 --> 00:42:02.080]   we're going to do an exponential moving average.
[00:42:02.080 --> 00:42:04.760]   So we take the previous value times some factor
[00:42:04.760 --> 00:42:08.960]   plus 1 minus this factor times the current squared gradient.
[00:42:08.960 --> 00:42:10.380]   So that's RMSProp.
[00:42:10.380 --> 00:42:12.720]   And then there's Adam, which is essentially
[00:42:12.720 --> 00:42:15.380]   a combination of RMSProp with momentum, which
[00:42:15.380 --> 00:42:16.200]   is more involved.
[00:42:16.200 --> 00:42:17.960]   And I won't have time to describe it here,
[00:42:17.960 --> 00:42:21.440]   but that's another method that's often actually implemented
[00:42:21.440 --> 00:42:24.400]   in these different softwares and that people seem
[00:42:24.400 --> 00:42:28.120]   to use with a lot of success.
[00:42:28.120 --> 00:42:31.400]   And finally, in terms of actually debugging
[00:42:31.400 --> 00:42:33.160]   your implementations--
[00:42:33.160 --> 00:42:35.280]   so for instance, if you're lucky,
[00:42:35.280 --> 00:42:36.720]   you can build your neural network
[00:42:36.720 --> 00:42:38.680]   without difficulty using the current tools that
[00:42:38.680 --> 00:42:41.240]   are available in Torch or TensorFlow or Tiano.
[00:42:41.240 --> 00:42:42.840]   But maybe sometimes you actually have
[00:42:42.840 --> 00:42:45.640]   to implement certain gradients for a new module
[00:42:45.640 --> 00:42:47.920]   and a new box in your flow graph that
[00:42:47.920 --> 00:42:49.520]   isn't currently supported.
[00:42:49.520 --> 00:42:51.960]   If you do this, you should check that you've implemented
[00:42:51.960 --> 00:42:53.760]   your gradients correctly.
[00:42:53.760 --> 00:42:56.560]   And one way of doing that is to actually compare
[00:42:56.560 --> 00:42:58.560]   the gradients computed by your code
[00:42:58.560 --> 00:43:01.240]   with a finite difference of estimate.
[00:43:01.240 --> 00:43:03.160]   So what you do is, for each parameter,
[00:43:03.160 --> 00:43:07.220]   you add some very small epsilon value, say 10 to the minus 6,
[00:43:07.220 --> 00:43:10.760]   and you compute what is the output of your module.
[00:43:10.760 --> 00:43:12.360]   And then you subtract the same thing,
[00:43:12.360 --> 00:43:15.600]   but where you've subtracted the small quantity,
[00:43:15.600 --> 00:43:17.400]   and then you divide by 2 epsilon.
[00:43:17.400 --> 00:43:20.520]   So if epsilon converges to 0, then you actually
[00:43:20.520 --> 00:43:21.960]   get the partial derivative.
[00:43:21.960 --> 00:43:24.240]   But if it's just small, it's going to be an approximate.
[00:43:24.240 --> 00:43:26.600]   And usually, this finite difference estimate
[00:43:26.600 --> 00:43:29.520]   will be very close to a correct implementation
[00:43:29.520 --> 00:43:30.840]   of the real gradient.
[00:43:30.840 --> 00:43:33.380]   So you should definitely do that if you've actually
[00:43:33.380 --> 00:43:36.160]   implemented some of the gradients in your code.
[00:43:36.160 --> 00:43:37.900]   And then another useful thing to do
[00:43:37.900 --> 00:43:41.880]   is to actually do a very small experiment on a small data set
[00:43:41.880 --> 00:43:44.320]   before you actually run your full experiment
[00:43:44.320 --> 00:43:45.880]   on your complete data set.
[00:43:45.880 --> 00:43:47.760]   So use, say, 50 examples.
[00:43:47.760 --> 00:43:50.400]   So just taking a random subset of 50 examples
[00:43:50.400 --> 00:43:52.000]   from your data set.
[00:43:52.000 --> 00:43:54.640]   Actually, just make sure that your code can overfit
[00:43:54.640 --> 00:43:58.600]   to that data, can essentially classify it perfectly,
[00:43:58.600 --> 00:44:03.040]   given enough capacity that you would think it should get it.
[00:44:03.040 --> 00:44:06.300]   So if it's not the case, then there's a few things
[00:44:06.300 --> 00:44:08.320]   that you might want to investigate.
[00:44:08.320 --> 00:44:09.920]   Maybe your initialization is such
[00:44:09.920 --> 00:44:12.620]   that the units are already saturated initially,
[00:44:12.620 --> 00:44:14.800]   and so there's no actual optimization
[00:44:14.800 --> 00:44:17.440]   happening because some of the gradients on some of the weights
[00:44:17.440 --> 00:44:19.040]   are exactly zero.
[00:44:19.040 --> 00:44:22.160]   So you might want to check your initialization.
[00:44:22.160 --> 00:44:23.760]   Maybe your gradients are just--
[00:44:23.760 --> 00:44:26.040]   you're using a model you implemented gradients for,
[00:44:26.040 --> 00:44:28.920]   and maybe your gradients are not properly implemented.
[00:44:28.920 --> 00:44:31.040]   Maybe you haven't normalized your input, which
[00:44:31.040 --> 00:44:33.040]   creates some instability, making it
[00:44:33.040 --> 00:44:38.400]   harder for stochastic gradient descent to work successfully.
[00:44:38.400 --> 00:44:40.080]   Maybe your learning rate is too large.
[00:44:40.080 --> 00:44:42.840]   Then you should consider trying smaller learning rates.
[00:44:42.840 --> 00:44:44.940]   That's actually a pretty good way of adding
[00:44:44.940 --> 00:44:47.640]   some idea of the magnitude of the learning rate
[00:44:47.640 --> 00:44:49.320]   you should be using.
[00:44:49.320 --> 00:44:51.680]   And then once you actually overfit
[00:44:51.680 --> 00:44:53.060]   in your small training set, you're
[00:44:53.060 --> 00:44:56.640]   ready to do a full experiment on a larger data set.
[00:44:56.640 --> 00:44:58.880]   That said, this is not a replacement
[00:44:58.880 --> 00:45:00.240]   for gradient checking.
[00:45:00.240 --> 00:45:03.440]   So backprop and stochastic gradient descent,
[00:45:03.440 --> 00:45:06.800]   it's a great algorithm that's very bug resistant.
[00:45:06.800 --> 00:45:10.540]   You will potentially see some learning happening,
[00:45:10.540 --> 00:45:12.400]   even if some of your gradients are wrong,
[00:45:12.400 --> 00:45:13.840]   or say, exactly zero.
[00:45:13.840 --> 00:45:16.160]   So that's great if you're an engineer
[00:45:16.160 --> 00:45:17.800]   and you're implementing things.
[00:45:17.800 --> 00:45:19.960]   It's fun when code is somewhat bug resistant.
[00:45:19.960 --> 00:45:21.600]   But if you're actually doing science
[00:45:21.600 --> 00:45:23.960]   and trying to understand what's going on,
[00:45:23.960 --> 00:45:25.160]   that can be a complication.
[00:45:25.160 --> 00:45:29.160]   So do both, gradient checking and a small experiment
[00:45:29.160 --> 00:45:31.120]   like that.
[00:45:31.120 --> 00:45:33.000]   All right, and so for the last few minutes,
[00:45:33.000 --> 00:45:35.040]   I'll actually try to motivate what
[00:45:35.040 --> 00:45:40.280]   you'll be learning quite a bit about in the next two days.
[00:45:40.280 --> 00:45:43.880]   That is, the specific case for deep learning.
[00:45:43.880 --> 00:45:47.760]   So I've already told you that if I have a neural net with enough
[00:45:47.760 --> 00:45:49.880]   hidden units, theoretically, I can potentially
[00:45:49.880 --> 00:45:53.000]   represent pretty much any function, any classification
[00:45:53.000 --> 00:45:53.920]   function.
[00:45:53.920 --> 00:45:56.360]   So why would I want multiple layers?
[00:45:56.360 --> 00:45:59.000]   So there are a few motivations behind this.
[00:45:59.000 --> 00:46:02.160]   The first one is taken directly from our own brains.
[00:46:02.160 --> 00:46:05.320]   So we know in the visual cortex that the light that
[00:46:05.320 --> 00:46:08.520]   hits our retina eventually goes through several regions
[00:46:08.520 --> 00:46:09.880]   in the visual cortex.
[00:46:09.880 --> 00:46:12.520]   Eventually reaching an area known as V1,
[00:46:12.520 --> 00:46:16.040]   where you have units that are-- or neurons that are essentially
[00:46:16.040 --> 00:46:18.840]   tuned to small forms like edges.
[00:46:18.840 --> 00:46:20.480]   And then it goes on to V4, where it's
[00:46:20.480 --> 00:46:23.880]   slightly more complex patterns that the units are tuned for.
[00:46:23.880 --> 00:46:25.680]   And then you reach AIT, where you actually
[00:46:25.680 --> 00:46:27.680]   have neurons that are specific to certain objects
[00:46:27.680 --> 00:46:28.840]   or certain units.
[00:46:28.840 --> 00:46:30.960]   And so the idea here is that perhaps that's
[00:46:30.960 --> 00:46:35.800]   also what we want in an artificial, say, vision system.
[00:46:35.800 --> 00:46:37.760]   We'd like it, if it's detecting faces,
[00:46:37.760 --> 00:46:41.080]   to have a first layer that detects simple edges,
[00:46:41.080 --> 00:46:43.520]   and then another layer that perhaps puts these edges
[00:46:43.520 --> 00:46:45.840]   together, detecting slightly more complex things,
[00:46:45.840 --> 00:46:47.880]   like a nose or a mouth or eyes.
[00:46:47.880 --> 00:46:49.880]   And then eventually have a layer that combines
[00:46:49.880 --> 00:46:54.520]   these slightly less abstract or more abstract units
[00:46:54.520 --> 00:46:56.200]   to get something even more abstract,
[00:46:56.200 --> 00:46:58.540]   like a complete face.
[00:46:58.540 --> 00:47:00.680]   There's also some theoretical justification
[00:47:00.680 --> 00:47:04.160]   for using multiple layers.
[00:47:04.160 --> 00:47:06.360]   So the early results were mostly based
[00:47:06.360 --> 00:47:08.920]   on studying Boolean functions, or a function that
[00:47:08.920 --> 00:47:10.160]   takes as input--
[00:47:10.160 --> 00:47:12.640]   can think of it as a vector of just zeros and ones.
[00:47:12.640 --> 00:47:17.080]   And you could show that there are certain functions that,
[00:47:17.080 --> 00:47:19.960]   if you had essentially a Boolean neural network
[00:47:19.960 --> 00:47:22.840]   or essentially a Boolean circuit,
[00:47:22.840 --> 00:47:26.280]   and you restricted the number of layers of that circuit,
[00:47:26.280 --> 00:47:28.480]   that there are certain functions that, in this case,
[00:47:28.480 --> 00:47:31.160]   to represent certain Boolean functions exactly,
[00:47:31.160 --> 00:47:33.600]   you would need an exponential number of units
[00:47:33.600 --> 00:47:35.160]   in each of these layers.
[00:47:35.160 --> 00:47:37.160]   Whereas if you allowed yourself to have multiple layers,
[00:47:37.160 --> 00:47:39.840]   then you could represent these functions more compactly.
[00:47:39.840 --> 00:47:41.560]   And so that's another motivation,
[00:47:41.560 --> 00:47:42.960]   that perhaps with more layers, we
[00:47:42.960 --> 00:47:48.520]   can represent fairly complex functions in a more compact way.
[00:47:48.520 --> 00:47:51.160]   And then there's the reason that they just work.
[00:47:51.160 --> 00:47:53.920]   So we've seen in the past few years
[00:47:53.920 --> 00:47:56.440]   great success in speech recognition, where it's
[00:47:56.440 --> 00:47:59.020]   essentially revolutionized the field, where everyone's using
[00:47:59.020 --> 00:48:00.940]   deep learning for speech recognition,
[00:48:00.940 --> 00:48:03.840]   and same thing for visual object recognition,
[00:48:03.840 --> 00:48:05.400]   where, again, deep learning is sort
[00:48:05.400 --> 00:48:10.520]   of the method of choice for identifying objects in images.
[00:48:10.520 --> 00:48:13.760]   So then why are we doing this only recently?
[00:48:13.760 --> 00:48:16.640]   Why didn't we do deep learning way back
[00:48:16.640 --> 00:48:20.760]   when backprop was invented, which is essentially in 1980s
[00:48:20.760 --> 00:48:22.840]   and even before that?
[00:48:22.840 --> 00:48:24.760]   So it turns out training deep neural networks
[00:48:24.760 --> 00:48:26.040]   is actually not that easy.
[00:48:26.040 --> 00:48:30.000]   There are a few hurdles that one can be confronted with.
[00:48:30.000 --> 00:48:32.120]   I've already mentioned one of the issues, which
[00:48:32.120 --> 00:48:35.640]   is that some of the gradients might be fading as you go
[00:48:35.640 --> 00:48:37.280]   from the top layer to the bottom layer,
[00:48:37.280 --> 00:48:39.940]   because we keep multiplying by the derivative of the activation
[00:48:39.940 --> 00:48:40.520]   function.
[00:48:40.520 --> 00:48:41.960]   So that makes training hard.
[00:48:41.960 --> 00:48:44.560]   It could be that the lower layers at very small gradients
[00:48:44.560 --> 00:48:49.040]   are barely moving and exploring the space of correct features
[00:48:49.040 --> 00:48:51.320]   to learn for a given problem.
[00:48:51.320 --> 00:48:52.960]   Sometimes that's the problem you find.
[00:48:52.960 --> 00:48:54.880]   You have a hard time just fitting your data,
[00:48:54.880 --> 00:48:57.080]   and you're essentially underfitting.
[00:48:57.080 --> 00:49:00.360]   Or it could be that with deeper neural nets or bigger
[00:49:00.360 --> 00:49:02.200]   neural nets, we have more parameters.
[00:49:02.200 --> 00:49:04.240]   So perhaps sometimes we're actually overfitting.
[00:49:04.240 --> 00:49:07.680]   We're in a situation where all the functions that we
[00:49:07.680 --> 00:49:10.840]   can represent with the same neural net represented
[00:49:10.840 --> 00:49:14.040]   by this gray area function actually includes, yes,
[00:49:14.040 --> 00:49:15.660]   the right function, but it's so large
[00:49:15.660 --> 00:49:18.280]   that for a finite training set, the odds
[00:49:18.280 --> 00:49:19.780]   that I'm going to find the one that's
[00:49:19.780 --> 00:49:22.840]   close to the true classifying function, the real system
[00:49:22.840 --> 00:49:25.720]   that I'd like to have, is going to be very different.
[00:49:25.720 --> 00:49:28.080]   So in this case, I'm essentially overfitting,
[00:49:28.080 --> 00:49:31.200]   and that might also be a situation we're in.
[00:49:31.200 --> 00:49:35.880]   And unfortunately, there are many situations
[00:49:35.880 --> 00:49:40.920]   where one problem is observed, overfitting or underfitting.
[00:49:40.920 --> 00:49:43.480]   And so we essentially have, in the field,
[00:49:43.480 --> 00:49:46.140]   developed tools for fighting both situations.
[00:49:46.140 --> 00:49:49.600]   And I'm going to rapidly touch a few of those, which you will
[00:49:49.600 --> 00:49:53.560]   see will come up later on in multiple talks.
[00:49:53.560 --> 00:49:56.320]   So one of the first hypotheses, which
[00:49:56.320 --> 00:49:57.760]   might be that you're underfitting,
[00:49:57.760 --> 00:49:59.960]   well, you can essentially just fight this
[00:49:59.960 --> 00:50:01.880]   by waiting longer, so training longer.
[00:50:01.880 --> 00:50:03.600]   If you have your gradients are too small,
[00:50:03.600 --> 00:50:05.800]   and this is essentially why you're progressing very slowly
[00:50:05.800 --> 00:50:08.220]   when you're training, well, if you're using GPUs
[00:50:08.220 --> 00:50:11.480]   and are able to do more iterations over the same
[00:50:11.480 --> 00:50:15.120]   training set in less time, that might just
[00:50:15.120 --> 00:50:16.800]   solve your problem of underfitting.
[00:50:16.800 --> 00:50:18.760]   And I think we've seen some of that,
[00:50:18.760 --> 00:50:21.340]   and this is partly why GPUs have been so game-changing
[00:50:21.340 --> 00:50:22.520]   for deep learning.
[00:50:22.520 --> 00:50:26.120]   Or you can use just better optimization methods also.
[00:50:26.120 --> 00:50:28.200]   And if you're overfitting, well, we just
[00:50:28.200 --> 00:50:31.560]   need better regularization.
[00:50:31.560 --> 00:50:33.680]   I've been involved early on in my PhD
[00:50:33.680 --> 00:50:36.040]   on using unsupervised learning as a way
[00:50:36.040 --> 00:50:38.820]   to regularize neural nets.
[00:50:38.820 --> 00:50:40.940]   If I have time, I'll talk a little bit about that.
[00:50:40.940 --> 00:50:43.200]   And there's another method you might have heard
[00:50:43.200 --> 00:50:44.840]   about known as dropout.
[00:50:44.840 --> 00:50:49.560]   So I'll try to touch at least two methods that are essentially
[00:50:49.560 --> 00:50:51.360]   trying to address some of these issues.
[00:50:51.360 --> 00:50:54.720]   So the first one that I'll talk about is dropout.
[00:50:54.720 --> 00:50:57.920]   It's actually very easy, very simple.
[00:50:57.920 --> 00:51:01.120]   So the idea of if our neural net is essentially overfitting,
[00:51:01.120 --> 00:51:04.520]   so it's too good at training on the training set,
[00:51:04.520 --> 00:51:06.880]   well, we're essentially going to cripple training.
[00:51:06.880 --> 00:51:09.240]   We're going to make it harder to fit the training set.
[00:51:09.240 --> 00:51:11.240]   And the way we're going to do that in dropout
[00:51:11.240 --> 00:51:13.880]   is that we will stochastically remove
[00:51:13.880 --> 00:51:16.120]   hidden units independently.
[00:51:16.120 --> 00:51:18.880]   So for each hidden unit, before we do a forward pass,
[00:51:18.880 --> 00:51:20.240]   we'll flip a coin.
[00:51:20.240 --> 00:51:22.880]   And with probability half, we will
[00:51:22.880 --> 00:51:24.800]   multiply the activation by 0.
[00:51:24.800 --> 00:51:27.500]   And with probability half, we'll multiply it by 1.
[00:51:27.500 --> 00:51:30.840]   So what this means is that if a unit is multiplied by 0,
[00:51:30.840 --> 00:51:34.120]   it's effectively not in the neural net anymore.
[00:51:34.120 --> 00:51:36.360]   And we're doing this independently
[00:51:36.360 --> 00:51:37.880]   for each hidden units.
[00:51:37.880 --> 00:51:41.840]   So that means that in a layer, a unit cannot rely anymore
[00:51:41.840 --> 00:51:44.760]   on the presence on any other units
[00:51:44.760 --> 00:51:47.880]   to try to sort of synchronize and adapt
[00:51:47.880 --> 00:51:50.600]   to perform a complex classification
[00:51:50.600 --> 00:51:52.360]   or learn a complex feature.
[00:51:52.360 --> 00:51:54.680]   And that was partly the motivation behind dropout
[00:51:54.680 --> 00:51:57.480]   is that this procedure might encourage
[00:51:57.480 --> 00:51:59.760]   types of features that are not co-adapted
[00:51:59.760 --> 00:52:02.880]   and are less likely to overfit.
[00:52:02.880 --> 00:52:05.960]   So we often use 0.5 as the probability
[00:52:05.960 --> 00:52:08.040]   of dropping out a unit.
[00:52:08.040 --> 00:52:10.040]   It turns out it often, surprisingly,
[00:52:10.040 --> 00:52:11.160]   is the best value.
[00:52:11.160 --> 00:52:12.920]   But that's another hyperparameter
[00:52:12.920 --> 00:52:15.060]   you might want to tune.
[00:52:15.060 --> 00:52:18.560]   And in terms of how it impacts an implementation of backdrop,
[00:52:18.560 --> 00:52:20.000]   it's very simple.
[00:52:20.000 --> 00:52:21.600]   So the forward pass, before I do it,
[00:52:21.600 --> 00:52:24.760]   I just sample my binary masks for all my layers.
[00:52:24.760 --> 00:52:28.240]   And then when I'm performing backdrop, well,
[00:52:28.240 --> 00:52:30.000]   my gradient on the--
[00:52:30.000 --> 00:52:30.480]   oh, sorry.
[00:52:30.480 --> 00:52:31.580]   So that's the forward pass.
[00:52:31.580 --> 00:52:34.760]   I'm just multiplying by this binary mask here.
[00:52:34.760 --> 00:52:36.640]   So super simple change.
[00:52:36.640 --> 00:52:39.000]   And then in terms of backdrop, well, I'm
[00:52:39.000 --> 00:52:41.160]   also going to multiply by the mask
[00:52:41.160 --> 00:52:43.800]   when I get my gradient on the pre-activation.
[00:52:43.800 --> 00:52:47.080]   And also, don't forget that the activations are now different.
[00:52:47.080 --> 00:52:49.920]   They actually include the mask in my notation.
[00:52:49.920 --> 00:52:52.860]   So it's a very simple change in the forward and backward pass
[00:52:52.860 --> 00:52:54.280]   when you're training.
[00:52:54.280 --> 00:52:56.280]   And also, another thing that I should emphasize
[00:52:56.280 --> 00:52:59.600]   is that the mask is being resampled for every example.
[00:52:59.600 --> 00:53:02.160]   So before you do a forward pass, you resample the mask.
[00:53:02.160 --> 00:53:03.280]   You don't keep it--
[00:53:03.280 --> 00:53:07.560]   sample it once and then use it the whole time.
[00:53:07.560 --> 00:53:09.560]   And then at test time, because we don't really
[00:53:09.560 --> 00:53:13.600]   like a model that sort of randomly changes its output,
[00:53:13.600 --> 00:53:16.840]   because it will if we stochastically change the masks,
[00:53:16.840 --> 00:53:18.920]   what we do is we replace the mask
[00:53:18.920 --> 00:53:23.600]   by the probability of dropping out a unit,
[00:53:23.600 --> 00:53:25.480]   or actually of keeping a unit.
[00:53:25.480 --> 00:53:29.200]   So if we're using 0.5, that's just 0.5.
[00:53:29.200 --> 00:53:31.360]   We can actually show that if you have a neural net
[00:53:31.360 --> 00:53:34.720]   with a single hidden layer, doing this transformation
[00:53:34.720 --> 00:53:36.760]   at test time, multiplying by 0.5 is
[00:53:36.760 --> 00:53:39.640]   equivalent to doing a geometric average of all
[00:53:39.640 --> 00:53:42.120]   the possible neural networks with all the different binary
[00:53:42.120 --> 00:53:43.360]   mask patterns.
[00:53:43.360 --> 00:53:46.600]   So it's essentially one way of thinking about dropout
[00:53:46.600 --> 00:53:49.020]   in the single layer case is that it's kind of an assembling
[00:53:49.020 --> 00:53:51.040]   method where you have a lot of models,
[00:53:51.040 --> 00:53:52.760]   an exponential number of models, which
[00:53:52.760 --> 00:53:56.040]   are all sharing the same weights but have different masks.
[00:53:56.040 --> 00:53:59.520]   That intuition, though, doesn't transfer for deep neural nets
[00:53:59.520 --> 00:54:01.480]   in the sense that you cannot show this result.
[00:54:01.480 --> 00:54:05.920]   It really only applies to a single hidden layer.
[00:54:05.920 --> 00:54:08.480]   So in practice, it's very effective,
[00:54:08.480 --> 00:54:10.640]   but do expect some slowdown in training.
[00:54:10.640 --> 00:54:13.640]   So often, we tend to see that training a network
[00:54:13.640 --> 00:54:16.160]   to completion will take twice as many epochs
[00:54:16.160 --> 00:54:18.360]   if you're using dropout with 0.5.
[00:54:18.360 --> 00:54:19.980]   And here, you have the reference if you
[00:54:19.980 --> 00:54:21.920]   want to learn more about different variations
[00:54:21.920 --> 00:54:23.000]   of dropouts and so on.
[00:54:23.000 --> 00:54:28.920]   And I probably won't talk about unsupervised retraining
[00:54:28.920 --> 00:54:31.040]   for lack of time, but I'll talk about another thing
[00:54:31.040 --> 00:54:33.440]   that you'll definitely probably hear about,
[00:54:33.440 --> 00:54:35.740]   and that's implemented in these different packages, which
[00:54:35.740 --> 00:54:37.360]   is batch normalization.
[00:54:37.360 --> 00:54:39.660]   Batch normalization is kind of interesting in the sense
[00:54:39.660 --> 00:54:42.840]   that it's been shown to better optimize.
[00:54:42.840 --> 00:54:46.000]   That is, certain networks that would otherwise underfit
[00:54:46.000 --> 00:54:48.440]   would not underfit as much anymore
[00:54:48.440 --> 00:54:49.840]   if you use batch normalization.
[00:54:49.840 --> 00:54:51.440]   But also, it's been shown that when
[00:54:51.440 --> 00:54:54.720]   you use batch normalization, dropout is not as useful.
[00:54:54.720 --> 00:54:56.720]   And dropout being a regularization method,
[00:54:56.720 --> 00:54:59.760]   that suggests that perhaps batch normalization is also
[00:54:59.760 --> 00:55:01.280]   regularizing in some way.
[00:55:01.280 --> 00:55:03.800]   So these things are not one or the other.
[00:55:03.800 --> 00:55:05.160]   They're not mutually exclusive.
[00:55:05.160 --> 00:55:07.440]   You can have a regularizer that also, it turns out,
[00:55:07.440 --> 00:55:10.240]   helps you better optimize.
[00:55:10.240 --> 00:55:14.760]   So the intuition behind batch normalization
[00:55:14.760 --> 00:55:19.560]   is much like I've suggested that normalizing your inputs actually
[00:55:19.560 --> 00:55:21.120]   can help speeding up training.
[00:55:21.120 --> 00:55:24.380]   Well, how about we also normalize all the hidden layers
[00:55:24.380 --> 00:55:27.600]   when I'm doing my forward pass?
[00:55:27.600 --> 00:55:28.960]   Now, the problem in doing this is
[00:55:28.960 --> 00:55:31.880]   that I can compute the mean and the standard deviations
[00:55:31.880 --> 00:55:35.000]   of my inputs once and for all because they're constant.
[00:55:35.000 --> 00:55:36.640]   But my hidden layers are constantly
[00:55:36.640 --> 00:55:39.080]   changing because I'm training these parameters.
[00:55:39.080 --> 00:55:41.640]   So the mean and the standard deviation of my units
[00:55:41.640 --> 00:55:42.800]   will change.
[00:55:42.800 --> 00:55:45.920]   And so it would be very expensive
[00:55:45.920 --> 00:55:48.120]   if every time I did an update on my parameters,
[00:55:48.120 --> 00:55:50.540]   I recomputed the means and the standard deviations
[00:55:50.540 --> 00:55:52.400]   of all of my units.
[00:55:52.400 --> 00:55:54.980]   So batch normalization addresses some of these issues
[00:55:54.980 --> 00:55:56.360]   as follows.
[00:55:56.360 --> 00:55:59.520]   So the way it works is first, the normalization
[00:55:59.520 --> 00:56:02.240]   is going to be applied on actually the pre-activation.
[00:56:02.240 --> 00:56:03.660]   So not the activation of the unit,
[00:56:03.660 --> 00:56:06.480]   but before the non-linearity.
[00:56:06.480 --> 00:56:08.600]   During training, to address the issue
[00:56:08.600 --> 00:56:11.120]   that we don't want to compute means over the full training
[00:56:11.120 --> 00:56:12.720]   set because that would be too slow,
[00:56:12.720 --> 00:56:15.960]   I'm actually going to compute it on each mini-batch.
[00:56:15.960 --> 00:56:18.120]   So I have to do mini-batch training here.
[00:56:18.120 --> 00:56:21.560]   I'm going to take my small mini-batch of 64, 128 examples.
[00:56:21.560 --> 00:56:23.440]   And that's the set of examples on which
[00:56:23.440 --> 00:56:26.800]   I'm going to compute my means and standard deviations.
[00:56:26.800 --> 00:56:28.920]   And then when I do backprop, I'm actually
[00:56:28.920 --> 00:56:31.160]   going to take into account the normalization.
[00:56:31.160 --> 00:56:33.280]   So now there's going to be a gradient going
[00:56:33.280 --> 00:56:36.360]   through the computation of the mean and the standard deviation
[00:56:36.360 --> 00:56:38.320]   because they depend on the parameters
[00:56:38.320 --> 00:56:40.160]   of the neural network.
[00:56:40.160 --> 00:56:41.540]   And then at test time, we'll just
[00:56:41.540 --> 00:56:44.200]   use the global mean and global standard deviation.
[00:56:44.200 --> 00:56:45.880]   Once I finish training, I can actually
[00:56:45.880 --> 00:56:48.120]   do a full pass over the whole training set and get
[00:56:48.120 --> 00:56:52.000]   all of my means and standard deviations.
[00:56:52.000 --> 00:56:54.680]   So that's essentially the pseudocode for that,
[00:56:54.680 --> 00:56:57.360]   taken out of the paper directly.
[00:56:57.360 --> 00:57:00.520]   So if x is a pre-activation for a unit
[00:57:00.520 --> 00:57:02.640]   and have multiple pre-activations
[00:57:02.640 --> 00:57:05.240]   for a single unit across my mini-batch,
[00:57:05.240 --> 00:57:08.280]   I would compute what is the average for that unit
[00:57:08.280 --> 00:57:11.400]   pre-activation across my examples in my mini-batch,
[00:57:11.400 --> 00:57:15.000]   compute my variance, and then subtract the mean
[00:57:15.000 --> 00:57:17.760]   and divide by the square root of the variance,
[00:57:17.760 --> 00:57:19.680]   plus some epsilon for numerical stability
[00:57:19.680 --> 00:57:22.160]   in case the variance is too close to zero.
[00:57:22.160 --> 00:57:25.200]   And then another thing is that actually batch normalization
[00:57:25.200 --> 00:57:28.080]   doesn't just perform this normalization
[00:57:28.080 --> 00:57:30.400]   and outputs the normalized pre-activation.
[00:57:30.400 --> 00:57:34.640]   It then actually performs a linear transformation on it.
[00:57:34.640 --> 00:57:37.160]   So it multiplies it by this parameter gamma,
[00:57:37.160 --> 00:57:40.160]   which is going to be trained by gradient descent.
[00:57:40.160 --> 00:57:43.240]   And it's often called the gain parameter
[00:57:43.240 --> 00:57:45.760]   of batch normalization.
[00:57:45.760 --> 00:57:47.960]   And it adds a bias beta.
[00:57:47.960 --> 00:57:51.360]   And the reason is that if I'm subtracting by the mean,
[00:57:51.360 --> 00:57:54.620]   then each of these units have the bias parameter.
[00:57:54.620 --> 00:57:58.280]   So if I subtract it, then this essentially here,
[00:57:58.280 --> 00:57:59.680]   there's no bias anymore.
[00:57:59.680 --> 00:58:01.360]   It was present here, it was present here,
[00:58:01.360 --> 00:58:02.720]   and now it's been subtracted.
[00:58:02.720 --> 00:58:05.580]   So I have to add the bias, but after the batch normalization,
[00:58:05.580 --> 00:58:06.360]   essentially.
[00:58:06.360 --> 00:58:08.240]   So these betas here are essentially
[00:58:08.240 --> 00:58:10.380]   the new bias parameters.
[00:58:10.380 --> 00:58:11.800]   And those will actually be trained.
[00:58:11.800 --> 00:58:13.760]   So we do gradient descent also on those.
[00:58:13.760 --> 00:58:18.120]   So batch normalization adds a few parameters.
[00:58:18.120 --> 00:58:20.640]   All right, and as I said, I'm just going to skip over this.
[00:58:20.640 --> 00:58:23.200]   And I'm not showing what the gradients are when you back
[00:58:23.200 --> 00:58:24.760]   prop through the mean and so on.
[00:58:24.760 --> 00:58:27.300]   It's described in the paper if you want to see the gradients.
[00:58:27.300 --> 00:58:31.200]   But otherwise, in the different packages,
[00:58:31.200 --> 00:58:33.160]   you'll get the gradients automatically.
[00:58:33.160 --> 00:58:35.580]   It's usually been implemented.
[00:58:35.580 --> 00:58:38.080]   Skipping over that, I'll just finish.
[00:58:38.080 --> 00:58:40.160]   If you actually want to learn about unsupervised
[00:58:40.160 --> 00:58:42.480]   pre-training and why it works, I have videos on that.
[00:58:42.480 --> 00:58:44.200]   So you can check that out.
[00:58:44.200 --> 00:58:46.160]   And I guess that's it.
[00:58:46.160 --> 00:58:46.960]   Thank you.
[00:58:46.960 --> 00:58:49.880]   [APPLAUSE]
[00:58:49.880 --> 00:58:56.040]   Thanks, Hugo.
[00:58:56.040 --> 00:58:58.400]   So we have a few minutes for questions which
[00:58:58.400 --> 00:59:00.520]   are intermingled with a break.
[00:59:00.520 --> 00:59:04.800]   So feel free to either go for a break or ask questions to Hugo.
[00:59:04.800 --> 00:59:06.220]   I believe there are microphones.
[00:59:06.220 --> 00:59:07.760]   And I'll also stick around.
[00:59:07.760 --> 00:59:09.880]   So if you want to ask me questions offline,
[00:59:09.880 --> 00:59:10.960]   that's also fine.
[00:59:10.960 --> 00:59:13.040]   If anyone has questions, you can go to the mic.
[00:59:13.040 --> 00:59:23.080]   Go to the microphone.
[00:59:23.080 --> 00:59:23.580]   Hi.
[00:59:23.580 --> 00:59:24.080]   Hi.
[00:59:24.080 --> 00:59:26.340]   You mentioned the ReLU adds sparsity.
[00:59:26.340 --> 00:59:28.440]   Can you explain why?
[00:59:28.440 --> 00:59:34.400]   Yeah, so the first thing is that it's observed in practice.
[00:59:34.400 --> 00:59:37.200]   And it adds some sparsity in part
[00:59:37.200 --> 00:59:40.520]   because you have the non-linearity at 0 below.
[00:59:40.520 --> 00:59:43.560]   So it means that units are going to be potentially
[00:59:43.560 --> 00:59:49.480]   exactly sparse, essentially absent of the hidden layer.
[00:59:49.480 --> 00:59:54.400]   There are a few reasons to explain why you get sparsity.
[00:59:54.400 --> 00:59:56.900]   It turns out that this process of doing a linear
[00:59:56.900 --> 01:00:00.460]   transformation followed by the ReLU activation function
[01:00:00.460 --> 01:00:02.160]   is very close to some of the steps
[01:00:02.160 --> 01:00:04.840]   you would do when you're optimizing for sparse codes
[01:00:04.840 --> 01:00:07.700]   in a sparse coding model, if you know about sparse coding.
[01:00:07.700 --> 01:00:10.280]   So they're essentially an optimization method
[01:00:10.280 --> 01:00:12.720]   that, given some sparse coding model,
[01:00:12.720 --> 01:00:15.640]   will find what is the sparse representation,
[01:00:15.640 --> 01:00:17.580]   hidden representation for some input.
[01:00:17.580 --> 01:00:20.920]   And it's mostly a sequence of linear transformations
[01:00:20.920 --> 01:00:25.120]   followed by this ReLU-like activation function.
[01:00:25.120 --> 01:00:27.600]   And I think this is partly the explanation.
[01:00:27.600 --> 01:00:31.280]   Otherwise, I don't know of a solid explanation
[01:00:31.280 --> 01:00:34.280]   for why that is beyond what's observed in practice.
[01:00:34.280 --> 01:00:41.240]   Any more questions?
[01:00:41.240 --> 01:00:43.120]   If not, let's thank Hugo again.
[01:00:43.120 --> 01:00:47.600]   [APPLAUSE]
[01:00:47.600 --> 01:00:51.280]   And we are reconvening in 10 minutes.

