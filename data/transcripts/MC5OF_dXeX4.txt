
[00:00:00.000 --> 00:00:14.160]   echo. See you to take a second. Incredible. I can hear the echo
[00:00:14.160 --> 00:00:17.460]   which means we live on YouTube. Hey, everybody. Thanks to people
[00:00:17.460 --> 00:00:19.740]   who are in the zoom call. Thank you everyone for joining the
[00:00:19.740 --> 00:00:25.400]   YouTube live stream as well. This is my first time hosting
[00:00:25.400 --> 00:00:28.220]   the paper reading group. I've been doing a few community
[00:00:28.220 --> 00:00:32.120]   things for bits and bytes for a bit. But I'm excited to host my
[00:00:32.120 --> 00:00:35.400]   first paper reading group and I was talking to a few people in
[00:00:35.400 --> 00:00:38.000]   the zoom call. It's their first paper reading group as well,
[00:00:38.000 --> 00:00:41.200]   which means I'm allowed to mess up since they don't have a
[00:00:41.200 --> 00:00:44.780]   higher bar from earlier from my colleagues who've been hosting
[00:00:44.780 --> 00:00:48.860]   this. Awesome. Let's get started. Let me share my screen
[00:00:48.860 --> 00:00:55.280]   and tell everyone what I'm planning to cover. So my agenda
[00:00:55.280 --> 00:00:59.960]   for today is to cover the classic it's classic because it
[00:00:59.960 --> 00:01:03.880]   came out in 2015 and has been quite influential. Don't ask me
[00:01:03.880 --> 00:01:08.880]   for it. But I want to cover the unit CNNs for biomedical image
[00:01:08.880 --> 00:01:13.520]   segmentation paper that came out in 2015. At bits and biases,
[00:01:13.520 --> 00:01:16.160]   we've been hosting the paper reading group for quite a time
[00:01:16.160 --> 00:01:20.520]   and our sole goal with this series is basically to empower
[00:01:20.520 --> 00:01:24.800]   you, you to be able to read papers to be able to implement
[00:01:24.800 --> 00:01:27.800]   papers, which is as I'm sure everyone would have come to
[00:01:27.800 --> 00:01:31.800]   understand a core part of all of our jobs working in machine
[00:01:31.800 --> 00:01:36.040]   learning or data science. This is one of the image
[00:01:36.040 --> 00:01:39.040]   segmentation papers, one of the classic image segmentation
[00:01:39.040 --> 00:01:43.320]   papers that quite revolutionized this architecture called unit
[00:01:43.320 --> 00:01:48.400]   which has affected since then a lot of things. And here are the
[00:01:48.400 --> 00:01:52.280]   resources that I have studied basically, and I'll be stealing
[00:01:52.280 --> 00:01:56.760]   some stuff from I'll be giving credit of course, to cover the
[00:01:56.760 --> 00:02:02.320]   unit paper. My goal today is to cover the paper give you enough
[00:02:02.320 --> 00:02:07.000]   context to first of all, if you if you need a recap on CNNs and
[00:02:07.000 --> 00:02:09.280]   all of the architectural details, it quickly glance
[00:02:09.280 --> 00:02:14.000]   without, then we'll try to read the paper today and cover its
[00:02:14.000 --> 00:02:17.480]   implementation in pytorch. After that, I'll try to cover a few
[00:02:17.480 --> 00:02:20.840]   Kaggle competitions or at least leave you with some homework to
[00:02:21.520 --> 00:02:27.120]   work on from here. Let me make sure everyone's muted in the
[00:02:27.120 --> 00:02:40.080]   zoom call. Awesome. So one quick note to everyone on the YouTube
[00:02:40.080 --> 00:02:43.000]   live stream and in the zoom chat, if you could head over to
[00:02:43.000 --> 00:02:46.960]   this link, I'm sorry, I should stop sharing my screen. One
[00:02:46.960 --> 00:02:55.680]   second. So if you'll head over to this link, it should take you
[00:02:55.680 --> 00:02:58.760]   to our community where you can sign up and ask questions. I
[00:02:58.760 --> 00:03:01.600]   have one request if you all could please keep the
[00:03:01.600 --> 00:03:04.320]   discussion on here instead of the YouTube chat and the zoom
[00:03:04.320 --> 00:03:07.560]   chat since it's two different locations, difficult for me to
[00:03:07.560 --> 00:03:11.720]   keep an eye out if you're new to discourse. It's a
[00:03:11.720 --> 00:03:14.480]   straightforward interface. If you want to ask any questions,
[00:03:14.800 --> 00:03:26.800]   you can simply ask them here. And other folks can like your
[00:03:26.800 --> 00:03:30.680]   post or reply to it. So please post the questions and
[00:03:30.680 --> 00:03:35.320]   discussions here. So I first of all want to start with the
[00:03:35.320 --> 00:03:39.920]   question that I've literally posted on here. When do we use
[00:03:39.920 --> 00:03:44.640]   segmentation and just to recap what exactly segmentation is, if
[00:03:44.680 --> 00:03:47.280]   anyone in the zoom call would like to unmute themselves or if
[00:03:47.280 --> 00:03:51.400]   anyone would like to help me out here. The question is what is a
[00:03:51.400 --> 00:03:53.400]   mid segmentation and why is it useful?
[00:03:53.400 --> 00:04:05.760]   Well, I can go. So image segmentation is basically the
[00:04:05.760 --> 00:04:11.000]   target being that given an image without any depth information,
[00:04:11.280 --> 00:04:16.040]   is there a possibility that we can sort of group out objects
[00:04:16.040 --> 00:04:20.240]   that we feel that we know, because we are humans and we
[00:04:20.240 --> 00:04:22.760]   sort of understand what objects are and we sort of have much
[00:04:22.760 --> 00:04:26.320]   more information. But can we do the same thing based on images,
[00:04:26.320 --> 00:04:30.920]   just images that we can group out or segment out parts of the
[00:04:30.920 --> 00:04:32.760]   image that relate to the same thing?
[00:04:32.760 --> 00:04:37.200]   That's, that's a great answer. Thanks for that. When you say
[00:04:37.200 --> 00:04:39.720]   depth, could you elaborate on what do you mean by depth?
[00:04:40.680 --> 00:04:46.000]   Depth as in, so an image would get you 2D information that is
[00:04:46.000 --> 00:04:50.880]   all of those. The thing that you view, if it's from a single
[00:04:50.880 --> 00:04:54.120]   camera perspective, you basically end up with a 2D
[00:04:54.120 --> 00:04:58.440]   image, where this person here that has been segmented as blue,
[00:04:58.440 --> 00:05:03.320]   and this tree here that is in green, to a computer, both of
[00:05:03.320 --> 00:05:06.920]   these are in the same sort of space, because it does not know
[00:05:06.920 --> 00:05:09.960]   that the true is a tree is sort of behind the person, so to
[00:05:09.960 --> 00:05:16.320]   speak. And there are ways to I think, infer depth using depth
[00:05:16.320 --> 00:05:19.280]   sensors, I think infrared is one of those sensors, but I might be
[00:05:19.280 --> 00:05:24.240]   wrong. But there are other depth sensors, and we sort of can get
[00:05:24.240 --> 00:05:29.240]   depth information to get better segmentation. But that is what I
[00:05:29.240 --> 00:05:29.920]   mean by depth.
[00:05:29.920 --> 00:05:34.640]   Thanks. Thanks, Aurob. That's that's a spot on answer. So just
[00:05:34.640 --> 00:05:40.160]   to add to that. There are different categories of image
[00:05:40.160 --> 00:05:44.880]   tasks that are usually aimed for in people's image segmentation
[00:05:44.880 --> 00:05:48.680]   is one of them, sorry. Image classification is the classical
[00:05:48.680 --> 00:05:54.600]   hot dog or not hot dog, dogs and cats example. These one that
[00:05:54.600 --> 00:05:57.120]   comes from there is localization. So let's say you
[00:05:57.120 --> 00:06:00.640]   want to tell what's in an image like I want to point out where
[00:06:00.640 --> 00:06:04.160]   all of the humans are that will localization so I can maybe
[00:06:04.480 --> 00:06:07.840]   founding books. Thank you. Hello, or those people if not
[00:06:07.840 --> 00:06:09.920]   familiar with them, that's totally fine. Just think of the
[00:06:09.920 --> 00:06:14.720]   bounding boxes that you've seen a lot of animations that are
[00:06:14.720 --> 00:06:18.920]   thrown around on Facebook. That is localization. The next step
[00:06:18.920 --> 00:06:21.720]   from there is image segmentation. And like sort of
[00:06:21.720 --> 00:06:26.560]   just pointed out, whenever you want to understand every single
[00:06:26.560 --> 00:06:30.360]   pixel inside of an image, and assuming you don't have the
[00:06:30.360 --> 00:06:34.000]   depth knowledge, you don't have a depth mapping. And like sort of
[00:06:34.000 --> 00:06:37.880]   said, with 3d cars, you usually have LiDAR sensors, which are
[00:06:37.880 --> 00:06:42.960]   light image and detection, or you could have IR sensors,
[00:06:42.960 --> 00:06:46.520]   infrared sensors, which give you depth information. So you in
[00:06:46.520 --> 00:06:50.920]   your arrays stored in memory could have numbers that would
[00:06:50.920 --> 00:06:54.680]   tell you how far away is an image like right now you can
[00:06:54.680 --> 00:06:58.280]   sort of tell because I have a high fidelity camera that I'm
[00:06:58.280 --> 00:07:01.680]   slightly away from the screen and my hand is now closer,
[00:07:02.080 --> 00:07:04.640]   because it gets larger in size, but to compute is slightly
[00:07:04.640 --> 00:07:08.160]   harder to tell those things. So image segmentation is taking
[00:07:08.160 --> 00:07:13.760]   every single pixel and putting a label on it. And from there,
[00:07:13.760 --> 00:07:16.480]   you can derive different knowledge. So it's also one of
[00:07:16.480 --> 00:07:19.880]   the challenging, not the most challenging, but one of the most
[00:07:19.880 --> 00:07:27.920]   challenging problems, image tasks. So let's start with some
[00:07:27.920 --> 00:07:31.880]   background concepts. I want to give a quick brush up on all of
[00:07:31.880 --> 00:07:34.120]   these concepts. If you're familiar with these, I'll keep
[00:07:34.120 --> 00:07:36.560]   it short. If you're not familiar with this, I'd love to help you
[00:07:36.560 --> 00:07:39.560]   get up to speed. Let me start by talking about what are
[00:07:39.560 --> 00:07:43.080]   convolutions, what are max pooling layers, what is a
[00:07:43.080 --> 00:07:47.120]   transposed convolution, which was very key to this paper. And
[00:07:47.120 --> 00:07:49.960]   I'll quickly touch on what is cropping and data augmentation.
[00:07:49.960 --> 00:07:57.840]   Someone asked, where can they ask the questions, let me post
[00:07:57.840 --> 00:08:08.280]   the link real quick. All right, let me stop sharing my screen
[00:08:08.280 --> 00:08:11.080]   and share the correct one. By the way, as a reminder to
[00:08:11.080 --> 00:08:13.480]   everyone in the zoom call, please feel free to interrupt me
[00:08:13.480 --> 00:08:17.800]   anytime since you're all in the call literally, please unmute
[00:08:17.800 --> 00:08:22.960]   and interrupt anytime. So I'm going to use this Excel sheet
[00:08:22.960 --> 00:08:26.360]   that came out of a fast.ai lecture. It's one of the best
[00:08:26.400 --> 00:08:29.920]   deep learning courses in my opinion. And this is on a toy
[00:08:29.920 --> 00:08:33.280]   data set, at least in today's world called MNIST, which has
[00:08:33.280 --> 00:08:38.120]   numbers from zero to 10, zero to nine, sorry, 10 is one and
[00:08:38.120 --> 00:08:42.080]   zero. And this is just to demonstrate what a convolution
[00:08:42.080 --> 00:08:46.800]   operator is and what a max pool layer is. So here as all of you
[00:08:46.800 --> 00:08:50.880]   can clearly see this is a number seven looks like more of a bird
[00:08:50.880 --> 00:08:54.840]   or the Swift programming language logo. But that's that's
[00:08:54.840 --> 00:08:58.400]   the problem with humans. It's a seven if you look at the label,
[00:08:58.400 --> 00:09:02.000]   and that's where problems arise for machine learning models as
[00:09:02.000 --> 00:09:07.600]   well. The key thing to remember all of these numbers that we see
[00:09:07.600 --> 00:09:10.040]   these are black and white numbers, which means it's just
[00:09:10.040 --> 00:09:14.680]   one layer, or just arrays stored in a computer's memory, right?
[00:09:14.680 --> 00:09:19.080]   So wherever you have some value coming up, some shade coming up,
[00:09:19.080 --> 00:09:23.400]   you'd probably have 1.0, which means this is black, or in this
[00:09:23.400 --> 00:09:28.400]   case, a red pixel. And wherever it's nothing is written in
[00:09:28.400 --> 00:09:31.760]   memory, it has zero number on there. So this is our input
[00:09:31.760 --> 00:09:35.240]   image. And our task is to may be able to predict it. Let's not
[00:09:35.240 --> 00:09:39.120]   bother about it. For now, we'll take a filter, we'll actually
[00:09:39.120 --> 00:09:43.000]   take two of them and apply the convolution operator on it,
[00:09:43.000 --> 00:09:50.120]   which gives us this output table as con_fun. Now, how do we get
[00:09:50.120 --> 00:09:53.480]   these numbers in the con_fun layer? Let's take a look at this
[00:09:53.480 --> 00:09:56.440]   formula. And here's the nice thing about Excel, it tells me
[00:09:56.440 --> 00:09:59.560]   what exactly what's going on. So what we've done is we've
[00:09:59.560 --> 00:10:04.200]   created this filter that operates upon all of these
[00:10:04.200 --> 00:10:07.120]   numbers, and quite unsurprisingly should give us a
[00:10:07.120 --> 00:10:10.600]   zero because it's everything being multiplied by a zero.
[00:10:10.600 --> 00:10:15.880]   Let's see how do we get this four. So as a result of
[00:10:16.960 --> 00:10:21.920]   convolving, that's the term here, this filter over these
[00:10:21.920 --> 00:10:26.440]   numbers, we get this output. And we do this set of
[00:10:26.440 --> 00:10:30.400]   multiplication over the entire set of input image pixels to
[00:10:30.400 --> 00:10:34.000]   get this output. And we do this twice, because we have two
[00:10:34.000 --> 00:10:38.760]   filters. So as you can see, with just one convolution, we've
[00:10:38.760 --> 00:10:42.120]   increased the depth. This is a key thing to remember for unit
[00:10:42.120 --> 00:10:46.880]   and a challenge as well that we'll just get around to. From
[00:10:46.880 --> 00:10:50.160]   there, you can do more convolutions, which will keep
[00:10:50.160 --> 00:10:54.880]   giving you more outputs. Key thing to note here is as you
[00:10:54.880 --> 00:10:58.640]   progress inside of this layer, now you have two filters being
[00:10:58.640 --> 00:11:06.040]   applied on this. This single number has a receptive area of
[00:11:06.040 --> 00:11:10.520]   these numbers, which further are derived from let's say
[00:11:10.520 --> 00:11:14.160]   somewhere around this region. So the key point to remember
[00:11:14.160 --> 00:11:19.000]   here is as you go inside a network as you go deeper into
[00:11:19.000 --> 00:11:23.440]   convolutional neural network, one single pixel, that's this
[00:11:23.440 --> 00:11:26.080]   number would probably be corresponding to a lot of these.
[00:11:26.080 --> 00:11:30.800]   Now, as you'll see later, I'm giving a spoiler here with image
[00:11:30.800 --> 00:11:33.960]   segmentation. This is counterintuitive, right? Because
[00:11:33.960 --> 00:11:41.360]   if I stop sharing and reshare the correct screen again, what
[00:11:41.360 --> 00:11:44.360]   we want for image segmentation is the ability to be able to
[00:11:44.360 --> 00:11:48.320]   label every pixel for that you need a number for every pixel,
[00:11:48.320 --> 00:11:53.680]   a class label, let's say ground is class zero and the
[00:11:53.680 --> 00:11:57.120]   predictions, right? Now that's, that's hard to do with
[00:11:57.120 --> 00:12:01.840]   convolution neural networks as the output shrinks. So we look
[00:12:01.840 --> 00:12:05.040]   at how that is tackled, but that's one key thing to remember
[00:12:05.040 --> 00:12:11.280]   for now. So this is how we apply convolutions. Another operator
[00:12:11.280 --> 00:12:15.240]   I want to do a quick reminder of is max pool. So how do we get
[00:12:15.240 --> 00:12:19.080]   this number? That simple, we take the maximum of these four.
[00:12:19.080 --> 00:12:24.720]   So this is a max pool of two, which gives us this one output
[00:12:24.720 --> 00:12:27.920]   instead of these four numbers. The key thing to remember this
[00:12:27.920 --> 00:12:31.040]   is being written in an expanded fashion. But as you can see,
[00:12:31.040 --> 00:12:34.600]   this matrix is probably shrunk down a bit from the original
[00:12:34.600 --> 00:12:40.320]   one. So as you apply max pooling layers, the output size shrinks
[00:12:40.800 --> 00:12:45.240]   a bit. Let me look at another number. So how do we get this
[00:12:45.240 --> 00:12:50.960]   25? We take the maximum of these four numbers and just replace
[00:12:50.960 --> 00:12:56.120]   this pixel from that. So 25 looks like the maximum here. So
[00:12:56.120 --> 00:13:00.000]   we should be getting 25. Yes, we do. That is max pooling layer.
[00:13:00.000 --> 00:13:06.120]   Um, I'll probably skip the later details because those aren't
[00:13:06.120 --> 00:13:09.400]   important, but I quickly wanted to give a quick reminder on
[00:13:09.400 --> 00:13:12.320]   convolutions and max pooling. Any questions about this if
[00:13:12.320 --> 00:13:19.440]   anyone is new to it? Awesome. I don't see anything on the
[00:13:19.440 --> 00:13:37.600]   forums. I should probably share one note at this point. So now
[00:13:37.600 --> 00:13:42.880]   I want to cover another concept. I'm covering these topics with
[00:13:42.880 --> 00:13:47.400]   the goal in mind to get everyone up to speed with the background
[00:13:47.400 --> 00:13:52.120]   concepts required to understand. So now I want to cover up
[00:13:52.120 --> 00:13:56.520]   sampling, which I'm going to derive from a blog post called
[00:13:56.520 --> 00:13:59.640]   up sampling with transpose convolution. This is by Naoki.
[00:13:59.640 --> 00:14:04.080]   I've also linked this in the forum thread. But up sampling
[00:14:05.000 --> 00:14:09.840]   refers to, um, fractionally started convolutions or
[00:14:09.840 --> 00:14:13.800]   deconvolutions as the same thing. Someone's asking where
[00:14:13.800 --> 00:14:16.080]   can they find the Excel sheet? If you head over to the forum
[00:14:16.080 --> 00:14:19.360]   link that I've shared, I posted the entire resource list there.
[00:14:19.360 --> 00:14:27.120]   Please, please find it there. In case I haven't linked in there,
[00:14:27.120 --> 00:14:32.560]   please let me know. Awesome. So coming back to my one note.
[00:14:32.560 --> 00:14:40.960]   Now, we've just looked at convolutional operator. And
[00:14:40.960 --> 00:14:49.720]   let's say this input is a four by four array on which we apply
[00:14:49.720 --> 00:14:57.360]   a three by three kernel. And you would maybe get an output of two
[00:14:57.360 --> 00:15:02.840]   by two. Other problem here, as I alluded to earlier is for image
[00:15:02.840 --> 00:15:07.160]   segmentation, you need a label for every single pixel. But even
[00:15:07.160 --> 00:15:10.840]   if we get an output from this, let's say this is our input,
[00:15:10.840 --> 00:15:15.720]   these do not correspond. So the key thing here to understand is
[00:15:15.720 --> 00:15:22.800]   we need a way to be able to up sample upscale this. And for
[00:15:22.800 --> 00:15:24.960]   that, let's look at deconvolution. So it's the
[00:15:25.520 --> 00:15:31.760]   somewhat opposite of convolutions. And the blog post
[00:15:31.760 --> 00:15:34.760]   I've referred to for this actually shows you how it works.
[00:15:34.760 --> 00:15:38.680]   I'll just give the gist here to keep it brief. But let's say you
[00:15:38.680 --> 00:15:52.160]   have a three by three kernel. This can be rearranged into a
[00:15:52.160 --> 00:15:56.760]   four by 16 kernel. I'm just literally quoting the example I
[00:15:56.760 --> 00:16:00.560]   read in the blog post. I'm just trying to explain it. And what
[00:16:00.560 --> 00:16:04.160]   we end up doing here, let's say this is row one, this is row two,
[00:16:04.160 --> 00:16:08.840]   this is row three. We put row one here, zero under it, zero
[00:16:08.840 --> 00:16:22.440]   under it. We put row two here. Put a zero here. We put again,
[00:16:22.440 --> 00:16:27.440]   I believe row two. So like so we fill the same array expanded
[00:16:27.440 --> 00:16:32.680]   into this four by 16 convolution kernel. We've just taken this
[00:16:32.680 --> 00:16:36.440]   kernel and resize it to four by 16 in a way by filling it with
[00:16:36.440 --> 00:16:41.520]   zeros. In the blog post, they show you how performing the same
[00:16:41.520 --> 00:16:44.280]   convolution again that I referred to earlier with the four
[00:16:44.280 --> 00:16:50.200]   by four input would give you the same output as just multiplying
[00:16:50.200 --> 00:16:54.520]   this layer. So earlier, we were taking this four by four, let's
[00:16:54.520 --> 00:16:58.160]   say image. I mean, it's a small image, but let's, let's work
[00:16:58.160 --> 00:17:03.160]   with that. You perform an kernel operator or convolution on top
[00:17:03.160 --> 00:17:07.280]   of it, and you get a two by two output. Let's say if you do the
[00:17:07.280 --> 00:17:10.160]   math and if you actually check it out, if you take this
[00:17:10.160 --> 00:17:16.680]   resized four by 16 kernel that came out of this three by three
[00:17:16.680 --> 00:17:27.160]   kernel, and if you just resize the original four by four input
[00:17:28.280 --> 00:17:33.720]   into a 16 by one input, you get an output of four by one, which
[00:17:33.720 --> 00:17:38.240]   can be resized to two by two. And if you do the math and
[00:17:38.240 --> 00:17:41.600]   perform the convolutions and actually check the numbers,
[00:17:41.600 --> 00:17:44.400]   they should be the exact same thing. So the point being
[00:17:44.400 --> 00:17:49.320]   resizing the kernel like so doesn't change a lot of things.
[00:17:55.440 --> 00:17:59.080]   Now, D convolutions are also known as transpose convolutions
[00:17:59.080 --> 00:18:02.720]   for a reason. So let's say this is a convolutional layer.
[00:18:02.720 --> 00:18:06.840]   Actually it's a kernel, but let's call it C. We can perform
[00:18:06.840 --> 00:18:13.840]   a transpose of C and multiply it with, remember the four by
[00:18:13.840 --> 00:18:21.320]   four layer from earlier. Let's say we take the two by two layer,
[00:18:21.560 --> 00:18:27.200]   which was coming as an output. So we take the output layer
[00:18:27.200 --> 00:18:34.800]   resized into a column vector. So this would be four by one.
[00:18:40.040 --> 00:18:52.680]   This gives us an output of 16 by one, which can be resized
[00:18:52.680 --> 00:18:58.680]   to four by four. So we take the transpose of this kernel.
[00:18:58.680 --> 00:19:02.880]   That is just a resize of our original three by three kernel.
[00:19:02.880 --> 00:19:10.020]   So we take this transpose multiplied by the output and
[00:19:10.020 --> 00:19:16.440]   we can derive the input. So we've here achieved a way using
[00:19:16.440 --> 00:19:19.800]   this operator known as D convolution or transpose
[00:19:19.800 --> 00:19:25.720]   convolution, which allows us to increase the size of the output.
[00:19:25.720 --> 00:19:29.840]   By definition, convolutions, which we just looked at at the
[00:19:29.840 --> 00:19:35.800]   Excel sheet, shrink the size of the output. The key problem
[00:19:35.800 --> 00:19:39.040]   there was to work with image segmentation. We need a bigger
[00:19:39.640 --> 00:19:44.680]   output or an output that matches with the input image size. So
[00:19:44.680 --> 00:19:48.200]   inside of the unit paper, we'll see where this is implemented.
[00:19:48.200 --> 00:19:52.880]   We perform D convolution or transpose convolutions to get a
[00:19:52.880 --> 00:19:57.520]   bigger output size that towards the end will correspond to the
[00:19:57.520 --> 00:20:03.360]   input image. Let me take a quick break and see if there are any
[00:20:03.360 --> 00:20:04.680]   questions on our forums.
[00:20:07.880 --> 00:20:09.760]   So how so I'm so sorry.
[00:20:09.760 --> 00:20:11.720]   Please go for it.
[00:20:11.720 --> 00:20:18.920]   How are we getting that four cross 16 thing from so I get
[00:20:18.920 --> 00:20:22.240]   that the three, the three cross three images are filter, right?
[00:20:22.240 --> 00:20:26.920]   And we are passing an image through. And could you just tell
[00:20:26.920 --> 00:20:31.520]   me how are we getting that four cross 16 block?
[00:20:31.520 --> 00:20:36.560]   Absolutely. Sure. So I am not teaching it myself. I've stolen
[00:20:36.560 --> 00:20:43.200]   it from the last link in this list. If you head over to that.
[00:20:43.200 --> 00:20:49.480]   Okay. Yep. I'll still give a walkthrough. So this was our
[00:20:49.480 --> 00:20:52.680]   original three by three kernel in their example, I've not taken
[00:20:52.680 --> 00:20:54.920]   the numbers because you can run through them and get the idea.
[00:20:54.920 --> 00:20:58.160]   But if you just resize them, like so if you rearrange the
[00:20:58.160 --> 00:21:00.760]   same three by three kernel by putting a bunch of zeros, you
[00:21:00.760 --> 00:21:02.440]   can get the four by 16 kernel.
[00:21:03.320 --> 00:21:08.520]   Okay, so this is a predefined kernel mapping, right? It's not
[00:21:08.520 --> 00:21:14.320]   that is right. Yes. Okay. And this is specifically being used
[00:21:14.320 --> 00:21:18.480]   just for the upsampling process, right? So we want it. Okay.
[00:21:18.480 --> 00:21:22.440]   Thank you. That's correct. Thank you. Thanks. Thanks for the
[00:21:22.440 --> 00:21:26.000]   great question. Just to point the topic out. It's called
[00:21:26.000 --> 00:21:28.560]   upsampling with transverse convolutions by now.
[00:21:31.000 --> 00:21:37.480]   So, please, please. Yeah. So my question is that like, the whole
[00:21:37.480 --> 00:21:41.280]   purpose of doing the upsampling is to get a four by four output,
[00:21:41.280 --> 00:21:45.440]   right? So, like, we can achieve the same thing through same
[00:21:45.440 --> 00:21:48.280]   padding as well, right in the convolution operators.
[00:21:48.280 --> 00:21:51.960]   Say, if we apply the same padding.
[00:21:51.960 --> 00:21:55.760]   Yes. So sorry for not being clear. I'm rushing through
[00:21:55.760 --> 00:21:57.960]   things because I just have an hour and I also want to read the
[00:21:57.960 --> 00:22:00.520]   paper and implementation. I'll give the MOOC, I'll give the
[00:22:00.520 --> 00:22:05.480]   complete context. The problem, let me stop sharing to focus on
[00:22:05.480 --> 00:22:08.640]   my face. So the problem with convolutions is the output
[00:22:08.640 --> 00:22:12.840]   shrinks. And for segmentation, we want an output that
[00:22:12.840 --> 00:22:17.480]   corresponds to the input pixels. If we increase the stride, yes,
[00:22:17.480 --> 00:22:21.320]   you could do that. But that wouldn't exactly correspond to
[00:22:21.320 --> 00:22:26.920]   the input images. Does that answer your question?
[00:22:28.480 --> 00:22:33.720]   Not exactly. I mean, usually, if you wanted to maintain the size,
[00:22:33.720 --> 00:22:36.520]   we usually do padding, right? Zero padding or something.
[00:22:36.520 --> 00:22:39.440]   Accordingly, like if you apply the same padding, then we can
[00:22:39.440 --> 00:22:43.360]   anyway get the convolution output same as the input size.
[00:22:43.360 --> 00:22:48.160]   So like, is there any particular reason why we prefer like, the
[00:22:48.160 --> 00:22:52.080]   transverse convolution over normal, say applying same
[00:22:52.080 --> 00:22:52.480]   padding?
[00:22:52.480 --> 00:22:58.400]   Someone's already mentioned this in the chat, so I'll just use
[00:22:58.440 --> 00:23:03.960]   it. It doesn't meaningfully expand the tensor. So when you
[00:23:03.960 --> 00:23:09.240]   pad it, you lose out on the meaning from the layers.
[00:23:09.240 --> 00:23:18.920]   This is that helpful? Yeah. Okay, please feel free to ask
[00:23:18.920 --> 00:23:21.640]   in the forums as well. If I wasn't too clear.
[00:23:21.640 --> 00:23:28.400]   So in a sense, like padding will not upscale the image, right?
[00:23:28.680 --> 00:23:32.320]   It will pad stuff to it with random zero values or something,
[00:23:32.320 --> 00:23:35.400]   but it will not effectively upscale the image, which we are
[00:23:35.400 --> 00:23:36.960]   actually I think, looking for.
[00:23:36.960 --> 00:23:46.160]   Exactly. Right. There's a lot more detail, I think, in the DC
[00:23:46.160 --> 00:23:49.000]   GAN paper, where they showed you how to upscale images
[00:23:49.000 --> 00:23:52.920]   effectively. But there, they literally cover this in a lot of
[00:23:52.920 --> 00:23:58.080]   depth if you're curious. So the DC GAN paper is the one to look
[00:23:58.080 --> 00:24:01.520]   for. Okay, sure. I will go through.
[00:24:01.520 --> 00:24:07.720]   Yeah, sorry about, you know, rushing through things. But I
[00:24:07.720 --> 00:24:11.800]   try to pack a lot of knowledge because we have, you know, folks
[00:24:11.800 --> 00:24:15.840]   who are very advanced. I know I've seen some names in the zoom
[00:24:15.840 --> 00:24:18.280]   meeting who probably I think don't need to be in this call,
[00:24:18.280 --> 00:24:21.360]   but they're joining us as well. And people who don't know of
[00:24:21.360 --> 00:24:24.120]   convolutions might be joining as well. So I'm trying to cover all
[00:24:24.120 --> 00:24:27.000]   things here. Which is why you might feel I'm rushing through
[00:24:27.000 --> 00:24:30.320]   things. If you do, please feel free to interrupt anytime.
[00:24:30.320 --> 00:24:38.120]   Awesome. So now we've covered all of the topics that I needed
[00:24:38.120 --> 00:24:41.880]   to to start reading the paper. And once I figured out where it
[00:24:41.880 --> 00:24:46.200]   is in my 50 browser windows, I'll share the screen and get
[00:24:46.200 --> 00:24:51.280]   started with reading the paper. So in the next 15 minutes or so
[00:24:51.280 --> 00:24:54.160]   I'll try to cover the paper point out different things here.
[00:24:54.200 --> 00:24:57.160]   And after that, we'll try to cover its implementation by
[00:24:57.160 --> 00:25:01.920]   touch. Let me move the zoom things out of the way. And
[00:25:01.920 --> 00:25:04.200]   again, as a reminder to everybody in the call, please
[00:25:04.200 --> 00:25:11.280]   feel free to unmute and interrupt anytime. So this
[00:25:11.280 --> 00:25:14.960]   paper, as a reminder came out in 2015, which is where you'll
[00:25:14.960 --> 00:25:18.880]   find some things that may or may not be to introduce it. It
[00:25:18.880 --> 00:25:21.600]   starts with there's a large concern that successful training
[00:25:21.600 --> 00:25:27.640]   of deep networks acquired 1000s many 1000s of annotated training
[00:25:27.640 --> 00:25:32.520]   samples. I should probably not try to underline things. Sorry
[00:25:32.520 --> 00:25:37.280]   about that. So the authors present claim to present a
[00:25:37.280 --> 00:25:42.560]   network and strategy that relies on strong augmentation to be
[00:25:42.560 --> 00:25:47.240]   able to work more efficiently with samples. Now this is
[00:25:47.240 --> 00:25:50.760]   another thing that I want to point out many times papers are
[00:25:50.880 --> 00:25:54.720]   quite regional to conferences. So for this paper, they were
[00:25:54.720 --> 00:25:58.000]   aiming for a particular challenge. And even though in
[00:25:58.000 --> 00:26:00.640]   the main paragraph, they mentioned an emphasis on
[00:26:00.640 --> 00:26:04.520]   augmentations, we now have a lot more advanced augmentation. But
[00:26:04.520 --> 00:26:07.800]   still, the architecture is the one I think is of key importance
[00:26:07.800 --> 00:26:11.800]   here. So the architecture in the intro, as the authors point out
[00:26:11.800 --> 00:26:18.080]   consists of a contracting path. Think of this as your standard
[00:26:18.080 --> 00:26:21.960]   CNN. So in your head, just replace this with any resonant
[00:26:21.960 --> 00:26:28.640]   and we look at the architecture image in a second. The key thing
[00:26:28.640 --> 00:26:32.000]   after this, which I think this paper was the first to
[00:26:32.000 --> 00:26:37.360]   introduce is the symmetric expanding path that enables
[00:26:37.360 --> 00:26:42.480]   precise localization. Now, this was much better than the
[00:26:42.480 --> 00:26:46.000]   sliding window convolution approach that was being used at
[00:26:46.000 --> 00:26:49.680]   the time and there was another key reason to making the
[00:26:49.680 --> 00:26:55.040]   architecture work. And I'll so how I'll try to read the paper,
[00:26:55.040 --> 00:26:57.400]   I don't want to read every single line, I'll just be
[00:26:57.400 --> 00:27:00.200]   pointing out the key details. I've read this already for a
[00:27:00.200 --> 00:27:04.440]   group. And this is just to mention to everybody, if you'll
[00:27:04.440 --> 00:27:08.480]   be reading this paper later on, hopefully, please come back and
[00:27:08.480 --> 00:27:15.960]   ask any questions that you want. So in the introduction, the
[00:27:15.960 --> 00:27:20.680]   authors point out two challenges. The first one is
[00:27:20.680 --> 00:27:28.240]   that the CNN approaches the output is a single class label.
[00:27:28.240 --> 00:27:33.840]   But with biomedical images, it should include more
[00:27:33.840 --> 00:27:44.040]   localization. So more think pixel level annotations. And the
[00:27:44.040 --> 00:27:48.800]   second one, which I've already pointed out is the output size.
[00:27:48.800 --> 00:27:54.120]   Now at that time, this paper had trained a network with a
[00:27:54.120 --> 00:27:57.960]   sliding window approach that somewhat worked with this, but
[00:27:57.960 --> 00:28:02.760]   it had two limitations, there were a lot of patches. And these
[00:28:02.760 --> 00:28:05.440]   are usually more than the number of images. So it was quite
[00:28:05.440 --> 00:28:10.000]   intensive, quite compute intensive to train this model.
[00:28:11.440 --> 00:28:15.400]   And it would require a lot of max pooling layers. So this
[00:28:15.400 --> 00:28:19.040]   older approach of using sliding windows, which I'm just
[00:28:19.040 --> 00:28:21.480]   glancing over here. So if you've not heard of it, you won't
[00:28:21.480 --> 00:28:24.000]   understand this, you can hit the paper if you want, it's not
[00:28:24.000 --> 00:28:27.880]   necessary. But this paper required you to use a lot of
[00:28:27.880 --> 00:28:30.160]   max pooling layers. And generally, when you do that,
[00:28:30.160 --> 00:28:33.240]   when you increase them a lot, the accuracy starts to decrease.
[00:28:33.240 --> 00:28:38.040]   So these were the drawbacks of the original paper that this
[00:28:38.120 --> 00:28:44.160]   architecture tries to overcome. And they do this by using a
[00:28:44.160 --> 00:28:49.680]   fully connected network and up sampling the output. So let's
[00:28:49.680 --> 00:28:53.920]   look at the architectural image. This is the classic unit
[00:28:53.920 --> 00:28:59.600]   architecture, the key thing to make it work. Or there are a few
[00:28:59.600 --> 00:29:02.600]   key things, let me point out all of them. So think of this as an
[00:29:02.680 --> 00:29:10.560]   encoder. Think of this side as a decoder. So you have an encoder
[00:29:10.560 --> 00:29:13.640]   which shrinks the image size down, you have a decoder which
[00:29:13.640 --> 00:29:20.640]   increases the image size, it up samples, performs the up conf
[00:29:20.640 --> 00:29:26.960]   operator to give it the output segmentation map. And thirdly,
[00:29:26.960 --> 00:29:30.080]   this was quite key to making things work. We also have a
[00:29:30.200 --> 00:29:36.160]   crop, copy and crop, which copies over these feature maps
[00:29:36.160 --> 00:29:41.240]   over to the output. This I believe was introduced before
[00:29:41.240 --> 00:29:45.360]   resonates in this paper. And this was also key part of making
[00:29:45.360 --> 00:29:51.160]   resonates work. So this is the copy and crop. This arrow
[00:29:51.160 --> 00:29:55.120]   represents that. So inside of this architecture, you start
[00:29:55.120 --> 00:30:05.000]   with the 572 by 572 image. You apply a three by three conf and
[00:30:05.000 --> 00:30:10.080]   a value activation function on top of it, followed by which you
[00:30:10.080 --> 00:30:13.840]   apply a max pooling layer. You repeat this a bunch of times.
[00:30:13.840 --> 00:30:19.400]   And then you start the up conf or the up convolutional
[00:30:19.400 --> 00:30:25.000]   operator D convolution, which resizes the image. And again,
[00:30:25.000 --> 00:30:27.280]   I want to point another key thing out, you don't get the
[00:30:27.280 --> 00:30:33.880]   exact same sizes as the input, but it gets quite close. So to
[00:30:33.880 --> 00:30:39.520]   recap, the key things introduced in this architecture was an
[00:30:39.520 --> 00:30:46.000]   encoder layer, a decoder layer inside of the encoder layer. If
[00:30:46.000 --> 00:30:49.520]   you just forget that there, there's even a decoder layer,
[00:30:49.520 --> 00:30:53.560]   this looks just like a fully connected, not a fully
[00:30:53.560 --> 00:30:57.360]   connected, sorry, up convolutional network. Right? So
[00:30:57.360 --> 00:31:01.560]   think any ResNet or think any think AlexNet for that matter.
[00:31:01.560 --> 00:31:05.200]   You have a bunch of convolutions being applied, followed by max
[00:31:05.200 --> 00:31:09.160]   pooling, followed by a value. Repeat this a bunch of times.
[00:31:09.160 --> 00:31:20.920]   You get a 1024 by I think it's 28 by 28 output size. So this
[00:31:20.920 --> 00:31:25.000]   was already existent at that point. The key difference was
[00:31:25.000 --> 00:31:29.600]   having this D convolution layers, along with the copy and
[00:31:29.600 --> 00:31:34.200]   crop approach, which takes this feature maps, crops into it, and
[00:31:34.200 --> 00:31:42.000]   copies it across. Why do we crop into it? If you look closely,
[00:31:42.000 --> 00:31:45.080]   the sizes are slightly different. So we need to account for that.
[00:31:45.640 --> 00:31:52.240]   So what happens here is let's say you have a human for some
[00:31:52.240 --> 00:31:56.600]   reason standing in this image. The authors just perform a
[00:31:56.600 --> 00:32:01.240]   center crop. So center crop is just cropping literally into the
[00:32:01.240 --> 00:32:05.840]   center of the image by leaving some pixels. Now you wouldn't
[00:32:05.840 --> 00:32:08.680]   have that in the middle of the convolutional neural network. But
[00:32:08.680 --> 00:32:16.120]   as you you have this image in this layer, you crop into it and
[00:32:16.120 --> 00:32:20.080]   copy it over to the output layer. This was one of the key
[00:32:20.080 --> 00:32:21.200]   things to making this work.
[00:32:21.200 --> 00:32:27.400]   I had a question.
[00:32:27.400 --> 00:32:30.360]   Yes, you are. Please go for it.
[00:32:30.360 --> 00:32:36.080]   I see that there are two convolutions applied
[00:32:37.040 --> 00:32:40.920]   simultaneously, right in the very first like input image as
[00:32:40.920 --> 00:32:47.560]   in the original input size is 572 cross 572. And then the
[00:32:47.560 --> 00:32:50.200]   and then the authors have applied like convolutional
[00:32:50.200 --> 00:32:55.440]   layers. I was going through the VGG paper, VGG 16 architecture
[00:32:55.440 --> 00:33:00.760]   and here they had applied like three convolutional layers. Is
[00:33:00.760 --> 00:33:04.440]   there any like rule of thumb while applying convolutions in
[00:33:04.440 --> 00:33:05.880]   our DNN?
[00:33:06.880 --> 00:33:12.440]   Um, now that you've mentioned it, I'm trying to think of it.
[00:33:12.440 --> 00:33:17.120]   But no, I think it just depends on the architecture. And I
[00:33:17.120 --> 00:33:20.040]   assume it's also derived through a lot of experimentation.
[00:33:20.040 --> 00:33:25.640]   Yeah. So yeah, even I couldn't like find this like any like
[00:33:25.640 --> 00:33:30.440]   rule of thumb while deciding the number of convolutions. I think
[00:33:30.440 --> 00:33:34.360]   it is just empirical evidence. I think so it might perform.
[00:33:34.400 --> 00:33:39.160]   I believe so. Yeah. Unless anyone else wants to chime in.
[00:33:39.160 --> 00:33:52.240]   Okay, that that stays as a fact for now. Yeah. I wanted to do
[00:33:52.240 --> 00:33:55.240]   side by side comparison, but not just to interrupt the flow.
[00:33:55.240 --> 00:34:00.000]   I'll probably come back to it. If you start thinking of this in
[00:34:00.000 --> 00:34:03.600]   code, right? At the end of the day, you know, reading papers is
[00:34:03.600 --> 00:34:06.360]   exciting. You can try to go through them after a while you
[00:34:06.360 --> 00:34:10.920]   start getting the hang of the academic language and it's
[00:34:10.920 --> 00:34:13.960]   really fulfilling to be able to understand the paper from end to
[00:34:13.960 --> 00:34:16.840]   end. But what you really should be doing in my opinion and
[00:34:16.840 --> 00:34:20.920]   what's really helped me. So it's my personal experience, please
[00:34:20.920 --> 00:34:24.160]   take it with a grain of salt. But whenever you're looking at a
[00:34:24.160 --> 00:34:29.520]   paper, start thinking of it. In PyTorch. I use PyTorch, you
[00:34:29.520 --> 00:34:32.840]   could use Keras. Start thinking of what operators would replace
[00:34:32.840 --> 00:34:37.520]   the architecture. You can't always think of it, but it's a
[00:34:37.520 --> 00:34:42.280]   good idea to start building a mental image. So I'm thinking
[00:34:42.280 --> 00:34:45.480]   this could be called 2D layers, right? Followed by a MaxBooler.
[00:34:45.480 --> 00:34:48.560]   And this could become a block in itself. So let's say this is a
[00:34:48.560 --> 00:34:53.960]   block, repeat this block a bunch of times, get the output. Oh, so
[00:34:53.960 --> 00:34:56.720]   that becomes the encoder, right? Let's think how we could define
[00:34:56.720 --> 00:35:01.120]   the decoder. Let me actually write this down. So inside of
[00:35:01.120 --> 00:35:10.920]   the encoder block, as Devin pointed out, we have two
[00:35:10.920 --> 00:35:16.520]   converse 2D, let's say operators being applied, followed by a
[00:35:16.520 --> 00:35:24.840]   MaxBooler. And maybe a ReLU as well after these. Sorry. And
[00:35:24.840 --> 00:35:27.680]   this is repeated, let's say X number of times. So that is our
[00:35:27.680 --> 00:35:31.240]   encoder. Literally, you can call it as many times in the
[00:35:31.240 --> 00:35:33.840]   unit architecture that have called it a bunch of times in
[00:35:33.840 --> 00:35:38.240]   different architectures, you can see the number changing. But in
[00:35:38.240 --> 00:35:40.680]   code, you could define it like so, right? This could be a
[00:35:40.680 --> 00:35:44.600]   function called encoder that has these layers, they could be in
[00:35:44.600 --> 00:35:47.440]   sequential, or as you'll see, you could put them in a module
[00:35:47.440 --> 00:35:51.960]   list. That becomes your encoder function. Let's think of the
[00:35:51.960 --> 00:35:56.600]   decoder function. So now, this would be quite tricky to
[00:35:56.600 --> 00:36:00.480]   implement. And luckily, we don't have to, I don't have to, I'm
[00:36:00.480 --> 00:36:04.600]   sure others in the call would be able to do it. But there is a
[00:36:04.600 --> 00:36:09.520]   deconv operator, or a transpose conv operator in PyTorch. So
[00:36:09.520 --> 00:36:15.920]   decoder layer ideally would consist of them. So I'll keep
[00:36:15.920 --> 00:36:18.960]   writing that. But just to get a mental image, that's how it
[00:36:18.960 --> 00:36:22.480]   would look in code. And we'll look at the code in a bit. But
[00:36:22.480 --> 00:36:24.800]   this is the architecture to summarize it really quickly.
[00:36:26.120 --> 00:36:29.840]   Let's continue reading the paper and try to cover any things.
[00:36:29.840 --> 00:36:37.680]   So, further in the paper, the authors mentioned one more thing
[00:36:37.680 --> 00:36:41.320]   which I want to point out one more important modification in
[00:36:41.320 --> 00:36:48.440]   our architecture is that in the upsampling part, so focus on the
[00:36:48.440 --> 00:36:53.040]   right part in your mental image. So in this decoder layer in this
[00:36:53.080 --> 00:37:00.920]   upsampling part, we also have a large number of features
[00:37:00.920 --> 00:37:04.160]   channels. So if you look at the numbers again, I'm sorry, I'm
[00:37:04.160 --> 00:37:06.680]   scrolling back and forth, but it's hard to cover both things.
[00:37:06.680 --> 00:37:11.040]   These numbers are really starting to increase. So these
[00:37:11.040 --> 00:37:15.600]   are the features channels. They're quite large. That's what
[00:37:15.600 --> 00:37:21.000]   the authors are pointing out. And this is useful because it
[00:37:21.080 --> 00:37:25.120]   allows the network to propagate context information to higher
[00:37:25.120 --> 00:37:28.440]   resolution layers. So to answer your question from earlier,
[00:37:28.440 --> 00:37:31.280]   Sanjay, that's why this is important. If you just use
[00:37:31.280 --> 00:37:34.280]   padding, you don't have the contextual information, you
[00:37:34.280 --> 00:37:38.440]   could get away with having the same output sizes. But it won't
[00:37:38.440 --> 00:37:42.480]   be the same as having the feature channels. Because you
[00:37:42.480 --> 00:37:47.440]   lose out on the context. And that's why I'm sure it would be
[00:37:47.440 --> 00:37:50.480]   quite obvious by now. But that's why it's called UNIT because
[00:37:50.480 --> 00:37:57.520]   it's a U shaped architecture. I'll glance over the other
[00:37:57.520 --> 00:38:03.320]   details because in 2021, these are aren't too important. You
[00:38:03.320 --> 00:38:06.840]   can get a much better GPU today on Colab than the authors had
[00:38:06.840 --> 00:38:11.800]   at the research labs. Today research labs have better GPUs.
[00:38:11.800 --> 00:38:19.880]   The authors also talk about data augmentation, which I mostly
[00:38:19.880 --> 00:38:22.400]   skip because we also have better approaches. I'll point out the
[00:38:22.400 --> 00:38:34.480]   approaches that you can use instead. I've already covered
[00:38:34.480 --> 00:38:40.640]   the network architecture. So scrolling past it. And again, as
[00:38:40.640 --> 00:38:44.320]   a reminder, I'm glancing over details just to cover as much
[00:38:44.320 --> 00:38:47.600]   breadth of knowledge happy to go into depth, please interrupt
[00:38:47.600 --> 00:38:53.000]   anytime. The training in the paper is done with a CD. So the
[00:38:53.000 --> 00:38:57.720]   authors also prefer having smaller batch sizes to favor
[00:38:57.720 --> 00:39:05.440]   larger input tiles. So they prefer a large input size over a
[00:39:05.440 --> 00:39:09.000]   large batch size. Actually, they mentioned further they had a
[00:39:09.000 --> 00:39:12.120]   batch size of fun because at the time they had a six gigabyte
[00:39:12.120 --> 00:39:16.360]   GPU. On Colab you can try this exact same paper and have a much
[00:39:16.360 --> 00:39:20.640]   bigger batch size. They also mentioned that they used a high
[00:39:20.640 --> 00:39:23.560]   momentum value. If you don't know what that is, please ask.
[00:39:23.560 --> 00:39:28.080]   Otherwise, I'll skip. And further, they mentioned that
[00:39:28.080 --> 00:39:37.680]   using a weighted loss was also quite important. I should
[00:39:37.680 --> 00:39:41.560]   probably skip over the data augmentations. And I'd also skip
[00:39:41.560 --> 00:39:45.800]   all the experiments bit because since then a lot more papers
[00:39:45.800 --> 00:39:49.520]   have come out and you have different segmentation
[00:39:49.520 --> 00:39:54.040]   challenges, which I'll also point out. But to summarize at
[00:39:54.040 --> 00:39:58.120]   that point in time, this architecture was quite different
[00:39:58.120 --> 00:40:02.080]   from the ones that existed. It was really good in the fact that
[00:40:02.080 --> 00:40:06.520]   it first of all introduced this new approach of a unit
[00:40:06.520 --> 00:40:10.200]   architecture that actually worked. I've gone through the
[00:40:10.200 --> 00:40:14.240]   reference chain and noticed similar architecture at that
[00:40:14.240 --> 00:40:19.560]   point. But there were three things that helped it. So
[00:40:19.560 --> 00:40:23.240]   encoder decoder had been introduced earlier as well. But
[00:40:23.240 --> 00:40:28.840]   having this copy and crop approach along with strong data
[00:40:28.840 --> 00:40:38.840]   augmentations was really helpful in making this work. Skipping
[00:40:38.840 --> 00:40:41.760]   over more details. I think that's all I wanted to cover in
[00:40:41.760 --> 00:40:45.160]   the paper. So I've read through the paper, but not really
[00:40:45.160 --> 00:40:47.800]   because I've just covered the key details, which is what is
[00:40:47.800 --> 00:40:52.640]   possible in a few minutes. I'll take a quick pause to see if
[00:40:52.640 --> 00:40:53.720]   there are any questions.
[00:40:53.720 --> 00:41:00.120]   I have a quick question.
[00:41:00.120 --> 00:41:01.360]   Please go for it.
[00:41:01.360 --> 00:41:06.920]   And so so when they train this architecture, they were using
[00:41:06.920 --> 00:41:11.680]   the input data as augmented or otherwise normal images and
[00:41:11.680 --> 00:41:19.760]   the target data was segmented out labeled images, right? Okay.
[00:41:19.760 --> 00:41:24.640]   Okay. And, and we are saying that a convolutional set of
[00:41:24.640 --> 00:41:28.440]   layers and a deconvolutional set of layers are able to learn
[00:41:28.440 --> 00:41:34.280]   features that would be able to segment new images based on the
[00:41:34.280 --> 00:41:36.720]   features that they learn and blah, blah, blah. Right. That
[00:41:36.720 --> 00:41:37.440]   is what we are saying.
[00:41:37.440 --> 00:41:41.640]   Assuming they're similar images, like you're training
[00:41:41.640 --> 00:41:47.480]   on. Yep. Right. And so you mentioned that nowadays, there
[00:41:47.480 --> 00:41:51.480]   are a lot of more other architectures that sort of do a
[00:41:51.480 --> 00:41:56.600]   better job. So what, again, I'm dumb. So what is the current
[00:41:56.600 --> 00:42:00.280]   state of the art in terms of image segmentation? Is it the
[00:42:00.280 --> 00:42:04.360]   is it that that that Facebook's what was it? Something on the
[00:42:04.360 --> 00:42:04.640]   other?
[00:42:04.640 --> 00:42:09.400]   That's not a dumb question. Sort of. That's a fantastic
[00:42:09.400 --> 00:42:11.960]   question. And I'm quite dumb as well, because I don't know what
[00:42:11.960 --> 00:42:16.360]   that is. I'm hosting the session. So let's let's check
[00:42:16.360 --> 00:42:26.600]   it out here. It's a I want to rename it segmentation. And this
[00:42:26.600 --> 00:42:30.040]   quickly brings up the best models corresponding to that.
[00:42:30.040 --> 00:42:34.800]   But you can just head over to papers with code browse the
[00:42:34.800 --> 00:42:37.480]   state of the art head over to semantic segmentation and just
[00:42:37.480 --> 00:42:40.600]   find the best people will do. Thank you so much.
[00:42:40.600 --> 00:42:48.720]   Hi, just a quick question. Normal autoencoders like this
[00:42:48.720 --> 00:42:52.520]   seem to have this kind of prop and copy something of that kind
[00:42:52.520 --> 00:42:56.160]   right now. This is given into the explanation if I have any
[00:42:56.160 --> 00:42:57.560]   good understanding about it.
[00:42:57.560 --> 00:43:01.320]   Intuitive explanation of sorry,
[00:43:01.320 --> 00:43:06.920]   as in like, why, from encoder, they were cropping and copying
[00:43:06.920 --> 00:43:08.520]   it to the decoder parts like
[00:43:08.520 --> 00:43:11.640]   sure, I can go with that again.
[00:43:11.640 --> 00:43:28.840]   So I claimed like, let's say in the second layer, which would
[00:43:28.840 --> 00:43:32.840]   never be the case, but let's say you have this image or this
[00:43:32.840 --> 00:43:37.080]   feature map lying inside of it, where you have this human. Now
[00:43:37.080 --> 00:43:41.600]   to give this context over to the output layer, because remember
[00:43:41.600 --> 00:43:44.360]   this, these are like much smaller numbers, we looked at
[00:43:44.360 --> 00:43:48.120]   the Excel sheet, which don't exactly correspond to it. And
[00:43:48.120 --> 00:43:52.880]   these are just loan numbers. Now to give the output some context
[00:43:52.880 --> 00:43:56.920]   of it, we crop into it because it's slightly smaller. So if you
[00:43:56.920 --> 00:43:59.280]   look at the numbers, if you zoom in enough, you can look at the
[00:43:59.280 --> 00:44:02.360]   numbers, you can glance over the details. But it's slightly so
[00:44:02.360 --> 00:44:05.360]   smaller. And for that reason, we'll crop into the image. So
[00:44:05.360 --> 00:44:10.000]   we'll crop into the green area of this image, which I'm
[00:44:10.000 --> 00:44:14.440]   claiming would probably be in the second layer. And we copy it
[00:44:14.440 --> 00:44:16.680]   over to the output and concatenated there.
[00:44:16.680 --> 00:44:19.120]   Is that helpful?
[00:44:19.640 --> 00:44:20.080]   All right.
[00:44:20.080 --> 00:44:37.240]   Thanks for pointing the link out. Sahil, that's the file I'm
[00:44:37.240 --> 00:44:42.160]   using, file I was using for convolutions. And, yep,
[00:44:42.160 --> 00:44:46.680]   Detectron is one of the, I think it's one of the best
[00:44:46.680 --> 00:44:51.360]   architecture, if not the best. Thanks, Yash on segmentation, I
[00:44:51.360 --> 00:44:56.280]   might be wrong. Awesome. So further from here, I wanted to
[00:44:56.280 --> 00:45:00.280]   cover the implementation. And I'm quite lazy. And my
[00:45:00.280 --> 00:45:03.080]   colleague has already done it in his blog post. So I'll just
[00:45:03.080 --> 00:45:06.720]   steal his material from here. But to point it out, it's
[00:45:06.720 --> 00:45:10.360]   called a pytorch implementation and 60 lines of code for unit
[00:45:11.320 --> 00:45:18.760]   also linked in this post, please find it there. And I've copied
[00:45:18.760 --> 00:45:25.360]   over the code from his article. Let me sorry, I'm just trying to
[00:45:25.360 --> 00:45:28.440]   rearrange my screen so that I can have these side by side.
[00:45:28.440 --> 00:45:40.080]   Let me bring this up to the left hand side. Stop sharing. So what
[00:45:40.080 --> 00:45:43.080]   I'm trying to achieve here is just trying to share both the
[00:45:43.080 --> 00:45:48.680]   screen side by side. For some reason, zoom would allow that.
[00:45:48.680 --> 00:46:00.920]   Sorry, I'm quickly trying to arrange everything here. Let me
[00:46:00.920 --> 00:46:08.520]   try that now. Can everybody see both of these side by side, the
[00:46:08.520 --> 00:46:20.160]   paper and the Jupyter Notebook? Is the Jupyter Notebook visible
[00:46:20.160 --> 00:46:26.720]   alongside the paper? Yes. Okay, thanks. So like I claimed
[00:46:26.720 --> 00:46:30.640]   earlier, we have this input and I'm sorry, it's not exactly
[00:46:30.640 --> 00:46:33.640]   overlapped. I could do a better job at it. Let me move this
[00:46:33.640 --> 00:46:40.480]   over a bit. As you can tell, it's quite live and I haven't
[00:46:40.480 --> 00:46:44.560]   practiced this thing exactly. But I want to point out the
[00:46:44.560 --> 00:46:47.680]   implementation by literally looking at the architecture on
[00:46:47.680 --> 00:46:50.760]   the side. So I'll keep focusing on left and right and speak it
[00:46:50.760 --> 00:46:54.320]   out as I do so. So the first thing we want to do is we want
[00:46:54.320 --> 00:46:58.880]   to implement a block. Now what would the block for encoder
[00:46:58.880 --> 00:47:02.520]   include? And we're trying to implement the encoder first. So
[00:47:02.520 --> 00:47:05.480]   for that we need to implement a block of operators that get
[00:47:05.480 --> 00:47:09.960]   repeated every time. Remember, whenever you're working on code,
[00:47:09.960 --> 00:47:12.880]   you just want to use or reuse anything, it's probably a good
[00:47:12.880 --> 00:47:16.640]   idea to define a function. So we're going to do that. Inside
[00:47:16.640 --> 00:47:23.120]   of the encoder. A blue arrow means a three by three
[00:47:23.120 --> 00:47:27.600]   convolution followed by a ReLU. And look closely, this is being
[00:47:27.600 --> 00:47:31.640]   done twice followed by a red arrow. What is the red arrow?
[00:47:31.640 --> 00:47:37.320]   It's a max pool of two by two. So in the block, you of course
[00:47:37.320 --> 00:47:41.520]   have to call the init function inside of which we can define
[00:47:41.520 --> 00:47:45.600]   the input channels, the output channels, and then call the
[00:47:45.600 --> 00:47:52.640]   super classes constructor. The kernel size will be three. So
[00:47:52.640 --> 00:47:59.840]   inside of this, we define a conf 2d followed by a ReLU followed
[00:47:59.840 --> 00:48:06.080]   by another conf 2d. And the kernel size remains three. So
[00:48:06.080 --> 00:48:09.040]   let's look at the function signature the in channels
[00:48:09.040 --> 00:48:18.360]   defines. I just press shift tab twice for anyone that's not
[00:48:18.360 --> 00:48:21.160]   familiar with this. And what I was trying to achieve here is I
[00:48:21.160 --> 00:48:25.680]   wanted to point out what two different arguments do. So the
[00:48:25.680 --> 00:48:28.640]   in channels takes the number of channels in the input image, the
[00:48:28.640 --> 00:48:31.200]   out channels is the number of channels provided by the
[00:48:31.200 --> 00:48:35.120]   convolution. Kernel size, which is the third argument that we
[00:48:35.120 --> 00:48:37.560]   passing is size of the convolving kernels. It's a three
[00:48:37.560 --> 00:48:40.320]   by three kernel in the original paper, which is what we're
[00:48:40.320 --> 00:48:45.000]   following here. So that's how we define our first conv layer
[00:48:45.000 --> 00:48:50.480]   followed by ReLU followed by conf 2 layer. Inside of our
[00:48:50.480 --> 00:48:54.440]   forward function, remember the red arrow, please look on the
[00:48:54.440 --> 00:48:58.160]   right hand part of the screen. So we have two cons followed by
[00:48:58.160 --> 00:49:06.760]   a ReLU. First one, second one, and then ReLU. So the forward
[00:49:06.760 --> 00:49:10.640]   function inside of this block. So we're trying to repeat these
[00:49:10.640 --> 00:49:14.600]   blocks, which is why we're defining the function. The
[00:49:14.600 --> 00:49:23.120]   forward function returns ReLU applied on top of these. And
[00:49:23.120 --> 00:49:30.080]   this becomes a block inside of our encoder. From there, we can
[00:49:30.080 --> 00:49:35.400]   define our encoder class, which inherits from an end dot module.
[00:49:35.400 --> 00:49:41.000]   And here we'll define all of these channels. These
[00:49:41.000 --> 00:49:44.120]   correspond to the input image sizes. So I'm not going to go
[00:49:44.120 --> 00:49:47.920]   with them. But from here, you'd like to define the encoder
[00:49:47.920 --> 00:49:53.200]   blocks. And we pass this into an end module list. What is an
[00:49:53.200 --> 00:49:56.360]   end module list? You could press shift tab twice to get the
[00:49:56.360 --> 00:50:01.240]   details. Let's see. The docstring says it holds sub
[00:50:01.240 --> 00:50:06.520]   modules in a list of the long story short version is it
[00:50:06.520 --> 00:50:09.960]   basically allows you to create a list of PyTorch functions to
[00:50:09.960 --> 00:50:13.160]   which you can append, extend and perform all list like
[00:50:13.160 --> 00:50:19.120]   functions. So encoder blocks will essentially have a list of
[00:50:19.120 --> 00:50:25.920]   modules of encoder. So all of the blocks inside of encoder for
[00:50:25.920 --> 00:50:31.160]   that we'll unpack all of the channels. So for I in range of
[00:50:31.160 --> 00:50:34.360]   all channels, remember Python is zero indexed. So if you go from
[00:50:34.360 --> 00:50:40.440]   zero to length of channels, we create a block and store it
[00:50:40.480 --> 00:50:43.800]   inside of an end module list. What is block? We just defined
[00:50:43.800 --> 00:50:48.080]   it up above. So all of these operators get stored in encoder
[00:50:48.080 --> 00:50:53.840]   blocks. From there, you want to pass it to max pooling, which
[00:50:53.840 --> 00:50:58.280]   we haven't done. So remember, I'm looking at the right hand
[00:50:58.280 --> 00:51:01.320]   part of the screen, the red arrow is a max pool of two by
[00:51:01.320 --> 00:51:05.600]   two, we're implementing the exact same thing. So we'll pass
[00:51:05.600 --> 00:51:10.160]   it to a max pool 2d with a two argument wise to an argument
[00:51:10.160 --> 00:51:14.600]   because the first argument is kernel size. This applies a 2d
[00:51:14.600 --> 00:51:22.760]   max pooling over an input signal. And the kernel size is
[00:51:22.760 --> 00:51:25.400]   the size of the window to take a max or so you want to take a
[00:51:25.400 --> 00:51:29.960]   max or two by two, which is why we pass max pool 2d of two.
[00:51:29.960 --> 00:51:35.840]   Inside of the forward function, we create a list a list of FPRS
[00:51:36.960 --> 00:51:40.480]   and we go over all of the blocks inside of encoder blocks. So
[00:51:40.480 --> 00:51:44.080]   we go over all of the function or all of the blocks and we
[00:51:44.080 --> 00:51:48.640]   append this followed by a pooling and return this. So this
[00:51:48.640 --> 00:51:55.200]   defines our entire encoder block followed by which we'd like to
[00:51:55.200 --> 00:52:00.440]   define a decoder block as well. And if you pay close attention,
[00:52:00.440 --> 00:52:02.480]   you might see some correspondence from these
[00:52:02.480 --> 00:52:05.880]   numbers over here. These are the channels skip over it.
[00:52:06.880 --> 00:52:11.680]   And here we'll apply the conf transpose 2d function. Let's
[00:52:11.680 --> 00:52:12.360]   see what that is.
[00:52:12.360 --> 00:52:21.440]   I want to look at con 2d please transpose 2d. So let's look at
[00:52:21.440 --> 00:52:25.680]   the doc string. It applies a 2d transposed convolution operator
[00:52:25.680 --> 00:52:30.840]   over an input image composed of several input planes. That's
[00:52:30.840 --> 00:52:35.120]   what this function is going to do. And it can also compute the
[00:52:35.120 --> 00:52:40.720]   gradient, which is what we want. This takes the number of input
[00:52:40.720 --> 00:52:46.400]   channels, output channels and kernel size. And we pass the
[00:52:46.400 --> 00:52:52.360]   list of channels here as an argument. So we define a up
[00:52:52.360 --> 00:52:58.040]   conf lock where we pass this module list. And similarly for
[00:52:58.040 --> 00:53:03.760]   decoder blocks, we can create another module list where we
[00:53:03.760 --> 00:53:10.360]   pass blocks like so I'm sorry, I'm having to scroll a bit, but
[00:53:10.360 --> 00:53:12.760]   it's quite hard to split my screen. I have a smaller
[00:53:12.760 --> 00:53:16.920]   monitor than I would like to. From there, we define the
[00:53:16.920 --> 00:53:19.520]   forward function for doing a forward pass. Now we're looking
[00:53:19.520 --> 00:53:22.040]   inside of the decoder and here's how the forward function would
[00:53:22.040 --> 00:53:29.680]   look. So for the entire range of channels from zero to length
[00:53:29.680 --> 00:53:34.520]   minus one, because Python is zero in text, we take the up
[00:53:34.520 --> 00:53:43.000]   conf and up conf just takes the conv transpose 2d. We store it
[00:53:43.000 --> 00:53:47.800]   in encoder FTRS. Now here's another thing. Remember this
[00:53:47.800 --> 00:53:53.600]   copy and crop. This was missing in our definition earlier in
[00:53:53.600 --> 00:53:59.040]   the constructor. So we crop the encoder features. We take the
[00:53:59.040 --> 00:54:02.080]   encoder features, which are being passed here to this
[00:54:02.080 --> 00:54:05.960]   function. So in the forward pass, we take in the encoder
[00:54:05.960 --> 00:54:12.480]   features, which we crop into and called Torchcat. What does
[00:54:12.480 --> 00:54:16.640]   Torchcat do? Great question. Concatenates the given sequence
[00:54:16.640 --> 00:54:22.360]   of tensors in the given dimension. So we take these
[00:54:22.360 --> 00:54:28.000]   features, copy and crop, which is crop and cat. Now this is
[00:54:28.000 --> 00:54:31.680]   what I mentioned earlier, like to go back a minute in time
[00:54:31.680 --> 00:54:35.160]   when I said when you're looking at a paper, right? And ideally,
[00:54:35.160 --> 00:54:37.920]   you know, the end goal for everyone joining us today is to
[00:54:37.920 --> 00:54:41.480]   not be required to join these because you can do it on by
[00:54:41.480 --> 00:54:45.480]   yourself. You're so good, which would hurt my role a bit, but
[00:54:45.480 --> 00:54:48.960]   that's what we want. So the next time you see copy and crop,
[00:54:48.960 --> 00:54:54.000]   right? Don't think copy and crop. Think crop and cat. So
[00:54:54.000 --> 00:54:56.960]   always try to think of PyTorch functions in your head or
[00:54:57.080 --> 00:54:59.720]   Kerasy function. I'm not good at Kerasy, so I don't use it.
[00:54:59.720 --> 00:55:02.680]   Whenever you're looking at a paper, and that's how you'll
[00:55:02.680 --> 00:55:06.240]   start increasing your speed of trying to implement these. I
[00:55:06.240 --> 00:55:11.000]   haven't implemented this, Aman did, and I'm sure it took him a
[00:55:11.000 --> 00:55:14.120]   little while. And as you keep practicing this, this is what
[00:55:14.120 --> 00:55:18.680]   you should be focusing on. A little side note there, sorry
[00:55:18.680 --> 00:55:21.200]   about that. So this is how we've defined the forward function
[00:55:21.200 --> 00:55:28.440]   here and we return this. Now we haven't defined crop and this
[00:55:28.440 --> 00:55:34.280]   doesn't exist inside of PyTorch. So we define it here. It does
[00:55:34.280 --> 00:55:39.320]   exist inside of TorchVision transforms. And from there you
[00:55:39.320 --> 00:55:46.240]   can call center crop and you can return it. Later on inside of
[00:55:48.160 --> 00:55:53.240]   class unit, you can call all of these functions together. And
[00:55:53.240 --> 00:55:59.040]   I'm trying to see if there are any other details. No, there
[00:55:59.040 --> 00:56:01.320]   are not. You can just utilize all of these functions that
[00:56:01.320 --> 00:56:05.600]   you've created from earlier. And that becomes your architecture.
[00:56:05.600 --> 00:56:11.160]   So this complete image and this paper of five pages takes about
[00:56:11.160 --> 00:56:16.040]   60 lines of code to implement. I also want to stop sharing my
[00:56:16.040 --> 00:56:28.120]   screen and share the right one again. Go full screen. So also
[00:56:28.120 --> 00:56:35.960]   to point out the official implementation, the first link
[00:56:35.960 --> 00:56:41.280]   here, it's slightly different to the approach we've taken here.
[00:56:41.280 --> 00:56:45.160]   But you can also go through it and understand it real quickly.
[00:56:45.840 --> 00:56:48.840]   So inside of this repository, if you head over to the unit folder,
[00:56:48.840 --> 00:56:52.840]   you can find the implementation and the functions defined right
[00:56:52.840 --> 00:56:58.360]   here. Slightly different, but not too different. I can. I'm
[00:56:58.360 --> 00:57:01.360]   almost out of time, so I cannot cover them. But I just point
[00:57:01.360 --> 00:57:06.120]   everyone to this. So we have a few minutes and I'll try to take
[00:57:06.120 --> 00:57:07.360]   as many questions as I can.
[00:57:11.440 --> 00:57:15.400]   Hey, Sanyam. I have a quick question about augmentations.
[00:57:15.400 --> 00:57:20.120]   Sure. So for augmentation, some typical augmentations would be
[00:57:20.120 --> 00:57:25.680]   rotation and scale changes and such things. So I know
[00:57:25.680 --> 00:57:31.920]   segmentation is often used for OCR tech. And one of the
[00:57:31.920 --> 00:57:37.000]   considerations for OCR is some characters look different when
[00:57:37.000 --> 00:57:41.480]   you rotate them, they resemble some other character. So how is
[00:57:41.480 --> 00:57:47.000]   that generally like, like, how is that tackled when you're
[00:57:47.000 --> 00:57:47.960]   trying to do that?
[00:57:47.960 --> 00:57:52.040]   That's a great question. I've worked a bit on this problem.
[00:57:52.040 --> 00:57:56.520]   And I was, I did have my notes here, which I wasn't going
[00:57:56.520 --> 00:57:59.400]   through. But let's say you're trying to see what's in this
[00:57:59.400 --> 00:58:02.960]   notes, right notes, right on this paper. Ideally, you want to
[00:58:02.960 --> 00:58:05.680]   focus on this, I'm just going to derive the function. So the
[00:58:05.680 --> 00:58:08.120]   first thing you want to do is segment the paper out of this
[00:58:08.120 --> 00:58:11.280]   image, crop into that, right? Now let's say if cropped into
[00:58:11.280 --> 00:58:15.320]   it, the text is somewhat readable, but not the most
[00:58:15.320 --> 00:58:18.240]   readable, right? So ideally, if I had the time, I'd walk over
[00:58:18.240 --> 00:58:20.920]   switch off the light, which means I'll change the brightness
[00:58:20.920 --> 00:58:24.640]   so that it's more readable. Probably I'll adjust my camera a
[00:58:24.640 --> 00:58:31.120]   bit. Focus, which means I've changed the contrast. So the
[00:58:31.120 --> 00:58:34.960]   augmentations become cropping in, changing the brightness,
[00:58:35.160 --> 00:58:36.400]   changing the contrast.
[00:58:36.400 --> 00:58:43.080]   Right. So yeah, those are the first like things that you would
[00:58:43.080 --> 00:58:47.080]   do if you were doing an OCR. But other augmentations to make it
[00:58:47.080 --> 00:58:51.160]   more robust, you typically would rotate an image, right? Or
[00:58:51.160 --> 00:58:55.360]   rotate. Like, yeah, you would rotate an image to make it
[00:58:55.360 --> 00:58:58.600]   invariant to certain transformations.
[00:58:58.600 --> 00:59:04.120]   For OCR, maybe just to a few degrees, but not not like
[00:59:04.120 --> 00:59:05.600]   completely by 90 degrees.
[00:59:05.600 --> 00:59:08.720]   Okay, okay. All right. Thanks.
[00:59:08.720 --> 00:59:16.160]   Sure. Um, I'll probably also point out so I hosted the
[00:59:16.160 --> 00:59:19.600]   PyTorch reading group for a bit. And there we looked at
[00:59:19.600 --> 00:59:23.240]   augmentations in depth. But if you head over to the weights and
[00:59:23.240 --> 00:59:26.400]   biases channel, you should be able to find the albumentations
[00:59:26.400 --> 00:59:29.280]   walkthrough in one of the lectures. So if you go to our
[00:59:29.280 --> 00:59:31.800]   channel and check out, I'm not going to open my YouTube
[00:59:31.800 --> 00:59:36.240]   because you all will see my recommendations. But you find the
[00:59:36.240 --> 00:59:39.320]   PyTorch playlist, you can find a walkthrough of albumentations
[00:59:39.320 --> 00:59:42.000]   where I do discuss this in depth and I cover OCR there.
[00:59:42.000 --> 00:59:48.280]   But I would really trust this library a lot. It's implemented
[00:59:48.280 --> 00:59:54.320]   by one of the best Kaggle grandmasters and really, leaders
[00:59:54.320 --> 00:59:58.080]   of research and people who are in the industry really have
[00:59:58.080 --> 00:59:59.120]   thought a lot about this.
[00:59:59.120 --> 01:00:09.080]   Great question, Ramesh, I have to mention this as well. I
[01:00:09.080 --> 01:00:11.880]   don't understand why Ramesh joins my study groups because
[01:00:11.880 --> 01:00:16.120]   he doesn't need to learn about these things. Rather, he should
[01:00:16.120 --> 01:00:18.080]   be teaching them. So I'll probably invite him at some
[01:00:18.080 --> 01:00:24.400]   point. One of the suggested homework is all of the suggested
[01:00:24.400 --> 01:00:29.080]   homework is Kaggle competitions. So the official implementation
[01:00:29.080 --> 01:00:33.880]   actually walks you through how to take this and apply units to
[01:00:33.880 --> 01:00:36.280]   Caravana competition. All of these are Kaggle competitions,
[01:00:36.280 --> 01:00:40.160]   by the way. The other blog I referred to shows you how to
[01:00:40.160 --> 01:00:46.200]   implement it on TGS competition. Another one is on Newmoon
[01:00:46.200 --> 01:00:50.640]   tracks competition. And I want everyone to go to these
[01:00:50.640 --> 01:00:53.920]   competitions, apply unit without cheating, without looking at
[01:00:53.920 --> 01:00:59.040]   what we discussed and try to get into the top 30%. For that
[01:00:59.040 --> 01:01:01.760]   you'll need a few things and you'll also probably have to use
[01:01:01.760 --> 01:01:06.480]   albumentations. So great question for OCR. Luckily, we
[01:01:06.480 --> 01:01:11.280]   don't have to worry about being working with text. But we should
[01:01:11.280 --> 01:01:13.440]   look at image augmentation. So I'll leave you all with the
[01:01:13.440 --> 01:01:17.160]   homework to check out albumentations. Take the unit
[01:01:17.160 --> 01:01:20.040]   architecture and try to apply it on one or all of these
[01:01:20.040 --> 01:01:24.040]   competitions and please consider writing about it. I can go on
[01:01:24.040 --> 01:01:26.680]   for hours talking about how blogging about things has
[01:01:26.680 --> 01:01:30.320]   changed my career in machine learning. But if you write
[01:01:30.320 --> 01:01:33.200]   about anything you learn, in my opinion, it really solidifies
[01:01:33.200 --> 01:01:37.920]   your, your learning. So that's that's suggested homework.
[01:01:37.920 --> 01:01:43.640]   Thanks for asking that. All right, we're one minute
[01:01:43.640 --> 01:01:46.800]   overboard, but I'll take one last question if any.
[01:01:47.560 --> 01:01:50.400]   Awesome. Thanks, everyone for joining. I hope this was
[01:01:50.400 --> 01:01:53.920]   helpful to everybody. Please feel free to leave a reply for
[01:01:53.920 --> 01:01:57.920]   any papers you want me to cover. We'll be meeting at least once a
[01:01:57.920 --> 01:02:01.840]   month to go through papers. My goal is to teach the key concept
[01:02:01.840 --> 01:02:04.720]   in a classical paper, if it's a classical paper, I'll be
[01:02:04.720 --> 01:02:07.840]   teaching you how to do it. So if you have any questions, please
[01:02:07.840 --> 01:02:10.960]   feel free to ask me. I'll be happy to answer them. So I'll
[01:02:10.960 --> 01:02:13.600]   leave you with the homework. Thanks for watching.
[01:02:14.000 --> 01:02:17.720]   Thank you so much for joining on a Sunday morning, afternoon or
[01:02:17.720 --> 01:02:20.160]   evening, and I look forward to seeing you in the other
[01:02:20.160 --> 01:02:21.120]   community events.
[01:02:21.120 --> 01:02:33.120]   Transcribed by https://otter.ai
[01:02:33.120 --> 01:02:43.120]   Transcribed by https://otter.ai

