
[00:00:00.000 --> 00:00:06.840]   Hi, I'm Lucas, and you're listening to Gradient Dissent.
[00:00:06.840 --> 00:00:11.560]   We started this program because we're super passionate about making machine learning work
[00:00:11.560 --> 00:00:15.120]   in the real world by any means necessary.
[00:00:15.120 --> 00:00:19.880]   And one of the things that we discovered in the process of building machine learning tools
[00:00:19.880 --> 00:00:24.220]   is that our users and our customers, they have a lot of information in their heads that's
[00:00:24.220 --> 00:00:26.420]   not publicly available.
[00:00:26.420 --> 00:00:30.040]   And a lot of people that we talk to ask us what other people are talking about and what
[00:00:30.040 --> 00:00:32.720]   other people are doing, what are the best practices.
[00:00:32.720 --> 00:00:36.400]   And so we wanted to make all the interesting conversations that we're having and all the
[00:00:36.400 --> 00:00:39.780]   interesting things that we're learning available for everyone out there.
[00:00:39.780 --> 00:00:41.840]   So I hope you enjoy this.
[00:00:41.840 --> 00:00:48.800]   Brandon Rohr is a mechanical engineer turned machine learning engineer/data scientist.
[00:00:48.800 --> 00:00:54.480]   He's worked on some incredible robotics projects and then worked on data science projects at
[00:00:54.480 --> 00:00:59.960]   Facebook and Microsoft, and currently he's a principal data scientist at iRobot.
[00:00:59.960 --> 00:01:04.860]   At the same time, he's an instructor at End-to-End Learning, where he's made some amazing videos
[00:01:04.860 --> 00:01:07.620]   on convolutional neural nets and other things.
[00:01:07.620 --> 00:01:09.320]   I'm super excited to talk to him.
[00:01:09.320 --> 00:01:13.520]   Brandon, it's really nice to talk to you and thanks for taking the time.
[00:01:13.520 --> 00:01:21.240]   You've worked on machine learning at quite a range of different companies, and most recently
[00:01:21.240 --> 00:01:22.240]   iRobot.
[00:01:22.240 --> 00:01:27.560]   And so I can't help myself, but I'd love to just hear about what kinds of challenges you
[00:01:27.560 --> 00:01:31.240]   have at iRobot and robotics in general.
[00:01:31.240 --> 00:01:32.240]   Yeah.
[00:01:32.240 --> 00:01:34.520]   Thanks for having me on, Lucas and Lavanya.
[00:01:34.520 --> 00:01:36.520]   I really appreciate it.
[00:01:36.520 --> 00:01:45.160]   At iRobot, we get to actually support these little giant Frisbees that run around on people's
[00:01:45.160 --> 00:01:47.320]   floors and suck up dirt.
[00:01:47.320 --> 00:01:52.640]   Those are the vacuum cleaners, and also there are mops, which run around on people's hard
[00:01:52.640 --> 00:01:55.520]   floors and clean up messes.
[00:01:55.520 --> 00:02:02.360]   And what's really fun about this is if you think about production machine learning systems
[00:02:02.360 --> 00:02:12.080]   having to deal with whatever input or badly formed requests that you might encounter,
[00:02:12.080 --> 00:02:16.760]   imagine taking that to the physical world and you have something that is bopping around
[00:02:16.760 --> 00:02:20.000]   literally every type of home in the world.
[00:02:20.000 --> 00:02:23.320]   There's 30 million of these things out there now.
[00:02:23.320 --> 00:02:28.120]   And as hard as we try to imagine, we can't imagine all of the challenges that they will
[00:02:28.120 --> 00:02:29.500]   come up against.
[00:02:29.500 --> 00:02:36.600]   And so this is really fun from an algorithm design standpoint and just an engineering
[00:02:36.600 --> 00:02:45.140]   standpoint, making something that can get beat on, can have cats ride on it, can run
[00:02:45.140 --> 00:02:50.280]   into all kinds of things, encounter cords, encounter socks, encounter Legos, encounter
[00:02:50.280 --> 00:02:54.960]   Skittles, and how is it going to handle all of these things?
[00:02:54.960 --> 00:02:57.080]   That to me is really fun.
[00:02:57.080 --> 00:03:05.560]   It's the polar opposite of the sandbox carefully prescribed problem where you know exactly
[00:03:05.560 --> 00:03:09.360]   what your data is beforehand and you know it's been cleaned up so it's going to give
[00:03:09.360 --> 00:03:11.840]   you a good high quality answer on the other end.
[00:03:11.840 --> 00:03:15.520]   I'm curious, I feel like I've had a lot of friends in the last couple of years kind of
[00:03:15.520 --> 00:03:23.040]   move from consumer internet ML applications to robotics.
[00:03:23.040 --> 00:03:26.200]   Was there like big things, was it a big adjustment for you?
[00:03:26.200 --> 00:03:32.400]   Like were there big changes or was it mostly kind of the same set of issues?
[00:03:32.400 --> 00:03:37.120]   Let's see, definitely changes, but for me this was coming home.
[00:03:37.120 --> 00:03:40.340]   So robots for me is where I started.
[00:03:40.340 --> 00:03:45.480]   My degrees are in mechanical engineering and my graduate work was all about using robots
[00:03:45.480 --> 00:03:47.760]   to rehabilitate stroke patients.
[00:03:47.760 --> 00:03:52.120]   So you know, knowing things that could break all the time and not to trust your sensors,
[00:03:52.120 --> 00:03:55.320]   that's kind of what I grew up with.
[00:03:55.320 --> 00:04:03.040]   Then when data science became a more common career path, I rebranded myself as a data
[00:04:03.040 --> 00:04:10.880]   scientist and went to agriculture, went to Microsoft doing cloud machine learning solutions
[00:04:10.880 --> 00:04:13.040]   for a variety of different companies.
[00:04:13.040 --> 00:04:19.680]   I went to Facebook infrastructure, which is a fascinating set of problems around keeping
[00:04:19.680 --> 00:04:25.840]   one of the biggest networks and set of data centers in the world up and going and running
[00:04:25.840 --> 00:04:28.640]   efficiently.
[00:04:28.640 --> 00:04:35.800]   All of these things, what I enjoy about them is that you could not ignore where the data
[00:04:35.800 --> 00:04:36.960]   came from.
[00:04:36.960 --> 00:04:42.240]   You had to know something about either the people, what state they were in when they
[00:04:42.240 --> 00:04:44.320]   generated it.
[00:04:44.320 --> 00:04:47.320]   You had to know about what pre-processing it had.
[00:04:47.320 --> 00:04:50.680]   You had to know about the assumptions that were made along the way.
[00:04:50.680 --> 00:04:57.320]   If you didn't know this, then you couldn't build good models to get answers out of it.
[00:04:57.320 --> 00:05:02.960]   And so robots just take this and they put it front and center and take it to the extreme.
[00:05:02.960 --> 00:05:08.720]   Because if you don't know what a given sensor value means in the physical world, then it's
[00:05:08.720 --> 00:05:12.840]   really hard to build a good model around it and know how to interpret it.
[00:05:12.840 --> 00:05:17.720]   Naive models where you just throw things in, unless you get really lucky, they just don't
[00:05:17.720 --> 00:05:18.720]   work well.
[00:05:18.720 --> 00:05:23.880]   Does that lead you to simpler models?
[00:05:23.880 --> 00:05:27.840]   Are you more afraid of complexity then for these applications?
[00:05:27.840 --> 00:05:34.520]   My personal strategy when faced with something like that is, if I don't know everything that's
[00:05:34.520 --> 00:05:39.120]   going to come up against, the biggest thing I want to make sure is that when it performs
[00:05:39.120 --> 00:05:44.160]   poorly, it doesn't do horrendously bad.
[00:05:44.160 --> 00:05:45.160]   So I don't know.
[00:05:45.160 --> 00:05:46.160]   So how do you do that?
[00:05:46.160 --> 00:05:47.160]   Tell me about that.
[00:05:47.160 --> 00:05:50.160]   Simplicity is a good one.
[00:05:50.160 --> 00:05:53.520]   So knowing exactly what happens.
[00:05:53.520 --> 00:05:57.080]   One example of this is in agriculture.
[00:05:57.080 --> 00:06:02.880]   One of the hard modeling problems is, I have a field, I have some corn seed, I planted
[00:06:02.880 --> 00:06:06.480]   on a certain day, I use this much fertilizer, here's what the weather and the precipitation
[00:06:06.480 --> 00:06:07.860]   is all season.
[00:06:07.860 --> 00:06:10.640]   How much am I going to harvest at the end of the year?
[00:06:10.640 --> 00:06:16.360]   If you had a model that could spit that out, you would have solved agriculture, or at least
[00:06:16.360 --> 00:06:19.580]   the yield problem in agriculture.
[00:06:19.580 --> 00:06:24.200]   But there are so many variables and the model we were working with was a popular academic
[00:06:24.200 --> 00:06:29.360]   model that had literally hundreds of variables in it.
[00:06:29.360 --> 00:06:34.440]   And there's no way that you have enough data to train that model well.
[00:06:34.440 --> 00:06:38.880]   And what was really funny is when we did some analyses on that, using popular settings for
[00:06:38.880 --> 00:06:44.600]   that model, you could have a really naive model, which just estimated a flat rate for
[00:06:44.600 --> 00:06:47.080]   all fields everywhere for all conditions.
[00:06:47.080 --> 00:06:52.360]   And then this really elaborate several hundred parameter model, the flat rate model did like
[00:06:52.360 --> 00:06:53.360]   twice as good.
[00:06:53.360 --> 00:06:58.760]   So it's funny, because I would think with like plants, there'd be like a lot of complicated
[00:06:58.760 --> 00:07:03.560]   interactions that but I guess you just don't have enough data to know.
[00:07:03.560 --> 00:07:04.720]   And you're exactly right.
[00:07:04.720 --> 00:07:10.280]   And that's why the many parameter model didn't do so well.
[00:07:10.280 --> 00:07:14.560]   It did account for a lot of interactions, but to get them to work the way they were
[00:07:14.560 --> 00:07:19.080]   supposed to, you had to get all those parameter values correct or in the neighborhood.
[00:07:19.080 --> 00:07:22.120]   And we just didn't have enough information to do that.
[00:07:22.120 --> 00:07:27.900]   So an alternative approach then is to start with a very dumb estimate, and then incrementally
[00:07:27.900 --> 00:07:29.560]   make it a little smarter.
[00:07:29.560 --> 00:07:35.680]   I think by the time I was done, I was working with like a three parameter model, and one
[00:07:35.680 --> 00:07:41.760]   for precipitation and one for soil texture and one for something else.
[00:07:41.760 --> 00:07:47.040]   And to be able to check it each time and really just listen to your data.
[00:07:47.040 --> 00:07:50.720]   So same holds true then for robots or for anything else.
[00:07:50.720 --> 00:07:52.440]   It's like simple is good.
[00:07:52.440 --> 00:07:53.440]   Simple is good.
[00:07:53.440 --> 00:07:57.120]   Is there anything else though, like to make sure that your models are sort of robust in
[00:07:57.120 --> 00:08:00.080]   the face of different types of data?
[00:08:00.080 --> 00:08:05.520]   It sounds like you're also maybe like kind of pulling apart the problem into sub problems.
[00:08:05.520 --> 00:08:06.760]   Definitely that.
[00:08:06.760 --> 00:08:09.680]   Some problems are amenable to that.
[00:08:09.680 --> 00:08:14.720]   And to the extent you can separate it into a sub problem, that is a great strategy.
[00:08:14.720 --> 00:08:15.840]   Although not everyone thinks that, right?
[00:08:15.840 --> 00:08:20.280]   I mean, like sometimes people talk about sort of like end to end autonomy, right?
[00:08:20.280 --> 00:08:23.360]   So I do think that's a little bit of a point of view.
[00:08:23.360 --> 00:08:24.360]   That's true.
[00:08:24.360 --> 00:08:25.360]   That's a good footnote.
[00:08:25.360 --> 00:08:29.400]   I would say that that is my opinion that pulling it apart is.
[00:08:29.400 --> 00:08:32.120]   I don't think that's generally accepted.
[00:08:32.120 --> 00:08:34.720]   I agree with that.
[00:08:34.720 --> 00:08:41.360]   But it is easy to get over ambitious about, you know, to fall in love with your model
[00:08:41.360 --> 00:08:45.240]   and say, well, it explains potentially so many phenomena.
[00:08:45.240 --> 00:08:50.240]   Like it must be right if we can just get the data to train it correctly.
[00:08:50.240 --> 00:08:53.760]   But then when you actually in experience, like we've seen this even with some vision
[00:08:53.760 --> 00:09:01.160]   models, the actual high quality label data to train it well would cost so much to gather
[00:09:01.160 --> 00:09:02.160]   that it's impractical.
[00:09:02.160 --> 00:09:04.520]   And so in that case, model doesn't do much.
[00:09:04.520 --> 00:09:10.760]   And if you close your eyes to that, move ahead with poorly labeled data, then badness happens
[00:09:10.760 --> 00:09:14.760]   and you get models that are worse than no model at all.
[00:09:14.760 --> 00:09:20.520]   Yeah, I think we've all experienced that.
[00:09:20.520 --> 00:09:21.520]   I'm kind of curious.
[00:09:21.520 --> 00:09:22.520]   It's funny.
[00:09:22.520 --> 00:09:27.880]   I think you might have developed a real interest later in life in sort of mechanical engineering
[00:09:27.880 --> 00:09:28.880]   and electrical engineering.
[00:09:28.880 --> 00:09:32.840]   And I feel like you've kind of gone in the opposite direction.
[00:09:32.840 --> 00:09:38.640]   I guess one similarity that I've found, you know, of sort of like mechanical problems
[00:09:38.640 --> 00:09:44.680]   and machine learning is that you don't get good error messages in either domain.
[00:09:44.680 --> 00:09:51.880]   Do you think that your background in mechanical engineering has helped you in certain ways
[00:09:51.880 --> 00:09:52.880]   in machine learning?
[00:09:52.880 --> 00:09:57.240]   Or how did you go about learning a new field because lots of people want to do it?
[00:09:57.240 --> 00:10:00.460]   And how did you bring the knowledge you had to help you there?
[00:10:00.460 --> 00:10:05.400]   In my case, it was motivated by a problem I was trying to solve.
[00:10:05.400 --> 00:10:12.800]   And so in my work, we use robots to help rehabilitate stroke patients.
[00:10:12.800 --> 00:10:14.560]   We saw changes in their movements.
[00:10:14.560 --> 00:10:19.900]   A good research question is that, well, what's going on in the brain to make their movements
[00:10:19.900 --> 00:10:20.900]   get smoother?
[00:10:20.900 --> 00:10:22.720]   Like, what's going on there?
[00:10:22.720 --> 00:10:28.560]   And the more you dig into human movement, a whole collection of questions bubbles up.
[00:10:28.560 --> 00:10:35.600]   How does the brain control this hardware that's sloppy and that changes over time and that's
[00:10:35.600 --> 00:10:40.960]   not very accurate compared to like, you know, precision machine tool robots?
[00:10:40.960 --> 00:10:47.660]   And with huge time delays, time delays that would take any like off the shelf robot and
[00:10:47.660 --> 00:10:48.840]   drive it unstable.
[00:10:48.840 --> 00:10:51.360]   But the brain does it casually.
[00:10:51.360 --> 00:10:56.120]   Like we do it like we're half freezing and our neuromuscular dynamics all changes.
[00:10:56.120 --> 00:10:57.680]   The brain compensates.
[00:10:57.680 --> 00:11:01.520]   We're drunk and all of the time delays change.
[00:11:01.520 --> 00:11:02.880]   Brain compensates.
[00:11:02.880 --> 00:11:04.380]   Like how does this happen?
[00:11:04.380 --> 00:11:11.160]   So this was the problem that I wanted to solve is how could I make something that could control
[00:11:11.160 --> 00:11:15.040]   a piece of hardware that I didn't know or understand very well and it was going to change
[00:11:15.040 --> 00:11:17.200]   all over the place.
[00:11:17.200 --> 00:11:23.240]   And so that led me on the path of learning what I could about how the brain works, which
[00:11:23.240 --> 00:11:27.360]   if you from the point of view of, you know, now I want to turn it into an algorithm.
[00:11:27.360 --> 00:11:31.440]   There's still huge gaps there, even though we call neural networks, neural networks,
[00:11:31.440 --> 00:11:35.920]   they have no resemblance at all to anything that goes on in the brain.
[00:11:35.920 --> 00:11:40.400]   And so studying that and then figuring out if it was successful, what I what would I
[00:11:40.400 --> 00:11:41.720]   want it to do?
[00:11:41.720 --> 00:11:49.080]   And that did lead me then into studying different signal processing methods and different algorithms,
[00:11:49.080 --> 00:11:51.000]   different families of algorithms.
[00:11:51.000 --> 00:11:57.040]   And then once I started being a data scientist for my day job, then aside from this research
[00:11:57.040 --> 00:12:03.360]   interest, there was a whole other like professional motivation to dig into these things.
[00:12:03.360 --> 00:12:07.680]   And then once I started writing tutorials and teaching these, then there become even
[00:12:07.680 --> 00:12:12.200]   more motivation to learn these things and to be able to understand them well.
[00:12:12.200 --> 00:12:14.240]   So it kind of built on itself.
[00:12:14.240 --> 00:12:19.720]   That original problem of kind of building a general purpose, you know, brain or controller
[00:12:19.720 --> 00:12:24.000]   that you could pop into any robot and that it could learn what to do with it is still
[00:12:24.000 --> 00:12:26.480]   a long term passion of mine.
[00:12:26.480 --> 00:12:31.180]   You know, it's my like personal 30 year project.
[00:12:31.180 --> 00:12:32.360]   What makes that hard?
[00:12:32.360 --> 00:12:38.360]   The real world, the robots never going to experience the same thing twice.
[00:12:38.360 --> 00:12:43.640]   You're never going to get exactly the same camera image two times in a row.
[00:12:43.640 --> 00:12:49.080]   So one thing that's hard is you have to deal with always new experiences.
[00:12:49.080 --> 00:12:51.440]   So you can never learn exactly what you do in this situation.
[00:12:51.440 --> 00:12:56.280]   So you have to learn what other situations are similar.
[00:12:56.280 --> 00:13:06.200]   That sameness is very hard and we humans do it so well that it's almost it makes it harder
[00:13:06.200 --> 00:13:08.320]   to put down into code.
[00:13:08.320 --> 00:13:12.280]   You could say that about like image processing or audio processing.
[00:13:12.280 --> 00:13:15.960]   And I feel like when you look at the progress in terms of like, you know, facial recognition
[00:13:15.960 --> 00:13:18.920]   or like understanding voice, it's like really spectacular.
[00:13:18.920 --> 00:13:19.920]   Right.
[00:13:19.920 --> 00:13:23.240]   Like, you know, we see it like in our lives all the time, you know, for better or worse.
[00:13:23.240 --> 00:13:24.240]   Right.
[00:13:24.240 --> 00:13:31.080]   But I feel like we don't see like robots running around like we might have expected.
[00:13:31.080 --> 00:13:37.380]   And like the feats that robots do that I'm kind of wired to be impressed by are actually
[00:13:37.380 --> 00:13:40.480]   like incredibly unimpressive to my mother.
[00:13:40.480 --> 00:13:41.480]   Right.
[00:13:41.480 --> 00:13:44.880]   Whereas like, you know, I think in every other field, the stuff that ML is doing is sort
[00:13:44.880 --> 00:13:47.220]   of amazing, like, you know, compared to a human.
[00:13:47.220 --> 00:13:52.920]   But I feel like in robotics, you know, we like look at what Arumba does and it's like,
[00:13:52.920 --> 00:13:54.440]   you know, it kind of blows our mind.
[00:13:54.440 --> 00:13:57.880]   But, you know, my cat can do much more impressive stuff.
[00:13:57.880 --> 00:13:58.880]   Right.
[00:13:58.880 --> 00:14:01.520]   You know, less helpful, kind of creating a mess instead of undoing it.
[00:14:01.520 --> 00:14:06.800]   But still, like what why is like robotics so particularly hard?
[00:14:06.800 --> 00:14:11.660]   So the similarity problem, once you get past something concrete, like this face belongs
[00:14:11.660 --> 00:14:13.600]   to this person in different situations.
[00:14:13.600 --> 00:14:17.280]   But here's like I'm in a city I've never been before.
[00:14:17.280 --> 00:14:20.680]   If I had to guess which way the hospital is, you know, how would I do that?
[00:14:20.680 --> 00:14:27.000]   And so like we have a lot of subtle things that we do to like get oriented in novel situations.
[00:14:27.000 --> 00:14:29.260]   That's one aspect.
[00:14:29.260 --> 00:14:35.560]   Another is that machine learning, the way it's set up right now, it requires just a
[00:14:35.560 --> 00:14:38.360]   whole lot of data.
[00:14:38.360 --> 00:14:44.480]   So to learn basic things, categorizing images, just like, you know, something that we train
[00:14:44.480 --> 00:14:49.320]   animals to do on a regular basis, not even very brilliant animals.
[00:14:49.320 --> 00:14:52.320]   That requires huge amounts of well-labeled data.
[00:14:52.320 --> 00:14:55.740]   And if you get some poorly labeled data in there, you can mess it all up.
[00:14:55.740 --> 00:15:00.520]   To learn things, to do a thing where you've like maybe never done it before and you have
[00:15:00.520 --> 00:15:03.760]   to make a reasonable guess your first time.
[00:15:03.760 --> 00:15:08.460]   There are some people making efforts in that direction, but it's still very early days.
[00:15:08.460 --> 00:15:11.280]   What do you think about the approach of kind of simulating data?
[00:15:11.280 --> 00:15:13.280]   Does that seem promising to you?
[00:15:13.280 --> 00:15:14.280]   Yes.
[00:15:14.280 --> 00:15:16.880]   In fact, another thing that's really hard about doing robots is it's hard to keep your
[00:15:16.880 --> 00:15:17.880]   robot up and going.
[00:15:17.880 --> 00:15:24.360]   If you ever worked in a robotics lab, if you get like three solid runs, it's like, great.
[00:15:24.360 --> 00:15:25.360]   Write that up.
[00:15:25.360 --> 00:15:26.360]   That's your thesis.
[00:15:26.360 --> 00:15:29.520]   You're done before the next spring breaks.
[00:15:29.520 --> 00:15:35.620]   So a simulation is really useful for that because you can run a robot for thousands
[00:15:35.620 --> 00:15:42.820]   of years in simulation time and generate that volume of training data.
[00:15:42.820 --> 00:15:44.560]   It's not without its pitfalls, though.
[00:15:44.560 --> 00:15:49.920]   I've worked with simulations and if you talk to anyone who has, they'll have a story about
[00:15:49.920 --> 00:15:56.060]   how there's some quirk in the simulated physics or the simulated world and your reinforcement
[00:15:56.060 --> 00:15:58.420]   learning agent learned to take advantage of it.
[00:15:58.420 --> 00:16:02.820]   So like in mine, there was a seven degree of freedom robot arm and it learned to reach
[00:16:02.820 --> 00:16:08.240]   down into the table because I made it too soft and use the table as a guide and then
[00:16:08.240 --> 00:16:13.600]   come up underneath the thing it was supposed to pick up.
[00:16:13.600 --> 00:16:21.260]   So there is a paper that came out not too long ago about open AI using a shadow robot
[00:16:21.260 --> 00:16:28.660]   hand with a whole lot of robot simulation to do some of the steps to solve a Rubik's
[00:16:28.660 --> 00:16:30.340]   cube.
[00:16:30.340 --> 00:16:35.340]   And a good part of what they did was getting the simulation right.
[00:16:35.340 --> 00:16:40.780]   And in fact, if you read closely, they actually went back and modified their experiment and
[00:16:40.780 --> 00:16:45.540]   their physical hardware in order to make it simulatable.
[00:16:45.540 --> 00:16:50.060]   It does not come easy, but potentially it's a really useful thing.
[00:16:50.060 --> 00:16:54.780]   What is the toughest part about going from a model on your laptop to something in production?
[00:16:54.780 --> 00:17:03.180]   So when I'm working with data on my laptop, I run it, it all fits in RAM, get an answer,
[00:17:03.180 --> 00:17:07.460]   spits it out, whatever makes an image, saves it to a file.
[00:17:07.460 --> 00:17:08.460]   That's all good.
[00:17:08.460 --> 00:17:10.220]   Get an acceptable error rate.
[00:17:10.220 --> 00:17:12.780]   I'm good to go.
[00:17:12.780 --> 00:17:20.780]   Taking it into production, that trained model then becomes like the easiest part of the
[00:17:20.780 --> 00:17:23.580]   whole thing.
[00:17:23.580 --> 00:17:27.180]   Depending on what you're using it for, let's imagine you're using it as part of an app
[00:17:27.180 --> 00:17:32.460]   or part of a service where somebody somewhere on their phone or on their laptop has to do
[00:17:32.460 --> 00:17:34.420]   something that needs the result of this.
[00:17:34.420 --> 00:17:40.180]   Let's say it's a weather predictor or a coronavirus risk predictor or something like that.
[00:17:40.180 --> 00:17:47.500]   All of the pieces to get that request, to make sure that it's not part of some denial
[00:17:47.500 --> 00:17:54.220]   of service attack, to make sure that the request is well-formed and it's not going to gum up
[00:17:54.220 --> 00:17:59.220]   your model, to get the answer out, to make sure that it gets delivered.
[00:17:59.220 --> 00:18:05.420]   All of those pieces are, break them down individually and they're fairly simple.
[00:18:05.420 --> 00:18:08.740]   You put them on a piece of paper and it looks like a bunch of blocks connected by arrows.
[00:18:08.740 --> 00:18:10.300]   It's like, "Oh, okay.
[00:18:10.300 --> 00:18:11.620]   Here's what all the things do.
[00:18:11.620 --> 00:18:12.620]   That's great."
[00:18:12.620 --> 00:18:19.820]   I don't do this myself, but I sit next to people at work who spend their days making
[00:18:19.820 --> 00:18:23.860]   sure that all of these blocks run smoothly and all of these arrows are working the way
[00:18:23.860 --> 00:18:25.660]   they're supposed to.
[00:18:25.660 --> 00:18:34.500]   It is a full-time preoccupation or a full-time demand on your attention to care for and feed
[00:18:34.500 --> 00:18:35.500]   these.
[00:18:35.500 --> 00:18:39.300]   They're all running on computers that are in data centers somewhere.
[00:18:39.300 --> 00:18:43.260]   They're all running on software that's being regularly updated.
[00:18:43.260 --> 00:18:49.340]   Anyone who uses Amazon Web Services is probably familiar with new services and new capabilities
[00:18:49.340 --> 00:18:51.020]   coming all the time.
[00:18:51.020 --> 00:18:52.940]   Occasionally, there are breaking changes.
[00:18:52.940 --> 00:18:53.940]   "It worked yesterday.
[00:18:53.940 --> 00:18:56.820]   It doesn't work today."
[00:18:56.820 --> 00:19:01.660]   It is a lot harder than it sounds when someone tweets out, "Oh, cool.
[00:19:01.660 --> 00:19:06.220]   I spun up a cluster and now this thing is running a thousand times faster."
[00:19:06.220 --> 00:19:10.020]   It's like, "That's super cool," but that hides a lot of effort that goes underneath.
[00:19:10.020 --> 00:19:16.700]   It also hides a lot of the long-term investment required to keep that up and going.
[00:19:16.700 --> 00:19:22.900]   I totally agree, but the things that you've described feel like the difference between
[00:19:22.900 --> 00:19:27.820]   any kind of demo on your laptop and any kind of production thing.
[00:19:27.820 --> 00:19:34.540]   But I do feel like there's at least a trope or a meme or something about how machine learning
[00:19:34.540 --> 00:19:38.260]   is particularly hard to do this with.
[00:19:38.260 --> 00:19:42.220]   Do you think that there's something special about machine learning that makes it extra
[00:19:42.220 --> 00:19:45.020]   hard to put the stuff around it and make it stable?
[00:19:45.020 --> 00:19:50.180]   Or do you think people just get too excited about demos in general?
[00:19:50.180 --> 00:19:51.180]   Yes.
[00:19:51.180 --> 00:19:57.780]   Yes, so machine learning specific issues are – it's almost impossible to consider
[00:19:57.780 --> 00:19:59.980]   all the possible inputs you'll get.
[00:19:59.980 --> 00:20:06.100]   For instance, if you want to take an image as an input and say, put a filter on it or
[00:20:06.100 --> 00:20:08.540]   do some kind of identification on it.
[00:20:08.540 --> 00:20:10.180]   And so, it's very possible.
[00:20:10.180 --> 00:20:14.420]   Basically, your users are now adversarial.
[00:20:14.420 --> 00:20:17.500]   Some people out there are going to either intentionally or on accident come up with
[00:20:17.500 --> 00:20:20.360]   things that will break what you're doing.
[00:20:20.360 --> 00:20:24.620]   And so, being able to identify that – I mean, it may not take the service down, but
[00:20:24.620 --> 00:20:31.140]   it might produce a result that's undesirable or offensive or at least embarrassing.
[00:20:31.140 --> 00:20:34.260]   And so, you kind of have to keep an eye on that.
[00:20:34.260 --> 00:20:38.780]   The other thing is a lot of times when we train a machine learning model, there's a
[00:20:38.780 --> 00:20:42.260]   training set, validation set, maybe a test set.
[00:20:42.260 --> 00:20:47.720]   You train it, you get good results on that data, and you go.
[00:20:47.720 --> 00:20:52.300]   That assumes that the world doesn't change, which is a terrible assumption.
[00:20:52.300 --> 00:20:57.500]   As soon as you deploy that model, whatever phenomenon you are modeling is going to start
[00:20:57.500 --> 00:20:58.900]   gradually shifting.
[00:20:58.900 --> 00:21:01.300]   So, a great example of this is weather.
[00:21:01.300 --> 00:21:07.300]   So, if you had a really good weather predictor in 1970, it would probably not be worth very
[00:21:07.300 --> 00:21:09.060]   much today.
[00:21:09.060 --> 00:21:15.440]   And having the ability then to not have a static model or to periodically retrain and
[00:21:15.440 --> 00:21:22.100]   redeploy is important if you want to keep that up and going.
[00:21:22.100 --> 00:21:26.980]   Those are the two big ways that I've seen machine learning models in particular fail.
[00:21:26.980 --> 00:21:27.980]   Okay.
[00:21:27.980 --> 00:21:32.620]   You wrote yourself a really, really softball question, but I'm kind of dying to know
[00:21:32.620 --> 00:21:34.820]   what you're going to answer it.
[00:21:34.820 --> 00:21:37.380]   So, here you go, Brandon.
[00:21:37.380 --> 00:21:39.380]   What's so great about robots?
[00:21:39.380 --> 00:21:45.260]   So, I've alluded to this already, but there's more.
[00:21:45.260 --> 00:21:50.300]   So, for me personally, there is passion around robots.
[00:21:50.300 --> 00:21:56.860]   When I was a kid, I'm five years old, and I'm watching the Empire Strikes Back in the
[00:21:56.860 --> 00:21:58.580]   theater.
[00:21:58.580 --> 00:22:03.140]   And Luke, at the very end, he gets his hand cut off, of course, and he ends up with this
[00:22:03.140 --> 00:22:08.100]   prosthetic hand, and there's these mechanical actuators in place of tensions.
[00:22:08.100 --> 00:22:10.740]   I just thought that was the coolest thing I'd ever seen.
[00:22:10.740 --> 00:22:14.060]   So, that right there kind of set my career path.
[00:22:14.060 --> 00:22:18.820]   And so, I'm in graduate school now, mechanical engineering, and working with prostheses and
[00:22:18.820 --> 00:22:21.020]   stroke rehabilitation.
[00:22:21.020 --> 00:22:30.580]   And I have circuit boards out, and I'm on the phone with my dad, who's an analog electrical
[00:22:30.580 --> 00:22:31.580]   engineer.
[00:22:31.580 --> 00:22:36.020]   He's like, "Hey, I need to build a preamplifier for this signal because the sensor is not
[00:22:36.020 --> 00:22:42.460]   strong enough, and I have to package up and shrink wrap it and tape it to this motorized
[00:22:42.460 --> 00:22:45.140]   prosthesis that we're putting together.
[00:22:45.140 --> 00:22:51.140]   And then I have to read it in through the serial port and write some C code to pull
[00:22:51.140 --> 00:22:54.140]   the values off of it and read it in.
[00:22:54.140 --> 00:22:59.300]   And it's just like, it's down in the electrons.
[00:22:59.300 --> 00:23:06.940]   It's the interface between the physical and the software world, and maddening and frustrating.
[00:23:06.940 --> 00:23:09.060]   And so many times it doesn't work.
[00:23:09.060 --> 00:23:11.980]   And then after weeks, it does.
[00:23:11.980 --> 00:23:18.420]   And I just stand back and I look at the thing and I think, "Whoa, there's no boundary
[00:23:18.420 --> 00:23:22.700]   between the real world and the imaginary world and the computers.
[00:23:22.700 --> 00:23:23.780]   It's all real.
[00:23:23.780 --> 00:23:27.340]   There's physical and there's digital, but there's a blurry line in between."
[00:23:27.340 --> 00:23:30.420]   And robots span this.
[00:23:30.420 --> 00:23:37.660]   And so, with robots, you embrace all of the chaos of this physical world, and you really
[00:23:37.660 --> 00:23:43.780]   have to put your money where your mouth is with regards to control and learning.
[00:23:43.780 --> 00:23:47.660]   And you know that your sensors are going to fail and you know that your actuators are
[00:23:47.660 --> 00:23:49.940]   going to change performance over time.
[00:23:49.940 --> 00:23:52.780]   You have to be able to handle all of this stuff.
[00:23:52.780 --> 00:23:58.860]   And when you're done, if you do it right, you have a little thing that if you suspend
[00:23:58.860 --> 00:24:05.540]   disbelief, it looks like it might be alive in some small, cool way.
[00:24:05.540 --> 00:24:09.900]   And yeah, that's pretty cool.
[00:24:09.900 --> 00:24:10.900]   That gets my blurry.
[00:24:10.900 --> 00:24:12.180]   Okay, so I got to ask you.
[00:24:12.180 --> 00:24:18.020]   So one thing I think about is, it feels like we don't actually engage with many robots
[00:24:18.020 --> 00:24:25.140]   in our lives today, but I don't think it's so much the cost of the materials, right?
[00:24:25.140 --> 00:24:30.260]   I mean, iRobot maybe has one good example of a robot we do use regularly, but it seems
[00:24:30.260 --> 00:24:34.540]   like there's a lot more things where you could build them, but it would be hard to kind of
[00:24:34.540 --> 00:24:38.140]   make them smart enough to be useful.
[00:24:38.140 --> 00:24:39.500]   And software is so amazing, right?
[00:24:39.500 --> 00:24:43.340]   Because once you make it once, you can copy it and put it in everything.
[00:24:43.340 --> 00:24:48.180]   I sometimes wonder if a day will come that there will be robots just all over the place
[00:24:48.180 --> 00:24:49.900]   doing useful things for us.
[00:24:49.900 --> 00:24:55.860]   Do you imagine that will happen or are there breakthroughs that are kind of unforeseeable?
[00:24:55.860 --> 00:24:56.860]   What do you think about that?
[00:24:56.860 --> 00:24:58.380]   I very much do.
[00:24:58.380 --> 00:25:06.700]   And the chain of reasoning you just followed is exactly what I did when I was in my graduate
[00:25:06.700 --> 00:25:10.980]   program and I'm looking around and I'm thinking like, yeah, hardware capabilities are pretty
[00:25:10.980 --> 00:25:11.980]   cool.
[00:25:11.980 --> 00:25:15.400]   There were some robots at the time out at Berkeley that had crazy number of degrees
[00:25:15.400 --> 00:25:19.420]   of freedom, big as a person, could do all kinds of things.
[00:25:19.420 --> 00:25:24.780]   But the gap between what could I do if I was controlling it with a joystick and what can
[00:25:24.780 --> 00:25:28.540]   it do with its own brain was so big.
[00:25:28.540 --> 00:25:30.740]   And a joystick is not even a very good interface.
[00:25:30.740 --> 00:25:33.220]   What could I do if it was tied right into my brain?
[00:25:33.220 --> 00:25:35.060]   It's like huge.
[00:25:35.060 --> 00:25:39.340]   And I realized that I wanted, if you go into robotics, typically you focus on hardware
[00:25:39.340 --> 00:25:40.340]   or software.
[00:25:40.340 --> 00:25:44.500]   So it's like, well, software seems like the bottleneck here.
[00:25:44.500 --> 00:25:47.580]   So that's what I'm going to focus on.
[00:25:47.580 --> 00:25:55.420]   For now, a lot of the emphasis on machine learning methods is driven by performance
[00:25:55.420 --> 00:25:58.020]   on benchmarks.
[00:25:58.020 --> 00:26:03.980]   That's good if you have to publish papers, you need some basis for saying this method
[00:26:03.980 --> 00:26:08.180]   is as good as or better or close to some previous method.
[00:26:08.180 --> 00:26:10.680]   And the benchmarks are good for that.
[00:26:10.680 --> 00:26:15.720]   But it's gotten to the point, in my opinion, where the tail's kind of wagging the dog and
[00:26:15.720 --> 00:26:18.260]   we only pursue the problems that we have good benchmarks for.
[00:26:18.260 --> 00:26:23.060]   So image classification, in all the world of machine learning stuff, image classification
[00:26:23.060 --> 00:26:27.540]   is a tiny, tiny little piece of problem you could solve.
[00:26:27.540 --> 00:26:33.280]   But you wouldn't know that based on popular press and based on a random sampling of papers
[00:26:33.280 --> 00:26:36.120]   in Europe, for instance.
[00:26:36.120 --> 00:26:39.020]   So that is starting to change.
[00:26:39.020 --> 00:26:44.420]   The last couple of years, you see a little bit more kind of like people going rogue with
[00:26:44.420 --> 00:26:49.140]   architectures or with the problems that they're willing to handle.
[00:26:49.140 --> 00:26:55.700]   And I think more people are losing a little bit of patience with like, okay, image classification,
[00:26:55.700 --> 00:26:59.300]   like facial recognition is just another flavor of image classification.
[00:26:59.300 --> 00:27:05.780]   We can do all this pretty well, but we are really like bending our universe around this
[00:27:05.780 --> 00:27:09.260]   one point, why not branch out a little bit.
[00:27:09.260 --> 00:27:14.500]   And as we're willing then to cover a little bit more of the space of the problems you
[00:27:14.500 --> 00:27:19.820]   have to solve, we'll get robots who, you know, like the Roomba, you might not watch it and
[00:27:19.820 --> 00:27:24.620]   think like, man, that's like vacuuming more efficiently than I would be watching.
[00:27:24.620 --> 00:27:28.020]   You think like, okay, it's getting to all the corners, all the edges covering everything
[00:27:28.020 --> 00:27:29.020]   in its own time.
[00:27:29.020 --> 00:27:33.580]   Like, great, I can go off and have a coffee and be confident that it's going to do its
[00:27:33.580 --> 00:27:34.580]   job.
[00:27:34.580 --> 00:27:36.580]   I think we're going to get more and more of that.
[00:27:36.580 --> 00:27:37.580]   Cool.
[00:27:37.580 --> 00:27:38.580]   That's so cool.
[00:27:38.580 --> 00:27:44.580]   And I got to say, you know, I think the biggest pleasure of doing an ML tools company like
[00:27:44.580 --> 00:27:48.540]   we're doing is getting to talk to guys like you who are actually kind of taking these
[00:27:48.540 --> 00:27:53.660]   applications into, or taking the technology of machine learning into these applications
[00:27:53.660 --> 00:27:55.140]   where you like really see them.
[00:27:55.140 --> 00:27:58.540]   And it's so cool to just, you know, kind of see machine learning going everywhere.
[00:27:58.540 --> 00:28:01.260]   So I share your excitement.
[00:28:01.260 --> 00:28:02.260]   We should bottle up that.
[00:28:02.260 --> 00:28:04.940]   I feel like your answer to why a robot is cool.
[00:28:04.940 --> 00:28:10.940]   Like we should make that like a video on YouTube and everyone will want to know it's a robot.
[00:28:10.940 --> 00:28:13.260]   It's so well articulated and enthusiastic.
[00:28:13.260 --> 00:28:14.260]   Thanks.
[00:28:14.260 --> 00:28:17.300]   Oh man, I would be all behind that.
[00:28:17.300 --> 00:28:19.700]   I remember I watched Return of the Jedi in the theaters.
[00:28:19.700 --> 00:28:22.460]   I think I might be like two or three years younger than you.
[00:28:22.460 --> 00:28:25.820]   It left me like catatonic and like nightmares for years.
[00:28:25.820 --> 00:28:29.620]   So it's like a pretty different experience.
[00:28:29.620 --> 00:28:31.980]   Yeah, no, no.
[00:28:31.980 --> 00:28:38.940]   Actually Han getting frozen in carbonite was not a traumatic thing for me at the time.
[00:28:38.940 --> 00:28:42.660]   So what do you think is one underrated aspect of machine learning that you think people
[00:28:42.660 --> 00:28:44.660]   should pay more attention to?
[00:28:44.660 --> 00:28:49.220]   The next neighbor that I'm really excited about, so we have like, you know, image classification,
[00:28:49.220 --> 00:28:55.060]   also some really cool things with word prediction are happening.
[00:28:55.060 --> 00:29:02.740]   Type for the plucking is unsupervised methods, being able to automatically do clustering,
[00:29:02.740 --> 00:29:08.340]   automatically learn the similarities between things that are, you know, may have many variables,
[00:29:08.340 --> 00:29:15.260]   might be really complex, but to be able to say like, I've never seen this situation before,
[00:29:15.260 --> 00:29:19.260]   but it's kind of like this one I saw in the past and to be able to make use of that, what
[00:29:19.260 --> 00:29:21.100]   we've seen before.
[00:29:21.100 --> 00:29:26.100]   I think that there's a lot of work that could be done there for a modest amount of effort.
[00:29:26.100 --> 00:29:30.480]   And it suffers mostly from the fact that there's no one right answer.
[00:29:30.480 --> 00:29:33.420]   And so it doesn't lend itself to benchmarks.
[00:29:33.420 --> 00:29:37.300]   But you know, if anyone hearing this wants to say, you know, screw benchmarks, I'm going
[00:29:37.300 --> 00:29:42.220]   to go work with unsupervised learning, like, I expect it would be a really fruitful way
[00:29:42.220 --> 00:29:43.220]   to spend your time.
[00:29:44.220 --> 00:29:45.220]   Interesting.
[00:29:45.220 --> 00:29:49.220]   I want to, I want to debate you, but we're running out of time.
[00:29:49.220 --> 00:29:50.220]   Intriguing.
[00:29:50.220 --> 00:29:51.220]   All right.
[00:29:51.220 --> 00:29:57.260]   So next question is, what is the biggest challenge of machine learning in the real world?
[00:29:57.260 --> 00:30:04.900]   If I had to pick one that is the biggest in terms of impact, it is misapplication.
[00:30:04.900 --> 00:30:11.940]   It is easy to, you know, treat it like a hammer and beat on anything with it without regards
[00:30:11.940 --> 00:30:15.460]   to regard to whether the hammer is the right tool for that.
[00:30:15.460 --> 00:30:24.180]   And so we see places where, you know, models are trained on a grab bag of data about people
[00:30:24.180 --> 00:30:33.620]   that can transfer biases and transfer historical injustices, because those are the processes
[00:30:33.620 --> 00:30:35.160]   that generated this data.
[00:30:35.160 --> 00:30:39.860]   And the new model will just know blithely perpetuate that.
[00:30:39.860 --> 00:30:46.340]   And there are few people who are kind of intellectually in a position to see how that works.
[00:30:46.340 --> 00:30:53.020]   Some of them are wonderfully vocal, but it's still not all of them.
[00:30:53.020 --> 00:30:59.020]   And I think that the biggest downside, the biggest difficulty is that those who don't
[00:30:59.020 --> 00:31:05.980]   know or don't want to know about that will continue to use and perpetuate these to end
[00:31:05.980 --> 00:31:06.980]   up hurting people.
[00:31:07.100 --> 00:31:13.460]   Do you have a particular example that specifically bothers you or that you'd want to call out?
[00:31:13.460 --> 00:31:19.180]   So facial recognition in law enforcement is one that comes to mind right away.
[00:31:19.180 --> 00:31:23.020]   It is demonstrably inaccurate.
[00:31:23.020 --> 00:31:30.380]   And especially for non-white minorities, the accuracy is even worse than average.
[00:31:30.380 --> 00:31:36.540]   And so it's just a way to cause many more problems than it solves.
[00:31:36.540 --> 00:31:41.780]   On the surface, especially when sold the right way, it appears to be a useful tool and you
[00:31:41.780 --> 00:31:48.420]   can make lots of great claims about it, but that washes over the harms that it does.
[00:31:48.420 --> 00:31:56.380]   And that's not even touching facial recognition used for like overtly discriminatory purposes,
[00:31:56.380 --> 00:32:01.380]   which is completely unethical in my opinion.
[00:32:01.380 --> 00:32:06.940]   All right, well, let's close with one final question, which is where can people find you
[00:32:06.940 --> 00:32:09.020]   if they want to keep this conversation going?
[00:32:09.020 --> 00:32:10.540]   What's the best way for them to reach you?
[00:32:10.540 --> 00:32:11.540]   Yeah.
[00:32:11.540 --> 00:32:14.300]   So I'm online, fairly active on Twitter.
[00:32:14.300 --> 00:32:21.540]   Handle is underscore B-R-O-H-R-E-R underscore.
[00:32:21.540 --> 00:32:25.020]   Also on LinkedIn, regular posts, Brandon Rohr.
[00:32:25.020 --> 00:32:32.380]   And a lot of my choices stuff, my labor of love goes to the end to end machine learning
[00:32:32.380 --> 00:32:40.060]   school, some course materials I put online and that's at e2eml.school.
[00:32:40.060 --> 00:32:41.060]   So e2eml.school.
[00:32:41.060 --> 00:32:42.060]   Great, cool.
[00:32:42.060 --> 00:32:46.460]   We can put all these in the notes too so people can find them easily.
[00:32:46.460 --> 00:32:47.460]   Fantastic.
[00:32:47.460 --> 00:32:48.460]   That was awesome, man.
[00:32:48.460 --> 00:32:49.460]   That was so fun.
[00:32:49.460 --> 00:32:50.460]   Thank you so much.
[00:32:50.460 --> 00:32:51.460]   Thanks, Lucas.
[00:32:51.460 --> 00:32:52.460]   I really enjoyed the conversation.
[00:32:52.460 --> 00:32:54.580]   I appreciate you and Lavanya setting it up.
[00:32:54.580 --> 00:32:55.580]   All right.
[00:32:55.580 --> 00:32:58.100]   That was such a great conversation.
[00:32:58.100 --> 00:32:59.660]   Thank you, Brandon and Lucas.
[00:32:59.660 --> 00:33:04.820]   I'm going to add a link to Brandon's Twitter account and also to his course in the show
[00:33:04.820 --> 00:33:05.820]   notes below.
[00:33:05.820 --> 00:33:08.700]   I highly recommend that you guys check it out.
[00:33:08.700 --> 00:33:13.740]   If you'd like to continue the conversation, we do have a very active Slack community with
[00:33:13.740 --> 00:33:18.180]   over a thousand machine learning engineers, and I'd love to see you guys there.
[00:33:18.180 --> 00:33:22.860]   I'll add a link to the Slack community in the show notes below.
[00:33:22.860 --> 00:33:27.000]   Before we end for the day, I'd love to talk to you guys about something that I'm super
[00:33:27.000 --> 00:33:28.700]   excited about.
[00:33:28.700 --> 00:33:34.860]   So at Weights & Biases, we love traditional machine learning as much as we love deep learning.
[00:33:34.860 --> 00:33:41.060]   So we built a scikit-learn integration that lets you track your model performance, compare
[00:33:41.060 --> 00:33:44.340]   different models, and be able to pick the best model.
[00:33:44.340 --> 00:33:51.100]   We also help you do hyperparameter sweeps on your scikit-learn models that let you find
[00:33:51.100 --> 00:33:53.660]   the best iteration of your scikit model.
[00:33:53.660 --> 00:34:00.340]   With one line of code, you're able to create really cool plots like the ROC curves, precision
[00:34:00.340 --> 00:34:07.380]   recall plots, learning curves, confusion matrices, calibration curves, and a lot of really interesting
[00:34:07.380 --> 00:34:10.980]   classification, regression, and clustering plots.
[00:34:10.980 --> 00:34:16.140]   I'll add a link to the show notes below so you can get started right away.
[00:34:16.140 --> 00:34:20.340]   I would love to have you guys give it a try and tell us what you think.
[00:34:20.340 --> 00:34:25.220]   We're always trying to make our product better, and I would love to hear feedback from you
[00:34:25.220 --> 00:34:26.220]   guys.
[00:34:26.220 --> 00:34:27.220]   That's all for today.
[00:34:27.220 --> 00:34:30.220]   We'll see you next time with another great episode.

