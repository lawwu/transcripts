
[00:00:00.000 --> 00:00:01.380]   In this video, we're going to talk
[00:00:01.380 --> 00:00:04.260]   about text classification with neural nets,
[00:00:04.260 --> 00:00:06.460]   but actually not with LCMs, not today.
[00:00:06.460 --> 00:00:08.340]   Today it's going to be text classification
[00:00:08.340 --> 00:00:11.460]   with convolutional neural networks.
[00:00:11.460 --> 00:00:14.220]   And I think it's super cool because it wasn't obvious to me
[00:00:14.220 --> 00:00:16.420]   at first how you would take the convolutions that we
[00:00:16.420 --> 00:00:18.780]   do on 2D images and apply it to text,
[00:00:18.780 --> 00:00:20.020]   but actually you really can.
[00:00:20.020 --> 00:00:22.140]   And you can take all the things that we've learned,
[00:00:22.140 --> 00:00:24.740]   all the intuitions we have with processing images,
[00:00:24.740 --> 00:00:25.540]   and use it on text.
[00:00:25.540 --> 00:00:29.300]   So things like max pooling totally applies to text.
[00:00:29.300 --> 00:00:30.540]   And this is really practical.
[00:00:30.540 --> 00:00:32.780]   A lot of people do this in the real world.
[00:00:32.780 --> 00:00:35.100]   This is something that Facebook uses
[00:00:35.100 --> 00:00:38.060]   to do a lot of their text classification.
[00:00:38.060 --> 00:00:39.900]   And also, in order to do it, we're
[00:00:39.900 --> 00:00:41.660]   going to learn about embeddings.
[00:00:41.660 --> 00:00:44.140]   And I think embeddings are one of the most interesting,
[00:00:44.140 --> 00:00:47.060]   coolest topics in all of natural language processing.
[00:00:47.060 --> 00:00:48.820]   And we'll go deep on it in a later video,
[00:00:48.820 --> 00:00:50.180]   but the first time you see embeddings,
[00:00:50.180 --> 00:00:51.220]   I think it's pretty cool.
[00:00:51.220 --> 00:00:54.980]   So the big problem with using neural nets on text
[00:00:54.980 --> 00:00:56.900]   is that it's kind of hard to get things
[00:00:56.900 --> 00:01:01.060]   from text, which is arbitrary length strings,
[00:01:01.060 --> 00:01:03.620]   into the API that I'm always talking about,
[00:01:03.620 --> 00:01:06.300]   the specific API for any of these neural networks, which
[00:01:06.300 --> 00:01:08.380]   is basically a fixed length string.
[00:01:08.380 --> 00:01:10.500]   One of the ways to do it, the kind of most common way
[00:01:10.500 --> 00:01:13.740]   to do it circa 10 years ago, still super popular,
[00:01:13.740 --> 00:01:15.580]   is called bag of words.
[00:01:15.580 --> 00:01:17.980]   And we cover that in the first video
[00:01:17.980 --> 00:01:19.540]   that I did on text classification
[00:01:19.540 --> 00:01:21.660]   without neural nets.
[00:01:21.660 --> 00:01:24.980]   And this is, you might recall, just taking each word
[00:01:24.980 --> 00:01:27.780]   and counting how many times it occurs in each document.
[00:01:27.780 --> 00:01:30.820]   So you basically transform the string
[00:01:30.820 --> 00:01:32.820]   into a vector where the length is
[00:01:32.820 --> 00:01:34.580]   the number of words you have.
[00:01:34.580 --> 00:01:36.140]   And the problem, of course, with this
[00:01:36.140 --> 00:01:39.300]   is that you completely lose the order of words.
[00:01:39.300 --> 00:01:41.940]   So in most languages, definitely in English,
[00:01:41.940 --> 00:01:43.460]   the order of words actually matters.
[00:01:43.460 --> 00:01:45.540]   And so dropping that order, it's kind of amazing
[00:01:45.540 --> 00:01:47.340]   that classification can work at all.
[00:01:47.340 --> 00:01:50.860]   And certainly, it can't work as good as it could.
[00:01:50.860 --> 00:01:53.140]   So there's another transformation
[00:01:53.140 --> 00:01:55.620]   that we talked a lot about in the previous video
[00:01:55.620 --> 00:01:58.100]   where we're generating text with LSTMs.
[00:01:58.100 --> 00:02:00.460]   And that's using the individual characters,
[00:02:00.460 --> 00:02:04.340]   so basically one-hot encoding every character in the text.
[00:02:04.340 --> 00:02:05.960]   And that kind of makes intuitive sense.
[00:02:05.960 --> 00:02:07.580]   It's like a nice transformation.
[00:02:07.580 --> 00:02:09.660]   And we can actually pad out the characters
[00:02:09.660 --> 00:02:12.700]   to make all the text a fixed length of characters.
[00:02:12.700 --> 00:02:15.740]   But the problem there is that actually, in English,
[00:02:15.740 --> 00:02:17.300]   spaces matter a lot.
[00:02:17.300 --> 00:02:20.360]   The concept of word is a pretty useful concept
[00:02:20.360 --> 00:02:23.100]   that we might want to pass into our neural network.
[00:02:23.100 --> 00:02:26.020]   And so by just passing in each character
[00:02:26.020 --> 00:02:27.820]   as a character encoding, we're really
[00:02:27.820 --> 00:02:30.900]   making the neural network learn a lot about language.
[00:02:30.900 --> 00:02:32.860]   So it might be too raw, too extreme,
[00:02:32.860 --> 00:02:36.860]   unless we have truly massive amounts of data.
[00:02:36.860 --> 00:02:41.240]   So for best results, we really want something in between.
[00:02:41.240 --> 00:02:43.500]   And that's where word encodings come in.
[00:02:43.500 --> 00:02:44.580]   Here we have an example.
[00:02:44.580 --> 00:02:46.780]   We have the sentence, "I love this movie."
[00:02:46.780 --> 00:02:51.020]   And we're transforming each word systematically,
[00:02:51.020 --> 00:02:53.380]   deterministically, into a set of numbers.
[00:02:53.380 --> 00:02:55.060]   And in this case, we're transforming it
[00:02:55.060 --> 00:02:56.460]   into four numbers.
[00:02:56.460 --> 00:02:58.660]   So the word "I" always transforms
[00:02:58.660 --> 00:02:59.840]   into the same four numbers.
[00:02:59.840 --> 00:03:01.660]   The word "love" always transforms
[00:03:01.660 --> 00:03:03.340]   into the same four numbers.
[00:03:03.340 --> 00:03:06.060]   And we can do this with longer vectors.
[00:03:06.060 --> 00:03:08.620]   So in this table, we have each word
[00:03:08.620 --> 00:03:10.580]   that we might have seen in our vocabulary,
[00:03:10.580 --> 00:03:14.380]   and then the transform that each word gets turned into.
[00:03:14.380 --> 00:03:17.460]   So if we don't want to calculate the embedding ourselves,
[00:03:17.460 --> 00:03:19.940]   we can actually use some of the pre-computed embedding.
[00:03:19.940 --> 00:03:21.820]   So Word2Vec is a famous one.
[00:03:21.820 --> 00:03:24.260]   Today we're going to use the GloVe embedding.
[00:03:24.260 --> 00:03:26.060]   And this is something generated by Stanford
[00:03:26.060 --> 00:03:27.700]   on a huge set of data.
[00:03:27.700 --> 00:03:29.500]   And it actually has some amazing properties.
[00:03:29.500 --> 00:03:31.180]   They're incredible properties that
[00:03:31.180 --> 00:03:33.260]   would be worth a whole video to explore more.
[00:03:33.260 --> 00:03:34.800]   You can do some research on your own.
[00:03:34.800 --> 00:03:37.340]   But basically, if you take the actual embeddings--
[00:03:37.340 --> 00:03:39.740]   these are the actual numbers for woman--
[00:03:39.740 --> 00:03:42.140]   and you subtract from those numbers the set of numbers
[00:03:42.140 --> 00:03:45.100]   that got encoded as "man," and then you
[00:03:45.100 --> 00:03:48.900]   add in the set of numbers for "king,"
[00:03:48.900 --> 00:03:50.820]   you actually get the set of numbers
[00:03:50.820 --> 00:03:54.620]   or a set of numbers that's very close to the numbers for "queen."
[00:03:54.620 --> 00:03:56.100]   So this is really incredible.
[00:03:56.100 --> 00:03:58.460]   And what it shows is that these embeddings are actually
[00:03:58.460 --> 00:04:03.060]   encoding some semantic information about these words.
[00:04:03.060 --> 00:04:04.740]   And so actually using these embeddings,
[00:04:04.740 --> 00:04:07.440]   using these numbers that are pre-generated by Stanford,
[00:04:07.440 --> 00:04:09.020]   in this case, that you can download,
[00:04:09.020 --> 00:04:11.940]   can often make your models perform even better
[00:04:11.940 --> 00:04:14.100]   than trying to calculate these embeddings yourself.
[00:04:14.100 --> 00:04:17.980]   This is kind of like transfer learning, but for words,
[00:04:17.980 --> 00:04:19.060]   if that makes sense.
[00:04:19.060 --> 00:04:20.620]   So once we have these embeddings,
[00:04:20.620 --> 00:04:23.220]   once we've transformed all of our words
[00:04:23.220 --> 00:04:26.500]   into these fixed-length vectors--
[00:04:26.500 --> 00:04:29.300]   and remember that it has to be a fixed number of fixed-length
[00:04:29.300 --> 00:04:31.220]   vectors, so we actually have to transform--
[00:04:31.220 --> 00:04:34.180]   we have to add padding words to make
[00:04:34.180 --> 00:04:35.420]   each document the same length.
[00:04:35.420 --> 00:04:39.680]   Once we've done this, how do we turn it into a classifier?
[00:04:39.680 --> 00:04:41.420]   How do we make a convolutional classifier?
[00:04:41.420 --> 00:04:42.940]   What would that even mean?
[00:04:42.940 --> 00:04:45.220]   So I think it's useful to go back and review
[00:04:45.220 --> 00:04:47.660]   what we meant by a two-dimensional classifier
[00:04:47.660 --> 00:04:49.180]   and images.
[00:04:49.180 --> 00:04:51.580]   So remember that with a 2D classifier,
[00:04:51.580 --> 00:04:53.380]   we would take an input, and then we
[00:04:53.380 --> 00:04:57.820]   would multiply a weight by a block of values.
[00:04:57.820 --> 00:05:00.300]   And we put the weighted sum of that block
[00:05:00.300 --> 00:05:03.140]   into a subsequent output image.
[00:05:03.140 --> 00:05:06.060]   And then we move that block over by one or over by a stride.
[00:05:06.060 --> 00:05:07.620]   And then we do that same computation
[00:05:07.620 --> 00:05:08.660]   with the same weights.
[00:05:08.660 --> 00:05:11.220]   And then we fill in the next block over in the image.
[00:05:11.220 --> 00:05:13.600]   And remember that we can actually have multiple outputs.
[00:05:13.600 --> 00:05:15.500]   And what multiple outputs would mean
[00:05:15.500 --> 00:05:17.140]   is that we start with the same image,
[00:05:17.140 --> 00:05:19.420]   but we use different sets of weights.
[00:05:19.420 --> 00:05:21.260]   And so as we slide the block over,
[00:05:21.260 --> 00:05:23.260]   we're actually, in each case, multiplying it
[00:05:23.260 --> 00:05:26.740]   by different weights and then outputting multiple images.
[00:05:26.740 --> 00:05:28.820]   Or sometimes we call it multiple channels.
[00:05:28.820 --> 00:05:30.900]   And then you might have missed-- a lot of students
[00:05:30.900 --> 00:05:32.980]   missed this, how this exactly works.
[00:05:32.980 --> 00:05:35.340]   But you can actually take in multiple inputs.
[00:05:35.340 --> 00:05:38.620]   So if we had three input images-- in this case,
[00:05:38.620 --> 00:05:40.340]   actually, if we have a color image,
[00:05:40.340 --> 00:05:42.660]   we might turn it into three channels-- a red channel,
[00:05:42.660 --> 00:05:43.920]   green channel, blue channel.
[00:05:43.920 --> 00:05:46.640]   We can do the same thing with a convolution.
[00:05:46.640 --> 00:05:48.280]   And in this case, we actually have
[00:05:48.280 --> 00:05:51.080]   three different blocks of weights.
[00:05:51.080 --> 00:05:54.840]   And then we sum the result of the convolution
[00:05:54.840 --> 00:05:57.240]   of each block of weights on each one of the input channels.
[00:05:57.240 --> 00:05:59.200]   And we have a single output channel.
[00:05:59.200 --> 00:06:02.200]   So we can have multiple inputs and multiple outputs
[00:06:02.200 --> 00:06:03.420]   in this way.
[00:06:03.420 --> 00:06:05.360]   And now in text, we actually don't
[00:06:05.360 --> 00:06:06.560]   have a two-dimensional thing.
[00:06:06.560 --> 00:06:08.680]   We have a one-dimensional thing.
[00:06:08.680 --> 00:06:12.000]   So here you can think of that one dimension going across
[00:06:12.000 --> 00:06:14.040]   as the pixels of an image.
[00:06:14.040 --> 00:06:17.160]   And you can think of what I have as the y dimension
[00:06:17.160 --> 00:06:20.920]   here as actually the different channels.
[00:06:20.920 --> 00:06:23.120]   So instead of taking a two-dimensional block,
[00:06:23.120 --> 00:06:26.000]   we take a one-dimensional block across the pixels.
[00:06:26.000 --> 00:06:28.920]   So say, in this case, it's length three.
[00:06:28.920 --> 00:06:31.960]   And we take a weighted sum of each of the pixels.
[00:06:31.960 --> 00:06:33.880]   So in this case, we would have three weights.
[00:06:33.880 --> 00:06:36.360]   And we multiply them by one of the channels.
[00:06:36.360 --> 00:06:37.960]   And we take that weighted sum.
[00:06:37.960 --> 00:06:40.440]   And we fill in an output.
[00:06:40.440 --> 00:06:44.760]   And then we move that block one step over, or a stride step
[00:06:44.760 --> 00:06:45.600]   over to the right.
[00:06:45.600 --> 00:06:47.600]   And then we do the same weighted sum
[00:06:47.600 --> 00:06:50.160]   on the new data from our embedding.
[00:06:50.160 --> 00:06:52.960]   And we fill in the result in the next channel,
[00:06:52.960 --> 00:06:56.760]   or the next pixel over.
[00:06:56.760 --> 00:06:58.320]   And we actually run that weighted sum
[00:06:58.320 --> 00:07:01.280]   across all of the channels.
[00:07:01.280 --> 00:07:02.280]   And we take the sum.
[00:07:02.280 --> 00:07:03.360]   And we fill in one value.
[00:07:03.360 --> 00:07:05.400]   And now we could have multiple dimension output
[00:07:05.400 --> 00:07:06.920]   or multiple channel output.
[00:07:06.920 --> 00:07:08.520]   And in that case, we would just have
[00:07:08.520 --> 00:07:10.720]   different sets of weights for each of the channel
[00:07:10.720 --> 00:07:11.880]   that we're outputting.
[00:07:11.880 --> 00:07:13.540]   And in this case, we're actually going
[00:07:13.540 --> 00:07:16.960]   to learn the weights for all these different channels.
[00:07:16.960 --> 00:07:20.400]   And what this is going to do is combine the words
[00:07:20.400 --> 00:07:23.240]   into smaller values.
[00:07:23.240 --> 00:07:25.360]   It's in some sense going to give us information,
[00:07:25.360 --> 00:07:28.040]   or it's hopefully going to learn information about pairs
[00:07:28.040 --> 00:07:30.640]   and triples and more of words.
[00:07:30.640 --> 00:07:32.440]   So you might remember, with images,
[00:07:32.440 --> 00:07:35.320]   we would do this thing called a max pooling operation,
[00:07:35.320 --> 00:07:38.360]   where we would take a block, typically a two-by-two block,
[00:07:38.360 --> 00:07:43.120]   and we'd find the max of the pixels in a two-by-two region.
[00:07:43.120 --> 00:07:45.920]   Well, there's actually a really obvious 1D analogy
[00:07:45.920 --> 00:07:48.280]   to this, where we look at any particular channel.
[00:07:48.280 --> 00:07:50.840]   And we take, in this case, not a two-by-two,
[00:07:50.840 --> 00:07:52.360]   but just a length two, or it could
[00:07:52.360 --> 00:07:53.760]   be a different length block.
[00:07:53.760 --> 00:07:54.900]   And we find the max.
[00:07:54.900 --> 00:07:57.920]   Or in average pooling case, we find the average.
[00:07:57.920 --> 00:08:00.140]   And with images, this gave us a chance
[00:08:00.140 --> 00:08:02.240]   to find longer range dependencies
[00:08:02.240 --> 00:08:03.240]   with our convolutions.
[00:08:03.240 --> 00:08:06.080]   And with tests, it's exactly the same thing.
[00:08:06.080 --> 00:08:08.520]   So we can actually build up the same structure
[00:08:08.520 --> 00:08:12.360]   that we had for classifying digits with 2D operations
[00:08:12.360 --> 00:08:15.160]   with 1D operations on our text.
[00:08:15.160 --> 00:08:17.520]   So it's typical to have a convolution followed
[00:08:17.520 --> 00:08:19.800]   by some kind of pooling, followed by a convolution,
[00:08:19.800 --> 00:08:22.160]   followed by some kind of pooling.
[00:08:22.160 --> 00:08:25.560]   So let's go to the code and see how this really works.
[00:08:25.560 --> 00:08:28.960]   As usual, go into the Videos directory in ML class.
[00:08:28.960 --> 00:08:32.920]   And then go into CNN-text.
[00:08:32.920 --> 00:08:38.680]   And then open up imdb-cnn.py.
[00:08:38.680 --> 00:08:40.300]   And let's take a look at what we have.
[00:08:40.300 --> 00:08:44.200]   So the first 11 lines are just importing various libraries.
[00:08:44.200 --> 00:08:47.000]   And then line 17 through 24 basically
[00:08:47.000 --> 00:08:49.880]   sets some configuration parameters.
[00:08:49.880 --> 00:08:52.280]   One configuration parameter to point out here is the vocab
[00:08:52.280 --> 00:08:52.780]   size.
[00:08:52.780 --> 00:08:54.120]   So that's 1,000.
[00:08:54.120 --> 00:08:57.320]   And that's because our embedding has
[00:08:57.320 --> 00:08:59.920]   to take a fixed-length set of words
[00:08:59.920 --> 00:09:02.620]   and figure out what they map to.
[00:09:02.620 --> 00:09:04.800]   So in this case, we can only handle 1,000 words.
[00:09:04.800 --> 00:09:07.760]   So any words that are less frequent than the top 1,000
[00:09:07.760 --> 00:09:10.680]   are actually going to get removed from our data.
[00:09:10.680 --> 00:09:14.600]   So line 26 loads in the IMDb data.
[00:09:14.600 --> 00:09:18.040]   So I have an IMDb data set that's actually quite famous.
[00:09:18.040 --> 00:09:20.240]   And these are basically movie reviews.
[00:09:20.240 --> 00:09:23.320]   So if you run download-imdb, you'll
[00:09:23.320 --> 00:09:24.700]   get lots of movie reviews.
[00:09:24.700 --> 00:09:27.760]   And the idea is to classify just from the text of the movie
[00:09:27.760 --> 00:09:31.200]   review if the movie was positively reviewed
[00:09:31.200 --> 00:09:32.560]   or negatively reviewed.
[00:09:32.560 --> 00:09:34.480]   And neutral reviews are actually removed.
[00:09:34.480 --> 00:09:36.000]   So all these reviews are quite clear
[00:09:36.000 --> 00:09:38.480]   if they're positive or negative.
[00:09:38.480 --> 00:09:41.920]   So we're going to magically load that data into x_train,
[00:09:41.920 --> 00:09:46.040]   y_train, x_test, and y_test, which you might remember
[00:09:46.040 --> 00:09:50.120]   from MNIST is basically the training input and the training
[00:09:50.120 --> 00:09:52.840]   target, and then the validation input and the validation
[00:09:52.840 --> 00:09:54.400]   target.
[00:09:54.400 --> 00:09:57.480]   So in this case, the y value is actually only 0 or 1,
[00:09:57.480 --> 00:09:59.400]   basically negative or positive.
[00:09:59.400 --> 00:10:02.080]   There are no neutral reviews in this data set.
[00:10:02.080 --> 00:10:06.480]   Lines 28 through 31 basically turn this text into numbers.
[00:10:06.480 --> 00:10:10.840]   So the first thing that happens is line 28 sets up a tokenizer.
[00:10:10.840 --> 00:10:13.600]   And the important parameter passed into the tokenizer
[00:10:13.600 --> 00:10:15.840]   is the number of words that we're going to look at.
[00:10:15.840 --> 00:10:19.840]   So anything outside the top 1,000 most common words
[00:10:19.840 --> 00:10:20.880]   is going to get removed.
[00:10:20.880 --> 00:10:24.240]   And that was set in config.vocab_size.
[00:10:24.240 --> 00:10:28.280]   Line 29 does this actual fit on the text.
[00:10:28.280 --> 00:10:31.520]   And in this case, it's x_train that we fit on.
[00:10:31.520 --> 00:10:34.320]   So that looks at what actually are the most popular words.
[00:10:34.320 --> 00:10:37.760]   And then lines 30 and 31 do the transformation
[00:10:37.760 --> 00:10:42.040]   from the actual strings into numbers.
[00:10:42.040 --> 00:10:46.840]   In this case, it transforms them into a one-hot encoding
[00:10:46.840 --> 00:10:48.840]   based on the top 1,000 words.
[00:10:48.840 --> 00:10:52.560]   So the rows here are the individual words,
[00:10:52.560 --> 00:10:55.760]   and the columns are words in the text.
[00:10:55.760 --> 00:10:59.840]   Lines 33 and 34 take the x_train and x_test values,
[00:10:59.840 --> 00:11:01.680]   and they actually pad out the sequences.
[00:11:01.680 --> 00:11:03.320]   So this actually adds essentially
[00:11:03.320 --> 00:11:05.360]   empty words to the text.
[00:11:05.360 --> 00:11:08.480]   So the input to our model is all the same length.
[00:11:08.480 --> 00:11:11.320]   So there is a maximum length that we have to give to this,
[00:11:11.320 --> 00:11:13.440]   and that's in config.maxlen.
[00:11:13.440 --> 00:11:16.200]   In this case, the longest review that we're going to consider
[00:11:16.200 --> 00:11:17.480]   is 1,000-word review.
[00:11:17.480 --> 00:11:19.640]   So we could try to change that to a longer value
[00:11:19.640 --> 00:11:20.900]   and see if it matters.
[00:11:20.900 --> 00:11:22.280]   But in this case, all reviews are
[00:11:22.280 --> 00:11:23.860]   going to get truncated to 1,000 words.
[00:11:23.860 --> 00:11:26.520]   And we have to set that to something.
[00:11:26.520 --> 00:11:28.400]   Line 36 sets up this model.
[00:11:28.400 --> 00:11:31.360]   And then line 37 actually does a new kind of layer
[00:11:31.360 --> 00:11:34.040]   that we haven't seen before called an embedding layer.
[00:11:34.040 --> 00:11:38.080]   And this embedding layer takes as input the vocab size,
[00:11:38.080 --> 00:11:40.200]   because each word is going to be an input to this.
[00:11:40.200 --> 00:11:41.540]   So we're going to take the top 1,000 words
[00:11:41.540 --> 00:11:43.160]   and find an embedding for them.
[00:11:43.160 --> 00:11:46.040]   And the second input is the embedding dimension.
[00:11:46.040 --> 00:11:48.160]   So the bigger we make the embedding dimension,
[00:11:48.160 --> 00:11:50.920]   the more numbers we're transforming a word into.
[00:11:50.920 --> 00:11:53.760]   So if this is really big, our model might get too complicated.
[00:11:53.760 --> 00:11:56.320]   It might overfit or something like that.
[00:11:56.320 --> 00:11:58.480]   If this value is too small, then we
[00:11:58.480 --> 00:12:00.640]   might lose the information in the words,
[00:12:00.640 --> 00:12:02.760]   and our model may underfit the data.
[00:12:02.760 --> 00:12:04.840]   Then we add a dropout layer to prevent overfitting.
[00:12:04.840 --> 00:12:06.360]   You probably remember this from some
[00:12:06.360 --> 00:12:09.640]   of the image-based neural networks we were building.
[00:12:09.640 --> 00:12:11.840]   So then we add a conv 1D layer.
[00:12:11.840 --> 00:12:14.080]   This is just like the conv 2D layers
[00:12:14.080 --> 00:12:16.640]   that we're using on digit recognition or fashion
[00:12:16.640 --> 00:12:19.360]   recognition or any kind of image recognition.
[00:12:19.360 --> 00:12:21.400]   And again, we have this filters parameter,
[00:12:21.400 --> 00:12:24.200]   which is how many output channels this convolution
[00:12:24.200 --> 00:12:25.200]   layer has.
[00:12:25.200 --> 00:12:26.920]   And we also have a kernel size parameter.
[00:12:26.920 --> 00:12:29.440]   But instead of the kernel size being two numbers,
[00:12:29.440 --> 00:12:30.860]   it's one number because it's only
[00:12:30.860 --> 00:12:32.320]   a one-dimensional convolution.
[00:12:32.320 --> 00:12:33.780]   Padding equals valid basically means
[00:12:33.780 --> 00:12:36.720]   that we do no padding to our convolution.
[00:12:36.720 --> 00:12:39.120]   So it's actually going to shrink our output a little bit.
[00:12:39.120 --> 00:12:40.680]   And then activation equals relu means
[00:12:40.680 --> 00:12:44.200]   that we run a relu activation function
[00:12:44.200 --> 00:12:46.760]   at the end of this convolution.
[00:12:46.760 --> 00:12:48.320]   Then we have a max pooling layer.
[00:12:48.320 --> 00:12:51.160]   And then we go back to another convolutional layer,
[00:12:51.160 --> 00:12:52.800]   and then another max pooling layer.
[00:12:52.800 --> 00:12:54.920]   And then you might remember we do a flatten,
[00:12:54.920 --> 00:12:58.560]   and then a dense layer, and then finally one more dense layer.
[00:12:58.560 --> 00:13:01.800]   So this is just like the digit recognition classifier
[00:13:01.800 --> 00:13:04.080]   that we built earlier in this series.
[00:13:04.080 --> 00:13:06.680]   Now, because our model actually only outputs one number as
[00:13:06.680 --> 00:13:09.480]   opposed to two, but we're doing a two classifier,
[00:13:09.480 --> 00:13:11.480]   so there's positive and negative,
[00:13:11.480 --> 00:13:13.480]   we have to use binary cross entropy
[00:13:13.480 --> 00:13:15.600]   to properly calculate our loss as opposed
[00:13:15.600 --> 00:13:18.000]   to categorical cross entropy.
[00:13:18.000 --> 00:13:20.220]   We also use the atom optimizer, as we've mostly
[00:13:20.220 --> 00:13:22.060]   been using throughout these classes.
[00:13:22.060 --> 00:13:24.560]   And we also want to output the accuracy metric
[00:13:24.560 --> 00:13:26.480]   so we know how well our model's doing in sort
[00:13:26.480 --> 00:13:28.320]   of a human readable format.
[00:13:28.320 --> 00:13:31.640]   Then our last line calls model.fit with our X-train.
[00:13:31.640 --> 00:13:35.840]   And this is the input matrix.
[00:13:35.840 --> 00:13:40.920]   And Y-train is the classes, positive or negative sentiment.
[00:13:40.920 --> 00:13:42.720]   Our batch size is set in our configuration,
[00:13:42.720 --> 00:13:44.560]   and our epics is set in our configuration.
[00:13:44.560 --> 00:13:48.000]   And we also pass in our validation data.
[00:13:48.000 --> 00:13:49.080]   Let's run this model.
[00:13:52.960 --> 00:13:55.600]   So for example, here the validation loss
[00:13:55.600 --> 00:13:57.520]   is starting to go up, and the accuracy
[00:13:57.520 --> 00:13:59.920]   is starting to go down, which means that this model may
[00:13:59.920 --> 00:14:02.120]   be overfitting.
[00:14:02.120 --> 00:14:03.720]   And one thing to really be aware of
[00:14:03.720 --> 00:14:06.280]   is that the embedding adds a lot of free parameters.
[00:14:06.280 --> 00:14:09.360]   So if we look at the actual structure of this model,
[00:14:09.360 --> 00:14:11.560]   there's a lot of parameters contained
[00:14:11.560 --> 00:14:13.800]   within the embedding itself.
[00:14:13.800 --> 00:14:16.080]   So there's some extra things that we've made it learn.
[00:14:16.080 --> 00:14:18.200]   So it might be interesting to try
[00:14:18.200 --> 00:14:21.720]   to use the embedding that we can download from Stanford's
[00:14:21.720 --> 00:14:25.640]   website, the GloVe embedding that I talked about earlier.
[00:14:25.640 --> 00:14:27.280]   And you can see an example of where
[00:14:27.280 --> 00:14:30.920]   to do that in imdb-embedding.py.
[00:14:30.920 --> 00:14:33.300]   So at the top it tells you the first thing you need to do
[00:14:33.300 --> 00:14:39.120]   is actually download GloVe from the URL that I give you.
[00:14:39.120 --> 00:14:40.400]   So we'll go ahead and do that.
[00:14:49.360 --> 00:14:54.160]   So embedding is actually super similar to imdb-CNN,
[00:14:54.160 --> 00:14:56.160]   but there's a couple new lines I inserted here
[00:14:56.160 --> 00:14:59.520]   where we open up this embedding file, which is actually
[00:14:59.520 --> 00:15:01.200]   in a super simple format.
[00:15:01.200 --> 00:15:04.100]   And we pull out the words, and we pull out the numbers
[00:15:04.100 --> 00:15:07.040]   that these words correspond to.
[00:15:07.040 --> 00:15:09.440]   And then we take the embedding matrix,
[00:15:09.440 --> 00:15:12.640]   and we look inside the words that we have in our tokenizer,
[00:15:12.640 --> 00:15:17.760]   and we actually set the values inside of our embedding layer
[00:15:17.760 --> 00:15:20.960]   to be exactly the embeddings that we got from GloVe.
[00:15:20.960 --> 00:15:24.120]   And then in line 61, when we add our embedding layer,
[00:15:24.120 --> 00:15:27.080]   we actually set that trainable equals false.
[00:15:27.080 --> 00:15:30.200]   So we tell it its weights matrix by saying weights
[00:15:30.200 --> 00:15:32.080]   equals embedding matrix.
[00:15:32.080 --> 00:15:34.120]   And then we set trainable equal to false,
[00:15:34.120 --> 00:15:36.440]   which reduces the number of free parameters,
[00:15:36.440 --> 00:15:40.260]   makes our model train faster, and potentially overfit less.
[00:15:40.260 --> 00:15:41.680]   So I really like this because we're
[00:15:41.680 --> 00:15:44.480]   using the fact that someone spent a lot, lot of time
[00:15:44.480 --> 00:15:46.080]   training these values.
[00:15:46.080 --> 00:15:48.400]   And they can make our model better and everyone else's
[00:15:48.400 --> 00:15:51.080]   model better.
[00:15:51.080 --> 00:15:55.520]   So we can run this with Python imdb-embedding.py.
[00:15:55.520 --> 00:15:57.880]   Cool, so we learned about two really important things.
[00:15:57.880 --> 00:15:59.680]   The first is we learned about how
[00:15:59.680 --> 00:16:03.000]   to use word embeddings, which is practical all over the place,
[00:16:03.000 --> 00:16:04.480]   not just in this application.
[00:16:04.480 --> 00:16:06.120]   It's super, super cool.
[00:16:06.120 --> 00:16:08.440]   And this is just one example of how to do it.
[00:16:08.440 --> 00:16:11.420]   And the second is we learned how to take convolutions
[00:16:11.420 --> 00:16:14.400]   and pooling and all the things that we did on images
[00:16:14.400 --> 00:16:17.720]   and apply them to text in a really, really practical way
[00:16:17.720 --> 00:16:22.280]   to get high accuracy on the IMDB sentiment data set.
[00:16:22.280 --> 00:16:25.240]   In the next video, we're going to learn how to take LSTMs
[00:16:25.240 --> 00:16:28.040]   and apply this to the same data set.

