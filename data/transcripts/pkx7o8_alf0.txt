
[00:00:00.000 --> 00:00:03.680]   like as we get started here.
[00:00:03.680 --> 00:00:07.560]   What other opportunities do you see for bringing NLP
[00:00:07.560 --> 00:00:12.360]   to bear on parts of ML where it's not used quite so much?
[00:00:12.360 --> 00:00:14.680]   I think the potential of language
[00:00:14.680 --> 00:00:19.560]   is that language has evolved for human communication
[00:00:19.560 --> 00:00:20.240]   and learning.
[00:00:20.240 --> 00:00:24.160]   And as a result, it's ruthlessly efficient for communicating
[00:00:24.160 --> 00:00:27.160]   only the things that humans find relevant in the world.
[00:00:27.160 --> 00:00:28.640]   So for example, we have this issue
[00:00:28.640 --> 00:00:31.160]   where we have vision models or we have natural language
[00:00:31.160 --> 00:00:33.920]   inference or processing models that latch
[00:00:33.920 --> 00:00:35.720]   onto spurious correlations.
[00:00:35.720 --> 00:00:37.520]   They care more about textures.
[00:00:37.520 --> 00:00:40.080]   They're vulnerable to these adversarial examples.
[00:00:40.080 --> 00:00:42.840]   And to me, that's like all signs that what the model is
[00:00:42.840 --> 00:00:44.920]   learning does not align with human intuitions
[00:00:44.920 --> 00:00:46.360]   about what we should learn.
[00:00:46.360 --> 00:00:48.720]   And so then the question is, can language actually
[00:00:48.720 --> 00:00:53.080]   be used as a framework for encouraging a model
[00:00:53.080 --> 00:00:55.080]   to learn the right abstractions?
[00:00:55.080 --> 00:00:57.760]   So to learn the relevant features of some input.
[00:00:57.760 --> 00:01:01.320]   So we teach a model that a dog is a dog because it has ears
[00:01:01.320 --> 00:01:02.800]   and it has four legs and whatnot,
[00:01:02.800 --> 00:01:05.640]   and not because the texture is a certain pattern
[00:01:05.640 --> 00:01:10.800]   or there's a certain RGB pixel value that is indicative.
[00:01:10.800 --> 00:01:15.960]   And so being able to learn the right kind of features
[00:01:15.960 --> 00:01:19.640]   and decomposing those features through language, I think,
[00:01:19.640 --> 00:01:21.680]   is super interesting.
[00:01:21.680 --> 00:01:22.880]   Yeah, I like that.
[00:01:22.880 --> 00:01:26.960]   It's sort of like a language briar.
[00:01:26.960 --> 00:01:29.800]   OK, well, so I'll go ahead and start us off here.
[00:01:29.800 --> 00:01:33.160]   So welcome to the Weights and Biases Deep Learning Salon.
[00:01:33.160 --> 00:01:36.160]   I am your regular host, Charles Frey.
[00:01:36.160 --> 00:01:38.920]   I've got with me Jesse Mew today.
[00:01:38.920 --> 00:01:42.240]   And I wanted to do a quick couple of announcements
[00:01:42.240 --> 00:01:43.520]   before we get started.
[00:01:43.520 --> 00:01:48.320]   First is announcing our speakers for the salon
[00:01:48.320 --> 00:01:49.280]   at the end of the month.
[00:01:49.280 --> 00:01:52.440]   So in two weeks, we'll have two speakers, Richard Crabe,
[00:01:52.440 --> 00:01:54.960]   who is the founder and CEO of Numeri,
[00:01:54.960 --> 00:02:00.480]   which is the world's biggest and, in his words,
[00:02:00.480 --> 00:02:03.080]   will soon be the world's last hedge fund.
[00:02:03.080 --> 00:02:06.440]   It's a collaborative approach to solving
[00:02:06.440 --> 00:02:10.560]   the problem of pricing and investment with data science
[00:02:10.560 --> 00:02:13.520]   run by anonymous, essentially, data scientists,
[00:02:13.520 --> 00:02:15.240]   collaborating and competing in order
[00:02:15.240 --> 00:02:18.600]   to win cryptocurrency prizes.
[00:02:18.600 --> 00:02:20.840]   So they've got some new features they're rolling out,
[00:02:20.840 --> 00:02:22.240]   and he'll be on to talk about those
[00:02:22.240 --> 00:02:26.040]   and about the problem of non-stationary time series,
[00:02:26.040 --> 00:02:27.520]   which is really cool.
[00:02:27.520 --> 00:02:29.800]   Well, in addition, we'll have another co-founder,
[00:02:29.800 --> 00:02:34.160]   Alexa Milosevic, who is the co-founder of an NLP startup
[00:02:34.160 --> 00:02:36.440]   called Jarvis Management that's essentially
[00:02:36.440 --> 00:02:40.560]   applying NLP to the problems that basically Jira solves now.
[00:02:40.560 --> 00:02:41.960]   And he'll be talking, I think we'll
[00:02:41.960 --> 00:02:43.560]   get a little bit about Jarvis, but I
[00:02:43.560 --> 00:02:45.560]   think the main thing he plans to talk about
[00:02:45.560 --> 00:02:50.800]   is about making NLP applications work well and work fast.
[00:02:50.800 --> 00:02:54.400]   So I'm really excited to hear about that.
[00:02:54.400 --> 00:02:56.080]   We've got a lot of folks, I think,
[00:02:56.080 --> 00:02:59.920]   in the audience who are interested in applications
[00:02:59.920 --> 00:03:03.120]   and ML ops in getting good performance
[00:03:03.120 --> 00:03:07.600]   in actual applications built with ML.
[00:03:07.600 --> 00:03:12.000]   So I'm excited to hear about that from Alexa.
[00:03:12.000 --> 00:03:15.480]   Just a reminder, we push these salons up on YouTube.
[00:03:15.480 --> 00:03:18.160]   So if you head to our YouTube channel, Waits Biases,
[00:03:18.160 --> 00:03:20.120]   you can catch all the YouTube--
[00:03:20.120 --> 00:03:22.040]   sorry, the deep learning salons you've missed
[00:03:22.040 --> 00:03:24.400]   and a bunch of other things, including our podcast,
[00:03:24.400 --> 00:03:28.320]   Gradient Dissent, hosted by our CEO, Lucas Biewald,
[00:03:28.320 --> 00:03:31.640]   along with a bunch of other things, app tutorials and more.
[00:03:31.640 --> 00:03:34.440]   The podcast is really great.
[00:03:34.440 --> 00:03:36.520]   We had Jeremy Howard recently.
[00:03:36.520 --> 00:03:38.520]   We've had folks-- the most recent one
[00:03:38.520 --> 00:03:45.440]   was with the fairness lead at Facebook AI Research.
[00:03:45.440 --> 00:03:47.760]   Lots of really great stuff in there.
[00:03:47.760 --> 00:03:51.000]   We also have, as always, our Slack forum
[00:03:51.000 --> 00:03:55.760]   at bit.ly/slackforum, where you can participate
[00:03:55.760 --> 00:03:59.360]   in a bunch of AMAs with some other really great people
[00:03:59.360 --> 00:04:02.440]   in the ML community.
[00:04:02.440 --> 00:04:04.360]   We had the CEO of Kaggle, Anthony Goldblum,
[00:04:04.360 --> 00:04:11.080]   the author of Ludwig, which is Uber AI's auto ML solution.
[00:04:11.080 --> 00:04:14.160]   And so just come by our Slack forum.
[00:04:14.160 --> 00:04:14.920]   It's pretty active.
[00:04:14.920 --> 00:04:17.720]   We've got hundreds of people posting every single day.
[00:04:17.720 --> 00:04:21.600]   So you'll find lots of interesting stuff there.
[00:04:21.600 --> 00:04:23.560]   All right, announcements out of the way,
[00:04:23.560 --> 00:04:28.640]   I'm going to toss it over to our speaker for the day, Jesse
[00:04:28.640 --> 00:04:33.080]   Mu, who is a grad student at Stanford in the Stanford NLP
[00:04:33.080 --> 00:04:37.160]   group and wrote this really elegant paper on generating
[00:04:37.160 --> 00:04:39.520]   explanations for neurons.
[00:04:39.520 --> 00:04:42.720]   And so I'm really excited to hear him talk about that.
[00:04:42.720 --> 00:04:45.280]   So Jesse, go ahead.
[00:04:45.280 --> 00:04:45.760]   Great.
[00:04:45.760 --> 00:04:46.720]   Thanks for having me.
[00:04:46.720 --> 00:04:48.080]   I'm really excited to be here.
[00:04:48.080 --> 00:04:49.240]   Let me share my screen.
[00:04:49.240 --> 00:04:55.280]   Cool, yeah.
[00:04:55.280 --> 00:04:57.280]   So really happy to be here.
[00:04:57.280 --> 00:05:01.160]   I'm talking about work that I've been doing with Jacob Andreas,
[00:05:01.160 --> 00:05:04.360]   who is in the language and intelligence group at MIT.
[00:05:04.360 --> 00:05:08.920]   And I personally am a third year CS PhD student at Stanford,
[00:05:08.920 --> 00:05:13.520]   working with Stanford NLP and Noah Goodman.
[00:05:13.520 --> 00:05:17.600]   So the way that we're going to begin this talk
[00:05:17.600 --> 00:05:19.920]   is by looking at our favorite deep model, which
[00:05:19.920 --> 00:05:22.520]   we're going to call M. It's some sort of black box.
[00:05:22.520 --> 00:05:24.080]   And we don't really know how it works.
[00:05:24.080 --> 00:05:26.920]   And the way M processes information
[00:05:26.920 --> 00:05:29.040]   is by taking some sort of input, x,
[00:05:29.040 --> 00:05:31.480]   and then producing some sort of output, y.
[00:05:31.480 --> 00:05:34.160]   So the kinds of tasks that we care about in machine learning
[00:05:34.160 --> 00:05:36.400]   are, for example, image processing tasks,
[00:05:36.400 --> 00:05:39.600]   like scene classification, or natural language processing
[00:05:39.600 --> 00:05:43.360]   tasks, like natural language inference.
[00:05:43.360 --> 00:05:45.920]   The challenge in machine learning
[00:05:45.920 --> 00:05:47.680]   is really identifying what exactly
[00:05:47.680 --> 00:05:50.080]   is going on in this model, and how does it actually learn
[00:05:50.080 --> 00:05:51.320]   to solve a task at hand.
[00:05:51.320 --> 00:05:52.840]   So if we take a closer look, we find
[00:05:52.840 --> 00:05:55.840]   that most deep models take some high dimensional input, x,
[00:05:55.840 --> 00:05:58.400]   and then transform it into some lower dimensional space, which
[00:05:58.400 --> 00:05:59.720]   we'll call theta.
[00:05:59.720 --> 00:06:01.880]   And the goal of model interpretability
[00:06:01.880 --> 00:06:04.560]   is, given this representation, theta,
[00:06:04.560 --> 00:06:07.240]   what kind of information does a representation encode?
[00:06:07.240 --> 00:06:09.080]   And does that information closely
[00:06:09.080 --> 00:06:11.800]   align with human intuitions about the kind of information
[00:06:11.800 --> 00:06:15.280]   that model should encode?
[00:06:15.280 --> 00:06:17.320]   So one of the most popular kind of approaches
[00:06:17.320 --> 00:06:20.280]   to analyzing the information inside learned deep
[00:06:20.280 --> 00:06:21.720]   representations is what I'm going
[00:06:21.720 --> 00:06:25.160]   to call representation level analysis, or probing, which
[00:06:25.160 --> 00:06:27.080]   started very recently, I think.
[00:06:27.080 --> 00:06:29.720]   And the basic idea is, we take some model which
[00:06:29.720 --> 00:06:31.080]   learns representations.
[00:06:31.080 --> 00:06:33.600]   Let's imagine here it's for some sort of machine translation
[00:06:33.600 --> 00:06:34.400]   task.
[00:06:34.400 --> 00:06:37.680]   And then we train a slightly smaller supervised model
[00:06:37.680 --> 00:06:40.400]   called a probe that goes in the representation
[00:06:40.400 --> 00:06:42.800]   and tries to predict some sort of property of the input.
[00:06:42.800 --> 00:06:44.440]   In this case, let's imagine it's trying
[00:06:44.440 --> 00:06:47.200]   to predict the part of speech of the word dog.
[00:06:47.200 --> 00:06:49.160]   And if this probe ends up getting high accuracy
[00:06:49.160 --> 00:06:51.600]   on this task, then we can then claim
[00:06:51.600 --> 00:06:54.360]   that the representation theta encodes information
[00:06:54.360 --> 00:06:56.120]   about the part of speech, because the probe
[00:06:56.120 --> 00:06:57.800]   is able to use those features to predict
[00:06:57.800 --> 00:07:00.160]   the property of interest.
[00:07:00.160 --> 00:07:01.640]   So this has been, I think, enormously
[00:07:01.640 --> 00:07:03.280]   influential for model interpretability.
[00:07:03.280 --> 00:07:05.840]   But there's one primary issue with this kind of work,
[00:07:05.840 --> 00:07:07.760]   which is that there's been a recent debate
[00:07:07.760 --> 00:07:10.000]   in the interpretability literature about whether
[00:07:10.000 --> 00:07:12.760]   the success of a probe means that the information is
[00:07:12.760 --> 00:07:15.280]   encoded in the representation, or rather that we just
[00:07:15.280 --> 00:07:18.280]   have a very powerful probe that has memorized the task.
[00:07:18.280 --> 00:07:22.160]   And so this has been a back and forth debate.
[00:07:22.160 --> 00:07:24.560]   So instead, I want to advocate for an alternative way
[00:07:24.560 --> 00:07:26.000]   of analyzing representations.
[00:07:26.000 --> 00:07:28.560]   And this is by analyzing the individual features,
[00:07:28.560 --> 00:07:32.040]   or neurons, of deep representations.
[00:07:32.040 --> 00:07:34.280]   And so analyzing individual neurons, of course,
[00:07:34.280 --> 00:07:37.360]   has some advantages in that we can't detect concepts that
[00:07:37.360 --> 00:07:39.880]   are distributed across multiple neurons.
[00:07:39.880 --> 00:07:41.880]   But it has several advantages.
[00:07:41.880 --> 00:07:43.520]   First, analyzing neurons allows us
[00:07:43.520 --> 00:07:45.560]   to measure the extent to which representations
[00:07:45.560 --> 00:07:49.600]   are disentangled or decomposed into individual concepts that
[00:07:49.600 --> 00:07:51.800]   lie along individual features.
[00:07:51.800 --> 00:07:53.400]   Also, because we've greatly reduced
[00:07:53.400 --> 00:07:55.560]   the complexity of the behavior we're looking at,
[00:07:55.560 --> 00:07:57.200]   we don't do any sort of transformation.
[00:07:57.200 --> 00:07:59.120]   We don't do any sort of supervised learning.
[00:07:59.120 --> 00:08:01.240]   We're only inspecting kind of surface level behavior
[00:08:01.240 --> 00:08:01.960]   of a neuron.
[00:08:01.960 --> 00:08:04.160]   And we avoid those past debates about how complex
[00:08:04.160 --> 00:08:05.240]   probing methods should be.
[00:08:08.120 --> 00:08:09.960]   So analyzing individual neurons has also
[00:08:09.960 --> 00:08:12.280]   seen a lot of interest in the interpretability literature
[00:08:12.280 --> 00:08:13.400]   recently.
[00:08:13.400 --> 00:08:15.560]   Part of it started from this great work
[00:08:15.560 --> 00:08:18.240]   by David Bao, also at MIT, called NetDissect.
[00:08:18.240 --> 00:08:20.280]   And we're going to describe this method in detail
[00:08:20.280 --> 00:08:22.280]   because we build off of this basic idea.
[00:08:22.280 --> 00:08:25.200]   And it's also been applied in NLP as well.
[00:08:25.200 --> 00:08:27.160]   But I think one of the fundamental limitations
[00:08:27.160 --> 00:08:30.000]   of the existing work on interpretability so far
[00:08:30.000 --> 00:08:31.920]   is the following.
[00:08:31.920 --> 00:08:34.160]   So imagine we're trying to explain
[00:08:34.160 --> 00:08:35.840]   a neuron in some sort of vision network
[00:08:35.840 --> 00:08:37.120]   for seeing classification.
[00:08:37.120 --> 00:08:39.560]   And the way we might do so, and the way NetDissect proposes
[00:08:39.560 --> 00:08:42.480]   to do this, is by looking at the images that most maximally
[00:08:42.480 --> 00:08:44.440]   activate the neuron.
[00:08:44.440 --> 00:08:45.960]   So in this case, we have four images.
[00:08:45.960 --> 00:08:48.320]   And the neuron is active in those highlighted regions.
[00:08:48.320 --> 00:08:50.120]   And if we take a look at these four images,
[00:08:50.120 --> 00:08:53.440]   it becomes obvious to us that it seems like this neuron is
[00:08:53.440 --> 00:08:54.880]   detecting ball rings.
[00:08:54.880 --> 00:08:56.640]   And this is the explanation that NetDissect
[00:08:56.640 --> 00:08:59.480]   assigns this neuron.
[00:08:59.480 --> 00:09:02.600]   However, if we begin to look at the other images for which
[00:09:02.600 --> 00:09:05.720]   this neuron activates, we see a very different story.
[00:09:05.720 --> 00:09:07.640]   So now it's clear that this neuron is not just
[00:09:07.640 --> 00:09:10.040]   firing for ball rings, but rather
[00:09:10.040 --> 00:09:12.040]   for a bunch of different kinds of sports fields,
[00:09:12.040 --> 00:09:13.920]   baseball fields, and whatnot.
[00:09:13.920 --> 00:09:17.320]   And so reality, to understand what this neuron is doing,
[00:09:17.320 --> 00:09:19.200]   we really need a much richer explanation
[00:09:19.200 --> 00:09:20.840]   of what's going on in this neuron.
[00:09:20.840 --> 00:09:22.080]   And the focus of this talk is how
[00:09:22.080 --> 00:09:26.040]   to generate such an explanation.
[00:09:26.040 --> 00:09:27.960]   In general, we're taking this philosophy
[00:09:27.960 --> 00:09:30.440]   that neurons, especially later in the network,
[00:09:30.440 --> 00:09:32.360]   are not just simple feature detectors,
[00:09:32.360 --> 00:09:35.080]   but rather can be considered as implementing complex decision
[00:09:35.080 --> 00:09:37.960]   rules, or you can even think of them as programs composed
[00:09:37.960 --> 00:09:40.280]   of multiple concepts.
[00:09:40.280 --> 00:09:42.840]   So how do we generate these?
[00:09:42.840 --> 00:09:45.960]   So I'll first describe the basic technique of NetDissect
[00:09:45.960 --> 00:09:48.040]   and then propose an extension to that
[00:09:48.040 --> 00:09:50.800]   to handle compositional concepts.
[00:09:50.800 --> 00:09:53.120]   The idea is that we have some data set of inputs, which
[00:09:53.120 --> 00:09:55.480]   we'll call x, and some sort of model that transform
[00:09:55.480 --> 00:09:58.120]   these inputs into representations.
[00:09:58.120 --> 00:09:59.680]   We can inspect individual neurons
[00:09:59.680 --> 00:10:03.160]   of the representation-- here it's unit 483 in ResNet--
[00:10:03.160 --> 00:10:07.120]   and examine the activations of this neuron over the inputs.
[00:10:07.120 --> 00:10:09.520]   And the challenge is to try to identify or explain
[00:10:09.520 --> 00:10:13.800]   this neuron's behavior in human understandable terms.
[00:10:13.800 --> 00:10:16.240]   So the way that NetDissect proposes to do this
[00:10:16.240 --> 00:10:20.280]   is by first segmenting the neurons into binary masks.
[00:10:20.280 --> 00:10:21.640]   So we determine some threshold.
[00:10:21.640 --> 00:10:24.440]   Let's say the top 1% of values this neuron takes.
[00:10:24.440 --> 00:10:27.240]   And whenever this neuron exceeds that threshold,
[00:10:27.240 --> 00:10:31.200]   we consider the neuron active at that point.
[00:10:31.200 --> 00:10:32.880]   Now to generate an explanation, we
[00:10:32.880 --> 00:10:36.160]   need some sort of gold hand-annotated inventory
[00:10:36.160 --> 00:10:38.280]   of concepts, which are also represented
[00:10:38.280 --> 00:10:43.200]   as segmentation masks, such as water or river or even colors.
[00:10:43.200 --> 00:10:45.080]   Then the challenge is to find the explanation
[00:10:45.080 --> 00:10:49.040]   that most closely matches the behavior of a neuron.
[00:10:49.040 --> 00:10:51.040]   And the way we do so is by some sort of measure
[00:10:51.040 --> 00:10:52.600]   of goodness of an explanation.
[00:10:52.600 --> 00:10:55.000]   And for that, we'll use the intersection of reunion score
[00:10:55.000 --> 00:10:56.080]   or IOU.
[00:10:56.080 --> 00:10:58.840]   This is commonly used in, say, bounding box prediction
[00:10:58.840 --> 00:11:01.440]   in computer vision.
[00:11:01.440 --> 00:11:03.640]   If we measure IOU, we find that the best explanation
[00:11:03.640 --> 00:11:05.480]   we have out of this simple concept inventory
[00:11:05.480 --> 00:11:07.800]   is the water concept.
[00:11:07.800 --> 00:11:12.840]   And this is the explanation that NetDissect obtains.
[00:11:12.840 --> 00:11:14.420]   But of course, we take a closer look.
[00:11:14.420 --> 00:11:16.480]   It's clear that this is not the entire story.
[00:11:16.480 --> 00:11:18.280]   It seems like the behavior is a little bit more
[00:11:18.280 --> 00:11:19.840]   sophisticated than just detecting
[00:11:19.840 --> 00:11:21.920]   all bodies of water, because it does not activate
[00:11:21.920 --> 00:11:24.000]   for that middle image.
[00:11:24.000 --> 00:11:27.240]   So our contribution is to combinatorially expand
[00:11:27.240 --> 00:11:29.600]   the possible concepts under consideration
[00:11:29.600 --> 00:11:32.200]   with compositional operators on concepts.
[00:11:32.200 --> 00:11:35.320]   So consider the logical operations and, or, and not.
[00:11:35.320 --> 00:11:37.200]   And then we can use these logical operations
[00:11:37.200 --> 00:11:39.680]   to combine existing concepts to form progressively
[00:11:39.680 --> 00:11:40.840]   more complex ones.
[00:11:40.840 --> 00:11:43.800]   And we'll iteratively construct these more complex operations--
[00:11:43.800 --> 00:11:48.160]   sorry, concepts via beam search.
[00:11:48.160 --> 00:11:50.960]   So at one step of beam search, we might compose, for example,
[00:11:50.960 --> 00:11:55.280]   and construct a water or river concept, which just
[00:11:55.280 --> 00:11:57.520]   is the logical or of the two masks.
[00:11:57.520 --> 00:12:03.040]   Or we might invert the blue mask to get non-blue colors.
[00:12:03.040 --> 00:12:04.120]   Finally, we keep going.
[00:12:04.120 --> 00:12:06.280]   And let's imagine that we stop at this explanation.
[00:12:06.280 --> 00:12:09.320]   So this is kind of a length three logical explanation,
[00:12:09.320 --> 00:12:13.740]   which has a much higher IOU compared to the water neuron
[00:12:13.740 --> 00:12:17.680]   and is more closely descriptive of the behavior of this neuron.
[00:12:17.680 --> 00:12:19.060]   And in particular, the explanation
[00:12:19.060 --> 00:12:21.880]   we have assigned now is that this neuron fires
[00:12:21.880 --> 00:12:23.520]   for water that's not blue.
[00:12:27.400 --> 00:12:28.800]   So that's the basic technique.
[00:12:28.800 --> 00:12:29.960]   I think there are a number of questions
[00:12:29.960 --> 00:12:32.160]   we can begin to explore with this technique.
[00:12:32.160 --> 00:12:34.480]   But here are three that we looked at in the paper.
[00:12:34.480 --> 00:12:37.600]   The first is, do neurons learn compositional concepts?
[00:12:37.600 --> 00:12:40.840]   So does this explanation technique
[00:12:40.840 --> 00:12:44.080]   bias anything over just simple metaseqt?
[00:12:44.080 --> 00:12:46.000]   The second is whether the interpretability
[00:12:46.000 --> 00:12:47.600]   of individual concepts in a model
[00:12:47.600 --> 00:12:50.920]   relates to downstream task performance in any way.
[00:12:50.920 --> 00:12:53.120]   And the third is whether we can use our explanations
[00:12:53.120 --> 00:12:55.480]   to begin to probe or manipulate model behavior.
[00:12:56.200 --> 00:12:58.000]   So let's look at these three questions.
[00:12:58.000 --> 00:13:00.720]   The first is, do neurons learn compositional concepts?
[00:13:00.720 --> 00:13:02.200]   So here, what I'm going to do is I'm
[00:13:02.200 --> 00:13:04.600]   going to plot on the x-axis the maximum formula
[00:13:04.600 --> 00:13:07.200]   length of the explanations we generate with our procedure.
[00:13:07.200 --> 00:13:10.520]   And on the y-axis, the average explanation quality, or IOU,
[00:13:10.520 --> 00:13:11.800]   of each neuron.
[00:13:11.800 --> 00:13:14.080]   So at formula length one, we had metaseqt.
[00:13:14.080 --> 00:13:15.680]   And as we increase the formula length,
[00:13:15.680 --> 00:13:20.880]   we get progressively more complex explanations.
[00:13:20.880 --> 00:13:22.880]   So what we see here is a positive relationship
[00:13:22.880 --> 00:13:25.120]   between formula length one and formula length two.
[00:13:25.120 --> 00:13:26.960]   Between formula length and IOU, indicating
[00:13:26.960 --> 00:13:29.840]   that we are indeed generating higher quality explanations,
[00:13:29.840 --> 00:13:33.360]   as measured by IOU, with about a 68% increase in explanation
[00:13:33.360 --> 00:13:35.920]   quality when we go from, say, one to length 10.
[00:13:35.920 --> 00:13:40.840]   We can take a look at, qualitatively,
[00:13:40.840 --> 00:13:43.480]   what kind of concepts are identified in this model.
[00:13:43.480 --> 00:13:46.720]   This is the model ResNet trained on a scene classification task.
[00:13:46.720 --> 00:13:48.600]   And we see that a lot of interesting abstractions
[00:13:48.600 --> 00:13:49.480]   emerge.
[00:13:49.480 --> 00:13:52.640]   So I can categorize these broadly into four categories.
[00:13:52.640 --> 00:13:54.480]   The first of which are meaningful perceptual
[00:13:54.480 --> 00:13:55.880]   abstractions.
[00:13:55.880 --> 00:13:56.920]   We can take a look here.
[00:13:56.920 --> 00:13:59.000]   Unit 1 and 2 fires for towers.
[00:13:59.000 --> 00:14:02.280]   Unit 310 fires for, say, bathroom appliances.
[00:14:02.280 --> 00:14:04.120]   And we're going to call these abstractions
[00:14:04.120 --> 00:14:07.160]   both perceptually meaningful and lexically meaningful,
[00:14:07.160 --> 00:14:09.280]   because we can take a look at the explanation
[00:14:09.280 --> 00:14:11.840]   and understand the kinds of concepts
[00:14:11.840 --> 00:14:14.960]   that are being fired for.
[00:14:14.960 --> 00:14:17.320]   We also have maybe about 22% of neurons,
[00:14:17.320 --> 00:14:20.200]   which fall into this category of learning an abstraction that
[00:14:20.200 --> 00:14:22.080]   is perceptually meaningful, but does not
[00:14:22.080 --> 00:14:24.400]   exist in our concept inventory.
[00:14:24.400 --> 00:14:26.960]   And so unit 321, for example, is clearly
[00:14:26.960 --> 00:14:29.120]   some sort of ball or sphere detector.
[00:14:29.120 --> 00:14:31.840]   But we do not have annotations of balls or spheres
[00:14:31.840 --> 00:14:33.440]   in our library.
[00:14:33.440 --> 00:14:35.960]   And therefore, the explanation we have, ball pit or orchard
[00:14:35.960 --> 00:14:37.800]   or bounce game, it's a little bit less clear
[00:14:37.800 --> 00:14:40.360]   how that relates to the activations.
[00:14:40.360 --> 00:14:45.160]   We also see examples of specialization.
[00:14:45.160 --> 00:14:47.040]   So this is where neurons activate
[00:14:47.040 --> 00:14:48.800]   for more specific versions of concepts
[00:14:48.800 --> 00:14:50.080]   that are in our inventory.
[00:14:50.080 --> 00:14:52.680]   This includes that famous non-blue water neuron,
[00:14:52.680 --> 00:14:54.480]   and also an attic neuron that fires only
[00:14:54.480 --> 00:14:58.440]   for the top triangular part and not the floor.
[00:14:58.440 --> 00:14:59.920]   And finally, we see examples of what
[00:14:59.920 --> 00:15:02.080]   I'm going to call polysemanticity, which
[00:15:02.080 --> 00:15:04.760]   is the tendency for neurons to fire for completely
[00:15:04.760 --> 00:15:05.920]   separate concepts.
[00:15:05.920 --> 00:15:08.040]   And so that includes neurons that fires for, say,
[00:15:08.040 --> 00:15:11.200]   operating rooms or castles or bathrooms or bakeries
[00:15:11.200 --> 00:15:12.720]   or shop fronts and et cetera.
[00:15:12.720 --> 00:15:17.360]   So this has given us an explanation
[00:15:17.360 --> 00:15:19.960]   of what we're seeing in the behavior of a neural network
[00:15:19.960 --> 00:15:21.920]   that's been trained for seeing classification.
[00:15:21.920 --> 00:15:23.520]   Now let's take a look at whether or not
[00:15:23.520 --> 00:15:27.520]   the concepts here relate to model performance in any way.
[00:15:27.520 --> 00:15:28.880]   So the way we're going to do this
[00:15:28.880 --> 00:15:31.520]   is by obtaining some sort of measure of the reliability
[00:15:31.520 --> 00:15:32.920]   or accuracy of the neuron.
[00:15:32.920 --> 00:15:33.760]   And what we're going to do is we're
[00:15:33.760 --> 00:15:36.000]   going to measure the model accuracy on inputs
[00:15:36.000 --> 00:15:37.920]   when the neuron is maximally active.
[00:15:37.920 --> 00:15:39.280]   So when the neuron lights up, when
[00:15:39.280 --> 00:15:41.080]   it contributes to a decision, how accurate
[00:15:41.080 --> 00:15:44.240]   is the model's decision on average?
[00:15:44.240 --> 00:15:46.640]   Here I'm plotting on the x-axis the average explanation
[00:15:46.640 --> 00:15:49.080]   quality of our neuron, and on the y-axis
[00:15:49.080 --> 00:15:51.800]   the accuracy of the neuron when it fires.
[00:15:51.800 --> 00:15:54.520]   And what we see is a positive relationship, which is noisy,
[00:15:54.520 --> 00:15:56.280]   but highly significant.
[00:15:56.280 --> 00:15:57.600]   And importantly, this correlation
[00:15:57.600 --> 00:16:01.200]   increases as we increase the quality of our explanations.
[00:16:01.200 --> 00:16:03.560]   And so what this is saying is that at least in the computer
[00:16:03.560 --> 00:16:06.440]   vision case, neurons that are more human interpretable,
[00:16:06.440 --> 00:16:09.600]   that are more compositional in their concepts,
[00:16:09.600 --> 00:16:12.720]   end up being more reliable for test time performance.
[00:16:12.720 --> 00:16:17.720]   And finally, here's one last question,
[00:16:17.720 --> 00:16:19.280]   which I think is the most interesting one, which
[00:16:19.280 --> 00:16:21.080]   is, can our explanations give us insight
[00:16:21.080 --> 00:16:24.600]   into how we can control and manipulate model behavior?
[00:16:24.600 --> 00:16:26.760]   So we've probed the final convolutional layer
[00:16:26.760 --> 00:16:28.760]   before the prediction of ResNet, which
[00:16:28.760 --> 00:16:30.920]   allows us to look at what kind of explanations
[00:16:30.920 --> 00:16:33.160]   contribute to certain class predictions.
[00:16:33.160 --> 00:16:35.760]   So here's an example of class 324, or swimming hole.
[00:16:35.760 --> 00:16:37.760]   I'm going to take a look at the kinds of concepts
[00:16:37.760 --> 00:16:39.000]   that feed into this class.
[00:16:39.000 --> 00:16:40.040]   A lot of them are sensible.
[00:16:40.040 --> 00:16:41.540]   We have foliage, and we have waters,
[00:16:41.540 --> 00:16:44.280]   and we have creeks and deserts.
[00:16:44.280 --> 00:16:45.840]   But in particular, there's a return
[00:16:45.840 --> 00:16:48.840]   of that non-blue water neuron.
[00:16:48.840 --> 00:16:50.640]   This non-blue water neuron is interesting.
[00:16:50.640 --> 00:16:53.400]   What happens is if we take a swimming hole image, which
[00:16:53.400 --> 00:16:55.600]   a bunch of neural networks think is swimming hole,
[00:16:55.600 --> 00:16:59.440]   and we paint the water blue, we manipulate the model prediction
[00:16:59.440 --> 00:17:01.280]   to Grotto in three out of the four networks
[00:17:01.280 --> 00:17:02.960]   that we explored, including networks
[00:17:02.960 --> 00:17:06.480]   that are outside of the probe 1, which is ResNet-18.
[00:17:06.480 --> 00:17:08.200]   So this indicates that we're latching
[00:17:08.200 --> 00:17:10.000]   onto some sort of data set bias here,
[00:17:10.000 --> 00:17:12.000]   that swimming holes always have non-blue water.
[00:17:12.000 --> 00:17:14.400]   And these explanations have given us insight
[00:17:14.400 --> 00:17:16.280]   into how we can then manipulate this behavior.
[00:17:16.280 --> 00:17:21.960]   Another example here is a clean room.
[00:17:21.960 --> 00:17:24.760]   So it's a little bit less clear what kind of concepts
[00:17:24.760 --> 00:17:26.240]   are feeding into this prediction.
[00:17:26.240 --> 00:17:28.360]   But in particular, there's one concept here
[00:17:28.360 --> 00:17:30.200]   that fires for igloos.
[00:17:30.200 --> 00:17:33.320]   And so if we take a corridor and put an igloo in it,
[00:17:33.320 --> 00:17:35.880]   we change the prediction from corridor to clean room,
[00:17:35.880 --> 00:17:41.000]   but only here in the probed ResNet-18.
[00:17:41.000 --> 00:17:42.600]   And finally, one more example.
[00:17:42.600 --> 00:17:44.080]   Here's a viaduct.
[00:17:44.080 --> 00:17:45.920]   The concepts that feed into this prediction
[00:17:45.920 --> 00:17:47.360]   are quite sensible, but there's one
[00:17:47.360 --> 00:17:50.320]   neuron that fires for washers and laundromats as well.
[00:17:50.320 --> 00:17:52.000]   So we take a forest path, we stick
[00:17:52.000 --> 00:17:53.440]   a bunch of washing machines there,
[00:17:53.440 --> 00:17:55.160]   and we change the prediction to viaduct.
[00:17:55.160 --> 00:18:01.240]   So the method that I described here is, in general,
[00:18:01.240 --> 00:18:02.080]   it's task-agnostic.
[00:18:02.080 --> 00:18:03.600]   We don't have to apply it to vision.
[00:18:03.600 --> 00:18:06.000]   We can apply it to any kind of representation learned
[00:18:06.000 --> 00:18:07.440]   by some deep model.
[00:18:07.440 --> 00:18:09.320]   And just as proof of concept, we can
[00:18:09.320 --> 00:18:11.440]   explore a natural language processing task.
[00:18:11.440 --> 00:18:13.680]   So this is the task of natural language inference.
[00:18:13.680 --> 00:18:16.120]   The basic idea is that we're given two sentences.
[00:18:16.120 --> 00:18:19.320]   One is a premise sentence, such as a woman in a light blue
[00:18:19.320 --> 00:18:21.160]   jacket is riding a bike.
[00:18:21.160 --> 00:18:23.160]   And then there's a hypothesis sentence,
[00:18:23.160 --> 00:18:25.320]   like a woman in a jacket is riding a bike.
[00:18:25.320 --> 00:18:27.480]   And the objective is to determine the relationship
[00:18:27.480 --> 00:18:29.800]   between the truth conditions of the two sentences.
[00:18:29.800 --> 00:18:32.760]   So in this case, the premise entails the hypothesis.
[00:18:32.760 --> 00:18:34.560]   But if we change it to a bus, for example,
[00:18:34.560 --> 00:18:35.960]   then it's a contradiction.
[00:18:35.960 --> 00:18:37.960]   And finally, there might be a neutral prediction.
[00:18:37.960 --> 00:18:42.200]   There's no relation between the premise and the hypothesis.
[00:18:42.200 --> 00:18:45.120]   So I'm picking this task as a representative NLP task
[00:18:45.120 --> 00:18:48.200]   because in recent years, this task has come under scrutiny
[00:18:48.200 --> 00:18:50.800]   because it's unclear how much actual inference is happening
[00:18:50.800 --> 00:18:51.720]   in natural language inference.
[00:18:51.720 --> 00:18:54.000]   So I'm going to surround inference with quotes here.
[00:18:54.000 --> 00:18:57.120]   And here are two hints as to why this might be happening.
[00:18:57.120 --> 00:18:58.840]   The first is we take some model that
[00:18:58.840 --> 00:19:00.440]   encodes the premise and the hypothesis
[00:19:00.440 --> 00:19:01.720]   and produces some prediction.
[00:19:01.720 --> 00:19:03.160]   Some sort of standard neural model
[00:19:03.160 --> 00:19:05.160]   gets us around 78% accuracy.
[00:19:05.160 --> 00:19:07.520]   So that's pretty good.
[00:19:07.520 --> 00:19:10.160]   However, if we design an ablated version of the model, which
[00:19:10.160 --> 00:19:11.800]   only takes the hypothesis's input--
[00:19:11.800 --> 00:19:13.840]   so it completely ignores the premise--
[00:19:13.840 --> 00:19:16.840]   this model still gets 69% accuracy.
[00:19:16.840 --> 00:19:18.080]   So there's a drop in accuracy.
[00:19:18.080 --> 00:19:19.680]   But this is actually still really good.
[00:19:19.680 --> 00:19:21.600]   And importantly, it's far above chance.
[00:19:21.600 --> 00:19:23.640]   So this is an indicator that there's not actually
[00:19:23.640 --> 00:19:27.600]   much inference happening in this model.
[00:19:27.600 --> 00:19:30.680]   Similarly, we can come up with certain heuristics,
[00:19:30.680 --> 00:19:33.320]   some heuristic rules, such as predict entailment
[00:19:33.320 --> 00:19:35.200]   when all of the words in the hypothesis
[00:19:35.200 --> 00:19:38.880]   are also in the premise, as is the case here.
[00:19:38.880 --> 00:19:40.760]   Turns out if you apply these heuristics,
[00:19:40.760 --> 00:19:44.200]   when they do apply, you get 90% accuracy on examples
[00:19:44.200 --> 00:19:45.440]   where this heuristic applies.
[00:19:45.440 --> 00:19:49.080]   And there are several such heuristics like this.
[00:19:49.080 --> 00:19:51.920]   So this indicates to us that these models are maybe not
[00:19:51.920 --> 00:19:54.640]   really learning the right thing in trying to solve these tasks
[00:19:54.640 --> 00:19:57.120]   and are instead latching on to certain spurious correlations
[00:19:57.120 --> 00:19:58.400]   or data set biases.
[00:19:58.400 --> 00:20:00.120]   And we wanted to look at whether or not
[00:20:00.120 --> 00:20:02.720]   we can uncover these behaviors with our explainability
[00:20:02.720 --> 00:20:05.920]   technique.
[00:20:05.920 --> 00:20:08.360]   So here's our NLP model, our NLI model.
[00:20:08.360 --> 00:20:11.920]   And we're going to probe the final layer of the multilayer
[00:20:11.920 --> 00:20:12.480]   perceptron.
[00:20:12.480 --> 00:20:14.880]   So we encode the premise in the hypothesis with LSTMs.
[00:20:14.880 --> 00:20:17.200]   We combine them and then try to produce some prediction.
[00:20:17.200 --> 00:20:18.880]   And we're going to probe the final layer
[00:20:18.880 --> 00:20:21.120]   before the prediction.
[00:20:21.120 --> 00:20:24.080]   Our concepts will be very, very simple bag of words concepts.
[00:20:24.080 --> 00:20:26.400]   So given some sort of premise hypothesis pair,
[00:20:26.400 --> 00:20:28.480]   we extract these very simple concepts
[00:20:28.480 --> 00:20:31.120]   where pre colon woman indicates that the word woman
[00:20:31.120 --> 00:20:32.640]   appears in the premise.
[00:20:32.640 --> 00:20:34.960]   Pre colon nn indicates the word--
[00:20:34.960 --> 00:20:37.200]   sorry, the noun appears in a sentence premise,
[00:20:37.200 --> 00:20:40.120]   anywhere in the premise, and so on.
[00:20:40.120 --> 00:20:42.560]   And additionally, there's this kind of overlap feature
[00:20:42.560 --> 00:20:44.520]   which indicates the degree to which the premise
[00:20:44.520 --> 00:20:46.320]   and the hypothesis share words.
[00:20:46.320 --> 00:20:51.400]   So in this case, they share 75% of unique words.
[00:20:51.400 --> 00:20:52.720]   And finally, as our compositions,
[00:20:52.720 --> 00:20:54.480]   we can use and, or, and not.
[00:20:54.480 --> 00:20:57.400]   But just as a hint that we can use more unique compositions,
[00:20:57.400 --> 00:21:00.600]   I'll also define the neighbors composition, which
[00:21:00.600 --> 00:21:02.640]   the neighbors of a certain token is
[00:21:02.640 --> 00:21:06.360]   the logical or across the five most similar words
[00:21:06.360 --> 00:21:09.400]   to this token in a glove embedding space.
[00:21:09.400 --> 00:21:11.520]   So the idea here is we're capturing this intuition
[00:21:11.520 --> 00:21:13.880]   that a neuron might fire for semantically similar words.
[00:21:13.880 --> 00:21:19.040]   Let's now answer our three questions.
[00:21:19.040 --> 00:21:21.280]   First, do neurons learn compositional concepts?
[00:21:21.280 --> 00:21:23.560]   Here is a model trained on the Stanford natural language
[00:21:23.560 --> 00:21:24.600]   inference data set.
[00:21:24.600 --> 00:21:26.400]   And we again see this positive relationship
[00:21:26.400 --> 00:21:29.240]   between our formula length and explanation quality.
[00:21:29.240 --> 00:21:35.480]   Qualitatively, let's take a look at what's going on.
[00:21:35.480 --> 00:21:37.800]   So here is unit 870 in this model.
[00:21:37.800 --> 00:21:40.600]   Here's the explanation we assign along with its IOU.
[00:21:40.600 --> 00:21:45.080]   And here are some examples of the neuron being most active.
[00:21:45.080 --> 00:21:47.720]   So in general, if we take a look here,
[00:21:47.720 --> 00:21:49.720]   we would really label this neuron as being gender
[00:21:49.720 --> 00:21:50.240]   sensitive.
[00:21:50.240 --> 00:21:53.440]   So it activates when the premise contains the word man
[00:21:53.440 --> 00:21:55.240]   and does not contain the word woman,
[00:21:55.240 --> 00:21:57.760]   and the hypothesis does not contain the word man.
[00:21:57.760 --> 00:21:59.720]   So basically, whenever there is a gender switch
[00:21:59.720 --> 00:22:01.520]   between the premise and the hypothesis,
[00:22:01.520 --> 00:22:03.880]   this neuron votes in favor of contradiction.
[00:22:03.880 --> 00:22:06.160]   So we can analyze the weights of this neuron
[00:22:06.160 --> 00:22:10.560]   towards the three class predictions here.
[00:22:10.560 --> 00:22:12.720]   Here's another neuron that fires.
[00:22:12.720 --> 00:22:14.800]   There's a few noisy explanations here.
[00:22:14.800 --> 00:22:18.120]   But the key here is that overlap of 75% neuron.
[00:22:18.120 --> 00:22:20.400]   This neuron fires when the premise and the hypothesis
[00:22:20.400 --> 00:22:22.360]   share many, many common words, and it
[00:22:22.360 --> 00:22:26.360]   votes towards entailment.
[00:22:26.360 --> 00:22:28.560]   So these are the kinds of lexical overlap heuristics
[00:22:28.560 --> 00:22:31.120]   I was talking about earlier.
[00:22:31.120 --> 00:22:33.640]   Finally, here's a neuron that activates
[00:22:33.640 --> 00:22:35.640]   whenever the word sitting is in the hypothesis.
[00:22:35.640 --> 00:22:37.600]   So these are words where the verb is actually
[00:22:37.600 --> 00:22:39.280]   quite indicative of the class prediction,
[00:22:39.280 --> 00:22:41.200]   even though they really shouldn't be.
[00:22:41.200 --> 00:22:42.720]   And so this one activates whenever
[00:22:42.720 --> 00:22:45.720]   the word sitting is in the hypothesis but not
[00:22:45.720 --> 00:22:48.560]   the premise.
[00:22:48.560 --> 00:22:50.880]   And lastly, we have neurons that are not well explained
[00:22:50.880 --> 00:22:51.800]   by a future set.
[00:22:51.800 --> 00:22:53.640]   So this one activates whenever there
[00:22:53.640 --> 00:22:56.000]   is a noun in the premise, which is almost all the time.
[00:22:59.080 --> 00:23:00.720]   We can, again, relate interpretability
[00:23:00.720 --> 00:23:02.240]   to model performance.
[00:23:02.240 --> 00:23:04.400]   And here, we actually see a negative relationship.
[00:23:04.400 --> 00:23:06.080]   So what this is saying is that the better
[00:23:06.080 --> 00:23:08.400]   we are able to describe our neurons with our explanation
[00:23:08.400 --> 00:23:10.760]   technique, the less accurate the neuron is.
[00:23:10.760 --> 00:23:12.840]   And the reason why is because the kinds of features
[00:23:12.840 --> 00:23:14.480]   we're using are very, very simple.
[00:23:14.480 --> 00:23:16.360]   They're just these really simple bag of words features,
[00:23:16.360 --> 00:23:18.720]   which you would not expect to really be involved
[00:23:18.720 --> 00:23:20.960]   in any sort of robust natural language inference.
[00:23:20.960 --> 00:23:23.120]   And so the better we can describe neurons
[00:23:23.120 --> 00:23:25.680]   as using these very, very simple decision rules,
[00:23:25.680 --> 00:23:28.240]   the less accurate they are on average.
[00:23:28.240 --> 00:23:31.520]   And interestingly, the simpler our explanations,
[00:23:31.520 --> 00:23:35.840]   the better we capture this anti-correlation.
[00:23:35.840 --> 00:23:39.600]   So one key caveat here is that interpretability is not a priori
[00:23:39.600 --> 00:23:40.840]   correlated with performance.
[00:23:40.840 --> 00:23:42.600]   It really depends on the space of concepts
[00:23:42.600 --> 00:23:43.720]   that we're searching for.
[00:23:43.720 --> 00:23:45.200]   So in the vision case, we really were
[00:23:45.200 --> 00:23:46.880]   searching for meaningful abstractions,
[00:23:46.880 --> 00:23:48.280]   whereas in this case, we're trying
[00:23:48.280 --> 00:23:50.000]   to identify undesirable behaviors.
[00:23:50.000 --> 00:23:55.400]   And finally, let's take a look at how we can now manipulate
[00:23:55.400 --> 00:23:57.680]   model behavior in the same way.
[00:23:57.680 --> 00:23:59.600]   So here's an explanation of a neuron.
[00:23:59.600 --> 00:24:02.400]   This one fires whenever the word nobody is in the hypothesis.
[00:24:02.400 --> 00:24:04.880]   So that is a very clear signal that there's a contradiction,
[00:24:04.880 --> 00:24:06.640]   at least in this data set.
[00:24:06.640 --> 00:24:08.200]   So we take a premise hypothesis pair,
[00:24:08.200 --> 00:24:11.000]   such as three women prepare a meal in the kitchen.
[00:24:11.000 --> 00:24:13.640]   And we can modify the hypothesis to nobody,
[00:24:13.640 --> 00:24:15.720]   but the ladies are cooking.
[00:24:15.720 --> 00:24:18.120]   And so this should change the true label from entailment
[00:24:18.120 --> 00:24:21.520]   to neutral, but it changes the true prediction-- sorry,
[00:24:21.520 --> 00:24:24.360]   the model prediction from entailment to contradiction.
[00:24:24.360 --> 00:24:29.240]   So we've induced adversarial behavior in this model.
[00:24:29.240 --> 00:24:30.800]   Here's another example.
[00:24:30.800 --> 00:24:32.760]   This neuron fires for whenever there
[00:24:32.760 --> 00:24:35.120]   is some sort of word related to a couch or a table
[00:24:35.120 --> 00:24:37.840]   or a seat in the hypothesis.
[00:24:37.840 --> 00:24:40.160]   And so we take this premise and hypothesis pair,
[00:24:40.160 --> 00:24:43.240]   and we add the word couch in the hypothesis.
[00:24:43.240 --> 00:24:46.240]   We change the true prediction from entailment to neutral,
[00:24:46.240 --> 00:24:48.960]   but the model prediction from entailment to contradiction.
[00:24:51.640 --> 00:24:52.680]   Finally, one more.
[00:24:52.680 --> 00:24:54.640]   Here's that sitting neuron.
[00:24:54.640 --> 00:24:56.520]   Here we can modify the premise instead.
[00:24:56.520 --> 00:24:58.880]   So we have a blonde woman is holding two golf balls
[00:24:58.880 --> 00:25:00.480]   or reaching down into a golf ball,
[00:25:00.480 --> 00:25:03.640]   and the hypothesis is a blonde woman is sitting down.
[00:25:03.640 --> 00:25:05.760]   It turns out if we just cut out most of the premise
[00:25:05.760 --> 00:25:09.000]   and we just have a blonde woman is holding two golf balls,
[00:25:09.000 --> 00:25:11.200]   the model still votes for contradiction,
[00:25:11.200 --> 00:25:13.320]   even though the true label is now neutral.
[00:25:13.320 --> 00:25:15.080]   So it's just the presence of sitting there
[00:25:15.080 --> 00:25:17.280]   that's determining the model prediction in this case.
[00:25:21.600 --> 00:25:23.520]   So just to summarize, I've described
[00:25:23.520 --> 00:25:25.200]   a method for model interpretability
[00:25:25.200 --> 00:25:27.720]   and neuron interpretability specifically
[00:25:27.720 --> 00:25:29.680]   that generates compositional explanations
[00:25:29.680 --> 00:25:33.400]   of the individual neurons inside deep representations, which
[00:25:33.400 --> 00:25:36.120]   allow us to identify interesting, more rich behavior
[00:25:36.120 --> 00:25:37.240]   in neurons.
[00:25:37.240 --> 00:25:38.920]   So this includes meaningful abstractions
[00:25:38.920 --> 00:25:42.160]   in vision, polysematicity, and spurious correlations
[00:25:42.160 --> 00:25:45.120]   in language.
[00:25:45.120 --> 00:25:46.920]   These concepts actually have something
[00:25:46.920 --> 00:25:49.680]   to do with the downstream task performance of the model.
[00:25:49.680 --> 00:25:51.440]   So we can actually use these explanations
[00:25:51.440 --> 00:25:54.040]   to disambiguate neurons that are better or worse with respect
[00:25:54.040 --> 00:25:56.200]   to performance.
[00:25:56.200 --> 00:25:58.160]   And finally, we've shown some early evidence
[00:25:58.160 --> 00:25:59.800]   that we can now predictably manipulate
[00:25:59.800 --> 00:26:02.600]   the kind of behaviors of neurons by analyzing the explanations
[00:26:02.600 --> 00:26:05.320]   and then staging interventions based on the explanations.
[00:26:05.320 --> 00:26:10.080]   So future questions I'd like to explore.
[00:26:10.080 --> 00:26:12.720]   One is, can we better look at connections between layers?
[00:26:12.720 --> 00:26:15.200]   So here we always probe the final layer of a neural network,
[00:26:15.200 --> 00:26:17.440]   but we can definitely look at intermediate layers.
[00:26:17.440 --> 00:26:19.520]   And if you're interested in this kind of work,
[00:26:19.520 --> 00:26:22.040]   the circuits work from the OpenAI team with Chris Ola
[00:26:22.040 --> 00:26:26.120]   has done really great work in this kind of area.
[00:26:26.120 --> 00:26:28.320]   And finally, insofar as we've identified
[00:26:28.320 --> 00:26:31.000]   that more interpretable concepts are maybe more desirable when
[00:26:31.000 --> 00:26:32.680]   it comes to performance, can we begin
[00:26:32.680 --> 00:26:35.680]   to use interpretability as a sort of training signal?
[00:26:35.680 --> 00:26:37.240]   We can try to encourage a model to be
[00:26:37.240 --> 00:26:38.520]   interpretable from the start.
[00:26:38.520 --> 00:26:42.360]   And does this lead to better performance?
[00:26:42.360 --> 00:26:43.000]   So that's it.
[00:26:43.000 --> 00:26:45.680]   I wanted to thank my collaborators and Jacob Andreas.
[00:26:45.680 --> 00:26:48.360]   Here are some links to code and the preprints.
[00:26:48.360 --> 00:26:51.720]   And I'm happy to take questions at this point.
[00:26:51.720 --> 00:26:52.680]   Great.
[00:26:52.680 --> 00:26:57.280]   Thanks a lot, Jesse, for really interesting work
[00:26:57.280 --> 00:26:59.520]   and well-presented also.
[00:26:59.520 --> 00:27:03.320]   So I encourage folks on the Zoom to put their questions
[00:27:03.320 --> 00:27:07.720]   in the Q&A. And folks on YouTube can put them in the live chat.
[00:27:07.720 --> 00:27:10.320]   But to kick us off, I guess-- so actually,
[00:27:10.320 --> 00:27:12.640]   one of my questions you already covered
[00:27:12.640 --> 00:27:15.100]   in your future direction, so I wanted to probe a little bit
[00:27:15.100 --> 00:27:16.880]   deeper on that, which is, so what
[00:27:16.880 --> 00:27:19.420]   do you think the prospects are for using something like this
[00:27:19.420 --> 00:27:21.280]   as a form of regularization?
[00:27:21.280 --> 00:27:24.920]   It seems like, since you have these sort of two sides,
[00:27:24.920 --> 00:27:27.960]   where it's like, OK, if the explanations are bad
[00:27:27.960 --> 00:27:31.920]   heuristics, that's--
[00:27:31.920 --> 00:27:33.560]   if they're bad heuristics, that's
[00:27:33.560 --> 00:27:35.280]   maybe something that you want to penalize.
[00:27:35.280 --> 00:27:37.280]   If they are rich compositional explanations,
[00:27:37.280 --> 00:27:41.120]   maybe that's something that you want to encourage.
[00:27:41.120 --> 00:27:41.600]   Yeah.
[00:27:41.600 --> 00:27:43.400]   So I think it really depends, as you said,
[00:27:43.400 --> 00:27:45.240]   in the explanation we're generating
[00:27:45.240 --> 00:27:47.480]   and whether or not they're correlated with desirable
[00:27:47.480 --> 00:27:49.160]   or undesirable behaviors.
[00:27:49.160 --> 00:27:50.960]   In terms of-- in the desirable case,
[00:27:50.960 --> 00:27:52.500]   like in the vision case, for example,
[00:27:52.500 --> 00:27:55.280]   we might encourage the development of neurons
[00:27:55.280 --> 00:27:59.160]   to recognize well-defined atomic concepts like dogs and cats.
[00:27:59.160 --> 00:28:01.440]   That seems like something that we can definitely do.
[00:28:01.440 --> 00:28:04.200]   But then the question becomes, at what level
[00:28:04.200 --> 00:28:06.400]   do we really need to encourage this interpretability?
[00:28:06.400 --> 00:28:09.120]   So do we need to encourage it at the level of individual neurons,
[00:28:09.120 --> 00:28:09.960]   as we've done before?
[00:28:09.960 --> 00:28:12.840]   Or can we maybe encourage it basically just--
[00:28:12.840 --> 00:28:14.640]   it needs to encode the concept somewhere,
[00:28:14.640 --> 00:28:16.360]   but it doesn't have to be individual neurons.
[00:28:16.360 --> 00:28:17.920]   I think the individual neuron thing is interesting.
[00:28:17.920 --> 00:28:19.200]   I definitely think that there are ways
[00:28:19.200 --> 00:28:21.360]   that we can adapt this metric into something that's
[00:28:21.360 --> 00:28:23.360]   explicitly optimizable so that we can actually
[00:28:23.360 --> 00:28:27.000]   use it as some sort of auxiliary regularization technique.
[00:28:27.000 --> 00:28:28.440]   And maybe the benefit we get there
[00:28:28.440 --> 00:28:30.760]   is very, very clean interpretability.
[00:28:30.760 --> 00:28:33.160]   We know there is a cat and dog neuron in this model
[00:28:33.160 --> 00:28:36.060]   because we've trained the model to expose that feature.
[00:28:36.060 --> 00:28:38.060]   Whether or not it matters for performance at all,
[00:28:38.060 --> 00:28:40.480]   compared to maybe more general regularization techniques,
[00:28:40.480 --> 00:28:41.180]   I have no clue.
[00:28:41.180 --> 00:28:43.120]   But it's an interesting avenue for sure.
[00:28:43.120 --> 00:28:44.720]   Yeah, I think that would be cool.
[00:28:44.720 --> 00:28:47.960]   It seems-- yeah, the challenge that I immediately see
[00:28:47.960 --> 00:28:51.600]   is coming up with a way to quantify it so that you can
[00:28:51.600 --> 00:28:54.240]   include it easily in backprop, right?
[00:28:54.240 --> 00:28:56.240]   Yeah, so the basic idea, I think,
[00:28:56.240 --> 00:28:58.260]   is to do some sort of continuous version
[00:28:58.260 --> 00:28:59.840]   of this discrete thing, right?
[00:28:59.840 --> 00:29:01.900]   So instead of having to segment, we
[00:29:01.900 --> 00:29:04.140]   could imagine just doing kind of a soft matching
[00:29:04.140 --> 00:29:06.320]   between the activations of a neuron
[00:29:06.320 --> 00:29:08.920]   and these ground truth concepts.
[00:29:08.920 --> 00:29:11.240]   And that would allow us to backprop through it.
[00:29:11.240 --> 00:29:13.840]   The other challenge then is just figuring out,
[00:29:13.840 --> 00:29:16.440]   but when neurons don't learn concepts at all,
[00:29:16.440 --> 00:29:17.840]   how do we know or how do we--
[00:29:17.840 --> 00:29:20.320]   which concepts are we pointing our neurons to in the first
[00:29:20.320 --> 00:29:21.280]   place, right?
[00:29:21.280 --> 00:29:23.200]   And generating these explanations takes time.
[00:29:23.200 --> 00:29:26.600]   And that's somewhat unclear.
[00:29:26.600 --> 00:29:27.560]   Definitely.
[00:29:27.560 --> 00:29:30.200]   So one thing, I guess I kind of want
[00:29:30.200 --> 00:29:32.720]   to ask what makes--
[00:29:32.720 --> 00:29:34.760]   what do you think makes individual neurons more
[00:29:34.760 --> 00:29:37.200]   explainable versus less explainable?
[00:29:37.200 --> 00:29:40.800]   So I mean, things like architecture choices,
[00:29:40.800 --> 00:29:44.760]   regularization choices, data augmentation, or its absence,
[00:29:44.760 --> 00:29:46.040]   do you have any sense for that?
[00:29:46.040 --> 00:29:52.400]   Yeah, it's a really tricky question.
[00:29:52.400 --> 00:29:55.440]   I think a lot of it is going to be driven by data.
[00:29:55.440 --> 00:29:58.720]   I think a neuron is predisposed to do the easiest possible
[00:29:58.720 --> 00:30:01.240]   thing, which is just to latch on to something non-interpretable
[00:30:01.240 --> 00:30:04.720]   or spurious, as long as the data doesn't really sufficiently
[00:30:04.720 --> 00:30:07.720]   test the capabilities of that neuron in a way, right?
[00:30:07.720 --> 00:30:09.480]   So we can use these cheats to kind of get away
[00:30:09.480 --> 00:30:15.200]   with simple cheats.
[00:30:15.200 --> 00:30:16.640]   That, for example, is an explanation
[00:30:16.640 --> 00:30:18.680]   of why there might be polysematicity in neurons,
[00:30:18.680 --> 00:30:21.440]   that a neuron can serve double duty as a bakery and a shop
[00:30:21.440 --> 00:30:24.000]   front, for example, because it never has to really cleanly
[00:30:24.000 --> 00:30:26.480]   distinguish between bakeries and shop fronts, right?
[00:30:26.480 --> 00:30:28.520]   If we had a more robust data set where you really
[00:30:28.520 --> 00:30:30.520]   had to differentiate between the two,
[00:30:30.520 --> 00:30:32.600]   the neuron might be forced to be more interpretable,
[00:30:32.600 --> 00:30:33.760]   or the entire model in general might
[00:30:33.760 --> 00:30:35.240]   be forced to be more interpretable
[00:30:35.240 --> 00:30:37.360]   and better kind of organize its representation
[00:30:37.360 --> 00:30:40.840]   space into meaningful ways.
[00:30:40.840 --> 00:30:43.640]   And is there a clean way to distinguish
[00:30:43.640 --> 00:30:48.720]   between apparent polysemanticity of an individual neuron
[00:30:48.720 --> 00:30:50.960]   and just representations being, say,
[00:30:50.960 --> 00:30:57.320]   like holographic or dense in the layer of all the neurons?
[00:30:57.320 --> 00:30:58.320]   That's a great question.
[00:31:02.480 --> 00:31:04.120]   Yeah, I would say not entirely.
[00:31:04.120 --> 00:31:07.120]   So I mean, this work really looked at individual neurons.
[00:31:07.120 --> 00:31:09.720]   And so we really can't determine the degree to which,
[00:31:09.720 --> 00:31:11.960]   for example, a neuron is polysemantic for bakeries
[00:31:11.960 --> 00:31:12.880]   and shop fronts, right?
[00:31:12.880 --> 00:31:14.480]   But maybe it's just like one component
[00:31:14.480 --> 00:31:17.280]   of a much larger distributed bakery detector.
[00:31:17.280 --> 00:31:19.840]   I think the most promising work that
[00:31:19.840 --> 00:31:22.640]   tries to identify that is by looking
[00:31:22.640 --> 00:31:25.320]   at what kind of neurons activate for individual predictions,
[00:31:25.320 --> 00:31:25.800]   right?
[00:31:25.800 --> 00:31:27.960]   So given a single bakery image, take a look
[00:31:27.960 --> 00:31:29.240]   at the neurons that are firing.
[00:31:29.240 --> 00:31:31.800]   Maybe it turns out that there are multiple bakery detectors.
[00:31:31.800 --> 00:31:35.000]   And so we can actually identify the specific subspace that
[00:31:35.000 --> 00:31:37.160]   lights up for bakery, for example.
[00:31:37.160 --> 00:31:39.480]   That's definitely an avenue of future work, which I think
[00:31:39.480 --> 00:31:41.240]   is definitely worth pursuing.
[00:31:41.240 --> 00:31:43.080]   Yeah, it seems like one of the challenges
[00:31:43.080 --> 00:31:46.000]   there is that one of the problems you solved elegantly
[00:31:46.000 --> 00:31:48.520]   in this paper is like, OK, if we combine
[00:31:48.520 --> 00:31:51.040]   a bunch of simple things in a principled fashion,
[00:31:51.040 --> 00:31:55.240]   then we get access to a large space of possible explanations.
[00:31:55.240 --> 00:31:57.400]   But then when you try to understand polysemiticity,
[00:31:57.400 --> 00:32:00.000]   you also have another exponential explosion problem,
[00:32:00.000 --> 00:32:03.560]   which is that all the possible polysemantic combination--
[00:32:03.560 --> 00:32:05.960]   or no, all the possible combinations
[00:32:05.960 --> 00:32:07.880]   of neurons that could be representing a concept
[00:32:07.880 --> 00:32:12.760]   is way bigger than just the set of all neurons in the layer.
[00:32:12.760 --> 00:32:14.880]   Yeah.
[00:32:14.880 --> 00:32:17.040]   Yeah, that's the challenge.
[00:32:17.040 --> 00:32:19.200]   And there's not a clear trade-off between the two.
[00:32:19.200 --> 00:32:22.400]   I mean, this is why people are advocating for the supervised
[00:32:22.400 --> 00:32:23.640]   methods, right?
[00:32:23.640 --> 00:32:25.720]   And maybe we need to look towards that literature
[00:32:25.720 --> 00:32:29.320]   to really identify concepts that basically in any way, shape,
[00:32:29.320 --> 00:32:31.920]   or form are somehow encoded in representations.
[00:32:31.920 --> 00:32:33.920]   But of course, that already has its own debates.
[00:32:33.920 --> 00:32:35.160]   Although I think we're getting to steps
[00:32:35.160 --> 00:32:36.960]   where we're beginning to understand the kind
[00:32:36.960 --> 00:32:38.200]   of information that's encoded.
[00:32:38.200 --> 00:32:39.600]   But certainly, you can definitely
[00:32:39.600 --> 00:32:43.080]   make, say, slightly simpler assumptions about the way
[00:32:43.080 --> 00:32:45.800]   concepts are encoded and then begin to look at, for example,
[00:32:45.800 --> 00:32:47.200]   multi-neuron concepts, right?
[00:32:47.200 --> 00:32:49.720]   So maybe it's not dramatically too difficult
[00:32:49.720 --> 00:32:51.720]   to imagine that individual neurons fly
[00:32:51.720 --> 00:32:53.280]   for certain concepts and then consider
[00:32:53.280 --> 00:32:55.840]   a limited subset of linear combinations of those neurons
[00:32:55.840 --> 00:32:58.720]   and identify to what extent those fly for concepts.
[00:32:58.720 --> 00:33:02.560]   That might not increase the search space too much.
[00:33:02.560 --> 00:33:04.040]   Yeah, that does make sense.
[00:33:04.040 --> 00:33:07.480]   What role do you think that optimization-based explanations
[00:33:07.480 --> 00:33:09.480]   of neurons have to play?
[00:33:09.480 --> 00:33:13.240]   So basically like Lucid or Deep Dream, where it's like, OK,
[00:33:13.240 --> 00:33:15.680]   take an image or take a baseline and increase
[00:33:15.680 --> 00:33:18.080]   the activation of this neuron, this linear subspace
[00:33:18.080 --> 00:33:20.320]   of neurons, do you think that's something that could help
[00:33:20.320 --> 00:33:21.840]   with other kinds of explanation?
[00:33:21.840 --> 00:33:22.720]   Is it orthogonal?
[00:33:22.720 --> 00:33:24.160]   Is it maybe misleading?
[00:33:24.160 --> 00:33:24.960]   What do you think?
[00:33:24.960 --> 00:33:27.200]   No, I think all that work is really, really excellent.
[00:33:27.200 --> 00:33:28.920]   I think the main thing is that there's
[00:33:28.920 --> 00:33:32.080]   a lot of really interesting kind of model visualization
[00:33:32.080 --> 00:33:35.960]   or interpretability techniques that require that a practitioner
[00:33:35.960 --> 00:33:38.800]   implement this algorithm or runs it and then inspects
[00:33:38.800 --> 00:33:40.080]   the activation patterns, right?
[00:33:40.080 --> 00:33:41.440]   And so you do some Deep Dream thing.
[00:33:41.440 --> 00:33:44.000]   You say, oh, this is the neuron that this is firing for, right?
[00:33:44.000 --> 00:33:45.400]   But the problem is that you have to go in there
[00:33:45.400 --> 00:33:47.080]   and actually look at the neuron and say,
[00:33:47.080 --> 00:33:49.200]   oh, this is fighting for dogs and cats, right?
[00:33:49.200 --> 00:33:53.000]   There is no automatic explanation that's generated.
[00:33:53.000 --> 00:33:55.040]   And so what results is that it can be potentially
[00:33:55.040 --> 00:33:56.360]   very time consuming.
[00:33:56.360 --> 00:33:58.720]   So the kind of approach that we're describing here
[00:33:58.720 --> 00:34:01.080]   is more of like an automated analysis, where it's like,
[00:34:01.080 --> 00:34:03.880]   let's automatically identify behaviors that are encoded
[00:34:03.880 --> 00:34:07.000]   and maybe try to surface the interesting ones.
[00:34:07.000 --> 00:34:09.800]   And so I think in some ways are complementary, right?
[00:34:09.800 --> 00:34:12.880]   Like you do automated analysis of the type described here.
[00:34:12.880 --> 00:34:14.880]   And then maybe if you need deeper dives or even
[00:34:14.880 --> 00:34:17.200]   pre-revisualizations, you can inspect individual neurons
[00:34:17.200 --> 00:34:19.240]   by themselves.
[00:34:19.240 --> 00:34:20.160]   Yeah.
[00:34:20.160 --> 00:34:21.600]   That-- yeah, that makes sense.
[00:34:21.600 --> 00:34:26.040]   It does seem-- yeah, that Chris said something at a talk
[00:34:26.040 --> 00:34:30.160]   I saw once of just like he'd spent thousands of hours
[00:34:30.160 --> 00:34:34.960]   with just the one that they did for circuits.
[00:34:34.960 --> 00:34:36.760]   I forget whether it was Inception V1,
[00:34:36.760 --> 00:34:37.760]   I think is what it was.
[00:34:37.760 --> 00:34:39.400]   Anyway, he spent thousands of hours
[00:34:39.400 --> 00:34:41.520]   with just this neural network, like looking
[00:34:41.520 --> 00:34:42.640]   through all of its neurons.
[00:34:42.640 --> 00:34:44.280]   He knows them by name.
[00:34:44.280 --> 00:34:48.120]   He's got friends and enemies in the units of Inception.
[00:34:48.120 --> 00:34:50.080]   And that's useful.
[00:34:50.080 --> 00:34:51.120]   And we've learned a lot.
[00:34:51.120 --> 00:34:53.240]   But it's not scalable the way an automated approach
[00:34:53.240 --> 00:34:54.400]   like yours is.
[00:34:54.400 --> 00:34:56.000]   Yeah, I think as a result, like,
[00:34:56.000 --> 00:34:58.920]   that work has generated just much richer understanding
[00:34:58.920 --> 00:35:01.200]   of the way that certain constants are composed together
[00:35:01.200 --> 00:35:04.120]   in neural networks to form more sophisticated object
[00:35:04.120 --> 00:35:05.560]   detectors.
[00:35:05.560 --> 00:35:07.360]   But you're right, it's not scalable, right?
[00:35:07.360 --> 00:35:08.800]   So the goal, I think, is to bring
[00:35:08.800 --> 00:35:10.800]   this kind of automatic generation
[00:35:10.800 --> 00:35:13.360]   closer to the kinds of insights that this kind of hand-carried
[00:35:13.360 --> 00:35:15.680]   circuits work can bring us.
[00:35:15.680 --> 00:35:16.320]   That's cool.
[00:35:16.320 --> 00:35:17.640]   That's a great-- that's a great ambition.
[00:35:17.640 --> 00:35:19.760]   And I look forward to seeing the rest of your work.
[00:35:19.760 --> 00:35:23.840]   That's all the time we have for the first speaker slot.
[00:35:23.840 --> 00:35:26.240]   So thank you for coming.
[00:35:26.240 --> 00:35:27.520]   Thank you for sharing.
[00:35:27.520 --> 00:35:30.720]   I look forward to see the rest of your work in the future.
[00:35:30.720 --> 00:35:31.720]   Yeah, no, really happy.
[00:35:31.720 --> 00:35:33.480]   Thanks.
[00:35:33.480 --> 00:35:34.520]   All right.
[00:35:34.520 --> 00:35:41.040]   So today is a very special instance
[00:35:41.040 --> 00:35:47.440]   of the Weights and Biases Deep Learning Salon, which
[00:35:47.440 --> 00:35:48.600]   is--
[00:35:48.600 --> 00:35:49.960]   I'm going to give a talk.
[00:35:49.960 --> 00:35:54.720]   So in case you were not yet tired of hearing my voice,
[00:35:54.720 --> 00:35:58.120]   the talk I'm going to give has the provocative title,
[00:35:58.120 --> 00:36:01.280]   Neural Networks Are for Typecasting.
[00:36:01.280 --> 00:36:05.600]   So the TLDR here is that the embeddings
[00:36:05.600 --> 00:36:08.160]   of a neural network obtained by training on a difficult task
[00:36:08.160 --> 00:36:10.960]   are often more useful than the outputs.
[00:36:10.960 --> 00:36:13.440]   And the main contribution here is
[00:36:13.440 --> 00:36:17.000]   that I like to think about these embeddings, actually,
[00:36:17.000 --> 00:36:21.800]   as types, that they're sort of analogous to 32-bit floats
[00:36:21.800 --> 00:36:23.240]   or 8-bit integers.
[00:36:23.240 --> 00:36:27.280]   And the only difference is that they are learned rather than
[00:36:27.280 --> 00:36:29.240]   designed.
[00:36:29.240 --> 00:36:31.480]   And so the goal here is basically
[00:36:31.480 --> 00:36:36.320]   to present an intuition for embeddings in neural networks
[00:36:36.320 --> 00:36:39.840]   that are maybe closer to the intuition
[00:36:39.840 --> 00:36:42.360]   that people who have a programming and CS background
[00:36:42.360 --> 00:36:45.480]   bring, rather than a math and physics background, which
[00:36:45.480 --> 00:36:49.080]   is where the idea of embeddings comes from.
[00:36:49.080 --> 00:36:52.680]   Or if you're not one for nuance, if I
[00:36:52.680 --> 00:36:54.960]   wanted to put this talk out there on the internet
[00:36:54.960 --> 00:36:58.840]   to get clout and put out my hot takes,
[00:36:58.840 --> 00:37:04.560]   the purpose of neural networks is just glorified typecasting.
[00:37:04.560 --> 00:37:06.960]   So to be clear, when I say types,
[00:37:06.960 --> 00:37:09.480]   a type is just a collection of data values with a name.
[00:37:09.480 --> 00:37:11.640]   So integers, this is a collection
[00:37:11.640 --> 00:37:14.800]   of bit strings that have a particular name.
[00:37:14.800 --> 00:37:17.680]   Dictionaries, phone numbers, these are the same thing.
[00:37:17.680 --> 00:37:20.560]   And that name basically just describes how those values
[00:37:20.560 --> 00:37:21.840]   are to be interpreted.
[00:37:21.840 --> 00:37:25.360]   Maybe it suggests, OK, what do you do with a phone number?
[00:37:25.360 --> 00:37:27.680]   Well, maybe you pull off the area code,
[00:37:27.680 --> 00:37:30.960]   or maybe you send it to a phone app.
[00:37:30.960 --> 00:37:33.500]   Whereas an integer, even though it might have the same underlying
[00:37:33.500 --> 00:37:35.880]   bit representation, you would do something very different
[00:37:35.880 --> 00:37:36.640]   with it.
[00:37:36.640 --> 00:37:39.100]   And it's important to note that though types are completely
[00:37:39.100 --> 00:37:41.320]   unnecessary for computation, you can do anything
[00:37:41.320 --> 00:37:44.120]   that you could do with types without them.
[00:37:44.120 --> 00:37:47.880]   They're extremely useful for thinking about computation.
[00:37:47.880 --> 00:37:49.720]   And so when we want to organize our thoughts
[00:37:49.720 --> 00:37:53.040]   around our computation, when we want
[00:37:53.040 --> 00:37:56.440]   to communicate what's important, what's not important in our
[00:37:56.440 --> 00:37:59.040]   programs, or when we want to obtain
[00:37:59.040 --> 00:38:02.160]   a mathematical understanding of what is and is not
[00:38:02.160 --> 00:38:04.120]   possible with different kinds of computing,
[00:38:04.120 --> 00:38:06.480]   adding types is very useful, even down
[00:38:06.480 --> 00:38:09.320]   at the level of models of computation,
[00:38:09.320 --> 00:38:12.920]   like the typed lambda calculus as opposed to the just baseline
[00:38:12.920 --> 00:38:16.960]   lambda calculus, these very low-level models of what
[00:38:16.960 --> 00:38:18.240]   computation is.
[00:38:18.240 --> 00:38:20.880]   So this is a really useful idea and abstraction.
[00:38:20.880 --> 00:38:23.080]   And one of the points of this talk
[00:38:23.080 --> 00:38:25.360]   is to bring some of these ideas about types
[00:38:25.360 --> 00:38:28.240]   into neural networks.
[00:38:28.240 --> 00:38:29.920]   So the fundamental purpose of types
[00:38:29.920 --> 00:38:31.880]   is to give meaning to binary values.
[00:38:31.880 --> 00:38:36.200]   So this binary string that I've got there, 11001001, et
[00:38:36.200 --> 00:38:38.640]   cetera, could be interpreted as two different things.
[00:38:38.640 --> 00:38:40.920]   It could be interpreted as a bitmap, in which case
[00:38:40.920 --> 00:38:43.680]   you get the image in the bottom left corner of the slide.
[00:38:43.680 --> 00:38:47.160]   Or it could be interpreted as a real number, a binary sequence
[00:38:47.160 --> 00:38:49.800]   that corresponds to a particular real number.
[00:38:49.800 --> 00:38:52.440]   And in that case, it would be interpreted as pi.
[00:38:52.440 --> 00:38:55.280]   This is the binary expansion of pi.
[00:38:55.280 --> 00:38:59.080]   And once we have types, we can convert in between types.
[00:38:59.080 --> 00:39:01.680]   So maybe at one point, I need to, say,
[00:39:01.680 --> 00:39:05.960]   communicate this number pi through a QR code type
[00:39:05.960 --> 00:39:06.800]   mechanism.
[00:39:06.800 --> 00:39:08.440]   And then this image representation of pi
[00:39:08.440 --> 00:39:09.760]   would be really useful.
[00:39:09.760 --> 00:39:12.040]   Or maybe I want to compress it, try compressing it,
[00:39:12.040 --> 00:39:15.280]   and I want to use, say, an image compression algorithm to do it.
[00:39:15.280 --> 00:39:17.200]   So being able to convert it to a different type
[00:39:17.200 --> 00:39:21.280]   gives me access to a whole bunch of different algorithms,
[00:39:21.280 --> 00:39:23.520]   algorithms that are not available for working
[00:39:23.520 --> 00:39:27.640]   with just binary sequences that are real numbers.
[00:39:27.640 --> 00:39:30.360]   And new types can just, in general, make your life easier.
[00:39:30.360 --> 00:39:33.240]   So for floats, multiplying is easy, but adding is hard,
[00:39:33.240 --> 00:39:34.640]   because fundamentally, floats are
[00:39:34.640 --> 00:39:37.480]   a logarithmic representation of numbers.
[00:39:37.480 --> 00:39:39.960]   Whereas for binary numbers, just binary sequences
[00:39:39.960 --> 00:39:41.760]   to represent our numbers, multiplying
[00:39:41.760 --> 00:39:45.360]   becomes kind of hard, but adding is really, really easy.
[00:39:45.360 --> 00:39:47.800]   Adding just basically becomes the XOR operation.
[00:39:47.800 --> 00:39:49.960]   It's not even just a single line of Python.
[00:39:49.960 --> 00:39:52.080]   It's like a single CPU instruction,
[00:39:52.080 --> 00:39:54.760]   which is amazingly simple.
[00:39:54.760 --> 00:39:57.120]   And so just switching to a different type
[00:39:57.120 --> 00:39:59.640]   can sometimes take a problem that was really difficult
[00:39:59.640 --> 00:40:02.160]   and turn it into a problem that is really easy.
[00:40:02.160 --> 00:40:04.880]   So for example, if you're manipulating a directory
[00:40:04.880 --> 00:40:07.920]   by its name, if that name is a string,
[00:40:07.920 --> 00:40:09.440]   then there are lots of problems that
[00:40:09.440 --> 00:40:11.320]   can be kind of difficult to solve,
[00:40:11.320 --> 00:40:13.680]   unless you convert it to a path first.
[00:40:13.680 --> 00:40:17.160]   So the path object available in Python's pathlib
[00:40:17.160 --> 00:40:19.240]   is like a representation--
[00:40:19.240 --> 00:40:20.880]   it's basically you put in a string,
[00:40:20.880 --> 00:40:24.360]   and you get out a structured representation in which you
[00:40:24.360 --> 00:40:26.520]   can easily make the manipulations that you
[00:40:26.520 --> 00:40:27.880]   need to make with directories.
[00:40:27.880 --> 00:40:29.920]   It's much harder to treat it the way you would normally
[00:40:29.920 --> 00:40:30.920]   treat a string.
[00:40:30.920 --> 00:40:32.920]   It's harder to, say, make it all uppercase.
[00:40:32.920 --> 00:40:34.720]   But you don't need to do that with a path.
[00:40:34.720 --> 00:40:36.840]   You need to do things like get rid of the directory
[00:40:36.840 --> 00:40:40.080]   or append a file extension, something like that.
[00:40:40.080 --> 00:40:42.040]   So I'm going to work through an example of this
[00:40:42.040 --> 00:40:45.360]   to make sort of really clear just how important it
[00:40:45.360 --> 00:40:49.560]   can be to think in terms of different types.
[00:40:49.560 --> 00:40:51.640]   And so the example is going to be based off
[00:40:51.640 --> 00:40:56.080]   of a failed Python enhancement proposal, PEP 313, which
[00:40:56.080 --> 00:40:59.880]   was an April Fool's suggestion to add Roman numeral
[00:40:59.880 --> 00:41:01.960]   literals to Python.
[00:41:02.480 --> 00:41:05.920]   Unfortunately, this PEP was rejected and not
[00:41:05.920 --> 00:41:07.320]   added to Python.
[00:41:07.320 --> 00:41:10.680]   And so there are no Roman numeral literals in Python.
[00:41:10.680 --> 00:41:12.160]   So if we want Roman numerals, we're
[00:41:12.160 --> 00:41:15.280]   going to need to roll our own.
[00:41:15.280 --> 00:41:17.800]   So what we want to be able to do, at least to just get
[00:41:17.800 --> 00:41:20.080]   started with this Roman numerals type,
[00:41:20.080 --> 00:41:23.960]   is we would want to take the built-in integer
[00:41:23.960 --> 00:41:27.800]   type in Python, which can handle Arabic numerals,
[00:41:27.800 --> 00:41:31.400]   so the numerals that folks who write their numbers in English
[00:41:31.400 --> 00:41:33.000]   are very familiar with.
[00:41:33.000 --> 00:41:35.560]   And we want to take those kinds of integers
[00:41:35.560 --> 00:41:38.000]   and turn them into Roman numerals.
[00:41:38.000 --> 00:41:39.840]   Now, a fun fact about Roman numerals
[00:41:39.840 --> 00:41:43.400]   is actually they stop at 3,999.
[00:41:43.400 --> 00:41:45.760]   The Romans did not need to count things as big
[00:41:45.760 --> 00:41:52.120]   as we need to count in the year MMXX, the year 2020.
[00:41:52.120 --> 00:41:55.320]   But so we only need to focus on these numbers.
[00:41:55.320 --> 00:41:57.480]   But even just in this smaller set of numbers
[00:41:57.480 --> 00:42:00.600]   that the Roman numerals properly cover,
[00:42:00.600 --> 00:42:02.720]   there's actually some interesting stuff going on.
[00:42:02.720 --> 00:42:05.000]   So it starts off looking like a tally system.
[00:42:05.000 --> 00:42:07.080]   1 is a single i.
[00:42:07.080 --> 00:42:08.320]   2 is two i's.
[00:42:08.320 --> 00:42:09.400]   3 is three i's.
[00:42:09.400 --> 00:42:12.520]   But then 4 comes out of nowhere with an IV.
[00:42:12.520 --> 00:42:15.640]   And then 5 is a V. After that, 6 is a VI.
[00:42:15.640 --> 00:42:19.680]   Then these more complicated patterns start to arise.
[00:42:19.680 --> 00:42:24.080]   And by the time we get to 3,999, the mapping
[00:42:24.080 --> 00:42:27.240]   between the number in Arabic and the Roman numeral
[00:42:27.240 --> 00:42:29.200]   representation is not so simple.
[00:42:29.200 --> 00:42:35.640]   So we go from those four numbers, 3,999, to MMCMXCIX.
[00:42:35.640 --> 00:42:39.360]   And so just looking at this problem as a single step,
[00:42:39.360 --> 00:42:43.120]   as a type conversion from Arabic to Roman numerals,
[00:42:43.120 --> 00:42:45.120]   this looks pretty challenging.
[00:42:45.120 --> 00:42:47.000]   But there's this actually great blog post
[00:42:47.000 --> 00:42:51.080]   by Sandy Metz, who is a Ruby programmer, who
[00:42:51.080 --> 00:42:53.320]   described an insight that actually there
[00:42:53.320 --> 00:42:55.400]   used to be several kinds of Roman numerals.
[00:42:55.400 --> 00:42:59.160]   And one of them is what she calls additive Roman numerals.
[00:42:59.160 --> 00:43:01.640]   And so converting-- the additive Roman numerals
[00:43:01.640 --> 00:43:03.100]   are actually really, really simple.
[00:43:03.100 --> 00:43:06.520]   So the Roman numeral for 4, or the additive Roman numeral
[00:43:06.520 --> 00:43:09.480]   for 4, is just I, I, I, I. It's four i's.
[00:43:09.480 --> 00:43:14.080]   And then the additive Roman numeral for 9 is VIII.
[00:43:14.080 --> 00:43:17.280]   And so it's much simpler to convert from Arabic
[00:43:17.280 --> 00:43:19.920]   to these additive Roman numerals than to convert from Arabic
[00:43:19.920 --> 00:43:24.160]   to the Roman numerals, like the ones everybody knows about,
[00:43:24.160 --> 00:43:26.000]   which she calls subtractive Roman numerals,
[00:43:26.000 --> 00:43:28.460]   because they've got this special thing where sometimes you
[00:43:28.460 --> 00:43:30.160]   subtract one of the values.
[00:43:30.160 --> 00:43:35.000]   I from V gives you 4 in normal Roman numerals.
[00:43:35.000 --> 00:43:36.800]   And so it's really easy, actually,
[00:43:36.800 --> 00:43:41.120]   to translate from additive Roman numerals to Roman numerals.
[00:43:41.120 --> 00:43:43.960]   It's very straightforward to just
[00:43:43.960 --> 00:43:47.600]   replace those specific patterns with this subtraction.
[00:43:47.600 --> 00:43:52.240]   And so her insight was actually converting from Arabic to Roman
[00:43:52.240 --> 00:43:56.600]   is one idea, but that should actually be sort of split up
[00:43:56.600 --> 00:43:57.720]   into two pieces.
[00:43:57.720 --> 00:44:00.320]   Arabic is much closer to additive Roman numerals
[00:44:00.320 --> 00:44:03.200]   than it is to Roman numerals, so let's do that step first,
[00:44:03.200 --> 00:44:06.440]   rather than sort of conflating these ideas into two steps.
[00:44:06.440 --> 00:44:08.880]   So I actually had read this blog post a while ago,
[00:44:08.880 --> 00:44:12.320]   but tracked it back down for the purposes of this talk,
[00:44:12.320 --> 00:44:14.280]   and decided to implement it myself.
[00:44:14.280 --> 00:44:16.080]   And in the end, it turned out to be actually
[00:44:16.080 --> 00:44:18.240]   just as easy as advertised.
[00:44:18.240 --> 00:44:21.000]   I needed to do a little bit of chin scratching,
[00:44:21.000 --> 00:44:24.440]   but in the end, I got this really, really simple
[00:44:24.440 --> 00:44:29.480]   20-something lines of Python to convert Roman numerals
[00:44:29.480 --> 00:44:33.520]   from their integer representations.
[00:44:33.520 --> 00:44:36.200]   And just as she said, it was so much easier
[00:44:36.200 --> 00:44:38.240]   once I thought of it as a multi-step process
[00:44:38.240 --> 00:44:41.720]   with multiple types in it, not just Arabic numerals and Roman
[00:44:41.720 --> 00:44:44.720]   numerals, but also this new additive Roman numeral type.
[00:44:44.720 --> 00:44:47.400]   And I even added an additional type in there,
[00:44:47.400 --> 00:44:49.960]   a semi-additive string to make it even simpler,
[00:44:49.960 --> 00:44:56.320]   and turned it into basically just three steps, essentially.
[00:44:56.320 --> 00:45:01.160]   First to this additive type, then to a semi-additive type,
[00:45:01.160 --> 00:45:04.400]   then finally to this subtractive Roman numeral type,
[00:45:04.400 --> 00:45:09.480]   using only the very most basic of Python operations.
[00:45:09.480 --> 00:45:11.800]   So new types can just make your life a lot easier.
[00:45:11.800 --> 00:45:14.240]   We already saw that add Roman is--
[00:45:14.240 --> 00:45:16.320]   additive Roman is much easier to translate
[00:45:16.320 --> 00:45:17.800]   into the full Roman numerals.
[00:45:17.800 --> 00:45:19.640]   But it's also true that addition and multiplication
[00:45:19.640 --> 00:45:22.320]   are actually kind of easier in additive Roman.
[00:45:22.320 --> 00:45:25.600]   So to implement addition on Roman numerals, which
[00:45:25.600 --> 00:45:27.080]   you actually probably want to do,
[00:45:27.080 --> 00:45:29.360]   is convert them to this additive Roman format,
[00:45:29.360 --> 00:45:32.520]   then implement addition there, and then maybe transmit it back
[00:45:32.520 --> 00:45:35.240]   to Roman numerals.
[00:45:35.240 --> 00:45:37.720]   So the addition of this type gives us--
[00:45:37.720 --> 00:45:39.760]   not only does it make our initial problem easier,
[00:45:39.760 --> 00:45:40.840]   but it actually makes a whole bunch
[00:45:40.840 --> 00:45:44.120]   of other things we might want to do with Roman numerals easier.
[00:45:44.120 --> 00:45:47.360]   Because we've got this nice representation,
[00:45:47.360 --> 00:45:50.600]   this alternate representation in a different type.
[00:45:50.600 --> 00:45:53.880]   And so what this means is this additive Roman type
[00:45:53.880 --> 00:45:56.640]   becomes kind of like a hub if we want to switch between things.
[00:45:56.640 --> 00:45:59.120]   So if we want to switch over to a Babylonian number system,
[00:45:59.120 --> 00:46:02.440]   which is sexagesimal, meaning it is base 60,
[00:46:02.440 --> 00:46:04.120]   it's where our time system comes from.
[00:46:04.120 --> 00:46:05.640]   If we want to switch over to that,
[00:46:05.640 --> 00:46:07.800]   additive Roman, easier to switch over to Babylonian
[00:46:07.800 --> 00:46:08.640]   than from Roman.
[00:46:08.640 --> 00:46:11.200]   Similarly, if we wanted to switch over to pure tally marks,
[00:46:11.200 --> 00:46:13.160]   it's extremely trivial to switch over
[00:46:13.160 --> 00:46:14.920]   to tally marks from additive Roman,
[00:46:14.920 --> 00:46:17.680]   but much harder in regular Roman.
[00:46:17.680 --> 00:46:20.840]   So if we make something that can convert between these types,
[00:46:20.840 --> 00:46:23.080]   then we actually give ourselves essentially
[00:46:23.080 --> 00:46:26.080]   like an exponential increase in the number of things
[00:46:26.080 --> 00:46:30.160]   that we can do just by doing type conversions.
[00:46:30.160 --> 00:46:32.640]   The issue with this is that new types are actually
[00:46:32.640 --> 00:46:33.880]   kind of hard to find.
[00:46:33.880 --> 00:46:36.000]   So we were able to make this additive Roman type.
[00:46:36.000 --> 00:46:38.720]   And actually, I think in Sandy's description,
[00:46:38.720 --> 00:46:41.000]   she found it in the Wikipedia page describing
[00:46:41.000 --> 00:46:43.320]   the different versions of Roman numerals
[00:46:43.320 --> 00:46:44.280]   that are around there.
[00:46:44.280 --> 00:46:46.800]   But the types that we can make are actually pretty limited,
[00:46:46.800 --> 00:46:48.800]   as always, when we're doing traditional computing
[00:46:48.800 --> 00:46:52.480]   with programming in Python, rather than machine learning,
[00:46:52.480 --> 00:46:53.920]   by our capacity to reason.
[00:46:53.920 --> 00:46:55.960]   We have to reason our way to a type.
[00:46:55.960 --> 00:46:57.640]   We have to say, okay, here's the way
[00:46:57.640 --> 00:46:59.680]   this binary representation is used.
[00:46:59.680 --> 00:47:02.160]   Let's turn it into a different binary representation
[00:47:02.160 --> 00:47:05.040]   and use it in a different way.
[00:47:05.040 --> 00:47:08.480]   And we have to think through that explicitly.
[00:47:08.480 --> 00:47:10.800]   It wouldn't be great if we could actually just sort of like
[00:47:10.800 --> 00:47:13.280]   discover the types that we need, right?
[00:47:13.280 --> 00:47:16.960]   If we could automate the process of finding the types
[00:47:16.960 --> 00:47:19.200]   that we need for our computer programs,
[00:47:19.200 --> 00:47:21.160]   that would be really, really great.
[00:47:21.160 --> 00:47:24.360]   And the problem, I guess, for traditional machine learning
[00:47:24.360 --> 00:47:27.760]   is that linear models cannot discover new types.
[00:47:27.760 --> 00:47:29.480]   So if I do a linear prediction on data,
[00:47:29.480 --> 00:47:32.600]   so let's imagine a classic example of predicting
[00:47:32.600 --> 00:47:35.040]   whether something is a picture of a cat or a dog.
[00:47:35.040 --> 00:47:36.960]   If I do a linear prediction on that data,
[00:47:36.960 --> 00:47:39.240]   I really, all that happens is the data comes in,
[00:47:39.240 --> 00:47:41.800]   I do one operation on it.
[00:47:41.800 --> 00:47:45.240]   And actually, after that linear operation, I'm basically done.
[00:47:45.240 --> 00:47:47.400]   People include a softmax.
[00:47:47.400 --> 00:47:49.840]   That's not really strictly necessary
[00:47:49.840 --> 00:47:51.040]   to get out your prediction.
[00:47:51.040 --> 00:47:53.160]   Really, once you've done the linear model,
[00:47:53.160 --> 00:47:54.960]   or sorry, once you've done that linear transformation
[00:47:54.960 --> 00:47:57.880]   of taking the weights and dot-producting them
[00:47:57.880 --> 00:48:01.400]   with the data, you're basically just, you're done.
[00:48:01.400 --> 00:48:02.240]   That's it.
[00:48:02.240 --> 00:48:04.480]   So there's none of that ability to sort of like say,
[00:48:04.480 --> 00:48:06.200]   oh, what if this data were a different type?
[00:48:06.200 --> 00:48:09.120]   Maybe my prediction problem would be easier.
[00:48:09.120 --> 00:48:11.880]   Whereas neural networks can actually discover new types.
[00:48:11.880 --> 00:48:12.920]   And so if I build,
[00:48:12.920 --> 00:48:15.160]   if I think about what my neural network is doing,
[00:48:15.160 --> 00:48:17.040]   it's applying a couple of different
[00:48:17.040 --> 00:48:19.080]   transformations in sequence.
[00:48:19.080 --> 00:48:20.840]   Right here, I've basically only done one,
[00:48:20.840 --> 00:48:24.840]   but one could do it for like a much deeper network.
[00:48:24.840 --> 00:48:27.960]   And then at the very end, once I've gotten that,
[00:48:27.960 --> 00:48:33.320]   just before I'm ready to make my prediction,
[00:48:33.320 --> 00:48:35.760]   I actually apply that linear predict function.
[00:48:35.760 --> 00:48:38.960]   I take some weights, I multiply them by the activations
[00:48:38.960 --> 00:48:40.160]   of the previous layer,
[00:48:40.160 --> 00:48:41.680]   and then I pass it through a softmax
[00:48:41.680 --> 00:48:42.760]   and that gives me my prediction.
[00:48:42.760 --> 00:48:45.120]   So the final layer, actually,
[00:48:45.120 --> 00:48:49.040]   the top of the neural network is this linear prediction step.
[00:48:49.040 --> 00:48:52.000]   And so you could think of it as actually what's going on
[00:48:52.000 --> 00:48:54.280]   is that every other part of the neural network
[00:48:54.280 --> 00:48:58.680]   is doing a type conversion from data,
[00:48:58.680 --> 00:49:01.160]   the type data, which here is images,
[00:49:01.160 --> 00:49:04.040]   to the type embedding or embeddings of images.
[00:49:04.040 --> 00:49:06.640]   And those embeddings are gonna be some complicated,
[00:49:06.640 --> 00:49:08.440]   high dimensional representation.
[00:49:08.440 --> 00:49:12.840]   They're hard to understand what's going on in an embedding,
[00:49:12.840 --> 00:49:14.640]   but the property that they have
[00:49:14.640 --> 00:49:16.240]   is that it's really easy to use them
[00:49:16.240 --> 00:49:19.400]   to do the downstream prediction tasks, sort of by design.
[00:49:19.400 --> 00:49:23.600]   So this is typically called an embedding, right?
[00:49:23.600 --> 00:49:28.280]   So this view that what we're doing is creating a new type.
[00:49:28.280 --> 00:49:31.240]   The result of that type is typically called an embedding.
[00:49:31.240 --> 00:49:33.600]   And the intuition there is that our data
[00:49:33.600 --> 00:49:35.720]   sitting in its natural form,
[00:49:35.720 --> 00:49:38.000]   in its natural like binary format,
[00:49:38.000 --> 00:49:39.960]   or in its natural matrix format,
[00:49:39.960 --> 00:49:42.320]   is in this like tangled up state
[00:49:42.320 --> 00:49:45.400]   where maybe like pictures of some foxes and dogs
[00:49:45.400 --> 00:49:47.800]   are very close to pictures of some cats
[00:49:47.800 --> 00:49:49.160]   'cause they share colors.
[00:49:49.160 --> 00:49:50.600]   And then other pictures of things
[00:49:50.600 --> 00:49:53.760]   that are very much like dogs are very far away from dogs.
[00:49:53.760 --> 00:49:58.400]   And so it can be very difficult to, with a simple model,
[00:49:58.400 --> 00:50:00.960]   split out our dogs from our cats
[00:50:00.960 --> 00:50:03.800]   or solve in general our task.
[00:50:03.800 --> 00:50:06.040]   And so what happens instead is that
[00:50:06.040 --> 00:50:09.840]   in the course of that neural networks application
[00:50:09.840 --> 00:50:11.880]   of transformations to data,
[00:50:11.880 --> 00:50:15.040]   it produces an alternative coordinate space,
[00:50:15.040 --> 00:50:17.320]   an alternative representation of the data
[00:50:17.320 --> 00:50:20.240]   such that all the dogs are on say the left-hand side
[00:50:20.240 --> 00:50:23.080]   and all the cats are on the right-hand side
[00:50:23.080 --> 00:50:25.400]   and things that are kind of ambiguous right there,
[00:50:25.400 --> 00:50:26.960]   which we're not sure whether this, you know,
[00:50:26.960 --> 00:50:28.920]   pangolin is a dog or a cat.
[00:50:28.920 --> 00:50:33.160]   That's not one of our possible responses is not pangolin.
[00:50:33.160 --> 00:50:35.320]   And so it sort of ends up right here in the middle.
[00:50:35.320 --> 00:50:37.120]   And so this intuition that it's embedding
[00:50:37.120 --> 00:50:38.920]   comes from like the world of topology,
[00:50:38.920 --> 00:50:39.960]   comes from physics,
[00:50:39.960 --> 00:50:43.040]   the idea of like taking a collection of points
[00:50:43.040 --> 00:50:45.600]   and then putting them in a different shape
[00:50:45.600 --> 00:50:47.640]   in a different space.
[00:50:47.640 --> 00:50:50.200]   So it's an intuition that doesn't bring ideas
[00:50:50.200 --> 00:50:51.080]   from computer science
[00:50:51.080 --> 00:50:53.520]   so much as it does bring ideas from applied math.
[00:50:53.520 --> 00:50:56.280]   So the, I think,
[00:50:56.280 --> 00:51:00.760]   one of the consequences of this perspective
[00:51:00.760 --> 00:51:01.920]   is that we should think of neural networks
[00:51:01.920 --> 00:51:03.920]   as being cut kind of differently.
[00:51:03.920 --> 00:51:05.920]   The traditional way of representing a neural network,
[00:51:05.920 --> 00:51:10.680]   and this is from a Conde Nast/Tech article,
[00:51:10.680 --> 00:51:12.720]   is to think of them as an input layer,
[00:51:12.720 --> 00:51:15.480]   then hidden layers, then an output layer.
[00:51:15.480 --> 00:51:18.280]   And that emphasizes this like split
[00:51:18.280 --> 00:51:19.760]   between things that are visible
[00:51:19.760 --> 00:51:21.000]   and things that are invisible,
[00:51:21.000 --> 00:51:22.360]   sort of things that are on the interior
[00:51:22.360 --> 00:51:24.080]   and things that are on the exterior.
[00:51:24.080 --> 00:51:24.920]   But this is, I think,
[00:51:24.920 --> 00:51:26.560]   not the right way to split them up.
[00:51:26.560 --> 00:51:27.960]   The way we should sort of split them up
[00:51:27.960 --> 00:51:30.200]   is actually, so four parts.
[00:51:30.200 --> 00:51:31.760]   So rather than tripartite,
[00:51:31.760 --> 00:51:35.920]   a tetrapartite partition of our neural networks,
[00:51:35.920 --> 00:51:39.000]   where we have data and predictions on the ends there,
[00:51:39.000 --> 00:51:40.840]   that's, we can still keep those.
[00:51:40.840 --> 00:51:42.120]   But then we split basically
[00:51:42.120 --> 00:51:45.200]   between that final linearly separable embedding,
[00:51:45.200 --> 00:51:48.480]   that embedding that is really useful for a linear model.
[00:51:48.480 --> 00:51:51.040]   And then we split that off from basically everything else,
[00:51:51.040 --> 00:51:53.000]   which is the sort of internal representations
[00:51:53.000 --> 00:51:53.880]   of this model,
[00:51:53.880 --> 00:51:56.680]   which need not necessarily be a useful embedding
[00:51:56.680 --> 00:51:58.640]   for any linear model.
[00:51:59.720 --> 00:52:02.960]   So we should focus on that embedding part there.
[00:52:02.960 --> 00:52:05.520]   And actually this connects to some ways
[00:52:05.520 --> 00:52:06.680]   of thinking about neural networks
[00:52:06.680 --> 00:52:09.920]   that are popular in the sort of traditional stats community.
[00:52:09.920 --> 00:52:13.960]   So this is two tweets from Daniela Witten,
[00:52:13.960 --> 00:52:17.320]   who was at one point the MC
[00:52:17.320 --> 00:52:21.040]   of this Women in Statistics and Data Science Twitter account,
[00:52:21.040 --> 00:52:25.600]   which every week features a different woman in data science.
[00:52:25.600 --> 00:52:27.360]   And so during her week,
[00:52:27.360 --> 00:52:32.240]   she made sure to basically kind of troll the internet
[00:52:32.240 --> 00:52:33.080]   as much as possible.
[00:52:33.080 --> 00:52:34.800]   And one thing she did was point out
[00:52:34.800 --> 00:52:37.520]   how many things are just linear models,
[00:52:37.520 --> 00:52:40.120]   where you're trying to guess something,
[00:52:40.120 --> 00:52:45.120]   and you're trying to make a model that guesses an output,
[00:52:45.120 --> 00:52:48.880]   and she'll say whether it's just a linear model or not.
[00:52:48.880 --> 00:52:51.080]   And her answer for deep learning was kind of interesting.
[00:52:51.080 --> 00:52:54.560]   She said, "As a function of the nonlinear activations,
[00:52:54.560 --> 00:52:56.840]   it's just a linear model."
[00:52:56.840 --> 00:52:58.160]   That is like, once you only think
[00:52:58.160 --> 00:53:00.000]   about that last layer there,
[00:53:00.000 --> 00:53:03.240]   basically everything we think about neural networks
[00:53:03.240 --> 00:53:05.480]   is pulled from linear models,
[00:53:05.480 --> 00:53:08.160]   whether things like SGD and regularization.
[00:53:08.160 --> 00:53:12.880]   Like we draw those ideas from the world of linear modeling.
[00:53:12.880 --> 00:53:16.520]   And what we've changed is just the representation
[00:53:16.520 --> 00:53:17.560]   that goes into the linear model.
[00:53:17.560 --> 00:53:18.920]   We've changed the type of data
[00:53:18.920 --> 00:53:20.560]   that the linear model operates on.
[00:53:20.560 --> 00:53:22.360]   And that's actually a much smaller change
[00:53:22.360 --> 00:53:26.800]   than you would think for how impressive
[00:53:26.800 --> 00:53:29.200]   the behavior and performance of neural networks are.
[00:53:29.200 --> 00:53:31.320]   I think it sort of suggests how useful
[00:53:31.320 --> 00:53:35.200]   a good linear model can be once it's fed good data.
[00:53:35.200 --> 00:53:39.840]   Though the one important difference I wanna point out
[00:53:39.840 --> 00:53:42.320]   is that the intermediate layers of a neural network
[00:53:42.320 --> 00:53:44.720]   are also typecasting functions.
[00:53:44.720 --> 00:53:46.880]   So it's almost as though if we wanted to convert
[00:53:46.880 --> 00:53:49.680]   from a float 32 to say a float 16,
[00:53:49.680 --> 00:53:53.640]   so from single precision floats to half precision floats,
[00:53:53.640 --> 00:53:56.360]   on our way to doing that, we also spun off,
[00:53:56.360 --> 00:53:59.080]   oh, this is what it would look like as a 24-bit float.
[00:53:59.080 --> 00:54:00.880]   And this isn't the way our normal
[00:54:00.880 --> 00:54:03.560]   sort of typecasting functions behave, right?
[00:54:03.560 --> 00:54:08.240]   Normally we would just go straight from float 32 to float 16.
[00:54:08.240 --> 00:54:09.800]   But neural networks are not like that.
[00:54:09.800 --> 00:54:11.560]   Neural networks actually produce effectively
[00:54:11.560 --> 00:54:14.000]   a series of embeddings that get progressively
[00:54:14.000 --> 00:54:17.120]   more useful to a linear model.
[00:54:17.120 --> 00:54:18.920]   So the way that I actually like to think about it
[00:54:18.920 --> 00:54:22.000]   is that a neural network is a composable typecaster.
[00:54:22.000 --> 00:54:25.120]   It's self-composed of composable typecasters.
[00:54:25.120 --> 00:54:27.000]   So each chunk of your neural network
[00:54:27.000 --> 00:54:28.720]   as you slice off each layer
[00:54:28.720 --> 00:54:31.480]   is a different casting to a different type.
[00:54:31.480 --> 00:54:33.520]   And you can compose these together
[00:54:33.520 --> 00:54:36.680]   to get a casting into the destination type,
[00:54:36.680 --> 00:54:39.720]   this linearly separable data at the end.
[00:54:39.720 --> 00:54:44.680]   So I think it's important to take a moment
[00:54:44.680 --> 00:54:48.440]   to explicitly state why I think this perspective is useful.
[00:54:48.440 --> 00:54:51.160]   I think the biggest thing is that it makes it more clear
[00:54:51.160 --> 00:54:53.520]   how to compose neural networks, right?
[00:54:53.520 --> 00:54:55.240]   So one of the advantages of neural networks,
[00:54:55.240 --> 00:54:56.440]   if you hear people talk about like,
[00:54:56.440 --> 00:54:58.560]   okay, why are neural networks so good?
[00:54:58.560 --> 00:55:00.800]   And why did the people who worked on neural networks
[00:55:00.800 --> 00:55:03.320]   back in the late 90s to early 2000s,
[00:55:03.320 --> 00:55:04.960]   when they weren't that impressive
[00:55:04.960 --> 00:55:06.360]   in terms of their practical performance,
[00:55:06.360 --> 00:55:08.920]   why did they think neural networks were what it was?
[00:55:08.920 --> 00:55:11.320]   Why did they think that this was the important technology?
[00:55:11.320 --> 00:55:15.000]   And the answer, like almost all of them give
[00:55:15.000 --> 00:55:16.800]   is that neural networks are composable.
[00:55:16.800 --> 00:55:19.280]   I can take little modules and combine them together.
[00:55:19.280 --> 00:55:21.160]   But if you look at the way we write neural networks,
[00:55:21.160 --> 00:55:23.720]   we often write them such that the output
[00:55:23.720 --> 00:55:25.920]   is this final prediction, right?
[00:55:25.920 --> 00:55:28.560]   The output of the network is just dog
[00:55:28.560 --> 00:55:31.960]   or like a one hot vector that is the dog
[00:55:31.960 --> 00:55:36.480]   or like the softmax of the logits, whatever it is.
[00:55:36.480 --> 00:55:40.480]   And that's actually not something you'd wanna put
[00:55:40.480 --> 00:55:41.800]   into another neural network, right?
[00:55:41.800 --> 00:55:45.560]   You wouldn't actually really want to take that output,
[00:55:45.560 --> 00:55:47.560]   that softmax output directly
[00:55:47.560 --> 00:55:50.320]   and put it into another neural network, right?
[00:55:50.320 --> 00:55:55.320]   That's basically just a distribution over your classes.
[00:55:55.320 --> 00:55:58.280]   And that's not always the really useful thing.
[00:55:58.280 --> 00:56:00.080]   The thing that's actually much more useful
[00:56:00.080 --> 00:56:01.720]   and actually makes a good input
[00:56:01.720 --> 00:56:03.280]   to a downstream neural network
[00:56:03.280 --> 00:56:06.920]   is that layer before the linear transformation,
[00:56:06.920 --> 00:56:08.400]   the softmax, that embedding,
[00:56:08.400 --> 00:56:11.200]   or even just like concatenate together several layers
[00:56:11.200 --> 00:56:14.720]   and turn that into, say like a multi-channel image
[00:56:14.720 --> 00:56:17.320]   that represents the input to the neural network.
[00:56:17.320 --> 00:56:20.760]   That is a really good input to a downstream neural network.
[00:56:20.760 --> 00:56:23.120]   So this allows us to compose,
[00:56:23.120 --> 00:56:26.120]   to combine in a chain multiple neural networks.
[00:56:26.120 --> 00:56:30.560]   It also encourages you to de-emphasize supervised learning.
[00:56:30.560 --> 00:56:32.920]   Right, if the goal of training a neural network
[00:56:32.920 --> 00:56:35.160]   is to get this embedding here at the end,
[00:56:35.160 --> 00:56:37.760]   rather than directly to solve the task,
[00:56:37.760 --> 00:56:40.000]   then the task is actually less important.
[00:56:40.000 --> 00:56:43.240]   The important part of what the task I think really brings us
[00:56:43.240 --> 00:56:44.840]   is that it tries to encourage
[00:56:44.880 --> 00:56:49.040]   a rich, useful embedding representation.
[00:56:49.040 --> 00:56:51.800]   So it encourages you to instead think about
[00:56:51.800 --> 00:56:53.920]   unsupervised learning, or as I have it here,
[00:56:53.920 --> 00:56:56.280]   fun supervised learning,
[00:56:56.280 --> 00:57:00.520]   which is in addition to probably generating better embeddings
[00:57:00.520 --> 00:57:02.320]   is also less expensive, right?
[00:57:02.320 --> 00:57:04.280]   If I wanna label,
[00:57:04.280 --> 00:57:05.880]   if I wanna do supervised learning
[00:57:05.880 --> 00:57:07.880]   on every image posted on the internet,
[00:57:07.880 --> 00:57:09.160]   not only do I need to download
[00:57:09.160 --> 00:57:10.600]   every image posted on the internet,
[00:57:10.600 --> 00:57:12.240]   I need to get humans to label
[00:57:12.240 --> 00:57:14.200]   every single image posted on the internet.
[00:57:14.200 --> 00:57:15.760]   And humans have written a bunch of stuff
[00:57:15.760 --> 00:57:17.920]   around images on the internet,
[00:57:17.920 --> 00:57:18.920]   but they haven't done it in a way
[00:57:18.920 --> 00:57:21.320]   that's quite useful enough to be,
[00:57:21.320 --> 00:57:22.800]   or that's quite the right format
[00:57:22.800 --> 00:57:24.760]   to be useful for supervised learning.
[00:57:24.760 --> 00:57:27.520]   But if I do something like an autoencoder,
[00:57:27.520 --> 00:57:29.720]   so an unsupervised learning task on images,
[00:57:29.720 --> 00:57:31.280]   where the goal is to take an image in
[00:57:31.280 --> 00:57:32.880]   and return it at the end,
[00:57:32.880 --> 00:57:36.680]   then I don't actually need those labels anymore.
[00:57:36.680 --> 00:57:39.000]   And so I remove actually what's the most expensive
[00:57:39.000 --> 00:57:42.720]   and difficult step in my supervised learning pipeline.
[00:57:43.600 --> 00:57:47.720]   So by focusing on generating good embeddings,
[00:57:47.720 --> 00:57:51.000]   you focus, you actually, I think,
[00:57:51.000 --> 00:57:53.360]   recognize that there are better ways
[00:57:53.360 --> 00:57:57.160]   to get the downstream final thing that you need, right?
[00:57:57.160 --> 00:57:59.960]   When it comes time to do the downstream supervised task,
[00:57:59.960 --> 00:58:02.120]   you could take those embeddings that you learned
[00:58:02.120 --> 00:58:05.560]   in an easier, cheaper, more scalable, unsupervised manner,
[00:58:05.560 --> 00:58:09.680]   and then fine tune them to get that final supervised,
[00:58:09.680 --> 00:58:13.000]   that final supervised task solved.
[00:58:13.000 --> 00:58:15.960]   And in that case, you're just training a linear model
[00:58:15.960 --> 00:58:18.520]   and all the tools and ideas from linear models
[00:58:18.520 --> 00:58:20.160]   and statistics come in handy.
[00:58:20.160 --> 00:58:24.160]   This perspective also pushes you
[00:58:24.160 --> 00:58:26.520]   towards this model amalgamation style.
[00:58:26.520 --> 00:58:28.400]   So this is a sort of graph
[00:58:28.400 --> 00:58:31.520]   of a fake machine learning pipeline
[00:58:31.520 --> 00:58:33.600]   that takes in raw data
[00:58:33.600 --> 00:58:36.000]   and then generates three different things,
[00:58:36.000 --> 00:58:39.200]   a prediction of, let's say this is a social network,
[00:58:39.200 --> 00:58:41.320]   who might be friends with a user.
[00:58:41.320 --> 00:58:42.840]   So this is raw data about a user,
[00:58:42.840 --> 00:58:44.800]   maybe what they've been clicking on,
[00:58:44.800 --> 00:58:47.320]   like their email and information about them.
[00:58:47.320 --> 00:58:50.040]   And it produces first a user embedding
[00:58:50.040 --> 00:58:52.200]   and then a prediction of who they are friends with.
[00:58:52.200 --> 00:58:54.720]   And also a recommendation of maybe other people
[00:58:54.720 --> 00:58:56.520]   they could be friends with or other content
[00:58:56.520 --> 00:58:58.880]   on this social media network that they might like to see.
[00:58:58.880 --> 00:59:00.760]   It also predicts what they're gonna click next.
[00:59:00.760 --> 00:59:01.760]   Maybe that might be helpful
[00:59:01.760 --> 00:59:03.600]   for your content delivery network
[00:59:03.600 --> 00:59:06.240]   to come up with smarter things to cache.
[00:59:06.240 --> 00:59:07.840]   But all of them pass through,
[00:59:07.840 --> 00:59:09.880]   rather than going straight from raw user data
[00:59:09.880 --> 00:59:10.720]   to click prediction,
[00:59:10.720 --> 00:59:12.720]   or straight from raw data to friend prediction,
[00:59:12.720 --> 00:59:15.000]   they pass through this central node here
[00:59:15.000 --> 00:59:16.320]   of the user embedding.
[00:59:16.320 --> 00:59:18.240]   And so this is behaving kind of like
[00:59:18.240 --> 00:59:20.920]   that additive Roman type that we had before, right?
[00:59:20.920 --> 00:59:25.120]   It's a useful intermediate type
[00:59:25.120 --> 00:59:28.760]   on which the operations we really wanna do are easier.
[00:59:28.760 --> 00:59:31.640]   And so all of these arrows here are actually DNNs,
[00:59:31.640 --> 00:59:34.160]   but the difference is that sort of the most important DNN
[00:59:34.160 --> 00:59:36.800]   in the entire pipeline is this first one here
[00:59:36.800 --> 00:59:39.640]   that generates from the raw data a user embedding.
[00:59:39.640 --> 00:59:42.800]   And that neural network is the most important one.
[00:59:42.800 --> 00:59:45.720]   And that's the one that's best understood
[00:59:45.720 --> 00:59:48.320]   as just a type casting network.
[00:59:48.320 --> 00:59:51.080]   And in fact, all of these click prediction recommendation
[00:59:51.080 --> 00:59:51.920]   and friend prediction,
[00:59:51.920 --> 00:59:53.840]   these could even just be linear models on the user embedding
[00:59:53.840 --> 00:59:54.800]   if it's good enough.
[00:59:54.800 --> 00:59:59.000]   And actually, this is not just a sort of toy example,
[00:59:59.000 --> 01:00:01.440]   or at least it's not just for toy examples.
[01:00:01.440 --> 01:00:04.560]   So I've talked with folks at Google and folks at Twitter,
[01:00:04.560 --> 01:00:06.000]   and this is actually what they do.
[01:00:06.000 --> 01:00:09.320]   They may have like 150 models in production,
[01:00:09.320 --> 01:00:13.840]   but actually the core of it is an embedding model
[01:00:13.840 --> 01:00:16.200]   that then gets a model that produces these embeddings
[01:00:16.200 --> 01:00:17.840]   that gets fed to everything else.
[01:00:17.840 --> 01:00:20.840]   It also raises some important questions
[01:00:20.840 --> 01:00:23.280]   for the future of ML power technology.
[01:00:23.280 --> 01:00:26.080]   So just like three kind of research almost questions
[01:00:26.080 --> 01:00:30.440]   or broad scale questions that come up out of this.
[01:00:30.440 --> 01:00:34.640]   The first is sort of how can we document our types?
[01:00:34.640 --> 01:00:39.080]   An IEEE of NNLearn types would actually be really great.
[01:00:39.080 --> 01:00:42.000]   So IEEE is the ones who put out that floating point standard
[01:00:42.000 --> 01:00:44.040]   and they define how you're supposed to behave
[01:00:44.040 --> 01:00:47.440]   on a bunch of like, if you implement floating point,
[01:00:47.440 --> 01:00:49.000]   like what does that mean?
[01:00:49.000 --> 01:00:52.040]   Like how does that constrict the behavior of your type?
[01:00:52.040 --> 01:00:55.000]   An IEEE standard for say embeddings of images,
[01:00:55.000 --> 01:00:58.680]   embeddings of natural languages,
[01:00:58.680 --> 01:01:00.400]   like there's lots of data types out there
[01:01:00.400 --> 01:01:02.520]   that get used all the time in machine learning.
[01:01:02.520 --> 01:01:04.280]   And having a centralized standard
[01:01:04.280 --> 01:01:06.480]   would both level the ML playing field
[01:01:06.480 --> 01:01:08.960]   and make it easier to develop ML applications
[01:01:08.960 --> 01:01:11.360]   without having Google scale resources.
[01:01:11.360 --> 01:01:12.960]   And it would actually centralize
[01:01:12.960 --> 01:01:14.880]   some of the really hard work that's out there,
[01:01:14.880 --> 01:01:19.880]   like removing bias, extremely difficult to do on your own.
[01:01:19.880 --> 01:01:24.040]   The techniques are very sort of like experimental
[01:01:24.040 --> 01:01:27.760]   and there's not a clear choice, there's not a cookbook.
[01:01:27.760 --> 01:01:31.120]   But if we had a centralized repository for doing this,
[01:01:31.120 --> 01:01:32.840]   then we could come to a consensus
[01:01:32.840 --> 01:01:35.520]   on what an unbiased embedding of an image is.
[01:01:35.520 --> 01:01:37.040]   And that would be extremely useful.
[01:01:37.040 --> 01:01:39.520]   Also things like compression to get high performance,
[01:01:39.520 --> 01:01:42.360]   this is non-trivial, putting this all in one place,
[01:01:42.360 --> 01:01:43.800]   we could end up with a neural network
[01:01:43.800 --> 01:01:45.280]   that does that first embedding step
[01:01:45.280 --> 01:01:48.560]   and does it in a millisecond instead of a second.
[01:01:48.560 --> 01:01:51.320]   And so I think that adopting an approach
[01:01:51.320 --> 01:01:54.200]   in which we have defined types that are,
[01:01:54.200 --> 01:01:56.400]   and an embedding image available
[01:01:56.400 --> 01:01:58.160]   in the Python standard library
[01:01:58.160 --> 01:02:00.800]   could be really useful for machine learning.
[01:02:00.800 --> 01:02:02.880]   Also on a more research level,
[01:02:02.880 --> 01:02:05.440]   there are a bunch of ideas in algebraic data types.
[01:02:05.440 --> 01:02:08.360]   So like once you have types, you can actually,
[01:02:08.360 --> 01:02:10.760]   there's like a mathematics of working with types.
[01:02:10.760 --> 01:02:13.800]   So there are some types, products types, exponential types.
[01:02:13.800 --> 01:02:15.320]   There's also been some exotic ideas
[01:02:15.320 --> 01:02:17.160]   in I think the last 10 years or so
[01:02:17.160 --> 01:02:20.760]   of calculus on data types to generate things like lists
[01:02:20.760 --> 01:02:23.080]   arise from applying Taylor expansion
[01:02:23.080 --> 01:02:26.800]   to expressions in the algebra of data types.
[01:02:26.800 --> 01:02:27.840]   And I think that there might be
[01:02:27.840 --> 01:02:29.520]   some low hanging fruit out there
[01:02:29.520 --> 01:02:31.880]   for ways that we can combine the results of neural networks
[01:02:31.880 --> 01:02:33.940]   in a smart way or extend them.
[01:02:34.880 --> 01:02:36.480]   And then lastly, now that we,
[01:02:36.480 --> 01:02:38.760]   if we really think that what's going on here
[01:02:38.760 --> 01:02:41.360]   is that the neural networks are learning a smart embedding,
[01:02:41.360 --> 01:02:43.320]   maybe neural networks aren't the right choice.
[01:02:43.320 --> 01:02:46.520]   It hurts me to say this as a big fan of neural networks
[01:02:46.520 --> 01:02:48.920]   and somebody who's kind of all in on them.
[01:02:48.920 --> 01:02:51.720]   But maybe there's actually better ways to learn types.
[01:02:51.720 --> 01:02:53.760]   The fundamental goal here in machine learning
[01:02:53.760 --> 01:02:57.000]   is to learn programs, is to learn computer programs
[01:02:57.000 --> 01:03:00.040]   that take inputs and produce the appropriate outputs.
[01:03:00.040 --> 01:03:02.800]   And if we actually take a sort of more typed approach
[01:03:02.800 --> 01:03:04.800]   to this and we realize that what we're really trying
[01:03:04.800 --> 01:03:06.640]   to do most of the time is learning types
[01:03:06.640 --> 01:03:09.120]   that make our programs trivial to implement,
[01:03:09.120 --> 01:03:12.120]   that really changes what we think are the most important
[01:03:12.120 --> 01:03:14.840]   sort of research directions to pursue.
[01:03:14.840 --> 01:03:18.480]   So there's a nice little blog post there
[01:03:18.480 --> 01:03:21.920]   that indicates what those, that calculus of data types,
[01:03:21.920 --> 01:03:24.280]   where that arises from.
[01:03:24.280 --> 01:03:28.560]   So this is just a sort of perspective.
[01:03:28.560 --> 01:03:30.720]   I've come from like sort of combining together
[01:03:30.720 --> 01:03:32.480]   a couple of different ideas that are out there
[01:03:32.480 --> 01:03:35.200]   in the world of ML that you may have come across
[01:03:35.200 --> 01:03:37.000]   and trying to take this idea that the embeddings
[01:03:37.000 --> 01:03:39.320]   of a neural network are actually more useful
[01:03:39.320 --> 01:03:40.760]   than the outputs and trying to give it a bit
[01:03:40.760 --> 01:03:42.600]   of a more computer science flavor
[01:03:42.600 --> 01:03:45.440]   and to think of these embeddings actually as types.
[01:03:45.440 --> 01:03:47.800]   And I think, this has actually helped me interpret
[01:03:47.800 --> 01:03:49.400]   a lot of the things I've learned
[01:03:49.400 --> 01:03:52.280]   in the sort of recent salons that we've had
[01:03:52.280 --> 01:03:54.040]   and in recent papers that I've read.
[01:03:54.040 --> 01:03:57.120]   And hopefully you'll find this useful as well.
[01:03:57.120 --> 01:04:01.640]   All right, so if there's any questions,
[01:04:01.640 --> 01:04:04.040]   if anybody has any questions they want to ask,
[01:04:04.040 --> 01:04:07.640]   I'll stick around for a little bit to answer them.
[01:04:07.640 --> 01:04:11.120]   If not, we are a little bit over our time.
[01:04:11.120 --> 01:04:13.200]   And so I'll just end it.
[01:04:13.200 --> 01:04:21.360]   - Charlotte, that was a great talk.
[01:04:21.360 --> 01:04:23.080]   I have one comment, I guess.
[01:04:23.080 --> 01:04:25.360]   I'm curious about, there's probably an analog here
[01:04:25.360 --> 01:04:27.880]   between like neural networks that are explicitly designed
[01:04:27.880 --> 01:04:29.280]   to be compositional or modular.
[01:04:29.280 --> 01:04:32.240]   So I'm thinking of kind of modular neural network
[01:04:32.240 --> 01:04:33.200]   architectures that, for example,
[01:04:33.200 --> 01:04:35.040]   Jacob Andreas has worked on where you explicitly
[01:04:35.040 --> 01:04:37.120]   have different neural modules,
[01:04:37.120 --> 01:04:39.240]   which you can piece together in various ways.
[01:04:39.240 --> 01:04:41.680]   And the output of one neural module, for example,
[01:04:41.680 --> 01:04:43.760]   is like some sort of embedding that's useful
[01:04:43.760 --> 01:04:46.600]   for identifying the color of an object in an image,
[01:04:46.600 --> 01:04:48.440]   some sort of embedding that's useful for identifying
[01:04:48.440 --> 01:04:51.160]   the count, the number of objects in an image, right?
[01:04:51.160 --> 01:04:52.560]   And so there probably are some tie-ins
[01:04:52.560 --> 01:04:54.880]   between the kinds of implicit types you've mentioned here
[01:04:54.880 --> 01:04:56.240]   and these neural network architectures
[01:04:56.240 --> 01:04:59.040]   that specialize for producing certain outputs
[01:04:59.040 --> 01:05:00.240]   of those specific types.
[01:05:00.240 --> 01:05:02.520]   - Yeah, yeah, that's actually a good point.
[01:05:02.520 --> 01:05:06.120]   I think one thing that is suggested by this
[01:05:06.120 --> 01:05:08.280]   is that maybe actually you want those intermediate types
[01:05:08.280 --> 01:05:12.040]   to also be, say, useful for linearly predicting features,
[01:05:12.040 --> 01:05:13.800]   like you mentioned, the color of an image
[01:05:13.800 --> 01:05:14.640]   or something like that.
[01:05:14.640 --> 01:05:17.120]   Or like it could also aid with like problems
[01:05:17.120 --> 01:05:18.480]   of explainability and interpretability
[01:05:18.480 --> 01:05:20.600]   if you have sub-modules of the network
[01:05:20.600 --> 01:05:23.680]   that are explicitly dedicated by means of like,
[01:05:23.680 --> 01:05:25.360]   they must produce an embedding that's useful
[01:05:25.360 --> 01:05:28.720]   for a particular task to say shape inference
[01:05:28.720 --> 01:05:33.280]   or texture inference or like being useful downstream
[01:05:33.280 --> 01:05:36.120]   for segmentation or not being useful for segmentation.
[01:05:36.120 --> 01:05:37.640]   You could also explicitly say,
[01:05:37.640 --> 01:05:40.960]   do not be useful for this particular downstream task.
[01:05:40.960 --> 01:05:44.520]   I think, yeah, you could enrich these intermediate embeddings
[01:05:44.520 --> 01:05:47.360]   that the neural networks are producing and make them,
[01:05:47.360 --> 01:05:53.840]   yeah, make them more powerful, make them more useful
[01:05:53.840 --> 01:05:54.960]   and make them more modular.
[01:05:54.960 --> 01:05:58.360]   So I think it's an idea a lot of people are kind of,
[01:05:58.360 --> 01:05:59.800]   you know, circling around
[01:05:59.800 --> 01:06:02.160]   and I'm excited to see where it goes.
[01:06:02.160 --> 01:06:06.960]   Yeah, Jack Wimbish in the YouTube chat says,
[01:06:06.960 --> 01:06:09.000]   "Great analogy with the additive Roman numeral type
[01:06:09.000 --> 01:06:10.360]   really helps explain the intuition."
[01:06:10.360 --> 01:06:15.360]   So no notes there, no questions, but some appreciation.
[01:06:15.360 --> 01:06:18.880]   So thanks, Jack.
[01:06:18.880 --> 01:06:21.080]   I hope you should definitely check out that blog post
[01:06:21.080 --> 01:06:22.840]   that I linked there, Sandy Metz's blog post.
[01:06:22.840 --> 01:06:26.600]   It's really a really interesting sort of thought about like,
[01:06:26.600 --> 01:06:30.240]   okay, what can we learn from this example?
[01:06:30.240 --> 01:06:34.200]   All right, well, yeah, Jesse, thanks so much
[01:06:34.200 --> 01:06:37.640]   for coming on the salon and presenting your research.
[01:06:37.640 --> 01:06:41.120]   As I said, I'll be watching closely to see your future work
[01:06:41.120 --> 01:06:42.680]   and thanks to everybody for coming.
[01:06:42.680 --> 01:06:43.520]   Take care all.
[01:06:43.520 --> 01:06:49.160]   (SILENCE)

