
[00:00:00.000 --> 00:00:21.880]   Hi, everyone. Thanks for taking the time to be here today. Welcome to this workshop where you
[00:00:21.880 --> 00:00:27.520]   will learn about agents, multimodality, and hopefully get to build -- not hopefully, you will
[00:00:27.520 --> 00:00:34.100]   build a multimodal agent of your own from scratch. Whose first workshop of the day? How
[00:00:34.100 --> 00:00:40.760]   many? Show of hands. Okay. First workshop. How many people already been in a workshop? Oh,
[00:00:40.760 --> 00:00:47.460]   wow. Like, really proactive, overachieving crowd. Great to see that. All right. With
[00:00:47.460 --> 00:00:53.880]   that, here's a little bit about me. I'm Apoorva. I'll be your lead instructor for today. I'm also
[00:00:53.880 --> 00:01:02.400]   joined by my awesome team here. We have Richmond. He's waving at you. There's Rafa back there.
[00:01:02.400 --> 00:01:09.040]   He's waving at you as well. There's Tebow. And we have Mikiko. But yeah, here's a bit about
[00:01:09.040 --> 00:01:15.460]   me. I'm Apoorva. I'm currently an AI-focused developer advocate at MongoDB, which means I spend a lot
[00:01:15.460 --> 00:01:22.040]   of my time building workshops like this one for AI developers like you to help them build AI
[00:01:22.040 --> 00:01:29.080]   applications of their own. Prior to this role, I spent about six years working as a data scientist
[00:01:29.080 --> 00:01:34.980]   in the cybersecurity space. And outside of work, I like to read a lot. I try to yoga pretty regularly,
[00:01:34.980 --> 00:01:42.860]   or used to try to until I busted my knee a while ago. And I'm always on a mission to visit as many local
[00:01:42.860 --> 00:01:49.560]   coffee shops as I can in whichever city I'm in. So this is what the next hour and 20 minutes is going
[00:01:49.560 --> 00:01:57.380]   to look like. We'll be going over key concepts of AI agents, discuss what multimodality is, and finally,
[00:01:57.380 --> 00:02:04.100]   we are going to put the two together and build a multimodal AI agent from scratch using good old Python.
[00:02:04.100 --> 00:02:10.820]   And since this is a relatively short workshop session, this is what you can expect. We are going to focus
[00:02:10.820 --> 00:02:17.780]   on getting concepts down, and we'll see what we have time for. Based on my practice sessions, I believe you
[00:02:17.780 --> 00:02:26.980]   get about 55 minutes to an hour to actually take your time to write code or just, like, get a really good
[00:02:26.980 --> 00:02:33.700]   understanding of how multimodal agents work in code. So just wanted to set that expectation there.
[00:02:33.700 --> 00:02:41.780]   And with that, let's get started. So let's first answer two questions. What are AI agents and why do
[00:02:41.780 --> 00:02:48.900]   we need them? But even before I get into that, how many of you already have some experience with AI agents?
[00:02:48.900 --> 00:02:55.780]   Okay, there's some of you. Hopefully, there'll be something new for you at this workshop. And for those of
[00:02:55.780 --> 00:03:02.900]   you who are new to the concept, there's a lot of stuff coming your way. So apologies for that. Or not.
[00:03:02.900 --> 00:03:11.940]   Okay, so here's my definition of an AI agent. I like to define an AI agent as a system that uses
[00:03:11.940 --> 00:03:18.420]   a large language model. I'll be referring to this as an LLM. To reason through a problem, create a plan to
[00:03:18.420 --> 00:03:23.620]   solve the problem, and execute and iterate on the plan with the help of a set of tools.
[00:03:25.380 --> 00:03:31.460]   So in the past two years or so, we have seen three main paradigms for interacting with LLMs. There's
[00:03:31.460 --> 00:03:37.860]   simple prompting, rag, and agents. So let's briefly talk about each of these because this will help us
[00:03:37.860 --> 00:03:45.060]   build an intuition for when you want to use an AI agent versus something else. So with simple prompting,
[00:03:45.060 --> 00:03:51.780]   you're simply asking the LLM questions and expecting the LLM to rely on its pre-trained, or as we call it,
[00:03:51.780 --> 00:03:58.100]   parametric knowledge to answer these questions. So this means the LLM cannot answer questions
[00:03:58.100 --> 00:04:04.100]   if the information required to answer them is not present in its pre-trained knowledge. It cannot
[00:04:04.100 --> 00:04:10.500]   really handle complex queries, and it cannot provide personalized responses or even refine its responses.
[00:04:12.260 --> 00:04:18.820]   With rag, how many of you have built a rag application? Okay, some of you. So you know,
[00:04:18.820 --> 00:04:24.420]   with rag, you take this up a notch, you augment the LLM's knowledge with information from external data
[00:04:24.420 --> 00:04:31.380]   sources. And as you can imagine, this solves some problems where you can now be reasonably certain
[00:04:31.380 --> 00:04:39.220]   that the LLM has the information required to answer user questions and also incorporate some basic light
[00:04:39.220 --> 00:04:44.980]   personalization if it's given access to the right sources of information. But this still doesn't equip
[00:04:44.980 --> 00:04:52.980]   the LLM with the ability to handle complex multi-step tasks or to self-refinance responses. But that's okay,
[00:04:52.980 --> 00:05:02.500]   because not all tasks might require this capability. And finally, 2025 is the year of Agents. So if you
[00:05:02.500 --> 00:05:10.740]   need, if you have complex multi-step tasks or need a deep personalization, any sort of adaptive learning in
[00:05:10.740 --> 00:05:17.380]   your applications, then you'd want to use AI Agents. So with AI Agents, what we've done is given LLM the
[00:05:17.380 --> 00:05:23.700]   agency to determine the sequence of steps required to complete a particular task. And they do this by
[00:05:23.700 --> 00:05:30.260]   taking actions with the help of tools that you provide them and reasoning through the results of
[00:05:30.260 --> 00:05:38.500]   these tool executions and also its past interactions to inform what to do next. And this is what makes
[00:05:38.500 --> 00:05:45.060]   agents extremely flexible and capable of handling a wide variety of complex tasks. But the, yeah?
[00:05:45.060 --> 00:05:47.140]   Sorry, can we share these presentations?
[00:05:47.140 --> 00:05:51.700]   Yes, it will be shared. The recordings will be up on YouTube as well. So, yeah.
[00:05:51.700 --> 00:05:58.420]   The one thing to note here, though, is that Agents come with a higher cost and latency. After all,
[00:05:58.420 --> 00:06:04.500]   you're expecting LLMs to do all the heavy lifting of thinking through the problem, coming up with a plan of
[00:06:04.500 --> 00:06:13.380]   action, executing the actions, rectifying its responses. So my word of caution here is only use Agents if you need
[00:06:13.380 --> 00:06:18.820]   to. Don't complicate whatever it is you're trying to build. But we are building an AI Agent today. So for
[00:06:18.820 --> 00:06:27.220]   today, let's just throw an agent at the problem, a simple problem. Okay. So to summarize, use Agents for
[00:06:27.220 --> 00:06:33.540]   complex tasks that don't have a structured workflow or where the series of steps required to solve the
[00:06:33.540 --> 00:06:40.660]   problem is hard to predict. Or tasks that have a high latency tolerance, as I was just mentioning. Or for
[00:06:40.660 --> 00:06:46.820]   tasks where it's acceptable for your application or system to return non-deterministic outputs,
[00:06:46.820 --> 00:06:54.020]   which means the same result is not guaranteed for the same inputs. And this is true for any application
[00:06:54.020 --> 00:07:01.460]   that uses LLMs. But this effect is especially amplified in agentic workflows. And finally, tasks
[00:07:01.460 --> 00:07:07.700]   that might benefit from any sort of personalization or adaptive behavior over a long period of time,
[00:07:07.700 --> 00:07:16.020]   all of these are fair game for AI agents. Now let's talk about the different components of AI agents,
[00:07:16.020 --> 00:07:24.260]   just to get a better understanding of how these systems work. So an agent typically has four main
[00:07:24.260 --> 00:07:31.860]   components. There's perception, which is how agents gather information about their environment. Planning and
[00:07:31.860 --> 00:07:37.060]   reasoning, which helps the agent reason through a problem and come up with a plan to solve it.
[00:07:37.060 --> 00:07:43.700]   Then there's tools, which are external interfaces that help the agent act upon and solve a problem.
[00:07:43.700 --> 00:07:51.860]   And memory, which helps agents learn from past interactions. And if you are passionate about memory,
[00:07:51.860 --> 00:07:59.220]   we have Richmond, who knows a lot about this topic. So definitely catch him after or during the presentation.
[00:08:00.020 --> 00:08:05.700]   So all of this sounds a bit like a human, doesn't it? But that's the whole goal of agents. The goal with
[00:08:05.700 --> 00:08:12.420]   LLM-based agents is to give these systems the autonomy to carry out complex tasks, much like we humans
[00:08:12.420 --> 00:08:19.620]   do. So it's not a surprise that the components kind of resemble how we think through problems and go about the
[00:08:19.620 --> 00:08:27.620]   world. So let's dive a bit deeper into each of these components. Let's talk about perception first. So
[00:08:27.620 --> 00:08:32.660]   perception, as I said, is the mechanism by which agents gather information about their environment.
[00:08:32.660 --> 00:08:38.420]   And this happens via some form of inputs, whether it's a user like you interacting with the agent or
[00:08:38.420 --> 00:08:45.300]   triggered by something else, like an email or a Slack message. And text inputs have been the most common
[00:08:45.300 --> 00:08:52.980]   form of interacting with LLMs and agents so far. But over the past few months, we've seen images, voice,
[00:08:52.980 --> 00:08:58.820]   video also being part of this perception mechanism for agents. And in today's workshop, we'll be working
[00:08:58.820 --> 00:09:05.940]   with two of these, which is text and images. The next component we have is planning and reasoning.
[00:09:07.460 --> 00:09:15.220]   And shocker, the component that helps agents plan and reason is LLMs. So given a user query, it's the
[00:09:15.220 --> 00:09:21.300]   LLM's job to determine how to go about solving the problem. But they can't do all of this on their own.
[00:09:21.300 --> 00:09:27.460]   They need some guidance. And the way to provide guidance at this point is to prompt the LLM.
[00:09:28.980 --> 00:09:34.420]   And you can start simple by prompting the LLM to create a plan of action based on its initial
[00:09:34.420 --> 00:09:39.860]   understanding of the problem. And this is what we call planning without feedback, since the LLM doesn't
[00:09:39.860 --> 00:09:46.660]   really modify its initial plan of action based on information gathered from tool outcomes or its own
[00:09:46.660 --> 00:09:51.780]   reasoning traces. And a common design pattern for this kind of planning is chain of thought.
[00:09:53.860 --> 00:09:59.220]   And chain of thought is as simple as prompting the LLM to think through a problem step by step without
[00:09:59.220 --> 00:10:06.980]   directly jumping to giving the user an answer. And you can do this in two ways. In a zero-shot manner,
[00:10:06.980 --> 00:10:12.340]   where you literally prompt the LLM, like tell it, like, let's think step by step. Or you can do this
[00:10:12.340 --> 00:10:17.380]   in a few-shot manner, where you're providing examples of how the LLM should go about thinking through a
[00:10:17.380 --> 00:10:23.700]   problem so that the next time you give it another problem, it'll use your examples to guide its reasoning.
[00:10:23.700 --> 00:10:32.180]   process. Then there's planning with feedback, where you can prompt the LLM to adjust and refine its
[00:10:32.180 --> 00:10:38.340]   initial plan based on new information. Again, obtained from tool outcomes or based on its own
[00:10:38.340 --> 00:10:46.500]   previous reasoning. And a common design pattern that you will implement today is React, which is short for
[00:10:46.500 --> 00:10:51.460]   reasoning and act. And what we do in this pattern is you prompt the LLM to generate
[00:10:51.460 --> 00:11:00.020]   verbal reasoning traces and also tell you the actions that it will take to solve the task. And then after
[00:11:00.020 --> 00:11:06.100]   each action, we ask the LLM to make an observation based on the information that it gathered from that
[00:11:06.100 --> 00:11:12.820]   tool execution and think through that and plan what to do next. And this continues until the LLM determines that
[00:11:12.820 --> 00:11:18.820]   I have the final answer and that's when it will exit that execution loop and provide the answer to the user.
[00:11:18.820 --> 00:11:28.980]   Then the next thing we have is tools. And tools are essentially interfaces for agents to interact with
[00:11:28.980 --> 00:11:35.460]   their external world in order to achieve their objectives. And these tools can range from simple
[00:11:35.460 --> 00:11:42.340]   APIs, such as I'm sure you've seen examples of weather and search APIs, to vector stores, to even
[00:11:42.340 --> 00:11:50.100]   specialized machine learning models. And tools for LLMs are typically defined as functions. And most recent
[00:11:50.100 --> 00:11:56.500]   LLMs have been trained to identify when a function should be called and also the arguments for a function
[00:11:56.500 --> 00:12:04.180]   call. But the one thing to note is the LLM doesn't actually execute the function. This is something we will have to
[00:12:04.180 --> 00:12:11.060]   implement in our code. And in addition to actually defining the function, you typically also need to
[00:12:11.060 --> 00:12:20.900]   provide the LLM a function or tool schema. And this basically is just a JSON file or with MCP servers.
[00:12:20.900 --> 00:12:25.620]   You might have seen a different way of defining tools, but essentially you're providing
[00:12:27.220 --> 00:12:33.140]   the name of the tool to call a description of what the tool does and the parameters that the tool takes and
[00:12:33.140 --> 00:12:39.220]   also their types and descriptions. So for example, I have a weather tool here and in my tool schema, I'm
[00:12:39.220 --> 00:12:45.540]   saying the name of the tool, the description and saying the input that it takes is a city, the type is string and
[00:12:46.100 --> 00:12:53.620]   the description of that parameter. And finally, the last component is memory.
[00:12:53.620 --> 00:13:01.620]   This component is what allows AI agents to store and recall past conversations and enables them to learn
[00:13:01.620 --> 00:13:06.820]   from these interactions. And memory, if you think of human memory, it's a pretty nebulous concept. There's so
[00:13:06.820 --> 00:13:11.300]   many different types of memory. If you ask a psychologist, they'll tell you all about that.
[00:13:11.300 --> 00:13:20.660]   But I'm not a psychologist, so I think of it in pretty primitive terms. I think of it in as two broad
[00:13:20.660 --> 00:13:27.060]   categories. One is short term, which deals with storing and retrieving information from a single
[00:13:27.060 --> 00:13:32.260]   conversation. And then there's long term, which deals with storing, updating, and retrieving information
[00:13:32.260 --> 00:13:39.140]   obtained over multiple conversations had with the agent over a longer period of time. And this is really
[00:13:39.140 --> 00:13:45.620]   what enables agents to personalize their responses over a long period of time. But in today's lab,
[00:13:45.620 --> 00:13:52.900]   we'll implement short term memory for our multimodal agent. And again, if you want to learn more about this
[00:13:54.500 --> 00:14:01.700]   nebulous and extensive topic, then here's a talk that I gave a few months ago. So I'll leave this here
[00:14:01.700 --> 00:14:08.660]   for a few seconds. But if you want to talk to someone live, then talk to me, Richmond, Mikiko, anyone from our team.
[00:14:08.660 --> 00:14:18.580]   Moving on. Okay, so let's take an example and understand how all of these components work together,
[00:14:18.580 --> 00:14:25.460]   right? So the first thing that happens is a query comes in to the agent here. I'm asking this agent,
[00:14:25.460 --> 00:14:33.380]   what's the weather in San Francisco today? The agent forwards the query to an LLM. So think of agents as
[00:14:33.380 --> 00:14:39.380]   a software application or system with different components, one of them being one or more LLMs.
[00:14:39.380 --> 00:14:46.340]   And the LLM in this case has access to a set of tools. In this case, it has access to a weather and
[00:14:46.340 --> 00:14:52.500]   search API and also its past interactions or memories. So based on the tools it has access to,
[00:14:52.500 --> 00:15:00.020]   in this case for this query, the LLM might decide that the weather API would be the most suitable to get
[00:15:00.020 --> 00:15:05.220]   information about this query. And it will also parse out the arguments for this tool from the user query.
[00:15:07.060 --> 00:15:13.380]   And like I mentioned, your agent also needs to have code to actually execute the tools. So we have that in
[00:15:13.380 --> 00:15:19.380]   our agent. It's going to make a call to the weather API with the arguments extracted by the LLM,
[00:15:19.380 --> 00:15:27.860]   get a response back from the API, forward that to the LLM. And at this point, the LLM has two options. It can
[00:15:27.860 --> 00:15:33.540]   either decide that it needs more information and decide to call more tools, or it can be like,
[00:15:33.540 --> 00:15:38.900]   I have the final answer. I'm going to generate that now. So in this case, it has the temperature in San
[00:15:38.900 --> 00:15:44.980]   Francisco. So it might be like, I know the answer. It generates a natural language response. And that
[00:15:44.980 --> 00:15:53.620]   gets forwarded to the user. So that's kind of the full flow of how a tool calling, simple tool calling agent
[00:15:53.620 --> 00:16:02.580]   works. The other thing we need to talk about today, which is the more interesting part, I believe, is
[00:16:02.580 --> 00:16:09.540]   multimodality, because we are, after all, building a multimodal agent. So what is multimodality?
[00:16:09.540 --> 00:16:16.980]   Multimodality in the context of machine learning or AI is the ability of machine learning models to process,
[00:16:16.980 --> 00:16:24.500]   understand, and at this point, even generate different types of data, such as text, images, audio,
[00:16:24.500 --> 00:16:29.700]   video, etc. And like I mentioned, in today's lab, we'll be working with two of these modalities,
[00:16:29.700 --> 00:16:36.980]   which is text and images. So here's some real world examples of data that contains a combination of
[00:16:36.980 --> 00:16:43.300]   images and text, just to give you some inspiration for the kind of problems and domains you can apply
[00:16:43.300 --> 00:16:51.460]   your learnings from today, too. So there's graphs, tables, and then there's these types of data
[00:16:51.460 --> 00:16:57.460]   interleave with text. So think of research papers, financial reports, any sort of organizational reporting,
[00:16:57.460 --> 00:17:05.460]   which typically has like some graphs, analysis, and text all combined together, or healthcare documents.
[00:17:05.460 --> 00:17:11.700]   The list is virtually endless. There's a lot of real world data looks something like this.
[00:17:11.700 --> 00:17:19.220]   So to make sense of this type of data, we currently have two classes of multimodal machine learning models.
[00:17:19.220 --> 00:17:25.140]   And the first type of models we see are multimodal embedding models. And the job of these models is
[00:17:25.140 --> 00:17:31.780]   essentially to take multiple types of data as input and generate embeddings for them so that all of these
[00:17:31.780 --> 00:17:37.460]   diverse data types can be searched and retrieved together using techniques like vector search, hybrid
[00:17:37.460 --> 00:17:44.180]   search, graph-based retrieval, whatever retrieval mechanism you want. And the other class of models
[00:17:44.180 --> 00:17:54.100]   is multimodal LLMs, which can be DeepSeq does that at this point, Claude, OpenAI, like ChatGPT has a voice
[00:17:54.100 --> 00:17:59.620]   mode. So the job of these LLMs is to take all of these different data types as input and also generate
[00:18:00.420 --> 00:18:07.780]   outputs in these different data formats. Now, if you give a multimodal LLM tools to search through
[00:18:07.780 --> 00:18:13.700]   multimodal data and use its reasoning capabilities to make sense of this information and to solve
[00:18:13.700 --> 00:18:21.060]   complex problems, what you have at your hands is a multimodal agent. So let's build that.
[00:18:21.060 --> 00:18:25.380]   Enough talk. Let's actually talk about the agent that we are going to build today.
[00:18:27.220 --> 00:18:34.420]   So we are going to start with something simple. We're going to remove as many abstractions as
[00:18:34.420 --> 00:18:40.500]   possible, start with very simple objectives, and build an agent from scratch. You get a really good
[00:18:40.500 --> 00:18:47.700]   understanding of what it really takes to build a multimodal agent in practice. So our agent has two
[00:18:47.700 --> 00:18:53.780]   simple objectives, the first one being answer questions about a large corpus of documents, and then
[00:18:53.780 --> 00:19:00.340]   also given a chart or diagram help the user make sense of it by explaining and analyzing that figure.
[00:19:00.340 --> 00:19:04.180]   I do this all the time. When I'm reading research papers, I'll just take a screenshot,
[00:19:04.180 --> 00:19:09.380]   pass it to Claude, and be like, explain that equation, especially with all those like mathematical symbols
[00:19:09.380 --> 00:19:19.220]   and whatnot. Sounds pretty reasonable? Easy? Not quite. There's a small catch. And the catch is we want to search
[00:19:19.220 --> 00:19:27.460]   over documents with mixed modalities. So in our case, our corpus is going to be documents that have text
[00:19:27.460 --> 00:19:34.420]   interleaved with things like images and tables. And that complicates things because retrieving the right
[00:19:34.420 --> 00:19:38.340]   information from mixed modality documents is not a trivial problem.
[00:19:39.780 --> 00:19:44.820]   The challenge lies in actually preparing the corpus of documents for search and retrieval. So typically,
[00:19:44.820 --> 00:19:49.780]   for text-based documents, if you've built a RAG application, you chunk up those documents, embed
[00:19:49.780 --> 00:19:56.180]   those chunks, and then retrieve relevant chunks to pass as context to an LLM. But you can't really do this
[00:19:56.180 --> 00:20:01.220]   when you have images and tables in your documents. And one way to do this, there's so many tools out in the
[00:20:01.220 --> 00:20:08.100]   market. There's like LLMAPARs, unstructured, that use vision transformers or object recognition models to
[00:20:08.100 --> 00:20:14.580]   first identify and extract the different elements. Like they'll extract text, images, tables separately,
[00:20:14.580 --> 00:20:21.380]   then you chunk the text as usual, but you summarize the images and tables instead, and then basically
[00:20:21.380 --> 00:20:27.380]   convert everything to the text domain by creating embeddings of the text chunks and summaries using a text
[00:20:27.380 --> 00:20:34.500]   embedding model. I know that's already a mouthful, but you'll see how to simplify this process using a new
[00:20:34.500 --> 00:20:40.660]   type of models. I'll just get to that in just a little bit. Another technique similar to the previous
[00:20:40.660 --> 00:20:46.420]   one is that you still extract the text and non-text elements. You chunk the text, but instead of summarizing
[00:20:46.420 --> 00:20:52.420]   the images and tables, you would embed all of these, like the text chunks, images, and tables using a
[00:20:52.420 --> 00:20:59.780]   multimodal embedding model because it has the capacity to understand and embed all of these
[00:20:59.780 --> 00:21:07.540]   different data types. I already see some of you losing me because these data processing pipelines are
[00:21:07.540 --> 00:21:13.540]   pretty complex and they come with their own limitations, right? They sound promising, but they have
[00:21:13.540 --> 00:21:18.740]   mainly these two limitations. So the first one is that they face the same drawbacks
[00:21:19.380 --> 00:21:25.460]   that you see with chunking. So to me, the biggest problem with chunking is the loss of context at the
[00:21:25.460 --> 00:21:30.420]   chunk boundaries, which is why techniques like parent document retrieval, metadata pre-filtering,
[00:21:30.420 --> 00:21:35.620]   are becoming popular where you add back context that was lost during chunking at either retrieval or
[00:21:35.620 --> 00:21:44.180]   generation time. Also notice how complex these processing pipelines were, right? You need an object recognition model
[00:21:44.900 --> 00:21:52.100]   to extract the elements, potentially another LM call to actually summarize these elements in addition to
[00:21:52.100 --> 00:22:00.180]   chunking and embedding, which is already one too many steps. Another limitation with a lot of multimodal
[00:22:00.180 --> 00:22:06.580]   embedding models lies in the architecture of the models themselves. So until recently, the architecture of
[00:22:06.580 --> 00:22:12.020]   most multimodal embedding models, at least for text and images, was based on OpenAI's clip model.
[00:22:12.020 --> 00:22:19.380]   And what happens in this architecture is text and images are passed through separate networks for
[00:22:19.380 --> 00:22:21.540]   generating the embeddings of these data types.
[00:22:21.540 --> 00:22:30.580]   And this results in something we call a modality gap, where irrelevant items of the same modality end up
[00:22:30.580 --> 00:22:38.500]   close to each other rather than relevant items of different modalities. So in a clip model, for example,
[00:22:38.500 --> 00:22:44.980]   text vectors of irrelevant text might appear close together in vector space rather than text and images
[00:22:44.980 --> 00:22:51.940]   corresponding to related subjects. And that's a problem. But this has changed with the advent of
[00:22:51.940 --> 00:23:01.380]   vision language model, or VLM-based architectures. So in this architecture, both modalities are vectorized
[00:23:01.380 --> 00:23:07.860]   using the same encoder. And this ensures that both text and visual features are treated as part of
[00:23:07.860 --> 00:23:11.780]   a more unified representation than as distinct components.
[00:23:11.780 --> 00:23:17.700]   So with these models, all you really need is a screenshot of documents containing
[00:23:17.700 --> 00:23:24.820]   whether it's purely images, purely text, or a combination of text, images, tables, etc.
[00:23:24.820 --> 00:23:30.900]   And this is what, because of that unified architecture, it ensures that the contextual relationships between
[00:23:30.900 --> 00:23:33.540]   text and visual data is preserved.
[00:23:33.540 --> 00:23:39.300]   So as you can imagine, this greatly simplifies the data processing pipeline for multimodal data
[00:23:39.300 --> 00:23:45.620]   and also ensures that you get better retrieval quality because you're no longer separating these
[00:23:45.620 --> 00:23:51.300]   texts and images. So basically, given a document containing a combination of text and images,
[00:23:51.300 --> 00:23:57.140]   you simply take a screenshot of it, pass it through a multimodal embedding model, and the embedding
[00:23:57.140 --> 00:24:03.220]   that you get from that makes this data ready for retrieval. Pretty straightforward process there.
[00:24:05.140 --> 00:24:10.580]   So let's quickly look at how some of the key features of the agent that we are going to build is
[00:24:10.580 --> 00:24:17.540]   work. And then we can go implement these in code. So let's talk about the data preparation pipeline for
[00:24:17.540 --> 00:24:22.020]   the corpus of documents that our agent is going to use to answer questions.
[00:24:22.020 --> 00:24:28.740]   So like I mentioned, the first thing we are going to do is for each document in our corpus,
[00:24:28.740 --> 00:24:34.500]   we are going to convert that into a set of screenshots. And in our case, each screenshot is going to
[00:24:34.500 --> 00:24:36.500]   represent a page in the document.
[00:24:36.500 --> 00:24:41.940]   We'll then store the screenshots locally, but if you were to do this in production,
[00:24:41.940 --> 00:24:45.460]   then you might want to store them to some form of blob storage like S3,
[00:24:45.460 --> 00:24:49.940]   Google Cloud Storage, whatever your preferred cloud provider is.
[00:24:49.940 --> 00:24:55.300]   And we'll also note the path to where the image is stored.
[00:24:55.300 --> 00:25:03.300]   And then store this as metadata along with the embeddings of the screenshots generated using a multimodal embedding model
[00:25:03.860 --> 00:25:06.500]   and store these into a vector database.
[00:25:06.500 --> 00:25:15.700]   So in our lab, we'll use the latest multimodal embedding model from YGI, and we'll use MongoDB as a vector database.
[00:25:15.700 --> 00:25:18.020]   Because I work at MongoDB.
[00:25:18.020 --> 00:25:25.300]   So one important thing to note here that we are not storing the raw screenshots. Yes?
[00:25:25.300 --> 00:25:43.140]   So you are retrieving the documents into screenshots, several screenshots or you're picking up the screenshots from the document and sending it to the --
[00:25:43.140 --> 00:26:01.540]   So, okay. So say I have a PDF containing multiple pages. Each page in the PDF, I'm going to take a screenshot of it. And each screenshot is going to be saved separately in blob storage. And references will be stored as metadata along with embeddings in the vector database.
[00:26:01.540 --> 00:26:08.180]   And why do you have to take the screenshot of each page? Is it to save the space?
[00:26:08.180 --> 00:26:20.820]   So, like, I showed you two methods before, right? Like, to be able to use the image and table data for reasoning, you need to be able to retrieve that document that contains all of that together.
[00:26:20.820 --> 00:26:30.180]   And the reason I'm taking a screenshot is to preserve, like, the continuity between the text elements and the image elements. So all of them can be retrieved together as context.
[00:26:30.180 --> 00:26:32.580]   But this is worse than chunks.
[00:26:32.580 --> 00:26:33.460]   What's that?
[00:26:33.460 --> 00:26:39.620]   I mean, a screenshot for each page, it was the context of the whole, I mean, that.
[00:26:39.620 --> 00:26:46.980]   But then with chunking, it's worse because you have, like, a way -- usually you're keeping, like, two paragraphs together or something.
[00:26:46.980 --> 00:26:54.900]   So, like, slightly better, you would always, of course, maybe want to augment this method with metadata pre-filtering, other methods that you use for traditional chunking.
[00:26:54.900 --> 00:27:00.340]   But I still think one page is better than two paragraphs or a small paragraph of text.
[00:27:00.340 --> 00:27:04.500]   Do you want overlapping segments here as well, just like you would make something?
[00:27:04.500 --> 00:27:05.620]   Yes. Yeah.
[00:27:05.620 --> 00:27:09.140]   So when you -- so here we'll take screenshots of, like, distinct pages.
[00:27:09.140 --> 00:27:13.940]   But if you want that continuity, you might want to, like, keep some overlap as well.
[00:27:13.940 --> 00:27:15.380]   Yeah. Good point.
[00:27:15.380 --> 00:27:17.860]   Better than giving it the full PDF at once.
[00:27:17.860 --> 00:27:21.940]   Because LLMs have, like, that lost-in-the-middle problem.
[00:27:21.940 --> 00:27:26.100]   So if you give it -- like, just because you have a large context window doesn't mean you should flood it.
[00:27:26.100 --> 00:27:33.300]   Because they still have the problem of searching through, like, the full -- a large document to find the right information.
[00:27:33.300 --> 00:27:34.100]   So you're trying to --
[00:27:34.100 --> 00:27:36.900]   So the relation between these different states should be maintained?
[00:27:39.220 --> 00:27:45.780]   Yeah. I think, as she was pointing out, either you'd have to, like, maybe structure it a little differently to have some overlap
[00:27:45.780 --> 00:27:52.260]   or store some additional metadata to maintain that continuity, like page numbers or -- and whenever
[00:27:52.260 --> 00:27:57.220]   you're retrieving a page, you could do something like retrieve the previous two, next two pages, things like that.
[00:27:58.340 --> 00:28:00.260]   Yeah. Any more questions?
[00:28:00.260 --> 00:28:00.820]   Yeah.
[00:28:00.820 --> 00:28:06.420]   You talked about Voyage multimodal 3, like, why that one versus others?
[00:28:06.420 --> 00:28:08.900]   Any VLM-based model, really.
[00:28:08.900 --> 00:28:13.620]   Like, the whole point is to show you that clip-based models have that -- I'm not using a clip-based model
[00:28:13.620 --> 00:28:19.460]   because it has a modality gap, but any VLM-based model is very good. Yeah?
[00:28:19.460 --> 00:28:23.060]   So this works for text and image.
[00:28:23.060 --> 00:28:23.540]   Yeah.
[00:28:23.540 --> 00:28:26.420]   And they're more, like, video.
[00:28:26.420 --> 00:28:27.300]   Right. Yeah.
[00:28:27.300 --> 00:28:29.540]   Like, yep, 100%.
[00:28:29.540 --> 00:28:31.700]   So this doesn't really deal with that.
[00:28:31.700 --> 00:28:37.460]   But essentially, you could extend this concept to different modalities.
[00:28:37.460 --> 00:28:42.340]   It's just I typically don't see, like, video or audio occurring with text.
[00:28:42.820 --> 00:28:48.980]   Like, I just chose this because, like, images, figures typically occur with text.
[00:28:48.980 --> 00:28:53.620]   But so, yeah, screenshots might not apply to other modalities.
[00:28:53.620 --> 00:28:54.980]   There are different ways to handle it.
[00:28:54.980 --> 00:28:57.140]   Today, we are only focusing on images and text.
[00:28:57.140 --> 00:29:00.020]   Okay. Two more questions.
[00:29:00.020 --> 00:29:00.740]   Okay. Yeah.
[00:29:00.740 --> 00:29:06.020]   So, VLM, I guess this is different from the last language model, right?
[00:29:06.020 --> 00:29:09.540]   So it has, I guess, smaller parameters than last language model.
[00:29:09.540 --> 00:29:12.020]   But then for home, it's a bit less, I guess.
[00:29:12.020 --> 00:29:16.180]   It's just, it's still a large --
[00:29:16.180 --> 00:29:20.180]   For example, like we make an embedding from the VLM.
[00:29:20.180 --> 00:29:20.740]   Mm-hmm.
[00:29:20.740 --> 00:29:25.060]   And then if you make an embedding from the, let's say,
[00:29:25.060 --> 00:29:30.500]   other 002, which is bigger parameters, then performance probably --
[00:29:30.500 --> 00:29:32.180]   actually is probably less.
[00:29:32.180 --> 00:29:34.180]   VLMs tend to get pretty big, too.
[00:29:34.180 --> 00:29:36.340]   So they're basically still large models.
[00:29:36.340 --> 00:29:38.980]   They just can handle, like, images and text.
[00:29:38.980 --> 00:29:42.500]   But they're still pretty sizable models, yeah.
[00:29:42.500 --> 00:29:48.500]   And I can point you to some benchmarks that show that they're still good at even purely text or purely
[00:29:48.500 --> 00:29:50.500]   image data and then a combination of both.
[00:29:50.500 --> 00:29:51.460]   How to run in the local?
[00:29:51.460 --> 00:29:52.500]   What's that?
[00:29:52.500 --> 00:29:54.500]   How to run in the local machine, I guess.
[00:29:54.500 --> 00:30:00.660]   Like, you'd find a model that works with your hardware specifications, just like an LLM, right?
[00:30:00.660 --> 00:30:03.540]   Like, not all LLMs can be run on your machine.
[00:30:03.540 --> 00:30:06.660]   So it would be similar to those, yeah.
[00:30:06.660 --> 00:30:07.700]   You know, there was one.
[00:30:07.700 --> 00:30:08.820]   No, there was one.
[00:30:08.820 --> 00:30:09.700]   Let's skip one.
[00:30:09.700 --> 00:30:11.140]   Okay, one more.
[00:30:11.140 --> 00:30:14.420]   Yeah, so in this case, you're using multimodal, right?
[00:30:14.420 --> 00:30:15.140]   Mm-hmm.
[00:30:15.140 --> 00:30:19.140]   It's true that your image and text is strongly aligned to the modalities.
[00:30:19.140 --> 00:30:22.900]   But what if you have a modality that's really weakly aligned to the time series, right?
[00:30:22.900 --> 00:30:25.220]   So which means your data space, they're not pretty close.
[00:30:25.220 --> 00:30:26.180]   How do you handle those?
[00:30:26.180 --> 00:30:27.700]   Sorry, can you say that again?
[00:30:27.700 --> 00:30:32.340]   If you have a modality that's not strongly aligned with the rest of the modalities,
[00:30:32.340 --> 00:30:34.020]   like, for example, time series, right?
[00:30:34.020 --> 00:30:39.860]   If you embed it into the same data space, they are not, like, really close to each other.
[00:30:39.860 --> 00:30:40.420]   Yeah.
[00:30:40.420 --> 00:30:42.900]   So in both situations, how do you handle that?
[00:30:42.900 --> 00:30:45.620]   So, like, time series data with text?
[00:30:45.620 --> 00:30:50.100]   Like, I'm trying to understand, like, a situation where you would have totally disparate.
[00:30:50.100 --> 00:30:53.140]   So you have, like, text, you may have time series, too, right?
[00:30:53.140 --> 00:30:53.540]   Mm-hmm.
[00:30:53.540 --> 00:30:55.860]   And that time series data may not be really aligned with.
[00:30:55.860 --> 00:30:56.580]   Yeah.
[00:30:56.580 --> 00:31:00.500]   And that means when you clip them, they don't really go very well.
[00:31:00.500 --> 00:31:01.940]   So how do you handle those?
[00:31:01.940 --> 00:31:05.540]   Yeah, I think time series data, typically, you don't even, like, use embeddings for it.
[00:31:05.540 --> 00:31:11.220]   It's just, like, you treat them like any other features like you would for traditional ML models.
[00:31:11.220 --> 00:31:15.060]   You definitely want, like, a different retrieval strategy for those.
[00:31:15.060 --> 00:31:19.860]   It would be hard to put them in the same, yeah, vector space as text and images.
[00:31:19.860 --> 00:31:23.380]   So you might need to work with, like, different retrieval methodologies.
[00:31:23.380 --> 00:31:24.500]   Yeah.
[00:31:24.500 --> 00:31:25.780]   Yeah.
[00:31:25.780 --> 00:31:26.820]   All right.
[00:31:26.820 --> 00:31:32.900]   I'm going to move forward here very-- like, in a few minutes, we will hit the hands-on portion.
[00:31:32.900 --> 00:31:38.180]   So if we have more questions, just call out to our team, and we'll take more questions then.
[00:31:38.180 --> 00:31:39.700]   Cool.
[00:31:39.700 --> 00:31:40.820]   All right.
[00:31:40.820 --> 00:31:41.700]   OK.
[00:31:41.700 --> 00:31:42.100]   OK.
[00:31:42.100 --> 00:31:44.820]   Let's quickly talk about the workflow of our agent.
[00:31:44.820 --> 00:31:50.020]   We looked at a random example before, but let's talk about the agent you're going to build.
[00:31:50.020 --> 00:31:56.100]   So query comes in, agent forwards the query to a multimodal LLM.
[00:31:56.100 --> 00:31:59.780]   So note, we are going to use a multimodal embedding model for retrieval,
[00:31:59.780 --> 00:32:01.780]   but we also need a multimodal LLM.
[00:32:01.780 --> 00:32:06.820]   We are going to use, I think, Gemini 2.0 Flash Experimental, some long name.
[00:32:06.820 --> 00:32:13.780]   But yeah, basically, we need that LLM because once we give it, like, that interleaved document
[00:32:13.780 --> 00:32:18.820]   with text and images, we need an LLM that can make sense of both these modalities.
[00:32:18.820 --> 00:32:21.300]   So that's why I'm using that LLM.
[00:32:21.300 --> 00:32:28.020]   It has just one tool, which is a vector search tool to retrieve those multimodal documents
[00:32:28.020 --> 00:32:30.100]   and also its past interactions and memory.
[00:32:30.100 --> 00:32:35.620]   So based on the query, the LLM can decide to call the vector search tool.
[00:32:35.620 --> 00:32:41.220]   And if it does that, it'll return the name of the tool and the arguments to use to call the tool.
[00:32:41.220 --> 00:32:47.060]   Again, the agent has code to actually call the tool, so it calls the vector search tool.
[00:32:48.020 --> 00:32:53.940]   And typically, if you're working with text-based data, you get the documents back directly from
[00:32:53.940 --> 00:32:58.820]   vector search. But in this case, what we are going to get back is references to the screenshots.
[00:32:58.820 --> 00:33:04.500]   Remember, we didn't store those in the vector database. Those are in our local or blob storage.
[00:33:04.500 --> 00:33:10.100]   So then our agent needs to have that additional step of using those image references to actually get the
[00:33:10.100 --> 00:33:16.580]   screenshots from blob storage. And then it's going to pass those images along with the original user query
[00:33:16.580 --> 00:33:24.420]   and any past conversational history to the multimodal LLM. So each time an LLM call is made, whether it's to
[00:33:24.420 --> 00:33:31.220]   determine what tools to call or generate the final answer, the images are also going to be passed along with
[00:33:31.220 --> 00:33:39.380]   the query and conversation history to the LLM. Then it generates an answer and that gets returned back to the user.
[00:33:41.620 --> 00:33:46.820]   And finally, depending on the query, the LLM might also decide that it doesn't need to call a tool.
[00:33:46.820 --> 00:33:51.220]   So, for example, if the user is simply asking, like, "Hey, summarize this image," it might not need to
[00:33:51.220 --> 00:33:58.260]   call a tool. So in that case, it'll say, "I don't need to call tools." It'll simply generate an answer,
[00:33:58.260 --> 00:34:06.100]   and that gets forwarded to the user. And the final thing, let's talk about the memory management
[00:34:06.100 --> 00:34:13.140]   mechanism for our agent because this is important for it to actually have coherent multi-term conversations
[00:34:13.140 --> 00:34:19.140]   with the user. So, like I mentioned before, we'll be implementing short-term memory for the agent. And
[00:34:19.140 --> 00:34:26.660]   the way this works is each user query is associated with a session ID just as some identifier to distinguish
[00:34:26.660 --> 00:34:33.620]   between different conversations. So, given a user query, we obtain its session or conversation ID,
[00:34:33.620 --> 00:34:39.940]   and we query a database consisting of previous turns in the conversation to get that chat history for
[00:34:39.940 --> 00:34:47.060]   that session. And each time, again, in addition to the context, we also pass in that chat history,
[00:34:47.060 --> 00:34:53.380]   just so the LLM can use that as additional context to determine if it even needs to call tools or not.
[00:34:55.060 --> 00:34:59.300]   And then, when the LLM generates a response, the other thing that happens is we add
[00:34:59.300 --> 00:35:06.260]   this current response and the current query back to the database to add on to
[00:35:06.260 --> 00:35:12.420]   the history for that session. Now, you can also log a tool call, their outcomes, any
[00:35:12.420 --> 00:35:20.020]   reasoning traces from the LLM, but at a minimum, you at least want to be logging the LLM's response and
[00:35:20.020 --> 00:35:26.980]   the user queries themselves. And finally, that is enough talking from me. You will be quiet for
[00:35:26.980 --> 00:35:36.660]   the rest of the workshop. We have about 45-ish minutes. So, head over to that link to access the
[00:35:36.660 --> 00:35:42.980]   hands-on lab. Recommend running that on your laptop. So, instead of that QR code, actually type in that URL.
[00:35:42.980 --> 00:35:49.140]   This should take you to a GitHub repo. Follow the instructions in the readme to get set up for the
[00:35:49.140 --> 00:35:55.220]   lab. That should take about 10 minutes. And then, you have two options. You can either -- there are two
[00:35:55.220 --> 00:36:02.180]   notebooks in there. One is called lab.ipynb. And if you actually want to -- if you're in the mood to
[00:36:02.180 --> 00:36:06.820]   actually write code right now, that's the notebook you'll be using. You'll have -- you'll see reference
[00:36:06.820 --> 00:36:12.820]   documentation in line in the notebook indicated by that books emoji that tells you use this documentation,
[00:36:12.820 --> 00:36:19.060]   fill in your code. You can do that if that sounds too daunting. There's also a notebook called
[00:36:19.060 --> 00:36:25.620]   solutions.ipynb that has all the code pre-filled. So, you can just run through that notebook,
[00:36:25.620 --> 00:36:31.300]   read the comments to get an understanding of how the agent works. But which option you use,
[00:36:31.300 --> 00:36:37.540]   I'm here. My team is here. Just call on us to -- if you have any questions. And yeah,
[00:36:37.540 --> 00:36:42.660]   for anyone actually filling in the code, you can also refer to the solutions. Don't get too frustrated
[00:36:42.660 --> 00:36:57.620]   if you get stuck. We don't want that. All right. So, I'm shutting up now. Let's go ahead and build that agent.

