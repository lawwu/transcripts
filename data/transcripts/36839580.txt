
[00:00:00.000 --> 00:00:09.540]   Welcome, friends, to this episode of the Everyday Educator podcast.
[00:00:09.540 --> 00:00:15.760]   I'm your host, Lisa Bailey, and I'm excited to spend some time today with you as we encourage
[00:00:15.760 --> 00:00:22.580]   one another, learn together, and ponder the delights and challenges that make homeschooling
[00:00:22.580 --> 00:00:24.140]   the adventure of a lifetime.
[00:00:24.140 --> 00:00:31.800]   Whether you are just considering this homeschooling possibility or deep into the daily delight
[00:00:31.800 --> 00:00:36.720]   of family learning, I believe you'll enjoy thinking along with us.
[00:00:36.720 --> 00:00:44.520]   But don't forget, although this online community is awesome, you'll find even closer support
[00:00:44.520 --> 00:00:46.900]   in a local CC community.
[00:00:46.900 --> 00:00:54.500]   So, go to classicalconversations.com and find a community near you today.
[00:00:54.500 --> 00:00:56.560]   Welcome, ladies.
[00:00:56.560 --> 00:00:57.840]   Hi.
[00:00:57.840 --> 00:00:58.720]   Thanks so much, Lisa.
[00:00:58.720 --> 00:00:59.400]   Hi, Delise.
[00:00:59.400 --> 00:01:01.600]   Great to have you guys, or great to be here with you all today.
[00:01:01.600 --> 00:01:04.660]   Yes, we're so happy to have you.
[00:01:04.660 --> 00:01:11.940]   You guys, Kathy Gibbons is in the house today, and she is probably not new to you, but just
[00:01:11.940 --> 00:01:14.680]   in case, Kathy, will you introduce yourself to our audience?
[00:01:14.680 --> 00:01:16.420]   Yes, so I'm Kathy Gibbons.
[00:01:16.420 --> 00:01:19.100]   I am a long-time homeschool mom.
[00:01:19.100 --> 00:01:26.120]   In fact, we just graduated our daughter out of Challenge 4, which, oh my goodness, I feel
[00:01:26.120 --> 00:01:26.500]   like it was...
[00:01:26.500 --> 00:01:26.740]   Congratulations.
[00:01:26.740 --> 00:01:27.860]   Thank you.
[00:01:27.860 --> 00:01:31.220]   I feel like it was just yesterday we were entering the program.
[00:01:31.220 --> 00:01:33.300]   She was four years old and...
[00:01:33.300 --> 00:01:34.100]   Oh, my word.
[00:01:34.100 --> 00:01:40.640]   ...whispering the tiniest little presentations, you know, up in front of her class, hiding behind
[00:01:40.640 --> 00:01:41.320]   our legs.
[00:01:41.320 --> 00:01:48.180]   And then just like two weeks ago, she confidently defended her senior thesis in front of this
[00:01:48.180 --> 00:01:49.080]   panel of judges.
[00:01:49.080 --> 00:01:53.460]   And it was just like, oh, my goodness, just the journey of a lifetime.
[00:01:53.460 --> 00:01:55.140]   So anyways, that's me.
[00:01:55.140 --> 00:01:55.660]   I'm Kathy.
[00:01:55.660 --> 00:01:57.580]   That is so awesome.
[00:01:57.580 --> 00:01:59.420]   That is really awesome.
[00:01:59.420 --> 00:02:06.740]   And you did not say, tell us your other job, because you are sitting in a different seat
[00:02:06.740 --> 00:02:08.500]   than you're used to sitting in.
[00:02:08.500 --> 00:02:10.300]   You're in the guest seat today.
[00:02:10.300 --> 00:02:10.740]   Yes.
[00:02:10.740 --> 00:02:12.480]   But usually you're the host seat.
[00:02:12.480 --> 00:02:13.060]   Tell us.
[00:02:13.060 --> 00:02:13.820]   Tell us about it.
[00:02:13.820 --> 00:02:14.380]   So I am.
[00:02:14.380 --> 00:02:14.720]   Yes.
[00:02:14.720 --> 00:02:15.700]   And I'll tell you how I got there.
[00:02:15.700 --> 00:02:18.860]   I am the host of the Filter It Through a Brain Cell podcast.
[00:02:19.540 --> 00:02:21.680]   I've been podcasting for the last couple of years.
[00:02:21.680 --> 00:02:27.520]   On this podcast, I teach critical thinking to parents, middle schoolers, high schoolers.
[00:02:27.520 --> 00:02:32.880]   It's really kind of very family friendly, but specifically those age groups in short, fun
[00:02:32.880 --> 00:02:33.220]   episodes.
[00:02:33.220 --> 00:02:38.320]   The whole podcast is designed so that parents who are running around, going to soccer practice,
[00:02:38.320 --> 00:02:42.220]   going to the grocery store, family vacation, whatever, they're just living their life.
[00:02:42.220 --> 00:02:48.020]   They can hit play in the car with their kids, learn about critical thinking, learn a really
[00:02:48.020 --> 00:02:51.720]   good skill, and then have a conversation about it as a family.
[00:02:51.720 --> 00:02:54.960]   And it's something that everybody can listen to, everybody can learn from.
[00:02:54.960 --> 00:02:59.080]   And how I got into this was actually through CC.
[00:02:59.080 --> 00:03:03.240]   I was the director when my daughter was in Challenge A.
[00:03:03.240 --> 00:03:09.800]   And one of the things that they learn in second semester Challenge A is they read a book called
[00:03:09.800 --> 00:03:10.960]   The Fallacy Detective.
[00:03:10.960 --> 00:03:13.260]   Well, these kids loved it.
[00:03:13.260 --> 00:03:15.640]   These seventh graders, they love The Fallacy Detective.
[00:03:15.640 --> 00:03:16.660]   We had so much fun.
[00:03:16.660 --> 00:03:20.480]   You know, it's such the perfect age for them to be learning critical thinking.
[00:03:20.480 --> 00:03:27.720]   They love to find mistakes in the world and what grown-up thinkers are doing wrong.
[00:03:27.720 --> 00:03:28.580]   They're perfect.
[00:03:28.580 --> 00:03:29.620]   Perfect age.
[00:03:29.620 --> 00:03:29.760]   Love it.
[00:03:29.760 --> 00:03:32.520]   I mean, all they want to do is argue anyway.
[00:03:32.520 --> 00:03:34.020]   So it's a perfect time.
[00:03:34.020 --> 00:03:37.760]   I'm like, OK, if they're going to argue, let's teach them how to do it well and how to do it
[00:03:37.760 --> 00:03:37.940]   right.
[00:03:38.240 --> 00:03:43.660]   So the year that my daughter went through Challenge A was 2019-2020 school year.
[00:03:43.660 --> 00:03:47.920]   OK, so that was the year that COVID hit, shut down the world.
[00:03:47.920 --> 00:03:49.160]   So we're into COVID.
[00:03:49.160 --> 00:03:52.140]   We're into the internet, online fact checkers.
[00:03:52.140 --> 00:03:57.340]   We go into this crazy presidential election that happened.
[00:03:57.340 --> 00:04:03.220]   And the thing that was so interesting to me was watching these kids who had had only just
[00:04:03.220 --> 00:04:08.360]   just an introduction, just a smattering of introduction to critical thinking and logical
[00:04:08.360 --> 00:04:08.820]   fallacies.
[00:04:08.820 --> 00:04:13.620]   But watching them go through that time period as opposed to some of their peers who didn't
[00:04:13.620 --> 00:04:14.180]   have it.
[00:04:14.180 --> 00:04:14.680]   Yes.
[00:04:14.680 --> 00:04:15.820]   It was incredible.
[00:04:15.820 --> 00:04:17.480]   I was like, oh, my goodness.
[00:04:17.480 --> 00:04:20.760]   The thing that I realized is, number one, you can't fool these kids.
[00:04:20.760 --> 00:04:21.280]   Right.
[00:04:21.280 --> 00:04:25.240]   Watching a political debate with a bunch of kids who are like, oh, that's an ad hominem.
[00:04:25.240 --> 00:04:26.860]   That was a red herring.
[00:04:26.860 --> 00:04:27.160]   Right.
[00:04:27.160 --> 00:04:28.580]   Like, they called it out.
[00:04:28.580 --> 00:04:29.120]   Yeah.
[00:04:29.120 --> 00:04:33.720]   And I thought, you cannot fool these kids because they got it.
[00:04:33.720 --> 00:04:36.160]   They know how to think and they know how to recognize it.
[00:04:36.420 --> 00:04:43.840]   The other thing that it did for them is it drastically lowered the emotional rollercoaster
[00:04:43.840 --> 00:04:49.540]   that most people were riding because they were able to recognize just bad thinking.
[00:04:49.540 --> 00:04:50.360]   And they were-
[00:04:50.360 --> 00:04:50.420]   Right.
[00:04:50.420 --> 00:04:52.360]   That's not necessarily true.
[00:04:52.360 --> 00:04:54.080]   That's somebody else's opinion.
[00:04:54.080 --> 00:04:57.340]   That it may not be as bad as this person's painting it.
[00:04:57.340 --> 00:04:59.420]   Oh, they're just trying to get me emotional.
[00:04:59.420 --> 00:05:00.900]   That's an appeal to emotion.
[00:05:00.900 --> 00:05:03.100]   And so they didn't have to get all angry.
[00:05:03.100 --> 00:05:05.540]   They didn't have to feel sad and guilty about things.
[00:05:06.000 --> 00:05:08.140]   They were able to really look at it.
[00:05:08.140 --> 00:05:14.540]   And therefore, their mental and emotional health was so much better going through that
[00:05:14.540 --> 00:05:15.000]   time period.
[00:05:15.000 --> 00:05:16.940]   And I thought, oh, my goodness.
[00:05:16.940 --> 00:05:21.400]   If we can teach, I mean, not just this generation, but let's talk about this generation.
[00:05:21.400 --> 00:05:28.600]   If we can teach them the skills of thinking well and the value of finding truth, they will
[00:05:28.600 --> 00:05:32.160]   not be fooled by all this craziness that is all around us.
[00:05:32.160 --> 00:05:34.080]   So that's why I started the podcast.
[00:05:34.080 --> 00:05:34.940]   Yes.
[00:05:35.040 --> 00:05:41.600]   What a blessing, because it really enables a whole generation and more, right, whoever's
[00:05:41.600 --> 00:05:45.020]   listening with them, to not be reactionary.
[00:05:45.020 --> 00:05:46.420]   That's what I noticed.
[00:05:46.420 --> 00:05:52.600]   The kids who were in logic classes, because they do a whole year of formal logic in B, and
[00:05:52.600 --> 00:05:54.260]   then they do logic.
[00:05:54.760 --> 00:05:59.520]   By the time they graduate, they get two more years of logic.
[00:05:59.520 --> 00:06:00.440]   And so you're right.
[00:06:00.440 --> 00:06:04.400]   It's really difficult to fool these students.
[00:06:04.400 --> 00:06:12.960]   And they are much less reactionary because they are much more about thinking the issue through
[00:06:12.960 --> 00:06:18.640]   and looking for fallacies or, dare I say, bias.
[00:06:19.260 --> 00:06:19.500]   Yep.
[00:06:19.500 --> 00:06:20.860]   Cognitive bias.
[00:06:20.860 --> 00:06:22.340]   Yeah, exactly.
[00:06:22.340 --> 00:06:24.620]   And I've loved listening to your show.
[00:06:24.620 --> 00:06:26.060]   It's so fascinating to me.
[00:06:26.680 --> 00:06:31.120]   And I didn't realize that season one was going to be one way, and then we're going to take
[00:06:31.120 --> 00:06:36.460]   a totally different turn and look at critical thinking through a completely different lens,
[00:06:36.460 --> 00:06:39.420]   which would be even more fascinating to me than season one.
[00:06:39.660 --> 00:06:46.660]   So in season two, you guys, and please go listen to it, she is beginning to look, just like
[00:06:46.660 --> 00:06:49.940]   Lisa said, at cognitive biases.
[00:06:49.940 --> 00:06:56.660]   And I, at first, when you said, I said, oh, yeah, people need to work on their cognitive biases
[00:06:56.660 --> 00:06:58.640]   because, you know, they're really messed up.
[00:06:58.640 --> 00:06:59.360]   It's really bad.
[00:06:59.360 --> 00:07:02.500]   Everyone out here needs a reality check.
[00:07:02.820 --> 00:07:09.980]   And we'll get into it in a minute, but I realized after really pondering what you were presenting
[00:07:09.980 --> 00:07:13.520]   that I need to take a look at my cognitive biases as well.
[00:07:13.520 --> 00:07:20.560]   But I'm curious, what made you select that topic and really want to hone in on that specific
[00:07:20.560 --> 00:07:22.240]   aspect of logical thinking?
[00:07:22.240 --> 00:07:23.000]   Okay.
[00:07:23.100 --> 00:07:28.840]   Now, I feel like you're going to do this, but just for the benefit of the listeners out
[00:07:28.840 --> 00:07:33.640]   there who are thinking, yeah, I want to talk about cognitive bias, but first, I want to know
[00:07:33.640 --> 00:07:34.520]   what this is.
[00:07:34.520 --> 00:07:36.720]   So define it for us.
[00:07:36.720 --> 00:07:39.080]   We'll use one of our classical skills.
[00:07:39.080 --> 00:07:42.120]   Define it for us and then talk about it.
[00:07:42.120 --> 00:07:42.740]   Yes.
[00:07:42.740 --> 00:07:46.420]   I'm going to define a couple terms that we've already, a couple terms, one that we've already
[00:07:46.420 --> 00:07:50.720]   used because before I studied this in Challenge A, I didn't know what it was.
[00:07:50.720 --> 00:07:52.280]   So we've been talking about logical fallacies.
[00:07:52.700 --> 00:07:53.260]   What is that?
[00:07:53.260 --> 00:07:54.600]   A logical fallacy.
[00:07:54.600 --> 00:07:58.540]   So logic is just your thinking and a fallacy is an error.
[00:07:58.540 --> 00:08:01.440]   So a logical fallacy is just an error in thinking.
[00:08:01.440 --> 00:08:03.520]   And we all do it.
[00:08:03.520 --> 00:08:06.500]   And we all have been exposed to them.
[00:08:06.500 --> 00:08:09.720]   We all, and we've kind of wondered, well, that doesn't really make sense, but I don't
[00:08:09.720 --> 00:08:10.900]   know why it doesn't make sense.
[00:08:10.900 --> 00:08:17.380]   Well, cognitive, logical fallacies teach us what's wrong.
[00:08:17.380 --> 00:08:19.780]   And we get exposed to them in the fallacy detective.
[00:08:19.780 --> 00:08:22.060]   There's about 30, 33 of them in the book.
[00:08:22.300 --> 00:08:26.320]   Did you know that there's over 300 named logical fallacies?
[00:08:26.320 --> 00:08:29.200]   Like there is no shortage of ways to think wrong.
[00:08:29.200 --> 00:08:30.880]   So that's one side.
[00:08:30.880 --> 00:08:33.660]   So that is helping us recognize bad arguments.
[00:08:33.660 --> 00:08:41.900]   Now, a cognitive bias is a limitation in our ability to see things objectively.
[00:08:42.640 --> 00:08:48.080]   So cognitive, again, having to do with thinking and then a bias is leaning toward one particular
[00:08:48.080 --> 00:08:50.140]   viewpoint or particular action.
[00:08:50.140 --> 00:08:53.200]   And here's the thing, because you're right, Delise.
[00:08:53.200 --> 00:08:55.340]   Like we all think, oh, we should be unbiased.
[00:08:55.340 --> 00:08:56.140]   I'm unbiased.
[00:08:56.380 --> 00:08:59.080]   The reality is there's no such thing as being unbiased.
[00:08:59.080 --> 00:09:01.080]   You can't be.
[00:09:01.080 --> 00:09:02.040]   You can't be.
[00:09:02.040 --> 00:09:03.980]   We all have biases.
[00:09:03.980 --> 00:09:04.660]   Biases.
[00:09:04.660 --> 00:09:06.500]   And we have them for a couple of reasons.
[00:09:06.500 --> 00:09:13.280]   So number one, we have biases just because of who we are, how we've been raised, and how
[00:09:13.280 --> 00:09:13.920]   we view the world.
[00:09:13.920 --> 00:09:16.640]   There's things about me that I can't change.
[00:09:16.740 --> 00:09:22.720]   I am a woman, I'm a wife, I'm a mother, I'm a Christian, I'm all these things, and these
[00:09:22.720 --> 00:09:24.460]   all affect how I view the world.
[00:09:24.460 --> 00:09:29.500]   The second, you know, where I grew up, the country that I grew up in, the experiences that
[00:09:29.500 --> 00:09:30.200]   I've had growing up.
[00:09:30.200 --> 00:09:34.820]   Yeah, how many siblings you have, yeah, what kind of vacations you took.
[00:09:34.820 --> 00:09:39.120]   Yes, the exposure you've had to the world, like all of these things are going to affect.
[00:09:39.120 --> 00:09:45.460]   The second thing for why we have cognitive biases is because of the way that God designed
[00:09:45.460 --> 00:09:49.360]   our brain, our brain really likes to be efficient.
[00:09:49.360 --> 00:09:52.940]   Our brain really likes to make sense of things.
[00:09:52.940 --> 00:09:57.480]   Our brain really likes to protect us and to keep us safe.
[00:09:57.480 --> 00:10:00.800]   Like there's all these things that our brain is designed to do.
[00:10:00.800 --> 00:10:03.980]   And so therefore, it makes shortcuts.
[00:10:03.980 --> 00:10:09.440]   Our brain likes to take shortcuts because it's faster, it's efficient, it keeps us safe, it
[00:10:09.440 --> 00:10:12.600]   helps us focus on other things that might be more important at that time.
[00:10:12.760 --> 00:10:18.780]   And each of those things, actually, it does something very important for us, but it can
[00:10:18.780 --> 00:10:22.360]   limit us in our ability to think well about the thing that we're seeing.
[00:10:22.360 --> 00:10:24.900]   And that's where these biases come from.
[00:10:24.900 --> 00:10:26.740]   And that's why we all have them.
[00:10:26.740 --> 00:10:32.540]   They happen naturally, but they will affect our ability to think well and to find objective
[00:10:32.540 --> 00:10:32.880]   truth.
[00:10:32.960 --> 00:10:38.420]   And so really, the big thing is, OK, we just need to be aware of them and take some steps
[00:10:38.420 --> 00:10:40.960]   to kind of get past our biases a little bit.
[00:10:40.960 --> 00:10:45.620]   So it's not inherently wrong for people to have biases.
[00:10:46.740 --> 00:10:56.780]   But it is best for us to be aware that we have them and maybe what they are so that it keeps
[00:10:56.780 --> 00:11:04.300]   us on our guard that we're not pulled in a certain way without thinking about the full
[00:11:04.300 --> 00:11:05.160]   situation.
[00:11:05.300 --> 00:11:11.980]   And it just brings it back to one more reason why, you know, in Scripture, God tells us to
[00:11:11.980 --> 00:11:13.800]   take every thought captive.
[00:11:13.800 --> 00:11:17.480]   This is a very active thing that we're told to do.
[00:11:17.480 --> 00:11:19.160]   And this is one of the reasons why.
[00:11:19.160 --> 00:11:21.920]   Like, we have to control our thinking.
[00:11:21.920 --> 00:11:26.800]   We have to actively be on guard with what we're thinking and where our minds and our brains are
[00:11:26.800 --> 00:11:27.080]   going.
[00:11:27.080 --> 00:11:33.360]   And this is one of the reasons is because the brain left on its own will take some shortcuts
[00:11:33.360 --> 00:11:35.600]   that might not lead us to truth.
[00:11:35.600 --> 00:11:37.360]   And we want to be aware of that.
[00:11:37.360 --> 00:11:42.620]   So maybe what our brain does is sort things into piles that are mostly alike.
[00:11:42.620 --> 00:11:45.300]   And then our brain just sees them as alike.
[00:11:45.300 --> 00:11:52.600]   And if we aren't careful to differentiate, we could end up with the wrong conclusion because
[00:11:52.600 --> 00:11:55.640]   we didn't follow a path that was different.
[00:11:56.120 --> 00:11:57.140]   Yeah, gotcha.
[00:11:57.140 --> 00:11:57.460]   Yeah.
[00:11:57.460 --> 00:11:58.140]   Well said.
[00:11:58.140 --> 00:12:04.360]   I think an example of this that I, even today, just on the way to the studio, I was talking
[00:12:04.360 --> 00:12:07.240]   to my brother who lives in Japan right now.
[00:12:07.240 --> 00:12:07.740]   Oh.
[00:12:07.740 --> 00:12:12.200]   And I realized another one of my cognitive biases.
[00:12:12.200 --> 00:12:16.600]   And it was sitting somewhere that I just wasn't expecting.
[00:12:16.600 --> 00:12:19.000]   So it kind of blindsided me, to be honest.
[00:12:19.000 --> 00:12:24.160]   And it came at the intersection of what I believe about the world and my faith.
[00:12:24.900 --> 00:12:33.520]   And essentially, he was telling me that there is a lot of proof for the fact that these ancient
[00:12:33.520 --> 00:12:42.020]   Asian cultures had a deep understanding of the world as we would see it from an evangelical
[00:12:42.020 --> 00:12:42.700]   perspective.
[00:12:43.400 --> 00:12:47.300]   So I won't get into it because I'm going to say some of these things wrong.
[00:12:47.300 --> 00:12:54.500]   But in the way that their language is written, like in their kanji, the story of creation and
[00:12:54.500 --> 00:12:56.660]   just all kinds of different things are embedded there.
[00:12:56.660 --> 00:13:02.560]   And it is very old, much older than anything that America can touch or even Europe can touch.
[00:13:02.560 --> 00:13:03.460]   It is ancient.
[00:13:03.460 --> 00:13:12.340]   And I thought to myself, wow, I see that I am sitting on top of an enormous Western bias because
[00:13:12.340 --> 00:13:20.020]   I just assume, because of the way that I've been taught history, that we need to go and
[00:13:20.020 --> 00:13:23.420]   educate you about these things that you've, quote, never heard.
[00:13:23.600 --> 00:13:29.340]   But the truth about what I see on this paper or even hieroglyphics in your country would
[00:13:29.340 --> 00:13:31.100]   say that you heard it before me.
[00:13:31.100 --> 00:13:37.780]   And I'm just circling back around to something that you all have known and perhaps even know
[00:13:37.780 --> 00:13:40.960]   more about than anything I've ever experienced.
[00:13:41.160 --> 00:13:42.760]   And it just shook me.
[00:13:42.760 --> 00:13:46.040]   And I thought, wow, I'm not sure.
[00:13:46.040 --> 00:13:48.120]   I'm not sure I know what to do with this.
[00:13:48.120 --> 00:13:49.420]   I don't know what to do with that.
[00:13:49.420 --> 00:13:50.600]   Exactly.
[00:13:50.600 --> 00:13:58.520]   Have you had any big surprises in your research about cognitive biases, like personally or things
[00:13:58.520 --> 00:14:01.580]   that sort of just upset the way that you were thinking, Kathy?
[00:14:01.580 --> 00:14:06.820]   Oh my gosh, Delice, you just nailed it so well.
[00:14:06.820 --> 00:14:13.500]   And that is the experience of coming to realize, of having the openness and the awareness to
[00:14:13.500 --> 00:14:18.920]   realize, okay, maybe the way I see things isn't the whole picture here.
[00:14:18.920 --> 00:14:21.820]   So many people cannot do that.
[00:14:21.820 --> 00:14:27.480]   And we see the outcome of people's inability to do that when they're faced with something
[00:14:27.480 --> 00:14:30.780]   new or something that is different or that they just didn't know.
[00:14:30.780 --> 00:14:32.540]   We see this every day on social media.
[00:14:32.540 --> 00:14:37.220]   It's why there's so many arguments and fights and just all kinds of craziness and drama on
[00:14:37.220 --> 00:14:42.560]   social media because somebody runs into an idea that goes against what they knew, know,
[00:14:42.560 --> 00:14:45.700]   or think they know about whatever the topic is.
[00:14:45.700 --> 00:14:49.860]   And they can't even stop and be a little bit curious about it.
[00:14:49.860 --> 00:14:52.200]   So let me give you a couple.
[00:14:52.200 --> 00:14:53.980]   I have all kinds of examples.
[00:14:53.980 --> 00:14:56.120]   So here's one that I have committed.
[00:14:56.900 --> 00:15:02.280]   It's called the Dunning-Kruger effect and it is a cognitive bias.
[00:15:02.280 --> 00:15:05.760]   The Dunning-Kruger effect is just named after the people who kind of did the study and discovered
[00:15:05.760 --> 00:15:05.940]   it.
[00:15:05.940 --> 00:15:08.860]   So the name does not help us to understand it at all.
[00:15:08.860 --> 00:15:14.080]   So, but the Dunning-Kruger effect happens when a person, and this has been scientifically
[00:15:14.080 --> 00:15:20.100]   studied, when a person learns a little bit about something, they are full of confidence and
[00:15:20.100 --> 00:15:23.080]   they feel like they know everything about that topic.
[00:15:23.080 --> 00:15:27.560]   And they will go forth with all the confidence of somebody who has learned a little bit as
[00:15:27.560 --> 00:15:28.480]   if they know everything.
[00:15:28.480 --> 00:15:35.160]   However, the people who study the thing and go on, you know, and they go on, the more that
[00:15:35.160 --> 00:15:41.160]   they learn, yes, the more that they learn, the more they realize they don't know.
[00:15:41.160 --> 00:15:45.380]   And that the more that they, and so there is this overconfidence with people who only know
[00:15:45.380 --> 00:15:48.500]   a little and this deep humility with people who know a lot.
[00:15:48.660 --> 00:15:53.340]   I experienced this after I graduated from high school, I went to a two-year Bible college.
[00:15:53.340 --> 00:15:57.420]   And in this two-year Bible college, we essentially went through the whole Bible, right?
[00:15:57.420 --> 00:15:58.720]   Studied the whole Bible.
[00:15:58.720 --> 00:16:04.620]   I came out of that two years feeling like I knew everything there was to know about the
[00:16:04.620 --> 00:16:04.820]   Bible.
[00:16:04.820 --> 00:16:05.540]   Of course I did.
[00:16:05.540 --> 00:16:07.440]   I had studied it for two years, right?
[00:16:07.440 --> 00:16:08.740]   A lot of time.
[00:16:08.820 --> 00:16:14.280]   And oh my gosh, you know, at all of 20 years old, I, you know, was just so confident in
[00:16:14.280 --> 00:16:15.040]   my Bible knowledge.
[00:16:15.040 --> 00:16:21.220]   Well, the older I've gotten, the more I have realized, oh my goodness, I don't know.
[00:16:21.220 --> 00:16:23.320]   I barely scratched the surface.
[00:16:23.320 --> 00:16:30.340]   There are people who they have specialized, like one specialty would be ancient Near Eastern,
[00:16:30.340 --> 00:16:33.100]   what they would have believed during Bible times.
[00:16:33.200 --> 00:16:37.660]   They've studied, like they have gone so deep into just this one little aspect of scripture
[00:16:37.660 --> 00:16:41.280]   and they have spent an entire career doing so.
[00:16:41.280 --> 00:16:47.840]   And it just made me realize there's not even enough time in a lifetime to learn everything
[00:16:47.840 --> 00:16:49.940]   there is to know about the Bible, right?
[00:16:49.940 --> 00:16:56.640]   And to have the humility to say, there's people who've studied this for years and years that
[00:16:56.640 --> 00:17:00.020]   I could learn from and I still, you know, and I still don't know, right?
[00:17:00.080 --> 00:17:06.620]   So there's just so many, there's so many things just like that, that if we can come with humility
[00:17:06.620 --> 00:17:11.400]   and curiosity, oh my goodness, the world can open up to us.
[00:17:11.400 --> 00:17:14.540]   Yeah, that is, that is so true.
[00:17:14.540 --> 00:17:17.280]   It, it takes a long time.
[00:17:17.280 --> 00:17:19.740]   And I think like you hit the nail on the head.
[00:17:19.740 --> 00:17:27.040]   It takes a great deal of humility to realize that we maybe are standing in this position,
[00:17:27.040 --> 00:17:31.320]   looking at something and we think we see it.
[00:17:31.320 --> 00:17:33.220]   We've looked at it for a long time.
[00:17:33.220 --> 00:17:34.820]   We're looking at it carefully.
[00:17:34.820 --> 00:17:39.600]   We're asking God to show us and we're convinced we see the whole thing,
[00:17:39.600 --> 00:17:43.300]   except for we're not standing over here.
[00:17:43.300 --> 00:17:46.020]   So we're not seeing that whole side.
[00:17:46.020 --> 00:17:51.280]   And so the humility to say, everything I know may be true,
[00:17:51.600 --> 00:17:55.260]   but it may not be everything there is to know.
[00:17:55.260 --> 00:18:02.560]   And so, but it is just so, it's so hard to recognize your own bias,
[00:18:02.560 --> 00:18:07.900]   but it's really harder for somebody else to accuse you of being biased.
[00:18:07.900 --> 00:18:12.200]   We don't like to think that we're only seeing it for one perspective
[00:18:12.200 --> 00:18:15.620]   and that we might be guilty of bias.
[00:18:15.620 --> 00:18:23.000]   So, I mean, really the natural response to being accused of bias is to be defensive.
[00:18:23.000 --> 00:18:23.920]   That's what I do.
[00:18:23.920 --> 00:18:29.160]   When somebody says, well, you're just, you think that because you're biased.
[00:18:29.160 --> 00:18:30.280]   You don't know the whole story.
[00:18:30.280 --> 00:18:31.860]   I feel very defensive.
[00:18:31.860 --> 00:18:34.920]   But what's a better way to respond?
[00:18:34.920 --> 00:18:37.740]   Because, because I'm sure defensiveness is not it.
[00:18:38.580 --> 00:18:42.820]   And I think that's kind of the natural thing because in our, in our world,
[00:18:42.820 --> 00:18:45.540]   we see bias as being a negative thing, right?
[00:18:45.540 --> 00:18:47.440]   We've always kind of seen it that way.
[00:18:47.440 --> 00:18:49.300]   It'll keep you from seeing the truth.
[00:18:49.300 --> 00:18:51.340]   It'll keep you from being the better person.
[00:18:51.340 --> 00:18:53.580]   It'll keep you from being enlightened.
[00:18:53.580 --> 00:18:55.960]   And while those things, right.
[00:18:55.960 --> 00:18:58.240]   And while those things are probably actually true,
[00:18:58.240 --> 00:19:03.420]   but I think the reality is we all just need to come to accept that if somebody accuses us of being biased,
[00:19:03.420 --> 00:19:05.000]   we need to say, yeah, you're right.
[00:19:05.000 --> 00:19:06.380]   I totally do.
[00:19:06.380 --> 00:19:07.760]   Like we're all biased.
[00:19:07.900 --> 00:19:10.600]   We all have different biases and our brains do these things.
[00:19:10.600 --> 00:19:11.020]   Right.
[00:19:11.020 --> 00:19:17.640]   So the question is, you know, the response should be, yeah, you're totally right.
[00:19:17.640 --> 00:19:18.500]   I'm sure I'm biased.
[00:19:18.500 --> 00:19:19.620]   How do you see this?
[00:19:19.620 --> 00:19:21.320]   Tell me how you see it.
[00:19:21.320 --> 00:19:21.540]   Yes.
[00:19:21.540 --> 00:19:24.860]   Like if we could just do that one, if we could come with curiosity,
[00:19:24.860 --> 00:19:29.840]   that is the number one thing that could change so many things if both people.
[00:19:29.840 --> 00:19:30.700]   You're so right.
[00:19:30.700 --> 00:19:37.220]   Preferably if both people, but even if one person can come with the attitude of curiosity, oh my gosh, goodness.
[00:19:37.220 --> 00:19:38.040]   How do you see that?
[00:19:38.040 --> 00:19:39.800]   I'm super curious.
[00:19:39.800 --> 00:19:40.980]   That is so good.
[00:19:40.980 --> 00:19:47.160]   My sister-in-law is, and she may never hear our podcast, so she won't know that I'm saying all these nice things about her.
[00:19:47.840 --> 00:19:59.160]   One year, years and years ago, I heard her be confronted by somebody, I mean, she was just confronted by somebody else who had, who was basically accusing her of bias.
[00:19:59.660 --> 00:20:10.160]   And I could say that it really bothered her, but she stopped herself and she said to them so kindly with lots of humility and a great deal of curiosity, that's so interesting.
[00:20:10.160 --> 00:20:14.880]   Help me understand what you're saying.
[00:20:15.480 --> 00:20:17.260]   Help me understand.
[00:20:17.260 --> 00:20:19.500]   And I thought, that's it.
[00:20:19.500 --> 00:20:23.740]   I don't have to apologize for the way I see it.
[00:20:23.740 --> 00:20:33.640]   I could see this as an opportunity for us to share an understanding that will be totally new to both of us because we're coming at it from different sides.
[00:20:34.000 --> 00:20:38.640]   What if we could both see more of the sphere than we've ever seen before?
[00:20:38.640 --> 00:20:39.020]   Yes.
[00:20:39.020 --> 00:20:40.340]   That would be so awesome.
[00:20:40.340 --> 00:20:41.200]   Yep.
[00:20:41.200 --> 00:20:42.280]   This was a big one.
[00:20:42.820 --> 00:20:47.020]   And I think I talked about this in the second episode of season two.
[00:20:47.020 --> 00:20:50.840]   So coming out of this last election period, right?
[00:20:50.840 --> 00:20:55.140]   And this happens every election based on who wins and who doesn't win.
[00:20:55.140 --> 00:20:58.960]   You're going to have very dramatic responses on both sides.
[00:20:58.960 --> 00:21:03.120]   And then depending on who wins or loses the next time, it switches, right?
[00:21:03.120 --> 00:21:06.880]   And the people doing the dramatics, it's either side is going to be on one or the other.
[00:21:07.560 --> 00:21:18.620]   And here's what I, one of the things that I kept seeing is I can't believe so-and-so, I can't believe the other side thinks this, or I can't believe they're having this reaction or this response.
[00:21:18.620 --> 00:21:20.580]   And I kept thinking, I can.
[00:21:20.580 --> 00:21:26.740]   I totally can understand why both sides are responding and reacting the way they are.
[00:21:26.740 --> 00:21:36.980]   Because if I had been seeing the things that they've been seeing, if I had been hearing the messages that they've been getting, I would think the same thing too.
[00:21:36.980 --> 00:21:37.420]   Right.
[00:21:37.420 --> 00:21:43.160]   If I had the same social media algorithm that they had, I would be thinking and I would be afraid.
[00:21:43.160 --> 00:21:45.280]   I would be excited, whatever it is.
[00:21:45.280 --> 00:21:50.180]   If all the people around me thought the same thing, I'm probably going to think that.
[00:21:50.180 --> 00:21:52.240]   I would think that same way too.
[00:21:52.240 --> 00:21:58.580]   And so when we can step back and realize, okay, people think what they think for a reason.
[00:21:59.000 --> 00:22:08.360]   Now let's step in with curiosity and let's step in with the powerful tool of asking good questions and now we can have a conversation, hopefully.
[00:22:08.360 --> 00:22:16.420]   Now, not everybody is emotionally able to engage in a conversation and that's not our job to get them there.
[00:22:16.740 --> 00:22:20.600]   But we can come with that curiosity and we can have that heart.
[00:22:20.600 --> 00:22:21.500]   Yeah.
[00:22:21.500 --> 00:22:28.000]   Well, and really what I feel like you're showcasing there is empathy, right?
[00:22:28.100 --> 00:22:32.300]   Like how can we be a more empathetic listener?
[00:22:32.300 --> 00:22:42.420]   Because I think there is a point to which once you teach a person to think logically, they can say, okay, well, that's just done because X, Y, Z.
[00:22:42.420 --> 00:22:42.840]   You know?
[00:22:42.840 --> 00:22:43.520]   And it might be.
[00:22:43.520 --> 00:22:44.680]   It might be silly.
[00:22:44.680 --> 00:22:45.240]   It might be.
[00:22:45.240 --> 00:22:46.320]   Like, that's okay.
[00:22:46.320 --> 00:22:53.080]   And if you do see the whole picture, it's okay to say, hey, you're having a cow about this, but there's a bigger issue here.
[00:22:53.080 --> 00:22:59.800]   And to acknowledge both sides, however, love would come at that with a perspective of empathy.
[00:22:59.800 --> 00:23:00.380]   Yes.
[00:23:01.460 --> 00:23:14.380]   And so I love what you're saying and you guys are making me think of the fact that, you know, even our eyes, like the human eye, has a limited spectrum of light and a limited ability to see color.
[00:23:14.380 --> 00:23:18.900]   And we had a whole debate in the office the other day about the color of someone's shirt.
[00:23:18.900 --> 00:23:26.800]   And the truth is that I'm sure it looks exactly as was described by all parties, but our eyes are different.
[00:23:26.800 --> 00:23:33.920]   You know, and so like, it doesn't matter if it's the computer thwarting the image or if it's the person in the room there with you.
[00:23:33.920 --> 00:23:35.880]   My mother and I still call one color.
[00:23:35.880 --> 00:23:37.820]   She calls it green and I call it blue.
[00:23:37.820 --> 00:23:39.600]   And I just have to get over it.
[00:23:39.600 --> 00:23:41.320]   Stephanie and I do the same thing.
[00:23:41.320 --> 00:23:41.900]   Yeah.
[00:23:41.900 --> 00:23:43.580]   We have a set of yellow glasses.
[00:23:43.580 --> 00:23:46.140]   She's declared her green since she was a child.
[00:23:46.140 --> 00:23:47.140]   There you go.
[00:23:47.140 --> 00:23:55.180]   But I think that what you're saying, Kathy, and what you're observing is the fact that the person's eyes actually see that way.
[00:23:55.320 --> 00:24:01.180]   And you can choose to accept that or not accept that or to learn more about that or not learn more about that.
[00:24:01.180 --> 00:24:04.640]   But it is still a fact that that's the way they're perceiving things.
[00:24:04.640 --> 00:24:22.300]   And I'd love to hear you share a little bit about for those who find it perhaps more natural now because they've worked out their logic muscles to identify a fallacy or to observe someone's cognitive bias.
[00:24:23.100 --> 00:24:31.880]   How can that person protect themselves and keep themselves in a position of humility and walking in love?
[00:24:31.880 --> 00:24:32.740]   Yep.
[00:24:32.740 --> 00:24:33.500]   Okay.
[00:24:33.500 --> 00:24:34.800]   I love this question.
[00:24:34.800 --> 00:24:37.200]   I love this question for a couple reasons.
[00:24:37.880 --> 00:24:41.720]   And I have two things that I want to say, so don't let me forget to the second one.
[00:24:41.720 --> 00:24:52.380]   But I also, I want to just give a plug for the upper level of challenge to answer this question.
[00:24:52.380 --> 00:24:59.420]   Because it is not enough to just learn, oh, here's the biases and here's the logical fallacies.
[00:24:59.420 --> 00:24:59.920]   Fantastic.
[00:25:00.220 --> 00:25:00.640]   That's right.
[00:25:00.640 --> 00:25:03.620]   Information is great.
[00:25:03.620 --> 00:25:11.320]   But information can also lead to a harsh and a critical spirit because we can say, oh, well, they're just making fallacies.
[00:25:11.320 --> 00:25:12.200]   Okay.
[00:25:12.200 --> 00:25:13.920]   They're just too ignorant to see.
[00:25:13.920 --> 00:25:15.200]   They just don't never know.
[00:25:15.200 --> 00:25:16.080]   They're just dumb.
[00:25:16.560 --> 00:25:20.240]   And we just write people off or we just don't engage or we make fun or whatever.
[00:25:20.240 --> 00:25:26.660]   What we learned to do in Challenge 3 and especially, oh my gosh, I got to direct Challenge 4 this last year.
[00:25:26.660 --> 00:25:38.380]   And it was so amazing because what the kids learn to do, what the students learn to do is now take that truth and how do we engage the world with it.
[00:25:38.580 --> 00:25:41.880]   And it's not about beating people over the head.
[00:25:41.880 --> 00:25:46.880]   It's not about going out there and telling everybody they're idiots, right?
[00:25:46.880 --> 00:25:47.860]   Because, duh.
[00:25:47.860 --> 00:26:01.060]   It is about how can we take the truth and engage culture and how can we take the truth and really be somebody that somebody else will actually listen to and wants to listen to.
[00:26:01.140 --> 00:26:13.560]   And not that we have to hold ourselves and bend ourselves into culture to do that, but there is a way that we can stand for truth and that we can engage truth in a way that loves people.
[00:26:13.560 --> 00:26:14.740]   Because guess what?
[00:26:14.740 --> 00:26:18.280]   You may disagree with that other person, but they're still made in the image of God.
[00:26:18.280 --> 00:26:20.300]   And God loves them.
[00:26:20.300 --> 00:26:23.420]   And God sees them as a precious son or a precious daughter.
[00:26:23.420 --> 00:26:24.100]   Okay.
[00:26:24.100 --> 00:26:28.100]   So how do we engage them with truth as an image bearer of God?
[00:26:28.100 --> 00:26:32.240]   And that's a skill that we can be learned and that we can cultivate.
[00:26:32.240 --> 00:26:37.400]   And one of the ways that we can do it is we have to show that as parents.
[00:26:37.400 --> 00:26:40.340]   When we get something new, okay, math map.
[00:26:40.340 --> 00:26:41.520]   Let's just go there.
[00:26:41.520 --> 00:26:42.640]   Math map is new.
[00:26:42.640 --> 00:26:46.780]   Oh, I'll never forget practicum last year when it was getting rolled out.
[00:26:46.780 --> 00:26:52.300]   And for me, I'm one of these people that I thought it was genius.
[00:26:52.300 --> 00:26:53.920]   I was like, are you kidding me?
[00:26:53.920 --> 00:26:56.320]   I wish I'd learned math that way.
[00:26:56.320 --> 00:26:58.080]   So I wish I'd learned math.
[00:26:58.080 --> 00:26:59.680]   I wish my daughter had learned math.
[00:26:59.680 --> 00:27:01.620]   I think she would have done very well.
[00:27:01.620 --> 00:27:04.060]   I wish like I was like, oh, it just made my heart ache.
[00:27:04.060 --> 00:27:06.020]   I said, well, you can homeschool your kids and teach them this way.
[00:27:06.020 --> 00:27:07.040]   Do it with your kids and learn.
[00:27:07.040 --> 00:27:08.180]   And you'll get to learn it again.
[00:27:08.180 --> 00:27:09.260]   Redeem your own education.
[00:27:09.260 --> 00:27:15.860]   But I also think it's fascinating that Lee Bortons looked at math, something that all of us, right?
[00:27:15.860 --> 00:27:17.980]   Delise, to your point, this is just how math is done.
[00:27:17.980 --> 00:27:18.840]   We all know this.
[00:27:18.840 --> 00:27:20.040]   There's only one way to do math.
[00:27:20.040 --> 00:27:25.760]   And she just took all the pieces and changed it all around and jumbled it up and presented it.
[00:27:25.760 --> 00:27:26.540]   It totally knew.
[00:27:26.540 --> 00:27:30.620]   And so for me, I'm sitting there and I'm thinking, this is amazing.
[00:27:30.620 --> 00:27:31.800]   This is so great.
[00:27:31.800 --> 00:27:32.700]   I'm so interested.
[00:27:33.240 --> 00:27:36.900]   But not everybody at my table thought of it that way.
[00:27:36.900 --> 00:27:38.360]   Embraced it that way.
[00:27:38.360 --> 00:27:46.740]   There were these precious, beautiful homeschool mamas that were kind of mad about it because they didn't want to learn a new way of doing math.
[00:27:46.740 --> 00:27:48.280]   They didn't understand it.
[00:27:48.280 --> 00:27:52.600]   And when, you know, the presenter came around, I said, well, why don't we ask the questions?
[00:27:52.600 --> 00:27:55.060]   They didn't even want to ask the questions, right?
[00:27:55.200 --> 00:28:01.560]   And so I thought, okay, so if we can, you know, and it's hard to learn new things.
[00:28:01.560 --> 00:28:06.980]   It's hard to be open to a new way of thinking because, again, it's how our brains work.
[00:28:06.980 --> 00:28:08.400]   Our brains want to keep us safe.
[00:28:08.400 --> 00:28:10.560]   Our brains want to just go, no, we already know this.
[00:28:10.560 --> 00:28:11.140]   We got this.
[00:28:11.140 --> 00:28:13.800]   We know this way and we got there fine.
[00:28:13.800 --> 00:28:15.580]   Yes, we got there fine.
[00:28:15.580 --> 00:28:17.040]   So why mess it all up?
[00:28:17.040 --> 00:28:18.500]   You know, it's scary.
[00:28:18.500 --> 00:28:19.540]   It's new, blah, blah, blah.
[00:28:19.540 --> 00:28:20.620]   All the things, right?
[00:28:20.620 --> 00:28:21.720]   And I get it.
[00:28:21.720 --> 00:28:22.240]   I get it.
[00:28:22.240 --> 00:28:24.880]   This is how, this is what our brains do.
[00:28:24.880 --> 00:28:32.680]   But again, if we can come with the idea of being open, of asking questions, and just
[00:28:32.680 --> 00:28:37.500]   be curious, it doesn't ever hurt us to learn something new, right?
[00:28:37.500 --> 00:28:39.960]   It never hurts us to learn something new.
[00:28:39.960 --> 00:28:46.680]   And one of the things that I learned just from directing challenge and just life in general
[00:28:46.680 --> 00:28:53.460]   is sometimes asking the right question is far more important than getting the right answer.
[00:28:53.780 --> 00:28:59.040]   And when we can come with this spirit of, wow, let's just learn.
[00:28:59.040 --> 00:28:59.800]   Guess what?
[00:28:59.800 --> 00:29:01.860]   Learning something new doesn't mean you have to do it.
[00:29:01.860 --> 00:29:03.200]   Doesn't mean you have to agree with it.
[00:29:03.200 --> 00:29:04.580]   Doesn't mean you have to like it.
[00:29:04.580 --> 00:29:07.500]   But doesn't it help just to know about it?
[00:29:07.500 --> 00:29:09.040]   Why don't you know it?
[00:29:09.040 --> 00:29:12.740]   Learn about what you don't like or learn about this thing that you disagree with.
[00:29:12.740 --> 00:29:15.360]   It doesn't hurt you at all to do that.
[00:29:15.420 --> 00:29:19.960]   And so if we can just cultivate this spirit of just being curious, right?
[00:29:19.960 --> 00:29:22.240]   Let me just be curious about this thing and find out.
[00:29:22.240 --> 00:29:24.600]   And maybe I don't, I don't have to agree with it.
[00:29:24.600 --> 00:29:31.420]   Whatever, like insert any topic from society, from culture, from religion, insert any topic.
[00:29:31.420 --> 00:29:32.540]   Guess what?
[00:29:32.540 --> 00:29:37.960]   It's, it's a good mind that can hold two ideas and look at them and say, okay, well, what are
[00:29:37.960 --> 00:29:38.220]   they?
[00:29:38.300 --> 00:29:39.340]   Let's just find out what they are.
[00:29:39.340 --> 00:29:40.400]   What's the truth?
[00:29:40.400 --> 00:29:41.880]   How do they, how do they compare?
[00:29:41.880 --> 00:29:44.460]   How do they, like, what do we think about this thing?
[00:29:44.460 --> 00:29:46.140]   That's a beautiful thing.
[00:29:46.140 --> 00:29:51.280]   And so we can teach our kids to do that and we can learn, yes, even though our brains are
[00:29:51.280 --> 00:29:52.360]   set and all this stuff.
[00:29:52.360 --> 00:29:53.120]   No, they're not.
[00:29:53.120 --> 00:29:54.260]   We can learn these things too.
[00:29:54.260 --> 00:29:56.700]   I love what you're saying, Kathy.
[00:29:56.700 --> 00:30:02.240]   That is a way to be winsome, to be winsome as a person.
[00:30:02.500 --> 00:30:09.680]   And, you know, in CC, we spend all of these years saying that our mission is to know God
[00:30:09.680 --> 00:30:11.400]   and to make him known.
[00:30:11.400 --> 00:30:18.080]   And so we raise our children to see God in the word and to see God in the world.
[00:30:18.080 --> 00:30:24.940]   And we give them all the tools of communication that they need to go out and speak God's truth
[00:30:24.940 --> 00:30:25.920]   on his behalf.
[00:30:25.920 --> 00:30:34.260]   But I think that sometimes we forget that one of the ways to make God known is to be
[00:30:34.260 --> 00:30:45.940]   Jesus, to be like Jesus, and to draw people to that spirit of love and winsome curiosity
[00:30:45.940 --> 00:30:55.720]   and open-mindedness that will say not open-minded to take in what's not true, but open-minded to
[00:30:55.720 --> 00:31:02.200]   asking you what you believe and why you think that and to explore it together.
[00:31:02.200 --> 00:31:04.760]   That's what Jesus would do.
[00:31:04.760 --> 00:31:08.180]   And so that is one way to make him known.
[00:31:08.180 --> 00:31:10.040]   That's good.
[00:31:10.040 --> 00:31:10.460]   Yep.
[00:31:10.460 --> 00:31:11.320]   A hundred percent.
[00:31:11.320 --> 00:31:12.060]   I agree.
[00:31:12.060 --> 00:31:12.900]   Yeah.
[00:31:12.900 --> 00:31:18.960]   I'm curious about your perspective of technology.
[00:31:18.960 --> 00:31:23.060]   I've asked many people about their perspective of technology lately, just because I want to
[00:31:23.060 --> 00:31:25.280]   know we're at the cusp of a new era.
[00:31:25.520 --> 00:31:32.220]   And so when we're thinking about thinking critically, teaching our students to think critically,
[00:31:32.220 --> 00:31:38.160]   teaching them to think logically, identify their biases, and then we put AI in the mix,
[00:31:38.160 --> 00:31:44.160]   we get an interesting collection of scenarios.
[00:31:44.160 --> 00:31:54.860]   How do you think the use of AI is affecting or even empowering people?
[00:31:54.860 --> 00:31:57.340]   I don't want to, you know, I don't want to dictate how you're going to answer.
[00:31:57.340 --> 00:31:57.660]   Right.
[00:31:57.660 --> 00:32:02.480]   Or even empowering people to navigate issues.
[00:32:02.860 --> 00:32:10.420]   And what would it look like for us to set our children up for success knowing that the presence
[00:32:10.420 --> 00:32:12.040]   of AI is now permanent?
[00:32:12.040 --> 00:32:12.620]   Yep.
[00:32:12.620 --> 00:32:13.740]   It is not going away.
[00:32:13.740 --> 00:32:15.440]   Parents, it is not going away.
[00:32:15.440 --> 00:32:21.060]   It is only going to become ever more pervasive in society.
[00:32:21.320 --> 00:32:23.980]   And so, yes, this is something that's interesting to me.
[00:32:23.980 --> 00:32:25.200]   I think it's a great question.
[00:32:25.200 --> 00:32:29.960]   And this is one of the things that I encourage parents to have an ongoing conversation with
[00:32:29.960 --> 00:32:30.480]   their kids about.
[00:32:30.480 --> 00:32:31.880]   Like, we need to be talking about this.
[00:32:31.880 --> 00:32:32.440]   Yeah.
[00:32:33.000 --> 00:32:36.500]   And so, here's where I'm at today, right?
[00:32:36.500 --> 00:32:39.580]   My ideas about this may change the more I learn or the more I grow.
[00:32:39.580 --> 00:32:46.240]   But here's where I'm at today is, okay, AI is a tool that can be used rightly and can be used
[00:32:46.240 --> 00:32:46.720]   wrongly.
[00:32:46.720 --> 00:32:48.420]   This is how I see it today.
[00:32:48.420 --> 00:32:55.240]   I see that there are some incredible applications for AI in culture, in society, in life.
[00:32:55.240 --> 00:32:59.580]   And I see that there are applications that can absolutely destroy people.
[00:32:59.580 --> 00:33:01.520]   So, let me give a few examples.
[00:33:02.780 --> 00:33:08.640]   I have read articles about kids who have trouble, like they're getting into the high
[00:33:08.640 --> 00:33:13.400]   school and the college years, and they have trouble formulating ideas, having conversations
[00:33:13.400 --> 00:33:16.320]   because they have not been forced to think.
[00:33:16.320 --> 00:33:21.640]   They have been relying on AI to do their work and their thinking for them.
[00:33:21.640 --> 00:33:22.700]   To spit it out.
[00:33:22.700 --> 00:33:22.700]   To spit it out.
[00:33:22.700 --> 00:33:26.160]   To spit out the answer without processing the information.
[00:33:26.160 --> 00:33:27.380]   So, guess what?
[00:33:27.380 --> 00:33:28.500]   Their brain is atrophied.
[00:33:28.500 --> 00:33:31.360]   They are actually atrophying their brain.
[00:33:31.360 --> 00:33:37.180]   And the brain is the one organ that the more you use it, the more you're able to use it.
[00:33:37.180 --> 00:33:42.380]   And the less you use it, the less capacity you are going to have in your future to be able
[00:33:42.380 --> 00:33:42.860]   to use it.
[00:33:42.860 --> 00:33:45.900]   And so, in that way, I think it can be really dangerous.
[00:33:45.900 --> 00:33:48.580]   The other thing is, AI is sometimes wrong.
[00:33:48.960 --> 00:33:54.020]   And so, we have to teach our kids, look, just because you get an answer on AI does not mean
[00:33:54.020 --> 00:33:54.360]   it's right.
[00:33:54.360 --> 00:33:55.060]   It could be wrong.
[00:33:55.060 --> 00:33:57.800]   AI is only as good as what it's feeding from.
[00:33:57.800 --> 00:34:00.380]   And that can be biased.
[00:34:00.380 --> 00:34:02.240]   And so, guess what?
[00:34:02.240 --> 00:34:05.300]   Whatever tool you're using, it could be biased.
[00:34:05.300 --> 00:34:06.580]   And you have to be aware of it.
[00:34:06.820 --> 00:34:12.540]   Now, the other thing that I think we have to be very aware of with AI is it is now being
[00:34:12.540 --> 00:34:17.160]   developed to be a replacement for human connection and relationship.
[00:34:17.160 --> 00:34:24.540]   So, I got an ad in my social media just recently where it was advertising an AI tool that would
[00:34:24.540 --> 00:34:27.240]   be a replacement for a therapist or a counselor.
[00:34:27.920 --> 00:34:35.000]   I have even  there is even programs where you can get an AI boyfriend or a girlfriend.
[00:34:35.000 --> 00:34:35.940]   Oh, my word.
[00:34:35.940 --> 00:34:37.060]   I've heard about that.
[00:34:37.060 --> 00:34:37.980]   I thought it was a joke.
[00:34:37.980 --> 00:34:38.660]   Nope.
[00:34:38.660 --> 00:34:39.280]   It's not a joke.
[00:34:39.280 --> 00:34:40.160]   It's a real thing.
[00:34:40.160 --> 00:34:41.020]   It's totally real.
[00:34:41.020 --> 00:34:49.220]   And so, now somebody can have what's meant to be this experience with another flawed but
[00:34:49.220 --> 00:34:55.560]   beautiful person with a machine that's only going to give you one type of feedback and
[00:34:55.560 --> 00:35:01.860]   like, oh, just the implications could be really detrimental and scary.
[00:35:01.860 --> 00:35:05.960]   So, but this is all our kids are going to know.
[00:35:05.960 --> 00:35:11.240]   So, we have to have this conversation with them and we got to stay up with what's going
[00:35:11.240 --> 00:35:13.360]   on and talk to them about this stuff.
[00:35:13.360 --> 00:35:13.960]   Right.
[00:35:13.960 --> 00:35:16.920]   We can't just say, oh, I don't want to think about that.
[00:35:16.920 --> 00:35:18.140]   That's not worth thinking about.
[00:35:18.140 --> 00:35:24.000]   Well, you got to think about it because you need to arm them with good thinking about it.
[00:35:24.000 --> 00:35:24.160]   Yep.
[00:35:24.500 --> 00:35:25.560]   That's really good.
[00:35:25.560 --> 00:35:30.500]   So, am I hearing you say, and you can say, no, you're not hearing me say that to Lisa,
[00:35:30.500 --> 00:35:38.920]   but am I hearing you say that you think maybe an appropriate application of AI would be,
[00:35:38.920 --> 00:35:44.500]   as you said at the beginning, to use it as a tool and then to have your own tools to check
[00:35:44.500 --> 00:35:44.740]   it?
[00:35:44.740 --> 00:35:46.680]   Oh, 100%.
[00:35:46.680 --> 00:35:51.480]   And that's how you see it playing out even in a, quote, classical education.
[00:35:51.480 --> 00:35:52.540]   Yes.
[00:35:52.540 --> 00:35:55.880]   I think there are, I think there is a place for it.
[00:35:55.880 --> 00:36:01.760]   I think in a classical education, I think there's less of a place for it because we're not necessarily
[00:36:01.760 --> 00:36:03.920]   relying on it that way.
[00:36:04.060 --> 00:36:08.880]   I'm talking about in society, I think there's, you know, there's going to be medical applications
[00:36:08.880 --> 00:36:09.200]   for it.
[00:36:09.200 --> 00:36:13.320]   There's going to be technical applications for it where it is going to be fantastic.
[00:36:13.820 --> 00:36:19.040]   But we have, but you can only apply a tool when you understand the tool.
[00:36:19.040 --> 00:36:24.220]   And so when we understand what the tool is and what it does, we can apply it well.
[00:36:24.220 --> 00:36:28.880]   But when we're starting to apply it, where it's replacing our brain, where it's replacing other
[00:36:28.880 --> 00:36:35.000]   people's, where it's replacing human connection, the way that we're created as human beings to do,
[00:36:35.000 --> 00:36:36.000]   I think that's wrong.
[00:36:36.000 --> 00:36:41.100]   And so that's where, that's where we've got to be very careful.
[00:36:41.100 --> 00:36:48.340]   I have not seen anything with my daughter going from foundations to challenge for where she has
[00:36:48.340 --> 00:36:48.860]   needed AI.
[00:36:48.860 --> 00:36:53.220]   Like, I don't know that it's got a big place in a classical education, at least to the point
[00:36:53.220 --> 00:36:53.920]   that we're at.
[00:36:53.920 --> 00:36:58.240]   But that doesn't mean that I don't see applications for it in society in certain places.
[00:36:58.240 --> 00:36:59.600]   So yeah, sure.
[00:36:59.600 --> 00:37:01.560]   So that's, that is what you're hearing me say, Elise.
[00:37:01.560 --> 00:37:01.880]   Sure.
[00:37:01.880 --> 00:37:02.460]   Yeah.
[00:37:02.460 --> 00:37:11.700]   Classical education is so much about wrestling with big ideas in community, wrestling together
[00:37:11.700 --> 00:37:13.700]   to see, this is what I saw.
[00:37:13.700 --> 00:37:15.700]   This is what I heard from what we just read.
[00:37:15.700 --> 00:37:19.240]   Oh, well, I heard something different, or it made me think in a different way.
[00:37:19.240 --> 00:37:23.620]   And then you've got a third person saying, well, I saw something different than both of you,
[00:37:23.620 --> 00:37:27.000]   but maybe the truth is in the middle and, and that wrestling.
[00:37:27.000 --> 00:37:35.620]   And so that is more a relational community aspect of education than AI could produce.
[00:37:35.620 --> 00:37:41.200]   But I love it, Kathy, that you are thinking about that and that you know that you want
[00:37:41.200 --> 00:37:42.000]   to learn more.
[00:37:42.000 --> 00:37:45.420]   I really appreciated you giving the caveat.
[00:37:45.420 --> 00:37:51.220]   God, this is what I think today, but I reserve the right to change as I learn because you
[00:37:51.220 --> 00:37:55.200]   model the lifelong learner so beautifully.
[00:37:55.200 --> 00:38:01.200]   I mean, it's not that you have now arrived and you know enough about logic to have a podcast
[00:38:01.200 --> 00:38:02.700]   that everybody will listen to.
[00:38:02.700 --> 00:38:04.320]   You're still learning.
[00:38:04.920 --> 00:38:07.480]   And I think it's because you're such a curious person.
[00:38:07.480 --> 00:38:09.760]   Like you yourself have a lot of curiosity.
[00:38:09.760 --> 00:38:13.060]   What is it that feeds your curiosity?
[00:38:13.060 --> 00:38:15.880]   Well, I think that's a great question.
[00:38:17.140 --> 00:38:21.600]   Number one, it's a mindset that I can just learn whatever I need to know.
[00:38:21.600 --> 00:38:23.340]   I might not know it, but I can learn it.
[00:38:23.340 --> 00:38:24.260]   But you can learn it.
[00:38:24.260 --> 00:38:25.080]   We can learn it.
[00:38:25.080 --> 00:38:25.880]   We can learn anything.
[00:38:25.880 --> 00:38:28.840]   If there's anything I wanted to learn, I could learn it.
[00:38:28.840 --> 00:38:35.380]   But it's also giving myself the freedom to follow my interest and to say, well, I'm really
[00:38:35.380 --> 00:38:36.740]   interested in this.
[00:38:36.740 --> 00:38:39.000]   So like it's with your kid.
[00:38:39.060 --> 00:38:42.120]   If you've got a kid who's into something, let them go all in.
[00:38:42.120 --> 00:38:46.440]   Like let there's so much learning that can happen around Legos, right?
[00:38:46.440 --> 00:38:55.460]   Dinosaurs or whatever the interest is that a child currently has, you know, and because
[00:38:55.460 --> 00:39:00.420]   I see this, right, even in classical conversations about how, you know, parents, they get nervous.
[00:39:00.420 --> 00:39:02.000]   Well, I could never direct challenge.
[00:39:02.000 --> 00:39:02.660]   I could never.
[00:39:02.660 --> 00:39:03.360]   Oh, my word.
[00:39:03.360 --> 00:39:04.500]   I don't know this.
[00:39:04.500 --> 00:39:05.200]   I don't know.
[00:39:05.200 --> 00:39:09.040]   Trust me, going into challenge four, I didn't know anything about.
[00:39:09.040 --> 00:39:09.700]   Greek literature.
[00:39:09.700 --> 00:39:11.540]   I've never read the Odyssey before.
[00:39:11.540 --> 00:39:14.700]   I'd never like that's never been part of my education.
[00:39:14.700 --> 00:39:15.340]   Yes.
[00:39:15.340 --> 00:39:20.280]   But if we can just have the attitude of, OK, well, let's figure it out.
[00:39:20.280 --> 00:39:24.280]   I'm I'm reading it at the same time that these kids are reading it.
[00:39:24.280 --> 00:39:26.300]   And that's that's fine.
[00:39:26.300 --> 00:39:31.480]   Like, even if I'm just one day ahead of the kids in my class, that's enough.
[00:39:31.480 --> 00:39:32.440]   That's enough.
[00:39:32.440 --> 00:39:35.640]   You know, and I think that is one of the most important things.
[00:39:35.640 --> 00:39:40.600]   It's if we model this mind that's just excited about learning something new, right?
[00:39:40.600 --> 00:39:42.620]   And you just be excited about learning something new.
[00:39:42.620 --> 00:39:43.620]   That's all you have to do.
[00:39:43.620 --> 00:39:46.840]   Or even just willing to learn something new.
[00:39:46.840 --> 00:39:50.600]   That we really I mean, that's a great encouragement because you're right.
[00:39:50.600 --> 00:39:55.580]   There are a lot of parents who think, oh, I'm going to I'm going to need to put them in school
[00:39:55.580 --> 00:39:58.240]   or we're going to have to do dual enrollment or something.
[00:39:58.240 --> 00:40:03.920]   Because when we get to upper level math or when we get to fourth year Latin or when we
[00:40:03.920 --> 00:40:08.620]   get to Greek literature, when we get to policy debate, I don't know anything about that.
[00:40:08.620 --> 00:40:09.860]   I'm just out of my depth.
[00:40:09.860 --> 00:40:16.020]   I love the encouragement that you have of learning alongside your kids.
[00:40:16.020 --> 00:40:22.660]   Really, some of the best lessons that you will teach as a lead learner in your home.
[00:40:23.120 --> 00:40:30.540]   You teach when you don't know what you're doing in that subject because you teach your child to stay
[00:40:30.540 --> 00:40:35.600]   that even people who know nothing can learn something.
[00:40:35.600 --> 00:40:36.380]   Yep.
[00:40:36.380 --> 00:40:39.640]   And that's a great encouragement.
[00:40:39.640 --> 00:40:39.820]   Yeah.
[00:40:39.820 --> 00:40:42.540]   And it's all about the power of learning to ask good questions.
[00:40:42.540 --> 00:40:43.200]   Yeah.
[00:40:43.200 --> 00:40:49.020]   And this is the this is the the trick to not knowing is asking the questions, because if
[00:40:49.020 --> 00:40:53.340]   you can ask the question now, you've got something to go off of.
[00:40:53.340 --> 00:40:53.520]   Right.
[00:40:53.520 --> 00:40:57.640]   Because we just we just tend to the reason we get stuck is we're like, well, I just I just
[00:40:57.640 --> 00:40:58.640]   don't get it.
[00:40:58.640 --> 00:41:00.580]   OK, well, you're not helpless.
[00:41:00.580 --> 00:41:02.560]   You're like nobody is helpless here.
[00:41:02.560 --> 00:41:04.820]   So if you don't know, you don't know.
[00:41:04.820 --> 00:41:05.160]   Right.
[00:41:05.160 --> 00:41:05.380]   Right.
[00:41:05.380 --> 00:41:06.840]   Go back to what you did know.
[00:41:06.840 --> 00:41:07.360]   Yeah.
[00:41:07.360 --> 00:41:08.660]   Just ask the next question.
[00:41:08.660 --> 00:41:09.400]   That's all you have to do.
[00:41:09.400 --> 00:41:10.400]   Ask the next question.
[00:41:10.780 --> 00:41:13.460]   Be interested in learning what is the next thing.
[00:41:13.460 --> 00:41:15.580]   And you don't have to know all of it.
[00:41:15.580 --> 00:41:17.240]   Just find out the next thing.
[00:41:17.240 --> 00:41:18.140]   What's the next thing?
[00:41:18.140 --> 00:41:19.100]   That's all you got to do.
[00:41:19.100 --> 00:41:20.180]   I love that.
[00:41:20.180 --> 00:41:21.260]   The next thing.
[00:41:21.260 --> 00:41:22.080]   So that's right.
[00:41:22.080 --> 00:41:27.060]   All of us just need to take a deep breath and go learn the next thing.
[00:41:27.060 --> 00:41:28.000]   Yeah.
[00:41:28.000 --> 00:41:28.760]   Got it.
[00:41:28.760 --> 00:41:29.120]   Yeah.
[00:41:29.120 --> 00:41:34.020]   And going back to what you were saying, Lisa, a couple of minutes ago, I mean, that is
[00:41:34.020 --> 00:41:38.860]   really a Christ like blueprint because he would just ask questions.
[00:41:38.860 --> 00:41:39.280]   Yes.
[00:41:39.280 --> 00:41:44.600]   Even though he knew the answers, which to me is just the most humble.
[00:41:44.600 --> 00:41:47.660]   Like, obviously, we're asking questions and we don't know the answer.
[00:41:47.660 --> 00:41:49.180]   So there goes our limitation.
[00:41:49.180 --> 00:41:52.740]   But he's asking questions and he does know the answer.
[00:41:52.740 --> 00:41:55.740]   And we're both trying to lead toward a new discovery.
[00:41:55.740 --> 00:41:59.000]   And so we can do that simple.
[00:41:59.000 --> 00:41:59.920]   Yes.
[00:41:59.920 --> 00:42:00.800]   That simple step.
[00:42:00.800 --> 00:42:01.620]   Yes.
[00:42:01.840 --> 00:42:07.180]   And I just think about, like, for example, the woman at the well and, like, the beautiful
[00:42:07.180 --> 00:42:11.760]   discoveries that she unpacked with a series of very simple questions.
[00:42:12.120 --> 00:42:12.920]   Where's your husband?
[00:42:12.920 --> 00:42:14.600]   Like, what's going on?
[00:42:14.600 --> 00:42:16.480]   Just tell me a little bit more about you.
[00:42:16.480 --> 00:42:17.680]   And she starts gushing.
[00:42:17.680 --> 00:42:23.180]   And you're just making me excited as two women who are further down the road in your homeschooling
[00:42:23.180 --> 00:42:29.680]   journeys than I am for the things that I'm sure I'm going to discover with my boys by just
[00:42:29.680 --> 00:42:31.280]   not having to have the answers.
[00:42:31.500 --> 00:42:33.060]   You know, that takes that pressure off.
[00:42:33.060 --> 00:42:37.180]   So, you know, Kathy, talking to you has been a joy.
[00:42:37.180 --> 00:42:38.400]   I knew it was going to be.
[00:42:38.400 --> 00:42:42.120]   I said so before we started recording because I just love hearing your thoughts.
[00:42:42.120 --> 00:42:46.840]   And I know that our listeners probably want to hear more of your thoughts from this conversation.
[00:42:46.840 --> 00:42:50.740]   So if they're saying, hey, where can I find her online?
[00:42:50.740 --> 00:42:51.960]   Where would you send them?
[00:42:51.960 --> 00:42:52.820]   Yeah, absolutely.
[00:42:52.820 --> 00:42:55.640]   So the podcast is called Filter It Through a Brain Cell.
[00:42:55.640 --> 00:42:58.280]   They can find it on pretty much any podcast listener.
[00:42:58.280 --> 00:42:59.920]   That's a really great place to start.
[00:42:59.920 --> 00:43:04.800]   I recommend go back and start with season one because it kind of builds on itself as you go.
[00:43:04.800 --> 00:43:07.920]   I do have a free quiz on my website.
[00:43:07.920 --> 00:43:12.700]   So people can go to filter it through brain cell dot com forward slash quiz that they can take.
[00:43:12.700 --> 00:43:15.620]   You can have your middle schoolers or high schoolers take it to you.
[00:43:15.620 --> 00:43:16.520]   It's got 10 different.
[00:43:16.520 --> 00:43:21.340]   I love teaching through memes, headlines, articles like stuff that you see in real life.
[00:43:21.340 --> 00:43:21.620]   Right.
[00:43:21.620 --> 00:43:25.600]   And it's basically, can you figure out what the fallacy is there?
[00:43:25.940 --> 00:43:39.140]   And for all my challenge a families or directors, I have a matchup that I created of podcast episodes that goes along with each chapter each week of the fallacy detective as you're going through that.
[00:43:39.140 --> 00:43:43.360]   So filter it through brain cell dot com forward slash a you can download it for free.
[00:43:43.720 --> 00:43:48.820]   And it's just a fun way to go along with what they're learning in challenge a so anyways that yeah.
[00:43:48.820 --> 00:43:50.640]   Or they can find me on Instagram.
[00:43:50.640 --> 00:43:51.640]   I'm on social media.
[00:43:51.640 --> 00:43:52.560]   So either way.
[00:43:52.560 --> 00:43:53.660]   So cool.
[00:43:53.660 --> 00:43:54.920]   That's so cool.
[00:43:54.920 --> 00:43:56.100]   This has been great.
[00:43:56.100 --> 00:43:58.340]   It's been really nice to have you, Kathy.
[00:43:58.340 --> 00:44:00.060]   Well, thanks so much for having me on.
[00:44:00.060 --> 00:44:02.480]   I love partnering with I mean, we love classical conversations.
[00:44:02.480 --> 00:44:15.060]   I love partnering with CC and yeah, I just to me, I think that people who care about if we can teach our kids to not just know truth, because that's important.
[00:44:15.060 --> 00:44:19.780]   But I think the even more important tool and skill is to love truth.
[00:44:19.780 --> 00:44:23.680]   If we can teach our children to love the truth.
[00:44:23.680 --> 00:44:24.120]   Yes.
[00:44:24.120 --> 00:44:25.860]   That changes a generation.
[00:44:26.320 --> 00:44:32.560]   And that is my hope and that is my goal is that we can help this next I have a lot of hope for this next generation.
[00:44:32.560 --> 00:44:36.280]   And I pray that they can learn to love the truth.
[00:44:36.280 --> 00:44:38.060]   That's awesome.
[00:44:38.060 --> 00:44:39.060]   That's awesome.
[00:44:39.060 --> 00:44:46.780]   So parents, we hope that you have been blessed, that you have been encouraged to go out and learn the next new thing.
[00:44:46.780 --> 00:44:54.200]   And that generally the everyday educator encourages you to keep learning on your own and to see what's new.
[00:44:54.680 --> 00:44:59.320]   You may want to see what's new with Classical Conversations books this summer.
[00:44:59.320 --> 00:45:10.740]   If you are looking for something to do this summer to expand your mind or to get ahead of your kids, take a look at what is coming out, what's new in the bookstore.
[00:45:10.740 --> 00:45:14.320]   We've got coming out the Math Map Digits.
[00:45:14.320 --> 00:45:20.140]   Kathy mentioned the Math Map, our new classical education approach to teaching math with your family.
[00:45:20.620 --> 00:45:24.880]   So we've got Math Map Digits, integers, fractions.
[00:45:24.880 --> 00:45:36.220]   We've got two sets of flashcards that offer some really simple classical ways to practice those essential math skills.
[00:45:36.220 --> 00:45:40.680]   Math Map Monomials is coming out for our Challenge B families.
[00:45:40.680 --> 00:45:43.240]   They will be using that this fall.
[00:45:43.760 --> 00:45:49.300]   There is a gorgeous new Reasoning Together philosophy textbook.
[00:45:49.300 --> 00:45:53.500]   It has got wonderful excerpts.
[00:45:53.500 --> 00:46:07.820]   It helps our students explore great thinkers' ideas through reading those works and then participating in Community Day in some Socratic discussions.
[00:46:08.820 --> 00:46:11.320]   And there's always new Copper Lodge Library books.
[00:46:11.320 --> 00:46:13.080]   We've got four new classics.
[00:46:13.080 --> 00:46:19.080]   Pilgrim's Progress is one of them that have great illustrations and some really good insights.
[00:46:19.080 --> 00:46:21.180]   You could explore all the new products.
[00:46:21.180 --> 00:46:24.820]   And if you don't know which one you're the most interested in, here's what you do.
[00:46:24.820 --> 00:46:33.880]   Go to classicalconversations.com forward slash what's new and you'll find everything that's there.
[00:46:33.880 --> 00:46:34.560]   Okay?
[00:46:34.560 --> 00:46:38.900]   Well, guys, thank you for listening to our show today.
[00:46:38.900 --> 00:46:40.900]   Thank you again, Kathy, for being here.
[00:46:40.900 --> 00:46:42.660]   It has meant a lot.
[00:46:42.660 --> 00:46:54.860]   And we will see you over on social media at Everyday Educator Podcast or on YouTube if you want to watch, if you want to see Kathy's smiling face, we're over there on the podcast channel on YouTube as well.
[00:46:54.860 --> 00:46:55.880]   So we'll talk to you soon.
[00:46:55.880 --> 00:46:56.340]   Thanks.
[00:46:56.340 --> 00:46:57.260]   All right.
[00:46:57.260 --> 00:46:58.300]   Bye, guys.
[00:46:58.300 --> 00:46:59.300]   Bye.
[00:46:59.300 --> 00:47:01.300]   Bye.
[00:47:01.300 --> 00:47:01.800]   you
[00:47:01.800 --> 00:47:03.860]   you
[00:47:03.860 --> 00:47:04.880]   Thank you.

