
[00:00:00.000 --> 00:00:06.000]   When you have small teams, like the value of ML is you could start to like really scale
[00:00:06.000 --> 00:00:09.220]   things out because you start to use machines as like the assistant to you, right?
[00:00:09.220 --> 00:00:13.640]   So you train something manually and then you like send it out in the world and then it
[00:00:13.640 --> 00:00:17.880]   does that at scale for you, which is like a really, like it's like a superpower.
[00:00:17.880 --> 00:00:22.800]   I just started going farther and farther down the, down the path of saying, Hey, like there's,
[00:00:22.800 --> 00:00:27.540]   we can make this team of four people, you know, behave like a team of 50 people.
[00:00:27.540 --> 00:00:31.880]   If we start to use ML more and more, you're listening to gradient descent, a show about
[00:00:31.880 --> 00:00:33.980]   machine learning in the real world.
[00:00:33.980 --> 00:00:36.040]   And I'm your host, Lucas Biewald.
[00:00:36.040 --> 00:00:40.420]   Chris Alvin is the director of machine learning at the Wikimedia Foundation.
[00:00:40.420 --> 00:00:45.200]   And before that he had a number of really interesting jobs, director of data science
[00:00:45.200 --> 00:00:50.320]   at Devoted Health, director of data science at Ushahidi, which is an open source nonprofit
[00:00:50.320 --> 00:00:55.380]   that did mapping and a project director at Frontline SMS.
[00:00:55.380 --> 00:01:01.360]   He's also a well-known educator on machine learning, the author of machine learning flashcards,
[00:01:01.360 --> 00:01:05.660]   machine learning with Python cookbook, and several fantastic machine learning tutorials.
[00:01:05.660 --> 00:01:09.160]   I'm super excited to talk to him today.
[00:01:09.160 --> 00:01:14.960]   Maybe we'll jump into, there's kind of a theme around, I guess, moderation and truth and
[00:01:14.960 --> 00:01:18.520]   security that I'm sure you think about a lot.
[00:01:18.520 --> 00:01:23.560]   And one question we got from Twitter was basically someone was wondering if Wikipedia's experimented
[00:01:23.560 --> 00:01:28.520]   with tools for moderators or kind of tools for educating disputes.
[00:01:28.520 --> 00:01:35.120]   I have to say, I've seen a lot of fighting in the comments on Wikipedia pages.
[00:01:35.120 --> 00:01:40.280]   I'm kind of always impressed that they resolve, but are there, I guess, special tools or algorithms
[00:01:40.280 --> 00:01:41.280]   that you'll use?
[00:01:41.280 --> 00:01:42.280]   No.
[00:01:42.280 --> 00:01:49.040]   And I mean, that's really sort of foundational to Wikipedia that it is, at the end of the
[00:01:49.040 --> 00:01:53.280]   day, it is a human project of humans deciding, like trying to get the truth.
[00:01:53.280 --> 00:01:54.280]   What is the perspective?
[00:01:54.280 --> 00:01:56.840]   Like what's that neutral perspective between the parties?
[00:01:56.840 --> 00:02:02.440]   And I have said that after joining the foundation, the new thing that I do is I care less about
[00:02:02.440 --> 00:02:03.440]   the Wikipedia page.
[00:02:03.440 --> 00:02:05.600]   I really care about what's called the talk page.
[00:02:05.600 --> 00:02:11.800]   So every page on Wikipedia has like a separate comment page where people are constantly discussing,
[00:02:11.800 --> 00:02:15.720]   like over and over again, discussing, debating, like finding new information, going back and
[00:02:15.720 --> 00:02:16.720]   forth.
[00:02:16.720 --> 00:02:22.440]   And so we've, you know, with things around disinformation, we've definitely been exploring
[00:02:22.440 --> 00:02:25.280]   areas of, for example, sock puppet protection.
[00:02:25.280 --> 00:02:29.600]   So sock puppets, if you have like lots of accounts and trying to build models that like
[00:02:29.600 --> 00:02:34.920]   help predict that around like dispute resolution, like at the end of the day, you know, if you
[00:02:34.920 --> 00:02:37.680]   see something on Wikipedia, we really want you to think, okay, cool.
[00:02:37.680 --> 00:02:41.000]   A human has like, at the end of the day, a human has decided this, a human has made this
[00:02:41.000 --> 00:02:43.060]   kind of decision.
[00:02:43.060 --> 00:02:47.540]   And so things like algorithmically sort of like making those decisions for people, that
[00:02:47.540 --> 00:02:52.640]   kind of stuff is an anathema to everything, which is why frankly, people love it so much.
[00:02:52.640 --> 00:02:53.640]   Right.
[00:02:53.640 --> 00:02:57.180]   And which is why doing machine learning in that environment is so interesting because
[00:02:57.180 --> 00:03:00.820]   you are trying to do things at scale with a human in the loop.
[00:03:00.820 --> 00:03:04.580]   You just have thousands of thousands of thousands of humans who are willing to help you out
[00:03:04.580 --> 00:03:05.580]   in the loop.
[00:03:05.580 --> 00:03:06.580]   Totally.
[00:03:06.580 --> 00:03:13.020]   And I guess another question along the same lines is someone was asking, you know, what
[00:03:13.020 --> 00:03:18.420]   are some of the most contentious Wikipedia articles and does your team ever get involved
[00:03:18.420 --> 00:03:21.860]   to kind of resolve edit wars in any way?
[00:03:21.860 --> 00:03:23.060]   Yeah.
[00:03:23.060 --> 00:03:27.420]   There's a lot of contentious pages across many, many languages.
[00:03:27.420 --> 00:03:33.540]   The interesting thing that I think people don't realize a lot is that I work for the
[00:03:33.540 --> 00:03:35.340]   Wikimedia Foundation.
[00:03:35.340 --> 00:03:42.180]   We are the nonprofit organization that helps sort of keep the infrastructure up.
[00:03:42.180 --> 00:03:46.020]   We fight legal battles for different Wikipedia communities.
[00:03:46.020 --> 00:03:50.840]   But each individual language of Wikipedia manages their own show with their own rules
[00:03:50.840 --> 00:03:53.620]   based on a common set of norms across all Wikipedias.
[00:03:53.620 --> 00:03:54.940]   But it is like their own show.
[00:03:54.940 --> 00:04:01.900]   Like Wikipedia, like English Wikipedia has an incredibly elaborate system of like dispute
[00:04:01.900 --> 00:04:05.880]   resolution, different levels of user access between the admins.
[00:04:05.880 --> 00:04:10.380]   Like there is a full volunteer organization in English Wikipedia that is managing those
[00:04:10.380 --> 00:04:11.500]   kind of things.
[00:04:11.500 --> 00:04:12.820]   And that's the same with other languages.
[00:04:12.820 --> 00:04:17.660]   So for us at the foundation, it is very critical that we actually don't get involved and jump
[00:04:17.660 --> 00:04:21.220]   in because our role is sort of the folks who are like one step back.
[00:04:21.220 --> 00:04:22.900]   We're like, we'll make the site better for you.
[00:04:22.900 --> 00:04:24.540]   We'll make things, your experience better.
[00:04:24.540 --> 00:04:26.460]   We'll recommend things that we think are interesting.
[00:04:26.460 --> 00:04:30.700]   We'll highlight, you know, we'll help you work faster as an editor using ML, but we're
[00:04:30.700 --> 00:04:35.140]   not going to jump in and say, hey, you know, Steve is right.
[00:04:35.140 --> 00:04:37.900]   And you know, like Jason was wrong in this particular article.
[00:04:37.900 --> 00:04:41.820]   But that is not our role of which I think if we played that role, we would be the most
[00:04:41.820 --> 00:04:43.380]   hated organization very quickly.
[00:04:43.380 --> 00:04:44.780]   Right, right, right.
[00:04:44.780 --> 00:04:51.140]   And I guess, you know, someone else was asking, you know, do you, is sort of the implications
[00:04:51.140 --> 00:04:54.140]   of ML kind of top of mind?
[00:04:54.140 --> 00:04:58.220]   Like I would imagine it's, you know, it's hard to be like really neutral with any kind
[00:04:58.220 --> 00:04:59.220]   of tool, right?
[00:04:59.220 --> 00:05:03.780]   Like do you ever feel like there's sort of implications for how your tooling works, even
[00:05:03.780 --> 00:05:06.500]   though you're really just supporting moderators?
[00:05:06.500 --> 00:05:11.780]   I would imagine that like, for example, subtle changes in how search works might actually
[00:05:11.780 --> 00:05:14.580]   really change, you know, like what content people are seeing, because there's such a
[00:05:14.580 --> 00:05:17.780]   high profile set of web pages.
[00:05:17.780 --> 00:05:24.500]   No, I think probably one of the key foundations of the team is the idea that any kind of ML
[00:05:24.500 --> 00:05:27.460]   that we do is not neutral at the end of the day.
[00:05:27.460 --> 00:05:34.700]   And so our gold standard when we are making models is that the model reflects the training
[00:05:34.700 --> 00:05:38.080]   data from the particular community that is served by that model.
[00:05:38.080 --> 00:05:44.860]   So for example, French Wikipedia wants a model that say, predicts the article quality, like
[00:05:44.860 --> 00:05:48.440]   if this article is really good or bad, help editors decide which articles they should
[00:05:48.440 --> 00:05:50.180]   really jump in and help in.
[00:05:50.180 --> 00:05:54.900]   We want to get that data from the French Wikipedia community, train it, train that model, and
[00:05:54.900 --> 00:05:58.920]   then serve it back to the French Wikipedia community and give that community the ability
[00:05:58.920 --> 00:06:02.220]   to actually manage and govern the use of that model in their system.
[00:06:02.220 --> 00:06:07.180]   And so what we're saying is like, hey, there is no neutrality here, but we will try to
[00:06:07.180 --> 00:06:12.620]   limit our ability to like, say, train something on English Wikipedia and then apply it to,
[00:06:12.620 --> 00:06:17.460]   you know, say Vietnamese Wikipedia by gathering the training data from that original community
[00:06:17.460 --> 00:06:18.860]   and then serving back.
[00:06:18.860 --> 00:06:23.540]   It's not possible all the times because, you know, some models have to be like, you need
[00:06:23.540 --> 00:06:26.580]   to be global scalable, there isn't like enough training data and that kind of stuff.
[00:06:26.580 --> 00:06:30.180]   But that is our gold standard that we go for and that we've done many, many times over
[00:06:30.180 --> 00:06:31.180]   the years.
[00:06:31.180 --> 00:06:32.180]   Got it.
[00:06:32.180 --> 00:06:35.220]   And I guess one other kind of question on this theme of moderation that somebody asked
[00:06:35.220 --> 00:06:40.380]   and I'm kind of curious about is what's the most common type of spam attack that you deal
[00:06:40.380 --> 00:06:41.380]   with?
[00:06:41.380 --> 00:06:46.020]   Sort of like adversarial problems that you come across on your different properties?
[00:06:46.020 --> 00:06:51.800]   Yeah, well, I mean, the most common one is someone like putting in like poop or swear
[00:06:51.800 --> 00:06:54.660]   words like randomly into articles.
[00:06:54.660 --> 00:06:58.660]   And so, you know, like detecting that through like the community has actually done a great
[00:06:58.660 --> 00:07:02.620]   job because I think people don't realize that English Wikipedia community and other Wikipedia
[00:07:02.620 --> 00:07:06.900]   communities actually have developed their own machine learning models like as bots that
[00:07:06.900 --> 00:07:09.420]   they deploy by themselves with no need from the foundation.
[00:07:09.420 --> 00:07:13.980]   We host them, but it is like theirs to like do whatever they want with.
[00:07:13.980 --> 00:07:19.260]   But the most common one is definitely like adding swear words in something, you know,
[00:07:19.260 --> 00:07:21.660]   as you can imagine.
[00:07:21.660 --> 00:07:28.900]   You know, the ones that are the most dangerous are definitely the ones that the attackers
[00:07:28.900 --> 00:07:30.900]   have a lot of resources.
[00:07:30.900 --> 00:07:36.220]   So one of the things that you quickly realize when you work here is that all of our models
[00:07:36.220 --> 00:07:37.660]   are open source.
[00:07:37.660 --> 00:07:38.800]   Everything we do is open source.
[00:07:38.800 --> 00:07:40.940]   You can see the whole thing.
[00:07:40.940 --> 00:07:43.460]   You can see my internal chat.
[00:07:43.460 --> 00:07:45.140]   You can see my ticketing system.
[00:07:45.140 --> 00:07:47.400]   Like my Jira is totally public.
[00:07:47.400 --> 00:07:49.220]   Like what I'm working on on a given day is public.
[00:07:49.220 --> 00:07:52.340]   I'm live streaming the work that I'm doing every single like other week or something
[00:07:52.340 --> 00:07:53.340]   like that.
[00:07:53.340 --> 00:07:59.500]   Like all this is open and every single article on adversarial, you know, like not adversarial
[00:07:59.500 --> 00:08:01.980]   like machine learning, but like adversarial attacks on machine learning systems.
[00:08:01.980 --> 00:08:05.300]   It's like, well, if you have the model or you could actually like use the prediction
[00:08:05.300 --> 00:08:08.820]   really, really quickly, you can start to figure out how to game the system because you have
[00:08:08.820 --> 00:08:09.820]   such exposure.
[00:08:09.820 --> 00:08:13.740]   We are exposing ourselves to that all the time by showing them exactly what's happening
[00:08:13.740 --> 00:08:16.740]   with the model, by giving them the training data.
[00:08:16.740 --> 00:08:22.380]   And you know, that is that sort of give and take that you sit where, okay, how do we,
[00:08:22.380 --> 00:08:26.540]   you know, work to sort of see how other people are behaving in the system in order to like
[00:08:26.540 --> 00:08:30.660]   detect any kind of problems while also making it that like we have, you know, all of our
[00:08:30.660 --> 00:08:34.580]   models, you could hit an API for free and just use, use as much as you want.
[00:08:34.580 --> 00:08:37.620]   As long as you don't crash the system, like you're, you're good to go.
[00:08:37.620 --> 00:08:39.340]   And so people are using it tons of times.
[00:08:39.340 --> 00:08:41.940]   They can download the model, they can download training data, they can run it locally, they
[00:08:41.940 --> 00:08:44.180]   can do whatever they want.
[00:08:44.180 --> 00:08:45.760]   But of course there's risk in that, right?
[00:08:45.760 --> 00:08:48.060]   Because there's no, people can see your entire hand.
[00:08:48.060 --> 00:08:51.060]   It's like playing poker where like you're showing your whole hand and they're not showing
[00:08:51.060 --> 00:08:52.060]   any of their hand.
[00:08:52.060 --> 00:08:59.460]   You're definitely at a disadvantage, but it is, you know, it is a trust-based activity
[00:08:59.460 --> 00:09:05.540]   that people who spend hours and hours and hours, you know, making changes to the site,
[00:09:05.540 --> 00:09:09.060]   writing new articles, finding some new, interesting facts, and then hunting down like where to
[00:09:09.060 --> 00:09:14.580]   put that in or sitting on those talk pages and debating and discussing like how to exactly
[00:09:14.580 --> 00:09:18.780]   like phrase a single sentence about some article, because it's really important to get that
[00:09:18.780 --> 00:09:19.780]   right.
[00:09:19.780 --> 00:09:24.300]   That only works if they can come to see my team and say, Hey, they're, they're doing
[00:09:24.300 --> 00:09:25.300]   everything.
[00:09:25.300 --> 00:09:26.300]   Like I can see what they're doing.
[00:09:26.300 --> 00:09:27.300]   I understand what they're doing.
[00:09:27.300 --> 00:09:30.340]   I understand like, you know, where they're coming from.
[00:09:30.340 --> 00:09:31.580]   And I can participate in that.
[00:09:31.580 --> 00:09:37.820]   That's like the only way we have anything because the worst case scenario would be that
[00:09:37.820 --> 00:09:41.540]   people thought that what we were doing was like a black box that you just couldn't see.
[00:09:41.540 --> 00:09:43.540]   And there was some mystery behind what it was.
[00:09:43.540 --> 00:09:44.980]   And we were just like, Oh, just trust us.
[00:09:44.980 --> 00:09:45.980]   Just trust us.
[00:09:45.980 --> 00:09:47.620]   You know don't trust us.
[00:09:47.620 --> 00:09:50.300]   Come and look, come and see, run the code yourself.
[00:09:50.300 --> 00:09:51.300]   Tell us we're wrong.
[00:09:51.300 --> 00:09:52.300]   We're Wikipedia.
[00:09:52.300 --> 00:09:56.220]   So like, we'll definitely, you know, invite changes all the time.
[00:09:56.220 --> 00:10:01.500]   What does it, what does it feel like working with that level of transparency?
[00:10:01.500 --> 00:10:04.940]   Like, you know, besides, I mean, I couldn't see how, you know, it really must keep you
[00:10:04.940 --> 00:10:11.340]   honest about around like, you know, like security holes and, and thinking really carefully around,
[00:10:11.340 --> 00:10:15.100]   you know, like not, not doing security through obscurity, but like, what's the experience
[00:10:15.100 --> 00:10:16.100]   like?
[00:10:16.100 --> 00:10:20.180]   I mean, I assume that your previous roles, you didn't live stream your work as you're
[00:10:20.180 --> 00:10:21.180]   doing it.
[00:10:21.180 --> 00:10:22.180]   Yeah.
[00:10:22.180 --> 00:10:23.180]   It's interesting.
[00:10:23.180 --> 00:10:29.140]   Cause I've worked in the nonprofit space and the startup space for a long time.
[00:10:29.140 --> 00:10:33.900]   And in both of those spaces that I've, Trish, she works, even when we were doing open source
[00:10:33.900 --> 00:10:35.700]   work, it was sort of off in a corner, right?
[00:10:35.700 --> 00:10:38.200]   And like, there wasn't really that many people who are paying attention to it.
[00:10:38.200 --> 00:10:40.620]   Or if you're at a startup, it's literally all IP.
[00:10:40.620 --> 00:10:44.740]   And so you're like deep in the bowels of the organization in the back, working on some
[00:10:44.740 --> 00:10:47.740]   algorithm that people hope will like help them raise money or something like that.
[00:10:47.740 --> 00:10:48.740]   But no one's going to see it.
[00:10:48.740 --> 00:10:50.100]   No one, you know, you're never going to publish it.
[00:10:50.100 --> 00:10:51.380]   There's never going to be a paper about it.
[00:10:51.380 --> 00:10:54.740]   It's just, you know, your secret sauce in the rear.
[00:10:54.740 --> 00:11:01.700]   And so at, you know, at Wikipedia, because we do everything so open, I have learned to
[00:11:01.700 --> 00:11:07.100]   lean in on the idea of being open with a large amount of humility.
[00:11:07.100 --> 00:11:15.060]   So just to give a real example we are going to start releasing a model cards.
[00:11:15.060 --> 00:11:19.820]   So an individual like page that describes every single model that we host.
[00:11:19.820 --> 00:11:23.220]   And we've been looking at, we've been sort of making prototypes and experimenting with
[00:11:23.220 --> 00:11:25.500]   them and the experiments are public.
[00:11:25.500 --> 00:11:28.820]   You can take a look at the experiment page and let's sort of like see what's happening.
[00:11:28.820 --> 00:11:31.460]   But some of the models are going to look embarrassing.
[00:11:31.460 --> 00:11:34.300]   If you're going to look and be like, wow, that's a really bad model.
[00:11:34.300 --> 00:11:36.540]   I can't believe you put that in production.
[00:11:36.540 --> 00:11:40.820]   And we just need to like, that is the only way to go in this scenario is to just say
[00:11:40.820 --> 00:11:42.540]   like, Hey, we are going to be open.
[00:11:42.540 --> 00:11:45.980]   We're not going to take offense to something that you say our model is like crappy, like
[00:11:45.980 --> 00:11:46.980]   come help us fix it.
[00:11:46.980 --> 00:11:52.500]   Like we will lean into all the humility that we can, because that is the only way to do
[00:11:52.500 --> 00:11:53.500]   this.
[00:11:53.500 --> 00:11:57.100]   The only way to do this is just to come in with a huge heaping pile of humility and openness
[00:11:57.100 --> 00:11:58.660]   and just let things go.
[00:11:58.660 --> 00:12:06.260]   It is weird and it is different because when you work on the team, you work on this sort
[00:12:06.260 --> 00:12:11.100]   of nexus between machine learning, which a lot of people are interested in and Wikipedia,
[00:12:11.100 --> 00:12:12.980]   which a lot of people are interested in.
[00:12:12.980 --> 00:12:17.700]   And so like, it's sort of like working under a spotlight in a sense.
[00:12:17.700 --> 00:12:22.100]   So like, I do live streams of myself working in the first few weeks, like a hundred people
[00:12:22.100 --> 00:12:26.840]   were showing up and they would just watch me like not know something, like not understand
[00:12:26.840 --> 00:12:31.220]   how something's working, not understand how my system's working.
[00:12:31.220 --> 00:12:36.340]   And then, you know, just, I mean, another example is I, there was, there was this bug
[00:12:36.340 --> 00:12:40.980]   report that I randomly saw that showed that like a huge percentage of the traffic of one
[00:12:40.980 --> 00:12:46.140]   of our data centers was because of one image, like one image was all the data.
[00:12:46.140 --> 00:12:48.140]   And it was something like flower or something like that.
[00:12:48.140 --> 00:12:49.700]   And I was just like, Oh, that's kind of cool.
[00:12:49.700 --> 00:12:50.700]   I'll tweet about it.
[00:12:50.700 --> 00:12:52.420]   And so I just, I like, I tweeted it.
[00:12:52.420 --> 00:12:57.220]   And then within 24 hours, it was like a hundred articles about this flower that was causing
[00:12:57.220 --> 00:12:59.440]   all this problems on Wikipedia.
[00:12:59.440 --> 00:13:03.920]   And then people kept on coming to the fabricator tickets, like the JIRA ticket that the engineers
[00:13:03.920 --> 00:13:05.240]   were working on to fix it.
[00:13:05.240 --> 00:13:08.440]   And it was like crashing fabricator because of so much traffic.
[00:13:08.440 --> 00:13:12.040]   And they were like sending in messages of support and all these comments and ideas of
[00:13:12.040 --> 00:13:13.040]   what they thought it was.
[00:13:13.040 --> 00:13:15.040]   And then all the engineers were like, just stop, just stop.
[00:13:15.040 --> 00:13:16.040]   We think we got it.
[00:13:16.040 --> 00:13:19.200]   It's like, stop posting comments.
[00:13:19.200 --> 00:13:21.080]   But you are like, you're, you're in the open.
[00:13:21.080 --> 00:13:22.080]   You're in the open.
[00:13:22.080 --> 00:13:26.120]   You're the public and you cannot be defensive with how you do it.
[00:13:26.120 --> 00:13:29.640]   Because I mean, if you're really defensive about it, it's probably not a great job.
[00:13:29.640 --> 00:13:30.640]   It's your problem.
[00:13:30.640 --> 00:13:31.640]   Probably won't be that enjoyable.
[00:13:31.640 --> 00:13:35.000]   Do you think you've had to develop a thicker skin?
[00:13:35.000 --> 00:13:39.960]   Like I know whenever, you know, I do anything that's very public, you know, mostly the feedback
[00:13:39.960 --> 00:13:43.960]   is positive, but I really feel the negative feedback much more.
[00:13:43.960 --> 00:13:48.320]   And it, I think it causes, you know, any kind of public thing we did to, I feel like a little
[00:13:48.320 --> 00:13:49.520]   tinge of stress.
[00:13:49.520 --> 00:13:54.000]   Like I kind of can't imagine if everything was like public invisible and people are watching
[00:13:54.000 --> 00:13:55.000]   it.
[00:13:55.000 --> 00:13:57.840]   Has it kind of changed your mindset at all or the way you work?
[00:13:57.840 --> 00:13:58.840]   Oh yeah.
[00:13:58.840 --> 00:14:06.640]   I think when I started, I think I had a reasonably thick, I think I was fine.
[00:14:06.640 --> 00:14:10.480]   You know, I had a regular thickness of skin.
[00:14:10.480 --> 00:14:11.800]   And I was like, I'll do fine in this role.
[00:14:11.800 --> 00:14:12.800]   What would you do possibly?
[00:14:12.800 --> 00:14:15.200]   And then you see what happens, right?
[00:14:15.200 --> 00:14:17.920]   Where like people don't like what you're working on.
[00:14:17.920 --> 00:14:20.440]   You think people don't think the foundation should exist.
[00:14:20.440 --> 00:14:22.880]   People don't think there should be machine learning in it.
[00:14:22.880 --> 00:14:26.120]   People think your model's wrong or dumb or stupid, or why would you do it?
[00:14:26.120 --> 00:14:28.640]   Or like, there's this particular problem or why aren't you working on this other thing
[00:14:28.640 --> 00:14:30.560]   or 10,000 things.
[00:14:30.560 --> 00:14:31.920]   And remember everything we do is public.
[00:14:31.920 --> 00:14:37.360]   So like someone can post a comment about a ticket from like 2014 and say, oh, this is
[00:14:37.360 --> 00:14:38.360]   stupid or whatever.
[00:14:38.360 --> 00:14:43.960]   And people can like take your code and say, oh, you know, like you get that all the time.
[00:14:43.960 --> 00:14:49.280]   It is something that I think everyone on the team just learns to be okay with.
[00:14:49.280 --> 00:14:54.680]   And I think the best people who do it are the people who just come in and as I was saying,
[00:14:54.680 --> 00:14:57.640]   just lean into the idea like, hey, it's okay.
[00:14:57.640 --> 00:15:01.320]   Like people like what we're doing, like, you know, to the most part, some people won't,
[00:15:01.320 --> 00:15:02.320]   that's okay.
[00:15:02.320 --> 00:15:04.860]   But there'll be people who just won't like it.
[00:15:04.860 --> 00:15:08.600]   And that's totally, there's nothing to do about that, right?
[00:15:08.600 --> 00:15:10.200]   There's no other way to operate.
[00:15:10.200 --> 00:15:16.160]   But yeah, there's definitely, you know, there's definitely times where you're like, oh my
[00:15:16.160 --> 00:15:20.040]   God, this is brutal.
[00:15:20.040 --> 00:15:22.920]   This person really doesn't like me.
[00:15:22.920 --> 00:15:28.240]   But you know, all of that pales in comparison to like the simple fact that like I get up
[00:15:28.240 --> 00:15:33.440]   every single morning and people pay me money to work on Wikipedia and all the other projects.
[00:15:33.440 --> 00:15:34.480]   That's what I do all day.
[00:15:34.480 --> 00:15:38.040]   Like all I do is I sit down and I'm like, oh, this would be a cool thing to do here.
[00:15:38.040 --> 00:15:39.400]   We should work on this.
[00:15:39.400 --> 00:15:40.400]   Let's change this up.
[00:15:40.400 --> 00:15:45.560]   Like, all that's all I do just make Wikipedia, like work on Wikidata, work on like Wikicommons,
[00:15:45.560 --> 00:15:51.320]   like all of the cool projects for all these people who volunteered, like volunteered thousands
[00:15:51.320 --> 00:15:52.880]   of hours to work on this stuff.
[00:15:52.880 --> 00:15:54.920]   And my salary is paid by donations.
[00:15:54.920 --> 00:15:59.200]   So like people are donating five, $10 to like make my salary, right?
[00:15:59.200 --> 00:16:02.120]   Like that is how I'm working on it.
[00:16:02.120 --> 00:16:06.360]   Once you put that more into sort of perspective, you're able to take a lot of peat.
[00:16:06.360 --> 00:16:09.880]   That makes sense.
[00:16:09.880 --> 00:16:13.040]   Do you find yourself getting distracted by the content?
[00:16:13.040 --> 00:16:15.280]   I mean, Wikipedia I find so fascinating.
[00:16:15.280 --> 00:16:19.320]   I would think if I was like working directly on it, I actually remember my first job, I
[00:16:19.320 --> 00:16:20.760]   was writing a search engine.
[00:16:20.760 --> 00:16:21.760]   We were practicing on Wikipedia.
[00:16:21.760 --> 00:16:25.360]   And I remember I just like every time I was editing the, or like monitoring the search
[00:16:25.360 --> 00:16:30.240]   results, I just go down these rabbit holes on whatever topic it was pulling up.
[00:16:30.240 --> 00:16:32.640]   It is, it is genuinely hard.
[00:16:32.640 --> 00:16:37.480]   And not just like the straight content, but all the layers underneath it.
[00:16:37.480 --> 00:16:43.280]   Because when you start to work on it, you realize like all these little decisions that
[00:16:43.280 --> 00:16:46.120]   were made around like, oh, how do we do licensing?
[00:16:46.120 --> 00:16:48.200]   Or like, what is the kind of ramifications of this?
[00:16:48.200 --> 00:16:52.480]   So for example, like we have like Wikimedia Commons, which is like all the images that
[00:16:52.480 --> 00:16:53.480]   we have.
[00:16:53.480 --> 00:16:54.960]   And it's like, oh, there's faces in the images.
[00:16:54.960 --> 00:16:57.760]   Like, is that like, what's the, why are there faces?
[00:16:57.760 --> 00:16:59.040]   Like why are faces allowed in these?
[00:16:59.040 --> 00:17:00.320]   Like what's the rabbit hole?
[00:17:00.320 --> 00:17:04.200]   And that's like been a huge multi-year discussion by these, you know, like these folks of what
[00:17:04.200 --> 00:17:06.960]   to do about that and if that's okay and that kind of stuff.
[00:17:06.960 --> 00:17:10.040]   And then you just look at the talk pages and you look at the discourse and there's just,
[00:17:10.040 --> 00:17:13.960]   there's so much, it is like a, you know, the classic iceberg diagram.
[00:17:13.960 --> 00:17:17.520]   There is so much that's all public, but just not that front page.
[00:17:17.520 --> 00:17:21.240]   So there'll be a page of like, you know, like Dalmatian puppies.
[00:17:21.240 --> 00:17:27.160]   And then there's like just a huge, massive like discussion of licenses and like behind
[00:17:27.160 --> 00:17:30.320]   the scenes of like how to do certain things.
[00:17:30.320 --> 00:17:35.600]   And I have definitely become very distractible, you know, because like there's like research
[00:17:35.600 --> 00:17:38.960]   comes out about really interesting ideas and I'm sort of constantly being pinged by like,
[00:17:38.960 --> 00:17:41.720]   oh, there's this cool thing about how to audit translate stuff.
[00:17:41.720 --> 00:17:45.400]   Or there's this cool idea of like, you know, how do we detect these particular stuff?
[00:17:45.400 --> 00:17:49.440]   Like maybe you should work on this and like trying to keep the team sort of focused on
[00:17:49.440 --> 00:17:54.360]   like pursuing just a few, just a few things to move forward is hard enough.
[00:17:54.360 --> 00:17:57.360]   But I definitely do like that.
[00:17:57.360 --> 00:18:01.120]   I can have Wikipedia open on my browser window forever.
[00:18:01.120 --> 00:18:05.720]   And it's like technically working, even though I'm like randomly scrolling, you know, like
[00:18:05.720 --> 00:18:11.360]   Prussian military history or something like super duper duper random like topic.
[00:18:11.360 --> 00:18:17.600]   And you have a favorite Wikipedia page or topic that I can look at after this interview?
[00:18:17.600 --> 00:18:18.600]   I do.
[00:18:18.600 --> 00:18:19.600]   I do.
[00:18:19.600 --> 00:18:22.000]   It is called perpetual stew.
[00:18:22.000 --> 00:18:27.520]   Perpetual stew is the idea of a bowl of stew that is never stopped cooking.
[00:18:27.520 --> 00:18:28.760]   So it is cooked forever.
[00:18:28.760 --> 00:18:32.760]   And the idea is you're constantly adding to the pot and as you're taking out from the
[00:18:32.760 --> 00:18:37.280]   pot and it's such like a crazy concept when you think about it, that you just have this
[00:18:37.280 --> 00:18:40.040]   like, you know, a hundred year old stew that you're doing.
[00:18:40.040 --> 00:18:42.000]   It seems a little disgusting.
[00:18:42.000 --> 00:18:43.000]   Is it good?
[00:18:43.000 --> 00:18:47.480]   See this is why, and the photo is amazing because it has like a whole fish photo, which
[00:18:47.480 --> 00:18:49.200]   is like someone just threw a whole fish.
[00:18:49.200 --> 00:18:55.080]   It's a weird, it's weird, but it is, it like, it is, that is not something that I would
[00:18:55.080 --> 00:18:58.240]   ever imagine, but yet it's, it's a cool idea, but there's another.
[00:18:58.240 --> 00:19:00.760]   That was a great answer that you just had like instantly.
[00:19:00.760 --> 00:19:05.600]   We didn't, we didn't spend all day looking at Wikipedia, literally every, all my conversations
[00:19:05.600 --> 00:19:08.220]   about Wikipedia, like all the time.
[00:19:08.220 --> 00:19:10.560]   It's like the images of it, the like different parts of it.
[00:19:10.560 --> 00:19:15.360]   So yeah, I definitely have a long, a long list of ones that I do.
[00:19:15.360 --> 00:19:16.560]   I think are great.
[00:19:16.560 --> 00:19:22.080]   I do think, you know, some of the ones that I have really appreciated have been the ones
[00:19:22.080 --> 00:19:23.520]   that are sort of in the news.
[00:19:23.520 --> 00:19:29.460]   Like I don't think I really appreciated how much work the volunteers do when something
[00:19:29.460 --> 00:19:31.000]   is like fast moving news.
[00:19:31.000 --> 00:19:35.960]   Like I remember during the US presidential election and I was like going to the page
[00:19:35.960 --> 00:19:37.840]   and like, there's all these procedures in place.
[00:19:37.840 --> 00:19:40.800]   They like, like the volunteers all on their own, like, like, you know, they locked down
[00:19:40.800 --> 00:19:42.240]   the page through this process.
[00:19:42.240 --> 00:19:45.200]   They, oh yeah, these kinds of edits go through, how do you make changes?
[00:19:45.200 --> 00:19:48.760]   How do we do the wording of this kind of stuff and all that, that happens like in the moment,
[00:19:48.760 --> 00:19:52.980]   like live which is just so cool to watch.
[00:19:52.980 --> 00:19:56.800]   And now whenever there's some kind of event, I like immediately go to like the relevant
[00:19:56.800 --> 00:20:00.520]   Wikipedia page and like go to the talk page and watch people like hash it out to like
[00:20:00.520 --> 00:20:02.520]   figure out how to work, which is just so cool.
[00:20:02.520 --> 00:20:03.520]   That's awesome.
[00:20:03.520 --> 00:20:07.920]   I mean, one of the, I think one of the things I was excited to talk to you about is actually
[00:20:07.920 --> 00:20:10.480]   the, the ML infrastructure and Wikipedia.
[00:20:10.480 --> 00:20:14.360]   Cause a lot of the real world people we talked to have to be a little bit, you know, cagey
[00:20:14.360 --> 00:20:17.720]   or vague about exactly, you know, what the problems are with the infrastructure is, but
[00:20:17.720 --> 00:20:20.600]   you've been, you know, you're so open about this stuff that I think we can really get
[00:20:20.600 --> 00:20:23.120]   into the nitty gritty.
[00:20:23.120 --> 00:20:24.120]   We are all open.
[00:20:24.120 --> 00:20:27.680]   But I guess before, before diving in and this is actually a question that somebody asked,
[00:20:27.680 --> 00:20:30.800]   but you know, I think it's a really good one to start with is, you know, what are the like
[00:20:30.800 --> 00:20:33.440]   important ML applications at Wikipedia?
[00:20:33.440 --> 00:20:36.240]   I mean, you mentioned some of them and you say some of them aren't even run by your team,
[00:20:36.240 --> 00:20:41.820]   but like just like on the top of your head, like what are the things going on using ML?
[00:20:41.820 --> 00:20:46.320]   So we do a lot of models that help editors.
[00:20:46.320 --> 00:20:49.900]   That's sort of like probably our main body of work.
[00:20:49.900 --> 00:20:55.600]   So this would be things like, for example, predict if a particular edit is we think it's
[00:20:55.600 --> 00:21:00.200]   a productive edit or not, or whether we think it's like a damaging edit or not.
[00:21:00.200 --> 00:21:06.560]   And the idea is not to make changes to Wikipedia ourselves, but to flag it for editors in the
[00:21:06.560 --> 00:21:10.040]   UI, literally the UI changes that they can say, oh, okay, cool.
[00:21:10.040 --> 00:21:13.260]   Like I should go, you know, I should go deal with this because this, this edit is probably
[00:21:13.260 --> 00:21:14.260]   bad.
[00:21:14.260 --> 00:21:18.300]   So I can skip this particular edit and I can go to this other, sort of prioritize work.
[00:21:18.300 --> 00:21:23.060]   We also are working on some things that we call structured tasks.
[00:21:23.060 --> 00:21:28.100]   The idea is that there are many ways to participate in, in Wikipedia.
[00:21:28.100 --> 00:21:32.820]   And one of the hardest barriers is that you try to get your first edit in and it's like
[00:21:32.820 --> 00:21:36.620]   instantly rejected because it like, you know, fails some like, you know, long established
[00:21:36.620 --> 00:21:38.820]   rule about how things should go.
[00:21:38.820 --> 00:21:43.880]   And so one thing we've been doing with structured tasks is like, can we use ML to recommend
[00:21:43.880 --> 00:21:46.400]   edits that we think will like pass, right?
[00:21:46.400 --> 00:21:51.520]   So like sort of like an easy mode and they might be something simple like grammar, or
[00:21:51.520 --> 00:21:54.600]   they might be the one that we're working on right now is like a link.
[00:21:54.600 --> 00:21:58.280]   So like whether or not, is this word a link to another, like, is this word like a link
[00:21:58.280 --> 00:21:59.280]   to another article?
[00:21:59.280 --> 00:22:00.280]   Like, should that be true?
[00:22:00.280 --> 00:22:04.360]   And so we'll like highlight the word and then highlight the sort of like where we think
[00:22:04.360 --> 00:22:08.320]   the art, it should be pointing to, and then ask them like, is this right or not?
[00:22:08.320 --> 00:22:12.000]   And then if they say, yeah, it becomes an edit that gets just, you know, pushed to quote
[00:22:12.000 --> 00:22:14.240]   unquote production.
[00:22:14.240 --> 00:22:19.240]   But that's not like our, our big focus is to make that editor and reader experience
[00:22:19.240 --> 00:22:22.040]   better using ML.
[00:22:22.040 --> 00:22:25.960]   And you know, there's other things that we do, like we predict the topic of the article
[00:22:25.960 --> 00:22:28.520]   and we look at sock puppet stuff.
[00:22:28.520 --> 00:22:33.320]   But the big one is trying to make editors experience better.
[00:22:33.320 --> 00:22:35.600]   And do you build separate models for every language?
[00:22:35.600 --> 00:22:40.680]   Or is this kind of all baked together as like a single model?
[00:22:40.680 --> 00:22:45.520]   We traditionally do one model per language.
[00:22:45.520 --> 00:22:53.540]   Right now, I'm, I'm looking at a kind of shift where we end up doing one model per language
[00:22:53.540 --> 00:22:59.720]   for every single model that we can, but then doing a language agnostic model for everything
[00:22:59.720 --> 00:23:00.720]   else.
[00:23:00.720 --> 00:23:04.320]   So basically, I mean, you can imagine of the 300 languages that we would support, there
[00:23:04.320 --> 00:23:08.320]   would be a language agnostic model that would work for all of them, but not as good as a
[00:23:08.320 --> 00:23:11.520]   language specific model of where we can.
[00:23:11.520 --> 00:23:17.320]   And it's, it's because gathering the training data from each individual community is really,
[00:23:17.320 --> 00:23:18.320]   it's really time consuming.
[00:23:18.320 --> 00:23:23.400]   And so you can't do that, you know, 300 times with a, with a really small team.
[00:23:23.400 --> 00:23:28.160]   And so trying to like do that balance where we can do that global coverage, but believe
[00:23:28.160 --> 00:23:32.160]   that the gold standard should be an individual language based model.
[00:23:32.160 --> 00:23:33.600]   It doesn't happen for everything.
[00:23:33.600 --> 00:23:39.120]   So for example, when recommending whether a link is, or rather recommending whether
[00:23:39.120 --> 00:23:42.160]   like a word is a link or not for like that link recommender, which I just described,
[00:23:42.160 --> 00:23:44.280]   like we don't need to have a language specific model for that.
[00:23:44.280 --> 00:23:45.560]   We can take advantage of that.
[00:23:45.560 --> 00:23:48.920]   But I have, I know one of the questions that someone asked on Twitter was like, what am
[00:23:48.920 --> 00:23:50.800]   I interested in NLP?
[00:23:50.800 --> 00:23:55.320]   And language agnostic models is the thing that I'm really interested in.
[00:23:55.320 --> 00:24:00.320]   Because when you start to do one model per language, you run into like a scalability
[00:24:00.320 --> 00:24:01.320]   problem pretty quick.
[00:24:01.320 --> 00:24:09.360]   Like how do you maintain with fresh training data, with monitoring, with all that stuff
[00:24:09.360 --> 00:24:14.880]   of like a huge breadth of languages, well beyond the languages spoke on the team.
[00:24:14.880 --> 00:24:17.560]   And so like, how do you like maintain that?
[00:24:17.560 --> 00:24:21.320]   And so having some kind of idea of like, okay, cool, let's do like a mix where we'll have
[00:24:21.320 --> 00:24:23.640]   like some models that are just across all languages.
[00:24:23.640 --> 00:24:27.760]   And then, but our gold standard whenever we could is to make like one model per individual
[00:24:27.760 --> 00:24:28.760]   language.
[00:24:28.760 --> 00:24:33.720]   And then what we want, right, is that the community governs the Wikimedia Foundation.
[00:24:33.720 --> 00:24:35.720]   Like they're the ones who select members to the board.
[00:24:35.720 --> 00:24:39.520]   And then like, you know, the board decides what like the priorities of the organization
[00:24:39.520 --> 00:24:40.520]   are.
[00:24:40.520 --> 00:24:42.040]   And that sort of trickles down to me.
[00:24:42.040 --> 00:24:47.720]   For us, like we want communities to feel that they have the power to decide what they want
[00:24:47.720 --> 00:24:48.720]   to do with the model.
[00:24:48.720 --> 00:24:52.880]   So like if someone, if French Wikipedia is like, hey, we want a model that predicts,
[00:24:52.880 --> 00:24:54.600]   you know, the edit quality, great.
[00:24:54.600 --> 00:24:58.080]   We'll like help them and get training data and put that model out.
[00:24:58.080 --> 00:25:00.920]   If they then decide that they don't want that model anymore, we'll turn it off.
[00:25:00.920 --> 00:25:01.920]   Right.
[00:25:01.920 --> 00:25:04.720]   Because the goal is that they, you know, we're here to support them and like their stuff.
[00:25:04.720 --> 00:25:07.840]   They're the ones who are putting like the huge amount of like hours and effort and time
[00:25:07.840 --> 00:25:09.480]   unpaid to make this stuff.
[00:25:09.480 --> 00:25:13.040]   We're just trying to like make their lives a little bit better.
[00:25:13.040 --> 00:25:16.800]   I would imagine you have probably more requests than you can really feel like.
[00:25:16.800 --> 00:25:21.280]   How do you prioritize all the requests that come in for different models and also improving
[00:25:21.280 --> 00:25:22.560]   existing models?
[00:25:23.040 --> 00:25:30.480]   Yeah, a lot of times what is really hard is distinguishing different types of requests.
[00:25:30.480 --> 00:25:37.360]   So one of the things that you, that happens a lot is that volunteers have like really
[00:25:37.360 --> 00:25:39.040]   spiky participation.
[00:25:39.040 --> 00:25:40.320]   This is just sort of natural, right?
[00:25:40.320 --> 00:25:44.680]   Like they do a lot of work on something and then they get a new job.
[00:25:44.680 --> 00:25:46.480]   And so they kind of like disappear with six months.
[00:25:46.480 --> 00:25:49.960]   Then they like, and then they come and like do a lot of participation again.
[00:25:49.960 --> 00:25:50.960]   Right.
[00:25:50.960 --> 00:25:54.440]   And that's really how volunteering works because you know, like you're volunteering, you have
[00:25:54.440 --> 00:25:57.840]   other things, school starts, you have a new kid, you decide that you're bored of doing
[00:25:57.840 --> 00:26:00.040]   it, you take them another hobby and that kind of stuff.
[00:26:00.040 --> 00:26:07.520]   And so that kind of like really spiky participation means that, you know, when I took over the
[00:26:07.520 --> 00:26:12.320]   team, like we talked about it a lot and we decided that what we wanted to do is that
[00:26:12.320 --> 00:26:17.840]   if we ended up hosting anything on the foundation servers, that we will own it.
[00:26:17.840 --> 00:26:22.280]   So like if someone comes in and like really works like, you know, like with us and helps
[00:26:22.280 --> 00:26:23.600]   us build a model and that kind of stuff.
[00:26:23.600 --> 00:26:25.520]   And then they, you know, they go off and do something else.
[00:26:25.520 --> 00:26:30.320]   Like we will continue to maintain that model in perpetuity and to keep on like running
[00:26:30.320 --> 00:26:31.320]   with it.
[00:26:31.320 --> 00:26:36.560]   And that means that you have to be selective of what you take because you can't take every
[00:26:36.560 --> 00:26:40.440]   single thing that people are asking for if you're going to own everything that, that,
[00:26:40.440 --> 00:26:41.440]   that comes in.
[00:26:41.440 --> 00:26:45.560]   And so there is a process of sort of deliberating like what that would be and whatnot.
[00:26:45.560 --> 00:26:48.760]   And you know, there's other ways that people can host models at the foundation.
[00:26:48.760 --> 00:26:53.440]   So like if any, this is a technical podcast, people are probably familiar with AWS and
[00:26:53.440 --> 00:26:54.440]   EC2.
[00:26:54.440 --> 00:26:59.780]   Like we, we run our own EC2 instance essentially which is, you know, which you call cloud services
[00:26:59.780 --> 00:27:03.040]   where like people can actually go and like host their own stuff.
[00:27:03.040 --> 00:27:05.600]   So like if they wanted to host their own things on our servers, that's totally fine and they
[00:27:05.600 --> 00:27:06.600]   can do it through there.
[00:27:06.600 --> 00:27:12.080]   But when it comes to like my team, we know that we need to own something because part
[00:27:12.080 --> 00:27:18.160]   of our idea of what it would look like to do community-based like public ethical ML
[00:27:18.160 --> 00:27:22.000]   is, is ownership of us saying like, Hey, we screwed up that this model is bad.
[00:27:22.000 --> 00:27:23.520]   We screwed up that this model is harmful.
[00:27:23.520 --> 00:27:27.520]   And the only way we can do that is if we actually own the model and we understand how it works
[00:27:27.520 --> 00:27:29.160]   and that kind of stuff.
[00:27:29.160 --> 00:27:32.840]   And evaluating models that get submitted or requests for models and that kind of stuff
[00:27:32.840 --> 00:27:37.320]   is like a real, a real challenge of which is like unique to the foundation in a weird
[00:27:37.320 --> 00:27:38.320]   way.
[00:27:38.320 --> 00:27:44.760]   So I guess how many, how many models are you owning, like running and at any given time?
[00:27:44.760 --> 00:27:49.840]   So we have, I think 120 models right now.
[00:27:49.840 --> 00:27:55.760]   And maybe like five that are currently being, that are being built.
[00:27:55.760 --> 00:28:01.720]   We stopped building new models for quite a while over the last year because we're switching
[00:28:01.720 --> 00:28:06.040]   infrastructures for model deployment, which we could, we could talk about.
[00:28:06.040 --> 00:28:11.200]   But it was, there was definitely this moment where we were like this, the current infrastructure,
[00:28:11.200 --> 00:28:12.920]   which has lasted us a really long time.
[00:28:12.920 --> 00:28:19.000]   And you know, is, is sort of like what got ML at the Wikimedia foundation off the ground
[00:28:19.000 --> 00:28:20.960]   is like not serving us anymore.
[00:28:20.960 --> 00:28:23.560]   We need to go back and figure out what to do.
[00:28:23.560 --> 00:28:28.680]   And because of the nuances of, of the foundation, the foundation is a strong believer of privacy
[00:28:28.680 --> 00:28:32.160]   and of open source, which means we don't use cloud hosted services.
[00:28:32.160 --> 00:28:36.200]   You're not on AWS, like, except for like very, very small things.
[00:28:36.200 --> 00:28:37.960]   We're not on Google cloud compute.
[00:28:37.960 --> 00:28:42.040]   We are like on our own servers in our own data center or not our own data center, but
[00:28:42.040 --> 00:28:44.240]   in our own racks in the data center.
[00:28:44.240 --> 00:28:50.600]   And so building out a new model deployment system was literally starting off with like,
[00:28:50.600 --> 00:28:53.040]   what are the specs of the servers that you want?
[00:28:53.040 --> 00:28:55.520]   Like how many sticks of Ram?
[00:28:55.520 --> 00:28:59.600]   And you know, just to show you like the level, like I had conversations about how the racking
[00:28:59.600 --> 00:29:01.720]   was going to go.
[00:29:01.720 --> 00:29:07.160]   I, we bought a GPU to try to test if we could, you know, use it in our, in our server.
[00:29:07.160 --> 00:29:11.240]   And like, I got this photo from the, you know, the person in the data center, like the Wikimedia
[00:29:11.240 --> 00:29:12.800]   foundation employee in the data center.
[00:29:12.800 --> 00:29:13.880]   He's like trying to get the photo.
[00:29:13.880 --> 00:29:18.600]   He's trying to install the GPU into the server blade and he can't, it doesn't fit.
[00:29:18.600 --> 00:29:20.720]   And he's like showing me in the photo that it doesn't fit.
[00:29:20.720 --> 00:29:25.000]   Like that's the level of, of that, like bare metal up.
[00:29:25.000 --> 00:29:30.120]   Which as a technical challenge is really fun.
[00:29:30.120 --> 00:29:35.380]   It is, it is, I've taken a lot of appreciation that the foundation actually like care so
[00:29:35.380 --> 00:29:39.360]   much about privacy that it is like unwilling to give up anything.
[00:29:39.360 --> 00:29:43.400]   It is very, very big, you know, it is, it is funny because there was a ton of SREs at
[00:29:43.400 --> 00:29:44.400]   the foundation.
[00:29:44.400 --> 00:29:47.640]   Like most of the like tech stuff is by SREs because you constantly need to have these
[00:29:47.640 --> 00:29:51.680]   people like maintaining the systems and building the systems at that low level.
[00:29:51.680 --> 00:29:53.520]   But yeah.
[00:29:53.520 --> 00:29:58.200]   So what, what, what are these models?
[00:29:58.200 --> 00:30:02.080]   Like a lot of the questions that we got were actually like, are, is, you know, is Wikimedia
[00:30:02.080 --> 00:30:03.400]   using deep learning?
[00:30:03.400 --> 00:30:06.680]   I guess I should just ask that, but you know, I actually want to be even more specific of
[00:30:06.680 --> 00:30:10.960]   like, can you tell, describe like, you know, what, what, you know, what frameworks are
[00:30:10.960 --> 00:30:12.480]   you building these models in?
[00:30:12.480 --> 00:30:13.840]   What are they like?
[00:30:13.840 --> 00:30:14.840]   Yeah.
[00:30:14.840 --> 00:30:19.240]   So right now we have a lot of models in scikit-learn.
[00:30:19.240 --> 00:30:21.680]   That was sort of the initial set of models.
[00:30:21.680 --> 00:30:27.400]   These are the ones that are predicting article quality and the quality of an edit or like
[00:30:27.400 --> 00:30:30.680]   the topic of that kind of stuff.
[00:30:30.680 --> 00:30:36.160]   We've started to move towards more deep learning based models, particularly around like computer
[00:30:36.160 --> 00:30:42.640]   vision and NLP, because there's just like big advantages to, you know, using that.
[00:30:42.640 --> 00:30:47.160]   And so we, there was, you know, right as I joined the foundation they were setting up
[00:30:47.160 --> 00:30:50.720]   some GPUs in like, you know, cause we have to use our own stack.
[00:30:50.720 --> 00:30:55.880]   So like literally installing the GPUs in the machines and starting to work on them there.
[00:30:55.880 --> 00:31:00.000]   But you know, as we move forward, you know, I know we're using fast text for some model,
[00:31:00.000 --> 00:31:04.360]   which is that Facebook library.
[00:31:04.360 --> 00:31:10.360]   For me, you know, as the person who's, who's sort of, you know, herding the cats in this,
[00:31:10.360 --> 00:31:18.120]   in this instance, I have become very interested in simple models because the goal of what
[00:31:18.120 --> 00:31:20.680]   we do at the foundation is accessibility.
[00:31:20.680 --> 00:31:22.640]   You should be able to understand what we're doing.
[00:31:22.640 --> 00:31:24.440]   Like not every single person, right.
[00:31:24.440 --> 00:31:28.120]   It's okay if like not everyone who doesn't, you know, work in ML understands what we're
[00:31:28.120 --> 00:31:29.120]   doing.
[00:31:29.120 --> 00:31:33.760]   But my goal is that if you see a model that we're using, here's the foundation's model
[00:31:33.760 --> 00:31:39.520]   for detecting whether or not, you know, like this piece of text is a link or something
[00:31:39.520 --> 00:31:43.160]   that you can go to an open source page on GitLab.
[00:31:43.160 --> 00:31:45.860]   You can see the code that's plainly documented.
[00:31:45.860 --> 00:31:49.200]   You can see the link to the data that you use to train it.
[00:31:49.200 --> 00:31:50.360]   You understand what's happening.
[00:31:50.360 --> 00:31:55.840]   Cause it's not insanely, you know, like it isn't so insanely like complex that it's impossible
[00:31:55.840 --> 00:31:57.800]   to access.
[00:31:57.800 --> 00:31:59.800]   And then you can fix it.
[00:31:59.800 --> 00:32:00.800]   Like you can make it better.
[00:32:00.800 --> 00:32:01.920]   You can throw in improvements.
[00:32:01.920 --> 00:32:02.920]   That's what I want.
[00:32:02.920 --> 00:32:04.440]   I want people to see what we're doing.
[00:32:04.440 --> 00:32:09.080]   And so I am less interested in the most technical like solution.
[00:32:09.080 --> 00:32:12.880]   I'm definitely more in the sort of practical, like what is the sort of lowest common bar
[00:32:12.880 --> 00:32:14.240]   that that does it.
[00:32:14.240 --> 00:32:17.680]   That said, there's some things that are, you know, frankly, particularly with NLP that
[00:32:17.680 --> 00:32:21.560]   I feel are just really complex and we were just talking this morning about some models
[00:32:21.560 --> 00:32:27.960]   using BERT to try to like basically replace some of the models that we're using scikit-learn
[00:32:27.960 --> 00:32:30.960]   models on to like, she'd actually use BERT to like throw in there to make it better.
[00:32:30.960 --> 00:32:36.240]   So there is value, there is value in complexity, but you know, it goes back to the idea that
[00:32:36.240 --> 00:32:39.760]   like, I don't, I don't want people to think that we have a secret sauce.
[00:32:39.760 --> 00:32:45.520]   I want people to think that we're like, you know, a set of like, you know, hopefully somewhat
[00:32:45.520 --> 00:32:50.560]   humble people building out in the open and you can come and help us and participate and
[00:32:50.560 --> 00:32:52.440]   challenge us and ask those questions.
[00:32:52.440 --> 00:32:55.320]   And so the more accessible we use, the better.
[00:32:55.320 --> 00:32:58.280]   If we end up using like a proprietary system to make it that, I mean, that would never
[00:32:58.280 --> 00:33:01.320]   happen, but the reason that would never happen is you'd never be able to like trust us that
[00:33:01.320 --> 00:33:02.320]   it was true.
[00:33:02.320 --> 00:33:03.680]   It would just like work or not work.
[00:33:03.680 --> 00:33:04.680]   And you'd have to believe it.
[00:33:04.680 --> 00:33:06.560]   We want you to go and take it.
[00:33:06.560 --> 00:33:08.920]   So we are moving into deep learning.
[00:33:08.920 --> 00:33:14.520]   I actually have a big ask for GPUs of which it is really hard to buy GPUs in case anybody
[00:33:14.520 --> 00:33:15.520]   has ever been in that world.
[00:33:15.520 --> 00:33:18.080]   It's super hard to do that.
[00:33:18.080 --> 00:33:23.960]   So we're sort of out there hunting around for GPUs that fit into our servers at this,
[00:33:23.960 --> 00:33:26.560]   at this moment.
[00:33:26.560 --> 00:33:29.160]   You mentioned an infrastructure change.
[00:33:29.160 --> 00:33:34.080]   Can you talk about, you know, what, what prompted that, like what was happening and what infrastructure
[00:33:34.080 --> 00:33:35.560]   you moved to?
[00:33:35.560 --> 00:33:44.440]   So our system and how it's run since the beginning was on what's called ORS, which is
[00:33:44.440 --> 00:33:47.840]   our homegrown model management system.
[00:33:47.840 --> 00:33:55.240]   So before there was any kind of, you know, before there was Kubeflow, MLflow, or before
[00:33:55.240 --> 00:33:59.320]   MLOps was a thing, there was people at the foundation that were building essentially
[00:33:59.320 --> 00:34:02.600]   those functionalities from scratch.
[00:34:02.600 --> 00:34:10.880]   And it is 18 servers split across two data centers, one in Virginia and one set in Virginia,
[00:34:10.880 --> 00:34:13.000]   one set in Texas.
[00:34:13.000 --> 00:34:19.800]   And it, there was, there was issues around, you know, one of the things that it does is
[00:34:19.800 --> 00:34:24.640]   it is for deploying a very certain type of model, like particularly edit quality ones
[00:34:24.640 --> 00:34:26.000]   and that kind of stuff.
[00:34:26.000 --> 00:34:28.120]   And it's really paired with the training system.
[00:34:28.120 --> 00:34:32.640]   So the training system and the deployment system are like very, very, very interconnected,
[00:34:32.640 --> 00:34:37.440]   which means that you couldn't add, say a deep learning model in there because it wasn't
[00:34:37.440 --> 00:34:40.960]   part of the training system, which is also a home grown system.
[00:34:40.960 --> 00:34:48.240]   The big one for me as sort of the director of the project was that how, because it doesn't
[00:34:48.240 --> 00:34:54.200]   use serverless infrastructure, there is a hard memory requirement.
[00:34:54.200 --> 00:35:00.100]   So if your model is, I think the machines have 128 gigabytes of memory, each of them.
[00:35:00.100 --> 00:35:05.420]   And if your model is two gigabytes, you now only have 126 gigabytes of memory left.
[00:35:05.420 --> 00:35:09.000]   So there's like literally, no matter how much that model is used, it could be used, you
[00:35:09.000 --> 00:35:11.880]   know, every single second, it could be used once a month.
[00:35:11.880 --> 00:35:18.320]   It is like a, like a finite amount of resource, which is very problematic for us because so
[00:35:18.320 --> 00:35:21.520]   many, as we were talking about, so many people come to us and are interested in deploying
[00:35:21.520 --> 00:35:27.000]   a model or interested in sort of how we do things, which means that we need to, in order
[00:35:27.000 --> 00:35:31.960]   to participate with those people at a real level, we need to not so much care if something
[00:35:31.960 --> 00:35:33.160]   is really used or not, right?
[00:35:33.160 --> 00:35:36.960]   If someone comes in and they say, Hey, I have a great idea for this project.
[00:35:36.960 --> 00:35:39.760]   And we work on up with them and we create a model and then we deploy it.
[00:35:39.760 --> 00:35:43.520]   We need to be fine with it being dormant for months.
[00:35:43.520 --> 00:35:46.560]   And maybe it's only used once a year, or maybe it's used all the time and that's okay.
[00:35:46.560 --> 00:35:50.440]   But that you, when you reach that finite level of like, literally you're running out of RAM
[00:35:50.440 --> 00:35:54.120]   and every single time you need to like, you're like, it's a zero sum game where you're using
[00:35:54.120 --> 00:35:57.680]   more and more of the physical RAM to hold the models in memory.
[00:35:57.680 --> 00:35:58.680]   Yeah.
[00:35:58.680 --> 00:35:59.680]   It just got, it got too far.
[00:35:59.680 --> 00:36:08.080]   And what I think happened is that the, this was sort of a pioneer in the space of, of
[00:36:08.080 --> 00:36:09.720]   sort of ML ops.
[00:36:09.720 --> 00:36:13.800]   And now what has happened is there's so many great projects out there that are doing ML
[00:36:13.800 --> 00:36:16.540]   ops that there's like such a value to switching over.
[00:36:16.540 --> 00:36:22.560]   So we've moved to setting up what we call Liftwing, which is a Kubeflow instance on
[00:36:22.560 --> 00:36:25.000]   a new Kubernetes cluster that we do it in.
[00:36:25.000 --> 00:36:30.240]   Kubeflow is a open source project for ML ops on, on Kubeflow.
[00:36:30.240 --> 00:36:34.200]   And there's so many great advantages of that, that we've been taking in, for example, like
[00:36:34.200 --> 00:36:35.360]   the custom libraries.
[00:36:35.360 --> 00:36:40.400]   So we had a researcher, he used fast text and didn't tell us, cause like, we just hadn't
[00:36:40.400 --> 00:36:42.120]   made that communication and it was fine.
[00:36:42.120 --> 00:36:43.120]   Right.
[00:36:43.120 --> 00:36:45.080]   Like he gave us the same, we were like, we've never seen fast text before, but Hey, we'll
[00:36:45.080 --> 00:36:46.800]   build the, we'll build the server for it.
[00:36:46.800 --> 00:36:49.040]   Like we'll, you know, build the service for it and the thing.
[00:36:49.040 --> 00:36:53.640]   And it'll run, it means you could run deep learning models or TensorFlow, PyTorch, or
[00:36:53.640 --> 00:36:55.840]   whatever you want to do in that system.
[00:36:55.840 --> 00:36:57.160]   Everything's all nicely dockerized.
[00:36:57.160 --> 00:37:02.160]   So like we've been dockerizing our models that we have on oars and just dockering and
[00:37:02.160 --> 00:37:05.040]   then move the docker file over to the new system.
[00:37:05.040 --> 00:37:09.200]   There is way more stored analytics around things are working.
[00:37:09.200 --> 00:37:12.040]   We want to pair it with a full training suite.
[00:37:12.040 --> 00:37:15.360]   So right now we're sort of focused on model deployment, but we want to get to the point
[00:37:15.360 --> 00:37:17.520]   where we're doing nightly retrainings.
[00:37:17.520 --> 00:37:20.480]   So that would mean that we could do things like shadow models.
[00:37:20.480 --> 00:37:24.320]   So a prediction comes in, we serve it to two versions of the model, sort of compare the
[00:37:24.320 --> 00:37:30.200]   stats of like how it's doing sort of an AB test, except for one of the, one of the, I
[00:37:30.200 --> 00:37:34.440]   guess the a actually serves back a prediction to the user.
[00:37:34.440 --> 00:37:38.800]   But just a, you know, a huge amount of taking advantage of that modern infrastructure.
[00:37:38.800 --> 00:37:41.720]   And it wasn't because, you know, like when this was started at the foundation, there
[00:37:41.720 --> 00:37:43.680]   just wasn't this infrastructure and now there is.
[00:37:43.680 --> 00:37:49.480]   And so like taking a step back and building that out has been really fun.
[00:37:49.480 --> 00:37:56.360]   I will completely admit that it is somewhat terrifying to start at a job, look around
[00:37:56.360 --> 00:37:59.200]   and say, Hey, I think we need to build the infrastructure from scratch, which becomes
[00:37:59.200 --> 00:38:03.520]   like a planning document, which becomes a budget line, which becomes like server specs,
[00:38:03.520 --> 00:38:07.840]   which becomes like a server box deployed to like a data center, like the plugged in, which
[00:38:07.840 --> 00:38:12.920]   becomes like hiring SREs, which becomes like slowly configuring the system, which becomes
[00:38:12.920 --> 00:38:15.480]   like running through a thousand problems.
[00:38:15.480 --> 00:38:18.840]   But it was, I mean, right now, like, like where are we right now in that system?
[00:38:18.840 --> 00:38:25.360]   Yes, no, two days ago, we got our hello world that we served a prediction using the system,
[00:38:25.360 --> 00:38:31.960]   which was so cool to see after, after all that, all that work.
[00:38:31.960 --> 00:38:36.640]   But that's really the, you know, the, the fun part about the foundation is that you're
[00:38:36.640 --> 00:38:40.640]   doing something out in the open and you're doing something like, frankly, from a technical
[00:38:40.640 --> 00:38:44.800]   perspective from bare metal, like from bare metal all the way up, that's how you're figuring
[00:38:44.800 --> 00:38:45.800]   it out.
[00:38:45.800 --> 00:38:49.200]   And sometimes you hate your life for it because you're like, you know, what's easy AWS, AWS
[00:38:49.200 --> 00:38:50.200]   is easy.
[00:38:50.200 --> 00:38:53.320]   Look at all these wonderful services of which they provide people.
[00:38:53.320 --> 00:38:58.040]   But at the same time, having the control to sort of own the system from scratch and know
[00:38:58.040 --> 00:39:02.240]   that, you know, people's privacy is protected, that we have control over everything, where
[00:39:02.240 --> 00:39:06.060]   any of the data goes, any of that kind of stuff, which means that people can participate
[00:39:06.060 --> 00:39:10.200]   in the projects with, you know, with feeling safe that they're not going to be exposed
[00:39:10.200 --> 00:39:14.120]   because they edited an LGBTQ article or something like that.
[00:39:14.120 --> 00:39:19.280]   Like, you know, we have that ability, which is so nice and it feels so good to have that.
[00:39:19.280 --> 00:39:25.080]   But it is a, it is a going to be a long process of us getting from, you know, moving on, like
[00:39:25.080 --> 00:39:29.120]   we're going to build a second cluster, which we're going to be using mostly for training.
[00:39:29.120 --> 00:39:35.440]   So in our architecture, we're trying to split up one Kubeflow instance for a model serving
[00:39:35.440 --> 00:39:40.740]   and sort of keep that with really good uptime and keep that really, really simple.
[00:39:40.740 --> 00:39:43.840]   And then we're having a second one, which has access to the data center, which is more
[00:39:43.840 --> 00:39:45.920]   like if it goes down for a day, that's fine.
[00:39:45.920 --> 00:39:46.920]   Right.
[00:39:46.920 --> 00:39:48.000]   And so we can be a little bit more experimental.
[00:39:48.000 --> 00:39:49.620]   We can push it a little bit farther.
[00:39:49.620 --> 00:39:51.440]   We can give more people access to the system.
[00:39:51.440 --> 00:39:56.900]   So like they can come in and break it without any kind of interruption to service and then
[00:39:56.900 --> 00:39:59.400]   move the models between the two.
[00:39:59.400 --> 00:40:03.900]   And so what's the, what's the piece that Kubeflow is doing for you?
[00:40:03.900 --> 00:40:05.640]   It's the swapping in and out of the models.
[00:40:05.640 --> 00:40:08.440]   Is that like the key thing?
[00:40:08.440 --> 00:40:10.680]   The big part is the resource management.
[00:40:10.680 --> 00:40:19.040]   And I think that's always been the real value in that you, for us, our model usage is really
[00:40:19.040 --> 00:40:23.960]   spiky because no one sort of, there's sort of like always a hum, like a certain amount
[00:40:23.960 --> 00:40:25.700]   of noise of people using the models.
[00:40:25.700 --> 00:40:29.860]   And then there'll be someone who wants to know a prediction of every single article
[00:40:29.860 --> 00:40:30.860]   on Swahili Wikipedia.
[00:40:30.860 --> 00:40:39.360]   And so you get this huge spike and we, we try very, very hard to not limit people.
[00:40:39.360 --> 00:40:43.440]   Like when we, when we're limiting people's API access, it's because you're going to break
[00:40:43.440 --> 00:40:46.680]   the system if you do more, that is really our goal.
[00:40:46.680 --> 00:40:49.740]   Like, you know, we're, we're, we're funded by people.
[00:40:49.740 --> 00:40:52.560]   So like people should be able to use it.
[00:40:52.560 --> 00:40:58.720]   And along those lines, like having, being able to maintain that really spiky structure,
[00:40:58.720 --> 00:41:01.080]   particularly with models over like a broad range of systems.
[00:41:01.080 --> 00:41:06.240]   So like maybe one requires a GPU that uses TensorFlow, maybe two don't require one that
[00:41:06.240 --> 00:41:12.360]   uses, you know, like scikit-learn and then sort of managing all those resources in an
[00:41:12.360 --> 00:41:17.600]   automated fashion is super powerful for us because it, it means that we can not have
[00:41:17.600 --> 00:41:22.680]   what we have currently with ORS, which doesn't do that so well around like, you know, like
[00:41:22.680 --> 00:41:28.440]   I think a year ago, I, like I had my kid on my lap and I was manually restarting the prod
[00:41:28.440 --> 00:41:34.920]   server using a, using a script I'd written like, you know, after a glass of wine to sort
[00:41:34.920 --> 00:41:39.160]   of like get the surfer back up, try to like get around those kinds of issues with, with
[00:41:39.160 --> 00:41:41.320]   something that balances those resources really well.
[00:41:41.320 --> 00:41:44.280]   I think there's other things that we care about at the foundation, like, because it's
[00:41:44.280 --> 00:41:48.280]   an open source project, the foundation believes in open source projects, or like we want to
[00:41:48.280 --> 00:41:49.760]   contribute back to them.
[00:41:49.760 --> 00:41:55.680]   I think there's lots of nice on the training side, I'm pretty excited about some of the
[00:41:55.680 --> 00:41:59.460]   UI, UI parts of it.
[00:41:59.460 --> 00:42:04.240]   So for example, like Jupyter notebooks that could connect, like would allow our researchers
[00:42:04.240 --> 00:42:08.400]   to actually connect to the database and actually train, like construct models in the Jupyter
[00:42:08.400 --> 00:42:11.320]   notebook and then push a button to put it in production.
[00:42:11.320 --> 00:42:15.840]   Those are some of the things I'm, I'm interested down the road, but just straight resource
[00:42:15.840 --> 00:42:22.200]   management is a big deal because, you know, it's, it's weird because the, the thing about
[00:42:22.200 --> 00:42:25.960]   the foundation is the foundation is 500 people, which are like, wow, that's a lot of people,
[00:42:25.960 --> 00:42:28.680]   but you're running like one of the top 10 websites in the world.
[00:42:28.680 --> 00:42:30.600]   The scale is crazy.
[00:42:30.600 --> 00:42:35.480]   And so trying to like, do that with like, with what ends up being a small team, when
[00:42:35.480 --> 00:42:38.560]   you sort of cut people down, you know, like down to, okay, people who are working on the
[00:42:38.560 --> 00:42:41.760]   tech department, people are working on ML, people are working on this particular system.
[00:42:41.760 --> 00:42:47.400]   It gets like very small number of people have a lot of responsibility for things.
[00:42:47.400 --> 00:42:52.360]   And so automating what you can out to these systems is pretty nice and leaning on open
[00:42:52.360 --> 00:42:54.920]   source projects that like other people can help you with issues.
[00:42:54.920 --> 00:42:57.280]   That's definitely true.
[00:42:57.280 --> 00:42:58.280]   Makes sense.
[00:42:58.280 --> 00:43:01.040]   I guess there's like another theme of questions I want to make sure that we cover here just
[00:43:01.040 --> 00:43:06.000]   in our, in our crowdsourcing of questions for you, which I sort of summarizes as sort
[00:43:06.000 --> 00:43:12.400]   of like I think people admire the career that you've had and working on really impactful
[00:43:12.400 --> 00:43:16.760]   stuff in machine learning.
[00:43:16.760 --> 00:43:19.960]   I guess, how did you get into machine learning and how have you thought about your career?
[00:43:19.960 --> 00:43:27.200]   Like how do you feel like you've managed to get to all these super interesting projects?
[00:43:27.200 --> 00:43:35.080]   Uh, so my formal training is in quantitative research, actually quantitative social science
[00:43:35.080 --> 00:43:36.080]   research.
[00:43:36.080 --> 00:43:41.240]   So that was, I went to a PhD program that was all about stats basically.
[00:43:41.240 --> 00:43:49.080]   And when I was graduating, uh, I knew some people who are working on a Kenyan nonprofit
[00:43:49.080 --> 00:43:52.440]   and I just joined them, you know, and kind of was working on that.
[00:43:52.440 --> 00:43:55.800]   And then from there you sort of grow a community of people in a social network that you know,
[00:43:55.800 --> 00:43:59.000]   and people keep on pulling you into other things to work on.
[00:43:59.000 --> 00:44:05.920]   Um, I, I think, you know, for me, ML where the appeal was, was, you know, and I'm going
[00:44:05.920 --> 00:44:09.400]   to anger some statisticians on here, so, you know, this is hot take, hot take.
[00:44:09.400 --> 00:44:10.400]   Nice.
[00:44:10.400 --> 00:44:11.400]   Yeah.
[00:44:11.400 --> 00:44:17.840]   But I, yeah, the thing that frustrated me about, about statistics is I tended to not
[00:44:17.840 --> 00:44:21.320]   care about the causal inference about a lot of things.
[00:44:21.320 --> 00:44:23.360]   Like I cared about like the results that was happening.
[00:44:23.360 --> 00:44:27.760]   Cause I was doing a lot of this stuff, you know, as a job in like impact.
[00:44:27.760 --> 00:44:29.280]   So I was doing like election monitoring.
[00:44:29.280 --> 00:44:32.480]   So it's like helping someone set up a SMS page election monitoring.
[00:44:32.480 --> 00:44:36.800]   I didn't so much care about the causal relationship between like whether or not someone would
[00:44:36.800 --> 00:44:38.400]   send a message in or not.
[00:44:38.400 --> 00:44:39.680]   Like I cared if they did or not.
[00:44:39.680 --> 00:44:40.680]   Right.
[00:44:40.680 --> 00:44:42.760]   Like I, like a really, really focused on outcomes.
[00:44:42.760 --> 00:44:48.200]   And when you have small teams, like the value of ML is you could start to like really scale
[00:44:48.200 --> 00:44:51.240]   things out because you start to use machines as like the assistant to you.
[00:44:51.240 --> 00:44:52.240]   Right.
[00:44:52.240 --> 00:44:55.640]   So you design something manually and then you like send it out in the world.
[00:44:55.640 --> 00:44:58.520]   And then it does that at scale for you, which is like a really, like, it's like, it's like
[00:44:58.520 --> 00:44:59.520]   a superpower.
[00:44:59.520 --> 00:45:04.280]   And so I just started going farther and farther down the, down the path of saying, Hey, like
[00:45:04.280 --> 00:45:09.760]   there's, we can make this team of four people, you know, behave like a team of 50 people.
[00:45:09.760 --> 00:45:14.200]   If we start to use ML more and more and, you know, keep, keep walking down that and just
[00:45:14.200 --> 00:45:15.680]   get more and more complex.
[00:45:15.680 --> 00:45:20.600]   And then as I started doing things in more scale, you sort of move from the modeling
[00:45:20.600 --> 00:45:25.080]   side to the sort of engineering side of like, okay, now we need to like, you know, now we
[00:45:25.080 --> 00:45:26.080]   have 200 models.
[00:45:26.080 --> 00:45:29.320]   Well, how do we like make sure every single model is running at all times and it's totally
[00:45:29.320 --> 00:45:30.320]   okay.
[00:45:30.320 --> 00:45:31.960]   And like, how do we do that at scale?
[00:45:31.960 --> 00:45:35.960]   And so you sort of like constantly moving to like the next more technical challenge
[00:45:35.960 --> 00:45:37.320]   in those range.
[00:45:37.320 --> 00:45:44.240]   But it is, you know, for me, I've, I feel like I have stumbled into this stuff, but
[00:45:44.240 --> 00:45:50.500]   really it was probably because when I got started, I, I knew some people who are working
[00:45:50.500 --> 00:45:56.000]   at the Saktini little tech nonprofit in Kenya and just like got to know them.
[00:45:56.000 --> 00:45:58.720]   And then like, then they were sort of like, Oh, what about this other place?
[00:45:58.720 --> 00:46:01.000]   And then I switched places and then like, Hey, what about this other thing?
[00:46:01.000 --> 00:46:02.000]   And I joined that.
[00:46:02.000 --> 00:46:04.560]   And then it was like, you just sort of go from one thing to another, to another, to
[00:46:04.560 --> 00:46:05.560]   another.
[00:46:05.560 --> 00:46:11.600]   And I mean, it's true that, you know, some of the people that I worked with 10 years
[00:46:11.600 --> 00:46:16.400]   ago on like various projects around, you know, like environmental projects and that kind
[00:46:16.400 --> 00:46:18.520]   of stuff, like work at Wikipedia, right?
[00:46:18.520 --> 00:46:21.280]   Like there's like work at Wikimedia, like they're still here.
[00:46:21.280 --> 00:46:23.880]   Like there there's this, there's this like, you know, group of people who are working
[00:46:23.880 --> 00:46:24.880]   on stuff.
[00:46:24.880 --> 00:46:27.040]   It doesn't mean that other people don't come in and it doesn't mean that it's not a job,
[00:46:27.040 --> 00:46:28.040]   right?
[00:46:28.040 --> 00:46:35.280]   Like it is a job that I go to every day and I, I do my job, but it is you start to see
[00:46:35.280 --> 00:46:39.680]   the same faces over and over again as you do this for a while and people invite you
[00:46:39.680 --> 00:46:44.080]   to come and apply for a role or that kind of stuff.
[00:46:44.080 --> 00:46:49.320]   And how does it relate to your well-known ML flashcards and tutorials?
[00:46:49.320 --> 00:46:50.720]   What prompted you to do that?
[00:46:50.720 --> 00:46:57.380]   Do you think it's similar to your focus on, on maybe outcomes and applications versus
[00:46:57.380 --> 00:46:59.000]   the underlying statistics?
[00:46:59.000 --> 00:47:01.900]   Yeah, no, completely.
[00:47:01.900 --> 00:47:07.200]   So I think, you know, when people will, so I make these flashcards, like they're hand
[00:47:07.200 --> 00:47:09.600]   drawn, they're all about ML concepts.
[00:47:09.600 --> 00:47:14.000]   And people have come to me over the years and been like, Hey, this is like, you know,
[00:47:14.000 --> 00:47:18.760]   you should really read, read more books about ML rather than like flashcards.
[00:47:18.760 --> 00:47:23.240]   And I was like, well, one, I've read a lot of books about ML at this point.
[00:47:23.240 --> 00:47:29.580]   But the point of the flashcards and the point has always been one single thing that ML interviews
[00:47:29.580 --> 00:47:32.500]   require a certain amount of rote memorization, right?
[00:47:32.500 --> 00:47:35.440]   There are people that try to throw you gotcha questions.
[00:47:35.440 --> 00:47:40.400]   And I have received those questions and, you know, like describe a random forest, like
[00:47:40.400 --> 00:47:42.560]   from scratch and that kind of stuff.
[00:47:42.560 --> 00:47:48.560]   And those, you know, those questions, it's just easier to just memorize them, right?
[00:47:48.560 --> 00:47:50.560]   To just sit down and memorize it.
[00:47:50.560 --> 00:47:52.800]   And interviews shouldn't be run that way.
[00:47:52.800 --> 00:47:56.240]   I totally understand that, you know, we should all get to a better place where like, that's
[00:47:56.240 --> 00:48:00.380]   not happening, but yet it does happen in most job interviews.
[00:48:00.380 --> 00:48:03.920]   And so for me, I just started making flashcards for it.
[00:48:03.920 --> 00:48:05.040]   Like what is this concept?
[00:48:05.040 --> 00:48:06.960]   What is this concept, right?
[00:48:06.960 --> 00:48:07.960]   Like can I do it?
[00:48:07.960 --> 00:48:10.720]   And just looking at the flashcards over and over again.
[00:48:10.720 --> 00:48:15.720]   And from there, I just sort of developed more and more of them.
[00:48:15.720 --> 00:48:18.360]   And then, you know, other people got interested in that kind of stuff.
[00:48:18.360 --> 00:48:22.160]   But it is about, you know, getting that stuff into your brain.
[00:48:22.160 --> 00:48:27.240]   That's just, you know, it's not something that you can read.
[00:48:27.240 --> 00:48:30.120]   You know, if you read a thousand books, maybe you probably forget the concepts because there
[00:48:30.120 --> 00:48:31.120]   would just be so many.
[00:48:31.120 --> 00:48:33.920]   Instead, these are the concepts that I've run into at interviews and other people have
[00:48:33.920 --> 00:48:38.920]   run into the interviews and like memorize it and then regurgitate it back up.
[00:48:38.920 --> 00:48:43.120]   Because you'll look really cool when you write an equation from scratch or something like
[00:48:43.120 --> 00:48:46.120]   that because you had it in your brain.
[00:48:46.120 --> 00:48:49.640]   And it goes back to the idea that like, I am interested in impact.
[00:48:49.640 --> 00:48:52.280]   I'm a thousand percent interested in impact.
[00:48:52.280 --> 00:48:56.120]   And like a game is being played in an interview where they try to stump you with like, you
[00:48:56.120 --> 00:48:59.540]   know, describe, you know, gradient descent to me.
[00:48:59.540 --> 00:49:00.540]   That's a game.
[00:49:00.540 --> 00:49:03.400]   They're trying to throw a trick question at you, crush it, right?
[00:49:03.400 --> 00:49:06.320]   Memorize the concept and then crush it at the interview.
[00:49:06.320 --> 00:49:07.320]   That's it.
[00:49:07.320 --> 00:49:10.240]   And I wish people didn't throw those kinds of questions, but they do.
[00:49:10.240 --> 00:49:11.240]   And so, great.
[00:49:11.240 --> 00:49:15.000]   I will make flashcards to like get past that part.
[00:49:15.000 --> 00:49:18.680]   It's less an issue now because I do more like management stuff.
[00:49:18.680 --> 00:49:22.760]   So the questions are, you know, not so deep.
[00:49:22.760 --> 00:49:24.880]   But it's definitely was like a big part of my career.
[00:49:24.880 --> 00:49:28.600]   It was like, especially for me, because people look at me with like a social science background.
[00:49:28.600 --> 00:49:31.120]   Like literally my PhD is in political science.
[00:49:31.120 --> 00:49:36.480]   People were like, oh, so you're like, you know, a terrible coder and non-technical person.
[00:49:36.480 --> 00:49:39.320]   So like, I'm going to throw you some gotchas in there.
[00:49:39.320 --> 00:49:44.600]   And just being able to memorize it and, you know, spit it out has been frankly a really
[00:49:44.600 --> 00:49:46.200]   useful tool.
[00:49:46.200 --> 00:49:51.440]   So when you interview like a technical person, now that you're a manager, how do you approach
[00:49:51.440 --> 00:49:52.440]   that?
[00:49:52.440 --> 00:49:53.440]   How do you avoid gotcha questions?
[00:49:53.440 --> 00:49:57.680]   Like what questions do you ask to sort of get at the competence of somebody's work?
[00:49:57.680 --> 00:50:04.480]   Yeah, I actually really prefer to give people a choice of what they talk about.
[00:50:04.480 --> 00:50:09.040]   And so like some of the questions that I've really liked have been like, you know, tell
[00:50:09.040 --> 00:50:12.560]   me about like just what algorithm can you actually describe in detail?
[00:50:12.560 --> 00:50:14.440]   Like, you know, what's that, whatever, whatever you want.
[00:50:14.440 --> 00:50:16.460]   You like, what's that one that you like?
[00:50:16.460 --> 00:50:18.560]   What's that one that you have as your go-to?
[00:50:18.560 --> 00:50:23.800]   I like that because I'm not trying to say, hey, in my experience, this algorithm is important.
[00:50:23.800 --> 00:50:26.960]   And therefore, if you don't know that particular one, you're, you know, not qualified.
[00:50:26.960 --> 00:50:30.800]   And so instead of saying like, hey, like go, like, I want you to go deep, but pick, you
[00:50:30.800 --> 00:50:34.920]   can pick anything that you get to go deep in and let's just jam out about it.
[00:50:34.920 --> 00:50:41.240]   And I have really, really, really appreciated that because I have, I have had candidates
[00:50:41.240 --> 00:50:47.320]   who come in, who have been, have been like pretty, pretty nervous.
[00:50:47.320 --> 00:50:50.640]   And you know, it can come off that like, they don't know what they're talking about or something
[00:50:50.640 --> 00:50:51.640]   like that.
[00:50:51.640 --> 00:50:54.200]   And I'll throw that question to them and they will just destroy it.
[00:50:54.200 --> 00:50:58.680]   Like they will just go so incredibly deep and they'll start to geek out on it and they'll
[00:50:58.680 --> 00:51:02.040]   start to enjoy the whole interview process because they get to talk about what they know
[00:51:02.040 --> 00:51:04.160]   and they light up about it.
[00:51:04.160 --> 00:51:06.320]   And it is so fun to participate in that.
[00:51:06.320 --> 00:51:10.680]   And it shows you that like, you know, it shows you that people have these like very variety
[00:51:10.680 --> 00:51:15.960]   of expertises, like, cause they did this particular ML model for four years and they really, really,
[00:51:15.960 --> 00:51:16.960]   really know it.
[00:51:16.960 --> 00:51:19.800]   And so you can say like, okay, you know, like that's cool.
[00:51:19.800 --> 00:51:21.840]   Like it'd be fun to work with that person.
[00:51:21.840 --> 00:51:28.240]   That's the kind of stuff that I have grown to like because the fundamental truth about
[00:51:28.240 --> 00:51:34.440]   data science is that it's such a broad field that your questions that you get in an individual
[00:51:34.440 --> 00:51:40.280]   interview can be all over the place from like deep statistics.
[00:51:40.280 --> 00:51:45.240]   Like I've had to write a statistical proof at one point to like model production stuff.
[00:51:45.240 --> 00:51:48.600]   So like MLE, like, you know, engine, ML ops kind of things.
[00:51:48.600 --> 00:51:52.760]   Like how would you architect a system that do this to computer science stuff, to social
[00:51:52.760 --> 00:51:55.400]   science stuff, just all over the place.
[00:51:55.400 --> 00:52:00.500]   And frankly, I'm amazed that anybody passes these interviews.
[00:52:00.500 --> 00:52:07.160]   So I sort of liked giving people the opportunity to dive into wherever they want.
[00:52:07.160 --> 00:52:10.760]   If they can't find a place that they really dive into, that's also a signal, right?
[00:52:10.760 --> 00:52:11.760]   That makes sense.
[00:52:11.760 --> 00:52:16.680]   Well, you know, we're almost out of time and we have two questions that we'd like to end
[00:52:16.680 --> 00:52:20.040]   with, but I think you'll have interesting responses.
[00:52:20.040 --> 00:52:26.120]   So I guess the second to last question is what's an underrated aspect of machine learning
[00:52:26.120 --> 00:52:29.000]   that you think people should pay more attention to?
[00:52:29.000 --> 00:52:31.240]   Oh, wow.
[00:52:31.240 --> 00:52:32.240]   Wow.
[00:52:32.240 --> 00:52:35.720]   That's an interesting approach.
[00:52:35.720 --> 00:52:42.760]   I think the one that I really have started to like a lot is low power models.
[00:52:42.760 --> 00:52:44.360]   So models that don't require...
[00:52:44.360 --> 00:52:47.600]   So there's one direction that ML is taking, which is bigger and bigger and bigger and
[00:52:47.600 --> 00:52:49.280]   bigger models.
[00:52:49.280 --> 00:52:52.200]   And it's sort of like getting a bigger and bigger, bigger truck, right?
[00:52:52.200 --> 00:52:54.280]   You just like, oh, you know, what'd be better?
[00:52:54.280 --> 00:52:55.280]   Two engines.
[00:52:55.280 --> 00:52:56.280]   You know, what's better than two engines?
[00:52:56.280 --> 00:52:57.280]   Six engines.
[00:52:57.280 --> 00:52:58.280]   You know, what's better?
[00:52:58.280 --> 00:52:59.760]   Like 24 engines.
[00:52:59.760 --> 00:53:06.720]   And I have really started to like a teeny ML, like very, very, very small ML that you
[00:53:06.720 --> 00:53:09.480]   can run on a Raspberry Pi and that kind of stuff.
[00:53:09.480 --> 00:53:15.560]   And I think there's a pureness around it, but there's also like creativity comes from
[00:53:15.560 --> 00:53:16.560]   constraints.
[00:53:16.560 --> 00:53:21.680]   And so constraining yourself to like very, very low resource settings is really interesting.
[00:53:21.680 --> 00:53:28.000]   And I think it opens up stuff around with cheaper smartphones and that kind of stuff,
[00:53:28.000 --> 00:53:31.360]   which it's just a different direction than you're going to get from some of the really
[00:53:31.360 --> 00:53:35.760]   cool but like huge models that take $24 million to train or something like that.
[00:53:35.760 --> 00:53:36.760]   Oh, yeah.
[00:53:36.760 --> 00:53:38.120]   And even a Raspberry Pi is kind of big.
[00:53:38.120 --> 00:53:39.120]   I mean, it's quite unreasonable.
[00:53:39.120 --> 00:53:40.120]   Yeah.
[00:53:40.120 --> 00:53:41.120]   It's hard.
[00:53:41.120 --> 00:53:42.120]   All right.
[00:53:42.120 --> 00:53:49.120]   And then the final question is, what's the biggest challenge of making these models actually
[00:53:49.120 --> 00:53:50.120]   run in the real world?
[00:53:50.120 --> 00:53:53.440]   I mean, you're actually responsible for running models.
[00:53:53.440 --> 00:53:56.440]   What's your biggest challenge?
[00:53:56.440 --> 00:54:06.720]   The biggest one I think that I face is, well, I'll take a step back.
[00:54:06.720 --> 00:54:10.400]   When sort of me and you were getting into ML, because we're both slightly older, I don't
[00:54:10.400 --> 00:54:15.680]   want to claim that you're old, but you're around my age, ML was just starting off.
[00:54:15.680 --> 00:54:22.680]   And so you could totally join an organization and make any model and run it on your laptop.
[00:54:22.680 --> 00:54:25.520]   And it was better than the hard-coded thing that you were using.
[00:54:25.520 --> 00:54:29.120]   And you were like amazing.
[00:54:29.120 --> 00:54:31.200]   That's no longer the case.
[00:54:31.200 --> 00:54:37.240]   Now it's the case that they've had 10 years worth of models that they've made, all in
[00:54:37.240 --> 00:54:40.120]   these different settings, all in these different contexts.
[00:54:40.120 --> 00:54:42.360]   And they're retraining models every single night.
[00:54:42.360 --> 00:54:46.800]   And so they have thousands or tens of thousands of models to deal with.
[00:54:46.800 --> 00:54:53.720]   And a big part of what I found is hard is how do you just manage all those models?
[00:54:53.720 --> 00:54:56.760]   And this is a real pitch for ML Ops.
[00:54:56.760 --> 00:55:02.320]   It is hard to manage just all those models all the time and make sure they're all not
[00:55:02.320 --> 00:55:04.240]   broken, not old data.
[00:55:04.240 --> 00:55:07.200]   They throw errors, there's dependency management around it.
[00:55:07.200 --> 00:55:13.160]   It is difficult to have in the real world setting, hundreds of models going out all
[00:55:13.160 --> 00:55:19.520]   the time, whether you're at a company or whether you're at the Wikimedia Foundation.
[00:55:19.520 --> 00:55:20.920]   It is just hard to do that.
[00:55:20.920 --> 00:55:27.160]   And it is not a surprise to me that ML Ops has become the thing that is really, really
[00:55:27.160 --> 00:55:33.440]   helping people in this field out because it is something that is otherwise just difficult.
[00:55:33.440 --> 00:55:38.680]   It's insurmountable to do it yourself because it's easy when you have one model, right?
[00:55:38.680 --> 00:55:42.720]   And you can be like, oh, let me think about this particular hyper parameter deeply after
[00:55:42.720 --> 00:55:43.720]   reading a book.
[00:55:43.720 --> 00:55:48.040]   It's another where it's like, we're going to be training 6,000 models tonight.
[00:55:48.040 --> 00:55:49.040]   How do you keep them organized?
[00:55:49.040 --> 00:55:50.040]   How do you keep them up?
[00:55:50.040 --> 00:55:51.480]   How do you see how they're being used?
[00:55:51.480 --> 00:55:53.560]   How do you maintain them?
[00:55:53.560 --> 00:55:57.520]   That is a different game, which is where we're going for sure.
[00:55:57.520 --> 00:55:58.520]   Awesome.
[00:55:58.520 --> 00:55:59.520]   Thanks so much.
[00:55:59.520 --> 00:56:00.520]   Great to end on.
[00:56:00.520 --> 00:56:01.520]   Yeah.
[00:56:01.520 --> 00:56:02.520]   Appreciate it, Chris.
[00:56:02.520 --> 00:56:06.960]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:56:06.960 --> 00:56:11.680]   to the show notes in the description where you can find links to all the papers that
[00:56:11.680 --> 00:56:16.080]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:56:16.080 --> 00:56:17.080]   produce.
[00:56:17.080 --> 00:56:18.080]   So check it out.
[00:56:18.080 --> 00:56:20.660]   (upbeat music)
[00:56:20.660 --> 00:56:24.000]   [MUSIC PLAYING]

