
[00:00:00.720 --> 00:00:08.480]   Okay, today I have the pleasure of interviewing Shane Legge, who is a founder and the chief
[00:00:08.480 --> 00:00:12.320]   AGI scientist of Google DeepMind. Shane, welcome to the podcast.
[00:00:12.320 --> 00:00:13.920]   Thank you. It's a pleasure to be here.
[00:00:13.920 --> 00:00:18.880]   So first question, how do we measure progress towards AGI concretely?
[00:00:18.880 --> 00:00:23.760]   So we have these loss numbers and we can see how the loss improves from one model to another,
[00:00:23.760 --> 00:00:25.360]   but it's just a number. How do we interpret this?
[00:00:25.360 --> 00:00:27.280]   How do we see how much progress we're actually making?
[00:00:28.800 --> 00:00:30.240]   That's a hard question, actually.
[00:00:30.240 --> 00:00:38.080]   AGI, by its definition, is about generality. So it's not about doing a specific thing. It's much
[00:00:38.080 --> 00:00:42.320]   easier to measure performance when you have a very specific thing in mind, because you can
[00:00:42.320 --> 00:00:47.280]   construct a test around that. Well, maybe I should first explain what do I mean by AGI,
[00:00:47.280 --> 00:00:53.280]   because there are a few different notions around. When I say AGI, I mean a machine that can do
[00:00:53.280 --> 00:00:57.600]   the sorts of cognitive things that people can typically do, possibly more.
[00:00:58.160 --> 00:01:01.360]   But to be an AGI, that's kind of the bar you need to meet.
[00:01:01.360 --> 00:01:06.720]   So if we want to test whether we're meeting this threshold or we're getting close to the
[00:01:06.720 --> 00:01:13.600]   threshold, what we actually need then is a lot of different kinds of measurements and tests
[00:01:13.600 --> 00:01:19.040]   spans the breadth of all the sorts of cognitive tasks that people can do,
[00:01:19.040 --> 00:01:24.960]   and then to have a sense of what is human performance on these sorts of tasks.
[00:01:24.960 --> 00:01:28.720]   And that then allows us to sort of judge whether or not we're there.
[00:01:28.720 --> 00:01:33.600]   It's difficult because you'll never have a complete set of everything that people can do,
[00:01:33.600 --> 00:01:37.600]   because it's such a large set. But I think that if you ever get to the point where
[00:01:37.600 --> 00:01:43.920]   you have a pretty good range of tests of all sorts of different things that people do,
[00:01:43.920 --> 00:01:47.520]   cognitive things people do, and you have an AI system which can
[00:01:47.520 --> 00:01:54.160]   meet human performance and all those things, and with some effort, you can't actually come up with
[00:01:54.160 --> 00:02:00.880]   new examples of cognitive tasks where the machine is below human performance, then at that point,
[00:02:00.880 --> 00:02:06.160]   it's conceptually possible that there is something that the machine can't do that
[00:02:06.160 --> 00:02:10.400]   people can do. But if you can't find it with some effort, I think for practical purposes,
[00:02:10.400 --> 00:02:13.920]   you now have an AGI. So let's get more concrete.
[00:02:13.920 --> 00:02:19.760]   We measure the performance of these large language models on MMLU or something, and maybe you can
[00:02:19.760 --> 00:02:24.880]   explain what all these different benchmarks are. But the ones we use right now that you might see
[00:02:24.880 --> 00:02:30.560]   in a paper, what are they missing? What aspect of human cognition do they not measure adequately?
[00:02:30.560 --> 00:02:38.400]   Yeah, I never had hard green. These are quite big areas. So they don't measure things like
[00:02:38.400 --> 00:02:42.880]   understanding streaming video, for example, because these are language models and people
[00:02:42.880 --> 00:02:50.480]   can do things like understand streaming video. They don't do things, like humans have what we
[00:02:50.480 --> 00:02:54.720]   call episodic memory. So we have a working memory, which are things that have happened
[00:02:54.720 --> 00:02:59.040]   quite recently, and then we have the sort of cortical memory. So these are things that have
[00:02:59.040 --> 00:03:04.080]   sort of been in our cortex, but there's also a system in between, which is episodic memory,
[00:03:04.080 --> 00:03:09.680]   which is the hippocampus. And so this is about learning specific things very, very rapidly.
[00:03:09.680 --> 00:03:13.680]   So some of the things I say to you today, if you remember them tomorrow, that'll be your
[00:03:13.680 --> 00:03:18.560]   episodic memory, your hippocampus. Our models don't really have that kind of thing, and we
[00:03:18.560 --> 00:03:22.800]   don't really test for that kind of thing. We just sort of try to make the context windows, which is
[00:03:22.800 --> 00:03:27.040]   I think more like a working memory, longer and longer to sort of compensate for this.
[00:03:27.040 --> 00:03:33.200]   But yeah, we don't really test for that kind of a thing. So there is all sorts of bits and pieces,
[00:03:33.200 --> 00:03:37.920]   but it is a difficult question because you really need to, as I said, intelligence,
[00:03:37.920 --> 00:03:42.160]   the generality of human intelligence is very, very broad. So you really have to start going
[00:03:42.160 --> 00:03:47.280]   into the weeds of trying to find if there's specific types of things that are missing
[00:03:47.280 --> 00:03:52.800]   from existing benchmarks or different categories of benchmarks that don't currently exist or
[00:03:52.800 --> 00:03:58.160]   something. Yeah. The thing you're referring to with episodic memory, would it be fair to call
[00:03:58.160 --> 00:04:03.680]   that sample efficiency or is that a different? It's very much related to sample efficiency. It's
[00:04:03.680 --> 00:04:09.520]   one of the things that enables humans to be very sample efficient. Right. Large language models
[00:04:09.520 --> 00:04:14.080]   have a certain kind of sample efficiency because when something's in their context window,
[00:04:14.080 --> 00:04:19.840]   they can then, that sort of biases the distribution to behave in a different way.
[00:04:19.840 --> 00:04:24.400]   And so that's a very rapid kind of learning. So there are multiple kinds of learning and
[00:04:24.400 --> 00:04:29.840]   the existing systems have some of them, but not others. So it's a little bit complicated.
[00:04:29.840 --> 00:04:36.960]   So this kind of memory or we call it sample efficiency, whatever, is it a fatal flaw of
[00:04:36.960 --> 00:04:41.520]   these deep learning models that it just takes trillions of tokens, far more, many orders of
[00:04:41.520 --> 00:04:45.200]   magnitude more than a human will see throughout their lifetime? Or is this something that just
[00:04:45.200 --> 00:04:51.680]   solved over time? So the models can learn things immediately when it's in a context window.
[00:04:51.680 --> 00:04:55.040]   And then they have this sort of this longer process of when you actually train the base
[00:04:55.040 --> 00:04:59.120]   model and so on. And that's, they're learning over trillions of tokens, but they sort of
[00:04:59.120 --> 00:05:02.160]   miss something in the middle. Right. Right. That's sort of what I'm getting at here.
[00:05:02.160 --> 00:05:09.040]   I don't think it's a fundamental limitation. I think what's happened with large language models
[00:05:09.040 --> 00:05:15.760]   is something fundamental has changed. We know how to build models now that have some degree of,
[00:05:15.760 --> 00:05:22.080]   I would say, understanding of what's going on. And that did not exist in the past. And because
[00:05:22.080 --> 00:05:27.520]   we've got a scalable way to do this now, that unlocks lots and lots of new things.
[00:05:28.240 --> 00:05:34.000]   Now we can then look at things which are missing, such as the sort of episodic memory type thing,
[00:05:34.000 --> 00:05:42.160]   and we can then start to imagine ways to address that. So my feeling is that there are kind of
[00:05:42.160 --> 00:05:48.400]   relatively clear paths forwards now to address most of the shortcomings we see in existing models,
[00:05:48.400 --> 00:05:54.320]   whether it's about delusions, factuality, the type of memory and learning that they have,
[00:05:54.320 --> 00:06:00.400]   or understanding video or all sorts of things like that. So I don't see there are big blockers here.
[00:06:00.400 --> 00:06:05.920]   I don't see big walls in front of us. I just see there's more research and work and these things
[00:06:05.920 --> 00:06:11.360]   will improve and probably be adequately solved. But going back to the original question of how
[00:06:11.360 --> 00:06:18.640]   do you measure when human level AI has arrived or beyond it, as you mentioned, there's these other
[00:06:18.640 --> 00:06:23.680]   sorts of benchmarks you can use and other sorts of traits. But concretely, what would it have to
[00:06:23.680 --> 00:06:26.960]   do for you to be like, okay, we've reached human level? Would it have to beat Minecraft from start
[00:06:26.960 --> 00:06:32.800]   to finish? Would it have to get 100% on MMLU? What would it have to do? There is no one thing
[00:06:32.800 --> 00:06:36.640]   that would do it, because I think that's the nature of it. It's about general intelligence,
[00:06:36.640 --> 00:06:41.200]   so I'd have to make sure it could do lots and lots of different things and it didn't have a gap.
[00:06:41.200 --> 00:06:46.560]   We already have systems that can do very impressive categories of things to human
[00:06:46.560 --> 00:06:54.560]   level or even beyond. So I would want a whole suite of tests that I felt was very comprehensive.
[00:06:54.560 --> 00:06:59.600]   And then furthermore, when people come in and say, okay, so it's passing a big suite of tests,
[00:06:59.600 --> 00:07:07.280]   let's try to find examples. Let's take an adversarial approach to this. Let's deliberately
[00:07:07.280 --> 00:07:12.720]   try to find examples where people can clearly typically do this, but the machine fails.
[00:07:12.720 --> 00:07:16.240]   And when those people cannot succeed, I'll go, okay, we're probably there.
[00:07:16.480 --> 00:07:20.880]   A lot of your earlier research, at least on that Zack I find,
[00:07:20.880 --> 00:07:28.080]   emphasized that AI should be able to manipulate and succeed in a variety of open-ended environments.
[00:07:28.080 --> 00:07:30.960]   It kind of sounds like a video game almost. Is that where your head is still at now,
[00:07:30.960 --> 00:07:32.080]   or do you think about it differently?
[00:07:32.080 --> 00:07:39.280]   Yeah, it's evolved a bit. When I did my thesis work around universal intelligence and so on,
[00:07:39.840 --> 00:07:47.520]   I was trying to come up with a sort of extremely universal, general, mathematically clean
[00:07:47.520 --> 00:07:57.280]   framework for defining and measuring intelligence. And I think there were aspects of that that were
[00:07:57.280 --> 00:08:04.560]   successful. I think in my own mind, it clarified the nature of intelligence as being able to
[00:08:04.560 --> 00:08:09.040]   perform well in lots of different domains and different tasks and so on. It's about that sort
[00:08:09.040 --> 00:08:16.960]   of capability of performance and the breadth of performance. So I found that was quite helpful
[00:08:16.960 --> 00:08:24.000]   and enlightening. There was always the issue of the reference machine, because in the framework,
[00:08:24.000 --> 00:08:29.920]   you have a weighting of things according to their complexity. It's like an Occam's razor type of
[00:08:29.920 --> 00:08:37.600]   thing, where you weight tasks, environments, which is simpler, more highly in this sort of...
[00:08:37.600 --> 00:08:44.480]   Because you've got an infinite, it's a countable space of different computable environments,
[00:08:44.480 --> 00:08:51.040]   or semi-computable environments. And that combined complexity measure has something
[00:08:51.040 --> 00:08:55.600]   built into it, which is called a reference machine. And that's a free parameter.
[00:08:55.600 --> 00:09:02.160]   So that means that the intelligence measure has a free parameter in it. And as you change that
[00:09:02.160 --> 00:09:07.040]   free parameter, it changes the weighting and the distribution over the space of all the different
[00:09:07.040 --> 00:09:12.240]   tasks and environments. So this is sort of an unresolved part of the whole problem.
[00:09:12.240 --> 00:09:21.120]   So what reference machine should we ideally use? There isn't really a... There's no universal,
[00:09:21.120 --> 00:09:26.480]   like one specific reference machine. People will usually put a universal Turing machine in there,
[00:09:26.480 --> 00:09:30.240]   but there are many kinds of universal Turing machines. You have to put a universal Turing
[00:09:30.240 --> 00:09:35.600]   machine in there, but there are many different ones. So I think, given that it's a free parameter,
[00:09:35.600 --> 00:09:42.800]   I think the most natural thing to do is say, okay, let's think about what's meaningful to us
[00:09:42.800 --> 00:09:47.520]   in terms of intelligence. I think human intelligence is meaningful to us and the
[00:09:47.520 --> 00:09:52.720]   environment that we live in. We know what human intelligence is. We are human too. We interact
[00:09:52.720 --> 00:09:57.040]   with other people who have human intelligence. We know that human intelligence is possible,
[00:09:57.040 --> 00:10:02.000]   obviously, because it exists in the world. We know that human intelligence is very,
[00:10:02.000 --> 00:10:06.080]   very powerful because it's affected the world profoundly in countless ways.
[00:10:06.080 --> 00:10:13.120]   And we know if human-level intelligence was achieved, that would be economically transformative
[00:10:13.120 --> 00:10:17.600]   because the types of cognitive tasks people do in the economy could be done by machines then.
[00:10:17.600 --> 00:10:23.520]   And it would be philosophically important because this is sort of how we often think
[00:10:23.520 --> 00:10:28.480]   about intelligence. And I think historically it would be a key point. So I think that human
[00:10:28.480 --> 00:10:32.720]   intelligence is actually quite, in a human-like environment, is quite a natural sort of reference
[00:10:32.720 --> 00:10:42.960]   point. So you could imagine setting your reference machine to be such that it emphasizes the kinds
[00:10:42.960 --> 00:10:47.840]   of environments that we live in, as opposed to some abstract mathematical environment or something
[00:10:47.840 --> 00:10:53.440]   like that. And so that's how I've kind of gone on this journey of, let's try to define a completely
[00:10:53.440 --> 00:11:00.080]   universal, clean, mathematical notion of intelligence to, well, it's got a free parameter.
[00:11:00.080 --> 00:11:05.600]   One way of thinking about it is say, okay, let's think more concretely now about human intelligence.
[00:11:05.600 --> 00:11:10.640]   And can we build machines that can match human intelligence? Because we understand what that is,
[00:11:10.640 --> 00:11:16.400]   and we know that that is a very powerful thing. It has economic, philosophical, historical kind
[00:11:16.400 --> 00:11:22.160]   of importance. So that's kind of the... And the other aspect, of course, is that in this pure
[00:11:22.160 --> 00:11:26.880]   formulation of combographic complexity, it's actually not computable. And I obviously knew
[00:11:26.880 --> 00:11:33.120]   that there was a limitation at the time, but it was an effort to say, okay, can we just even very
[00:11:33.120 --> 00:11:37.600]   theoretically come up with a clean definition? I think we can sort of get there, but we have this
[00:11:37.600 --> 00:11:43.840]   issue of a reference machine, which is unspecified. - So before we move on, I do want to ask,
[00:11:43.840 --> 00:11:50.000]   on the original point you made about these machines, or these LLMs need episodic memory,
[00:11:51.680 --> 00:11:57.600]   you said that these are problems that we can solve. These are not fundamental impediments.
[00:11:57.600 --> 00:12:02.000]   But when you say that, do you think they'll just be solved by scale? Or do each of these need a
[00:12:02.000 --> 00:12:07.440]   fine-grained specific solution that is architectural in nature? - I think it'll be architectural in
[00:12:07.440 --> 00:12:14.240]   nature, because the... Well, the current architectures, they don't really have what
[00:12:14.240 --> 00:12:18.880]   you need to do this. They basically have a context window, which is very, very fluid,
[00:12:18.880 --> 00:12:23.360]   of course, and they have the weights, which things get baked into very slowly. So to my mind,
[00:12:23.360 --> 00:12:28.880]   it feels like working memory, which is like the activations in your brain, and then the weights,
[00:12:28.880 --> 00:12:34.480]   the synapses and so on in your cortex. Now, the brain separates these things out. It has a separate
[00:12:34.480 --> 00:12:40.320]   mechanism for rapidly learning specific information, because that's a different type of
[00:12:40.320 --> 00:12:48.560]   optimization problem compared to slowly learning deep generalities, right? There's a tension
[00:12:48.560 --> 00:12:53.520]   between the two, but you want to be able to do both. You want to be able to, I don't know,
[00:12:53.520 --> 00:12:57.680]   hear someone's name and remember it the next day. And you also want to be able to integrate
[00:12:57.680 --> 00:13:03.120]   information over a lifetime, so you start to see deeper patterns in the world. These are quite
[00:13:03.120 --> 00:13:11.520]   different optimization targets, different processes, but a comprehensive system should
[00:13:11.520 --> 00:13:16.080]   be able to do both. And so I think it's conceivable you could build one system that does both,
[00:13:16.080 --> 00:13:19.120]   but you can see because they're quite different things that it makes sense for them to be
[00:13:19.120 --> 00:13:24.080]   different. I think that's why the brain does it separately. I'm curious about how concretely you
[00:13:24.080 --> 00:13:29.040]   think that would be achieved. And I'm specifically curious, I guess you can answer this as part of
[00:13:29.040 --> 00:13:35.680]   the answer. You know, DeepMind has been working on these domain-specific reinforcement learning
[00:13:35.680 --> 00:13:42.720]   type setups, AlphaFold, AlphaCode, and so on. How does that fit into what you see as a path to AGI?
[00:13:42.720 --> 00:13:48.160]   Have these just been orthogonal domain-specific models, or do they feed into the eventual AGI?
[00:13:48.160 --> 00:13:56.800]   Things like AlphaFold are not really feeding into AGI. We may learn things in the process
[00:13:56.800 --> 00:14:04.400]   that may end up being relevant, but I don't see them as likely being on the path to AGI.
[00:14:04.400 --> 00:14:10.400]   But yeah, we're a big group. We've got hundreds and hundreds and hundreds of PhDs working on lots
[00:14:10.400 --> 00:14:17.280]   of different projects. So when we find what we see like opportunities to do something significant
[00:14:17.280 --> 00:14:24.400]   like AlphaFold, we'll go and do it. It's not like we only do AGI type work. We work on fusion
[00:14:24.400 --> 00:14:34.400]   reactors and various things in sustainability, energy. We've got people looking at satellite
[00:14:34.400 --> 00:14:41.840]   images of deforestation. We have people looking at weather forecasting. We've got tons of people.
[00:14:41.840 --> 00:14:47.040]   We've got lots of things. On the point you made earlier about the reference class or the reference
[00:14:47.040 --> 00:14:51.600]   machine as human intelligence, it's interesting because in your 2008 thesis, one of the things
[00:14:51.600 --> 00:14:55.680]   you mentioned almost as a side note is, well, how would you measure intelligence? And you said,
[00:14:55.680 --> 00:15:00.640]   well, you could do a compression test and you could see if it fills in words and a sample of
[00:15:00.640 --> 00:15:05.760]   text, and that could measure intelligence. And funnily enough, that's basically how LLMs are
[00:15:05.760 --> 00:15:11.120]   trained. At the time, did it stick out to you as an especially fruitful thing to train for?
[00:15:11.120 --> 00:15:16.720]   Well, yeah. I mean, in a sense, what's happened is actually very aligned with
[00:15:16.720 --> 00:15:23.200]   what I write about my thesis, which are the ideas from Markus Hatta with AIX, where
[00:15:23.200 --> 00:15:29.440]   you take Solomonov induction, which is this incomputable, but sort of theoretically very
[00:15:29.440 --> 00:15:38.800]   elegant and extremely sample efficient prediction system. And then once you have that, you can build
[00:15:38.800 --> 00:15:45.840]   a general agent on top of it by basically adding search and reinforcement signal. That's what you
[00:15:45.840 --> 00:15:52.960]   do with AIX. But what that sort of tells you is that if you have a fantastically good sequence
[00:15:52.960 --> 00:16:00.080]   predictor, some approximation of Solomonov induction, then going from that to a very powerful,
[00:16:00.080 --> 00:16:08.480]   very general AGI system is just sort of another step. You've actually solved a lot of the problem
[00:16:08.480 --> 00:16:13.520]   already. And I think that's what we're seeing today, actually, that these incredibly powerful
[00:16:13.520 --> 00:16:18.480]   foundation models are incredibly good sequence predictors. They're compressing the world based
[00:16:18.480 --> 00:16:23.680]   on all this data, and then you will be able to extend these in different ways and build very,
[00:16:23.680 --> 00:16:27.040]   very powerful agents out of them. Okay. Let me ask you more about that. So
[00:16:27.040 --> 00:16:31.920]   Richard Sutton's bitter lesson essay says that there's two things you can scale,
[00:16:31.920 --> 00:16:36.880]   search and learning. And I guess you could say that LLMs are about the learning aspect.
[00:16:36.880 --> 00:16:41.440]   The search stuff, which you've worked on throughout your career, where you have an
[00:16:41.440 --> 00:16:48.560]   agent that is interacting with this environment. Is that the direction that needs to be explored
[00:16:48.560 --> 00:16:51.840]   again? Or is that something that needs to be added to LLMs where they can actually interact
[00:16:51.840 --> 00:16:58.000]   with their data or the world or in some way? Yeah. I think that's on the right track. I think
[00:16:58.000 --> 00:17:07.360]   these foundation models are world models of a kind. And to do really creative problem solving,
[00:17:07.920 --> 00:17:13.920]   you need to start searching. So if I think about something like AlphaGo and the move 37,
[00:17:13.920 --> 00:17:20.400]   the famous move 37, where did that come from? Did that come from all this data that it's seen of
[00:17:20.400 --> 00:17:26.880]   human games or something like that? No, it didn't. It came from it identifying a move as being
[00:17:26.880 --> 00:17:34.560]   quite unlikely, but possible. And then via a process of search, coming to understand that
[00:17:35.440 --> 00:17:41.840]   was actually a very, very good move. So to get real creativity, you need to search through spaces
[00:17:41.840 --> 00:17:45.360]   of possibilities and find these sort of hidden gems. That's what creativity is.
[00:17:45.360 --> 00:17:52.000]   I think current language models, they don't really do that kind of a thing. They really are
[00:17:52.000 --> 00:17:57.600]   mimicking the data. They are mimicking all the human ingenuity and everything, which they have
[00:17:57.600 --> 00:18:02.400]   seen from all this data that's coming from the internet that's originally derived from humans.
[00:18:02.400 --> 00:18:08.560]   If you want a system that can go truly beyond that and not just generalize in novel ways,
[00:18:08.560 --> 00:18:15.280]   these models can blend things. They can do Harry Potter in the style of a Kanye West rap or
[00:18:15.280 --> 00:18:19.440]   something, even though it's never happened. They can blend things together. But to do something
[00:18:19.440 --> 00:18:25.120]   that's truly creative, that is not just a blending of existing things, that requires searching
[00:18:25.120 --> 00:18:31.360]   through a space of possibilities and finding these hidden gems that are sort of hidden away in there
[00:18:31.360 --> 00:18:38.720]   somewhere. And that requires search. So I don't think we'll see systems that truly step beyond
[00:18:38.720 --> 00:18:42.160]   their training data until we have powerful search in the process.
[00:18:42.160 --> 00:18:47.520]   So there are rumors that Google DeepMind is training newer models, and you don't have to
[00:18:47.520 --> 00:18:53.520]   comment on those specifically. But when you do that, if it's the case that search or something
[00:18:53.520 --> 00:18:58.160]   like that is required to go to the next level, are you training in a completely different way
[00:18:58.160 --> 00:19:04.880]   than, say, GPT-4 or other transformers are trained? I can't say much about how we're training. I think
[00:19:04.880 --> 00:19:12.080]   it's fair to say we're doing the sorts of scaling and training, roughly, that you see many people in
[00:19:12.080 --> 00:19:19.040]   the field doing. But we have our own take on it and our own different tricks and techniques.
[00:19:19.040 --> 00:19:23.200]   Okay, maybe we'll come back to it and get another answer on that. But let's talk about alignment
[00:19:23.200 --> 00:19:32.320]   briefly. So what will it take to align human-level and superhuman AIs? And it's interesting because
[00:19:32.320 --> 00:19:36.640]   the sorts of reinforcement learning and self-play kinds of setups that are popular now, like
[00:19:36.640 --> 00:19:43.760]   Constitution AI or RLHF, DeepMind obviously has expertise in it for decades longer. So I'm curious
[00:19:43.760 --> 00:19:49.120]   what you think of the current landscape and how DeepMind pursues that problem of safety towards
[00:19:49.120 --> 00:19:52.800]   human-level models. So do you want to know about what we're currently doing, or do you want me to
[00:19:52.800 --> 00:19:55.840]   have a stab at what I think needs to be done? Needs to be done.
[00:19:55.840 --> 00:20:00.400]   Needs to be done. So in terms of what we're currently doing, we're doing lots of things.
[00:20:00.400 --> 00:20:04.880]   We're doing interpretability, we're doing process supervision, we're doing red teaming,
[00:20:04.880 --> 00:20:11.120]   we're doing evaluation for dangerous capabilities, we're doing work on institutions and governance
[00:20:11.120 --> 00:20:16.000]   and tons of stuff. There's lots of different things. Anyway, what do I think needs to be done?
[00:20:17.120 --> 00:20:24.880]   So I think that powerful machine learning, powerful AGI is coming in some time.
[00:20:24.880 --> 00:20:32.000]   And if the system is really capable, really intelligent, really powerful,
[00:20:32.000 --> 00:20:36.960]   trying to somehow contain it or limit it is probably not a winning strategy because these
[00:20:36.960 --> 00:20:42.400]   systems ultimately will be very, very capable. So what you have to do is you have to align it.
[00:20:42.400 --> 00:20:50.880]   You have to get it so it's fundamentally a highly ethical value-aligned system from the get-go.
[00:20:50.880 --> 00:21:00.640]   How do you do that? Well, maybe this is slightly naive, but this is my take on it.
[00:21:00.640 --> 00:21:06.720]   How do people do it? If you have a really difficult ethical decision in front of you,
[00:21:06.720 --> 00:21:14.560]   what do you do? Well, you don't just do the first thing that comes to mind because there could be a
[00:21:14.560 --> 00:21:19.040]   lot of emotions involved in other things. It's a difficult problem. So what you have to do is you
[00:21:19.040 --> 00:21:23.040]   have to calm yourself down, you've got to sit down, and you've got to think about it. You've
[00:21:23.040 --> 00:21:27.760]   got to think, well, okay, what could I do? I could do this, I could do this, I could do this.
[00:21:27.760 --> 00:21:34.640]   If I do each of these things, what will happen? And then you have to think about,
[00:21:34.640 --> 00:21:39.760]   so that requires a model of the world, and then you have to think about ethically,
[00:21:39.760 --> 00:21:46.000]   how do I view each of these different actions and the possibilities of what may happen from it?
[00:21:46.000 --> 00:21:53.440]   What is the right thing to do? And as you think about all the different possibilities and your
[00:21:53.440 --> 00:21:57.920]   actions and what can follow from them and how it aligns with your values and your ethics,
[00:21:57.920 --> 00:22:03.360]   you can then come to some conclusion of what is really the best choice that you should be making
[00:22:03.360 --> 00:22:09.840]   if you want to be really ethical about this. I think AI systems need to essentially do the
[00:22:09.840 --> 00:22:14.000]   same thing. So when you sample from a foundational model at the moment,
[00:22:14.000 --> 00:22:20.320]   it's like it's blurting out the first thing. It's like system one, if you like, from psychology,
[00:22:20.320 --> 00:22:28.800]   from Kahneman, right? That's not good enough. And if we do RLHF, or what's it called? I can't
[00:22:28.800 --> 00:22:34.320]   remember. Anyway, it's the AI version without the human feedback. RAIF, is that what it is?
[00:22:34.320 --> 00:22:37.920]   Oh, gosh, I'm confusing myself. Anyway, constitutional AI tries to do that sort of
[00:22:37.920 --> 00:22:44.160]   thing. You're trying to fix the underlying system one in a sense, right? And that can shift the
[00:22:44.160 --> 00:22:48.800]   distribution and that can be very helpful, but it's a very high dimensional distribution and
[00:22:48.800 --> 00:22:54.080]   you're sort of poking it in a whole lot of points. And so it's not likely to be a very robust
[00:22:54.080 --> 00:22:59.520]   solution, right? It's like trying to train yourself out of a bad habit. You can sort of
[00:22:59.520 --> 00:23:05.760]   do it eventually. But what you need to do is you need to have a system two. You need the system
[00:23:05.760 --> 00:23:11.680]   to not just sample from the model, you need the system to go, "Okay, I'm going to reason this
[00:23:11.680 --> 00:23:15.600]   through. I'm going to do like step-by-step reasoning. What are the options in front of me?
[00:23:15.600 --> 00:23:20.400]   I'm going to use my world model now and I'm going to use a good world model to understand
[00:23:20.400 --> 00:23:25.680]   what's likely to happen from each of these options and then reason about each of these
[00:23:25.680 --> 00:23:30.800]   from an ethical perspective." So you need a system which has a deep understanding of the
[00:23:30.800 --> 00:23:34.960]   world, has a good world model, it has a good understanding of people, it has a good understanding
[00:23:34.960 --> 00:23:40.880]   of ethics, and it has robust and very reliable reasoning. And then you set it up in such a way
[00:23:40.880 --> 00:23:46.240]   that it applies this reasoning and this understanding of ethics to analyze the different
[00:23:46.240 --> 00:23:52.160]   options which are in front of it and then execute on which is the most ethical way forwards.
[00:23:52.160 --> 00:23:56.000]   But I think when a lot of people think about the fundamental alignment problem,
[00:23:56.000 --> 00:24:00.800]   the worry is not that it's not going to have a world model necessary to understand
[00:24:00.800 --> 00:24:05.760]   its actions, or sorry, to understand the effects of its actions. I guess it's one
[00:24:05.760 --> 00:24:12.480]   worry but not the main worry. The main worry is that the effects it cares about are not the ones
[00:24:12.480 --> 00:24:16.880]   we will care about. And so even if you improve its systems you're thinking and do better planning,
[00:24:16.880 --> 00:24:20.560]   the fundamental problem of we have this really nuanced values about what we want,
[00:24:20.560 --> 00:24:25.920]   how do we communicate those values and make sure they're reinforced in the AI?
[00:24:25.920 --> 00:24:30.960]   It needs not just a good model of the world, but it needs a really good understanding of ethics.
[00:24:30.960 --> 00:24:35.840]   And we need to communicate to the system what ethics and values it should be following.
[00:24:35.840 --> 00:24:40.240]   And how do we do that in a way that we can be confident that a human level,
[00:24:40.240 --> 00:24:44.880]   or eventually a superhuman level model will preserve those values or learn them in the first
[00:24:44.880 --> 00:24:50.720]   place? Well, it should preserve them because if it's making all its decisions based on a good
[00:24:50.720 --> 00:24:55.920]   understanding of ethics and values, and it's consistent in doing this, it shouldn't take
[00:24:55.920 --> 00:24:59.600]   actions which undermine that. That would be inconsistent. Right. So then how do we get to
[00:24:59.600 --> 00:25:04.400]   the point where it's learned them in the first place? Yeah, that's the challenge. We need to
[00:25:04.400 --> 00:25:11.360]   have systems. The way I think about it is this, to have a profoundly ethical AI system, it also
[00:25:11.360 --> 00:25:16.080]   has to be very, very capable. It needs a really good world model, a really good understanding
[00:25:16.080 --> 00:25:20.560]   of ethics, and it needs really good reasoning. Because if you don't have any of those things,
[00:25:20.560 --> 00:25:28.000]   how can you possibly be consistently profoundly ethical? You can't. So we actually need better
[00:25:28.000 --> 00:25:33.040]   reasoning, better understanding of the world and better understanding of ethics in our systems.
[00:25:33.040 --> 00:25:36.880]   Right. So it seems to me the former two would just come along for the ride as these models
[00:25:36.880 --> 00:25:41.680]   get more powerful. Yeah. So that's a nice property because it's actually a capabilities thing.
[00:25:41.680 --> 00:25:46.080]   Right. But then if the third one is a bottleneck, or if the third one is a thing that doesn't come
[00:25:46.080 --> 00:25:50.800]   along with the AI itself, what is the actual technique to make sure that that happens?
[00:25:50.800 --> 00:25:54.480]   The third one, sorry. The ethical model. What do humans value?
[00:25:54.480 --> 00:26:00.800]   Well, we've got a couple of problems. First of all, we need to decide, we should train the system
[00:26:00.800 --> 00:26:06.160]   on ethics generally. I mean, there's a lot of lectures and papers and books and all sorts of
[00:26:06.160 --> 00:26:11.920]   things. So it understands human ethics well. And we need to make sure it understands humans ethics
[00:26:11.920 --> 00:26:20.480]   well, because that's important, at least as well as a very good ethicist. And we then need to decide,
[00:26:20.480 --> 00:26:26.480]   okay, of this sort of general understanding of ethics, what do we want the system to actually
[00:26:27.280 --> 00:26:33.600]   value? And what sort of ethics do we want it to apply? Now, that's not a technical problem.
[00:26:33.600 --> 00:26:42.240]   That's a problem for society and ethicists and so on to come up with. Now, I'm not sure there's
[00:26:42.240 --> 00:26:47.280]   such a thing as true or correct optimal ethics or something like that. But I'm pretty sure
[00:26:47.280 --> 00:26:56.640]   that it's possible to come up with a set of ethics, which is much better than the so-called
[00:26:56.640 --> 00:27:02.000]   doomers worry about in terms of the behavior of these AGI systems. And then what you do is you
[00:27:02.000 --> 00:27:09.360]   engineer the system to actually follow these things. So every time it makes a decision,
[00:27:09.360 --> 00:27:16.480]   it does an analysis using a deep understanding of the world and of ethics and very robust and
[00:27:16.480 --> 00:27:21.920]   precise reasoning to do an ethical analysis of what it's doing. And of course, we'd want lots
[00:27:21.920 --> 00:27:27.600]   of other things. We want people checking these processes of reasoning. We'd want people verifying
[00:27:27.600 --> 00:27:33.120]   that it's behaving itself in terms of how it reaches these conclusions. But I still feel
[00:27:33.120 --> 00:27:37.600]   like I don't understand how that fundamental problem of making sure it follows that ethic,
[00:27:37.600 --> 00:27:42.160]   because presumably, it has Mao's little red book, so it understands Maoist ethics and understands
[00:27:42.160 --> 00:27:48.000]   all these other ethics. How do we make sure the ethic that we say, this is the one we've decided
[00:27:48.000 --> 00:27:52.560]   at this society and so on today, that is the one it ends up following and not the other ones it
[00:27:52.560 --> 00:27:57.200]   understands. Right. So you have to specify to the system, these are the ethical principles that you
[00:27:57.200 --> 00:28:01.120]   should follow. And how do we make sure it does that? We have to check it as it's doing it. We
[00:28:01.120 --> 00:28:08.000]   have to assure ourselves that it is consistently following these ethical principles, at least,
[00:28:08.000 --> 00:28:13.360]   I mean, I'm not sure there's such a thing as optimally, but at least as well as a group of
[00:28:13.360 --> 00:28:18.000]   human experts. Are you worried that if you do it the default way, which is just reinforcing it
[00:28:18.000 --> 00:28:21.440]   whenever it seems to be following them, you could be training deception as well?
[00:28:21.440 --> 00:28:31.680]   Reinforcement has some dangerous aspects to it. I think it's actually more robust to
[00:28:31.680 --> 00:28:39.760]   check the process of reasoning and check its understanding of ethics. So to reassure
[00:28:39.760 --> 00:28:43.920]   ourselves that the system has a really good understanding of ethics, it should be grilled
[00:28:43.920 --> 00:28:51.440]   for some time to try to really pull apart its understanding and make sure it has a very robust.
[00:28:51.440 --> 00:28:58.240]   And then also if it's deployed, we should have people constantly looking for how the decision
[00:28:58.240 --> 00:29:04.400]   is making and the reasoning process that goes into those decisions to try to understand how
[00:29:04.400 --> 00:29:09.600]   that is correctly reasoning about these types of things. Speaking of which, do you at Google
[00:29:09.600 --> 00:29:15.280]   DeepMind have some sort of framework for this? This is not so much a Google DeepMind perspective
[00:29:15.280 --> 00:29:21.360]   on this. This is my take on how I think we need to do this kind of thing. There are many different
[00:29:21.360 --> 00:29:27.040]   views and there are different variants on these sorts of ideas as well. So then do you personally
[00:29:27.040 --> 00:29:31.200]   think there needs to be some sort of framework for as you arrive at certain capabilities,
[00:29:31.200 --> 00:29:36.080]   these are the concrete safety benchmarks that you must have instated at this point or you should
[00:29:36.080 --> 00:29:40.800]   pause or slow down or something? I think that's a sensible thing to do. It's actually quite hard to
[00:29:40.800 --> 00:29:46.000]   do. And there are some people thinking about, I know Anthropx has put out some things like that.
[00:29:46.000 --> 00:29:52.720]   We're thinking about similar things. Actually, putting concrete things down is actually quite
[00:29:52.720 --> 00:29:56.480]   a hard thing to do. So I think it's an important problem and I certainly encourage people to work
[00:29:56.480 --> 00:30:03.120]   on it. It's interesting because you have these blog posts that you wrote when you started
[00:30:03.120 --> 00:30:12.240]   DeepMind back in 2008 where you talk about the motivation was to accelerate safety. On net,
[00:30:12.240 --> 00:30:16.320]   what do you think the impact of DeepMind has been on safety versus capabilities?
[00:30:19.360 --> 00:30:27.360]   Interesting. I don't know. It's hard to judge, actually.
[00:30:27.360 --> 00:30:36.320]   I've been worried about AGI safety for a long time, well before DeepMind.
[00:30:36.320 --> 00:30:43.360]   But it was always really hard to hire people, actually, particularly in the early days to work
[00:30:43.360 --> 00:30:50.880]   on AGI safety. I think back in 2013 or so, I think we had the first hire and he only agreed to do it
[00:30:50.880 --> 00:30:58.400]   part-time because he didn't want to drop all the capabilities work because the impact it would have
[00:30:58.400 --> 00:31:02.640]   on his career and stuff. And this was someone who had already previously been publishing in AGI
[00:31:02.640 --> 00:31:12.080]   safety. So, yeah, I don't know. It's hard to know what is the counterfactual if we weren't there
[00:31:12.080 --> 00:31:23.120]   doing it. I think we have been a group that's been talked about this openly. I've talked about
[00:31:23.120 --> 00:31:28.720]   this on many occasions, the importance of it. We've been hiring people to work on these topics.
[00:31:28.720 --> 00:31:35.040]   I know a lot of other people in the area and I've talked to them over many, many years. I've known
[00:31:35.040 --> 00:31:41.760]   Dario since 2005 or something or rather. We've talked on and off about AGI safety and so on. So,
[00:31:41.760 --> 00:31:46.880]   I don't know. The impact that DeepMind has had, I guess we were the first,
[00:31:46.880 --> 00:31:55.680]   I'd say the first AGI company. And as the first AGI company, we always had an AGI safety group.
[00:31:55.680 --> 00:32:02.320]   We've been publishing papers in this for many years. I think that's lent some credibility
[00:32:02.320 --> 00:32:06.560]   to the area when people see, "Oh, here's a AGI." I mean, AGI was a, you know,
[00:32:06.560 --> 00:32:11.280]   there was a fringe term not that long ago. And this person's doing AGI safety and they're DeepMind?
[00:32:11.280 --> 00:32:16.880]   Oh, okay. I hope that sort of, you know, creates some space for people.
[00:32:16.880 --> 00:32:21.600]   And where do you think AI progress itself would have been without DeepMind? And this is not just
[00:32:21.600 --> 00:32:24.880]   a point that people make about DeepMind. I think this is a general point we make about
[00:32:24.880 --> 00:32:30.400]   OpenAI and Anthropic as well, that these people went into the business to accelerate safety
[00:32:30.400 --> 00:32:33.600]   and sort of the net effect might've been to accelerate capabilities far more.
[00:32:33.600 --> 00:32:39.680]   Right. I think we have accelerated capabilities, but again, the counterfactuals are quite difficult.
[00:32:39.680 --> 00:32:45.440]   I mean, we didn't do ImageNet, for example, and ImageNet, I think, was very influential in
[00:32:45.440 --> 00:32:54.000]   attracting investment to the field. We did do AlphaGo and that changed some people's minds.
[00:32:54.960 --> 00:32:59.920]   But, you know, the community is a lot bigger than just DeepMind. I mean, we have,
[00:32:59.920 --> 00:33:05.680]   well, not so much now, but because there are a number of other, you know,
[00:33:05.680 --> 00:33:10.560]   players with significant resources. But if you went back more than five years in the future,
[00:33:10.560 --> 00:33:16.960]   we were able to do bigger projects with bigger teams and take on more ambitious things than
[00:33:16.960 --> 00:33:22.560]   a lot of the smaller academic groups, right? And so the sort of nature of the type of work
[00:33:22.560 --> 00:33:28.880]   we could do was a bit different. And that, I think, that affected the dynamics in some ways.
[00:33:28.880 --> 00:33:32.640]   But, you know, the community is much, much bigger than, say, DeepMind. So
[00:33:32.640 --> 00:33:38.400]   maybe we've sped things up a bit, but I think a lot of these things would have happened
[00:33:38.400 --> 00:33:46.160]   before too long anyway. I think these often good ideas are kind of in the air and, you know,
[00:33:46.160 --> 00:33:51.040]   as a researcher, you know, when sometimes you publish something or you're about to publish
[00:33:51.040 --> 00:33:54.800]   something, you see somebody else who's got a very similar idea coming out with some good results.
[00:33:54.800 --> 00:34:01.280]   I think often it's the time is right for things. So, you know, I find it very hard to reason about
[00:34:01.280 --> 00:34:06.240]   the counterfactuals there. Speaking of the early years, it's really interesting that in 2009,
[00:34:06.240 --> 00:34:12.880]   you had a blog post where you say, "My modal expectation of when we get human-level AI is 2025.
[00:34:12.880 --> 00:34:19.760]   Expected value is 2028." And this is before deep learning. This is when nobody's talking about AI.
[00:34:19.760 --> 00:34:23.520]   And it turns out, like, if you, if the trends continue, this is not an unreasonable prediction.
[00:34:23.520 --> 00:34:28.160]   This was, how did you, I mean, before all these trends came into effect, how did you
[00:34:28.160 --> 00:34:31.360]   have that accurate an estimate? Well, first, let's say it's not before deep learning.
[00:34:31.360 --> 00:34:38.560]   Deep learning was getting started around 2008. Oh, sorry. I meant to say before ImageNet.
[00:34:38.560 --> 00:34:48.560]   Before ImageNet. That was 2012. Yeah. So, well, I first formed those beliefs in about 2001 after
[00:34:48.560 --> 00:34:54.800]   reading Ray Kurzweil's The Age of Spiritual Machines. And I came to the conclusion he was,
[00:34:54.800 --> 00:35:02.160]   there was two really important points that in his book that I came to believe is true. One is that
[00:35:02.160 --> 00:35:09.200]   computational power would grow exponentially for at least a few decades. And that the quantity
[00:35:09.200 --> 00:35:15.680]   of data in the world would grow exponentially for a few decades. And when you have exponentially
[00:35:15.680 --> 00:35:22.720]   increasing quantities of computation and data, then the value of highly scalable algorithms
[00:35:22.720 --> 00:35:28.000]   gets higher and higher. So then there's a lot of incentive to make a more scalable algorithm to
[00:35:28.000 --> 00:35:33.840]   harness all this computing data. And so I thought it would be very likely that we'll start to
[00:35:33.840 --> 00:35:39.600]   discover scalable algorithms to do this. And then there's a positive feedback between all these
[00:35:39.600 --> 00:35:44.320]   things, because if your algorithm gets better at harnessing computing data, then the value of the
[00:35:44.320 --> 00:35:48.640]   data and the compute goes up because it can be more effectively used. And so that drives more
[00:35:48.640 --> 00:35:53.920]   investment to these areas. If your compute performance goes up, then the value of the
[00:35:53.920 --> 00:35:57.920]   data goes up because you can utilize more data. So there are positive feedback loops between all
[00:35:57.920 --> 00:36:03.760]   these things. So that was the first thing. And then the second thing was just looking at the trends.
[00:36:03.760 --> 00:36:13.040]   If the scalable algorithms were to be discovered, then during the 2020s, it should be possible to
[00:36:13.040 --> 00:36:18.320]   start training models on significantly more data than a human would experience in a lifetime.
[00:36:18.320 --> 00:36:23.760]   And I figured that that would be a time where big things would start to happen,
[00:36:23.760 --> 00:36:29.520]   and that would eventually unlock AGI. So that was my reasoning process. And I think we're now
[00:36:29.520 --> 00:36:34.400]   at that first part. I think we can start training models now where the scale of the data is beyond
[00:36:34.400 --> 00:36:38.080]   what a human can experience in a lifetime. So I think this is the first unlocking step.
[00:36:39.120 --> 00:36:46.240]   And so yeah, I think there's a 50% chance that something like 2028. Now, it's just a 50% chance.
[00:36:46.240 --> 00:36:50.160]   I mean, I'm sure what's going to happen. It's going to get to 2029 and someone's going to say,
[00:36:50.160 --> 00:36:56.080]   oh, Shane, you were wrong. It's like, come on, it's 50% chance. So yeah, I think it's
[00:36:56.080 --> 00:37:03.200]   entirely plausible. Yeah, it's a 50% chance it could happen by 2028. But I'm not going to be
[00:37:03.200 --> 00:37:11.200]   surprised if it doesn't happen by then. You often hit unexpected problems in research and science,
[00:37:11.200 --> 00:37:15.440]   but sometimes things take longer than you expect. If there was a problem that caused it,
[00:37:15.440 --> 00:37:19.440]   if we're in 2029 and it hasn't happened yet, looking back, what would be the most likely
[00:37:19.440 --> 00:37:30.480]   reason that would be the case? I don't know. I don't know. At the moment, it looks to me
[00:37:31.680 --> 00:37:38.000]   like all the problems are likely solvable with a number of years of research. That's my current
[00:37:38.000 --> 00:37:43.600]   sense. And what does the time from here to 2028 look like if the 2028 ends up being the year?
[00:37:43.600 --> 00:37:47.920]   Is it just we have trillions of dollars of economic impact in the meantime and
[00:37:47.920 --> 00:37:55.360]   the world gets crazy or what happens? I think what you'll see is the existing models maturing.
[00:37:55.360 --> 00:38:01.520]   They'll be less delusional, much more factual. They'll be more up to date on what's currently
[00:38:01.520 --> 00:38:08.960]   going on when they answer questions. They'll become multimodal much more than they currently
[00:38:08.960 --> 00:38:14.720]   are. And this will just make them much more useful. So I think probably what we'll see
[00:38:14.720 --> 00:38:21.600]   more than anything is just loads of great applications for the coming years. I think
[00:38:21.600 --> 00:38:28.400]   there can be some misuse cases as well. I'm sure somebody will come up with something to do with
[00:38:28.400 --> 00:38:34.640]   these models that is quite unhelpful. But my expectation for the coming years is mostly a
[00:38:34.640 --> 00:38:41.040]   positive one. We'll see all kinds of really impressive, really amazing applications for
[00:38:41.040 --> 00:38:47.680]   the coming years. And on the safety point, you mentioned these different research directions
[00:38:47.680 --> 00:38:51.040]   that are out there and that you are doing internally in DeepMind as well. Interoperability,
[00:38:51.600 --> 00:39:01.920]   RAIF and so on. Which are you most optimistic about? I don't know. I don't pick favorites.
[00:39:01.920 --> 00:39:06.560]   It's hard picking favorites. I know the people working on all these areas.
[00:39:06.560 --> 00:39:18.320]   I think things of the sort of system two flavor. There's a work we have going on that Jeffrey
[00:39:18.320 --> 00:39:24.400]   Irving leads called Deliberative Dialogue, which kind of has the system two flavor where
[00:39:24.400 --> 00:39:32.880]   you have this sort of debate takes place about the actions that an agent could take or what's
[00:39:32.880 --> 00:39:38.880]   the correct answer to something like this. And people then can sort of review these debates and
[00:39:38.880 --> 00:39:46.240]   so on. And they use these sort of AI algorithms to help them judge the correct outcomes and so on.
[00:39:46.240 --> 00:39:53.840]   And so this is sort of meant to be a way in which to try to scale the alignment to sort of
[00:39:53.840 --> 00:40:01.280]   increasingly powerful systems. So I think things of that kind of flavor, I think have quite a lot
[00:40:01.280 --> 00:40:06.160]   of promise in my opinion, but that's kind of quite a broad category. There are many different topics
[00:40:06.160 --> 00:40:13.440]   within that. That's interesting. So you mentioned two areas in which algorithms need to improve.
[00:40:13.440 --> 00:40:17.840]   One is the episodic memory and the other is the system two thinking. Are those two related or are
[00:40:17.840 --> 00:40:25.280]   they two separate drawbacks? I think they're fairly separate,
[00:40:25.280 --> 00:40:32.080]   but they can be somewhat related. So you can learn different ways of thinking through problems
[00:40:32.080 --> 00:40:37.120]   and actually learn about this rapidly using your episodic memory. So all these different
[00:40:37.120 --> 00:40:42.240]   systems and subsystems interact, so they're never completely separate. But I think conceptually,
[00:40:42.240 --> 00:40:46.800]   you can probably think of them as quite separate things. I think delusions and factuality is
[00:40:46.800 --> 00:40:52.960]   another area that's going to be quite important and particularly important in lots of applications.
[00:40:52.960 --> 00:40:59.040]   If you want a model that writes creative poetry, then that's fine because you want to be able to
[00:40:59.040 --> 00:41:03.440]   be very free to suggest all kinds of possibilities and so on. You're not really constrained by a
[00:41:03.440 --> 00:41:10.160]   specific reality. Whereas if you want something that's in a particular application, normally you
[00:41:10.160 --> 00:41:15.120]   have to be quite concrete about what's currently going on and what is true and what is not true
[00:41:15.120 --> 00:41:21.040]   and so on. Models are a little bit freewheeling when it comes to truth and creativity at the
[00:41:21.040 --> 00:41:27.680]   moment. That, I think, limits their applications in many ways. So final question is this. You've
[00:41:27.680 --> 00:41:34.000]   been in this field for over a decade, much longer than many others, and you've seen these different
[00:41:34.000 --> 00:41:39.920]   landmarks, ImageNet, transformers. What do you think the next landmark will look like?
[00:41:39.920 --> 00:41:50.160]   I think the next landmark that people will think back to and remember is going much more fully
[00:41:50.160 --> 00:41:58.160]   multimodal, I think. Because I think that will open out the sort of understanding that you see
[00:41:58.160 --> 00:42:04.160]   in language models into a much larger space of possibilities. And when people think back,
[00:42:04.160 --> 00:42:08.560]   they'll think about, "Oh, those old-fashioned models, they just did like chat. They just did
[00:42:08.560 --> 00:42:14.880]   text." It just felt like a very narrow thing. Whereas now they understand when you talk to them,
[00:42:14.880 --> 00:42:20.080]   and they understand images and pictures and video, and you can show them things or things like that,
[00:42:20.080 --> 00:42:23.680]   and they will have much more understanding of what's going on. And it'll feel like the system's
[00:42:23.680 --> 00:42:28.800]   kind of opened up into the world in a much more powerful way.
[00:42:28.800 --> 00:42:33.600]   Do you mind if I ask a follow-up on that? So Chad GPT just released their multimodal feature,
[00:42:33.600 --> 00:42:39.440]   and then you in DeepMind, you had the Gato paper where you have this one model, you can images,
[00:42:39.440 --> 00:42:46.800]   even actions, video games, whatever you can throw in there. And so far, it hasn't percolated as
[00:42:46.800 --> 00:42:51.520]   much as even like Chad GPT initially from GPT-3 or something. What explains that? Is it just that
[00:42:51.520 --> 00:42:54.400]   people haven't learned to use multimodality, they're not powerful enough yet?
[00:42:54.400 --> 00:43:00.720]   I think it's early days. I think there's, you can see promise there, understanding images and things
[00:43:00.720 --> 00:43:06.880]   more and more. But I think it's early days in this transition, is when you start really digesting a
[00:43:06.880 --> 00:43:12.560]   lot of video and other things like that, that the systems will start having a much more grounded
[00:43:12.560 --> 00:43:17.200]   understanding of the world and all kinds of other aspects. And then when that works well,
[00:43:17.200 --> 00:43:22.720]   that will open up naturally lots and lots of new applications and all sorts of new possibilities
[00:43:22.720 --> 00:43:28.000]   because you're not confined to text chat anymore. The new avenues of training data as well, right?
[00:43:28.000 --> 00:43:33.360]   Yeah, new training data and all kinds of different applications that aren't just purely textual
[00:43:33.360 --> 00:43:39.360]   anymore. And what are those applications? Well, probably a lot of them we can't even imagine at
[00:43:39.360 --> 00:43:43.760]   the moment, because there are just so many possibilities once you can start dealing with
[00:43:43.760 --> 00:43:47.920]   all sorts of different modalities in a consistent way. Awesome. Shane, I think that's an excellent
[00:43:47.920 --> 00:43:50.960]   place to leave it off. Thank you so much for coming on the podcast. Thank you.
[00:43:50.960 --> 00:43:57.440]   Hey everybody. I hope you enjoyed that episode. As always, the most helpful thing you can do
[00:43:57.440 --> 00:44:01.760]   is to share the podcast. Send it to people you think might enjoy it, put it in Twitter,
[00:44:01.760 --> 00:44:15.200]   your group chats, etc. It just splits the world. Appreciate you listening. I'll see you next time.
[00:44:15.200 --> 00:44:18.200]   [Music]

