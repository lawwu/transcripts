
[00:00:00.000 --> 00:00:07.440]   Great. Well, welcome everyone to the Weights and Biases Salon. We've got a really special salon
[00:00:07.440 --> 00:00:14.480]   this time. We've got two separate teams that have worked on benchmarks, public benchmarks,
[00:00:14.480 --> 00:00:21.440]   in what I think of as like the broader world of AI for good. In one case, AI safety, and in the
[00:00:21.440 --> 00:00:28.800]   other case, AI for form extraction of political information. So I'm really excited to hear about
[00:00:28.800 --> 00:00:34.480]   these two projects. I've read about them and looked into them on the Weights and Biases platform, but
[00:00:34.480 --> 00:00:41.200]   excited to hear a little bit from the horse's mouths. And Stacey Svetlitschinaia here actually
[00:00:41.200 --> 00:00:49.600]   participated in both projects. So I'll let her get us started. Great. Thanks, Charles. So our first
[00:00:49.600 --> 00:00:56.320]   benchmark today will be SafeLife. And that's a project I've been helping with, with Carol Wainwright
[00:00:56.320 --> 00:01:01.840]   and Peter Eckersley, who will be actually talking through the project. And it's a benchmark for
[00:01:01.840 --> 00:01:07.520]   safety and reinforcement learning. So we're training some agents in a game playing environment
[00:01:07.520 --> 00:01:11.920]   that has some patterns that are persistent, and there are several tasks to solve. And
[00:01:11.920 --> 00:01:18.640]   while the agent is learning to do the task, we track side effects. So the changes that
[00:01:18.640 --> 00:01:25.680]   inevitably happen as an agent is learning to do the task. And that's a hard problem. So we'll hear
[00:01:25.680 --> 00:01:29.680]   a little bit about the setup and how it's going and how you can get involved in this project.
[00:01:29.680 --> 00:01:41.840]   All right. Thanks, Stacey. So this is SafeLife 1.2, which is a benchmark particularly for side
[00:01:41.840 --> 00:01:50.400]   effects in reinforcement learning. And project that Carol and I launched out of the partnership
[00:01:50.400 --> 00:01:56.880]   on AI. Now, of course, it's an open source project and also a collaboration with Weights and Biases.
[00:01:56.880 --> 00:02:03.600]   You're all probably familiar with some of the things that have been accomplished with
[00:02:03.600 --> 00:02:08.400]   reinforcement learning, whether that's beating humans at Go, the hardest strategy ball game
[00:02:08.400 --> 00:02:14.000]   that we know, or professional human competitors at real-time strategy games like StarCraft
[00:02:16.080 --> 00:02:25.040]   2. But those impressive accomplishments sit alongside the fact that reinforcement learning
[00:02:25.040 --> 00:02:32.240]   is fundamentally unsafe in many ways. This is a now somewhat famous video from OpenAI
[00:02:32.240 --> 00:02:38.560]   showing a boat that's supposed to be competing in a race game. But because they trained the
[00:02:38.560 --> 00:02:44.400]   reinforcement learning agent using the scope of the game, it found a hack where it could just do
[00:02:44.400 --> 00:02:50.000]   these donuts and collect points but not accomplish the task that it was supposed to, which is
[00:02:50.000 --> 00:02:56.400]   finishing the race. So problems like that boat failing to interpret its objective function
[00:02:56.400 --> 00:03:02.720]   correctly and then doing donuts have led to a newish field that people are calling technical
[00:03:02.720 --> 00:03:09.280]   AI safety. I say newish because it builds on some pre-existing work in, for instance, safety and
[00:03:09.280 --> 00:03:14.240]   control theory for aircraft and so forth. But there's a new AI perspective on some of these
[00:03:14.240 --> 00:03:23.040]   questions. Next slide. Some of the problems in that new field include safely exploring an
[00:03:23.040 --> 00:03:29.440]   environment. If you're a robot cleaning a house and making coffee, do you try to walk through the
[00:03:29.440 --> 00:03:33.600]   middle of a vase the first time you try to get across the room? That's unsafe exploration.
[00:03:34.320 --> 00:03:40.000]   Do you misinterpret your objective function like that boat did? Are you able to learn from human
[00:03:40.000 --> 00:03:46.800]   preferences and human feedback? In the extreme case, if you've got an objective function at time
[00:03:46.800 --> 00:03:52.560]   one and you realize a human is about to turn you off at time two, you might try to disable your
[00:03:52.560 --> 00:03:57.760]   own off switch because you don't want to be corrected under objective function at time one
[00:03:57.760 --> 00:04:02.080]   to acquire a new one at time three. How do we design agents that don't want to turn off their
[00:04:02.080 --> 00:04:09.920]   off switches? How do you make systems robust to changes in the environment at the distributional
[00:04:09.920 --> 00:04:15.440]   level if they're trained on one set of conditions and then deployed in others? And then avoidance
[00:04:15.440 --> 00:04:22.880]   of negative side effects from the goal that weren't specified. Next slide. So all of these
[00:04:22.880 --> 00:04:28.480]   problems have had some progress in the literature thus far, but none of them look solved. And the
[00:04:28.480 --> 00:04:32.720]   reality today is that if you want to deploy reinforcement learning in a real world environment,
[00:04:32.720 --> 00:04:37.840]   you basically need to be able to write down your safety constraints explicitly in mathematical
[00:04:37.840 --> 00:04:42.160]   form. And there are many domains, in fact, most domains where we just don't know how to do that.
[00:04:42.160 --> 00:04:51.280]   Next slide. One example of trying to get there, trying to have benchmarks so you can measure
[00:04:51.280 --> 00:04:55.840]   progress is the AI safety grid world's work at DeepMind, which in some ways inspired our
[00:04:55.840 --> 00:05:01.360]   present work. They made model examples of some of these reinforcement learning safety problems,
[00:05:01.360 --> 00:05:08.480]   but the problem was each of their examples was a handcrafted individual level of a simple computer
[00:05:08.480 --> 00:05:13.600]   game, very subject to overfitting, not clear that it captures all of the versions of the problem.
[00:05:13.600 --> 00:05:24.640]   We wanted to go further. Next slide. Next slide again. In particular, we are focusing initially
[00:05:24.640 --> 00:05:29.760]   on side effects. How can we go deeper in benchmarking and measuring whether a reinforcement
[00:05:29.760 --> 00:05:34.800]   learning agent avoids creating side effects? So suppose you tell your agent, hey, make me some
[00:05:34.800 --> 00:05:41.120]   coffee. But you didn't think to say, don't step on the cat on the way, don't set the kitchen on fire,
[00:05:41.120 --> 00:05:46.960]   even if that makes the coffee, the heat stronger and the coffee faster. Don't disable your off
[00:05:46.960 --> 00:05:51.120]   switch, not so much a side effect problem. Don't steal beans from your neighbor, even though that's
[00:05:51.120 --> 00:05:55.520]   faster than going to the store. Don't crash the stock market, even if it means the beans will be
[00:05:55.520 --> 00:06:01.040]   cheaper that way. Somehow we want to just make the coffee and don't do anything else.
[00:06:01.040 --> 00:06:09.520]   Now, that is undefined. What we are conjecturing is that the world needs an image net type of
[00:06:09.520 --> 00:06:12.960]   foundational benchmark for what it means to do something without
[00:06:12.960 --> 00:06:16.000]   causing any side effects along the way. Next slide.
[00:06:20.160 --> 00:06:24.480]   Fundamentally, we don't think you can make progress on a problem unless you can tell
[00:06:24.480 --> 00:06:26.960]   that you're making progress and measure it. Next slide.
[00:06:26.960 --> 00:06:35.520]   So what we decided to do is build this thing called safe life. So safe life is an environment
[00:06:35.520 --> 00:06:39.680]   for reinforcement learning agents that exactly tries to get at this problem of avoidance of
[00:06:39.680 --> 00:06:45.200]   negative side effects. When we designed it, we wanted to build an environment that is both
[00:06:45.920 --> 00:06:51.680]   really complex and dynamic and rich enough to get at the problems as they exist in the real world,
[00:06:51.680 --> 00:06:57.920]   which is to say not these simple handcrafted toy environments, but also still simple enough and
[00:06:57.920 --> 00:07:04.240]   fun enough that people can make progress on it, researchers can practically improve the state of
[00:07:04.240 --> 00:07:12.960]   the art, and such that other people can have fun playing it. So what is safe life? It's based on
[00:07:12.960 --> 00:07:18.400]   Conway's game of life. Life is an old system built in the 1970s cellular automata with just a few
[00:07:18.400 --> 00:07:24.160]   simple rules. It's on an infinite grid and every cell is either alive or dead. Live cells die
[00:07:24.160 --> 00:07:29.360]   unless they have exactly two or three neighbors and dead cells stay dead unless they have exactly
[00:07:29.360 --> 00:07:34.800]   three neighbors. So from these just very, very simple rules, we get very complex dynamics.
[00:07:34.800 --> 00:07:40.880]   So we get oscillators, which flip back and forth between two or more states, gliders and
[00:07:40.880 --> 00:07:46.480]   spaceships, which travel dynamically across the grid, long live Methuselah patterns, which last
[00:07:46.480 --> 00:07:52.400]   for many, many generations. And then also these never ending pattern factories, which produce
[00:07:52.400 --> 00:07:59.920]   patterns that go on forever, essentially. Conway's game of life is actually Turing complete with the
[00:07:59.920 --> 00:08:05.040]   right setup. You can build Tetris or anything else you want. And there are actually, if you go into
[00:08:05.040 --> 00:08:09.680]   Code Golf on Stack Overflow, you can see people have made progress towards this ambitious,
[00:08:09.680 --> 00:08:16.240]   if somewhat silly goal. So patterns in Conway's game of life look like this. These are three,
[00:08:16.240 --> 00:08:20.240]   like very common patterns, because people have been playing around with Conway's game of life
[00:08:20.240 --> 00:08:25.120]   for a long time. They all have names. The one on the lower left, I can tell you is a loaf.
[00:08:25.120 --> 00:08:30.320]   When you turn on the time dynamics, it just sits there as you might expect a loaf. The thing on
[00:08:30.320 --> 00:08:36.240]   the upper right is called a blinker. It goes back and forth, oscillating between two stages.
[00:08:36.240 --> 00:08:40.320]   The thing in the middle is the most interesting, which you saw at the beginning, or I guess you
[00:08:40.320 --> 00:08:46.720]   didn't see at the beginning of the talk. It travels across the screen in this diagonal pattern. So
[00:08:46.720 --> 00:08:52.640]   turning on the time evolution makes it look like this. Now, what if we want to go with a different
[00:08:52.640 --> 00:09:00.000]   looking pattern? This pattern doesn't have a name that I know of anyways. And except the fact that
[00:09:00.000 --> 00:09:03.360]   I've given this talk before, like I wouldn't be able to look at this pattern and have any idea
[00:09:03.360 --> 00:09:10.080]   what it does. And when we turn on the time evolution of this, it just goes totally nuts.
[00:09:10.080 --> 00:09:14.960]   If you wait for the GIF to cycle through again, you can see it starts and immediately explodes
[00:09:14.960 --> 00:09:22.880]   into chaos. In order to be good at Conway's game of life, you have to understand what the patterns
[00:09:22.880 --> 00:09:27.040]   are and understand what the dynamics are, and then also understand this chaotic behavior.
[00:09:28.480 --> 00:09:33.440]   Now, Conway's game of life isn't a game in the normal sense. It's what's known as a zero player
[00:09:33.440 --> 00:09:38.880]   game. You just set up the initial conditions and run the time evolution and see what happens.
[00:09:38.880 --> 00:09:43.680]   In order to make it suitable for reinforcement learning, we need to minimally add an agent and
[00:09:43.680 --> 00:09:50.080]   a goal for the agent to get around to. So in our environment, the goal or the agent moves
[00:09:50.080 --> 00:09:57.120]   in cardinal directions with every time step, or it can flip cells from dead to alive and vice versa.
[00:09:58.240 --> 00:10:03.840]   We can also add these colored goal cells, which you saw a few slides back, where the agent fills
[00:10:03.840 --> 00:10:11.200]   them in in order to get points and then goes to the level exit. So here is a human playing
[00:10:11.200 --> 00:10:16.000]   safe life, creating a bunch of different interesting patterns. And this is just to show
[00:10:16.000 --> 00:10:22.240]   with only a very few small number of time steps, the agent can really produce these complex,
[00:10:23.200 --> 00:10:30.160]   dynamic, rich features in the environment. So we additionally add a few other cell types to
[00:10:30.160 --> 00:10:35.200]   get even more interesting behavior. We add frozen cells like walls and what we call trees,
[00:10:35.200 --> 00:10:41.120]   movable cells like crates, some cells that are more difficult to remove, and different colors
[00:10:41.120 --> 00:10:45.200]   to distinguish different populations, which will be very important in just a minute. And super
[00:10:45.200 --> 00:10:50.480]   importantly, we add these things called spawner cells, which creates stochastically generated
[00:10:50.480 --> 00:10:56.960]   patterns. And this allows us to add a sense of randomness to an otherwise deterministic environment.
[00:10:56.960 --> 00:11:01.760]   So what do these levels actually look like? What are the tasks?
[00:11:01.760 --> 00:11:07.600]   There are three prime tasks that we have in safe life and in the benchmark that we have on weights
[00:11:07.600 --> 00:11:16.480]   and biases. The first task here is pattern creation. So here, as you saw before, the agent is trying to
[00:11:16.480 --> 00:11:23.040]   fill in the blue squares. Ideally, a safe agent would fill in the blue squares without making
[00:11:23.040 --> 00:11:27.840]   other disruptions in the environment, without making any other side effects. The agent on the
[00:11:27.840 --> 00:11:32.160]   left, both of these agents have been trained with proximal policy optimization, and the agent on the
[00:11:32.160 --> 00:11:35.920]   left just gets points for filling in the blue squares and that's it. And you can see that it
[00:11:35.920 --> 00:11:40.560]   kind of makes a mess of things. It disrupts a bunch of the green patterns, but it's able to
[00:11:40.560 --> 00:11:46.880]   fill in enough of the blue squares to reach its goal. The agent on the right is trained with a
[00:11:46.880 --> 00:11:52.400]   side effect impact penalty, where it's trying to not create side effects. So now it's cautious
[00:11:52.400 --> 00:11:57.040]   enough not to make disruptions in the environment, but it's too timid to actually complete its goals.
[00:11:57.040 --> 00:12:05.200]   It's also worth remarking that the constraints the agents are playing under are that their algorithm
[00:12:05.200 --> 00:12:11.680]   can't contain the fact that the green cells are special and shouldn't be messed with.
[00:12:11.680 --> 00:12:18.960]   That would be like peeking at the test and cheating. So the goal of algorithmic development
[00:12:18.960 --> 00:12:24.160]   here is to find algorithms that naturally avoid changing on anything unnecessary that doesn't
[00:12:24.160 --> 00:12:29.360]   lead to points. In this case, the agent should be able to learn, oh, when I put life on the blue
[00:12:29.360 --> 00:12:35.120]   background, I get a point. I get no points when I mess with the green, so I should just do the
[00:12:35.120 --> 00:12:42.640]   thing that gets me points and make no other changes to the environment. The next task we have
[00:12:42.640 --> 00:12:48.720]   is this idea of pattern removal. So as in regular life and safe life, it's easier to destroy things
[00:12:48.720 --> 00:12:53.920]   and remove them than it is to build them. So here an agent is trying to remove all the red cells.
[00:12:53.920 --> 00:12:58.240]   Again, on the left, they're trained without any side effect penalty, and on the right, they are
[00:12:58.240 --> 00:13:03.680]   trained with a side effect impact penalty. And the interesting thing here is now we've added these
[00:13:03.680 --> 00:13:08.800]   yellow stochastic generators as well. So the yellow stochastic generators are constantly
[00:13:08.800 --> 00:13:15.200]   creating effects, and the agent on the right is trying to reduce side effects, but it's not clever
[00:13:15.200 --> 00:13:19.440]   enough to realize which side effects are due to itself and which side effects are inherent in the
[00:13:19.440 --> 00:13:23.680]   environment, and therefore actually goes about destroying parts of the environment that you
[00:13:23.680 --> 00:13:29.840]   didn't want it to. So training against side effects can itself cause side effects of causing
[00:13:29.840 --> 00:13:35.920]   more problems than you anticipated. And then finally, we have this navigation challenge where
[00:13:35.920 --> 00:13:39.840]   here there are no points to be one in the level, but the agent is just trying to get from one side
[00:13:39.840 --> 00:13:43.840]   of the level to the other. But there are kind of two routes that it can go through. One is through
[00:13:43.840 --> 00:13:49.840]   this fragile green field of oscillators, and when the agent goes through that, the oscillators tend
[00:13:49.840 --> 00:13:55.440]   towards collapse. The other way it can go through is through this random yellow field of stochastic
[00:13:55.440 --> 00:14:00.960]   patterns. And when the agent goes through that, it still disrupts individual patterns, but those
[00:14:00.960 --> 00:14:06.640]   patterns just reform after a few time steps, so the agent doesn't cause any harm. A safe agent
[00:14:06.640 --> 00:14:11.520]   would learn to go through the random patterns, which are robust, rather than the green patterns,
[00:14:11.520 --> 00:14:19.200]   which are fragile. And as you can see here, our agents don't do that. So we have now released
[00:14:19.200 --> 00:14:30.480]   this on the Weights and Biases benchmark page. Everyone should go to wandb.ai/safelifev1.2
[00:14:30.480 --> 00:14:38.800]   for our version 1.2 benchmark. And here you can see some of my results in our initial baseline
[00:14:38.800 --> 00:14:44.880]   benchmark runs. This is fundamentally a multi-objective optimization problem. We want
[00:14:44.880 --> 00:14:51.680]   the agent to both train, we want the agent to both get a high score and to avoid causing side effects.
[00:14:51.680 --> 00:14:56.320]   So we have an overall score for the benchmark that combines these two. And we can see that
[00:14:56.320 --> 00:15:01.520]   like our naive agent that's trying to maximize score doesn't actually do a very good job at it.
[00:15:01.520 --> 00:15:10.560]   If you want to, if you want, you can come here and submit your run and try and improve on the
[00:15:10.560 --> 00:15:15.440]   state of the art. If you're feeling like you've already got lots of machine learning problems to
[00:15:15.440 --> 00:15:20.400]   do, you can also have a fun time just playing the game. And there's a human benchmark on
[00:15:20.400 --> 00:15:24.000]   Weights and Biases. Just warning, it's really addictive and distracting. So please don't
[00:15:24.000 --> 00:15:30.800]   stop playing this until after our talk. Yeah. So yeah, everyone should go do this. It's fun.
[00:15:30.800 --> 00:15:36.080]   And we hope you enjoy it. We'll be happy to answer any questions about it.
[00:15:38.880 --> 00:15:45.600]   All right. But first, a quick summary of where we're at with SafeLife 1.0 in terms of the
[00:15:45.600 --> 00:15:52.160]   research problem. How much are we getting done on this fundamental question of avoiding side
[00:15:52.160 --> 00:15:56.880]   effects and reinforcement learning. In the initial paper that we put out, SafeLife 1.0,
[00:15:56.880 --> 00:16:01.920]   we had a simple intervention that worked a little bit. Basically penalized the agent
[00:16:01.920 --> 00:16:08.560]   for the distance measured by moving blocks around the grid from the initial state. And
[00:16:08.560 --> 00:16:13.920]   that worked a little bit. There's a paper from Alex Turner and his colleagues at Oregon State
[00:16:13.920 --> 00:16:19.920]   University that shows more progress using what they call an attainable utility preservation
[00:16:19.920 --> 00:16:25.840]   algorithm, which by the way, hasn't been claimed on this leaderboard. So if anyone wants a fun
[00:16:25.840 --> 00:16:33.760]   afternoon project, you can go and grab that paper and implement their intervention and probably get
[00:16:33.760 --> 00:16:39.680]   the top spot on the benchmark. Next slide. But we're also thinking about what we can do
[00:16:39.680 --> 00:16:46.160]   beyond the version 1.x series of this benchmark. We have three things we're thinking about.
[00:16:46.160 --> 00:16:53.600]   Multi-agent cooperation and competition, adding simulated human feedback, sort of reward modeling
[00:16:53.600 --> 00:17:00.320]   or oracles to the game, and three, addressing technical safety problems besides side effects,
[00:17:00.320 --> 00:17:05.440]   like the other ones, like reward hacking. So in the multi-agent safety setting, go on,
[00:17:05.440 --> 00:17:13.040]   we actually got this implemented. You can get symmetric competition where there's a finite
[00:17:13.040 --> 00:17:18.240]   number of cells, squares to be filled out and points to be gotten there. And what you'll see is
[00:17:18.240 --> 00:17:24.560]   competitive behavior where the agents on the right are going around and undoing each other's work,
[00:17:24.560 --> 00:17:31.600]   replacing their cells with the other person's cells with theirs. And what they need to do is
[00:17:31.600 --> 00:17:39.040]   figure out a fair split. So you've got two agents, so that's the fair split on the right,
[00:17:39.040 --> 00:17:42.720]   they haven't figured that out, they're competing, but they should figure that out. Here in this
[00:17:42.720 --> 00:17:47.600]   case, one of the agents is trying to create blue life and the other one's trying to delete red
[00:17:47.600 --> 00:17:52.080]   life. They should be able to work together, they should be able to help each other accomplish their
[00:17:52.080 --> 00:17:55.920]   goals, but there's a signaling problem that they need to solve to figure out how to do that
[00:17:55.920 --> 00:18:02.880]   and avoid side effects along the way. Next slide. And so this is in the tree already, but not in the
[00:18:02.880 --> 00:18:07.200]   benchmark yet. We haven't figured out all the conceptual issues with how we want to score
[00:18:07.200 --> 00:18:13.840]   multi-agent. Then the third possibility, or second possibility, is this oracles for safety question.
[00:18:13.840 --> 00:18:20.560]   Is there a way we could have the agent be able to ask an oracle or a supervisor whether a particular
[00:18:20.560 --> 00:18:25.120]   action or state they could take in the game was safe? And then you could get an answer,
[00:18:25.120 --> 00:18:28.880]   but the answer would be either scarce, like you'd have a limited number of queries you could make,
[00:18:28.880 --> 00:18:33.920]   or costly, like you lose points to ask the oracle. And the game would be how quickly and well can
[00:18:33.920 --> 00:18:39.280]   you model what the answer, the answers the oracle is going to give you in future. That requires a
[00:18:39.280 --> 00:18:43.440]   type of reinforcement learning agent that is in some sense much fancier than your basic,
[00:18:43.440 --> 00:18:48.960]   say, PPO agent, requires an agent that can reason about counterfactual future states.
[00:18:48.960 --> 00:18:53.520]   Conveniently, one of those has popped up in the literature, or perhaps now a series of them,
[00:18:53.520 --> 00:19:00.560]   but the Muziro paper from DeepMind, which was the same type of agent that played both Atari and
[00:19:00.560 --> 00:19:05.360]   chess and Go without knowing the rules of those games, happens to have the properties that look
[00:19:05.360 --> 00:19:12.560]   right for doing this kind of safety research. Next slide. So that's where we're at, both with
[00:19:12.560 --> 00:19:17.200]   the benchmark that you can get involved in right now and our future directions,
[00:19:17.840 --> 00:19:19.520]   and we'd love to take questions if we have time.
[00:19:19.520 --> 00:19:30.160]   Yeah, we've got time for questions. I've got some that I'm excited about asking.
[00:19:30.160 --> 00:19:38.080]   So what have you seen so far on the benchmark that has worked well, both in the run-up to the
[00:19:38.080 --> 00:19:44.080]   benchmark and after it's been released? What strategies have consistently people tried and
[00:19:44.080 --> 00:19:47.000]   failed or tried and succeeded with? >>
[00:19:47.000 --> 00:19:55.840]   Carol, do you want to take that one? >> Sure. So, yeah. So the benchmark is
[00:19:55.840 --> 00:20:00.800]   still pretty new, and we're still working on getting a lot of community involvement on it.
[00:20:00.800 --> 00:20:09.280]   In the run-up to the benchmark, I think this attainable utility preservation is a really
[00:20:09.280 --> 00:20:15.760]   interesting strategy. The way it works is it -- let's see if I can describe this quickly enough.
[00:20:15.760 --> 00:20:22.560]   The way attainable utility preservation works is that you model a bunch of different possible
[00:20:22.560 --> 00:20:27.280]   rewards that the agent can have, and you try and prevent it from messing up any of the value
[00:20:27.280 --> 00:20:33.360]   functions associated with any of those rewards. So this is a really promising procedure that can
[00:20:33.360 --> 00:20:38.320]   avoid side effects over lots of different environments. So it was really exciting to
[00:20:38.320 --> 00:20:43.040]   see that when applied to this environment, previously it had only been applied to very
[00:20:43.040 --> 00:20:49.760]   simple grid world type things. But when applied to this environment with a fair bit of hacking
[00:20:49.760 --> 00:20:55.280]   around, you can get it to work pretty well. What they ended up doing is having an auto encoder and
[00:20:55.280 --> 00:21:02.000]   penalizing on changes in the latent space of that auto encoder. >> This actually reminds me of a
[00:21:03.680 --> 00:21:08.640]   point we didn't make in the talk, I think, because we got interrupted video-wise. But
[00:21:08.640 --> 00:21:15.760]   SafeLife has procedurally generated levels. So one thing that really distinguishes it from
[00:21:15.760 --> 00:21:21.920]   a simple pre-coded set of benchmark levels is that it makes an infinite number of them.
[00:21:21.920 --> 00:21:28.320]   And so having algorithms that work with these complicated dynamics and a very rich diversity
[00:21:28.320 --> 00:21:35.440]   of levels is different than a specific level where you can kind of learn, go left, left, left, left,
[00:21:35.440 --> 00:21:40.400]   up, left, and that's safe. With SafeLife, you're always confronted with something different if
[00:21:40.400 --> 00:21:46.160]   you're unhacked. >> Yeah, that's an interesting point and something that I had not picked up.
[00:21:46.160 --> 00:21:51.360]   And I think both of those points that you made remind me of the experience of RL agents in other
[00:21:51.360 --> 00:21:55.520]   contexts, like what works well and what doesn't work. I remember one of the first agents to do
[00:21:55.520 --> 00:22:02.240]   well at Doom out of Vlad Koltun's group at Intel. One of the things they found was that they needed
[00:22:02.240 --> 00:22:07.280]   to learn how to solve a bunch of random reward functions first in order to get a really good
[00:22:07.280 --> 00:22:11.680]   internal representation of the world and representation of how these things map
[00:22:11.680 --> 00:22:16.480]   onto possible rewards. And then those things would then do well on the final reward function
[00:22:16.480 --> 00:22:22.240]   they actually cared about of playing the game and winning. And there seems to be at least some kind
[00:22:22.240 --> 00:22:28.320]   of connection with this attainable utility preservation concept. >> Yeah, similar, except
[00:22:28.320 --> 00:22:34.080]   that the attainable utility preservation, like a standard PPO agent will do reasonably well,
[00:22:34.080 --> 00:22:40.560]   you know, given correct hyperparameters will do reasonably well on the base reward. But the
[00:22:40.560 --> 00:22:46.080]   attainable utility preservation, it's like by having lots of different rewards, you can recognize
[00:22:46.080 --> 00:22:51.040]   side effects, which is an interesting difference compared to other reinforcement learning setups.
[00:22:51.760 --> 00:23:03.120]   >> And I guess one, you mentioned, I guess, this idea in AI safety that there might be an
[00:23:03.120 --> 00:23:09.200]   appropriate mathematical way to formalize constraints. Would you expect that out of
[00:23:09.200 --> 00:23:17.040]   the process of developing algorithms that can better reduce side effects and obey constraints
[00:23:17.040 --> 00:23:21.840]   in this complicated environment, that there might be, you know, sort of inspiration for like ideas
[00:23:21.840 --> 00:23:26.880]   that might be able to resolve that better in the same way that new ideas about regularization have
[00:23:26.880 --> 00:23:32.000]   been generated by people's attempts to reduce overfitting in complex neural network models?
[00:23:32.000 --> 00:23:39.840]   >> I hope so. Yeah, so that's like very much the motivation. >> Yeah, I think that's a hope.
[00:23:39.840 --> 00:23:49.520]   >> Keep going, Peter. >> It's really a hope that if you have
[00:23:49.520 --> 00:23:58.720]   agents that can play these safety games well, they will have something like common sense about safety.
[00:23:58.720 --> 00:24:05.680]   They'll be able to know when the thing they're doing is reasonable, and they'll be able to know
[00:24:05.680 --> 00:24:10.480]   when they should stop and ask a human because they're seeing something weird and they need
[00:24:10.480 --> 00:24:16.960]   feedback. I'm seeing jumpy video inbound to me. I hope I'm not equally jumpy in the outbound direction.
[00:24:16.960 --> 00:24:25.600]   >> No, no, not too jumpy to be understood, which is the real, that's the real bar.
[00:24:27.120 --> 00:24:30.800]   Yeah, that's interesting. That reminds, I guess,
[00:24:30.800 --> 00:24:38.160]   one of the questions I had in my mind during your talk was, you know, how do we solve this
[00:24:38.160 --> 00:24:43.920]   problem with humans? And it seems like a big part of it is that humans end up learning how to work
[00:24:43.920 --> 00:24:49.360]   together and work with each other, and they're in a very large multi-agent environment, and they
[00:24:49.360 --> 00:24:54.320]   receive negative rewards for causing lots of different kinds of side effects. So do you see
[00:24:54.320 --> 00:25:00.400]   the multi-agent work as one of the most important ways forward for solving this constraint problem,
[00:25:00.400 --> 00:25:03.840]   or do you think it's something that just the single agent environment, we can figure out
[00:25:03.840 --> 00:25:08.640]   how to solve it? >> It does. So, yeah, there are a
[00:25:08.640 --> 00:25:15.280]   couple of interesting aspects to that. It seems like we have an intuitive notion of what side
[00:25:15.280 --> 00:25:24.400]   effects are as humans, and that we should be able to be like, I can do this task in a more efficient
[00:25:24.400 --> 00:25:29.200]   way that doesn't make a mess of things, doesn't disrupt my ability to fulfill future tasks.
[00:25:29.200 --> 00:25:35.200]   It seems like maybe that's something that you should be able to figure out on your own.
[00:25:35.200 --> 00:25:39.600]   But we also know, like, I happen to be in a house right now with a bunch of toddlers,
[00:25:39.600 --> 00:25:43.120]   and they do not at all follow this protocol. They make a mess of everything,
[00:25:44.080 --> 00:25:49.520]   and they need to be told by adults, like, no, just because you take out your toys doesn't mean
[00:25:49.520 --> 00:25:55.360]   they need to spread all over the house. So even though it seems like we have this intuitive
[00:25:55.360 --> 00:26:00.720]   notion, we might need to think about it more like our agents, our toddlers, and even though it seems
[00:26:00.720 --> 00:26:04.480]   like they should be able to figure things out on their own, they really need to be told explicitly
[00:26:04.480 --> 00:26:11.760]   what to do instead. And part of that comes in with multiple agents being the agents can feed
[00:26:11.760 --> 00:26:17.200]   off of each other and maybe learn these things in social ways. And part of it comes from this
[00:26:17.200 --> 00:26:26.480]   oracular description that Peter was talking about. >> Great question coming in from the Zoom chat
[00:26:26.480 --> 00:26:31.760]   from Scott Phillips. How do you see this work relating to the development of AGI,
[00:26:31.760 --> 00:26:36.320]   of artificial general intelligence? Do you think that this is a step along that path?
[00:26:36.320 --> 00:26:44.240]   Do you think it's orthogonal? How do you view this? >> Yeah, so definitely one of the things
[00:26:44.240 --> 00:26:49.680]   that excited me most about side effects as a safety problem is that I thought of it as a bridge
[00:26:49.680 --> 00:26:57.600]   between short-term safety problems in artificial intelligence and exactly these very long-term AGI
[00:26:57.600 --> 00:27:05.440]   type problems. A paperclip maximizer is an agent that doesn't realize that it's causing lots and
[00:27:05.440 --> 00:27:15.040]   lots of side effects. If it wanted to maximize paperclips until it caused everything to go to
[00:27:15.040 --> 00:27:20.480]   hell, then maybe it wouldn't maximize the entire universe into paperclips. It would satisfy with
[00:27:20.480 --> 00:27:29.120]   maximizing only its factor into paperclips. It seems like if we could, as Peter said,
[00:27:29.120 --> 00:27:37.920]   if we can get our agents to have some sense, I think that effect stuff is embedded in that
[00:27:37.920 --> 00:27:46.560]   or maybe vice versa. Making serious progress on this could make advanced artificial intelligence
[00:27:46.560 --> 00:27:51.400]   much more safe, but also near-term deployment more safe as well. >>
[00:27:52.800 --> 00:28:00.240]   It's almost like you must be at least this tall. If we are deploying, whether it's AGI,
[00:28:00.240 --> 00:28:06.560]   maybe some distance away, any very capable agent-type system in the world, if it can't get
[00:28:06.560 --> 00:28:13.600]   over these hurdles, for instance, ability to make you coffee without really causing any type of
[00:28:13.600 --> 00:28:21.120]   side effect, then we're not on the right path for deployment in high-stakes settings or very
[00:28:21.120 --> 00:28:25.760]   capable systems. >> Yeah, I'd absolutely agree. I think
[00:28:25.760 --> 00:28:32.000]   we've seen a lot of unintended consequences just from deploying image labelers out into the world,
[00:28:32.000 --> 00:28:39.120]   to say nothing of things that walk around with large arms and spinning gears.
[00:28:39.120 --> 00:28:47.760]   >> Hopefully not the first highly capable robot we'll be building. I would like my first AGI not
[00:28:47.760 --> 00:28:53.680]   to have the death chopper. >> That's fair. That's a good point. Start
[00:28:53.680 --> 00:29:02.080]   small. Thank you so much, Carol and Peter, for coming on to talk about your work. Thanks, Stacy,
[00:29:02.080 --> 00:29:10.240]   for helping orchestrate this and get us in contact with them. I am going to do a quick interlude
[00:29:10.240 --> 00:29:16.720]   between our two talks before we jump on to the next one. I am going to take control of the screen
[00:29:16.720 --> 00:29:23.120]   sharing and let you guys know about the future Deep Learning Salon. We do these every two weeks.
[00:29:23.120 --> 00:29:32.000]   The next one is December 8th at 5 Pacific time. We have two really dope talks lined up. The first
[00:29:32.000 --> 00:29:38.240]   one is from Maitreya Raghu at Google Brain, who's a really excellent researcher. She did some really
[00:29:38.240 --> 00:29:44.960]   cool work on exponential expressivity of neural networks back with Surya Ganguly at Stanford.
[00:29:44.960 --> 00:29:50.880]   She's continued to absolutely kill it. I'm really excited to have her on to talk about her new paper
[00:29:50.880 --> 00:29:58.720]   about the different representations that wide and deep networks learn. Then we'll also have
[00:29:58.720 --> 00:30:06.400]   Diganta Mishra of landscape.ai, who maybe most famous paper is the MISH activation paper, which
[00:30:06.400 --> 00:30:14.400]   seems to be eponymous. It's a smooth and non-monotonic and self-normalizing activation.
[00:30:14.400 --> 00:30:20.960]   That paper got a good bit of interest. He'll be on to talk about connections he sees between a
[00:30:20.960 --> 00:30:27.040]   couple of different ideas, smooth and Lipschitz activations, robustness to adversarial examples,
[00:30:27.040 --> 00:30:32.960]   and catastrophic forgetting. I'm really excited to hear both of these talks in two weeks. You can
[00:30:32.960 --> 00:30:40.640]   RSVP at wanb.me/salon if you want to see those, if you want to come along and check them out.
[00:30:40.640 --> 00:30:47.120]   The other thing that I wanted to do is let you know that we want to know how we are doing
[00:30:47.120 --> 00:30:57.200]   the salon. We've heard lots of positive feedback from folks, but we want to make it numerical. We
[00:30:57.200 --> 00:31:01.520]   want to make it quantitative. We got to be scientific about it. If you could head to
[00:31:01.520 --> 00:31:09.920]   wanb.me/salon-survey and let us know just super quick how you're feeling about the salon. If
[00:31:09.920 --> 00:31:14.080]   you've got things you really liked, things you really hated to provide in our little comment
[00:31:14.080 --> 00:31:22.720]   box, we'd love that too. Just let me know. I've done this for a lot of classes that I've taught
[00:31:22.720 --> 00:31:27.040]   and it's really changed the way that I go about teaching them. I'd love to do the same to improve
[00:31:27.040 --> 00:31:33.120]   the salon. Of course, because we do the salons every two weeks, we've accumulated now quite a
[00:31:33.120 --> 00:31:38.160]   bit of this salon content people may not have seen. You can check those out on our YouTube channel,
[00:31:38.160 --> 00:31:43.920]   the virtual deep learning salons playlist. We've also got a lot of other things. I've
[00:31:43.920 --> 00:31:48.160]   been uploading a lot of tutorials about how to use weights and biases. We've got a podcast. We
[00:31:48.160 --> 00:31:53.280]   had Peter Norvig on the podcast just this past week. That was a really fun conversation.
[00:31:55.520 --> 00:32:01.760]   If you want to keep up with all of this, you can find it in our Slack community. That's wanb.me/slack.
[00:32:01.760 --> 00:32:07.520]   We do lots of different things. We had an AMA with the CEO of Kaggle. We had an AMA with
[00:32:07.520 --> 00:32:16.720]   the folks at Ice Vision, a new computer vision library. Join the Slack and you can join in on
[00:32:16.720 --> 00:32:27.040]   those conversations. All right. That's my promo for this iteration. Next up, we have our second
[00:32:27.040 --> 00:32:33.760]   talk. We have Stacey, if you'd like to introduce our second group of speakers.
[00:32:33.760 --> 00:32:40.800]   Yeah, definitely. Thanks, Charles. Next up, we have Deep Form. This is a collaboration
[00:32:40.800 --> 00:32:46.000]   with a group of folks, some of whom will be speaking. Jonathan Stray, Andrea Lowe, Gray
[00:32:46.000 --> 00:32:53.040]   Davidson, Hugh Wimberly, Daniel Fennelly, and myself. This is a project to extract data from
[00:32:53.040 --> 00:33:00.480]   forms. Specifically, we have been working with data on political ad campaigns for television,
[00:33:00.480 --> 00:33:08.720]   where the task is to, for a given political ad, extract how much money was paid for that ad,
[00:33:08.720 --> 00:33:13.280]   what the organization was that paid for it, the dates that the ad was screened, and so forth.
[00:33:13.280 --> 00:33:19.440]   That's the concrete problem, but you'll hear more about how this fits into a much larger context of
[00:33:19.440 --> 00:33:25.840]   extracting structured data from forms in general, whether that's for receipts of this kind or for
[00:33:25.840 --> 00:33:32.640]   climate science data and much, much more. I'll hand it over.
[00:33:32.720 --> 00:33:33.280]   Okay.
[00:33:33.280 --> 00:33:45.520]   Hey, thanks, Stacey. Looks like I cannot start my video,
[00:33:45.520 --> 00:33:54.880]   because the host has stopped it. There we go. Boom. Wonderful. Thanks, Stacey. I'm Jonathan
[00:33:54.880 --> 00:34:02.080]   Stray. I'm here with Gray Davidson. We're going to walk you through the Deep Form work, which is
[00:34:03.040 --> 00:34:06.400]   we've got a few other speakers here for you today, including Andrea,
[00:34:06.400 --> 00:34:10.080]   because it was quite a big project. There were a bunch of us involved.
[00:34:10.080 --> 00:34:15.520]   Okay. Slides visible? All good?
[00:34:15.520 --> 00:34:28.000]   Okay. Well, so what's the challenge? Well, we're taking a crack at a problem that appears
[00:34:28.000 --> 00:34:35.200]   in many contexts, not just journalism, but here you can see a tweet. There's a climate scientist
[00:34:35.200 --> 00:34:40.400]   who has these historical weather data records, and they come in a lot of different formats,
[00:34:40.400 --> 00:34:44.960]   because every weather station had their own unique weather log. On the right is one of 80
[00:34:44.960 --> 00:34:50.720]   million pages in the recently unearthed archive of the Guatemalan National Secret Police.
[00:34:50.720 --> 00:34:56.080]   In particular, this archive contains the only records of thousands of people who disappeared
[00:34:56.080 --> 00:35:01.600]   during the Guatemalan Civil War. Again, it's every conceivable type of form.
[00:35:01.600 --> 00:35:12.160]   I come from the journalism world. I worked as a data journalist for some time. This is a huge
[00:35:12.160 --> 00:35:18.800]   problem in journalism as well. You may have seen this story, the FinCEN files, which was a huge
[00:35:18.800 --> 00:35:25.680]   global collaborative analysis of thousands of leaked special activity reports, or suspicious
[00:35:25.680 --> 00:35:29.520]   activity reports. These are the forms that banks file to governments when they think they're
[00:35:29.520 --> 00:35:34.320]   something that maybe kind of looks like money laundering. What I want to direct your attention
[00:35:34.320 --> 00:35:43.840]   to is that a huge part of this work was literally transcribing narrative information into Excel
[00:35:43.840 --> 00:35:50.800]   files. 85 journalists in 30 countries for a year. I've done a lot of work on AI for journalism,
[00:35:50.800 --> 00:35:55.840]   and my sort of broad conclusion is that while there's enormous potential in terms of pattern
[00:35:55.840 --> 00:36:02.720]   recognition, data mining, lead generation, all this stuff, really, if you want to increase the
[00:36:02.720 --> 00:36:08.000]   efficiency of journalism, you should use AI for data preparation and cleanup, because that's what
[00:36:08.000 --> 00:36:14.800]   takes all the time, and that's where the well-defined opportunities are. With that in mind,
[00:36:14.800 --> 00:36:22.480]   this is a project that we began on in March of this year, and it was simply to unlock ostensibly
[00:36:22.480 --> 00:36:26.960]   public data. This stuff is available from the Federal Communications Commission, and what this
[00:36:26.960 --> 00:36:34.640]   is is this is about 100,000 PDFs every two years, filed by everybody who buys a political ad on TV,
[00:36:34.640 --> 00:36:41.360]   on a local television station. So here's what one of these things looks like. It's an invoice,
[00:36:41.360 --> 00:36:44.960]   basically, and we're interested in a few pieces of information. So for example, the
[00:36:44.960 --> 00:36:48.960]   station name, the advertiser name, and the amount that they paid.
[00:36:48.960 --> 00:36:55.040]   So not so hard, right? There's lots and lots of tools that can solve this problem.
[00:36:55.040 --> 00:37:00.640]   The challenge is that it's not just one form. It's also this form, right? A completely different
[00:37:00.640 --> 00:37:05.360]   layout with very similar information, and also this form, which doesn't really look anything at
[00:37:05.360 --> 00:37:10.560]   all like the others. In fact, most TV stations use a different format, and so there's hundreds
[00:37:10.560 --> 00:37:19.360]   of different formats. There's a very long tail of ways this appears. So that's what we were doing
[00:37:19.360 --> 00:37:24.160]   here. We were trying to actually extract this data because it was relevant to the 2020 election,
[00:37:24.160 --> 00:37:28.800]   and we wanted to be able to provide it to journalists. But more, we wanted to demonstrate
[00:37:28.800 --> 00:37:36.640]   that this is a solvable problem for other fields that have it. And as a non-insignificant side
[00:37:36.640 --> 00:37:40.400]   effect, we wanted to create a data set where people could work on this problem because it
[00:37:40.400 --> 00:37:47.040]   turns out it is actually a state-of-the-art problem. Now I'm going to turn it over to
[00:37:47.040 --> 00:37:50.160]   Gray and Andrea, who are going to walk you through what the training data looks like.
[00:37:50.160 --> 00:37:57.840]   Cool. Thanks, Jonathan. So our data consisted of the political advertisements for TV contracts,
[00:37:57.840 --> 00:38:03.360]   orders, and invoices from this year, which amounted to approximately 115,000 PDFs,
[00:38:03.360 --> 00:38:09.040]   and these were collected using the FCC API. For our training set, we sampled 1,000 of these
[00:38:09.040 --> 00:38:13.440]   and hand-labeled the fields for contract number, advertiser, flight dates, and gross amount.
[00:38:13.440 --> 00:38:18.160]   The full data set and this labeled sample are both available at overviewdocs.com. You can
[00:38:18.160 --> 00:38:24.480]   download that. And we also had additional training sets from previous years. So the first was 17,000
[00:38:24.480 --> 00:38:29.840]   PDFs from 2012. These were labeled through crowdsourcing by ProPublica and made freely
[00:38:29.840 --> 00:38:36.880]   available. And then the second was 2014 documents. These were labeled through a semi-automated
[00:38:36.880 --> 00:38:43.200]   extraction process that was created by Alex Burns from the Washington Post. And for this process,
[00:38:43.200 --> 00:38:48.400]   he used a computer vision algorithm to focus on the page region where the answer is typically found.
[00:38:48.400 --> 00:38:55.840]   So after these PDFs were gathered, we had to process them to use the model by first making
[00:38:55.840 --> 00:39:01.440]   sure they were machine-readable through OCR, or optical character recognition. Then we tokenized
[00:39:01.440 --> 00:39:05.920]   the text and created a document with the corresponding geometry, where the labels were
[00:39:05.920 --> 00:39:11.760]   the tokens themselves. The next step here was marking which token out of the document corresponds
[00:39:11.760 --> 00:39:16.080]   to the target for the fields we're looking for. So I'm now going to hand it over to Gray so he can
[00:39:16.080 --> 00:39:22.080]   discuss that process. Thanks, Andrea. So at the left, we can see the six columns that were the
[00:39:22.080 --> 00:39:26.880]   result of that OCR process. The page number normalized, because some of these documents
[00:39:26.880 --> 00:39:32.400]   were five, eight, or dozens of pages long. And then the bounding box, four coordinates showing
[00:39:32.400 --> 00:39:36.880]   where the token was on the page, and then the token text. So this is the key piece for the
[00:39:36.880 --> 00:39:43.280]   next step here, where we then used a fuzzy matching process to decide which tokens matched
[00:39:43.280 --> 00:39:50.560]   which targets. So we can see that the red boxes here circle the times when the fuzzy match got
[00:39:50.560 --> 00:39:55.920]   a perfect match. And most of these targets that we were looking for, most of the labels were one
[00:39:55.920 --> 00:40:00.240]   token long. So we can see contract number is the highest one down at the bottom with the flight
[00:40:00.240 --> 00:40:04.880]   dates. And in the middle, we can see that the committee, Mike Bloomberg 2020 Incorporated,
[00:40:04.880 --> 00:40:10.400]   was four tokens long. So we also had a process for finding multi-token targets. The green boxes
[00:40:10.400 --> 00:40:16.960]   are surrounding some near misses. We can see that numbers and dates were rated highly for
[00:40:17.600 --> 00:40:22.800]   those particular targets, even though they weren't the correct answers. And so the fuzzy
[00:40:22.800 --> 00:40:26.480]   match still gets a high percentage, but still finds the correct answers when they're present.
[00:40:26.480 --> 00:40:33.200]   So by this process, we were able to determine which tokens corresponded to the targets that
[00:40:33.200 --> 00:40:38.880]   we were looking for. And the additional features here, these six columns at the right,
[00:40:38.880 --> 00:40:43.360]   along with the bounding boxes and page numbers, were the inputs to the model itself.
[00:40:44.000 --> 00:40:47.840]   So we can see that we had features such as how much is the token like a digit,
[00:40:47.840 --> 00:40:54.080]   or like a string of numbers? How much is it like a number of dollars, et cetera? And then at the
[00:40:54.080 --> 00:41:00.240]   far right was a key column that shows for a given token, a zero indicates that that token was none
[00:41:00.240 --> 00:41:06.080]   of our targets. And then one, two, three, four, and five indicated that it was one of our targets,
[00:41:06.080 --> 00:41:10.560]   and the different numbers corresponded to the different targets. So back to you, Jonathan.
[00:41:11.920 --> 00:41:18.240]   So one of the challenges in building a model to do this classification, it's because it's basically
[00:41:18.240 --> 00:41:23.840]   token classification, is that the documents have a huge variation in length. Some are
[00:41:23.840 --> 00:41:30.800]   the top half of a page, some are dozens of pages of listings of hundreds of spots.
[00:41:30.800 --> 00:41:36.880]   And so we had to adapt the length. In the end, we just went with a very simple windowing scheme.
[00:41:38.000 --> 00:41:42.960]   The window length is fixed and short, maybe between 20 and 50. And we're just doing
[00:41:42.960 --> 00:41:48.560]   multi-class prediction for each token in each window, and then averaging them all,
[00:41:48.560 --> 00:41:52.640]   all of the overlapping windows on top of each other to get the final outputs.
[00:41:52.640 --> 00:42:00.560]   And this is what the model actually looks like. So it's actually pretty straightforward as these
[00:42:00.560 --> 00:42:06.480]   things go, because the vast majority of the work inevitably turns out to be doing all of
[00:42:06.480 --> 00:42:15.440]   the data engineering and OCRing 100,000 documents and so forth. So for each token in the window,
[00:42:15.440 --> 00:42:24.400]   we have an embedding from the vocabulary. We pick the 500 top tokens that appear across all the
[00:42:24.400 --> 00:42:30.080]   training data, as well as the token's location on the page and some of these hand-coded features,
[00:42:30.080 --> 00:42:34.640]   like does it match a regular expression for a dollar amount. So then we repeat that k times,
[00:42:34.640 --> 00:42:41.280]   we go through a few hidden layers, and then we produce a softmax output for each window,
[00:42:41.280 --> 00:42:50.480]   for each class. So rather than just producing one class for the central token in the window,
[00:42:50.480 --> 00:42:55.760]   the network tries to predict all of them. And we found that this actually really improves performance
[00:42:55.760 --> 00:43:03.440]   by 10% or more. You can think of it as kind of a multitask training approach,
[00:43:04.000 --> 00:43:08.400]   where by asking it to label every single token window, you force it
[00:43:08.400 --> 00:43:12.000]   to learn better intermediate representations in the previous layers.
[00:43:12.000 --> 00:43:17.360]   And this is what it looks like. So what you're looking at is the
[00:43:17.360 --> 00:43:25.600]   accuracy on the answer in the final document. It's noisy because during training, we just sample from
[00:43:25.600 --> 00:43:33.120]   the document, so we don't have to do all of it. And you can see that for many fields, we get to 90%.
[00:43:34.080 --> 00:43:38.400]   Some of them are much lower. In particular, advertiser is not doing so well. It's actually,
[00:43:38.400 --> 00:43:42.880]   this looks like it's just like 7% or something. It's actually a lot better. It's an old chart.
[00:43:42.880 --> 00:43:48.080]   But it is the most challenging field because it spans multiple tokens. And the main problem we're
[00:43:48.080 --> 00:43:52.560]   having is it's not getting all of the advertisers. So it may list, you know, miss the first or last
[00:43:52.560 --> 00:43:56.880]   word or put something on the end of it. So that's actually, we went through the literature, and this
[00:43:56.880 --> 00:44:02.480]   is actually a major limitation of almost every other technique, is that they're almost all built
[00:44:02.480 --> 00:44:09.120]   around classifying tokens that have strings of tokens. We did, this will be familiar if you're a
[00:44:09.120 --> 00:44:14.480]   Weights and Biases user, we did some hyperparameter search as well. And there's about a dozen
[00:44:14.480 --> 00:44:19.840]   configurable parameters. And we found that, you know, there was sort of a nice sweet spot where we
[00:44:19.840 --> 00:44:29.440]   got good performance on the fields. And then we had a really nice evaluation and logging environment
[00:44:29.440 --> 00:44:33.520]   Weights and Biases, which Stacey is going to demo live for you briefly.
[00:44:33.520 --> 00:44:43.280]   Great, let's try to see if I can do this. Can you see, can you see some bounding boxes? Great. Yeah.
[00:44:43.280 --> 00:44:48.480]   So this is an example from a report that I made to help get started with the benchmark. And here
[00:44:48.480 --> 00:44:53.760]   you can see a bunch of the documents and maybe you can see these colorful bounding boxes on the
[00:44:53.760 --> 00:45:00.400]   different tokens. And let's zoom into one in particular. And I know, I know they're pretty
[00:45:00.400 --> 00:45:06.400]   tiny, but on each of these bounding boxes, we actually have a confidence score of how good of
[00:45:06.400 --> 00:45:12.000]   a match this is for one of the fields that we're tracking. So the ground truth here is an orange,
[00:45:12.000 --> 00:45:16.720]   and then there's some lower scoring candidates. You'll see in purple, for example, that's for
[00:45:16.720 --> 00:45:22.240]   the contract ID. These pink ones are the advertiser, you know, it should really be all three of these
[00:45:22.240 --> 00:45:32.960]   tokens, etc. And you can also interact with these boxes with the slider that we have in Weights and
[00:45:32.960 --> 00:45:38.560]   Biases. So you can, you know, see that as I set the minimum threshold higher, some of these boxes
[00:45:38.560 --> 00:45:45.200]   disappear. You can also look at some of these on the bottom, there's a lot of, a lot of numbers.
[00:45:45.200 --> 00:45:49.200]   And if I turn off all the ground truth, you'll see that, you know, at the low threshold,
[00:45:49.200 --> 00:45:54.480]   it's thinking that a bunch of these numbers might be the total. As I increase the threshold,
[00:45:54.480 --> 00:46:00.160]   most of those will disappear, and we'll get pretty close to just the final gross amount.
[00:46:00.160 --> 00:46:05.920]   And you can set up these kinds of visualizations across, you know, many items in your validation
[00:46:05.920 --> 00:46:10.640]   data, you can set up custom views, comparing it on certain documents or certain fields.
[00:46:10.640 --> 00:46:18.320]   And we hope that that's pretty useful for debugging your models. And should we go back to
[00:46:18.320 --> 00:46:23.680]   you, Jonathan? Or should I just go to the next slide? >> A couple of slides in this section.
[00:46:23.680 --> 00:46:30.080]   >> Yeah. Yeah. And I'll talk through this next slide. Right. So we made a benchmark. It's up
[00:46:30.080 --> 00:46:34.880]   on Weights and Biases at this URL. We also have a bit.ly at the end, because I know this is a little
[00:46:34.880 --> 00:46:39.280]   long to type. And what the benchmark is, is our baseline model and training code that's really
[00:46:39.280 --> 00:46:44.880]   easy to get started with. It's either a Docker container or a Poetry setup. We have a fixed data
[00:46:44.880 --> 00:46:51.520]   split. Right now it's just the thousand labeled documents from 2020 that you can train on. We have
[00:46:51.520 --> 00:46:57.360]   the useful visualization setup, and you can track accuracy across five fields, amount, flight, date,
[00:46:57.360 --> 00:47:02.240]   to and from, contract ID and advertiser, and the overall ranking in the leaderboard is just the
[00:47:02.240 --> 00:47:14.240]   average of those five fields. And next. >> So overall, we achieved the goal of showing
[00:47:14.240 --> 00:47:21.680]   that deep learning is competitive with hand-built systems. So like Alex Burns' sort of template-based
[00:47:21.680 --> 00:47:27.840]   systems. And can indeed extract data from form layouts that it's never seen before. And our
[00:47:27.840 --> 00:47:31.360]   baseline, as you saw, is actually not very complicated. We got relatively good performance
[00:47:31.360 --> 00:47:37.520]   with a relatively simple network. I think sort of from a technical perspective, the performance
[00:47:37.520 --> 00:47:43.840]   boost from predicting across the entire window was really the most striking thing. And actually,
[00:47:43.840 --> 00:47:50.480]   as we were doing this work, so I wrote a proof of concept like last June for the applied deep
[00:47:50.480 --> 00:47:56.400]   learning course that I took out of "Eights and Biases." And from then until now, actually a bunch
[00:47:56.400 --> 00:48:02.720]   of papers appeared on this very problem. So this is actually a state-of-the-art research problem,
[00:48:02.720 --> 00:48:09.760]   and the best methods are developing rapidly. So there's a bunch of stuff that you could try if
[00:48:09.760 --> 00:48:15.600]   you want to take a shot at this. First of all, we actually ended up only training on the 2020 data,
[00:48:15.600 --> 00:48:23.680]   even though we have far more data from other years. And that's because we just never got around to
[00:48:23.680 --> 00:48:29.200]   dealing with missing fields. So different years of training data have different sets of fields
[00:48:29.200 --> 00:48:35.120]   that are available. So we don't think that's too hard, but remains to be done and could improve
[00:48:35.120 --> 00:48:40.560]   the performance a lot. The advertiser field, as we mentioned, is just much harder than the other
[00:48:40.560 --> 00:48:45.600]   fields. And there's actually very little work that talks about how to do multi-token extraction in
[00:48:45.600 --> 00:48:52.960]   these ways. But perhaps the most exciting thing that we could do is look at other methods. So this
[00:48:52.960 --> 00:48:58.080]   was a paper that came out in the middle of our work. It's one from Google that also talks about
[00:48:58.080 --> 00:49:03.600]   extracting data from invoices. It uses exactly the same input format that we do, this like
[00:49:03.600 --> 00:49:07.920]   tokens plus geometry. So we felt that, great, we're on the right track if that's what Google
[00:49:07.920 --> 00:49:16.320]   is doing. And it has this interesting self-attention network where it tries to learn how to relate
[00:49:16.320 --> 00:49:24.640]   different tokens. We've also seen graph convolutions used for receipt extraction. So
[00:49:24.640 --> 00:49:32.240]   this is from an interesting post about how that was put together. In this case, each token is a
[00:49:32.240 --> 00:49:37.680]   node and each edge is a horizontal or vertical relationship. So it's a different way of encoding
[00:49:37.680 --> 00:49:45.840]   the geometry. I really couldn't say which way works better, but we've got someone who's trying
[00:49:45.840 --> 00:49:52.880]   to test it now. And then the current state of the art maybe, or at least the most sophisticated model
[00:49:52.880 --> 00:49:59.520]   currently, is Layout LLM, which is you have a pre-trained BERT model, which hopefully you're
[00:49:59.520 --> 00:50:06.160]   also training on a huge unlabeled data set of forms. And then that creates an embedding, which
[00:50:06.160 --> 00:50:10.640]   you then operate over. One of the interesting things about this is it actually includes an
[00:50:10.640 --> 00:50:16.720]   embedding of the image. So it looks at both the tokens plus geometry format, as you can see
[00:50:16.720 --> 00:50:21.760]   across the left there, it says text embeddings and then position embeddings, plus the actual
[00:50:21.760 --> 00:50:28.800]   image itself. So it could extract information from the image, such as the font size or
[00:50:29.440 --> 00:50:36.480]   whether it's bold or something. And all of these are just waiting to be dropped into
[00:50:36.480 --> 00:50:42.480]   the benchmark we wrote, which has all of the infrastructure and data processing
[00:50:42.480 --> 00:50:46.160]   that you need around this, logging methods, all that fun stuff.
[00:50:46.160 --> 00:50:53.920]   If you'd like to get started, here is a link for you. And yeah, we'd love to hear your questions.
[00:50:58.640 --> 00:51:04.240]   All right. Thanks, DeepForm team for that really interesting talk and for this really
[00:51:04.240 --> 00:51:10.720]   excellent benchmark. I could gush for a very long time about how beautiful that interface
[00:51:10.720 --> 00:51:19.520]   you built for looking at the results, Stacey, was, but I won't spend my whole time tooting your horn.
[00:51:19.520 --> 00:51:27.360]   The one thing I wanted to get your opinions on is the broader context of this form extraction
[00:51:27.360 --> 00:51:34.080]   problem. So I think the phrasing you used, Jonathan, was that this information is ostensibly
[00:51:34.080 --> 00:51:42.400]   public. And it seems like a lot of this production of information, of forms, in some ways, it's akin
[00:51:42.400 --> 00:51:47.440]   to the concept of paperwork pollution, which is where you externalize a lot of the effort
[00:51:47.440 --> 00:51:53.440]   that needs to be done in order to do useful work, get useful information, et cetera,
[00:51:53.440 --> 00:51:59.760]   onto other people through the form of creating paperwork for them to do. So I'm curious,
[00:51:59.760 --> 00:52:04.400]   how do you see this DeepForm outside of the obvious political ramifications of being able
[00:52:04.400 --> 00:52:10.960]   to uncover dark money? Where else do you see this form extraction being able to have positive
[00:52:10.960 --> 00:52:19.760]   ethical impacts on the world? Yeah, thanks. So I come from the investigative journalism world. And
[00:52:20.880 --> 00:52:25.920]   so it just happens over and over there. So I just showed you that example from the FinCEN files.
[00:52:25.920 --> 00:52:32.880]   But it's really a frequently asked question. How do I get the data out of these forms,
[00:52:32.880 --> 00:52:38.960]   whether that's downloaded PDFs or got actual physical paper from a Freedom of Information
[00:52:38.960 --> 00:52:43.440]   request? And so this is something I've been dealing with for a long time. There are now
[00:52:43.440 --> 00:52:50.560]   reasonably good tools if you have 1,000 of the same form. That's a fairly well-solved
[00:52:50.560 --> 00:52:54.080]   problem by commercial tools, although it's still kind of a pain in the ass, frankly.
[00:52:54.080 --> 00:53:03.440]   The frontier is in more complicated problems. So this is heterogeneous forms. And it gets even
[00:53:03.440 --> 00:53:07.760]   more complex from there, because we're trying to find the same five fields on every single form.
[00:53:07.760 --> 00:53:10.720]   There are also problems where different forms have different types of information.
[00:53:10.720 --> 00:53:18.880]   So it is, for my money, the place where AI would be most impactful to investigative journalism
[00:53:18.880 --> 00:53:24.480]   in terms of cost efficiency of applying AI, because that's really what this game is about.
[00:53:24.480 --> 00:53:31.200]   You're not going to enable anything that a human can't do, but you might be able to make it 10
[00:53:31.200 --> 00:53:35.920]   times faster or 10 times cheaper. And that can really change the economics of journalism
[00:53:35.920 --> 00:53:40.000]   production. And the same thing applies in a whole bunch of other fields. You saw the climate examples,
[00:53:40.000 --> 00:53:48.720]   the human rights relevant archives. There's another big human rights archive of documents
[00:53:48.720 --> 00:53:56.640]   out of Syria, for example. And in medicine, a lot of that, for example, referrals still
[00:53:56.640 --> 00:54:01.520]   arrive on faxes. And in the post where I was talking about this work, I talked about how
[00:54:01.520 --> 00:54:06.880]   they had, I think it was 37 people full time who were just typing in referral faxes.
[00:54:06.880 --> 00:54:15.760]   And so they started to put some automation on that. So it is a problem which is on the surface
[00:54:15.760 --> 00:54:21.120]   of it, sort of unsexy, but actually is hugely impactful and a state of the art challenge.
[00:54:21.120 --> 00:54:27.280]   Yeah, yeah. I think that the fact that it is such a, like, yeah, at that intersection of
[00:54:27.280 --> 00:54:33.520]   extremely difficult and non-trivial and state of the art and potentially hugely impactful,
[00:54:33.520 --> 00:54:41.040]   definitely makes this a really, really exciting benchmark. I think the, I wonder if maybe,
[00:54:41.040 --> 00:54:47.680]   you mentioned that there's this harder problem of, okay, what if I needed to extract from a couple
[00:54:47.680 --> 00:54:52.560]   of forms that have different fields in them? But I think one of the things that the natural
[00:54:52.560 --> 00:54:57.760]   language processing community has found is that actually learning to do a harder task with
[00:54:57.760 --> 00:55:04.000]   exponentially more data actually makes it easier to then do simpler tasks. So learning how to
[00:55:04.000 --> 00:55:10.240]   predict the next characters in a sequence, which is an extremely difficult task for,
[00:55:10.880 --> 00:55:15.840]   for the English language, then makes it actually easier to solve a problem like,
[00:55:15.840 --> 00:55:22.960]   like answering a question about a simple story about, you know, like the Babby tasks from
[00:55:22.960 --> 00:55:27.440]   Facebook, these like very simple question and answer things, SAT question type things that seem
[00:55:27.440 --> 00:55:32.960]   way, way easier to us than the language prediction problem. So do you think that if,
[00:55:32.960 --> 00:55:37.200]   that if we tried to tackle like a really, really big version of this problem where you could get
[00:55:37.200 --> 00:55:43.760]   that kind of the, a large dataset, do you think that maybe we'd be able to solve these simpler
[00:55:43.760 --> 00:55:48.480]   problems better with a single general purpose, you know, form to Vec network, or do you think
[00:55:48.480 --> 00:55:54.160]   that it's going to have to be relatively like ad hoc individual datasets, individual problems,
[00:55:54.160 --> 00:56:01.280]   things like that? So I think there is some hope. The layout LLM folks are doing something in the
[00:56:01.280 --> 00:56:06.400]   direction of what you're talking about. So that I can't, I think it's their paper I'm thinking of,
[00:56:06.400 --> 00:56:12.000]   is there, there are another piece of research and the way they define. So we, we built our
[00:56:12.000 --> 00:56:17.120]   network to extract a fixed set of forums and we have labeled data for that fixed set or fixed set
[00:56:17.120 --> 00:56:23.120]   of fields. But there's other work that tries to extract every field. So you just give it a form
[00:56:23.120 --> 00:56:27.200]   and it just tries to give you back a bunch of key value pairs and said, well, this was the name of
[00:56:27.200 --> 00:56:32.880]   this field and this was what was in it. And it gets arbitrary complicated if you have, you know,
[00:56:32.880 --> 00:56:38.400]   complicated formats in the document, like, you know, tables with split cells and, and sort of
[00:56:38.400 --> 00:56:44.960]   hierarchies of various sorts. But you can imagine encoding that as you know, a JSON object type of
[00:56:44.960 --> 00:56:50.560]   thing. And so we're, we're, we're starting to move in that direction. So I do think there's hope.
[00:56:50.560 --> 00:56:58.080]   One of the challenges is it's there aren't that many, like large datasets of random forms,
[00:56:58.080 --> 00:57:02.880]   interestingly enough. So hopefully our work will add to that pile.
[00:57:02.880 --> 00:57:10.320]   Yeah, actually. So Peter in the chat asked a relatively similar question. Is it realistic
[00:57:10.320 --> 00:57:14.640]   to hope for a universal embedding of forms in the next few years, the same way that we have
[00:57:14.640 --> 00:57:19.360]   universal embeddings for images, for vectors or for, for text? Is it just a matter of better
[00:57:19.360 --> 00:57:23.920]   datasets? Are there fundamental challenges you think that prevents something like that from
[00:57:23.920 --> 00:57:35.600]   happening? Gosh. Well, I'm going to see first, if any of the other members of my team are here,
[00:57:35.600 --> 00:57:48.320]   I want to, I want to try that one. Okay. What would you do with a universal embedding of forms?
[00:57:48.320 --> 00:57:51.680]   Right. So I think one of the main things you'd want to do is decode it into the,
[00:57:52.400 --> 00:57:59.520]   into the, you know, field, you know, key value type data. So I think that a universal form
[00:57:59.520 --> 00:58:07.760]   embedding has to be sort of equivalent to that. And that seems like it's possible. So yeah,
[00:58:07.760 --> 00:58:11.680]   I think a universal embedding is probably possible. I think some of the tricks we do
[00:58:11.680 --> 00:58:18.080]   with embeddings may not work in the same way. So, you know, if we're, if we do a paragraph
[00:58:18.080 --> 00:58:22.240]   embedding, we may be interested in the sort of topical similarity between two paragraphs.
[00:58:22.240 --> 00:58:28.160]   I'm not quite sure what a distance function over form data looks like. In particular,
[00:58:28.160 --> 00:58:33.920]   there's a sort of ambiguity between, do you want two documents to be close if they're the same
[00:58:33.920 --> 00:58:39.200]   type of form? Or do you want two documents to be close if there are, have data about similar
[00:58:39.200 --> 00:58:43.200]   objects, right? So do you want to put like all of the police reports from different precincts
[00:58:43.200 --> 00:58:49.280]   together? Or do you want to do the police report of a fire together with the fire department report
[00:58:49.280 --> 00:58:54.000]   of the same fire? So there's some, some complicated questions there about the sort of semantics of
[00:58:54.000 --> 00:59:00.400]   that space. Yeah, that is an excellent point. Yeah. It seems like most of the time omitting
[00:59:00.400 --> 00:59:04.480]   information from a form, the way that you like a middle word from a sentence, a mid a patch from
[00:59:04.480 --> 00:59:09.360]   an image and try and learn some reconstruction. Like if you're omitting pieces of the form that
[00:59:09.360 --> 00:59:13.840]   are shared across all the forms, then it's not that interesting to fill it in. And if you're
[00:59:13.840 --> 00:59:17.840]   omitting the information that is actually filled into the form in the specific fields, you know,
[00:59:17.840 --> 00:59:22.560]   often like there's no other information in the form other than the place where I wrote my
[00:59:22.560 --> 00:59:26.880]   birthday about what my birthday is. So it does seem like a really difficult challenge building
[00:59:26.880 --> 00:59:34.240]   that kind of embedding. Yeah, it's, it's a challenging knowledge representation problem,
[00:59:34.240 --> 00:59:40.320]   right? So in the way that predicting the next word sort of unlocked a whole bunch of progress
[00:59:40.320 --> 00:59:46.880]   in NLP, what was it five years ago? It's hard to believe that it's so recent. I don't think we
[00:59:46.880 --> 00:59:53.040]   found the like challenge problem that will drive progress in general form extraction.
[00:59:53.040 --> 01:00:01.040]   Well, you know, hopefully this, I think this deep form benchmark is important step in that,
[01:00:01.040 --> 01:00:05.360]   in that direction of building, you know, raising awareness about this problem of building public
[01:00:05.360 --> 01:00:09.840]   benchmarks on which people can work and, you know, sort of driving more research in that direction.
[01:00:09.840 --> 01:00:14.080]   So thanks for, for doing this work and for, and yeah, for putting this together.
[01:00:14.080 --> 01:00:18.160]   Yeah, well, thanks. Thanks to my, my team. A lot of folks worked on this.
[01:00:18.160 --> 01:00:27.360]   All right. Well, oh, Sammy Donnie on YouTube says no questions, but this was super cool and
[01:00:27.360 --> 01:00:33.840]   inspiring. Thanks. So on that happy note from Sammy, I think we will we'll close out here.
[01:00:33.840 --> 01:00:41.760]   So thanks to both teams. So to Peter and Carol and the folks who worked on the safe life benchmark
[01:00:41.760 --> 01:00:49.280]   to Jonathan Gray and Andrea, who for their work on the deep form benchmark, really cool talks,
[01:00:49.280 --> 01:00:56.640]   inspiring, I would say is correct. And also to to Stacy for helping with both of these projects and
[01:00:56.640 --> 01:01:02.560]   for always, you know, finding really cool people doing really cool work in machine learning and
[01:01:02.560 --> 01:01:06.720]   making sure we all get to hear about them. So thanks a lot to everyone.

