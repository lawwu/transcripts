
[00:00:00.000 --> 00:00:04.160]   What if we see some rhymes for a first-grade student in one poem?
[00:00:04.160 --> 00:00:10.000]   Jupiter is the fifth from the sun. It's the biggest in the solar system. It is the brightest of them
[00:00:10.000 --> 00:00:15.600]   all. The Romans named it after the god Jupiter. Is it really doing these rhymes?
[00:00:15.600 --> 00:00:22.480]   Hey, what is up everybody? Ivan from Weights & Biases here and in this video I'll show you a
[00:00:22.480 --> 00:00:28.000]   really quick and a really simple way that you can start using OpenAI's really cool GPT-3 models in
[00:00:28.000 --> 00:00:32.880]   Python. And if you have fine-tuned the GPT-3 model and wondering how you can use your fine-tuned GPT-3
[00:00:32.880 --> 00:00:38.080]   model, I'll also show you how you can do that in the video. Let's get started. So I would say the
[00:00:38.080 --> 00:00:42.960]   main reason as to why you would want to deploy GPT-3 in Python is if you wanted to build some
[00:00:42.960 --> 00:00:50.560]   sort of application using the magic of this really really cool large language model. So
[00:00:50.560 --> 00:00:56.880]   maybe you're coming into this video after you've played around with chatGPT which has very much
[00:00:56.880 --> 00:01:03.520]   taken the internet by storm in the last couple months. And chatGPT is essentially an application,
[00:01:03.520 --> 00:01:11.920]   a nicer way to ask questions from GPT-3 and essentially like a GPT-3 with a nicer UI and
[00:01:11.920 --> 00:01:17.360]   couple tweaks that make it possible. But the actual thing that generates text is still a
[00:01:17.360 --> 00:01:23.280]   variation of the GPT-3 model that they're using for chatGPT. And so if you want to do cool stuff
[00:01:23.280 --> 00:01:27.600]   with GPT-3, that's what we're gonna learn about using in this video and hopefully you'll build
[00:01:27.600 --> 00:01:32.720]   some cool applications for it. And this is going to be a series of videos where in this one we'll
[00:01:32.720 --> 00:01:38.400]   cover setting it up in Python but in the next videos we'll actually cover what kind of applications
[00:01:38.400 --> 00:01:45.120]   you can build with it. So stay tuned for that. And now let's jump into setting up an OpenAI
[00:01:45.120 --> 00:01:49.760]   account and getting the API keys and all that good stuff that we need to actually start using
[00:01:49.760 --> 00:01:56.400]   it in Python. So I am on the OpenAI website right now and to use the API we click on the API and
[00:01:56.400 --> 00:02:03.600]   that's the part where you need to either create a new account within OpenAI or log into an existing
[00:02:03.600 --> 00:02:09.600]   one. I already have an account but it's a fairly simple way to create it. So just go ahead, click
[00:02:09.600 --> 00:02:15.440]   sign up and put in your details and it'll get you an account. I'm gonna log into mine right now.
[00:02:15.520 --> 00:02:19.920]   [Music]
[00:02:19.920 --> 00:02:27.120]   All right, so now I've logged into the OpenAI API part of the OpenAI's website. So here's a
[00:02:27.120 --> 00:02:31.920]   few things that we can do right off the bat. For example, you can go to the playground here and
[00:02:31.920 --> 00:02:41.120]   here you can actually try to use GPT-3. Essentially, the same results that we'll get in this
[00:02:41.120 --> 00:02:46.720]   web client will be getting in Python via the API. So for example, maybe we want to
[00:02:46.720 --> 00:02:53.920]   "here's a poem about OpenAI API". So maybe we'll ask it to write a poem about OpenAI API.
[00:02:53.920 --> 00:03:06.320]   It's definitely not the most humble AI system that I've seen but
[00:03:08.960 --> 00:03:15.520]   I don't think it's very wrong. What's it saying here? So as you can see, it works and that's what
[00:03:15.520 --> 00:03:22.160]   we'll be getting to use in Python. And to use it in Python, we need to get an API key. So API key
[00:03:22.160 --> 00:03:30.160]   is essentially what will allow OpenAI to know that a particular Python program that's calling the API
[00:03:30.160 --> 00:03:37.360]   is attached to our account and this way they can, you know, charge you money if you're using it a
[00:03:37.360 --> 00:03:42.240]   lot and things like that. And if you're curious about prices, I'm showing you some on screen right
[00:03:42.240 --> 00:03:49.200]   now. As you can see, the prices vary by how powerful is the model and also by how much text
[00:03:49.200 --> 00:03:56.000]   you're generating with it. So here they're saying that the prices are per 1000 tokens. You can think
[00:03:56.000 --> 00:04:02.240]   of tokens as pieces of words where 1000 tokens is about 750 words. And as you can see, you can
[00:04:02.240 --> 00:04:09.120]   start experimenting for free using $18 of free credits which you can use in your first three
[00:04:09.120 --> 00:04:15.440]   months of using it and then you can pay as you go and, you know, you can choose your model. But
[00:04:15.440 --> 00:04:20.720]   we've already talked about the fact that you can choose your model. So yeah, refer to openai.com/
[00:04:20.720 --> 00:04:24.480]   api/pricing for the most relevant pricing information. But this is just for me to give
[00:04:24.480 --> 00:04:31.680]   you like an overview of what you can expect when you first start using the API. So now we're going
[00:04:31.680 --> 00:04:37.840]   to go and get our API keys. To do that, we'll click on our account. We'll click on view API keys.
[00:04:37.840 --> 00:04:42.640]   So here we can see the existing API keys that have been tied to our account and when they've
[00:04:42.640 --> 00:04:49.120]   been last used. And I'm going to create one right now to show in this video. But don't worry though,
[00:04:49.120 --> 00:04:55.200]   obviously, after I've finished editing and making this video, I'll click on
[00:04:55.200 --> 00:05:01.680]   delete this API key and, you know, it's... I'm going to show you an example but
[00:05:01.680 --> 00:05:08.880]   don't think that you can go and use it then because I've revoked it. So let's click on this
[00:05:08.880 --> 00:05:15.120]   button to create a new secret key. API key generated. So here it's saying that it's only
[00:05:15.120 --> 00:05:22.960]   showing us the API key once and once we've copied it and saved it somewhere safe, it's not going to
[00:05:22.960 --> 00:05:30.640]   show it to us again. It's okay. I'll copy it to clipboard and press okay. Now we have a new API
[00:05:30.640 --> 00:05:38.800]   key that we'll use in the code. And to do that, let's obviously jump in the code. So the first
[00:05:38.800 --> 00:05:45.040]   thing we'll do here is we'll paste our very secret API key here, which will essentially set up an
[00:05:45.040 --> 00:05:52.240]   environmental variable with the open AI key and that'll, in the future, will let the open AI
[00:05:52.240 --> 00:05:59.520]   Python client know what our API key is. So let's do that. So we've pasted the API key here,
[00:05:59.520 --> 00:06:06.480]   we run the self code and the variable is defined. So we'll install open AI, which is open AI's
[00:06:06.480 --> 00:06:13.280]   Python library, and we'll install 1DB, which is the Weights and Biases Python library. And
[00:06:13.280 --> 00:06:17.520]   Weights and Biases, if you're not familiar, is an MLOps platform which helps you with a lot of the
[00:06:17.520 --> 00:06:26.080]   machine learning pipeline things like tracking your data, training your models, optimizing the
[00:06:26.080 --> 00:06:32.240]   hyperparameters and all of that. And since we're looking at this video as an example of a use case
[00:06:32.240 --> 00:06:36.560]   where you want to go and develop some sort of application using GPT-3, it becomes really
[00:06:36.560 --> 00:06:42.880]   important that you have a way to keep track of your prompts because a really important part of
[00:06:42.880 --> 00:06:47.360]   building on top of GPT-3 is prompt engineering, which is kind of like figuring out a nice way
[00:06:47.360 --> 00:06:54.640]   to almost like ask GPT-3 to give you a desired outcome. And for that, it's very vital that you
[00:06:54.640 --> 00:06:59.600]   have a way to keep track while you're experimenting of your prompts and the completions that you're
[00:06:59.600 --> 00:07:04.480]   getting so that you would not lose the best performing prompts, for example. And that is
[00:07:04.480 --> 00:07:08.720]   what we're going to use Weights and Biases for in this video. So let's click on the cell and
[00:07:08.720 --> 00:07:12.880]   install the libraries. And kind of like an additional fun fact about the two companies is
[00:07:12.880 --> 00:07:18.880]   that OpenAI is the first customer of Weights and Biases and they actually use WNB to train all of
[00:07:18.880 --> 00:07:24.560]   this amazing GPT models. And when it comes to fine tuning, WNB is also integrated into OpenAI
[00:07:24.560 --> 00:07:28.640]   so that you can get like with one line of code, all of the training metrics from your fine tunes
[00:07:28.640 --> 00:07:33.600]   into Weights and Biases and all that good stuff. And all that is to kind of say that there's a lot
[00:07:33.600 --> 00:07:38.880]   of merit in using the two products together. So next up, we'll import some libraries and pass
[00:07:38.880 --> 00:07:45.280]   the API key to OpenAI. And so next up, we're going to go and define a new Weights and Biases run
[00:07:45.280 --> 00:07:51.040]   inside a project that we'll name GPT-3 App in Python. And we'll also define a new Weights and
[00:07:51.040 --> 00:07:57.840]   Biases table with two columns for prompt and completion as we want to keep track of that.
[00:07:57.840 --> 00:08:04.560]   And WNB tables is a product inside WNB that lets you interactively query and explore tabular data,
[00:08:04.560 --> 00:08:07.920]   which is what we're going to do in this video with all the cool predictions.
[00:08:08.720 --> 00:08:13.440]   And so now in the next cell, we'll actually perform inference on our prompt using GPT-3
[00:08:13.440 --> 00:08:20.320]   using the API. So here, let me make it larger. So here's the command in Python that we're using to
[00:08:20.320 --> 00:08:27.280]   access GPT-3. It's OpenAI.completion.create. And here we pass the name of the engine,
[00:08:27.280 --> 00:08:31.280]   which is, as you remember, there's a couple of variations of GPT-3s. There's different like
[00:08:31.280 --> 00:08:37.280]   Arababa, Shkuri, DaVinci models, and different versions of these models. For this one,
[00:08:37.280 --> 00:08:43.520]   we're going to use the most powerful, as of right now, text DaVinci version 3 model.
[00:08:43.520 --> 00:08:48.720]   Then we pass our prompt. As you can see, our prompt is we define it here. And our prompt is
[00:08:48.720 --> 00:08:54.000]   to correct this to standard English. She no went to the market. So it's going to do some grammar
[00:08:54.000 --> 00:09:00.080]   correction stuff. So we pass in our prompt. Then we define temperature. And temperature is a hyper
[00:09:00.080 --> 00:09:06.800]   parameter that is responsible for how random the predictions that GPT-3 gives us are. If it's like
[00:09:06.800 --> 00:09:13.120]   zero, it's always going to be the same thing. If it's closer to one, it's actually going to
[00:09:13.120 --> 00:09:18.560]   throw in some different stuff occasionally there. And then max tokens is for how many tokens do we
[00:09:18.560 --> 00:09:24.400]   want to run our predictions. So the more tokens we have, the more text GPT-3 can generate. But
[00:09:24.400 --> 00:09:30.720]   the key word here is may. So for example, when it finishes correcting this sentence to proper grammar,
[00:09:30.720 --> 00:09:36.480]   it'll output like a stop sequence, which means that GPT-3 is done with the predictions. Because
[00:09:36.480 --> 00:09:42.480]   if you think about it, it's just important for the model to know when to stop its generations as to
[00:09:42.480 --> 00:09:47.040]   be able to generate in general. And that is essentially trying to say that this hyper parameter
[00:09:47.040 --> 00:09:53.840]   controls how many tokens the model can predict in our case. But it does not mean that it'll always
[00:09:53.840 --> 00:09:58.400]   be 256 tokens, if that makes sense. Because sometimes it'll run to that stop sequence.
[00:09:58.400 --> 00:10:03.200]   And then there's stop. And then there's stop p, frequency penalty and presence penalty,
[00:10:03.200 --> 00:10:07.920]   which also we kind of use to guide the predictions a little bit. And then it prints out the response.
[00:10:07.920 --> 00:10:13.760]   And we add to the WMB table our prompt in the first column. And in the second column,
[00:10:13.760 --> 00:10:19.120]   the response that GPT-3 gave us. So let's finally run this cell and see what it gives us.
[00:10:19.120 --> 00:10:25.920]   So as you can see, we had she no went to the market. It transformed it into she did not go
[00:10:25.920 --> 00:10:32.080]   to the market, which is correct, if you ask me. And the logic of using WMB tables in this case is
[00:10:32.080 --> 00:10:35.600]   that we can go and we can run this cell with different prompts for a little bit. So for
[00:10:35.600 --> 00:10:43.120]   example, we'll ask it to correct he no went to the store instead of the market. It'll give us
[00:10:43.120 --> 00:10:50.400]   the prediction. He did not go to the store. All that stuff gets added to the WMB tables in this
[00:10:50.400 --> 00:10:55.200]   case. And then once we're done with the predictions, we can call WMB.log to log this
[00:10:55.200 --> 00:11:01.280]   predictions table and call WMB.finish to finish our particular run.
[00:11:01.280 --> 00:11:06.480]   And I'll kind of show you what that gives us right now. And so it gives us a link now to our
[00:11:06.480 --> 00:11:13.040]   project page and to this run to which we've logged this table so we can go and we can see here in the
[00:11:13.040 --> 00:11:18.960]   table our predictions. She no went to the market. She no went to the market. One time I did this
[00:11:18.960 --> 00:11:26.480]   off-camera to test it. And he no went to the store. He did not go to the store. So there we have it.
[00:11:26.480 --> 00:11:32.160]   And so the usage here is that you go, you try different prompts, you log them to table. Once
[00:11:32.160 --> 00:11:36.480]   you're done, you log the table and then you can explore your prompts. And by the way, you can find
[00:11:36.480 --> 00:11:40.800]   more information about this hyper parameters in the OpenAI docs. I'll leave a link to them in the
[00:11:40.800 --> 00:11:45.280]   video description. And if you want some inspiration for all the cool tasks that you can do with GPT-3,
[00:11:45.280 --> 00:11:52.480]   you can go to overview. Then you can go to like it's on the OpenAI API website, right? So you go
[00:11:52.480 --> 00:11:58.000]   to overview and then here you can click on examples and then examples here. We have all
[00:11:58.000 --> 00:12:02.880]   sorts of different tasks. And by the way, I've taken the grammar one right here. It's an example
[00:12:02.880 --> 00:12:08.480]   like it's an example right here in there in their docs in this way. And as you can see,
[00:12:08.480 --> 00:12:13.680]   it has all sorts of tasks. It has a skill translate classification. So here, for example,
[00:12:13.680 --> 00:12:19.600]   we have this like task example summarize for a second grader. So go here. The prompt is summarize
[00:12:19.600 --> 00:12:25.600]   this for a second grade student. Here's some information about Jupiter and we can go here.
[00:12:25.600 --> 00:12:30.320]   Look at the code. Look at the prompt. OK, so let's say, for example, I want to work closely
[00:12:30.320 --> 00:12:35.680]   with this task. So what I'll do is I'll go and I'll start a new run and wait and buy something
[00:12:35.680 --> 00:12:41.760]   new table because our previous run was around this task of correcting grammar. So I'll start
[00:12:41.760 --> 00:12:51.680]   a new one and then I'll go and I'll update a prompt. So copy this and paste it here as a prompt.
[00:12:51.680 --> 00:12:55.360]   And then there's also some hyper parameter difference here that OpenAI people have put
[00:12:55.360 --> 00:13:02.960]   for this task. So copy and paste the hyper parameters for this task right here. And let's
[00:13:02.960 --> 00:13:09.760]   try running it. So Jupiter is the fifth planet from the sun and the biggest in our solar system.
[00:13:09.760 --> 00:13:14.720]   It is so bright that it can cast shadows on Earth. It is named after the Roman god Jupiter.
[00:13:14.720 --> 00:13:21.040]   Also, and has been known to people since before recorded history. And we've started with
[00:13:21.040 --> 00:13:30.560]   some reason for second graders. It's a lot of math stuff. Yes, it's noticeably less complex than
[00:13:30.560 --> 00:13:34.320]   what we've started with, I would say. And let's say maybe we want to vary this a little bit. So
[00:13:34.320 --> 00:13:44.320]   let's try summarize this for, let's say, first grade student in one sentence. Let's say we'll
[00:13:44.320 --> 00:13:51.520]   make it like even simpler. We'll try this prompt. So it just gives us Jupiter is the fifth planet
[00:13:51.520 --> 00:13:59.360]   from the sun and it's very bright and big, named after the Roman god Jupiter. OK, so maybe we want
[00:13:59.360 --> 00:14:05.760]   to vary this prompt even more. What if we see some rises for first grade student in one poem?
[00:14:05.760 --> 00:14:10.240]   Because Jupiter three in recent update, it's gotten really good with rhyming. So
[00:14:10.240 --> 00:14:19.520]   OK, Jupiter is the fifth. Jupiter is the fifth from the sun. It's the biggest in the solar system.
[00:14:19.520 --> 00:14:25.200]   It is the brightest of them all. The Romans named it after the god Jupiter.
[00:14:26.080 --> 00:14:31.440]   Is it really doing this rhymes? So we know it by that name. It's so bright that you can
[00:14:31.440 --> 00:14:38.400]   cast shadows with its light. It's the third brightest in the night. Wow, this is pretty
[00:14:38.400 --> 00:14:42.160]   cool. Yeah, it's quite fun and you can do a lot of variability with this prompt. And also,
[00:14:42.160 --> 00:14:48.000]   like this rhymes, like if it's really pulling up this rhymes, sun, system, like there's some
[00:14:48.000 --> 00:14:54.160]   cool rhymes. And keep in mind, we've been lagging all of this like prompting completion pairs,
[00:14:54.160 --> 00:14:58.480]   W and B table, so that we if we stumble on some of this, the cool prompt ideas,
[00:14:58.480 --> 00:15:03.600]   we're not losing them and we can go and log them to weights and biases now and then click the link
[00:15:03.600 --> 00:15:11.840]   to navigate to our run page to which we've logged the table and then see our prompts and completions.
[00:15:11.840 --> 00:15:17.920]   So here's the yeah, so here's the latest one summarizes for
[00:15:19.040 --> 00:15:24.640]   a first grade student in the poem. And here are previous ones. This way you can keep track of
[00:15:24.640 --> 00:15:30.720]   all that good stuff. And the last thing I wanted to show you is how you can use a fine tuned GPT-3
[00:15:30.720 --> 00:15:36.480]   model in Python using the API as well. In the video, which will pop up somewhere in one of
[00:15:36.480 --> 00:15:43.200]   the corners, I find GPT-3 to generate new Doctor Who episodes, snapshots, like come up with new
[00:15:43.200 --> 00:15:49.840]   sci-fi TV show ideas. And I'm going to show you how you can use those types of fine tune models
[00:15:49.840 --> 00:15:54.960]   in the API as well. So a couple of things you need to change. So in that video, like I've done a
[00:15:54.960 --> 00:16:00.080]   detailed explanation of all the steps there. So go and watch that video to learn how I find it and
[00:16:00.080 --> 00:16:03.840]   all that good stuff. And so to use a fine tune model, you need to know the name of that fine
[00:16:03.840 --> 00:16:08.720]   tune. And in that video, we use the opening and W and B integration. So I've been logging
[00:16:08.720 --> 00:16:13.040]   all that stuff to weights and biases. And here, for example, let's say I want to use this model
[00:16:13.040 --> 00:16:22.880]   with its name, Curie, FT, 1DB, et cetera, et cetera. I go and I copy the name of the fine tune
[00:16:22.880 --> 00:16:31.040]   and then I paste the name of the fine tune in the name of the engine. Like, let's see, let's see.
[00:16:31.040 --> 00:16:37.520]   I do it like this because that's like our engine and it's the name of the model and then we'll
[00:16:37.520 --> 00:16:43.360]   find tune on that model. And if that fine tune is in your open AI account, it will be able to
[00:16:43.360 --> 00:16:49.840]   access it via the API in this way. And then we also need to, we can play with parameters,
[00:16:49.840 --> 00:16:55.600]   but I don't think that's necessary. I mean, maybe we'll give it more tokens to generate with,
[00:16:55.600 --> 00:17:02.960]   maybe a temperature seems good. And we also need to change our prompt to the type of prompt
[00:17:02.960 --> 00:17:07.600]   completion structure that we've used for fine tuning. So for example, in the Dr. Who fine
[00:17:07.600 --> 00:17:14.560]   tunes, I've used imaginary name of an episode. So let's, for example, try invasion of alien fish.
[00:17:14.560 --> 00:17:18.880]   And then this like arrow symbol to know the transition from where the prompt ends to where
[00:17:18.880 --> 00:17:24.320]   the completion should begin. That's like the fine tuning stuff. Like, watch that video. I go into
[00:17:24.320 --> 00:17:29.280]   way more depth on this particular topic. So now that we've done that, I'll go and I'll start a
[00:17:29.280 --> 00:17:34.400]   new weights and biases run and we'll run this cell to perform inference on our prompt using
[00:17:34.400 --> 00:17:39.680]   the fine tuned model. So here we can see the results, the completion for the prompt invasion
[00:17:39.680 --> 00:17:45.680]   of alien fish. An ancient race of amphibious creatures known only as the invasions have
[00:17:45.680 --> 00:17:50.880]   taken over the British coastline and are now in a bid to take over the world. Their leader Ingrid
[00:17:50.880 --> 00:17:55.600]   has kidnapped the doctor's companion, Joe Grant, and forced the doctor to come to their aid.
[00:17:55.600 --> 00:18:01.600]   Doctor must then go on a dangerous mission to defeat the invaders. But we'll hear return with
[00:18:01.600 --> 00:18:09.040]   news of Ingrid's weakness and new credits followed by the epilogue. Yeah, it goes a bit of the rail
[00:18:09.040 --> 00:18:15.120]   here with the ending stuff. So I think it may benefit here if we also add stop sequence. So as
[00:18:15.120 --> 00:18:18.160]   you can see in the last example, it kind of went over the rails at the end because I forgot to
[00:18:18.160 --> 00:18:22.640]   include a stop sequence. So here in the docs, I found that we can pass the agreement stop.
[00:18:22.640 --> 00:18:27.440]   And so here I'll put the stop sequence. I think it was just end for that particular fine tune.
[00:18:27.440 --> 00:18:32.800]   And let's try to generate some more. And so it's 90s and started us hurling towards a futuristic
[00:18:32.800 --> 00:18:39.440]   factory on earth. Yada, yada. And yeah, as you can see now, it ends normally when the factory
[00:18:39.440 --> 00:18:44.800]   explodes and destroys a life on earth. Well, not normally in the synopsis sense, but normally in
[00:18:44.800 --> 00:18:49.600]   the sense that it's not spamming all of like and and then then then then stop sequences.
[00:18:49.600 --> 00:18:54.800]   And it actually stops generation when it stumbles into an end. So that's like a useful one to know
[00:18:54.800 --> 00:19:02.000]   when using this when using your fine tune models with the open API in Python. And in the end,
[00:19:02.000 --> 00:19:08.800]   we'll lock the table and call one DB that finish. And here we can see all the prompt and their
[00:19:08.800 --> 00:19:14.000]   completions that we've tried this time. So thank you so much for watching this video. We've covered
[00:19:14.000 --> 00:19:18.800]   all the stuff that I could think of that can be useful when deploying GPT-3 in Python with the
[00:19:18.800 --> 00:19:23.840]   open API. And if you have any questions, please feel free to leave them in the comment section
[00:19:23.840 --> 00:19:29.120]   down below and I'll be happy to answer you. And if you like this video, consider smashing the like
[00:19:29.120 --> 00:19:34.720]   button and subscribing to our channel to see more tutorials, interviews and talks. And yeah, thank
[00:19:34.720 --> 00:19:38.960]   you so much for watching this video. I really hope you enjoyed it and found it useful.

