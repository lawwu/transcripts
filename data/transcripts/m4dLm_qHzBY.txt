
[00:00:00.000 --> 00:00:10.400]   [MUSIC]
[00:00:10.400 --> 00:00:13.760]   >> Okay. This panel that we have together,
[00:00:13.760 --> 00:00:17.320]   today we're going to talk about MLOps tooling.
[00:00:17.320 --> 00:00:22.480]   The background is, we keep hearing from colleagues and friends in
[00:00:22.480 --> 00:00:25.140]   the industry that data science and machine learning,
[00:00:25.140 --> 00:00:28.280]   and in some ways a lot harder today,
[00:00:28.280 --> 00:00:32.520]   than it feels like it was maybe a decade ago.
[00:00:32.520 --> 00:00:36.080]   The thing that we keep hearing is we're constantly inundated
[00:00:36.080 --> 00:00:39.640]   with new MLOps tools and data tools.
[00:00:39.640 --> 00:00:41.760]   It can be really difficult to navigate all of
[00:00:41.760 --> 00:00:45.120]   these things and figure out which ones are right for you.
[00:00:45.120 --> 00:00:49.320]   What we have today is a panel,
[00:00:49.320 --> 00:00:54.800]   all-star panel of data scientists and people who build tools as well.
[00:00:54.800 --> 00:00:57.000]   All of these people at some point in
[00:00:57.000 --> 00:00:59.600]   their careers have been hands-on data scientists
[00:00:59.600 --> 00:01:02.280]   or engineers in the trenches.
[00:01:02.280 --> 00:01:05.640]   They've all experienced the pain of
[00:01:05.640 --> 00:01:09.640]   operationalizing data and ML products.
[00:01:09.640 --> 00:01:13.360]   What we're hoping to do is talk about
[00:01:13.360 --> 00:01:15.840]   these various issues and opportunities with
[00:01:15.840 --> 00:01:18.240]   these explosion of tools and how to navigate that,
[00:01:18.240 --> 00:01:20.080]   how to think about it.
[00:01:20.080 --> 00:01:23.920]   I'm going to go ahead and introduce our panel.
[00:01:23.920 --> 00:01:28.840]   Brian was not able to be with us today because he's sick.
[00:01:28.840 --> 00:01:31.560]   I'll start with Lindsay.
[00:01:31.560 --> 00:01:34.560]   Lindsay, I met her at Airbnb and she's
[00:01:34.560 --> 00:01:38.240]   definitely one of the most impressive people I've ever met.
[00:01:38.240 --> 00:01:41.320]   She did a lot of different things at Airbnb.
[00:01:41.320 --> 00:01:45.400]   She touched almost every area of data science I can think of.
[00:01:45.400 --> 00:01:47.920]   She worked on engagements, notifications,
[00:01:47.920 --> 00:01:50.320]   booking, conversions, sign-up, login.
[00:01:50.320 --> 00:01:53.280]   She worked on international growth.
[00:01:53.280 --> 00:01:56.760]   A lot of product analytics in general.
[00:01:56.760 --> 00:01:59.720]   She's been hands-on with probably
[00:01:59.720 --> 00:02:03.760]   the largest breadth of disciplines in data science I can think of.
[00:02:03.760 --> 00:02:06.720]   She's worked a lot on experimentation and analytics.
[00:02:06.720 --> 00:02:09.360]   She's done machine learning as well.
[00:02:09.360 --> 00:02:14.000]   Most people only get chance to work in one of those.
[00:02:14.000 --> 00:02:16.320]   She's actually worked across a lot of them.
[00:02:16.320 --> 00:02:19.880]   Then recently, she found Iggy.
[00:02:19.880 --> 00:02:25.760]   It's a company to make location-based data accessible to everybody.
[00:02:25.760 --> 00:02:28.720]   I think it's really interesting.
[00:02:28.720 --> 00:02:31.840]   I find Lindsay's path to be extremely interesting.
[00:02:31.840 --> 00:02:36.560]   She worked on such a wide range of data products,
[00:02:36.560 --> 00:02:38.440]   but she gravitated back towards what I think
[00:02:38.440 --> 00:02:42.120]   is one of the most important parts,
[00:02:42.120 --> 00:02:44.920]   is having high-quality data.
[00:02:44.920 --> 00:02:47.200]   I think this is one of the things that's really
[00:02:47.200 --> 00:02:49.520]   under-looked or under-appreciated,
[00:02:49.520 --> 00:02:51.680]   especially when building ML tooling
[00:02:51.680 --> 00:02:53.320]   and building your ML stack,
[00:02:53.320 --> 00:02:54.360]   that people don't talk about that.
[00:02:54.360 --> 00:02:59.880]   But it's actually the bottleneck that I personally encounter.
[00:02:59.880 --> 00:03:02.600]   Also, just like the startup,
[00:03:02.600 --> 00:03:06.640]   the company she found to work on geospatial data,
[00:03:06.640 --> 00:03:12.720]   I personally always find that I'm lacking quality geospatial data.
[00:03:12.720 --> 00:03:15.480]   I'm really excited that she's on this panel.
[00:03:15.480 --> 00:03:18.280]   The second person is Xander.
[00:03:18.280 --> 00:03:22.000]   Actually, I also work with Xander at GitHub.
[00:03:22.000 --> 00:03:27.720]   He actually built a lot of the tools for ML and analytics there.
[00:03:27.720 --> 00:03:30.800]   He has a crazy amount of technical experience
[00:03:30.800 --> 00:03:33.720]   across lots of different infrastructure.
[00:03:33.720 --> 00:03:39.280]   It's surprising, he's a Swiss army knife in terms of ML and tooling.
[00:03:39.280 --> 00:03:43.000]   He's one of the best engineers and people I ever worked with.
[00:03:43.000 --> 00:03:46.400]   It's hard to categorize him as a specific type of person
[00:03:46.400 --> 00:03:49.080]   just because he does so many different things.
[00:03:49.080 --> 00:03:51.560]   Now, he's a CEO of ByteWax,
[00:03:51.560 --> 00:03:57.200]   which is a really cool platform for streaming data.
[00:03:57.200 --> 00:04:00.600]   It's also open source, so you can check it out.
[00:04:00.600 --> 00:04:04.160]   What I really like about Xander is he has a lot of empathy for
[00:04:04.160 --> 00:04:06.720]   the data scientists being on himself previously,
[00:04:06.720 --> 00:04:09.560]   and it shows up in the way he builds tools.
[00:04:09.560 --> 00:04:12.240]   Then the last person that I want to introduce,
[00:04:12.240 --> 00:04:14.920]   last but not least, is Jeremy Lui.
[00:04:14.920 --> 00:04:20.080]   I met Jeremy while doing open source work on ML tooling.
[00:04:20.080 --> 00:04:26.920]   Jeremy co-founded and was a technical leader for Kubeflow at Google,
[00:04:26.920 --> 00:04:29.080]   which is one of the most popular machine learning
[00:04:29.080 --> 00:04:31.760]   infrastructure tools in use today.
[00:04:31.760 --> 00:04:37.640]   Jeremy eventually moved on from Google to lead ML infrastructure at Primer.
[00:04:37.640 --> 00:04:40.440]   If you're not familiar with Primer,
[00:04:40.440 --> 00:04:44.560]   it offers AI platforms, especially for NLP.
[00:04:44.560 --> 00:04:49.440]   Then recently, Jeremy left Primer to explore other areas
[00:04:49.440 --> 00:04:53.000]   with respect to ML tooling like federated learning.
[00:04:53.000 --> 00:04:57.480]   He's currently building his own thing right now.
[00:04:57.480 --> 00:05:03.520]   Just to kick this off a bit,
[00:05:03.520 --> 00:05:08.080]   and feel free to just chime in at any time.
[00:05:08.640 --> 00:05:10.840]   My first question is,
[00:05:10.840 --> 00:05:16.360]   there's so many different points solutions.
[00:05:16.360 --> 00:05:17.480]   When we talk about tooling,
[00:05:17.480 --> 00:05:22.920]   all of you folks have built tools in our building tools.
[00:05:22.920 --> 00:05:25.280]   There's so many things that you have to think about,
[00:05:25.280 --> 00:05:27.360]   like orchestrators, feature stores,
[00:05:27.360 --> 00:05:29.280]   metric stores, data labeling,
[00:05:29.280 --> 00:05:34.080]   model monitoring, surveying, reverse ETL, experiment tracking.
[00:05:34.080 --> 00:05:36.840]   It just keeps going on and on.
[00:05:37.480 --> 00:05:41.680]   A lot of people are overwhelmed by all of these things.
[00:05:41.680 --> 00:05:44.960]   They're also being told implicitly,
[00:05:44.960 --> 00:05:50.000]   many times, if you're not using many of these things, you're doing it wrong.
[00:05:50.000 --> 00:05:55.960]   How do you begin to navigate the space of this tooling,
[00:05:55.960 --> 00:05:57.680]   if you're trying to build ML infrastructure?
[00:05:57.680 --> 00:06:01.240]   How do you begin to think about all this stuff?
[00:06:02.920 --> 00:06:07.600]   Feel free to, okay, I'll choose Lindsay to go first.
[00:06:07.600 --> 00:06:12.200]   >> Wow, it's a big question.
[00:06:12.200 --> 00:06:23.520]   I'm hesitating to share this answer,
[00:06:23.520 --> 00:06:26.080]   but I'll just go for it.
[00:06:26.080 --> 00:06:29.600]   You know me, so this won't be a surprise.
[00:06:29.600 --> 00:06:32.520]   But for me, it's always problem-centric.
[00:06:32.520 --> 00:06:37.120]   I feel like there's often like, okay, well, we need ML.
[00:06:37.120 --> 00:06:38.400]   Let's take a step back.
[00:06:38.400 --> 00:06:41.120]   What's the problem you're trying to answer?
[00:06:41.120 --> 00:06:47.040]   How is that problem necessarily suited to machine learning?
[00:06:47.040 --> 00:06:49.360]   This is a very simple thing to ask,
[00:06:49.360 --> 00:06:53.920]   but I think you're absolutely right in that there is so much pressure,
[00:06:53.920 --> 00:07:00.200]   buzzwords, this whole ecosystem that we all work in,
[00:07:00.200 --> 00:07:02.720]   live in, make our careers out of,
[00:07:02.720 --> 00:07:08.880]   puts a lot of expectations and pressure on us often to jump to solutions
[00:07:08.880 --> 00:07:11.240]   that are way too complicated for the problem.
[00:07:11.240 --> 00:07:13.800]   So I could go on and on about that,
[00:07:13.800 --> 00:07:16.280]   but I'll stop there because I'm intrigued to hear what others have to say.
[00:07:16.280 --> 00:07:20.160]   So for me, it's just like, first of all, take a step back.
[00:07:20.160 --> 00:07:24.160]   Define the problem that you're trying to solve,
[00:07:24.160 --> 00:07:31.800]   rather than looking for a solution to solve a problem that you haven't defined.
[00:07:31.800 --> 00:07:36.440]   So yeah, throwing that out there.
[00:07:36.440 --> 00:07:37.800]   - I'll add to that.
[00:07:37.800 --> 00:07:41.120]   I think when you're picking a tool for an organization,
[00:07:41.120 --> 00:07:45.400]   I think it helps to try to understand like how far you can push an organization
[00:07:45.400 --> 00:07:46.920]   and in what directions.
[00:07:46.920 --> 00:07:51.800]   So, you know, typically, anytime you introduce some new tool in an organization,
[00:07:51.800 --> 00:07:57.440]   you need to get adoption and you will probably face some form of uphill adoption.
[00:07:57.440 --> 00:07:59.080]   People often want to roll their own.
[00:07:59.080 --> 00:08:07.840]   So I think understanding, you know, in what directions you can, you know, push folks.
[00:08:07.840 --> 00:08:13.760]   So as an example, you know, we were trying to adopt an ETL tool
[00:08:13.760 --> 00:08:18.120]   and we were looking at various ETL tools and trying to figure out
[00:08:18.120 --> 00:08:22.400]   what data processing framework could look, would work best.
[00:08:22.400 --> 00:08:24.560]   And I think if you look at it, right, like traditionally,
[00:08:24.560 --> 00:08:27.400]   the ETL frameworks were written in Java.
[00:08:27.400 --> 00:08:30.840]   And if you have a Java framework, they're often, you know,
[00:08:30.840 --> 00:08:33.000]   much better at scale and in performance.
[00:08:33.000 --> 00:08:38.800]   But, you know, we were a Python shop and like pushing folks to adopt Java
[00:08:38.800 --> 00:08:41.080]   just did not seem like a tractable proposition.
[00:08:41.080 --> 00:08:45.400]   So, you know, I think that knowing those like really constrains the space
[00:08:45.400 --> 00:08:48.680]   because as soon as you start, we're like, OK, I need a Python ETL framework.
[00:08:48.680 --> 00:08:52.920]   Suddenly the space gets a lot narrower and becomes easier to decide.
[00:08:52.920 --> 00:08:59.520]   Also, maybe a build on what Jeremy was saying,
[00:08:59.520 --> 00:09:02.480]   but first I wanted to say thank you for the sales pitch on ByteWax.
[00:09:02.480 --> 00:09:06.720]   Go check it out if you're a Python shop.
[00:09:06.720 --> 00:09:12.040]   Yeah, so I mean, what I want to build on is just like that organizational piece.
[00:09:12.040 --> 00:09:17.280]   Machine learning often comes in as like a optimization or like a force multiplier.
[00:09:17.280 --> 00:09:21.160]   And it's maybe not the first thing that you use to solve your problem.
[00:09:21.160 --> 00:09:24.520]   Maybe coming back to what Lindsey said as well.
[00:09:24.520 --> 00:09:30.640]   And so the organization has probably developed all sorts of like operational maturity,
[00:09:30.640 --> 00:09:32.680]   I don't know, practices, et cetera.
[00:09:32.680 --> 00:09:37.360]   And I think that's what is really important is looking at those and using those
[00:09:37.360 --> 00:09:41.120]   because at the end of the day, MLOps, some of it is around like operational maturity.
[00:09:41.120 --> 00:09:46.720]   It's like, how can you now run and use this thing at scale and understand it, et cetera, et cetera.
[00:09:46.720 --> 00:09:52.320]   And so I would always try and look at what your organization is already using
[00:09:52.320 --> 00:09:57.480]   and how if you're bringing in new tools, how they will fit into the organization
[00:09:57.480 --> 00:10:02.880]   to kind of minimize, I don't know, blowback of choosing something that ends up in like,
[00:10:02.880 --> 00:10:07.040]   you know, the scope just blows out and you have this huge maintenance burden, et cetera.
[00:10:09.360 --> 00:10:11.440]   Yeah, I don't know. That's what I'll say.
[00:10:11.440 --> 00:10:12.680]   That's really interesting.
[00:10:12.680 --> 00:10:18.160]   I guess to kind of build on this question is, okay, let's say you figured out,
[00:10:18.160 --> 00:10:23.440]   and I love Lindsey's point about asking if ML is even needed.
[00:10:23.440 --> 00:10:25.640]   I really love that.
[00:10:25.640 --> 00:10:30.040]   Also, I love to ask people this question.
[00:10:30.040 --> 00:10:34.880]   But once you figure out, once you, let's say, decide that, okay, ML is needed
[00:10:34.880 --> 00:10:41.840]   and after you map out these constraints, it still feels like there's tons of tools.
[00:10:41.840 --> 00:10:44.040]   And it's pretty crazy.
[00:10:44.040 --> 00:10:48.440]   And people, even if you look at a point solution,
[00:10:48.440 --> 00:10:54.080]   even like experiment tracking systems, for example, there's quite a few of those.
[00:10:54.080 --> 00:11:00.160]   And then so a lot of people ask me, how do you navigate that space?
[00:11:00.160 --> 00:11:02.880]   Do you just try all of them?
[00:11:02.880 --> 00:11:04.920]   You know, how do you, is there a way to think about that?
[00:11:04.920 --> 00:11:09.840]   Should you sort of try to buy more of them?
[00:11:09.840 --> 00:11:16.360]   Like, you know, how do you think about monoliths versus point solutions?
[00:11:16.360 --> 00:11:21.240]   And I guess a related question is that may be helpful is,
[00:11:21.240 --> 00:11:25.120]   what mistakes have you seen people make when, you know,
[00:11:25.120 --> 00:11:27.600]   trying to select tools or build tools,
[00:11:27.600 --> 00:11:32.200]   having built tools and having done this yourself in organizations?
[00:11:32.200 --> 00:11:39.720]   I guess I can go first.
[00:11:39.720 --> 00:11:44.000]   I mean, I think a good way to get started is trying to understand
[00:11:44.000 --> 00:11:46.480]   where you are in your ML/AI journey.
[00:11:46.480 --> 00:11:50.920]   And I think for me, a key sort of distinction is, you know,
[00:11:50.920 --> 00:11:59.280]   have you proven yet that ML can sort of impact key business metrics
[00:11:59.280 --> 00:12:01.080]   and are sort of critical to the business, right?
[00:12:01.080 --> 00:12:03.800]   So I think, you know, in certain industries like video,
[00:12:03.800 --> 00:12:08.200]   if you're doing any sort of recommendation engine for e-commerce or,
[00:12:08.200 --> 00:12:13.640]   you know, videos or music, I think it's pretty clear that without recommendation,
[00:12:13.640 --> 00:12:17.360]   without ML and recommendations to power discovery,
[00:12:17.360 --> 00:12:23.480]   like your ability to drive traffic and make revenue is not going to work very well.
[00:12:23.480 --> 00:12:25.200]   So it's, I mean, I think it's there.
[00:12:25.200 --> 00:12:28.080]   It's already common practice that like you need those components
[00:12:28.080 --> 00:12:31.080]   and therefore you're going to invest heavily in making them work well.
[00:12:31.080 --> 00:12:35.200]   I think in a lot of places where ML is more on the frontier,
[00:12:35.200 --> 00:12:41.120]   it's not proven that ML can impact the business yet.
[00:12:41.120 --> 00:12:46.120]   So, and I think if you're at that point where you haven't proven that,
[00:12:46.120 --> 00:12:48.280]   you know, ML can impact the business,
[00:12:48.280 --> 00:12:50.960]   then I think that's the problem you need to focus on.
[00:12:50.960 --> 00:12:56.800]   And then that probably affects the decisions about which tools
[00:12:56.800 --> 00:12:59.320]   and problems you need to solve because you don't want to over-engineer things.
[00:12:59.320 --> 00:13:01.760]   And I think, you know, there's a lot of, you know,
[00:13:01.760 --> 00:13:04.840]   people talk about, you know, drift detection, for example.
[00:13:04.840 --> 00:13:08.360]   But, you know, if your models aren't impacting the business,
[00:13:08.360 --> 00:13:11.200]   then, you know, what do you, does drift detection matter?
[00:13:11.200 --> 00:13:14.000]   Because it's not going to, you know, if your model is performing worse,
[00:13:14.000 --> 00:13:14.840]   it doesn't matter, right?
[00:13:14.840 --> 00:13:17.160]   So, I think that's really helpful.
[00:13:17.160 --> 00:13:20.640]   And then based on that, you can decide what you're optimizing for, right?
[00:13:20.640 --> 00:13:23.960]   So, if you're trying to prove, you know, ML is going to impact the business,
[00:13:23.960 --> 00:13:27.280]   you probably want to optimize for velocity.
[00:13:27.280 --> 00:13:28.800]   And then once you've proven that,
[00:13:28.800 --> 00:13:32.920]   then that's how you can make the justification for doing further investment
[00:13:32.920 --> 00:13:36.440]   to, you know, improve and refine and track.
[00:13:36.440 --> 00:13:38.920]   So, one question I have about this, and that's a good point,
[00:13:38.920 --> 00:13:43.040]   is this chicken and egg problem that we always get into.
[00:13:43.040 --> 00:13:46.040]   Is, you know, like ML on the frontier,
[00:13:46.040 --> 00:13:48.560]   you know, there's a lot of businesses that want to use ML
[00:13:48.560 --> 00:13:49.800]   that hasn't necessarily proven it.
[00:13:49.800 --> 00:13:54.840]   I would say most businesses are like this in some sense.
[00:13:54.840 --> 00:13:57.000]   And then, you know, the argument always, you know,
[00:13:57.000 --> 00:14:00.040]   the data scientist lands, the ML person lands and says,
[00:14:00.040 --> 00:14:02.040]   "Oh, wow, there's no tooling.
[00:14:02.040 --> 00:14:03.600]   You know, I need some tooling."
[00:14:03.600 --> 00:14:07.480]   And then the business says, "Well, prove it first."
[00:14:07.480 --> 00:14:09.240]   And so, the vicious cycle.
[00:14:09.240 --> 00:14:10.840]   How do you think about that?
[00:14:10.840 --> 00:14:12.840]   I mean, that comes up a lot.
[00:14:12.840 --> 00:14:22.360]   Anyone can answer the question if you want.
[00:14:22.360 --> 00:14:23.880]   Feel free to.
[00:14:23.880 --> 00:14:29.720]   I mean, we've probably all been in some sort of situation that is like this.
[00:14:29.720 --> 00:14:37.440]   And I think it comes down for the willingness for the business to take a risk on it.
[00:14:37.440 --> 00:14:41.160]   If it's brand new, it's like a research and development area.
[00:14:41.160 --> 00:14:44.760]   So, is the business one that they're willing to spend money on
[00:14:44.760 --> 00:14:50.040]   this sort of research development to get potentially incremental gains or no gains?
[00:14:50.040 --> 00:14:56.520]   If it's a proven path, then I think that there should be some way to,
[00:14:56.520 --> 00:15:00.720]   you know, show up with an argument that there will be ROI
[00:15:00.720 --> 00:15:05.960]   and that you should have some budget for tooling and a team, et cetera.
[00:15:05.960 --> 00:15:10.760]   I want to jump in here in that, like, this is how new companies are born, right?
[00:15:10.760 --> 00:15:16.920]   Like, Iggy came out of my experience at Airbnb and a lack of tooling
[00:15:16.920 --> 00:15:19.800]   for what I consider to be a really big problem,
[00:15:19.800 --> 00:15:22.520]   which, you know, now two years after I've left Airbnb,
[00:15:22.520 --> 00:15:27.760]   I've also realized that and, you know, released a product
[00:15:27.760 --> 00:15:31.960]   very much in the realm of, like, the tooling that I have gone on to create at Iggy
[00:15:31.960 --> 00:15:33.360]   and that I wanted to create at Airbnb.
[00:15:33.360 --> 00:15:38.400]   So, this is a bit of a broader answer in that I think we often think about
[00:15:38.400 --> 00:15:43.480]   career success or success in our roles in terms of, like, these narrow, narrow lanes,
[00:15:43.480 --> 00:15:48.120]   which could very well be, like, this chicken and egg problem.
[00:15:48.120 --> 00:15:52.440]   And for some people, success is, yes, convincing leadership that, like,
[00:15:52.440 --> 00:15:53.960]   you know, here's how we do it.
[00:15:53.960 --> 00:15:57.120]   Another path, obviously I'm very biased here, but, you know,
[00:15:57.120 --> 00:16:00.080]   another path to success is building a company around it, right?
[00:16:00.080 --> 00:16:05.400]   And I know that the market is changing, but it's still a very generous time
[00:16:05.400 --> 00:16:11.840]   in terms of taking some of these risks around, you know, around some of these ideas.
[00:16:11.840 --> 00:16:13.120]   So.
[00:16:13.120 --> 00:16:14.400]   - Dayana, please.
[00:16:14.400 --> 00:16:15.400]   - Yeah, sorry.
[00:16:15.400 --> 00:16:18.440]   - I mean, I think the thing that I would add is, like, you know,
[00:16:18.440 --> 00:16:24.440]   while I think the ecosystem of tools is still evolving and immature in certain areas,
[00:16:24.440 --> 00:16:27.320]   in other areas, it's fairly mature, right?
[00:16:27.320 --> 00:16:32.240]   Like, compared to a couple of, you know, five years ago where getting a GPU is really hard,
[00:16:32.240 --> 00:16:37.400]   getting a GPU these days is fairly easy, you know, assuming you have access to one
[00:16:37.400 --> 00:16:40.880]   of the public clouds, which I think most companies probably do, right?
[00:16:40.880 --> 00:16:47.320]   So I think if you look at most of the public clouds, like, you have a lot of hosted services
[00:16:47.320 --> 00:16:52.920]   for, you know, the core building blocks of getting started with data science, you know,
[00:16:52.920 --> 00:16:57.760]   whether it's running a notebook, getting a GPU, deploying a model,
[00:16:57.760 --> 00:17:00.800]   auto ML services, natural language services, et cetera.
[00:17:00.800 --> 00:17:09.840]   So I think, you know, if you haven't proven out ML yet at your organization, you know,
[00:17:09.840 --> 00:17:14.800]   I would say you probably don't need to be building your own tools.
[00:17:14.800 --> 00:17:18.560]   You should probably be focused on, like, what tools can I use out of the box?
[00:17:18.560 --> 00:17:25.040]   And, you know, how can I just make them work and build something quick to prove the value?
[00:17:25.040 --> 00:17:26.120]   >> That's a great point.
[00:17:26.120 --> 00:17:33.120]   And I think as well, you know, the fragmentation of the landscape is real
[00:17:33.120 --> 00:17:38.520]   and it is overwhelming as Hemel, you know, said.
[00:17:38.520 --> 00:17:42.680]   And I think, you know, doing -- we all have to do our own research, right?
[00:17:42.680 --> 00:17:46.520]   Like, you know, to a certain extent about, you know,
[00:17:46.520 --> 00:17:48.840]   where to start given what's out there.
[00:17:48.840 --> 00:17:54.480]   And, you know, this is more related to, like, go-to-market strategies of many of these, well,
[00:17:54.480 --> 00:17:56.240]   younger startups and some of these companies.
[00:17:56.240 --> 00:18:01.200]   It's like, it does make a difference when you can, like, get started on your own, right?
[00:18:01.200 --> 00:18:04.600]   I put that very much in the realm of this sort of research.
[00:18:04.600 --> 00:18:06.560]   You know, Hemel, do you try everything?
[00:18:06.560 --> 00:18:07.640]   Probably not.
[00:18:07.640 --> 00:18:13.440]   But, like, you know, there's these amazing data communities on Slack, on Twitter, you know,
[00:18:13.440 --> 00:18:15.600]   where you can get some purchase about where to start.
[00:18:15.600 --> 00:18:18.200]   And, of course, there's always bias there, so you have to root through it.
[00:18:18.200 --> 00:18:24.800]   But I do think there is something quite remarkable about, you know,
[00:18:24.800 --> 00:18:32.360]   some of these go-to-market models, which are now very common, right, that allow us, as ICs,
[00:18:32.360 --> 00:18:36.840]   to make purchase on -- there's a pun there that wasn't intended.
[00:18:36.840 --> 00:18:44.320]   But, like, you know, make progress in terms of some of these areas of work before, you know,
[00:18:44.320 --> 00:18:46.280]   in order to help convince an organization.
[00:18:46.280 --> 00:18:52.200]   So, I don't know, it's kind of an exciting time in terms of that, like, how much you can, you know,
[00:18:52.200 --> 00:18:55.520]   do as an IC to help your cause here.
[00:18:55.520 --> 00:18:59.680]   >> Yeah, that makes a lot of sense.
[00:18:59.680 --> 00:19:04.960]   I like that you mentioned kind of this other career path, like, when you notice a problem.
[00:19:04.960 --> 00:19:14.200]   I think, you know, we have all -- I mean, all of us on this call here have basically have done that.
[00:19:14.200 --> 00:19:17.520]   And it makes a lot of sense.
[00:19:17.520 --> 00:19:22.920]   One interesting question that's a little bit unrelated is, what are some areas
[00:19:22.920 --> 00:19:25.600]   where you think there are a dearth of tools that --
[00:19:25.600 --> 00:19:33.400]   where there's a large opportunity for tooling that you see, if any?
[00:19:33.400 --> 00:19:36.720]   Jeremy, I think you're smiling.
[00:19:36.720 --> 00:19:41.000]   >> Yeah, I'm going to shamelessly plug potentially ByteWax here.
[00:19:41.000 --> 00:19:49.920]   I think data processing frameworks for Python are still sort of, you know,
[00:19:49.920 --> 00:19:52.280]   an unsolved area in my mind, right?
[00:19:52.280 --> 00:19:58.840]   So, I think, you know, I really liked the Spark and Dataflow programming model.
[00:19:58.840 --> 00:20:05.120]   I think that really works well when you're building a data processing workflow for, you know, shipping data.
[00:20:05.120 --> 00:20:10.080]   But I think, you know, it wasn't necessarily intended for Python.
[00:20:10.080 --> 00:20:11.720]   It was built off of the JVM.
[00:20:11.720 --> 00:20:17.560]   And I think when you start to run Python on top of that, it has some problems and, you know,
[00:20:17.560 --> 00:20:19.480]   certainly has some operational complexity.
[00:20:19.480 --> 00:20:27.280]   But I also think, like, you know, oftentimes when you're building data processing pipelines these days,
[00:20:27.280 --> 00:20:33.360]   you're calling out to external services or models and also doing long-running operations.
[00:20:33.360 --> 00:20:37.880]   And that's not what those frameworks were intended for.
[00:20:37.880 --> 00:20:43.080]   And you start to see that run into -- create operational problems for you.
[00:20:43.080 --> 00:20:49.280]   So, I think that to me is one area where I see some opportunity.
[00:20:49.280 --> 00:20:52.240]   >> Thank you.
[00:20:52.240 --> 00:21:03.000]   I think one other area where I see some opportunity is based on what we're working on at Biwax.
[00:21:03.000 --> 00:21:10.160]   So, right now, if you're using SQL to do any of your data transformations, it's super portable.
[00:21:10.160 --> 00:21:11.960]   And, you know, there's such a great decoupling.
[00:21:11.960 --> 00:21:17.600]   You can write ANSI-type, you know, based SQL, and you can run it virtually anywhere.
[00:21:17.600 --> 00:21:20.200]   And you don't have this sort of runtime environment problem.
[00:21:20.200 --> 00:21:28.680]   When you're running or you're executing code, you end up running into a lot more problems
[00:21:28.680 --> 00:21:30.600]   with managing your runtime environment.
[00:21:30.600 --> 00:21:36.440]   You have this sort of runtime problem, and it hasn't been solved that well.
[00:21:36.440 --> 00:21:39.720]   Of course, you can always say, yeah, like, put it in a Docker container and then run it
[00:21:39.720 --> 00:21:43.080]   on some container service or, yeah, just set up Kubernetes.
[00:21:43.080 --> 00:21:47.000]   But it's still a whole lot harder than SQL.
[00:21:47.000 --> 00:21:49.280]   And so, I think there's still opportunity there.
[00:21:49.280 --> 00:21:53.720]   And I don't know where WebAssembly will end up going to address this.
[00:21:53.720 --> 00:22:00.120]   If it does, I'm sure this will be something in the future that some of the bigger data vendors
[00:22:00.120 --> 00:22:06.480]   potentially allow you to run code closer to the data or something like that and kind of solve
[00:22:06.480 --> 00:22:11.640]   that runtime problem for us.
[00:22:11.640 --> 00:22:14.680]   Yeah, and it comes back to, like, the long-running services.
[00:22:14.680 --> 00:22:21.280]   We have so many great tools for, like, running batch things or running short services or
[00:22:21.280 --> 00:22:23.360]   web services.
[00:22:23.360 --> 00:22:26.440]   But, like, anything in between doesn't work out that well.
[00:22:26.440 --> 00:22:29.920]   So I think that's maybe an opportunity.
[00:22:29.920 --> 00:22:37.840]   Yeah, for me, you know, not a surprise.
[00:22:37.840 --> 00:22:38.840]   You know, it's all about the data.
[00:22:38.840 --> 00:22:43.960]   I think that most of us here and probably in the audience have, you know, at least heard
[00:22:43.960 --> 00:22:48.600]   of or follow along with, like, data-centric AI approach now, which is, like, for me is,
[00:22:48.600 --> 00:22:56.640]   like, so welcome because, you know, we have made so much progress on certain, like, mechanistic
[00:22:56.640 --> 00:22:59.840]   aspects of AI and ML.
[00:22:59.840 --> 00:23:04.320]   But to my mind, at the end of the day, like, we're barely scratching the surface in terms
[00:23:04.320 --> 00:23:10.200]   of the data that, you know, we can bring to our models and even, you know, the models
[00:23:10.200 --> 00:23:14.560]   we have of our models and, you know, how we think about whether it's probabilistic processes
[00:23:14.560 --> 00:23:16.080]   or other processes in the world.
[00:23:16.080 --> 00:23:22.880]   And so, you know, the thing that, like, really makes me smile in a panel like this is, you
[00:23:22.880 --> 00:23:25.640]   know, you always just feel like you're amongst friends and that the way that we experience
[00:23:25.640 --> 00:23:30.840]   the world is always similar to one another in that, like, we're frustrated by inefficiencies.
[00:23:30.840 --> 00:23:33.640]   We're frustrated by how hard certain things are.
[00:23:33.640 --> 00:23:40.120]   And for me, again, it's, like, how it's so hard to get particularly geospatial data,
[00:23:40.120 --> 00:23:42.680]   you know, not just into models, into products.
[00:23:42.680 --> 00:23:44.480]   And so, you know, I get really excited.
[00:23:44.480 --> 00:23:47.880]   And of course, that's what I'm building my company around is, like, how do we make that
[00:23:47.880 --> 00:23:49.680]   part so much easier, right?
[00:23:49.680 --> 00:23:55.000]   Like, there is so much data out there in the world, as we all know.
[00:23:55.000 --> 00:23:56.000]   It's scattered.
[00:23:56.000 --> 00:24:01.280]   I always give this example with geospatial, like, you know, imagine if the monthly jobs
[00:24:01.280 --> 00:24:06.440]   report was in a format that, like, a small segment of people could understand.
[00:24:06.440 --> 00:24:09.800]   Like, that is not the world that I want.
[00:24:09.800 --> 00:24:18.240]   And yet, that is the world that exists when, this is a bit of a niche, but, like, you know,
[00:24:18.240 --> 00:24:24.120]   when so much of what I'll call, like, geospatial data is in, you know, there's no common formats,
[00:24:24.120 --> 00:24:25.120]   right?
[00:24:25.120 --> 00:24:26.480]   There are proprietary formats.
[00:24:26.480 --> 00:24:33.000]   So, obviously, you know, plugging my own company here, Iggy, but I think there's so much potential,
[00:24:33.000 --> 00:24:38.040]   right, to help not just data scientists, as I mentioned, get more and better data into
[00:24:38.040 --> 00:24:42.200]   their models, but, you know, really anyone across the product spectrum.
[00:24:42.200 --> 00:24:50.120]   Yeah, you know, it would be amazing as a data scientist to kind of have the data acquisition
[00:24:50.120 --> 00:24:51.600]   to be made a lot easier.
[00:24:51.600 --> 00:24:58.920]   I'm really curious, is there any other types of data enrichment sort of opportunities out
[00:24:58.920 --> 00:25:03.080]   there beyond geospatial that you have come across, like, that might be interesting for
[00:25:03.080 --> 00:25:09.600]   people that are interested in this, you know, kind of data, you know, this kind of building
[00:25:09.600 --> 00:25:11.080]   more tools like Iggy?
[00:25:11.080 --> 00:25:13.880]   Oh, man, that's a great question.
[00:25:13.880 --> 00:25:16.280]   And I'm so focused on geospatial.
[00:25:16.280 --> 00:25:21.800]   It's like my whole mind is over, I mean, overtaken with geospatial.
[00:25:21.800 --> 00:25:25.600]   I mean, it is obviously a very general problem, though, right?
[00:25:25.600 --> 00:25:31.160]   I think that there's a very general problem in terms of access to data.
[00:25:31.160 --> 00:25:33.760]   I think, like, geospatial formats make it quite difficult.
[00:25:33.760 --> 00:25:37.160]   Like, you know, one of the challenges is joins, right?
[00:25:37.160 --> 00:25:45.760]   Like, joins are uniquely difficult in ways that with other types of data we've solved.
[00:25:45.760 --> 00:25:50.960]   So I'm curious to hear what other people, if other people have thoughts there.
[00:25:50.960 --> 00:25:55.200]   I mean, I think something that I have a question that I've been sort of asking myself lately
[00:25:55.200 --> 00:26:00.800]   is why isn't there sort of, you know, more structured data published on the web, right?
[00:26:00.800 --> 00:26:06.680]   So I think, like, if you look at the typical website, right, it's sort of published and
[00:26:06.680 --> 00:26:12.960]   formatted with the expectation of being read by humans, even though, you know, so much
[00:26:12.960 --> 00:26:17.400]   of the time we would rather consume that data in bulk by machines, right?
[00:26:17.400 --> 00:26:22.800]   So, like, you know, most companies have lists of their products, employees, et cetera, right?
[00:26:22.800 --> 00:26:27.400]   That would be very useful if that was published in a more machine-readable and structured
[00:26:27.400 --> 00:26:32.680]   format that made it sort of easy to, like, index and surface that information.
[00:26:32.680 --> 00:26:34.280]   But instead it's not.
[00:26:34.280 --> 00:26:38.520]   And, you know, instead the sort of trend right now is, like, well, we're going to throw some
[00:26:38.520 --> 00:26:44.080]   really large natural language understanding model at it, right?
[00:26:44.080 --> 00:26:47.680]   And it's interesting, I think, because, like, just from, like, a cost perspective, if you
[00:26:47.680 --> 00:26:52.120]   think about doing that at web scale, like, that's not cost-effective right now.
[00:26:52.120 --> 00:26:56.240]   So I think that's something that's interesting to me is, like, why isn't there more structured
[00:26:56.240 --> 00:27:02.040]   data on the web?
[00:27:02.040 --> 00:27:06.120]   I love that, especially given how much, you know, as you know, like, how much time we
[00:27:06.120 --> 00:27:09.640]   spend in creating structure out of unstructured data.
[00:27:09.640 --> 00:27:19.320]   Are there any models for that that we should, like, I'm trying to think about, like, yeah,
[00:27:19.320 --> 00:27:20.320]   what gives us hope?
[00:27:20.320 --> 00:27:25.960]   Or, you know, models, what should be done?
[00:27:25.960 --> 00:27:26.960]   For that particular problem?
[00:27:26.960 --> 00:27:32.800]   I mean, to me, I think it seems more like an incentive and an alignment problem, you
[00:27:32.800 --> 00:27:33.800]   know, because, right?
[00:27:33.800 --> 00:27:38.680]   Like, the technology has been around for, like, decades, like, the semantic web, right?
[00:27:38.680 --> 00:27:41.280]   We're just not using it.
[00:27:41.280 --> 00:27:46.240]   And, you know, and a lot of that information probably comes from structured databases,
[00:27:46.240 --> 00:27:47.240]   right?
[00:27:47.240 --> 00:27:53.640]   Like, a CSV table or a content management system somewhere, or just there's the incentives
[00:27:53.640 --> 00:27:55.080]   right now are, like, misaligned, right?
[00:27:55.080 --> 00:27:59.240]   Like, we're still sort of back in the days where we thought our web pages were only going
[00:27:59.240 --> 00:28:00.600]   to be read by humans, right?
[00:28:00.600 --> 00:28:03.840]   And so, you know, the biggest incentive right now is search ranking, right?
[00:28:03.840 --> 00:28:07.880]   So, it's like, if Google says, do X, and you'll be ranked higher, like, people will do it.
[00:28:07.880 --> 00:28:09.880]   But potentially other incentives.
[00:28:09.880 --> 00:28:17.120]   Yeah, it's wild that you say that, because, again, I'll talk less about Iggy, but I think
[00:28:17.120 --> 00:28:19.360]   about the problem of maps the same way, right?
[00:28:19.360 --> 00:28:24.040]   Like, we've incentivized, and this is, of course, related to Google and their dominance,
[00:28:24.040 --> 00:28:28.680]   but like, incentivize, like, getting data on maps and a visual experience of data that
[00:28:28.680 --> 00:28:30.400]   we, like, where is it coming from?
[00:28:30.400 --> 00:28:32.280]   It's always coming from a database, right?
[00:28:32.280 --> 00:28:33.280]   It's there somewhere.
[00:28:33.280 --> 00:28:37.040]   So, yeah, that's an interesting connection.
[00:28:37.040 --> 00:28:38.520]   Okay.
[00:28:38.520 --> 00:28:47.080]   Yeah, for the next question, I'm going to show you folks, like, four images, and then
[00:28:47.080 --> 00:28:49.160]   have you react to them.
[00:28:49.160 --> 00:28:50.160]   So let me --
[00:28:50.160 --> 00:28:52.440]   Is this a psychological experiment?
[00:28:52.440 --> 00:28:54.440]   Kind of, yes.
[00:28:54.440 --> 00:28:58.400]   Okay, so the first one is the one that you might be familiar with.
[00:28:58.400 --> 00:29:02.000]   This is from the hidden technical data machine learning systems.
[00:29:02.000 --> 00:29:07.920]   And basically, what it's trying to show is, in the middle, there's kind of ML code, which
[00:29:07.920 --> 00:29:10.960]   is what we talk about a lot as an industry.
[00:29:10.960 --> 00:29:19.000]   But then, you know, what in reality is, there's so much other things going on around ML, that
[00:29:19.000 --> 00:29:25.760]   kind of, you know, make the ML, the actual ML code feel really insignificant.
[00:29:25.760 --> 00:29:27.360]   And I think this resonates with most people.
[00:29:27.360 --> 00:29:33.520]   I think this diagram has kind of been accepted as sort of a shared understanding amongst
[00:29:33.520 --> 00:29:35.880]   data professionals.
[00:29:35.880 --> 00:29:37.600]   But then this is the next slide.
[00:29:37.600 --> 00:29:47.160]   This is where it starts to become slightly more, you know, overwhelming.
[00:29:47.160 --> 00:29:52.580]   And so, this is Matt Turk's, you know, sort of infographic of all the different tools
[00:29:52.580 --> 00:29:55.760]   and kind of categorize them in a lot of different space.
[00:29:55.760 --> 00:29:59.920]   And like, the first thing that anyone, I mean, at least for me, and anyone else I talk to
[00:29:59.920 --> 00:30:01.320]   when they see this, it's pretty overwhelming.
[00:30:01.320 --> 00:30:04.000]   You have to have like a microscope just to read anything.
[00:30:04.000 --> 00:30:11.800]   And it just hits you in the face as this magic, or this gigantic tidal wave of tools.
[00:30:11.800 --> 00:30:17.560]   And kind of this taxonomy of things that you may want to be aware of.
[00:30:17.560 --> 00:30:20.720]   Then this paper came out recently, has been making the rounds.
[00:30:20.720 --> 00:30:25.840]   This is kind of a end to end flowchart of the machine learning process.
[00:30:25.840 --> 00:30:32.200]   And also, I would say, so this is quite, you know, is quite detailed.
[00:30:32.200 --> 00:30:38.680]   And of course, you know, the debate about this chart has been kind of twofold.
[00:30:38.680 --> 00:30:43.500]   On one side, people have said, okay, yeah, there's a, you know, machine learning is complicated
[00:30:43.500 --> 00:30:46.080]   and requires a lot of different things.
[00:30:46.080 --> 00:30:50.240]   So you know, this flowchart is accurate.
[00:30:50.240 --> 00:30:55.120]   And then the other side of the debate is, oh, wow, like, are we making things a bit
[00:30:55.120 --> 00:31:04.120]   too complicated in terms of, is the discourse around what we, you know, how to set up ML,
[00:31:04.120 --> 00:31:07.440]   is it too complicated in general?
[00:31:07.440 --> 00:31:13.320]   Like, you know, are we showing people, do we tend to show people more complicated things?
[00:31:13.320 --> 00:31:16.120]   Or do we bias that way?
[00:31:16.120 --> 00:31:18.240]   And then this is another Venn diagram.
[00:31:18.240 --> 00:31:24.440]   So you know, there's been so many Venn diagrams, but this is the newest one.
[00:31:24.440 --> 00:31:28.080]   For me personally, I think this is kind of overwhelming.
[00:31:28.080 --> 00:31:33.240]   I mean, you might as well just put, I mean, I can simplify this diagram for myself and
[00:31:33.240 --> 00:31:38.480]   saying, well, there's just a circle of somebody that just knows everything in the world.
[00:31:38.480 --> 00:31:42.200]   That's the way I feel when I look at this.
[00:31:42.200 --> 00:31:49.080]   But you know, in some ways, you can argue to pull off data science or ML, successful
[00:31:49.080 --> 00:31:50.080]   ML projects.
[00:31:50.080 --> 00:31:51.080]   Yeah, it is a team effort.
[00:31:51.080 --> 00:31:58.440]   But on the same token, you know, the ways that this is being communicated in the industry
[00:31:58.440 --> 00:32:05.640]   right now is, hey, like, we want people to be skilled in all these areas.
[00:32:05.640 --> 00:32:11.080]   There is significant discourse around, hey, like, you should learn all of these things.
[00:32:11.080 --> 00:32:17.540]   So I want to stop there and kind of flip it back to you folks to kind of get your reactions
[00:32:17.540 --> 00:32:23.960]   on all of those things and your kind of how you feel about these various messages.
[00:32:23.960 --> 00:32:29.680]   The first thing that is clear is there were all those founders that started companies
[00:32:29.680 --> 00:32:35.440]   in the second slide clearly were having the problem in 2015.
[00:32:35.440 --> 00:32:42.240]   So over the next seven years, they are six, they all left and started companies.
[00:32:42.240 --> 00:32:44.960]   And then we ended up with that new graphic.
[00:32:44.960 --> 00:32:49.320]   I know it's super overwhelming.
[00:32:49.320 --> 00:32:57.120]   It's I mean, I don't spend my days as an IC doing in that space right now.
[00:32:57.120 --> 00:32:58.680]   It's just so overwhelming.
[00:32:58.680 --> 00:33:04.040]   I actually am thankful that I don't have to be like drinking out of a fire hose every
[00:33:04.040 --> 00:33:10.360]   other day or that's what it would feel like to me.
[00:33:10.360 --> 00:33:12.880]   Do you feel like we're over complicating things at all?
[00:33:12.880 --> 00:33:21.160]   Like do you feel like the discussion and the discourse around it tends to bias people towards
[00:33:21.160 --> 00:33:27.320]   talking about more complicated things to the point where we don't show how to do things
[00:33:27.320 --> 00:33:32.320]   in kind of a pragmatic way or what is your feeling in general?
[00:33:32.320 --> 00:33:35.480]   I think we maybe ended up here.
[00:33:35.480 --> 00:33:38.480]   And then I'll stop talking about others.
[00:33:38.480 --> 00:33:41.880]   Ended up here because of a bunch of reasons.
[00:33:41.880 --> 00:33:44.560]   There's no like single reason.
[00:33:44.560 --> 00:33:46.000]   It's a complex process.
[00:33:46.000 --> 00:33:49.720]   There were people who were involved who maybe weren't involved in other parts of software
[00:33:49.720 --> 00:33:50.800]   engineering.
[00:33:50.800 --> 00:33:55.000]   And so they didn't bring some of the tools that already existed.
[00:33:55.000 --> 00:33:59.400]   We were in a time where funding was relatively cheap.
[00:33:59.400 --> 00:34:00.920]   Money was cheap for a long time.
[00:34:00.920 --> 00:34:03.240]   It was easy to start a company.
[00:34:03.240 --> 00:34:06.800]   And there was a ton of hype around machine learning.
[00:34:06.800 --> 00:34:08.480]   We were at that right point in the hype cycle.
[00:34:08.480 --> 00:34:14.040]   And we are now still in the right moment of the hype cycle for data where there's just
[00:34:14.040 --> 00:34:16.880]   this perfect convergence of...
[00:34:16.880 --> 00:34:22.440]   And it almost feels like a natural thing where we'll have this explosion of tools.
[00:34:22.440 --> 00:34:29.040]   And then some will, because of what's happening in the world and just the points of the cycle
[00:34:29.040 --> 00:34:36.080]   we're at from economic to software development, et cetera, we'll have things die off and then
[00:34:36.080 --> 00:34:39.960]   aggregate or in some way, shape or form.
[00:34:39.960 --> 00:34:40.960]   So it's...
[00:34:40.960 --> 00:34:41.960]   Yeah.
[00:34:41.960 --> 00:34:42.960]   I don't know.
[00:34:42.960 --> 00:34:47.080]   There's no simple answer for how we ended up here today, I don't think.
[00:34:47.080 --> 00:34:52.520]   I actually see a disconnect between the two, the diagrams.
[00:34:52.520 --> 00:34:54.720]   So the fragmented landscape, right?
[00:34:54.720 --> 00:34:55.840]   Super overwhelming.
[00:34:55.840 --> 00:35:01.080]   Every time I try to look at that, I just give up.
[00:35:01.080 --> 00:35:05.200]   There's that, and that is true.
[00:35:05.200 --> 00:35:06.600]   Part of me is like...
[00:35:06.600 --> 00:35:09.840]   And this shouldn't be a surprise given one of my previous answers.
[00:35:09.840 --> 00:35:12.120]   Choice should be good, right?
[00:35:12.120 --> 00:35:15.000]   All of us should appreciate choice.
[00:35:15.000 --> 00:35:21.360]   And maybe it's a beautiful thing that there are so many options because it is...
[00:35:21.360 --> 00:35:25.760]   I find it easier to do these things today than it was five years ago.
[00:35:25.760 --> 00:35:26.760]   That is progress.
[00:35:26.760 --> 00:35:31.640]   Now, do I have to make more choices?
[00:35:31.640 --> 00:35:33.920]   Yes.
[00:35:33.920 --> 00:35:35.800]   But I think optionality is really good.
[00:35:35.800 --> 00:35:38.680]   So that's sort of the second diagram.
[00:35:38.680 --> 00:35:40.360]   And then I can't remember which one it was.
[00:35:40.360 --> 00:35:44.040]   Maybe the Venn diagram.
[00:35:44.040 --> 00:35:49.680]   The Venn diagram to me is frustrating because it's just a fantasy, right?
[00:35:49.680 --> 00:35:50.680]   And it's like...
[00:35:50.680 --> 00:35:55.080]   When I saw this, I'm thinking about our experiences at Airbnb.
[00:35:55.080 --> 00:35:59.000]   There's a massive hit by a bus factor in the middle of that circle, right?
[00:35:59.000 --> 00:36:03.320]   And everyone wants that person.
[00:36:03.320 --> 00:36:04.840]   I would never want to be that person.
[00:36:04.840 --> 00:36:08.320]   I mean, maybe you make a lot of money in the meantime.
[00:36:08.320 --> 00:36:12.160]   But everyone wants that person in the middle of the Venn diagram.
[00:36:12.160 --> 00:36:13.160]   But come on.
[00:36:13.160 --> 00:36:14.760]   Who are we kidding?
[00:36:14.760 --> 00:36:18.360]   And if we want that, then I actually think we're ignoring the previous diagram, which
[00:36:18.360 --> 00:36:22.480]   suggests that it's the tools that connect the different circles, right?
[00:36:22.480 --> 00:36:24.960]   It's not to say that that's not difficult.
[00:36:24.960 --> 00:36:27.480]   It's not to say that there aren't a lot of choices.
[00:36:27.480 --> 00:36:30.960]   But the Venn diagram, we went through it with data science.
[00:36:30.960 --> 00:36:32.880]   Of course, we're going through it with machine learning.
[00:36:32.880 --> 00:36:37.520]   It's more of a reflection of a fantasy of how you want a single person to be able to
[00:36:37.520 --> 00:36:39.360]   do all of these things.
[00:36:39.360 --> 00:36:40.960]   I think this is why we create tools.
[00:36:40.960 --> 00:36:44.760]   You don't have to have a single person creating all of these things.
[00:36:44.760 --> 00:36:51.120]   So I would love pushback on that, of course.
[00:36:51.120 --> 00:36:54.880]   Jeremy, any thoughts on this?
[00:36:54.880 --> 00:37:02.360]   I mean, I think some of these diagrams, maybe particularly like three and one, I think there's
[00:37:02.360 --> 00:37:04.560]   some important context here.
[00:37:04.560 --> 00:37:10.680]   And I think a lot of times, they speak to maturity of an organization where they are
[00:37:10.680 --> 00:37:14.160]   with their journey.
[00:37:14.160 --> 00:37:19.240]   I think a lot of times these diagrams come from organizations which are very mature in
[00:37:19.240 --> 00:37:20.800]   their use of AI.
[00:37:20.800 --> 00:37:26.120]   And they're showing where they got to after years of investment.
[00:37:26.120 --> 00:37:29.320]   And that's probably not where they started.
[00:37:29.320 --> 00:37:32.560]   I worked at YouTube on video recommendations.
[00:37:32.560 --> 00:37:34.640]   That system today is highly optimized.
[00:37:34.640 --> 00:37:41.240]   There's multiple teams building tools, infrastructure, optimizing everything from the logging pipeline
[00:37:41.240 --> 00:37:45.120]   to how models get built, making it easy to run experiments, everything.
[00:37:45.120 --> 00:37:48.920]   But that's not how it started.
[00:37:48.920 --> 00:37:58.720]   The common knowledge that people promulgated at Google was when you get started with AI,
[00:37:58.720 --> 00:38:00.360]   don't start with a complicated model.
[00:38:00.360 --> 00:38:07.520]   Try to find the simplest heuristic that you think will get you some benefit and build
[00:38:07.520 --> 00:38:10.440]   the end-to-end system around that and then iterate.
[00:38:10.440 --> 00:38:15.840]   So I think early iterations around video recommendations at YouTube were just based off of popular
[00:38:15.840 --> 00:38:17.440]   videos and things like that.
[00:38:17.440 --> 00:38:22.080]   And then it sort of went from there to, okay, how do we get more diversity?
[00:38:22.080 --> 00:38:24.160]   Then it's like, what metrics can we optimize for?
[00:38:24.160 --> 00:38:27.160]   And it's like, well, we can optimize for clicks because we have that data.
[00:38:27.160 --> 00:38:30.080]   And then we were like, well, that's not the right metric to optimize for.
[00:38:30.080 --> 00:38:31.440]   We should optimize for watch time.
[00:38:31.440 --> 00:38:33.560]   Well, how do we optimize for watch time?
[00:38:33.560 --> 00:38:39.880]   Well, that ended up being like a two-year investment in the logging pipeline and front-end
[00:38:39.880 --> 00:38:44.120]   code in order to collect those metrics and feed them into the training and then serving.
[00:38:44.120 --> 00:38:50.920]   So I think that's the context that I think people might be missing when they get this
[00:38:50.920 --> 00:38:51.920]   diagram.
[00:38:51.920 --> 00:38:56.120]   Like if you're just trying to optimize your ticket flow or something else to sort of reduce
[00:38:56.120 --> 00:39:05.400]   toil, building all this is probably not necessarily what to start with in day zero.
[00:39:05.400 --> 00:39:06.400]   That makes sense.
[00:39:06.400 --> 00:39:14.440]   I guess my question is, do you think that there's not as much discussion about, let's
[00:39:14.440 --> 00:39:19.080]   say, the how to do things when you're starting out?
[00:39:19.080 --> 00:39:23.480]   There's definitely a lot of things how to do things, like how to do stuff when you're
[00:39:23.480 --> 00:39:25.920]   like kind of at stage zero.
[00:39:25.920 --> 00:39:30.960]   Like how do you train a model in a notebook or how do you serve something with SageMaker?
[00:39:30.960 --> 00:39:39.480]   Like kind of this middle ground, do you feel like there's enough discourse around that
[00:39:39.480 --> 00:39:43.960]   or if that's well-represented at all?
[00:39:43.960 --> 00:39:50.440]   My initial reaction to that is, to me, that speaks more to a type of person than particular
[00:39:50.440 --> 00:39:53.600]   tools, skill sets, other things.
[00:39:53.600 --> 00:40:00.480]   So I've seen some people be really, really good at this and they're just capable of taking
[00:40:00.480 --> 00:40:07.800]   a problem and sort of understanding and grabbing all kinds of different things, models, infrastructure,
[00:40:07.800 --> 00:40:13.000]   building front ends, everything, in order to get it done and build that end-to-end prototype
[00:40:13.000 --> 00:40:19.600]   and also just kind of pushing through obstacles as they encounter them.
[00:40:19.600 --> 00:40:24.320]   So it's a very different type of mentality and skill set, I think, to sort of thinking
[00:40:24.320 --> 00:40:32.960]   through, like, okay, what's the right way to design this and how do I build for the
[00:40:32.960 --> 00:40:37.040]   long term versus sort of like hacking things together and really getting that intent system
[00:40:37.040 --> 00:40:38.040]   together.
[00:40:38.040 --> 00:40:41.440]   So, you know, personally, I'm pretty in awe of those folks because I don't think that's
[00:40:41.440 --> 00:40:48.840]   my skill set, but I also don't know what advice to get other than sort of like recognize those
[00:40:48.840 --> 00:40:51.280]   people and grab them when you see them.
[00:40:51.280 --> 00:40:52.280]   I see.
[00:40:52.280 --> 00:40:53.280]   Okay.
[00:40:53.280 --> 00:40:59.520]   So this kind of dovetails into the next question is, okay, so all of you are building tools
[00:40:59.520 --> 00:41:03.160]   right now of one kind or the other, of ML tools.
[00:41:03.160 --> 00:41:06.960]   So how do you build tools in this environment?
[00:41:06.960 --> 00:41:14.240]   Like, you know, it's overwhelming, you have lots of noise, it's crowded.
[00:41:14.240 --> 00:41:17.360]   How do you position your tools?
[00:41:17.360 --> 00:41:24.640]   Like, how do you think about this, you know, kind of in this, given what we just observed?
[00:41:24.640 --> 00:41:34.240]   I can take a stab at it, unless you guys, Lindsey, Zender, do you want to go?
[00:41:34.240 --> 00:41:40.080]   I mean, my general thinking was like, you know, I sort of, the first distinction I make
[00:41:40.080 --> 00:41:43.920]   is whether you're from strategically, you're sort of going broad or deep.
[00:41:43.920 --> 00:41:50.480]   So I think like, if you think about SageMaker, Cloud protects AI, and I'm not sure what Microsoft
[00:41:50.480 --> 00:41:58.200]   calls its AI platform, or, you know, trying to build like the entire intent workflow for
[00:41:58.200 --> 00:41:59.200]   AI.
[00:41:59.200 --> 00:42:01.320]   So it's like one stop shop, right?
[00:42:01.320 --> 00:42:07.120]   So I think that's increasingly a difficult proposition for a startup to do, right?
[00:42:07.120 --> 00:42:09.560]   Because it just requires so much investment.
[00:42:09.560 --> 00:42:16.040]   And, you know, the existing incumbent are like far ahead at this point, right?
[00:42:16.040 --> 00:42:22.280]   So the other approach, I think, is to go, you know, sort of, you know, deep in a particular
[00:42:22.280 --> 00:42:26.960]   area, I feel like this is how Weights and Biases started, you know, with the experiment
[00:42:26.960 --> 00:42:28.800]   tracking model monitoring piece, right?
[00:42:28.800 --> 00:42:34.200]   And sort of understand, where's there a gap in a need, and then trying to build the tool
[00:42:34.200 --> 00:42:37.160]   to cap that need.
[00:42:37.160 --> 00:42:44.800]   Yeah, I almost think that only those large clouds are capable of starting with a vision
[00:42:44.800 --> 00:42:51.200]   that is like an end to end vision, because they just, they have their sources, and they
[00:42:51.200 --> 00:42:53.880]   already have probably the audience.
[00:42:53.880 --> 00:43:00.680]   And the generic, you should ask me this in another few years, if by rec said there's
[00:43:00.680 --> 00:43:08.400]   any sort of fails, but the generic advice of like, like Jeremy was saying is to build
[00:43:08.400 --> 00:43:15.720]   to a need, like and also to find a niche or an area where you can really focus and serve
[00:43:15.720 --> 00:43:25.840]   the hair on fire customer is seems to be the like generic business advice that probably
[00:43:25.840 --> 00:43:28.440]   fills here too.
[00:43:28.440 --> 00:43:34.520]   Yeah, I feel like I have to go generic too, but it was so often overlooked.
[00:43:34.520 --> 00:43:41.560]   It's, of course, related to the hair on fire problem, but it's like, listening.
[00:43:41.560 --> 00:43:45.480]   Like, you know, I think all of us are makers, right?
[00:43:45.480 --> 00:43:46.480]   We're all builders.
[00:43:46.480 --> 00:43:50.960]   That's what binds us together in so many ways.
[00:43:50.960 --> 00:43:56.240]   And you know, we get really excited when we're when we're building things.
[00:43:56.240 --> 00:43:57.840]   And they work.
[00:43:57.840 --> 00:44:01.240]   And you know, that that is exciting.
[00:44:01.240 --> 00:44:03.320]   It's great.
[00:44:03.320 --> 00:44:11.680]   And that experience of like, getting something in front of a customer, getting feedback,
[00:44:11.680 --> 00:44:13.760]   and it can be positive feedback, it can be negative feedback.
[00:44:13.760 --> 00:44:20.680]   But like, I feel like, you know, often what you build is the excuse for like the conversation
[00:44:20.680 --> 00:44:23.800]   that you should be having regardless that conversation.
[00:44:23.800 --> 00:44:28.040]   It's like, you know, here's what we're really struggling with.
[00:44:28.040 --> 00:44:34.920]   And I just, again, it's like so generic, but like, that is so, so critical is like that
[00:44:34.920 --> 00:44:41.320]   experience of like, actually listening to folks, and it can lead you to paths that you
[00:44:41.320 --> 00:44:48.000]   didn't expect, but like, really getting into the, you know, the nitty gritty of like, what
[00:44:48.000 --> 00:44:50.440]   frustrates them, keeps them up at night.
[00:44:50.440 --> 00:44:56.200]   I mean, yeah, it's how I guess how it started on this panel was like problem orientation,
[00:44:56.200 --> 00:44:57.200]   right?
[00:44:57.200 --> 00:45:05.160]   But like, what, what really try to understand folks understand problems first, seems to
[00:45:05.160 --> 00:45:08.880]   be the way to go about it.
[00:45:08.880 --> 00:45:09.880]   That makes sense.
[00:45:09.880 --> 00:45:18.120]   And I guess just one thought one question came to my mind based on Lindsay's observation
[00:45:18.120 --> 00:45:22.880]   that a lot of there's a career path for a lot of people to build data tools.
[00:45:22.880 --> 00:45:27.640]   I mean, that resonates with me, I got frustrated, and I also started building data tools.
[00:45:27.640 --> 00:45:32.320]   And so, what are some lessons you've learned, like while building tools, for example, is
[00:45:32.320 --> 00:45:36.160]   there certain aspects you would have focused on more or less?
[00:45:36.160 --> 00:45:42.280]   Like, if you're like, maybe building the next tool, with like, with the benefit of some
[00:45:42.280 --> 00:45:47.880]   hindsight, like, for example, like, you know, would you focus on some aspect more, like
[00:45:47.880 --> 00:45:50.600]   marketing or community or documentation or something like that?
[00:45:50.600 --> 00:45:58.440]   You know, it'd be interesting to hear, like, from as tool builders, like, what kind of
[00:45:58.440 --> 00:46:05.560]   advice you have for people wanting to maybe build tools now?
[00:46:05.560 --> 00:46:10.160]   I think building an open core business is really hard.
[00:46:10.160 --> 00:46:18.240]   And so I think if you're and I think building non non open source tools is also very hard.
[00:46:18.240 --> 00:46:22.880]   So I think if you're, if you're thinking about building a tool, and you think about building
[00:46:22.880 --> 00:46:27.600]   an open source, I think there's there's an inherent advantage if you can do that at a
[00:46:27.600 --> 00:46:36.280]   company that has the business model that that that can support investments in open source,
[00:46:36.280 --> 00:46:40.480]   which are sort of limited to a couple really big companies, right?
[00:46:40.480 --> 00:46:50.120]   I think if you are, if you are building an open source tool or infrastructure, I think
[00:46:50.120 --> 00:46:53.400]   you kind of want to if you're building a standalone company, I think you want to start to start
[00:46:53.400 --> 00:46:58.040]   to think about first, you know, what's your business strategy, and then think about what
[00:46:58.040 --> 00:47:08.400]   does being open, accelerate that strategy, and making some decisions to serve that business
[00:47:08.400 --> 00:47:09.400]   strategy.
[00:47:09.400 --> 00:47:15.080]   I think otherwise, that can be a genie that's hard to put back in the bottle later on.
[00:47:15.080 --> 00:47:16.080]   So,
[00:47:16.080 --> 00:47:20.960]   Genie, putting back in the bottle in terms of, hey, like, you've already made it open
[00:47:20.960 --> 00:47:25.680]   source, maybe that wasn't the best strategy, but you can't really reverse that.
[00:47:25.680 --> 00:47:31.800]   I mean, that I mean, that but also like other other decisions, even if you're our open core,
[00:47:31.800 --> 00:47:35.680]   like I think a big one, if you're thinking about open sources, what type of open source
[00:47:35.680 --> 00:47:37.320]   project are you going to build?
[00:47:37.320 --> 00:47:45.120]   Like, are you going to be sort of a community led open source project where you know, there's
[00:47:45.120 --> 00:47:47.320]   lots of stakeholders and decision makers?
[00:47:47.320 --> 00:47:53.160]   Or is it going to be open source, but you know, it's going to be sort of tightly controlled
[00:47:53.160 --> 00:47:55.800]   and by a single company, right?
[00:47:55.800 --> 00:48:01.600]   And I think if you if you make sort of an implicit commitment to be a community led
[00:48:01.600 --> 00:48:05.760]   project, and then you want to reverse that later on, because it doesn't align with the
[00:48:05.760 --> 00:48:12.080]   business strategy, that's going to be very costly in terms of credibility, and mind share.
[00:48:12.080 --> 00:48:17.480]   So I think that's, that's the kind of, you know, things you want to think through and
[00:48:17.480 --> 00:48:27.720]   get right first off, because I think reversing course later on can really be difficult.
[00:48:27.720 --> 00:48:30.240]   And like, just to drill into that more, this is very interesting.
[00:48:30.240 --> 00:48:35.240]   I mean, like, what are some considerations that you have, like when deciding that?
[00:48:35.240 --> 00:48:39.920]   Because I feel like everybody kind of thinks about that, at least when they're building
[00:48:39.920 --> 00:48:43.640]   tools, like, hey, should this be open source, maybe it shouldn't be open source, just given
[00:48:43.640 --> 00:48:48.120]   the prevalence of open source, right now, I feel like that's something that weighs on
[00:48:48.120 --> 00:48:51.960]   people.
[00:48:51.960 --> 00:48:58.960]   I can just speak to what we how we thought about it is, open source is a go to market
[00:48:58.960 --> 00:49:00.600]   strategy.
[00:49:00.600 --> 00:49:03.960]   And so it's essentially the top of our funnel.
[00:49:03.960 --> 00:49:09.160]   What we were, what we have been trying to do is make it as easy as possible for someone
[00:49:09.160 --> 00:49:10.680]   to get started.
[00:49:10.680 --> 00:49:14.400]   So you can, you know, pip install, by wax and get going.
[00:49:14.400 --> 00:49:19.000]   And what is what are the various ways we can do that could be a hosted thing, or it could
[00:49:19.000 --> 00:49:21.120]   be open source.
[00:49:21.120 --> 00:49:25.080]   And then if we look at some of the requirements of the users, oftentimes, there's data privacy
[00:49:25.080 --> 00:49:26.360]   and security concerns.
[00:49:26.360 --> 00:49:29.180]   So maybe the hosted thing is actually more of a barrier.
[00:49:29.180 --> 00:49:30.440]   So it's not as easy to use.
[00:49:30.440 --> 00:49:36.800]   So it made sense from that perspective, ignoring some of the other complicating things that
[00:49:36.800 --> 00:49:41.880]   Jeremy was mentioning, to go open source so that we can have broad adoption of the tool
[00:49:41.880 --> 00:49:50.120]   and we can use, you know, leverage, leverage that in a way for people to get onto the product
[00:49:50.120 --> 00:49:56.520]   easily.
[00:49:56.520 --> 00:50:02.040]   And what about like, okay, open source is one thing, what about other types of dimensions
[00:50:02.040 --> 00:50:03.880]   when it comes to building data tools?
[00:50:03.880 --> 00:50:08.480]   Like, you know, there's so many other factors, I guess, to consider if you can, you know,
[00:50:08.480 --> 00:50:12.400]   other than open source or not open source, like, what is anything else that kind of is
[00:50:12.400 --> 00:50:20.520]   top of mind that sort of, you know, mistakes that you have made or things that you wish
[00:50:20.520 --> 00:50:23.360]   focused on more that, you know, could be interesting.
[00:50:23.360 --> 00:50:29.200]   I mean, I think one thing here, so you had mentioned, like marketing potentially, and
[00:50:29.200 --> 00:50:37.960]   I mean, I guess to take a step back, it's like, we talked about tools and like starting
[00:50:37.960 --> 00:50:41.160]   companies, you don't always have to start a company, lots of eventual companies come
[00:50:41.160 --> 00:50:44.120]   out of, you know, side projects and other things.
[00:50:44.120 --> 00:50:50.440]   But I think if you are going, you know, the startup route, you know, I'm a former data
[00:50:50.440 --> 00:50:52.600]   scientist, not a marketer.
[00:50:52.600 --> 00:50:56.600]   And I think for many of us, like marketing doesn't come easily.
[00:50:56.600 --> 00:50:57.760]   What is, what does it even mean?
[00:50:57.760 --> 00:50:58.760]   Right?
[00:50:58.760 --> 00:51:02.880]   And we sort of struggle with what it means, which means it's hard to do.
[00:51:02.880 --> 00:51:06.840]   And if you're in a startup, you know, you're scrappy and founders are, you know, you're
[00:51:06.840 --> 00:51:11.000]   often you have founder led sales and founders are the marketers in the beginning as well.
[00:51:11.000 --> 00:51:16.640]   And I think one thing that I struggled with early on was like, I was very uncomfortable
[00:51:16.640 --> 00:51:17.640]   with marketing.
[00:51:17.640 --> 00:51:22.680]   Now I also, you know, raised a pre-seed round, like really early pre-product.
[00:51:22.680 --> 00:51:23.680]   And so it's like, what am I marketing?
[00:51:23.680 --> 00:51:24.680]   Right?
[00:51:24.680 --> 00:51:25.680]   Like, I don't have a product.
[00:51:25.680 --> 00:51:28.680]   You have a product, but you're kind of testing different products out.
[00:51:28.680 --> 00:51:34.080]   And like, I kind of felt this pressure to like, talk about the product and that's not
[00:51:34.080 --> 00:51:35.760]   what you should be doing at all.
[00:51:35.760 --> 00:51:39.560]   What I've come to understand, you talk about the problem again, like I didn't plan these
[00:51:39.560 --> 00:51:46.760]   lines through my answers here, but like, I wish I had gotten that sooner that, that it's
[00:51:46.760 --> 00:51:49.440]   the problem that compels people.
[00:51:49.440 --> 00:51:54.480]   It's the problem that, that, you know, gets, gets you on.
[00:51:54.480 --> 00:51:55.480]   And there's like a whole theory.
[00:51:55.480 --> 00:51:56.800]   And of course I read this book.
[00:51:56.800 --> 00:51:59.680]   It just didn't like sit anywhere.
[00:51:59.680 --> 00:52:03.900]   It's like the theory is too serious of a word, but there's this great book I love called
[00:52:03.900 --> 00:52:04.900]   Play Bigger.
[00:52:04.900 --> 00:52:11.120]   And it's about like category defining companies and you know, how, how I lived in DC for many
[00:52:11.120 --> 00:52:16.720]   years and wasn't really taking cabs, but like, you know, Uber came along and told me how
[00:52:16.720 --> 00:52:19.640]   bad the experience of taking a cab was right.
[00:52:19.640 --> 00:52:25.240]   Like they weren't like, well, they were like, get in our black cars, but, but what they
[00:52:25.240 --> 00:52:28.600]   led with was this experience is miserable.
[00:52:28.600 --> 00:52:29.600]   Right.
[00:52:29.600 --> 00:52:35.760]   And so, yeah, I would definitely, I don't know if this is about giving advice or reflecting,
[00:52:35.760 --> 00:52:41.320]   but like really just centering communication with the outside world, which is exactly what
[00:52:41.320 --> 00:52:44.720]   marketing is like on the problem that you're trying to solve.
[00:52:44.720 --> 00:52:46.520]   That's, that's when it gets exciting.
[00:52:46.520 --> 00:52:48.240]   That's when people start engaging, right?
[00:52:48.240 --> 00:52:50.280]   And that's when they actually want to talk to you as a vendor.
[00:52:50.280 --> 00:52:54.320]   Like, it's very weird to me that there are people who like don't want to talk because
[00:52:54.320 --> 00:52:57.840]   I'm a vendor, which I obviously do not identify as such.
[00:52:57.840 --> 00:53:03.640]   But when you're building a company that that's like the world you operate in, but you know,
[00:53:03.640 --> 00:53:08.080]   you do problem centric marketing, I guess.
[00:53:08.080 --> 00:53:10.360]   And I guess you're more than a vendor.
[00:53:10.360 --> 00:53:11.360]   We'll see.
[00:53:11.360 --> 00:53:12.360]   - Makes sense.
[00:53:13.360 --> 00:53:14.360]   Okay.
[00:53:14.360 --> 00:53:18.720]   My last question before we might get onto, let's say audience questions, we might have
[00:53:18.720 --> 00:53:25.640]   time for one is, are there any problems that you keep seeing as you work with customers
[00:53:25.640 --> 00:53:31.040]   and organizations that you think are common that cannot be solved with tools that you
[00:53:31.040 --> 00:53:32.040]   keep running into?
[00:53:32.040 --> 00:53:37.120]   I mean, we keep, I mean, you know, there's kind of a bias I think in some sense in tech
[00:53:37.120 --> 00:53:41.040]   to just try to solve everything with tools as much as possible.
[00:53:41.040 --> 00:53:49.840]   But just curious, you know, is there anything that you've observed out there that people
[00:53:49.840 --> 00:53:58.600]   should watch out for sort of kind of patterns that you've seen in terms of this?
[00:53:58.600 --> 00:54:10.920]   - Wow, this is a can of worms, but one thing I'll share, you know, we're living in an interesting
[00:54:10.920 --> 00:54:18.400]   time and place and a lot of people see data as the answer.
[00:54:18.400 --> 00:54:26.240]   Like that data is, and in some ways, like I'm, that's the fix that I'm selling.
[00:54:26.240 --> 00:54:35.200]   But like what I want to say here, it's like tooling to my mind cannot solve creativity
[00:54:35.200 --> 00:54:40.240]   or like deep thinking about a problem, about a process, about your customers.
[00:54:40.240 --> 00:54:48.320]   Like we do a lot of really, you know, close work with customers who will say to us, like,
[00:54:48.320 --> 00:54:50.240]   well just give us this data.
[00:54:50.240 --> 00:54:56.000]   And it's like, I can give you that, but like, how are you thinking about that?
[00:54:56.000 --> 00:54:57.800]   How are your customers thinking about this problem?
[00:54:57.800 --> 00:54:59.000]   How are you thinking about it?
[00:54:59.000 --> 00:55:00.960]   Like what is your mental model?
[00:55:00.960 --> 00:55:04.440]   And again, maybe it's because some people see us as a data vendor.
[00:55:04.440 --> 00:55:07.920]   They just like sometimes expect that we can like hand something over and it's going to
[00:55:07.920 --> 00:55:11.400]   answer their question.
[00:55:11.400 --> 00:55:13.960]   So I don't know, I'm like trying to avoid specifics.
[00:55:13.960 --> 00:55:20.800]   So maybe I'm being a little too vague, but like tooling cannot solve like a thinking
[00:55:20.800 --> 00:55:22.680]   problem.
[00:55:22.680 --> 00:55:29.440]   And that's like a very big statement, but like, I think that the scarcest skill we have
[00:55:29.440 --> 00:55:36.120]   has to do with like creativity, problem solving, models, not in terms of the models that we
[00:55:36.120 --> 00:55:39.840]   build, but models we have of the world, because that's where those models come from.
[00:55:39.840 --> 00:55:43.720]   So yeah, I don't really think tooling can solve that.
[00:55:43.720 --> 00:55:51.880]   And like, how do you find people who can work with data, but who are thinking really deeply
[00:55:51.880 --> 00:55:58.920]   and creatively about these processes we're all trying to understand and solve?
[00:55:58.920 --> 00:56:01.680]   Yeah, I'll stop there.
[00:56:01.680 --> 00:56:02.680]   It's a lot.
[00:56:02.680 --> 00:56:03.680]   No, I like that a lot.
[00:56:03.680 --> 00:56:08.680]   I mean, yeah, that resonates with me in my experience as well, that people wanting some
[00:56:08.680 --> 00:56:14.280]   kind of data or specific solution, not thinking about the problem.
[00:56:14.280 --> 00:56:15.280]   Anyone else?
[00:56:15.280 --> 00:56:16.280]   Jeremy, Xander?
[00:56:16.280 --> 00:56:17.280]   I'm sure.
[00:56:17.280 --> 00:56:22.960]   I mean, I think if you want to be effective and put ML to work, like I think the key is
[00:56:22.960 --> 00:56:25.800]   ML needs to sort of be for lack of a better term embodied, right?
[00:56:25.800 --> 00:56:28.300]   Like it has to be integrated into some system.
[00:56:28.300 --> 00:56:30.880]   So I think a good example is like GitHub Copilot, right?
[00:56:30.880 --> 00:56:35.680]   Like that model didn't really become useful and get traction until it got integrated into
[00:56:35.680 --> 00:56:38.320]   people's IDEs, right?
[00:56:38.320 --> 00:56:45.400]   And so to me, what I often see is like you have a highly complex system requiring specialized
[00:56:45.400 --> 00:56:47.520]   skill to like build the models.
[00:56:47.520 --> 00:56:54.680]   Then you also have a highly specialized complex system to actually integrate the model into
[00:56:54.680 --> 00:56:58.840]   actual products where those predictions are made useful.
[00:56:58.840 --> 00:57:03.600]   So to me, in order to be effective, those two groups of folks need to be collaborating
[00:57:03.600 --> 00:57:05.120]   and working closely in a line.
[00:57:05.120 --> 00:57:08.840]   And so to me, that's like an organizational challenge.
[00:57:08.840 --> 00:57:12.560]   And so I think that's a problem that you can't solve with tools, right?
[00:57:12.560 --> 00:57:16.600]   You have to have an organizational structure that can foster that type of collaboration
[00:57:16.600 --> 00:57:18.600]   in order to be effective with ML.
[00:57:18.600 --> 00:57:22.120]   Xander, you want to say anything?
[00:57:22.120 --> 00:57:29.680]   You don't have to if you don't want to, but I want to give you the chance.
[00:57:29.680 --> 00:57:32.400]   No, I think they both made really good points.
[00:57:32.400 --> 00:57:35.760]   Like it's about organizational problems.
[00:57:35.760 --> 00:57:39.840]   And I just, all I keep thinking about is this metaphor when Lindsay was speaking of like,
[00:57:39.840 --> 00:57:42.440]   you can have the best tools in the world.
[00:57:42.440 --> 00:57:45.400]   And if you don't know how to build a house, you're not going to be able to build a house.
[00:57:45.400 --> 00:57:47.560]   Or you can have the best skills in the world.
[00:57:47.560 --> 00:57:50.320]   But if you don't have the materials or the tools, you're not going to be able to build
[00:57:50.320 --> 00:57:51.320]   a house.
[00:57:51.320 --> 00:57:52.320]   And so you need to have all these things.
[00:57:52.320 --> 00:57:57.000]   But I don't know, I was just stuck in my head and I couldn't stop myself.
[00:57:57.000 --> 00:57:58.760]   No, it's totally fine.
[00:57:58.760 --> 00:58:02.280]   I think we should get to some of the questions.
[00:58:02.280 --> 00:58:03.280]   Yeah, okay.
[00:58:03.280 --> 00:58:04.760]   There's a question from Saurav.
[00:58:04.760 --> 00:58:09.960]   He's asking, can you suggest a roadmap to get started with operationalizing ML?
[00:58:09.960 --> 00:58:12.000]   I actually get this question a lot.
[00:58:12.000 --> 00:58:14.080]   I say, hey, is there a roadmap?
[00:58:14.080 --> 00:58:20.480]   And I never have a great solution to that because it's hard to have a roadmap.
[00:58:20.480 --> 00:58:26.400]   But I mean, you already touched on this a little bit, but does somebody want to, some
[00:58:26.400 --> 00:58:28.040]   thoughts on that?
[00:58:28.040 --> 00:58:37.800]   I mean, I think that it really depends on your system or product, right?
[00:58:37.800 --> 00:58:41.200]   Like I think going back to what Lindsey started off with, right, it's what problem are you
[00:58:41.200 --> 00:58:42.320]   trying to solve, right?
[00:58:42.320 --> 00:58:47.720]   So are you trying to improve video recommendations, routing of support tickets, et cetera?
[00:58:47.720 --> 00:58:56.680]   And so I think if you haven't gotten ML, some sort of model or heuristic integrated and
[00:58:56.680 --> 00:59:02.600]   affecting your system, like your roadmap is, what's the path to get that done for your
[00:59:02.600 --> 00:59:03.600]   organization, right?
[00:59:04.400 --> 00:59:11.040]   That's probably going to be many ways be specific to your infrastructure and product.
[00:59:11.040 --> 00:59:13.400]   Like, you know, do I serve my model?
[00:59:13.400 --> 00:59:17.520]   Do I like integrate my model into my code?
[00:59:17.520 --> 00:59:19.600]   How do I, do I pre-compute predictions?
[00:59:19.600 --> 00:59:22.840]   Do I have a certain infrastructure?
[00:59:22.840 --> 00:59:28.840]   But I think that the goal is to like have some evidence that you can impact the business
[00:59:28.840 --> 00:59:31.600]   or metrics that you care about.
[00:59:31.600 --> 00:59:32.600]   Makes sense.
[00:59:32.600 --> 00:59:33.600]   Yeah.
[00:59:33.600 --> 00:59:34.600]   Okay.
[00:59:34.600 --> 00:59:35.600]   It looks like we're at time actually.
[00:59:35.600 --> 00:59:39.080]   So it was fun to hang out with everybody.
[00:59:39.080 --> 00:59:40.080]   Yeah.
[00:59:40.080 --> 00:59:43.080]   I think it was really interesting.
[00:59:43.080 --> 00:59:45.640]   May have surfaced more questions than we answered.
[00:59:45.640 --> 00:59:47.320]   That's probably okay.
[00:59:47.320 --> 00:59:53.840]   I recommend everybody in the audience to check out BiteWax and Iggy and also Kubeflow and
[00:59:53.840 --> 00:59:57.560]   basically stalk Jeremy, whatever he's creating next to see what that is.
[00:59:57.560 --> 01:00:01.760]   But yeah, I'll leave it at that.
[01:00:01.760 --> 01:00:03.200]   And yeah, thank you.
[01:00:03.200 --> 01:00:05.780]   (upbeat music)
[01:00:05.780 --> 01:00:08.360]   (upbeat music)
[01:00:08.360 --> 01:00:10.940]   (upbeat music)
[01:00:10.940 --> 01:00:13.520]   (upbeat music)
[01:00:13.520 --> 01:00:16.080]   (upbeat music)

