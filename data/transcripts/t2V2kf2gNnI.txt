
[00:00:00.000 --> 00:00:09.340]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:09.340 --> 00:00:11.000]   models work in the real world.
[00:00:11.000 --> 00:00:13.040]   I'm your host, Lukas Biewald.
[00:00:13.040 --> 00:00:18.800]   Jeremy Howard created the Fast.ai course, which is maybe the most popular course to
[00:00:18.800 --> 00:00:21.920]   learn machine learning, and there are a lot out there.
[00:00:21.920 --> 00:00:26.240]   He's also the author of the book, Deep Learning for Coders with Fast.ai and PyTorch.
[00:00:26.240 --> 00:00:31.860]   And in that process, he made the Fast.ai library, which lots of people use independently to
[00:00:31.860 --> 00:00:34.280]   write deep learning code.
[00:00:34.280 --> 00:00:39.340]   Before that, he was the CEO and co-founder of Nlytic, an exciting startup that applies
[00:00:39.340 --> 00:00:41.940]   deep learning to healthcare applications.
[00:00:41.940 --> 00:00:46.800]   And before that, he was the president of Kaggle, one of the most exciting earliest machine
[00:00:46.800 --> 00:00:47.840]   learning companies.
[00:00:47.840 --> 00:00:49.360]   I'm super excited to talk to him.
[00:00:49.360 --> 00:00:50.880]   So Jeremy, it's nice to talk to you.
[00:00:50.880 --> 00:00:56.000]   And in preparing the questions, I kind of realized that every time I've talked to you,
[00:00:56.000 --> 00:00:59.120]   there've been kind of a few gems that I've remembered that I would never think to ask
[00:00:59.120 --> 00:01:00.120]   about.
[00:01:00.120 --> 00:01:03.640]   Like one time you told me about how you learned Chinese, and another time you gave me dad
[00:01:03.640 --> 00:01:08.720]   parenting advice, like very specific advice that's been actually super helpful.
[00:01:08.720 --> 00:01:09.720]   So it was kind of funny.
[00:01:09.720 --> 00:01:10.720]   Oh, great.
[00:01:10.720 --> 00:01:13.800]   Hey, tell me what dad parenting advice worked out.
[00:01:13.800 --> 00:01:19.480]   Well, what you told me was when you change diapers, use a blow dryer to change a really
[00:01:19.480 --> 00:01:22.600]   frustrating experience into a really joyful experience.
[00:01:22.600 --> 00:01:24.040]   And it's like such good advice.
[00:01:24.040 --> 00:01:27.560]   I don't know how you, I guess I can imagine how you thought of it, but it's...
[00:01:27.560 --> 00:01:29.640]   Yeah, they love the whooshing sound.
[00:01:29.640 --> 00:01:30.640]   They love the warmth.
[00:01:30.640 --> 00:01:32.440]   I'm kind of obsessed about dad things.
[00:01:32.440 --> 00:01:34.640]   So I'm always happy to talk about dad things.
[00:01:34.640 --> 00:01:35.640]   That is this podcast.
[00:01:35.640 --> 00:01:36.640]   Can we start with that?
[00:01:36.640 --> 00:01:39.720]   Now that my daughter's eight months old, do you have any suggestions for her?
[00:01:39.720 --> 00:01:42.080]   Oh my goodness, eight months old.
[00:01:42.080 --> 00:01:44.560]   You know, it's like the same with any kind of learning.
[00:01:44.560 --> 00:01:46.000]   It's all about consistency.
[00:01:46.000 --> 00:01:50.560]   So I think that the main thing we did right with Claire, who's just this delightful child
[00:01:50.560 --> 00:01:52.640]   now, is we were just super consistent.
[00:01:52.640 --> 00:01:58.840]   Like if we said like, you can't have X unless you do Y, we would never give her X if you
[00:01:58.840 --> 00:01:59.840]   didn't do Y.
[00:01:59.840 --> 00:02:03.240]   And if we're like, if you want to take your scooter down to the bottom of the road, you
[00:02:03.240 --> 00:02:05.000]   have to carry it back up again.
[00:02:05.000 --> 00:02:09.880]   We read this great book that was saying like, if you're not consistent, it becomes like
[00:02:09.880 --> 00:02:12.160]   this thing, like it's like a gambler.
[00:02:12.160 --> 00:02:16.640]   It's like sometimes you get the thing you want, so you just have to keep trying.
[00:02:16.640 --> 00:02:19.200]   So that's my number one piece of advice.
[00:02:19.200 --> 00:02:21.360]   It's the same with like teaching machine learning.
[00:02:21.360 --> 00:02:27.120]   We always tell people that tenacity is the most important thing for students, like to
[00:02:27.120 --> 00:02:28.520]   stick with it, do it every day.
[00:02:28.520 --> 00:02:32.920]   I guess just in the spirit of questions, I'm genuinely curious about, you know, you've
[00:02:32.920 --> 00:02:38.360]   built this kind of amazing framework and sort of teaching thing that I think is maybe the
[00:02:38.360 --> 00:02:40.840]   most popular and most appreciated framework.
[00:02:40.840 --> 00:02:44.600]   I was wondering if you could start by telling me the story of what inspired you to do that
[00:02:44.600 --> 00:02:50.280]   and what was the kind of journey to making fast AI the curriculum and fast AI the ML
[00:02:50.280 --> 00:02:51.280]   framework?
[00:02:51.280 --> 00:02:58.360]   So it was something that my wife Rachel and I started together.
[00:02:58.360 --> 00:03:06.520]   And so Rachel has a math PhD, super technical background, early data scientist and engineer
[00:03:06.520 --> 00:03:07.520]   at Uber.
[00:03:07.520 --> 00:03:13.320]   I don't, you know, I have a just scraped by a philosophy undergrad and have no technical
[00:03:13.320 --> 00:03:14.320]   background.
[00:03:14.320 --> 00:03:18.240]   But, you know, from both of our different directions, we both had this frustration that
[00:03:18.240 --> 00:03:26.880]   like neural networks in 2012, super important, clearly going to change the world, but super
[00:03:26.880 --> 00:03:27.880]   inaccessible.
[00:03:27.880 --> 00:03:33.120]   And, you know, so we would go to meetups and try to figure out like how do we, like I knew
[00:03:33.120 --> 00:03:37.960]   the basic idea, I'd coded neural networks 20 years ago, but like how do you make them
[00:03:37.960 --> 00:03:38.960]   really good?
[00:03:38.960 --> 00:03:45.360]   There wasn't any kind of open source software at the time for running on GPUs, you know,
[00:03:45.360 --> 00:03:49.160]   Dan Sirison and JÃ¼rgen Schmidhuder's thing was available, but you had to pay for it.
[00:03:49.160 --> 00:03:51.280]   There was no source code.
[00:03:51.280 --> 00:03:58.060]   And we just thought, oh, we've got to change this because the history of technology leaps
[00:03:58.060 --> 00:04:05.100]   has been that it generally increases inequality because the people with resources can access
[00:04:05.100 --> 00:04:06.480]   the new technology.
[00:04:06.480 --> 00:04:11.440]   And then that leads to kind of societal upheaval and a lot of unhappiness.
[00:04:11.440 --> 00:04:13.360]   So we thought, well, we should just do what we can.
[00:04:13.360 --> 00:04:17.880]   So we thought, how are we going to fix this?
[00:04:17.880 --> 00:04:23.840]   And so basically the goal was and still is be able to use deep learning without requiring
[00:04:23.840 --> 00:04:25.180]   any code.
[00:04:25.180 --> 00:04:29.360]   So that, you know, because the vast majority of the world can't code.
[00:04:29.360 --> 00:04:32.800]   We kind of thought, well, to get there, we should first of all see like, well, what exists
[00:04:32.800 --> 00:04:38.040]   right now, learn how to use it as best as we can ourselves, teach people how to best
[00:04:38.040 --> 00:04:43.600]   use it as we can, and then make it better, which requires doing research and then turning
[00:04:43.600 --> 00:04:48.520]   that into software and then changing the course to teach the hopefully slightly easier version
[00:04:48.520 --> 00:04:53.400]   and repeat that again and again for a few years.
[00:04:53.400 --> 00:04:56.360]   And so that's, we're kind of in that process.
[00:04:56.360 --> 00:04:58.320]   That's so interesting.
[00:04:58.320 --> 00:05:03.680]   Do you worry that the stuff you're teaching, you're sort of trying to make it obsolete,
[00:05:03.680 --> 00:05:04.680]   right?
[00:05:04.680 --> 00:05:06.480]   Because you're trying to build higher level abstractions.
[00:05:06.480 --> 00:05:09.360]   Like I think one of the things that people really appreciate your course is the sort
[00:05:09.360 --> 00:05:13.400]   of really clear in-depth explanations of how these things work.
[00:05:13.400 --> 00:05:17.040]   Do you think that that's eventually going to be not necessary or how do you think about
[00:05:17.040 --> 00:05:18.040]   that?
[00:05:18.040 --> 00:05:19.040]   Yeah.
[00:05:19.040 --> 00:05:20.040]   To some extent.
[00:05:20.040 --> 00:05:28.440]   I mean, so if you look at the new book and the new course, the chapter one starts with
[00:05:28.440 --> 00:05:34.480]   like really, really foundational stuff around like, what is a machine learning algorithm?
[00:05:34.480 --> 00:05:36.080]   What do we mean to learn an algorithm?
[00:05:36.080 --> 00:05:40.240]   What's the difference between traditional programming and machine learning to solve
[00:05:40.240 --> 00:05:42.920]   the same problem?
[00:05:42.920 --> 00:05:49.280]   And those kinds of basic foundations I think will always be useful, even at the point you're
[00:05:49.280 --> 00:05:50.280]   not using any code.
[00:05:50.280 --> 00:05:56.640]   I feel like even right now, if somebody's using like platform AI or some kind of code
[00:05:56.640 --> 00:06:02.320]   free framework, you still need to understand these basics of like, okay, an algorithm can
[00:06:02.320 --> 00:06:04.880]   only learn based on the data you provide.
[00:06:04.880 --> 00:06:09.680]   You know, it's generally not going to be able to extrapolate to patterns it's not seeing
[00:06:09.680 --> 00:06:12.880]   yet, stuff like that.
[00:06:12.880 --> 00:06:19.080]   But yeah, I mean, we have so far released two new courses every year, you know, a part
[00:06:19.080 --> 00:06:23.240]   one and a part two every year, because every year it's totally out of date.
[00:06:23.240 --> 00:06:29.040]   We always say to our students at the start of part one, look, you know, none of the details
[00:06:29.040 --> 00:06:32.960]   you're learning are going to be of any use in a year or two's time.
[00:06:32.960 --> 00:06:38.400]   There's a good, you know, when we're doing Theano and then TensorFlow and Keras, you
[00:06:38.400 --> 00:06:42.800]   know, and then plain PyTorch, we always say, look, don't worry too much about the software
[00:06:42.800 --> 00:06:50.640]   we're using because none of it's still any good, you know, it's all changing rapidly,
[00:06:50.640 --> 00:06:56.560]   you know, faster than JavaScript frameworks, but the concepts are important.
[00:06:56.560 --> 00:07:03.960]   And yeah, you can pick up a new library in, I don't know, a week, I guess.
[00:07:03.960 --> 00:07:10.160]   >> Do you, it seems like you've thought pretty deeply about learning both, you know, human
[00:07:10.160 --> 00:07:12.160]   learning and machine learning.
[00:07:12.160 --> 00:07:16.760]   Had you or Rachel had practice teaching before?
[00:07:16.760 --> 00:07:20.000]   Was this kind of your first teaching experience?
[00:07:20.000 --> 00:07:24.160]   >> You know, I've actually had a lot of practice teaching of this kind, but in this really
[00:07:24.160 --> 00:07:32.420]   informal way, partly it's because I don't have a technical educational background myself.
[00:07:32.420 --> 00:07:36.040]   So I found it very easy to empathize with people who don't know what's going on, because
[00:07:36.040 --> 00:07:38.280]   I don't know what's going on.
[00:07:38.280 --> 00:07:44.000]   And so way back when I was doing management consulting, you know, 25 years ago, I was
[00:07:44.000 --> 00:07:49.980]   always using data-driven approaches rather than expertise and interview-driven approaches
[00:07:49.980 --> 00:07:54.720]   to solve problems, because I didn't have any expertise, and I couldn't really interview
[00:07:54.720 --> 00:07:58.440]   people because nobody took me seriously, because I was too young.
[00:07:58.440 --> 00:08:04.520]   And so then I would like have to explain to my client and to the engagement manager, like,
[00:08:04.520 --> 00:08:09.440]   well, I solved this problem using this thing called linear programming or multiple regression
[00:08:09.440 --> 00:08:13.000]   or a database or whatever.
[00:08:13.000 --> 00:08:17.400]   And yeah, what I found was I very, I wouldn't say very quickly, but within a couple of years
[00:08:17.400 --> 00:08:23.200]   in consulting, I started finding myself like running training programs for what we would
[00:08:23.200 --> 00:08:28.640]   today call data science, but 20 something years before we were using that word.
[00:08:28.640 --> 00:08:34.320]   Yeah, basically teaching our client and, you know, so when I was at AT Kearney, I ran a
[00:08:34.320 --> 00:08:42.360]   course for the whole company, basically, that every associate MBA had to do in what we would
[00:08:42.360 --> 00:08:47.360]   today call data science, you know, a bit of SQL, a bit of progression, a bit of spreadsheets,
[00:08:47.360 --> 00:08:49.920]   a bit of Monte Carlo.
[00:08:49.920 --> 00:08:55.720]   So yeah, I've actually done quite a lot of that now you mention it, and certainly Rachel
[00:08:55.720 --> 00:09:03.980]   also, but for her on pure math, you know, so she ran some courses at Duke University
[00:09:03.980 --> 00:09:06.360]   and stuff for postgrads.
[00:09:06.360 --> 00:09:10.960]   So yeah, I guess we both had some practice and we were pretty passionate about it.
[00:09:10.960 --> 00:09:20.200]   Yeah, we also study the literature of how to teach a lot, which most teachers, weirdly
[00:09:20.200 --> 00:09:21.200]   enough, don't.
[00:09:21.200 --> 00:09:22.200]   So, so that's good.
[00:09:22.200 --> 00:09:29.240]   Do you, do you feel like there are things that you feel like uniquely proud of in your
[00:09:29.240 --> 00:09:34.760]   teaching or like things that you're doing particularly well compared to, you know, other
[00:09:34.760 --> 00:09:36.240]   classes that people might take?
[00:09:36.520 --> 00:09:41.760]   Yeah, I mean, I wouldn't say unique, because there's always other people doing good stuff,
[00:09:41.760 --> 00:09:42.760]   you know.
[00:09:42.760 --> 00:09:47.280]   I think we're notable for two things in particular.
[00:09:47.280 --> 00:09:51.400]   One is code first, and the other is top-down.
[00:09:51.400 --> 00:09:57.760]   So you know, I make a very conscious decision in kind of everything I do to focus on myself
[00:09:57.760 --> 00:09:59.280]   as the audience.
[00:09:59.280 --> 00:10:05.240]   I'm not a good mathematician, you know, I'm like, I'm, I'm, I'm capable nowadays, but
[00:10:05.240 --> 00:10:07.800]   it's not something that's really in my, in my background.
[00:10:07.800 --> 00:10:09.360]   It doesn't come naturally to me.
[00:10:09.360 --> 00:10:15.480]   For me, the best explanation of a technical thing is like an example in some code that
[00:10:15.480 --> 00:10:21.320]   I can run, debug, look at the intermediate inputs and outputs.
[00:10:21.320 --> 00:10:26.560]   So, so I make a conscious decision in my teaching to, to teach to people who are like me.
[00:10:26.560 --> 00:10:35.440]   And although most people at kind of graduate level in technical degrees are not like me,
[00:10:35.440 --> 00:10:39.940]   they've all done a lot of math, most people that are interested in this material are like
[00:10:39.940 --> 00:10:40.940]   me.
[00:10:40.940 --> 00:10:45.480]   They're people who don't have graduate degrees, and they're really underrepresented in the
[00:10:45.480 --> 00:10:50.560]   teaching group, because like nearly all teachers are academics, and so they can't empathize
[00:10:50.560 --> 00:10:58.000]   with people who don't love Greek letters, you know, and integrals and stuff.
[00:10:58.000 --> 00:11:04.940]   So yeah, so, so it's, so I always explain things by showing code examples.
[00:11:04.940 --> 00:11:10.980]   So and then the other is top-down, which is, again, the vast majority of humans, not necessarily
[00:11:10.980 --> 00:11:16.560]   the vast majority of people who have spent a long time in technical degrees and made
[00:11:16.560 --> 00:11:21.120]   it all the way to being professors, but most regular people learn much better when they
[00:11:21.120 --> 00:11:22.980]   have context.
[00:11:22.980 --> 00:11:24.640]   Why are you learning this?
[00:11:24.640 --> 00:11:26.680]   What's an example of it being applied?
[00:11:26.680 --> 00:11:30.600]   You know, what are some of the pros and cons of using this approach before you start talking
[00:11:30.600 --> 00:11:34.120]   about, you know, the details of how it's put together.
[00:11:34.120 --> 00:11:40.400]   So in we, this is really hard to do, but we try to make it so that every time we introduce
[00:11:40.400 --> 00:11:46.840]   a topic, it's because we kind of need to show it in order to explain something else, or
[00:11:46.840 --> 00:11:48.840]   in order to improve something else.
[00:11:48.840 --> 00:11:55.680]   And this is so hard, because obviously everything I'm teaching is stuff that I know really well.
[00:11:55.680 --> 00:11:59.720]   And so it's really easy for me to just say like, okay, you start here, and you build
[00:11:59.720 --> 00:12:02.360]   on this, and you build on this, and you build on this, and here you are.
[00:12:02.360 --> 00:12:06.840]   And that's, that's just the natural way to try to teach something, but it's not the natural
[00:12:06.840 --> 00:12:08.160]   way to learn it.
[00:12:08.160 --> 00:12:15.360]   So I, I don't think people realize how difficult top-down teaching is, but people seem to really
[00:12:15.360 --> 00:12:16.360]   appreciate it.
[00:12:16.360 --> 00:12:18.960]   Yeah, they do seem to really appreciate it.
[00:12:18.960 --> 00:12:23.120]   Do you think, I mean, I'd love to talk to Rachel about this directly, but do you think
[00:12:23.120 --> 00:12:25.120]   Rachel has the same approach as you?
[00:12:25.120 --> 00:12:27.560]   Because it sounds like she has a pretty different background.
[00:12:27.560 --> 00:12:31.400]   Yeah, she does have a different background.
[00:12:31.400 --> 00:12:35.560]   But she certainly has the same approach, because we've talked about it, and she, we both kind
[00:12:35.560 --> 00:12:41.120]   of, kind of jump on each other to say like, hey, you know, because we kind of do a lot
[00:12:41.120 --> 00:12:46.880]   of development together, or we did before she got onto the data ethics stuff more.
[00:12:46.880 --> 00:12:50.360]   And sometimes, you know, I'll say to her like, hey, that seems pretty bottom-up, don't you
[00:12:50.360 --> 00:12:51.360]   think?
[00:12:51.360 --> 00:12:53.480]   And she'll be like, oh, yeah, it is.
[00:12:53.480 --> 00:12:55.880]   Damn it, start again, you know.
[00:12:55.880 --> 00:13:00.720]   So we both know it's important, and we both try really hard to do it, but we don't always
[00:13:00.720 --> 00:13:01.720]   succeed.
[00:13:01.880 --> 00:13:06.360]   And can you tell me about the library that you built, like how that came about?
[00:13:06.360 --> 00:13:10.280]   Do you think it was necessary to do it to teach the way you wanted to?
[00:13:10.280 --> 00:13:13.560]   Well, it's not, it's, remember, the purpose of this is not teaching.
[00:13:13.560 --> 00:13:15.800]   So we want there to be no teaching.
[00:13:15.800 --> 00:13:21.520]   So the goal is that there, or minimal teaching, the goal is that there should be no code,
[00:13:21.520 --> 00:13:24.520]   and it should be something you can pick up in half an hour and get going.
[00:13:24.520 --> 00:13:34.200]   So the fact that we have to teach what ends up being about 140 hours of work is a failure,
[00:13:34.200 --> 00:13:36.680]   you know, we're still failing.
[00:13:36.680 --> 00:13:43.000]   And so the only way to fix that is to create software, which makes everything dramatically
[00:13:43.000 --> 00:13:45.320]   easier.
[00:13:45.320 --> 00:13:52.200]   So really, the software is, that's actually the thing, that's actually our goal.
[00:13:52.200 --> 00:13:58.400]   But we can't get there until, you know, we first of all teach people to use what already
[00:13:58.400 --> 00:14:04.040]   exists and to do the research to figure out, like, well, why is it still hard?
[00:14:04.040 --> 00:14:05.040]   Why is it still too slow?
[00:14:05.040 --> 00:14:07.360]   Why does it still take too much compute?
[00:14:07.360 --> 00:14:09.440]   Why does it still take too much data?
[00:14:09.440 --> 00:14:12.920]   Like what are all the things that limit accessibility?
[00:14:12.920 --> 00:14:15.800]   Do the research to try and improve each of those things a little bit.
[00:14:15.800 --> 00:14:18.880]   Okay, how can we kind of embed that into software?
[00:14:19.480 --> 00:14:22.800]   Yeah, the software is kind of the end result of this.
[00:14:22.800 --> 00:14:28.760]   I mean, it's still a loop, but eventually, hopefully, it'll all be in the software.
[00:14:28.760 --> 00:14:34.680]   And I guess we've gotten to a point now where we feel like we understood some of the key
[00:14:34.680 --> 00:14:39.000]   missing things in deep learning libraries, at least.
[00:14:39.000 --> 00:14:43.920]   We're still a long way away from being no code, but we at least saw things like, oh,
[00:14:43.920 --> 00:14:50.880]   you know, basic object-oriented design is largely impossible because tensors don't have
[00:14:50.880 --> 00:14:52.360]   any kind of semantic types.
[00:14:52.360 --> 00:14:55.320]   So let's add that and see where it takes us.
[00:14:55.320 --> 00:14:57.720]   And, you know, kind of stuff like that.
[00:14:57.720 --> 00:14:59.680]   We really tried to get back to the foundations.
[00:14:59.680 --> 00:15:00.920]   Were there any other ones?
[00:15:00.920 --> 00:15:01.920]   That was a good one.
[00:15:01.920 --> 00:15:04.920]   Any other that come to mind?
[00:15:04.920 --> 00:15:05.920]   Yeah.
[00:15:05.920 --> 00:15:09.920]   I mean, you know, I mean, dispatch is a key one.
[00:15:09.920 --> 00:15:17.480]   So the fact that the kind of Julia-style dispatch is not built into Python, so function dispatch
[00:15:17.480 --> 00:15:23.520]   on typed arguments, we kind of felt like we had to fix that because really in the data
[00:15:23.520 --> 00:15:30.020]   science, the kind of data you have impacts what has to happen.
[00:15:30.020 --> 00:15:39.000]   And so if you say rotate, then depending on whether it's a 3D CT scan or an image or a
[00:15:39.000 --> 00:15:46.920]   point cloud or a set of key points for a human pose, rotate semantically means the same thing,
[00:15:46.920 --> 00:15:50.360]   but requires different implementations.
[00:15:50.360 --> 00:15:56.880]   So yeah, we built this kind of Julia-inspired type dispatch system.
[00:15:56.880 --> 00:16:02.680]   Also like realizing that to go with, again, it's really all about types, I guess.
[00:16:02.680 --> 00:16:08.800]   When you have semantic types, they need to go all the way in and out, by which I mean
[00:16:08.800 --> 00:16:15.040]   you put an image in, it's a pillow, you know, image object.
[00:16:15.040 --> 00:16:20.560]   It needs to come all the way out the other side as, you know, an image tensor, go into
[00:16:20.560 --> 00:16:29.680]   your model, the model then needs to produce an image, you know, an image tensor or a category,
[00:16:29.680 --> 00:16:32.880]   you know, type or whatever, and then that needs to come out all the way the other side
[00:16:32.880 --> 00:16:36.700]   to be able to be displayed on your screen correctly.
[00:16:36.700 --> 00:16:41.320]   So we had to make sure that the entire transformation pipeline was reversible, so we had to set
[00:16:41.320 --> 00:16:50.400]   up a new system of reversible composable transforms.
[00:16:50.400 --> 00:16:56.160]   So this stuff is all like, as much as possible, we try to hide it behind the scenes, but without
[00:16:56.160 --> 00:17:03.960]   these things, our eventual goal of no code would be impossible, because, you know, you
[00:17:03.960 --> 00:17:08.840]   would have to tell the computer like, oh this tensor that's come out actually represents,
[00:17:08.840 --> 00:17:15.720]   you know, three bounding boxes along with associated categories, you know, and describe
[00:17:15.720 --> 00:17:18.040]   how to display it and stuff.
[00:17:18.040 --> 00:17:25.200]   So it's all pretty foundational to both making the process of coding easy, and then down
[00:17:25.200 --> 00:17:29.940]   the track over the next couple of years, you know, removing the need for the code entirely.
[00:17:29.940 --> 00:17:36.440]   And what did you, like, what was the big goal behind releasing a v2 of the library?
[00:17:36.440 --> 00:17:41.680]   That was kind of a bold choice, right, to just make a complete rewrite.
[00:17:41.680 --> 00:17:49.360]   Yeah, I'm, you know, I'm a big fan of second system, you know, the kind of the opposite
[00:17:49.360 --> 00:17:53.680]   of Joel Spolsky, you know, I love rewriting.
[00:17:53.680 --> 00:17:57.880]   I'm more, I mean, I'm no Arthur Whitney, but you know, Arthur Whitney who created K and
[00:17:57.880 --> 00:18:05.280]   KDB, every version he rewrites the entire thing from scratch, and he's done many versions
[00:18:05.280 --> 00:18:06.280]   now.
[00:18:06.280 --> 00:18:12.560]   But that's, that's, I really like that as a general approach, which is like, if I haven't
[00:18:12.560 --> 00:18:19.280]   learned so much that my previous version seems like ridiculously naive and, and pathetic,
[00:18:19.280 --> 00:18:24.760]   then I'm, I'm not moving forwards, you know, so I do find every year I look back at any
[00:18:24.760 --> 00:18:28.240]   code I've got and think like, oh, that could be so much better.
[00:18:28.240 --> 00:18:29.640]   And then you rewrite it from scratch.
[00:18:29.640 --> 00:18:35.280]   I did the same thing with the book, you know, I rewrote every chapter from scratch a second
[00:18:35.280 --> 00:18:38.440]   time.
[00:18:38.440 --> 00:18:42.720]   So it's partly that, and it's partly also just that it took a few years to get to a
[00:18:42.720 --> 00:18:50.240]   point where I felt like I, I actually had some solid understanding of what was needed,
[00:18:50.240 --> 00:18:54.060]   you know, the kind of things I just described.
[00:18:54.060 --> 00:18:59.080]   And some of, a lot of it came from like a lot of conversations with Chris Latner, the,
[00:18:59.080 --> 00:19:02.280]   the inventor of Swift and LLVM.
[00:19:02.280 --> 00:19:09.120]   So when we taught together, it was great sitting with him and talking about like porting fast
[00:19:09.120 --> 00:19:17.360]   AI to Swift and like the type system in Swift, and then working with Alexis Gallagher, who's
[00:19:17.360 --> 00:19:21.880]   like maybe the world's foremost expert on the, on Swift's value type system.
[00:19:21.880 --> 00:19:27.540]   And he helped us build a new data block API for Swift.
[00:19:27.540 --> 00:19:31.840]   And so kind of through that process as well, it made me realize like, yeah, you know, this
[00:19:31.840 --> 00:19:39.040]   is, this is actually a real lasting idea.
[00:19:39.040 --> 00:19:45.080]   And actually I should mention that it goes back to the very idea of the data block API,
[00:19:45.080 --> 00:19:51.440]   which actually goes back to fast AI version 1, which is this idea that, and again, it's
[00:19:51.440 --> 00:19:56.720]   kind of based on really thinking carefully about the foundations, which is like rather
[00:19:56.720 --> 00:20:02.920]   than have a library, which every possible combination of inputs and outputs ends up
[00:20:02.920 --> 00:20:09.160]   being this totally different class, you know, with a different API and different ideas,
[00:20:09.160 --> 00:20:14.880]   let's have some types that represent, that could be either an input or an output, and
[00:20:14.880 --> 00:20:16.880]   then let's figure out the actual steps you need.
[00:20:16.880 --> 00:20:21.200]   It's like, okay, you've, you know, how do you figure out what the input items are?
[00:20:21.200 --> 00:20:22.920]   How do you figure out what the output items are?
[00:20:22.920 --> 00:20:25.640]   How do you figure out how to spit out the validation set?
[00:20:25.640 --> 00:20:29.200]   How do you figure out how to get the labels?
[00:20:29.200 --> 00:20:34.160]   So again, these things are just like, yeah, we, you know, came to them by stepping back
[00:20:34.160 --> 00:20:39.280]   and saying, what is actually foundationally, what's going on here?
[00:20:39.280 --> 00:20:42.480]   And let's do it properly, you know.
[00:20:42.480 --> 00:20:50.680]   So fast AI 2 is really our first time where we just stepped back and, you know, literally
[00:20:50.680 --> 00:20:55.000]   I said, you know, so Sylvain and I worked on it, and I said to Sylvain, like, we're
[00:20:55.000 --> 00:21:01.360]   not gonna push out any piece of this until it's the absolute best we can make it, you
[00:21:01.360 --> 00:21:03.840]   know, right now.
[00:21:03.840 --> 00:21:07.480]   Which I know Sylvain kind of got a bit, you know, thought I was a bit crazy sometimes,
[00:21:07.480 --> 00:21:15.360]   like the transforms API, transforms API, I think I went through like 27 rewrites.
[00:21:15.360 --> 00:21:17.560]   But you know, I kept thinking like, no, this is not good enough.
[00:21:17.560 --> 00:21:20.040]   No, this is not good enough, you know.
[00:21:20.040 --> 00:21:23.360]   Until eventually it's like, okay, this is actually good now.
[00:21:23.360 --> 00:21:27.440]   >> So is the hardest part the external APIs then?
[00:21:27.440 --> 00:21:32.240]   Because that does seem like it'd be really tricky to make that, I mean, that seems like
[00:21:32.240 --> 00:21:36.800]   an endless task to make these APIs like clear enough and organized.
[00:21:36.800 --> 00:21:40.200]   >> Well, I never think of them as external APIs.
[00:21:40.200 --> 00:21:42.040]   To me, they're always internal APIs.
[00:21:42.040 --> 00:21:45.080]   >> Because you want to make a bigger system.
[00:21:45.080 --> 00:21:48.040]   >> Yeah, what am I building the rest of the software with?
[00:21:48.040 --> 00:21:49.040]   Exactly.
[00:21:49.040 --> 00:21:53.480]   So we went all the way back to like thinking like, well, how do we even write software?
[00:21:53.480 --> 00:21:57.480]   You know, I'm a huge fan, I've always been a huge fan of the idea of literate programming,
[00:21:57.480 --> 00:21:59.240]   but never found anything that made it work.
[00:21:59.240 --> 00:22:05.680]   And you know, we've been big proponents of Jupyter notebook forever.
[00:22:05.680 --> 00:22:10.040]   And it was always upsetting to me that I had this like Jupyter world that I loved being
[00:22:10.040 --> 00:22:20.440]   in, and this like IDE world, which I didn't have the same ability to explore in a documented,
[00:22:20.440 --> 00:22:28.240]   reproducible way and incorporate that exploration and explanation into the code as I wrote.
[00:22:28.240 --> 00:22:31.520]   So yeah, we went all the way back and said like, oh, I wonder if there's a way to actually
[00:22:31.520 --> 00:22:38.960]   use Jupyter notebooks to create an integrated system of documentation and code and tests
[00:22:38.960 --> 00:22:41.880]   and exploration.
[00:22:41.880 --> 00:22:45.680]   And it turns out the answer was yes.
[00:22:45.680 --> 00:22:52.200]   So yeah, it's really like just going right back at every point that I kind of felt like
[00:22:52.200 --> 00:22:55.240]   I'm less than entirely happy with the way I'm doing something right now.
[00:22:55.240 --> 00:22:57.560]   It's like to say, okay, can we fix that?
[00:22:57.560 --> 00:22:59.600]   Can we make it better?
[00:22:59.600 --> 00:23:01.480]   And Python really helped there, right?
[00:23:01.480 --> 00:23:07.520]   Because Python is so hackable, you know, the whole, the fact that you can actually go into
[00:23:07.520 --> 00:23:12.640]   the meta object system and change how type dispatch works and change how inheritance
[00:23:12.640 --> 00:23:14.040]   works.
[00:23:14.040 --> 00:23:20.000]   So like our type dispatch system has its own inheritance implementation built into it.
[00:23:20.000 --> 00:23:22.080]   It's yeah, it's amazing you can do that.
[00:23:22.080 --> 00:23:24.320]   Wow, why?
[00:23:24.320 --> 00:23:31.760]   Because the type dispatch system needs to understand inheritance when it comes to how
[00:23:31.760 --> 00:23:39.000]   do I decide if you call a function on a and b that, you know, on types a and b, and there's
[00:23:39.000 --> 00:23:47.200]   something registered for that function, which has some superclass of a and some higher superclass
[00:23:47.200 --> 00:23:49.680]   of b and something else with a slightly different combination.
[00:23:49.680 --> 00:23:54.520]   How do you decide which one matches, you know?
[00:23:54.520 --> 00:24:00.160]   So in the first version of it, I ignored inheritance entirely, and it would only dispatch if you
[00:24:00.160 --> 00:24:05.280]   had the types exactly matched or one of the types was none.
[00:24:05.280 --> 00:24:07.600]   But then later on I added, yeah, I added inheritance.
[00:24:07.600 --> 00:24:13.520]   So now you can, you've got this nice combination of multiple dispatch and inheritance, which
[00:24:13.520 --> 00:24:16.640]   is really convenient.
[00:24:16.640 --> 00:24:19.680]   Can you give me some examples of how the inheritance works with your types?
[00:24:19.680 --> 00:24:23.400]   Because I would think it could get kind of tricky, like what's even inheriting from what
[00:24:23.400 --> 00:24:26.720]   and the types that just quickly come to mind for me.
[00:24:26.720 --> 00:24:31.240]   Like if you have an image with multiple bounding boxes, would that inherit from like just a
[00:24:31.240 --> 00:24:32.240]   raw image?
[00:24:32.240 --> 00:24:35.360]   Yeah, so generally those kind of things will compose, you know.
[00:24:35.360 --> 00:24:40.520]   So we, I don't think we ever use multiple inheritance.
[00:24:40.520 --> 00:24:45.960]   I try to stay away from it because I've always found it a bit hairy.
[00:24:45.960 --> 00:24:49.160]   So instead things tend to be a lot more functional.
[00:24:49.160 --> 00:24:57.120]   So you know, a black and white image inherits from image, and I think a DICOM image, which
[00:24:57.120 --> 00:24:59.960]   is a medical image, also inherits from image.
[00:24:59.960 --> 00:25:05.000]   And then there are transforms with the type signatures which will take an image, and then
[00:25:05.000 --> 00:25:07.600]   there will be others which will take a DICOM image.
[00:25:07.600 --> 00:25:14.400]   And so if you call something with a DICOM image for which there isn't a registered function
[00:25:14.400 --> 00:25:17.320]   that takes a DICOM image, but there is one that takes an image, it'll call the image
[00:25:17.320 --> 00:25:18.920]   one.
[00:25:18.920 --> 00:25:24.880]   And so, and then we kind of use a row there in ways where, you know, there'll be a kind
[00:25:24.880 --> 00:25:29.000]   of, we use a lot of duck typing.
[00:25:29.000 --> 00:25:33.560]   So there'll be like a, you know, call dot method, and dot method can be implemented
[00:25:33.560 --> 00:25:38.560]   differently in the various image subclasses.
[00:25:38.560 --> 00:25:41.920]   And some, the other thing you can do with our type dispatch system is you can use a
[00:25:41.920 --> 00:25:47.460]   tuple of types, which means that that function argument can be any of those types.
[00:25:47.460 --> 00:25:52.400]   So you can kind of create union types on the fly, which is pretty convenient too.
[00:25:52.400 --> 00:25:56.080]   Are there parts of, in the V2 that you're still not happy with?
[00:25:56.080 --> 00:25:59.080]   Are we really able to realize that vision of?
[00:25:59.080 --> 00:26:01.600]   There are still some parts, yeah.
[00:26:01.600 --> 00:26:04.920]   Partly that happened kind of because of COVID.
[00:26:04.920 --> 00:26:15.040]   And you know, I unfortunately found myself the kind of face of the global masks movement,
[00:26:15.040 --> 00:26:18.980]   which didn't leave much room for more interesting things like deep learning.
[00:26:18.980 --> 00:26:25.180]   So some of the things that we kind of added in towards the end, like some of the stuff
[00:26:25.180 --> 00:26:34.060]   around inference is still a little, possibly a little clunky.
[00:26:34.060 --> 00:26:37.460]   But you know, it's only, it's only, it's only some little pieces, like on, I mean, on the
[00:26:37.460 --> 00:26:39.260]   whole inference is, is pretty good.
[00:26:39.260 --> 00:26:43.220]   But for example, I didn't really look at at all at, you know, how things would work with
[00:26:43.220 --> 00:26:45.700]   O and an X, for example.
[00:26:45.700 --> 00:26:55.220]   So kind of mobile or highly scalable serving.
[00:26:55.220 --> 00:26:59.380]   Also the, the training loop needs to be a little bit more flexible to handle things
[00:26:59.380 --> 00:27:04.760]   like the hugging face transformers API makes different assumptions that don't quite fit
[00:27:04.760 --> 00:27:06.540]   our assumptions.
[00:27:06.540 --> 00:27:14.060]   TPU training, because of the way it like runs on this separate machine that you don't have
[00:27:14.060 --> 00:27:19.860]   access to, you kind of have to find ways to do things that have, that accept really high
[00:27:19.860 --> 00:27:20.860]   latency.
[00:27:20.860 --> 00:27:28.820]   And so like for TPU, we kind of, it's particularly important because we've built a whole new
[00:27:28.820 --> 00:27:34.740]   computer vision library that runs on the GPU or runs in PyTorch, you know, which generally
[00:27:34.740 --> 00:27:41.660]   is targeting the GPU and PyTorch has a pretty good GPU launch latency along with a good
[00:27:41.660 --> 00:27:43.260]   NVIDIA driver.
[00:27:43.260 --> 00:27:50.780]   So we can do a lot of like stuff on the GPU around transformations and stuff.
[00:27:50.780 --> 00:27:57.380]   That all breaks down with TPU because like every time you do another thing on the TPU,
[00:27:57.380 --> 00:28:01.380]   you have to go through that whole nasty latency.
[00:28:01.380 --> 00:28:05.100]   So yeah, there's a few little things like that that need to be improved.
[00:28:05.100 --> 00:28:11.540]   Is it important to you that your library is used widely outside of a learning context?
[00:28:11.540 --> 00:28:16.380]   Like is it, is one of your goals to make it kind of widespread in production systems?
[00:28:16.380 --> 00:28:17.380]   Yeah, yeah, yeah.
[00:28:17.380 --> 00:28:20.900]   I mean, cause, cause the learning context hopefully goes away eventually.
[00:28:20.900 --> 00:28:24.700]   Hopefully there will be no fast AI course and it'll just be software.
[00:28:24.700 --> 00:28:30.260]   So if people are only using our software in a learning context, it won't be used at all.
[00:28:30.260 --> 00:28:32.660]   Yeah, we want it used everywhere or something like it.
[00:28:32.660 --> 00:28:35.740]   I mean, I don't care whether it's fast AI or if somebody else comes along and creates
[00:28:35.740 --> 00:28:36.740]   something better.
[00:28:36.740 --> 00:28:40.940]   We just want to make sure that deep learning is accessible.
[00:28:40.940 --> 00:28:42.220]   That's super important.
[00:28:42.220 --> 00:28:49.500]   And the funny thing is because deep learning is so new and it kind of appeared so quickly,
[00:28:49.500 --> 00:28:54.740]   a lot of the decision makers, even commercially, are people that are highly academic.
[00:28:54.740 --> 00:29:02.100]   The whole kind of academic ecosystem is really important, much more so than in any other
[00:29:02.100 --> 00:29:05.260]   field I've ever been in.
[00:29:05.260 --> 00:29:09.940]   So one of the things we need to make too is make sure that researchers are using fast
[00:29:09.940 --> 00:29:10.940]   AI.
[00:29:10.940 --> 00:29:12.380]   So we try, you know, and we're researchers too.
[00:29:12.380 --> 00:29:18.340]   So we try to make it very researcher friendly and that's one of the key focuses really at
[00:29:18.340 --> 00:29:19.340]   the moment.
[00:29:19.340 --> 00:29:24.720]   Does that, so I mean, I would think just naively like making something research friendly wouldn't
[00:29:24.720 --> 00:29:30.860]   involve kind of the opposite of making it like a single clean API, like, or like, you
[00:29:30.860 --> 00:29:32.020]   know, abstracting away all the details.
[00:29:32.020 --> 00:29:37.020]   Like I would think researchers would want to really tinker with the low level assumptions.
[00:29:37.020 --> 00:29:38.020]   Yeah.
[00:29:38.020 --> 00:29:42.460]   Well, that's why you need a layered API.
[00:29:42.460 --> 00:29:48.060]   Because the first thing to realize is it's getting to the point now, or maybe it's at
[00:29:48.060 --> 00:29:51.780]   the point now where most researchers doing research with deep learning are not deep learning
[00:29:51.780 --> 00:29:53.340]   researchers.
[00:29:53.340 --> 00:30:01.020]   You know, they're proteomics researchers or genomics researchers or animal husbandry researchers
[00:30:01.020 --> 00:30:03.020]   or whatever, you know, or astrophysics.
[00:30:03.020 --> 00:30:05.180]   Animal husbandry I've not heard.
[00:30:05.180 --> 00:30:10.580]   I was the keynote speaker at a couple of years ago, the major international animal husbandry
[00:30:10.580 --> 00:30:11.580]   Congress.
[00:30:11.580 --> 00:30:12.580]   So, right.
[00:30:12.580 --> 00:30:16.100]   I got a nice trip to Auckland with the family.
[00:30:16.100 --> 00:30:17.820]   It was very pleasant.
[00:30:17.820 --> 00:30:22.860]   In fact, Hadley Wickham's father organized it and he invited me.
[00:30:22.860 --> 00:30:23.860]   Yeah.
[00:30:23.860 --> 00:30:24.860]   Well, I'm sorry I cut you off.
[00:30:24.860 --> 00:30:28.380]   You're making an interesting point that I interrupted for no reason.
[00:30:28.380 --> 00:30:31.580]   I didn't know that you were so ignorant about animal husbandry, Lucas.
[00:30:31.580 --> 00:30:32.580]   I'm disgusted, dude.
[00:30:32.580 --> 00:30:36.420]   I love, I love all the unusual use cases of deep learning.
[00:30:36.420 --> 00:30:41.740]   It's definitely something I collect, but that's, I have not heard that one.
[00:30:41.740 --> 00:30:42.740]   Yeah.
[00:30:42.740 --> 00:30:44.620]   So, sorry, where were we?
[00:30:44.620 --> 00:30:47.660]   We were talking about, oh yeah, researchers.
[00:30:47.660 --> 00:30:49.980]   So you're doing research into a thing, right?
[00:30:49.980 --> 00:30:55.820]   So like, I don't know, maybe it's like you're trying to find a better way to do gradient
[00:30:55.820 --> 00:31:04.000]   accumulation for FP16 training, or maybe you're trying a new activation function, or maybe
[00:31:04.000 --> 00:31:08.620]   you're trying to find out whether, you know, this different way of handling four channel
[00:31:08.620 --> 00:31:13.380]   input works well for, you know, hyperspectral satellite imagery or whatever.
[00:31:13.380 --> 00:31:18.220]   And so you, you know, the idea is to let you focus on that thing and not all the other
[00:31:18.220 --> 00:31:23.500]   things, but then you want all the other things to be done as well as possible, because if
[00:31:23.500 --> 00:31:28.220]   you do a shitty job of all the other things, then you might say like, oh, my activation
[00:31:28.220 --> 00:31:32.220]   function is actually really good, but then somebody else might notice that like, oh no,
[00:31:32.220 --> 00:31:37.260]   it was just doing it like a, it was just doing a kind of a crappy version of data augmentation
[00:31:37.260 --> 00:31:42.500]   effectively, because if we add dropout, then your thing doesn't help anymore.
[00:31:42.500 --> 00:31:48.900]   So with a layered API, you can use the high level easy, easiest bits with like all the
[00:31:48.900 --> 00:31:55.300]   defaults that work nicely together, and then you just pick the bit that you want and delve
[00:31:55.300 --> 00:31:56.500]   in as deep as you like.
[00:31:56.500 --> 00:32:00.380]   So there's kind of really four layers, key layers in our API.
[00:32:00.380 --> 00:32:03.580]   So maybe you'll go in and create a new data block, or maybe you'll go and create a new
[00:32:03.580 --> 00:32:07.900]   transform, or maybe you'll go in and create a new callback.
[00:32:07.900 --> 00:32:15.220]   So like the thing about fast.ai is it's actually far more hackable than say Keras, right, being
[00:32:15.220 --> 00:32:17.100]   take what I'm very familiar with.
[00:32:17.100 --> 00:32:23.460]   So like with Keras, you kind of have this pretty well-defined transformation pipeline,
[00:32:23.460 --> 00:32:30.740]   or tf.data, if you're using that, pretty well-defined set of atomic units you can use, and if you
[00:32:30.740 --> 00:32:34.640]   want to customize them, you're kind of out of luck, you know, often it requires going
[00:32:34.640 --> 00:32:38.480]   and creating a new tf.op in C++ or something.
[00:32:38.480 --> 00:32:44.700]   So it really helps using PyTorch, they kind of provide these really nice low latency primitives,
[00:32:44.700 --> 00:32:47.820]   and then we build out everything out of those latency primitives, and we kind of gradually
[00:32:47.820 --> 00:32:52.020]   layer the APIs on top of each other, and we make sure that they're very well documented
[00:32:52.020 --> 00:32:53.300]   all the way down.
[00:32:53.300 --> 00:32:58.780]   So you don't kind of get to a point where it's like, oh you're now in the internal API, good
[00:32:58.780 --> 00:32:59.780]   luck.
[00:32:59.780 --> 00:33:03.860]   It's like, no, it's all external API, and it's all documented, and it all has tests,
[00:33:03.860 --> 00:33:10.700]   and it all has examples, it all has explanations, so you can put your research in at the point
[00:33:10.700 --> 00:33:11.700]   that you need it.
[00:33:11.700 --> 00:33:12.700]   I see.
[00:33:12.700 --> 00:33:19.980]   But I guess when you talk about academics then, or researchers, sorry, not academics,
[00:33:19.980 --> 00:33:23.980]   you're imagining like actual machine learning researchers researching on machine learning
[00:33:23.980 --> 00:33:29.220]   itself versus like an animal husbandry researcher who needs an application of machine learning.
[00:33:29.220 --> 00:33:30.940]   I guess you're speaking to both, yeah.
[00:33:30.940 --> 00:33:31.940]   Yeah, both.
[00:33:32.940 --> 00:33:38.940]   And so, I mean, it's much easier for me to understand the needs of ML researchers, because
[00:33:38.940 --> 00:33:44.420]   that's what I do, and that's who I generally hang out with.
[00:33:44.420 --> 00:33:45.460]   But there's a lot of overlap.
[00:33:45.460 --> 00:33:50.340]   Like I found back in the days when we had conferences that you could go to, you know,
[00:33:50.340 --> 00:33:54.940]   as I walked around Europe, a lot of people would come up to me and say like, oh I just
[00:33:54.940 --> 00:33:59.220]   gave this talk, or I just gave this poster presentation, and three years ago I was a
[00:33:59.220 --> 00:34:06.300]   fast AI student, before that I was a meteorologist, or an astrophysicist, or neuroscientist, or
[00:34:06.300 --> 00:34:07.300]   whatever.
[00:34:07.300 --> 00:34:13.340]   And, you know, I used your course to understand the subject, and then I used your software,
[00:34:13.340 --> 00:34:18.100]   and then I brought in these ideas from astrophysics, or neuroscience, whatever, and now I'm here,
[00:34:18.100 --> 00:34:20.180]   I am presenting them in Europe.
[00:34:20.180 --> 00:34:24.220]   And so there's kind of like this, yeah, really interesting overlap now between the worlds
[00:34:24.220 --> 00:34:32.460]   of ML research and domain expertise, in that increasingly domain experts are becoming,
[00:34:32.460 --> 00:34:38.820]   you know, pretty well noted and well respected ML researchers as well.
[00:34:38.820 --> 00:34:42.940]   Because you kind of have to be, you know, like if you want to do a real kick-ass job
[00:34:42.940 --> 00:34:48.300]   of medical imaging, for instance, there's still a lot of foundational questions you
[00:34:48.300 --> 00:34:55.580]   have to answer about like, how do you actually deal with large 3D volumes?
[00:34:55.580 --> 00:35:00.620]   You know, it's still, these things are not solved, and so you do have to become a really
[00:35:00.620 --> 00:35:02.140]   good deep learning researcher as well.
[00:35:02.140 --> 00:35:07.020]   You know, I think one of the things that I always worry about for myself is kind of,
[00:35:07.020 --> 00:35:08.020]   you know, getting out of date.
[00:35:08.020 --> 00:35:12.980]   Like I remember being in my early 20s, and looking at some of the, you know, the tenured
[00:35:12.980 --> 00:35:17.020]   professors that were my age now, and thinking, well, you know, they've just not stayed current
[00:35:17.020 --> 00:35:18.940]   in the state of machine learning.
[00:35:18.940 --> 00:35:23.380]   And then, you know, I started a company and I kind of, you know, realized that, you know,
[00:35:23.380 --> 00:35:27.460]   I actually wasn't staying, you know, up to date myself, and, you know, kind of often
[00:35:27.460 --> 00:35:30.620]   stuck in like older techniques that I was more comfortable with, or like languages I
[00:35:30.620 --> 00:35:31.620]   was more comfortable with.
[00:35:31.620 --> 00:35:35.060]   And I feel like one of the things that you do just phenomenally well, at least from the
[00:35:35.060 --> 00:35:38.700]   outside, is staying kind of really current and on top of stuff.
[00:35:38.700 --> 00:35:40.980]   And I was just wondering if you have any thoughts on how you do that, because I-
[00:35:40.980 --> 00:35:47.260]   Well, I mean, I got to say, I really admired what you did with moving away from your world
[00:35:47.260 --> 00:35:49.380]   of crowdsourcing into deep learning.
[00:35:49.380 --> 00:35:52.980]   And I think you took like a year or so just to figure it out, right?
[00:35:52.980 --> 00:35:59.860]   Not many people do that, you know, and I think a lot of people assume they can't, because
[00:35:59.860 --> 00:36:05.100]   if you get to, I don't know, your mid-30s or whatever, and you haven't learned a significant
[00:36:05.100 --> 00:36:11.020]   new domain for the last decade, you could easily believe that you're not capable of
[00:36:11.020 --> 00:36:12.020]   doing so.
[00:36:12.020 --> 00:36:16.340]   So I think you kind of have to do what you do, which is just to decide to do it.
[00:36:16.340 --> 00:36:23.900]   I mean, for me, I took a rather extreme decision when I was 18, which was to make sure I spent
[00:36:23.900 --> 00:36:30.780]   half of every day learning or practicing something new for the rest of my life, which I've stuck
[00:36:30.780 --> 00:36:32.860]   to, certainly on average.
[00:36:32.860 --> 00:36:38.020]   Nowadays, it's, yeah, nowadays it's more like 80%.
[00:36:38.020 --> 00:36:43.940]   Yeah, I mean, it's, so for me, I mean, it's weird.
[00:36:43.940 --> 00:36:47.540]   My brain still tells me I won't be able to understand this new thing, because I start
[00:36:47.540 --> 00:36:50.460]   reading something and I don't understand it straight away, and my brain's like, okay,
[00:36:50.460 --> 00:36:51.780]   this is too hard for you.
[00:36:51.780 --> 00:36:54.140]   So you kind of have to push through that.
[00:36:54.140 --> 00:37:02.580]   But yeah, for me, I kind of had this realization, you know, as a teenager, that learning new
[00:37:02.580 --> 00:37:06.140]   skills is this high leverage activity.
[00:37:06.140 --> 00:37:09.380]   And so I kind of hypothesized that if you keep doing it for your whole life, like I
[00:37:09.380 --> 00:37:12.180]   noticed nobody did, like nobody I knew did.
[00:37:12.180 --> 00:37:18.380]   And I thought, well, if you did, wouldn't you get this kind of like exponential returns?
[00:37:18.380 --> 00:37:20.300]   And so I thought I should try to do that.
[00:37:20.300 --> 00:37:22.860]   So that's kind of been my approach.
[00:37:22.860 --> 00:37:25.180]   So you reasoned your way into that choice.
[00:37:25.180 --> 00:37:26.180]   That's amazing.
[00:37:26.180 --> 00:37:34.580]   Is it like, do you have to kind of fight your immediate instincts to do that?
[00:37:34.580 --> 00:37:36.060]   Or is it kind of a pleasure to...
[00:37:36.060 --> 00:37:37.580]   My instincts are fine now.
[00:37:37.580 --> 00:37:40.620]   What you do, I do have to do is to fight.
[00:37:40.620 --> 00:37:44.260]   Well, not anymore, not now that I work with my wife.
[00:37:44.260 --> 00:37:48.100]   And you know, I'm working with Sylvain, who's super understanding and understood me and
[00:37:48.100 --> 00:37:49.100]   is similar.
[00:37:49.100 --> 00:37:54.380]   But for nearly all my working life, fighting, or at least dealing with the people around
[00:37:54.380 --> 00:37:59.980]   me, because if somebody's like, particularly when you're the boss and you're like, okay,
[00:37:59.980 --> 00:38:02.180]   we urgently need to do X.
[00:38:02.180 --> 00:38:06.820]   And somebody can clearly see that, like, why the fuck are you like using Julia for the
[00:38:06.820 --> 00:38:07.940]   first time to use X?
[00:38:07.940 --> 00:38:08.980]   We don't even know Julia.
[00:38:08.980 --> 00:38:11.980]   You could have had it done already if you just used Perl or Python or some shit that
[00:38:11.980 --> 00:38:12.980]   you already knew.
[00:38:12.980 --> 00:38:20.940]   And it's like, well, you know, I just wanted to learn Julia.
[00:38:20.940 --> 00:38:28.180]   So yeah, it's like it drives people around me crazy that I'm working with, because everybody's
[00:38:28.180 --> 00:38:35.660]   busy and it's hard to, in the moment, appreciate that, like, okay, this moment isn't actually
[00:38:35.660 --> 00:38:38.500]   more important than every other moment for the rest of your life.
[00:38:38.500 --> 00:38:43.660]   And so if you don't spend time now getting better at your skills, then the rest of your
[00:38:43.660 --> 00:38:46.940]   life, you're going to be a little bit slower and a little bit less capable and a little
[00:38:46.940 --> 00:38:48.340]   bit less knowledgeable.
[00:38:48.340 --> 00:38:50.780]   So that's the hard bit.
[00:38:50.780 --> 00:38:54.140]   >> It also sounds to me like just from the examples that you've given that you have a
[00:38:54.140 --> 00:38:56.340]   real bias to learning by doing.
[00:38:56.340 --> 00:38:57.340]   Is that right?
[00:38:57.340 --> 00:39:01.060]   Like, do you also, like, kind of read papers and synthesize that in a different way?
[00:39:01.060 --> 00:39:05.460]   >> Yeah, but if I read a paper, I only read it until I get to the point where I decide
[00:39:05.460 --> 00:39:10.940]   it's something I want to implement or not.
[00:39:10.940 --> 00:39:16.020]   Or that there's some idea that I want to take away from it to implement.
[00:39:16.020 --> 00:39:23.220]   Yeah, so like I find doing things, I don't know, I'm a very intuitive person.
[00:39:23.220 --> 00:39:29.020]   So I find doing things and experimenting a lot, I kind of get a sense of how things kind
[00:39:29.020 --> 00:39:30.020]   of fit together.
[00:39:30.020 --> 00:39:35.140]   I really like the way Richard Feynman talked about his research and his understanding of
[00:39:35.140 --> 00:39:41.860]   papers was that he always thinks about a physical analogy every time he reads a paper and he
[00:39:41.860 --> 00:39:45.260]   doesn't go any further on a paper until he has a physical analogy in mind.
[00:39:45.260 --> 00:39:50.100]   And then he always found that he could spot the errors in papers straight away by recognizing
[00:39:50.100 --> 00:39:52.900]   that the physical analogy would break down.
[00:39:52.900 --> 00:39:53.900]   So I'm kind of a bit like that.
[00:39:53.900 --> 00:40:00.220]   I'm always looking for the context and the understanding of what it's for and then try
[00:40:00.220 --> 00:40:01.220]   to implement it.
[00:40:01.220 --> 00:40:02.220]   >> I see.
[00:40:02.220 --> 00:40:05.980]   So should we expect the next version of Fastai to be in a new language?
[00:40:05.980 --> 00:40:08.180]   Have you thought about moving away from Python?
[00:40:08.180 --> 00:40:14.100]   >> Oh, I mean, obviously I have because I looked at Swift, you know, and sadly, you
[00:40:14.100 --> 00:40:17.620]   know, Chris Latner left Google.
[00:40:17.620 --> 00:40:21.340]   So I don't know, you know, they've got some good folks still there.
[00:40:21.340 --> 00:40:22.780]   Maybe they'll make something great of it.
[00:40:22.780 --> 00:40:30.420]   But you know, I tend to kind of follow people like, you know, people who have been successful
[00:40:30.420 --> 00:40:32.960]   many times and Chris was one of those people.
[00:40:32.960 --> 00:40:33.960]   So yeah, I mean, what's next?
[00:40:33.960 --> 00:40:34.960]   I don't know.
[00:40:34.960 --> 00:40:39.020]   I mean, certainly like Python is not the future of machine learning.
[00:40:39.020 --> 00:40:47.980]   It can't be, you know, it's so nicely hackable, but it's so frustrating to work with a language
[00:40:47.980 --> 00:40:56.140]   where you can't do anything fast enough unless you, you know, call out to some external coder
[00:40:56.140 --> 00:40:59.300]   or see code.
[00:40:59.300 --> 00:41:03.900]   And you can't run anything in parallel unless you like put in a whole other process.
[00:41:03.900 --> 00:41:09.620]   Like I find working with Python, there's just so much overhead in my brain to try to get
[00:41:09.620 --> 00:41:13.740]   it to work fast enough.
[00:41:13.740 --> 00:41:17.740]   It's obviously fine for a lot of things, but not really in the deep learning world or not
[00:41:17.740 --> 00:41:20.540]   really in the machine learning world.
[00:41:20.540 --> 00:41:25.940]   So like I really hope that Julia is really successful because like there's a language
[00:41:25.940 --> 00:41:29.260]   with a nicely designed type system and a nicely designed dispatch system.
[00:41:29.260 --> 00:41:33.640]   And most importantly, it's kind of Julia all the way down.
[00:41:33.640 --> 00:41:42.720]   So you can get in and write your GPU kernel in Julia or you can, you know, all the basic
[00:41:42.720 --> 00:41:45.980]   stuff is implemented in Julia all the way down until you hit the LLVM.
[00:41:45.980 --> 00:41:48.220]   So this is an embarrassing question.
[00:41:48.220 --> 00:41:49.700]   Is Julia kind of like MATLAB?
[00:41:49.700 --> 00:41:51.660]   Is that what I should be thinking?
[00:41:51.660 --> 00:42:02.220]   It was designed to be something that MATLAB people could use, but no, it's more like,
[00:42:02.220 --> 00:42:07.420]   I don't know, like common Lisp meets MATLAB meets Python.
[00:42:07.420 --> 00:42:13.260]   Sounds a little bit like R maybe.
[00:42:13.260 --> 00:42:22.740]   You see, R has some nice ideas, but the R object system, I mean, A, there's too many
[00:42:22.740 --> 00:42:27.100]   of them and B, they're all such a hack.
[00:42:27.100 --> 00:42:30.460]   And then C, it's because it's so dynamic, it's very slow.
[00:42:30.460 --> 00:42:33.820]   So again, you have to implement everything in something that's not R and R just becomes
[00:42:33.820 --> 00:42:35.340]   a glue language on top of it.
[00:42:35.340 --> 00:42:40.020]   I mean, I spent so, so many years writing R and it's certainly better than what came
[00:42:40.020 --> 00:42:42.740]   before, but I never enjoyed it.
[00:42:42.740 --> 00:42:52.220]   So Julia is a compiled language and it's got a rich type system and it's entirely based
[00:42:52.220 --> 00:42:56.620]   on function dispatch using the type system.
[00:42:56.620 --> 00:43:02.220]   It's got a very strong kind of metaprogramming approach, so that's why you can write your
[00:43:02.220 --> 00:43:05.980]   CUDA kernel in Julia, for example.
[00:43:05.980 --> 00:43:10.740]   You know, it's got AutoGrad, again, it's written in Julia.
[00:43:10.740 --> 00:43:19.940]   It's got a lot of nice features, but unfortunately it hasn't really got the corporate buy-in
[00:43:19.940 --> 00:43:22.020]   yet.
[00:43:22.020 --> 00:43:29.020]   So it's highly reliant on a kind of this core group of super smart people that started
[00:43:29.020 --> 00:43:33.820]   it and now run Julia computing, which doesn't seem to have a business model as far as I
[00:43:33.820 --> 00:43:40.660]   can tell, other than keep getting funding from VCs, which works for a while, but at
[00:43:40.660 --> 00:43:42.100]   some point stops.
[00:43:42.100 --> 00:43:48.820]   I guess, what is the, as I know, what is the FastAI business model?
[00:43:48.820 --> 00:43:50.020]   Is there a business model?
[00:43:50.020 --> 00:43:54.380]   The FastAI business model is that I take money out of my bank account to pay for things I
[00:43:54.380 --> 00:43:57.260]   need and that's about it.
[00:43:57.260 --> 00:43:58.260]   Awesome.
[00:43:58.260 --> 00:44:00.060]   Well, you know, we always end with two questions.
[00:44:00.060 --> 00:44:04.980]   I want to make sure we have time for that to have a little bit of consistency here.
[00:44:04.980 --> 00:44:10.660]   And the first one is, you know, when you look at the different topics and, you know, kind
[00:44:10.660 --> 00:44:15.860]   of machine learning broadly defined, is there a topic that you think that people should
[00:44:15.860 --> 00:44:19.460]   pay a lot more attention to than they generally are paying attention to?
[00:44:19.460 --> 00:44:27.340]   Yes, and I think it's the world of deep learning outside of the area that you're familiar with.
[00:44:27.340 --> 00:44:35.300]   So for example, when I got started in NLP, I was shocked to discover that nobody I spoke
[00:44:35.300 --> 00:44:39.900]   to in the world of NLP had any familiarity with the last three or four years of development
[00:44:39.900 --> 00:44:41.940]   in computer vision.
[00:44:41.940 --> 00:44:49.140]   The idea of like transfer learning, for example, and how incredibly flexible it was.
[00:44:49.140 --> 00:44:55.740]   So that's what led to ULM fit, which in turn led to GPT, which in turn led to GPT2.
[00:44:55.740 --> 00:45:00.420]   And before ULM fit happened, every NLP researcher I spoke to, I said, like, what do you think,
[00:45:00.420 --> 00:45:05.580]   you know, about this idea of like super massive transfer learning from language models?
[00:45:05.580 --> 00:45:08.420]   Everybody I spoke to in NLP said that's a stupid idea.
[00:45:08.420 --> 00:45:11.580]   And everybody I spoke to in computer vision said, yes, of course, I'm sure everybody does
[00:45:11.580 --> 00:45:12.820]   that already.
[00:45:12.820 --> 00:45:18.660]   So yeah, I think in general, people are way too specialized in deep learning.
[00:45:18.660 --> 00:45:23.260]   And there's a lot of good ideas in other parts of it.
[00:45:23.260 --> 00:45:24.260]   Interesting.
[00:45:24.260 --> 00:45:25.260]   Cool.
[00:45:25.260 --> 00:45:30.140]   And then our final question we always ask, and I kind of wonder, you'll have an interesting
[00:45:30.140 --> 00:45:31.140]   perspective on this.
[00:45:31.140 --> 00:45:36.300]   You know, typically we're talking to people that are trying to use machine learning model
[00:45:36.300 --> 00:45:42.540]   for some purpose, like animal husbandry, but you've sort of seen this wide range of applications.
[00:45:42.540 --> 00:45:47.900]   And when you look across the things that you've seen kind of go from like ideation to like
[00:45:47.900 --> 00:45:55.420]   deployed thing that's working and useful, where do you see the biggest bottleneck?
[00:45:55.420 --> 00:45:59.020]   I mean, the projects I've been involved in throughout my life around machine learning
[00:45:59.020 --> 00:46:02.780]   have always been successfully deployed, you know.
[00:46:02.780 --> 00:46:06.060]   So I kind of get frustrated with all these people who tell me that machine learning is
[00:46:06.060 --> 00:46:09.220]   just this abstract thing that no one's actually using.
[00:46:09.220 --> 00:46:15.740]   I think a big part of the problem is there's kind of people that understand business and
[00:46:15.740 --> 00:46:20.180]   logistic and process management, and there's kind of people that understand AI and algorithms
[00:46:20.180 --> 00:46:24.380]   and data, and there's not much connectivity between the two.
[00:46:24.380 --> 00:46:27.620]   So like I spent 10 years working as a management consultant.
[00:46:27.620 --> 00:46:33.420]   So all my life was logistics and business processes and HR and all that stuff.
[00:46:33.420 --> 00:46:36.020]   It's kind of hard to picture you as a management consultant.
[00:46:36.020 --> 00:46:39.380]   You must have been a surprising consultant.
[00:46:39.380 --> 00:46:42.700]   Yeah, I tried to fake it as best as I could.
[00:46:42.700 --> 00:46:44.700]   For sure.
[00:46:44.700 --> 00:46:50.700]   I've noticed a lot of people in the kind of machine learning world really underappreciate
[00:46:50.700 --> 00:46:57.780]   the complexity of dealing with constraints and finding opportunities and disaggregating
[00:46:57.780 --> 00:46:59.700]   value chains.
[00:46:59.700 --> 00:47:00.900]   Or they'll do the opposite.
[00:47:00.900 --> 00:47:06.380]   They'll just assume it's so hard that it's impossible without realizing there's like,
[00:47:06.380 --> 00:47:09.620]   you know, large groups of people around the world who spend their lives studying these
[00:47:09.620 --> 00:47:13.060]   questions and finding solutions to them.
[00:47:13.060 --> 00:47:20.940]   I think in general, I'd love to see better cross-disciplinary teams and more people
[00:47:20.940 --> 00:47:26.460]   on the kind of the MBA side developing kind of AI skills and more people on the AI side
[00:47:26.460 --> 00:47:31.580]   kind of developing understanding of business and teams and all that.
[00:47:31.580 --> 00:47:37.580]   I guess you have this broad view from your background and you've watched these ML projects
[00:47:37.580 --> 00:47:38.820]   kind of get deployed in these roles.
[00:47:38.820 --> 00:47:43.140]   So I guess maybe the question is more like, were there points that sort of surprised you
[00:47:43.140 --> 00:47:47.900]   with their level of difficulty just to kind of move through it?
[00:47:47.900 --> 00:47:53.780]   Did you have mishaps where you thought the model was working and then when it was deployed
[00:47:53.780 --> 00:47:58.900]   into production, it didn't work as well as you were hoping or thought it would?
[00:47:58.900 --> 00:48:01.060]   No, not at all.
[00:48:01.060 --> 00:48:07.060]   I don't know if that sounds weird, but it's just, you know, even a small amount of background
[00:48:07.060 --> 00:48:12.540]   in like doing the actual work that the thing you're building is meant to be integrating
[00:48:12.540 --> 00:48:13.540]   with.
[00:48:13.540 --> 00:48:19.900]   You know, I spent 10 years, eight years working on an insurance pricing business entirely
[00:48:19.900 --> 00:48:22.540]   based on operations research and machine learning.
[00:48:22.540 --> 00:48:27.220]   But before that, you know, the last four or five years of my management consulting career
[00:48:27.220 --> 00:48:29.220]   was nearly entirely in insurance.
[00:48:29.220 --> 00:48:33.580]   So you know, there's not much very surprising that happens.
[00:48:33.580 --> 00:48:37.820]   I know the people, I know the processes.
[00:48:37.820 --> 00:48:42.940]   And so that's why I think like I would much rather see, I don't know, like if somebody's
[00:48:42.940 --> 00:48:49.300]   going to do a paralegal AI business, I'd much rather see a paralegal do it than an AI person
[00:48:49.300 --> 00:48:50.300]   do it.
[00:48:50.300 --> 00:48:55.300]   Or if they're going to do a like, you know, HR recruiting AI business, I'd much rather
[00:48:55.300 --> 00:48:58.380]   see someone with an HR recruiting background do it.
[00:48:58.380 --> 00:49:01.340]   Like it's super difficult.
[00:49:01.340 --> 00:49:07.060]   Like there's just no way to understand an industry really well without doing that industry
[00:49:07.060 --> 00:49:09.060]   for a few years.
[00:49:09.060 --> 00:49:12.500]   And what would you, so like, you know, because I know some of these people and I get this
[00:49:12.500 --> 00:49:15.900]   question all the time, I'll channel a question that I'm sure is in people's heads watching
[00:49:15.900 --> 00:49:16.900]   this.
[00:49:16.900 --> 00:49:21.180]   So if you are that, you know, paralegal who's starting, you know, a paralegal AI enabled
[00:49:21.180 --> 00:49:25.780]   business, how would you do the AI part?
[00:49:25.780 --> 00:49:30.220]   Well, obviously I would take the fast AI courses.
[00:49:30.220 --> 00:49:33.460]   I mean, I would, I mean, seriously, I would make sure I was good at coding.
[00:49:33.460 --> 00:49:39.460]   You know, I'd spend a year working on coding.
[00:49:39.460 --> 00:49:44.340]   And yeah, I mean, the fast AI courses are absolutely designed for you.
[00:49:44.340 --> 00:49:52.620]   And I would be careful of bringing on a so-called AI expert until you've had a go at doing it
[00:49:52.620 --> 00:49:57.460]   all yourself, because I found like most people in that situation, for obvious reasons, feel
[00:49:57.460 --> 00:50:03.100]   pretty intimidated by the AI world and kind of a bit humbled by it, a bit overwhelmed
[00:50:03.100 --> 00:50:04.100]   by it.
[00:50:04.100 --> 00:50:07.100]   And they'll bring on, you know, a self-described expert.
[00:50:07.100 --> 00:50:09.780]   They have no ability to judge the expertise of that person.
[00:50:09.780 --> 00:50:15.580]   So they end up bringing somebody who's just good at projecting confidence, which is probably
[00:50:15.580 --> 00:50:17.860]   negatively correlated with actual effectiveness.
[00:50:17.860 --> 00:50:25.820]   So I'd say, yeah, do it, do it yourself for a year, build the best stuff you can.
[00:50:25.820 --> 00:50:31.820]   I do find a lot of fast AI alum with kind of backgrounds of domain experts are shocked
[00:50:31.820 --> 00:50:36.920]   when they then get involved in the world of AI experts and they find they're much better
[00:50:36.920 --> 00:50:43.780]   at training models that actually predict things correctly than the modeling experts are.
[00:50:43.780 --> 00:50:48.180]   I'm sure you've had that experience as somebody who, you know, like me, doesn't have a technical
[00:50:48.180 --> 00:50:49.180]   background in this area.
[00:50:49.180 --> 00:50:50.180]   Yeah.
[00:50:50.180 --> 00:50:51.900]   Well, thank you so much.
[00:50:51.900 --> 00:50:55.260]   This is super fun and educational for me.
[00:50:55.260 --> 00:50:57.100]   Thank you very much for having me.
[00:50:57.100 --> 00:50:57.780]   Yeah, my pleasure.
[00:50:57.780 --> 00:51:01.140]   [MUSIC PLAYING]
[00:51:01.140 --> 00:51:04.500]   [MUSIC PLAYING]
[00:51:04.500 --> 00:51:07.080]   (gentle music)
[00:51:07.080 --> 00:51:13.160]   [Music]

