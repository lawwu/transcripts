
[00:00:00.000 --> 00:00:05.480]   Hey, my name is Chip and I'm here to hopefully talk about topics that is
[00:00:05.480 --> 00:00:09.560]   In the mind on the mind a lot of people they have a talk about recently
[00:00:09.560 --> 00:00:11.240]   So everywhere I go today
[00:00:11.240 --> 00:00:16.560]   I see that companies want to do something with Jerev AI and when I ask for more information
[00:00:16.560 --> 00:00:20.680]   It's like yeah, we don't exactly know what we are doing
[00:00:20.680 --> 00:00:28.640]   So our curiosity who here are thinking about doing something with Jerev AI, but not quite sure what to do yet
[00:00:28.720 --> 00:00:30.320]   Raise your hand
[00:00:30.320 --> 00:00:32.680]   Okay, I can see quite a lot of hands here
[00:00:32.680 --> 00:00:37.120]   So I would say that like if your leadership wants you to do something with Jerev AI
[00:00:37.120 --> 00:00:42.840]   You're pretty lucky because I also meet a lot of people was like, okay, I see the incredible high
[00:00:42.840 --> 00:00:48.720]   There's so much excitement and I just want to do something with it, but my company doesn't let me do it
[00:00:48.720 --> 00:00:50.720]   So I feel like you're in a great spot
[00:00:50.720 --> 00:00:55.020]   So the agenda is pretty clear like it's pretty short
[00:00:55.280 --> 00:00:59.880]   So I think it's gonna divide the two phases once it's like when we are in
[00:00:59.880 --> 00:01:05.440]   Explorations to decide what you do and then once you figure on use case and we want to build
[00:01:05.440 --> 00:01:09.200]   Then how do we go about it and seeing we have only 15 minutes?
[00:01:09.200 --> 00:01:13.360]   So it's not gonna be very in-depth discussions and we know that we still in the early days
[00:01:13.360 --> 00:01:19.240]   So I hope that if there's anything you feel like we can relate to or you don't think it's quite right hit me up
[00:01:19.240 --> 00:01:21.240]   I would happy to talk to with you more
[00:01:22.080 --> 00:01:28.760]   So during explorations I would go through like four steps like set expectations minimize risk
[00:01:28.760 --> 00:01:32.000]   Invest in things at last and just experiment
[00:01:32.000 --> 00:01:35.320]   So set expectations at this point
[00:01:35.320 --> 00:01:42.000]   I think that a lot of us has realized that it's pretty easy to build some cool demos with LMS
[00:01:42.000 --> 00:01:44.480]   But very hard to build an actual product
[00:01:44.480 --> 00:01:50.840]   So in the beginning maybe can test some goal if you just want to build some cool demos you show to the customers
[00:01:50.840 --> 00:01:54.100]   You show that hey, we are ahead of the curve. We know what's up
[00:01:54.100 --> 00:01:54.920]   It's okay
[00:01:54.920 --> 00:01:59.560]   If you just want to like put some engineers at it just so you can build out the in-house
[00:01:59.560 --> 00:02:01.920]   Muscles, it's okay as well
[00:02:01.920 --> 00:02:04.320]   But if you want to build an actual product
[00:02:04.320 --> 00:02:10.080]   Then you would need to think about it more carefully and see like what you want to achieve with this product how much money you
[00:02:10.080 --> 00:02:12.500]   Want to put in it and what you will get out of it
[00:02:12.500 --> 00:02:19.720]   So we see in the early day, right like charge EPT came out in November last year
[00:02:19.720 --> 00:02:24.400]   So it less than a year old since so far we have seen incredible capacity of it
[00:02:24.400 --> 00:02:29.360]   So there are many things that LMS can do but whether this thing would actually
[00:02:29.360 --> 00:02:33.440]   Translate to net revenue for the company is still unclear
[00:02:33.440 --> 00:02:36.820]   So people when I told people I'm giving this talk
[00:02:36.820 --> 00:02:43.240]   They said like oh, I would love to hear more about what LMS can do cannot do and I think it's a pretty hard
[00:02:43.240 --> 00:02:44.120]   So there's a book
[00:02:44.120 --> 00:02:51.240]   I think it's a really interesting book by Arthur Clark called called like profiles of the future and he mentioned that like no one should
[00:02:51.240 --> 00:02:55.840]   Ever go but go around and tell people what technology cannot do in the future
[00:02:55.840 --> 00:02:58.960]   Because they will always be proven wrong in the long run
[00:02:58.960 --> 00:03:03.160]   So I do think that's like there are a lot of things today that LMS cannot do very well
[00:03:03.160 --> 00:03:06.880]   But we have no idea like for how long this will stay true
[00:03:06.880 --> 00:03:13.400]   So I would say that we are living in an era of like a lot of changes and uncertainty
[00:03:13.680 --> 00:03:18.000]   So we do not know what's gonna happen a year down the line or what will happen
[00:03:18.000 --> 00:03:20.640]   It's like even it's like six months down the line
[00:03:20.640 --> 00:03:28.280]   so in this time one foundation making frameworks that I really appreciate that comes from finance and
[00:03:28.280 --> 00:03:31.840]   Reinforcement learning is a framework to minimize risks
[00:03:31.840 --> 00:03:38.440]   So there are three questions that I would try to go into when I try to evaluate my
[00:03:38.440 --> 00:03:41.560]   JWA a strategy or what I have seen company adopt that
[00:03:42.280 --> 00:03:49.140]   So the first question is is we need to evaluate how disruptive JFA I is gonna be to our business
[00:03:49.140 --> 00:03:51.880]   So I break them down into three category
[00:03:51.880 --> 00:03:58.200]   So I first like if I don't do anything can competitors with JFA I make me obsolete
[00:03:58.200 --> 00:04:04.040]   So we have seen that many industries that we see clearly is are going to be disrupted by JFA
[00:04:04.040 --> 00:04:10.600]   I so any industries that involves a lot of creative work like advertising gaming media entertainment
[00:04:10.640 --> 00:04:14.560]   It's clearly being very disrupted and it really should be worried
[00:04:14.560 --> 00:04:21.620]   All right industries that involved a lot of document processing like legal HR insurance claim processing
[00:04:21.620 --> 00:04:28.120]   So another category is if I don't do anything when I miss out on
[00:04:28.120 --> 00:04:30.600]   Opportunity to increase revenue
[00:04:30.600 --> 00:04:37.160]   So these are like a lot of a lot of tasks that you can use JFA I should like boost productivity
[00:04:37.280 --> 00:04:44.840]   So for example, like you can use LM should like have better customer support either with chat or in the true call centers
[00:04:44.840 --> 00:04:50.920]   You can have better search and recommendations. You can have like productivity enhancements who like automated note-taking
[00:04:50.920 --> 00:04:53.920]   summarizations
[00:04:53.920 --> 00:04:58.740]   Information aggregations for some boy can have LM to look at a lot of like company
[00:04:58.740 --> 00:05:04.060]   Information and try to like Jared reports like on how best you like approach and sell to them
[00:05:05.040 --> 00:05:09.520]   Another category is very interesting. Is this like if there are opportunities?
[00:05:09.520 --> 00:05:14.360]   What advantage what competitive advantage do I have to capture those?
[00:05:14.360 --> 00:05:16.940]   so there are certain aspect of
[00:05:16.940 --> 00:05:20.960]   There are certain things that we need to like be really competitive in this era
[00:05:20.960 --> 00:05:26.120]   And if you have any of those maybe you should consider going into them. So for example
[00:05:26.120 --> 00:05:30.320]   Proprietary data. So this point we know this like on the LM
[00:05:30.320 --> 00:05:34.200]   Genera requires a lot of data if you somehow have a lot of data
[00:05:34.200 --> 00:05:39.160]   Maybe you should be wondering like how do we take advantage of this study that we have?
[00:05:39.160 --> 00:05:43.660]   I'm pretty sure companies like read it and it's like a flow for a kicker themselves right now
[00:05:43.660 --> 00:05:47.840]   Like why did we let companies like open AI use our data and make so much money?
[00:05:47.840 --> 00:05:53.220]   So another category is that you need another type of advantage is that you have a lot of compute
[00:05:53.220 --> 00:05:59.480]   so because of the GPU shortage right now, so any companies that sadly have like a big stock of like a
[00:06:00.000 --> 00:06:01.560]   300s lying around
[00:06:01.560 --> 00:06:05.520]   Probably want you think like what can I do to like take advantage of?
[00:06:05.520 --> 00:06:10.440]   some commodities that I have and a lot of other people are trying to get a hold of and
[00:06:10.440 --> 00:06:15.560]   Another is an existing use user base. So like I know this I'm not sure you
[00:06:15.560 --> 00:06:23.240]   Recently have seen the article called like we have no mode and neither does open AI from the Google engineer
[00:06:23.240 --> 00:06:28.420]   So I don't think this is quite true. I do think that's like having an existing user base
[00:06:28.420 --> 00:06:34.520]   It's it's a big mode, right? So like yes a startup can have an incredible say photo editing tool
[00:06:34.520 --> 00:06:39.120]   But then I if you are Adobe and you just have like a lot of Photoshop user
[00:06:39.120 --> 00:06:43.640]   It can just like use that as a feature in the product. So I'm really hesitant to
[00:06:43.640 --> 00:06:48.320]   So I see a lot of startups that build like really cool applications
[00:06:48.320 --> 00:06:53.840]   But then I just wonder like do I want to be a startup that can be just a feature of like Google?
[00:06:54.320 --> 00:07:02.140]   Google Google Docs or a feature of notions or features like Google Photos. It's very hard to compete with this company
[00:07:02.140 --> 00:07:09.700]   So depending on what's a category you're in but like if you if your business is being threatened by
[00:07:09.700 --> 00:07:15.840]   Comparison AI I do think that's like it might be the time you go on in and the try to get stay ahead
[00:07:15.840 --> 00:07:18.300]   If you don't do anything
[00:07:18.300 --> 00:07:23.160]   It will if I don't do anything I will miss out opportunity to boost revenue
[00:07:23.180 --> 00:07:26.780]   So if you're in this category, maybe it's time to evaluate
[00:07:26.780 --> 00:07:31.560]   Builder versus by decisions for example, if you have to do a lot of customer support
[00:07:31.560 --> 00:07:39.440]   You can either consider building their own model in-house or using one of the like many services that provide chatbot for the company
[00:07:39.440 --> 00:07:43.500]   So they have certain there's a certain advantage and disadvantage or each approach
[00:07:43.500 --> 00:07:45.260]   But I don't think there's a general framework
[00:07:45.260 --> 00:07:47.340]   Should I decide that and as a third?
[00:07:47.340 --> 00:07:53.060]   Category is it's a lot more interesting and I do think this is time for you to sit back and like have to make some
[00:07:53.060 --> 00:07:54.300]   bets because yes
[00:07:54.300 --> 00:07:59.500]   You has a competitive advantage but like who else and so have them and like what you can do about it
[00:07:59.500 --> 00:08:04.180]   So another things that I would probably
[00:08:04.180 --> 00:08:07.540]   Go into right now and I see a lot of companies are doing it
[00:08:07.540 --> 00:08:13.140]   It should figure out the data story if there's one thing we learned from all the Jeff AI hype
[00:08:13.140 --> 00:08:20.100]   Is that data matters a lot and I if you have data if I don't know what to do about it, then you are
[00:08:20.100 --> 00:08:21.900]   actually
[00:08:21.900 --> 00:08:23.500]   at a disadvantage
[00:08:23.500 --> 00:08:28.620]   So so far I would say so I see that I could see over over again
[00:08:28.620 --> 00:08:30.620]   So like companies with a lot of data
[00:08:30.620 --> 00:08:36.420]   But there's not like a way to like use them say like the data they have is not very easily accessible
[00:08:36.420 --> 00:08:40.420]   Maybe they are like share across different teams or like they are not in the same
[00:08:40.420 --> 00:08:43.380]   Central locations or is this really fragmented?
[00:08:43.380 --> 00:08:46.060]   So I think would be a time to like it would be a good time
[00:08:46.060 --> 00:08:51.060]   Just figure out like how to consolidate all the data as they already have and I what you do about it
[00:08:51.980 --> 00:08:55.060]   And another thing is that you update the data terms of use
[00:08:55.060 --> 00:08:58.400]   Especially if you have users and have user data
[00:08:58.400 --> 00:09:01.860]   Like what do you want to like you do with the user data?
[00:09:01.860 --> 00:09:06.980]   And what do you want to have the term of service for the company for other services that my?
[00:09:06.980 --> 00:09:09.820]   Scrapes or data. So I think that's what like
[00:09:09.820 --> 00:09:15.020]   What is the first things that like that it is like overflow did was to change the data term of service to prevent?
[00:09:15.420 --> 00:09:20.020]   Companies like opening I to scrape the data and use a trade model and of course, yes
[00:09:20.020 --> 00:09:27.020]   I always to put guardrails around it a quality and governance because now you have a lot of data from different sources
[00:09:27.020 --> 00:09:32.660]   You might want to control like who can access them to make sure that you don't make yourself
[00:09:32.660 --> 00:09:36.580]   vulnerable to a lawsuit from users for privacy violations
[00:09:36.580 --> 00:09:43.820]   And I think that's the thing I would say just like really pants like if you decide to go or in just great
[00:09:43.900 --> 00:09:51.340]   But I also see a lot of companies because it's a time uncertainty. It can be dangerous to make very big sweeping decisions
[00:09:51.340 --> 00:09:57.260]   So for example, I have since companies saying okay scrape what have we been building is the last four years now
[00:09:57.260 --> 00:09:59.420]   We're jet of AI startup, which is fine
[00:09:59.420 --> 00:10:03.460]   but like it's a big bet you make and as long I hope that like it's gonna be a good bet or
[00:10:03.460 --> 00:10:09.940]   I seen a lot of companies they did my 2023 planning and just scrape that we just like everything we do right now
[00:10:09.940 --> 00:10:15.300]   It's jet of AI or as companies go out and they bite on the a 100 available
[00:10:15.300 --> 00:10:18.180]   So one of this like I'm not saying that's like you shouldn't do it
[00:10:18.180 --> 00:10:22.540]   But I do think that's like they have big decisions and I just hope that says like concrete
[00:10:22.540 --> 00:10:25.380]   Lysis or evidence to back up the decisions
[00:10:25.380 --> 00:10:31.520]   Another thing is like, of course everyone wants to invest in things at last, right?
[00:10:31.520 --> 00:10:37.980]   But like in hindsight, we know what last but in the moment is really hard to say like to know what what will last
[00:10:38.200 --> 00:10:43.880]   So one heuristics that I have seen that I think it could be pretty interesting is called Lindy's law
[00:10:43.880 --> 00:10:49.880]   So it states that the future life expectancy of some of like technology on idea is
[00:10:49.880 --> 00:10:52.840]   Proportional to how long it has been in existence
[00:10:52.840 --> 00:10:55.440]   So say if a technology has been around for 10 years
[00:10:55.440 --> 00:10:59.480]   Then maybe we can even heuristic should predict that it will last for another years
[00:10:59.480 --> 00:11:03.600]   I think you can use a tiding for relationship like if I've been dating for a year
[00:11:03.600 --> 00:11:07.280]   Then maybe you can be together for another year and it was a heuristic is not correct
[00:11:07.720 --> 00:11:12.120]   But like it's weird if you just like met someone three months ago and decide to plan for two years ahead
[00:11:12.120 --> 00:11:14.760]   But anyway, I digress
[00:11:14.760 --> 00:11:20.560]   So so like even though last language models are new a lot of the fundamental
[00:11:20.560 --> 00:11:24.480]   Fundamentals behind them are not are not are not new at all. They've been around for a long time
[00:11:24.480 --> 00:11:30.840]   First of all language modeling it was like the concept was proposed by Klaus Shannon back in the 50s
[00:11:30.840 --> 00:11:37.200]   So it's not it was like it was not it's not new or the concept embeddings has been around in like for two decades
[00:11:37.200 --> 00:11:44.080]   Also vector databases. I know there's a lot of there seems to be a lot of vector databases company right now, but but
[00:11:44.080 --> 00:11:47.960]   They've been around for a while like Facebook has its open source files
[00:11:47.960 --> 00:11:53.360]   Like six years ago Google has scans in like three years ago and they're both really good
[00:11:53.360 --> 00:11:59.040]   And also I do believe that like making data cheaper faster more accessible
[00:11:59.040 --> 00:12:05.280]   We never go away and just want you plug in. That's what we do. That's what our company clipboard does
[00:12:06.280 --> 00:12:07.600]   So
[00:12:07.600 --> 00:12:12.680]   For me, I have a personalism of tests like when I try to predict whether something okay
[00:12:12.680 --> 00:12:14.760]   A lot of my prediction are pretty horrible
[00:12:14.760 --> 00:12:17.560]   but I was like
[00:12:17.560 --> 00:12:20.400]   Some lakes when I see some new technology
[00:12:20.400 --> 00:12:25.560]   I was trying to do thing like try to get to know it and decided does it sound a little bit too hacky for me?
[00:12:25.560 --> 00:12:29.180]   So this is a bit get us into the realm of like speculations
[00:12:29.180 --> 00:12:32.240]   So say like for things like from engineering, right?
[00:12:32.240 --> 00:12:37.440]   So I think a lot of discussions on my weather prom engineering will stay so I do think it's like there a lot
[00:12:37.440 --> 00:12:44.920]   I think that context learning is possible will stay because it's very useful to potion a input information into the LLM and
[00:12:44.920 --> 00:12:51.040]   Make predictions based on the context. However, it's a lot of hacky stuff like replace
[00:12:51.040 --> 00:12:57.600]   Questions with Q or like ask the LM to answer truthfully or like they pretend to be XYZ
[00:12:57.600 --> 00:13:00.680]   I think a lot of that could get into the rebel like a little bit too hacky
[00:13:00.680 --> 00:13:06.360]   and I'm not sure how long maybe in the future models when be able to address them and he user don't have to use some
[00:13:06.360 --> 00:13:08.360]   hack anymore or
[00:13:08.360 --> 00:13:15.040]   Another thing is about more architectures things change over time and I know the transformer architecture has been very
[00:13:15.040 --> 00:13:20.560]   Resilient and has been a great but I do think that's like if we look at the history of more architectures
[00:13:20.560 --> 00:13:24.960]   They change over time. So I wouldn't be surprised if it's the next few years. It might see something new
[00:13:24.960 --> 00:13:29.200]   So I was talking to a friend about like what does AI literacy means?
[00:13:29.240 --> 00:13:35.440]   So I do think that's like a literate AI literacy for the future means less about knowing how to build
[00:13:35.440 --> 00:13:42.300]   Transformer from scratch but learning about like how this model works what limitations they have and how to overcome these limitations
[00:13:42.300 --> 00:13:45.080]   and of course like
[00:13:45.080 --> 00:13:49.640]   You pretty want to experiment. So even though the experiment is important
[00:13:49.640 --> 00:13:54.480]   I do think it's important to timebox experiments and set clear goals for like what you want to achieve out
[00:13:54.600 --> 00:13:58.040]   At the end of it for example in my set at 10 bucks a month
[00:13:58.040 --> 00:14:02.560]   I end up with one engineer on it for a month and I decide like what is a decision?
[00:14:02.560 --> 00:14:03.840]   We want to make at the end of the month
[00:14:03.840 --> 00:14:08.240]   I do you want to pursue and train a model like fine-tune or do you want to decide that?
[00:14:08.240 --> 00:14:14.080]   Okay, this is not our core focus right now and we just move on and go back to like business as usual
[00:14:14.080 --> 00:14:19.960]   And of course, I went to experiments. I know this incredible open-source model out there, but during experiments
[00:14:19.960 --> 00:14:22.960]   It's probably like a good idea just to use whatever is the easiest
[00:14:23.080 --> 00:14:28.440]   So API is actually pretty cheap to to experiment with and I say a lot of tutorials
[00:14:28.440 --> 00:14:34.620]   I had cathons you can join and I do believe it's like a hundred dollars and one weekend can get you a long way
[00:14:34.620 --> 00:14:38.480]   Just to get a sense of like what is out there and what LMS can do and cannot do
[00:14:38.480 --> 00:14:43.000]   And of course nice ago maybe me through the
[00:14:43.000 --> 00:14:50.120]   experimentations that you can like get a good sense of the L and behaviors and trying to decide if this and if this
[00:14:50.240 --> 00:14:52.560]   Behaviors are deal-breakers for you or not
[00:14:52.560 --> 00:14:58.360]   So first of all this now we know that like LMS I have like very ambiguous input and output
[00:14:58.360 --> 00:15:03.880]   So say that if you change this in pushes a little bit first of all you change a lower case uppercase
[00:15:03.880 --> 00:15:06.680]   You might get different result or you might ask them
[00:15:06.680 --> 00:15:10.440]   You might ask a model to my score give a score text from 0 to 10
[00:15:10.440 --> 00:15:15.880]   But there's no way you guarantee that the model will always return exactly 0 to us from 0 to 10
[00:15:15.880 --> 00:15:22.380]   So that could be like tricky like if you have downstream applications that depend on the output and the output just break the format
[00:15:22.380 --> 00:15:23.560]   That could be very tricky
[00:15:23.560 --> 00:15:32.040]   And I'm also I must have heard people trying to use LMG in first schemas data schemas and that make me like really really
[00:15:32.040 --> 00:15:39.200]   Scare because for the schema you want something that's very exact so I wouldn't want to use LMS to do anything with schemas
[00:15:39.200 --> 00:15:45.000]   Of course nice as you know, that's like LMS have a lot how you so this is just like hallucinate randomly
[00:15:45.000 --> 00:15:48.880]   And if we saw the case when the lawyers use LMS
[00:15:48.880 --> 00:15:54.600]   Should I summarize some legal documents and it turned out it's like LMS just made up stuff like it was not as a little
[00:15:54.600 --> 00:16:00.440]   Document and now that lawyer in trouble so so I know this I people say okay, it can make up stuff sometimes
[00:16:00.440 --> 00:16:03.520]   But then we expect human to like correct it
[00:16:03.520 --> 00:16:06.360]   so I have not seen that to be very successful because
[00:16:06.360 --> 00:16:11.300]   Somehow I do think that we so as an engineer as a programmer
[00:16:11.300 --> 00:16:17.880]   I know that I much prefer writing code from scratch then I inherit somebody else's code and try to like fix it
[00:16:17.880 --> 00:16:20.880]   So I wonder it could be the same like for people like okay
[00:16:20.880 --> 00:16:24.600]   Do they prefer just like read it themselves and summarize it or just like expect?
[00:16:24.600 --> 00:16:28.460]   LLM to summarize and then correct the summarizations
[00:16:28.460 --> 00:16:34.080]   So another thing is privacy so say if you have a lot of come a day like you have a lot of
[00:16:34.600 --> 00:16:41.380]   Documentations locally and you want to train maybe the chatbot for able to access a lot like internal document
[00:16:41.380 --> 00:16:47.060]   like is there a way to prevent your LLM from just like accidentally like divulge like
[00:16:47.060 --> 00:16:53.540]   Private information PII information because if it does it's really it's you you're getting trouble
[00:16:53.540 --> 00:16:59.400]   Of course, I saw like they're also like unstable infrastructure like in terms of performance and latency
[00:16:59.420 --> 00:17:06.500]   If peppery experience that in the last few weeks, for example, people have have a lot of complaints about GPT for performance
[00:17:06.500 --> 00:17:08.420]   It's not not being very good
[00:17:08.420 --> 00:17:13.660]   Somehow there's a huge drop in the performance and any application that depend on
[00:17:13.660 --> 00:17:18.500]   GPT for which actually got impacted and of course their latency if you use API's
[00:17:18.500 --> 00:17:23.260]   There's always network latency and suddenly there's a surge of traffic and the model just like a slower
[00:17:23.260 --> 00:17:28.780]   and also, of course the inference cost GPT for cost of fortune to to the inference and
[00:17:29.280 --> 00:17:32.920]   Also, like their minor problem like forward and backward compatibility
[00:17:32.920 --> 00:17:37.300]   So like if you have have a lot of problems and suddenly there's a newer model
[00:17:37.300 --> 00:17:42.480]   There's no way to ensure that's an existing problems will work with the newer models
[00:17:42.480 --> 00:17:48.900]   Okay, so I will go so far cool. Oh, it's really hot in here. Oh my god
[00:17:48.900 --> 00:17:56.340]   So, okay. So like for the next way, I hope that you have a lot of fun doing experimentations and we want to get into building
[00:17:56.420 --> 00:17:59.660]   So in this phase you pretty want to take things more seriously
[00:17:59.660 --> 00:18:05.800]   So right so we need to pretty need to understand the LM stack like how different models are being trained
[00:18:05.800 --> 00:18:11.500]   So you can evaluate which model is good for you and you want to evaluate which level of the stack so you want to get into
[00:18:11.500 --> 00:18:18.100]   And then of course you need to actually go to the process of implementations and also evaluate the model
[00:18:18.100 --> 00:18:22.380]   So the air is like I think it's my pretty not new to anyone
[00:18:22.380 --> 00:18:28.020]   So we have like the prom engineering fight tuning and distillate installations and also cheating model from scratch
[00:18:28.020 --> 00:18:34.020]   And of course is all the infrastructure LMS like databases like very databases like locks
[00:18:34.020 --> 00:18:40.500]   To like get to locks a report to request responses and also like caching so they can I reduce a cost
[00:18:40.500 --> 00:18:48.580]   so prompting and fight tuning so we think of my prompting fight tuning it's like shifting more of the
[00:18:49.620 --> 00:18:54.180]   Information you want to put into context to bake that into the model. So it's a pretty funny
[00:18:54.180 --> 00:18:59.280]   I actually went to a panel recently and someone is the audience as it's like hey
[00:18:59.280 --> 00:19:06.100]   This company is using a lot of prompt and somehow I just want to like Jerry like I just don't want to input that prompt
[00:19:06.100 --> 00:19:11.900]   Every single time what do I do? And was like, this is fine tuning. Yeah, just like just if you already have a lot of
[00:19:12.940 --> 00:19:19.820]   Prompts and responses so you can fight on the model and it's a pretty straightforward. So it's actually nice because for API's
[00:19:19.820 --> 00:19:23.500]   API's charge you per input token
[00:19:23.500 --> 00:19:27.900]   So the more tokens it puts along as a prompt the more money you have to pay
[00:19:27.900 --> 00:19:33.140]   So if you can offset you can bake as a prompt into your fight to model
[00:19:33.140 --> 00:19:37.520]   Then the input tokens could be shorter and it might be what's happening
[00:19:38.020 --> 00:19:43.980]   And it's and it's my severe money. But the caveat is that like companies they open AI is a fight to an API
[00:19:43.980 --> 00:19:45.980]   It might be a bit more expensive
[00:19:45.980 --> 00:19:50.740]   And things I don't think we have time to go into this whole things
[00:19:50.740 --> 00:19:56.460]   But it could be the breakdown of like there's a process of training some model and a charge ipt
[00:19:56.460 --> 00:20:02.020]   so the first phase could be as a language modeling when you get out a pre-trained language model something like
[00:20:02.620 --> 00:20:05.140]   funkin bloom stable as they were
[00:20:05.140 --> 00:20:07.820]   so so you can think of this as
[00:20:07.820 --> 00:20:14.620]   Completion machines so language modeling is just like you train a models you given the existing tokens
[00:20:14.620 --> 00:20:21.500]   Predicts the next token. So like if you input it some like a sentence if we try to complete the sentence, so
[00:20:21.500 --> 00:20:25.180]   But completion is not the same as conversations, right?
[00:20:25.180 --> 00:20:31.500]   So like when you ask the model something the model can complete it by like adding more context of the questions
[00:20:31.500 --> 00:20:35.780]   It can ask another questions or it can actually give the response to that questions
[00:20:35.780 --> 00:20:39.500]   So that was the next phase is about when you fight to it for dialogue
[00:20:39.500 --> 00:20:48.200]   And the last phase is reinforcement learning from reinforcement learning when you want to train the model to to act according to human preference
[00:20:48.200 --> 00:20:56.860]   Okay, so so I hope that we get a good understanding of the models architectures and there's a stack and how to create a model and
[00:20:56.860 --> 00:20:58.340]   Get the base models that we want
[00:20:58.340 --> 00:21:02.420]   And I think the next phase that we see I see a lot of companies go into is like
[00:21:02.420 --> 00:21:07.060]   Deciding on like what model she used and one big question is choosing model size
[00:21:07.060 --> 00:21:13.980]   So I talked a lot of companies and when we choosing a model we probably want to balance between cost and performance
[00:21:13.980 --> 00:21:20.620]   So I have seen that the sweet spot for it a lot of companies is that like they choose a model between five and thirteen
[00:21:20.620 --> 00:21:25.000]   Billion parameters it can still give like not the best possible performance
[00:21:25.000 --> 00:21:28.620]   But it's good enough for a lot of use cases and it's still cheap enough
[00:21:28.620 --> 00:21:33.940]   So for example, like if you want to go into like more fighting specific tasks
[00:21:33.940 --> 00:21:37.740]   Then you can pretty get away with something on the lower size for some of three billion
[00:21:37.740 --> 00:21:42.640]   But if you want some more general model like a chatbot that can respond to any kind of questions
[00:21:42.640 --> 00:21:45.220]   Then you might want to go into a larger model size
[00:21:45.220 --> 00:21:47.340]   and there's something nice about being a
[00:21:47.940 --> 00:21:54.040]   7 billion is that like if you quantize that to like before 16 or even like in 8 a bit
[00:21:54.040 --> 00:22:00.040]   Then you can still be small enough to run on a local MacBook or like you can print a car on different devices
[00:22:00.040 --> 00:22:08.160]   Okay, so I think the last phase is like this is a very short slide because I do think that evaluation right now is a
[00:22:08.160 --> 00:22:11.360]   Big mess and I don't think we quite know what is happening yet
[00:22:11.360 --> 00:22:18.100]   I think it's expected because if a standardized evaluations is always catching up with actual use cases
[00:22:18.100 --> 00:22:23.540]   And as we see that the people are still freaker are still trying to figure out the best use cases
[00:22:23.540 --> 00:22:28.940]   We're not quite sure what metric is the best you use so I would imagine it could be something like ML perv
[00:22:28.940 --> 00:22:32.960]   So ML perv like when you measure hardware performance
[00:22:32.960 --> 00:22:38.940]   But then you divide them into a different model types different use cases and have different little board for different
[00:22:39.260 --> 00:22:43.540]   Model type and and use cases I would imagine something similar for LLM
[00:22:43.540 --> 00:22:49.700]   So whenever it LLMs, I was also reiterate is the same things have been talking about in for machine learning
[00:22:49.700 --> 00:22:54.380]   It's just like model performance has to be tied to business metrics and you
[00:22:54.380 --> 00:22:58.820]   Definitely would need to build some into a test set so you can build trust in your model
[00:22:58.820 --> 00:23:02.460]   Okay, so here's a takeaway some of the talk
[00:23:02.460 --> 00:23:09.100]   It's just like you set concrete goals have expectations and I think that data story is more important now
[00:23:09.100 --> 00:23:15.500]   Than ever invest in things at last blah blah blah and the most important point that I hope that you have fun
[00:23:15.500 --> 00:23:18.220]   I do things I believe in a very exciting time
[00:23:18.220 --> 00:23:23.620]   There's so many many fun use cases and I know that it feels overwhelming sometimes
[00:23:23.620 --> 00:23:29.780]   But it's a really exciting to go out there and talk to people and see how they'd learn how they think about the space
[00:23:29.780 --> 00:23:31.780]   So yeah, I hope they have fun with
[00:23:31.780 --> 00:23:33.940]   exploring LLM
[00:23:33.940 --> 00:23:35.940]   Thank you so much
[00:23:35.940 --> 00:23:37.940]   [Applause]
[00:23:38.500 --> 00:23:40.500]   [Music]
[00:23:40.500 --> 00:23:43.080]   (upbeat music)
[00:23:43.080 --> 00:23:53.080]   [BLANK_AUDIO]

