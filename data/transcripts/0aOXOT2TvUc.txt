
[00:00:00.000 --> 00:00:07.000]   Astronomy and physics works in a world that's sensor-based, fundamentally, in terms of our
[00:00:07.000 --> 00:00:08.000]   observations.
[00:00:08.000 --> 00:00:09.960]   Because it's sensor-based, there's noise.
[00:00:09.960 --> 00:00:17.460]   So unlike in the kind of AlphaGo, Atari world, where every pixel has a perfect measurement,
[00:00:17.460 --> 00:00:22.720]   if you take an image of the sky or you measure some time series, there's noise associated
[00:00:22.720 --> 00:00:23.720]   with it.
[00:00:23.720 --> 00:00:28.520]   And because there's noise, and because there's a finite amount of training data, if you build
[00:00:28.520 --> 00:00:34.000]   models off of that, you get uncertainties in the models because of its lack of expressiveness
[00:00:34.000 --> 00:00:37.400]   or its overgeneralization or overfitting.
[00:00:37.400 --> 00:00:42.720]   And then you also have a source of uncertainty in what it is that you're trying to understand
[00:00:42.720 --> 00:00:45.800]   just because, fundamentally, you don't have a perfect measurement.
[00:00:45.800 --> 00:00:47.400]   Your signal noise isn't perfect.
[00:00:47.400 --> 00:00:51.780]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:51.780 --> 00:00:54.160]   and I'm your host, Lucas Biewald.
[00:00:54.160 --> 00:01:00.000]   Today I'm talking to Josh Bloom, who is the chair of the UC Berkeley astronomy department.
[00:01:00.000 --> 00:01:07.200]   And astronomy has been the source of many innovations in data and machine learning,
[00:01:07.200 --> 00:01:10.400]   and it's also changed a lot due to machine learning.
[00:01:10.400 --> 00:01:14.560]   And so I'm really excited to talk to him about astronomy in general, but also how machine
[00:01:14.560 --> 00:01:16.520]   learning has affected the field.
[00:01:16.520 --> 00:01:19.360]   Josh, so thanks so much for doing this.
[00:01:19.360 --> 00:01:25.440]   I have so many questions about astronomy in general as someone interested in it, but not
[00:01:25.440 --> 00:01:26.560]   very knowledgeable about it.
[00:01:26.560 --> 00:01:29.760]   I'm going to try to control myself from just going down that path.
[00:01:29.760 --> 00:01:37.080]   But one thing I was thinking about is it seems like astronomy has informed, or ex-astronomers
[00:01:37.080 --> 00:01:39.480]   have done so much interesting work in machine learning.
[00:01:39.480 --> 00:01:43.920]   I was wondering if you have any thoughts on why that is, why there's such a path from
[00:01:43.920 --> 00:01:45.880]   astronomy into machine learning.
[00:01:45.880 --> 00:01:49.840]   I feel like it must have something to do with the large datasets that you all deal with,
[00:01:49.840 --> 00:01:50.840]   but is there something there?
[00:01:50.840 --> 00:01:54.720]   I mean, even you went into a startup at some point and kind of came back into the field,
[00:01:54.720 --> 00:01:55.720]   right?
[00:01:55.720 --> 00:01:56.720]   Yeah.
[00:01:56.720 --> 00:02:01.800]   I mean, the way I'd put it this way is that astronomers are quite good at using and co-opting
[00:02:01.800 --> 00:02:06.440]   tools that are built elsewhere to get our work done.
[00:02:06.440 --> 00:02:11.680]   And maybe the most famous example is this guy named Galileo who heard about this thing
[00:02:11.680 --> 00:02:12.680]   called the telescope.
[00:02:12.680 --> 00:02:17.640]   And instead of pointing at the horizon, looking for enemy ships, he sort of pointed up that
[00:02:17.640 --> 00:02:18.840]   way.
[00:02:18.840 --> 00:02:20.640]   And the rest is history.
[00:02:20.640 --> 00:02:26.240]   We have been co-opting tools for centuries for our own benefit.
[00:02:26.240 --> 00:02:33.040]   And partly that's because I think astronomers are naturally curious people, but also because
[00:02:33.040 --> 00:02:35.000]   we're looking for an edge fundamentally.
[00:02:35.000 --> 00:02:42.320]   We are often working right at the limit of where there's an obvious answer, where you
[00:02:42.320 --> 00:02:46.280]   have a lot of data and it's high signal noise to where it's just complete noise.
[00:02:46.280 --> 00:02:51.640]   And the real discoveries are happening essentially at the five sigma level.
[00:02:51.640 --> 00:02:58.520]   So we are incentivized in many ways to pull in all these different tools and toolkits
[00:02:58.520 --> 00:03:00.840]   from all over the place.
[00:03:00.840 --> 00:03:05.160]   Astronomers obviously aren't just using these tools.
[00:03:05.160 --> 00:03:11.960]   We're using a whole bunch of kind of inference techniques and problem solving skills in a
[00:03:11.960 --> 00:03:17.960]   way that I think it becomes very valuable outside of the specific questions that we
[00:03:17.960 --> 00:03:19.040]   ask.
[00:03:19.040 --> 00:03:25.720]   So for sure, when I started a company in the machine learning space, and we can talk about
[00:03:25.720 --> 00:03:32.120]   the origin story of that if you're interested in how we sort of came to ML, we started hiring.
[00:03:32.120 --> 00:03:36.960]   And while we certainly weren't looking to hire people that had a similar background
[00:03:36.960 --> 00:03:43.360]   to us, oftentimes when we got into coding exercises and we got into solving problems,
[00:03:43.360 --> 00:03:48.600]   a lot of the people that were making it through that we were excited about had a physics,
[00:03:48.600 --> 00:03:51.440]   more broadly an astronomy background.
[00:03:51.440 --> 00:03:56.120]   And there were people that could work with something that they had potentially never
[00:03:56.120 --> 00:04:01.520]   seen before, analyze it in a way an engineer might to sort of get it down to its constituent
[00:04:01.520 --> 00:04:04.160]   parts and then innovate on top of that.
[00:04:04.160 --> 00:04:05.160]   But I think you're right.
[00:04:05.160 --> 00:04:10.800]   The other big component, at least in these days, is the availability of just so much
[00:04:10.800 --> 00:04:17.900]   data and our need to do something with that data in real time with limited resources is
[00:04:17.900 --> 00:04:21.800]   a natural entree into where machine learning comes in.
[00:04:21.800 --> 00:04:26.680]   From your perspective, what do you feel like the big interesting questions are right now
[00:04:26.680 --> 00:04:27.680]   in astronomy?
[00:04:27.680 --> 00:04:33.120]   What do you feel like we might learn in the next, I don't know, a couple of decades that
[00:04:33.120 --> 00:04:36.920]   would really kind of change the field?
[00:04:36.920 --> 00:04:38.360]   Well it's all over the place.
[00:04:38.360 --> 00:04:43.960]   First of all, one way to think about astronomy is as a great laboratory for physics.
[00:04:43.960 --> 00:04:49.320]   So if we start there, and I think it's maybe somewhat apocryphal, but Einstein really didn't
[00:04:49.320 --> 00:04:53.680]   like astronomers, but it turns out most tests of general relativity happen in the astronomy
[00:04:53.680 --> 00:04:54.680]   context.
[00:04:54.680 --> 00:04:58.560]   There are many extraterrestrial ways in which we can test GR, but most of the really interesting
[00:04:58.560 --> 00:05:04.440]   tests of GR these days and has been for a hundred years is by looking at the skies and
[00:05:04.440 --> 00:05:10.360]   specific events and specific large scale structure of the skies gives us clues into some of the
[00:05:10.360 --> 00:05:18.520]   very basics of how the universe works at a, not just global scale, but at a microscopic
[00:05:18.520 --> 00:05:19.520]   scale.
[00:05:19.520 --> 00:05:26.080]   And we're also testing our understanding of how atoms work and understanding even what's
[00:05:26.080 --> 00:05:32.720]   going inside of the nucleus of atoms by looking at what happens on extremely large scales,
[00:05:32.720 --> 00:05:35.500]   which is just mind blowing to think about.
[00:05:35.500 --> 00:05:41.720]   So if we think about astronomy as that laboratory for physics, another way to ask that question
[00:05:41.720 --> 00:05:46.600]   would be what are the really important physics questions that we have?
[00:05:46.600 --> 00:05:52.840]   One is what is the nature of matter at extremely high densities and temperatures beyond nuclear
[00:05:52.840 --> 00:05:53.840]   density?
[00:05:53.840 --> 00:05:58.600]   So we have objects like neutron stars, which are extremely compact stars that have the
[00:05:58.600 --> 00:06:03.960]   same mass thereabouts of our sun, but are the size of San Francisco.
[00:06:03.960 --> 00:06:08.300]   And so that, at that density, we can't reproduce that in the lab.
[00:06:08.300 --> 00:06:13.320]   We need to look at how those stars behave when matter pinches upon them, or just even
[00:06:13.320 --> 00:06:20.540]   what their static distributions are in radius and mass to learn something about what's happening
[00:06:20.540 --> 00:06:24.600]   with nuclear matter at those really high densities.
[00:06:24.600 --> 00:06:27.560]   We don't know whether general relativity is right.
[00:06:27.560 --> 00:06:32.840]   It looks like it's really, really good on a lot of different scales and a lot of different
[00:06:32.840 --> 00:06:36.000]   mass scales and a lot of different length scales.
[00:06:36.000 --> 00:06:41.740]   But we're constantly testing this hypothesis that is general relativity of whether it is
[00:06:41.740 --> 00:06:47.080]   a perfect description of how matter moves in the universe and how the universe is shaped
[00:06:47.080 --> 00:06:48.440]   by matter.
[00:06:48.440 --> 00:06:52.840]   We know it can't be perfect because it breaks down at the quantum mechanical scale.
[00:06:52.840 --> 00:06:57.960]   And there are things that happen in astronomy that allow us to test some fundamental precepts
[00:06:57.960 --> 00:07:00.600]   and hypotheses that come out of general relativity.
[00:07:00.600 --> 00:07:06.740]   So in the gravitational wave world, which is essentially the ripples of space-time due
[00:07:06.740 --> 00:07:12.200]   to the changing locations of matter around other pieces of matter, we've had massive
[00:07:12.200 --> 00:07:17.440]   breakthroughs in just the last couple of years, observationally, where we've seen the inspirals
[00:07:17.440 --> 00:07:22.560]   of black holes and potentially the inspiral of neutron stars smashing into each other.
[00:07:22.560 --> 00:07:27.320]   In the last few seconds, there's a huge burst of gravitational wave energy, which we can
[00:07:27.320 --> 00:07:28.680]   detect on earth.
[00:07:28.680 --> 00:07:32.920]   But we can also now start to see glimmers of the idea that we can start testing some
[00:07:32.920 --> 00:07:37.600]   basic ideas of general relativity in those last even milliseconds.
[00:07:37.600 --> 00:07:43.920]   So as instrumentation gets better there, I suspect our understanding of where GR is working
[00:07:43.920 --> 00:07:48.920]   and where it potentially breaks down will become really interesting.
[00:07:48.920 --> 00:07:55.520]   We're also interested on cosmological scales in understanding the expansion history of
[00:07:55.520 --> 00:07:57.840]   the universe, the origin of the universe.
[00:07:57.840 --> 00:08:04.160]   Why did the universe appear to inflate and exponentiate so rapidly in just even less
[00:08:04.160 --> 00:08:08.280]   than a millisecond, the 10 to the minus 43 kind of seconds?
[00:08:08.280 --> 00:08:10.160]   Why did it grow so quickly?
[00:08:10.160 --> 00:08:14.360]   We know it had to based on observations at later times.
[00:08:14.360 --> 00:08:20.860]   And what's absolutely remarkable right now is that when we look at the constituent parts
[00:08:20.860 --> 00:08:26.500]   that drives the dynamics of how the universe we think changes, as in how fast it grows
[00:08:26.500 --> 00:08:32.300]   and how fast it appears to be accelerating in its growth, ordinary matters, I'm sure
[00:08:32.300 --> 00:08:38.560]   you and your listeners know very well, makes up only a few percent of that recipe.
[00:08:38.560 --> 00:08:41.560]   Dark matter is of order, you know, a quarter of it.
[00:08:41.560 --> 00:08:46.000]   And then dark energy is the other part of it.
[00:08:46.000 --> 00:08:48.860]   We really don't know anything about dark energy.
[00:08:48.860 --> 00:08:50.440]   We don't know whether it's a particle.
[00:08:50.440 --> 00:08:54.360]   We don't know whether it's something even deeper than that.
[00:08:54.360 --> 00:09:00.680]   We don't know whether dark matter is a particle on a tiny scale that isn't predicted by the
[00:09:00.680 --> 00:09:07.440]   standard theory or whether it's large clumps of black holes that were left over after the
[00:09:07.440 --> 00:09:10.080]   primordial expansion of the universe.
[00:09:10.080 --> 00:09:15.400]   So the biggest breakthroughs may come in a deep and fundamental understanding of what
[00:09:15.400 --> 00:09:17.160]   are those constituent parts.
[00:09:17.160 --> 00:09:23.520]   It may also come with a recognition that the framework that we have for understanding how
[00:09:23.520 --> 00:09:26.760]   the universe unfolds is right now fundamentally wrong.
[00:09:26.760 --> 00:09:31.920]   And we'll look back on this in a couple of decades and say, boy, we were only looking
[00:09:31.920 --> 00:09:33.760]   at just part of the elephant.
[00:09:33.760 --> 00:09:38.240]   And now when we have a bigger picture of it, you know, things become more clear.
[00:09:38.240 --> 00:09:39.240]   There's more, obviously.
[00:09:39.240 --> 00:09:43.520]   And the last thing I'll just say, because I'd be remiss not to, is understanding the
[00:09:43.520 --> 00:09:49.640]   origins of life and the prevalence of planets that can sustain life outside of our solar
[00:09:49.640 --> 00:09:51.040]   system.
[00:09:51.040 --> 00:09:57.120]   There's a huge push both at Berkeley, where I am, and then across the world in building
[00:09:57.120 --> 00:10:03.800]   new instrumentation and new theory that helps us understand how planets evolve, where habitable
[00:10:03.800 --> 00:10:08.560]   planets could be around sun-like stars, and how we're actually going to find them, characterize
[00:10:08.560 --> 00:10:13.080]   them and potentially even understand what potentially primitive forms of life there
[00:10:13.080 --> 00:10:14.080]   are in those atmospheres.
[00:10:14.080 --> 00:10:18.840]   So I have a feeling it's probably an annoying question, but it comes up a lot when I talk
[00:10:18.840 --> 00:10:23.480]   to ML people just in casual conversation who don't really know about astronomy.
[00:10:23.480 --> 00:10:28.160]   So I'll just ask it because I hear it a lot and I'm kind of curious.
[00:10:28.160 --> 00:10:35.040]   I guess when I hear about dark energy and dark matter, I kind of wonder, is that just
[00:10:35.040 --> 00:10:40.860]   sort of like a fudge factor that shows that we don't really understand what the physical
[00:10:40.860 --> 00:10:42.040]   laws of the universe are?
[00:10:42.040 --> 00:10:45.880]   Is there a reason to call it matter and energy?
[00:10:45.880 --> 00:10:48.720]   Is there some sense that you're sure that it is matter?
[00:10:48.720 --> 00:10:50.960]   There are kind of two fudge factors, right?
[00:10:50.960 --> 00:10:54.240]   Fudge factor A, which we'll call dark matter, and fudge factor B, which we'll call dark
[00:10:54.240 --> 00:10:55.440]   energy.
[00:10:55.440 --> 00:11:01.160]   Dark matter is much better understood in how it behaves than dark energy.
[00:11:01.160 --> 00:11:05.560]   There's a lot of evidence that this stuff actually exists.
[00:11:05.560 --> 00:11:10.560]   I won't go into all the details here, but on many different scales, we have observational
[00:11:10.560 --> 00:11:16.320]   evidence that shows that while there are some people in the theory world that feel like
[00:11:16.320 --> 00:11:21.280]   they can explain away some pieces of that evidence, there is no successful alternative
[00:11:21.280 --> 00:11:27.360]   theory for explaining away this fudge factor with just sort of a different way of thinking
[00:11:27.360 --> 00:11:28.600]   about the universe.
[00:11:28.600 --> 00:11:30.280]   It looks like it's actually stuff.
[00:11:30.280 --> 00:11:36.120]   We know it interacts gravitationally, and we hope that it interacts weakly in other
[00:11:36.120 --> 00:11:37.120]   ways.
[00:11:37.120 --> 00:11:42.400]   There are lots of endeavors actually looking for dark matter within a lab or within a cave,
[00:11:42.400 --> 00:11:47.880]   and there's some other ideas of how astronomers could actually find the details of how dark
[00:11:47.880 --> 00:11:51.360]   matter interacts with itself and maybe with ordinary matter.
[00:11:51.360 --> 00:11:56.360]   So yes, it's a fudge factor in some sense to explain the overall evolution of the universe,
[00:11:56.360 --> 00:12:03.360]   but it was originally discovered to explain the anomalously fast motions of galaxies in
[00:12:03.360 --> 00:12:05.200]   clusters of galaxies.
[00:12:05.200 --> 00:12:09.720]   You just sort of add up the total mass associated with the light of galaxies, because we know
[00:12:09.720 --> 00:12:16.360]   how to roughly map the light of a star to its rough mass and the distribution thereof.
[00:12:16.360 --> 00:12:17.720]   There just wasn't enough mass.
[00:12:17.720 --> 00:12:20.520]   So there was this missing mass that's associated with galaxies.
[00:12:20.520 --> 00:12:24.760]   It turns out there's also missing mass associated with our own galaxy.
[00:12:24.760 --> 00:12:28.660]   And we've been able to systematically rule out sort of ordinary matter like electrons
[00:12:28.660 --> 00:12:35.600]   and protons, but I think the best bet is that it is some other series of particles that
[00:12:35.600 --> 00:12:39.440]   we haven't yet envisioned, but one day we may be able to find.
[00:12:39.440 --> 00:12:44.000]   Do you know its distribution on a scale like a solar system?
[00:12:44.000 --> 00:12:48.760]   Can you tell where it would be from gravitational effects?
[00:12:48.760 --> 00:12:53.440]   It sounds like it follows a similar distribution to matter we can observe on a galaxy.
[00:12:53.440 --> 00:12:54.440]   That's right.
[00:12:54.440 --> 00:12:58.800]   Yeah, but it actually, we think within our own galaxy, the dark matter, which is all
[00:12:58.800 --> 00:13:05.320]   around us, either as essentially a fluid, so there's particles of dark matter running
[00:13:05.320 --> 00:13:11.360]   through you all the time, or in like extreme clumps in the form of primordial black holes,
[00:13:11.360 --> 00:13:17.120]   that's the other extreme, that have the mass of a comet or a mountain.
[00:13:17.120 --> 00:13:20.560]   There'd be dozens of those flying through our solar system.
[00:13:20.560 --> 00:13:24.640]   There are potential ways in which we could actually discover these dark clumps.
[00:13:24.640 --> 00:13:29.300]   And we have a whole series of observations looking for the particle version of that.
[00:13:29.300 --> 00:13:35.000]   It behaves a lot like ordinary matter, but in our own galaxy, while gas and stars, at
[00:13:35.000 --> 00:13:40.800]   least in the local solar neighborhood, are moving of order, something like 200 kilometers
[00:13:40.800 --> 00:13:47.720]   a second around the galaxy, we think that there is a fluid or these large clumps of
[00:13:47.720 --> 00:13:51.720]   it which are moving in slightly different ways than the ordinary matter.
[00:13:51.720 --> 00:13:55.560]   So the ordinary matter and the dark matter, by definition of the gravitational interactions,
[00:13:55.560 --> 00:13:59.120]   actually do talk to each other and they do influence each other.
[00:13:59.120 --> 00:14:03.000]   But because the dark matter is sort of non-compressive and unlike gas, when you smash it together
[00:14:03.000 --> 00:14:10.240]   and you get heat, this fluid sort of sloshes around back and forth.
[00:14:10.240 --> 00:14:15.200]   I don't know of a way in which we'd be able to detect the amount of dark matter that we
[00:14:15.200 --> 00:14:19.200]   think must be, let's say, in the sun, because there's almost certainly some amount of dark
[00:14:19.200 --> 00:14:20.640]   matter that's been captured in the sun.
[00:14:20.640 --> 00:14:26.440]   It's such a small fraction compared to ordinary matter around us that there are plenty of
[00:14:26.440 --> 00:14:30.080]   ways in which it could be hiding in plain sight.
[00:14:30.080 --> 00:14:36.560]   On the dark energy side, that is very much more of a fudge factor to explain the dynamics
[00:14:36.560 --> 00:14:39.840]   of how the universe expands.
[00:14:39.840 --> 00:14:45.280]   And in fact, again, going back to Einstein, when he was working out some of the dynamics
[00:14:45.280 --> 00:14:49.640]   of the universe, he had this thing that he called his biggest blunder, which was coming
[00:14:49.640 --> 00:14:53.960]   up with this fudge factor constant to sort of make the left-hand side and the right-hand
[00:14:53.960 --> 00:14:55.520]   side of the equation work.
[00:14:55.520 --> 00:15:00.480]   And then when it was found in the '30s and '40s and '50s that there wasn't any of this
[00:15:00.480 --> 00:15:05.040]   accelerating expansion, he thought it was a big blunder, but ironically, we actually
[00:15:05.040 --> 00:15:06.560]   needed that fudge factor.
[00:15:06.560 --> 00:15:09.960]   What's interesting is that we have that as a constant.
[00:15:09.960 --> 00:15:15.240]   So that is, it's got a constant amount of energy per unit volume.
[00:15:15.240 --> 00:15:17.400]   That's sort of the simplest way to think about it.
[00:15:17.400 --> 00:15:19.720]   But we also don't know whether it's constant in time.
[00:15:19.720 --> 00:15:22.560]   It could actually be changing its constant.
[00:15:22.560 --> 00:15:23.560]   So there could be a temporal dynamic.
[00:15:23.560 --> 00:15:26.280]   Do you see that in different rates of expansion?
[00:15:26.280 --> 00:15:27.280]   Yeah.
[00:15:27.280 --> 00:15:28.440]   So there'd be different rates.
[00:15:28.440 --> 00:15:33.400]   There's already different rates of expansion just because in the early history of the universe,
[00:15:33.400 --> 00:15:39.480]   ordinary matter and dark matter sort of dominated the expansion as in it was sort of slowing
[00:15:39.480 --> 00:15:40.480]   up.
[00:15:40.480 --> 00:15:46.840]   And then as the universe became more tenuous and this material basically lost its dominance
[00:15:46.840 --> 00:15:53.200]   in these equations, there was a time several billion years ago when dark energy sort of
[00:15:53.200 --> 00:15:55.920]   took over and is now the thing driving the dynamic.
[00:15:55.920 --> 00:16:02.000]   So if dark energy is a constant and we've measured it well enough, then the universe
[00:16:02.000 --> 00:16:05.800]   will just sort of continue to exponentiate and just grow.
[00:16:05.800 --> 00:16:10.720]   And there'll be sort of this big, it won't be exactly an evaporation, but it'll be called
[00:16:10.720 --> 00:16:11.720]   the big rip.
[00:16:11.720 --> 00:16:16.280]   We'll basically all just rip apart from each other and cosmology in the next hundred billion
[00:16:16.280 --> 00:16:22.440]   years will stop being about the observations of 40 billion galaxies and turn in just observations
[00:16:22.440 --> 00:16:26.360]   of stuff in our solar neighborhood because all the other galaxies will run far away from
[00:16:26.360 --> 00:16:27.360]   us.
[00:16:27.360 --> 00:16:29.240]   But we don't know that that's the case.
[00:16:29.240 --> 00:16:32.160]   It could actually, that constant could turn off for some reason.
[00:16:32.160 --> 00:16:35.560]   It could have other terms that haven't yet expressed themselves.
[00:16:35.560 --> 00:16:36.560]   Cool.
[00:16:36.560 --> 00:16:37.560]   Well, okay.
[00:16:37.560 --> 00:16:42.260]   So I have to also ask you about the other topic that you brought up on finding so many
[00:16:42.260 --> 00:16:48.160]   habitable or seemingly possibly habitable planets in the universe, or at least in our,
[00:16:48.160 --> 00:16:49.160]   that we can see.
[00:16:49.160 --> 00:16:54.320]   Do you have any kind of thoughts on that or are there theories why we don't see life on,
[00:16:54.320 --> 00:16:58.280]   if there's so many planets out there, why we don't see other life?
[00:16:58.280 --> 00:17:05.360]   Well, I think we know now that life, at least intelligent life is not teeming, right?
[00:17:05.360 --> 00:17:10.760]   And Enrico Fermi had the Fermi paradox, the idea of life is so ubiquitous, why aren't
[00:17:10.760 --> 00:17:11.760]   they all around?
[00:17:11.760 --> 00:17:16.920]   It's pretty clear that it's not as ubiquitous as every solar system has intelligent life.
[00:17:16.920 --> 00:17:18.440]   That's not a big surprise.
[00:17:18.440 --> 00:17:26.920]   What we haven't yet nailed down in the overall demographics is what is the exact set of conditions
[00:17:26.920 --> 00:17:29.880]   that could give rise to any sort of life.
[00:17:29.880 --> 00:17:35.400]   We have a reasonable understanding now that of order one solar system around a sun like
[00:17:35.400 --> 00:17:39.180]   star will have of order one habitable planet.
[00:17:39.180 --> 00:17:43.680]   Maybe it's two or maybe it's a half, but it's not zero and it's not 10.
[00:17:43.680 --> 00:17:49.000]   And then getting into the actual chemistry of what leads to and biology of what leads
[00:17:49.000 --> 00:17:54.440]   to life that's sustainable, that's really kind of where the cutting edge questions are
[00:17:54.440 --> 00:17:55.720]   on the theory side.
[00:17:55.720 --> 00:18:00.160]   And obviously we have some great laboratories in our own solar system to ask those questions,
[00:18:00.160 --> 00:18:02.000]   form of atmospheres of other planets.
[00:18:02.000 --> 00:18:07.120]   And we're just now entering an era where we have sensitive enough equipment to be able
[00:18:07.120 --> 00:18:13.760]   to measure detailed chemical properties of atmospheres of other planets and other solar
[00:18:13.760 --> 00:18:14.760]   systems.
[00:18:14.760 --> 00:18:20.000]   And what I think will become clear over the next, let's say two decades is exactly what
[00:18:20.000 --> 00:18:27.400]   the rate is of planets that are in habitable zones around their sun like stars that appear
[00:18:27.400 --> 00:18:30.280]   to be in some sort of disequilibrium.
[00:18:30.280 --> 00:18:35.960]   When you look at the overall chemistry of, and the temperature profile of those atmospheres,
[00:18:35.960 --> 00:18:41.440]   how is it that we have something that is a volatile element that is still around?
[00:18:41.440 --> 00:18:44.520]   It means that there's something else on the surface that's producing it.
[00:18:44.520 --> 00:18:46.360]   It won't guarantee that it's life.
[00:18:46.360 --> 00:18:52.000]   The question about finding other intelligent life that we could potentially interact with
[00:18:52.000 --> 00:18:56.640]   in some sense is beyond the horizon of modern astronomy.
[00:18:56.640 --> 00:19:02.320]   But there are groups, as you know, that are using modern astronomy tools to do those sorts
[00:19:02.320 --> 00:19:03.320]   of searches.
[00:19:03.320 --> 00:19:08.120]   And when you say disequilibrium, is that something that we would notice about earth if we were
[00:19:08.120 --> 00:19:10.720]   far away from it and looking at it?
[00:19:10.720 --> 00:19:15.920]   Yeah, it's a little bit outside of my field, but if you took a spectrum of the earth's
[00:19:15.920 --> 00:19:19.160]   atmosphere, and people have done this by looking at the earth's shine.
[00:19:19.160 --> 00:19:23.520]   So right around the time of the crescent moon, as it's setting right after the sunset, you
[00:19:23.520 --> 00:19:28.160]   often can see the unilluminated part of the moon.
[00:19:28.160 --> 00:19:32.880]   And that's because what you're seeing is the sun's light reflecting off of the earth's
[00:19:32.880 --> 00:19:35.920]   atmosphere, bouncing off the moon, and then coming back to your eyes.
[00:19:35.920 --> 00:19:41.440]   You can take a spectrum of that earth's shine, and there are signatures in that, that if
[00:19:41.440 --> 00:19:47.000]   we saw that in other planets, we would say, "Aha, there's something that," and I don't
[00:19:47.000 --> 00:19:52.520]   know the details of which element does what, "that is not in an equilibrium given the temperature
[00:19:52.520 --> 00:19:53.520]   of the earth."
[00:19:53.520 --> 00:19:54.520]   Oh, cool.
[00:19:54.520 --> 00:19:55.520]   Interesting.
[00:19:55.880 --> 00:20:01.840]   It seems like astronomy has made so many advances in my lifetime, which is so cool.
[00:20:01.840 --> 00:20:10.280]   Do you think that that's mostly due to better equipment to see more, or do you think it's
[00:20:10.280 --> 00:20:15.400]   better use or figuring things out from what we're seeing?
[00:20:15.400 --> 00:20:21.120]   I guess it must be both, but some of the astronomy experiments that I read about just seem totally
[00:20:21.120 --> 00:20:23.800]   brilliant of synthesizing.
[00:20:23.800 --> 00:20:27.880]   It seems like we get one snapshot of the world around us, and it's incredible to me how much
[00:20:27.880 --> 00:20:33.120]   physics, or how many things we discover from just looking up into space.
[00:20:33.120 --> 00:20:39.640]   Well, I think a big part of that is indeed the 20th century was the opening of our eyes
[00:20:39.640 --> 00:20:42.520]   beyond visible wavelengths.
[00:20:42.520 --> 00:20:48.480]   And X-ray astronomy really only started in the '50s, gamma-ray astronomy around that
[00:20:48.480 --> 00:20:49.480]   time as well.
[00:20:49.480 --> 00:20:53.000]   Once we get above the earth's atmosphere, which absorbs a lot of the light, thankfully,
[00:20:53.000 --> 00:20:59.520]   at other wavebands, we just see a whole universe that we either didn't imagine or only had
[00:20:59.520 --> 00:21:01.680]   a vague idea could be out there.
[00:21:01.680 --> 00:21:08.160]   So a big part of the 20th century was just opening up our eyes to new wavebands and understanding
[00:21:08.160 --> 00:21:13.800]   the connection between different objects and events, like supernovae, how they are connected
[00:21:13.800 --> 00:21:18.520]   to each other across different wavebands, and what their role is in driving the dynamics
[00:21:18.520 --> 00:21:24.160]   of a specific galaxy, and what their role is in the creation of elements.
[00:21:24.160 --> 00:21:29.120]   We didn't really even know how to ask those questions, I think, properly until the last
[00:21:29.120 --> 00:21:30.120]   several decades.
[00:21:30.120 --> 00:21:35.720]   So part of it's that, and that opening of the eyes is driven a lot by technology.
[00:21:35.720 --> 00:21:39.640]   But then it's, "Okay, well, I have my eyes open, but they're blurry, so how do I sharpen
[00:21:39.640 --> 00:21:40.640]   them?"
[00:21:40.640 --> 00:21:44.280]   There are plenty of things, again, back to the original conversation at the beginning
[00:21:44.280 --> 00:21:47.880]   around co-opting tools.
[00:21:47.880 --> 00:21:53.040]   Researchers learned about adaptive optics being used for military purposes, and were
[00:21:53.040 --> 00:21:58.760]   able to get much clearer images of the sky because we're now pointing lasers up into
[00:21:58.760 --> 00:22:02.760]   the atmosphere and exciting a sodium layer high up in the atmosphere, which acts as a
[00:22:02.760 --> 00:22:03.760]   temporary star.
[00:22:03.760 --> 00:22:10.880]   And we have corrective lenses that, at many, many times a second, are correcting the waveform
[00:22:10.880 --> 00:22:15.720]   errors that come as the star's light comes through the atmosphere and gets blurred.
[00:22:15.720 --> 00:22:17.680]   So we have all that kind of technology.
[00:22:17.680 --> 00:22:23.520]   Of course, digital technology starting in the early 1980s meant we're taking very high
[00:22:23.520 --> 00:22:28.100]   dynamic range images of the same parts of the sky we were looking at before.
[00:22:28.100 --> 00:22:33.080]   So we were able to see farther away, see fainter objects.
[00:22:33.080 --> 00:22:36.800]   And at the same time, there were a number of innovations in the telescope world, even
[00:22:36.800 --> 00:22:40.560]   on the ground, that allowed us to build bigger and bigger telescopes.
[00:22:40.560 --> 00:22:45.720]   In the end, we haven't gone that far from Galileo's telescope to the world's largest
[00:22:45.720 --> 00:22:48.000]   telescopes, 10-meter class telescopes today.
[00:22:48.000 --> 00:22:50.300]   That's just bigger and bigger collecting light.
[00:22:50.300 --> 00:22:54.680]   But the innovations that it's taken for us to get there have been real and have been
[00:22:54.680 --> 00:23:01.920]   driven by the need for seeing fainter objects, seeing with greater clarity, seeing across
[00:23:01.920 --> 00:23:03.640]   more wave bands.
[00:23:03.640 --> 00:23:07.720]   Some of the biggest discoveries in some sense happened outside of the electromagnetic band
[00:23:07.720 --> 00:23:09.180]   over the last many years.
[00:23:09.180 --> 00:23:14.620]   The observations of very high energy cosmic rays, so these very high energy charged particles
[00:23:14.620 --> 00:23:22.320]   moving very close to the speed of light, understanding the origins of those is still an active topic.
[00:23:22.320 --> 00:23:28.280]   And the discovery of gravitational waves directly using interferometers on the ground is a massive
[00:23:28.280 --> 00:23:34.840]   innovation that took arguably 40 years for us to get there technologically and several
[00:23:34.840 --> 00:23:40.040]   billion dollars of taxpayer money that went into that.
[00:23:40.040 --> 00:23:44.580]   And it took a large number of people to be very, very convinced that the physics was
[00:23:44.580 --> 00:23:46.740]   right and they'd be able to get there.
[00:23:46.740 --> 00:23:51.520]   And so the fact that they were is one of the great sort of crowning achievements of our
[00:23:51.520 --> 00:23:57.940]   field is a recognition that driven by theory, we were able to invest billions of dollars
[00:23:57.940 --> 00:24:02.700]   to get to a set of discoveries that we could have only dreamed of 10 years ago.
[00:24:02.700 --> 00:24:09.140]   Do you think that gravitational wave sensor was more of an engineering feat?
[00:24:09.140 --> 00:24:13.360]   Because it just seems so incredible to be able to send something so small, or was it
[00:24:13.360 --> 00:24:14.360]   more of like a theory?
[00:24:14.360 --> 00:24:18.640]   I guess, what was the hardest part of that?
[00:24:18.640 --> 00:24:25.600]   Well, there are early days that predated me where theorists were in active discussion
[00:24:25.600 --> 00:24:31.560]   about whether you could even use these so-called interferometers with lasers to look for this
[00:24:31.560 --> 00:24:32.800]   deformation.
[00:24:32.800 --> 00:24:37.320]   And once people became convinced that the theory was right, you're exactly right.
[00:24:37.320 --> 00:24:43.720]   This became an engineering feat, which to maybe more of an interest of your listeners
[00:24:43.720 --> 00:24:49.680]   is about project management and about people management and bringing the right people to
[00:24:49.680 --> 00:24:53.220]   the table with the right skillset.
[00:24:53.220 --> 00:24:59.360]   And recognizing, of course, that the entire endeavor doesn't need to be one big innovation.
[00:24:59.360 --> 00:25:03.580]   There are places where you absolutely need to innovate and create new things that don't
[00:25:03.580 --> 00:25:06.080]   exist for you to get to your goal.
[00:25:06.080 --> 00:25:14.160]   But to do this essentially on time and on budget on a 23rd year timescale is just mind
[00:25:14.160 --> 00:25:15.160]   boggling.
[00:25:15.160 --> 00:25:21.960]   Is the achievement of that just verifying that gravitational waves exist, or do we have
[00:25:21.960 --> 00:25:28.400]   a new type of sensor that might somehow find interesting stuff in our world?
[00:25:28.400 --> 00:25:36.440]   Well, without trying to prejudice where things are going, I will say that the history of
[00:25:36.440 --> 00:25:43.220]   astronomy in that context of opening up your eyes to new things invariably leads to discoveries
[00:25:43.220 --> 00:25:46.560]   that were unexpected.
[00:25:46.560 --> 00:25:51.160]   So far, I'd say the only large unexpected thing that's come out of the gravitational
[00:25:51.160 --> 00:25:56.340]   wave set of observations is the sheer mass, the enormity of the individual black holes
[00:25:56.340 --> 00:25:57.720]   that are colliding.
[00:25:57.720 --> 00:26:02.480]   There wasn't a lot of great motivation to say that we'd be seeing 100 and 200 solar
[00:26:02.480 --> 00:26:04.560]   mass black holes that were colliding into each other.
[00:26:04.560 --> 00:26:09.840]   So in some sense, it comes back to the astrophysicist asked the question, how do you even make 100
[00:26:09.840 --> 00:26:14.160]   solar mass black holes and then put them in the vicinity of another 100 solar mass black
[00:26:14.160 --> 00:26:15.160]   hole?
[00:26:15.160 --> 00:26:19.040]   We were thinking in the end it would be 10 solar mass and 20 solar mass black holes.
[00:26:19.040 --> 00:26:21.440]   That was the best bet if you ask most astronomers.
[00:26:21.440 --> 00:26:24.040]   So there's a little bit of surprise on that.
[00:26:24.040 --> 00:26:27.640]   None of us were surprised of the existence of gravitational waves.
[00:26:27.640 --> 00:26:32.280]   There had actually been Nobel prizes given out for the indirect discovery of the existence
[00:26:32.280 --> 00:26:38.040]   of gravitational radiation by looking at the orbit decay of neutron stars in a binary system.
[00:26:38.040 --> 00:26:43.120]   So we had known that it was very likely that this existed, but the direct detection of
[00:26:43.120 --> 00:26:45.680]   that was a very beautiful vindication.
[00:26:45.680 --> 00:26:50.920]   And now that we're there and we're having to grapple with understanding the demographics
[00:26:50.920 --> 00:26:58.520]   of the black hole population, a real interesting question is how, as I was saying earlier,
[00:26:58.520 --> 00:27:05.620]   how can we use our observations going forward to test general ideas about general relativity?
[00:27:05.620 --> 00:27:09.360]   When I was a kid, I remember learning about the Hubble telescope and the excitement around,
[00:27:09.360 --> 00:27:14.480]   I mean, I think in general, putting telescopes into space was this big, exciting project
[00:27:14.480 --> 00:27:15.680]   that seemed really cool.
[00:27:15.680 --> 00:27:21.960]   I guess, have we sort of gotten so good at signal processing or sort of undoing the effects
[00:27:21.960 --> 00:27:27.600]   of the atmosphere that that's no longer such an important thing to do to get our telescopes
[00:27:27.600 --> 00:27:28.600]   up in space?
[00:27:28.600 --> 00:27:34.000]   I mean, it also seems like when I was a kid, I had the sense that telescopes were getting
[00:27:34.000 --> 00:27:38.800]   bigger and bigger and we were seeing more and more things, but has that maybe stopped?
[00:27:38.800 --> 00:27:44.000]   Do we still aspire to make even more gigantic telescopes to see deeper into space?
[00:27:44.000 --> 00:27:45.800]   Or where do you think that's going?
[00:27:45.800 --> 00:27:46.800]   Well, it's a great question.
[00:27:46.800 --> 00:27:47.800]   It depends on who you ask.
[00:27:47.800 --> 00:27:52.920]   There isn't a general consensus of the right answer for that, which is good because the
[00:27:52.920 --> 00:27:57.240]   right answer is you do what the science demands.
[00:27:57.240 --> 00:28:04.200]   And there is a very successful satellite that was launched into space called the TESS satellite,
[00:28:04.200 --> 00:28:11.160]   whose sole purpose was to look for Earth mass planets around sun-like stars and to find
[00:28:11.160 --> 00:28:17.000]   those just using the so-called transit technique where one planet moves in front of its parent
[00:28:17.000 --> 00:28:19.720]   star and that star slightly dims.
[00:28:19.720 --> 00:28:24.600]   To do that, you need to see the dimming of a star in one part in 10 to the 5 or one part
[00:28:24.600 --> 00:28:27.880]   in 10 to the 6, which you just can't do from the ground.
[00:28:27.880 --> 00:28:31.800]   There's just too much atmospheric flickering that you just can't correct.
[00:28:31.800 --> 00:28:35.720]   We can get down to one part in 10 to the 3 or maybe one part in 10 to the 4 from the
[00:28:35.720 --> 00:28:39.820]   ground, but that's pretty much as far as we're going to be able to go.
[00:28:39.820 --> 00:28:45.040]   So for finding exoplanets of Earth mass size, you pretty much have to go into space.
[00:28:45.040 --> 00:28:50.920]   And so rather than build a huge telescope, what they did is mount the equivalent of a
[00:28:50.920 --> 00:28:55.080]   bunch of glorified cannons and a bunch of glorified iPhones to look at a very large
[00:28:55.080 --> 00:28:59.120]   swath of the space so they could study many, many stars simultaneously.
[00:28:59.120 --> 00:29:03.140]   And there, they weren't all that interested in looking at stars that were faint because
[00:29:03.140 --> 00:29:07.040]   once you discover one of these planets, you want to have lots and lots of photons with
[00:29:07.040 --> 00:29:10.200]   other follow-up facilities to actually do all the work.
[00:29:10.200 --> 00:29:14.160]   So they actually needed a very wide field to get very bright stars.
[00:29:14.160 --> 00:29:19.800]   But there are other people who are launching large satellites with large mirrors because
[00:29:19.800 --> 00:29:26.360]   they want to look at very faint explosions, supernovae in very, very distant galaxies.
[00:29:26.360 --> 00:29:29.000]   And yes, you can do that from the ground.
[00:29:29.000 --> 00:29:33.760]   It just turns out from a price perspective, there are some types of science that are actually
[00:29:33.760 --> 00:29:36.320]   easier and cheaper to do from space.
[00:29:36.320 --> 00:29:41.360]   My own interest is sort of depends upon, can I do this from the ground?
[00:29:41.360 --> 00:29:44.960]   If not, what's the simplest and cheapest thing that we can do from space?
[00:29:44.960 --> 00:29:49.600]   One of the things I'm really excited about, which you may not be aware of, is there is
[00:29:49.600 --> 00:29:56.780]   a quite a big and interesting push now towards smaller format satellites, i.e. CubeSats.
[00:29:56.780 --> 00:30:01.640]   In part because if you have a very dedicated science goal and you need to look at, let's
[00:30:01.640 --> 00:30:08.000]   say, one object for just a month, but you need to do that one second cadence, that's
[00:30:08.000 --> 00:30:10.000]   really hard to do from the ground.
[00:30:10.000 --> 00:30:14.560]   But you could potentially do it from space very, very cheaply now because the actual
[00:30:14.560 --> 00:30:17.020]   parts are largely commoditized.
[00:30:17.020 --> 00:30:23.600]   And the launch, which is a very dominant cost for heavy space vehicles, is more or less
[00:30:23.600 --> 00:30:28.360]   zero because there's so many launch vehicles going up into space for all these different
[00:30:28.360 --> 00:30:29.360]   reasons.
[00:30:29.360 --> 00:30:33.640]   You can pay back a whole bunch of these small satellites more or less for free.
[00:30:33.640 --> 00:30:40.040]   So what I think you'll see in the next 10 years or so is a renaissance, not so much
[00:30:40.040 --> 00:30:44.480]   at the large telescope level, but at the small telescope level in space.
[00:30:44.480 --> 00:30:49.040]   And the last thing I'll just say is that we sometimes have to go to space because the
[00:30:49.040 --> 00:30:51.200]   Earth's atmosphere blocks certain wavelengths.
[00:30:51.200 --> 00:30:56.200]   So if we're interested, for instance, in the ultraviolet sky and phenomena at the ultraviolet
[00:30:56.200 --> 00:31:00.920]   sky, well, because of our ozone layer, we block off most of the UV light.
[00:31:00.920 --> 00:31:02.840]   So we couldn't do anything from the ground.
[00:31:02.840 --> 00:31:03.840]   Cool.
[00:31:03.840 --> 00:31:09.600]   Well, I guess I want to make sure I ask you some questions about machine learning also.
[00:31:09.600 --> 00:31:11.080]   I wanted to ask you about...
[00:31:11.080 --> 00:31:15.920]   So you have this kind of group like ML for science, right?
[00:31:15.920 --> 00:31:21.000]   I'm curious, what inspired you to put that together and what kinds of stuff you work
[00:31:21.000 --> 00:31:22.400]   on there?
[00:31:22.400 --> 00:31:28.200]   Well, it might be worth talking a little about how we stumbled upon machine learning in my
[00:31:28.200 --> 00:31:33.760]   research and kind of where that's led to.
[00:31:33.760 --> 00:31:40.040]   About 12, 13 years ago, we were actually dealing with lots of images coming off of telescopes
[00:31:40.040 --> 00:31:41.520]   from the ground.
[00:31:41.520 --> 00:31:47.440]   And the normal behavior when you get lots of data had been, and in many circles still
[00:31:47.440 --> 00:31:50.920]   is, just to hire more grad students to look at the data.
[00:31:50.920 --> 00:31:55.920]   So I was looking for ways to kind of scale our way out of what was a very repetitive
[00:31:55.920 --> 00:32:00.680]   inference task, which was the discovery of new events in the sky.
[00:32:00.680 --> 00:32:07.240]   And what we typically deal with is a new image that's taken of the sky, and you have a template
[00:32:07.240 --> 00:32:12.000]   image of that same part of the sky taken in the months prior where you've stacked up a
[00:32:12.000 --> 00:32:15.400]   whole bunch of really good images and you subtract the two off.
[00:32:15.400 --> 00:32:19.760]   That subtraction process is imperfect because of the atmosphere, because of instrumentation
[00:32:19.760 --> 00:32:20.920]   effects.
[00:32:20.920 --> 00:32:25.320]   And what people would do is look at postage stamps around all the kind of five sigma new
[00:32:25.320 --> 00:32:29.700]   positive signals, but most of those are actually spurious.
[00:32:29.700 --> 00:32:35.280]   So the first place where we landed in the utility of machine learning for my own research
[00:32:35.280 --> 00:32:41.160]   was creating what we call a real bogus detector, where we trained off of good subtractions,
[00:32:41.160 --> 00:32:46.680]   i.e. of real objects and bad ones because of all these different detector effects and
[00:32:46.680 --> 00:32:48.320]   instrument effects.
[00:32:48.320 --> 00:32:53.280]   And we're able to build something with good enough false positive and false negative rates
[00:32:53.280 --> 00:32:59.580]   that we're able to put that into production and kind of reduce the amount of time it would
[00:32:59.580 --> 00:33:06.880]   take a person to look at a whole night's worth of candidates from hours down to minutes,
[00:33:06.880 --> 00:33:09.940]   and still keeping a person in the loop.
[00:33:09.940 --> 00:33:14.080]   At the time, I sort of had the conceit that, okay, if we can do this, it means then we
[00:33:14.080 --> 00:33:17.080]   don't need people to look at the follow-up data.
[00:33:17.080 --> 00:33:21.160]   We can actually just get to the point of almost writing a paper without any people in the
[00:33:21.160 --> 00:33:22.160]   loop.
[00:33:22.160 --> 00:33:27.160]   But as you know well from your current work in your previous company, people in the real-time
[00:33:27.160 --> 00:33:32.620]   loop is still important and can be very important, even when it's machine learning assisted.
[00:33:32.620 --> 00:33:34.760]   So that was very successful in that.
[00:33:34.760 --> 00:33:42.000]   It was back in the old days of random forest before deep learning kind of had its renaissance.
[00:33:42.000 --> 00:33:48.440]   And now this idea of real bogus discovery happens pretty much in every project going
[00:33:48.440 --> 00:33:53.560]   way beyond where we were a while ago, now using modern deep learning techniques.
[00:33:53.560 --> 00:33:59.200]   Can I ask, before you go further, in my previous work, I always admired the site Galaxy Zoo,
[00:33:59.200 --> 00:34:03.640]   where they kind of got lots of people to sort of crowdsource some of the labeling of these
[00:34:03.640 --> 00:34:04.640]   images.
[00:34:04.640 --> 00:34:05.640]   Did you look at that at all?
[00:34:05.640 --> 00:34:07.720]   That always seemed like such a cool project to me.
[00:34:07.720 --> 00:34:09.760]   I did look at that.
[00:34:09.760 --> 00:34:11.840]   Yeah, yes, I did look at that.
[00:34:11.840 --> 00:34:19.600]   I think the idea of labeling in astronomy has been really wonderful as an outreach tool.
[00:34:19.600 --> 00:34:23.760]   And there certainly have been some scientific papers that have come out of that.
[00:34:23.760 --> 00:34:28.420]   In particular, there was a discovery of sort of a weird class of gas around certain types
[00:34:28.420 --> 00:34:32.280]   of galaxies that was made by somebody looking at Galaxy Zoo images.
[00:34:32.280 --> 00:34:39.720]   But a lot of the labeling, if I'm being really honest, by people in the Galaxy Zoo world
[00:34:39.720 --> 00:34:44.080]   could have been done and ought to have been done by machine learning classifiers.
[00:34:44.080 --> 00:34:46.080]   So is this a spiral galaxy?
[00:34:46.080 --> 00:34:47.560]   Is this a red galaxy?
[00:34:47.560 --> 00:34:51.960]   The questions that generally are asked in that world, I've done this in classes that
[00:34:51.960 --> 00:34:57.960]   I've taught, have a student for a final project, try to reproduce the rock curves of people
[00:34:57.960 --> 00:35:00.800]   in classification, and they can do well.
[00:35:00.800 --> 00:35:07.280]   And we actually showed for the supernova classification challenge, we were able to build a machine
[00:35:07.280 --> 00:35:12.880]   learning classifier off of the original training data from Galaxy Zoo and outperform Galaxy
[00:35:12.880 --> 00:35:15.320]   Zoo in the false positive, false negative sense.
[00:35:15.320 --> 00:35:22.920]   So one of the challenges that I think all of us have in employing people to do repetitive
[00:35:22.920 --> 00:35:29.480]   inference tasks is to ask ourselves the hard question of, can I have a machine do it?
[00:35:29.480 --> 00:35:34.480]   And if the goal is to involve people so that they're involved in research and they're helping,
[00:35:34.480 --> 00:35:35.640]   that's fantastic.
[00:35:35.640 --> 00:35:39.920]   If the goal is to get people looking at data because maybe they'll also see something that
[00:35:39.920 --> 00:35:44.200]   an answer question that we didn't even ask, that's fantastic as well.
[00:35:44.200 --> 00:35:51.800]   But for the specific tasks that a lot of crowdsourcing questions have asked, I think especially with
[00:35:51.800 --> 00:35:57.000]   where computer vision has arisen, we're able to do that better.
[00:35:57.000 --> 00:36:00.720]   Moreover, we're able to do it faster and moreover, we're able to do it in a repeatable way.
[00:36:00.720 --> 00:36:04.640]   So one of the other challenges, of course, if you ask somebody to label a bunch of data
[00:36:04.640 --> 00:36:08.440]   and then you ask them to come back tomorrow after they've had a beer and label the same
[00:36:08.440 --> 00:36:10.120]   data, you'll get a different answer.
[00:36:10.120 --> 00:36:15.240]   And so from understanding the demographics of everything we see, I think it becomes a
[00:36:15.240 --> 00:36:19.720]   lot harder when you have people that are deeply part of that process.
[00:36:19.720 --> 00:36:20.720]   Gotcha.
[00:36:20.720 --> 00:36:21.720]   So I cut you off though.
[00:36:21.720 --> 00:36:26.760]   So it sounds like, I mean, you were doing this quite a while ago and especially vision
[00:36:26.760 --> 00:36:29.520]   techniques I think have massively improved.
[00:36:29.520 --> 00:36:32.160]   I don't even especially vision techniques, but there was kind of this moment where vision
[00:36:32.160 --> 00:36:33.520]   got quite a lot better.
[00:36:33.520 --> 00:36:38.080]   Did that affect the way you used machine learning in your work?
[00:36:38.080 --> 00:36:39.680]   Yeah.
[00:36:39.680 --> 00:36:46.240]   So we always are careful in the sense to try not to look around in astronomy and say, "That's
[00:36:46.240 --> 00:36:51.480]   a computer vision task that's clearly solvable now by CNN, so let's go work on that problem."
[00:36:51.480 --> 00:36:55.680]   There is a little bit of the everything looks like a nail because we have this really cool
[00:36:55.680 --> 00:36:56.720]   hammer.
[00:36:56.720 --> 00:37:01.000]   That was a computer vision task, this real bogus detector that we had to solve if we
[00:37:01.000 --> 00:37:03.600]   were going to break this sort of grad student bottleneck.
[00:37:03.600 --> 00:37:09.080]   There are plenty of tasks that people are doing, asking questions of images that were
[00:37:09.080 --> 00:37:15.000]   around before, but perhaps weren't as interesting because we had no way of solving those problems
[00:37:15.000 --> 00:37:16.760]   and now we can do those at scale.
[00:37:16.760 --> 00:37:24.000]   I actually focus less on images now and focus more on a regular time series data.
[00:37:24.000 --> 00:37:28.380]   But I think one of the important things to recognize about where astronomy is maybe relative
[00:37:28.380 --> 00:37:34.200]   to many of the other fields of the people you've talked about on this podcast is that
[00:37:34.200 --> 00:37:40.360]   we haven't had that moment maybe that existed in NLP where, you know, John said, "Every
[00:37:40.360 --> 00:37:45.480]   time I fire a linguist, my language detector gets better."
[00:37:45.480 --> 00:37:51.120]   The idea that if you start removing domain experts out of the loop and you actually start
[00:37:51.120 --> 00:37:56.800]   building language models, just learning off of data and it gets better and better, we
[00:37:56.800 --> 00:37:59.520]   don't have that moment in astronomy.
[00:37:59.520 --> 00:38:02.860]   And computer vision is the same thing too, right?
[00:38:02.860 --> 00:38:08.140]   You fire a bunch of old school computer vision experts that learned about Hough transforms
[00:38:08.140 --> 00:38:12.160]   and stuff, and now you just throw it into a big CNN with lots of training data, you
[00:38:12.160 --> 00:38:16.720]   get better answers than what you were ever able to do in the past.
[00:38:16.720 --> 00:38:19.240]   That hasn't happened in astronomy, right?
[00:38:19.240 --> 00:38:26.280]   We've used ML in lots of different places as sort of accelerants and as surrogates to
[00:38:26.280 --> 00:38:29.440]   harder computations so we can get faster answers.
[00:38:29.440 --> 00:38:32.800]   We can do inference at scale in ways we weren't able to do before.
[00:38:32.800 --> 00:38:35.520]   But it's the same thing in biology, right?
[00:38:35.520 --> 00:38:43.160]   ML didn't invent CRISPR and Caitlin Carrico, who was this person who toiled away for decades
[00:38:43.160 --> 00:38:50.480]   trying to understand how mRNA could lead to a vaccine, she was actively denied tenure
[00:38:50.480 --> 00:38:53.880]   and actively denied grants.
[00:38:53.880 --> 00:38:59.360]   She had nothing to do with ML, but if it wasn't for her, we wouldn't have vaccines for COVID.
[00:38:59.360 --> 00:39:05.200]   Biology I think also hasn't had its ML moment where you can start firing domain experts
[00:39:05.200 --> 00:39:06.320]   and start doing things.
[00:39:06.320 --> 00:39:13.680]   Right now, physics, astronomy, chemistry, for the most part, we're working in a world
[00:39:13.680 --> 00:39:20.360]   where machine learning is this really big, important tool in our toolbox, but it's not
[00:39:20.360 --> 00:39:25.560]   become the fundamental driver of how new insights happen.
[00:39:25.560 --> 00:39:31.040]   But I guess one key difference here though is the work product of astronomy is kind of
[00:39:31.040 --> 00:39:34.600]   explaining the world that we live in, right?
[00:39:34.600 --> 00:39:39.280]   Whereas first of all, I'm not sure if I agree with the comment about linguists and I don't
[00:39:39.280 --> 00:39:40.280]   want to go on record.
[00:39:40.280 --> 00:39:41.280]   I think it's on my phone.
[00:39:41.280 --> 00:39:42.280]   But no, no, I know, I know.
[00:39:42.280 --> 00:39:43.640]   I know what you mean.
[00:39:43.640 --> 00:39:50.000]   But I think definitely linguists still do the best job of kind of explaining language
[00:39:50.000 --> 00:39:51.000]   like in the context.
[00:39:51.000 --> 00:39:56.640]   I mean, I think linguists would probably say, we use ML techniques to kind of understand
[00:39:56.640 --> 00:39:59.560]   language better, not like we've replaced ourselves.
[00:39:59.560 --> 00:40:04.320]   Although it seems like modern translation techniques are less informed by linguistics
[00:40:04.320 --> 00:40:07.960]   than you might, well, I might've expected when I was younger.
[00:40:07.960 --> 00:40:15.440]   So I wonder if it's sort of like a function of a domain being more trying to engineer
[00:40:15.440 --> 00:40:19.920]   a certain solution to a specific problem versus do some kind of explanation.
[00:40:19.920 --> 00:40:23.920]   We actually talked to a whole bunch of biologists and it does sort of seem like some of the
[00:40:23.920 --> 00:40:28.240]   processes around drug discovery are starting to be more and more informed by ML and kind
[00:40:28.240 --> 00:40:29.720]   of moving in that direction.
[00:40:29.720 --> 00:40:31.600]   Yeah, I certainly don't.
[00:40:31.600 --> 00:40:37.000]   And I think that's where we exactly are right now is there is a huge amount of ML that is
[00:40:37.000 --> 00:40:45.000]   informing astronomy, but I don't think we're there anywhere near where the NLP world is.
[00:40:45.000 --> 00:40:50.120]   And in part it's because we haven't, to your point, we haven't really been able to articulate
[00:40:50.120 --> 00:40:59.160]   a set of outcomes that are comparable or have as much weight or as much import as an NLP
[00:40:59.160 --> 00:41:00.680]   task like translation, right?
[00:41:00.680 --> 00:41:06.120]   You can directly correlate, I assume, the quality of a translation from language A to
[00:41:06.120 --> 00:41:08.720]   language B to some dollar outcome.
[00:41:08.720 --> 00:41:11.720]   And in astronomy, we don't have the ability to do that.
[00:41:11.720 --> 00:41:15.200]   So our loss function is a little bit more complicated.
[00:41:15.200 --> 00:41:22.280]   And as we're learning these various different tasks as part of our workflow, we don't have
[00:41:22.280 --> 00:41:28.240]   the ability in the same way many other fields do to articulate that loss function in terms
[00:41:28.240 --> 00:41:31.920]   that have this sort of monetary value.
[00:41:31.920 --> 00:41:37.600]   When you ask this question about what is the nature of dark energy or dark matter, or how
[00:41:37.600 --> 00:41:44.240]   many exoplanets are there out there that host life, those are in some sense quantifiable
[00:41:44.240 --> 00:41:45.240]   answers.
[00:41:45.240 --> 00:41:49.960]   But as you're saying, that's sort of where more of the explainability has to come in.
[00:41:49.960 --> 00:41:55.480]   And I certainly don't think we're even trying to get to the point where we fire a physicist
[00:41:55.480 --> 00:41:58.360]   so that I can hire a computer scientist.
[00:41:58.360 --> 00:42:03.760]   It's going to be the marriage of those two people or as an individual and their skill
[00:42:03.760 --> 00:42:06.040]   sets who are going to make a lot of the progress.
[00:42:06.040 --> 00:42:10.780]   And I think the really exciting place where we could get to, and there are little tiny
[00:42:10.780 --> 00:42:19.080]   pieces of this starting to happen, is whether an application of ML to a bunch of data can
[00:42:19.080 --> 00:42:24.640]   be something that leads to a discovery on a bunch of questions that we didn't even know
[00:42:24.640 --> 00:42:26.520]   how to ask.
[00:42:26.520 --> 00:42:30.600]   And that would be a real hallmark moment in our field.
[00:42:30.600 --> 00:42:35.000]   Right now, everything is done largely in a supervised context.
[00:42:35.000 --> 00:42:40.680]   We sort of have some semi-supervised and unsupervised ways of looking for anomalies and outliers
[00:42:40.680 --> 00:42:41.680]   and things like that.
[00:42:41.680 --> 00:42:47.720]   But even that, it becomes a guide to a domain scientist looking at this and say, "Oh yeah,
[00:42:47.720 --> 00:42:53.200]   of course I know what these things are," or "This is because the data is spurious."
[00:42:53.200 --> 00:43:00.160]   Maybe what's really fundamental if I think about it, is that the job of these ML pipelines
[00:43:00.160 --> 00:43:05.760]   that we build on different parts of our data isn't so much about prediction in the same
[00:43:05.760 --> 00:43:09.640]   way that if I need to predict what the next word is, or I need to predict if this is a
[00:43:09.640 --> 00:43:16.400]   cat or a dog, or what the best thing to show somebody is next, that is the proof in the
[00:43:16.400 --> 00:43:17.400]   pudding.
[00:43:17.400 --> 00:43:21.560]   And you've done well because you can measure what the outcomes are after that.
[00:43:21.560 --> 00:43:26.600]   If I make a prediction in astronomy, that's really just for hypothesis testing.
[00:43:26.600 --> 00:43:31.280]   If I have a new theory that's gleaned off of data, the job of that theory is to make
[00:43:31.280 --> 00:43:36.040]   a prediction about what happens if I observe outside of the domain of the data that I already
[00:43:36.040 --> 00:43:38.060]   have, to falsify itself.
[00:43:38.060 --> 00:43:43.040]   We haven't really wrapped our head around the idea that ML in the context of the physical
[00:43:43.040 --> 00:43:48.320]   sciences isn't just about making predictions at scale so that we can get slightly better
[00:43:48.320 --> 00:43:51.160]   data farther down the work chain.
[00:43:51.160 --> 00:43:56.160]   If it's going to actually drive our deeper understanding of how the universe works, it
[00:43:56.160 --> 00:44:04.320]   has to couch itself in the terms of hypothesis testing, Occam's razor, and we haven't really
[00:44:04.320 --> 00:44:05.720]   gotten there yet.
[00:44:05.720 --> 00:44:11.360]   I'm so surprised to hear you say that because I feel like it sort of seems like we fund
[00:44:11.360 --> 00:44:16.900]   all this work to make better devices and telescopes, and it seems like they pay us back in terms
[00:44:16.900 --> 00:44:21.760]   of these really awesome new understandings about the physical world.
[00:44:21.760 --> 00:44:27.640]   And it sort of seems like you make a bigger telescope, that's sort of just like seeing
[00:44:27.640 --> 00:44:30.480]   things slightly better, however you put it.
[00:44:30.480 --> 00:44:32.960]   Isn't it kind of similar?
[00:44:32.960 --> 00:44:37.800]   If ML could help you get better data to inform your predictions, wouldn't that be a big deal?
[00:44:37.800 --> 00:44:44.560]   Do you really need to be completely replaced by ML for it to be-
[00:44:44.560 --> 00:44:46.160]   No, no, no, no.
[00:44:46.160 --> 00:44:53.160]   I certainly don't want to come off in trying to make the argument that ML hasn't been important.
[00:44:53.160 --> 00:44:59.000]   We're currently working on a project where a big part of that whole data taking, data
[00:44:59.000 --> 00:45:06.280]   planning, data reduction, initial discovery, initial inference, initial follow-up, that
[00:45:06.280 --> 00:45:07.320]   all happens.
[00:45:07.320 --> 00:45:10.140]   There's little pieces of ML through that entire chain.
[00:45:10.140 --> 00:45:14.480]   That all happens without people in the loop now, which is absolutely incredible.
[00:45:14.480 --> 00:45:17.600]   It's more or less talking to telescopes mitigated by ML.
[00:45:17.600 --> 00:45:19.400]   This is where we are.
[00:45:19.400 --> 00:45:21.360]   There's only going to be more of that going on.
[00:45:21.360 --> 00:45:25.980]   And what we're doing is we're optimizing our resources and our resource allocation because
[00:45:25.980 --> 00:45:28.200]   we're using ML.
[00:45:28.200 --> 00:45:35.760]   But I still see that as fundamentally an accelerant and a surrogate to what we were pretty much
[00:45:35.760 --> 00:45:36.960]   doing in the past.
[00:45:36.960 --> 00:45:42.480]   I haven't seen anything that fundamentally changes the way we conduct ourselves as physicists.
[00:45:42.480 --> 00:45:47.200]   But again, as I said, there are little pieces starting to show up or the rediscovery of
[00:45:47.200 --> 00:45:55.520]   the Higgs boson using pure ML without reference to the basic physics of how particles interact.
[00:45:55.520 --> 00:45:56.520]   Wow.
[00:45:56.520 --> 00:45:58.800]   Do you think that would have worked without knowing about it?
[00:45:58.800 --> 00:45:59.800]   I mean, is that-
[00:45:59.800 --> 00:46:00.800]   That's the question, right?
[00:46:00.800 --> 00:46:06.360]   So until we get to the point where someone says, "I ran my ML pipeline on this particle
[00:46:06.360 --> 00:46:11.560]   physics data and I saw this new thing and everyone in the group didn't believe me until
[00:46:11.560 --> 00:46:14.480]   they got 10 times more data and it turns out it was there."
[00:46:14.480 --> 00:46:16.440]   We haven't really, really gotten there yet.
[00:46:16.440 --> 00:46:21.840]   There's been a few places where people have found another exoplanet in a complicated data
[00:46:21.840 --> 00:46:24.080]   set that people hadn't seen before.
[00:46:24.080 --> 00:46:29.120]   But astronomers for the most part are still Bayesians, right?
[00:46:29.120 --> 00:46:35.880]   And we're still governed by Bayes rule where we come to our problem with a bunch of priors.
[00:46:35.880 --> 00:46:41.720]   We get data that updates our beliefs and we get slightly better or sometimes much better
[00:46:41.720 --> 00:46:43.840]   understandings in our posteriors.
[00:46:43.840 --> 00:46:48.600]   If we talk about inference and understanding, we need to couch it in terms of what we think
[00:46:48.600 --> 00:46:52.880]   are the physical properties and the physical things and the parameters that describe the
[00:46:52.880 --> 00:46:54.640]   object that we're looking at.
[00:46:54.640 --> 00:46:55.640]   We're getting better at that.
[00:46:55.640 --> 00:47:01.840]   One of the big things I'm doing in ML right now is trying to use different types of networks
[00:47:01.840 --> 00:47:07.580]   and a whole new class of approaches called likelihood-free inference to go directly
[00:47:07.580 --> 00:47:14.320]   from raw observations to posteriors or approximate posteriors.
[00:47:14.320 --> 00:47:19.200]   I think that's extremely exciting and it could be applied to a whole bunch of different places.
[00:47:19.200 --> 00:47:20.200]   Cool.
[00:47:20.200 --> 00:47:21.200]   That's so cool.
[00:47:21.200 --> 00:47:25.800]   I guess one thing that I wonder about, it must be kind of interesting being in your
[00:47:25.800 --> 00:47:26.800]   shoes.
[00:47:26.800 --> 00:47:29.960]   Are you doing most of this with ML grad students?
[00:47:29.960 --> 00:47:38.880]   Are you at the point with your data pipelines where you need to pull experts from industry?
[00:47:38.880 --> 00:47:44.200]   It's so funny how much of our data pipeline stuff has come originally from astronomy and
[00:47:44.200 --> 00:47:45.200]   sciences in general.
[00:47:45.200 --> 00:47:49.640]   Maybe it's always at the cutting edge or do you feel like you need to get experts in terms
[00:47:49.640 --> 00:47:54.480]   of just handling these volumes of datasets and building these gigantic models?
[00:47:54.480 --> 00:47:55.480]   It's a great question.
[00:47:55.480 --> 00:47:58.760]   Again, I think the answer depends.
[00:47:58.760 --> 00:48:06.480]   There are lots of examples in my own research and my own work where hiring very good data
[00:48:06.480 --> 00:48:11.840]   engineers and having some ML expertise on the team suffices.
[00:48:11.840 --> 00:48:16.560]   It's where you actually need to innovate, create new algorithms, take some existing
[00:48:16.560 --> 00:48:21.720]   network and completely blow it up and change the way that it works that you do need somebody
[00:48:21.720 --> 00:48:26.080]   with a deep domain background in CS/ML.
[00:48:26.080 --> 00:48:32.880]   One of the beautiful things about being on the Berkeley campus is how just everyone across
[00:48:32.880 --> 00:48:35.920]   campus is looking to work with each other.
[00:48:35.920 --> 00:48:43.080]   Because again, we all kind of recognize, at least from the physical domain side, that
[00:48:43.080 --> 00:48:47.400]   there's incredible work that's happening in the computer science department, the stats
[00:48:47.400 --> 00:48:48.400]   department.
[00:48:48.400 --> 00:48:53.520]   I've just become a member of the faculty of Bayer on the Berkeley AI research group.
[00:48:53.520 --> 00:48:57.240]   So I get to interact with those grad students and those postdocs.
[00:48:57.240 --> 00:49:04.920]   We still, I think, face the challenge that any academic arena does when crossing over
[00:49:04.920 --> 00:49:10.560]   into other fields of trying to make the kinds of problems we have compelling for the other
[00:49:10.560 --> 00:49:15.200]   side and have the other side recognize that they're not just setting up a Spark cluster
[00:49:15.200 --> 00:49:18.760]   for us and downloading ResNet.
[00:49:18.760 --> 00:49:24.400]   What the people in computer science and stats need to realize is that we are asking questions
[00:49:24.400 --> 00:49:31.600]   of data in a way that they are not, of the sort of benchmark datasets that they're often
[00:49:31.600 --> 00:49:33.800]   working on algorithmically.
[00:49:33.800 --> 00:49:39.040]   And because of that, there needs to be some real fundamental innovation.
[00:49:39.040 --> 00:49:44.160]   I've been really fortunate in my career to have gotten grants that have allowed me to
[00:49:44.160 --> 00:49:49.880]   hire people outside of a traditional astronomy background, hired PhDs in computer science
[00:49:49.880 --> 00:49:51.440]   and statistics.
[00:49:51.440 --> 00:49:55.440]   And that's where some of the most interesting innovations happen.
[00:49:55.440 --> 00:50:01.120]   Where we're at, I'd say now, as a profession, is really struggling with the idea of how
[00:50:01.120 --> 00:50:10.000]   much should our students have to learn for them to be able to work on this as their main
[00:50:10.000 --> 00:50:11.000]   endeavor.
[00:50:11.000 --> 00:50:16.920]   We don't have, as part of our curriculum, a deep training in stats, let alone ML, let
[00:50:16.920 --> 00:50:18.960]   alone software engineering.
[00:50:18.960 --> 00:50:25.160]   And so I don't know where they find the time and where they will find the time going forward
[00:50:25.160 --> 00:50:28.160]   to be able to get all of that in at a fundamental level.
[00:50:28.160 --> 00:50:29.160]   We're working on it.
[00:50:29.160 --> 00:50:30.160]   We're trying.
[00:50:30.160 --> 00:50:34.520]   Berkeley has started a new data science major that the astronomy department is connecting
[00:50:34.520 --> 00:50:36.360]   up into with their own classes.
[00:50:36.360 --> 00:50:41.000]   But there isn't, at the national level, a holistic understanding of how we're going
[00:50:41.000 --> 00:50:45.160]   to do training of the next generation of physical scientists.
[00:50:45.160 --> 00:50:50.600]   So they're not just conversing in ML, but they can actually do a bunch themselves.
[00:50:50.600 --> 00:50:56.640]   Well, actually, the question I wanted to ask you, which is a good segue, when I was looking
[00:50:56.640 --> 00:51:03.640]   at your website, I found hundreds of research papers, but also mixed in some opinionated
[00:51:03.640 --> 00:51:07.080]   blog posts on programming language details.
[00:51:07.080 --> 00:51:12.040]   And I was actually kind of wondering for you, and maybe this is something I'm asking myself,
[00:51:12.040 --> 00:51:14.160]   how did you stay current on...
[00:51:14.160 --> 00:51:18.560]   How did you even find time to get to a high level of programming at all?
[00:51:18.560 --> 00:51:20.960]   And how do you kind of stay on top of that?
[00:51:20.960 --> 00:51:24.360]   Are you spending time writing code yourself as a professor?
[00:51:24.360 --> 00:51:26.880]   Yeah, I write a lot of code.
[00:51:26.880 --> 00:51:28.560]   That's some of my happiest times.
[00:51:28.560 --> 00:51:33.560]   So in some sense, that's my hobby.
[00:51:33.560 --> 00:51:39.720]   I came to programming early on in my academic career when I was an undergraduate, where
[00:51:39.720 --> 00:51:45.120]   I was basically told by my future advisor at Los Alamos, I can't work there for the
[00:51:45.120 --> 00:51:50.440]   summer unless I've taken a class in C. And I did.
[00:51:50.440 --> 00:51:54.880]   And that was more or less the only class I ever took in computer science.
[00:51:54.880 --> 00:52:01.160]   But then again, it was this matter of necessity, just like it is with building better telescopes.
[00:52:01.160 --> 00:52:07.840]   I decided when I was a postdoc to automate an old mothball telescope, which was a fairly
[00:52:07.840 --> 00:52:12.840]   large one meter class telescope in Arizona, and kind of take all the pieces that had been
[00:52:12.840 --> 00:52:18.040]   manual when the telescope had been run before, and automate every single piece of it so it
[00:52:18.040 --> 00:52:20.200]   could run autonomously.
[00:52:20.200 --> 00:52:24.600]   And I asked a friend of mine at Los Alamos, which language should I write in?
[00:52:24.600 --> 00:52:25.600]   And he said, Python.
[00:52:25.600 --> 00:52:26.600]   And I said, what's that?
[00:52:26.600 --> 00:52:28.100]   He said, just do it.
[00:52:28.100 --> 00:52:29.100]   It's a cool language.
[00:52:29.100 --> 00:52:30.100]   That was in-
[00:52:30.100 --> 00:52:31.100]   Wait, what year is this?
[00:52:31.100 --> 00:52:32.100]   2002.
[00:52:32.100 --> 00:52:33.100]   Oh, wow.
[00:52:33.100 --> 00:52:38.180]   So I wrote a whole telescope automation software package using state machines and connecting
[00:52:38.180 --> 00:52:44.400]   up to device drivers and C++ in 2002, where I was just kind of feeling my way through
[00:52:44.400 --> 00:52:45.400]   it.
[00:52:45.400 --> 00:52:49.320]   I think I wrote my own date time module and I didn't realize date time was there.
[00:52:49.320 --> 00:52:51.840]   So I just stumbled upon it.
[00:52:51.840 --> 00:52:56.680]   And then what do you do when you're an academic and you wind up realizing something's interesting
[00:52:56.680 --> 00:53:00.080]   is that you feel bad that you're not teaching it to your students, so you do.
[00:53:00.080 --> 00:53:07.380]   So I started in 2008 a bunch of Python boot camps on campus to get people into Python,
[00:53:07.380 --> 00:53:14.000]   in part because, especially at Berkeley, we kind of caught the open source ethos pretty
[00:53:14.000 --> 00:53:20.040]   early and the kinds of languages that people around me were using, like IDL, interactive
[00:53:20.040 --> 00:53:24.000]   data language, and MATLAB, were just expensive.
[00:53:24.000 --> 00:53:31.360]   And moreover, as scientists, we certainly want to understand the algorithms that we
[00:53:31.360 --> 00:53:35.220]   apply and we want to at least be able to look under the hood if we need to.
[00:53:35.220 --> 00:53:42.280]   So I started evangelizing Python around these parts and started building classes on top
[00:53:42.280 --> 00:53:43.280]   of Python.
[00:53:43.280 --> 00:53:47.980]   So a graduate level seminar on how do you actually use Python in the real world, ranging
[00:53:47.980 --> 00:53:55.580]   everywhere from doing stats to scaling Python programs, to testing frameworks, to interacting
[00:53:55.580 --> 00:53:56.800]   with hardware.
[00:53:56.800 --> 00:54:02.200]   So that class still goes on, but I've got to say, I've ossified a little bit around
[00:54:02.200 --> 00:54:03.200]   Python.
[00:54:03.200 --> 00:54:08.000]   I've spent a little bit of time with a few other languages, but for me, I've become conversant
[00:54:08.000 --> 00:54:13.160]   enough and gotten fairly deep into this scientific Python community.
[00:54:13.160 --> 00:54:20.020]   Jupyter, for instance, with Fernando Perez here in the stats department, has really been
[00:54:20.020 --> 00:54:23.640]   a huge part of what I've used for teaching for a long time.
[00:54:23.640 --> 00:54:28.020]   And the NumPy and the SciPy stack have a lot of activity here on campus as well.
[00:54:28.020 --> 00:54:30.500]   Stefan van der Waal has a huge role in that.
[00:54:30.500 --> 00:54:34.440]   So it's sort of in the water, I'd say.
[00:54:34.440 --> 00:54:40.000]   Definitely the proof is in the pudding, having recognized that Python is extremely versatile
[00:54:40.000 --> 00:54:43.940]   as a sort of super glue language for all the kinds of stuff that we do.
[00:54:43.940 --> 00:54:46.240]   And yes, I still code.
[00:54:46.240 --> 00:54:50.080]   Last summer during the pandemic, the happiest times were me learning React.
[00:54:50.080 --> 00:54:54.360]   So I could build this large scale React app that we're doing for astronomers to interact
[00:54:54.360 --> 00:54:55.360]   over data.
[00:54:55.360 --> 00:54:56.360]   Is it React fun?
[00:54:56.360 --> 00:54:57.360]   I love it.
[00:54:57.360 --> 00:54:58.360]   React is so much fun for me.
[00:54:58.360 --> 00:54:59.360]   I thought I hated front end programming.
[00:54:59.360 --> 00:55:00.360]   Fun?
[00:55:00.360 --> 00:55:05.160]   I don't know if I would use the word fun.
[00:55:05.160 --> 00:55:09.920]   What I love about it is just so wonderfully different than the way you think about Python
[00:55:09.920 --> 00:55:10.920]   programming.
[00:55:10.920 --> 00:55:15.920]   And obviously it's rewarding in a sense that you build it, you ship it, and users see it
[00:55:15.920 --> 00:55:21.320]   right away in a way that if you build some cool Python tool, you may be the only one
[00:55:21.320 --> 00:55:25.280]   in the world that uses it just because it's on PyPy doesn't mean that somebody is actually
[00:55:25.280 --> 00:55:27.720]   going to download it, use it.
[00:55:27.720 --> 00:55:28.720]   Does it make you...
[00:55:28.720 --> 00:55:30.920]   Well, did you use TypeScript with React?
[00:55:30.920 --> 00:55:31.920]   No.
[00:55:31.920 --> 00:55:32.920]   No?
[00:55:32.920 --> 00:55:35.120]   No, we were like JSX kind of.
[00:55:35.120 --> 00:55:36.120]   I see.
[00:55:36.120 --> 00:55:37.120]   Yeah.
[00:55:37.120 --> 00:55:38.120]   Cool.
[00:55:38.120 --> 00:55:39.120]   Cool.
[00:55:39.120 --> 00:55:43.800]   I was writing a lot of front end stuff around 2008 and found it frustrating and then kind
[00:55:43.800 --> 00:55:48.800]   of went back to it a few years ago and just was impressed how much things had evolved
[00:55:48.800 --> 00:55:49.800]   in the decades.
[00:55:49.800 --> 00:55:53.880]   I love React, but I don't like testing React apps.
[00:55:53.880 --> 00:55:54.880]   Yeah.
[00:55:54.880 --> 00:56:03.040]   I was trying to add typing to some stuff recently actually with your student, Danny, and I was
[00:56:03.040 --> 00:56:07.160]   really wishing that Python's typing worked a little bit more elegantly, especially in
[00:56:07.160 --> 00:56:12.760]   the scientific computing domain.
[00:56:12.760 --> 00:56:13.760]   Do you feel like...
[00:56:13.760 --> 00:56:23.360]   I mean, I felt like when I was doing research briefly, the code bases were truly messy in
[00:56:23.360 --> 00:56:25.440]   a way I've never experienced in industry.
[00:56:25.440 --> 00:56:29.080]   This may be a long time ago.
[00:56:29.080 --> 00:56:34.280]   Do you do things in your lab to kind of keep things maintainable, like maintainable as
[00:56:34.280 --> 00:56:38.280]   kind of students come in now, do you need to do various research projects?
[00:56:38.280 --> 00:56:43.320]   Are you able to kind of find time to clean up code and eliminate tech debt and things
[00:56:43.320 --> 00:56:44.320]   like that?
[00:56:44.320 --> 00:56:48.760]   I mean, I think we're probably better than most, but you're never as good as I'd like
[00:56:48.760 --> 00:56:51.600]   to be is probably the reasonable answer.
[00:56:51.600 --> 00:56:58.840]   I'm at least aware of the existence of things like unit tests, unlike many of my colleagues
[00:56:58.840 --> 00:56:59.840]   in our field.
[00:56:59.840 --> 00:57:04.160]   Yeah, it is a mess.
[00:57:04.160 --> 00:57:08.400]   And again, it comes back down to loss functions and incentives, right?
[00:57:08.400 --> 00:57:10.520]   Yeah, totally.
[00:57:10.520 --> 00:57:16.920]   When we write a grant, there's no imperative as much as I think it'd be great to say, "By
[00:57:16.920 --> 00:57:23.120]   the way, one of the outcomes if you're writing code has to be that this is going on GitHub
[00:57:23.120 --> 00:57:27.680]   and that it's going to have a CI/CD, like a Travis attached to it so that when pull
[00:57:27.680 --> 00:57:31.680]   requests come in, you know whether they're going to be working or not."
[00:57:31.680 --> 00:57:33.000]   There's none of that at all.
[00:57:33.000 --> 00:57:37.880]   So if you do any of it, you're doing it out of the goodness of your heart at zeroth order.
[00:57:37.880 --> 00:57:43.040]   But as you know, first order, it's because you're doing it to help yourself in the future.
[00:57:43.040 --> 00:57:47.760]   Oftentimes in a research context, and this gets back to, I think a question you're asking
[00:57:47.760 --> 00:57:52.580]   about do you need to hire ML people to work with massive amounts of data?
[00:57:52.580 --> 00:57:55.760]   What I was going to say is that not all of what we do is massive data.
[00:57:55.760 --> 00:58:00.440]   Astronomy has a lot of data, but we have like only a small number of labels, for instance.
[00:58:00.440 --> 00:58:04.080]   So we have like, it's a big data problem, but actually a tiny number of labels for the
[00:58:04.080 --> 00:58:08.260]   kinds of stuff that I'm interested in, or zero labels.
[00:58:08.260 --> 00:58:12.440]   So how do you do one-shot learning is a really interesting kind of problem in a physical
[00:58:12.440 --> 00:58:16.940]   context in the presence of noise and uncertainty and model uncertainty.
[00:58:16.940 --> 00:58:22.200]   There's lots of questions that we ask in the context of ML that are actually kind of smallish
[00:58:22.200 --> 00:58:28.500]   data problems, or they're large computation problems because the forward model is extremely
[00:58:28.500 --> 00:58:33.960]   expensive and requires a supercomputer, but the amount of data we're dealing with is thumb
[00:58:33.960 --> 00:58:36.680]   drive level.
[00:58:36.680 --> 00:58:46.820]   But because of that, we tend to atomize our activities around projects, around papers.
[00:58:46.820 --> 00:58:50.920]   So I write a paper with a student, we figure out a cool new thing to do in the machine
[00:58:50.920 --> 00:58:52.680]   learning context.
[00:58:52.680 --> 00:58:59.060]   And unless that is going to be like a major new widget that gets plugged into some new
[00:58:59.060 --> 00:59:04.520]   facility or existing facility, then it's just out there in the world and people can write
[00:59:04.520 --> 00:59:07.560]   papers saying their scaling curve is better than my scaling curve.
[00:59:07.560 --> 00:59:11.140]   And we can have an argument in a conference one day.
[00:59:11.140 --> 00:59:15.180]   That's sort of the end of that code base, right?
[00:59:15.180 --> 00:59:22.340]   Whereas as you know, in the industry world, you're generally not writing code as a one-off
[00:59:22.340 --> 00:59:25.340]   and then just casting it aside.
[00:59:25.340 --> 00:59:32.620]   So the incentives there to keep things maintainable, keep things up to the latest versions of Python
[00:59:32.620 --> 00:59:36.340]   and blah, blah, blah, they just really aren't there for most of what we do.
[00:59:36.340 --> 00:59:41.020]   There is a subset of what we do where it absolutely has to be battle tested because more and more
[00:59:41.020 --> 00:59:44.220]   people are going to be downloading it and using it.
[00:59:44.220 --> 00:59:49.480]   I tend to see those projects as extremely exciting, but there's not a lot of, I'd say
[00:59:49.480 --> 01:00:00.340]   astronomers who have the experience with full CI/CD pipelines and in production DevOps that
[01:00:00.340 --> 01:00:01.860]   I've been lucky to have in my career.
[01:00:01.860 --> 01:00:03.620]   Well, let me ask you this.
[01:00:03.620 --> 01:00:06.100]   What does your lab's tech stack look like?
[01:00:06.100 --> 01:00:08.180]   Are you using like PyTorch?
[01:00:08.180 --> 01:00:10.180]   What's your standard tooling?
[01:00:10.180 --> 01:00:11.180]   Are you on Python 27?
[01:00:11.180 --> 01:00:16.100]   Well, it's actually been pretty agnostic as students have come in because students tend
[01:00:16.100 --> 01:00:20.340]   to gravitate towards me who are interested in ML and I naturally gravitate towards them.
[01:00:20.340 --> 01:00:22.540]   That's how I guess gravity works.
[01:00:22.540 --> 01:00:27.540]   I've been agnostic to whether it's their TensorFlow land or PyTorch land.
[01:00:27.540 --> 01:00:33.740]   I think that's becoming less and less important as TensorFlow has evolved sort of more towards
[01:00:33.740 --> 01:00:35.980]   the PyTorch way of thinking about the problem.
[01:00:35.980 --> 01:00:44.040]   But if you said, "Build me an ML thing right now," I'd probably start in Keras just based
[01:00:44.040 --> 01:00:46.720]   on my own past experience.
[01:00:46.720 --> 01:00:51.620]   But obviously I'm looking at code with PyTorch and PyTorch Lightning, I think is from a teaching
[01:00:51.620 --> 01:00:56.340]   perspective, that's the last time that I had to teach some ML, I was doing it in PyTorch
[01:00:56.340 --> 01:00:57.340]   Lightning.
[01:00:57.340 --> 01:01:02.940]   Although I had a notebook in Keras and I reproduced the same thing in PyTorch Lightning.
[01:01:02.940 --> 01:01:05.420]   And of course we had weights and biases there as well for-
[01:01:05.420 --> 01:01:06.420]   Oh, nice.
[01:01:06.420 --> 01:01:07.420]   That really warms my heart.
[01:01:07.420 --> 01:01:11.500]   I've been introducing a new cohort of people to your product.
[01:01:11.500 --> 01:01:12.500]   Thank you.
[01:01:12.500 --> 01:01:16.300]   So that's obviously top of the stack.
[01:01:16.300 --> 01:01:19.140]   It very much is a Pythonic world now.
[01:01:19.140 --> 01:01:23.740]   And as I was saying before, in this other large project, which is called Sky Portal,
[01:01:23.740 --> 01:01:28.820]   that we're using as an interaction platform with now hundreds of people are using it on
[01:01:28.820 --> 01:01:34.300]   daily basis, looking at real data as it's flowing in and interacting over individual
[01:01:34.300 --> 01:01:35.300]   objects.
[01:01:35.300 --> 01:01:37.780]   That tech stack is obviously more complicated.
[01:01:37.780 --> 01:01:42.060]   There is a component of it, which is slightly external to the stuff that I built in my group,
[01:01:42.060 --> 01:01:48.740]   as part of our project, which is more or less a large MongoDB engine that's dealing with
[01:01:48.740 --> 01:01:51.180]   terabytes of data.
[01:01:51.180 --> 01:01:55.940]   And there's a bunch of ML plugins to that, that run in real time.
[01:01:55.940 --> 01:01:58.860]   And that's, I think, using TensorFlow.
[01:01:58.860 --> 01:02:05.380]   And then what we've built is essentially a tornado base, API first backend, and it attaches
[01:02:05.380 --> 01:02:07.980]   to a really large Postgres database.
[01:02:07.980 --> 01:02:10.020]   And on the front end is React.
[01:02:10.020 --> 01:02:11.020]   Cool.
[01:02:11.020 --> 01:02:13.460]   Well, we're almost out of time.
[01:02:13.460 --> 01:02:16.940]   Maybe we're even over time, but we always end with two questions that I'd love to ask
[01:02:16.940 --> 01:02:17.940]   you.
[01:02:17.940 --> 01:02:23.540]   And the second to last is basically, is there a topic in ML that you think should be studied
[01:02:23.540 --> 01:02:24.540]   more than it is?
[01:02:24.540 --> 01:02:29.860]   Is there something that you would look into if you had extra time on your hands?
[01:02:29.860 --> 01:02:30.860]   Yeah.
[01:02:30.860 --> 01:02:34.860]   I mean, there are a lot of things I wish I had more time for.
[01:02:34.860 --> 01:02:41.180]   Where I think there needs to be more work in the ML world is around UQ, uncertainty
[01:02:41.180 --> 01:02:42.180]   quantification.
[01:02:42.180 --> 01:02:47.620]   Astronomy and physics works in a world that's sensor-based, fundamentally, in terms of our
[01:02:47.620 --> 01:02:48.620]   observations.
[01:02:48.620 --> 01:02:50.700]   And because it's sensor-based, there's noise.
[01:02:50.700 --> 01:02:58.220]   So unlike in the kind of AlphaGo, Atari world, where every pixel has a perfect measurement,
[01:02:58.220 --> 01:03:03.460]   if you take an image of the sky, you measure some time series, there's noise associated
[01:03:03.460 --> 01:03:04.460]   with it.
[01:03:04.460 --> 01:03:09.260]   And because there's noise, and because there's a finite amount of training data, if you build
[01:03:09.260 --> 01:03:14.760]   models off of that, you get uncertainties in the models because of its lack of expressiveness
[01:03:14.760 --> 01:03:18.140]   or its overgeneralization or overfitting.
[01:03:18.140 --> 01:03:23.700]   And then you also have a source of uncertainty in what it is that you're trying to understand,
[01:03:23.700 --> 01:03:26.540]   just because fundamentally, you don't have a perfect measurement.
[01:03:26.540 --> 01:03:28.100]   Your signal noise isn't perfect.
[01:03:28.100 --> 01:03:32.100]   And I see some of that research, again, coming out of the ML world, but I see some of the
[01:03:32.100 --> 01:03:38.460]   stuff I'm most interested in is coming out of the physics and astronomy meets ML world.
[01:03:38.460 --> 01:03:41.540]   And I'd love to see more of that more broadly.
[01:03:41.540 --> 01:03:48.100]   I think it's partly our fault as domain scientists for not coming up with the equivalent of grand
[01:03:48.100 --> 01:03:54.380]   challenges like with a protein folding, where if we had this, we would be able to make great
[01:03:54.380 --> 01:03:55.940]   strides.
[01:03:55.940 --> 01:04:01.980]   We need to have those sort of, not just benchmark data sets for other fields to be playing with,
[01:04:01.980 --> 01:04:05.660]   but we also need to be really clear about some of the important questions that we're
[01:04:05.660 --> 01:04:06.780]   asking.
[01:04:06.780 --> 01:04:11.420]   And I think in the end, back to a lot of what we were talking about throughout this whole
[01:04:11.420 --> 01:04:19.820]   interview, doing inference and doing interpretability on the models that we build requires a fundamental
[01:04:19.820 --> 01:04:23.300]   understanding of the noise model of the data.
[01:04:23.300 --> 01:04:27.300]   And without that, nothing of what we do is going to be believable.
[01:04:27.300 --> 01:04:28.300]   Interesting.
[01:04:28.700 --> 01:04:35.380]   I guess that's a good segment to my final question, which is, when you look at making
[01:04:35.380 --> 01:04:40.140]   the machine learning models actually work for you, actually do something useful, what
[01:04:40.140 --> 01:04:43.460]   are the big challenges that you typically run into?
[01:04:43.460 --> 01:04:51.740]   Well, it is a good segue from the previous one because we are struggling, I'd say, as
[01:04:51.740 --> 01:04:58.620]   a community with recognizing that there's this large algorithmic toolkit that has been
[01:04:58.620 --> 01:05:05.020]   developed in the computer vision NLP world that we could just make a couple of modifications
[01:05:05.020 --> 01:05:09.260]   to and do what we're already doing better, faster, and at scale.
[01:05:09.260 --> 01:05:15.100]   And as I was arguing through the middle part of the interview, that isn't where I think
[01:05:15.100 --> 01:05:18.980]   the biggest revolutions are going to come from, or at least I hope that's not where
[01:05:18.980 --> 01:05:22.380]   they come from if ML is going to wind up being involved.
[01:05:22.380 --> 01:05:29.060]   So one of the harder problems is articulating what are the really hard problems in astronomy
[01:05:29.060 --> 01:05:35.620]   that can only be solved with new ML tools or new ML innovation.
[01:05:35.620 --> 01:05:37.980]   And we're all working on it in different ways.
[01:05:37.980 --> 01:05:39.060]   We all have our different biases.
[01:05:39.060 --> 01:05:40.820]   I think we may wind up getting there.
[01:05:40.820 --> 01:05:47.100]   The other one is maybe more practical, which is that it is very hard to put machine learning
[01:05:47.100 --> 01:05:49.020]   into practice.
[01:05:49.020 --> 01:05:55.420]   It's easy to write a paper on machine learning and convince a referee that you're doing pretty
[01:05:55.420 --> 01:05:56.500]   well.
[01:05:56.500 --> 01:06:00.400]   Maybe release some code, maybe have the referee kick the tires on that code.
[01:06:00.400 --> 01:06:02.900]   That's pretty much where we're at as a community.
[01:06:02.900 --> 01:06:08.700]   But trying to get it into a real workflow that affects real people's lives on the other
[01:06:08.700 --> 01:06:14.780]   side of that, there's not a lot of us that have experience with it and no one's really
[01:06:14.780 --> 01:06:16.480]   trained to do it well.
[01:06:16.480 --> 01:06:22.220]   So most of the time when it's done, it's done in an ad hoc way and leveraging some understanding
[01:06:22.220 --> 01:06:24.020]   of how software engineering is supposed to work.
[01:06:24.020 --> 01:06:27.900]   But as you know well, machine learning and production is a very different beast than
[01:06:27.900 --> 01:06:30.060]   ordinary software and production.
[01:06:30.060 --> 01:06:34.500]   And I don't think as a community, we fully grasp how hard it is.
[01:06:34.500 --> 01:06:37.700]   The other side of that, of course, is that because machine learning is so exciting to
[01:06:37.700 --> 01:06:44.640]   so many, we're starting to train a number of students that have just enough knowledge
[01:06:44.640 --> 01:06:46.200]   to be dangerous.
[01:06:46.200 --> 01:06:50.940]   But because again, everything looks like a nail when you've got a new hammer, a lot
[01:06:50.940 --> 01:06:54.760]   of people I think are going off hitting nails that they ought not to be.
[01:06:54.760 --> 01:06:58.900]   And one of the things that I always say when somebody says, "What's the worst thing about
[01:06:58.900 --> 01:06:59.900]   machine learning?"
[01:06:59.900 --> 01:07:02.500]   Is I always say, "It's because you always get an answer."
[01:07:02.500 --> 01:07:07.580]   And especially in the context that we're looking at, if we always get an answer and we're getting
[01:07:07.580 --> 01:07:11.740]   data that's outside of our original domain, or there's some notions of concept drift or
[01:07:11.740 --> 01:07:16.740]   something because the instrument is changing, we don't have any guardrails against that.
[01:07:16.740 --> 01:07:23.380]   Luckily, unlike in many of the fields that your listeners work in, if we make a mistake,
[01:07:23.380 --> 01:07:28.840]   people don't die and we don't blow up billion dollar facilities and things like that.
[01:07:28.840 --> 01:07:34.820]   So we live in a little bit of a nice sandbox where the mistakes that we make may have implications
[01:07:34.820 --> 01:07:40.540]   for lack of good resource allocation, but we still could wind up making statements about
[01:07:40.540 --> 01:07:45.860]   how the universe works that is fundamentally wrong because we don't know enough about what's
[01:07:45.860 --> 01:07:48.020]   happening under the hood.
[01:07:48.020 --> 01:07:50.340]   Well, Josh, thank you so much for your time.
[01:07:50.340 --> 01:07:51.340]   I really appreciate it.
[01:07:51.340 --> 01:07:52.340]   That was super fun.
[01:07:52.340 --> 01:07:53.340]   Oh, this has been great.
[01:07:53.340 --> 01:07:54.340]   Yeah.
[01:07:54.340 --> 01:07:55.340]   Thank you, Lucas.
[01:07:55.340 --> 01:07:56.340]   Great questions.
[01:07:56.340 --> 01:08:00.660]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:08:00.660 --> 01:08:05.380]   to the show notes in the description where you can find links to all the papers that
[01:08:05.380 --> 01:08:09.660]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:08:09.660 --> 01:08:10.660]   produce.
[01:08:10.660 --> 01:08:10.660]   So check it out.
[01:08:10.660 --> 01:08:11.160]   Transcribed by https://otter.ai
[01:08:11.160 --> 01:08:16.160]   Transcribed by https://otter.ai

