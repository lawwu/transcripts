
[00:00:00.000 --> 00:00:03.740]   what they achieved is singular, never been done before.
[00:00:03.740 --> 00:00:06.900]   Just to put in perspective, 100,000 GPUs,
[00:00:06.900 --> 00:00:10.300]   that's easily the fastest supercomputer on the planet.
[00:00:10.300 --> 00:00:11.400]   That's one cluster.
[00:00:11.400 --> 00:00:16.060]   A supercomputer that you would build
[00:00:16.060 --> 00:00:18.860]   would take normally three years to plan,
[00:00:18.860 --> 00:00:21.780]   and then they deliver the equipment,
[00:00:21.780 --> 00:00:25.720]   and it takes one year to get it all working.
[00:00:25.720 --> 00:00:28.900]   We're talking about 19 days.
[00:00:29.260 --> 00:00:31.840]   (upbeat music)
[00:00:31.840 --> 00:00:43.420]   - Jensen, nice glasses.
[00:00:43.420 --> 00:00:45.100]   - Hey, yeah, you too.
[00:00:45.100 --> 00:00:46.300]   - It's great to be with you.
[00:00:46.300 --> 00:00:48.460]   - Yeah, I got my ugly glasses on just like you.
[00:00:48.460 --> 00:00:49.700]   - Come on, those aren't ugly.
[00:00:49.700 --> 00:00:51.060]   These are pretty good.
[00:00:51.060 --> 00:00:52.360]   Do you like the red ones better?
[00:00:52.360 --> 00:00:54.420]   - There's something only your family could love.
[00:00:54.420 --> 00:00:55.460]   (laughing)
[00:00:55.460 --> 00:00:57.700]   - Well, it's Friday, October 4th.
[00:00:57.700 --> 00:00:59.020]   We're at the NVIDIA headquarters
[00:00:59.020 --> 00:01:00.940]   just down the street from Altimeter.
[00:01:00.940 --> 00:01:01.780]   - Welcome.
[00:01:01.780 --> 00:01:03.220]   - Thank you, thank you.
[00:01:03.220 --> 00:01:04.900]   And we have our investor meeting,
[00:01:04.900 --> 00:01:07.220]   our annual investor meeting on Monday,
[00:01:07.220 --> 00:01:10.100]   where we're gonna debate all the consequences of AI,
[00:01:10.100 --> 00:01:11.940]   how fast we're scaling intelligence.
[00:01:11.940 --> 00:01:13.680]   And I couldn't think of anybody better, really,
[00:01:13.680 --> 00:01:15.260]   to kick it off with than you.
[00:01:15.260 --> 00:01:16.380]   - I appreciate that.
[00:01:16.380 --> 00:01:18.980]   - As both a shareholder, as a thought partner,
[00:01:18.980 --> 00:01:22.260]   kicking ideas back and forth, you really make us smarter.
[00:01:22.260 --> 00:01:24.140]   And we're just grateful for the friendship.
[00:01:24.140 --> 00:01:25.220]   So thanks for being here.
[00:01:25.220 --> 00:01:26.480]   - Happy to be here.
[00:01:26.480 --> 00:01:28.340]   - You know, this year, the theme
[00:01:28.340 --> 00:01:31.060]   is scaling intelligence to AGI.
[00:01:31.060 --> 00:01:32.340]   And it's pretty mind-boggling
[00:01:32.340 --> 00:01:33.940]   that when we did this two years ago,
[00:01:33.940 --> 00:01:35.920]   we did it on the age of AI,
[00:01:35.920 --> 00:01:38.300]   and that was two months before Chat GPT,
[00:01:38.300 --> 00:01:39.660]   and to think about all that's changed.
[00:01:39.660 --> 00:01:42.300]   So I thought we would kick it off with a thought experiment
[00:01:42.300 --> 00:01:44.020]   and maybe a prediction.
[00:01:44.020 --> 00:01:46.740]   If I colloquially think of AGI
[00:01:46.740 --> 00:01:49.600]   as that personal assistant in my pocket.
[00:01:49.600 --> 00:01:51.100]   (laughing)
[00:01:51.100 --> 00:01:54.220]   If I think of AGI as that colloquial assistant in my pocket.
[00:01:54.220 --> 00:01:55.060]   - Oh, getting used to it.
[00:01:55.060 --> 00:01:55.980]   - Exactly.
[00:01:55.980 --> 00:01:56.820]   - Yeah.
[00:01:56.820 --> 00:01:59.300]   - You know, that knows everything about me.
[00:01:59.300 --> 00:02:00.740]   That has perfect memory of me.
[00:02:00.740 --> 00:02:02.940]   That can communicate with me.
[00:02:02.940 --> 00:02:04.260]   That can book a hotel for me,
[00:02:04.260 --> 00:02:06.960]   or maybe book a doctor's appointment for me.
[00:02:06.960 --> 00:02:09.940]   When you look at the rate of change in the world today,
[00:02:09.940 --> 00:02:11.700]   when do you think we're going to have
[00:02:11.700 --> 00:02:13.980]   that personal assistant in our pocket?
[00:02:13.980 --> 00:02:17.900]   - Soon, in some form.
[00:02:17.900 --> 00:02:18.740]   - Yeah.
[00:02:18.740 --> 00:02:19.940]   - Yeah, soon in some form.
[00:02:19.940 --> 00:02:24.940]   And that assistant will get better over time.
[00:02:25.780 --> 00:02:28.260]   That's the beauty of technology as we know it.
[00:02:28.260 --> 00:02:32.220]   So I think in the beginning it'll be quite useful,
[00:02:32.220 --> 00:02:33.260]   but not perfect.
[00:02:33.260 --> 00:02:35.140]   And then it gets more and more perfect over time,
[00:02:35.140 --> 00:02:36.300]   like all technology.
[00:02:36.300 --> 00:02:38.260]   - When we look at the rate of change,
[00:02:38.260 --> 00:02:39.560]   I think Elon has said,
[00:02:39.560 --> 00:02:42.260]   "The only thing that really matters is rate of change."
[00:02:42.260 --> 00:02:45.380]   It sure feels to us like the rate of change
[00:02:45.380 --> 00:02:47.140]   has accelerated dramatically,
[00:02:47.140 --> 00:02:49.860]   is the fastest rate of change we've ever seen
[00:02:49.860 --> 00:02:50.720]   on these questions.
[00:02:50.720 --> 00:02:52.380]   Because we've been around the rim like you
[00:02:52.380 --> 00:02:55.420]   on AI for a decade now.
[00:02:55.420 --> 00:02:57.260]   You even longer.
[00:02:57.260 --> 00:02:59.100]   Is this the fastest rate of change
[00:02:59.100 --> 00:03:00.500]   you've seen in your career?
[00:03:00.500 --> 00:03:03.980]   - It is because we've reinvented computing.
[00:03:03.980 --> 00:03:06.620]   You know, a lot of this is happening
[00:03:06.620 --> 00:03:10.420]   because we drove the marginal cost of computing down
[00:03:10.420 --> 00:03:14.460]   by 100,000X over the course of 10 years.
[00:03:14.460 --> 00:03:16.980]   Moore's law would have been about 100X.
[00:03:16.980 --> 00:03:19.100]   And we did it in several ways.
[00:03:19.100 --> 00:03:22.080]   We did it by one, introducing accelerated computing,
[00:03:22.080 --> 00:03:27.080]   taking what is work that is not very effective on CPUs
[00:03:27.080 --> 00:03:29.480]   and put it on top of GPUs.
[00:03:29.480 --> 00:03:33.320]   We did it by inventing new numerical precisions.
[00:03:33.320 --> 00:03:37.160]   We did it by new architectures, inventing a tensor core.
[00:03:37.160 --> 00:03:40.880]   The way systems are formulated, NVLink,
[00:03:40.880 --> 00:03:45.880]   added insanely fast memories, HBM,
[00:03:45.880 --> 00:03:50.680]   and scaling things up with NVLink and InfiniBand,
[00:03:51.640 --> 00:03:54.000]   and working across the entire stack.
[00:03:54.000 --> 00:03:57.080]   Basically, everything that I describe
[00:03:57.080 --> 00:03:59.040]   about how NVIDIA does things
[00:03:59.040 --> 00:04:03.880]   led to a super Moore's law rate of innovation.
[00:04:03.880 --> 00:04:05.920]   Now, the thing that's really amazing
[00:04:05.920 --> 00:04:07.840]   is that as a result of that,
[00:04:07.840 --> 00:04:12.320]   we went from human programming to machine learning.
[00:04:12.320 --> 00:04:13.900]   And the amazing thing about machine learning
[00:04:13.900 --> 00:04:16.840]   is that machine learning can learn pretty fast,
[00:04:16.840 --> 00:04:17.800]   as it turns out.
[00:04:17.800 --> 00:04:22.400]   And so as we reformulated the way we distribute computing,
[00:04:22.400 --> 00:04:26.780]   we did a lot of parallelism of all kinds, right?
[00:04:26.780 --> 00:04:28.360]   Tensor parallelism, pipeline parallelism,
[00:04:28.360 --> 00:04:30.480]   parallelism of all kinds.
[00:04:30.480 --> 00:04:35.480]   And we became good at inventing new algorithms
[00:04:35.480 --> 00:04:38.600]   on top of that, and new training methods,
[00:04:38.600 --> 00:04:41.900]   and all of this invention is compounding
[00:04:41.900 --> 00:04:44.280]   on top of each other as a result, right?
[00:04:44.280 --> 00:04:45.820]   And back in the old days,
[00:04:45.820 --> 00:04:48.520]   if you look at the way Moore's law was working,
[00:04:48.520 --> 00:04:50.640]   the software was static.
[00:04:50.640 --> 00:04:51.680]   - Right.
[00:04:51.680 --> 00:04:53.400]   - It was pre-compiled, it was shrink-wrapped,
[00:04:53.400 --> 00:04:55.480]   put into a store, it was static.
[00:04:55.480 --> 00:04:59.640]   And the hardware underneath was growing at Moore's law rate.
[00:04:59.640 --> 00:05:01.760]   Now we've got the whole stack growing, right?
[00:05:01.760 --> 00:05:03.080]   Innovating across the whole stack.
[00:05:03.080 --> 00:05:04.680]   And so I think that that's the,
[00:05:04.680 --> 00:05:07.360]   now all of a sudden we're seeing scaling.
[00:05:07.360 --> 00:05:11.000]   That is extraordinary, of course.
[00:05:11.020 --> 00:05:15.820]   But we used to talk about pre-trained models
[00:05:15.820 --> 00:05:17.820]   and scaling at that level,
[00:05:17.820 --> 00:05:20.620]   and how we're doubling the model size,
[00:05:20.620 --> 00:05:21.980]   and doubling therefore appropriately,
[00:05:21.980 --> 00:05:23.540]   and doubling the data size.
[00:05:23.540 --> 00:05:26.780]   And as a result, the computing capacity necessary
[00:05:26.780 --> 00:05:29.500]   is increasing by a factor of four every year.
[00:05:29.500 --> 00:05:30.320]   - Right.
[00:05:30.320 --> 00:05:31.160]   - That was a big deal.
[00:05:31.160 --> 00:05:32.000]   - Right.
[00:05:32.000 --> 00:05:35.180]   - But now we're seeing scaling with post-training,
[00:05:35.180 --> 00:05:37.060]   and we're seeing scaling at inference.
[00:05:37.060 --> 00:05:37.900]   Isn't that right?
[00:05:37.900 --> 00:05:38.740]   - Right.
[00:05:38.740 --> 00:05:40.660]   - And so people used to think that pre-training
[00:05:40.660 --> 00:05:43.120]   was hard and inference was easy.
[00:05:43.120 --> 00:05:44.440]   Now everything is hard.
[00:05:44.440 --> 00:05:45.280]   - Right, right.
[00:05:45.280 --> 00:05:46.520]   - Which is kind of sensible.
[00:05:46.520 --> 00:05:51.020]   The idea that all of human thinking is one shot
[00:05:51.020 --> 00:05:52.720]   is kind of ridiculous.
[00:05:52.720 --> 00:05:55.280]   And so there must be a concept of fast thinking,
[00:05:55.280 --> 00:05:58.600]   and slow thinking, and reasoning, and reflection,
[00:05:58.600 --> 00:06:01.360]   and iteration, and simulation, and all that.
[00:06:01.360 --> 00:06:02.880]   And that now it's coming in.
[00:06:02.880 --> 00:06:03.800]   - Yeah.
[00:06:03.800 --> 00:06:05.000]   I think to that point,
[00:06:05.000 --> 00:06:08.080]   one of the most misunderstood things about NVIDIA
[00:06:08.080 --> 00:06:11.540]   is how deep the true NVIDIA moat is, right?
[00:06:11.540 --> 00:06:12.860]   I think there's a notion out there
[00:06:12.860 --> 00:06:17.060]   that as soon as someone invents a new chip,
[00:06:17.060 --> 00:06:19.900]   a better chip, that they've won.
[00:06:19.900 --> 00:06:22.320]   But the truth is you've been spending the past decade
[00:06:22.320 --> 00:06:25.260]   building the full stack from the GPU, to the CPU,
[00:06:25.260 --> 00:06:27.740]   to the networking, and especially the software
[00:06:27.740 --> 00:06:31.460]   and libraries that enable applications to run on NVIDIA.
[00:06:31.460 --> 00:06:32.300]   - Yeah.
[00:06:32.300 --> 00:06:33.980]   - So I think you spoke to that.
[00:06:34.000 --> 00:06:39.000]   But when you think about NVIDIA's moat today, right?
[00:06:39.000 --> 00:06:42.440]   Do you think NVIDIA's moat today is greater
[00:06:42.440 --> 00:06:45.840]   or smaller than it was three to four years ago?
[00:06:45.840 --> 00:06:49.520]   - Well, I appreciate you recognizing
[00:06:49.520 --> 00:06:50.800]   how computing has changed.
[00:06:50.800 --> 00:06:53.560]   In fact, the reason why people thought,
[00:06:53.560 --> 00:06:57.440]   and many still do, that you designed a better chip,
[00:06:57.440 --> 00:07:00.440]   it has more flops, has more flips, and flops,
[00:07:00.440 --> 00:07:02.400]   and bits, and bytes, you know what I'm saying?
[00:07:02.400 --> 00:07:03.240]   - Yeah.
[00:07:03.240 --> 00:07:05.920]   - You see their keynote slides,
[00:07:05.920 --> 00:07:07.400]   and it's got all these flips and flops,
[00:07:07.400 --> 00:07:09.560]   and bar charts, and things like that.
[00:07:09.560 --> 00:07:10.480]   And that's all good.
[00:07:10.480 --> 00:07:13.640]   I mean, look, horsepower does matter.
[00:07:13.640 --> 00:07:14.460]   - Yes.
[00:07:14.460 --> 00:07:17.040]   - So these things fundamentally do matter.
[00:07:17.040 --> 00:07:22.040]   However, unfortunately, that's old thinking.
[00:07:22.040 --> 00:07:25.200]   It is old thinking in the sense that the software
[00:07:25.200 --> 00:07:29.120]   was some application running on Windows,
[00:07:29.120 --> 00:07:30.840]   and the software is static.
[00:07:30.840 --> 00:07:31.660]   - Right.
[00:07:31.660 --> 00:07:33.940]   - Which means that the best way for you
[00:07:33.940 --> 00:07:38.100]   to improve the system is just making faster and faster chips.
[00:07:38.100 --> 00:07:41.780]   But we realized that machine learning
[00:07:41.780 --> 00:07:43.400]   is not human programming.
[00:07:43.400 --> 00:07:48.060]   Machine learning is not about just the software.
[00:07:48.060 --> 00:07:50.020]   It's about the entire data pipeline.
[00:07:50.020 --> 00:07:53.740]   It's about, in fact, the flywheel of machine learning
[00:07:53.740 --> 00:07:54.580]   is the most important thing.
[00:07:54.580 --> 00:07:59.100]   So how do you think about enabling this flywheel
[00:07:59.100 --> 00:08:02.820]   on the one hand, and enabling data scientists
[00:08:02.820 --> 00:08:06.340]   and researchers to be productive in this flywheel?
[00:08:06.340 --> 00:08:11.220]   And that flywheel starts at the very, very beginning.
[00:08:11.220 --> 00:08:13.340]   A lot of people don't even realize
[00:08:13.340 --> 00:08:18.340]   that it takes AI to curate data to teach an AI.
[00:08:18.340 --> 00:08:20.820]   And that AI alone is pretty complicated.
[00:08:20.820 --> 00:08:22.740]   - And is that AI itself is improving?
[00:08:22.740 --> 00:08:24.660]   Is it also accelerating?
[00:08:24.660 --> 00:08:26.380]   You know, again, when we think about
[00:08:26.380 --> 00:08:28.580]   the competitive advantage, right?
[00:08:28.580 --> 00:08:30.500]   It's combinatorial of all these systems.
[00:08:30.500 --> 00:08:32.140]   - It's exactly, exactly.
[00:08:32.140 --> 00:08:34.020]   And that was exactly gonna lead to that.
[00:08:34.020 --> 00:08:38.180]   Because of smarter AIs to curate the data,
[00:08:38.180 --> 00:08:40.740]   we now even have synthetic data generation
[00:08:40.740 --> 00:08:43.920]   and all kinds of different ways of curating data,
[00:08:43.920 --> 00:08:47.460]   presenting data to, and so before you even get the training,
[00:08:47.460 --> 00:08:50.900]   you've got massive amounts of data processing involved.
[00:08:50.900 --> 00:08:54.480]   And so people think about, oh, PyTorch,
[00:08:54.480 --> 00:08:55.820]   that's the beginning end of the world,
[00:08:55.820 --> 00:08:57.580]   and it was very important.
[00:08:57.580 --> 00:09:00.700]   But don't forget before PyTorch, there's amount of work.
[00:09:00.700 --> 00:09:02.780]   After PyTorch, there's amount of work.
[00:09:02.780 --> 00:09:05.260]   And the thing about the flywheel
[00:09:05.260 --> 00:09:06.980]   is really the way you ought to think.
[00:09:06.980 --> 00:09:08.980]   How do I think about this entire flywheel?
[00:09:08.980 --> 00:09:11.620]   And how do I design a computing system,
[00:09:11.620 --> 00:09:14.700]   a computing architecture that helps you take this flywheel
[00:09:14.700 --> 00:09:16.660]   and be as effective as possible?
[00:09:16.660 --> 00:09:21.460]   It's not one slice of an application, training.
[00:09:21.460 --> 00:09:22.300]   Does that make sense?
[00:09:22.300 --> 00:09:24.420]   That's just one step, okay?
[00:09:24.420 --> 00:09:27.060]   Every step along that flywheel is hard.
[00:09:27.060 --> 00:09:30.140]   And so the first thing that you should do,
[00:09:30.140 --> 00:09:33.260]   instead of thinking about, how do I make Excel faster?
[00:09:33.260 --> 00:09:35.580]   How do I make, you know, Doom faster?
[00:09:35.580 --> 00:09:37.860]   That was kind of the old days, isn't that right?
[00:09:37.860 --> 00:09:38.700]   Now you have to think about
[00:09:38.700 --> 00:09:40.940]   how do I make this flywheel faster?
[00:09:40.940 --> 00:09:43.780]   And this flywheel has a whole bunch of different steps.
[00:09:43.780 --> 00:09:45.620]   And there's nothing easy about machine learning,
[00:09:45.620 --> 00:09:46.440]   as you guys know.
[00:09:46.440 --> 00:09:49.060]   There's nothing easy about what OpenAI does, or X does,
[00:09:49.060 --> 00:09:51.780]   or Gemini and the team at DeepMind does.
[00:09:51.780 --> 00:09:54.140]   I mean, there's nothing easy about what they do.
[00:09:54.140 --> 00:09:56.420]   And so we decided, look,
[00:09:56.420 --> 00:09:58.860]   this is really what you ought to be thinking about.
[00:09:58.860 --> 00:10:00.660]   This is the entire process.
[00:10:00.660 --> 00:10:03.540]   You want to accelerate every part of that.
[00:10:03.540 --> 00:10:05.580]   You want to respect Amdahl's Law.
[00:10:05.580 --> 00:10:08.100]   You want to, Amdahl's Law would suggest,
[00:10:08.100 --> 00:10:10.940]   well, if this is 30% of the time,
[00:10:10.940 --> 00:10:13.740]   and I accelerated that by a factor of three,
[00:10:13.740 --> 00:10:17.460]   I didn't really accelerate the entire process by that much.
[00:10:17.460 --> 00:10:18.500]   Does that make sense?
[00:10:18.500 --> 00:10:20.860]   And you really want to create a system
[00:10:20.860 --> 00:10:23.300]   that accelerates every single step of that,
[00:10:23.300 --> 00:10:25.060]   because only in doing the whole thing
[00:10:25.060 --> 00:10:29.220]   can you really materially improve that cycle time.
[00:10:29.220 --> 00:10:33.580]   And that flywheel, that rate of learning
[00:10:33.580 --> 00:10:37.380]   is really, in the end, what causes the exponential rise.
[00:10:37.380 --> 00:10:41.780]   And so what I'm trying to say is that our perspective about,
[00:10:41.780 --> 00:10:43.140]   you know, a company's perspective
[00:10:43.140 --> 00:10:45.220]   about what you're really doing
[00:10:45.220 --> 00:10:48.100]   manifests itself into the product.
[00:10:48.100 --> 00:10:50.460]   And notice, I've been talking about this flywheel--
[00:10:50.460 --> 00:10:51.580]   - The entire cycle, yeah.
[00:10:51.580 --> 00:10:52.660]   - That's right.
[00:10:52.660 --> 00:10:54.940]   And we accelerate everything.
[00:10:54.940 --> 00:10:58.660]   Right now, the main focus is video.
[00:10:58.660 --> 00:11:02.620]   A lot of people are focused on physical AI
[00:11:02.620 --> 00:11:04.420]   and video processing.
[00:11:04.420 --> 00:11:06.180]   Just imagine that front end.
[00:11:06.180 --> 00:11:07.020]   - Right.
[00:11:07.020 --> 00:11:10.420]   - The terabytes per second of data
[00:11:10.420 --> 00:11:12.740]   that are coming into the system.
[00:11:12.740 --> 00:11:14.940]   Give me an example of a pipeline
[00:11:14.940 --> 00:11:18.660]   that is going to ingest all of that data,
[00:11:18.660 --> 00:11:21.020]   prepare it for training in the first place.
[00:11:21.020 --> 00:11:23.860]   So that entire thing is CUDA accelerated.
[00:11:23.860 --> 00:11:27.100]   - And people are only thinking about text models today.
[00:11:27.100 --> 00:11:27.940]   - Yeah.
[00:11:27.940 --> 00:11:31.020]   - But the future is, you know, this video models,
[00:11:31.020 --> 00:11:33.100]   as well as, you know, using, you know,
[00:11:33.100 --> 00:11:35.580]   some of these text models, like O1,
[00:11:35.580 --> 00:11:37.580]   to really process a lot of that data
[00:11:37.580 --> 00:11:38.820]   before we even get there.
[00:11:38.820 --> 00:11:39.660]   - Yeah. - Right?
[00:11:39.660 --> 00:11:40.500]   - Yeah, yeah.
[00:11:40.500 --> 00:11:43.540]   Language models are gonna be involved in everything.
[00:11:43.540 --> 00:11:48.020]   It took the industry enormous technology and effort
[00:11:48.020 --> 00:11:49.060]   to train a language model,
[00:11:49.060 --> 00:11:50.580]   to train these large language models.
[00:11:50.580 --> 00:11:52.060]   Now we're using a large language model
[00:11:52.060 --> 00:11:53.980]   in every single step of the way.
[00:11:53.980 --> 00:11:56.180]   It's pretty phenomenal.
[00:11:56.180 --> 00:11:58.580]   - I don't mean to be overly simplistic about this,
[00:11:58.580 --> 00:12:00.460]   but again, you know,
[00:12:00.460 --> 00:12:03.700]   we hear it all the time from investors, right?
[00:12:03.700 --> 00:12:06.700]   Yes, but what about custom ASICs?
[00:12:06.700 --> 00:12:08.780]   Yes, but their competitive mode
[00:12:08.780 --> 00:12:10.260]   is going to be pierced by this.
[00:12:10.260 --> 00:12:14.180]   What I hear you saying is that in a combinatorial system,
[00:12:14.180 --> 00:12:16.380]   the advantage grows over time.
[00:12:16.380 --> 00:12:20.140]   So I heard you say that our advantage is greater today
[00:12:20.140 --> 00:12:21.580]   than it was three to four years ago
[00:12:21.580 --> 00:12:24.620]   because we're improving every component
[00:12:24.620 --> 00:12:26.340]   and that's combinatorial.
[00:12:26.340 --> 00:12:29.580]   Is that, you know, when you think about, for example,
[00:12:29.580 --> 00:12:33.220]   as a business case study, Intel, right?
[00:12:33.220 --> 00:12:36.580]   Who had a dominant mode, a dominant position in the stack
[00:12:36.580 --> 00:12:38.900]   relative to where you are today.
[00:12:38.900 --> 00:12:41.980]   Perhaps just, you know, again, boil it down a little bit.
[00:12:41.980 --> 00:12:45.580]   You know, compare, contrast your competitive advantage
[00:12:45.580 --> 00:12:47.740]   to maybe the competitive advantage they had
[00:12:47.740 --> 00:12:49.940]   at the peak of their cycle.
[00:12:49.940 --> 00:12:53.420]   Well, Intel is extraordinary.
[00:12:53.420 --> 00:12:56.940]   Intel is extraordinary because they were probably
[00:12:56.940 --> 00:13:01.940]   the first company that was incredibly good
[00:13:01.940 --> 00:13:07.460]   at manufacturing, process engineering, manufacturing,
[00:13:07.460 --> 00:13:12.660]   and that one click above manufacturing,
[00:13:12.660 --> 00:13:14.420]   which is building the chip.
[00:13:14.420 --> 00:13:15.420]   Right.
[00:13:15.420 --> 00:13:18.580]   And designing the chip and architecting the chip
[00:13:19.460 --> 00:13:23.020]   in the x86 architecture
[00:13:23.020 --> 00:13:25.820]   and building faster and faster x86 chips.
[00:13:25.820 --> 00:13:27.140]   That was their brilliance.
[00:13:27.140 --> 00:13:29.540]   And they fused that with manufacturing.
[00:13:29.540 --> 00:13:34.220]   Our company is a little different in the sense that,
[00:13:34.220 --> 00:13:38.820]   and we recognize this, that in fact, parallel processing
[00:13:38.820 --> 00:13:42.140]   doesn't require every transistor to be excellent.
[00:13:42.140 --> 00:13:45.180]   Serial processing requires every transistor to be excellent.
[00:13:45.180 --> 00:13:48.980]   Parallel processing requires lots and lots of transistors
[00:13:48.980 --> 00:13:50.780]   to be more cost-effective.
[00:13:50.780 --> 00:13:55.780]   I'd rather have 10 times more transistors, 20% slower,
[00:13:55.780 --> 00:13:59.700]   than 10 times less transistors, 20% faster.
[00:13:59.700 --> 00:14:00.660]   Does that make sense?
[00:14:00.660 --> 00:14:02.940]   They were like the opposite.
[00:14:02.940 --> 00:14:04.340]   And so single-threaded performance,
[00:14:04.340 --> 00:14:06.780]   single-threaded processing and parallel processing
[00:14:06.780 --> 00:14:07.620]   was very different.
[00:14:07.620 --> 00:14:10.780]   And so we observed that, in fact, our world
[00:14:10.780 --> 00:14:13.260]   is not about being better going down.
[00:14:13.260 --> 00:14:16.420]   We want to be very good, as good as we can be.
[00:14:16.420 --> 00:14:19.780]   But our world is really about much better going up.
[00:14:19.780 --> 00:14:22.300]   Parallel computing, parallel processing is hard
[00:14:22.300 --> 00:14:27.300]   because every single algorithm requires a different way
[00:14:27.300 --> 00:14:30.860]   of refactoring and re-architecting the algorithm
[00:14:30.860 --> 00:14:32.500]   for the architecture.
[00:14:32.500 --> 00:14:35.540]   What people don't realize is that you can have
[00:14:35.540 --> 00:14:37.620]   three different ISAs, CPU ISAs.
[00:14:37.620 --> 00:14:39.060]   They all have their own C compilers.
[00:14:39.060 --> 00:14:42.140]   You could take software and compile down to the ISA.
[00:14:42.140 --> 00:14:43.820]   That's not possible in accelerated computing.
[00:14:43.820 --> 00:14:45.580]   That's not possible in parallel computing.
[00:14:45.580 --> 00:14:47.340]   The company who comes up with the architecture
[00:14:47.340 --> 00:14:50.700]   has to come up with their own OpenGL.
[00:14:50.700 --> 00:14:52.900]   So we revolutionized deep learning
[00:14:52.900 --> 00:14:56.900]   because of our domain-specific library called CUDNN.
[00:14:56.900 --> 00:14:58.660]   Without CUDNN, nobody talks about CUDNN
[00:14:58.660 --> 00:15:03.180]   because it's one layer underneath PyTorch and TensorFlow
[00:15:03.180 --> 00:15:08.180]   and back in the old days, CAFE and Theano and now Triton.
[00:15:08.180 --> 00:15:10.900]   There's a whole bunch of different frameworks.
[00:15:10.900 --> 00:15:14.140]   So that domain-specific library, CUDNN,
[00:15:14.140 --> 00:15:16.580]   a domain-specific library called Optics,
[00:15:16.580 --> 00:15:20.060]   we have a domain-specific library called Quantum,
[00:15:20.060 --> 00:15:24.380]   Rapids, the list of aerial for--
[00:15:24.380 --> 00:15:28.260]   - Industry-specific algorithms that sit below
[00:15:28.260 --> 00:15:30.180]   that PyTorch layer that everybody's focused on.
[00:15:30.180 --> 00:15:33.300]   Like I've heard oftentimes, well, if LLMs--
[00:15:33.300 --> 00:15:37.580]   - If we didn't invent that, no application on top could work.
[00:15:37.580 --> 00:15:38.820]   You guys understand what I'm saying?
[00:15:38.820 --> 00:15:40.860]   So the mathematics is really,
[00:15:40.860 --> 00:15:43.020]   what NVIDIA is really good at is algorithm.
[00:15:43.020 --> 00:15:47.060]   That fusion between the science above,
[00:15:47.060 --> 00:15:48.940]   the architecture on the bottom,
[00:15:48.940 --> 00:15:50.900]   that's what we're really good at, yeah.
[00:15:50.900 --> 00:15:55.420]   - There's all this attention now on inference, finally.
[00:15:55.420 --> 00:16:00.100]   But I remember two years ago, Brad and I had dinner with you
[00:16:00.100 --> 00:16:01.740]   and we asked you the question,
[00:16:01.740 --> 00:16:06.700]   "Do you think your moat will be as strong in inference
[00:16:06.700 --> 00:16:08.100]   "as it is in training?"
[00:16:08.100 --> 00:16:11.820]   - Yeah, and I'm sure I said it would be greater.
[00:16:11.820 --> 00:16:14.420]   - Yeah, yeah, and you touched upon
[00:16:14.420 --> 00:16:16.100]   a lot of these elements just now,
[00:16:16.100 --> 00:16:18.300]   just the composability between,
[00:16:18.300 --> 00:16:22.220]   or we don't know the total mix at one point,
[00:16:22.220 --> 00:16:24.180]   and to a customer, it's very important
[00:16:24.180 --> 00:16:26.460]   to be able to be flexible in between.
[00:16:26.460 --> 00:16:27.700]   - That's right.
[00:16:27.700 --> 00:16:29.300]   - But can you just touch upon,
[00:16:29.300 --> 00:16:32.300]   now that we're in this era of inference?
[00:16:32.300 --> 00:16:36.660]   - It was inference, training is inferencing at scale.
[00:16:36.660 --> 00:16:38.060]   I mean, you're right.
[00:16:38.060 --> 00:16:42.420]   And so if you train well,
[00:16:42.420 --> 00:16:44.540]   it is very likely you'll inference well.
[00:16:44.540 --> 00:16:46.780]   If you built it on this architecture
[00:16:46.780 --> 00:16:48.100]   without any consideration,
[00:16:48.100 --> 00:16:50.260]   it will run on this architecture.
[00:16:50.260 --> 00:16:53.460]   You could still go and optimize it for other architectures,
[00:16:53.460 --> 00:16:54.860]   but at the very minimum,
[00:16:54.860 --> 00:16:57.100]   since it's already been architected,
[00:16:57.100 --> 00:16:59.340]   built on NVIDIA, it will run on NVIDIA.
[00:16:59.340 --> 00:17:01.460]   Now, the other aspect, of course,
[00:17:01.460 --> 00:17:05.900]   it's just kind of capital investment aspect,
[00:17:05.900 --> 00:17:08.500]   which is when you're training new models,
[00:17:08.500 --> 00:17:13.500]   you want your best new gear to be used for training,
[00:17:13.500 --> 00:17:18.300]   which leaves behind gear that you used yesterday.
[00:17:18.300 --> 00:17:20.980]   Well, that gear is perfect for inference.
[00:17:20.980 --> 00:17:25.140]   And so there's a trail of free gear.
[00:17:25.140 --> 00:17:27.780]   There's a trail of free infrastructure
[00:17:27.780 --> 00:17:30.780]   behind the new infrastructure that's CUDA compatible.
[00:17:30.780 --> 00:17:32.820]   And so we're very disciplined
[00:17:32.820 --> 00:17:37.220]   about making sure that we're compatible throughout,
[00:17:37.220 --> 00:17:39.980]   so that everything that we leave behind
[00:17:39.980 --> 00:17:41.380]   will continue to be excellent.
[00:17:41.380 --> 00:17:42.820]   Now, we also put a lot of energy
[00:17:42.820 --> 00:17:45.340]   into continuously reinventing new algorithms,
[00:17:45.340 --> 00:17:48.740]   so that when the time comes,
[00:17:48.740 --> 00:17:52.580]   the Hopper architecture is two, three, four times better
[00:17:52.580 --> 00:17:54.180]   than when they bought it,
[00:17:54.180 --> 00:17:57.980]   so that infrastructure continues to be really effective.
[00:17:57.980 --> 00:18:00.140]   And so all of the work that we do,
[00:18:00.140 --> 00:18:02.340]   improving new algorithms, new frameworks,
[00:18:02.340 --> 00:18:06.820]   notice it helps every single install base that we have.
[00:18:06.820 --> 00:18:09.260]   Hopper is better for it, Ampere is better for it,
[00:18:09.260 --> 00:18:11.460]   even Volta is better for it, okay?
[00:18:11.460 --> 00:18:13.380]   And I think Sam was just telling me
[00:18:13.380 --> 00:18:17.420]   that they had just decommissioned the Volta infrastructure
[00:18:17.420 --> 00:18:18.900]   that they have at OpenAI recently.
[00:18:18.900 --> 00:18:23.380]   And so I think we leave behind this trail of install base.
[00:18:23.380 --> 00:18:25.660]   Just like all computing, install base matters.
[00:18:25.660 --> 00:18:27.500]   And NVIDIA's in every single cloud,
[00:18:27.500 --> 00:18:31.260]   we're on-prem and all the way out to the edge.
[00:18:31.260 --> 00:18:35.300]   And so the VILA vision language model
[00:18:35.300 --> 00:18:37.460]   that's been created in the cloud
[00:18:37.460 --> 00:18:40.540]   works perfectly at the edge on the robots,
[00:18:40.540 --> 00:18:41.740]   without modification.
[00:18:41.740 --> 00:18:42.980]   It's all CUDA compatible.
[00:18:42.980 --> 00:18:47.980]   And so I think this idea of architecture compatibility
[00:18:47.980 --> 00:18:50.220]   was important for large...
[00:18:50.220 --> 00:18:51.740]   It's no different for iPhones,
[00:18:51.740 --> 00:18:52.900]   no different for anything else.
[00:18:52.900 --> 00:18:55.340]   I think the install base is really important for inference.
[00:18:55.340 --> 00:19:00.340]   But the thing that we really benefit from
[00:19:00.340 --> 00:19:03.540]   is because we're working on training
[00:19:03.540 --> 00:19:07.020]   these large language models and the new architectures of it,
[00:19:07.020 --> 00:19:11.820]   we're able to think about how do we create architectures
[00:19:11.820 --> 00:19:15.260]   that's excellent at inference someday when the time comes.
[00:19:15.260 --> 00:19:18.780]   And so we've been thinking about iterative models
[00:19:18.780 --> 00:19:20.900]   for reasoning models,
[00:19:20.900 --> 00:19:25.900]   and how do we create very interactive inference experiences
[00:19:25.900 --> 00:19:29.300]   for this personal agent of yours.
[00:19:29.300 --> 00:19:30.820]   You don't want to say something
[00:19:30.820 --> 00:19:32.220]   and have to go off and think about it for a while.
[00:19:32.220 --> 00:19:34.500]   You want it to interact with you quite quickly.
[00:19:34.500 --> 00:19:35.620]   So how do we create such a thing?
[00:19:35.620 --> 00:19:37.420]   And what came out of it was NVLink.
[00:19:37.420 --> 00:19:40.860]   NVLink so that we could take these systems
[00:19:40.860 --> 00:19:42.580]   that are excellent for training,
[00:19:42.580 --> 00:19:43.700]   but when you're done with it,
[00:19:43.700 --> 00:19:47.020]   the inference performance is exceptional.
[00:19:47.020 --> 00:19:52.020]   And so you want to optimize for this time to first token.
[00:19:52.020 --> 00:19:57.020]   And time to first token is insanely hard to do actually,
[00:19:57.020 --> 00:20:01.420]   because time to first token requires a lot of bandwidth.
[00:20:01.420 --> 00:20:03.380]   But if your context is also rich,
[00:20:03.380 --> 00:20:07.340]   then you need a lot of flops.
[00:20:07.340 --> 00:20:09.620]   And so you need an infinite amount of bandwidth,
[00:20:09.620 --> 00:20:11.860]   infinite amount of flops at the same time
[00:20:11.860 --> 00:20:15.620]   in order to achieve just a few millisecond response time.
[00:20:15.620 --> 00:20:18.460]   And so that architecture is really hard to do.
[00:20:18.460 --> 00:20:21.500]   And we invented a Grace Blackwell NVLink for that.
[00:20:21.500 --> 00:20:22.500]   - Right.
[00:20:22.500 --> 00:20:24.580]   In the spirit of time, I have more questions about that,
[00:20:24.580 --> 00:20:25.620]   but-
[00:20:25.620 --> 00:20:26.460]   - Don't worry about the time.
[00:20:26.460 --> 00:20:29.340]   Hey guys, hey, hey, hey, listen, Janine?
[00:20:29.340 --> 00:20:30.180]   - Yeah.
[00:20:30.180 --> 00:20:31.020]   - Look.
[00:20:31.020 --> 00:20:31.860]   - Let's do it until it's right.
[00:20:31.860 --> 00:20:33.140]   - Let's do it until right, there you go.
[00:20:33.140 --> 00:20:33.980]   - I love it, I love it.
[00:20:33.980 --> 00:20:38.420]   So, you know, I was at a dinner with Andy Jassy earlier.
[00:20:38.420 --> 00:20:40.300]   - See, now we don't have to worry about the time.
[00:20:40.300 --> 00:20:42.780]   - With Andy Jassy earlier this week.
[00:20:42.900 --> 00:20:45.620]   And Andy said, you know, we've got Tranium, you know,
[00:20:45.620 --> 00:20:47.340]   coming and Inferencia coming.
[00:20:47.340 --> 00:20:50.220]   And I think most people, again,
[00:20:50.220 --> 00:20:52.300]   view these as a problem for NVIDIA.
[00:20:52.300 --> 00:20:54.900]   But in the very next breath, he said,
[00:20:54.900 --> 00:20:57.820]   NVIDIA is a huge and important partner to us
[00:20:57.820 --> 00:21:00.700]   and will remain a huge and important partner for us.
[00:21:00.700 --> 00:21:02.860]   As far as I can see into the future,
[00:21:02.860 --> 00:21:05.300]   the world runs on NVIDIA, right?
[00:21:05.300 --> 00:21:08.140]   So when you think about the custom ASICs
[00:21:08.140 --> 00:21:09.620]   that are being built,
[00:21:09.620 --> 00:21:12.260]   that are going to go after targeted application,
[00:21:12.260 --> 00:21:14.300]   maybe the inference accelerator at Meta,
[00:21:14.300 --> 00:21:18.060]   maybe, you know, Tranium at Amazon, you know,
[00:21:18.060 --> 00:21:19.460]   or Google's TPUs.
[00:21:19.460 --> 00:21:22.020]   And then you think about the supply shortage
[00:21:22.020 --> 00:21:23.620]   that you have today.
[00:21:23.620 --> 00:21:28.260]   Do any of those things change that dynamic, right?
[00:21:28.260 --> 00:21:30.820]   Or are they complements to the systems
[00:21:30.820 --> 00:21:33.100]   that they're all buying from you?
[00:21:33.100 --> 00:21:34.340]   - We're just doing different things.
[00:21:34.340 --> 00:21:35.820]   - Yes.
[00:21:35.820 --> 00:21:38.700]   - We're trying to accomplish different things.
[00:21:38.700 --> 00:21:39.820]   You know, what NVIDIA is trying to do
[00:21:39.820 --> 00:21:42.820]   is build a computing platform for this new world,
[00:21:42.820 --> 00:21:44.060]   this machine learning world,
[00:21:44.060 --> 00:21:46.860]   this generative AI world, this agentic AI world.
[00:21:46.860 --> 00:21:49.700]   We're trying to create, you know, as you know,
[00:21:49.700 --> 00:21:52.860]   and what's just so deeply profound
[00:21:52.860 --> 00:21:54.940]   is after 60 years of computing,
[00:21:54.940 --> 00:21:58.500]   we reinvented the entire computing stack.
[00:21:58.500 --> 00:22:01.500]   The way you write software from programming
[00:22:01.500 --> 00:22:02.660]   to machine learning,
[00:22:02.660 --> 00:22:06.300]   the way that you process software from CPUs to GPU,
[00:22:06.300 --> 00:22:11.060]   the way that the applications from software
[00:22:11.060 --> 00:22:12.700]   to artificial intelligence, right?
[00:22:12.700 --> 00:22:16.620]   And so software tools to artificial intelligence.
[00:22:16.620 --> 00:22:19.020]   So every aspect of the computing stack
[00:22:19.020 --> 00:22:21.420]   and the technology stack has been changed.
[00:22:21.420 --> 00:22:23.140]   You know, what we would like to do
[00:22:23.140 --> 00:22:26.300]   is to create a computing platform
[00:22:26.300 --> 00:22:27.940]   that's available everywhere.
[00:22:27.940 --> 00:22:31.100]   And this is really the complexity of what we do.
[00:22:31.100 --> 00:22:32.100]   The complexity of what we do
[00:22:32.100 --> 00:22:33.820]   is if you think about what we do,
[00:22:33.820 --> 00:22:36.460]   we're building an entire AI infrastructure
[00:22:36.460 --> 00:22:38.380]   and we think of it as one computer.
[00:22:38.380 --> 00:22:39.540]   I've said before,
[00:22:39.540 --> 00:22:43.020]   the data center is now the unit of computing.
[00:22:43.020 --> 00:22:44.740]   To me, when I think about a computer,
[00:22:44.740 --> 00:22:46.220]   I'm not thinking about that chip.
[00:22:46.220 --> 00:22:47.260]   I'm thinking about this thing.
[00:22:47.260 --> 00:22:49.380]   That's my mental model and all the software
[00:22:49.380 --> 00:22:50.500]   and all the orchestration,
[00:22:50.500 --> 00:22:51.900]   all the machinery that's inside.
[00:22:51.900 --> 00:22:54.380]   That's my computer.
[00:22:54.380 --> 00:22:56.780]   And we're trying to build a new one every year.
[00:22:56.780 --> 00:22:58.020]   - Yeah.
[00:22:58.020 --> 00:22:59.100]   - That's insane.
[00:22:59.100 --> 00:23:01.020]   Nobody has ever done that before.
[00:23:01.020 --> 00:23:04.140]   We're trying to build a brand new one every single year.
[00:23:04.140 --> 00:23:04.980]   And every single year,
[00:23:04.980 --> 00:23:08.300]   we deliver two or three times more performance.
[00:23:08.300 --> 00:23:09.620]   As a result, every single year,
[00:23:09.620 --> 00:23:11.660]   we reduce the cost by two or three times.
[00:23:11.660 --> 00:23:12.580]   Every single year,
[00:23:12.580 --> 00:23:15.460]   we improve the energy efficiency by two or three times.
[00:23:15.460 --> 00:23:16.300]   Right?
[00:23:16.300 --> 00:23:18.140]   And so we ask our customers,
[00:23:18.140 --> 00:23:20.060]   don't buy everything at one time,
[00:23:20.060 --> 00:23:21.700]   buy a little every year.
[00:23:21.700 --> 00:23:22.540]   Okay?
[00:23:22.540 --> 00:23:23.380]   And the reason for that,
[00:23:23.380 --> 00:23:25.340]   we want them cost averaged into the future.
[00:23:25.340 --> 00:23:28.700]   All of it's architecturally compatible.
[00:23:28.700 --> 00:23:29.540]   Okay?
[00:23:29.540 --> 00:23:32.260]   Now, so that building that alone
[00:23:32.260 --> 00:23:35.740]   at the pace that we're doing is incredibly hard.
[00:23:35.740 --> 00:23:36.900]   Now, the double part,
[00:23:36.900 --> 00:23:38.340]   the double hard part,
[00:23:38.340 --> 00:23:40.100]   is then we take that all of that,
[00:23:40.100 --> 00:23:43.700]   and instead of selling it as a infrastructure,
[00:23:43.700 --> 00:23:45.260]   or selling it as a service,
[00:23:45.260 --> 00:23:47.620]   we disaggregate all of it,
[00:23:47.620 --> 00:23:49.860]   and we integrate it into GCP.
[00:23:49.860 --> 00:23:51.980]   We integrate it into AWS.
[00:23:51.980 --> 00:23:53.580]   We integrate it into Azure.
[00:23:53.580 --> 00:23:55.300]   We integrate it into X.
[00:23:55.300 --> 00:23:56.140]   Does that make sense?
[00:23:56.140 --> 00:23:56.980]   - Yes.
[00:23:57.100 --> 00:23:59.140]   - Everybody's integration is different.
[00:23:59.140 --> 00:24:03.220]   We have to get all of our architectural libraries,
[00:24:03.220 --> 00:24:04.500]   and all of our algorithms,
[00:24:04.500 --> 00:24:05.580]   and all of our frameworks,
[00:24:05.580 --> 00:24:07.060]   and integrate it into theirs.
[00:24:07.060 --> 00:24:09.460]   We get our security system integrated into theirs.
[00:24:09.460 --> 00:24:11.420]   We get our networking integrated into theirs.
[00:24:11.420 --> 00:24:12.260]   Isn't that right?
[00:24:12.260 --> 00:24:13.100]   - Right.
[00:24:13.100 --> 00:24:15.580]   - Then we do basically 10 integrations.
[00:24:15.580 --> 00:24:17.140]   And we do this every single year.
[00:24:17.140 --> 00:24:18.380]   - Right.
[00:24:18.380 --> 00:24:21.060]   - Now, that is the miracle.
[00:24:21.060 --> 00:24:22.300]   That is the miracle.
[00:24:22.300 --> 00:24:23.140]   - Why?
[00:24:23.140 --> 00:24:24.420]   I mean, it's madness.
[00:24:24.420 --> 00:24:26.540]   It's madness that you're trying to do this every year.
[00:24:26.540 --> 00:24:27.380]   - I'm thinking about it.
[00:24:27.380 --> 00:24:31.780]   - So, what drove you to do it every year,
[00:24:31.780 --> 00:24:33.660]   and then related to that,
[00:24:33.660 --> 00:24:36.780]   Clark's just back from Taipei, and Korea, and Japan,
[00:24:36.780 --> 00:24:39.460]   when meeting with all your supply partners,
[00:24:39.460 --> 00:24:42.460]   who you have decade-long relationships with.
[00:24:42.460 --> 00:24:45.700]   How important are those relationships
[00:24:45.700 --> 00:24:48.260]   to, again, the combinatorial math
[00:24:48.260 --> 00:24:50.180]   that builds that competitive moat?
[00:24:50.180 --> 00:24:55.860]   - Yeah, when you break it down systematically,
[00:24:55.860 --> 00:24:57.260]   the more you guys break it down,
[00:24:57.260 --> 00:24:58.780]   the more everybody breaks it down,
[00:24:58.780 --> 00:25:00.460]   the more amazed that they are.
[00:25:00.460 --> 00:25:01.300]   - Yes.
[00:25:01.300 --> 00:25:04.060]   - And how is it possible
[00:25:04.060 --> 00:25:08.340]   that the entire ecosystem of electronics today
[00:25:08.340 --> 00:25:10.780]   is dedicated in working with us
[00:25:10.780 --> 00:25:14.500]   to build, ultimately, this cube of a computer
[00:25:14.500 --> 00:25:17.380]   integrated into all of these different ecosystems,
[00:25:17.380 --> 00:25:20.420]   and the coordination is so seamless?
[00:25:20.420 --> 00:25:24.260]   So, there's obviously APIs, and methodologies,
[00:25:24.260 --> 00:25:27.340]   and business processes, and design rules
[00:25:27.340 --> 00:25:29.340]   that we've propagated backwards,
[00:25:29.340 --> 00:25:31.500]   and methodologies, and architectures,
[00:25:31.500 --> 00:25:34.340]   and APIs that we've propagated forward.
[00:25:34.340 --> 00:25:36.660]   - That have been hardened for decades.
[00:25:36.660 --> 00:25:37.940]   - Hardened for decades, yeah,
[00:25:37.940 --> 00:25:40.180]   and also evolving as we go.
[00:25:40.180 --> 00:25:43.180]   But these APIs have to come together.
[00:25:43.180 --> 00:25:44.020]   - Right, right.
[00:25:44.020 --> 00:25:47.060]   - When the time comes, all these things in Taiwan,
[00:25:47.060 --> 00:25:48.820]   all over the world being manufactured,
[00:25:48.820 --> 00:25:51.460]   they're gonna land somewhere in Azure's data center,
[00:25:51.460 --> 00:25:52.300]   they're gonna come together,
[00:25:52.300 --> 00:25:54.580]   click, click, click, click, click, click.
[00:25:54.580 --> 00:25:57.620]   - Someone just calls an OpenAI API and it just works.
[00:25:57.620 --> 00:25:59.820]   - That's right, yeah, exactly.
[00:25:59.820 --> 00:26:00.660]   - Yeah, there's a whole chain.
[00:26:00.660 --> 00:26:01.500]   - It's kind of craziness, right?
[00:26:01.500 --> 00:26:02.340]   - There's a whole chain.
[00:26:02.340 --> 00:26:03.180]   - And so, that's what we invented,
[00:26:03.180 --> 00:26:04.020]   that's what we invented,
[00:26:04.020 --> 00:26:07.580]   this massive infrastructure of computing.
[00:26:07.580 --> 00:26:10.180]   The whole planet is working with us on it.
[00:26:10.180 --> 00:26:12.540]   It's integrated into everywhere.
[00:26:12.540 --> 00:26:14.180]   It's, you could sell it through Dell,
[00:26:14.180 --> 00:26:15.900]   you could sell it through HPE.
[00:26:15.900 --> 00:26:17.980]   It's hosted in the cloud.
[00:26:17.980 --> 00:26:21.100]   It's all the way out at the edge.
[00:26:21.100 --> 00:26:25.420]   People use it in robotic systems now and human robots.
[00:26:25.420 --> 00:26:27.260]   They're in self-driving cars.
[00:26:27.260 --> 00:26:29.140]   They're all architecturally compatible.
[00:26:29.140 --> 00:26:32.460]   Pretty kind of craziness.
[00:26:32.460 --> 00:26:33.420]   - It's craziness.
[00:26:33.420 --> 00:26:35.540]   - Clark, I don't want you to leave the impression
[00:26:35.540 --> 00:26:36.860]   I didn't answer the question.
[00:26:36.860 --> 00:26:38.100]   In fact, I did.
[00:26:38.100 --> 00:26:41.340]   What I meant by that when relating to your ASIC
[00:26:41.340 --> 00:26:45.100]   is the way to think about,
[00:26:45.100 --> 00:26:46.660]   we're just doing something different.
[00:26:46.660 --> 00:26:48.060]   - Yes.
[00:26:48.260 --> 00:26:53.260]   - As a company, we want to be situationally aware,
[00:26:53.260 --> 00:26:55.940]   and I'm very situationally aware
[00:26:55.940 --> 00:26:59.180]   of everything around our company and our ecosystem.
[00:26:59.180 --> 00:27:01.540]   I'm aware of all the people doing alternative things
[00:27:01.540 --> 00:27:03.180]   and what they're doing,
[00:27:03.180 --> 00:27:08.180]   and sometimes it's adversarial to us, sometimes it's not.
[00:27:08.180 --> 00:27:10.380]   I'm super aware of it.
[00:27:10.380 --> 00:27:14.660]   But that doesn't change what the purpose of the company is.
[00:27:14.660 --> 00:27:16.620]   The singular purpose of the company
[00:27:16.620 --> 00:27:18.220]   is to build an architecture,
[00:27:18.220 --> 00:27:23.020]   that a platform that could be everywhere.
[00:27:23.020 --> 00:27:24.500]   - Right.
[00:27:24.500 --> 00:27:26.860]   - That is our goal.
[00:27:26.860 --> 00:27:28.580]   We're not trying to take any share from anybody.
[00:27:28.580 --> 00:27:32.260]   NVIDIA is a market maker, not share taker.
[00:27:32.260 --> 00:27:34.140]   If you look at our company slides,
[00:27:34.140 --> 00:27:38.420]   not one day does this company talk about market share,
[00:27:38.420 --> 00:27:39.260]   not inside.
[00:27:39.260 --> 00:27:43.580]   All we're talking about is how do we create the next thing?
[00:27:43.580 --> 00:27:45.540]   What's the next problem we can solve?
[00:27:45.540 --> 00:27:49.100]   In that flywheel, how can we do a better job for people?
[00:27:49.100 --> 00:27:52.380]   How do we take that flywheel that used to take about a year,
[00:27:52.380 --> 00:27:54.620]   how do we crank it down to about a month?
[00:27:54.620 --> 00:27:55.740]   - Yes, yes.
[00:27:55.740 --> 00:27:57.180]   - What's the speed of light of that?
[00:27:57.180 --> 00:27:58.020]   Isn't that right?
[00:27:58.020 --> 00:28:00.100]   And so we're thinking about all these different things,
[00:28:00.100 --> 00:28:02.460]   but the one thing we're not,
[00:28:02.460 --> 00:28:05.180]   we're situationally aware of everything,
[00:28:05.180 --> 00:28:07.820]   but we're certain that what our mission is,
[00:28:07.820 --> 00:28:09.620]   is very singular.
[00:28:09.620 --> 00:28:12.380]   The only question is whether that mission is necessary.
[00:28:12.380 --> 00:28:13.220]   Does that make sense?
[00:28:13.220 --> 00:28:14.380]   - Yes.
[00:28:14.380 --> 00:28:16.580]   - And all companies, all great companies,
[00:28:16.580 --> 00:28:18.660]   ought to have that at its core.
[00:28:18.660 --> 00:28:21.260]   It's about what are you doing?
[00:28:21.260 --> 00:28:22.100]   - For sure.
[00:28:22.100 --> 00:28:23.060]   - The only question, is it necessary?
[00:28:23.060 --> 00:28:24.020]   Is it valuable?
[00:28:24.020 --> 00:28:24.860]   - Right.
[00:28:24.860 --> 00:28:25.700]   - Is it impactful?
[00:28:25.700 --> 00:28:26.980]   Does it help people?
[00:28:26.980 --> 00:28:29.900]   And I am certain that you're a developer,
[00:28:29.900 --> 00:28:32.180]   you're a generative AI startup,
[00:28:32.180 --> 00:28:35.580]   and you're about to decide how to become a company.
[00:28:35.580 --> 00:28:38.260]   The one choice that you don't have to make
[00:28:38.260 --> 00:28:41.940]   is which one of the A6 do I support?
[00:28:41.940 --> 00:28:43.820]   If you just support a CUDA,
[00:28:43.820 --> 00:28:45.660]   you know you could go everywhere.
[00:28:45.660 --> 00:28:47.500]   You could always change your mind later.
[00:28:47.500 --> 00:28:48.340]   - Right.
[00:28:48.340 --> 00:28:50.860]   - But we're the on-ramp to the world of AI.
[00:28:50.860 --> 00:28:51.700]   Isn't that right?
[00:28:51.700 --> 00:28:54.180]   Once you decide to come onto our platform,
[00:28:54.180 --> 00:28:56.020]   the other decisions you could defer.
[00:28:56.020 --> 00:28:59.220]   You could always build your own A6 later.
[00:28:59.220 --> 00:29:00.060]   - Right.
[00:29:00.060 --> 00:29:00.900]   - You know, we're not against that.
[00:29:00.900 --> 00:29:02.220]   We're not offended by any of that.
[00:29:02.220 --> 00:29:03.060]   When I work with,
[00:29:03.060 --> 00:29:05.700]   when we work with all the GCPs,
[00:29:05.700 --> 00:29:06.980]   the GCPs Azure,
[00:29:06.980 --> 00:29:10.260]   we present our roadmap to them years in advance.
[00:29:10.260 --> 00:29:12.580]   They don't present their A6 roadmap to us,
[00:29:12.580 --> 00:29:14.740]   and it doesn't ever offend us.
[00:29:14.740 --> 00:29:15.820]   Does that make sense?
[00:29:15.820 --> 00:29:16.660]   We create,
[00:29:16.660 --> 00:29:17.500]   we're in a,
[00:29:17.500 --> 00:29:18.940]   if you have a sole purpose,
[00:29:18.940 --> 00:29:21.060]   and your purpose is meaningful,
[00:29:21.060 --> 00:29:23.300]   and your mission is dear to you,
[00:29:23.300 --> 00:29:25.260]   and is dear to everybody else,
[00:29:25.260 --> 00:29:27.060]   then you could be transparent.
[00:29:27.060 --> 00:29:30.180]   Notice my roadmap is transparent at GTC.
[00:29:30.180 --> 00:29:32.740]   My roadmap goes way deeper
[00:29:32.740 --> 00:29:34.500]   to our friends at Azure,
[00:29:34.500 --> 00:29:35.340]   and AWS,
[00:29:35.340 --> 00:29:36.860]   and others.
[00:29:36.860 --> 00:29:38.340]   We have no trouble doing any of that,
[00:29:38.340 --> 00:29:40.460]   even as they're building their own A6.
[00:29:40.460 --> 00:29:41.460]   - I think,
[00:29:41.460 --> 00:29:42.300]   you know,
[00:29:42.300 --> 00:29:45.260]   when people observe the business,
[00:29:45.260 --> 00:29:48.940]   you said recently that the demand for Blackwell is insane.
[00:29:48.940 --> 00:29:51.220]   You said one of the hardest parts of your job
[00:29:51.220 --> 00:29:54.740]   is the emotional toll of saying no to people
[00:29:54.740 --> 00:29:58.180]   in a world that has a shortage
[00:29:58.180 --> 00:29:59.620]   of the compute that you,
[00:29:59.620 --> 00:30:02.020]   that you can produce and have on offer.
[00:30:02.020 --> 00:30:04.540]   But critics say this is just a moment in time, right?
[00:30:04.540 --> 00:30:08.260]   They say this is just like Cisco in 2000,
[00:30:08.260 --> 00:30:09.980]   we're overbuilding fiber.
[00:30:09.980 --> 00:30:12.300]   It's gonna be boom and bust.
[00:30:12.300 --> 00:30:13.140]   You know,
[00:30:13.140 --> 00:30:17.140]   I think about the start of 23 when we were having dinner.
[00:30:17.140 --> 00:30:22.140]   The forecast for NVIDIA at that dinner in January of 23
[00:30:22.140 --> 00:30:25.420]   was that you would do 26 billion of revenue
[00:30:25.420 --> 00:30:27.020]   for the year 2023.
[00:30:27.020 --> 00:30:28.500]   You did 60 billion,
[00:30:28.500 --> 00:30:29.340]   right?
[00:30:29.340 --> 00:30:30.180]   The 25 people-
[00:30:30.180 --> 00:30:31.020]   - Let's just,
[00:30:31.020 --> 00:30:32.300]   let the truth be known.
[00:30:32.300 --> 00:30:35.060]   That is the single greatest failure
[00:30:35.060 --> 00:30:36.980]   of forecasting the world has ever seen.
[00:30:36.980 --> 00:30:37.980]   - Right, right, right.
[00:30:37.980 --> 00:30:38.820]   - Can we all,
[00:30:38.820 --> 00:30:40.340]   can we all at least admit that?
[00:30:40.340 --> 00:30:41.180]   - What, what, what, what?
[00:30:41.180 --> 00:30:42.020]   To me, to me-
[00:30:42.020 --> 00:30:42.860]   - That was my takeaway.
[00:30:42.860 --> 00:30:43.700]   I just go-
[00:30:43.700 --> 00:30:44.540]   (laughing)
[00:30:44.540 --> 00:30:45.380]   - And that was,
[00:30:45.380 --> 00:30:46.220]   and that was,
[00:30:46.220 --> 00:30:47.780]   we got so excited in November 22
[00:30:47.780 --> 00:30:51.540]   because we had folks like Mustafa from Inflection
[00:30:51.540 --> 00:30:54.180]   and Noah from Character coming in our office
[00:30:54.180 --> 00:30:56.580]   talking about investing in their companies.
[00:30:56.580 --> 00:30:57.420]   And they said,
[00:30:57.420 --> 00:31:00.180]   "Well, if you can't pencil out investing in our companies,
[00:31:00.180 --> 00:31:01.100]   then buy NVIDIA."
[00:31:01.100 --> 00:31:03.180]   Because everybody in the world
[00:31:03.180 --> 00:31:04.540]   is trying to get NVIDIA chips
[00:31:04.540 --> 00:31:05.900]   to build these applications
[00:31:05.900 --> 00:31:07.300]   that are gonna change the world.
[00:31:07.300 --> 00:31:08.140]   And of course,
[00:31:08.140 --> 00:31:10.980]   the Cambrian moment occurred with CHAT GPT,
[00:31:10.980 --> 00:31:12.980]   and notwithstanding that fact,
[00:31:12.980 --> 00:31:16.540]   these 25 analysts were so focused on the crypto winner
[00:31:16.540 --> 00:31:19.180]   that they couldn't get their head around an imagination
[00:31:19.180 --> 00:31:22.060]   of what was happening in the world, okay?
[00:31:22.060 --> 00:31:24.540]   So it ended up being way bigger.
[00:31:24.540 --> 00:31:26.580]   You say in very plain English,
[00:31:26.580 --> 00:31:28.940]   the demand is insane for Blackwell,
[00:31:28.940 --> 00:31:31.660]   that it's going to be that way for as far as you can,
[00:31:31.660 --> 00:31:33.140]   you know, for as far as you can see.
[00:31:33.140 --> 00:31:35.780]   Of course, the future is unknown and unknowable,
[00:31:35.780 --> 00:31:38.220]   but why are the critics so wrong
[00:31:38.220 --> 00:31:42.700]   that this isn't going to be the Cisco-like situation
[00:31:42.700 --> 00:31:45.540]   of overbuilding in 2000?
[00:31:45.540 --> 00:31:46.380]   - Yeah.
[00:31:46.380 --> 00:31:50.940]   The best way to think about the future
[00:31:50.940 --> 00:31:53.340]   is reason about it from first principles.
[00:31:53.340 --> 00:31:54.180]   - Correct.
[00:31:54.180 --> 00:31:55.660]   - Okay, so the question is,
[00:31:55.660 --> 00:31:57.420]   what are the first principles of what we're doing?
[00:31:57.420 --> 00:31:58.980]   Number one, what are we doing?
[00:31:58.980 --> 00:32:01.660]   What are we doing?
[00:32:01.660 --> 00:32:03.060]   The first thing that we are doing
[00:32:03.060 --> 00:32:05.060]   is we are reinventing computing.
[00:32:05.060 --> 00:32:05.900]   Do we not?
[00:32:05.900 --> 00:32:06.940]   We just said that.
[00:32:06.940 --> 00:32:09.020]   The way that computing will be done in the future
[00:32:09.020 --> 00:32:11.460]   will be highly machine-learned.
[00:32:11.460 --> 00:32:12.300]   - Yes.
[00:32:12.300 --> 00:32:13.140]   - Highly machine-learned, okay?
[00:32:13.140 --> 00:32:14.540]   Almost everything that we do,
[00:32:14.540 --> 00:32:16.380]   almost every single application,
[00:32:16.380 --> 00:32:21.380]   Word, Excel, PowerPoint, Photoshop, Premier, you know,
[00:32:21.380 --> 00:32:27.100]   AutoCAD, you give me your favorite application
[00:32:27.100 --> 00:32:29.020]   that was all hand-engineered,
[00:32:29.020 --> 00:32:31.820]   I promise you it will be highly machine-learned
[00:32:31.820 --> 00:32:33.460]   in the future, isn't that right?
[00:32:33.460 --> 00:32:34.620]   And so all these tools will be,
[00:32:34.620 --> 00:32:37.060]   and on top of that, you're gonna have machines,
[00:32:37.060 --> 00:32:38.940]   agents that help you use them.
[00:32:38.940 --> 00:32:39.780]   - Right.
[00:32:39.780 --> 00:32:40.620]   - Okay?
[00:32:40.620 --> 00:32:43.140]   And so we know this for a fact at this point, right?
[00:32:43.140 --> 00:32:43.980]   Isn't that right?
[00:32:43.980 --> 00:32:46.020]   We've reinvented computing, we're not going back.
[00:32:46.020 --> 00:32:47.660]   The entire computing technology stack
[00:32:47.660 --> 00:32:48.500]   is being reinvented.
[00:32:48.500 --> 00:32:50.300]   Okay, so now that we've done that,
[00:32:50.300 --> 00:32:52.620]   we said that software is gonna be different.
[00:32:52.620 --> 00:32:54.620]   What software can write is gonna be different.
[00:32:54.620 --> 00:32:56.460]   How we use software will be different.
[00:32:56.460 --> 00:32:59.060]   So let's now acknowledge that.
[00:32:59.060 --> 00:33:01.180]   So those are my ground truth now.
[00:33:01.180 --> 00:33:02.140]   - Yes.
[00:33:02.220 --> 00:33:05.220]   - Now the question, therefore, is what happens?
[00:33:05.220 --> 00:33:07.140]   And so let's go back and let's just take a look
[00:33:07.140 --> 00:33:09.100]   at how's computing done in the past.
[00:33:09.100 --> 00:33:10.700]   So we have a trillion dollars worth of computers
[00:33:10.700 --> 00:33:11.540]   in the past.
[00:33:11.540 --> 00:33:12.860]   We look at it, just open the door,
[00:33:12.860 --> 00:33:13.700]   look at the data center,
[00:33:13.700 --> 00:33:14.740]   and you look at it and say,
[00:33:14.740 --> 00:33:16.900]   are those the computers you want doing that,
[00:33:16.900 --> 00:33:17.740]   doing that future?
[00:33:17.740 --> 00:33:18.780]   And the answer is no.
[00:33:18.780 --> 00:33:19.620]   - Right.
[00:33:19.620 --> 00:33:20.780]   - Right, you got all these CPUs back there.
[00:33:20.780 --> 00:33:23.380]   We know what it can do and what it can't do.
[00:33:23.380 --> 00:33:25.020]   And we just know that we have a trillion dollars
[00:33:25.020 --> 00:33:26.900]   worth of data centers that we have to modernize.
[00:33:26.900 --> 00:33:28.340]   And so right now, as we speak,
[00:33:28.340 --> 00:33:31.180]   if we were to have a trajectory over the next four
[00:33:31.180 --> 00:33:33.820]   or five years to modernize that old stuff,
[00:33:33.820 --> 00:33:35.860]   that's not unreasonable.
[00:33:35.860 --> 00:33:36.700]   - Right.
[00:33:36.700 --> 00:33:37.540]   - Sensible.
[00:33:37.540 --> 00:33:38.380]   So we have a trillion--
[00:33:38.380 --> 00:33:39.340]   - And you're having those conversations
[00:33:39.340 --> 00:33:40.780]   with the people who have to modernize it.
[00:33:40.780 --> 00:33:41.620]   - Yeah.
[00:33:41.620 --> 00:33:42.820]   - And they're modernizing it on GPU.
[00:33:42.820 --> 00:33:43.660]   - That's right.
[00:33:43.660 --> 00:33:46.340]   I mean, well, let's make another test.
[00:33:46.340 --> 00:33:50.580]   You have $50 billion of CapEx you'd like to spend.
[00:33:50.580 --> 00:33:53.940]   Option A, option B, build CapEx for the future.
[00:33:53.940 --> 00:33:54.780]   - Right.
[00:33:54.780 --> 00:33:56.340]   - Or build CapEx like the past.
[00:33:56.340 --> 00:33:57.180]   - Right.
[00:33:57.180 --> 00:34:00.300]   - Now you already have the CapEx of the past.
[00:34:00.300 --> 00:34:01.460]   - Right, right.
[00:34:01.460 --> 00:34:02.500]   - It's sitting right there.
[00:34:02.500 --> 00:34:04.020]   It's not getting much better anyways.
[00:34:04.020 --> 00:34:05.380]   Moore's law has largely ended.
[00:34:05.380 --> 00:34:07.380]   And so why rebuild that?
[00:34:07.380 --> 00:34:09.780]   Let's just take $50 billion, put it into generative AI.
[00:34:09.780 --> 00:34:10.900]   Isn't that right?
[00:34:10.900 --> 00:34:12.700]   And so now your company just got better.
[00:34:12.700 --> 00:34:13.780]   - Right.
[00:34:13.780 --> 00:34:15.900]   - Now, how much of that 50 billion would you put in?
[00:34:15.900 --> 00:34:18.300]   Well, I would put in 100% of the 50 billion
[00:34:18.300 --> 00:34:19.860]   because I've already got four years
[00:34:19.860 --> 00:34:23.180]   of infrastructure behind me that's of the past.
[00:34:23.180 --> 00:34:26.900]   And so now you just, I just reasoned about it
[00:34:26.900 --> 00:34:28.620]   from the perspective of somebody thinking about it
[00:34:28.620 --> 00:34:30.420]   from first principles and that's what they're doing.
[00:34:30.420 --> 00:34:32.260]   Smart people are doing smart things.
[00:34:32.260 --> 00:34:34.060]   Now the second part is this.
[00:34:34.060 --> 00:34:35.860]   So now we have a trillion dollars worth of capacity
[00:34:35.860 --> 00:34:36.700]   to go build, right?
[00:34:36.700 --> 00:34:37.660]   Trillion dollars worth of infrastructure.
[00:34:37.660 --> 00:34:40.060]   What about, you know, call it $150 billion into it.
[00:34:40.060 --> 00:34:41.180]   - Right.
[00:34:41.180 --> 00:34:42.020]   - Okay.
[00:34:42.020 --> 00:34:44.980]   So we have a trillion dollars in infrastructure
[00:34:44.980 --> 00:34:46.780]   to go build over the next four or five years.
[00:34:46.780 --> 00:34:48.700]   Well, the second thing that we observe
[00:34:48.700 --> 00:34:53.580]   is that the way that software is written is different
[00:34:53.580 --> 00:34:56.420]   but how software is gonna be used is different.
[00:34:56.420 --> 00:34:57.780]   In the future, we're gonna have agents.
[00:34:57.780 --> 00:34:58.620]   Isn't that right?
[00:34:58.620 --> 00:34:59.460]   - Correct.
[00:34:59.460 --> 00:35:01.140]   - We're gonna have digital employees in our company.
[00:35:01.140 --> 00:35:03.860]   In your inbox, you have all these little dots
[00:35:03.860 --> 00:35:04.860]   and these little faces.
[00:35:04.860 --> 00:35:08.180]   In the future, there's gonna be icons of AIs.
[00:35:08.180 --> 00:35:09.100]   Isn't that right?
[00:35:09.100 --> 00:35:10.660]   I'm gonna be sending them.
[00:35:10.660 --> 00:35:13.580]   I'm gonna be, I'm no longer gonna program computers
[00:35:13.580 --> 00:35:14.740]   with C++.
[00:35:14.740 --> 00:35:18.660]   I'm gonna program AIs with prompting.
[00:35:18.660 --> 00:35:19.580]   Isn't that right?
[00:35:19.580 --> 00:35:21.660]   Now this is no different than me talking to my,
[00:35:21.660 --> 00:35:23.780]   you know, this morning, I wrote a bunch of emails
[00:35:23.780 --> 00:35:24.660]   before I came here.
[00:35:24.660 --> 00:35:26.500]   I was prompting my teams.
[00:35:26.500 --> 00:35:27.340]   - Of course.
[00:35:27.340 --> 00:35:28.180]   Yeah.
[00:35:28.180 --> 00:35:29.460]   - And I would describe the context.
[00:35:29.460 --> 00:35:32.380]   I would describe the fundamental constraints
[00:35:32.380 --> 00:35:33.740]   that I know of.
[00:35:33.740 --> 00:35:35.460]   And I would describe the mission for them.
[00:35:35.460 --> 00:35:36.900]   I would leave it sufficiently,
[00:35:36.900 --> 00:35:40.180]   I would be sufficiently directional
[00:35:40.180 --> 00:35:41.740]   so that they understand what I need.
[00:35:41.740 --> 00:35:44.100]   And I wanna be clear about what the outcome should be,
[00:35:44.100 --> 00:35:45.460]   as clear as I can be.
[00:35:45.460 --> 00:35:48.180]   But I leave enough ambiguous space on, you know,
[00:35:48.180 --> 00:35:50.020]   a creativity space so they can surprise me.
[00:35:50.020 --> 00:35:50.860]   Isn't that right?
[00:35:50.860 --> 00:35:51.700]   - Absolutely.
[00:35:51.700 --> 00:35:53.020]   - It's no different than how I prompt an AI today.
[00:35:53.020 --> 00:35:53.860]   - Yeah.
[00:35:53.860 --> 00:35:55.140]   - It's exactly how I prompt an AI.
[00:35:55.140 --> 00:35:56.740]   And so what's gonna happen is,
[00:35:56.740 --> 00:35:59.660]   is on top of this infrastructure of IT
[00:35:59.660 --> 00:36:01.220]   that we're gonna modernize,
[00:36:01.220 --> 00:36:03.660]   there's gonna be a new infrastructure.
[00:36:03.660 --> 00:36:07.060]   This new infrastructure are going to be AI factories
[00:36:07.060 --> 00:36:10.380]   that operate these digital humans.
[00:36:10.380 --> 00:36:13.180]   And they're gonna be running all the time, 24/7.
[00:36:13.180 --> 00:36:14.020]   - Right.
[00:36:14.020 --> 00:36:16.020]   - We're gonna have 'em for all of our companies
[00:36:16.020 --> 00:36:17.380]   all over the world.
[00:36:17.380 --> 00:36:18.980]   We're gonna have 'em in factories.
[00:36:18.980 --> 00:36:20.860]   We're gonna have 'em in autonomous systems.
[00:36:20.860 --> 00:36:21.740]   Isn't that right?
[00:36:21.740 --> 00:36:24.860]   So there's a whole layer of computing fabric,
[00:36:24.860 --> 00:36:27.220]   a whole layer of what I call AI factories
[00:36:27.220 --> 00:36:28.580]   that the world has to make
[00:36:28.580 --> 00:36:30.300]   that doesn't exist today at all.
[00:36:30.300 --> 00:36:31.140]   - Right.
[00:36:31.140 --> 00:36:32.260]   - So the question is, how big is that?
[00:36:32.260 --> 00:36:33.100]   - Right.
[00:36:33.100 --> 00:36:34.460]   - Unknowable at the moment.
[00:36:34.460 --> 00:36:36.180]   Probably a few trillion dollars.
[00:36:36.180 --> 00:36:37.020]   - Right.
[00:36:37.020 --> 00:36:38.380]   - Unknowable at the moment,
[00:36:38.380 --> 00:36:40.580]   but as we're sitting here building in,
[00:36:40.580 --> 00:36:42.580]   the beautiful thing is the architecture
[00:36:42.580 --> 00:36:45.420]   for this modernizing this new data center
[00:36:45.420 --> 00:36:48.860]   and the architecture for the AI factory is the same.
[00:36:48.860 --> 00:36:49.700]   - Right.
[00:36:49.700 --> 00:36:50.540]   - That's the nice thing.
[00:36:50.540 --> 00:36:52.620]   - And you made this clear.
[00:36:52.620 --> 00:36:54.100]   You've got a trillion of old stuff.
[00:36:54.100 --> 00:36:55.060]   You've got to modernize.
[00:36:55.060 --> 00:36:58.380]   You at least have a trillion of new AI workloads coming on.
[00:36:58.380 --> 00:36:59.220]   - Yeah.
[00:36:59.220 --> 00:37:02.860]   - Give or take, you'll do 125 billion in revenue this year.
[00:37:02.860 --> 00:37:04.820]   You know, there was, at one point somebody told you
[00:37:04.820 --> 00:37:07.100]   the company would never be worth more than a billion.
[00:37:07.100 --> 00:37:10.540]   As you sit here today, is there any reason, right,
[00:37:10.540 --> 00:37:14.580]   if you're only 125 billion out of a multi-trillion, Tam,
[00:37:14.580 --> 00:37:16.780]   that you're not going to have 2X the revenue,
[00:37:16.780 --> 00:37:20.340]   3X the revenue in the future that you have today?
[00:37:20.340 --> 00:37:22.380]   Is there any reason your revenue doesn't?
[00:37:23.420 --> 00:37:24.260]   - No.
[00:37:24.260 --> 00:37:25.100]   - Yeah.
[00:37:25.100 --> 00:37:25.940]   - Yeah.
[00:37:25.940 --> 00:37:27.740]   As you know, it's not about,
[00:37:27.740 --> 00:37:33.940]   everything is, you know, companies are only limited
[00:37:33.940 --> 00:37:36.580]   by the size of the fish pond, you know?
[00:37:36.580 --> 00:37:37.420]   - Yes, yes.
[00:37:37.420 --> 00:37:39.580]   - A goldfish can only be so big.
[00:37:39.580 --> 00:37:42.940]   And so the question is, what is our fish pond?
[00:37:42.940 --> 00:37:44.540]   What is our pond?
[00:37:44.540 --> 00:37:46.380]   And that requires a little imagination.
[00:37:46.380 --> 00:37:50.180]   And this is the reason why market makers think
[00:37:50.180 --> 00:37:54.180]   about that future, creating that new fish pond.
[00:37:54.180 --> 00:37:57.620]   It's hard to figure this out looking backwards
[00:37:57.620 --> 00:37:58.900]   and try to take share.
[00:37:58.900 --> 00:37:59.740]   - Right.
[00:37:59.740 --> 00:38:01.340]   - You know, share takers can only be so big.
[00:38:01.340 --> 00:38:02.180]   - For sure.
[00:38:02.180 --> 00:38:03.900]   - Market makers can be quite large.
[00:38:03.900 --> 00:38:04.740]   - For sure.
[00:38:04.740 --> 00:38:07.660]   - Yeah, and so, you know, I think the good fortune
[00:38:07.660 --> 00:38:09.900]   that our company has is that since the very beginning
[00:38:09.900 --> 00:38:12.340]   of our company, we had to invent the market
[00:38:12.340 --> 00:38:13.860]   for us to go swim in.
[00:38:13.860 --> 00:38:16.140]   That market, and people don't realize this back then,
[00:38:16.140 --> 00:38:20.380]   but anymore, but, you know, we were at ground zero
[00:38:20.380 --> 00:38:22.540]   of creating the 3D gaming PC market.
[00:38:22.540 --> 00:38:23.620]   - Right, right.
[00:38:23.620 --> 00:38:27.220]   - We largely invented this market and all the ecosystem
[00:38:27.220 --> 00:38:30.380]   and all the graphics card ecosystem, we invented all that.
[00:38:30.380 --> 00:38:35.380]   And so the need to invent a new market to go serve it later
[00:38:35.380 --> 00:38:38.900]   is something that's very comfortable for us.
[00:38:38.900 --> 00:38:40.060]   - Exactly, exactly.
[00:38:40.060 --> 00:38:42.860]   And speaking to somebody who's invented a new market,
[00:38:42.860 --> 00:38:45.100]   you know, let's shift gears a little bit to models
[00:38:45.100 --> 00:38:47.860]   and open AI, open AI raised, as you know,
[00:38:47.860 --> 00:38:50.900]   six and a half billion dollars this week,
[00:38:50.900 --> 00:38:54.460]   at like $150 billion valuation.
[00:38:54.460 --> 00:38:56.180]   We both participated.
[00:38:56.180 --> 00:38:58.420]   - Yeah, really happy for them,
[00:38:58.420 --> 00:38:59.700]   really happy they came together.
[00:38:59.700 --> 00:39:00.540]   - Right.
[00:39:00.540 --> 00:39:01.380]   - Yeah, they did a great stand
[00:39:01.380 --> 00:39:03.300]   and the team did a great job, yeah.
[00:39:03.300 --> 00:39:07.020]   - Reports are that they'll do 5 billion-ish of revenue
[00:39:07.020 --> 00:39:08.780]   or run rate revenue this year,
[00:39:08.780 --> 00:39:11.260]   maybe going to 10 billion next year.
[00:39:11.260 --> 00:39:13.220]   If you look at the business today,
[00:39:13.220 --> 00:39:16.220]   it's about twice the revenue as Google was
[00:39:16.220 --> 00:39:18.100]   at the time of its IPO.
[00:39:18.100 --> 00:39:19.700]   They have 250 million-- - Is that right?
[00:39:19.700 --> 00:39:22.700]   - Yeah, 250 million weekly average users,
[00:39:22.700 --> 00:39:25.420]   which we estimate is twice the amount Google had
[00:39:25.420 --> 00:39:27.260]   at the time of its IPO. - Is that right, okay, wow.
[00:39:27.260 --> 00:39:29.380]   - And if you look at the multiple of the business,
[00:39:29.380 --> 00:39:31.140]   if you believe 10 billion next year,
[00:39:31.140 --> 00:39:33.780]   it's about 15 times the forward revenue,
[00:39:33.780 --> 00:39:35.540]   which is about the multiple of Google and Meta
[00:39:35.540 --> 00:39:37.660]   at the time of their IPO, right?
[00:39:37.660 --> 00:39:41.700]   When you think about a company that had zero revenue,
[00:39:41.700 --> 00:39:44.780]   zero weekly average users 22 months ago--
[00:39:44.780 --> 00:39:47.540]   - Brad has an incredible command of history.
[00:39:47.540 --> 00:39:49.220]   - When you think about that,
[00:39:49.220 --> 00:39:53.580]   talk to us about the importance of open AI
[00:39:53.580 --> 00:39:57.620]   as a partner to you and open AI as a force
[00:39:57.620 --> 00:39:59.620]   in kind of driving forward, you know,
[00:39:59.620 --> 00:40:02.740]   kind of public awareness and usage around AI.
[00:40:02.740 --> 00:40:07.500]   - Well, this is one of the most consequential companies
[00:40:07.500 --> 00:40:08.340]   of our time.
[00:40:10.380 --> 00:40:15.380]   The, a pure play AI company
[00:40:15.380 --> 00:40:24.180]   pursuing the vision of AGI
[00:40:24.180 --> 00:40:28.300]   and whatever its definition.
[00:40:28.300 --> 00:40:31.620]   I almost don't think it matters fully
[00:40:31.620 --> 00:40:34.420]   what the definition is, nor do I,
[00:40:34.420 --> 00:40:40.220]   you know, really believe that the timing matters.
[00:40:40.220 --> 00:40:45.060]   The one thing that I know is that AI is gonna have
[00:40:45.060 --> 00:40:48.300]   a roadmap of capabilities over time.
[00:40:48.300 --> 00:40:50.740]   And that roadmap of capabilities over time
[00:40:50.740 --> 00:40:53.020]   is gonna be quite spectacular.
[00:40:53.020 --> 00:40:57.340]   And along the way, long before it even gets
[00:40:57.340 --> 00:40:59.660]   to anybody's definition of AGI,
[00:40:59.660 --> 00:41:01.260]   we're gonna put it to great use.
[00:41:01.260 --> 00:41:05.540]   All you have to do is right now as we speak,
[00:41:05.540 --> 00:41:09.460]   go talk to digital biologists,
[00:41:09.460 --> 00:41:13.540]   climate tech researchers, material researchers,
[00:41:13.540 --> 00:41:19.620]   physical sciences, astrophysicists, quantum chemists.
[00:41:19.620 --> 00:41:23.580]   You go ask video game designers,
[00:41:23.580 --> 00:41:27.780]   manufacturing engineers,
[00:41:27.780 --> 00:41:30.980]   roboticists, pick your favorite,
[00:41:30.980 --> 00:41:33.540]   whatever industry you wanna go pick.
[00:41:33.540 --> 00:41:35.780]   And you go deep in there and you talk to the people
[00:41:35.780 --> 00:41:37.620]   that matter and you ask them,
[00:41:37.620 --> 00:41:41.860]   has AI revolutionized the way you work?
[00:41:41.860 --> 00:41:44.340]   And you take those data points and you come back
[00:41:44.340 --> 00:41:46.900]   and you then get to ask yourself,
[00:41:46.900 --> 00:41:50.780]   how skeptical do you wanna be?
[00:41:50.780 --> 00:41:53.460]   Because they're not talking about AI
[00:41:53.460 --> 00:41:56.820]   as a conceptual benefit someday.
[00:41:56.820 --> 00:42:00.220]   They're talking about using AI right now.
[00:42:00.220 --> 00:42:04.700]   Right now, ag tech, material tech, climate tech,
[00:42:04.700 --> 00:42:08.220]   you pick your tech, you pick your field of science.
[00:42:08.220 --> 00:42:12.300]   They are advancing, AI is helping them advancing their work
[00:42:12.300 --> 00:42:13.860]   right now as we speak.
[00:42:13.860 --> 00:42:16.540]   Every single industry, every single company,
[00:42:16.540 --> 00:42:20.020]   every university, unbelievable, isn't that right?
[00:42:20.020 --> 00:42:20.900]   - Right.
[00:42:20.900 --> 00:42:25.900]   - It is absolutely going to somehow transform business.
[00:42:25.900 --> 00:42:28.460]   We know that.
[00:42:28.460 --> 00:42:29.540]   - Right.
[00:42:29.540 --> 00:42:32.180]   - I mean, it's so tangible, you could--
[00:42:32.180 --> 00:42:33.180]   - It's happening today.
[00:42:33.180 --> 00:42:34.020]   - It's happening today.
[00:42:34.020 --> 00:42:34.860]   - It's happening today.
[00:42:34.860 --> 00:42:35.700]   - Yeah, yeah.
[00:42:35.700 --> 00:42:40.700]   And so I think that the awakening of AI,
[00:42:40.700 --> 00:42:46.900]   chat GPT triggered, it's completely incredible.
[00:42:46.900 --> 00:42:52.980]   And I love their velocity and their singular purpose
[00:42:52.980 --> 00:42:56.300]   of advancing this field.
[00:42:56.300 --> 00:42:58.860]   And so really, really consequential.
[00:42:58.860 --> 00:43:01.020]   And they build an economic engine
[00:43:01.020 --> 00:43:04.860]   that can finance the next frontier of models, right?
[00:43:04.860 --> 00:43:08.580]   And I think there's a growing consensus in Silicon Valley
[00:43:08.580 --> 00:43:11.140]   that the whole model layer is commoditizing.
[00:43:11.140 --> 00:43:14.980]   Lama is making it very cheap
[00:43:14.980 --> 00:43:16.460]   for lots of people to build models.
[00:43:16.460 --> 00:43:19.300]   And so early on here, we had a lot of model companies,
[00:43:19.300 --> 00:43:23.100]   character and inflection and Cohere and Mistral
[00:43:23.100 --> 00:43:25.180]   and go through the list.
[00:43:25.180 --> 00:43:27.580]   And a lot of people question whether or not
[00:43:27.580 --> 00:43:31.140]   those companies can build the escape velocity
[00:43:31.140 --> 00:43:34.420]   on the economic engine that can continue funding
[00:43:34.420 --> 00:43:35.860]   those next generation.
[00:43:35.860 --> 00:43:38.380]   My own sense is that there's gonna be,
[00:43:38.380 --> 00:43:40.820]   that's why you're seeing the consolidation, right?
[00:43:40.820 --> 00:43:43.180]   Open AI clearly has hit that escape velocity.
[00:43:43.180 --> 00:43:45.180]   They can fund their own future.
[00:43:45.180 --> 00:43:48.820]   It's not clear to me that many of these other companies can.
[00:43:48.820 --> 00:43:52.300]   Is that a fair kind of review of the state of things
[00:43:52.300 --> 00:43:53.940]   in the model layer that we're going to have
[00:43:53.940 --> 00:43:56.940]   this consolidation like we have in lots of other markets
[00:43:56.940 --> 00:43:58.860]   to market leaders who can afford,
[00:43:58.860 --> 00:44:01.220]   who have an economic engine, an application
[00:44:01.220 --> 00:44:03.860]   that allows them to continue to invest?
[00:44:03.860 --> 00:44:09.860]   - First of all, there's a different fundamental difference
[00:44:09.860 --> 00:44:14.180]   between a model and artificial intelligence, right?
[00:44:14.180 --> 00:44:15.020]   - Yeah.
[00:44:15.020 --> 00:44:17.740]   - A model is an essential ingredient
[00:44:17.740 --> 00:44:19.100]   for artificial intelligence.
[00:44:19.100 --> 00:44:20.540]   It's necessary, but not sufficient.
[00:44:20.540 --> 00:44:21.620]   - Correct.
[00:44:21.620 --> 00:44:26.060]   - And so, and artificial intelligence is a capability,
[00:44:26.060 --> 00:44:27.300]   but for what?
[00:44:27.300 --> 00:44:28.140]   - Right.
[00:44:28.140 --> 00:44:29.100]   - Then what's the application?
[00:44:29.100 --> 00:44:29.940]   - Right.
[00:44:29.940 --> 00:44:32.140]   - The artificial intelligence for self-driving cars
[00:44:32.140 --> 00:44:35.100]   is related to the artificial intelligence
[00:44:35.100 --> 00:44:37.660]   for human or robots, but it's not the same,
[00:44:37.660 --> 00:44:40.020]   which is related to the artificial intelligence
[00:44:40.020 --> 00:44:41.620]   for a chatbot, but not the same.
[00:44:41.620 --> 00:44:42.460]   - Correct.
[00:44:42.460 --> 00:44:46.220]   - And so, you have to understand the taxonomy of--
[00:44:46.220 --> 00:44:47.060]   - Stack.
[00:44:47.060 --> 00:44:48.060]   - Yeah, of the stack.
[00:44:48.060 --> 00:44:49.780]   And at every layer of the stack,
[00:44:49.780 --> 00:44:51.860]   there will be opportunities,
[00:44:51.860 --> 00:44:53.580]   but not infinite opportunities for everybody
[00:44:53.580 --> 00:44:55.460]   at every single layer of the stack.
[00:44:55.460 --> 00:44:57.540]   Now, I just said something,
[00:44:57.540 --> 00:45:01.340]   all you have to do is replace the word model with GPU.
[00:45:01.340 --> 00:45:04.700]   In fact, this was the great observation
[00:45:04.700 --> 00:45:06.900]   of our company 32 years ago,
[00:45:06.900 --> 00:45:10.140]   that there's a fundamental difference between GPU,
[00:45:10.140 --> 00:45:15.140]   graphics chip or GPU, versus accelerated computing.
[00:45:15.140 --> 00:45:18.460]   And accelerated computing is a different thing
[00:45:18.460 --> 00:45:22.140]   than the work that we do with AI infrastructure.
[00:45:22.140 --> 00:45:24.220]   It's related, but it's not exactly the same.
[00:45:24.220 --> 00:45:25.540]   It's built on top of each other.
[00:45:25.540 --> 00:45:26.820]   It's not exactly the same.
[00:45:26.820 --> 00:45:29.380]   And each one of these layers of abstraction
[00:45:29.380 --> 00:45:33.380]   requires fundamental different skills.
[00:45:33.380 --> 00:45:35.740]   Somebody who's really, really good at building GPUs
[00:45:35.740 --> 00:45:38.620]   have no clue how to be an accelerated computing company.
[00:45:38.620 --> 00:45:42.140]   I can, there are a whole lot of people who build GPUs.
[00:45:42.140 --> 00:45:44.980]   And I don't know which one came,
[00:45:44.980 --> 00:45:45.900]   we invented the GPU,
[00:45:45.900 --> 00:45:48.620]   but you know that we're not the only company
[00:45:48.620 --> 00:45:49.660]   that makes GPUs today.
[00:45:49.660 --> 00:45:50.500]   - Correct.
[00:45:50.500 --> 00:45:52.860]   - And so, there are GPUs everywhere,
[00:45:52.860 --> 00:45:55.980]   but they're not accelerated computing companies.
[00:45:55.980 --> 00:45:57.740]   And there are a lot of people who,
[00:45:57.740 --> 00:46:00.620]   you know, they're accelerators,
[00:46:00.620 --> 00:46:03.980]   accelerators that does application acceleration,
[00:46:03.980 --> 00:46:06.300]   but that's different than an accelerated computing company.
[00:46:06.300 --> 00:46:10.500]   And so for example, a very specialized AI application.
[00:46:10.500 --> 00:46:11.340]   - Right.
[00:46:11.340 --> 00:46:12.940]   - Could be a very successful thing.
[00:46:12.940 --> 00:46:13.780]   - Correct.
[00:46:13.780 --> 00:46:14.780]   And that is MTIA.
[00:46:14.780 --> 00:46:15.620]   - That's right.
[00:46:15.620 --> 00:46:17.500]   But it might not be the type of company
[00:46:17.500 --> 00:46:20.860]   that had broad reach and broad capabilities.
[00:46:20.860 --> 00:46:23.620]   And so, you've got to decide where you want to be.
[00:46:23.620 --> 00:46:25.900]   There's opportunities probably in all these different areas,
[00:46:25.900 --> 00:46:27.420]   but like building companies,
[00:46:27.420 --> 00:46:30.380]   you have to be mindful of the shifting of the ecosystem
[00:46:30.380 --> 00:46:32.740]   and what gets commoditized over time.
[00:46:32.740 --> 00:46:37.220]   Recognizing what's a feature versus a product.
[00:46:37.220 --> 00:46:38.060]   - Right.
[00:46:38.060 --> 00:46:38.900]   - Versus a company.
[00:46:38.900 --> 00:46:39.740]   - For sure.
[00:46:39.740 --> 00:46:40.580]   - Okay.
[00:46:40.580 --> 00:46:41.540]   I just went through, okay.
[00:46:41.540 --> 00:46:42.620]   And there's a lot of different ways
[00:46:42.620 --> 00:46:44.020]   you can think about this.
[00:46:44.020 --> 00:46:46.140]   - Of course, there's one new entrant
[00:46:46.140 --> 00:46:49.300]   that has the money, the smarts, the ambition.
[00:46:49.300 --> 00:46:51.020]   That's X.AI.
[00:46:51.020 --> 00:46:51.860]   - Yeah.
[00:46:51.860 --> 00:46:52.700]   - Right?
[00:46:52.700 --> 00:46:54.660]   And well, there are reports out there
[00:46:54.660 --> 00:46:56.780]   that you and Larry and Elon had dinner.
[00:46:56.780 --> 00:47:00.060]   They talked to you out of 100,000 H100s.
[00:47:00.060 --> 00:47:03.660]   They went to Memphis and built a large coherent super cluster
[00:47:03.660 --> 00:47:05.380]   in a matter of months.
[00:47:05.380 --> 00:47:07.580]   - You know.
[00:47:07.580 --> 00:47:11.660]   - So first, three points don't make a line, okay.
[00:47:11.660 --> 00:47:13.340]   Yes, I had dinner with them.
[00:47:13.340 --> 00:47:15.940]   (laughing)
[00:47:15.940 --> 00:47:18.100]   Causality is there.
[00:47:18.100 --> 00:47:19.620]   What do you think about their ability
[00:47:19.620 --> 00:47:21.980]   to stand up that super cluster?
[00:47:21.980 --> 00:47:23.220]   And there's talk out there
[00:47:23.220 --> 00:47:26.700]   that they want another 100,000 H200s, right?
[00:47:26.700 --> 00:47:29.300]   To expand the size of that super cluster.
[00:47:29.300 --> 00:47:32.060]   You know, first talk to us a little bit about X
[00:47:32.060 --> 00:47:33.580]   and their ambitions and what they've achieved.
[00:47:33.580 --> 00:47:37.900]   But also, are we already at the age of clusters
[00:47:37.900 --> 00:47:40.740]   of 200 and 300,000 GPUs?
[00:47:40.740 --> 00:47:43.660]   - The answer is yes.
[00:47:43.660 --> 00:47:46.540]   And then the, first of all,
[00:47:46.540 --> 00:47:51.860]   acknowledgement of achievement where it's deserved.
[00:47:51.860 --> 00:47:56.860]   From the moment of concept to a data center
[00:47:56.860 --> 00:48:01.900]   that's ready for NVIDIA to have our gear there,
[00:48:01.900 --> 00:48:05.420]   to the moment that we powered it on,
[00:48:05.420 --> 00:48:08.740]   had it all hooked up, and it did its first training.
[00:48:08.740 --> 00:48:09.580]   - Yeah.
[00:48:09.580 --> 00:48:10.420]   - Okay?
[00:48:10.420 --> 00:48:11.260]   - Correct.
[00:48:11.780 --> 00:48:16.780]   - That first part, just building a massive factory,
[00:48:16.780 --> 00:48:21.500]   liquid cooled, energized, permitted,
[00:48:21.500 --> 00:48:23.820]   in the short time that was done.
[00:48:23.820 --> 00:48:26.940]   I mean, that is like superhuman.
[00:48:26.940 --> 00:48:27.780]   - Right.
[00:48:27.780 --> 00:48:30.380]   - Yeah, and as far as I know,
[00:48:30.380 --> 00:48:32.420]   there's only one person in the world who could do that.
[00:48:32.420 --> 00:48:33.260]   - Right.
[00:48:33.260 --> 00:48:35.860]   - I mean, Elon is singular in this understanding
[00:48:35.860 --> 00:48:39.820]   of engineering and construction and large systems
[00:48:39.820 --> 00:48:44.820]   and marshaling resources.
[00:48:44.820 --> 00:48:45.780]   - Incredible.
[00:48:45.780 --> 00:48:47.340]   - Yeah, just, it's unbelievable.
[00:48:47.340 --> 00:48:50.220]   And then, and of course,
[00:48:50.220 --> 00:48:52.260]   then his engineering team is extraordinary.
[00:48:52.260 --> 00:48:54.100]   I mean, the software team's great,
[00:48:54.100 --> 00:48:55.060]   the networking team's great,
[00:48:55.060 --> 00:48:56.300]   the infrastructure team is great.
[00:48:56.300 --> 00:48:58.580]   You know, Elon understands this deeply.
[00:48:58.580 --> 00:49:02.580]   And from the moment that we decided to get to go,
[00:49:02.580 --> 00:49:06.300]   the planning with our engineering team,
[00:49:06.300 --> 00:49:08.940]   our networking team, our infrastructure computing team,
[00:49:08.940 --> 00:49:12.140]   the software team, all of the preparation advance,
[00:49:12.140 --> 00:49:17.540]   then all of the infrastructure, all of the logistics
[00:49:17.540 --> 00:49:21.020]   and the amount of technology and equipment that came in
[00:49:21.020 --> 00:49:23.740]   on that day, NVIDIA's infrastructure
[00:49:23.740 --> 00:49:26.420]   and computing infrastructure and all that technology,
[00:49:26.420 --> 00:49:28.460]   to training, 19 days.
[00:49:28.460 --> 00:49:32.540]   Hang on, you just, you know what?
[00:49:32.540 --> 00:49:34.980]   - Did anybody sleep 24/7?
[00:49:34.980 --> 00:49:36.300]   - No question that nobody slept.
[00:49:36.860 --> 00:49:41.700]   But first of all, 19 days is incredible,
[00:49:41.700 --> 00:49:43.820]   but it's also kind of nice to just take a step back
[00:49:43.820 --> 00:49:46.780]   and just, do you know how many days 19 days is?
[00:49:46.780 --> 00:49:48.500]   It's just a couple of weeks.
[00:49:48.500 --> 00:49:51.700]   And the amount of technology, if you were to see it,
[00:49:51.700 --> 00:49:52.580]   is unbelievable.
[00:49:52.580 --> 00:49:55.700]   All of the wiring and the networking and, you know,
[00:49:55.700 --> 00:49:58.020]   networking NVIDIA gear is very different
[00:49:58.020 --> 00:50:00.620]   than networking hyperscale data centers, okay?
[00:50:00.620 --> 00:50:03.500]   The number of wires that goes in one node,
[00:50:03.500 --> 00:50:05.900]   the back of a computer is all wires.
[00:50:05.900 --> 00:50:09.100]   And just getting this mountain of technology integrated
[00:50:09.100 --> 00:50:11.060]   and all the software, incredible.
[00:50:11.060 --> 00:50:14.700]   Yeah, so I think what Elon and the X team did,
[00:50:14.700 --> 00:50:19.180]   and I'm really appreciative that he acknowledges
[00:50:19.180 --> 00:50:21.580]   the engineering work that we did with him
[00:50:21.580 --> 00:50:24.300]   and the planning work and all that stuff.
[00:50:24.300 --> 00:50:28.620]   But what they achieved is singular, never been done before.
[00:50:28.620 --> 00:50:31.740]   Just to put in perspective, 100,000 GPUs,
[00:50:31.740 --> 00:50:35.180]   that's easily the fastest supercomputer on the planet
[00:50:35.180 --> 00:50:36.260]   as one cluster.
[00:50:36.260 --> 00:50:40.940]   A supercomputer that you would build
[00:50:40.940 --> 00:50:43.980]   would take normally three years to plan.
[00:50:43.980 --> 00:50:44.820]   - Right.
[00:50:44.820 --> 00:50:46.620]   - And then they deliver the equipment
[00:50:46.620 --> 00:50:51.340]   and it takes one year to get it all working.
[00:50:51.340 --> 00:50:52.180]   - Yes.
[00:50:52.180 --> 00:50:53.980]   - We're talking about 19 days.
[00:50:53.980 --> 00:50:54.820]   - Wow.
[00:50:54.820 --> 00:50:57.180]   - What's the credit of the NVIDIA platform, right?
[00:50:57.180 --> 00:50:59.420]   That it's, the whole processes are hardened.
[00:50:59.420 --> 00:51:00.780]   - That's right, yeah.
[00:51:00.780 --> 00:51:02.460]   Everything's already working.
[00:51:02.460 --> 00:51:05.380]   And of course there's a whole bunch of X algorithms
[00:51:05.380 --> 00:51:08.580]   and X framework and X stack and things like that.
[00:51:08.580 --> 00:51:11.580]   And we got a ton of integration we have to do,
[00:51:11.580 --> 00:51:13.420]   but the planning of it was extraordinary.
[00:51:13.420 --> 00:51:15.700]   Just pre-planning of it to, you know.
[00:51:15.700 --> 00:51:16.860]   - N of one is right.
[00:51:16.860 --> 00:51:18.500]   Elon is an N of one.
[00:51:18.500 --> 00:51:20.940]   But you answered that question by starting off saying,
[00:51:20.940 --> 00:51:25.940]   yes, 200 to 300,000 GPU clusters are here, right?
[00:51:25.940 --> 00:51:31.100]   Does that scale to 500,000?
[00:51:31.100 --> 00:51:34.260]   Does it scale to a million?
[00:51:34.260 --> 00:51:38.340]   And does the demand for your products
[00:51:38.340 --> 00:51:41.980]   depend on it scaling to millions?
[00:51:41.980 --> 00:51:46.220]   - That part, the last part is no.
[00:51:46.220 --> 00:51:50.140]   My sense is that distributed training will have to work.
[00:51:50.140 --> 00:51:50.980]   - Right.
[00:51:50.980 --> 00:51:53.500]   - And my sense is that distributed computing
[00:51:53.500 --> 00:51:54.660]   will be invented.
[00:51:54.660 --> 00:51:55.500]   - Right.
[00:51:55.500 --> 00:51:57.900]   - And some form of federated learning
[00:51:58.340 --> 00:52:03.340]   and distributed, asynchronous distributed computing
[00:52:03.340 --> 00:52:06.180]   is going to be discovered.
[00:52:06.180 --> 00:52:09.540]   And I'm very enthusiastic and very optimistic about that.
[00:52:09.540 --> 00:52:17.260]   Of course, the thing to realize is that
[00:52:17.260 --> 00:52:20.220]   the scaling law used to be about pre-training.
[00:52:20.220 --> 00:52:22.060]   Now we've gone to multimodality,
[00:52:22.060 --> 00:52:24.020]   we've gone to synthetic data generation.
[00:52:24.020 --> 00:52:25.220]   - Right.
[00:52:25.220 --> 00:52:29.740]   - Post-training has now scaled up incredibly.
[00:52:29.740 --> 00:52:32.140]   Synthetic data generation, reward systems,
[00:52:32.140 --> 00:52:33.820]   reinforcement learning based.
[00:52:33.820 --> 00:52:37.980]   And then now inference scaling has gone through the roof.
[00:52:37.980 --> 00:52:38.820]   - Right.
[00:52:38.820 --> 00:52:42.460]   - The idea that a model, before it answers your answer,
[00:52:42.460 --> 00:52:47.460]   had already done internal inference 10,000 times,
[00:52:47.460 --> 00:52:49.460]   is probably not unreasonable.
[00:52:49.460 --> 00:52:50.980]   And it's probably done tree search,
[00:52:50.980 --> 00:52:52.660]   it's probably done reinforcement learning on that,
[00:52:52.660 --> 00:52:55.860]   it's probably done some simulations,
[00:52:55.860 --> 00:52:57.260]   surely done a lot of reflection,
[00:52:57.260 --> 00:52:58.500]   it probably looked up some data,
[00:52:58.500 --> 00:53:00.420]   it looked up some information, isn't that right?
[00:53:00.420 --> 00:53:02.860]   And so its context is probably fairly large.
[00:53:02.860 --> 00:53:06.540]   I mean, this type of intelligence is,
[00:53:06.540 --> 00:53:08.500]   well, that's what we do.
[00:53:08.500 --> 00:53:09.340]   - Right.
[00:53:09.340 --> 00:53:10.740]   - That's what we do, isn't that right?
[00:53:10.740 --> 00:53:14.500]   And so the ability, this scaling,
[00:53:14.500 --> 00:53:18.900]   if you just did that math and you compound it with,
[00:53:18.900 --> 00:53:21.900]   you compound that with 4X per year
[00:53:21.900 --> 00:53:25.340]   on model size and computing size.
[00:53:25.340 --> 00:53:26.460]   And then on the other hand,
[00:53:26.460 --> 00:53:28.540]   demand continues to grow in usage.
[00:53:28.540 --> 00:53:32.060]   Do we think that we need millions of GPUs?
[00:53:32.060 --> 00:53:33.060]   No doubt.
[00:53:33.060 --> 00:53:33.900]   - Yeah.
[00:53:33.900 --> 00:53:35.820]   - Yeah, that is a first certainty now.
[00:53:35.820 --> 00:53:36.660]   - Yeah.
[00:53:36.660 --> 00:53:37.580]   - And so the question is,
[00:53:37.580 --> 00:53:39.860]   how do we architect it from a data center perspective?
[00:53:39.860 --> 00:53:42.020]   And that has a lot to do with,
[00:53:42.020 --> 00:53:45.620]   are there data centers that are gigawatts at a time,
[00:53:45.620 --> 00:53:47.380]   or are they 250 megawatts at a time?
[00:53:47.380 --> 00:53:51.660]   And my sense is that you're gonna get both.
[00:53:51.660 --> 00:53:54.060]   - I think analysts always focus on
[00:53:54.060 --> 00:53:56.060]   the current architectural bet.
[00:53:56.060 --> 00:53:58.060]   But I think one of the biggest takeaways
[00:53:58.060 --> 00:53:59.700]   from this conversation is that
[00:53:59.700 --> 00:54:02.700]   you're thinking about the entire ecosystem
[00:54:02.700 --> 00:54:04.140]   and many years out.
[00:54:04.140 --> 00:54:07.620]   So the idea that,
[00:54:07.620 --> 00:54:10.220]   because NVIDIA is just scaling up or scaling out,
[00:54:10.220 --> 00:54:12.940]   it's to meet the future.
[00:54:12.940 --> 00:54:17.140]   It's not such that you're only dependent on a world
[00:54:17.140 --> 00:54:21.060]   where there's a 500,000 or a million GPU cluster.
[00:54:21.060 --> 00:54:24.740]   By the time there's distributed training,
[00:54:24.740 --> 00:54:27.860]   you'll have written the software to enable that.
[00:54:27.860 --> 00:54:28.700]   - That's right.
[00:54:28.700 --> 00:54:32.580]   Remember, without Megatron that we developed
[00:54:32.580 --> 00:54:34.500]   some seven years ago now,
[00:54:34.500 --> 00:54:36.700]   the scaling of these large training jobs
[00:54:36.700 --> 00:54:38.060]   wouldn't have happened.
[00:54:38.060 --> 00:54:39.700]   And so we invented Megatron,
[00:54:39.700 --> 00:54:42.820]   we invented Nickel, GPU Direct,
[00:54:42.820 --> 00:54:45.300]   all of the work that we did with RDMA,
[00:54:45.300 --> 00:54:47.660]   that made it possible for easily to do
[00:54:47.660 --> 00:54:52.180]   pipeline parallelism, right?
[00:54:52.180 --> 00:54:56.300]   And so all the model parallelism that's being done,
[00:54:56.300 --> 00:54:58.460]   all the breaking of the distributed training
[00:54:58.460 --> 00:54:59.980]   and all the batching and all that,
[00:54:59.980 --> 00:55:04.540]   all of that stuff is because we did the early work.
[00:55:04.540 --> 00:55:05.780]   And now we're doing the early work
[00:55:05.780 --> 00:55:07.580]   for the future generation.
[00:55:07.580 --> 00:55:11.060]   - So let's talk about Strawberry and O1.
[00:55:11.060 --> 00:55:12.700]   I wanna be respectful of your time.
[00:55:12.700 --> 00:55:14.580]   - I got all the time in the world, guys.
[00:55:14.580 --> 00:55:15.980]   - Well, you're very generous.
[00:55:15.980 --> 00:55:17.620]   - Yeah, I've got all the time in the world.
[00:55:17.620 --> 00:55:21.140]   - But first, I think it's cool that they named O1
[00:55:21.140 --> 00:55:23.460]   after the O1 visa, right?
[00:55:23.460 --> 00:55:26.940]   Which is about recruiting the world's best and brightest,
[00:55:26.940 --> 00:55:29.020]   you know, and bringing them to the United States.
[00:55:29.020 --> 00:55:32.380]   It's something I know we're both deeply passionate about.
[00:55:32.380 --> 00:55:35.980]   So I love the idea that building a model that thinks
[00:55:35.980 --> 00:55:37.700]   and that takes us to the next level
[00:55:37.700 --> 00:55:40.100]   of scaling intelligence, right?
[00:55:40.100 --> 00:55:43.900]   Is an homage to the fact that it's these people
[00:55:43.900 --> 00:55:47.700]   who come to the United States by way of immigration
[00:55:47.700 --> 00:55:49.340]   that have made us what we are,
[00:55:49.340 --> 00:55:51.220]   bring their collective intelligence to the United States.
[00:55:51.220 --> 00:55:53.660]   - Surely an alien intelligence.
[00:55:53.660 --> 00:55:54.500]   - Certainly.
[00:55:54.500 --> 00:55:55.340]   - Yeah.
[00:55:55.340 --> 00:55:56.540]   - You know, it was spearheaded by our friend,
[00:55:56.540 --> 00:55:57.900]   Noam Brown, of course.
[00:55:57.900 --> 00:56:01.140]   He worked at Pluribus and Cicero when he was at Meta.
[00:56:01.140 --> 00:56:04.180]   How big a deal is inference time reasoning
[00:56:04.180 --> 00:56:08.140]   as a totally new vector of scaling intelligence,
[00:56:08.140 --> 00:56:10.660]   separate and distinct from, right,
[00:56:10.660 --> 00:56:12.580]   just building larger models?
[00:56:12.580 --> 00:56:13.580]   - It's a huge deal.
[00:56:13.580 --> 00:56:14.420]   It's a huge deal.
[00:56:14.420 --> 00:56:15.260]   I think the,
[00:56:15.260 --> 00:56:21.020]   a lot of intelligence can't be done a priori.
[00:56:21.020 --> 00:56:21.940]   - Right.
[00:56:21.940 --> 00:56:24.780]   - You know, and a lot of computing,
[00:56:24.780 --> 00:56:28.300]   even a lot of computing can't be reordered.
[00:56:28.300 --> 00:56:30.820]   I mean, just, you know, out of order execution
[00:56:30.820 --> 00:56:32.500]   can't be done a priori, you know?
[00:56:32.500 --> 00:56:35.740]   And so a lot of things that can only be done in runtime.
[00:56:35.740 --> 00:56:36.580]   - Right.
[00:56:36.580 --> 00:56:38.940]   - And so whether you think about it
[00:56:38.940 --> 00:56:40.300]   from a computer science perspective,
[00:56:40.340 --> 00:56:45.140]   or you think about it from an intelligence perspective,
[00:56:45.140 --> 00:56:47.580]   too much of it requires context.
[00:56:47.580 --> 00:56:48.620]   - Right.
[00:56:48.620 --> 00:56:49.860]   - The circumstance.
[00:56:49.860 --> 00:56:50.820]   - Right.
[00:56:50.820 --> 00:56:54.420]   - The type of answer you're looking for.
[00:56:54.420 --> 00:56:56.340]   Sometimes just a quick answer is good enough.
[00:56:56.340 --> 00:56:57.380]   - Right.
[00:56:57.380 --> 00:57:02.180]   - Depends on the consequential, you know,
[00:57:02.180 --> 00:57:03.580]   impact of the answer.
[00:57:03.580 --> 00:57:04.420]   - Right.
[00:57:04.420 --> 00:57:05.700]   - You know, depending on the nature of the usage
[00:57:05.700 --> 00:57:06.540]   of that answer.
[00:57:06.540 --> 00:57:11.020]   So some answers, "Please take a night."
[00:57:11.020 --> 00:57:12.180]   Some answers, "Take a week."
[00:57:12.180 --> 00:57:13.020]   - Yes.
[00:57:13.020 --> 00:57:13.860]   - Is that right?
[00:57:13.860 --> 00:57:16.980]   So I could totally imagine me sending off a prompt
[00:57:16.980 --> 00:57:20.420]   to my AI and telling it, you know,
[00:57:20.420 --> 00:57:21.540]   "Think about it for a night."
[00:57:21.540 --> 00:57:22.380]   - Right.
[00:57:22.380 --> 00:57:23.220]   - "Think about it overnight.
[00:57:23.220 --> 00:57:24.300]   "Don't tell me right away."
[00:57:24.300 --> 00:57:25.140]   - Right.
[00:57:25.140 --> 00:57:26.780]   - "I want you to think about it all night.
[00:57:26.780 --> 00:57:27.860]   "And then come back and tell me tomorrow
[00:57:27.860 --> 00:57:30.580]   "what's your best answer and reason about it for me."
[00:57:30.580 --> 00:57:35.580]   And so I think the segmentation of intelligence
[00:57:36.580 --> 00:57:39.060]   from now, from a product perspective,
[00:57:39.060 --> 00:57:40.700]   there's going to be one-shot versions of it.
[00:57:40.700 --> 00:57:41.700]   - Right, for sure.
[00:57:41.700 --> 00:57:42.540]   - Yeah.
[00:57:42.540 --> 00:57:46.940]   And then there'll be some that take five minutes, you know.
[00:57:46.940 --> 00:57:49.580]   - And the intelligence layer that roots those questions
[00:57:49.580 --> 00:57:50.660]   to the right model.
[00:57:50.660 --> 00:57:51.500]   - Yeah.
[00:57:51.500 --> 00:57:52.340]   - For the right use case.
[00:57:52.340 --> 00:57:53.780]   I mean, we were using advanced voice mode
[00:57:53.780 --> 00:57:55.900]   and no one preview last night.
[00:57:55.900 --> 00:58:00.740]   I was coaching my son for his AP history test.
[00:58:00.740 --> 00:58:04.540]   And it was like having the world's best AP history teacher.
[00:58:04.540 --> 00:58:05.380]   - Yeah, right.
[00:58:05.380 --> 00:58:06.220]   - Right next to you.
[00:58:06.220 --> 00:58:07.060]   - Yeah.
[00:58:07.060 --> 00:58:08.100]   - Thinking about these questions.
[00:58:08.100 --> 00:58:09.860]   It was truly extraordinary.
[00:58:09.860 --> 00:58:11.140]   Again, they're--
[00:58:11.140 --> 00:58:12.700]   - My tutor's an AI today.
[00:58:12.700 --> 00:58:13.540]   - Right, right.
[00:58:13.540 --> 00:58:14.380]   - I'm serious.
[00:58:14.380 --> 00:58:15.220]   - Of course, they're here today.
[00:58:15.220 --> 00:58:16.060]   - Yeah.
[00:58:16.060 --> 00:58:17.020]   - Which comes back to this, you know,
[00:58:17.020 --> 00:58:19.780]   over 40% of your revenue today is inference.
[00:58:19.780 --> 00:58:21.380]   But inference is about ready
[00:58:21.380 --> 00:58:24.180]   because of chain of reasoning.
[00:58:24.180 --> 00:58:25.020]   - Yeah.
[00:58:25.020 --> 00:58:25.860]   - Right?
[00:58:25.860 --> 00:58:26.700]   It's about ready--
[00:58:26.700 --> 00:58:27.540]   - It's about to go up by a billion times.
[00:58:27.540 --> 00:58:30.700]   - Right, by a million X, by a billion X.
[00:58:30.700 --> 00:58:31.540]   - That's right.
[00:58:31.540 --> 00:58:34.660]   That's the part that most people have, you know,
[00:58:34.660 --> 00:58:36.500]   haven't completely internalized.
[00:58:36.500 --> 00:58:38.380]   This is that industry we were talking about,
[00:58:38.380 --> 00:58:40.540]   but this is the industrial revolution.
[00:58:40.540 --> 00:58:41.860]   - Right.
[00:58:41.860 --> 00:58:43.980]   That's the production of intelligence.
[00:58:43.980 --> 00:58:44.820]   - That's right.
[00:58:44.820 --> 00:58:45.660]   - Right?
[00:58:45.660 --> 00:58:46.500]   - Yeah.
[00:58:46.500 --> 00:58:47.340]   It's going to go up a billion times.
[00:58:47.340 --> 00:58:48.180]   - Right.
[00:58:48.180 --> 00:58:51.740]   And so, you know, everybody's so hyper-focused on NVIDIA
[00:58:51.740 --> 00:58:55.380]   as kind of like doing training on bigger models.
[00:58:55.380 --> 00:58:56.220]   - Yeah.
[00:58:56.220 --> 00:58:57.060]   - Right?
[00:58:57.060 --> 00:59:00.420]   Isn't it the case that your revenue, if it's 50/50 today,
[00:59:00.420 --> 00:59:02.940]   you're going to do way more inference in the future.
[00:59:02.940 --> 00:59:03.780]   - Yeah.
[00:59:03.780 --> 00:59:06.180]   - And then, I mean, training will always be important,
[00:59:06.180 --> 00:59:09.260]   but just the growth of inference is going to be way larger
[00:59:09.260 --> 00:59:10.380]   than the growth in training.
[00:59:10.380 --> 00:59:11.220]   - We hope, we hope.
[00:59:11.220 --> 00:59:12.780]   - It's almost impossible to conceive otherwise.
[00:59:12.780 --> 00:59:13.620]   - Yeah, we hope.
[00:59:13.620 --> 00:59:14.460]   That's right, that's right.
[00:59:14.460 --> 00:59:15.300]   - Right.
[00:59:15.300 --> 00:59:17.220]   - Yeah, I mean, it's good to go to school.
[00:59:17.220 --> 00:59:18.060]   - Yes.
[00:59:18.060 --> 00:59:19.620]   - But the goal is so that you can be productive
[00:59:19.620 --> 00:59:20.700]   in society later.
[00:59:20.700 --> 00:59:22.540]   And so it's good that we train these models,
[00:59:22.540 --> 00:59:24.740]   but the goal is to inference them, you know?
[00:59:24.740 --> 00:59:29.740]   - Are you already using chain of reasoning and, you know,
[00:59:30.100 --> 00:59:32.380]   tools like O1 in your own business
[00:59:32.380 --> 00:59:33.860]   to improve your own business?
[00:59:33.860 --> 00:59:35.580]   - Yeah, our cybersecurity system today
[00:59:35.580 --> 00:59:38.780]   can't run without our own agents.
[00:59:38.780 --> 00:59:39.620]   - Okay.
[00:59:39.620 --> 00:59:42.340]   - We have agents helping us design chips.
[00:59:42.340 --> 00:59:43.420]   Hopper wouldn't be possible.
[00:59:43.420 --> 00:59:44.420]   Blackwell wouldn't be possible.
[00:59:44.420 --> 00:59:46.220]   Rubin, don't even think about it.
[00:59:46.220 --> 00:59:49.420]   We have digital, we have AI chip designers,
[00:59:49.420 --> 00:59:52.100]   AI software engineers, AI verification engineers.
[00:59:52.100 --> 00:59:55.340]   And we build them all inside because, you know,
[00:59:55.340 --> 00:59:59.140]   we have the ability and we rather use it,
[00:59:59.140 --> 01:00:01.660]   use the opportunity to explore the technology ourselves.
[01:00:01.660 --> 01:00:03.380]   - You know, when I walked into the building today,
[01:00:03.380 --> 01:00:05.380]   somebody came up to me and said, you know,
[01:00:05.380 --> 01:00:06.820]   "Ask Jensen about the culture.
[01:00:06.820 --> 01:00:08.020]   "It's all about the culture."
[01:00:08.020 --> 01:00:09.740]   I look at the business, you know,
[01:00:09.740 --> 01:00:12.140]   we talk a lot about fitness and efficiency,
[01:00:12.140 --> 01:00:16.380]   flat organizations that can execute quickly, smaller teams.
[01:00:16.380 --> 01:00:20.580]   You know, NVIDIA is in a league of its own, really,
[01:00:20.580 --> 01:00:23.860]   you know, at about 4 million of revenue per employee,
[01:00:23.860 --> 01:00:28.620]   about 2 million of profits or free cashflow per employee.
[01:00:28.620 --> 01:00:30.660]   You've built a culture of efficiency
[01:00:30.660 --> 01:00:35.660]   that really has unleashed creativity and innovation
[01:00:35.660 --> 01:00:37.780]   and ownership and responsibility.
[01:00:37.780 --> 01:00:40.140]   You've broken the mold on kind of functional management.
[01:00:40.140 --> 01:00:44.420]   Everybody likes to talk about all of your direct reports.
[01:00:44.420 --> 01:00:49.220]   Is the leveraging of AI the thing
[01:00:49.220 --> 01:00:53.220]   that's going to continue to allow you to be hyper-creative
[01:00:53.220 --> 01:00:55.500]   while at the same time being efficient?
[01:00:55.500 --> 01:00:56.900]   - No question.
[01:00:56.900 --> 01:00:59.420]   I'm hoping that someday,
[01:00:59.420 --> 01:01:02.180]   NVIDIA has 32,000 employees today.
[01:01:02.180 --> 01:01:05.140]   And we have 4,000 families in Israel.
[01:01:05.140 --> 01:01:06.060]   I hope they're well.
[01:01:06.060 --> 01:01:07.260]   I'm thinking of you guys.
[01:01:07.260 --> 01:01:12.060]   And I'm hoping that NVIDIA someday
[01:01:12.060 --> 01:01:14.660]   will be a 50,000 employee company
[01:01:14.660 --> 01:01:20.860]   with a hundred million, you know, AI assistants.
[01:01:20.860 --> 01:01:24.180]   And they're in every single group.
[01:01:25.540 --> 01:01:29.340]   We'll have a whole directory of AIs
[01:01:29.340 --> 01:01:31.820]   that are just generally good at doing things.
[01:01:31.820 --> 01:01:34.620]   We'll also have, our inbox is gonna full of directories
[01:01:34.620 --> 01:01:37.540]   of AIs that we work with that we know are really good,
[01:01:37.540 --> 01:01:39.580]   specialized at our skill.
[01:01:39.580 --> 01:01:43.340]   And so AIs will recruit other AIs to solve problems.
[01:01:43.340 --> 01:01:46.340]   AIs will be in, you know, Slack channels with each other.
[01:01:46.340 --> 01:01:47.180]   - And with humans.
[01:01:47.180 --> 01:01:48.620]   - Right, and with humans.
[01:01:48.620 --> 01:01:52.020]   And so we'll just be one large, you know,
[01:01:52.020 --> 01:01:54.220]   employee base, if you will.
[01:01:54.220 --> 01:01:57.460]   Some of 'em are digital and AI, some of 'em are biological.
[01:01:57.460 --> 01:02:00.540]   And I'm hoping some of 'em even in megatronics.
[01:02:00.540 --> 01:02:03.180]   - I think from a business perspective,
[01:02:03.180 --> 01:02:04.980]   it's something that's greatly misunderstood.
[01:02:04.980 --> 01:02:09.980]   You just described a company that's producing the output
[01:02:09.980 --> 01:02:13.060]   of a company with 150,000 people,
[01:02:13.060 --> 01:02:14.860]   but you're doing it with 50,000 people.
[01:02:14.860 --> 01:02:17.900]   Now, you didn't say I was gonna get rid of all my employees.
[01:02:17.900 --> 01:02:20.100]   You're still growing the number of employees
[01:02:20.100 --> 01:02:24.060]   in the organization, but the output of that organization,
[01:02:24.060 --> 01:02:26.020]   right, is gonna be dramatically more.
[01:02:26.020 --> 01:02:28.380]   - This is often misunderstood.
[01:02:28.380 --> 01:02:33.380]   AI is not, it's not, AI will change every job.
[01:02:33.380 --> 01:02:37.780]   AI will have a seismic impact
[01:02:37.780 --> 01:02:39.820]   on how people think about work.
[01:02:39.820 --> 01:02:41.780]   Let's acknowledge that.
[01:02:41.780 --> 01:02:44.420]   AI has the potential to do incredible good.
[01:02:44.420 --> 01:02:46.180]   It has the potential to do harm.
[01:02:46.180 --> 01:02:49.220]   We have to build safe AI.
[01:02:49.220 --> 01:02:51.420]   Let's just make that foundational, okay?
[01:02:51.420 --> 01:02:54.300]   The part that is overlooked is
[01:02:54.300 --> 01:02:58.220]   when companies become more productive
[01:02:58.220 --> 01:03:00.140]   using artificial intelligence,
[01:03:00.140 --> 01:03:02.860]   it is likely that it manifests itself
[01:03:02.860 --> 01:03:07.860]   into either better earnings or better growth or both.
[01:03:07.860 --> 01:03:08.860]   - Right.
[01:03:08.860 --> 01:03:13.180]   - And when that happens, the next email from the CEO
[01:03:13.180 --> 01:03:16.180]   is likely not a layoff announcement.
[01:03:16.180 --> 01:03:17.900]   - Of course, 'cause you're growing.
[01:03:17.900 --> 01:03:19.420]   - Yeah, and the reason for that is
[01:03:19.420 --> 01:03:21.940]   because we have more ideas than we can explore,
[01:03:21.940 --> 01:03:24.660]   and we need people to help us think through it
[01:03:24.660 --> 01:03:25.980]   before we automate it.
[01:03:25.980 --> 01:03:30.660]   And so the automation part of it, AI can help us do.
[01:03:30.660 --> 01:03:33.540]   Obviously, it's gonna help us think through it as well,
[01:03:33.540 --> 01:03:35.740]   but it's still gonna require us to go figure out
[01:03:35.740 --> 01:03:37.740]   what problems do I wanna solve?
[01:03:37.740 --> 01:03:39.340]   There are a trillion things we can go solve.
[01:03:39.340 --> 01:03:41.660]   What problems does this company have to go solve?
[01:03:41.660 --> 01:03:44.340]   And select those ideas and figure out a way
[01:03:44.340 --> 01:03:46.740]   to automate and scale.
[01:03:46.740 --> 01:03:49.180]   And so as a result, we're gonna hire more people
[01:03:49.180 --> 01:03:50.700]   as we become more productive.
[01:03:50.700 --> 01:03:52.780]   People forget that, you know?
[01:03:52.780 --> 01:03:55.780]   And if you go back in time,
[01:03:55.780 --> 01:03:58.860]   obviously we have more ideas today than 200 years ago.
[01:03:58.860 --> 01:04:00.300]   That's the reason why GDPs are larger
[01:04:00.300 --> 01:04:01.420]   and more people are employed,
[01:04:01.420 --> 01:04:04.540]   and even though we're automating like crazy underneath.
[01:04:04.540 --> 01:04:07.220]   - It's such an important point
[01:04:07.220 --> 01:04:09.500]   of this period that we're entering.
[01:04:09.500 --> 01:04:13.300]   One, almost all human productivity,
[01:04:13.300 --> 01:04:16.460]   almost all human prosperity is the byproduct
[01:04:16.460 --> 01:04:20.580]   of the automation and the technology of the last 200 years.
[01:04:20.580 --> 01:04:22.500]   I mean, you can look at, you know,
[01:04:22.500 --> 01:04:26.700]   from Adam Smith and Schumpeter's creative destruction,
[01:04:26.700 --> 01:04:29.740]   you can look at charted GDP growth per person
[01:04:29.740 --> 01:04:31.460]   over the course of the last 200 years,
[01:04:31.460 --> 01:04:33.140]   and it's just accelerated.
[01:04:33.140 --> 01:04:35.100]   Which leads me to this question.
[01:04:35.100 --> 01:04:36.300]   If you look at the '90s,
[01:04:36.300 --> 01:04:38.660]   our productivity growth in the United States
[01:04:38.660 --> 01:04:41.820]   was about 2 1/2 to 3% a year, okay?
[01:04:41.820 --> 01:04:46.100]   And then in the 2000s, it slowed down to about 1.8%.
[01:04:46.100 --> 01:04:47.340]   And then the last 10 years
[01:04:47.340 --> 01:04:49.660]   has been the slowest productivity growth.
[01:04:49.660 --> 01:04:51.820]   So that's the amount of labor and capital,
[01:04:51.820 --> 01:04:53.140]   or the amount of output we have
[01:04:53.140 --> 01:04:54.780]   for a fixed amount of labor and capital.
[01:04:54.780 --> 01:04:58.100]   The slowest we've had on record, actually.
[01:04:58.100 --> 01:05:00.540]   And a lot of people have debated the reasoning for this,
[01:05:00.540 --> 01:05:02.980]   but if the world is as you just described,
[01:05:02.980 --> 01:05:06.500]   and we're going to leverage and manufacture intelligence,
[01:05:06.500 --> 01:05:08.660]   then isn't it the case that we're on the verge
[01:05:08.660 --> 01:05:12.100]   of a dramatic expansion in terms of human productivity?
[01:05:12.100 --> 01:05:13.020]   - That's our hope.
[01:05:13.020 --> 01:05:14.060]   - Right. - That's our hope.
[01:05:14.060 --> 01:05:17.100]   And of course, you know, we live in this world,
[01:05:17.100 --> 01:05:18.620]   so we have direct evidence of it.
[01:05:18.620 --> 01:05:19.460]   - Right.
[01:05:19.460 --> 01:05:21.300]   - We have direct evidence of it,
[01:05:21.300 --> 01:05:25.900]   either as isolated of a case as a individual researcher.
[01:05:25.900 --> 01:05:26.740]   - For sure.
[01:05:26.740 --> 01:05:28.540]   - Who is able to, with AI,
[01:05:28.540 --> 01:05:33.380]   now explore science at such an extraordinary scale
[01:05:33.380 --> 01:05:35.940]   that is unimaginable.
[01:05:35.940 --> 01:05:36.940]   That's productivity.
[01:05:36.940 --> 01:05:37.780]   - Right, 100%.
[01:05:37.780 --> 01:05:39.260]   - Measure of productivity.
[01:05:39.260 --> 01:05:44.260]   Or that we're designing chips that are so incredible
[01:05:44.260 --> 01:05:48.620]   at such a high pace and the chip complexities
[01:05:48.620 --> 01:05:51.140]   and the computer complexities we're building
[01:05:51.140 --> 01:05:52.660]   are going up exponentially
[01:05:52.660 --> 01:05:54.340]   while the company's employee base
[01:05:54.340 --> 01:05:57.540]   is not measure of productivity.
[01:05:57.540 --> 01:05:58.500]   - Correct.
[01:05:58.500 --> 01:06:00.060]   - The software that we're developing
[01:06:00.060 --> 01:06:01.940]   better and better and better
[01:06:01.940 --> 01:06:05.340]   because we're using AI and supercomputers to help us.
[01:06:05.340 --> 01:06:08.220]   The number of employees is growing barely linearly.
[01:06:08.220 --> 01:06:10.260]   - Okay, okay, okay.
[01:06:10.260 --> 01:06:13.260]   Another demonstration of productivity.
[01:06:13.260 --> 01:06:14.940]   So whether it's, I can go into,
[01:06:14.940 --> 01:06:17.380]   I can spot check it in a whole bunch of different industries.
[01:06:17.380 --> 01:06:18.220]   - Yes.
[01:06:18.220 --> 01:06:19.860]   - I could gut check it myself.
[01:06:19.860 --> 01:06:21.380]   - Yes, you're in business.
[01:06:21.380 --> 01:06:22.220]   - That's right.
[01:06:22.220 --> 01:06:24.820]   And so I can, you know, and of course,
[01:06:24.820 --> 01:06:28.620]   you can't, we could be overfit,
[01:06:28.620 --> 01:06:30.620]   but the artistry of it, of course,
[01:06:30.620 --> 01:06:33.740]   is to generalize what is it that we're observing
[01:06:33.740 --> 01:06:36.020]   and whether this could manifest in other industries.
[01:06:36.020 --> 01:06:39.340]   And there's no question that intelligence
[01:06:39.340 --> 01:06:41.860]   is the single most valuable commodity
[01:06:41.860 --> 01:06:43.180]   the world's ever known.
[01:06:43.180 --> 01:06:45.420]   And now we're gonna manufacture it at scale.
[01:06:45.420 --> 01:06:49.420]   And we, all of us have to get good at,
[01:06:49.420 --> 01:06:51.220]   you know, what would happen
[01:06:51.220 --> 01:06:53.820]   if you're surrounded by these AIs
[01:06:53.820 --> 01:06:58.060]   and they're doing things so incredibly well
[01:06:58.060 --> 01:06:59.420]   and so much better than you?
[01:06:59.420 --> 01:07:00.260]   - Right.
[01:07:00.260 --> 01:07:03.540]   - And when I reflect on that, that's my life.
[01:07:04.420 --> 01:07:06.060]   I have 60 direct reports.
[01:07:06.060 --> 01:07:06.900]   - Right.
[01:07:06.900 --> 01:07:09.220]   - The reason why they're on eStaff
[01:07:09.220 --> 01:07:10.980]   is because they're world-class at what they do.
[01:07:10.980 --> 01:07:12.660]   And they do it better than I do.
[01:07:12.660 --> 01:07:13.500]   - Right.
[01:07:13.500 --> 01:07:14.380]   - Much better than I do.
[01:07:14.380 --> 01:07:15.460]   - Right.
[01:07:15.460 --> 01:07:17.700]   - I have no trouble interacting with them.
[01:07:17.700 --> 01:07:21.100]   And I have no trouble prompt engineering them.
[01:07:21.100 --> 01:07:22.300]   - Right, totally.
[01:07:22.300 --> 01:07:24.540]   - I have no trouble programming them.
[01:07:24.540 --> 01:07:25.380]   - Right, right.
[01:07:25.380 --> 01:07:27.780]   - And so I think that that's the thing
[01:07:27.780 --> 01:07:30.580]   that people are going to learn,
[01:07:30.580 --> 01:07:32.220]   is that they're all gonna be CEOs.
[01:07:32.220 --> 01:07:33.060]   - Right.
[01:07:33.060 --> 01:07:34.780]   - They're all gonna be CEOs of AI agents.
[01:07:34.780 --> 01:07:35.820]   - Right.
[01:07:35.820 --> 01:07:40.820]   - And their ability to have the creativity,
[01:07:40.820 --> 01:07:43.140]   the will,
[01:07:43.140 --> 01:07:49.540]   and some knowledge on how to reason,
[01:07:49.540 --> 01:07:51.100]   break problems down,
[01:07:51.100 --> 01:07:55.580]   so that you can program these AIs
[01:07:55.580 --> 01:07:57.620]   to help you achieve something like I do.
[01:07:57.620 --> 01:07:58.460]   - Right.
[01:07:58.460 --> 01:07:59.300]   - You know, it's called running companies.
[01:07:59.300 --> 01:08:01.500]   - Right, now it's, you mentioned something,
[01:08:01.500 --> 01:08:03.380]   this alignment and the safe AI.
[01:08:03.380 --> 01:08:08.780]   You mentioned the tragedy going on in the Middle East.
[01:08:08.780 --> 01:08:11.820]   You know, we have a lot of autonomy
[01:08:11.820 --> 01:08:14.020]   and a lot of AI that's being used
[01:08:14.020 --> 01:08:15.580]   in different parts of the world.
[01:08:15.580 --> 01:08:18.300]   So let's talk for a second about bad actors,
[01:08:18.300 --> 01:08:23.220]   about safe AI, about coordination with Washington.
[01:08:23.220 --> 01:08:24.460]   How do you feel today?
[01:08:24.460 --> 01:08:25.820]   Are we on the right path?
[01:08:25.820 --> 01:08:28.540]   Do we have a sufficient level of coordination?
[01:08:28.540 --> 01:08:30.340]   You know, I think Mark Zuckerberg has said,
[01:08:30.340 --> 01:08:33.380]   the way we beat the bad AIs is we make the good AIs better.
[01:08:33.380 --> 01:08:38.300]   How would you characterize your view
[01:08:38.300 --> 01:08:41.140]   of how we make sure
[01:08:41.140 --> 01:08:45.500]   that this is a positive net benefit for humanity,
[01:08:45.500 --> 01:08:47.140]   as opposed to, you know,
[01:08:47.140 --> 01:08:50.020]   leaving us in this dystopian world without purpose?
[01:08:50.020 --> 01:08:52.380]   - The conversation about safety
[01:08:52.380 --> 01:08:53.780]   is really important and good.
[01:08:53.780 --> 01:08:54.980]   - Yes.
[01:08:54.980 --> 01:08:57.500]   - The abstracted view,
[01:08:57.500 --> 01:08:59.460]   this conceptual view of AI
[01:08:59.460 --> 01:09:02.460]   being a large giant neural network,
[01:09:02.460 --> 01:09:03.580]   not so good.
[01:09:03.580 --> 01:09:04.420]   - Right, right.
[01:09:04.420 --> 01:09:05.300]   - Okay.
[01:09:05.300 --> 01:09:08.620]   And the reason for that is because as we know,
[01:09:08.620 --> 01:09:10.460]   artificial intelligence and large language models
[01:09:10.460 --> 01:09:11.900]   are related and not the same.
[01:09:11.900 --> 01:09:16.180]   There are many things that are being done
[01:09:16.180 --> 01:09:17.260]   that I think are excellent.
[01:09:17.260 --> 01:09:20.340]   One, open sourcing models
[01:09:20.340 --> 01:09:23.420]   so that the entire community of researchers
[01:09:23.420 --> 01:09:24.540]   and every single industry
[01:09:24.540 --> 01:09:27.420]   and every single company can engage AI
[01:09:27.420 --> 01:09:30.140]   and go learn how to harness this capability
[01:09:30.140 --> 01:09:31.780]   for their application, excellent.
[01:09:31.780 --> 01:09:35.420]   Number two, it is under-celebrated
[01:09:35.420 --> 01:09:38.420]   the amount of technology that is dedicated
[01:09:38.420 --> 01:09:41.740]   to inventing AI to keep AI safe.
[01:09:41.740 --> 01:09:42.580]   - Yes.
[01:09:42.580 --> 01:09:45.820]   - AIs to curate data, to curate information,
[01:09:45.820 --> 01:09:49.060]   to train an AI, AI created to align AI,
[01:09:49.060 --> 01:09:50.420]   synthetic data generation,
[01:09:50.420 --> 01:09:52.820]   AI to expand the knowledge of AI,
[01:09:52.820 --> 01:09:54.980]   to cause it to hallucinate less.
[01:09:54.980 --> 01:09:57.020]   All of the AIs that are being created
[01:09:57.020 --> 01:10:03.340]   for vectorization or graphing or whatever it is,
[01:10:03.340 --> 01:10:06.900]   to inform an AI, guard railing AI,
[01:10:06.900 --> 01:10:08.820]   AIs to monitor other AIs,
[01:10:08.820 --> 01:10:12.900]   that the system of AIs to create safe AI
[01:10:12.900 --> 01:10:14.060]   is under-celebrated.
[01:10:14.060 --> 01:10:14.900]   - Right.
[01:10:14.900 --> 01:10:16.300]   That we've already built.
[01:10:16.300 --> 01:10:19.940]   - That we're building everybody all over the industry,
[01:10:19.940 --> 01:10:22.900]   the methodologies, the red teaming, the process,
[01:10:23.500 --> 01:10:28.500]   the model cards, the evaluation systems,
[01:10:28.500 --> 01:10:30.260]   the benchmarking systems.
[01:10:30.260 --> 01:10:34.100]   All of that, all of the harnesses that are being built
[01:10:34.100 --> 01:10:36.460]   at the velocity that's been built is incredible.
[01:10:36.460 --> 01:10:37.300]   - I wonder if the--
[01:10:37.300 --> 01:10:39.100]   - Under-celebrated, do you guys understand?
[01:10:39.100 --> 01:10:39.940]   - Yes.
[01:10:39.940 --> 01:10:40.780]   - The world still think--
[01:10:40.780 --> 01:10:42.580]   - And there's no government regulation
[01:10:42.580 --> 01:10:43.860]   saying you have to do this.
[01:10:43.860 --> 01:10:44.700]   - Yeah, right.
[01:10:44.700 --> 01:10:46.500]   - This is the actors in the space today
[01:10:46.500 --> 01:10:48.540]   who are building these AIs,
[01:10:48.540 --> 01:10:51.100]   are taking seriously and coordinating
[01:10:51.100 --> 01:10:55.540]   around best practices with respect to these critical matters.
[01:10:55.540 --> 01:10:56.460]   - That's right, exactly.
[01:10:56.460 --> 01:10:59.420]   And so that's under-celebrated, under-understood.
[01:10:59.420 --> 01:11:00.260]   - Yes.
[01:11:00.260 --> 01:11:03.540]   - Somebody needs to, well, everybody needs
[01:11:03.540 --> 01:11:06.860]   to start talking about AI as a system of AIs
[01:11:06.860 --> 01:11:09.500]   and system of engineered systems,
[01:11:09.500 --> 01:11:12.740]   engineered systems that are well-engineered,
[01:11:12.740 --> 01:11:14.540]   built from first principles,
[01:11:14.540 --> 01:11:16.780]   well-tested, so on and so forth.
[01:11:16.780 --> 01:11:21.140]   Regulation, remember, AI is a capability
[01:11:21.140 --> 01:11:23.700]   that can be applied.
[01:11:23.700 --> 01:11:28.700]   And don't, it's necessary to have regulation
[01:11:28.700 --> 01:11:32.620]   for important technologies,
[01:11:32.620 --> 01:11:37.020]   but it's also, don't overreach to the point
[01:11:37.020 --> 01:11:40.380]   where some of the regulation ought to be done,
[01:11:40.380 --> 01:11:42.780]   most of the regulation ought to be done at the applications.
[01:11:42.780 --> 01:11:43.620]   - Right.
[01:11:43.620 --> 01:11:47.180]   - FDA, NHTSA, FDA, you name it, right?
[01:11:47.180 --> 01:11:50.540]   All of the different, all of the different ecosystems
[01:11:50.540 --> 01:11:53.380]   that already regulate applications of technology.
[01:11:53.380 --> 01:11:54.220]   - Right.
[01:11:54.220 --> 01:11:57.260]   - Now have to regulate the application of technology
[01:11:57.260 --> 01:11:58.660]   that is now infused with AI.
[01:11:58.660 --> 01:11:59.500]   - Right.
[01:11:59.500 --> 01:12:04.500]   - And so, and so I think, I think there's,
[01:12:04.500 --> 01:12:07.140]   don't, don't, don't misunderstand,
[01:12:07.140 --> 01:12:10.540]   don't overlook the overwhelming amount of regulation
[01:12:10.540 --> 01:12:13.260]   in the world that are going to have to be activated
[01:12:13.260 --> 01:12:17.500]   for AI, and don't rely on just one universal,
[01:12:17.500 --> 01:12:20.500]   galactic, you know, AI council
[01:12:20.500 --> 01:12:22.460]   that's gonna possibly be able to do this,
[01:12:22.460 --> 01:12:23.820]   because there's a reason why
[01:12:23.820 --> 01:12:25.780]   all of these different agencies were created.
[01:12:25.780 --> 01:12:27.220]   There was, there's a reason why
[01:12:27.220 --> 01:12:30.780]   all these different regulatory bodies were created.
[01:12:30.780 --> 01:12:32.180]   We'll go back to first principles again.
[01:12:32.180 --> 01:12:34.420]   - I'd get in trouble by my partner, Bill Gurley,
[01:12:34.420 --> 01:12:37.540]   if I didn't go back to the open source point.
[01:12:37.540 --> 01:12:39.980]   You guys launched a very important, very large,
[01:12:39.980 --> 01:12:41.860]   very capable open source model.
[01:12:41.860 --> 01:12:42.900]   - Yeah.
[01:12:42.900 --> 01:12:43.740]   - Recently.
[01:12:43.740 --> 01:12:44.580]   - Yeah.
[01:12:44.580 --> 01:12:45.420]   - Recently.
[01:12:45.420 --> 01:12:46.260]   - Yeah.
[01:12:46.260 --> 01:12:50.620]   - Obviously, meta is making significant contributions
[01:12:50.620 --> 01:12:51.940]   to open source.
[01:12:51.940 --> 01:12:53.740]   I find when I read Twitter, you know,
[01:12:53.740 --> 01:12:56.460]   you have this kind of open versus closed,
[01:12:56.460 --> 01:12:59.340]   a lot of, a lot of chatter about it.
[01:12:59.340 --> 01:13:00.180]   - Yeah.
[01:13:00.180 --> 01:13:02.660]   - How do you feel about open source,
[01:13:02.660 --> 01:13:05.620]   your own open source models' ability
[01:13:05.620 --> 01:13:07.900]   to keep up with frontier?
[01:13:07.900 --> 01:13:08.980]   That would be the first question.
[01:13:08.980 --> 01:13:10.580]   The second question would be,
[01:13:10.580 --> 01:13:13.300]   is that, you know, having that open source model
[01:13:13.300 --> 01:13:16.740]   and also having closed source models, you know,
[01:13:16.740 --> 01:13:18.700]   that are powering commercial operations,
[01:13:18.700 --> 01:13:20.740]   is that what you see into the future?
[01:13:20.740 --> 01:13:21.980]   And do those two things,
[01:13:21.980 --> 01:13:25.380]   does that create the healthy tension for safety?
[01:13:25.380 --> 01:13:26.220]   - Mm-hmm.
[01:13:26.220 --> 01:13:31.260]   Open source versus closed source is related to safety,
[01:13:31.260 --> 01:13:32.300]   but not only about safety.
[01:13:32.300 --> 01:13:33.140]   - Yes.
[01:13:33.140 --> 01:13:35.020]   - You know, and so, so for example,
[01:13:35.020 --> 01:13:37.460]   there's absolutely nothing wrong
[01:13:37.460 --> 01:13:39.740]   with having closed source models
[01:13:39.740 --> 01:13:42.900]   that are, that are the engines of an economic model.
[01:13:42.900 --> 01:13:43.740]   - Exactly.
[01:13:43.740 --> 01:13:45.300]   - Necessary to sustain innovation.
[01:13:45.300 --> 01:13:46.140]   - Right.
[01:13:46.140 --> 01:13:48.260]   - Okay, I celebrate that wholeheartedly.
[01:13:48.260 --> 01:13:49.860]   - Right.
[01:13:49.860 --> 01:13:52.340]   - It is, it is, it is, I believe,
[01:13:52.340 --> 01:13:57.780]   wrong-minded to be closed versus open.
[01:13:57.780 --> 01:13:58.620]   - Right.
[01:13:58.620 --> 01:13:59.620]   - It should be closed and open.
[01:13:59.620 --> 01:14:00.460]   - Plus open.
[01:14:00.460 --> 01:14:03.540]   - Yeah, right, because open is necessary
[01:14:03.540 --> 01:14:05.500]   for many industries to be activated.
[01:14:05.500 --> 01:14:06.700]   Right now, if we didn't have open source,
[01:14:06.700 --> 01:14:08.860]   how would all these different fields of science
[01:14:08.860 --> 01:14:11.340]   be able to activate, be activated on AI?
[01:14:11.340 --> 01:14:12.180]   - Right.
[01:14:12.180 --> 01:14:13.020]   - Right, because they have to develop
[01:14:13.020 --> 01:14:14.820]   their own domain-specific AIs,
[01:14:14.820 --> 01:14:16.420]   and they have to develop their own,
[01:14:16.420 --> 01:14:20.980]   using open source models, create domain-specific AIs.
[01:14:20.980 --> 01:14:22.580]   They're related, again, not the same.
[01:14:22.580 --> 01:14:23.420]   - Right.
[01:14:23.420 --> 01:14:24.580]   - Just because you have an open source model
[01:14:24.580 --> 01:14:25.420]   doesn't mean you have an AI,
[01:14:25.420 --> 01:14:27.660]   and so you have to have that open source model
[01:14:27.660 --> 01:14:30.340]   to enable the creation of AIs.
[01:14:30.340 --> 01:14:33.020]   So financial services, healthcare, transportation,
[01:14:33.020 --> 01:14:35.300]   the list of industries, fields of science
[01:14:35.300 --> 01:14:36.580]   that has now been enabled
[01:14:36.580 --> 01:14:38.860]   as a result of open source, unbelievable.
[01:14:38.860 --> 01:14:40.060]   - Are you seeing a lot of demand
[01:14:40.060 --> 01:14:41.860]   for your open source models?
[01:14:41.860 --> 01:14:44.260]   - Our open source models, so first of all,
[01:14:44.260 --> 01:14:47.660]   Lama Downloads, right, obviously.
[01:14:47.660 --> 01:14:49.580]   Yeah, Mark and the work that they've done,
[01:14:49.580 --> 01:14:51.420]   incredible, off the charts.
[01:14:51.420 --> 01:14:52.260]   - Yes.
[01:14:52.260 --> 01:14:53.940]   - And it completely activated
[01:14:53.940 --> 01:14:57.180]   and engaged every single industry,
[01:14:57.180 --> 01:14:58.620]   every single field of science.
[01:14:58.620 --> 01:14:59.660]   - Right, right, it's terrific.
[01:14:59.660 --> 01:15:01.220]   - The reason why we did Nemotron
[01:15:01.220 --> 01:15:04.180]   was for synthetic data generation.
[01:15:05.380 --> 01:15:08.900]   Intuitively, the idea that one AI
[01:15:08.900 --> 01:15:10.620]   would somehow sit there and loop
[01:15:10.620 --> 01:15:12.420]   and generate data to learn itself,
[01:15:12.420 --> 01:15:15.540]   it sounds brittle.
[01:15:15.540 --> 01:15:16.380]   - Yes.
[01:15:16.380 --> 01:15:18.660]   - And how many times you can go around
[01:15:18.660 --> 01:15:22.420]   that infinite loop, that loop, you know, questionable.
[01:15:22.420 --> 01:15:25.500]   However, it's kind of, my mental image
[01:15:25.500 --> 01:15:28.700]   is kind of like, you get a super smart person,
[01:15:28.700 --> 01:15:31.140]   put him into a padded room,
[01:15:31.140 --> 01:15:33.100]   close the door for about a month.
[01:15:33.100 --> 01:15:36.860]   You know, what comes out is probably not a smarter person.
[01:15:36.860 --> 01:15:40.460]   And so, but the idea that you could have
[01:15:40.460 --> 01:15:42.140]   two or three people sit around
[01:15:42.140 --> 01:15:44.300]   and we have different AIs,
[01:15:44.300 --> 01:15:46.460]   we have different distributions of knowledge
[01:15:46.460 --> 01:15:48.900]   and we can go QA back and forth,
[01:15:48.900 --> 01:15:50.740]   all three of us can come out smarter.
[01:15:50.740 --> 01:15:51.580]   - Right.
[01:15:51.580 --> 01:15:54.940]   - And so the idea that you can have AI models
[01:15:54.940 --> 01:15:58.540]   exchanging, interacting, going back and forth,
[01:15:58.540 --> 01:16:01.340]   debating, reinforcement learning,
[01:16:01.340 --> 01:16:04.060]   synthetic data generation, for example,
[01:16:04.060 --> 01:16:08.140]   kind of intuitively suggests it makes sense, yeah.
[01:16:08.140 --> 01:16:11.580]   And so our model, Nemotron 350B is,
[01:16:11.580 --> 01:16:16.580]   340B is the best model in the world for reward systems.
[01:16:16.580 --> 01:16:18.780]   And so it is the best critique.
[01:16:18.780 --> 01:16:19.860]   - Okay, interesting.
[01:16:19.860 --> 01:16:23.420]   - Yeah, and so a fantastic model
[01:16:23.420 --> 01:16:26.220]   for enhancing everybody else's models.
[01:16:26.220 --> 01:16:29.740]   Irrespective of how great somebody else's model is,
[01:16:29.740 --> 01:16:32.860]   I'd heavily recommend using Nemotron 340B
[01:16:32.860 --> 01:16:34.220]   to enhance and make it better.
[01:16:34.220 --> 01:16:35.980]   And we've already seen, made Lama better,
[01:16:35.980 --> 01:16:37.620]   made all the other models better.
[01:16:37.620 --> 01:16:41.700]   - Well, we're coming to the end.
[01:16:41.700 --> 01:16:43.940]   - Thank goodness.
[01:16:43.940 --> 01:16:48.700]   - As somebody who delivered DGX-1 in 2016,
[01:16:48.700 --> 01:16:51.500]   it's really been an incredible journey.
[01:16:51.500 --> 01:16:56.100]   Your journey is unlikely and incredible at the same time.
[01:16:56.100 --> 01:16:56.940]   - Thank you.
[01:16:56.940 --> 01:17:00.580]   - You survived, like just surviving the early days
[01:17:00.580 --> 01:17:02.020]   was pretty extraordinary.
[01:17:02.020 --> 01:17:06.020]   You delivered the first DGX-1 in 2016.
[01:17:06.020 --> 01:17:10.380]   We had this Cambrian moment in 2022.
[01:17:10.380 --> 01:17:11.820]   And so I'm gonna ask you the question
[01:17:11.820 --> 01:17:16.820]   I often get asked, which is,
[01:17:16.820 --> 01:17:21.380]   how long can you sustain what you're doing today?
[01:17:21.380 --> 01:17:26.140]   With 60 direct reports, you're everywhere.
[01:17:26.140 --> 01:17:28.500]   You're driving this revolution.
[01:17:28.500 --> 01:17:31.420]   Are you having fun?
[01:17:31.420 --> 01:17:36.260]   And is there something else that you would rather be doing?
[01:17:36.260 --> 01:17:42.900]   - Is this a question about the last hour and a half?
[01:17:42.900 --> 01:17:45.780]   The answer is I had a great time.
[01:17:45.780 --> 01:17:48.420]   I couldn't imagine anything else I'd rather be doing.
[01:17:48.420 --> 01:17:51.140]   Let's see.
[01:17:53.500 --> 01:17:57.260]   I don't think it's right to leave the impression
[01:17:57.260 --> 01:18:02.260]   that our job is fun all the time.
[01:18:02.260 --> 01:18:05.140]   My job isn't fun all the time,
[01:18:05.140 --> 01:18:07.500]   nor do I expect it to be fun all the time.
[01:18:07.500 --> 01:18:10.420]   Was that ever an expectation that it was fun all the time?
[01:18:10.420 --> 01:18:14.740]   I think it's important all the time.
[01:18:14.740 --> 01:18:16.380]   I don't take myself too seriously.
[01:18:16.380 --> 01:18:18.060]   I take the work very seriously.
[01:18:18.060 --> 01:18:19.940]   I take our responsibility very seriously.
[01:18:19.940 --> 01:18:22.220]   I take our contribution and our moment in time
[01:18:22.220 --> 01:18:23.060]   very seriously.
[01:18:23.060 --> 01:18:26.820]   Is that always fun?
[01:18:26.820 --> 01:18:27.660]   No.
[01:18:27.660 --> 01:18:30.180]   But do I always love it?
[01:18:30.180 --> 01:18:31.860]   Yes.
[01:18:31.860 --> 01:18:32.900]   Like all things.
[01:18:32.900 --> 01:18:37.460]   Whether it is family, friends, children,
[01:18:37.460 --> 01:18:38.660]   is it always fun?
[01:18:38.660 --> 01:18:39.500]   No.
[01:18:39.500 --> 01:18:40.660]   Do we always love it?
[01:18:40.660 --> 01:18:42.460]   Absolutely, deeply.
[01:18:42.460 --> 01:18:45.860]   And so I think the,
[01:18:49.700 --> 01:18:51.460]   how long can I do this?
[01:18:51.460 --> 01:18:56.820]   The real question is how long can I be relevant?
[01:18:56.820 --> 01:19:01.620]   And that only matters, that piece of information,
[01:19:01.620 --> 01:19:03.140]   that question can only be answered
[01:19:03.140 --> 01:19:06.860]   with how am I gonna continue to learn?
[01:19:06.860 --> 01:19:09.140]   And I am a lot more optimistic today.
[01:19:09.140 --> 01:19:12.460]   I'm not saying this simply because of our topic today.
[01:19:12.460 --> 01:19:15.020]   I'm a lot more optimistic about my ability
[01:19:15.020 --> 01:19:19.620]   to stay relevant and continue to learn because of AI.
[01:19:19.620 --> 01:19:22.060]   I use it, I don't know, but I'm sure you guys do.
[01:19:22.060 --> 01:19:24.660]   I use it literally every day.
[01:19:24.660 --> 01:19:26.100]   There's not one piece of research
[01:19:26.100 --> 01:19:28.060]   that I don't involve AI with.
[01:19:28.060 --> 01:19:31.540]   There's not one question that even if I know the answer,
[01:19:31.540 --> 01:19:33.620]   I double check on it with AI.
[01:19:33.620 --> 01:19:35.900]   And surprisingly, you know,
[01:19:35.900 --> 01:19:38.180]   the next two or three questions I ask it
[01:19:38.180 --> 01:19:40.820]   reveals something I didn't know.
[01:19:40.820 --> 01:19:42.260]   You pick your topic.
[01:19:42.260 --> 01:19:43.700]   You pick your topic.
[01:19:43.700 --> 01:19:47.180]   And I think that AI as a tutor,
[01:19:47.180 --> 01:19:48.540]   AI as an assistant,
[01:19:48.540 --> 01:19:54.780]   AI as a partner to brainstorm with,
[01:19:54.780 --> 01:19:57.980]   double check my work.
[01:19:57.980 --> 01:20:02.460]   You know, boy, you guys, it's completely revolutionary.
[01:20:02.460 --> 01:20:05.100]   And that's just, you know, I'm an information worker.
[01:20:05.100 --> 01:20:06.900]   My output is information.
[01:20:06.900 --> 01:20:10.220]   And so I think the contributions
[01:20:10.220 --> 01:20:13.340]   that I'll have on society is pretty extraordinary.
[01:20:13.340 --> 01:20:16.460]   So I think if that's the case,
[01:20:16.460 --> 01:20:18.260]   if I could stay relevant like this
[01:20:18.260 --> 01:20:21.860]   and I can continue to make a contribution,
[01:20:21.860 --> 01:20:25.740]   I know that the work is important enough
[01:20:25.740 --> 01:20:28.180]   for me to want to continue to pursue it.
[01:20:28.180 --> 01:20:30.580]   And my quality of life is incredible.
[01:20:30.580 --> 01:20:32.060]   So I mean, what's there to complain about?
[01:20:32.060 --> 01:20:33.540]   - I'll say, I can't imagine,
[01:20:33.540 --> 01:20:35.260]   you and I have been at this for a few decades.
[01:20:35.260 --> 01:20:37.060]   I can't imagine missing this moment.
[01:20:37.060 --> 01:20:37.900]   - Yeah, right.
[01:20:37.900 --> 01:20:40.140]   - It's the most consequential moment of our careers.
[01:20:40.140 --> 01:20:42.300]   We're deeply grateful for the partnership.
[01:20:42.300 --> 01:20:43.780]   - Don't miss the next 10 years.
[01:20:43.780 --> 01:20:45.620]   - For the thought partnership.
[01:20:45.620 --> 01:20:46.660]   - You make us smarter.
[01:20:46.660 --> 01:20:47.660]   - Thank you.
[01:20:47.660 --> 01:20:50.020]   - And I think you're really important
[01:20:50.020 --> 01:20:51.660]   as part of the leadership, right?
[01:20:51.660 --> 01:20:55.940]   That's going to optimistically and safely lead this forward.
[01:20:55.940 --> 01:20:56.780]   So thank you for being with us.
[01:20:56.780 --> 01:20:57.620]   - Thank you.
[01:20:57.620 --> 01:20:58.540]   Really enjoyed it.
[01:20:58.540 --> 01:20:59.380]   Thanks, Brad.
[01:20:59.380 --> 01:21:00.220]   Thanks, Clark.
[01:21:00.220 --> 01:21:01.060]   Good job.
[01:21:01.060 --> 01:21:03.580]   (upbeat music)
[01:21:03.580 --> 01:21:11.500]   - As a reminder to everybody,
[01:21:11.500 --> 01:21:13.740]   just our opinions, not investment advice.

