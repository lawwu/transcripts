
[00:00:00.000 --> 00:00:07.280]   So we're not talking about humans, but if we think about building artificial intelligence
[00:00:07.280 --> 00:00:14.420]   systems, robots, do you think all the features and bugs that you have highlighted in human
[00:00:14.420 --> 00:00:18.540]   beings are useful for constructing AI systems?
[00:00:18.540 --> 00:00:23.480]   So both systems are useful for perhaps instilling in robots?
[00:00:23.480 --> 00:00:33.960]   What is happening these days is that actually what is happening in deep learning is more
[00:00:33.960 --> 00:00:37.800]   like a system one product than like a system two product.
[00:00:37.800 --> 00:00:43.680]   I mean, deep learning matches patterns and anticipate what's going to happen, so it's
[00:00:43.680 --> 00:00:46.160]   highly predictive.
[00:00:46.160 --> 00:00:54.800]   But what deep learning doesn't have, and many people think that this is the critical, it
[00:00:54.800 --> 00:00:59.840]   doesn't have the ability to reason, so there is no system two there.
[00:00:59.840 --> 00:01:06.560]   But I think very importantly, it doesn't have any causality or any way to represent meaning
[00:01:06.560 --> 00:01:08.960]   and to represent real interaction.
[00:01:08.960 --> 00:01:19.840]   So until that is solved, what can be accomplished is marvelous and very exciting, but limited.
[00:01:19.840 --> 00:01:24.480]   That's actually really nice to think of current advances in machine learning as essentially
[00:01:24.480 --> 00:01:26.560]   system one advances.
[00:01:26.560 --> 00:01:29.940]   So how far can we get with just system one?
[00:01:29.940 --> 00:01:34.360]   If we think of deep learning and artificial intelligence systems?
[00:01:34.360 --> 00:01:41.640]   It's very clear that deep mind has already gone way beyond what people thought was possible.
[00:01:41.640 --> 00:01:48.080]   I think the thing that has impressed me most about the developments in AI is the speed.
[00:01:48.080 --> 00:01:53.520]   It's that things, at least in the context of deep learning, and maybe this is about
[00:01:53.520 --> 00:01:59.200]   to slow down, but things moved a lot faster than anticipated.
[00:01:59.200 --> 00:02:09.760]   The transition from solving chess to solving Go, that's bewildering how quickly it went.
[00:02:09.760 --> 00:02:15.840]   The move from AlphaGo to AlphaZero is sort of bewildering the speed at which they accomplished
[00:02:15.840 --> 00:02:16.840]   that.
[00:02:16.840 --> 00:02:24.680]   Now, clearly, there are many problems that you can solve that way, but there are some
[00:02:24.680 --> 00:02:28.280]   problems for which you need something else.
[00:02:28.280 --> 00:02:29.280]   Something like reasoning.
[00:02:29.280 --> 00:02:37.480]   Well, reasoning and also, you know, one of the real mysteries, psychologist Gary Marcus,
[00:02:37.480 --> 00:02:47.360]   who is also a critic of AI, I mean, what he points out, and I think he has a point, is
[00:02:47.360 --> 00:02:53.160]   that humans learn quickly.
[00:02:53.160 --> 00:02:59.200]   Humans don't need a million examples, they need two or three examples.
[00:02:59.200 --> 00:03:02.440]   So clearly, there is a fundamental difference.
[00:03:02.440 --> 00:03:11.160]   And what enables a machine to learn quickly, what you have to build into the machine, because
[00:03:11.160 --> 00:03:16.860]   it's clear that you have to build some expectations or something in the machine to make it ready
[00:03:16.860 --> 00:03:23.320]   to learn quickly, that at the moment seems to be unsolved.
[00:03:23.320 --> 00:03:31.400]   I'm pretty sure that DeepMind is working on it, but if they have solved it, I haven't
[00:03:31.400 --> 00:03:32.400]   heard yet.
[00:03:32.400 --> 00:03:38.500]   They're trying to actually, them and OpenAI are trying to start to get to use neural networks
[00:03:38.500 --> 00:03:40.160]   to reason.
[00:03:40.160 --> 00:03:47.660]   So assembled knowledge, of course, causality is, temporal causality is out of reach to
[00:03:47.660 --> 00:03:49.640]   most everybody.
[00:03:49.640 --> 00:03:54.040]   You mentioned the benefits of System 1 is essentially that it's fast, allows us to
[00:03:54.040 --> 00:03:55.040]   function in the world.
[00:03:55.040 --> 00:03:56.040]   Fast and skilled, yeah.
[00:03:56.040 --> 00:03:57.040]   It's skilled.
[00:03:57.040 --> 00:04:00.460]   And it has a model of the world.
[00:04:00.460 --> 00:04:10.680]   You know, in a sense, I mean, there was the earlier phase of AI attempted to model reasoning,
[00:04:10.680 --> 00:04:14.440]   and they were moderately successful, but, you know, reasoning by itself doesn't get
[00:04:14.440 --> 00:04:18.360]   you much.
[00:04:18.360 --> 00:04:22.920]   Deep learning has been much more successful in terms of, you know, what they can do.
[00:04:22.920 --> 00:04:28.120]   But now, it's an interesting question, whether it's approaching its limits.
[00:04:28.120 --> 00:04:29.620]   What do you think?
[00:04:29.620 --> 00:04:30.620]   I think absolutely.
[00:04:30.620 --> 00:04:38.800]   So I just talked to Gian Lacoon, you mentioned, you know, so he thinks that the limits, we're
[00:04:38.800 --> 00:04:43.840]   not going to hit the limits with neural networks, that ultimately this kind of System 1 pattern
[00:04:43.840 --> 00:04:52.320]   matching will start to start to look like System 2 without significant transformation
[00:04:52.320 --> 00:04:53.960]   of the architecture.
[00:04:53.960 --> 00:04:59.060]   So I'm more with the majority of the people who think that yes, neural networks will hit
[00:04:59.060 --> 00:05:01.140]   a limit in their capability.
[00:05:01.140 --> 00:05:07.860]   He, on the one hand, I have heard him tell the Mises-Sabies essentially that, you know,
[00:05:07.860 --> 00:05:13.220]   what they have accomplished is not a big deal, that they have just touched, that basically,
[00:05:13.220 --> 00:05:18.900]   you know, they can't do unsupervised learning in an effective way.
[00:05:18.900 --> 00:05:24.340]   But you're telling me that he thinks that the current, within the current architecture,
[00:05:24.340 --> 00:05:26.600]   you can do causality and reasoning?
[00:05:26.600 --> 00:05:31.520]   So he's very much a pragmatist in a sense that's saying that we're very far away, that
[00:05:31.520 --> 00:05:39.120]   there's still, I think there's this idea that he says is we can only see one or two mountain
[00:05:39.120 --> 00:05:45.160]   peaks ahead and there might be either a few more after or thousands more after.
[00:05:45.160 --> 00:05:46.160]   So that kind of idea.
[00:05:46.160 --> 00:05:48.160]   I heard that metaphor.
[00:05:48.160 --> 00:05:56.880]   Right. But nevertheless, it doesn't see a, the final answer not fundamentally looking
[00:05:56.880 --> 00:05:59.340]   like one that we currently have.
[00:05:59.340 --> 00:06:02.600]   So neural networks being a huge part of that.
[00:06:02.600 --> 00:06:03.600]   Yeah.
[00:06:03.600 --> 00:06:09.200]   I mean, that's very likely because, because pattern matching is so much of what's going
[00:06:09.200 --> 00:06:10.200]   on.
[00:06:10.200 --> 00:06:11.200]   But.
[00:06:11.200 --> 00:06:14.200]   And you can think of neural networks as processing information sequentially.
[00:06:14.200 --> 00:06:22.360]   Yeah, I mean, you know, there is, there is an important aspect to, for example, you get
[00:06:22.360 --> 00:06:27.720]   systems that translate and they do a very good job, but they really don't know what
[00:06:27.720 --> 00:06:31.400]   they're talking about.
[00:06:31.400 --> 00:06:33.840]   And for that, I'm really quite surprised.
[00:06:33.840 --> 00:06:41.200]   For that, you would need, you would need an AI that has sensation, an AI that is in touch
[00:06:41.200 --> 00:06:42.200]   with the world.
[00:06:42.200 --> 00:06:48.880]   Yes, self-awareness and maybe even something that resembles consciousness kind of ideas.
[00:06:48.880 --> 00:06:54.160]   Certainly awareness of, you know, awareness of what's going on so that the words have
[00:06:54.160 --> 00:06:59.920]   meaning or can get, are in touch with some perception or some action.
[00:06:59.920 --> 00:07:00.920]   Yeah.
[00:07:00.920 --> 00:07:06.920]   So that's a big thing for Jan and as what he refers to as grounding to the physical
[00:07:06.920 --> 00:07:07.920]   space.
[00:07:07.920 --> 00:07:10.400]   So, so that's what we're talking about the same.
[00:07:10.400 --> 00:07:11.400]   Yeah.
[00:07:11.400 --> 00:07:13.440]   So how, how you ground.
[00:07:13.440 --> 00:07:18.960]   I mean the grounding, without grounding, then you get, you get a machine that doesn't know
[00:07:18.960 --> 00:07:24.360]   what it's talking about because it is talking about the world ultimately.
[00:07:24.360 --> 00:07:26.800]   The question, the open question is what it means to ground.
[00:07:26.800 --> 00:07:33.720]   I mean, we're very human centric in our thinking, but what does it mean for a machine to understand
[00:07:33.720 --> 00:07:36.640]   what it means to be in this world?
[00:07:36.640 --> 00:07:38.800]   Does it need to have a body?
[00:07:38.800 --> 00:07:42.360]   Does it need to have a finiteness like we humans have?
[00:07:42.360 --> 00:07:46.360]   All of these elements, it's a very, it's an open question.
[00:07:46.360 --> 00:07:50.640]   You know, I'm not sure about having a body, but having a perceptual system, having a body
[00:07:50.640 --> 00:07:52.280]   would be very helpful too.
[00:07:52.280 --> 00:07:59.920]   I mean, if, if you think about human mimicking human, but having a perception that seems
[00:07:59.920 --> 00:08:06.800]   to be essential so that you can build, you can accumulate knowledge about the world.
[00:08:06.800 --> 00:08:13.400]   However, you can, you can imagine a human completely paralyzed and there is a lot that
[00:08:13.400 --> 00:08:17.640]   the human brain could learn, you know, with a paralyzed body.
[00:08:17.640 --> 00:08:23.520]   So if we got a machine that could do that, that would be a big deal.
[00:08:23.520 --> 00:08:29.000]   And then the flip side of that, something you see in children and something in machine
[00:08:29.000 --> 00:08:31.440]   learning world is called active learning.
[00:08:31.440 --> 00:08:36.780]   Maybe it is also is being able to play with the world.
[00:08:36.780 --> 00:08:43.040]   How important for developing system one or system two, do you think it is to play with
[00:08:43.040 --> 00:08:44.040]   the world?
[00:08:44.040 --> 00:08:45.040]   To be able to interact with it?
[00:08:45.040 --> 00:08:51.880]   Well, certainly a lot, a lot of what you learn as you learn to anticipate the outcomes of
[00:08:51.880 --> 00:08:52.880]   your actions.
[00:08:52.880 --> 00:08:57.400]   I mean, you can see that how babies learn it, you know, with their hands, they, how
[00:08:57.400 --> 00:09:03.600]   they learn, you know, to connect, you know, the movements of their hands with something
[00:09:03.600 --> 00:09:08.880]   that clearly is something that happens in the brain and, and, and the ability of the
[00:09:08.880 --> 00:09:11.660]   brain to learn new patterns.
[00:09:11.660 --> 00:09:17.200]   So you know, it's the kind of thing that you get with artificial limbs that you connected
[00:09:17.200 --> 00:09:24.400]   and then people learn to operate the artificial limb, you know, really impressively quickly,
[00:09:24.400 --> 00:09:28.120]   at least from, from what I hear.
[00:09:28.120 --> 00:09:33.200]   So we have a system that is ready to learn the world through action.
[00:09:33.200 --> 00:09:39.760]   At the risk of going into way too mysterious of land, what do you think it takes to build
[00:09:39.760 --> 00:09:42.920]   a system like that?
[00:09:42.920 --> 00:09:49.000]   Obviously we're very far from understanding how the brain works, but how difficult is
[00:09:49.000 --> 00:09:52.880]   it to build this mind of ours?
[00:09:52.880 --> 00:09:57.600]   You know, I mean, I think that Jan LeCun's answer that we don't know how many mountains
[00:09:57.600 --> 00:09:58.600]   there are.
[00:09:58.600 --> 00:10:00.560]   I think that's a very good answer.
[00:10:00.560 --> 00:10:06.640]   I think that, you know, if you, if you look at what Ray Kurzweil is saying, that strikes
[00:10:06.640 --> 00:10:13.480]   me as off the wall, but, but I think people are much more realistic than that.
[00:10:13.480 --> 00:10:19.480]   We're actually, Demis Hassabis is and Jan is, and so the people are actually doing the
[00:10:19.480 --> 00:10:23.840]   work fairly realistic, I think.
[00:10:23.840 --> 00:10:28.960]   To maybe phrase it another way, from a perspective, not of building it, but from understanding
[00:10:28.960 --> 00:10:36.080]   it, how complicated are human beings in the following sense?
[00:10:36.080 --> 00:10:41.400]   You know, I work with autonomous vehicles and pedestrians, so we tried to model pedestrians.
[00:10:41.400 --> 00:10:48.800]   How difficult is it to model a human being, their perception of the world, the two systems
[00:10:48.800 --> 00:10:53.200]   they operate under, sufficiently to be able to predict whether the pedestrian is going
[00:10:53.200 --> 00:10:55.040]   to cross the road or not?
[00:10:55.040 --> 00:11:00.800]   I'm, you know, I'm fairly optimistic about that, actually, because what we're talking
[00:11:00.800 --> 00:11:10.280]   about is a huge amount of information that every vehicle has, and that feeds into one
[00:11:10.280 --> 00:11:13.220]   system, into one gigantic system.
[00:11:13.220 --> 00:11:19.200]   And so anything that any vehicle learns becomes part of what the whole system knows.
[00:11:19.200 --> 00:11:25.240]   And with a system multiplier like that, there is a lot that you can do.
[00:11:25.240 --> 00:11:32.160]   So human beings are very complicated, but, and, you know, system is going to make mistakes,
[00:11:32.160 --> 00:11:34.160]   but human makes mistakes.
[00:11:34.160 --> 00:11:41.520]   I think that they'll be able to, I think they are able to anticipate pedestrians, otherwise
[00:11:41.520 --> 00:11:42.680]   a lot would happen.
[00:11:42.680 --> 00:11:51.360]   They're able to, you know, they're able to get into a roundabout and into traffic, so
[00:11:51.360 --> 00:11:58.840]   they must know both to expect or to anticipate how people will react when they're sneaking
[00:11:58.840 --> 00:11:59.840]   in.
[00:11:59.840 --> 00:12:03.840]   And there's a lot of learning that's involved in that.
[00:12:03.840 --> 00:12:12.040]   Currently, the pedestrians are treated as things that cannot be hit, and they're not
[00:12:12.040 --> 00:12:18.800]   treated as agents with whom you interact in a game-theoretic way.
[00:12:18.800 --> 00:12:24.320]   So I mean, it's not, it's a totally open problem, and every time somebody tries to solve it,
[00:12:24.320 --> 00:12:27.120]   it seems to be harder than we think.
[00:12:27.120 --> 00:12:32.360]   And nobody's really tried to seriously solve the problem of that dance, because I'm not
[00:12:32.360 --> 00:12:36.920]   sure if you've thought about the problem of pedestrians, but you're really putting your
[00:12:36.920 --> 00:12:39.040]   life in the hands of the driver.
[00:12:39.040 --> 00:12:45.000]   You know, there is a dance, there's part of the dance that would be quite complicated,
[00:12:45.000 --> 00:12:49.840]   but for example, when I cross the street and there is a vehicle approaching, I look the
[00:12:49.840 --> 00:12:54.000]   driver in the eye, and I think many people do that.
[00:12:54.000 --> 00:13:00.320]   And you know, that's a signal that I'm sending, and I would be sending that machine to an
[00:13:00.320 --> 00:13:06.080]   autonomous vehicle and it had better understand it, because it means I'm crossing.
[00:13:06.080 --> 00:13:12.040]   So and there's another thing you do that actually, so I'll tell you what you do, because I've
[00:13:12.040 --> 00:13:17.000]   watched hundreds of hours of video on this, is when you step in the street, you do that
[00:13:17.000 --> 00:13:21.040]   before you step in the street, and when you step in the street, you actually look away.
[00:13:21.040 --> 00:13:22.040]   Look away.
[00:13:22.040 --> 00:13:23.040]   Yeah.
[00:13:23.040 --> 00:13:25.880]   Now, what is that?
[00:13:25.880 --> 00:13:31.400]   What that's saying is, I mean, you're trusting that the car, who hasn't slown down yet, will
[00:13:31.400 --> 00:13:32.400]   slow down.
[00:13:32.400 --> 00:13:33.400]   Yeah.
[00:13:33.400 --> 00:13:35.760]   And you're telling him, I'm committed.
[00:13:35.760 --> 00:13:37.760]   I mean, this is like in a game of chicken.
[00:13:37.760 --> 00:13:41.480]   So I'm committed, and if I'm committed, I'm looking away.
[00:13:41.480 --> 00:13:44.960]   So there is, you just have to stop.
[00:13:44.960 --> 00:13:49.760]   So the question is whether a machine that observes that needs to understand mortality.
[00:13:49.760 --> 00:13:59.680]   Here, I'm not sure that it's got to understand so much as it's got to anticipate.
[00:13:59.680 --> 00:14:08.280]   So and here, but you know, you're surprising me, because here I would think that maybe
[00:14:08.280 --> 00:14:14.120]   you can anticipate without understanding, because I think this is clearly what's happening
[00:14:14.120 --> 00:14:20.440]   in playing Go or in playing chess, there's a lot of anticipation and there is zero understanding.
[00:14:20.440 --> 00:14:30.480]   So I thought that you didn't need a model of the human and a model of the human mind
[00:14:30.480 --> 00:14:35.280]   to avoid hitting pedestrians, but you are suggesting that actually.
[00:14:35.280 --> 00:14:36.280]   There you go, yeah.
[00:14:36.280 --> 00:14:37.280]   You do.
[00:14:37.280 --> 00:14:40.840]   And then it's a lot harder, I thought.
[00:14:40.960 --> 00:14:41.460]   Yeah.
[00:14:41.460 --> 00:14:48.240]   [BLANK_AUDIO]
[00:14:48.240 --> 00:14:58.240]   [BLANK_AUDIO]
[00:14:58.240 --> 00:15:02.440]   [ Prevention is important ]

