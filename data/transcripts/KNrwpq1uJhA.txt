
[00:00:00.000 --> 00:00:07.800]   At least what I saw was that people's workflows kind of shifted when they realized that they
[00:00:07.800 --> 00:00:10.280]   could use this a little bit more like a scalpel, right?
[00:00:10.280 --> 00:00:11.820]   Instead of a sledgehammer.
[00:00:11.820 --> 00:00:18.380]   So if you've got the ability to go in and do sort of targeted delivery to only the things
[00:00:18.380 --> 00:00:22.700]   that you care about, then that changes your workflow.
[00:00:22.700 --> 00:00:27.060]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:00:27.060 --> 00:00:29.220]   And I'm your host, Lukas Biewald.
[00:00:29.220 --> 00:00:34.820]   This is a conversation with Chris Padwick, who is director of computer vision and machine
[00:00:34.820 --> 00:00:41.580]   learning at Blue River, which is a company acquired by John Deere that helps farmers
[00:00:41.580 --> 00:00:50.340]   strategically spray pesticides and herbicides to help the environment and help their customers.
[00:00:50.340 --> 00:00:56.020]   It's a super cool application of machine learning because it's so concrete and so unambiguously
[00:00:56.020 --> 00:00:59.040]   great for the world, but also super difficult.
[00:00:59.040 --> 00:01:02.780]   They run into kind of every edge case you can possibly imagine.
[00:01:02.780 --> 00:01:08.500]   And it's really fun to talk to someone that's spent so much time working on a single hard
[00:01:08.500 --> 00:01:09.500]   problem.
[00:01:09.500 --> 00:01:10.500]   Chris, thanks for doing this.
[00:01:10.500 --> 00:01:11.500]   Yeah, absolutely.
[00:01:11.500 --> 00:01:12.500]   Thanks for having me.
[00:01:12.500 --> 00:01:13.500]   Lukas, this is going to be a lot of fun.
[00:01:13.500 --> 00:01:14.500]   Awesome.
[00:01:14.500 --> 00:01:18.060]   You work on some of my favorite ML applications.
[00:01:18.060 --> 00:01:24.060]   I'd love to maybe start with an explanation of what you're working on at John Deere.
[00:01:24.060 --> 00:01:25.780]   Yeah, absolutely.
[00:01:25.780 --> 00:01:31.260]   So one of our products that we're working on with John Deere is called Seed and Spray.
[00:01:31.260 --> 00:01:33.080]   And the idea is really interesting.
[00:01:33.080 --> 00:01:39.600]   So if you look at how a farmer does their workflow right now, there's a bunch of parts
[00:01:39.600 --> 00:01:40.600]   to farming, right?
[00:01:40.600 --> 00:01:46.940]   If you think of it as a software stack, you can describe it as there's a tillage portion
[00:01:46.940 --> 00:01:51.660]   of that where you're preparing the soil, then you're planting the soil, then you're weeding
[00:01:51.660 --> 00:01:54.860]   the soil or you're weeding the plants, and you're harvesting.
[00:01:54.860 --> 00:01:57.100]   There's kind of those four sections.
[00:01:57.100 --> 00:02:03.140]   Seed and Spray targets the weeding section, where what we've done is taken an existing
[00:02:03.140 --> 00:02:07.840]   sprayer that folks have, and these are huge machines, actually.
[00:02:07.840 --> 00:02:14.540]   They have 120-foot spray boom, and they're capable of upwards of 800 acres a day in weeding.
[00:02:14.540 --> 00:02:16.420]   So these things are...
[00:02:16.420 --> 00:02:23.580]   I'm six foot two, and this machine, when I stand beside the wheel, the wheel is taller
[00:02:23.580 --> 00:02:24.580]   than my head, right?
[00:02:24.580 --> 00:02:28.020]   So this is a giant, giant, giant machine.
[00:02:28.020 --> 00:02:33.660]   And it can do tremendous amounts of productivity, is what people buy this for.
[00:02:33.660 --> 00:02:38.860]   And so the application that we're targeting is instead of spraying your entire field to
[00:02:38.860 --> 00:02:42.660]   kill the weeds, you really only need to spray the weeds.
[00:02:42.660 --> 00:02:43.660]   And that's what we built.
[00:02:43.660 --> 00:02:50.420]   So we built a computer vision system with AI and specifically deep learning that does
[00:02:50.420 --> 00:02:54.700]   discrimination between crop and weed, and then a robotic system around that that only
[00:02:54.700 --> 00:02:56.540]   targets the weeds.
[00:02:56.540 --> 00:03:00.140]   And so is this something that gets pulled behind a tractor?
[00:03:00.140 --> 00:03:03.100]   What does this machine actually look like?
[00:03:03.100 --> 00:03:09.420]   Yeah, this is an existing product called a self-propelled sprayer, and it's really a
[00:03:09.420 --> 00:03:10.740]   purpose-built device.
[00:03:10.740 --> 00:03:13.060]   And so I'll do a little impression of it for you.
[00:03:13.060 --> 00:03:14.860]   I should have brought a little toy.
[00:03:14.860 --> 00:03:19.900]   But what happens is it has these spray booms, and I'm actually not flexible enough to do
[00:03:19.900 --> 00:03:23.300]   it, but imagine my elbows are pointing exactly forward.
[00:03:23.300 --> 00:03:30.540]   And then when we unfurl the boom, these booms go out, and they're 60 feet on each side.
[00:03:30.540 --> 00:03:36.820]   And then we have 98 nozzles actually spaced throughout the boom.
[00:03:36.820 --> 00:03:42.380]   And as the farmer goes through the field, what happens is the cameras, we have cameras
[00:03:42.380 --> 00:03:46.300]   basically spaced roughly every meter.
[00:03:46.300 --> 00:03:49.340]   And cameras take pictures as you go through the field.
[00:03:49.340 --> 00:03:54.460]   Those pictures go into a machine learning algorithm that we've trained to distinguish
[00:03:54.460 --> 00:04:01.500]   crop from weed, and that's a convolutional neural network using deep learning.
[00:04:01.500 --> 00:04:07.580]   And then we map those to individual sprayer locations and then try to only spray the weed
[00:04:07.580 --> 00:04:09.420]   as we're going over it.
[00:04:09.420 --> 00:04:10.420]   Wow.
[00:04:10.420 --> 00:04:17.060]   So you have like-- did you say you have like 120 cameras then that unfurl or something
[00:04:17.060 --> 00:04:18.060]   like that?
[00:04:18.060 --> 00:04:20.700]   It's a 120 foot boom.
[00:04:20.700 --> 00:04:24.260]   And on one of our configurations, we've got 36 cameras.
[00:04:24.260 --> 00:04:25.260]   Oh, one every meter.
[00:04:25.260 --> 00:04:26.260]   I see.
[00:04:26.260 --> 00:04:27.260]   Yeah, that's right.
[00:04:27.260 --> 00:04:28.260]   Gotcha.
[00:04:28.260 --> 00:04:29.260]   Yeah.
[00:04:29.260 --> 00:04:32.620]   But then the sprayers are actually at a smaller interval than that.
[00:04:32.620 --> 00:04:39.100]   Is each one over each one row of crops, or are there multiple-- how does that work?
[00:04:39.100 --> 00:04:40.300]   Yeah, that's right.
[00:04:40.300 --> 00:04:44.240]   So there's different kind of configurations.
[00:04:44.240 --> 00:04:47.740]   It's kind of dictated by what the farmer is doing with the machine.
[00:04:47.740 --> 00:04:51.860]   So we do row crops right now.
[00:04:51.860 --> 00:04:54.580]   So three target crops.
[00:04:54.580 --> 00:04:58.140]   There's soybean, and cotton, and corn.
[00:04:58.140 --> 00:05:04.820]   And depending where you are in the US, you might grow-- if you're in Texas, you might
[00:05:04.820 --> 00:05:10.780]   grow cotton at sort of 40 inch rows, in which case you'd sort of take a 20 inch machine
[00:05:10.780 --> 00:05:13.780]   where you'd have a sprayer every 20 inches.
[00:05:13.780 --> 00:05:19.220]   So you're spraying-- if you kind of imagine the two rows, and they're 40 inches apart,
[00:05:19.220 --> 00:05:23.100]   you've got a sprayer there, and then a sprayer on the row.
[00:05:23.100 --> 00:05:28.300]   The other common one is 30 inch, which is found a little bit more in the Midwest rather
[00:05:28.300 --> 00:05:29.300]   than the South.
[00:05:29.300 --> 00:05:32.840]   And so the cameras are kind of pointed down at the crops.
[00:05:32.840 --> 00:05:36.940]   And I guess the cameras are in front of the sprayers, so they know when the weeds are
[00:05:36.940 --> 00:05:37.940]   coming?
[00:05:37.940 --> 00:05:38.940]   Yeah.
[00:05:38.940 --> 00:05:39.940]   Yeah, that's right.
[00:05:39.940 --> 00:05:47.420]   So our geometry is that we actually have a camera that's tilted forward so that we can
[00:05:47.420 --> 00:05:50.620]   see the crops and then react to them.
[00:05:50.620 --> 00:05:57.380]   We did try to-- kind of like the most ideal geometry is in a side view, is if you have
[00:05:57.380 --> 00:06:01.980]   something sticking out and then the camera pointing straight down.
[00:06:01.980 --> 00:06:07.300]   But unfortunately, anything that sticks out of the boom is something that will probably
[00:06:07.300 --> 00:06:10.100]   get torn off when it hits something.
[00:06:10.100 --> 00:06:15.100]   And so I don't know if you've ever driven something that has a 60-foot thing sticking
[00:06:15.100 --> 00:06:17.500]   out of it on each side.
[00:06:17.500 --> 00:06:19.980]   It's really, really hard not to hit stuff.
[00:06:19.980 --> 00:06:25.180]   And actually, so that was one of our design constraints, is that we really couldn't--
[00:06:25.180 --> 00:06:28.100]   we could put things that come up from the boom, because those are safer.
[00:06:28.100 --> 00:06:35.460]   But we couldn't put things that go out from the boom, because that would be a mechanical
[00:06:35.460 --> 00:06:41.100]   risk, that we would just end up breaking parts and breaking the sprayer.
[00:06:41.100 --> 00:06:47.540]   And so what's the benefit here to the farmers that they use less, I guess-- what do you
[00:06:47.540 --> 00:06:49.700]   even call the thing that kills the weeds?
[00:06:49.700 --> 00:06:50.700]   Like herbicide?
[00:06:50.700 --> 00:06:51.700]   Yeah, herbicide.
[00:06:51.700 --> 00:06:52.700]   Yeah.
[00:06:52.700 --> 00:06:54.380]   Yeah, that's one of the big benefits.
[00:06:54.380 --> 00:07:01.300]   So I'll kind of put this problem into perspective for you and try to give you a little bit of
[00:07:01.300 --> 00:07:06.660]   an understanding as to why farmers are so excited by this product.
[00:07:06.660 --> 00:07:13.260]   So let's sort of take an example and say that you're a 5,000-acre cotton farmer.
[00:07:13.260 --> 00:07:18.220]   So you've got 5,000 acres total, maybe spread over multiple fields.
[00:07:18.220 --> 00:07:22.740]   And the amount of money that you're going to spend just on herbicides-- so we actually
[00:07:22.740 --> 00:07:25.060]   call this kind of like the inputs.
[00:07:25.060 --> 00:07:29.380]   And so your inputs are fuel for your sprayer.
[00:07:29.380 --> 00:07:31.020]   Maybe you have to buy a new sprayer.
[00:07:31.020 --> 00:07:32.580]   That could be an input.
[00:07:32.580 --> 00:07:35.180]   Depreciation costs, that could be an input.
[00:07:35.180 --> 00:07:41.500]   But seed costs and also herbicide are one of your really main cost drivers.
[00:07:41.500 --> 00:07:47.220]   And so just in terms of the herbicide, you could be spending up to-- it wouldn't be uncommon
[00:07:47.220 --> 00:07:53.140]   to spend $150,000 spraying herbicide on your field.
[00:07:53.140 --> 00:07:58.180]   So if I come to you with a tool like this, the Seed and Spray System, which is more of
[00:07:58.180 --> 00:08:03.460]   a targeted system, and say, hey, depending on your weed pressure and depending on your
[00:08:03.460 --> 00:08:09.300]   farming practices, depending on a few input variables, we actually might be able to save
[00:08:09.300 --> 00:08:11.780]   you a lot of money there.
[00:08:11.780 --> 00:08:18.100]   And maybe you only have to spray 50% or maybe 70% or maybe 30%.
[00:08:18.100 --> 00:08:19.580]   It depends on a lot of factors.
[00:08:19.580 --> 00:08:25.980]   But if I could put a dent in that herbicide cost for you, then that's a really interesting
[00:08:25.980 --> 00:08:31.420]   proposition, because that's money that you can reinvest in your farm.
[00:08:31.420 --> 00:08:36.100]   And you can do something else with that money rather than spray herbicide.
[00:08:36.100 --> 00:08:41.460]   So that's one of the biggest cost drivers for the farmer.
[00:08:41.460 --> 00:08:52.840]   The second driver is wanting to be more sustainable, participate in more sustainable farming practices.
[00:08:52.840 --> 00:08:59.980]   And that's on a lot of farmers' minds of trying to spray less herbicide and be better stewards
[00:08:59.980 --> 00:09:00.980]   of the land.
[00:09:00.980 --> 00:09:05.140]   And this product goes directly into that use case.
[00:09:05.140 --> 00:09:09.420]   So folks are very excited by those two things.
[00:09:09.420 --> 00:09:14.180]   And that's the main value propositions for the product.
[00:09:14.180 --> 00:09:15.980]   That's super cool.
[00:09:15.980 --> 00:09:21.340]   And does it have to then do the inference on computers that live on this device?
[00:09:21.340 --> 00:09:27.140]   Do you actually put a whole bunch of computers to process all these images?
[00:09:27.140 --> 00:09:29.140]   How does that work?
[00:09:29.140 --> 00:09:31.060]   Yeah, that's exactly right.
[00:09:31.060 --> 00:09:36.980]   We've built some custom electronics with John Deere that will survive in the agricultural
[00:09:36.980 --> 00:09:37.980]   environment.
[00:09:37.980 --> 00:09:44.860]   So it's sort of funny when we think about, OK, where would you put a cluster of computers?
[00:09:44.860 --> 00:09:53.340]   And probably your last choice would be in Texas in 115 degree Fahrenheit heat with lots
[00:09:53.340 --> 00:09:54.340]   of dust.
[00:09:54.340 --> 00:09:59.300]   That's kind of not the thing that comes to mind when you're building clusters of computers.
[00:09:59.300 --> 00:10:01.340]   But that's actually exactly what we've done.
[00:10:01.340 --> 00:10:07.460]   So in the current design, we have these compute units on a machine.
[00:10:07.460 --> 00:10:13.420]   And they function in literally the worst environment you can imagine.
[00:10:13.420 --> 00:10:16.500]   And it takes a lot of engineering to make that work.
[00:10:16.500 --> 00:10:22.420]   But yeah, when we first kind of described this workflow to people, they sort of think,
[00:10:22.420 --> 00:10:26.660]   oh, so you're pushing stuff up to the cloud and doing the inference and sending result
[00:10:26.660 --> 00:10:27.660]   back.
[00:10:27.660 --> 00:10:32.420]   And actually, we're doing it all on board, just like an auto driving car, because we
[00:10:32.420 --> 00:10:34.160]   just don't have the time.
[00:10:34.160 --> 00:10:36.540]   We don't have the latency to be able to do that.
[00:10:36.540 --> 00:10:37.540]   We need to make decisions.
[00:10:37.540 --> 00:10:42.600]   As soon as we see the weed, we have to react within milliseconds.
[00:10:42.600 --> 00:10:43.780]   So it's all done.
[00:10:43.780 --> 00:10:47.500]   Yeah, it's all done on the platform.
[00:10:47.500 --> 00:10:50.620]   And is this a hard vision problem?
[00:10:50.620 --> 00:10:53.100]   Like just to get the accuracy to what you need?
[00:10:53.100 --> 00:10:55.140]   Is that a big challenge for you?
[00:10:55.140 --> 00:10:58.500]   Yeah, it really has been a hard vision problem.
[00:10:58.500 --> 00:11:03.900]   So there's sort of two types of products that we talk about.
[00:11:03.900 --> 00:11:06.820]   One of them is called a green on brown product.
[00:11:06.820 --> 00:11:08.860]   And that's out in the market today.
[00:11:08.860 --> 00:11:11.600]   It's marketed as a C and Spray Select.
[00:11:11.600 --> 00:11:21.300]   So that's a computer vision based AI system that is capable of spraying weeds that are
[00:11:21.300 --> 00:11:22.300]   in the furrow.
[00:11:22.300 --> 00:11:27.340]   So if you think of a row crop, you've got the two rows here and the furrow in the middle.
[00:11:27.340 --> 00:11:31.240]   So this product can spray weeds that are in the furrow.
[00:11:31.240 --> 00:11:35.580]   And the next level of that product is the green on green product, where now you can
[00:11:35.580 --> 00:11:41.580]   say, OK, well, I can tell the difference between a weed and a crop, even if it's in the furrow.
[00:11:41.580 --> 00:11:45.940]   Because your weed can kind of grow anywhere.
[00:11:45.940 --> 00:11:49.620]   It's not only constrained to just like anyone who has a garden, right?
[00:11:49.620 --> 00:11:52.340]   You know that weeds will grow wherever they want.
[00:11:52.340 --> 00:11:55.020]   And they're not constrained to just grow in the furrow.
[00:11:55.020 --> 00:12:00.420]   So the green on green product is sort of the next level of the capability.
[00:12:00.420 --> 00:12:02.860]   And it's a really hard vision problem.
[00:12:02.860 --> 00:12:08.940]   One of the things that makes it really hard is that it's tough to get labels that are
[00:12:08.940 --> 00:12:10.460]   correct for these.
[00:12:10.460 --> 00:12:16.820]   So I don't know about you, but it's tough for me to tell the difference between, say,
[00:12:16.820 --> 00:12:24.820]   a pigweed and a cotton plant that's a certain size, or a velvet leaf, or a morning glory,
[00:12:24.820 --> 00:12:25.820]   right?
[00:12:25.820 --> 00:12:31.340]   These are weeds that look a lot like maybe a cotton plant or maybe a soybean plant at
[00:12:31.340 --> 00:12:33.740]   a certain time of its life.
[00:12:33.740 --> 00:12:41.180]   And in order for our product to be successful, our labels on the data have to be correct.
[00:12:41.180 --> 00:12:48.380]   And what we've kind of found through trial and error is that it's pretty easy to find
[00:12:48.380 --> 00:12:52.380]   people that will label images for you.
[00:12:52.380 --> 00:12:57.060]   But it's actually really, really tough to find people that know the difference between
[00:12:57.060 --> 00:13:00.140]   these kinds of weeds and these crops.
[00:13:00.140 --> 00:13:06.660]   And so that's been kind of our main challenge on this project is assembling what I would
[00:13:06.660 --> 00:13:13.380]   call an expert workforce of agronomists, actually, who have-- some of them actually have a PhD
[00:13:13.380 --> 00:13:15.020]   in weed science.
[00:13:15.020 --> 00:13:22.380]   And these folks help us develop training materials and help us tell the difference between these
[00:13:22.380 --> 00:13:24.940]   different varieties of crop and weed.
[00:13:24.940 --> 00:13:30.660]   And yeah, that's really, really important as we look at our pipeline and our stack.
[00:13:30.660 --> 00:13:37.100]   That's kind of like the thing that we spend probably the most time on is talking about
[00:13:37.100 --> 00:13:41.300]   label quality and how to improve it and how to measure it.
[00:13:41.300 --> 00:13:44.740]   So yeah, it's a huge, huge topic for us.
[00:13:44.740 --> 00:13:51.360]   Is that partially because have weeds evolved to look like plants so that human farmers
[00:13:51.360 --> 00:13:53.940]   don't pull them up?
[00:13:53.940 --> 00:13:55.820]   There's some truth to that, yeah.
[00:13:55.820 --> 00:14:03.140]   I think there's-- I'm going to repeat a story that my agronomist told me that there's a
[00:14:03.140 --> 00:14:09.900]   weed that kind of like what's called a mimic weed in rice.
[00:14:09.900 --> 00:14:16.620]   And what happened is when people started hand weeding rice, all of a sudden, it was sort
[00:14:16.620 --> 00:14:20.660]   of a selective process where things that didn't look like rice got weeded out.
[00:14:20.660 --> 00:14:23.060]   But things that did look like rice start to survive.
[00:14:23.060 --> 00:14:25.580]   And so this mimic weed kind of evolved.
[00:14:25.580 --> 00:14:31.460]   And it sort of came to the point that it looked so much like rice while it was growing that
[00:14:31.460 --> 00:14:39.420]   a sort of trained person in the field had a roughly 70% chance of telling whether it
[00:14:39.420 --> 00:14:40.780]   was rice or not.
[00:14:40.780 --> 00:14:45.620]   So that's how good the mimicry had gotten.
[00:14:45.620 --> 00:14:48.620]   And we definitely do see that in our models too.
[00:14:48.620 --> 00:14:54.380]   So we have a-- we don't officially call it like the FBI's most wanted list.
[00:14:54.380 --> 00:14:56.340]   But maybe we should call it that.
[00:14:56.340 --> 00:15:02.140]   But we do have, yeah, some weeds that are much more challenging to differentiate.
[00:15:02.140 --> 00:15:06.500]   And they look a lot like-- sometimes we have arguments on the CVML team like, OK, well,
[00:15:06.500 --> 00:15:08.060]   what do we think this is, right?
[00:15:08.060 --> 00:15:12.260]   And some of these cases are pretty ambiguous.
[00:15:12.260 --> 00:15:18.380]   I guess if you know where you planted the plants, though, couldn't you say that any
[00:15:18.380 --> 00:15:24.220]   plant that's sort of not in the place where you think you planted a plant is a weed or
[00:15:24.220 --> 00:15:25.220]   should be sprayed?
[00:15:25.220 --> 00:15:29.900]   Or why do you need to identify exactly what kind of plant it is?
[00:15:29.900 --> 00:15:31.560]   Oh, yeah.
[00:15:31.560 --> 00:15:33.120]   That's a really good question.
[00:15:33.120 --> 00:15:35.580]   There's kind of two answers to that question.
[00:15:35.580 --> 00:15:41.140]   So answer number one is that you certainly can--
[00:15:41.140 --> 00:15:45.780]   you do have that information, especially if you're using a John Deere planting stack.
[00:15:45.780 --> 00:15:47.660]   Like I was talking about the farming stack.
[00:15:47.660 --> 00:15:53.320]   If you've got sort of John Deere machines at every part of that stack, then you've got
[00:15:53.320 --> 00:15:55.740]   information about everything you've done.
[00:15:55.740 --> 00:16:02.300]   So like specifically with XactiMerge, it's a technology for planting where you can actually
[00:16:02.300 --> 00:16:08.500]   tell precisely where the seeds were planted.
[00:16:08.500 --> 00:16:10.840]   Now the thing that you don't know is what's emerged.
[00:16:10.840 --> 00:16:15.940]   So you do know what was planted, but you don't really have a good sense of what's emerged.
[00:16:15.940 --> 00:16:21.980]   And so that's one reason that you do have to do kind of a vision-based approach to this.
[00:16:21.980 --> 00:16:30.340]   The second one reason is actually a really kind of a subtle one.
[00:16:30.340 --> 00:16:33.940]   So when we're talking about herbicides, it's funny.
[00:16:33.940 --> 00:16:38.420]   When I started at Blue River, I thought that, oh, killing plants is easy.
[00:16:38.420 --> 00:16:41.500]   You just spray a herbicide on them and they die.
[00:16:41.500 --> 00:16:44.500]   And it's actually very, very complicated.
[00:16:44.500 --> 00:16:53.780]   And one of the things that's most interesting here is that you've kind of got like two species,
[00:16:53.780 --> 00:16:56.140]   rough kind of breakdowns of plants.
[00:16:56.140 --> 00:16:58.780]   You got broadleaf and grasses.
[00:16:58.780 --> 00:17:04.100]   And you actually use different herbicides to go after broadleaf versus grasses.
[00:17:04.100 --> 00:17:10.260]   So if I can tell you with my machine learning model that, okay, this is a weed and it's
[00:17:10.260 --> 00:17:15.100]   a broadleaf weed, then you're going to put something different in your tank mix to actually
[00:17:15.100 --> 00:17:17.020]   attack that weed.
[00:17:17.020 --> 00:17:22.660]   And similarly, if you have a grass weed, like if you try to spray a broadleaf herbicide
[00:17:22.660 --> 00:17:25.000]   on your grass weed, it's not going to do anything.
[00:17:25.000 --> 00:17:30.580]   So there's the opportunity there for more savings for the customer and more effective
[00:17:30.580 --> 00:17:38.660]   weed control by identifying roughly kind of, are we dealing with broadleaf or grass?
[00:17:38.660 --> 00:17:43.900]   And then targeting the seed and spray, it's targeting the herbicides directly to the plant
[00:17:43.900 --> 00:17:44.900]   that needs them.
[00:17:44.900 --> 00:17:45.900]   I see.
[00:17:45.900 --> 00:17:47.400]   That's very cool.
[00:17:47.400 --> 00:17:49.900]   So how deployed is this?
[00:17:49.900 --> 00:17:56.140]   Like if I went to fields in Texas, would I see this device in use?
[00:17:56.140 --> 00:17:57.580]   Yeah.
[00:17:57.580 --> 00:18:02.180]   If you had been in our field season, so I guess our field season is still technically
[00:18:02.180 --> 00:18:03.180]   going.
[00:18:03.180 --> 00:18:09.360]   We're into what we call fallow operations right now, which is basically identify any
[00:18:09.360 --> 00:18:12.900]   plants that are in a fallow field and spray them.
[00:18:12.900 --> 00:18:22.660]   But between the sort of the months of roughly March to say August, that was kind of our
[00:18:22.660 --> 00:18:29.060]   main weeding season for soy and cotton and corn.
[00:18:29.060 --> 00:18:36.460]   And if you had come to Texas and the Midwest and that area of the US, then yeah, you would
[00:18:36.460 --> 00:18:38.540]   have seen this system deployed.
[00:18:38.540 --> 00:18:45.920]   And we did something really kind of a first for Blue River this past year.
[00:18:45.920 --> 00:18:52.500]   So in previous years, what we've done is we built a machine and then we'd taken it to
[00:18:52.500 --> 00:18:57.340]   a grower's field, gotten a cooperating grower, and then we'd operated the machine and kind
[00:18:57.340 --> 00:19:00.820]   of done demos to get product feedback.
[00:19:00.820 --> 00:19:05.260]   And that worked really well, but it's not the same as a customer actually operating
[00:19:05.260 --> 00:19:06.260]   machine.
[00:19:06.260 --> 00:19:12.180]   This year, what we did is we actually handed the keys over with this brand new sprayer
[00:19:12.180 --> 00:19:17.460]   that's right off the line, has all the bells and whistles and a bunch of brand new technology.
[00:19:17.460 --> 00:19:24.340]   We handed them to growers and said, you gave them a little lesson on how to run it and
[00:19:24.340 --> 00:19:31.420]   said, you actually run this machine and we're just going to sit on the road and kind of
[00:19:31.420 --> 00:19:34.820]   watch you do it and we're not going to interfere with you.
[00:19:34.820 --> 00:19:43.540]   And so, yeah, we actually did give customers the ability to run the machine and just the
[00:19:43.540 --> 00:19:45.180]   learnings from that were really great.
[00:19:45.180 --> 00:19:51.780]   So we're actually just sort of still compiling the learnings and bubbling the biggest things
[00:19:51.780 --> 00:19:56.460]   that we want to work on up to the top so we can hit the ground running again next year.
[00:19:56.460 --> 00:19:57.460]   Wow.
[00:19:57.460 --> 00:19:59.660]   Were there any surprises when you did that?
[00:19:59.660 --> 00:20:01.380]   Yeah, yeah.
[00:20:01.380 --> 00:20:02.380]   Big time.
[00:20:03.060 --> 00:20:06.180]   There was one that I think is really funny.
[00:20:06.180 --> 00:20:13.100]   So on CVML, like computer vision machine learning, we tend to look at the world in a certain
[00:20:13.100 --> 00:20:14.100]   way.
[00:20:14.100 --> 00:20:20.620]   And that way is like, okay, the most important thing for the machine to do is identify the
[00:20:20.620 --> 00:20:22.940]   weed and then spray the weed.
[00:20:22.940 --> 00:20:27.860]   And there's a pretty good reason for that because what happens to a small weed?
[00:20:27.860 --> 00:20:31.380]   Well, it turns into a big weed and that becomes a problem.
[00:20:31.380 --> 00:20:36.900]   So hitting weeds when they're small is something that we think we need to work on.
[00:20:36.900 --> 00:20:43.060]   So we worked on that problem very diligently and we've got a solution that definitely targets
[00:20:43.060 --> 00:20:44.140]   the smallest weeds.
[00:20:44.140 --> 00:20:49.100]   So we do this with a little sensitivity knob on the model.
[00:20:49.100 --> 00:20:54.300]   And what it's doing there is it's sort of thresholding the focal loss in our network.
[00:20:54.300 --> 00:20:58.980]   We're kind of thresholding that value and then making a decision based on that threshold
[00:20:58.980 --> 00:20:59.980]   setting.
[00:20:59.980 --> 00:21:01.940]   And as a user, you could target it, right?
[00:21:01.940 --> 00:21:07.020]   So you could say, okay, I want it to go really sensitive and target the smallest of the small
[00:21:07.020 --> 00:21:13.740]   weeds or I can go less sensitive and only kind of care about the big weeds.
[00:21:13.740 --> 00:21:18.660]   And what we found our customers doing was kind of using this in ways that we hadn't
[00:21:18.660 --> 00:21:19.660]   envisioned.
[00:21:19.660 --> 00:21:23.780]   And so one of those ways that was a total surprise.
[00:21:23.780 --> 00:21:28.740]   And I remember when I was talking to this, I spent a bunch of time out in the field this
[00:21:28.740 --> 00:21:31.980]   summer working with customers and observing their workflows.
[00:21:31.980 --> 00:21:36.420]   And when I came back and said, yeah, like one of the favorite things to do is to set
[00:21:36.420 --> 00:21:42.180]   this thing to be really low sensitivity and then go after only the biggest weeds.
[00:21:42.180 --> 00:21:44.380]   And my team's heads exploded.
[00:21:44.380 --> 00:21:47.500]   They're like, wow, they're doing it wrong.
[00:21:47.500 --> 00:21:48.500]   That's wrong.
[00:21:48.500 --> 00:21:49.620]   They're missing weeds.
[00:21:49.620 --> 00:21:54.900]   And it was an interesting one because as the farmers explained it to me, it made a lot
[00:21:54.900 --> 00:21:55.900]   more sense.
[00:21:55.900 --> 00:22:05.460]   So what happens with row crops is when you have just kind of picture a bare field and
[00:22:05.460 --> 00:22:11.740]   then picture putting these crops in rows and then picture like the crop emerging.
[00:22:11.740 --> 00:22:21.260]   So in that time when you're actually growing, say a soybean crop, you do want to target
[00:22:21.260 --> 00:22:25.100]   all the weeds because what will happen is those weeds will compete with your crop and
[00:22:25.100 --> 00:22:28.100]   they'll compete for nutrients and it'll reduce your yield.
[00:22:28.100 --> 00:22:35.380]   So like going pretty aggressively after those weeds in the early first post, we talk about
[00:22:35.380 --> 00:22:36.380]   pre and post.
[00:22:36.380 --> 00:22:42.400]   And so pre applications are pre planting and post are like post planting.
[00:22:42.400 --> 00:22:48.820]   So the first time you get in after planting is really called your like first post pass,
[00:22:48.820 --> 00:22:50.380]   if you will.
[00:22:50.380 --> 00:22:54.420]   And it makes a lot of sense to be aggressive at that early time.
[00:22:54.420 --> 00:23:00.980]   But as the crop starts to grow, what happens is that the crop, if you're successful, the
[00:23:00.980 --> 00:23:03.380]   crop will grow faster than the weeds.
[00:23:03.380 --> 00:23:09.460]   And because of the spacing of which they're planted, they'll actually start to canopy
[00:23:09.460 --> 00:23:10.700]   over.
[00:23:10.700 --> 00:23:15.380]   And at that point, then they've actually won largely.
[00:23:15.380 --> 00:23:21.420]   This is not a generally true statement, but it's almost generally true that largely your
[00:23:21.420 --> 00:23:24.460]   crop is won when it's canopied over.
[00:23:24.460 --> 00:23:25.460]   Interesting.
[00:23:25.460 --> 00:23:30.580]   Do you have computer vision people on your team that have deep knowledge about farming?
[00:23:30.580 --> 00:23:35.220]   I would think that might be a low overlap set of knowledge.
[00:23:35.220 --> 00:23:38.020]   Yeah, it definitely is.
[00:23:38.020 --> 00:23:40.860]   I guess that I have the most.
[00:23:40.860 --> 00:23:47.720]   So I grew up in rural Saskatchewan and we had a small like quarter section farm.
[00:23:47.720 --> 00:23:51.980]   So I think I used to ride horses and stuff when I was a kid.
[00:23:51.980 --> 00:23:56.860]   So I think I've probably got the strongest farming background on the computer vision
[00:23:56.860 --> 00:23:59.380]   machine learning team.
[00:23:59.380 --> 00:24:04.140]   But what we tell people is that, hey, you don't have to know a lot about farming to
[00:24:04.140 --> 00:24:05.700]   come and work for us.
[00:24:05.700 --> 00:24:10.300]   And what you do have to do, though, is not be afraid to go out to the field because we
[00:24:10.300 --> 00:24:13.260]   believe like that's where we learn the most.
[00:24:13.260 --> 00:24:19.740]   And that's been part of kind of Blue River's DNA, I think, for forever, essentially, is
[00:24:19.740 --> 00:24:24.500]   that we we think like, OK, you could go and like talk about stuff on the whiteboard and
[00:24:24.500 --> 00:24:26.340]   you definitely should do that.
[00:24:26.340 --> 00:24:32.440]   But you need to reduce that idea to practice and get into the field as fast as possible
[00:24:32.440 --> 00:24:37.380]   so that you can learn how you can blow up your assumptions, basically.
[00:24:37.380 --> 00:24:43.680]   So that's kind of like one of our guiding principles, I guess, at Blue River.
[00:24:43.680 --> 00:24:50.860]   And so one of my one of my friends, he calls us, we don't like to hire house cats.
[00:24:50.860 --> 00:24:54.780]   And what that means is like, if you're the sort of person that just likes to kind of
[00:24:54.780 --> 00:24:59.460]   like sit in their office and work on their problem and not go out to the field, then
[00:24:59.460 --> 00:25:04.740]   this probably may not be the greatest place for you.
[00:25:04.740 --> 00:25:07.420]   So yeah, no farming knowledge required.
[00:25:07.420 --> 00:25:08.420]   Nice.
[00:25:08.420 --> 00:25:09.540]   But I like a good customer empathy.
[00:25:09.540 --> 00:25:13.540]   I feel the same way with the Weights and Biases engineering team.
[00:25:13.540 --> 00:25:19.740]   Have there been any other surprises when you've taken these devices into the fields?
[00:25:19.740 --> 00:25:22.220]   Yeah, there was actually.
[00:25:22.220 --> 00:25:29.380]   So we have this, if you're sitting in the cab, actually, you know, the cab of these
[00:25:29.380 --> 00:25:32.220]   machines are just absolutely fantastic.
[00:25:32.220 --> 00:25:39.260]   So you hop in the cab, right, and you're sitting in this chair that feels like a fighter jet.
[00:25:39.260 --> 00:25:44.820]   And part of the allure of the fighter jet is you've got this joystick that has all these
[00:25:44.820 --> 00:25:46.100]   buttons on it.
[00:25:46.100 --> 00:25:50.220]   And the self-propelled sprayer is exactly the same.
[00:25:50.220 --> 00:25:55.620]   And without a word of a lie, there's like on the order of like 24, 25 buttons on this
[00:25:55.620 --> 00:25:58.340]   joystick and they all do something different.
[00:25:58.340 --> 00:26:02.260]   And so, you know, it's really fun to get in this thing and like, wow, you know, this is
[00:26:02.260 --> 00:26:03.380]   really, really cool.
[00:26:03.380 --> 00:26:08.260]   And so we also have a display that you can see.
[00:26:08.260 --> 00:26:09.580]   And there's a couple of displays.
[00:26:09.580 --> 00:26:15.020]   There's one that's kind of sitting here and then one that's kind of up in the, you know,
[00:26:15.020 --> 00:26:16.280]   up more at your eye level.
[00:26:16.280 --> 00:26:19.540]   So there's two displays that you can look at and you can control the system through
[00:26:19.540 --> 00:26:22.060]   the displays.
[00:26:22.060 --> 00:26:27.220]   When we launched the product for customers, you know, we thought that the kind of the
[00:26:27.220 --> 00:26:29.820]   driving factor would be killing weeds, right?
[00:26:29.820 --> 00:26:35.060]   We said, hey, you know, everybody, all the feedback we've heard is that people want to
[00:26:35.060 --> 00:26:38.120]   control their weeds and that's the most important thing.
[00:26:38.120 --> 00:26:41.140]   And then savings would be kind of second on that list.
[00:26:41.140 --> 00:26:45.900]   So that's kind of how we came into the season.
[00:26:45.900 --> 00:26:49.100]   And you know, certainly some farmers are like that.
[00:26:49.100 --> 00:26:53.420]   And I think initially when they started the machine, like when they started using the
[00:26:53.420 --> 00:26:58.060]   machine, that was their first concern is like, okay, I'm going to go with high sensitivity.
[00:26:58.060 --> 00:26:59.420]   I'm going to kill all my weeds.
[00:26:59.420 --> 00:27:02.340]   So I get the same weed control as broadcast.
[00:27:02.340 --> 00:27:05.140]   And then any savings I get on that are going to be a bonus.
[00:27:05.140 --> 00:27:08.100]   But, you know, I'm not actually going after savings.
[00:27:08.100 --> 00:27:13.540]   And at least what I saw was that people's workflows kind of shifted when they realized
[00:27:13.540 --> 00:27:18.580]   that they could use this a little bit more like a scalpel, right, instead of a sledgehammer.
[00:27:18.580 --> 00:27:25.220]   So if you've got the ability to go in and do sort of targeted delivery to only the things
[00:27:25.220 --> 00:27:29.980]   that you care about, then that changes your workflow.
[00:27:29.980 --> 00:27:35.180]   And I think what I saw that was most interesting is back to this display.
[00:27:35.180 --> 00:27:40.300]   As you're going through the field, it has something we call the applied rate map, which
[00:27:40.300 --> 00:27:46.340]   is a, it's basically like a, it's a geospatial map.
[00:27:46.340 --> 00:27:49.780]   And it shows you what the boom is doing in real time.
[00:27:49.780 --> 00:27:54.940]   So you can actually see the sprays laid down on this map as you're going over it.
[00:27:54.940 --> 00:28:00.500]   And it's sort of like a real time, you know, real time measurement of weed pressure, if
[00:28:00.500 --> 00:28:02.140]   you will.
[00:28:02.140 --> 00:28:07.940]   And I think what I was surprised by was that customers usually don't look at that display
[00:28:07.940 --> 00:28:09.220]   because it's really boring.
[00:28:09.220 --> 00:28:13.060]   If you're, if you're just doing an, you know, a broadcast application, then the applied
[00:28:13.060 --> 00:28:15.020]   rate is always the same.
[00:28:15.020 --> 00:28:18.860]   And it's not really an interesting map other than like, you know, is the sprayer on or
[00:28:18.860 --> 00:28:19.860]   not.
[00:28:19.860 --> 00:28:26.620]   But with Cnspray, it's actually a really interesting map because you can see the patches coming
[00:28:26.620 --> 00:28:30.700]   down on each individual spray nozzle.
[00:28:30.700 --> 00:28:36.740]   And what I saw that was really cool was growers were looking at that map and then they were
[00:28:36.740 --> 00:28:38.900]   looking outside and saying, oh yeah, that makes sense.
[00:28:38.900 --> 00:28:41.740]   I know I have more weeds in this area of the field.
[00:28:41.740 --> 00:28:45.500]   And then they get to another area where it wasn't spraying as much and oh, you know,
[00:28:45.500 --> 00:28:46.500]   that makes sense.
[00:28:46.500 --> 00:28:51.180]   I know that, you know, this, this area isn't as wet, so I don't have many, as many weeds.
[00:28:51.180 --> 00:28:55.820]   So I think that was really interesting to, because their eyes were just kind of glued
[00:28:55.820 --> 00:29:01.940]   to this real time mechanism to see, you know, what's, you know, what's my sprayer doing
[00:29:01.940 --> 00:29:04.380]   and what's my weed pressure like.
[00:29:04.380 --> 00:29:09.500]   And that was really cool to see folks using that.
[00:29:09.500 --> 00:29:12.100]   How much have your models improved?
[00:29:12.100 --> 00:29:14.700]   Like it sounds like they're over a threshold where it's useful.
[00:29:14.700 --> 00:29:21.180]   Do you still feel like there's a ways to go in terms of the quality of detection?
[00:29:21.180 --> 00:29:24.700]   Yeah, that's a, that's a great question.
[00:29:24.700 --> 00:29:30.420]   So the, the models have improved dramatically year over year.
[00:29:30.420 --> 00:29:33.900]   So, and that's mainly due to better data labeling.
[00:29:33.900 --> 00:29:40.180]   And yeah, there's kind of two parts to that, getting smarter with our labels and labeling,
[00:29:40.180 --> 00:29:43.900]   you know, labeling with a better workforce, which we talked about.
[00:29:43.900 --> 00:29:49.700]   Also being more targeted in what we label and really kind of preferring quality over
[00:29:49.700 --> 00:29:50.700]   quantity.
[00:29:50.700 --> 00:29:55.380]   I think when we, when we sort of got into this, we a few years ago, we, we always had
[00:29:55.380 --> 00:30:01.660]   the back of our mind, you know, quantity is really important and specifically diversity
[00:30:01.660 --> 00:30:03.300]   is important.
[00:30:03.300 --> 00:30:10.820]   And the way that we kind of approach this collection of diversity is to try to collect
[00:30:10.820 --> 00:30:14.760]   data in every kind of growing condition we can get our hands on.
[00:30:14.760 --> 00:30:20.020]   So it's interesting to see how much different the ground looks, you know, when you're, when
[00:30:20.020 --> 00:30:25.340]   we're kind of talking about soybeans, we might have a picture in our minds of a, of a soybean
[00:30:25.340 --> 00:30:30.860]   on dark soil and, you know, a really pristine kind of computer vision environment that you
[00:30:30.860 --> 00:30:34.100]   could train a model on in 10 minutes and do something.
[00:30:34.100 --> 00:30:36.580]   And it turns out not to be true at all.
[00:30:36.580 --> 00:30:40.220]   There are so many confounding factors.
[00:30:40.220 --> 00:30:44.940]   One sort of visually confounding factor that's really interesting is folks are really getting
[00:30:44.940 --> 00:30:51.180]   into no-till planting and no-till planting is exactly what it sounds like.
[00:30:51.180 --> 00:30:56.380]   You just sort of don't do the tillage step and you keep the cover crop that was there
[00:30:56.380 --> 00:31:02.940]   last year, so say, you know, let's say you're rotating corn and soybean, you might, you
[00:31:02.940 --> 00:31:07.940]   know, grow corn this year and then next year you plant soybeans, but you don't actually
[00:31:07.940 --> 00:31:12.540]   like till it under, you just like run your planter through the old dead corn, right?
[00:31:12.540 --> 00:31:17.220]   So you had all these like stalks sticking up and they're all dead and then you've got
[00:31:17.220 --> 00:31:21.260]   some plants that are emerging that are alive and then you've got weeds.
[00:31:21.260 --> 00:31:26.700]   And so it's almost like the most confusing like computer vision environment you could
[00:31:26.700 --> 00:31:30.100]   possibly imagine.
[00:31:30.100 --> 00:31:31.820]   And that makes it really hard, right?
[00:31:31.820 --> 00:31:39.780]   And so we've been working really hard on beefing our models up to work in these different situations.
[00:31:39.780 --> 00:31:44.260]   Another really good one is just the soil color and the farming practices.
[00:31:44.260 --> 00:31:51.740]   You know, in countries like Brazil, they actually don't really plant on 30 inch rows.
[00:31:51.740 --> 00:31:56.420]   They plant, you know, sometimes much, much denser than that.
[00:31:56.420 --> 00:32:00.060]   And that means that you can't really put a sprayer between the rows anymore.
[00:32:00.060 --> 00:32:05.660]   So they actually drive like 45 degrees across the rows and, you know, kind of kill plants
[00:32:05.660 --> 00:32:09.900]   with or run over plants with the sprayers.
[00:32:09.900 --> 00:32:16.860]   That's another like situation that our model has to handle is these different farming practices
[00:32:16.860 --> 00:32:18.820]   in different regions.
[00:32:18.820 --> 00:32:22.340]   I guess as an aside, but how do they then do other, don't they always have to drive
[00:32:22.340 --> 00:32:25.060]   some machine over their fields?
[00:32:25.060 --> 00:32:32.700]   Why would they put them so close together that they can't drive machines over the fields
[00:32:32.700 --> 00:32:36.340]   without squashing plants?
[00:32:36.340 --> 00:32:40.500]   It's a little bit of a mix of different types of machines.
[00:32:40.500 --> 00:32:48.860]   So yeah, sometimes what we see is that the farming practices kind of in the US, someone
[00:32:48.860 --> 00:32:52.580]   described this to me as like, how would you build a factory, right?
[00:32:52.580 --> 00:32:58.580]   Well, what you do is you'd mechanize every part of the operation and you'd build machines
[00:32:58.580 --> 00:33:00.980]   that do this over and over and over again.
[00:33:00.980 --> 00:33:06.660]   And you can think of farming the same way, except basically the factory goes to the plants,
[00:33:06.660 --> 00:33:10.820]   not the other way around, you know, excluding kind of vertical farming, which is a different
[00:33:10.820 --> 00:33:12.180]   thing.
[00:33:12.180 --> 00:33:17.460]   So when you have sort of like the John Deere, you know, if you have a full John Deere stack
[00:33:17.460 --> 00:33:20.980]   for all this, then you don't really have this problem.
[00:33:20.980 --> 00:33:28.980]   Like you can, yeah, you can plant with it as, you know, a compatible spacing with your
[00:33:28.980 --> 00:33:32.660]   planter and your weeder and your harvester all sort of compatible.
[00:33:32.660 --> 00:33:36.860]   But when you mix and match and you don't have, you know, you don't have that kind of end
[00:33:36.860 --> 00:33:44.940]   to end solution, then you sort of do end up into an interesting area where you've got
[00:33:44.940 --> 00:33:46.820]   to make some decisions.
[00:33:46.820 --> 00:33:51.420]   And so Brazil is a special climate too.
[00:33:51.420 --> 00:33:55.580]   It's a lot more humid and, you know, they actually kind of grow year round, right?
[00:33:55.580 --> 00:34:01.600]   So, you know, it's a really interesting one and soybeans in that kind of environment love
[00:34:01.600 --> 00:34:03.780]   to be way dense.
[00:34:03.780 --> 00:34:10.140]   So it's kind of an interesting one, but yeah, that's one of the challenges we have with
[00:34:10.140 --> 00:34:14.620]   computer vision of teaching your model to identify weeds really at any kind of orientation.
[00:34:14.620 --> 00:34:15.620]   Yeah.
[00:34:15.620 --> 00:34:18.380]   I'm seeing why this is a harder problem than I was imagining.
[00:34:18.380 --> 00:34:21.180]   These are really evocative examples.
[00:34:21.180 --> 00:34:25.660]   Does it ever happen that your model has like an escape valve or something, or like it has
[00:34:25.660 --> 00:34:28.300]   some sense that, you know what, this is too hard.
[00:34:28.300 --> 00:34:33.340]   Like I, like Red Alert, don't, I don't want to like touch any plants.
[00:34:33.340 --> 00:34:41.260]   Yeah, actually we have exactly this system and it's how we, it's really in a nutshell
[00:34:41.260 --> 00:34:42.900]   how we respond to dust.
[00:34:42.900 --> 00:34:49.980]   So you can imagine that dust is a really complicated environment because, you know, there's really
[00:34:49.980 --> 00:34:55.300]   two things working against you is number one, you know, it's almost impossible to get good
[00:34:55.300 --> 00:35:01.780]   quality labels on a dusty image because, Hey, if people can't tell what's in the image,
[00:35:01.780 --> 00:35:06.500]   then Hey, it's going to be really hard to get a machine to do that too.
[00:35:06.500 --> 00:35:09.160]   So labels are tough to do.
[00:35:09.160 --> 00:35:17.540]   But the other thing that I like to talk about is these models oftenly, and there's research,
[00:35:17.540 --> 00:35:22.620]   I think to back this up, I'm trying to remember the name of the paper that came out, but it's,
[00:35:22.620 --> 00:35:28.540]   I think it's called like confidently incorrect, but these machine learning models can be very
[00:35:28.540 --> 00:35:29.980]   confidently incorrect.
[00:35:29.980 --> 00:35:35.540]   And so like, just to kind of like really stupid example is, you know, if we have a elephant
[00:35:35.540 --> 00:35:39.220]   versus giraffe classifier, then you show it a rhino, right.
[00:35:39.220 --> 00:35:42.580]   It's going to be confidently incorrect.
[00:35:42.580 --> 00:35:44.020]   And just kind of by definition.
[00:35:44.020 --> 00:35:45.020]   Right.
[00:35:45.020 --> 00:35:50.460]   So what we've done is we've actually trained, we sort of have a architecture that's a little
[00:35:50.460 --> 00:35:54.980]   bit like the Tesla model where we've got this backbone and then these heads that do different
[00:35:54.980 --> 00:35:55.980]   things.
[00:35:55.980 --> 00:36:02.740]   And so we have this image quality head that tells us our dust probability, and we've trained
[00:36:02.740 --> 00:36:05.860]   it to really detect the presence of dust.
[00:36:05.860 --> 00:36:10.460]   And once that's above a certain threshold, then we say, ah, you know, probably the results
[00:36:10.460 --> 00:36:13.900]   from the model are, are not to be trusted in this scenario there.
[00:36:13.900 --> 00:36:17.300]   You know, the model might be confidently incorrect.
[00:36:17.300 --> 00:36:22.700]   And then what we do is we, we do in our system, it's pretty easy.
[00:36:22.700 --> 00:36:24.300]   We have what's called a fallback.
[00:36:24.300 --> 00:36:25.860]   So we fall back to broadcast.
[00:36:25.860 --> 00:36:30.620]   So the idea is like, you know, if you're not sure what it is in your model, you don't trust
[00:36:30.620 --> 00:36:32.540]   the results of the model because it's dusty.
[00:36:32.540 --> 00:36:37.340]   Then you just turn the sprayer on and that way you make sure you're not missing weeds.
[00:36:37.340 --> 00:36:38.340]   Oh, interesting.
[00:36:38.340 --> 00:36:41.260]   So, so you're really doing multitask learning.
[00:36:41.260 --> 00:36:42.980]   Is that, is that right?
[00:36:42.980 --> 00:36:44.660]   Yeah, that's right.
[00:36:44.660 --> 00:36:50.340]   We have a few other kind of image quality related heads that's, that's in our architecture.
[00:36:50.340 --> 00:36:55.940]   And yeah, we're always kind of thinking about adding, adding more, you know, as we, as we
[00:36:55.940 --> 00:37:03.220]   discover situations that we need to detect, like, you know, a good example is implement
[00:37:03.220 --> 00:37:04.220]   occlusion.
[00:37:04.220 --> 00:37:08.380]   So if, if part of the implement gets into the camera frame, you know, we want to be
[00:37:08.380 --> 00:37:13.980]   able to detect that and, and, you know, you doing that as, as a kind of a detection head
[00:37:13.980 --> 00:37:20.260]   off of, you know, an encoder backbone is, is a really very efficient way to do that.
[00:37:20.260 --> 00:37:22.220]   Because you don't pay much of a runtime.
[00:37:22.220 --> 00:37:26.260]   You know, I think, I think we pay, I don't know, the numbers aren't quite in my head,
[00:37:26.260 --> 00:37:33.220]   but I think we pay like much less than a millisecond runtime for, you know, doing a dust classification.
[00:37:33.220 --> 00:37:35.460]   And how frequently do you update these models?
[00:37:35.460 --> 00:37:38.780]   Like, can you update the model on one of these devices?
[00:37:38.780 --> 00:37:40.220]   Would you do that?
[00:37:40.220 --> 00:37:42.740]   Yeah, we do do that.
[00:37:42.740 --> 00:37:49.260]   And it's really kind of, it's going to be up to the farmer ultimately how, how often
[00:37:49.260 --> 00:37:50.260]   they update.
[00:37:50.260 --> 00:37:53.220]   And really what's going to dictate that is their connectivity.
[00:37:53.220 --> 00:37:58.580]   So some of the farmers that we worked with, you know, had really good connectivity and
[00:37:58.580 --> 00:38:03.660]   we could push updates to the machine very frequently if we wanted to.
[00:38:03.660 --> 00:38:08.020]   And other farmers that we worked with, you know, they're, they're in very remote areas
[00:38:08.020 --> 00:38:14.060]   of sort of, you know, West Texas and, and there are no bars, you know, you have to go
[00:38:14.060 --> 00:38:19.140]   20, 30 miles away before you get to like one 4G bar.
[00:38:19.140 --> 00:38:23.260]   And you know, those, I think what's going to happen is we're going to see that the folks
[00:38:23.260 --> 00:38:27.660]   that have connectivity are going to be updating more often and they're going to be getting
[00:38:27.660 --> 00:38:30.780]   like kind of the latest and the greatest models.
[00:38:30.780 --> 00:38:37.420]   The folks that are not are, are going to be just, you know, updating much, much less often.
[00:38:37.420 --> 00:38:42.940]   And are you able to use the data that the cameras collect in the fields to improve the
[00:38:42.940 --> 00:38:43.940]   models?
[00:38:43.940 --> 00:38:46.180]   Yeah, yeah, we do that.
[00:38:46.180 --> 00:38:51.380]   So just like every other, you know, company that has a bunch of sensors, you, you quickly
[00:38:51.380 --> 00:38:54.300]   figure out like, okay, well, we can't record all the time.
[00:38:54.300 --> 00:38:59.580]   And so our solution to this is that we, we do some sort of like very sparse triggered
[00:38:59.580 --> 00:39:04.660]   recording as folks are going through the field within minutes of them going through the field,
[00:39:04.660 --> 00:39:08.980]   you know, subject to the connectivity constraint that I talked about.
[00:39:08.980 --> 00:39:15.100]   The machines are uploading data so that we can validate the performance of that data.
[00:39:15.100 --> 00:39:21.140]   And we like to think of this sort of like ML flywheel concept, right, where I think
[00:39:21.140 --> 00:39:26.380]   Andrew Inkels is kind of like the virtuous cycle of AI, where you train a model, you
[00:39:26.380 --> 00:39:31.700]   know, it does some predictions, you evaluate those predictions, you know, train more models,
[00:39:31.700 --> 00:39:33.980]   and your model gets better and better.
[00:39:33.980 --> 00:39:38.180]   And so we have folks sign a sort of a data use agreement that allows us to do this.
[00:39:38.180 --> 00:39:44.540]   But this data is a goldmine for us in terms of finding edge conditions in the model.
[00:39:44.540 --> 00:39:48.780]   And, you know, when you're training machine learning models, you kind of like, quickly
[00:39:48.780 --> 00:39:54.860]   start breaking champagne, you know, after you get x number of model of images in a model,
[00:39:54.860 --> 00:39:58.820]   because you know, you're sort of like, getting to this point of dimension returns pretty
[00:39:58.820 --> 00:40:00.180]   fast.
[00:40:00.180 --> 00:40:04.460]   But that doesn't mean your model is working well, that means your model is generally okay,
[00:40:04.460 --> 00:40:08.060]   but it might actually really suck in some situations, right.
[00:40:08.060 --> 00:40:15.060]   And so what our metric is, is how many fields are you actually passing our spec on, right,
[00:40:15.060 --> 00:40:18.900]   instead of, you know, instead of aggregating all fields together into a single number,
[00:40:18.900 --> 00:40:21.580]   we break them out on a per field basis.
[00:40:21.580 --> 00:40:25.180]   And our goal is to always drive that number higher.
[00:40:25.180 --> 00:40:30.140]   And so it really comes to kind of into exception management, where you've, you know, you very
[00:40:30.140 --> 00:40:35.140]   quickly kind of reach this this point where the model is actually doing pretty well, but
[00:40:35.140 --> 00:40:37.100]   then it has like these notable failures.
[00:40:37.100 --> 00:40:41.780]   And now your attention shifts to addressing while detecting and then addressing these
[00:40:41.780 --> 00:40:42.780]   failures.
[00:40:42.780 --> 00:40:47.740]   And, and that's what the ultra sparse logging helps us to do, because we can, we can really
[00:40:47.740 --> 00:40:52.260]   just get the data from the, from the customers, you know, in the fields they're actually trying
[00:40:52.260 --> 00:40:55.620]   to work on and we can improve their models.
[00:40:55.620 --> 00:41:03.300]   So you guys are really doing a hard real application of ML on massive data sizes, and you've been
[00:41:03.300 --> 00:41:07.340]   doing it for long enough to really be battle hardened, I guess.
[00:41:07.340 --> 00:41:13.100]   So I'm really dying to know, like, what, what is your ML stack look like?
[00:41:13.100 --> 00:41:14.820]   And how did you get to it?
[00:41:14.820 --> 00:41:18.620]   Like, like, how did you decide on like your ML framework?
[00:41:18.620 --> 00:41:21.180]   What are the other tools that are really important to you?
[00:41:21.180 --> 00:41:23.940]   Can you talk a little bit about that?
[00:41:23.940 --> 00:41:25.820]   Oh, yeah, sure.
[00:41:25.820 --> 00:41:29.420]   It's been a really kind of a fun journey.
[00:41:29.420 --> 00:41:34.580]   Kind of going back to 2016, you know, we were training our first models in CAFE, because
[00:41:34.580 --> 00:41:39.300]   that was the, you know, nobody's heard of CAFE these days, but boy, that was like the
[00:41:39.300 --> 00:41:42.500]   tool that everybody used back in 2016.
[00:41:42.500 --> 00:41:47.140]   It was way further ahead than say TensorFlow at the time.
[00:41:47.140 --> 00:41:50.980]   So yeah, we started with CAFE, and we built a system out in CAFE.
[00:41:50.980 --> 00:41:54.420]   And then we saw some interesting things going with TensorFlow.
[00:41:54.420 --> 00:41:58.860]   And you know, I suspect this will be kind of a familiar story to a lot of your, hopefully
[00:41:58.860 --> 00:42:00.380]   a lot of your listeners.
[00:42:00.380 --> 00:42:03.980]   We moved to TensorFlow in sort of 2018.
[00:42:03.980 --> 00:42:06.900]   And then we saw, hey, you know, PyTorch is getting really interesting.
[00:42:06.900 --> 00:42:10.660]   So we moved to PyTorch about a year ago.
[00:42:10.660 --> 00:42:14.940]   And so we've been doing most of our work in PyTorch now.
[00:42:14.940 --> 00:42:17.380]   What caused you to move from TensorFlow to PyTorch?
[00:42:17.380 --> 00:42:19.540]   Was it like a feature that mattered?
[00:42:19.540 --> 00:42:23.140]   What was the driving reason?
[00:42:23.140 --> 00:42:25.060]   It was really adoption.
[00:42:25.060 --> 00:42:28.900]   So we found like the...
[00:42:28.900 --> 00:42:35.060]   Now this sort of our experience with TensorFlow was really before the eager interface, you
[00:42:35.060 --> 00:42:37.220]   know, and so yeah, so prior to eager...
[00:42:37.220 --> 00:42:39.860]   So this is more than a year ago, probably, right?
[00:42:39.860 --> 00:42:45.140]   Yeah, I think it was like 2018 when we were doing PyTorch.
[00:42:45.140 --> 00:42:51.740]   Yeah, and so it was really before the eager interface and our folks are finding like,
[00:42:51.740 --> 00:42:54.580]   you know, it follows the papers a little bit too, right?
[00:42:54.580 --> 00:43:00.860]   So when folks write a lot of papers, and there's, you know, if you go to papers with code, and
[00:43:00.860 --> 00:43:05.940]   you can download the PyTorch thing, you know, that's just like a much lower startup cost,
[00:43:05.940 --> 00:43:06.940]   right?
[00:43:06.940 --> 00:43:09.700]   So to some sense, it follows that.
[00:43:09.700 --> 00:43:11.220]   Interesting.
[00:43:11.220 --> 00:43:12.220]   And what else?
[00:43:12.220 --> 00:43:14.340]   Do you guys do hyperparameter optimization?
[00:43:14.340 --> 00:43:15.340]   Is that important to you?
[00:43:15.340 --> 00:43:17.740]   Do you use like a data store for this?
[00:43:17.740 --> 00:43:19.860]   What are you using?
[00:43:19.860 --> 00:43:27.220]   Yeah, so in terms of our training stack, you know, PyTorch is kind of our main tool.
[00:43:27.220 --> 00:43:30.700]   We have done a little bit of hyperparameter search.
[00:43:30.700 --> 00:43:36.780]   We, you know, one of the things that's a big challenge for us is that, you know, at some
[00:43:36.780 --> 00:43:39.620]   level, we're stuck between this like rock and a hard place.
[00:43:39.620 --> 00:43:45.360]   And the rock is the accuracy, you know, we want the highest accuracy.
[00:43:45.360 --> 00:43:49.560]   But the hard place is also we need to make this run as fast as possible.
[00:43:49.560 --> 00:43:59.500]   So in our system, the speed at which the farmer could drive the machine is directly gated
[00:43:59.500 --> 00:44:01.860]   by how fast our inference goes.
[00:44:01.860 --> 00:44:08.300]   So we have to take great pains to, you know, try to pick an engineer and network architecture
[00:44:08.300 --> 00:44:16.240]   that's going to, you know, satisfy our accuracy, but run in the time constraints that we need.
[00:44:16.240 --> 00:44:21.540]   And so that actually kind of dictates a lot of our, you know, network architecture choices.
[00:44:21.540 --> 00:44:28.440]   And in terms of our kind of deployment stack, what we, you know, so back to hyperparameter.
[00:44:28.440 --> 00:44:32.420]   So what that means is that we actually don't have a tremendous number of hyperparameters
[00:44:32.420 --> 00:44:33.420]   to search for.
[00:44:33.420 --> 00:44:38.380]   It's really just sort of learning rate, learning rate scheduler, you know, and a handful of
[00:44:38.380 --> 00:44:39.380]   other things.
[00:44:39.380 --> 00:44:42.960]   But we're sort of like by definition, almost not.
[00:44:42.960 --> 00:44:46.380]   Our search space and architecture isn't very large.
[00:44:46.380 --> 00:44:51.440]   So some of the things that you would normally do with hyperparameters, we haven't found
[00:44:51.440 --> 00:44:55.820]   them to be all that useful for our problem.
[00:44:55.820 --> 00:45:00.940]   And I was going to ask, this just came to me, but it seems like a self-driving car is
[00:45:00.940 --> 00:45:04.980]   hard, but a self-driving tractor might be pretty easy, especially if someone's just
[00:45:04.980 --> 00:45:08.260]   looking at a console instead of the field.
[00:45:08.260 --> 00:45:11.660]   Why does a human have to drive the tractor if you can do these really smart things to
[00:45:11.660 --> 00:45:13.940]   figure out where the weeds are?
[00:45:13.940 --> 00:45:19.280]   Yeah, you're right on the money there, Lucas, as usual.
[00:45:19.280 --> 00:45:26.120]   If you think about what are the challenges of an auto driving car, and they're really
[00:45:26.120 --> 00:45:27.120]   large, right?
[00:45:27.120 --> 00:45:32.960]   But just to pick one example, let's say you're cruising along the freeway and you're going
[00:45:32.960 --> 00:45:33.960]   100 kilometers an hour.
[00:45:33.960 --> 00:45:38.560]   I'm from Canada, so that's why I think of kilometers.
[00:45:38.560 --> 00:45:44.280]   So you're going 100 kilometers an hour and then something happens and you need to slam
[00:45:44.280 --> 00:45:46.440]   on the brakes, right?
[00:45:46.440 --> 00:45:48.920]   Is that the right thing to do on the freeway?
[00:45:48.920 --> 00:45:51.560]   And the answer is, well, maybe, I don't know.
[00:45:51.560 --> 00:45:54.920]   It depends what the problem is.
[00:45:54.920 --> 00:46:01.640]   With tractor automation, it's actually fine because your field isn't full of tractors
[00:46:01.640 --> 00:46:07.520]   going 100 kilometers an hour and nobody's going to sideswipe you with another tractor.
[00:46:07.520 --> 00:46:12.320]   And if you see something weird, you could just stop and it's totally fine.
[00:46:12.320 --> 00:46:16.080]   So yeah, you're right on the money.
[00:46:16.080 --> 00:46:17.080]   And it's geofenced, right?
[00:46:17.080 --> 00:46:22.840]   So you could say, okay, well, I'm just going to allow myself to be automated in this area.
[00:46:22.840 --> 00:46:24.680]   And there's a lot of it.
[00:46:24.680 --> 00:46:34.200]   Yeah, definitely a lot of advantages in terms of our problem space for automation.
[00:46:34.200 --> 00:46:35.200]   Huge number of advantages.
[00:46:35.200 --> 00:46:39.160]   And is that something that you might work on?
[00:46:39.160 --> 00:46:43.680]   You did see that John Deere acquired Bearflag recently.
[00:46:43.680 --> 00:46:50.000]   So I think you can probably guess that John Deere is very interested in this.
[00:46:50.000 --> 00:46:52.360]   And I think some more information will be coming.
[00:46:52.360 --> 00:46:53.360]   Awesome.
[00:46:53.360 --> 00:46:58.440]   Are there any other farming applications that you're really excited about?
[00:46:58.440 --> 00:47:06.240]   Yeah, we view weeding as kind of like almost the beachhead project, right?
[00:47:06.240 --> 00:47:11.200]   And I think what-- I've got a background in astronomy.
[00:47:11.200 --> 00:47:16.880]   So this metaphor kind of makes sense to me of like, when you build a new telescope that
[00:47:16.880 --> 00:47:21.840]   has maybe different modalities or a different resolution, all of a sudden, you get this
[00:47:21.840 --> 00:47:23.560]   data from this new telescope.
[00:47:23.560 --> 00:47:27.760]   And then you start answering questions you never even thought to ask before.
[00:47:27.760 --> 00:47:31.720]   And I kind of feel like the same thing is going to happen with our system.
[00:47:31.720 --> 00:47:37.760]   The fact that you have cameras that are three feet away from plants taking pictures at a
[00:47:37.760 --> 00:47:43.280]   high rate of speed and high resolution pictures is really going to open our eyes and change
[00:47:43.280 --> 00:47:45.720]   the things that we're doing with the data, right?
[00:47:45.720 --> 00:47:50.320]   So you can actually see the weeds evolve as you spray them.
[00:47:50.320 --> 00:47:53.780]   So you can go in today, do your sprays.
[00:47:53.780 --> 00:48:00.280]   You can look at that map and say, OK, where did I actually spray the most?
[00:48:00.280 --> 00:48:02.160]   And which weeds am I concerned about?
[00:48:02.160 --> 00:48:05.960]   And you can plan your next workflow based on that data.
[00:48:05.960 --> 00:48:06.960]   Cool.
[00:48:06.960 --> 00:48:11.280]   Well, we always end with two open-ended questions.
[00:48:11.280 --> 00:48:14.320]   And one doesn't necessarily even need to be about farming.
[00:48:14.320 --> 00:48:20.960]   It's kind of your take on what's a topic in machine learning that you think is underappreciated
[00:48:20.960 --> 00:48:25.960]   or something that you'd like to dig more into if you had more time in your life?
[00:48:25.960 --> 00:48:26.960]   Oh, yeah.
[00:48:26.960 --> 00:48:31.080]   Yeah, I did put a little bit of thought on this one.
[00:48:31.080 --> 00:48:39.200]   So I think a really underrated aspect of machine learning, there was a paper in, I think it
[00:48:39.200 --> 00:48:42.720]   was 2013, it was called Technical Debt in Machine Learning Systems.
[00:48:42.720 --> 00:48:43.720]   Oh, yeah.
[00:48:43.720 --> 00:48:44.720]   It was by Google.
[00:48:44.720 --> 00:48:45.720]   Google, yeah.
[00:48:45.720 --> 00:48:46.720]   You know this paper, yeah.
[00:48:46.720 --> 00:48:47.720]   Yeah.
[00:48:47.720 --> 00:48:52.680]   And the thing that really jumped out to me on this paper is when you think about the
[00:48:52.680 --> 00:48:59.840]   whole machine learning pipeline, sort of like 20% of it is the cool, sexy stuff that everybody
[00:48:59.840 --> 00:49:07.160]   wants to do, and network design, and hyperparameters, and all that fun stuff, training on thousands
[00:49:07.160 --> 00:49:09.640]   of GPUs, right?
[00:49:09.640 --> 00:49:14.640]   That's all fun and very necessary, but it's about sort of 20% of the problem.
[00:49:14.640 --> 00:49:20.080]   And really, if you don't have a really strong data infrastructure around that, then you
[00:49:20.080 --> 00:49:25.320]   actually can't do that part, that 20% part.
[00:49:25.320 --> 00:49:26.480]   So you can't make your product.
[00:49:26.480 --> 00:49:32.760]   So having an excellent data pipeline, I think, is kind of underrated.
[00:49:32.760 --> 00:49:36.600]   And it's something that everybody kind of, I think, underestimates when they get into
[00:49:36.600 --> 00:49:37.600]   machine learning.
[00:49:37.600 --> 00:49:44.000]   It's like, OK, all I got to do is take this TensorFlow tutorial and do MNIST, and OK,
[00:49:44.000 --> 00:49:45.000]   awesome.
[00:49:45.000 --> 00:49:48.120]   Now, just fine tune a model, and I'm off to the races.
[00:49:48.120 --> 00:49:53.880]   And if you've got an awesome data pipeline, then that's kind of largely true.
[00:49:53.880 --> 00:49:59.120]   But if you don't, then you've got to focus your efforts on building that data pipeline,
[00:49:59.120 --> 00:50:08.640]   because that's really, I think, probably the most underrated problem in building ML systems,
[00:50:08.640 --> 00:50:09.640]   in my mind.
[00:50:09.640 --> 00:50:14.840]   Do you have any specific suggestions for someone putting together a data pipeline, from approach
[00:50:14.840 --> 00:50:19.160]   to even what software they might look at first?
[00:50:19.160 --> 00:50:21.320]   Yeah, that's a great question.
[00:50:21.320 --> 00:50:24.760]   So I'll kind of draw a parallel to labeling.
[00:50:24.760 --> 00:50:29.960]   So in 2016, we wanted to do labeling on images.
[00:50:29.960 --> 00:50:34.280]   And we couldn't really find what we wanted.
[00:50:34.280 --> 00:50:39.000]   So we kind of had to build our own system based on mTurk to do that.
[00:50:39.000 --> 00:50:43.520]   And that made sense at the time, because we couldn't sort of look at the marketplace and
[00:50:43.520 --> 00:50:47.400]   pick the thing that looked good and then go with it.
[00:50:47.400 --> 00:50:53.080]   And then sort of in 2018, well, we said, "Okay, well, this exists now, so we can just do this."
[00:50:53.080 --> 00:50:57.160]   And that's kind of how we met you with your old company.
[00:50:57.160 --> 00:51:05.640]   And so I think what I'm seeing is the startup world is afire with MLOps solutions, right?
[00:51:05.640 --> 00:51:12.280]   So folks have figured out, "Wow, these problems are hard, and they're hairy, and they're nasty."
[00:51:12.280 --> 00:51:16.640]   And I would sort of encourage folks, when they're looking at data pipeline, to, yeah,
[00:51:16.640 --> 00:51:20.040]   play the space.
[00:51:20.040 --> 00:51:24.680]   There's sort of new offerings kind of every day out there, right?
[00:51:24.680 --> 00:51:30.120]   And I think you'd be doing yourself a disservice if you didn't at least evaluate them before
[00:51:30.120 --> 00:51:33.320]   you embark on your data pipeline journey.
[00:51:33.320 --> 00:51:38.240]   I don't know that there's a solution out there that's going to do everything that you want,
[00:51:38.240 --> 00:51:41.920]   but there could be something that gets you pretty far, and then you can add your own
[00:51:41.920 --> 00:51:46.080]   plugins or something to that effect, right?
[00:51:46.080 --> 00:51:51.560]   But I would say, yeah, if you're sort of like, "Clean slate, how do I get started?"
[00:51:51.560 --> 00:51:55.040]   Well, you should definitely look at some of the software packages that are out there,
[00:51:55.040 --> 00:52:00.240]   because this is not really a problem that you necessarily want to solve unless you absolutely
[00:52:00.240 --> 00:52:01.240]   have to.
[00:52:01.240 --> 00:52:06.240]   Well, as someone that makes one of these software packages, I strongly agree with that.
[00:52:06.240 --> 00:52:11.320]   Yeah, I can imagine.
[00:52:11.320 --> 00:52:15.760]   I guess that's a good segue to our final question, which is, and you guys have had one of the,
[00:52:15.760 --> 00:52:21.400]   I think the longest journeys of sort of imagining a solution and then actually getting it working
[00:52:21.400 --> 00:52:22.400]   in production.
[00:52:22.400 --> 00:52:30.680]   I mean, what's it been like, eight or nine years maybe for that full cycle?
[00:52:30.680 --> 00:52:36.320]   Where have the biggest bottlenecks or the most surprising bottlenecks been?
[00:52:36.320 --> 00:52:40.400]   Yeah, that's a deep one for sure.
[00:52:40.400 --> 00:52:48.440]   So Blue River's kind of first computer vision product was the lettuce thinning machine.
[00:52:48.440 --> 00:52:57.920]   And that kind of operated from 2013 to 2016 in the Salinas Valley in Yuma, Arizona.
[00:52:57.920 --> 00:53:02.280]   And then we kind of pivoted to do row crops.
[00:53:02.280 --> 00:53:08.960]   So that's kind of when I got involved in the company is 2016, working on row crops.
[00:53:08.960 --> 00:53:13.400]   And so, yeah, our journey has kind of been from 2016 to roughly now.
[00:53:13.400 --> 00:53:21.280]   And the things that are really, I think, hard is when you're talking about building a piece
[00:53:21.280 --> 00:53:26.840]   of hardware and a piece of software and ML that work together, right?
[00:53:26.840 --> 00:53:29.600]   That's a really, really hard thing to do.
[00:53:29.600 --> 00:53:35.040]   And I think there's definitely a huge difference between kind of building, like being really
[00:53:35.040 --> 00:53:39.400]   scrappy and building a prototype that is going to work out in the field and get customer
[00:53:39.400 --> 00:53:40.720]   feedback.
[00:53:40.720 --> 00:53:46.700]   That is a tremendously different proposition from scaling these things out to thousands
[00:53:46.700 --> 00:53:47.700]   of machines.
[00:53:47.700 --> 00:53:50.640]   I know with John Deere, right?
[00:53:50.640 --> 00:53:51.640]   That's a huge jump.
[00:53:51.640 --> 00:53:55.240]   And so I think that's kind of like scaling.
[00:53:55.240 --> 00:54:00.600]   And I was looking at a book just yesterday written by Reed Nelson.
[00:54:00.600 --> 00:54:09.120]   I think the title was "Scaling" and achieving scale is really a big challenge.
[00:54:09.120 --> 00:54:13.600]   And I think it's even more complicated when you've got kind of hardware in the loop, because
[00:54:13.600 --> 00:54:18.640]   now instead of just having the software portion or just the ML portion, now you've actually
[00:54:18.640 --> 00:54:26.880]   got hardware, which necessarily takes a much longer cycle time to improve or fix.
[00:54:26.880 --> 00:54:34.440]   So I think that's just been, I think the biggest challenge is marrying the two, software and
[00:54:34.440 --> 00:54:39.500]   the hardware together and getting something that drives customer value.
[00:54:39.500 --> 00:54:43.800]   That's been, I think, the biggest challenge for sure.
[00:54:43.800 --> 00:54:49.300]   Well, congratulations on making such an amazing product that helps farmers and helps the world.
[00:54:49.300 --> 00:54:51.920]   It's great to talk to you about it.
[00:54:51.920 --> 00:54:54.380]   Yeah, thanks a lot.
[00:54:54.380 --> 00:54:57.720]   We definitely are excited by this product.
[00:54:57.720 --> 00:55:03.780]   And what I'm personally really excited about is as we scale this thing out to tens of machines
[00:55:03.780 --> 00:55:08.280]   and hundreds of machines and thousands of machines, those savings are going to go proportionally.
[00:55:08.280 --> 00:55:11.480]   And I've been talking to John Deere about this.
[00:55:11.480 --> 00:55:16.980]   I want to get a savings, gallons of herbicides saved on the John Deere website.
[00:55:16.980 --> 00:55:20.600]   It's going to just keep increasing, going to this really big number.
[00:55:20.600 --> 00:55:23.600]   It's going to be like the national debt, but it'll be a good number.
[00:55:23.600 --> 00:55:24.600]   Awesome.
[00:55:24.600 --> 00:55:30.160]   Well, even when that happens, I'll take a look and feel a little bit of pride.
[00:55:30.160 --> 00:55:33.160]   I'll give you the link.
[00:55:33.160 --> 00:55:34.160]   Awesome.
[00:55:34.160 --> 00:55:35.160]   Excellent.
[00:55:35.160 --> 00:55:36.720]   Thanks for your time.
[00:55:36.720 --> 00:55:37.720]   Okay.
[00:55:37.720 --> 00:55:38.720]   Thanks, Lucas.
[00:55:38.720 --> 00:55:39.720]   Take care.
[00:55:39.720 --> 00:55:43.680]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:55:43.680 --> 00:55:48.420]   to the show notes in the description where you can find links to all the papers that
[00:55:48.420 --> 00:55:52.680]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:55:52.680 --> 00:55:53.680]   produce.
[00:55:53.680 --> 00:55:54.680]   So check it out.
[00:55:54.680 --> 00:55:58.440]   I guess this doesn't have much to do with machine learning, but it's my podcast.
[00:55:58.440 --> 00:56:00.040]   I'm going to ask it anyway.
[00:56:00.040 --> 00:56:06.120]   If there was a transporter, like a Star Trek style transporter that would like disintegrate
[00:56:06.120 --> 00:56:12.400]   your body and rematerialize it somewhere else, does that seem like a safe thing to use?
[00:56:12.400 --> 00:56:16.160]   Would you get into that and use it to transport yourself?
[00:56:16.160 --> 00:56:19.700]   Oh, that's a good one.
[00:56:19.700 --> 00:56:21.080]   So here's the answer.
[00:56:21.080 --> 00:56:23.280]   I'd love to tell you, it's like, "Oh yeah, awesome.
[00:56:23.280 --> 00:56:24.280]   That would be amazing."
[00:56:24.280 --> 00:56:26.160]   But yeah, I would never get into that.
[00:56:26.160 --> 00:56:29.240]   Just like I would never jump out of a plane.
[00:56:29.240 --> 00:56:34.320]   Sometimes you learn things about yourself and they're just, the fact is that, yeah,
[00:56:34.320 --> 00:56:38.120]   I'm not going to jump out of a plane and I think I'm not going to be an early adopter
[00:56:38.120 --> 00:56:41.280]   of the Starship transporter.
[00:56:41.280 --> 00:56:42.280]   So yeah.
[00:56:42.280 --> 00:56:43.280]   Sorry, two answers.
[00:56:43.280 --> 00:56:47.840]   Would you be a late adopter?
[00:56:47.840 --> 00:56:50.880]   Like if everyone else was using it, do you think that would convince you?
[00:56:50.880 --> 00:56:51.880]   I think so.
[00:56:51.880 --> 00:56:52.880]   Yeah.
[00:56:52.880 --> 00:56:59.720]   I definitely think it would have so many advantages that I think it would be almost irresistible
[00:56:59.720 --> 00:57:04.160]   to not to use it.
[00:57:04.160 --> 00:57:10.360]   But yeah, I don't see myself as one of the first people jumping into the transporter,
[00:57:10.360 --> 00:57:19.160]   but certainly it would save us so much time and we could just get to London for a meeting
[00:57:19.160 --> 00:57:20.960]   like that and then come back, right?
[00:57:20.960 --> 00:57:22.960]   That would be super awesome.
[00:57:22.960 --> 00:57:26.160]   You wouldn't be concerned though that it's like a different person who shows up in London,
[00:57:26.160 --> 00:57:29.280]   even though they act like you and look like you?
[00:57:29.280 --> 00:57:30.280]   Yeah.
[00:57:30.280 --> 00:57:32.760]   Well, it's interesting.
[00:57:32.760 --> 00:57:37.720]   You know, I haven't, it's been a while since I've been, you know, I used to do a fair amount
[00:57:37.720 --> 00:57:44.040]   of quantum physics and I haven't done a lot, you know, very much lately in my current job.
[00:57:44.040 --> 00:57:52.160]   But as far as I can remember, you know, there's not really a bound on how your molecules get
[00:57:52.160 --> 00:57:53.600]   reassociated, right?
[00:57:53.600 --> 00:57:56.920]   And there's definitely entropy and energy loss in that process.
[00:57:56.920 --> 00:58:01.800]   So, you know, I think there are definitely some challenges for sure, you know, and we'll
[00:58:01.800 --> 00:58:05.760]   have to see, you know, well, that would definitely be a good test.
[00:58:05.760 --> 00:58:09.520]   Like maybe you have to, you know, pass some kind of a test when you get out of the transporter
[00:58:09.520 --> 00:58:11.000]   to see if you're still you.
[00:58:11.000 --> 00:58:12.000]   Interesting.
[00:58:12.000 --> 00:58:13.000]   All right.
[00:58:13.000 --> 00:58:14.000]   Okay.
[00:58:14.000 --> 00:58:17.280]   Here's another sort of fun one that we've been asking.
[00:58:17.280 --> 00:58:20.840]   Guess is, I mean, what do you think about the singularity?
[00:58:20.840 --> 00:58:27.040]   Like do you imagine a world where ML gets smarter than us in kind of every way and we
[00:58:27.040 --> 00:58:28.040]   just stop working?
[00:58:28.040 --> 00:58:31.160]   Does that seem likely or unlikely to you?
[00:58:31.160 --> 00:58:32.360]   Oh yeah.
[00:58:32.360 --> 00:58:34.320]   That's such a great question.
[00:58:34.320 --> 00:58:39.320]   I think my thoughts on this have been, you know, kind of formed by like the Minsky school
[00:58:39.320 --> 00:58:42.160]   of thought to some extent.
[00:58:42.160 --> 00:58:48.800]   And it's interesting that as we get into like a more connected society, right?
[00:58:48.800 --> 00:58:53.960]   Like you can kind of think of intelligence as these nodes that are connected, right?
[00:58:53.960 --> 00:58:57.400]   And it's information gets shared between the nodes.
[00:58:57.400 --> 00:59:01.760]   And I think like one thing that's sort of really interesting about that is that as we
[00:59:01.760 --> 00:59:06.480]   put more and more nodes into the network, you know, whether it be sort of Facebook or
[00:59:06.480 --> 00:59:11.400]   social media or a computer, as we add more and more nodes, you know, we're actually not
[00:59:11.400 --> 00:59:15.120]   seeing intelligence emerge out of that.
[00:59:15.120 --> 00:59:20.000]   In some sense, in some of the social media sites, we're seeing the opposite, right?
[00:59:20.000 --> 00:59:23.360]   Sort of like an anti-intelligence almost emerging.
[00:59:23.360 --> 00:59:28.560]   And it is sort of like a really interesting thing because I don't think anyone would have
[00:59:28.560 --> 00:59:32.480]   predicted that sort of 10 years ago that, you know, we'd be having these problems.
[00:59:32.480 --> 00:59:38.680]   All of a sudden when you, you know, put everybody in communication, we have all these problems
[00:59:38.680 --> 00:59:40.160]   that kind of popped out of that, right?
[00:59:40.160 --> 00:59:42.160]   I don't think anybody would have predicted that.
[00:59:42.160 --> 00:59:44.080]   Certainly I didn't.
[00:59:44.080 --> 00:59:51.080]   In terms of like, you know, a sentient computer that's going to take over everything, I think
[00:59:51.080 --> 00:59:52.600]   we're a ways away from that.
[00:59:52.600 --> 00:59:56.080]   The, you know, is it a possibility in the future?
[00:59:56.080 --> 00:59:57.640]   Yeah, absolutely.
[00:59:57.640 --> 00:59:58.960]   How close are we to that?
[00:59:58.960 --> 01:00:01.480]   You know, it's pretty tough to say.
[01:00:01.480 --> 01:00:07.880]   I know that with training, you know, computer vision models, these things are pretty far
[01:00:07.880 --> 01:00:10.320]   from like AGI, right?
[01:00:10.320 --> 01:00:17.880]   There's the teams that I work with and the folks that I interact with are under no illusions
[01:00:17.880 --> 01:00:24.480]   that this thing is anything other than a specially trained model to do a certain task, right?
[01:00:24.480 --> 01:00:28.200]   So they're nowhere close to what I would sort of describe as AGI.
[01:00:28.200 --> 01:00:34.360]   So I do think that we have a long way to go before we really have to worry about, you
[01:00:34.360 --> 01:00:38.800]   know, a Skynet sort of thing taking over the world.
[01:00:38.800 --> 01:00:41.640]   I don't think we're anywhere close to that by any means.
[01:00:41.840 --> 01:00:58.840]   [Music]

