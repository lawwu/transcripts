
[00:00:00.000 --> 00:00:04.400]   Speaking of the early years, it's really interesting that in 2009, you had a blog post
[00:00:04.400 --> 00:00:11.440]   where you say my modal expectation of when we get human level AI is 2025, expected value is 2028.
[00:00:11.440 --> 00:00:17.680]   And this is before deep learning. This is when nobody's talking about AI. And it turns out like
[00:00:17.680 --> 00:00:21.600]   if you, if the trends continue, this is not an unreasonable prediction. Before all these trends
[00:00:21.600 --> 00:00:25.680]   came into effect, how did you have that accurate an estimate? Well, first of all, it's not before
[00:00:25.680 --> 00:00:32.720]   deep learning. Deep learning was getting started around 2008. Oh, sorry. I meant to say before
[00:00:32.720 --> 00:00:43.120]   ImageNet. Before ImageNet, that was 2012. Yeah. So, well, I first formed those beliefs in about 2001
[00:00:43.120 --> 00:00:49.680]   after reading Ray Kurzweil's The Age of Spiritual Machines. And I came to the conclusion he was,
[00:00:51.200 --> 00:00:57.120]   there's two really important points that in his book that I came to believe is true. One is that
[00:00:57.120 --> 00:01:04.240]   computational power would grow exponentially for at least a few decades. And that the quantity of
[00:01:04.240 --> 00:01:10.640]   data in the world would grow exponentially for a few decades. And when you have exponentially
[00:01:10.640 --> 00:01:17.680]   increasing quantities of computation and data, then the value of highly scalable algorithms
[00:01:17.680 --> 00:01:22.960]   gets higher and higher. So, then there's a lot of incentive to make a more scalable algorithm to
[00:01:22.960 --> 00:01:28.800]   harness all this computing data. And so, I thought it would be very likely that we'll start to
[00:01:28.800 --> 00:01:34.560]   discover scalable algorithms to do this. And then there's a positive feedback between all these
[00:01:34.560 --> 00:01:39.200]   things, because if your algorithm gets better at harnessing computing data, then the value of the
[00:01:39.200 --> 00:01:43.600]   data in the compute goes up because it can be more effectively used. And so, that drives more
[00:01:43.600 --> 00:01:49.120]   investment to these areas. If your compute performance goes up, then the value of the data
[00:01:49.120 --> 00:01:52.880]   goes up because you can utilize more data. So, there are positive feedback loops between all
[00:01:52.880 --> 00:01:58.240]   these things. So, that was the first thing. And then the second thing was just looking at the
[00:01:58.240 --> 00:02:07.440]   trends. If these scalable algorithms were to be discovered, then during the 2020s, it should be
[00:02:07.440 --> 00:02:12.720]   possible to start training models on significantly more data than a human would experience in a
[00:02:12.720 --> 00:02:18.960]   lifetime. And I figured that that would be a time where big things would start to happen and that
[00:02:18.960 --> 00:02:24.720]   would eventually unlock AGI. So, that was my reasoning process. And I think we're now at that
[00:02:24.720 --> 00:02:29.520]   first part. I think we can start training models now where the scale of the data is beyond what a
[00:02:29.520 --> 00:02:35.200]   human can experience in a lifetime. So, I think this is the first unlocking step. And so, yeah,
[00:02:35.200 --> 00:02:41.360]   I think there's a 50% chance that somehow in 2028... Now, it's just a 50% chance. I mean,
[00:02:41.360 --> 00:02:45.040]   I'm sure what's going to happen is it's going to get to 2029 and someone's going to say,
[00:02:45.040 --> 00:02:50.960]   "Oh, Shane, you were wrong." It's like, "Come on, there's 50% chance." So, yeah, I think it's
[00:02:50.960 --> 00:02:58.080]   entirely plausible. Yeah, it's a 50% chance it could happen by 2028. But I'm not going to be
[00:02:58.080 --> 00:03:04.880]   surprised if it doesn't happen by then. Maybe, you know, you often hit unexpected problems in
[00:03:04.880 --> 00:03:08.480]   research and sciences and sometimes things take longer than you expect.

