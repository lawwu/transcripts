
[00:00:00.000 --> 00:00:05.800]   Hi, I'm Lucas, and you're listening to Gradient Dissent.
[00:00:05.800 --> 00:00:10.520]   We started this program because we're super passionate about making machine learning work
[00:00:10.520 --> 00:00:14.080]   in the real world by any means necessary.
[00:00:14.080 --> 00:00:18.840]   And one of the things that we discovered in the process of building machine learning tools
[00:00:18.840 --> 00:00:23.160]   is that our users and our customers, they have a lot of information in their heads that's
[00:00:23.160 --> 00:00:25.360]   not publicly available.
[00:00:25.360 --> 00:00:29.000]   And a lot of people that we talk to ask us what other people are talking about and what
[00:00:29.000 --> 00:00:31.680]   other people are doing, what are the best practices.
[00:00:31.680 --> 00:00:35.360]   And so we wanted to make all the interesting conversations that we're having and all the
[00:00:35.360 --> 00:00:38.920]   interesting things that we're learning available for everyone out there.
[00:00:38.920 --> 00:00:41.280]   So I hope you enjoy this.
[00:00:41.280 --> 00:00:47.160]   Today our guest is Rachel Tatman, who is a linguist and developer advocate for Rasa,
[00:00:47.160 --> 00:00:52.920]   who helps developers build and deploy conversational applications using an open source framework.
[00:00:52.920 --> 00:00:57.920]   Before that, she was a data scientist at Kaggle, where she was also a Kaggle Grandmaster.
[00:00:57.920 --> 00:01:02.240]   She also did really interesting work at the University of Washington as part of her PhD
[00:01:02.240 --> 00:01:03.240]   in linguistics.
[00:01:03.240 --> 00:01:04.240]   Thank you for having me.
[00:01:04.240 --> 00:01:05.240]   My pleasure.
[00:01:05.240 --> 00:01:08.360]   The place where I was hoping to start, if it's all right, is kind of your experience
[00:01:08.360 --> 00:01:13.860]   at Kaggle, just because I think that's such an interesting website with so many interesting
[00:01:13.860 --> 00:01:15.320]   learnings about machine learning.
[00:01:15.320 --> 00:01:20.200]   I feel like a lot of new stuff, it kind of happens on Kaggle first sometimes, and the
[00:01:20.200 --> 00:01:23.280]   insights that folks have are so interesting.
[00:01:23.280 --> 00:01:26.500]   Could you just maybe tell us a little bit about what your experience at Kaggle was like
[00:01:26.500 --> 00:01:30.040]   and kind of what you learned working there?
[00:01:30.040 --> 00:01:36.200]   So I was at Kaggle for two and a half years, and I think most people who are familiar with
[00:01:36.200 --> 00:01:39.640]   Kaggle are familiar with the competitions, which are generally supervised machine learning
[00:01:39.640 --> 00:01:43.360]   competitions where everyone's working on the same problem with the same dataset.
[00:01:43.360 --> 00:01:49.400]   And I never actually supported competitions directly, so I worked on the dataset hosting
[00:01:49.400 --> 00:01:56.000]   platform, and I worked on the hosted Jupyter notebook environment that Kaggle develops,
[00:01:56.000 --> 00:01:59.200]   which is called notebooks, Kaggle notebooks at this point.
[00:01:59.200 --> 00:02:04.320]   It was called kernels for a while, because you could also have scripts, and also the
[00:02:04.320 --> 00:02:10.320]   forums for developers to talk with each other, and the learn content I also worked on a bit
[00:02:10.320 --> 00:02:11.320]   as well.
[00:02:11.320 --> 00:02:14.480]   So learn is Kaggle's sort of machine learning courses.
[00:02:14.480 --> 00:02:19.040]   They're becoming more sort of structured and fully featured over time.
[00:02:19.040 --> 00:02:20.760]   So I worked on all those parts of the website.
[00:02:20.760 --> 00:02:26.960]   I worked a lot with the community, developing educational content, making product recommendations.
[00:02:26.960 --> 00:02:31.320]   So one of the things that I'm most proud of is I mentioned we had scripts that were sort
[00:02:31.320 --> 00:02:37.200]   of flat hosted Python or R files or R markdown, and we also had notebooks, but for a while,
[00:02:37.200 --> 00:02:43.040]   if you had a module you're working on a script, there was no way to use that module in a notebook.
[00:02:43.040 --> 00:02:47.360]   So I worked with the engineering team to sort of spec out what it would look like to have
[00:02:47.360 --> 00:02:50.520]   importable scripts, and now you can do that.
[00:02:50.520 --> 00:02:53.520]   So it was built out and was, I think, pretty successful.
[00:02:53.520 --> 00:02:56.880]   So was Kaggle your first kind of industry job coming out of a PhD?
[00:02:56.880 --> 00:02:58.560]   It was.
[00:02:58.560 --> 00:03:04.360]   So I started right after I graduated, and I was actually at the time I was still applying
[00:03:04.360 --> 00:03:06.660]   for faculty positions as well.
[00:03:06.660 --> 00:03:10.440]   So I was in this sort of limbo where I was working at Kaggle, and I was also in the faculty
[00:03:10.440 --> 00:03:16.360]   job market for academic positions, and I found that I really enjoyed working in industry,
[00:03:16.360 --> 00:03:20.960]   and the things that I would have liked about faculty positions, the teaching and helping
[00:03:20.960 --> 00:03:26.000]   people build cool things, my preference would have been more languagey things, which is
[00:03:26.000 --> 00:03:27.960]   why I'm at Roslyn now.
[00:03:27.960 --> 00:03:34.400]   I had a lot more reach and impact at Kaggle than I would have had in a university setting.
[00:03:34.400 --> 00:03:36.340]   So I found that really appealing.
[00:03:36.340 --> 00:03:40.120]   Were there any kind of things that you had to get used to, like kind of going into an
[00:03:40.120 --> 00:03:41.120]   industry job out of academia?
[00:03:41.120 --> 00:03:44.640]   Like, I feel like that's a hard adjustment for some people, but it sounded like you really
[00:03:44.640 --> 00:03:45.640]   enjoyed it.
[00:03:45.640 --> 00:03:46.640]   I did.
[00:03:46.640 --> 00:03:51.080]   I think one of the biggest changes for me, I know a lot of people talk about sort of
[00:03:51.080 --> 00:03:54.400]   the pace of work, that the pace of academia is much, much slower.
[00:03:54.400 --> 00:03:58.560]   And if you've ever been in industry or academia trying to collaborate across those fields,
[00:03:58.560 --> 00:04:00.800]   it can be a little challenging.
[00:04:00.800 --> 00:04:06.840]   But for me, I think a bigger change was that the North Star of what success looked like
[00:04:06.840 --> 00:04:10.060]   would really change very dramatically.
[00:04:10.060 --> 00:04:15.440]   So Kaggle was a startup that was acquired by Google, and it had still sort of a startup
[00:04:15.440 --> 00:04:17.740]   mentality of iterating fairly quickly.
[00:04:17.740 --> 00:04:22.820]   So when I first started, my focus was really on trying to increase the number of datasets
[00:04:22.820 --> 00:04:23.820]   on the dataset platform.
[00:04:23.820 --> 00:04:26.780]   And we found eventually that was sort of happening organically.
[00:04:26.780 --> 00:04:31.900]   And then my focus changed to helping people write notebooks, Kaggle notebooks, using the
[00:04:31.900 --> 00:04:36.380]   hosted platform, and then to just sort of growing the community and helping them grow
[00:04:36.380 --> 00:04:38.060]   their skills.
[00:04:38.060 --> 00:04:42.260]   Because in academia, you know you need to publish top tier conferences and journals,
[00:04:42.260 --> 00:04:45.220]   I guess show up for the class you're teaching, they would prefer that.
[00:04:45.220 --> 00:04:50.140]   It's not, the effort isn't really rewarded for most most tenure tech jobs.
[00:04:50.140 --> 00:04:53.500]   And you know that in order to continue to advance, you need to have the highest number
[00:04:53.500 --> 00:04:56.120]   of high quality publications possible, basically.
[00:04:56.120 --> 00:05:01.300]   So that North Star of what success will look like for you in academia is very, very static.
[00:05:01.300 --> 00:05:05.500]   And in industry, at least in my experience, it has been much more variable.
[00:05:05.500 --> 00:05:06.500]   I wouldn't call it a North Star.
[00:05:06.500 --> 00:05:08.500]   I'd call it like a comet.
[00:05:08.500 --> 00:05:11.500]   In that it's sort of moving and shifting.
[00:05:11.500 --> 00:05:14.500]   A planet, a planet, it's a planet you're trying to follow.
[00:05:14.500 --> 00:05:15.500]   Your North Planet.
[00:05:15.500 --> 00:05:18.820]   So I guess was that like a frustrating thing?
[00:05:18.820 --> 00:05:21.260]   Or were there good parts of that?
[00:05:21.260 --> 00:05:23.460]   Yeah, I was gonna say I like change.
[00:05:23.460 --> 00:05:26.540]   I don't think that that's like categorically true.
[00:05:26.540 --> 00:05:30.700]   But I enjoyed the challenge of things changing relatively quickly.
[00:05:30.700 --> 00:05:34.340]   And it made things feel very fresh and attainable to me.
[00:05:34.340 --> 00:05:36.300]   And also the goalposts are much, much closer.
[00:05:36.300 --> 00:05:40.460]   So you know, you can be working on a single research paper for years and years and years
[00:05:40.460 --> 00:05:41.460]   and years.
[00:05:41.460 --> 00:05:47.260]   Whereas a project that's a long term project in an industry setting would be like a quarter.
[00:05:47.260 --> 00:05:49.580]   That would be like a big ask.
[00:05:49.580 --> 00:05:52.180]   At least again, in my experience, it might be different in different companies.
[00:05:52.180 --> 00:05:54.420]   But that's what I've discovered.
[00:05:54.420 --> 00:05:56.220]   What were the kind of goals of Kaggle?
[00:05:56.220 --> 00:05:58.860]   Is it to sort of increase the engagement of the users then?
[00:05:58.860 --> 00:06:03.020]   Like you want more data sets, but like, what do you think like are sort of the big goals
[00:06:03.020 --> 00:06:04.020]   there?
[00:06:04.020 --> 00:06:06.860]   I mean, Kaggle really helps a lot of people kind of get into machine learning.
[00:06:06.860 --> 00:06:09.740]   And they've made, I mean, so many kind of open data sets.
[00:06:09.740 --> 00:06:12.420]   The kernel stuff is so cool with the collaboration.
[00:06:12.420 --> 00:06:13.780]   Like how do you think about that?
[00:06:13.780 --> 00:06:16.340]   Or what was the what were the big kind of goals, I guess?
[00:06:16.340 --> 00:06:23.180]   Yeah, I think the big goal of Kaggle is to really help all data scientists with their
[00:06:23.180 --> 00:06:24.180]   work.
[00:06:24.180 --> 00:06:26.060]   And I mean, I don't, I'm not the company anymore.
[00:06:26.060 --> 00:06:28.820]   So I don't really know the new term goals at the moment.
[00:06:28.820 --> 00:06:36.740]   But sort of all of the different things that Kaggle is doing are in service of that, you
[00:06:36.740 --> 00:06:40.460]   know, higher goal to help people basically get better at their job and then do their
[00:06:40.460 --> 00:06:42.300]   job successfully.
[00:06:42.300 --> 00:06:44.980]   So that definitely includes like, you know, people who are brand new to the field sort
[00:06:44.980 --> 00:06:50.900]   of sort of starting to try it out and get their first steps and also people who are
[00:06:50.900 --> 00:06:54.700]   really advanced in the field and want a challenge.
[00:06:54.700 --> 00:06:59.380]   Because certainly, as most professional data scientists, machine learning engineers know,
[00:06:59.380 --> 00:07:01.660]   you don't spend most of your time building model.
[00:07:01.660 --> 00:07:05.780]   That's a fairly small slice of the of the data science workflow.
[00:07:05.780 --> 00:07:09.100]   But I think it is for many people what drew them to data science and machine learning
[00:07:09.100 --> 00:07:10.100]   in the first place.
[00:07:10.100 --> 00:07:13.940]   So having a place where you can go and just like, you know, just have the part of the
[00:07:13.940 --> 00:07:19.660]   task that you really like doing in a very challenging way, I think is really appealing
[00:07:19.660 --> 00:07:20.660]   to people.
[00:07:20.660 --> 00:07:26.620]   So practically, XGBoost or some sort of radio boosted model will work for most things.
[00:07:26.620 --> 00:07:30.540]   It's fast, it's cheap to run, like, probably that's what you're going to be doing day to
[00:07:30.540 --> 00:07:32.160]   day.
[00:07:32.160 --> 00:07:34.820]   And you don't really need to, you know, get much fancier.
[00:07:34.820 --> 00:07:38.460]   So having a place to let go and cut loose is nice as well.
[00:07:38.460 --> 00:07:39.460]   Cool.
[00:07:39.460 --> 00:07:41.180]   You've been at Rasa for about a month, you said?
[00:07:41.180 --> 00:07:43.580]   Yeah, I guess coming up on two months.
[00:07:43.580 --> 00:07:44.580]   Yeah, cool.
[00:07:44.580 --> 00:07:46.660]   And so what is Rasa?
[00:07:46.660 --> 00:07:54.620]   So Rasa is a startup that has an open source conversational AI framework.
[00:07:54.620 --> 00:08:02.580]   So basically to take in text in a conversation, figure out the information that is in that
[00:08:02.580 --> 00:08:07.060]   text, decide what to do next, and then take that action, whether that's a turn, whether
[00:08:07.060 --> 00:08:10.300]   that's running some code.
[00:08:10.300 --> 00:08:15.380]   And then on top of that open source framework, there is a free platform called RasaX.
[00:08:15.380 --> 00:08:22.500]   And RasaX lets you deploy models, have people test your models, annotate data and sort of
[00:08:22.500 --> 00:08:23.500]   fold them back in.
[00:08:23.500 --> 00:08:27.620]   So you have a little bit more of a human in the loop learning process where you're iterating.
[00:08:27.620 --> 00:08:31.780]   And then if you are a business that wanted to use these tools, we have Rasa Enterprise,
[00:08:31.780 --> 00:08:34.660]   which has lots of additional features.
[00:08:34.660 --> 00:08:36.900]   And this is focused around kind of conversational?
[00:08:36.900 --> 00:08:37.900]   Yes.
[00:08:37.900 --> 00:08:42.260]   So chatbots, virtual assistants, anything where you would be interacting with an automated
[00:08:42.260 --> 00:08:47.500]   system through a text conversation or voice conversation rather than through a GUI or
[00:08:47.500 --> 00:08:49.140]   a command line.
[00:08:49.140 --> 00:08:51.420]   What makes you excited about conversation?
[00:08:51.420 --> 00:08:53.920]   Like what's the sort of the promise of conversational AI?
[00:08:53.920 --> 00:08:57.580]   So I think we've all had probably bad experiences with chatbots in the past.
[00:08:57.580 --> 00:09:01.500]   There was definitely a period in the last couple of years when people were very excited
[00:09:01.500 --> 00:09:03.100]   to try the technology.
[00:09:03.100 --> 00:09:07.520]   And I think the sort of industry-wide design expertise wasn't there yet.
[00:09:07.520 --> 00:09:10.100]   So there were lots of, I don't know, I had frustrating experiences.
[00:09:10.100 --> 00:09:16.220]   I think there's a study that like 54% of people have had just like a bad chatbot experience.
[00:09:16.220 --> 00:09:23.860]   But as design has really matured, I think it opens up being able to do computational
[00:09:23.860 --> 00:09:27.300]   tasks to a much wider variety of people.
[00:09:27.300 --> 00:09:29.740]   So tasks, sorry, what are computational tasks?
[00:09:29.740 --> 00:09:32.380]   So anything where you need to use a computer.
[00:09:32.380 --> 00:09:33.660]   Oh, I see.
[00:09:33.660 --> 00:09:41.620]   So as an example, people who aren't literate have sort of limited ability to use GUIs and
[00:09:41.620 --> 00:09:46.820]   sort of have to memorize where things are, but probably have had conversations in the
[00:09:46.820 --> 00:09:54.060]   past and can, especially with voice technology, really interact very naturally with whatever
[00:09:54.060 --> 00:09:57.340]   computational system they're interacting with.
[00:09:57.340 --> 00:10:02.900]   And even just thinking of computer literacy, using a mouse is not, I mean, if you're a
[00:10:02.900 --> 00:10:05.380]   computer literate, it's second nature to you, but it's a learned skill.
[00:10:05.380 --> 00:10:07.300]   It takes a while to acquire.
[00:10:07.300 --> 00:10:13.900]   So being able to provide services and open up access to people with a variety of different
[00:10:13.900 --> 00:10:19.820]   abilities and backgrounds, I think is to me, the most appealing part of conversational
[00:10:19.820 --> 00:10:21.180]   AI.
[00:10:21.180 --> 00:10:25.980]   But also it comes with a second challenge that people who are using conversational AI
[00:10:25.980 --> 00:10:30.620]   come in knowing how to conversations work and will always judge a conversational system
[00:10:30.620 --> 00:10:35.620]   against a human because this conversation is high quality.
[00:10:35.620 --> 00:10:38.100]   If I were having this with a bot, my mind would be blown.
[00:10:38.100 --> 00:10:41.700]   I don't think that's necessarily where we're at quite yet.
[00:10:41.700 --> 00:10:48.900]   So being able to achieve that really fluent level of conversational interaction is a really
[00:10:48.900 --> 00:10:53.340]   large engineering challenge and a really large machine learning challenge.
[00:10:53.340 --> 00:10:55.460]   And I am really excited to be working on it.
[00:10:55.460 --> 00:11:02.500]   Do you have like an example today of a kind of conversational thing that you can actually
[00:11:02.500 --> 00:11:05.620]   interact with that's like a really good experience that sort of like you would point to as like
[00:11:05.620 --> 00:11:09.180]   that's like how things should go?
[00:11:09.180 --> 00:11:14.940]   I think the most recent one that I had that was really fantastic was, actually I don't
[00:11:14.940 --> 00:11:19.620]   think it's publicly available, but it was for booking time off.
[00:11:19.620 --> 00:11:23.020]   So I knew the day that I went into my vacation, I knew that if I was going to go through the
[00:11:23.020 --> 00:11:26.460]   website, I'd have to do like 80 different things, I wasn't really sure how the process
[00:11:26.460 --> 00:11:27.460]   works.
[00:11:27.460 --> 00:11:28.460]   I'd never done it before.
[00:11:28.460 --> 00:11:31.500]   And a coworker of mine was like, hey, use this bot.
[00:11:31.500 --> 00:11:34.380]   And it was a really fantastic experience.
[00:11:34.380 --> 00:11:35.820]   It was really well designed.
[00:11:35.820 --> 00:11:43.740]   So in turns where there were very few possible options, instead of having me just like generate
[00:11:43.740 --> 00:11:46.020]   text, it had buttons.
[00:11:46.020 --> 00:11:50.060]   So using buttons in that conversational flow made it feel much faster.
[00:11:50.060 --> 00:11:55.540]   And the whole process was, there was sort of like a variety of different things that
[00:11:55.540 --> 00:11:57.340]   needed to happen.
[00:11:57.340 --> 00:11:59.460]   It kept track of sort of the things that I'd said before.
[00:11:59.460 --> 00:12:02.920]   So like the dates and the things that I needed.
[00:12:02.920 --> 00:12:05.900]   And at the end, it took maybe two minutes to do it.
[00:12:05.900 --> 00:12:07.900]   Otherwise, it would have taken me a good half hour.
[00:12:07.900 --> 00:12:13.700]   So in general, I think a good conversational interaction is one where it is faster to do
[00:12:13.700 --> 00:12:16.340]   the thing that you need to do than it would be otherwise.
[00:12:16.340 --> 00:12:19.540]   And you think like things are getting to that point now where it actually is fast.
[00:12:19.540 --> 00:12:22.940]   It sounds like your experience was that it was a lot faster to go through the conversational
[00:12:22.940 --> 00:12:23.940]   interface.
[00:12:23.940 --> 00:12:24.940]   Yeah, yeah.
[00:12:24.940 --> 00:12:29.660]   I think, again, it's a young field.
[00:12:29.660 --> 00:12:35.220]   We're definitely, we as conversational AI people are definitely learning what makes
[00:12:35.220 --> 00:12:37.260]   these systems work very well and be very delightful.
[00:12:37.260 --> 00:12:39.260]   But yeah, I think we're getting there.
[00:12:39.260 --> 00:12:43.180]   I guess I have this feeling that like NLP has made some huge improvements in the last
[00:12:43.180 --> 00:12:44.660]   year or two.
[00:12:44.660 --> 00:12:49.380]   Is that sort of like, are these things like already sort of deployed in conversational
[00:12:49.380 --> 00:12:53.860]   agents or is there kind of more work to do to make them actually like production ready?
[00:12:53.860 --> 00:12:55.100]   Yeah.
[00:12:55.100 --> 00:13:00.300]   So we at Rasa have recently added an option for BERT embeddings, which I think is probably
[00:13:00.300 --> 00:13:02.060]   one of the things you're thinking of.
[00:13:02.060 --> 00:13:05.100]   But definitely transformer architectures are there.
[00:13:05.100 --> 00:13:06.100]   We use them.
[00:13:06.100 --> 00:13:07.100]   And of course, we're open source.
[00:13:07.100 --> 00:13:09.220]   So if you wanted to use something else, you'd be welcome to.
[00:13:09.220 --> 00:13:13.740]   But we use contextual embeddings, specifically conversationally trained contextual embeddings.
[00:13:13.740 --> 00:13:16.900]   So if you wanted to use BERT instead, you would definitely be welcome to.
[00:13:16.900 --> 00:13:19.580]   And that paper was last year, two years ago.
[00:13:19.580 --> 00:13:21.340]   So fairly recent.
[00:13:21.340 --> 00:13:25.040]   A lot of the more recent work that I think has been a little bit more headline grabby
[00:13:25.040 --> 00:13:27.900]   has been around natural language generation.
[00:13:27.900 --> 00:13:34.140]   So the GPT-2 stuff, MENA, which is a Google project that came out relatively recently.
[00:13:34.140 --> 00:13:40.460]   And I think natural language generation is much trickier to get right.
[00:13:40.460 --> 00:13:46.140]   So the default setup, certainly for Rasa, is that you have a limited number of utterances
[00:13:46.140 --> 00:13:47.140]   that your bot can say.
[00:13:47.140 --> 00:13:48.140]   You might have some slot fillings.
[00:13:48.140 --> 00:13:50.500]   You might say, hello, Lucas.
[00:13:50.500 --> 00:13:52.180]   I see that you recently went to Vienna.
[00:13:52.180 --> 00:13:54.740]   I don't know if you've ever been to Vienna.
[00:13:54.740 --> 00:13:55.940]   Do you want to rate your hotel?
[00:13:55.940 --> 00:13:59.660]   Or whatever it is that your interaction is.
[00:13:59.660 --> 00:14:05.060]   The tricky thing about a lot of the natural language generation is that it sounds very
[00:14:05.060 --> 00:14:06.060]   fluent.
[00:14:06.060 --> 00:14:10.100]   It sounds like something a human conceivably could say, which is very exciting.
[00:14:10.100 --> 00:14:11.100]   No small feat.
[00:14:11.100 --> 00:14:12.820]   But it's not grounded.
[00:14:12.820 --> 00:14:17.780]   So like the GPT-2 text examples, if you look through them, there is one where, like, oh,
[00:14:17.780 --> 00:14:19.820]   these scientists discovered unicorns.
[00:14:19.820 --> 00:14:21.700]   Scientists have not discovered unicorns.
[00:14:21.700 --> 00:14:27.900]   It doesn't have ties to any sort of knowledge base that is the ground truth that the text
[00:14:27.900 --> 00:14:29.940]   is being generated around.
[00:14:29.940 --> 00:14:36.180]   And I think my worry is that, especially people without a deeper understanding of NLP, will
[00:14:36.180 --> 00:14:40.580]   see these very fluent text productions and be like, oh, I don't have to build a bot.
[00:14:40.580 --> 00:14:45.060]   I can just sort of like pipe the user input into this predictor that will come up with
[00:14:45.060 --> 00:14:47.300]   some sort of text that I should say back.
[00:14:47.300 --> 00:14:49.380]   And there's nothing to stop it from being completely infactual.
[00:14:49.380 --> 00:14:53.220]   Yes, of course we'll give you a full refund on your house.
[00:14:53.220 --> 00:14:58.100]   There's nothing to stop it from being abusive.
[00:14:58.100 --> 00:15:01.660]   Most of the large language models, there are certain sort of adversarial texts that you
[00:15:01.660 --> 00:15:06.660]   can use, like small text strings, like 10 to 12 characters, that will cause it to produce
[00:15:06.660 --> 00:15:09.940]   really vile, abusive output.
[00:15:09.940 --> 00:15:13.340]   And that's obviously not something you want to be showing to people.
[00:15:13.340 --> 00:15:14.340]   Hopefully, obviously.
[00:15:14.340 --> 00:15:17.460]   So let me put it this way.
[00:15:17.460 --> 00:15:22.980]   I would not be comfortable doing a completely neural, natural language generation conversational
[00:15:22.980 --> 00:15:23.980]   assistant.
[00:15:23.980 --> 00:15:26.780]   I definitely would want to have more control over utterances.
[00:15:26.780 --> 00:15:30.220]   So the way Raza works is it figures out an intention.
[00:15:30.220 --> 00:15:31.220]   Is that right?
[00:15:31.220 --> 00:15:33.940]   And then it sort of fills in sort of like slots.
[00:15:33.940 --> 00:15:35.420]   Is that right?
[00:15:35.420 --> 00:15:36.420]   Yeah.
[00:15:36.420 --> 00:15:43.700]   So that's sort of the current approach is to do entity extraction and then intention
[00:15:43.700 --> 00:15:44.700]   identification.
[00:15:44.700 --> 00:15:48.940]   So intents, where you have a set number of intents that you've provided training data
[00:15:48.940 --> 00:15:51.020]   for.
[00:15:51.020 --> 00:15:57.780]   Going forward, we're combining intents and entities together.
[00:15:57.780 --> 00:16:01.460]   So instead of having intents, extraction is a single part of the pipeline, making it a
[00:16:01.460 --> 00:16:08.620]   little bit more tied in to the rest of the NLU that's going on, which is a research that
[00:16:08.620 --> 00:16:09.620]   we're working on.
[00:16:09.620 --> 00:16:15.740]   Can you talk about kind of what you think is like hard about making a chatbot work?
[00:16:15.740 --> 00:16:21.220]   Like what are the kind of core technical challenges to make one and deploy it?
[00:16:21.220 --> 00:16:22.220]   Yeah.
[00:16:22.220 --> 00:16:25.340]   I mean, the first hurdle is the first hurdle you'll get with any machine learning project,
[00:16:25.340 --> 00:16:27.580]   which is getting the data.
[00:16:27.580 --> 00:16:35.340]   There's a sort of an older school approach, which is to build a state machine where you
[00:16:35.340 --> 00:16:38.340]   have like, okay, the person said, hi, we're going to say hi.
[00:16:38.340 --> 00:16:43.100]   The person wants to know how to rent a car or the person wants to know how to find the
[00:16:43.100 --> 00:16:44.100]   dealerships.
[00:16:44.100 --> 00:16:45.100]   We're going to go down that path.
[00:16:45.100 --> 00:16:46.100]   Okay.
[00:16:46.100 --> 00:16:47.100]   They wanted a car.
[00:16:47.100 --> 00:16:48.100]   What type of car did they want?
[00:16:48.100 --> 00:16:52.820]   So that's sort of like a decision tree, but for a dialogue agent.
[00:16:52.820 --> 00:16:57.140]   And one of the big challenges is if people are like, okay, I want a car.
[00:16:57.140 --> 00:16:59.580]   Where's the dealership?
[00:16:59.580 --> 00:17:07.180]   Being able to recover from someone sort of interleaving other types of intents into your
[00:17:07.180 --> 00:17:13.260]   happy path that you've constructed is fairly challenging for that sort of state machine
[00:17:13.260 --> 00:17:15.340]   based approach.
[00:17:15.340 --> 00:17:21.060]   So one thing that we've done at Rasa is we have an intention model, attention with an
[00:17:21.060 --> 00:17:22.060]   A, sorry, not I.
[00:17:22.060 --> 00:17:23.900]   An attention model.
[00:17:23.900 --> 00:17:32.860]   So instead of having a just like a straight through the tree, you'll start with sample
[00:17:32.860 --> 00:17:33.860]   stories.
[00:17:33.860 --> 00:17:36.900]   So dialogues that someone might have.
[00:17:36.900 --> 00:17:39.660]   And then when it comes to turn, pick the next turn.
[00:17:39.660 --> 00:17:45.820]   If you have an exact example of the specific conversational flow that you've seen before,
[00:17:45.820 --> 00:17:48.460]   you're just going to continue on that flow because you've seen it before.
[00:17:48.460 --> 00:17:50.680]   You're pretty sure it's right.
[00:17:50.680 --> 00:17:56.460]   If you aren't sure what the intention or the entities or any of the required information
[00:17:56.460 --> 00:18:00.020]   is, then you'll have a fallback where you're like, could you rephrase that?
[00:18:00.020 --> 00:18:05.060]   Or I'm not entirely sure what you're asking for, but here's the next two closest results
[00:18:05.060 --> 00:18:06.860]   or that sort of thing.
[00:18:06.860 --> 00:18:13.060]   And then also a machine learning based policy that ranks the possible responses and says,
[00:18:13.060 --> 00:18:16.260]   okay, I think this one's probably the correct next one.
[00:18:16.260 --> 00:18:17.700]   And then those are all considered.
[00:18:17.700 --> 00:18:21.880]   And if there's one that's highly likely, then that's the one that you go with.
[00:18:21.880 --> 00:18:24.200]   And if there isn't, you go back to the fallback policy.
[00:18:24.200 --> 00:18:29.640]   So it can handle these sort of interleaved aside type structures in conversations in
[00:18:29.640 --> 00:18:33.560]   a way that sort of a more rigid state machine cannot.
[00:18:33.560 --> 00:18:37.800]   Wait, so let me, can I repeat this back to you and see if I understood it?
[00:18:37.800 --> 00:18:42.640]   So it sounds like the sort of first thing is just like, it's like kind of like a, essentially
[00:18:42.640 --> 00:18:45.280]   like a state machine kind of rule based system.
[00:18:45.280 --> 00:18:46.280]   Is that?
[00:18:46.280 --> 00:18:50.060]   So yes, that's, that's not what underlies Rasa, but that's a very common approach to
[00:18:50.060 --> 00:18:51.580]   building conversational systems.
[00:18:51.580 --> 00:18:52.580]   That's right.
[00:18:52.580 --> 00:18:55.700]   So the first, what's the first browser approach that you said?
[00:18:55.700 --> 00:19:02.020]   So we have a variety of policies that are all considered together.
[00:19:02.020 --> 00:19:05.020]   And then the one that has the highest confidence is usually the one that's selected.
[00:19:05.020 --> 00:19:06.380]   And what is a policy?
[00:19:06.380 --> 00:19:09.300]   Is that, would that be like a rule or would that be like a.
[00:19:09.300 --> 00:19:10.820]   So it's more probabilistic.
[00:19:10.820 --> 00:19:14.460]   It's selecting the, you can think of it as multiclass classification across all of the
[00:19:14.460 --> 00:19:15.460]   possible responses.
[00:19:15.460 --> 00:19:19.940]   And then it'll select the one that's most likely based on the training data.
[00:19:19.940 --> 00:19:20.940]   I see.
[00:19:20.940 --> 00:19:24.660]   And so the training data would be like, you know, like an utterance or, or like a, would
[00:19:24.660 --> 00:19:28.340]   it be like a conversation and the intent or.
[00:19:28.340 --> 00:19:30.700]   So two types of training data.
[00:19:30.700 --> 00:19:33.460]   One are examples, intents and an example entity.
[00:19:33.460 --> 00:19:37.540]   So things that the user would say and these, you might just sit there and come up with,
[00:19:37.540 --> 00:19:41.640]   you might collect them from maybe FAQs that you've gotten.
[00:19:41.640 --> 00:19:45.100]   So that's more on the NLU side.
[00:19:45.100 --> 00:19:49.300]   And then on the other side to determine the dialogue policy, what gets said next, you'll
[00:19:49.300 --> 00:19:51.620]   have examples of conversations.
[00:19:51.620 --> 00:19:54.100]   So you'll probably have the one that's like, okay, this is the ideal.
[00:19:54.100 --> 00:19:56.780]   The person that's high, they want to know what car to do.
[00:19:56.780 --> 00:20:00.020]   I'm going to like create the database and get the available cars and then tell them
[00:20:00.020 --> 00:20:02.300]   the available cars and all of that.
[00:20:02.300 --> 00:20:05.260]   And then you might have other terms like other possible stories.
[00:20:05.260 --> 00:20:06.540]   Like someone's like, Hey, are you a bot?
[00:20:06.540 --> 00:20:07.540]   And you're like, yeah, I'm a bot.
[00:20:07.540 --> 00:20:09.900]   And it's like, I want to talk to a human or whatever.
[00:20:09.900 --> 00:20:14.100]   And then it helps them out with the thing that they need.
[00:20:14.100 --> 00:20:19.900]   And then those stories and those example utterances together are used to train the model.
[00:20:19.900 --> 00:20:21.800]   You don't need that much.
[00:20:21.800 --> 00:20:25.460]   If you are using a language for which we have free trained embeddings, you don't need that
[00:20:25.460 --> 00:20:26.460]   much data to get started.
[00:20:26.460 --> 00:20:32.100]   And the idea is you build a minimally viable assistant, and then you deploy it and you
[00:20:32.100 --> 00:20:37.380]   have people make test conversations with it, make test conversations, have test conversations
[00:20:37.380 --> 00:20:38.380]   with it.
[00:20:38.380 --> 00:20:42.540]   You go back, you annotate those, you put those back into the training data, you retrain and
[00:20:42.540 --> 00:20:45.100]   you continue on in that way.
[00:20:45.100 --> 00:20:49.700]   So you could add additional stories, you could add new intents, you could add new utterances,
[00:20:49.700 --> 00:20:53.900]   you can sort of change your model to fit the actual conversations that you see.
[00:20:53.900 --> 00:20:54.900]   I see.
[00:20:54.900 --> 00:20:59.020]   How much training do you think you need before you get something reasonable?
[00:20:59.020 --> 00:21:04.940]   So for some of the examples that I've worked on, you'll probably need, you know, 10 to
[00:21:04.940 --> 00:21:08.820]   20 examples per intent, and then maybe three or four stories.
[00:21:08.820 --> 00:21:09.820]   I see.
[00:21:09.820 --> 00:21:14.660]   And because we're using pre-trained embeddings, so you know that like, I want a car and I
[00:21:14.660 --> 00:21:18.260]   want a vehicle are going to be similar because the embeddings for car and vehicle are similar.
[00:21:18.260 --> 00:21:23.940]   So you have the sort of the fuzziness of machine learning to help you out with that.
[00:21:23.940 --> 00:21:25.700]   Maybe like shifting gears a little bit.
[00:21:25.700 --> 00:21:28.660]   You know, I know that you write a lot about papers that you've read.
[00:21:28.660 --> 00:21:32.140]   And I think a really common question I get is kind of like, how do I kind of approach
[00:21:32.140 --> 00:21:33.140]   papers?
[00:21:33.140 --> 00:21:35.580]   How do I find like what papers to read?
[00:21:35.580 --> 00:21:38.900]   You know, how do I even like, you know, kind of go about like, like reading a paper?
[00:21:38.900 --> 00:21:43.020]   Do you I would think you'd have some smart advice on that.
[00:21:43.020 --> 00:21:45.780]   Yeah, so don't go to archive every day.
[00:21:45.780 --> 00:21:47.780]   You'll just make yourself upset.
[00:21:47.780 --> 00:21:53.180]   I don't try to stay on top of things right after they're written.
[00:21:53.180 --> 00:21:58.020]   So how I will come to know that there's a paper that I want to read is I, you know,
[00:21:58.020 --> 00:22:00.460]   I'm very active on Twitter, as you mentioned.
[00:22:00.460 --> 00:22:04.380]   So if a lot of people are talking about a specific paper, either as they like it, or
[00:22:04.380 --> 00:22:08.220]   they are not a fan of it, either way.
[00:22:08.220 --> 00:22:12.940]   I will once I have enough people that I trust be like, hey, this is an important paper for
[00:22:12.940 --> 00:22:13.940]   one reason or another.
[00:22:13.940 --> 00:22:16.420]   I'll set aside time to read it.
[00:22:16.420 --> 00:22:21.940]   And my usual approach to reading it, first of all, if it's a really seminal paper, like
[00:22:21.940 --> 00:22:27.780]   the transformer paper, oftentimes there will be a blog or a talk that someone has done,
[00:22:27.780 --> 00:22:31.380]   and you can read that instead of the paper and get the same information.
[00:22:31.380 --> 00:22:35.540]   If there isn't, I would start by reading the abstract, and then I like to read the introduction
[00:22:35.540 --> 00:22:36.540]   and then the conclusion.
[00:22:36.540 --> 00:22:40.940]   So I have like a good general idea of what's going on with the paper.
[00:22:40.940 --> 00:22:46.980]   And then after that, starting from the top in the related work section, or the literature
[00:22:46.980 --> 00:22:53.980]   review section, sometimes it's at the end, I wouldn't go and chase down all of those
[00:22:53.980 --> 00:22:56.500]   terms that might be new to you right away.
[00:22:56.500 --> 00:23:00.820]   So just sort of skim that section, go to the methods.
[00:23:00.820 --> 00:23:04.820]   And if you see terms there that are repeated that you saw previously, and they look like
[00:23:04.820 --> 00:23:07.900]   they're going to be used a lot in paper, then go and look those up if you're not familiar
[00:23:07.900 --> 00:23:09.420]   with them.
[00:23:09.420 --> 00:23:17.060]   And when you get to math, so when you get to an equation, my strategy is always to try
[00:23:17.060 --> 00:23:22.220]   and take that and put it into human words, like the way that I would say it.
[00:23:22.220 --> 00:23:24.740]   And in the process of doing that, I'm usually like, oh, I don't actually know what this
[00:23:24.740 --> 00:23:25.740]   term is.
[00:23:25.740 --> 00:23:29.100]   Can I figure out what this term is from other places in the text?
[00:23:29.100 --> 00:23:32.340]   And for me, that's the part that takes the longest in reading a paper, which I think
[00:23:32.340 --> 00:23:37.140]   is true for most people, unless it's very similar to a field you already work in and
[00:23:37.140 --> 00:23:40.220]   you're very familiar with sort of the bones of the equation.
[00:23:40.220 --> 00:23:44.380]   And then from there, continuing through, I always skim the results, because it's usually
[00:23:44.380 --> 00:23:45.700]   like, oh, look, here's our results.
[00:23:45.700 --> 00:23:46.700]   Here's other people's results.
[00:23:46.700 --> 00:23:47.700]   We got state of the art.
[00:23:47.700 --> 00:23:48.700]   Huzzah.
[00:23:48.700 --> 00:23:51.620]   Unless there's something very specific that you're interested in.
[00:23:51.620 --> 00:23:55.620]   And then I will pay more attention to the ablation results, if they have any ablation.
[00:23:55.620 --> 00:23:58.700]   So ablations are when you have a full model, and you start to take parts out of it, and
[00:23:58.700 --> 00:24:01.700]   you see what changes what.
[00:24:01.700 --> 00:24:05.220]   So I find those to be, particularly if you're a practitioner, if you're thinking, oh, maybe
[00:24:05.220 --> 00:24:08.140]   I can, maybe I want to implement this, but that's a lot of layers.
[00:24:08.140 --> 00:24:10.660]   Maybe I want a couple fewer layers.
[00:24:10.660 --> 00:24:14.060]   Figuring out what you can get rid of that may be practical in an academic setting, but
[00:24:14.060 --> 00:24:18.660]   not in a production setting is really helpful.
[00:24:18.660 --> 00:24:26.900]   If the paper is on OpenSoft, I know ICLR is, there's a subset of conferences where the
[00:24:26.900 --> 00:24:28.940]   reviews are made public.
[00:24:28.940 --> 00:24:31.900]   So it might be helpful for you to go back and also read the reviews of the paper.
[00:24:31.900 --> 00:24:34.580]   If it's, again, something you're like, do I want to put this into production?
[00:24:34.580 --> 00:24:36.980]   What are other people saying?
[00:24:36.980 --> 00:24:42.620]   So the more you care about the paper, the further down that list you will go.
[00:24:42.620 --> 00:24:45.940]   The more you're like, I just sort of kind of want to know what's going on, the nearer
[00:24:45.940 --> 00:24:47.820]   the top of the list, I will start.
[00:24:47.820 --> 00:24:48.820]   Gotcha.
[00:24:48.820 --> 00:24:51.740]   What's a paper that you've gone like pretty deep on recently?
[00:24:51.740 --> 00:24:58.060]   I am like probably 50% of the way through the list for the convert paper, which I mentioned
[00:24:58.060 --> 00:24:59.060]   earlier.
[00:24:59.060 --> 00:25:07.140]   So that is a paper that we have implemented into Rasa.
[00:25:07.140 --> 00:25:11.340]   And that is Henderson et al. 2019.
[00:25:11.340 --> 00:25:15.540]   It's a transformer embedding architecture specifically for conversational data, which
[00:25:15.540 --> 00:25:18.140]   is obviously very relevant to us.
[00:25:18.140 --> 00:25:19.140]   Cool.
[00:25:19.140 --> 00:25:25.020]   I was looking at your papers from grad school and I saw you had a bunch of papers on kind
[00:25:25.020 --> 00:25:28.100]   of like Twitter and things like that.
[00:25:28.100 --> 00:25:33.580]   And I remember being super interested in that earlier in my career.
[00:25:33.580 --> 00:25:37.460]   I'm kind of curious, do you have any like favorite paper, any favorite result that you
[00:25:37.460 --> 00:25:39.220]   wanted to talk about?
[00:25:39.220 --> 00:25:41.300]   Oh, from my work?
[00:25:41.300 --> 00:25:46.480]   So I think the result that I would most like to let people know about.
[00:25:46.480 --> 00:25:50.500]   So I got a lot of sort of traction with my paper that was looking at automatic speech
[00:25:50.500 --> 00:25:53.660]   recognition and accuracy across different demographic groups.
[00:25:53.660 --> 00:25:59.140]   And I had two papers, one at EACL and then one at Interspeech.
[00:25:59.140 --> 00:26:04.740]   EACL is an NLP conference, Interspeech is a speech conference.
[00:26:04.740 --> 00:26:09.940]   And the EACL paper was on gender and region.
[00:26:09.940 --> 00:26:12.580]   And I found differences for both of them.
[00:26:12.580 --> 00:26:16.100]   But that was using user uploaded YouTube videos.
[00:26:16.100 --> 00:26:22.620]   And again, I was guessing at gender in a not particularly, I would say ecologically valid
[00:26:22.620 --> 00:26:23.620]   way.
[00:26:23.620 --> 00:26:28.580]   So not the absolute best methodology.
[00:26:28.580 --> 00:26:37.420]   But when I repeated the experiment with additional APIs and using higher quality audio where
[00:26:37.420 --> 00:26:41.140]   the signal to noise ratio had been controlled, so basically recorded in a quiet environment
[00:26:41.140 --> 00:26:47.860]   with a high quality microphone, I found that the gender differences disappeared.
[00:26:47.860 --> 00:26:51.180]   And for this one, I did have self-reported gender from users.
[00:26:51.180 --> 00:26:56.100]   The demographic regional differences were still there.
[00:26:56.100 --> 00:26:59.660]   And this time I also had access to race and ethnicity data.
[00:26:59.660 --> 00:27:02.340]   And I found a really strong difference.
[00:27:02.340 --> 00:27:08.380]   So when signal to noise ratio is controlled, you don't have, or at least I didn't find
[00:27:08.380 --> 00:27:13.660]   that the gender difference obtained, but there was a really strong difference in accuracy
[00:27:13.660 --> 00:27:15.700]   for people of different geographic regions.
[00:27:15.700 --> 00:27:23.300]   So the sort of general American prestige, educated upper middle class dialect had the
[00:27:23.300 --> 00:27:24.620]   best recognition rates.
[00:27:24.620 --> 00:27:26.980]   Any other regional dialect had lower recognition rates.
[00:27:26.980 --> 00:27:27.980]   And then-
[00:27:27.980 --> 00:27:30.820]   Is that part of its own dialect?
[00:27:30.820 --> 00:27:37.140]   It's interesting because in England, there is like a very specific pronunciation set
[00:27:37.140 --> 00:27:41.300]   of rules that are considered the standard received pronunciation or RP.
[00:27:41.300 --> 00:27:43.580]   Anything else is considered a regional dialect.
[00:27:43.580 --> 00:27:49.500]   In the United States, you can have quite a bit of variation in pronunciation and still
[00:27:49.500 --> 00:27:52.420]   be considered a general American speaker.
[00:27:52.420 --> 00:27:58.700]   And it seems to be more around lexical items and grammatical features that make you sound
[00:27:58.700 --> 00:27:59.700]   not accented.
[00:27:59.700 --> 00:28:02.280]   Everybody has an accent.
[00:28:02.280 --> 00:28:04.340]   So I would say it's a variety.
[00:28:04.340 --> 00:28:08.100]   I would say it's less internally consistent of a dialect than a lot of other dialects,
[00:28:08.100 --> 00:28:10.700]   like California English, for example.
[00:28:10.700 --> 00:28:16.820]   It sounds like you basically found that the, I mean, I guess this makes sense that the
[00:28:16.820 --> 00:28:21.300]   quality got worse with any variation from kind of the standard, or what did you call
[00:28:21.300 --> 00:28:22.300]   it?
[00:28:22.300 --> 00:28:25.140]   The general American, standard American English.
[00:28:25.140 --> 00:28:27.380]   You'll hear those terms a lot.
[00:28:27.380 --> 00:28:32.020]   So sort of the variety where speakers are consciously avoiding using regional forms.
[00:28:32.020 --> 00:28:38.420]   So both in region, but also African American speakers had much higher error rates.
[00:28:38.420 --> 00:28:43.540]   And that's not due to African American English being any less internally consistent or easy
[00:28:43.540 --> 00:28:44.540]   to recognize.
[00:28:44.540 --> 00:28:47.580]   It's almost certainly due to imbalances in the training data.
[00:28:47.580 --> 00:28:50.140]   So your classic imbalance class problem.
[00:28:50.140 --> 00:28:51.140]   Right, right.
[00:28:51.140 --> 00:28:55.420]   And so did you have recommendations on how to deal with this?
[00:28:55.420 --> 00:28:56.900]   Yeah.
[00:28:56.900 --> 00:29:00.780]   So the gender thing is real.
[00:29:00.780 --> 00:29:04.500]   There is a gender difference, but it's more on the signal processing side and less on
[00:29:04.500 --> 00:29:08.020]   the automatic speech recognition, sort of machine learning modeling side.
[00:29:08.020 --> 00:29:10.540]   Wait, what do you mean on the signal processing side?
[00:29:10.540 --> 00:29:12.300]   Yeah, so a couple of things.
[00:29:12.300 --> 00:29:17.140]   So one is that women in general tend to be slightly smaller, tend to have slightly lower
[00:29:17.140 --> 00:29:20.080]   lung capacity, tend to be slightly quieter.
[00:29:20.080 --> 00:29:25.980]   So for an equal decibel level of noise, you'll tend to have a little bit less signal.
[00:29:25.980 --> 00:29:31.980]   So when we were developing sort of telephony and recording in general, the band that was
[00:29:31.980 --> 00:29:36.860]   picked to be sort of the target band for the frequency band that was picked to be the target
[00:29:36.860 --> 00:29:43.180]   band for all systems, basically, and that a lot of the speech recognition comes directly
[00:29:43.180 --> 00:29:49.860]   from Bell Labs and a lot of the telephone work in earlier days was picked to suit a
[00:29:49.860 --> 00:29:55.420]   male voice and not any of the other types of voices you might encounter.
[00:29:55.420 --> 00:30:01.980]   So children also tend to have really high error recognition rates.
[00:30:01.980 --> 00:30:05.460]   Partially that's due to children varying more as they're learning the language, but partially
[00:30:05.460 --> 00:30:09.500]   that's just due to their frequency range not being represented as well.
[00:30:09.500 --> 00:30:15.260]   But I guess downstream then, it actually, the error rate is higher?
[00:30:15.260 --> 00:30:22.460]   So definitely the regional and racial differences are due to things that you could fix with
[00:30:22.460 --> 00:30:27.340]   machine learning, whereas I think the other differences are not due to that.
[00:30:27.340 --> 00:30:29.180]   How were you able to pull that apart?
[00:30:29.180 --> 00:30:31.660]   I believe I had fairly balanced classes.
[00:30:31.660 --> 00:30:36.540]   Specifically on the modeling side, I used mixed effects models.
[00:30:36.540 --> 00:30:42.660]   So you can control for some features while identifying the effects of others.
[00:30:42.660 --> 00:30:47.460]   What do you think is an underrated aspect of machine learning that you think people
[00:30:47.460 --> 00:30:49.900]   should pay more attention to?
[00:30:49.900 --> 00:30:50.900]   Data visualization.
[00:30:50.900 --> 00:30:51.900]   Oh, cool.
[00:30:51.900 --> 00:31:00.620]   I've seen a lot of really excellent machine learning engineers who have a hard time communicating
[00:31:00.620 --> 00:31:05.020]   their results and models because their charts are just unreadable.
[00:31:05.020 --> 00:31:11.180]   Does anything come to mind where you saw a really good visualization or something you
[00:31:11.180 --> 00:31:12.180]   want to call out as excellent?
[00:31:12.180 --> 00:31:20.540]   So if you want to see some master classes in visualization, the pudding, which is actually
[00:31:20.540 --> 00:31:26.140]   a journalist thing, what would you call that?
[00:31:26.140 --> 00:31:27.140]   Data journalism.
[00:31:27.140 --> 00:31:34.060]   Yeah, so it's a data journalism web magazine, I guess, has really, really stunning visualizations
[00:31:34.060 --> 00:31:39.420]   that just sort of push the limit of the art.
[00:31:39.420 --> 00:31:41.220]   So data visualization.
[00:31:41.220 --> 00:31:46.500]   One of my biggest pet peeves is you should not use lines to connect points unless there's
[00:31:46.500 --> 00:31:48.100]   a logical reason to do it.
[00:31:48.100 --> 00:31:54.820]   Like it's a time series or like something could exist in the space between the two points.
[00:31:54.820 --> 00:31:55.820]   Don't do it for categories.
[00:31:55.820 --> 00:31:56.820]   It drives me nuts.
[00:31:56.820 --> 00:31:57.820]   Up the wall.
[00:31:57.820 --> 00:31:58.820]   Wild question.
[00:31:58.820 --> 00:32:01.940]   How do you feel about 3D visualizations?
[00:32:01.940 --> 00:32:04.980]   Are you in AR?
[00:32:04.980 --> 00:32:07.980]   Are you presenting them in a way that people can like walk around them?
[00:32:07.980 --> 00:32:09.660]   Like I'm not against them in general.
[00:32:09.660 --> 00:32:12.700]   I just find them harder to parse.
[00:32:12.700 --> 00:32:20.180]   I mean, there's a whole field of study that specifically looks at how humans process information
[00:32:20.180 --> 00:32:26.060]   and what is most useful for conveying different types of information visually.
[00:32:26.060 --> 00:32:30.180]   Please read any visualization papers, y'all.
[00:32:30.180 --> 00:32:34.020]   What do you think is the biggest challenge for making machine learning work in the real
[00:32:34.020 --> 00:32:35.540]   world right now?
[00:32:35.540 --> 00:32:41.620]   It's not really a technical challenge, but I think the biggest thing that trips people
[00:32:41.620 --> 00:32:47.260]   up is deciding to build things that don't need to or shouldn't exist.
[00:32:47.260 --> 00:32:50.740]   I understand it is a very exciting time to be in machine learning.
[00:32:50.740 --> 00:32:54.860]   We all want to work on fun projects and change the world.
[00:32:54.860 --> 00:32:59.900]   But particularly if you are building anything that would deal with a sensitive or a vulnerable
[00:32:59.900 --> 00:33:05.820]   community, I would highly encourage you to reach out to people from that community and
[00:33:05.820 --> 00:33:10.980]   work with them and make sure that it's something that does need to exist and that you are building
[00:33:10.980 --> 00:33:13.780]   it in such a way that it's actually useful.
[00:33:13.780 --> 00:33:18.300]   So an example that comes to mind is there have been a number of projects built by people
[00:33:18.300 --> 00:33:23.820]   who are not deaf and who are not signers to help deaf people communicate.
[00:33:23.820 --> 00:33:28.220]   And usually they take the form of gloves or computer vision to take sign language and
[00:33:28.220 --> 00:33:31.940]   turn it into a different language.
[00:33:31.940 --> 00:33:38.120]   That's not usually the problem in speaking deaf communication situations.
[00:33:38.120 --> 00:33:43.260]   Usually it's that the speaking person does not have a very good way of communicating
[00:33:43.260 --> 00:33:45.340]   their intent.
[00:33:45.340 --> 00:33:49.820]   In general, deaf people are masterful communicators and do not struggle with getting themselves
[00:33:49.820 --> 00:33:51.420]   understood.
[00:33:51.420 --> 00:33:55.780]   So that's just an example of I get it, I understand that it's an exciting project and you are
[00:33:55.780 --> 00:33:59.460]   very passionate about it, but before you spend a lot of time and money and resources building
[00:33:59.460 --> 00:34:03.300]   something, make sure people want it.
[00:34:03.300 --> 00:34:04.300]   Good advice.
[00:34:04.300 --> 00:34:06.300]   It makes sense to me.
[00:34:06.300 --> 00:34:07.300]   Cool.
[00:34:07.300 --> 00:34:10.020]   I think that might be a nice place to wrap up.
[00:34:10.020 --> 00:34:11.500]   Rachel, thanks so much for your time.
[00:34:11.500 --> 00:34:12.500]   I really appreciate it.
[00:34:12.500 --> 00:34:13.500]   Yeah.
[00:34:13.500 --> 00:34:14.500]   Thank you for having me.
[00:34:14.500 --> 00:34:15.500]   All right.
[00:34:15.500 --> 00:34:18.500]   That was such a great conversation.
[00:34:18.500 --> 00:34:20.020]   Thanks, Lucas and Rachel.
[00:34:20.020 --> 00:34:24.660]   I'm going to drop Rachel's Twitter account and some other links in the show notes below.
[00:34:24.660 --> 00:34:27.340]   I highly recommend that you guys check her out.
[00:34:27.340 --> 00:34:33.740]   She does these really cool live streams on NLP that are really worth watching.
[00:34:33.740 --> 00:34:38.300]   So before we go, I'd love to tell you guys about how Weights and Biases can help you
[00:34:38.300 --> 00:34:41.260]   get to the Kaggle leaderboard faster.
[00:34:41.260 --> 00:34:46.660]   So Weights and Biases is an experiment tracking and hyperparameter optimization platform.
[00:34:46.660 --> 00:34:52.020]   So what we let you do is track the performance of your models in real time.
[00:34:52.020 --> 00:34:57.340]   So you can try different experiments, try different model architectures, try different
[00:34:57.340 --> 00:35:02.460]   hyperparameter values and see how your models are doing in real time.
[00:35:02.460 --> 00:35:07.940]   We also let you log the outputs, the predictions of your models.
[00:35:07.940 --> 00:35:13.500]   So if you're working with images, videos, audios, or you can see here we're logging
[00:35:13.500 --> 00:35:20.220]   protein structures, you can actually see how your model is performing at every epoch and
[00:35:20.220 --> 00:35:23.620]   be able to debug it really easily.
[00:35:23.620 --> 00:35:29.180]   We also let you see how your model is using its resources.
[00:35:29.180 --> 00:35:33.180]   So you can see the GPU memory usage and all that kind of stuff.
[00:35:33.180 --> 00:35:39.020]   And you can compare, for instance, the effect of different batch sizes on GPU usage.
[00:35:39.020 --> 00:35:43.700]   You can also run hyperparameter sweeps very easily.
[00:35:43.700 --> 00:35:49.760]   So for example, you can pass a dictionary with a range of different hyperparameter values
[00:35:49.760 --> 00:35:51.740]   that you'd like us to try.
[00:35:51.740 --> 00:35:58.180]   And we'll automatically run all of those different models for you and show you which of those
[00:35:58.180 --> 00:36:00.420]   hyperparameter values did the best.
[00:36:00.420 --> 00:36:07.100]   We also, with this hyperparameter importance plot, we show you which of the hyperparameters
[00:36:07.100 --> 00:36:12.500]   were the most important and how their values were correlated to the metric that you care
[00:36:12.500 --> 00:36:13.500]   about.
[00:36:13.500 --> 00:36:18.380]   Finally, integrating weights and biases with your models is very simple.
[00:36:18.380 --> 00:36:23.740]   You just import weights and biases, you initialize a project, and then you can start logging
[00:36:23.740 --> 00:36:25.220]   any metrics that you want.
[00:36:25.220 --> 00:36:33.300]   We also have integrations for Keras, Scikit, LightGBM, XGBoost, and many other different
[00:36:33.300 --> 00:36:34.940]   kinds of models.
[00:36:34.940 --> 00:36:40.980]   And if you want to get started really quickly, we'll link this page down below, and you can
[00:36:40.980 --> 00:36:44.820]   get started with weights and biases in five minutes.
[00:36:44.820 --> 00:36:46.980]   And that's all I have for you guys for today.
[00:36:46.980 --> 00:36:50.140]   We'll see you guys next week with another great episode.

