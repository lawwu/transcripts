
[00:00:00.000 --> 00:00:07.280]   I witness almost daily people that are either in government or even friends of ours who
[00:00:07.280 --> 00:00:11.000]   say we have to win the AI war with China.
[00:00:11.000 --> 00:00:12.560]   And I don't know what that means.
[00:00:12.560 --> 00:00:18.520]   I can't imagine an end state where we control all the AI and they don't have any.
[00:00:18.520 --> 00:00:19.520]   It's already too late.
[00:00:19.520 --> 00:00:20.520]   It's too late.
[00:00:20.520 --> 00:00:21.520]   And they're smart.
[00:00:21.520 --> 00:00:25.800]   And the reality is that we just need to focus on running our fastest race.
[00:00:25.800 --> 00:00:26.800]   We need the Teslas.
[00:00:26.800 --> 00:00:27.960]   We need the open AIs.
[00:00:27.960 --> 00:00:30.280]   We need rockets that land themselves.
[00:00:30.280 --> 00:00:31.360]   We need all of this.
[00:00:31.360 --> 00:00:34.960]   But to think that they're not going to have BYD building great cars, or they're not going
[00:00:34.960 --> 00:00:39.160]   to have DeepSeek building great models, or they're not going to have rocket companies
[00:00:39.160 --> 00:00:43.360]   that copy us and can land themselves like that, that would be naive.
[00:00:43.360 --> 00:00:49.320]   It's remarkably naive.
[00:00:49.320 --> 00:00:58.240]   Bill, it's good to be with you.
[00:00:58.240 --> 00:00:59.240]   Good to see you.
[00:00:59.240 --> 00:01:03.480]   I mean, we're in this-- wait, should we tell them that Steve Ballmer gave us this man cave?
[00:01:03.480 --> 00:01:06.320]   It's a private cave in the interior.
[00:01:06.320 --> 00:01:12.440]   I mean, the truth of the matter is, the hardest thing about this pod-- I love this pod-- but
[00:01:12.440 --> 00:01:15.360]   you and I getting our schedules to match and actually getting together.
[00:01:15.360 --> 00:01:17.080]   And so I've gotten a ton of feedback.
[00:01:17.080 --> 00:01:20.720]   I've seen on Twitter people like, when are you guys going to record the pod first?
[00:01:20.720 --> 00:01:25.200]   Thank you for the audience encouraging us to do this, because I love doing it.
[00:01:25.200 --> 00:01:27.240]   Know that we would love to do it more.
[00:01:27.240 --> 00:01:29.860]   It's just a little challenging to get together and to do it.
[00:01:29.860 --> 00:01:35.440]   We have an ongoing dialogue, I think pretty much 24 by 7, about this stuff going on in
[00:01:35.440 --> 00:01:36.440]   the world.
[00:01:36.440 --> 00:01:39.600]   And then occasionally, we get to get together and share it with you all.
[00:01:39.600 --> 00:01:40.600]   So I don't know.
[00:01:40.600 --> 00:01:44.720]   I thought maybe today, Bill, we'd kick it off with Grok 3.
[00:01:44.720 --> 00:01:52.360]   So we're now like 10 days out since Elon and his team unveiled, in pretty record time,
[00:01:52.360 --> 00:01:53.920]   an unbelievable model.
[00:01:53.920 --> 00:02:01.360]   And so maybe you can just help us zero-base what you thought when the model came out,
[00:02:01.360 --> 00:02:06.720]   where it stands in the rankings, then we can have a conversation about the impact and what
[00:02:06.720 --> 00:02:07.720]   it means.
[00:02:07.720 --> 00:02:08.720]   Yeah.
[00:02:08.720 --> 00:02:14.040]   So we've talked about this in the past, but everyone in the ecosystem was super impressed
[00:02:14.040 --> 00:02:20.760]   with how quickly they built the Memphis facility, and how big it was, and it's the largest contiguous
[00:02:20.760 --> 00:02:23.240]   cluster in the world.
[00:02:23.240 --> 00:02:26.040]   And there was a lot of chatter about that ahead of time.
[00:02:26.040 --> 00:02:32.240]   And I can remember some of the investors there saying, this will prove that pre-training
[00:02:32.240 --> 00:02:37.220]   still has headroom, because this will be the biggest cluster ever trained on.
[00:02:37.220 --> 00:02:44.020]   And you can decide what your expectation was after that kind of line in the sand.
[00:02:44.020 --> 00:02:50.060]   The things that happened, I mean, I think the generic way of saying it is it went right
[00:02:50.060 --> 00:02:52.820]   up near the top of all the benchmarks.
[00:02:52.820 --> 00:02:53.820]   Right.
[00:02:53.820 --> 00:02:55.860]   Ahead on some, not on others.
[00:02:55.860 --> 00:03:00.300]   Some people argued about whether they-- The reasoning component, the beta reasoning.
[00:03:00.300 --> 00:03:03.200]   Or did they cheat or over-tune to a benchmark?
[00:03:03.200 --> 00:03:05.420]   But I don't think it matters.
[00:03:05.420 --> 00:03:11.500]   I think the biggest positive takeaway is there's a new player in the model market.
[00:03:11.500 --> 00:03:14.900]   And we had often, a lot of people said, as a sport of Kings, there's only going to be
[00:03:14.900 --> 00:03:15.900]   so many players.
[00:03:15.900 --> 00:03:16.900]   Correct.
[00:03:16.900 --> 00:03:20.060]   There's a new one in the market that invested what they needed to do, that has access to
[00:03:20.060 --> 00:03:26.380]   capital, has a data asset that they argue is important and special, and was able to
[00:03:26.380 --> 00:03:29.980]   get at the front of the race, let's just call it that.
[00:03:29.980 --> 00:03:30.980]   Right.
[00:03:30.980 --> 00:03:35.140]   And you're looking at, we're looking at this artificial analysis that just shows, I mean,
[00:03:35.140 --> 00:03:38.180]   there's this clustering here in the upper right.
[00:03:38.180 --> 00:03:40.660]   Deep Seek kind of got up there a couple of weeks before.
[00:03:40.660 --> 00:03:41.660]   You had Grok.
[00:03:41.660 --> 00:03:46.660]   You know, what's interesting is they all seem to be coalescing in an impressive way around
[00:03:46.660 --> 00:03:48.140]   the top of these benchmarks.
[00:03:48.140 --> 00:03:52.660]   And when we say they all, I mean, we're really only talking about five or six players who
[00:03:52.660 --> 00:03:55.180]   have a chance to be in this game at this point.
[00:03:55.180 --> 00:03:56.180]   Correct.
[00:03:56.180 --> 00:03:57.180]   Correct.
[00:03:57.180 --> 00:04:03.300]   And I saw people who interpreted Grok's fast rise as proof that pre-trainings still got
[00:04:03.300 --> 00:04:09.060]   legs, and to me, I kind of had the opposite reaction, which is I felt like they just slammed
[00:04:09.060 --> 00:04:12.860]   up against the ceiling that's holding everyone in.
[00:04:12.860 --> 00:04:13.860]   Right.
[00:04:13.860 --> 00:04:18.140]   Although, again, an incredibly capable, right, level.
[00:04:18.140 --> 00:04:19.140]   So...
[00:04:19.140 --> 00:04:20.140]   Oh, no doubt.
[00:04:20.140 --> 00:04:25.680]   I just, you know, I've said this for a while, I've been concerned that the way an LLM works
[00:04:25.680 --> 00:04:30.100]   and the way it's optimized, that building bigger clusters and more parameters won't
[00:04:30.100 --> 00:04:36.140]   buy you much, and whether I said it or not, Ilya said it, Andreessen said it, other people
[00:04:36.140 --> 00:04:37.500]   said the same thing.
[00:04:37.500 --> 00:04:40.780]   And so, to me, this reinforced that point.
[00:04:40.780 --> 00:04:46.700]   I was expecting, if there were pre-training headroom, I was expecting this to go through.
[00:04:46.700 --> 00:04:48.860]   Now, I will qualify.
[00:04:48.860 --> 00:04:50.380]   This was their first run.
[00:04:50.380 --> 00:04:51.380]   Right.
[00:04:51.380 --> 00:04:52.660]   Maybe there were some tricks they didn't know.
[00:04:52.660 --> 00:04:53.660]   Right.
[00:04:53.660 --> 00:04:57.660]   They could very well back up and do another run on that same large cluster and maybe shoot
[00:04:57.660 --> 00:04:58.660]   past these people.
[00:04:58.660 --> 00:04:59.660]   Yeah.
[00:04:59.660 --> 00:05:02.100]   So, benchmarks aren't the exact right thing to be looking at.
[00:05:02.100 --> 00:05:03.100]   Right.
[00:05:03.100 --> 00:05:04.100]   Well, I would say a couple of other things.
[00:05:04.100 --> 00:05:06.300]   Number one, it's not just a pre-trained model.
[00:05:06.300 --> 00:05:11.460]   They also have an inference time reasoning component to the model that's incredibly capable.
[00:05:11.460 --> 00:05:15.020]   We have this benchmark chart, right, that I tweeted the other day, and I compared it
[00:05:15.020 --> 00:05:18.860]   to kind of the search index benchmarks that we all used to track.
[00:05:18.860 --> 00:05:23.820]   And, you know, the benchmarks are one thing, but the reality is, how do we feel when we're
[00:05:23.820 --> 00:05:24.820]   using the product?
[00:05:24.820 --> 00:05:33.220]   And what I will say is Grok 3 rocketed to the top of all app downloads, you know, on
[00:05:33.220 --> 00:05:34.220]   the iPhone charts.
[00:05:34.220 --> 00:05:40.740]   You know, at least my Twitter thread was full of people having great experiences, showing
[00:05:40.740 --> 00:05:43.340]   those great experiences on Grok 3.
[00:05:43.340 --> 00:05:48.220]   So it had a personality and an interaction with people that I think people were enjoying.
[00:05:48.220 --> 00:05:50.980]   So number one, it just has to be capable enough.
[00:05:50.980 --> 00:05:51.980]   Right.
[00:05:51.980 --> 00:05:55.740]   And they clearly crossed the threshold of being capable enough.
[00:05:55.740 --> 00:06:01.820]   Now the real question shifts to, can they leverage the X platform, right, which reaches
[00:06:01.820 --> 00:06:05.340]   a massive and important audience to really drive that.
[00:06:05.340 --> 00:06:10.860]   And what I would say, the early indications to me, when you compare it, for example, to
[00:06:10.860 --> 00:06:17.580]   how Meta has used Meta AI, like, as incredible as I think Zuckerberg and Meta are and the
[00:06:17.580 --> 00:06:22.340]   advancements they've made, I have not particularly been impressed by the productization of Meta
[00:06:22.340 --> 00:06:23.340]   AI.
[00:06:23.340 --> 00:06:24.340]   Right.
[00:06:24.340 --> 00:06:27.820]   It's basically just a search box stuck at the top of Instagram or stuck in my WhatsApp
[00:06:27.820 --> 00:06:28.820]   thread.
[00:06:28.820 --> 00:06:35.060]   And when I'm on it, I never intend to be there, whereas on X, they figured out, you know,
[00:06:35.060 --> 00:06:39.540]   the first thing they did is they put that button at the bottom of the app that clearly
[00:06:39.540 --> 00:06:43.060]   distinguishes it as its own standalone application.
[00:06:43.060 --> 00:06:45.100]   They launched a standalone application.
[00:06:45.100 --> 00:06:48.340]   They're using X to drive those app downloads.
[00:06:48.340 --> 00:06:54.020]   And now I just opened up my, you know, my X app today, and it said, "Hey, go out and
[00:06:54.020 --> 00:06:56.580]   try the new voice for Grok 3."
[00:06:56.580 --> 00:07:02.140]   So to me, the execution on the product side to drive consumer use has been pretty damn
[00:07:02.140 --> 00:07:04.940]   impressive and took them to the top of the charts.
[00:07:04.940 --> 00:07:12.460]   Yeah, and only DeepSeek and Grok, of all the others, have shown the ability to break into
[00:07:12.460 --> 00:07:15.900]   the top 10 on the app store download.
[00:07:15.900 --> 00:07:16.900]   Exactly.
[00:07:16.900 --> 00:07:20.340]   And so I think that, you know, while we all have a fascination where, where did they get
[00:07:20.340 --> 00:07:22.260]   to on the benchmarks?
[00:07:22.260 --> 00:07:27.820]   My own sense at this point in time is, you know, this is going to be one of these battles,
[00:07:27.820 --> 00:07:32.380]   kind of like search was at this point in time, where, you know, the five or six players,
[00:07:32.380 --> 00:07:36.620]   and it's going to be, you know, just out today, literally, as we're about ready to go on,
[00:07:36.620 --> 00:07:41.580]   you know, OpenAI has released ChatGPT 4.5.
[00:07:41.580 --> 00:07:47.500]   And they've kind of hinted in, I guess, this presentation as to ChatGPT 5 or 6, I guess,
[00:07:47.500 --> 00:07:49.100]   that was shown on a screen.
[00:07:49.100 --> 00:07:53.580]   And if you look at 4.5, one of the important distinguishing elements that they're pitching
[00:07:53.580 --> 00:07:54.580]   is it's more human-like.
[00:07:54.580 --> 00:07:55.580]   Yeah.
[00:07:55.580 --> 00:07:58.460]   It gives better answers, more concise answers, etc.
[00:07:58.460 --> 00:08:02.820]   Not a big breakthrough on the evals, although there are some improvements in the early looks
[00:08:02.820 --> 00:08:04.980]   against the evals.
[00:08:04.980 --> 00:08:09.700]   But I think ultimately, we're going to measure the success of these things by how many people
[00:08:09.700 --> 00:08:10.700]   are using them.
[00:08:10.700 --> 00:08:14.740]   Well, so I think one thing to be good for the audience, I know you've said it in the
[00:08:14.740 --> 00:08:20.580]   past, but you're an investor in OpenAI, and I think you have a theory about their prowess
[00:08:20.580 --> 00:08:23.380]   in the consumer market and their lead in the consumer market.
[00:08:23.380 --> 00:08:24.940]   So why don't you reiterate that?
[00:08:24.940 --> 00:08:25.940]   Yeah.
[00:08:25.940 --> 00:08:31.060]   Well, I mean, you know, I've showed, I've showed this chart before, right, that in the
[00:08:31.060 --> 00:08:36.500]   search wars, we had Google and Yahoo and AltaVista and Lycos and Ashgeese and Excite and Infoseek.
[00:08:36.500 --> 00:08:40.800]   And by the way, they all did pretty damn good on the benchmarks, right?
[00:08:40.800 --> 00:08:45.580]   But the reality is that didn't get them to any value creation because ultimately, all
[00:08:45.580 --> 00:08:48.900]   the consumers aggregated around Google.
[00:08:48.900 --> 00:08:55.500]   So the real question is, does that same pattern play out of winner-take-most in consumer around
[00:08:55.500 --> 00:08:56.860]   AI, right?
[00:08:56.860 --> 00:09:00.100]   It did in search, and it did in social.
[00:09:00.100 --> 00:09:04.900]   But it's not necessarily, you know, follow on that it will in AI because, you know, I'll
[00:09:04.900 --> 00:09:10.260]   stipulate X has an incredible installed base that they can market into, Meta has an incredible
[00:09:10.260 --> 00:09:11.780]   installed base, Google has it.
[00:09:11.780 --> 00:09:16.840]   And it's existential for those companies in order to, you know, market to those consumers.
[00:09:16.840 --> 00:09:19.980]   So I don't think it's going to be winner-take-as-much.
[00:09:19.980 --> 00:09:23.260]   I don't think we're going to see a 99% monopoly here.
[00:09:23.260 --> 00:09:28.460]   But I do expect that, you know, we're going to see 70% or 80% share go to the winner.
[00:09:28.460 --> 00:09:32.740]   Now if we look at the numbers, yeah, if we look at the numbers today, I think last week
[00:09:32.740 --> 00:09:39.000]   Sarah reported that OpenAI has crossed 400 million weekly average users.
[00:09:39.000 --> 00:09:41.580]   That's a user number, not a paid user.
[00:09:41.580 --> 00:09:43.020]   That's a user number, right?
[00:09:43.020 --> 00:09:45.700]   The number of paid users is a fraction of that.
[00:09:45.700 --> 00:09:50.700]   I think they also reported last week something like $11 or $12 billion in expected revenue
[00:09:50.700 --> 00:09:51.700]   this year.
[00:09:51.700 --> 00:09:57.920]   So you can reverse engineer your way into kind of what percentage are paying for that.
[00:09:57.920 --> 00:10:02.820]   But more importantly, I think the number of monthly average users must be somewhere on
[00:10:02.820 --> 00:10:07.780]   the order of magnitude of 700 to 800 million monthly average users.
[00:10:07.780 --> 00:10:11.080]   And there, you know, you and I followed consumer for a long time.
[00:10:11.080 --> 00:10:14.420]   There's this magic number around a billion that, I mean, like I already think they're
[00:10:14.420 --> 00:10:16.480]   nearer at escape velocity.
[00:10:16.480 --> 00:10:21.740]   But at a billion monthlies, you can funnel all of those folks into weeklies.
[00:10:21.740 --> 00:10:26.540]   And then you funnel the weeklies into paying subscribers or people who are consuming advertising.
[00:10:26.540 --> 00:10:31.180]   So what I have seen is everybody else catch up on the benchmarks.
[00:10:31.180 --> 00:10:35.980]   What I have not seen is people catch up on the consumer velocity.
[00:10:35.980 --> 00:10:39.780]   Let's handicap some of the other players a bit.
[00:10:39.780 --> 00:10:43.540]   Who do you think is closest from a user standpoint?
[00:10:43.540 --> 00:10:48.580]   Is it probably, you'd have to count the Gemini searches in the Google search, right?
[00:10:48.580 --> 00:10:50.540]   To get to a number that's close to OpenAI.
[00:10:50.540 --> 00:10:51.540]   Yeah.
[00:10:51.540 --> 00:10:55.940]   I mean, listen, I think, you know, let's just start with Google, OK?
[00:10:55.940 --> 00:10:59.060]   So there's been a lot of reports out over the course of the last couple of weeks.
[00:10:59.060 --> 00:11:04.780]   We have public companies now reporting that are reporting their Google organic clicks
[00:11:04.780 --> 00:11:08.120]   are down 20 to 40% year to date.
[00:11:08.120 --> 00:11:09.120]   So the question is, why?
[00:11:09.120 --> 00:11:11.620]   Like, why is Google's clicks down so much?
[00:11:11.620 --> 00:11:16.140]   Because if I do a Google search today on my phone, half of the page is taken up with an
[00:11:16.140 --> 00:11:21.980]   AI answer to whatever my Google query is, and the rest are all paid links, right?
[00:11:21.980 --> 00:11:26.180]   So I think Google, I think that's the right decision for them to make, right?
[00:11:26.180 --> 00:11:31.460]   If you want to compete, you ultimately have to be willing to, you know, take the innovator's
[00:11:31.460 --> 00:11:35.140]   dilemma head on and really just cannibalize your product with AI.
[00:11:35.140 --> 00:11:36.140]   Now if they do that-
[00:11:36.140 --> 00:11:40.380]   And if you're one of these humans that thinks SEO wasn't dead already, which I would have
[00:11:40.380 --> 00:11:44.660]   declared it dead a while ago, it's really f*cking dead, yeah.
[00:11:44.660 --> 00:11:46.840]   SEO is dead.
[00:11:46.840 --> 00:11:52.180]   But just, you know, those are the free links that was the core product that used to attract
[00:11:52.180 --> 00:11:53.820]   everybody to Google.
[00:11:53.820 --> 00:11:57.460]   And the idea that SEO is basically now gone is pretty absurd.
[00:11:57.460 --> 00:12:02.380]   You know, I think, and this is just an aside, but I've been remarkably frustrated with Google's
[00:12:02.380 --> 00:12:08.180]   organic links for the past five years because you go in and search for your favorite team's
[00:12:08.180 --> 00:12:11.460]   schedule and all the ticket guys are up front.
[00:12:11.460 --> 00:12:14.540]   Now, like the link you're looking for, you have to hunt for it.
[00:12:14.540 --> 00:12:16.580]   Okay, so let's talk about that for a second.
[00:12:16.580 --> 00:12:21.820]   I mean, now the obscure link or the obscure information that you and I may be looking
[00:12:21.820 --> 00:12:24.420]   for may be on page three, four, or five.
[00:12:24.420 --> 00:12:27.700]   You and I are never going to get to page three, four, or five.
[00:12:27.700 --> 00:12:33.560]   What's so interesting, for example, about OpenAI's deep research, now if I launch a
[00:12:33.560 --> 00:12:41.420]   query using deep research, it will go to page four or five or 10 or 100 and find those obscure
[00:12:41.420 --> 00:12:42.760]   pieces of information.
[00:12:42.760 --> 00:12:49.160]   So I think the evolution of Google actually provides acceleration to the deep research
[00:12:49.160 --> 00:12:55.640]   project because I don't want to go do that deep research.
[00:12:55.640 --> 00:13:00.000]   So Google, I think you just can't discount their installed base.
[00:13:00.000 --> 00:13:05.360]   The number of people going there who will, inertia will continue to carry them there.
[00:13:05.360 --> 00:13:11.320]   But I would say this, and you can go search for this on Twitter or anywhere else.
[00:13:11.320 --> 00:13:16.640]   And I know certainly with my own behavior, the amount of activity that I used to do on
[00:13:16.640 --> 00:13:23.480]   Google has been 80% cannibalized by ChatGPT because there's search embedded within ChatGPT.
[00:13:23.480 --> 00:13:27.080]   And so I'm getting all of that information, all of those answers.
[00:13:27.080 --> 00:13:30.760]   So I think that they're going to be formidable.
[00:13:30.760 --> 00:13:35.040]   I think they're being bolder than they've been, but I think they'll have to continue
[00:13:35.040 --> 00:13:36.040]   to do that.
[00:13:36.040 --> 00:13:39.680]   I think that, and we've talked about this, but I think some of their assets are remarkable.
[00:13:39.680 --> 00:13:46.400]   I mean, you've got the YouTube data set and all the search queries over all the years,
[00:13:46.400 --> 00:13:51.800]   their understanding of structure, of structured data around a lot of the consumer verticals.
[00:13:51.800 --> 00:13:55.040]   I mean, they built that out in airlines and things.
[00:13:55.040 --> 00:14:00.720]   They should be able to do those agent type queries better, faster, should.
[00:14:00.720 --> 00:14:03.160]   Their velocity on product has not been impressive.
[00:14:03.160 --> 00:14:04.160]   Wait, let me finish.
[00:14:04.160 --> 00:14:05.960]   Their velocity on consumer has not been impressive.
[00:14:05.960 --> 00:14:07.200]   So they have them.
[00:14:07.200 --> 00:14:08.960]   They've had them for a long time, Bill.
[00:14:08.960 --> 00:14:11.520]   They had ChatGPT before ChatGPT.
[00:14:11.520 --> 00:14:15.640]   They also have Android, which is a massive asset.
[00:14:15.640 --> 00:14:23.200]   And they also have browser, their own browser, which both Perplexity and OpenAI have started
[00:14:23.200 --> 00:14:29.040]   toying with the idea of either having a browser or in the operator case of using a browser
[00:14:29.040 --> 00:14:31.440]   in the cloud to go do this work.
[00:14:31.440 --> 00:14:34.200]   Anyway, they have so much.
[00:14:34.200 --> 00:14:38.940]   I still think they have a bit of the innovators dilemma in that they can't, they still have
[00:14:38.940 --> 00:14:42.960]   to try and maintain those paid links on that page.
[00:14:42.960 --> 00:14:48.840]   This chart here, the black line, is Google's paid click growth plotted against the weekly
[00:14:48.840 --> 00:14:51.040]   average user growth at OpenAI.
[00:14:51.040 --> 00:14:52.640]   It's not going in the right direction.
[00:14:52.640 --> 00:15:00.080]   Well, and I think it benefits from the fact that informational searches are what ChatGPT
[00:15:00.080 --> 00:15:04.440]   cannibalized first, not the commerce searches, which is where most of the money is on the
[00:15:04.440 --> 00:15:05.440]   paid link.
[00:15:05.440 --> 00:15:10.620]   Now, again, and we're going to see this out of X, we're going to see it out of everybody.
[00:15:10.620 --> 00:15:15.780]   Everything, the entire domain of the internet is the domain of agents.
[00:15:15.780 --> 00:15:21.400]   So if you think about Operator as one of the first agents rolled out by OpenAI, what does
[00:15:21.400 --> 00:15:22.400]   Operator do?
[00:15:22.400 --> 00:15:26.780]   It goes and it mimics me as a human going out and researching a hotel and booking a
[00:15:26.780 --> 00:15:28.620]   hotel or whatever on the internet.
[00:15:28.620 --> 00:15:30.380]   And we're in a very embryonic state.
[00:15:30.380 --> 00:15:34.780]   I agree with you, we're not there yet, but it's very clear what the roadmap is going
[00:15:34.780 --> 00:15:35.780]   to be.
[00:15:35.780 --> 00:15:36.780]   It's going to want your credentials.
[00:15:36.780 --> 00:15:40.180]   And whether you give it your credentials or not is going to matter because it's searching
[00:15:40.180 --> 00:15:41.300]   against it.
[00:15:41.300 --> 00:15:42.700]   So let's talk about Meta.
[00:15:42.700 --> 00:15:49.340]   Actually, one last thing on Google, there was a point where Meta went public at 40.
[00:15:49.340 --> 00:15:50.340]   We had some.
[00:15:50.340 --> 00:15:51.340]   Backed up the truck.
[00:15:51.340 --> 00:15:53.860]   We had some, so I was paying attention.
[00:15:53.860 --> 00:15:59.180]   And Zuck, as he has many times, got woken up on mobile.
[00:15:59.180 --> 00:16:00.180]   100%.
[00:16:00.180 --> 00:16:03.620]   Everyone thought he was dead because Apple was messing with him.
[00:16:03.620 --> 00:16:05.660]   And he had also built an HTML5.
[00:16:05.660 --> 00:16:07.780]   He didn't believe in native app.
[00:16:07.780 --> 00:16:11.860]   There's a whole thing that they weren't going to be able to monetize mobile.
[00:16:11.860 --> 00:16:16.740]   It was on the cover of Barron's magazine, the weekend magazine.
[00:16:16.740 --> 00:16:19.900]   It was like Meta's dead or Facebook's dead.
[00:16:19.900 --> 00:16:22.020]   But he woke up and fixed it.
[00:16:22.020 --> 00:16:23.700]   Can Google do that here?
[00:16:23.700 --> 00:16:24.780]   Is that possible?
[00:16:24.780 --> 00:16:28.180]   Can they have a similar, and what would it look like?
[00:16:28.180 --> 00:16:29.180]   And where would it take?
[00:16:29.180 --> 00:16:37.620]   I mean, listen, I've said publicly that Google's moat was not a technological moat with search.
[00:16:37.620 --> 00:16:39.540]   Their moat was a distribution moat.
[00:16:39.540 --> 00:16:41.580]   Their moat was a mind share moat.
[00:16:41.580 --> 00:16:45.540]   We Googled everything when we wanted to know anything.
[00:16:45.540 --> 00:16:49.420]   And the only thing that could attack Google was never anything head on.
[00:16:49.420 --> 00:16:54.340]   It had to be an orthogonal attack from something that was 10x better, 100x better, because
[00:16:54.340 --> 00:16:57.600]   it gave us answers instead of blue links, right?
[00:16:57.600 --> 00:17:02.780]   That's why it was such a mortal sin for them to ever, ever allow anybody else to go first.
[00:17:02.780 --> 00:17:07.500]   Because the only thing that could give you a trillion dollars worth of free mindshare
[00:17:07.500 --> 00:17:10.260]   is going first with something that was 100x better.
[00:17:10.260 --> 00:17:13.540]   And that's exactly what ChatGPT did at the end of 2020.
[00:17:13.540 --> 00:17:14.540]   All right.
[00:17:14.540 --> 00:17:15.540]   Go to Meta.
[00:17:15.540 --> 00:17:22.040]   So, I mean, you know, again, Meta, if you had to handicap the big guys, 3 billion users
[00:17:22.040 --> 00:17:29.100]   of their product, I think they have products that are tailor-made for chat-oriented AI,
[00:17:29.100 --> 00:17:33.420]   whether it's Instagram and having shopping agents and, you know, co-shopping agents or
[00:17:33.420 --> 00:17:38.620]   whether it's WhatsApp and just having a bunch of agents live within my WhatsApp channel.
[00:17:38.620 --> 00:17:42.720]   It feels natively much better positioned for AI.
[00:17:42.720 --> 00:17:48.700]   And we know that Zuckerberg is, you know, in complete beast mode.
[00:17:48.700 --> 00:17:54.420]   But I am surprised, I have to say, that we're now kind of 18 months into kind of the llama
[00:17:54.420 --> 00:18:00.460]   thing and it feels like the manifestation of it into the product was slower than I expected
[00:18:00.460 --> 00:18:01.460]   in 2024.
[00:18:01.460 --> 00:18:02.460]   Back to your product point.
[00:18:02.460 --> 00:18:03.460]   Right.
[00:18:03.460 --> 00:18:04.460]   Exactly.
[00:18:04.460 --> 00:18:05.460]   Right.
[00:18:05.460 --> 00:18:06.460]   Like Meta hasn't.
[00:18:06.460 --> 00:18:12.880]   And I will say, I will say even, you know, we know he was ripped about DeepSeek, right?
[00:18:12.880 --> 00:18:17.920]   Kind of blindsiding llama in the release of R1.
[00:18:17.920 --> 00:18:20.340]   And so I would say it's not just product for them.
[00:18:20.340 --> 00:18:25.440]   I think they have, you know, I heard from several inference players that you and I are
[00:18:25.440 --> 00:18:31.920]   friends with that all of a sudden DeepSeek rather than llama is the enterprise open source
[00:18:31.920 --> 00:18:35.700]   model of choice that everybody's experimenting with and playing with.
[00:18:35.700 --> 00:18:38.640]   And so that becomes a real problem for them as well.
[00:18:38.640 --> 00:18:42.020]   So I think 2025 is a critical year.
[00:18:42.020 --> 00:18:43.300]   I think they will come through.
[00:18:43.300 --> 00:18:48.800]   And remember, when it comes to almost all product stuff, stories, copy, you know, catching
[00:18:48.800 --> 00:18:54.840]   up with with Snapchat or or whether it's Reels catching up with TikTok, they've always showed
[00:18:54.840 --> 00:18:59.520]   up to the party late, but they are grinders and they always deliver the product.
[00:18:59.520 --> 00:19:01.400]   It would be interesting to see what they do.
[00:19:01.400 --> 00:19:03.200]   Who else would be in the list?
[00:19:03.200 --> 00:19:05.680]   I mean, Anthropic has really not been.
[00:19:05.680 --> 00:19:08.840]   No, they've pretty much seeded the game on consumer.
[00:19:08.840 --> 00:19:11.840]   You know, there was a product announcement yesterday about they're going to be powering
[00:19:11.840 --> 00:19:13.560]   Alexa.
[00:19:13.560 --> 00:19:15.800]   But you know, now we're stretching.
[00:19:15.800 --> 00:19:16.800]   Right.
[00:19:16.800 --> 00:19:19.820]   And Amazon did do a big Alexa launch yesterday.
[00:19:19.820 --> 00:19:20.820]   Correct.
[00:19:20.820 --> 00:19:22.240]   Pretty late in this game.
[00:19:22.240 --> 00:19:23.240]   Yeah.
[00:19:23.240 --> 00:19:28.480]   And by the way, Alexa is not really in again, it's it occupies a different space in most
[00:19:28.480 --> 00:19:30.200]   consumers minds.
[00:19:30.200 --> 00:19:32.400]   It is not what chat GPT does.
[00:19:32.400 --> 00:19:37.320]   And so I think to dislodge something that has the momentum chat GPT does, you have to
[00:19:37.320 --> 00:19:41.920]   go at them and do better than what they do at the thing that they do.
[00:19:41.920 --> 00:19:46.560]   And this is why I think X, you know, if I go through the whole list here, X to me is
[00:19:46.560 --> 00:19:54.240]   so interesting because they have a platform that is the number one news platform in every
[00:19:54.240 --> 00:19:55.760]   country on the planet.
[00:19:55.760 --> 00:20:00.000]   The people who are most actively engaged are using this platform and they go there for
[00:20:00.000 --> 00:20:01.000]   information.
[00:20:01.000 --> 00:20:02.000]   Right.
[00:20:02.000 --> 00:20:03.000]   They go there for answers.
[00:20:03.000 --> 00:20:04.580]   They go there to engage.
[00:20:04.580 --> 00:20:09.560]   So I think it's an audience that's very well suited for AI.
[00:20:09.560 --> 00:20:11.960]   I think the integration they've done is as good.
[00:20:11.960 --> 00:20:16.240]   By the way, they've done this in a very short period of time, you know, and I mean, I'm
[00:20:16.240 --> 00:20:18.400]   talking everything from the logo.
[00:20:18.400 --> 00:20:24.080]   Some of the tweets will have the logo pop up and then it'll summarize or do more.
[00:20:24.080 --> 00:20:30.680]   So I'm I'm really, really impressed at the velocity of not only catching up on the benchmark,
[00:20:30.680 --> 00:20:33.080]   but catching up on the consumer product side.
[00:20:33.080 --> 00:20:38.500]   And so, you know, listen, they're number one on the App Store and that stands for something,
[00:20:38.500 --> 00:20:39.500]   you know, coming out of nowhere.
[00:20:39.500 --> 00:20:42.680]   And people said that Elon couldn't, you know, couldn't do this.
[00:20:42.680 --> 00:20:47.700]   I never doubted that they would catch up on the benchmarks if they got a big enough cluster
[00:20:47.700 --> 00:20:53.060]   because Elon set a mission that people become messianic about, you know, and his engineering
[00:20:53.060 --> 00:20:55.340]   capability to build out the cluster and do all those things.
[00:20:55.340 --> 00:20:57.100]   So that was never the question for me.
[00:20:57.100 --> 00:21:04.580]   The real question was, can anybody, can he close the gap on the consumer race, OK?
[00:21:04.580 --> 00:21:11.260]   And there I think, you know, the odds on favorite there has to be OpenAI.
[00:21:11.260 --> 00:21:14.220]   I think they continue to widen their gap, by the way, Bill.
[00:21:14.220 --> 00:21:17.820]   I think they're accelerating at scale, but that's where the race is.
[00:21:17.820 --> 00:21:23.740]   And it may very well be that coming in second place with 20 percent share is a pretty good
[00:21:23.740 --> 00:21:24.740]   place to be.
[00:21:24.740 --> 00:21:30.900]   I want to mention one more company and then I'm going to make a guess at four ways someone
[00:21:30.900 --> 00:21:32.860]   could try and win this game.
[00:21:32.860 --> 00:21:37.880]   But the one I want to mention before I do that is just Perflexity, real briefly.
[00:21:37.880 --> 00:21:45.260]   I will give them credit for being product centric, to your point, and innovative in
[00:21:45.260 --> 00:21:47.860]   ways that the others haven't.
[00:21:47.860 --> 00:21:52.940]   And kind of on their own terms, they don't have near the usage of OpenAI.
[00:21:52.940 --> 00:21:57.660]   So it is a question that kind of looks like an acquisition candidate to me.
[00:21:57.660 --> 00:22:01.860]   I don't know if anyone can agree on price, but for one of these other players that hasn't
[00:22:01.860 --> 00:22:03.980]   been as successful from a product standpoint.
[00:22:03.980 --> 00:22:06.500]   I mean, you could, you and I just had this conversation.
[00:22:06.500 --> 00:22:10.820]   I mean, you could imagine, for example, a world in which Microsoft were to buy Perflexity
[00:22:10.820 --> 00:22:13.080]   and now they have a consumer brand to go battle it out.
[00:22:13.080 --> 00:22:16.140]   We know how much Satya wants to win in consumer.
[00:22:16.140 --> 00:22:22.140]   Now, he owns a bunch of OpenAI and so he's got some potential channel conflict there.
[00:22:22.140 --> 00:22:26.260]   But I think the bigger issue at this point now, you know, we know with Leanicon out,
[00:22:26.260 --> 00:22:29.820]   you're probably more likely to be able to do a deal like that.
[00:22:29.820 --> 00:22:35.140]   But when founders are raising $8, $9 billion, you know, it becomes...
[00:22:35.140 --> 00:22:36.140]   You take it off the table.
[00:22:36.140 --> 00:22:40.500]   It becomes a much, much more difficult decision for a company like Microsoft.
[00:22:40.500 --> 00:22:45.000]   Not saying that it couldn't happen, and I will say this, that when it comes to, you
[00:22:45.000 --> 00:22:53.380]   know, punching up, being innovative, being scrappy, product velocity, the founder there,
[00:22:53.380 --> 00:22:56.960]   Arvind Syk and the team, it's been super impressive to watch.
[00:22:56.960 --> 00:22:59.340]   I think they've made other people better.
[00:22:59.340 --> 00:23:04.800]   But the numbers, I think, as we look at them today, you know, they're really powerful,
[00:23:04.800 --> 00:23:06.360]   but much, much smaller player.
[00:23:06.360 --> 00:23:07.360]   So here we go.
[00:23:07.360 --> 00:23:08.360]   Here we go.
[00:23:08.360 --> 00:23:13.240]   I have four things I'm watching out for that could potentially lead to either further lock-in
[00:23:13.240 --> 00:23:17.520]   by OpenAI or a window for someone to do something else.
[00:23:17.520 --> 00:23:22.600]   And some of them I mentioned before, but I think, you know, memory is still this thing
[00:23:22.600 --> 00:23:25.480]   that could just tie you to something.
[00:23:25.480 --> 00:23:31.400]   And OpenAI has probably done more with memory than anyone else, but no one's really got
[00:23:31.400 --> 00:23:37.760]   to the place where I'm telling it to remember things, to store things, to create lists,
[00:23:37.760 --> 00:23:43.100]   like where it starts to become like an executive assistant for you.
[00:23:43.100 --> 00:23:44.400]   And I haven't seen that yet.
[00:23:44.400 --> 00:23:49.760]   I still think that's a dimension that could really be important.
[00:23:49.760 --> 00:23:54.480]   Voice, we've talked about, and they're all playing with it.
[00:23:54.480 --> 00:23:58.840]   I think voice also ties in with device type.
[00:23:58.840 --> 00:24:06.360]   And this is where Alexa may have some, you know, some assets, but like, you know, if
[00:24:06.360 --> 00:24:10.560]   the voice were spectacular, they might not have to carry the phone around as much.
[00:24:10.560 --> 00:24:11.560]   And by the way, by the way-
[00:24:11.560 --> 00:24:12.560]   But I need it.
[00:24:12.560 --> 00:24:13.560]   I need it.
[00:24:13.560 --> 00:24:14.560]   Yeah, you need an earbud.
[00:24:14.560 --> 00:24:15.560]   You need an earbud.
[00:24:15.560 --> 00:24:21.440]   I will say, advanced voice mode on ChatGPT is excellent, Grok 3's new voice, excellent,
[00:24:21.440 --> 00:24:23.760]   and they're getting better and accelerating rate.
[00:24:23.760 --> 00:24:28.120]   You know, we're an investor in this company, LiveKit, that's powering a lot of this voice.
[00:24:28.120 --> 00:24:32.260]   And I will tell you what I see in the product pipeline is super impressive as to what's
[00:24:32.260 --> 00:24:35.320]   coming with voice.
[00:24:35.320 --> 00:24:41.840]   The third one's nebulous, but just someone could focus on a feature that no one has to
[00:24:41.840 --> 00:24:42.840]   date.
[00:24:42.840 --> 00:24:48.000]   And right now, the game looks so much like with the benchmarks and voice, like everyone's
[00:24:48.000 --> 00:24:50.160]   running at the same place.
[00:24:50.160 --> 00:24:55.200]   So I don't, that's an easy thing to say, but it'd have to be really out of the box.
[00:24:55.200 --> 00:25:00.680]   And then fourth, I just have been thinking about this, no one's really thought about
[00:25:00.680 --> 00:25:02.400]   a network effect.
[00:25:02.400 --> 00:25:09.600]   And I wonder how you could make the quality of the AI experience a function of your user
[00:25:09.600 --> 00:25:10.600]   base.
[00:25:10.600 --> 00:25:15.040]   Let me give you an example of a network effect I think that is happening.
[00:25:15.040 --> 00:25:22.160]   I think around model improvement, if you have seven or 800 million monthly average users,
[00:25:22.160 --> 00:25:26.000]   your diversity of information and questions and answers and follow-ons, et cetera, is
[00:25:26.000 --> 00:25:27.560]   much, much higher.
[00:25:27.560 --> 00:25:31.040]   Those questions and data, that's now being fed back into the models to improve the model.
[00:25:31.040 --> 00:25:37.720]   And some users may have seen, I know I have, you get two answers and the OpenAI asks you
[00:25:37.720 --> 00:25:38.720]   to.
[00:25:38.720 --> 00:25:39.720]   Right, right.
[00:25:39.720 --> 00:25:45.640]   So I think that's an example of OpenAI very actively building, attempting to build network
[00:25:45.640 --> 00:25:49.360]   effects in terms of the quality of the model, the quality of the answers going back to the
[00:25:49.360 --> 00:25:50.360]   user.
[00:25:50.360 --> 00:25:56.200]   But there could be a more intense form of network effect if you found a way to leverage
[00:25:56.200 --> 00:25:59.000]   the user base as part of the value property.
[00:25:59.000 --> 00:26:01.320]   Let me go back to your first one, memory.
[00:26:01.320 --> 00:26:03.320]   Because you and I have talked about this a lot, right?
[00:26:03.320 --> 00:26:06.960]   If you get memory, the switching costs explode, right?
[00:26:06.960 --> 00:26:12.240]   And I would argue not only is switching costs explode, the conversion rate from free to
[00:26:12.240 --> 00:26:14.160]   paid probably also goes up, right?
[00:26:14.160 --> 00:26:15.160]   I think you're right.
[00:26:15.160 --> 00:26:16.160]   Just because the value delivered.
[00:26:16.160 --> 00:26:17.160]   I think you're right.
[00:26:17.160 --> 00:26:21.320]   And so I was with my 89-year-old mother last Sunday.
[00:26:21.320 --> 00:26:26.400]   And my mom has wanted to write a story of her life for a long time.
[00:26:26.400 --> 00:26:30.840]   And the reality is she's never going to sit down and write the story of her life.
[00:26:30.840 --> 00:26:35.480]   And yet when I'm with her, podcast style, I'll ask her questions.
[00:26:35.480 --> 00:26:38.380]   And I'll just record it on my phone, right?
[00:26:38.380 --> 00:26:41.200]   So that I have it and I could perhaps go back later.
[00:26:41.200 --> 00:26:43.880]   And then I started thinking about it.
[00:26:43.880 --> 00:26:47.280]   And I said, I don't need to be the interviewer, right?
[00:26:47.280 --> 00:26:49.200]   Advanced voice mode could be the interviewer.
[00:26:49.200 --> 00:26:52.080]   And so I was sitting there with her last weekend.
[00:26:52.080 --> 00:26:55.600]   Here's the prompt that I gave to advanced voice mode.
[00:26:55.600 --> 00:27:00.920]   I said, I'm sitting with my 89-year-old mother tonight who wants to write her life story.
[00:27:00.920 --> 00:27:06.360]   I want you to interview her about her life, to ask questions about her childhood stories,
[00:27:06.360 --> 00:27:12.160]   having kids, working, growing up in the depression, her love of computers and travel, to remember
[00:27:12.160 --> 00:27:17.320]   everything you talk about, and then compose a story of her life that her grandchildren
[00:27:17.320 --> 00:27:19.840]   would like to read, OK?
[00:27:19.840 --> 00:27:22.840]   And then advanced voice mode just started asking her questions.
[00:27:22.840 --> 00:27:23.840]   How long did it go on?
[00:27:23.840 --> 00:27:30.000]   I mean, my mom was really nervous at the start, but then like a little tear wells in my mom's
[00:27:30.000 --> 00:27:35.280]   eye, you know, because she realizes all of a sudden that, oh my God, this could be a
[00:27:35.280 --> 00:27:36.320]   massive unlock.
[00:27:36.320 --> 00:27:38.560]   So here's the thing, Bill.
[00:27:38.560 --> 00:27:41.660]   Advanced voice mode in ChatGBT already has memory.
[00:27:41.660 --> 00:27:43.060]   You can already do these things.
[00:27:43.060 --> 00:27:48.740]   The problem is the nature of the product, you don't know that it can do those things.
[00:27:48.740 --> 00:27:55.500]   So part of the challenge about designing a product where prompt is your way in is you've
[00:27:55.500 --> 00:28:00.020]   got to help people imagine, like you and I could have imagined in the age of internet,
[00:28:00.020 --> 00:28:04.420]   somebody building an internet website that just did that thing, OK?
[00:28:04.420 --> 00:28:08.820]   So I think that's one of the challenges all these companies faces, and the innovation
[00:28:08.820 --> 00:28:14.220]   around that top end of the funnel in the prompt that can help people better get into it.
[00:28:14.220 --> 00:28:21.100]   I'll give you another example, Deep Reasoning, right, which is really fascinating.
[00:28:21.100 --> 00:28:26.820]   They basically took the O3 series of models and fine-tuned it end-to-end based upon all
[00:28:26.820 --> 00:28:30.260]   these browser interactions, right?
[00:28:30.260 --> 00:28:35.420]   But you need to, the more specific the prompt, the better the deep research report is going
[00:28:35.420 --> 00:28:36.420]   to be.
[00:28:36.420 --> 00:28:41.540]   So a lot of people are using O1 to help them build sophisticated prompts that they then
[00:28:41.540 --> 00:28:44.480]   feed in to Deep Reasoning.
[00:28:44.480 --> 00:28:49.300]   And so I think that there's something in there where we're effectively using AI to get us
[00:28:49.300 --> 00:28:54.860]   to the point where we're better prompting, and one of the ways will be very simple, right?
[00:28:54.860 --> 00:28:58.540]   Once I have this assistant and I'm having an interaction, I just say to my assistant,
[00:28:58.540 --> 00:29:00.380]   "Hey, my mom wants to tell her life story.
[00:29:00.380 --> 00:29:02.300]   I'm not sure how to go about doing that.
[00:29:02.300 --> 00:29:03.300]   You have any ideas?"
[00:29:03.300 --> 00:29:06.540]   And she would say, "Hey, yeah, just use this prompt."
[00:29:06.540 --> 00:29:13.300]   And I also think that there are other assets that could play a role like a contact database,
[00:29:13.300 --> 00:29:14.300]   an email.
[00:29:14.300 --> 00:29:15.300]   >> Yes.
[00:29:15.300 --> 00:29:20.260]   >> I mean, I can't even imagine moving my email to one that's integrated inside just
[00:29:20.260 --> 00:29:21.260]   because-
[00:29:21.260 --> 00:29:22.260]   >> Yes, yes.
[00:29:22.260 --> 00:29:23.260]   Contacts is a great one.
[00:29:23.260 --> 00:29:24.260]   If they just cleaned up your contacts.
[00:29:24.260 --> 00:29:25.260]   >> Well, knowing my contacts.
[00:29:25.260 --> 00:29:26.260]   >> Right.
[00:29:26.260 --> 00:29:27.260]   And know your contacts.
[00:29:27.260 --> 00:29:28.260]   >> Yeah, yeah, yeah.
[00:29:28.260 --> 00:29:29.260]   But send an email, send a text.
[00:29:29.260 --> 00:29:31.300]   I mean, there's a lot big.
[00:29:31.300 --> 00:29:36.100]   And then for anyone that works with content, there's some app, you know.
[00:29:36.100 --> 00:29:40.220]   All my writing and everything I've done for the past 10 years has been in Quip, but some
[00:29:40.220 --> 00:29:41.220]   people use Notion.
[00:29:41.220 --> 00:29:42.220]   >> Yes.
[00:29:42.220 --> 00:29:48.060]   >> And like that type of repository and how these things can interact, there's just a
[00:29:48.060 --> 00:29:50.340]   lot of surface area to figure out.
[00:29:50.340 --> 00:29:51.340]   >> Yeah.
[00:29:51.340 --> 00:29:52.340]   Well, and this-
[00:29:52.340 --> 00:29:53.340]   >> Like where does that story land?
[00:29:53.340 --> 00:29:54.340]   Where is it stored-
[00:29:54.340 --> 00:29:55.340]   >> Right.
[00:29:55.340 --> 00:29:56.340]   >> Once you did it?
[00:29:56.340 --> 00:29:57.340]   >> Exactly.
[00:29:57.340 --> 00:29:58.340]   >> Or do you have to take it out of OpenAI?
[00:29:58.340 --> 00:30:00.740]   Like, you know, you'd rather just have a place.
[00:30:00.740 --> 00:30:01.740]   >> Correct.
[00:30:01.740 --> 00:30:03.540]   And now you have projects in OpenAI and other things.
[00:30:03.540 --> 00:30:04.540]   >> Right.
[00:30:04.540 --> 00:30:06.740]   >> And it brings me to a point.
[00:30:06.740 --> 00:30:10.900]   When you think about these research labs and you look at the number of people who work
[00:30:10.900 --> 00:30:12.340]   there, right?
[00:30:12.340 --> 00:30:16.440]   Just the fact we even call them research labs, you and I haven't, you know, nobody called
[00:30:16.440 --> 00:30:17.980]   Google a research lab.
[00:30:17.980 --> 00:30:18.980]   >> Yeah.
[00:30:18.980 --> 00:30:19.980]   >> Right?
[00:30:19.980 --> 00:30:20.980]   It was a company.
[00:30:20.980 --> 00:30:22.940]   It had product teams, it had marketing teams, it had finance teams, et cetera.
[00:30:22.940 --> 00:30:27.740]   But I think because a lot of these people came out of research, that you look at them
[00:30:27.740 --> 00:30:33.460]   and they're still very small teams, very heavily tilted toward building toward the benchmark.
[00:30:33.460 --> 00:30:35.300]   OpenAI now has thousands of people.
[00:30:35.300 --> 00:30:38.980]   I know, you know, Kevin Wiley who runs the product team over there.
[00:30:38.980 --> 00:30:43.380]   So all these companies, if you're going to win this race, you got to do all the things,
[00:30:43.380 --> 00:30:45.620]   great things that great product teams do.
[00:30:45.620 --> 00:30:47.740]   And it's building all the shit that you're talking about.
[00:30:47.740 --> 00:30:48.740]   And that's hard.
[00:30:48.740 --> 00:30:51.580]   And you got to be thoughtful and you got to growth hack and you got to, you know, get
[00:30:51.580 --> 00:30:53.900]   those customers to use the product more and more.
[00:30:53.900 --> 00:30:58.260]   >> One thing came out this week, which I don't know if it was intentional or not.
[00:30:58.260 --> 00:30:59.620]   You may know more than me.
[00:30:59.620 --> 00:31:07.020]   There was someone published the internal forecast of OpenAI, which included, I think in '25
[00:31:07.020 --> 00:31:10.140]   and '26, losing $20 billion each year.
[00:31:10.140 --> 00:31:12.260]   And someone said to me, why would they publish that?
[00:31:12.260 --> 00:31:14.020]   Why would you lose that?
[00:31:14.020 --> 00:31:21.900]   And to me, I think there is a information war out there trying to scare capital in or
[00:31:21.900 --> 00:31:28.980]   out and to lay a statement that if you want to be in this game, and keep in mind, there's
[00:31:28.980 --> 00:31:32.580]   a variable cost every time you serve a deep researcher.
[00:31:32.580 --> 00:31:39.740]   And so, if this is one of these situations where people think it's winner take all, just
[00:31:39.740 --> 00:31:47.160]   like we had with Uber Lyft, and they're going to go hard at trying to win, you probably
[00:31:47.160 --> 00:31:51.220]   need to be willing to lose $20 billion a year to step into this game.
[00:31:51.220 --> 00:31:56.340]   And X looks like they have the potential to raise that kind of money.
[00:31:56.340 --> 00:32:03.440]   I don't know if a Microsoft or an Amazon are prepared to lose incrementally that amount
[00:32:03.440 --> 00:32:04.440]   of money.
[00:32:04.440 --> 00:32:07.820]   Well, I mean, this is just such a fascinating segue into--
[00:32:07.820 --> 00:32:10.840]   By the way, there's one company we didn't even mention.
[00:32:10.840 --> 00:32:14.420]   Like when we went through all this, we didn't mention Apple, like at all.
[00:32:14.420 --> 00:32:15.420]   Pretty shocking.
[00:32:15.420 --> 00:32:16.420]   Right.
[00:32:16.420 --> 00:32:17.420]   Pretty shocking.
[00:32:17.420 --> 00:32:18.420]   Why didn't we mention them?
[00:32:18.420 --> 00:32:23.280]   Well, I mean, so Apple has self-selected out of the race, right?
[00:32:23.280 --> 00:32:25.100]   They're not building a big model.
[00:32:25.100 --> 00:32:29.940]   They've been very public about, they think that they can be kind of late mover here.
[00:32:29.940 --> 00:32:34.760]   They did this Apple intelligence integration with chat GPT, and now they're going to do
[00:32:34.760 --> 00:32:35.760]   a Gemini.
[00:32:35.760 --> 00:32:40.900]   And the reality is, as I shared with an Apple executive the other day, I said, here's the
[00:32:40.900 --> 00:32:45.300]   only integration that matters, my chat GPT app on the front page of my Apple phone.
[00:32:45.300 --> 00:32:46.300]   Right?
[00:32:46.300 --> 00:32:47.300]   Wow, that's mean.
[00:32:47.300 --> 00:32:48.420]   Well, I mean, it's like the truth.
[00:32:48.420 --> 00:32:49.420]   It's the truth.
[00:32:49.420 --> 00:32:53.060]   Like I just don't use any of the integrated features on the phone, which I think creates
[00:32:53.060 --> 00:32:55.460]   vulnerabilities for Apple.
[00:32:55.460 --> 00:32:58.620]   But I think, you know, listen, this is the first time that they've been faced, I think,
[00:32:58.620 --> 00:33:01.180]   with this level of product risk.
[00:33:01.180 --> 00:33:04.500]   But the reality is, they have such lock in on this device.
[00:33:04.500 --> 00:33:08.100]   For somebody else to build a device, maybe Huawei around the world, they're going to
[00:33:08.100 --> 00:33:12.060]   be able to ship, you know, perhaps better AI phones around the world.
[00:33:12.060 --> 00:33:14.460]   They're not going to be able to ship them into the United States.
[00:33:14.460 --> 00:33:18.980]   We'll see what this Google ruling is at the end of the year, if Google is no longer allowed
[00:33:18.980 --> 00:33:23.260]   to be the default search app because of this consent decree.
[00:33:23.260 --> 00:33:28.180]   You know, do they really turn Android into the thing that it potentially could be?
[00:33:28.180 --> 00:33:30.180]   So I think there's a bunch of potential risks.
[00:33:30.180 --> 00:33:31.180]   So we didn't talk about Apple.
[00:33:31.180 --> 00:33:34.420]   I'd say the other company we didn't really talk about, because we're so focused on the
[00:33:34.420 --> 00:33:38.620]   United States, is when you look outside the United States, you really have to look to
[00:33:38.620 --> 00:33:40.060]   China.
[00:33:40.060 --> 00:33:47.740]   Because I would say the acceleration and velocity of AI in China is off the charts.
[00:33:47.740 --> 00:33:51.780]   You know, we've talked a lot over the last few weeks about DeepSeek clearly coming out
[00:33:51.780 --> 00:33:58.740]   of left field, very efficiently building a frontier quality open source model.
[00:33:58.740 --> 00:34:04.020]   But most people have kind of quietly ignored probably the company that's the leader in
[00:34:04.020 --> 00:34:07.220]   AI in China, and that's ByteDance.
[00:34:07.220 --> 00:34:12.920]   Their AI, you know, their chat GPT equivalent is number one in China, right?
[00:34:12.920 --> 00:34:18.260]   And they've been using AI to drive TikTok globally for a very long period of time.
[00:34:18.260 --> 00:34:21.120]   So I know you have strong opinions on this.
[00:34:21.120 --> 00:34:25.680]   It seems to me the US has underestimated China at AI.
[00:34:25.680 --> 00:34:30.000]   And now we're at this inflection point where I think there are a lot of people who say,
[00:34:30.000 --> 00:34:33.340]   well, they must be smuggling GPUs into China or this or that.
[00:34:33.340 --> 00:34:37.700]   But the reality is China is going to have frontier AI.
[00:34:37.700 --> 00:34:43.140]   And almost all of the things we do to try to slow them down and stop them are backfiring
[00:34:43.140 --> 00:34:44.140]   on the United States.
[00:34:44.140 --> 00:34:46.500]   I couldn't agree more.
[00:34:46.500 --> 00:34:55.540]   I witness almost daily people that are either in government or even friends of ours who
[00:34:55.540 --> 00:34:59.140]   say we have to win the AI war with China.
[00:34:59.140 --> 00:35:01.140]   And I don't know what that means.
[00:35:01.140 --> 00:35:07.660]   Like, I can't imagine an end state where we control all the AI and they don't have any.
[00:35:07.660 --> 00:35:08.660]   It's already too late.
[00:35:08.660 --> 00:35:09.660]   It's too late.
[00:35:09.660 --> 00:35:14.140]   And they're smart as possibly can be.
[00:35:14.140 --> 00:35:15.140]   And they're innovating.
[00:35:15.140 --> 00:35:19.500]   And you look at all the other products that they're crushing it in.
[00:35:19.500 --> 00:35:20.860]   Yeah, I just don't understand.
[00:35:20.860 --> 00:35:25.420]   And the reality is that we just need to focus on running our fastest race.
[00:35:25.420 --> 00:35:26.420]   We need the Teslas.
[00:35:26.420 --> 00:35:27.580]   We need the open AIs.
[00:35:27.580 --> 00:35:29.900]   We need rockets that land themselves.
[00:35:29.900 --> 00:35:31.000]   We need all of this.
[00:35:31.000 --> 00:35:34.580]   But to think that they're not going to have BYD building great cars, or they're not going
[00:35:34.580 --> 00:35:39.220]   to have DeepSeek building great models, or they're not going to have rocket companies
[00:35:39.220 --> 00:35:43.420]   that copy us and can land themselves, that would be naive.
[00:35:43.420 --> 00:35:45.340]   It's remarkably naive.
[00:35:45.340 --> 00:35:46.340]   Yeah.
[00:35:46.340 --> 00:35:53.580]   And it's going to lead to people making decisions, like you said, that either slow us down ourselves--
[00:35:53.580 --> 00:36:01.380]   a lot of the AI regulation would definitely do that-- or just provoke them in ways that
[00:36:01.380 --> 00:36:03.460]   isn't helpful.
[00:36:03.460 --> 00:36:04.780]   And it's not going to slow them down.
[00:36:04.780 --> 00:36:06.920]   Well, let me give you one example of this.
[00:36:06.920 --> 00:36:10.380]   And then I want to move on talking about the arms race, if you will.
[00:36:10.380 --> 00:36:14.540]   But during the Biden administration, they passed something called a diffusion rule out
[00:36:14.540 --> 00:36:19.720]   of the Commerce Department, which we've mentioned on this pod before, which created this convoluted
[00:36:19.720 --> 00:36:26.340]   set of rules by which US semiconductor companies could export outside the United States.
[00:36:26.340 --> 00:36:28.100]   Now, this wasn't exporting to China.
[00:36:28.100 --> 00:36:31.540]   We already have export restrictions with respect to China.
[00:36:31.540 --> 00:36:36.580]   But it basically made all these tiers and classifications on how much you could distribute.
[00:36:36.580 --> 00:36:39.180]   Did you have to distribute it through a hyperscaler or not?
[00:36:39.180 --> 00:36:44.220]   And the whole idea was to somehow prevent these chips from getting to China.
[00:36:44.220 --> 00:36:50.140]   But what it really does is it causes us to have to compete globally with Huawei with
[00:36:50.140 --> 00:36:51.900]   one hand tied behind our back.
[00:36:51.900 --> 00:36:57.540]   And it almost guarantees a Huawei-level belt and road initiative around the world.
[00:36:57.540 --> 00:37:02.900]   And the world is going to run on Huawei AI chips, which gives them then the demand that
[00:37:02.900 --> 00:37:05.420]   they need to build a frontier AI chip.
[00:37:05.420 --> 00:37:12.200]   And so, again, well-intentioned, perhaps, by the Biden administration, but totally backfires.
[00:37:12.200 --> 00:37:17.660]   And hopefully, Howard Lutnick and this administration will throw that out.
[00:37:17.660 --> 00:37:23.220]   I think there are a remarkable amount of people in Washington on both sides of the aisle that
[00:37:23.220 --> 00:37:30.840]   have a perspective about China that they use words, "enemy," "threat," "have to win the
[00:37:30.840 --> 00:37:32.660]   AI war."
[00:37:32.660 --> 00:37:36.780]   And those terms are so loaded.
[00:37:36.780 --> 00:37:39.820]   But I think they think they can achieve something.
[00:37:39.820 --> 00:37:46.780]   And if I owned NVIDIA, my number one concern would be excessive regulation coming out of
[00:37:46.780 --> 00:37:47.780]   Washington.
[00:37:47.780 --> 00:37:49.220]   My number one concern.
[00:37:49.220 --> 00:37:50.460]   Let's shift gears here for a second.
[00:37:50.460 --> 00:37:56.380]   You talked about OpenAI losing this report that they were losing $20 billion a year.
[00:37:56.380 --> 00:38:01.740]   One thing I would just say, I'm not going to share anything that I shouldn't share.
[00:38:01.740 --> 00:38:07.220]   However, I think one always has to keep in mind, what is operating expense and what is
[00:38:07.220 --> 00:38:09.340]   capital expense?
[00:38:09.340 --> 00:38:14.460]   And there's a variable cost of serving a chat GPT query.
[00:38:14.460 --> 00:38:21.580]   And I would posit that those variable expenses are not very high, like at maturity.
[00:38:21.580 --> 00:38:29.340]   Although a deep, like a O1 Pro search or deep research could cost $20, $40, $50x the other.
[00:38:29.340 --> 00:38:30.340]   Correct.
[00:38:30.340 --> 00:38:31.340]   Correct.
[00:38:31.340 --> 00:38:34.700]   But I would just posit for you that you'll be able to come up with a variable expense
[00:38:34.700 --> 00:38:38.820]   structure using the right mix of models that will be a great margin.
[00:38:38.820 --> 00:38:42.660]   May not be as high as retrieval was for Google, but still a great margin.
[00:38:42.660 --> 00:38:47.860]   I think what people are conflating, Bill, is when you decide to spend $20 billion a
[00:38:47.860 --> 00:38:51.860]   year to build out Stargate, to build out clusters, to do all these things.
[00:38:51.860 --> 00:38:57.540]   Now, as you know, a component of that is the CapEx needed to serve the inference and a
[00:38:57.540 --> 00:39:01.780]   component of that is CapEx to build future products, right?
[00:39:01.780 --> 00:39:06.980]   And so, for example, if we're looking at Facebook or we're looking at Google or we're looking
[00:39:06.980 --> 00:39:12.700]   at Microsoft, Microsoft, I think, is spending 80% of their free cash flow, right, on CapEx.
[00:39:12.700 --> 00:39:15.740]   Now, we don't quote that as their profitability.
[00:39:15.740 --> 00:39:20.580]   They have their net income and then they have their net income less CapEx.
[00:39:20.580 --> 00:39:21.620]   I would keep that in mind.
[00:39:21.620 --> 00:39:29.060]   But what I would say is these folks are very committed to continue to invest aggressively
[00:39:29.060 --> 00:39:31.420]   in a future that they see as big.
[00:39:31.420 --> 00:39:39.660]   What we heard from Satya on the Dworkish podcast, right, what many are characterizing as a pushback
[00:39:39.660 --> 00:39:44.100]   against these high levels of spending.
[00:39:44.100 --> 00:39:50.420]   I think of my fleet, even, as a ratio of the AI accelerator to storage to compute.
[00:39:50.420 --> 00:39:53.900]   And at scale, you've got to grow it.
[00:39:53.900 --> 00:40:02.060]   And so, that infrastructure need for the world is just going to be exponentially growing,
[00:40:02.060 --> 00:40:03.060]   right?
[00:40:03.060 --> 00:40:08.940]   So, in fact, it's mana from heaven to have these AI workloads because, guess what, they're
[00:40:08.940 --> 00:40:11.500]   more hungry for more compute, right?
[00:40:11.500 --> 00:40:13.900]   Not just for training, but we now know for test time.
[00:40:13.900 --> 00:40:14.900]   And as I said, test time.
[00:40:14.900 --> 00:40:15.900]   Like, here's an interesting thing.
[00:40:15.900 --> 00:40:21.060]   When you think of an AI agent, it turns out the AI agents is going to exponentially increase
[00:40:21.060 --> 00:40:26.820]   compute usage because you're now not even bound by just one human invoking a program.
[00:40:26.820 --> 00:40:30.980]   It's one human invoking programs that invoke lots more programs.
[00:40:30.980 --> 00:40:36.140]   And so, that's going to create massive, massive demand and scale for compute infrastructure.
[00:40:36.140 --> 00:40:40.020]   So, our hyperscale business, Azure business, I think that's like, and other hyperscalers,
[00:40:40.020 --> 00:40:41.740]   I think that's a big thing.
[00:40:41.740 --> 00:40:45.300]   And I think on the pod, he reiterated, "We're going to spend $80 billion this year.
[00:40:45.300 --> 00:40:46.980]   We'll spend more next year.
[00:40:46.980 --> 00:40:51.180]   So, there's not a world in which we're just going to have unlimited, unconstrained spending."
[00:40:51.180 --> 00:40:58.340]   Now, this week, we also saw rumored that Meta is out shopping for a campus, a data center
[00:40:58.340 --> 00:40:59.420]   campus.
[00:40:59.420 --> 00:41:04.780]   The rumored amount is $200 billion, capable of building six to eight gigawatts.
[00:41:04.780 --> 00:41:09.260]   Now, that sounds a lot like Stargate, which is kind of in that six to eight gigawatts.
[00:41:09.260 --> 00:41:13.700]   Microsoft, I think, has five gigs installed, probably is going to build a few.
[00:41:13.700 --> 00:41:14.700]   Worldwide.
[00:41:14.700 --> 00:41:15.700]   What?
[00:41:15.700 --> 00:41:16.700]   Five gig worldwide.
[00:41:16.700 --> 00:41:17.700]   Is that what you mean?
[00:41:17.700 --> 00:41:18.700]   Correct.
[00:41:18.700 --> 00:41:19.700]   Yeah.
[00:41:19.700 --> 00:41:20.700]   And going to build more.
[00:41:20.700 --> 00:41:24.820]   So, again, it seems to me that if you want to be in the group of five or six, that's
[00:41:24.820 --> 00:41:27.180]   kind of the calling card you have to have.
[00:41:27.180 --> 00:41:31.580]   You have to either have a business or the ability to raise capital, such that you can
[00:41:31.580 --> 00:41:35.140]   deploy a sufficient amount to build out that level of compute.
[00:41:35.140 --> 00:41:42.020]   Now, in the case of OpenAI, enter Masa, back to Lyft, Uber.
[00:41:42.020 --> 00:41:48.740]   And Masa's rumored to be leading a very big round, $40 billion round, with a lot, which
[00:41:48.740 --> 00:41:50.820]   we saw they announced at the White House.
[00:41:50.820 --> 00:41:52.060]   It is important.
[00:41:52.060 --> 00:41:57.420]   Many people interpreted Satya's comments as a tapping of the brakes.
[00:41:57.420 --> 00:41:58.420]   Yes.
[00:41:58.420 --> 00:41:59.420]   So tell me how you interpret it.
[00:41:59.420 --> 00:42:05.540]   Because he said, "I'm happy that some of these are leases," which I don't know any other
[00:42:05.540 --> 00:42:06.780]   way to interpret that.
[00:42:06.780 --> 00:42:08.300]   Well, there's two ways you can interpret it.
[00:42:08.300 --> 00:42:15.940]   One is he's telling you, like, "I'm hedged against this being overbuilt," or, "I'm better
[00:42:15.940 --> 00:42:20.500]   off canceling a lease than sitting on infrastructure."
[00:42:20.500 --> 00:42:22.340]   I would say it even a little bit more.
[00:42:22.340 --> 00:42:23.340]   Let's be honest.
[00:42:23.340 --> 00:42:28.460]   Satya said last June, we talked about on this pod, that it was very likely that at some
[00:42:28.460 --> 00:42:31.820]   point there would be a supply and a demand mismatch.
[00:42:31.820 --> 00:42:36.660]   And you had to build a resilient company that could go through a zone of disillusionment.
[00:42:36.660 --> 00:42:41.060]   So he basically said the reckoning is coming at some point in time.
[00:42:41.060 --> 00:42:42.700]   And so now he goes on Dworkish.
[00:42:42.700 --> 00:42:47.540]   He kind of sounds like he's tapping the brakes a little bit, you know.
[00:42:47.540 --> 00:42:53.980]   And so I think that the interpretations of that should not be that he doesn't believe
[00:42:53.980 --> 00:42:54.980]   in AI.
[00:42:54.980 --> 00:42:58.860]   I think he very much believes in AI, but he's running a public company.
[00:42:58.860 --> 00:43:02.460]   And I think that he's made commitments to his shareholders, and he's saying, "Listen,
[00:43:02.460 --> 00:43:07.820]   I need to see a certain amount of inference revenue in real time to justify that level
[00:43:07.820 --> 00:43:08.820]   of capex."
[00:43:08.820 --> 00:43:09.820]   Yep.
[00:43:09.820 --> 00:43:12.220]   Look, I mean, I think everyone believes in AI.
[00:43:12.220 --> 00:43:19.060]   This amount of spend is something we've never seen before.
[00:43:19.060 --> 00:43:22.860]   That's why I've said, you know, that it's like better than watching "Secession."
[00:43:22.860 --> 00:43:27.940]   This is just like, it's a massive sport of kings.
[00:43:27.940 --> 00:43:33.500]   And I think some of the things, whether it's the 20 billion losses, or Satya's saying he's
[00:43:33.500 --> 00:43:34.740]   glad he's got leases.
[00:43:34.740 --> 00:43:42.500]   Some of these might be part of an information war with other players trying to talk capital
[00:43:42.500 --> 00:43:43.500]   in or out.
[00:43:43.500 --> 00:43:44.500]   For sure.
[00:43:44.500 --> 00:43:46.340]   And it's a high stakes game.
[00:43:46.340 --> 00:43:47.340]   It's fun to watch.
[00:43:47.340 --> 00:43:53.020]   Well, I think resiliency, business model resiliency is going to be critical here.
[00:43:53.020 --> 00:43:54.340]   And what do I mean by that?
[00:43:54.340 --> 00:44:02.460]   It means liquidity, because we know in the internet there was a zone of disillusionment.
[00:44:02.460 --> 00:44:04.920]   We know in social, there was a zone of disillusionment.
[00:44:04.920 --> 00:44:07.700]   We know in cloud, there was a zone of disillusionment, right?
[00:44:07.700 --> 00:44:09.700]   A period, what do I mean by that?
[00:44:09.700 --> 00:44:15.020]   A period where the prices and the spend got ahead of the revenue, right?
[00:44:15.020 --> 00:44:21.920]   And given the level of competition, some people describe as a prisoner's dilemma, right?
[00:44:21.920 --> 00:44:26.800]   In the case of Google and Meta, they literally have a printing press in the back room spitting
[00:44:26.800 --> 00:44:28.880]   out billion dollar bills, right?
[00:44:28.880 --> 00:44:30.320]   So they are resilient.
[00:44:30.320 --> 00:44:32.080]   Microsoft, resilient, right?
[00:44:32.080 --> 00:44:36.340]   In the case of OpenAI, they have to raise money, right?
[00:44:36.340 --> 00:44:39.280]   So you need to have a big stack behind you.
[00:44:39.280 --> 00:44:43.120]   In the case of X, right, they need to be able to raise capital.
[00:44:43.120 --> 00:44:46.000]   I think there were some numbers out there last week.
[00:44:46.000 --> 00:44:49.400]   Obviously, Elon is the wealthiest person on the planet.
[00:44:49.400 --> 00:44:51.340]   He can sell shares in some things.
[00:44:51.340 --> 00:44:57.880]   But I think the most powerful thing Elon has is a global belief in him as an entrepreneur,
[00:44:57.880 --> 00:45:02.900]   which gives him an opportunity to raise capital from sovereigns around the world.
[00:45:02.900 --> 00:45:06.840]   And so if you said, is this still an open sport, I'd say, no way, right?
[00:45:06.840 --> 00:45:09.620]   I don't know anybody else other than Elon and Sam at this point.
[00:45:09.620 --> 00:45:11.120]   Although DeepSeek surprised everybody.
[00:45:11.120 --> 00:45:14.520]   Well, I'm saying if you're going to play that game, right?
[00:45:14.520 --> 00:45:22.100]   To remind you, DeepSeek spent more than the amount reported in their last training run.
[00:45:22.100 --> 00:45:27.020]   But even more importantly, to serve an explosive amount of inference, they would have to spend
[00:45:27.020 --> 00:45:28.020]   a lot of money to build--
[00:45:28.020 --> 00:45:32.460]   I want to make a point that we'll probably come back to much later.
[00:45:32.460 --> 00:45:43.500]   But when you have a scenario that has this much ambition, and this much competition,
[00:45:43.500 --> 00:45:52.100]   and this much CapEx as part of the game, it's easy to lose sight of the microeconomics.
[00:45:52.100 --> 00:45:56.220]   It's easy to lose sight of the unit economics.
[00:45:56.220 --> 00:46:02.980]   So if you're an anthropic, and you've got training credits over here, and you've got
[00:46:02.980 --> 00:46:09.340]   CapEx, and you do your-- am I thinking about depreciation or that when I say, oh, this
[00:46:09.340 --> 00:46:13.300]   is profitable, or when I price my API product?
[00:46:13.300 --> 00:46:19.700]   And you've got this razor edge pricing thing that I've never seen before.
[00:46:19.700 --> 00:46:20.700]   Explain what you mean by that.
[00:46:20.700 --> 00:46:28.880]   I mean, the price difference between today's model and yesterday's model is 20x.
[00:46:28.880 --> 00:46:33.940]   So it's a fast depreciating asset the second you're off the frontier.
[00:46:33.940 --> 00:46:34.940]   Yes.
[00:46:34.940 --> 00:46:35.940]   Yes.
[00:46:35.940 --> 00:46:39.300]   And so it's just-- it's a dangerous-- these are all traps.
[00:46:39.300 --> 00:46:40.300]   Yeah.
[00:46:40.300 --> 00:46:44.340]   And it makes it, once again, fascinating.
[00:46:44.340 --> 00:46:48.820]   Maybe we can transition to the public markets a bit, but there's a lot of talk that we're
[00:46:48.820 --> 00:46:50.860]   going to see a CoreWeave filing.
[00:46:50.860 --> 00:46:55.540]   And I'm just excited to see the numbers and to piece together more of the information.
[00:46:55.540 --> 00:46:56.540]   Yeah.
[00:46:56.540 --> 00:46:59.100]   There's a rumor out there that the CoreWeave is going to file an IPO, and so you can see
[00:46:59.100 --> 00:47:00.100]   the numbers.
[00:47:00.100 --> 00:47:02.900]   Well, I just want to underscore this point that you just made, though.
[00:47:02.900 --> 00:47:12.660]   Because I mean, and there is some rhyming to Masa coming back into the scene here, right?
[00:47:12.660 --> 00:47:18.440]   Masa is one of the greats of this industry of the last 25 years.
[00:47:18.440 --> 00:47:24.300]   But I think people would also describe him as somebody who's a bit of a gambler and places
[00:47:24.300 --> 00:47:26.860]   gigantic bets, right?
[00:47:26.860 --> 00:47:31.500]   And some people would say that he's a total visionary, and other people would say he's
[00:47:31.500 --> 00:47:34.220]   just not price discriminating, right?
[00:47:34.220 --> 00:47:40.140]   But clearly, he's shoving all in with open-- I don't think he knows any-- I don't think
[00:47:40.140 --> 00:47:41.980]   he has any other way of operating.
[00:47:41.980 --> 00:47:42.980]   Right.
[00:47:42.980 --> 00:47:52.060]   And so I think the point being that we're at this moment in time where the danger for
[00:47:52.060 --> 00:47:54.720]   the company-- I just want to underscore what you said.
[00:47:54.720 --> 00:48:00.600]   The danger for the company of getting this volume of capital is that it's hard to focus
[00:48:00.600 --> 00:48:07.420]   on really building the muscle and the grit and the ingenuity on how to drive unit economics.
[00:48:07.420 --> 00:48:10.260]   Think about what Elon had to do at Tesla, right?
[00:48:10.260 --> 00:48:12.400]   Because capital was hard to come by.
[00:48:12.400 --> 00:48:15.740]   So he had to figure out how to make money on every damn car.
[00:48:15.740 --> 00:48:21.100]   How do I take costs out of the manufacturing at every single stage of production?
[00:48:21.100 --> 00:48:24.700]   And when you have excess capital, right, you lose that discipline.
[00:48:24.700 --> 00:48:25.980]   You don't build that muscle.
[00:48:25.980 --> 00:48:32.380]   And so I think it's an important admonition for the board and leadership at OpenAI and
[00:48:32.380 --> 00:48:37.340]   all these companies to hear that, sure, it's one thing to invest aggressively in the future,
[00:48:37.340 --> 00:48:40.860]   but you better make sure that along the way, your unit economics work.
[00:48:40.860 --> 00:48:41.860]   No doubt.
[00:48:41.860 --> 00:48:44.060]   I know you've been thinking a lot-- let's switch gears.
[00:48:44.060 --> 00:48:49.260]   You've been thinking a lot about Doge and if it happens, what it means for the capital
[00:48:49.260 --> 00:48:50.260]   markets.
[00:48:50.260 --> 00:48:51.260]   Yeah.
[00:48:51.260 --> 00:48:55.500]   And it's interesting to even say if it happens, because as I watch the press every day, there's
[00:48:55.500 --> 00:48:58.900]   an equal number of people that say, oh, this is going to take out all these costs.
[00:48:58.900 --> 00:49:03.720]   And there's other people that say, oh, they're just saying things, but they're not actually
[00:49:03.720 --> 00:49:04.720]   going to happen.
[00:49:04.720 --> 00:49:05.720]   Yeah.
[00:49:05.720 --> 00:49:06.720]   Yeah.
[00:49:06.720 --> 00:49:11.540]   You and I said some-- so I think on our pod on like February 6 or something, when you
[00:49:11.540 --> 00:49:15.940]   asked me about the markets, I said, hey, we have peak political uncertainty, right,
[00:49:15.940 --> 00:49:17.740]   because we have a lot of things changing.
[00:49:17.740 --> 00:49:19.500]   We have peak economic uncertainty.
[00:49:19.500 --> 00:49:23.460]   And that's not just Doge, because-- Right, because we have tariffs and other things.
[00:49:23.460 --> 00:49:28.900]   And I said that we have peak technology uncertainty, i.e., it's hard to predict the future, what
[00:49:28.900 --> 00:49:31.660]   software company is going to be worth what in five years.
[00:49:31.660 --> 00:49:34.580]   And that causes discount rates to go up.
[00:49:34.580 --> 00:49:36.860]   It causes multiples to come down.
[00:49:36.860 --> 00:49:41.900]   And I said I was surprised how resilient the market was in the face of all this uncertainty.
[00:49:41.900 --> 00:49:47.420]   Well, now I would argue we're starting to see a few cracks in that.
[00:49:47.420 --> 00:49:54.060]   And so if you look at this chart, Bill, it's really the NASDAQ since the election.
[00:49:54.060 --> 00:49:55.260]   And we ran way up.
[00:49:55.260 --> 00:49:58.660]   The NASDAQ was up as high as 10% post-election.
[00:49:58.660 --> 00:50:03.260]   And now we've come off four or five points from that high.
[00:50:03.260 --> 00:50:08.300]   But we're still four or five points higher than we were on the night of the election.
[00:50:08.300 --> 00:50:13.260]   And so one thing I just have been thinking a lot about, and I've been talking a lot about,
[00:50:13.260 --> 00:50:18.740]   is this difference between stimulus and austerity.
[00:50:18.740 --> 00:50:23.540]   Over the last three or four years, we had massive stimulus into the economy.
[00:50:23.540 --> 00:50:29.540]   Now, you and I both supported it in March and April of 2020.
[00:50:29.540 --> 00:50:34.500]   Right when we were in the depths of COVID, you had to prevent the economy from coming
[00:50:34.500 --> 00:50:36.180]   to a screeching halt.
[00:50:36.180 --> 00:50:41.360]   And so the Fed went all in and Congress went all in in order to save the economy.
[00:50:41.360 --> 00:50:46.460]   But then we also were very critical that the Fed moved way too slow.
[00:50:46.460 --> 00:50:49.420]   The second stimulus package was way too large.
[00:50:49.420 --> 00:50:51.340]   And it led to this runaway inflation.
[00:50:51.340 --> 00:50:54.100]   We saw inflation hit 9%.
[00:50:54.100 --> 00:51:00.260]   But the one thing that all of that monetary liquidity did to the system is it caused risk
[00:51:00.260 --> 00:51:03.740]   assets to go up in value.
[00:51:03.740 --> 00:51:07.680]   And now we're in this period where we're talking about not adding a trillion and a half of
[00:51:07.680 --> 00:51:12.220]   liquidity to the system, we're talking about pulling a trillion and a half out.
[00:51:12.220 --> 00:51:13.740]   Now, what do I mean by that?
[00:51:13.740 --> 00:51:19.780]   Okay, so last year, we had $56 billion of tariffs imposed on other countries.
[00:51:19.780 --> 00:51:22.260]   That's the amount of revenue we collected from tariffs.
[00:51:22.260 --> 00:51:25.420]   We're talking about that going to $500 billion.
[00:51:25.420 --> 00:51:27.420]   So 10x in the amount of tariffs.
[00:51:27.420 --> 00:51:31.100]   Well, we know that some of those will be eaten by producers, right?
[00:51:31.100 --> 00:51:35.100]   The company that's producing something in China will just take a lower margin.
[00:51:35.100 --> 00:51:39.540]   But we know a lot of those will be felt by U.S. consumers who just end up paying higher
[00:51:39.540 --> 00:51:44.340]   prices for their Dell computer because Dell passes along the price increase of the computer
[00:51:44.340 --> 00:51:47.340]   made in Mexico, as an example.
[00:51:47.340 --> 00:51:48.440]   So that's $500 billion.
[00:51:48.440 --> 00:51:53.300]   On the other hand, I think Doge, there's no doubt in my mind at this point in time, and
[00:51:53.300 --> 00:51:59.780]   we'll show this chart of the likely spending cuts, they're not only making big cuts, and
[00:51:59.780 --> 00:52:05.820]   the president has now just last week said he wants Elon to be more aggressive, right?
[00:52:05.820 --> 00:52:11.020]   They sent this email out to every employee that said, you know, respond back to us or
[00:52:11.020 --> 00:52:13.740]   you'll be deemed, you know, to have resigned.
[00:52:13.740 --> 00:52:15.900]   Now they're giving them more, you know, shots on goal.
[00:52:15.900 --> 00:52:20.140]   But the message is very clear that I think there's going to be a downsizing of the federal
[00:52:20.140 --> 00:52:24.180]   government to the tune of, let's call it 40 or 50%.
[00:52:24.180 --> 00:52:27.660]   Now, a lot of people have been giving a lot of grief to Doge.
[00:52:27.660 --> 00:52:33.180]   But I remind you, and I tweeted this the other day, that Bill Clinton, right, did Doge in
[00:52:33.180 --> 00:52:34.180]   the late '90s.
[00:52:34.180 --> 00:52:35.180]   Yeah.
[00:52:35.180 --> 00:52:37.820]   I don't know the exact percentage of federal employees they let go.
[00:52:37.820 --> 00:52:40.020]   It was like between 10 and 20%.
[00:52:40.020 --> 00:52:45.340]   But we had a balanced budget, you know, in three fiscal years, we had a $230 billion
[00:52:45.340 --> 00:52:46.340]   surplus.
[00:52:46.340 --> 00:52:50.220]   Now, it was helped by the internet, but now we're going to be helped by AI.
[00:52:50.220 --> 00:52:54.420]   So like, I think that you can see some replay of that.
[00:52:54.420 --> 00:52:59.020]   But it does mean that we're probably going to take $500 billion to $1 trillion out of
[00:52:59.020 --> 00:53:01.800]   federal spending over the course of the next couple of years.
[00:53:01.800 --> 00:53:08.480]   And all I'm suggesting is that austerity has the reversed impact of liquidity from government
[00:53:08.480 --> 00:53:09.480]   into the system.
[00:53:09.480 --> 00:53:14.940]   So if you think about, go back to our GDP calculation, right, in macroeconomics, C plus
[00:53:14.940 --> 00:53:19.060]   I plus G, where G is the amount of money the government's spending.
[00:53:19.060 --> 00:53:22.180]   Well, the amount of money the government's spending is going down.
[00:53:22.180 --> 00:53:26.060]   So tariffs is a headwind to the economy, and this austerity out of the government.
[00:53:26.060 --> 00:53:28.860]   Now, I am 100% in agreement.
[00:53:28.860 --> 00:53:33.500]   This is the short-term shock therapy we need in order to get our fiscal house in order,
[00:53:33.500 --> 00:53:34.500]   right?
[00:53:34.500 --> 00:53:37.700]   But you've got to think about this as, you know, somebody says, hey, you're out of shape,
[00:53:37.700 --> 00:53:39.060]   you're going to have a heart attack.
[00:53:39.060 --> 00:53:42.180]   You've got to, you know, you've got to take this medicine, this short-term pain, you've
[00:53:42.180 --> 00:53:47.540]   got to work out every day, you've got to get fit, right, in order to avoid the heart attack.
[00:53:47.540 --> 00:53:48.980]   You would do it every day of the week.
[00:53:48.980 --> 00:53:51.620]   We need to get fit in order to avoid bankruptcy.
[00:53:51.620 --> 00:53:55.040]   And all I'm suggesting is-- It might affect markets.
[00:53:55.040 --> 00:53:56.700]   That it might affect markets.
[00:53:56.700 --> 00:54:02.920]   So markets may, in fact, right, my risk profile is lower than our standard risk profile.
[00:54:02.920 --> 00:54:03.920]   What do I mean by that?
[00:54:03.920 --> 00:54:09.700]   Very simply, you know, I own half as much as I would normally own at a point in time.
[00:54:09.700 --> 00:54:13.900]   Now, do I think that's because the future, you know, is bleak?
[00:54:13.900 --> 00:54:17.080]   No, I believe aggressively in the future.
[00:54:17.080 --> 00:54:20.660]   But I think we're going to have to take a little bit of short-term pain, which means
[00:54:20.660 --> 00:54:26.400]   we could see just a random run of the mill, 10% to 15% drawdown, right, in the markets
[00:54:26.400 --> 00:54:30.180]   while the market gets its head around the fact that the economy is going to grow a little
[00:54:30.180 --> 00:54:31.180]   slower.
[00:54:31.180 --> 00:54:35.340]   When the economy grows a little slower, that means companies grow a little bit slower.
[00:54:35.340 --> 00:54:39.420]   When they grow slower, you know, the earnings goes down and the multiple goes down.
[00:54:39.420 --> 00:54:41.280]   Let me ask you this question.
[00:54:41.280 --> 00:54:47.340]   When Elon went into Twitter, one of the stories that came out was that they found there were
[00:54:47.340 --> 00:54:52.440]   software licenses for a whole bunch of people that they weren't using and that they cut
[00:54:52.440 --> 00:54:53.980]   that dramatically.
[00:54:53.980 --> 00:55:02.660]   Do you anticipate that one of the outcomes of Doge will be a, you know, obviously a headwind
[00:55:02.660 --> 00:55:08.980]   for a bunch of companies that have sold software and/or services into the market?
[00:55:08.980 --> 00:55:09.980]   100%.
[00:55:09.980 --> 00:55:11.260]   I mean, like, there's just no way around it.
[00:55:11.260 --> 00:55:14.620]   You know, if you go from 3 million federal employees to a million and a half federal
[00:55:14.620 --> 00:55:18.820]   employees, then you don't need as many licenses, you don't have as much cloud consumption,
[00:55:18.820 --> 00:55:19.820]   right?
[00:55:19.820 --> 00:55:23.860]   And so if you think about the multiplier, right, you take the federal government or
[00:55:23.860 --> 00:55:28.240]   federal person's salary, employee's salary, now you have all the healthcare and benefits
[00:55:28.240 --> 00:55:31.940]   and pension and all the other stuff, and then you have all the ancillary spend.
[00:55:31.940 --> 00:55:38.220]   So I won't say the exact company, but I talked to an airline the other day, and at this airline,
[00:55:38.220 --> 00:55:43.180]   their number of government tickets sold year to date is down 50%.
[00:55:43.180 --> 00:55:44.900]   Already impacted.
[00:55:44.900 --> 00:55:45.900]   50%.
[00:55:45.900 --> 00:55:51.100]   Yeah, because they said, we don't want, you know, you traveling, we want you in the office
[00:55:51.100 --> 00:55:53.220]   every day and all this other stuff.
[00:55:53.220 --> 00:55:55.380]   And so this airline has already been impacted.
[00:55:55.380 --> 00:56:00.780]   So I think everybody in the ecosystem, if you have revenue line items, if you're a business,
[00:56:00.780 --> 00:56:04.540]   if you're a public company, you have revenue line items from the federal government.
[00:56:04.540 --> 00:56:07.980]   It's not just that the rate of growth is going to slow.
[00:56:07.980 --> 00:56:11.020]   It's that they're actually going to be negative on a year on year basis.
[00:56:11.020 --> 00:56:16.160]   Now again, I happen to think this is a generally a good sacrifice for us to make.
[00:56:16.160 --> 00:56:17.380]   Those are our tax dollars.
[00:56:17.380 --> 00:56:18.960]   There is no government money.
[00:56:18.960 --> 00:56:22.700]   This is our money that's being, you know, being consumed.
[00:56:22.700 --> 00:56:27.580]   But I don't think the public markets or investors generally, and certainly not Silicon Valley,
[00:56:27.580 --> 00:56:29.820]   has kind of got in their head around what this means.
[00:56:29.820 --> 00:56:30.820]   Now what's the-
[00:56:30.820 --> 00:56:35.660]   Well, and in fact, I would say, like ironically, this happens quite a bit in our world, but
[00:56:35.660 --> 00:56:41.940]   the Silicon Valley and the venture capitalists have just gotten comfortable with backing
[00:56:41.940 --> 00:56:43.580]   companies that sell to government.
[00:56:43.580 --> 00:56:44.580]   Exactly.
[00:56:44.580 --> 00:56:45.580]   We see a lot of that.
[00:56:45.580 --> 00:56:46.880]   It's an interesting timing.
[00:56:46.880 --> 00:56:47.880]   People get excited about it.
[00:56:47.880 --> 00:56:52.620]   Well, you saw what happened to Palantir stock the other day when, you know, the president
[00:56:52.620 --> 00:56:59.300]   directed his cabinet members, Secretary of Defense, to find 8% cuts in the Department
[00:56:59.300 --> 00:57:01.980]   of Defense every year, right?
[00:57:01.980 --> 00:57:05.340]   And so this austerity, again, is real.
[00:57:05.340 --> 00:57:10.860]   Now, that probably means we're going to have a rotation of money out of, like, the less
[00:57:10.860 --> 00:57:15.220]   technologically innovative folks into the more technologically innovative folks.
[00:57:15.220 --> 00:57:16.220]   But you know-
[00:57:16.220 --> 00:57:17.220]   He went further.
[00:57:17.220 --> 00:57:18.380]   Trump went further.
[00:57:18.380 --> 00:57:22.180]   He suggested in one thing, which I was really blown away by this.
[00:57:22.180 --> 00:57:25.300]   I actually thought it was kind of the most interesting thing he's done.
[00:57:25.300 --> 00:57:31.020]   He suggested to Xi that China and America should both cut their military budgets in
[00:57:31.020 --> 00:57:32.020]   half.
[00:57:32.020 --> 00:57:33.020]   Yes.
[00:57:33.020 --> 00:57:34.020]   Now, maybe that's provocative.
[00:57:34.020 --> 00:57:35.020]   Maybe it's-
[00:57:35.020 --> 00:57:36.020]   By the way-
[00:57:36.020 --> 00:57:37.020]   That'd be an amazing idea.
[00:57:37.020 --> 00:57:38.020]   I thought that was extraordinary.
[00:57:38.020 --> 00:57:40.500]   Back to the, you know, this idea, you know, we've been-
[00:57:40.500 --> 00:57:42.860]   We both blow up the world many times over, so-
[00:57:42.860 --> 00:57:43.860]   Right.
[00:57:43.860 --> 00:57:46.860]   There's a certain camp of folks, right?
[00:57:46.860 --> 00:57:50.220]   And I think Mearsheimer is in this camp, right?
[00:57:50.220 --> 00:57:54.740]   Which is great power politics and, like, you just got to build, build, and build, and,
[00:57:54.740 --> 00:57:57.460]   you know, eventually you're going to have a war or something like this.
[00:57:57.460 --> 00:58:02.840]   Or maybe the fact that you have these stockpiles deters, you know, the ultimate war.
[00:58:02.840 --> 00:58:10.060]   One thing that is just fascinating, I've never heard an American president in my lifetime
[00:58:10.060 --> 00:58:14.900]   suggest that he wanted to sit down at a table with China and Russia and talk about they
[00:58:14.900 --> 00:58:17.900]   could cut their- they could collectively cut their military spending in half.
[00:58:17.900 --> 00:58:18.900]   Right.
[00:58:18.900 --> 00:58:23.100]   So, from an entrepreneur perspective, like, isn't it- it caused me to stop in my tracks
[00:58:23.100 --> 00:58:25.340]   and be like, "Hmm, that's an interesting idea."
[00:58:25.340 --> 00:58:28.860]   I thought it was the coolest thing he's thought of.
[00:58:28.860 --> 00:58:29.860]   That's an interesting idea.
[00:58:29.860 --> 00:58:35.940]   Well, I will tell you back on the public markets, the other interesting thing here, Warren Buffett,
[00:58:35.940 --> 00:58:38.780]   you know, just put out his annual letter, he's going to have his annual meeting coming
[00:58:38.780 --> 00:58:44.140]   up here, has a $400 billion cash stockpile, has been liquidating stocks-
[00:58:44.140 --> 00:58:45.140]   Yeah.
[00:58:45.140 --> 00:58:46.140]   Right?
[00:58:46.380 --> 00:58:48.860]   -for a while, in ages.
[00:58:48.860 --> 00:58:54.680]   Stan Druckenmiller, Howard Marks, Stevie Cohen came out over the weekend and said, "I'm nervous
[00:58:54.680 --> 00:58:58.580]   about the markets," for the same reason that we were talking about a month ago.
[00:58:58.580 --> 00:59:02.140]   So I think there is a growing chorus of players.
[00:59:02.140 --> 00:59:04.820]   Now, what's the flip side to this?
[00:59:04.820 --> 00:59:10.500]   Well, since Trump's been elected, the cost of a mortgage or credit card or etc. is starting
[00:59:10.500 --> 00:59:11.500]   to come down.
[00:59:11.500 --> 00:59:12.500]   Why is that?
[00:59:12.500 --> 00:59:13.500]   Right?
[00:59:13.500 --> 00:59:14.500]   There are two reasons.
[00:59:14.500 --> 00:59:19.220]   The first reason, I think, is because we're saying, okay, the economy is going to slow
[00:59:19.220 --> 00:59:20.220]   a little bit.
[00:59:20.220 --> 00:59:26.940]   And if the economy slows, equities, as an investment, are a little less positive relative
[00:59:26.940 --> 00:59:27.940]   to a bond.
[00:59:27.940 --> 00:59:29.780]   So you rotate that into cash.
[00:59:29.780 --> 00:59:33.620]   And when the cash is sitting on the sideline, it's invested in a U.S. treasury.
[00:59:33.620 --> 00:59:38.500]   Just to put it in perspective, and the only anecdote I really ever hear about this is,
[00:59:38.500 --> 00:59:41.700]   well, China doesn't want to own our treasuries anymore.
[00:59:41.700 --> 00:59:45.660]   China buys 3% of our treasuries annually.
[00:59:45.660 --> 00:59:46.660]   It's tiny.
[00:59:46.660 --> 00:59:51.260]   They used to buy, 10 years ago, they bought 12% of our treasuries, and everybody panicked
[00:59:51.260 --> 00:59:53.020]   that they were too big a buyer.
[00:59:53.020 --> 00:59:55.460]   So what I see is just the opposite.
[00:59:55.460 --> 01:00:00.340]   Every sovereign around the world and every domestic investor who's starting to put more
[01:00:00.340 --> 01:00:04.680]   money into cash, who's hedging a little bit, all of that's going into U.S. treasuries.
[01:00:04.680 --> 01:00:09.340]   So I just think that one should brace over the next three months.
[01:00:09.340 --> 01:00:13.900]   I think these tariffs are very real, they're structural, and the president is committed
[01:00:13.900 --> 01:00:14.900]   to them.
[01:00:14.900 --> 01:00:18.860]   I think, number two, the reconciliation package is now rolling.
[01:00:18.860 --> 01:00:24.180]   And I think they are very committed to balancing the budget within this president's term.
[01:00:24.180 --> 01:00:29.300]   And the only way you balance the budget is a trillion dollars has to come out of spending.
[01:00:29.300 --> 01:00:34.220]   Remember, 2019 baseline, we were spending about $5 trillion.
[01:00:34.220 --> 01:00:36.980]   The COVID high, $7 trillion.
[01:00:36.980 --> 01:00:42.400]   We got to get that back down to at least $6 trillion, probably to $5.75 if you're going
[01:00:42.400 --> 01:00:43.740]   to balance the budget.
[01:00:43.740 --> 01:00:47.340]   That means a trillion out in a year, that's austerity, and that's going to be a headwind
[01:00:47.340 --> 01:00:48.340]   to the economy.
[01:00:48.340 --> 01:00:49.340]   But it's the right thing to do.
[01:00:49.340 --> 01:00:50.340]   OK.
[01:00:50.340 --> 01:00:56.100]   That's a tough note to end on, so I'll switch to something more positive.
[01:00:56.100 --> 01:01:00.760]   I got invited to the Golden State Warrior game on Tuesday night.
[01:01:00.760 --> 01:01:02.980]   The butler trade looks like it's working.
[01:01:02.980 --> 01:01:03.980]   It's incredible.
[01:01:03.980 --> 01:01:06.540]   Six and one, I think, since the trade.
[01:01:06.540 --> 01:01:07.540]   It's incredible.
[01:01:07.540 --> 01:01:10.160]   I was related.
[01:01:10.160 --> 01:01:15.940]   I happened to go get an invite to the banner ceremony and dinner afterward for good friend
[01:01:15.940 --> 01:01:18.020]   Andre Iguodala.
[01:01:18.020 --> 01:01:22.380]   And Steph gave an incredible speech.
[01:01:22.380 --> 01:01:27.060]   And I had Andre speak at our investor day maybe two years ago.
[01:01:27.060 --> 01:01:34.940]   And two things Steph said that really stood out to me about Andre.
[01:01:34.940 --> 01:01:41.100]   Number one, he said, there is no this without Andre, right?
[01:01:41.100 --> 01:01:47.380]   And by this, and he explained it to me, he said, he came at a moment in time, even his
[01:01:47.380 --> 01:01:52.300]   decision to come to the Warriors made us believe in ourselves.
[01:01:52.300 --> 01:01:56.420]   And then he came here and he did whatever it took.
[01:01:56.420 --> 01:02:04.260]   And the second thing he said is Andre Iguodala always put excellence over ego.
[01:02:04.260 --> 01:02:08.380]   The guy would be the first to never pound it on the bench.
[01:02:08.380 --> 01:02:11.340]   When he came off the floor, he was the first to get guys fired up.
[01:02:11.340 --> 01:02:13.740]   And Steph talked about game six in Boston.
[01:02:13.740 --> 01:02:14.860]   I remember that game.
[01:02:14.860 --> 01:02:16.740]   I was at that game.
[01:02:16.740 --> 01:02:20.860]   And I remember Andre.
[01:02:20.860 --> 01:02:23.340]   He must have played five minutes in that game.
[01:02:23.340 --> 01:02:28.220]   And he was so fired up and really willed all the players to up their game.
[01:02:28.220 --> 01:02:29.780]   And so I was so happy for him.
[01:02:29.780 --> 01:02:36.660]   Yes, and you know, me and our good friend Jason Chang, I never bet on sports.
[01:02:36.660 --> 01:02:38.100]   I never bet on sports.
[01:02:38.100 --> 01:02:39.100]   And he talks me in.
[01:02:39.100 --> 01:02:41.060]   We're at a Warriors game during the losing streak.
[01:02:41.060 --> 01:02:45.060]   And the odds are so great that they're not going to win at all.
[01:02:45.060 --> 01:02:48.060]   He talks me into placing a bet on them winning at all.
[01:02:48.060 --> 01:02:51.740]   And at the time it was like 40 to one, right, against them.
[01:02:51.740 --> 01:02:53.660]   And all of a sudden they're on this six game winning streak.
[01:02:53.660 --> 01:02:57.060]   They trade for Jimmy Butler and they may win this whole thing.
[01:02:57.060 --> 01:02:59.620]   So, you know, fingers crossed.
[01:02:59.620 --> 01:03:03.740]   It now has me with a focused mind.
[01:03:03.740 --> 01:03:04.740]   With a focused mind.
[01:03:04.740 --> 01:03:05.740]   It's great to see you.
[01:03:05.740 --> 01:03:06.740]   Great to be with you.
[01:03:06.740 --> 01:03:07.740]   Take care.
[01:03:07.740 --> 01:03:19.740]   As a reminder to everybody, just our opinions, not investment advice.

