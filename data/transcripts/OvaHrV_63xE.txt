
[00:00:00.000 --> 00:00:03.600]   they're not epistemically modest enough when it comes to existential risk.
[00:00:03.600 --> 00:00:07.360]   These very particular hypotheses about AGI, and we've got to prevent this, and
[00:00:07.360 --> 00:00:12.000]   that's where I really differ from them. I think that if AGI is a risk, it's the
[00:00:12.000 --> 00:00:16.960]   worst set of procedures that will do you in, and you can't regulate those very well at all.
[00:00:16.960 --> 00:00:21.760]   And putting everyone at your favorite tech company through this training about alignment,
[00:00:21.760 --> 00:00:28.240]   I'm not against doing that, but come on. If it's going to happen, it's like handling
[00:00:28.880 --> 00:00:33.120]   pandemic materials. It's the sloppiest people you've got to worry about, and they are not
[00:00:33.120 --> 00:00:38.720]   sitting in on your class on AGI and alignment. I'm all for things to make nuclear weapons safer,
[00:00:38.720 --> 00:00:44.080]   but it's hard to know exactly what you do. There's not some special branch of EA long-termism
[00:00:44.080 --> 00:00:48.640]   that tells you what to do in Ukraine, and those people, if anything, tend to be kind of
[00:00:48.640 --> 00:00:53.280]   under-invested in historical and cultural forms of knowledge. Plenty of people in the
[00:00:53.280 --> 00:00:58.160]   U.S. foreign policy establishment who think about all this stuff. It doesn't change the debate much.

