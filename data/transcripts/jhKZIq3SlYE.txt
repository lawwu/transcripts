
[00:00:00.000 --> 00:00:02.800]   The following is a conversation with David Sinclair.
[00:00:02.800 --> 00:00:06.240]   He's a professor in the Department of Genetics at Harvard
[00:00:06.240 --> 00:00:09.000]   and co-director of the Paul F. Glenn Center
[00:00:09.000 --> 00:00:12.320]   for the Biology of Aging at Harvard Medical School.
[00:00:12.320 --> 00:00:14.240]   He's the author of the book "Lifespan"
[00:00:14.240 --> 00:00:16.920]   and co-founder of several biotech companies.
[00:00:16.920 --> 00:00:20.840]   He works on turning age into an engineering problem
[00:00:20.840 --> 00:00:23.960]   and solving it, driven by a vision of a world
[00:00:23.960 --> 00:00:26.000]   where billions of people can live much longer
[00:00:26.000 --> 00:00:28.180]   and much healthier lives.
[00:00:28.180 --> 00:00:30.380]   Quick mention of our sponsors,
[00:00:30.380 --> 00:00:33.400]   Onnit, Clear, National Instruments,
[00:00:33.400 --> 00:00:36.840]   and I, SimpliSafe and Linode.
[00:00:36.840 --> 00:00:40.040]   Check them out in the description to support this podcast.
[00:00:40.040 --> 00:00:42.780]   As a side note, let me say that longevity research
[00:00:42.780 --> 00:00:45.880]   challenges us to think how science and engineering
[00:00:45.880 --> 00:00:47.660]   will change society.
[00:00:47.660 --> 00:00:50.400]   Imagine if we can live 100,000 years,
[00:00:50.400 --> 00:00:51.920]   even under controlled conditions,
[00:00:51.920 --> 00:00:53.720]   like in a spaceship, say,
[00:00:53.720 --> 00:00:55.960]   then suddenly a trip to Alpha Centauri
[00:00:55.960 --> 00:00:58.840]   that is 4.37 light years away
[00:00:58.840 --> 00:01:01.280]   takes a single human lifespan.
[00:01:01.280 --> 00:01:05.160]   And on the psychological, maybe even philosophical level,
[00:01:05.160 --> 00:01:08.720]   as the horizons of death drifts farther into the distance,
[00:01:08.720 --> 00:01:11.120]   how will our search for meaning change?
[00:01:11.120 --> 00:01:13.080]   Does meaning require death
[00:01:13.080 --> 00:01:15.720]   or does it merely require struggle?
[00:01:15.720 --> 00:01:18.360]   Reprogramming our biology will require us
[00:01:18.360 --> 00:01:21.440]   to delve deeper into understanding the human mind
[00:01:21.440 --> 00:01:23.680]   and the robot mind.
[00:01:23.680 --> 00:01:26.560]   Both of these efforts are as exciting of a journey
[00:01:26.560 --> 00:01:28.160]   as I could imagine.
[00:01:28.160 --> 00:01:30.320]   This is the Lex Friedman Podcast
[00:01:30.320 --> 00:01:33.400]   and here is my conversation with David Sinclair.
[00:01:33.400 --> 00:01:38.320]   I usually feel like the same person when I was 12.
[00:01:38.320 --> 00:01:42.460]   Like when I, right now, as I think about myself,
[00:01:42.460 --> 00:01:45.040]   I feel like exactly the same person
[00:01:45.040 --> 00:01:46.400]   that I was when I was 12.
[00:01:46.400 --> 00:01:51.400]   And yet, I am getting older, both body and mind,
[00:01:52.040 --> 00:01:54.280]   and still feel like time hasn't passed at all.
[00:01:54.280 --> 00:01:56.920]   Do you feel this tension in yourself
[00:01:56.920 --> 00:02:01.040]   that you're the same person and yet you're aging?
[00:02:01.040 --> 00:02:04.680]   - Yeah, I have this tension that I'm still a kid.
[00:02:04.680 --> 00:02:05.920]   But that helps in my career.
[00:02:05.920 --> 00:02:07.960]   Scientists need to have a wonder about the world
[00:02:07.960 --> 00:02:09.800]   and you don't wanna grow up.
[00:02:09.800 --> 00:02:11.700]   12-year-olds, and even younger,
[00:02:11.700 --> 00:02:13.400]   I would say six, seven-year-olds,
[00:02:13.400 --> 00:02:16.580]   I've still got that boy in me and I can look at things.
[00:02:16.580 --> 00:02:18.260]   It's a gift, I think, that I can see things
[00:02:18.260 --> 00:02:20.000]   for the first time if I choose to
[00:02:20.000 --> 00:02:23.160]   and then explain them as I would to a six-year-old
[00:02:23.160 --> 00:02:24.960]   'cause I am that mentally.
[00:02:24.960 --> 00:02:26.720]   But on the other hand, I'm getting older.
[00:02:26.720 --> 00:02:29.260]   I run a lab of 20 people at Harvard.
[00:02:29.260 --> 00:02:33.520]   I've got a book, I've got science to do, companies to run.
[00:02:33.520 --> 00:02:35.680]   And so I have to, on most days,
[00:02:35.680 --> 00:02:38.160]   just pretend to be a grown-up and be mature,
[00:02:38.160 --> 00:02:40.680]   but I definitely don't feel that way.
[00:02:40.680 --> 00:02:43.400]   - There's something I really appreciated
[00:02:43.400 --> 00:02:44.920]   in the opening of your book.
[00:02:44.920 --> 00:02:46.920]   You talked about your grandmother.
[00:02:46.920 --> 00:02:50.360]   And on this kind of theme, on this kind of topic,
[00:02:50.360 --> 00:02:53.720]   she, first of all, had a big influence on you.
[00:02:53.720 --> 00:02:56.520]   My grandmother had a big influence on me.
[00:02:56.520 --> 00:02:59.080]   And you also mentioned this poem
[00:02:59.080 --> 00:03:03.760]   by the author of "Winnie the Pooh," Alan Alexander Milne.
[00:03:03.760 --> 00:03:06.880]   Maybe I can read it real quick 'cause I...
[00:03:06.880 --> 00:03:08.480]   (laughs)
[00:03:08.480 --> 00:03:10.280]   On the topic of being children,
[00:03:10.280 --> 00:03:12.560]   when I was one, I had just begun.
[00:03:12.560 --> 00:03:14.800]   When I was two, I was nearly new.
[00:03:14.800 --> 00:03:17.280]   When I was three, I was hardly me.
[00:03:17.280 --> 00:03:19.840]   When I was four, I was not much more.
[00:03:19.840 --> 00:03:24.320]   When I was five, I was just alive, but now I am six.
[00:03:24.320 --> 00:03:27.500]   I am as clever, as clever, so I think I'll be six,
[00:03:27.500 --> 00:03:29.600]   now, forever, and ever.
[00:03:29.600 --> 00:03:36.440]   So this idea of being six and staying six forever,
[00:03:36.440 --> 00:03:40.560]   being youthful, being curious, being childlike,
[00:03:41.360 --> 00:03:46.360]   this and other things, what influence has your grandmother
[00:03:46.360 --> 00:03:51.520]   had on your thinking about life, about death, about love?
[00:03:51.520 --> 00:03:54.920]   - Yeah, I was getting misty-eyed as you read that
[00:03:54.920 --> 00:03:58.060]   because that poem was read to me very often,
[00:03:58.060 --> 00:03:59.520]   if not every day, by my grandmother
[00:03:59.520 --> 00:04:01.400]   who partially raised me.
[00:04:01.400 --> 00:04:06.440]   And she was as much a bohemian as an artist, philosopher.
[00:04:06.440 --> 00:04:08.880]   And she's one of those people that wouldn't talk
[00:04:08.880 --> 00:04:09.760]   about the little things.
[00:04:09.760 --> 00:04:11.520]   She said, "I hate small talk.
[00:04:11.520 --> 00:04:13.900]   "Don't talk to me about politics or the weather.
[00:04:13.900 --> 00:04:16.960]   "Yeah, talk to me about human beings and culture."
[00:04:16.960 --> 00:04:19.720]   So I was raised on that, and this poem was one
[00:04:19.720 --> 00:04:22.480]   that she read to me often because she knew
[00:04:22.480 --> 00:04:27.480]   that the mind of a child is precious, it's honest,
[00:04:27.480 --> 00:04:31.520]   it's pure, and she grew up during the Second World War
[00:04:31.520 --> 00:04:35.720]   and in Hungary, in Budapest, witnessed the worst of humanity.
[00:04:35.720 --> 00:04:40.100]   She was trying to save a whole group of Jewish friends
[00:04:40.100 --> 00:04:43.220]   in her apartment, saw what happened after the World War,
[00:04:43.220 --> 00:04:47.800]   which was there was, the Russians were in control
[00:04:47.800 --> 00:04:50.360]   and locals weren't necessarily treated well
[00:04:50.360 --> 00:04:52.560]   if they were rebellious, which she was.
[00:04:52.560 --> 00:04:54.480]   And then there was the revolution in '56,
[00:04:54.480 --> 00:04:57.280]   which she was part of and had to escape the country.
[00:04:57.280 --> 00:05:01.300]   So she saw what can happen when humans do their worst.
[00:05:01.300 --> 00:05:05.280]   And her words to me, expressed in part through that poem,
[00:05:05.280 --> 00:05:09.500]   was, "David, always stay young and innocent
[00:05:09.500 --> 00:05:11.520]   "and have wonder about the world,
[00:05:11.520 --> 00:05:15.440]   "and then do your best to make humanity the best it can be."
[00:05:15.440 --> 00:05:18.340]   And that's who I am, that's what I live for,
[00:05:18.340 --> 00:05:19.800]   that's what I get up in the morning to do,
[00:05:19.800 --> 00:05:21.680]   is to leave the world a better place
[00:05:21.680 --> 00:05:24.040]   and show to whoever's watching us, whether it's aliens
[00:05:24.040 --> 00:05:27.640]   or some future human historian, that we can do better
[00:05:27.640 --> 00:05:29.280]   than we did in the 20th century.
[00:05:29.280 --> 00:05:33.200]   - You know, we mentioned offline this idea
[00:05:33.200 --> 00:05:34.920]   of bringing people back to life
[00:05:34.920 --> 00:05:38.240]   through artificial intelligence.
[00:05:38.240 --> 00:05:40.880]   Sort of, I don't know if you've seen videos
[00:05:40.880 --> 00:05:45.200]   of basically animating people back to life.
[00:05:45.200 --> 00:05:47.960]   Meaning, whether it's, for me personally,
[00:05:47.960 --> 00:05:51.600]   I've been working on, specifically about Albert Einstein,
[00:05:51.600 --> 00:05:55.740]   but also Alan Turing, Isaac Newton, and Richard Feynman.
[00:05:55.740 --> 00:05:59.540]   And it's an opportunity to bring people
[00:05:59.540 --> 00:06:02.360]   that meant a lot to others in the world.
[00:06:02.360 --> 00:06:05.600]   And animate them, and be able to have
[00:06:05.600 --> 00:06:06.840]   a conversation with them.
[00:06:06.840 --> 00:06:10.260]   At first, to try to visually,
[00:06:10.260 --> 00:06:17.200]   visually explore the full richness of character
[00:06:17.200 --> 00:06:18.800]   that they had as they struggled
[00:06:18.800 --> 00:06:20.880]   with the ideas of the modern age.
[00:06:20.880 --> 00:06:24.100]   Sort of, it's less about bringing back their mind,
[00:06:24.100 --> 00:06:27.420]   and more bringing back the visual quirks
[00:06:27.420 --> 00:06:28.920]   that made them who they are.
[00:06:28.920 --> 00:06:30.920]   And then maybe in the future,
[00:06:30.920 --> 00:06:33.160]   it's using the textual, the visual,
[00:06:33.160 --> 00:06:38.160]   the video, the audio data to actually compress
[00:06:38.160 --> 00:06:41.040]   down the person for who they are,
[00:06:41.040 --> 00:06:42.440]   and be able to generate text.
[00:06:42.440 --> 00:06:44.480]   There's a few companies, there's Replica,
[00:06:44.480 --> 00:06:46.880]   which is a chat engine that was born
[00:06:46.880 --> 00:06:48.440]   out of the idea of bringing,
[00:06:48.440 --> 00:06:53.440]   the founder lost her friend to,
[00:06:53.440 --> 00:06:55.840]   he got ran over by a car.
[00:06:55.840 --> 00:06:59.720]   And the initial reason she founded the company
[00:06:59.720 --> 00:07:02.840]   was trying to just have a conversation with her friend.
[00:07:02.840 --> 00:07:05.520]   She trained a machine learning,
[00:07:05.520 --> 00:07:08.240]   natural language system on the text
[00:07:08.240 --> 00:07:09.480]   that they exchanged with each other,
[00:07:09.480 --> 00:07:11.880]   and she had a conversation with him,
[00:07:11.880 --> 00:07:13.880]   sort of after he was gone.
[00:07:13.880 --> 00:07:16.880]   And it's very, the conversation was very trivial.
[00:07:16.880 --> 00:07:19.560]   It was obvious that it's, you know,
[00:07:19.560 --> 00:07:23.200]   a AI agent, but it gave her solace.
[00:07:23.200 --> 00:07:26.080]   It made her actually feel really good.
[00:07:26.080 --> 00:07:27.920]   And that's the way I wonder if it's possible
[00:07:27.920 --> 00:07:30.840]   to bring back people that are,
[00:07:30.840 --> 00:07:32.520]   that mean something to us personally,
[00:07:32.520 --> 00:07:36.600]   not just Einstein, but people that we've lost,
[00:07:36.600 --> 00:07:40.480]   and in that way achieve a kind of small,
[00:07:40.480 --> 00:07:42.680]   artificial immortality.
[00:07:42.680 --> 00:07:44.720]   I don't know if you think about this kind of stuff.
[00:07:44.720 --> 00:07:46.640]   - Well, I definitely think about a lot of things.
[00:07:46.640 --> 00:07:47.640]   That one's a really good one.
[00:07:47.640 --> 00:07:49.160]   There's a great Black Mirror episode
[00:07:49.160 --> 00:07:52.840]   about the wife who brings back the boyfriend or husband.
[00:07:52.840 --> 00:07:54.280]   I think one of the challenges
[00:07:54.280 --> 00:07:55.520]   with bringing back Richard Feynman
[00:07:55.520 --> 00:07:58.080]   would be to capture his sense of humor,
[00:07:58.080 --> 00:07:59.300]   but that would be awesome.
[00:07:59.300 --> 00:08:00.920]   But yeah, bringing back loved ones would be great,
[00:08:00.920 --> 00:08:04.000]   especially if it's, you know,
[00:08:04.000 --> 00:08:07.680]   they're young and they die early.
[00:08:07.680 --> 00:08:09.180]   Though it may hold you back from moving on.
[00:08:09.180 --> 00:08:11.840]   That's another thing that could happen as a negative.
[00:08:11.840 --> 00:08:12.680]   But I think that's great,
[00:08:12.680 --> 00:08:14.680]   and I also think that it's gonna be possible,
[00:08:14.680 --> 00:08:17.280]   especially when we're recording, some of us,
[00:08:17.280 --> 00:08:18.440]   every aspect of our lives,
[00:08:18.440 --> 00:08:22.640]   whether it's our face or things we see.
[00:08:22.640 --> 00:08:26.000]   Eventually one day, everything we see can be recorded.
[00:08:26.000 --> 00:08:28.960]   And then you can build somebody's experience
[00:08:28.960 --> 00:08:31.840]   and thoughts, speech,
[00:08:31.840 --> 00:08:34.980]   and you will have replicas of everybody,
[00:08:34.980 --> 00:08:37.000]   at least digitally and physically,
[00:08:37.000 --> 00:08:38.720]   you could do that too one day.
[00:08:38.720 --> 00:08:40.360]   But that's a good idea,
[00:08:40.360 --> 00:08:42.620]   especially 'cause there are people that I'd like to meet,
[00:08:42.620 --> 00:08:44.920]   and I think it's easier than building a time machine.
[00:08:44.920 --> 00:08:47.320]   One person I'd love to meet is Benjamin Franklin.
[00:08:47.320 --> 00:08:48.160]   - Really?
[00:08:48.160 --> 00:08:51.120]   - Well, I wouldn't go back in time, I would,
[00:08:51.120 --> 00:08:54.080]   but I'd prefer to bring him into the future and say,
[00:08:54.080 --> 00:08:57.000]   "Can you believe we have this thinking machine
[00:08:57.000 --> 00:08:58.440]   in our pockets now?"
[00:08:58.440 --> 00:09:00.240]   And just see the look on his face
[00:09:00.240 --> 00:09:01.320]   as to where humanity has come.
[00:09:01.320 --> 00:09:03.500]   'Cause I think of him as a modern guy
[00:09:03.500 --> 00:09:05.480]   that just was before his time.
[00:09:05.480 --> 00:09:08.520]   - Yeah, so you're thinking Benjamin Franklin is a scientist,
[00:09:08.520 --> 00:09:10.400]   not Benjamin Franklin, the political thing.
[00:09:10.400 --> 00:09:13.040]   'Cause he'd be very upset with Congress right now.
[00:09:13.040 --> 00:09:13.880]   - Right.
[00:09:13.880 --> 00:09:16.160]   - So maybe talk to him about science and technology,
[00:09:16.160 --> 00:09:18.160]   not politics.
[00:09:18.160 --> 00:09:19.580]   Or maybe just don't get him on Twitter,
[00:09:19.580 --> 00:09:22.400]   because he'll be very upset with human civilization.
[00:09:22.400 --> 00:09:25.360]   You know, I wonder what their personalities are like.
[00:09:25.360 --> 00:09:28.640]   Isaac Newton, it does seem complicated
[00:09:28.640 --> 00:09:30.480]   to figure out what their personality is like.
[00:09:30.480 --> 00:09:33.320]   Even Friedrich Nietzsche, who I also thought about.
[00:09:33.320 --> 00:09:36.800]   Feynman is, we just have enough video
[00:09:36.800 --> 00:09:39.360]   where we get the full kind of,
[00:09:39.360 --> 00:09:42.440]   I mean, it shows you how important it is
[00:09:42.440 --> 00:09:47.440]   to get not the official kind of book-level presentation
[00:09:47.560 --> 00:09:49.560]   of a human, but the authentic,
[00:09:49.560 --> 00:09:51.940]   the full spectrum of humanity.
[00:09:51.940 --> 00:09:54.420]   You mentioned collecting data about a person,
[00:09:54.420 --> 00:09:57.260]   collecting the whole thing, the whole of life,
[00:09:57.260 --> 00:09:59.500]   the ups and downs, the embarrassing stuff,
[00:09:59.500 --> 00:10:00.660]   the beautiful stuff,
[00:10:00.660 --> 00:10:03.180]   not just the things that's condensed into a book.
[00:10:03.180 --> 00:10:05.860]   And then with Feynman, you start to see that a little bit.
[00:10:05.860 --> 00:10:07.940]   Through conversations, you start to see peaks
[00:10:07.940 --> 00:10:09.420]   of like that genius.
[00:10:09.420 --> 00:10:12.900]   And then through stories about him from others.
[00:10:12.900 --> 00:10:14.700]   And then certainly you,
[00:10:15.160 --> 00:10:17.720]   the sad thing about Alan Turing, for example,
[00:10:17.720 --> 00:10:22.240]   is there's very little, if any, recording of him.
[00:10:22.240 --> 00:10:24.180]   In fact, I haven't been able to find recording.
[00:10:24.180 --> 00:10:27.600]   Allegedly, there's supposed to be a recording of him
[00:10:27.600 --> 00:10:29.720]   doing some kind of radio broadcast,
[00:10:29.720 --> 00:10:32.120]   but I haven't been able to find anything.
[00:10:32.120 --> 00:10:34.720]   And so that's truly sad.
[00:10:34.720 --> 00:10:36.840]   That it feels like, it makes you realize
[00:10:36.840 --> 00:10:42.320]   how the upside, how nice it is to collect data
[00:10:42.320 --> 00:10:45.900]   about a person, to capture that person.
[00:10:45.900 --> 00:10:48.740]   There's, that's the upside of the modern internet age,
[00:10:48.740 --> 00:10:51.320]   the digital age, that that information,
[00:10:51.320 --> 00:10:54.260]   yeah, creates a kind of immortality.
[00:10:54.260 --> 00:10:57.100]   And then you can choose to highlight
[00:10:57.100 --> 00:10:58.220]   the best parts of the person,
[00:10:58.220 --> 00:11:00.580]   maybe throw away the ugly parts
[00:11:00.580 --> 00:11:03.460]   and celebrate them even after they're gone.
[00:11:03.460 --> 00:11:05.100]   So that's a really interesting opportunity.
[00:11:05.100 --> 00:11:07.900]   You've also mentioned to me offline
[00:11:07.900 --> 00:11:11.060]   that you're really excited about all the different wearables
[00:11:11.060 --> 00:11:14.040]   and all the different ways we can collect information
[00:11:14.040 --> 00:11:18.400]   about our bodies, about, well, the whole thing.
[00:11:18.400 --> 00:11:20.280]   What's most exciting to you
[00:11:20.280 --> 00:11:25.280]   in terms of collecting the biological data
[00:11:25.280 --> 00:11:27.840]   about a human being?
[00:11:27.840 --> 00:11:29.320]   - Well, so I'm a biologist.
[00:11:29.320 --> 00:11:33.140]   I find animals and humans as machines very interesting.
[00:11:33.140 --> 00:11:36.500]   It's one of the reasons I didn't become an engineer
[00:11:36.500 --> 00:11:38.140]   or a surgeon, I wanted to understand
[00:11:38.140 --> 00:11:39.440]   how we actually are built.
[00:11:40.600 --> 00:11:45.600]   And so I think a lot about machines merging with humans.
[00:11:45.600 --> 00:11:49.040]   And the first of that are the bio wearables.
[00:11:49.040 --> 00:11:50.300]   And so I talked a lot about this,
[00:11:50.300 --> 00:11:52.440]   I wrote about it in "Lifespan," the book,
[00:11:52.440 --> 00:11:56.860]   and pictured a future where you would be monitored constantly
[00:11:56.860 --> 00:11:59.280]   so that you wouldn't suddenly have a heart attack,
[00:11:59.280 --> 00:12:00.240]   you'd know that was coming,
[00:12:00.240 --> 00:12:02.640]   or you wouldn't go to the doctor
[00:12:02.640 --> 00:12:05.680]   and they don't know if you need an antibiotic or not.
[00:12:05.680 --> 00:12:10.000]   Long-term, how old are you, how to fix things,
[00:12:10.000 --> 00:12:11.560]   what should you eat, what should you take,
[00:12:11.560 --> 00:12:12.980]   what should your doctor do?
[00:12:12.980 --> 00:12:16.100]   These devices, I predicted, would be smarter,
[00:12:16.100 --> 00:12:18.080]   better educated than your physician,
[00:12:18.080 --> 00:12:19.720]   and would augment them,
[00:12:19.720 --> 00:12:21.460]   and then there'd be a human that would just tick off
[00:12:21.460 --> 00:12:24.320]   to see if it's correct and they approve.
[00:12:24.320 --> 00:12:26.840]   I also was predicting in the book
[00:12:26.840 --> 00:12:30.200]   that we would have video conferences with our doctors
[00:12:30.200 --> 00:12:32.040]   and that medicines would be delivered,
[00:12:32.040 --> 00:12:33.960]   initially by courier, but eventually by drones
[00:12:33.960 --> 00:12:36.240]   and get it to you, sometimes in an emergency,
[00:12:36.240 --> 00:12:38.000]   and that we could even have pills
[00:12:38.000 --> 00:12:41.820]   that were synthesized or delivered in your kitchen,
[00:12:41.820 --> 00:12:43.920]   and combined, certainly.
[00:12:43.920 --> 00:12:47.120]   What's amazing about that is that, what are we now,
[00:12:47.120 --> 00:12:50.200]   two years since the book came out, even less,
[00:12:50.200 --> 00:12:52.360]   and that future is basically here already.
[00:12:52.360 --> 00:12:56.880]   COVID-19 accelerated that incredibly.
[00:12:56.880 --> 00:12:58.480]   So where we're at now in society is,
[00:12:58.480 --> 00:12:59.740]   if you want to pay for it,
[00:12:59.740 --> 00:13:01.680]   you can have a blood test that will detect cancer
[00:13:01.680 --> 00:13:03.600]   10, 20 years earlier than it would
[00:13:03.600 --> 00:13:05.360]   before it forms a tumor.
[00:13:05.360 --> 00:13:07.920]   You can, of course, do your genome very cheaply
[00:13:07.920 --> 00:13:09.360]   for less than $100 now.
[00:13:09.360 --> 00:13:12.320]   There are bio-wearables already.
[00:13:12.320 --> 00:13:14.160]   I wear this ring from Aura
[00:13:14.160 --> 00:13:17.200]   that I have a number of years of data.
[00:13:17.200 --> 00:13:19.240]   I've been doing blood tests for the last 12 years
[00:13:19.240 --> 00:13:22.600]   with a company called InsideTracker, which I consult for,
[00:13:22.600 --> 00:13:24.200]   and so I have all of that data as well,
[00:13:24.200 --> 00:13:27.720]   and there's 34 different parameters on my testosterone,
[00:13:27.720 --> 00:13:30.040]   my blood glucose, my inflammation,
[00:13:30.040 --> 00:13:32.720]   and I use all that data to, of course,
[00:13:32.720 --> 00:13:34.720]   I wear a watch that measures things as well.
[00:13:34.720 --> 00:13:39.280]   I use that data to keep my body in optimal shape.
[00:13:39.280 --> 00:13:42.160]   So I'm now 51, and according to those parameters,
[00:13:42.160 --> 00:13:45.760]   I'm at least as good as someone in their early 40s,
[00:13:45.760 --> 00:13:47.200]   and if I really work at it,
[00:13:47.200 --> 00:13:52.000]   I can get my biochemistry down to mid-30s,
[00:13:52.000 --> 00:13:56.240]   though I like to now eat a little dessert once in a while.
[00:13:56.240 --> 00:13:58.680]   So that's the future we're in right now.
[00:13:58.680 --> 00:14:00.920]   Anyone can do what I just said,
[00:14:00.920 --> 00:14:04.320]   but in the very near future, just in the next few years,
[00:14:04.320 --> 00:14:06.400]   you can be wearing wearables.
[00:14:06.400 --> 00:14:08.560]   So I'm currently wearing a little,
[00:14:08.560 --> 00:14:10.160]   what's called a bio sticker.
[00:14:10.160 --> 00:14:14.120]   This one I just put on last night.
[00:14:14.120 --> 00:14:18.640]   It's about an inch long, a few millimeters.
[00:14:18.640 --> 00:14:21.680]   - Yeah, for people just listening, it's on David's chest.
[00:14:21.680 --> 00:14:23.200]   It's just the, how does it attach?
[00:14:23.200 --> 00:14:24.320]   It's just kinda--
[00:14:24.320 --> 00:14:25.400]   - It sticks on. - Sticks on.
[00:14:25.400 --> 00:14:27.760]   - Yeah, so on one side, you have an on button that you press.
[00:14:27.760 --> 00:14:30.520]   The lights come on, flashes four times, it's good to go.
[00:14:30.520 --> 00:14:33.000]   It immediately syncs to your phone,
[00:14:33.000 --> 00:14:37.280]   and this one, it's called a bio button, nice name.
[00:14:37.280 --> 00:14:39.480]   And there's another one that I have that I haven't tried yet
[00:14:39.480 --> 00:14:41.500]   that does EKG on your heart.
[00:14:41.500 --> 00:14:43.360]   This is mainly for doctors to monitor patients
[00:14:43.360 --> 00:14:45.640]   that go home after a heart attack or surgery,
[00:14:45.640 --> 00:14:48.960]   but that's medical-grade, FDA-approved device.
[00:14:48.960 --> 00:14:51.240]   So there will be a day, in fact, it's already here,
[00:14:51.240 --> 00:14:55.280]   that doctors are using these to get patients to go home
[00:14:55.280 --> 00:14:59.440]   and save a week in hospital, $2,000 at least
[00:14:59.440 --> 00:15:00.920]   for each patient.
[00:15:00.920 --> 00:15:04.440]   That's massive savings for the hospital.
[00:15:04.440 --> 00:15:06.560]   But ultimately, what I'm excited about is a future
[00:15:06.560 --> 00:15:10.760]   that isn't that far off where everybody,
[00:15:10.760 --> 00:15:11.760]   certainly in developed countries,
[00:15:11.760 --> 00:15:14.360]   eventually these will cost a few cents and rechargeable.
[00:15:14.360 --> 00:15:16.620]   The only cost will be the software subscription
[00:15:16.620 --> 00:15:19.240]   that can be monitored constantly.
[00:15:19.240 --> 00:15:21.020]   And to give you an idea what this is measuring me
[00:15:21.020 --> 00:15:26.020]   at 1,000 times a second is my vibrations as I speak,
[00:15:26.020 --> 00:15:29.600]   my orientation, it already has told me this morning
[00:15:29.600 --> 00:15:33.220]   how I slept, where I slept, what side I slept on.
[00:15:33.220 --> 00:15:38.060]   We've got sneezing, coughing, body temperature, heart rate,
[00:15:38.060 --> 00:15:40.880]   heart, other parameters of the heart
[00:15:40.880 --> 00:15:42.520]   that would indicate heart health.
[00:15:42.520 --> 00:15:49.040]   These data are being used to now to predict sickness.
[00:15:49.040 --> 00:15:53.700]   So eventually we'll have, just in the next year or so,
[00:15:53.700 --> 00:15:55.560]   the ability to predict whether something,
[00:15:55.560 --> 00:15:58.200]   or diagnose whether something is pneumonia
[00:15:58.200 --> 00:16:02.200]   or just a rhinovirus that can be treated or not.
[00:16:02.200 --> 00:16:06.400]   This is really going to not just revolutionize medicine,
[00:16:06.400 --> 00:16:08.520]   but I think extend lives dramatically.
[00:16:08.520 --> 00:16:11.560]   'Cause if I'm gonna have a heart attack next week,
[00:16:11.560 --> 00:16:14.480]   and that's possible, this device should know that
[00:16:14.480 --> 00:16:17.000]   and I'll be in hospital before I even have it.
[00:16:17.000 --> 00:16:19.320]   Maybe you can talk a little bit about InsideTracker
[00:16:19.320 --> 00:16:22.220]   'cause I saw that there's some really cool things in there.
[00:16:22.220 --> 00:16:23.060]   (laughs)
[00:16:23.060 --> 00:16:26.000]   Like it actually, so maybe you can talk about,
[00:16:26.000 --> 00:16:28.040]   I guess that you're collecting blood
[00:16:28.040 --> 00:16:29.280]   to give it the data.
[00:16:29.280 --> 00:16:32.520]   So, and it has like basic recommendations
[00:16:32.520 --> 00:16:34.200]   on how to improve your life.
[00:16:34.200 --> 00:16:36.560]   So we're not just talking about diseases, right?
[00:16:36.560 --> 00:16:39.080]   Like anticipating having a particular disease,
[00:16:39.080 --> 00:16:41.880]   but it's almost like guiding your trajectory to life,
[00:16:41.880 --> 00:16:44.600]   how to, whether it's extend your life
[00:16:44.600 --> 00:16:46.680]   or just live a more fulfilling,
[00:16:46.680 --> 00:16:48.200]   like improve the quality of life,
[00:16:48.200 --> 00:16:50.280]   I suppose this is the right way to say it.
[00:16:50.280 --> 00:16:52.480]   What, how does InsideTracker work?
[00:16:52.480 --> 00:16:53.320]   What the heck is it?
[00:16:53.320 --> 00:16:55.120]   'Cause I saw that there was also pretty cool.
[00:16:55.120 --> 00:16:55.960]   - Yeah. - What is it?
[00:16:55.960 --> 00:16:58.000]   I guess it's something other people can use.
[00:16:58.000 --> 00:16:59.720]   - You can definitely use it.
[00:16:59.720 --> 00:17:02.080]   You can sign up, it's consumer.
[00:17:02.080 --> 00:17:04.480]   - It's like a company, consumer facing company?
[00:17:04.480 --> 00:17:06.080]   - It is, yeah. - Okay, cool.
[00:17:06.080 --> 00:17:09.000]   - And I also want to democratize the ability
[00:17:09.000 --> 00:17:11.040]   to just take a mouth swab eventually.
[00:17:11.040 --> 00:17:13.600]   We don't need to have a blood test necessarily,
[00:17:13.600 --> 00:17:15.080]   but for now it's a blood test
[00:17:15.080 --> 00:17:18.420]   and you'd go to a lab core request in the US.
[00:17:18.420 --> 00:17:19.880]   It's also available overseas.
[00:17:19.880 --> 00:17:22.560]   You can upload your own data for minimal cost
[00:17:22.560 --> 00:17:26.600]   and get the algorithms, the AI in the background
[00:17:26.600 --> 00:17:30.160]   to take that data, plot where you are against others
[00:17:30.160 --> 00:17:33.600]   in your age group, in terms of health and longevity,
[00:17:33.600 --> 00:17:36.040]   bio age they call it, no, inner age.
[00:17:36.040 --> 00:17:38.160]   But also it provides recommendations.
[00:17:38.160 --> 00:17:39.680]   And this isn't just a bunch of BS.
[00:17:39.680 --> 00:17:41.640]   It sounds like it might be to say,
[00:17:41.640 --> 00:17:44.400]   "Oh, go eat this or go to that restaurant and order that."
[00:17:44.400 --> 00:17:46.440]   But it's actually based on,
[00:17:46.440 --> 00:17:49.840]   they basically, this company has entered hundreds,
[00:17:49.840 --> 00:17:51.800]   now it would be thousands of scientific papers
[00:17:51.800 --> 00:17:53.200]   into their database,
[00:17:53.200 --> 00:17:55.760]   and hundreds of thousands of human data points.
[00:17:55.760 --> 00:17:59.080]   And they have tens of thousands of individuals
[00:17:59.080 --> 00:18:00.400]   that have been tracked over time.
[00:18:00.400 --> 00:18:02.960]   And anonymously, that data is used to say
[00:18:02.960 --> 00:18:04.280]   what works and what doesn't.
[00:18:04.280 --> 00:18:05.760]   If you eat that, what works?
[00:18:05.760 --> 00:18:07.640]   If you take that supplement, what works?
[00:18:07.640 --> 00:18:09.640]   And I was a co-author on a paper that showed
[00:18:09.640 --> 00:18:13.440]   that the recommendations for food and supplements
[00:18:13.440 --> 00:18:18.080]   was better than the leading drug for type two diabetes.
[00:18:18.080 --> 00:18:19.800]   - That's so cool.
[00:18:19.800 --> 00:18:21.680]   The idea that you can connect,
[00:18:21.680 --> 00:18:24.920]   like skipping the human having to do this work,
[00:18:24.920 --> 00:18:27.400]   you can connect the scientific papers,
[00:18:27.400 --> 00:18:30.120]   almost like meta-analysis of the science
[00:18:30.120 --> 00:18:32.520]   connected to the individual data.
[00:18:32.520 --> 00:18:35.440]   And then based on that, connect your data
[00:18:35.440 --> 00:18:37.920]   to whatever the proper group is
[00:18:37.920 --> 00:18:40.480]   within whatever the scientific paper is
[00:18:40.480 --> 00:18:45.480]   to make the suggestion of how that work applies to your life.
[00:18:45.480 --> 00:18:49.680]   And then that ultimately maps to a recommendation
[00:18:49.680 --> 00:18:52.280]   of what you should do with your life.
[00:18:52.280 --> 00:18:55.760]   Like this giant system that ultimately recommends
[00:18:55.760 --> 00:18:58.480]   you should drink more coffee or less.
[00:18:58.480 --> 00:19:00.320]   - Right, and we'll have the genome in there as well.
[00:19:00.320 --> 00:19:02.000]   You can upload that.
[00:19:02.000 --> 00:19:04.680]   And so these programs will know us way better
[00:19:04.680 --> 00:19:07.520]   than we do and our doctors as well.
[00:19:07.520 --> 00:19:09.000]   The idea of going to a doctor once a year
[00:19:09.000 --> 00:19:11.520]   for an annual checkup and having males
[00:19:11.520 --> 00:19:14.680]   get a finger up their butt and you cough,
[00:19:14.680 --> 00:19:16.040]   that to me is a joke.
[00:19:16.040 --> 00:19:18.160]   That's medieval medicine.
[00:19:18.160 --> 00:19:21.480]   And that's very soon going to be seen as medieval.
[00:19:21.480 --> 00:19:25.960]   - Yeah, to me as a computer science person,
[00:19:25.960 --> 00:19:28.640]   it's always upsetting to go to the doctor
[00:19:28.640 --> 00:19:32.540]   and just look at him and realize you know nothing about me.
[00:19:32.540 --> 00:19:37.840]   You're making your opinions based on,
[00:19:37.840 --> 00:19:42.520]   it is very valuable, years of intuition building
[00:19:42.520 --> 00:19:45.880]   about basic symptoms, but you're just like, it is medieval.
[00:19:45.880 --> 00:19:47.200]   They're very good at it.
[00:19:47.200 --> 00:19:49.600]   In fact, doctors in medieval times
[00:19:49.600 --> 00:19:53.600]   were probably damn good at working with very little.
[00:19:53.600 --> 00:19:58.520]   But the thing is, I'd rather prefer a doctor
[00:19:58.520 --> 00:20:00.060]   that doesn't really know what they're doing
[00:20:00.060 --> 00:20:03.060]   but has a huge amount of data to work with.
[00:20:03.060 --> 00:20:03.900]   - Well, you're right.
[00:20:03.900 --> 00:20:05.600]   And many of my good friends are doctors.
[00:20:05.600 --> 00:20:06.520]   I work at Harvard.
[00:20:06.520 --> 00:20:09.640]   So I'm not against the profession at all.
[00:20:09.640 --> 00:20:11.680]   But I think that they need just as much help
[00:20:11.680 --> 00:20:13.480]   as anyone else does.
[00:20:13.480 --> 00:20:15.300]   We wouldn't drive a car without a dashboard.
[00:20:15.300 --> 00:20:16.140]   We wouldn't think of it.
[00:20:16.140 --> 00:20:17.880]   So why would doctors do the same?
[00:20:18.720 --> 00:20:21.400]   If we could, can we step back to the big,
[00:20:21.400 --> 00:20:23.680]   profound, philosophical, both tragic
[00:20:23.680 --> 00:20:25.500]   and beautiful question about age?
[00:20:25.500 --> 00:20:28.720]   How and why do we age?
[00:20:28.720 --> 00:20:31.520]   Is it, from an engineering perspective,
[00:20:31.520 --> 00:20:33.720]   you said you like the biological machine.
[00:20:33.720 --> 00:20:37.760]   Is that a feature or a bug of the biological machine?
[00:20:37.760 --> 00:20:40.740]   - It is both a bug and a feature.
[00:20:40.740 --> 00:20:44.640]   Evolutionary speaking, we only live as long as we need to
[00:20:44.640 --> 00:20:46.280]   to replace ourselves efficiently.
[00:20:47.360 --> 00:20:49.120]   If you're a mouse, you're only gonna live
[00:20:49.120 --> 00:20:50.600]   two and a half years, three years.
[00:20:50.600 --> 00:20:52.460]   You're probably gonna die of starvation, predation,
[00:20:52.460 --> 00:20:54.440]   freezing in the winter.
[00:20:54.440 --> 00:20:57.120]   So they divert most of their resources
[00:20:57.120 --> 00:20:59.320]   to reproducing rapidly,
[00:20:59.320 --> 00:21:00.960]   but they don't put a lot of energy
[00:21:00.960 --> 00:21:04.320]   into preserving their soma, which is their body.
[00:21:04.320 --> 00:21:07.400]   Conversely, a baleen type of whale, a bowhead whale
[00:21:07.400 --> 00:21:09.320]   in particular, will live hundreds of years
[00:21:09.320 --> 00:21:11.080]   because they're at the top of the food chain
[00:21:11.080 --> 00:21:12.640]   and they can live as long as they want.
[00:21:12.640 --> 00:21:15.140]   So they breed slowly and build a body that lasts.
[00:21:15.140 --> 00:21:16.660]   We're somewhere in between
[00:21:16.660 --> 00:21:19.680]   because we've really only just come out of the savannas
[00:21:19.680 --> 00:21:22.080]   where we could be picked off by a cat.
[00:21:22.080 --> 00:21:25.400]   We were pretty wimpy going back 6 million years ago.
[00:21:25.400 --> 00:21:30.080]   So we actually need to evolve quicker than evolution will.
[00:21:30.080 --> 00:21:33.560]   And that's why we can use our oversized brains
[00:21:33.560 --> 00:21:36.140]   and intuition to give us what evolution
[00:21:36.140 --> 00:21:38.280]   not only didn't give us, but took away from us.
[00:21:38.280 --> 00:21:39.320]   Now we're pathetic.
[00:21:39.320 --> 00:21:40.680]   Look at our bodies.
[00:21:40.680 --> 00:21:42.140]   These arms, if any of us,
[00:21:42.140 --> 00:21:43.260]   even the strongest person in the world
[00:21:43.260 --> 00:21:45.000]   went in a cage with a chimpanzee,
[00:21:45.000 --> 00:21:46.400]   the chimp could knock that person's head off.
[00:21:46.400 --> 00:21:47.400]   No question.
[00:21:47.400 --> 00:21:48.240]   So we're pathetic.
[00:21:48.240 --> 00:21:49.440]   So we need to engineer ourselves
[00:21:49.440 --> 00:21:51.580]   to be healthier and longer lived.
[00:21:51.580 --> 00:21:55.680]   So getting to aging, we can do better.
[00:21:55.680 --> 00:21:57.260]   Whales do way better.
[00:21:57.260 --> 00:21:59.520]   We're trying to learn how whales do that.
[00:21:59.520 --> 00:22:02.840]   And if you ask really anybody in the field now,
[00:22:02.840 --> 00:22:06.320]   professor, they'll say there are eight or nine
[00:22:06.320 --> 00:22:08.120]   hallmarks of aging, which are really,
[00:22:08.120 --> 00:22:11.520]   it's a word for causes of aging.
[00:22:11.520 --> 00:22:13.500]   So you probably have heard of some of these.
[00:22:13.500 --> 00:22:16.580]   Your listeners will have loss of telomeres,
[00:22:16.580 --> 00:22:17.740]   the ends of the chromosomes,
[00:22:17.740 --> 00:22:21.860]   like the little ends of shoelaces, that kind of thing.
[00:22:21.860 --> 00:22:23.540]   They get too short, cells stop dividing,
[00:22:23.540 --> 00:22:24.860]   become senescent.
[00:22:24.860 --> 00:22:28.580]   They put out what are called mitogens
[00:22:28.580 --> 00:22:31.020]   that cause cancer and inflammatory molecules.
[00:22:31.020 --> 00:22:34.260]   So that's another aspect of aging, cellular senescence.
[00:22:34.260 --> 00:22:35.900]   Another one is loss of the energetics.
[00:22:35.900 --> 00:22:38.680]   So mitochondria, the battery packs, wind down.
[00:22:38.680 --> 00:22:43.060]   There's a whole bunch, stem cells, proteostasis.
[00:22:43.060 --> 00:22:45.420]   Well, these are our Achilles heels that I'm talking about
[00:22:45.420 --> 00:22:48.380]   that are common amongst all life forms, really.
[00:22:48.380 --> 00:22:51.640]   But if you want me to jump to the chasers to where,
[00:22:51.640 --> 00:22:54.400]   what is the upstream defining factor?
[00:22:54.400 --> 00:22:57.300]   If we boil it down, what do we get?
[00:22:57.300 --> 00:22:59.700]   So most biologists would say you can't boil it down.
[00:22:59.700 --> 00:23:01.200]   It's too complex.
[00:23:01.200 --> 00:23:03.660]   I would say you can boil it down to an equation,
[00:23:03.660 --> 00:23:05.860]   which is the preservation of information
[00:23:05.860 --> 00:23:08.820]   and loss due to entropy, i.e. noise.
[00:23:08.820 --> 00:23:11.100]   And that is the basis of my research.
[00:23:12.380 --> 00:23:14.820]   It originally came out of discoveries in yeast cells
[00:23:14.820 --> 00:23:17.260]   where I went to MIT in the 1990s.
[00:23:17.260 --> 00:23:18.780]   - You studied bread.
[00:23:18.780 --> 00:23:20.220]   - I kind of did.
[00:23:20.220 --> 00:23:22.980]   I studied the makers of bread,
[00:23:22.980 --> 00:23:25.460]   a little yeast called Saccharomyces cerevisiae,
[00:23:25.460 --> 00:23:28.060]   which at the time was one of the hottest,
[00:23:28.060 --> 00:23:30.920]   excuse the pun, organisms to work on.
[00:23:30.920 --> 00:23:35.300]   But we figured out in the lab why yeast cells get old
[00:23:35.300 --> 00:23:37.460]   and found genes that control that process
[00:23:37.460 --> 00:23:38.980]   and made them live longer,
[00:23:38.980 --> 00:23:41.380]   which was an amazing four years of my life.
[00:23:42.340 --> 00:23:46.740]   One of those genes had a name with an acronym SIR2.
[00:23:46.740 --> 00:23:49.700]   Now the two is irrelevant.
[00:23:49.700 --> 00:23:51.820]   The SIR is important.
[00:23:51.820 --> 00:23:53.960]   And the most important letter out of all of those three
[00:23:53.960 --> 00:23:57.020]   is I, which stands for information.
[00:23:57.020 --> 00:23:59.660]   Silent information regulator number two,
[00:23:59.660 --> 00:24:01.140]   when you put more copies of that gene in,
[00:24:01.140 --> 00:24:02.800]   just put in one more copy,
[00:24:02.800 --> 00:24:04.740]   the yeast cells live 30% longer
[00:24:04.740 --> 00:24:06.100]   and suppress the cause of aging,
[00:24:06.100 --> 00:24:09.020]   which was the dysregulation of information in the cell.
[00:24:09.020 --> 00:24:11.660]   And then, so fast forward to now,
[00:24:11.660 --> 00:24:14.620]   I've been looking in humans and mice,
[00:24:14.620 --> 00:24:17.780]   'cause they live shorter and cheaper to study,
[00:24:17.780 --> 00:24:20.780]   where the loss of information in our bodies
[00:24:20.780 --> 00:24:22.580]   is a root cause of aging.
[00:24:22.580 --> 00:24:24.060]   And I think it is.
[00:24:24.060 --> 00:24:28.060]   - Your boldness in viewing biology in this way
[00:24:28.060 --> 00:24:31.820]   is fascinating because that also leads to a kind of,
[00:24:34.500 --> 00:24:39.500]   it's almost like allows for a theory of aging,
[00:24:39.500 --> 00:24:43.060]   like you could boil it down to a single equation
[00:24:43.060 --> 00:24:45.920]   and it leads to perhaps a metric
[00:24:45.920 --> 00:24:48.620]   that allows you to optimize aging,
[00:24:48.620 --> 00:24:50.660]   sort of in the fight against entropy.
[00:24:50.660 --> 00:24:53.820]   To figure out which mechanisms, like you said,
[00:24:53.820 --> 00:24:55.700]   the silent information regulator,
[00:24:55.700 --> 00:24:58.540]   which mechanisms allow you to preserve information
[00:24:58.540 --> 00:25:03.700]   without injecting noise, without creating entropy,
[00:25:03.700 --> 00:25:06.980]   without creating degradation of that information.
[00:25:06.980 --> 00:25:11.140]   For some reason, converting biology,
[00:25:11.140 --> 00:25:13.500]   which I thought was mostly impossible,
[00:25:13.500 --> 00:25:15.660]   into an engineering problem,
[00:25:15.660 --> 00:25:19.380]   feels like it makes it amenable to optimization,
[00:25:19.380 --> 00:25:23.420]   to solving problems, to creating technology that can,
[00:25:23.420 --> 00:25:26.660]   whether that's genetic engineering or AI,
[00:25:26.660 --> 00:25:30.920]   it makes it possible to create the technology
[00:25:30.920 --> 00:25:35.920]   that would improve the degradation of information and aging.
[00:25:35.920 --> 00:25:38.500]   Is there more concrete ways you think about
[00:25:38.500 --> 00:25:41.460]   the kind of information you want to preserve?
[00:25:41.460 --> 00:25:46.140]   And also, is there good ideas about regulators
[00:25:46.140 --> 00:25:51.140]   of that information, about ways to prevent the distortion,
[00:25:51.140 --> 00:25:54.020]   the degradation of that information?
[00:25:54.020 --> 00:25:56.580]   - Right, so we have silent information regulator genes
[00:25:56.580 --> 00:25:58.820]   in our bodies, we have seven of them.
[00:25:58.820 --> 00:26:00.380]   SIRT1 through seven, they're called.
[00:26:00.380 --> 00:26:02.340]   And we found in mice,
[00:26:02.340 --> 00:26:04.020]   one way to slow down the loss of information
[00:26:04.020 --> 00:26:08.420]   is to just give more of these, to upregulate these genes.
[00:26:08.420 --> 00:26:11.980]   So we made a mouse that has more of this SIRT1 gene,
[00:26:11.980 --> 00:26:15.140]   turned it on, and that slowed down the aging of the brain
[00:26:15.140 --> 00:26:16.500]   and preserved their information.
[00:26:16.500 --> 00:26:19.440]   Now, what information am I talking about, you might ask?
[00:26:19.440 --> 00:26:21.980]   Well, again, you can simplify biology.
[00:26:21.980 --> 00:26:25.100]   There are two types of information in the cell, primarily.
[00:26:25.100 --> 00:26:27.180]   The one we all read about and know about
[00:26:27.180 --> 00:26:29.020]   is the DNA, the genome.
[00:26:29.020 --> 00:26:32.460]   And that's base four information, ATCG,
[00:26:32.460 --> 00:26:35.260]   the four chemicals that make up the various sequences
[00:26:35.260 --> 00:26:37.020]   of the genome, billions of letters.
[00:26:37.020 --> 00:26:39.980]   And that also degrades over time.
[00:26:39.980 --> 00:26:42.620]   But what's been fascinating is that we find
[00:26:42.620 --> 00:26:45.140]   that that information is pretty much intact
[00:26:45.140 --> 00:26:47.300]   in old animals and people.
[00:26:47.300 --> 00:26:49.100]   You can clone a dog, one of my friends in LA
[00:26:49.100 --> 00:26:50.980]   just cloned his dog three times.
[00:26:50.980 --> 00:26:52.060]   So this is doable, right?
[00:26:52.060 --> 00:26:53.780]   That means that the genome can be intact.
[00:26:53.780 --> 00:26:56.220]   But what's the other type of information?
[00:26:56.220 --> 00:26:57.460]   It's the epigenome,
[00:26:57.460 --> 00:27:01.220]   the regulators of the genetic information.
[00:27:01.220 --> 00:27:04.380]   And physically, that's really just how the DNA is wrapped up
[00:27:04.380 --> 00:27:08.060]   or looped out for the cell to access it and read it.
[00:27:08.060 --> 00:27:11.220]   So it's similar to, and excuse this analogy,
[00:27:11.220 --> 00:27:15.180]   but it's a good one, a compact disc or DVD.
[00:27:15.180 --> 00:27:18.500]   Those pits in the foil are the digital information,
[00:27:18.500 --> 00:27:19.540]   that's the genome.
[00:27:19.540 --> 00:27:22.260]   And the epigenome is the reader of that information.
[00:27:22.260 --> 00:27:25.300]   And in a different cell, you'd read different music,
[00:27:25.300 --> 00:27:28.020]   different songs, different symphonies.
[00:27:28.020 --> 00:27:31.620]   And that's what gets laid down when we're in the womb.
[00:27:31.620 --> 00:27:34.620]   And that makes a skin cell forever a skin cell
[00:27:34.620 --> 00:27:36.340]   and not a brain cell tomorrow.
[00:27:36.340 --> 00:27:38.740]   Thank God, otherwise our brains wouldn't work very well.
[00:27:38.740 --> 00:27:41.220]   But over time, what we see is that the brain cells
[00:27:41.220 --> 00:27:42.860]   start to look more like skin cells.
[00:27:42.860 --> 00:27:45.620]   And the kidney cells start to look more like liver cells.
[00:27:45.620 --> 00:27:48.100]   And they, what we call X differentiate,
[00:27:48.100 --> 00:27:49.420]   this is a term that we use in my lab
[00:27:49.420 --> 00:27:52.080]   but isn't yet widely used.
[00:27:52.080 --> 00:27:53.420]   But we needed a term to explain this.
[00:27:53.420 --> 00:27:55.820]   And that process of X differentiation,
[00:27:55.820 --> 00:28:00.820]   the loss of the reader of the CD or the DVD,
[00:28:00.820 --> 00:28:06.140]   we liken that to scratches on the DVD
[00:28:06.140 --> 00:28:09.420]   so that the reader cannot fully access the information.
[00:28:09.420 --> 00:28:11.460]   Now we can slow down the scratches, as I mentioned.
[00:28:11.460 --> 00:28:13.140]   We can turn on these genes.
[00:28:13.140 --> 00:28:15.860]   We can even put in molecules into the cell
[00:28:15.860 --> 00:28:18.980]   or even eat them and turn on those pathways,
[00:28:18.980 --> 00:28:21.520]   which my father and I have been trying to do
[00:28:21.520 --> 00:28:24.400]   for about a decade to slow things down.
[00:28:24.400 --> 00:28:26.380]   But the question that I've had is,
[00:28:26.380 --> 00:28:30.820]   is there a repository of information still in the body?
[00:28:30.820 --> 00:28:32.220]   Because anyone who knows anything
[00:28:32.220 --> 00:28:33.400]   about the loss of information
[00:28:33.400 --> 00:28:35.540]   or even has tried to copy a cassette tape
[00:28:35.540 --> 00:28:37.580]   or photocopy or Xerox anything
[00:28:37.580 --> 00:28:41.780]   knows that over time you lose that information irreparably.
[00:28:41.780 --> 00:28:43.940]   So I've been looking for a backup copy
[00:28:43.940 --> 00:28:46.700]   inspired largely by Claude Shannon's work
[00:28:46.700 --> 00:28:48.780]   at MIT as well in the 1940s.
[00:28:49.660 --> 00:28:53.040]   His mathematical theory of communication is just brilliant.
[00:28:53.040 --> 00:28:55.480]   And so I've been looking for what he called the observer,
[00:28:55.480 --> 00:28:57.200]   which is the backup copy.
[00:28:57.200 --> 00:29:01.500]   We today might call that the TCP/IP protocol of the internet
[00:29:01.500 --> 00:29:04.000]   that stores information in case it doesn't make it
[00:29:04.000 --> 00:29:06.920]   to your computer, it will fill in the gaps.
[00:29:06.920 --> 00:29:09.360]   And we've been spending about the last five years
[00:29:09.360 --> 00:29:12.160]   to try and find if there really is a backup copy in the body
[00:29:12.160 --> 00:29:15.960]   to reset the epigenome and polish those scratches away.
[00:29:15.960 --> 00:29:16.960]   - That's incredible.
[00:29:16.960 --> 00:29:18.520]   So finding the backup,
[00:29:18.520 --> 00:29:21.400]   so whenever there are too many scratches pile up,
[00:29:21.400 --> 00:29:24.520]   you can just write a new version.
[00:29:24.520 --> 00:29:26.540]   Like write, not a new version,
[00:29:26.540 --> 00:29:29.120]   but go to the backup and restore it.
[00:29:29.120 --> 00:29:31.680]   - Right, that's really all we're talking about.
[00:29:31.680 --> 00:29:34.440]   It's not that hard once you know the trick.
[00:29:34.440 --> 00:29:36.720]   - And for people that actually remember
[00:29:36.720 --> 00:29:41.280]   like DVDs and scratches on them, how frustrating it is,
[00:29:41.280 --> 00:29:44.180]   that's a brilliant metaphor for aging.
[00:29:45.800 --> 00:29:50.800]   And then the reader is the thing that skips
[00:29:50.800 --> 00:29:53.400]   and then it could destroy your experience,
[00:29:53.400 --> 00:29:54.800]   the richness of the experience
[00:29:54.800 --> 00:29:57.140]   that is listening to your favorite song.
[00:29:57.140 --> 00:29:59.000]   - Right, but in biology, it's even worse
[00:29:59.000 --> 00:30:01.120]   'cause you'll lose your memory, your kidneys will fail,
[00:30:01.120 --> 00:30:03.640]   you'll get diabetes, your heart will fail.
[00:30:03.640 --> 00:30:06.880]   And we call that aging and age-related diseases.
[00:30:06.880 --> 00:30:10.040]   So most people forget that diseases that we get
[00:30:10.040 --> 00:30:13.860]   when we get old are 80 to 90% caused by aging.
[00:30:13.860 --> 00:30:15.960]   And we've been trying to fix things with Band-Aids
[00:30:15.960 --> 00:30:19.040]   after they occur without even generally talking
[00:30:19.040 --> 00:30:21.360]   about the root cause of the problem.
[00:30:21.360 --> 00:30:26.240]   - Is there the scratches, do those come from,
[00:30:26.240 --> 00:30:31.900]   are those programmed or are they failures?
[00:30:31.900 --> 00:30:36.440]   Meaning is it, so if it's by design,
[00:30:36.440 --> 00:30:40.960]   then there's like a encoded timeline schedule
[00:30:40.960 --> 00:30:43.000]   that the body's just on purpose
[00:30:43.000 --> 00:30:44.560]   of degrading the whole thing.
[00:30:44.560 --> 00:30:47.080]   And then there's the just the wear and tear
[00:30:47.080 --> 00:30:50.160]   of like the scratches and a disc that happen through time.
[00:30:50.160 --> 00:30:52.760]   Which one is it that's the source of aging?
[00:30:52.760 --> 00:30:56.920]   - It's more akin to wear and tear, there isn't a program.
[00:30:56.920 --> 00:31:00.960]   Getting back to evolution, there's no selection for aging.
[00:31:00.960 --> 00:31:03.800]   We're not designed to age, we just live as long as we need to
[00:31:03.800 --> 00:31:06.140]   and then we're at the whim of entropy basically.
[00:31:06.140 --> 00:31:09.120]   Second law of thermodynamics, stuff falls apart.
[00:31:09.120 --> 00:31:11.200]   We live a bit longer than age 40
[00:31:11.200 --> 00:31:13.400]   only because there are robust resilient systems
[00:31:13.400 --> 00:31:15.320]   but eventually they fail as well.
[00:31:15.320 --> 00:31:16.940]   Current limit to the human lifespan
[00:31:16.940 --> 00:31:19.300]   where they completely fail is 122.
[00:31:19.300 --> 00:31:23.240]   But I don't like to think of it as wear and tear
[00:31:23.240 --> 00:31:25.880]   because there's two aspects to it.
[00:31:25.880 --> 00:31:28.080]   There's a system that's built to keep us alive
[00:31:28.080 --> 00:31:30.960]   when we're young but actually goes,
[00:31:30.960 --> 00:31:33.280]   comes back to bite us as we get older.
[00:31:33.280 --> 00:31:37.800]   And we call this issue antagonistic pleiotropy.
[00:31:37.800 --> 00:31:39.480]   What's good for you when you're young
[00:31:39.480 --> 00:31:42.920]   can cause problems when you're older.
[00:31:42.920 --> 00:31:44.640]   So we've been looking what is the cause of,
[00:31:44.640 --> 00:31:46.140]   the main causes of the noise
[00:31:46.140 --> 00:31:49.320]   and we've found two of them definitively.
[00:31:49.320 --> 00:31:52.000]   The first one is broken chromosomes.
[00:31:52.000 --> 00:31:55.000]   When a chromosome breaks, the cell has to panic
[00:31:55.000 --> 00:31:58.280]   because that's either gonna cause a cancer or kill the cell.
[00:31:58.280 --> 00:32:01.200]   There's only two outcomes, it's pretty much a problem.
[00:32:01.200 --> 00:32:05.080]   And so what the cell does is it reorganizes the epigenome
[00:32:05.080 --> 00:32:06.100]   in a massive way.
[00:32:07.360 --> 00:32:09.160]   What that leads to is,
[00:32:09.160 --> 00:32:12.600]   think of it as a tennis match or a ping pong game.
[00:32:12.600 --> 00:32:14.120]   The proteins are the balls
[00:32:14.120 --> 00:32:15.920]   and they now leave where they should be,
[00:32:15.920 --> 00:32:19.120]   which is regulating the genes that make the cell type,
[00:32:19.120 --> 00:32:20.040]   whatever it is.
[00:32:20.040 --> 00:32:21.600]   And they have a dual function,
[00:32:21.600 --> 00:32:23.080]   they actually go to the break,
[00:32:23.080 --> 00:32:25.620]   the chromosome will break and fix that.
[00:32:25.620 --> 00:32:26.820]   And then they come back.
[00:32:26.820 --> 00:32:29.320]   You might ask, well, why is it set up that way?
[00:32:29.320 --> 00:32:30.320]   Well, it's a beautiful system,
[00:32:30.320 --> 00:32:31.840]   it coordinates gene expression,
[00:32:31.840 --> 00:32:33.780]   the control systems with the repair.
[00:32:33.780 --> 00:32:35.080]   You want them coordinated.
[00:32:35.960 --> 00:32:37.560]   Problem is as we get older,
[00:32:37.560 --> 00:32:39.500]   this ping pong game, some of the balls get lost.
[00:32:39.500 --> 00:32:42.600]   They don't come back to where they originally started.
[00:32:42.600 --> 00:32:46.960]   And that's what we think is the main noise for aging.
[00:32:46.960 --> 00:32:49.440]   And we've also, the other cause of aging that we found
[00:32:49.440 --> 00:32:52.920]   is cell stress, we damage nerves and they age rapidly.
[00:32:52.920 --> 00:32:54.640]   So that's the other issue.
[00:32:54.640 --> 00:32:55.840]   There's probably others.
[00:32:55.840 --> 00:32:58.220]   Smoking chemicals, for example,
[00:32:58.220 --> 00:33:00.960]   we know accelerates biological age pretty dramatically.
[00:33:00.960 --> 00:33:04.160]   But the question is, can you slow that down
[00:33:04.160 --> 00:33:06.200]   or can you reset them to get those ping pong balls
[00:33:06.200 --> 00:33:09.680]   to go back to where they originally started in the game?
[00:33:09.680 --> 00:33:12.040]   And we think we've found a way to do that.
[00:33:12.040 --> 00:33:13.920]   - What, can you give me hints?
[00:33:13.920 --> 00:33:15.960]   Whose fault is it, and the ball's not coming back,
[00:33:15.960 --> 00:33:17.720]   is it the proteins themselves?
[00:33:17.720 --> 00:33:20.120]   Like, are they starting?
[00:33:20.120 --> 00:33:22.280]   Again, I've been obsessed with the protein folding problem
[00:33:22.280 --> 00:33:23.240]   from the AI perspective.
[00:33:23.240 --> 00:33:25.680]   So is it the proteins or is it something else?
[00:33:25.680 --> 00:33:29.280]   - Well, we know who hits the balls and recruits them.
[00:33:29.280 --> 00:33:33.080]   So that the break is recognized by proteins
[00:33:33.080 --> 00:33:37.040]   who send out a signal through phosphorylation
[00:33:37.040 --> 00:33:40.960]   is typical way cells talk to other proteins.
[00:33:40.960 --> 00:33:43.540]   And that recruits those repair factors,
[00:33:43.540 --> 00:33:45.080]   those ping pong balls to the break.
[00:33:45.080 --> 00:33:49.120]   So the cell's actively doing this to try and help itself.
[00:33:49.120 --> 00:33:53.920]   But we don't know who's to blame for them not coming back.
[00:33:53.920 --> 00:33:58.480]   That could just be a flaw in the quote unquote design.
[00:33:58.480 --> 00:34:00.280]   I don't think that there's something saying,
[00:34:00.280 --> 00:34:04.200]   well, 1% of you balls, proteins never go back.
[00:34:04.200 --> 00:34:06.120]   I just think it's hard to reset a system
[00:34:06.120 --> 00:34:07.680]   that's constantly changing.
[00:34:07.680 --> 00:34:11.340]   We have in our bodies close to a trillion DNA breaks
[00:34:11.340 --> 00:34:14.280]   every day, and imagine that over 80 years,
[00:34:14.280 --> 00:34:17.520]   what damage that does to our epigenomic information.
[00:34:17.520 --> 00:34:20.280]   Now we know that this is, well,
[00:34:20.280 --> 00:34:21.540]   we never know anything in biology,
[00:34:21.540 --> 00:34:23.960]   but we have strong evidence that this is true
[00:34:23.960 --> 00:34:27.580]   because we can mess with animals,
[00:34:27.580 --> 00:34:30.060]   we can create DNA breaks and tickle them
[00:34:30.060 --> 00:34:32.940]   with a few breaks, maybe raise it by threefold
[00:34:32.940 --> 00:34:35.800]   over background levels of normal breakage.
[00:34:35.800 --> 00:34:38.320]   And if we're right, those mice should get old.
[00:34:38.320 --> 00:34:40.380]   And they do.
[00:34:40.380 --> 00:34:42.420]   We can actually, we've created these breaks
[00:34:42.420 --> 00:34:44.000]   in a way that's titratable.
[00:34:44.000 --> 00:34:46.680]   We can, it's like a rheostat, we can send it to 11.
[00:34:46.680 --> 00:34:51.360]   I drove my Tesla here, I'm a big fan of Spinal Tap 2,
[00:34:51.360 --> 00:34:52.200]   going to 11.
[00:34:52.200 --> 00:34:54.200]   If we go to 11, we can make a mouse old
[00:34:54.200 --> 00:34:55.740]   in a matter of months.
[00:34:55.740 --> 00:34:58.600]   We prefer to go to a level of about four,
[00:34:58.600 --> 00:35:00.180]   and it gets old in 10 months.
[00:35:00.180 --> 00:35:01.160]   But it's definitely old.
[00:35:01.160 --> 00:35:03.700]   It's got all of the hallmarks of aging,
[00:35:03.700 --> 00:35:06.020]   it's got diseases, it looks old,
[00:35:06.020 --> 00:35:08.020]   its skin is old, it's got gray hair.
[00:35:08.020 --> 00:35:09.980]   But importantly, we can now measure age
[00:35:09.980 --> 00:35:11.400]   by looking at the scratches.
[00:35:11.400 --> 00:35:13.420]   We can look at the epigenome, we can measure it,
[00:35:13.420 --> 00:35:15.780]   and use machine learning to give us a number,
[00:35:15.780 --> 00:35:19.140]   and those mice are 50% older than normal.
[00:35:19.140 --> 00:35:20.700]   - So you can replicate the aging process
[00:35:20.700 --> 00:35:21.860]   in a controlled way.
[00:35:21.860 --> 00:35:24.060]   You can, I mean, in a way that you,
[00:35:24.060 --> 00:35:25.700]   I mean, you could accelerate it,
[00:35:27.680 --> 00:35:31.680]   in a controlled way, and measure how much exactly it's aging,
[00:35:31.680 --> 00:35:34.960]   and that gives you step one of a two-step process
[00:35:34.960 --> 00:35:36.320]   to when you can then figure out,
[00:35:36.320 --> 00:35:37.960]   well, how can we reverse this?
[00:35:37.960 --> 00:35:40.160]   - And now we're reversing those mice.
[00:35:40.160 --> 00:35:42.520]   - Is there a good, I love what you said.
[00:35:42.520 --> 00:35:44.620]   I mean, in biology, you really don't know.
[00:35:44.620 --> 00:35:47.640]   It's such a beautiful mess.
[00:35:47.640 --> 00:35:51.480]   Is there ideas how to do that?
[00:35:51.480 --> 00:35:55.000]   Is that on a genetic engineering level?
[00:35:55.000 --> 00:35:57.580]   Is it, like, what can you mess with?
[00:35:57.580 --> 00:36:02.280]   Is it going to the, trying to discover the backup copies,
[00:36:02.280 --> 00:36:04.180]   and restoring from them?
[00:36:04.180 --> 00:36:06.380]   Like, what's, if it's possible to convert it
[00:36:06.380 --> 00:36:09.480]   to natural language words, what are the ideas here?
[00:36:09.480 --> 00:36:11.460]   - What is the observer, and how do we contact it?
[00:36:11.460 --> 00:36:14.300]   - Exactly, what's the observer, and how do you contact it?
[00:36:14.300 --> 00:36:15.860]   Or if there's other ideas,
[00:36:15.860 --> 00:36:20.580]   how to reverse the balls-getting-lost process.
[00:36:20.580 --> 00:36:22.860]   - Yeah, well, you can slow it down.
[00:36:22.860 --> 00:36:23.700]   - Slow it.
[00:36:23.700 --> 00:36:26.260]   But we found a reset switch recently.
[00:36:26.260 --> 00:36:31.260]   We just published this in the December 2020 issue of Nature.
[00:36:31.260 --> 00:36:36.900]   And what we found is that there are three embryonic genes
[00:36:36.900 --> 00:36:39.800]   that we could put into the adult animal
[00:36:39.800 --> 00:36:42.040]   to reset the age of the tissues.
[00:36:42.040 --> 00:36:44.700]   And it only takes four to eight weeks to work well.
[00:36:44.700 --> 00:36:46.060]   And we can take a blind mouse
[00:36:46.060 --> 00:36:48.340]   that's lost its vision due to aging,
[00:36:48.340 --> 00:36:50.460]   neurons aren't working well towards the brain,
[00:36:50.460 --> 00:36:52.660]   reset those neurons back to a younger age,
[00:36:52.660 --> 00:36:54.220]   and now the mice can see again.
[00:36:54.220 --> 00:36:57.220]   These three genes are famous, actually,
[00:36:57.220 --> 00:36:59.460]   because they're a set of four genes
[00:36:59.460 --> 00:37:01.700]   discovered by Shinya Yamanaka,
[00:37:01.700 --> 00:37:04.020]   who won the Nobel Prize in 2016,
[00:37:04.020 --> 00:37:05.900]   for discovering that those four genes,
[00:37:05.900 --> 00:37:09.620]   when turned on at high levels in adult cells,
[00:37:09.620 --> 00:37:11.880]   can generate stem cells.
[00:37:11.880 --> 00:37:14.060]   And this is, I think, well-known now
[00:37:14.060 --> 00:37:16.900]   that we can create stem cells from adult tissue.
[00:37:16.900 --> 00:37:19.460]   But what wasn't known is, can you partially take age back
[00:37:19.460 --> 00:37:20.820]   without becoming a tumor,
[00:37:20.820 --> 00:37:22.540]   or generating a stem cell in the eye,
[00:37:22.540 --> 00:37:24.100]   which would be a disaster?
[00:37:24.100 --> 00:37:24.940]   And the answer is yes.
[00:37:24.940 --> 00:37:26.620]   There is a system in the body
[00:37:26.620 --> 00:37:29.060]   that can take the age of a cell back to a certain point,
[00:37:29.060 --> 00:37:32.300]   but no further, safely, and reset the age.
[00:37:32.300 --> 00:37:36.580]   And we're now using that to reset the age of the brain
[00:37:36.580 --> 00:37:39.340]   of those mice that we aged prematurely,
[00:37:39.340 --> 00:37:42.540]   and they're getting their ability to learn back.
[00:37:42.540 --> 00:37:44.300]   - This is really exciting, right?
[00:37:44.300 --> 00:37:46.860]   Like, what's the downside of this?
[00:37:46.860 --> 00:37:49.100]   - Well, the downside is if you overdo it,
[00:37:49.100 --> 00:37:52.180]   and you don't get it right, you might cause tumors.
[00:37:53.020 --> 00:37:54.940]   But we do it very carefully,
[00:37:54.940 --> 00:37:57.900]   and we also know that in the eye, it's very safe.
[00:37:57.900 --> 00:38:01.620]   We also injected these, we deliver them by viruses,
[00:38:01.620 --> 00:38:06.040]   so we can control where and when they get turned on.
[00:38:06.040 --> 00:38:08.540]   And in this paper, we've published that
[00:38:08.540 --> 00:38:10.540]   if we put high levels in the mouse,
[00:38:10.540 --> 00:38:12.380]   into their veins, throughout the body,
[00:38:12.380 --> 00:38:14.720]   they don't get cancer for over a year.
[00:38:14.720 --> 00:38:18.620]   So I'm so optimistic that we're going into human studies
[00:38:18.620 --> 00:38:20.420]   in less than two years from now.
[00:38:20.420 --> 00:38:23.980]   - Is there a place where AI can help?
[00:38:23.980 --> 00:38:27.700]   Sorry to inject one of the things
[00:38:27.700 --> 00:38:30.220]   I'm very excited about and passionate about.
[00:38:30.220 --> 00:38:35.060]   So Google DeepMind recently had a big breakthrough
[00:38:35.060 --> 00:38:39.100]   with AlphaFold2, but also AlphaFold two years ago,
[00:38:39.100 --> 00:38:44.100]   with achieving sort of state-of-the-art performance
[00:38:44.100 --> 00:38:49.260]   on the protein folding problem, single protein folding.
[00:38:49.260 --> 00:38:52.300]   But it also paints a hopeful picture
[00:38:52.300 --> 00:38:54.940]   of what's possible to do in terms of simulating
[00:38:54.940 --> 00:38:56.100]   the folding of proteins,
[00:38:56.100 --> 00:39:00.740]   but also simulating biological systems through AI.
[00:39:00.740 --> 00:39:04.900]   Is there something to you, combined with this brilliant work
[00:39:04.900 --> 00:39:07.940]   on the biology side that you're hopeful about,
[00:39:07.940 --> 00:39:10.300]   where AI can be a tool to help?
[00:39:10.300 --> 00:39:11.580]   - Where isn't that a tool?
[00:39:11.580 --> 00:39:13.540]   I mean, if you're not using AI right now in biology,
[00:39:13.540 --> 00:39:14.700]   you're getting left behind.
[00:39:14.700 --> 00:39:15.580]   We use it all the time.
[00:39:15.580 --> 00:39:18.620]   We're using it to generate these biological clocks
[00:39:18.620 --> 00:39:21.220]   to be able to read those scratches.
[00:39:21.220 --> 00:39:24.020]   We're using it to predict the folding of proteins
[00:39:24.020 --> 00:39:27.500]   so we can target molecules and modulate their activity.
[00:39:27.500 --> 00:39:30.620]   We're using it to assemble genomes of different species.
[00:39:30.620 --> 00:39:32.460]   What else?
[00:39:32.460 --> 00:39:35.900]   We use it to predict the longevity of a mouse
[00:39:35.900 --> 00:39:38.660]   based on how it reacts to certain things,
[00:39:38.660 --> 00:39:41.140]   hearing, eyesight, generally frailty.
[00:39:41.140 --> 00:39:43.740]   So we just put out a paper last year on that.
[00:39:43.740 --> 00:39:46.940]   The other thing we can use it for,
[00:39:46.940 --> 00:39:49.140]   which is a little off the track here,
[00:39:49.140 --> 00:39:51.340]   but we use it for predicting
[00:39:51.340 --> 00:39:53.300]   which microorganisms are in your body.
[00:39:53.300 --> 00:39:55.700]   Actually, not predicting, telling you.
[00:39:55.700 --> 00:39:58.420]   So our daughter, Natalie,
[00:39:58.420 --> 00:40:00.940]   was infected with Lyme disease a few years ago,
[00:40:00.940 --> 00:40:02.440]   almost went blind from it.
[00:40:02.440 --> 00:40:03.780]   And the test took four days.
[00:40:03.780 --> 00:40:06.940]   And I thought, just give me the DNA from her spinal fluid.
[00:40:06.940 --> 00:40:07.920]   I'll go tell you what's in it,
[00:40:07.920 --> 00:40:09.380]   if it's Lyme disease or not.
[00:40:09.380 --> 00:40:10.220]   They refused.
[00:40:10.220 --> 00:40:12.620]   And so at that point I said, this has to be done better.
[00:40:12.620 --> 00:40:15.740]   So I've started a company that now can take a sample
[00:40:15.740 --> 00:40:17.300]   of any part of your body.
[00:40:17.300 --> 00:40:21.660]   It's typically done now with liver transplant patients
[00:40:21.660 --> 00:40:24.920]   to detect viruses that come out of their organs.
[00:40:24.920 --> 00:40:28.420]   But that's another area that AI is extremely important for.
[00:40:28.420 --> 00:40:31.500]   I think if you're not, in five years,
[00:40:31.500 --> 00:40:35.380]   if you're not using deep learning, you've got a problem.
[00:40:35.380 --> 00:40:36.940]   Because the amount of data that we generate now
[00:40:36.940 --> 00:40:39.460]   as biologists is just terabytes.
[00:40:39.460 --> 00:40:40.460]   It can be terabytes per week,
[00:40:40.460 --> 00:40:42.300]   it'll eventually be terabytes per day.
[00:40:42.300 --> 00:40:43.940]   And then we just go from there.
[00:40:43.940 --> 00:40:46.340]   And I actually have trouble recruiting enough
[00:40:46.340 --> 00:40:47.680]   bioinformaticians.
[00:40:47.680 --> 00:40:51.380]   A lot of our work is now just number crunching.
[00:40:51.380 --> 00:40:55.140]   - A part of that is collecting the data,
[00:40:55.140 --> 00:40:57.580]   which is kind of something we've talked a little bit about.
[00:40:57.580 --> 00:41:02.480]   But is there something you can say about how we can collect
[00:41:02.480 --> 00:41:04.460]   more and more data?
[00:41:04.460 --> 00:41:07.580]   Not just on the one person level,
[00:41:07.580 --> 00:41:12.580]   like for you to understand your various markers,
[00:41:13.220 --> 00:41:16.620]   but to create huge datasets,
[00:41:16.620 --> 00:41:20.460]   to understand how we can detect certain pathogens,
[00:41:20.460 --> 00:41:23.120]   detect certain properties, characteristics of,
[00:41:23.120 --> 00:41:25.580]   whether it's aging or all the other ways
[00:41:25.580 --> 00:41:27.300]   that a human body can fail.
[00:41:27.300 --> 00:41:29.740]   It seems like with biology,
[00:41:29.740 --> 00:41:33.420]   there's a kind of privacy concerns that,
[00:41:33.420 --> 00:41:34.940]   well, actually not privacy concerns,
[00:41:34.940 --> 00:41:38.220]   it's almost like regulation that kind of prevents
[00:41:38.220 --> 00:41:40.340]   like hospitals from sharing data.
[00:41:42.820 --> 00:41:44.500]   I'm not sure exactly how to say it,
[00:41:44.500 --> 00:41:48.280]   but it seems like when you look at autonomous vehicles,
[00:41:48.280 --> 00:41:50.220]   people are much more willing to share data.
[00:41:50.220 --> 00:41:52.580]   When you look at human biology system,
[00:41:52.580 --> 00:41:54.380]   people are much less willing to share data.
[00:41:54.380 --> 00:41:57.940]   Is there a hopeful path forward where we can share
[00:41:57.940 --> 00:42:00.240]   more and more data at a large scale
[00:42:00.240 --> 00:42:03.060]   that ultimately ends up helping us understand
[00:42:03.060 --> 00:42:06.780]   the human body and then treat problems with the human body?
[00:42:06.780 --> 00:42:08.300]   - So we are right in the middle.
[00:42:08.300 --> 00:42:10.140]   We're living through what's going to be seen
[00:42:10.140 --> 00:42:12.640]   as one of the biggest revolutions in human health,
[00:42:12.640 --> 00:42:16.380]   through the gathering of data about our bodies.
[00:42:16.380 --> 00:42:19.500]   And 20 years ago, people didn't want to go on social media,
[00:42:19.500 --> 00:42:20.340]   they're worried about it.
[00:42:20.340 --> 00:42:22.720]   Now you have to, if you're a kid, that's for sure.
[00:42:22.720 --> 00:42:25.500]   Same with medical records.
[00:42:25.500 --> 00:42:29.420]   These are becoming all digitized and expanded.
[00:42:29.420 --> 00:42:31.300]   Ultimately, we're going to,
[00:42:31.300 --> 00:42:34.780]   even if we don't want to, have to be monitored.
[00:42:34.780 --> 00:42:36.700]   There's going to be a court case that,
[00:42:36.700 --> 00:42:39.780]   I bet two, three years from now, someone's going to say,
[00:42:39.780 --> 00:42:42.380]   how come my father died from a heart attack?
[00:42:42.380 --> 00:42:45.740]   You had these biosensors, 20 bucks, and you didn't use it.
[00:42:45.740 --> 00:42:47.460]   Lawsuit right there, and suddenly,
[00:42:47.460 --> 00:42:49.500]   all hospitals have to give you one of these.
[00:42:49.500 --> 00:42:51.800]   - There'll be a reversal, like to where,
[00:42:51.800 --> 00:42:54.780]   it's your fault if you don't collect the data,
[00:42:54.780 --> 00:42:58.140]   that's brilliant, and that's absolutely right.
[00:42:58.140 --> 00:43:00.800]   I mean, that's absolutely right.
[00:43:00.800 --> 00:43:03.860]   That's the frustration I feel in going to the doctor,
[00:43:03.860 --> 00:43:08.860]   is like, it's almost negligent to not collect the data,
[00:43:10.580 --> 00:43:11.900]   because you're making,
[00:43:11.900 --> 00:43:14.020]   there's something really wrong with me,
[00:43:14.020 --> 00:43:17.920]   and you're making decisions based on very few tests.
[00:43:17.920 --> 00:43:20.140]   That's almost negligent, when you have the opportunity
[00:43:20.140 --> 00:43:22.000]   to collect a huge amount more data.
[00:43:22.000 --> 00:43:24.000]   - Well, let me tell you something, Lex.
[00:43:24.000 --> 00:43:29.060]   I've got this inside tracker data for myself over a decade,
[00:43:29.060 --> 00:43:31.820]   and you'd think my doctor would roll his eyes at this.
[00:43:31.820 --> 00:43:34.780]   Oh, he's gone to a consumer company, blah, blah, blah.
[00:43:34.780 --> 00:43:37.420]   I had my first checkup in a year with him
[00:43:37.420 --> 00:43:39.120]   through video conference,
[00:43:39.120 --> 00:43:42.680]   and he was running blind.
[00:43:42.680 --> 00:43:45.380]   He really didn't know what was going on with me.
[00:43:45.380 --> 00:43:47.180]   He asked the usual things, how am I sleeping,
[00:43:47.180 --> 00:43:50.060]   how am I eating, these kind of usual things.
[00:43:50.060 --> 00:43:52.340]   And I said, well, I've got new tests back
[00:43:52.340 --> 00:43:54.420]   from inside tracker, and he said, great,
[00:43:54.420 --> 00:43:55.900]   I'd love to see them.
[00:43:55.900 --> 00:43:58.120]   So I share screen, and we look at the graphs,
[00:43:58.120 --> 00:44:00.860]   look at the data, and he's loving it,
[00:44:00.860 --> 00:44:04.500]   'cause he cannot order these tests willy-nilly.
[00:44:04.500 --> 00:44:08.980]   So I said, well, let's order a HbA1c blood glucose levels,
[00:44:08.980 --> 00:44:10.140]   because I'm very interested in that.
[00:44:10.140 --> 00:44:11.400]   That tracks with longevity.
[00:44:11.400 --> 00:44:13.940]   And he said, well, I have no reason to order that.
[00:44:13.940 --> 00:44:15.500]   Do you have a family history?
[00:44:15.500 --> 00:44:16.340]   No.
[00:44:16.340 --> 00:44:18.860]   Do you have any symptoms of diabetes?
[00:44:18.860 --> 00:44:20.660]   No, well, I can't order the test.
[00:44:20.660 --> 00:44:22.180]   I almost wanted to reach through the computer
[00:44:22.180 --> 00:44:25.980]   and strangle him, but instead, I pay a little bit
[00:44:25.980 --> 00:44:28.300]   to get these tests done, and then he looks at them.
[00:44:28.300 --> 00:44:30.600]   So that's now the way consumer health is going,
[00:44:30.600 --> 00:44:32.820]   is that you can get better data than your doctor can,
[00:44:32.820 --> 00:44:34.500]   but they like you to do that.
[00:44:34.500 --> 00:44:38.320]   - Quick human question, maybe you can educate me.
[00:44:38.320 --> 00:44:42.420]   I think doctors sometimes have a bit of an ego.
[00:44:42.420 --> 00:44:44.700]   I understand that the doctor's super experienced
[00:44:44.700 --> 00:44:47.620]   with a lot of things, but this is a fundamental question
[00:44:47.620 --> 00:44:49.260]   of human variability.
[00:44:49.260 --> 00:44:51.580]   Like, I know a lot of specific details about,
[00:44:51.580 --> 00:44:54.660]   I mean, it depends, of course, what we're talking about,
[00:44:54.660 --> 00:44:58.460]   but I bring a lot of knowledge, and if I have data with me,
[00:44:58.460 --> 00:45:02.980]   then I have several orders of magnitude more knowledge.
[00:45:02.980 --> 00:45:04.780]   And I think there's an aspect to it
[00:45:04.780 --> 00:45:09.720]   where the doctor has to put their expert hat,
[00:45:09.720 --> 00:45:13.240]   like, take it off, and actually be a curious,
[00:45:13.240 --> 00:45:16.540]   open-minded person, and study, and look at that data.
[00:45:16.540 --> 00:45:20.000]   Do you think it's possible to sort of change the culture
[00:45:20.000 --> 00:45:22.800]   of the medical system to where the doctors are almost,
[00:45:22.800 --> 00:45:25.700]   as you said, are excited to see the data?
[00:45:25.700 --> 00:45:26.840]   Or is that already happening?
[00:45:26.840 --> 00:45:27.680]   - It's really happening.
[00:45:27.680 --> 00:45:30.640]   Now, we've probably lost the last generation.
[00:45:30.640 --> 00:45:33.080]   There are no hopers, but, so I teach
[00:45:33.080 --> 00:45:35.960]   at Harvard Medical School, and they're excited about this.
[00:45:35.960 --> 00:45:37.120]   They're excited about aging,
[00:45:37.120 --> 00:45:39.560]   which is a new aspect to medicine.
[00:45:39.560 --> 00:45:41.640]   Oh, wow, we can do something about that?
[00:45:41.640 --> 00:45:43.880]   And then, yeah, all this data, what do we do with it?
[00:45:43.880 --> 00:45:45.320]   There's still the traditional pathology
[00:45:45.320 --> 00:45:47.180]   and all that stuff, which they need to know.
[00:45:47.180 --> 00:45:52.020]   But time will change their mindset.
[00:45:52.020 --> 00:45:54.360]   I'm not worried about that.
[00:45:54.360 --> 00:45:57.380]   And like we were discussing, this isn't a question of if,
[00:45:57.380 --> 00:45:59.020]   it's just a matter of when.
[00:45:59.020 --> 00:46:02.320]   And it's, you know, I have a front row seat on all of this.
[00:46:02.320 --> 00:46:04.520]   I had breakfast with a CEO
[00:46:04.520 --> 00:46:08.300]   who is making this happen just yesterday.
[00:46:08.300 --> 00:46:12.760]   I can tell you for sure that most people have no idea
[00:46:12.760 --> 00:46:14.620]   that this revolution is occurring
[00:46:14.620 --> 00:46:16.120]   and is happening so quickly.
[00:46:16.120 --> 00:46:18.360]   If you're running a hospital
[00:46:18.360 --> 00:46:21.360]   and you can save $2,000 per cardiac patient,
[00:46:21.360 --> 00:46:22.400]   what are you going to do?
[00:46:22.400 --> 00:46:23.380]   You have to use it.
[00:46:23.380 --> 00:46:25.640]   Otherwise, you know, the hospital down the road
[00:46:25.640 --> 00:46:28.160]   is going to be beating you.
[00:46:28.160 --> 00:46:30.880]   And there are large hospital aggregations.
[00:46:30.880 --> 00:46:32.920]   So there's Ascension and others
[00:46:32.920 --> 00:46:37.560]   that just have to go this way for budgetary reasons.
[00:46:37.560 --> 00:46:39.800]   And right now the US spends, what is it,
[00:46:39.800 --> 00:46:42.460]   17% of their GDP on healthcare.
[00:46:42.460 --> 00:46:45.920]   Let's say one of these buttons on my chest costs 20 bucks,
[00:46:45.920 --> 00:46:48.600]   it's rechargeable, and it can predict people's health
[00:46:48.600 --> 00:46:51.960]   and save on antibiotics, prevent heart attacks.
[00:46:51.960 --> 00:46:54.920]   How many billions, if not trillions of dollars
[00:46:54.920 --> 00:46:57.240]   will that save over the next decade?
[00:46:57.240 --> 00:47:00.240]   - Yeah, so when the public wakes up to this,
[00:47:00.240 --> 00:47:01.480]   they'll almost demand it.
[00:47:01.480 --> 00:47:04.040]   Like this should be accepted everywhere,
[00:47:04.040 --> 00:47:06.000]   this is obvious, it's going to save a lot of money,
[00:47:06.000 --> 00:47:07.640]   it's going to improve the quality of life.
[00:47:07.640 --> 00:47:11.480]   - Well, and the CFOs of hospital groups will have to.
[00:47:11.480 --> 00:47:15.720]   And insurance companies are going to want to get in on this.
[00:47:15.720 --> 00:47:17.600]   So now that gets to privacy, right?
[00:47:17.600 --> 00:47:20.440]   Should an insurance company have access to your data?
[00:47:20.440 --> 00:47:23.320]   I would say no, but you could voluntarily show them
[00:47:23.320 --> 00:47:25.480]   some of it if they give you a discount.
[00:47:25.480 --> 00:47:27.580]   And that's also being worked on right now.
[00:47:28.980 --> 00:47:30.920]   - I hope that we do create kind of systems
[00:47:30.920 --> 00:47:33.160]   where I can volunteer to share my data,
[00:47:33.160 --> 00:47:35.800]   and I can also take the data back,
[00:47:35.800 --> 00:47:39.040]   meaning like delete the data, request deletion of data.
[00:47:39.040 --> 00:47:41.440]   And then maybe policy creates rules
[00:47:41.440 --> 00:47:45.120]   to where you can share data, you could delete the data.
[00:47:45.120 --> 00:47:50.120]   And I think if I have the option to delete all my data
[00:47:50.120 --> 00:47:52.660]   that a particular company has,
[00:47:52.660 --> 00:47:54.560]   then I'll share my data with everyone.
[00:47:55.940 --> 00:48:00.940]   I feel like if, because that gives me the tools
[00:48:00.940 --> 00:48:04.580]   to be a consumer, an intelligent consumer,
[00:48:04.580 --> 00:48:08.700]   of awarding my data to a company that deserves it
[00:48:08.700 --> 00:48:11.260]   and taking it back when the company is misbehaving.
[00:48:11.260 --> 00:48:14.260]   And in that way, encourage as a consumer
[00:48:14.260 --> 00:48:15.800]   in the capitalist system,
[00:48:15.800 --> 00:48:18.780]   encourage the companies that are doing great work
[00:48:18.780 --> 00:48:20.300]   with that data.
[00:48:20.300 --> 00:48:24.260]   - Well, yeah, healthcare data security is number one
[00:48:24.260 --> 00:48:27.740]   in my mind, InsideTracker made sure that that was true.
[00:48:27.740 --> 00:48:31.180]   But these buttons on your chest,
[00:48:31.180 --> 00:48:32.200]   there's very private stuff.
[00:48:32.200 --> 00:48:35.560]   They can probably tell if you're having sex one night.
[00:48:35.560 --> 00:48:37.820]   So this is not the kind of stuff you want leaked.
[00:48:37.820 --> 00:48:39.860]   So I don't know whether it's blockchain or something.
[00:48:39.860 --> 00:48:41.860]   - Speak for yourself, I want this public.
[00:48:41.860 --> 00:48:43.060]   (Luke laughs)
[00:48:43.060 --> 00:48:45.660]   - Well, I guess it depends on how you go.
[00:48:45.660 --> 00:48:48.980]   But there's a lot of stuff you don't want out there.
[00:48:48.980 --> 00:48:51.620]   And this definitely has to be number one,
[00:48:51.620 --> 00:48:54.500]   'cause it's one thing to have your credit card
[00:48:54.500 --> 00:48:55.860]   information stolen, it's another thing
[00:48:55.860 --> 00:48:57.700]   your health records are permanently out there.
[00:48:57.700 --> 00:48:59.860]   - Yeah, so there's, on the biology side,
[00:48:59.860 --> 00:49:03.620]   super exciting ways to slow aging.
[00:49:03.620 --> 00:49:05.580]   But there's also on the lifestyle side.
[00:49:05.580 --> 00:49:08.060]   I've recently did a 72 hour fast,
[00:49:08.060 --> 00:49:11.680]   just an opportunity to take a pause and appreciate life.
[00:49:11.680 --> 00:49:14.380]   Think about, there's something about fasting
[00:49:14.380 --> 00:49:19.300]   that encourages you to reflect deeper
[00:49:19.300 --> 00:49:21.080]   than you otherwise might.
[00:49:21.080 --> 00:49:25.100]   The time kind of slows, and you also realize
[00:49:25.100 --> 00:49:27.420]   that you're human because your body needs food.
[00:49:27.420 --> 00:49:31.020]   And you start to see your body's almost as a machine
[00:49:31.020 --> 00:49:33.720]   that takes food and produces thoughts.
[00:49:33.720 --> 00:49:35.180]   (Luke laughs)
[00:49:35.180 --> 00:49:38.620]   And then ends, I mean, you start to,
[00:49:38.620 --> 00:49:41.780]   depending who you are, if you're engineering minded,
[00:49:41.780 --> 00:49:43.660]   you start to think of this whole thing
[00:49:43.660 --> 00:49:45.380]   as a kind of, yeah, as a machine.
[00:49:46.220 --> 00:49:50.900]   And then also feelings fill this machine.
[00:49:50.900 --> 00:49:52.500]   Feelings of gratitude, of love,
[00:49:52.500 --> 00:49:56.580]   but also the uglier things of jealousy,
[00:49:56.580 --> 00:49:59.700]   and greed, and hate, and all those kinds of things.
[00:49:59.700 --> 00:50:04.540]   You start to think, okay, how do I manage this body
[00:50:04.540 --> 00:50:06.180]   to create a rich experience?
[00:50:06.180 --> 00:50:07.780]   All of that comes from fasting for me.
[00:50:07.780 --> 00:50:11.780]   Anyway, but there's also health benefits to fasting.
[00:50:11.780 --> 00:50:13.380]   I intermittent fast a lot.
[00:50:13.380 --> 00:50:16.580]   I eat just one meal a day most of the time.
[00:50:16.580 --> 00:50:18.220]   Is there something you could say
[00:50:18.220 --> 00:50:20.540]   about the benefits of fasting in your own life,
[00:50:20.540 --> 00:50:23.500]   and in general, the anti-aging process?
[00:50:23.500 --> 00:50:25.660]   - Wow, you're a philosopher too.
[00:50:25.660 --> 00:50:26.940]   - Sorry, I apologize.
[00:50:26.940 --> 00:50:28.500]   - No, I'm impressed.
[00:50:28.500 --> 00:50:30.420]   True Renaissance man.
[00:50:30.420 --> 00:50:32.020]   It's a joy to be here.
[00:50:32.020 --> 00:50:34.140]   So when it comes to fasting, this is,
[00:50:34.140 --> 00:50:37.220]   being abstemious is one of the oldest ways
[00:50:37.220 --> 00:50:38.500]   to improve health.
[00:50:38.500 --> 00:50:41.620]   Probably they knew this 5,000 plus years ago.
[00:50:41.620 --> 00:50:43.180]   So that's not new.
[00:50:43.180 --> 00:50:45.660]   But what we're figuring out is what is optimal,
[00:50:45.660 --> 00:50:47.100]   and how does it work?
[00:50:47.100 --> 00:50:49.540]   And one of the things we help contribute to,
[00:50:49.540 --> 00:50:51.820]   which I can speak to with some authority,
[00:50:51.820 --> 00:50:54.420]   is that these longevity genes we work on,
[00:50:54.420 --> 00:50:56.380]   we showed back in the early 2000s,
[00:50:56.380 --> 00:50:58.620]   are turned on by fasting.
[00:50:58.620 --> 00:51:01.380]   And at least in yeast, we were the first to show
[00:51:01.380 --> 00:51:04.500]   how calorie restriction fasting works to extend lifespan.
[00:51:04.500 --> 00:51:06.480]   And that was the first for any species.
[00:51:06.480 --> 00:51:08.220]   Something similar happens in our bodies.
[00:51:08.220 --> 00:51:10.540]   When we're hungry, or put our bodies
[00:51:10.540 --> 00:51:12.660]   under any other perceived adversity,
[00:51:12.660 --> 00:51:14.700]   such as running, our bodies think,
[00:51:14.700 --> 00:51:17.940]   wow, we're getting run, chased by a saber-toothed cat
[00:51:17.940 --> 00:51:19.540]   or something.
[00:51:19.540 --> 00:51:22.460]   If we're really hot or cold, these probably also work.
[00:51:22.460 --> 00:51:24.780]   To put our bodies in this defensive state,
[00:51:24.780 --> 00:51:27.180]   to activate these genes in the way that whales do
[00:51:27.180 --> 00:51:28.640]   and mice don't.
[00:51:28.640 --> 00:51:31.340]   And so hunger is the best way to do that.
[00:51:31.340 --> 00:51:33.760]   In fact, I don't think you have to feel hungry.
[00:51:33.760 --> 00:51:35.340]   You can get used to it.
[00:51:35.340 --> 00:51:38.300]   But if there was one thing I would recommend to anybody
[00:51:38.300 --> 00:51:42.260]   to slow down aging, would be to skip a meal or two a day.
[00:51:42.860 --> 00:51:44.460]   Now, it doesn't mean you don't have to live well.
[00:51:44.460 --> 00:51:45.300]   You can go out.
[00:51:45.300 --> 00:51:47.720]   I go to restaurants, I eat regular food.
[00:51:47.720 --> 00:51:50.060]   I try to be as healthy as possible.
[00:51:50.060 --> 00:51:53.380]   But I've gone from skipping breakfast most of my life,
[00:51:53.380 --> 00:51:55.460]   now to skipping lunch as well.
[00:51:55.460 --> 00:51:59.300]   And I have my physique back that I had when I was 20.
[00:51:59.300 --> 00:52:01.260]   I feel 20 mentally.
[00:52:01.260 --> 00:52:02.500]   I'm much sharper.
[00:52:02.500 --> 00:52:03.500]   I don't feel tired anymore.
[00:52:03.500 --> 00:52:04.660]   I sleep well.
[00:52:04.660 --> 00:52:07.380]   So I'm a huge fan of the one meal a day thing.
[00:52:07.380 --> 00:52:11.120]   Where I'm not good at is going beyond one day.
[00:52:11.120 --> 00:52:12.380]   But if you do three days--
[00:52:12.380 --> 00:52:15.620]   - Have you ever fasted longer than 24 hours?
[00:52:15.620 --> 00:52:17.120]   - I tried doing two days.
[00:52:17.120 --> 00:52:20.180]   I might've made it to the third and given up.
[00:52:20.180 --> 00:52:23.820]   I just find that I don't have a lot of willpower.
[00:52:23.820 --> 00:52:24.900]   I also hate exercise.
[00:52:24.900 --> 00:52:27.820]   So I'm not sure how long I'm gonna live.
[00:52:27.820 --> 00:52:29.140]   But I've managed to do one meal a day.
[00:52:29.140 --> 00:52:31.980]   So if I can do that, seriously, anybody can do that.
[00:52:31.980 --> 00:52:35.980]   To your listeners and viewers, I would say,
[00:52:35.980 --> 00:52:38.100]   don't try to do it all at once.
[00:52:38.100 --> 00:52:40.700]   You can't go from snacking and eating three meals a day
[00:52:40.700 --> 00:52:42.800]   to what I do easily.
[00:52:42.800 --> 00:52:45.560]   Work your way up to it, but also compensate with drinking.
[00:52:45.560 --> 00:52:48.840]   If you like tea, if you like coffee, put some milk in it.
[00:52:48.840 --> 00:52:49.680]   That's fine.
[00:52:49.680 --> 00:52:52.960]   You can fill your stomach up with liquids, diet sodas.
[00:52:52.960 --> 00:52:54.140]   I get criticized for drinking,
[00:52:54.140 --> 00:52:56.500]   but I'm gonna continue to have those.
[00:52:56.500 --> 00:52:58.220]   But then I power through the day.
[00:52:58.220 --> 00:52:59.680]   I definitely don't feel tired.
[00:52:59.680 --> 00:53:00.820]   I don't have a lag anymore.
[00:53:00.820 --> 00:53:02.640]   But also give it at least two weeks
[00:53:02.640 --> 00:53:04.860]   'cause there's a habit as well.
[00:53:04.860 --> 00:53:06.480]   Having something in your mouth, chewing,
[00:53:06.480 --> 00:53:08.400]   feeling that fullness.
[00:53:08.400 --> 00:53:09.460]   You can break that habit.
[00:53:09.460 --> 00:53:12.040]   And within two, three weeks, you'll have done it.
[00:53:12.040 --> 00:53:12.880]   - Absolutely.
[00:53:12.880 --> 00:53:14.440]   So I'm not actually even that strict about it.
[00:53:14.440 --> 00:53:15.960]   You said diet soda.
[00:53:15.960 --> 00:53:18.480]   Yeah, people are very kind of weirdly strict
[00:53:18.480 --> 00:53:20.840]   about fasting, the rules and fasting.
[00:53:20.840 --> 00:53:25.640]   Like for example, I drank Element Electrolytes
[00:53:25.640 --> 00:53:28.520]   when I was fasting, and that has like five calories.
[00:53:28.520 --> 00:53:31.200]   And so technically it's not fasting.
[00:53:31.200 --> 00:53:33.700]   Or people will say like, if you drink coffee,
[00:53:33.700 --> 00:53:34.540]   there's caffeine.
[00:53:34.540 --> 00:53:36.960]   And they'll say that's technically not fasting
[00:53:36.960 --> 00:53:39.200]   'cause there's some kind of biological effects of caffeine.
[00:53:39.200 --> 00:53:40.280]   But whatever.
[00:53:40.280 --> 00:53:42.400]   Of course, there's like biological benefits
[00:53:42.400 --> 00:53:43.520]   that you can argue about.
[00:53:43.520 --> 00:53:45.960]   But there's also just experiential benefits.
[00:53:45.960 --> 00:53:48.780]   Just calorie restriction broadly
[00:53:48.780 --> 00:53:51.260]   has a certain experience to it that,
[00:53:51.260 --> 00:53:53.640]   like for me personally, just as you said,
[00:53:53.640 --> 00:53:55.160]   has made me feel really good.
[00:53:55.160 --> 00:54:00.000]   That said, like especially I've gained quite a bit of weight,
[00:54:00.000 --> 00:54:03.320]   like maybe even like 15 pounds, something like that,
[00:54:03.320 --> 00:54:05.640]   since I moved to Austin, Texas.
[00:54:05.640 --> 00:54:08.480]   And I still keep the same diet.
[00:54:08.480 --> 00:54:12.840]   But I eat a lot of meat in that one,
[00:54:12.840 --> 00:54:14.040]   just because it's delicious,
[00:54:14.040 --> 00:54:19.040]   because it's also the amazing people I met in Texas.
[00:54:19.040 --> 00:54:22.280]   It's just there's like a camaraderie, a friendship,
[00:54:22.280 --> 00:54:25.320]   a love to the people that like makes you really enjoy
[00:54:25.320 --> 00:54:29.320]   the atmosphere of eating the brisket and the meat.
[00:54:29.320 --> 00:54:31.060]   - Is this Joe Rogan insisting?
[00:54:31.060 --> 00:54:33.620]   - Joe is, I mean, it's very different.
[00:54:34.720 --> 00:54:38.680]   Joe loves bread and pasta.
[00:54:38.680 --> 00:54:42.200]   Like he knows that his body feels best
[00:54:42.200 --> 00:54:44.280]   doing keto or carnivore.
[00:54:44.280 --> 00:54:47.440]   So that's what he usually tries to stick to.
[00:54:47.440 --> 00:54:50.120]   But he also does not hold back.
[00:54:50.120 --> 00:54:52.680]   And he'll just eat pasta when he does pasta.
[00:54:52.680 --> 00:54:55.120]   And he sort of enjoys life in that way.
[00:54:55.120 --> 00:54:57.840]   I can't, I don't know how to enjoy life in that way.
[00:54:57.840 --> 00:55:01.840]   I also love pasta, but I'm just not going to enjoy it
[00:55:01.840 --> 00:55:05.760]   because I know my body ultimately
[00:55:05.760 --> 00:55:07.000]   does not feel good with pasta.
[00:55:07.000 --> 00:55:09.360]   So it's a funny kind of dichotomy.
[00:55:09.360 --> 00:55:13.800]   I would like to cheat, I guess,
[00:55:13.800 --> 00:55:17.080]   by eating more meat than I, you know,
[00:55:17.080 --> 00:55:20.400]   like overeating on the things
[00:55:20.400 --> 00:55:22.340]   that I know my body feels good on,
[00:55:22.340 --> 00:55:24.320]   as opposed to eating stuff I shouldn't,
[00:55:24.320 --> 00:55:26.400]   like cake and all those kinds of things.
[00:55:26.400 --> 00:55:31.400]   I tend to find happiness in overeating the good stuff
[00:55:32.400 --> 00:55:35.920]   versus eating the bad stuff.
[00:55:35.920 --> 00:55:37.720]   And that's the kind of balance.
[00:55:37.720 --> 00:55:40.060]   Him, he's like, fuck it.
[00:55:40.060 --> 00:55:43.360]   Every once in a while, you got to enjoy it.
[00:55:43.360 --> 00:55:48.360]   And then also coupled with that for him is just exercise,
[00:55:48.360 --> 00:55:51.060]   like then faces demons the next day
[00:55:51.060 --> 00:55:53.480]   and just like burn a huge amount of calories,
[00:55:53.480 --> 00:55:58.480]   which is, I mean, whatever's up with that guy's mind,
[00:55:58.800 --> 00:56:03.760]   there's an ability to fully experience life,
[00:56:03.760 --> 00:56:06.000]   which is represented by the pasta
[00:56:06.000 --> 00:56:09.160]   and the ability to just like fight the demons,
[00:56:09.160 --> 00:56:11.400]   which is represented by all the crazy kettleballs
[00:56:11.400 --> 00:56:13.040]   and running the hills
[00:56:13.040 --> 00:56:14.720]   and all this kind of stuff that he does.
[00:56:14.720 --> 00:56:16.040]   That takes a lot out of you
[00:56:16.040 --> 00:56:17.520]   doing that kind of insane exercise.
[00:56:17.520 --> 00:56:20.400]   And I think I'm more like you,
[00:56:20.400 --> 00:56:22.520]   or at least towards your direction is like,
[00:56:22.520 --> 00:56:24.400]   I really hate exercise.
[00:56:24.400 --> 00:56:26.760]   So I do it, but I really hate it.
[00:56:26.760 --> 00:56:29.600]   And so it's a balance that you have to strike.
[00:56:29.600 --> 00:56:30.640]   Is there something you could say
[00:56:30.640 --> 00:56:34.900]   about the diet side of that for you personally,
[00:56:34.900 --> 00:56:39.760]   but in general, in order to achieve calorie restriction,
[00:56:39.760 --> 00:56:43.920]   like for me eating, I know it may not sound healthy,
[00:56:43.920 --> 00:56:47.800]   but eating carnivore, eating mostly meat
[00:56:47.800 --> 00:56:52.800]   has made me feel really good, both mentally and physically.
[00:56:52.800 --> 00:56:56.240]   Is there something you could say about the kinds of diets
[00:56:56.240 --> 00:56:57.760]   that may improve longevity,
[00:56:57.760 --> 00:57:01.160]   but also enable calorie restriction?
[00:57:01.160 --> 00:57:02.000]   - Well, sure.
[00:57:02.000 --> 00:57:04.560]   I mean, the first thing that's important to know
[00:57:04.560 --> 00:57:08.720]   is that while many people are interested/obsessed
[00:57:08.720 --> 00:57:11.220]   with what they eat,
[00:57:11.220 --> 00:57:13.600]   the data that's come out of animal studies at least
[00:57:13.600 --> 00:57:17.320]   is it's far more important when you eat than what you eat.
[00:57:17.320 --> 00:57:20.120]   And this was a fantastic study a few years ago
[00:57:20.120 --> 00:57:22.000]   by my friend, Rafael de Cabo
[00:57:22.000 --> 00:57:24.720]   at the National Institutes of Health in Bethesda.
[00:57:24.720 --> 00:57:26.920]   And he had 10,000 mice on different diets,
[00:57:26.920 --> 00:57:30.960]   hoping to find the perfect mix of carbs, protein, and fat.
[00:57:30.960 --> 00:57:33.240]   And it turns out that the only ones that lived longer
[00:57:33.240 --> 00:57:35.840]   were the ones that only ate once a day.
[00:57:35.840 --> 00:57:37.920]   And so that, we're not mice,
[00:57:37.920 --> 00:57:40.280]   but I think that we're close enough to mice
[00:57:40.280 --> 00:57:42.600]   that this tells us a lot.
[00:57:42.600 --> 00:57:44.880]   But okay, but I still think the best bang
[00:57:44.880 --> 00:57:47.840]   for the longevity buck is to do both well,
[00:57:47.840 --> 00:57:51.600]   eat less often and eat the right things.
[00:57:51.600 --> 00:57:54.360]   Now I'll preface this to say, I'm not a nut about this.
[00:57:54.360 --> 00:57:57.940]   I will eat very occasionally a dessert.
[00:57:57.940 --> 00:58:00.880]   Usually I steal from others, which doesn't count, right?
[00:58:00.880 --> 00:58:01.720]   - Exactly.
[00:58:01.720 --> 00:58:02.680]   - But you gotta live life, right?
[00:58:02.680 --> 00:58:05.480]   What's a long life if it's not enjoyable anyway?
[00:58:05.480 --> 00:58:07.060]   But what I also found,
[00:58:07.060 --> 00:58:09.200]   and this is, I'll get to your question in a second,
[00:58:09.200 --> 00:58:12.520]   but my microbiome right now and stomach is at a point
[00:58:12.520 --> 00:58:15.700]   where if I try to overeat on a steak,
[00:58:15.700 --> 00:58:16.880]   which I did a couple of days ago,
[00:58:16.880 --> 00:58:21.880]   I actually had a fried chicken specifically,
[00:58:21.880 --> 00:58:23.880]   for two days I felt terrible.
[00:58:23.880 --> 00:58:26.040]   I couldn't sleep, it wouldn't go down.
[00:58:26.040 --> 00:58:28.280]   So I'm now at a point where even if I want to binge
[00:58:28.280 --> 00:58:31.620]   on meat and fried foods, I just can't, it just feels bad.
[00:58:31.620 --> 00:58:34.280]   But what do I recommend?
[00:58:34.280 --> 00:58:37.040]   Well, what the data says, which I try to follow
[00:58:37.040 --> 00:58:40.360]   is that plant-based foods will be better
[00:58:40.360 --> 00:58:41.280]   than meat-based foods.
[00:58:41.280 --> 00:58:43.900]   And I know that there are a lot of people who disagree,
[00:58:43.900 --> 00:58:46.220]   but one of the facts is, well, there's a few facts.
[00:58:46.220 --> 00:58:47.620]   One is that people who live a long time
[00:58:47.620 --> 00:58:48.800]   tend to eat those type of diets,
[00:58:48.800 --> 00:58:51.040]   Mediterranean, Okinawa diet.
[00:58:51.040 --> 00:58:53.960]   They're eating mostly plants with a little bit of meat
[00:58:53.960 --> 00:58:55.920]   and not a lot of red meat.
[00:58:55.920 --> 00:58:57.760]   And the other fact is that in animals,
[00:58:57.760 --> 00:59:01.320]   we know that there's a mechanism that's called mTOR,
[00:59:01.320 --> 00:59:05.400]   little m, capital T-O-R, that responds to certain amino acids
[00:59:05.400 --> 00:59:07.600]   that are found in more abundance in meat.
[00:59:07.600 --> 00:59:10.240]   And when it responds, it actually shortens lifespan.
[00:59:10.240 --> 00:59:13.600]   And the converse, if you starve it of those three amino acids
[00:59:13.600 --> 00:59:17.520]   mostly in meat, then it extends lifespan.
[00:59:17.520 --> 00:59:19.480]   And there's a drug called rapamycin,
[00:59:19.480 --> 00:59:22.720]   which some people are experimenting with, that does that.
[00:59:22.720 --> 00:59:24.080]   So you might be able to,
[00:59:24.080 --> 00:59:26.600]   I'm just saying this here from all my colleagues,
[00:59:26.600 --> 00:59:27.640]   we don't know the results here,
[00:59:27.640 --> 00:59:30.640]   but you could potentially take a rapamycin-like drug
[00:59:30.640 --> 00:59:34.280]   and counteract the effects of meat in the long run.
[00:59:34.280 --> 00:59:35.800]   Don't know, we should try that, actually.
[00:59:35.800 --> 00:59:37.320]   We could do that in the lab.
[00:59:37.320 --> 00:59:39.360]   But getting to the bottom of this,
[00:59:39.360 --> 00:59:41.240]   what I think is going on is that
[00:59:41.240 --> 00:59:42.920]   just like testosterone and growth hormone,
[00:59:42.920 --> 00:59:47.080]   you will get temporary, maybe not temporary,
[00:59:47.080 --> 00:59:48.580]   immediate health benefits.
[00:59:48.580 --> 00:59:51.920]   You'll feel great, you'll get more muscle energy.
[00:59:51.920 --> 00:59:54.320]   But the problem is, I think it's at the expense
[00:59:54.320 --> 00:59:56.140]   of long-term health and longevity.
[00:59:56.140 --> 01:00:00.480]   - Well, this is actually something I worry about
[01:00:00.480 --> 01:00:03.180]   in terms of long-term effects
[01:00:03.180 --> 01:00:05.880]   or the cost in terms of longevity.
[01:00:05.880 --> 01:00:07.280]   It's very difficult to know
[01:00:07.280 --> 01:00:09.920]   how your choices affect your longevity
[01:00:09.920 --> 01:00:12.920]   because the impact is down the line.
[01:00:12.920 --> 01:00:17.760]   Just because something makes me feel good now,
[01:00:17.760 --> 01:00:20.420]   like eating only meat makes me feel good now,
[01:00:20.420 --> 01:00:22.420]   I wonder what are the costs down the line.
[01:00:22.420 --> 01:00:23.860]   - Well, think about what I was saying
[01:00:23.860 --> 01:00:27.240]   about the trade-offs between growth and reproduction,
[01:00:27.240 --> 01:00:28.640]   which is what a mouse does,
[01:00:28.640 --> 01:00:31.660]   and a whale that grows slowly, reproduces slowly,
[01:00:31.660 --> 01:00:33.060]   lives a long time.
[01:00:33.060 --> 01:00:35.020]   It's called the disposable soma theory.
[01:00:35.020 --> 01:00:38.820]   Kirkwood just proposed that in the '70s.
[01:00:38.820 --> 01:00:42.140]   What meat probably does is put you in the mouse category,
[01:00:42.140 --> 01:00:44.780]   super fertile, grow fast, heal fast.
[01:00:44.780 --> 01:00:46.660]   And then if you want to be a whale,
[01:00:46.660 --> 01:00:49.680]   you should restrict meat and do things
[01:00:49.680 --> 01:00:53.380]   that promote the preservation of your body.
[01:00:53.380 --> 01:00:57.860]   - Is it difficult to eat a plant-based diet
[01:00:57.860 --> 01:01:01.940]   that you perform well under, so mentally and physically?
[01:01:01.940 --> 01:01:06.940]   Just almost, I'm asking almost like an anecdotal question,
[01:01:06.940 --> 01:01:08.400]   unless you know the science.
[01:01:08.400 --> 01:01:12.180]   - Well, the science is still being worked out,
[01:01:12.180 --> 01:01:15.220]   but from the synthesis of everything that I've read,
[01:01:15.220 --> 01:01:20.160]   I try to eat a diet that's definitely full of leafy greens,
[01:01:20.160 --> 01:01:22.620]   particularly spinach is great,
[01:01:22.620 --> 01:01:25.860]   'cause it's got the iron that we need, plenty of vitamins.
[01:01:25.860 --> 01:01:30.860]   I also try to avoid too much fruit and berries,
[01:01:30.860 --> 01:01:34.020]   particularly fruit juice,
[01:01:34.020 --> 01:01:35.900]   definitely avoid that sugar high.
[01:01:35.900 --> 01:01:39.900]   Spiking your sugar is not healthy in the long run.
[01:01:39.900 --> 01:01:42.100]   The other thing that's interesting is we discovered
[01:01:42.660 --> 01:01:45.500]   what we called xenohormetic molecules.
[01:01:45.500 --> 01:01:47.540]   Let me unpack that, 'cause it's a terrible name,
[01:01:47.540 --> 01:01:49.460]   and I take full responsibility
[01:01:49.460 --> 01:01:51.740]   with my friend Conrad Howitz.
[01:01:51.740 --> 01:01:53.900]   The xeno means cross species,
[01:01:53.900 --> 01:01:57.400]   and hormesis is the term that what doesn't kill you
[01:01:57.400 --> 01:02:01.180]   makes you live longer and be healthier.
[01:02:01.180 --> 01:02:04.660]   And so we're getting cross species health improvements
[01:02:04.660 --> 01:02:06.980]   by molecules that plants make.
[01:02:06.980 --> 01:02:08.260]   And plants make these molecules
[01:02:08.260 --> 01:02:11.820]   when they're also under adversity or perceived adversity.
[01:02:11.820 --> 01:02:14.180]   For instance, I understand
[01:02:14.180 --> 01:02:16.740]   if you want really healthy or good oranges,
[01:02:16.740 --> 01:02:19.360]   you can drive nails into the bark of the tree
[01:02:19.360 --> 01:02:20.400]   before you harvest.
[01:02:20.400 --> 01:02:23.260]   Same with wine, you typically want them to be dry
[01:02:23.260 --> 01:02:26.020]   before you harvest or covered in fungus.
[01:02:26.020 --> 01:02:28.860]   And that's because these plants make these colorful
[01:02:28.860 --> 01:02:30.720]   and xenohormetic molecules
[01:02:30.720 --> 01:02:33.420]   that make themselves stress resistant,
[01:02:33.420 --> 01:02:37.740]   turn on their sirtuin defenses, the sirt genes, remember.
[01:02:37.740 --> 01:02:40.340]   And when we eat them, we get those same benefits.
[01:02:40.340 --> 01:02:42.260]   That's the idea, and we've evolved to do so.
[01:02:42.260 --> 01:02:43.740]   This isn't a coincidence.
[01:02:43.740 --> 01:02:45.040]   It's my theory, our theory,
[01:02:45.040 --> 01:02:48.360]   that we want to know when our food supply
[01:02:48.360 --> 01:02:52.020]   is under adversity because we need to get ready for a famine.
[01:02:52.020 --> 01:02:54.580]   And so we hunker down and preserve our body.
[01:02:54.580 --> 01:02:55.860]   And by eating these colored foods,
[01:02:55.860 --> 01:02:58.780]   so practically speaking, if it's full of color,
[01:02:58.780 --> 01:03:01.940]   or if there's been some chewing by a caterpillar,
[01:03:01.940 --> 01:03:05.000]   organic, grown locally in local farms,
[01:03:05.000 --> 01:03:09.640]   I'll eat that versus a watery, insipid, light-colored,
[01:03:10.520 --> 01:03:12.800]   lettuce that's been grown in California.
[01:03:12.800 --> 01:03:14.560]   - So you want vegetables that have suffered.
[01:03:14.560 --> 01:03:16.720]   You want the David Goggins' of vegetables.
[01:03:16.720 --> 01:03:19.280]   That's the xenohormetic molecules.
[01:03:19.280 --> 01:03:20.800]   - I love that term.
[01:03:20.800 --> 01:03:23.320]   I'm gonna take that one with me, thank you.
[01:03:23.320 --> 01:03:24.160]   - Yeah.
[01:03:24.160 --> 01:03:27.360]   Oh, I follow him on Instagram, he's always screaming.
[01:03:27.360 --> 01:03:29.920]   So you want the, he's basically
[01:03:29.920 --> 01:03:34.880]   the xenohormetic version of a human.
[01:03:34.880 --> 01:03:36.920]   I like it.
[01:03:36.920 --> 01:03:39.000]   So these are the molecules that are representative
[01:03:39.000 --> 01:03:44.000]   of the stress that a plant has been under.
[01:03:44.000 --> 01:03:46.720]   - Yeah, the best example of that is resveratrol,
[01:03:46.720 --> 01:03:48.200]   which many people, including myself,
[01:03:48.200 --> 01:03:49.720]   take as a supplement.
[01:03:49.720 --> 01:03:52.720]   Grapes, grapevines produce that in abundance
[01:03:52.720 --> 01:03:55.840]   when they're dried out or they have too much light
[01:03:55.840 --> 01:03:56.880]   or fungus.
[01:03:56.880 --> 01:04:00.920]   And that, we've shown, activates the SIR2 enzyme
[01:04:00.920 --> 01:04:02.280]   in our bodies, which, remember,
[01:04:02.280 --> 01:04:03.840]   is what extends lifespan in yeast
[01:04:03.840 --> 01:04:05.840]   and slows down aging in the brain.
[01:04:05.840 --> 01:04:06.980]   - That's beautiful.
[01:04:06.980 --> 01:04:09.300]   Yeah, I tend to avoid fruit as well.
[01:04:09.300 --> 01:04:12.720]   So green veggies, anything that's not very sweet.
[01:04:12.720 --> 01:04:15.460]   So I would just say you're relatively low,
[01:04:15.460 --> 01:04:19.180]   like you try to avoid sugary things as well.
[01:04:19.180 --> 01:04:21.540]   - Yeah, I'm fairly militant about that.
[01:04:21.540 --> 01:04:23.940]   I rarely would add sugar to anything.
[01:04:23.940 --> 01:04:27.860]   Occasionally I would eat a slice of cheesecake,
[01:04:27.860 --> 01:04:31.100]   but that would be maybe once or twice a year.
[01:04:31.100 --> 01:04:32.940]   You have to give in occasionally.
[01:04:32.940 --> 01:04:34.820]   But yeah, anything that's sweet,
[01:04:34.820 --> 01:04:37.500]   I would rather substitute something like Stevia
[01:04:37.500 --> 01:04:38.660]   if I need a sugar hit.
[01:04:38.660 --> 01:04:43.620]   - What about exercise, your favorite topic?
[01:04:43.620 --> 01:04:45.300]   (both laugh)
[01:04:45.300 --> 01:04:46.140]   Is there a--
[01:04:46.140 --> 01:04:47.580]   - I don't like talking about it.
[01:04:47.580 --> 01:04:48.420]   (both laugh)
[01:04:48.420 --> 01:04:49.260]   - Yeah, okay, great.
[01:04:49.260 --> 01:04:53.500]   Is there benefits to longevity from exercise?
[01:04:53.500 --> 01:04:54.340]   - Well, no doubt.
[01:04:54.340 --> 01:04:55.980]   That's proven.
[01:04:55.980 --> 01:04:59.260]   Just like fasting, it's pretty clear that that works.
[01:04:59.260 --> 01:05:01.900]   For example, there are studies of cyclists.
[01:05:01.900 --> 01:05:04.220]   It was something like people that cycle
[01:05:04.220 --> 01:05:07.700]   over 80 miles a week have a 40% reduction
[01:05:07.700 --> 01:05:10.400]   in a variety of diseases, certainly heart disease.
[01:05:10.400 --> 01:05:11.640]   So that's not even a question.
[01:05:11.640 --> 01:05:13.500]   But what's interesting is that we're learning
[01:05:13.500 --> 01:05:15.660]   that you don't need much to have a big benefit.
[01:05:15.660 --> 01:05:17.460]   It's an asymptotic curve.
[01:05:17.460 --> 01:05:19.180]   And in fact, if you overdo it,
[01:05:19.180 --> 01:05:20.680]   you probably have reduced benefits,
[01:05:20.680 --> 01:05:22.180]   particularly if you start to wear out joints,
[01:05:22.180 --> 01:05:23.540]   that kind of thing.
[01:05:23.540 --> 01:05:25.740]   But just 10 minutes on a treadmill a few times a week,
[01:05:25.740 --> 01:05:28.580]   getting your, lose your breath, get hypoxic as it's called,
[01:05:28.580 --> 01:05:32.660]   seems to be very beneficial for long-term health.
[01:05:32.660 --> 01:05:36.460]   And that's the kind of exercise that I like to do, aerobic.
[01:05:36.460 --> 01:05:38.520]   Though I do enjoy lifting weights.
[01:05:38.520 --> 01:05:40.920]   So that is what I call my exercise,
[01:05:40.920 --> 01:05:41.860]   which has other benefits,
[01:05:41.860 --> 01:05:46.240]   including maintaining hormone levels, male hormone levels.
[01:05:46.240 --> 01:05:49.900]   But also really why I do it is I want to be able
[01:05:49.900 --> 01:05:53.500]   to counteract the effect of sitting for most of the day.
[01:05:53.500 --> 01:05:55.780]   And as you get older, you lose muscle mass.
[01:05:55.780 --> 01:05:57.660]   It's a percent or so a year.
[01:05:57.660 --> 01:05:59.460]   And I don't want to be frail when I'm older
[01:05:59.460 --> 01:06:00.860]   and fall over and break my hip,
[01:06:00.860 --> 01:06:04.140]   which happens every 20 seconds in this country.
[01:06:04.140 --> 01:06:05.500]   - So maintaining that strength,
[01:06:05.500 --> 01:06:07.940]   but also doing the cardio for the longevity,
[01:06:07.940 --> 01:06:10.180]   for avoiding the heart disease.
[01:06:10.180 --> 01:06:13.100]   Yeah, I definitely, just like with fasting,
[01:06:13.100 --> 01:06:15.620]   have the philosophical benefit of running long
[01:06:15.620 --> 01:06:17.120]   and running slow.
[01:06:17.120 --> 01:06:19.340]   I enjoy it 'cause it kind of clears the mind
[01:06:19.340 --> 01:06:20.600]   and allows you to think.
[01:06:20.600 --> 01:06:23.060]   I actually listen to brown noise as I run.
[01:06:23.060 --> 01:06:26.380]   It really helps remove myself from the world
[01:06:26.380 --> 01:06:28.740]   and just like zoom in on particular thoughts.
[01:06:28.740 --> 01:06:30.060]   - What is brown noise?
[01:06:30.060 --> 01:06:31.900]   - It's like white noise, but deeper.
[01:06:31.900 --> 01:06:35.100]   So like white noise is like shh,
[01:06:35.100 --> 01:06:37.140]   and then brown noise is more like,
[01:06:37.140 --> 01:06:38.880]   (makes whooshing sound)
[01:06:38.880 --> 01:06:39.940]   like ocean.
[01:06:39.940 --> 01:06:40.860]   - That sounds great.
[01:06:40.860 --> 01:06:41.740]   I might try that.
[01:06:41.740 --> 01:06:43.500]   - Yeah, yeah, it's a--
[01:06:43.500 --> 01:06:44.940]   - It's more soothing probably.
[01:06:44.940 --> 01:06:45.780]   - I'm not sure.
[01:06:45.780 --> 01:06:46.600]   There could be science to this.
[01:06:46.600 --> 01:06:47.640]   I need to look this up.
[01:06:47.640 --> 01:06:48.860]   I've been meaning to.
[01:06:48.860 --> 01:06:53.820]   But when I started, this is maybe like five years ago,
[01:06:53.820 --> 01:06:56.080]   I started listening to brown noise when I work.
[01:06:56.080 --> 01:06:58.400]   And the first time I listened to it,
[01:06:58.400 --> 01:07:00.300]   something happened to my mind
[01:07:00.300 --> 01:07:01.660]   where it just went like,
[01:07:01.660 --> 01:07:02.500]   (makes whooshing sound)
[01:07:02.500 --> 01:07:04.500]   zoomed in to like,
[01:07:04.500 --> 01:07:07.860]   in a way that it felt like really weird,
[01:07:07.860 --> 01:07:12.660]   like how precisely it was able to sort of remove
[01:07:12.660 --> 01:07:16.580]   the distractions of the world and really help my mind.
[01:07:16.580 --> 01:07:19.660]   Obviously, like the mind is trying to focus
[01:07:19.660 --> 01:07:21.820]   and then it just enabled that process
[01:07:21.820 --> 01:07:24.540]   of trying to focus on a particular problem.
[01:07:24.540 --> 01:07:26.340]   I don't know if this is generalizable to others.
[01:07:26.340 --> 01:07:29.040]   People should definitely try it if you're listening to this.
[01:07:29.040 --> 01:07:30.260]   Maybe it's just my own mind.
[01:07:30.260 --> 01:07:32.320]   But it's funny, like,
[01:07:32.320 --> 01:07:35.880]   it made me, brown noise made me realize
[01:07:35.880 --> 01:07:38.720]   that there's probably hacks out there
[01:07:38.720 --> 01:07:41.880]   that work for me that I should be constantly looking for.
[01:07:41.880 --> 01:07:46.380]   It's almost like an encouraging and motivating event
[01:07:46.380 --> 01:07:49.980]   that maybe there's other stuff out there.
[01:07:49.980 --> 01:07:52.840]   Maybe there's other brown noise-like things out there
[01:07:52.840 --> 01:07:56.120]   that truly, like almost immediately make me feel better.
[01:07:56.120 --> 01:07:57.840]   I don't know if it's generalizable to others,
[01:07:57.840 --> 01:08:01.680]   but it does seem that it's the case that
[01:08:01.680 --> 01:08:03.280]   there's probably for many others,
[01:08:03.280 --> 01:08:05.880]   things like that that could be discovered.
[01:08:05.880 --> 01:08:10.360]   And so it's always disappointing when I find things in life
[01:08:10.360 --> 01:08:12.960]   that I wish I would found earlier.
[01:08:12.960 --> 01:08:17.360]   I got LASIK eye surgery a few years ago.
[01:08:17.360 --> 01:08:19.400]   And the first thought I had like the next day
[01:08:19.400 --> 01:08:21.360]   when I woke up is like,
[01:08:21.360 --> 01:08:24.200]   damn it, why didn't I do this way earlier?
[01:08:24.200 --> 01:08:27.320]   There's other stuff of that nature
[01:08:27.320 --> 01:08:29.640]   that are yet to be discovered.
[01:08:29.640 --> 01:08:31.720]   So it pays to explore.
[01:08:31.720 --> 01:08:33.000]   - Yeah, though you have a different mind.
[01:08:33.000 --> 01:08:34.520]   You have quite a beautiful mind.
[01:08:34.520 --> 01:08:37.200]   So I suspect brown noise helps you focus
[01:08:37.200 --> 01:08:39.140]   and 'cause you're probably all over the place
[01:08:39.140 --> 01:08:40.240]   if you don't control it.
[01:08:40.240 --> 01:08:41.080]   - Yeah, exactly.
[01:08:41.080 --> 01:08:42.360]   It means something about it.
[01:08:42.360 --> 01:08:43.560]   It's a programmer thing.
[01:08:43.560 --> 01:08:50.000]   A programming is a really difficult mental journey
[01:08:50.000 --> 01:08:52.920]   'cause you have to keep a lot of things in mind.
[01:08:52.920 --> 01:08:56.520]   You have to, so you're constantly designing things.
[01:08:56.520 --> 01:08:58.120]   Then you have to be extremely precise
[01:08:58.120 --> 01:09:01.120]   by making those things concrete in code.
[01:09:01.120 --> 01:09:05.160]   You also have to look stuff up on the internet
[01:09:05.160 --> 01:09:08.680]   to sort of feed like information
[01:09:08.680 --> 01:09:10.440]   and looking up stuff on the internet.
[01:09:10.440 --> 01:09:12.400]   Internet is full of like distracting things.
[01:09:12.400 --> 01:09:13.800]   So you have to be really focused
[01:09:13.800 --> 01:09:15.560]   in the way you look stuff up
[01:09:15.560 --> 01:09:16.800]   in pulling that information in.
[01:09:16.800 --> 01:09:19.120]   So it requires a certain discipline
[01:09:19.120 --> 01:09:23.440]   and a certain focus that I've been very much exploring
[01:09:23.440 --> 01:09:24.280]   how to do.
[01:09:24.280 --> 01:09:26.360]   Like I do it really well in the morning,
[01:09:26.360 --> 01:09:28.320]   coffee's involved, all those kinds of things.
[01:09:28.320 --> 01:09:29.680]   You're trying to optimize,
[01:09:29.680 --> 01:09:33.560]   keeping very positive inspired, no social media,
[01:09:33.560 --> 01:09:36.160]   all those kinds of things and trying to optimize for.
[01:09:36.160 --> 01:09:38.680]   And everybody has their own kind of little journey
[01:09:38.680 --> 01:09:40.560]   that they try to understand.
[01:09:40.560 --> 01:09:41.960]   You get this from like writers.
[01:09:41.960 --> 01:09:45.840]   When you read about the habits of writers,
[01:09:45.840 --> 01:09:47.880]   like the habits they do in the morning,
[01:09:47.880 --> 01:09:49.840]   they usually write like two, three, four hours a day
[01:09:49.840 --> 01:09:51.000]   and that's it.
[01:09:51.000 --> 01:09:53.440]   It's like they optimize that ritual.
[01:09:53.440 --> 01:09:55.480]   And then there's always Hunter Stobson.
[01:09:55.480 --> 01:10:00.480]   So sometimes it pays off to be wild.
[01:10:00.480 --> 01:10:04.600]   What about sleep?
[01:10:04.600 --> 01:10:06.760]   How important is sleep for longevity?
[01:10:06.760 --> 01:10:10.760]   - I would guess based on the evidence
[01:10:10.760 --> 01:10:12.200]   that it's really important.
[01:10:12.200 --> 01:10:15.080]   And because we don't know for sure.
[01:10:15.080 --> 01:10:17.600]   But what we know from animal studies is the following.
[01:10:17.600 --> 01:10:20.280]   If you restrict sleep from a rat for just two weeks,
[01:10:20.280 --> 01:10:22.200]   it'll develop type two diabetes.
[01:10:22.200 --> 01:10:23.420]   It's that important.
[01:10:23.420 --> 01:10:25.840]   So that's the main thing.
[01:10:25.840 --> 01:10:28.480]   What we also know is at the molecular level
[01:10:28.480 --> 01:10:33.360]   that if you disrupt your sleep-wake cycle,
[01:10:33.360 --> 01:10:35.120]   so we actually have proteins that go up and down
[01:10:35.120 --> 01:10:36.720]   that control our sleep-wake.
[01:10:36.720 --> 01:10:39.240]   All of us, most of our cells do that.
[01:10:39.240 --> 01:10:42.480]   If you disrupt that, you'll get premature aging.
[01:10:42.480 --> 01:10:43.320]   And guess what?
[01:10:43.320 --> 01:10:44.140]   The opposite is true.
[01:10:44.140 --> 01:10:46.760]   That as you get older, that cycle,
[01:10:46.760 --> 01:10:49.480]   the amplitude becomes diminished.
[01:10:49.480 --> 01:10:51.880]   And this is why it's harder to get to sleep as you get older
[01:10:51.880 --> 01:10:54.480]   and then you got all sorts of problems.
[01:10:54.480 --> 01:10:56.920]   And I think what's going on is there's positive feedback
[01:10:56.920 --> 01:10:59.420]   which is a disaster in your old age,
[01:10:59.420 --> 01:11:02.040]   which is you're aging.
[01:11:02.040 --> 01:11:05.760]   You can't at this moment totally prevent that.
[01:11:05.760 --> 01:11:06.880]   And then it's disrupting your sleep
[01:11:06.880 --> 01:11:08.020]   and you get not enough sleep.
[01:11:08.020 --> 01:11:10.440]   And then that's gonna accelerate your aging process.
[01:11:10.440 --> 01:11:13.720]   And so it's known that people who are shift workers
[01:11:13.720 --> 01:11:17.560]   are more susceptible to certain age-related diseases.
[01:11:17.560 --> 01:11:19.960]   So bottom line, you definitely wanna work on that.
[01:11:19.960 --> 01:11:21.840]   It's one of the reasons I have this ring on my finger
[01:11:21.840 --> 01:11:23.320]   which helps me optimize my sleep
[01:11:23.320 --> 01:11:27.420]   and learn what I do the day before if it was a bad idea.
[01:11:27.420 --> 01:11:29.920]   And I'll stop doing that like eating a fried chicken.
[01:11:29.920 --> 01:11:31.760]   (laughing)
[01:11:31.760 --> 01:11:34.860]   - I see you're still carrying the burdens of that decision.
[01:11:34.860 --> 01:11:37.920]   But is, yeah, you know, sleep is one of those things
[01:11:37.920 --> 01:11:41.600]   that's making me wonder about the variability
[01:11:41.600 --> 01:11:44.360]   between humans a little bit and how science
[01:11:44.360 --> 01:11:46.080]   is often focused on,
[01:11:46.080 --> 01:11:51.500]   like it's not often focused on high performers
[01:11:51.500 --> 01:11:52.600]   in a particular way.
[01:11:52.600 --> 01:11:55.200]   And it's looking at the aggregate
[01:11:55.200 --> 01:11:56.940]   versus the individual cases.
[01:11:56.940 --> 01:11:59.360]   For example, like for me,
[01:11:59.360 --> 01:12:00.940]   I don't know what the exact hours are,
[01:12:00.940 --> 01:12:04.280]   but like power naps are incredible.
[01:12:04.280 --> 01:12:10.680]   I tend to look at the metric of stress and happiness
[01:12:10.680 --> 01:12:13.080]   and joy and try to optimize those.
[01:12:13.080 --> 01:12:16.020]   So decreasing stress, increasing happiness,
[01:12:16.020 --> 01:12:20.160]   and using sleep as just one of the tools to do that.
[01:12:20.160 --> 01:12:23.720]   Because like hitting the five, six, seven, eight,
[01:12:23.720 --> 01:12:27.200]   nine hour mark or whatever the correct mark is,
[01:12:27.200 --> 01:12:30.000]   I find that to be stress inducing for me
[01:12:30.000 --> 01:12:32.320]   versus stress relieving.
[01:12:32.320 --> 01:12:34.200]   Like thinking about that,
[01:12:34.200 --> 01:12:37.360]   I feel best if I sleep sometimes for eight hours,
[01:12:37.360 --> 01:12:39.600]   sometimes for four hours and then power nap.
[01:12:39.600 --> 01:12:42.200]   And as long as I have a stupid,
[01:12:42.200 --> 01:12:44.480]   private usually smile on my face,
[01:12:44.480 --> 01:12:46.200]   that's when I'm doing good
[01:12:46.200 --> 01:12:49.480]   as opposed to getting a perfect amount of sleep
[01:12:49.480 --> 01:12:53.320]   according to whatever the latest blog post is.
[01:12:53.320 --> 01:12:55.400]   And I also pull all nighters still.
[01:12:55.400 --> 01:12:59.360]   I also think there's something about the body,
[01:12:59.360 --> 01:13:02.900]   like as long as you do it regularly,
[01:13:02.900 --> 01:13:04.440]   it's not as stress inducing.
[01:13:04.440 --> 01:13:06.840]   Like you know what it is.
[01:13:06.840 --> 01:13:09.000]   The reason I pull all nighters isn't for like,
[01:13:09.000 --> 01:13:11.600]   I'm playing Diablo three or something,
[01:13:11.600 --> 01:13:14.000]   is because I'm doing something I'm truly passionate about.
[01:13:14.000 --> 01:13:15.860]   Well, I'm also a video games,
[01:13:15.860 --> 01:13:18.480]   but I'm doing something I'm truly passionate about.
[01:13:18.480 --> 01:13:21.440]   And it's almost like there's the Jocko Willink feeling
[01:13:21.440 --> 01:13:25.560]   of when I'm up at 7 a.m. and I haven't slept all night
[01:13:25.560 --> 01:13:27.200]   and still I'm working on it.
[01:13:27.200 --> 01:13:29.520]   There's a kind of a celebration of the human spirit
[01:13:29.520 --> 01:13:30.880]   that I really enjoy it.
[01:13:30.880 --> 01:13:33.760]   And that's happiness.
[01:13:33.760 --> 01:13:36.000]   And to sort of then,
[01:13:36.000 --> 01:13:38.000]   and I usually don't tell that kind of stuff to people
[01:13:38.000 --> 01:13:40.880]   because their first statement will be like,
[01:13:40.880 --> 01:13:42.620]   you should get more sleep.
[01:13:42.620 --> 01:13:46.240]   It's like, no, I'm doing stuff I love.
[01:13:46.240 --> 01:13:49.040]   You should get more love in your life, bro.
[01:13:49.040 --> 01:13:50.440]   - That's right.
[01:13:50.440 --> 01:13:52.560]   - So, but that said in aggregate,
[01:13:52.560 --> 01:13:55.400]   when you look at the full span of life,
[01:13:55.400 --> 01:13:58.480]   is probably you should be getting
[01:13:58.480 --> 01:14:00.760]   a consistent amount of sleep.
[01:14:00.760 --> 01:14:04.960]   And it seems like it's in that seven, eight hour range.
[01:14:04.960 --> 01:14:06.760]   - Yeah, but it's similar to food.
[01:14:06.760 --> 01:14:09.360]   It's the quality, not the quantity.
[01:14:09.360 --> 01:14:10.760]   And when you get it.
[01:14:10.760 --> 01:14:14.520]   So I look at my data pretty often.
[01:14:14.520 --> 01:14:16.040]   And what makes a difference to me
[01:14:16.040 --> 01:14:19.200]   is not the amount of hours, but the quality, the depth
[01:14:19.200 --> 01:14:21.960]   and the deep sleep is what'll do it.
[01:14:21.960 --> 01:14:24.880]   So if I have a lot of alcohol before going to sleep
[01:14:24.880 --> 01:14:26.400]   and I can see my heart rate being different,
[01:14:26.400 --> 01:14:28.440]   but what really kills me is that I don't get a lot
[01:14:28.440 --> 01:14:32.480]   of that deep sleep and I wake up barely remembering stuff.
[01:14:32.480 --> 01:14:34.440]   So that, like you say, if you're happy and contented
[01:14:34.440 --> 01:14:37.160]   and you don't have these cortisol chemicals
[01:14:37.160 --> 01:14:38.420]   going through your body,
[01:14:38.420 --> 01:14:40.720]   you will more naturally get into that deep state.
[01:14:40.720 --> 01:14:42.320]   And even if you just get four hours,
[01:14:42.320 --> 01:14:45.420]   way better than eight hours of none of that.
[01:14:45.420 --> 01:14:46.800]   - Yeah, yeah, that's beautiful.
[01:14:46.800 --> 01:14:48.240]   And some of that could be genetic.
[01:14:48.240 --> 01:14:52.040]   For me, I just fall asleep like this.
[01:14:52.040 --> 01:14:54.160]   If you want me to fall asleep right now, I can do it.
[01:14:54.160 --> 01:14:58.180]   It's no, I have no problem with it combined with coffee.
[01:14:58.180 --> 01:15:01.480]   I just had two energy drinks, I can probably sleep.
[01:15:01.480 --> 01:15:03.560]   So that, I don't know if that's genetics
[01:15:03.560 --> 01:15:06.680]   or it's kind of, I don't know what it is.
[01:15:06.680 --> 01:15:09.720]   Or maybe that I don't have kids and I'm single.
[01:15:09.720 --> 01:15:11.920]   So I don't have, I'm almost listening
[01:15:11.920 --> 01:15:15.400]   to some kind of biological signal versus societal signal
[01:15:15.400 --> 01:15:17.080]   on when I'm supposed to go to sleep.
[01:15:17.080 --> 01:15:20.720]   So I just go to sleep whenever I feel like going to sleep.
[01:15:20.720 --> 01:15:22.480]   - Well, that's 'cause you're self-employed.
[01:15:22.480 --> 01:15:23.360]   - Self-employed. - Most people
[01:15:23.360 --> 01:15:26.120]   don't have that luxury, but we're lucky, the two of us,
[01:15:26.120 --> 01:15:28.280]   that we can make our own hours.
[01:15:28.280 --> 01:15:29.800]   But yeah, it's super important.
[01:15:29.800 --> 01:15:32.360]   And those people who have the shift work,
[01:15:32.360 --> 01:15:35.480]   I mean, they really need to change the way that works
[01:15:35.480 --> 01:15:37.840]   because they're literally killing those people.
[01:15:37.840 --> 01:15:43.640]   - Is there something you could say about the mind
[01:15:43.640 --> 01:15:48.200]   and stress in terms of effect on longevity?
[01:15:48.200 --> 01:15:52.520]   'Cause I don't know if you think about it this way,
[01:15:52.520 --> 01:15:55.100]   but when you talk about the biological machine,
[01:15:55.100 --> 01:15:58.820]   it's always these mechanisms that are not necessarily
[01:15:58.820 --> 01:16:00.920]   directly connected to the brain
[01:16:00.920 --> 01:16:02.640]   or the operation of the brain.
[01:16:02.640 --> 01:16:06.360]   Like what's the role about stress and happiness
[01:16:06.360 --> 01:16:10.720]   and yeah, the sort of higher cognitive things
[01:16:10.720 --> 01:16:13.060]   going on in the brain on longevity?
[01:16:13.060 --> 01:16:16.240]   - Right, well, that's a great point.
[01:16:16.240 --> 01:16:19.320]   The brain is the center for longevity, actually.
[01:16:19.320 --> 01:16:20.300]   We do know that.
[01:16:20.300 --> 01:16:24.660]   First off, when I'm stressed, I can see, mentally stressed,
[01:16:24.660 --> 01:16:27.120]   then I can see it in my body.
[01:16:27.120 --> 01:16:29.560]   Heart rate, hormones, it's clear.
[01:16:29.560 --> 01:16:31.460]   That's no true surprise.
[01:16:31.460 --> 01:16:34.000]   So you've got to work on your brain first and foremost.
[01:16:34.000 --> 01:16:39.000]   If you are totally freaked out, agitated all the time,
[01:16:39.000 --> 01:16:41.360]   you will live shorter.
[01:16:41.360 --> 01:16:42.520]   I'm certain of it.
[01:16:42.520 --> 01:16:43.480]   I keep fish.
[01:16:43.480 --> 01:16:47.520]   I'm a big aquarium guy.
[01:16:47.520 --> 01:16:49.500]   And you can see the difference between the fish
[01:16:49.500 --> 01:16:51.480]   that's having a good time and dominant
[01:16:51.480 --> 01:16:53.340]   and the one that gets picked on.
[01:16:53.340 --> 01:16:55.200]   It just looks like crap.
[01:16:55.200 --> 01:16:57.580]   You don't want to be that, the little fish getting picked on
[01:16:57.580 --> 01:16:58.680]   if you can help it.
[01:16:58.680 --> 01:17:00.980]   So I used to be extremely stressed as a kid.
[01:17:00.980 --> 01:17:03.380]   I was a perfectionist, very shy,
[01:17:03.380 --> 01:17:06.120]   always worried about being a failure.
[01:17:06.120 --> 01:17:07.440]   If I didn't get an A plus, you know,
[01:17:07.440 --> 01:17:11.000]   I was crying in my bedroom, that kind of sad existence.
[01:17:11.000 --> 01:17:14.820]   I got into my 20s, then in my 30s and realized
[01:17:14.820 --> 01:17:15.800]   that's not the way to live.
[01:17:15.800 --> 01:17:18.000]   So I've worked very hard to get to this point
[01:17:18.000 --> 01:17:21.240]   where I almost never get stressed, never.
[01:17:21.240 --> 01:17:23.640]   There's nothing that, I've never gotten angry in my lab.
[01:17:23.640 --> 01:17:24.640]   I've got 20 kids.
[01:17:24.640 --> 01:17:26.780]   Sometimes it's like a, most of the time,
[01:17:26.780 --> 01:17:28.840]   it's like a kindergarten.
[01:17:28.840 --> 01:17:31.440]   I haven't lost my temper, I'm very calm.
[01:17:31.440 --> 01:17:32.920]   But that's intentional.
[01:17:32.920 --> 01:17:34.960]   And I don't worry about stuff.
[01:17:34.960 --> 01:17:37.960]   Millions of dollars, billions of dollars at stake sometimes.
[01:17:37.960 --> 01:17:39.920]   Keep it cool.
[01:17:39.920 --> 01:17:40.740]   It's only life.
[01:17:40.740 --> 01:17:42.760]   We're all headed to the same place anyway.
[01:17:42.760 --> 01:17:44.280]   Don't worry about it.
[01:17:44.280 --> 01:17:47.760]   But to answer your question, I think in a better way,
[01:17:47.760 --> 01:17:49.760]   if you manipulate the brain of an animal,
[01:17:49.760 --> 01:17:51.560]   I'll give you an example.
[01:17:51.560 --> 01:17:54.920]   If we turn on this SIRT gene that I mentioned, SIRT1,
[01:17:54.920 --> 01:17:58.100]   we, a good friend of mine at Wash U, she and I did this.
[01:17:58.100 --> 01:18:01.800]   They upregulated that gene just in the neurons
[01:18:01.800 --> 01:18:03.220]   of the animal.
[01:18:03.220 --> 01:18:04.800]   It lived longer.
[01:18:04.800 --> 01:18:06.960]   So that's sufficient to extend lifespan.
[01:18:06.960 --> 01:18:10.040]   We also know that you can manipulate the part of the brain
[01:18:10.040 --> 01:18:12.700]   called the hypothalamus, which leeches a lot of chemicals
[01:18:12.700 --> 01:18:17.120]   into the body and proteins, most of which we don't know yet,
[01:18:17.120 --> 01:18:19.880]   but just changing the inflammation of that little organ
[01:18:21.040 --> 01:18:22.920]   or part of the brain is sufficient
[01:18:22.920 --> 01:18:24.960]   to make animals live longer as well.
[01:18:24.960 --> 01:18:26.520]   So get your brain in order first
[01:18:26.520 --> 01:18:29.480]   before you tackle anything else, I would say.
[01:18:29.480 --> 01:18:31.700]   - So you kind of mentioned this.
[01:18:31.700 --> 01:18:34.440]   With the Insight Tracker, there's ability
[01:18:34.440 --> 01:18:38.400]   to take blood measurement and then infer from that
[01:18:38.400 --> 01:18:41.000]   a bunch of different things about your body
[01:18:41.000 --> 01:18:44.720]   and how you can improve the longevity.
[01:18:44.720 --> 01:18:49.720]   And you've also mentioned saliva and more efficient ways
[01:18:50.000 --> 01:18:52.520]   to get data.
[01:18:52.520 --> 01:18:55.440]   What does that involve?
[01:18:55.440 --> 01:18:57.440]   What's the future of data collection look like
[01:18:57.440 --> 01:18:59.240]   for the human biological system?
[01:18:59.240 --> 01:19:01.640]   - Right, well, yeah, the issue with blood
[01:19:01.640 --> 01:19:03.400]   is you need someone to take it.
[01:19:03.400 --> 01:19:06.080]   I mean, or you prick your finger, which hurts.
[01:19:06.080 --> 01:19:07.160]   So you've got to have something better.
[01:19:07.160 --> 01:19:09.040]   So I think what the future looks like
[01:19:09.040 --> 01:19:12.400]   is that you'll spit onto a little piece of paper
[01:19:12.400 --> 01:19:14.760]   and stick it in a machine, it'll do that for you.
[01:19:14.760 --> 01:19:15.640]   But we're not there yet.
[01:19:15.640 --> 01:19:20.640]   So the intermediate future that I'm building right now
[01:19:20.640 --> 01:19:24.220]   is that you would take a swab of the inside of your mouth,
[01:19:24.220 --> 01:19:26.800]   which is the easiest way to take cells out of your body,
[01:19:26.800 --> 01:19:28.360]   and just ship them off.
[01:19:28.360 --> 01:19:30.800]   Okay, so it's called a buckle swab.
[01:19:30.800 --> 01:19:32.920]   I think we became very used to that.
[01:19:32.920 --> 01:19:34.360]   Right now, because of COVID,
[01:19:34.360 --> 01:19:36.120]   people don't like going to the doctor as much.
[01:19:36.120 --> 01:19:36.960]   They don't like going out.
[01:19:36.960 --> 01:19:38.840]   They just want to have home tests.
[01:19:38.840 --> 01:19:40.800]   And so that I think is the next 10 years
[01:19:40.800 --> 01:19:43.160]   where you'll get a kit in the mail,
[01:19:43.160 --> 01:19:45.280]   you'll swab your cheeks, stick it back in an envelope,
[01:19:45.280 --> 01:19:47.440]   send it off, and a week later,
[01:19:47.440 --> 01:19:50.240]   you have either a doctor's report
[01:19:50.240 --> 01:19:52.880]   or a health recommendation.
[01:19:52.880 --> 01:19:55.000]   And what can you get off a cheek swab?
[01:19:55.000 --> 01:19:55.840]   Well, you can get anything.
[01:19:55.840 --> 01:20:00.840]   You can get hormones, stress hormones, blood glucose levels.
[01:20:00.840 --> 01:20:04.400]   You can also tell your age reasonably accurately doing that,
[01:20:04.400 --> 01:20:05.920]   actually quite accurately.
[01:20:05.920 --> 01:20:08.240]   And those clocks cannot just tell you
[01:20:08.240 --> 01:20:10.200]   how you're doing over time,
[01:20:10.200 --> 01:20:12.200]   but can be used to give you recommendations
[01:20:12.200 --> 01:20:14.200]   to slow that process down.
[01:20:14.200 --> 01:20:16.640]   'Cause some people sometimes are 10 years older biologically
[01:20:16.640 --> 01:20:19.600]   than their actual chronological age.
[01:20:19.600 --> 01:20:21.940]   I mean, why does it matter how many times
[01:20:21.940 --> 01:20:23.000]   the Earth's gone around the sun?
[01:20:23.000 --> 01:20:24.920]   Seriously, who cares about birthdays?
[01:20:24.920 --> 01:20:27.280]   It's how long your body's clock has been ticking,
[01:20:27.280 --> 01:20:28.560]   and how fast.
[01:20:28.560 --> 01:20:31.040]   So I could take a cheek swab from you today, Lex,
[01:20:31.040 --> 01:20:32.720]   take it back to my lab,
[01:20:32.720 --> 01:20:35.400]   and we then by tomorrow tell you
[01:20:35.400 --> 01:20:36.880]   how old you are biologically
[01:20:36.880 --> 01:20:40.920]   based on what we call the epigenetic clock.
[01:20:40.920 --> 01:20:43.120]   And you might be freaked out, you might be happy,
[01:20:43.120 --> 01:20:45.560]   but either way, we can advise you
[01:20:45.560 --> 01:20:47.840]   on how to improve the trajectory.
[01:20:47.840 --> 01:20:49.420]   'Cause we know that smoking increases
[01:20:49.420 --> 01:20:50.880]   the speed of that clock.
[01:20:50.880 --> 01:20:52.880]   We also know that fasting and people
[01:20:52.880 --> 01:20:55.960]   who eat the right foods have a slower clock.
[01:20:55.960 --> 01:20:58.520]   Without that knowledge, you're flying blind.
[01:20:58.520 --> 01:21:01.280]   But I like the idea of a swab 'cause it's just so easy.
[01:21:01.280 --> 01:21:03.280]   A lot of us have done something like that for COVID tests.
[01:21:03.280 --> 01:21:04.120]   It's not a big deal.
[01:21:04.120 --> 01:21:06.160]   - Yeah, I've been doing a nonstop rapid antigen test.
[01:21:06.160 --> 01:21:11.080]   So let me say that particular one, rapid antigen test,
[01:21:11.080 --> 01:21:12.680]   they've been a source of frustration for me
[01:21:12.680 --> 01:21:14.840]   because everybody should be doing it.
[01:21:14.840 --> 01:21:16.080]   It's so easy.
[01:21:16.080 --> 01:21:17.280]   - We've also been working in my lab
[01:21:17.280 --> 01:21:19.120]   on democratizing these tests
[01:21:19.120 --> 01:21:22.080]   to bring them down from a few hundred bucks to a dollar.
[01:21:22.080 --> 01:21:24.240]   - So just to clarify, you're talking about not research,
[01:21:24.240 --> 01:21:25.800]   you're talking about company stuff,
[01:21:25.800 --> 01:21:28.600]   like actual consumer-facing things?
[01:21:28.600 --> 01:21:29.440]   - Well, right.
[01:21:29.440 --> 01:21:31.000]   The research on bringing the price down
[01:21:31.000 --> 01:21:32.920]   has occurred in my lab at Harvard.
[01:21:32.920 --> 01:21:35.080]   And then that intellectual property is being licensed
[01:21:35.080 --> 01:21:37.240]   and has been licensed out to a company
[01:21:37.240 --> 01:21:40.300]   that will be consumer-facing.
[01:21:40.300 --> 01:21:43.320]   So anybody for a small amount of money can do this.
[01:21:43.320 --> 01:21:46.400]   - Well, you got subscriber number one obsessed.
[01:21:46.400 --> 01:21:48.640]   I think that's a beautiful, beautiful idea.
[01:21:48.640 --> 01:21:51.480]   So somebody who maybe I would have been more hesitant
[01:21:51.480 --> 01:21:56.360]   about it until COVID, but home tests are super easy.
[01:21:56.360 --> 01:21:59.360]   I almost wanted to share that data with the world,
[01:21:59.360 --> 01:22:01.840]   like in some way, not the entirety of the data,
[01:22:01.840 --> 01:22:05.800]   but like some visualization of like how I'm doing.
[01:22:05.800 --> 01:22:09.400]   Like, it's almost like, you know, when you share,
[01:22:09.400 --> 01:22:11.440]   if you had like a long run or something like that,
[01:22:11.440 --> 01:22:14.560]   I wish I could share, 'cause it inspires others.
[01:22:14.560 --> 01:22:16.920]   And then you can have a conversation about like,
[01:22:16.920 --> 01:22:18.820]   well, what are the hacks that you've tried?
[01:22:18.820 --> 01:22:21.400]   And have a conversation about like how to improve lifestyle
[01:22:21.400 --> 01:22:23.680]   and those kinds of things that's grounded in data.
[01:22:23.680 --> 01:22:25.580]   - That's exactly, that's what's gonna happen.
[01:22:25.580 --> 01:22:27.440]   Now, everything's anonymous, of course.
[01:22:27.440 --> 01:22:29.600]   We talked about security there,
[01:22:29.600 --> 01:22:32.560]   but once it's anonymized, you can then plot these numbers.
[01:22:32.560 --> 01:22:35.580]   And I've plotted my epigenetic age
[01:22:35.580 --> 01:22:39.080]   versus hundreds of other people who've taken this test now.
[01:22:39.080 --> 01:22:41.440]   And I can tell you where I fit relative to others
[01:22:41.440 --> 01:22:43.600]   in terms of my biological age.
[01:22:43.600 --> 01:22:45.080]   And I'm happy to share that with you all,
[01:22:45.080 --> 01:22:46.340]   'cause it's pretty low.
[01:22:46.340 --> 01:22:48.520]   You can choose to share it, of course,
[01:22:48.520 --> 01:22:50.440]   not everyone wants to share that.
[01:22:50.440 --> 01:22:52.240]   But when you go to the doctor,
[01:22:52.240 --> 01:22:54.840]   first of all, your doctor does treat you
[01:22:54.840 --> 01:22:55.960]   as though you're an average person,
[01:22:55.960 --> 01:22:58.600]   and none of us are average, there's no such thing.
[01:22:58.600 --> 01:23:00.920]   But second of all, we never know how we're doing
[01:23:00.920 --> 01:23:04.280]   relative to others, 'cause we all, most of us,
[01:23:04.280 --> 01:23:05.820]   we don't share our information.
[01:23:05.820 --> 01:23:08.040]   So we might have this number and that number,
[01:23:08.040 --> 01:23:09.480]   but do you know that your numbers
[01:23:09.480 --> 01:23:11.280]   are good for your age or not?
[01:23:11.280 --> 01:23:12.720]   You have no idea.
[01:23:12.720 --> 01:23:14.560]   Even your doctor probably doesn't even know.
[01:23:14.560 --> 01:23:16.360]   So this graph that I'm talking about
[01:23:16.360 --> 01:23:18.040]   is the beginning of a world where you can say,
[01:23:18.040 --> 01:23:19.360]   how am I doing?
[01:23:19.360 --> 01:23:22.720]   I'm a, for the two of us, we're white and we're male
[01:23:22.720 --> 01:23:25.400]   and we're this age, and we do this.
[01:23:25.400 --> 01:23:27.260]   Are we good?
[01:23:27.260 --> 01:23:28.600]   Are we doing the right things or the wrong things?
[01:23:28.600 --> 01:23:30.440]   Do we need to fix certain things?
[01:23:30.440 --> 01:23:32.120]   And this is what the future is.
[01:23:32.120 --> 01:23:35.360]   It's, forget about just experimenting
[01:23:35.360 --> 01:23:36.320]   and not knowing the result.
[01:23:36.320 --> 01:23:37.280]   I mean, who doesn't experiment
[01:23:37.280 --> 01:23:38.520]   and doesn't look at the data?
[01:23:38.520 --> 01:23:40.220]   No one, it makes no sense.
[01:23:40.220 --> 01:23:41.240]   So we're gonna enter a world
[01:23:41.240 --> 01:23:43.480]   where we have a dashboard on our body,
[01:23:43.480 --> 01:23:46.460]   the swabs, the blood tests, the biosensors,
[01:23:46.460 --> 01:23:48.520]   where our doctors can look at that,
[01:23:48.520 --> 01:23:51.600]   but we can also look at it and they can recommend,
[01:23:51.600 --> 01:23:52.960]   go to this restaurant down the road,
[01:23:52.960 --> 01:23:54.400]   they've got this great meal.
[01:23:54.400 --> 01:23:56.320]   It's high in whatever you need today,
[01:23:56.320 --> 01:23:58.480]   'cause you're lacking vitamin D and vitamin K too,
[01:23:58.480 --> 01:23:59.320]   go for it.
[01:23:59.320 --> 01:24:03.240]   - Ridiculous question, or perhaps not.
[01:24:03.240 --> 01:24:05.080]   If you look maybe 50 years from now
[01:24:05.080 --> 01:24:07.760]   or 100 years from now, a person born then,
[01:24:07.760 --> 01:24:09.280]   what do you think is a good goal
[01:24:09.280 --> 01:24:12.320]   in terms of how long a person would live?
[01:24:12.320 --> 01:24:14.760]   Like what is the maximum longevity
[01:24:14.760 --> 01:24:17.360]   that we can achieve through the methods
[01:24:17.360 --> 01:24:19.860]   that we have today of,
[01:24:19.860 --> 01:24:21.120]   or are developing some of the things
[01:24:21.120 --> 01:24:22.320]   we've been talking about
[01:24:22.320 --> 01:24:26.280]   in terms of genetics, in terms of biology?
[01:24:26.280 --> 01:24:28.160]   Is there a number?
[01:24:28.160 --> 01:24:31.760]   - Right, well, so it changes all the time
[01:24:31.760 --> 01:24:33.420]   because technology's changing so quickly.
[01:24:33.420 --> 01:24:36.240]   I keep revising the number upward,
[01:24:36.240 --> 01:24:38.660]   but I would say that if you do the right things
[01:24:38.660 --> 01:24:40.760]   during your life and start at an early age,
[01:24:40.760 --> 01:24:43.280]   let's say 25, we don't want malnutrition, starvation,
[01:24:43.280 --> 01:24:45.160]   that's not what I'm talking about,
[01:24:45.160 --> 01:24:48.840]   but in your 20s, start eating the kind of diets
[01:24:48.840 --> 01:24:51.880]   that I talked about, skipping meals.
[01:24:51.880 --> 01:24:55.920]   In animals, that gives you an extra 20 to 30%.
[01:24:55.920 --> 01:24:57.840]   We don't know if that's true for humans,
[01:24:57.840 --> 01:25:00.840]   and that would, even 5% more would be a good,
[01:25:00.840 --> 01:25:02.140]   a big deal for the planet.
[01:25:03.220 --> 01:25:05.120]   I think that we should all aim
[01:25:05.120 --> 01:25:06.640]   to at least reach a century.
[01:25:06.640 --> 01:25:09.360]   I'm a little bit behind.
[01:25:09.360 --> 01:25:12.040]   I was born too early to benefit the most
[01:25:12.040 --> 01:25:13.840]   from all of this discovery.
[01:25:13.840 --> 01:25:16.000]   Those of you who are in your 20s,
[01:25:16.000 --> 01:25:18.680]   you should definitely aim to reach 100.
[01:25:18.680 --> 01:25:19.680]   I don't see why not.
[01:25:19.680 --> 01:25:23.140]   Consider this, this is really important.
[01:25:23.140 --> 01:25:25.040]   The average lifespan of a human
[01:25:25.040 --> 01:25:28.120]   that looks after themselves but doesn't pay attention
[01:25:28.120 --> 01:25:31.000]   is about 80, okay?
[01:25:31.000 --> 01:25:34.160]   Japan, that's the average age for a male, a bit higher.
[01:25:34.160 --> 01:25:37.220]   If you do the right things in your life,
[01:25:37.220 --> 01:25:41.140]   which is eat healthy food, don't overeat,
[01:25:41.140 --> 01:25:42.900]   don't become obese, do a bit of exercise,
[01:25:42.900 --> 01:25:44.720]   get good sleep, and don't stress,
[01:25:44.720 --> 01:25:47.220]   that gives you, on average, 14 extra years.
[01:25:47.220 --> 01:25:49.060]   That gets you to 94.
[01:25:49.060 --> 01:25:51.100]   So getting to 100, if you just focus
[01:25:51.100 --> 01:25:53.700]   on what I'm talking about, it's not a big deal.
[01:25:53.700 --> 01:25:54.620]   So what's the maximum?
[01:25:54.620 --> 01:25:57.380]   Well, we know that one human made it to 122,
[01:25:57.380 --> 01:26:00.360]   and a number of them make it into their teens.
[01:26:00.360 --> 01:26:02.180]   I think that's also the next level
[01:26:02.180 --> 01:26:06.300]   of where we can get to with the types of technologies
[01:26:06.300 --> 01:26:07.580]   that I'm talking about.
[01:26:07.580 --> 01:26:09.980]   Medicines, like I mentioned rapamycin,
[01:26:09.980 --> 01:26:10.980]   there's one called metformin,
[01:26:10.980 --> 01:26:14.420]   which is the diabetes drug which I take.
[01:26:14.420 --> 01:26:16.460]   That, in combination with these lifestyle changes,
[01:26:16.460 --> 01:26:18.380]   should get us beyond 100.
[01:26:18.380 --> 01:26:19.640]   How long can we ultimately live?
[01:26:19.640 --> 01:26:22.260]   Well, there's no maximum limit to human lifespan.
[01:26:22.260 --> 01:26:24.600]   Why can a whale live 300 years, but we cannot?
[01:26:24.600 --> 01:26:26.260]   We're basically the same structure.
[01:26:26.260 --> 01:26:27.980]   We just need to learn from them.
[01:26:27.980 --> 01:26:31.080]   So anyone who says, "Oh, you max out at X,"
[01:26:31.080 --> 01:26:33.280]   I think is full of it.
[01:26:33.280 --> 01:26:35.360]   There's nothing that I've seen that says
[01:26:35.360 --> 01:26:37.520]   biological organisms have to die.
[01:26:37.520 --> 01:26:39.520]   There are trees that live for thousands of years,
[01:26:39.520 --> 01:26:42.480]   and their biochemistry is pretty close to ours.
[01:26:42.480 --> 01:26:44.960]   - What do you think it means to live for a very long time?
[01:26:44.960 --> 01:26:47.280]   Let's say if it's 200 years we're talking about,
[01:26:47.280 --> 01:26:48.640]   or 1,000 years.
[01:26:48.640 --> 01:26:54.600]   There's some sense, you could argue
[01:26:54.600 --> 01:26:58.020]   that there is immortal organisms already living on Earth.
[01:26:58.020 --> 01:26:59.040]   Like there's bacteria.
[01:26:59.040 --> 01:27:03.740]   So there's certain living organisms
[01:27:03.740 --> 01:27:07.940]   that in some fundamental way do not die,
[01:27:07.940 --> 01:27:10.260]   because they keep replicating their genetic information.
[01:27:10.260 --> 01:27:12.120]   They keep cloning themselves.
[01:27:12.120 --> 01:27:18.800]   Is it the same human if we can somehow persist
[01:27:18.800 --> 01:27:23.900]   the human mind, like copy-clone certain aspects,
[01:27:23.900 --> 01:27:26.340]   and just keep replacing body parts?
[01:27:26.340 --> 01:27:30.420]   Do you think that's another way to achieve immortality?
[01:27:30.420 --> 01:27:33.740]   To achieve a prolonged, sort of increased longevity
[01:27:33.740 --> 01:27:37.100]   is to replace the parts that break easily,
[01:27:37.100 --> 01:27:39.980]   and keep, 'cause actually from your theory
[01:27:39.980 --> 01:27:44.440]   of aging as a degradation of information,
[01:27:44.440 --> 01:27:46.540]   so an information theory view of aging,
[01:27:46.540 --> 01:27:51.480]   like what is the key information that makes a human?
[01:27:51.480 --> 01:27:53.440]   Can we persist that information,
[01:27:53.440 --> 01:27:56.200]   and just replace the trivial parts?
[01:27:56.200 --> 01:27:59.340]   - Yeah, I mean the short answer is yes.
[01:27:59.340 --> 01:28:01.340]   We're already replacing body parts,
[01:28:01.340 --> 01:28:03.820]   but what makes us human is our brain.
[01:28:03.820 --> 01:28:07.360]   Everything else is suboptimal except our brain.
[01:28:07.360 --> 01:28:13.760]   The ability to replace actual neurons is really hard.
[01:28:13.760 --> 01:28:16.220]   I think it might be easy to upload
[01:28:16.220 --> 01:28:17.940]   rather than replace neurons,
[01:28:17.940 --> 01:28:20.500]   because they're so tight, it's such a network,
[01:28:20.500 --> 01:28:22.260]   and just perturbing the system.
[01:28:22.800 --> 01:28:25.720]   You know, it's Schrdinger's cat.
[01:28:25.720 --> 01:28:28.760]   You change everything once you get in there.
[01:28:28.760 --> 01:28:31.800]   The problem is, well I guess the solution,
[01:28:31.800 --> 01:28:34.160]   let me go to the solution, that's more interesting.
[01:28:34.160 --> 01:28:35.000]   What we're learning is that
[01:28:35.000 --> 01:28:37.380]   if you reverse the age of nerve cells,
[01:28:37.380 --> 01:28:41.400]   it looks like they get their memories back.
[01:28:41.400 --> 01:28:42.640]   So the memories are not lost,
[01:28:42.640 --> 01:28:45.320]   they're just that the cells don't know how to interpret them
[01:28:45.320 --> 01:28:46.960]   and function correctly.
[01:28:46.960 --> 01:28:48.840]   And this is one of the things we're studying in my lab.
[01:28:48.840 --> 01:28:50.440]   If you take an old mouse that has learned something
[01:28:50.440 --> 01:28:53.660]   when it was young, but forgotten, does it get that back?
[01:28:53.660 --> 01:28:56.460]   And all evidence points to that being true.
[01:28:56.460 --> 01:28:59.620]   So I'd rather go in and rejuvenate the brain as it sits
[01:28:59.620 --> 01:29:01.300]   rather than replace individual cells,
[01:29:01.300 --> 01:29:03.100]   which would be really hard.
[01:29:03.100 --> 01:29:06.720]   - What do you think about efforts like Neuralink,
[01:29:06.720 --> 01:29:10.260]   which basically, you mentioned uploading,
[01:29:10.260 --> 01:29:11.740]   are trying to figure out,
[01:29:11.740 --> 01:29:13.660]   so creating brain-computer interfaces
[01:29:13.660 --> 01:29:14.620]   that are trying to figure out
[01:29:14.620 --> 01:29:16.900]   how to communicate with the brain.
[01:29:16.900 --> 01:29:18.100]   But one of the features of that
[01:29:18.100 --> 01:29:20.860]   is trying to record the human brain
[01:29:20.860 --> 01:29:22.740]   more and more accurately.
[01:29:22.740 --> 01:29:26.060]   Do you have hope for that to,
[01:29:26.060 --> 01:29:30.780]   of course, it will lead to us better understanding
[01:29:30.780 --> 01:29:33.220]   from a neuroscience perspective, the human mind,
[01:29:33.220 --> 01:29:36.060]   but do you have hope for it increasing longevity
[01:29:36.060 --> 01:29:38.340]   in terms of how it's used?
[01:29:38.340 --> 01:29:41.180]   - I think that it can help with certain diseases.
[01:29:41.180 --> 01:29:42.980]   But I see, at least within our lifetime,
[01:29:42.980 --> 01:29:43.980]   that's the best use of it,
[01:29:43.980 --> 01:29:46.100]   is to be able to replace parts of the body
[01:29:46.100 --> 01:29:49.380]   that are not functioning, such as the retina
[01:29:49.380 --> 01:29:51.860]   and other parts, the visual cortex back here.
[01:29:51.860 --> 01:29:53.900]   That's going to be doable.
[01:29:53.900 --> 01:29:55.300]   In terms of longevity,
[01:29:55.300 --> 01:29:58.020]   maybe we could put something on the hypothalamus
[01:29:58.020 --> 01:30:00.660]   and start secreting those hormones and get that back.
[01:30:00.660 --> 01:30:06.780]   Ultimately, I think the best way to preserve the brain
[01:30:06.780 --> 01:30:10.780]   is going to be to record it,
[01:30:10.780 --> 01:30:13.860]   but also, I think it's going to require death, unfortunately,
[01:30:13.860 --> 01:30:16.980]   to then do very detailed scans,
[01:30:16.980 --> 01:30:19.180]   even if you have enough time and money,
[01:30:19.180 --> 01:30:22.500]   atomic microscopy, and rebuild the brain from scratch.
[01:30:22.500 --> 01:30:24.340]   - Rebuild from scratch, yeah.
[01:30:24.340 --> 01:30:28.020]   We are living more and more in a digital world.
[01:30:28.020 --> 01:30:31.500]   I wonder if the scanning is good enough
[01:30:31.500 --> 01:30:34.420]   for the critical things in terms of memories,
[01:30:34.420 --> 01:30:36.420]   in terms of the particular quirks
[01:30:36.420 --> 01:30:38.100]   of your cognitive processes.
[01:30:38.100 --> 01:30:38.940]   - They're not.
[01:30:38.940 --> 01:30:42.020]   - We're not close, yes,
[01:30:42.060 --> 01:30:44.740]   but we've made quite a bit of progress.
[01:30:44.740 --> 01:30:50.540]   If you're an exponential type of person.
[01:30:50.540 --> 01:30:52.140]   - Well, let's dream a little here.
[01:30:52.140 --> 01:30:53.180]   - Yes, that's the point.
[01:30:53.180 --> 01:30:55.820]   - The way it would work, that I could see it working,
[01:30:55.820 --> 01:31:00.820]   is you take a single cell slice through your dead brain,
[01:31:00.820 --> 01:31:04.140]   and we can now, the problem with the engineering aspect
[01:31:04.140 --> 01:31:06.060]   is that the engineering is,
[01:31:06.060 --> 01:31:09.700]   the physical aspect of the brain is not even half the problem.
[01:31:09.700 --> 01:31:12.980]   The problem is which genes are switched on and off.
[01:31:12.980 --> 01:31:14.460]   This experience that we're having here
[01:31:14.460 --> 01:31:18.260]   is altering certain genes in neurons
[01:31:18.260 --> 01:31:22.060]   that will be preserved, hopefully, for a number of decades.
[01:31:22.060 --> 01:31:25.300]   But you cannot see that with a microscope easily.
[01:31:25.300 --> 01:31:27.780]   But there are technologies invented,
[01:31:27.780 --> 01:31:30.760]   actually, just down the hall in the building I'm at,
[01:31:30.760 --> 01:31:33.500]   George Church invented a way, his lab invented a way
[01:31:33.500 --> 01:31:36.820]   to look at which genes are switched on and off,
[01:31:36.820 --> 01:31:38.060]   not only in a single cell,
[01:31:38.060 --> 01:31:40.100]   which any lab can do these days,
[01:31:40.100 --> 01:31:42.980]   but in situ, where it's situated in the brain.
[01:31:42.980 --> 01:31:45.260]   So you can say, okay, this nerve cell
[01:31:45.260 --> 01:31:47.700]   had these genes switched on and these switched off,
[01:31:47.700 --> 01:31:49.740]   we can recreate that.
[01:31:49.740 --> 01:31:50.980]   But just scanning the brain
[01:31:50.980 --> 01:31:52.540]   and looking how the nerves are touching each other
[01:31:52.540 --> 01:31:53.540]   is not gonna do it.
[01:31:53.540 --> 01:31:58.140]   - Wow, okay, so you have to scan the full biology,
[01:31:58.140 --> 01:31:59.220]   the full details.
[01:31:59.220 --> 01:32:00.420]   - And look at the epigenome.
[01:32:00.420 --> 01:32:01.580]   - And the epigenome, too.
[01:32:01.580 --> 01:32:03.140]   - Yeah, which genes are on and off.
[01:32:03.140 --> 01:32:05.100]   It's just easier to reset the epigenome
[01:32:05.100 --> 01:32:07.540]   and get them to work like they used to.
[01:32:07.540 --> 01:32:08.380]   We're doing that now.
[01:32:08.380 --> 01:32:09.760]   - Use the hardware we already have,
[01:32:09.760 --> 01:32:13.860]   just figure out how to make that hardware last longer.
[01:32:13.860 --> 01:32:15.700]   - Right, ultimately, information will be lost.
[01:32:15.700 --> 01:32:19.380]   Even genetic information degrades slowly through mutation.
[01:32:19.380 --> 01:32:22.500]   So immortality is not achievable through that means,
[01:32:22.500 --> 01:32:25.260]   though I think we could potentially reset the body
[01:32:25.260 --> 01:32:28.460]   hundreds of times and live for thousands of years.
[01:32:28.460 --> 01:32:31.540]   - Okay, so we talked about biology.
[01:32:31.540 --> 01:32:34.620]   Let's, forgive me, but let's talk about philosophy
[01:32:34.620 --> 01:32:36.740]   for just a brief moment.
[01:32:36.740 --> 01:32:39.420]   So somebody I've enjoyed reading, Ernest Becker,
[01:32:39.420 --> 01:32:40.700]   wrote "The Denial of Death."
[01:32:40.700 --> 01:32:42.980]   There's also Martin Heidegger.
[01:32:42.980 --> 01:32:45.100]   There's a bunch of philosophers who claim
[01:32:45.100 --> 01:32:51.940]   that most people live life in denial of death.
[01:32:51.940 --> 01:32:56.940]   Sort of we don't fully internalize
[01:32:56.940 --> 01:33:00.100]   the idea that we're going to die.
[01:33:04.180 --> 01:33:06.940]   Because if we did, as they say,
[01:33:06.940 --> 01:33:10.180]   there will be a kind of terror of,
[01:33:10.180 --> 01:33:14.060]   I mean, a deep fear of death.
[01:33:14.060 --> 01:33:17.220]   The fact that we don't know what's,
[01:33:17.220 --> 01:33:22.220]   like we almost don't know what to do with non-existence,
[01:33:22.220 --> 01:33:24.820]   with disappearing.
[01:33:24.820 --> 01:33:28.300]   Like our, the way we draw meaning from life
[01:33:28.300 --> 01:33:30.820]   seems to be grounded in the fact that we exist
[01:33:30.820 --> 01:33:34.580]   and that we at some point will not exist is terrifying.
[01:33:34.580 --> 01:33:37.580]   And so we live in an illusion that we're not going to die
[01:33:37.580 --> 01:33:39.660]   and we run from that terror.
[01:33:39.660 --> 01:33:41.700]   That's what Ernest Becker would say.
[01:33:41.700 --> 01:33:44.220]   Do you think there's any truth to that?
[01:33:44.220 --> 01:33:45.420]   - Oh, I know there's truth to that.
[01:33:45.420 --> 01:33:47.780]   I experience it every day when I talk to people.
[01:33:47.780 --> 01:33:49.660]   We have to live that way.
[01:33:49.660 --> 01:33:51.380]   Although, unfortunately, I can't.
[01:33:51.380 --> 01:33:56.380]   But for most people, it's extremely distressing
[01:33:56.380 --> 01:33:59.360]   to think about their own mortality.
[01:33:59.360 --> 01:34:00.540]   We think about it occasionally.
[01:34:00.540 --> 01:34:02.260]   And if we really thought about it every day,
[01:34:02.260 --> 01:34:03.980]   we'd probably be brought to tears.
[01:34:03.980 --> 01:34:05.740]   How much we'd not just miss ourselves,
[01:34:05.740 --> 01:34:07.540]   but miss our family, our friends.
[01:34:07.540 --> 01:34:11.740]   We are of, all living life forms have evolved
[01:34:11.740 --> 01:34:14.380]   to not want to die.
[01:34:14.380 --> 01:34:18.180]   And when I mean want, biochemically, genetically, physically.
[01:34:18.180 --> 01:34:21.620]   That yeast cell, the cells that I studied at MIT,
[01:34:21.620 --> 01:34:23.660]   they were fighting for their lives.
[01:34:23.660 --> 01:34:25.220]   They didn't think.
[01:34:25.220 --> 01:34:28.940]   But our brain has evolved the same survival aspect.
[01:34:28.940 --> 01:34:30.340]   Of course, we don't want to die.
[01:34:30.340 --> 01:34:32.500]   But the problem for us, unfortunately,
[01:34:32.500 --> 01:34:34.920]   it's a curse and a blessing, is that we're now conscious.
[01:34:34.920 --> 01:34:37.140]   We know that we're going to die.
[01:34:37.140 --> 01:34:40.260]   Most species that have ever existed don't.
[01:34:40.260 --> 01:34:42.260]   That's a burden, that's a curse.
[01:34:42.260 --> 01:34:44.700]   And so what I think's happened is we've evolved, certainly,
[01:34:44.700 --> 01:34:49.120]   to want to live for a long time, perhaps never want to die.
[01:34:49.120 --> 01:34:52.120]   But the thought about dying is so traumatic
[01:34:52.120 --> 01:34:55.560]   that there is an innate part of our brains,
[01:34:55.560 --> 01:34:59.200]   and it's probably genetically wired, to not think about it.
[01:35:00.040 --> 01:35:03.140]   I really think that's part of being human.
[01:35:03.140 --> 01:35:05.240]   Because, you know, I think about tribes
[01:35:05.240 --> 01:35:07.800]   that obsessed with longevity every day
[01:35:07.800 --> 01:35:09.580]   and that were going to die.
[01:35:09.580 --> 01:35:12.280]   They probably didn't make much technological progress
[01:35:12.280 --> 01:35:14.540]   because they were just crying in their huts every day,
[01:35:14.540 --> 01:35:16.240]   or, you know, in the savanna.
[01:35:16.240 --> 01:35:20.320]   I really think that we've evolved to naturally deny aging.
[01:35:20.320 --> 01:35:23.180]   And it's one of the problems that I face in my career,
[01:35:23.180 --> 01:35:26.760]   and, you know, when I speak publicly and on social media,
[01:35:26.760 --> 01:35:28.340]   is that it's shocking.
[01:35:28.340 --> 01:35:29.900]   People don't want to think about their age,
[01:35:29.900 --> 01:35:31.720]   but I think it's getting better.
[01:35:31.720 --> 01:35:33.920]   I think my book has helped.
[01:35:33.920 --> 01:35:36.280]   These tests that we're developing should help people
[01:35:36.280 --> 01:35:39.080]   understand it's not a problem to think about
[01:35:39.080 --> 01:35:40.320]   your long-term health.
[01:35:40.320 --> 01:35:42.680]   In fact, if you don't, you're going to reach 80
[01:35:42.680 --> 01:35:43.840]   and really regret it.
[01:35:43.840 --> 01:35:47.740]   - And the other side of it, so again, Ernest Becker,
[01:35:47.740 --> 01:35:49.160]   but also Viktor Frankl,
[01:35:49.160 --> 01:35:51.320]   I recommend a highly manned social meeting.
[01:35:51.320 --> 01:35:54.900]   Bernard Williams is a moral philosopher.
[01:35:54.900 --> 01:35:58.600]   They kind of argue that this knowledge of death,
[01:35:58.600 --> 01:36:03.600]   even if we often don't contemplate it, we do at times.
[01:36:03.600 --> 01:36:06.960]   And the very, what you call the curse,
[01:36:06.960 --> 01:36:10.640]   which I agree with you, it's a curse and a blessing
[01:36:10.640 --> 01:36:13.880]   that we're able to contemplate our own mortality.
[01:36:13.880 --> 01:36:16.580]   That gives meaning to life.
[01:36:16.580 --> 01:36:18.840]   So death gives meaning to life.
[01:36:18.840 --> 01:36:21.140]   As what Viktor Frankl argues,
[01:36:21.140 --> 01:36:22.600]   I would probably argue the same.
[01:36:22.600 --> 01:36:25.180]   There's something about the scarcity of life
[01:36:25.180 --> 01:36:27.140]   and contemplating that,
[01:36:27.140 --> 01:36:30.180]   that makes each moment that much sweeter.
[01:36:30.180 --> 01:36:32.380]   Is there something to that?
[01:36:32.380 --> 01:36:33.860]   - I think it's individual.
[01:36:33.860 --> 01:36:35.940]   In my case, it's completely wrong.
[01:36:35.940 --> 01:36:38.060]   (laughing)
[01:36:38.060 --> 01:36:39.820]   - I appreciate you saying that.
[01:36:39.820 --> 01:36:41.860]   - I don't get joy out of every day
[01:36:41.860 --> 01:36:43.780]   because I think I'm going to die.
[01:36:43.780 --> 01:36:46.060]   I get joy out of every day because every day is joyous
[01:36:46.060 --> 01:36:47.460]   and I make it that way.
[01:36:47.460 --> 01:36:50.460]   And even if I thought I was going to live forever,
[01:36:50.460 --> 01:36:53.140]   I would still be enjoying this moment just as much.
[01:36:53.140 --> 01:36:56.340]   And I bet you would too.
[01:36:56.340 --> 01:36:59.900]   - Well, that's, I think about that a lot.
[01:36:59.900 --> 01:37:03.580]   I think it's very difficult to know.
[01:37:03.580 --> 01:37:06.720]   I'm almost afraid that I wouldn't enjoy it as much
[01:37:06.720 --> 01:37:07.980]   if I was immortal.
[01:37:07.980 --> 01:37:11.900]   I'm almost afraid to want to be immortal or to live longer
[01:37:11.900 --> 01:37:16.900]   because it perhaps is a kind of justification for me
[01:37:18.900 --> 01:37:21.700]   to accept that I'm going to die.
[01:37:21.700 --> 01:37:23.420]   It's saying like, oh, if I was immortal,
[01:37:23.420 --> 01:37:26.160]   I wouldn't be able to enjoy life as much as I do.
[01:37:26.160 --> 01:37:28.300]   But it's very possible that I wouldn't enjoy it
[01:37:28.300 --> 01:37:29.260]   just as much.
[01:37:29.260 --> 01:37:34.220]   Of course, enjoying life, whether you're mortal or not,
[01:37:34.220 --> 01:37:35.540]   takes work.
[01:37:35.540 --> 01:37:38.940]   Like it requires you to have the right kind of frame
[01:37:38.940 --> 01:37:39.840]   of mind.
[01:37:39.840 --> 01:37:42.460]   You can discover, you can focus your mind
[01:37:42.460 --> 01:37:44.500]   on the ugliness of life.
[01:37:44.500 --> 01:37:46.900]   There's plenty of ugly things in this world
[01:37:46.900 --> 01:37:47.960]   and you can focus on them.
[01:37:47.960 --> 01:37:49.320]   You can complain.
[01:37:49.320 --> 01:37:53.460]   Whenever, like, you know, if it's raining outside,
[01:37:53.460 --> 01:37:56.740]   you can focus on the fact that you have shelter
[01:37:56.740 --> 01:37:58.460]   and enjoy the hell out of it.
[01:37:58.460 --> 01:38:02.020]   Or you can enjoy running in the rain when it's warm
[01:38:02.020 --> 01:38:05.700]   and like the beauty of nature, just being one with nature.
[01:38:05.700 --> 01:38:06.780]   Or you can just complain,
[01:38:06.780 --> 01:38:08.980]   this fucking weather again in Boston.
[01:38:08.980 --> 01:38:11.740]   And then we see they're always raining or freezing, damn it.
[01:38:11.740 --> 01:38:16.740]   And like the same thing with like wifi going out
[01:38:16.760 --> 01:38:18.100]   on airplanes.
[01:38:18.100 --> 01:38:23.100]   Like you can either complain about like stupid wifi
[01:38:23.100 --> 01:38:25.860]   and JetBlue or something.
[01:38:25.860 --> 01:38:27.620]   Or you could say like how incredible it is
[01:38:27.620 --> 01:38:29.180]   that I can fly through the sky
[01:38:29.180 --> 01:38:31.820]   and in a matter of hours be anywhere else in the world.
[01:38:31.820 --> 01:38:34.020]   And then I could also on occasion watch,
[01:38:34.020 --> 01:38:37.540]   like check email and even watch movies
[01:38:37.540 --> 01:38:39.540]   while connecting through satellites
[01:38:39.540 --> 01:38:40.500]   that are flying through space.
[01:38:40.500 --> 01:38:41.860]   So it's a matter of perspective.
[01:38:41.860 --> 01:38:44.980]   And perhaps there's an extra level of work required
[01:38:44.980 --> 01:38:46.540]   when you're a mortal.
[01:38:46.540 --> 01:38:48.680]   Because it's easier when you're a mortal
[01:38:48.680 --> 01:38:53.000]   or live longer to be lazy, to delay stuff.
[01:38:53.000 --> 01:38:54.100]   But if you're not,
[01:38:54.100 --> 01:38:56.220]   you can still derive the same amount of joy.
[01:38:56.220 --> 01:38:59.060]   So it's possible, it's possible.
[01:38:59.060 --> 01:38:59.900]   - It's definitely possible.
[01:38:59.900 --> 01:39:02.360]   In my life, I went from being the,
[01:39:02.360 --> 01:39:06.620]   nothing's working to every day's great to wake up to.
[01:39:06.620 --> 01:39:08.920]   And I think even if you live,
[01:39:08.920 --> 01:39:10.420]   think you're going to live forever,
[01:39:10.420 --> 01:39:12.140]   you can enjoy every day.
[01:39:12.140 --> 01:39:14.000]   What I do is everything's relative.
[01:39:14.900 --> 01:39:17.860]   We can compare ourselves to our neighbor who has more money
[01:39:17.860 --> 01:39:20.440]   or to the flight that should have had wifi.
[01:39:20.440 --> 01:39:23.540]   Or which is what I do, I'm still six years old, remember.
[01:39:23.540 --> 01:39:25.280]   What a six year old does says,
[01:39:25.280 --> 01:39:29.900]   look, I can, when I tell my fingers to form a fist,
[01:39:29.900 --> 01:39:31.260]   they actually do that.
[01:39:31.260 --> 01:39:32.660]   That's really cool.
[01:39:32.660 --> 01:39:34.720]   That's how I live my life.
[01:39:34.720 --> 01:39:36.800]   I can pick up on your desk here, this metal object.
[01:39:36.800 --> 01:39:39.820]   It's a metal cube, about an inch by an inch by an inch.
[01:39:39.820 --> 01:39:42.540]   And I tell myself not about cubes,
[01:39:42.540 --> 01:39:44.280]   but about inanimate objects.
[01:39:44.880 --> 01:39:46.280]   Probably once a day I'll say,
[01:39:46.280 --> 01:39:48.240]   I'm a living thing.
[01:39:48.240 --> 01:39:49.880]   I can think, I can move, I can eat.
[01:39:49.880 --> 01:39:51.840]   I am full of energy.
[01:39:51.840 --> 01:39:53.880]   And there's that leaf or this cube here
[01:39:53.880 --> 01:39:55.800]   that will never be alive.
[01:39:55.800 --> 01:39:59.160]   That's what I look at and compare myself to.
[01:39:59.160 --> 01:40:01.080]   And for as long as I live, if it's forever,
[01:40:01.080 --> 01:40:04.000]   of course it won't be, but even if it was forever,
[01:40:04.000 --> 01:40:07.800]   the relative to this lump of metal on this table here,
[01:40:07.800 --> 01:40:10.840]   we are wondrous things in the universe.
[01:40:10.840 --> 01:40:13.760]   And probably the most wondrous things in the universe.
[01:40:13.760 --> 01:40:18.680]   Yeah, we're able to deeply appreciate the leaf or the cube
[01:40:18.680 --> 01:40:20.560]   and deeply appreciate ourselves,
[01:40:20.560 --> 01:40:24.400]   which is, it can be a curse, but it's mostly a gift.
[01:40:24.400 --> 01:40:28.140]   Especially when you're, it's such a beautiful poem.
[01:40:28.140 --> 01:40:31.680]   Now I'm six, I'm as clever as clever.
[01:40:31.680 --> 01:40:35.400]   So I think I'll be six now forever and ever.
[01:40:35.400 --> 01:40:37.880]   That's a good thing to aspire to.
[01:40:37.880 --> 01:40:40.640]   Your grandmother was onto something.
[01:40:40.640 --> 01:40:43.000]   David, this is a incredible conversation.
[01:40:43.000 --> 01:40:44.640]   I'm a huge fan of your work.
[01:40:44.640 --> 01:40:49.360]   So thank you for wasting your valuable time with me today.
[01:40:49.360 --> 01:40:50.520]   I really, really appreciate it.
[01:40:50.520 --> 01:40:51.360]   This was awesome.
[01:40:51.360 --> 01:40:54.000]   Thank you for having me on Lex, appreciate it.
[01:40:54.000 --> 01:40:55.520]   Thanks for listening to this conversation
[01:40:55.520 --> 01:40:56.560]   with David Sinclair.
[01:40:56.560 --> 01:41:01.240]   A thank you to Onnit, Clear, National Instruments,
[01:41:01.240 --> 01:41:03.640]   Simply Safe, and Linode.
[01:41:03.640 --> 01:41:07.160]   Check them out in the description to support this podcast.
[01:41:07.160 --> 01:41:08.720]   And now let me leave you with some words
[01:41:08.720 --> 01:41:10.640]   from Arthur Schopenhauer.
[01:41:10.640 --> 01:41:13.720]   All truth passes through three stages.
[01:41:13.720 --> 01:41:15.520]   First, it is ridiculed.
[01:41:15.520 --> 01:41:18.280]   Second, it is violently opposed.
[01:41:18.280 --> 01:41:22.320]   Third, it is accepted as being self-evident.
[01:41:22.320 --> 01:41:24.960]   Thank you for listening and hope to see you next time.
[01:41:25.040 --> 01:41:28.600]   [AUDIO OUT]
[01:41:28.600 --> 01:41:31.600]   [AUDIO OUT]
[01:41:31.600 --> 01:41:33.660]   you

