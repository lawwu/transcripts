
[00:00:00.000 --> 00:00:07.520]   I'll hit go live. And we should be live on YouTube. Let me just
[00:00:07.520 --> 00:00:17.360]   refresh the page to make sure we live. All right, I can hear
[00:00:17.360 --> 00:00:20.760]   myself in an echo. That means you're live. Good to see
[00:00:20.760 --> 00:00:24.880]   everyone. Welcome back to the Kaggle Talks. I'm really excited
[00:00:24.880 --> 00:00:27.680]   because I'm not talking to one. In fact, I'm talking to three
[00:00:27.680 --> 00:00:31.720]   Kagglers today. We're talking to the fifth position from the
[00:00:31.720 --> 00:00:35.480]   RSNA COVID-19 competition. I'll just quickly introduce our
[00:00:35.480 --> 00:00:38.120]   guests in a minute. But first, I'll probably introduce the
[00:00:38.120 --> 00:00:45.040]   session. Let me quickly share my screen and make sure I get the
[00:00:45.040 --> 00:00:52.240]   right one because I always have a lot of them. So today we're
[00:00:52.240 --> 00:00:58.320]   learning about Shivam, Ayushman and Nishty's fifth place solution
[00:00:58.320 --> 00:01:01.480]   from the SIIM. I'm spelling this out in case you'd like to look
[00:01:01.480 --> 00:01:05.760]   it up on Kaggle. It's SIIM, FISAP-BO, RSNA COVID-19
[00:01:05.760 --> 00:01:10.120]   competition. Thank you so much, all three of you for joining me
[00:01:10.120 --> 00:01:11.680]   and welcome to the session.
[00:01:11.680 --> 00:01:15.560]   Thanks, Sam. It's a pleasure to be here. Yeah.
[00:01:15.560 --> 00:01:17.960]   Thank you for inviting us.
[00:01:17.960 --> 00:01:19.400]   Thank you for inviting us.
[00:01:20.040 --> 00:01:23.440]   So I'll introduce the session and then do a formal intro
[00:01:23.440 --> 00:01:27.480]   about your background. This is the second session in the
[00:01:27.480 --> 00:01:31.520]   series. The previous one was with Anjum Syed. We talked
[00:01:31.520 --> 00:01:34.160]   about his fourth place solution in the competition. You can find
[00:01:34.160 --> 00:01:39.280]   that on a YouTube page. Today is the second one. And I have the
[00:01:39.280 --> 00:01:42.080]   chance of learning again from top Kagglers. I'm very
[00:01:42.080 --> 00:01:46.400]   fortunate to be the first person who's broadcasting this. Quick
[00:01:46.400 --> 00:01:49.560]   note to the audience. So for questions, we'll be taking them
[00:01:49.560 --> 00:01:53.280]   through here. Since this is being live streamed on YouTube
[00:01:53.280 --> 00:02:01.880]   and the chat is always crazy. This link bondb.me/ctkt-2
[00:02:01.880 --> 00:02:05.360]   should take you to our forums. These are discourse forums. I
[00:02:05.360 --> 00:02:08.760]   really like discourse. I've been using them on fasta forums and
[00:02:08.760 --> 00:02:11.040]   you can simply ask your questions here after signing up.
[00:02:17.800 --> 00:02:19.920]   So if you're watching this live, or if you're watching this
[00:02:19.920 --> 00:02:23.240]   later, I'll request you to leave your questions here. And with
[00:02:23.240 --> 00:02:27.280]   during the session, I'll keep taking breaks and keeping an eye
[00:02:27.280 --> 00:02:32.240]   out on this. Let me make sure I put this in the YouTube chat as
[00:02:32.240 --> 00:02:32.400]   well.
[00:02:32.400 --> 00:02:41.000]   So here's the agenda for today. This is I've introduced the
[00:02:41.000 --> 00:02:43.280]   session already. So I'm not going to go over what Chai Time
[00:02:43.280 --> 00:02:47.440]   Data Science is. I'll start by understanding Shivam, Ayushman
[00:02:47.440 --> 00:02:50.640]   and Nischay's journey. I used to do this on my podcast. And it's
[00:02:50.640 --> 00:02:54.400]   always about understanding today. Ayushman is a student,
[00:02:54.400 --> 00:02:57.160]   Nischay is a student as well. How are they making it on the
[00:02:57.160 --> 00:03:01.800]   top of the leaderboard? Nischay, I think is the top ranked Indian
[00:03:01.800 --> 00:03:07.960]   Kaggler across the world. How did he reach there? Shivam has
[00:03:07.960 --> 00:03:11.200]   started his own venture today. I would love to understand where
[00:03:11.200 --> 00:03:14.600]   Kaggle was helpful. So this is what these question and answer
[00:03:14.600 --> 00:03:17.920]   and Chai Time Data Science one was about. I call this 2.0
[00:03:17.920 --> 00:03:20.560]   because we'll also go through the code also understand the
[00:03:20.560 --> 00:03:28.200]   solution. So now, as I said earlier, I'd like to introduce
[00:03:28.200 --> 00:03:32.000]   again, our guest Ayushman is currently a student as as I
[00:03:32.000 --> 00:03:35.400]   mentioned Nischay as well. Nischay is one of the top ranked
[00:03:35.400 --> 00:03:39.560]   Kagglers currently Kaggle master. And Shivam is a Kaggle
[00:03:39.560 --> 00:03:42.880]   competitions master as well. He's working on his own venture.
[00:03:42.920 --> 00:03:45.640]   If you all could please go in a sequence and introduce yourselves
[00:03:45.640 --> 00:03:47.360]   as well to the audience. Ayushman.
[00:03:47.360 --> 00:03:52.240]   Yeah, sure. Hi, everyone. I am Ayushman Buragohai. I am from
[00:03:52.240 --> 00:03:56.600]   Guwahati. And currently I am pursuing my undergrad in
[00:03:56.600 --> 00:04:01.640]   computer science. I'm a fourth year student. And this is my
[00:04:01.640 --> 00:04:06.960]   first Kaggle competition gold medal. And it was really a
[00:04:06.960 --> 00:04:12.640]   pleasure for in teaming up with Nischay and Shivam. So yeah,
[00:04:12.640 --> 00:04:16.160]   that's that's great to hear. We learn how that journey was and
[00:04:16.160 --> 00:04:19.720]   how exciting those leaderboard shifts were for your first
[00:04:19.720 --> 00:04:22.720]   competition. I remember it was very exciting for me just seeing
[00:04:22.720 --> 00:04:25.080]   yourself go up there and then someone would release a kernel
[00:04:25.080 --> 00:04:27.880]   you'd fall down. I digress. Nischay.
[00:04:27.880 --> 00:04:35.240]   So yeah, I'm Nischay currently an undergraduate in electronics
[00:04:35.240 --> 00:04:39.880]   engineering. I'm pursuing my B.Tech from NSU to Delhi. Also,
[00:04:39.880 --> 00:04:43.320]   I am a Kaggle competitions master as Niamh has already
[00:04:43.320 --> 00:04:48.000]   told with a global rank of 32. Also, I have been actively doing
[00:04:48.000 --> 00:04:51.400]   Kaggle since last one and half year and have worked with
[00:04:51.400 --> 00:04:55.120]   several data scientists. So it has been a great journey for me
[00:04:55.120 --> 00:05:00.880]   so far. Also, like I would also like to mention like how it all
[00:05:00.880 --> 00:05:06.160]   started. So it all began when the pandemic struck. So before
[00:05:06.160 --> 00:05:08.840]   that, I was just randomly experimenting with everything,
[00:05:08.840 --> 00:05:12.560]   you know, I was preparing for government sector exams, I was
[00:05:12.560 --> 00:05:15.960]   learning Arduino language, and all that stuff. But then one of
[00:05:15.960 --> 00:05:20.440]   my good friend who knew that I'm quite passionate about
[00:05:20.440 --> 00:05:23.640]   mathematics and also he suggested me about data science
[00:05:23.640 --> 00:05:28.760]   and Kaggle. So that's where the things got started. And like I
[00:05:28.800 --> 00:05:32.320]   landed up on Kaggle last year. And yeah, since then, I'm like,
[00:05:32.320 --> 00:05:37.000]   quite addicted to all the competitions and all. Yeah.
[00:05:37.000 --> 00:05:41.120]   You're being too humble. You've been too modest. We'll come back
[00:05:41.120 --> 00:05:43.960]   to you. I know you're one of the top ranked actors today. You
[00:05:43.960 --> 00:05:46.960]   started just one year ago. And in that span, you've made it to
[00:05:46.960 --> 00:05:50.680]   top 50 of world rankings. That's that's just incredible. We'll
[00:05:50.680 --> 00:05:54.040]   come back to that Shivam. I'd love to hear about your journey
[00:05:54.040 --> 00:05:56.320]   as well. How did you get interested in data science and
[00:05:56.320 --> 00:05:57.180]   Kaggle?
[00:05:58.440 --> 00:06:04.360]   Yeah, so basically, I graduated from IIT Kharagpur in 2017. So I
[00:06:04.360 --> 00:06:08.080]   did master's from there. Earlier to that I had background in
[00:06:08.080 --> 00:06:11.560]   mathematics. So I did PhD on mathematics from Delhi University
[00:06:11.560 --> 00:06:14.840]   and then did MSc Mathematics and Science in the company from an
[00:06:14.840 --> 00:06:19.520]   ITL, Allahabad. And then I was going to MTech plus PhD that is
[00:06:19.520 --> 00:06:22.480]   integrated PhD program in IIT Kharagpur. But what happened
[00:06:22.480 --> 00:06:26.720]   that when I went there, I studied two years of MTech
[00:06:26.720 --> 00:06:30.440]   degree. And then I realized that it will be better to go for a
[00:06:30.440 --> 00:06:34.320]   corporate job and be a part of company and doing tech rather
[00:06:34.320 --> 00:06:38.560]   than doing PhD. So I went and joined Huawei there. And when I
[00:06:38.560 --> 00:06:42.600]   joined Huawei, initially my managers and my teammates thought
[00:06:42.600 --> 00:06:45.880]   that he has a mathematics background. So they gave me a
[00:06:45.880 --> 00:06:49.280]   team which is doing data science. So they gave me a
[00:06:49.280 --> 00:06:52.800]   position where I have to develop algorithms, ML algorithms and
[00:06:52.960 --> 00:06:57.440]   basically implement using big data and Spark tools. So that
[00:06:57.440 --> 00:07:00.600]   time I was introduced with machine learning. To be frank, I
[00:07:00.600 --> 00:07:03.800]   was never introduced with machine learning till 2017, even
[00:07:03.800 --> 00:07:07.760]   in college. But I saw many of my colleagues were doing projects
[00:07:07.760 --> 00:07:11.040]   in ML and I thought ML and of course, blockchain and Bitcoin
[00:07:11.040 --> 00:07:13.800]   and those things. But I never got interested in that I was
[00:07:13.800 --> 00:07:17.040]   like, what is that but I never got interested. I was more
[00:07:17.040 --> 00:07:21.240]   interested in computer science and some part in mathematics. So
[00:07:21.240 --> 00:07:25.280]   in 2017, I started my journey I was doing this and then in
[00:07:25.280 --> 00:07:31.640]   Bangalore, I used to go in one of the MLBLR meetups in
[00:07:31.640 --> 00:07:35.040]   Bangalore. So I went there I learned machine learning because
[00:07:35.040 --> 00:07:39.080]   I saw because my background was mathematics and I was doing in
[00:07:39.080 --> 00:07:43.000]   company this ML work and so I joined there I got interested in
[00:07:43.000 --> 00:07:47.680]   this ML and one of my friend Apshay he told me let's try in
[00:07:47.680 --> 00:07:50.920]   Kaggle competitions and it is very good we can do good in
[00:07:50.920 --> 00:07:54.240]   that. So I joined just for fun and competed in Kaggle
[00:07:54.240 --> 00:07:56.800]   competitions. And when I competed in Kaggle competition,
[00:07:56.800 --> 00:07:59.920]   I got addicted to that. I mean, it gave me a lot of learning
[00:07:59.920 --> 00:08:03.000]   also and I got addicted to that I learned a lot of different
[00:08:03.000 --> 00:08:06.040]   data sets problem statement how to solve them. My initial
[00:08:06.040 --> 00:08:09.360]   competition was on CV mostly. So it was quick draw competition
[00:08:09.360 --> 00:08:13.160]   from Google I competed in that and fortunately we won our first
[00:08:13.160 --> 00:08:17.160]   medal in that so it was a very good journey in starting itself.
[00:08:17.440 --> 00:08:21.640]   So I did some competition in starting in 2017 and 18. And
[00:08:21.640 --> 00:08:25.520]   then I made a break and then I tried to switch companies and
[00:08:25.520 --> 00:08:28.600]   from Huawei I went to Microsoft. So I went Microsoft and joined
[00:08:28.600 --> 00:08:32.040]   as a data and applied scientist there for two years. And I had a
[00:08:32.040 --> 00:08:36.240]   one year gap in Kaggle I was not doing anything. And then I met
[00:08:36.240 --> 00:08:40.120]   with Nishay and team up with one of the competition in melanoma
[00:08:40.120 --> 00:08:43.640]   competition I met with Nishay and some of other Kagglers and
[00:08:43.640 --> 00:08:47.480]   teamed up with them and tried to do good in that competition.
[00:08:47.480 --> 00:08:50.760]   And what happened that after that, I initially made a very
[00:08:50.760 --> 00:08:55.920]   good friendship. He is a student but he our basically our
[00:08:55.920 --> 00:08:58.880]   training is great. I mean, we understand each other properly
[00:08:58.880 --> 00:09:03.280]   and we are like colleagues and friends. So I used to discuss
[00:09:03.280 --> 00:09:07.320]   with him a lot of things related to ML and Kaggle. And he told me
[00:09:07.320 --> 00:09:10.480]   let's join another competition I joined with him and this journey
[00:09:10.480 --> 00:09:14.920]   started like anything else. I mean, we were competing in two
[00:09:14.920 --> 00:09:18.040]   three competitions together. Some of them were really good.
[00:09:18.040 --> 00:09:21.760]   Then he introduced me some other Kagglers for example, Anul is
[00:09:21.760 --> 00:09:26.480]   one of the good Kagglers and we participated with him and Rajneesh
[00:09:26.480 --> 00:09:30.360]   is one of the Kagglers in Kaggle. BMS competition we won
[00:09:30.360 --> 00:09:33.720]   gold medal. And always my philosophy was winning is a
[00:09:33.720 --> 00:09:38.320]   habit unfortunately so is losing. So when we got gold
[00:09:38.320 --> 00:09:41.320]   medal in BMS we were sure we will do good in another
[00:09:41.320 --> 00:09:45.440]   competition which was my after BMS my second competition was
[00:09:45.440 --> 00:09:49.240]   this COVID-19 competition where I had a personal feeling that I
[00:09:49.240 --> 00:09:52.720]   wanted to do good because of some unfortunate things. So I
[00:09:52.720 --> 00:09:55.880]   participated with Nish and Nish told that he wanted to win a
[00:09:55.880 --> 00:10:00.120]   student prize in this competition and we really worked
[00:10:00.120 --> 00:10:04.320]   very hard every day five to six hours we both and then Aishwarya
[00:10:04.320 --> 00:10:08.960]   one joined and we did a lot of things experiments together and
[00:10:08.960 --> 00:10:12.000]   this is how we got to fifth position. So this journey was
[00:10:12.000 --> 00:10:15.640]   like continuously put putting in this and learning from the
[00:10:15.640 --> 00:10:19.360]   forum itself. Yeah, that was all my journey. But currently I've
[00:10:19.360 --> 00:10:22.760]   left Microsoft and I have started my own venture. So I am
[00:10:22.760 --> 00:10:26.320]   a co founder in Ninja Salty, which is a fintech platform, we
[00:10:26.320 --> 00:10:29.320]   can talk about it somewhere later. But yeah, so I have
[00:10:29.320 --> 00:10:35.840]   started my own own venture and pursue entrepreneurship. Yeah.
[00:10:35.840 --> 00:10:40.840]   So your colleagues introduced you to all of these things. They
[00:10:40.840 --> 00:10:43.720]   also mentioned Bitcoin. Did you get into crypto at the right
[00:10:43.720 --> 00:10:46.280]   time? Did you make any 100x return? Or no?
[00:10:46.280 --> 00:10:51.200]   No, no, no. I thought it's better to win a Kaggle prize.
[00:10:51.200 --> 00:10:55.200]   I mean, you also found gold there. So that's that's
[00:10:55.200 --> 00:11:00.160]   incredible. Yeah. Yeah. Yeah. So question to all of you and
[00:11:00.160 --> 00:11:02.400]   maybe you can start with Ayushman. There's this tipping
[00:11:02.400 --> 00:11:05.480]   point. I know I've read about all of your journey. All of you
[00:11:05.480 --> 00:11:08.840]   been through online courses. And you're done a bunch of online
[00:11:08.840 --> 00:11:14.160]   courses as the same for both of you. When did you decide to sign
[00:11:14.160 --> 00:11:17.560]   up for Kaggle? And how much of a difference of learning was how
[00:11:17.560 --> 00:11:21.120]   were the first few competitions like and how did you go about
[00:11:21.120 --> 00:11:21.800]   learning there?
[00:11:22.680 --> 00:11:27.200]   Yeah, I actually I learned competition. Yes. So yeah, so I
[00:11:27.200 --> 00:11:30.920]   learned Kaggle about Kaggle from my brother actually. And I was
[00:11:30.920 --> 00:11:34.040]   introduced to Kaggle around I think in my second year of
[00:11:34.040 --> 00:11:38.120]   college. I signed up but I wasn't active there. Like the
[00:11:38.120 --> 00:11:42.200]   commitment was not there. So I was just thinking what should I
[00:11:42.200 --> 00:11:46.320]   do? Like should I go to data science? Should I go towards web
[00:11:46.320 --> 00:11:49.280]   development? Should I go towards game development? I had a bunch
[00:11:49.280 --> 00:11:55.680]   of interests. So then came pandemic. And at that time, I
[00:11:55.680 --> 00:12:01.200]   think we got access our college got access to Coursera. So I
[00:12:01.200 --> 00:12:04.800]   thought, let's give Andrew and his legendary machine learning
[00:12:04.800 --> 00:12:08.600]   course a try. And from that onwards, I got hooked to machine
[00:12:08.600 --> 00:12:12.720]   learning. So little by little, I started spending time on Kaggle.
[00:12:12.720 --> 00:12:17.320]   I joined a few competitions, but unfortunately, I didn't have the
[00:12:17.320 --> 00:12:21.440]   commitment to see them through. So then finally came this
[00:12:21.440 --> 00:12:24.480]   current competition, SimCovid. And this competition, I saw
[00:12:24.480 --> 00:12:27.840]   there was a special student prize. And I decided that, yeah,
[00:12:27.840 --> 00:12:31.960]   if I wanted to do something like this was the competition. So I
[00:12:31.960 --> 00:12:35.000]   started working hard from it. I joined the competition quite
[00:12:35.000 --> 00:12:40.560]   early. I put around six, seven hours almost every day into the
[00:12:40.560 --> 00:12:45.320]   competition. So I kept working. And one day I got an invitation
[00:12:45.320 --> 00:12:49.840]   from Nisha and Shiva. And the rest is history. Like you can
[00:12:49.840 --> 00:12:50.360]   see,
[00:12:50.360 --> 00:12:53.960]   that that also gives us an insight of how much hard work
[00:12:53.960 --> 00:12:57.240]   went into it. We will dive into more of that as we understand
[00:12:57.240 --> 00:12:59.400]   the solution. But Nisha, you started your journey, I think
[00:12:59.400 --> 00:13:01.520]   about one and a half years ago.
[00:13:01.520 --> 00:13:07.760]   I would say like, yeah, so my journey started March 2020,
[00:13:07.760 --> 00:13:11.640]   which is last year when COVID struck. So to be honest, I
[00:13:11.640 --> 00:13:15.760]   started with Kaggle first before having any, any, any knowledge
[00:13:15.760 --> 00:13:19.040]   about machine learning or data science. So I started with
[00:13:19.040 --> 00:13:22.320]   Kaggle mini courses and all with have a basic knowledge of
[00:13:22.320 --> 00:13:29.280]   Python. And then I went to do some micro courses on Coursera
[00:13:29.280 --> 00:13:35.600]   as well as edX and other stuff. So then I started first with
[00:13:35.600 --> 00:13:41.320]   Titanic problem, I guess. Yeah. So it was like I do something
[00:13:41.360 --> 00:13:44.640]   submissions on Titanic, Titanic, which is a tutorial
[00:13:44.640 --> 00:13:48.920]   competition. And I usually practice those stuff, which I
[00:13:48.920 --> 00:13:51.640]   learned from the courses, like I learned a new machine learning
[00:13:51.640 --> 00:13:54.840]   model, I first tried to implement it on Kaggle on the
[00:13:54.840 --> 00:13:58.080]   Titanic data set. And I made a submission. So that way I
[00:13:58.080 --> 00:14:02.000]   started learning machine learning algorithms. And then
[00:14:02.000 --> 00:14:05.840]   yeah, like I always continue to do courses alongside the Kaggle
[00:14:05.840 --> 00:14:10.480]   competitions. And before landing upon the future competitions
[00:14:10.480 --> 00:14:14.320]   which are happening, like now, I used to participate in past
[00:14:14.320 --> 00:14:17.800]   competitions for so I read their top solutions and try to
[00:14:17.800 --> 00:14:23.000]   implement them myself. So yeah, like I did tutorial competitions
[00:14:23.000 --> 00:14:25.960]   at first, which gave me a good insight about what exactly
[00:14:25.960 --> 00:14:30.120]   happens on Kaggle leaderboard and other stuff. So yeah, I got
[00:14:30.120 --> 00:14:33.680]   to know some good tricks about Kaggle competitions like
[00:14:33.680 --> 00:14:37.400]   Ensembl and post processing, which is I would say like, I'm
[00:14:37.440 --> 00:14:42.160]   mastering that, rather than more training and that stuff. So
[00:14:42.160 --> 00:14:42.480]   yeah.
[00:14:42.480 --> 00:14:46.640]   So how did you pick these competitions as you were
[00:14:46.640 --> 00:14:48.760]   progressing in your journey? And how did you decide which
[00:14:48.760 --> 00:14:51.520]   solutions to replicate? Because that's one question that
[00:14:51.520 --> 00:14:54.080]   everyone is curious about all the time. How should they go
[00:14:54.080 --> 00:14:54.680]   about that?
[00:14:54.680 --> 00:14:58.680]   Yeah, so like I told earlier, like I started with tutorial
[00:14:58.680 --> 00:15:01.960]   competitions, which include house price prediction, advanced
[00:15:01.960 --> 00:15:05.280]   house price prediction and Titanic problem. Then I went to
[00:15:05.280 --> 00:15:08.160]   do some tabular competitions, which happened in the past, like
[00:15:08.160 --> 00:15:11.960]   one was PUBG competition, which I remember, so it was quite
[00:15:11.960 --> 00:15:16.280]   familiar with me. I had a lot of future engineering from them.
[00:15:16.280 --> 00:15:21.080]   Like how could how could kills define correlated with the
[00:15:21.080 --> 00:15:25.560]   target and all that. So it, it made me more addicted to
[00:15:25.560 --> 00:15:29.160]   Kaggle. So firstly, I started with tabular competitions to do
[00:15:29.160 --> 00:15:33.160]   hardware limitation work. So I did basic tabular competitions,
[00:15:33.160 --> 00:15:36.680]   which were currently going on like trends, neuroimaging
[00:15:36.680 --> 00:15:41.760]   competition, which was the then, with some experience, I started
[00:15:41.760 --> 00:15:44.680]   with melanoma in melanoma, it also it was an image
[00:15:44.680 --> 00:15:48.320]   classification problem, but I was more into stacking and all
[00:15:48.320 --> 00:15:53.560]   that stuff, as it also had some tabular data. Yeah, then I got a
[00:15:53.560 --> 00:15:58.920]   good grasp of competitions. And also I teamed up with Sivam
[00:15:58.920 --> 00:16:03.120]   before that. So I moved on to mechanism of action competition.
[00:16:03.280 --> 00:16:07.880]   Which happened in August last year, I guess. So in that
[00:16:07.880 --> 00:16:10.440]   competition, I like put up everything which I would
[00:16:10.440 --> 00:16:15.480]   spending 10 of 10 hours daily, like it was really hectic. So
[00:16:15.480 --> 00:16:19.760]   fortunately, I won that competition, placing first of
[00:16:19.760 --> 00:16:25.400]   like 4000 plus teams, I got good teammates, we had a like, very
[00:16:25.400 --> 00:16:29.640]   pleasant time working together. I learned a lot of stuff. I met
[00:16:29.640 --> 00:16:35.080]   with few great masters, I guess, Mark Peng and all, then Tano
[00:16:35.080 --> 00:16:38.400]   approached me for further competitions. I did some tabular
[00:16:38.400 --> 00:16:43.080]   competitions with him. Yeah, then Sivam came back in Kaggle.
[00:16:43.080 --> 00:16:47.320]   So we didn't have much hardware problems and all because he had
[00:16:47.320 --> 00:16:52.120]   GPU. So we started with CV competitions, like I started CV
[00:16:52.120 --> 00:16:56.160]   competitions quite later. Firstly, I tried to master tabular
[00:16:56.160 --> 00:16:59.600]   competitions and NLP competitions. Then I moved on to
[00:16:59.600 --> 00:17:02.200]   CV competitions. And now currently I'm doing object
[00:17:02.200 --> 00:17:02.880]   detection and all.
[00:17:02.880 --> 00:17:08.720]   Yeah, shout out to Tanul and Abhishek. We have two
[00:17:08.720 --> 00:17:12.640]   grandmasters one for exam. Also Tanul who's also a Kaggle
[00:17:12.640 --> 00:17:15.840]   Grandmaster in the chat. Thanks. Thanks for joining guys. But I'm
[00:17:15.840 --> 00:17:20.480]   curious, Nisheshu, you, you made it up to the global rankings.
[00:17:20.480 --> 00:17:24.440]   Was that a conscious effort? And how much effort went into that?
[00:17:24.440 --> 00:17:27.080]   Were you were you aiming for that? And when did that goal
[00:17:27.080 --> 00:17:29.160]   become visible if at all?
[00:17:29.160 --> 00:17:33.720]   I would say like, I created a sort of bucket list that I have
[00:17:33.720 --> 00:17:37.800]   to somehow reach master tag till the December and all that. So,
[00:17:37.800 --> 00:17:42.640]   like, firstly, I was I was more into solo doing solo
[00:17:42.640 --> 00:17:46.640]   competitions. But then once I realized it is a really great
[00:17:46.640 --> 00:17:50.320]   experience, especially learning great experience for me in a
[00:17:50.320 --> 00:17:54.320]   team. So I started doing competitions as a team. And
[00:17:54.320 --> 00:17:58.440]   thereafter, like, it has been exponential growth for me, like
[00:17:58.440 --> 00:18:03.080]   I would say, like before, when I when am I I directly jumped from
[00:18:03.080 --> 00:18:09.280]   500 rank globally to some, like 58 something. So yeah, it gave
[00:18:09.280 --> 00:18:11.200]   us a good amount of jump to me.
[00:18:11.200 --> 00:18:15.120]   That's incredible. Thanks. Thanks for sharing your journey.
[00:18:15.120 --> 00:18:17.560]   Shivam, when did you get addicted to Kaggle? You
[00:18:17.560 --> 00:18:20.720]   mentioned you got addicted? And how did you go about learning
[00:18:20.720 --> 00:18:22.280]   there? I want to hear what you did.
[00:18:22.280 --> 00:18:29.360]   Yeah, so basically, I got addicted with Kaggle when Nisha
[00:18:29.360 --> 00:18:34.840]   approached me and we did one competition. So I saw that these
[00:18:34.840 --> 00:18:39.080]   teammates and great and they work hard in the competition,
[00:18:39.080 --> 00:18:43.560]   right. So then I also thought like, I will put much better
[00:18:43.560 --> 00:18:47.400]   efforts from my side and then we can win a competition. So I
[00:18:47.400 --> 00:18:53.000]   always aim higher. So then those teammates I had, and I always
[00:18:53.000 --> 00:18:56.440]   knew that they will team up with me and they be used to whenever
[00:18:56.440 --> 00:18:58.880]   a competition is going to finish, they already are
[00:18:58.880 --> 00:19:01.640]   discussing with me that's how we will approach another
[00:19:01.640 --> 00:19:03.960]   competition, which is already in the Kaggle forum.
[00:19:03.960 --> 00:19:04.640]   You don't trust.
[00:19:04.640 --> 00:19:10.320]   Yeah, yeah. So we didn't take a break because we already knew
[00:19:10.320 --> 00:19:13.880]   that competitions are there, these teammates are great and we
[00:19:13.920 --> 00:19:17.480]   all can do it. So always there was feeling inside me that we
[00:19:17.480 --> 00:19:21.880]   can do it. For example, before BMS, we were basically if you
[00:19:21.880 --> 00:19:26.400]   see our competitions, we were placed under 20. In two
[00:19:26.400 --> 00:19:29.040]   competitions, we were placed under 20, but we never reached
[00:19:29.040 --> 00:19:32.760]   to the gold point, right. But I always believe if we will put
[00:19:32.760 --> 00:19:36.160]   more effort, then we can definitely win a gold medal or
[00:19:36.160 --> 00:19:39.680]   in fact prize money also. So always there was something
[00:19:39.680 --> 00:19:44.200]   inside me that I want to win a Kaggle competition. And that's
[00:19:44.200 --> 00:19:48.120]   how you can see and go down, you will see that I finished 20 to
[00:19:48.120 --> 00:19:52.520]   26, right, 15 even. And then what happened that I thought,
[00:19:52.520 --> 00:19:56.960]   yeah, if I put much more effort, then as a team, we can win the
[00:19:56.960 --> 00:20:00.880]   competition. And then definitely this happened to me. And it was
[00:20:00.880 --> 00:20:04.360]   all about motivation that if I put more effort, I can win. And
[00:20:04.360 --> 00:20:09.120]   that, that led me to addiction of Kaggle and let me do this,
[00:20:09.120 --> 00:20:13.280]   let me do that. And this was the main thing. And about
[00:20:13.280 --> 00:20:17.280]   approaching Kaggle competition. So how we do it is like we see
[00:20:17.280 --> 00:20:20.040]   that whether in the Kaggle competition, how much is the
[00:20:20.040 --> 00:20:24.040]   data size, and we see whether we have expertise in that or not.
[00:20:24.040 --> 00:20:27.840]   For example, we see we have done a CV competition. So we will be
[00:20:27.840 --> 00:20:30.360]   focusing on another CV competition because we have
[00:20:30.360 --> 00:20:33.200]   recently got a good CV competition ending, finish,
[00:20:33.200 --> 00:20:37.240]   finish. And we see that we can apply those solutions in some
[00:20:37.240 --> 00:20:41.240]   other competition which is in CV. So similarly, we choose that
[00:20:41.240 --> 00:20:45.080]   competition. Also, we see that whether the competition data and
[00:20:45.080 --> 00:20:50.520]   timeline is matching with our, our, our plan or not, because we
[00:20:50.520 --> 00:20:53.840]   have some of our own plans, like we will approach this competition
[00:20:53.840 --> 00:20:57.000]   than this competition. So we will have that okay, this
[00:20:57.000 --> 00:20:59.360]   timeline is matching with other competitions. So we can put a
[00:20:59.360 --> 00:21:02.040]   part in this and then we can jump to another competition. So
[00:21:02.040 --> 00:21:04.720]   similarly, we approach the competitions and always we see
[00:21:04.720 --> 00:21:08.400]   that whether JP have a good baseline or not, but the data is
[00:21:08.400 --> 00:21:11.320]   perfect in that competition, whether it is messy or not, for
[00:21:11.320 --> 00:21:15.000]   example, in SimCovid starting the test data was really messy
[00:21:15.000 --> 00:21:19.160]   and they created a correct data. So then we joined COVID
[00:21:19.160 --> 00:21:22.600]   competition not earlier. So these things we make sure before
[00:21:22.600 --> 00:21:25.720]   we join the competition so that in later point, this doesn't
[00:21:25.720 --> 00:21:27.880]   happen that we all the efforts are going into this.
[00:21:27.880 --> 00:21:34.400]   Any, any tips on teaming up in a competition people on
[00:21:34.400 --> 00:21:37.400]   every single looking for a team thread, they're always asking
[00:21:37.400 --> 00:21:40.880]   how do I team up? How do I find the teammates? You've had great
[00:21:40.880 --> 00:21:45.200]   success in your teams, Shivam and Nisheh, any thoughts on what
[00:21:45.200 --> 00:21:47.600]   makes great teammates? How do you team up with people?
[00:21:47.600 --> 00:21:53.480]   So I mean, it's a bit sometimes sometimes it's a it's it's luck
[00:21:53.480 --> 00:21:57.520]   also. And sometimes when you participate with somebody by
[00:21:57.520 --> 00:22:01.280]   asking in the forum and you do one competition, you realize
[00:22:01.280 --> 00:22:04.040]   that this person is good, right? And he's continuously putting a
[00:22:04.040 --> 00:22:07.560]   person tackle. That is one way. Another way is for example,
[00:22:07.560 --> 00:22:11.680]   Iceman. So let me tell you how I got Iceman. So he put he has
[00:22:11.680 --> 00:22:14.880]   already put a lot of effort in COVID competition, he was above
[00:22:14.880 --> 00:22:19.480]   us in the ranking. And then we realized that because we never
[00:22:19.480 --> 00:22:22.800]   knew Iceman, if you will see his profile, you will realize that
[00:22:22.800 --> 00:22:26.000]   Iceman's other competitions were not that good. And we thought
[00:22:26.000 --> 00:22:30.000]   why he has not done anything good in bagel competition. And
[00:22:30.000 --> 00:22:32.920]   he's now in top in this competition, we have a lot of
[00:22:32.920 --> 00:22:36.640]   effort, but we are not in top. So then I approached him, I saw
[00:22:36.640 --> 00:22:39.920]   his GitHub profile, I saw his basically LinkedIn profile, and
[00:22:39.920 --> 00:22:43.640]   I realized that he's a good coder, and he's into machine
[00:22:43.640 --> 00:22:46.720]   learning. Maybe he has not done good in the earlier competition.
[00:22:46.720 --> 00:22:50.960]   I asked him, he approached me and he told me yes, we can join.
[00:22:50.960 --> 00:22:54.840]   And when he joined with me, I realized that he is a very good
[00:22:54.840 --> 00:23:00.040]   ML engineer. In fact, his coding skills are great. And I also
[00:23:00.040 --> 00:23:02.840]   realized that in the previous competition, his ranking was not
[00:23:02.840 --> 00:23:05.600]   good, because he has not put effort in that because he was
[00:23:05.600 --> 00:23:08.200]   busy with some other things. And he was only joining the
[00:23:08.200 --> 00:23:11.640]   competition and leaving it just like that. So we realized that
[00:23:11.640 --> 00:23:15.160]   yeah, this happened with people. So the approaching thing will
[00:23:15.160 --> 00:23:19.240]   be you have to put some effort in the competition, you have to
[00:23:19.240 --> 00:23:23.080]   be in a decent rank, then you will approach anybody who is
[00:23:23.080 --> 00:23:26.480]   maybe in your range in the ranking system, or you can
[00:23:26.480 --> 00:23:30.160]   approach them by telling them that you are currently expert in
[00:23:30.160 --> 00:23:32.920]   this these areas, and you are putting a lot of effort and you
[00:23:32.920 --> 00:23:36.800]   can do it with teaming up with them, then the other people will
[00:23:36.800 --> 00:23:39.520]   also think that you have already put some effort so they can
[00:23:39.520 --> 00:23:42.360]   join with you. Without putting effort if somebody approaches
[00:23:42.360 --> 00:23:45.000]   you, you also don't know whether he will put effort or not.
[00:23:45.000 --> 00:23:48.680]   Because we have seen in fact, I'm not disclosing a name, but
[00:23:48.680 --> 00:23:52.000]   we have seen that some people join with you in the team, and
[00:23:52.000 --> 00:23:55.520]   then they don't work at all, right? And they get three medals.
[00:23:55.520 --> 00:23:59.640]   And so just because to avoid that you have to put effort and
[00:23:59.640 --> 00:24:02.400]   then raise the request for teaming up and people will
[00:24:02.400 --> 00:24:04.920]   definitely team up with you. It's not like they will not
[00:24:04.920 --> 00:24:05.680]   team up. Yeah.
[00:24:05.680 --> 00:24:10.560]   If I may share my journey, it's also an honest conversation with
[00:24:10.560 --> 00:24:13.000]   the teammates. I told my teammates, hey, I'm a noob on
[00:24:13.000 --> 00:24:16.200]   Kaggle and I would love to learn from you. They were like, sure.
[00:24:16.200 --> 00:24:21.640]   And although it was more me learning from them, we did get
[00:24:21.640 --> 00:24:23.960]   to team up and got great results. All of my Kaggle
[00:24:23.960 --> 00:24:27.320]   results are just thanks to my teammates. But it's also about
[00:24:27.320 --> 00:24:30.280]   being honest that I am not going to lead the team. It's more
[00:24:30.280 --> 00:24:32.000]   about me learning from you sometimes.
[00:24:32.000 --> 00:24:37.000]   Yeah, that can also happen. I mean, in fact, many people I got
[00:24:37.000 --> 00:24:39.240]   in LinkedIn, many people approaching me and telling me
[00:24:39.240 --> 00:24:42.360]   can we participate in this I don't have. So I tell them you
[00:24:42.360 --> 00:24:47.200]   can approach by this way, which I exactly told you right now. So
[00:24:47.200 --> 00:24:50.360]   do good in the competition, or at least start competing before
[00:24:50.360 --> 00:24:54.360]   even putting a submission you ask to join. I mean, quite weird,
[00:24:54.360 --> 00:24:57.280]   but you have to put some effort at least before asking for
[00:24:57.400 --> 00:25:01.520]   merging. And people, you will get to know people slowly in the
[00:25:01.520 --> 00:25:01.960]   forum.
[00:25:01.960 --> 00:25:05.760]   Absolutely. I should also mention, Ayushman joined me on a
[00:25:05.760 --> 00:25:08.480]   live stream and I was looking at his code. I was quite embarrassed
[00:25:08.480 --> 00:25:11.840]   of the stuff I do. He had this really nice structure to the
[00:25:11.840 --> 00:25:14.920]   competition code. And that was incredible to see. Nishty I saw
[00:25:14.920 --> 00:25:16.600]   you unmute yourself. Do you have anything to add?
[00:25:16.600 --> 00:25:17.760]   Yeah, I am.
[00:25:17.760 --> 00:25:20.640]   Please go ahead.
[00:25:20.640 --> 00:25:23.280]   Yeah, what did you ask?
[00:25:24.040 --> 00:25:27.120]   I just asked about your experience with teaming up and
[00:25:27.120 --> 00:25:28.240]   any strategies that
[00:25:28.240 --> 00:25:31.800]   I have been a great experience as yeah, Sivam also told like,
[00:25:31.800 --> 00:25:35.840]   if you would put effort in a specific competition and be
[00:25:35.840 --> 00:25:40.000]   engaged in a discussion forum, or telling people publishing
[00:25:40.000 --> 00:25:44.320]   other notebooks publicly like ED notebooks and all. So people
[00:25:44.320 --> 00:25:49.080]   will always be attracted to you like to team up. So it won't be
[00:25:49.080 --> 00:25:52.360]   difficult for you to team up like that. Like many people also
[00:25:52.360 --> 00:25:55.000]   approached me on LinkedIn for teaming up in next competition.
[00:25:55.000 --> 00:25:58.440]   I always used to say like, I would love to do that if you are
[00:25:58.440 --> 00:26:02.640]   also also competing in the same competition as of mine, and you
[00:26:02.640 --> 00:26:05.680]   are doing well. So I would love to team up with you. Yeah.
[00:26:05.680 --> 00:26:10.120]   Also about the COVID competition. Yeah, it has been
[00:26:10.120 --> 00:26:14.040]   a great like, I always loved teaming up with Sivam. We have
[00:26:14.040 --> 00:26:18.760]   a great chat like we used to meet daily for hard and to do
[00:26:18.800 --> 00:26:22.840]   coding stuff together, train models together. And yeah, then
[00:26:22.840 --> 00:26:26.240]   I was one also joined. So it was a great experience for me as I
[00:26:26.240 --> 00:26:29.960]   also had a good support from another student. We both are of
[00:26:29.960 --> 00:26:32.600]   same age group. So yeah, it was quite fun for me.
[00:26:32.600 --> 00:26:37.520]   Awesome. I have I think this would be my last question before
[00:26:37.520 --> 00:26:40.960]   we dive into a solution. But how has Kaggle helped you in your
[00:26:40.960 --> 00:26:43.760]   journey outside? Aishwarya, after you finished on this
[00:26:43.760 --> 00:26:47.560]   competition, did you have any, any recruiters or any people
[00:26:47.560 --> 00:26:49.760]   approach you from this?
[00:26:49.760 --> 00:26:55.840]   Not exactly like, since I guess it's because since it's my first
[00:26:55.840 --> 00:26:59.440]   win, like, people are waiting, like I still have to show them
[00:26:59.440 --> 00:27:04.600]   something. But yeah, like, it actually helped me a lot. Like I
[00:27:04.600 --> 00:27:08.440]   am now quite motivated to do well in future competitions. So
[00:27:08.440 --> 00:27:11.280]   currently, I'm doing another competition. Let's see where
[00:27:11.280 --> 00:27:17.120]   that goes. But yeah, like, actually, since now I have the
[00:27:17.120 --> 00:27:22.440]   results. So it's like, I can believe that if I put some
[00:27:22.440 --> 00:27:26.600]   effort, I can do well in other competitions too. And the only
[00:27:26.600 --> 00:27:29.760]   thing that I was lacking before from my side would be the
[00:27:29.760 --> 00:27:33.520]   motivation because I would like in previous competitions, I
[00:27:33.520 --> 00:27:37.520]   would just like enter, do one or two submissions, then just go
[00:27:37.520 --> 00:27:41.480]   out. But ever since I have teamed up with Nisha and Shivam
[00:27:41.480 --> 00:27:44.640]   and winning, getting fifth place in this competition, like the
[00:27:44.640 --> 00:27:48.440]   motivation is there. So yeah, I'll continue to be there in
[00:27:48.440 --> 00:27:48.880]   Kaggle.
[00:27:48.880 --> 00:27:52.560]   That's great. Mr. Deep.
[00:27:52.560 --> 00:27:59.200]   Please. Some of the startups has approached me for like, part
[00:27:59.200 --> 00:28:03.560]   time jobs or internship. But like, I used to say like, I have
[00:28:03.560 --> 00:28:06.800]   currently I have a currently a goal, which I have set like to
[00:28:06.800 --> 00:28:09.800]   become a Kaggle competition's grand master and to publish some
[00:28:10.120 --> 00:28:15.280]   research papers and all that stuff. So like, it's not like I
[00:28:15.280 --> 00:28:18.360]   don't need a job right now or something. I am just here to
[00:28:18.360 --> 00:28:21.960]   learn and do Kaggle competitions and gain some more knowledge
[00:28:21.960 --> 00:28:24.880]   about data science and all. I'm not in a hurry of all that
[00:28:24.880 --> 00:28:29.360]   stuff. Also to mention I have joined as a global ambassador in
[00:28:29.360 --> 00:28:33.880]   a cloud service based startup, which is Jarvis Labs.ai. They
[00:28:33.880 --> 00:28:38.960]   helped us to provide some GPU hardware GPU in last competition
[00:28:39.040 --> 00:28:43.000]   in which we were able to manage to get 15th place in like just
[00:28:43.000 --> 00:28:47.400]   five or six days. So it was quite a good experience for me.
[00:28:47.400 --> 00:28:50.920]   That's that's great to hear that you have all of these goals.
[00:28:50.920 --> 00:28:54.120]   What I was trying to point out here as students and I also
[00:28:54.120 --> 00:28:56.800]   learned this, when you get a great result, and if you share
[00:28:56.800 --> 00:29:00.520]   it nicely, you don't need to go through all of the job
[00:29:00.520 --> 00:29:04.240]   application board people who respect Kaggle. They'll come to
[00:29:04.240 --> 00:29:06.680]   you, you probably will even end up skipping the interview
[00:29:06.680 --> 00:29:10.960]   process somewhere. Shivam, what was it? Did you share a similar
[00:29:10.960 --> 00:29:15.120]   experience in your transition to the industry? And did Kaggle
[00:29:15.120 --> 00:29:18.640]   help you in your currently bootstrap startup?
[00:29:18.640 --> 00:29:25.080]   So Kaggle, so mostly I saw indirect effects from my Kaggle.
[00:29:25.080 --> 00:29:30.040]   So the most important thing was I get shortlisted easily by
[00:29:30.040 --> 00:29:34.080]   putting my Kaggle experience in my resume. So shortlisting
[00:29:34.080 --> 00:29:38.240]   happens. And even what happens that in some of the competition
[00:29:38.240 --> 00:29:42.160]   which I've done great, I used to discuss that in interview and
[00:29:42.160 --> 00:29:46.120]   walk them through the process I have learned in Kaggle. So that
[00:29:46.120 --> 00:29:49.560]   helps a lot in interview process also. So somehow this helps
[00:29:49.560 --> 00:29:54.560]   really helps in recruitment and hiring. Definitely this helps.
[00:29:54.560 --> 00:29:59.040]   Yeah. So and even in startup actually, so this startup is not
[00:29:59.040 --> 00:30:03.120]   about AI right now, it's in fintech space. So definitely we
[00:30:03.120 --> 00:30:06.400]   will see when we have a we will have data how we can utilize to
[00:30:06.400 --> 00:30:09.080]   build models and use AI on top of that. Yeah.
[00:30:09.080 --> 00:30:14.280]   Awesome. All right. I know you all you all have sorry, you all
[00:30:14.280 --> 00:30:17.960]   have shared your presentation as well. I'll pass it back to you
[00:30:17.960 --> 00:30:20.240]   in a second. But what I'll try to do is try to explain the
[00:30:20.240 --> 00:30:23.400]   problem statement from my side as I understand it. And please
[00:30:23.400 --> 00:30:26.560]   feel free to interrupt or correct me as I will probably
[00:30:26.560 --> 00:30:31.880]   end up messing up. So what I like to usually do is I'd
[00:30:31.880 --> 00:30:34.120]   probably just started the competition page go through
[00:30:34.120 --> 00:30:38.160]   there. One of my mistakes is never understanding the problem
[00:30:38.160 --> 00:30:41.160]   statement correctly. So what I understood here is you have all
[00:30:41.160 --> 00:30:47.640]   of these patient scans from which you need to select a few
[00:30:47.640 --> 00:30:51.040]   and let me pull up a kernel where I can point out but there
[00:30:51.040 --> 00:30:54.080]   are different classes, pneumonia, no pneumonia and
[00:30:54.080 --> 00:30:59.320]   COVID. We're trying to predict that it's essentially a object
[00:30:59.320 --> 00:31:01.640]   detection problem if I understand that correctly.
[00:31:01.640 --> 00:31:05.920]   Yeah, like,
[00:31:05.920 --> 00:31:11.720]   really good classification. And so classification and object
[00:31:11.720 --> 00:31:14.760]   detection, like, you can split it up into that.
[00:31:14.760 --> 00:31:19.320]   Yeah, I know. That's, that's what you did in your solution as
[00:31:19.320 --> 00:31:23.960]   well. So we have quite a big data set. There are all of these
[00:31:23.960 --> 00:31:27.120]   scans, the and the next thing would, of course, be the
[00:31:27.120 --> 00:31:31.440]   evaluation metric. And this is the mean average precision.
[00:31:31.440 --> 00:31:34.280]   Again, this is the standard for these problems. I won't go into
[00:31:34.280 --> 00:31:36.680]   all of those details. But these are the labels you're trying to
[00:31:36.680 --> 00:31:44.080]   predict. So it's in the test set, the labels may contain all
[00:31:44.080 --> 00:31:47.200]   of these, and you would have these bounding boxes as well.
[00:31:47.200 --> 00:31:50.680]   The other thing I learned, and I'll probably go through a
[00:31:50.680 --> 00:31:53.800]   kernel, this is just me rambling right now. But one thing I
[00:31:53.800 --> 00:31:58.080]   realized with these DICOM files, you have these study IDs, where
[00:31:58.080 --> 00:32:02.120]   you find all of the scans under one folder, that's the structure
[00:32:02.120 --> 00:32:05.440]   of the data. And inside of this, you have all of the files that
[00:32:05.440 --> 00:32:09.880]   contain the actual DICOM files. Am I correct so far? Yeah, yeah.
[00:32:09.880 --> 00:32:10.600]   Yeah.
[00:32:10.600 --> 00:32:16.040]   So I'll come back to your solution. But let me find the
[00:32:16.040 --> 00:32:21.520]   kernel that I have open. And Dada is, as a disclaimer, she's
[00:32:21.560 --> 00:32:24.840]   one of our ambassadors at Bateson Biosys. But her kernels
[00:32:24.840 --> 00:32:28.720]   are always so amazing. So I'll shamelessly use her work to skip
[00:32:28.720 --> 00:32:31.200]   my ADA homework and just go through that.
[00:32:31.200 --> 00:32:33.600]   Same goes for us.
[00:32:33.600 --> 00:32:40.600]   She's also created a really nice dashboard on Bateson Biosys,
[00:32:40.600 --> 00:32:43.280]   which I'll probably default to right after this.
[00:32:43.280 --> 00:32:46.960]   But this kernel is actually one of the best to get started with
[00:32:46.960 --> 00:32:47.760]   this competition.
[00:32:49.520 --> 00:32:53.040]   That's and Dada has awesome stuff all of the time. She has
[00:32:53.040 --> 00:32:56.840]   this nice artistry approach as well. As you can see all of
[00:32:56.840 --> 00:32:59.720]   this colors. She's she's very creative with her kernels.
[00:32:59.720 --> 00:33:00.280]   Right.
[00:33:00.280 --> 00:33:05.280]   So like I said, you have these labels negative for pneumonia,
[00:33:05.280 --> 00:33:08.440]   typical appearance, indeterminate appearance and
[00:33:08.440 --> 00:33:11.920]   atypical appearance. These are just a few options that are
[00:33:11.920 --> 00:33:15.840]   there with COVID. You could have a patient who's not symptomatic,
[00:33:15.840 --> 00:33:18.240]   you could have someone who's symptomatic, we have labels for
[00:33:18.240 --> 00:33:22.600]   that. And you have the corresponding bounding boxes as
[00:33:22.600 --> 00:33:26.960]   well. Like I said, structured inside of the different folders
[00:33:26.960 --> 00:33:29.040]   for every study as they call it.
[00:33:29.040 --> 00:33:34.680]   I'll probably just switch to the dashboard because it's easier
[00:33:34.680 --> 00:33:36.400]   to look at all of the graphs there.
[00:33:36.400 --> 00:33:41.560]   I'm trying to establish the problem statement and tell how
[00:33:41.560 --> 00:33:44.720]   difficult or easy the problem was. So and Dada has plotted all
[00:33:44.720 --> 00:33:48.640]   of these bounding boxes already for us. And if I shift through
[00:33:48.640 --> 00:33:55.880]   the indexes, as you can see, it's different boxes along the
[00:33:55.880 --> 00:34:01.200]   region where probably the disease was annotated by the
[00:34:01.200 --> 00:34:05.440]   experts. There are about if I zoom in a little
[00:34:05.440 --> 00:34:13.400]   2040 images that don't have a box and 4000 that have a box
[00:34:13.440 --> 00:34:19.800]   slide, a date time balance. And from what I see just on this
[00:34:19.800 --> 00:34:27.160]   dashboard, there are some label misbalance as well across
[00:34:27.160 --> 00:34:36.040]   different classes as well. And the most number of classes or
[00:34:36.040 --> 00:34:40.120]   most number of examples are for the typical appearance. Did I
[00:34:40.120 --> 00:34:41.560]   miss anything else in the data?
[00:34:41.560 --> 00:34:46.640]   Yeah, that's correct. To summarize, I would say like it
[00:34:46.640 --> 00:34:50.520]   was a this competition could be divided into two stages. The
[00:34:50.520 --> 00:34:54.080]   first one was object detection in which we have to predict the
[00:34:54.080 --> 00:34:57.120]   bounding boxes location and the confidence score. And the other
[00:34:57.120 --> 00:35:00.960]   one was was multi class classification problem in which
[00:35:00.960 --> 00:35:04.360]   we had to predict the four classes which are given to us.
[00:35:04.360 --> 00:35:08.920]   So as you has already shown that data was quite imbalanced. So it
[00:35:09.000 --> 00:35:16.040]   was it was a task to also tackle with this. So yeah, we would
[00:35:16.040 --> 00:35:20.160]   explain it further in a pre precision start how how we did
[00:35:20.160 --> 00:35:23.520]   stratified cables, K fold and all that stuff to tackle that.
[00:35:23.520 --> 00:35:29.520]   So I'll pass it back to you. But I'm just curious to, I guess
[00:35:29.520 --> 00:35:32.720]   since you were working together of Shiva, Mr. When you saw the
[00:35:32.720 --> 00:35:36.000]   data set, how did you decide to approach it? Like, I'm trying to
[00:35:36.000 --> 00:35:38.960]   understand when an experienced calculator sees a problem
[00:35:38.960 --> 00:35:41.240]   statement, what comes to their mind. So if you remember what
[00:35:41.240 --> 00:35:43.200]   came to your mind when you first saw this problem,
[00:35:43.200 --> 00:35:47.280]   for object detection, I would say as it was our first
[00:35:47.280 --> 00:35:51.160]   competition, so we completely neglected it for the beginning
[00:35:51.160 --> 00:35:54.480]   part, like we would just focus on classification part, and
[00:35:54.480 --> 00:35:58.840]   we'll maybe team up with someone having good knowledge of object
[00:35:58.840 --> 00:36:02.400]   detection, or maybe proceed with it later on in the competition.
[00:36:02.640 --> 00:36:07.000]   So we set a goal in the beginning to maximize our
[00:36:07.000 --> 00:36:10.720]   classification model score. So that will begin with the
[00:36:10.720 --> 00:36:16.440]   classification problem and did all that. As firstly, we also
[00:36:16.440 --> 00:36:20.000]   noticed that the data was quite imbalanced. And also we noticed
[00:36:20.000 --> 00:36:24.680]   that some of the patient has multiple images. So there was
[00:36:24.680 --> 00:36:28.400]   some ways to tackle all that stuff. And we proceeded further
[00:36:28.400 --> 00:36:32.560]   with group plus stratified K fold in which we first grouped
[00:36:32.560 --> 00:36:36.880]   all the patients and based on them be divided our fault so
[00:36:36.880 --> 00:36:42.240]   that we we won't have any sort of leakage in cross validation
[00:36:42.240 --> 00:36:46.160]   score. And then we proceeded with stratified K fold in which
[00:36:46.160 --> 00:36:50.280]   we balanced out the data for each fold. And then yeah, like
[00:36:50.280 --> 00:36:53.120]   we proceeded with the with the baseline model, I created a
[00:36:53.120 --> 00:36:58.240]   baseline for the classification. Then we started experimenting
[00:36:58.280 --> 00:37:02.920]   with those in with a smaller model and a smaller MSI so that
[00:37:02.920 --> 00:37:06.320]   we could experiment with them faster. And yeah, then like we
[00:37:06.320 --> 00:37:09.240]   started proceeding with further experiments and all that stuff.
[00:37:09.240 --> 00:37:13.520]   And I saw from sorry, please, please.
[00:37:13.520 --> 00:37:16.600]   Yeah. So I was saying that mostly that pre processing and
[00:37:16.600 --> 00:37:20.080]   by looking into data, what we can do as a in any condition, in
[00:37:20.080 --> 00:37:23.480]   fact, I will say there are very good Kagglers in discussions
[00:37:23.480 --> 00:37:27.480]   forums, which they share insights about data, initial
[00:37:27.480 --> 00:37:32.600]   data and how you can handle that unbalanced thing or impact how
[00:37:32.600 --> 00:37:38.960]   you can do your split on K fold, right. So as Nisha mentioned,
[00:37:38.960 --> 00:37:43.640]   we use stratified K group fold. So these all things are usually
[00:37:43.640 --> 00:37:47.240]   in any competition when you start, if it has been a week or
[00:37:47.240 --> 00:37:50.200]   two weeks, so people will start saving those things how you can
[00:37:50.200 --> 00:37:54.480]   initially start with pre processing. For example, a great
[00:37:54.480 --> 00:37:58.320]   discussion was there to convert the BTM files to PNG and I
[00:37:58.320 --> 00:38:03.520]   compile to PNG, right. So that is the standard and we beat in
[00:38:03.520 --> 00:38:07.160]   fact, everybody took that and use that. So any in any
[00:38:07.160 --> 00:38:11.400]   competition took those discussion forward and go with
[00:38:11.400 --> 00:38:14.080]   that. And that will help you and save a lot of time.
[00:38:14.080 --> 00:38:19.880]   Yeah, I just want to how did you approach the problem? And what
[00:38:19.880 --> 00:38:23.000]   were your first thoughts when you saw the data set? How did
[00:38:23.000 --> 00:38:24.120]   you go about approaching it?
[00:38:24.120 --> 00:38:29.600]   Yeah, so coincidentally, like the our pre processing was my
[00:38:29.600 --> 00:38:33.240]   pre processing was almost similar to Shivam's and Nisha's.
[00:38:33.240 --> 00:38:37.080]   Like I did the similar thing stratified plus group K fold.
[00:38:37.080 --> 00:38:44.800]   And I like the host also put up that there were some studies
[00:38:44.800 --> 00:38:49.080]   contain multiple images and like not all the images had bounding
[00:38:49.080 --> 00:38:53.960]   box labels. So like the host suggested that you should only
[00:38:53.960 --> 00:38:57.840]   take the images which had the bounding box labels. So I
[00:38:57.840 --> 00:39:02.600]   excluded all the images which did not have bounding box boxes
[00:39:02.600 --> 00:39:06.160]   from a particular study. So like for a particular study, I will
[00:39:06.160 --> 00:39:08.800]   have approximately only one image which contains all the
[00:39:08.800 --> 00:39:13.640]   bounding boxes. So I took those images, then I do stratified K
[00:39:13.640 --> 00:39:18.360]   fold. And yeah, from there I started I started I think with a
[00:39:18.360 --> 00:39:22.720]   pretty dumb baseline. Like I only predicted I think it was
[00:39:22.720 --> 00:39:26.640]   for the typical class which has the highest distribution. Like I
[00:39:26.640 --> 00:39:31.240]   only predicted like for all the class, all my predictions were
[00:39:31.240 --> 00:39:35.760]   like one for the typical class. So I started from that that was
[00:39:35.760 --> 00:39:41.280]   like my, you know, the very dumb baseline. So yeah, I would also
[00:39:41.280 --> 00:39:43.640]   like to point out that at that time, the leaderboard was
[00:39:43.640 --> 00:39:47.960]   broken. So like, even if you had an object detection model, like
[00:39:47.960 --> 00:39:50.800]   you were predicting the bounding boxes, the leaderboard would not
[00:39:50.800 --> 00:39:53.920]   give you the score, like your scores are basically the ones
[00:39:53.920 --> 00:39:58.160]   that you had from the classification part. So then I
[00:39:58.160 --> 00:40:02.320]   thought that when the leaderboard gets fixed, I will
[00:40:02.320 --> 00:40:05.960]   start with the object detection part. In the meantime, I will
[00:40:05.960 --> 00:40:10.200]   start with the classification part. Then I also saw that
[00:40:10.200 --> 00:40:13.800]   Kaggle had previously organized some similar competitions and
[00:40:14.040 --> 00:40:17.800]   other competitions were also organized elsewhere like
[00:40:17.800 --> 00:40:21.520]   Chexpert and all that. So I started going through all of
[00:40:21.520 --> 00:40:24.960]   their solutions. And that helped me a lot. Like in our final
[00:40:24.960 --> 00:40:30.240]   solution, we have we took a lot of ideas from those previous
[00:40:30.240 --> 00:40:34.400]   solutions. Like, yeah, that's how I started.
[00:40:34.400 --> 00:40:40.240]   Awesome. I'll let me quickly summarize what we've understood
[00:40:40.240 --> 00:40:43.200]   so far. This is another dashboard by one of our masters
[00:40:43.200 --> 00:40:46.360]   for cheese. He's also put out a kernel along with this. The
[00:40:46.360 --> 00:40:48.880]   dashboard is just a summary of that and I'll easily summarize
[00:40:48.880 --> 00:40:52.760]   everything. So you have this slight imbalance, like we
[00:40:52.760 --> 00:40:55.440]   quickly said, as you can see, all of the classes have this
[00:40:55.440 --> 00:41:00.840]   imbalance, we learn more how the team dealt with that. And DICOM
[00:41:00.840 --> 00:41:05.800]   images, as I learned have also further details to the image to
[00:41:05.800 --> 00:41:08.360]   the patient. So they have some metadata, that's how you can
[00:41:08.360 --> 00:41:13.520]   understand the patients gender, along with other details. Is it
[00:41:13.520 --> 00:41:18.880]   the chest? See, radiograph? Is it the skull? So on and so
[00:41:18.880 --> 00:41:24.520]   forth. I'll pass it back to you and feel free to share your
[00:41:24.520 --> 00:41:27.600]   screen. I understand you have a presentation where we can
[00:41:27.600 --> 00:41:28.520]   understand the solution.
[00:41:28.520 --> 00:41:34.840]   Yeah, sure. I would like to, you know, share the link to our
[00:41:34.840 --> 00:41:39.320]   solution that we posted in the Kaggle forum. So yeah, let's go
[00:41:39.320 --> 00:41:41.840]   there. Just let me
[00:41:41.840 --> 00:41:45.720]   I'll drop it in the chat.
[00:41:45.720 --> 00:41:53.680]   Yeah. So yeah, this is our final solution. And this is the one
[00:41:53.680 --> 00:41:59.040]   that we posted. So basically, what we did was we approach this
[00:41:59.040 --> 00:42:03.480]   competition as a joint task of classification and object
[00:42:03.480 --> 00:42:08.360]   detection. So like we had six classes to predict. So one were
[00:42:08.360 --> 00:42:12.440]   for the study labels, which were a typical, typical,
[00:42:12.440 --> 00:42:17.680]   indeterminate, and negative, I think. And the other one was
[00:42:17.680 --> 00:42:21.880]   opacity and none. So basically, the structure of the data set
[00:42:21.880 --> 00:42:26.360]   was like, for you had different studies, and each studies had
[00:42:26.360 --> 00:42:30.400]   different images. So your study became your study level
[00:42:30.400 --> 00:42:33.760]   predictions. And the images became your image level
[00:42:33.760 --> 00:42:38.480]   predictions. So at the image level, you had to predict a
[00:42:38.480 --> 00:42:43.040]   bounding box also along with the class. And for study level
[00:42:43.040 --> 00:42:47.240]   predictions, you did not have to, you only had to predict the
[00:42:47.240 --> 00:42:48.320]   class problem.
[00:42:48.320 --> 00:42:52.400]   So there was just the classification problem. And for
[00:42:52.400 --> 00:42:54.720]   for study, it says classification and for others,
[00:42:54.720 --> 00:42:56.080]   it's object detection.
[00:42:56.320 --> 00:43:01.240]   Yeah, you can break it up like that. So we did exactly like
[00:43:01.240 --> 00:43:05.520]   that. We created two pipelines. One was for one was image
[00:43:05.520 --> 00:43:08.880]   classification, that was for the four classes present in the
[00:43:08.880 --> 00:43:12.960]   study. And one was an object detection pipeline, which was
[00:43:12.960 --> 00:43:15.800]   for the opacity predictions, the opacity predictions were
[00:43:15.800 --> 00:43:20.120]   actually the predictions which had a bounding box. And this was
[00:43:20.120 --> 00:43:23.080]   the image level predictions. And I would also like to add that
[00:43:23.080 --> 00:43:26.800]   the image level predictions had another one, another class,
[00:43:26.800 --> 00:43:29.960]   which was none. So like if your image did not contain a
[00:43:29.960 --> 00:43:33.320]   bounding box, you had to predict you had to give the result.
[00:43:33.320 --> 00:43:39.240]   None, like it did not have a bounding box. So it was like
[00:43:39.240 --> 00:43:45.320]   that. So basically, in our pre processing, Shiv, I would like
[00:43:45.320 --> 00:43:48.440]   to pass it over to Shivam. He will go through our pre
[00:43:48.440 --> 00:43:49.760]   processing steps.
[00:43:52.320 --> 00:43:55.160]   Sorry, Shivam, just just before that, Aishwan, when you said
[00:43:55.160 --> 00:43:57.520]   it's different pipelines, so these are just two different
[00:43:57.520 --> 00:44:00.360]   models that you ensemble the results from?
[00:44:00.360 --> 00:44:03.520]   Or yeah, yeah. So we had different models for
[00:44:03.520 --> 00:44:06.880]   classification and different models for object detection. But
[00:44:06.880 --> 00:44:11.800]   at the end, you will learn that we did a trick like to, you
[00:44:11.800 --> 00:44:14.480]   know, merge the predictions from both the object detection and
[00:44:14.480 --> 00:44:17.080]   the classification parts. We did a trick. So we'll go through
[00:44:17.080 --> 00:44:18.720]   that towards the end.
[00:44:18.720 --> 00:44:21.680]   Yeah, I'm here to learn all the secrets. I look forward to
[00:44:21.680 --> 00:44:21.960]   that.
[00:44:22.720 --> 00:44:24.240]   Yeah, this year we'll go over to you.
[00:44:24.240 --> 00:44:30.680]   Yeah. So about pre processing, as we discussed earlier, also,
[00:44:30.680 --> 00:44:35.120]   we converted the DICOM file to PNG, right? And then and then
[00:44:35.120 --> 00:44:40.920]   we also resized images to 1024 size by keeping aspects ratio
[00:44:40.920 --> 00:44:44.520]   same. As in the discussion forum, it was already mentioned
[00:44:44.520 --> 00:44:47.400]   that we have to remove duplicates because the data was
[00:44:47.440 --> 00:44:52.120]   having duplicate images. And also, if you will see that we
[00:44:52.120 --> 00:44:55.480]   have the given bounding box label, so we can extract the
[00:44:55.480 --> 00:44:58.360]   bounding boxes, right? So from the image, we can extract the
[00:44:58.360 --> 00:45:02.960]   bounding boxes that we also did and store the images in bounding
[00:45:02.960 --> 00:45:06.640]   boxes. So the bounding box extraction marks were already
[00:45:06.640 --> 00:45:13.120]   stored. And as simple as they told, we did group K fold
[00:45:13.160 --> 00:45:19.160]   stratified so that we can use that for CV for our for our CV
[00:45:19.160 --> 00:45:24.200]   competition. So, so initially, our summary of the solution will
[00:45:24.200 --> 00:45:28.480]   be if you see that our summary of the solution is mostly about
[00:45:28.480 --> 00:45:33.640]   final model is 11 multiclass classification models. So 11
[00:45:33.640 --> 00:45:37.000]   multiclass classification models for with four different
[00:45:37.000 --> 00:45:41.760]   architectures. So the four different architecture were V2M
[00:45:41.760 --> 00:45:46.480]   model, V2L, V5 and V7 efficient. So we found out that efficient
[00:45:46.480 --> 00:45:48.880]   are working good in this competition for study
[00:45:48.880 --> 00:45:53.560]   classification. So we use 11 multiclass models for
[00:45:53.560 --> 00:45:56.440]   multiclass classification where we have to predict four classes.
[00:45:56.440 --> 00:46:00.920]   And then we use two five fold binary classification model
[00:46:00.920 --> 00:46:05.480]   where we have to predict the non classes, whether an image has
[00:46:05.480 --> 00:46:11.640]   basically opacity or not. So let me briefly tell you that here in
[00:46:11.640 --> 00:46:14.480]   this competition, what we are doing is given an image, we have
[00:46:14.480 --> 00:46:19.960]   to figure out whether that image that x ray image has COVID or
[00:46:19.960 --> 00:46:23.920]   not. So how you can get to know is basically, if there is
[00:46:23.920 --> 00:46:27.200]   pneumonia or basically any of the three classes that are
[00:46:27.200 --> 00:46:30.760]   negative, so there is a chance that the person will have COVID.
[00:46:30.760 --> 00:46:34.720]   Right? If he has COVID, we have to also localize that place. So
[00:46:34.720 --> 00:46:37.920]   in the test image, we have to tell that this is the person
[00:46:38.160 --> 00:46:41.680]   where the model is seeing that COVID is present. So basically,
[00:46:41.680 --> 00:46:44.320]   there will be some blurry space. So you can basically if you
[00:46:44.320 --> 00:46:50.000]   read more about it, how in text, you can see whether the person
[00:46:50.000 --> 00:46:55.040]   has COVID or not. So you will have some blurriness in the in
[00:46:55.040 --> 00:46:59.040]   the x ray image so that your WBlock is actually inside.
[00:46:59.040 --> 00:47:01.800]   That's what opacity refers to when you say,
[00:47:01.800 --> 00:47:06.880]   Yeah, yeah. So opacity means there is pneumonia present in
[00:47:06.880 --> 00:47:10.160]   the image pneumonia or COVID-19. You can call basically a main
[00:47:10.160 --> 00:47:13.520]   symptom of COVID-19 is pneumonia, right? So, so you
[00:47:13.520 --> 00:47:16.920]   will, you will see that there is opacity and this is the bonding
[00:47:16.920 --> 00:47:21.760]   box in the image, right? So in the binary, we train these
[00:47:21.760 --> 00:47:26.000]   fivefold two models in two different architectures. And,
[00:47:26.000 --> 00:47:30.880]   and we use an object detection fivefold 25 total basically
[00:47:30.880 --> 00:47:34.480]   five fivefold model, which was having five different
[00:47:34.480 --> 00:47:38.080]   architectures like YOLO, RetinaNet and EfficientJet. And
[00:47:38.080 --> 00:47:41.520]   that was our final submission. So if you go up, I just want to
[00:47:41.520 --> 00:47:45.960]   go up in the solution. So you'll see that our final leaderboard
[00:47:45.960 --> 00:47:51.600]   submission public was 0.649. And our private LB was 0.628 for
[00:47:51.600 --> 00:47:55.320]   the best submission which we selected. And this is how we did
[00:47:55.320 --> 00:47:58.720]   our ensemble. So our final ensemble was having multiple
[00:47:58.720 --> 00:48:02.520]   models, but it was only 11 multi-class classifications, 10
[00:48:02.520 --> 00:48:06.760]   models for binary and 25 for object detection. So these we
[00:48:06.760 --> 00:48:09.480]   all trained and did a lot of experiment. Yeah,
[00:48:09.480 --> 00:48:12.760]   I should also point out 11 sounds like a large number. But
[00:48:12.760 --> 00:48:16.400]   on Kaggle last week, we had Anjum, he did an ensemble of 38.
[00:48:16.400 --> 00:48:20.160]   And that is also a modest number on Kaggle. We've seen numbers
[00:48:20.160 --> 00:48:22.920]   higher than nothing against that the industry doesn't like that
[00:48:22.920 --> 00:48:26.000]   number, but it's what it takes to get to the top of the
[00:48:26.000 --> 00:48:26.680]   leaderboard.
[00:48:26.680 --> 00:48:30.400]   Yeah, but it is notebook only competition. So our competition,
[00:48:30.440 --> 00:48:34.680]   the final competition will be below nine hours. So I mean,
[00:48:34.680 --> 00:48:38.240]   that we were making sure that it all happens below nine hours.
[00:48:38.240 --> 00:48:40.120]   Otherwise, the submission will not happen.
[00:48:40.120 --> 00:48:43.040]   Yeah, another thing I would like to add was during the
[00:48:43.040 --> 00:48:46.840]   submission, you also had to convert the DICOMs to PNGs. So
[00:48:46.840 --> 00:48:51.080]   converting those, yeah, converting those alone took more
[00:48:51.080 --> 00:48:55.160]   than 45, it took around 50 minutes, I guess. So like, you
[00:48:55.160 --> 00:48:59.160]   can assume that one hour went to converting all those, then we
[00:48:59.160 --> 00:49:03.120]   only had seven hours, sorry, eight hours left out of the nine
[00:49:03.120 --> 00:49:06.040]   total hours allocated. So you know, get the predictions of the
[00:49:06.040 --> 00:49:06.440]   model.
[00:49:06.440 --> 00:49:10.960]   Yeah, yeah. So just to point out to the audience Kaggle is now
[00:49:10.960 --> 00:49:15.800]   making sure we don't have 500 model ensembles by imposing some
[00:49:15.800 --> 00:49:19.960]   restrictions where they require you to have a certain number of
[00:49:19.960 --> 00:49:22.760]   sometimes hours to just run the inference where you can train
[00:49:22.760 --> 00:49:25.520]   your model offline, upload it, and then you need to be able to
[00:49:25.520 --> 00:49:28.800]   compute the inference in a few hours. Sometimes you need to do
[00:49:28.800 --> 00:49:32.120]   both the training and inference on Kaggle. Just wanted to point
[00:49:32.120 --> 00:49:32.440]   that out.
[00:49:32.440 --> 00:49:40.120]   Yeah, some conditions like that. Okay, so about the model summary
[00:49:40.120 --> 00:49:44.120]   and details, I think Aisman and Nisheb will tell in detail about
[00:49:44.120 --> 00:49:44.360]   that.
[00:49:44.360 --> 00:49:50.960]   Yeah, thank you, Shivam. So basically, we use efficient nets
[00:49:50.960 --> 00:49:54.280]   for our classification models. At the beginning of the
[00:49:54.280 --> 00:49:58.000]   competition, we tried a variety of, you know, SOTA
[00:49:58.000 --> 00:50:02.480]   classification models, like we tried ResNets. And we also tried
[00:50:02.480 --> 00:50:06.520]   DenseNets, which are known to be good in this medical domain. We
[00:50:06.520 --> 00:50:10.200]   also tried the recent transformer based models, but at
[00:50:10.200 --> 00:50:14.560]   the end, we found out that they could not outperform efficient
[00:50:14.560 --> 00:50:18.200]   nets, like the efficient nets work best for us, especially the
[00:50:18.200 --> 00:50:21.400]   efficient net V2 models that recently came out, they work the
[00:50:21.400 --> 00:50:27.240]   best for us. We started by training normal efficient nets,
[00:50:27.240 --> 00:50:31.520]   like just normal, we didn't do anything normal, just we changed
[00:50:31.520 --> 00:50:35.880]   the number of output classes to four, and we just trained them.
[00:50:35.880 --> 00:50:39.360]   But one thing we started finding out that our model was
[00:50:39.360 --> 00:50:45.520]   overfitting quite easily on the typical, sorry, on the atypical,
[00:50:45.520 --> 00:50:49.280]   that I forgot the class, but the one that had the largest
[00:50:50.080 --> 00:50:54.400]   imbalance, it was overfitting on that quickly, like, because we
[00:50:54.400 --> 00:50:57.680]   had very less images for some of the classes. So the model was
[00:50:57.680 --> 00:50:59.680]   not learning anything for those classes.
[00:50:59.680 --> 00:51:02.680]   I just looked at the dashboard. I think it's the atypical class
[00:51:02.680 --> 00:51:04.480]   that has the largest imbalance.
[00:51:04.480 --> 00:51:08.680]   Yes. Yeah. So our model was overfitting quite badly on that.
[00:51:08.680 --> 00:51:16.480]   And the data, you might see the data looks large. But yeah,
[00:51:16.480 --> 00:51:19.120]   that's quite misleading. Because after removing all those
[00:51:19.120 --> 00:51:23.480]   duplicates and converting it to, you know, PNG images, the size
[00:51:23.480 --> 00:51:28.280]   is reduced quite a lot. Like it's around two or three GB, I
[00:51:28.280 --> 00:51:32.280]   think at the maximum, like, that's how much it gets reduced.
[00:51:32.280 --> 00:51:38.040]   So at the same time, Heng, who is actually a great Kaggler,
[00:51:38.040 --> 00:51:41.760]   like everyone who is on a Kaggle will know his name. So he
[00:51:41.760 --> 00:51:46.480]   posted a discussion where he stated, you know, to battle this
[00:51:46.640 --> 00:51:52.280]   imbalance, let's use a regular regularization strategy. So in
[00:51:52.280 --> 00:51:55.880]   those regularization strategy, what he proposed was using
[00:51:55.880 --> 00:52:00.200]   segmentation loss along with the classification loss. So
[00:52:00.200 --> 00:52:05.600]   basically, what was what he proposed was for every image, we
[00:52:05.600 --> 00:52:10.480]   had a bounding, bounding box. So using that bounding box, he
[00:52:10.480 --> 00:52:14.480]   proposed that we should create a mask for that image. So like,
[00:52:14.480 --> 00:52:18.360]   basically, with the point where your bounding box was present,
[00:52:18.360 --> 00:52:22.280]   you would color that part with white and the rest of your
[00:52:22.280 --> 00:52:26.680]   picture will be negative. So you had a binary image in which the
[00:52:26.680 --> 00:52:29.520]   portion where the bounding box will originally be present,
[00:52:29.520 --> 00:52:32.880]   will, there will actually be a white spot. So basically, that
[00:52:32.880 --> 00:52:37.800]   worked as a segmentation mask. And what would happen in
[00:52:37.800 --> 00:52:41.040]   training was this, you can see in the figure, so like this
[00:52:41.040 --> 00:52:48.960]   backbone part will be your normal model. And from the from
[00:52:48.960 --> 00:52:52.240]   the second last block, you will actually take the feature maps
[00:52:52.240 --> 00:52:55.800]   and pass them to a different segmentation part of the
[00:52:55.800 --> 00:52:59.640]   segmentation head. And using that segmentation head, you will
[00:52:59.640 --> 00:53:03.200]   get a different output other than the classification. So the
[00:53:03.200 --> 00:53:06.040]   output that you get from the segmentation head, you will
[00:53:06.040 --> 00:53:08.520]   compare that with the segmentation mask that you
[00:53:08.520 --> 00:53:11.640]   created above using the bounding box labels. So
[00:53:11.640 --> 00:53:14.640]   essentially, you will have another loss, which will
[00:53:14.640 --> 00:53:20.600]   regularize your classification loss. So that's what almost
[00:53:20.600 --> 00:53:26.160]   every top solution did. And we also did the same, we use the
[00:53:26.160 --> 00:53:29.720]   segmentation loss as well. And for the segmentation loss, we
[00:53:29.720 --> 00:53:33.440]   actually use the lowest loss function. And for
[00:53:33.440 --> 00:53:37.160]   classification, we use normal binary cross entropy loss in
[00:53:37.160 --> 00:53:42.600]   PyTorch. So yeah, our solution, the our whole code was code
[00:53:42.600 --> 00:53:46.640]   basis written in PyTorch. I would also like to add that. And
[00:53:46.640 --> 00:53:51.640]   so going through the architectures, how we chose
[00:53:51.640 --> 00:53:56.800]   them, it was like, we tried, it was almost like brute force, we
[00:53:56.800 --> 00:54:00.880]   tried all the efficient net architectures. And we tried
[00:54:00.880 --> 00:54:04.440]   them on a variety of images and tried them on different folds.
[00:54:04.440 --> 00:54:07.960]   And at the last, we took the ones that performed the best. We
[00:54:07.960 --> 00:54:13.120]   I personally started from efficient net B0 and V2S. Then
[00:54:13.120 --> 00:54:17.520]   once I joined with Nishan and Shivam, we like scaled up to B7
[00:54:17.520 --> 00:54:19.880]   efficient net V2L. And at the end, we also...
[00:54:19.880 --> 00:54:23.760]   How are you keeping track of all of these experiments? I was as
[00:54:23.760 --> 00:54:25.600]   you were just trying all of these ideas?
[00:54:25.600 --> 00:54:30.640]   And yeah, when I was beginning in the competition, I use 1DB
[00:54:30.640 --> 00:54:32.600]   to actually keep track of all my experiments.
[00:54:32.600 --> 00:54:33.240]   That's great to hear.
[00:54:33.640 --> 00:54:40.760]   Yeah, I can show you my dashboard also, if my internet
[00:54:40.760 --> 00:54:41.280]   works.
[00:54:41.280 --> 00:54:43.560]   Take your time.
[00:54:43.560 --> 00:54:47.520]   Yeah, I don't think it will.
[00:54:47.520 --> 00:54:54.000]   For me, it used to be a lot of comments in back in the day, I'm
[00:54:54.000 --> 00:54:57.440]   still not organized, though, to be honest, I would just comment
[00:54:57.440 --> 00:55:00.240]   a lot in my Jupyter notebooks, I would use just Jupyter
[00:55:00.240 --> 00:55:03.160]   notebooks. And that's how I would keep track of my ideas.
[00:55:04.160 --> 00:55:09.680]   Yeah, like I also did that, like, I wrote, I think I, my
[00:55:09.680 --> 00:55:14.160]   model, you know, all the... I don't know why it's not working
[00:55:14.160 --> 00:55:17.680]   now. Like everything was stored in 1DB, all my dashboards are
[00:55:17.680 --> 00:55:21.240]   still there. Like, if it works, I would be happy to show you. So
[00:55:21.240 --> 00:55:25.920]   yeah, basically, I started from the most smallest model V2S and
[00:55:25.920 --> 00:55:29.240]   you know, we scaled up towards the biggest at the end of the
[00:55:29.240 --> 00:55:32.920]   competition. We also tried efficient net V2XL. Like,
[00:55:32.920 --> 00:55:36.400]   that was the biggest model I think that we could fit. But
[00:55:36.400 --> 00:55:39.320]   unfortunately, when we tried that in our submission, the
[00:55:39.320 --> 00:55:41.800]   submission failed, like we exceeded the compute that was
[00:55:41.800 --> 00:55:42.320]   available.
[00:55:42.320 --> 00:55:50.480]   Yeah, so coming on to the model training strategies. So first of
[00:55:50.480 --> 00:55:54.920]   all, our main baseline was actually this efficient net V2M
[00:55:54.920 --> 00:56:01.160]   model. So what we did was we used, we took the efficient net
[00:56:01.160 --> 00:56:06.000]   V2M model, and we only took the feature extractor part of the
[00:56:06.000 --> 00:56:08.960]   model, we removed the pooling layers, the classification head
[00:56:08.960 --> 00:56:11.920]   and everything. So basically, for the model, we only had the
[00:56:11.920 --> 00:56:14.320]   feature extractor, and that would be our backbone.
[00:56:14.320 --> 00:56:17.960]   And did you decide that? How did you decide which layers to
[00:56:17.960 --> 00:56:18.360]   remove?
[00:56:18.360 --> 00:56:24.440]   Okay, yeah, so like, that's actually quite common, like you
[00:56:24.440 --> 00:56:30.840]   will keep your main model, but you will modify your head. So
[00:56:30.840 --> 00:56:33.960]   other than modifying our head, we also modified our pooling
[00:56:33.960 --> 00:56:37.920]   layer as well. Because at the start of the competition, I
[00:56:37.920 --> 00:56:41.280]   don't know, I had this idea that, you know, average pooling
[00:56:41.280 --> 00:56:46.440]   might not work. Because like, if you look at the X-ray images,
[00:56:46.440 --> 00:56:51.920]   even as a human, like it's quite tough to, you know, sometimes
[00:56:51.920 --> 00:56:55.280]   make out the different diseases. So like, if you are averaging
[00:56:55.280 --> 00:57:00.320]   out your feature maps, so like your model might learn less
[00:57:00.320 --> 00:57:03.120]   I don't know, like I had the notion and so I started
[00:57:03.120 --> 00:57:06.320]   exploring different pooling ideas and all and I removed the
[00:57:06.320 --> 00:57:09.280]   pooling as well. And classification head also I
[00:57:09.280 --> 00:57:12.200]   experimented with a lot of, you know, classification head
[00:57:12.200 --> 00:57:17.480]   architectures like there is a quite a famous classification
[00:57:17.480 --> 00:57:20.400]   architecture head that was actually made famous by the fast
[00:57:20.400 --> 00:57:25.040]   AI. So we tried that. We also tried normal linear layers.
[00:57:25.040 --> 00:57:29.440]   Yeah, so we'll go through that. So coming back to the V2M, the
[00:57:29.440 --> 00:57:34.600]   our baseline model, our V2M, the one that is the baseline was
[00:57:34.600 --> 00:57:38.560]   actually a fivefold model trained on 512 cross 512 image
[00:57:38.560 --> 00:57:43.760]   sizes. So this model was actually trained in two stages.
[00:57:43.760 --> 00:57:51.200]   So in the first stage, we took the model and we removed the
[00:57:51.200 --> 00:57:55.320]   pooling layer and replace the pooling layer with a PCM pooling
[00:57:55.320 --> 00:58:00.600]   layer. PCM pooling is essentially it was introduced in
[00:58:00.600 --> 00:58:05.000]   Chexpert, which is another you know, one of the most famous
[00:58:05.000 --> 00:58:09.840]   x-ray classification competitions like this was PCM
[00:58:09.840 --> 00:58:13.120]   pool was introduced by one of the winning solutions there. And
[00:58:13.120 --> 00:58:19.200]   what PCM pooling does is it takes the gradient channel
[00:58:19.200 --> 00:58:22.400]   activation maps that are generated during training and
[00:58:22.400 --> 00:58:27.800]   using those activation maps it computes weighted it computes
[00:58:27.800 --> 00:58:32.200]   weights for every features and merge merges them with your
[00:58:32.200 --> 00:58:39.600]   feature maps. So what it does is for every different class, your
[00:58:39.600 --> 00:58:44.040]   feature maps will become more specialized. So if you check out
[00:58:44.040 --> 00:58:47.760]   the gradient cams for PCM pooling, like we found out that
[00:58:48.480 --> 00:58:52.240]   our model was learning better. So we stuck with PCM pooling.
[00:58:52.240 --> 00:58:56.880]   And for loss function, we use binary cross entropy loss and
[00:58:56.880 --> 00:59:00.760]   for segmentation loss, actually we use two losses, low loss loss
[00:59:00.760 --> 00:59:06.800]   and BC loss. And we also added weights to the loss was 0.75 for
[00:59:06.800 --> 00:59:10.960]   low loss loss and 0.25 for binary cross entropy loss. This
[00:59:10.960 --> 00:59:15.720]   part was actually this idea was actually introduced in the Kaggle
[00:59:15.720 --> 00:59:18.840]   discussion somewhere itself. So I just took it from there and
[00:59:18.840 --> 00:59:20.880]   this work for us and we kept it.
[00:59:20.880 --> 00:59:25.360]   Sorry to interrupt. I'm sorry, I keep interrupting you, but I'm
[00:59:25.360 --> 00:59:27.240]   just curious to learn from you.
[00:59:27.240 --> 00:59:27.720]   Sure.
[00:59:27.720 --> 00:59:31.720]   How when you mentioned you found you tried PCM pooling, how are
[00:59:31.720 --> 00:59:35.160]   you how were you finding all of these ideas? Or how were you
[00:59:35.160 --> 00:59:37.520]   also deciding when you were brute forcing through the
[00:59:37.520 --> 00:59:40.200]   models? How did you decide or find these ideas?
[00:59:40.200 --> 00:59:43.800]   Yeah, so as I told before, I was going through some of the
[00:59:43.800 --> 00:59:47.000]   previous solutions, both in Kaggle, as well as some other
[00:59:47.000 --> 00:59:50.560]   competitions. So from there, I found out I was expert. And I
[00:59:50.560 --> 00:59:53.480]   started looking at few of the top solutions. And that's how I
[00:59:53.480 --> 00:59:59.920]   came across PCM pooling. So initially, I was trying average
[00:59:59.920 --> 01:00:03.800]   pooling, max pooling, then concat pooling, everything was
[01:00:03.800 --> 01:00:06.920]   done with the same V2M model. So basically, what I would do was
[01:00:06.920 --> 01:00:10.920]   keep everything same image size, learning rate, everything same
[01:00:10.920 --> 01:00:15.120]   and I would just, you know, change the pooling layer, change
[01:00:15.120 --> 01:00:17.840]   it and I found out that you know, PCM pooling was giving
[01:00:17.840 --> 01:00:24.000]   quite a boost. So I stuck with it. So yeah, like that we are
[01:00:24.000 --> 01:00:28.320]   along with PCM pooling, I we also added an attention map, SAM
[01:00:28.320 --> 01:00:34.040]   which is actually the spatial attention map. And we found out
[01:00:34.040 --> 01:00:37.960]   that this also helped boosted our score. And so we kept it.
[01:00:38.480 --> 01:00:44.000]   This was all actually based on experiments like we we would I
[01:00:44.000 --> 01:00:47.320]   would what we would do was we would find new ideas. And we
[01:00:47.320 --> 01:00:50.400]   would try to incorporate them into our existing pipeline. If
[01:00:50.400 --> 01:00:53.840]   it works, we'll keep it if it does not work. Let's move on to
[01:00:53.840 --> 01:01:00.840]   a new topic. Okay. So yeah, the efficient that V2M model was
[01:01:00.840 --> 01:01:04.280]   trained with the Ranger optimizer and for learning rate,
[01:01:04.280 --> 01:01:07.560]   we use a cosine handling learning rate. And we use a
[01:01:07.560 --> 01:01:11.360]   Gaussian learning rate. And I would also like to add that we
[01:01:11.360 --> 01:01:14.160]   also replace the activation layers of the model with mesh
[01:01:14.160 --> 01:01:19.120]   activation. Like I came across a mesh plus the combination of
[01:01:19.120 --> 01:01:24.560]   mesh activation plus Ranger in the fast AI forums like that.
[01:01:24.560 --> 01:01:27.560]   That's where I guess mesh plus Ranger was made famous when the
[01:01:27.560 --> 01:01:32.560]   team won the image wolf competition. So I thought let's
[01:01:32.560 --> 01:01:38.000]   try to make a model that works for us. So all the efficient
[01:01:38.000 --> 01:01:41.520]   that V2M model, sorry, the all the efficient that V2 models
[01:01:41.520 --> 01:01:46.000]   were trained using Ranger and mesh activation only. So yeah,
[01:01:46.000 --> 01:01:50.360]   that's actually the first stage. And another thing we did that
[01:01:50.360 --> 01:01:53.560]   boosted our score a lot was we use noisy student training
[01:01:53.560 --> 01:01:57.200]   method. So essentially, in noisy student training method, what we
[01:01:57.200 --> 01:02:01.480]   do was we train a baseline model, then we take out the
[01:02:01.480 --> 01:02:06.200]   out of fold model. And we train either that same model or a
[01:02:06.200 --> 01:02:12.240]   bigger model using from scratch using the predictions that were
[01:02:12.240 --> 01:02:17.840]   taken out from your base model. So what we did was we trained
[01:02:17.840 --> 01:02:22.000]   it efficient at V2M, then we took out the five we trained is
[01:02:22.000 --> 01:02:26.320]   efficient at V2M five fold. Then what we did was for each fold,
[01:02:26.320 --> 01:02:30.520]   we took out the out of fold predictions from the model. And
[01:02:30.520 --> 01:02:34.480]   then using those predicted labels, along with the original
[01:02:34.480 --> 01:02:39.360]   labels, we train this model again with the noisy student
[01:02:39.360 --> 01:02:42.200]   training method. And this actually boosted our score a
[01:02:42.200 --> 01:02:46.720]   lot. I think our high classification score was based
[01:02:46.720 --> 01:02:50.840]   on this. Like if we go through our final solution and the
[01:02:50.840 --> 01:02:54.960]   breakdown of the scores, we'll find out that the major boost
[01:02:54.960 --> 01:02:59.400]   that came came in our solution was actually from our
[01:02:59.400 --> 01:03:02.440]   classification models, like our classification models were
[01:03:02.440 --> 01:03:08.480]   performing much better than every solution that's been made
[01:03:08.480 --> 01:03:12.960]   public, like but we were lacking in our object detection part. So
[01:03:12.960 --> 01:03:16.280]   maybe if we did something there, we might have finished even
[01:03:16.280 --> 01:03:16.720]   higher.
[01:03:16.720 --> 01:03:20.720]   Yeah, I also quickly want to point out that my colleague Aman
[01:03:20.720 --> 01:03:23.640]   Arora and he's also co-chair drinker on chai time data
[01:03:23.640 --> 01:03:27.640]   since we both run the channel, we went into much more depth of
[01:03:27.640 --> 01:03:30.520]   labels because there's much more to it. And you've also open
[01:03:30.520 --> 01:03:34.320]   source your solution. It's a nice, nicely very elegant code
[01:03:34.320 --> 01:03:38.280]   there. I've it's also the in the resources links. So please
[01:03:38.280 --> 01:03:41.080]   check that out for more details. But again, that becomes an
[01:03:41.080 --> 01:03:43.720]   hour long discussion in itself. There's so much depth to that.
[01:03:43.720 --> 01:03:44.720]   Just wanted to point that out.
[01:03:44.720 --> 01:03:52.880]   So yeah, that V2M was actually our baseline. And we actually
[01:03:52.880 --> 01:03:58.200]   use this V2M predictions for for again, noisy training another
[01:03:58.200 --> 01:04:04.840]   V2L model. So this V2L model was trained on 512 image sizes.
[01:04:04.840 --> 01:04:08.960]   Again, same PCM pooling and same attention Ranger Mesh
[01:04:08.960 --> 01:04:14.440]   activation. So we actually had multiple V2M and V2L models. So
[01:04:14.440 --> 01:04:19.360]   our baseline was this 512 cross 512 image size train. And that
[01:04:19.360 --> 01:04:24.040]   model we use fivefold. We also had another V2M model that was
[01:04:24.040 --> 01:04:28.640]   noisiest student train again using the our baseline V2M
[01:04:28.640 --> 01:04:33.040]   model. But the difference in this V2M model was the image
[01:04:33.040 --> 01:04:39.880]   size we used was 640. And there was another V2M model, which was
[01:04:39.880 --> 01:04:43.600]   which again, we use the noisy levels from our baseline. And
[01:04:43.600 --> 01:04:47.680]   the difference in this V2M was we changed we raised the image
[01:04:47.680 --> 01:04:49.480]   size up to 1024.
[01:04:49.480 --> 01:04:53.120]   How are you deciding these different image sizes? Any logic
[01:04:53.120 --> 01:04:53.640]   behind that?
[01:04:53.640 --> 01:04:58.000]   Actually, there is no logic like there are some common image
[01:04:58.000 --> 01:05:04.240]   sizes that people generally use like 242, 320, 520. Like these
[01:05:04.240 --> 01:05:08.680]   are some common image common and like 1024 was the highest we
[01:05:08.680 --> 01:05:11.240]   could go with the available compute that was available to
[01:05:11.240 --> 01:05:14.920]   us. So yeah, so that's the highest.
[01:05:16.760 --> 01:05:19.680]   I would like to add like we trained on larger size images
[01:05:19.680 --> 01:05:23.880]   and different image size because to get some diversity in model
[01:05:23.880 --> 01:05:27.760]   so that way, we could make benefit of them violence and
[01:05:27.760 --> 01:05:31.920]   things so then did quite boost violence and make sense.
[01:05:31.920 --> 01:05:36.760]   Yeah, so basically, during inference, what would happen was
[01:05:36.760 --> 01:05:41.120]   you would have inference in multi scale. So like your feature
[01:05:41.120 --> 01:05:44.600]   maps will be different. And when you merge those predictions,
[01:05:44.600 --> 01:05:48.760]   like your predictions become more robust to you know, changes.
[01:05:48.760 --> 01:05:51.960]   That's why we use a variety of image sizes and different
[01:05:51.960 --> 01:05:57.320]   architectures as well. So yeah, that ends the V2M. Then coming
[01:05:57.320 --> 01:05:59.720]   back to our the second architecture that we use was
[01:05:59.720 --> 01:06:06.400]   actually a V2L model. V2L was basically, you know, bumped up
[01:06:06.400 --> 01:06:11.080]   V2M like it's a larger model. That's the best way to describe
[01:06:11.080 --> 01:06:17.920]   it, I guess. So, so here also, we use 512 cross 512 images at
[01:06:17.920 --> 01:06:22.920]   the beginning. And we also use another model with 640. And all
[01:06:22.920 --> 01:06:26.600]   these models were trained using noisy student method using the
[01:06:26.600 --> 01:06:31.560]   predictions from our baseline V2M model. So the backbone of
[01:06:31.560 --> 01:06:34.720]   our whole solution, if you would say was our baseline V2M model,
[01:06:34.720 --> 01:06:38.800]   like that, that's the model we use for noisy student training
[01:06:38.800 --> 01:06:44.640]   and all that. So rest, everything was same for the V2L
[01:06:44.640 --> 01:06:49.360]   model with the V2M model, we just tune the learning rate in
[01:06:49.360 --> 01:06:54.360]   some parts in some models. And yeah, also, I would like to I
[01:06:54.360 --> 01:06:57.160]   almost forgot that we also fine tune the models. So
[01:06:57.160 --> 01:07:01.160]   essentially, what we did was we would train the model for like
[01:07:01.160 --> 01:07:05.480]   1520 epochs, then what we would do was we would decrease the
[01:07:05.480 --> 01:07:09.120]   learning rate, like we would decrease the learning rate by
[01:07:09.120 --> 01:07:13.120]   100. And we would again, train for a lesser number of epochs,
[01:07:13.120 --> 01:07:16.360]   like let's say five, three epochs, and sometimes it would
[01:07:16.360 --> 01:07:19.640]   give a boost, sometimes it would not. And we would take the model
[01:07:19.640 --> 01:07:26.440]   which would give a boost. And yeah, so this is the these are
[01:07:26.440 --> 01:07:30.360]   the models that we train in the EfficientNet V2 series. We also
[01:07:30.360 --> 01:07:33.080]   had models from the normal EfficientNet series basically,
[01:07:33.280 --> 01:07:37.680]   we had, we took the EfficientNet B5 and B7 models from the
[01:07:37.680 --> 01:07:45.960]   normal EfficientNet series. So in the B5, normal EfficientNet
[01:07:45.960 --> 01:07:48.680]   training of the normal EfficientNet series was totally
[01:07:48.680 --> 01:07:54.120]   separate from the V2 series. So we did not use like these were
[01:07:54.120 --> 01:07:56.880]   also trained with noisy student, but we did not use the
[01:07:56.880 --> 01:08:02.080]   predictions from the V2 models, like we decided that while
[01:08:02.080 --> 01:08:06.040]   doing noisy student, we will, you know, keep the backbone
[01:08:06.040 --> 01:08:09.760]   family same, like, if we are using noisy predictions from the
[01:08:09.760 --> 01:08:13.680]   V2 models, then we would only use those models to noisy
[01:08:13.680 --> 01:08:18.160]   student trained V2 models. So yeah, that's how we did it. So
[01:08:18.160 --> 01:08:21.640]   initially, we trained in EfficientNet B5, we also trained
[01:08:21.640 --> 01:08:27.360]   B3 less models, I mean, smaller models, but yeah, B5 was our
[01:08:27.360 --> 01:08:32.800]   best. So we trained B5 on 640 image sizes. And in this model,
[01:08:32.800 --> 01:08:36.160]   we found out that PCAM pooling was actually not working. We
[01:08:36.160 --> 01:08:40.280]   tried a bunch of different heads, and we stuck with a head
[01:08:40.280 --> 01:08:45.920]   using average pooling plus SC-SC attention, which was actually
[01:08:45.920 --> 01:08:49.560]   described in another Kaggle competition. The competition was
[01:08:49.560 --> 01:08:57.280]   Renzer competition and it was introduced by Kaggle master at
[01:08:57.480 --> 01:09:03.240]   Tawara, I think. And he proposed that for each class, why don't
[01:09:03.240 --> 01:09:07.600]   we have specialized detectors? So like we our model had four
[01:09:07.600 --> 01:09:11.640]   different classification heads, one for each classification
[01:09:11.640 --> 01:09:12.080]   level.
[01:09:12.080 --> 01:09:17.640]   So we're forcing the model to pay attention to individual
[01:09:17.640 --> 01:09:18.880]   classes as I understand.
[01:09:18.880 --> 01:09:24.280]   Yeah, yeah. So that's what we are doing. So basically, you
[01:09:24.280 --> 01:09:28.520]   would have a single feature map, I mean, and your feature map
[01:09:28.520 --> 01:09:34.280]   will go through four different heads, and each head would give
[01:09:34.280 --> 01:09:38.120]   predictions for a single class. So in that way, your model
[01:09:38.120 --> 01:09:41.280]   becomes a little bit more specialized. And we also found
[01:09:41.280 --> 01:09:46.480]   that like in between the study levels, there was no correlation
[01:09:46.480 --> 01:09:50.800]   like and also that every image had a single level only. So we
[01:09:50.800 --> 01:09:55.040]   decided that to know to maximize the competition metric, it would
[01:09:55.040 --> 01:09:59.000]   be better if we make, you know, specialized predictions so we
[01:09:59.000 --> 01:10:03.840]   can boost our score. So here also, we used the same
[01:10:03.840 --> 01:10:10.880]   segmentation regularization loss technique that was described by
[01:10:10.880 --> 01:10:16.160]   Heng. And everything was similar, but here we change up
[01:10:16.160 --> 01:10:20.440]   the training strategy. So here we use Adam W plus one cycle
[01:10:20.440 --> 01:10:24.640]   LR scheduling, which again was made famous by fast AI. So
[01:10:24.640 --> 01:10:29.040]   that's what we use. And yeah, so at the beginning, we trained
[01:10:29.040 --> 01:10:33.320]   this B5 models and similar to what we did with the V2M, we
[01:10:33.320 --> 01:10:37.240]   took out the OOF predictions of this model, and again, train
[01:10:37.240 --> 01:10:41.400]   this model once another time with this OOF predictions. So
[01:10:41.400 --> 01:10:42.440]   that's the final B5.
[01:10:42.440 --> 01:10:45.400]   For people that don't know, OOF stands for out of fold
[01:10:45.400 --> 01:10:49.640]   predictions, the fold that we've left out of the five fold model.
[01:10:50.640 --> 01:10:54.200]   Yeah. So for each fold, we will take out the predictions and
[01:10:54.200 --> 01:10:59.040]   using those we would again, train the model. And that gives
[01:10:59.040 --> 01:11:03.200]   us quite a boost. So that's our B5. And we had another we
[01:11:03.200 --> 01:11:08.200]   trained another B6 model using the same strategy described in
[01:11:08.200 --> 01:11:13.640]   B5. So the B6 we did not use in our final ensemble, but B6 was
[01:11:13.640 --> 01:11:17.560]   actually used in training this B7 model. So basically, what we
[01:11:17.560 --> 01:11:21.960]   did was we took out the predictions from the B5 model, as
[01:11:21.960 --> 01:11:25.640]   well as the B6 model. And then we merge those predictions
[01:11:25.640 --> 01:11:30.280]   giving 0.5 weights to each and that will become your noisy
[01:11:30.280 --> 01:11:35.760]   labels for your efficient B7 model. So that's what we did.
[01:11:35.760 --> 01:11:42.880]   And rest was similar. So this was our classification part. And
[01:11:42.880 --> 01:11:46.720]   also, all the models were trained on five folds, but in
[01:11:46.720 --> 01:11:50.320]   the final ensemble, not all the five folds from each model were
[01:11:50.320 --> 01:11:58.000]   taken. So what we basically did was we using mix using different
[01:11:58.000 --> 01:12:02.120]   experiments, we actually created the blend which would, you know,
[01:12:02.120 --> 01:12:07.920]   give the maximum CV. So for example, if we look at these
[01:12:07.920 --> 01:12:12.000]   models, so in our final classification, we had our
[01:12:12.000 --> 01:12:16.360]   baseline efficient for our baseline efficient and V2M
[01:12:16.360 --> 01:12:21.120]   model, we use all the five folds, but for the model trained
[01:12:21.120 --> 01:12:25.800]   using 640 and 1024 images, we only use the model that was
[01:12:25.800 --> 01:12:32.000]   trained on the 0th fold. And for V2L also 512, we use all the
[01:12:32.000 --> 01:12:38.200]   five folds, but 640 we use a single fold only. And similarly
[01:12:38.200 --> 01:12:42.000]   in here also we took the model, we took the best models,
[01:12:42.400 --> 01:12:47.240]   according to foldwise network maximizing our CV score. So
[01:12:47.240 --> 01:12:52.240]   that's how we ensemble our models. And if we were to
[01:12:52.240 --> 01:12:56.920]   summarize on what gave us the boost for in our classification,
[01:12:56.920 --> 01:13:00.680]   the main boost will be using noisy student training, also
[01:13:00.680 --> 01:13:05.680]   using segmentation mask to regularize our loss. And
[01:13:05.680 --> 01:13:09.720]   attention also gave us quite a boost using use of attention.
[01:13:10.000 --> 01:13:14.320]   And during inference, we also use horizontal flip as test
[01:13:14.320 --> 01:13:20.160]   time augmentation. So that concludes the classification
[01:13:20.160 --> 01:13:23.120]   study level part. And coming back to the...
[01:13:23.120 --> 01:13:26.320]   Real quick, when you mentioned the attention, this is from the
[01:13:26.320 --> 01:13:28.320]   Kaggle master Tawada, right?
[01:13:28.320 --> 01:13:29.600]   Correct.
[01:13:29.600 --> 01:13:35.920]   Multihead was from him and PCAM head was something and use of
[01:13:35.920 --> 01:13:39.520]   attention in PCAM was something we, you know, tried different
[01:13:39.520 --> 01:13:43.520]   heads and all. We just did a lot of experiments and came to
[01:13:43.520 --> 01:13:45.920]   that, that if we use attention, this would actually boost our
[01:13:45.920 --> 01:13:46.320]   score.
[01:13:46.320 --> 01:13:47.320]   Gotcha.
[01:13:47.320 --> 01:13:47.760]   That's what...
[01:13:47.760 --> 01:13:50.520]   Sorry, I'm really slow with understanding this. So I keep
[01:13:50.520 --> 01:13:52.480]   interrupting you. Please, please continue.
[01:13:52.480 --> 01:14:00.320]   Yeah. So coming back to the image level solution, in image
[01:14:00.320 --> 01:14:04.160]   level, we basically had to predict opacity and non-class.
[01:14:04.160 --> 01:14:07.920]   And for opacity, we also had to predict the bounding boxes. So
[01:14:07.920 --> 01:14:13.200]   we divided this into two parts. We had, we for predicting
[01:14:13.200 --> 01:14:16.640]   binary, sorry, for predicting the non-class, we actually
[01:14:16.640 --> 01:14:19.680]   treated this as a normal classification problem. Because
[01:14:19.680 --> 01:14:23.280]   for this, we did not have to predict any bounding boxes. So
[01:14:23.280 --> 01:14:28.600]   we tackle this as a classification problem. And our
[01:14:28.600 --> 01:14:32.760]   object detection pipeline would only predict for the opacity
[01:14:32.760 --> 01:14:37.600]   class. So like it would predict only bounding boxes. So in our
[01:14:37.600 --> 01:14:41.840]   image level, binary image level for the binary classification,
[01:14:41.840 --> 01:14:46.120]   that is for the non-class, we use an efficient net B6 model.
[01:14:46.120 --> 01:14:51.920]   So the interesting thing that we found during the competition
[01:14:51.920 --> 01:14:56.080]   was that the non-labels that were given were actually having
[01:14:56.080 --> 01:14:59.480]   a quite a nice and high correlation with the negative
[01:14:59.480 --> 01:15:03.760]   labels that were given. Like the correlation was quite high.
[01:15:04.160 --> 01:15:09.280]   And we also found out that our model predictions correlation,
[01:15:09.280 --> 01:15:13.880]   along with the non-class was also quite high. So before
[01:15:13.880 --> 01:15:19.000]   merging, Nishay and Shivam, they had a quite nice binary
[01:15:19.000 --> 01:15:23.720]   classification model for non-class. So like, I would like
[01:15:23.720 --> 01:15:27.920]   to say like, before merging, like I was basically above
[01:15:27.920 --> 01:15:32.200]   because of my classification model. And Nishay and Shivam,
[01:15:32.200 --> 01:15:36.880]   they had trained a very nice model for predicting non-class.
[01:15:36.880 --> 01:15:43.120]   Sorry, excuse me. So yeah. And when we started, you know,
[01:15:43.120 --> 01:15:46.640]   working towards merging our models, different merging our
[01:15:46.640 --> 01:15:49.960]   models, we actually were analyzing our model predictions.
[01:15:49.960 --> 01:15:54.320]   And we found out that the predictions from my baseline V2M,
[01:15:54.320 --> 01:15:59.080]   the negative predictions were actually correlated very high.
[01:15:59.800 --> 01:16:03.200]   There was actually a very high correlation with the binary
[01:16:03.200 --> 01:16:08.120]   model predictions that Nishay and Shivam's model was giving.
[01:16:08.120 --> 01:16:13.040]   Like the correlation was above 0.9 something. So we decided
[01:16:13.040 --> 01:16:16.640]   that, let's you know, instead of putting the non-predictions
[01:16:16.640 --> 01:16:20.040]   there, since my V2M was performing quite well on the
[01:16:20.040 --> 01:16:23.360]   leaderboard as well as the CV, we decided to replace the
[01:16:23.360 --> 01:16:26.880]   non-predictions with the negative predictions from my V2M
[01:16:26.880 --> 01:16:31.880]   model. And that worked a lot. So that worked a lot. We, that
[01:16:31.880 --> 01:16:37.320]   increased our CV and we also jumped around three or three or
[01:16:37.320 --> 01:16:41.080]   two places in the leaderboard. So we thought that if we look
[01:16:41.080 --> 01:16:44.120]   further towards, we might get something there.
[01:16:44.120 --> 01:16:47.880]   So it's really exciting when you make that submission and your
[01:16:47.880 --> 01:16:50.880]   position goes up the leaderboard, you see it going up.
[01:16:50.880 --> 01:16:54.120]   That's so exciting. Like, I don't think anyone who's not
[01:16:54.120 --> 01:16:58.520]   Kaggle can understand it, but it's, it's, that's I think the
[01:16:58.520 --> 01:16:59.440]   most addictive thing.
[01:16:59.440 --> 01:17:05.720]   Yeah, actually Nishay found this out and like, he was going
[01:17:05.720 --> 01:17:09.480]   through all those predictions and he was computing all these,
[01:17:09.480 --> 01:17:11.920]   you know, correlation between them, this and that. And he
[01:17:11.920 --> 01:17:16.000]   said that we are having a high correlation, which was also
[01:17:16.000 --> 01:17:18.600]   true with the original labels that were given. So he said,
[01:17:18.600 --> 01:17:23.120]   you know, we had spare submissions that day. So like,
[01:17:23.160 --> 01:17:26.440]   let's try this, let's see where this goes. And you know, like
[01:17:26.440 --> 01:17:31.040]   our CV increased as well as our leaderboard score increased.
[01:17:31.040 --> 01:17:35.520]   But then again, we tried this with all our classification
[01:17:35.520 --> 01:17:38.920]   models, but we found out that this was happening only in the
[01:17:38.920 --> 01:17:43.520]   case of our baseline V2M model. Like other models did not have
[01:17:43.520 --> 01:17:47.080]   this thing. Like some models, they would increase your CV if
[01:17:47.080 --> 01:17:50.520]   you did this and, but the LB would decrease. And similarly,
[01:17:50.520 --> 01:17:54.840]   some models, they will do the opposite. But in case of the
[01:17:54.840 --> 01:17:59.040]   baseline V2M, it was increasing both our CV and our LB. So we
[01:17:59.040 --> 01:18:02.480]   stuck with this and this was also used in our final pipeline.
[01:18:02.480 --> 01:18:07.880]   So that concludes, that was one of our models, fivefold model
[01:18:07.880 --> 01:18:11.520]   for binary classification. We also had a different binary
[01:18:11.520 --> 01:18:15.360]   model, which was essentially trained only to identify the
[01:18:15.360 --> 01:18:19.840]   non-class. Because since we were a bit skeptical, like since we
[01:18:19.840 --> 01:18:23.560]   were, you know, mismatching the non-predictions with the
[01:18:23.560 --> 01:18:26.480]   negative predictions. So we were not quite sure how it would do
[01:18:26.480 --> 01:18:30.080]   on the hidden test set. So just to be safe, we trained another
[01:18:30.080 --> 01:18:35.640]   model, which was specially trained for the non-class and,
[01:18:35.640 --> 01:18:38.400]   you know, merge these predictions with our V2M
[01:18:38.400 --> 01:18:42.920]   predictions, giving weights to them. So like the V2M
[01:18:42.920 --> 01:18:47.840]   predictions would have 0.85 weights and the specialized
[01:18:47.840 --> 01:18:50.880]   non-class model would have a weight of zero, predictions
[01:18:50.880 --> 01:18:55.600]   would have a weight of 0.25. So the specialized model for this,
[01:18:55.600 --> 01:19:00.320]   that we trained was actually the efficient net B6 model. And this
[01:19:00.320 --> 01:19:03.880]   was also actually trained using noisy student method. But
[01:19:03.880 --> 01:19:08.040]   another interesting thing that we did for training this with
[01:19:08.040 --> 01:19:13.800]   noisy student was, instead of taking the noisy labels from the
[01:19:13.800 --> 01:19:18.880]   non-predictions, what we did was we already had trained a B5 and
[01:19:18.880 --> 01:19:23.400]   a B6 on classification problem. So we took the negative
[01:19:23.400 --> 01:19:28.040]   predictions from those models, and we treated them as our noisy
[01:19:28.040 --> 01:19:32.320]   non-predictions. And using those noisy non-predictions, we train
[01:19:32.320 --> 01:19:37.840]   an efficient net B6 model. So that's a bit confusing, like, we
[01:19:37.840 --> 01:19:41.360]   are, you know, mismatching predictions from here, taking
[01:19:41.720 --> 01:19:44.840]   out the full predictions from there, doing noisy student
[01:19:44.840 --> 01:19:48.960]   training. That's a bit confusing. But yeah, that's what
[01:19:48.960 --> 01:19:49.520]   we did.
[01:19:49.520 --> 01:19:54.280]   I'm curious, how did you come up with that idea? Because it's
[01:19:54.280 --> 01:19:57.280]   slightly mind bending to understand it. How did that
[01:19:57.280 --> 01:19:58.520]   thought come around?
[01:19:58.520 --> 01:20:02.480]   Yeah, actually, we found out that using noisy student
[01:20:02.480 --> 01:20:05.760]   training was boosting our score quite a lot. So you know, we
[01:20:05.760 --> 01:20:09.760]   went crazy with the experiments and started doing a lot of other
[01:20:09.840 --> 01:20:14.280]   things like we would take the noisy, we would take the out of
[01:20:14.280 --> 01:20:16.960]   full predictions from a different model. Like we also
[01:20:16.960 --> 01:20:20.000]   had ResNets and DenseNets at the beginning, I told you. So
[01:20:20.000 --> 01:20:22.400]   sometimes we would take the predictions from those models,
[01:20:22.400 --> 01:20:25.440]   treat them as noisy labels, and we would train an efficient net.
[01:20:25.440 --> 01:20:31.320]   So like, we did all of these things like, some experiments
[01:20:31.320 --> 01:20:35.840]   didn't even make sense. Like, we don't have any, you know, base
[01:20:35.840 --> 01:20:39.800]   for doing those experiments, but we still did those. And we
[01:20:39.800 --> 01:20:43.760]   found out that some of them gave us quite a boost both in our CV
[01:20:43.760 --> 01:20:47.680]   and LV. And the ones that gave us boost in our CB as well as
[01:20:47.680 --> 01:20:53.480]   our LV, we stuck with those. So this is how we came out using,
[01:20:53.480 --> 01:20:57.400]   you know, not using the non predictions, but actually using
[01:20:57.400 --> 01:21:00.920]   the negative predictions as our noisy labels that gave us a
[01:21:00.920 --> 01:21:06.040]   boost. So we stuck with that. So coming into the details of the
[01:21:06.080 --> 01:21:11.200]   next model for non predictions. This was actually quite a normal
[01:21:11.200 --> 01:21:16.280]   model, we did not change anything, average pooling, all
[01:21:16.280 --> 01:21:19.920]   the only thing that we changed was, you know, added a
[01:21:19.920 --> 01:21:24.680]   segmentation loss and change the final linear layer to output one
[01:21:24.680 --> 01:21:30.480]   class only. And the model was trained using Ranger optimizer
[01:21:30.480 --> 01:21:34.280]   and we, the learning rate, we also schedule the learning rate
[01:21:35.000 --> 01:21:38.280]   with warm up and after the warm up schedule was over, we
[01:21:38.280 --> 01:21:45.600]   basically did cosine annealing. So yeah, so our classification
[01:21:45.600 --> 01:21:49.440]   ensemble, as I was explaining before, so you basically have
[01:21:49.440 --> 01:21:54.000]   these classification models. So your, this is the efficient at
[01:21:54.000 --> 01:21:57.200]   basics that was especially trained for the non predictions.
[01:21:57.200 --> 01:22:02.160]   So your point four weighted will go from the non predictions. And
[01:22:02.160 --> 01:22:05.920]   point six negative will actually come from our fivefold baseline
[01:22:05.920 --> 01:22:07.000]   V2M prediction.
[01:22:07.000 --> 01:22:10.720]   And for the other models, did you remove the non class since
[01:22:10.720 --> 01:22:12.320]   you have a separate model for it?
[01:22:12.320 --> 01:22:17.920]   No, we actually, this diagram is a bit misleading. So what we
[01:22:17.920 --> 01:22:21.880]   did was not the all the other models. So basically, we took
[01:22:21.880 --> 01:22:26.480]   only the V2M fivefold baseline V2M model, and the non
[01:22:26.480 --> 01:22:30.840]   predictions instead, so sorry, the negative predictions, we
[01:22:30.840 --> 01:22:34.320]   merge the negative predictions from our baseline V2M. And the
[01:22:34.320 --> 01:22:37.840]   non predictions from our basics that was especially trained for
[01:22:37.840 --> 01:22:41.720]   non class, we merge those predictions. So that is our
[01:22:41.720 --> 01:22:45.640]   final ensemble for the non class.
[01:22:45.640 --> 01:22:47.080]   Gotcha.
[01:22:47.080 --> 01:22:53.040]   So that's basically what we did. And in the classification part,
[01:22:53.040 --> 01:22:56.560]   we basically had multiple models. And we arranged those
[01:22:56.560 --> 01:23:01.840]   multiple models and gave weight to those predictions using CV.
[01:23:01.840 --> 01:23:04.680]   And we also use Optuna to maximize our score.
[01:23:04.680 --> 01:23:07.280]   Like, so you were adding something?
[01:23:07.280 --> 01:23:13.680]   No, I was just adding like, it doesn't seem misleading. As I
[01:23:13.680 --> 01:23:17.400]   have already mentioned that I only only use the efficient V2M
[01:23:17.400 --> 01:23:20.040]   predictions for non predictions.
[01:23:20.040 --> 01:23:21.440]   Yeah, so
[01:23:21.600 --> 01:23:26.000]   made two errors, the below one was for non predictions.
[01:23:26.000 --> 01:23:30.520]   So what we basically did was our efficient V2M model, this was
[01:23:30.520 --> 01:23:33.160]   kept separate. So the predictions that we generated
[01:23:33.160 --> 01:23:37.480]   from this V2M was generated separately. And these are the
[01:23:37.480 --> 01:23:39.880]   other classification models, these predictions were
[01:23:39.880 --> 01:23:45.280]   generated together. So what we did was the efficient V2M
[01:23:45.280 --> 01:23:49.120]   predictions would have a weightage of 0.15. And the other
[01:23:49.120 --> 01:23:53.280]   classification models would have a weightage of 0.85 in the
[01:23:53.280 --> 01:23:58.240]   classification ensemble. And for the binary non class ensemble,
[01:23:58.240 --> 01:24:01.280]   what will happen was the predictions from the negative
[01:24:01.280 --> 01:24:05.560]   predictions of V2M will have a weightage of 0.6. And in non
[01:24:05.560 --> 01:24:11.360]   predictions from our B6 would have 0.4 weightage. So like the
[01:24:11.360 --> 01:24:15.480]   V2M was used both in classification and both in the
[01:24:15.480 --> 01:24:23.720]   binary problem, the baseline V2M. And now coming to the
[01:24:23.720 --> 01:24:28.400]   object detection part. So our object detection part was, you
[01:24:28.400 --> 01:24:33.000]   know, nothing special, like we use normal training strategies
[01:24:33.000 --> 01:24:37.440]   that was discussed previously in different Kaggle competitions.
[01:24:37.440 --> 01:24:42.520]   So the final ensemble of the object detection part had the
[01:24:42.520 --> 01:24:47.440]   efficient net efficient net D3 efficient net D5, two YOLO V5
[01:24:47.440 --> 01:24:50.880]   models and a single retina net. I would like to pass it over to
[01:24:50.880 --> 01:24:53.160]   Nishay to further explain this part.
[01:24:53.160 --> 01:24:57.760]   Yeah, so coming on to the object detection part, as you could see
[01:24:57.760 --> 01:25:00.840]   in the block diagram, like we use the ensemble of five
[01:25:00.840 --> 01:25:05.680]   different architectures, each having five foldage. So our sole
[01:25:05.680 --> 01:25:09.640]   aim for object detection was to get a decent score to maintain
[01:25:09.640 --> 01:25:13.320]   our position in the leaderboard. So somehow we managed to
[01:25:13.320 --> 01:25:16.600]   perform quite well in object detection as well. So from the
[01:25:16.600 --> 01:25:20.920]   beginning, we decided to train like each model with having
[01:25:20.920 --> 01:25:24.120]   different architecture so that we could have diverse models,
[01:25:24.120 --> 01:25:28.080]   even if they don't have a good single model score, they might
[01:25:28.080 --> 01:25:30.880]   perform well during the ensemble. So that way we were
[01:25:30.880 --> 01:25:34.880]   able to generate some good predictions and a better score.
[01:25:35.400 --> 01:25:40.120]   So also shown here like we used five models which which includes
[01:25:40.120 --> 01:25:43.800]   three different variants of efficient net, YOLO and retina
[01:25:43.800 --> 01:25:50.320]   net. So summarizing it further for efficient net D5, we trained
[01:25:50.320 --> 01:25:54.640]   in on just training data, which was provided in the competition.
[01:25:54.640 --> 01:25:59.160]   And the MSI used was 512. Apart from that, it was trained in two
[01:25:59.160 --> 01:26:02.360]   stages, just like classification models, like we would first
[01:26:02.360 --> 01:26:06.080]   train it on higher learning rate and then fine tune it on lower
[01:26:06.080 --> 01:26:09.640]   learning rate. So that way we were able to get quite a boost.
[01:26:09.640 --> 01:26:13.240]   Apart from that, we also use EMA, which is also known as
[01:26:13.240 --> 01:26:18.440]   exponential moving average. So yeah, for efficient net D3
[01:26:18.440 --> 01:26:22.120]   coming on to it, like we trained it on training data as well as
[01:26:22.120 --> 01:26:25.640]   public test data pseudo levels, you must be wondering like how
[01:26:25.640 --> 01:26:30.720]   did we generate a pseudo level. So firstly, we took four models
[01:26:30.760 --> 01:26:34.560]   which were two efficient net that and two YOLO models. So we
[01:26:34.560 --> 01:26:38.640]   first trained them on just training data with normal
[01:26:38.640 --> 01:26:43.240]   training, and we ensemble them and use their public test data
[01:26:43.240 --> 01:26:46.280]   predictions as pseudo levels. So we first generated their
[01:26:46.280 --> 01:26:49.040]   bounding boxes and applied them in for the training for
[01:26:49.040 --> 01:26:52.680]   efficient net D3 and YOLO variants. So for efficient net
[01:26:52.680 --> 01:26:59.280]   D3, we used its default size, which was 896 x 896. And it was
[01:26:59.280 --> 01:27:03.600]   also trained like efficient that in two stages, which I mentioned
[01:27:03.600 --> 01:27:10.360]   earlier. For YOLO variants, we used YOLO v5 LX, as well as
[01:27:10.360 --> 01:27:13.840]   YOLO v5X. Both of them were trained on pseudo levels as well
[01:27:13.840 --> 01:27:18.560]   as training data. And both were trained on 640 MSIs. Apart from
[01:27:18.560 --> 01:27:21.480]   that, we also took some of the images which were not having
[01:27:21.480 --> 01:27:27.200]   bounding box like 20% or something. So for for YOLO
[01:27:27.200 --> 01:27:32.360]   models, like including them, improve this code, as well as in
[01:27:32.360 --> 01:27:37.080]   both CV as well as on LB. So we proceeded with that. Apart from
[01:27:37.080 --> 01:27:40.640]   that for retina net model, we use the baseline of ResNxt 101.
[01:27:40.640 --> 01:27:45.720]   And MSIs use was maximum which we could get for height and
[01:27:45.720 --> 01:27:50.520]   width, which was when triple 3,800. So yeah, in this model,
[01:27:50.520 --> 01:27:53.720]   we didn't use pseudo level set as they were performing that
[01:27:53.720 --> 01:27:58.880]   well on this. And we didn't include much models as our sole
[01:27:58.880 --> 01:28:03.040]   aim was to get a decent score and to finish all our inference
[01:28:03.040 --> 01:28:06.880]   in the given amount of time. So we stick with these five models.
[01:28:06.880 --> 01:28:10.280]   Yeah, just I just wanted to point out people sometimes
[01:28:10.280 --> 01:28:13.320]   complain that Kaggle solutions aren't useful. And this is my
[01:28:13.320 --> 01:28:17.440]   goal to bring out these these little details you you're using
[01:28:17.440 --> 01:28:21.960]   a model and on sampling a few models outputs on the test
[01:28:21.960 --> 01:28:27.520]   dataset and using that as pseudo labels. The like, where else
[01:28:27.520 --> 01:28:30.200]   can we find these tricks apart from the all all of the other
[01:28:30.200 --> 01:28:32.800]   ones that you discussed earlier, I just wanted to point out that
[01:28:32.800 --> 01:28:36.680]   only top Kaggle solutions have all of these amazing tricks.
[01:28:36.680 --> 01:28:42.120]   Yeah, so coming on to the post processing, I would say it was
[01:28:42.120 --> 01:28:45.880]   one of our secret sauce we used, which was not used by many of
[01:28:45.880 --> 01:28:50.200]   other teams. So it gave us a good amount of boost in both
[01:28:50.200 --> 01:28:52.600]   Hoof score as well as leaderboard. So to come up with
[01:28:52.600 --> 01:28:55.920]   this idea of post processing, like I had to go through a lot
[01:28:55.920 --> 01:28:59.040]   of stuff digging up post from past competitions,
[01:28:59.040 --> 01:29:02.000]   understanding the metric in depth as it is necessary if you
[01:29:02.000 --> 01:29:04.960]   are doing post processing to prevent any overfitting in
[01:29:04.960 --> 01:29:08.640]   publicly to go to something. So yeah, thereafter, I came up with
[01:29:08.640 --> 01:29:11.520]   this an idea of how about merging our object detection
[01:29:11.520 --> 01:29:17.600]   models with the classification model. So as, as I mentioned,
[01:29:17.600 --> 01:29:20.680]   we trained two different type of models for image level
[01:29:20.680 --> 01:29:25.680]   predictions, which were for opacity and non prediction. So
[01:29:25.680 --> 01:29:30.760]   for that, as we didn't have any end to end model for these,
[01:29:30.760 --> 01:29:34.760]   like some of the top teams, what they did was they trained a
[01:29:34.760 --> 01:29:38.600]   single model for all the six classes to predict all the six
[01:29:38.600 --> 01:29:42.280]   classes. So as we didn't do that, so it was quite a
[01:29:42.280 --> 01:29:46.080]   necessity for us to combine all those predictions together. So
[01:29:46.080 --> 01:29:50.280]   the best way with which I could come up was the idea of somehow
[01:29:50.280 --> 01:29:54.440]   ensemble these confident confidence score of bounding
[01:29:54.440 --> 01:29:58.480]   boxes along with the binary prediction. So for that, I
[01:29:58.480 --> 01:30:02.480]   tried several ensemble strategies like taking out their
[01:30:02.480 --> 01:30:07.000]   mean or weighted mean, then I also tried geometric mean, but
[01:30:07.000 --> 01:30:11.280]   none of them perform as well as power blend. So basically,
[01:30:11.280 --> 01:30:16.200]   power blend is something like, you will simply give them both
[01:30:16.200 --> 01:30:19.840]   predictions of it, but that way, it would be given in a power
[01:30:19.840 --> 01:30:23.960]   form, like it is shown in the block diagram as well as you
[01:30:23.960 --> 01:30:30.320]   could see in the red color. Yeah, sure. So as you could
[01:30:30.320 --> 01:30:34.320]   also see in the formula, like, what we did was for opacity
[01:30:34.320 --> 01:30:39.240]   predictions, firstly, we took, we given it 0.5 bits for both
[01:30:39.240 --> 01:30:43.840]   the models, but then shifted the weights, depending upon the
[01:30:43.840 --> 01:30:47.600]   off score we were getting after the ensemble. So that way, we
[01:30:47.600 --> 01:30:52.320]   were able to maximize our predictions for opacity in a
[01:30:52.320 --> 01:30:57.640]   way like giving 0.8 weights to opacity predictions, and 0.2
[01:30:57.640 --> 01:31:01.800]   weights of one minus none of prediction because opacity could
[01:31:01.800 --> 01:31:05.920]   be treated as the negatively correlated with non prediction.
[01:31:05.920 --> 01:31:09.360]   So that way, we did power blending for opacity
[01:31:09.360 --> 01:31:12.760]   predictions. Further, then I thought like, if we could do
[01:31:12.760 --> 01:31:16.200]   this for opacity predictions, then we may also do that for
[01:31:16.200 --> 01:31:19.960]   non predictions as well. So I come up with another ensemble,
[01:31:19.960 --> 01:31:25.520]   which is mentioned below for non prediction as well. So that way
[01:31:25.520 --> 01:31:29.360]   we were able to get quite a boost using power blend in a
[01:31:29.360 --> 01:31:33.600]   final solution without any excessive training or inference
[01:31:33.600 --> 01:31:36.920]   time. Like it was just five simple code of lines.
[01:31:36.920 --> 01:31:43.000]   So I'd point again, the audience to your GitHub repository as
[01:31:43.000 --> 01:31:47.080]   well. It's linked in your write up as a line point them to put
[01:31:47.080 --> 01:31:51.720]   up those, those are great resources. I think that that
[01:31:51.720 --> 01:31:58.120]   completes the solution. Yeah, yeah. So I'll try to summarize
[01:31:58.120 --> 01:32:00.560]   what I understood. Please correct me if I'm wrong. The
[01:32:00.560 --> 01:32:04.800]   tricks that worked really well were first of all, the
[01:32:04.800 --> 01:32:09.440]   competition master Tawara's trick around attention head.
[01:32:09.440 --> 01:32:12.640]   And I'm sorry if I'm glancing over a few details, but these
[01:32:12.640 --> 01:32:14.880]   things really stood out to me. So I'm just pointing them out.
[01:32:14.880 --> 01:32:18.600]   You took the efficient models, you started with those after
[01:32:18.600 --> 01:32:22.360]   brute forcing a lot. You took their backbone, change their
[01:32:22.360 --> 01:32:26.120]   heads, and added two different loss values. And you were using
[01:32:26.120 --> 01:32:28.760]   these for different models. And there were some changes here and
[01:32:28.760 --> 01:32:33.560]   there you tried different image sizes. After that, there was an
[01:32:33.560 --> 01:32:37.640]   ensemble and in the ensemble, you were also using a classifier
[01:32:37.640 --> 01:32:42.720]   for just the non class and a weighted mean across different
[01:32:42.720 --> 01:32:46.760]   models. And for the post processing, I think it's called
[01:32:46.760 --> 01:32:48.400]   power ensemble, this technique, correct?
[01:32:48.400 --> 01:32:55.240]   Yes. And also to add the noisy student training was the most
[01:32:55.240 --> 01:32:57.360]   effective part. Yeah.
[01:32:57.360 --> 01:33:02.320]   Got it. Yeah, if I'm sure if I was able to understand most
[01:33:02.320 --> 01:33:05.240]   people, all of them would have understood the solution. So
[01:33:05.240 --> 01:33:10.480]   thanks to that. I know we've run out of time. So I'll quickly
[01:33:10.480 --> 01:33:14.600]   end with a few questions that I always ask. Aishwaryaan.
[01:33:14.600 --> 01:33:16.120]   Yeah.
[01:33:16.120 --> 01:33:19.800]   Sorry, I was just making sure that nothing's left in the
[01:33:19.800 --> 01:33:20.240]   solution.
[01:33:20.240 --> 01:33:24.560]   Yeah, no. Yeah, we went over most of the important details.
[01:33:24.800 --> 01:33:29.440]   We went over all the details. Everything. We went over
[01:33:29.440 --> 01:33:30.800]   everything like that's what we did.
[01:33:30.800 --> 01:33:36.040]   Awesome. Great. So the questions I asked towards the end, I'll
[01:33:36.040 --> 01:33:40.720]   try to wrap up as fast as I can. What's your best advice to
[01:33:40.720 --> 01:33:44.160]   someone starting their CAD tech journey? To all of you?
[01:33:44.160 --> 01:33:51.200]   My best advice would be, you know, to, like, first, as Nisha
[01:33:51.200 --> 01:33:54.360]   has said, like starting with Titanic, small problems first.
[01:33:54.360 --> 01:34:01.840]   And then you know, you should start having, you know, start
[01:34:01.840 --> 01:34:04.560]   tackling problems that are related to your area of
[01:34:04.560 --> 01:34:07.520]   research. For me, like, I am very interested in computer
[01:34:07.520 --> 01:34:11.360]   vision. I'm fascinated by it. So I started exploring a few
[01:34:11.360 --> 01:34:17.360]   computer vision images. Then I came to that's how I came to
[01:34:17.360 --> 01:34:21.520]   this same COVID competition. Like you should always start out
[01:34:21.520 --> 01:34:24.240]   with your interest because in that way, you can keep doing
[01:34:24.240 --> 01:34:27.440]   experiments and you won't get tired of like, because Kaggle
[01:34:27.440 --> 01:34:30.640]   competitions, they would offer over three months and it's very
[01:34:30.640 --> 01:34:33.520]   easy to you know, like, give it up in the middle, like I have
[01:34:33.520 --> 01:34:38.320]   done it previously. So the best way to get started would be to
[01:34:38.320 --> 01:34:42.480]   you know, take a competition that's, you know, very closely
[01:34:42.480 --> 01:34:47.840]   related to your area of interest. That's what I would
[01:34:47.840 --> 01:34:50.640]   say. And yeah, never give up, like keep doing experiments,
[01:34:50.640 --> 01:34:55.200]   like even if nothing is working, keep doing like, we, like, I
[01:34:55.200 --> 01:35:01.280]   would like to share that after merging, we jumped towards the
[01:35:01.280 --> 01:35:04.080]   top of the leaderboard in quite a few days, like within three
[01:35:04.080 --> 01:35:07.440]   days of merging three or four days of merging, we jumped up
[01:35:07.440 --> 01:35:09.680]   towards the top of the leaderboard. But then after that
[01:35:09.680 --> 01:35:13.280]   for like, 20, almost 20 to 25 days, we did not have any
[01:35:13.280 --> 01:35:16.800]   progress. We were stuck there, like, but we still did not give
[01:35:16.800 --> 01:35:21.440]   up. And finally, we kept doing and doing and towards the end,
[01:35:21.440 --> 01:35:26.000]   we were able to achieve public LV second place.
[01:35:26.000 --> 01:35:29.280]   Unfortunately, we again, dropped down to third on the
[01:35:29.280 --> 01:35:32.960]   final day in the public leaderboard. And finally, in the
[01:35:32.960 --> 01:35:35.040]   private, we came in fifth.
[01:35:37.040 --> 01:35:40.800]   Yeah, I love your advice on not not giving up. And thanks for
[01:35:40.800 --> 01:35:43.680]   the back back story on your journey. Nishty, what's your
[01:35:43.680 --> 01:35:44.320]   best advice?
[01:35:44.320 --> 01:35:48.560]   Like, I would say it would depend upon everyone's like, it
[01:35:48.560 --> 01:35:51.760]   was different for me, it might be different for someone else.
[01:35:51.760 --> 01:35:55.440]   But to conclude, I would also only say like, don't lose hope
[01:35:55.440 --> 01:35:59.680]   early. What many people do, they directly jump on the onto the
[01:35:59.680 --> 01:36:03.920]   featured competitions, and they might not get better rank. So
[01:36:03.920 --> 01:36:08.080]   they would simply lose hope and then starting like, we don't
[01:36:08.080 --> 01:36:11.760]   know how to begin and all that stuff. So I would say, like,
[01:36:11.760 --> 01:36:15.200]   firstly, start with the basics, and start with some tutorial
[01:36:15.200 --> 01:36:18.560]   competitions. Also, also, I would mention, like, from my
[01:36:18.560 --> 01:36:21.440]   point of view, I also started in the beginning, like doing
[01:36:21.440 --> 01:36:24.960]   some featured competitions, then I did pretty bad in that,
[01:36:24.960 --> 01:36:29.520]   like, I was placing in 90 to 95% of the people which were in
[01:36:29.520 --> 01:36:36.480]   the, like, last of 5%. So it was a quite rough journey for me
[01:36:36.480 --> 01:36:40.960]   during the beginning. But then I, the only thing which I did
[01:36:40.960 --> 01:36:45.040]   was I never lost hope, like giving up early or something, I
[01:36:45.040 --> 01:36:48.800]   stick to it, I did some more tutorial competitions, I stick
[01:36:48.800 --> 01:36:52.400]   to some past competitions, I did more courses and all that
[01:36:52.400 --> 01:36:56.800]   stuff. And then yeah, then things get started getting
[01:36:56.800 --> 01:37:00.480]   better. I used to get some bronze medal, silver medals,
[01:37:00.480 --> 01:37:04.000]   lots of silver, silver medals, actually, like 1012 or
[01:37:04.000 --> 01:37:10.400]   something like if you could see. So yeah, now I think, like, I
[01:37:10.400 --> 01:37:14.160]   always used to give 10 to 12 hours daily in the beginning,
[01:37:14.160 --> 01:37:18.080]   doing some EDF and EDN stuff. So you could also begin with
[01:37:18.080 --> 01:37:22.320]   that, publishing some notebooks and keep yourself engaged in
[01:37:22.320 --> 01:37:25.840]   discussions, have some good people around you connect with
[01:37:25.840 --> 01:37:29.840]   some people on LinkedIn, work with them, like you could do
[01:37:29.840 --> 01:37:33.520]   all this stuff. For me, it was like I started with two of my
[01:37:33.520 --> 01:37:40.080]   college friends. So it was quite good for me. Like, I didn't
[01:37:40.080 --> 01:37:44.000]   have much like long period in Kaggle. Firstly, I had some
[01:37:44.000 --> 01:37:47.200]   college friends, then I met some friends through Kaggle,
[01:37:47.200 --> 01:37:51.120]   which with which I also recently met with them in real
[01:37:51.120 --> 01:37:54.240]   life. So yeah, it's quite fun, if you would stick to it.
[01:37:54.960 --> 01:37:57.600]   Yeah, the teaming up aspect is always a lot of fun.
[01:37:57.600 --> 01:38:01.440]   Another thing I would small thing I would like to add was
[01:38:01.440 --> 01:38:04.560]   you should always try to write your own code like that will
[01:38:04.560 --> 01:38:07.600]   help you a lot. Like in this competition, like every code
[01:38:07.600 --> 01:38:11.200]   that we written was almost written by us from scratch. Like,
[01:38:11.200 --> 01:38:14.160]   so if that happens, like if you have some problems in your
[01:38:14.160 --> 01:38:17.680]   training, and if there is some problems in your pipeline, you
[01:38:17.680 --> 01:38:20.960]   will figure it out quite easily. And that helps you a lot.
[01:38:22.240 --> 01:38:25.680]   Yeah, but we all all we all of us start from just forking a
[01:38:25.680 --> 01:38:28.720]   kernel changing hyper parameters and you see the thrill of going
[01:38:28.720 --> 01:38:32.080]   up that's that's okay to do. With time you'll start to
[01:38:32.080 --> 01:38:35.200]   understand that you need to just take those ideas and add to
[01:38:35.200 --> 01:38:37.680]   your code base. So it's only fine to copy also.
[01:38:37.680 --> 01:38:39.780]   Yeah.
[01:38:39.780 --> 01:38:42.480]   Shivam, your best advice.
[01:38:42.480 --> 01:38:46.720]   Yeah, I mean, I totally agree with Nish and Ayutman whatever
[01:38:46.720 --> 01:38:49.680]   they have told. So you start with some developer competitions
[01:38:49.680 --> 01:38:52.880]   like Titanic one, understand the platform first. I mean,
[01:38:52.880 --> 01:38:55.280]   that's very important because if you don't understand the
[01:38:55.280 --> 01:38:58.400]   platform and how to use it, you will get frustrated and you
[01:38:58.400 --> 01:39:02.000]   will give up. That's the truth. So even if the platform Kaggle
[01:39:02.000 --> 01:39:05.200]   platform is very easy to use, but initially if you haven't
[01:39:05.200 --> 01:39:07.840]   tried it, you will get frustrated and sometimes maybe
[01:39:07.840 --> 01:39:11.040]   you will not understand what to do. So you have to understand
[01:39:11.040 --> 01:39:14.960]   the platform, give some time and not try one competition like
[01:39:14.960 --> 01:39:20.080]   Titanic one and get your hands on on tabular competitions,
[01:39:20.080 --> 01:39:22.480]   maybe some other tabular competitions and then you get
[01:39:22.480 --> 01:39:26.240]   some confidence that yes, you can do some missions, you can
[01:39:26.240 --> 01:39:30.080]   train models on Kaggle, try to take a problem which interests
[01:39:30.080 --> 01:39:35.840]   you and then the regular part which is basically taking a
[01:39:35.840 --> 01:39:39.280]   base kernel which is basically a baseline for a competition
[01:39:39.280 --> 01:39:42.160]   and doing changes on top of that learning from the previous
[01:39:42.160 --> 01:39:45.120]   competition. So those things will keep on coming when you
[01:39:45.120 --> 01:39:48.560]   basically spend some time on Kaggle platform. So just start
[01:39:48.560 --> 01:39:52.080]   with Kaggle platform with the basic competition and slowly
[01:39:52.080 --> 01:39:54.800]   you will move. Never give up. You will definitely learn and
[01:39:54.800 --> 01:39:57.760]   you will do something good in that. Not the best, but of
[01:39:57.760 --> 01:40:01.520]   course, very good in that. Awesome. Thanks. Thanks for
[01:40:01.520 --> 01:40:04.240]   sharing that. Let me quickly make sure the audience know how
[01:40:04.240 --> 01:40:10.240]   to find you. So this is Ayushwant's Kaggle profile. Make
[01:40:10.240 --> 01:40:12.400]   sure you go there because you'll see a lot of more gold
[01:40:12.400 --> 01:40:18.640]   medals coming up very soon. I hope he's also on Twitter at
[01:40:18.640 --> 01:40:25.280]   Ayushman75139217. Quite the username. He's also on LinkedIn
[01:40:25.280 --> 01:40:30.560]   and same for other both of our guests. Nischay is he mentioned
[01:40:30.560 --> 01:40:33.360]   he has a lot of silver medals and also a few gold medals.
[01:40:33.360 --> 01:40:36.640]   That's always incredible to see. Nischay is also on LinkedIn.
[01:40:37.440 --> 01:40:40.960]   You can find him by just looking up his username and he is
[01:40:40.960 --> 01:40:50.640]   nischay_twt on Twitter. Shivam just competes. He doesn't look
[01:40:50.640 --> 01:40:54.000]   at the other sides of Kaggle. This is his Kaggle profile. He
[01:40:54.000 --> 01:40:59.680]   is shmgupta on Twitter and he's also shivamgupta on LinkedIn.
[01:40:59.680 --> 01:41:01.840]   Any other platforms that you all would like to mention?
[01:41:04.720 --> 01:41:08.480]   I mean, no, I think that's all. I mean, we usually can be found
[01:41:08.480 --> 01:41:13.360]   in Kaggle easily. I think now. So I mean, that's all. In Kaggle
[01:41:13.360 --> 01:41:17.920]   we have mentioned it. Yeah. I was usually more active in GitHub
[01:41:17.920 --> 01:41:24.800]   before this. Yes. Yes. Awesome. Thank you so much to all of you.
[01:41:24.800 --> 01:41:27.680]   We also ran over time. So thanks for sticking to that and
[01:41:27.680 --> 01:41:30.480]   thanks for saving your solution and your journey. Thank you so
[01:41:30.480 --> 01:41:35.440]   much. Thanks. Thank you for inviting us. Thanks a lot. Have
[01:41:35.440 --> 01:41:36.240]   a nice day. Bye.
[01:41:36.240 --> 01:41:37.080]   - Bye.
[01:41:37.080 --> 01:41:37.920]   - Yeah.
[01:41:37.920 --> 01:41:40.320]   So, I just.

