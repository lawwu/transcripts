
[00:00:00.000 --> 00:00:05.360]   If you think of professional athletes, they do a lot of training.
[00:00:05.360 --> 00:00:07.880]   Calculus Community is over 5 million people now.
[00:00:07.880 --> 00:00:12.640]   The people at the top are... it's just like gone from more of an amateur sport to more
[00:00:12.640 --> 00:00:14.200]   of a professional sport.
[00:00:14.200 --> 00:00:18.240]   And so I think the difference isn't that the results would be better, but the top performers
[00:00:18.240 --> 00:00:20.960]   now would get to those results faster.
[00:00:20.960 --> 00:00:24.800]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:24.800 --> 00:00:26.440]   models work in the real world.
[00:00:26.440 --> 00:00:28.480]   I'm your host, Lukas Biewald.
[00:00:28.480 --> 00:00:32.520]   Anthony Goldbloom is the co-founder and CEO of Kaggle.
[00:00:32.520 --> 00:00:37.000]   Kaggle started as a competitions platform and is probably the largest community of data
[00:00:37.000 --> 00:00:40.080]   scientists and machine learning practitioners on the planet.
[00:00:40.080 --> 00:00:44.080]   They also now do datasets and kernels, and we're going to talk a lot about that.
[00:00:44.080 --> 00:00:46.040]   I'm super excited to talk to him today.
[00:00:46.040 --> 00:00:50.960]   I remember you were talking about deep learning before I'd really heard about it.
[00:00:50.960 --> 00:00:53.800]   You're kind of the first person I knew that was really thinking about it.
[00:00:53.800 --> 00:00:58.360]   I think it was because you saw that people were winning Kaggle competitions with kind
[00:00:58.360 --> 00:01:02.200]   of new methods that were less in the mainstream at the time.
[00:01:02.200 --> 00:01:06.680]   And I'm kind of wondering if on Kaggle you're seeing people doing things that you don't
[00:01:06.680 --> 00:01:11.280]   think are in the academic mainstream, or if you're seeing things that kind of point to
[00:01:11.280 --> 00:01:15.320]   what you think will be in sort of mainstream production in the next few years.
[00:01:15.320 --> 00:01:16.320]   It's a good question.
[00:01:16.320 --> 00:01:21.440]   I mean, to be honest, the most glaring thing that we see on Kaggle that is not fashionable
[00:01:21.440 --> 00:01:25.600]   in academia is that we're still seeing gradient boosting machines do very well on a lot of
[00:01:25.600 --> 00:01:27.560]   structured data problems, right?
[00:01:27.560 --> 00:01:32.720]   And there's not a lot of research attention on things like gradient boosting machines
[00:01:32.720 --> 00:01:33.720]   now.
[00:01:33.720 --> 00:01:38.200]   And it begs the question, like, have we done everything we can there?
[00:01:38.200 --> 00:01:43.640]   Or is it an area where there is still more that can be done, but it's just not the trendy
[00:01:43.640 --> 00:01:44.640]   thing?
[00:01:44.640 --> 00:01:47.640]   It's hard to get papers published, and so it's just not getting the attention.
[00:01:47.640 --> 00:01:53.320]   To be honest, that's the most glaring difference we see between what is doing well on Kaggle
[00:01:53.320 --> 00:01:56.200]   and what is fashionable in the academic literature.
[00:01:56.200 --> 00:01:58.220]   Well, that's really interesting.
[00:01:58.220 --> 00:02:03.040]   We are seeing some novel uses of things like BERT and WaveNet.
[00:02:03.040 --> 00:02:06.520]   We've seen WaveNet be used on forecasting problems.
[00:02:06.520 --> 00:02:12.200]   We've seen BERT do really well on chemical informatics type problems that have to do
[00:02:12.200 --> 00:02:14.200]   with gene sequences and things like that.
[00:02:14.200 --> 00:02:21.360]   So we're seeing perhaps unknown use cases for some well-established technologies.
[00:02:21.360 --> 00:02:26.640]   But I think that just the lack of academic focus on these gradient boosting machine algorithms
[00:02:26.640 --> 00:02:30.520]   is probably the biggest glaring distinction that we see.
[00:02:30.520 --> 00:02:37.000]   And so in a structured data competition where gradient boost wins, what are the details
[00:02:37.000 --> 00:02:39.280]   that the winners do to win those competitions?
[00:02:39.280 --> 00:02:41.880]   Is it still feature engineering or is there other stuff?
[00:02:41.880 --> 00:02:42.880]   Exactly.
[00:02:42.880 --> 00:02:48.400]   So it's finding clever features that other people aren't finding.
[00:02:48.400 --> 00:02:51.080]   And so perhaps that's it.
[00:02:51.080 --> 00:02:52.960]   That's the reason.
[00:02:52.960 --> 00:02:59.200]   Or maybe that's where you could have more academic focus.
[00:02:59.200 --> 00:03:03.600]   There is no doubt in my mind there are things you could do to automate feature engineering
[00:03:03.600 --> 00:03:04.600]   to some extent.
[00:03:04.600 --> 00:03:08.480]   And maybe these are the things that companies like DataRobot and H2O are doing where they're
[00:03:08.480 --> 00:03:09.800]   baking in these recipes.
[00:03:09.800 --> 00:03:17.240]   So as an example, you see a date or a timestamp, for instance, that is an incredibly rich field
[00:03:17.240 --> 00:03:19.000]   that can become like 70 things.
[00:03:19.000 --> 00:03:24.240]   Let's say you're doing traffic forecasting, a timestamp can be turned into, is it rush
[00:03:24.240 --> 00:03:29.360]   hour, is it not rush hour, is it a weekend, is it not weekend, is it summer, is it winter?
[00:03:29.360 --> 00:03:35.400]   Therefore does it change the probability of rain on a given day or adverse snow on the
[00:03:35.400 --> 00:03:37.360]   road and things like that?
[00:03:37.360 --> 00:03:41.600]   There are definitely things that could be done to automate components or help with some
[00:03:41.600 --> 00:03:43.400]   of the heavy lifting of feature engineering.
[00:03:43.400 --> 00:03:47.640]   And so that could be an area of focus or perhaps it is.
[00:03:47.640 --> 00:03:52.000]   Maybe it's not an academic area, but it's the kind of things that H2O and DataRobot
[00:03:52.000 --> 00:03:54.960]   and companies like that can build into their products.
[00:03:54.960 --> 00:03:57.000]   Well, that's so interesting.
[00:03:57.000 --> 00:04:03.000]   So if you roll back 10 years or you took a competition at the start of Kaggle and then
[00:04:03.000 --> 00:04:06.640]   you take it out to today, how much better do people do?
[00:04:06.640 --> 00:04:07.640]   Do they even do better?
[00:04:07.640 --> 00:04:08.640]   Do you think you could win?
[00:04:08.640 --> 00:04:13.520]   Could you take modern tools and win every competition at the start of Kaggle?
[00:04:13.520 --> 00:04:17.560]   I guess my question is, has the feature engineering really improved?
[00:04:17.560 --> 00:04:22.440]   So the thing when Kaggle first got started, people used to use all sorts of things like
[00:04:22.440 --> 00:04:28.120]   support vector machines, self-organizing maps, really a large range of things.
[00:04:28.120 --> 00:04:32.720]   The first big development that we saw or the first big contribution I think Kaggle made
[00:04:32.720 --> 00:04:41.320]   is we made it very clear that random forest was the best algorithm for actually most problems
[00:04:41.320 --> 00:04:44.240]   at that time.
[00:04:44.240 --> 00:04:51.160]   And then let's say around the 2014, yeah, about 2014, Tianqi Chen at the University
[00:04:51.160 --> 00:04:53.920]   of Washington released XGBoost.
[00:04:53.920 --> 00:05:01.000]   I think it was always thought that gradient boosting machines should be better.
[00:05:01.000 --> 00:05:04.840]   It's a very smart approach to ensembling decision trees.
[00:05:04.840 --> 00:05:05.840]   They should be better.
[00:05:05.840 --> 00:05:11.880]   They were very, very finicky before XGBoost and once Tianqi Chen launched XGBoost, it
[00:05:11.880 --> 00:05:14.120]   really took over from random forest.
[00:05:14.120 --> 00:05:19.800]   I'd say unlike the difference between deep neural networks on computer vision problems
[00:05:19.800 --> 00:05:26.120]   versus random forest, the XGBoost increase was not a huge increase, but it was enough.
[00:05:26.120 --> 00:05:32.240]   And so to be honest, unstructured data problems, I think that's probably where most of the
[00:05:32.240 --> 00:05:36.520]   software driven improvements have come from.
[00:05:36.520 --> 00:05:42.500]   It's probably the case if you took a problem from the early days of Kaggle that was one
[00:05:42.500 --> 00:05:46.760]   with random forest and you reran it today, I think you'd get a little bit of a better
[00:05:46.760 --> 00:05:48.640]   answer because of the XGBoost.
[00:05:48.640 --> 00:05:53.100]   I don't actually think the way people are doing feature engineering has really improved
[00:05:53.100 --> 00:05:54.100]   very much.
[00:05:54.100 --> 00:05:58.200]   However, I do think that you would get, what we always say is that Kaggle competitions
[00:05:58.200 --> 00:06:03.440]   typically the top teams all converge on about the same level of accuracy and the intuition
[00:06:03.440 --> 00:06:06.880]   there is there's only so much signal in a dataset, right?
[00:06:06.880 --> 00:06:11.040]   And so people compete to the point where they've extracted all the signal.
[00:06:11.040 --> 00:06:16.400]   And so I believe in the early Kaggle competitions, people always found the key features and they
[00:06:16.400 --> 00:06:18.080]   got all the signal out of the dataset.
[00:06:18.080 --> 00:06:22.440]   I think what might happen is it had happened a fair bit faster now.
[00:06:22.440 --> 00:06:25.660]   If you think of professional athletes, they do a lot of training.
[00:06:25.660 --> 00:06:28.220]   Kaggle's community is over 5 million people now.
[00:06:28.220 --> 00:06:32.960]   The people at the top are, it's just like gone from more of an amateur sport to more
[00:06:32.960 --> 00:06:34.500]   of a professional sport, right?
[00:06:34.500 --> 00:06:40.340]   And so I think people, I think the difference isn't that the results would be better, but
[00:06:40.340 --> 00:06:43.580]   the top performers now would get to those results faster.
[00:06:43.580 --> 00:06:44.580]   Interesting.
[00:06:44.580 --> 00:06:47.420]   Is my guess.
[00:06:47.420 --> 00:06:53.280]   There are definitely, we ran a challenge with Pete Warden, who's now at Google.
[00:06:53.280 --> 00:06:56.960]   He was running a company called Jetpack and I think it was a distinguished between cats
[00:06:56.960 --> 00:06:57.960]   versus dogs.
[00:06:57.960 --> 00:07:03.680]   I think we did that, if I remember correctly, we did that before deep neural networks and
[00:07:03.680 --> 00:07:07.480]   afterwards and obviously saw a fairly big lift.
[00:07:07.480 --> 00:07:11.440]   We ran a challenge with the Allen Institute for artificial intelligence on solving an
[00:07:11.440 --> 00:07:12.800]   eighth grade science quiz.
[00:07:12.800 --> 00:07:16.080]   This was before the BERT innovations.
[00:07:16.080 --> 00:07:21.440]   People were getting about 60% accuracy using information retrieval methods.
[00:07:21.440 --> 00:07:25.680]   Allen have now run BERT like solutions on it and they're getting about 90%.
[00:07:25.680 --> 00:07:36.040]   So you definitely see anything, before deep learning and after deep learning, you see
[00:07:36.040 --> 00:07:40.760]   very large changes in results for sure.
[00:07:40.760 --> 00:07:43.720]   And I would think that on some structured data, like language models and things would
[00:07:43.720 --> 00:07:45.600]   really make a big difference, right?
[00:07:45.600 --> 00:07:46.600]   Yeah.
[00:07:46.600 --> 00:07:52.440]   I mean, so you definitely have structured data where you have fields that are, you know,
[00:07:52.440 --> 00:07:53.800]   text fields, et cetera, et cetera.
[00:07:53.800 --> 00:08:01.320]   And so maybe you use language models as a, you know, to create features that ultimately
[00:08:01.320 --> 00:08:04.080]   go into, I mean, that's a common strategy.
[00:08:04.080 --> 00:08:09.200]   We run multimodal competitions sometimes where you have images and someone will run a convolutional
[00:08:09.200 --> 00:08:14.320]   neural network in order to come out with features out of that image that just, they then get
[00:08:14.320 --> 00:08:19.320]   fed into a gradient boosting machine classifier.
[00:08:19.320 --> 00:08:20.760]   So that definitely happens.
[00:08:20.760 --> 00:08:26.760]   And so just as it gets done for images that might be part of a multimodal dataset, of
[00:08:26.760 --> 00:08:33.640]   course it can happen as well when there are columns that are text or data sources for
[00:08:33.640 --> 00:08:35.920]   a challenge that are text.
[00:08:35.920 --> 00:08:39.400]   Are there interesting data augmentation strategies that you see people using?
[00:08:39.400 --> 00:08:43.640]   Because I feel like that's often people talk about that as a major area of innovation.
[00:08:43.640 --> 00:08:45.440]   Do you see that on Kaggle?
[00:08:45.440 --> 00:08:49.240]   There's a couple of ways people win Kaggle competitions.
[00:08:49.240 --> 00:08:52.500]   In the world of structured data problems, it's clever feature engineering.
[00:08:52.500 --> 00:09:00.200]   It's very often that, let's say for natural language processing or for computer vision
[00:09:00.200 --> 00:09:04.840]   problems, it's clever data augmentation that wins competitions.
[00:09:04.840 --> 00:09:07.840]   Some of my favorite examples, the Kaggle community is really creative.
[00:09:07.840 --> 00:09:14.920]   I remember we, I think it was with Quora, we did a challenge around detecting insincere
[00:09:14.920 --> 00:09:15.920]   questions.
[00:09:15.920 --> 00:09:17.600]   I think that was the challenge.
[00:09:17.600 --> 00:09:21.840]   And the winning strategy there, or the thing that the winners did that others didn't do,
[00:09:21.840 --> 00:09:27.160]   is that what they would do is they would translate the question from English to some other language
[00:09:27.160 --> 00:09:28.640]   and then translate it back.
[00:09:28.640 --> 00:09:32.180]   Because if you use Google Translate, it's not a symmetrical translation, right?
[00:09:32.180 --> 00:09:37.740]   And so it was like a clever way to augment their dataset.
[00:09:37.740 --> 00:09:44.120]   So there are the standard techniques, rotating, et cetera, for images, but then there are
[00:09:44.120 --> 00:09:47.640]   clever creative tricks, like that translation one.
[00:09:47.640 --> 00:09:52.920]   There have also been a bunch of, one of the libraries that has really taken off on Kaggle,
[00:09:52.920 --> 00:09:57.800]   I think it was written by a Kaggle master, it's called Applementations, which makes it
[00:09:57.800 --> 00:10:03.080]   much easier to do sophisticated data augmentation, particularly on, that's designed for images.
[00:10:03.080 --> 00:10:06.880]   So it's definitely an area of a lot of focus in our community.
[00:10:06.880 --> 00:10:07.880]   That's really interesting.
[00:10:07.880 --> 00:10:11.680]   Do you think that with feature engineering and augmentation being so important, and then
[00:10:11.680 --> 00:10:17.120]   also like compute resources increasing, has overfitting in the training process become
[00:10:17.120 --> 00:10:21.040]   more of a problem over time if people come up with new ways to address that?
[00:10:21.040 --> 00:10:25.760]   I would think that if I'm just spinning through millions of possible feature combinations,
[00:10:25.760 --> 00:10:28.760]   I would end up overfitting on the validation data, right?
[00:10:28.760 --> 00:10:33.200]   Earlier I said there are two things required to win Kaggle competitions, tenacity and creativity.
[00:10:33.200 --> 00:10:39.040]   I actually think there are three, and that's been statistically wise, right?
[00:10:39.040 --> 00:10:44.560]   So one pattern we see all the time with people who are competing in their first Kaggle competition
[00:10:44.560 --> 00:10:51.200]   is they'll submit, they'll be top of the public leaderboard, and then what we do is we, at
[00:10:51.200 --> 00:10:58.480]   the end of a competition, we rescore everybody's algorithms on a second test dataset that they've
[00:10:58.480 --> 00:11:00.520]   never seen before.
[00:11:00.520 --> 00:11:06.280]   And also, let's say you submitted 150 models, you have to select two that get rescored,
[00:11:06.280 --> 00:11:07.440]   right?
[00:11:07.440 --> 00:11:12.040]   And so a very, very, very common pattern is that somebody will be on top of the public
[00:11:12.040 --> 00:11:17.680]   leaderboard, we then rescore them on the second test dataset, and then they've dropped like
[00:11:17.680 --> 00:11:20.080]   90 places.
[00:11:20.080 --> 00:11:26.920]   And it turns out to be an amazing way to learn the lesson of overfitting because you're staying
[00:11:26.920 --> 00:11:32.120]   up till midnight, our leaderboards turn over midnight, UTC, that's when we reveal who actually
[00:11:32.120 --> 00:11:33.120]   won.
[00:11:33.120 --> 00:11:35.760]   And so you're staying up till midnight, you're hitting refresh, refresh, refresh.
[00:11:35.760 --> 00:11:36.760]   Am I in first?
[00:11:36.760 --> 00:11:38.920]   Am I in first?
[00:11:38.920 --> 00:11:42.560]   And you look at the public leaderboard, the private leaderboard when we switch over, you're
[00:11:42.560 --> 00:11:47.920]   not in first, you're not in second, you're in 80th position.
[00:11:47.920 --> 00:11:55.120]   That person who overfits, in a situation like that, they never overfit again, right?
[00:11:55.120 --> 00:12:00.800]   And I'd say that if anything, that was a bigger problem, coming back to your question, has
[00:12:00.800 --> 00:12:04.840]   overfitting become on the validations that become more prevalent, I would actually say
[00:12:04.840 --> 00:12:11.260]   it's become less prevalent because it has become well known that to do well on Kaggle,
[00:12:11.260 --> 00:12:13.800]   you really need to be careful of overfitting.
[00:12:13.800 --> 00:12:18.160]   But just one more kind of adjacent point I want to make here is you'd be surprised at
[00:12:18.160 --> 00:12:23.480]   the credentials of some of the people who have had that happen to them where they're
[00:12:23.480 --> 00:12:27.080]   coming first and then they drop to 100th spot.
[00:12:27.080 --> 00:12:31.240]   It makes me wonder how many of the world's research papers are actually overfitted, how
[00:12:31.240 --> 00:12:33.960]   many of the world's algorithms in production are overfitted.
[00:12:33.960 --> 00:12:39.160]   If experienced good people who have a lot of models in production or a lot of research
[00:12:39.160 --> 00:12:43.720]   papers out in the world, when they come to Kaggle in a place where they cannot overfit
[00:12:43.720 --> 00:12:50.560]   and they still overfit, it really does make me wonder what percentage of the world's algorithms
[00:12:50.560 --> 00:12:51.560]   are actually overfit.
[00:12:51.560 --> 00:12:54.360]   I mean, it sounds really tough, right?
[00:12:54.360 --> 00:12:59.720]   Because I feel like I imagine myself in this situation where I'm trying to be statistically
[00:12:59.720 --> 00:13:04.160]   rigorous, but I think when you're testing lots of features, I feel like it's tough to
[00:13:04.160 --> 00:13:07.440]   be perfectly statistically rigorous in that process.
[00:13:07.440 --> 00:13:11.600]   Do you have a sense of best practices that you tell people or where do people land that
[00:13:11.600 --> 00:13:12.760]   they don't overfit at all?
[00:13:12.760 --> 00:13:13.760]   I think it's cross-validation.
[00:13:13.760 --> 00:13:18.880]   I think cross-validation works well.
[00:13:18.880 --> 00:13:25.080]   There are techniques as a Google researcher, Moritz Hart, who has a kind of clever approach
[00:13:25.080 --> 00:13:32.480]   if you have a very small dataset and it's difficult to cross-validate where you don't
[00:13:32.480 --> 00:13:36.720]   tell yourself, you don't get told that your algorithm outperformed unless it outperformed
[00:13:36.720 --> 00:13:41.320]   by a statistically significant margin is the intuition there.
[00:13:41.320 --> 00:13:46.600]   So you either get no information or you outperformed by a meaningful amount.
[00:13:46.600 --> 00:13:47.600]   That actually sounds pretty good.
[00:13:47.600 --> 00:13:50.000]   Although you might miss stuff, I suppose.
[00:13:50.000 --> 00:13:52.160]   But yeah, that's an interesting idea.
[00:13:52.160 --> 00:13:53.160]   Yeah.
[00:13:53.160 --> 00:13:57.840]   I mean, I think cross-validation is still the right approach if you have a large enough
[00:13:57.840 --> 00:13:59.400]   dataset.
[00:13:59.400 --> 00:14:06.080]   And if you don't, Moritz's technique is at least the one I'm aware of that allows you
[00:14:06.080 --> 00:14:10.960]   to still prevent against overfitting.
[00:14:10.960 --> 00:14:11.960]   Interesting.
[00:14:11.960 --> 00:14:17.080]   Also, one thing I really wanted to ask you is, I feel like there's almost like this trope
[00:14:17.080 --> 00:14:22.360]   in the ML zeitgeist that's like, the real world isn't a Kaggle competition.
[00:14:22.360 --> 00:14:25.680]   I'm sure people ask you about this all the time, but I was curious what you think about
[00:14:25.680 --> 00:14:26.880]   that, what the differences are.
[00:14:26.880 --> 00:14:31.720]   Do Kaggle grandmasters tend to do well in the real world?
[00:14:31.720 --> 00:14:36.080]   What parts of Kaggle translates to actual applications?
[00:14:36.080 --> 00:14:37.080]   What parts don't?
[00:14:37.080 --> 00:14:38.080]   Yeah.
[00:14:38.080 --> 00:14:41.480]   So I guess there are two.
[00:14:41.480 --> 00:14:48.000]   First, bucket machine learning into, or building a successful productionized machine learning
[00:14:48.000 --> 00:14:49.320]   model into three stages.
[00:14:49.320 --> 00:14:54.880]   Firstly, you've got to turn your business problem into a machine learning problem.
[00:14:54.880 --> 00:15:00.040]   The second is you've got to train a classifier that's robust, et cetera, et cetera.
[00:15:00.040 --> 00:15:01.040]   And that really works.
[00:15:01.040 --> 00:15:05.600]   And then the third is you have to productionize that classifier.
[00:15:05.600 --> 00:15:09.440]   Kaggle is obviously phenomenal for number two.
[00:15:09.440 --> 00:15:18.600]   I really think if you do well on Kaggle or you train through Kaggle, you become as good
[00:15:18.600 --> 00:15:23.000]   as anybody in the world on number two.
[00:15:23.000 --> 00:15:26.000]   And I just want to maybe extend that a little bit more.
[00:15:26.000 --> 00:15:32.720]   A wonderful story from a very elite Kaggler who's a senior engineer in Lyft's autonomous
[00:15:32.720 --> 00:15:34.080]   driving unit.
[00:15:34.080 --> 00:15:41.120]   He started there as a very junior engineer and he made a name for himself in a bunch
[00:15:41.120 --> 00:15:45.120]   of well-credentialed people on that team.
[00:15:45.120 --> 00:15:52.080]   PhDs from famous machine learning universities, been working on a problem for three months.
[00:15:52.080 --> 00:15:57.840]   He took it home over the weekend and got way further over the course of the weekend than
[00:15:57.840 --> 00:15:59.640]   they had gotten over a three-month period.
[00:15:59.640 --> 00:16:04.440]   And that's because Kaggle challenges, you see lots of different data sets and you're
[00:16:04.440 --> 00:16:05.520]   working to a deadline.
[00:16:05.520 --> 00:16:09.040]   And so he was able to look at that problem.
[00:16:09.040 --> 00:16:15.120]   He'd seen enough problems directionally similar, had a good intuition for what the right approach
[00:16:15.120 --> 00:16:16.120]   was going to be.
[00:16:16.120 --> 00:16:20.200]   And so he was able to come up with a very, very, very good solution very quickly.
[00:16:20.200 --> 00:16:25.620]   And he's now a very senior engineer on that team and that's how he made his name.
[00:16:25.620 --> 00:16:29.980]   And so I think Kaggle is outstanding for number two.
[00:16:29.980 --> 00:16:35.120]   People say we're irrelevant for number one, which is turning a business problem into a
[00:16:35.120 --> 00:16:36.120]   machine learning problem.
[00:16:36.120 --> 00:16:38.320]   I actually don't agree with that.
[00:16:38.320 --> 00:16:45.400]   I think Kaggle trains that muscle obviously less directly than the actual training of
[00:16:45.400 --> 00:16:46.400]   models.
[00:16:46.400 --> 00:16:53.920]   But the way it trains that muscle is you see the business problem is described in the text
[00:16:53.920 --> 00:16:58.360]   of the challenge and then you see how that team set it up.
[00:16:58.360 --> 00:17:03.560]   And so you see lots and lots and lots of examples of how different teams have taken a business
[00:17:03.560 --> 00:17:08.520]   problem and set it up as a machine learning challenge.
[00:17:08.520 --> 00:17:13.320]   The area where I think Kaggle gets dinged and fairly so is on number three, taking a
[00:17:13.320 --> 00:17:16.840]   prototype model and productionizing.
[00:17:16.840 --> 00:17:20.720]   This is the kind of thing that really you have to be inside a company that's productionizing
[00:17:20.720 --> 00:17:23.000]   models in order to get experience.
[00:17:23.000 --> 00:17:27.920]   But it's not like I don't think today that people are missing out on a lot.
[00:17:27.920 --> 00:17:34.120]   The process of handing a model that has been prototyped in a Jupyter notebook and then
[00:17:34.120 --> 00:17:36.440]   productionized is an incredibly painful thing.
[00:17:36.440 --> 00:17:40.920]   It's not like there are wonderful and by the way, I think this will change over the next
[00:17:40.920 --> 00:17:48.800]   few years, but I don't think that somebody trained on Kaggle is going to go into a company
[00:17:48.800 --> 00:17:53.140]   and think, "Oh, there's this whole world of things that are sort of well-established practices
[00:17:53.140 --> 00:17:54.640]   that I don't know."
[00:17:54.640 --> 00:18:02.480]   I think number three is currently a mess and it's a painful thing in just about any company
[00:18:02.480 --> 00:18:04.000]   you go and work for.
[00:18:04.000 --> 00:18:07.640]   So I just don't think that you're missing out on too much.
[00:18:07.640 --> 00:18:14.080]   It's going to be painful whether you've come up through engineering at Google or you've
[00:18:14.080 --> 00:18:15.080]   come up through Kaggle.
[00:18:15.760 --> 00:18:17.040]   I'm going to channel some of our audience.
[00:18:17.040 --> 00:18:22.720]   I know just from the comments that we get and the questions in the Slack channel, I
[00:18:22.720 --> 00:18:27.120]   think we have a lot of folks that are kind of trying to break into Silicon Valley type
[00:18:27.120 --> 00:18:28.120]   jobs.
[00:18:28.120 --> 00:18:30.600]   And I think Kaggle is a good way to do that.
[00:18:30.600 --> 00:18:35.480]   Do you have any advice for someone that's trying to get into machine learning through
[00:18:35.480 --> 00:18:36.480]   Kaggle?
[00:18:36.480 --> 00:18:41.080]   Have you seen that work for people and how that process has gone?
[00:18:41.080 --> 00:18:42.080]   Absolutely.
[00:18:42.080 --> 00:18:48.840]   Kaggle is now at this point a pretty well-recognized credential and in my view, a faster, more
[00:18:48.840 --> 00:18:55.600]   accessible way to break in than a PhD from Stanford or the University of Toronto.
[00:18:55.600 --> 00:18:57.080]   In your unbiased view.
[00:18:57.080 --> 00:19:01.440]   In my completely unbiased view.
[00:19:01.440 --> 00:19:08.440]   We see people, anyone who's a grandmaster on Kaggle, and I think if you work at it,
[00:19:08.440 --> 00:19:11.200]   you can get a PhD in five years.
[00:19:11.200 --> 00:19:19.440]   If you really work at it, you can go from an engineer who's okay with math to a grandmaster
[00:19:19.440 --> 00:19:21.560]   in a year.
[00:19:21.560 --> 00:19:26.280]   And it doesn't cost you whatever student debt, etc.
[00:19:26.280 --> 00:19:29.120]   But how do you actually do that?
[00:19:29.120 --> 00:19:31.640]   Is it just doing competitions?
[00:19:31.640 --> 00:19:32.640]   It's doing competitions.
[00:19:32.640 --> 00:19:34.000]   It's reading the forums.
[00:19:34.000 --> 00:19:38.040]   We have Kaggle notebooks where you can start with other people's code.
[00:19:38.040 --> 00:19:39.040]   You fork it.
[00:19:39.040 --> 00:19:43.160]   You ask a lot of questions in the forums.
[00:19:43.160 --> 00:19:48.280]   And you can become a grandmaster across one of four dimensions.
[00:19:48.280 --> 00:19:53.600]   You can be a competition grandmaster, a data sets grandmaster, a discussion grandmaster
[00:19:53.600 --> 00:19:56.440]   and a notebooks grandmaster.
[00:19:56.440 --> 00:20:03.800]   In rank order, the most respected probably by employers is competitions.
[00:20:03.800 --> 00:20:07.600]   In my view, unjustifiably so.
[00:20:07.600 --> 00:20:13.600]   We prefer people who are either notebooks or discussion grandmasters.
[00:20:13.600 --> 00:20:18.680]   And the reason being is you have to be insightful enough that you're writing comments that people
[00:20:18.680 --> 00:20:21.320]   upvote or you're writing notebooks that people upvote.
[00:20:21.320 --> 00:20:27.080]   And you have to write clear enough code or communicate clearly enough that you would
[00:20:27.080 --> 00:20:31.400]   also make for the kind of thing you want on a team.
[00:20:31.400 --> 00:20:32.880]   I would pick one of those three categories.
[00:20:32.880 --> 00:20:38.480]   And the kinds of places, you know, NVIDIA has hired somewhere in the order of 10 to
[00:20:38.480 --> 00:20:44.960]   20, I'm not sure the exact number, Kaggle grandmasters over the last, let's say, six
[00:20:44.960 --> 00:20:45.960]   months.
[00:20:45.960 --> 00:20:48.800]   So they've been on a hiring tear for Kaggle grandmasters.
[00:20:48.800 --> 00:20:52.600]   DeepMind has hired quite a few.
[00:20:52.600 --> 00:20:58.080]   H2O have a team of about 15 or so Kaggle grandmasters.
[00:20:58.080 --> 00:21:08.120]   So it's not a credential that every company goes for, but a meaningful amount of top AI
[00:21:08.120 --> 00:21:10.720]   companies value this credential.
[00:21:10.720 --> 00:21:14.120]   And then once you've got your first job, you're in, right?
[00:21:14.120 --> 00:21:16.400]   You'll continue to get other good jobs.
[00:21:16.400 --> 00:21:21.480]   I feel compelled to add that I also love to see Kaggle credentials on a resume.
[00:21:21.480 --> 00:21:25.160]   And for me, it weights and biases top ML company.
[00:21:25.160 --> 00:21:31.600]   But I'll say for me, I even appreciate, you know, we hire a lot of engineers back in front
[00:21:31.600 --> 00:21:37.720]   end and sometimes they just be like, "Hey, I did a couple of Kaggle competitions."
[00:21:37.720 --> 00:21:42.560]   And I think that just says so many good things about a prospective employee.
[00:21:42.560 --> 00:21:44.040]   I'd love to see it.
[00:21:44.040 --> 00:21:48.400]   So I just totally agree with you, I guess.
[00:21:48.400 --> 00:21:54.520]   I'm maybe slightly less obviously biased, but I love Kaggle credentials and I love it
[00:21:54.520 --> 00:21:57.640]   when people share them on their resume.
[00:21:57.640 --> 00:22:01.720]   One thing I want to make sure I had time to get to, just because you've told me kind of
[00:22:01.720 --> 00:22:08.160]   privately, I think that in some ways the maybe lesser known publicly Kaggle products have
[00:22:08.160 --> 00:22:10.400]   in some ways more traction than the competition product.
[00:22:10.400 --> 00:22:15.400]   Can you say what the other ones are and why you decided to build them and how they're
[00:22:15.400 --> 00:22:16.400]   doing?
[00:22:16.400 --> 00:22:17.400]   Yeah, sure.
[00:22:17.400 --> 00:22:25.040]   So Kaggle, between 2010 and 2015, we were all machine learning competitions.
[00:22:25.040 --> 00:22:29.400]   And we always thought that machine learning competition is very powerful, but also not
[00:22:29.400 --> 00:22:30.760]   going to appeal to everybody.
[00:22:30.760 --> 00:22:34.000]   And as data science machine learning grew as a profession, we wanted to make sure that
[00:22:34.000 --> 00:22:36.280]   we were continuing to grow with it.
[00:22:36.280 --> 00:22:41.000]   The first thing we launched is Kaggle Notebooks, which is basically a hosted Jupyter Notebook.
[00:22:41.000 --> 00:22:44.880]   And really you can have a, it doesn't cost you anything.
[00:22:44.880 --> 00:22:48.140]   You get a CPU, a GPU or a TPU.
[00:22:48.140 --> 00:22:50.880]   So you get some, you get as much CPU as you want.
[00:22:50.880 --> 00:22:55.640]   And then you get 30 hours a week of either of GPU and TPU.
[00:22:55.640 --> 00:22:57.880]   So you get quite a lot of free accelerator as well.
[00:22:57.880 --> 00:22:58.880]   That's so great that you do that.
[00:22:58.880 --> 00:22:59.880]   That's awesome.
[00:22:59.880 --> 00:23:00.880]   You know, we're part of Google.
[00:23:00.880 --> 00:23:06.280]   This is how, you know, we would not have been able to do this as a standalone company.
[00:23:06.280 --> 00:23:11.360]   But the really nice thing here is, you know, you come to a Kaggle, any notebook on Kaggle,
[00:23:11.360 --> 00:23:13.880]   you can hit the copy and edit button.
[00:23:13.880 --> 00:23:18.160]   We used to call it fork and you get somebody else's code running in an environment that
[00:23:18.160 --> 00:23:19.160]   will run it.
[00:23:19.160 --> 00:23:20.160]   Right.
[00:23:20.160 --> 00:23:21.640]   So it's completely reproducible.
[00:23:21.640 --> 00:23:27.120]   We launched it initially inside competitions because we noticed people sharing their code
[00:23:27.120 --> 00:23:35.720]   in forum posts, either linking to a GitHub repo or, you know, attaching a Python script.
[00:23:35.720 --> 00:23:41.680]   We also noticed that those forum posts, people came to Kaggle to learn.
[00:23:41.680 --> 00:23:43.680]   Those should be the most valuable forum posts.
[00:23:43.680 --> 00:23:48.360]   They got dramatically less interest than forum posts where people shared ideas.
[00:23:48.360 --> 00:23:51.800]   So it's like the most valuable content was not really getting utilized.
[00:23:51.800 --> 00:23:55.560]   And that's because, you know, you get someone's Python script, you have to get the same version
[00:23:55.560 --> 00:24:01.680]   of Python, the same version of, you know, you're on the right side of TensorFlow 1.0,
[00:24:01.680 --> 00:24:03.680]   2.0, like all these dependencies.
[00:24:03.680 --> 00:24:07.600]   It's like really takes some number of hours to get somebody else's code to run.
[00:24:07.600 --> 00:24:08.920]   And then you don't know if it's any good.
[00:24:08.920 --> 00:24:09.920]   Right.
[00:24:09.920 --> 00:24:12.720]   So we wanted to create an environment where it was much easier for people to share code
[00:24:12.720 --> 00:24:15.920]   and not have to worry about the environment behind it.
[00:24:15.920 --> 00:24:19.760]   And that was the insight behind Kaggle Notebooks.
[00:24:19.760 --> 00:24:25.480]   We have somewhere in the order of 800,000 users every month.
[00:24:25.480 --> 00:24:29.920]   Actually, it might have -- I'm on parental leave.
[00:24:29.920 --> 00:24:32.080]   My knowledge of the data might be a little out of date.
[00:24:32.080 --> 00:24:36.560]   It might be over a million people a month looking at other people's notebooks, which
[00:24:36.560 --> 00:24:39.160]   is just extraordinary.
[00:24:39.160 --> 00:24:43.560]   The other product we have, kind of like how people share videos on YouTube, we allow people
[00:24:43.560 --> 00:24:45.420]   to share datasets on Kaggle.
[00:24:45.420 --> 00:24:50.520]   And where this grew out of is we noticed when we launched Kaggle Notebooks, they were initially
[00:24:50.520 --> 00:24:55.640]   only useful for people competing in competitions to share code alongside competitions.
[00:24:55.640 --> 00:25:00.920]   But we noticed people kind of using Kaggle Notebooks to share kind of more freeform insights.
[00:25:00.920 --> 00:25:06.000]   And so it's like, oh, wouldn't it be great if we gave people datasets as well that they
[00:25:06.000 --> 00:25:08.280]   could do more freeform type work on.
[00:25:08.280 --> 00:25:12.000]   And so we launched this platform where anyone can share any dataset.
[00:25:12.000 --> 00:25:16.120]   And we have, at least last time I looked at the metrics, which again was before parental
[00:25:16.120 --> 00:25:20.200]   leave, was over 400,000 people a month downloading Kaggle datasets.
[00:25:20.200 --> 00:25:26.200]   So I think these are -- remember, we're a data science machine learning site.
[00:25:26.200 --> 00:25:28.360]   It's still an emerging profession.
[00:25:28.360 --> 00:25:33.080]   And to have those sorts of numbers on these products, they're getting really meaningful
[00:25:33.080 --> 00:25:34.080]   traction.
[00:25:34.080 --> 00:25:35.860]   Yeah, that's really incredible.
[00:25:35.860 --> 00:25:36.920]   And they're also a nice way.
[00:25:36.920 --> 00:25:39.800]   Like if you don't -- a Kaggle competition is tremendous commitment.
[00:25:39.800 --> 00:25:45.200]   If you want to get involved in Kaggle, but don't have the time to put into a competition,
[00:25:45.200 --> 00:25:48.280]   these are ways to get involved in a lighter weight way.
[00:25:48.280 --> 00:25:53.880]   Do you have a favorite kernel or dataset that maybe doesn't get as much attention as it
[00:25:53.880 --> 00:25:54.880]   deserves?
[00:25:54.880 --> 00:25:55.880]   Man, there's lots.
[00:25:55.880 --> 00:26:00.080]   There are all sorts of things.
[00:26:00.080 --> 00:26:04.280]   We've already uploaded a dataset taking x-rays.
[00:26:04.280 --> 00:26:09.720]   And you had to predict the age of the person based on their x-rays.
[00:26:09.720 --> 00:26:13.840]   It was called the bone age dataset, which I thought was kind of cool.
[00:26:13.840 --> 00:26:14.840]   That is really cool.
[00:26:14.840 --> 00:26:15.840]   Wow.
[00:26:15.840 --> 00:26:16.840]   Just like really random.
[00:26:16.840 --> 00:26:17.840]   Oh, man.
[00:26:17.840 --> 00:26:18.840]   What else?
[00:26:18.840 --> 00:26:24.520]   We've got a lot of cool COVID-19 datasets.
[00:26:24.520 --> 00:26:29.040]   Actually, one of the things that has been nice with the COVID-19 datasets that have
[00:26:29.040 --> 00:26:35.280]   been shared is that you've got the John Hopkins University dataset, for instance, and everyone's
[00:26:35.280 --> 00:26:36.280]   looking at that.
[00:26:36.280 --> 00:26:39.520]   One thing that people have been doing on Kaggle, which is really powerful, is they've been
[00:26:39.520 --> 00:26:45.920]   creating these very rich panels where they join the John Hopkins dataset to daily data
[00:26:45.920 --> 00:26:49.560]   from the nearest weather station.
[00:26:49.560 --> 00:26:55.880]   And so there's this debate about how does temperature and seasonality impact the transmission
[00:26:55.880 --> 00:26:56.880]   of COVID-19?
[00:26:56.880 --> 00:26:59.440]   Well, somebody has just...
[00:26:59.440 --> 00:27:00.860]   And I've seen some of these studies.
[00:27:00.860 --> 00:27:07.200]   Some study will look at 100 cities in China and draw some conclusion about the rate of
[00:27:07.200 --> 00:27:10.480]   transmission and how temperature impacts.
[00:27:10.480 --> 00:27:18.080]   Well people in the Kaggle community have just hand-delivered all whatever 4,000 locations
[00:27:18.080 --> 00:27:19.440]   with the nearest weather station.
[00:27:19.440 --> 00:27:24.200]   And so those kind of things are also, I think, really cool and really powerful.
[00:27:24.200 --> 00:27:25.920]   That is so cool.
[00:27:25.920 --> 00:27:29.400]   And it's been out for four or five years, I guess?
[00:27:29.400 --> 00:27:30.400]   Kaggle datasets launched.
[00:27:30.400 --> 00:27:34.800]   So Kaggle notebooks launched in 2015 and datasets.
[00:27:34.800 --> 00:27:37.200]   And that was about May.
[00:27:37.200 --> 00:27:44.360]   And by the way, Kaggle notebooks really started as an edit box with a run button.
[00:27:44.360 --> 00:27:46.440]   Nice.
[00:27:46.440 --> 00:27:50.760]   And we also launched very lightweight.
[00:27:50.760 --> 00:27:52.840]   We launched with R, not Python.
[00:27:52.840 --> 00:27:53.840]   And then...
[00:27:53.840 --> 00:27:54.840]   Oh, wow.
[00:27:54.840 --> 00:27:55.840]   That's a sign of the times.
[00:27:55.840 --> 00:27:56.840]   That's amazing.
[00:27:56.840 --> 00:28:00.240]   The number one feature request was, "Can you add Python?"
[00:28:00.240 --> 00:28:05.320]   Although I should say, just on R, still some of the most beautiful notebooks are written
[00:28:05.320 --> 00:28:13.920]   in R. So still, particularly for data analysis, data visualization, it does well on Kaggle.
[00:28:13.920 --> 00:28:21.040]   Beautiful content still created with R. And then datasets launched in August of 2016.
[00:28:21.040 --> 00:28:22.040]   That's funny.
[00:28:22.040 --> 00:28:23.040]   I really want to ask.
[00:28:23.040 --> 00:28:26.480]   These are just the questions I selfishly want to ask, but I guess it's my show so I can
[00:28:26.480 --> 00:28:27.480]   do it.
[00:28:27.480 --> 00:28:33.440]   When you look at the tools people are using, just as an example, do people use anything
[00:28:33.440 --> 00:28:35.160]   besides Python and R?
[00:28:35.160 --> 00:28:37.800]   And do people actually win competitions with R?
[00:28:37.800 --> 00:28:40.760]   I say this as a longtime R user who switched to Python.
[00:28:40.760 --> 00:28:45.320]   So I'm not against R, but it does seem like almost everyone has switched at this point.
[00:28:45.320 --> 00:28:46.320]   Yeah.
[00:28:46.320 --> 00:28:49.400]   I mean, when Kaggle first started, it was like MATLAB.
[00:28:49.400 --> 00:28:51.000]   We even saw SAS.
[00:28:51.000 --> 00:28:52.360]   We saw a whole lot of stuff.
[00:28:52.360 --> 00:29:00.840]   And then it quickly became R. And then Python's rise has just been sort of a fairly steady,
[00:29:00.840 --> 00:29:03.600]   it's just like steadily taken share.
[00:29:03.600 --> 00:29:06.120]   I'm not actually sure what the numbers are now.
[00:29:06.120 --> 00:29:11.640]   I know a couple of years ago it was two-thirds Python, one-third R. I suspect it's probably
[00:29:11.640 --> 00:29:14.360]   closer to 90% Python, 10% R now.
[00:29:14.360 --> 00:29:16.440]   I'm actually not certain of that, though.
[00:29:16.440 --> 00:29:20.680]   I'm not aware really of R doing particularly well in competitions.
[00:29:20.680 --> 00:29:25.920]   I think Python is hard to beat, like the support for neural networks, et cetera, et cetera,
[00:29:25.920 --> 00:29:28.720]   is I think a fair bit stronger in Python.
[00:29:28.720 --> 00:29:33.520]   And so I'd say the place where R has really shone in the community is beautiful, beautiful
[00:29:33.520 --> 00:29:39.800]   notebooks, new competition launches, somebody writes a notebook.
[00:29:39.800 --> 00:29:45.800]   We have a user I have in mind when I think of, I have a persona in mind, heads or tails,
[00:29:45.800 --> 00:29:52.000]   is his Kaggle alias, writes beautiful R notebooks, sort of helping anyone competing in a competition
[00:29:52.000 --> 00:29:55.160]   get their arms around what's in that data set.
[00:29:55.160 --> 00:29:57.200]   So it still plays a nice role.
[00:29:57.200 --> 00:30:01.620]   I'd say that we've seen a bit, to the extent that people are using different types of technologies
[00:30:01.620 --> 00:30:09.720]   on Kaggle, we had a challenge fairly recently where H2O's driverless AI did really well.
[00:30:09.720 --> 00:30:11.680]   It was a forecasting challenge.
[00:30:11.680 --> 00:30:15.040]   We've had challenges in the past where Google's AutoML have done well.
[00:30:15.040 --> 00:30:19.960]   I happen to be a huge believer in that category.
[00:30:19.960 --> 00:30:23.480]   So much of training in neural network is turning knobs, right?
[00:30:23.480 --> 00:30:27.160]   Why shouldn't that be automated?
[00:30:27.160 --> 00:30:31.920]   Even so much of feature engineering, we spoke about this earlier on, exploding a date can
[00:30:31.920 --> 00:30:35.600]   probably be written into software.
[00:30:35.600 --> 00:30:41.080]   So I'm a huge believer in the next big thing in terms of model training is probably more
[00:30:41.080 --> 00:30:46.160]   and more automation of the things that are relatively easier to automate.
[00:30:46.160 --> 00:30:51.720]   And then the other place that we've seen new technologies get adopted, like some of the
[00:30:51.720 --> 00:30:57.320]   things that are generating excitement in the community, we launched TPU notebooks and that's
[00:30:57.320 --> 00:31:03.600]   certainly some class of problems where TPUs are doing extremely well, particularly on
[00:31:03.600 --> 00:31:08.040]   the natural language processing models, which are very, very memory heavy.
[00:31:08.040 --> 00:31:11.600]   I think there's a lot of excitement in videos coming out with a new chip, coming out with
[00:31:11.600 --> 00:31:15.040]   a new chip soon, which I think there's a lot of excitement around.
[00:31:15.040 --> 00:31:19.160]   So any advances in accelerators gets people excited.
[00:31:19.160 --> 00:31:24.160]   And wait, sorry, so this is actually, is this faster training times or literally the models
[00:31:24.160 --> 00:31:27.080]   perform better?
[00:31:27.080 --> 00:31:32.000]   So one of the reasons that faster training times matter is because we give people nine
[00:31:32.000 --> 00:31:34.400]   hours Kaggle notebooks.
[00:31:34.400 --> 00:31:39.480]   And so it's faster training times, but we give people, a lot of people are using our
[00:31:39.480 --> 00:31:41.120]   Excel at Kaggle notebooks.
[00:31:41.120 --> 00:31:47.040]   And so the fact that you can train a model faster, there also are some issues.
[00:31:47.040 --> 00:31:53.720]   I think that people have trained things like Roberta, which is a particularly memory intensive
[00:31:53.720 --> 00:31:55.040]   version of BERT.
[00:31:55.040 --> 00:31:59.240]   And so the TPUs we make available have enough memory such that you can train, you know,
[00:31:59.240 --> 00:32:02.800]   it's the difference being able to train a Roberta model or not.
[00:32:02.800 --> 00:32:04.520]   Right, right.
[00:32:04.520 --> 00:32:05.520]   What about frameworks?
[00:32:05.520 --> 00:32:07.480]   Like how do they break down?
[00:32:07.480 --> 00:32:10.320]   Is everyone using TensorFlow or what's?
[00:32:10.320 --> 00:32:18.360]   No, I think PyTorch is really, we just see PyTorch is doing really well.
[00:32:18.360 --> 00:32:22.760]   It's actually, I don't know, I don't know what the TensorFlow 2.0 was obviously a big
[00:32:22.760 --> 00:32:23.760]   release.
[00:32:23.760 --> 00:32:24.760]   People love Keras.
[00:32:24.760 --> 00:32:30.120]   It's really, Keras has dominated on Kaggle for a long time.
[00:32:30.120 --> 00:32:35.760]   And now PyTorch is, I'd say maybe Keras and PyTorch, at least last time I checked was
[00:32:35.760 --> 00:32:42.560]   sort of roughly equivalent, but the trajectory on PyTorch was very strong.
[00:32:42.560 --> 00:32:52.000]   I don't know that I have a good sense of the extent to which TensorFlow 2.0 has changed
[00:32:52.000 --> 00:32:57.360]   the PyTorch TensorFlow, what that trajectory looks like yet.
[00:32:57.360 --> 00:33:02.920]   But certainly PyTorch's rise has been very, very, really incredible.
[00:33:02.920 --> 00:33:09.080]   And what about like the simpler, the sort of like layers on top of PyTorch, like Lightning
[00:33:09.080 --> 00:33:10.880]   and Fast.ai and Ignite?
[00:33:10.880 --> 00:33:12.360]   Do you see those?
[00:33:12.360 --> 00:33:15.040]   Yeah, I mean, Fast.ai definitely.
[00:33:15.040 --> 00:33:18.560]   We see I think Fast.ai, quite a lot of people do that course.
[00:33:18.560 --> 00:33:23.000]   And then they, Jeremy mentions Kaggle a fair bit in that course.
[00:33:23.000 --> 00:33:26.800]   And so I think that ends up being a decent feeder to Kaggle.
[00:33:26.800 --> 00:33:31.600]   And so people coming out of that course bring Fast.ai.
[00:33:31.600 --> 00:33:34.560]   The other two, not so much.
[00:33:34.560 --> 00:33:38.160]   Is there like, do Grandmasters have like different set of tools?
[00:33:38.160 --> 00:33:41.960]   Do you see the more experienced people kind of switching to stuff?
[00:33:41.960 --> 00:33:49.400]   I think what they often have is not so much their own tools, but they have their own,
[00:33:49.400 --> 00:33:52.240]   very often have built like, what's the way to put it?
[00:33:52.240 --> 00:33:56.920]   Like their own little framework so that when they see a computer vision problem, they're
[00:33:56.920 --> 00:34:03.840]   not starting from scratch, but they're starting with code that they know well, that they know
[00:34:03.840 --> 00:34:06.400]   how to optimize well.
[00:34:06.400 --> 00:34:10.880]   And by the way, the fact that so many Grandmasters are creating their own little frameworks suggests
[00:34:10.880 --> 00:34:16.600]   that the PyTorches and the TensorFlow's aren't, there's something missing, right?
[00:34:16.600 --> 00:34:21.760]   Maybe it's that people like to do things in their own way, or maybe they're like slightly
[00:34:21.760 --> 00:34:29.280]   too low for at least the kinds of low level for the kinds of things that people are doing
[00:34:29.280 --> 00:34:31.000]   in the Kaggle community.
[00:34:31.000 --> 00:34:37.120]   So what are like the common things that these frameworks do?
[00:34:37.120 --> 00:34:41.440]   I think that they're very often like all the pipeline steps, right?
[00:34:41.440 --> 00:34:49.440]   So you've got your new image dataset come in.
[00:34:49.440 --> 00:34:52.640]   The first thing you want to do is you want to do data augmentation and they have their
[00:34:52.640 --> 00:34:57.640]   preferred way of implementing data augmentation and then passing it on to the next.
[00:34:57.640 --> 00:35:02.200]   So I think it's really focused on creating a pipeline that starts with a new dataset
[00:35:02.200 --> 00:35:05.040]   and ends in a Kaggle submission file.
[00:35:05.040 --> 00:35:08.800]   Okay, I want to totally shift gears because there's a couple more questions I want to
[00:35:08.800 --> 00:35:10.200]   make sure I get to in this time.
[00:35:10.200 --> 00:35:11.200]   Sure.
[00:35:11.200 --> 00:35:14.720]   So this is actually a Lavanya suggestion for a question and I love it.
[00:35:14.720 --> 00:35:18.360]   So I remember you giving a TED talk about the jobs that we'll lose to machines.
[00:35:18.360 --> 00:35:19.720]   I remember you telling me about it.
[00:35:19.720 --> 00:35:24.880]   I thought it was super smart and you had a way of thinking about the jobs that you should
[00:35:24.880 --> 00:35:25.880]   be worried about losing.
[00:35:25.880 --> 00:35:29.600]   So I was hoping you could describe that to this audience who maybe haven't seen the TED
[00:35:29.600 --> 00:35:35.240]   talk, but I also was wondering if your thinking has changed at all since you gave the talk
[00:35:35.240 --> 00:35:37.960]   because that was now in 2016.
[00:35:37.960 --> 00:35:39.520]   Things have changed a bit.
[00:35:39.520 --> 00:35:42.960]   Yeah, at the time maybe it was slightly non-obvious.
[00:35:42.960 --> 00:35:46.120]   To me, it reads now as like very obvious.
[00:35:46.120 --> 00:35:51.000]   The basic conclusion is jobs are at risk if you're doing the same thing again and again
[00:35:51.000 --> 00:35:53.240]   and again, right?
[00:35:53.240 --> 00:35:58.800]   We at Kaggle had done a pathbreaking competition taking images of the eye and diagnosing diabetic
[00:35:58.800 --> 00:36:03.240]   retinopathy and the results on that were just outstanding.
[00:36:03.240 --> 00:36:06.760]   We've done a lot of other medical imaging problems and you think about what a, you know,
[00:36:06.760 --> 00:36:11.480]   this is like not, this is fairly hackneyed at this point, but a radiologist does the
[00:36:11.480 --> 00:36:14.280]   same thing again and again and again, right?
[00:36:14.280 --> 00:36:21.320]   Looking at an image, making a diagnosis of that image or an ophthalmologist.
[00:36:21.320 --> 00:36:27.040]   Whereas if your job requires you to be creative and to be sort of doing different things on
[00:36:27.040 --> 00:36:30.840]   a daily basis, you're probably much safer.
[00:36:30.840 --> 00:36:34.120]   You know, some of the professions I think I mentioned in the talk, at least I had in
[00:36:34.120 --> 00:36:38.800]   the back of my mind, but surprisingly haven't gone, you know, you haven't seen much automation
[00:36:38.800 --> 00:36:42.120]   yet as like auditing, vetting of basically legal contracts.
[00:36:42.120 --> 00:36:46.880]   Like, do you really need a lawyer to look at yet another relatively boilerplate NDA?
[00:36:46.880 --> 00:36:51.960]   Do you really need an auditor to go over the majority, you know, the majority of the company's
[00:36:51.960 --> 00:36:54.920]   accounts are probably fairly standard.
[00:36:54.920 --> 00:36:59.760]   So there's still a lot of rote work that in the four years since the talk, I don't think,
[00:36:59.760 --> 00:37:03.120]   I'm not aware of a ton of work being done to automate.
[00:37:03.120 --> 00:37:08.760]   So it's sort of points to, you know, perhaps opportunities or perhaps things that still
[00:37:08.760 --> 00:37:10.240]   haven't been done.
[00:37:10.240 --> 00:37:15.400]   But I think the conclusion still stands, you know, I was in 2016, fairly skeptical that
[00:37:15.400 --> 00:37:19.240]   we were going to make a lot of progress towards artificial general intelligence.
[00:37:19.240 --> 00:37:25.200]   I still, like, I just don't see it with the current set of tools that we have, I think
[00:37:25.200 --> 00:37:30.120]   are incredibly useful and they're very powerful and they allow us to do a lot of really cool
[00:37:30.120 --> 00:37:31.120]   things.
[00:37:31.120 --> 00:37:37.720]   I don't see a path from, you know, where we are, like a smooth path that doesn't involve,
[00:37:37.720 --> 00:37:40.800]   you know, multiple step changes from where we are today to AGI.
[00:37:40.800 --> 00:37:48.080]   And so the distinction between repetitive tasks and tasks that require more creativity
[00:37:48.080 --> 00:37:53.080]   and more moving around still sticks to me as true as, you know, where jobs are safe
[00:37:53.080 --> 00:37:54.080]   and where they aren't.
[00:37:54.080 --> 00:37:57.520]   Do you have any opinions on reinforcement learning?
[00:37:57.520 --> 00:38:02.680]   I feel like we've seen a lot of interesting stuff coming out that certainly feels like
[00:38:02.680 --> 00:38:04.480]   a different kind of intelligence to me.
[00:38:04.480 --> 00:38:07.160]   Like, do you, have you looked at that?
[00:38:07.160 --> 00:38:14.120]   Yeah, I guess with reinforcement learning, where Kaggle aims to be is somewhere between
[00:38:14.120 --> 00:38:19.720]   where the cutting edge of academia is and where the average Fortune 500 company is,
[00:38:19.720 --> 00:38:20.720]   right?
[00:38:20.720 --> 00:38:26.560]   So I guess we believe in reinforcement learning enough that we've started actually, I guess
[00:38:26.560 --> 00:38:31.440]   two or three weeks ago, we launched our second ever simulation competition.
[00:38:31.440 --> 00:38:36.840]   It was with Two Sigma where people are riding AI bots to beat each other in a game called
[00:38:36.840 --> 00:38:37.840]   Halite.
[00:38:37.840 --> 00:38:46.480]   In January, we tested the concept with a ConnectX, a game of ConnectX.
[00:38:46.480 --> 00:38:52.760]   And so we are now investing in reinforcement learning.
[00:38:52.760 --> 00:38:57.920]   You know, I have heard of some cool, you know, pragmatic applications as well, well covered,
[00:38:57.920 --> 00:39:02.520]   deep mind, optimizing data, Google data centers.
[00:39:02.520 --> 00:39:11.560]   I've heard about potential uses in search and ad targeting in stock market trading.
[00:39:11.560 --> 00:39:17.720]   So I guess I'll start to get excited when there are more, Kaggle is excited enough that
[00:39:17.720 --> 00:39:23.800]   we're investing and that we're making reinforcement learning challenges available to our community.
[00:39:23.800 --> 00:39:30.880]   I'll be more excited than I am today when there's more pragmatic use cases that we can
[00:39:30.880 --> 00:39:33.440]   point to where it's really making a positive difference.
[00:39:33.440 --> 00:39:34.440]   That makes sense.
[00:39:34.440 --> 00:39:35.440]   All right.
[00:39:35.440 --> 00:39:38.800]   So we always end with two questions.
[00:39:38.800 --> 00:39:46.600]   So the penultimate question here is, what's the topic in machine learning that people
[00:39:46.600 --> 00:39:48.520]   don't talk about as much as they should?
[00:39:48.520 --> 00:39:53.120]   I mean, one of the things I'm really energized about at the moment, just particularly with
[00:39:53.120 --> 00:39:57.960]   the success of Kaggle's datasets platform, is I want to make it easier for people to
[00:39:57.960 --> 00:40:05.160]   find access and join external datasets to their own datasets.
[00:40:05.160 --> 00:40:07.320]   Datasets, they're our raw material, right?
[00:40:07.320 --> 00:40:15.000]   And so the easier it is for us to integrate them into our machine learning algorithms,
[00:40:15.000 --> 00:40:17.240]   the more powerful our work is going to be.
[00:40:17.240 --> 00:40:18.800]   And so that's one area.
[00:40:18.800 --> 00:40:21.920]   And then the second one is one I mentioned earlier.
[00:40:21.920 --> 00:40:26.240]   Gradient boosting machines still do incredibly well in Kaggle challenges, right?
[00:40:26.240 --> 00:40:29.600]   And it's just not an area of academic study at all.
[00:40:29.600 --> 00:40:32.480]   And it makes me wonder, is there more that could be done?
[00:40:32.480 --> 00:40:34.080]   Is this an area that's being overlooked?
[00:40:34.080 --> 00:40:40.080]   Is there more that could be done around gradient boosting machine like technologies?
[00:40:40.080 --> 00:40:41.520]   Have we abandoned them too fast?
[00:40:41.520 --> 00:40:46.440]   It's funny, I worked a lot in gradient boosted trees back in the day.
[00:40:46.440 --> 00:40:48.560]   It's probably the algorithm I've spent the most time with.
[00:40:48.560 --> 00:40:52.240]   And I remember feeling like they had trouble learning linear relationships.
[00:40:52.240 --> 00:40:55.200]   They're so good at these step function things.
[00:40:55.200 --> 00:41:00.840]   But is it just gradient boosted trees or do people add more weak learners to their gradient
[00:41:00.840 --> 00:41:02.160]   boosting?
[00:41:02.160 --> 00:41:06.720]   What we typically find with structured data problems is people will train, whether it's
[00:41:06.720 --> 00:41:10.640]   XGBoost or LightGBM or some gradient boosting machines framework.
[00:41:10.640 --> 00:41:18.960]   And if you look at it, that is what's doing 99.9% of the work.
[00:41:18.960 --> 00:41:24.680]   Very often people will ensemble in other things just to get a bit more diversity.
[00:41:24.680 --> 00:41:33.280]   In most cases, I think that it really, ensembling other things helps take you from 20th to first.
[00:41:33.280 --> 00:41:37.600]   But actually when a company looks to productionize a model that comes out of a Kaggle challenge,
[00:41:37.600 --> 00:41:39.520]   they will strip out all the other stuff, right?
[00:41:39.520 --> 00:41:42.920]   Because you just don't want the complexity.
[00:41:42.920 --> 00:41:51.820]   So I think gradient boosting machines is really typically enough on most structures.
[00:41:51.820 --> 00:41:55.600]   That plus clever feature engineering is enough on most structured data problems.
[00:41:55.600 --> 00:41:56.600]   Got it.
[00:41:56.600 --> 00:41:57.600]   I'm glad I asked.
[00:41:57.600 --> 00:41:58.600]   That was interesting.
[00:41:58.600 --> 00:42:05.480]   The final question is, when you look at, and this is maybe outside of Kaggle, but at Google
[00:42:05.480 --> 00:42:13.000]   and all the companies that you've talked to, when you look at the path from inception to
[00:42:13.000 --> 00:42:17.680]   deployed machine learning software, where do you see the biggest bottlenecks?
[00:42:17.680 --> 00:42:18.840]   I like this analogy.
[00:42:18.840 --> 00:42:22.440]   There's this company at the moment that's pretty hot called Webflow.
[00:42:22.440 --> 00:42:33.880]   And what they do is they help with the seam between a designer and a front end engineer
[00:42:33.880 --> 00:42:43.360]   by making it much easier to get a design into HTML, CSS, JavaScript code.
[00:42:43.360 --> 00:42:51.920]   And I think that that's probably an area, taking a prototype model written in Python,
[00:42:51.920 --> 00:42:56.000]   possibly in Jupyter notebooks into a production system, maybe the production system is in
[00:42:56.000 --> 00:43:01.800]   Java or something else, is really nasty.
[00:43:01.800 --> 00:43:03.480]   You've seen a bunch of companies invest.
[00:43:03.480 --> 00:43:05.280]   Google obviously has invested internally.
[00:43:05.280 --> 00:43:07.680]   Uber invested in a system called Michelangelo.
[00:43:07.680 --> 00:43:16.040]   There's a lot of systems that have been built inside companies to try and solve that problem.
[00:43:16.040 --> 00:43:19.880]   But those systems, there's a bunch of startups now trying to take those systems that have
[00:43:19.880 --> 00:43:29.120]   been built internally for the likes of Google, for the likes of Uber, and to make them available
[00:43:29.120 --> 00:43:30.120]   to the wider world.
[00:43:30.120 --> 00:43:33.720]   I think that's definitely a problem that urgently needs to be solved.
[00:43:33.720 --> 00:43:40.480]   The seam between data scientist or machine learning researcher and data engineer is a
[00:43:40.480 --> 00:43:43.200]   really gnarly seam at the moment.
[00:43:43.200 --> 00:43:44.200]   Well said.
[00:43:44.200 --> 00:43:45.200]   Awesome.
[00:43:45.200 --> 00:43:46.200]   Well, thanks so much, Anshuli.
[00:43:46.200 --> 00:43:47.200]   That was really fun.
[00:43:47.200 --> 00:43:48.200]   Cool.
[00:43:48.200 --> 00:43:49.200]   Thank you.
[00:43:49.200 --> 00:43:50.200]   Thanks for having me.
[00:43:50.200 --> 00:43:55.520]   When we first started making these videos, we didn't know if anyone would be interested
[00:43:55.520 --> 00:43:58.560]   or want to see them, but we made them for fun.
[00:43:58.560 --> 00:44:01.520]   And we started off by making videos that would teach people.
[00:44:01.520 --> 00:44:05.480]   And now we get these great interviews with real industry practitioners.
[00:44:05.480 --> 00:44:09.640]   And I love making this available to the whole world so everyone can watch these things for
[00:44:09.640 --> 00:44:10.640]   free.
[00:44:10.640 --> 00:44:12.480]   The more feedback you give us, the better stuff we can produce.
[00:44:12.480 --> 00:44:15.400]   So please subscribe, leave a comment, engage with us.
[00:44:15.400 --> 00:44:16.280]   We really appreciate it.

