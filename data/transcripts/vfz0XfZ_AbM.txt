
[00:00:00.000 --> 00:00:09.360]   I'll quickly make sure we live. I always get worried if I am missing something and sometimes
[00:00:09.360 --> 00:00:27.240]   it doesn't go through. Just give me 10 seconds. Sure, sure. Awesome. I believe we live on
[00:00:27.240 --> 00:00:32.400]   YouTube. I'll quickly introduce today's guest. Hey, everybody. Thanks for coming back to
[00:00:32.400 --> 00:00:37.680]   the Jack's Talk series. This is co-hosted with Christian who will join us in a few minutes,
[00:00:37.680 --> 00:00:42.560]   I believe. Christian is the real host who brings in great guests like Eugene who is
[00:00:42.560 --> 00:00:47.280]   a research software engineer at Google Brain Japan, which is why we have the chance to
[00:00:47.280 --> 00:00:52.080]   host this during the day this time. Sometimes it's in the evening for me. So that's why
[00:00:52.080 --> 00:00:57.080]   we have the DVD. Christian is here. Hey, Christian. I'm just introducing Eugene.
[00:00:57.080 --> 00:00:59.080]   Hey, everyone.
[00:00:59.080 --> 00:01:01.080]   Hi, Christian.
[00:01:01.080 --> 00:01:06.960]   I was just mentioning about EvoJax. And for the audience, Eugene will be talking about
[00:01:06.960 --> 00:01:13.280]   EvoJax, which is a hardware accelerated neuroevolution framework built on top of Jax. We've been
[00:01:13.280 --> 00:01:19.080]   hosting a series about a bunch of RL libraries recently. And this is somewhat similar to
[00:01:19.080 --> 00:01:23.720]   that Eugene also has some interesting examples to show us some live demos. So I'm always
[00:01:23.720 --> 00:01:30.000]   excited about that. With that, I'll hand it over to Eugene. And before that, Christian,
[00:01:30.000 --> 00:01:36.200]   do you have any any other comments for the audience?
[00:01:36.200 --> 00:01:45.120]   For now, no, I'm super excited about this topic. It's one of the interesting cases where
[00:01:45.120 --> 00:01:49.440]   it's OK, let's see something new.
[00:01:49.440 --> 00:01:53.280]   Awesome. Over to you, Eugene.
[00:01:53.280 --> 00:01:58.360]   OK, cool. Yeah. Thanks for having me here. So, hi again. I'm Eugene. I'm a research software
[00:01:58.360 --> 00:02:03.920]   engineer at the Google Brain Tokyo team. So what I'm going to talk about is a software
[00:02:03.920 --> 00:02:10.640]   toolkit. It's called EvoJax. It's a hardware accelerated neuroevolution toolkit. So this
[00:02:10.640 --> 00:02:17.000]   is a joint work with my colleagues, Yingtao Tan and David Ha. So let me give you a brief
[00:02:17.000 --> 00:02:22.200]   description. So first of all, you may wonder, what is EvoJax? Apparently it's got two words
[00:02:22.200 --> 00:02:29.200]   there, Evo and Jax. So if I have to summarize EvoJax with really simple sentences, then
[00:02:29.200 --> 00:02:37.480]   EvoJax is a neuroevolution toolkit that's accelerated by hardware accelerators. So first,
[00:02:37.480 --> 00:02:45.600]   I'll just talk about what's neuroevolution. Neuroevolution uses evolutionary algorithms
[00:02:45.600 --> 00:02:52.360]   to say, for example, generate artificial neural networks and also train their weights to solve
[00:02:52.360 --> 00:03:00.320]   problems. And one feature that I want to highlight is that unlike backpropagation, neuroevolution
[00:03:00.320 --> 00:03:06.580]   does not rely on the differentiability of the entire system. So that means with neuroevolution
[00:03:06.580 --> 00:03:12.600]   and the genetic algorithm, our evolution strategies included, it could solve problems that involve
[00:03:12.600 --> 00:03:18.320]   non-differentiable systems. I'll talk about examples of non-differential systems later.
[00:03:18.320 --> 00:03:24.920]   But this is just one feature of evolution. Another feature is that, so evolution, the
[00:03:24.920 --> 00:03:30.600]   second keyword is Jax, right? So we built this entire software using Jax, the currently
[00:03:30.600 --> 00:03:38.000]   popular ML framework. And because of that, our toolkit is hardware accelerated. It runs
[00:03:38.000 --> 00:03:45.360]   on CPU, GPU, and TPU. And most importantly, it does not require any complex settings.
[00:03:45.360 --> 00:03:50.460]   So you could run all this code within one box as long as it has sufficient hardware
[00:03:50.460 --> 00:03:57.560]   support there. I gave several examples at the bottom of this page. For example, the
[00:03:57.560 --> 00:04:03.360]   first two are kind of like the popular tasks there. The first one is like a robotic locomotion
[00:04:03.360 --> 00:04:09.360]   controller learned in a physics simulation environment. In this case, we used Brax. I
[00:04:09.360 --> 00:04:13.160]   guess the audiences here are familiar with Brax, right? That's also something developed
[00:04:13.160 --> 00:04:18.200]   by Google recently. And the second example is the classic card
[00:04:18.200 --> 00:04:25.720]   pose swing-up task. Like these two tasks, previously using a conventional framework
[00:04:25.720 --> 00:04:31.400]   like evolution framework from Google Brain, when we compared it, we found that these tasks
[00:04:31.400 --> 00:04:37.040]   could reach 10 times or 20 times the speed up if we train these tasks with neural evolution
[00:04:37.040 --> 00:04:40.880]   with Apple Jax. So that's kind of the speed up we could get.
[00:04:40.880 --> 00:04:45.000]   The other two tasks I want you to see is that you see the third picture. It's called like
[00:04:45.000 --> 00:04:53.280]   a water world. The original author is like Andres Caparthy. He used JavaScript to develop
[00:04:53.280 --> 00:04:58.640]   this interesting environment where the agent has an array of sensors and is supposed to
[00:04:58.640 --> 00:05:04.360]   catch food as much as possible while avoiding these poisons. So the agent has a certain
[00:05:04.360 --> 00:05:12.560]   dynamics and the algorithm has to learn to control the agent such that the agent could
[00:05:12.560 --> 00:05:17.080]   steer clearly out of the way to get as much food as possible.
[00:05:17.080 --> 00:05:23.400]   And the last example shown here is from Intel, our colleague. In this example, he showed
[00:05:23.400 --> 00:05:28.920]   that it's possible to do some abstract painting with only triangles. In this example, actually,
[00:05:28.920 --> 00:05:35.020]   there are only 50 triangles and the agent is supposed to arrange these triangles such
[00:05:35.020 --> 00:05:41.800]   that the position as well as the color could match a certain given image. In this example,
[00:05:41.800 --> 00:05:46.440]   it's a water world. So I just want to highlight that with Apple Jax, we can do all this kind
[00:05:46.440 --> 00:05:50.160]   of stuff and with great speed up.
[00:05:50.160 --> 00:05:56.800]   So I guess you get a rough idea of what Apple Jax is. Then the next question is why we made
[00:05:56.800 --> 00:06:05.200]   Apple Jax. So we got two things in mind when we designed Apple Jax. The first thing is
[00:06:05.200 --> 00:06:11.360]   that we want to broaden the range of tasks and problems that neural evolution could solve
[00:06:11.360 --> 00:06:17.160]   currently. Like deep learning works great. We have this deep learning for supervisor
[00:06:17.160 --> 00:06:22.880]   tasks and also deep reinforcement learning for some reinforcement like the robotic control
[00:06:22.880 --> 00:06:29.560]   tasks. But the thing with that is that they are all trained to rely majorly on back propagation,
[00:06:29.560 --> 00:06:34.600]   which requires the network or the systems to be differentiable. But there are a lot
[00:06:34.600 --> 00:06:40.560]   of problems in the real world that does not guarantee us of this differentiability.
[00:06:40.560 --> 00:06:47.080]   I can give you an example. I put a robotic example here. So in robotic tasks, there are
[00:06:47.080 --> 00:06:52.840]   a lot of distant continuities induced, for example, by collision between these rigid
[00:06:52.840 --> 00:06:59.440]   bodies. So if you want to learn the physics of a simulator system, what we usually do
[00:06:59.440 --> 00:07:04.760]   is to create a neural network, usually of continuous operations. And then we then collect
[00:07:04.760 --> 00:07:14.000]   as much sample as possible to try to fit its neural network to say, imitate the physical
[00:07:14.000 --> 00:07:19.860]   behaviors of the real environment. But usually this kind of discontinuous behaviors is hard
[00:07:19.860 --> 00:07:26.320]   to learn for a neural network full of continuous operations. So we are thinking of, say, maybe
[00:07:26.320 --> 00:07:32.160]   we could compose a network, a network model that contains also discontinuous operations
[00:07:32.160 --> 00:07:37.200]   and then use that model to fit the entire physical dynamics. Maybe that will make the
[00:07:37.200 --> 00:07:42.080]   problem easier. So that's one way we could broaden the range of the tasks. Another thing
[00:07:42.080 --> 00:07:48.160]   is not on the task side, but on the solution side. Say, for example, in evolution, in the
[00:07:48.160 --> 00:07:54.440]   evolution researches, we have this spike in neural networks where the operations are discrete
[00:07:54.440 --> 00:07:59.760]   and not differentiable, not easily trainable with a supervised, sorry, with deep learning
[00:07:59.760 --> 00:08:05.040]   methods. Although there are strategies that try to, say, develop some surrogate losses,
[00:08:05.040 --> 00:08:10.160]   or say, surrogate functions try to bridge this gap so that spike neural network can
[00:08:10.160 --> 00:08:15.600]   also be trained with deep learning. But I think that's kind of, you know, kind of an
[00:08:15.600 --> 00:08:21.200]   entry view and it requires some creativity in designing search tricks. And also, like
[00:08:21.200 --> 00:08:26.760]   what I show here, like in the middle, is another work that we released, I think, two years
[00:08:26.760 --> 00:08:32.440]   ago where the network contains also some discrete functions. In these examples, we asked the
[00:08:32.440 --> 00:08:39.280]   agent to segment this input image into patches. And then the patches will vote to each other,
[00:08:39.280 --> 00:08:44.120]   say, who's the most important patches among all of them, right? Then the agent will select
[00:08:44.120 --> 00:08:49.760]   only the top K in this example, 10, the top K patches out of the entire image, and then
[00:08:49.760 --> 00:08:54.840]   make decisions based solely on the information provided by these 10 images. So that's why
[00:08:54.840 --> 00:09:01.320]   you see there are like white patches. So these white patches are where the agent decides
[00:09:01.320 --> 00:09:05.920]   where it should focus its attention on. Like in the car racing example above, you can see
[00:09:05.920 --> 00:09:11.960]   that almost all the white patches are on the road boundaries. So apparently, this agent
[00:09:11.960 --> 00:09:16.120]   figures out that to drive in the car, you need to watch out where the road is. And in
[00:09:16.120 --> 00:09:21.320]   the second example, in the bottom row, the agent, where the agent tries to dodge the
[00:09:21.320 --> 00:09:26.520]   fireballs from the enemies, it apparently focuses majorly on the enemies as well as
[00:09:26.520 --> 00:09:33.120]   the fireballs. So for this solution network, we have the sorting of the patches as well
[00:09:33.120 --> 00:09:39.240]   as the truncating of these patches. Both of them are like discontinuous operations, which
[00:09:39.240 --> 00:09:44.880]   is not easily trainable with backpropagation. And that's why in that paper, we trained with
[00:09:44.880 --> 00:09:50.320]   the CMES, a neural evolution algorithm. So with that said, what I try to say is that
[00:09:50.320 --> 00:09:56.440]   with neural evolutions, we could potentially broaden the task range and the solution range
[00:09:56.440 --> 00:10:02.880]   that we can explore and try and solve in the future. And the second reason is that neural
[00:10:02.880 --> 00:10:09.000]   evolution has been for a while. However, like most of the conventional, like for genetic
[00:10:09.000 --> 00:10:14.640]   algorithms, for example, the conventional implementations usually will spawn a population
[00:10:14.640 --> 00:10:21.160]   of potential solutions together with the tasks. Each solution is evaluated in a separate thread
[00:10:21.160 --> 00:10:26.480]   or maybe process, which will utilize, say, a different CPU, for example. And then in
[00:10:26.480 --> 00:10:32.360]   each iteration, these individuals will, after evaluation, their fitness scores, or say utility
[00:10:32.360 --> 00:10:38.360]   scores will be collected and evaluated back by the neural evolution algorithm, which will
[00:10:38.360 --> 00:10:44.520]   then try to get to who is the fittest, right? Try to make those the ancestor of the next
[00:10:44.520 --> 00:10:48.840]   generation. And this iteration goes on and on. So that's basically how neural evolution
[00:10:48.840 --> 00:10:54.640]   or say genetic algorithm works. So this process is okay. But in a conventional computational
[00:10:54.640 --> 00:11:01.560]   framework, this majorly relies on CPUs. And apparently, they mostly run on a cluster of
[00:11:01.560 --> 00:11:06.880]   machines. So this brought like two problems. First, maintaining a cluster for machine or
[00:11:06.880 --> 00:11:12.000]   setting up a cluster for machine is non-trivial. So this kind of engineering task is not designed
[00:11:12.000 --> 00:11:17.680]   by everyone, right? And the second thing is that if you try to implement this framework
[00:11:17.680 --> 00:11:23.560]   with say modern ML framework, say TensorFlow or PyTorch, because these frameworks were
[00:11:23.560 --> 00:11:30.880]   originally designed for deep learning tasks, and they didn't have this like a distributed
[00:11:30.880 --> 00:11:36.360]   computation in mind in terms of evolution computations. So the implementation is going
[00:11:36.360 --> 00:11:42.560]   to be tricky. And we often run into difficulties implementing this such a systems. Plus, this
[00:11:42.560 --> 00:11:50.640]   will require you to say, maintain in each process a separate policy. And in this way,
[00:11:50.640 --> 00:11:55.960]   essentially, each process is going to create an identical computational graph for the same
[00:11:55.960 --> 00:12:01.200]   policy, which is not a very efficient way of using resources. So with these two major
[00:12:01.200 --> 00:12:07.520]   difficulties, we present EvoJax here. So as I said previously, EvoJax is built on top
[00:12:07.520 --> 00:12:15.800]   of JAX, another recently popular ML framework. So with JAX, we changed, we implemented the
[00:12:15.800 --> 00:12:21.520]   three major components of neural evolution, the neural evolution algorithm, the policy,
[00:12:21.520 --> 00:12:29.080]   and the task, all in JAX. And we compiled it just in time. And we fully utilized JAX's
[00:12:29.080 --> 00:12:36.360]   auto vectorization mechanism provided by JAX. So by doing so, the benefit we get is that,
[00:12:36.360 --> 00:12:46.160]   so first on the algorithm side, from JAX and other examples, we got this advanced optimization
[00:12:46.160 --> 00:12:53.920]   functionalities. For example, we could use Adam or some other deep learning tailored
[00:12:53.920 --> 00:13:00.760]   tricks, all from this toolbox. So this gives us access to this matured machine learning
[00:13:00.760 --> 00:13:06.440]   like a tool. So that's one major benefit we get. And the second thing is that with auto
[00:13:06.440 --> 00:13:13.040]   vectorization provided by JAX, we no longer need to create a separate process for each
[00:13:13.040 --> 00:13:19.400]   evaluation process. Instead, we could only create a global policy network in JAX. And
[00:13:19.400 --> 00:13:26.000]   then during evaluation, instead, we provide the two batches of data. One is the parameters,
[00:13:26.000 --> 00:13:30.760]   that's the potential solutions. The other is the observations. So they are treated as
[00:13:30.760 --> 00:13:37.680]   batch data, just like in supervised fashions. And then the network will spit out the different
[00:13:37.680 --> 00:13:44.720]   actions for different policies. And then these outputs will be input to the third component,
[00:13:44.720 --> 00:13:52.280]   which is the vectorized task, also implemented in JAX. In this way, we observe high data
[00:13:52.280 --> 00:13:58.520]   throughput through the entire data pipeline. And usually, a task takes only minutes or
[00:13:58.520 --> 00:14:04.640]   tens of minutes to train, instead of days or maybe weeks in the conventional framework.
[00:14:04.640 --> 00:14:11.160]   So I think that's all I want to cover about the motivation about AbleJAX. Next, I want
[00:14:11.160 --> 00:14:19.480]   to present some of the demos, like what AbleJAX could do. Just give me a sec.
[00:14:19.480 --> 00:14:24.040]   In the meantime, I'm not sure, Christian, if you have any comments. I had a silly question.
[00:14:24.040 --> 00:14:29.280]   Please correct me. Please stop me if this is too stupid. You mentioned traditional frameworks
[00:14:29.280 --> 00:14:35.960]   weren't built for distributed training. So when we talk about PyTorch distributed or
[00:14:35.960 --> 00:14:42.440]   TensorFlow has similar extensions, how are they not sufficient? Could you please clarify?
[00:14:42.440 --> 00:14:50.960]   So the way we see it, I specifically said that they do not support distributed computation
[00:14:50.960 --> 00:14:57.800]   in the context of evolution strategies. So what I mean is that with supervised learning,
[00:14:57.800 --> 00:15:03.000]   you could distribute your data or your policy. And maybe part of the data is spread out on
[00:15:03.000 --> 00:15:08.240]   multiple devices. And maybe this policy network is partially resides on different devices.
[00:15:08.240 --> 00:15:13.560]   So that's what they can do. What I mean is that for each evaluation process, you need
[00:15:13.560 --> 00:15:22.840]   to -- this is specific in this genetic algorithm or neural evolution context. We need to create
[00:15:22.840 --> 00:15:30.640]   a computational graph for the same policy on each process. For this way, usually, for
[00:15:30.640 --> 00:15:37.160]   PyTorch, for example, it will complain, say, you cannot create on the same devices the
[00:15:37.160 --> 00:15:42.320]   exact policies, cannot spawn it twice. So it's kind of become tricky. You have to say
[00:15:42.320 --> 00:15:46.960]   if you are going to spawn this process using the multi-threading system, you have to use
[00:15:46.960 --> 00:15:55.080]   fork instead of sprawn. So it's doable, but it's non-trivial. So there is a layer of difficulty
[00:15:55.080 --> 00:16:03.080]   on top of it, which is not easy for us, like neural evolution practitioners.
[00:16:03.080 --> 00:16:16.920]   I have a related question. Because we had one of the others of RACS here. So it was
[00:16:16.920 --> 00:16:26.200]   okay. This is really cool. You can run the -- basically here would be kind of the environment
[00:16:26.200 --> 00:16:37.680]   on also on the TPU, which I mean, it's a bit crazy. But some of the examples, I don't know
[00:16:37.680 --> 00:16:49.600]   if they are kind of more traditional. So what I wanted to ask is, does it require the environment
[00:16:49.600 --> 00:16:56.680]   itself to also be compiled through JAX, or is it like you can just use whatever?
[00:16:56.680 --> 00:17:01.760]   I see. I see. Yeah, that's a great question. So right now for the current release, we require
[00:17:01.760 --> 00:17:08.240]   the task to be also implemented in JAX. So that is from a performance perspective. So
[00:17:08.240 --> 00:17:14.520]   when we developed Apple JAX, we tried to integrate the traditional tasks as well. But most of
[00:17:14.520 --> 00:17:20.720]   the traditional tasks, say, for example, it runs on CPUs, right? So although the policy
[00:17:20.720 --> 00:17:27.120]   and the optimization itself are run on GPU, but this data transportation between CPU and
[00:17:27.120 --> 00:17:32.800]   the GPU is going to kill the extra benefit. So that's why we recommend for currently all
[00:17:32.800 --> 00:17:38.960]   the tasks to be implemented in JAX. With that being said, there are other tasks, say, iSAC
[00:17:38.960 --> 00:17:44.560]   Gym, which is released by NVIDIA, right? So although it's not in JAX, it's currently implemented
[00:17:44.560 --> 00:17:49.960]   a major -- I mean, it's currently support PyTorch users. So in the next release, Apple
[00:17:49.960 --> 00:17:56.000]   JAX could also integrate with iSAC Gym, which is not in JAX, but in PyTorch. So we could
[00:17:56.000 --> 00:18:01.360]   train this kind of hardware accelerant environment as well, but not in the current release, just
[00:18:01.360 --> 00:18:04.680]   to be clear.
[00:18:04.680 --> 00:18:13.080]   And for environments that are not in JAX, like currently the strategy is kind of to
[00:18:13.080 --> 00:18:16.640]   run them on the host and send the data, something like that?
[00:18:16.640 --> 00:18:19.260]   Sorry, can you say it again?
[00:18:19.260 --> 00:18:24.720]   So for example, for environments that are not in JAX, which I guess that means they're
[00:18:24.720 --> 00:18:37.480]   not easily distributable to the cluster, they would run on the host machine, I don't know,
[00:18:37.480 --> 00:18:43.160]   the host environment, and then you send the data to the cluster. And that's kind of how
[00:18:43.160 --> 00:18:50.960]   it would work. Or currently, you require also the environment to be able to somehow distribute
[00:18:50.960 --> 00:18:51.960]   itself.
[00:18:51.960 --> 00:18:55.320]   Yeah, I see. I see. I see. That's a good question. So what comes with Apple JAX is
[00:18:55.320 --> 00:19:00.000]   that we -- although I said that there are three major components provided by Apple JAX,
[00:19:00.000 --> 00:19:06.040]   the algorithm, the policy, and the task, right? But beyond that, we also provide a super fast
[00:19:06.040 --> 00:19:11.960]   training loop to orchestrate, to maintain an entire training process that's transparent
[00:19:11.960 --> 00:19:18.720]   to the users. And due to this efficiency, we want to get this efficiency. So this training
[00:19:18.720 --> 00:19:26.240]   loop utilizes a function that's in JAX, it's called a scan. So we utilize that one to make
[00:19:26.240 --> 00:19:33.040]   this training loop really fast. Unfortunately, this function scan does not support other
[00:19:33.040 --> 00:19:38.600]   like environments that's not implemented in JAX or say in NumPy friendly way. So there
[00:19:38.600 --> 00:19:45.880]   is no way to incorporate the other relatively complex tasks. But if the task is simple like
[00:19:45.880 --> 00:19:50.760]   in NumPy, I guess that's okay. But if, for example, maybe PyBullet and other stuff, maybe
[00:19:50.760 --> 00:19:53.680]   we have some difficulties here right now.
[00:19:53.680 --> 00:20:02.400]   Interesting. So for example, the car environment you showed in the pictures, you ported it
[00:20:02.400 --> 00:20:03.400]   to JAX?
[00:20:03.400 --> 00:20:10.040]   No, no, no. So that one is used here to illustrate the potential solutions that we could try
[00:20:10.040 --> 00:20:16.720]   with Apple JAX. And for that one, the simulation environment is written in Box2D. So I see
[00:20:16.720 --> 00:20:21.440]   some people are trying to migrate Box2D to either NumPy or JAX. I don't know whether
[00:20:21.440 --> 00:20:27.000]   this is happening in the short term, but that's not what we have in the Apple JAX repository
[00:20:27.000 --> 00:20:28.000]   right now.
[00:20:28.000 --> 00:20:34.840]   Okay. So the requirement is everything in JAX?
[00:20:34.840 --> 00:20:42.760]   Yeah, in the current release. But as I said, this is for a reason, because we want to make
[00:20:42.760 --> 00:20:47.920]   everything faster, right? Making part of it run on CPU and the data transport between
[00:20:47.920 --> 00:20:53.720]   CPU and GPU will kill the entire benefit from accelerator. So I guess that's the reason.
[00:20:53.720 --> 00:21:00.440]   I have another question. I don't know if this is too deep, but I was kind of looking at
[00:21:00.440 --> 00:21:07.120]   the code. I saw some examples where you were using VMAP.
[00:21:07.120 --> 00:21:15.040]   But you're kind of also using something like PMAP, like using the TPU cluster. I don't
[00:21:15.040 --> 00:21:22.960]   know. This is true or you try to run them on CPU?
[00:21:22.960 --> 00:21:40.320]   So using VMAP is majorly a way to make the task easy. So as I said, for Apple JAX, we
[00:21:40.320 --> 00:21:46.240]   created these vectorized environments, right? But we don't want users to keep in consciousness
[00:21:46.240 --> 00:21:51.400]   and say this environment is going to be vectorized. So I have to always remember there's going
[00:21:51.400 --> 00:21:56.360]   to be a batch dimension. So we don't want users to keep that in mind. So that's why
[00:21:56.360 --> 00:22:03.080]   we use VMAP mostly in the task of creations. In that way, for example, the card poll could
[00:22:03.080 --> 00:22:08.840]   be composed as if it's just a single environment, right? And then later on, we just wrap it
[00:22:08.840 --> 00:22:13.720]   with VMAP and then it automatically becomes a vectorized environment. So that's why we
[00:22:13.720 --> 00:22:21.000]   use the VMAP. For PMAP, we just want to distribute. When you have multiple TPUs or GPUs, you want
[00:22:21.000 --> 00:22:26.120]   to distribute to both the policy network as well as the tasks and to multiple devices.
[00:22:26.120 --> 00:22:33.880]   So that's why we use this PMAP mostly to distribute this workload to multiple accelerators. Yeah,
[00:22:33.880 --> 00:22:37.960]   that's kind of the reason of using VMAP and PMAP here.
[00:22:37.960 --> 00:22:44.480]   That's really cool. So the card poll is implemented in JAX right now?
[00:22:44.480 --> 00:22:52.960]   Right, it's in Jax. It's kind of easy. I'll show that as well later. We provided a tutorial
[00:22:52.960 --> 00:22:59.280]   for how to create tasks, policies, and neural evolution algorithms all in JAX. So we have
[00:22:59.280 --> 00:23:04.560]   defined several interfaces. As long as these interfaces are implemented, the user extensions
[00:23:04.560 --> 00:23:07.760]   could be run seamlessly within AbleJAX.
[00:23:07.760 --> 00:23:16.960]   Yeah, that's really cool. So I don't know, because most people out there are probably
[00:23:16.960 --> 00:23:30.720]   thinking of something like OpenAI Gym. And I guess that uses sometimes Box2D. So you're
[00:23:30.720 --> 00:23:37.280]   kind of dependent on this idea of, okay, let's implement Box2D in JAX also.
[00:23:37.280 --> 00:23:43.280]   Yeah, what I'm saying is that, yes, I admit there will be some limitations here. Like
[00:23:43.280 --> 00:23:49.280]   for Box2D stuff, probably it's not easy to run it right now. At least not in the current
[00:23:49.280 --> 00:23:53.680]   release because we only support JAX-based environments. But it will be possible in the
[00:23:53.680 --> 00:23:58.960]   future, as I said. But again, although it's possible to run that, but the performance
[00:23:58.960 --> 00:24:04.240]   may not be that great. Because as I said, you are going to face this data transportation
[00:24:04.240 --> 00:24:10.960]   between CPUs and GPUs. So yeah, that's kind of the trade-off you have to pay there.
[00:24:10.960 --> 00:24:14.240]   Okay, interesting.
[00:24:14.240 --> 00:24:25.040]   Yeah, we expect, especially the researchers from neural evolution societies, to put their
[00:24:25.040 --> 00:24:31.280]   implementations of novel tasks within the framework of AbleJAX. In that way, they could
[00:24:31.280 --> 00:24:39.040]   quickly try their ideas and get some feedback in minutes or hours instead of days and weeks.
[00:24:39.040 --> 00:24:43.280]   Yeah, you'll see some interesting examples later in this talk.
[00:24:43.280 --> 00:24:51.600]   Sorry, I'll ask one question from the audience. Have you seen Evo-SACS and what is the
[00:24:51.600 --> 00:24:56.960]   difference between these two? Yeah, yeah, yeah. If you go to our GitHub front page,
[00:24:56.960 --> 00:25:02.800]   you can see that Evo-SACS is a sister project to AbleJAX. Actually, Robert, the author of
[00:25:02.800 --> 00:25:08.240]   Evo-SACS has contributed to quite a few PRs to AbleJAX. So the major difference is that
[00:25:08.240 --> 00:25:18.080]   we focus on different parts. AbleSACS has a wider range of neural evolution algorithms
[00:25:18.080 --> 00:25:25.280]   implemented, whereas AbleJAX focuses on the entire pipeline. So we have this algorithm,
[00:25:25.280 --> 00:25:30.640]   but at the same time, we focus on the easy integration of new tasks as well as novel
[00:25:30.640 --> 00:25:37.520]   policies. So the focus is different. I would say this is the major difference there. But as I said,
[00:25:37.520 --> 00:25:42.320]   if you go to our page, you will see some AbleSACS implementations already in AbleJAX,
[00:25:42.320 --> 00:25:46.720]   and it's runnable there. Thanks for answering that.
[00:25:47.920 --> 00:25:55.360]   Yeah. Okay. Do you mind if I proceed to demos? Sorry for asking so many questions, please.
[00:25:55.360 --> 00:26:02.880]   Yeah, yeah. No problem. No problem. Okay. I will present you two demos in this talk.
[00:26:02.880 --> 00:26:10.320]   So if you go to our GitHub, there is an example directory under which we have a lot of notebooks
[00:26:10.320 --> 00:26:15.600]   that are readily runnable. I'm going to show you. I guess you've already seen this Brex
[00:26:15.600 --> 00:26:32.080]   tasks before. Yeah. So I won't run the examples here because I saw the presentation by Daniel
[00:26:32.080 --> 00:26:36.880]   last time. He already presented similar examples. So what I want to demonstrate is that this
[00:26:36.880 --> 00:26:43.840]   notebook is mostly based on their notebook. So a lot of task creations code, we are using the
[00:26:43.840 --> 00:26:49.280]   same part. What's different is that the training part. So in this example, for example, we are
[00:26:49.280 --> 00:26:55.840]   training locomotion controller for this ant robot. So the difference is that this is the part that
[00:26:55.840 --> 00:27:02.480]   we replace the training code with AbleJAX. What I want to highlight is that... So first, let's look
[00:27:02.480 --> 00:27:11.120]   at the training time. So AbleJAX is able to train the ant task to a decent score. Actually, this
[00:27:11.120 --> 00:27:17.600]   score is higher than what they show in the Brex demo. I think it's within 10 minutes, something
[00:27:17.600 --> 00:27:24.480]   like that. And what I want to highlight, the other is that... So this is the training loop we provide
[00:27:24.480 --> 00:27:31.920]   here. As I mentioned previously, so in addition to the three major components provided by AbleJAX,
[00:27:31.920 --> 00:27:38.000]   we also provide a trainer and a simulation manager that orchestrates the entire training loop.
[00:27:38.000 --> 00:27:44.240]   And what I'm demonstrating here is the logic of this trainer stuff. So as you can see here,
[00:27:44.240 --> 00:27:50.960]   the code is relatively short and the logic, I think, is quite simple. For users, a user simply
[00:27:50.960 --> 00:27:57.040]   provides two tasks, one for training and the other for validations, right? And then just to create
[00:27:57.040 --> 00:28:03.520]   an instance of the policy that he or she wants to run, and then create a policy, a solver,
[00:28:03.520 --> 00:28:09.680]   which is the neural evolution algorithm in this notebook, which we provide a PGP algorithm. But
[00:28:09.680 --> 00:28:15.600]   there are a whole lot of other algorithms as well. Then you simply plug all this stuff into this
[00:28:15.600 --> 00:28:24.240]   sim manager, then run this loop. So if you want to write this loop, this is all you need to do.
[00:28:24.240 --> 00:28:30.960]   But apparently, actually, AbleJAX wraps this all in a trainer. So this entire process will be
[00:28:30.960 --> 00:28:37.440]   transparent to the users. What the users need to provide is this algorithm to specify this
[00:28:37.440 --> 00:28:45.040]   algorithm that he or she wants to use to create his or her own policies and the tasks, and that's
[00:28:45.040 --> 00:28:51.040]   all. So this will allow AbleJAX to train the policies smoothly. And within a short time,
[00:28:51.040 --> 00:28:57.840]   you will get some decent feedbacks. OK, so this is the first demo or code that I want to illustrate.
[00:28:57.840 --> 00:29:01.360]   The next thing is probably more interesting. Let me show here.
[00:29:01.360 --> 00:29:09.760]   So the next demo that I want to show-- I'm actually going to run this demo-- is, say,
[00:29:09.760 --> 00:29:16.240]   abstract painting. So in the previous slides, I showed an example of painting a concrete image,
[00:29:16.240 --> 00:29:22.480]   say, given an image of a Mona Lisa, and train an agent to paint with only triangles, say,
[00:29:22.480 --> 00:29:28.800]   50 triangles, to arrange this triangle such that the result of the image resembles the given image.
[00:29:28.800 --> 00:29:36.640]   So that was the previous demo. In this demo, we are using the clip model, where the agent will
[00:29:36.640 --> 00:29:42.480]   try to fit a concept. For example, in here, if I say the prompt is "Great Wave Kanagawa,"
[00:29:42.480 --> 00:29:49.680]   then the agent will, through learning, say, learn to paint this kind of images. So eventually,
[00:29:49.680 --> 00:29:54.560]   it's going to give you this kind of abstract painting that resembles the famous painting
[00:29:54.560 --> 00:30:00.720]   in Japan that's the Wave of the Kanagawa. Notice that the resemblance between these paintings,
[00:30:00.720 --> 00:30:06.800]   they both have this mountain Fuji here. And you can see here, this kind of resembles a wave here,
[00:30:06.800 --> 00:30:16.320]   right? So that's what Ablegex is capable of doing. So this thing, what we want to demonstrate in
[00:30:16.320 --> 00:30:24.080]   this notebook is that Ablegex is a modulized library. So although I said there are three
[00:30:24.080 --> 00:30:29.520]   components that the users could use, users are not required to use all of them. Actually,
[00:30:29.520 --> 00:30:35.840]   you could extract any one of them and use them as a standalone part. For this demo, we used only the
[00:30:35.840 --> 00:30:43.200]   evolution, neural evolution optimization part, and used that part to train a network. So the code
[00:30:43.200 --> 00:30:53.120]   itself runs on TPUs and GPUs, and it could scale really well. For example, if you have eight GPUs,
[00:30:53.120 --> 00:30:59.440]   then probably you run twice as fast as you run on four GPUs, and same applies for TPUs. What I want
[00:30:59.440 --> 00:31:05.680]   to mention is that this work was previously implemented by Intel, my colleague, and before
[00:31:05.680 --> 00:31:14.960]   Ablegex, the setup requires both decent CPUs and GPUs. But after we adopted Ablegex, this notebook
[00:31:14.960 --> 00:31:21.520]   can actually run on a single colab. So you can see the complexity of the hardware is actually
[00:31:21.520 --> 00:31:26.960]   simplified, thus making this demo more accessible to a broader range of audiences.
[00:31:26.960 --> 00:31:32.960]   Okay, I promised you to run this demo, so let's try a different idea, for example.
[00:31:33.440 --> 00:31:42.000]   I'm going to go to this page, for example. Let's try instead of grid wave, let's try,
[00:31:42.000 --> 00:31:46.000]   say, I don't know, maybe brain, for example.
[00:32:00.080 --> 00:32:01.200]   Okay, it's training.
[00:32:01.200 --> 00:32:12.400]   Yeah, right now, at first, you can see those triangles are really randomly placed there.
[00:32:12.400 --> 00:32:18.320]   It doesn't form any like a meaningful picture, but after a while, you are going to see some
[00:32:18.320 --> 00:32:23.200]   patterns. And while it's training, I want to show you the GPU usage here.
[00:32:26.960 --> 00:32:31.360]   Yeah, so you see Ablegex fully utilizes all the GPUs that reside on the machine,
[00:32:31.360 --> 00:32:36.560]   so we see really high throughput when we train with Ablegex here.
[00:32:36.560 --> 00:32:46.240]   And as you can see here, yeah, I don't know about you, but I guess this is supposed to be a brain
[00:32:46.240 --> 00:32:55.120]   that Ablegex is trying to evolve here. Yeah, so right now, as you can see, each 50 iterations
[00:32:55.120 --> 00:33:06.080]   takes about 10, 13 seconds. So this task, I'm currently running on 4NV100 GPUs. So with more
[00:33:06.080 --> 00:33:14.640]   GPUs, this experiment will finish sooner in a relatively shorter time. Yeah, we'll just let it
[00:33:14.640 --> 00:33:15.200]   run. Yeah.
[00:33:15.200 --> 00:33:22.080]   - I just wanted to comment that this also feels like it's running faster than a Clip+VQGAN,
[00:33:22.080 --> 00:33:27.440]   although I've not tried it on a 4NV100 system, but it looks like it's running faster than that.
[00:33:27.440 --> 00:33:30.800]   - Sorry, it's running faster than?
[00:33:30.800 --> 00:33:37.520]   - Have you seen the Clip+VQGAN collab that has been out there, which is somewhat similar?
[00:33:37.520 --> 00:33:42.800]   - Okay, okay, okay. I haven't played with that notebook. Yeah, yeah, what we are trying to say
[00:33:42.800 --> 00:33:50.320]   is that Intel really put great effort into this implementation. So we made sure that this notebook
[00:33:50.320 --> 00:33:56.320]   or Ablegex could fully utilize all the hardware resources that the user is providing and try to
[00:33:56.320 --> 00:33:58.640]   give results as soon as possible.
[00:33:58.640 --> 00:34:09.840]   - Just like a question, you're running on this collab, but you're telling it to run on your
[00:34:09.840 --> 00:34:10.960]   local machine or?
[00:34:10.960 --> 00:34:17.280]   - Yeah, yeah, I'm running it on my GCP machine. The reason is that for default collab users,
[00:34:17.280 --> 00:34:24.560]   which I am, you will only get a K40 or maybe K80, right? So I want to demonstrate that
[00:34:24.560 --> 00:34:32.000]   we could scale really good on multiple GPUs, so that's why I connected to a local GCP machine.
[00:34:32.000 --> 00:34:38.800]   - Okay, yeah, just to clarify, because you never get more than one GPU on...
[00:34:38.800 --> 00:34:44.320]   - But you could run on TPU as well, because by default you get eight core TPUs. So this
[00:34:44.320 --> 00:34:52.480]   code runs on TPU as well, just for the record. - Yeah, it would be really cool. Does it run on
[00:34:52.480 --> 00:35:01.920]   the TPUs like V2, like the ones you get on free collab?
[00:35:01.920 --> 00:35:06.720]   - Yeah, yeah, what I meant is like a free collab, you could run the same code on free collab as
[00:35:06.720 --> 00:35:10.400]   well, no problem. - Okay, yeah, it would be...
[00:35:10.400 --> 00:35:14.160]   - Yeah, I just wanted to tell you that because the Brax example was running on TPU, right? So
[00:35:14.160 --> 00:35:17.200]   I tried a different hardware accelerator in this example, right?
[00:35:17.200 --> 00:35:26.720]   - Okay, awesome. When it's running on... Because you showed you had like four GPUs or something like
[00:35:26.720 --> 00:35:31.200]   that. - Yeah, yeah, like if I go to the top of it,
[00:35:31.200 --> 00:35:37.120]   top of the notebook, you can see I'm currently running on four GPUs.
[00:35:37.120 --> 00:35:44.720]   - Like when you're distributing it, like you use, for example, a PMAP, right?
[00:35:44.720 --> 00:35:49.920]   - Yeah, yeah, we're using PMAP, yeah. - And you just divide it in four?
[00:35:49.920 --> 00:35:56.800]   - Yeah, we divided it by the batch size by four and then distributed it to the devices.
[00:35:56.800 --> 00:36:02.320]   - And then within each device, because I'm thinking, or I don't know if this is true,
[00:36:02.960 --> 00:36:10.240]   like, okay, maybe within each device, you can also divide it into whatever, let's say 10.
[00:36:10.240 --> 00:36:17.040]   - Why do you want to divide it further into a single device? I'm not sure I understand.
[00:36:17.040 --> 00:36:25.520]   - I'm just thinking of ideas. Is it computationally... Because, for example,
[00:36:25.520 --> 00:36:35.120]   if you have a super big model, well, I mean, usually that fits each of your devices really
[00:36:35.120 --> 00:36:43.840]   quick. But I don't know if this is a different type of neural evolution, but sometimes these
[00:36:43.840 --> 00:36:50.720]   evolutionary algorithms are each model is rather small, or I don't know if these models are bigger.
[00:36:51.360 --> 00:36:54.880]   So I was thinking, okay, if each model is small, then maybe you could
[00:36:54.880 --> 00:37:01.360]   subdivide it within each device to get whatever speed up, I don't know.
[00:37:01.360 --> 00:37:10.400]   - Yeah, yeah, the thing is that, yeah, you're right. Especially for deep learning tasks,
[00:37:10.400 --> 00:37:13.120]   because we have this clip model here, right? Some models could be large.
[00:37:14.640 --> 00:37:22.880]   It could consume a lot of the memory spaces. But the point is that we could quickly distribute all
[00:37:22.880 --> 00:37:29.120]   the workload evenly, in this example, for example, evenly to all the hardware devices available on
[00:37:29.120 --> 00:37:34.240]   the platform. And so, I mean, that's kind of the best you can do, right? With all the hardware.
[00:37:34.240 --> 00:37:37.520]   That's the efficient way of using all the hardware accelerators.
[00:37:37.520 --> 00:37:48.240]   - Okay, I had a related question. Because maybe I'm thinking, so most of the intuitions I have
[00:37:48.240 --> 00:37:53.840]   about neural evolution come, actually they come from NEET, from the NEET algorithm, which-
[00:37:53.840 --> 00:37:55.760]   - Yeah, NEET is one of them, yeah, yeah, right, right.
[00:37:55.760 --> 00:38:06.240]   - But that algorithm, I don't know if it's maybe too dynamic for the kind of stuff
[00:38:07.280 --> 00:38:17.440]   that maybe you're doing here, because I see a lot of ES strategies, which I guess
[00:38:17.440 --> 00:38:23.040]   are a bit different than NEET. Can you clarify what kind of algorithms
[00:38:23.040 --> 00:38:27.440]   you're running on this neural evolution? Because I remember NEET
[00:38:27.440 --> 00:38:32.480]   build a dynamic graph of operations. - Yeah, yeah, sure, sure, sure. Right, right,
[00:38:32.480 --> 00:38:38.080]   right. I actually have one slide, so later on to explain, to demonstrate all the available
[00:38:38.080 --> 00:38:45.040]   supported algorithms currently in Evoljax. But just to give you a slide to tell you here.
[00:38:45.040 --> 00:38:50.800]   So, yeah, yeah, you're right. What we are implementing right now are majorly genetic
[00:38:50.800 --> 00:38:55.840]   algorithms or say evolution strategies. But there are a lot of other more, right? Like NEET being
[00:38:55.840 --> 00:39:01.840]   one of them, right? It is possible to implement NEET and integrate that into Evoljax, no problem.
[00:39:01.840 --> 00:39:06.800]   As long as the interfaces that we designed is implemented, then it could be inserted and run
[00:39:06.800 --> 00:39:14.480]   smoothly on this entire platform. So that's the major message that we want to send to the users.
[00:39:14.480 --> 00:39:19.440]   - I'll wait till we get to that slide. I'm very curious about that.
[00:39:19.440 --> 00:39:25.280]   - Yeah, cool, cool. Yeah, thanks for the question. Yeah, okay. So, yeah, it's currently
[00:39:25.280 --> 00:39:33.120]   training, but you can see this is almost brain like stuff, right? So, yeah, feel free to play
[00:39:33.120 --> 00:39:38.880]   with the notebook and try your own ideas. So here's some like a funny story about this idea.
[00:39:38.880 --> 00:39:48.400]   So before release of Evoljax, Intel and I actually tried to design a logo for Evoljax using Evoljax.
[00:39:48.400 --> 00:39:54.720]   So we tried like a various of keywords, like a neuroevolution and maybe something like a science
[00:39:54.720 --> 00:40:00.560]   and math there. So we got some interesting images there, but we have different preferences. So we
[00:40:00.560 --> 00:40:07.840]   never settled to one single image that could represent the icon of Evoljax. But that's just
[00:40:07.840 --> 00:40:13.040]   another interesting story that I want to share with you. So that's kind of a cool usage for
[00:40:13.040 --> 00:40:19.360]   Evoljax, especially this ES clip demo, right? To design some logos using evolution strategies.
[00:40:20.880 --> 00:40:28.960]   Okay, I want to get back to the slides here. Yeah, so what I presented previously is the first
[00:40:28.960 --> 00:40:37.040]   two demos. Another one that I want to highlight is this multi-agent training is possible in Evoljax.
[00:40:37.040 --> 00:40:43.280]   So what I mean by multi-agent training is that, so this is the world example that I showed you
[00:40:43.280 --> 00:40:50.160]   earlier. But right now this is a modified version where say we spawned entire population into the
[00:40:50.160 --> 00:40:56.800]   same environment and these agents can see each other. So that means there are interactions between
[00:40:56.800 --> 00:41:03.360]   these agents. So what we find is that with this kind of a design, so first of all, it's apparently
[00:41:03.360 --> 00:41:09.520]   possible to train multiple agents in a single environment, in a single rollout. The benefit of
[00:41:09.520 --> 00:41:16.400]   this scheme is that because the agents could see each other, so the environment has a certain
[00:41:16.400 --> 00:41:23.760]   richness brought by these interactions between agents. And this kind of richness, environmental
[00:41:23.760 --> 00:41:30.560]   richness, is way beyond human design. This is good for especially reinforcement learning because
[00:41:30.560 --> 00:41:36.960]   that, say we all know that the amount of data matters a lot for supervisor learning, right?
[00:41:36.960 --> 00:41:43.600]   The equivalent of data is the task's richness. So with more reach for tasks, the agent you trained
[00:41:43.600 --> 00:41:50.320]   could be better, say either more skillful behaviors or maybe robustness demonstrated
[00:41:50.320 --> 00:41:58.720]   by the agent. So this kind of a multi-agent training presented by Evoljax is what could
[00:41:58.720 --> 00:42:03.760]   possibly bring better performance for future reinforcement learning practitioners.
[00:42:03.760 --> 00:42:09.600]   Yeah, okay. So these are the demos that I want to highlight in Evoljax.
[00:42:10.240 --> 00:42:16.720]   Okay, in the next three slides, I want to talk about user extensions. So that's also the
[00:42:16.720 --> 00:42:23.600]   question that Christian previously had, right? So the first component that I want to describe
[00:42:23.600 --> 00:42:29.280]   is the algorithm component. So as I said, we got three major components. The algorithm is the first
[00:42:29.280 --> 00:42:38.640]   part. So we have carefully designed an interface in Evoljax where you simply need to implement this
[00:42:38.640 --> 00:42:45.360]   MLE algorithm interface, which has got two methods, the ask method and the tell method.
[00:42:45.360 --> 00:42:51.360]   And these methods are already familiar for most of the neuroevolution practitioners,
[00:42:51.360 --> 00:42:57.760]   I think, where the ask method simply asks the neuroevolution algorithm for a collection of
[00:42:57.760 --> 00:43:04.960]   candidate solutions to be evaluated. And the tell method simply reports back the fitness scores of
[00:43:04.960 --> 00:43:11.280]   each of the populations so that the neuroevolution algorithm could update its internal state for the
[00:43:11.280 --> 00:43:17.840]   next generation. So as I said, the user simply has to implement these two methods and then
[00:43:17.840 --> 00:43:23.440]   his or her algorithm will be ready to run in Evoljax. So as I said, we've already
[00:43:23.440 --> 00:43:32.400]   received a lot of PRs from external contributors, Evoljax being one of them. So in this table,
[00:43:32.400 --> 00:43:38.880]   what I'm demonstrating is the currently supported algorithms in Evoljax. For each of the algorithms,
[00:43:38.880 --> 00:43:45.520]   we provide a benchmark for all the tasks and you can actually see the scores, how each algorithm
[00:43:45.520 --> 00:43:51.600]   is performing. So this information, we hope that this information could provide some useful
[00:43:51.600 --> 00:43:58.080]   insights to the users when he or she wants to select the appropriate algorithm to run for his
[00:43:58.080 --> 00:44:07.680]   or her tasks. So that's what I want to show you here. I want to give you a walkthrough of the
[00:44:07.680 --> 00:44:15.680]   extension of the code quickly. Let me just pick one really simple example. For example,
[00:44:15.680 --> 00:44:22.720]   this CMA wrapper. So in this CMA wrapper, we simply wrapped an existing implementation,
[00:44:22.720 --> 00:44:29.520]   which runs on CPU. But we demonstrate that that's also possible to run in Evoljax. It's just that
[00:44:29.520 --> 00:44:36.640]   the performance may not be as good in terms of speed. So as you can see, first, this code has
[00:44:36.640 --> 00:44:44.160]   only maybe less than 100 lines. So what I meant is that a user extension is simple in terms that,
[00:44:44.160 --> 00:44:49.920]   for example, if he or she already has some implementations of a certain algorithm
[00:44:49.920 --> 00:44:55.840]   and want to integrate quickly into Evoljax to test its performance, for example, just to test
[00:44:55.840 --> 00:45:02.320]   the performance, then what he or she can do is just to create a wrapper, create an instance in the
[00:45:02.320 --> 00:45:10.400]   instantiation method, then simply call the ask and tell methods in the corresponding method,
[00:45:10.400 --> 00:45:15.440]   which I believe is widely available in the majority of the algorithms. So that's all the
[00:45:15.440 --> 00:45:21.520]   user needs to do. So as long as interfaces are implemented, the algorithm could be run smoothly
[00:45:21.520 --> 00:45:27.520]   in Evoljax. So it's just that simple. OK, back to the slide.
[00:45:27.520 --> 00:45:41.360]   OK, the next thing that is also the major focus of Evoljax is that we try to provide a lot of the
[00:45:41.360 --> 00:45:48.720]   examples in Evoljax to try to demonstrate the capability of this software toolkit. For example,
[00:45:48.720 --> 00:45:56.000]   in the first release, we provided six tasks, say MNIST and sequence-to-sequence. So these two tasks
[00:45:56.000 --> 00:46:02.320]   fall into the supervised learning domain. We want to demonstrate that Evoljax could train,
[00:46:02.320 --> 00:46:08.320]   supervise, and learn the tasks also quickly. For example, MNIST could be trained on a single GPU
[00:46:08.320 --> 00:46:15.520]   within five minutes and reach a testing accuracy above 98%, which was not possible for a
[00:46:15.520 --> 00:46:23.440]   conventional implementation that utilizes only, say, CPUs. And for sequence-to-sequence, that's a
[00:46:23.440 --> 00:46:30.160]   demo provided by Flex, which in this task, the agent will receive a sequence, say this kind of
[00:46:30.160 --> 00:46:36.000]   a simple math, right? 1, 2, 3, plus 4, 5, 6, for example. And the agent is supposed to give out
[00:46:36.000 --> 00:46:44.320]   the correct answer also in terms of a string or say embedding space. So in this task, we purposefully
[00:46:44.320 --> 00:46:51.360]   composed a relatively large network. It contains, I guess, hundreds of thousands of weights. We want
[00:46:51.360 --> 00:46:59.040]   to demonstrate that Evoljax is capable of training a moderately large network instead of only focusing
[00:46:59.040 --> 00:47:05.760]   on small networks. So these are the purpose of these two tasks. Then in the next set, we trained
[00:47:05.760 --> 00:47:12.960]   on CardPol Swing and this robotic control task majorly with Brax to demonstrate the speed up,
[00:47:12.960 --> 00:47:20.640]   right? We could train these tasks really fast, usually 10 or 20 times speed up. Then we demonstrated
[00:47:20.640 --> 00:47:25.920]   two novel tasks, the water world and the abstract painting that I just demonstrated. The last
[00:47:25.920 --> 00:47:33.440]   example is created by our intern, Mayu Ogishi, recently, where she created this flocking
[00:47:33.440 --> 00:47:38.480]   simulation system. So what you are seeing here, so this is the environment. What you are seeing
[00:47:38.480 --> 00:47:46.080]   here, so these blue like buoys represent the fishes in a pond, for example. And each fish
[00:47:46.080 --> 00:47:53.680]   could observe the nearest five fishes around itself. It is required to, they all move in
[00:47:53.680 --> 00:48:00.160]   constant speed, but they could select the orientation to move into the next time step.
[00:48:00.160 --> 00:48:08.160]   So the goal is everybody try to speed out a coherent action such that the entire flocking
[00:48:08.160 --> 00:48:15.280]   could move in a more coherent fashion, what we observe in nature. So we defined
[00:48:15.280 --> 00:48:22.640]   several reward functions in the task. And then we train an MLP to simply simulate the
[00:48:22.640 --> 00:48:28.400]   behaviors of these agents. And as you can see here, we kind of reached our goal, right? Each
[00:48:28.400 --> 00:48:35.680]   agent is moving in a smooth fashion, in a coherent fashion here. So this is for task integration.
[00:48:35.680 --> 00:48:43.600]   Again, all the users need to do is to implement this, say, vectorize the task interface,
[00:48:43.600 --> 00:48:50.240]   which have this step and the reset interface, which is quite similar to OpenAI's gym environment,
[00:48:50.240 --> 00:48:56.960]   right? You simply implement these two functions and you're ready to go. And as I
[00:48:56.960 --> 00:49:04.160]   mentioned earlier in this talk, although this is a vectorized task, the user does not have to
[00:49:04.160 --> 00:49:10.320]   remind him or herself that this is a vectorized task. So I have to keep the batch dimension
[00:49:10.320 --> 00:49:16.400]   in mind. Instead, you just write it as a single task and then later wrap it with the Vmap function.
[00:49:16.400 --> 00:49:21.520]   Then that's all you need to do. The Jaxx will just make auto-vectorization of the entire task.
[00:49:21.520 --> 00:49:29.120]   And lastly, what I want to show you is the policy. So that's another component, a major
[00:49:29.120 --> 00:49:34.400]   component on Avvo Jaxx, right? So for policy, it's the neural network part that the user
[00:49:34.400 --> 00:49:41.280]   could compose. For policy, the major function that a user wants to implement is this
[00:49:42.080 --> 00:49:49.280]   getAction function. So the getAction function, unlike in, say, traditional policies, it receives
[00:49:49.280 --> 00:49:56.160]   states. And also, besides that, it will also receive the parameters for the policies.
[00:49:56.160 --> 00:50:02.960]   Because for Avvo Jaxx, we have a global policy, one policy, one instance of policy. And instead,
[00:50:02.960 --> 00:50:07.680]   it accepts batches of data, one batch being the states, the observations, the other being the
[00:50:07.680 --> 00:50:15.360]   parameters. Then they are matched one by one so that the global policy could generate different
[00:50:15.360 --> 00:50:21.680]   behaviors for different, say, individuals in the population. And that's how we eliminate this,
[00:50:21.680 --> 00:50:28.080]   say, separated policies in each process problem. Right now, in Avvo Jaxx, we have implementations
[00:50:28.080 --> 00:50:33.680]   of four kinds of policies. The feed-forward network, which is simply MLPs, and the convolution
[00:50:33.680 --> 00:50:40.160]   net, that's what we used for MNIST classification. And recurrent networks, including a sequence-to-
[00:50:40.160 --> 00:50:45.520]   sequence model. And finally, we have this permutation-invariant MLP, which just wants
[00:50:45.520 --> 00:50:52.640]   to demonstrate a more complicated policy is also possible within Avvo Jaxx. Yeah, so
[00:50:52.640 --> 00:50:57.600]   yeah, I think that's all I want to cover in this talk.
[00:50:59.840 --> 00:51:10.400]   I had a question. Yeah, sure, sure. So first, I get that, okay, Avvo Jaxx is kind of like,
[00:51:10.400 --> 00:51:18.160]   more like, okay, we're going to define these interfaces that play well within each other.
[00:51:18.160 --> 00:51:26.560]   And I guess the trainer puts them all together, right? So if some folks want to concentrate on
[00:51:26.560 --> 00:51:34.960]   creating new tasks, and they can do that, then I guess Avvo Jaxx. But what the previous question
[00:51:34.960 --> 00:51:47.360]   asked would be more on the, well, I guess on the ask-and-tell interface, which what was that one
[00:51:47.360 --> 00:51:59.600]   called? I'm guessing the algorithm? So there's a task, there's the, I guess, the environment,
[00:51:59.600 --> 00:52:06.880]   and what was the other interface? Yeah, we have an algorithm interface, we have a policy interface,
[00:52:08.400 --> 00:52:19.680]   task interfaces. Okay, so for example, in the policy one, that's where, for example,
[00:52:19.680 --> 00:52:26.480]   you would code something with flags or something like that. Yeah, yeah, in the policy, yeah. Let
[00:52:26.480 --> 00:52:38.240]   me just give you a quick example. Yeah, so these are the policies we currently have. So we have
[00:52:39.200 --> 00:52:46.880]   simplified a task, for example, this MLP, where we use flags to compose network, although this is
[00:52:46.880 --> 00:52:53.120]   not necessary, but I think that's a great library, so we use that one. As you can see here, so this
[00:52:53.120 --> 00:52:58.640]   is the policy itself, right? We simply compose it, so you can do that with other framework as well.
[00:52:58.640 --> 00:53:04.560]   Then what we do is simply get the parameters there, right? So simply initialize this entire
[00:53:04.560 --> 00:53:11.520]   policy network, get the placeholders for the parameters, then try to initialize them so that
[00:53:11.520 --> 00:53:20.400]   later on we could use these placeholders to feed in the real parameters to get the actions.
[00:53:20.400 --> 00:53:30.000]   Okay, it's really cool. So I could use, let's say, a task and environment, but then code my
[00:53:30.000 --> 00:53:37.360]   own policy, and then if they follow the interface, then everything should work. Yeah, yeah, yeah,
[00:53:37.360 --> 00:53:44.160]   so what I'm showing, MLP is a relatively simple policy, right? So you can do all the crazy stuff,
[00:53:44.160 --> 00:53:50.400]   for example, this is what I mentioned, we have this permutation invariant policy that's from our
[00:53:50.400 --> 00:53:55.840]   earlier paper. So you can see the policy is kind of complex compared with the previous one, right?
[00:53:57.200 --> 00:54:00.240]   I guess it's this part, so it's relatively longer, it contains all the
[00:54:00.240 --> 00:54:07.520]   LSTMs here and some attention mechanisms here as well, but as long as the interfaces are implemented,
[00:54:07.520 --> 00:54:15.440]   the whole complexity is hidden away, users don't have to worry about that, and AppleJax can simply
[00:54:15.440 --> 00:54:22.240]   run the entire, and the code interface to run the policies smoothly and integrate the entire stuff
[00:54:22.240 --> 00:54:30.400]   into the training pipeline. I see, so and then I guess the training algorithm, like for example,
[00:54:30.400 --> 00:54:38.000]   evolution strategies, that would be, I mean, it would be the other interface. Yeah.
[00:54:38.000 --> 00:54:44.880]   Okay, this is really cool. So I don't know, this is more general because I don't know,
[00:54:46.080 --> 00:54:53.040]   I think most of us don't work with evolution strategies daily. Can you tell me a bit about
[00:54:53.040 --> 00:55:06.800]   this versus regular reinforcement learning? What's the difference between the evolution
[00:55:06.800 --> 00:55:13.360]   strategies versus gradient-based methods, I guess it's kind of what I'm asking.
[00:55:13.360 --> 00:55:19.040]   Yeah, I see, I see. So, when I earlier published the paper, it says that evolution strategy is an
[00:55:19.040 --> 00:55:24.480]   alternative to reinforcement learning, right? So that's kind of the position there. So evolution,
[00:55:24.480 --> 00:55:31.200]   I like it to solve this kind of reinforcement tasks, like using PPO or say other gradient-based
[00:55:31.200 --> 00:55:36.080]   RL algorithms one way, but ES is the other, or say neural evolution algorithm is the other way,
[00:55:36.080 --> 00:55:43.040]   right? Actually in some tasks, for example, like traditional Atari games, neural evolution
[00:55:43.040 --> 00:55:48.960]   algorithm has actually given better performances than other traditional RL algorithms, right? So
[00:55:48.960 --> 00:55:53.200]   that's kind of promising. Another difference is that what I mentioned earlier in the talk,
[00:55:53.200 --> 00:55:58.960]   so most of the algorithms rely on differentiability, at least for the policy networks,
[00:55:58.960 --> 00:56:05.200]   but for neural evolution, we don't have such constraints. We don't rely on the differentiability
[00:56:05.200 --> 00:56:11.680]   of the networks, right? So this gives you all the freedom in exploring some crazy architectures,
[00:56:11.680 --> 00:56:18.000]   right? As I mentioned, you could try to patch the entire screen and then try to pick out the
[00:56:18.000 --> 00:56:23.520]   most important patches instead of using the entire screen being one example, right? So this kind of
[00:56:23.520 --> 00:56:31.280]   advantage is brought by neural evolution algorithms. Yeah, and one more difference,
[00:56:31.280 --> 00:56:39.200]   it's not a difference that I want to mention is that for one interesting stuff,
[00:56:40.320 --> 00:56:45.680]   a major contribution from RL is that they provided this value-based method, right? Where
[00:56:45.680 --> 00:56:51.840]   you learn a value network to estimate how good or bad the current situation is, right? By collecting
[00:56:51.840 --> 00:56:59.280]   data, say from a random policy or say a behavior policy to sample data, then run a value network
[00:56:59.280 --> 00:57:06.160]   on top of this buffered data, right? So this was possible because all of this run on a stand-alone
[00:57:06.160 --> 00:57:13.360]   machine. So this data collection resembles the supervised learning work, which we usually see,
[00:57:13.360 --> 00:57:19.760]   right? This is difficult to implement in conventional neural evolution frameworks
[00:57:19.760 --> 00:57:25.520]   because all the processes are distributed on different machines. And when you collect
[00:57:25.520 --> 00:57:30.960]   experience, you have to transform back into a centralized location to learn this value network.
[00:57:30.960 --> 00:57:36.560]   It's doable, but it's apparently tedious, right? But right now with AppleJax, because everything
[00:57:36.560 --> 00:57:41.360]   runs on a stand-alone machine, so we could do that as well, right? We have all the data collected
[00:57:41.360 --> 00:57:48.480]   on a single machine. So it's possible to learn a value network on this machine and use that value
[00:57:48.480 --> 00:57:55.680]   network to evaluate how good or bad the current situation is and provide training signals for the
[00:57:55.680 --> 00:58:01.120]   neural evolution algorithms as well. So that's another advantage if you are using AppleJax now.
[00:58:01.120 --> 00:58:06.320]   When you say with AppleJax you have it on a single machine,
[00:58:06.320 --> 00:58:16.880]   like what does that mean exactly? I mean a single process, sorry. So you previously
[00:58:16.880 --> 00:58:23.760]   have to spawn different processes or different threads, right? To do evaluations. So each
[00:58:23.760 --> 00:58:29.920]   process or thread is going to collect its own data, observation data, right? You have to somehow
[00:58:29.920 --> 00:58:37.040]   collect them and buffer them into the same replay buffer for a value network to run, to learn. So
[00:58:37.040 --> 00:58:42.880]   this data collection is kind of tedious. What I mean right now is that with AppleJax, because we
[00:58:42.880 --> 00:58:49.040]   run in a single thread, you run data collection, it's just from a vectorized task. So naturally,
[00:58:49.040 --> 00:58:54.560]   it's easy to store these experiences, right? Then you apply the same process, you run the value
[00:58:54.560 --> 00:58:59.440]   network on top of this play buffer. So that's kind of easier for the practitioners.
[00:58:59.440 --> 00:59:07.200]   Okay, I see. I have a question. I remember, I don't read a lot of reinforcement learning,
[00:59:07.200 --> 00:59:16.960]   but I read the original ES strategies paper, which was a long time ago, I guess. And I remember they
[00:59:16.960 --> 00:59:28.480]   said like, okay, like you can beat RL, but you need like a cluster, like with a thousand CPUs.
[00:59:28.480 --> 00:59:30.320]   Right, right, right.
[00:59:30.320 --> 00:59:37.280]   So it was, okay, like you can beat it, but you have to scale massively. I remember that was kind
[00:59:37.280 --> 00:59:53.680]   of the idea. So here, like with EvoJax, we're in the same like regime, like you still need like a
[00:59:53.680 --> 01:00:03.200]   lot of, let's say compute, or I don't know, because here we would be talking about probably TPUs or
[01:00:03.200 --> 01:00:14.000]   something, but the idea is still to have a large number of, let's say, agents to eventually beat
[01:00:14.000 --> 01:00:21.760]   RL in speed. Or has there been other breakthroughs that make it such that you don't need a lot of
[01:00:21.760 --> 01:00:22.900]   hardware?
[01:00:22.900 --> 01:00:31.120]   So the thing is that, like in terms of hardware, amount of hardware, so we probably don't need
[01:00:31.120 --> 01:00:38.080]   CPUs, but we need GPUs or TPUs, right? So that's not going to change. You want something, you got
[01:00:38.080 --> 01:00:44.160]   to have the appropriate hardware. What's changing is that for CPUs, you have to maintain clusters.
[01:00:44.160 --> 01:00:50.480]   For example, most of my previous experiments run on Kubernetes, so that's clusters, orchestrates,
[01:00:50.480 --> 01:00:56.000]   right? So running experiments on that is fast, but you have to spawn a lot of the machines,
[01:00:56.000 --> 01:01:00.720]   and to maintain this cluster is non-trivial. And probably machine learning practitioners don't
[01:01:00.720 --> 01:01:07.120]   want to maintain those clusters, they want to kind of avoid them, right? So for some tasks,
[01:01:07.120 --> 01:01:13.760]   maybe you need not only CPUs, but also you need maybe for some parts, for example, if the task
[01:01:13.760 --> 01:01:20.400]   relies on a deeper model, then you also need to use GPUs for the task itself, right? So this kind
[01:01:20.400 --> 01:01:28.640]   of a mixture of hardware, so add one more layer of complexity for setups, whereas in AppleJax,
[01:01:28.640 --> 01:01:35.200]   so you can run all of this on GPUs, so that's all right. You don't set up clusters, you don't set
[01:01:35.200 --> 01:01:42.560]   up different CPUs, GPUs, so I suggest that this makes life much easier for machine learning
[01:01:42.560 --> 01:01:50.080]   practitioners. Okay, yeah, that makes sense. So the idea is to use Jack's, let's call it,
[01:01:50.080 --> 01:01:57.040]   native distribution, which is not... Distribution in different sense, like you're relying on
[01:01:57.040 --> 01:02:05.280]   TPUs, for example, which are kind of like, I don't know, in the biggest case, I guess you could get
[01:02:05.280 --> 01:02:16.720]   a pod which has, what, like more than 128 devices? And then you're going to distribute it in this,
[01:02:17.920 --> 01:02:28.320]   how do we call it? It's kind of a cluster, but it's not distributed in the sense that you have
[01:02:28.320 --> 01:02:34.960]   to do the I/O yourself or something like that, but Jack's just does it for you natively.
[01:02:34.960 --> 01:02:41.920]   Right, right. Nowadays, having access to multiple GPUs is kind of common in most of the labs,
[01:02:41.920 --> 01:02:47.520]   because this popularity of deep learning, people are already using multiple GPUs to train a single
[01:02:47.520 --> 01:02:52.320]   task. So I would say this kind of setting is common for machine learning practitioners,
[01:02:52.320 --> 01:02:56.080]   whereas a cluster of CPUs is probably not so common, right?
[01:02:56.080 --> 01:03:09.600]   Yeah, a related question, have you checked maybe Jack's, like, I think Jack's is trying to run also
[01:03:09.600 --> 01:03:17.760]   multi-host, this kind of something I've seen around in the Jack's forum, and there's also this
[01:03:17.760 --> 01:03:30.720]   open, how is it called, MPI for Jack's? Have you checked all these other ways of trying to create
[01:03:30.720 --> 01:03:38.240]   like Jack's clusters? No, I haven't. But with that being said, I think that's a plus, right? So what
[01:03:38.240 --> 01:03:44.400]   they are trying to do is scaling up, which is a plus for Apple Jack's, right? With data, so Apple
[01:03:44.400 --> 01:03:49.440]   Jack's could further scale up, run on multiple hosts, right? And I hope that, as you said,
[01:03:49.440 --> 01:03:54.640]   this kind of complexity could be transparent to the end users of such data. We still run the same
[01:03:54.640 --> 01:03:59.360]   code, but the world will suddenly get better once we have more machines, right? Yeah, it'll be really
[01:03:59.360 --> 01:04:06.880]   cool, because then, I don't know, if somebody, I don't know, your devop friend, sponsor a
[01:04:06.880 --> 01:04:17.120]   Kubernetes cluster with a thousand hosts, then you can maybe try to use this multi-host, and then,
[01:04:17.120 --> 01:04:24.000]   well, same code, right? That would be really, really cool. I don't know how evolved that is,
[01:04:24.000 --> 01:04:34.960]   honestly, but it's something very interesting. I don't know if you've tried to go that way with
[01:04:34.960 --> 01:04:43.200]   Evo Jacks and see if it works. I haven't done so, but as I said, so the major motivation,
[01:04:43.200 --> 01:04:49.520]   one of the major motivations is to make things fast, right? So as I said, for previous paper
[01:04:49.520 --> 01:04:55.120]   publications or the experiments I ran, I exclusively used Kubernetes, and we released the code,
[01:04:55.120 --> 01:05:00.320]   but we've got feedback from end users that say this is kind of a slope, because most of the
[01:05:00.320 --> 01:05:08.880]   users don't have access to clusters. They have to rely on a moderately sized server, maybe 48 or say
[01:05:08.880 --> 01:05:16.560]   96 CPUs, although you have a lot of CPUs, but they're still running slow. So that's what we
[01:05:16.560 --> 01:05:23.440]   hope, that Evo Jacks could help them get faster IDE iterations and to replicate the results really
[01:05:23.440 --> 01:05:30.640]   fast. That's kind of what we want to do. - Okay, awesome. As a user, I saw there's this
[01:05:30.640 --> 01:05:39.840]   train trainer object. As a user, you can tell, I don't know what's the interface, because Jacks
[01:05:39.840 --> 01:05:46.880]   has a lot of ways of running, like you have JIT, you have PMAP, you have PJIT, you have XMAP.
[01:05:48.400 --> 01:05:56.480]   Can you tell Evo Jacks to run the computation in a certain way that maybe the user knows
[01:05:56.480 --> 01:06:03.360]   exploits better the hardware? - Yeah, so that's kind of a question about the design philosophy,
[01:06:03.360 --> 01:06:10.160]   right? So right now, our design philosophy is to hide this complexity from the users. So
[01:06:10.160 --> 01:06:16.960]   what users want to care will be interested, probably, is just the algorithm, the policy,
[01:06:16.960 --> 01:06:24.000]   and the task itself, the trainer itself, like how each of the operations to put on each device,
[01:06:24.000 --> 01:06:28.240]   you don't have to care about it, you just write the policy, then everything will run for you.
[01:06:28.240 --> 01:06:34.080]   So that's what we are targeting for. But what you said is probably for more advanced users,
[01:06:34.080 --> 01:06:40.240]   where he or she wants a higher level control, more detailed control, right? But that's doable,
[01:06:40.240 --> 01:06:46.480]   right? As I described, Evo Jacks is designed to be modular, modularized, so you can use any
[01:06:46.480 --> 01:06:52.560]   component as a standalone component, then write the rest of the task, for example, to distribute
[01:06:52.560 --> 01:06:58.160]   the task to different devices by your own, right? So nothing's stopping you from doing that, so
[01:06:58.160 --> 01:07:04.240]   that's also doable. But right now, in Evo Jacks, we probably are going to hide this complexity
[01:07:04.240 --> 01:07:13.280]   from the end users. I see. So I'm guessing, okay, you control the loop, you control the training
[01:07:13.280 --> 01:07:20.240]   loop, and then you say, okay, if it has only one device, then JIT, if it has more than one device,
[01:07:20.240 --> 01:07:27.760]   then you switch to PMAP and that kind of stuff? Right, right. And if, well, I don't know.
[01:07:28.640 --> 01:07:36.240]   I don't know if you've thought about this, but for example, I'm guessing, okay, let's say
[01:07:36.240 --> 01:07:49.440]   eventually P-JIT is optimal. Is there kind of a, I don't know if you're thinking about this,
[01:07:49.440 --> 01:07:58.080]   a way for the user to somehow specify this? But I think it's very tricky, but I don't know if
[01:07:58.080 --> 01:08:05.360]   you've thought about how the user can try to specify this kind of... Yeah, I see, I see. Yeah,
[01:08:05.360 --> 01:08:10.720]   yeah, that's a good question. So as I said, we have just released Evo Jacks for maybe a little
[01:08:10.720 --> 01:08:18.720]   bit around one month, I think. We are getting user feedbacks. So right now, we haven't seen such
[01:08:18.720 --> 01:08:24.160]   like requests now, but sure, in the future, if we see such requests, we'll consider adding features,
[01:08:24.160 --> 01:08:28.880]   like giving users detailed controls, as you said, and maybe adding more advanced
[01:08:28.880 --> 01:08:35.760]   Jacks functionalities to allow further control of different details for the end users.
[01:08:35.760 --> 01:08:44.320]   Okay. So right now, it's okay. It's really nice. It's kind of a plug and play.
[01:08:44.320 --> 01:08:50.800]   Yeah, that's what we are designing. That's kind of a design philosophy right now.
[01:08:52.160 --> 01:09:01.680]   Okay, that's really cool. I have a question regarding some of the demos.
[01:09:01.680 --> 01:09:12.400]   So the environments you provide, they have this render function, kind of like Jim?
[01:09:13.200 --> 01:09:23.440]   Yeah, let me just give you a quick example. So for example, the card poll, for example.
[01:09:23.440 --> 01:09:30.800]   So for each render, although this is not required, but the user can implement a render function,
[01:09:30.800 --> 01:09:36.320]   right? So for this render function, it's going to get the state. State is the kind of observation
[01:09:36.320 --> 01:09:41.920]   from this vectorized environment, and the user can specify a certain index of the task. Say,
[01:09:41.920 --> 01:09:47.120]   I want to see this environment, for example. Then this function will just plot the state
[01:09:47.120 --> 01:09:53.440]   for this task instead of the entire stack, right? So the user is responsible for implementing this,
[01:09:53.440 --> 01:09:57.760]   apparently, because we don't know what the task is going to be. So there is no way for us to
[01:09:57.760 --> 01:10:04.160]   implement this for the users. Right now, we are just using this pillow, just the image
[01:10:04.160 --> 01:10:10.640]   drawers. So we could plot whatever we want in this environment, and the users may have
[01:10:10.640 --> 01:10:17.040]   different needs. So it's possible to also do some crazy stuff in this render function.
[01:10:17.040 --> 01:10:25.440]   Okay, that's really cool. But this runs on the host, right? Because Peel is not in Jack.
[01:10:25.440 --> 01:10:33.680]   Yeah, it runs on the CPU. So the render function is mainly for a visual confirmation of the learned
[01:10:33.680 --> 01:10:38.640]   policy. So although you see the scores are rising, but you don't know whether the policy
[01:10:38.640 --> 01:10:43.920]   is really learning stuff or has it discovered those kind of cheating stuff, right? Cheating
[01:10:43.920 --> 01:10:57.200]   maneuvers. Okay, that's really cool. So this is more like a user question. If people say,
[01:10:57.200 --> 01:11:06.160]   want to try to integrate something that is not yet in Jack's OpenAI Gym, I don't know, the
[01:11:07.600 --> 01:11:11.520]   Lunar Lander. Is it currently possible to do this kind of stuff?
[01:11:11.520 --> 01:11:20.080]   In the current release, it's not possible. So as I mentioned earlier, the current training loop,
[01:11:20.080 --> 01:11:26.640]   we use the scan function from Jack. In the next release, actually, we are developing it right now.
[01:11:26.640 --> 01:11:34.160]   We are using the traditional for loop in that way, even not JIT-function. I mean, functions
[01:11:34.160 --> 01:11:40.240]   that are not JIT-compiled could also be integrated into the for loop. In that release, it will be
[01:11:40.240 --> 01:11:46.240]   possible to run conventional tasks. But again, there will probably be a trade-off in terms of
[01:11:46.240 --> 01:11:55.600]   performance speed. Yeah, and also, I guess there would be a limitation because then
[01:11:57.840 --> 01:12:02.800]   you can only run the environment in a single device. Is this correct?
[01:12:02.800 --> 01:12:15.600]   That's not true. So say, for example, you could use the vectorized environment wrapper from the
[01:12:15.600 --> 01:12:20.800]   baselines, I think, from the baselines toolkit. And then you simply wrap, for example, the
[01:12:20.800 --> 01:12:26.400]   OpenAI Gym environments. Then this environment, because it's a wrapper, right? It will provide
[01:12:27.120 --> 01:12:34.400]   the vectorized observations. Then after that, you simply convert it into a Jaxx tensor. Then
[01:12:34.400 --> 01:12:38.480]   it can plug into Apple Jaxx. So that's how it will work.
[01:12:38.480 --> 01:12:51.920]   From the implementation side, because Jaxx is very restrictive in a lot of ways, how do you
[01:12:54.240 --> 01:13:02.160]   tell Jaxx to, "Hey, this is not Jaxx, but put it in a different..."
[01:13:02.160 --> 01:13:11.200]   You don't have to do that. Say, you have this vectorized environment wrapper, right?
[01:13:11.200 --> 01:13:18.080]   So you wrap this wrapper around a bunch of tasks, OpenAI Gym tasks, for example. Then
[01:13:18.080 --> 01:13:25.440]   the observations will be a tensor with batch sizes. So probably it's in NumPy,
[01:13:25.440 --> 01:13:31.600]   NumPy for format. It's a NumPy tensor. Then you simply convert it into Jaxx tensor.
[01:13:31.600 --> 01:13:35.280]   So that's just one function, one call, right?
[01:13:35.280 --> 01:13:36.480]   Okay, I see.
[01:13:36.480 --> 01:13:39.520]   From Jaxx tensor to Apple Jaxx, that's all, right?
[01:13:39.520 --> 01:13:44.960]   Okay, so it's kind of vectorized, but it runs on the host.
[01:13:44.960 --> 01:13:46.720]   Yeah, it runs on the CPUs, I would say.
[01:13:47.360 --> 01:13:49.680]   Ah, okay, okay. I don't have...
[01:13:49.680 --> 01:13:58.240]   Well, it depends how you're going to run it. But have you checked these functions from Jaxx called
[01:13:58.240 --> 01:14:06.720]   I don't know, IDTAP or... What's the package?
[01:14:06.720 --> 01:14:14.400]   It's... I forgot the package name. It's in experimental and it lets you...
[01:14:14.880 --> 01:14:20.160]   The host callbacks. Have you checked that functionality?
[01:14:20.160 --> 01:14:21.840]   No, I haven't.
[01:14:21.840 --> 01:14:31.040]   Okay, I've only used it for debugging, but it lets you run Python code
[01:14:31.040 --> 01:14:42.160]   well, on the devices. I guess it's what it's trying to do.
[01:14:42.800 --> 01:14:49.120]   I only use it for printing. But I don't know if you could try to
[01:14:49.120 --> 01:14:58.240]   run the environment on each device also. I don't know how that... I think
[01:14:58.240 --> 01:15:03.760]   that doesn't make sense for GPUs, but maybe for TPUs.
[01:15:03.760 --> 01:15:08.560]   Yeah, I see what you mean. So yeah, it's possible to insert a callback into
[01:15:09.280 --> 01:15:14.640]   this JTED function such that this part will be treated especially.
[01:15:14.640 --> 01:15:20.000]   What I know is that yes, you can insert your printing or other stuff in that,
[01:15:20.000 --> 01:15:26.640]   but if you want to convert a Jaxx tensor, say you've got three segments of code,
[01:15:26.640 --> 01:15:31.440]   so the front one will just run some computation, then it's your host callback,
[01:15:31.440 --> 01:15:36.640]   then you get back to the Jaxx domain. So if you generate some tensors in segment number one,
[01:15:36.640 --> 01:15:43.760]   and you want to use the callback to convert this Jaxx tensor into something else,
[01:15:43.760 --> 01:15:50.880]   then probably that's not doable, if I'm correct. So there's going to be some data transfer
[01:15:50.880 --> 01:15:57.040]   limitations. So you can do something, if that's not related to any tensors from segment one,
[01:15:57.040 --> 01:16:02.240]   it's okay. But if that's the case, then probably you are going to run into some difficulties there.
[01:16:03.200 --> 01:16:11.680]   Yeah, it's not fast. I don't know, I'm not sure about this. I would love to ask
[01:16:11.680 --> 01:16:24.880]   the people who design XLA or Jaxx itself, if this host callback, because I understand that TPUv3,
[01:16:27.280 --> 01:16:31.760]   each device actually has a host. That's what I understood from the
[01:16:31.760 --> 01:16:39.600]   V3, TPUv3 architecture. So it made me think, okay, if each device also has a host,
[01:16:39.600 --> 01:16:51.440]   like a VM, then can it also run some Python code there, was my question. And then, okay,
[01:16:53.120 --> 01:16:59.280]   if each device can run some Python code, then it would be very interesting if the device,
[01:16:59.280 --> 01:17:05.360]   the host callback could actually, for example, in this case, run the environment.
[01:17:05.360 --> 01:17:15.520]   Yeah, that would be really cool. I'm not familiar with TPUs, all the usages of TPUs,
[01:17:15.520 --> 01:17:22.880]   and also Jaxx team is really amazing. They're working on these new features almost every day.
[01:17:22.880 --> 01:17:26.240]   So what I said could be wrong, previously said, right? So this host callback stuff.
[01:17:26.240 --> 01:17:31.600]   So I'm sure that one day they are accepting user requests, like they are trying to fulfill user
[01:17:31.600 --> 01:17:36.640]   requests constantly. So I'm sure one day what we want to do could be implemented in Jaxx.
[01:17:36.640 --> 01:17:47.040]   Okay, yeah, very cool. I'm saying this because I once got in a discussion on the Jaxx forum of
[01:17:47.920 --> 01:17:53.360]   how to try to load data into each device.
[01:17:53.360 --> 01:18:06.160]   Like imagine like a PyTorch data loader, but distributed. And then people were giving ideas,
[01:18:06.160 --> 01:18:14.160]   and one idea was, hey, why don't you use this device callback? And the thing, I think it's
[01:18:14.160 --> 01:18:21.440]   very similar to what you're saying. Like the question was, okay, I want to scan over the
[01:18:21.440 --> 01:18:30.400]   training, but I cannot scan the data generating process, which in this case is kind of the
[01:18:30.400 --> 01:18:37.520]   environment. And then, I don't know, like somebody from the Jaxx team suggested, hey, use this.
[01:18:38.320 --> 01:18:43.280]   And I tried it, it actually works. But it was very slow.
[01:18:43.280 --> 01:18:53.520]   Yeah, but as I said, Jaxx is advancing every day, right? So maybe later they will release a
[01:18:53.520 --> 01:18:58.560]   feature that will allow this to happen and run fast, right? So right now, like for Apple Jaxx,
[01:18:58.560 --> 01:19:05.600]   our short-term solution is to support both the scan and the traditional for loop. For Jaxx-based
[01:19:05.600 --> 01:19:10.880]   tasks, the users can rely on the scan function, whereas for conventional tasks, the user could
[01:19:10.880 --> 01:19:18.400]   choose to fall back to the traditional for loop. Besides some performance fallbacks in terms of
[01:19:18.400 --> 01:19:25.440]   speed, it should be trainable. Like as I mentioned previously, we are testing, say,
[01:19:25.440 --> 01:19:32.000]   Isaac's Gym as well. Isaac's Gym is accelerated by GPU, but also it's not Jaxx. But with for loop,
[01:19:32.000 --> 01:19:36.960]   it's also possible to train Isaac's environment or say tasks in Isaac's Gym.
[01:19:36.960 --> 01:19:44.480]   Okay, yeah, that makes a lot of sense. I think the for loop is not that slow.
[01:19:44.480 --> 01:19:54.160]   If each step is sufficiently, I guess, big or...
[01:19:55.280 --> 01:20:02.960]   Yeah, it's not that slow. I think it depends on how you write the training loop inside that
[01:20:02.960 --> 01:20:09.280]   for loop, right? So if you properly jit everything and make sure that everything is on GPU,
[01:20:09.280 --> 01:20:18.320]   the performance difference is not that obvious. Yeah, a scan is ideal, right? It's kind of the...
[01:20:19.360 --> 01:20:23.520]   Yeah, everything is Jaxx. Yeah, it's a unit operation.
[01:20:23.520 --> 01:20:30.240]   But yeah, what I kind of got the sense is okay, if your step is sufficiently big,
[01:20:30.240 --> 01:20:38.800]   like it's okay to use like the for loop. And
[01:20:38.800 --> 01:20:48.160]   yes, especially if you want to interact with something because the host callback again was
[01:20:48.160 --> 01:20:57.680]   not that fast. I had like a... Okay, no, I had a question, but I forgot. Sorry.
[01:20:57.680 --> 01:21:08.240]   No problem. So as I said, we encourage users to extend Apple Jaxx and run our experiments.
[01:21:08.240 --> 01:21:13.120]   And if you have any questions, you can just send emails to us. So we have put our email address in
[01:21:13.120 --> 01:21:19.600]   the GitHub page. So feel free to ping us or say send us open an issue there or send us emails.
[01:21:19.600 --> 01:21:25.680]   We welcome all the user feedbacks and we are open to any user suggestions. Like what Christian
[01:21:25.680 --> 01:21:30.800]   mentioned, we will consider release high level control, I mean, detail level control, if that's
[01:21:30.800 --> 01:21:38.480]   what the majority of the user requires. Awesome. I'll wrap up in that case. And
[01:21:38.480 --> 01:21:43.440]   sorry, we went over time. Thanks. Thanks for staying on Eugene. And thank you, Christian,
[01:21:43.440 --> 01:21:49.440]   for the great questions. I'll thank the audience as well for joining. And again, I'll update if
[01:21:49.440 --> 01:21:53.840]   you're watching the recording, there'll be the link to EvoJaxx repository in the description.
[01:21:53.840 --> 01:21:59.120]   Click on that and reach out to the authors and creators if you have any questions. I'm sure
[01:21:59.120 --> 01:22:03.520]   they're super active and will be happy to help you. So thank you so much again, Eugene and Christian.
[01:22:03.520 --> 01:22:08.400]   Awesome. Thank you. Thanks. Yeah. Okay.

