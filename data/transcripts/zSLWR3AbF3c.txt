
[00:00:00.000 --> 00:00:04.720]   Today, I have the pleasure of speaking with Tony Blair, who was, of course, Prime Minister
[00:00:04.720 --> 00:00:11.420]   of the UK from 1997 to 2007, and now leads the Tony Blair Institute, which advises dozens
[00:00:11.420 --> 00:00:16.260]   of governments on improving governance, reform, adding technology.
[00:00:16.260 --> 00:00:20.180]   My first question, I want to go back to your time in office.
[00:00:20.180 --> 00:00:23.180]   And when you first got in, you had these large majorities.
[00:00:23.180 --> 00:00:27.620]   What are the constraints on a Prime Minister, despite the fact that they have these large
[00:00:27.620 --> 00:00:28.620]   majorities?
[00:00:28.620 --> 00:00:31.180]   Are the other members of your party fighting against you?
[00:00:31.180 --> 00:00:32.180]   Is it the deep state?
[00:00:32.180 --> 00:00:34.900]   Like, what part is constraining you at that point?
[00:00:34.900 --> 00:00:40.460]   The biggest constraint is that politics, and in particular, political leadership, is probably
[00:00:40.460 --> 00:00:48.940]   the only walk of life in which someone is put into an immensely powerful and important
[00:00:48.940 --> 00:00:53.060]   position with absolutely zero qualifications or experience.
[00:00:53.060 --> 00:00:56.420]   I mean, I never had a ministerial appointment before.
[00:00:56.420 --> 00:01:00.700]   My one and only was being Prime Minister, which, you know, is great if you want to start
[00:01:00.700 --> 00:01:01.700]   at the top.
[00:01:01.700 --> 00:01:04.060]   But it's that that's most difficult.
[00:01:04.060 --> 00:01:09.580]   So you come in, and you often come in as - when you're running for office, you have to be
[00:01:09.580 --> 00:01:11.080]   the great persuader.
[00:01:11.080 --> 00:01:14.700]   The moment you get into office, you really have to be the great chief executive.
[00:01:14.700 --> 00:01:17.180]   And those two skill sets are completely different.
[00:01:17.180 --> 00:01:22.100]   And a lot of political leaders fail because they've failed to make the transition.
[00:01:22.100 --> 00:01:30.940]   And those executive skills, which are about focus, prioritisation, good policy, building
[00:01:30.940 --> 00:01:37.820]   the right team of people who can actually help you govern - because the moment you become
[00:01:37.820 --> 00:01:44.060]   the government, you end up leaving aside the saying that becomes less important than the
[00:01:44.060 --> 00:01:45.060]   doing.
[00:01:45.060 --> 00:01:48.180]   Whereas when you're in opposition and you're running for office, it's all about saying.
[00:01:48.180 --> 00:01:57.020]   So all of these things mean that it's a much more difficult, much more focused, and, you
[00:01:57.020 --> 00:02:01.740]   know, it's suddenly you're thrust into this completely new environment when you come in.
[00:02:01.740 --> 00:02:04.420]   And that's what makes it - that's the hardest thing.
[00:02:04.420 --> 00:02:11.220]   And then of course, you know, you do have a situation in which the system, as a system,
[00:02:11.220 --> 00:02:16.260]   it's not that it's a - I'm not a believer that this is great deep state theory, we can
[00:02:16.260 --> 00:02:17.260]   talk about that.
[00:02:17.260 --> 00:02:18.940]   But that's not the problem with government.
[00:02:18.940 --> 00:02:23.860]   The problem with government is that it's not a conspiracy, either left wing or right wing.
[00:02:23.860 --> 00:02:26.020]   It's a conspiracy for inertia.
[00:02:26.020 --> 00:02:31.260]   The thing about government systems is that they always think we're permanent, you've
[00:02:31.260 --> 00:02:37.100]   come in as the elected politician, you're temporary, and, you know, we know how to do
[00:02:37.100 --> 00:02:38.100]   this.
[00:02:38.100 --> 00:02:42.620]   And if you only just let us alone, we would carry on managing the status quo in the right
[00:02:42.620 --> 00:02:44.020]   way.
[00:02:44.020 --> 00:02:46.260]   And so that's the toughest thing.
[00:02:46.260 --> 00:02:47.260]   It's making that transition.
[00:02:47.260 --> 00:02:49.300]   Okay, so that's really interesting.
[00:02:49.300 --> 00:02:55.940]   Now, if we take you back everything you knew, let's say in 2007, but you have the majorities
[00:02:55.940 --> 00:03:00.180]   and maybe the popularity you had in 1997, what fundamentally - you said you have these
[00:03:00.180 --> 00:03:04.420]   executive skills now - what fundamentally, is it that you know what a time waste - what
[00:03:04.420 --> 00:03:05.700]   kinds of things are time wasters?
[00:03:05.700 --> 00:03:08.500]   So you say, I'm not going to do these PMQs, they're total theatrics.
[00:03:08.500 --> 00:03:11.700]   I'm not going to, you know, I'm not going to meet the Queen or something.
[00:03:11.700 --> 00:03:12.700]   Is it the time?
[00:03:12.700 --> 00:03:15.980]   Is it that you're going to be going against the bureaucracy and say, I think you're wrong
[00:03:15.980 --> 00:03:17.300]   about your inertia?
[00:03:17.300 --> 00:03:18.300]   What fundamentally changes?
[00:03:18.300 --> 00:03:21.060]   Well, it wouldn't be that you wouldn't do Prime Minister's questions because Parliament
[00:03:21.060 --> 00:03:22.540]   will insist on that.
[00:03:22.540 --> 00:03:26.700]   And you certainly wouldn't want to offend, well, it was the Queen in my time.
[00:03:26.700 --> 00:03:28.380]   No, but you're right.
[00:03:28.380 --> 00:03:33.220]   What you would do is have a much clearer idea of how to give direction to the bureaucracy
[00:03:33.220 --> 00:03:38.500]   and how to bring in outside skilled people who can help you deliver change.
[00:03:38.500 --> 00:03:44.340]   And so, you know, I always split my premiership into the first five years, which in some ways
[00:03:44.340 --> 00:03:46.740]   were the easiest.
[00:03:46.740 --> 00:03:49.980]   We were doing things that were important, like a minimum wage.
[00:03:49.980 --> 00:03:51.140]   We did big devolution.
[00:03:51.140 --> 00:03:55.740]   We did the Good Friday peace agreement in Northern Ireland, but it was only really in
[00:03:55.740 --> 00:04:00.980]   the second half of my premiership that we started to reform healthcare, education, criminal
[00:04:00.980 --> 00:04:02.100]   justice.
[00:04:02.100 --> 00:04:09.320]   And those systemic reforms require - that's when your skill set as a chief executive really
[00:04:09.320 --> 00:04:10.860]   comes into play.
[00:04:10.860 --> 00:04:15.180]   There's a perception that many people have that if you could get a successful CEO or
[00:04:15.180 --> 00:04:20.300]   business person into office, these executive skills would actually transfer over pretty
[00:04:20.300 --> 00:04:22.860]   well into becoming a head of state.
[00:04:22.860 --> 00:04:23.860]   Is that true?
[00:04:23.860 --> 00:04:27.100]   And if not, what is it that they'd be lacking, that you need to be an electable leader?
[00:04:27.100 --> 00:04:29.580]   Yeah, so this is really interesting, and I think a lot about this.
[00:04:29.580 --> 00:04:36.140]   The truth is those skills would transfer to being a political leader, but they're not
[00:04:36.140 --> 00:04:40.060]   the only skills you need because you've still got to be a political leader.
[00:04:40.060 --> 00:04:42.620]   And therefore, you've got to know how to manage your party.
[00:04:42.620 --> 00:04:45.860]   You've got to know how you frame certain things.
[00:04:45.860 --> 00:04:51.020]   You've got to know, frankly, as a CEO of a company, you're the person in charge, right?
[00:04:51.020 --> 00:04:53.540]   You can more or less lay down the law, right?
[00:04:53.540 --> 00:04:55.420]   Politics is more complicated than that.
[00:04:55.420 --> 00:05:03.900]   So when you get highly skilled CEOs come into politics, oftentimes they don't succeed.
[00:05:03.900 --> 00:05:07.840]   But that's not because their executive skill set is the problem, it's because they haven't
[00:05:07.840 --> 00:05:10.420]   developed a political skill set.
[00:05:10.420 --> 00:05:12.100]   Interesting.
[00:05:12.100 --> 00:05:15.540]   I was reading your memoir, and one thing I thought was interesting is there were a couple
[00:05:15.540 --> 00:05:20.340]   of times you said that you realized later on that you had more leverage or you were
[00:05:20.340 --> 00:05:24.100]   able to do things in retrospect that you didn't do at the time.
[00:05:24.100 --> 00:05:27.100]   And is that one of the things that would change?
[00:05:27.100 --> 00:05:31.740]   And if you go back to 1997, you realize I actually can fire this entire team if I don't
[00:05:31.740 --> 00:05:32.740]   think they're doing a good job.
[00:05:32.740 --> 00:05:35.380]   I actually can cancel my meetings with ambassadors.
[00:05:35.380 --> 00:05:38.480]   How much of that - did you have more leverage than you realized at the time?
[00:05:38.480 --> 00:05:42.760]   Well, in terms of running the system, yes.
[00:05:42.760 --> 00:05:47.200]   Definitely with the benefit of experience, I would have given much clearer directions.
[00:05:47.200 --> 00:05:49.520]   I would have moved people much faster.
[00:05:49.520 --> 00:05:55.880]   I would have probably done - because in politics, again, this is where it's different from running
[00:05:55.880 --> 00:05:58.200]   a company.
[00:05:58.200 --> 00:06:04.200]   In a company, by and large, you can put the people in the places you want them, right?
[00:06:04.200 --> 00:06:09.340]   And except in exceptional circumstances - I think this is true - except in exceptional
[00:06:09.340 --> 00:06:14.680]   circumstances, if you're running a company, you've got no one in the senior management
[00:06:14.680 --> 00:06:18.740]   who you don't want to be in the senior management, right?
[00:06:18.740 --> 00:06:23.500]   Politics isn't like that because you've got elements, political elements you may have
[00:06:23.500 --> 00:06:24.500]   to pacify.
[00:06:24.500 --> 00:06:30.140]   There may be people that you don't particularly want because they're not particularly good
[00:06:30.140 --> 00:06:34.680]   at being ministers, but they may be very good at managing your party or your government
[00:06:34.680 --> 00:06:37.380]   in order to get things through.
[00:06:37.380 --> 00:06:42.120]   What I learned over time is the important thing is to put in the core positions that
[00:06:42.120 --> 00:06:43.700]   really matter to you.
[00:06:43.700 --> 00:06:48.380]   You can't - don't fall short on quality.
[00:06:48.380 --> 00:06:52.620]   It's one of those really interesting things, because I think being a political leader is
[00:06:52.620 --> 00:06:59.840]   the same as leading a company or a community center or a football team.
[00:06:59.840 --> 00:07:04.420]   It all comes to the same thing, but you realize it's such an obvious thing to say that it's
[00:07:04.420 --> 00:07:08.220]   all about the people, but it's all about the people.
[00:07:08.220 --> 00:07:14.140]   If you get really good, strong, determined people who are prepared to share your vision
[00:07:14.140 --> 00:07:19.180]   and are prepared to get behind and really push, then you don't need that many of them
[00:07:19.180 --> 00:07:22.260]   actually to change a country, but they do need to be there.
[00:07:22.260 --> 00:07:23.700]   That's really interesting.
[00:07:23.700 --> 00:07:29.340]   So when I think about - this is not particularly even to the UK - but even in Western governments,
[00:07:29.340 --> 00:07:34.020]   people are often frustrated that they elect somebody they think is a changemaker.
[00:07:34.020 --> 00:07:36.540]   Things don't necessarily change that much.
[00:07:36.540 --> 00:07:38.340]   The system feels an inertia.
[00:07:38.340 --> 00:07:41.140]   If this is the case, is it because they didn't have the right team around them?
[00:07:41.140 --> 00:07:45.060]   Because if you think of like, I don't know, Obama or Trump or Biden, at the very top,
[00:07:45.060 --> 00:07:49.180]   I assume they can recruit top people to the extent that they weren't able to exact the
[00:07:49.180 --> 00:07:50.180]   change they wanted.
[00:07:50.180 --> 00:07:54.140]   Is it because - well, it must not have been because they didn't get the right chief of
[00:07:54.140 --> 00:07:55.140]   staff, right?
[00:07:55.140 --> 00:07:56.540]   They can probably get the right chief of staff.
[00:07:56.540 --> 00:07:57.740]   Yeah, absolutely.
[00:07:57.740 --> 00:07:59.620]   And they can get really good people.
[00:07:59.620 --> 00:08:04.440]   One of the things you actually learn about being at the top of a government is pretty
[00:08:04.440 --> 00:08:08.620]   much if you pick up the phone to someone and say, "I need you to come and to help," that
[00:08:08.620 --> 00:08:09.620]   they will come.
[00:08:09.620 --> 00:08:12.740]   I think the problem's not that.
[00:08:12.740 --> 00:08:20.820]   The problem is that, number one - and I say this often to the leaders that I work with,
[00:08:20.820 --> 00:08:26.340]   because we work in roughly 40 different countries in the world today, and that's only growing.
[00:08:26.340 --> 00:08:30.700]   We have teams of people that go and live and work alongside the president's team, and I
[00:08:30.700 --> 00:08:35.100]   kind of talk and exchange views with the president or the prime minister.
[00:08:35.100 --> 00:08:38.020]   And very often, the two problems are these.
[00:08:38.020 --> 00:08:43.660]   Number one, people confuse ambitions with policies, right?
[00:08:43.660 --> 00:08:48.900]   So often I will speak to a leader and I'll say, "So what are your policies?"
[00:08:48.900 --> 00:08:54.300]   And he'll give me a list of things, and actually I say to them, "Those aren't really policies,
[00:08:54.300 --> 00:08:55.580]   they're just ambitions."
[00:08:55.580 --> 00:09:00.180]   And ambitions in politics are very easy to have, because they're just general expressions
[00:09:00.180 --> 00:09:02.100]   of good intention.
[00:09:02.100 --> 00:09:10.140]   The problem comes with the second challenge, which is that though politics at one level
[00:09:10.140 --> 00:09:12.660]   is very crude, right?
[00:09:12.660 --> 00:09:17.020]   You're shaking hands, kissing babies, making speeches, devising slogans, attacking your
[00:09:17.020 --> 00:09:18.020]   opponents.
[00:09:18.020 --> 00:09:20.580]   That's quite a crude business.
[00:09:20.580 --> 00:09:25.400]   When it comes to policy, it's a really intellectual business, politics.
[00:09:25.400 --> 00:09:33.020]   And that's why people often, if they've only got ambitions, then they haven't really undertaken
[00:09:33.020 --> 00:09:37.460]   the intellectual exercise to turn those into policies.
[00:09:37.460 --> 00:09:38.940]   And policies are hard.
[00:09:38.940 --> 00:09:40.820]   It's hard to work out what the right policy is.
[00:09:40.820 --> 00:09:47.380]   If you take this AI revolution, and I think we're living through a period of massive change.
[00:09:47.380 --> 00:09:53.420]   This is the biggest technological change since the Industrial Revolution, for sure.
[00:09:53.420 --> 00:09:57.860]   For political leaders today to understand that, to work out what the right policy is,
[00:09:57.860 --> 00:10:03.980]   to access the opportunities, mitigate the risks, regulate it, this is really difficult
[00:10:03.980 --> 00:10:04.980]   work.
[00:10:04.980 --> 00:10:08.660]   And so what happens a lot of the time is that people are elected on the basis they are change
[00:10:08.660 --> 00:10:13.300]   makers because they've articulated a general vision for change.
[00:10:13.300 --> 00:10:18.580]   But when you then come to, "Okay, what does that really mean in specific terms?"
[00:10:18.580 --> 00:10:21.700]   That's where the hard work hasn't been done.
[00:10:21.700 --> 00:10:27.460]   And if you don't do that hard work and really dig deep, then what you end up with, as I
[00:10:27.460 --> 00:10:30.140]   say, are just their ambitions, and they just remain ambitions.
[00:10:30.140 --> 00:10:34.060]   >> Okay, so now that you've brought up AI, I want to ask about this.
[00:10:34.060 --> 00:10:37.140]   I do a lot of episodes on AI.
[00:10:37.140 --> 00:10:41.740]   And to the people who are in the industry, it seems plausible, though potentially unlikely,
[00:10:41.740 --> 00:10:47.900]   that in the next few years you could have a huge sort of July 1914 type moment.
[00:10:47.900 --> 00:10:50.000]   But for AI, there's a big crisis.
[00:10:50.000 --> 00:10:54.320]   Something major has happened in terms of misuse or a warning shot.
[00:10:54.320 --> 00:10:59.280]   Today's governments, given how they function, either in the West or how you see them function
[00:10:59.280 --> 00:11:03.240]   with the other leaders you advise, how well would they deal with this?
[00:11:03.240 --> 00:11:07.680]   So they get this news about some AI that's escaped or some bioweapon that's been made
[00:11:07.680 --> 00:11:08.680]   because of AI.
[00:11:08.680 --> 00:11:12.800]   Would it immediately kick off a sort of a race dynamic from the West to China?
[00:11:12.800 --> 00:11:15.040]   Would they, do they have the technical competence to deal with this?
[00:11:15.040 --> 00:11:16.040]   How would that shape out?
[00:11:16.040 --> 00:11:18.400]   >> Right now, definitely not.
[00:11:18.400 --> 00:11:23.600]   And one of the things that we do as an institute, one of the reasons I'm here actually in Silicon
[00:11:23.600 --> 00:11:28.880]   Valley is to try and bridge the gap between what I call the change makers and the policy
[00:11:28.880 --> 00:11:30.220]   makers.
[00:11:30.220 --> 00:11:34.760]   Because the policy makers a lot of the time just fear the change makers.
[00:11:34.760 --> 00:11:37.880]   And the change makers a lot of the time don't want really anything to do with the policy
[00:11:37.880 --> 00:11:40.520]   makers because they just think they get in the way, okay?
[00:11:40.520 --> 00:11:42.720]   So you don't have a dialogue.
[00:11:42.720 --> 00:11:49.160]   But if what you're describing, Duakesh, were to happen, and by the way, I think it's possible
[00:11:49.160 --> 00:11:51.320]   at some point it does happen.
[00:11:51.320 --> 00:11:57.880]   If it happened right now, I think political leaders wouldn't have the, they wouldn't know
[00:11:57.880 --> 00:12:01.000]   where to begin in solving that problem or what it might mean.
[00:12:01.000 --> 00:12:05.960]   So I think this is why I keep saying to the political leaders I'm talking to today, and
[00:12:05.960 --> 00:12:11.240]   we're likely to have a change of government in the UK this year.
[00:12:11.240 --> 00:12:15.200]   And I am constantly saying to my own party, the Labour Party, which will probably win
[00:12:15.200 --> 00:12:20.120]   this election, you've got to focus on this technology revolution.
[00:12:20.120 --> 00:12:22.560]   It's not an afterthought.
[00:12:22.560 --> 00:12:28.040]   It's the single biggest thing that's happening in the world today of a real world nature
[00:12:28.040 --> 00:12:29.720]   that is going to change everything.
[00:12:29.720 --> 00:12:35.280]   Leave aside all the geopolitics and the conflicts and war and America, China, all the rest of
[00:12:35.280 --> 00:12:36.280]   it.
[00:12:36.280 --> 00:12:40.720]   This revolution is going to change everything about our society, our economy, the way we
[00:12:40.720 --> 00:12:43.160]   live, the way we interact with each other.
[00:12:43.160 --> 00:12:48.840]   And if you don't get across it, then when there is a crisis like the one you're positing
[00:12:48.840 --> 00:12:52.600]   could happen, you're going to find you've got no idea how to deal with it.
[00:12:52.600 --> 00:12:53.600]   Yeah.
[00:12:53.600 --> 00:12:54.600]   Okay.
[00:12:54.600 --> 00:12:59.120]   So I think COVID is maybe a good case study to analyse how these systems function.
[00:12:59.120 --> 00:13:04.000]   And Tony Blair Institute made, I think, what were very sensible recommendations to governments,
[00:13:04.000 --> 00:13:07.400]   many of which went unheeded.
[00:13:07.400 --> 00:13:11.360]   And what I thought was especially alarming about COVID was not only that governments
[00:13:11.360 --> 00:13:15.880]   made these mistakes with vaccine rollout and testing and so forth, but that these mistakes
[00:13:15.880 --> 00:13:19.120]   were so correlated across major governments.
[00:13:19.120 --> 00:13:23.600]   No Western government basically got COVID right, maybe no government got COVID right.
[00:13:23.600 --> 00:13:26.200]   What is the fundamental source of that correlation?
[00:13:26.200 --> 00:13:32.360]   In the way that governments are bad at dealing with crises, they seem in some correlated
[00:13:32.360 --> 00:13:33.600]   way bad with crises.
[00:13:33.600 --> 00:13:35.600]   Is it because the same people are running these governments?
[00:13:35.600 --> 00:13:37.680]   Is it because the apparatus is the same?
[00:13:37.680 --> 00:13:38.680]   Why is that?
[00:13:38.680 --> 00:13:42.880]   Well, first of all, to be fair to people who were in government at the time of COVID, it
[00:13:42.880 --> 00:13:44.600]   was a difficult thing to deal with.
[00:13:44.600 --> 00:13:50.620]   You know, I always said the problem with COVID was that it was plainly more serious than
[00:13:50.620 --> 00:13:54.520]   your average flu, but it wasn't the bubonic plague.
[00:13:54.520 --> 00:14:00.740]   So to begin with, there was one very difficult question, which is to what degree do you try
[00:14:00.740 --> 00:14:04.080]   and shut the place down in order to get rid of the disease?
[00:14:04.080 --> 00:14:08.160]   And you had various approaches to that, but that's one very difficult question.
[00:14:08.160 --> 00:14:14.720]   And most governments kind of try to strike a middle course, right, to do restrictions,
[00:14:14.720 --> 00:14:17.260]   but then ease them up over time.
[00:14:17.260 --> 00:14:18.840]   And then you have the issue of vaccination.
[00:14:18.840 --> 00:14:21.840]   Now normally with drugs, it takes you years to trial a drug, right?
[00:14:21.840 --> 00:14:24.240]   You had to accelerate all of that.
[00:14:24.240 --> 00:14:29.460]   And that was done to be fair, but then you have to distribute it.
[00:14:29.460 --> 00:14:32.380]   And that is also a major challenge.
[00:14:32.380 --> 00:14:41.080]   So I think part of the problem was that governments weren't sure where to go for advice.
[00:14:41.080 --> 00:14:45.620]   You know, they had scientific advice, they had medical advice, but then they had to balance
[00:14:45.620 --> 00:14:52.980]   that with the needs of their economy and the anxiety a lot of people had that when you
[00:14:52.980 --> 00:14:57.960]   were having a large shutdown, that they were going to be hugely disadvantaged, as indeed
[00:14:57.960 --> 00:14:59.240]   people were.
[00:14:59.240 --> 00:15:06.340]   And I think one of the things that COVID did was for the developing world, I think there's
[00:15:06.340 --> 00:15:11.160]   an argument for saying for the developing world that lockdowns probably did more harm
[00:15:11.160 --> 00:15:12.160]   than good.
[00:15:12.160 --> 00:15:13.160]   Right.
[00:15:13.160 --> 00:15:16.160]   But so it sounds like you're saying that they made the trade-off which you're describing
[00:15:16.160 --> 00:15:21.160]   in the wrong way where they could have gone heavier on the testing and vaccination rollout
[00:15:21.160 --> 00:15:24.440]   so that the lockdowns could have been avoided and fundamentally more people's lives could
[00:15:24.440 --> 00:15:25.440]   have been saved.
[00:15:26.440 --> 00:15:27.440]   I still don't understand.
[00:15:27.440 --> 00:15:31.760]   So, I mean, in some fundamental sense, the pandemic is a simpler problem to deal with
[00:15:31.760 --> 00:15:34.520]   than an AI crisis in a technical sense.
[00:15:34.520 --> 00:15:37.560]   Like yes, you have to fast-track the vaccines, but it's a thing we've dealt with before,
[00:15:37.560 --> 00:15:38.560]   right?
[00:15:38.560 --> 00:15:39.560]   There's vaccines, you roll them out.
[00:15:39.560 --> 00:15:43.180]   If the government can't get that right, how worried should we be about their ability to
[00:15:43.180 --> 00:15:45.040]   deal with AI risk?
[00:15:45.040 --> 00:15:50.520]   And should we then just be fundamentally averse to a government-led answer to the solution?
[00:15:50.520 --> 00:15:53.320]   Should we hope that the private sector can solve this because the government was so bad
[00:15:53.320 --> 00:15:54.320]   at COVID?
[00:15:54.320 --> 00:15:59.440]   Well, what the private sector can do is to input into the public sector.
[00:15:59.440 --> 00:16:07.720]   And so, you know, in COVID, I mean, the countries that handed vaccine procurement, well, not
[00:16:07.720 --> 00:16:12.600]   so much vaccine procurement, vaccine production, if they handed it to the private sector and
[00:16:12.600 --> 00:16:17.680]   said run with it, right, those are the countries that did best, frankly.
[00:16:17.680 --> 00:16:23.760]   And I think, especially with something as technically complex as AI, you know, you are
[00:16:23.760 --> 00:16:31.840]   going to rely on the private sector for the facts of what is happening and to be able
[00:16:31.840 --> 00:16:34.880]   to establish the options about what you do.
[00:16:34.880 --> 00:16:39.080]   But in the end, the government or the public sector will have to decide which option to
[00:16:39.080 --> 00:16:40.080]   take.
[00:16:40.080 --> 00:16:43.640]   And the thing that makes governing difficult is I will say to people, when you decide,
[00:16:43.640 --> 00:16:44.640]   you divide.
[00:16:44.640 --> 00:16:50.280]   You know, the moment you take a decision on a public policy question, there are always
[00:16:50.280 --> 00:16:51.920]   two ways you can go, right?
[00:16:51.920 --> 00:16:56.800]   Like with COVID, you could have decided to do what Sweden did and let the disease run,
[00:16:56.800 --> 00:16:57.800]   pretty much.
[00:16:57.800 --> 00:17:01.100]   You could have decided to do what China did and lock down completely.
[00:17:01.100 --> 00:17:06.080]   But then what happened with China was once you got the Omicron variant, and it became
[00:17:06.080 --> 00:17:11.160]   obvious you weren't going to be able to keep COVID out, they didn't have the facility or
[00:17:11.160 --> 00:17:13.000]   the agility to go and change policy.
[00:17:13.000 --> 00:17:15.620]   But, you know, these policy questions are hard.
[00:17:15.620 --> 00:17:19.880]   And it's very easy with hindsight, you know, to say, yeah, you should have done this, should
[00:17:19.880 --> 00:17:20.880]   have done that.
[00:17:20.880 --> 00:17:28.680]   But I think if this happened in relation to AI, you would absolutely depend on the people
[00:17:28.680 --> 00:17:35.800]   who were developing AI to be able to know what decision you should take, you know, not
[00:17:35.800 --> 00:17:40.280]   necessarily how you decide it, but what is the decision.
[00:17:40.280 --> 00:17:41.280]   Yeah.
[00:17:41.280 --> 00:17:47.240]   I still don't fundamentally understand the answer to, okay, so before COVID, we had these
[00:17:47.240 --> 00:17:50.880]   bureaucracies and we hope they function, health bureaucracies, we hope they function well.
[00:17:50.880 --> 00:17:53.440]   It turns out many of them didn't.
[00:17:53.440 --> 00:17:58.480]   We probably have equivalents in terms of AI, where we have government departments to deal
[00:17:58.480 --> 00:18:01.160]   with technology and commerce and so forth.
[00:18:01.160 --> 00:18:06.220]   And if they're as potentially broken as we found out that much of our health bureaucracy
[00:18:06.220 --> 00:18:13.120]   is, what if you're a prime minister now or the next government, what would you do other
[00:18:13.120 --> 00:18:16.600]   than we're going to have a task force, we're going to like make sure we're making good
[00:18:16.600 --> 00:18:17.600]   decisions.
[00:18:17.600 --> 00:18:20.480]   But, you know, I'm sure like when people were trying to make sure the CDC was functional
[00:18:20.480 --> 00:18:24.120]   and like it wasn't like, would you just like fire everybody there and like make a new department?
[00:18:24.120 --> 00:18:27.600]   How would you go about like making sure it's really ready for the AI crisis?
[00:18:27.600 --> 00:18:28.600]   Yeah.
[00:18:28.600 --> 00:18:32.000]   So I think you've got to distinguish between two separate things.
[00:18:32.000 --> 00:18:42.600]   One is making the system have the skills and the sensitivity to know the different contours
[00:18:42.600 --> 00:18:49.440]   of the crisis that you've got and to be able to produce potential solutions, right, for
[00:18:49.440 --> 00:18:50.960]   what you do.
[00:18:50.960 --> 00:18:57.360]   So we needed to rely upon the scientific community to say, this is how we think the disease is
[00:18:57.360 --> 00:18:58.680]   going to run.
[00:18:58.680 --> 00:19:03.500]   We relied on the private sector to say, this is how we could develop vaccines.
[00:19:03.500 --> 00:19:08.680]   We had to rely on different agencies in order to say, well, I think you could concertina
[00:19:08.680 --> 00:19:13.120]   the trial period to get the drugs.
[00:19:13.120 --> 00:19:17.160]   But in the end, the decision where you lock down or you don't lock down, you can't really
[00:19:17.160 --> 00:19:22.600]   leave it to those people because that's not their, you see what I mean?
[00:19:22.600 --> 00:19:30.400]   So in the end, as with AI, okay, if you look at it at the moment, so some people want to
[00:19:30.400 --> 00:19:35.640]   regulate AI now and regulate it on the basis that it's going to cause enormous dangers
[00:19:35.640 --> 00:19:37.080]   and problems, right?
[00:19:37.080 --> 00:19:40.960]   And because it's general purpose technology, yeah, there are real risks and problems associated
[00:19:40.960 --> 00:19:41.960]   with it.
[00:19:41.960 --> 00:19:48.480]   So Europe is already moving in quite a, frankly, an adverse regulatory way.
[00:19:48.480 --> 00:19:52.080]   On the other hand, there will be people saying, well, if you do that, you're going to stifle
[00:19:52.080 --> 00:19:56.480]   innovation and we're going to lose the opportunities that come with this new technology.
[00:19:56.480 --> 00:19:59.880]   But balancing those two things, I mean, that's what politics is about.
[00:19:59.880 --> 00:20:06.360]   Now you need the experts, the people who know what they're talking about to tell you, this
[00:20:06.360 --> 00:20:07.920]   is how AI is going to be.
[00:20:07.920 --> 00:20:09.120]   This is what it can do.
[00:20:09.120 --> 00:20:10.760]   This is what it can't do.
[00:20:10.760 --> 00:20:16.880]   Okay, we are explaining the technicality to you, but ultimately what your policy is, you've
[00:20:16.880 --> 00:20:18.240]   got to decide that.
[00:20:18.240 --> 00:20:21.400]   And by the way, whichever way you decide it, someone's going to attack you for it.
[00:20:21.400 --> 00:20:26.600]   Look, I'm incredibly grateful that I've been able to turn this podcast into an actual business.
[00:20:26.600 --> 00:20:29.560]   And one of the reasons why is Stripe.
[00:20:29.560 --> 00:20:31.760]   Stripe is how I set myself up an LLC.
[00:20:31.760 --> 00:20:35.740]   It's what I use to send invoices to my sponsors and get paid by them.
[00:20:35.740 --> 00:20:37.480]   And I'm not the only one.
[00:20:37.480 --> 00:20:42.160]   Millions of businesses use Stripe to accept payments and move money around the world.
[00:20:42.160 --> 00:20:45.660]   That includes small businesses and startups like mine, where we're just trying to build
[00:20:45.660 --> 00:20:49.040]   our product instead of worrying about the hassle of the payment rails.
[00:20:49.040 --> 00:20:53.000]   But it also includes big businesses like Amazon and Hertz.
[00:20:53.000 --> 00:20:56.600]   These guys have unlimited engineers that they could throw out this problem, but they prefer
[00:20:56.600 --> 00:20:59.080]   to have Stripe build payments for them.
[00:20:59.080 --> 00:21:03.960]   And that's because Stripe can fine tune the details of the payment experience across billions
[00:21:03.960 --> 00:21:07.800]   of transactions and then serve what works best for yours.
[00:21:07.800 --> 00:21:12.200]   That obviously means higher conversion rates and as a result, more revenue for you.
[00:21:12.200 --> 00:21:16.880]   So thanks to Stripe for sponsoring this episode and also for making it possible for me to
[00:21:16.880 --> 00:21:19.160]   earn a living doing what I love.
[00:21:19.160 --> 00:21:20.560]   And now back to Tony Blair.
[00:21:20.560 --> 00:21:28.280]   Okay, so let's go back to the topics we discussed with foreign leaders at TBI.
[00:21:28.280 --> 00:21:32.840]   If you were, you know, take Lee Kuan Yew in his position in the 1960s and the Singapore
[00:21:32.840 --> 00:21:38.160]   he inherited, if you were advising Lee Kuan Yew in the 60s with the advice you would likely
[00:21:38.160 --> 00:21:43.080]   give to a developing country now, would Singapore have been even more successful than it ended
[00:21:43.080 --> 00:21:44.080]   up?
[00:21:44.080 --> 00:21:45.080]   Would it have been less successful?
[00:21:45.080 --> 00:21:47.440]   What would be the effect of your advice now have been on Singapore in the 60s?
[00:21:47.440 --> 00:21:51.880]   With Lee Kuan Yew in Singapore, I would, I mean, it's the wrong way around.
[00:21:51.880 --> 00:21:55.640]   I mean, I learned so much from him.
[00:21:55.640 --> 00:22:00.400]   And I went to see him first back in the 1990s when I was leader of the Labour Party.
[00:22:00.400 --> 00:22:04.940]   And I went in to see him in Singapore, and the Labour Party had been really critical
[00:22:04.940 --> 00:22:05.940]   of Singapore.
[00:22:05.940 --> 00:22:09.000]   And so the first thing he said to me when I came in the room was, "Why are you seeing
[00:22:09.000 --> 00:22:10.000]   me?
[00:22:10.000 --> 00:22:11.580]   You know, your party's always hated me."
[00:22:11.580 --> 00:22:14.760]   And so I said, "I want to see you because I've watched what you do in government.
[00:22:14.760 --> 00:22:16.140]   I want to learn from it."
[00:22:16.140 --> 00:22:18.560]   So I don't think there's anything I could have told Lee Kuan Yew.
[00:22:18.560 --> 00:22:24.760]   But the interesting thing, because he's a fascinating leader, and this is so interesting
[00:22:24.760 --> 00:22:33.360]   about government, and what I say is that you can look upon government like, don't look
[00:22:33.360 --> 00:22:41.360]   upon it as a branch of politics, look upon it as its own discipline, professional discipline,
[00:22:41.360 --> 00:22:45.800]   and you can learn lessons of what's worked and what doesn't work.
[00:22:45.800 --> 00:22:50.880]   And so the fascinating thing about Lee Kuan Yew is he took three decisions, really three
[00:22:50.880 --> 00:22:55.400]   important decisions, right at the beginning for Singapore.
[00:22:55.400 --> 00:22:57.280]   Each one of them now seems obvious.
[00:22:57.280 --> 00:23:00.060]   Each one at the time was deeply contested.
[00:23:00.060 --> 00:23:05.040]   Number one, he said, "Everyone's going to speak English in Singapore."
[00:23:05.040 --> 00:23:08.240]   Now there were lots of people who said to him at the time, "No, no, we've been thrown
[00:23:08.240 --> 00:23:10.080]   out of Malaysia effectively.
[00:23:10.080 --> 00:23:13.560]   We're now a fledgling country, a city-state country.
[00:23:13.560 --> 00:23:18.640]   We need to have our own local language, you know, we need to be true to our roots and
[00:23:18.640 --> 00:23:19.640]   everything."
[00:23:19.640 --> 00:23:23.720]   So he said, "English is the language of the world, and we're all going to speak English."
[00:23:23.720 --> 00:23:25.440]   That's what happens in Singapore.
[00:23:25.440 --> 00:23:31.640]   Secondly, he said, "We're going to get the best intellectual capital and management capital
[00:23:31.640 --> 00:23:35.200]   from wherever it exists in the world, and we're going to bring it to Singapore."
[00:23:35.200 --> 00:23:38.680]   And again, people said, "No, we should stand on our own two feet.
[00:23:38.680 --> 00:23:42.680]   And you're also bringing in the British, who we've got all these disputes with."
[00:23:42.680 --> 00:23:45.120]   And he said, "No, I'm going to bring in the best from wherever they are, and they're going
[00:23:45.120 --> 00:23:46.800]   to come to Singapore."
[00:23:46.800 --> 00:23:50.240]   Today Singapore exports intellectual capital.
[00:23:50.240 --> 00:23:54.120]   And the third thing he did was he said, "There's going to be no corruption, and one of the
[00:23:54.120 --> 00:23:59.240]   ways we're going to do that is we're going to make sure our political leaders are well-paid,
[00:23:59.240 --> 00:24:04.240]   which the Singapore leaders are the best paid in the world by a factor of about 10 for the
[00:24:04.240 --> 00:24:08.520]   next person, and there's going to be zero tolerance of corruption.
[00:24:08.520 --> 00:24:10.400]   Zero tolerance of corruption."
[00:24:10.400 --> 00:24:14.040]   And those are the three decisions that were instrumental in building Singapore today.
[00:24:14.040 --> 00:24:19.720]   So, to the extent that Western governments have, you know, you could go to the UK right
[00:24:19.720 --> 00:24:25.680]   now or the US, and fundamentally, if you had to narrow down the list of the three key priorities,
[00:24:25.680 --> 00:24:30.920]   and what would somebody who was in power to do so, what could they do to fix that?
[00:24:30.920 --> 00:24:35.800]   Do Western leaders, if Starmer is elected in the UK or whoever becomes president of
[00:24:35.800 --> 00:24:41.580]   the US, would they have the power to enact what the equivalent would be for their societies
[00:24:41.580 --> 00:24:42.840]   right now?
[00:24:42.840 --> 00:24:48.080]   Or is it just sort of like, you have all this inertia, you can't start fresh like Singapore
[00:24:48.080 --> 00:24:49.080]   could start in the 60s?
[00:24:49.080 --> 00:24:50.360]   MG No, you definitely can.
[00:24:50.360 --> 00:24:56.800]   I mean, the American system is different because it's a federal system, and there's probably,
[00:24:56.800 --> 00:25:01.520]   in many ways, it's a good thing that there are limits to what the federal government
[00:25:01.520 --> 00:25:03.380]   can do in the US.
[00:25:03.380 --> 00:25:08.600]   But if you take the UK or most governments where there's a lot of power at the centre,
[00:25:08.600 --> 00:25:12.720]   I mean, what we've just been talking about, which is the technology revolution - how do
[00:25:12.720 --> 00:25:17.480]   you use it to transform healthcare, education, the way government functions?
[00:25:17.480 --> 00:25:23.640]   How do you help educate the private sector as to how they can embrace AI in order to
[00:25:23.640 --> 00:25:24.640]   improve productivity?
[00:25:24.640 --> 00:25:28.400]   I mean, this is a huge agenda for a government and a really exciting one.
[00:25:28.400 --> 00:25:33.400]   I mean, I keep saying to people that were in politics today, because sometimes people
[00:25:33.400 --> 00:25:37.600]   get a bit depressed about being in politics because you get all this criticism.
[00:25:37.600 --> 00:25:42.240]   People certainly in the West feel society's not changing fast enough and well enough.
[00:25:42.240 --> 00:25:45.000]   And I say, no, it's a really exciting time to be in politics because you've got this
[00:25:45.000 --> 00:25:48.640]   massive revolution that you've got to come to terms with.
[00:25:48.640 --> 00:25:52.000]   JS Speaking of federalism, do you worry that,
[00:25:52.000 --> 00:25:56.280]   so you advise these dozens of governments, and for any one leader, you're probably giving
[00:25:56.280 --> 00:26:01.960]   very sensible advice for their country, its positive expected value.
[00:26:01.960 --> 00:26:07.960]   But to the extent that that limits the variance and experimentation across countries of different
[00:26:07.960 --> 00:26:14.240]   ways to govern or different policies, are we losing the ability to discover a new Singapore
[00:26:14.240 --> 00:26:21.480]   because there's, you know, Western NGOs or whatever global institutions we have will
[00:26:21.480 --> 00:26:25.080]   give you good recommendations and maybe there's like some missing thing we don't understand
[00:26:25.080 --> 00:26:26.560]   that an experimentation would reveal?
[00:26:26.560 --> 00:26:27.560]   DS Yes.
[00:26:27.560 --> 00:26:31.280]   So we really don't do that with our governments because, by the way, one of the things governments
[00:26:31.280 --> 00:26:34.080]   should be able to do is experiment to a degree.
[00:26:34.080 --> 00:26:38.800]   And part of the problem with systems is there's always a bias towards caution.
[00:26:38.800 --> 00:26:42.160]   That's what I mean by saying that the systems, if they're a conspiracy for anything, it's
[00:26:42.160 --> 00:26:43.760]   for inertia.
[00:26:43.760 --> 00:26:46.440]   But there are some things that we do.
[00:26:46.440 --> 00:26:50.600]   We concentrate with governments on what are true no matter what government you're in.
[00:26:50.600 --> 00:26:56.180]   So I describe four Ps of government when you get into power, right?
[00:26:56.180 --> 00:27:00.680]   Number one, you've got to prioritise because if you try to do everything, you'll do nothing.
[00:27:00.680 --> 00:27:05.440]   Number two, you've got to get the right policy, what we were talking about before.
[00:27:05.440 --> 00:27:09.640]   That means going deep and getting the right answer, and that means often bringing people
[00:27:09.640 --> 00:27:12.840]   in from the outside who can tell you what the right answer is, which has nothing to
[00:27:12.840 --> 00:27:13.840]   do with left, right.
[00:27:13.840 --> 00:27:15.900]   It's usually to do with practicality.
[00:27:15.900 --> 00:27:17.840]   Number three, you've got to have the right personnel.
[00:27:17.840 --> 00:27:20.080]   Number four, you've got to performance manage.
[00:27:20.080 --> 00:27:24.940]   Once you've decided something and you've got a policy, you've got to focus on the implementation.
[00:27:24.940 --> 00:27:29.920]   Now whether you're running the United States of America, you're running, you know, a small
[00:27:29.920 --> 00:27:31.040]   African country.
[00:27:31.040 --> 00:27:32.280]   Those things are always true.
[00:27:32.280 --> 00:27:36.980]   Okay, let's talk about foreign policy for a second.
[00:27:36.980 --> 00:27:42.240]   This is not just you, but every sort of administration has to deal, especially Western administration,
[00:27:42.240 --> 00:27:45.880]   has to deal with these irascible dictatorial regimes.
[00:27:45.880 --> 00:27:52.880]   And they're like right on the brink of WMDs and they make all these demands in order to
[00:27:52.880 --> 00:27:55.640]   put off their path towards WMDs.
[00:27:55.640 --> 00:27:57.120]   Obviously you had to deal with Saddam.
[00:27:57.120 --> 00:27:59.880]   Today we have to deal with Iran and North Korea.
[00:27:59.880 --> 00:28:02.120]   It seems like sanctions don't seem to work.
[00:28:02.120 --> 00:28:04.360]   Regime change is really expensive.
[00:28:04.360 --> 00:28:09.560]   Is there any fundamental solution to this kind of dilemma that we keep being put into
[00:28:09.560 --> 00:28:10.560]   decade after decade?
[00:28:10.560 --> 00:28:16.040]   Can we just buy them a nice mansion in Costa Rica or what can we do about these kinds of
[00:28:16.040 --> 00:28:17.040]   regimes?
[00:28:17.040 --> 00:28:20.000]   Yeah, it's very difficult.
[00:28:20.000 --> 00:28:25.560]   I think you can do, I mean, if you take Iran today, I don't think there's any appetite
[00:28:25.560 --> 00:28:33.120]   in the West certainly to go and enforce regime change.
[00:28:33.120 --> 00:28:38.400]   But I think you could do two things that are really important because Iran is basically
[00:28:38.400 --> 00:28:48.680]   the origin of most of the destabilization across the Middle East region and beyond.
[00:28:48.680 --> 00:28:51.980]   First of all, you can constrain it as much as possible.
[00:28:51.980 --> 00:29:00.080]   And secondly, you can build alliances, which mean that their ability to impact is reduced.
[00:29:00.080 --> 00:29:07.000]   But it's a constant problem because they're determined to acquire nuclear weapons capability.
[00:29:07.000 --> 00:29:08.560]   We want to stop them doing that.
[00:29:08.560 --> 00:29:10.760]   We don't want to engage in regime change.
[00:29:10.760 --> 00:29:16.560]   On the other hand, all the other things that you do will be limited in their effect.
[00:29:16.560 --> 00:29:17.680]   So it's difficult.
[00:29:17.680 --> 00:29:23.720]   It's very difficult, particularly now where you have an alliance that has grown up where
[00:29:23.720 --> 00:29:29.360]   China, Russia, Iran, to a degree, North Korea work closely together.
[00:29:29.360 --> 00:29:34.120]   As a leader, how do you distinguish cases when the intelligence communities come to
[00:29:34.120 --> 00:29:39.500]   you and say, well, how do you distinguish a case like Iraq where potentially they got
[00:29:39.500 --> 00:29:42.880]   it wrong versus Ukraine where it seems like they were on the ball?
[00:29:42.880 --> 00:29:46.760]   How do you know which intelligence to trust and how good is Western intelligence generally?
[00:29:46.760 --> 00:29:48.640]   How good are the five eyes?
[00:29:48.640 --> 00:29:51.440]   Generally it's extremely good, and the five eyes is extremely good.
[00:29:51.440 --> 00:29:54.000]   And how do you distinguish the cases where they're not?
[00:29:54.000 --> 00:29:55.000]   Well, it's difficult.
[00:29:55.000 --> 00:30:00.780]   And with experience, the benefit of hindsight, particularly in relation to Iraq, you've got
[00:30:00.780 --> 00:30:07.600]   to go much deeper and you've got to not take the fact that there was all these problems
[00:30:07.600 --> 00:30:14.580]   in the past as an indication of what's happening now or in the future.
[00:30:14.580 --> 00:30:19.760]   But I think on the whole, Western intelligence is reasonably good and, of course, will get
[00:30:19.760 --> 00:30:23.640]   much better now with the tools it's got at its disposal.
[00:30:23.640 --> 00:30:26.960]   How much situational awareness do they have about the topics you're talking about, whether
[00:30:26.960 --> 00:30:29.880]   it's AI or the next pandemic?
[00:30:29.880 --> 00:30:34.480]   These forward-looking kinds of problems rather than who's doing an invasion when, which maybe
[00:30:34.480 --> 00:30:39.520]   they have a lot of expertise and decades of experience with, but predicting who's got
[00:30:39.520 --> 00:30:42.000]   the data center where and those kinds of things.
[00:30:42.000 --> 00:30:47.240]   I think they're all over this stuff now, the intelligence services here in America, in
[00:30:47.240 --> 00:30:48.240]   the UK.
[00:30:48.240 --> 00:30:57.620]   But you also got a whole new category of threat to deal with because cyber threats are real
[00:30:57.620 --> 00:31:01.020]   and potentially devastating in their impact.
[00:31:01.020 --> 00:31:05.220]   And you can see from Ukraine, I mean, war is going to be fought in a completely different
[00:31:05.220 --> 00:31:07.580]   way in the future as well.
[00:31:07.580 --> 00:31:10.840]   When you look at the different leaders you're advising, and maybe just through experience
[00:31:10.840 --> 00:31:14.820]   talking to them while you were in office and seeing how their countries progressed, how
[00:31:14.820 --> 00:31:19.820]   much of the variance in outcomes of countries is explained by the quality of the leadership
[00:31:19.820 --> 00:31:24.260]   versus other endogenous factors, human capital, geography, whatever else?
[00:31:24.260 --> 00:31:25.260]   Right.
[00:31:25.260 --> 00:31:30.720]   So I think, I mean, the whole reason I started this institute was because I think the quality
[00:31:30.720 --> 00:31:37.540]   of governance, of which the leadership is a big part, I think it is the determinant.
[00:31:37.540 --> 00:31:46.500]   In today's world where capital's mobile, technology's mobile, any country with good leadership,
[00:31:46.500 --> 00:31:48.420]   it can make a success.
[00:31:48.420 --> 00:31:56.500]   And if you take, you can take two countries side by side, same resources, same opportunities,
[00:31:56.500 --> 00:31:58.220]   same potential, therefore.
[00:31:58.220 --> 00:31:59.580]   One succeeds, one fails.
[00:31:59.580 --> 00:32:02.760]   If you look at it, it's always about the quality of decision making.
[00:32:02.760 --> 00:32:07.200]   So if you take, for example, before the Ukraine war, if you take Poland and Ukraine, when
[00:32:07.200 --> 00:32:13.340]   both came out of the Soviet Union in the early 1990s, I think people would have given Ukraine
[00:32:13.340 --> 00:32:15.500]   as much chance as Poland of doing well.
[00:32:15.500 --> 00:32:18.380]   Poland today is doing really well.
[00:32:18.380 --> 00:32:19.380]   Why is that?
[00:32:19.380 --> 00:32:22.940]   Because they've joined the European Union, they've had to make huge changes and reforms,
[00:32:22.940 --> 00:32:24.800]   and therefore they're a successful country.
[00:32:24.800 --> 00:32:26.180]   You look at Rwanda and Burundi.
[00:32:26.180 --> 00:32:31.620]   It was Rwanda that suffered the genocide, but Rwanda today, I mean, it's one of the
[00:32:31.620 --> 00:32:34.420]   most respected countries in Africa.
[00:32:34.420 --> 00:32:38.780]   And then you look at the Korean peninsula, which is the biggest experiment in human governance
[00:32:38.780 --> 00:32:41.100]   there's ever been, North Korea and South Korea.
[00:32:41.100 --> 00:32:48.580]   South Korea had the same GDP per head as Sierra Leone in the 1960s, and now it's one of the
[00:32:48.580 --> 00:32:49.580]   top countries in the world.
[00:32:49.580 --> 00:32:53.580]   And you think that's a fundamentally a question of who the leadership was, like Park in South
[00:32:53.580 --> 00:32:57.740]   Korea, or there are other factors that are obviously different between these countries,
[00:32:57.740 --> 00:32:58.740]   right?
[00:32:58.740 --> 00:33:02.380]   And also you mentioned leadership determines governance.
[00:33:02.380 --> 00:33:08.700]   But I guess if you look at a system like the United States or the UK, we've had good and
[00:33:08.700 --> 00:33:10.400]   bad leaders.
[00:33:10.400 --> 00:33:13.740]   Fundamentally the quality of the governance doesn't seem to shift that much between who
[00:33:13.740 --> 00:33:15.420]   the leader is.
[00:33:15.420 --> 00:33:20.420]   Is the quality of governance and the quality of institutions a sort of separate endogenous
[00:33:20.420 --> 00:33:22.500]   variable from the leadership?
[00:33:22.500 --> 00:33:27.980]   Well, the institutions matter and good leaders should be able to build good institutions.
[00:33:27.980 --> 00:33:30.940]   But we were talking about Lee Kuan Yew in Singapore.
[00:33:30.940 --> 00:33:34.980]   Would Singapore be Singapore without those decisions that he took?
[00:33:34.980 --> 00:33:35.980]   No.
[00:33:35.980 --> 00:33:42.040]   And if you look at, take for example, China and Deng Xiaoping.
[00:33:42.040 --> 00:33:47.440]   I mean, when he decided to switch after the death of Mao and switch policy completely
[00:33:47.440 --> 00:33:53.060]   to open China up, that China opening up, I mean, that made the difference.
[00:33:53.060 --> 00:33:59.660]   If you look, if you track back India's development over the past 25 years, you can see the points
[00:33:59.660 --> 00:34:06.740]   at which decisions were taken that gave India the chance it has today.
[00:34:06.740 --> 00:34:14.640]   So I actually think the interesting thing about this is how much it really does matter
[00:34:14.640 --> 00:34:19.660]   who the leader is and the governance of the country.
[00:34:19.660 --> 00:34:26.300]   And I think the interesting thing today, which is what I say to people engaged in this groundbreaking
[00:34:26.300 --> 00:34:31.420]   revolution of artificial intelligence, is we need your help in changing government and
[00:34:31.420 --> 00:34:33.140]   changing countries.
[00:34:33.140 --> 00:34:38.300]   Because we could do things, what I say when I'm talking to people in the developing world
[00:34:38.300 --> 00:34:43.260]   today and leaders that we're working with, I say to them, don't try and replicate the
[00:34:43.260 --> 00:34:44.300]   systems of the West.
[00:34:44.300 --> 00:34:45.920]   You don't need to do that.
[00:34:45.920 --> 00:34:51.380]   You can teach your children better and differently without building the same type of system we
[00:34:51.380 --> 00:34:52.940]   have in the West.
[00:34:52.940 --> 00:34:58.500]   I wouldn't design the healthcare system in the UK today as it is now, if we had the benefit
[00:34:58.500 --> 00:35:00.580]   of generative AI.
[00:35:00.580 --> 00:35:07.500]   So the leaders that are going to succeed in these next years will be the people that can
[00:35:07.500 --> 00:35:10.580]   understand what is happening in places like this.
[00:35:10.580 --> 00:35:17.020]   And the frustrating thing from our perspective of leaders is that there's very few people
[00:35:17.020 --> 00:35:21.780]   in the technology sector who really - even though they would be probably very well intentioned
[00:35:21.780 --> 00:35:25.540]   towards the developing world - they sort of think, well, I don't know what I can do in
[00:35:25.540 --> 00:35:26.540]   order to help.
[00:35:26.540 --> 00:35:29.300]   But actually, there's massive amounts they can do in order to help.
[00:35:29.300 --> 00:35:34.300]   You know, you talk about improving public services with AI, but when you look at the
[00:35:34.300 --> 00:35:39.700]   IT revolution and how much it's improved, let's say, market services versus how much
[00:35:39.700 --> 00:35:46.100]   it's improved public sector services, there's clearly been a big difference.
[00:35:46.100 --> 00:35:50.100]   If you go back to IT, would it have been better to privatize the things that IT could have
[00:35:50.100 --> 00:35:52.580]   enabled more of, like education?
[00:35:52.580 --> 00:35:56.660]   And what lessons does that have for AI, where should we just - the public sector didn't
[00:35:56.660 --> 00:36:00.300]   seem that good at integrating IT, maybe we'll be bad at integrating AI, let's just privatize
[00:36:00.300 --> 00:36:03.380]   healthcare education as much as possible, because all the productivity gains will come
[00:36:03.380 --> 00:36:04.860]   from the private sector and those things anyways.
[00:36:04.860 --> 00:36:06.340]   Yeah, no, it's a great question.
[00:36:06.340 --> 00:36:11.020]   And it's the single most difficult thing, because you can't just hand everything over
[00:36:11.020 --> 00:36:14.500]   to the private sector, because in the end, the public will expect the public interest
[00:36:14.500 --> 00:36:17.700]   to be taken account of by government.
[00:36:17.700 --> 00:36:21.500]   And you know, you may say, "Well, government's useless at protecting the public interest."
[00:36:21.500 --> 00:36:22.500]   That's another matter.
[00:36:22.500 --> 00:36:28.820]   But on the whole, people in America, people in the UK, they're not going to say, "Okay,
[00:36:28.820 --> 00:36:32.820]   just hand it over to these tech giants and let them run everything."
[00:36:32.820 --> 00:36:38.620]   However, I do think what should happen - and we have a whole program in my institute now,
[00:36:38.620 --> 00:36:41.500]   which we call the Reimagined State.
[00:36:41.500 --> 00:36:47.980]   And I think if you look at - there was a minimalist state in the 18th century, and in the first
[00:36:47.980 --> 00:36:52.740]   part of the 19th century, that grew in the last part of the 19th century, and first part
[00:36:52.740 --> 00:36:58.540]   of the 20th century, into a maximalist state, where you look for government to do a lot
[00:36:58.540 --> 00:37:01.900]   of things for you, and the state grows large.
[00:37:01.900 --> 00:37:07.620]   I think we should reimagine the state today, as a result of this technology revolution,
[00:37:07.620 --> 00:37:10.140]   and make it much more strategic.
[00:37:10.140 --> 00:37:18.140]   It's much more about setting a framework, and then allowing much more diversity, competition.
[00:37:18.140 --> 00:37:25.600]   And the hardest thing about the public sector in those circumstances, is to create self-perpetuating
[00:37:25.600 --> 00:37:28.020]   innovation.
[00:37:28.020 --> 00:37:31.520]   If you don't innovate in the private sector, you've got a business.
[00:37:31.520 --> 00:37:36.140]   If you don't innovate in the public sector, I mean, you're still there, right?
[00:37:36.140 --> 00:37:38.320]   It's just the service has got worse.
[00:37:38.320 --> 00:37:44.500]   And so I think this is the really tough intellectual task.
[00:37:44.500 --> 00:37:48.860]   How do you, for example, in education today, I mean, how many kids in America, actually,
[00:37:48.860 --> 00:37:52.380]   you'll have a significant tail of kids that are taught really badly, right?
[00:37:52.380 --> 00:37:55.900]   Okay, same probably in any Western country.
[00:37:55.900 --> 00:37:59.380]   No one today should be taught badly, right?
[00:37:59.380 --> 00:38:03.380]   Everyone should be taught, by the way, also on an individual basis, on a personalized
[00:38:03.380 --> 00:38:04.380]   basis.
[00:38:04.380 --> 00:38:08.960]   And if you look at what, because we work with them, what Sal Khan's doing, for example,
[00:38:08.960 --> 00:38:14.980]   the Khan Academy, and there are other people doing great things in education using technology,
[00:38:14.980 --> 00:38:19.820]   we should be able to create a situation in which young people today are able to learn
[00:38:19.820 --> 00:38:23.140]   at the pace that is good for them.
[00:38:23.140 --> 00:38:26.400]   And no young person should be without opportunity.
[00:38:26.400 --> 00:38:31.320]   But how you reform the system to allow that to happen, that's the big challenge.
[00:38:31.320 --> 00:38:36.080]   But you know, in time, and like with the healthcare system, you will end up with an AI doctor,
[00:38:36.080 --> 00:38:37.580]   you'll end up with an AI tutor.
[00:38:37.580 --> 00:38:41.780]   The question will be, what's the framework within which those things operate?
[00:38:41.780 --> 00:38:49.240]   And how do we use them to allow a better service, and probably to allow a lot of the people
[00:38:49.240 --> 00:38:55.400]   within the healthcare or education systems to concentrate on the most important part
[00:38:55.400 --> 00:38:59.540]   of their learning, rather than, for example, if you're a doctor, having to write up a whole
[00:38:59.540 --> 00:39:03.920]   lot of notes after a consultation, or do lesson planning if you're a teacher.
[00:39:03.920 --> 00:39:04.920]   Yeah.
[00:39:04.920 --> 00:39:10.860]   Going back to TBI for a second, when you give a leader some sensible advice, and then they
[00:39:10.860 --> 00:39:14.800]   don't follow through on it, what usually is the reason that happens?
[00:39:14.800 --> 00:39:18.000]   Is this because it's not politically palatable in their country?
[00:39:18.000 --> 00:39:20.320]   Is it because they don't get it?
[00:39:20.320 --> 00:39:24.120]   Why is, to the extent that you have good advice that's ignored, why does that happen?
[00:39:24.120 --> 00:39:28.020]   It happens, usually, for two reasons.
[00:39:28.020 --> 00:39:31.840]   Number one, it's really hard to make change.
[00:39:31.840 --> 00:39:37.440]   What I learned about making change is that there's a certain rhythm to it.
[00:39:37.440 --> 00:39:42.420]   When you first propose a reform, people tell you it's a terrible idea.
[00:39:42.420 --> 00:39:45.940]   When you're doing it, it's absolute hell, and after you've done it, you wish you'd done
[00:39:45.940 --> 00:39:49.320]   more of it.
[00:39:49.320 --> 00:39:51.800]   Sometimes people just find the system too resistant.
[00:39:51.800 --> 00:39:55.260]   There might be vested interests that get in the way of it.
[00:39:55.260 --> 00:39:59.760]   I mean, I sometimes come across countries that are island states with warm weather,
[00:39:59.760 --> 00:40:05.500]   but they get all their electricity from heavy fuel oil, when they've got limitless amounts
[00:40:05.500 --> 00:40:09.480]   of solar and wind that they could be using, but it's vested interest.
[00:40:09.480 --> 00:40:17.140]   The other thing is, I sometimes say that government is a conspiracy of distraction, because you've
[00:40:17.140 --> 00:40:20.480]   got events and crises and scandals.
[00:40:20.480 --> 00:40:26.640]   The most difficult thing is to keep focused when you've got so many things that are diverting
[00:40:26.640 --> 00:40:29.480]   you from that core task.
[00:40:29.480 --> 00:40:36.040]   What often happens with leaders, sometimes what we do with our leaders is we say to them,
[00:40:36.040 --> 00:40:42.020]   "Okay, we're going to do an analysis of, here are your priorities, here's how much time
[00:40:42.020 --> 00:40:43.020]   you spend on them."
[00:40:43.020 --> 00:40:48.280]   And you end up literally with people spending 4% of their time on their priorities.
[00:40:48.280 --> 00:40:50.640]   And you say, "Well, no wonder you're not succeeding."
[00:40:50.640 --> 00:40:54.760]   In my recent chat with Mark Zuckerberg, he mentioned how cybercrime is a huge concern
[00:40:54.760 --> 00:40:59.000]   with these new AI models, where criminals can use these models to increase the volume
[00:40:59.000 --> 00:41:01.360]   and complexity of their attacks.
[00:41:01.360 --> 00:41:04.760]   Current cybersecurity defenses are not prepared for this future.
[00:41:04.760 --> 00:41:11.240]   As new threats emerge, organizations lack the tools to effectively and quickly assess
[00:41:11.240 --> 00:41:13.320]   and augment their defensive posture.
[00:41:13.320 --> 00:41:18.880]   My sponsor today, Prelude Security, ensures that organizations can exceed the speed of
[00:41:18.880 --> 00:41:23.480]   new threats by automating critical functions of their security response.
[00:41:23.480 --> 00:41:30.200]   Using AI, Prelude rapidly transforms complex intelligence into actionable outcomes to find
[00:41:30.200 --> 00:41:34.880]   attackers, update defenses, and validate that you're protected against them.
[00:41:34.880 --> 00:41:40.560]   Once a new threat arises, Prelude is the first place your board members, security leaders,
[00:41:40.560 --> 00:41:46.080]   and sec ops teams will look to know with certainty that their defenses will protect them against
[00:41:46.080 --> 00:41:47.360]   the latest threats.
[00:41:47.360 --> 00:41:54.360]   So if you're responsible for your organization's cyber defense, check out PreludeSecurity.com/speed.
[00:41:54.360 --> 00:41:58.960]   All right, back to Tony Blair.
[00:41:58.960 --> 00:42:02.480]   So when you look back at your time as Prime Minister, or I'm not necessarily picking on
[00:42:02.480 --> 00:42:08.880]   your time, just any head of state in Western government or any government, how much of
[00:42:08.880 --> 00:42:15.040]   the time they spend is fundamentally wasted in the sense of the three key priorities you
[00:42:15.040 --> 00:42:18.480]   would have identified for Singapore in the 1960s or something?
[00:42:18.480 --> 00:42:22.880]   It's not fundamentally moving the ball forward on things like that.
[00:42:22.880 --> 00:42:27.400]   You know, it's like meeting people, ambassadors, press, whatever.
[00:42:27.400 --> 00:42:29.440]   How much of the time is just that?
[00:42:29.440 --> 00:42:30.440]   A lot.
[00:42:30.440 --> 00:42:31.440]   I mean, I don't know.
[00:42:31.440 --> 00:42:32.440]   Greater than 80%?
[00:42:32.440 --> 00:42:33.440]   90%?
[00:42:33.440 --> 00:42:34.440]   That would be too high, I think.
[00:42:34.440 --> 00:42:41.920]   But a lot, and a lot more today, because people are expected.
[00:42:41.920 --> 00:42:48.560]   So I have this thing that I often say to leaders, and I think this is the single biggest problem
[00:42:48.560 --> 00:42:50.520]   with Western politics today.
[00:42:50.520 --> 00:42:55.720]   You know, sometimes when I used to, my kids were younger and I used to speak with them,
[00:42:55.720 --> 00:43:01.640]   and I would say to them, you know, work hard, play hard, right?
[00:43:01.640 --> 00:43:05.320]   Work hard, play hard equals possibility of success.
[00:43:05.320 --> 00:43:09.280]   Play hard, work hard is a certain failure, right?
[00:43:09.280 --> 00:43:12.380]   Because you'll play so hard you'll never end up working hard, right?
[00:43:12.380 --> 00:43:19.360]   The equivalent in politics is policy first, politics second.
[00:43:19.360 --> 00:43:25.340]   In other words, work out what the right answer is, and then work out how you shape the politics
[00:43:25.340 --> 00:43:27.000]   around that.
[00:43:27.000 --> 00:43:32.760]   But actually what happens in a lot of systems today is it's politics first and policy second.
[00:43:32.760 --> 00:43:37.440]   So people end up with a political position, they've chosen for political reasons, and
[00:43:37.440 --> 00:43:41.780]   then they try and shape a policy around that politics, and it never works.
[00:43:41.780 --> 00:43:46.300]   Because the most important thing - and this is why I say to you about policy makers and
[00:43:46.300 --> 00:43:49.560]   intellectual business - is you've got to get the right answer.
[00:43:49.560 --> 00:43:51.840]   And there is a right answer, by the way.
[00:43:51.840 --> 00:43:56.800]   And often, you know, the reason why it's so difficult to govern today is there's so much
[00:43:56.800 --> 00:44:06.760]   political noise, it's hard to get out of that zone of noise and sit in a room with some
[00:44:06.760 --> 00:44:10.960]   people who really know what they're talking about and go into the detail of what is the
[00:44:10.960 --> 00:44:12.680]   solution to a problem.
[00:44:12.680 --> 00:44:19.480]   And you know, sometimes when I talk to leaders about this, I find that they just say to me,
[00:44:19.480 --> 00:44:21.720]   "I don't have the time to do that."
[00:44:21.720 --> 00:44:26.160]   And I say, "If you don't have the time to do that, you are going to fail.
[00:44:26.160 --> 00:44:30.200]   Because in the end, you won't have the right answer."
[00:44:30.200 --> 00:44:37.000]   And you've got to believe over time that the best policy is the best politics, over time.
[00:44:37.000 --> 00:44:38.000]   Yeah.
[00:44:38.000 --> 00:44:42.840]   So structurally, how would you change the - it's not just the time you had to spend,
[00:44:42.840 --> 00:44:47.200]   for example, talking to the press, but also the kinds of topics that draws attention to,
[00:44:47.200 --> 00:44:54.160]   which are what statement your cabinet minister made on the BBC and some latest scandal.
[00:44:54.160 --> 00:44:57.280]   Is that fundamentally - like the 30 minutes of the PMQs is not the big deal, it's the
[00:44:57.280 --> 00:45:00.680]   two days you spend in anxiety and preparation that you were talking about in the book.
[00:45:00.680 --> 00:45:04.360]   Is that fundamentally the attention distraction is the bigger issue than the actual time you
[00:45:04.360 --> 00:45:05.880]   spend on these events?
[00:45:05.880 --> 00:45:08.560]   Yeah, I think it is to a degree.
[00:45:08.560 --> 00:45:14.800]   I mean, I think the other thing is, you undergo a lot of attack today in politics.
[00:45:14.800 --> 00:45:22.720]   And at a certain level, what happens - I mean, it could happen to celebrities, but they tend
[00:45:22.720 --> 00:45:27.440]   to have at least some sort of fan base that are constantly supporting them.
[00:45:27.440 --> 00:45:36.080]   But with politics today, you can often be in a situation where you're almost dehumanized,
[00:45:36.080 --> 00:45:37.080]   right?
[00:45:37.080 --> 00:45:45.680]   You're subject to attacks on your integrity, your character, your intentions.
[00:45:45.680 --> 00:45:50.480]   It's possible if you're not careful that you're just sitting there thinking, "This is really
[00:45:50.480 --> 00:45:51.480]   unfair."
[00:45:51.480 --> 00:45:54.820]   And you get distracted from focusing on the business.
[00:45:54.820 --> 00:45:59.360]   And that's why I always say to people, one part of being a political leader - or any
[00:45:59.360 --> 00:46:06.000]   leader, I think - is to be able to have a certain sort of what I call a kind of Zen,
[00:46:06.000 --> 00:46:12.660]   a bit of a Zen-like attitude to all the criticism and the disputatiousness that will go on around
[00:46:12.660 --> 00:46:14.400]   you because it's just going to happen.
[00:46:14.400 --> 00:46:17.440]   And then today with social media, it happens to an even greater degree.
[00:46:17.440 --> 00:46:22.640]   And one of the things I often say to leaders is, "You cannot pay attention to this stuff."
[00:46:22.640 --> 00:46:27.520]   I mean, okay, get someone to summarize it for you in half a page and you read it in
[00:46:27.520 --> 00:46:28.520]   the morning.
[00:46:28.520 --> 00:46:34.160]   Honestly, you start going in, you go down that rabbit hole, you'll never reemerge.
[00:46:34.160 --> 00:46:40.040]   Going back a little bit to your time in office, there was a unique unipolar moment that happens
[00:46:40.040 --> 00:46:45.120]   very rarely in history where the Anglosphere had much more power in the '90s and early
[00:46:45.120 --> 00:46:50.720]   2000s than the rest of the world.
[00:46:50.720 --> 00:46:53.280]   In what way did that feel different from today's world?
[00:46:53.280 --> 00:46:57.840]   And is there something you wish in the way the institutions were set up at the time and
[00:46:57.840 --> 00:47:04.000]   were carried forward now that you would maybe change?
[00:47:04.000 --> 00:47:05.840]   There was a key opportunity in the unipolar moment.
[00:47:05.840 --> 00:47:06.840]   How should that have been used?
[00:47:06.840 --> 00:47:07.840]   How well was it used?
[00:47:07.840 --> 00:47:08.840]   Yeah, it's difficult.
[00:47:08.840 --> 00:47:18.080]   I mean, we did try a lot, contrary to what's sometimes written, for example, with Russia.
[00:47:18.080 --> 00:47:27.800]   And I dealt with President Putin a lot when I was prime minister.
[00:47:27.800 --> 00:47:32.500]   We also took the, I think it was myself and President Clinton, took the crucial decision
[00:47:32.500 --> 00:47:39.600]   to bring China into the world's trading framework.
[00:47:39.600 --> 00:47:47.320]   And the G7 at the time was the G8 with Russia there, and China would always be invited.
[00:47:47.320 --> 00:47:49.400]   I think we did try.
[00:47:49.400 --> 00:47:55.840]   I honestly think we tried a lot to recognize that we were going to live in a new world.
[00:47:55.840 --> 00:48:00.840]   The power was going to not shift from the West in the sense that the West will become
[00:48:00.840 --> 00:48:06.240]   not powerful, but the East was going to become also powerful.
[00:48:06.240 --> 00:48:11.480]   And I think we kind of did understand that and worked towards that.
[00:48:11.480 --> 00:48:18.440]   The problem is that, and particularly in these last few years, certainly China and Russia
[00:48:18.440 --> 00:48:26.000]   have come to a position that is, in terms of fundamental values and systems, seemingly
[00:48:26.000 --> 00:48:30.960]   hostile to Western democracy.
[00:48:30.960 --> 00:48:33.480]   And that's difficult.
[00:48:33.480 --> 00:48:39.360]   I think what we underestimated was probably how fast India would rise, because at the
[00:48:39.360 --> 00:48:44.080]   time it didn't seem India was still going to be quite constrained.
[00:48:44.080 --> 00:48:49.040]   We live in a multipolar world today, and personally I think that's a good thing.
[00:48:49.040 --> 00:48:53.320]   And I think it's, in any event, it's an inevitable thing.
[00:48:53.320 --> 00:49:02.600]   And I think it's really important always to give this message to China, for example, that
[00:49:02.600 --> 00:49:07.640]   China as of right is one of the big powers in the world, and as of right should have
[00:49:07.640 --> 00:49:09.220]   a huge influence.
[00:49:09.220 --> 00:49:15.600]   And I don't believe in trying to constrain or contain China, but we do have to accept
[00:49:15.600 --> 00:49:24.640]   that the Chinese system as it presently is, is run on different lines to our own and is
[00:49:24.640 --> 00:49:31.480]   overtly in some degree hostile, which is why it's important for us to retain military and
[00:49:31.480 --> 00:49:39.620]   technological superiority; even though I believe passionately that it's important that we leave
[00:49:39.620 --> 00:49:44.100]   space for cooperation and engagement with China.
[00:49:44.100 --> 00:49:48.980]   Now how much could we have foreseen of all of this back in those days?
[00:49:48.980 --> 00:49:49.980]   I'm not sure.
[00:49:49.980 --> 00:49:56.900]   But I think sometimes one of the problems of the West is that we always see it through
[00:49:56.900 --> 00:50:01.200]   our own lens, and we always think, "Well, we could have done something different to
[00:50:01.200 --> 00:50:03.260]   change the world."
[00:50:03.260 --> 00:50:07.940]   But actually the rest of the world operates on its own principles as well, so sometimes
[00:50:07.940 --> 00:50:12.660]   the change happens not because we didn't do something, but because the rest of the world
[00:50:12.660 --> 00:50:13.660]   did.
[00:50:13.660 --> 00:50:14.660]   Sure.
[00:50:14.660 --> 00:50:15.660]   Final question.
[00:50:15.660 --> 00:50:22.000]   You interact with all these leaders today across probably dozens, maybe hundreds of
[00:50:22.000 --> 00:50:23.000]   countries.
[00:50:23.000 --> 00:50:28.280]   Which among them is best playing the deck they've been dealt?
[00:50:28.280 --> 00:50:33.700]   Who is the most impressive leader adjusting for, you know, it doesn't have to be a huge
[00:50:33.700 --> 00:50:36.400]   country or anything, given the deck they've been dealt, who's best?
[00:50:36.400 --> 00:50:42.380]   You see, one of the things you must never, ever do in politics is say who's your favorite
[00:50:42.380 --> 00:50:49.960]   leader, who's done well, because you will make one friend and many, many enemies.
[00:50:49.960 --> 00:50:59.240]   So I'm just going to answer it in this way, that if you look at the countries that have
[00:50:59.240 --> 00:51:04.040]   succeeded today, if you look, for example, at any country that was third world become
[00:51:04.040 --> 00:51:10.320]   first world, was second world become first world, there are certain things that stand
[00:51:10.320 --> 00:51:14.460]   out and are clear.
[00:51:14.460 --> 00:51:18.700]   Number one, they have stable macroeconomic policy.
[00:51:18.700 --> 00:51:23.460]   Number two, they allow business and enterprise to flourish.
[00:51:23.460 --> 00:51:26.560]   Number three, they have the rule of law.
[00:51:26.560 --> 00:51:29.560]   Number four, they educate their people well.
[00:51:29.560 --> 00:51:34.360]   And wherever you look around the world and you see those things in place, you will find
[00:51:34.360 --> 00:51:35.560]   success.
[00:51:35.560 --> 00:51:41.840]   And whenever you find their absence, you will find either the fact or the possibility of
[00:51:41.840 --> 00:51:42.880]   failure.
[00:51:42.880 --> 00:51:51.480]   The one thing, however, that any country leader should focus on today is the possibility of
[00:51:51.480 --> 00:51:56.280]   all of these rules being rewritten by the importance of technology.
[00:51:56.280 --> 00:52:01.680]   And the single most important thing today, if I was back in the frontline of politics,
[00:52:01.680 --> 00:52:09.200]   would be, as I say, to engage with this revolution, to understand it, to bring in the people into
[00:52:09.200 --> 00:52:16.460]   the discussions and the councils of government who also get it, and to take the key decisions
[00:52:16.460 --> 00:52:21.000]   that will allow us to access the opportunities and mitigate the risks.
[00:52:21.000 --> 00:52:22.000]   That's a wonderful place to close.
[00:52:22.000 --> 00:52:24.400]   Mr. Vallaire, thank you so much for coming on the podcast.
[00:52:24.400 --> 00:52:25.800]   Thank you for having me.
[00:52:25.800 --> 00:52:28.060]   I hope you enjoyed this episode with Tony Blair.
[00:52:28.060 --> 00:52:31.600]   If you did, I appreciate you sharing it with people who you think might enjoy it.
[00:52:31.600 --> 00:52:36.800]   As always, Twitter or group chats, wherever else, is extremely helpful.
[00:52:36.800 --> 00:52:41.560]   And I'm doing ads on the podcast now, so if you're interested in advertising, please reach
[00:52:41.560 --> 00:52:44.440]   the form and link in the description below.
[00:52:44.440 --> 00:52:46.200]   Other than that, I guess I'll see you next time.
[00:52:46.200 --> 00:52:46.700]   Cheers.
[00:52:46.700 --> 00:52:50.060]   [MUSIC PLAYING]
[00:52:50.060 --> 00:52:52.120]   you

