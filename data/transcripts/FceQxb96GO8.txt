
[00:00:00.000 --> 00:00:05.160]   We all saw that GPT-4 is able to create a website from handwriting on a napkin.
[00:00:05.160 --> 00:00:08.940]   But with all the news since, the focus on vision has been lost.
[00:00:08.940 --> 00:00:15.500]   Meanwhile, in the last few hours and days, a select few with full access to multimodal GPT-4
[00:00:15.500 --> 00:00:18.340]   have been releasing snapshots of what it can do.
[00:00:18.340 --> 00:00:21.680]   I want to show you not only what is imminent with GPT-4 vision,
[00:00:21.680 --> 00:00:28.120]   but with releases this week in text to 3D, text inside 3D, speech to text,
[00:00:28.120 --> 00:00:33.020]   and even embodiment, we're going to see how language and visual model innovations
[00:00:33.020 --> 00:00:36.940]   are complementing each other and beginning to snowball.
[00:00:36.940 --> 00:00:38.980]   But let's start with images.
[00:00:38.980 --> 00:00:44.120]   Do you remember from the GPT-4 technical report when the model was able to manipulate,
[00:00:44.120 --> 00:00:47.480]   when prompted, a human into solving captures for it?
[00:00:47.480 --> 00:00:49.460]   Well, that may no longer be needed.
[00:00:49.460 --> 00:00:54.440]   It solves this one pretty easily, so no, captures are not going to slow down GPT-4.
[00:00:54.440 --> 00:00:56.040]   Next, medical imagery.
[00:00:56.240 --> 00:01:01.920]   It was able to interpret this complex image and spot elements of a brain tumor.
[00:01:01.920 --> 00:01:05.900]   Now, it did not spot the full diagnosis, but I want to point something out.
[00:01:05.900 --> 00:01:09.660]   This paper from OpenAI was released only a few days ago,
[00:01:09.660 --> 00:01:12.680]   and it tested GPT-4 on medical questions.
[00:01:12.680 --> 00:01:18.320]   They found that GPT-4 can attain outstanding results exceeding human performance levels,
[00:01:18.320 --> 00:01:20.400]   and that that was without vision.
[00:01:20.400 --> 00:01:23.780]   The images and graphs were not passed to the model.
[00:01:23.780 --> 00:01:26.020]   And as you can see, when the questions did have,
[00:01:26.040 --> 00:01:29.320]   a lot of media in them, it brought down GPT-4's average.
[00:01:29.320 --> 00:01:32.320]   It will be very interesting to see GPT-4's results
[00:01:32.320 --> 00:01:35.600]   when its multimodal capabilities are accounted for.
[00:01:35.600 --> 00:01:40.360]   Next is humor, and I'm not showing these to say that they're necessarily going to change the world,
[00:01:40.360 --> 00:01:43.680]   but it does demonstrate the raw intellect of GPT-4.
[00:01:43.680 --> 00:01:45.760]   To suss out why these images are funny,
[00:01:45.760 --> 00:01:49.080]   you have to have quite a nuanced understanding of humanity.
[00:01:49.080 --> 00:01:52.440]   Let's just say that it probably understood this meme quicker than I did.
[00:01:52.440 --> 00:01:55.680]   Quick thing to point out, by the way, it won't do faces.
[00:01:55.840 --> 00:02:00.400]   For pretty obvious privacy reasons, they won't allow the model to recognize faces.
[00:02:00.400 --> 00:02:03.720]   Whether that ability gets jailbreak, only time will tell.
[00:02:03.720 --> 00:02:07.720]   Meanwhile, it can read menus and interpret the physical world,
[00:02:07.720 --> 00:02:11.080]   which is an amazing asset for visually impaired people.
[00:02:11.080 --> 00:02:17.440]   I want to move on to another fascinating ability that the vision model inside GPT-4 possesses,
[00:02:17.440 --> 00:02:20.400]   and that is reading graphs and text from images.
[00:02:20.400 --> 00:02:25.640]   Its ability to interpret complex diagrams and captions is going to change the world.
[00:02:25.640 --> 00:02:32.840]   Here it is understanding a complex diagram and caption from the Palm e-paper released only about three weeks ago,
[00:02:32.840 --> 00:02:34.600]   which I have done a video on, by the way.
[00:02:34.600 --> 00:02:37.240]   But just how good is it at reading text from an image?
[00:02:37.240 --> 00:02:42.400]   Well, let's take a look at GPT-4's score on the text VQA benchmark.
[00:02:42.400 --> 00:02:45.560]   Now, I've covered quite a few of the other benchmarks in other videos,
[00:02:45.560 --> 00:02:47.360]   but I want to focus on this one here.
[00:02:47.360 --> 00:02:54.240]   Notice how GPT-4 got 78%, which is better than the previous state of the art model, which got 72%.
[00:02:54.240 --> 00:02:55.440]   Now, try to remember that
[00:02:55.440 --> 00:02:56.760]   78% figure.
[00:02:56.760 --> 00:02:59.040]   What exactly is this testing, you ask?
[00:02:59.040 --> 00:03:02.160]   Well, reading text from complex images.
[00:03:02.160 --> 00:03:07.840]   This is the original text VQA academic paper, and you can see some of the sample questions above.
[00:03:07.840 --> 00:03:11.240]   To be honest, if you want to test your own eyesight, you can try them yourself.
[00:03:11.240 --> 00:03:13.440]   So how does the average human perform?
[00:03:13.440 --> 00:03:19.680]   Well, on page seven, we have this table and we get this figure for humans, 85%.
[00:03:19.680 --> 00:03:24.120]   You don't need me to tell you that's just 7% better than GPT-4.
[00:03:24.120 --> 00:03:25.040]   The thing is, though,
[00:03:25.240 --> 00:03:26.880]   these models aren't slowing down.
[00:03:26.880 --> 00:03:29.560]   As the Vision co-lead at OpenAI put it,
[00:03:29.560 --> 00:03:34.000]   "Scale is all you need until everyone else realizes it too."
[00:03:34.000 --> 00:03:37.840]   But the point of this video is to show you that improvements in one area are
[00:03:37.840 --> 00:03:40.720]   starting to bleed into improvements in other areas.
[00:03:40.720 --> 00:03:45.240]   We already saw that an image of bad handwriting could be translated into a website.
[00:03:45.240 --> 00:03:49.000]   As you can see here, even badly written natural language can now be
[00:03:49.000 --> 00:03:54.840]   translated directly into code in Blender, creating detailed 3D models with fascinating physics.
[00:03:55.040 --> 00:04:00.160]   The borders of text, image, 3D and embodiment are beginning to be broken down.
[00:04:00.160 --> 00:04:02.640]   And of course, other companies are jumping in too.
[00:04:02.640 --> 00:04:06.960]   Here's Adobe showing how you can edit 3D images using text.
[00:04:06.960 --> 00:04:12.240]   And how long will it really be before we go direct from text to physical models,
[00:04:12.240 --> 00:04:14.760]   all mediated through natural language?
[00:04:14.760 --> 00:04:19.120]   And it's not just about creating 3D, it's about interacting with it through text.
[00:04:19.120 --> 00:04:24.640]   Notice how we can pick out both text and higher level concepts like objects.
[00:04:24.840 --> 00:04:29.160]   The advanced 3D field was captured using 2D images from a phone.
[00:04:29.160 --> 00:04:31.400]   This paper was released only 10 days ago.
[00:04:31.400 --> 00:04:35.320]   But notice how now we have language embedded inside the model.
[00:04:35.320 --> 00:04:38.280]   We can search and scan for more abstract
[00:04:38.280 --> 00:04:43.680]   concepts like yellow or even utensils or electricity.
[00:04:43.680 --> 00:04:44.840]   It's not perfect.
[00:04:44.840 --> 00:04:48.120]   And for some reason, it really struggled with recognizing Raman.
[00:04:48.120 --> 00:04:54.440]   But it does represent state of the art image into 3D interpreted through text.
[00:04:54.640 --> 00:04:55.720]   And you don't even want to type.
[00:04:55.720 --> 00:04:57.080]   You just want to use your voice.
[00:04:57.080 --> 00:05:02.320]   Just three weeks ago, I did a video on how voice recognition will change everything.
[00:05:02.320 --> 00:05:05.760]   And I was talking about OpenAI's Whisper API.
[00:05:05.760 --> 00:05:09.280]   But now we have Conformer, which is better than Whisper.
[00:05:09.280 --> 00:05:11.000]   Here is the chart to prove it.
[00:05:11.000 --> 00:05:16.920]   And look how Conformer makes fewer errors even than Whisper at recognizing speech.
[00:05:16.920 --> 00:05:20.600]   The cool thing is you can test it for yourself and the link is in the description.
[00:05:20.600 --> 00:05:24.240]   And while you're passing by the description, don't forget to leave a like and a
[00:05:24.440 --> 00:05:26.880]   comment to let me know if you've learned anything from this video.
[00:05:26.880 --> 00:05:29.400]   As you'd expect, I tested it myself and it
[00:05:29.400 --> 00:05:34.400]   did amazingly at transcribing my recent video on GPT-4.
[00:05:34.400 --> 00:05:38.360]   There were only a handful of mistakes in a 12 minute transcript.
[00:05:38.360 --> 00:05:40.880]   At this point, you're probably thinking, what's next?
[00:05:40.880 --> 00:05:45.560]   Well, look at the route sketched out two years ago by Sam Altman.
[00:05:45.560 --> 00:05:50.000]   He said in the next five years, computer programs that can think will read
[00:05:50.000 --> 00:05:54.240]   legal documents and give medical advice. With GPT-4 passing the bar,
[00:05:54.240 --> 00:05:56.400]   I would say so far he's two for two.
[00:05:56.400 --> 00:05:58.200]   He goes on, in the next decade,
[00:05:58.200 --> 00:06:02.480]   they will do assembly line work and maybe even become companions.
[00:06:02.480 --> 00:06:06.080]   He's talking about the physical embodiment of language models.
[00:06:06.080 --> 00:06:10.840]   Back then, OpenAI had a robotics team themselves that could do things like this.
[00:06:10.840 --> 00:06:17.520]   Here is a robotic hand solving a Rubik's Cube despite interruptions from a giraffe
[00:06:17.520 --> 00:06:21.600]   and someone putting a pen to interrupt the model.
[00:06:21.600 --> 00:06:23.840]   It still solved the cube.
[00:06:24.040 --> 00:06:26.240]   But then that team got disbanded and it
[00:06:26.240 --> 00:06:29.040]   seems like they've moved into investing in startups.
[00:06:29.040 --> 00:06:32.800]   They are leading a $23 million investment
[00:06:32.800 --> 00:06:37.280]   in OneX, a startup developing a human-like robot.
[00:06:37.280 --> 00:06:39.600]   Here is the OneX website and it features
[00:06:39.600 --> 00:06:43.720]   this rather startling image and it says Summer 2023.
[00:06:43.720 --> 00:06:47.600]   Our newest Android iteration, Neo, will explore how artificial
[00:06:47.600 --> 00:06:50.240]   intelligence can take form in a human-like body.
[00:06:50.240 --> 00:06:53.720]   Now, of course, for many of you, a humanoid robot won't
[00:06:53.840 --> 00:06:54.840]   be that surprising.
[00:06:54.840 --> 00:07:00.040]   Here is the obligatory clip from Boston Dynamics.
[00:07:00.040 --> 00:07:18.960]   And of course, these models don't have to be humanoid.
[00:07:18.960 --> 00:07:22.440]   Here is a demonstration from a paper published just four days ago.
[00:07:22.440 --> 00:07:23.640]   This is not just walking.
[00:07:23.640 --> 00:07:27.520]   It's climbing up, balancing, pressing and operating buttons.
[00:07:27.520 --> 00:07:30.520]   And before you think all of this is really far away,
[00:07:30.520 --> 00:07:34.240]   these assembly line robots are now commercially available.
[00:07:34.240 --> 00:07:38.240]   I still think there's a long way to go before embodiment becomes mainstream.
[00:07:38.240 --> 00:07:39.520]   But my point is this.
[00:07:39.520 --> 00:07:44.320]   All these improvements that we're seeing in text, audio, 3D and embodiment,
[00:07:44.320 --> 00:07:47.520]   they're starting to merge into each other, complement each other.
[00:07:47.520 --> 00:07:49.520]   On their own, they're cool and a bit nerdy.
[00:07:49.520 --> 00:07:53.240]   But once they start synergizing, fusing together, they could be
[00:07:53.440 --> 00:07:54.320]   revolutionary.
[00:07:54.320 --> 00:07:58.360]   As Sam Altman said on the Lex Friedman podcast released yesterday,
[00:07:58.360 --> 00:08:02.960]   embodiment might not be needed for AGI, but it's coming anyway.
[00:08:02.960 --> 00:08:06.440]   Let me know what you think in the comments and have a wonderful day.

