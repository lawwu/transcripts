
[00:00:00.000 --> 00:00:07.000]   People just don't have an understanding or a grasp of what is happening.
[00:00:07.000 --> 00:00:11.760]   Fundamentally, what are these models trying to do?
[00:00:11.760 --> 00:00:14.520]   And how do they respond to certain things?
[00:00:14.520 --> 00:00:18.660]   This is not anything anyone's ever had experience with before.
[00:00:18.660 --> 00:00:22.360]   So coming in, we're not just teaching them how to use our product.
[00:00:22.360 --> 00:00:29.720]   We're trying to teach them fundamentally, here's even what AI-generated content means.
[00:00:29.720 --> 00:00:34.080]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:34.080 --> 00:00:37.080]   and I'm your host, Lukas Biewald.
[00:00:37.080 --> 00:00:42.840]   This is an interview with Dave Roggenmoser, the CEO of Jasper AI, and Saad Ansari, the
[00:00:42.840 --> 00:00:44.800]   head of AI at Jasper AI.
[00:00:44.800 --> 00:00:50.160]   Jasper is one of the most exciting breakout successes in text generation right now, and
[00:00:50.160 --> 00:00:54.400]   a pioneer in using prompt engineering to build successful business.
[00:00:54.400 --> 00:00:58.480]   This is a really interesting interview, both about entrepreneurship and applied machine
[00:00:58.480 --> 00:01:03.640]   learning, and also technical details around large language models and the future of how
[00:01:03.640 --> 00:01:05.240]   prompt engineering will work.
[00:01:05.240 --> 00:01:07.680]   I learned a lot from this interview, and I hope you enjoy it.
[00:01:07.680 --> 00:01:10.840]   Why don't we start a little differently?
[00:01:10.840 --> 00:01:17.240]   I was thinking this is what I would need as an ML researcher, which is mostly our audience.
[00:01:17.240 --> 00:01:23.520]   Could you explain how a marketer would use Jasper and what they would get out of it?
[00:01:23.520 --> 00:01:26.640]   And maybe even get concrete about what people love so much about it?
[00:01:26.640 --> 00:01:31.080]   Because you mentioned that people have a real palpable excitement about using the product.
[00:01:31.080 --> 00:01:32.960]   Why is that happening?
[00:01:32.960 --> 00:01:41.120]   Yeah, marketers have a lot of content to create, and most of them would create infinite amounts
[00:01:41.120 --> 00:01:42.120]   of it.
[00:01:42.120 --> 00:01:45.120]   So nobody ever has enough blog posts.
[00:01:45.120 --> 00:01:46.120]   Nobody has enough...
[00:01:46.120 --> 00:01:50.720]   I mean, at some level, you probably have enough to add creative, but you run a bunch of tests,
[00:01:50.720 --> 00:01:55.640]   and all of a sudden, a week later, everybody sits down, and they write all this stuff,
[00:01:55.640 --> 00:01:56.640]   and then they test it.
[00:01:56.640 --> 00:01:58.720]   A week later, they're out of things to test.
[00:01:58.720 --> 00:02:03.720]   They go six months without ever testing another headline again for their Facebook ad.
[00:02:03.720 --> 00:02:09.800]   And so the only way to do this has been just through manpower and just trying to hire more
[00:02:09.800 --> 00:02:12.760]   people and dedicate more time to it.
[00:02:12.760 --> 00:02:19.740]   And with marketing, it's such a thing that a little bit better headline can be the difference
[00:02:19.740 --> 00:02:27.880]   between successfully and profitably spending $100 million on ads or spending $3 million
[00:02:27.880 --> 00:02:29.320]   and having to shut the whole thing down.
[00:02:29.320 --> 00:02:32.080]   So a lot of this is pretty thin margins.
[00:02:32.080 --> 00:02:36.960]   It's got a whole campaign working really well and it never getting off the ground at all.
[00:02:36.960 --> 00:02:42.200]   And so, yeah, there's just so much at stake and so much value to add there.
[00:02:42.200 --> 00:02:48.200]   And marketers think highly of themselves, think, "My writing's different," and all of
[00:02:48.200 --> 00:02:49.400]   those things.
[00:02:49.400 --> 00:02:51.420]   And it is for a lot of them.
[00:02:51.420 --> 00:02:57.440]   But I think when they saw Jasper, the fact that it was pretty good, in some cases better
[00:02:57.440 --> 00:03:01.320]   than them, and it could do it in an instant.
[00:03:01.320 --> 00:03:07.080]   And it freed them up to go from the marketer that has to stare at the blank page and do
[00:03:07.080 --> 00:03:11.160]   it themselves to now a little bit more of the managing editor.
[00:03:11.160 --> 00:03:16.400]   Jasper will give me all the raw materials here, and he'll give me the first draft, and
[00:03:16.400 --> 00:03:19.200]   I can pick and choose and assemble and all that stuff.
[00:03:19.200 --> 00:03:21.760]   But it just moves everyone up a level there.
[00:03:21.760 --> 00:03:26.520]   So that's what marketers use Jasper for.
[00:03:26.520 --> 00:03:30.960]   Some of it's just high volume stuff, they just need to create a lot of it.
[00:03:30.960 --> 00:03:38.360]   Some of it is creating great content that's made better than you would already create.
[00:03:38.360 --> 00:03:44.200]   All of it is marketers trying to just get an edge and just get out a little bit better
[00:03:44.200 --> 00:03:47.640]   content, a little bit more content faster.
[00:03:47.640 --> 00:03:54.720]   And I guess you were one of the first companies to really make commercial use of large language
[00:03:54.720 --> 00:03:55.720]   models.
[00:03:55.720 --> 00:03:59.760]   Could you talk about what that process looks like?
[00:03:59.760 --> 00:04:04.880]   How do you make the large language model into something that people are actually willing
[00:04:04.880 --> 00:04:08.800]   to pay real money for?
[00:04:08.800 --> 00:04:16.040]   I think the first thing is you've got to know a customer base and know it really deeply.
[00:04:16.040 --> 00:04:19.880]   And that's, I think, always been our secret.
[00:04:19.880 --> 00:04:22.440]   I just know the customer base super deeply.
[00:04:22.440 --> 00:04:23.760]   I am the customer.
[00:04:23.760 --> 00:04:25.760]   I've sold a lot of stuff to them before.
[00:04:25.760 --> 00:04:29.120]   They're my friends, they're our community.
[00:04:29.120 --> 00:04:34.960]   I'm so in it, and I'm always looking for ways to make their lives easier, make my own life
[00:04:34.960 --> 00:04:35.960]   easier.
[00:04:35.960 --> 00:04:42.680]   I think a lot of people just aren't connected to any sort of end user in any meaningful
[00:04:42.680 --> 00:04:43.680]   way.
[00:04:43.680 --> 00:04:48.680]   I joined the OpenAI Slack community back in the day.
[00:04:48.680 --> 00:04:52.960]   And the day I got to Gritt-Edge, I was like, "I am Thanos here."
[00:04:52.960 --> 00:04:55.520]   I'm like, "This is ultimate power."
[00:04:55.520 --> 00:05:01.280]   And I get in there, and I'm the only one talking about building something that people would
[00:05:01.280 --> 00:05:02.280]   want.
[00:05:02.280 --> 00:05:06.400]   Literally, there's a thousand people in there, and they're all translating the Declaration
[00:05:06.400 --> 00:05:12.240]   of Independence into Elvish, and then back out into album art.
[00:05:12.240 --> 00:05:19.120]   And I'm just like, "This is cool, but literally no one's talking about letting regular people
[00:05:19.120 --> 00:05:20.120]   use this stuff."
[00:05:20.120 --> 00:05:24.680]   And it was confusing to me, still maybe perhaps confusing to me.
[00:05:24.680 --> 00:05:31.240]   I think there's just a huge market for taking this stuff and making it useful, and solving
[00:05:31.240 --> 00:05:33.320]   some specific problem in some way.
[00:05:33.320 --> 00:05:38.560]   But I think that that's where it started for us, was just like, "Hey, we met this tool.
[00:05:38.560 --> 00:05:44.840]   I knew deeply what we wanted, and I felt like I knew the outputs that would get customers
[00:05:44.840 --> 00:05:47.800]   excited and that would get me excited about."
[00:05:47.800 --> 00:05:49.280]   So I think early days, it was just like playing them.
[00:05:49.280 --> 00:05:54.680]   I didn't know anything about prompting, or I'd love to see my first prompts, and they
[00:05:54.680 --> 00:05:55.680]   were just probably nothing.
[00:05:55.680 --> 00:06:01.080]   And I just dove in for a week and just started really crafting these things one by one, and
[00:06:01.080 --> 00:06:03.360]   messing around with it all.
[00:06:03.360 --> 00:06:06.560]   I'm sure Sod has replaced everything I've done at this point now.
[00:06:06.560 --> 00:06:09.160]   But early days, it was just a lot of testing out.
[00:06:09.160 --> 00:06:15.520]   I think everyone was learning what to do there, but really, I just kept coming back to the
[00:06:15.520 --> 00:06:16.520]   customer.
[00:06:16.520 --> 00:06:18.240]   I care about all the settings and all the prompts.
[00:06:18.240 --> 00:06:22.240]   All that is just a byproduct of we've got this problem that needs solving.
[00:06:22.240 --> 00:06:26.320]   And if this tool can solve it, then let's try it.
[00:06:26.320 --> 00:06:30.960]   And is the problem somehow more specific than just, "Write me some content on this topic"?
[00:06:30.960 --> 00:06:32.240]   How do you think about that?
[00:06:32.240 --> 00:06:38.160]   Again, I'm not a marketer, so walk me through what they're thinking.
[00:06:38.160 --> 00:06:44.200]   Or even maybe, could you point me to a specific marketer that works with you and tell me how
[00:06:44.200 --> 00:06:47.080]   they think about the content that they get?
[00:06:47.080 --> 00:06:52.440]   Let's use blog posts, probably our most popular use case.
[00:06:52.440 --> 00:06:58.600]   You search for something on Google, and there's only going to be 10 results that pop up on
[00:06:58.600 --> 00:07:00.680]   the first page.
[00:07:00.680 --> 00:07:07.800]   Your blog post that you write has to be better than quite literally millions of blog posts
[00:07:07.800 --> 00:07:10.280]   that could surface and be somewhat there.
[00:07:10.280 --> 00:07:14.200]   So it's like, you're going to be in the top 10, but then even past that, you really got
[00:07:14.200 --> 00:07:17.160]   to be the top two.
[00:07:17.160 --> 00:07:22.360]   This is a pretty big fall off, even if you scroll past the top two or three there.
[00:07:22.360 --> 00:07:29.960]   So it's not just about getting a blog post out, which is maybe what earlier models could
[00:07:29.960 --> 00:07:30.960]   do.
[00:07:30.960 --> 00:07:32.680]   It's about getting a really good blog post out.
[00:07:32.680 --> 00:07:36.200]   And it's really got to win in this marketplace of ideas.
[00:07:36.200 --> 00:07:37.200]   It's got to be compelling.
[00:07:37.200 --> 00:07:38.440]   It's got to be engaging.
[00:07:38.440 --> 00:07:39.600]   It's got to be factual.
[00:07:39.600 --> 00:07:40.760]   It's got to be helpful.
[00:07:40.760 --> 00:07:43.360]   It's got to be written by somebody that deeply knows the audience.
[00:07:43.360 --> 00:07:50.400]   And so just clicking "compose" on some large language model, it's not going to be enough.
[00:07:50.400 --> 00:07:52.760]   It's not going to be enough to win, particularly if other people have that.
[00:07:52.760 --> 00:07:57.680]   And so I think our customers, they want really high-quality content.
[00:07:57.680 --> 00:08:02.640]   And I always challenge them, I'm like, "Don't even bother writing low-quality content.
[00:08:02.640 --> 00:08:03.920]   This isn't some article spinner."
[00:08:03.920 --> 00:08:07.320]   One, I just don't feel good about building that kind of product.
[00:08:07.320 --> 00:08:10.360]   But two, it's just not going to work for marketers.
[00:08:10.360 --> 00:08:11.800]   It's just not going to get clicks.
[00:08:11.800 --> 00:08:13.240]   It's not going to rank on Google.
[00:08:13.240 --> 00:08:16.080]   It's not going to get people excited when they come to your landing page if it's just
[00:08:16.080 --> 00:08:17.840]   kind of filler stuff.
[00:08:17.840 --> 00:08:22.920]   And so I think from the beginning, we've always said, "Hey, we're here for high-quality content."
[00:08:22.920 --> 00:08:27.080]   And to the degree that we can help people produce that, we will.
[00:08:27.080 --> 00:08:32.120]   That's going to be a big part of our focus, as opposed to just an article spinner that
[00:08:32.120 --> 00:08:35.120]   just spins out tons and tons of stuff.
[00:08:35.120 --> 00:08:40.640]   It's just not going to stand up and actually produce the ROI that marketers want.
[00:08:40.640 --> 00:08:47.680]   I guess my experience of just working with GPT-3 is it's an impressive product for sure,
[00:08:47.680 --> 00:08:53.680]   but I don't think I get what I would consider high-quality blog posts out of it when I just
[00:08:53.680 --> 00:08:56.240]   sort of mess around with it.
[00:08:56.240 --> 00:09:00.920]   Can you talk about how you actually got it to deliver high-quality content?
[00:09:00.920 --> 00:09:03.120]   Is there kind of a human in the loop here that's tweaking it?
[00:09:03.120 --> 00:09:05.720]   Or what's the process like?
[00:09:05.720 --> 00:09:10.760]   Yeah, well, you certainly, even in Jasper, can't just go and click a button and get a
[00:09:10.760 --> 00:09:12.400]   high-quality blog post out now.
[00:09:12.400 --> 00:09:14.080]   So we really talk a lot about that.
[00:09:14.080 --> 00:09:15.760]   It's like, "Hey, it's a dance."
[00:09:15.760 --> 00:09:19.680]   It's like Jasper's there to help give you...
[00:09:19.680 --> 00:09:22.840]   I might start with blog post titles or blog post topics.
[00:09:22.840 --> 00:09:26.720]   "Hey, Jasper, give me 10 blog post ideas around this topic."
[00:09:26.720 --> 00:09:29.000]   "Okay, that's a pretty good one.
[00:09:29.000 --> 00:09:31.280]   That helps me start off in a better spot."
[00:09:31.280 --> 00:09:34.840]   "Jasper, give me 10 titles about that topic."
[00:09:34.840 --> 00:09:35.840]   "Okay, cool.
[00:09:35.840 --> 00:09:36.840]   That's pretty cool."
[00:09:36.840 --> 00:09:40.160]   "Jasper, give me an outline that I can start to work off of here."
[00:09:40.160 --> 00:09:42.160]   "That first one stinks."
[00:09:42.160 --> 00:09:44.000]   "Okay, give me three more outlines."
[00:09:44.000 --> 00:09:49.600]   So you're basically going back and forth with Jasper to help assemble it.
[00:09:49.600 --> 00:09:52.800]   If you don't know what a good blog post is, you're going to be in trouble.
[00:09:52.800 --> 00:09:56.520]   If you don't know what your reader might...
[00:09:56.520 --> 00:10:00.120]   If you don't really know what your reader wants, you're going to be in trouble because
[00:10:00.120 --> 00:10:02.880]   Jasper is not going to know all that.
[00:10:02.880 --> 00:10:05.920]   But I think Jasper does do a great job of...
[00:10:05.920 --> 00:10:11.120]   If you are able to direct it, you're able to help piece that together, you can assemble
[00:10:11.120 --> 00:10:13.800]   a really great blog post.
[00:10:13.800 --> 00:10:18.720]   Some of it will be you, some of it will be steering the output there.
[00:10:18.720 --> 00:10:22.120]   But it's just using a variety of different tools to do that.
[00:10:22.120 --> 00:10:27.000]   And you can get some really, really, really high-quality content that's really remarkable
[00:10:27.000 --> 00:10:29.960]   and that readers really want.
[00:10:29.960 --> 00:10:33.080]   But it is going to take a human in the loop doing that.
[00:10:33.080 --> 00:10:34.240]   So it's going to happen in there.
[00:10:34.240 --> 00:10:39.720]   And then our team in the background is testing out all different models that produce blog
[00:10:39.720 --> 00:10:40.720]   posts best.
[00:10:40.720 --> 00:10:44.840]   And there's all different prompts that produce different types of blog posts.
[00:10:44.840 --> 00:10:49.480]   And should we have one tool that creates a general blog post?
[00:10:49.480 --> 00:10:52.800]   Or are there actually five types of blog posts?
[00:10:52.800 --> 00:10:57.320]   And we need five different models, each one a little bit more specific to a listicle or
[00:10:57.320 --> 00:11:00.440]   informational blog post or whatever.
[00:11:00.440 --> 00:11:06.480]   I think we're trying to do all of that behind the scenes and simplify that for the user.
[00:11:06.480 --> 00:11:11.560]   And just turn it into this magical experience that they can just show up and start getting
[00:11:11.560 --> 00:11:12.560]   to work.
[00:11:12.560 --> 00:11:16.760]   And our goal is that the software would become invisible.
[00:11:16.760 --> 00:11:18.960]   And you were the first person doing the prompt engineering.
[00:11:18.960 --> 00:11:19.960]   Is that right?
[00:11:19.960 --> 00:11:20.960]   It was me.
[00:11:20.960 --> 00:11:21.960]   So what did you learn?
[00:11:21.960 --> 00:11:24.960]   Teach me how to do prompt engineering.
[00:11:24.960 --> 00:11:29.000]   What are the first couple of things that you figured out when you were just messing around
[00:11:29.000 --> 00:11:30.000]   with it?
[00:11:30.000 --> 00:11:32.040]   Oh man, I just didn't know anything.
[00:11:32.040 --> 00:11:40.200]   At first, I was probably treating it like these instruct models where it's like, "Write
[00:11:40.200 --> 00:11:41.200]   a blog post for me."
[00:11:41.200 --> 00:11:42.520]   And it's just like, "Write a blog post for me.
[00:11:42.520 --> 00:11:43.520]   Write a blog post for me.
[00:11:43.520 --> 00:11:44.520]   Write a blog post for me.
[00:11:44.520 --> 00:11:45.520]   Write a blog post for me."
[00:11:45.520 --> 00:11:46.520]   Okay, cool.
[00:11:46.520 --> 00:11:47.520]   What's happening here?
[00:11:47.520 --> 00:11:51.080]   And there's patterns and it's trying to figure out what I want and all of that.
[00:11:51.080 --> 00:11:56.800]   I think particularly early days, we still get a lot of benefit from this, is the examples
[00:11:56.800 --> 00:12:04.520]   that we would give it for a few shot outputs really were important.
[00:12:04.520 --> 00:12:08.760]   And I felt like a lot of our competitors were marketers.
[00:12:08.760 --> 00:12:10.680]   So we're just probably sticking...
[00:12:10.680 --> 00:12:15.720]   I don't really know what any of them were doing, but we're just sticking whatever decent
[00:12:15.720 --> 00:12:17.680]   examples in there.
[00:12:17.680 --> 00:12:23.480]   And I had a really high bar and I was able to use examples that I knew for a fact converted
[00:12:23.480 --> 00:12:24.480]   really well on Facebook.
[00:12:24.480 --> 00:12:27.280]   I knew for a fact performed really well there.
[00:12:27.280 --> 00:12:31.400]   And so we just always use stuff that was proven in the market to start to steer and give examples
[00:12:31.400 --> 00:12:32.400]   there.
[00:12:32.400 --> 00:12:37.960]   And so we'd get really, I'd say highly opinionated, but really high quality outputs out of it
[00:12:37.960 --> 00:12:38.960]   there.
[00:12:38.960 --> 00:12:42.320]   But yeah, it was just me reading every doc I could.
[00:12:42.320 --> 00:12:45.360]   There weren't that many docs back then and just talking to everybody and like, "What's
[00:12:45.360 --> 00:12:47.960]   top P mean?"
[00:12:47.960 --> 00:12:51.100]   All of these things, I just had no idea.
[00:12:51.100 --> 00:12:56.960]   But I knew the output I was trying to get to and I wouldn't stop until it was like,
[00:12:56.960 --> 00:12:58.240]   "Man, that is really good.
[00:12:58.240 --> 00:12:59.240]   I would really use that."
[00:12:59.240 --> 00:13:05.120]   And I think that's probably what's harder, I suppose, than figuring out what top P really
[00:13:05.120 --> 00:13:07.280]   does.
[00:13:07.280 --> 00:13:08.280]   What does top P do?
[00:13:08.280 --> 00:13:09.280]   What is that?
[00:13:09.280 --> 00:13:14.800]   Oh gosh, I was just hoping you weren't going to ask me what that means.
[00:13:14.800 --> 00:13:15.800]   We got people here now.
[00:13:15.800 --> 00:13:18.480]   We got Saad that can do all that stuff.
[00:13:18.480 --> 00:13:23.520]   But if you're listening, let my ignorance on top P go to show you where I think the
[00:13:23.520 --> 00:13:25.280]   real value lies here.
[00:13:25.280 --> 00:13:27.560]   It's outside.
[00:13:27.560 --> 00:13:32.600]   Or in addition to knowing what all the little things do, it's like, what is it useful for?
[00:13:32.600 --> 00:13:35.200]   And where does it really play a role in society?
[00:13:35.200 --> 00:13:39.880]   Well, so when you think about hiring a prompt engineer, what do you look for?
[00:13:39.880 --> 00:13:44.480]   Is it domain expertise in marketing and content?
[00:13:44.480 --> 00:13:49.400]   And I guess what else would you ask me if I showed up interviewing for being a prompt
[00:13:49.400 --> 00:13:50.400]   engineer?
[00:13:50.400 --> 00:14:00.760]   Yeah, I think for us, I look first and foremost for an understanding of the customer or at
[00:14:00.760 --> 00:14:07.720]   least a willingness and desire to understand the customer and an empathy for the customer.
[00:14:07.720 --> 00:14:14.560]   We have people that apply that really want to do AI.
[00:14:14.560 --> 00:14:15.560]   I really want to do AI.
[00:14:15.560 --> 00:14:19.320]   I don't want to be a cutting edge company and we want to generate AI so cool and all
[00:14:19.320 --> 00:14:20.320]   that.
[00:14:20.320 --> 00:14:21.320]   That's fine.
[00:14:21.320 --> 00:14:25.000]   But if it's just that, you're going to struggle here.
[00:14:25.000 --> 00:14:28.640]   I really want to use AI.
[00:14:28.640 --> 00:14:30.160]   I love the customer base.
[00:14:30.160 --> 00:14:31.400]   I can see the problem.
[00:14:31.400 --> 00:14:34.000]   I can articulate the problem that we're working on here.
[00:14:34.000 --> 00:14:36.720]   And man, AI happens to be a really great fit there.
[00:14:36.720 --> 00:14:40.720]   And I'm excited to find what else really helps solve that problem.
[00:14:40.720 --> 00:14:47.640]   That's really what I'm looking for as opposed to just, I just want to do AI.
[00:14:47.640 --> 00:14:49.800]   And so I think about a prompt engineer.
[00:14:49.800 --> 00:14:57.120]   We got away for a long time without anyone that really knew AI, I would say.
[00:14:57.120 --> 00:15:01.280]   We were technical and we could hack it and we were fine tuning models.
[00:15:01.280 --> 00:15:06.480]   We were still doing some fancy stuff, but it wasn't like we're doing anything that would
[00:15:06.480 --> 00:15:10.000]   blow anyone's minds.
[00:15:10.000 --> 00:15:11.240]   But it still just kind of worked.
[00:15:11.240 --> 00:15:16.520]   And so I really, again, going back to that, I think generating prompts and working on
[00:15:16.520 --> 00:15:21.040]   that, starting with a deep understanding of the customer will get you so, so far.
[00:15:21.040 --> 00:15:26.680]   Well, I mean, I think you're kind of the poster child for prompt engineering.
[00:15:26.680 --> 00:15:32.360]   And certainly some people think that machine learning kind of goes away or takes a back
[00:15:32.360 --> 00:15:34.480]   seat in the sense of training models.
[00:15:34.480 --> 00:15:39.360]   And there's sort of this new role of prompt engineer that kind of uses these models for
[00:15:39.360 --> 00:15:42.480]   some purpose.
[00:15:42.480 --> 00:15:46.760]   My background is in machine learning, but I'm certainly not, I'm open-minded to different
[00:15:46.760 --> 00:15:48.720]   paths that the industry could take.
[00:15:48.720 --> 00:15:56.160]   Do you think that in your world, machine learning, technical ability even matters at all?
[00:15:56.160 --> 00:16:01.520]   Do you try to hire machine learning people themselves?
[00:16:01.520 --> 00:16:03.560]   I think it does matter.
[00:16:03.560 --> 00:16:08.320]   I think a lot of this stuff over a long enough time horizon gets commoditized and it matters.
[00:16:08.320 --> 00:16:10.160]   I don't know if less.
[00:16:10.160 --> 00:16:15.280]   The thing that used to matter technically are probably kind of solved much more now.
[00:16:15.280 --> 00:16:23.480]   And so now, I mean, we've got a really strong internal AI team that's full of really smart,
[00:16:23.480 --> 00:16:26.480]   what I would call the AI people.
[00:16:26.480 --> 00:16:29.080]   Assad's putting together an awesome team there.
[00:16:29.080 --> 00:16:33.800]   And so we want to have a ton of those people that can just help us in develop modes and
[00:16:33.800 --> 00:16:39.920]   develop IP and again, solve customers' problems in a deep way.
[00:16:39.920 --> 00:16:41.240]   And so I think it does matter.
[00:16:41.240 --> 00:16:42.600]   I think we'll always have that.
[00:16:42.600 --> 00:16:50.720]   I want to have a 500-person team full of ML engineers and AI people doing all of that.
[00:16:50.720 --> 00:16:55.160]   But I don't want to be driven by that.
[00:16:55.160 --> 00:16:59.720]   That's always a by-product of the thing that we're trying to do.
[00:16:59.720 --> 00:17:05.320]   And if we can solve customers' problems using all that stuff, then we're so much better
[00:17:05.320 --> 00:17:06.320]   off for it.
[00:17:06.320 --> 00:17:07.320]   It definitely gives us an advantage.
[00:17:07.320 --> 00:17:12.360]   I don't know, Assad, do you have any thoughts on there you want to add in?
[00:17:12.360 --> 00:17:13.360]   I totally agree.
[00:17:13.360 --> 00:17:19.000]   Essentially, what we're asking is the customer wants something and the customer has a vision
[00:17:19.000 --> 00:17:21.760]   maybe or maybe they're trying to discover a new idea.
[00:17:21.760 --> 00:17:24.560]   And then there's this ideal output out there somewhere that would make them happy and delight
[00:17:24.560 --> 00:17:27.040]   them and give them a lot of value.
[00:17:27.040 --> 00:17:32.360]   Between their input and then the ideal output for them, there's choosing the right AI system.
[00:17:32.360 --> 00:17:33.720]   And it's not necessarily one model.
[00:17:33.720 --> 00:17:34.840]   It could be a number of models.
[00:17:34.840 --> 00:17:37.280]   And I think that's where the AI team plays a role.
[00:17:37.280 --> 00:17:38.280]   What is the right system?
[00:17:38.280 --> 00:17:39.280]   What is the right base?
[00:17:39.280 --> 00:17:40.520]   And I think you're right as well.
[00:17:40.520 --> 00:17:43.440]   Prompt engineering is going to play a huge role.
[00:17:43.440 --> 00:17:46.640]   And as Dave said, Dave's a perfect prompt engineer and is somebody who loves the customer
[00:17:46.640 --> 00:17:55.280]   and is willing to iterate through n number of cycles to find the right prompt.
[00:17:55.280 --> 00:17:56.960]   Just an interesting point there as well.
[00:17:56.960 --> 00:17:57.960]   There's this idea of expertise.
[00:17:57.960 --> 00:18:01.320]   An expert is somebody who's learned something and knows it from experience.
[00:18:01.320 --> 00:18:04.560]   I think one of the really interesting and fun things about a lot of these models is
[00:18:04.560 --> 00:18:07.600]   that even the people who made them are experts on them.
[00:18:07.600 --> 00:18:10.320]   It often happens like an R&D center will come out and give us a model and we have to test
[00:18:10.320 --> 00:18:12.800]   it and they'll tell us all these things about the model.
[00:18:12.800 --> 00:18:17.000]   And then within a few minutes of us testing it, we've already falsified a lot of those
[00:18:17.000 --> 00:18:18.000]   assumptions.
[00:18:18.000 --> 00:18:20.240]   Nobody's an expert in prompt engineering.
[00:18:20.240 --> 00:18:23.800]   It just takes a love of the end use and the customer and the product and just willing
[00:18:23.800 --> 00:18:27.040]   to be patient with it.
[00:18:27.040 --> 00:18:31.280]   I guess one of the things that seems like it might be hard in putting myself in your
[00:18:31.280 --> 00:18:37.400]   shoes is actually quantifying if your models are improving over time.
[00:18:37.400 --> 00:18:43.080]   Is there some way that you know, even Dave, that as you iterate, that they're better besides
[00:18:43.080 --> 00:18:48.560]   just sort of eyeballing the content that's getting produced?
[00:18:48.560 --> 00:18:52.720]   Early days, it was eyeballing.
[00:18:52.720 --> 00:18:57.560]   Sometimes we'd in Slack just pop in two screenshots, "Hey, everybody vote on one of these.
[00:18:57.560 --> 00:19:00.480]   Which one do you think is better?"
[00:19:00.480 --> 00:19:04.000]   All this had happened in my head a lot where it's like, I would just keep cycling until
[00:19:04.000 --> 00:19:06.960]   I could feel it getting better just from my own expertise there.
[00:19:06.960 --> 00:19:07.960]   It was nothing scientific.
[00:19:07.960 --> 00:19:10.720]   And even a lot of stuff we didn't test.
[00:19:10.720 --> 00:19:17.280]   Once I found the right setting on something, I probably wouldn't even test around and find
[00:19:17.280 --> 00:19:18.280]   the optimal one.
[00:19:18.280 --> 00:19:22.560]   It was just locally pretty good.
[00:19:22.560 --> 00:19:24.520]   And then yeah, we'd release it to customers.
[00:19:24.520 --> 00:19:31.600]   And I think anecdotally, they'd share feedback, whether stuff was getting better or worse.
[00:19:31.600 --> 00:19:36.480]   We sort of track things like, are they favoriting it?
[00:19:36.480 --> 00:19:38.400]   Are they copying it to their clipboard?
[00:19:38.400 --> 00:19:44.320]   It was a signal that was even maybe stronger than favoriting it.
[00:19:44.320 --> 00:19:46.880]   And then are they kind of using it other places in the product?
[00:19:46.880 --> 00:19:50.000]   We sort of track those as real signals there.
[00:19:50.000 --> 00:19:52.960]   It is funny, especially early days.
[00:19:52.960 --> 00:19:55.040]   This probably happens now.
[00:19:55.040 --> 00:20:02.320]   But a lot of people's perception can really sway a whole community.
[00:20:02.320 --> 00:20:06.200]   And so we'd have somebody complain about a template.
[00:20:06.200 --> 00:20:10.520]   And they'd say, "Something changed in the last five hours.
[00:20:10.520 --> 00:20:11.520]   It's totally different.
[00:20:11.520 --> 00:20:12.520]   It's way worse.
[00:20:12.520 --> 00:20:13.520]   Please revert it.
[00:20:13.520 --> 00:20:14.520]   Davis is getting worse."
[00:20:14.520 --> 00:20:15.520]   So you'd have a bunch of people like, "Yeah, something changed.
[00:20:15.520 --> 00:20:16.520]   Something changed."
[00:20:16.520 --> 00:20:19.760]   To the best of my knowledge, nothing had changed.
[00:20:19.760 --> 00:20:20.760]   Nobody was touching that.
[00:20:20.760 --> 00:20:21.760]   Nothing shipped.
[00:20:21.760 --> 00:20:22.760]   Nothing happened.
[00:20:22.760 --> 00:20:29.000]   But people just kind of pile on that as a way to highlight maybe their frustrations with
[00:20:29.000 --> 00:20:30.000]   it.
[00:20:30.000 --> 00:20:31.000]   And the same thing reversed.
[00:20:31.000 --> 00:20:34.680]   People would be like, "Hey, anybody notice that the paragraph generator is way better
[00:20:34.680 --> 00:20:35.680]   now?"
[00:20:35.680 --> 00:20:39.120]   And again, nothing had happened since before.
[00:20:39.120 --> 00:20:41.160]   They're all just people kind of piling on, "It's so much better.
[00:20:41.160 --> 00:20:42.160]   I love it.
[00:20:42.160 --> 00:20:43.160]   Thank you for all you guys do."
[00:20:43.160 --> 00:20:49.800]   And so I think companies can get a lot of mileage out of frequent improvements and having
[00:20:49.800 --> 00:20:54.400]   a culture of improving because everyone just kind of always assumes everything is improving
[00:20:54.400 --> 00:20:55.400]   all over the place.
[00:20:55.400 --> 00:20:58.280]   You get the benefit of the doubt over things you haven't even touched.
[00:20:58.280 --> 00:21:02.320]   Like man, everything's just getting so much better all the time because they've come to
[00:21:02.320 --> 00:21:06.040]   see that that's a general theme in our company.
[00:21:06.040 --> 00:21:09.560]   And then if we kind of slowed down and stopped, again, I think it atrophies.
[00:21:09.560 --> 00:21:11.320]   We are like the customer trust that erodes.
[00:21:11.320 --> 00:21:14.000]   They start thinking more stuff is worse than it really is there.
[00:21:14.000 --> 00:21:20.280]   So it is hard to quantify, but a lot of it is just customers sometimes just kind of feeling
[00:21:20.280 --> 00:21:23.400]   like things are getting better and they're being heard and they're seeing improvements
[00:21:23.400 --> 00:21:26.720]   there and they'll give you a ton of benefit of the doubt from that.
[00:21:26.720 --> 00:21:30.360]   Do you feel like you benefit from improvements in GPT-3?
[00:21:30.360 --> 00:21:32.520]   I've heard different things from other people.
[00:21:32.520 --> 00:21:36.720]   Some people seem to feel like GPT-3 will launch a new thing and it'll kind of break all the
[00:21:36.720 --> 00:21:37.720]   prompts.
[00:21:37.720 --> 00:21:43.040]   Other people tell me that it's actually much better than it used to be.
[00:21:43.040 --> 00:21:44.320]   What's your experience like?
[00:21:44.320 --> 00:21:46.520]   Yeah, I think we get benefits.
[00:21:46.520 --> 00:21:52.800]   And even I tell our team, "Whatever, let's do a lot of our own stuff and have our own
[00:21:52.800 --> 00:21:53.800]   IP."
[00:21:53.800 --> 00:21:59.160]   But if OpenAI is going to just do all this free work and then just push it to some API
[00:21:59.160 --> 00:22:03.880]   point and then now you've got all this new functionality that takes us 20 minutes to
[00:22:03.880 --> 00:22:06.200]   test it and implement it, let's use that.
[00:22:06.200 --> 00:22:10.720]   Let's always be sure that we're testing all this new stuff that they've got 200 people
[00:22:10.720 --> 00:22:15.720]   building for us and not just rely on our own stuff.
[00:22:15.720 --> 00:22:17.440]   And so it's hit or miss.
[00:22:17.440 --> 00:22:20.880]   It's not everything they roll out is better.
[00:22:20.880 --> 00:22:26.080]   We A/B test, I think, pretty much every new model and update that they make.
[00:22:26.080 --> 00:22:30.040]   And we'll fine tune our own models and it's definitely hit or miss whether those are even
[00:22:30.040 --> 00:22:31.040]   better.
[00:22:31.040 --> 00:22:35.800]   And so, yeah, even if you've got some of the best people in the world working on this and
[00:22:35.800 --> 00:22:39.040]   they'll be super excited about some model we'll test, it'll be like, "This is actually
[00:22:39.040 --> 00:22:41.640]   performing worse across the board.
[00:22:41.640 --> 00:22:43.200]   We're not going to roll this thing out there."
[00:22:43.200 --> 00:22:49.400]   So it is interesting how that all works, but we definitely try to use all the stuff that
[00:22:49.400 --> 00:22:51.480]   they release.
[00:22:51.480 --> 00:22:53.840]   Do you find fine tuning useful?
[00:22:53.840 --> 00:22:58.800]   Again, some people say that Prompt Engineering makes fine tuning obsolete.
[00:22:58.800 --> 00:23:00.560]   What's your opinion right now?
[00:23:00.560 --> 00:23:03.440]   It's November 7th, 2022.
[00:23:03.440 --> 00:23:04.440]   What's the take?
[00:23:04.440 --> 00:23:05.440]   Yeah, yeah.
[00:23:05.440 --> 00:23:06.760]   It's going to be obsolete November 8th.
[00:23:06.760 --> 00:23:13.440]   I mean, yeah, generally we find it helpful.
[00:23:13.440 --> 00:23:20.000]   I think a lot of what I worry about in this space or try to allocate, it's allocating
[00:23:20.000 --> 00:23:23.640]   resources correctly here where you're not doing all of this work that then just gets
[00:23:23.640 --> 00:23:25.840]   obsolete out of the blue.
[00:23:25.840 --> 00:23:29.480]   It's like, "Oh, we just spent all this time fine tuning all these models and oh, this
[00:23:29.480 --> 00:23:34.520]   new model makes fine tuning obsolete."
[00:23:34.520 --> 00:23:38.520]   That is so much of what we're trying to do is just figure out where's our space, where's
[00:23:38.520 --> 00:23:41.760]   our special secret sauce that we can go and implement there.
[00:23:41.760 --> 00:23:45.040]   So yeah, we've got different fine tune models running.
[00:23:45.040 --> 00:23:50.480]   But even as I say that, I'm thinking like, "Man, I don't know when the last time we tested
[00:23:50.480 --> 00:23:54.760]   just a new model that might very well be better than an old fine tune model."
[00:23:54.760 --> 00:23:57.000]   Because this stuff changes so fast.
[00:23:57.000 --> 00:24:01.200]   I think a lot of what we think through is just being sure we're always going back to
[00:24:01.200 --> 00:24:06.440]   the whole system and updating with new stuff and testing it with new stuff.
[00:24:06.440 --> 00:24:07.440]   Okay.
[00:24:07.440 --> 00:24:10.400]   Another question that's probably not going to age well, but I'm curious your current
[00:24:10.400 --> 00:24:15.320]   take as far as you can tell me is how you think about the different LLMs out there.
[00:24:15.320 --> 00:24:21.640]   I think you're famously using GPT-3, but I'm sure you've tried Bloom and maybe other ones.
[00:24:21.640 --> 00:24:26.440]   What do you think about all the LLMs out there?
[00:24:26.440 --> 00:24:32.680]   Yeah, we use GPT-3 primarily and not exclusively.
[00:24:32.680 --> 00:24:36.920]   We've got other stuff going on and we're always testing new stuff there.
[00:24:36.920 --> 00:24:42.240]   It's funny, I think I get down and it's almost like an Android-Apple conversation.
[00:24:42.240 --> 00:24:48.480]   You get down into the weeds and people that really are the know, "Oh yeah, GPT-3, not
[00:24:48.480 --> 00:24:50.360]   even top five anymore."
[00:24:50.360 --> 00:24:53.120]   And after people say stuff like, "Dad, this one's better, this one's better, this one's
[00:24:53.120 --> 00:24:54.120]   better."
[00:24:54.120 --> 00:24:58.880]   And I'm like, "I just don't see that bubbling up to being a more user-friendly model or
[00:24:58.880 --> 00:25:00.200]   really doing things."
[00:25:00.200 --> 00:25:07.640]   But I still feel like GPT-3, from my perspective, is generally far better than a lot that's
[00:25:07.640 --> 00:25:08.640]   out there.
[00:25:08.640 --> 00:25:10.000]   I don't doubt that.
[00:25:10.000 --> 00:25:11.800]   And we've seen this ourselves too.
[00:25:11.800 --> 00:25:14.560]   There's things that can do specific things better.
[00:25:14.560 --> 00:25:20.400]   I think by and large, GPT-3 still reigns supreme in my mind.
[00:25:20.400 --> 00:25:27.760]   And most people producing high-quality content are using that primarily.
[00:25:27.760 --> 00:25:35.800]   I'm always asking people this question, "What else is out there?
[00:25:35.800 --> 00:25:36.800]   What are you seeing?"
[00:25:36.800 --> 00:25:43.360]   And I think a lot of people just really know, "Ah, GPT-3 is pretty dang good."
[00:25:43.360 --> 00:25:44.960]   This is almost like a puzzle of three black boxes.
[00:25:44.960 --> 00:25:47.000]   You have the black box of what does the customer want?
[00:25:47.000 --> 00:25:48.840]   The black box of what do the models do?
[00:25:48.840 --> 00:25:51.120]   And then the black box of everything in the middle and what are we going to do about all
[00:25:51.120 --> 00:25:52.960]   that stuff?
[00:25:52.960 --> 00:25:56.480]   I think customers want different things for different use cases.
[00:25:56.480 --> 00:26:00.160]   For blog posts, maybe they want to have something that's more semantically complex.
[00:26:00.160 --> 00:26:01.160]   The language is richer.
[00:26:01.160 --> 00:26:03.960]   If they're doing product descriptions, maybe they want to make sure the facts are preserved
[00:26:03.960 --> 00:26:12.360]   and it's more domain-specific and that it's able to speak to and sustain the data and
[00:26:12.360 --> 00:26:14.440]   the specifications that was in their initial product descriptions.
[00:26:14.440 --> 00:26:17.400]   What we're finding is that different models have trade-offs.
[00:26:17.400 --> 00:26:22.360]   So typically, when you increase in semantic complexity, the same processes which get you
[00:26:22.360 --> 00:26:27.640]   that also get you to break down facts and it gets better at representing what looks
[00:26:27.640 --> 00:26:29.960]   like facts, but it might actually be aligned way better.
[00:26:29.960 --> 00:26:34.400]   And so you can't even tell it's representing facts in a false way.
[00:26:34.400 --> 00:26:38.840]   Similarly, these models are a little bit more literal, but they're not able to be very semantically
[00:26:38.840 --> 00:26:40.400]   complex or speak like Shakespeare.
[00:26:40.400 --> 00:26:45.480]   GPT-3 is great, but it wasn't trained on a lot of foreign languages, whereas Bloom was.
[00:26:45.480 --> 00:26:50.200]   And so I think ultimately it's about what does the customer want for that specific use
[00:26:50.200 --> 00:26:52.680]   case and then what is the best way to get there?
[00:26:52.680 --> 00:26:53.680]   What are the approaches and processes?
[00:26:53.680 --> 00:26:55.560]   And it's not necessarily one model all the time.
[00:26:55.560 --> 00:26:59.160]   Maybe it can be a combination of models, like an adapter model with a base model.
[00:26:59.160 --> 00:27:01.520]   And then how do you tie together those initial models?
[00:27:01.520 --> 00:27:06.040]   It's almost like a menu of options to get the best effect, the best output for the highest
[00:27:06.040 --> 00:27:07.040]   efficiency.
[00:27:07.040 --> 00:27:09.560]   So I think it's more of a puzzle piece.
[00:27:09.560 --> 00:27:11.800]   You'll always have these three variables you're dealing with.
[00:27:11.800 --> 00:27:12.800]   Well, sure.
[00:27:12.800 --> 00:27:23.040]   But okay, at this moment, is there another model that you genuinely use or is there a
[00:27:23.040 --> 00:27:26.680]   number two that you would point to other than GPT-3?
[00:27:26.680 --> 00:27:31.760]   P5 is really interesting for some of its instruct capabilities and its ability to be fine-tuned
[00:27:31.760 --> 00:27:33.280]   for very specific things.
[00:27:33.280 --> 00:27:38.480]   And indeed, you could say there's a hypothesis that a really good AI architecture will always
[00:27:38.480 --> 00:27:43.160]   have two things, a really generalized model that's powerful in probably semantics, which
[00:27:43.160 --> 00:27:47.000]   is one half of the thing about language, and then a secondary model that's either really
[00:27:47.000 --> 00:27:55.400]   good at that instruction, like specific instruct, or at some sort of fact adaptation.
[00:27:55.400 --> 00:28:02.040]   Because it's like one model will become specifically more complex at the cost of sustaining facts,
[00:28:02.040 --> 00:28:05.520]   whereas another model can preserve facts at the cost of not being the most semantically
[00:28:05.520 --> 00:28:07.000]   complex one.
[00:28:07.000 --> 00:28:08.000]   So this is a hypothesis.
[00:28:08.000 --> 00:28:11.080]   We'll think we'll probably end up finding a lot of these different pairs to get the
[00:28:11.080 --> 00:28:13.400]   best of both worlds.
[00:28:13.400 --> 00:28:17.040]   Interesting.
[00:28:17.040 --> 00:28:26.960]   I guess maybe overall at this moment, November 7th, 2022, do you have a characterization
[00:28:26.960 --> 00:28:32.280]   in your mind of what large language models can do and can't do?
[00:28:32.280 --> 00:28:33.800]   Where do you feel like the limit is?
[00:28:33.800 --> 00:28:40.000]   Is there a type of content that you feel like you couldn't create well, given the current
[00:28:40.000 --> 00:28:45.440]   state of things that you might be able to do in 2023 or 2024?
[00:28:45.440 --> 00:28:49.320]   I can bubble up a few big customer complaints.
[00:28:49.320 --> 00:28:54.840]   Perhaps the biggest one is factuality.
[00:28:54.840 --> 00:29:02.320]   It'd be one thing if they just always had incorrect facts, and then you could see it.
[00:29:02.320 --> 00:29:04.920]   It's one thing if you see a fact and then you go correct it, but it kind of lulls you
[00:29:04.920 --> 00:29:05.920]   to sleep.
[00:29:05.920 --> 00:29:10.200]   It's like, man, it knows a lot and you all of a sudden start trusting everything.
[00:29:10.200 --> 00:29:12.920]   And then it's like, you don't want to look up every fact because you've seen four that
[00:29:12.920 --> 00:29:15.760]   are totally right, but then it'll just say the opposite.
[00:29:15.760 --> 00:29:22.720]   I can remember one time I was asking, "Who won the 2021 Super Bowl?"
[00:29:22.720 --> 00:29:26.120]   And it was like, I forget what it was.
[00:29:26.120 --> 00:29:31.000]   It was basically the right team, it was the right teams, the right score, the right location,
[00:29:31.000 --> 00:29:34.560]   the right date, I would build that up, but it actually switched the team.
[00:29:34.560 --> 00:29:36.760]   So it seemed it lost, it said won.
[00:29:36.760 --> 00:29:38.720]   And it just lulls you to sleep.
[00:29:38.720 --> 00:29:43.360]   It all looks pretty good, it probably passed the SNP test, and then you realize, oh man,
[00:29:43.360 --> 00:29:46.280]   I just shipped the exact wrong answer.
[00:29:46.280 --> 00:29:52.800]   And so that's just a big thing that we've got to figure out how to control for, how
[00:29:52.800 --> 00:29:58.400]   to identify, and how to be more truthful, I suppose.
[00:29:58.400 --> 00:30:01.920]   Another one is just obviously getting it to follow instructions.
[00:30:01.920 --> 00:30:04.200]   It can tend to repeat itself.
[00:30:04.200 --> 00:30:08.240]   If it hasn't quite picked up the pattern or the instruction, it just thinks you want it
[00:30:08.240 --> 00:30:11.280]   to say the same thing over and over again.
[00:30:11.280 --> 00:30:18.320]   And then if our customers set that pattern, let's say you do it twice, and you keep trying
[00:30:18.320 --> 00:30:20.400]   to write, it's like, "So now it's gonna do it even more."
[00:30:20.400 --> 00:30:24.360]   Then you just reinforce this, it spirals out where it's like, "Well, why is it always saying
[00:30:24.360 --> 00:30:25.360]   the same thing over and over?"
[00:30:25.360 --> 00:30:30.120]   It's like you set the pattern at the beginning that makes it do that.
[00:30:30.120 --> 00:30:36.960]   You misspell one word in an intro paragraph of a blog post, and then you realize that
[00:30:36.960 --> 00:30:39.000]   Jasper misspells it the entire way.
[00:30:39.000 --> 00:30:40.800]   And you're just like, "What is happening?"
[00:30:40.800 --> 00:30:41.800]   It's a common word.
[00:30:41.800 --> 00:30:43.920]   It's like, "Well, you misspelled it."
[00:30:43.920 --> 00:30:47.200]   And so Jasper thinks that's how you want the word to be spelled.
[00:30:47.200 --> 00:30:51.960]   So there's definitely a lot of just steering content and teaching people.
[00:30:51.960 --> 00:30:58.880]   I also think people just don't have an understanding or a grasp of what is happening.
[00:30:58.880 --> 00:31:02.120]   Fundamentally what are these models trying to do?
[00:31:02.120 --> 00:31:04.920]   And how do they respond to certain things?
[00:31:04.920 --> 00:31:09.040]   This is not anything anyone's ever had experience with before.
[00:31:09.040 --> 00:31:12.720]   So coming in, we're not just teaching them how to use our product.
[00:31:12.720 --> 00:31:19.680]   We're trying to teach them fundamentally, here's even what AI-generated content means.
[00:31:19.680 --> 00:31:22.240]   And here's the limitations of these kinds of models.
[00:31:22.240 --> 00:31:23.720]   And here's what they're really good at.
[00:31:23.720 --> 00:31:26.920]   We're trying to teach all of them that in a very simple way.
[00:31:26.920 --> 00:31:34.120]   Saad, have you seen anything that you feel like we can't do?
[00:31:34.120 --> 00:31:38.600]   So just the way we evaluate and think about models is you have an XY axis, you have semantic
[00:31:38.600 --> 00:31:41.360]   complexity, and then you have domain fit, which has a lot of different features like
[00:31:41.360 --> 00:31:42.360]   factuality.
[00:31:42.360 --> 00:31:47.040]   Then you have a bunch of additional capabilities like multilingualism and so on.
[00:31:47.040 --> 00:31:49.960]   And we also pay a lot of attention to instructing the customers to get what they want out of
[00:31:49.960 --> 00:31:50.960]   it.
[00:31:50.960 --> 00:31:55.200]   In terms of semantic complexity, I think it can probably end up doing everything at the
[00:31:55.200 --> 00:31:56.200]   end of the day.
[00:31:56.200 --> 00:31:59.760]   There was this article about, "Does Moore's Law apply to generative AI?"
[00:31:59.760 --> 00:32:01.880]   And I think it kind of does, but not really.
[00:32:01.880 --> 00:32:05.240]   It applies for semantic complexity really well.
[00:32:05.240 --> 00:32:11.520]   But unlike Moore's Law, which continues on forever, there's diminishing returns.
[00:32:11.520 --> 00:32:14.360]   Eventually you'll start hitting an asymptote and level off.
[00:32:14.360 --> 00:32:18.160]   Because what does it mean to become infinitely good at semantics or language?
[00:32:18.160 --> 00:32:19.160]   Because humans have a limit.
[00:32:19.160 --> 00:32:20.160]   You can't really beat that.
[00:32:20.160 --> 00:32:23.600]   Because what would it even mean for the customer going above that?
[00:32:23.600 --> 00:32:26.680]   So I think it'll actually get really good for semantics.
[00:32:26.680 --> 00:32:30.120]   I think there's a lot of limit around domain fit and factuality.
[00:32:30.120 --> 00:32:35.560]   And there's actually an aspect of it that kind of worries me a little bit.
[00:32:35.560 --> 00:32:39.680]   I would really be worried if anybody using a generative model started using it to get
[00:32:39.680 --> 00:32:42.040]   advice, legal advice or medical advice.
[00:32:42.040 --> 00:32:45.560]   And it kind of speaks to what Dave was saying about factuality.
[00:32:45.560 --> 00:32:47.640]   It's actually getting better at lying.
[00:32:47.640 --> 00:32:49.200]   So it looks like it's more factual.
[00:32:49.200 --> 00:32:55.120]   If you ask it for legal advice, some of these models can cite legal papers and even come
[00:32:55.120 --> 00:32:56.840]   up with fake court cases.
[00:32:56.840 --> 00:32:58.640]   But it's totally made up.
[00:32:58.640 --> 00:33:02.200]   So it's actually not just a limitation, but almost a risk.
[00:33:02.200 --> 00:33:05.180]   I think our community is really wise to not use it for that.
[00:33:05.180 --> 00:33:07.560]   But you'll see this kind of get better and worse at the same time.
[00:33:07.560 --> 00:33:12.880]   People get better at representing citations in a really strange way.
[00:33:12.880 --> 00:33:16.800]   I think for domain fit and factuality, we actually have the perfect tool for factuality.
[00:33:16.800 --> 00:33:18.000]   We've always had it for decades.
[00:33:18.000 --> 00:33:19.440]   It's a copy and paste.
[00:33:19.440 --> 00:33:24.360]   And so the question is, if we want to increase factuality, are we able to bring in a database
[00:33:24.360 --> 00:33:28.280]   that has the facts that the user cares about and then have those stupid models?
[00:33:28.280 --> 00:33:29.280]   I call it stupid factuality.
[00:33:29.280 --> 00:33:32.240]   Do you have stupid factuality, smart factuality and false factuality?
[00:33:32.240 --> 00:33:35.840]   Can we replicate stupid factuality with one model and then have another model be semantically
[00:33:35.840 --> 00:33:41.200]   complex and bring the best of both to the user?
[00:33:41.200 --> 00:33:44.280]   So I think the limitations are around factuality, as Dave said.
[00:33:44.280 --> 00:33:49.080]   I think everything else though, there's like, in a way, it's not that the sky's the limit,
[00:33:49.080 --> 00:33:50.840]   but the users are the limit.
[00:33:50.840 --> 00:33:55.760]   We'll be able to accomplish what a lot of humans can accomplish in the realms of language
[00:33:55.760 --> 00:33:58.640]   and also semantics.
[00:33:58.640 --> 00:34:03.400]   When you say semantic complexity, can you give me an example of what you mean?
[00:34:03.400 --> 00:34:07.840]   What would be a very semantically complex thing to say?
[00:34:07.840 --> 00:34:13.320]   Yeah, let's do two examples, one being a tweet and then one being the longest form possible.
[00:34:13.320 --> 00:34:22.080]   So a tweet, if you say a sentence like, "The dog barked," or let's just say you say a sentence
[00:34:22.080 --> 00:34:26.280]   like, "The dog barks," and you say, "The dog likes Jurassic Park," and you say, "The dog
[00:34:26.280 --> 00:34:30.340]   likes Jurassic Bark," the last one is a pun.
[00:34:30.340 --> 00:34:33.520]   For the model to know that's like, "Hey, write me a funny joke about a dog," it would have
[00:34:33.520 --> 00:34:35.400]   to know that bark and dog is related.
[00:34:35.400 --> 00:34:39.160]   It would have to know that Jurassic Park was a movie and that you can replace bark with
[00:34:39.160 --> 00:34:40.160]   bark.
[00:34:40.160 --> 00:34:41.800]   There's a lot of semantic complexity going into that sentence.
[00:34:41.800 --> 00:34:46.440]   You're getting a higher density of meaning within shorter tokens or word count, whereas
[00:34:46.440 --> 00:34:50.880]   the first sentence is almost the same word length, but it's less complex.
[00:34:50.880 --> 00:34:55.000]   So semantic complexity is the ability for it to have different layers of meaning within
[00:34:55.000 --> 00:34:56.360]   a given space.
[00:34:56.360 --> 00:35:01.740]   In terms of the longest form, think about a play or something by Lin-Manuel Miranda.
[00:35:01.740 --> 00:35:05.740]   You have questions of plot where you're getting the end of the play to reference something
[00:35:05.740 --> 00:35:11.500]   in the beginning of the play or different paragraphs are referring to each other.
[00:35:11.500 --> 00:35:14.380]   If you imagine the words being linked to each other, it's like you have more links between
[00:35:14.380 --> 00:35:15.380]   words and between paragraphs.
[00:35:15.380 --> 00:35:16.380]   That's like semantic complexity.
[00:35:16.380 --> 00:35:20.820]   It's like more dimensions to it.
[00:35:20.820 --> 00:35:26.900]   And so far as these LLMs or large language models are predicting the next word in a string
[00:35:26.900 --> 00:35:31.180]   of tokens, you can see why it's hard for them to accomplish this, but at the same time,
[00:35:31.180 --> 00:35:35.820]   why they mathematically can end up doing so.
[00:35:35.820 --> 00:35:36.820]   It's interesting.
[00:35:36.820 --> 00:35:40.380]   I feel like I've spent a little more time with Dolly maybe because my daughter loves
[00:35:40.380 --> 00:35:41.380]   Dolly.
[00:35:41.380 --> 00:35:45.140]   And I feel like there, we have such basic problems.
[00:35:45.140 --> 00:35:49.300]   We try to get it to draw the mom with black hair instead of blonde hair.
[00:35:49.300 --> 00:35:53.100]   It drives my daughter nuts and actually my wife nuts too.
[00:35:53.100 --> 00:35:58.220]   That just seems like such basic semantic understanding of a sentence.
[00:35:58.220 --> 00:36:02.180]   It'll often take a different person in the scene that we're trying to describe and give
[00:36:02.180 --> 00:36:06.540]   them black hair instead of the mom.
[00:36:06.540 --> 00:36:11.380]   I'm curious, do you think there's something different about image generation?
[00:36:11.380 --> 00:36:15.740]   Because it doesn't seem like it has very much understanding of what I'm asking, at least
[00:36:15.740 --> 00:36:16.740]   in that domain.
[00:36:16.740 --> 00:36:19.980]   I think image generation is interesting.
[00:36:19.980 --> 00:36:24.660]   And obviously it can be so visual and instant that it's really easy to synthesize the whole
[00:36:24.660 --> 00:36:27.540]   thing in half a second.
[00:36:27.540 --> 00:36:31.700]   Where I think, if you had just run a blog post, it's like, "Ah, is this a good blog
[00:36:31.700 --> 00:36:32.700]   post?
[00:36:32.700 --> 00:36:33.700]   Is it what I wanted?"
[00:36:33.700 --> 00:36:37.500]   It's going to take me two minutes to figure that out, do all of that.
[00:36:37.500 --> 00:36:41.620]   So there's something just like, I'm sure there's a lot of weird stuff happening.
[00:36:41.620 --> 00:36:44.780]   And obviously text generation has been around longer than image generation.
[00:36:44.780 --> 00:36:54.260]   So this image generation will probably be super easy and awesome in a year or 72 hours.
[00:36:54.260 --> 00:36:56.940]   I'm sure there's weird stuff happening that's just harder to see.
[00:36:56.940 --> 00:37:00.540]   It's harder to see that, "Oh, it gave the wrong hair color to the wrong person," or
[00:37:00.540 --> 00:37:04.980]   "It gave the wrong conclusion to this thing that I thought it did there."
[00:37:04.980 --> 00:37:12.100]   And also, as far as image generation, you could say, "Don't paint this car pink."
[00:37:12.100 --> 00:37:13.100]   What's it going to do?
[00:37:13.100 --> 00:37:18.220]   It's going to paint the car pink because it doesn't know that don't and pink are tied
[00:37:18.220 --> 00:37:19.220]   there.
[00:37:19.220 --> 00:37:25.180]   And so it still feels, I think image generation probably still feels much more dumb than text
[00:37:25.180 --> 00:37:26.740]   generation.
[00:37:26.740 --> 00:37:31.620]   And I assume it's just the state of the technology being earlier as opposed to maybe something
[00:37:31.620 --> 00:37:34.140]   being more complex there, but I could be wrong.
[00:37:34.140 --> 00:37:35.140]   Interesting.
[00:37:35.140 --> 00:37:41.580]   I'm curious, and I'm not here to grill you at all on your business model, but I feel
[00:37:41.580 --> 00:37:47.620]   like I have to ask, you made this awesome business, it sounds like, in a few weeks of
[00:37:47.620 --> 00:37:51.340]   effort at first and it kind of just took off.
[00:37:51.340 --> 00:37:53.660]   How do you think about defending your business?
[00:37:53.660 --> 00:37:58.880]   Don't you worry that someone might come along with a similar approach or maybe they find
[00:37:58.880 --> 00:38:04.460]   something that's a little bit better somehow in such a fast moving space?
[00:38:04.460 --> 00:38:06.460]   How do you stay ahead of that?
[00:38:06.460 --> 00:38:10.180]   Well, I don't think we made an awesome business in a few weeks.
[00:38:10.180 --> 00:38:16.020]   I think we made a crappy MVP for how to do Facebook ads in a few weeks.
[00:38:16.020 --> 00:38:22.580]   And then I've spent every day since then building out all the other parts of a scalable, repeatable
[00:38:22.580 --> 00:38:23.580]   business.
[00:38:23.580 --> 00:38:25.300]   But it's a super valid question.
[00:38:25.300 --> 00:38:30.980]   And I think I spent a lot of time just thinking about moats over the last 18 months.
[00:38:30.980 --> 00:38:31.980]   What's real, what's not real?
[00:38:31.980 --> 00:38:35.980]   I was looking at the B2B companies, where are really moats?
[00:38:35.980 --> 00:38:41.100]   Obviously, you've got people with emotion thinking about maybe network effects, you
[00:38:41.100 --> 00:38:46.260]   think Uber going into a city or you think Amazon having warehouses everywhere.
[00:38:46.260 --> 00:38:50.300]   Things are so structurally obvious.
[00:38:50.300 --> 00:38:54.900]   But then you also take maybe HubSpot or maybe Adobe.
[00:38:54.900 --> 00:38:55.900]   What's the moat there?
[00:38:55.900 --> 00:38:56.900]   I don't know.
[00:38:56.900 --> 00:39:00.940]   They kind of knew a customer and they built a good team and they had good culture and
[00:39:00.940 --> 00:39:05.100]   they maybe got a little lucky and they kept executing over and over.
[00:39:05.100 --> 00:39:07.460]   And they had a second product, a third product, a fourth product.
[00:39:07.460 --> 00:39:14.780]   I think in B2B, that's probably far more common than this Amazon example, where you just end
[00:39:14.780 --> 00:39:19.500]   up building a good company that can continue to execute at a fast pace, knowing the customer
[00:39:19.500 --> 00:39:20.500]   deeply.
[00:39:20.500 --> 00:39:28.420]   And I think you've got moats like brand, you've got moats like community, you've got distribution.
[00:39:28.420 --> 00:39:29.700]   We want to have all of those.
[00:39:29.700 --> 00:39:34.180]   We also want to keep developing strong product and tech moats too.
[00:39:34.180 --> 00:39:40.260]   So I think at some level, that means just we've got to have a continually improving
[00:39:40.260 --> 00:39:41.540]   product and we've got to...
[00:39:41.540 --> 00:39:46.620]   So if you end up having so much product built, maybe not if it's hard to build, it's just
[00:39:46.620 --> 00:39:47.860]   it would be hard to build all of it.
[00:39:47.860 --> 00:39:51.940]   And by the time you built all of it, your competitor, I'd be gone too.
[00:39:51.940 --> 00:39:58.740]   But I think when it comes to our AI, yeah, there's a ton of differentiation just around
[00:39:58.740 --> 00:40:00.220]   the models that you use.
[00:40:00.220 --> 00:40:02.140]   We want to be really nimble.
[00:40:02.140 --> 00:40:06.420]   We're always building in such a way that we can replace everything wholesale very, very
[00:40:06.420 --> 00:40:08.380]   quickly.
[00:40:08.380 --> 00:40:12.700]   I think a lot of companies maybe are going to get stuck on some old model or some old
[00:40:12.700 --> 00:40:13.700]   way of doing things.
[00:40:13.700 --> 00:40:16.660]   And that's going to be the death of them.
[00:40:16.660 --> 00:40:22.780]   We also realized that we've got a really unique dataset, just that our customers are giving
[00:40:22.780 --> 00:40:24.420]   us and we're seeing how they use it.
[00:40:24.420 --> 00:40:30.020]   They're generating all sorts of content that nobody else in the world has.
[00:40:30.020 --> 00:40:35.780]   So the degree that we can use that to go and make models better, any model, open AI, this
[00:40:35.780 --> 00:40:38.980]   new one that comes out, the new one that comes out tomorrow, whatever, we would be able to
[00:40:38.980 --> 00:40:45.860]   take that dataset and very, very quickly fine tune and train those models to be great for
[00:40:45.860 --> 00:40:46.860]   our customers.
[00:40:46.860 --> 00:40:48.220]   It may not be great for anybody else's customers.
[00:40:48.220 --> 00:40:53.380]   It may not be great for any other use cases, but it's what our customers want and we've
[00:40:53.380 --> 00:40:54.820]   got to get inside track there.
[00:40:54.820 --> 00:41:01.140]   And so all that being said, I think moats are something to be working towards.
[00:41:01.140 --> 00:41:02.820]   I think there's a lot of pieces.
[00:41:02.820 --> 00:41:12.140]   I don't think Jasper or almost any other company B2B will live and die on one perfect moat.
[00:41:12.140 --> 00:41:17.180]   It'd be a combination of six, seven, eight different things that make it hard to do it
[00:41:17.180 --> 00:41:18.180]   all together.
[00:41:18.180 --> 00:41:24.180]   And I guess from a technical perspective, are there things that you do to stay on top
[00:41:24.180 --> 00:41:27.380]   of the generative AI space broadly?
[00:41:27.380 --> 00:41:35.180]   I'd be curious to hear that thought, but I see a lot of it on Twitter, to be honest.
[00:41:35.180 --> 00:41:40.460]   It's like, where do you go for freaking information every 10 minutes?
[00:41:40.460 --> 00:41:41.460]   It's like it's Twitter.
[00:41:41.460 --> 00:41:45.740]   And by the time it makes it into some newsletter roundup, that stuff's obsolete now.
[00:41:45.740 --> 00:41:50.260]   So a lot of it's just finding and curating a good Twitter list of people that are just
[00:41:50.260 --> 00:41:52.380]   in the know and all of that.
[00:41:52.380 --> 00:41:57.900]   I think, yeah, just conversations with other founders like yourself or other stuff like
[00:41:57.900 --> 00:41:58.900]   that.
[00:41:58.900 --> 00:41:59.900]   I hear a ton there.
[00:41:59.900 --> 00:42:06.420]   It's just the only way to stay up to date is to really get all the way in because no
[00:42:06.420 --> 00:42:09.980]   one's going to curate it and spoon feed it to you.
[00:42:09.980 --> 00:42:13.260]   And by the time they do, you'll have kind of missed it.
[00:42:13.260 --> 00:42:15.260]   But yeah, what do you think, Saad?
[00:42:15.260 --> 00:42:16.260]   Yeah.
[00:42:16.260 --> 00:42:20.100]   So before I started at Jasper, I called up one of my mentors because I ran a bunch of
[00:42:20.100 --> 00:42:23.420]   R&D laboratories and just research processes.
[00:42:23.420 --> 00:42:24.420]   And I was like, "How do you do it?
[00:42:24.420 --> 00:42:27.180]   How do you become a successful R&D leader?"
[00:42:27.180 --> 00:42:31.300]   And he was like, "Well, you're probably never going to beat everybody else in the world
[00:42:31.300 --> 00:42:34.100]   at everything because you have the whole world and they're all researching and coming up
[00:42:34.100 --> 00:42:35.100]   with the best stuff.
[00:42:35.100 --> 00:42:38.580]   So definitely stay on top of that, but make that a small percentage of your focus and
[00:42:38.580 --> 00:42:40.740]   then find the one thing that you can be the best in the world at."
[00:42:40.740 --> 00:42:45.980]   And as Dave said, we want to be the most customer obsessed company.
[00:42:45.980 --> 00:42:49.300]   We want to understand what we can do with customer data.
[00:42:49.300 --> 00:42:53.300]   It'd be great if a customer could say, "I thought it and Jasper got it."
[00:42:53.300 --> 00:42:56.380]   They go from their idea to something that's in their hands, like some content created
[00:42:56.380 --> 00:42:58.580]   in the fastest, easiest way possible.
[00:42:58.580 --> 00:43:03.540]   And so I think that customer data, being able to take the best of the world's R&D and saying,
[00:43:03.540 --> 00:43:06.420]   "Hey, you're coming up with this new model that can be fine tuned in this many ways.
[00:43:06.420 --> 00:43:08.100]   You have these new prompting techniques.
[00:43:08.100 --> 00:43:15.140]   You have these new base models or methods to hook on and adapt your models to get better
[00:43:15.140 --> 00:43:16.140]   outputs."
[00:43:16.140 --> 00:43:17.140]   We want to take all of that.
[00:43:17.140 --> 00:43:20.820]   And then the one big thing we want to do is find a way to use our customer data to get
[00:43:20.820 --> 00:43:23.580]   more customer fit, I think.
[00:43:23.580 --> 00:43:24.580]   And that's a big deal.
[00:43:24.580 --> 00:43:28.020]   Like I said, models are either going to get semantically more complex or they're better
[00:43:28.020 --> 00:43:29.060]   at domain fit.
[00:43:29.060 --> 00:43:32.500]   So I think that's almost the second axis, the whole second axis that we can be the best
[00:43:32.500 --> 00:43:33.500]   in the world at.
[00:43:33.500 --> 00:43:34.500]   Yeah.
[00:43:34.500 --> 00:43:41.460]   And I guess from a hiring perspective, do you actually try to hire experts on generative
[00:43:41.460 --> 00:43:42.460]   AI?
[00:43:42.460 --> 00:43:43.460]   Is that even possible?
[00:43:43.460 --> 00:43:46.540]   Would anyone pass that bar at this point?
[00:43:46.540 --> 00:43:49.960]   So I just would like to this word of what does expertise mean?
[00:43:49.960 --> 00:43:53.020]   The paper on transformers came out in 2017.
[00:43:53.020 --> 00:43:58.180]   And we've gotten tons of amazing applicants who have a lot of AI experience.
[00:43:58.180 --> 00:43:59.740]   So I think that's the question.
[00:43:59.740 --> 00:44:01.540]   Is there an expert in generative AI?
[00:44:01.540 --> 00:44:03.540]   We're all learning these things together.
[00:44:03.540 --> 00:44:07.500]   Even the R&D centers, these world-class folks that come up with a model, they don't even
[00:44:07.500 --> 00:44:10.260]   know what the model can really do until they test it.
[00:44:10.260 --> 00:44:11.580]   It really is a black box.
[00:44:11.580 --> 00:44:16.180]   And I don't think that this idea of super explainable AI can apply to this field super
[00:44:16.180 --> 00:44:17.180]   quickly.
[00:44:17.180 --> 00:44:18.660]   So what does it mean to be an expert?
[00:44:18.660 --> 00:44:22.700]   I think what we're looking for is people who are obsessed with customers who are fantastic
[00:44:22.700 --> 00:44:27.900]   problem solvers, who are creative and able to navigate this uniquely interdisciplinary
[00:44:27.900 --> 00:44:28.900]   space.
[00:44:28.900 --> 00:44:30.900]   You have to be really good at the AI and the data science.
[00:44:30.900 --> 00:44:33.660]   You also have to be really good at language, and you have to love the customer.
[00:44:33.660 --> 00:44:38.020]   And it's a pretty rare mix.
[00:44:38.020 --> 00:44:40.780]   It's like the book by Walter Isaacson, Innovators.
[00:44:40.780 --> 00:44:44.420]   The people who are good at the art, good at the science, and they have that customer obsession.
[00:44:44.420 --> 00:44:46.700]   So I think those are probably the three right ingredients.
[00:44:46.700 --> 00:44:49.420]   And we've been lucky to get some really great candidates along that line.
[00:44:49.420 --> 00:44:52.220]   And I think it makes the space uniquely interesting.
[00:44:52.220 --> 00:44:53.220]   It's definitely not boring.
[00:44:53.220 --> 00:45:00.980]   Yeah, we want to have a pretty diverse set of experiences there because you just got
[00:45:00.980 --> 00:45:02.260]   to be tapped in broadly.
[00:45:02.260 --> 00:45:06.500]   I mean, I said this earlier, but I think it's worth saying again.
[00:45:06.500 --> 00:45:11.860]   I have never worked particularly well with people that come in in an interview or something
[00:45:11.860 --> 00:45:15.020]   and say, "I really want to use this technology.
[00:45:15.020 --> 00:45:16.820]   I really want to use Terraform."
[00:45:16.820 --> 00:45:21.300]   Or whenever it is, it's just like, man, I don't know.
[00:45:21.300 --> 00:45:22.700]   It's just never worked out.
[00:45:22.700 --> 00:45:24.740]   It tends to be, "Hey, we got this hammer.
[00:45:24.740 --> 00:45:26.620]   We're just walking up with our nails all the time."
[00:45:26.620 --> 00:45:30.740]   Instead of just realizing, "Oh my gosh, a screwdriver would have done it so much simpler,
[00:45:30.740 --> 00:45:31.740]   so much faster there."
[00:45:31.740 --> 00:45:40.180]   And so we tend to shy away from folks that just want to do some cool technology.
[00:45:40.180 --> 00:45:44.740]   And if they get excited about that as a way to solve a problem, then that's huge.
[00:45:44.740 --> 00:45:47.660]   That's actually a super cool point, Dave, too.
[00:45:47.660 --> 00:45:49.700]   We've been talking a lot about fine tuning.
[00:45:49.700 --> 00:45:52.900]   When people imagine fine tuning, they think about the most complicated things first.
[00:45:52.900 --> 00:45:56.220]   I don't know if this is a trade secret or something, but there's actually so many simple
[00:45:56.220 --> 00:45:59.420]   things you can do to get major uplift.
[00:45:59.420 --> 00:46:01.300]   And yeah, that's crackiness.
[00:46:01.300 --> 00:46:02.300]   It goes a long way.
[00:46:02.300 --> 00:46:04.020]   Wait, wait, give me an example.
[00:46:04.020 --> 00:46:07.020]   You can't just say there's so many things.
[00:46:07.020 --> 00:46:11.300]   That's the first thing you would do.
[00:46:11.300 --> 00:46:12.660]   So for example, even with prompting.
[00:46:12.660 --> 00:46:16.740]   So right now, when you think of prompting, maybe you think about a user putting in a
[00:46:16.740 --> 00:46:21.460]   prompt, or maybe you think about some backend prompting that a template has and then the
[00:46:21.460 --> 00:46:24.220]   user interacts with that and then it sends it off.
[00:46:24.220 --> 00:46:25.960]   Just a simple thing, too.
[00:46:25.960 --> 00:46:31.180]   If you have a store of what you can call context, a series of pieces of information that the
[00:46:31.180 --> 00:46:32.180]   customer cares about.
[00:46:32.180 --> 00:46:35.180]   It could be their voice.
[00:46:35.180 --> 00:46:36.640]   It could be a list of their products.
[00:46:36.640 --> 00:46:39.020]   It could be their customer's voice.
[00:46:39.020 --> 00:46:42.420]   It could be an example of a customer review that somebody left them and was really good.
[00:46:42.420 --> 00:46:45.080]   It could be a speech that maybe one of the leaders gave.
[00:46:45.080 --> 00:46:49.340]   You could actually just concact up various pieces of context and then have a prompt and
[00:46:49.340 --> 00:46:51.140]   then get a really cool...
[00:46:51.140 --> 00:46:54.940]   If you think of the generative model as a remixer, remix these various pieces of context
[00:46:54.940 --> 00:46:57.100]   and give me something new, it actually works really well.
[00:46:57.100 --> 00:46:58.940]   And it's nothing super fancy.
[00:46:58.940 --> 00:46:59.940]   It's like fine-tuning the model.
[00:46:59.940 --> 00:47:01.740]   You're just doing really clever prompting.
[00:47:01.740 --> 00:47:07.520]   I was showing our business dev guy, our business pod leader, and he's been really impressed
[00:47:07.520 --> 00:47:08.520]   with it.
[00:47:08.520 --> 00:47:09.520]   It's just like these little hacks.
[00:47:09.520 --> 00:47:11.640]   There's thousands of these things.
[00:47:11.640 --> 00:47:19.040]   Or even to simplify it, even this is Dave level, let's say I wanted to make our paragraph
[00:47:19.040 --> 00:47:22.720]   generator 10% better, maybe measured by...
[00:47:22.720 --> 00:47:26.080]   It gets copied to clipboard 10% more often.
[00:47:26.080 --> 00:47:28.960]   There's obviously a bunch of ways to do that.
[00:47:28.960 --> 00:47:34.700]   One way that somebody that really wants to do AI, we probably talk to them and they'd
[00:47:34.700 --> 00:47:40.280]   say, "Oh, we got this T5 thing and we're going to spin up our own infrastructure and all
[00:47:40.280 --> 00:47:41.280]   of this stuff.
[00:47:41.280 --> 00:47:43.840]   It'll probably take a month and a half.
[00:47:43.840 --> 00:47:47.040]   But we'll get this thing and we'll fine-tune it on our past customer data."
[00:47:47.040 --> 00:47:48.040]   Yada, yada.
[00:47:48.040 --> 00:47:51.460]   You do all that and whatever, it could probably work.
[00:47:51.460 --> 00:47:55.640]   You could also, for us, we got this built in a way that non-technical people can do
[00:47:55.640 --> 00:47:56.640]   it.
[00:47:56.640 --> 00:48:03.440]   We can measure temperature from 0.7 to 0.4 and we might find that, holy cow, that actually
[00:48:03.440 --> 00:48:06.760]   produces way better paragraphs and nobody had ever even thought to test it and that
[00:48:06.760 --> 00:48:08.600]   takes six minutes.
[00:48:08.600 --> 00:48:13.600]   Now the customer gets the 10% improvement either way.
[00:48:13.600 --> 00:48:15.520]   Did they care about the T5 version?
[00:48:15.520 --> 00:48:17.920]   Think that's so cool and so amazing and awesome?
[00:48:17.920 --> 00:48:19.400]   No, they're just like 10% better.
[00:48:19.400 --> 00:48:21.240]   I'll just take whatever one you give me.
[00:48:21.240 --> 00:48:24.200]   I'm trying to write this blog post so I can get home to my kid's baseball game.
[00:48:24.200 --> 00:48:29.200]   So I think we're always pushing ourselves to be like, "What we care about is the lift.
[00:48:29.200 --> 00:48:31.920]   What are all the ways we could do that?
[00:48:31.920 --> 00:48:38.120]   Let's start with the simplest one first as opposed to just plain startup or plain AI
[00:48:38.120 --> 00:48:41.160]   or doing just whatever new cool white paper came out yesterday."
[00:48:41.160 --> 00:48:42.160]   Yeah, that's right.
[00:48:42.160 --> 00:48:47.840]   And just to refine that even a little bit more, it's actually really surprising sometimes
[00:48:47.840 --> 00:48:49.480]   too.
[00:48:49.480 --> 00:48:53.080]   We did this experiment where we had to do models and we actually thought, let's just
[00:48:53.080 --> 00:48:56.360]   say model A and model B. We thought model A was going to win because it was better and
[00:48:56.360 --> 00:48:59.560]   more slightly complex and all that type of stuff, but our customers like model B better.
[00:48:59.560 --> 00:49:01.560]   And we thought about why.
[00:49:01.560 --> 00:49:04.080]   B was a little bit more wordy, more flowery.
[00:49:04.080 --> 00:49:08.480]   So if you were an English teacher, you wouldn't have liked that, but you like model A. But
[00:49:08.480 --> 00:49:10.920]   our customers really like model B way better.
[00:49:10.920 --> 00:49:14.600]   And so it kind of dawned upon us that the customers were using the content outputs much
[00:49:14.600 --> 00:49:16.800]   like a sculptor looks at a big rock.
[00:49:16.800 --> 00:49:20.840]   They're actually trying to get something that's easy for them to delete from rather than something
[00:49:20.840 --> 00:49:22.880]   perfect that they want to add to.
[00:49:22.880 --> 00:49:24.920]   I think it's like the models are complicated.
[00:49:24.920 --> 00:49:28.800]   It's really interesting, but the customer is also very interesting and we don't fully
[00:49:28.800 --> 00:49:31.440]   understand exactly what they want and what they like either.
[00:49:31.440 --> 00:49:34.760]   So being able to focus on that using these hacks is a way to just understand the customer
[00:49:34.760 --> 00:49:38.120]   better and faster too.
[00:49:38.120 --> 00:49:46.320]   When you think about your R&D budget today, is it more like 90% prompt engineering and
[00:49:46.320 --> 00:49:51.360]   10% machine learning and fine tuning and this fancy stuff?
[00:49:51.360 --> 00:49:55.960]   Or is it more like 90% fancy stuff and 10% prompt engineering?
[00:49:55.960 --> 00:50:01.720]   How would you describe where you put your investments right now?
[00:50:01.720 --> 00:50:05.680]   So I do view them kind of tied together.
[00:50:05.680 --> 00:50:07.560]   For us, there hasn't been such a big divide.
[00:50:07.560 --> 00:50:12.960]   We'll get the new model or we'll come up with a new adapter situation.
[00:50:12.960 --> 00:50:15.240]   And we then have a black box.
[00:50:15.240 --> 00:50:18.560]   And the question is, will this black box be better for our customers, the same or worse?
[00:50:18.560 --> 00:50:23.520]   And then we definitely put it into an experimentation kind of situation.
[00:50:23.520 --> 00:50:25.400]   And we start running it through numbers of tasks.
[00:50:25.400 --> 00:50:26.760]   These tasks can involve different prompts.
[00:50:26.760 --> 00:50:29.280]   It can involve different configurations.
[00:50:29.280 --> 00:50:33.560]   And we have a bunch of internal metrics we're running against too.
[00:50:33.560 --> 00:50:36.360]   Everything really just represents our hypotheses for what we think the customer will like more.
[00:50:36.360 --> 00:50:40.680]   Will they like more varied sentence structures, more longer stuff?
[00:50:40.680 --> 00:50:44.040]   Will they like something that's more on topic or something, so on?
[00:50:44.040 --> 00:50:47.440]   So I think it's all a part of the toolkit.
[00:50:47.440 --> 00:50:51.880]   And the right percentage is the one that results in the biggest uplift fastest.
[00:50:51.880 --> 00:50:54.080]   So I'm sure it'll always be moving around.
[00:50:54.080 --> 00:50:55.560]   Prompt engineering plays a huge role right now.
[00:50:55.560 --> 00:50:59.320]   But we know that a lot of customers have asked for more domain specificity.
[00:50:59.320 --> 00:51:01.760]   So that's an area of research where a lot of our new budget is going to.
[00:51:01.760 --> 00:51:04.640]   But once you overcome this initial hump, then maybe we'll go back to prompt engineering
[00:51:04.640 --> 00:51:05.640]   again.
[00:51:05.640 --> 00:51:13.160]   Yeah, it's probably less prompt engineering as a percentage than you probably think.
[00:51:13.160 --> 00:51:16.680]   Less than 50%, maybe a lot less.
[00:51:16.680 --> 00:51:17.680]   Yeah, I don't know.
[00:51:17.680 --> 00:51:19.400]   And I think there's definitely some diminishing returns.
[00:51:19.400 --> 00:51:24.280]   You play around with that stuff, you get some great gains, and you keep trying and you can't
[00:51:24.280 --> 00:51:27.680]   really get anything else to really break through there.
[00:51:27.680 --> 00:51:31.240]   So prompt engineering gets you pretty far pretty fast.
[00:51:31.240 --> 00:51:37.040]   But at some point, you've got to do a lot of the outside stuff to just keep getting
[00:51:37.040 --> 00:51:38.040]   big improvements.
[00:51:38.040 --> 00:51:39.040]   Yeah.
[00:51:39.040 --> 00:51:46.400]   And I guess, is it hard as you scale to kind of institutionalize the things that you're
[00:51:46.400 --> 00:51:47.400]   learning?
[00:51:47.400 --> 00:51:49.880]   I picture your company running all these different experiments, do all these different things
[00:51:49.880 --> 00:51:51.720]   to make customers happy.
[00:51:51.720 --> 00:51:56.240]   But does every new hire have to come in and learn on their own, all these things?
[00:51:56.240 --> 00:52:03.000]   How do you keep track of all these insights that you're having?
[00:52:03.000 --> 00:52:05.280]   That is a challenge.
[00:52:05.280 --> 00:52:12.240]   And I think so much of what I try to do all day, and just for context, we've gone 10 people
[00:52:12.240 --> 00:52:14.640]   at the beginning of the year to 150 people now.
[00:52:14.640 --> 00:52:15.640]   Wow, that's incredible.
[00:52:15.640 --> 00:52:21.680]   So it's a ton of just, you're trying to find somebody that knows what happened three months
[00:52:21.680 --> 00:52:25.120]   ago and you asked 10 people and you can't find anyone that was even here.
[00:52:25.120 --> 00:52:31.680]   So yes, I think that's been a lot of the work and just trying to give people context over
[00:52:31.680 --> 00:52:35.240]   and over, try to point people to past things that were done or past experiments that were
[00:52:35.240 --> 00:52:36.240]   done or worked.
[00:52:36.240 --> 00:52:40.800]   And luckily, Slack is a pretty good record for a lot of that.
[00:52:40.800 --> 00:52:43.200]   We've got this channel that's like a shipped channel.
[00:52:43.200 --> 00:52:45.680]   Anytime something gets shipped to customers, we'll go in there.
[00:52:45.680 --> 00:52:48.600]   So you can learn a lot just by scrolling back through that and seeing all the different
[00:52:48.600 --> 00:52:49.600]   winning A/B tests.
[00:52:49.600 --> 00:52:51.120]   We always publish that.
[00:52:51.120 --> 00:52:56.720]   We even try to write that in a way that's customer facing, just to fully wrap your head
[00:52:56.720 --> 00:52:58.760]   around what are we trying to do here?
[00:52:58.760 --> 00:53:00.560]   Put it in a way that the customer would appreciate.
[00:53:00.560 --> 00:53:03.920]   Don't just talk about latency.
[00:53:03.920 --> 00:53:10.320]   Talk about, man, now customers can generate 18% faster and yada, yada, yada.
[00:53:10.320 --> 00:53:14.680]   So yeah, I think a lot of it is just kind of changing to call people back to the past,
[00:53:14.680 --> 00:53:18.040]   what we've done and aggregate that in the best ways that we can.
[00:53:18.040 --> 00:53:24.360]   But we have not found a really easy way or we don't have a super cool training course
[00:53:24.360 --> 00:53:26.600]   on all the insights that we've had.
[00:53:26.600 --> 00:53:30.320]   I think we're getting better at trying to aggregate those and make sure the right people
[00:53:30.320 --> 00:53:31.320]   have them.
[00:53:31.320 --> 00:53:34.560]   I don't know how you feel about this, Dave.
[00:53:34.560 --> 00:53:38.360]   I think remoteness has a lot of benefits and has a lot of challenges as well.
[00:53:38.360 --> 00:53:42.320]   I think a lot of folks pre-COVID are used to just getting into a team room and having
[00:53:42.320 --> 00:53:44.800]   a sprint and you kind of learn through that sprint.
[00:53:44.800 --> 00:53:47.440]   It's a little bit more, it's just different in a remote setting.
[00:53:47.440 --> 00:53:52.040]   So I think just the world is getting used to how do you apprentice in a remote setting.
[00:53:52.040 --> 00:53:57.680]   We definitely try to simulate that with off-site and getting to meet the team.
[00:53:57.680 --> 00:54:00.440]   But I'm not saying it's a challenge, but it's definitely something we're learning about
[00:54:00.440 --> 00:54:01.440]   as we go.
[00:54:01.440 --> 00:54:04.560]   Are you guys fully remote, Lucas?
[00:54:04.560 --> 00:54:07.520]   We're basically fully remote.
[00:54:07.520 --> 00:54:11.700]   We do have a headquarters in San Francisco, but our meetings are generally remote first
[00:54:11.700 --> 00:54:14.520]   and we'll hire people in any geography.
[00:54:14.520 --> 00:54:15.520]   Yeah.
[00:54:15.520 --> 00:54:16.520]   Yeah.
[00:54:16.520 --> 00:54:25.560]   I definitely think going from 10 to 150 would be easier in person, but that doesn't mean
[00:54:25.560 --> 00:54:27.280]   that the fullness of the company.
[00:54:27.280 --> 00:54:32.040]   I think that there's outsized benefits over a long enough time horizon to be remote, but
[00:54:32.040 --> 00:54:36.880]   it definitely feels like this early forming phase, trying to get knowledge in the right
[00:54:36.880 --> 00:54:37.880]   spot.
[00:54:37.880 --> 00:54:39.720]   It's tough remote, I think.
[00:54:39.720 --> 00:54:44.760]   And the initial team was all in the same room.
[00:54:44.760 --> 00:54:51.360]   I think it'd be very hard to find product market shit and have the year we did kind
[00:54:51.360 --> 00:54:54.360]   of out of the gate if we were all just remote at that time.
[00:54:54.360 --> 00:55:00.960]   Yeah, I guess on my end, I've kind of appreciated some of the discipline that remote work forces
[00:55:00.960 --> 00:55:01.960]   you to do.
[00:55:01.960 --> 00:55:06.200]   I think we write a lot more down and we keep better agendas and records and things like
[00:55:06.200 --> 00:55:07.200]   that.
[00:55:07.200 --> 00:55:10.600]   So for me, it hasn't been all bad, but I think there's so many different ways to run a company.
[00:55:10.600 --> 00:55:13.920]   And I think different teams even, some prefer to do lots of onsites and some don't care
[00:55:13.920 --> 00:55:14.920]   at all.
[00:55:14.920 --> 00:55:15.920]   And so, yeah.
[00:55:15.920 --> 00:55:21.640]   No, I do love that it forces you to just think more clearly, communicate more clearly, plan
[00:55:21.640 --> 00:55:25.200]   ahead so you're not just always putting out fire stuff today.
[00:55:25.200 --> 00:55:28.440]   I really think it does do some really good stuff there.
[00:55:28.440 --> 00:55:33.600]   I'm curious, we don't intentionally try to invite only customers on the podcast, but
[00:55:33.600 --> 00:55:37.640]   I think in the end, we typically are talking to people that are customers usually, and
[00:55:37.640 --> 00:55:41.280]   you guys actually aren't a customer of Weights & Biases.
[00:55:41.280 --> 00:55:47.440]   And I kind of wonder, if you were in my shoes, would you be worried?
[00:55:47.440 --> 00:55:52.160]   Do you feel like there's this big trend happening that sort of undercuts what Weights & Biases
[00:55:52.160 --> 00:55:53.160]   does?
[00:55:53.160 --> 00:55:55.200]   I think, Saad, you're probably more familiar with what we do.
[00:55:55.200 --> 00:55:57.400]   So maybe I'll let you answer that question.
[00:55:57.400 --> 00:56:01.240]   And I promise I won't be offended with any direction you want to take that.
[00:56:01.240 --> 00:56:02.240]   Yeah.
[00:56:02.240 --> 00:56:03.440]   So first of all, Weights & Biases is a great company.
[00:56:03.440 --> 00:56:07.160]   A lot of friends are there now.
[00:56:07.160 --> 00:56:11.280]   So and just thank you for inviting us, even though we're not customers.
[00:56:11.280 --> 00:56:18.800]   So I think, correct me if I'm wrong, but I feel like Weights & Biases, it increases in
[00:56:18.800 --> 00:56:23.800]   value as the customer has more and more models.
[00:56:23.800 --> 00:56:26.920]   It's essentially a thing that scales in value as the number of models scales.
[00:56:26.920 --> 00:56:27.920]   Is that right?
[00:56:27.920 --> 00:56:30.280]   I think that's what customers tell us.
[00:56:30.280 --> 00:56:31.960]   Yeah, for sure.
[00:56:31.960 --> 00:56:38.320]   So I guess it varies a lot in terms of how this generative AI space will kind of shape
[00:56:38.320 --> 00:56:39.320]   up.
[00:56:39.320 --> 00:56:44.040]   I can see some companies developing in this space that are like monomodel companies.
[00:56:44.040 --> 00:56:47.600]   They just have their one model and they're specializing in that one model and its use
[00:56:47.600 --> 00:56:48.600]   cases.
[00:56:48.600 --> 00:56:49.800]   So obviously for that, that'd be a challenge.
[00:56:49.800 --> 00:56:53.960]   I can see other companies, though, that they have maybe a few big base models.
[00:56:53.960 --> 00:56:54.960]   These things are pretty huge.
[00:56:54.960 --> 00:57:00.080]   You don't want to be fine tuning a multi-hundred billion dollar or a hundred billion parameter
[00:57:00.080 --> 00:57:03.160]   model all the time unnecessarily.
[00:57:03.160 --> 00:57:06.440]   But maybe these companies have maybe two, three, four bigger models, but they have tons
[00:57:06.440 --> 00:57:09.080]   of adapter models or they have some small models used for different things.
[00:57:09.080 --> 00:57:11.400]   I can see Waste and Bias as being ultra useful for that.
[00:57:11.400 --> 00:57:16.400]   So I think overall, the answer to your question is no, it'll still continue to be really useful.
[00:57:16.400 --> 00:57:21.200]   I think how people think about scaling models and when and how is it viable, that might
[00:57:21.200 --> 00:57:22.200]   change it.
[00:57:22.200 --> 00:57:25.040]   And I think we're still learning what the best architectures are that'll sustain in
[00:57:25.040 --> 00:57:26.040]   the space.
[00:57:26.040 --> 00:57:27.040]   Okay.
[00:57:27.040 --> 00:57:32.360]   Well, usually with the nerds that we have on this show, we end with two questions.
[00:57:32.360 --> 00:57:35.800]   I might slightly modify it for you guys because I have a slightly different version that I
[00:57:35.800 --> 00:57:36.800]   want to ask you.
[00:57:36.800 --> 00:57:42.640]   Typically, we ask, if you had more time to research a different topic in machine learning,
[00:57:42.640 --> 00:57:43.640]   what would that be?
[00:57:43.640 --> 00:57:48.800]   But I want to ask you all, if you had a different domain that you think these models might apply
[00:57:48.800 --> 00:57:55.440]   phenomenally well to where there's no one like you who's come in with that customer
[00:57:55.440 --> 00:58:02.240]   empathy, what do you think is ripe for disruption with these generative models?
[00:58:02.240 --> 00:58:03.240]   I think about this a lot.
[00:58:03.240 --> 00:58:07.320]   And I think we've seen, we just did our big series A announcement.
[00:58:07.320 --> 00:58:12.960]   There's probably just an army of clones gaining strength in the corners of the universe right
[00:58:12.960 --> 00:58:15.960]   now that will all pop up in the next two months.
[00:58:15.960 --> 00:58:23.440]   And they'll be just very much like Jasper and even probably good products and all that.
[00:58:23.440 --> 00:58:25.640]   That's a bad way to do it.
[00:58:25.640 --> 00:58:26.640]   You don't want to compete.
[00:58:26.640 --> 00:58:30.080]   You don't want to compete against us at our game.
[00:58:30.080 --> 00:58:31.080]   We have to be really good at our game.
[00:58:31.080 --> 00:58:33.360]   But we have one of a million games.
[00:58:33.360 --> 00:58:39.240]   And I think what I would encourage people to do is this could be done anywhere.
[00:58:39.240 --> 00:58:44.640]   And you could take this and put it in any subset of any industry.
[00:58:44.640 --> 00:58:47.120]   You could do legal stuff.
[00:58:47.120 --> 00:58:49.040]   You could do stuff for doctors.
[00:58:49.040 --> 00:58:51.600]   You could do stuff for different teams and companies.
[00:58:51.600 --> 00:58:58.600]   You could do stuff for, if you think about CRMs now, CRMs over 20 years, I've got a
[00:58:58.600 --> 00:59:03.920]   friend that just started a cleaning company, a local cleaning company.
[00:59:03.920 --> 00:59:06.080]   And he was like, it's so easy to start.
[00:59:06.080 --> 00:59:08.200]   There's the CRM that just does everything for you.
[00:59:08.200 --> 00:59:10.200]   I was like, what was it, HubSpot or Salesforce?
[00:59:10.200 --> 00:59:14.160]   No, it's some rando thing I've never heard of in my life.
[00:59:14.160 --> 00:59:19.720]   It's a big company that just does CRM for cleaning companies.
[00:59:19.720 --> 00:59:25.680]   That's where this goes to is you take your end user, you understand them deeply, you
[00:59:25.680 --> 00:59:29.640]   take all the noise, you simplify it for them and you give them a product that just does
[00:59:29.640 --> 00:59:32.640]   what they're trying to do better.
[00:59:32.640 --> 00:59:41.680]   And anyone could beat Jasper by going deeper in any lower vertical that we're not fully
[00:59:41.680 --> 00:59:42.920]   focused on there.
[00:59:42.920 --> 00:59:47.560]   Anyone could do better at something that's a little bit more specific.
[00:59:47.560 --> 00:59:50.040]   And this is true, I guess, of models too.
[00:59:50.040 --> 00:59:53.600]   You could get a better model if it was just more specific there.
[00:59:53.600 --> 00:59:58.760]   So that's kind of my encouragement to people and maybe just the community at large.
[00:59:58.760 --> 01:00:05.440]   This is all so cool, but there's so many people that would be thrilled to use this technology
[01:00:05.440 --> 01:00:08.040]   if you would just package it up for them.
[01:00:08.040 --> 01:00:14.000]   And I think the key is to not try and be the next Jasper or do exactly what we do.
[01:00:14.000 --> 01:00:19.520]   It's take the essence of it and then go for a different customer segment.
[01:00:19.520 --> 01:00:25.440]   And there's so much opportunity out there right now that's just completely untouched
[01:00:25.440 --> 01:00:27.560]   that nobody has tried to build for.
[01:00:27.560 --> 01:00:32.680]   And you go find a community and do that, you'll be in really good shape.
[01:00:32.680 --> 01:00:38.200]   But do you mean making marketing copy for lawyers or do you mean doing some kind of
[01:00:38.200 --> 01:00:40.080]   specific other thing for lawyers?
[01:00:40.080 --> 01:00:43.360]   I mean, there could be any.
[01:00:43.360 --> 01:00:47.400]   I mean, I don't think marketing copy as much.
[01:00:47.400 --> 01:00:50.880]   I'm thinking, well, I mean, one, I would just want to talk to lawyers.
[01:00:50.880 --> 01:00:54.200]   So I say, "Hey, can you tell me about what you write all day?"
[01:00:54.200 --> 01:00:55.200]   That's where I would start.
[01:00:55.200 --> 01:00:56.200]   I'm not a lawyer.
[01:00:56.200 --> 01:01:05.000]   I'm guessing it's a lot of explaining in short form to customers over email what that
[01:01:05.000 --> 01:01:06.000]   document means.
[01:01:06.000 --> 01:01:11.200]   And you can just build a quick summarizer that just hooks into Gmail and spits that
[01:01:11.200 --> 01:01:12.480]   out really fast.
[01:01:12.480 --> 01:01:17.840]   It could be you training up paralegals to understand more stuff.
[01:01:17.840 --> 01:01:22.840]   So you build a little tool that helps them kind of synthesize documents and explain it
[01:01:22.840 --> 01:01:24.120]   to just train up people internally.
[01:01:24.120 --> 01:01:32.240]   It could be generating some boilerplate content that maybe I talk to a lawyer and I say, "Hey,
[01:01:32.240 --> 01:01:33.760]   show me how you put together a document."
[01:01:33.760 --> 01:01:40.960]   Maybe it's a lot of them going to Google and searching for boilerplate stuff and then adding
[01:01:40.960 --> 01:01:41.960]   it in.
[01:01:41.960 --> 01:01:42.960]   Or it could be how engineers write code.
[01:01:42.960 --> 01:01:47.120]   A lot of it's kind of watching them go to Stack Overflow and copy and paste and that
[01:01:47.120 --> 01:01:50.200]   kind of codex or whatever's framed all that up.
[01:01:50.200 --> 01:01:56.160]   So I don't know because I haven't spent the time in there, but I think there's probably
[01:01:56.160 --> 01:02:00.360]   a ton of opportunity that I would never even think of that if you just spend time with
[01:02:00.360 --> 01:02:07.000]   a group of people, you'd see a lot of opportunity to do a lot of things because you'd be like,
[01:02:07.000 --> 01:02:10.240]   "Oh my gosh, this model from two years ago does that out of the box.
[01:02:10.240 --> 01:02:12.160]   We'll just spin that up for you."
[01:02:12.160 --> 01:02:17.320]   That's more of what I'm talking than thinking, "Oh, marketing copy for this niche there."
[01:02:17.320 --> 01:02:20.120]   Lukas: Well, one quick thought here, and it's just kind of a funny point.
[01:02:20.120 --> 01:02:25.080]   So I tried to read all of Jasper's churn notes, who's leaving Jasper and why.
[01:02:25.080 --> 01:02:28.120]   And there's a funny demographic of students who use Jasper.
[01:02:28.120 --> 01:02:30.960]   Maybe they're using it to do their homework.
[01:02:30.960 --> 01:02:32.200]   I'm not sure exactly.
[01:02:32.200 --> 01:02:34.820]   I think some of them are.
[01:02:34.820 --> 01:02:39.480]   So that combined with another insight of the education sector is one of the hardest sectors.
[01:02:39.480 --> 01:02:43.280]   It's so hard to innovate in education, especially technology-wise.
[01:02:43.280 --> 01:02:47.080]   This is less that I think it's a good idea, and it's more of just sending good vibes to
[01:02:47.080 --> 01:02:50.160]   whoever tries to use generative AI in education.
[01:02:50.160 --> 01:02:53.600]   Students seem to be using generative AI to do different things for their assignments
[01:02:53.600 --> 01:02:54.600]   and homework.
[01:02:54.600 --> 01:02:59.640]   I think it'd be really interesting if somebody did a Taekwondo move and took some of the
[01:02:59.640 --> 01:03:03.440]   capabilities of generative AI that would make a student want to use it, maybe for something
[01:03:03.440 --> 01:03:07.800]   that's kind of cheating, like they're doing their English essay on a generative AI app,
[01:03:07.800 --> 01:03:09.560]   but actually combining that with pedagogy.
[01:03:09.560 --> 01:03:12.880]   So they're actually still learning through that.
[01:03:12.880 --> 01:03:18.000]   Super hard to do these sort of startups, but wouldn't it be cool if you could do procedurally
[01:03:18.000 --> 01:03:22.760]   generated lessons for students, just kind of using this and kind of making it really
[01:03:22.760 --> 01:03:23.760]   fun.
[01:03:23.760 --> 01:03:24.760]   It's a hard space.
[01:03:24.760 --> 01:03:28.080]   I'm not saying it's going to be the next quarter or anything like that, but if somebody can
[01:03:28.080 --> 01:03:33.800]   succeed at using this in education, it's just impressive if somebody could pull that off.
[01:03:33.800 --> 01:03:34.800]   Cool.
[01:03:34.800 --> 01:03:35.800]   All right.
[01:03:35.800 --> 01:03:37.720]   So final question.
[01:03:37.720 --> 01:03:41.760]   We usually ask what's the hard part about making machine learning models work in the
[01:03:41.760 --> 01:03:46.520]   real world, but maybe for you, I'm just sort of curious more broadly, kind of making this
[01:03:46.520 --> 01:03:51.280]   company, making this product that people appreciate so much, what's something unexpectedly challenging
[01:03:51.280 --> 01:03:53.280]   about making it work?
[01:03:53.280 --> 01:04:00.280]   So I think it just goes back to this insight that people are more complicated than AI.
[01:04:00.280 --> 01:04:04.120]   And what their user, what their UX preferences are, how they want to use it, how to simplify
[01:04:04.120 --> 01:04:08.200]   it for them.
[01:04:08.200 --> 01:04:12.120]   Solving the black box of AI, there's probably thousands, maybe tens of thousands of permutations,
[01:04:12.120 --> 01:04:15.840]   but solving the right design for the human, there's infinite amounts of permutations for
[01:04:15.840 --> 01:04:16.840]   that.
[01:04:16.840 --> 01:04:19.400]   So yeah, this is new for everybody.
[01:04:19.400 --> 01:04:23.200]   And we're still learning how people use this.
[01:04:23.200 --> 01:04:27.040]   Maybe it's obvious to Dave and everybody else, but I was really surprised to realize that
[01:04:27.040 --> 01:04:30.840]   customers want a lot of text, but then they want to delete it and find the gem inside
[01:04:30.840 --> 01:04:31.840]   of that boulder.
[01:04:31.840 --> 01:04:35.320]   I just don't write like that, but it was fascinating to see that's how our customers do.
[01:04:35.320 --> 01:04:37.320]   There's probably going to be like a thousand other things we learn about people and how
[01:04:37.320 --> 01:04:38.320]   they use generative AI.
[01:04:38.320 --> 01:04:41.760]   Like it's infinite, what we can learn about people and how they want to use this.
[01:04:41.760 --> 01:04:47.560]   So I think it's hard, but inspiring at the same time.
[01:04:47.560 --> 01:04:53.640]   I was thinking something that's hard, this is a step off of the actual tech, but it's
[01:04:53.640 --> 01:04:55.880]   like the community aspect of it.
[01:04:55.880 --> 01:05:00.640]   I mean, arguably, our community has been one of our biggest advantages.
[01:05:00.640 --> 01:05:05.640]   So we've got customers with tattoos and we're just all riffing there.
[01:05:05.640 --> 01:05:08.520]   It's just a lot of fun, but it's like, man, it's exhausting.
[01:05:08.520 --> 01:05:13.800]   And I've had the whole community turn on me and all of a sudden, Instagram's blowing up,
[01:05:13.800 --> 01:05:15.840]   people are pissed, people are canceling and leaving.
[01:05:15.840 --> 01:05:16.840]   It's like, I'm the bad guy.
[01:05:16.840 --> 01:05:20.320]   I've got to go in there and save it.
[01:05:20.320 --> 01:05:30.840]   It's emotionally challenging to be connected to people in such a meaningful or powerful
[01:05:30.840 --> 01:05:33.280]   or exciting kind of way.
[01:05:33.280 --> 01:05:34.280]   And it's work.
[01:05:34.280 --> 01:05:39.640]   I mean, there's times that I really have to get hyped up to like, "I'm going to go and
[01:05:39.640 --> 01:05:42.120]   kind of do this."
[01:05:42.120 --> 01:05:45.480]   But to me, it just feels like table stakes.
[01:05:45.480 --> 01:05:48.360]   And if you're not willing to do that, it's okay.
[01:05:48.360 --> 01:05:53.320]   Maybe find a customer base that you would be willing to do that with.
[01:05:53.320 --> 01:05:57.320]   Because it's such a valuable part of the game, just building a company and building great
[01:05:57.320 --> 01:06:04.520]   products that if you find yourself avoiding or not wanting to spend time with your customers,
[01:06:04.520 --> 01:06:07.080]   then it's just going to be so hard to do the rest of it there.
[01:06:07.080 --> 01:06:12.840]   So communities are a lot of fun and high stakes.
[01:06:12.840 --> 01:06:17.160]   So fun when they're going really well, so miserable when they're not going well.
[01:06:17.160 --> 01:06:19.240]   But it's really, really worth it.
[01:06:19.240 --> 01:06:21.160]   And I think, yeah, the benefits are immense.
[01:06:21.160 --> 01:06:26.520]   You know, it's funny, Dave, the stock entrepreneurship advice that I always give people, and I haven't
[01:06:26.520 --> 01:06:29.920]   heard anyone else say it, is exactly what you said, which is find a customer that you
[01:06:29.920 --> 01:06:33.760]   like because you have to spend so much time with them and you'll just be so much happier.
[01:06:33.760 --> 01:06:38.240]   It's funny, I think both of our companies, we've had the identical approach of just sort
[01:06:38.240 --> 01:06:42.800]   of like approaching a really specific customer type with like a clean sheet of paper of like,
[01:06:42.800 --> 01:06:44.080]   "What do you need?"
[01:06:44.080 --> 01:06:47.520]   So it's interesting to learn that from this interview.
[01:06:47.520 --> 01:06:52.320]   And I mean, congrats on such a vibrant community and such a well-loved product.
[01:06:52.320 --> 01:06:53.320]   That's so cool.
[01:06:53.320 --> 01:06:54.320]   Yeah.
[01:06:54.320 --> 01:06:56.640]   But do you want to brag about any stats?
[01:06:56.640 --> 01:07:00.640]   I mean, everyone says, you know, Jasper is like one of the fastest growing companies
[01:07:00.640 --> 01:07:01.640]   of all time.
[01:07:01.640 --> 01:07:05.080]   Is there any numbers that you make public that you'd want to tell the world?
[01:07:05.080 --> 01:07:11.320]   Yeah, I think overall public, you know, I think what got public was, you know, year
[01:07:11.320 --> 01:07:19.800]   one by the end of the year, we got to 35 million ARR and we only had like nine people, I think
[01:07:19.800 --> 01:07:21.560]   at the end of the year that we're doing that.
[01:07:21.560 --> 01:07:22.560]   Pretty good.
[01:07:22.560 --> 01:07:24.560]   Don't tell my investors about that, please.
[01:07:24.560 --> 01:07:25.560]   Yeah, yeah, totally.
[01:07:25.560 --> 01:07:27.760]   No, it was pretty good.
[01:07:27.760 --> 01:07:31.880]   And it was like, you know, it's a mix of luck, it's a mix of right time, right place, the
[01:07:31.880 --> 01:07:32.880]   right team.
[01:07:32.880 --> 01:07:36.640]   And, you know, that's one of those like early, you know, eye popping things.
[01:07:36.640 --> 01:07:43.880]   And I think like two months into it, we added like a little over 3 million ARR in like a
[01:07:43.880 --> 01:07:44.880]   three day period.
[01:07:44.880 --> 01:07:45.880]   Oh my God.
[01:07:45.880 --> 01:07:46.880]   We launched this like new product.
[01:07:46.880 --> 01:07:52.280]   And it was just, I mean, I was just like hyperventilating, like, oh my God, like I cannot believe this.
[01:07:52.280 --> 01:07:56.680]   I've stood my whole life failing and you finally kind of hit one.
[01:07:56.680 --> 01:08:02.880]   So no, man, I think we're, I am just as shocked and grateful, you know, to kind of be a part
[01:08:02.880 --> 01:08:05.400]   of this as the, you know, the next person.
[01:08:05.400 --> 01:08:10.880]   And, you know, I think, I think, yeah, it's been a wild ride and we're trying to just
[01:08:10.880 --> 01:08:12.440]   stay humble and stay focused.
[01:08:12.440 --> 01:08:19.040]   And, you know, for the first year we didn't hire anybody, we didn't have any meetings,
[01:08:19.040 --> 01:08:20.640]   we didn't do any investor call.
[01:08:20.640 --> 01:08:26.160]   Like it was just like so simple, like almost this like garden of Eden of like startups.
[01:08:26.160 --> 01:08:30.080]   So just like us hanging out with our customers like all the time and like trying to build
[01:08:30.080 --> 01:08:31.840]   some stuff that they wanted.
[01:08:31.840 --> 01:08:32.840]   And that really worked.
[01:08:32.840 --> 01:08:36.440]   So I think, yeah, as we scale, like we're trying to just like keep that ethos, you know,
[01:08:36.440 --> 01:08:41.800]   and infuse that throughout the rest of the company because there's something nice about
[01:08:41.800 --> 01:08:45.960]   simplicity and something essential about simplicity as you scale.
[01:08:45.960 --> 01:08:46.960]   Absolutely.
[01:08:46.960 --> 01:08:49.080]   Well, thanks so much.
[01:08:49.080 --> 01:08:50.080]   Super fun interview.
[01:08:50.080 --> 01:08:51.080]   I really appreciate it.
[01:08:51.080 --> 01:08:52.080]   You bet, man.
[01:08:52.080 --> 01:08:53.080]   Appreciate you having us on.
[01:08:53.080 --> 01:08:54.080]   This has been so fun.
[01:08:54.080 --> 01:08:58.640]   Yeah, thanks for having us.
[01:08:58.640 --> 01:09:03.020]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:09:03.020 --> 01:09:07.760]   to the show notes in the description where you can find links to all the papers that
[01:09:07.760 --> 01:09:12.080]   are mentioned, supplemental material and a transcription that we work really hard to
[01:09:12.080 --> 01:09:13.080]   produce.
[01:09:13.080 --> 01:09:13.080]   So check it out.
[01:09:13.080 --> 01:09:16.420]   [MUSIC PLAYING]

