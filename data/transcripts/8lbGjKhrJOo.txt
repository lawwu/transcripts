
[00:00:00.000 --> 00:00:05.360]   In this video, we're going to talk about recursive neural networks, assuming no background in
[00:00:05.360 --> 00:00:10.080]   recursive neural networks, and we're going to talk about handling time series data because
[00:00:10.080 --> 00:00:13.460]   that's a great application of recursive neural networks.
[00:00:13.460 --> 00:00:18.840]   In subsequent videos, we're going to show how to take these RNNs, especially LSTMs,
[00:00:18.840 --> 00:00:20.060]   and apply them to text data.
[00:00:20.060 --> 00:00:25.040]   If you haven't watched the earlier videos on perceptrons or multilayer perceptrons,
[00:00:25.040 --> 00:00:27.420]   you may want to go back and watch those first.
[00:00:27.420 --> 00:00:31.920]   So far, if you've been following along in this series, you know enough to teach my little
[00:00:31.920 --> 00:00:34.740]   friend here to recognize images.
[00:00:34.740 --> 00:00:37.780]   But the real world happens over time.
[00:00:37.780 --> 00:00:41.880]   So today, we're going to introduce recursive neural networks, and along with that, we're
[00:00:41.880 --> 00:00:45.640]   going to introduce time series and handling data that changes over time.
[00:00:45.640 --> 00:00:47.600]   I keep saying recursive neural network.
[00:00:47.600 --> 00:00:51.360]   I think it's recurrent neural network, right?
[00:00:51.360 --> 00:00:52.360]   RNNs.
[00:00:52.360 --> 00:00:56.660]   So when we talk about time series data, there actually is a pretty famous data set, maybe
[00:00:56.660 --> 00:01:00.720]   not quite as famous as MNIST, but a really common data set to start with.
[00:01:00.720 --> 00:01:02.520]   And it's a super exciting data set.
[00:01:02.520 --> 00:01:09.320]   It's airline sales in the US from 1945 to 1965.
[00:01:09.320 --> 00:01:14.520]   So the goal here is to look at airline sales in the past and predict airline sales in the
[00:01:14.520 --> 00:01:15.760]   future.
[00:01:15.760 --> 00:01:18.840]   And actually, here's a graph of that data set that you can look at, and you can see
[00:01:18.840 --> 00:01:22.600]   that there's sort of two patterns happening at the same time.
[00:01:22.600 --> 00:01:29.240]   First, airline sales overall are rising, but there's also a strong cyclic seasonality component.
[00:01:29.240 --> 00:01:33.460]   So how do we use neural networks to model this data?
[00:01:33.460 --> 00:01:37.360]   So if you remember, I'm constantly talking about the API of neural networks, and I'm
[00:01:37.360 --> 00:01:41.720]   mentioning over and over in my first couple of videos on Perceptrons that all the data
[00:01:41.720 --> 00:01:45.180]   that you input into your neural network has to be fixed size.
[00:01:45.180 --> 00:01:46.920]   But with time series, it feels different, right?
[00:01:46.920 --> 00:01:49.660]   You have these variable sizes of data.
[00:01:49.660 --> 00:01:55.360]   So how do we turn that into a data set with a training set and a test set that we can
[00:01:55.360 --> 00:01:58.240]   actually input into our algorithm?
[00:01:58.240 --> 00:02:04.240]   So there's one super common, super simple way to do it, and that's called the sliding
[00:02:04.240 --> 00:02:06.240]   window approach.
[00:02:06.240 --> 00:02:11.160]   And the way this works is we take a window of some fixed size, maybe 10 elements, and
[00:02:11.160 --> 00:02:12.160]   we go across.
[00:02:12.160 --> 00:02:16.520]   We take the first 10 elements, and we put that in the first row of our training data.
[00:02:16.520 --> 00:02:21.040]   And then from the first 10 elements, we want to output the 11th element, which we call
[00:02:21.040 --> 00:02:22.800]   our label for that data.
[00:02:22.800 --> 00:02:28.680]   Then we slide over by 1, and so we take elements 2 through 11 as our inputs.
[00:02:28.680 --> 00:02:33.760]   And from elements 2 through 11, we try to predict the 12th record in our data set.
[00:02:33.760 --> 00:02:37.800]   And we keep sliding that window across, and here we've actually created a data set that's
[00:02:37.800 --> 00:02:43.320]   exactly the same as the data set that we input into our Perceptron in the very first video
[00:02:43.320 --> 00:02:45.120]   in this series.
[00:02:45.120 --> 00:02:52.320]   So this is a super common approach and a super simple approach, and it can be extremely effective.
[00:02:52.320 --> 00:02:53.880]   So should we go to the code?
[00:02:53.880 --> 00:03:01.040]   So here we are, and we need to go into the ml class videos directory, and then we need
[00:03:01.040 --> 00:03:05.000]   to go into the time series directory.
[00:03:05.000 --> 00:03:08.040]   Then open up perceptron.py.
[00:03:08.040 --> 00:03:10.200]   So you recognize a lot of this code at the top.
[00:03:10.200 --> 00:03:13.880]   So lines 1 through 10 are kind of the standard boilerplate.
[00:03:13.880 --> 00:03:19.660]   And then lines 12 and 13 is actually importing a library that I use to plot our predictions
[00:03:19.660 --> 00:03:22.960]   in this code.
[00:03:22.960 --> 00:03:28.800]   Line 19 here, I set a parameter called lookback, which is actually the size of the window that
[00:03:28.800 --> 00:03:29.800]   we're using.
[00:03:29.800 --> 00:03:36.780]   So we're taking 20 inputs, and we're predicting one output into the future.
[00:03:36.780 --> 00:03:42.560]   So we have a function here, load data, and that actually loads in the data from a CSV,
[00:03:42.560 --> 00:03:45.600]   international airline passengers.csv.
[00:03:45.600 --> 00:03:49.360]   You can easily add your own CSV file if you want to do time series prediction on some
[00:03:49.360 --> 00:03:51.200]   other data.
[00:03:51.200 --> 00:03:56.120]   Line 34 defines another function called create data set, and this actually does that thing
[00:03:56.120 --> 00:04:01.880]   I was talking about, taking an array of data and turning it into that matrix where you
[00:04:01.880 --> 00:04:08.520]   have inputs that are size lookback and an output that's size 1.
[00:04:08.520 --> 00:04:15.520]   So as usual, data x is the inputs and data y is the outputs here.
[00:04:15.520 --> 00:04:21.540]   So line 42 actually calls the load data function and puts the airline data into the data variable.
[00:04:21.540 --> 00:04:29.460]   So this is a single dimension output which corresponds to the airline sales at each month.
[00:04:29.460 --> 00:04:33.620]   And then if you remember from our Perceptron video, all of these things work better if
[00:04:33.620 --> 00:04:36.380]   you normalize the data to be between 0 and 1.
[00:04:36.380 --> 00:04:42.140]   And so I actually do that in lines 45, 46, and 47, just subtracting out the min value
[00:04:42.140 --> 00:04:45.340]   and then dividing by the difference between the max and the min.
[00:04:45.340 --> 00:04:51.380]   Now lines 50, 51, and 52, what they do is they split it into a test and train data set.
[00:04:51.380 --> 00:04:56.340]   And unlike with MNIST data or non-time series data sets, I think it's better to split it
[00:04:56.340 --> 00:04:57.340]   into chunks.
[00:04:57.340 --> 00:05:02.260]   So rather than randomly taking values, we take the first 70% of our data, which is maybe
[00:05:02.260 --> 00:05:05.420]   in the past, and we use that as the training.
[00:05:05.420 --> 00:05:09.500]   And then we take the final 30% of the data and we use that as the validation set.
[00:05:09.500 --> 00:05:13.220]   This is because what we'd really like to do from here is take our data set and predict
[00:05:13.220 --> 00:05:16.700]   it on data into the future.
[00:05:16.700 --> 00:05:21.320]   So lines 54 and 55 do that transformation I was talking about where we take the single
[00:05:21.320 --> 00:05:27.000]   dimension data and we turn it into a matrix where train x is the inputs and then train
[00:05:27.000 --> 00:05:28.820]   y is the output that we want.
[00:05:28.820 --> 00:05:33.780]   And here test x is the input on the test or validation data set and test y is the output
[00:05:33.780 --> 00:05:35.420]   that we're hoping for.
[00:05:35.420 --> 00:05:40.060]   Lines 57 and 58 add a new dimension to the data.
[00:05:40.060 --> 00:05:45.580]   This is similar to what we had to do to use convolutions on our data in a previous video.
[00:05:45.580 --> 00:05:50.380]   They basically just add an empty dimension to our matrix of data.
[00:05:50.380 --> 00:05:55.220]   So here we are in line 61 and we're going to finally create our neural network architecture.
[00:05:55.220 --> 00:05:58.940]   And it says RNN, but actually this isn't really a recursive neural network.
[00:05:58.940 --> 00:06:03.660]   This is the exact same perceptron that you would have seen in the first video.
[00:06:03.660 --> 00:06:08.220]   So just like we had to do in the first video, we add a flattened layer that takes our multidimensional
[00:06:08.220 --> 00:06:12.620]   data and smashes it down into a single dimensional input.
[00:06:12.620 --> 00:06:18.900]   And then line 63 adds a single perceptron that outputs a single value based on the input
[00:06:18.900 --> 00:06:20.260]   data.
[00:06:20.260 --> 00:06:23.460]   And in this case, our activation function is linear, but we could certainly change it
[00:06:23.460 --> 00:06:25.660]   to something else.
[00:06:25.660 --> 00:06:28.420]   Line 64 compiles that.
[00:06:28.420 --> 00:06:30.800]   Here we use our atom optimizer as usual.
[00:06:30.800 --> 00:06:33.920]   We actually don't use categorical cross entropy as our loss.
[00:06:33.920 --> 00:06:35.900]   You might want to stop and think about why.
[00:06:35.900 --> 00:06:39.980]   Do you remember why?
[00:06:39.980 --> 00:06:44.540]   Well we don't use categorical, excuse me, we don't use categorical cross entropy for
[00:06:44.540 --> 00:06:48.200]   our loss because we're actually predicting a scalar value.
[00:06:48.200 --> 00:06:49.440]   We're not predicting a category.
[00:06:49.440 --> 00:06:53.020]   So in this case, because we're predicting a number, we use mean squared error, but you
[00:06:53.020 --> 00:06:57.100]   could try something else like mean absolute error if you wanted to.
[00:06:57.100 --> 00:07:00.780]   And our final line, 65, actually does this fit.
[00:07:00.780 --> 00:07:04.220]   So here train x as usual is the input data.
[00:07:04.220 --> 00:07:08.860]   Train y is the labels we want to predict or the future value of this time series.
[00:07:08.860 --> 00:07:12.460]   We're going to train it for 1,000 epics because we have a pretty small data set.
[00:07:12.460 --> 00:07:14.860]   And we're going to use a batch size of 10.
[00:07:14.860 --> 00:07:18.380]   We also pass in our validation data, test x and test y.
[00:07:18.380 --> 00:07:23.220]   And then we have some functions to actually plot what's happening as the system runs.
[00:07:23.220 --> 00:07:28.300]   So you can go back into your terminal and you can run python perceptron.py.
[00:07:28.300 --> 00:07:30.780]   So you can watch it run here.
[00:07:30.780 --> 00:07:34.340]   It's super, super fast because the perceptron is a small amount of input data.
[00:07:34.340 --> 00:07:35.540]   Check this out.
[00:07:35.540 --> 00:07:41.060]   So here the blue line is the training data and the orange line is the test data that
[00:07:41.060 --> 00:07:42.600]   we held out.
[00:07:42.600 --> 00:07:45.620]   And the green line is the prediction that our system is making.
[00:07:45.620 --> 00:07:48.620]   And you can see that our predictions here are pretty reasonable.
[00:07:48.620 --> 00:07:53.820]   So the way we do these predictions is we keep feeding in the predicted input as input into
[00:07:53.820 --> 00:07:54.980]   the next prediction.
[00:07:54.980 --> 00:07:57.980]   So if we kind of get off, these things can go haywire.
[00:07:57.980 --> 00:08:03.020]   But in this case, our little perceptron with a small window of past data is actually doing
[00:08:03.020 --> 00:08:07.140]   a pretty reasonable job of forecasting airline predictions.
[00:08:07.140 --> 00:08:09.860]   So we're going to make this thing more complicated.
[00:08:09.860 --> 00:08:13.580]   But actually, this is not a bad result on a data set like this.
[00:08:13.580 --> 00:08:17.420]   And you could really use this in production on certain types of time series data.
[00:08:17.420 --> 00:08:18.420]   OK.
[00:08:18.420 --> 00:08:22.700]   But you guys didn't come to this video to learn about using perceptrons on time series
[00:08:22.700 --> 00:08:23.700]   data.
[00:08:23.700 --> 00:08:27.140]   I'm sure that what you care about is recursive neural networks.
[00:08:27.140 --> 00:08:30.820]   But you might stop and think, why do we use recursive neural networks at all?
[00:08:30.820 --> 00:08:35.500]   What's better about a recursive neural network than a dense perceptron?
[00:08:35.500 --> 00:08:37.740]   What is it missing?
[00:08:37.740 --> 00:08:43.220]   And I think what these perceptrons are missing is the element of time.
[00:08:43.220 --> 00:08:50.180]   So if you scrambled the inputs from 1 to 20 in the lookback that we fed into this perceptron,
[00:08:50.180 --> 00:08:53.460]   it wouldn't make any difference in the accuracy of the algorithm.
[00:08:53.460 --> 00:08:57.540]   If we scrambled the past, it has no effect on the prediction.
[00:08:57.540 --> 00:09:00.060]   So we're putting a lot of work into our perceptron.
[00:09:00.060 --> 00:09:03.980]   We're making it learn, actually, causality of time in a way.
[00:09:03.980 --> 00:09:04.980]   So there's lots of parameters.
[00:09:04.980 --> 00:09:08.460]   And on the small data set, it works OK.
[00:09:08.460 --> 00:09:11.060]   But it struggles on bigger, more complicated data sets.
[00:09:11.060 --> 00:09:15.500]   And any time you can put some knowledge that you have about the world into the architecture
[00:09:15.500 --> 00:09:20.420]   of your model, it's going to generally make the models do better, especially when the
[00:09:20.420 --> 00:09:23.660]   data sets get more complicated.
[00:09:23.660 --> 00:09:28.740]   Recursive neural networks, generally, they take the same kind of input as the dense neural
[00:09:28.740 --> 00:09:29.740]   network.
[00:09:29.740 --> 00:09:36.260]   So they take in a set of data over time in numbers or vectors of numbers.
[00:09:36.260 --> 00:09:41.380]   And they output a single number or a list of numbers over time.
[00:09:41.380 --> 00:09:42.740]   But now here's the difference.
[00:09:42.740 --> 00:09:46.820]   They actually keep a state that they pass through to themselves.
[00:09:46.820 --> 00:09:51.540]   So this is a diagram of a simple recursive neural network.
[00:09:51.540 --> 00:09:55.860]   And it's basically taking a state from the previous neural network, and it's outputting
[00:09:55.860 --> 00:09:56.860]   something.
[00:09:56.860 --> 00:10:00.780]   Now, what happens inside of these recursive neural networks is different depending on
[00:10:00.780 --> 00:10:01.980]   the type that you use.
[00:10:01.980 --> 00:10:05.180]   So in Keras, you'll have a simple recursive neural network.
[00:10:05.180 --> 00:10:06.180]   You'll have an LSTM.
[00:10:06.180 --> 00:10:07.900]   And you'll also have a GRU.
[00:10:07.900 --> 00:10:11.780]   Those tend to be the most common neural networks that you'll see in the wild.
[00:10:11.780 --> 00:10:13.940]   So let's start with the simple recursive neural network.
[00:10:13.940 --> 00:10:15.500]   How does that work?
[00:10:15.500 --> 00:10:18.780]   So our simple recursive neural network, it takes in an input.
[00:10:18.780 --> 00:10:23.180]   In this case, it's a single number, but it could be a larger dimensional thing.
[00:10:23.180 --> 00:10:24.780]   And it also passes through some state.
[00:10:24.780 --> 00:10:28.720]   And also in this case, it's a single number, but it could be larger later.
[00:10:28.720 --> 00:10:35.220]   So now it has two inputs, one from the outside and one from its previous self.
[00:10:35.220 --> 00:10:40.120]   It takes those two numbers, and it actually does the exact same calculation as a perceptron,
[00:10:40.120 --> 00:10:42.960]   so weighted sum with an activation function.
[00:10:42.960 --> 00:10:46.200]   And it outputs a single number.
[00:10:46.200 --> 00:10:52.040]   It then takes its output, and it passes it into the next recursive neural network.
[00:10:52.040 --> 00:10:57.320]   And now that network takes in a 4, and it also takes in the output from the previous
[00:10:57.320 --> 00:10:58.320]   one.
[00:10:58.320 --> 00:11:01.840]   And it does the exact same calculation with the same weights.
[00:11:01.840 --> 00:11:05.480]   And it does that 10 times or 20 times, or as long as our window is.
[00:11:05.480 --> 00:11:07.900]   And at the end, it outputs a number.
[00:11:07.900 --> 00:11:12.800]   And we take that output to be its prediction of the next value.
[00:11:12.800 --> 00:11:18.120]   And we can do all the same things we did with a perceptron or a CNN, where we do back propagation.
[00:11:18.120 --> 00:11:20.880]   In this case, it's called back propagation through time.
[00:11:20.880 --> 00:11:26.140]   And we find the best set of parameters to make this output prediction exactly what we
[00:11:26.140 --> 00:11:27.420]   want it to be.
[00:11:27.420 --> 00:11:29.800]   So let's see how this looks in the code.
[00:11:29.800 --> 00:11:33.760]   So that was a lot to take at once, and you might have missed a little of that.
[00:11:33.760 --> 00:11:38.840]   But it's actually very easy for us to swap in a simple RNN for the perceptron that we
[00:11:38.840 --> 00:11:39.840]   had.
[00:11:39.840 --> 00:11:44.280]   You can open up RNN.py, and you'll see that there's only one small change I made.
[00:11:44.280 --> 00:11:49.260]   Before we had flatten, and we had a dense layer, I added simple RNN.
[00:11:49.260 --> 00:11:51.140]   And then I have this one number here.
[00:11:51.140 --> 00:11:55.580]   And what that one number means is that its output, and also the thing that is passing
[00:11:55.580 --> 00:11:58.480]   from cell to cell, is a single dimensional thing.
[00:11:58.480 --> 00:12:02.180]   And that's important because our output dimension is actually a scalar.
[00:12:02.180 --> 00:12:05.140]   It's a one dimensional number at each time step.
[00:12:05.140 --> 00:12:10.500]   So we can run this RNN by typing python rnn.py.
[00:12:10.500 --> 00:12:14.620]   And I also save this to save us time.
[00:12:14.620 --> 00:12:20.380]   So here we have our blue line is the training data of this airline time series.
[00:12:20.380 --> 00:12:23.380]   And then the orange line here is the actual data.
[00:12:23.380 --> 00:12:25.500]   And the green line is our prediction of the RNN.
[00:12:25.500 --> 00:12:28.860]   And you can see that this prediction is a lot, lot worse.
[00:12:28.860 --> 00:12:33.100]   It's not predicting anything like what the data shows us.
[00:12:33.100 --> 00:12:36.620]   So here's a great case where we can look at the loss, and we can look at the validation
[00:12:36.620 --> 00:12:37.620]   loss.
[00:12:37.620 --> 00:12:39.660]   And we can see that those are both improving.
[00:12:39.660 --> 00:12:41.700]   So we can let this thing run for a while.
[00:12:41.700 --> 00:12:45.940]   But actually what happens if we run this over time is that it never learns to actually fit
[00:12:45.940 --> 00:12:47.460]   our data.
[00:12:47.460 --> 00:12:50.260]   So here's what other videos don't show you.
[00:12:50.260 --> 00:12:53.220]   This is a non-working recursive neural network.
[00:12:53.220 --> 00:12:56.780]   And so what I'm going to show you here, which I think is really important, is how to debug
[00:12:56.780 --> 00:12:59.100]   this problem and how to fix it.
[00:12:59.100 --> 00:13:03.540]   So one thing I like to do when I'm dealing with a broken neural network is I like to
[00:13:03.540 --> 00:13:05.940]   run it on really, really simple data.
[00:13:05.940 --> 00:13:09.260]   So how could we make this airline data even simpler?
[00:13:09.260 --> 00:13:12.140]   Well, one way is to use synthetic data.
[00:13:12.140 --> 00:13:17.340]   So I actually have a little program make-sine.py where I just output a sine wave.
[00:13:17.340 --> 00:13:22.340]   I just want to see can this neural network model an actual sine wave.
[00:13:22.340 --> 00:13:29.740]   So if you go in and you change this load data thing to take in a parameter sine, S-I-N,
[00:13:29.740 --> 00:13:33.460]   now our model is trying to model a sine wave.
[00:13:33.460 --> 00:13:38.180]   This seems like it should be almost the easiest time series data to model.
[00:13:38.180 --> 00:13:42.420]   So let's make that change, and let's run our program.
[00:13:42.420 --> 00:13:44.500]   Whoa.
[00:13:44.500 --> 00:13:48.980]   And so you can actually see here that this neural network is not modeling a sine wave
[00:13:48.980 --> 00:13:49.980]   at all.
[00:13:49.980 --> 00:13:53.660]   It's actually predicting negative 1 for all these values of the sine wave, which it never
[00:13:53.660 --> 00:13:55.460]   even sees in the data.
[00:13:55.460 --> 00:13:59.300]   So first of all, how is this thing even predicting a negative 1?
[00:13:59.300 --> 00:14:02.220]   How is that even a possible prediction that this thing could make?
[00:14:02.220 --> 00:14:06.140]   And it actually reminds me that I forgot to tell you something about these neural networks,
[00:14:06.140 --> 00:14:10.100]   which is they have a new activation function, typically.
[00:14:10.100 --> 00:14:11.660]   And we'll get into why that is.
[00:14:11.660 --> 00:14:16.060]   But this activation function is called a hyperbolic tangent.
[00:14:16.060 --> 00:14:22.260]   So I don't know how many people remember hyperbolic tangent from maybe their trigonometry class.
[00:14:22.260 --> 00:14:27.340]   I'm not sure I had thought about it much until I saw it appear in a neural network.
[00:14:27.340 --> 00:14:32.060]   The important thing to know about hyperbolic tangent is that it's basically like a sigmoid
[00:14:32.060 --> 00:14:37.300]   function, but instead of going from 0 to 1, it goes from negative 1 to 1.
[00:14:37.300 --> 00:14:41.500]   So in this case, a really, really negative number will output a negative 1, and a really,
[00:14:41.500 --> 00:14:44.860]   really positive number will output a positive 1.
[00:14:44.860 --> 00:14:50.060]   And LSTMs and GRUs, they use hyperbolic tangent activation function like crazy.
[00:14:50.060 --> 00:14:56.500]   So Keras actually makes the default activation function this hyperbolic tangent, even though
[00:14:56.500 --> 00:14:59.500]   it's a simple RNN, and it probably doesn't really need to have that.
[00:14:59.500 --> 00:15:05.700]   But now, the big issue here, the reason that this RNN can't learn such a simple thing is
[00:15:05.700 --> 00:15:09.340]   that it's actually only passing across a single parameter.
[00:15:09.340 --> 00:15:15.300]   So actually, just passing one number from point in time to point in time is not enough
[00:15:15.300 --> 00:15:19.540]   to even learn a pattern as simple as a sine wave function.
[00:15:19.540 --> 00:15:24.240]   So what we need to do is we need to let it pass through more state, more than just one
[00:15:24.240 --> 00:15:25.920]   single number.
[00:15:25.920 --> 00:15:30.800]   So we can actually do that by changing the 1 into a higher number.
[00:15:30.800 --> 00:15:32.060]   So let's do that in the code.
[00:15:32.060 --> 00:15:37.460]   So here on line 62, where we see simple RNN 1, let's try changing that to a 5.
[00:15:37.460 --> 00:15:41.180]   So now it's going to pass across five numbers instead of one.
[00:15:41.180 --> 00:15:45.820]   And maybe that can encode the state of affairs.
[00:15:45.820 --> 00:15:53.380]   Ah, so we get an error.
[00:15:53.380 --> 00:15:57.100]   This is what the other videos don't show you, these errors.
[00:15:57.100 --> 00:16:01.300]   So I'm going to debug this with you, but you might want to stop the video and think about
[00:16:01.300 --> 00:16:02.620]   why we got this error.
[00:16:02.620 --> 00:16:07.020]   Because this is a really common error to get when dealing with neural networks of any type.
[00:16:07.020 --> 00:16:08.660]   It's a dimension error.
[00:16:08.660 --> 00:16:17.460]   So it was expecting simple RNN to have shape 5, but it got an array with shape 1.
[00:16:17.460 --> 00:16:21.660]   So what happened here?
[00:16:21.660 --> 00:16:23.660]   So what's going on?
[00:16:23.660 --> 00:16:27.820]   So we're outputting a five-dimensional thing, but actually-- because if you look at this
[00:16:27.820 --> 00:16:33.980]   diagram, so simple RNN, it actually outputs the same thing that it's sending to the next
[00:16:33.980 --> 00:16:37.100]   cell in the recursive neural network.
[00:16:37.100 --> 00:16:40.220]   And so it's outputting a five-dimensional thing, and we can't use that.
[00:16:40.220 --> 00:16:43.500]   So how do we turn this five-dimensional output into a single-dimensional output?
[00:16:43.500 --> 00:16:47.020]   Well, one way to do it is to actually just add a dense layer at the very end.
[00:16:47.020 --> 00:16:48.020]   And this is super common.
[00:16:48.020 --> 00:16:52.740]   So what this is going to do is it'll take the five numbers that this network outputs,
[00:16:52.740 --> 00:16:56.580]   and then it'll add a final perceptron that does a weighted sum that takes it down to
[00:16:56.580 --> 00:16:59.180]   a single output.
[00:16:59.180 --> 00:17:06.980]   And so we can do that here by just adding a line that says dense.
[00:17:06.980 --> 00:17:10.300]   So we add model.add dense1.
[00:17:10.300 --> 00:17:13.380]   And we could actually add a different activation function if we wanted to.
[00:17:13.380 --> 00:17:18.100]   So by default, it'll use a linear activation function, which will let it output any number.
[00:17:18.100 --> 00:17:20.860]   But we're trying to do a sine wave.
[00:17:20.860 --> 00:17:25.260]   We add a single line, model.add dense1, that adds our perception at the end.
[00:17:25.260 --> 00:17:28.380]   And we could actually add a different activation function here.
[00:17:28.380 --> 00:17:31.860]   In this case, our data is normalized to be between 0 and 1.
[00:17:31.860 --> 00:17:34.940]   So I think a sigmoid probably makes sense.
[00:17:34.940 --> 00:17:40.140]   So let's say activation equals sigmoid.
[00:17:40.140 --> 00:17:42.420]   Awesome.
[00:17:42.420 --> 00:17:44.300]   Let's run this network.
[00:17:44.300 --> 00:17:48.300]   And so you can see at first, this network is actually doing already a much better job
[00:17:48.300 --> 00:17:49.580]   of modeling the sine wave.
[00:17:49.580 --> 00:17:53.820]   So at first, it seems to get it, but it kind of dampens over time.
[00:17:53.820 --> 00:17:56.220]   So it doesn't actually swing as much as the sine wave is swinging.
[00:17:56.220 --> 00:18:00.260]   And this sort of shows you actually how modeling time series is tougher than modeling other
[00:18:00.260 --> 00:18:05.220]   things because errors that you make in the beginning of predicting the future then feed
[00:18:05.220 --> 00:18:08.780]   in to your model and cause further errors as you predict out further.
[00:18:08.780 --> 00:18:12.220]   So this seems to start OK, but then get bad.
[00:18:12.220 --> 00:18:17.100]   But then after 100 epics, it's starting to really model the nature of the sine wave.
[00:18:17.100 --> 00:18:22.060]   So I think we can stop and say that this network architecture is working much better than the
[00:18:22.060 --> 00:18:23.940]   previous one that we had.
[00:18:23.940 --> 00:18:26.780]   And then we can take it and we can try it back on the airline data.
[00:18:26.780 --> 00:18:32.380]   So now that we've debugged our model, let's go back to the original data set by just removing
[00:18:32.380 --> 00:18:34.380]   the sign here.
[00:18:34.380 --> 00:18:35.740]   And let's try running it.
[00:18:35.740 --> 00:18:36.740]   Cool.
[00:18:36.740 --> 00:18:41.060]   And so you can actually see that this is starting to model this airline data much better.
[00:18:41.060 --> 00:18:45.140]   And over time, it gets better and better and captures more and more of the way that the
[00:18:45.140 --> 00:18:49.020]   airline sales data set actually really looks.
[00:18:49.020 --> 00:18:51.940]   And so I know you came here for an LSTM.
[00:18:51.940 --> 00:18:55.340]   And we're going to go deeper in the next video into how LSTMs really work.
[00:18:55.340 --> 00:18:59.980]   But I want to show you just as a taste of how easy it is to switch this simple RNN to
[00:18:59.980 --> 00:19:01.540]   an LSTM.
[00:19:01.540 --> 00:19:07.300]   So if we go back into the code, into RNN.py, all we need to do is change this layer where
[00:19:07.300 --> 00:19:12.420]   it says simple RNN to LSTM.
[00:19:12.420 --> 00:19:16.560]   Go into our terminal and we type python rnn.py.
[00:19:16.560 --> 00:19:19.820]   And we're running our first LSTM.
[00:19:19.820 --> 00:19:23.700]   One thing you'll notice about this LSTM right away is that it runs a lot slower than the
[00:19:23.700 --> 00:19:24.700]   simple RNN.
[00:19:24.700 --> 00:19:28.840]   And it runs much, much slower than the Perceptron.
[00:19:28.840 --> 00:19:31.140]   And that's because there's a lot going on here.
[00:19:31.140 --> 00:19:35.540]   And that power is going to be really important in some of the subsequent things that we talk
[00:19:35.540 --> 00:19:38.540]   about, especially when you run them on text data.
[00:19:38.540 --> 00:19:42.760]   Because if you think about it, one way to look at text data is just a really complicated
[00:19:42.760 --> 00:19:44.820]   type of time series data.
[00:19:44.820 --> 00:19:50.900]   Today we saw how to take a recurrent neural network and use it inside Keras on time series
[00:19:50.900 --> 00:19:51.980]   data.
[00:19:51.980 --> 00:19:56.540]   And we even saw a little bit about how to debug a recurrent neural network.
[00:19:56.540 --> 00:20:02.100]   And at the very end, we swapped in for our simple recurrent neural network in LSTM, which
[00:20:02.100 --> 00:20:04.900]   we're going to use quite a bit into the future.
[00:20:04.900 --> 00:20:10.220]   We also learned how to take data and turn it into the format that you need in order
[00:20:10.220 --> 00:20:14.860]   to apply any type of time series algorithm.
[00:20:14.860 --> 00:20:16.860]   [END]
[00:20:16.860 --> 00:20:26.860]   [BLANK_AUDIO]

