
[00:00:00.000 --> 00:00:06.680]   Yes, Kirijesh, I saw your notes. They were incredible. So I
[00:00:06.680 --> 00:00:10.260]   watched the recordings and I am slightly concerned by no one has
[00:00:10.260 --> 00:00:13.920]   commented this on the videos. But I look like a chipmunk
[00:00:13.920 --> 00:00:17.120]   rotating my head from left to right as I try to figure out if
[00:00:17.120 --> 00:00:19.880]   I'm live or not. So if you are watching the recording now you
[00:00:19.880 --> 00:00:28.200]   know why that happens. Ideally, it should work but it's always
[00:00:28.200 --> 00:00:34.040]   good to double check. All right, awesome. So I can hear an echo
[00:00:34.040 --> 00:00:36.800]   which means I'm live on YouTube. That's why I can hear myself
[00:00:36.800 --> 00:00:40.880]   talk. Looks like everything is perfect. Welcome back everyone.
[00:00:40.880 --> 00:00:45.400]   Good evening Adil. Great to have you. Hey, everyone. Great to see
[00:00:45.400 --> 00:00:51.000]   so many people joining us again. I'd like to say that there's
[00:00:51.000 --> 00:00:54.280]   always this drop off that happens in courses, especially
[00:00:54.280 --> 00:00:57.200]   online courses. I don't know about university. I did go to
[00:00:57.200 --> 00:01:00.680]   university. I'm not, I'm not a dropout. But with online
[00:01:00.680 --> 00:01:04.440]   courses, there's this drop off that happens. And the largest
[00:01:04.440 --> 00:01:07.320]   drop off happens right after the first lecture. So if you've made
[00:01:07.320 --> 00:01:10.440]   it, or if you're watching the recording that still counts.
[00:01:10.440 --> 00:01:14.080]   You've made across the biggest barrier, which is the second
[00:01:14.080 --> 00:01:18.160]   session. So thanks for coming back. We have a lot of exciting
[00:01:18.160 --> 00:01:21.720]   stuff to dive into. So I'll share my screen and get to it.
[00:01:26.160 --> 00:01:28.520]   While I figure out which screen to share.
[00:01:28.520 --> 00:01:31.160]   And I did.
[00:01:31.160 --> 00:01:36.920]   Okay, let me move out the zoom things and hit present.
[00:01:36.920 --> 00:01:41.400]   So welcome back. This is the third session, officially the
[00:01:41.400 --> 00:01:45.000]   second session since the first one was session zero where we
[00:01:45.000 --> 00:01:50.320]   learned from the co author Thomas Freeman also co developer
[00:01:50.320 --> 00:01:53.400]   of PyTorch. He shared his incredible wisdom, a lot of
[00:01:53.400 --> 00:01:58.560]   things to learn. Last week, we had an ambitious agenda also as
[00:01:58.560 --> 00:02:01.480]   a little sick, as you can tell, I'm fully recovered. Thanks. A
[00:02:01.480 --> 00:02:04.600]   few people were asking that, which means hopefully I can pick
[00:02:04.600 --> 00:02:07.880]   up the pace, but we tried to cover PyTorch basics, we did get
[00:02:07.880 --> 00:02:11.680]   to a bit of tensors. And we need to get to neural networks in
[00:02:11.680 --> 00:02:15.800]   PyTorch. I'm also not trying to rush through these topics. So
[00:02:15.800 --> 00:02:20.200]   I'll set up an agenda. But if there are enough questions that
[00:02:20.200 --> 00:02:23.800]   need to be answered or enough discussions that need to be
[00:02:23.800 --> 00:02:27.720]   taken care of, my focus would be making sure that those are
[00:02:27.720 --> 00:02:32.560]   answered instead of just rushing through all of the chapters. So
[00:02:32.560 --> 00:02:37.840]   I'd like to point this out this link. And let me put it in the
[00:02:37.840 --> 00:02:40.120]   zoom chart as well as the YouTube chart.
[00:02:40.120 --> 00:02:47.840]   So if you all could please head over to this link, the reason
[00:02:47.840 --> 00:02:53.960]   we're using this is because as you can see, I'm already
[00:02:53.960 --> 00:02:58.280]   fumbling with live on YouTube. I'm also in a zoom call and
[00:02:58.280 --> 00:03:03.400]   it's really tough to monitor both of them. So if you head
[00:03:03.400 --> 00:03:06.840]   over to that link, it should redirect you to this write up.
[00:03:06.840 --> 00:03:10.320]   And you all can start asking your questions here. You can
[00:03:10.400 --> 00:03:19.880]   quickly hit reply and I was going to write something nasty
[00:03:19.880 --> 00:03:24.320]   against TensorFlow. Let me not do that. What are the
[00:03:24.320 --> 00:03:28.760]   TorchTensors? Feel free to ask any questions. And I'll keep
[00:03:28.760 --> 00:03:34.080]   looking here from time to time. Also note, this is a wiki. So
[00:03:34.080 --> 00:03:38.880]   what we're using here is discourse. And I'd request you
[00:03:38.880 --> 00:03:41.560]   all especially the ones who are watching live, I'll keep
[00:03:41.560 --> 00:03:44.520]   pointing out resources. And if you feel anything should be
[00:03:44.520 --> 00:03:48.360]   added here for people watching later, I really appreciate it.
[00:03:48.360 --> 00:03:52.200]   If you could click edit and just add those links here. This uses
[00:03:52.200 --> 00:03:54.720]   markdown so it should be straightforward. There's no
[00:03:54.720 --> 00:03:58.480]   obligation to do it. I'll be doing it later on anyways. But
[00:03:58.480 --> 00:04:02.520]   if you'd like if you'd like to help out the community, and
[00:04:02.520 --> 00:04:06.040]   that's the right answer. Greedy as hell. And you can like each
[00:04:06.040 --> 00:04:10.160]   other's answers as you can see. If you'd like to help the
[00:04:10.160 --> 00:04:13.560]   community out, please feel free to edit and add these resources.
[00:04:13.560 --> 00:04:17.320]   Just wanted to point that out. And as a reminder, I'll drop the
[00:04:17.320 --> 00:04:29.320]   link again. Input the charts. And to people who will be
[00:04:29.320 --> 00:04:32.280]   sticking through if anyone asks any questions, please make sure
[00:04:32.280 --> 00:04:36.000]   you redirect them to that. One thing that really upsets me is
[00:04:36.680 --> 00:04:40.440]   people not getting their answers and I want to make sure they get
[00:04:40.440 --> 00:04:43.480]   their answers. So I really wanted to emphasize on this. I
[00:04:43.480 --> 00:04:49.600]   want to point out this blog post by Gary J's. He's Gary J's
[00:04:49.600 --> 00:04:54.000]   underscore on Twitter. That's his username. And he has
[00:04:54.000 --> 00:04:56.920]   written two really nice summaries. So I'd point you to
[00:04:56.920 --> 00:05:01.760]   that instead of summarizing them myself. I'm quite the lazy guy
[00:05:01.760 --> 00:05:04.520]   as you can see. So please feel free to go through them. I read
[00:05:04.560 --> 00:05:06.920]   I quickly skimmed through them and these are very well written.
[00:05:06.920 --> 00:05:11.000]   He's looking into how to fix these image render issue. But
[00:05:11.000 --> 00:05:15.040]   apart from that, it's quite a crisp and nice summary of the
[00:05:15.040 --> 00:05:18.480]   sessions. Thanks. Thanks for this writer. Please feel free to
[00:05:18.480 --> 00:05:21.840]   add it to the discussion. BK if you'd like to share it with the
[00:05:21.840 --> 00:05:27.080]   readers. I also want to mention that Twitter is an incredible
[00:05:27.080 --> 00:05:30.880]   place to first of all share your content. And when I say your
[00:05:30.880 --> 00:05:35.280]   content, I mean, as we go through this book, you all
[00:05:35.280 --> 00:05:40.160]   should be writing blog posts or creating GitHub repositories.
[00:05:40.160 --> 00:05:46.000]   Why? Because that that becomes your resume. In a way. As you do
[00:05:46.000 --> 00:05:48.640]   that, let me stop sharing my screen so that I can emphasize
[00:05:48.640 --> 00:05:54.400]   on my face. But as you do that, it becomes a signal to people in
[00:05:54.400 --> 00:05:57.240]   the industry. I can speak from my experience. It's been
[00:05:57.240 --> 00:06:01.720]   incredible to just share this across. And anyone can quickly
[00:06:01.720 --> 00:06:05.840]   look at your blog and understand your depth of knowledge. It's
[00:06:05.840 --> 00:06:08.720]   also really helpful in solidifying what you've learned.
[00:06:08.720 --> 00:06:15.000]   So at least in these chapters, the first few will be a shortage
[00:06:15.000 --> 00:06:17.720]   of content, but you can definitely write about a few
[00:06:17.720 --> 00:06:20.840]   things. And I'll keep giving out pointers about things you can
[00:06:20.840 --> 00:06:26.080]   write off. My only request is please consider doing that
[00:06:26.080 --> 00:06:28.800]   because at least for me that has really changed my career
[00:06:28.800 --> 00:06:31.480]   trajectory with the things I've been doing, especially writing
[00:06:31.480 --> 00:06:36.160]   blog posts and later now, creating videos. I quite enjoy
[00:06:36.160 --> 00:06:38.920]   it. As you can see, this is being done on a Sunday evening.
[00:06:38.920 --> 00:06:44.000]   I would encourage everyone to take a look in that direction at
[00:06:44.000 --> 00:06:48.520]   least. So Twitter is a really incredible place to share what
[00:06:48.520 --> 00:06:52.560]   you've been learning and just interact with people. I tend to
[00:06:52.560 --> 00:06:55.640]   share a lot of PyTorch resources, you can connect with
[00:06:55.640 --> 00:06:58.640]   me on Twitter as well. The last few days have just been
[00:06:58.640 --> 00:07:02.480]   PyTorch resources. I wanted to point that out. But apart from
[00:07:02.480 --> 00:07:06.480]   that, feel free to connect with each other. Via the forums, you
[00:07:06.480 --> 00:07:09.080]   can share each other's Twitters and you can learn from each
[00:07:09.080 --> 00:07:14.560]   other through there. Awesome. I see a few answers already.
[00:07:14.560 --> 00:07:18.480]   That's really great. And I'll come to your question Shashank in
[00:07:18.480 --> 00:07:25.400]   a minute. So for today, we're, I'm aiming to go through PyTorch
[00:07:25.400 --> 00:07:30.840]   tensors. I want to wrap that up. We had left that topic in the
[00:07:30.840 --> 00:07:39.080]   last session. Let me take a sip of tea real quick. While I try
[00:07:39.080 --> 00:07:44.120]   not to burn my tongue. We'll try to wrap up the tensors chapter,
[00:07:44.120 --> 00:07:47.480]   continue with the autograd and then try to cover neural
[00:07:47.480 --> 00:07:51.600]   networks. We might not be able to that simply because I don't
[00:07:51.600 --> 00:07:55.280]   want to rush through anything. These are basic topics. Basic
[00:07:55.880 --> 00:07:58.120]   but I really want to make sure that everyone gets a good
[00:07:58.120 --> 00:08:01.800]   understanding. So you can always watch the recording and to
[00:08:01.800 --> 00:08:05.920]   experience if these are really easy topics for you, but I'll
[00:08:05.920 --> 00:08:12.920]   initially go slowly through them. Sorry. So the agenda for
[00:08:12.920 --> 00:08:16.160]   today is to quickly recap what we learned from the last session.
[00:08:16.160 --> 00:08:21.080]   glance through chapter three and five. I would like to spend a
[00:08:21.080 --> 00:08:25.000]   significant time understanding the code in these notebooks
[00:08:25.080 --> 00:08:29.200]   that the authors have shared on GitHub and towards actually not
[00:08:29.200 --> 00:08:32.240]   towards the end, I'll keep looking at Q&A. I'll also try to
[00:08:32.240 --> 00:08:37.240]   ask a few questions to you so that we can make this a little
[00:08:37.240 --> 00:08:40.360]   interactive even though it's being broadcast, but I'd like to
[00:08:40.360 --> 00:08:43.680]   do that. As a reminder, please ask the questions in this link.
[00:08:43.680 --> 00:08:53.320]   I'd like to start with one correction. My colleague kindly
[00:08:53.320 --> 00:08:56.480]   pointed this out, I skipped mentioning this little detail.
[00:08:56.480 --> 00:09:00.200]   But the main difference between deep learning and traditional
[00:09:00.200 --> 00:09:03.400]   machine learning is deep learning scales really well with
[00:09:03.400 --> 00:09:09.800]   more data. Now, what does that mean? As you tend to have bigger
[00:09:09.800 --> 00:09:14.000]   data sets, it works much better than traditional machine
[00:09:14.000 --> 00:09:17.440]   learning algorithms. Now going back to what the difference
[00:09:17.440 --> 00:09:19.960]   between these was, and again, I'm trying to recap what we
[00:09:19.960 --> 00:09:23.640]   learned in the last discussion. Traditional machine learning
[00:09:23.640 --> 00:09:27.000]   involved a lot of hand crafting of features we looked at last
[00:09:27.000 --> 00:09:30.440]   week, what OpenCV2 does, how does that pipeline look like?
[00:09:30.440 --> 00:09:35.640]   How many things do we have to fine man tune ourselves? We need
[00:09:35.640 --> 00:09:40.480]   to fine tune a lot of parameters manually. In deep learning,
[00:09:40.480 --> 00:09:43.240]   everything is taken care of. I mean, there are a few hyper
[00:09:43.240 --> 00:09:46.880]   parameters that we need to tune. And we need to define a few
[00:09:46.880 --> 00:09:51.080]   things. Yes, but those are much, much, much lesser than machine
[00:09:51.080 --> 00:09:55.720]   learning. The correction I'm pointing out here is again, deep
[00:09:55.720 --> 00:09:59.440]   learning works better with more data, not always, not always, of
[00:09:59.440 --> 00:10:02.080]   course, the data needs to be in a certain format. We learned
[00:10:02.080 --> 00:10:08.920]   that today. But usually, that statement holds true. As a
[00:10:08.920 --> 00:10:14.480]   reminder, what is a model? We looked at it as a black box of
[00:10:14.480 --> 00:10:17.800]   sorts, it has these millions of numbers, I gave an example of
[00:10:17.800 --> 00:10:23.600]   these being knobs that are being adjusted automatically, you
[00:10:23.600 --> 00:10:27.360]   have an input going in and you have an output going out. So
[00:10:27.360 --> 00:10:31.640]   that was a very high level, not satisfactory description I had
[00:10:31.640 --> 00:10:37.600]   given. And again, I'm trying to convert this into a format of
[00:10:37.600 --> 00:10:40.880]   learning that I really enjoy, which is the top down learning
[00:10:40.880 --> 00:10:45.560]   fashion, which means that you won't be walking away every week
[00:10:45.560 --> 00:10:49.960]   with a complete understanding of everything. But I'll try to give
[00:10:49.960 --> 00:10:55.320]   out directions or suggestions of things you should practice that
[00:10:55.320 --> 00:10:58.880]   in my opinion should help you become a better PyTorch
[00:10:58.880 --> 00:11:01.560]   practitioner, which is why I believe all of us are here for.
[00:11:01.560 --> 00:11:09.040]   So we'll be continuing with tensors. And let me quickly hop
[00:11:09.040 --> 00:11:13.400]   over to the forums to make sure if there are any questions from
[00:11:13.400 --> 00:11:22.080]   the last session. So Shashank, it's the same thing. fine
[00:11:22.080 --> 00:11:25.480]   tuning happens under transfer learning. So when we're doing
[00:11:25.480 --> 00:11:30.080]   transfer learning, we have this model that has been trained on a
[00:11:30.080 --> 00:11:34.880]   certain data set. We bring it on, we take that knowledge from
[00:11:34.880 --> 00:11:39.440]   the model and apply it to a new area. That's why we transfer its
[00:11:39.440 --> 00:11:44.720]   knowledge. Fine tuning is when we are transferring this
[00:11:44.720 --> 00:11:50.160]   knowledge, we can also train the model again, on a newer data set
[00:11:50.160 --> 00:11:55.000]   or a target problem that is known as fine tuning. If that
[00:11:55.000 --> 00:11:58.400]   didn't make sense, don't go into that depth. You don't need to
[00:11:58.400 --> 00:12:04.080]   know the theory as we do these things as we apply these, you'll
[00:12:04.080 --> 00:12:05.680]   have a much better understanding.
[00:12:05.680 --> 00:12:13.200]   Um, Gautam is asking deep learning requires lesser feature
[00:12:13.200 --> 00:12:18.040]   engineering. So I wasn't when I said feature engineering, I
[00:12:18.040 --> 00:12:22.760]   meant handcrafted features. So I was I'm not sure if it
[00:12:22.760 --> 00:12:25.880]   classifies as feature engineering for images as well.
[00:12:25.880 --> 00:12:29.040]   I'm thinking it might be the technical term would be
[00:12:29.040 --> 00:12:34.000]   pre-processing. But for tabular data, of course, you still
[00:12:34.000 --> 00:12:39.760]   need to do a few things for deep learning. And I believe deep
[00:12:39.760 --> 00:12:43.480]   learning still needs that revolution in tabular data. It's
[00:12:43.480 --> 00:12:46.800]   still not that well understood how can we apply all of these
[00:12:46.800 --> 00:12:55.880]   things. Let me stop sharing and share my screen to the book.
[00:12:55.880 --> 00:13:03.600]   I'd like to first explain broadcasting. Make sure I have
[00:13:03.600 --> 00:13:09.600]   the pen with me. Yes. Please excuse this guys. I have a lot
[00:13:09.600 --> 00:13:13.360]   of things on my desk and I'm trying to make sure I don't drop
[00:13:13.360 --> 00:13:17.040]   anything, especially the monitor. So I'm sorry, I'm not
[00:13:17.040 --> 00:13:20.560]   explaining broadcasting. I'm explaining slicing on paper
[00:13:20.560 --> 00:13:29.720]   first. So this is how you usually index into tensors. This
[00:13:29.720 --> 00:13:33.280]   is the row, this is the column. So let's say you have a tensor
[00:13:33.280 --> 00:13:50.600]   of three by three length. If you go a zero comma one, this is
[00:13:50.600 --> 00:13:55.760]   our tensor zero comma one would be this one. So we'll return
[00:13:55.760 --> 00:13:59.400]   this value when we try to index like so this is known as
[00:13:59.400 --> 00:14:03.520]   indexing. We'll also learn advanced indexing today. It's
[00:14:03.520 --> 00:14:06.880]   nothing too fancy. You can you can also pass Boolean values
[00:14:06.880 --> 00:14:10.360]   here so you can pass true or false. And that does a few
[00:14:10.360 --> 00:14:15.320]   things as we learn. So what slicing does is it's this fancy
[00:14:15.320 --> 00:14:24.320]   way of selecting a subset inside of our tensor. If I pass a colon
[00:14:24.720 --> 00:14:32.840]   comma column name, how I look at it is if I have x colon, this
[00:14:32.840 --> 00:14:36.240]   acts as a stopper. So this is this is my way of remembering
[00:14:36.240 --> 00:14:39.800]   this, you can ignore it totally. But wherever the number is that
[00:14:39.800 --> 00:14:42.320]   is the wall beyond which everything gets picked. That's
[00:14:42.320 --> 00:14:45.080]   how I remember it. I'm sorry, that's the explanation I have if
[00:14:45.080 --> 00:14:48.160]   you didn't like it. And everything beyond this gets
[00:14:48.160 --> 00:14:52.240]   picked. If it's like so everything before this gets
[00:14:52.240 --> 00:14:58.320]   picked. If there are two numbers, it becomes everything
[00:14:58.320 --> 00:15:04.360]   in between them. And if there's nothing, so how I remember it
[00:15:04.360 --> 00:15:08.680]   in my head, there's nothing to stop this slicing. So when we're
[00:15:08.680 --> 00:15:11.040]   slicing, this is how I remember it in my head when we're
[00:15:11.040 --> 00:15:14.760]   slicing into a tensor, we're cutting through it. And there's
[00:15:14.760 --> 00:15:17.840]   nothing to stop our knife. So essentially everything's good,
[00:15:17.880 --> 00:15:25.640]   everything gets selected. So in this case, all elements from
[00:15:25.640 --> 00:15:29.720]   here will get selected. That's that's all you need to know
[00:15:29.720 --> 00:15:33.640]   about how slicing works. And you'll get to see a lot of it,
[00:15:33.640 --> 00:15:38.880]   especially as you go into a lot of implementations, you'll see a
[00:15:38.880 --> 00:15:43.280]   lot of things. Usually it's not involving specific numbers
[00:15:43.280 --> 00:15:48.400]   because it's supposed to be generally generalistic. A lot of
[00:15:48.400 --> 00:15:56.880]   what you'll see will be unsqueeze blah, it'll also be,
[00:15:56.880 --> 00:16:04.000]   let's say, like, so selecting everything along that channel,
[00:16:04.000 --> 00:16:10.960]   so on and so forth. So quickly wanted to point that out. This
[00:16:10.960 --> 00:16:17.120]   was a quick way of recapping how slicing works. Let me share my
[00:16:17.120 --> 00:16:30.760]   screen again and make sure I picked the right notebook. So
[00:16:30.760 --> 00:16:34.720]   I've downloaded the repository and this is from the authors
[00:16:34.720 --> 00:16:37.920]   themselves, you can again find the resource in the discourse
[00:16:37.920 --> 00:16:47.760]   thread that I had shared. And we were at chapter three looking
[00:16:47.760 --> 00:16:53.600]   at tensors. So I'll start by recapping this again. You can
[00:16:53.600 --> 00:16:56.640]   start by creating a list as as you can see, this is simply a
[00:16:56.640 --> 00:17:00.160]   list you can index into a list. So this is Python list. This has
[00:17:00.160 --> 00:17:05.080]   nothing to do with PyTorch at this moment. You can change its
[00:17:05.520 --> 00:17:08.680]   variables like so selecting the index assign a value to it.
[00:17:08.680 --> 00:17:15.800]   There are different ways of creating tensors inside of
[00:17:15.800 --> 00:17:22.080]   PyTorch. So A now becomes a tensor. If I print out its type,
[00:17:22.080 --> 00:17:31.880]   it's a tors.tensor. And what are we doing here? We're calling
[00:17:31.880 --> 00:17:34.560]   tors.once. I want to remind everyone of the shortcut, I
[00:17:34.560 --> 00:17:39.000]   press shift tab twice inside of a function that I want to know
[00:17:39.000 --> 00:17:43.160]   about that brings up its doc string as a scroll down value.
[00:17:43.160 --> 00:17:46.960]   So I can see this returns a tensor filled with a scalar
[00:17:46.960 --> 00:17:49.920]   value of one. So we looked at a bunch of function these source,
[00:17:49.920 --> 00:17:53.840]   these were tensor.once it creates a tensor of one value.
[00:17:53.840 --> 00:17:58.280]   We can also create a tensor by just calling tors.tensor passing
[00:17:58.280 --> 00:18:02.360]   values or a list to it. How do I know that? Again, there's
[00:18:02.360 --> 00:18:06.680]   another way of doing this. You can write two question marks.
[00:18:06.680 --> 00:18:16.320]   And don't put the brackets against it, I always mess that
[00:18:16.320 --> 00:18:19.960]   up. But this would also bring up the doc string. So shift tab or
[00:18:19.960 --> 00:18:25.960]   two question marks should do the trick. We looked at how can we
[00:18:25.960 --> 00:18:30.080]   convert it to floating point? How do we get the shape? How do
[00:18:30.080 --> 00:18:34.120]   we create a tensor filled with zero values? We looked at how
[00:18:34.120 --> 00:18:37.520]   are these stored in memory? What does stride mean? Are these
[00:18:37.520 --> 00:18:42.000]   continuously stored? How are these offset in memory? If it's
[00:18:42.000 --> 00:18:45.320]   floating points are stored with different offset compared to
[00:18:45.320 --> 00:18:54.480]   integers, let's say. We learned how can we clone a tensor? How
[00:18:54.480 --> 00:18:58.320]   can we copy it? So whenever we assign tensors, there's this
[00:18:58.320 --> 00:19:02.680]   problem that happens. Not always is a copy of it created.
[00:19:02.680 --> 00:19:08.120]   So if you don't clone it, sometimes you could, assumingly
[00:19:08.120 --> 00:19:11.320]   change second point and that would reflect in points as well,
[00:19:11.320 --> 00:19:15.480]   which is not what you want. So this is something you need to be
[00:19:15.480 --> 00:19:19.640]   cautious of. Now I was talking about writing blog posts and
[00:19:19.640 --> 00:19:23.520]   sharing stuff a minute ago. I know there's plenty of
[00:19:23.520 --> 00:19:28.400]   documentation alongside how do you create a tensor filled with
[00:19:28.400 --> 00:19:33.320]   one values? How do you store it? But just doing an example out
[00:19:33.320 --> 00:19:36.280]   there, you could write something about or just summarizing all of
[00:19:36.280 --> 00:19:40.280]   these functions. So in a quick cheat sheet or such a format,
[00:19:40.280 --> 00:19:44.240]   you could just write all of the torch tensor functions that you
[00:19:44.240 --> 00:19:47.200]   really like. And that's a good blog post, I would suggest as a
[00:19:47.200 --> 00:19:51.200]   starting point. Why am I emphasizing on this? At least
[00:19:51.200 --> 00:19:55.440]   when I was starting out, I was really worried about, I need to
[00:19:55.440 --> 00:19:59.320]   write about advanced things. And advanced things only not things
[00:19:59.320 --> 00:20:03.160]   that people have written about. That's not true. You can write
[00:20:03.160 --> 00:20:07.280]   about anything you need to add value. And I would find value in
[00:20:07.280 --> 00:20:10.360]   about reading all of these functions in a cheat sheet
[00:20:10.360 --> 00:20:23.800]   format. So we can see that some are stored contiguously, if you
[00:20:23.800 --> 00:20:28.640]   store a transpose of it, and I'll remind you all what
[00:20:28.640 --> 00:20:34.200]   transposes by painfully sharing and switching my screens. Sorry,
[00:20:34.200 --> 00:20:36.640]   guys, this always takes a minute. And it makes me nervous
[00:20:36.640 --> 00:20:39.760]   because I feel like I'm taking too much time. But
[00:20:39.760 --> 00:20:50.120]   row by column. So when we take a transpose, now this is almost
[00:20:50.120 --> 00:20:53.760]   like a square matrix, it wouldn't make sense here. But
[00:20:53.760 --> 00:20:57.060]   when we take a transpose, the rows become the columns and the
[00:20:57.060 --> 00:21:02.920]   columns become zero. So we just invert it in a way. Actually,
[00:21:02.920 --> 00:21:06.400]   there's more to it than that. But that's how I remember it is
[00:21:06.960 --> 00:21:13.040]   you just take every row and you flip it. So this first row
[00:21:13.040 --> 00:21:16.720]   becomes the first column, the second row becomes the second
[00:21:16.720 --> 00:21:22.120]   column, so on and so forth. So if this was your and that's a
[00:21:22.120 --> 00:21:27.240]   transpose. So if this was your original matrix, a transpose
[00:21:27.240 --> 00:21:34.080]   will be like so quickly wanted to remind you all of that.
[00:21:34.080 --> 00:21:45.160]   So if you look at the original tensor, it is stored
[00:21:45.160 --> 00:21:49.720]   continuously in memory. But the transpose of it is not
[00:21:49.720 --> 00:21:53.640]   continuous. The reason being, if you remember transpose isn't
[00:21:53.640 --> 00:21:57.920]   something new that gets created. It's only a change of stride
[00:21:57.920 --> 00:22:04.040]   that happens. So the way the tensor is stored doesn't change.
[00:22:04.720 --> 00:22:08.720]   Just the representation or the way of showing it to us gets
[00:22:08.720 --> 00:22:13.800]   changed. So if you make changes to this, it will get reflected
[00:22:13.800 --> 00:22:18.120]   in the original tensor as well. Always be careful with that. I
[00:22:18.120 --> 00:22:21.880]   am alluding to it quite a bit, but it's it's a place where I
[00:22:21.880 --> 00:22:27.240]   mess up a lot. So I want to make sure no one else does. Alright,
[00:22:27.240 --> 00:22:31.920]   continuing we can create our double value tensors like so by
[00:22:31.920 --> 00:22:36.160]   simply putting a double function against it. You can also convert
[00:22:36.160 --> 00:22:41.480]   the value. So if you add dot two and pass the value, you should
[00:22:41.480 --> 00:22:46.080]   be able to convert it into a given data type or simply you
[00:22:46.080 --> 00:22:50.240]   can pass what would you like to convert it to and torch will
[00:22:50.240 --> 00:22:56.720]   convert it to that data type. You can create random values.
[00:23:00.920 --> 00:23:05.960]   And these are a few ways of indexing inside of different
[00:23:05.960 --> 00:23:12.480]   things. So I'll skip over these. I don't think anything missing.
[00:23:12.480 --> 00:23:16.880]   Anything is missing. But the one thing I want to point out, you
[00:23:16.880 --> 00:23:23.920]   can also negatively index. So minus one just loops around the
[00:23:23.920 --> 00:23:28.560]   tensor or the list. Slicing works with lists also. So it's
[00:23:28.560 --> 00:23:32.480]   the same thing. There's nothing new here. But it also goes
[00:23:32.480 --> 00:23:36.120]   around. And another thing you can do is you can pass none.
[00:23:36.120 --> 00:23:41.160]   What that does, I won't answer here or tie it out. I want you
[00:23:41.160 --> 00:23:43.200]   to tie these things and understand what's going on
[00:23:43.200 --> 00:23:47.440]   inside of the memory. So let's skip that again. You can save
[00:23:47.440 --> 00:23:50.960]   these things to the memory. That's what the next section
[00:23:50.960 --> 00:23:54.640]   covers. And I believe we had ended right here. So I quickly
[00:23:54.640 --> 00:23:58.880]   showed you how do we slice inside of tensors or lists. And
[00:23:58.880 --> 00:24:03.680]   I'm leaving this as a homework. I'll be adding this to the
[00:24:03.680 --> 00:24:06.960]   discussion thread that I pointed out. But if anyone else would
[00:24:06.960 --> 00:24:12.800]   like to make sure that's taken care of. So you can save these
[00:24:12.800 --> 00:24:22.920]   tensors like so. So point is a tensor. And this is how we're
[00:24:22.920 --> 00:24:27.680]   saving it to the memory, we can again later read it by opening
[00:24:27.680 --> 00:24:35.600]   it in a by calling the open function, which is the standard
[00:24:35.600 --> 00:24:40.520]   for IO. That's how I know this W is for writing. So W make sure
[00:24:40.520 --> 00:24:45.520]   we can write to it. And P if I remember correctly, make sure
[00:24:45.520 --> 00:24:51.160]   it's of byte types. Sorry, B stands for binary mode. So with
[00:24:51.160 --> 00:24:56.000]   this flag, we can write to it and we can open it. Open this
[00:24:56.000 --> 00:24:59.880]   file and we can save it to the memory. For loading it, you can
[00:24:59.880 --> 00:25:02.760]   simply load the value inside of a tensor like so by calling
[00:25:02.760 --> 00:25:12.440]   torch.load. You can also save it as h5 pi by as htf5 sorry, by
[00:25:12.440 --> 00:25:17.320]   using the h5 pi module. Quick note, this isn't installed by
[00:25:17.320 --> 00:25:20.160]   default with torch, you'd have to install it separately. I
[00:25:20.160 --> 00:25:24.760]   learned it. That's why it works for me. But I leave you to
[00:25:24.760 --> 00:25:29.120]   exploring this function. So as we build upon these things, one
[00:25:29.120 --> 00:25:32.640]   thing I would like to point out, instead of going back and
[00:25:32.640 --> 00:25:37.520]   reading the book or trying something else, I would suggest
[00:25:37.520 --> 00:25:41.880]   looking up this library and just seeing what's going on here. So
[00:25:41.880 --> 00:25:44.800]   late, let's see if I can find the docs. That's the second
[00:25:44.800 --> 00:26:01.520]   link. So let's see here. So I can write files in the htf5
[00:26:01.520 --> 00:26:06.000]   mode. Now, what I'd like you to try after this is try other
[00:26:06.000 --> 00:26:11.720]   techniques. Try other ways of saving your values to the
[00:26:11.720 --> 00:26:15.480]   memory or your tensors to the memory. See what those are. See
[00:26:15.480 --> 00:26:19.240]   how do they compare against this method? And tell us what you
[00:26:19.240 --> 00:26:23.840]   learned. Write about it. Write about what you learned. We look
[00:26:23.840 --> 00:26:26.320]   at a different module right after this, which is known as
[00:26:26.320 --> 00:26:32.080]   image IO. So that's used for image input output. As as the
[00:26:32.080 --> 00:26:37.040]   name gives away. See what these do. And see their API, try to
[00:26:37.040 --> 00:26:42.560]   play around with their API. I told you about tensor indexing.
[00:26:42.560 --> 00:26:47.160]   This looks weird if the first if this is the first time you're
[00:26:47.160 --> 00:26:54.040]   looking at it, try to take this apart. So just print d set minus
[00:26:54.040 --> 00:27:01.760]   two colon above this. And if you don't load it properly, you
[00:27:01.760 --> 00:27:16.680]   should get an error like I am. Oh, I see the file is already
[00:27:16.680 --> 00:27:27.440]   open. So that's what's missing. I guess what's happening right
[00:27:27.440 --> 00:27:30.160]   now is I've already left it open and not posted somewhere. So
[00:27:30.160 --> 00:27:33.120]   that's why I'm getting this error. I don't want to fix this
[00:27:33.120 --> 00:27:36.200]   right now. But what I want to point out here is, it's really
[00:27:36.200 --> 00:27:39.440]   easy to look at these functions and understand, okay, maybe this
[00:27:39.440 --> 00:27:42.320]   is what's going on. But really try to break it down. So see
[00:27:42.320 --> 00:27:46.760]   what this particular indexing is doing. Try to understand that.
[00:27:46.760 --> 00:27:52.280]   That's that's one of the suggested one looks. So how do
[00:27:52.280 --> 00:27:55.160]   you put things on the GPU? This is one thing that gets me
[00:27:55.160 --> 00:28:00.520]   excited a lot. But in every, almost everything in PyTorch
[00:28:00.520 --> 00:28:04.680]   has this way of transferring it to the GPU, especially tensor.
[00:28:04.680 --> 00:28:09.040]   So if right now the tensors was stored on your computer's
[00:28:09.040 --> 00:28:13.360]   memory on your CPU's memory, which means RAM. But if you'd
[00:28:13.360 --> 00:28:17.120]   like to transfer it to the GPU memory, you simply pass pass the
[00:28:17.120 --> 00:28:24.160]   device as CUDA. And by default, this is device equals CPU. So if
[00:28:24.160 --> 00:28:28.200]   you don't pass anything, it by default goes to the CPU. If you
[00:28:28.200 --> 00:28:33.000]   have multiple GPUs, like I do, you can select the GPU number
[00:28:33.000 --> 00:28:37.640]   and get allocated on that particular one. And then you
[00:28:37.640 --> 00:28:43.280]   can move it back to the CPU like so. And all of these functions
[00:28:43.280 --> 00:28:47.320]   that we've looked at work really well on GPU or CPU regardless.
[00:28:52.160 --> 00:28:58.240]   Awesome. Moving for moving on further. I think this I'm not
[00:28:58.240 --> 00:29:01.760]   sure how much depth should I go into for these things. But this
[00:29:01.760 --> 00:29:05.760]   was an quick explanation of named tensors inside of the
[00:29:05.760 --> 00:29:11.280]   book. This is an experimental API. As you declare it, you can
[00:29:11.280 --> 00:29:14.200]   see that those complaints, name tensors and all of their
[00:29:14.200 --> 00:29:17.320]   associated API's are an experimental feature subject to
[00:29:17.320 --> 00:29:20.960]   change, please do not use them for anything important. So it's
[00:29:20.960 --> 00:29:24.640]   good to know, but not really important to use it. Where does
[00:29:24.640 --> 00:29:29.360]   it come in handy? Let's say you're creating this. Remember
[00:29:29.360 --> 00:29:32.120]   last week, we looked at the state of the art Bobby detector,
[00:29:32.120 --> 00:29:35.440]   Bobby was our golden retriever. If I remember correctly, yes.
[00:29:35.440 --> 00:29:40.440]   So it's stored like so. So you have the channels, you have the
[00:29:40.440 --> 00:29:43.360]   rows and you have the columns. Now when you're performing
[00:29:43.360 --> 00:29:47.440]   tensor multiplications, first of all, your value should match so
[00:29:47.440 --> 00:29:51.480]   they should be compatible. But then you also need to make sure
[00:29:51.480 --> 00:29:55.400]   these channels are aligned in the right fashion. So let's say
[00:29:55.400 --> 00:29:59.880]   adding two things you don't want to add height to width that
[00:29:59.880 --> 00:30:02.760]   first of all, that wouldn't work. But if even if it does,
[00:30:02.760 --> 00:30:06.600]   that's that's a failure. So you can make sure that does not
[00:30:06.600 --> 00:30:14.680]   happen by declaring the name of the particular I'd like to say,
[00:30:15.120 --> 00:30:19.680]   dimension, yes, dimension would be right word. And that takes
[00:30:19.680 --> 00:30:30.320]   care of it. So one thing I'm just running everything to make
[00:30:30.320 --> 00:30:34.880]   sure I can do this. I'd like to point out the unsqueeze
[00:30:34.880 --> 00:30:39.520]   function. And this is quite mind boggling the first time you
[00:30:39.520 --> 00:30:44.200]   experience it. So please explore it a bit and try to print out
[00:30:44.200 --> 00:30:47.840]   what's going in what's coming out. But what's going on here is
[00:30:47.840 --> 00:30:52.600]   we're creating a batch and trying to understand what's
[00:30:52.600 --> 00:30:55.600]   happening here. Let me see I see a few questions coming in.
[00:30:55.600 --> 00:30:56.920]   Quickly.
[00:30:56.920 --> 00:30:58.840]   To them.
[00:31:02.520 --> 00:31:27.760]   Durga these these are from the books GitHub. Oh, sorry,
[00:31:27.760 --> 00:31:31.600]   graduation already answer that. That's, please make sure you
[00:31:31.600 --> 00:31:35.240]   could if you could please add it this add this to the top.
[00:31:35.240 --> 00:31:36.520]   Thanks.
[00:31:36.520 --> 00:31:41.600]   Alright, continuing further.
[00:31:41.600 --> 00:31:52.560]   We were looking at how to create batches from images. So
[00:31:52.560 --> 00:31:57.480]   everything inside of deep learning happens in a batch. And
[00:31:57.480 --> 00:32:00.560]   I'll explain why is that the case. But for now, we're trying
[00:32:00.560 --> 00:32:05.280]   to understand how do we put things on a batch? Because you
[00:32:05.280 --> 00:32:09.320]   buy $1,000 GPU, let's say, those are much more expensive than
[00:32:09.320 --> 00:32:12.840]   that. But let's say somehow magically got one for $1,000.
[00:32:12.840 --> 00:32:20.120]   You'd be pushing through a few 1000 a few hundred images. And
[00:32:20.120 --> 00:32:23.240]   so far, we've only read in a single image. So last week, we
[00:32:23.240 --> 00:32:26.600]   looked at how to read a single image of our cute dog Bobby.
[00:32:26.600 --> 00:32:37.520]   But what you'd like to do is place them on a batch. So
[00:32:37.520 --> 00:32:40.440]   that's what we are learning here. How do we put things on a
[00:32:40.440 --> 00:32:45.280]   batch. And for that, we need to add an extra dimension. So you
[00:32:45.280 --> 00:32:50.480]   can either unsqueeze or you can declare it like so. So this
[00:32:50.480 --> 00:32:54.480]   particular example has two batches, three channels, because
[00:32:54.480 --> 00:32:58.500]   our images are RGB, for some reason, they are five by five in
[00:32:58.500 --> 00:33:05.480]   length and width. And that's how this is defined. I'll skip over
[00:33:05.480 --> 00:33:11.160]   these parts. Because they are fairly straightforward. So
[00:33:11.160 --> 00:33:14.520]   remember, when I quickly pointed out that you'd want to make sure
[00:33:14.520 --> 00:33:18.760]   these channels are correctly aligned, or when you operating
[00:33:18.760 --> 00:33:22.960]   on two different tensors that they work really well, if you
[00:33:22.960 --> 00:33:27.040]   pass the name of the channel, those are the ones I'll get
[00:33:27.040 --> 00:33:32.320]   added. So it takes care of broadcasting by default, if you
[00:33:32.320 --> 00:33:35.600]   name the tensors. Although again, I'd like to remind you
[00:33:35.600 --> 00:33:39.840]   that this is an experimental feature and not everyone names
[00:33:39.840 --> 00:33:46.080]   their tensors a lot. And now we get to the exciting bit. So the
[00:33:46.080 --> 00:33:49.080]   chapter after this, I think it was the fourth chapter. Let me
[00:33:49.080 --> 00:33:53.200]   quickly glance at it to make sure yes. So in the fourth
[00:33:53.200 --> 00:33:57.760]   chapter, we look at how to read in different formats of data
[00:33:57.760 --> 00:34:03.320]   set. And the first one is an image. The one after that is a
[00:34:03.320 --> 00:34:07.880]   CT scan, we also look at how to read in a tabular data set, a
[00:34:07.880 --> 00:34:20.960]   time series data set, and also how to read in boards. So what
[00:34:20.960 --> 00:34:24.840]   we are doing here is we've imported this image IO module,
[00:34:24.840 --> 00:34:29.880]   and we'll be using this another suggested homework for you all
[00:34:29.880 --> 00:34:33.320]   try to understand how this works. If you look at the API,
[00:34:33.320 --> 00:34:36.280]   there's not much going on, you can, you can only use it for
[00:34:36.280 --> 00:34:40.560]   reading images and CT scans. That's what the author have
[00:34:40.560 --> 00:34:44.520]   done. But one thing I'd like you to try as a homework is try
[00:34:44.520 --> 00:34:49.080]   image IO and torch vision is also great. Compare those two
[00:34:49.080 --> 00:34:54.840]   APIs, develop a feel of when you'd like to use what one of
[00:34:54.840 --> 00:34:58.080]   the main problems with one of the main things that you need to
[00:34:58.080 --> 00:35:02.320]   take care of a deep learning is you need to get the data to the
[00:35:02.320 --> 00:35:05.840]   model. And that's that's where a lot of the time or the effort
[00:35:05.840 --> 00:35:08.720]   goes into because once you get it to the model, things things
[00:35:08.720 --> 00:35:13.960]   are relatively easier from there. So try to get a feel of
[00:35:13.960 --> 00:35:17.640]   how different API is work. I as you look through different
[00:35:17.640 --> 00:35:21.320]   Kaggle kernels, you'll find people using the same modules,
[00:35:21.320 --> 00:35:24.280]   different modules. And there's a reason for that they have this
[00:35:24.280 --> 00:35:27.920]   intuition. They like this, they really like this one framework.
[00:35:27.920 --> 00:35:31.400]   I don't know why the authors used image IO, but I'm assuming
[00:35:31.400 --> 00:35:33.880]   they really liked it for this particular problem. And that's
[00:35:33.880 --> 00:35:38.280]   why they've done this. So try to get a feel of what you like and
[00:35:38.280 --> 00:35:42.200]   what works best for you. Another thing I'd want to point out
[00:35:42.200 --> 00:35:47.880]   here. So we've read this image, but Jupyter is also really nice.
[00:35:47.880 --> 00:35:50.640]   So I'm inside of the home directory that I've grown. So
[00:35:50.640 --> 00:35:54.600]   that is a GitHub repository. And if I go inside of the data
[00:35:54.600 --> 00:35:59.000]   folder, and down to the doc folder, I can look at the
[00:35:59.000 --> 00:36:03.320]   documents like so. So Jupyter is can Jupyter notebooks can render
[00:36:03.320 --> 00:36:09.840]   anything that you can render inside of an HTML file. So
[00:36:09.840 --> 00:36:12.480]   that's one way of also making sure your data is in there. I
[00:36:12.480 --> 00:36:16.440]   just wanted to point that out. You can browse through different
[00:36:16.440 --> 00:36:20.440]   directories, all the what you'd like to use is the Python OS
[00:36:20.480 --> 00:36:34.760]   module. So as you can see, it has all of these functions, OS
[00:36:34.760 --> 00:36:39.440]   path, OS name, it returns current directory, it can return
[00:36:39.440 --> 00:36:42.400]   the parent directory, you can traverse directories, you can do
[00:36:42.400 --> 00:36:45.680]   anything that you could basically do with double clicks
[00:36:45.680 --> 00:36:48.960]   by opening a folder or closing a folder, but inside of Python,
[00:36:49.480 --> 00:36:53.400]   anything that takes you away from GUI and more towards
[00:36:53.400 --> 00:36:59.400]   programming is always great. So I've read in this image. And
[00:36:59.400 --> 00:37:06.480]   there's one problem with this. torch expects images in the
[00:37:06.480 --> 00:37:14.040]   format of channel height and width. This is width height
[00:37:14.040 --> 00:37:18.160]   channel. Can anyone would anyone like to answer how do we fix
[00:37:18.160 --> 00:37:23.480]   this? So the problem is we've read in an image and it's not
[00:37:23.480 --> 00:37:27.960]   in the right format. So how do we fix that? I'm just trying to
[00:37:27.960 --> 00:37:31.000]   make this a little interactive, feel free to let me know if you
[00:37:31.000 --> 00:37:33.600]   don't like this format, and the next one, we can skip it. But
[00:37:33.600 --> 00:37:37.160]   it's more of a broadcasting since you all are attending
[00:37:37.160 --> 00:37:39.800]   live, I'd also like you to enjoy it a little. So that's why I'm
[00:37:39.800 --> 00:37:42.000]   trying to ask a few questions. I have at least five or six
[00:37:42.000 --> 00:37:46.440]   simple ones lined up for you. As you'll reply quickly. Look over
[00:37:46.440 --> 00:37:53.000]   and make sure everything is there. Yeah, I don't see any
[00:37:53.000 --> 00:37:58.760]   questions on the YouTube chart. You've got his we can use
[00:37:58.760 --> 00:38:08.520]   straight. Yes, that could be one way. A few people have seen
[00:38:08.520 --> 00:38:14.640]   ahead of time and they're applying in the zoom chat. Also,
[00:38:14.640 --> 00:38:17.080]   guys, I'm just monitoring this. So please make sure to apply
[00:38:17.080 --> 00:38:23.920]   here. You could reshape that's that's a great answer. Awesome.
[00:38:23.920 --> 00:38:26.720]   So I'd like to introduce this function of all of those
[00:38:26.720 --> 00:38:30.040]   answers are correct. Nothing is wrong there. And I'd like you to
[00:38:30.040 --> 00:38:33.940]   try replacing this with art. So what we're using here is a new
[00:38:33.940 --> 00:38:37.440]   function. It's called toaster permute. And I'd like you to try
[00:38:37.440 --> 00:38:40.600]   reshaping instead of this and see how that works. For that
[00:38:40.600 --> 00:38:44.480]   you'd have to assign it to a new tensor and then assign it back
[00:38:44.560 --> 00:38:48.520]   and it should work. Also, you'd have to clone it otherwise you'd
[00:38:48.520 --> 00:38:53.080]   mess up the different allocations. Just pointing this
[00:38:53.080 --> 00:38:56.040]   out, try this as a homework if you like or not totally up to
[00:38:56.040 --> 00:39:01.120]   you. So what we are doing here is permuting through them. And I
[00:39:01.120 --> 00:39:04.280]   shift tabbed here nothing useful came up. So what I would
[00:39:04.280 --> 00:39:10.760]   suggest you to do is look up tors permute, see if the tors
[00:39:10.760 --> 00:39:17.280]   come up. I clicked on the wrong link. So I'll go back. Let's see
[00:39:17.280 --> 00:39:20.760]   if we can find anything useful here. No, we cannot. So this is
[00:39:20.760 --> 00:39:24.160]   a useful opportunity, let's say even to contribute to this
[00:39:24.160 --> 00:39:30.040]   particular documentation and value your first PyTorch
[00:39:30.040 --> 00:39:32.320]   open source contribution, just throwing that out there. It's
[00:39:32.320 --> 00:39:36.720]   really exciting when that happens. But what's going on
[00:39:36.720 --> 00:39:42.480]   here, as I painfully make my way back to it, I pointed out that
[00:39:42.480 --> 00:39:46.360]   torch expects an image into be the following format, they
[00:39:46.360 --> 00:39:50.280]   expect the channel to be forced the height and width afterwards.
[00:39:50.280 --> 00:39:54.720]   As we read in our image through image IO it read in like so
[00:39:54.720 --> 00:39:57.680]   it's RGB. So there are three channels red, green and blue.
[00:39:57.680 --> 00:40:01.160]   These are the dimensions of the image we're permuting through
[00:40:01.160 --> 00:40:05.440]   it. So we bring the second, actually the third but the
[00:40:05.700 --> 00:40:12.440]   number two dimension towards the first one. So we bring this
[00:40:12.440 --> 00:40:17.480]   or we permute it as the first one. The zero one comes
[00:40:17.480 --> 00:40:21.840]   afterwards, and the first one afterwards. So we've rearranged
[00:40:21.840 --> 00:40:29.760]   the columns a little to make sure it is how we someone's
[00:40:29.760 --> 00:40:32.440]   complaining about the audio. Is it is it working for everyone?
[00:40:32.440 --> 00:40:42.420]   Awesome. Thanks for confirming. Could you please try
[00:40:42.420 --> 00:40:49.100]   rejoining? That should fix your problem. Okay, thanks. Thanks
[00:40:49.100 --> 00:40:54.040]   for confirming. If anything goes wrong, please let me know
[00:40:54.040 --> 00:40:56.620]   because I'm just running the show solo today my colleague is
[00:40:56.620 --> 00:40:59.700]   on vacation. She makes sure nothing goes wrong. That's why
[00:40:59.700 --> 00:41:02.180]   things are a little rough around the edges. But anyways,
[00:41:02.180 --> 00:41:07.280]   coming back to this, the problem was, we'd want the columns or
[00:41:07.280 --> 00:41:10.280]   the dimensions to be in a certain way, torch expects them
[00:41:10.280 --> 00:41:13.220]   like so this is how we're fixing that problem. This is how we're
[00:41:13.220 --> 00:41:16.340]   taking care of that problem. Why are we looking at this as a
[00:41:16.340 --> 00:41:19.720]   reminder, we're learning how to read in different forms of data.
[00:41:19.720 --> 00:41:24.640]   We just looked at image we look at CT scans right after this.
[00:41:24.640 --> 00:41:30.200]   And then we look at tabular data followed by NLP. So so far, we
[00:41:30.200 --> 00:41:37.840]   figured out how to set the image in the correct format. It's
[00:41:37.840 --> 00:41:41.640]   always great to check the shapes from now on. So as you can see,
[00:41:41.640 --> 00:41:46.300]   we fixed that. And what we've done here is we've selected a
[00:41:46.300 --> 00:41:53.960]   batch size of three. We've created a batch of three images
[00:41:54.440 --> 00:42:00.560]   with three channels and 256 by 256 size. So remember, we can
[00:42:00.560 --> 00:42:04.440]   only pass we should only pass images in a batch. That's what
[00:42:04.440 --> 00:42:09.840]   we're doing here. Now what we're doing after this is a sort of
[00:42:09.840 --> 00:42:12.240]   give it away. I didn't realize I didn't look down and realize
[00:42:12.240 --> 00:42:17.120]   we're using OS module here. But we're reading in images and
[00:42:17.120 --> 00:42:24.000]   passing them on to a batch after setting the correct way of the
[00:42:24.000 --> 00:42:28.760]   dimensions as torch expected. I'd leave you to explore what's
[00:42:28.760 --> 00:42:33.120]   going on here. One nice way of doing it is by looking at this
[00:42:33.120 --> 00:42:37.560]   code block. And I'm sorry, if you're already experienced in
[00:42:37.560 --> 00:42:41.080]   Python, I'm just trying to point out how you can become better at
[00:42:41.080 --> 00:42:44.440]   this. So list comprehensions are a little mind boggling when you
[00:42:44.440 --> 00:42:47.580]   first see them. What I would do here is I would just leave a
[00:42:47.580 --> 00:42:59.020]   comment. So read in filenames after listing everything. And
[00:42:59.020 --> 00:43:02.340]   I'd basically comment what's going down, what's going on
[00:43:02.340 --> 00:43:06.780]   here, and then delete the cell and try to actively write it
[00:43:06.780 --> 00:43:10.980]   maybe immediately or maybe after a few hours or maybe in the next
[00:43:10.980 --> 00:43:16.080]   day. That'll really help you not mug this thing up, but really
[00:43:16.080 --> 00:43:19.940]   help you understand why are we doing this. And the reason we're
[00:43:19.940 --> 00:43:23.620]   doing this is we're learning how to load data into our memory for
[00:43:23.620 --> 00:43:26.620]   our models. When do we get to that we'll get to that later.
[00:43:26.620 --> 00:43:31.580]   One thing that I'll point out with a handwavy explanation,
[00:43:31.580 --> 00:43:35.140]   we'd always want our values to be between zero and one when
[00:43:35.140 --> 00:43:40.080]   working with models. Why? Because that really works well
[00:43:40.080 --> 00:43:43.380]   with them. That's the only explanation I leave to you.
[00:43:43.700 --> 00:43:49.220]   Instead of finding out why, try to see why are we doing this. So
[00:43:49.220 --> 00:43:52.220]   try to change these things around. Don't subtract the mean
[00:43:52.220 --> 00:43:55.700]   see what happens. Don't divide by standard deviation, see what
[00:43:55.700 --> 00:43:59.300]   happens. So this is how we normalize, quote unquote,
[00:43:59.300 --> 00:44:02.580]   normalize the values. We subtract the mean and divide by
[00:44:02.580 --> 00:44:07.860]   standard deviation. Try not doing this and see how that
[00:44:07.860 --> 00:44:10.820]   changes the values. You could of course look up the theory why
[00:44:10.820 --> 00:44:14.300]   this is going on, I would suggest understanding what's
[00:44:14.300 --> 00:44:18.940]   going on in the code instead. So this is how we read in an image
[00:44:18.940 --> 00:44:25.540]   file. After this, the authors explain how to read in a DICOM
[00:44:25.540 --> 00:44:31.060]   file. And it's again, they're using the image IO module and
[00:44:31.060 --> 00:44:35.820]   they're using the vol read function. It reads the volume
[00:44:35.820 --> 00:44:39.860]   from the specified file returns a numpy array. So it's not a
[00:44:39.860 --> 00:44:46.460]   torch array that will be returning. Excuse me. And it
[00:44:46.460 --> 00:44:49.420]   comes with a dictionary of metadata. Why do we get a
[00:44:49.420 --> 00:44:54.660]   dictionary of metadata? That's because DICOM images, I forget
[00:44:54.660 --> 00:45:02.540]   what it stands for. So look it up. Digital imaging and
[00:45:02.540 --> 00:45:05.780]   communications in medicine, this particular format of images
[00:45:05.780 --> 00:45:10.260]   always have some mirror, some metadata, apologies, some
[00:45:10.260 --> 00:45:15.660]   metadata about the image. And it comes with those details along
[00:45:15.660 --> 00:45:19.140]   with it. So you can find the age of the patient, so on and so
[00:45:19.140 --> 00:45:25.620]   forth. What are these? These are scans, I believe CT scans of the
[00:45:25.620 --> 00:45:28.740]   patients. And we're trying to understand how do we read them
[00:45:28.740 --> 00:45:37.100]   in as we read them in as a numpy array. I believe volume
[00:45:37.100 --> 00:45:45.860]   underscore is still a numpy array. Okay, so it's an image
[00:45:45.860 --> 00:45:51.860]   core array. I thought this returns a numpy array. It
[00:45:51.860 --> 00:45:58.780]   should. Interesting. I'll try to get the bottom to the bottom
[00:45:58.780 --> 00:46:01.380]   of this, but I was assuming this should be a numpy array. I'm
[00:46:01.380 --> 00:46:07.260]   not sure why that's not the case. Maybe these these two are
[00:46:07.260 --> 00:46:10.540]   the same thing. I'll have to double check. And I'll confirm
[00:46:10.540 --> 00:46:14.060]   this in the next week. Sorry, guys. So what's going on here is
[00:46:14.060 --> 00:46:18.620]   we've passed the location of one particular DICOM file and read
[00:46:18.620 --> 00:46:26.100]   that. This one has 99 channels. So an RGB image has three
[00:46:26.100 --> 00:46:33.820]   channels. This has 99 of them. And 512 by 512 image size.
[00:46:33.820 --> 00:46:36.980]   Actually, I'm sorry, I believe I'm confusing myself. We're
[00:46:36.980 --> 00:46:46.180]   reading in 99 files, I believe. I'll quickly look at the book
[00:46:46.180 --> 00:47:07.100]   to make sure I'm not confusing myself. Yeah, sorry, I messed
[00:47:07.100 --> 00:47:13.780]   up. It's it's 99 images that we've written. And we've just
[00:47:13.820 --> 00:47:17.100]   plotted the one at the 50th value. Sorry about the
[00:47:17.100 --> 00:47:22.020]   confusion. Why are we unsqueezing it? That's I leave
[00:47:22.020 --> 00:47:26.020]   you to that. I leave you to finding that out. Thanks for
[00:47:26.020 --> 00:47:29.180]   pointing out my do yes, it does say 99 files. I didn't look at
[00:47:29.180 --> 00:47:35.300]   that. I leave you to find understanding by do we use
[00:47:35.300 --> 00:47:41.980]   tors.unsqueeze. But the takeaway here is again, you can read in
[00:47:41.980 --> 00:47:48.060]   DICOM images like so. And you can display them using matplotlib.
[00:47:48.060 --> 00:47:54.500]   You can explore this API. It's quite great. So this brings us
[00:47:54.500 --> 00:47:57.980]   to the next thing, which is quite exciting as a deep
[00:47:57.980 --> 00:48:01.900]   learning practitioner, which is understanding a data set of
[00:48:01.900 --> 00:48:06.700]   finds. JSU write unsqueeze is used to create a batch for
[00:48:06.700 --> 00:48:09.580]   people that are new, please try playing around with the
[00:48:09.580 --> 00:48:15.540]   unsqueeze function and see what what does it do. So the task we
[00:48:15.540 --> 00:48:19.300]   are assigned after this, as a reminder, in the broader scheme
[00:48:19.300 --> 00:48:22.140]   of things, I'm just summarizing what's taught in the chapter.
[00:48:22.140 --> 00:48:28.220]   So that's why I'm going through this. But the task after this is
[00:48:28.220 --> 00:48:34.740]   trying to predict everything inside of this data set. It's a
[00:48:34.740 --> 00:48:39.060]   tabular data set that has vine quality, and we're trying to
[00:48:39.060 --> 00:48:42.540]   predict how well is the quality of the vine using the data.
[00:48:42.540 --> 00:48:46.220]   Usually with tabular data, it doesn't work really well. But
[00:48:46.220 --> 00:48:49.180]   we're trying this. Would anyone like to answer why are we
[00:48:49.180 --> 00:48:53.820]   skipping the first row here? So the question is, why do we skip
[00:48:53.820 --> 00:48:57.660]   the first row here, it's a CSV file that we read in. So we read
[00:48:57.660 --> 00:49:02.180]   the CSV file in as an umpire using a load txt function, we
[00:49:02.180 --> 00:49:06.340]   pass the path, the data type and delimiter and we skip the first
[00:49:06.340 --> 00:49:06.580]   row.
[00:49:06.580 --> 00:49:20.660]   The questions are a way for me to sip chai sneakily.
[00:49:20.660 --> 00:49:36.100]   Yep, both of these answers are right the first. So when you
[00:49:36.100 --> 00:49:44.940]   read in a CSV file, the topmost column is the header. So that
[00:49:44.940 --> 00:49:47.540]   tells you about the data set what's inside of the data set.
[00:49:48.020 --> 00:49:54.180]   Let's see if I can take my Jupyter server to that. So I'll
[00:49:54.180 --> 00:50:03.620]   go to data, chapter four. tabular vine. Click on this. So
[00:50:03.620 --> 00:50:11.340]   this is how CSV files are stored inside of the memory. I will
[00:50:11.340 --> 00:50:14.380]   fix your issue of not being able to post before I'm sorry about
[00:50:14.380 --> 00:50:19.820]   that. I'll fix that issue right after the school. So the first
[00:50:19.820 --> 00:50:23.300]   column is the header which tells you about what's inside of
[00:50:23.300 --> 00:50:27.060]   every single column. And it's a CSV which means it's a comma
[00:50:27.060 --> 00:50:32.700]   separated file. Everything is separated by a semi semicolon.
[00:50:32.700 --> 00:50:36.260]   So the first column is fixed acidity. The second one is
[00:50:36.260 --> 00:50:41.740]   volatile acidity, citric acid. I don't know much about vines. So
[00:50:41.740 --> 00:50:45.420]   I'm assuming these things are important. And there's a value
[00:50:45.420 --> 00:50:50.260]   about quality alongside with it. So that's what's inside of the
[00:50:50.260 --> 00:50:53.660]   data set. We're skipping the first row by loading it through
[00:50:53.660 --> 00:50:58.060]   a NumPy module called load text, skip the first row to skip the
[00:50:58.060 --> 00:51:07.780]   header. And this gives us a nice NumPy array like so. We grab
[00:51:07.780 --> 00:51:11.420]   the column list using this function, and we specify the
[00:51:11.420 --> 00:51:17.740]   delimiter to the CSV reader function that allows us to grab
[00:51:17.740 --> 00:51:22.100]   everything without it being too annoying. And we can then list
[00:51:22.100 --> 00:51:26.100]   it out and look at the column list. So as I pointed out, these
[00:51:26.100 --> 00:51:29.500]   are the columns. And we're trying to predict as a deep
[00:51:29.500 --> 00:51:32.140]   learning practitioner, you are tasked with the important
[00:51:32.140 --> 00:51:36.700]   mission of predicting how good is your wine using all of these
[00:51:36.700 --> 00:51:39.180]   values. Let's say that's that's your mission on your first day
[00:51:39.180 --> 00:51:44.220]   of your job. So the first thing we need to do, quite importantly,
[00:51:44.220 --> 00:51:46.060]   since we are PyTorch practitioners, we're not
[00:51:46.060 --> 00:51:49.340]   TensorFlow people, we're not, what's the other framework, you
[00:51:49.340 --> 00:51:53.260]   can think of Jacks, we're not using Jacks, we need to bring
[00:51:53.260 --> 00:51:57.940]   everything inside of Torch. So remember, we read in everything
[00:51:57.940 --> 00:52:01.380]   as a NumPy array, we now convert it into Torch by using the from
[00:52:01.380 --> 00:52:03.580]   NumPy array, you would know of this since you might have
[00:52:03.580 --> 00:52:07.940]   written a blog by next week, make sure to do that. Or if you
[00:52:07.940 --> 00:52:12.060]   would have played around. And then you can print out the
[00:52:12.060 --> 00:52:15.460]   shape, you can look at the data type. Why is it important to do
[00:52:15.460 --> 00:52:19.640]   so? The authors do this inside of the book to tell you that
[00:52:19.640 --> 00:52:22.220]   hey, there are so many examples inside of this particular
[00:52:22.220 --> 00:52:26.400]   dataset. It's also important while you're working on it to
[00:52:26.400 --> 00:52:28.940]   make sure that things are getting the right thing. So when
[00:52:28.940 --> 00:52:36.340]   you point things to wrong path, or things aren't, sorry, things
[00:52:36.340 --> 00:52:40.940]   aren't working as they should. This would be zero by zero,
[00:52:40.940 --> 00:52:44.940]   there would be nothing displayed here. So it's good from time to
[00:52:44.940 --> 00:52:47.320]   time to make sure things are in the right shape that you
[00:52:47.320 --> 00:52:50.340]   expected that in the right data type, it's always good to do
[00:52:50.340 --> 00:52:58.900]   that. So we take in the data and skip the last column. So we're
[00:52:58.900 --> 00:53:03.100]   skipping the last column. The reason being the last column is
[00:53:03.100 --> 00:53:09.460]   quality, we're trying to predict that. So we'll separate it from
[00:53:09.460 --> 00:53:13.860]   our training data. And we'll just load everything else. This
[00:53:13.860 --> 00:53:16.700]   is our target column. So this is the target we're trying to
[00:53:16.700 --> 00:53:22.460]   predict. And we store it separately inside of a target
[00:53:22.460 --> 00:53:28.980]   tensor, which has the same size as this entire one, just that
[00:53:28.980 --> 00:53:34.740]   it's a single column tensor or a single row tensor. We can
[00:53:34.740 --> 00:53:39.620]   convert these to long, like so. There's a comprehensive
[00:53:39.620 --> 00:53:42.220]   discussion inside of the book where they talk about what are
[00:53:42.220 --> 00:53:45.500]   continuous variables, what are ordinal variables, what are
[00:53:45.500 --> 00:53:52.020]   categorical variables, I'll talk about that real quick. So
[00:53:52.020 --> 00:53:55.620]   continuous variable, that's the name says are simply continuous
[00:53:55.620 --> 00:53:59.140]   values. Actually, it might be better if I write about this.
[00:53:59.140 --> 00:54:10.340]   Continuous variables could be 1.1, 1.2. So let's say we're
[00:54:10.340 --> 00:54:15.820]   trying to predict the price of a certain crypto coin. There would
[00:54:15.820 --> 00:54:21.180]   be a lot more zeros than these are continuous values. So these
[00:54:21.180 --> 00:54:23.900]   fluctuate in real time. Let's say this is a continuous value.
[00:54:25.340 --> 00:54:28.180]   A categorical one would be let's say you're trying to drink
[00:54:28.180 --> 00:54:34.820]   tea, it's either hot, it's at room temperature or it's cold.
[00:54:34.820 --> 00:54:40.940]   So these are the three different categories. And ordinal values
[00:54:40.940 --> 00:54:43.780]   lie somewhere in between. It's covered in the chapter. So I'll
[00:54:43.780 --> 00:54:47.620]   skip that. But usually when you're working with these two
[00:54:47.620 --> 00:54:50.860]   formats, there are different ways of dealing with them.
[00:54:50.860 --> 00:55:07.700]   Switch back to the right screen. Actually, I wanted to
[00:55:07.700 --> 00:55:10.380]   point out the different details, but I believe I'll come back to
[00:55:10.380 --> 00:55:14.980]   this. So we're on reading tableau data. What I was trying
[00:55:14.980 --> 00:55:18.580]   to point out is what are ordinal values, but I think I can skip
[00:55:18.580 --> 00:55:23.860]   that. There's this nice flowchart inside of the book
[00:55:23.860 --> 00:55:29.580]   that discusses how can you cover these different data types. And
[00:55:29.580 --> 00:55:35.420]   that's what I wanted to discuss, but I'll skip that. So what's
[00:55:35.420 --> 00:55:39.580]   going on here is we're creating a one hot encoding. And this is
[00:55:39.580 --> 00:55:45.500]   happening because we have all of these different types. And we
[00:55:45.500 --> 00:55:50.500]   are treating it as a categorical variable. Now, this case is
[00:55:50.500 --> 00:55:55.300]   fairly straightforward, but with different X or categories, it
[00:55:55.300 --> 00:56:00.140]   might be a little different. And remember, we can only feed
[00:56:00.140 --> 00:56:05.300]   numbers to our model. So we can't feed text, we can't feed
[00:56:05.300 --> 00:56:09.300]   anything else. How do we deal with that we create a one hot
[00:56:09.300 --> 00:56:12.780]   encoding. I'll have to again, switch back screens and point
[00:56:12.780 --> 00:56:17.700]   out to the correct one. I'll try to explain this by writing.
[00:56:17.700 --> 00:56:29.460]   So let's say we have hot, cold, and room temperature. We're
[00:56:29.460 --> 00:56:37.020]   trying to pass this inside of the memory to model. One way of
[00:56:37.020 --> 00:56:41.040]   representing hot would be let's say you can put one here, and
[00:56:41.040 --> 00:56:49.580]   00. So this is one hot encoding. We create a tensor having three
[00:56:49.580 --> 00:56:53.060]   values. This one representing hot, this one representing
[00:56:53.060 --> 00:56:57.700]   cold, this one representing room temperature. And if we pass 010,
[00:56:57.700 --> 00:57:07.540]   this is cold. So this is how we are encoding our knowledge using
[00:57:07.540 --> 00:57:17.440]   ones inside of a tensor, hence the name one hot encoding. So
[00:57:17.440 --> 00:57:20.200]   that's what we are doing in code, there are simpler ways of
[00:57:20.200 --> 00:57:23.280]   doing it. scikit-learn has a very convenient function, if I
[00:57:23.280 --> 00:57:26.800]   remember correctly, you can simply call that. But we are
[00:57:26.800 --> 00:57:29.440]   learning how to do it here. So we do it manually, we first of
[00:57:29.440 --> 00:57:33.960]   all create a zero filled tensor by calling toss dot zeros, we
[00:57:33.960 --> 00:57:40.820]   define its shape by passing the two values, we pass the length
[00:57:40.820 --> 00:57:47.140]   of the zero column, and 10 values. And then we create a
[00:57:47.140 --> 00:57:52.220]   one hot encoded tensor by calling scatter in place. So
[00:57:52.220 --> 00:57:55.540]   whenever we pass on underscore against a function name that
[00:57:55.540 --> 00:58:00.500]   creates a tensor in place in memory, which means this won't
[00:58:00.500 --> 00:58:03.540]   create a new copy of it, this would just replace everything.
[00:58:04.040 --> 00:58:13.520]   So we unsqueeze this and create a one hot encoded tensor. This
[00:58:13.520 --> 00:58:16.800]   will write all values from tensor that we pass inside of
[00:58:16.800 --> 00:58:23.560]   it. And the tensor we are passing here is one. So we are
[00:58:23.560 --> 00:58:26.280]   one hot encoding. That's why I'll just broadcast the value
[00:58:26.280 --> 00:58:31.480]   one. Add the indents is specified in the attribute
[00:58:31.760 --> 00:58:37.900]   afterwards. So we're trying to encode all of that information
[00:58:37.900 --> 00:58:40.580]   by putting the number one against different categories.
[00:58:40.580 --> 00:58:48.220]   That's how we create a one hot encoded array. Now you can also
[00:58:48.220 --> 00:58:51.180]   understand all of these functions, I'm glancing through
[00:58:51.180 --> 00:58:56.740]   them because mean simply takes the mean. Let's see what bad
[00:58:56.740 --> 00:59:05.980]   does it calculates the variance of all elements inside of the
[00:59:05.980 --> 00:59:10.780]   tensor. Why do we need these because we're trying to
[00:59:10.780 --> 00:59:14.420]   normalize the data. Why do we want to normalize the data?
[00:59:14.420 --> 00:59:17.820]   Because for some reason that I didn't explain, and I'll pass
[00:59:17.820 --> 00:59:23.340]   on for now, you want the values to be between zero and one. We
[00:59:23.340 --> 00:59:26.620]   might get to training our first model today. So if you'd want
[00:59:26.620 --> 00:59:29.700]   to understand why do we want to pass values that are between zero
[00:59:29.700 --> 00:59:35.020]   and one? Try not normalizing and see what happens. See what
[00:59:35.020 --> 00:59:39.700]   happens to your loss values and see what happens when you don't
[00:59:39.700 --> 00:59:42.160]   let's say take the square root or divide by the square root or
[00:59:42.160 --> 00:59:49.540]   when you subtract with the mean. Sorry, a great is right. Thanks
[00:59:49.540 --> 00:59:52.980]   to correction, it should be between minus one and one. And
[00:59:52.980 --> 00:59:55.260]   the standard deviation should be between zero and one. I'm
[00:59:55.260 --> 00:59:59.260]   confusing myself a little sorry about that. When we normalize
[00:59:59.260 --> 01:00:01.860]   the value should be between minus one and one. Sorry about
[01:00:01.860 --> 01:00:05.540]   that, guys. And it's it's a continuous range. So these can
[01:00:05.540 --> 01:00:09.420]   be floating point variables like they are between these two
[01:00:09.420 --> 01:00:17.980]   numbers. I'm reading through this code to try and understand
[01:00:17.980 --> 01:00:21.700]   what's important to highlight here. Again, I am just trying to
[01:00:21.700 --> 01:00:24.420]   summarize everything from the book because I assume you'll
[01:00:24.940 --> 01:00:27.940]   either read this afterwards or have read this before and just
[01:00:27.940 --> 01:00:32.380]   trying to point out the valuable things. One good thing I'd like
[01:00:32.380 --> 01:00:36.500]   to point out here is the authors are creating this certain
[01:00:36.500 --> 01:00:39.740]   category. So what they've done is remember, we had this quality
[01:00:39.740 --> 01:00:43.460]   column above. Now they've defined a certain value and
[01:00:43.460 --> 01:00:46.340]   they're saying if the quality is less than number less than
[01:00:46.340 --> 01:00:50.780]   equal to number three, that means this is of bad quality.
[01:00:50.780 --> 01:00:53.500]   For some reason, I don't know much about wine. So I don't know
[01:00:53.500 --> 01:00:58.900]   what those values represent. But we've created this tensor of
[01:00:58.900 --> 01:01:03.380]   type Boolean. How do I know it's type Boolean because we print
[01:01:03.380 --> 01:01:07.980]   it and it has the values equivalent to the entirety of
[01:01:07.980 --> 01:01:14.140]   the data set. And now we can index inside of a tensor by
[01:01:14.140 --> 01:01:18.340]   passing Boolean values. So this is known as advanced indexing.
[01:01:18.340 --> 01:01:21.740]   How does that work? Try to try to understand that and feel free
[01:01:21.740 --> 01:01:25.340]   to ask any questions if you don't understand this. But what
[01:01:25.340 --> 01:01:30.260]   the authors have done here is they've created three ranges. So
[01:01:30.260 --> 01:01:33.300]   bad data or bad wine is wine with the value less than equal
[01:01:33.300 --> 01:01:36.780]   to three. Mid data has wine between the values three and
[01:01:36.780 --> 01:01:45.580]   seven quality and good wine has the value more than seven. After
[01:01:45.580 --> 01:01:48.820]   that, they summarize different values and that's fairly good.
[01:01:50.100 --> 01:01:52.260]   Let me hop over to the forums and see if there are any
[01:01:52.260 --> 01:01:55.420]   questions, any questions so far with anything we've discussed.
[01:01:55.420 --> 01:02:08.620]   Okay, I don't see any questions. Let me check this chart as well.
[01:02:08.620 --> 01:02:09.620]   Perfect.
[01:02:13.860 --> 01:02:22.260]   So the next problem we arrive at is a time series one. This
[01:02:22.260 --> 01:02:26.260]   has the task of reading in different values and trying to
[01:02:26.260 --> 01:02:29.380]   predict how the bikes are being used across the hour. I'm trying
[01:02:29.380 --> 01:02:33.340]   to recall if any new concept is introduced here. I don't think
[01:02:33.340 --> 01:02:37.020]   so. So I'll, I'll probably skip over this and leave this as a
[01:02:37.020 --> 01:02:40.820]   homework to you all. If you have any questions here, you can ask
[01:02:40.820 --> 01:02:45.500]   them now or later. One thing I'd want to point out, we're using
[01:02:45.500 --> 01:02:47.860]   the lambda function for the first time in the book here. So
[01:02:47.860 --> 01:02:50.300]   if you're not familiar with these, please feel free to ask
[01:02:50.300 --> 01:02:54.100]   or look them up. I'd be happy to take that. I'm just skipping
[01:02:54.100 --> 01:03:00.420]   ahead of this. After this, we come to text. So, so far, we've
[01:03:00.420 --> 01:03:05.820]   learned how to deal with images, also CT scans, tabular data, but
[01:03:05.820 --> 01:03:09.940]   there's a problem with text, which is how do we convert these
[01:03:09.940 --> 01:03:13.820]   words into numbers? So again, I pass this back as a question to
[01:03:13.820 --> 01:03:19.780]   you. How do we read and text data into our models? Assuming
[01:03:19.780 --> 01:03:26.060]   we're trying to let's do any form of work with text. How do
[01:03:26.060 --> 01:03:51.540]   we load that into the memory? The scary thing about zoom is
[01:03:51.540 --> 01:03:54.340]   you're on this call and you wait for people to type sometimes
[01:03:54.340 --> 01:03:56.780]   they do sometimes they don't, it's always a little scary.
[01:03:56.780 --> 01:04:02.740]   That's a great answer. JC 17. You could do one encoding for
[01:04:02.740 --> 01:04:08.140]   sure. And what does this answer tell us? The computer can only
[01:04:08.140 --> 01:04:13.620]   read in numbers. So we need to find a way of representing it as
[01:04:13.620 --> 01:04:20.700]   numbers. But if we do one hot encoding, JC 17. Sorry, I can't
[01:04:20.700 --> 01:04:23.860]   see your name. So I'm just calling you that. It's Josh.
[01:04:23.860 --> 01:04:28.220]   Sorry, Josh, how do we address for spaces there? And how do we
[01:04:28.220 --> 01:04:34.260]   address punctuation? Any thoughts on that? We could use
[01:04:34.260 --> 01:04:39.460]   word to it for sure. We need to find a way to convert the
[01:04:39.460 --> 01:04:43.140]   representation in numbers, create a dictionary of words and
[01:04:43.140 --> 01:04:47.220]   represent a word in dictionary, which is what word to vectors.
[01:04:47.380 --> 01:04:57.860]   So thanks for defining word to it for us. Adele has answered
[01:04:57.860 --> 01:05:02.900]   we can load the text tokenize it. Can you tell everyone what
[01:05:02.900 --> 01:05:09.140]   tokenize stands for and after that we can numeralize it so
[01:05:09.140 --> 01:05:19.500]   that we have numbers. So my question is, how do we address
[01:05:19.500 --> 01:05:23.060]   spaces, punctuation, everything else? How do we take care of
[01:05:23.060 --> 01:05:28.380]   that in this scheme? Yep, that's that's a great answer. We can
[01:05:28.380 --> 01:05:31.060]   just remove it for now. So we're trying to come up with the
[01:05:31.060 --> 01:05:33.620]   simplest idea. Don't try to think of anything. That's a
[01:05:33.620 --> 01:05:38.020]   great way we can just remove everything. I'll take this last
[01:05:38.020 --> 01:05:55.540]   slide. Okay, looks like it's coming through. So what we're
[01:05:55.540 --> 01:06:05.620]   gonna do, let me embarrassingly scroll to the right section. And
[01:06:05.620 --> 01:06:09.120]   this was a flowchart I was trying to show earlier for
[01:06:09.120 --> 01:06:12.040]   continuous ordinal values and categorical data. It's figure
[01:06:12.040 --> 01:06:15.780]   4.4 in the book. You can look at it. I won't go through the
[01:06:15.780 --> 01:06:31.060]   details. It's there in the book. I'm trying to embarrassingly
[01:06:31.060 --> 01:06:36.660]   scroll to the NLP section. I did try opening multiple instances
[01:06:36.660 --> 01:06:39.060]   of the book, but that did not happen. So so far we've
[01:06:39.060 --> 01:06:44.740]   discussed we have let's say, let's say this is the sentence.
[01:06:44.740 --> 01:06:54.140]   And I can to make it difficult, I can make some punctuation
[01:06:54.140 --> 01:06:59.260]   marks. It doesn't have to make sense. So we need to represent
[01:06:59.260 --> 01:07:03.420]   this as numbers for the model to work on it. For that, one of
[01:07:03.420 --> 01:07:06.260]   the suggestions we've received is convert everything into a
[01:07:06.260 --> 01:07:10.660]   number and feed it into a model. Girijay suggested that we could
[01:07:10.660 --> 01:07:15.140]   remove all of the punctuation. So what I'm going to do is I'm
[01:07:15.140 --> 01:07:20.020]   going to remove the punctuation. And I could write a function
[01:07:20.020 --> 01:07:22.900]   that just goes through all of this and doesn't look at it. Or
[01:07:22.900 --> 01:07:25.540]   I could simply read in all of the words and ignore everything.
[01:07:25.540 --> 01:07:30.580]   That's one way. And how do I define words if they do not
[01:07:30.580 --> 01:07:33.460]   start with a special character and they have a space between
[01:07:33.460 --> 01:07:38.260]   them, they are words. So in this scheme, let's becomes zero, say
[01:07:38.260 --> 01:07:42.820]   becomes one, this becomes two, so on and so forth. And our
[01:07:42.820 --> 01:07:47.860]   sentence now can be written as 01234. And whenever the next
[01:07:47.860 --> 01:07:53.420]   time we see the word let's, we see the word let, we just
[01:07:53.420 --> 01:07:58.020]   replace it, replace let so zero and let will be a new word in
[01:07:58.020 --> 01:08:02.660]   this scheme. So one problem with this scheme is, even though
[01:08:02.660 --> 01:08:06.580]   these two are similar words, they are stored differently. So
[01:08:06.580 --> 01:08:09.500]   as you can see, it becomes memory intensive a little and
[01:08:09.500 --> 01:08:14.780]   it's difficult to process through all of that. Sorry, I
[01:08:14.780 --> 01:08:23.900]   shared the wrong screen again. Let me share Safari. Make it
[01:08:23.900 --> 01:08:32.180]   back to our text module. Adil has answered what is
[01:08:32.180 --> 01:08:35.460]   tokenization. Let me go through it. Tokenization is a way of
[01:08:35.460 --> 01:08:39.300]   separating a piece of text into smaller unit called tokens.
[01:08:39.820 --> 01:08:45.300]   They can be words, characters or sub words. That's that's a
[01:08:45.300 --> 01:08:49.220]   perfect answer. Thanks for sharing that. So we also what
[01:08:49.220 --> 01:08:52.060]   we're trying to do is represent everything as numbers.
[01:08:52.060 --> 01:08:55.100]   Tokenization is one way of doing that. I'm going to details
[01:08:55.100 --> 01:08:59.140]   because that's not discussed in the book. So what we've done
[01:08:59.140 --> 01:09:07.180]   here is we've created again, a letters tensor. And we're going
[01:09:07.180 --> 01:09:12.540]   through all of the lines. We're calling a strip function. What
[01:09:12.540 --> 01:09:16.900]   does that do? We'll look at the docs to see what that does. But
[01:09:16.900 --> 01:09:21.780]   that just removes the end of the line. And we get fresh lines.
[01:09:21.780 --> 01:09:25.340]   What are we doing here? Why are we looking at these values?
[01:09:25.340 --> 01:09:28.980]   We're looking at ASCII values. And we know where the numbers,
[01:09:28.980 --> 01:09:34.060]   what numbers define characters. So we're just checking those.
[01:09:35.620 --> 01:09:38.980]   After this, we clean the words. One of the suggestions I was
[01:09:38.980 --> 01:09:43.860]   hoping to get would be we could also have capital or small
[01:09:43.860 --> 01:09:49.100]   letters. And how do we address that? So for that, we convert
[01:09:49.100 --> 01:09:54.420]   everything to lower, we replace new lines with the space. And
[01:09:54.420 --> 01:09:59.940]   we split these into words. Now we create a word list out of
[01:09:59.940 --> 01:10:04.060]   this. So this leaves us with a simple list of all of the
[01:10:04.060 --> 01:10:10.060]   words, every single word that is there inside of our text data
[01:10:10.060 --> 01:10:13.780]   set. As you can imagine, this gets like really complex really
[01:10:13.780 --> 01:10:18.340]   fast because people have trained models on the entire Wikipedia.
[01:10:18.340 --> 01:10:22.740]   And it gets quite quite memory intensive at that point, you
[01:10:22.740 --> 01:10:26.740]   can't load Wikipedia into your RAM or on your graphic card
[01:10:26.740 --> 01:10:29.340]   memory for that matter, unless you're in video and you just
[01:10:29.340 --> 01:10:32.340]   stop the production and just load everything onto the memory
[01:10:32.340 --> 01:10:36.300]   of everything that's on the production then somehow maybe
[01:10:36.300 --> 01:10:40.580]   that that might fit Wikipedia. I'm not sure. Anyways, I
[01:10:40.580 --> 01:10:44.140]   digress. So as you can see, this this becomes as a memory
[01:10:44.140 --> 01:10:49.660]   intensive method. But what we're doing here is we're doing a word
[01:10:49.660 --> 01:10:55.220]   to index mapping. So we convert this into a set, we call the
[01:10:55.220 --> 01:11:00.580]   clean words function, which cleans everything. We sort this,
[01:11:01.820 --> 01:11:06.620]   we sort the set. So what does sort do it for words, it'll just
[01:11:06.620 --> 01:11:09.980]   make sure everything is in alphabetical order. And it
[01:11:09.980 --> 01:11:15.700]   creates this dictionary where we map the words to numbers after
[01:11:15.700 --> 01:11:20.740]   enumerating the list of words. So what this gives us is we get
[01:11:20.740 --> 01:11:28.540]   certain numbers against different words. The reason for
[01:11:28.540 --> 01:11:31.460]   being all of this is we're trying to load this into the
[01:11:31.460 --> 01:11:34.140]   memory, we want it to be represented as numbers. So
[01:11:34.140 --> 01:11:40.580]   that's what we're doing. And that is how we process a certain
[01:11:40.580 --> 01:11:46.020]   sentence to numbers as we try to load it into our memory. I'm
[01:11:46.020 --> 01:11:49.420]   skipping across a lot of details because inside of the book,
[01:11:49.420 --> 01:11:52.300]   these are explained very thoroughly. So I'm being very
[01:11:52.300 --> 01:11:54.940]   careful of the things that are important. And I'm just pointing
[01:11:54.940 --> 01:11:59.140]   that out. Why am I saying this, if you have any things that
[01:11:59.140 --> 01:12:01.900]   aren't clear, I'd really encourage you and remind you
[01:12:01.900 --> 01:12:04.460]   that you're welcome to ask anything our community is quite
[01:12:04.460 --> 01:12:08.660]   open, you can use our forums anytime. Pointing this out real
[01:12:08.660 --> 01:12:16.380]   quick, inside of the repository, there are also ways of loading
[01:12:16.380 --> 01:12:20.900]   audio and video. So we're loading, we're learning how to
[01:12:20.900 --> 01:12:25.500]   load audio. And we also there's an example of how to load video
[01:12:25.500 --> 01:12:32.140]   which I couldn't make, which I couldn't make it to work for
[01:12:32.140 --> 01:12:35.500]   some reason. But I'll probably try that over the week. That
[01:12:35.500 --> 01:12:37.740]   might be because image is broken. So I couldn't figure
[01:12:37.740 --> 01:12:50.660]   that out. I'll come back to this some month afterwards. So let's
[01:12:50.660 --> 01:12:55.580]   see, I think this is a good time to go back to my slides and
[01:12:55.580 --> 01:12:58.220]   remind you all of the learning process. So this is how learning
[01:12:58.220 --> 01:13:04.020]   happens inside of a model. I had hinted at this, we quickly
[01:13:04.020 --> 01:13:12.900]   learned about this. But we have a certain number of inputs that
[01:13:12.900 --> 01:13:16.500]   go through a bit of weights. We learned of weights as being
[01:13:16.500 --> 01:13:20.340]   knobs that are model adjust, and we get outputs through it.
[01:13:21.340 --> 01:13:25.380]   So we do forward pass through this, we get a certain error,
[01:13:25.380 --> 01:13:29.580]   the error tells us how wrong our predictions are. We didn't
[01:13:29.580 --> 01:13:34.700]   understand how that is derived. And then somehow we tell our
[01:13:34.700 --> 01:13:38.820]   model that, Hey, this is how wrong you are. And we try to
[01:13:38.820 --> 01:13:44.340]   adjust these knobs a little. And by doing that, we're trying to
[01:13:44.340 --> 01:13:49.060]   make our predictions better and reducing, we're trying to reduce
[01:13:49.060 --> 01:13:53.180]   our errors. So in practice, what is error?
[01:13:53.180 --> 01:14:05.260]   Error is error could simply be the difference between let me
[01:14:05.260 --> 01:14:09.060]   make sure I share the correct screen. And sorry, zoom is being
[01:14:09.060 --> 01:14:12.700]   annoying. For some reason today, I think it's because I'm sharing
[01:14:12.700 --> 01:14:15.100]   too many and sharing too many times and stopping sharing.
[01:14:15.660 --> 01:14:19.780]   Anyways, so one way of deriving the error is you could simply
[01:14:19.780 --> 01:14:22.860]   subtract the value trying to predict with the value of
[01:14:22.860 --> 01:14:27.340]   predicted. So that tells you how off you are. There are other
[01:14:27.340 --> 01:14:34.700]   ways as well defined in the book that we'll get to learn. So here
[01:14:34.700 --> 01:14:38.180]   learning becomes parameter estimation. That's what the
[01:14:38.180 --> 01:14:41.940]   chapter calls it. And that's because we're trying to estimate
[01:14:41.940 --> 01:14:46.980]   these parameters, these knobs, like I said inside of the model.
[01:14:46.980 --> 01:15:03.420]   So this is a simple problem, they've defined it like
[01:15:03.420 --> 01:15:09.540]   predicting temperatures. So your job is to predict temperature
[01:15:09.540 --> 01:15:13.020]   values as a deep learning practitioner. Let me see if I
[01:15:13.020 --> 01:15:18.300]   can find the plot. So they've also taught us to look at the
[01:15:18.300 --> 01:15:20.980]   data before processing everything. I'm not sure if this
[01:15:20.980 --> 01:15:25.220]   is the final model or if this is before that. So we've plotted
[01:15:25.220 --> 01:15:28.980]   the temperature in Celsius against temperature in Fahrenheit
[01:15:28.980 --> 01:15:32.780]   as you can expect, it's a straight line. I'm trying to
[01:15:32.780 --> 01:15:39.020]   recall if they do plot the data in the book, I don't think they
[01:15:39.020 --> 01:15:46.540]   do. Okay, nobody's. So we define the two values and what we're
[01:15:46.540 --> 01:15:51.900]   trying to predict here using a model. We move those to a
[01:15:51.900 --> 01:15:57.700]   tensor and we define a simple model. So I had called this
[01:15:57.700 --> 01:16:01.900]   entire thing and model, our goal is to pass these inputs and
[01:16:01.900 --> 01:16:05.780]   predict those outputs. So our model now becomes fates
[01:16:05.780 --> 01:16:09.620]   multiplied by T underscore u plus bias. It's a simple linear
[01:16:09.620 --> 01:16:13.540]   model. Why is it a linear model? Because this is the definition
[01:16:13.540 --> 01:16:16.900]   of a straight line in mathematics. If you don't know
[01:16:16.900 --> 01:16:19.580]   that, don't worry, you don't need to I just know that because
[01:16:19.580 --> 01:16:22.260]   I painfully went through an engineering course. And that's
[01:16:22.260 --> 01:16:26.860]   one thing I remember. So don't worry about it. Next, we define
[01:16:26.860 --> 01:16:32.140]   a way of measuring how can we measure the difference between
[01:16:32.140 --> 01:16:36.860]   the predicted values and what they should be. Because we need
[01:16:36.860 --> 01:16:40.020]   this to tell our model how wrong or how right it is so that we
[01:16:40.020 --> 01:16:45.460]   can correct it. And we do this by taking this squared
[01:16:45.460 --> 01:16:48.740]   difference of them inside of the book, they show a really nice
[01:16:48.740 --> 01:16:52.740]   plot and help you understand how these shape the model. But I'll
[01:16:52.740 --> 01:16:58.780]   skip those details because it's quite elaborate. And we define
[01:16:58.780 --> 01:17:02.500]   the weights and biases function, weights and biases function.
[01:17:02.500 --> 01:17:07.460]   See what I did there. Pass them to the model along with the
[01:17:07.460 --> 01:17:17.020]   data. And this gives us a predicted value. But this isn't
[01:17:17.020 --> 01:17:21.860]   learning. Because all we've done here is we've passed these
[01:17:21.860 --> 01:17:25.460]   values through there and we've returned the value. So what's
[01:17:25.460 --> 01:17:30.460]   happened here is just one single forward pass. And we can
[01:17:30.460 --> 01:17:34.300]   calculate the loss value. Now as a rule of thumb, generally
[01:17:34.300 --> 01:17:37.460]   speaking, lower the loss better your accuracy better model is
[01:17:37.460 --> 01:17:42.260]   doing. This seems like a large value. So our model is quite
[01:17:42.260 --> 01:17:47.220]   off. Let's see how we can fix that. But right now we haven't
[01:17:47.220 --> 01:17:49.380]   learned anything. We've simply done a forward pass and
[01:17:49.380 --> 01:17:59.060]   calculated the loss. So we define all of these parameters
[01:17:59.060 --> 01:18:03.780]   and we look at their shapes. These will come in handy as we
[01:18:03.780 --> 01:18:06.420]   try to understand the derivatives. I'll skip through
[01:18:06.420 --> 01:18:10.020]   the derivatives and I'll try to understand how does the learning
[01:18:10.020 --> 01:18:14.660]   work here by looking at the graph. As I scroll down to it
[01:18:14.660 --> 01:18:36.340]   embarrassingly in the book and take a minute to do it. So this
[01:18:36.340 --> 01:18:40.180]   is our loss landscape. Usually it's in 3D sometimes it's in
[01:18:40.180 --> 01:18:43.820]   50 D you can't imagine those space where you just tell yourself
[01:18:43.820 --> 01:18:46.660]   that this technique should work. But since we're in a linear
[01:18:46.660 --> 01:18:51.980]   space right now, or since we're plotting loss against weights,
[01:18:51.980 --> 01:18:55.900]   we can look at this. So what we're trying to minimize here is
[01:18:55.900 --> 01:19:03.260]   the loss. And we do this by using the derivative of this
[01:19:03.260 --> 01:19:07.580]   graph. So we calculate the derivative by differentiating
[01:19:07.580 --> 01:19:10.380]   the slope at that point. And that gives us the direction of
[01:19:10.380 --> 01:19:14.500]   where we should be moving. That's quite mathematical. But
[01:19:14.500 --> 01:19:18.220]   all we're doing here and all you need to know is we're computing
[01:19:18.220 --> 01:19:22.500]   the direction of the slope and we're trying to move our weights
[01:19:22.500 --> 01:19:26.580]   towards that. So see at this particular point for your
[01:19:26.580 --> 01:19:29.740]   weights, the loss is minimum, at least for this given problem.
[01:19:29.740 --> 01:19:35.420]   We'd like our model to be in this vicinity. So we'd want to
[01:19:35.420 --> 01:19:39.940]   move the weights towards this. How do we do that? We calculate
[01:19:39.940 --> 01:19:45.620]   the slope of this graph, and we move our weights towards that.
[01:19:45.620 --> 01:19:49.660]   That's how we fix this problem. But that's how we make our model
[01:19:49.660 --> 01:20:07.100]   train. So we calculate the rate of loss change, which is nothing
[01:20:07.100 --> 01:20:11.940]   but the derivative. And we define a learning rate. So
[01:20:11.940 --> 01:20:15.100]   learning rate tells us how fast would we like to move those
[01:20:15.100 --> 01:20:21.940]   weights in that graph. And we update our weights by multiplying
[01:20:21.940 --> 01:20:25.860]   the learning rate with this value, which is the derivative.
[01:20:25.860 --> 01:20:29.860]   This is explained in the book, how did the authors arrive at
[01:20:29.860 --> 01:20:34.020]   this, it requires some level of math knowledge, you really don't
[01:20:34.020 --> 01:20:38.020]   need it. As we learn how auto grad takes care of this, so I'll
[01:20:38.020 --> 01:20:41.460]   skip that. But what we've done here is we've multiplied the
[01:20:41.460 --> 01:20:46.700]   derivative, and we've updated the weights. And we'd also like
[01:20:46.700 --> 01:20:54.420]   to do this for B. So the model we had defined was w times t
[01:20:54.420 --> 01:20:57.380]   underscore u plus b. And now we'd also like to update the
[01:20:57.380 --> 01:21:05.340]   B values. And we do this a few number of times. And that
[01:21:05.340 --> 01:21:08.660]   becomes our training. So we define different functions that
[01:21:08.660 --> 01:21:13.980]   take care of this, and we update different values inside of this
[01:21:13.980 --> 01:21:19.620]   model. As we do this, our loss reaches infinity. This is
[01:21:19.620 --> 01:21:22.140]   something that you totally don't want to see. And this is
[01:21:22.140 --> 01:21:26.300]   something that really would give you terrors. The reason being,
[01:21:27.060 --> 01:21:32.220]   we defined a quite big learning rate. So our model, instead of
[01:21:32.220 --> 01:21:37.300]   converging, it diverged. So it went beyond loss landscape. To
[01:21:37.300 --> 01:21:41.220]   fix that, you'd like to fix the learning rate to correct value.
[01:21:41.220 --> 01:21:45.700]   And when we define a slow learning rate or a smaller
[01:21:45.700 --> 01:21:51.780]   learning rate, what does that do inside of this graph? Instead of
[01:21:51.780 --> 01:21:55.940]   having crazy upgrades. So let's say when we had the learning
[01:21:55.940 --> 01:22:02.660]   rate of large value, from going here, the model would quickly go
[01:22:02.660 --> 01:22:07.780]   here, it would then go here. And as you can see, it will quickly
[01:22:07.780 --> 01:22:12.020]   go out of this particular scope and probably settle on a really
[01:22:12.020 --> 01:22:16.260]   large value. But if we define a small value, the model would go
[01:22:16.260 --> 01:22:21.620]   from here to here, and slowly and steadily make its place to
[01:22:21.620 --> 01:22:24.380]   here. Now you might be tempted to ask, but what will happen
[01:22:24.380 --> 01:22:27.620]   it'll probably keep moving around it. We define a certain
[01:22:27.620 --> 01:22:31.580]   number of epochs, let's say 10. And by the 10th one, it would be
[01:22:31.580 --> 01:22:36.380]   somewhere around here. So safe to say it's like a good number
[01:22:36.380 --> 01:22:39.140]   of epochs and a good learning rate to make your model
[01:22:39.140 --> 01:22:42.540]   converge. How do we know that by defining and printing these
[01:22:42.540 --> 01:22:47.140]   values and by looking at the printouts from the epochs and
[01:22:47.140 --> 01:22:53.180]   also the graph values. So remember when I claimed that we
[01:22:53.180 --> 01:22:57.180]   only define a few numbers, these are the few numbers that we need
[01:22:57.180 --> 01:23:04.980]   to take care of inside of our training loop. I believe that
[01:23:04.980 --> 01:23:10.100]   was our first model trained. Yep. These are the things I
[01:23:10.100 --> 01:23:18.740]   wanted to point out here. I think we're short on time. So I
[01:23:18.740 --> 01:23:21.740]   instead of rushing through, I'll explain a few things. I'll come
[01:23:21.740 --> 01:23:25.220]   back to AutoGrad in the next session. But I'll talk about
[01:23:25.220 --> 01:23:27.520]   what is overfitting and underfitting. It's quite
[01:23:27.520 --> 01:23:32.020]   important. So usually things can be simplified to graph inside
[01:23:32.020 --> 01:23:38.540]   of deep learning. And as we overfit, the loss value really
[01:23:38.540 --> 01:23:45.100]   shoots up with time with iterations. This model is not
[01:23:45.100 --> 01:23:47.460]   training at all, because ideally, your loss should go
[01:23:47.460 --> 01:23:52.660]   down with iterations. That does happen here, but the red line
[01:23:52.660 --> 01:23:55.540]   shoots up. And the red line, if I remember correctly,
[01:23:55.540 --> 01:24:00.620]   represents our validation loss. So what's a validation data set?
[01:24:00.620 --> 01:24:07.260]   It's the data set that we're benchmarking for. I see Jess's
[01:24:07.260 --> 01:24:11.300]   question, was that a single perceptron model? I don't think
[01:24:11.300 --> 01:24:13.740]   that was a person. Yes, that was a single perceptron. I was
[01:24:13.740 --> 01:24:16.660]   trying to think if that can be classified as a single neuron.
[01:24:16.700 --> 01:24:19.180]   Yes, it can be. So that was a single perceptron model.
[01:24:19.180 --> 01:24:26.020]   Validation data set is the data set that we set aside, we don't
[01:24:26.020 --> 01:24:29.660]   train on it, but we just look at the loss values. And we try to
[01:24:29.660 --> 01:24:32.280]   use that as a benchmark to see if our model is training. So
[01:24:32.280 --> 01:24:34.980]   ideally, with time, the loss should go down. And your
[01:24:34.980 --> 01:24:37.500]   training loss and validation loss shouldn't be too different.
[01:24:37.500 --> 01:24:43.040]   Again, this is, this isn't a well defined explanation of it.
[01:24:43.040 --> 01:24:45.820]   That's because later in the book, we learn how to create a
[01:24:45.820 --> 01:24:50.740]   validation data set, and I'll come around to it. C seems to
[01:24:50.740 --> 01:24:54.340]   be the correctly trained data, the model because both the
[01:24:54.340 --> 01:24:58.620]   values come down properly and reach a nice minima. I would
[01:24:58.620 --> 01:25:03.180]   say this is underfitting because the validation loss isn't as
[01:25:03.180 --> 01:25:07.780]   less as the training loss. And this is overfitting. So
[01:25:07.780 --> 01:25:12.060]   overfitting is when the model just learns your training data.
[01:25:12.060 --> 01:25:15.620]   Underfitting is when it doesn't learn the training data quite
[01:25:15.620 --> 01:25:26.620]   well. And this can be claimed to be a perfect fit. I had
[01:25:26.620 --> 01:25:29.600]   introduced the jargon of artificial neural networks, we
[01:25:29.600 --> 01:25:35.540]   did look at one of these. But quite simply put, it's a bunch
[01:25:35.540 --> 01:25:39.800]   of matrix functions that are happening after a few
[01:25:39.800 --> 01:25:43.100]   nonlinearities followed by another bunch of nonlinearities.
[01:25:45.380 --> 01:25:49.260]   If you want to represent this mathematically and as well as in
[01:25:49.260 --> 01:25:56.960]   code, let's see if I can find some space I like here. So
[01:25:56.960 --> 01:26:03.760]   simply put, it becomes let's say, this is our model. You
[01:26:03.760 --> 01:26:09.940]   can call it model, model equals a function. But has a
[01:26:09.940 --> 01:26:17.560]   nonlinearity, some matrix function, a nonlinearity, so on
[01:26:17.560 --> 01:26:22.560]   and so forth. That's the crux of deep learning. Just
[01:26:22.560 --> 01:26:27.300]   nonlinearity is followed by matrix multiplications. And as
[01:26:27.300 --> 01:26:30.900]   you can see, this is like, straightforward to define in
[01:26:30.900 --> 01:26:34.260]   code as well, right? You can just define a particular matrix
[01:26:34.260 --> 01:26:38.500]   multiplication, you can pass it to, I didn't mean to copy that,
[01:26:38.540 --> 01:26:42.220]   you can pass it to nonlinearity. So it becomes another function.
[01:26:42.220 --> 01:26:45.240]   And then you can pass the values returned through here to
[01:26:45.240 --> 01:26:49.540]   another function. That becomes the definition of a neural
[01:26:49.540 --> 01:27:01.260]   network in memory. I'm trying to share the right screen again,
[01:27:01.260 --> 01:27:13.340]   and for some reason, it always gets shuffled. Okay. So if
[01:27:13.340 --> 01:27:18.500]   you'd like to learn more about these topics, I'd point you to
[01:27:18.500 --> 01:27:22.340]   this book. This is an amazing book called neural networks and
[01:27:22.340 --> 01:27:24.940]   deep learning. That's the website as neural networks and
[01:27:24.940 --> 01:27:28.580]   deep learning.com. Why am I recommending this because this
[01:27:28.620 --> 01:27:32.580]   has nice code exercises along with it as well. Do you need to
[01:27:32.580 --> 01:27:35.500]   go through this? Not really. But it has some really nice
[01:27:35.500 --> 01:27:39.260]   animation and explanation. This is written by Michael Nielsen.
[01:27:39.260 --> 01:27:43.980]   One of the go to books and really fantastic book. If you
[01:27:43.980 --> 01:27:48.740]   want to understand more about these theory, intense side of
[01:27:48.740 --> 01:27:52.500]   things and understand some basic code, I believe it's in NumPy
[01:27:52.500 --> 01:27:55.580]   that goes along with what's a neural network, how does that
[01:27:55.620 --> 01:28:00.100]   work? You can learn through this. I would advise skipping
[01:28:00.100 --> 01:28:03.820]   that. And I would point you to this particular blog post by
[01:28:03.820 --> 01:28:19.100]   Parul Pandey. And she talks about or she shares her secrets
[01:28:19.100 --> 01:28:23.700]   to creating data sets. So why am I sharing these? And why is
[01:28:23.700 --> 01:28:27.500]   this relevant? We learned that how can we today we learned how
[01:28:27.500 --> 01:28:31.180]   do we process or load different data sets. So instead of trying
[01:28:31.180 --> 01:28:34.620]   to go through the notebooks, which you should, and instead of
[01:28:34.620 --> 01:28:38.860]   just loading the same data sets again, try to find similar data
[01:28:38.860 --> 01:28:42.020]   sets, or try downloading any damn thing from the internet.
[01:28:42.020 --> 01:28:46.900]   That is a data set of some format and loaded into the
[01:28:46.900 --> 01:28:50.460]   memory, load it using PyTorch. Try to figure out how do you do
[01:28:50.460 --> 01:28:56.220]   that. And share a GitHub notebook with us share a Jupyter
[01:28:56.220 --> 01:28:59.420]   notebook with us share a GitHub gist with us write about it,
[01:28:59.420 --> 01:29:03.140]   share it in the forums. That's, that's another suggested
[01:29:03.140 --> 01:29:06.900]   homework. I believe we're at the top of the hour. So I'll take
[01:29:06.900 --> 01:29:11.660]   two minutes to ask or to answer any questions. We'll take a
[01:29:11.660 --> 01:29:15.100]   pause and see if there are any other questions coming in. To
[01:29:15.100 --> 01:29:20.380]   answer Girijish your question. This is a linearity. So this is
[01:29:20.380 --> 01:29:23.340]   a linear equation. Yes, I was talking about neural networks in
[01:29:23.340 --> 01:29:27.580]   general. So generally speaking, neural networks would have a
[01:29:27.580 --> 01:29:31.460]   non linearity outside of this. So not this value would be
[01:29:31.460 --> 01:29:35.780]   passed to a nonlinear function. And we'd go from here, we could
[01:29:35.780 --> 01:29:37.060]   have multiple of those happen.
[01:29:48.900 --> 01:29:52.220]   I don't think there's any need to share the slides. It's just
[01:29:52.220 --> 01:29:55.740]   so I didn't mention this, but just to credit again, it's from
[01:29:55.740 --> 01:29:59.180]   the book, all of those images are from the book, I just snap
[01:29:59.180 --> 01:30:03.620]   those and put them in there to highlight things. The goal of
[01:30:03.620 --> 01:30:07.620]   this group is to more of first of all, share some suggestions
[01:30:07.620 --> 01:30:10.340]   on things you can do with PyTorch as we especially get to
[01:30:10.340 --> 01:30:16.140]   the project section. And also try to recap the book. So if
[01:30:16.140 --> 01:30:19.420]   you're trying to read the book, we come together we through it.
[01:30:19.420 --> 01:30:28.140]   And you get a heads up or you get a recap. Awesome. I don't
[01:30:28.140 --> 01:30:32.020]   see any questions. So I'll end the live stream. And thanks
[01:30:32.020 --> 01:30:34.860]   everyone for joining. I look forward to seeing you next week.
[01:30:34.860 --> 01:30:38.020]   Please feel free to hop onto the forums. Ask any questions if
[01:30:38.020 --> 01:30:40.900]   you'd like. I'm always available. We have an amazing
[01:30:40.900 --> 01:30:44.740]   community. Consider doing those homeworks. Consider exploring
[01:30:44.740 --> 01:30:48.140]   different code functions and tell us what you learned. And
[01:30:48.140 --> 01:30:50.100]   I'll see you next week. Thanks for joining. Have a great
[01:30:50.100 --> 01:30:50.540]   weekend.
[01:30:50.540 --> 01:31:00.540]   [BLANK_AUDIO]

