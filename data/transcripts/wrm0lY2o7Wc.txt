
[00:00:00.000 --> 00:00:06.520]   Soon after I graduated from UCLA, my kind of experience in architecture has been all
[00:00:06.520 --> 00:00:07.520]   over the place.
[00:00:07.520 --> 00:00:12.920]   So on the one hand, I was teaching undergraduate studio at Berkeley, and on the other, I was
[00:00:12.920 --> 00:00:18.320]   working in an office in San Francisco, and I also work with urban design think tanks.
[00:00:18.320 --> 00:00:23.280]   In the office, I spend a lot of time just drawing bathrooms, fixing window bays, and
[00:00:23.280 --> 00:00:27.160]   drawing stairs, and making sure that drawings have the correct line weight.
[00:00:27.160 --> 00:00:31.480]   Basically, I do a lot of labor-intensive work, which people in AI these days would expect
[00:00:31.480 --> 00:00:36.080]   to have them all replaced in maybe 15 years.
[00:00:36.080 --> 00:00:41.520]   Architects obviously make a lot of claims about how the projects are very impactful,
[00:00:41.520 --> 00:00:46.800]   but as scientists know, every time you try to make a claim, you have to state your assumptions,
[00:00:46.800 --> 00:00:51.280]   and you have to find ways to validate your assumptions, and you have to set out a very
[00:00:51.280 --> 00:00:56.800]   rigorous path about why your actions or doing certain things lead to certain impacts.
[00:00:56.800 --> 00:01:03.320]   And in the traditionally creative way of practice, people tend to believe that if they can frame
[00:01:03.320 --> 00:01:07.960]   their problems in a creative and relatable way, then the solutions would come up, and
[00:01:07.960 --> 00:01:10.400]   the solutions would lead to impacts.
[00:01:10.400 --> 00:01:14.640]   But as I go through more and more of these urban development projects, and really engage
[00:01:14.640 --> 00:01:18.760]   with stakeholders and communities, I really see that people don't really have trust in
[00:01:18.760 --> 00:01:20.120]   that mode of practice.
[00:01:20.120 --> 00:01:25.360]   It kind of got me to start thinking about a more data-driven career.
[00:01:25.360 --> 00:01:30.800]   I was thinking about being an analyst or a data scientist, or maybe moving deeper into
[00:01:30.800 --> 00:01:36.160]   AI, but that was all kind of open-ended at that moment.
[00:01:36.160 --> 00:01:41.400]   So the first time I got to learn some aspects of deep learning was in this class at Berkeley
[00:01:41.400 --> 00:01:43.640]   called NLP with Deep Learning.
[00:01:43.640 --> 00:01:48.280]   It was a really interesting class because we get to understand many interesting aspects
[00:01:48.280 --> 00:01:52.200]   about language itself, and teaching machines to understand language.
[00:01:52.200 --> 00:01:57.380]   And I would say subsequently I started TAing for this class and tried to find different
[00:01:57.380 --> 00:02:01.980]   internships that would allow me to do this kind of work, and then started taking free
[00:02:01.980 --> 00:02:08.380]   online classes, and I go to Waits and Baskies classes, and try to kind of get more practical
[00:02:08.380 --> 00:02:10.800]   experience in deep learning.
[00:02:10.800 --> 00:02:16.640]   I think the analogy here is to compare somebody who studies mechanical engineering versus
[00:02:16.640 --> 00:02:21.000]   somebody who just wants to build a car in their backyard.
[00:02:21.000 --> 00:02:25.400]   If you want to build a car in your backyard, you just maybe read through books, watch some
[00:02:25.400 --> 00:02:31.400]   videos, talk to several friends to see what's the most straightforward way to get this thing
[00:02:31.400 --> 00:02:36.320]   working, to get a certain part of your car starting or working.
[00:02:36.320 --> 00:02:39.800]   As you go through that process, if you find that you need a better understanding of a
[00:02:39.800 --> 00:02:45.140]   certain mechanism in your car, you just go to the more advanced set of materials.
[00:02:45.140 --> 00:02:49.520]   But the path for a mechanical engineer can be quite different because they really have
[00:02:49.520 --> 00:02:55.120]   to learn it from the level of how all kinds of mechanical systems work.
[00:02:55.120 --> 00:03:00.480]   And I think for me, I'm definitely starting as a person who's building the car in her
[00:03:00.480 --> 00:03:01.480]   backyard.
[00:03:01.480 --> 00:03:04.340]   Neural networks are a little bit like Lego blocks.
[00:03:04.340 --> 00:03:10.200]   If we learn how different layers or different blocks work, then we can think of creative
[00:03:10.200 --> 00:03:13.160]   ways to combine and change them together.
[00:03:13.160 --> 00:03:22.400]   Outside of school, I always go to these more hands-on, pragmatic, code-first videos to
[00:03:22.400 --> 00:03:27.880]   understand how I can implement these things, to understand the most basic diagram and idea
[00:03:27.880 --> 00:03:29.520]   behind that neural network.
[00:03:29.520 --> 00:03:36.920]   And then gradually, as I hear more and more about a certain neural network mechanism,
[00:03:36.920 --> 00:03:45.120]   I may move on to classes that focus more on a lower-level implementation of that mechanism.
[00:03:45.120 --> 00:03:51.520]   For instance, I may move on from a class that primarily focuses on higher-level TensorFlow
[00:03:51.520 --> 00:03:58.720]   or Keras API to a class that focuses more on implementing certain mechanisms on the
[00:03:58.720 --> 00:04:05.120]   level of NumPy, and then gradually maybe move on to a paper on which this type of neural
[00:04:05.120 --> 00:04:06.920]   network first appeared.
[00:04:06.920 --> 00:04:12.000]   And then later on, if I really feel that I absolutely need to know everything about this
[00:04:12.000 --> 00:04:19.000]   neural network, then I may have to start-- I might have to dig into the linear algebra,
[00:04:19.000 --> 00:04:24.600]   all the matrix transformations, or even start-- even try to replicate a certain part of that
[00:04:24.600 --> 00:04:25.600]   neural network.
[00:04:25.600 --> 00:04:30.360]   I spent the first year of my data science or deep learning career just on the simplest
[00:04:30.360 --> 00:04:37.360]   sets of classes, and then later on, as I find a need to implement these projects, I go to
[00:04:37.360 --> 00:04:41.440]   a deeper and more difficult classes.
[00:04:41.440 --> 00:04:45.400]   After I joined the data science program at Berkeley, there were interesting projects
[00:04:45.400 --> 00:04:51.280]   and readings about the topic of how you connect policies with social impact.
[00:04:51.280 --> 00:04:57.440]   In this semester in particular, I'm working with three other teammates from Berkeley on
[00:04:57.440 --> 00:05:01.280]   a project called Gentrify.
[00:05:01.280 --> 00:05:11.480]   It's a policy simulation tool that kind of helps policymakers to understand or project
[00:05:11.480 --> 00:05:19.160]   the potential impacts of a certain policy change, such as increasing a certain number
[00:05:19.160 --> 00:05:24.360]   of affordable density-- affordable housing units in a place like Boyle Heights.
[00:05:24.360 --> 00:05:33.800]   If we do that, what's the change in terms of, let's say, median housing income or demographic
[00:05:33.800 --> 00:05:34.920]   shifts?
[00:05:34.920 --> 00:05:40.880]   Since I started my career or study in data science, machine learning, and deep learning,
[00:05:40.880 --> 00:05:44.120]   every three months I would work on a different project.
[00:05:44.120 --> 00:05:48.440]   Sometimes they were more related to my past career, and sometimes they were less related
[00:05:48.440 --> 00:05:55.200]   to my past career and more related to just learning the state-of-the-art models in general.
[00:05:55.200 --> 00:05:57.160]   That has been quite helpful for me.
[00:05:57.160 --> 00:06:02.200]   Setting these different projects deadlines, like seasonal project deadlines, definitely
[00:06:02.200 --> 00:06:04.900]   helped to build up that portfolio.
[00:06:04.900 --> 00:06:09.920]   In deep learning researchers, in the process of solving machine learning problems, many
[00:06:09.920 --> 00:06:14.640]   times they would have to break down a very complex problem into sub-problems.
[00:06:14.640 --> 00:06:18.840]   And so that process of breaking down complex problems is actually something that I had
[00:06:18.840 --> 00:06:22.400]   to go through in architecture as well.
[00:06:22.400 --> 00:06:25.660]   Urban design and development projects are very complex.
[00:06:25.660 --> 00:06:31.360]   We have to come up with creative ways to break down that problem into sub-pieces.
[00:06:31.360 --> 00:06:35.640]   And to my surprise, I actually had to do that a lot as well.
[00:06:35.640 --> 00:06:43.560]   What was incredibly helpful from my prior experience was the ability to use graphic
[00:06:43.560 --> 00:06:45.720]   illustrations.
[00:06:45.720 --> 00:06:50.480]   When I started working at Bayer, the pipeline itself is very complex to run, and the data
[00:06:50.480 --> 00:06:53.560]   management is kind of complex as well.
[00:06:53.560 --> 00:07:00.120]   It was not until I joined and actually started a new set of slide deck and diagrams that
[00:07:00.120 --> 00:07:02.200]   they finally feel that that problem is solved.
[00:07:02.200 --> 00:07:07.680]   And from now on, they can onboard somebody to this really complex project in a week.
[00:07:07.680 --> 00:07:14.040]   In the NLP class that I TA for, every semester there's a question about tooling.
[00:07:14.040 --> 00:07:22.720]   And also every semester there was a question about how teams can collaborate on model training,
[00:07:22.720 --> 00:07:26.560]   because everyone's setup can be a little different.
[00:07:26.560 --> 00:07:31.000]   And then they may be using different data sets, and maybe using different numbers of
[00:07:31.000 --> 00:07:32.000]   parameters.
[00:07:32.000 --> 00:07:35.400]   And so it became impossible to track.
[00:07:35.400 --> 00:07:40.160]   And the question is also, how do you compare all these different models?
[00:07:40.160 --> 00:07:48.040]   And so when I took Lucas' class last year, what was really helpful with the Weights and
[00:07:48.040 --> 00:07:54.960]   Biases platform was that different team members get to submit their code to this common platform,
[00:07:54.960 --> 00:07:58.560]   and the platform tracks everyone's model performance.
[00:07:58.560 --> 00:08:04.720]   And it also shows the number of hyperparameters and lets you customize the metric graphs,
[00:08:04.720 --> 00:08:11.120]   which you use to compare, let's say, validation or training accuracy or human performance.
[00:08:11.120 --> 00:08:18.640]   I think the tool was really helpful for teams to collaboratively train models based on the
[00:08:18.640 --> 00:08:21.800]   same data set and for the same problem in that way.
[00:08:21.800 --> 00:08:28.880]   And also, just for myself working alone, I also find it incredibly helpful to have the
[00:08:28.880 --> 00:08:34.880]   Weights and Biases tool, because I can compare one model to the next, versus in the past
[00:08:34.880 --> 00:08:44.640]   I always have to write my own logging code to keep track of all the graphs and all the
[00:08:44.640 --> 00:08:51.280]   numbers and then write another Python module to parse all the metrics.
[00:08:51.280 --> 00:08:56.560]   With the Weights and Biases tool, you only need to add two lines of code in your Python
[00:08:56.560 --> 00:09:02.520]   module, and the tool itself kind of does all the rest for you.
[00:09:02.520 --> 00:09:08.340]   And every time I submit a piece of code, it will return a single line of URL, and I can
[00:09:08.340 --> 00:09:13.800]   just use that line of URL to log on to my browser and see how my current model is doing
[00:09:13.800 --> 00:09:16.800]   and compare it to all the past models.
[00:09:16.800 --> 00:09:23.280]   Versus a tool like TensorFlow, there is still a lot more to keep track of manually.
[00:09:23.280 --> 00:09:29.600]   I understand that sometimes there's value in that if your pipeline is highly customized,
[00:09:29.600 --> 00:09:38.520]   but for someone who's just starting on machine learning or deep learning or someone who already
[00:09:38.520 --> 00:09:44.240]   has their requirements quite well defined, having that Weights and Biases tool to take
[00:09:44.240 --> 00:09:47.880]   care of the rest for you is actually very beneficial.
[00:09:47.880 --> 00:09:53.800]   During my experience in the past two to three years, most of my peers, whether in my Berkeley
[00:09:53.800 --> 00:09:59.200]   program, whether in any workshops that I ran into, or through an online community, a lot
[00:09:59.200 --> 00:10:03.320]   of these people actually are mid-career changers.
[00:10:03.320 --> 00:10:09.560]   A lot of these people have prior backgrounds in the medical industry, in the legal industry.
[00:10:09.560 --> 00:10:17.120]   I have worked with PhDs in psychologies before.
[00:10:17.120 --> 00:10:21.240]   One of my peers at Berkeley is even an English literature graduate.
[00:10:21.240 --> 00:10:26.640]   But everyone is still really passionate and deeply believes that they would have a bright
[00:10:26.640 --> 00:10:33.760]   future once they acquire this amazing skill set and either switch to an entirely new industry
[00:10:33.760 --> 00:10:40.640]   or bring some new elements and improvement to their existing industry.
[00:10:40.640 --> 00:10:46.000]   And I sort of think that it's important for any mid-career changers who's interested in
[00:10:46.000 --> 00:10:50.840]   the space of machine learning and deep learning to feel that the door is always open for them
[00:10:50.840 --> 00:10:53.440]   and age should not be a factor, I think.

