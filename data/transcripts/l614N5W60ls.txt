
[00:00:00.000 --> 00:00:23.600]   Hello everybody, my name is Aima. I'm a Program Manager at Microsoft. It's a pleasure to talk to
[00:00:23.600 --> 00:00:31.120]   you today about Foundry Local which enables developers to easily build up cross-platform
[00:00:31.120 --> 00:00:40.080]   applications powered by local AI. So let's get started. The first question is if the cloud
[00:00:40.080 --> 00:00:50.400]   AI is so powerful, why do we need local AI? So here are four key reasons based on our conversations
[00:00:50.400 --> 00:00:59.920]   and observations with our customers. So first of all, how does cloud AI work in environments with
[00:00:59.920 --> 00:01:09.200]   low network bandwidth or even offline access? Many of you have experienced the bad internet during this
[00:01:09.200 --> 00:01:16.960]   conference, right? And a common sentence we have heard from many speakers with live demo is, oh,
[00:01:16.960 --> 00:01:23.920]   finger crossed, hopefully the WiFi connection is good. So it's not their fault. It's required by cloud AI.
[00:01:23.920 --> 00:01:31.280]   But it's not a concern at all for my session because my live demo runs entirely locally.
[00:01:31.280 --> 00:01:39.520]   So reason number two, privacy and security. Many companies work with very sensitive data,
[00:01:39.520 --> 00:01:48.800]   such as legal documents and patient information. They need to process that data entirely locally,
[00:01:48.800 --> 00:01:59.440]   without anything ever leaving device, right? And reason number three, cost efficiency. Think about game
[00:01:59.440 --> 00:02:08.080]   applications, which is deployed to millions of devices, with hundreds of millions of inference calls every day.
[00:02:08.080 --> 00:02:17.040]   It's not just sustainable. And reason number three, real time latency. So many AI applications need to
[00:02:17.040 --> 00:02:26.880]   respond in real time. And it's not just not possible if we wait on cloud. So that's why we need a local AI.
[00:02:26.880 --> 00:02:31.040]   Then the next question, whether local AI is ready now?
[00:02:31.040 --> 00:02:40.400]   So thanks to the decades of progress in computing, hardware has become more and more powerful.
[00:02:40.400 --> 00:02:48.960]   So many current devices are equipped with modern GPUs, NPUs, capable of running advanced AI models.
[00:02:48.960 --> 00:02:58.960]   Meanwhile, model companies are keeping publishing more and more models, which are leaner, faster, more optimized for
[00:02:59.600 --> 00:03:09.120]   local inference, such as 5.4 milli and deep-seek small variants. And we are also seeing more and
[00:03:09.120 --> 00:03:17.280]   more advanced state-of-art optimization techniques introduced at runtime level. So this convergence
[00:03:17.280 --> 00:03:24.480]   now makes the local AI a reality. Then how do we build up the solution?
[00:03:24.480 --> 00:03:35.600]   So Microsoft already has many great assets. Azure AI Foundry introduced last year at Microsoft Eagle Light,
[00:03:35.600 --> 00:03:47.920]   has been trusted by over 70,000 organizations, with over 1,900 models. And then our cross-platform high
[00:03:47.920 --> 00:03:56.720]   performance on device inference engine has now seen over 10 million downloads per month. And our customers
[00:03:56.720 --> 00:04:03.760]   are pretty happy with the significant performance acceleration provided by Onyx runtime compared to
[00:04:03.760 --> 00:04:12.880]   PyTorch. And lastly, let's not forget Windows. The scale and the reach of Windows on client devices are
[00:04:12.880 --> 00:04:23.440]   massive. So when we think about democratizing AI, the millions of devices and the millions of customers
[00:04:23.440 --> 00:04:32.080]   using Windows devices really matter to us. So we are not starting from scratch. We are bringing all those
[00:04:32.080 --> 00:04:42.480]   advanced assets into Foundry local and optimized end-to-end solution for seamless on-device AI. So
[00:04:42.480 --> 00:04:51.040]   at the bottom, as you can see, it used Onyx runtime to accelerate the performance across various camps of
[00:04:51.040 --> 00:05:00.080]   hardware. On the top, we are introducing a new Foundry local management service, which hosts and manage
[00:05:00.080 --> 00:05:09.840]   model on your client device. It also connects to Azure AI Foundry to download open source models on demand.
[00:05:09.840 --> 00:05:20.320]   And for user experience, we provide Foundry local CLI, which allows you to easily explore models on device.
[00:05:20.320 --> 00:05:28.080]   And we also provide SDK so that developers can easily integrate Foundry local into your own applications
[00:05:28.080 --> 00:05:35.840]   applications from cloud to local from different hardware. So Foundry local was officially announced
[00:05:35.840 --> 00:05:45.120]   just one month ago at Microsoft Builder conference. It's available on both Windows and Mac OS. On the Windows,
[00:05:45.120 --> 00:05:52.800]   it is integrated into the platform, which makes the experience even simpler for Windows AI developer.
[00:05:55.600 --> 00:06:02.640]   As I just mentioned on Foundry local accelerator performance across different kinds of silicon. We
[00:06:02.640 --> 00:06:11.040]   have been closely working with hardware vendors, including Nvidia, Intel, AMD, Qualcomm to integrate their
[00:06:11.040 --> 00:06:19.280]   hardware accelerators into Foundry local, ensuring the best-in-class performance that you can get on their hardware.
[00:06:22.240 --> 00:06:31.120]   So before our official announcement, over 100 customers joined our private preview. They have shared a valuable
[00:06:31.120 --> 00:06:40.320]   feedback on how easy Foundry local is to use and how good the performance it is. So let's hear some of their feedback.
[00:06:46.000 --> 00:06:52.880]   Hey there, Savu here, CEO and co-founder at Pieces, where we've been on an ambitious journey to give developers
[00:06:52.880 --> 00:07:01.520]   artificial long-term memory across the OS. Now, offline-first AI is core to this vision. And in late 2022, we began to explore
[00:07:01.520 --> 00:07:14.320]   small language models running at the edge on all major platforms. But between rolling our own, Llama C++, and then to O Llama, frustrations around versioning, performance, and end-user experience still remained.
[00:07:14.320 --> 00:07:21.360]   That was until our recent partnership with Microsoft on their new Foundry local platform, an end-to-end AI inference
[00:07:21.360 --> 00:07:27.440]   solution that offers ease of use and high performance across different hardware. In no time, our team went from
[00:07:27.440 --> 00:07:34.160]   documentation access to a production-ready build with noticeable improvements in memory management, time-to-first token,
[00:07:34.160 --> 00:07:39.680]   and tokens per second. If you're looking to deploy on-device models, you can't go wrong with Foundry local.
[00:07:39.680 --> 00:07:46.720]   We have been working on AI projects for our customers for several years now.
[00:07:46.720 --> 00:07:53.120]   Some clients want to use the latest AI technologies, but are restricted from using external services
[00:07:53.120 --> 00:07:58.400]   when the information they want to process contains sensitive data. Foundry local is a perfect solution for
[00:07:58.400 --> 00:08:04.720]   these scenarios, as it allows us to easily run Gen AI models locally. Here, we can see a solution that combines
[00:08:04.720 --> 00:08:18.160]   Foundry local with a speech-to-text service, which also runs locally. One aspect we were really impressed by
[00:08:18.160 --> 00:08:23.440]   was the simplicity of the installation and the ease of using the models. With Foundry local, we can now
[00:08:23.440 --> 00:08:30.480]   create hybrid solutions where part of the solution can be run locally. It's been our privilege to work with
[00:08:30.480 --> 00:08:39.440]   these customers to improve Foundry local together. All right, I will talk in love. Who wants to see live demos?
[00:08:41.280 --> 00:08:53.040]   Okay, let's do that. So first of all, we can let's see our CLI experience. So on Windows platform,
[00:08:53.040 --> 00:09:00.880]   you can install Foundry local package using Winget, and on Mac OS, you can use the homebrew commands.
[00:09:02.240 --> 00:09:20.720]   So I have already installed Foundry local. So first, we want to see what models supported by Foundry local. So we can type Foundry
[00:09:23.120 --> 00:09:34.160]   Model list. So as you can see, it supports many popular generative AI models. And for each model,
[00:09:34.160 --> 00:09:41.840]   you can get different variants optimized for different hardware. So you can see we have optimization version
[00:09:41.840 --> 00:09:51.040]   for CPU, for CUDA, for integrated GPU. We also provide NPU variants because my device doesn't contain
[00:09:51.040 --> 00:10:00.640]   core count NPU. So that variants doesn't show up. Okay, so we want to run some models, right? And if you
[00:10:00.640 --> 00:10:07.520]   haven't downloaded the model before, the Foundry local will download the model from the cloud and then
[00:10:07.520 --> 00:10:14.960]   run the model. It requires internet. But I have already pre-downloaded the model, so we don't need that.
[00:10:14.960 --> 00:10:20.240]   So we're going to see what model I have already downloaded using Foundry.
[00:10:20.240 --> 00:10:29.600]   Foundry cache next. So as you can see, I have downloaded four models here. So I want to, during our
[00:10:29.600 --> 00:10:36.720]   experiments, we might want to explore different models to see the quality, to see the performance,
[00:10:36.720 --> 00:10:43.760]   then decide which model we want to use to build up our application, right? So firstly, I want to try
[00:10:44.560 --> 00:10:54.960]   Foundry model run QWIN 2.5, 1.5 billion model.
[00:10:54.960 --> 00:11:10.560]   Since I have already downloaded this model, so the model loading is pretty quick, should be pretty quick.
[00:11:11.520 --> 00:11:18.880]   Okay, the model is set up. You can talk to the model directly. So let's ask a simple question. What was our
[00:11:18.880 --> 00:11:30.720]   next runtime? Oh, it's pretty quick, right? So I think we may want to see the latency number. So let's
[00:11:30.720 --> 00:11:41.840]   let's exit here and rerun it with verbose mode. And same question.
[00:11:41.840 --> 00:11:52.880]   Okay, so here we get around the 90 tokens per second. We also want to try our different model. So let's do that.
[00:11:54.480 --> 00:12:04.720]   We want to try our different model. So we want to try foundry models on this 5.4 mini.
[00:12:04.720 --> 00:12:15.120]   Also with verbose mode.
[00:12:15.120 --> 00:12:20.400]   So it's loading the model.
[00:12:20.400 --> 00:12:33.200]   Okay, model is set up. Same question. What's our next long time?
[00:12:33.200 --> 00:12:43.280]   So 5.4 mini is more advanced than like Cuban model. The model size is bigger than the Cuban model. So
[00:12:44.080 --> 00:12:51.520]   I would say in terms of the performance, it is a bit smaller than Cuban model. But in terms of the
[00:12:51.520 --> 00:12:57.120]   quality, as you can see, 5.4 mini can provide more detailed information.
[00:12:57.120 --> 00:13:06.720]   All right. So personally, I vote on 5.4 mini. So I want to use this model to build up an application.
[00:13:06.720 --> 00:13:13.920]   So what application do we want to build? I guess many of you have such experience. Your team moved to a
[00:13:13.920 --> 00:13:20.960]   new organization, needed to ramp up the existing project very quickly. And there are many long,
[00:13:20.960 --> 00:13:28.640]   detailed documents you needed to read. And it's very time consuming to read every word, right? So you
[00:13:28.640 --> 00:13:35.920]   might want some high level summarized version of all of this project. So you can quickly ramp up. And
[00:13:35.920 --> 00:13:42.800]   but this project is an internal project. You cannot upload all those documents to a cloud. And meanwhile,
[00:13:42.800 --> 00:13:48.400]   you have, you know, some of your team members are using Windows, some of your team members are using
[00:13:48.400 --> 00:13:55.600]   mic. So you want to build a application cross platform powered by local AI. So let's do that.
[00:13:55.600 --> 00:14:09.600]   So I have this application setting up. So what it does, so we can run it first.
[00:14:09.600 --> 00:14:23.600]   And let's quick the existing conversation.
[00:14:23.600 --> 00:14:35.920]   All right. So the app is setting up. Basically, it is used to summarize content. You can give it a
[00:14:35.920 --> 00:14:43.520]   URL or you give it a local file. So it can do summarization. And it also has a setting tab. You can
[00:14:43.520 --> 00:14:52.720]   choose the model you want to run. As mentioned before, I prefer 5.4 mini. So because I want to get some more
[00:14:52.720 --> 00:15:06.720]   more detailed information. So we can put this model ID here.
[00:15:06.720 --> 00:15:19.200]   Then I will pass it with our project document. And that's it to give me some high level information.
[00:15:19.200 --> 00:15:28.880]   And I will hit summarization. So as you can see, the summarized version is coming out. And it says
[00:15:28.880 --> 00:15:35.600]   "Foundry local is useful to build up across platform AI applications that run directly on device." That's
[00:15:35.600 --> 00:15:45.600]   pretty cool. And then let's take a look at the code quickly. So as you can see, we, in terms of SDK, we
[00:15:46.640 --> 00:15:58.800]   provide Python SDK and the JavaScript SDK. So here we use a JavaScript one. So we create the Foundry local manager. And we
[00:16:02.400 --> 00:16:24.240]   So we initiate the manager with the model name. So as you can see, I passed the 5.4 mini here. And then
[00:16:24.800 --> 00:16:33.200]   it uses Foundry local endpoint to create a client. And then you just wait for the chat to be complete,
[00:16:33.200 --> 00:16:40.320]   to be completed, and output the result. So this is the application running on Windows.
[00:16:40.320 --> 00:16:51.680]   I, somehow, one team member is using Mac. So I want he to use my app as well. So I package the whole project and
[00:16:51.680 --> 00:16:56.480]   share it to him. And then let's see what his experience is.
[00:16:56.480 --> 00:17:08.560]   He take the project and record a demo for me. So as you can see, the exactly the same code. And he just
[00:17:08.560 --> 00:17:18.880]   used the same command in npm run start to start this application. And exactly UI, exactly application. And oh,
[00:17:18.880 --> 00:17:32.880]   he chose Q1 model. Maybe he likes this model more. And he also used the same documents I used in my previous demo. And hit summarize button.
[00:17:35.200 --> 00:17:54.400]   So as we see in the previous demo, Q1 model is kind of provide more brief information than 5.4 mini model. So as you can see here, it also shows the summarization is more shorter than what 5.4 mini provides.
[00:17:55.360 --> 00:18:05.680]   Okay, so we build up cross platform applications. Is that all my demo? Of course not. We forgot one important thing.
[00:18:05.680 --> 00:18:22.800]   Agent, right? So everybody talks about agent. So we, so do I. So Foundry Local enables you to easily create and build and run a local agent using local model and MCP servers.
[00:18:23.520 --> 00:18:32.800]   This feature is still in private preview. But I want to give you a quick look. So you know how it works.
[00:18:32.800 --> 00:18:44.640]   So let's back to our CMD. So we can use Foundry Agent List
[00:18:46.880 --> 00:18:55.280]   to show all the available agents in Foundry Local. As you can see, we have built up three sample agents here.
[00:18:55.280 --> 00:19:06.160]   So in terms of the concept, an agent in Foundry Local consists of one model and one more MCP servers based on your need.
[00:19:06.160 --> 00:19:14.080]   So you can use one model from the list and pick up whatever MCP server you like to create your own agent.
[00:19:14.080 --> 00:19:24.880]   But here we want to run existing one. So I'm interested in this OCR agent. So let's see. Foundry
[00:19:24.880 --> 00:19:31.040]   agent info to know what it can do.
[00:19:35.360 --> 00:19:44.560]   Okay. Okay. So it can extract the text from images in your local device. And this agent contains one model,
[00:19:44.560 --> 00:19:53.360]   which is the FIFO mini, my favorite one. And the two MCP servers. One is file system MCP server, one OCR mini MCP server.
[00:19:53.920 --> 00:19:59.040]   So let's run this agent. Foundry agent run.
[00:19:59.040 --> 00:20:11.200]   So this command will check the dependencies of this agent first. If the dependency hasn't been installed
[00:20:11.200 --> 00:20:17.920]   before, it will be installed with your permission. So I have already installed all those dependencies.
[00:20:17.920 --> 00:20:26.240]   So it just run. Okay. The agent is setting up. It asks for permission to use this MCP server.
[00:20:26.240 --> 00:20:39.680]   So let's say yes. And he asks directory, you want it to access. I give it the demo folder.
[00:20:40.320 --> 00:20:45.840]   And he also asks permission for the OCR MCP server. I would say yes.
[00:20:45.840 --> 00:20:56.640]   So, all right. So from here, you can get all the tools supported by this agent. So literally,
[00:20:56.640 --> 00:21:03.120]   the tools provided in the MCP servers. So you can get some tools related to file system,
[00:21:03.120 --> 00:21:15.920]   tools related to OCR. So what we want it to do. So here is the use query. I want it to find my receipt,
[00:21:15.920 --> 00:21:25.600]   process it, and get the total amount from it. So let's see whether it can complete this task or not.
[00:21:25.600 --> 00:21:33.040]   So it starts to thinking because it needs to figure out which tool to use. Okay. The first tool
[00:21:33.040 --> 00:21:40.240]   tool to use is search file because it needs to find the receipt. And then he figure out to use the
[00:21:40.240 --> 00:21:51.120]   after search, it'll use the OCR one to extract the text and then get the output, get the total amount.
[00:21:51.120 --> 00:21:58.560]   Okay. That's cool. So that's all my demo. So finally, I know it's a little bit over time,
[00:21:58.560 --> 00:22:06.800]   but I just quickly run POP. Foundry local enables you to build up applications powered by local AI. And
[00:22:06.800 --> 00:22:21.200]   one best practice. So local model generally are not that capable as cloud model. So you cannot expect it to
[00:22:21.200 --> 00:22:29.280]   do the fancy work that cloud model or cloud agent can do. But it has unlocked a lot of potential. So
[00:22:29.280 --> 00:22:37.040]   I leave that to you guys to explore. If you want to get more information, here's the link. And you want
[00:22:37.040 --> 00:22:50.880]   to try out our agent feature, you can sign up our private preview form. All right. Thanks, everyone.

