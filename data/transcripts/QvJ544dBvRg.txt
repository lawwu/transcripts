
[00:00:00.000 --> 00:00:05.840]   What we're going to talk about today is a sort of different approach to doing, in particular,
[00:00:05.840 --> 00:00:09.600]   vector calculus based off of something called the Frechet derivative.
[00:00:09.600 --> 00:00:16.880]   And vector calculus is something that is at the center of machine learning. So, for one,
[00:00:16.880 --> 00:00:20.880]   it's how we get our exact solutions for linear regression. If you've ever done the normal
[00:00:20.880 --> 00:00:26.720]   equations in a stats class to figure out what the values of the weights are for your parameters
[00:00:26.720 --> 00:00:31.200]   in linear regression, you people use vector calculus to come up with that answer. When we
[00:00:31.200 --> 00:00:35.360]   calculate eigenvectors and eigenvalues, we're using methods from vector calculus to determine
[00:00:35.360 --> 00:00:41.200]   what algorithms will work and how well they will work. But maybe a little bit less basic and more
[00:00:41.200 --> 00:00:49.040]   central to the contemporary deep learning methods, gradient descent is based off of gradients,
[00:00:49.040 --> 00:00:54.560]   which are derivatives of functions that take in vectors. Backpropagation, as an algorithm,
[00:00:54.560 --> 00:00:59.440]   it uses a whole bunch of different ideas from vector calculus. These are really core ideas
[00:00:59.440 --> 00:01:04.080]   in deep learning. But vector calculus is something, it's essentially the same thing
[00:01:04.080 --> 00:01:09.280]   as what people would call multivariable calculus in at least the American high school system.
[00:01:09.280 --> 00:01:15.440]   It's something that people have a lot of sort of like fear of or dislike for. And I think the
[00:01:15.440 --> 00:01:21.200]   reason why is that vector calculus combines two things that people are a little sketchy on,
[00:01:21.200 --> 00:01:26.560]   linear algebra and calculus. And part of the reason why people are sketchy about linear
[00:01:26.560 --> 00:01:31.360]   algebra and calculus is that people explain both of those things the wrong way. You'll hear people
[00:01:31.360 --> 00:01:36.800]   say that linear algebra is an algebra for solving equations with vectors and matrices instead of
[00:01:36.800 --> 00:01:43.360]   just numbers. And they'll say that calculus is the study of methods for mathematically
[00:01:43.360 --> 00:01:48.640]   representing rates of change and areas. And while these are definitely true statements, and they
[00:01:48.640 --> 00:01:52.640]   give deep insight to these two mathematical disciplines, they're not really the right way
[00:01:52.640 --> 00:01:58.480]   to think about either of these things when it comes to doing machine learning. The former uses
[00:01:58.480 --> 00:02:02.880]   a lot of intuition that people develop as pure mathematicians, and the latter uses a lot of
[00:02:02.880 --> 00:02:09.040]   intuition that people develop as physicists or engineers. But most people doing machine learning
[00:02:09.040 --> 00:02:15.280]   aren't either of those things. Maybe one of the common tool kits is more software engineering
[00:02:15.280 --> 00:02:20.960]   than hardware engineering, signal processing, things like that. And so there are better ways
[00:02:20.960 --> 00:02:27.920]   to understand these two general mathematical ideas that make use of concepts that are more
[00:02:27.920 --> 00:02:36.720]   close to the wheelhouses of folks who are doing software development and software engineering
[00:02:36.720 --> 00:02:42.000]   every day. So linear algebra can be thought of as the study of functions that can be represented by
[00:02:42.000 --> 00:02:48.320]   arrays. And these functions are also known as linear maps. So at the last deep learning salon,
[00:02:48.320 --> 00:02:53.280]   I talked about this approach to linear algebra. So there's a link there to the video on that.
[00:02:53.280 --> 00:03:00.160]   So the question is, is there a better way to understand calculus in the same way? And the
[00:03:00.160 --> 00:03:05.840]   answer is yes. If we think of calculus, and in particular the derivative calculus, which is what
[00:03:05.840 --> 00:03:12.480]   we mostly need for machine learning, it's really about methods for linearly approximating functions.
[00:03:12.480 --> 00:03:16.880]   So there's this deep connection between what happens in linear algebra and what happens in
[00:03:16.880 --> 00:03:22.720]   calculus that isn't really taught, especially when people teach single variable calculus,
[00:03:22.720 --> 00:03:29.520]   but sort of unifies this two sets of ideas and connects to things that we end up using all over
[00:03:29.520 --> 00:03:36.560]   the place in our machine learning and deep learning. So the way that we're going to approach
[00:03:36.560 --> 00:03:41.360]   this by actually defining the derivative in a different way, and that's going to end up making
[00:03:41.360 --> 00:03:47.840]   our vector calculus easier. So the name of this style of defining the derivative is called the
[00:03:47.840 --> 00:03:54.480]   Frechet derivative, after a French mathematician who invented it. And while it's only going to
[00:03:54.480 --> 00:04:00.640]   give us the same answers as what we learned in our calculus classes, but it's going to give us a new
[00:04:00.640 --> 00:04:06.160]   way of thinking. And there are three major benefits to thinking this way. One is that we're going to
[00:04:06.160 --> 00:04:12.880]   end up with a single sort of style, a single definition, a single method of proof and of
[00:04:12.880 --> 00:04:18.560]   understanding that works for gradients of single variable vector and even matrix functions. And
[00:04:18.560 --> 00:04:23.760]   even better, this extends all the way into the heady realms of functional calculus. It's not
[00:04:23.760 --> 00:04:29.040]   where we'll be going today, but it's cool to know that this single idea can go from the kinds of
[00:04:29.040 --> 00:04:34.720]   calculus that one can learn as a high school student up to the sort of most complicated kinds
[00:04:34.720 --> 00:04:40.720]   of calculus you can do. In many problems, and this was where I got the title for my talk,
[00:04:40.720 --> 00:04:45.760]   all the indices disappear from our calculations. We no longer have to juggle lots of I's and J's
[00:04:45.760 --> 00:04:53.440]   and K's telling us which part of which matrix or which vector belongs to which component of the
[00:04:53.760 --> 00:04:57.600]   gradient of the derivative. Everything will be done in a completely index-free manner. It's
[00:04:57.600 --> 00:05:04.960]   less cluttered. It's easier to follow. It shortens things. And for me, it's just simpler. And then,
[00:05:04.960 --> 00:05:10.480]   lastly, we're going to end up using the little o notation instead of limits, which is sort of akin
[00:05:10.480 --> 00:05:15.360]   to the big O notation from computer science. And this is something that maybe people have a bit
[00:05:15.360 --> 00:05:20.480]   better intuition about from having had to understand some of these things about how quickly
[00:05:22.000 --> 00:05:27.680]   certain algorithms run. So Hans Stock had some big O statements about how long algorithms would
[00:05:27.680 --> 00:05:33.120]   take, but none of the talks so far have included limits, if that gives you a sense for which of
[00:05:33.120 --> 00:05:40.640]   these kinds of things are more common in the toolkits and the practice of machine learning
[00:05:40.640 --> 00:05:47.040]   and software engineering. So we're going to spend most of our time talking just about how this gives
[00:05:47.040 --> 00:05:52.720]   us a way to do calculus on single variable functions. But we will, in the end, sort of
[00:05:52.720 --> 00:05:59.440]   point to how it gets used on for vector calculus. And basically, this talk is kind of a teaser for
[00:05:59.440 --> 00:06:05.440]   the sequence of blog posts where I go through how to actually use this set of techniques, line by
[00:06:05.440 --> 00:06:10.960]   line, every single manipulation, like carefully explained. And so if you want to get that,
[00:06:10.960 --> 00:06:16.000]   you'll have to check out that sequence of blog posts. I didn't want to be doing math in front
[00:06:16.000 --> 00:06:22.720]   of people for half an hour. So the links for that will be at the end. So single variable function,
[00:06:22.720 --> 00:06:28.320]   what I mean by that, when we do calculus, we're typically thinking about functions that take in
[00:06:28.320 --> 00:06:34.880]   and put out real numbers. But real numbers aren't real, at least for computers. There's no real
[00:06:34.880 --> 00:06:39.520]   numbers inside computers. So any time that I say anything about real numbers, you might substitute
[00:06:39.520 --> 00:06:44.560]   floating point numbers instead. So we're thinking about functions that take in one single floating
[00:06:44.560 --> 00:06:50.160]   point number and put out another one. So squaring, adding to, multiplying by five, these are all the
[00:06:50.160 --> 00:06:57.280]   kinds of functions that we might be thinking about. So the standard definition of a derivative
[00:06:57.280 --> 00:07:04.720]   that one learns in a calculus class is that the derivative is the limit of a certain ratio.
[00:07:04.720 --> 00:07:11.920]   So it's the ratio between the change in output and the change in input as the change in input
[00:07:11.920 --> 00:07:19.520]   gets smaller. So that limit there means make this change smaller, make the, this epsilon value go to
[00:07:19.520 --> 00:07:26.960]   zero. And we say that the limit of that ratio is the derivative. So we're specifically defining
[00:07:26.960 --> 00:07:34.880]   the derivative at a point by means of a limit. And I don't know about you, but when I took calculus
[00:07:34.880 --> 00:07:41.360]   classes, that was basically the moment for me when I like lost track of math. It took me a long time
[00:07:41.360 --> 00:07:46.880]   to sort of come back around and get back to understanding and enjoying math the way I did
[00:07:46.880 --> 00:07:54.160]   before calculus. And so I have a preference for doing things with as few limits involved as
[00:07:54.160 --> 00:08:01.200]   possible. So the Frechet definition gives you the same results, but it centers the concepts of
[00:08:01.200 --> 00:08:09.040]   approximation and linearity rather than limits and ratios. So what the Frechet definition says
[00:08:09.040 --> 00:08:16.240]   is that if the change in the value of the output at a point epsilon away is some function of x
[00:08:16.240 --> 00:08:22.880]   times epsilon plus something smaller, we call that function the derivative. So it's weird about this
[00:08:22.880 --> 00:08:28.720]   definition is that the thing we're defining isn't like alone on one side of the equation the way we
[00:08:28.720 --> 00:08:36.000]   normally do. So it's this sort of like operational definition rather than a definition just in
[00:08:36.720 --> 00:08:48.960]   the usual sense. And the second thing that's sort of different about this definition is that instead
[00:08:48.960 --> 00:08:53.600]   of the limits appearing explicitly, we've sort of hidden them away in this last little term
[00:08:53.600 --> 00:08:58.560]   highlighted in purple, the little o of epsilon. We'll talk a lot about what exactly that term
[00:08:58.560 --> 00:09:05.760]   means, but the nice thing is that very sort of loosely and intuitively, it just means something
[00:09:05.760 --> 00:09:10.960]   smaller than epsilon. And so all the gnarliness with limits, all the complexities of calculating
[00:09:10.960 --> 00:09:17.360]   those things gets sort of stuffed away inside this little o. And we do most of our work in
[00:09:17.360 --> 00:09:22.960]   calculating our gradients, our derivatives, by just doing a little bit of regular old algebra
[00:09:22.960 --> 00:09:31.760]   or linear algebra on all the rest of the stuff. So then finally, so that's the centering of
[00:09:31.760 --> 00:09:38.160]   approximation. The other thing is that what we have here is an equation for a line. So the other
[00:09:38.160 --> 00:09:45.680]   way of sort of stating that definition is that if I can always approximate the behavior of f near x
[00:09:45.680 --> 00:09:51.200]   with a line passing through f of x with slope f prime of x, we call the function that gives us
[00:09:51.200 --> 00:09:59.520]   the slope the derivative. So this, our approximation of our function f, which could be wobbling and
[00:09:59.520 --> 00:10:06.480]   wiggling all over the place. It could be sine of x or cosine of x. It could be something exotic,
[00:10:06.480 --> 00:10:12.960]   like the loss function of a neural network. But we're always going to pretend that it's just a
[00:10:12.960 --> 00:10:18.080]   line and say we're going to approximate it with a line, and we want the sort of best line to
[00:10:18.080 --> 00:10:22.800]   approximate it. And the slope of that line is what we call the derivative. So the key idea--
[00:10:22.800 --> 00:10:27.440]   the key ideas are that we're going to approximate the function and that the way we're going to do
[00:10:27.440 --> 00:10:34.400]   it is with a line. And these are basically the important ideas about derivatives for machine
[00:10:34.400 --> 00:10:39.840]   learning, especially deep learning. So say, like, I just want to update my model's parameters in
[00:10:39.840 --> 00:10:44.480]   order to make it perform better. We have all learned lots of ways to do that, but, you know,
[00:10:44.480 --> 00:10:51.200]   let's pretend a brief tabula rasa, veil of ignorance moment. How do I do that? One way is
[00:10:51.200 --> 00:10:57.360]   that I could choose my new parameters such that my best linear guess says that the performance
[00:10:57.360 --> 00:11:03.360]   will be better. So it's really hard to know what the loss is going to be at tens of thousands of
[00:11:03.360 --> 00:11:08.400]   values of the parameters. It can be expensive. Han was pointing out that evaluating the loss
[00:11:08.400 --> 00:11:12.960]   for a transformer network can pretty quickly get you out of memory errors. So what we're going to
[00:11:12.960 --> 00:11:19.680]   do is that we're going to try and come up with a best linear guess that tells us how the function
[00:11:19.680 --> 00:11:24.720]   is going to behave at a different point. And that's what we're going to use to update our
[00:11:24.720 --> 00:11:30.560]   parameters. It's not going to be perfect. In fact, we know that as we move further and further away,
[00:11:30.560 --> 00:11:35.600]   it gets worse. That's another thing that this little o of epsilon tells us. It says that the
[00:11:35.600 --> 00:11:40.560]   gap between these two things is going to get bigger at about the same rate that that value
[00:11:40.560 --> 00:11:47.680]   epsilon gets bigger. So we don't want to change it too much. That's the last piece that this -- that
[00:11:47.680 --> 00:11:52.960]   we can read off from this equation. And this leads us directly to gradient descent. Gradient descent
[00:11:52.960 --> 00:11:57.600]   says that our new parameters on the left-hand side of that assignment arrow at the bottom are
[00:11:57.600 --> 00:12:04.000]   equal to our previous parameters plus some small number times that linear approximation, the --
[00:12:04.000 --> 00:12:13.040]   evaluated at our current value of the parameters. So what we want to use our definition for is to
[00:12:13.040 --> 00:12:18.160]   figure out from knowledge of what a function is, we want to know what is that function's derivative.
[00:12:18.800 --> 00:12:23.920]   So the way we do it is we write out the left-hand side of our definition for the function we're
[00:12:23.920 --> 00:12:29.840]   interested in. So the left-hand side of our function definition was f of x plus epsilon.
[00:12:29.840 --> 00:12:34.960]   So if we consider the function x squared, we would just be writing out x plus epsilon squared.
[00:12:34.960 --> 00:12:42.400]   And x plus epsilon squared, that's a binomial, right? If I were to multiply that out, it's x
[00:12:42.400 --> 00:12:49.600]   plus epsilon times x plus epsilon. And so what we'll get out of that is we'll get an x squared
[00:12:49.600 --> 00:12:56.320]   and an epsilon squared, and then we'll get one x times epsilon and one epsilon times x, right? So
[00:12:56.320 --> 00:13:03.200]   first, outer, inner, last is how people talk about it in the American school system. So combining
[00:13:03.200 --> 00:13:09.600]   those two x times epsilons, what we see is that x plus epsilon squared is equal to x squared plus
[00:13:09.600 --> 00:13:15.600]   two x epsilon plus epsilon squared. And then the next step is that we pattern match the right-hand
[00:13:15.600 --> 00:13:20.400]   sides. So pattern matching is something that people are maybe more comfortable doing with
[00:13:20.400 --> 00:13:26.640]   from computer science, from programming. Functional programming languages elevate
[00:13:26.640 --> 00:13:32.640]   pattern matching to actually sort of the way some functions are defined. So pattern matching is a
[00:13:34.000 --> 00:13:40.960]   sort of key skill in computer science. So we pattern match our two right-hand sides. So down
[00:13:40.960 --> 00:13:49.520]   here, I've got f of x plus something that's a function of x times epsilon plus something that's
[00:13:49.520 --> 00:13:56.640]   smaller than epsilon. And those are exactly the three terms in that equation at the top, x squared
[00:13:57.280 --> 00:14:04.480]   plus two x, that's a function of x, times epsilon plus epsilon squared. And for small things,
[00:14:04.480 --> 00:14:11.360]   something squared is smaller than something. So if I'm less than one, then squaring makes me
[00:14:11.360 --> 00:14:17.760]   smaller. So epsilon squared is also something smaller than epsilon, so little o of epsilon.
[00:14:17.760 --> 00:14:26.560]   So now that tells us that the derivative of x squared is two x. So, so far, you know, so simple,
[00:14:26.560 --> 00:14:30.960]   like x squared, that's a function lots of people don't have that much trouble understanding the
[00:14:30.960 --> 00:14:37.280]   derivative of. So we'll see in a little bit that the same pattern matching technique also applies
[00:14:37.280 --> 00:14:43.520]   when it comes to vector calculus. And in that case, it's much easier to, much easier to do.
[00:14:43.520 --> 00:14:52.560]   So I want to talk a little bit about this little o term and what it means, in part because hopefully
[00:14:52.560 --> 00:14:56.640]   this will help build your intuition and understanding for big O. Every, you know,
[00:14:56.640 --> 00:15:00.560]   pretty often when I have to think about what big O really means, I have to go back to Wikipedia,
[00:15:00.560 --> 00:15:06.960]   back to a whiteboard, think about it for a while. So hopefully this will help speed that process up
[00:15:06.960 --> 00:15:14.400]   the next time you have to do that. So effectively, a term is little o of epsilon, if it gets smaller,
[00:15:14.400 --> 00:15:20.560]   much faster than epsilon does. It's like a fancy notion of what it means to be less than, right?
[00:15:22.320 --> 00:15:26.800]   One number is less than another number, pretty easy to figure out. This is a way of talking
[00:15:26.800 --> 00:15:33.680]   about how one function is in some sense less than another function. So the technical definition
[00:15:33.680 --> 00:15:39.600]   is right there in the middle of the slide. If as x gets small, the ratio of these two functions
[00:15:39.600 --> 00:15:47.840]   goes to zero, then we can replace f of x with little o of g of x, which says f of x is less
[00:15:47.840 --> 00:15:55.520]   than g of x in this sense. This means that for a small enough epsilon, anything little o of epsilon
[00:15:55.520 --> 00:16:00.880]   can be safely ignored. So this is what we mean when we say that this derivative allows us to
[00:16:00.880 --> 00:16:07.840]   approximate the original function. And so this is pretty close to the core idea of what limits were
[00:16:07.840 --> 00:16:13.040]   used for in the definition of the derivative, but now we've sort of separated it out from the part
[00:16:13.040 --> 00:16:17.520]   where we calculate the derivative and put this on just the part where we figure out how bad our
[00:16:17.520 --> 00:16:22.720]   approximation is. And that's a really nice sort of separation of concerns. It's a way of abstracting
[00:16:22.720 --> 00:16:30.880]   away how bad our approximation is. So here's a couple of examples. Zero is going to be smaller
[00:16:30.880 --> 00:16:38.080]   than x, x squared is smaller than x, and any number times x squared is also smaller than x.
[00:16:38.080 --> 00:16:42.640]   So what that means is anytime we see any of these terms, anything that looks like this
[00:16:43.280 --> 00:16:50.000]   in an equation that we're writing, we can replace it with little o of x and continue.
[00:16:50.000 --> 00:16:56.800]   So some terms that are not o of x, one, that's something that doesn't get smaller at all.
[00:16:56.800 --> 00:17:03.920]   And so it doesn't get smaller faster than x gets smaller. x does not get smaller faster than x
[00:17:03.920 --> 00:17:08.800]   because they're the same. So the ratio of those two things is always one. And so that's a term
[00:17:08.800 --> 00:17:15.440]   that's not little o of x. If I multiply x by something, the ratio of kx and x is going to be k.
[00:17:15.440 --> 00:17:21.520]   That's also not zero. And so that term is not o of x. So there's lots of rules for manipulating
[00:17:21.520 --> 00:17:27.760]   these terms. And I find them a lot easier to remember and to use than the whole cavalcade
[00:17:27.760 --> 00:17:33.520]   of complicated rules for calculating limits. So we sort of pack away all of those rules into
[00:17:34.480 --> 00:17:41.680]   sort of like calculation strategy for working with little o terms. And it pays a lot of dividends.
[00:17:41.680 --> 00:17:47.920]   It makes a lot of derivative calculations much easier. So it's similar in spirit to big O.
[00:17:47.920 --> 00:17:52.480]   Right. So big O notation from computer science lets us talk about how fast algorithms are
[00:17:52.480 --> 00:17:57.920]   while abstracting away any irrelevant details. So for example, maybe we've got n data values,
[00:17:57.920 --> 00:18:03.280]   and we want to check if a query is one of them. If the data is in a list, searching it will take
[00:18:03.280 --> 00:18:08.960]   time proportional to the length. On the other hand, if the data is in a binary tree, famously,
[00:18:08.960 --> 00:18:16.000]   searching it will take time proportional to the logarithm n in order to find the data.
[00:18:16.000 --> 00:18:21.440]   So if what we want to do with our data is search it, a binary tree is better because O log n
[00:18:21.440 --> 00:18:26.560]   is faster than O n. Now there's lots and lots of important details when it comes to actually
[00:18:26.560 --> 00:18:32.000]   implementing these algorithms to figuring out exactly how fast each one is. But just generally,
[00:18:32.000 --> 00:18:37.760]   if we're working with a really big number, we know that the thing that is O log n is almost
[00:18:37.760 --> 00:18:42.880]   always going to be smaller than the thing that is O n. So it saves us a lot of effort and work and
[00:18:42.880 --> 00:18:48.080]   cognitive load to use this big O notation to think about things rather than trying to calculate the
[00:18:48.080 --> 00:18:56.000]   exact speeds and the exact number of operations that something takes. So it lets us abstract
[00:18:56.000 --> 00:19:02.720]   away those irrelevant details and talk about which algorithm is fastest, avoiding really
[00:19:02.720 --> 00:19:08.640]   specifying them. Similarly, little o lets us talk about the best linear approximation while
[00:19:08.640 --> 00:19:13.360]   abstracting away exactly how good or bad that approximation is. Because that's often a lot
[00:19:13.360 --> 00:19:19.040]   harder to really understand. I can say that this thing is best without knowing how much better it
[00:19:19.040 --> 00:19:27.520]   is than other choices or how much worse it is than the best possible choice. So if you want to read
[00:19:27.520 --> 00:19:34.480]   more about these, the sort of general term for all these ways of turning limits into these simpler
[00:19:34.480 --> 00:19:40.080]   expressions, they're called Landau symbols. So I would definitely check that out. There's lots of
[00:19:40.080 --> 00:19:48.960]   good tutorial material online about them. Okay. So now that we understand how this Frechet style
[00:19:48.960 --> 00:19:57.120]   of derivative lets us do the kinds of derivatives one does in an A, B, or B, C sort of intro calculus
[00:19:57.120 --> 00:20:03.360]   class, we now apply them to functions of vectors. And we can define the gradient in exactly the same
[00:20:03.360 --> 00:20:10.160]   way. So when I say that something is a function of a vector, for folks who do pure math, I'm
[00:20:10.160 --> 00:20:16.720]   talking about something that takes in an element of Rn, a real value vector, and returns a real
[00:20:16.720 --> 00:20:21.680]   value. And then the gradient is a function that takes in a vector and spits back out a vector.
[00:20:21.680 --> 00:20:26.960]   But for folks who are doing machine learning, what I'm thinking of is a function that takes
[00:20:26.960 --> 00:20:32.560]   in an array of floats and returns a single float. And then the gradient is something that takes in
[00:20:32.560 --> 00:20:36.000]   an array of floats and returns an array of floats of the same shape.
[00:20:36.000 --> 00:20:45.520]   So, again, the Frechet definition is going to center approximation and linearity. And what it
[00:20:45.520 --> 00:20:51.360]   says is that if the change in the value of the output at a point epsilon away is now the inner
[00:20:51.360 --> 00:20:56.720]   product of some function of x with epsilon plus something smaller, we call that function the
[00:20:56.720 --> 00:21:00.960]   gradient. We call it the vector derivative or just the derivative, but people like to call it the
[00:21:00.960 --> 00:21:09.440]   gradient, especially in machine learning. So the only real difference between this definition and
[00:21:09.440 --> 00:21:17.120]   the first one is that instead of taking the slope or the value of the derivative and multiplying it
[00:21:17.120 --> 00:21:22.800]   by epsilon, we're taking this inner product or dot product or scalar product, depending on your
[00:21:22.800 --> 00:21:30.800]   discipline, between those two things. So this inner product can be written many, many ways. I
[00:21:30.800 --> 00:21:34.400]   think the most important alternative way to write the linear product is in terms of matrix
[00:21:34.400 --> 00:21:39.840]   multiplication. So when we write it that way, instead of writing those two angled brackets,
[00:21:39.840 --> 00:21:45.760]   we write vector transpose other vectors. So the gradient is a vector, epsilon is a vector,
[00:21:45.760 --> 00:21:52.000]   and the way we multiply one vector with another is by taking the transpose of one and then doing
[00:21:52.000 --> 00:21:59.520]   matrix multiplication between the two of them. So the important things to note are that this
[00:21:59.520 --> 00:22:04.960]   reduces to the derivative for vectors of length one. So this inner product here, this vector
[00:22:04.960 --> 00:22:10.720]   transpose vector, is the sum of the products of the elements of the vectors. Go into each,
[00:22:10.720 --> 00:22:15.600]   do a little for loop over each vector, the vectors zip together and multiply those values.
[00:22:15.600 --> 00:22:20.400]   So if there's only one thing, then it's just multiply the two values together. And that's
[00:22:20.400 --> 00:22:26.000]   exactly what we did when we did the scalar value derivative. So this is basically the same
[00:22:26.000 --> 00:22:30.800]   definition, but now we've gone up one level and defined it in a way that works for both vectors
[00:22:30.800 --> 00:22:38.160]   and scalars. And then, sorry, I think I didn't mention the scalar is the term people use in
[00:22:38.160 --> 00:22:47.200]   linear algebra for just a single number. The second term in this guy, the gradient transpose
[00:22:47.200 --> 00:22:54.080]   epsilon, that's a linear function of epsilon for a fixed x. So what I'm doing is I'm just multiplying
[00:22:54.080 --> 00:22:59.120]   these things and adding them together. That function is always going to be linear. It defines
[00:22:59.120 --> 00:23:07.120]   effectively a plane or a hyperplane, which is the higher dimensional equivalent of a line.
[00:23:07.120 --> 00:23:11.760]   So this is just like how we had the notion of the derivative as a line in one dimensions.
[00:23:11.760 --> 00:23:17.040]   We have the notion of the derivative as like a plane or a hyperplane, a linear function that
[00:23:17.040 --> 00:23:26.000]   approximates the original function. So here's how we end up using our definition. It looks exactly
[00:23:26.000 --> 00:23:34.160]   the same way that we used our definition for the scalar derivative. So if we want to know what the
[00:23:34.160 --> 00:23:39.120]   derivative of the L2 norm, the squared norm, people use this for regularization in machine
[00:23:39.120 --> 00:23:45.040]   learning, gradients get calculated on it. Maybe you wondered what that gradient was. What we do
[00:23:45.040 --> 00:23:54.400]   is we multiply it out. The squared norm looks just like the x plus epsilon squared that we had
[00:23:54.400 --> 00:23:59.520]   before. We get one term that's the squared norm of x, another term that's the squared norm of epsilon,
[00:23:59.520 --> 00:24:05.600]   and then a term in the middle that's basically two times x times epsilon. So when we do things
[00:24:05.600 --> 00:24:12.720]   this way, the connections between scalar calculus, vector calculus, and calculus with matrices become
[00:24:12.720 --> 00:24:21.360]   really, really clear because we're using the same symbols and same tools for each one of these
[00:24:21.360 --> 00:24:30.800]   subcategories, subdisciplines of calculus. And again, we just pattern match the right-hand sides.
[00:24:30.800 --> 00:24:37.120]   So we look to make sure that there's one term that looks like the original function, blue.
[00:24:37.120 --> 00:24:45.120]   There's one term that looks smaller than epsilon. So that's smaller than the norm of epsilon. So
[00:24:45.120 --> 00:24:51.520]   norm of epsilon squared, again, squaring makes things smaller. So that guy in purple is our
[00:24:51.520 --> 00:25:00.320]   little o of norm epsilon term. And then the middle term, that's the term that gives us our gradient.
[00:25:00.320 --> 00:25:08.080]   So it says that the gradient of the squared norm is 2x. Notice that there's a transpose built into
[00:25:08.080 --> 00:25:14.000]   that middle term, so we drop the transpose when we decide what the vector is. It's really easy to see
[00:25:14.000 --> 00:25:18.880]   with vectors because the gradient needs to be the same shape as the inputs or else we couldn't add
[00:25:18.880 --> 00:25:22.800]   them together when doing gradient descent. But with matrices, that's an important one that you're
[00:25:22.800 --> 00:25:29.040]   going to need to watch out for. The other little gotcha is that people often like to just write
[00:25:29.040 --> 00:25:37.920]   little o of epsilon without the two straight lines around it that indicate the norm, just as a
[00:25:37.920 --> 00:25:44.720]   notational convenience when writing little o stuff. So when you go out there and look for other people
[00:25:44.720 --> 00:25:48.160]   using the Frechet derivative style, you might notice that they do that a little bit differently
[00:25:48.160 --> 00:25:56.960]   than what I do here. So lastly, we can use this to do derivatives of functions of matrices. And
[00:25:56.960 --> 00:26:01.200]   the definition is exactly the same. And this is, I think, the real killer app for the Frechet
[00:26:01.200 --> 00:26:06.400]   derivative. I've found that every other approach just falls apart, gets much more complicated,
[00:26:06.400 --> 00:26:10.960]   really difficult to follow when it comes to functions of matrices. And I really suffered
[00:26:10.960 --> 00:26:17.520]   through trying to calculate derivatives for things like linear regression, when these should be
[00:26:17.520 --> 00:26:21.200]   things that we can do pretty quickly and easily. And the Frechet derivative lets us do that.
[00:26:22.320 --> 00:26:27.840]   So it's literally the exact same definition, but now with capital letters, because people like to
[00:26:27.840 --> 00:26:33.760]   use capital letters when they're working with matrices. So again, we look at the value at a
[00:26:33.760 --> 00:26:38.800]   point epsilon away. Capital epsilon looks like an e. Sorry, I didn't come up with the Greek language.
[00:26:38.800 --> 00:26:46.480]   So we look at the value at a point epsilon away. It's now the inner product of some function of x
[00:26:46.480 --> 00:26:53.520]   with e plus something smaller. And again, we call that function the gradient. So the only thing
[00:26:53.520 --> 00:26:58.000]   that's different here is we now need to know how to do an inner product and calculate a norm of a
[00:26:58.000 --> 00:27:03.280]   matrix, which maybe people are a little bit less familiar with than they are with inner products
[00:27:03.280 --> 00:27:08.320]   of matrices. Or sorry, of vectors, right? The inner product of vectors already showed up actually in
[00:27:08.320 --> 00:27:14.320]   Han's talk, right, with that cosine similarity. The cosine similarity is just the normalized
[00:27:14.320 --> 00:27:23.040]   version of the dot product. So what is the inner product of two matrices? What is the norm of a
[00:27:23.040 --> 00:27:29.040]   matrix? So there's this particular definition in terms of traces that you can find if you look
[00:27:29.040 --> 00:27:35.280]   online. But the way to get that definition that I really like is that actually the inner product
[00:27:35.280 --> 00:27:41.120]   of two matrices is just turn each matrix into a vector and then calculate that inner product.
[00:27:41.120 --> 00:27:48.480]   So that's this last line here. It says vectorize x. That's literally like flatten, right, in NumPy
[00:27:48.480 --> 00:27:54.800]   or in TensorFlow or whatever your favorite tensor library is. Just flatten them out and then do the
[00:27:54.800 --> 00:27:59.600]   dot product between those two. It blows my mind that this actually works and gives you a really
[00:27:59.600 --> 00:28:04.800]   good answer for what it means to do the inner product of two matrices. But that's how it's done.
[00:28:04.800 --> 00:28:09.440]   It turns out that the way that's easier to work with when we're doing our algebra,
[00:28:09.440 --> 00:28:14.240]   when we're calculating stuff with the Frechet derivative, is this thing in terms of the trace.
[00:28:14.240 --> 00:28:20.640]   If you want more details about how we calculate matrix-valued derivatives and how we understand
[00:28:20.640 --> 00:28:27.680]   what this sort of maybe arcane and strange trace-based expression for the inner product
[00:28:27.680 --> 00:28:32.800]   is, check out the blog post that is linked in the corner of this slide and should show up in the
[00:28:32.800 --> 00:28:39.120]   chat about sort of why this is such a sensible way to calculate the lengths of matrices. It turns out
[00:28:39.120 --> 00:28:47.680]   to be really closely related to things like PCA. So I don't have an example of calculating a
[00:28:47.680 --> 00:28:52.960]   matrix-valued derivative because, you know, maybe you should always leave people wanting a little
[00:28:52.960 --> 00:28:57.360]   bit more. So if you want to see how to do it and get some really cool examples, you'll need to
[00:28:57.360 --> 00:29:03.760]   check out the series of blog posts. But hopefully I've convinced you that these benefits are there
[00:29:03.760 --> 00:29:08.080]   and are real for thinking of vector and matrix calculus in terms of the Frechet derivative.
[00:29:08.080 --> 00:29:15.200]   That we can write one definition, one single sort of style of doing calculations that works for
[00:29:15.200 --> 00:29:21.760]   gradients of all kinds of functions. That you'll notice that basically indices disappeared. I used
[00:29:21.760 --> 00:29:25.280]   a few in calculating those inner products, but that was just to define those things.
[00:29:25.280 --> 00:29:32.800]   No need to use any indices at all. And we kept the limits to a minimum by hiding them essentially
[00:29:32.800 --> 00:29:40.080]   inside our little-o notation. So that's - those are I think the sort of three big benefits of
[00:29:40.080 --> 00:29:47.680]   using this Frechet derivative style. So the blog post series is linked at the top there. So there's
[00:29:47.680 --> 00:29:52.720]   sort of three plus one blog posts. I had initially intended to write just this linear regression one
[00:29:52.720 --> 00:29:56.800]   and one on deep linear neural networks, which are really near and dear to my heart. Neural networks
[00:29:56.800 --> 00:30:01.840]   with no non-linearities. They're nice because you can do math on them even though they can't like,
[00:30:01.840 --> 00:30:05.920]   you know, solve image net or anything like that. But then while I was writing this blog post,
[00:30:05.920 --> 00:30:11.760]   literally in the middle of it, I came across another example just coincidentally, where
[00:30:11.760 --> 00:30:18.480]   calculating the derivative of the determinant, it turns out to be much, much easier in this way.
[00:30:18.480 --> 00:30:23.600]   And it's something that you do sometimes have to calculate. I have another blog post on Gaussians
[00:30:23.600 --> 00:30:27.600]   as exponential families. This is an idea that, you know, was bigger in vogue when people liked
[00:30:27.600 --> 00:30:33.920]   graphical models and things like that. But there's Gaussians, the normal distribution,
[00:30:33.920 --> 00:30:40.400]   multivariate normals, have this really, really elegant connection to information geometry and
[00:30:40.400 --> 00:30:45.440]   these very cool, heady ideas. But the way that this connection is often presented is really hard
[00:30:45.440 --> 00:30:50.160]   to follow because there's a lot of vector calculus. So I went ahead and calculated these things in the
[00:30:50.160 --> 00:30:54.560]   Frechet style because I couldn't find anybody else doing it that way and found that it just
[00:30:54.560 --> 00:31:01.040]   compressed things so much shorter, so much easier to write down. And it really emphasized what was
[00:31:01.040 --> 00:31:06.400]   important and unique in that case and not all the machinery and mechanics of calculating these
[00:31:06.400 --> 00:31:12.240]   gradients. And also a shout out to Terry Tao's blog. Terry Tao's one of, you know, the greatest
[00:31:12.240 --> 00:31:17.360]   living mathematicians. And if you need another endorsement for using the Frechet derivative,
[00:31:17.360 --> 00:31:21.920]   that's the style he does in almost all of his calculations of derivatives and gradients and
[00:31:21.920 --> 00:31:29.040]   things. So that's where I first came across this style. And it's me doing calculus for neural
[00:31:29.040 --> 00:31:33.840]   networks, for linear algebra, for lots of things that I'm interested in, just so much easier. And
[00:31:33.840 --> 00:31:39.840]   it's really deepened my understanding of both linear algebra and calculus. So that's all I have.
[00:31:39.840 --> 00:31:44.240]   Hopefully there are questions. But if not, if you can't think of any questions now, feel free to hit
[00:31:44.240 --> 00:31:55.600]   me up via email or on Twitter. Thanks a lot. >> Thank you, Charles. That made me hate calculus
[00:31:55.600 --> 00:32:00.960]   a lot less. Or a little bit less, if I'm being honest. So thank you for that. We have a few
[00:32:00.960 --> 00:32:08.160]   questions. So Rishav really liked your talk. He said beautiful talk. He was wondering if you have
[00:32:08.160 --> 00:32:14.080]   any other recommendations to go deeper into these topics in the way that you are teaching. And
[00:32:14.080 --> 00:32:20.000]   I posted all of the links from your slides in the chat already. Do you have any more?
[00:32:20.000 --> 00:32:29.920]   >> Yeah. So I think the -- there are links in those blog posts to other places where I had
[00:32:29.920 --> 00:32:38.880]   come across these ideas. I don't have them immediately on the top of my head. So I guess,
[00:32:38.880 --> 00:32:44.240]   yeah, Rishav, if you could just send me your email, I'll make sure to contact you specifically
[00:32:44.240 --> 00:32:50.240]   with some additional resources once I have a chance to look at it. >> Cool. And then someone
[00:32:50.240 --> 00:32:57.760]   asked, are lambdas the same as functional calculus? >> Lambdas the same as functional calculus? So
[00:32:57.760 --> 00:33:04.800]   you're talking -- I'm guessing that this is about the lambda calculus for -- as a model of
[00:33:04.800 --> 00:33:11.680]   computation. So if you wanted to do calculus on lambda expressions, you would need to do
[00:33:11.680 --> 00:33:15.840]   functional calculus. But lambda calculus and functional calculus are different things.
[00:33:15.840 --> 00:33:23.920]   >> And then the last question, someone asked, when is Frechet's method applied in practice?
[00:33:23.920 --> 00:33:28.720]   Does it give the exact same results as the traditional methods? And how can you verify
[00:33:28.720 --> 00:33:35.200]   that? >> Yeah. So it gives the -- the results are exactly the same for anything except functional
[00:33:35.200 --> 00:33:39.760]   calculus, basically. Once you get to functional calculus, it turns out that you need a little
[00:33:39.760 --> 00:33:44.320]   bit of extra machinery, and there's the Frechet derivative and the Gatow derivative are the two
[00:33:44.320 --> 00:33:55.760]   ways people do it, and those give slightly different answers. So the -- so you could prove
[00:33:55.760 --> 00:33:59.920]   that they give you the same answer, so that would be something you'd have to go about doing.
[00:33:59.920 --> 00:34:07.520]   And I would say it's used -- I have found that a lot of people who are doing stuff like developing
[00:34:07.520 --> 00:34:13.520]   automatic differentiation libraries use an approach much closer to this one than the
[00:34:13.520 --> 00:34:20.480]   like traditional calculus approach. So one person would be Michael Betancourt, who does Stan,
[00:34:21.680 --> 00:34:26.960]   which is a Bayesian inference library that also has automatic differentiation, just like tensor
[00:34:26.960 --> 00:34:34.720]   flow, and his style is a little bit different than this one, but it also is -- centers approximation
[00:34:34.720 --> 00:34:40.000]   linearity and things like that. So I don't know if you would explicitly say he does the Frechet
[00:34:40.000 --> 00:34:47.360]   derivative, but it's the same -- same set of ideas. >> Cool. >> I saw actually a question about --
[00:34:47.360 --> 00:34:52.880]   from Bodian asking, shouldn't it be little o of epsilon squared? And this is actually an
[00:34:52.880 --> 00:34:57.680]   interesting point. So I didn't get a chance to mention this, but little o is kind of like a
[00:34:57.680 --> 00:35:03.600]   less than sign, and people are usually used to working with big O, which is like a less than
[00:35:03.600 --> 00:35:09.840]   or equal to sign. And so with little o, the powers are a little bit different than they are with big
[00:35:09.840 --> 00:35:13.840]   O. So if you were -- if you were surprised that it wasn't an epsilon squared, because you're used
[00:35:13.840 --> 00:35:19.360]   to that way of writing Taylor expansions, then that's the reason why, because there is this tiny
[00:35:19.360 --> 00:35:24.800]   difference. And I just slightly prefer it, but you can do the whole thing with big O instead.

