
[00:00:00.000 --> 00:00:03.360]   [MUSIC PLAYING]
[00:00:03.360 --> 00:00:10.320]   Welcome, everyone, to Navigating Vision Systems
[00:00:10.320 --> 00:00:13.120]   with Weights and Biases and SmartCal.
[00:00:13.120 --> 00:00:14.840]   Today, we will see a presentation
[00:00:14.840 --> 00:00:16.420]   from Sumik, who's a machine learning
[00:00:16.420 --> 00:00:18.200]   engineer at Weights and Biases, and Anton,
[00:00:18.200 --> 00:00:21.800]   who's a senior software engineer, ML, and solutions
[00:00:21.800 --> 00:00:23.440]   at SmartCal.
[00:00:23.440 --> 00:00:25.800]   Today, we will start with Sumik's presentation
[00:00:25.800 --> 00:00:28.840]   on the role of ultralytics in elevating computer vision
[00:00:28.840 --> 00:00:31.920]   tasks and a big picture of the weights and biases ecosystem
[00:00:31.920 --> 00:00:33.680]   for computer vision.
[00:00:33.680 --> 00:00:35.040]   Right after Sumik's presentation,
[00:00:35.040 --> 00:00:37.400]   we will have a Q&A session.
[00:00:37.400 --> 00:00:41.640]   So please don't hesitate to post your questions in the chat.
[00:00:41.640 --> 00:00:44.320]   Thanks, everyone, and thanks, Sumik, for joining us.
[00:00:44.320 --> 00:00:47.360]   And you can start.
[00:00:47.360 --> 00:00:48.240]   Hello, everyone.
[00:00:48.240 --> 00:00:49.880]   So today, I'm going to be speaking
[00:00:49.880 --> 00:00:52.760]   on how you can supercharge your computer vision
[00:00:52.760 --> 00:00:57.280]   workflows using ultralytics and weights and biases.
[00:00:57.280 --> 00:01:01.120]   So if you want to follow along the presentation,
[00:01:01.120 --> 00:01:05.240]   you can do so at this particular link, which
[00:01:05.240 --> 00:01:06.440]   is present on screen.
[00:01:06.440 --> 00:01:09.040]   And you can also scan this particular QR code
[00:01:09.040 --> 00:01:11.760]   to get access to the presentation.
[00:01:11.760 --> 00:01:14.840]   So I'll give you a moment to scan the presentation
[00:01:14.840 --> 00:01:18.600]   or jot down the link.
[00:01:18.600 --> 00:01:19.840]   Yes, it's the same thing.
[00:01:19.840 --> 00:01:26.080]   And before we get started, a little bit about myself.
[00:01:26.080 --> 00:01:29.000]   So as Agata mentioned, I'm a machine learning engineer
[00:01:29.000 --> 00:01:30.800]   at Weights and Biases.
[00:01:30.800 --> 00:01:35.320]   I'm also a Google Developer Expert for JAX.
[00:01:35.320 --> 00:01:38.920]   I have formerly served as a software engineer at IBM
[00:01:38.920 --> 00:01:43.280]   and a staff machine learning engineer at Igniterium.
[00:01:43.280 --> 00:01:46.240]   I often build a lot of MLF pipelines
[00:01:46.240 --> 00:01:50.960]   for open source repositories, including Keras, Ultralytics,
[00:01:50.960 --> 00:01:53.720]   Hugging Phase Diffusers, and a lot more.
[00:01:53.720 --> 00:01:58.240]   I am an active contributor to Weights and Biases add-ons,
[00:01:58.240 --> 00:02:01.920]   which host all the management of your model lifecycle.
[00:02:01.920 --> 00:02:06.000]   So Weights and Biases offerings span almost the entirety
[00:02:06.000 --> 00:02:08.080]   of the MLF's lifecycle.
[00:02:08.080 --> 00:02:12.360]   It is integrated with any of the frameworks
[00:02:12.360 --> 00:02:14.400]   that you might be wanting to use,
[00:02:14.400 --> 00:02:17.840]   and also with the training inference or workflow
[00:02:17.840 --> 00:02:20.960]   environment that you might want to use.
[00:02:20.960 --> 00:02:25.280]   And we are also trusted by the leading machine learning teams
[00:02:25.280 --> 00:02:28.240]   all across the world, including OpenAI, Meta, NVIDIA,
[00:02:28.240 --> 00:02:28.880]   and many more.
[00:02:28.880 --> 00:02:33.920]   Well, Ultralytics, at this point,
[00:02:33.920 --> 00:02:37.160]   is the best way to define Ultralytics
[00:02:37.160 --> 00:02:41.080]   is the home of state-of-the-art computer vision models.
[00:02:41.080 --> 00:02:47.440]   And right from YOLO models, right from V3 to V8,
[00:02:47.440 --> 00:02:52.480]   the latest ones being YOLO-NAS, which are also integrated.
[00:02:52.480 --> 00:02:56.480]   Segmented Athing model, or SAM, RTD, PR,
[00:02:56.480 --> 00:02:58.240]   and a lot of other models are currently
[00:02:58.240 --> 00:02:59.600]   available on Ultralytics.
[00:02:59.600 --> 00:03:01.800]   And it's a low-code platform, which
[00:03:01.800 --> 00:03:04.840]   makes it very easy for us to use these models,
[00:03:04.840 --> 00:03:07.800]   to fine-tune these models, to deploy these models
[00:03:07.800 --> 00:03:11.120]   in production, and et cetera.
[00:03:11.120 --> 00:03:14.720]   So a simple workflow using Ultralytics
[00:03:14.720 --> 00:03:17.280]   might look something like the following.
[00:03:17.280 --> 00:03:21.960]   So you initialize the YOLO checkpoint
[00:03:21.960 --> 00:03:24.360]   that you want to be using.
[00:03:24.360 --> 00:03:28.920]   You pass all your training data, your metadata, your epochs,
[00:03:28.920 --> 00:03:31.360]   your image size, your batch size, everything
[00:03:31.360 --> 00:03:34.080]   in model.train if you want to fine-tune that model
[00:03:34.080 --> 00:03:36.240]   or perform transfer learning.
[00:03:36.240 --> 00:03:40.400]   If you want to validate it on your validation data set
[00:03:40.400 --> 00:03:44.120]   or your holdout data set, you just run model.validate.
[00:03:44.120 --> 00:03:47.160]   And if you want to perform inference,
[00:03:47.160 --> 00:03:52.080]   you just pass your images, your video feed, your camera feed,
[00:03:52.080 --> 00:03:57.120]   anything into the model, and it does the inference for you.
[00:03:57.120 --> 00:04:00.520]   So yeah, it's just five lines of code
[00:04:00.520 --> 00:04:03.840]   for the entire computer vision workflow we can think of.
[00:04:03.840 --> 00:04:08.800]   So yeah, Ultralytics is kind of a superpower in that sense.
[00:04:08.800 --> 00:04:16.000]   But we would still like to supercharge our superpower
[00:04:16.000 --> 00:04:20.720]   by adding weights and biases in Ultralytics workflow.
[00:04:20.720 --> 00:04:24.640]   So the weights and biases callback
[00:04:24.640 --> 00:04:27.400]   or the weights and biases integration for Ultralytics,
[00:04:27.400 --> 00:04:28.240]   it's very simple.
[00:04:28.240 --> 00:04:33.200]   It doesn't change your Ultralytics code much at all.
[00:04:33.200 --> 00:04:36.480]   All you need to do is just import that function
[00:04:36.480 --> 00:04:40.800]   from oneb.integration.Ultralytics import add oneb callback
[00:04:40.800 --> 00:04:44.160]   and just call the function before you train your model,
[00:04:44.160 --> 00:04:45.640]   before you validate your model,
[00:04:45.640 --> 00:04:48.400]   or before you perform inference with your model.
[00:04:48.400 --> 00:04:51.720]   So the weights and biases callback for Ultralytics
[00:04:51.720 --> 00:04:53.680]   is very useful.
[00:04:53.680 --> 00:04:57.280]   It does a lot of things other than tracking your experiments.
[00:04:57.280 --> 00:05:00.680]   So it helps make your experiments reproducible
[00:05:00.680 --> 00:05:05.360]   by managing all the matrix, the configs and everything.
[00:05:05.360 --> 00:05:10.360]   It enables you to visualize how your model is learning
[00:05:10.360 --> 00:05:14.400]   on an epoch by epoch basis, basically.
[00:05:14.400 --> 00:05:17.440]   And it also synchronizes and versions
[00:05:17.440 --> 00:05:22.000]   your model checkpoints using model registry and artifacts.
[00:05:22.000 --> 00:05:28.200]   So we will see some code in action
[00:05:28.200 --> 00:05:30.960]   to determine how useful this might be.
[00:05:30.960 --> 00:05:35.960]   So I would request you to click on this link
[00:05:35.960 --> 00:05:38.800]   or scan this QR code to get access to this code.
[00:05:38.800 --> 00:05:42.000]   This is the code that I'll be showcasing right now.
[00:05:42.000 --> 00:05:45.640]   So let me just transition
[00:05:45.640 --> 00:05:47.800]   into this particular Google Colab notebook.
[00:05:47.800 --> 00:05:51.000]   So all we will be doing in this notebook
[00:05:51.000 --> 00:05:55.160]   is seeing some Ultralytics and weights and biases action
[00:05:55.160 --> 00:05:58.680]   hands-on and what we will be doing
[00:05:58.680 --> 00:06:03.680]   is we will be training a very simple YOLOv8N
[00:06:03.680 --> 00:06:06.120]   or N stands for nano here.
[00:06:06.120 --> 00:06:10.000]   So we will be fine tuning YOLOv8N pre-trained checkpoint
[00:06:10.000 --> 00:06:15.000]   for a very specific dataset, which is the COCO128 dataset.
[00:06:15.000 --> 00:06:17.920]   So it's a subset, it's a very small subset
[00:06:17.920 --> 00:06:19.960]   of the MSCOCO dataset,
[00:06:19.960 --> 00:06:22.520]   which enables us to experiment rapidly.
[00:06:22.520 --> 00:06:27.200]   And if you take a look at the code,
[00:06:27.200 --> 00:06:28.760]   if I start from the beginning,
[00:06:28.760 --> 00:06:31.280]   so we are installing Ultralytics
[00:06:31.280 --> 00:06:33.680]   and weights and biases basically.
[00:06:33.680 --> 00:06:38.400]   And as usual, we are just importing weights and biases
[00:06:38.400 --> 00:06:40.680]   and the add one we call back,
[00:06:40.680 --> 00:06:42.360]   which is basically the weights and biases integration
[00:06:42.360 --> 00:06:43.240]   for Ultralytics.
[00:06:43.240 --> 00:06:47.560]   And what we are doing is we have said the,
[00:06:47.560 --> 00:06:50.400]   you can just, this is a Colab form using which you can,
[00:06:50.400 --> 00:06:52.400]   you know, just change the model checkpoint
[00:06:52.400 --> 00:06:55.800]   to something else, maybe YOLOv8X for extra large
[00:06:55.800 --> 00:06:56.640]   or something like that.
[00:06:56.640 --> 00:06:58.320]   We will go with nano for this one.
[00:06:58.320 --> 00:07:04.400]   And yes, so Ultralytics also supports a couple of datasets
[00:07:04.400 --> 00:07:08.160]   like COCO128, which you can use out of the box
[00:07:08.160 --> 00:07:10.160]   by just mentioning the dataset.
[00:07:10.160 --> 00:07:14.160]   So yeah, so you can just change that here
[00:07:14.160 --> 00:07:16.080]   and it would automatically download the dataset
[00:07:16.080 --> 00:07:18.400]   and start training the model.
[00:07:18.400 --> 00:07:23.120]   I won't be running this Colab right now,
[00:07:23.120 --> 00:07:25.360]   but it takes a very less amount of time
[00:07:25.360 --> 00:07:28.200]   and you can run it with a free tier Google Colab notebook.
[00:07:28.200 --> 00:07:33.200]   So I highly encourage you to run this at your free time.
[00:07:33.200 --> 00:07:37.840]   But if you run this, you will, you're, you know,
[00:07:37.840 --> 00:07:39.680]   the integration is actually gonna create
[00:07:39.680 --> 00:07:41.840]   a weights and biases run, which is basically
[00:07:41.840 --> 00:07:45.400]   an analogous for your machine learning experiment
[00:07:45.400 --> 00:07:47.920]   in terms of weights and biases lingo.
[00:07:47.920 --> 00:07:51.360]   So what a weights and biases run is,
[00:07:51.360 --> 00:07:53.480]   it kind of looks something like this.
[00:07:54.400 --> 00:07:58.520]   And yes, if you see carefully,
[00:07:58.520 --> 00:08:01.400]   if I, you know, move this for a little bit,
[00:08:01.400 --> 00:08:04.640]   you know, the integration actually tracks everything,
[00:08:04.640 --> 00:08:06.680]   right from how the learning rate scheduler
[00:08:06.680 --> 00:08:08.720]   is behaving during your training
[00:08:08.720 --> 00:08:13.080]   to all the media logs that Ultralytics itself produces
[00:08:13.080 --> 00:08:17.760]   for the validation metrics that Ultralytics creates
[00:08:17.760 --> 00:08:19.040]   over the epochs.
[00:08:19.040 --> 00:08:21.960]   In this case, I have just trained for two epochs.
[00:08:21.960 --> 00:08:24.360]   The training metric, the training losses,
[00:08:24.360 --> 00:08:29.360]   the validation losses, you know,
[00:08:29.360 --> 00:08:32.480]   some parameters about our model, for example,
[00:08:32.480 --> 00:08:35.240]   it's calculating the number of model parameters.
[00:08:35.240 --> 00:08:39.720]   And in this case, it shows the speed of inference
[00:08:39.720 --> 00:08:41.160]   of this particular model.
[00:08:41.160 --> 00:08:44.640]   So a lot of things is tracked and even the system metrics.
[00:08:44.640 --> 00:08:47.560]   So your GPU utilization, your CPU utilization,
[00:08:47.560 --> 00:08:50.960]   how much power your GPU is drawing.
[00:08:50.960 --> 00:08:54.040]   So a lot of things similarly is tracked also,
[00:08:54.040 --> 00:08:56.640]   automatically out of the box.
[00:08:56.640 --> 00:09:00.200]   And I wanna draw your attention to, you know,
[00:09:00.200 --> 00:09:01.960]   one of the tables here.
[00:09:01.960 --> 00:09:06.080]   So these are, you know, interactive tables,
[00:09:06.080 --> 00:09:07.720]   which are automatically created
[00:09:07.720 --> 00:09:10.080]   by the Ultralytics integration.
[00:09:10.080 --> 00:09:12.920]   And what it does is, if you see carefully,
[00:09:12.920 --> 00:09:16.320]   it basically takes a validation batch
[00:09:16.320 --> 00:09:20.160]   and, you know, plots the validation,
[00:09:20.160 --> 00:09:22.160]   plots the predicted bounding box,
[00:09:22.160 --> 00:09:24.560]   or in case you're using something other
[00:09:24.560 --> 00:09:26.920]   than object detection, semantic segmentation,
[00:09:26.920 --> 00:09:29.120]   or something else on top of each other
[00:09:29.120 --> 00:09:31.400]   so that you can compare the performance of your model
[00:09:31.400 --> 00:09:33.280]   on an epoch-wise basis.
[00:09:33.280 --> 00:09:34.720]   And let's say in this case,
[00:09:34.720 --> 00:09:36.320]   if I want to just look at the latest epoch,
[00:09:36.320 --> 00:09:40.240]   so I can say, use a very simple filter operation,
[00:09:40.240 --> 00:09:43.720]   like row of epoch, sorry,
[00:09:43.720 --> 00:09:47.160]   row of epoch is equal to one.
[00:09:47.160 --> 00:09:49.760]   And if I hit apply,
[00:09:49.760 --> 00:09:52.240]   it would only show us the latest epoch.
[00:09:52.240 --> 00:09:55.040]   And if you kind of see one of the images,
[00:09:55.040 --> 00:09:57.960]   you can see the ground truth
[00:09:57.960 --> 00:10:01.240]   and the predicted bounding boxes
[00:10:01.240 --> 00:10:02.480]   were laid on top of each other.
[00:10:02.480 --> 00:10:03.520]   So for one moment,
[00:10:03.520 --> 00:10:06.320]   let us remove the ground truth predictions
[00:10:06.320 --> 00:10:08.600]   and just focus on the predicted bounding box.
[00:10:08.600 --> 00:10:12.840]   So we can see, yes, the model is doing pretty good, right?
[00:10:12.840 --> 00:10:16.160]   But let's say we do not, in this particular case,
[00:10:16.160 --> 00:10:20.040]   at least we do not want the model to pick up people
[00:10:20.040 --> 00:10:21.600]   or detect people from the crowd itself.
[00:10:21.600 --> 00:10:24.400]   So one of the things in object detection literature
[00:10:24.400 --> 00:10:27.040]   we can do is increase the confidence threshold
[00:10:27.040 --> 00:10:29.360]   or the intersection over union threshold,
[00:10:29.360 --> 00:10:32.640]   which we can do simply by moving the slider.
[00:10:32.640 --> 00:10:35.760]   So we can kind of,
[00:10:35.760 --> 00:10:38.440]   for the sake of our application,
[00:10:38.440 --> 00:10:41.320]   we can analyze it in this kind of fine-grained details,
[00:10:41.320 --> 00:10:44.480]   like we can manipulate the confidence threshold
[00:10:44.480 --> 00:10:46.760]   of the prediction of our model
[00:10:46.760 --> 00:10:48.080]   and see how it performs
[00:10:48.080 --> 00:10:51.960]   with respect to high and low thresholds, respectively.
[00:10:51.960 --> 00:10:55.760]   So we can see that although all the people
[00:10:55.760 --> 00:10:56.720]   in the crowd disappear,
[00:10:56.720 --> 00:10:59.760]   if I move to a higher confidence threshold,
[00:10:59.760 --> 00:11:01.800]   the baseball glove and the baseball bat
[00:11:01.800 --> 00:11:05.360]   are objects which are probably in the foreground of the image
[00:11:05.360 --> 00:11:06.520]   and should be detected,
[00:11:06.520 --> 00:11:09.320]   are still detected with a pretty high confidence.
[00:11:09.320 --> 00:11:12.120]   And if you want to explore all the confidence details
[00:11:12.120 --> 00:11:15.320]   in a classifies manner, in further fine-grained manner,
[00:11:15.320 --> 00:11:17.880]   you can do so by scrolling
[00:11:17.880 --> 00:11:19.720]   through this mean confidence section.
[00:11:19.720 --> 00:11:24.720]   So it's the confidence meaned across each occurrence
[00:11:24.720 --> 00:11:28.040]   of a particular category for this particular image.
[00:11:28.040 --> 00:11:30.880]   And similarly, when you hit,
[00:11:30.880 --> 00:11:33.360]   this table is produced when you hit model.train.
[00:11:33.360 --> 00:11:35.760]   And similarly, when you hit model.validate,
[00:11:35.760 --> 00:11:39.000]   this, the similar kind of table is produced
[00:11:39.000 --> 00:11:41.920]   on your validation dataset after you have trained.
[00:11:41.920 --> 00:11:44.920]   So this is also quite useful in a similar manner.
[00:11:44.920 --> 00:11:50.800]   And I'll go back to the presentation for a moment now.
[00:11:50.800 --> 00:11:54.120]   So, you know, since Ultralytics supports
[00:11:54.120 --> 00:11:58.400]   a myriad of multiple tasks and not just object detection,
[00:11:58.400 --> 00:12:00.240]   so we also asked the same question,
[00:12:00.240 --> 00:12:02.000]   like why just support object detection
[00:12:02.000 --> 00:12:05.160]   when we can support all kinds of modalities,
[00:12:05.160 --> 00:12:07.240]   including semantic segmentation,
[00:12:07.240 --> 00:12:09.760]   suppose estimation, instant segmentation masks,
[00:12:09.760 --> 00:12:12.760]   bounding boxes, everything all at once,
[00:12:12.760 --> 00:12:14.080]   why not go for that?
[00:12:14.080 --> 00:12:18.240]   And so I just, you know, kind of want to play this video
[00:12:18.240 --> 00:12:21.920]   for you all, which showcases the full extent
[00:12:21.920 --> 00:12:24.360]   of the Weizenbass' integration for Ultralytics
[00:12:24.360 --> 00:12:25.680]   in its full glory.
[00:12:25.680 --> 00:12:28.760]   So what you can see here is the same thing
[00:12:28.760 --> 00:12:30.720]   for an instant segmentation workflow
[00:12:30.720 --> 00:12:33.920]   where we have the predicted segmentation masks,
[00:12:33.920 --> 00:12:36.160]   the predicted ground truth bounding boxes,
[00:12:36.160 --> 00:12:39.320]   the, sorry, the ground truth bounding boxes
[00:12:39.320 --> 00:12:41.520]   and as well as the predicted bounding boxes.
[00:12:41.520 --> 00:12:44.680]   And you can interact with each and every class
[00:12:44.680 --> 00:12:47.040]   and each and every annotation,
[00:12:47.040 --> 00:12:50.040]   right from the confidence threshold to the prediction masks
[00:12:50.040 --> 00:12:52.920]   to the bounding boxes, everything in an,
[00:12:52.920 --> 00:12:55.240]   using this particular interactive workflow,
[00:12:55.240 --> 00:12:59.840]   which is automatically logged by your Weizenbass' integration
[00:12:59.840 --> 00:13:02.120]   as part of your Ultralytics workflow.
[00:13:03.680 --> 00:13:07.640]   So I'm gonna let, I'm not gonna, you know,
[00:13:07.640 --> 00:13:09.200]   play this video for three minutes.
[00:13:09.200 --> 00:13:11.800]   I'll rather move on to the next slide.
[00:13:11.800 --> 00:13:14.280]   Sorry.
[00:13:14.280 --> 00:13:17.160]   So I wholeheartedly recommend you that
[00:13:17.160 --> 00:13:19.040]   if you want to know more about this integration
[00:13:19.040 --> 00:13:21.720]   and if you want to know more about how you can,
[00:13:21.720 --> 00:13:24.640]   in, you know, supercharge your own computer vision workflows
[00:13:24.640 --> 00:13:25.720]   using Ultralytics,
[00:13:25.720 --> 00:13:27.720]   you can check out this particular report I wrote,
[00:13:27.720 --> 00:13:30.760]   which details all the functions of this integration,
[00:13:30.760 --> 00:13:32.200]   so all the features of this integration
[00:13:32.200 --> 00:13:34.880]   and how you can use it very easily
[00:13:34.880 --> 00:13:37.440]   for your particular computer vision use case.
[00:13:37.440 --> 00:13:40.200]   So that's all from my end.
[00:13:40.200 --> 00:13:43.320]   If you have any questions, feel free to drop them.
[00:13:43.320 --> 00:13:53.800]   - Presentation, we have a couple of questions.
[00:13:53.800 --> 00:13:57.640]   So let me start with first one.
[00:13:57.640 --> 00:13:59.600]   So in the Weizenbass' ecosystem,
[00:13:59.600 --> 00:14:01.680]   what are the most critical factors to consider
[00:14:01.680 --> 00:14:04.160]   when optimizing for computer vision models
[00:14:04.160 --> 00:14:06.240]   for scalability and efficiency?
[00:14:06.240 --> 00:14:11.760]   - Well, so we have this one very particular tool
[00:14:11.760 --> 00:14:13.200]   as part of the Weizenbass' ecosystem,
[00:14:13.200 --> 00:14:15.080]   which is called Sweeps,
[00:14:15.080 --> 00:14:17.320]   which is, which on the surface is,
[00:14:17.320 --> 00:14:18.800]   it's basically a very powerful
[00:14:18.800 --> 00:14:22.360]   hyperparameter search optimization system.
[00:14:22.360 --> 00:14:24.600]   But unlike most other available
[00:14:24.600 --> 00:14:26.400]   hyperparameter search optimization system,
[00:14:26.400 --> 00:14:28.280]   what Sweeps enables you to do is,
[00:14:29.520 --> 00:14:32.440]   it enables you to basically analyze different trends
[00:14:32.440 --> 00:14:33.680]   in your hyperparameters
[00:14:33.680 --> 00:14:39.320]   and analyze trends for correlation,
[00:14:39.320 --> 00:14:41.520]   analyze parameters importance
[00:14:41.520 --> 00:14:46.160]   for a given particular hyperparameter sweep.
[00:14:46.160 --> 00:14:49.960]   And you can actually determine
[00:14:49.960 --> 00:14:53.640]   which particular metric you want to optimize for.
[00:14:53.640 --> 00:14:55.520]   Usually it's the business metric.
[00:14:55.520 --> 00:14:58.120]   For example, in case of object detection,
[00:14:58.120 --> 00:15:00.040]   it's gonna be something like map, for example.
[00:15:00.040 --> 00:15:01.840]   So if you're optimizing for map,
[00:15:01.840 --> 00:15:04.160]   which particular hyperparameters,
[00:15:04.160 --> 00:15:09.160]   affect your map the most in the best possible manner
[00:15:09.160 --> 00:15:14.280]   or which particular model backbones from Ultralytics
[00:15:14.280 --> 00:15:17.400]   is gonna help you maximize the map
[00:15:17.400 --> 00:15:19.360]   for your particular use case,
[00:15:19.360 --> 00:15:22.480]   or maybe it's IOU for your segmentation use case.
[00:15:22.480 --> 00:15:25.400]   So yeah, Sweeps is a tool that I highly recommend
[00:15:25.400 --> 00:15:29.320]   that you check out for these kinds of optimization scenarios.
[00:15:29.320 --> 00:15:30.600]   - Cool.
[00:15:30.600 --> 00:15:31.920]   I think that might actually be connected
[00:15:31.920 --> 00:15:33.920]   to the next question that we had,
[00:15:33.920 --> 00:15:36.920]   which is what would be your top tip
[00:15:36.920 --> 00:15:38.960]   for using weights and biases for object detection?
[00:15:38.960 --> 00:15:39.800]   Is it Sweeps?
[00:15:39.800 --> 00:15:43.840]   - I would say running Sweeps is actually
[00:15:43.840 --> 00:15:45.920]   in a lot of cases computationally expensive.
[00:15:45.920 --> 00:15:50.920]   So if you have the necessary compute,
[00:15:50.920 --> 00:15:52.080]   I mean, go for it.
[00:15:53.240 --> 00:15:57.560]   I typically run Sweeps for like 80 to 100 runs,
[00:15:57.560 --> 00:16:02.040]   which in many cases takes several days to run.
[00:16:02.040 --> 00:16:05.080]   But yeah, Sweeps is not a one-shot solution
[00:16:05.080 --> 00:16:10.080]   to maximize the quality of your computation results.
[00:16:10.080 --> 00:16:16.520]   For that, all we can do is build the best tools for you.
[00:16:16.520 --> 00:16:17.360]   At the end of the day,
[00:16:17.360 --> 00:16:19.560]   it's you, the machine learning engineer,
[00:16:19.560 --> 00:16:21.320]   who is wielding these tools,
[00:16:21.320 --> 00:16:24.640]   and it's up to you how smartly you can use it.
[00:16:24.640 --> 00:16:26.680]   And we are always up for feedback.
[00:16:26.680 --> 00:16:28.640]   So if you're looking for a particular tool,
[00:16:28.640 --> 00:16:30.160]   which is not in our arsenal,
[00:16:30.160 --> 00:16:31.840]   feel free to submit a feature request.
[00:16:31.840 --> 00:16:34.440]   We are always looking out for new feature requests.
[00:16:34.440 --> 00:16:36.240]   - Cool, perfect.
[00:16:36.240 --> 00:16:37.440]   Thank you.
[00:16:37.440 --> 00:16:38.560]   We got a couple more.
[00:16:38.560 --> 00:16:41.160]   In terms of model performance,
[00:16:41.160 --> 00:16:45.560]   what do you prioritize when it comes to metrics,
[00:16:45.560 --> 00:16:48.120]   when it comes to evaluating computer vision models?
[00:16:49.320 --> 00:16:51.280]   - That's an excellent question, actually.
[00:16:51.280 --> 00:16:55.640]   So business metrics can come in a lot of forms, right?
[00:16:55.640 --> 00:16:57.760]   For example, mean average precision,
[00:16:57.760 --> 00:16:58.600]   which is the map,
[00:16:58.600 --> 00:17:02.280]   which is a very common metric for object detection.
[00:17:02.280 --> 00:17:06.520]   There's box classification accuracy,
[00:17:06.520 --> 00:17:08.520]   because object detection at the end of the day
[00:17:08.520 --> 00:17:11.040]   is a detection plus classification problem.
[00:17:11.040 --> 00:17:12.200]   For semantic segmentation,
[00:17:12.200 --> 00:17:16.040]   there's something like intersection over union.
[00:17:16.040 --> 00:17:20.040]   So the business metrics for different problems vary,
[00:17:20.040 --> 00:17:21.560]   but there's a common business metric,
[00:17:21.560 --> 00:17:25.040]   which connects all kinds of computer vision applications,
[00:17:25.040 --> 00:17:26.160]   irrespective of the task,
[00:17:26.160 --> 00:17:30.120]   which is the number of parameters of your model,
[00:17:30.120 --> 00:17:31.520]   the inference speed,
[00:17:31.520 --> 00:17:33.880]   or in one word, you can say the flops,
[00:17:33.880 --> 00:17:36.840]   or floating operations per second for your particular model.
[00:17:36.840 --> 00:17:38.880]   So that's an interesting,
[00:17:38.880 --> 00:17:42.040]   and I would say oftentimes the most important
[00:17:42.040 --> 00:17:45.680]   business metric to optimize for,
[00:17:45.680 --> 00:17:49.360]   even it means sacrificing a little bit on the other ones.
[00:17:49.360 --> 00:17:54.400]   So, because oftentimes we are actually gonna be deploying
[00:17:54.400 --> 00:17:57.600]   our computer vision applications on edge devices.
[00:17:57.600 --> 00:18:00.960]   So we would ideally want to reduce their memory footprint,
[00:18:00.960 --> 00:18:03.920]   their inference lags, and the flops in general.
[00:18:03.920 --> 00:18:10.360]   - Thank you so much for such a great answer.
[00:18:10.360 --> 00:18:12.680]   I think this is the last one we have so far,
[00:18:12.680 --> 00:18:15.200]   unless someone else submits anything now,
[00:18:15.200 --> 00:18:18.680]   but are there any future computer vision integrations
[00:18:18.680 --> 00:18:20.000]   that we're planning or any upgrades
[00:18:20.000 --> 00:18:22.840]   to the existing integrations that we have?
[00:18:22.840 --> 00:18:24.680]   - Absolutely.
[00:18:24.680 --> 00:18:29.680]   So one of the biggest tasks, day-to-day tasks,
[00:18:29.680 --> 00:18:31.800]   which I have to encounter daily is,
[00:18:31.800 --> 00:18:34.920]   maintaining our existing computer vision integrations.
[00:18:34.920 --> 00:18:37.520]   So that takes precedence because,
[00:18:37.520 --> 00:18:39.240]   computer vision is a rapidly moving field
[00:18:39.240 --> 00:18:40.800]   and all the libraries,
[00:18:40.800 --> 00:18:43.000]   computer vision libraries that we have integrations for
[00:18:43.000 --> 00:18:44.800]   are also evolving rapidly.
[00:18:44.800 --> 00:18:48.960]   So it's one of our top priorities is the,
[00:18:48.960 --> 00:18:51.240]   once we already have integrations for,
[00:18:51.240 --> 00:18:55.680]   we should maintain them and keep them up to date.
[00:18:55.680 --> 00:18:59.840]   Other than that, one of the libraries
[00:18:59.840 --> 00:19:02.680]   that we are probably gonna integrate,
[00:19:02.680 --> 00:19:07.280]   or we are scoping or planning for an integration
[00:19:07.280 --> 00:19:11.760]   is Autodistill, which is a distillation library
[00:19:11.760 --> 00:19:14.200]   for computer vision models.
[00:19:14.200 --> 00:19:17.080]   So yeah, that's kind of the plan right now.
[00:19:17.080 --> 00:19:19.160]   - That's cool.
[00:19:19.160 --> 00:19:20.560]   Thank you so much Samekh.
[00:19:20.560 --> 00:19:22.000]   Thanks so much for your presentation
[00:19:22.000 --> 00:19:23.280]   and for joining us today.
[00:19:23.280 --> 00:19:26.720]   We can move on to Anton's presentation.
[00:19:26.720 --> 00:19:30.000]   Anton will focus on crucial metric and tests
[00:19:30.000 --> 00:19:31.800]   for understanding object detection
[00:19:31.800 --> 00:19:34.360]   and tracking performances and smart count,
[00:19:34.360 --> 00:19:37.000]   and how these can be implemented with weights and biases
[00:19:37.000 --> 00:19:41.480]   using ultralytic repository as a practical example.
[00:19:41.480 --> 00:19:43.480]   Hi Anton, thanks so much for joining us.
[00:19:43.480 --> 00:19:45.680]   And yeah, you can take it away.
[00:19:45.680 --> 00:19:46.920]   - Yes, thanks Agata.
[00:19:46.920 --> 00:19:49.440]   Let me share my slides.
[00:19:49.440 --> 00:19:55.720]   So today we will try to learn
[00:19:55.720 --> 00:19:57.440]   how to navigate vision systems
[00:19:57.440 --> 00:20:00.200]   with weights and biases and smart count.
[00:20:00.200 --> 00:20:02.640]   But first let me introduce myself.
[00:20:02.640 --> 00:20:04.040]   My name is Anton.
[00:20:04.040 --> 00:20:05.800]   I had a couple of years of experience
[00:20:05.800 --> 00:20:08.160]   working in different startups,
[00:20:08.160 --> 00:20:11.640]   both from software engineering standpoint
[00:20:11.640 --> 00:20:12.680]   and machine learning.
[00:20:13.680 --> 00:20:17.320]   I built advanced video analytics products,
[00:20:17.320 --> 00:20:21.160]   both for AIoT and on-premise GPUs.
[00:20:21.160 --> 00:20:23.560]   Also I use tools as DeepStreamer,
[00:20:23.560 --> 00:20:26.360]   DeepStream, GStreamer,
[00:20:26.360 --> 00:20:29.200]   and different computer vision models.
[00:20:29.200 --> 00:20:31.960]   You can connect me through social medias
[00:20:31.960 --> 00:20:34.120]   such as X and LinkedIn,
[00:20:34.120 --> 00:20:37.760]   and you can find the tips that I share on my YouTube channel.
[00:20:37.760 --> 00:20:39.600]   If you want to take coffee
[00:20:39.600 --> 00:20:42.600]   and speak about the different data projects,
[00:20:42.600 --> 00:20:46.040]   we also have a community that's called Paris Data Branch.
[00:20:46.040 --> 00:20:48.840]   Link you will find in the slides of this presentation.
[00:20:48.840 --> 00:20:54.680]   I work in a smart cow as a senior software,
[00:20:54.680 --> 00:20:56.920]   machine learning and solutions engineer.
[00:20:56.920 --> 00:21:00.440]   Smart cow was founded in 2016
[00:21:00.440 --> 00:21:05.440]   and builds AI pipeline with both hardware and software.
[00:21:05.440 --> 00:21:08.400]   Here you can see a couple of examples
[00:21:08.400 --> 00:21:10.680]   of our recent products.
[00:21:10.680 --> 00:21:13.120]   First one is a smart camera
[00:21:13.120 --> 00:21:16.200]   with real-time analytics capabilities.
[00:21:16.200 --> 00:21:21.200]   And second one is an industry 4.0 AI-based controller
[00:21:21.200 --> 00:21:23.920]   that has NVIDIA Jetson inside it.
[00:21:23.920 --> 00:21:28.480]   What we will try to learn today.
[00:21:28.480 --> 00:21:32.520]   We will revise a structure of computer vision project
[00:21:33.960 --> 00:21:38.960]   and how we can think about building computer vision products
[00:21:38.960 --> 00:21:42.640]   to easily navigate engineering efforts
[00:21:42.640 --> 00:21:44.680]   for improving quality of our product.
[00:21:44.680 --> 00:21:50.640]   We also will take a strategic overview
[00:21:50.640 --> 00:21:53.360]   of preparing performant object detection
[00:21:53.360 --> 00:21:55.040]   and object tracking systems.
[00:21:55.040 --> 00:22:01.320]   A lot of advanced video analytics products
[00:22:01.320 --> 00:22:04.920]   are based on object detection and object tracking.
[00:22:04.920 --> 00:22:07.840]   Those are two fundamentals algorithms
[00:22:07.840 --> 00:22:11.960]   that you can see in almost any computer vision product.
[00:22:11.960 --> 00:22:19.280]   In today's seminar, we will see the path
[00:22:19.280 --> 00:22:21.480]   from idea to production,
[00:22:21.480 --> 00:22:25.400]   and we will concentrate on a couple of metrics
[00:22:25.400 --> 00:22:28.720]   for object detection and object tracking.
[00:22:28.720 --> 00:22:32.240]   And we will see some nuance and caveats
[00:22:32.240 --> 00:22:35.680]   that you will only see when you go to the real production.
[00:22:35.680 --> 00:22:44.200]   So we want to build some computer vision product.
[00:22:44.200 --> 00:22:46.160]   It's quite simple.
[00:22:46.160 --> 00:22:49.440]   We start as always from problem analysis.
[00:22:49.440 --> 00:22:52.680]   We defined what kind of objects we want to detect.
[00:22:52.680 --> 00:22:56.840]   We defined what kind of constraints we have
[00:22:56.840 --> 00:23:01.400]   on hardware, on latency speed of our models.
[00:23:01.400 --> 00:23:05.720]   We also tried to think about what kind of datasets
[00:23:05.720 --> 00:23:07.120]   could be available
[00:23:07.120 --> 00:23:09.440]   and how difficult it is to collect new data.
[00:23:09.440 --> 00:23:13.760]   Second step is to build in first baseline
[00:23:13.760 --> 00:23:16.000]   in order to understand the complexity
[00:23:16.000 --> 00:23:18.120]   of the problem at hands.
[00:23:18.120 --> 00:23:23.120]   We prepare the dataset, configuration file for training,
[00:23:23.120 --> 00:23:25.320]   and also a couple of scripts
[00:23:25.320 --> 00:23:28.240]   that will include experiment tracking as well.
[00:23:28.240 --> 00:23:33.800]   At this point in time, we are not going to production,
[00:23:33.800 --> 00:23:39.200]   and we still have a chance to make a couple of iterations
[00:23:39.200 --> 00:23:41.240]   in order to improve our current models.
[00:23:41.240 --> 00:23:45.120]   Probably we can get some data
[00:23:45.120 --> 00:23:48.600]   that is quite similar to production environment
[00:23:48.600 --> 00:23:50.600]   in order to improve our models further.
[00:23:50.600 --> 00:23:54.080]   At this point, we also can use sweeps
[00:23:55.000 --> 00:23:57.520]   in order to understand which parameters
[00:23:57.520 --> 00:23:59.520]   are most suitable for our tasks.
[00:23:59.520 --> 00:24:04.720]   Fourth step is a deployment of our first model.
[00:24:04.720 --> 00:24:07.640]   If everything is okay,
[00:24:07.640 --> 00:24:12.080]   after a while, we can congratulate ourselves.
[00:24:12.080 --> 00:24:15.120]   But as you already understood,
[00:24:15.120 --> 00:24:16.960]   it's only the beginning of my talk.
[00:24:16.960 --> 00:24:20.480]   So it's really a beginning of our process
[00:24:20.480 --> 00:24:23.920]   of building a really stable product
[00:24:23.920 --> 00:24:26.240]   based on computer vision models
[00:24:26.240 --> 00:24:28.320]   that could bring value to our clients.
[00:24:28.320 --> 00:24:34.120]   There are a lot of myths
[00:24:34.120 --> 00:24:36.520]   about machine learning models development.
[00:24:36.520 --> 00:24:40.920]   Let's see a couple of them.
[00:24:40.920 --> 00:24:43.800]   First one is that you only deploy
[00:24:43.800 --> 00:24:48.440]   one or two machine learning models at a time.
[00:24:48.440 --> 00:24:53.640]   It's usually is false, and in extreme cases,
[00:24:53.640 --> 00:24:57.520]   you will have to support multiple models
[00:24:57.520 --> 00:25:01.680]   per client and maybe per use case.
[00:25:01.680 --> 00:25:04.720]   If we are speaking about computer vision analytics,
[00:25:04.720 --> 00:25:06.960]   you will have multiple versions of models
[00:25:06.960 --> 00:25:11.960]   or multiple different models for each smart camera
[00:25:11.960 --> 00:25:15.600]   that films in some part of the street.
[00:25:15.600 --> 00:25:20.800]   A second myth is that if we don't do anything,
[00:25:20.800 --> 00:25:22.560]   model performance remains the same.
[00:25:23.560 --> 00:25:27.560]   Usually, if you try to deploy some models in production,
[00:25:27.560 --> 00:25:30.840]   you know that environment will change.
[00:25:30.840 --> 00:25:32.760]   It's true that different environments
[00:25:32.760 --> 00:25:34.480]   will change with different speeds,
[00:25:34.480 --> 00:25:38.960]   but you will always have some variables,
[00:25:38.960 --> 00:25:41.840]   some variations that you didn't take into account
[00:25:41.840 --> 00:25:43.480]   because you didn't know about it.
[00:25:43.480 --> 00:25:49.120]   So when you plan the delivery of your project,
[00:25:50.200 --> 00:25:53.800]   even from the very first stage of problem analysis,
[00:25:53.800 --> 00:25:55.080]   you should take into account
[00:25:55.080 --> 00:25:58.320]   how you will update models into production.
[00:25:58.320 --> 00:26:03.440]   Sometimes managers and even engineers,
[00:26:03.440 --> 00:26:07.880]   they don't bother about these questions at the beginning,
[00:26:07.880 --> 00:26:12.000]   and then it's very difficult to make a successful product
[00:26:12.000 --> 00:26:13.000]   after deployment.
[00:26:13.000 --> 00:26:18.120]   So the third myth is that you won't need
[00:26:18.120 --> 00:26:20.320]   to update your models as much.
[00:26:20.320 --> 00:26:24.320]   Might be true sometime if it's one of project
[00:26:24.320 --> 00:26:27.080]   or maybe it was just a proof of concept.
[00:26:27.080 --> 00:26:30.040]   Yes, maybe you will not continue
[00:26:30.040 --> 00:26:33.120]   and you don't need to update those models.
[00:26:33.120 --> 00:26:36.640]   But when you are building a really successful product
[00:26:36.640 --> 00:26:39.120]   with machine learning,
[00:26:39.120 --> 00:26:43.680]   you want to design your system
[00:26:43.680 --> 00:26:47.320]   in a way that you can update models as fast
[00:26:47.320 --> 00:26:50.000]   and as frequently as possible.
[00:26:50.000 --> 00:26:57.320]   So the last in this small list of myths
[00:26:57.320 --> 00:27:01.360]   is that most machine learning engineers
[00:27:01.360 --> 00:27:03.280]   don't need to worry about scale.
[00:27:03.280 --> 00:27:10.240]   For advanced video analytics, it's not particularly true
[00:27:10.240 --> 00:27:13.400]   because usually we are trying to analyze
[00:27:13.400 --> 00:27:15.360]   real-time stream of video,
[00:27:16.800 --> 00:27:19.760]   ranging from a couple of frames per second
[00:27:19.760 --> 00:27:24.280]   to 30 frames per second, or even at higher speeds.
[00:27:24.280 --> 00:27:29.440]   And even if you have dozens or maybe hundreds of cameras,
[00:27:29.440 --> 00:27:32.160]   it's already quite an intense scale.
[00:27:32.160 --> 00:27:36.360]   And you need to consider if your model is scalable enough
[00:27:36.360 --> 00:27:37.320]   from the beginning.
[00:27:37.320 --> 00:27:41.800]   For other insights into production
[00:27:41.800 --> 00:27:44.080]   with machine learning systems,
[00:27:44.080 --> 00:27:45.160]   take a look at this book,
[00:27:45.160 --> 00:27:47.800]   "Design in Machine Learning Systems."
[00:27:47.800 --> 00:27:49.880]   It helped me quite a lot in my career.
[00:27:49.880 --> 00:28:00.880]   This is a photo of quite old paper
[00:28:00.880 --> 00:28:04.480]   from 2014 or maybe 2015.
[00:28:04.480 --> 00:28:06.240]   It's called "Hidden Technical Debt
[00:28:06.240 --> 00:28:08.200]   in Machine Learning Systems."
[00:28:08.200 --> 00:28:10.560]   And they had a very good visualization
[00:28:10.560 --> 00:28:14.960]   of what we have inside machine learning system
[00:28:14.960 --> 00:28:17.600]   or machine learning product.
[00:28:17.600 --> 00:28:22.120]   You see this black square in the middle?
[00:28:22.120 --> 00:28:24.760]   This is our machine learning code.
[00:28:24.760 --> 00:28:29.760]   Usually, this code that provides the core value
[00:28:29.760 --> 00:28:34.560]   of our product is only a small part of a whole product.
[00:28:34.560 --> 00:28:39.880]   Machine learning models by itself
[00:28:39.880 --> 00:28:43.360]   cannot bring solid value to the clients
[00:28:43.360 --> 00:28:46.240]   without a lot of supporting infrastructure,
[00:28:46.240 --> 00:28:49.160]   which is classical software around it.
[00:28:49.160 --> 00:28:53.160]   Here, I can advise you to read
[00:28:53.160 --> 00:28:55.200]   "Modern Software Engineering" book,
[00:28:55.200 --> 00:28:58.280]   which provides a lot of ideas
[00:28:58.280 --> 00:29:03.280]   how to build robust software better and faster.
[00:29:03.280 --> 00:29:09.080]   And that could help you avoid very strange mistakes
[00:29:09.960 --> 00:29:13.560]   and we'd rather work on improving
[00:29:13.560 --> 00:29:17.120]   the quality of our machine learning models
[00:29:17.120 --> 00:29:20.520]   rather than working to find some bugs
[00:29:20.520 --> 00:29:22.440]   in our software in production.
[00:29:22.440 --> 00:29:30.160]   So, with these caveats,
[00:29:30.160 --> 00:29:34.160]   after deployment phase,
[00:29:34.160 --> 00:29:37.360]   the most interesting part will begin.
[00:29:38.400 --> 00:29:39.520]   After deployment,
[00:29:39.520 --> 00:29:43.840]   we need to collect data from production.
[00:29:43.840 --> 00:29:48.840]   We need to get some process in place
[00:29:48.840 --> 00:29:53.760]   in order to annotate the new data from production
[00:29:53.760 --> 00:29:56.840]   or to find the ground truth for this data
[00:29:56.840 --> 00:29:58.080]   and compute metrics.
[00:29:58.080 --> 00:30:04.200]   Those are real metrics and real performance of our models.
[00:30:04.200 --> 00:30:05.520]   And the challenge is
[00:30:08.400 --> 00:30:10.840]   to review your training process,
[00:30:10.840 --> 00:30:15.680]   to make as many iterations of fine tuning for your models
[00:30:15.680 --> 00:30:18.480]   as you need in order to bridge the gap
[00:30:18.480 --> 00:30:23.320]   between your training metrics
[00:30:23.320 --> 00:30:26.880]   and the metrics that you will deliver in production.
[00:30:26.880 --> 00:30:33.760]   After that, you will continue iterating on your model.
[00:30:33.760 --> 00:30:36.360]   You will continue iterating
[00:30:36.360 --> 00:30:38.600]   on the inference side of your model.
[00:30:38.600 --> 00:30:41.280]   You will try to learn which trade-offs
[00:30:41.280 --> 00:30:44.720]   between hyperparameters and model performance you can do
[00:30:44.720 --> 00:30:48.280]   in order to optimize resource utilization
[00:30:48.280 --> 00:30:49.880]   or maybe even performance.
[00:30:49.880 --> 00:30:53.680]   Then, when you collect enough of the data
[00:30:53.680 --> 00:30:57.160]   and you will analyze the data,
[00:30:57.160 --> 00:31:00.560]   you will start to understand what kind of metrics
[00:31:00.560 --> 00:31:02.320]   that you are using,
[00:31:02.320 --> 00:31:05.080]   that you optimize directly in your training process.
[00:31:06.080 --> 00:31:09.640]   And this could be connected to the business metrics
[00:31:09.640 --> 00:31:12.120]   that you provide to the clients.
[00:31:12.120 --> 00:31:18.480]   Because sometimes there is a slight disconnect
[00:31:18.480 --> 00:31:22.160]   between functions that you optimize
[00:31:22.160 --> 00:31:24.000]   during training process
[00:31:24.000 --> 00:31:28.480]   and the business metrics that clients are interested.
[00:31:28.480 --> 00:31:32.080]   For example, if you use model detection,
[00:31:32.080 --> 00:31:33.720]   object detection model,
[00:31:33.720 --> 00:31:37.200]   you will usually optimize for mean average precision.
[00:31:37.200 --> 00:31:40.160]   But then, clients will be interested
[00:31:40.160 --> 00:31:43.800]   how accurately you will count objects on a video.
[00:31:43.800 --> 00:31:48.240]   And through time, you will find the connection
[00:31:48.240 --> 00:31:49.960]   between those two metrics
[00:31:49.960 --> 00:31:53.080]   that will help you guide the model choice further.
[00:31:53.080 --> 00:31:57.800]   And even at the beginning of your project,
[00:31:57.800 --> 00:31:59.880]   at the problem analysis stage,
[00:32:00.640 --> 00:32:04.080]   you need to design some instruments to monitor
[00:32:04.080 --> 00:32:06.520]   and to observe the behavior of your model.
[00:32:06.520 --> 00:32:10.040]   Because usually the environment is much,
[00:32:10.040 --> 00:32:15.960]   is quite richer than you think about it at the first place.
[00:32:15.960 --> 00:32:23.800]   After this overview into the path to production,
[00:32:23.800 --> 00:32:28.600]   you might easily understand why Ultralytics
[00:32:29.600 --> 00:32:32.360]   YOLO models is very interesting,
[00:32:32.360 --> 00:32:37.160]   especially for the first deployment.
[00:32:37.160 --> 00:32:41.800]   The interface is very useful.
[00:32:41.800 --> 00:32:45.200]   There is a wide range of models
[00:32:45.200 --> 00:32:47.080]   from starting from very small
[00:32:47.080 --> 00:32:53.040]   that has good performance on edge devices
[00:32:53.040 --> 00:32:56.400]   and with models that have more parameters
[00:32:56.400 --> 00:32:59.720]   in order to provide better accuracy.
[00:32:59.720 --> 00:33:04.840]   Also, I found very useful the feature
[00:33:04.840 --> 00:33:09.520]   that YOLO models are quite simple by architecture,
[00:33:09.520 --> 00:33:13.800]   and it's very easy to convert them to ONX formats
[00:33:13.800 --> 00:33:16.840]   and to the special optimized runtimes,
[00:33:16.840 --> 00:33:19.240]   such as NVIDIA TensorRT runtime.
[00:33:19.240 --> 00:33:24.160]   Also, we have a native integration with Weights and Biases
[00:33:24.160 --> 00:33:25.840]   and quite simple integration
[00:33:25.840 --> 00:33:30.840]   with streaming analytics software, such as DeepStream.
[00:33:30.840 --> 00:33:38.680]   Second tool in our toolbox would be Weights and Biases.
[00:33:38.680 --> 00:33:43.560]   Weights and Biases provides a lot of features
[00:33:43.560 --> 00:33:47.520]   to control your model training and your model lifecycle,
[00:33:47.520 --> 00:33:51.920]   ranging from dataset versioning, experiment tracking,
[00:33:51.920 --> 00:33:55.120]   model versioning, Slack notifications,
[00:33:55.120 --> 00:33:57.960]   different webhooks that might be useful
[00:33:57.960 --> 00:33:59.720]   in order to package your model,
[00:33:59.720 --> 00:34:02.680]   for example, convert to ONX
[00:34:02.680 --> 00:34:05.280]   or convert to other runtimes automatically.
[00:34:05.280 --> 00:34:09.000]   And also, they have an interesting feature of reports
[00:34:09.000 --> 00:34:13.440]   where you can discuss your findings with colleagues
[00:34:13.440 --> 00:34:18.440]   and have a good tables and visualization
[00:34:18.440 --> 00:34:20.200]   to explain model performance.
[00:34:21.200 --> 00:34:25.200]   When you start, you might want to consider
[00:34:25.200 --> 00:34:30.200]   only subset of those features and use it consistently.
[00:34:30.200 --> 00:34:33.120]   Then you can broaden, with experience,
[00:34:33.120 --> 00:34:36.200]   you can broaden the usage of Weights and Biases
[00:34:36.200 --> 00:34:38.800]   to step up your game of model training.
[00:34:38.800 --> 00:34:43.800]   As a feature that I find useful is a lightweight integration
[00:34:43.800 --> 00:34:48.080]   even though you might want to use other repositories,
[00:34:48.080 --> 00:34:53.080]   maybe MM detection or PyTorch Lightning.
[00:34:53.080 --> 00:34:55.920]   It's always easier with a couple of lines of code
[00:34:55.920 --> 00:34:58.000]   to integrate Weights and Biases
[00:34:58.000 --> 00:35:02.600]   and to observe your experiments in a more consistent way.
[00:35:02.600 --> 00:35:06.200]   Also, Weights and Biases provides very good default system
[00:35:06.200 --> 00:35:07.520]   and environment logging.
[00:35:07.520 --> 00:35:11.160]   Usually, you don't use it that much,
[00:35:11.160 --> 00:35:14.800]   but when you have some problems in model performance,
[00:35:14.800 --> 00:35:17.120]   you might want to take a look at it.
[00:35:17.120 --> 00:35:20.560]   And also you have great documents and courses
[00:35:20.560 --> 00:35:23.600]   about machine learning operations with Weights and Biases.
[00:35:23.600 --> 00:35:29.480]   If you ask yourself how to do things with Weights and Biases,
[00:35:29.480 --> 00:35:34.280]   you can check out a lot of YouTube videos of Weights and Biases
[00:35:34.280 --> 00:35:38.200]   and their special pages with courses.
[00:35:38.200 --> 00:35:41.600]   Those courses are very short and compact
[00:35:41.600 --> 00:35:44.120]   and you learn all the features
[00:35:44.960 --> 00:35:49.040]   and the optimal ways of their applications.
[00:35:49.040 --> 00:35:53.280]   But our challenge will be
[00:35:53.280 --> 00:35:56.000]   which things to do with Weights and Biases.
[00:35:56.000 --> 00:36:00.240]   What kind of questions we want to concentrate our effort
[00:36:00.240 --> 00:36:07.880]   to improve models performance in production.
[00:36:07.880 --> 00:36:13.080]   So in every product that I have ever built,
[00:36:13.080 --> 00:36:18.080]   if you are able to precisely answer those four questions,
[00:36:18.080 --> 00:36:21.800]   your iteration speeds of model improvement
[00:36:21.800 --> 00:36:23.360]   will be quite fast.
[00:36:23.360 --> 00:36:27.120]   And no matter what has happened in production,
[00:36:27.120 --> 00:36:29.440]   what kind of events, what kind of anomalies,
[00:36:29.440 --> 00:36:33.600]   what kind of new behavior you observed in production,
[00:36:33.600 --> 00:36:35.560]   you will know for sure how much time
[00:36:35.560 --> 00:36:39.080]   it will take to fix those problems.
[00:36:39.080 --> 00:36:41.680]   So first question is how to get performance measurement
[00:36:41.680 --> 00:36:43.720]   from production?
[00:36:43.720 --> 00:36:47.040]   You should think about it from the start,
[00:36:47.040 --> 00:36:50.440]   from the beginning of requirement collections
[00:36:50.440 --> 00:36:52.640]   and project planning.
[00:36:52.640 --> 00:36:56.240]   And maybe you should try to prepare
[00:36:56.240 --> 00:36:58.320]   external localization team,
[00:36:58.320 --> 00:37:01.640]   or maybe you should think about what kind of tools you need
[00:37:01.640 --> 00:37:05.120]   to make this process faster.
[00:37:05.120 --> 00:37:07.240]   In the best case scenario,
[00:37:07.240 --> 00:37:10.200]   you have some automatic process
[00:37:10.200 --> 00:37:12.760]   that will provide you with ground truth,
[00:37:12.760 --> 00:37:16.160]   but usually that happens quite rarely.
[00:37:16.160 --> 00:37:21.160]   Second question is that all computer vision products,
[00:37:21.160 --> 00:37:24.160]   they consist of multiple models.
[00:37:24.160 --> 00:37:29.240]   And these multiple models are working as a pipeline,
[00:37:29.240 --> 00:37:33.480]   and you feed the output of one model
[00:37:33.480 --> 00:37:35.440]   as an input to another model.
[00:37:35.440 --> 00:37:38.720]   And you will always ask yourself
[00:37:38.720 --> 00:37:41.920]   which part of pipeline has an issue.
[00:37:41.920 --> 00:37:48.560]   In computer vision analytics for smart cities,
[00:37:48.560 --> 00:37:52.000]   usually we have detection, tracking algorithm,
[00:37:52.000 --> 00:37:56.880]   and after that we process the results of those two models.
[00:37:56.880 --> 00:38:02.600]   And we really should understand the internals of algorithms
[00:38:02.600 --> 00:38:05.080]   and to know how to measure their performance
[00:38:05.080 --> 00:38:08.400]   in order to separate the influence
[00:38:08.400 --> 00:38:12.520]   of different stages of pipeline on the final result,
[00:38:12.520 --> 00:38:13.880]   which might be challenging.
[00:38:13.880 --> 00:38:20.080]   Third question is how much time you spend on each iteration.
[00:38:20.080 --> 00:38:23.160]   This question is important because you need to understand
[00:38:23.160 --> 00:38:28.880]   how much, what kind of experiments you can do
[00:38:28.880 --> 00:38:30.280]   in a short span of time.
[00:38:31.480 --> 00:38:36.200]   So usually you should know the numbers for data collection,
[00:38:36.200 --> 00:38:37.480]   how much data you need,
[00:38:37.480 --> 00:38:40.640]   and how much time it will take to annotate it.
[00:38:40.640 --> 00:38:45.360]   You should prepare a couple of schedules for fine tuning.
[00:38:45.360 --> 00:38:48.280]   I found that it might be interesting
[00:38:48.280 --> 00:38:53.040]   to get fixed time schedules in order to iterate over data
[00:38:53.040 --> 00:38:55.960]   rather than over hyper parameters of a model.
[00:38:55.960 --> 00:38:59.680]   After a couple of first iteration,
[00:38:59.680 --> 00:39:03.040]   this is what will bring you additional percentage
[00:39:03.040 --> 00:39:03.880]   of accuracy.
[00:39:03.880 --> 00:39:08.760]   Third one, you need to build a process of model validation.
[00:39:08.760 --> 00:39:14.240]   When you're really close to nearly perfect performance
[00:39:14.240 --> 00:39:18.760]   in production, it's very difficult to compare models.
[00:39:18.760 --> 00:39:20.960]   But we will see in the metric discussion
[00:39:20.960 --> 00:39:22.000]   how we will do that.
[00:39:22.000 --> 00:39:28.080]   And also, as we have seen in the machine learning myths,
[00:39:28.520 --> 00:39:31.880]   you need to understand how fast you can redeploy models
[00:39:31.880 --> 00:39:32.720]   and how often.
[00:39:32.720 --> 00:39:36.640]   The last but not least is,
[00:39:36.640 --> 00:39:42.440]   the question is my candidate model that I verified,
[00:39:42.440 --> 00:39:44.360]   that it's quite good.
[00:39:44.360 --> 00:39:46.800]   Is it really better than the model in production?
[00:39:46.800 --> 00:39:50.480]   Sometimes, even though the model gives you
[00:39:50.480 --> 00:39:54.600]   slightly better results on your training data sets,
[00:39:54.600 --> 00:39:58.320]   it might not give you improvements in production.
[00:39:58.320 --> 00:40:03.800]   So there are a couple of methods to solve this problem.
[00:40:03.800 --> 00:40:05.800]   And one of them is to try,
[00:40:05.800 --> 00:40:10.360]   continue, always continue the collection of new data sets
[00:40:10.360 --> 00:40:14.920]   that will approach your training samples
[00:40:14.920 --> 00:40:16.800]   to the environment in production.
[00:40:16.800 --> 00:40:21.280]   We can use weights and biases
[00:40:21.280 --> 00:40:24.360]   and the login of different experiments
[00:40:24.360 --> 00:40:29.160]   and also such features as model zoo and dataset tracking
[00:40:29.160 --> 00:40:33.760]   to help us answering questions two, three, and four.
[00:40:33.760 --> 00:40:43.120]   Right now, let's speak about a couple of metrics.
[00:40:43.120 --> 00:40:48.120]   And we will see a little bit more nuanced approach
[00:40:48.120 --> 00:40:50.680]   to evaluation of object detection
[00:40:50.680 --> 00:40:52.960]   and object tracking metrics.
[00:40:52.960 --> 00:40:55.200]   So basics of object detection metrics.
[00:40:55.200 --> 00:40:57.600]   Small disclaimer here,
[00:40:57.600 --> 00:41:00.800]   we will not speak about the details
[00:41:00.800 --> 00:41:04.040]   of implementation of every metric.
[00:41:04.040 --> 00:41:06.360]   There are a lot of resources about that,
[00:41:06.360 --> 00:41:11.360]   but I will give you a couple of tips
[00:41:11.360 --> 00:41:16.240]   of which part of metrics we should concentrate our effort
[00:41:16.240 --> 00:41:19.680]   and what was beneficial for model performance
[00:41:20.640 --> 00:41:24.000]   in the past in my experience.
[00:41:24.000 --> 00:41:28.760]   So the most common metric is mean average precision
[00:41:28.760 --> 00:41:31.480]   for object detection.
[00:41:31.480 --> 00:41:34.120]   You see that metric in all research papers
[00:41:34.120 --> 00:41:38.280]   for object detection, and you can compare this metric,
[00:41:38.280 --> 00:41:42.160]   you can compare models using this metric in a general sense.
[00:41:42.160 --> 00:41:46.160]   But this might not be, it's a good proxy.
[00:41:46.160 --> 00:41:49.080]   It's always, you start always from here,
[00:41:49.080 --> 00:41:54.080]   but we might want to tweak it when we need to scratch
[00:41:54.080 --> 00:41:57.360]   this percentage, additional percentage of performance
[00:41:57.360 --> 00:41:58.840]   in the production.
[00:41:58.840 --> 00:42:02.040]   Also, you have a confusion matrix to understand
[00:42:02.040 --> 00:42:05.200]   what kind of classes are mixed by the model.
[00:42:05.200 --> 00:42:09.400]   You have a mean average precision,
[00:42:09.400 --> 00:42:14.240]   but in different implementation for different object sizes
[00:42:14.240 --> 00:42:16.160]   with different thresholds,
[00:42:18.040 --> 00:42:21.640]   which are used in COCO dataset evaluation,
[00:42:21.640 --> 00:42:24.600]   this is also quite a good start.
[00:42:24.600 --> 00:42:32.440]   For object detection, as in classic machine learning,
[00:42:32.440 --> 00:42:35.600]   precision, recall, accuracy, all these metrics,
[00:42:35.600 --> 00:42:39.240]   they will give you some information about your models.
[00:42:39.240 --> 00:42:45.680]   But those metrics will, if your models are good enough
[00:42:46.120 --> 00:42:48.320]   that you already went to the production,
[00:42:48.320 --> 00:42:50.160]   but you still need to go further,
[00:42:50.160 --> 00:42:54.160]   those metrics, they will not vary that much.
[00:42:54.160 --> 00:42:57.360]   So you might want to consider more nuanced approach.
[00:42:57.360 --> 00:43:01.800]   For example, tight framework that will break down
[00:43:01.800 --> 00:43:05.320]   object detection errors into five classes.
[00:43:05.320 --> 00:43:08.600]   When you misclassified object,
[00:43:08.600 --> 00:43:12.640]   when you have problems with localization of an object,
[00:43:12.640 --> 00:43:17.480]   when you mixed an object with the background,
[00:43:17.480 --> 00:43:23.720]   or maybe you have duplicates of your predictions.
[00:43:23.720 --> 00:43:29.640]   This will also give you some ideas about model performance,
[00:43:29.640 --> 00:43:34.360]   but all these metrics are quite basic.
[00:43:34.360 --> 00:43:37.000]   And sometimes when you have problem
[00:43:37.000 --> 00:43:39.400]   only with one class of objects
[00:43:39.400 --> 00:43:41.680]   for one particular point of view,
[00:43:41.680 --> 00:43:45.160]   you might need to dig a little bit deeper.
[00:43:45.160 --> 00:43:50.600]   Let's take a closer look at CoCo metrics.
[00:43:50.600 --> 00:43:53.400]   Here on the right side, let's concentrate
[00:43:53.400 --> 00:43:57.000]   on the average precision for small, medium,
[00:43:57.000 --> 00:43:58.200]   and large objects.
[00:43:58.200 --> 00:44:05.000]   Usually your task in video analytics
[00:44:05.000 --> 00:44:08.440]   is not that general as research papers.
[00:44:09.360 --> 00:44:14.360]   And you can try to understand what size of objects
[00:44:14.360 --> 00:44:16.000]   you really need to analyze.
[00:44:16.000 --> 00:44:22.320]   And maybe you can introduce mean average precision
[00:44:22.320 --> 00:44:24.720]   for these sizes of an objects.
[00:44:24.720 --> 00:44:30.960]   For many use cases of analyzing CCTV camera feedbacks
[00:44:30.960 --> 00:44:36.920]   from smart cities, you are not really interested
[00:44:36.920 --> 00:44:38.840]   in very small objects.
[00:44:38.840 --> 00:44:40.840]   You are mostly interested in the objects
[00:44:40.840 --> 00:44:45.000]   that are on the front plan,
[00:44:45.000 --> 00:44:49.000]   or that are quite large to be able to analyze
[00:44:49.000 --> 00:44:54.160]   different vehicles and maybe metadata for these vehicles,
[00:44:54.160 --> 00:44:56.720]   such as color or license plate.
[00:44:56.720 --> 00:45:00.040]   So you might want to consider changing these parameters
[00:45:00.040 --> 00:45:02.320]   of your metric calculation,
[00:45:02.320 --> 00:45:05.560]   and that will give you a better idea
[00:45:05.560 --> 00:45:08.360]   and maybe some room for improvement.
[00:45:09.360 --> 00:45:12.840]   If you want to go one step further,
[00:45:12.840 --> 00:45:16.760]   you might want to consider these CoCo metrics
[00:45:16.760 --> 00:45:20.680]   with object sizes that are more appropriate
[00:45:20.680 --> 00:45:22.200]   to your problem at hand.
[00:45:22.200 --> 00:45:27.560]   And then you will start to work in with your datasets,
[00:45:27.560 --> 00:45:29.520]   because every image in your dataset
[00:45:29.520 --> 00:45:32.240]   could have a different metadata.
[00:45:32.240 --> 00:45:36.760]   It might be images from day or images from night.
[00:45:36.760 --> 00:45:39.200]   You might have CCTV cameras
[00:45:39.200 --> 00:45:43.920]   that have different point of view of different angle,
[00:45:43.920 --> 00:45:46.240]   or maybe it's a bird's eye view.
[00:45:46.240 --> 00:45:49.760]   And you really need to work on your dataset,
[00:45:49.760 --> 00:45:53.240]   the combination of training and validation dataset
[00:45:53.240 --> 00:45:57.040]   that will be quite close to the real world example.
[00:45:57.040 --> 00:46:02.640]   Second approach is to prepare some robustness test.
[00:46:02.640 --> 00:46:06.920]   Robustness test, they might be of two types.
[00:46:06.920 --> 00:46:10.440]   First one is an image augmentation, which is quite simple,
[00:46:10.440 --> 00:46:15.040]   and we already use this technique during training,
[00:46:15.040 --> 00:46:17.400]   but you can go one step further
[00:46:17.400 --> 00:46:20.080]   and prepare special training datasets
[00:46:20.080 --> 00:46:22.200]   that you are not using,
[00:46:22.200 --> 00:46:25.840]   special test datasets that you are not using in training,
[00:46:25.840 --> 00:46:29.200]   and introduce image corruptions
[00:46:29.200 --> 00:46:31.160]   that are quite similar to production.
[00:46:32.080 --> 00:46:34.360]   And that might give you some ideas
[00:46:34.360 --> 00:46:38.120]   what to expect if there will be a lot of rain
[00:46:38.120 --> 00:46:39.240]   or a lot of snow.
[00:46:39.240 --> 00:46:45.600]   Next, you can connect to some online streams
[00:46:45.600 --> 00:46:48.680]   that are filmed in streets in different cities
[00:46:48.680 --> 00:46:50.120]   around the world,
[00:46:50.120 --> 00:46:55.120]   and run your pipelines 24/7
[00:46:55.120 --> 00:47:00.160]   to see if there are a lot of mistakes
[00:47:00.160 --> 00:47:02.840]   and collect those mistakes for analysis.
[00:47:02.840 --> 00:47:08.280]   Fourth is business metrics.
[00:47:08.280 --> 00:47:11.960]   So if you're able to directly optimize business metrics,
[00:47:11.960 --> 00:47:14.240]   that is the best case scenario.
[00:47:14.240 --> 00:47:15.840]   Usually it's not the case.
[00:47:15.840 --> 00:47:20.840]   Usually you have some kind of object detection metrics,
[00:47:20.840 --> 00:47:23.560]   but you still need to make a lot of experiments
[00:47:23.560 --> 00:47:25.320]   and to collect the data from production
[00:47:25.320 --> 00:47:28.960]   to understand the connection between those two metrics.
[00:47:28.960 --> 00:47:30.560]   To be able to answer the question,
[00:47:30.560 --> 00:47:34.280]   okay, I have this level of mean average precision
[00:47:34.280 --> 00:47:39.800]   for all these type of objects.
[00:47:39.800 --> 00:47:42.800]   That means that I might be quite safe
[00:47:42.800 --> 00:47:44.600]   to put this model into production.
[00:47:44.600 --> 00:47:49.560]   But this comes with experimentation and experience.
[00:47:49.560 --> 00:47:56.240]   After making a lot of deployments
[00:47:56.240 --> 00:47:59.160]   and observing the behavior of models,
[00:47:59.160 --> 00:48:02.480]   you might have the opportunity
[00:48:02.480 --> 00:48:05.960]   to collect a lot of images from production
[00:48:05.960 --> 00:48:10.960]   and combine and cluster them to compare,
[00:48:10.960 --> 00:48:15.640]   to prepare the datasets that are specifically fine-tuned
[00:48:15.640 --> 00:48:17.600]   for the production that you expect
[00:48:17.600 --> 00:48:23.560]   to be next deployment environment.
[00:48:24.520 --> 00:48:28.600]   And this might give you a boost of performance.
[00:48:28.600 --> 00:48:35.040]   Here is an example of a robustness test.
[00:48:35.040 --> 00:48:37.440]   There is a library called image corruption
[00:48:37.440 --> 00:48:41.640]   that provides 15 different image augmentations.
[00:48:41.640 --> 00:48:45.520]   And some of them are quite realistic
[00:48:45.520 --> 00:48:47.960]   and you can easily see them in production.
[00:48:47.960 --> 00:48:53.880]   Example of images that I have seen in production
[00:48:53.880 --> 00:48:57.480]   is a fog image, which is quite realistic.
[00:48:57.480 --> 00:49:00.000]   Also, you have a lot of pixelated cameras.
[00:49:00.000 --> 00:49:04.280]   In general, you might want to constrain your clients
[00:49:04.280 --> 00:49:07.360]   on using one camera, but not another.
[00:49:07.360 --> 00:49:10.960]   But sometimes if your training pipeline is good enough,
[00:49:10.960 --> 00:49:13.960]   you might broaden your client base on,
[00:49:13.960 --> 00:49:20.200]   even for a client that don't have the latest cameras
[00:49:20.240 --> 00:49:24.760]   in terms of quality or image resolution.
[00:49:24.760 --> 00:49:27.480]   You can go one step further.
[00:49:27.480 --> 00:49:30.440]   And on a recent webinar of NVIDIA,
[00:49:30.440 --> 00:49:34.120]   they provided a couple of examples of generative,
[00:49:34.120 --> 00:49:38.840]   how to use a generative AI to prepare the datasets.
[00:49:38.840 --> 00:49:43.840]   Here we have maybe some warehouse or factory,
[00:49:46.120 --> 00:49:50.320]   and you can create a palette, a template, a mask,
[00:49:50.320 --> 00:49:54.640]   and then ask stable diffusion or other generative AI models
[00:49:54.640 --> 00:49:59.120]   with text prompts to generate different types of images.
[00:49:59.120 --> 00:50:01.560]   And you can create this dataset.
[00:50:01.560 --> 00:50:06.560]   You can add this dataset to weights and biases
[00:50:06.560 --> 00:50:10.760]   to trace the lineage and continue experiments
[00:50:10.760 --> 00:50:13.200]   for training your models.
[00:50:13.200 --> 00:50:15.800]   That might also gain you some stability
[00:50:15.800 --> 00:50:18.080]   and a couple of percentage of performance.
[00:50:18.080 --> 00:50:24.160]   Let's take a look at object tracking metrics
[00:50:24.160 --> 00:50:27.040]   for object tracking system.
[00:50:27.040 --> 00:50:32.040]   We have two parts, detection of our model detection,
[00:50:32.040 --> 00:50:37.040]   of our detection model, and also association
[00:50:37.040 --> 00:50:42.120]   of the new detections with the current objects
[00:50:42.120 --> 00:50:43.920]   that we trace.
[00:50:44.680 --> 00:50:48.680]   And we need some kind of metrics that will be able
[00:50:48.680 --> 00:50:52.840]   to distinguish errors from these two stages
[00:50:52.840 --> 00:50:55.760]   of our processing pipeline.
[00:50:55.760 --> 00:51:00.760]   And you will see that it's not a simple task.
[00:51:00.760 --> 00:51:05.840]   In order to understand what kind of metrics
[00:51:05.840 --> 00:51:09.920]   people used throughout the last 20 years
[00:51:09.920 --> 00:51:14.920]   of object detection and object tracking evolution,
[00:51:14.920 --> 00:51:19.120]   you can take a look at these two websites.
[00:51:19.120 --> 00:51:23.440]   First one is multiple object tracking benchmark.
[00:51:23.440 --> 00:51:26.840]   And the second one is a recent challenge
[00:51:26.840 --> 00:51:30.480]   from CVPR of this year about object tracking
[00:51:30.480 --> 00:51:31.640]   and object detection.
[00:51:31.640 --> 00:51:38.000]   So when you see what kind of metrics people used
[00:51:38.000 --> 00:51:42.160]   throughout the last 20 years,
[00:51:42.160 --> 00:51:48.560]   and you will see the papers, the recent papers
[00:51:48.560 --> 00:51:52.360]   that describe new approach to the metrics,
[00:51:52.360 --> 00:51:59.120]   you will observe that if you analyze
[00:51:59.120 --> 00:52:05.720]   more rigorously what is produced by those metrics,
[00:52:07.000 --> 00:52:09.760]   you will have a lot of mathematical problems
[00:52:09.760 --> 00:52:14.560]   with initial metrics proposed like a couple of years ago.
[00:52:14.560 --> 00:52:19.560]   For example, a motor metric has such problems.
[00:52:19.560 --> 00:52:25.920]   It puts bigger weight on detection errors
[00:52:25.920 --> 00:52:28.160]   rather than association errors.
[00:52:28.160 --> 00:52:32.480]   And when you have a metric that is not symmetric
[00:52:32.480 --> 00:52:35.080]   or you don't have a good breakdown
[00:52:35.080 --> 00:52:37.080]   between detection and association,
[00:52:37.080 --> 00:52:39.960]   what you really have is the problems
[00:52:39.960 --> 00:52:44.080]   that you really don't know which part of pipeline
[00:52:44.080 --> 00:52:45.720]   cause you some problems.
[00:52:45.720 --> 00:52:48.320]   And you need to develop some additional tools
[00:52:48.320 --> 00:52:52.000]   and maybe even trace detections and associations
[00:52:52.000 --> 00:52:54.200]   frame by frame in order to understand
[00:52:54.200 --> 00:52:58.840]   which part of your pipeline you need to improve.
[00:52:58.840 --> 00:53:01.800]   Also, we don't go through all the details,
[00:53:01.800 --> 00:53:04.360]   but if you look at the fifth problem,
[00:53:04.360 --> 00:53:07.520]   it turns out that for many years,
[00:53:07.520 --> 00:53:12.200]   people used as a metric that was FPS dependent.
[00:53:12.200 --> 00:53:16.560]   So the algorithm could produce the same detection
[00:53:16.560 --> 00:53:19.800]   and tracking results, but if you analyze video
[00:53:19.800 --> 00:53:23.880]   at five frames per second,
[00:53:23.880 --> 00:53:26.640]   and another algorithm analyzed video
[00:53:26.640 --> 00:53:28.680]   at 20 frames per second,
[00:53:28.680 --> 00:53:32.360]   the metric will produce different numbers.
[00:53:32.360 --> 00:53:36.160]   And this is usually is not a good feature
[00:53:36.160 --> 00:53:37.920]   that we want from our metric
[00:53:37.920 --> 00:53:40.880]   because we want to be able to play
[00:53:40.880 --> 00:53:44.680]   these number of frames that we analyze per second.
[00:53:44.680 --> 00:53:48.960]   If we have some constraints on our model or on our hardware.
[00:53:48.960 --> 00:53:57.120]   Next, we had some metrics that, for example, IDF1,
[00:53:57.120 --> 00:54:00.040]   that was a little bit difficult to interpret.
[00:54:00.040 --> 00:54:03.000]   And one of the goal of this metric
[00:54:03.000 --> 00:54:07.200]   was to give more weight to association errors.
[00:54:07.200 --> 00:54:12.360]   And this metric is also used in many benchmarks,
[00:54:12.360 --> 00:54:15.280]   research benchmarks, research papers,
[00:54:15.280 --> 00:54:20.280]   and in industry as well to understand
[00:54:20.280 --> 00:54:23.960]   if your algorithms that you develop is good enough
[00:54:23.960 --> 00:54:26.920]   in comparison to other models produced
[00:54:26.920 --> 00:54:28.520]   by research community.
[00:54:29.480 --> 00:54:33.360]   Also, there are some special metrics as TRAC MAP,
[00:54:33.360 --> 00:54:36.040]   which is also difficult to interpret
[00:54:36.040 --> 00:54:38.680]   and had some additional requirements
[00:54:38.680 --> 00:54:42.760]   about assigning confidence to the whole trajectory,
[00:54:42.760 --> 00:54:45.280]   which might be a little bit complicated
[00:54:45.280 --> 00:54:49.960]   for many basic algorithms
[00:54:49.960 --> 00:54:53.240]   that are quite useful in production.
[00:54:53.240 --> 00:54:57.280]   So the fourth and the fifth metrics are quite similar.
[00:54:58.680 --> 00:55:02.440]   HOTA metric is higher order tracking accuracy.
[00:55:02.440 --> 00:55:09.320]   It's in its original paper from 2020.
[00:55:09.320 --> 00:55:14.520]   They described all the problems with the previous metrics
[00:55:14.520 --> 00:55:17.680]   and tried to introduce the metrics
[00:55:17.680 --> 00:55:21.680]   that hasn't those problems,
[00:55:21.680 --> 00:55:25.520]   that assigns equivalent weight to detection
[00:55:25.520 --> 00:55:27.240]   and to association errors.
[00:55:28.600 --> 00:55:31.960]   And also has some additional features,
[00:55:31.960 --> 00:55:34.400]   mathematical features that are quite useful
[00:55:34.400 --> 00:55:38.600]   in analysis of the performance of our pipeline.
[00:55:38.600 --> 00:55:42.920]   And in CVPR challenge of this year,
[00:55:42.920 --> 00:55:45.040]   they introduced slight modification
[00:55:45.040 --> 00:55:49.840]   for TRAC everything, tracking accuracy metric,
[00:55:49.840 --> 00:55:57.080]   to take into account the difference
[00:55:58.120 --> 00:56:02.080]   between detection and association metrics
[00:56:02.080 --> 00:56:05.720]   for their particular benchmark.
[00:56:05.720 --> 00:56:12.960]   So you see from this small history
[00:56:12.960 --> 00:56:16.200]   that it's very challenging to understand
[00:56:16.200 --> 00:56:19.480]   what kind of tracking metrics could be useful
[00:56:19.480 --> 00:56:20.960]   to improving our models.
[00:56:20.960 --> 00:56:26.360]   I advise to use HOTA metrics,
[00:56:26.360 --> 00:56:31.080]   but it's better to use scripts
[00:56:31.080 --> 00:56:34.720]   provided by multiple object tracking challenge
[00:56:34.720 --> 00:56:36.480]   to compute different metrics
[00:56:36.480 --> 00:56:38.560]   and log them with weights and biases.
[00:56:38.560 --> 00:56:42.880]   Of course, you will take decisions based on the HOTA metric
[00:56:42.880 --> 00:56:45.120]   because it's easier to interpret
[00:56:45.120 --> 00:56:48.240]   and it's easier to understand where to find,
[00:56:48.240 --> 00:56:50.680]   where to search some problems
[00:56:50.680 --> 00:56:53.920]   in your object detection and tracking algorithm.
[00:56:53.920 --> 00:56:56.920]   But sometimes other metrics could give you
[00:56:56.920 --> 00:57:00.160]   additional information that could help you
[00:57:00.160 --> 00:57:02.400]   debug your machine learning models.
[00:57:02.400 --> 00:57:10.800]   So we have seen a lot of metrics
[00:57:10.800 --> 00:57:15.160]   and some of them are better for one task,
[00:57:15.160 --> 00:57:17.240]   some of them are better for other task.
[00:57:17.240 --> 00:57:21.200]   No matter what type of problem you have,
[00:57:21.200 --> 00:57:24.880]   no matter what at which stage of model improvements
[00:57:24.880 --> 00:57:26.720]   you are right now,
[00:57:26.720 --> 00:57:29.720]   there are two golden rules for metric choice.
[00:57:29.720 --> 00:57:36.080]   The metrics that you use for decision-making
[00:57:36.080 --> 00:57:38.840]   should be as close as possible
[00:57:38.840 --> 00:57:41.560]   or should have a really good correlation
[00:57:41.560 --> 00:57:45.280]   with business metrics that you promise to your clients.
[00:57:45.280 --> 00:57:47.880]   That will help you a lot.
[00:57:47.880 --> 00:57:51.040]   That will make your product more robust and more stable.
[00:57:51.040 --> 00:57:52.440]   And your clients more happy.
[00:57:52.440 --> 00:57:59.320]   A second rule is to choose one metric for decision-making.
[00:57:59.320 --> 00:58:03.400]   If you need to respect some service level agreements
[00:58:03.400 --> 00:58:06.520]   or some level of performance for your models,
[00:58:06.520 --> 00:58:11.440]   you can put a hard threshold on other metrics.
[00:58:11.440 --> 00:58:14.920]   But use only one metric to make decisions
[00:58:14.920 --> 00:58:17.800]   about which model to push into production
[00:58:17.800 --> 00:58:20.080]   and to understand which model is better.
[00:58:21.080 --> 00:58:23.800]   (mouse clicking)
[00:58:23.800 --> 00:58:26.400]   And to conclude this presentation,
[00:58:26.400 --> 00:58:29.440]   we have a couple of take-home ideas.
[00:58:29.440 --> 00:58:32.560]   So first deployment is only the beginning of your journey
[00:58:32.560 --> 00:58:36.760]   and you have a lot of learning to do
[00:58:36.760 --> 00:58:39.920]   and a lot of experiments to do after the first deployment
[00:58:39.920 --> 00:58:44.920]   in order to get a really solid model
[00:58:44.920 --> 00:58:48.560]   that generalize well on your production environment.
[00:58:48.560 --> 00:58:52.040]   When you are going to the high precision
[00:58:52.040 --> 00:58:55.000]   in your object detection and object tracking model,
[00:58:55.000 --> 00:58:58.880]   when you try to get this last percentage of performance,
[00:58:58.880 --> 00:59:02.040]   it's very important to go into more nuanced metrics
[00:59:02.040 --> 00:59:06.160]   to see the metrics by object size,
[00:59:06.160 --> 00:59:08.200]   by different data sets,
[00:59:08.200 --> 00:59:14.160]   and by different metadata of the images that you analyze.
[00:59:15.560 --> 00:59:18.120]   And the last one is you should concentrate
[00:59:18.120 --> 00:59:19.720]   on a couple of metrics first,
[00:59:19.720 --> 00:59:24.560]   and then you can broaden the scope
[00:59:24.560 --> 00:59:26.440]   if you see no difference,
[00:59:26.440 --> 00:59:29.400]   no improvements in the score metrics.
[00:59:29.400 --> 00:59:36.760]   Yes, we can start with the Q&A session.
[00:59:36.760 --> 00:59:39.040]   - Thank you so much, Anton.
[00:59:39.040 --> 00:59:41.720]   That was such a comprehensive presentation.
[00:59:41.720 --> 00:59:43.560]   We have a bunch of questions,
[00:59:44.400 --> 00:59:47.360]   but the time allows probably for a couple of quick ones.
[00:59:47.360 --> 00:59:49.080]   So let me choose.
[00:59:49.080 --> 00:59:52.760]   What strategies do you implement
[00:59:52.760 --> 00:59:54.680]   to handle large volume of data,
[00:59:54.680 --> 00:59:57.760]   particularly for real-time video analytics?
[00:59:57.760 --> 01:00:03.040]   - So we have two approaches.
[01:00:03.040 --> 01:00:07.680]   First one is if you use AIoT devices,
[01:00:07.680 --> 01:00:12.360]   you can use this device for every camera.
[01:00:13.200 --> 01:00:16.280]   So it will scale with the number of devices
[01:00:16.280 --> 01:00:17.200]   that you're using.
[01:00:17.200 --> 01:00:20.880]   So usually there is no problem except for the cost,
[01:00:20.880 --> 01:00:24.080]   but it's cheaper than to send all the images
[01:00:24.080 --> 01:00:26.320]   to centralized servers.
[01:00:26.320 --> 01:00:32.520]   If you are trying to use inference on GPU,
[01:00:32.520 --> 01:00:34.480]   on server GPUs,
[01:00:34.480 --> 01:00:38.080]   you might want to consider specialized inference servers,
[01:00:38.080 --> 01:00:41.080]   such as Triton inference server from NVIDIA,
[01:00:41.080 --> 01:00:46.080]   which handles the load balancing between different models
[01:00:46.080 --> 01:00:49.280]   and provides better performance.
[01:00:49.280 --> 01:00:53.080]   - Amazing, thanks so much, Anton.
[01:00:53.080 --> 01:00:56.960]   Yeah, we have a minute for one more.
[01:00:56.960 --> 01:01:01.200]   You mentioned in the myths that you presented
[01:01:01.200 --> 01:01:04.680]   in the beginning that you deploy more than one or two models.
[01:01:04.680 --> 01:01:07.040]   Do you know maybe roughly how many models
[01:01:07.040 --> 01:01:09.640]   are maintained right now in production in your team?
[01:01:10.160 --> 01:01:12.960]   - It's a dozen of models.
[01:01:12.960 --> 01:01:16.280]   - Cool, thank you.
[01:01:16.280 --> 01:01:18.560]   - Then in my previous experience,
[01:01:18.560 --> 01:01:21.800]   it was still up to dozens,
[01:01:21.800 --> 01:01:25.800]   but we try to sometimes when we make fine tuning,
[01:01:25.800 --> 01:01:30.720]   we try to keep the number of models moderate
[01:01:30.720 --> 01:01:35.680]   and we observed as when we produced,
[01:01:35.680 --> 01:01:38.240]   we collect new data and we find out
[01:01:38.240 --> 01:01:41.440]   that we get new data and we make like a general fine tuning
[01:01:41.440 --> 01:01:43.720]   to get the new generalizable model.
[01:01:43.720 --> 01:01:47.680]   We can push as this model to replace
[01:01:47.680 --> 01:01:49.160]   as a fine tuned models to keep
[01:01:49.160 --> 01:01:52.080]   the number of models manageable.
[01:01:52.080 --> 01:01:56.600]   - So it is better to have one bigger model
[01:01:56.600 --> 01:01:59.400]   to manage multiple different needs
[01:01:59.400 --> 01:02:01.920]   rather than have a few smaller ones, sorry.
[01:02:01.920 --> 01:02:06.600]   - Yes, it's quite simpler and it's easier to manage
[01:02:06.600 --> 01:02:09.560]   and you can better understand the performance.
[01:02:09.560 --> 01:02:11.240]   You can spend more time with this model
[01:02:11.240 --> 01:02:15.080]   to understand the limitations of this model.
[01:02:15.080 --> 01:02:18.200]   But if you don't have any choice,
[01:02:18.200 --> 01:02:19.400]   please fine tune your model
[01:02:19.400 --> 01:02:21.280]   and push this model to production,
[01:02:21.280 --> 01:02:25.800]   but you need good tools to observe all the models.
[01:02:25.800 --> 01:02:27.720]   - That's great.
[01:02:27.720 --> 01:02:30.720]   Yeah, sorry, go on.
[01:02:30.720 --> 01:02:32.760]   - Yes, and keep track of them.
[01:02:35.360 --> 01:02:37.920]   - Cool, all right, one quick question.
[01:02:37.920 --> 01:02:41.440]   What other tools my iDot except for Weights and Biases
[01:02:41.440 --> 01:02:42.960]   that you rely on every day?
[01:02:42.960 --> 01:02:52.080]   - So there are a lot of tools for annotation.
[01:02:52.080 --> 01:02:55.560]   Sometimes with the current development
[01:02:55.560 --> 01:02:58.960]   of computer vision tools, especially for object detection,
[01:02:58.960 --> 01:03:01.080]   you don't need a lot of images
[01:03:01.080 --> 01:03:04.000]   to be able to get first results
[01:03:04.000 --> 01:03:07.880]   that will provide some substantial value to our clients.
[01:03:07.880 --> 01:03:12.120]   Very often it will be around 500,
[01:03:12.120 --> 01:03:15.040]   maybe a couple of thousand images.
[01:03:15.040 --> 01:03:17.280]   And you can label those images
[01:03:17.280 --> 01:03:20.720]   in a couple of days with your team,
[01:03:20.720 --> 01:03:24.080]   even without spending time to explain this task
[01:03:24.080 --> 01:03:25.680]   to some outsourced people.
[01:03:25.680 --> 01:03:32.240]   And there are good tools, CDAT, Label Studio,
[01:03:33.520 --> 01:03:36.160]   which has a quite interesting interface
[01:03:36.160 --> 01:03:37.680]   and very easy to integrate.
[01:03:37.680 --> 01:03:42.600]   Also, there is another tool for managing datasets
[01:03:42.600 --> 01:03:44.040]   that's called Active Loop,
[01:03:44.040 --> 01:03:48.920]   which is quite promising to visualize the data
[01:03:48.920 --> 01:03:53.640]   and to provide data loaders directly from your database
[01:03:53.640 --> 01:03:55.360]   to your training pipelines.
[01:03:55.360 --> 01:03:59.320]   - Amazing, thank you so much, Anton.
[01:03:59.320 --> 01:04:00.520]   We have a couple more questions,
[01:04:00.520 --> 01:04:02.680]   but as we're over time,
[01:04:02.680 --> 01:04:05.240]   if you guys wanna reach out to Anton on LinkedIn,
[01:04:05.240 --> 01:04:07.120]   he shared his socials.
[01:04:07.120 --> 01:04:09.400]   We will be following up with an email
[01:04:09.400 --> 01:04:11.040]   to everyone who has registered
[01:04:11.040 --> 01:04:13.760]   where we will put the links to the slides.
[01:04:13.760 --> 01:04:16.880]   We will give you a link to the recording for it.
[01:04:16.880 --> 01:04:18.560]   So thank you so much for Anton.
[01:04:18.560 --> 01:04:20.280]   Thank you so much, Sumiks, for joining in
[01:04:20.280 --> 01:04:22.680]   and sharing your time with us and your knowledge.
[01:04:22.680 --> 01:04:25.720]   Funny enough, Anton, that you mentioned our courses
[01:04:25.720 --> 01:04:26.920]   that you're on.
[01:04:26.920 --> 01:04:28.520]   For the fans of computer vision,
[01:04:28.520 --> 01:04:30.280]   please do watch our courses space
[01:04:30.280 --> 01:04:32.560]   as there's gonna be something coming up soon.
[01:04:33.200 --> 01:04:35.400]   Once again, thanks so much everyone for your time.
[01:04:35.400 --> 01:04:37.480]   I hope you have enjoyed our event
[01:04:37.480 --> 01:04:39.800]   and yeah, feel free to subscribe to Weights & Biases
[01:04:39.800 --> 01:04:42.640]   and SmartCal to see more of our events
[01:04:42.640 --> 01:04:44.040]   coming up in the future.
[01:04:44.040 --> 01:04:46.080]   Thanks everyone, have a great day.
[01:04:46.080 --> 01:04:47.400]   - Thanks, bye bye.
[01:04:47.400 --> 01:04:50.000]   (upbeat music)
[01:04:50.000 --> 01:04:52.600]   (upbeat music)
[01:04:52.600 --> 01:04:55.180]   (upbeat music)
[01:04:55.180 --> 01:04:56.900]   (upbeat music)

