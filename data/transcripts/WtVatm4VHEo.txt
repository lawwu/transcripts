
[00:00:00.000 --> 00:00:04.000]   on YouTube and wait for an echo.
[00:00:04.000 --> 00:00:15.800]   Awesome.
[00:00:15.800 --> 00:00:17.560]   I hear the echo, which means we're live.
[00:00:17.560 --> 00:00:19.440]   There's always a small delay that happens.
[00:00:19.440 --> 00:00:21.020]   Welcome back everyone.
[00:00:21.020 --> 00:00:24.760]   Uh, this will be the last study group call.
[00:00:24.760 --> 00:00:29.480]   Uh, so we have been meeting for, I think this is a 10th week running
[00:00:29.520 --> 00:00:35.360]   and I am excited to also wrap this up, but also, uh, this has been quite fun.
[00:00:35.360 --> 00:00:40.280]   Uh, I'll, I'll save the, save the speech for the later end, but, um, thanks
[00:00:40.280 --> 00:00:42.680]   for joining for the last study group as well, let me share my
[00:00:42.680 --> 00:00:44.000]   screen and we can get started.
[00:00:44.000 --> 00:00:54.280]   Also, I'm working with the new setup, which means you'll see me struggle
[00:00:54.280 --> 00:00:56.600]   more than, uh, the usual amount.
[00:00:56.600 --> 00:00:57.540]   I'm sorry about that.
[00:00:59.280 --> 00:01:04.320]   So, uh, in this week, uh, in this session, we'll essentially
[00:01:04.320 --> 00:01:06.120]   bring all of the pieces together.
[00:01:06.120 --> 00:01:09.400]   We've been working on all of these different parts of creating a data set,
[00:01:09.400 --> 00:01:11.760]   creating a data, creating data loaders.
[00:01:11.760 --> 00:01:14.920]   We learned how to create a classification models.
[00:01:14.920 --> 00:01:16.160]   We learned about resonates.
[00:01:16.160 --> 00:01:18.440]   Uh, we learned about segmentation last week.
[00:01:18.440 --> 00:01:23.440]   Uh, we learned about units and this is the part where we bring
[00:01:23.440 --> 00:01:24.960]   all of those things together.
[00:01:24.960 --> 00:01:27.920]   So to remind you, I'm sorry, let me take my headphones off.
[00:01:28.760 --> 00:01:35.120]   Oh, so to remind you, we've been working on the Luna 16 data set, which was a
[00:01:35.120 --> 00:01:38.120]   part of the data science bowl competition.
[00:01:38.120 --> 00:01:42.000]   Uh, it was independent of the competition, but it was also involved on Kaggle.
[00:01:42.000 --> 00:01:46.960]   And we have been slowly stepping towards this end to end pipeline.
[00:01:46.960 --> 00:01:49.040]   Now that, uh, we'll get to see it.
[00:01:49.040 --> 00:01:53.200]   And we'll also look at a few options about how can we deploy this to production?
[00:01:53.200 --> 00:01:55.480]   A quick spoiler about the second end.
[00:01:55.480 --> 00:01:56.880]   It won't be much detail.
[00:01:56.880 --> 00:01:58.400]   It's quite short in the book as well.
[00:01:58.400 --> 00:02:00.880]   And I'll, I'll try to keep it quite concise as well.
[00:02:00.880 --> 00:02:07.120]   Um, deploying to production for any model literally means sending
[00:02:07.120 --> 00:02:08.400]   your model to customers.
[00:02:08.400 --> 00:02:12.960]   And that highly, highly varies depending on business problems, depending on the
[00:02:12.960 --> 00:02:19.640]   model, uh, depending on the constraints, budget time team, uh, business
[00:02:19.640 --> 00:02:21.280]   constraints that exist.
[00:02:21.280 --> 00:02:26.000]   So it's quite handcrafted still at the moment, even though there are quite
[00:02:26.040 --> 00:02:31.160]   out of the shop options as well that you can use, um, still I'll try to keep
[00:02:31.160 --> 00:02:32.840]   it short just because it varies so much.
[00:02:32.840 --> 00:02:39.520]   We started last week about unit image segmentation and how to deeper
[00:02:39.520 --> 00:02:41.120]   dive into image augmentations.
[00:02:41.120 --> 00:02:43.240]   I'll start by recapping, recapping that.
[00:02:43.240 --> 00:02:45.000]   And here was a suggested homework.
[00:02:45.000 --> 00:02:49.280]   We had emailed it, which in biases report to you that contain more details about
[00:02:49.280 --> 00:02:52.280]   the image segmentation approaches and units.
[00:02:52.400 --> 00:02:57.480]   Uh, it, the report also had a fast unit learner and showed you how to make that
[00:02:57.480 --> 00:03:01.560]   work with weights and biases and how to make it work on a driving data set,
[00:03:01.560 --> 00:03:04.280]   self-driving car, uh, data set.
[00:03:04.280 --> 00:03:08.760]   If I remember correctly, we learned how to implement units last week.
[00:03:08.760 --> 00:03:12.720]   Uh, and we, and the suggestion was to try a newer segmentation approach.
[00:03:12.720 --> 00:03:15.960]   Sadly this week, I didn't see anyone turn in any homework.
[00:03:15.960 --> 00:03:20.920]   Uh, so I wish I can see some of the things come in the following week.
[00:03:20.960 --> 00:03:22.920]   Let me remind you of this link.
[00:03:22.920 --> 00:03:28.200]   So I'm going to post this in the zoom chart.
[00:03:28.200 --> 00:03:42.520]   If you head over to this link, it should take you to our forums in a minute.
[00:03:42.520 --> 00:03:47.760]   And, uh, we use discourse for all of the questions since this event
[00:03:47.760 --> 00:03:49.800]   is hosted on zoom plus YouTube.
[00:03:50.160 --> 00:03:52.520]   It's quite difficult for me to monitor both of them.
[00:03:52.520 --> 00:03:53.120]   I'm sorry.
[00:03:53.120 --> 00:03:56.760]   I've been reminding everyone about this for 10 weeks, but every week we have a
[00:03:56.760 --> 00:04:00.760]   few new people join in and I don't want them to be unhappy by the fact that
[00:04:00.760 --> 00:04:02.320]   I'm not answering any questions.
[00:04:02.320 --> 00:04:06.080]   So to ask any questions, if you head over to this link, you can simply.
[00:04:06.080 --> 00:04:09.360]   Right about them here.
[00:04:09.360 --> 00:04:09.960]   Uh,
[00:04:19.040 --> 00:04:20.800]   and, uh, that's how you can ask questions.
[00:04:20.800 --> 00:04:22.680]   Hopefully they'll be better than this one.
[00:04:22.680 --> 00:04:29.880]   Let me also put this link, uh, in the Pytorch, sorry, in the
[00:04:29.880 --> 00:04:31.240]   Pytorch stream on YouTube.
[00:04:31.240 --> 00:04:39.760]   Let's ask questions here and let me pin that comment as you can see it's live.
[00:04:39.760 --> 00:04:43.880]   So that means there's always this initial period of struggle that I go through where
[00:04:43.880 --> 00:04:49.720]   I have to make everyone wait while I painfully try to figure out how to do
[00:04:49.720 --> 00:04:51.840]   very basic stuff, sorry about that.
[00:04:51.840 --> 00:04:54.880]   Awesome.
[00:04:54.880 --> 00:04:56.080]   So I've posted the link.
[00:04:56.080 --> 00:04:57.400]   Let me pin that.
[00:04:57.400 --> 00:05:00.280]   And I think we should be good.
[00:05:00.280 --> 00:05:04.760]   So as a reminder, please ask all questions here.
[00:05:09.920 --> 00:05:15.080]   This week, as I mentioned, we'll be learning about how to put all of these
[00:05:15.080 --> 00:05:20.200]   pieces together and maybe a bit about how can we deploy all of these models
[00:05:20.200 --> 00:05:21.920]   that we've been looking at here's the agenda.
[00:05:21.920 --> 00:05:26.600]   So I'll try to like always recap what we studied last week, understand the pipeline,
[00:05:26.600 --> 00:05:33.000]   glance through chapter 14 to 15, and then, uh, explain the interesting
[00:05:33.000 --> 00:05:35.680]   bits at least from the code.
[00:05:35.680 --> 00:05:39.320]   Now we're at the point where it's 800 plus lines.
[00:05:39.320 --> 00:05:42.840]   I think thousand plus lines, which doesn't mean anything by itself, but
[00:05:42.840 --> 00:05:46.880]   means for me to explain all of those things to you would be very painful
[00:05:46.880 --> 00:05:50.560]   for you to watch and would be very slow for me to explain.
[00:05:50.560 --> 00:05:53.320]   So that means I'll only cover the interesting bits at this point.
[00:05:53.320 --> 00:05:59.400]   I should remove that.
[00:05:59.400 --> 00:06:02.840]   Um, so here's what we started last week.
[00:06:02.840 --> 00:06:04.840]   We looked at image segmentation.
[00:06:04.840 --> 00:06:08.880]   Uh, why do we even need that sort of problem?
[00:06:08.920 --> 00:06:10.360]   Where is it even useful?
[00:06:10.360 --> 00:06:16.560]   We looked at units and a deeper dive into image augmentation for which if
[00:06:16.560 --> 00:06:19.680]   you signed up for the group, we would have emailed you certain resources
[00:06:19.680 --> 00:06:21.160]   as well for you to take a look at.
[00:06:21.160 --> 00:06:28.160]   So image segmentation is an approach where we literally try
[00:06:28.160 --> 00:06:30.440]   to label every single pixel.
[00:06:30.440 --> 00:06:35.160]   It's more about understanding an entire scene, understanding an entire
[00:06:35.160 --> 00:06:37.640]   image and what's going on there.
[00:06:37.680 --> 00:06:43.160]   Uh, there are all sorts of possible use cases where this is useful than just,
[00:06:43.160 --> 00:06:45.880]   um, image classification.
[00:06:45.880 --> 00:06:48.240]   So just object classification or object detection.
[00:06:48.240 --> 00:06:50.880]   And we spoke about all of those things.
[00:06:50.880 --> 00:06:53.600]   So if you're new here, you can watch the recording on our channel.
[00:06:53.600 --> 00:06:54.720]   I'm not going to go through it.
[00:06:54.720 --> 00:06:56.920]   We learned about units.
[00:06:56.920 --> 00:06:59.760]   Uh, and since I've read the paper, I'll also be hosting a
[00:06:59.760 --> 00:07:01.160]   paper reading group around this.
[00:07:01.160 --> 00:07:04.840]   But this is what the architecture looks like.
[00:07:05.240 --> 00:07:10.360]   Uh, the unit names come obviously from this U-shaped architecture.
[00:07:10.360 --> 00:07:14.080]   And these were one of the first papers.
[00:07:14.080 --> 00:07:18.640]   In fact, one of the first architectures to even include the skip connections.
[00:07:18.640 --> 00:07:20.760]   And this was even before ResNets.
[00:07:20.760 --> 00:07:25.800]   Uh, if I remember correctly, and this is based on what's mentioned in the book.
[00:07:25.800 --> 00:07:32.080]   So to summarize again, I think these are quite important.
[00:07:32.080 --> 00:07:33.680]   So I'll quickly go over these topics.
[00:07:33.680 --> 00:07:34.800]   It's a U-shaped network.
[00:07:34.800 --> 00:07:36.360]   There's an encoder decoder.
[00:07:36.360 --> 00:07:38.360]   The left side acts in an encoder.
[00:07:38.360 --> 00:07:40.360]   The right side acts as an decoder.
[00:07:40.360 --> 00:07:44.600]   The problem that we were trying to understand actually requires us
[00:07:44.600 --> 00:07:46.880]   to label every single pixel, right?
[00:07:46.880 --> 00:07:52.720]   The challenge here with any simple CNN becomes if you're just downsizing,
[00:07:52.720 --> 00:07:57.160]   there's no one-on-one mapping with the input and output for that reason.
[00:07:57.160 --> 00:08:00.240]   The U-shaped architectures were quite interesting.
[00:08:00.240 --> 00:08:01.800]   Sorry.
[00:08:03.320 --> 00:08:06.440]   But making these work was quite challenging as well.
[00:08:06.440 --> 00:08:10.280]   So the key trick here was the skip connections, the lines you see as
[00:08:10.280 --> 00:08:16.520]   gray arrow, the things that connect the input to the output or whatever's
[00:08:16.520 --> 00:08:23.040]   going on in these layers to the corresponding output layer, and these
[00:08:23.040 --> 00:08:27.240]   had the secret sauce before ResNets, the skip connection, and they
[00:08:27.240 --> 00:08:29.080]   also incorporated a weighted loss.
[00:08:31.080 --> 00:08:34.280]   So I'll ask this question and I'll wait for your replies like always.
[00:08:34.280 --> 00:08:37.560]   What will an end-to-end pipeline look like at this point?
[00:08:37.560 --> 00:08:42.120]   So let me hop over to the forums and the question that I'm asking everyone
[00:08:42.120 --> 00:08:49.320]   to write an answer to on the forums is what will an end-to-end pipeline look like?
[00:08:49.320 --> 00:09:09.880]   When I say end-to-end pipeline, I'm talking about all of the steps that
[00:09:09.880 --> 00:09:13.200]   we've been building so far, all of the things we've understood.
[00:09:13.200 --> 00:09:18.720]   How do you envision all of them coming together and where should we
[00:09:18.720 --> 00:09:20.440]   connect all of these pieces?
[00:09:20.440 --> 00:09:27.280]   So the pieces to remind you is the dataset class, the classifier,
[00:09:27.280 --> 00:09:29.760]   unit segmentation model.
[00:09:29.760 --> 00:09:33.160]   I'm trying to think if I missed anything.
[00:09:33.160 --> 00:09:34.120]   I think that's about it.
[00:09:34.120 --> 00:09:41.400]   So I'll give Mateo a minute to answer and anyone else that might
[00:09:41.400 --> 00:09:42.840]   want to jump in and answer anything.
[00:09:42.840 --> 00:09:59.440]   Oh, Ramesh is here as well.
[00:09:59.440 --> 00:10:00.320]   Good to see you Ramesh.
[00:10:00.320 --> 00:10:16.920]   It's funny with the forums, you start to recognize everyone by
[00:10:16.920 --> 00:10:19.080]   these little icons.
[00:10:19.080 --> 00:10:23.240]   I've met people from the fastA forums who I just knew as those little icons
[00:10:23.240 --> 00:10:25.360]   and I didn't recognize them in real life.
[00:10:25.360 --> 00:10:29.120]   It's always incredible how these work.
[00:10:30.000 --> 00:10:34.400]   This I think is a good enough answer.
[00:10:34.400 --> 00:10:38.880]   Prepare the data, extract the nodules and find the suspicious nodules.
[00:10:38.880 --> 00:10:42.880]   I'll wait for Ramesh to answer as well.
[00:10:42.880 --> 00:10:56.320]   In the meantime, I just want us to appreciate the problem again, because
[00:10:56.320 --> 00:11:00.880]   it's such an interesting one, especially with tumor classification.
[00:11:00.880 --> 00:11:06.120]   Even though the dataset imbalance is much smaller than this, generally
[00:11:06.120 --> 00:11:08.960]   with any tumor classifier, sorry.
[00:11:08.960 --> 00:11:14.920]   To appreciate this problem when we were working, especially with this
[00:11:14.920 --> 00:11:18.960]   dataset, when you're trying to scan and work with all of these voxels,
[00:11:18.960 --> 00:11:23.360]   here's what the final pipeline looks like.
[00:11:23.360 --> 00:11:28.200]   So you have two to the power 25 voxels out of which you segment out two to
[00:11:28.200 --> 00:11:34.360]   the power 20 and you try to select the candidates from which you classify
[00:11:34.360 --> 00:11:39.240]   the nodules from where you further classify if one's malignant or not.
[00:11:39.240 --> 00:11:46.400]   And you get the final output, which are about two of these two to the power 25.
[00:11:46.400 --> 00:11:50.160]   So it's literally like finding a needle in a haystack.
[00:11:50.200 --> 00:11:53.600]   That's why it's such an interesting and such a challenging problem.
[00:11:53.600 --> 00:11:59.920]   This was quite difficult in 2017 when the Kaggle competition launched and
[00:11:59.920 --> 00:12:06.520]   still is quite challenging, if not one of the most difficult problems.
[00:12:06.520 --> 00:12:10.600]   So let's hop back to the forums and then I can continue.
[00:12:10.600 --> 00:12:18.600]   Webof says preparing the dataset image segmentation, nodule
[00:12:18.600 --> 00:12:22.320]   determination, sorry, and malignancy.
[00:12:22.320 --> 00:12:23.400]   That's great.
[00:12:23.400 --> 00:12:27.280]   I'll come back to Ramesh's answer and meanwhile continue glancing through.
[00:12:27.280 --> 00:12:31.080]   I see his answer.
[00:12:31.080 --> 00:12:32.920]   So let me go there.
[00:12:32.920 --> 00:12:42.680]   So you, I am, it's always tricky if you should do the EDF first or just start
[00:12:42.680 --> 00:12:47.640]   modeling, but I think I'm, I'm of the camp of EDF first.
[00:12:47.960 --> 00:12:52.040]   So one thing we've not learned in this book that Ramesh has pointed out,
[00:12:52.040 --> 00:12:53.480]   you'd want to perform EDA.
[00:12:53.480 --> 00:13:00.000]   You would want to evaluate after performing the training.
[00:13:00.000 --> 00:13:04.760]   I think Ramesh is joining for the first time and just to add some more context
[00:13:04.760 --> 00:13:08.560]   for you, Ramesh, we've been looking at different parts of this complete
[00:13:08.560 --> 00:13:10.560]   pipeline that the authors had suggested.
[00:13:10.560 --> 00:13:14.920]   And my question was about how would these pieces come together?
[00:13:17.520 --> 00:13:23.680]   So the pieces that I keep mentioning are like, so we start with the
[00:13:23.680 --> 00:13:25.480]   nodule candidate generation.
[00:13:25.480 --> 00:13:28.920]   So we take the entire scan that we have.
[00:13:28.920 --> 00:13:34.160]   And from that, you'd want to segment and then group out individual voxels.
[00:13:34.160 --> 00:13:37.800]   With this dataset, you don't have tumors labeled.
[00:13:37.800 --> 00:13:42.760]   So when we say grouping, first of all, you'd want to segment any possible tumor.
[00:13:44.040 --> 00:13:46.800]   And now the problem is since this is a 3d space, right?
[00:13:46.800 --> 00:13:51.680]   And a nodule isn't a sphere, it's not a defined shape.
[00:13:51.680 --> 00:13:53.480]   It's not a cuboid.
[00:13:53.480 --> 00:13:58.720]   You'd have to group all of these together by sort of going through the search
[00:13:58.720 --> 00:14:02.040]   algorithm, if you're from a computer science background, if you're not from
[00:14:02.040 --> 00:14:04.960]   a computer science background, you'd have to look around every pixel.
[00:14:04.960 --> 00:14:07.000]   That's what a search algorithm does.
[00:14:07.000 --> 00:14:08.880]   It looks around in the neighborhood.
[00:14:10.680 --> 00:14:17.960]   So by doing that, you'd want to find the center of all of these
[00:14:17.960 --> 00:14:21.720]   and you classify these as groups.
[00:14:21.720 --> 00:14:22.440]   So, okay.
[00:14:22.440 --> 00:14:29.680]   This set of pixels, this set of voxels to be more precise is one tumor.
[00:14:29.680 --> 00:14:33.680]   These set of pixels in another neighborhood would be another one.
[00:14:33.680 --> 00:14:38.720]   So from there, we'd have a coordinate system in the IRC format.
[00:14:38.800 --> 00:14:41.600]   If you remember, there are two different ones, the patient
[00:14:41.600 --> 00:14:43.560]   coordinate system and another.
[00:14:43.560 --> 00:14:49.000]   And we'd want to have those four again, interchanging efficiently from there.
[00:14:49.000 --> 00:14:54.560]   Once we have the tumors classified, sorry, the tumors segmented.
[00:14:54.560 --> 00:14:57.120]   And now we know where all of the tumors are located.
[00:14:57.120 --> 00:15:01.880]   Remember not all tumors are cancerous.
[00:15:01.880 --> 00:15:05.520]   So first of all, from there, you'd want to classify these.
[00:15:06.240 --> 00:15:11.280]   And in this chapter, you get introduced to the ROC AUC matrix from where since
[00:15:11.280 --> 00:15:14.360]   you've been training all of these models in the past, the authors also
[00:15:14.360 --> 00:15:16.880]   introduced fine tuning the model.
[00:15:16.880 --> 00:15:20.600]   And that's what your end to end pipeline would look like.
[00:15:20.600 --> 00:15:24.120]   From there, you'd just have a final answer if it's malignant or not.
[00:15:24.120 --> 00:15:29.720]   Again, this from a high level looks very simple, but as we've painfully
[00:15:29.720 --> 00:15:32.960]   gone through understanding every single error that we've made, every
[00:15:32.960 --> 00:15:36.600]   single part that we've learned, it's quite a challenging problem.
[00:15:36.600 --> 00:15:40.320]   And now we're at this complete, completely big stage.
[00:15:40.320 --> 00:15:43.800]   Today also will be a short call I suspect because we'll just be
[00:15:43.800 --> 00:15:45.320]   bringing a few steps together.
[00:15:45.320 --> 00:15:50.120]   It'll just be more of a discussion of for loops, but every individual
[00:15:50.120 --> 00:15:51.840]   step has its own problems.
[00:15:51.840 --> 00:15:58.240]   And one thing I'd also like to point out here, the book doesn't discuss
[00:15:58.240 --> 00:16:02.720]   this in much depth, but there's this constant debate of, let me
[00:16:02.720 --> 00:16:05.480]   stop sharing my screen so I can emphasize on myself.
[00:16:05.480 --> 00:16:10.920]   There's this constant debate of, should you have an entire automated
[00:16:10.920 --> 00:16:13.200]   pipeline, should you have an assisted pipeline?
[00:16:13.200 --> 00:16:17.600]   As you can clearly see from our experience to me, it's quite obvious.
[00:16:17.600 --> 00:16:21.680]   I'm hoping it is to you as well, that we should have an assisted system
[00:16:21.680 --> 00:16:28.760]   where you get an output from a classifier that does the boring job for you, but
[00:16:28.760 --> 00:16:30.680]   you still get the doctor's opinion.
[00:16:30.680 --> 00:16:35.960]   And the reason being a data scientist like me would have made assumptions
[00:16:35.960 --> 00:16:38.040]   that you don't want the doctor to miss out.
[00:16:38.040 --> 00:16:42.400]   So maybe one evening I was quite tired and I decided to skip a set of
[00:16:42.400 --> 00:16:46.560]   augmentations because they were just giving a small boost and maybe they
[00:16:46.560 --> 00:16:52.240]   were really important on one stupid assumption that I might have made with
[00:16:52.240 --> 00:16:58.480]   creating this cluster or grouping this cluster as a tumor might be quite
[00:16:58.480 --> 00:17:02.480]   important because again, remember we're looking for a needle in a haystack,
[00:17:02.480 --> 00:17:04.000]   especially with such problems.
[00:17:04.000 --> 00:17:10.440]   First of all, it's a quite daunting task to find the correct tumors.
[00:17:10.440 --> 00:17:16.200]   And secondly, it's also a place where you have human lives at stake.
[00:17:16.200 --> 00:17:21.440]   So I want to spend a minute to talk about that, that it's quite important
[00:17:21.440 --> 00:17:26.640]   to have a human in the loop system and try and understand different methods
[00:17:26.640 --> 00:17:28.280]   of interpreting these models.
[00:17:28.880 --> 00:17:34.560]   You don't always have steps to interpret most of the deep learning models, but
[00:17:34.560 --> 00:17:39.840]   it's quite important for that reason to always investigate every single step.
[00:17:39.840 --> 00:17:45.240]   When we say, try to print outputs to see if your model is working, try to see
[00:17:45.240 --> 00:17:50.960]   the outputs of a model that is working as well to see if it's working as expected.
[00:17:50.960 --> 00:17:55.800]   That is a simple, simplest possible way of debugging anything that you could do.
[00:17:55.800 --> 00:18:06.280]   Oops, sorry.
[00:18:06.280 --> 00:18:10.520]   Sometimes zoom is a little annoying to me.
[00:18:10.520 --> 00:18:17.880]   So to see how this would look like and for anyone that's been joining the group,
[00:18:17.880 --> 00:18:23.480]   we've essentially created separate Python files for all of these tasks.
[00:18:23.480 --> 00:18:27.600]   Now we look at how to combine those now is the easy part.
[00:18:27.600 --> 00:18:31.960]   And to anyone that's new, we've learned all of these steps and you can find the
[00:18:31.960 --> 00:18:34.920]   older recordings, which I won't strongly recommend.
[00:18:34.920 --> 00:18:37.160]   I would rather recommend that you read the book because the
[00:18:37.160 --> 00:18:38.800]   book is a better representation.
[00:18:38.800 --> 00:18:44.680]   But the first step is to generate the nodule candidates and you do that by
[00:18:44.680 --> 00:18:48.680]   segmenting them from the group them and then construct the tuple.
[00:18:50.400 --> 00:18:52.120]   After that, you can classify them.
[00:18:52.120 --> 00:18:54.920]   And for that, we'll implement a classifier.
[00:18:54.920 --> 00:18:58.200]   I'll quickly go over ROC or AUC matrix.
[00:18:58.200 --> 00:19:02.360]   Those are introduced in the last chapter and we'll be fine tuning the
[00:19:02.360 --> 00:19:04.160]   models that we have already created.
[00:19:04.160 --> 00:19:06.680]   And that's how you bring all of these pieces together.
[00:19:06.680 --> 00:19:12.800]   One another thing that you should be careful of here is leakage.
[00:19:12.800 --> 00:19:17.800]   And when I say leakage, that's because in this older pipeline, we've created
[00:19:17.800 --> 00:19:23.960]   separate dataset classes for segmentation and for the classifier, which means our
[00:19:23.960 --> 00:19:27.800]   assumptions for creating the validation dataset would be different.
[00:19:27.800 --> 00:19:31.600]   And that means there might be a leakage.
[00:19:31.600 --> 00:19:38.440]   So a leakage happens when your model learns something about the test set
[00:19:38.440 --> 00:19:43.400]   through a feature that you didn't intend to intend it to see.
[00:19:44.160 --> 00:19:46.400]   This is quite helpful in Kaggle competitions.
[00:19:46.400 --> 00:19:50.800]   If you're trying to win a hit, if you can find a leakage, you get that boost
[00:19:50.800 --> 00:19:56.400]   in your score and you get to win the competition for a serious model that
[00:19:56.400 --> 00:20:00.720]   you're trying to deploy to production, which in all fairness, I've never done.
[00:20:00.720 --> 00:20:07.280]   So I won't act too wise, but if you want to deploy a model at a very high
[00:20:07.280 --> 00:20:08.680]   level, that's accurate enough.
[00:20:08.680 --> 00:20:13.080]   You want it to be accurate in real life and not just accurate on your particular
[00:20:13.080 --> 00:20:18.000]   dataset. So that's why considering this is quite important.
[00:20:18.000 --> 00:20:22.440]   This is just one slide, but in the book, there's an entire discussion around how
[00:20:22.440 --> 00:20:25.560]   to set up a right validation dataset.
[00:20:25.560 --> 00:20:28.560]   I've already shared the incredible blog post by Rachel Thomas.
[00:20:28.560 --> 00:20:31.480]   I'd again, encourage everyone to check that out if you haven't.
[00:20:31.480 --> 00:20:37.720]   And now's the interesting bit where we can quickly bring all of these pieces
[00:20:37.720 --> 00:20:40.080]   together and start looking at the code.
[00:20:40.960 --> 00:20:46.320]   So just as a reminder, since we have new folks joining in today, I have simply
[00:20:46.320 --> 00:20:50.960]   pulled the entire GitHub repository and that's what we're looking through.
[00:20:50.960 --> 00:20:56.760]   So today we're on chapter 14, which means I'm inside of this folder and we can
[00:20:56.760 --> 00:20:58.920]   start by looking at nodule analysis.
[00:20:58.920 --> 00:21:06.560]   I think this was the first function that I wanted to cover as well.
[00:21:06.560 --> 00:21:11.600]   I have my notes right here, but again, bringing the pipeline together, most of
[00:21:11.600 --> 00:21:15.920]   these, these functions have been written by us a long, long time ago.
[00:21:15.920 --> 00:21:21.120]   So I won't go in detail for all of this, but I mentioned this because you're still
[00:21:21.120 --> 00:21:26.840]   very welcome to ask any questions that you like or really feel free to dive into
[00:21:26.840 --> 00:21:27.840]   anything that you want.
[00:21:27.840 --> 00:21:34.720]   Someone's asking for the GitHub link.
[00:21:34.720 --> 00:21:35.080]   Sure.
[00:21:35.240 --> 00:21:38.680]   Could you please direct your questions to this forum thread as well?
[00:21:38.680 --> 00:21:40.920]   And I'll request someone in the chat to post a link.
[00:21:40.920 --> 00:21:44.760]   Oh, no worries.
[00:21:44.760 --> 00:21:46.120]   Thanks for joining still Ramesh.
[00:21:46.120 --> 00:21:48.280]   Let me find the GitHub link.
[00:21:48.280 --> 00:21:54.760]   Is it on Manning?
[00:21:54.760 --> 00:21:56.440]   I don't think so.
[00:21:56.440 --> 00:22:10.960]   Yup.
[00:22:10.960 --> 00:22:11.520]   There we go.
[00:22:11.520 --> 00:22:16.120]   And let me post that link here.
[00:22:16.120 --> 00:22:20.320]   Thanks Mateo already shared the link.
[00:22:20.320 --> 00:22:20.920]   Thanks Mateo.
[00:22:20.920 --> 00:22:32.240]   Someone asked if I'm following the 2020 World Cup.
[00:22:32.240 --> 00:22:36.200]   No, I've been missing all of the F1 races as well, because I was more
[00:22:36.200 --> 00:22:38.880]   interested in telling everyone about Pytorch.
[00:22:38.880 --> 00:22:39.840]   It's so incredible.
[00:22:39.840 --> 00:22:43.360]   And I really regret missing the live F1 races as well.
[00:22:43.360 --> 00:22:48.880]   But I think Pytorch is much more exciting to me in my personal and professional life.
[00:22:49.240 --> 00:22:51.080]   So no, I'm not following the World Cup.
[00:22:51.080 --> 00:22:56.400]   Let's look at segment models.
[00:22:56.400 --> 00:23:00.480]   So the first step would be to segment all of these nodules that we've been looking at.
[00:23:00.480 --> 00:23:05.840]   We'd set torch to no grad and from there we'll grab the output.
[00:23:05.840 --> 00:23:12.400]   We set the D type because this is the type of tensor that we can expect from here.
[00:23:12.400 --> 00:23:15.280]   Is this text good enough for everybody?
[00:23:15.280 --> 00:23:16.480]   Should I zoom in further?
[00:23:16.480 --> 00:23:18.040]   Could someone please confirm?
[00:23:18.400 --> 00:23:21.240]   I'm using a different monitor today, so I'm not too sure.
[00:23:21.240 --> 00:23:29.320]   Mateo says it's fine for him.
[00:23:29.320 --> 00:23:31.360]   So I trust him enough to continue further.
[00:23:31.360 --> 00:23:40.920]   You call zeros.legs to basically look in the neighborhood of the set density.
[00:23:40.920 --> 00:23:46.520]   So to remind everyone, HU is the unit that we use for measuring the density.
[00:23:47.160 --> 00:23:51.600]   And we basically do the search at this point to look for those models.
[00:23:51.600 --> 00:23:59.920]   Now we can simply call in the data loader to initialize a segmentation data loaders
[00:23:59.920 --> 00:24:02.120]   and we can pass the series UID.
[00:24:02.120 --> 00:24:07.360]   So again, to people joining in, we have all of these IDs, especially for CT scans.
[00:24:07.360 --> 00:24:11.960]   And for this part, since we've designed the function, like, so we passed the series UID.
[00:24:11.960 --> 00:24:16.040]   Now we can simply loop over all of the data loaders.
[00:24:16.120 --> 00:24:21.840]   And from there we pass, ideally at this point, you'd be using Kaggle or Colab or
[00:24:21.840 --> 00:24:26.640]   your own Indie graphic card, which is not a hard requirement, but hopefully you'll
[00:24:26.640 --> 00:24:30.160]   be passing this onto your GPU for faster training.
[00:24:30.160 --> 00:24:34.680]   And from there, you can simply grab your predictions by calling the segment
[00:24:34.680 --> 00:24:36.640]   model, which we've defined earlier.
[00:24:36.640 --> 00:24:41.560]   It's a unit model and it works fairly well for the books purposes.
[00:24:43.120 --> 00:24:52.440]   And from here, we'll set this threshold of 50% and grab all of the outputs from there.
[00:24:52.440 --> 00:24:59.320]   We'll also call a binary erosion after it, and those will be added to the
[00:24:59.320 --> 00:25:01.120]   mask and we'll return this mask.
[00:25:01.120 --> 00:25:05.640]   So from here, we're trying to segment all of the tumors out and returning those
[00:25:05.640 --> 00:25:10.600]   masks to our calling function, which will be in the main task.
[00:25:11.600 --> 00:25:16.520]   Any questions around this function?
[00:25:16.520 --> 00:25:26.640]   I know I see quite a few new names today, but really at the end of this study
[00:25:26.640 --> 00:25:29.720]   group, I'm not sure if I should again go through the things again.
[00:25:29.720 --> 00:25:34.280]   Okay.
[00:25:34.280 --> 00:25:35.280]   I don't see any questions.
[00:25:35.280 --> 00:25:37.160]   I'll continue and come back to check again.
[00:25:37.160 --> 00:25:44.720]   The next function I want to cover is the classify candidates, which I'll
[00:25:44.720 --> 00:25:46.720]   have to again, painfully search for.
[00:25:46.720 --> 00:25:56.040]   Oh, it's right above.
[00:25:56.040 --> 00:26:03.040]   So now that we have these outputs, we can simply classify the candidates from there
[00:26:03.640 --> 00:26:09.160]   and we'll take in the candidate info list because those are the ones that we want to
[00:26:09.160 --> 00:26:13.720]   classify from and we can pass it to a data loader by initializing the
[00:26:13.720 --> 00:26:15.360]   classification data loader class.
[00:26:15.360 --> 00:26:25.440]   In this entire study group, there's never been a point where I've not
[00:26:25.440 --> 00:26:27.360]   finished my chai in less than 30 minutes.
[00:26:27.360 --> 00:26:31.640]   I always fill my cup for me to go for an entire hour.
[00:26:32.160 --> 00:26:35.200]   But that's been one of the major failures.
[00:26:35.200 --> 00:26:37.480]   Sorry about the side rant.
[00:26:37.480 --> 00:26:43.680]   Now we can initialize a classification list in which we'll have the classified
[00:26:43.680 --> 00:26:48.480]   tumors and we can iterate over the state a loader that we've created
[00:26:48.480 --> 00:26:50.400]   by passing the candidate info list.
[00:26:50.400 --> 00:26:56.240]   Again, you'd want to pause, pass this onto your device.
[00:26:56.680 --> 00:27:03.720]   Ideally this would just be .cuda, but usually in your code, you want to keep it
[00:27:03.720 --> 00:27:07.000]   as device, especially with open source code.
[00:27:07.000 --> 00:27:10.080]   You'd want most of you will to be able to use your stuff.
[00:27:10.080 --> 00:27:16.320]   So that's why it's a better option to also let folks run your code on the CPU, even
[00:27:16.320 --> 00:27:22.680]   though it's not the recommended approach from there, you can grab the probability
[00:27:22.880 --> 00:27:34.360]   and also see if it's a malignant tumor or not, and simply I treat all of these
[00:27:34.360 --> 00:27:40.160]   coordinates from there and return the right XYZ coordinates in a tuple format.
[00:27:40.160 --> 00:27:45.280]   And you can return this classification list after appending all of
[00:27:45.280 --> 00:27:46.680]   these tuple values to there.
[00:27:46.680 --> 00:27:49.320]   I'm glancing over.
[00:27:49.640 --> 00:27:54.080]   I'm not going through every single line because again, I assume we've
[00:27:54.080 --> 00:27:57.240]   gone through this many times that it should be straightforward to everyone.
[00:27:57.240 --> 00:28:00.960]   I say this because if, if it's not clear to you, please feel free to interrupt
[00:28:00.960 --> 00:28:02.600]   me and ask, ask about it again.
[00:28:02.600 --> 00:28:06.240]   But since we've gone through this multiple times, I also don't
[00:28:06.240 --> 00:28:07.640]   want to spend more time on it.
[00:28:07.640 --> 00:28:17.760]   After this, let's scroll up to the main function from where we will
[00:28:17.760 --> 00:28:19.640]   be calling both of these classes.
[00:28:19.640 --> 00:28:27.720]   So to point out the lines where these are called, I'm sorry, I didn't want to
[00:28:27.720 --> 00:28:31.160]   point out the lines where these are called, but above we have the main class
[00:28:31.160 --> 00:28:35.120]   and I'm sure everyone is familiar by we define main inside of Python files.
[00:28:35.120 --> 00:28:40.080]   If you're not, please do ask, but we set the threshold at 50%.
[00:28:40.080 --> 00:28:46.320]   So if our model things with a probability of more than 50% that this is a tumor,
[00:28:46.360 --> 00:28:48.120]   we say, okay, model, we trust you enough.
[00:28:48.120 --> 00:28:50.360]   And 50% is the threshold.
[00:28:50.360 --> 00:28:58.240]   We'll mark those or keep the threshold at that point.
[00:28:58.240 --> 00:29:01.640]   Now, again, I want to go through this point as well.
[00:29:01.640 --> 00:29:06.600]   So I made this discussion and argument about having human in the loop
[00:29:06.600 --> 00:29:09.920]   classifiers and having end to end pipelines.
[00:29:09.920 --> 00:29:14.880]   These are the sort of assumptions that go into different pipelines that
[00:29:16.120 --> 00:29:17.600]   the doctors need to know.
[00:29:17.600 --> 00:29:21.160]   So if we just send a black box model to them, they will know of it.
[00:29:21.160 --> 00:29:23.800]   And I want to emphasize this point.
[00:29:23.800 --> 00:29:32.080]   We also want to increase, not really increase more false positives are fine.
[00:29:32.080 --> 00:29:32.760]   Right.
[00:29:32.760 --> 00:29:37.440]   But more false negatives are not acceptable.
[00:29:37.440 --> 00:29:43.000]   So in the ideal best case scenario, you don't want either of them, but you
[00:29:43.000 --> 00:29:45.320]   definitely don't want false negatives.
[00:29:45.320 --> 00:29:50.320]   And you can maybe sacrifice a bit for false positive, maybe you can suggest
[00:29:50.320 --> 00:29:56.960]   another test to the person because you, what you don't want happening is a
[00:29:56.960 --> 00:30:01.080]   cancer patient being told they don't have cancer and later finding out that
[00:30:01.080 --> 00:30:01.680]   they do.
[00:30:01.680 --> 00:30:08.240]   And this also happens with human doctors, but since this is just more abstract,
[00:30:08.240 --> 00:30:10.600]   you want to be really careful about that as well.
[00:30:11.080 --> 00:30:16.360]   So I'm sorry, I'll be bringing these ethical topics up as well in today's
[00:30:16.360 --> 00:30:20.480]   discussion, but I really think it's, it's quite important to have this
[00:30:20.480 --> 00:30:21.520]   perspective as well.
[00:30:21.520 --> 00:30:24.920]   Going back to sharing my screen.
[00:30:24.920 --> 00:30:34.320]   I think after this, all that's left is to find you in the model.
[00:30:34.320 --> 00:30:37.000]   So let me find that function.
[00:30:37.000 --> 00:31:00.760]   I think it might be a different file or it should be in training probably.
[00:31:00.760 --> 00:31:11.160]   Initialize data loaders.
[00:31:11.160 --> 00:31:13.080]   No, I don't want to look at that, please.
[00:31:13.080 --> 00:31:21.080]   I'm sorry, I usually have the right line open, but Jupiter refreshes Safari
[00:31:21.080 --> 00:31:25.360]   actually refreshes the tab and then Jupiter just scrolls up to the top.
[00:31:25.360 --> 00:31:26.880]   So that's why I'm confusing myself.
[00:31:26.880 --> 00:31:27.800]   Sorry about this.
[00:31:27.800 --> 00:31:36.080]   Let me see if I can find it.
[00:31:36.080 --> 00:31:46.400]   I'm trying to find where the fine tuning happens, but even otherwise I can just
[00:31:46.400 --> 00:31:52.880]   speak at a higher level to like save everyone's time and embarrassment for me
[00:31:52.880 --> 00:31:56.480]   where I have to go through all of these lines of code.
[00:31:56.760 --> 00:31:57.920]   One second.
[00:31:57.920 --> 00:32:15.080]   I'm trying to scroll in the book to the right path, or I can just speak again
[00:32:15.080 --> 00:32:22.840]   for this without having any material.
[00:32:22.840 --> 00:32:25.520]   Let me connect my writing pad real quick.
[00:32:25.520 --> 00:32:26.400]   Sorry about that.
[00:32:26.400 --> 00:32:41.840]   For some reason, it's also choosing not to light up today.
[00:32:41.840 --> 00:32:42.360]   Okay.
[00:32:42.360 --> 00:33:05.680]   Okay, I don't want to waste any time for everyone, but I'll quickly speak about it.
[00:33:05.680 --> 00:33:11.920]   So far we've trained all of these models that we've been saving all along.
[00:33:11.920 --> 00:33:19.000]   And ideally, anytime you do that, you really want to reuse and now it's working.
[00:33:19.000 --> 00:33:26.520]   As you guys can see, I'm quite clumsy today because it's a different setup.
[00:33:26.520 --> 00:33:34.080]   So anytime you have any model that's been trained on a remotely similar
[00:33:34.080 --> 00:33:40.080]   data set, you'd always want to use the weights from here and apply
[00:33:40.080 --> 00:33:42.080]   these to your target problem.
[00:33:42.080 --> 00:33:43.560]   This is transfer learning.
[00:33:43.560 --> 00:33:48.240]   We've gone over this multiple times, but the reason we discussed this is because
[00:33:48.240 --> 00:33:51.240]   we've been training our model already.
[00:33:51.240 --> 00:33:56.280]   Parts of our pipeline on different bits.
[00:33:56.280 --> 00:34:01.400]   And we can simply, instead of throwing these weights away, we can simply use
[00:34:01.400 --> 00:34:05.360]   our model based from earlier and transfer them to the target problem,
[00:34:05.360 --> 00:34:06.680]   which is the final data set.
[00:34:06.680 --> 00:34:16.480]   In the book, you also learn about fine tuning just the last layer.
[00:34:16.480 --> 00:34:22.640]   So this is the head and to remind you, the head is the last layer.
[00:34:22.640 --> 00:34:24.480]   You have the backbone before that.
[00:34:24.480 --> 00:34:28.800]   And before that you have your tail.
[00:34:29.520 --> 00:34:34.200]   And this is quite the opposite because of the terminology where
[00:34:34.200 --> 00:34:35.760]   head is supposed to be the output.
[00:34:35.760 --> 00:34:45.200]   So am I sharing the right screen?
[00:34:45.200 --> 00:34:51.200]   I'm again, confusing myself, but I'll continue nevertheless.
[00:34:51.200 --> 00:34:53.240]   I'm assuming everyone can see what I'm writing.
[00:34:53.240 --> 00:34:54.040]   Sorry.
[00:34:54.040 --> 00:34:57.160]   Usually I have two split screens where I can see what I'm sharing, but
[00:34:57.720 --> 00:35:01.360]   I'm quite all over the place today.
[00:35:01.360 --> 00:35:02.840]   Okay.
[00:35:02.840 --> 00:35:04.040]   Thanks for confirming my tail.
[00:35:04.040 --> 00:35:07.040]   As you can see, it's live.
[00:35:07.040 --> 00:35:13.760]   And since, even after hosting 10 weeks of this, I still struggle quite a lot.
[00:35:13.760 --> 00:35:18.400]   So you have your model defined like so where you have the tail,
[00:35:18.400 --> 00:35:20.160]   the backbone and the head.
[00:35:20.160 --> 00:35:26.760]   Fast.ai takes care of this by itself, but whenever you're fine tuning your model,
[00:35:26.760 --> 00:35:29.160]   usually you just want to tune your head.
[00:35:29.160 --> 00:35:34.400]   Maybe, maybe even the backbone, but there's also another factor
[00:35:34.400 --> 00:35:37.080]   of using differential learning rate.
[00:35:37.080 --> 00:35:42.560]   So pardon my handwriting, but this is differential learning rates, which
[00:35:42.560 --> 00:35:47.080]   means the layers that are closer to the output.
[00:35:47.080 --> 00:35:52.080]   So this is more closer to the output should be trained a little more than
[00:35:52.080 --> 00:35:54.120]   the layers that are inside of the model.
[00:35:54.520 --> 00:36:00.720]   The reason being the layers inside are quite well trained.
[00:36:00.720 --> 00:36:04.480]   Maybe they've learned some structural details that you
[00:36:04.480 --> 00:36:06.240]   don't want to change at all.
[00:36:06.240 --> 00:36:09.920]   So it's best to leave those at that.
[00:36:09.920 --> 00:36:13.440]   Now let's go back to the Python file.
[00:36:13.440 --> 00:36:18.800]   And now that Mateo has kindly shared the function, we can scroll down to that.
[00:36:18.800 --> 00:36:21.320]   Thanks Mateo.
[00:36:21.320 --> 00:36:48.040]   So I want to point out Mateo said it's line 124.
[00:36:48.040 --> 00:36:49.680]   So I'm just trying to read this one.
[00:36:49.680 --> 00:36:54.400]   Yes, it is.
[00:36:54.400 --> 00:36:55.000]   Thanks Mateo.
[00:36:55.000 --> 00:37:04.640]   So from here, what we're doing is we're just loading our trained parameters
[00:37:04.640 --> 00:37:12.440]   and we can determine the fine tune blocks by the CLI argument.
[00:37:12.440 --> 00:37:17.600]   So again, I'm not going through this entire function, but this accepts CLI arguments.
[00:37:17.600 --> 00:37:22.120]   So whenever you see someone, especially with paper repositories, you can
[00:37:22.120 --> 00:37:32.240]   train them like, so usually you see some arguments like GPUs, 0, 1, 2.
[00:37:32.240 --> 00:37:35.280]   So if you have like a ton of them, you can specify them.
[00:37:35.280 --> 00:37:43.360]   And for this example, you can pass, you can really pass another argument that
[00:37:43.360 --> 00:37:47.280]   says fine tune the last 12 layers, please fine tune the last two layers.
[00:37:47.280 --> 00:37:49.920]   Please find you in the just last layer.
[00:37:49.920 --> 00:37:53.360]   So we accept a CLI argument for that.
[00:37:53.360 --> 00:37:55.960]   Fastly takes care of this internally.
[00:37:55.960 --> 00:38:02.840]   So the creators have this open-ended way of selecting which layer should you be fine
[00:38:02.840 --> 00:38:03.240]   tuning.
[00:38:03.240 --> 00:38:07.000]   And that is what is being set here.
[00:38:07.000 --> 00:38:15.240]   I think that was the line I wanted to point out.
[00:38:15.240 --> 00:38:22.920]   Let me again, hop back to the book and see if I have another thing marked.
[00:38:22.920 --> 00:38:36.080]   For some reason, my laptop's being slow today with the screen share.
[00:38:36.080 --> 00:38:39.280]   Okay.
[00:38:39.280 --> 00:38:44.560]   I was just making sure if there's anything left in the book to point out.
[00:38:44.600 --> 00:38:45.800]   I don't think there is.
[00:38:45.800 --> 00:38:54.160]   So we can quickly discuss about deploying to production.
[00:38:54.160 --> 00:38:59.400]   I won't be going into any details at all about this.
[00:38:59.400 --> 00:39:00.800]   And there's a good reason for that.
[00:39:00.800 --> 00:39:05.080]   I've never really put any interesting models of production.
[00:39:05.080 --> 00:39:06.720]   I know what those words mean.
[00:39:06.720 --> 00:39:10.920]   I've, I've been on teams that have put interesting models of production, but
[00:39:11.280 --> 00:39:17.520]   personally have not deployed, I'd say a deep learning model to production.
[00:39:17.520 --> 00:39:25.120]   And with that disclaimer out of the way, I want to go over what these words mean.
[00:39:25.120 --> 00:39:32.200]   And what do the authors refer to what the steps are mentioned in the book?
[00:39:32.200 --> 00:39:40.400]   Once you've trained these models that you spend weeks over, you really want your
[00:39:40.400 --> 00:39:44.400]   users or your end customers to be able to use them.
[00:39:44.400 --> 00:39:49.960]   So that's what we say when we, that's what we mean when we say deploying to production.
[00:39:49.960 --> 00:39:54.760]   Now, this could be totally different if you're creating an app, if you're
[00:39:54.760 --> 00:39:59.600]   creating a website, or if you're supposed to be working inside of hospitals, right?
[00:39:59.600 --> 00:40:04.440]   For this type of pipeline, you don't need instant results.
[00:40:04.480 --> 00:40:11.720]   No one wants to immediately know in a matter of nanoseconds, if their CT scan
[00:40:11.720 --> 00:40:18.040]   is cancerous or not, if they can, well, that's, that's well and good, but they're
[00:40:18.040 --> 00:40:19.320]   happy to wait half a day.
[00:40:19.320 --> 00:40:21.400]   And that's, that's quite a good time.
[00:40:21.400 --> 00:40:27.880]   And the reason I point these different steps out is because when you're trying
[00:40:27.880 --> 00:40:31.960]   to put your model into production, it really varies from industry to industry.
[00:40:31.960 --> 00:40:37.120]   So for finance, you'd want inference to be under milliseconds.
[00:40:37.120 --> 00:40:43.920]   People buy offices closer to the exchanges, dig in tunnels to put in
[00:40:43.920 --> 00:40:50.960]   optical fibers to have faster time to inference and even really, really slow,
[00:40:50.960 --> 00:40:55.520]   really, really minimal network latencies.
[00:40:55.520 --> 00:41:00.280]   In those industries, you have a completely different constraint versus an
[00:41:00.280 --> 00:41:07.000]   industry like the medical regime where you're dealing with human lives.
[00:41:07.000 --> 00:41:14.480]   Where you're dealing with this incredibly tough problem of having two pixels in a
[00:41:14.480 --> 00:41:16.520]   collection of two to the power 25.
[00:41:16.520 --> 00:41:21.160]   So with these things come different challenges.
[00:41:21.160 --> 00:41:23.400]   There are different business expectations.
[00:41:23.400 --> 00:41:27.360]   And that really affects how you can deploy these models.
[00:41:28.120 --> 00:41:32.000]   In the book, the authors teach you how to install a flask app.
[00:41:32.000 --> 00:41:33.440]   That is quite straightforward.
[00:41:33.440 --> 00:41:34.480]   I'm not going to do that.
[00:41:34.480 --> 00:41:37.560]   You also talk about how can you export the model?
[00:41:37.560 --> 00:41:43.720]   The bottom line is once you have these weights, which you can store in a pickle
[00:41:43.720 --> 00:41:49.360]   or any other format in an umpire file format, how do you send these to the
[00:41:49.360 --> 00:41:53.800]   endpoint where these can be used in whatever way it could be a touchscreen
[00:41:53.800 --> 00:41:55.200]   that a doctor is working with.
[00:41:55.680 --> 00:41:59.640]   It could be an app that runs it on your phone or on a website.
[00:41:59.640 --> 00:42:03.320]   With that come all sorts of interesting challenges.
[00:42:03.320 --> 00:42:09.920]   One thing I just forgot to mention was the fact that we also look at the
[00:42:09.920 --> 00:42:15.680]   evaluation metric, EUCROC on which I had written a short blog post.
[00:42:15.680 --> 00:42:19.040]   If I make other discussions do blur the lines.
[00:42:19.040 --> 00:42:25.280]   And I think so this was on a competition where I did really bad.
[00:42:25.880 --> 00:42:29.760]   But I was happy that this discussion was quite recognized.
[00:42:29.760 --> 00:42:31.640]   So I'll, I'll use this to talk about the thing.
[00:42:31.640 --> 00:42:37.600]   Uh, later authors introduced you to the EUCROC metric, which is better for such
[00:42:37.600 --> 00:42:39.920]   use cases like the one we're dealing with.
[00:42:39.920 --> 00:42:44.760]   It requires me to define a few fun, uh, functions, which I'll do first.
[00:42:44.760 --> 00:42:49.120]   So TP stands for true positive examples that have been classified
[00:42:49.120 --> 00:42:50.960]   positive while being positive.
[00:42:50.960 --> 00:42:55.200]   A false positive is something that's been classified as positive while
[00:42:55.200 --> 00:42:55.920]   being negative.
[00:42:55.920 --> 00:43:01.240]   Uh, so say I'm as a data scientist who's deployed incredible models to
[00:43:01.240 --> 00:43:02.840]   production is a false positive.
[00:43:02.840 --> 00:43:07.040]   Uh, because it's a positive statement, but it's false.
[00:43:07.040 --> 00:43:11.000]   That's a bad example, but these are examples that have been
[00:43:11.000 --> 00:43:12.520]   misclassified as positive.
[00:43:12.520 --> 00:43:18.240]   True negative is something that has been correctly classified as negative
[00:43:18.240 --> 00:43:22.440]   and false negative is something that's supposed to be positive and
[00:43:22.440 --> 00:43:23.640]   is classified as negative.
[00:43:24.120 --> 00:43:26.920]   And this is the thing we always strongly want to avoid because
[00:43:26.920 --> 00:43:29.000]   this creates serious problems.
[00:43:29.000 --> 00:43:36.240]   Uh, now something known as recall is defined as true positivity, which
[00:43:36.240 --> 00:43:41.520]   is the ratio of true positives divided by true positive plus false negative.
[00:43:41.520 --> 00:43:47.700]   False positive rate is defined as false positive divided by false
[00:43:47.700 --> 00:43:49.880]   positive plus true negative.
[00:43:51.520 --> 00:43:59.320]   ROC curve is the plot of true positive rates versus false positive rate.
[00:43:59.320 --> 00:44:10.440]   So the curve that denotes TPR versus FPR is the ROC curve.
[00:44:11.760 --> 00:44:21.640]   AUC curve refers to this area under this plot.
[00:44:21.640 --> 00:44:27.640]   So this line that denotes this plot of true positives versus false positives,
[00:44:27.640 --> 00:44:33.880]   the area under this curve is the AUC score.
[00:44:36.680 --> 00:44:45.280]   So a score greater than 0.5 means that the model is doing better than random.
[00:44:45.280 --> 00:44:47.920]   Uh, you can do the math, but that's the bottom line.
[00:44:47.920 --> 00:44:54.080]   0.5 means the model is completely random and the closer you get to one, it means
[00:44:54.080 --> 00:44:55.520]   the better your model is performing.
[00:44:55.520 --> 00:45:04.280]   So inside of, uh, the training, uh, Python file, the authors also define this.
[00:45:05.440 --> 00:45:13.360]   And, uh, one of the things I also want to point out, so you look at inside of this
[00:45:13.360 --> 00:45:15.960]   chapter, you also look at deployment options.
[00:45:15.960 --> 00:45:21.520]   Most of the times, wherever you want speedups, if you simply call JIT like so
[00:45:21.520 --> 00:45:25.040]   with Sun, so just in time compiler, it really speeds up a lot of function.
[00:45:25.040 --> 00:45:26.680]   You should know what you're doing.
[00:45:26.680 --> 00:45:31.800]   Uh, but if you know what you're doing, like, uh, a Kaggle grandmaster, such as
[00:45:31.800 --> 00:45:34.680]   CPMP, you can speed functions up.
[00:45:34.920 --> 00:45:40.720]   So another trick that goes into putting models into production is really
[00:45:40.720 --> 00:45:44.760]   benchmarking all of your files and understanding which ones do you want to
[00:45:44.760 --> 00:45:45.240]   change?
[00:45:45.240 --> 00:45:52.680]   Um, I'm trying to think of, I should be mentioning any other point from here.
[00:45:52.680 --> 00:45:54.160]   I don't think it's really required.
[00:45:54.160 --> 00:45:57.240]   So I'll hop back to this link.
[00:45:57.240 --> 00:46:01.840]   And I think that's mostly what I wanted to cover.
[00:46:01.840 --> 00:46:05.000]   So, uh, I think today's session will be quite short.
[00:46:05.000 --> 00:46:09.400]   Let me see if there are any questions in any of the charts, just to be sure.
[00:46:09.400 --> 00:46:17.400]   I see a question for AWS SageMaker.
[00:46:17.400 --> 00:46:20.800]   I'm not sure what the question is.
[00:46:20.800 --> 00:46:26.240]   Biswajit, could you please elaborate on your question?
[00:46:26.240 --> 00:46:39.640]   Okay.
[00:46:39.640 --> 00:46:41.640]   I don't see any questions anywhere.
[00:46:41.640 --> 00:46:50.240]   So, um, I'll give another minute for people to chime in, but I'll recap what
[00:46:50.240 --> 00:46:54.200]   we've managed to go through, through this entire study group.
[00:46:54.240 --> 00:46:59.600]   Uh, so far we've managed to learn about, uh, PyTorch's functions.
[00:46:59.600 --> 00:47:02.800]   We've managed to learn what a training loop looks like.
[00:47:02.800 --> 00:47:04.800]   How can we train models on the GPU?
[00:47:04.800 --> 00:47:07.160]   How do we create data loaders?
[00:47:07.160 --> 00:47:09.080]   Why is a dataset class necessary?
[00:47:09.080 --> 00:47:14.440]   From there, we started to build this end to end pipeline and we've managed to
[00:47:14.440 --> 00:47:16.720]   create a working end to end pipeline.
[00:47:16.720 --> 00:47:20.480]   Uh, I just learned so the details, but assuming everyone has been reading the
[00:47:20.480 --> 00:47:23.000]   book, you've managed to complete that.
[00:47:23.120 --> 00:47:30.600]   Um, what I want to mention here to ruin the moment in a way is this is what you're
[00:47:30.600 --> 00:47:35.000]   supposed to be doing in one day in your job, at your job ideally.
[00:47:35.000 --> 00:47:40.160]   So this end to end pipeline, uh, doesn't work quite well, according to the real
[00:47:40.160 --> 00:47:45.200]   world, this is what is ideally done on a Kaggle competition.
[00:47:45.200 --> 00:47:47.600]   This could even be done in six hours.
[00:47:47.600 --> 00:47:53.680]   So that is a speed at which, uh, we should be aiming for to operate.
[00:47:53.680 --> 00:47:58.520]   So to ruin the moment, but not quite, it's been an incredible learning journey,
[00:47:58.520 --> 00:48:02.280]   especially for me as well, being on the other side, I have to really make sure I
[00:48:02.280 --> 00:48:05.880]   understand every single line, understand every single concept.
[00:48:05.880 --> 00:48:10.000]   So in that aspect, I've enjoyed learning a lot.
[00:48:10.000 --> 00:48:14.920]   Uh, but I see, I seen all of you show up every week and it's been an incredible
[00:48:14.920 --> 00:48:17.840]   journey and I hope for you as well.
[00:48:17.840 --> 00:48:19.520]   I hope you've enjoyed it.
[00:48:19.520 --> 00:48:23.000]   Uh, so it's, it's a moment for all of us to part our backs really.
[00:48:23.000 --> 00:48:31.600]   I want to mention a few names, people that have really been contributing to
[00:48:31.600 --> 00:48:34.080]   these reading groups a lot.
[00:48:34.080 --> 00:48:37.880]   Uh, these are in alphabetical orders, but I'll still, still mention them.
[00:48:37.880 --> 00:48:44.560]   Uh, Adil, Vibhavashu, Gautam, Girijesh, Hari Sumanth.
[00:48:44.600 --> 00:48:47.160]   Also known as simply, simply Sumanth on our forums.
[00:48:47.160 --> 00:48:53.080]   Mateo Prabhav, uh, Prabhav is I think deep learner 007.
[00:48:53.080 --> 00:48:55.360]   I wish I had taken that username.
[00:48:55.360 --> 00:48:58.680]   Uh, Ravi, Vinayak and Yuvraj.
[00:48:58.680 --> 00:49:02.360]   All of these folks have really been contributing to our forums, uh, the
[00:49:02.360 --> 00:49:07.920]   study group, and also have written incredible blog posts, a good number of
[00:49:07.920 --> 00:49:11.720]   these folks, I think all of them have been writing incredible stuff.
[00:49:12.160 --> 00:49:17.080]   So, uh, really thank you for being such a central part of the study group.
[00:49:17.080 --> 00:49:21.040]   Uh, at Weights and Biases we do this just for the community and to me, it's really,
[00:49:21.040 --> 00:49:24.720]   it feels an honor to be a part of your journey, honestly.
[00:49:24.720 --> 00:49:27.720]   So, uh, thanks for all of these things.
[00:49:27.720 --> 00:49:32.320]   And with that, even though it's the last study group, I'll give some homework.
[00:49:32.320 --> 00:49:34.800]   The first one is to really congratulate yourself.
[00:49:34.800 --> 00:49:39.280]   You've managed to bear with me, put up with me teaching you something for 10 weeks.
[00:49:39.280 --> 00:49:41.280]   That's, that's a feat in itself.
[00:49:41.320 --> 00:49:42.600]   Uh, thanks for doing that.
[00:49:42.600 --> 00:49:45.760]   Now is the point where you can get started with Kaggle.
[00:49:45.760 --> 00:49:48.760]   So please do that as well.
[00:49:48.760 --> 00:49:52.480]   Continue blogging about PyTorch.
[00:49:52.480 --> 00:49:55.000]   I assume everyone has figured that process out.
[00:49:55.000 --> 00:49:57.080]   If you've not, please reach out to me.
[00:49:57.080 --> 00:49:58.680]   Please feel free to ask me anything.
[00:49:58.680 --> 00:50:01.720]   Please feel free to ask for my help or anyone's help on the forums.
[00:50:01.720 --> 00:50:04.320]   We'd be most happy to help you get started on that journey.
[00:50:04.320 --> 00:50:08.720]   That is one of the strongest signals to the world that you know about PyTorch.
[00:50:08.720 --> 00:50:11.960]   And I really, really, really want you to get started with that.
[00:50:11.960 --> 00:50:17.160]   If you haven't start writing about paper implementations, the best step right
[00:50:17.160 --> 00:50:21.680]   now would be to just take a paper that's in TensorFlow and put it to PyTorch.
[00:50:21.680 --> 00:50:26.760]   That is an incredibly difficult task in a way, but now you're at the
[00:50:26.760 --> 00:50:28.320]   point where you can take that up.
[00:50:28.320 --> 00:50:32.760]   And you should also consider joining our next study group, which I'll
[00:50:32.760 --> 00:50:34.880]   mention right now, since everyone is here.
[00:50:34.880 --> 00:50:45.280]   A small group of us have been working on rewriting the fast AI machine
[00:50:45.280 --> 00:50:50.800]   learning course and still a work in the progress, but the goal for us is
[00:50:50.800 --> 00:50:56.600]   to create an updated version of fast AI machine learning for coders.
[00:50:56.600 --> 00:50:59.760]   It won't be called that because we'll also be adding a few things there.
[00:51:00.680 --> 00:51:07.720]   If you're interested in contributing to that, and that is unrelated to deep
[00:51:07.720 --> 00:51:11.160]   learning, just let me know and I'd be happy to invite you there.
[00:51:11.160 --> 00:51:15.360]   So if you want to contribute to a course that we're building next, and I'll
[00:51:15.360 --> 00:51:17.560]   hopefully launch that in about three weeks.
[00:51:17.560 --> 00:51:21.160]   And if anyone knows me, that would probably be less than that, because
[00:51:21.160 --> 00:51:23.280]   I'm quite excited to kick it off.
[00:51:23.280 --> 00:51:45.640]   You could just leave a like here and I'll add you to the respective category.
[00:51:45.640 --> 00:51:51.280]   We mostly work async, but I really want to keep this group open to everyone.
[00:51:51.520 --> 00:51:55.120]   So if you're at all interested, please just like this comment and I'll
[00:51:55.120 --> 00:51:56.600]   invite you to the right category.
[00:51:56.600 --> 00:51:59.480]   I have one more request to everyone.
[00:51:59.480 --> 00:52:04.200]   I probably have done the stupidest thing possible, which is just read
[00:52:04.200 --> 00:52:06.320]   the book and summarize it every week.
[00:52:06.320 --> 00:52:11.560]   This was possible just because we have such incredible authors
[00:52:11.560 --> 00:52:13.800]   who've really created this book.
[00:52:13.800 --> 00:52:24.840]   So I sincerely want you to request you to really thank the authors in this thread.
[00:52:24.840 --> 00:52:29.120]   I want to send this to them as my note of thanks.
[00:52:29.120 --> 00:52:32.680]   So it will be better if all of the folks who have been learning, hopefully
[00:52:32.680 --> 00:52:36.040]   know more than me now can also leave a few words.
[00:52:36.040 --> 00:52:42.600]   So if you've enjoyed the book, I would really request you to write a few
[00:52:42.600 --> 00:52:47.120]   words for the author in this thread, please, so that we can send it as a thanks to them.
[00:52:47.120 --> 00:52:54.680]   And with that, I will be reaching out to a few folks in this list as a note of
[00:52:54.680 --> 00:52:56.600]   thanks from Weights and Biases.
[00:52:56.600 --> 00:52:58.360]   We'll be sending some swag your way.
[00:52:58.360 --> 00:53:01.680]   I've managed to negotiate that with the team.
[00:53:01.680 --> 00:53:02.840]   I've really fought for it.
[00:53:02.840 --> 00:53:05.560]   And I really want to thank everyone who's been a part of the group.
[00:53:05.560 --> 00:53:08.040]   So you'll be hearing from us soon.
[00:53:09.720 --> 00:53:14.000]   We might also distribute some compute credits.
[00:53:14.000 --> 00:53:16.360]   I'll try to make that possible.
[00:53:16.360 --> 00:53:23.480]   But again, it's been an incredible journey and I know all of us are just getting started.
[00:53:23.480 --> 00:53:25.560]   So I hope you continue learning about PyTorch.
[00:53:25.560 --> 00:53:28.320]   I hope you keep joining our study groups.
[00:53:28.320 --> 00:53:33.920]   I'll be launching the FastML group after this, and I'll also help host
[00:53:33.920 --> 00:53:35.280]   the paper reading groups from now.
[00:53:35.280 --> 00:53:39.040]   You're at the point where you can learn from both of those.
[00:53:39.040 --> 00:53:40.680]   So I want to encourage you to do that.
[00:53:40.680 --> 00:53:45.920]   But even more important than that, even more important than the fact for you to join any
[00:53:45.920 --> 00:53:50.760]   study group is to implement these things and really, really caggle or try to experiment that.
[00:53:50.760 --> 00:54:00.920]   So with all of that not so useful wisdom, if I may, I'll probably wrap up today half an
[00:54:00.920 --> 00:54:02.920]   hour early because we're at the end of the book.
[00:54:02.920 --> 00:54:04.760]   But thanks again, everyone.
[00:54:04.760 --> 00:54:07.440]   And you're always welcome to ask any questions on the forums.
[00:54:07.440 --> 00:54:12.680]   And I look forward to seeing all of you around on the forums or your book everywhere.
[00:54:12.680 --> 00:54:20.600]   I'll take one last look at the questions.
[00:54:20.600 --> 00:54:21.600]   If I see any.
[00:54:21.600 --> 00:54:27.800]   No, I do not.
[00:54:27.800 --> 00:54:28.760]   Awesome.
[00:54:28.760 --> 00:54:30.280]   I'll end the live stream.
[00:54:30.280 --> 00:54:31.080]   Thanks, everyone.
[00:54:31.080 --> 00:54:32.480]   I'll see you around on the forums.
[00:54:32.480 --> 00:54:37.480]   .
[00:54:37.480 --> 00:54:42.480]   .

